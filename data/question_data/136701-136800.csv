,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Spring Calculation - find mass,Spring Calculation - find mass,,"A spring with an $-kg$ mass and a damping constant $9$ can be held stretched $2.5 \text{ meters}$ beyond its natural length by a force of $7.5 \text{ Newtons}$. If the spring is stretched $5 \text{ meters}$ beyond its natural length and then released with zero velocity, find the mass that would produce critical damping. My work: The restoring force is $-kx$.  Then $$7.5 = -k(2.5) \\ -\frac{7.5}{2.5} = k \\  ma = -\frac{7.5x}{2.5} \\ my’’ + 9y’ + -3y = 0,\quad y(0) = 2.5, y(5) = 0 \\ \frac{-9 \pm \sqrt{81 + 4(m)(3)}}{2m} \\ -\frac{9}{2m} \pm \frac{\sqrt{81+12m}}{2m} \\ y = Ae^{-(9/2)x}\cos\left(\frac{\sqrt{81+12m}}{2m}x\right) + Be^{-(9/2)x}\sin\left(\frac{\sqrt{81+12m}}{2m}x\right) \\ 2.5 = A + B\cdot 0 \\ 0 = (2.5)e^{-45/2}\cos\left(\sqrt{81+12m}\frac{5}{2m}\right) + Be^{-45/2}\sin\left(\sqrt{81+12m}\frac{5}{2m}\right)$$ Any help would be appreciated","A spring with an $-kg$ mass and a damping constant $9$ can be held stretched $2.5 \text{ meters}$ beyond its natural length by a force of $7.5 \text{ Newtons}$. If the spring is stretched $5 \text{ meters}$ beyond its natural length and then released with zero velocity, find the mass that would produce critical damping. My work: The restoring force is $-kx$.  Then $$7.5 = -k(2.5) \\ -\frac{7.5}{2.5} = k \\  ma = -\frac{7.5x}{2.5} \\ my’’ + 9y’ + -3y = 0,\quad y(0) = 2.5, y(5) = 0 \\ \frac{-9 \pm \sqrt{81 + 4(m)(3)}}{2m} \\ -\frac{9}{2m} \pm \frac{\sqrt{81+12m}}{2m} \\ y = Ae^{-(9/2)x}\cos\left(\frac{\sqrt{81+12m}}{2m}x\right) + Be^{-(9/2)x}\sin\left(\frac{\sqrt{81+12m}}{2m}x\right) \\ 2.5 = A + B\cdot 0 \\ 0 = (2.5)e^{-45/2}\cos\left(\sqrt{81+12m}\frac{5}{2m}\right) + Be^{-45/2}\sin\left(\sqrt{81+12m}\frac{5}{2m}\right)$$ Any help would be appreciated",,"['ordinary-differential-equations', 'physics', 'classical-mechanics']"
1,Proving conservation of mass for linear advection,Proving conservation of mass for linear advection,,"Reading through some course notes about conservation of mass in linear advection approximation schemes, given $\phi(x, t) \in \mathbb{R}$ and is defined for $0 \leq x < 1$ with periodic boundary conditions, and given mass $M = \int_0^1{\phi dx}$ the proof begins as follows: $$\frac{dM}{dt} = \frac{d}{dt} \int_0^1 \phi dx = \int_0^1 {\frac{d\phi}{dt} dx} $$ A couple of questions: How can we justify moving $\frac{d}{dt}$ inside the integral? Shouldn't this now be a partial differential $\frac{\partial \phi}{\partial t}$ since $\phi$ is a function of space ($x$) and time ($t$)? The proof continues: $$-u \int_0^1\frac{d\phi}{dx}dx = -u \int_0^1 d \phi = -u \left[ \phi \right]_0^1 = 0$$ since $\phi(0, t) = \phi(1, t) \forall t$ due to the periodic boundary.","Reading through some course notes about conservation of mass in linear advection approximation schemes, given $\phi(x, t) \in \mathbb{R}$ and is defined for $0 \leq x < 1$ with periodic boundary conditions, and given mass $M = \int_0^1{\phi dx}$ the proof begins as follows: $$\frac{dM}{dt} = \frac{d}{dt} \int_0^1 \phi dx = \int_0^1 {\frac{d\phi}{dt} dx} $$ A couple of questions: How can we justify moving $\frac{d}{dt}$ inside the integral? Shouldn't this now be a partial differential $\frac{\partial \phi}{\partial t}$ since $\phi$ is a function of space ($x$) and time ($t$)? The proof continues: $$-u \int_0^1\frac{d\phi}{dx}dx = -u \int_0^1 d \phi = -u \left[ \phi \right]_0^1 = 0$$ since $\phi(0, t) = \phi(1, t) \forall t$ due to the periodic boundary.",,"['ordinary-differential-equations', 'numerical-methods']"
2,Help with I.V.P,Help with I.V.P,,"is the I.V.P: $$\begin{cases} \dfrac{dy}{dx}=\sin(e^{y})\\[8pt] y(0)=a \end{cases} \text{ where } a\in \mathbb{R}$$ a) Prove that the equation has unique solution $y(x,a)$ and the maximum range is $\langle- \infty, \infty\rangle$ b) Prove: $|y(x,a)-a |\le x$ I have tried to solve the equation $$\int \frac{dy}{\sin(e^{y})}=\int dx$$ for starters but no solution. any idea how to start?","is the I.V.P: $$\begin{cases} \dfrac{dy}{dx}=\sin(e^{y})\\[8pt] y(0)=a \end{cases} \text{ where } a\in \mathbb{R}$$ a) Prove that the equation has unique solution $y(x,a)$ and the maximum range is $\langle- \infty, \infty\rangle$ b) Prove: $|y(x,a)-a |\le x$ I have tried to solve the equation $$\int \frac{dy}{\sin(e^{y})}=\int dx$$ for starters but no solution. any idea how to start?",,['ordinary-differential-equations']
3,Construction of Homogenous Differential Equation,Construction of Homogenous Differential Equation,,"We have to construct a homogenous differential equation of second order which has y(t) = e^t cos(t) as solution. I know have some knowledge of how to solve (some) differential equations, but how do I construct one (systematically)?","We have to construct a homogenous differential equation of second order which has y(t) = e^t cos(t) as solution. I know have some knowledge of how to solve (some) differential equations, but how do I construct one (systematically)?",,[]
4,Solving (Frenet-Serret) differential equation system in Matlab,Solving (Frenet-Serret) differential equation system in Matlab,,"I'm about (trying) to solve the Frenet-Serret equation given by the known formulas, finding $e(s)$, $n(s)$, $b(s)$, where $e'(s) = \kappa(s)v(s)n(s)$ $n'(s) = -\kappa(s)v(s)e(s) + \tau(s)v(s)b(s)$ $b'(s) = -\tau(s)v(s)n(s)$ with the initial values $e(0)$, $n(0)$, $b(0)$, and given $\kappa$, $\tau$ and $v$. I have to solve this in Matlab, but I've got no idea, what function to use and how to parametrize it. Tried dsolve , but got syntax errors all the way. Help appreciated!","I'm about (trying) to solve the Frenet-Serret equation given by the known formulas, finding $e(s)$, $n(s)$, $b(s)$, where $e'(s) = \kappa(s)v(s)n(s)$ $n'(s) = -\kappa(s)v(s)e(s) + \tau(s)v(s)b(s)$ $b'(s) = -\tau(s)v(s)n(s)$ with the initial values $e(0)$, $n(0)$, $b(0)$, and given $\kappa$, $\tau$ and $v$. I have to solve this in Matlab, but I've got no idea, what function to use and how to parametrize it. Tried dsolve , but got syntax errors all the way. Help appreciated!",,"['ordinary-differential-equations', 'differential-geometry', 'matlab', 'plane-curves']"
5,"After Laplace Transformations, how do initial conditions come into play?","After Laplace Transformations, how do initial conditions come into play?",,"The Laplace transform is relatively simple to derive but my solution was wrong. I am positive it is because of the initial conditions. Find the laplace transform f(t)={0 for t < 3, (t-3)^2 for t greater than or equal to 3} My solution was 2/(s^3)-6/(s^2)+9/s","The Laplace transform is relatively simple to derive but my solution was wrong. I am positive it is because of the initial conditions. Find the laplace transform f(t)={0 for t < 3, (t-3)^2 for t greater than or equal to 3} My solution was 2/(s^3)-6/(s^2)+9/s",,"['ordinary-differential-equations', 'laplace-transform']"
6,Centre manifold non-hyperbolic fixed points,Centre manifold non-hyperbolic fixed points,,"I have the following dynamical system $\dot x=-x^3, \dot y=-y$. I would like to prove that there are an infinite number of trajectories that become tangent to the line $y=0$ for $x\rightarrow 0$. Each of the trajectories is a centre manifold and they satisfy $y=h(x)$ where all possible forms of $h$ have zero taylor expansiion about origin. First of all we know that the line $y=0$ is invariant. The only fixed point is $(0,0)$, the eigenvalues are $-1,0$ with Eigenvectors $(0,1), (1,0)$. How can I proceed? Do I have to find some guess for $h(x)?$","I have the following dynamical system $\dot x=-x^3, \dot y=-y$. I would like to prove that there are an infinite number of trajectories that become tangent to the line $y=0$ for $x\rightarrow 0$. Each of the trajectories is a centre manifold and they satisfy $y=h(x)$ where all possible forms of $h$ have zero taylor expansiion about origin. First of all we know that the line $y=0$ is invariant. The only fixed point is $(0,0)$, the eigenvalues are $-1,0$ with Eigenvectors $(0,1), (1,0)$. How can I proceed? Do I have to find some guess for $h(x)?$",,"['ordinary-differential-equations', 'dynamical-systems']"
7,Proof that a ODE system admits symmetric solutions,Proof that a ODE system admits symmetric solutions,,"I have a ODE system of the form $\dot{x} = f(x)$ with $x \in \mathbb{R}^3$. Now it is claimed that if $(x_1,x_2,x_3)$ is a solution to the system that also $(-x_1,-x_2,-x_3)$ is a solution. How can I show that this holds, I'm namely not sure how to deal with this? I first thought that I would just show that $-f(x) = f(-x)$  but when computing I saw I wasn't getting anywhere. ps. If you are wondering why I did not post the ODE system itself, it is because I want to try it for myself. -edit- Since I am not really getting response the ODE about which it is about it is $$\dot{x}_1 = -a_1 x_1 + b_1 x_2 \\ \dot{x}_2 = -a_2 x_2 + b_2 x_1 x_3 \\ \dot{x}_3 = -a_3(x_3 - \lambda) - b_3 x_1 x_2 $$ -edit 2- A friend of mine told me that if I would calculate the fixed points of the system with $(-x_1,-x_2,-x_3)$ that they give the same fixed points as $(x_1,x_2,x_3)$ and that it would imply that the solutions are the same? But I cannot see how this would imply that. -edit 3- apparently there was a error in the exercise that I had. I had to proof that if $(x_1,x_2,x_3)$ then $(-x_1,-x_2,x_3)$ was also a solution and not $-x_3$. Subsituting then these variables then in the system delivers the same state space system.","I have a ODE system of the form $\dot{x} = f(x)$ with $x \in \mathbb{R}^3$. Now it is claimed that if $(x_1,x_2,x_3)$ is a solution to the system that also $(-x_1,-x_2,-x_3)$ is a solution. How can I show that this holds, I'm namely not sure how to deal with this? I first thought that I would just show that $-f(x) = f(-x)$  but when computing I saw I wasn't getting anywhere. ps. If you are wondering why I did not post the ODE system itself, it is because I want to try it for myself. -edit- Since I am not really getting response the ODE about which it is about it is $$\dot{x}_1 = -a_1 x_1 + b_1 x_2 \\ \dot{x}_2 = -a_2 x_2 + b_2 x_1 x_3 \\ \dot{x}_3 = -a_3(x_3 - \lambda) - b_3 x_1 x_2 $$ -edit 2- A friend of mine told me that if I would calculate the fixed points of the system with $(-x_1,-x_2,-x_3)$ that they give the same fixed points as $(x_1,x_2,x_3)$ and that it would imply that the solutions are the same? But I cannot see how this would imply that. -edit 3- apparently there was a error in the exercise that I had. I had to proof that if $(x_1,x_2,x_3)$ then $(-x_1,-x_2,x_3)$ was also a solution and not $-x_3$. Subsituting then these variables then in the system delivers the same state space system.",,"['ordinary-differential-equations', 'symmetry']"
8,Is this ODE solvable?,Is this ODE solvable?,,"this equation popped up when I was trying to apply math: $$Af^2+B\left(\frac{df}{dx}\right)^2=C$$ Where $A,B,C$ are positive constants. Also,  $~f(0)=D$, another positive constant. What are the solutions? Is this solvable? If not, are there any texts you recommend to read? BTW you might want to know background: I am solving for the maximum velocity profile $v(s)$ for a car traveling on a curved path $\gamma(s)$ under the constraint that the max passenger g force, call it $\sqrt{C}$, is not exceeded. This results in $$\sqrt{g_l^2+g_r^2}=\sqrt{C}~~\to~~\left(\frac{v^2}{r}\right)^2+\left(\frac{dv}{dt}\right)^2=C$$ Where $r$ is the radius of curvature at the point $\gamma(s)$. I then made the substitution $f=v^2$ to get to an equation analogous to the one above. Also, I took $r$ to be piecewise (which is a good approximation in my usage case), which is why $A$ is constant.","this equation popped up when I was trying to apply math: $$Af^2+B\left(\frac{df}{dx}\right)^2=C$$ Where $A,B,C$ are positive constants. Also,  $~f(0)=D$, another positive constant. What are the solutions? Is this solvable? If not, are there any texts you recommend to read? BTW you might want to know background: I am solving for the maximum velocity profile $v(s)$ for a car traveling on a curved path $\gamma(s)$ under the constraint that the max passenger g force, call it $\sqrt{C}$, is not exceeded. This results in $$\sqrt{g_l^2+g_r^2}=\sqrt{C}~~\to~~\left(\frac{v^2}{r}\right)^2+\left(\frac{dv}{dt}\right)^2=C$$ Where $r$ is the radius of curvature at the point $\gamma(s)$. I then made the substitution $f=v^2$ to get to an equation analogous to the one above. Also, I took $r$ to be piecewise (which is a good approximation in my usage case), which is why $A$ is constant.",,['ordinary-differential-equations']
9,Show that the curves are circle.,Show that the curves are circle.,,"For example, this question is also similar to the previous question I have asked, which is the following link; How to show the curves are conics. Question: Solve the equation $$\frac{dx}{cy-bz}=\frac{dy}{az-cx}=\frac{dz}{bx-ay}$$ And show that they are circles. I solve the question Firstly I found the integral curves. Let $$P=cy-bz$$ $$Q=az-cx$$ $$R=bx-ay$$ Let's take $P'=a$ $Q'=b$ and $R'=c$ such that $PP'+QQ'+RR'=0$. Then $c_1=ax+by+cz$ Let $$\frac{xdx+ydy}{cxy-bxz+ayz-cxy}=\frac{dz}{bx-ay}$$ $$\frac{xdx+ydy}{z(ay-bx)}=\frac{dz}{-(ay-bx)}$$ Then $c_2=x^2+y^2+z^2$ However, honestly I dont know how to show that they are circles. And I cannot show the integral curves at previous question which I posted its link here above. Please help me my questions. I am new learner of PDE.","For example, this question is also similar to the previous question I have asked, which is the following link; How to show the curves are conics. Question: Solve the equation $$\frac{dx}{cy-bz}=\frac{dy}{az-cx}=\frac{dz}{bx-ay}$$ And show that they are circles. I solve the question Firstly I found the integral curves. Let $$P=cy-bz$$ $$Q=az-cx$$ $$R=bx-ay$$ Let's take $P'=a$ $Q'=b$ and $R'=c$ such that $PP'+QQ'+RR'=0$. Then $c_1=ax+by+cz$ Let $$\frac{xdx+ydy}{cxy-bxz+ayz-cxy}=\frac{dz}{bx-ay}$$ $$\frac{xdx+ydy}{z(ay-bx)}=\frac{dz}{-(ay-bx)}$$ Then $c_2=x^2+y^2+z^2$ However, honestly I dont know how to show that they are circles. And I cannot show the integral curves at previous question which I posted its link here above. Please help me my questions. I am new learner of PDE.",,"['ordinary-differential-equations', 'differential-geometry', 'partial-differential-equations', 'self-learning']"
10,$\dot y=y^2-t$ Differential Equation,Differential Equation,\dot y=y^2-t,Liouville proved that the differential equation $\dot y=y^2-t$ does not have a solution in form of algebraic equations. Do you know any reference where I can find the proof for that?,Liouville proved that the differential equation $\dot y=y^2-t$ does not have a solution in form of algebraic equations. Do you know any reference where I can find the proof for that?,,"['ordinary-differential-equations', 'reference-request']"
11,"Solve the pde $u_t(x,t)=u_{xx}(x,t)-bu(x,t)+q(t)$ for $u(x,t)$",Solve the pde  for,"u_t(x,t)=u_{xx}(x,t)-bu(x,t)+q(t) u(x,t)","I have the example pde $u_t(x,t)=u_{xx}(x,t)-b(t)u(x,t)+q_0$, where $b(t)$ is a function of only $t$ and $q_0$ is a constant, $0<x<\pi$, $t>0$. The subscripts denote derivatives. I also have some boundary conditions: $u(x,0)=0$ and $u(0,t)=0=u(\pi,t)$. The problem solution using variation of parameters is $$u(x,t)=\frac{4q_0}{\pi a(t)}\sum\limits_{n=1}^\infty \frac{\sin((2n-1)x)}{2n-1}\int\limits_0^t e^{-(2n-1)^2(t-\tau)}a(\tau)d\tau$$ where $$a(t)=e^{\int\limits_0^t b(\sigma)d\sigma}.$$ The problem I'm working on is slightly different. It gives the pde $u_t(x,t)=u_{xx}(x,t)-bu(x,t)+q(t)$ where $b$ is a constant and $q(t)$ is a function of only $t$, $0<x<\pi$, $t>0$. The solution should be very similar to that of the example. I don't know where to start. I'm using the textbook ""Fourier Series and Boundary Value Problems"" by Churchill and Brown, 7th edition, if anyone's interested. The ""example"" is problem 10 page 117 of that book. Any help/hints would be greatly appreciated. Thanks!","I have the example pde $u_t(x,t)=u_{xx}(x,t)-b(t)u(x,t)+q_0$, where $b(t)$ is a function of only $t$ and $q_0$ is a constant, $0<x<\pi$, $t>0$. The subscripts denote derivatives. I also have some boundary conditions: $u(x,0)=0$ and $u(0,t)=0=u(\pi,t)$. The problem solution using variation of parameters is $$u(x,t)=\frac{4q_0}{\pi a(t)}\sum\limits_{n=1}^\infty \frac{\sin((2n-1)x)}{2n-1}\int\limits_0^t e^{-(2n-1)^2(t-\tau)}a(\tau)d\tau$$ where $$a(t)=e^{\int\limits_0^t b(\sigma)d\sigma}.$$ The problem I'm working on is slightly different. It gives the pde $u_t(x,t)=u_{xx}(x,t)-bu(x,t)+q(t)$ where $b$ is a constant and $q(t)$ is a function of only $t$, $0<x<\pi$, $t>0$. The solution should be very similar to that of the example. I don't know where to start. I'm using the textbook ""Fourier Series and Boundary Value Problems"" by Churchill and Brown, 7th edition, if anyone's interested. The ""example"" is problem 10 page 117 of that book. Any help/hints would be greatly appreciated. Thanks!",,"['ordinary-differential-equations', 'partial-differential-equations']"
12,$\frac{dx}{dt} = |x|^{1/2}$,,\frac{dx}{dt} = |x|^{1/2},"Im looking to find 4 solutions to the ODE : $\frac{dx}{dt} = |x|^{1/2} , x(0)=0$. Clearly, $x=0$ is one solution. Using seperation of variables for $x>0$ yields $x= t^2/4$ as another solution, and if we consider $x<0$, I find that $x = -t^2/4$. Could someone give a hint as to where I am missing the last solution? Thanks!","Im looking to find 4 solutions to the ODE : $\frac{dx}{dt} = |x|^{1/2} , x(0)=0$. Clearly, $x=0$ is one solution. Using seperation of variables for $x>0$ yields $x= t^2/4$ as another solution, and if we consider $x<0$, I find that $x = -t^2/4$. Could someone give a hint as to where I am missing the last solution? Thanks!",,['ordinary-differential-equations']
13,Proving that $\frac{d}{dt}x=\sqrt{|x|}+a$ has unique solution,Proving that  has unique solution,\frac{d}{dt}x=\sqrt{|x|}+a,"Can you help me with this question, please : Given $a>0$,  I want to show that $\frac{d}{dt}x = \sqrt{|x|} + a $, with $x(0)=x_{0}$, has an unique solution. The existence is granted by Peano's Theorem, but I'm having problems proving that there is uniqueness.","Can you help me with this question, please : Given $a>0$,  I want to show that $\frac{d}{dt}x = \sqrt{|x|} + a $, with $x(0)=x_{0}$, has an unique solution. The existence is granted by Peano's Theorem, but I'm having problems proving that there is uniqueness.",,['ordinary-differential-equations']
14,Coupled differential equations,Coupled differential equations,,"The question I have trouble with is from Intro to electrodynamics by Griffiths I do not understand how the author obtained the general solution \begin{gather} y(t) = C_1 \cos (\omega t) + C_2 \sin (\omega t) + (E/B)t + C_3, \\ z(t) = C_2 \cos (\omega t) - C_1 \sin (\omega t) + C_4, \end{gather} and then found that $$y(t) = \frac{E}{\omega B}(\omega t - \sin(\omega t)),$$ and the corresponding equation for $z(t)$, since plugging in $y(0) = z(0) = 0$ yielded $C_1 = C_2 = C_3 = C_4 = 0$ for me. Thanks in advance.","The question I have trouble with is from Intro to electrodynamics by Griffiths I do not understand how the author obtained the general solution \begin{gather} y(t) = C_1 \cos (\omega t) + C_2 \sin (\omega t) + (E/B)t + C_3, \\ z(t) = C_2 \cos (\omega t) - C_1 \sin (\omega t) + C_4, \end{gather} and then found that $$y(t) = \frac{E}{\omega B}(\omega t - \sin(\omega t)),$$ and the corresponding equation for $z(t)$, since plugging in $y(0) = z(0) = 0$ yielded $C_1 = C_2 = C_3 = C_4 = 0$ for me. Thanks in advance.",,['ordinary-differential-equations']
15,"When does the IVP $\dot{y}=y^2,~y(0)=1$ have a unique solution?",When does the IVP  have a unique solution?,"\dot{y}=y^2,~y(0)=1","When does the IVP $$\begin{cases}\dot{y}=y^2 \\ y(0)=1,\end{cases}$$ with $(x,y)\in \Bbb R\times\Bbb R$ have a unique solution? For $$\begin{cases}\dot{y}=f(x,y) \\ y(0)=y_0 \end{cases}$$ when do we have a unique solution, no solution, or infinitely many solutions? i want to ask the first IVP have a unique solution in which of the following intervals.. a)($-\infty$,$\infty$) b)($-\infty$,1) c)(-2,2) d)(-1,$\infty$).and how the answer is given option b . please tell me how it comes","When does the IVP $$\begin{cases}\dot{y}=y^2 \\ y(0)=1,\end{cases}$$ with $(x,y)\in \Bbb R\times\Bbb R$ have a unique solution? For $$\begin{cases}\dot{y}=f(x,y) \\ y(0)=y_0 \end{cases}$$ when do we have a unique solution, no solution, or infinitely many solutions? i want to ask the first IVP have a unique solution in which of the following intervals.. a)($-\infty$,$\infty$) b)($-\infty$,1) c)(-2,2) d)(-1,$\infty$).and how the answer is given option b . please tell me how it comes",,['ordinary-differential-equations']
16,"Examine phase portrait $(f,f')$ for $f""+vf'+\alpha^2f(1-f)=0$ and determine asymptotic behavious of solutions as $x\to \pm \infty$",Examine phase portrait  for  and determine asymptotic behavious of solutions as,"(f,f') f""+vf'+\alpha^2f(1-f)=0 x\to \pm \infty","Find the travelling wave solutions, in the form $u(x,t)=f(x-vt)$, of Fisher's equation $$\frac{\partial u}{\partial t}=\frac{\partial^2u}{\partial x^2}+a^2u(1-u)$$ where $a>0$ is a constant. Investigate the nature of these solutions by examining the phase-plane $(f,f')$ and show, in particular, that there exist solutions such that $f\to0$ as $x\to-\infty$, $f\to1$ as $x\to\infty$, for all $v\le-2a$. This equation originally arose in a theory of genuine selection in a species (in a paper by Fisher, 1937). It also arise in the theory of combustion, and chemical kinetics. I have the Fisher equation: $$u_t=u_{xx}+\alpha^2u(1-u)$$ I use the ansatz $u=f(x-vt)$ so that the DE turns into $$f''+vf'+\alpha^2f(1-f)=0$$ Then I define $x_1=f$ and $x_2=f'$ so that: $$   \left( \begin{array}{cc} x_1 \\ x_2 \end{array} \right)'= \left( \begin{array}{cc} 0 & 1  \\ -\alpha^2(1-x_1) & -v  \end{array} \right)     \left( \begin{array}{cc} x_1 \\ x_2 \end{array} \right)$$ Now I need to examine the nature of the solutions $f$ by examining the phase portrait for $(x_1,x_2)$ and in particular show that there exists solutions such that $f\to0$ as $x\to -\infty$ and $f\to 1$ as $f\to \infty$ for all $v\leq 2\alpha$. I am not sure how to examine the phase portrait here because I have no experience drawing them for non-linear two-dimensional system of differential equations. Also, I am not sure how to show the desired solution exist satisfying $f\to0$ as $x\to -\infty$ and $f\to 1$ as $f\to \infty$ for all $v\leq 2\alpha$. If anyone could help me in solving or give me some hints I would be very grateful. Also if anyone knows of a free software package that can draw phase portraits like this for me I would be very happy. Thanks in advance! UPDATE: NOTE: I decided to not change to $x_1$ and $x_2$ but just keep $f$ and $f'$ instead. Now $f'' = -\alpha^2f(1-f)  -vf'$, then we can write: $$   \left( \begin{array}{cc} f \\ f' \end{array} \right)'= \left( \begin{array}{cc} 0 & 1  \\ -\alpha^2(1-f) & -v  \end{array} \right)     \left( \begin{array}{cc} f \\ f' \end{array} \right)$$\ We can find the stationary points by setting  $$ \left( \begin{array}{cc} f \\ f' \end{array} \right)' = \left( \begin{array}{cc} 0 & 1  \\ -\alpha^2(1-f) & -v  \end{array} \right)     \left( \begin{array}{cc} f \\ f' \end{array} \right) =  \left( \begin{array}{cc} 0 \\ 0 \end{array} \right)$$ This immediately gives us $f'=0$ and $\alpha^2(1-f)f=0$, so $f$ is either $0$ or $1$. This gives the stationary points for $(f,f')$ as $(1,0)$ and $(0,0)$. Now we can compute the Jacobian in these points, first we write: $$ \left( \begin{array}{cc} f \\ f' \end{array} \right)' = F(f,f') =  \left( \begin{array}{cc} f' \\ -\alpha^2(1-f)f -vf' \end{array} \right)$$ So the Jacobian $J(F)$ is: $$J(F)=\left( \begin{array}{cc} 0 & 1  \\ -\alpha^2(1-2f) & -v  \end{array} \right)  $$ So that: $$J(F(0,0))=F_{0,0}=\left( \begin{array}{cc} 0 & 1  \\ -\alpha^2 & -v  \end{array} \right)\ \ \ \ \text{and}\ \ \ \ \  J(F(1,0))=F_{1,0}=\left( \begin{array}{cc} 0 & 1  \\ \alpha^2 & -v  \end{array} \right)  $$ Now we can calculate their eigenvalues using the characteristic polynomials and setting it to 0: $$\det\left(\lambda I -F_{0,0}\right)      =   \left| \begin{array}{cc} \lambda & -1  \\ \alpha^2 & \lambda+v  \end{array} \right| = \lambda^2 +v\lambda + \alpha^2 = 0 $$ This gives us the eigenvalues of $F$ in the stationary point $(0,0)$ as: $$\lambda_{1,2}(F_{0,0}) = \frac{-v \pm \sqrt{  v^2 - 4\alpha^2 }}{2}$$ Similarly  $$\det\left(\lambda I -F_{1,0}\right)      =   \left| \begin{array}{cc} \lambda & -1  \\ -\alpha^2 & \lambda+v  \end{array} \right| = \lambda^2 +v\lambda - \alpha^2 = 0 $$ So $$\lambda_{1,2}(F_{1,0}) = \frac{-v \pm \sqrt{  v^2 + 4\alpha^2 }}{2}$$ I have a feeling this is incorrect though, because using $v \leq -2\alpha <  0$ both eigenvalues have positve real part which makes them unstable... Also, the fact that im looking for $v \leq -2\alpha (<0) $ leads me to believe that I need to have we need a positive discriminant. This would of course also be the case if $v>2\alpha$ but then $v$ is positive which I guess we dont want. Thanks for any help. I am sorry if this is unclear, plz ask any questions if you would like to know more!! Thanks 2ND UPDATE: Starting here: $$ \left( \begin{array}{cc} f \\ f' \end{array} \right)' = F(f,f')=\left( \begin{array}{cc} f_1(x,t) \\ f_2(x,t) \end{array} \right) =  \left( \begin{array}{cc} f'(x,t) \\ -\alpha^2[f(x,t)-f(x,t)^2] -vf'(x,t) \end{array} \right)$$ Remembering that $f=f(x-vt)$, then using the chain rule in a fairly straight forward manner we find that the Jacobian is given by: $$\left(     \begin{array}{cc} \frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial t}\\  \frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial t} \end{array}			\right)  =     \left(    \begin{array}{cc}  f'' & -vf'' \\ -\alpha^2(f'-2f) -vf'' & v\alpha^2(f'-2f) +  v^2f'' \end{array}				\right)$$ We have already found that $f'' = -vf' - \alpha^2(f-f^2)$. At the stationary points $(f,f') =(1,0)$ or $(0,0)$ we find that $f''= 0$. Thus at $(1,0)$ the Jacobian is: $$ F_{(1,0)}= \left(    \begin{array}{cc}  0 & 0 \\ 2\alpha^2  & -2v\alpha^2  \end{array}				\right)$$ And at $(f,f')=(0,0)$ the Jacobian is: $$F_{(0,0)} =  \left(    \begin{array}{cc}  0 & 0 \\ 0  & 0  \end{array}				\right)$$ So we find the eigenvalues: $$\det(\lambda I - F_{(1,0)})   =   \left|    \begin{array}{cc}  \lambda & 0 \\ -2\alpha^2  & \lambda -2v\alpha^2  \end{array}				\right| = \lambda^2 - \lambda2 v\alpha^2 $$ So we have a repeated eigenvalue $\lambda_{(1,0)}=2v\alpha^2$. For the stationary point $(0,0)$ the eigenvalues of the Jacobian is $\lambda_{(0,0)}=0$ repeated. Is this right? I am not sure how to continue form here... I worry that because I am using the notation $f'$ and then differentiating w.r.t. both $x$ and $t$ later on that I might be making mistakes. What I am using is that $f=f(\eta)$ where $\eta =x-vt$ and thus we can just write $f'$ and still differentiate w.r.t. $x$ and $t$ by using the chain rule.","Find the travelling wave solutions, in the form $u(x,t)=f(x-vt)$, of Fisher's equation $$\frac{\partial u}{\partial t}=\frac{\partial^2u}{\partial x^2}+a^2u(1-u)$$ where $a>0$ is a constant. Investigate the nature of these solutions by examining the phase-plane $(f,f')$ and show, in particular, that there exist solutions such that $f\to0$ as $x\to-\infty$, $f\to1$ as $x\to\infty$, for all $v\le-2a$. This equation originally arose in a theory of genuine selection in a species (in a paper by Fisher, 1937). It also arise in the theory of combustion, and chemical kinetics. I have the Fisher equation: $$u_t=u_{xx}+\alpha^2u(1-u)$$ I use the ansatz $u=f(x-vt)$ so that the DE turns into $$f''+vf'+\alpha^2f(1-f)=0$$ Then I define $x_1=f$ and $x_2=f'$ so that: $$   \left( \begin{array}{cc} x_1 \\ x_2 \end{array} \right)'= \left( \begin{array}{cc} 0 & 1  \\ -\alpha^2(1-x_1) & -v  \end{array} \right)     \left( \begin{array}{cc} x_1 \\ x_2 \end{array} \right)$$ Now I need to examine the nature of the solutions $f$ by examining the phase portrait for $(x_1,x_2)$ and in particular show that there exists solutions such that $f\to0$ as $x\to -\infty$ and $f\to 1$ as $f\to \infty$ for all $v\leq 2\alpha$. I am not sure how to examine the phase portrait here because I have no experience drawing them for non-linear two-dimensional system of differential equations. Also, I am not sure how to show the desired solution exist satisfying $f\to0$ as $x\to -\infty$ and $f\to 1$ as $f\to \infty$ for all $v\leq 2\alpha$. If anyone could help me in solving or give me some hints I would be very grateful. Also if anyone knows of a free software package that can draw phase portraits like this for me I would be very happy. Thanks in advance! UPDATE: NOTE: I decided to not change to $x_1$ and $x_2$ but just keep $f$ and $f'$ instead. Now $f'' = -\alpha^2f(1-f)  -vf'$, then we can write: $$   \left( \begin{array}{cc} f \\ f' \end{array} \right)'= \left( \begin{array}{cc} 0 & 1  \\ -\alpha^2(1-f) & -v  \end{array} \right)     \left( \begin{array}{cc} f \\ f' \end{array} \right)$$\ We can find the stationary points by setting  $$ \left( \begin{array}{cc} f \\ f' \end{array} \right)' = \left( \begin{array}{cc} 0 & 1  \\ -\alpha^2(1-f) & -v  \end{array} \right)     \left( \begin{array}{cc} f \\ f' \end{array} \right) =  \left( \begin{array}{cc} 0 \\ 0 \end{array} \right)$$ This immediately gives us $f'=0$ and $\alpha^2(1-f)f=0$, so $f$ is either $0$ or $1$. This gives the stationary points for $(f,f')$ as $(1,0)$ and $(0,0)$. Now we can compute the Jacobian in these points, first we write: $$ \left( \begin{array}{cc} f \\ f' \end{array} \right)' = F(f,f') =  \left( \begin{array}{cc} f' \\ -\alpha^2(1-f)f -vf' \end{array} \right)$$ So the Jacobian $J(F)$ is: $$J(F)=\left( \begin{array}{cc} 0 & 1  \\ -\alpha^2(1-2f) & -v  \end{array} \right)  $$ So that: $$J(F(0,0))=F_{0,0}=\left( \begin{array}{cc} 0 & 1  \\ -\alpha^2 & -v  \end{array} \right)\ \ \ \ \text{and}\ \ \ \ \  J(F(1,0))=F_{1,0}=\left( \begin{array}{cc} 0 & 1  \\ \alpha^2 & -v  \end{array} \right)  $$ Now we can calculate their eigenvalues using the characteristic polynomials and setting it to 0: $$\det\left(\lambda I -F_{0,0}\right)      =   \left| \begin{array}{cc} \lambda & -1  \\ \alpha^2 & \lambda+v  \end{array} \right| = \lambda^2 +v\lambda + \alpha^2 = 0 $$ This gives us the eigenvalues of $F$ in the stationary point $(0,0)$ as: $$\lambda_{1,2}(F_{0,0}) = \frac{-v \pm \sqrt{  v^2 - 4\alpha^2 }}{2}$$ Similarly  $$\det\left(\lambda I -F_{1,0}\right)      =   \left| \begin{array}{cc} \lambda & -1  \\ -\alpha^2 & \lambda+v  \end{array} \right| = \lambda^2 +v\lambda - \alpha^2 = 0 $$ So $$\lambda_{1,2}(F_{1,0}) = \frac{-v \pm \sqrt{  v^2 + 4\alpha^2 }}{2}$$ I have a feeling this is incorrect though, because using $v \leq -2\alpha <  0$ both eigenvalues have positve real part which makes them unstable... Also, the fact that im looking for $v \leq -2\alpha (<0) $ leads me to believe that I need to have we need a positive discriminant. This would of course also be the case if $v>2\alpha$ but then $v$ is positive which I guess we dont want. Thanks for any help. I am sorry if this is unclear, plz ask any questions if you would like to know more!! Thanks 2ND UPDATE: Starting here: $$ \left( \begin{array}{cc} f \\ f' \end{array} \right)' = F(f,f')=\left( \begin{array}{cc} f_1(x,t) \\ f_2(x,t) \end{array} \right) =  \left( \begin{array}{cc} f'(x,t) \\ -\alpha^2[f(x,t)-f(x,t)^2] -vf'(x,t) \end{array} \right)$$ Remembering that $f=f(x-vt)$, then using the chain rule in a fairly straight forward manner we find that the Jacobian is given by: $$\left(     \begin{array}{cc} \frac{\partial f_1}{\partial x} & \frac{\partial f_1}{\partial t}\\  \frac{\partial f_2}{\partial x} & \frac{\partial f_2}{\partial t} \end{array}			\right)  =     \left(    \begin{array}{cc}  f'' & -vf'' \\ -\alpha^2(f'-2f) -vf'' & v\alpha^2(f'-2f) +  v^2f'' \end{array}				\right)$$ We have already found that $f'' = -vf' - \alpha^2(f-f^2)$. At the stationary points $(f,f') =(1,0)$ or $(0,0)$ we find that $f''= 0$. Thus at $(1,0)$ the Jacobian is: $$ F_{(1,0)}= \left(    \begin{array}{cc}  0 & 0 \\ 2\alpha^2  & -2v\alpha^2  \end{array}				\right)$$ And at $(f,f')=(0,0)$ the Jacobian is: $$F_{(0,0)} =  \left(    \begin{array}{cc}  0 & 0 \\ 0  & 0  \end{array}				\right)$$ So we find the eigenvalues: $$\det(\lambda I - F_{(1,0)})   =   \left|    \begin{array}{cc}  \lambda & 0 \\ -2\alpha^2  & \lambda -2v\alpha^2  \end{array}				\right| = \lambda^2 - \lambda2 v\alpha^2 $$ So we have a repeated eigenvalue $\lambda_{(1,0)}=2v\alpha^2$. For the stationary point $(0,0)$ the eigenvalues of the Jacobian is $\lambda_{(0,0)}=0$ repeated. Is this right? I am not sure how to continue form here... I worry that because I am using the notation $f'$ and then differentiating w.r.t. both $x$ and $t$ later on that I might be making mistakes. What I am using is that $f=f(\eta)$ where $\eta =x-vt$ and thus we can just write $f'$ and still differentiate w.r.t. $x$ and $t$ by using the chain rule.",,"['ordinary-differential-equations', 'partial-differential-equations']"
17,Arnol'd's trivium problem 21,Arnol'd's trivium problem 21,,"Find the derivative of the solution of the equation $\ddot{x} = \dot{x}^2 + x^3$ with initial condition $x(0)=0$, $\dot{x}(0) = A$ with respect to A for A = 0. I have no idea about how to solve this problem, please help me.","Find the derivative of the solution of the equation $\ddot{x} = \dot{x}^2 + x^3$ with initial condition $x(0)=0$, $\dot{x}(0) = A$ with respect to A for A = 0. I have no idea about how to solve this problem, please help me.",,['ordinary-differential-equations']
18,Solve $x' = 2-4t-x+tx^2$,Solve,x' = 2-4t-x+tx^2,Let $x(t)\equiv 2$ be one of solution following differential equation:  $$x' = 2-4t-x+tx^2$$ Using $y(t) = \frac{1}{x(t)}$ find general solution. I think that $x'(t) = y'(t) \ln|y(t)| $ so we have: $$ y'(t) \ln|y(t)| = 2-4t - \frac{1}{y} + t \frac{1}{y^2}$$. I'm wrong or I don't know how can I solve it.,Let $x(t)\equiv 2$ be one of solution following differential equation:  $$x' = 2-4t-x+tx^2$$ Using $y(t) = \frac{1}{x(t)}$ find general solution. I think that $x'(t) = y'(t) \ln|y(t)| $ so we have: $$ y'(t) \ln|y(t)| = 2-4t - \frac{1}{y} + t \frac{1}{y^2}$$. I'm wrong or I don't know how can I solve it.,,['ordinary-differential-equations']
19,General solution to a Growth equation,General solution to a Growth equation,,"I'd like to compute a formula that describes a population growth. The population starts with $N(t=0)$ individuals. At each time step there are births and deaths. The number of births at time $t$ is given by $N(t-1)f$, where $f$ being the fecundity. The number of deaths is given by $N(t-ls)$, where $ls$ being the lifespan. Therefore the recursive equation is: $$ N(t)=N(t-1)+N(t-1) f-N(t-ls) $$  which also equals: $$ N(t)=N(t-1) (f+1)-N(t-ls) $$ What is the general solution? I mean an equation that gives the value $N(t)$ in function  of $ls$, $f$ and $N(0)$ only. Step-by-step solution is more than welcome. UPDATE: $ls$ and $t$ are integers.","I'd like to compute a formula that describes a population growth. The population starts with $N(t=0)$ individuals. At each time step there are births and deaths. The number of births at time $t$ is given by $N(t-1)f$, where $f$ being the fecundity. The number of deaths is given by $N(t-ls)$, where $ls$ being the lifespan. Therefore the recursive equation is: $$ N(t)=N(t-1)+N(t-1) f-N(t-ls) $$  which also equals: $$ N(t)=N(t-1) (f+1)-N(t-ls) $$ What is the general solution? I mean an equation that gives the value $N(t)$ in function  of $ls$, $f$ and $N(0)$ only. Step-by-step solution is more than welcome. UPDATE: $ls$ and $t$ are integers.",,"['ordinary-differential-equations', 'recurrence-relations']"
20,EDIT: Estimation of solution of variational problem,EDIT: Estimation of solution of variational problem,,"We consider the problem $$-u''+a(x)u=f, x\in (0,1)=\Omega , u(0)=\alpha,u(1)=\beta$$ with $f \in L^2(\Omega) , a(x) \geq a_0 > 0, a \in L^{\infty}(\Omega)$ Question 1: prove that the variational formulation associate to this problem admits a unique solution in an adequate Hilbert space $V$ (we can use for this proof the function $u_1(x)=\alpha +(\beta - \alpha) x$). Question 2: prove that $$||u||_V \leq C ||f||_{L^2}$$ For the question 1, I consider $u=w + u_1$ with $w \in V = H^1(\Omega)$ and solution for the equation of the problem, and I prove that $w$ is unique in $H^1_0(\Omega)$ by Lax-Milgram, so the problem admits a unique solution $u$ in $H^1_0.$ My problem is in question 2, I don't know how we prove it. Thank's for the help.","We consider the problem $$-u''+a(x)u=f, x\in (0,1)=\Omega , u(0)=\alpha,u(1)=\beta$$ with $f \in L^2(\Omega) , a(x) \geq a_0 > 0, a \in L^{\infty}(\Omega)$ Question 1: prove that the variational formulation associate to this problem admits a unique solution in an adequate Hilbert space $V$ (we can use for this proof the function $u_1(x)=\alpha +(\beta - \alpha) x$). Question 2: prove that $$||u||_V \leq C ||f||_{L^2}$$ For the question 1, I consider $u=w + u_1$ with $w \in V = H^1(\Omega)$ and solution for the equation of the problem, and I prove that $w$ is unique in $H^1_0(\Omega)$ by Lax-Milgram, so the problem admits a unique solution $u$ in $H^1_0.$ My problem is in question 2, I don't know how we prove it. Thank's for the help.",,"['functional-analysis', 'ordinary-differential-equations', 'sobolev-spaces']"
21,When can an unknown integral be written as an ODE,When can an unknown integral be written as an ODE,,"I am hoping to numerically solve for the unknown function $I(t)$ \begin{align} I(t) = \int_0^tf(x,t)\,dx \end{align} by converting it into one or more ODEs.  The function $f(x,t)$ is known.  If $f$ were only a function of one variable, then we trivially have \begin{align} \frac{dI}{dt} = f(t). \end{align} If $f(x,t) = g(x)h(t)$ is separable, then we can also succeed, but in general Leibniz's integral rule leads to complications. Need $f$ be separable to convert its integral into an ODE?","I am hoping to numerically solve for the unknown function $I(t)$ \begin{align} I(t) = \int_0^tf(x,t)\,dx \end{align} by converting it into one or more ODEs.  The function $f(x,t)$ is known.  If $f$ were only a function of one variable, then we trivially have \begin{align} \frac{dI}{dt} = f(t). \end{align} If $f(x,t) = g(x)h(t)$ is separable, then we can also succeed, but in general Leibniz's integral rule leads to complications. Need $f$ be separable to convert its integral into an ODE?",,"['integration', 'ordinary-differential-equations', 'multivariable-calculus']"
22,Strategies for solving second order DEs with variable coefficients,Strategies for solving second order DEs with variable coefficients,,"Does anyone have any good videos/ article links for techniques to solve second order DEs with variable coefficients? I mean, just as an example, a DE like: $$ x \frac{d^{2}y}{dx^{2}} - (2x-1) \frac{dy}{dx} + (x-1)y = 0$$ (this one is fairly simple to solve but the one I have in mind is tricker (for me anyway)). If anyone's curious, the DE I have is: $$\frac{d^2 y}{dx^2} + \frac{2(x^2 - 1)}{x^3} \frac{dy}{dx} + \frac{x^2 + 1}{x^6} y = 0$$ Are there any techniques to systematically find a general solution for DEs like this? Thanks very much","Does anyone have any good videos/ article links for techniques to solve second order DEs with variable coefficients? I mean, just as an example, a DE like: $$ x \frac{d^{2}y}{dx^{2}} - (2x-1) \frac{dy}{dx} + (x-1)y = 0$$ (this one is fairly simple to solve but the one I have in mind is tricker (for me anyway)). If anyone's curious, the DE I have is: $$\frac{d^2 y}{dx^2} + \frac{2(x^2 - 1)}{x^3} \frac{dy}{dx} + \frac{x^2 + 1}{x^6} y = 0$$ Are there any techniques to systematically find a general solution for DEs like this? Thanks very much",,['ordinary-differential-equations']
23,The 10.4 Problem on Benassy's *Macroeconomic Theory*,The 10.4 Problem on Benassy's *Macroeconomic Theory*,,"This a repost. In my first try at the problem here I was not very clear with my question and I was also troubled by my lack of knowledge of MathJax. And I probably lost the attention of you guys while trying to make things clearer. So here comes a second try: I'll post the problem: 10.4 A RBC model with generations We will study and solve analytically a simple model with generations where, as in reality, investment and consumption can react quite differently to productivity shocks. The model is adapted from Huffman (1995), who studies the more general case where agents have a positive probability of death. We assume that the population grows at the rate n, and the supply of labor also grows at the rate n. Each generation of households lives forever, works only in the rst period of its life, and consumes in all periods by saving under the form of capital. The household of generation has a utility function: $$\sum_{t=\tau}^\infty\beta^t \log C_t $$ which it maximizes subject to the budget constraints: $$ C_t+K_{t+1}=L_t w_t, \qquad\text{for } t=\tau, $$ $$ C_t+K_{t+1}=R_t K_t, \qquad\text{for } t>\tau. $$ Finally the production function is Cobb-Douglas: $$ Y_t = A_t K_t^\alpha L_t^{1-\alpha}. $$ 10.4.1 Questions 1. Compute the values of consumption and investment as functions of capital and output. 2. Show how consumption and investment respond to technology shocks. Now maximizing and manipulating the first order conditions you will get these answers for question 1 : $$C_{t}=(1-\beta)[Y_{t}+(1-\delta)K_{t}] \tag{1}$$ $$K_{t+1}=\beta[Y_{t}+(1-\delta)K_{t}]\tag{2}$$ $$I_{t}=\beta Y_{t}-(1-\beta +\delta\beta)K_{t} \tag{3}$$ Now here comes my puzzle with question 2 : I'm supposed to substitute the production function in every equation to show the interaction with the technology shocks  -- (1), (2) and (3). For instance, Equation (2) would become: $$K_{t+1}=\beta[A_tK^\alpha_{t}L^{1-\alpha}_t+(1-\delta)K_{t}]$$ Then I should try to solve the dynamics of capital in equation (2) and substitute the result in equations (1) and (3). But how can I do that? Log linearization does not seem give me a solution and it seems strange to assume $K_{t+1}$=$K_t$ (actually, can I do that?). I'll be glad if someone could point me the way. Observation : There is a solutions book of Benassy's Macroeconomic Theory available on the internet. You can google something like ""Benassy Solutions"" and you will find it. The solution for this problem is available at pages 83-84, but the book does not give the real answer for the second question of the problem. It says it would be ""tedious"" to derive the dynamics of capital and substitute in the equations of consumption and investment, so it just points the way (not with enough leads for me, at least) for the answer, as I described above.","This a repost. In my first try at the problem here I was not very clear with my question and I was also troubled by my lack of knowledge of MathJax. And I probably lost the attention of you guys while trying to make things clearer. So here comes a second try: I'll post the problem: 10.4 A RBC model with generations We will study and solve analytically a simple model with generations where, as in reality, investment and consumption can react quite differently to productivity shocks. The model is adapted from Huffman (1995), who studies the more general case where agents have a positive probability of death. We assume that the population grows at the rate n, and the supply of labor also grows at the rate n. Each generation of households lives forever, works only in the rst period of its life, and consumes in all periods by saving under the form of capital. The household of generation has a utility function: $$\sum_{t=\tau}^\infty\beta^t \log C_t $$ which it maximizes subject to the budget constraints: $$ C_t+K_{t+1}=L_t w_t, \qquad\text{for } t=\tau, $$ $$ C_t+K_{t+1}=R_t K_t, \qquad\text{for } t>\tau. $$ Finally the production function is Cobb-Douglas: $$ Y_t = A_t K_t^\alpha L_t^{1-\alpha}. $$ 10.4.1 Questions 1. Compute the values of consumption and investment as functions of capital and output. 2. Show how consumption and investment respond to technology shocks. Now maximizing and manipulating the first order conditions you will get these answers for question 1 : $$C_{t}=(1-\beta)[Y_{t}+(1-\delta)K_{t}] \tag{1}$$ $$K_{t+1}=\beta[Y_{t}+(1-\delta)K_{t}]\tag{2}$$ $$I_{t}=\beta Y_{t}-(1-\beta +\delta\beta)K_{t} \tag{3}$$ Now here comes my puzzle with question 2 : I'm supposed to substitute the production function in every equation to show the interaction with the technology shocks  -- (1), (2) and (3). For instance, Equation (2) would become: $$K_{t+1}=\beta[A_tK^\alpha_{t}L^{1-\alpha}_t+(1-\delta)K_{t}]$$ Then I should try to solve the dynamics of capital in equation (2) and substitute the result in equations (1) and (3). But how can I do that? Log linearization does not seem give me a solution and it seems strange to assume $K_{t+1}$=$K_t$ (actually, can I do that?). I'll be glad if someone could point me the way. Observation : There is a solutions book of Benassy's Macroeconomic Theory available on the internet. You can google something like ""Benassy Solutions"" and you will find it. The solution for this problem is available at pages 83-84, but the book does not give the real answer for the second question of the problem. It says it would be ""tedious"" to derive the dynamics of capital and substitute in the equations of consumption and investment, so it just points the way (not with enough leads for me, at least) for the answer, as I described above.",,"['ordinary-differential-equations', 'summation', 'economics']"
24,"How can i show, this is a fundamental system for a given differential equation?","How can i show, this is a fundamental system for a given differential equation?",,"My Problem is: i have a given differential equation: $$y^{\prime\prime\prime}-\frac{x^2}{x^2-2x+2}\cdot y^{\prime\prime}+\frac{2x}{x^2-2x+2}\cdot y^{\prime}-\frac{2}{x^2-2x+2}\cdot y= 0$$and the given functions : $$y_{1}=x \quad y_{2}=x^2 \quad y_{3}=e^{x}$$ A fundamental system is defined as a: $\{y_1,\ldots,y_n\} \quad $ with: $\quad \mathcal{L} := \{y \in C^1([a,b]; \mathbb{R}^n)\ |\ y = \sum_{k=1}^na_ky_k\ ,\ a_1, \ldots, a_n \in \mathbb{R}\}$ How can i show, that the given functions are a fundamental system for the given differential equation? My Approach was: i managed to show, that the given functions are solutions for the differential equation. for $y_{1} = x$ $$y^{\prime}=1 \quad y^{\prime\prime}=0 \quad y^{\prime\prime\prime}=0$$ that's why: $$0 - 0 + \frac{2x}{x^2-2x+2}\cdot 1 -\frac{2}{x^2-2x+2}\cdot x =0$$ $$\frac{2x}{x^2-2x+2} -\frac{2x}{x^2-2x+2}=0$$  $$0=0$$ for $y_{2} = x^2$ $$y^{\prime}=2x \quad y^{\prime\prime}=2 \quad y^{\prime\prime\prime}=0$$ that's why: $$0 - \frac{x^2}{x^2-2x+2}\cdot 2 +\frac{2x}{x^2-2x+2}\cdot 2x -\frac{2}{x^2-2x+2}\cdot x^2 =0$$ $$0 - \frac{2x^2}{x^2-2x+2} +\frac{4x^2}{x^2-2x+2} -\frac{2x^2}{x^2-2x+2} =0$$ $$0=0$$ for $y_{3} = e^x$ $$y^{\prime}= e^x \quad y^{\prime\prime}= e^x \quad y^{\prime\prime\prime}= e^x$$ that's why: $$ e^x - \frac{x^2 e^x}{x^2-2x+2} +\frac{2x e^x}{x^2-2x+2} -\frac{2 e^x}{x^2-2x+2} =0$$ $$e^x +\frac{ -e^x(x^2-2x+2)}{x^2-2x+2}=e^x-e^x=0$$ But now i am stuck, i don't think, that this is the proof i am looking for. i dont know how to show it... maybe you can help? P.S. : edits were made to improve language and latex","My Problem is: i have a given differential equation: $$y^{\prime\prime\prime}-\frac{x^2}{x^2-2x+2}\cdot y^{\prime\prime}+\frac{2x}{x^2-2x+2}\cdot y^{\prime}-\frac{2}{x^2-2x+2}\cdot y= 0$$and the given functions : $$y_{1}=x \quad y_{2}=x^2 \quad y_{3}=e^{x}$$ A fundamental system is defined as a: $\{y_1,\ldots,y_n\} \quad $ with: $\quad \mathcal{L} := \{y \in C^1([a,b]; \mathbb{R}^n)\ |\ y = \sum_{k=1}^na_ky_k\ ,\ a_1, \ldots, a_n \in \mathbb{R}\}$ How can i show, that the given functions are a fundamental system for the given differential equation? My Approach was: i managed to show, that the given functions are solutions for the differential equation. for $y_{1} = x$ $$y^{\prime}=1 \quad y^{\prime\prime}=0 \quad y^{\prime\prime\prime}=0$$ that's why: $$0 - 0 + \frac{2x}{x^2-2x+2}\cdot 1 -\frac{2}{x^2-2x+2}\cdot x =0$$ $$\frac{2x}{x^2-2x+2} -\frac{2x}{x^2-2x+2}=0$$  $$0=0$$ for $y_{2} = x^2$ $$y^{\prime}=2x \quad y^{\prime\prime}=2 \quad y^{\prime\prime\prime}=0$$ that's why: $$0 - \frac{x^2}{x^2-2x+2}\cdot 2 +\frac{2x}{x^2-2x+2}\cdot 2x -\frac{2}{x^2-2x+2}\cdot x^2 =0$$ $$0 - \frac{2x^2}{x^2-2x+2} +\frac{4x^2}{x^2-2x+2} -\frac{2x^2}{x^2-2x+2} =0$$ $$0=0$$ for $y_{3} = e^x$ $$y^{\prime}= e^x \quad y^{\prime\prime}= e^x \quad y^{\prime\prime\prime}= e^x$$ that's why: $$ e^x - \frac{x^2 e^x}{x^2-2x+2} +\frac{2x e^x}{x^2-2x+2} -\frac{2 e^x}{x^2-2x+2} =0$$ $$e^x +\frac{ -e^x(x^2-2x+2)}{x^2-2x+2}=e^x-e^x=0$$ But now i am stuck, i don't think, that this is the proof i am looking for. i dont know how to show it... maybe you can help? P.S. : edits were made to improve language and latex",,"['ordinary-differential-equations', 'integration']"
25,Path of particle under gravity,Path of particle under gravity,,If a particle is subjected to gravity then         $$\frac{∂^2 u}{∂\theta^2} +u = \frac{GM}{h^2} $$ where         $$ u = \frac{1}{r}$$ and         $$h = r^2\dot{\theta}.$$ If you solve this you get          $$u = A\sin\theta+B\cos\theta + \frac{GM}{h^2}.$$ But the general solution for this is just in terms of $\cos\theta$ because then you have the eqn for a conic. So why is $A = 0$?,If a particle is subjected to gravity then         $$\frac{∂^2 u}{∂\theta^2} +u = \frac{GM}{h^2} $$ where         $$ u = \frac{1}{r}$$ and         $$h = r^2\dot{\theta}.$$ If you solve this you get          $$u = A\sin\theta+B\cos\theta + \frac{GM}{h^2}.$$ But the general solution for this is just in terms of $\cos\theta$ because then you have the eqn for a conic. So why is $A = 0$?,,"['ordinary-differential-equations', 'dynamical-systems']"
26,"Find $\frac{d x}{d y_0}$ of $x'=xy+t^2,y'=\frac{-y^2}{2}$ for $x_0=3$ and $y_0=2$",Find  of  for  and,"\frac{d x}{d y_0} x'=xy+t^2,y'=\frac{-y^2}{2} x_0=3 y_0=2","We've got: $x' = xy +t^2$, $y' = \frac{-y^2}{2}$, $x(1)=x_0$,$y(1)=y_0$. My task is to find $\frac{d x}{d y_0}$ for $x_0 = 3$ and $y_0 = 2$. I computed $y$ for $y_0=2$ and so we have $y(t) = \frac{2}{t}$, hence $x' = x \cdot \frac{t}{2} + t^2$. But how to find $\frac{d x}{d y_0}$? I know how to find $\frac{d x}{d x_0}$, having a certain theorem, but it's not the question... Can anybody help?","We've got: $x' = xy +t^2$, $y' = \frac{-y^2}{2}$, $x(1)=x_0$,$y(1)=y_0$. My task is to find $\frac{d x}{d y_0}$ for $x_0 = 3$ and $y_0 = 2$. I computed $y$ for $y_0=2$ and so we have $y(t) = \frac{2}{t}$, hence $x' = x \cdot \frac{t}{2} + t^2$. But how to find $\frac{d x}{d y_0}$? I know how to find $\frac{d x}{d x_0}$, having a certain theorem, but it's not the question... Can anybody help?",,['ordinary-differential-equations']
27,first order ordinary differential equation,first order ordinary differential equation,,"How can I solve this ODE of first order: \begin{align} y^{'}= y^{2}+x, & \text{where }  y^{'}=\frac{dy}{dx} \end{align} Is there any exact mehod to solve it ? Thank you.","How can I solve this ODE of first order: \begin{align} y^{'}= y^{2}+x, & \text{where }  y^{'}=\frac{dy}{dx} \end{align} Is there any exact mehod to solve it ? Thank you.",,['ordinary-differential-equations']
28,"By making the substitution $t=e^x$ and setting $z(t)=y(x)$, rewrite the following differential equation","By making the substitution  and setting , rewrite the following differential equation",t=e^x z(t)=y(x),"By making the substitution $t=e^x$ and setting $z(t)=y(x)$, rewrite the following differential equation $y''-y'+e^{2x}y = xe^{2x}-1$,    (1) as one in terms of z and t. Hence find all solutions of (1). I know I should be using the chain rule but I can't work out how to do it. Could someone please explain how to rewrite the equation?","By making the substitution $t=e^x$ and setting $z(t)=y(x)$, rewrite the following differential equation $y''-y'+e^{2x}y = xe^{2x}-1$,    (1) as one in terms of z and t. Hence find all solutions of (1). I know I should be using the chain rule but I can't work out how to do it. Could someone please explain how to rewrite the equation?",,['ordinary-differential-equations']
29,Differential equations help: $x=y'+\sin y'.$,Differential equations help:,x=y'+\sin y'.,i have the equation $x=y'+\sin y'.$ To solve this I put $y'=p$ and $x=p+\sin p.$ $dx=(1+\cos p)dp\;$ and $\;dy=dx\cdot p=p+p\cos p$ To find $y$ I integrate this and I have $\;p^2/2+p\sin p+\cos p$. But the answer on my book is $x=p\cdot \ln p + \sin p+ p\cdot \cos p.$,i have the equation $x=y'+\sin y'.$ To solve this I put $y'=p$ and $x=p+\sin p.$ $dx=(1+\cos p)dp\;$ and $\;dy=dx\cdot p=p+p\cos p$ To find $y$ I integrate this and I have $\;p^2/2+p\sin p+\cos p$. But the answer on my book is $x=p\cdot \ln p + \sin p+ p\cdot \cos p.$,,['ordinary-differential-equations']
30,How to solve the two dimensional Laplace's equation for certain cases?,How to solve the two dimensional Laplace's equation for certain cases?,,"Had a doubt regarding Laplace's equation. In many textbooks, the general solution to the two dimensional Laplace equation is mentioned as: $$\Phi(\rho,\phi) = A_{0} + B_{0}\ln(\rho) + \sum_{n=1}^{\infty}\rho^n(A_n\cos(n\phi) + B_n \sin(n\phi)) + \sum_{n=1}^{\infty}\rho^{-n}(C_n \cos(n\phi) + D_n \sin(n\phi))$$ in polar coordinates. For convenience, I will name the two summation terms as: $$T_1 = \sum_{n=1}^{\infty}\rho^n(A_n\cos(n\phi) + B_n \sin(n\phi))$$ $$T_2 =  \sum_{n=1}^{\infty}\rho^{-n}(C_n \cos(n\phi) + D_n \sin(n\phi))$$ Not much has really been mentioned on whether these two series, $T_1$ and $T_2$ converge or not. When the solution space does not include either zero or infinity it is generally implicitly assumed that they converge. When the solution space includes the origin, generally the procedure has been to equate the coefficients of the second series, $T_2$, to zero, in other words, to exclude the second series from the solution.  In many physical problems of interest however, the solution space includes both the origin of the coordinate system and is an unbounded region in all directions. My question is, how can we use the above expansions to handle such cases? Especially when there is a boundary condition of zero (or $ k ln(\rho) $) at infinity. The radius of convergence doesn't seem to be known a priori as the coefficients are unknown.  For example, if you consider the case of an infinitely long conducting cable of arbitrary but uniform cross-section, (Arbitrary meaning that it need not be something geometrical or symmetric like a circle or a square, and could even be concave; and uniform meaning that the shape and area of the cross-section does not change in the $z$ direction so it can be reduced to a two dimensional problem.) maintained at a constant potential $V$, and where the potential is taken to approach $ k ln(\rho) $ at infinity. It may often be convenient to choose the origin outside the cable. In this case, if I exclude the term $T_2$ (for finiteness at the origin), again the boundary condition $\Phi\to k \ln(\rho)$ as $\rho\to\infty$ will imply that the coefficients in the term $T_1$ become zero as well. So the solution appears to be trivially $\Phi = A_0$ (obviously wrong!) even before I apply the boundary condition $\Phi = V$ at the surface of the cable.  Can you let me know what I am doing wrong? Can you throw some light on this? Thanks.","Had a doubt regarding Laplace's equation. In many textbooks, the general solution to the two dimensional Laplace equation is mentioned as: $$\Phi(\rho,\phi) = A_{0} + B_{0}\ln(\rho) + \sum_{n=1}^{\infty}\rho^n(A_n\cos(n\phi) + B_n \sin(n\phi)) + \sum_{n=1}^{\infty}\rho^{-n}(C_n \cos(n\phi) + D_n \sin(n\phi))$$ in polar coordinates. For convenience, I will name the two summation terms as: $$T_1 = \sum_{n=1}^{\infty}\rho^n(A_n\cos(n\phi) + B_n \sin(n\phi))$$ $$T_2 =  \sum_{n=1}^{\infty}\rho^{-n}(C_n \cos(n\phi) + D_n \sin(n\phi))$$ Not much has really been mentioned on whether these two series, $T_1$ and $T_2$ converge or not. When the solution space does not include either zero or infinity it is generally implicitly assumed that they converge. When the solution space includes the origin, generally the procedure has been to equate the coefficients of the second series, $T_2$, to zero, in other words, to exclude the second series from the solution.  In many physical problems of interest however, the solution space includes both the origin of the coordinate system and is an unbounded region in all directions. My question is, how can we use the above expansions to handle such cases? Especially when there is a boundary condition of zero (or $ k ln(\rho) $) at infinity. The radius of convergence doesn't seem to be known a priori as the coefficients are unknown.  For example, if you consider the case of an infinitely long conducting cable of arbitrary but uniform cross-section, (Arbitrary meaning that it need not be something geometrical or symmetric like a circle or a square, and could even be concave; and uniform meaning that the shape and area of the cross-section does not change in the $z$ direction so it can be reduced to a two dimensional problem.) maintained at a constant potential $V$, and where the potential is taken to approach $ k ln(\rho) $ at infinity. It may often be convenient to choose the origin outside the cable. In this case, if I exclude the term $T_2$ (for finiteness at the origin), again the boundary condition $\Phi\to k \ln(\rho)$ as $\rho\to\infty$ will imply that the coefficients in the term $T_1$ become zero as well. So the solution appears to be trivially $\Phi = A_0$ (obviously wrong!) even before I apply the boundary condition $\Phi = V$ at the surface of the cable.  Can you let me know what I am doing wrong? Can you throw some light on this? Thanks.",,"['ordinary-differential-equations', 'harmonic-functions']"
31,What is the time-integral of motion for first order differential equations?,What is the time-integral of motion for first order differential equations?,,"For a second order differential equation (many physical systems) in one variable, I know ""procedures"" to compute the energy. Given $$q''(t)=f(q(t),q'(t)),\ \  q(0)=q_0,\ \ q'(0)=v_0,$$  if we're lucky we can read off the related Lagrangian $L$, introduce $p=\frac{\partial L}{\partial q}$, do a Legendre transform and we got the Hamiltonian function $H(q,p)$ for which $\frac{\text d}{\text dt}H(q(t),p(t))=0$ for solutions of the differential equation. We can be more exact and give all the conditions for Noethers theorem to hold and the result is that along the flow $X(t)=\langle q(t),p(t)\rangle$ in phase space given by a solution with initial conditions $X(0)$, the function $H(q,p)$ always takes the same values. It defines surfaces in phase space indexed by initial conditions $X(0)$. I wonder how to view this for a priori first order systems $$q'(t)=f(q(t)),\ \ q(0)=q_0,$$ where I think this must be even simpler.  E.g. I thing some functional  $$q(t)\mapsto q(t)-\text{an integral over}\  f(q(t))\  \text{something},$$ should exists which will be constant for solutions of the equation, i.e. only depend on $q_0$. For each $f$, the functional dependence of this ""energy"" on ""$q_0$"" will be different. However, I can't seem to find a general relation. What's the theory behind this, is there an energy'ish time integral of motion? What is the functional dependence on the intial condition, for a suitable constant of motion for for first order systems. And what would be the interpretation, given that we speak of a situation with only one initial condition? If it is that case that the system is too restricted so that there is no meaningful geometrical interpretation, then let's think about a system of first order differential equations. This is like the one we generated from phase space, except that it doesn't really come from a second order situation and so the intial conditions aren't really e.g. intial position and velocity. I'm pretty sure there are situation where one considers such a directly generated flow (the equation might be more complicated than exponential flow $\dot x\propto x$), but I don't recall any talk about the time-constant of motion in these systems, or how to interpret it.","For a second order differential equation (many physical systems) in one variable, I know ""procedures"" to compute the energy. Given $$q''(t)=f(q(t),q'(t)),\ \  q(0)=q_0,\ \ q'(0)=v_0,$$  if we're lucky we can read off the related Lagrangian $L$, introduce $p=\frac{\partial L}{\partial q}$, do a Legendre transform and we got the Hamiltonian function $H(q,p)$ for which $\frac{\text d}{\text dt}H(q(t),p(t))=0$ for solutions of the differential equation. We can be more exact and give all the conditions for Noethers theorem to hold and the result is that along the flow $X(t)=\langle q(t),p(t)\rangle$ in phase space given by a solution with initial conditions $X(0)$, the function $H(q,p)$ always takes the same values. It defines surfaces in phase space indexed by initial conditions $X(0)$. I wonder how to view this for a priori first order systems $$q'(t)=f(q(t)),\ \ q(0)=q_0,$$ where I think this must be even simpler.  E.g. I thing some functional  $$q(t)\mapsto q(t)-\text{an integral over}\  f(q(t))\  \text{something},$$ should exists which will be constant for solutions of the equation, i.e. only depend on $q_0$. For each $f$, the functional dependence of this ""energy"" on ""$q_0$"" will be different. However, I can't seem to find a general relation. What's the theory behind this, is there an energy'ish time integral of motion? What is the functional dependence on the intial condition, for a suitable constant of motion for for first order systems. And what would be the interpretation, given that we speak of a situation with only one initial condition? If it is that case that the system is too restricted so that there is no meaningful geometrical interpretation, then let's think about a system of first order differential equations. This is like the one we generated from phase space, except that it doesn't really come from a second order situation and so the intial conditions aren't really e.g. intial position and velocity. I'm pretty sure there are situation where one considers such a directly generated flow (the equation might be more complicated than exponential flow $\dot x\propto x$), but I don't recall any talk about the time-constant of motion in these systems, or how to interpret it.",,['ordinary-differential-equations']
32,bifurcation value,bifurcation value,,"I tried to understand how to locate the bifurcation value for the one-parameter family. From my understanding the bifurcation value is the maximum or minimum point of a parabola, so I set the differential equals to zero to find the equilibrium points. From two equilibrium point, I'll find the midpoint and try to find the output value that corresponding to that midpoint. For example: I got this differential equation $dy/dt=y^2-ay+1$ I let $dy/dt=0$ then solve for $y$. I got $y= (a+\sqrt{a^2-4})/2$ and  $y= (a-\sqrt{a^2-4})/2$ . I find the midpoint of these two value and got $y=a/2$. Plug back to the differential equation, solve for $a$ and got $a=2$ and $a=-2$. What do I do from here if I want to use the values of $a$ to draw the phase line ?","I tried to understand how to locate the bifurcation value for the one-parameter family. From my understanding the bifurcation value is the maximum or minimum point of a parabola, so I set the differential equals to zero to find the equilibrium points. From two equilibrium point, I'll find the midpoint and try to find the output value that corresponding to that midpoint. For example: I got this differential equation $dy/dt=y^2-ay+1$ I let $dy/dt=0$ then solve for $y$. I got $y= (a+\sqrt{a^2-4})/2$ and  $y= (a-\sqrt{a^2-4})/2$ . I find the midpoint of these two value and got $y=a/2$. Plug back to the differential equation, solve for $a$ and got $a=2$ and $a=-2$. What do I do from here if I want to use the values of $a$ to draw the phase line ?",,['ordinary-differential-equations']
33,solving a differential equation in form of integrals,solving a differential equation in form of integrals,,"I have given the differential equation $x'=\alpha(t)x+\beta(t)$. How can you write the solution $x$ in form of integrals without $x$? I've tried multiplying with the factor $\exp\left\{-\int\alpha(s)\,ds\right\}$ but it doesn't get me anywhere. Thanks for helping!","I have given the differential equation $x'=\alpha(t)x+\beta(t)$. How can you write the solution $x$ in form of integrals without $x$? I've tried multiplying with the factor $\exp\left\{-\int\alpha(s)\,ds\right\}$ but it doesn't get me anywhere. Thanks for helping!",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
34,Lemme 2.4 in Morse theory by Milnor,Lemme 2.4 in Morse theory by Milnor,,"This is lemma 2.4 from ""Morse theory"" by Milnor ,with the prove I have some questions about this prove : 1) why  $\displaystyle\frac{dc}{dt}(f)=\lim_{h\rightarrow 0} \frac{fc(t+h)-fc(t)}{h}$ and who is $f$ ? 2)Why the differential equation has locally one solution which depends on the initial conditions ? 3) what is the purpose of the last step ""let $\varepsilon _0>0,\dots$"" Please help me Thank you .","This is lemma 2.4 from ""Morse theory"" by Milnor ,with the prove I have some questions about this prove : 1) why  $\displaystyle\frac{dc}{dt}(f)=\lim_{h\rightarrow 0} \frac{fc(t+h)-fc(t)}{h}$ and who is $f$ ? 2)Why the differential equation has locally one solution which depends on the initial conditions ? 3) what is the purpose of the last step ""let $\varepsilon _0>0,\dots$"" Please help me Thank you .",,"['ordinary-differential-equations', 'differential-geometry', 'manifolds', 'morse-theory']"
35,Differential Equation @ 2nd Order,Differential Equation @ 2nd Order,,How to solve this? $$\dfrac{d^2x}{dt^2}=-kx^2$$ I need it in physics problem. https://physics.stackexchange.com/questions/60202/non-shm-oscillatory-motion/ Though solved without it but still it would be better to have this.,How to solve this? $$\dfrac{d^2x}{dt^2}=-kx^2$$ I need it in physics problem. https://physics.stackexchange.com/questions/60202/non-shm-oscillatory-motion/ Though solved without it but still it would be better to have this.,,['ordinary-differential-equations']
36,What's The Degree Of Given ODE?,What's The Degree Of Given ODE?,,What is the degree (order looks 2) of the following ODE (and whats the way to find it?) ? $$k{(y'')}^2 = (1+ {(y'')}^2)^3 $$,What is the degree (order looks 2) of the following ODE (and whats the way to find it?) ? $$k{(y'')}^2 = (1+ {(y'')}^2)^3 $$,,['ordinary-differential-equations']
37,System of linear differential equations with time-varying coefficients,System of linear differential equations with time-varying coefficients,,"Could someone please suggest a technique for solving the following system of ODEs? $$\begin{aligned} x_1' &= \,\,\,\,(1 + 2 \cos 2t) x_1 + (1 - 2\sin 2t) x_2\\ x_2' &= - (1 + 2 \sin2t) x_1 + (1 - 2 \cos 2t) x_2 \end{aligned} $$ What I initially tried to do was differentiate the first equation to obtain an equation for $x_1''$ and then substitute expressions for $x_2$ and $x_2'$ .  This resulted in a second-order DE involving $x_1''$ , $x_1'$ , and $x_1$ .  But this equation was extremely complex in terms of its variable coefficients.  I am thinking there must be a simpler approach.  Thanks.","Could someone please suggest a technique for solving the following system of ODEs? What I initially tried to do was differentiate the first equation to obtain an equation for and then substitute expressions for and .  This resulted in a second-order DE involving , , and .  But this equation was extremely complex in terms of its variable coefficients.  I am thinking there must be a simpler approach.  Thanks.","\begin{aligned} x_1' &= \,\,\,\,(1 + 2 \cos 2t) x_1 + (1 - 2\sin 2t) x_2\\ x_2' &= - (1 + 2 \sin2t) x_1 + (1 - 2 \cos 2t) x_2 \end{aligned}  x_1'' x_2 x_2' x_1'' x_1' x_1",['ordinary-differential-equations']
38,Manipulating log function,Manipulating log function,,"I'm trying to find the value of a constant for $\ y(0) = 0$ in the following differential equation. $$\ 2\ln(2x+3y-1) - {2x+3y \over 2} = 2x+3y + k$$ Of course when plugging in the values, I get $\ 2\ln(-1) = k$ which errors. When entering this into Wolfram Alpha, they suggest rearranging the equation from this format, $$\ 2\ln(x-1) - {x \over 2} + constant$$ to $$\ - {x \over 2} + 2\ln(1-x) + {1 \over 2} + constant$$ ""Which is the equivalent for restricted x values"", which indeed I have. This would leave me with a positive $\ln(1)$, which would solve my problem... but... My question, how does this manipulation work? I've never seen this before. I don't understand how they've made that leap (or if it's even accurate). Can anyone educate me?","I'm trying to find the value of a constant for $\ y(0) = 0$ in the following differential equation. $$\ 2\ln(2x+3y-1) - {2x+3y \over 2} = 2x+3y + k$$ Of course when plugging in the values, I get $\ 2\ln(-1) = k$ which errors. When entering this into Wolfram Alpha, they suggest rearranging the equation from this format, $$\ 2\ln(x-1) - {x \over 2} + constant$$ to $$\ - {x \over 2} + 2\ln(1-x) + {1 \over 2} + constant$$ ""Which is the equivalent for restricted x values"", which indeed I have. This would leave me with a positive $\ln(1)$, which would solve my problem... but... My question, how does this manipulation work? I've never seen this before. I don't understand how they've made that leap (or if it's even accurate). Can anyone educate me?",,"['ordinary-differential-equations', 'logarithms']"
39,Finite family of analytic functions linearly dependent if and only if Wronskian is $~0~$,Finite family of analytic functions linearly dependent if and only if Wronskian is,~0~,"I know that given two analytic functions on some domain $D$ of the complex plane, then their Wronskian determinant being $0$ is equivalent to them being linearly dependent. I would like to generalize this to a finite family of analytic functions. Naturally, one should use induction. However, the proof of $n =2$ is horrifically computational (for me, that is). Is there any clever way to avoid the mess in the induction process? Here are some notes that contain the proof of $n =2$ in case one would like to look at it. Thank you very much in advance. I am much obliged.","I know that given two analytic functions on some domain of the complex plane, then their Wronskian determinant being is equivalent to them being linearly dependent. I would like to generalize this to a finite family of analytic functions. Naturally, one should use induction. However, the proof of is horrifically computational (for me, that is). Is there any clever way to avoid the mess in the induction process? Here are some notes that contain the proof of in case one would like to look at it. Thank you very much in advance. I am much obliged.",D 0 n =2 n =2,"['linear-algebra', 'ordinary-differential-equations']"
40,Retarded Functional Differential Equation,Retarded Functional Differential Equation,,"I would like some help understanding the proof of lemma 6.1 given here , for the case of an autonomous Retarded Functional Differential Equation (RFDE). The problem for the autonomous case (In the proof of the book, drop the dependence on $t$ in the first component and suppose $\sigma=0$) is as follows. Background: Suppose $r$ is fixed. Let $C=C([-r,0],\mathbb{R})$. An autonomous RFDE is a differential equation of the form $$x'(t)=f(x_t)$$ where $x_t \in C$ defined by $x_t(\theta)=x(t+\theta)$ for $\theta \in [-r,0]$. $f$ is a continuous function from $C$ to $\mathbb R$. Given an initial function $\phi \in C$ suppose there exists a function $x \in C[-r,A)$ such that $x_0=\phi$ and $x$ satisfies the RFDE given above. Then $x$ is called a solution of the RFDE with initial function $\phi$. In such a case when there is a unique solution of the RFDE, define the solution map $T_t:C\to C$ by $T_t(\phi)=x_t$, where $x$ is the solution of the RFDE with initial function $\phi$. Question Show that $T_t$ is locally completely continuous. That is for every $\phi \in C$ there is a neighbourhood $V_t$ of $\phi$ such that $T_t(V_t)$ is contained in a compact set of $C$. What I don't understand in the proof is how they conclude that there is a nbd $V_t$ of $\phi$ and a constant $M$ such that $|T_\tau V_t|_C \leq M$ and $|f(T_\tau V_t)|<M$ for all $0 \leq \tau \leq t$ ? How does one get such $V_t$ and $M$ that is uniform for all $0 \leq \tau \leq t$. If someone with some experience in this area reads the question, I would appreciate it if you can suggest me some references to learn about Autonomous RFDE's.","I would like some help understanding the proof of lemma 6.1 given here , for the case of an autonomous Retarded Functional Differential Equation (RFDE). The problem for the autonomous case (In the proof of the book, drop the dependence on $t$ in the first component and suppose $\sigma=0$) is as follows. Background: Suppose $r$ is fixed. Let $C=C([-r,0],\mathbb{R})$. An autonomous RFDE is a differential equation of the form $$x'(t)=f(x_t)$$ where $x_t \in C$ defined by $x_t(\theta)=x(t+\theta)$ for $\theta \in [-r,0]$. $f$ is a continuous function from $C$ to $\mathbb R$. Given an initial function $\phi \in C$ suppose there exists a function $x \in C[-r,A)$ such that $x_0=\phi$ and $x$ satisfies the RFDE given above. Then $x$ is called a solution of the RFDE with initial function $\phi$. In such a case when there is a unique solution of the RFDE, define the solution map $T_t:C\to C$ by $T_t(\phi)=x_t$, where $x$ is the solution of the RFDE with initial function $\phi$. Question Show that $T_t$ is locally completely continuous. That is for every $\phi \in C$ there is a neighbourhood $V_t$ of $\phi$ such that $T_t(V_t)$ is contained in a compact set of $C$. What I don't understand in the proof is how they conclude that there is a nbd $V_t$ of $\phi$ and a constant $M$ such that $|T_\tau V_t|_C \leq M$ and $|f(T_\tau V_t)|<M$ for all $0 \leq \tau \leq t$ ? How does one get such $V_t$ and $M$ that is uniform for all $0 \leq \tau \leq t$. If someone with some experience in this area reads the question, I would appreciate it if you can suggest me some references to learn about Autonomous RFDE's.",,"['functional-analysis', 'ordinary-differential-equations']"
41,Existence of Solution to Differential Equations.,Existence of Solution to Differential Equations.,,"$f$ is locally Lipschitz in $y$ if for every $(t_0, y_0) \in (c,d) \times U$, there exists a neighborhood V of $(t_0, y_0)$, (i.e. $V = \{f(t,y) \in(c,d) \times U : ||t-t_0|<a \ \text{and} \ |y-y_0|\leq b\}$) and a constant K = K(V) such that $||f(t,x)-f(t,y)||\leq K||x-y||$ for any $(t,x),(t,y) \in V$ Existence Theorum : Consider the IVP $y'=f(t,y), \ y(t_0) = y_0 $ (1) If f is continuous on $(c,d) \times U$ and locally Lipschitz then the IVP (1) had a unique local solution. More precisely, there exists a neighborhood $\Omega$ of $(t_0, y_0)$, that is $\Omega=\{(t,y)\in (c,d) \times U:|t-t_0|\leq a, |y-y_0|\leq b\}$ such that $f$ is Lipschitz in y with Lipschitz constant K on $\Omega$ and let M be a number such that $||f(t,y)||\leq M$ for $(t,y)\in \Omega$. Choose $0<\alpha<\min[\frac{1}{K},\frac{b}{M},a]$. Then there exists a unique solution of the IVP (1) valid on $[t_0-\alpha  , t_0 +\alpha ]$. QUESTION ) The function $f(y) = 1 + y^2$ is locally Lipschitzian. Consider the IVP $y' = 1 + y^2 ,y(0) = 0$: (a) Using the rectangle   in the hypothesis of the local existence and uniqueness Theorem, compute $\alpha$ in terms of a, b, M, and the Lipschitz constant of $f$. (b) Is it possible for your  found in part (a) to be greater than $\frac{\pi}{2}$? Justify your answer. [Do not compute explicitly the solution of the IVP. For this question, I was unable to start on part (a). Part (b), I realize that the solution is $y=tan(x)$, so crossing $\frac{\pi}{2}$ would mean that the function is not continuous because that is how the tan function behaves. But I do not know how to show that without computing IVP because I couldn't figure out part (a).","$f$ is locally Lipschitz in $y$ if for every $(t_0, y_0) \in (c,d) \times U$, there exists a neighborhood V of $(t_0, y_0)$, (i.e. $V = \{f(t,y) \in(c,d) \times U : ||t-t_0|<a \ \text{and} \ |y-y_0|\leq b\}$) and a constant K = K(V) such that $||f(t,x)-f(t,y)||\leq K||x-y||$ for any $(t,x),(t,y) \in V$ Existence Theorum : Consider the IVP $y'=f(t,y), \ y(t_0) = y_0 $ (1) If f is continuous on $(c,d) \times U$ and locally Lipschitz then the IVP (1) had a unique local solution. More precisely, there exists a neighborhood $\Omega$ of $(t_0, y_0)$, that is $\Omega=\{(t,y)\in (c,d) \times U:|t-t_0|\leq a, |y-y_0|\leq b\}$ such that $f$ is Lipschitz in y with Lipschitz constant K on $\Omega$ and let M be a number such that $||f(t,y)||\leq M$ for $(t,y)\in \Omega$. Choose $0<\alpha<\min[\frac{1}{K},\frac{b}{M},a]$. Then there exists a unique solution of the IVP (1) valid on $[t_0-\alpha  , t_0 +\alpha ]$. QUESTION ) The function $f(y) = 1 + y^2$ is locally Lipschitzian. Consider the IVP $y' = 1 + y^2 ,y(0) = 0$: (a) Using the rectangle   in the hypothesis of the local existence and uniqueness Theorem, compute $\alpha$ in terms of a, b, M, and the Lipschitz constant of $f$. (b) Is it possible for your  found in part (a) to be greater than $\frac{\pi}{2}$? Justify your answer. [Do not compute explicitly the solution of the IVP. For this question, I was unable to start on part (a). Part (b), I realize that the solution is $y=tan(x)$, so crossing $\frac{\pi}{2}$ would mean that the function is not continuous because that is how the tan function behaves. But I do not know how to show that without computing IVP because I couldn't figure out part (a).",,"['real-analysis', 'ordinary-differential-equations']"
42,Determine equilibrium temperature distribution.,Determine equilibrium temperature distribution.,,"I am trying to solve the following question in my text book. I've done simple versions of this question, but I fail to get right result using the same approach: Determine the equilibrium temperature distribution for a one-dimensional rod with constant thermal properties with the following sources and boundary conditions: $$Q=0$$ $$\frac{\partial u }{\partial x}(0,t) =u(0,t)-T$$  $$\frac{\partial u}{\partial x}(L,t)=\alpha$$ My approach: 1) Since $Q=0$, that means my answer should have the structure: $C_1(x)+C_2=u(x)$ 2) Now this is where I get messed up, instead of giving boundaries like $u(0)=0$ and $u(L)=T$, here they give me these differential equations. So what do I attempt? I try to get u(x) back by anti-differential. But this really gives me a weird result with which I don't think I can do anything...Any ideas?","I am trying to solve the following question in my text book. I've done simple versions of this question, but I fail to get right result using the same approach: Determine the equilibrium temperature distribution for a one-dimensional rod with constant thermal properties with the following sources and boundary conditions: $$Q=0$$ $$\frac{\partial u }{\partial x}(0,t) =u(0,t)-T$$  $$\frac{\partial u}{\partial x}(L,t)=\alpha$$ My approach: 1) Since $Q=0$, that means my answer should have the structure: $C_1(x)+C_2=u(x)$ 2) Now this is where I get messed up, instead of giving boundaries like $u(0)=0$ and $u(L)=T$, here they give me these differential equations. So what do I attempt? I try to get u(x) back by anti-differential. But this really gives me a weird result with which I don't think I can do anything...Any ideas?",,['ordinary-differential-equations']
43,definition of total differentiabiliy,definition of total differentiabiliy,,"Prove the following theorem, using the definition of total differentiability: Let $u: \mathbb{R}^m \rightarrow \mathbb{R}$ be totally differentiable in $x \in \mathbb{R^m}$. then $u$ is continuous in $x$. i need to use: a function $u$ is continuous in $x_0 \in \mathbb{R^m}$ if $\lim_{x \rightarrow x_0} u(x) = u(x_0)$ Some one please help - i have no idea how to set this out but i know the theorem.. thanks","Prove the following theorem, using the definition of total differentiability: Let $u: \mathbb{R}^m \rightarrow \mathbb{R}$ be totally differentiable in $x \in \mathbb{R^m}$. then $u$ is continuous in $x$. i need to use: a function $u$ is continuous in $x_0 \in \mathbb{R^m}$ if $\lim_{x \rightarrow x_0} u(x) = u(x_0)$ Some one please help - i have no idea how to set this out but i know the theorem.. thanks",,['ordinary-differential-equations']
44,Continuity of solutions of ODEs with Dirac delta forcing,Continuity of solutions of ODEs with Dirac delta forcing,,"The problem is : Find $y$, if $y''-4y=\delta(x-a)$, where $\delta$ is the Dirac delta function, and $y$ is bounded as $|x|\rightarrow\infty$. The solution goes like this: For $x<a$, $y=Ae^{2x}+Be^{-2x}$, and because of the boundary condition, $B=0$, so $y=Ae^{2x}$. For $x>a$, $y=Ce^{2x}+De^{-2x}$, and because of the boundary condition, $C=0$, so $y=De^{-2x}$. At $x=a$, the solution must be continuous, so $Ae^{2a}=De^{-2a}$. Also, the integral from $a^-$ to $a^+$ of the LHS of the equation must be equal to the same integral of the RHS of the equation, which is 1. This gives the other needed equation for $A$ and $D$ from which we can easily find the general solution (I didn't write it as it is irrelevant for the question, but if anyone cares, the general solution is $y=-\dfrac{e^{-2|x-a|}}4$ for $y\neq a$). My question is: why must $y$ be continuous at $a$?  An explanation that I have seen is the following: If $y$ had a simple discontinuity (finite jump) at $a$, then $y'$ would have a $\delta$-function singularity at $a$, and so would $y''$, but there is nothing in the equation to balance such a singularity. I think that this is an incorrect explanation, as $y''$ would actually have a -2-singularity at $x=a$, while $\delta$-singularity is -1-singularity. This offers an explanation, as the equation doesn't balance out -2-singularities, but is there a more rigorous and simpler proof why $y$ is continuous at $a$?","The problem is : Find $y$, if $y''-4y=\delta(x-a)$, where $\delta$ is the Dirac delta function, and $y$ is bounded as $|x|\rightarrow\infty$. The solution goes like this: For $x<a$, $y=Ae^{2x}+Be^{-2x}$, and because of the boundary condition, $B=0$, so $y=Ae^{2x}$. For $x>a$, $y=Ce^{2x}+De^{-2x}$, and because of the boundary condition, $C=0$, so $y=De^{-2x}$. At $x=a$, the solution must be continuous, so $Ae^{2a}=De^{-2a}$. Also, the integral from $a^-$ to $a^+$ of the LHS of the equation must be equal to the same integral of the RHS of the equation, which is 1. This gives the other needed equation for $A$ and $D$ from which we can easily find the general solution (I didn't write it as it is irrelevant for the question, but if anyone cares, the general solution is $y=-\dfrac{e^{-2|x-a|}}4$ for $y\neq a$). My question is: why must $y$ be continuous at $a$?  An explanation that I have seen is the following: If $y$ had a simple discontinuity (finite jump) at $a$, then $y'$ would have a $\delta$-function singularity at $a$, and so would $y''$, but there is nothing in the equation to balance such a singularity. I think that this is an incorrect explanation, as $y''$ would actually have a -2-singularity at $x=a$, while $\delta$-singularity is -1-singularity. This offers an explanation, as the equation doesn't balance out -2-singularities, but is there a more rigorous and simpler proof why $y$ is continuous at $a$?",,"['ordinary-differential-equations', 'continuity']"
45,Differential equation to Difference equation?,Differential equation to Difference equation?,,"I have the following equation : $$\frac{dx}{dt} = -5(x-2)$$ $$\frac{dy}{dt} = 0$$ How do I change this differential equation to a difference equation ? Do I use Euler forward method ? I remember taking this before but I have totally forgotten about it. I tried reading online to refresh my memory but I did not really grasp the idea. I would really appreciate if someone can solve this particular equation step by step so that I can fully understand the solution, along with supporting key concept points to grasp the idea.  Thanks.","I have the following equation : $$\frac{dx}{dt} = -5(x-2)$$ $$\frac{dy}{dt} = 0$$ How do I change this differential equation to a difference equation ? Do I use Euler forward method ? I remember taking this before but I have totally forgotten about it. I tried reading online to refresh my memory but I did not really grasp the idea. I would really appreciate if someone can solve this particular equation step by step so that I can fully understand the solution, along with supporting key concept points to grasp the idea.  Thanks.",,"['linear-algebra', 'ordinary-differential-equations', 'recurrence-relations']"
46,Maximal solutions of $x' = t^2 + x^2$,Maximal solutions of,x' = t^2 + x^2,"I found the following exercise on a test: Show that the maximal solutions of $x' = t^2 + x^2$ are defined on a bounded interval. Using the preceding part: Let $f_1, f_2$ smooth functions over $\mathbb{R}^2$ to $\mathbb{R}$ such that $f_1(t,x) < f_2 (t,x) \ \forall (t,x) \in \mathbb{R}^2$. Show that if $\varphi_i$ is solution to $x' = f_i(t,x)$ and there exist $t_0$ sucht that $\varphi_1(t_0) = \varphi_2(t_0)$ then $\varphi_1(t) \leq \varphi_2(t)$ if $t  \geq t_0$ and $\varphi_1(t) \geq \varphi_2(t)$ if $t  \leq t_0$. I had no troubles with the second statement, but with the first I take $f_1(t,x) = x^2$ and $f_2(t,x) = t^2 + x^2$ and then $f_1(t,x) < f_2 (t,x)$ (it's not exactly the same that above, but works). Now, the maximal solutions to $x' = x^2$ are $$\varphi(t)=\left\{\begin{array}{lr} \frac{1}{\frac{1}{x_0} + t_0 - t} & \mbox{if }  x_0 \neq 0\\ 0 & \mbox{if }  x_0 = 0\\ \end{array}\right.$$ where $(t_0,x_0)$ is the initial condition and so if you take $x_0 > 0$ then the maximal solution $\varphi_0$ of $x' = t^2 + x^2$ by $(t_0,x_0)$ holds $\varphi_0 \geq \varphi$, which is defined up to $\frac{1}{x_0} + t_0 > t_0$ and this show that $\varphi_0$ is also defined up to $\frac{1}{x_0} + t_0$. But with the other side I couldn't give a bound for the domain of $\varphi$ of the opposite side, similarly if $x_0 < 0$ and if $x_0 = 0$ I have no idea. Can somebody give an approach with this problem?","I found the following exercise on a test: Show that the maximal solutions of $x' = t^2 + x^2$ are defined on a bounded interval. Using the preceding part: Let $f_1, f_2$ smooth functions over $\mathbb{R}^2$ to $\mathbb{R}$ such that $f_1(t,x) < f_2 (t,x) \ \forall (t,x) \in \mathbb{R}^2$. Show that if $\varphi_i$ is solution to $x' = f_i(t,x)$ and there exist $t_0$ sucht that $\varphi_1(t_0) = \varphi_2(t_0)$ then $\varphi_1(t) \leq \varphi_2(t)$ if $t  \geq t_0$ and $\varphi_1(t) \geq \varphi_2(t)$ if $t  \leq t_0$. I had no troubles with the second statement, but with the first I take $f_1(t,x) = x^2$ and $f_2(t,x) = t^2 + x^2$ and then $f_1(t,x) < f_2 (t,x)$ (it's not exactly the same that above, but works). Now, the maximal solutions to $x' = x^2$ are $$\varphi(t)=\left\{\begin{array}{lr} \frac{1}{\frac{1}{x_0} + t_0 - t} & \mbox{if }  x_0 \neq 0\\ 0 & \mbox{if }  x_0 = 0\\ \end{array}\right.$$ where $(t_0,x_0)$ is the initial condition and so if you take $x_0 > 0$ then the maximal solution $\varphi_0$ of $x' = t^2 + x^2$ by $(t_0,x_0)$ holds $\varphi_0 \geq \varphi$, which is defined up to $\frac{1}{x_0} + t_0 > t_0$ and this show that $\varphi_0$ is also defined up to $\frac{1}{x_0} + t_0$. But with the other side I couldn't give a bound for the domain of $\varphi$ of the opposite side, similarly if $x_0 < 0$ and if $x_0 = 0$ I have no idea. Can somebody give an approach with this problem?",,['ordinary-differential-equations']
47,Submersion Theorem for Banach Spaces,Submersion Theorem for Banach Spaces,,"I'm having difficulty proving a well-known result from functional analysis. Any hints would be greatly appreciated. Fix a Fréchet differentiable map of Banach spaces $g: X \to B$. Assume that, at a point $x_0 \in X$ where $g(x_0) = 0$, the derivative $Dg$ is Fredholm and surjective, so in particular the index of $Dg$ is the dimension of its kernel. A typical place this might arise is in solving PDEs, where $g$ is given by some nice differential operator and $x_0$ is some generic solution to the PDE. I want to prove: The moduli space $\{x \in X \,  | \, g(x) = 0\}$ has a smooth chart near $x_0$ of dimension $index(Dg)$. I know this is supposed to be a consequence of the implicit function theorem for Frechet maps, which I understand. I also understand the proof of the submersion theorem for finite-dimensional manifolds, which is simply an application of the implicit function theorem in finite dimensions. I am trying, in essence, to prove the above result the same way one would prove the finite-dimensional submersion theorem. However, I'm having difficulty proving a version of the submersion theorem in the infinite-dimensional setting. I'll describe my approach here -- I suspect that maybe I'm trying to use the implicit function theorem in the wrong way. Defining the finite-dimensional vector space $Y = kernel(Dg)$ and the Banach space $Z = B \times Y$, if one can choose a splitting map $\pi: X \to Y$ , one can define a map $$F: X \times Y \to Z$$ given by  $$F(x,y) = (g(x), \pi(x) - y).$$ Then the implicit function theorem obviously gives a parametrization of the set $\{x \, | \, g(x) = 0\}$ near $x_0$, in terms of an open set in $Y$. The splitting map $\pi$ always exists when $X, B$ are finite dimensional, but my question is: can such a splitting always be chosen when $X, B$ are infinite-dimensional? And if not, how can I prove the main result above ? I'm trying to follow the finite-dimensional analogies, perhaps too naively, and failing. Any help would be appreciated.","I'm having difficulty proving a well-known result from functional analysis. Any hints would be greatly appreciated. Fix a Fréchet differentiable map of Banach spaces $g: X \to B$. Assume that, at a point $x_0 \in X$ where $g(x_0) = 0$, the derivative $Dg$ is Fredholm and surjective, so in particular the index of $Dg$ is the dimension of its kernel. A typical place this might arise is in solving PDEs, where $g$ is given by some nice differential operator and $x_0$ is some generic solution to the PDE. I want to prove: The moduli space $\{x \in X \,  | \, g(x) = 0\}$ has a smooth chart near $x_0$ of dimension $index(Dg)$. I know this is supposed to be a consequence of the implicit function theorem for Frechet maps, which I understand. I also understand the proof of the submersion theorem for finite-dimensional manifolds, which is simply an application of the implicit function theorem in finite dimensions. I am trying, in essence, to prove the above result the same way one would prove the finite-dimensional submersion theorem. However, I'm having difficulty proving a version of the submersion theorem in the infinite-dimensional setting. I'll describe my approach here -- I suspect that maybe I'm trying to use the implicit function theorem in the wrong way. Defining the finite-dimensional vector space $Y = kernel(Dg)$ and the Banach space $Z = B \times Y$, if one can choose a splitting map $\pi: X \to Y$ , one can define a map $$F: X \times Y \to Z$$ given by  $$F(x,y) = (g(x), \pi(x) - y).$$ Then the implicit function theorem obviously gives a parametrization of the set $\{x \, | \, g(x) = 0\}$ near $x_0$, in terms of an open set in $Y$. The splitting map $\pi$ always exists when $X, B$ are finite dimensional, but my question is: can such a splitting always be chosen when $X, B$ are infinite-dimensional? And if not, how can I prove the main result above ? I'm trying to follow the finite-dimensional analogies, perhaps too naively, and failing. Any help would be appreciated.",,"['functional-analysis', 'ordinary-differential-equations']"
48,Time Independent Schrodinger equation,Time Independent Schrodinger equation,,"$-k^2f''(x)=(E^2-V(x))f(x)$ I have checked for many V(x) that the set of solutions to this differential equation are complete and have an orthogonal basis set, is this always true for all V(x)?","$-k^2f''(x)=(E^2-V(x))f(x)$ I have checked for many V(x) that the set of solutions to this differential equation are complete and have an orthogonal basis set, is this always true for all V(x)?",,['ordinary-differential-equations']
49,Poincaré-Bendixson,Poincaré-Bendixson,,"Let $M$ be an open subset of $\mathbb{R}^2$ and consider the ordinary differential equation $\dot{x}(t) = f(x(t))$, where $f \in \mathcal{C}^{1}(M, \mathbb{R}^{2})$ and denote by $\Phi(.,.)$ its flow. For $x \in M$, let $\omega (x) := \{y \in M \ : \ \exists t_{n} \to +\infty \ s.t. \Phi(t_{n},x) \to y \}$ and $\alpha(x):= \{y \in M \ : \ \exists t_{n} \to -\infty \ s.t. \Phi(t_{n},x) \to y \}$. Suppose that $\omega(x)$ is compact. Then it is also connected (see for example the exercices in the according chapter in the book ""Differential Equations ..."" by Hirsch, Smale, Devaney). The Poincaré-Bendixson theorem states that if $\omega(x)$ contains only finitely many fixed points then $\omega(x)$ is either a) a single fixed point b) the periodic orbit of some $y \in \omega(x)$ or c) it consists of regular and fixed points and for every regular point $y \in \omega(x)$ we have that $\omega(y)$ consists of a single fixed point, and the same is true for $\alpha(y)$ . It can be also shown that for two distinct fixed points $z_{1}, z_{2} \in \omega(x)$ there is maximally one orbit $\gamma(y)\subset \omega(x)$ which joins them. So if one supposes additionally (to the compactness hypothesis) that in $\omega(x)$ there are only finitely many homoclinic orbits included, then it is easy to show that for each set of distinct fixed points $z_{1},z_{2} \in \omega(x)$  there is exactly one heteroclinc orbit connecting them (this is also called a ""graphic""). But what can be said if there is a fixed point $z_{0} \in \omega(x)$ with infinitely many homoclinic orbits in $\omega(x)$? Is then still true that any pair of distinct fixed point $z_{1}, z_{2}$ in $\omega(x)$ can be joined by an heteroclinic orbit contained in $\omega(x)$? This does not seem to be obvious, because (from what I know) it seems possible that the there is a sequence of regular points $y_{n} \subset \omega(x)$ such the orbits $\gamma(y_{n})$ are homoclinic ones which end in $z_{0}$ while the sequence $y_{n}$ converges to another fixed point $z_{1} \in \omega(x)$ in a way so that $\omega(x)$ is connected (think of the topologist's sine curve) without containing a heteroclinic orbit which joins the fixed points $z_{0}$ and $z_{1}$.","Let $M$ be an open subset of $\mathbb{R}^2$ and consider the ordinary differential equation $\dot{x}(t) = f(x(t))$, where $f \in \mathcal{C}^{1}(M, \mathbb{R}^{2})$ and denote by $\Phi(.,.)$ its flow. For $x \in M$, let $\omega (x) := \{y \in M \ : \ \exists t_{n} \to +\infty \ s.t. \Phi(t_{n},x) \to y \}$ and $\alpha(x):= \{y \in M \ : \ \exists t_{n} \to -\infty \ s.t. \Phi(t_{n},x) \to y \}$. Suppose that $\omega(x)$ is compact. Then it is also connected (see for example the exercices in the according chapter in the book ""Differential Equations ..."" by Hirsch, Smale, Devaney). The Poincaré-Bendixson theorem states that if $\omega(x)$ contains only finitely many fixed points then $\omega(x)$ is either a) a single fixed point b) the periodic orbit of some $y \in \omega(x)$ or c) it consists of regular and fixed points and for every regular point $y \in \omega(x)$ we have that $\omega(y)$ consists of a single fixed point, and the same is true for $\alpha(y)$ . It can be also shown that for two distinct fixed points $z_{1}, z_{2} \in \omega(x)$ there is maximally one orbit $\gamma(y)\subset \omega(x)$ which joins them. So if one supposes additionally (to the compactness hypothesis) that in $\omega(x)$ there are only finitely many homoclinic orbits included, then it is easy to show that for each set of distinct fixed points $z_{1},z_{2} \in \omega(x)$  there is exactly one heteroclinc orbit connecting them (this is also called a ""graphic""). But what can be said if there is a fixed point $z_{0} \in \omega(x)$ with infinitely many homoclinic orbits in $\omega(x)$? Is then still true that any pair of distinct fixed point $z_{1}, z_{2}$ in $\omega(x)$ can be joined by an heteroclinic orbit contained in $\omega(x)$? This does not seem to be obvious, because (from what I know) it seems possible that the there is a sequence of regular points $y_{n} \subset \omega(x)$ such the orbits $\gamma(y_{n})$ are homoclinic ones which end in $z_{0}$ while the sequence $y_{n}$ converges to another fixed point $z_{1} \in \omega(x)$ in a way so that $\omega(x)$ is connected (think of the topologist's sine curve) without containing a heteroclinic orbit which joins the fixed points $z_{0}$ and $z_{1}$.",,"['general-topology', 'ordinary-differential-equations']"
50,Symmetry in ordinary differential equations,Symmetry in ordinary differential equations,,"Suppose I am  given an ode $${dy\over dx}={1\over x^2}f(xy)$$ where $f$ is some arbitrary function. How then does doing the following help solve the equation? : First I have a vector field $$V=x\partial_x-y\partial_y$$ I see that this corresponds to the transformation $$(x,y)\to (x\exp(p), y\exp(p))$$ where $p$ is a parameter. Then I sought invariant coordinates $q_1,q_2$ of the vector field such that $$V(q_1)=1, V(q_2)=0$$ In this case I could take $q_1=xy$ and $q_2=\log|x|$. Though there might be other choices that are more suitable? Please help!","Suppose I am  given an ode $${dy\over dx}={1\over x^2}f(xy)$$ where $f$ is some arbitrary function. How then does doing the following help solve the equation? : First I have a vector field $$V=x\partial_x-y\partial_y$$ I see that this corresponds to the transformation $$(x,y)\to (x\exp(p), y\exp(p))$$ where $p$ is a parameter. Then I sought invariant coordinates $q_1,q_2$ of the vector field such that $$V(q_1)=1, V(q_2)=0$$ In this case I could take $q_1=xy$ and $q_2=\log|x|$. Though there might be other choices that are more suitable? Please help!",,"['ordinary-differential-equations', 'vector-spaces']"
51,Some problems solving an ODE,Some problems solving an ODE,,"Given $$y'=\frac{e^{-y^2}}{y(2x+x^2)}$$ What is the best method to solve this equation? I thought write it in seperable notation, obtaining $$ \frac{X_1(x)}{X_2(x)}+\frac{Y_2(y)}{Y_1(y)}y'=0,$$ which has a solution $$\int\frac{X_1(x)}{X_2(x)}dx+\int\frac{Y_2(y)}{Y_1(y)}dy=c$$ I defined $$X_1=1; \quad X_2=2x+x^2; \quad Y_2=-y; \quad Y_1= e^{y^2}.$$ My solution is $$\frac 1 2 \log(x)-\log(x+2) -\frac 1 2 e^{y^2}=c.$$ Is this the correct solution?","Given $$y'=\frac{e^{-y^2}}{y(2x+x^2)}$$ What is the best method to solve this equation? I thought write it in seperable notation, obtaining $$ \frac{X_1(x)}{X_2(x)}+\frac{Y_2(y)}{Y_1(y)}y'=0,$$ which has a solution $$\int\frac{X_1(x)}{X_2(x)}dx+\int\frac{Y_2(y)}{Y_1(y)}dy=c$$ I defined $$X_1=1; \quad X_2=2x+x^2; \quad Y_2=-y; \quad Y_1= e^{y^2}.$$ My solution is $$\frac 1 2 \log(x)-\log(x+2) -\frac 1 2 e^{y^2}=c.$$ Is this the correct solution?",,['ordinary-differential-equations']
52,2nd order differential equation which not in general form,2nd order differential equation which not in general form,,I'm having a really hard time solving this problem. $(1-x^2)y^{\prime \prime} - xy^{\prime} + y = 6x$ I basically tried the Power series method and Frobenius method to solve this. But in both cases the general equation doesn't include any constant like $-6x$ in this case.. But I'm pretty sure there must be a tricky way for solving this. I'm running out of time and any kind of help will be very grateful. Thanks in advance,I'm having a really hard time solving this problem. I basically tried the Power series method and Frobenius method to solve this. But in both cases the general equation doesn't include any constant like in this case.. But I'm pretty sure there must be a tricky way for solving this. I'm running out of time and any kind of help will be very grateful. Thanks in advance,(1-x^2)y^{\prime \prime} - xy^{\prime} + y = 6x -6x,['ordinary-differential-equations']
53,Solving differential equation invariant,Solving differential equation invariant,,"I have eqn: $\frac{dx}{dt} = -y(t)$ and $\frac{dy}{dt} = x(t)$ I know that $(x(0),y(0))= (1,0)$. I want to solve eqn and show that it admits an invariant $I = x(t)^2 + y(t)^2$. I know $x' = -y$, $y' = x$, $x^{\prime\prime} = -y' = -x$ I know general solution of $x"" = -x$ is $x = a\sin x = b\cos x$. I know  $x(0) = a\sin 0 + b\cos 0 = 1$ So $b = 1$ How can I show $a = 1$? (I think it should!) I tried $x' = a\cos x - b\sin x$ since $y = -x$ but it just gives $ a = 0$.","I have eqn: $\frac{dx}{dt} = -y(t)$ and $\frac{dy}{dt} = x(t)$ I know that $(x(0),y(0))= (1,0)$. I want to solve eqn and show that it admits an invariant $I = x(t)^2 + y(t)^2$. I know $x' = -y$, $y' = x$, $x^{\prime\prime} = -y' = -x$ I know general solution of $x"" = -x$ is $x = a\sin x = b\cos x$. I know  $x(0) = a\sin 0 + b\cos 0 = 1$ So $b = 1$ How can I show $a = 1$? (I think it should!) I tried $x' = a\cos x - b\sin x$ since $y = -x$ but it just gives $ a = 0$.",,['ordinary-differential-equations']
54,Rewrite matrix equation for Euler method and Improved Euler method,Rewrite matrix equation for Euler method and Improved Euler method,,"Consider a system of the form: (1) $x' = Ax + g$ For appropriate matrices $x'$, $A$, $x$, and $g$. If we let $y_n$ be the approximation to the solution of (1) at time step $t_n$, what matrix equation will give $y_{n+1}$ for Euler's method?  How about the improved Euler method (I understand this case may involve two equations)?","Consider a system of the form: (1) $x' = Ax + g$ For appropriate matrices $x'$, $A$, $x$, and $g$. If we let $y_n$ be the approximation to the solution of (1) at time step $t_n$, what matrix equation will give $y_{n+1}$ for Euler's method?  How about the improved Euler method (I understand this case may involve two equations)?",,"['ordinary-differential-equations', 'numerical-methods', 'matrices']"
55,Using the Wronskian and finding a general solution to a system of ODEs,Using the Wronskian and finding a general solution to a system of ODEs,,"For the system $$x' = \left[ \begin{array}{cccc} 2&6\\3&-1 \end{array} \right]x$$ with solutions $$x_1 = \left[ \begin{array}{cccc} 2e^{5t}\\e^{5t} \end{array} \right]\qquad \text{and}\quad x_2 = \left[ \begin{array}{cccc} e^{-4t}\\-e^{-4t} \end{array} \right]$$ A. Use the Wronskian to show that the solutions are linearly independent. B. Write the general solution to the problem. For A, would I just compute the determinant of the matrix $$x = \left[ \begin{array}{cccc} 2e^{5t}&e^{-4t}\\e^{5t}&-e^{-4t} \end{array} \right]$$ and show that it never equals zero, thus proving that the two solutions are linearly independent? For B, would the general solution just be $$x(t) = c_1\left[ \begin{array}{cccc} 2e^{5t}\\e^{5t} \end{array} \right] + c_2\left[ \begin{array}{cccc} e^{-4t}\\-e^{-4t} \end{array} \right]~?$$ Thanks!","For the system with solutions A. Use the Wronskian to show that the solutions are linearly independent. B. Write the general solution to the problem. For A, would I just compute the determinant of the matrix and show that it never equals zero, thus proving that the two solutions are linearly independent? For B, would the general solution just be Thanks!",x' = \left[ \begin{array}{cccc} 2&6\\3&-1 \end{array} \right]x x_1 = \left[ \begin{array}{cccc} 2e^{5t}\\e^{5t} \end{array} \right]\qquad \text{and}\quad x_2 = \left[ \begin{array}{cccc} e^{-4t}\\-e^{-4t} \end{array} \right] x = \left[ \begin{array}{cccc} 2e^{5t}&e^{-4t}\\e^{5t}&-e^{-4t} \end{array} \right] x(t) = c_1\left[ \begin{array}{cccc} 2e^{5t}\\e^{5t} \end{array} \right] + c_2\left[ \begin{array}{cccc} e^{-4t}\\-e^{-4t} \end{array} \right]~?,"['ordinary-differential-equations', 'wronskian']"
56,Solve $\partial^{2}_{x} \left[x^{2}p\right] + \partial_{x} \left[\left(x-1\right)p\right]$,Solve,\partial^{2}_{x} \left[x^{2}p\right] + \partial_{x} \left[\left(x-1\right)p\right],How do I solve the following differential equation? $\partial^{2}_{x} \left[x^{2}p\right] + \partial_{x} \left[\left(x-1\right)p\right] = 0$ I tried a Fourier transform which leads to $\left[k^{2}\partial^{2}_{k} + k\left(\partial_{k}+i\right)\right]\tilde{p} = 0$ where $\tilde{p}$ is the Fourier transform of $p$ but that doesn't really help. Any ideas?,How do I solve the following differential equation? $\partial^{2}_{x} \left[x^{2}p\right] + \partial_{x} \left[\left(x-1\right)p\right] = 0$ I tried a Fourier transform which leads to $\left[k^{2}\partial^{2}_{k} + k\left(\partial_{k}+i\right)\right]\tilde{p} = 0$ where $\tilde{p}$ is the Fourier transform of $p$ but that doesn't really help. Any ideas?,,['ordinary-differential-equations']
57,Chebyshev Diff EQ,Chebyshev Diff EQ,,"Find a power series solution about $x_0=0$ for the Chebyshev differential equation $$(1-x^2)y''-xy'+n^2 y=0,$$ as a function of of the integer $n$.  Show that the solutions form a terminating expansion for each value of $n$.  What is the orthogonality relationship for these polynomials?","Find a power series solution about $x_0=0$ for the Chebyshev differential equation $$(1-x^2)y''-xy'+n^2 y=0,$$ as a function of of the integer $n$.  Show that the solutions form a terminating expansion for each value of $n$.  What is the orthogonality relationship for these polynomials?",,"['ordinary-differential-equations', 'power-series']"
58,The system $x'=Ax$ is an attractor if and only if there is a positive quadratic form q such that $Dq(x)\cdot A(x)<0$ for all x,The system  is an attractor if and only if there is a positive quadratic form q such that  for all x,x'=Ax Dq(x)\cdot A(x)<0,"I need to show this result: Given the system of ODEs $x'=Ax$, the origin, $0$, is an attractor (equivalently, all the eigenvalues of the real matrix $A$ are negative) if and only if there exists a positive definite quadratic form $q$ such that $Dq(x)\cdot Ax<0$ for all $x\neq 0$ (D is the differential operator, $\cdot$ is the usual inner product). I have no idea how to start. I tried to do it in the 2x2 case, but expanding the expression gave me no clues. I believe I have to find an expression for $q$ which will imply in the inequality I want. If anyone could give me some hint, I would be grateful; I ask not to solve it fully, since I could use any development on Linear Algebra I could get.","I need to show this result: Given the system of ODEs $x'=Ax$, the origin, $0$, is an attractor (equivalently, all the eigenvalues of the real matrix $A$ are negative) if and only if there exists a positive definite quadratic form $q$ such that $Dq(x)\cdot Ax<0$ for all $x\neq 0$ (D is the differential operator, $\cdot$ is the usual inner product). I have no idea how to start. I tried to do it in the 2x2 case, but expanding the expression gave me no clues. I believe I have to find an expression for $q$ which will imply in the inequality I want. If anyone could give me some hint, I would be grateful; I ask not to solve it fully, since I could use any development on Linear Algebra I could get.",,"['ordinary-differential-equations', 'dynamical-systems']"
59,Some function space notations,Some function space notations,,"What do the symbols $C^{1,2}(\Sigma)$ and $C^{2,\alpha}(\Sigma)$ mean where $\Sigma$ is the domain? I know that $C^m(\Sigma)$ means $m$ times continuously differentiable functions. Also, what does $C^{\infty}_0(\Sigma)$ mean?","What do the symbols $C^{1,2}(\Sigma)$ and $C^{2,\alpha}(\Sigma)$ mean where $\Sigma$ is the domain? I know that $C^m(\Sigma)$ means $m$ times continuously differentiable functions. Also, what does $C^{\infty}_0(\Sigma)$ mean?",,"['analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
60,Is this a correct way to convert an convolution equation into differential/difference equation?,Is this a correct way to convert an convolution equation into differential/difference equation?,,"For functions $f,g,h$ that are defined over $\mathbb{R}$, suppose we have a convolution equation: $$ f = g * h. $$ I would like to convert it into a differential equation. Is it correct that $$ \frac{df}{dt} = \frac{dg}{dt} * h $$ under some conditions (unclear to me yet, differential under integral sign ?)? Why when the Laplace transform G of g is a rational function, the convolution is generally converted to a higher order differential equation, instead of first order one like above? Why when the Laplace transform G of g is not a rational function, the convolution is generally converted to a infinite order differential equation? Similarly, for functions $f,g,h$ that are defined over $\mathbb{Z}$, suppose we have a convolution equation: $$ f = g * h. $$ I would like to convert it into a difference equation. Is it correct that $$ df = dg * h $$ where $$ df(n) := f(n+1) - f(n),  $$ $$ dg(n) := g(n+1) - g(n) $$ under some conditions (unclear to me yet)? Why when the Z-transform G of g is a rational function, the convolution is generally converted to a higher order difference equation, instead of first order one like above? Why when the Z-transform G of g is not a rational function, the convolution is generally converted to a infinite order difference equation? The above questions arose from my difficulty understanding a reply by leonbloy . Thanks and regards!","For functions $f,g,h$ that are defined over $\mathbb{R}$, suppose we have a convolution equation: $$ f = g * h. $$ I would like to convert it into a differential equation. Is it correct that $$ \frac{df}{dt} = \frac{dg}{dt} * h $$ under some conditions (unclear to me yet, differential under integral sign ?)? Why when the Laplace transform G of g is a rational function, the convolution is generally converted to a higher order differential equation, instead of first order one like above? Why when the Laplace transform G of g is not a rational function, the convolution is generally converted to a infinite order differential equation? Similarly, for functions $f,g,h$ that are defined over $\mathbb{Z}$, suppose we have a convolution equation: $$ f = g * h. $$ I would like to convert it into a difference equation. Is it correct that $$ df = dg * h $$ where $$ df(n) := f(n+1) - f(n),  $$ $$ dg(n) := g(n+1) - g(n) $$ under some conditions (unclear to me yet)? Why when the Z-transform G of g is a rational function, the convolution is generally converted to a higher order difference equation, instead of first order one like above? Why when the Z-transform G of g is not a rational function, the convolution is generally converted to a infinite order difference equation? The above questions arose from my difficulty understanding a reply by leonbloy . Thanks and regards!",,"['ordinary-differential-equations', 'convolution', 'integral-transforms', 'recurrence-relations']"
61,Help Understanding Spectral Method for solving Differential Equations,Help Understanding Spectral Method for solving Differential Equations,,"I've posted a more detailed version of this question here : SE-ComputationalSci but I'm really struggling with a simpler and related question. Lets say one wants to solve (I made this equation up, right now) $F''(x) - F'(x) + x^2 F(x) = \lambda F(x)$, and you are given the boundary conditions $F(0)=F(X) =0$. The domain of $x$ is $(0,X)$. Now, if I want to use a spectral method, where I expand my functions in terms of Chebyshev polynomials. I would want to do something like this: 1) Change the domain, introduce $y = 2(x/X) -1$ then I would do 2) Express $F (x) = \Sigma_0^{\infty} f_i T_i(y)- \frac{1}{2}f_0$ , where $T_i$ are my Chebyshev polynomials. 3) Use the orthogonality of the polynomials to get set of algebraic equations to solve for $F(x)$. My question is what do I do with the $x^2$ term in my original equation. Do I rewrite that in terms of $y$, or do I define a new function $x^2*F(x) = G(x)$ and then try to work out how the expansion coefficients for $G(x)$ would be related to $f_i$? I may just be missing the whole idea of the spectral method, in the first place. The goal of understanding this better would be to reproduce the spectral method used in Appendix A of this paper: http://arxiv.org/pdf/gr-qc/0210102.pdf Thanks for any answers, and help.","I've posted a more detailed version of this question here : SE-ComputationalSci but I'm really struggling with a simpler and related question. Lets say one wants to solve (I made this equation up, right now) $F''(x) - F'(x) + x^2 F(x) = \lambda F(x)$, and you are given the boundary conditions $F(0)=F(X) =0$. The domain of $x$ is $(0,X)$. Now, if I want to use a spectral method, where I expand my functions in terms of Chebyshev polynomials. I would want to do something like this: 1) Change the domain, introduce $y = 2(x/X) -1$ then I would do 2) Express $F (x) = \Sigma_0^{\infty} f_i T_i(y)- \frac{1}{2}f_0$ , where $T_i$ are my Chebyshev polynomials. 3) Use the orthogonality of the polynomials to get set of algebraic equations to solve for $F(x)$. My question is what do I do with the $x^2$ term in my original equation. Do I rewrite that in terms of $y$, or do I define a new function $x^2*F(x) = G(x)$ and then try to work out how the expansion coefficients for $G(x)$ would be related to $f_i$? I may just be missing the whole idea of the spectral method, in the first place. The goal of understanding this better would be to reproduce the spectral method used in Appendix A of this paper: http://arxiv.org/pdf/gr-qc/0210102.pdf Thanks for any answers, and help.",,"['ordinary-differential-equations', 'special-functions']"
62,How might we find $\sigma$?,How might we find ?,\sigma,"How does one solve a ""differential equation"" for $\sigma$ of the form  $$  \sigma(v)w_i(v)={\partial \over \partial v_j}\left[\sigma(v)A_{ij}(v)\right] \quad i=1,\dots,n. $$  where the summation convention applies. $w,v$ are an $n$-D vectors, $\sigma$ is a scalar function, $A$ is an invertible $n\times n$ matrix? Perhaps there is a general solution form? References (links) for the treatment of such an equation is also appreciated. Thank you. Added: In light of drak's suggestion, here is a bit more Some thoughts: It might be friendlier to change ""variables"" to $A\sigma$? Is there a more familiar expression for the index notation ${\partial \over \partial v_j}M_{ij}(v)$ such as one in  terms of $\nabla$? It would seem to me that it is taking the divergence of each row of the matrix $M$. Some more thoughts: since the function $\sigma$ appears on both sides of the equation, it is likely that it is an exponential. A simplified version: What if we suppose that $A$ is a constant matrix?","How does one solve a ""differential equation"" for $\sigma$ of the form  $$  \sigma(v)w_i(v)={\partial \over \partial v_j}\left[\sigma(v)A_{ij}(v)\right] \quad i=1,\dots,n. $$  where the summation convention applies. $w,v$ are an $n$-D vectors, $\sigma$ is a scalar function, $A$ is an invertible $n\times n$ matrix? Perhaps there is a general solution form? References (links) for the treatment of such an equation is also appreciated. Thank you. Added: In light of drak's suggestion, here is a bit more Some thoughts: It might be friendlier to change ""variables"" to $A\sigma$? Is there a more familiar expression for the index notation ${\partial \over \partial v_j}M_{ij}(v)$ such as one in  terms of $\nabla$? It would seem to me that it is taking the divergence of each row of the matrix $M$. Some more thoughts: since the function $\sigma$ appears on both sides of the equation, it is likely that it is an exponential. A simplified version: What if we suppose that $A$ is a constant matrix?",,"['calculus', 'linear-algebra', 'ordinary-differential-equations', 'multivariable-calculus']"
63,First order ordinary differential equations involving powers of the slope,First order ordinary differential equations involving powers of the slope,,"Are there any general approaches to differential equations like $$x-x\ y(x)+y'(x)\ (y'(x)+x\ y(x))=0,$$ or that equation specifically? The problem seems to be the term $y'(x)^2$ . Solving the equation for $y'(x)$ like a qudratic equation gives some expression $y'(x)=F(y(x),x)$ , where $F$ is ""not too bad"" as it involves small polynomials in $x$ and $y$ and roots of such object. That might be a starting point for a numerical approach, but I'm actually more interested in theory now. $y(x)=1$ is a stationary solution. Plugging in $y(x)\equiv 1+z(x)$ and taking a look at the new equation makes me think functions of the form $\exp{(a\ x^n)}$ might be involved, but that's only speculation. I see no symmetry whatsoever and dimensional analysis fails.","Are there any general approaches to differential equations like or that equation specifically? The problem seems to be the term . Solving the equation for like a qudratic equation gives some expression , where is ""not too bad"" as it involves small polynomials in and and roots of such object. That might be a starting point for a numerical approach, but I'm actually more interested in theory now. is a stationary solution. Plugging in and taking a look at the new equation makes me think functions of the form might be involved, but that's only speculation. I see no symmetry whatsoever and dimensional analysis fails.","x-x\ y(x)+y'(x)\ (y'(x)+x\ y(x))=0, y'(x)^2 y'(x) y'(x)=F(y(x),x) F x y y(x)=1 y(x)\equiv 1+z(x) \exp{(a\ x^n)}",['ordinary-differential-equations']
64,First Integral of Motion for Upper Half Plane Metric,First Integral of Motion for Upper Half Plane Metric,,"The metric is $ds^2=\frac{dx^2+dy^2}{y^2}$. I have used the Euler-Lagrange equations to find the geodesics, and my equations are $\dot{x}=Ay^2$, $\ddot{y}+\frac{\dot{x}^2-\dot{y}^2}{y}=0$. I cannot seem to find the first integral for the second equation. I read online somewhere it is $\dot{y}=y\sqrt{1-Ay^2}$, but I can't seem to derive it. The only trick I currently know for doing these type of things is to multiple by $\dot{y}$ and then integrate, but that doesn't work here. Can anyone offer some guidance? I tried it a slightly different way, but it doesn't seem to work for some reason: Instead of parametrizing, x=x(t), y=y(t) and minimizing, I just minimized $\frac{1+y'(x)^2}{y^2}$. Using the Euler-Lagrange equations, I get $y''y-y'^2+1=0$, and $y(x)=sinh(x)$ is a solution to this...but the geodesics are suppose to be half circles, and this doesn't give me a half circle..I am quite confused.","The metric is $ds^2=\frac{dx^2+dy^2}{y^2}$. I have used the Euler-Lagrange equations to find the geodesics, and my equations are $\dot{x}=Ay^2$, $\ddot{y}+\frac{\dot{x}^2-\dot{y}^2}{y}=0$. I cannot seem to find the first integral for the second equation. I read online somewhere it is $\dot{y}=y\sqrt{1-Ay^2}$, but I can't seem to derive it. The only trick I currently know for doing these type of things is to multiple by $\dot{y}$ and then integrate, but that doesn't work here. Can anyone offer some guidance? I tried it a slightly different way, but it doesn't seem to work for some reason: Instead of parametrizing, x=x(t), y=y(t) and minimizing, I just minimized $\frac{1+y'(x)^2}{y^2}$. Using the Euler-Lagrange equations, I get $y''y-y'^2+1=0$, and $y(x)=sinh(x)$ is a solution to this...but the geodesics are suppose to be half circles, and this doesn't give me a half circle..I am quite confused.",,['ordinary-differential-equations']
65,Differential equation with gaussian noise,Differential equation with gaussian noise,,"The equation has the following form: $$x'' + w^2 x=n$$ $w=1$, $x(0)=1$, $n$ is Gaussian noise with mean $0$ and standard deviation of $1$. Without the Gaussian noise, i can easily solve the equation numerically by using ODE45 in matlab.The problem is, how can i deal with this equation when the Gaussian noise is taken into consideration?","The equation has the following form: $$x'' + w^2 x=n$$ $w=1$, $x(0)=1$, $n$ is Gaussian noise with mean $0$ and standard deviation of $1$. Without the Gaussian noise, i can easily solve the equation numerically by using ODE45 in matlab.The problem is, how can i deal with this equation when the Gaussian noise is taken into consideration?",,"['ordinary-differential-equations', 'stochastic-processes']"
66,Continuous variation from solution of one problem to solution of another problem,Continuous variation from solution of one problem to solution of another problem,,"This is a differential equations question.  The only thing I know about differential equations is that there can be many subtleties. Suppose that I have a function $g:\mathbb{R}^n \times \mathbb{R} \to \mathbb{R}^n$. I have a solution $x_0$ to $g(x_0, t=0)=0$.  I desire to show the existence of a solution to $g(x_1, t=1)=0$.  My strategy is to take the known solution $x_0$ and vary it from $t=0$ to $t=1$. Let $J$ be the Jacobian $J_{ij} = \partial g_i / \partial x_j$.  Define the function $f(x,t) = - J^{-1} \partial g/\partial t$.  Then, setting $dx/dt = f(x,t)$ gives \begin{equation} 	\frac{dg(x,t)}{dt} = J \frac{dx}{dt} + \frac{\partial g}{\partial t} = 0. \end{equation} So it would seem that integrating this differential equation from $t=0$ to $t=1$ will lead to the desired solution $g(x_1, t=1)=0$. What I need then is a theorem of the following form: Suppose $g(x_0, t=0)=0$, and suppose that $\exists f$ such that \begin{equation} 	\frac{dx}{dt}=f(x,t) \implies \frac{dg(x,t)}{dt}=0. \end{equation} If $f$ and $g$ are well behaved enough in the vicinity $g(x,t) \approx 0$, then $\exists x$ such that $g(x,1)=0$. When $g(x,t)=0$, I have a lower bound on the smallest singular value of $J$, and $\partial g/\partial t$ is bounded as well.  So, in this case $f$ is bounded. In fact, I think that I can pretty much prove any needed type of ""well-behavedness"" in the region $\left| g(x,t) \right| < \epsilon$ for some $\epsilon$.  So, what properties of $f$ and $g$ are needed, and what theorem will help me here?  It looks like Picard-Lindelof may help, but it seems to only give the existence of a unique solution to the differential equation, and I need to show that that solution satisfies $g(x_1, t=1)=0$.  Furthermore, $f$ is not well behaved when $g$ is far from zero, and so it seems I cannot use Picard-Lindelof without prior assumption that $g(x,t)$ stays small (which is kind of assumes the fact that I am trying to prove). NOTE: since I didn't get any answers here, I also asked on mathoverflow .","This is a differential equations question.  The only thing I know about differential equations is that there can be many subtleties. Suppose that I have a function $g:\mathbb{R}^n \times \mathbb{R} \to \mathbb{R}^n$. I have a solution $x_0$ to $g(x_0, t=0)=0$.  I desire to show the existence of a solution to $g(x_1, t=1)=0$.  My strategy is to take the known solution $x_0$ and vary it from $t=0$ to $t=1$. Let $J$ be the Jacobian $J_{ij} = \partial g_i / \partial x_j$.  Define the function $f(x,t) = - J^{-1} \partial g/\partial t$.  Then, setting $dx/dt = f(x,t)$ gives \begin{equation} 	\frac{dg(x,t)}{dt} = J \frac{dx}{dt} + \frac{\partial g}{\partial t} = 0. \end{equation} So it would seem that integrating this differential equation from $t=0$ to $t=1$ will lead to the desired solution $g(x_1, t=1)=0$. What I need then is a theorem of the following form: Suppose $g(x_0, t=0)=0$, and suppose that $\exists f$ such that \begin{equation} 	\frac{dx}{dt}=f(x,t) \implies \frac{dg(x,t)}{dt}=0. \end{equation} If $f$ and $g$ are well behaved enough in the vicinity $g(x,t) \approx 0$, then $\exists x$ such that $g(x,1)=0$. When $g(x,t)=0$, I have a lower bound on the smallest singular value of $J$, and $\partial g/\partial t$ is bounded as well.  So, in this case $f$ is bounded. In fact, I think that I can pretty much prove any needed type of ""well-behavedness"" in the region $\left| g(x,t) \right| < \epsilon$ for some $\epsilon$.  So, what properties of $f$ and $g$ are needed, and what theorem will help me here?  It looks like Picard-Lindelof may help, but it seems to only give the existence of a unique solution to the differential equation, and I need to show that that solution satisfies $g(x_1, t=1)=0$.  Furthermore, $f$ is not well behaved when $g$ is far from zero, and so it seems I cannot use Picard-Lindelof without prior assumption that $g(x,t)$ stays small (which is kind of assumes the fact that I am trying to prove). NOTE: since I didn't get any answers here, I also asked on mathoverflow .",,['ordinary-differential-equations']
67,Uniqueness of solution to 1st order pdes,Uniqueness of solution to 1st order pdes,,"I am given a 1st order partial differential equation $y{\partial \psi\over\partial x}+x{\partial \psi\over\partial y}=0$ subjected to boundary condition $\psi(x,0)=\exp(-x^2)$. I have found that a solution is $\psi(x,y)=\exp(y^2-x^2)$. But I am asked when the solution is unique. Could someone please explain how to answer this? Thanks.","I am given a 1st order partial differential equation $y{\partial \psi\over\partial x}+x{\partial \psi\over\partial y}=0$ subjected to boundary condition $\psi(x,0)=\exp(-x^2)$. I have found that a solution is $\psi(x,y)=\exp(y^2-x^2)$. But I am asked when the solution is unique. Could someone please explain how to answer this? Thanks.",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
68,Conditions for $A$ that ensure the boundedness of the solutions of $x'=Ax$,Conditions for  that ensure the boundedness of the solutions of,A x'=Ax,"I have to find conditions under which the solutions of $x'=Ax$ are bounded for $t\rightarrow \infty$ and $t\rightarrow \pm \infty$. We proved in our course that $\lim_{t \rightarrow \ \infty}  x(t)=0$ and $\lim_{t \rightarrow - \infty}  |x(t)|=\infty$, if all the eigenvalues of $A$ have negative real part (and for the last limit $x$ is not the zero solution); if $A$ has an eigenvalue greater that zero, then $\lim_{t \rightarrow  \infty}  |x(t)|=\infty$. So I think that the conditions are: $x$ is bounded for $t\rightarrow \infty$ , if the real part of all  the eigenvalues are negative (trivial, since it follows from my course) and bounded for $t\rightarrow \pm \infty$ if the real parts of all the eigenvalues vanishes. Do you think this is sufficient - or that my professor could want some other additional (finer) conditions ? Side question: Since we didn't specify what $|x(t)|$ means, I took it means the norm of $x(t)$ and not the vector of the absolute values of the components of $x(t)$, because in the latter case $\lim_{t \rightarrow  \infty}  |x(t)|=\infty$ doesn't makes sense to me, because only a sequence of numbers can have the limit $\infty$. Am I right that it means the norm ?","I have to find conditions under which the solutions of $x'=Ax$ are bounded for $t\rightarrow \infty$ and $t\rightarrow \pm \infty$. We proved in our course that $\lim_{t \rightarrow \ \infty}  x(t)=0$ and $\lim_{t \rightarrow - \infty}  |x(t)|=\infty$, if all the eigenvalues of $A$ have negative real part (and for the last limit $x$ is not the zero solution); if $A$ has an eigenvalue greater that zero, then $\lim_{t \rightarrow  \infty}  |x(t)|=\infty$. So I think that the conditions are: $x$ is bounded for $t\rightarrow \infty$ , if the real part of all  the eigenvalues are negative (trivial, since it follows from my course) and bounded for $t\rightarrow \pm \infty$ if the real parts of all the eigenvalues vanishes. Do you think this is sufficient - or that my professor could want some other additional (finer) conditions ? Side question: Since we didn't specify what $|x(t)|$ means, I took it means the norm of $x(t)$ and not the vector of the absolute values of the components of $x(t)$, because in the latter case $\lim_{t \rightarrow  \infty}  |x(t)|=\infty$ doesn't makes sense to me, because only a sequence of numbers can have the limit $\infty$. Am I right that it means the norm ?",,['ordinary-differential-equations']
69,Solving for $v$ in $v'(t) + R(t)\cdot v^{2/3}=J(t)$,Solving for  in,v v'(t) + R(t)\cdot v^{2/3}=J(t),"I have a differential equation that I can't figure out how to solve. It is a first order non-linear ordinary differential equation. Here it is: $$ v'(t) + R(t)\cdot v^{2/3}=J(t) $$ I want to solve for v. It looks like a linear one, but then you see the 2/3. Is there a way to solve this? I know how to integrate R and J if that helps. $$ R(t) = -3^{2/3}$$ and $$ J(t) = Qt+C_1 \textrm{, where }Q\textrm{ is a constant and }C_1\textrm{ is a constant of integration.}$$ This equation can also be written as $$u'(t) - 1 = J(t)\cdot u^{-2}$$ or $$w'(t)-2w^{1/2}=2J(t)\cdot w^{-1/2}$$ using change of variables where $v = \frac{u^3}{3}$, and $w=u^2$.","I have a differential equation that I can't figure out how to solve. It is a first order non-linear ordinary differential equation. Here it is: $$ v'(t) + R(t)\cdot v^{2/3}=J(t) $$ I want to solve for v. It looks like a linear one, but then you see the 2/3. Is there a way to solve this? I know how to integrate R and J if that helps. $$ R(t) = -3^{2/3}$$ and $$ J(t) = Qt+C_1 \textrm{, where }Q\textrm{ is a constant and }C_1\textrm{ is a constant of integration.}$$ This equation can also be written as $$u'(t) - 1 = J(t)\cdot u^{-2}$$ or $$w'(t)-2w^{1/2}=2J(t)\cdot w^{-1/2}$$ using change of variables where $v = \frac{u^3}{3}$, and $w=u^2$.",,"['calculus', 'ordinary-differential-equations']"
70,Nonlinear second-order ODE $yy''+(y')^2=x$ [closed],Nonlinear second-order ODE  [closed],yy''+(y')^2=x,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How to solve the ODE: $$yy''+(y')^2=x$$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question How to solve the ODE: $$yy''+(y')^2=x$$",,['ordinary-differential-equations']
71,Generic Sturm-Liouville Problem,Generic Sturm-Liouville Problem,,"The equation $\ddot{y}+(\lambda(r)^2-L^2r^{-2})y=0$ seems that can be cast in th eform of a generic Sturm-Liouville problem as $-\ddot{y}+q(r)y=\lambda_0\lambda(r)^2$ with  $q(r)=L^2r^{-2}$ It can also be considered as a kind of normal form representation of more general equations but is there a general classification of possible solutions for (a) the Initial Value problem and (b) for the Boundary Value problem?  From what is evident by inspection, for constant $\lambda=\lambda_0$ the problem becomes like the one for the Schroedinger equation with a potential $V(r)=L^2r^{-2}$ in which case spherical Bessel functions should be sufficient. A search with the use of the CONVODE package written for the REDUCE kernel showed some possibility for transforming the problem in two linear first order equations of which the second is a case of the Ricatti equation, at least for $\lambda(r)={\lambda_0r, \lambda_0r^{-1}}$. Is this relevant for other cases?","The equation $\ddot{y}+(\lambda(r)^2-L^2r^{-2})y=0$ seems that can be cast in th eform of a generic Sturm-Liouville problem as $-\ddot{y}+q(r)y=\lambda_0\lambda(r)^2$ with  $q(r)=L^2r^{-2}$ It can also be considered as a kind of normal form representation of more general equations but is there a general classification of possible solutions for (a) the Initial Value problem and (b) for the Boundary Value problem?  From what is evident by inspection, for constant $\lambda=\lambda_0$ the problem becomes like the one for the Schroedinger equation with a potential $V(r)=L^2r^{-2}$ in which case spherical Bessel functions should be sufficient. A search with the use of the CONVODE package written for the REDUCE kernel showed some possibility for transforming the problem in two linear first order equations of which the second is a case of the Ricatti equation, at least for $\lambda(r)={\lambda_0r, \lambda_0r^{-1}}$. Is this relevant for other cases?",,['ordinary-differential-equations']
72,Eigenvalues of diff-system(can't understand),Eigenvalues of diff-system(can't understand),,"In this paper the authors have the dynamical system $$\begin{align} T_f \dot{y}_f & = -y_f + (1-\alpha(v))\varphi(z,d) \\ T_r \dot{y}_r & = -y_r + \alpha(v) \varphi(z,d) \\ \dot{z}       & = -\varphi(z,d) + y_r + u \end{align}$$ and they state in eqns (8-10) that the eigenvalues of the linearization at the equilibrium points $(\overline{y}_f, \overline{y}_r, \overline{z})$ are $$\begin{align} \lambda_1 & = -T_f^{-1} \\ \lambda_2 + \lambda_3 & = -\varphi_z(\overline{z},d) - T_r^{-1} \\ \lambda_2 \lambda_3 & = T_r^{-1} \phi_z(\overline{z},d)(1-\alpha(\overline{v})) \end{align}$$ Can someone explain to me how these are derived?","In this paper the authors have the dynamical system $$\begin{align} T_f \dot{y}_f & = -y_f + (1-\alpha(v))\varphi(z,d) \\ T_r \dot{y}_r & = -y_r + \alpha(v) \varphi(z,d) \\ \dot{z}       & = -\varphi(z,d) + y_r + u \end{align}$$ and they state in eqns (8-10) that the eigenvalues of the linearization at the equilibrium points $(\overline{y}_f, \overline{y}_r, \overline{z})$ are $$\begin{align} \lambda_1 & = -T_f^{-1} \\ \lambda_2 + \lambda_3 & = -\varphi_z(\overline{z},d) - T_r^{-1} \\ \lambda_2 \lambda_3 & = T_r^{-1} \phi_z(\overline{z},d)(1-\alpha(\overline{v})) \end{align}$$ Can someone explain to me how these are derived?",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'dynamical-systems']"
73,Numerically Solving a Second Order Nonlinear ODE,Numerically Solving a Second Order Nonlinear ODE,,"Okay, I have this not so pretty 2nd order non-linear ODE I should be able to solve numerically. $$f''(R) + \frac{2}{R} f'(R)=\frac{0.7}{R} \left( \frac{1}{\sqrt{f(R)}} - \frac{0.3}{\sqrt{1-f(R)}} \right),$$ $$f(1)=1.$$ The function around the origin is behaving very wildly. I was thinking of breaking this guy up into a system of two first order ODE's and then solve, but I have no idea how to set this up. What method should I use to set up the system of ODE's? If there is some other method rather than numerically solving a system of differential equations, please feel welcome to share. Thanks.","Okay, I have this not so pretty 2nd order non-linear ODE I should be able to solve numerically. $$f''(R) + \frac{2}{R} f'(R)=\frac{0.7}{R} \left( \frac{1}{\sqrt{f(R)}} - \frac{0.3}{\sqrt{1-f(R)}} \right),$$ $$f(1)=1.$$ The function around the origin is behaving very wildly. I was thinking of breaking this guy up into a system of two first order ODE's and then solve, but I have no idea how to set this up. What method should I use to set up the system of ODE's? If there is some other method rather than numerically solving a system of differential equations, please feel welcome to share. Thanks.",,"['analysis', 'ordinary-differential-equations', 'math-software', 'numerical-methods', 'matlab']"
74,Missing solutions from $y'(x)^2 = 2 y(x) y''(x)$,Missing solutions from,y'(x)^2 = 2 y(x) y''(x),"I am looking into this ODE: $y'(x)^2 = 2 y(x) y''(x)$ , and I find some solutions through the following steps: $$ \frac{y'(x)}{y(x)} = 2 \frac{y''(x)}{y'(x)}$$ This suggests that by integration I should have $$ \ln( y(x)) = 2 \ln( y'(x) ) + C_0 $$ $$ y(x) = \lambda y'(x)^2 $$ with $\lambda = e^{C_0}$ Again, this suggests 2 possible pairs of signs: $$ \pm \lambda^{-\frac{1}{2}} = \frac{y'(x)}{ \pm \sqrt{y(x)}} $$ By integrating, I get 2 possible solutions: $$ \pm \lambda^{-\frac{1}{2}} x + C_1 = \ln{ \left| \sqrt{y(x)} \right| }$$ $$ y(x) = e^{\pm 2 \lambda^{-\frac{1}{2}} x + C_1}$$ where $C_1$ is allowed to be complex. These are certainly solutions, but apparently this procedure does not capture all possible solutions. In fact, if $y(x) = x^n$ , then we have $$ n \frac{x^{n-1}}{x^{n}} = 2 \frac{ (n - 1) x^{n-2}}{x^{n-1}} $$ which is a solution if $n = 2n - 2$ , which has a solution for $n=2$ So my question is here: why are these polynomial solutions missed from my 1st attempt at solving the ODE? If the procedure misses polynomial solutions, what other solutions might be also amiss?","I am looking into this ODE: , and I find some solutions through the following steps: This suggests that by integration I should have with Again, this suggests 2 possible pairs of signs: By integrating, I get 2 possible solutions: where is allowed to be complex. These are certainly solutions, but apparently this procedure does not capture all possible solutions. In fact, if , then we have which is a solution if , which has a solution for So my question is here: why are these polynomial solutions missed from my 1st attempt at solving the ODE? If the procedure misses polynomial solutions, what other solutions might be also amiss?",y'(x)^2 = 2 y(x) y''(x)  \frac{y'(x)}{y(x)} = 2 \frac{y''(x)}{y'(x)}  \ln( y(x)) = 2 \ln( y'(x) ) + C_0   y(x) = \lambda y'(x)^2  \lambda = e^{C_0}  \pm \lambda^{-\frac{1}{2}} = \frac{y'(x)}{ \pm \sqrt{y(x)}}   \pm \lambda^{-\frac{1}{2}} x + C_1 = \ln{ \left| \sqrt{y(x)} \right| }  y(x) = e^{\pm 2 \lambda^{-\frac{1}{2}} x + C_1} C_1 y(x) = x^n  n \frac{x^{n-1}}{x^{n}} = 2 \frac{ (n - 1) x^{n-2}}{x^{n-1}}  n = 2n - 2 n=2,['ordinary-differential-equations']
75,How to Handle a Singularity: Seeking Solutions for a Differential Equation System,How to Handle a Singularity: Seeking Solutions for a Differential Equation System,,"I am trying to solve the following differential equation system: $\begin{aligned} & r^2 K^{\prime \prime}=K(K-1)(K-2)+\frac{1}{4} h^2 K \\ & r^2 h^{\prime \prime}=\frac{1}{2} h K^2+\left(\lambda / g^2\right)\left(h^2-g^2 V_0^2 r^2\right) h\end{aligned}$ I have already tried Rk4 but it does not give me satisfying results (the problem is the singularity at r = 0). And before I try other methods, I wanted to ask if this type of differential equation is known or if it can at least be put into a known form so that I can think of a separate method. Normally the following procedure should let me solve the system numerically, or not? $\begin{aligned}\frac{du_1}{dr} &= u_2 \\ \frac{du_2}{dr} &= \frac{u_1(u_1-1)(u_1-2)}{r^2} + \frac{\frac{1}{4}u_3^2 u_1}{r^2} \\ \frac{du_3}{dr} &= u_4 \\ \frac{du_4}{dr} &= \frac{\frac{1}{2}u_1^2u_3 + \left(\frac{\lambda}{g^2}\right)(u_3^2 - g^2V_0^2r^2)u_3}{r^2}\end{aligned}$ with: $\begin{aligned} u_1 &= K  \\ u_2 &= K' = \frac{du_1}{dr} \\ u_3 &= h \\ u_4 &= h' = \frac{du_3}{dr}\end{aligned}$","I am trying to solve the following differential equation system: I have already tried Rk4 but it does not give me satisfying results (the problem is the singularity at r = 0). And before I try other methods, I wanted to ask if this type of differential equation is known or if it can at least be put into a known form so that I can think of a separate method. Normally the following procedure should let me solve the system numerically, or not? with:","\begin{aligned} & r^2 K^{\prime \prime}=K(K-1)(K-2)+\frac{1}{4} h^2 K \\ & r^2 h^{\prime \prime}=\frac{1}{2} h K^2+\left(\lambda / g^2\right)\left(h^2-g^2 V_0^2 r^2\right) h\end{aligned} \begin{aligned}\frac{du_1}{dr} &= u_2 \\
\frac{du_2}{dr} &= \frac{u_1(u_1-1)(u_1-2)}{r^2} + \frac{\frac{1}{4}u_3^2 u_1}{r^2} \\
\frac{du_3}{dr} &= u_4 \\
\frac{du_4}{dr} &= \frac{\frac{1}{2}u_1^2u_3 + \left(\frac{\lambda}{g^2}\right)(u_3^2 - g^2V_0^2r^2)u_3}{r^2}\end{aligned} \begin{aligned} u_1 &= K  \\
u_2 &= K' = \frac{du_1}{dr} \\
u_3 &= h \\
u_4 &= h' = \frac{du_3}{dr}\end{aligned}","['ordinary-differential-equations', 'numerical-methods', 'nonlinear-system']"
76,"Given a function, find a Stochastic Differential Equation that the function satisfies","Given a function, find a Stochastic Differential Equation that the function satisfies",,"Suppose we have an SDE: $$dX = A X dW_1 + B X dW_2, \tag{1}$$ where $A$ and $B$ are constant non-commuting matrices acting on the Hilbert space $\mathbb{C}^2$ , and $W_1(t), W_2(t)$ are independent 1-dimensional Wiener processes. Also, assume the initial condition is a constant vector $X_0 \in \mathbb{C}^2$ . Since $A$ and $B$ do not commute, there is no closed-form solution to (1). For example, the  solution below is valid only if $A$ and $B$ commute: $$Z^{(1)}(t)=\exp\left(-\frac{1}{2} \left(A^2 + B^2 \right)t + A W_1(t) + B W_2(t) \right)X_0. \tag{2}$$ However, $Z^{(1)}(t)$ is an approximate solution to (1), which can be obtained by the first-order Magnus expansion. To get a better approximation, we must introduce commutators of $A$ and $B$ . This yields a second-order Magnus expansion: \begin{align} Z^{(2)}(t) = \exp \left \{ -\frac{1}{2} \left(A^2 + B^2 \right)t \\ +A W_1(t) + B W_2(t) + \frac{1}{2}[A,B] \int_0^t W_2(s) dW_1(s) + \frac{1}{2}[B,A] \int_0^t W_1(s) dW_2(s) \right \}X_0. \tag{3} \end{align} My inquiry: I would like to find an approximate SDE that $Z^{(1)}(t)$ satisfies. From the argument above, we know that (1) is an approximate SDE to (2). However, the SDE in (1) is a pretty crude approximation; we could call it a ""first-order"" approximate SDE. I'm interested in the ""second-order"" approximate SDE. I tried applying Ito's lemma to (2), but the problem is that I don't know how to calculate partial derivatives ( i.e. $\frac{\partial}{\partial x_1}, \frac{\partial}{\partial x_2},\frac{\partial}{\partial t}$ ) of $\exp(-\frac{1}{2}(A^2 + B^2)t + A x_1 + B x_2 )$ . I looked at exponential map differentiation , but it does not yield the desired result because after differentiating, the exponential map $\exp(\cdot)$ is on the left-hand side of the $\text{ad}_{\chi}$ operator.","Suppose we have an SDE: where and are constant non-commuting matrices acting on the Hilbert space , and are independent 1-dimensional Wiener processes. Also, assume the initial condition is a constant vector . Since and do not commute, there is no closed-form solution to (1). For example, the  solution below is valid only if and commute: However, is an approximate solution to (1), which can be obtained by the first-order Magnus expansion. To get a better approximation, we must introduce commutators of and . This yields a second-order Magnus expansion: My inquiry: I would like to find an approximate SDE that satisfies. From the argument above, we know that (1) is an approximate SDE to (2). However, the SDE in (1) is a pretty crude approximation; we could call it a ""first-order"" approximate SDE. I'm interested in the ""second-order"" approximate SDE. I tried applying Ito's lemma to (2), but the problem is that I don't know how to calculate partial derivatives ( i.e. ) of . I looked at exponential map differentiation , but it does not yield the desired result because after differentiating, the exponential map is on the left-hand side of the operator.","dX = A X dW_1 + B X dW_2, \tag{1} A B \mathbb{C}^2 W_1(t), W_2(t) X_0 \in \mathbb{C}^2 A B A B Z^{(1)}(t)=\exp\left(-\frac{1}{2} \left(A^2 + B^2 \right)t + A W_1(t) + B W_2(t) \right)X_0. \tag{2} Z^{(1)}(t) A B \begin{align}
Z^{(2)}(t) = \exp \left \{ -\frac{1}{2} \left(A^2 + B^2 \right)t \\
+A W_1(t) + B W_2(t) + \frac{1}{2}[A,B] \int_0^t W_2(s) dW_1(s) + \frac{1}{2}[B,A] \int_0^t W_1(s) dW_2(s) \right \}X_0. \tag{3}
\end{align} Z^{(1)}(t) \frac{\partial}{\partial x_1}, \frac{\partial}{\partial x_2},\frac{\partial}{\partial t} \exp(-\frac{1}{2}(A^2 + B^2)t + A x_1 + B x_2 ) \exp(\cdot) \text{ad}_{\chi}","['ordinary-differential-equations', 'stochastic-processes', 'stochastic-calculus', 'stochastic-differential-equations']"
77,Solve the differential equation $ y_n^{\prime \prime}(r) + \frac{1}{r} y_n^{\prime}(r) - \frac{n^2}{r^2} y_n(r) = 0$,Solve the differential equation, y_n^{\prime \prime}(r) + \frac{1}{r} y_n^{\prime}(r) - \frac{n^2}{r^2} y_n(r) = 0,"I would like to solve the differential equation $$ y_n^{\prime \prime}(r) + \frac{1}{r} y_n^{\prime}(r) - \frac{n^2}{r^2} y_n(r) = 0, $$ I thought to start by trying a solution of the form $y_n(r) = r^m$ . Substituting $y_n(r) = r^m$ into the differential equation, we compute the first and second derivatives: $$ y_n^{\prime}(r) = m r^{m-1} $$ and $$ y_n^{\prime \prime}(r) = m (m-1) r^{m-2} $$ Now substituting $y_n(r) = r^m$ , $y_n^{\prime}(r) = m r^{m-1}$ , and $y_n^{\prime \prime}(r) = m(m-1) r^{m-2}$ into the differential equation, we get: $$ m(m-1) r^{m-2} + \frac{1}{r} (m r^{m-1}) - \frac{n^2}{r^2} r^m = 0 $$ Simplifying each term: $$ m(m-1) r^{m-2} + m r^{m-2} - n^2 r^{m-2} = 0 $$ Combining like terms: $$ [m(m-1) + m - n^2] r^{m-2} = 0 $$ Factoring out $r^{m-2}$ , we obtain the characteristic equation: $$ m(m-1) + m - n^2 = 0 $$ Simplifying the quadratic equation: $$ m^2 - m + m - n^2 = 0 $$ $$ m^2 - n^2 = 0 $$ This factors into: $$ (m-n)(m+n) = 0 $$ Thus, the solutions for $m$ are: $$ m = n \quad \text{or} \quad m = -n $$ Therefore, the general solution to the differential equation is a linear combination of the two independent solutions $r^n$ and $r^{-n}$ : $$ y_n(r) = C_1 r^n + C_2 r^{-n} $$ where $C_1$ and $C_2$ are arbitrary constants. Is my solution correct? Even if it turns out to be correct, I don't like that I arrived at this solution 'by trial and error'. Can someone solve this equation in a different way? Thanks in advance.","I would like to solve the differential equation I thought to start by trying a solution of the form . Substituting into the differential equation, we compute the first and second derivatives: and Now substituting , , and into the differential equation, we get: Simplifying each term: Combining like terms: Factoring out , we obtain the characteristic equation: Simplifying the quadratic equation: This factors into: Thus, the solutions for are: Therefore, the general solution to the differential equation is a linear combination of the two independent solutions and : where and are arbitrary constants. Is my solution correct? Even if it turns out to be correct, I don't like that I arrived at this solution 'by trial and error'. Can someone solve this equation in a different way? Thanks in advance.","
y_n^{\prime \prime}(r) + \frac{1}{r} y_n^{\prime}(r) - \frac{n^2}{r^2} y_n(r) = 0,
 y_n(r) = r^m y_n(r) = r^m 
y_n^{\prime}(r) = m r^{m-1}
 
y_n^{\prime \prime}(r) = m (m-1) r^{m-2}
 y_n(r) = r^m y_n^{\prime}(r) = m r^{m-1} y_n^{\prime \prime}(r) = m(m-1) r^{m-2} 
m(m-1) r^{m-2} + \frac{1}{r} (m r^{m-1}) - \frac{n^2}{r^2} r^m = 0
 
m(m-1) r^{m-2} + m r^{m-2} - n^2 r^{m-2} = 0
 
[m(m-1) + m - n^2] r^{m-2} = 0
 r^{m-2} 
m(m-1) + m - n^2 = 0
 
m^2 - m + m - n^2 = 0
 
m^2 - n^2 = 0
 
(m-n)(m+n) = 0
 m 
m = n \quad \text{or} \quad m = -n
 r^n r^{-n} 
y_n(r) = C_1 r^n + C_2 r^{-n}
 C_1 C_2",['ordinary-differential-equations']
78,Asymptotic expansion of a differential equation,Asymptotic expansion of a differential equation,,"I am a physicist studying a nonhomogenous differential equation of the general form \begin{equation} a(x)f^{(n)}(x) +b(x)f^{(n-1)}(x) +\dots + c(x)f(x) = \mathcal{F}(f(x),x), \tag{1} \end{equation} where $a(x)$ , $b(x)$ , $\dots$ , $c(x)$ are rational functions and $\mathcal{F}(f(x),x)$ is a complicated expression. The equation would be linear without this term.  Crucially, the explicit form of $\mathcal{F}(f(x),x)$ is not known unless one does an aymptotic expansion for $x \gg1$ , in which case one has \begin{equation} \mathcal{F}(f(x),x) = \frac{\log(x)}{x^4} + \mathcal{O}\left( \frac{1}{x^5} \right). \end{equation} Inserting this approximation into the equation makes it linear. The solutions to the differential equation are expected to go to zero at large $x$ , and so do the coefficient functions $a(x)$ , $b(x)$ , $\dots$ , $c(x)$ . However, whereas the left-hand side of $(1)$ is exact, substituting $\mathcal{F}(f(x),x) \to \log(x)/x^4$ on the right-hand side assumes an asymptotic expansion. This brings me to the question: is it possible to do the corresponding asymptotic expansion on the rest of the equation, from the known forms of $a(x)$ , $b(x)$ , $c(x)$ ? In other words, Is it possible to, from a given differential equation whose solution is regular at $x\to \infty$ , write another (hopefully simpler) equation for the asymptotic behavior of the solution? There is no dimensionless number to use as expansion parameter for perturbation theory, but only some terms of the asymptotic expansion of $f(x)$ at infinity are needed (no matching with the ""internal"" solution is required). I expect $f(x)$ to be of the form \begin{equation} f(x) = f_0+\frac{f_1(x)}{x}+\frac{f_2(x)}{x^2} + \dots \, , \hspace{1cm} \text{for $x\gg1$}, \end{equation} where the dominant dependence on $x$ of each term is given the power of $1/x$ , but the $f_i(x)$ may still have some milder dependence on $x$ . It is possible to expand each term of $(1)$ in powers of $1/x$ and look at each power separately, but I do know if that is a valid approach.","I am a physicist studying a nonhomogenous differential equation of the general form where , , , are rational functions and is a complicated expression. The equation would be linear without this term.  Crucially, the explicit form of is not known unless one does an aymptotic expansion for , in which case one has Inserting this approximation into the equation makes it linear. The solutions to the differential equation are expected to go to zero at large , and so do the coefficient functions , , , . However, whereas the left-hand side of is exact, substituting on the right-hand side assumes an asymptotic expansion. This brings me to the question: is it possible to do the corresponding asymptotic expansion on the rest of the equation, from the known forms of , , ? In other words, Is it possible to, from a given differential equation whose solution is regular at , write another (hopefully simpler) equation for the asymptotic behavior of the solution? There is no dimensionless number to use as expansion parameter for perturbation theory, but only some terms of the asymptotic expansion of at infinity are needed (no matching with the ""internal"" solution is required). I expect to be of the form where the dominant dependence on of each term is given the power of , but the may still have some milder dependence on . It is possible to expand each term of in powers of and look at each power separately, but I do know if that is a valid approach.","\begin{equation}
a(x)f^{(n)}(x) +b(x)f^{(n-1)}(x) +\dots + c(x)f(x) = \mathcal{F}(f(x),x),
\tag{1}
\end{equation} a(x) b(x) \dots c(x) \mathcal{F}(f(x),x) \mathcal{F}(f(x),x) x \gg1 \begin{equation}
\mathcal{F}(f(x),x) = \frac{\log(x)}{x^4} + \mathcal{O}\left( \frac{1}{x^5} \right).
\end{equation} x a(x) b(x) \dots c(x) (1) \mathcal{F}(f(x),x) \to \log(x)/x^4 a(x) b(x) c(x) x\to \infty f(x) f(x) \begin{equation}
f(x) = f_0+\frac{f_1(x)}{x}+\frac{f_2(x)}{x^2} + \dots \, , \hspace{1cm} \text{for x\gg1},
\end{equation} x 1/x f_i(x) x (1) 1/x","['ordinary-differential-equations', 'asymptotics']"
79,Is it possible to solve for a function of a differential in an integral equation?,Is it possible to solve for a function of a differential in an integral equation?,,"I am attempting to solve an equation for $\nu(x)$ . The exact equation is part of my PhD research and is needlessly complex for the core of my issue, I will give a simplified example instead: $\int_{R\backslash\{0\}} f(x,y) \nu(dy) = g(x)$ . Assume $f$ and $g$ are sufficiently well-behaved for this integral to work (feel free to choose specific $f$ and $g$ in your answer if you need more details to demonstrate a point) and $R$ represents the set real numbers. My idea is to somehow apply the fundamental theorem of calculus to both sides after differentiating to obtain a differential equation, but for whatever reason I am stuck on dealing with how $\nu$ would interact with the FTC part 1. This would be easy if we had something like $\nu(dx)=h(x)dx$ instead (for some function $h$ ), but I do not see how I could rewrite this equation like that without knowing $\nu$ already. For those wondering about context. I am trying to manually solve for the Levy measure of a particular stochastic process by equating the exponent of the Levy-Khintchine formula (left) to the exponent of the characteristic function (right). I have never been taught to manually solve for the Levy measure, we always just used intuition and comparison in class. So if there is any literature or formulas known to find a Levy measure directly, I will be grateful.","I am attempting to solve an equation for . The exact equation is part of my PhD research and is needlessly complex for the core of my issue, I will give a simplified example instead: . Assume and are sufficiently well-behaved for this integral to work (feel free to choose specific and in your answer if you need more details to demonstrate a point) and represents the set real numbers. My idea is to somehow apply the fundamental theorem of calculus to both sides after differentiating to obtain a differential equation, but for whatever reason I am stuck on dealing with how would interact with the FTC part 1. This would be easy if we had something like instead (for some function ), but I do not see how I could rewrite this equation like that without knowing already. For those wondering about context. I am trying to manually solve for the Levy measure of a particular stochastic process by equating the exponent of the Levy-Khintchine formula (left) to the exponent of the characteristic function (right). I have never been taught to manually solve for the Levy measure, we always just used intuition and comparison in class. So if there is any literature or formulas known to find a Levy measure directly, I will be grateful.","\nu(x) \int_{R\backslash\{0\}} f(x,y) \nu(dy) = g(x) f g f g R \nu \nu(dx)=h(x)dx h \nu","['calculus', 'integration', 'ordinary-differential-equations', 'levy-processes']"
80,How to find the inverse Laplace transform of function $s\csc(2s)$?,How to find the inverse Laplace transform of function ?,s\csc(2s),"When dealing with one ODE, it happened to find the inverse Laplace transform of function $$ F(s)=\frac{s}{\sin(2s)}. $$ I suppose it exists the inverse Laplace transform, but I could not find any standard formula from Laplace tables, either. I try to expand in Laurent series and found $$ \frac{1}{\sin(s)}={\frac {-2i{{\rm e}^{-is }}}{{{\rm e}^{-2is}}-1}}  $$ from here . But it seems incredibly difficult to proceed. Would someone has a simple way, please advise?","When dealing with one ODE, it happened to find the inverse Laplace transform of function I suppose it exists the inverse Laplace transform, but I could not find any standard formula from Laplace tables, either. I try to expand in Laurent series and found from here . But it seems incredibly difficult to proceed. Would someone has a simple way, please advise?","
F(s)=\frac{s}{\sin(2s)}.
 
\frac{1}{\sin(s)}={\frac {-2i{{\rm e}^{-is
}}}{{{\rm e}^{-2is}}-1}} 
","['functional-analysis', 'ordinary-differential-equations', 'laplace-transform', 'laurent-series']"
81,Non-autonomous system of two nonlinear ordinary differential equations with conditions,Non-autonomous system of two nonlinear ordinary differential equations with conditions,,"Consider the ODE system: $$ \frac{df}{dx}= -\sqrt{g},\tag{1} $$ $$ \frac{dg}{dx}= -\sqrt{x}f,\tag{2} $$ where $f=f\left(x\right)$ and $g=g\left(x\right)$ are the functions on the interval $x\in\left[0,1\right]$ . The following four conditions are also given: $$ f\left(0\right)=F>0,\quad g\left(0\right)=G>0,\tag{3} $$ $$ f\left(1\right)=0,\quad g\left(1\right)=0,\tag{4} $$ where $F$ and $G$ are some positive constants. I am interested in the following questions. Does there exist a continuous non-negative (i.e. $f\left(x\right)\geq0$ and $g\left(x\right)\geq0$ , $\forall x\in\left[0,1\right]$ ) non-trivial solution of the system (1) and (2) consistent with the additional conditions (3) and (4) for some positive constants $F$ and $G$ ? If so, is there any way to express the exact value of the constants $F$ and $G$ ? The second question is the most interesting for me. This issue originates from physics. Essentially, these equations are obtained from the one-dimensional diffusion problem. That is the reason why I am only interested in positive continuous solutions. An approximate numerical solution can be found. We can consider the equations (1) and (2) with initial conditions (4) as a Cauchy problem. Solving it allows us to determine the constants $F$ and $G$ from the conditions (3). Starting from the point $x=1$ , one can obtain only a trivial solution. Thus, we should start from a point close to unity: $x=1-\epsilon$ , $1\gg\epsilon>0$ , and use the asymptotics as the initial conditions: $$ f\left(1-\epsilon\right)\sim\frac{\epsilon^{3}}{36},\quad g\left(1-\epsilon\right)\sim\frac{\epsilon^{4}}{144},\qquad\epsilon\rightarrow+0. $$ The numerical solutions presented in the figures below correspond to the following constants: $$ F\simeq2.04\times10^{-2},\quad G\simeq2.35\times10^{-3}. $$","Consider the ODE system: where and are the functions on the interval . The following four conditions are also given: where and are some positive constants. I am interested in the following questions. Does there exist a continuous non-negative (i.e. and , ) non-trivial solution of the system (1) and (2) consistent with the additional conditions (3) and (4) for some positive constants and ? If so, is there any way to express the exact value of the constants and ? The second question is the most interesting for me. This issue originates from physics. Essentially, these equations are obtained from the one-dimensional diffusion problem. That is the reason why I am only interested in positive continuous solutions. An approximate numerical solution can be found. We can consider the equations (1) and (2) with initial conditions (4) as a Cauchy problem. Solving it allows us to determine the constants and from the conditions (3). Starting from the point , one can obtain only a trivial solution. Thus, we should start from a point close to unity: , , and use the asymptotics as the initial conditions: The numerical solutions presented in the figures below correspond to the following constants:","
\frac{df}{dx}= -\sqrt{g},\tag{1}
 
\frac{dg}{dx}= -\sqrt{x}f,\tag{2}
 f=f\left(x\right) g=g\left(x\right) x\in\left[0,1\right] 
f\left(0\right)=F>0,\quad g\left(0\right)=G>0,\tag{3}
 
f\left(1\right)=0,\quad g\left(1\right)=0,\tag{4}
 F G f\left(x\right)\geq0 g\left(x\right)\geq0 \forall x\in\left[0,1\right] F G F G F G x=1 x=1-\epsilon 1\gg\epsilon>0 
f\left(1-\epsilon\right)\sim\frac{\epsilon^{3}}{36},\quad g\left(1-\epsilon\right)\sim\frac{\epsilon^{4}}{144},\qquad\epsilon\rightarrow+0.
 
F\simeq2.04\times10^{-2},\quad G\simeq2.35\times10^{-3}.
","['ordinary-differential-equations', 'systems-of-equations', 'mathematical-physics', 'nonlinear-system', 'cauchy-problem']"
82,The distribution of the first hitting time for the Constant Elasticity of Variance process.,The distribution of the first hitting time for the Constant Elasticity of Variance process.,,"The Constant Elasticity of Variance (CEV) process is a one dimensional diffusion process given by the following stochastic differential equation. \begin{equation} d X_t = \mu X_t \cdot dt + \sigma X_t^\beta \cdot d B_t \tag{1} \end{equation} where $\mu,\sigma,\beta$ are positive real parameters and $B_t$ is the Brownian motion. In what follows we assume that the starting value of the process reads $X_0 = x > 0$ and that $\beta > 1 $ . The infinitesimal generator of this process reads $ {\mathfrak G}_z := \mu z d/d z + \sigma^2/2 z^{2 \beta} d^2/d z^2$ . The eigen-functions of this operator $ {\mathfrak G}_z \phi^{\pm} (z) = \lambda \phi^{\pm}(z) $ to the eigenvalue $\lambda > 0 $ are given below: \begin{eqnarray} \phi^{+}(z) &=&U\left(\frac{\lambda}{2(-1+\beta) \mu}, 1+ \frac{1}{2(-1+\beta)}, \frac{\mu z^{2-2\beta} }{(-1+\beta) \sigma^2} \right) \tag{2a} \\ \phi^{-}(z) &=& L_{-\frac{\lambda}{2(-1+\beta) \mu}}^{(\frac{1}{2(-1+\beta)})} \left( \frac{\mu z^{2-2\beta} }{(-1+\beta) \sigma^2}\right) \tag{2b}  \end{eqnarray} where $U$ is the confluent hypergeometric function and $L$ is the generalized Laguerre polynomial. We have checked that $U(z)$ is a strictly increasing function of $z$ . Now, by using the theory of diffusion processes, see section 4.6 pages 128-134 in Ito, K.; McKean, H. P. jun. , Diffusion processes and their sample paths, Berlin-Heidelberg-New York: Springer-Verlag. XVII, 321 p. (1965). ZBL0127.09503 . , we have found the Laplace transform of the first hitting time $\tau_y := inf(s>0:X_s = y)$ of a horizontal barrier $b$ by this process. The quantity in question reads: \begin{eqnarray} E_x \left[ e^{-\lambda \tau_y} \right] = \frac{\phi^{+}(x)}{\phi^{+}(y)} \quad \mbox{for $x \le y$} \tag{3} \end{eqnarray} Now, by inverting the Laplace transform in $(3)$ , by using the Bromwich integral and then the Cauchy theorem, we have expressed the probability density function of the first hitting time $n_x(t;y) := P_x\left( \tau_y \in dt\right)/dt $ as follows: \begin{eqnarray} n_x(t;y)  = \sum\limits_{p=1}^\infty  \underbrace{ \frac{U\left( -\zeta_p^{(y;\mu,\sigma,\beta)},               1+ \frac{1}{2(-1+\beta)}, \frac{\mu x^{2-2\beta}}{(-1+\beta) \sigma^2}  \right)}{ \zeta_p^{(y;\mu,\sigma,\beta)} U^{(1,0,0)}\left( -\zeta_p^{(y;\mu,\sigma,\beta)},               1+ \frac{1}{2(-1+\beta)}, \frac{\mu y^{2-2\beta}}{(-1+\beta) \sigma^2}  \right) } }_{{\mathfrak w}_p^{(y;\mu,\sigma,\beta)}} \cdot \underline{(2 (-1+\beta) \mu \zeta_p^{(y;\mu,\sigma,\beta)} ) \cdot e^{-2 (-1+\beta) \mu \cdot \zeta_p^{(y;\mu,\sigma,\beta)} \cdot t}} \tag{5} \end{eqnarray} As we can see the quantity in $(5)$ is an infinite linear combination of exponential distributions with weights $ \left( {\mathfrak w}_p^{(y;\mu,\sigma,\beta)} \right)_{p=1}^\infty $ that sum up to unity. Here $ \left( \zeta_p^{(y;\mu,\sigma,\beta)} \right)_{p=1}^\infty $ are zeros of the function $ {\mathbb R}_+ \ni \lambda \rightarrow U(-\lambda,1+ \frac{1}{2(-1+\beta)}, \frac{\mu y^{2-2\beta}}{(-1+\beta) \sigma^2}) \in {\mathbb R} $ . Now we took the following process parameters $\mu,\sigma,\beta = 3/2,1/2,5/2$ and the first value and the barrier $x,y = 3/2, 5/2 $ and we plotted the quantity $(5)$ below. We also verified the normalization numerically. Here we go: {\[Mu], \[Sigma], \[Beta]} = {3/2, 1/2, 5/2}; (*Here x\[LessEqual]y*) {x, y} = {3/2, 5/2};  SetOptions[FindRoot, WorkingPrecision -> mprec, PrecisionGoal -> prec]; mzeros = \[Lambda] /.     Table[FindRoot[      HypergeometricU[-\[Lambda],         1 + 1/(2 (-1 + \[Beta])), (\[Mu] y^(         2 - 2 \[Beta]))/((-1 + \[Beta]) \[Sigma]^2)] == 0, {\[Lambda],        n}], {n, 0, 50}]; mzeros = Sort[#[[1]] & /@ Tally[mzeros]];  ts = Array[# &, {300}, {1/100, 3}]; vals = {#,       Total[Table[         HypergeometricU[-mzeros[[p]],           1 + 1/(2 (-1 + \[Beta])), (\[Mu] x^(           2 - 2 \[Beta]))/((-1 + \[Beta]) \[Sigma]^2)]/ \!\(\*SuperscriptBox[\(HypergeometricU\),  TagBox[ RowBox[{""("",  RowBox[{""1"", "","", ""0"", "","", ""0""}], "")""}], Derivative], MultilineFunction->None]\)[-mzeros[[p]],           1 + 1/(2 (-1 + \[Beta])), (\[Mu] y^(           2 - 2 \[Beta]))/((-1 + \[Beta]) \[Sigma]^2)] (2 (-1 + \ \[Beta]) \[Mu]) Exp[-2 (-1 + \[Beta]) \[Mu] mzeros[[p]] #], {p, 1,          Length[mzeros]}]]} & /@ ts; ListPlot[vals, PlotRange :> All,   AxesLabel -> {""t"", ""\!\(\*SubscriptBox[\(n\), \(x\)]\)(t,y)""}] f = Interpolation[vals]; NIntegrate[f[xi], {xi, 0.01, 3}] As you can see the distribution in question has a correct shape and a correct normalization. The negative values close to the origin are due to a numerical error. Having said all this my question would be how do we evaluate the limit of $\beta \rightarrow 1_+$ . In this case the process tends towards the geometric Brownian motion and as such we should have: \begin{equation} \lim_{\beta \rightarrow 1_+} n_x(t;y) \stackrel{(??)}{=}  \frac{\left| \log(\frac{y}{x} )\right|}{\sqrt{2 \pi t^3} \sigma} e^{-\frac{1}{2 \sigma^2 t} \left[ \log(\frac{y}{x} - (\mu - \frac{\sigma^2}{2} ) t\right]^2}  \end{equation} as shown in a previous question on a similar topic . How do we work out this limit analytically in our framework? Update: We have verified numerically that the Laplace transform $ (3) $ approaches the correct limits when $ \beta \rightarrow 1_+ $ . See code below: {lmb, mu, sig} = RandomReal[{0, 1}, 3, WorkingPrecision -> 50];     x = RandomReal[{0, 1}, WorkingPrecision -> 50];     y = RandomReal[{x, 2}, WorkingPrecision -> 50];     NN = 100;     HypergeometricU[lmb/mu NN, NN, NN (2 mu/sig^2) x^(-1/NN)]/      HypergeometricU[lmb/mu NN, NN, NN (2 mu/sig^2) y^(-1/NN)]     (x/y)^((-2 mu + sig^2 + 2 Sqrt[2 lmb sig^2 + (mu - sig^2/2)^2])/(       2 sig^2))  0.275584165705622319278498109409236772998453 + 0.*10^-50 I  0.2729011283963510407220223125184479965801097988605","The Constant Elasticity of Variance (CEV) process is a one dimensional diffusion process given by the following stochastic differential equation. where are positive real parameters and is the Brownian motion. In what follows we assume that the starting value of the process reads and that . The infinitesimal generator of this process reads . The eigen-functions of this operator to the eigenvalue are given below: where is the confluent hypergeometric function and is the generalized Laguerre polynomial. We have checked that is a strictly increasing function of . Now, by using the theory of diffusion processes, see section 4.6 pages 128-134 in Ito, K.; McKean, H. P. jun. , Diffusion processes and their sample paths, Berlin-Heidelberg-New York: Springer-Verlag. XVII, 321 p. (1965). ZBL0127.09503 . , we have found the Laplace transform of the first hitting time of a horizontal barrier by this process. The quantity in question reads: Now, by inverting the Laplace transform in , by using the Bromwich integral and then the Cauchy theorem, we have expressed the probability density function of the first hitting time as follows: As we can see the quantity in is an infinite linear combination of exponential distributions with weights that sum up to unity. Here are zeros of the function . Now we took the following process parameters and the first value and the barrier and we plotted the quantity below. We also verified the normalization numerically. Here we go: {\[Mu], \[Sigma], \[Beta]} = {3/2, 1/2, 5/2}; (*Here x\[LessEqual]y*) {x, y} = {3/2, 5/2};  SetOptions[FindRoot, WorkingPrecision -> mprec, PrecisionGoal -> prec]; mzeros = \[Lambda] /.     Table[FindRoot[      HypergeometricU[-\[Lambda],         1 + 1/(2 (-1 + \[Beta])), (\[Mu] y^(         2 - 2 \[Beta]))/((-1 + \[Beta]) \[Sigma]^2)] == 0, {\[Lambda],        n}], {n, 0, 50}]; mzeros = Sort[#[[1]] & /@ Tally[mzeros]];  ts = Array[# &, {300}, {1/100, 3}]; vals = {#,       Total[Table[         HypergeometricU[-mzeros[[p]],           1 + 1/(2 (-1 + \[Beta])), (\[Mu] x^(           2 - 2 \[Beta]))/((-1 + \[Beta]) \[Sigma]^2)]/ \!\(\*SuperscriptBox[\(HypergeometricU\),  TagBox[ RowBox[{""("",  RowBox[{""1"", "","", ""0"", "","", ""0""}], "")""}], Derivative], MultilineFunction->None]\)[-mzeros[[p]],           1 + 1/(2 (-1 + \[Beta])), (\[Mu] y^(           2 - 2 \[Beta]))/((-1 + \[Beta]) \[Sigma]^2)] (2 (-1 + \ \[Beta]) \[Mu]) Exp[-2 (-1 + \[Beta]) \[Mu] mzeros[[p]] #], {p, 1,          Length[mzeros]}]]} & /@ ts; ListPlot[vals, PlotRange :> All,   AxesLabel -> {""t"", ""\!\(\*SubscriptBox[\(n\), \(x\)]\)(t,y)""}] f = Interpolation[vals]; NIntegrate[f[xi], {xi, 0.01, 3}] As you can see the distribution in question has a correct shape and a correct normalization. The negative values close to the origin are due to a numerical error. Having said all this my question would be how do we evaluate the limit of . In this case the process tends towards the geometric Brownian motion and as such we should have: as shown in a previous question on a similar topic . How do we work out this limit analytically in our framework? Update: We have verified numerically that the Laplace transform approaches the correct limits when . See code below: {lmb, mu, sig} = RandomReal[{0, 1}, 3, WorkingPrecision -> 50];     x = RandomReal[{0, 1}, WorkingPrecision -> 50];     y = RandomReal[{x, 2}, WorkingPrecision -> 50];     NN = 100;     HypergeometricU[lmb/mu NN, NN, NN (2 mu/sig^2) x^(-1/NN)]/      HypergeometricU[lmb/mu NN, NN, NN (2 mu/sig^2) y^(-1/NN)]     (x/y)^((-2 mu + sig^2 + 2 Sqrt[2 lmb sig^2 + (mu - sig^2/2)^2])/(       2 sig^2))  0.275584165705622319278498109409236772998453 + 0.*10^-50 I  0.2729011283963510407220223125184479965801097988605","\begin{equation}
d X_t = \mu X_t \cdot dt + \sigma X_t^\beta \cdot d B_t \tag{1}
\end{equation} \mu,\sigma,\beta B_t X_0 = x > 0 \beta > 1   {\mathfrak G}_z := \mu z d/d z + \sigma^2/2 z^{2 \beta} d^2/d z^2  {\mathfrak G}_z \phi^{\pm} (z) = \lambda \phi^{\pm}(z)  \lambda > 0  \begin{eqnarray}
\phi^{+}(z) &=&U\left(\frac{\lambda}{2(-1+\beta) \mu}, 1+ \frac{1}{2(-1+\beta)}, \frac{\mu z^{2-2\beta} }{(-1+\beta) \sigma^2} \right) \tag{2a} \\
\phi^{-}(z) &=& L_{-\frac{\lambda}{2(-1+\beta) \mu}}^{(\frac{1}{2(-1+\beta)})} \left( \frac{\mu z^{2-2\beta} }{(-1+\beta) \sigma^2}\right) \tag{2b} 
\end{eqnarray} U L U(z) z \tau_y := inf(s>0:X_s = y) b \begin{eqnarray}
E_x \left[ e^{-\lambda \tau_y} \right] = \frac{\phi^{+}(x)}{\phi^{+}(y)} \quad \mbox{for x \le y} \tag{3}
\end{eqnarray} (3) n_x(t;y) := P_x\left( \tau_y \in dt\right)/dt  \begin{eqnarray}
n_x(t;y)  = \sum\limits_{p=1}^\infty 
\underbrace{
\frac{U\left( -\zeta_p^{(y;\mu,\sigma,\beta)},
              1+ \frac{1}{2(-1+\beta)}, \frac{\mu x^{2-2\beta}}{(-1+\beta) \sigma^2}  \right)}{
\zeta_p^{(y;\mu,\sigma,\beta)}
U^{(1,0,0)}\left( -\zeta_p^{(y;\mu,\sigma,\beta)},
              1+ \frac{1}{2(-1+\beta)}, \frac{\mu y^{2-2\beta}}{(-1+\beta) \sigma^2}  \right)
}
}_{{\mathfrak w}_p^{(y;\mu,\sigma,\beta)}}
\cdot
\underline{(2 (-1+\beta) \mu \zeta_p^{(y;\mu,\sigma,\beta)} ) \cdot e^{-2 (-1+\beta) \mu \cdot \zeta_p^{(y;\mu,\sigma,\beta)} \cdot t}} \tag{5}
\end{eqnarray} (5)  \left( {\mathfrak w}_p^{(y;\mu,\sigma,\beta)} \right)_{p=1}^\infty   \left( \zeta_p^{(y;\mu,\sigma,\beta)} \right)_{p=1}^\infty   {\mathbb R}_+ \ni \lambda \rightarrow U(-\lambda,1+ \frac{1}{2(-1+\beta)}, \frac{\mu y^{2-2\beta}}{(-1+\beta) \sigma^2}) \in {\mathbb R}  \mu,\sigma,\beta = 3/2,1/2,5/2 x,y = 3/2, 5/2  (5) \beta \rightarrow 1_+ \begin{equation}
\lim_{\beta \rightarrow 1_+} n_x(t;y) \stackrel{(??)}{=} 
\frac{\left| \log(\frac{y}{x} )\right|}{\sqrt{2 \pi t^3} \sigma} e^{-\frac{1}{2 \sigma^2 t} \left[ \log(\frac{y}{x} - (\mu - \frac{\sigma^2}{2} ) t\right]^2} 
\end{equation}  (3)   \beta \rightarrow 1_+ ","['ordinary-differential-equations', 'stochastic-processes', 'stopping-times', 'hypergeometric-function', 'inverse-laplace']"
83,Numerical approach of integral equation,Numerical approach of integral equation,,"If I have a equation $$\int_0^{t} \int_{s(\tau)}^D f(x, \tau) d x d \tau=0$$ and I want to solve $s(t)$ for any given $t$ . Here $D$ is a constant and $f(x,\tau)$ is a known function, is there a numerical method for solving $s(t)$ ? I was thinking a method like Runge–Kutta method for ODE, but I'm not sure. Any help is appreciated.","If I have a equation and I want to solve for any given . Here is a constant and is a known function, is there a numerical method for solving ? I was thinking a method like Runge–Kutta method for ODE, but I'm not sure. Any help is appreciated.","\int_0^{t} \int_{s(\tau)}^D f(x, \tau) d x d \tau=0 s(t) t D f(x,\tau) s(t)","['integration', 'ordinary-differential-equations', 'numerical-methods']"
84,References for a statement on linear systems,References for a statement on linear systems,,"Suppose we have a linear system $\dot{x} = A x$ , where $x \in \mathbb{R}^n$ and $A$ is a constant matrix, such that all the trajectories of the system are bounded and bounded away from the origin (i.e., none of them converge to the origin). This I believe implies that all the eigenvalues of $A$ have 0 real parts and that $A$ is diagonalizable. I also believe that for this type of system, there exists a symmetric matrix $P$ such that $x^t P x$ is constant along the trajectories. I am interested in this as I use it in my research. I have a proof of it, though it seems to be a rather simple result, and I would prefer to simply cite a reference for it. Does anyone know of any references to these facts? (e.g., a book or paper that states/proves these statements)","Suppose we have a linear system , where and is a constant matrix, such that all the trajectories of the system are bounded and bounded away from the origin (i.e., none of them converge to the origin). This I believe implies that all the eigenvalues of have 0 real parts and that is diagonalizable. I also believe that for this type of system, there exists a symmetric matrix such that is constant along the trajectories. I am interested in this as I use it in my research. I have a proof of it, though it seems to be a rather simple result, and I would prefer to simply cite a reference for it. Does anyone know of any references to these facts? (e.g., a book or paper that states/proves these statements)",\dot{x} = A x x \in \mathbb{R}^n A A A P x^t P x,"['linear-algebra', 'ordinary-differential-equations', 'reference-request']"
85,What are the applications of stochastic differential equations to number theory?,What are the applications of stochastic differential equations to number theory?,,"This semester i'm taking a course about stochastic differential equations. This made me wonder what applications does this topic have to areas like number theory and algebraic geometry, specially arithmetic geometry. Unfortunately i wasn't able to find anything online, all i found was about applications to usual differential equations to number theory in this question.","This semester i'm taking a course about stochastic differential equations. This made me wonder what applications does this topic have to areas like number theory and algebraic geometry, specially arithmetic geometry. Unfortunately i wasn't able to find anything online, all i found was about applications to usual differential equations to number theory in this question.",,"['ordinary-differential-equations', 'number-theory', 'partial-differential-equations', 'reference-request', 'stochastic-differential-equations']"
86,Ruling out finite time explosions to infinity of Riccati differential equations,Ruling out finite time explosions to infinity of Riccati differential equations,,"Let $\xi^*,\varsigma,\eta:\mathbb{R}\rightarrow (0,\infty)$ . You may assume any degree of smoothness required of these functions. Assume that $\xi^{*}(t)$ and $\varsigma(t)$ have finite positive limits as $t\rightarrow\infty$ and as $t\rightarrow -\infty$ . Define $\delta:\mathbb{R}\rightarrow\mathbb{R}$ by: $$\delta(t)=\frac{\dot{\xi}^{*}(t)}{\xi^{*}(t)}-\varsigma(t),$$ for all $t\in\mathbb{R}$ . Finally, assume that for all $\tau\in\mathbb{R}$ , $\xi_{\tau}:[\tau,\infty)\rightarrow [0,\infty)$ satisfies: $$\xi_{\tau}(\tau)=\varsigma(\tau),$$ and: $$\dot{\xi}_{\tau}(t)={\xi_{\tau}(t)}^2+\left(\delta(t)-\eta(t) {\delta(t)}^2 \right) \xi_{\tau}(t)+\eta(t) {\delta(t)}^2 \xi^{*}(t).$$ In particular, I need that $\xi_{\tau}(t)<\infty$ for all $t\ge\tau$ , $\tau\in\mathbb{R}$ , so $\xi_{\tau}$ does not explode to infinity in finite time for any $\tau$ . This will hold for example, if $\eta(t)=0$ and $\varsigma(t)=\xi^{*}(t)$ for all $t\in\mathbb{R}$ , in which case a solution is $\xi_{\tau}(t)=\xi^{*}(t)$ . I.e., if we restrict BOTH $\varsigma$ and $\eta$ then we can definitely ensure that $\xi_{\tau}$ does not explode to infinity in finite time. My question is the following: Can you ensure that $\xi_{\tau}$ does not explode to infinity in finite time (for all $\tau$ ) by only restricting ONE of $\varsigma$ and $\eta$ , not both? (You may nonetheless impose restrictions on the limits of $\varsigma(t)$ as $t\rightarrow\pm\infty$ if necessary, even if you are restricting $\eta$ not $\varsigma$ .) Constant coefficient case In the case in which $\xi^*$ , $\varsigma$ and $\eta$ are all constant over time, Maple gives the solution as: $$2\xi_{\tau}(t)=\varsigma(\eta\varsigma+1)-\omega\tanh{\left[(t-\tau)\frac{\omega}{2}+\operatorname{arctanh}{\left(\frac{\varsigma(\eta\varsigma-1)}{\omega}\right)}\right]},$$ where $\omega = \varsigma\sqrt{\eta^2\varsigma^2+2\eta(\varsigma-2\xi^{*})+1}$ . So, in this case, for the solution to be bounded, we need $\varsigma>\max{\left\{\frac{1}{\eta},\frac{2\sqrt{\eta\xi^{*}}-1}{\eta}\right\}}$ OR $\varsigma\in\left(\xi^{*},\frac{1}{\eta}\right)$ so at least in this case, we only need to restrict $\varsigma$ , not $\eta$ . Another special case Suppose: $$\eta(t)=\frac{\delta(t)+2\xi^{*}(t)+2\sqrt{\xi^{*}(t){\left[\xi^{*}(t)+\delta(t)\right]}}}{{\delta(t)}^2},$$ then the two roots of the right hand side of the ODE are identical, meaning that $\dot{\xi}_{\tau}(t)\ge 0$ , and making the algebra much neater. This is positive and real, as required, as long as $\xi^{*}(t)+\delta(t)\ge 0$ , i.e. $\varsigma(t)\le \xi^{*}(t)+\frac{\dot{\xi}^{*}(t)}{\xi^{*}(t)}$ . With this particular $\eta(t)$ , the root of the right hand side of the ODE at $t$ is given by: $$r(t)=\xi^{*}(t)+\sqrt{\xi^{*}(t){\left[\xi^{*}(t)+\delta(t)\right]}}.$$ Using the results from Kilicaslan and Banks (2010) , we have that the system does not explode to infinity if and only if for all $t\in\mathbb{R}$ , $r(t)\ge\varsigma(t)$ , for which it is sufficient that $\varsigma(t)\le\xi^{*}(t)$ and $\varsigma(t)\le \xi^{*}(t)+\frac{\dot{\xi}^{*}(t)}{\xi^{*}(t)}$ (as already assumed). The formulas given in Kilicaslan and Banks (2010) imply that in this case, the solution is given by: $$\xi_{\tau}(t)=r(\infty)-\frac{r(\tau)-\varsigma(\tau)}{1+(t-\tau)\left[r(\tau)-\varsigma(\tau)\right]},$$ where $r(\infty):=\lim_{t\rightarrow\infty}{r(t)}$ , however, this does not appear to solve the original ODE, so something must be wrong (perhaps in the paper??). Yet another special case Suppose: $$\eta(t)=\frac{\bar{\xi}\left[\bar{\xi}+\delta(t)\right]}{{\delta(t)}^2\left[\bar{\xi}-\xi^{*}(t)\right]},$$ where $\bar{\xi}$ is a constant with $\bar{\xi}+\delta(t)>0$ and $\bar{\xi}-\xi^{*}(t)>0$ for all $t\in\mathbb{R}$ . Define: $$r(t)=\frac{\xi^{*}(t)\left[\bar{\xi}+\delta(t)\right]}{\bar{\xi}-\xi^{*}(t)}>0,$$ then the original ODE is: $$\dot{\xi}_{\tau}(t)=\left[\xi_{\tau}(t)-\bar{\xi}\right]\left[\xi_{\tau}(t)-r(t)\right].$$ For this not to explode to infinity in finite time, it is sufficient that $\xi_{\tau}(\tau)=\varsigma(\tau)<\bar{\xi}$ , so we need $\varsigma(t)<\bar{\xi}$ for all $t\in\mathbb{R}$ .","Let . You may assume any degree of smoothness required of these functions. Assume that and have finite positive limits as and as . Define by: for all . Finally, assume that for all , satisfies: and: In particular, I need that for all , , so does not explode to infinity in finite time for any . This will hold for example, if and for all , in which case a solution is . I.e., if we restrict BOTH and then we can definitely ensure that does not explode to infinity in finite time. My question is the following: Can you ensure that does not explode to infinity in finite time (for all ) by only restricting ONE of and , not both? (You may nonetheless impose restrictions on the limits of as if necessary, even if you are restricting not .) Constant coefficient case In the case in which , and are all constant over time, Maple gives the solution as: where . So, in this case, for the solution to be bounded, we need OR so at least in this case, we only need to restrict , not . Another special case Suppose: then the two roots of the right hand side of the ODE are identical, meaning that , and making the algebra much neater. This is positive and real, as required, as long as , i.e. . With this particular , the root of the right hand side of the ODE at is given by: Using the results from Kilicaslan and Banks (2010) , we have that the system does not explode to infinity if and only if for all , , for which it is sufficient that and (as already assumed). The formulas given in Kilicaslan and Banks (2010) imply that in this case, the solution is given by: where , however, this does not appear to solve the original ODE, so something must be wrong (perhaps in the paper??). Yet another special case Suppose: where is a constant with and for all . Define: then the original ODE is: For this not to explode to infinity in finite time, it is sufficient that , so we need for all .","\xi^*,\varsigma,\eta:\mathbb{R}\rightarrow (0,\infty) \xi^{*}(t) \varsigma(t) t\rightarrow\infty t\rightarrow -\infty \delta:\mathbb{R}\rightarrow\mathbb{R} \delta(t)=\frac{\dot{\xi}^{*}(t)}{\xi^{*}(t)}-\varsigma(t), t\in\mathbb{R} \tau\in\mathbb{R} \xi_{\tau}:[\tau,\infty)\rightarrow [0,\infty) \xi_{\tau}(\tau)=\varsigma(\tau), \dot{\xi}_{\tau}(t)={\xi_{\tau}(t)}^2+\left(\delta(t)-\eta(t) {\delta(t)}^2 \right) \xi_{\tau}(t)+\eta(t) {\delta(t)}^2 \xi^{*}(t). \xi_{\tau}(t)<\infty t\ge\tau \tau\in\mathbb{R} \xi_{\tau} \tau \eta(t)=0 \varsigma(t)=\xi^{*}(t) t\in\mathbb{R} \xi_{\tau}(t)=\xi^{*}(t) \varsigma \eta \xi_{\tau} \xi_{\tau} \tau \varsigma \eta \varsigma(t) t\rightarrow\pm\infty \eta \varsigma \xi^* \varsigma \eta 2\xi_{\tau}(t)=\varsigma(\eta\varsigma+1)-\omega\tanh{\left[(t-\tau)\frac{\omega}{2}+\operatorname{arctanh}{\left(\frac{\varsigma(\eta\varsigma-1)}{\omega}\right)}\right]}, \omega = \varsigma\sqrt{\eta^2\varsigma^2+2\eta(\varsigma-2\xi^{*})+1} \varsigma>\max{\left\{\frac{1}{\eta},\frac{2\sqrt{\eta\xi^{*}}-1}{\eta}\right\}} \varsigma\in\left(\xi^{*},\frac{1}{\eta}\right) \varsigma \eta \eta(t)=\frac{\delta(t)+2\xi^{*}(t)+2\sqrt{\xi^{*}(t){\left[\xi^{*}(t)+\delta(t)\right]}}}{{\delta(t)}^2}, \dot{\xi}_{\tau}(t)\ge 0 \xi^{*}(t)+\delta(t)\ge 0 \varsigma(t)\le \xi^{*}(t)+\frac{\dot{\xi}^{*}(t)}{\xi^{*}(t)} \eta(t) t r(t)=\xi^{*}(t)+\sqrt{\xi^{*}(t){\left[\xi^{*}(t)+\delta(t)\right]}}. t\in\mathbb{R} r(t)\ge\varsigma(t) \varsigma(t)\le\xi^{*}(t) \varsigma(t)\le \xi^{*}(t)+\frac{\dot{\xi}^{*}(t)}{\xi^{*}(t)} \xi_{\tau}(t)=r(\infty)-\frac{r(\tau)-\varsigma(\tau)}{1+(t-\tau)\left[r(\tau)-\varsigma(\tau)\right]}, r(\infty):=\lim_{t\rightarrow\infty}{r(t)} \eta(t)=\frac{\bar{\xi}\left[\bar{\xi}+\delta(t)\right]}{{\delta(t)}^2\left[\bar{\xi}-\xi^{*}(t)\right]}, \bar{\xi} \bar{\xi}+\delta(t)>0 \bar{\xi}-\xi^{*}(t)>0 t\in\mathbb{R} r(t)=\frac{\xi^{*}(t)\left[\bar{\xi}+\delta(t)\right]}{\bar{\xi}-\xi^{*}(t)}>0, \dot{\xi}_{\tau}(t)=\left[\xi_{\tau}(t)-\bar{\xi}\right]\left[\xi_{\tau}(t)-r(t)\right]. \xi_{\tau}(\tau)=\varsigma(\tau)<\bar{\xi} \varsigma(t)<\bar{\xi} t\in\mathbb{R}","['ordinary-differential-equations', 'quadratics', 'stability-theory']"
87,what more can I say using Sturm's comparison theorem?,what more can I say using Sturm's comparison theorem?,,"Can I use Sturm's comparison theorem to say something about the average? Let $f(t)$ be a continuous function such that $\lim_{t \to \infty } f(t) = \infty$ . Let us consider the following function $$ \ddot{x}(t) + t^2 x(t) = 0. $$ Let $C$ be a constant. Then, we can find $T$ such that $C^2 < t^2$ for all $t > T$ . By Sturm's comparison theorem, exist $t^*$ in $(\frac{2\pi k}{C},\frac{2\pi (k+1)}{C})$ such that $x(t^*) = 0$ for every $k$ big enough. Can we concluded the following? $$ \lim_{t \to \infty} x(t) =0 $$ or $$ \lim_{t \to \infty} \frac{1}{t}\int_0^t s x(s) ds =0 $$","Can I use Sturm's comparison theorem to say something about the average? Let be a continuous function such that . Let us consider the following function Let be a constant. Then, we can find such that for all . By Sturm's comparison theorem, exist in such that for every big enough. Can we concluded the following? or","f(t) \lim_{t \to \infty } f(t) = \infty  \ddot{x}(t) + t^2 x(t) = 0.  C T C^2 < t^2 t > T t^* (\frac{2\pi k}{C},\frac{2\pi (k+1)}{C}) x(t^*) = 0 k  \lim_{t \to \infty} x(t) =0   \lim_{t \to \infty} \frac{1}{t}\int_0^t s x(s) ds =0 ","['ordinary-differential-equations', 'average', 'sturm-liouville']"
88,Gradient estimates of linear elliptic PDE,Gradient estimates of linear elliptic PDE,,"Let $\Omega \subset \mathbb{R}^n$ be a bounded smooth domain. Assume that $u(x)$ is the classical solution solving $$a_{ij}(x)\partial_{ij}u(x)+b_i(x)\partial_iu(x)+c(x)u(x)=f(x)$$ $$u(x)\Big|_{\partial \Omega}=g(x)$$ for some smooth enough coefficients and uniformly elliptic $a_{ij}$ . I found that in Gilbarg and Trudinger's PDE book, Theorem 8.33 states that $$ |u|_{1,\alpha,\Omega'}\leq C(|u|_{0}+|g|_{0}+|f|_{0,\alpha}) $$ for $\Omega' \subset \Omega$ and some constant $C$ depending on the $C^{0,\alpha}$ -norms of $a_{ij}$ , $b_i$ and $c$ (and some other parameters). Now, I am looking for a upper bound of $|\nabla u|$ on $\Omega'$ , not the holder norm, can this upper be something like $$ \sup_{\Omega'}|u|\leq C(|u|_{0}+|g|_{0}+|f|_{0}) $$ for some constant $C$ depending on the $C^{0}$ -norms of $a_{ij}$ , $b_i$ and $c$ , but not their holder norms? If this is to complicated, we can deal with the ODE case or equation in divergence form.","Let be a bounded smooth domain. Assume that is the classical solution solving for some smooth enough coefficients and uniformly elliptic . I found that in Gilbarg and Trudinger's PDE book, Theorem 8.33 states that for and some constant depending on the -norms of , and (and some other parameters). Now, I am looking for a upper bound of on , not the holder norm, can this upper be something like for some constant depending on the -norms of , and , but not their holder norms? If this is to complicated, we can deal with the ODE case or equation in divergence form.","\Omega \subset \mathbb{R}^n u(x) a_{ij}(x)\partial_{ij}u(x)+b_i(x)\partial_iu(x)+c(x)u(x)=f(x) u(x)\Big|_{\partial \Omega}=g(x) a_{ij}  |u|_{1,\alpha,\Omega'}\leq C(|u|_{0}+|g|_{0}+|f|_{0,\alpha})  \Omega' \subset \Omega C C^{0,\alpha} a_{ij} b_i c |\nabla u| \Omega'  \sup_{\Omega'}|u|\leq C(|u|_{0}+|g|_{0}+|f|_{0})  C C^{0} a_{ij} b_i c","['ordinary-differential-equations', 'partial-differential-equations', 'reference-request', 'elliptic-equations']"
89,"Is there a theory of ""quadratic"" Hamiltonian evolutions on Poisson manifolds?","Is there a theory of ""quadratic"" Hamiltonian evolutions on Poisson manifolds?",,"I am dealing with a PDE which can be written in the form $$\frac{d}{dt} f(t) = \{a, f(t)\} + \{\{b, f(t)\}, f(t)\}$$ A Hamiltonian equation on a Poisson manifold has the following form: $$\frac{d}{dt} f(t) = \{a, f(t)\}$$ My equation seems to be a sort of ""quadratic"" Hamiltonian evolution. I hope that there exists some theory (existence, uniqueness, etc.) of such equations on Poisson manifolds which leverages the present algebraic structure. I suppose even more generally one could study equations of the form $$\frac{d}{dt} f(t) = \sum_{k=0}^n \{ \dots \{a_k, \underset{k}{\underbrace{f\}, \dots, f\}}}$$ . If one says that the Hamiltonian evolution is the simplest possible ODE one could study on a Poisson manifold, then clearly these equations come right after that in terms of simplicity and naturality.","I am dealing with a PDE which can be written in the form A Hamiltonian equation on a Poisson manifold has the following form: My equation seems to be a sort of ""quadratic"" Hamiltonian evolution. I hope that there exists some theory (existence, uniqueness, etc.) of such equations on Poisson manifolds which leverages the present algebraic structure. I suppose even more generally one could study equations of the form . If one says that the Hamiltonian evolution is the simplest possible ODE one could study on a Poisson manifold, then clearly these equations come right after that in terms of simplicity and naturality.","\frac{d}{dt} f(t) = \{a, f(t)\} + \{\{b, f(t)\}, f(t)\} \frac{d}{dt} f(t) = \{a, f(t)\} \frac{d}{dt} f(t) = \sum_{k=0}^n \{ \dots \{a_k, \underset{k}{\underbrace{f\}, \dots, f\}}}","['ordinary-differential-equations', 'partial-differential-equations', 'smooth-manifolds', 'hamilton-equations', 'poisson-geometry']"
90,Recurrence differential equations arising from the Normal PDF,Recurrence differential equations arising from the Normal PDF,,"Let $a,b,c:\mathbb{R}\rightarrow\mathbb{R}$ be differentiable functions, with $a(t)\rightarrow -\infty$ and $c(t)\rightarrow -\infty$ as $t\rightarrow -\infty$ , and with $a(t)\rightarrow\infty$ and $c(t)\rightarrow\infty$ as $t\rightarrow\infty$ . Let $\phi:\mathbb{R}\rightarrow\mathbb{R}^+$ be the density of a standard normal random variable, meaning that for all $z\in\mathbb{R}$ : $$\phi(z)=\frac{1}{\sqrt{2\pi}} \exp{\left(-\frac{z^2}{2}\right)}.$$ For $j\in\mathbb{N}$ , let $z_j:\mathbb{R}\rightarrow\mathbb{R}$ with: $$z_j(t)=\int_{-\infty}^t{\exp{\left(a(\tau)-a(t)\right)}\,\left(b(\tau)+c(t)-c(\tau)\right)^j\,\phi\left(b(\tau)+c(t)-c(\tau)\right)\,d\tau},$$ for all $t\in\mathbb{R}$ , where you may assume the integral exists and is finite for all $j\in\mathbb{N}$ and $t\in\mathbb{R}$ . Finally, let $z_{-1}:\mathbb{R}\rightarrow\mathbb{R}$ . Then from differentiating $z_j(t)$ (in $t$ ), we have that for $j\in\mathbb{N}$ and $t\in\mathbb{R}$ : $$z_j'(t)=\left(b(t)\right)^j\,\phi\left(b(t)\right)-a'(t)z_j(t)+c'(t)\left(jz_{j-1}(t)-z_{j+1}(t)\right),$$ where, as usual, dashes denote first derivatives. I want to derive a differential equation for $z_0(t)$ that does not depend on $z_1(t),z_2(t),\dots$ . Is there a way to solve the recurrence equation (holding $t$ fixed) here to leave a differential equation? Or is there some other direct way to get a differential equation for $z_0(t)$ ? The solution should be somehow related to the recurrence relation for $z_j$ ( $j\in\mathbb{N}$ ): $$0=\beta^j\varphi-\alpha z_j+\gamma\left(jz_{j-1}-z_{j+1}\right).$$ When $\phi=0$ or $\beta=0$ , and $\alpha=-\gamma$ , this gives: $$z_{j+1}=z_j+jz_{j-1},$$ which is the sequence here and here (the number of involutions on a finite set) for appropriate initial conditions.","Let be differentiable functions, with and as , and with and as . Let be the density of a standard normal random variable, meaning that for all : For , let with: for all , where you may assume the integral exists and is finite for all and . Finally, let . Then from differentiating (in ), we have that for and : where, as usual, dashes denote first derivatives. I want to derive a differential equation for that does not depend on . Is there a way to solve the recurrence equation (holding fixed) here to leave a differential equation? Or is there some other direct way to get a differential equation for ? The solution should be somehow related to the recurrence relation for ( ): When or , and , this gives: which is the sequence here and here (the number of involutions on a finite set) for appropriate initial conditions.","a,b,c:\mathbb{R}\rightarrow\mathbb{R} a(t)\rightarrow -\infty c(t)\rightarrow -\infty t\rightarrow -\infty a(t)\rightarrow\infty c(t)\rightarrow\infty t\rightarrow\infty \phi:\mathbb{R}\rightarrow\mathbb{R}^+ z\in\mathbb{R} \phi(z)=\frac{1}{\sqrt{2\pi}} \exp{\left(-\frac{z^2}{2}\right)}. j\in\mathbb{N} z_j:\mathbb{R}\rightarrow\mathbb{R} z_j(t)=\int_{-\infty}^t{\exp{\left(a(\tau)-a(t)\right)}\,\left(b(\tau)+c(t)-c(\tau)\right)^j\,\phi\left(b(\tau)+c(t)-c(\tau)\right)\,d\tau}, t\in\mathbb{R} j\in\mathbb{N} t\in\mathbb{R} z_{-1}:\mathbb{R}\rightarrow\mathbb{R} z_j(t) t j\in\mathbb{N} t\in\mathbb{R} z_j'(t)=\left(b(t)\right)^j\,\phi\left(b(t)\right)-a'(t)z_j(t)+c'(t)\left(jz_{j-1}(t)-z_{j+1}(t)\right), z_0(t) z_1(t),z_2(t),\dots t z_0(t) z_j j\in\mathbb{N} 0=\beta^j\varphi-\alpha z_j+\gamma\left(jz_{j-1}-z_{j+1}\right). \phi=0 \beta=0 \alpha=-\gamma z_{j+1}=z_j+jz_{j-1},","['ordinary-differential-equations', 'recurrence-relations', 'normal-distribution', 'involutions']"
91,Stability of a $3\times 3$ system of non-linear differential equations.,Stability of a  system of non-linear differential equations.,3\times 3,"I have the following system: $$\begin{cases} x'&=&2(z-1)y,\\y'&=&-(z-1)x,\\z'&=&-z^3. \end{cases}$$ and want to study the stability of it at the fixed point $(0,0,0)$ . Doing the linearization of the system, it predicts a linear center. Then, we cannot guarantee that it is a non-linear center. Then, I can think about the following options: Find a Lyapunov's function (if it is asymptotically stable). Check if the system s reversible by doing $t\mapsto -t$ , $(x,y,z)\mapsto R(x,y,z)$ where $R^2=\text{Id}$ , then we would have a non-linear center. Find a conserved quantity, i.e.m a real-valued continuous function $E(x,y,z)$ such that it is constant on trajectories $\left(\dfrac{dE}{dt}=0\right)$ . If we can write $(x',y',z')=-\nabla V$ ( $V$ potential function), where it would be impossible to find closed orbits. Dulac's Criterion for ruling out closed orbits. Poincaré-Bendixon to see if closed orbits exist. I unsuccessfuly tried the first three items above. Then I proceeded to draw the system on Maple and seems like there will be centers, but I need to solve this analytically. Any hint to continue this problem?","I have the following system: and want to study the stability of it at the fixed point . Doing the linearization of the system, it predicts a linear center. Then, we cannot guarantee that it is a non-linear center. Then, I can think about the following options: Find a Lyapunov's function (if it is asymptotically stable). Check if the system s reversible by doing , where , then we would have a non-linear center. Find a conserved quantity, i.e.m a real-valued continuous function such that it is constant on trajectories . If we can write ( potential function), where it would be impossible to find closed orbits. Dulac's Criterion for ruling out closed orbits. Poincaré-Bendixon to see if closed orbits exist. I unsuccessfuly tried the first three items above. Then I proceeded to draw the system on Maple and seems like there will be centers, but I need to solve this analytically. Any hint to continue this problem?","\begin{cases}
x'&=&2(z-1)y,\\y'&=&-(z-1)x,\\z'&=&-z^3.
\end{cases} (0,0,0) t\mapsto -t (x,y,z)\mapsto R(x,y,z) R^2=\text{Id} E(x,y,z) \left(\dfrac{dE}{dt}=0\right) (x',y',z')=-\nabla V V",['ordinary-differential-equations']
92,A troubling differential equation: $(xy+2y+x+2)y'=e^{-y}(x+3)$,A troubling differential equation:,(xy+2y+x+2)y'=e^{-y}(x+3),"Of course, I found this to be sort of troubling, you might not. So this is a differential equation that I came up with about 3 days ago that has taken me quite a while to solve (for some reason): $$(xy+2y+x+2)y'=e^{-y}(x+3)$$ which although I did solve it, I really am not sure if I made any incorrect assumptions while solving it or if my solution is incorrect. Here is my solving process: Right away I noticed that I could factor the $xy+2y+x+2$ as $(x+2)(y+1)$ , and dividing both sides by $x+2$ gets $$(y+1)y'=(e^{-y})\dfrac{x+3}{x+2}$$ and then I can multiply both sides by $e^y$ gets $$(y+1)y'e^y=\dfrac{x+3}{x+2}$$ which we can rewrite as $$(y+1)e^ydy=\dfrac{x+3}{x+2}dx$$ and this is where I feel like I might have made a mistake somewhere. I know the integral on the right hand is $x+\ln|x+2|+c_0$ , but the left hand side is a bit weird. I know I can write the left hand side as $$\int(y+1)e^ydy=\int ye^y+e^ydy=\int ye^ydy+e^y+c$$ but I was unsure on how to evalute $\int ye^ydy$ . I decided to use IBP (integration by parts) and got that the integral on the left hand side was equal to $\int(y+1)e^ydy=ye^y+c_1$ which now we can subtract $c_1$ from both sides and then take Lambert's $W$ function to find that $$y(x)=W(x+\ln|x+2|+b),b=c_0-c_1$$ but my question is Is my solution correct, or did I make any incorrect assumptions/get the wrong solution?","Of course, I found this to be sort of troubling, you might not. So this is a differential equation that I came up with about 3 days ago that has taken me quite a while to solve (for some reason): which although I did solve it, I really am not sure if I made any incorrect assumptions while solving it or if my solution is incorrect. Here is my solving process: Right away I noticed that I could factor the as , and dividing both sides by gets and then I can multiply both sides by gets which we can rewrite as and this is where I feel like I might have made a mistake somewhere. I know the integral on the right hand is , but the left hand side is a bit weird. I know I can write the left hand side as but I was unsure on how to evalute . I decided to use IBP (integration by parts) and got that the integral on the left hand side was equal to which now we can subtract from both sides and then take Lambert's function to find that but my question is Is my solution correct, or did I make any incorrect assumptions/get the wrong solution?","(xy+2y+x+2)y'=e^{-y}(x+3) xy+2y+x+2 (x+2)(y+1) x+2 (y+1)y'=(e^{-y})\dfrac{x+3}{x+2} e^y (y+1)y'e^y=\dfrac{x+3}{x+2} (y+1)e^ydy=\dfrac{x+3}{x+2}dx x+\ln|x+2|+c_0 \int(y+1)e^ydy=\int ye^y+e^ydy=\int ye^ydy+e^y+c \int ye^ydy \int(y+1)e^ydy=ye^y+c_1 c_1 W y(x)=W(x+\ln|x+2|+b),b=c_0-c_1","['calculus', 'ordinary-differential-equations', 'solution-verification', 'recreational-mathematics']"
93,How can I show that this difference-differential equation has a unique solution?,How can I show that this difference-differential equation has a unique solution?,,"Consider the following difference-differential equation $$ (1+\alpha t)y_k'(t) - (k\beta+r) y_k(t) = y_{k-1}(t) $$ with $\alpha,\beta,r\in\mathbb R$ and the initial conditions $y_k(0)=0$ for $k\geq1$ and $y_0(t)=(1+\alpha t)^{r/\alpha}$ . In a paper I'm currently reading it is said, that this differential equation has a unique solution under the above conditions and I'm wondering why. If I could rewrite the equation to $y_k'(t)=f(t,y_k)$ I could probably use the existence and uniqueness theorem, unfortunately the equation also contains $y_{k-1}(t)$ so I can just rewrite the equation as $$ y_k'(t) = f(t,y_k(t),y_{k-1}(t)). $$ How can I show that there exists a unique solution to this type of differential equation?","Consider the following difference-differential equation with and the initial conditions for and . In a paper I'm currently reading it is said, that this differential equation has a unique solution under the above conditions and I'm wondering why. If I could rewrite the equation to I could probably use the existence and uniqueness theorem, unfortunately the equation also contains so I can just rewrite the equation as How can I show that there exists a unique solution to this type of differential equation?"," (1+\alpha t)y_k'(t) - (k\beta+r) y_k(t) = y_{k-1}(t)  \alpha,\beta,r\in\mathbb R y_k(0)=0 k\geq1 y_0(t)=(1+\alpha t)^{r/\alpha} y_k'(t)=f(t,y_k) y_{k-1}(t)  y_k'(t) = f(t,y_k(t),y_{k-1}(t)). ","['ordinary-differential-equations', 'initial-value-problems']"
94,Closed form expression for an integral with a cosine function,Closed form expression for an integral with a cosine function,,"I am attempting to solve a first-order ODE of the form: $$y'(x) = {\rm sech}^2(x){\rm sech}'(x) \times(a + b\cos(cx))$$ The part with the constant was easy to solve and I get $$y(x) = a\int {\rm sech}^2(x){\rm sech}'(x) dx = a\frac{{\rm sech}^3(x)}{3} + C,$$ where $C$ is a constant. However, the part with the cosine term is harder to integrate. I tried simple integration-by-parts since I know the integral of ${\rm sech}^2(x){\rm sech}'(x)$ , but I didn't see it leading me anywhere. Wolfram Alpha gave me a hypergeometric function, but I wonder if there is a cleaner closed-form expression for the integral.","I am attempting to solve a first-order ODE of the form: The part with the constant was easy to solve and I get where is a constant. However, the part with the cosine term is harder to integrate. I tried simple integration-by-parts since I know the integral of , but I didn't see it leading me anywhere. Wolfram Alpha gave me a hypergeometric function, but I wonder if there is a cleaner closed-form expression for the integral.","y'(x) = {\rm sech}^2(x){\rm sech}'(x) \times(a + b\cos(cx)) y(x) = a\int {\rm sech}^2(x){\rm sech}'(x) dx = a\frac{{\rm sech}^3(x)}{3} + C, C {\rm sech}^2(x){\rm sech}'(x)","['ordinary-differential-equations', 'indefinite-integrals']"
95,Uniqueness of the solution to systems of first-order linear PDEs,Uniqueness of the solution to systems of first-order linear PDEs,,"Context: Let $\Omega \subset \mathbb{R}^p$ be an domain. For functions $A_{jk}^i : \Omega \to \mathbb{R}$ and $B_k^i : \Omega \to \mathbb{R}$ with some regularity, I am interested in the following system of PDEs: $$ \sum_{j = 1}^n \sum_{k = 1}^m  A_{jk}^i (x) \frac{\partial f_k}{\partial x_j} (x) + B_{k}^i (x) f_k (x) = 0 \ \ \text{for} \ \  i = 1, \dots, r $$ where $f = (f_1, \dots, f_m)$ are variables. Assume that the number of equations is no less than the number of variables, i.e., $r \geq m.$ Also, assume the Dirichlet boundary condition: $f_k = 0$ on $\partial \Omega.$ Question: It is obvious that $f_k = 0$ is a solution. I am wondering under what conditions on $A_{jk}^i$ and $B_k^i$ the uniqueness is guaranteed. What I have tried: I found some potentially useful books/papers/notes. (1) Wavefronts and Rays as Characteristics and Asymptotics by Andrej Bóna and Michael A Slawinski. In section 1.7 (page 24), they briefly explain how to solve the system. Their setup is almost the same as what I have in mind, but their method relies on the higher-order differentiability of $A_{jk}^i,$ which is not ideal for me. (2) METHODS OF MATHEMATICAL PHYSICS VOLUME II by R. COUHANT and D. HILBERT. In section 2.2 (pages 14-15), they address the system saying that by Cramer's rule, one can transform the system into independent PDEs. But I am not pretty sure how I should apply Cramer's rule to differential operators. (3) Lecture note by Evy Kersalé. In section 2.4, he discusses interesting methods to solve the system for $p = 2.$ I would like to know more general treatments. What I want: I conjecture that a sort of full rankness of $A_{jk}^i$ and $B_k^i$ is sufficient for the uniqueness, but I have not found such a result. Also, I do not want to put too strong differentiability assumptions on $A_{jk}^i$ and $B_k^i.$ Ideally, they should be up to $C^1.$ Any thoughts and references are helpful. Thanks!","Context: Let be an domain. For functions and with some regularity, I am interested in the following system of PDEs: where are variables. Assume that the number of equations is no less than the number of variables, i.e., Also, assume the Dirichlet boundary condition: on Question: It is obvious that is a solution. I am wondering under what conditions on and the uniqueness is guaranteed. What I have tried: I found some potentially useful books/papers/notes. (1) Wavefronts and Rays as Characteristics and Asymptotics by Andrej Bóna and Michael A Slawinski. In section 1.7 (page 24), they briefly explain how to solve the system. Their setup is almost the same as what I have in mind, but their method relies on the higher-order differentiability of which is not ideal for me. (2) METHODS OF MATHEMATICAL PHYSICS VOLUME II by R. COUHANT and D. HILBERT. In section 2.2 (pages 14-15), they address the system saying that by Cramer's rule, one can transform the system into independent PDEs. But I am not pretty sure how I should apply Cramer's rule to differential operators. (3) Lecture note by Evy Kersalé. In section 2.4, he discusses interesting methods to solve the system for I would like to know more general treatments. What I want: I conjecture that a sort of full rankness of and is sufficient for the uniqueness, but I have not found such a result. Also, I do not want to put too strong differentiability assumptions on and Ideally, they should be up to Any thoughts and references are helpful. Thanks!","\Omega \subset \mathbb{R}^p A_{jk}^i : \Omega \to \mathbb{R} B_k^i : \Omega \to \mathbb{R} 
\sum_{j = 1}^n \sum_{k = 1}^m 
A_{jk}^i (x) \frac{\partial f_k}{\partial x_j} (x)
+
B_{k}^i (x) f_k (x)
=
0
\ \ \text{for} \ \ 
i = 1, \dots, r
 f = (f_1, \dots, f_m) r \geq m. f_k = 0 \partial \Omega. f_k = 0 A_{jk}^i B_k^i A_{jk}^i, p = 2. A_{jk}^i B_k^i A_{jk}^i B_k^i. C^1.","['ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'linear-pde']"
96,How to prove the following region is positively invariant?,How to prove the following region is positively invariant?,,"I just consider another Lyapunov function as in question Question about the basin of attraction of the origin of the following ODE Regarding of the following system : $$\begin{cases} \dot x=-x^3 \\ \dot y=-x^2y-y-z^2y \\  \dot z=-\sin z \\  \end{cases}$$ Show that the point $(0,0,0)$ is asymptotically stable and find its attractiveness basin. Consider $V(x,y,z)=x^2+y^2+z^2$ and then $$ \dot V(x,y,z)=-2x^4-2y^2(x^2+z^2+1)-2z\sin z $$ which is strictly less than zero for $|z|<\pi$ except for the origin. Hence, it is asymptotically. This is one example in the page 198 in textbook Morris W. Hirsch, Stephen Smale and Robert L. Devaney (Auth.) - Differential Equations, Dynamical Systems, and an Introduction to Chaos-Academic Press . It says that In particular, any solution that begins in the region $|z|<\pi$ must remain trapped in this region for all time. Moreover, we can conclude that the basin of attraction of the origin is the entire region $|z|<\pi$ . Any solutions that starts insider a sphere of radius $r<\pi$ must tend to the origin. Outsider of the sphere of radius $\pi$ and between the planes $|z|=\pi$ , the function $V$ is still strictly decreasing. Since solutions are trapped between these two planes, it follows that they too must tend to the origin. For the first part, since $$ \Omega_1=\{(x,y,z): V(x,y,z) \mbox{ and } |z|<\pi\} $$ is positively invariant, then we can prove that ""Any solutions that starts insider a sphere of radius $r<\pi$ must tend to the origin."". Question: But how to prove that the second that solution starting from the outside of the sphere will converge to the origin?","I just consider another Lyapunov function as in question Question about the basin of attraction of the origin of the following ODE Regarding of the following system : Show that the point is asymptotically stable and find its attractiveness basin. Consider and then which is strictly less than zero for except for the origin. Hence, it is asymptotically. This is one example in the page 198 in textbook Morris W. Hirsch, Stephen Smale and Robert L. Devaney (Auth.) - Differential Equations, Dynamical Systems, and an Introduction to Chaos-Academic Press . It says that In particular, any solution that begins in the region must remain trapped in this region for all time. Moreover, we can conclude that the basin of attraction of the origin is the entire region . Any solutions that starts insider a sphere of radius must tend to the origin. Outsider of the sphere of radius and between the planes , the function is still strictly decreasing. Since solutions are trapped between these two planes, it follows that they too must tend to the origin. For the first part, since is positively invariant, then we can prove that ""Any solutions that starts insider a sphere of radius must tend to the origin."". Question: But how to prove that the second that solution starting from the outside of the sphere will converge to the origin?","\begin{cases}
\dot x=-x^3 \\
\dot y=-x^2y-y-z^2y \\ 
\dot z=-\sin z \\ 
\end{cases} (0,0,0) V(x,y,z)=x^2+y^2+z^2 
\dot V(x,y,z)=-2x^4-2y^2(x^2+z^2+1)-2z\sin z
 |z|<\pi |z|<\pi |z|<\pi r<\pi \pi |z|=\pi V 
\Omega_1=\{(x,y,z): V(x,y,z) \mbox{ and } |z|<\pi\}
 r<\pi","['ordinary-differential-equations', 'dynamical-systems']"
97,Equations modelling a snowboarder/skier on a jump,Equations modelling a snowboarder/skier on a jump,,"I am currently a highschool student elaborating an internal assessment for my mathematics AA HL course (a sort of research paper we must elaborate for the IB) and was looking into the relationship between mathematics and snowboarding. My current idea for research was investigating the feasibility of a quintuple cork given some initial conditions of the jump (angle, length, height, slope angle, etc.) and the rider (height, mass, etc.). However, I've encountered some difficulties in finding appropriate differential equations to model a snowboarder's motion during a jump. Most existing models are either too complex, relying on matrix methods and other mathematics that are beyond my current level of math, or too simplistic, relying on basic kinematic equations that wouldn't allow for a meaningful investigation. I would greatly appreciate your help in finding or developing differential equations that can accurately model a snowboarder's motion during a jump. I believe that with the right equations, I can utilize various numerical methods such as Euler and Runge-Kutta to analyze the feasibility of a quintuple cork on a specific jump. Whether you can suggest equations, recommend textbooks or research papers, or provide insights into the application of calculus and differential equations in this context, I would greatly appreciate your input. Let me make it clear I do not want my work done for me, since it would be against guidelines, rather some pointers to help me get started","I am currently a highschool student elaborating an internal assessment for my mathematics AA HL course (a sort of research paper we must elaborate for the IB) and was looking into the relationship between mathematics and snowboarding. My current idea for research was investigating the feasibility of a quintuple cork given some initial conditions of the jump (angle, length, height, slope angle, etc.) and the rider (height, mass, etc.). However, I've encountered some difficulties in finding appropriate differential equations to model a snowboarder's motion during a jump. Most existing models are either too complex, relying on matrix methods and other mathematics that are beyond my current level of math, or too simplistic, relying on basic kinematic equations that wouldn't allow for a meaningful investigation. I would greatly appreciate your help in finding or developing differential equations that can accurately model a snowboarder's motion during a jump. I believe that with the right equations, I can utilize various numerical methods such as Euler and Runge-Kutta to analyze the feasibility of a quintuple cork on a specific jump. Whether you can suggest equations, recommend textbooks or research papers, or provide insights into the application of calculus and differential equations in this context, I would greatly appreciate your input. Let me make it clear I do not want my work done for me, since it would be against guidelines, rather some pointers to help me get started",,"['calculus', 'ordinary-differential-equations', 'physics']"
98,"Solving an ""almost differential"" equation [closed]","Solving an ""almost differential"" equation [closed]",,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 8 months ago . Improve this question This problem comes from a problem set that my calculus professor assigned to me. Determine all functions $f: \mathbb{R} \to \mathbb{R}$ such that $$ f(x) = f\Bigl(\frac{x}{2}\Bigr) + f'(x)\, \frac{x}{2}  \quad \forall x \in \mathbb{R}. $$ The answer is all linear ones, for the record. I have an elementary, though ad-hoc, solution using specific supremum and infimum properties. What I wonder is whether or not problems of this type are prone to a specific or ""known"" method, and if so, how this method can be applied to this particular problem. I tried to prove it in a more standard manner but didn't find something useful: I differentiated both sides two times and deduced $f''(0)=0$ . Inductively, one can prove $f^{(n)}(0)=0$ for $n\geq 2$ . Now, would Taylor series suffice to prove $f(x)=ax+b$ ? Thanks in advance","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 8 months ago . Improve this question This problem comes from a problem set that my calculus professor assigned to me. Determine all functions such that The answer is all linear ones, for the record. I have an elementary, though ad-hoc, solution using specific supremum and infimum properties. What I wonder is whether or not problems of this type are prone to a specific or ""known"" method, and if so, how this method can be applied to this particular problem. I tried to prove it in a more standard manner but didn't find something useful: I differentiated both sides two times and deduced . Inductively, one can prove for . Now, would Taylor series suffice to prove ? Thanks in advance","f: \mathbb{R} \to \mathbb{R} 
f(x) = f\Bigl(\frac{x}{2}\Bigr) + f'(x)\, \frac{x}{2} 
\quad \forall x \in \mathbb{R}.
 f''(0)=0 f^{(n)}(0)=0 n\geq 2 f(x)=ax+b","['calculus', 'ordinary-differential-equations', 'functional-equations', 'supremum-and-infimum']"
99,How bistable is my system?,How bistable is my system?,,"Description: given ODE: $\dot{x} = a + bx +cx^2 +dx^3$ , I have mutliple combinations of the coefficients $a,b,c,d$ that I want to understand whether they make a bistable or not system. For this purpose, I started with the discriminant $\Delta$ of the cubic equation for $\dot{x}=0$ , which: for $\Delta > 0$ , 3 real solutions/fixed points (I guess 2 stable and 1 unstable) for $\Delta = 0$ , 1 and 1 double real solutions/fixed points (stability ?) for $\Delta<0$ , 1 real solution/fixed point and 2 imaginary. So, using $\Delta > 0$ I am able to find numerically which combinations of coefficients give me 3 real solutions and a clearly bistable system. But how strongly bistable are my systems? My current approach: I made the leap to hypothesize that the more positive my $\Delta$ is, the more bistable my system is. Basically I thought that because it seems that $\Delta$ is actually a metric of separating the solutions, since for $\Delta=0$ the double solution is just two collapsed solutions to the same point. So, it actually feels like $\Delta$ is like a control parameter of the system, which by varying it you cross bifurcation points and generate solutions. However, probably sth like that is not proved and it is like my approach makes some sort of sense but it is illegitimate. So, any comments on that would be highly appreciated. Energy landscapes ( $-\int\dot{x}dx$ ) for different combinations of $a,b,c,d$ and a range of positive $\Delta$ Another approach: More legitimate I thought that a proper stability analysis would be. So that would be to find numerical solutions of $\dot{x}=0$ , and do linearized stability analysis (get the Jacobian over the fixed points) to see the nature of the fixed points. But, my next question is, what should I look for to judge how strong the bistable system is? Is it how positive the unstable fixed point is (greater instability of the fixed point, more bistable)? Or is it a combination with how negative the stable fixed points are? Other approaches: At the end, I am not sure whether some sort of statistical significance could be made to determine how strong bistability exists (and stats is my weakest quality).","Description: given ODE: , I have mutliple combinations of the coefficients that I want to understand whether they make a bistable or not system. For this purpose, I started with the discriminant of the cubic equation for , which: for , 3 real solutions/fixed points (I guess 2 stable and 1 unstable) for , 1 and 1 double real solutions/fixed points (stability ?) for , 1 real solution/fixed point and 2 imaginary. So, using I am able to find numerically which combinations of coefficients give me 3 real solutions and a clearly bistable system. But how strongly bistable are my systems? My current approach: I made the leap to hypothesize that the more positive my is, the more bistable my system is. Basically I thought that because it seems that is actually a metric of separating the solutions, since for the double solution is just two collapsed solutions to the same point. So, it actually feels like is like a control parameter of the system, which by varying it you cross bifurcation points and generate solutions. However, probably sth like that is not proved and it is like my approach makes some sort of sense but it is illegitimate. So, any comments on that would be highly appreciated. Energy landscapes ( ) for different combinations of and a range of positive Another approach: More legitimate I thought that a proper stability analysis would be. So that would be to find numerical solutions of , and do linearized stability analysis (get the Jacobian over the fixed points) to see the nature of the fixed points. But, my next question is, what should I look for to judge how strong the bistable system is? Is it how positive the unstable fixed point is (greater instability of the fixed point, more bistable)? Or is it a combination with how negative the stable fixed points are? Other approaches: At the end, I am not sure whether some sort of statistical significance could be made to determine how strong bistability exists (and stats is my weakest quality).","\dot{x} = a + bx +cx^2 +dx^3 a,b,c,d \Delta \dot{x}=0 \Delta > 0 \Delta = 0 \Delta<0 \Delta > 0 \Delta \Delta \Delta=0 \Delta -\int\dot{x}dx a,b,c,d \Delta \dot{x}=0","['ordinary-differential-equations', 'nonlinear-system', 'stability-theory', 'discriminant']"
