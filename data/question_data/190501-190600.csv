,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Are turning points and stationary points the same?,Are turning points and stationary points the same?,,"I'm currently doing AS maths and my Pure 1 textbook treats stationary points and turning points as the same thing. Furthermore, my teacher said that stationary and turning points are the same. However, from my understanding, a turning point is where the gradient changes sign and a stationary point is where the derivative is 0. Hence they are slightly different, in the sense that a point of inflexion should not be a turning point. I'm slightly confused. Who is right? Could your provide citations and things to read to prove your answer?","I'm currently doing AS maths and my Pure 1 textbook treats stationary points and turning points as the same thing. Furthermore, my teacher said that stationary and turning points are the same. However, from my understanding, a turning point is where the gradient changes sign and a stationary point is where the derivative is 0. Hence they are slightly different, in the sense that a point of inflexion should not be a turning point. I'm slightly confused. Who is right? Could your provide citations and things to read to prove your answer?",,"['calculus', 'derivatives', 'graphing-functions']"
1,Find the derivative of $y=\sqrt{\ln\left(4x-x^2\right)}$,Find the derivative of,y=\sqrt{\ln\left(4x-x^2\right)},"Find the derivative of $$y=\sqrt{\ln{\left(4x-x^2\right)}}$$ So we can rewrite the function as $$y=\left[\ln\left(4x-x^2\right)\right]^\frac12$$ Let's try to break it down a bit. So let's set $$a(x)=x^\frac12$$ and $$b(x)=\ln(4x-x^2)$$ then $$y=a(b(x))$$ The chain rule then tells us that $$y'=a'(b(x))b'(x)$$ Now $a'$ we can easily find, as it is just $$a'(x)=\dfrac{1}{2\sqrt{x}},$$ but how do we find $b'$ ? Well, let's do the same thing again, namely notice that it's a composition and break it down. So now let $\alpha(x)=\ln x$ and $\beta(x)=4x-x^2$ . These two functions we know how to differentiate! Indeed $\alpha'(x)=\dfrac{1}{x}$ and $\beta'(x)=4-2x$ . Furthermore, the chain rule also tells us now that, as $b(x)=\alpha(\beta(x))$ , $b'(x)=\alpha'(\beta(x))\beta'(x)$ . Putting this all together we get that $$y'(x)=a'(b(x))\alpha'(\beta(x))\beta'(x)\\=\dfrac{1}{2\sqrt{\ln(4x-x^2)}}\cdot\dfrac{1}{4x-x^2}\cdot(4-2x)=\dfrac{1}{2x\sqrt{\ln(4x-x^2)}}.$$ The answer seems to differ...","Find the derivative of So we can rewrite the function as Let's try to break it down a bit. So let's set and then The chain rule then tells us that Now we can easily find, as it is just but how do we find ? Well, let's do the same thing again, namely notice that it's a composition and break it down. So now let and . These two functions we know how to differentiate! Indeed and . Furthermore, the chain rule also tells us now that, as , . Putting this all together we get that The answer seems to differ...","y=\sqrt{\ln{\left(4x-x^2\right)}} y=\left[\ln\left(4x-x^2\right)\right]^\frac12 a(x)=x^\frac12 b(x)=\ln(4x-x^2) y=a(b(x)) y'=a'(b(x))b'(x) a' a'(x)=\dfrac{1}{2\sqrt{x}}, b' \alpha(x)=\ln x \beta(x)=4x-x^2 \alpha'(x)=\dfrac{1}{x} \beta'(x)=4-2x b(x)=\alpha(\beta(x)) b'(x)=\alpha'(\beta(x))\beta'(x) y'(x)=a'(b(x))\alpha'(\beta(x))\beta'(x)\\=\dfrac{1}{2\sqrt{\ln(4x-x^2)}}\cdot\dfrac{1}{4x-x^2}\cdot(4-2x)=\dfrac{1}{2x\sqrt{\ln(4x-x^2)}}.","['calculus', 'derivatives']"
2,"Why is it said that Laplacians ""smooth solutions""?","Why is it said that Laplacians ""smooth solutions""?",,"Say you have some PDE for a function $f:[0,T]\times \mathbb{R}^n\to\mathbb C$ $$\frac{\partial f}{\partial t}=Lf,$$ for some differential operator $L$ . If $L$ contains a Laplacian term $$\Delta f=\sum_{i=1}^n \frac{\partial^2 f}{\partial x_i^2},$$ why do people claim (as a rule of thumb) that this helps ""smooth the solution""? From my understanding they are saying that the solution of the PDE will have better differentiability, but I am unsure why the Laplacian has this effect.","Say you have some PDE for a function for some differential operator . If contains a Laplacian term why do people claim (as a rule of thumb) that this helps ""smooth the solution""? From my understanding they are saying that the solution of the PDE will have better differentiability, but I am unsure why the Laplacian has this effect.","f:[0,T]\times \mathbb{R}^n\to\mathbb C \frac{\partial f}{\partial t}=Lf, L L \Delta f=\sum_{i=1}^n \frac{\partial^2 f}{\partial x_i^2},","['real-analysis', 'derivatives', 'partial-differential-equations', 'laplacian', 'smooth-functions']"
3,d(dx) exterior derivative in simple language?,d(dx) exterior derivative in simple language?,,"I’m only learning the basics of analysis, so I have no understanding of exterior derivatives. I simply need clarification on my misconception using simple words. I’m reading about second degree derivatives and found the following $$d\left( dy\right) =d\left( f'\left( x\right) dx\right)= f''(x) dx * dx= f''\left( x\right) \left( dx\right) ^{2}$$ So I was thinking of expanding $d(f’(x)dx)$ to $f’’(x) * dx + f’(x) * d(dx)$ . According to the axioms of exterior derivative, the latter part should equals to zero. However I was thinking $dx = 1 * \triangle(x)=(x-x_0)$ , $d(dx) = d(x - x_0) = d(x) - d(x_0) = d(x)$ According to the definition of the exterior derivative, this is wrong. I’m confused with the misconception I have. Please help me clarify this. Update on maybe a partial solution, I’ve done some research on the web and found that maybe dx should be viewed as a constant. In that case d(dx) = 0. Can we view dx as a constant at all?","I’m only learning the basics of analysis, so I have no understanding of exterior derivatives. I simply need clarification on my misconception using simple words. I’m reading about second degree derivatives and found the following So I was thinking of expanding to . According to the axioms of exterior derivative, the latter part should equals to zero. However I was thinking , According to the definition of the exterior derivative, this is wrong. I’m confused with the misconception I have. Please help me clarify this. Update on maybe a partial solution, I’ve done some research on the web and found that maybe dx should be viewed as a constant. In that case d(dx) = 0. Can we view dx as a constant at all?",d\left( dy\right) =d\left( f'\left( x\right) dx\right)= f''(x) dx * dx= f''\left( x\right) \left( dx\right) ^{2} d(f’(x)dx) f’’(x) * dx + f’(x) * d(dx) dx = 1 * \triangle(x)=(x-x_0) d(dx) = d(x - x_0) = d(x) - d(x_0) = d(x),"['real-analysis', 'calculus', 'derivatives', 'soft-question']"
4,Understanding Higher-order Differentiability Conceptually,Understanding Higher-order Differentiability Conceptually,,"I understand conceptually that a function $f\colon A\to\mathbf R$ is differentiable at a point $a\in A$ if it can be well approximated by a line there; more precisely, if we can find a constant $f'(a)$ such that $$f(a+h) = f(a) + f'(a)h+o(h).$$ My goal: I want to understand (intuitively) what it means when a function is twice differentiable, or thrice, etc, and likewise what it means when it isn't. In a very loose sense, I have the intuition that a function is somehow smooth er around a point if it has higher order differentiability there, and I suppose this somehow corresponds to the fact that it can be approximated well not only by a line but even better by a polynomial. (e.g. twice differentiability is $f(a+h) = f(a) + f'(a)h + \tfrac12f''(a)h^2 + o(h^2)$ , etc.). Does this mean polynomials canonically define the notion of ""smoothness""? Perhaps I can best get across what I'm asking for with an example. When I look at the graphs of $$y=x|x| \qquad \text{and} \qquad y=x^3$$ for instance, I see that the latter grows quicker than the former, but around zero, they both basically look ""smooth"" to me. Yes I know that one is made of the absolute value function which is pointy, but if you just showed me these two pictures: I wouldn't really feel that one is somehow ""smoother"" around zero than the other. Is there a better way I can think about this?","I understand conceptually that a function is differentiable at a point if it can be well approximated by a line there; more precisely, if we can find a constant such that My goal: I want to understand (intuitively) what it means when a function is twice differentiable, or thrice, etc, and likewise what it means when it isn't. In a very loose sense, I have the intuition that a function is somehow smooth er around a point if it has higher order differentiability there, and I suppose this somehow corresponds to the fact that it can be approximated well not only by a line but even better by a polynomial. (e.g. twice differentiability is , etc.). Does this mean polynomials canonically define the notion of ""smoothness""? Perhaps I can best get across what I'm asking for with an example. When I look at the graphs of for instance, I see that the latter grows quicker than the former, but around zero, they both basically look ""smooth"" to me. Yes I know that one is made of the absolute value function which is pointy, but if you just showed me these two pictures: I wouldn't really feel that one is somehow ""smoother"" around zero than the other. Is there a better way I can think about this?",f\colon A\to\mathbf R a\in A f'(a) f(a+h) = f(a) + f'(a)h+o(h). f(a+h) = f(a) + f'(a)h + \tfrac12f''(a)h^2 + o(h^2) y=x|x| \qquad \text{and} \qquad y=x^3,"['real-analysis', 'calculus', 'derivatives']"
5,How to rigorously define the parametric derivative?,How to rigorously define the parametric derivative?,,I'm trying to understand the parametric derivative identity $$ \frac{dy(t)}{dt} = \frac{dy}{dx} \frac{dx}{dt} \tag{1}$$ I feel this is not rigorous because we are say that $y(t)$ can be written as $y(x(t) )$ what conditions are required on $y$ for this to be true?,I'm trying to understand the parametric derivative identity I feel this is not rigorous because we are say that can be written as what conditions are required on for this to be true?, \frac{dy(t)}{dt} = \frac{dy}{dx} \frac{dx}{dt} \tag{1} y(t) y(x(t) ) y,"['derivatives', 'parametric']"
6,How do we calculate the directional derivative of a vector field? (If there is such a thing.),How do we calculate the directional derivative of a vector field? (If there is such a thing.),,"So, for a scalar field $T(x,y,z)$ , the derivative along $d\vec l$ is given by $$\frac {dT}{|d\vec l|} = |\vec \nabla T| \cos\theta$$ where $\theta$ is the angle between $\vec \nabla T$ and $d\vec l$ For a vector field $\vec V (x,y,z)$ , I understand that $\vec \nabla . \vec V$ and $\vec \nabla \times \vec V$ give the Divergence and the Curl respectively. But, is there a way in which $\vec \nabla$ can act on $\vec V$ to give an expression for $\frac {d \vec V}{|d\vec l|}$ , the directional derivative of $\vec V$ along $d\vec l$ ? PS: I've only just started to learn vector calculus, so pardon me if this question comes out as silly.","So, for a scalar field , the derivative along is given by where is the angle between and For a vector field , I understand that and give the Divergence and the Curl respectively. But, is there a way in which can act on to give an expression for , the directional derivative of along ? PS: I've only just started to learn vector calculus, so pardon me if this question comes out as silly.","T(x,y,z) d\vec l \frac {dT}{|d\vec l|} = |\vec \nabla T| \cos\theta \theta \vec \nabla T d\vec l \vec V (x,y,z) \vec \nabla . \vec V \vec \nabla \times \vec V \vec \nabla \vec V \frac {d \vec V}{|d\vec l|} \vec V d\vec l","['derivatives', 'field-theory', 'vector-analysis', 'vector-fields']"
7,Find a regular function $\varphi$ which verifies the following conditions,Find a regular function  which verifies the following conditions,\varphi,"I am looking for a function $\varphi \in C^{\infty}(K)$ where $$K=\{(x,y,z) \in \mathbb{R}^3 \  | \ x^2+y^2 \leq 1/16 \ \ and \ \ 0 \leq z\leq 2\} \setminus B$$ with $B=\{(x,y,z) \in \mathbb{R}^3 \  | \ x^2+y^2+z^2 \leq h^2 \} \cup \{(x,y,z) \in \mathbb{R}^3 \  | \ x^2+y^2+(z-2)^2 \leq h^2 \}$ the union of two spheres of radius $1>h>\frac{1}{4}$ . You will find a drawing of the set $K$ below. Domain K The function $\varphi$ must verifiy $$\partial_{11} \partial_{33} \varphi = 0 \ \ on \ \ K$$ with the following boundary conditions on the side $\Sigma_1$ and $\Sigma_2$ , where we note $a=\sqrt{h^2-x^2-y^2}$ $$\varphi(x,y,a)= x a + f(y) \ \ and \ \ \partial_3 \varphi(x,y,a)=x$$ $$\varphi(x,y,2-a)= -x a + g(y) \ \ and \ \ \partial_3 \varphi(x,y,2-a)=x$$ for all $x,y$ such as $0 \leq x^2 + y^2 \leq \frac{1}{16}$ , where $f,g$ are two given functions of $y$ . The difficulty here is to deal with the cross derivatives condition $\partial_{11} \partial_{33} \varphi=0$ . I had to deal previously with the same kind of problem but with the condition $\partial_{3333} \varphi =0 \ \ on \ \ K$ and the construction was much more easier. Does anyone have any ideas or advices that could help me on that problem ? Please feel free to ask me questions if you need more details.","I am looking for a function where with the union of two spheres of radius . You will find a drawing of the set below. Domain K The function must verifiy with the following boundary conditions on the side and , where we note for all such as , where are two given functions of . The difficulty here is to deal with the cross derivatives condition . I had to deal previously with the same kind of problem but with the condition and the construction was much more easier. Does anyone have any ideas or advices that could help me on that problem ? Please feel free to ask me questions if you need more details.","\varphi \in C^{\infty}(K) K=\{(x,y,z) \in \mathbb{R}^3 \  | \ x^2+y^2 \leq 1/16 \ \ and \ \ 0 \leq z\leq 2\} \setminus B B=\{(x,y,z) \in \mathbb{R}^3 \  | \ x^2+y^2+z^2 \leq h^2 \} \cup \{(x,y,z) \in \mathbb{R}^3 \  | \ x^2+y^2+(z-2)^2 \leq h^2 \} 1>h>\frac{1}{4} K \varphi \partial_{11} \partial_{33} \varphi = 0 \ \ on \ \ K \Sigma_1 \Sigma_2 a=\sqrt{h^2-x^2-y^2} \varphi(x,y,a)= x a + f(y) \ \ and \ \ \partial_3 \varphi(x,y,a)=x \varphi(x,y,2-a)= -x a + g(y) \ \ and \ \ \partial_3 \varphi(x,y,2-a)=x x,y 0 \leq x^2 + y^2 \leq \frac{1}{16} f,g y \partial_{11} \partial_{33} \varphi=0 \partial_{3333} \varphi =0 \ \ on \ \ K","['derivatives', 'partial-differential-equations', 'calculus-of-variations']"
8,Derivatives car velocity paradox,Derivatives car velocity paradox,,"This is my first post on the Maths section of StackExchange. I am a high school student from Greece studying for IAL Math and got a bit confused on derivatives while watching this video; specifically on minute 14:29. The author gives an example of a car traveling on the curve $x^3$ where $x \in [0, 3]$ . He then asks if the car is moving or not at $x = 0$ Let's use $x^2$ instead of $x^3$ here for simplicity. Through the use of the derivative of $x^2$ , $\frac{dy}{dx} = 2x$ we see that at $x=0$ the velocity of the car is $\frac{dy}{dx}=2(0)=0$ which suggests that the car is not moving. I can see, however, the point the author is making, arguing that the car is actually moving, since the derivative formula for $x^2$ is derived as such: $ \lim\limits_{h \to 0} \frac{f(x+h)-f(x)}{h}= \lim\limits_{h \to 0} \frac{(x+h)^2-x^2}{h}= \lim\limits_{h \to 0} \frac{x^2+2hx+h^2-x^2}{h}= \lim\limits_{h \to 0} \frac{2hx+h^2}{h}= \lim\limits_{h \to 0} 2x + h = 2x $ where in the penultimate step we can see that the derivative formula is actually $2x$ plus some very very small quantity because $h$ doesn't actually ever reach zero. So is the car actually moving with its velocity being that very very small quantity? How can someone first argue that $h$ approaching zero can be neglected from the equation(last step) and subsequently contradict himself and argue that $h$ approaching zero should actually be included? Is it safe to assume that for $\frac{dy}{dx}=0$ an object is stationary? (In the context where $y$ is the distance travelled and $x$ is time) Moreover, for $x=0$ the derivative formula's result is $0$ which should also be the gradient of the line tangent to the parabola at $x=0$ And indeed, the gradient of the tangent line is $y=0$ which crosses $y=x^2$ only at point $0$ and not at point $0$ plus some very very small quantity ( Desmos ) I am very confused by this and would really appreciate help. I have also tried to make my questions as clear as possible but since I do not seem to understand calculus and limits all that well I will be more than willing to provide any clarifications.","This is my first post on the Maths section of StackExchange. I am a high school student from Greece studying for IAL Math and got a bit confused on derivatives while watching this video; specifically on minute 14:29. The author gives an example of a car traveling on the curve where . He then asks if the car is moving or not at Let's use instead of here for simplicity. Through the use of the derivative of , we see that at the velocity of the car is which suggests that the car is not moving. I can see, however, the point the author is making, arguing that the car is actually moving, since the derivative formula for is derived as such: where in the penultimate step we can see that the derivative formula is actually plus some very very small quantity because doesn't actually ever reach zero. So is the car actually moving with its velocity being that very very small quantity? How can someone first argue that approaching zero can be neglected from the equation(last step) and subsequently contradict himself and argue that approaching zero should actually be included? Is it safe to assume that for an object is stationary? (In the context where is the distance travelled and is time) Moreover, for the derivative formula's result is which should also be the gradient of the line tangent to the parabola at And indeed, the gradient of the tangent line is which crosses only at point and not at point plus some very very small quantity ( Desmos ) I am very confused by this and would really appreciate help. I have also tried to make my questions as clear as possible but since I do not seem to understand calculus and limits all that well I will be more than willing to provide any clarifications.","x^3 x \in [0, 3] x = 0 x^2 x^3 x^2 \frac{dy}{dx} = 2x x=0 \frac{dy}{dx}=2(0)=0 x^2 
\lim\limits_{h \to 0} \frac{f(x+h)-f(x)}{h}=
\lim\limits_{h \to 0} \frac{(x+h)^2-x^2}{h}=
\lim\limits_{h \to 0} \frac{x^2+2hx+h^2-x^2}{h}=
\lim\limits_{h \to 0} \frac{2hx+h^2}{h}=
\lim\limits_{h \to 0} 2x + h =
2x
 2x h h h \frac{dy}{dx}=0 y x x=0 0 x=0 y=0 y=x^2 0 0","['calculus', 'derivatives']"
9,How to prove monotonicity of this function?,How to prove monotonicity of this function?,,"Let $a>0$ . How to prove that the function: $$f(x)=\frac{a x-1}{\log(a x)}\cdot\frac{\log x}{x-1},$$ is monotonic (depending on $a<1$ or $a>1$ ). I know that we can calculate the derivative and determine its sign, but this needs much of calculation. I'm wondering if we can decide the monotonicity using a simple trick.","Let . How to prove that the function: is monotonic (depending on or ). I know that we can calculate the derivative and determine its sign, but this needs much of calculation. I'm wondering if we can decide the monotonicity using a simple trick.","a>0 f(x)=\frac{a x-1}{\log(a x)}\cdot\frac{\log x}{x-1}, a<1 a>1","['real-analysis', 'derivatives', 'logarithms', 'monotone-functions']"
10,Show that the following function is differentiable,Show that the following function is differentiable,,"I'm trying to solve the following question: Let $U$ be a open set in $\mathbb{R}^n$ and $A: U \rightarrow \mathcal{L}(\mathbb{R}^n,\mathbb{R}^n)$ a differentiable aplication, with $\mathcal{L}(\mathbb{R}^n, \mathbb{R}^n)$ being the space of all linear applications $T: \mathbb{R}^n \rightarrow \mathbb{R}^n.$ Show that $\phi: U \rightarrow \mathbb{R}$ defined by $\phi(x) = \left \langle A(x)x, x\right \rangle$ is differentiable and find $\phi'(x).$ One of my main problems is that $A$ is very much not a linear aplication, and all the tecniques that I know about differentiation seems to fail on this problem. I'm thinking that if I show, somehow, that $\phi$ is a composition of differentiable function, it may work, but I can't think of any combination that does the job. Any ideas on how to proceed?","I'm trying to solve the following question: Let be a open set in and a differentiable aplication, with being the space of all linear applications Show that defined by is differentiable and find One of my main problems is that is very much not a linear aplication, and all the tecniques that I know about differentiation seems to fail on this problem. I'm thinking that if I show, somehow, that is a composition of differentiable function, it may work, but I can't think of any combination that does the job. Any ideas on how to proceed?","U \mathbb{R}^n A: U \rightarrow \mathcal{L}(\mathbb{R}^n,\mathbb{R}^n) \mathcal{L}(\mathbb{R}^n, \mathbb{R}^n) T: \mathbb{R}^n \rightarrow \mathbb{R}^n. \phi: U \rightarrow \mathbb{R} \phi(x) = \left \langle A(x)x, x\right \rangle \phi'(x). A \phi","['real-analysis', 'derivatives', 'inner-products']"
11,Properties of a derivative on a compact interval,Properties of a derivative on a compact interval,,"Suppose a function $F$ is differentiable on an interval $(a,b) \supset [0,1]$ . Denote its derivative by $f$ , and suppose that $f > 0$ on $[0,1]$ . Question 1: Is it true that $f$ can be bounded away from $0$ on $[0,1]$ , i.e. that there exists some $c > 0$ such that $f(x) > c$ for all $x \in [0,1]$ ? If $f$ is continuous, this is clearly true, as a continuous function attains its minimum on a compact set, and this minimum is $> 0$ by assumption. If $f$ were an arbitrary function (not a derivative), this is clearly false; for instance, consider the function $f(x) = 1$ when $x = 0$ and $f(x) = x$ elsewhere. But this function has a jump discontinuity, and therefore is not the derivative of any function. Question 2: Is it true that $f$ is bounded on $[0,1]$ ? Note that if we remove the $f > 0$ requirement, this is not true (for instance, consider $F(x) = x^2 \sin(1/x^2)$ , with $F(0) = 0$ ; $F$ is differentiable, so $f$ exists, but $f$ is not bounded). This question may be relevant, but it doesn't directly answer the above.","Suppose a function is differentiable on an interval . Denote its derivative by , and suppose that on . Question 1: Is it true that can be bounded away from on , i.e. that there exists some such that for all ? If is continuous, this is clearly true, as a continuous function attains its minimum on a compact set, and this minimum is by assumption. If were an arbitrary function (not a derivative), this is clearly false; for instance, consider the function when and elsewhere. But this function has a jump discontinuity, and therefore is not the derivative of any function. Question 2: Is it true that is bounded on ? Note that if we remove the requirement, this is not true (for instance, consider , with ; is differentiable, so exists, but is not bounded). This question may be relevant, but it doesn't directly answer the above.","F (a,b) \supset [0,1] f f > 0 [0,1] f 0 [0,1] c > 0 f(x) > c x \in [0,1] f > 0 f f(x) = 1 x = 0 f(x) = x f [0,1] f > 0 F(x) = x^2 \sin(1/x^2) F(0) = 0 F f f","['real-analysis', 'calculus', 'derivatives', 'examples-counterexamples', 'upper-lower-bounds']"
12,Function is partial differentiable but has no total derivative,Function is partial differentiable but has no total derivative,,"I have to give an example of a function $f \colon \mathbb{R}^2 \to \mathbb{R}$ that has partial derivatives $(\partial_1f)(0,0) = 1 = (\partial_2f)(0,0)$ but no total derivative in $(0,0)$. I think that the function  $$   f \colon \mathbb{R}^2 \to \mathbb{R},   \quad    f(x,y) =   \begin{cases}   \frac{xy}{x^2 + y^2} + x + y & \text{if $(x,y) \neq (0,0)$} \\   0 & \text{if $(x,y) = (0,0)$}    \end{cases} $$ is an example, I'm just not sure if my proof is correct. We know that $f$ is total differentiable if $(x,y) \neq (0,0)$ because the partial derivatives exist and are continuous. $f$ is also partial differentiable if $(x,y) = (0,0)$ because $f(x,0) = x$ and $f(0,y) = y$ and they are given by $(\partial_1 f)(x,0) = 1 = (\partial_2f)(0,y) = 1$. To proof that $f$ is total differentiable in $(0,0)$ we now only have to proof that  $$     f(x,y)   = (1,1)     \begin{pmatrix}       x \\       y \\     \end{pmatrix}     + o(\| (x,y) \|)   \qquad   \text{when $(x,y) \to (0,0)$} $$ but this doesn't hold when $x=y=t$ and $t \to 0$ and so $f$ is not total differentiable when $(x,y) = (0,0)$. Can someone confirm if this is correct?","I have to give an example of a function $f \colon \mathbb{R}^2 \to \mathbb{R}$ that has partial derivatives $(\partial_1f)(0,0) = 1 = (\partial_2f)(0,0)$ but no total derivative in $(0,0)$. I think that the function  $$   f \colon \mathbb{R}^2 \to \mathbb{R},   \quad    f(x,y) =   \begin{cases}   \frac{xy}{x^2 + y^2} + x + y & \text{if $(x,y) \neq (0,0)$} \\   0 & \text{if $(x,y) = (0,0)$}    \end{cases} $$ is an example, I'm just not sure if my proof is correct. We know that $f$ is total differentiable if $(x,y) \neq (0,0)$ because the partial derivatives exist and are continuous. $f$ is also partial differentiable if $(x,y) = (0,0)$ because $f(x,0) = x$ and $f(0,y) = y$ and they are given by $(\partial_1 f)(x,0) = 1 = (\partial_2f)(0,y) = 1$. To proof that $f$ is total differentiable in $(0,0)$ we now only have to proof that  $$     f(x,y)   = (1,1)     \begin{pmatrix}       x \\       y \\     \end{pmatrix}     + o(\| (x,y) \|)   \qquad   \text{when $(x,y) \to (0,0)$} $$ but this doesn't hold when $x=y=t$ and $t \to 0$ and so $f$ is not total differentiable when $(x,y) = (0,0)$. Can someone confirm if this is correct?",,"['real-analysis', 'derivatives', 'partial-derivative']"
13,Is a map between Riemannian manifolds with bounded derivative Lipschitz?,Is a map between Riemannian manifolds with bounded derivative Lipschitz?,,"Let $X,Y$ be Riemannian manifolds. Let $f:X \to Y$ be an everywhere differentiable map, such that the operator norm $\|df_p\|_{op}$ is globally bounded (from above) by $L$. Is it true that $f$ is Lipchitz? (what about $L$-Lipschitzity?) I am interested in the general question in the case of low regularity , i.e I do not assume $f$ is $C^1$ (then it's quite easy ). In particular this means that for a smooth path $\alpha:I \to X$, we do not know a-priori that $f\circ \alpha$ is Lipschitz, so we cannot necessarily write its length as the integral of its speed. Note that in the case where $X,Y$ are Euclidean spaces, the answer is positive and is known as the ""mean value inequality"". However, the standard proof does not seem to pass over to the general case. Updated ""conjecture"": I guess $f$ does have to be Lipschitz, and even $L$-Lipschitz. First, since locally the Riemannian metric is ""close"" to Euclidean metric, I guess one could claim local-Lipschitzity certainly holds (admittedly, some details are probably required to justify this rigorously). Then, for any smooth path $\alpha$ in $X$, $f \circ \alpha$ will also be locally-Lipchitz and (compactness?) even Lipchitz. So we can express its length as the integral of its speed, so $f$ will be $L$-Lipschitz by the usual argument.","Let $X,Y$ be Riemannian manifolds. Let $f:X \to Y$ be an everywhere differentiable map, such that the operator norm $\|df_p\|_{op}$ is globally bounded (from above) by $L$. Is it true that $f$ is Lipchitz? (what about $L$-Lipschitzity?) I am interested in the general question in the case of low regularity , i.e I do not assume $f$ is $C^1$ (then it's quite easy ). In particular this means that for a smooth path $\alpha:I \to X$, we do not know a-priori that $f\circ \alpha$ is Lipschitz, so we cannot necessarily write its length as the integral of its speed. Note that in the case where $X,Y$ are Euclidean spaces, the answer is positive and is known as the ""mean value inequality"". However, the standard proof does not seem to pass over to the general case. Updated ""conjecture"": I guess $f$ does have to be Lipschitz, and even $L$-Lipschitz. First, since locally the Riemannian metric is ""close"" to Euclidean metric, I guess one could claim local-Lipschitzity certainly holds (admittedly, some details are probably required to justify this rigorously). Then, for any smooth path $\alpha$ in $X$, $f \circ \alpha$ will also be locally-Lipchitz and (compactness?) even Lipchitz. So we can express its length as the integral of its speed, so $f$ will be $L$-Lipschitz by the usual argument.",,"['differential-geometry', 'derivatives', 'metric-spaces', 'riemannian-geometry', 'lipschitz-functions']"
14,Deriving things like chain rule,Deriving things like chain rule,,I wanted to see how much calculus I could derive from the ground up using basic definitions. I was able to show that for example \begin{align} \frac{d}{dx}cx^n &= \lim_{h \to 0} \frac{c \cdot(x+h)^n - cx^n}{h} \\&= \lim_{h \to 0} \frac{c}{h} \cdot((x+h)^n - x^n) \\&= \lim_{h \to 0} \frac{c}{h} \cdot\left(-x^n + \sum_{k=0}^{n}\binom{n}{k}x^kh^{n-k}\right) \\&= \lim_{h \to 0} \frac{c}{h} \cdot\left(\sum_{k=0}^{n-1}\binom{n}{k}x^kh^{n-k}\right) \\&= \lim_{h \to 0} \frac{c}{h} \cdot\left(nh\sum_{k=0}^{n-1}\binom{n-1}{k}x^kh^{n-1-k}\right) \\&= \lim_{h \to 0} \frac{c}{h} \cdot\left(nh(x+h)^{n-1}\right) \\&= \lim_{h \to 0} cn(x+h)^{n-1} \\&= cnx^{n-1} \end{align} Which surprised me that it actually worked! However when I tried the same thing for chain rule I was hopelessly stuck: \begin{align} \frac{d}{dx}f(g(x))&= \lim_{h \to 0} \frac{f(g(x+h)) - f(g(x))}{h} \end{align} And sadly that is as far as I could go! I have no idea how you're supposed to simplify that any further. Or should it be \begin{align} \frac{d}{dx}f(g(x))&= \lim_{h \to 0} \frac{f(g(x)+h) - f(g(x))}{h} \end{align} instead?,I wanted to see how much calculus I could derive from the ground up using basic definitions. I was able to show that for example \begin{align} \frac{d}{dx}cx^n &= \lim_{h \to 0} \frac{c \cdot(x+h)^n - cx^n}{h} \\&= \lim_{h \to 0} \frac{c}{h} \cdot((x+h)^n - x^n) \\&= \lim_{h \to 0} \frac{c}{h} \cdot\left(-x^n + \sum_{k=0}^{n}\binom{n}{k}x^kh^{n-k}\right) \\&= \lim_{h \to 0} \frac{c}{h} \cdot\left(\sum_{k=0}^{n-1}\binom{n}{k}x^kh^{n-k}\right) \\&= \lim_{h \to 0} \frac{c}{h} \cdot\left(nh\sum_{k=0}^{n-1}\binom{n-1}{k}x^kh^{n-1-k}\right) \\&= \lim_{h \to 0} \frac{c}{h} \cdot\left(nh(x+h)^{n-1}\right) \\&= \lim_{h \to 0} cn(x+h)^{n-1} \\&= cnx^{n-1} \end{align} Which surprised me that it actually worked! However when I tried the same thing for chain rule I was hopelessly stuck: \begin{align} \frac{d}{dx}f(g(x))&= \lim_{h \to 0} \frac{f(g(x+h)) - f(g(x))}{h} \end{align} And sadly that is as far as I could go! I have no idea how you're supposed to simplify that any further. Or should it be \begin{align} \frac{d}{dx}f(g(x))&= \lim_{h \to 0} \frac{f(g(x)+h) - f(g(x))}{h} \end{align} instead?,,"['calculus', 'derivatives', 'chain-rule']"
15,Set of values of $a$ for which function always increases,Set of values of  for which function always increases,a,"If the set of all values of the parameter $a$ for which the function $$f(x)=\sin (2x)-8(a+1) \sin x+(4a^2+8a-14)x$$ increases for all $x \in R$ and has no critical point for all $x \in R$ is $(- \infty, m- \sqrt n) \cup (\sqrt n, \infty)$, then find the value of $m^2+n^2$ (where $m,n$ are prime numbers). After finding $f'(x)$, we need to set $f'(x)>0$ but I am not having any clue how to extract condition on $a$? Could someone help me with this?","If the set of all values of the parameter $a$ for which the function $$f(x)=\sin (2x)-8(a+1) \sin x+(4a^2+8a-14)x$$ increases for all $x \in R$ and has no critical point for all $x \in R$ is $(- \infty, m- \sqrt n) \cup (\sqrt n, \infty)$, then find the value of $m^2+n^2$ (where $m,n$ are prime numbers). After finding $f'(x)$, we need to set $f'(x)>0$ but I am not having any clue how to extract condition on $a$? Could someone help me with this?",,"['calculus', 'derivatives', 'maxima-minima']"
16,derivative for a vector,derivative for a vector,,"Confused about how to get formula from (12.4) to (12.5), which derivative respect to $u_1$? I understand how to calculate derivative for a scalar parameter, but confused how to calculate derivative for a vector parameter like $u_1$, especially when there is both $u_1$ and transpose of $u^T_1$. If anyone could provide a bit more details, it will be great. The context of the question is from PCA.","Confused about how to get formula from (12.4) to (12.5), which derivative respect to $u_1$? I understand how to calculate derivative for a scalar parameter, but confused how to calculate derivative for a vector parameter like $u_1$, especially when there is both $u_1$ and transpose of $u^T_1$. If anyone could provide a bit more details, it will be great. The context of the question is from PCA.",,"['derivatives', 'vector-spaces', 'vectors']"
17,Find the number of solutions of $x^4-x\sin(\pi x) - \cos^3(\pi x)=0$,Find the number of solutions of,x^4-x\sin(\pi x) - \cos^3(\pi x)=0,"We are required to find the number of solutions in $\mathbb{R}$ of the equation $$x^4-x\sin(\pi x) - \cos^3(\pi x)=0 \tag{1}$$ This is a problem on the behaviour of functions and is likely based on some relevant properties of the derivatives of the above equation. In particular, finding the solutions explicitly is ruled out. I made the following observations: The left hand side of $(1)$ is even and since $x=0$ is not a solution only an even number of solutions exist. The LHS involves an $x^4$ term, and so after a sufficiently large $x$, the $x^4$ term dominates and no solutions exist. It is not difficult to show that no solutions exist for $x>2$.","We are required to find the number of solutions in $\mathbb{R}$ of the equation $$x^4-x\sin(\pi x) - \cos^3(\pi x)=0 \tag{1}$$ This is a problem on the behaviour of functions and is likely based on some relevant properties of the derivatives of the above equation. In particular, finding the solutions explicitly is ruled out. I made the following observations: The left hand side of $(1)$ is even and since $x=0$ is not a solution only an even number of solutions exist. The LHS involves an $x^4$ term, and so after a sufficiently large $x$, the $x^4$ term dominates and no solutions exist. It is not difficult to show that no solutions exist for $x>2$.",,"['calculus', 'derivatives']"
18,"Fixed point, bounded derivative","Fixed point, bounded derivative",,"Let $p\in\mathbb{N}$. Let $f:I\to\mathbb{R}$ differentiable in the closed interval $I$ (bounded or not), with $f(I) \subset I$, and let $g = f\circ f\circ \cdots \circ f = f^p$, where $\circ$ means composition. If $\lvert g'(x) \rvert \leq c < 1, \forall x\in I$ where $c$ is a constant, show that there is only one $a\in I$ such that $f(a) = a$. Some thoughts I proved it for $p=1$, i.e. $g = f$ and $\lvert g'(x) \rvert = \lvert f'(x) \rvert \leq c < 1$. Notice that $I$ can be unbounded so the existence of the fixed point is not trivial. For the task, I use two results: Let $f:I\to\mathbb{R}$ differentiable in the interval $I$. The function $f$ satisfies $\lvert f(x) - f(y) \rvert \leq c\lvert x - y \rvert$ for all $x,y\in I$ if and only if $\lvert f'(x) \rvert \leq c$ for all $x\in I$. Let $0\leq c < 1$. If the sequence $(x_n)$ is such that $\lvert x_{n+2} - x_{n+1}\rvert \leq c\lvert x_{n+1} - x_n\rvert $ for all $n\in \mathbb{N}$, then the sequence $(x_n)$ converges. Now take any point $x_0\in I$ and build the following sequence: $x_1 = g(x_0), x_2 = g(x_1), \cdots, x_n = g(x_{n-1}), \cdots$. Since $\lvert g'(x) \rvert \leq c$, from result 1 we have $\lvert g(x_{n+1}) - g(x_{n}) \rvert \leq c\lvert x_{n+1} - x_{n} \rvert$, then $\lvert x_{n+2} - x_{n+1} \rvert \leq c\lvert x_{n+1} - x_{n} \rvert$. Since $0\leq c < 1$, from result 2 we have $(x_n)\to a$. Since $I$ is closed, $a\in I$. Finally, since $x_n = g(x_{n-1})$, this means $g(a) = a$. The uniqueness comes from supposing another $b$ and applying IVT to get contradiction. I am having trouble trying to prove for $p > 1$. Update: I marked Matthew's suggestion because it contained the key idea for the solution. The remaining bits for the solution are in the comments to his suggestion.","Let $p\in\mathbb{N}$. Let $f:I\to\mathbb{R}$ differentiable in the closed interval $I$ (bounded or not), with $f(I) \subset I$, and let $g = f\circ f\circ \cdots \circ f = f^p$, where $\circ$ means composition. If $\lvert g'(x) \rvert \leq c < 1, \forall x\in I$ where $c$ is a constant, show that there is only one $a\in I$ such that $f(a) = a$. Some thoughts I proved it for $p=1$, i.e. $g = f$ and $\lvert g'(x) \rvert = \lvert f'(x) \rvert \leq c < 1$. Notice that $I$ can be unbounded so the existence of the fixed point is not trivial. For the task, I use two results: Let $f:I\to\mathbb{R}$ differentiable in the interval $I$. The function $f$ satisfies $\lvert f(x) - f(y) \rvert \leq c\lvert x - y \rvert$ for all $x,y\in I$ if and only if $\lvert f'(x) \rvert \leq c$ for all $x\in I$. Let $0\leq c < 1$. If the sequence $(x_n)$ is such that $\lvert x_{n+2} - x_{n+1}\rvert \leq c\lvert x_{n+1} - x_n\rvert $ for all $n\in \mathbb{N}$, then the sequence $(x_n)$ converges. Now take any point $x_0\in I$ and build the following sequence: $x_1 = g(x_0), x_2 = g(x_1), \cdots, x_n = g(x_{n-1}), \cdots$. Since $\lvert g'(x) \rvert \leq c$, from result 1 we have $\lvert g(x_{n+1}) - g(x_{n}) \rvert \leq c\lvert x_{n+1} - x_{n} \rvert$, then $\lvert x_{n+2} - x_{n+1} \rvert \leq c\lvert x_{n+1} - x_{n} \rvert$. Since $0\leq c < 1$, from result 2 we have $(x_n)\to a$. Since $I$ is closed, $a\in I$. Finally, since $x_n = g(x_{n-1})$, this means $g(a) = a$. The uniqueness comes from supposing another $b$ and applying IVT to get contradiction. I am having trouble trying to prove for $p > 1$. Update: I marked Matthew's suggestion because it contained the key idea for the solution. The remaining bits for the solution are in the comments to his suggestion.",,"['calculus', 'derivatives', 'fixed-point-theorems']"
19,"The function $\mathrm{Li}_2(x)=\int_2^x\frac{dt}{\log^2t}$, its inverse and summation","The function , its inverse and summation",\mathrm{Li}_2(x)=\int_2^x\frac{dt}{\log^2t},"I am reading the more understandable mathematics in the section Preliminary Results of a paper in which the authors give a explanation of facts for the logarithmic integral and its inverse. In this post I ask about similar equations but now for a different logarithmic integral function, now I consider $\mathrm{Li}_2(x)=\int_2^x\frac{dt}{\log^2t}$, and its inverse $\mathrm{Li}_2^{-1}(x)$. My goal is understand more these facts (the genuine computations from the authors), and refresh basics about analysis. I know the relation between a function that has inverse, its graph, the function $x$ and the inverse function (I write this because could be useful to find the asyptotic that I ask in my question, I don't know how deduce it now). I can write and know that $$\mathrm{Li}_2(x)\sim\frac{(1+o(1))x}{\log^2x}\quad\text{  as }x\to\infty,$$ but, Question 1. Can you give the correspoding and similar asymptotic for $\mathrm{Li}_2^{-1}(x)$, as $x\to\infty$? Here, as I've said $\mathrm{Li}_2^{-1}(x)$ is the inverse function of the logarithmic integral $\int_2^x\frac{dt}{\log^2t}$. I know that by differentiation of a inverse function and by the Fundamental Theorem of Calculus I can write my explanation for a similar equation for $\mathrm{Li}_2(x)$ (following the equation that the authors write for the logarithmic integral) $$\left(\mathrm{Li}_2^{-1}\right)'\left(\mathrm{Li}_2(x)\right)=\frac{1}{\frac{d}{dx}\left(\int_2^x\frac{dt}{\log^2t}\right)}=\log^2x,$$ and combining this last with the composition with $\mathrm{Li}^{-1}_2(x)$ one has the coresponding equation and compute $\left(\mathrm{Li}_2^{-1}\right)'(x)$ as  $$\left(\mathrm{Li}^{-1}_2\right)'\left(\mathrm{Li}_2\left(\mathrm{Li}_2^{-1}(x)\right)\right)=\log^2\left(\mathrm{Li}_2^{-1}(x)\right).$$ Question 2. Can you use the asymptotic that you've computed as answer of Question 1 , to get an asymptotic as $x\to\infty$ for $\log^2\left(\mathrm{Li}_2^{-1}(x)\right)$? I believe, following similar computations that authors give for their logarithmic integral function, that now I with $\mathrm{Li}_2(x)$ can write $$\sum_{1<m<n}\mathrm{Li}^{-1}_2(x)=\int_2^{\mathrm{Li}_2^{-1}(n)}\frac{tdt}{\log^2t}+O\left(\sum_{1\leq m\leq n}\log^2 m\right).$$ Question 3 (Now this question is optional, thus    it is not necessary to answer). I say that I've asked to me if previous identity is right when I write the similar statement for which I believe that holds. Can you refute previous statement? Can you prove it using previous answers to my questions (I believe that with these the authors can prove their identity), or at least give a reasonable approximation for other methods (I say perhaps partial summation)?","I am reading the more understandable mathematics in the section Preliminary Results of a paper in which the authors give a explanation of facts for the logarithmic integral and its inverse. In this post I ask about similar equations but now for a different logarithmic integral function, now I consider $\mathrm{Li}_2(x)=\int_2^x\frac{dt}{\log^2t}$, and its inverse $\mathrm{Li}_2^{-1}(x)$. My goal is understand more these facts (the genuine computations from the authors), and refresh basics about analysis. I know the relation between a function that has inverse, its graph, the function $x$ and the inverse function (I write this because could be useful to find the asyptotic that I ask in my question, I don't know how deduce it now). I can write and know that $$\mathrm{Li}_2(x)\sim\frac{(1+o(1))x}{\log^2x}\quad\text{  as }x\to\infty,$$ but, Question 1. Can you give the correspoding and similar asymptotic for $\mathrm{Li}_2^{-1}(x)$, as $x\to\infty$? Here, as I've said $\mathrm{Li}_2^{-1}(x)$ is the inverse function of the logarithmic integral $\int_2^x\frac{dt}{\log^2t}$. I know that by differentiation of a inverse function and by the Fundamental Theorem of Calculus I can write my explanation for a similar equation for $\mathrm{Li}_2(x)$ (following the equation that the authors write for the logarithmic integral) $$\left(\mathrm{Li}_2^{-1}\right)'\left(\mathrm{Li}_2(x)\right)=\frac{1}{\frac{d}{dx}\left(\int_2^x\frac{dt}{\log^2t}\right)}=\log^2x,$$ and combining this last with the composition with $\mathrm{Li}^{-1}_2(x)$ one has the coresponding equation and compute $\left(\mathrm{Li}_2^{-1}\right)'(x)$ as  $$\left(\mathrm{Li}^{-1}_2\right)'\left(\mathrm{Li}_2\left(\mathrm{Li}_2^{-1}(x)\right)\right)=\log^2\left(\mathrm{Li}_2^{-1}(x)\right).$$ Question 2. Can you use the asymptotic that you've computed as answer of Question 1 , to get an asymptotic as $x\to\infty$ for $\log^2\left(\mathrm{Li}_2^{-1}(x)\right)$? I believe, following similar computations that authors give for their logarithmic integral function, that now I with $\mathrm{Li}_2(x)$ can write $$\sum_{1<m<n}\mathrm{Li}^{-1}_2(x)=\int_2^{\mathrm{Li}_2^{-1}(n)}\frac{tdt}{\log^2t}+O\left(\sum_{1\leq m\leq n}\log^2 m\right).$$ Question 3 (Now this question is optional, thus    it is not necessary to answer). I say that I've asked to me if previous identity is right when I write the similar statement for which I believe that holds. Can you refute previous statement? Can you prove it using previous answers to my questions (I believe that with these the authors can prove their identity), or at least give a reasonable approximation for other methods (I say perhaps partial summation)?",,"['derivatives', 'asymptotics']"
20,explanation of $ \frac{dy}{dx} = \frac{1}{\frac{dx}{dy}} $?,explanation of ?, \frac{dy}{dx} = \frac{1}{\frac{dx}{dy}} ,"I'm studying about derivative of inverse function. The teacher in the video ( https://www.youtube.com/watch?v=3ReOtNCYuBw ) (at 9:00 minute) said this if a differentiable function, f has an inverse, then: $$ \frac{d}{dx}[f^{-1}(x)] = \frac{1}{f'[f^{-1}(x)]} $$ provided $f'[f^{-1}(x)]\neq 0$ then he said if we make $y = f^{-1}(x)$ then: $$ \frac{dy}{dx} = \frac{1}{\frac{dx}{dy}} $$ the last line is when I really get lost because it should be $ \frac{dy}{dx} = \frac{1}{f'[y]} $ not $ \frac{dy}{dx} = \frac{1}{\frac{dx}{dy}} $ isn't it? any please explain to me in very detail, I'm a newbie.","I'm studying about derivative of inverse function. The teacher in the video ( https://www.youtube.com/watch?v=3ReOtNCYuBw ) (at 9:00 minute) said this if a differentiable function, f has an inverse, then: $$ \frac{d}{dx}[f^{-1}(x)] = \frac{1}{f'[f^{-1}(x)]} $$ provided $f'[f^{-1}(x)]\neq 0$ then he said if we make $y = f^{-1}(x)$ then: $$ \frac{dy}{dx} = \frac{1}{\frac{dx}{dy}} $$ the last line is when I really get lost because it should be $ \frac{dy}{dx} = \frac{1}{f'[y]} $ not $ \frac{dy}{dx} = \frac{1}{\frac{dx}{dy}} $ isn't it? any please explain to me in very detail, I'm a newbie.",,"['calculus', 'derivatives', 'inverse']"
21,Is a series of successive derivatives known/useful?,Is a series of successive derivatives known/useful?,,"So, while trying to find something else, it looks like I've found, for many $f(x)$: $$f(x) + f'(x) + f''(x) + f^{(3)}(x) + \dots + f^{(n)}(x)$$ Assuming that there is an easy way to find this sum above, is there any use for it?  I will elaborate a bit.  I mean that I believe I have found a method that finds the sum of all of the derivatives above, and is much faster than calculating each derivative.  In fact, it seems that calculating the sum above for most functions isn't much harder than calculating $f^{(n)}(x)$, and it also should give a ""closed form"" of elementary expressions for most $f(x)$. I have one example that comes to mind: a ""closed form"" for a partial sum of $e^x$, as in this question .  If my ideas work, we would have the closed form that this question asks for. So I'm wondering, is there anything else that this method is useful for? IMPORTANT NOTE I'm assuming that we have use of the ""fractional calculus"", which gives us the ability to calculate $f^{(n)}$ reasonably well and efficiently, using ""differintegrals"".  This may make the sum above fairly trivial.  I'm sorry if I misled anyone.","So, while trying to find something else, it looks like I've found, for many $f(x)$: $$f(x) + f'(x) + f''(x) + f^{(3)}(x) + \dots + f^{(n)}(x)$$ Assuming that there is an easy way to find this sum above, is there any use for it?  I will elaborate a bit.  I mean that I believe I have found a method that finds the sum of all of the derivatives above, and is much faster than calculating each derivative.  In fact, it seems that calculating the sum above for most functions isn't much harder than calculating $f^{(n)}(x)$, and it also should give a ""closed form"" of elementary expressions for most $f(x)$. I have one example that comes to mind: a ""closed form"" for a partial sum of $e^x$, as in this question .  If my ideas work, we would have the closed form that this question asks for. So I'm wondering, is there anything else that this method is useful for? IMPORTANT NOTE I'm assuming that we have use of the ""fractional calculus"", which gives us the ability to calculate $f^{(n)}$ reasonably well and efficiently, using ""differintegrals"".  This may make the sum above fairly trivial.  I'm sorry if I misled anyone.",,"['calculus', 'derivatives', 'intuition']"
22,With the exception of the Total Derivative; Is the Chain Rule valid when it has partial derivatives mixed with ordinary derivatives?,With the exception of the Total Derivative; Is the Chain Rule valid when it has partial derivatives mixed with ordinary derivatives?,,"I know that the chain rule for a function of one variable $y=f(x)$ is written as $$\frac{{\rm d}}{{\rm d}x}=\frac{{\rm d}}{{\rm d}y}\times \frac{{\rm d}y}{{\rm d}x}\tag{1}$$ I also know that if $u=f(x,y)$ then the total derivative is $${\rm d}u=\frac{\partial u}{\partial x}\cdot{\rm d} x+\frac{\partial u}{\partial y}\cdot{\rm d}y$$ But from $(1)$ is there any plausibility in changing all the derivatives to partial derivatives such that $$\frac{\partial}{\partial x}=\frac{\partial}{\partial y}\times \frac{\partial y}{\partial x}$$ when $y=f(x)$ or does the above formula only hold iff $y=f(x,y)$? To illustrate my confusion I will add this question and solution to give some context: Start of Question: If $f$ is an arbitrary function, show that $\psi(z,t)=f(z-vt)$ is a solution to the wave equation $$\frac{\partial^2\psi}{\partial t^2}=v^2\frac{\partial^2\psi}{\partial z^2}$$ by writing $f(z-vt)$ as $f(y)$ and using the chain rule to differentiate with respect to $t$ and $z$. End of Question. The following is a word for word copy of the solution. I have marked $\color{red}{\mathrm{red}}$ the parts of the solution for which I do not understand and the parts marked with $\color{#180}{\text{green underbraces}}$ are not part of the solution and represent where I think the author has made mistakes: Start of Solution: The wave equation is $$\frac{\partial^2\psi}{\partial t^2}-v^2\frac{\partial^2\psi}{\partial z^2}=0$$   Writing $$f^{\prime}(y)=\frac{{\rm d}f(y)}{{\rm d}y}$$ and    $$f^{\prime\prime}(y)=\frac{{\rm d}^2f(y)}{{\rm d}y^2}$$ then writing $y=z-vt$, $$\frac{\partial f(y)}{\partial z}=f^{\prime}(y)\underbrace{\color{red}{\frac{{\rm d}y}{{\rm d}z}}}_{\color{#180}{\Large\frac{\partial y}{\partial z}}}=f^{\prime}(y)\tag{A}$$ and    $$\frac{\partial^2 f(y)}{\partial z^2}=\frac{{\rm d}f^{\prime}(y)}{{\rm d}y} \underbrace{\color{red}{\frac{{\rm d}y}{{\rm d}z}}}_{\color{#180}{\Large\frac{\partial y}{\partial z}}}=f^{\prime\prime}(y)$$ Similarly, $$\frac{\partial f}{\partial t}=f^{\prime}(y)\underbrace{\color{red}{\frac{{\rm d}y}{{\rm d}t}}}_{\color{#180}{\Large\frac{\partial y}{\partial t}}}=-vf^{\prime}(y)$$ and    $$\frac{\partial^2 f(y)}{\partial z^2}=\frac{{\rm d}\left(-v f^{\prime}(y)\right)}{{\rm d}y}\underbrace{\color{red}{\frac{{\rm d}y}{{\rm d} z}}}_{\color{#180}{\Large\frac{\partial y}{\partial z}}}=v^2f^{\prime\prime}(y)$$   Substituting into the LHS of the wave equation gives $$\frac{\partial^2 f(z-vt)}{\partial z^2}-\frac{1}{v^2}\frac{\partial^2 f(z-vt)}{\partial t^2}=f^{\prime\prime}(z-vt)-\frac{1}{v^2}\left(v^2f^{\prime\prime}(z-vt)\right)=0$$ End of Solution. I have three questions regarding the solution above: Are the parts marked in $\color{#180}{\text{green underbraces}}$ correct? I think they should be partial derivatives as $y$ is a function of two variables ($z$ and $t$) since $y=z-vt$. Iff the $\color{red}{{\rm red}}$ text turns out to be correct then from $({\rm A})$ $$\frac{\partial f(y)}{\partial z}=\frac{{\rm d}f(y)}{{\rm d}y}\times \frac{{\rm d}y}{{\rm d}z}$$ How can this possibly be true? Since we have a partial derivative on the LHS and two ordinary derivatives on the RHS. I didn't know that you could 'mix' up derivatives like that. If this is indeed true could someone please explain it to me? Iff the $\color{#180}{\text{green underbraces}}$ turn out to be correct then from $({\rm A})$ $$\frac{\partial f(y)}{\partial z}=\frac{{\rm d}f(y)}{{\rm d} y} \times \frac{\partial y}{\partial z}$$ But this means that the LHS is partial while the RHS is a mixture. Either way round, I don't understand why the partial derivatives can be mixed with ordinary derivatives. Is anyone able to explain why you can mix partial and ordinary derivatives in the chain rule? Feel free to answer using examples if it's easier to explain that way.","I know that the chain rule for a function of one variable $y=f(x)$ is written as $$\frac{{\rm d}}{{\rm d}x}=\frac{{\rm d}}{{\rm d}y}\times \frac{{\rm d}y}{{\rm d}x}\tag{1}$$ I also know that if $u=f(x,y)$ then the total derivative is $${\rm d}u=\frac{\partial u}{\partial x}\cdot{\rm d} x+\frac{\partial u}{\partial y}\cdot{\rm d}y$$ But from $(1)$ is there any plausibility in changing all the derivatives to partial derivatives such that $$\frac{\partial}{\partial x}=\frac{\partial}{\partial y}\times \frac{\partial y}{\partial x}$$ when $y=f(x)$ or does the above formula only hold iff $y=f(x,y)$? To illustrate my confusion I will add this question and solution to give some context: Start of Question: If $f$ is an arbitrary function, show that $\psi(z,t)=f(z-vt)$ is a solution to the wave equation $$\frac{\partial^2\psi}{\partial t^2}=v^2\frac{\partial^2\psi}{\partial z^2}$$ by writing $f(z-vt)$ as $f(y)$ and using the chain rule to differentiate with respect to $t$ and $z$. End of Question. The following is a word for word copy of the solution. I have marked $\color{red}{\mathrm{red}}$ the parts of the solution for which I do not understand and the parts marked with $\color{#180}{\text{green underbraces}}$ are not part of the solution and represent where I think the author has made mistakes: Start of Solution: The wave equation is $$\frac{\partial^2\psi}{\partial t^2}-v^2\frac{\partial^2\psi}{\partial z^2}=0$$   Writing $$f^{\prime}(y)=\frac{{\rm d}f(y)}{{\rm d}y}$$ and    $$f^{\prime\prime}(y)=\frac{{\rm d}^2f(y)}{{\rm d}y^2}$$ then writing $y=z-vt$, $$\frac{\partial f(y)}{\partial z}=f^{\prime}(y)\underbrace{\color{red}{\frac{{\rm d}y}{{\rm d}z}}}_{\color{#180}{\Large\frac{\partial y}{\partial z}}}=f^{\prime}(y)\tag{A}$$ and    $$\frac{\partial^2 f(y)}{\partial z^2}=\frac{{\rm d}f^{\prime}(y)}{{\rm d}y} \underbrace{\color{red}{\frac{{\rm d}y}{{\rm d}z}}}_{\color{#180}{\Large\frac{\partial y}{\partial z}}}=f^{\prime\prime}(y)$$ Similarly, $$\frac{\partial f}{\partial t}=f^{\prime}(y)\underbrace{\color{red}{\frac{{\rm d}y}{{\rm d}t}}}_{\color{#180}{\Large\frac{\partial y}{\partial t}}}=-vf^{\prime}(y)$$ and    $$\frac{\partial^2 f(y)}{\partial z^2}=\frac{{\rm d}\left(-v f^{\prime}(y)\right)}{{\rm d}y}\underbrace{\color{red}{\frac{{\rm d}y}{{\rm d} z}}}_{\color{#180}{\Large\frac{\partial y}{\partial z}}}=v^2f^{\prime\prime}(y)$$   Substituting into the LHS of the wave equation gives $$\frac{\partial^2 f(z-vt)}{\partial z^2}-\frac{1}{v^2}\frac{\partial^2 f(z-vt)}{\partial t^2}=f^{\prime\prime}(z-vt)-\frac{1}{v^2}\left(v^2f^{\prime\prime}(z-vt)\right)=0$$ End of Solution. I have three questions regarding the solution above: Are the parts marked in $\color{#180}{\text{green underbraces}}$ correct? I think they should be partial derivatives as $y$ is a function of two variables ($z$ and $t$) since $y=z-vt$. Iff the $\color{red}{{\rm red}}$ text turns out to be correct then from $({\rm A})$ $$\frac{\partial f(y)}{\partial z}=\frac{{\rm d}f(y)}{{\rm d}y}\times \frac{{\rm d}y}{{\rm d}z}$$ How can this possibly be true? Since we have a partial derivative on the LHS and two ordinary derivatives on the RHS. I didn't know that you could 'mix' up derivatives like that. If this is indeed true could someone please explain it to me? Iff the $\color{#180}{\text{green underbraces}}$ turn out to be correct then from $({\rm A})$ $$\frac{\partial f(y)}{\partial z}=\frac{{\rm d}f(y)}{{\rm d} y} \times \frac{\partial y}{\partial z}$$ But this means that the LHS is partial while the RHS is a mixture. Either way round, I don't understand why the partial derivatives can be mixed with ordinary derivatives. Is anyone able to explain why you can mix partial and ordinary derivatives in the chain rule? Feel free to answer using examples if it's easier to explain that way.",,"['calculus', 'derivatives', 'partial-derivative', 'proof-explanation', 'chain-rule']"
23,How is $\frac{ds}{dt}$ related to $\frac{dx}{dt}$?,How is  related to ?,\frac{ds}{dt} \frac{dx}{dt},"The problem states: Let $x$ and $y$ be differentiable functions of $t$, and let $s = \sqrt{4x^2+6y^2}$ be a function of $x$ and $y$. How is $\frac{ds}{dt}$ related to $\frac{dx}{dt}$ if $y$ is constant? My attempt: \begin{align} \frac{ds}{dt} & = \frac{d}{dt} \left( \sqrt{4x^2+6y^2} \right) \\  & = \frac{1}{2\sqrt{4x^2+6y^2}}*\left(8x\frac{dx}{dt}+12y\frac{dy}{dt}\right) \end{align} If $y$ is constant, then $\frac{dy}{dt}=0$ \begin{align} \frac{ds}{dt} & = \frac{1}{2\sqrt{4x^2+6y^2}}*\left(8x\frac{dx}{dt}+12y*0\right)\\ & =\left(\frac{8x}{2\sqrt{4x^2+6y^2}}\right)*\frac{dx}{dt} \end{align} This is the relation I have found, but it is not correct. What is the relation between $\frac{ds}{dt}$ and $\frac{dx}{dt}$ ?","The problem states: Let $x$ and $y$ be differentiable functions of $t$, and let $s = \sqrt{4x^2+6y^2}$ be a function of $x$ and $y$. How is $\frac{ds}{dt}$ related to $\frac{dx}{dt}$ if $y$ is constant? My attempt: \begin{align} \frac{ds}{dt} & = \frac{d}{dt} \left( \sqrt{4x^2+6y^2} \right) \\  & = \frac{1}{2\sqrt{4x^2+6y^2}}*\left(8x\frac{dx}{dt}+12y\frac{dy}{dt}\right) \end{align} If $y$ is constant, then $\frac{dy}{dt}=0$ \begin{align} \frac{ds}{dt} & = \frac{1}{2\sqrt{4x^2+6y^2}}*\left(8x\frac{dx}{dt}+12y*0\right)\\ & =\left(\frac{8x}{2\sqrt{4x^2+6y^2}}\right)*\frac{dx}{dt} \end{align} This is the relation I have found, but it is not correct. What is the relation between $\frac{ds}{dt}$ and $\frac{dx}{dt}$ ?",,"['calculus', 'derivatives']"
24,Derivative of power series with nonnegative coefficients,Derivative of power series with nonnegative coefficients,,"Let $$f(x) = \sum_{k=0}^\infty a_kx^k$$ be a power series mapping reals to reals, with radius of convergence $1$. Suppose $f'(x_0)$ exists in $(-1,1]$ (take the one-sided limit if $x_0 = 1$). Also suppose $a_k \geq 0$ for all $k$. Then is it always true that $$f'(x_0) = \sum_{k=0}^\infty ka_kx^{k-1}?$$ This clearly holds if $x_0 \in (-1,1)$. But what about $x_0 = 1$?","Let $$f(x) = \sum_{k=0}^\infty a_kx^k$$ be a power series mapping reals to reals, with radius of convergence $1$. Suppose $f'(x_0)$ exists in $(-1,1]$ (take the one-sided limit if $x_0 = 1$). Also suppose $a_k \geq 0$ for all $k$. Then is it always true that $$f'(x_0) = \sum_{k=0}^\infty ka_kx^{k-1}?$$ This clearly holds if $x_0 \in (-1,1)$. But what about $x_0 = 1$?",,"['calculus', 'real-analysis', 'derivatives', 'convergence-divergence', 'power-series']"
25,Fundamental proof of Taylor's theorem using little-o notations,Fundamental proof of Taylor's theorem using little-o notations,,"Is there a fundamental proof of Taylor's theorem using little-o notation? I assume $f:E\rightarrow F$ as a mapping between Banach spaces and write $(h^i)$ for $(h,\ldots,h)$ ($i$ times iterated). The definition of the derivative gives $$f(x+h)=f(x)+f'(x)(h)+o(\|h\|).$$  Applying this to $f'$ I get $$f'(x+h)=f'(x)+f''(x)(h)+o(\|h\|)$$ where $o(\|h\|)$ has to be interpreted in the context of the corresponding ambient spaces, i.e. $E$ in the first line and $L(E,F)$ in the second line.  Applying the second line to $h$ I get $$f'(x+h)(h)=f'(x)(h)+f''(x)(h^2)+o(\|h\|^2)$$ where the right hand looks at least similar to the right hand side of the equation I want to derive, namely $$f(x+h)=f(x)+f'(x)(h)+\frac{f''(x)(h^2)}{2}+o(\|h\|^2).$$  I could make it even more similar by writing $$f'(x+\frac{h}{2})(h)=f'(x)(h)+\frac{f''(x)(h^2)}{2}+o(\|h\|^2)$$ but I don't know how to proceed from there... Edit: Sorry, what I wrote in my last (deleted) edit does not apply, I misread the text and the author addressed another formula with his quote!","Is there a fundamental proof of Taylor's theorem using little-o notation? I assume $f:E\rightarrow F$ as a mapping between Banach spaces and write $(h^i)$ for $(h,\ldots,h)$ ($i$ times iterated). The definition of the derivative gives $$f(x+h)=f(x)+f'(x)(h)+o(\|h\|).$$  Applying this to $f'$ I get $$f'(x+h)=f'(x)+f''(x)(h)+o(\|h\|)$$ where $o(\|h\|)$ has to be interpreted in the context of the corresponding ambient spaces, i.e. $E$ in the first line and $L(E,F)$ in the second line.  Applying the second line to $h$ I get $$f'(x+h)(h)=f'(x)(h)+f''(x)(h^2)+o(\|h\|^2)$$ where the right hand looks at least similar to the right hand side of the equation I want to derive, namely $$f(x+h)=f(x)+f'(x)(h)+\frac{f''(x)(h^2)}{2}+o(\|h\|^2).$$  I could make it even more similar by writing $$f'(x+\frac{h}{2})(h)=f'(x)(h)+\frac{f''(x)(h^2)}{2}+o(\|h\|^2)$$ but I don't know how to proceed from there... Edit: Sorry, what I wrote in my last (deleted) edit does not apply, I misread the text and the author addressed another formula with his quote!",,"['calculus', 'derivatives', 'banach-spaces', 'taylor-expansion']"
26,Mean Value Theorem: finding two numbers in the same interval,Mean Value Theorem: finding two numbers in the same interval,,"Let $f,g:[0,1] \to \mathbb{R}$ such that $f'(x)>0,\ g'(x) >0,\ \forall x\in [0,1]$. Moreover, $f(0)= g(0)$ and $f(1)=g(1)$. Prove that exists $x_1, x_2 \in [0,1]$ such that $$f(x_1) = g(x_2), \ \text{and}\ \ f'(x_1) = g'(x_2)$$ I've tried to use Mean Value Theorem with: a) $h(x)= f(x)-g(x)$ b) $h(x) = f(x) - xg(x)$ c) $h(x) = f(x)g(x)$ d) Using $g(1-x)$ instead of $g(x)$ But nothing seems to help in order to find at least one of $x_1$ or $x_2$. I think that, finding one of them, the other could be easy to get. Thanks for the help!","Let $f,g:[0,1] \to \mathbb{R}$ such that $f'(x)>0,\ g'(x) >0,\ \forall x\in [0,1]$. Moreover, $f(0)= g(0)$ and $f(1)=g(1)$. Prove that exists $x_1, x_2 \in [0,1]$ such that $$f(x_1) = g(x_2), \ \text{and}\ \ f'(x_1) = g'(x_2)$$ I've tried to use Mean Value Theorem with: a) $h(x)= f(x)-g(x)$ b) $h(x) = f(x) - xg(x)$ c) $h(x) = f(x)g(x)$ d) Using $g(1-x)$ instead of $g(x)$ But nothing seems to help in order to find at least one of $x_1$ or $x_2$. I think that, finding one of them, the other could be easy to get. Thanks for the help!",,"['calculus', 'derivatives']"
27,Using quotient rule to differentiate,Using quotient rule to differentiate,,"Use the quotient rule to differentiate. I want to know if I'm doing this correctly: $$ f(x)=\frac {2x}{x^4+6} $$ First, I find $f$ prime of $x$ and $g$ prime of $x$: $$ f'(x) = 2 $$$$ g'(x) = 4x^3 $$ After using the rule, I end up with: $$ \frac {2x^4+12-8x^4}{(x^4+6)^2} = \frac {-6x^4+12}{(x^4+6)^2} $$ Would this be the final answer if I'm correct? Or do I need to expand the denominator?","Use the quotient rule to differentiate. I want to know if I'm doing this correctly: $$ f(x)=\frac {2x}{x^4+6} $$ First, I find $f$ prime of $x$ and $g$ prime of $x$: $$ f'(x) = 2 $$$$ g'(x) = 4x^3 $$ After using the rule, I end up with: $$ \frac {2x^4+12-8x^4}{(x^4+6)^2} = \frac {-6x^4+12}{(x^4+6)^2} $$ Would this be the final answer if I'm correct? Or do I need to expand the denominator?",,"['calculus', 'derivatives']"
28,Polynomial or Exponential,Polynomial or Exponential,,"PROBLEM: Let $f(x)$ be a polynomial function. It is known that for every $x$: $$ f'(x) \leq f(x) $$ Prove/disprove: For every $x$: $$ f(x) \geq 0 $$ MY INTUITION: Suppose by contradiction that $f(z)<0$ for some $z$. Then $f'(z)<0$ too, so $f$ must go down and down to $-\infty$. Moreover, the rate of decrease must by at least exponential because: $$ |f'(x) \geq f(x)| $$ and we know that equality holds for the exponential function. This contradict the fact that $f(x)$ is polynomial. MY QUESTIONS: Is my intuition true? If so, how to formalize it? If not, then what is the correct answer?","PROBLEM: Let $f(x)$ be a polynomial function. It is known that for every $x$: $$ f'(x) \leq f(x) $$ Prove/disprove: For every $x$: $$ f(x) \geq 0 $$ MY INTUITION: Suppose by contradiction that $f(z)<0$ for some $z$. Then $f'(z)<0$ too, so $f$ must go down and down to $-\infty$. Moreover, the rate of decrease must by at least exponential because: $$ |f'(x) \geq f(x)| $$ and we know that equality holds for the exponential function. This contradict the fact that $f(x)$ is polynomial. MY QUESTIONS: Is my intuition true? If so, how to formalize it? If not, then what is the correct answer?",,['derivatives']
29,Finding the derivative of $|x|^4$ using the chain rule.,Finding the derivative of  using the chain rule.,|x|^4,"I am presented with the following task: Can you use the chain rule to find the derivatives of $|x|^4$ and $|x^4|$ in $x = 0$? Do the derivatives exist in $x = 0$? I solved the task in a rather straight-forward way, but I am worried that there's more to the task: First of all, both functions is a variable to the power of an even number, so given that $x$ is a real number, we have that $|x^4| = |x|^4$. In order to force practical use of the chain rule, we write $|x|^4 = \sqrt{x^2}^4$. We are using the fact that taking a number to the power of an even number, and using the absolute value, gives us positive numbers exclusively. If we choose the chain $u = x^2$, thus $g(u) = \sqrt{u}^4$, we have that $u' = 2x$ og $g'(u) = (u^2)' = 2u$. Then we have that the derivative of the function, that I for pratical reasons will name $f(x)$, is $f'(x) = 2x^2 * 2x = 4x^3$. We see that the the general power rule applies here, seeing as we work with a variable to the power of an even number. The derivative in the point $x = 0$ is $4 * 0^3 = \underline{\underline{0}}$. Thus we can conclude that the derivative exists in $x = 0$. Is this fairly logical? I'm having a hard time seeing that there is anything more to this task, but it feels like it went a bit too straightforward.","I am presented with the following task: Can you use the chain rule to find the derivatives of $|x|^4$ and $|x^4|$ in $x = 0$? Do the derivatives exist in $x = 0$? I solved the task in a rather straight-forward way, but I am worried that there's more to the task: First of all, both functions is a variable to the power of an even number, so given that $x$ is a real number, we have that $|x^4| = |x|^4$. In order to force practical use of the chain rule, we write $|x|^4 = \sqrt{x^2}^4$. We are using the fact that taking a number to the power of an even number, and using the absolute value, gives us positive numbers exclusively. If we choose the chain $u = x^2$, thus $g(u) = \sqrt{u}^4$, we have that $u' = 2x$ og $g'(u) = (u^2)' = 2u$. Then we have that the derivative of the function, that I for pratical reasons will name $f(x)$, is $f'(x) = 2x^2 * 2x = 4x^3$. We see that the the general power rule applies here, seeing as we work with a variable to the power of an even number. The derivative in the point $x = 0$ is $4 * 0^3 = \underline{\underline{0}}$. Thus we can conclude that the derivative exists in $x = 0$. Is this fairly logical? I'm having a hard time seeing that there is anything more to this task, but it feels like it went a bit too straightforward.",,"['calculus', 'derivatives', 'absolute-value']"
30,Find sufficient and necessary conditions to guarantees this property,Find sufficient and necessary conditions to guarantees this property,,"Let $f$ be a real non polynomial analytic function. Suppose that the function $f$ assumes arbitrarily large and arbitrarily small values, i.e., for all $K>0$, there are $a,b$ with $f(a)<−K$ and $f(b)>K$. My question is: Can we find sufficient and necessary conditions to guarantees this property persists for the derivatives $f^{(k)}, k=1,2,..$.","Let $f$ be a real non polynomial analytic function. Suppose that the function $f$ assumes arbitrarily large and arbitrarily small values, i.e., for all $K>0$, there are $a,b$ with $f(a)<−K$ and $f(b)>K$. My question is: Can we find sufficient and necessary conditions to guarantees this property persists for the derivatives $f^{(k)}, k=1,2,..$.",,"['real-analysis', 'derivatives']"
31,A doubt over a differentiation problem.,A doubt over a differentiation problem.,,"The question is: Find the equation of the tangent to the curve $y^2=8x$ at a point $(x_0,y_0)$ . My teacher's approach : Differentiate the equation. we get $2y\cdot \dfrac {dy}{dx}=8 $ which gives us : $\dfrac {dy}{dx}=\dfrac4y$ . At a point $(x_0,y_0)$ , slope of the tangent = $\dfrac4{y_0}$ So the equation of the tangent is $\dfrac{y-y_0}{x-x_0}=\dfrac4{y_0}$ What bothers me here is the differentiation. According to wikipedia : the derivative is the ratio of the infinitesimal change of the output over the infinitesimal change of the input producing that change of output. For a real-valued function of a single real variable, the derivative at a point equals the slope of the tangent line to the graph of the function at that point. But, $y^2=8x$ is NOT a function. Then, how can one find the derivative? This bothers me a lot.. Can someone help me? Thank you.","The question is: Find the equation of the tangent to the curve at a point . My teacher's approach : Differentiate the equation. we get which gives us : . At a point , slope of the tangent = So the equation of the tangent is What bothers me here is the differentiation. According to wikipedia : the derivative is the ratio of the infinitesimal change of the output over the infinitesimal change of the input producing that change of output. For a real-valued function of a single real variable, the derivative at a point equals the slope of the tangent line to the graph of the function at that point. But, is NOT a function. Then, how can one find the derivative? This bothers me a lot.. Can someone help me? Thank you.","y^2=8x (x_0,y_0) 2y\cdot \dfrac {dy}{dx}=8  \dfrac {dy}{dx}=\dfrac4y (x_0,y_0) \dfrac4{y_0} \dfrac{y-y_0}{x-x_0}=\dfrac4{y_0} y^2=8x",['calculus']
32,"For a differentiable map $\Phi$ between manifolds $M$ and $W$, what is $d\Phi?$ (Aubin)","For a differentiable map  between manifolds  and , what is  (Aubin)",\Phi M W d\Phi?,"I can't understand a passage from A Course in Differential Geometry by T. Aubin. First, there is Definition 2.6., which I posted in this question . And now there's this: $(\Phi_*)_P$ is nothing else than $(d\Phi)_P.$ ($\Phi:M_n\to W_p$, where $M_n$ and $W_p$ are manifolds of dimensions $n$ and $p$ respectively. $P\in M_n.$) I don't understand what this sentence means. I can't find a definition of $(d\Phi)_P$ earlier in the book. There is a definition for the case of $\Phi$ being a map between open subsets of Euclidean spaces, but not for general manifolds. So how can we establish that the two objects are the same if we haven't defined one of them? The author goes on to explain thus: Indeed, consider a local chart at $P$ with coordinates $\{x^i\}$ and a local chart at $Q$ with coordinates $\{y^a\}$. $\Phi$ is defined in a neighbourhood of $P$ by $p$ real-valued functions $\Phi^a(x^1,x^2,\cdots,x^n),\;a=1,2,\cdots,p.$ I don't get it. What are the $\Phi^a?$ If they're real-valued, then $(\Phi^1,\cdots,\Phi^p)$ cannot be equal to $\Phi$ because $\Phi$ goes from $M_n$ to $W_p$, not to $\Bbb R^p$. To get to $\Bbb R^p$, I need to compose $\Phi$ with a chart. And aren't the coordinates $x^i$ real too? I understand that when I have a chart $(\Omega,\varphi)$ for an open set $\Omega\subseteq M_n$ and $\varphi:\Omega\to\Bbb R^n,$ then the coordinates $x^i$ of a point $m\in M_n$ are the coordinates of $\varphi(m)$ in $\Bbb R^n$. So that would mean that $\Phi^a$ are actually functions from $\Bbb R^n\supseteq\Omega$ to $\Bbb R$. And probably $(\Phi^1,\cdots,\Phi^p)=\psi\circ\Phi\circ\varphi^{-1}$ for some charts $\phi$ and $\psi$. Is that correct? If so, isn't the notation very counterintuitive? The author now says the following. I don't understand it at all. He uses the symbol $(d\Phi)_P$ again, and I think it still hasn't been defined. Using intrinsic notations to simplify, we get $$X(f\circ\Phi)=d(f\circ\Phi)_P\circ X=(df)\circ(d\Phi)_P\circ X=(df)\circ(\Phi_*)_PX.$$ Indeed, $\{X^i\}$ being the components of $X$ in the basis $\{(\partial/\partial x^i)_P\},$ the components of $Y=(\Phi_*)_PX$ are $$Y^a=\sum_{i=1}^n \frac{\partial\Phi^a}{\partial x^i}$$ in the basis $\{\partial/\partial y^a)_Q\}.$ When we use intrinsic notation, we do not specify the local charts. In the coordinate systems $\{x^i\}$ and $\{y^a\}$, the equality above shows that $(d\Phi)_P=((\partial\Phi^a/\partial x^i))_P=(\Phi_*)_P.$ I don't understand what the intrinsic notations are. I can't see how the notation is intrinsic if we use the coordinates $x^i$ and $y^a$ which are real.","I can't understand a passage from A Course in Differential Geometry by T. Aubin. First, there is Definition 2.6., which I posted in this question . And now there's this: $(\Phi_*)_P$ is nothing else than $(d\Phi)_P.$ ($\Phi:M_n\to W_p$, where $M_n$ and $W_p$ are manifolds of dimensions $n$ and $p$ respectively. $P\in M_n.$) I don't understand what this sentence means. I can't find a definition of $(d\Phi)_P$ earlier in the book. There is a definition for the case of $\Phi$ being a map between open subsets of Euclidean spaces, but not for general manifolds. So how can we establish that the two objects are the same if we haven't defined one of them? The author goes on to explain thus: Indeed, consider a local chart at $P$ with coordinates $\{x^i\}$ and a local chart at $Q$ with coordinates $\{y^a\}$. $\Phi$ is defined in a neighbourhood of $P$ by $p$ real-valued functions $\Phi^a(x^1,x^2,\cdots,x^n),\;a=1,2,\cdots,p.$ I don't get it. What are the $\Phi^a?$ If they're real-valued, then $(\Phi^1,\cdots,\Phi^p)$ cannot be equal to $\Phi$ because $\Phi$ goes from $M_n$ to $W_p$, not to $\Bbb R^p$. To get to $\Bbb R^p$, I need to compose $\Phi$ with a chart. And aren't the coordinates $x^i$ real too? I understand that when I have a chart $(\Omega,\varphi)$ for an open set $\Omega\subseteq M_n$ and $\varphi:\Omega\to\Bbb R^n,$ then the coordinates $x^i$ of a point $m\in M_n$ are the coordinates of $\varphi(m)$ in $\Bbb R^n$. So that would mean that $\Phi^a$ are actually functions from $\Bbb R^n\supseteq\Omega$ to $\Bbb R$. And probably $(\Phi^1,\cdots,\Phi^p)=\psi\circ\Phi\circ\varphi^{-1}$ for some charts $\phi$ and $\psi$. Is that correct? If so, isn't the notation very counterintuitive? The author now says the following. I don't understand it at all. He uses the symbol $(d\Phi)_P$ again, and I think it still hasn't been defined. Using intrinsic notations to simplify, we get $$X(f\circ\Phi)=d(f\circ\Phi)_P\circ X=(df)\circ(d\Phi)_P\circ X=(df)\circ(\Phi_*)_PX.$$ Indeed, $\{X^i\}$ being the components of $X$ in the basis $\{(\partial/\partial x^i)_P\},$ the components of $Y=(\Phi_*)_PX$ are $$Y^a=\sum_{i=1}^n \frac{\partial\Phi^a}{\partial x^i}$$ in the basis $\{\partial/\partial y^a)_Q\}.$ When we use intrinsic notation, we do not specify the local charts. In the coordinate systems $\{x^i\}$ and $\{y^a\}$, the equality above shows that $(d\Phi)_P=((\partial\Phi^a/\partial x^i))_P=(\Phi_*)_P.$ I don't understand what the intrinsic notations are. I can't see how the notation is intrinsic if we use the coordinates $x^i$ and $y^a$ which are real.",,"['differential-geometry', 'derivatives', 'definition']"
33,Identifying linear operator with a bilinear symmetric form using Theorem of Schwarz,Identifying linear operator with a bilinear symmetric form using Theorem of Schwarz,,"We study the energy functional $E$ of the form $$E(v)=\frac{1}{2}a(v,v)+\int_{\Omega}F(x,v).$$ Let $V$ be a real Banach space with norm $||\cdot||_{V}$ and denote by $V^{'}$ the dual space. By $\langle\cdot,\cdot\rangle_{V^{'}\times V}$ we denote the duality between the space $V$ and its dual $V^{'}$ . Let $U\subset V$ be open, and let $E\in C^2(U,\mathbb{R})$ . We denote by $\mathcal{M}\in C^1(U,V^{'})$ the first derivate of $E$ and by $\mathcal{L}\in C(U,B(V,V^{'}))$ the second derivate. Let $\varphi\in U$ be a stationary point for $E$ , i.e. $\mathcal{M}(\varphi)=0$ . Then, by the Theorem of Schwarz, we may identify the linear operator $\mathcal{L}(\varphi)$ with a bilinear symmetric form on $V\times V$ . I don't understand how the conclusion arises exactly. The Theorem of Schwarz yields the symmetry of the bilinear form. But why does such a bilinear form exist in the first place, given that the Riesz representation theorem only applies in Hilbert spaces? Edit: This question is about the proof of Lemma 3.3 in https://core.ac.uk/download/pdf/82204962.pdf","We study the energy functional of the form Let be a real Banach space with norm and denote by the dual space. By we denote the duality between the space and its dual . Let be open, and let . We denote by the first derivate of and by the second derivate. Let be a stationary point for , i.e. . Then, by the Theorem of Schwarz, we may identify the linear operator with a bilinear symmetric form on . I don't understand how the conclusion arises exactly. The Theorem of Schwarz yields the symmetry of the bilinear form. But why does such a bilinear form exist in the first place, given that the Riesz representation theorem only applies in Hilbert spaces? Edit: This question is about the proof of Lemma 3.3 in https://core.ac.uk/download/pdf/82204962.pdf","E E(v)=\frac{1}{2}a(v,v)+\int_{\Omega}F(x,v). V ||\cdot||_{V} V^{'} \langle\cdot,\cdot\rangle_{V^{'}\times V} V V^{'} U\subset V E\in C^2(U,\mathbb{R}) \mathcal{M}\in C^1(U,V^{'}) E \mathcal{L}\in C(U,B(V,V^{'})) \varphi\in U E \mathcal{M}(\varphi)=0 \mathcal{L}(\varphi) V\times V","['derivatives', 'operator-theory', 'banach-spaces', 'dual-spaces', 'bilinear-form']"
34,"A notion of ""differentiation"" based on secant rather than tangent","A notion of ""differentiation"" based on secant rather than tangent",,"Given a differentiable real function $f$ , the derivative $f'(x)$ is the slope of the tangent to the graph of $f$ at $(x,f(x))$ . Suppose that, instead of the tangent, we look at the secant to the graph of $f$ , between the point $(x,f(x))$ and the point $(x^+,f(x^+))$ , where $x^+$ is chosen such that $x^+>x$ and the distance between $(x,f(x))$ and $(x^+,f(x^+))$ , along the curve of $f$ , is some fixed constant $d$ : We define the "" $d$ -secant-derivative"" of $f$ at $x$ as the slope of that secant. Note that, as $d\to 0$ , the $d$ -secant-derivative at $x$ approaches $f'(x)$ (if $f$ is differentiable), but here $d$ is constant. Is anything known about this ""secant-derivative"" operator? Is there a simple way to compute or approximate it, like there are simple rules for computing derivatives? NOTE: As I am interested in approximations, it does not matter very much what distance measure is used: it can also be Euclidean distance, $\ell_1$ distance or $\ell_\infty$ distance; whatever makes the problem solvable. The reason I chose the distance along the curve is that (I think) it defines $x^+$ uniquely. But, I will also be happy for a solution using e.g. the $\ell_\infty$ metric, where $x+ = \inf\{y\geq x | \ell_\infty(y,x) = d\}$ .","Given a differentiable real function , the derivative is the slope of the tangent to the graph of at . Suppose that, instead of the tangent, we look at the secant to the graph of , between the point and the point , where is chosen such that and the distance between and , along the curve of , is some fixed constant : We define the "" -secant-derivative"" of at as the slope of that secant. Note that, as , the -secant-derivative at approaches (if is differentiable), but here is constant. Is anything known about this ""secant-derivative"" operator? Is there a simple way to compute or approximate it, like there are simple rules for computing derivatives? NOTE: As I am interested in approximations, it does not matter very much what distance measure is used: it can also be Euclidean distance, distance or distance; whatever makes the problem solvable. The reason I chose the distance along the curve is that (I think) it defines uniquely. But, I will also be happy for a solution using e.g. the metric, where .","f f'(x) f (x,f(x)) f (x,f(x)) (x^+,f(x^+)) x^+ x^+>x (x,f(x)) (x^+,f(x^+)) f d d f x d\to 0 d x f'(x) f d \ell_1 \ell_\infty x^+ \ell_\infty x+ = \inf\{y\geq x | \ell_\infty(y,x) = d\}","['real-analysis', 'derivatives', 'tangent-line', 'secant']"
35,Is Leibniz rule fundamental?,Is Leibniz rule fundamental?,,"Disclaimer, I am a physicist and mess up with math and really think that derivatives are just fractions (roughly). I am starting to study maths itself as the discipline it is and not as a tool for my science. Concretely, I am delving into differential forms and manifolds because there are some facts about physics that I do not really understand. For example, why is there a Lie algebra underlying Poisson Brackets, why the phase space is a 2-form and its role in geometric quantization. In this journey, an intuition is growing (that I do not know if is correct) on what is a derivative, let me explain. I have seen that every operator that somehow follows a Leibniz rule can be understood as a derivative. The derivation applied to functions, the external derivative and the Poisson Bracket follows a Leibniz rule-like. Regarding the last one, as I have seen, in the free coordinate formulation of differential forms and manifolds the Poisson Brackets are very familiar to Lie Derivatives. This motivated my question: is the Leibniz rule fundamental in the descriptions of derivatives? I mean, some operator that do not follow this kind of rule implies that is not a derivative? Thanks in advance for your response. T.","Disclaimer, I am a physicist and mess up with math and really think that derivatives are just fractions (roughly). I am starting to study maths itself as the discipline it is and not as a tool for my science. Concretely, I am delving into differential forms and manifolds because there are some facts about physics that I do not really understand. For example, why is there a Lie algebra underlying Poisson Brackets, why the phase space is a 2-form and its role in geometric quantization. In this journey, an intuition is growing (that I do not know if is correct) on what is a derivative, let me explain. I have seen that every operator that somehow follows a Leibniz rule can be understood as a derivative. The derivation applied to functions, the external derivative and the Poisson Bracket follows a Leibniz rule-like. Regarding the last one, as I have seen, in the free coordinate formulation of differential forms and manifolds the Poisson Brackets are very familiar to Lie Derivatives. This motivated my question: is the Leibniz rule fundamental in the descriptions of derivatives? I mean, some operator that do not follow this kind of rule implies that is not a derivative? Thanks in advance for your response. T.",,"['derivatives', 'differential-geometry', 'physics']"
36,$k$-th derivative of a rational function,-th derivative of a rational function,k,"Let $f$ be the function given by $$ f(x) = \frac{1}{x+1}. $$ As we easily check, the $k$ -th derivative of $f$ is given by $$ f^{(k)}(x) = (-1)^k \frac{k!}{(x+1)^{k+1}}. $$ In particular $|f^{(k)}(x)| = \frac{k!}{(x+1)^{k+1}}$ . Suppose that $R_n$ , $n \ge 1$ is a rational function such that $\displaystyle R_n(x) = \frac{P_n(x)}{Q_n(x)}$ , where $P_n$ and $Q_n$ are polynomials, $R_n$ is proper, that is the degree of $Q_n$ is greater than the degree of $P_n$ . all coefficients of $P_n$ and $Q_n$ are positive, ( EDIT: ) and $Q_n(0) \neq 0$ , and $R_n$ converges to $f$ , in the sense that $$ \lim_{n \to \infty} R_n(x) = f(x). $$ Does it follow that the $k$ -th derivative of $R_n$ is uniformly (in $k$ and $x$ ) bounded on the positive axis by the $k$ -th derivative of $f$ ? In other words, is it true that there exists $C > 0$ such that $$ |R_n^{(k)}(x)| \le C \frac{k!}{(x+1)^{k+1}}, \qquad x > 0,\ k \ge 1? $$ Note that I would like to obtain the above estimate just for $x > 0$ : in this interval $x \mapsto x + 1$ and $Q_n$ are positive (hence, nonzero).","Let be the function given by As we easily check, the -th derivative of is given by In particular . Suppose that , is a rational function such that , where and are polynomials, is proper, that is the degree of is greater than the degree of . all coefficients of and are positive, ( EDIT: ) and , and converges to , in the sense that Does it follow that the -th derivative of is uniformly (in and ) bounded on the positive axis by the -th derivative of ? In other words, is it true that there exists such that Note that I would like to obtain the above estimate just for : in this interval and are positive (hence, nonzero).","f 
f(x) = \frac{1}{x+1}.
 k f 
f^{(k)}(x) = (-1)^k \frac{k!}{(x+1)^{k+1}}.
 |f^{(k)}(x)| = \frac{k!}{(x+1)^{k+1}} R_n n \ge 1 \displaystyle R_n(x) = \frac{P_n(x)}{Q_n(x)} P_n Q_n R_n Q_n P_n P_n Q_n Q_n(0) \neq 0 R_n f 
\lim_{n \to \infty} R_n(x) = f(x).
 k R_n k x k f C > 0 
|R_n^{(k)}(x)| \le C \frac{k!}{(x+1)^{k+1}}, \qquad x > 0,\ k \ge 1?
 x > 0 x \mapsto x + 1 Q_n","['real-analysis', 'derivatives', 'polynomials', 'rational-functions']"
37,Help to understand a proof of $\frac{\mathrm{d}e^x}{\mathrm{d}x}\vert_{x=0}=1$,Help to understand a proof of,\frac{\mathrm{d}e^x}{\mathrm{d}x}\vert_{x=0}=1,"Context I am reading the textbook Calculus With Applications, by Peter D. Lax and having a problem  understanding the proof of $\frac{\mathrm{d}e^x}{\mathrm{d}x}\vert_{x=0}=1$ , the result of which is then used to prove the derivative of $e^x$ . The textbook first uses the inequalities $$ \left(1+\frac{1}{n}\right)^n<e<\left(1+\frac{1}{n}\right)^{n+1}<\left(1+\frac{1}{n-1}\right)^n \tag{1}\label{eq1} $$ for all integers $n>1$ . Then it takes $h=\frac{1}{n}$ as a special sequence of $h$ tending to zero. After a few derivations of the inequality, it get $$ 1\le\frac{e^h-1}{h}\le\frac{n}{n-1} $$ As n tends to infinity, the right-hand term tends to 1, so by the squeeze theorem, the center term tends to 1 also, which means the derivative of $e^x$ at $x=0$ is equal to 1. However , as it says, this does not quite finish the proof, since it has taken $h$ to be of the special form $h=\frac{1}{n}$ . So it leaves a problem to fill this gap: Complete problem description Recall from Eq. $\eqref{eq1}$ that e is between the increasing sequence $e_n$ and the decreasing sequence $f_n$ . Explain the following items. (a) If $h>0$ is not of the form $\frac{1}{n}$ , then there is an integer $n$ for which $\frac{1}{n}<h<\frac{1}{n-1}$ . (b) Using Eq. $\eqref{eq1}$ , one has $\left(1+\frac{1}{n}\right)^{nh}<e^h<\left(1+\frac{1}{n-2}\right)^{\left(n-1\right)h}$ . (c) $\left(n-1\right)h<1<nh$ (d) $1+\frac{1}{n}<e^h<1+\frac{1}{n-2}$ (e) $n-1<\frac{1}{h}<n$ , and $\frac{n-1}{n}<\frac{e^h-1}{h}<\frac{n}{n-2}$ . Conclude from this that $\frac{e^h-1}{h}$ tends to $1$ as $h$ tends to zero. My progress I have two main questions, the first one is how to prove (a) and the second one is how the final conclusion is obtained from (e). And I have proved the intermediate (b), (c), (d) and (e) process.","Context I am reading the textbook Calculus With Applications, by Peter D. Lax and having a problem  understanding the proof of , the result of which is then used to prove the derivative of . The textbook first uses the inequalities for all integers . Then it takes as a special sequence of tending to zero. After a few derivations of the inequality, it get As n tends to infinity, the right-hand term tends to 1, so by the squeeze theorem, the center term tends to 1 also, which means the derivative of at is equal to 1. However , as it says, this does not quite finish the proof, since it has taken to be of the special form . So it leaves a problem to fill this gap: Complete problem description Recall from Eq. that e is between the increasing sequence and the decreasing sequence . Explain the following items. (a) If is not of the form , then there is an integer for which . (b) Using Eq. , one has . (c) (d) (e) , and . Conclude from this that tends to as tends to zero. My progress I have two main questions, the first one is how to prove (a) and the second one is how the final conclusion is obtained from (e). And I have proved the intermediate (b), (c), (d) and (e) process.","\frac{\mathrm{d}e^x}{\mathrm{d}x}\vert_{x=0}=1 e^x 
\left(1+\frac{1}{n}\right)^n<e<\left(1+\frac{1}{n}\right)^{n+1}<\left(1+\frac{1}{n-1}\right)^n \tag{1}\label{eq1}
 n>1 h=\frac{1}{n} h 
1\le\frac{e^h-1}{h}\le\frac{n}{n-1}
 e^x x=0 h h=\frac{1}{n} \eqref{eq1} e_n f_n h>0 \frac{1}{n} n \frac{1}{n}<h<\frac{1}{n-1} \eqref{eq1} \left(1+\frac{1}{n}\right)^{nh}<e^h<\left(1+\frac{1}{n-2}\right)^{\left(n-1\right)h} \left(n-1\right)h<1<nh 1+\frac{1}{n}<e^h<1+\frac{1}{n-2} n-1<\frac{1}{h}<n \frac{n-1}{n}<\frac{e^h-1}{h}<\frac{n}{n-2} \frac{e^h-1}{h} 1 h","['calculus', 'derivatives', 'exponential-function']"
38,Does the inverse function theorem require continuity as a hypothesis?,Does the inverse function theorem require continuity as a hypothesis?,,"This question is about the inverse function theorem for real-valued functions. Suppose $f$ is a one-to-one, that $a$ is in the domain of $f$ , and that $f$ is defined on an open interval containing $a$ . Suppose further that $f$ is differentiable at $a$ , and $f'(a)\neq0$ . Does it follow that $f^{-1}$ is differentiable at $f(a)$ , and $$ \bigl(f^{-1}\bigr)'\bigl(f(a)\bigr)=\frac{1}{f'(a)} \, ? $$ I ask this question because some presentations of the inverse function theorem (e.g. in Spivak's Calculus ) seem to additionally require that $f$ is continuous on an open interval containing $a$ . I see three possibilities: That the hypotheses given above imply that $f$ is continuous on an open interval containing $a$ , and so it is redundant to state this as a hypothesis. That the hypotheses given above do not imply that $f$ is continuous on an open interval containing $a$ , but the theorem holds anyway. That the hypothesis that $f$ is continuous on an open interval containing $a$ is in fact necessary, and so there is a counter-example to the ""theorem"" stated above.","This question is about the inverse function theorem for real-valued functions. Suppose is a one-to-one, that is in the domain of , and that is defined on an open interval containing . Suppose further that is differentiable at , and . Does it follow that is differentiable at , and I ask this question because some presentations of the inverse function theorem (e.g. in Spivak's Calculus ) seem to additionally require that is continuous on an open interval containing . I see three possibilities: That the hypotheses given above imply that is continuous on an open interval containing , and so it is redundant to state this as a hypothesis. That the hypotheses given above do not imply that is continuous on an open interval containing , but the theorem holds anyway. That the hypothesis that is continuous on an open interval containing is in fact necessary, and so there is a counter-example to the ""theorem"" stated above.","f a f f a f a f'(a)\neq0 f^{-1} f(a) 
\bigl(f^{-1}\bigr)'\bigl(f(a)\bigr)=\frac{1}{f'(a)} \, ?
 f a f a f a f a","['real-analysis', 'calculus', 'derivatives', 'inverse-function', 'inverse-function-theorem']"
39,Does the following computer science/optimization theorem have a proof?,Does the following computer science/optimization theorem have a proof?,,"I have been reading about a theorem in math called the ""Schema Theorem"" - this theorem is one of the first theorems from the field of evolutionary computing and genetic algorithms, largely responsible for justifying the use of ""genetic algorithms"" to solve optimization problems when the derivative of the objective function is either unknown or not clearly defined. ""Genetic algorithms"" can be used in many different types of optimization problems, such as finding the roots of a polynomial. For example, we could use the ""genetic algorithm"" to find the roots (the zeros) of the following polynomial (this polynomial will be referred to as the ""objective function"", i.e. the objective of the optimization/root finding): f(x, y) = x sin(4x) + 1.1y sin(2y) Essentially, the ""genetic algorithm"" would start by taking random values of ""x"" and ""y"" and record the corresponding value of ""f(x,y)"" for each random combination of ""x"" and ""y"" : the value of ""f(x,y)"" for a given combination of ""x"" and ""y"" is called the ""fitness"". The ""genetic algorithm"" works by taking many such random combinations of ""x"" and ""y"" and recording which combinations produce lower fitness values (i.e. which coordinates of ""x"" and ""y"" correspond to low elevation regions on the f(x,y) surface). The ""genetic algorithm"" then ""combines"" (i.e. ""mutates"") combinations of ""x"" and ""y"" that produced low fitness values, and re-evaluates ""f(x,y)"" at these new ""mutated combinations"" of ""x"" and ""y"". The ""genetic algorithm"" repeats this mutation process many times until it successive differences in ""f(x,y)"" are negligible, or after a predefined number of iterations - this is very useful in problems where evaluating the derivative of the objective function can be extremely time consuming or costly, or the derivative of the objective function is poorly defined (i.e. standard optimization algorithms like gradient descent and newton-raphson can not be performed). The optimization process for the ""genetic algorithm"" can be seen below: An interesting observation about the ""genetic algorithm"" is that it does not ""promise or guarantee"" convergence to the true minimum point of the objective function during the optimization process. But what the ""genetic algorithm"" does guarantee, is that ""the overall fitness of the objective function is guaranteed to improve as the number of iterations increase"". In other words, the ""genetic algorithm"" will reach closer to the true minimum of the objective function after ""m iterations"" compared to ""n iterations"", where m >> n. This idea is expressed in one of the original and fundamental theorems of genetic algorithms, called the ""Schema Theorem"" (see below): Apparently, this theorem states that ""schemas"" (e.g. combinations of ""x"" and ""y"") that produce better fitness values are more likely to ""survive after mutation"" - i.e. the expected number of ""schemas"" that produce better fitness values are likely to increase as the number of iterations increase. Question: I have spent a long time trying to either find a derivation of proof of the Schema Theorem - something which attempts to mathematically explain why the ""genetic algorithm"" is guaranteed to move towards the true optimum of the objection function as the number of iterations increase (rhetorical question: after all, why does the ""genetic algorithm"" not result in worse fitness results?). After all: Why does this inequality hold? I am very curious to see if there is a mathematical proof that justifies one of the most important and earliest theorems in ""genetic algorithms"" and evolutionary computing - does anyone know if the Schema Theorem has a proof? Thanks! Reference: https://en.wikipedia.org/wiki/Holland%27s_schema_theorem https://www.whitman.edu/Documents/Academics/Mathematics/2014/carrjk.pdf https://www.sciencedirect.com/topics/computer-science/schema-theorem (alternate statement of theorem) https://www.researchgate.net/publication/51892214_The_Exact_Schema_Theorem/link/02bfe50f6181790d85000000/download http://dynamics.org/Altenberg/FILES/LeeSTPT.pdf https://en.wikipedia.org/wiki/Genetic_algorithm https://cran.r-project.org/web/packages/GA/vignettes/GA.html Note: An Example of Optimization using the Genetic Algorithm in the R Programming Language library(GA)     #define function to be optimized Rastrigin <- function(x1, x2) {   20 + x1^2 + x2^2 - 10*(cos(2*pi*x1) + cos(2*pi*x2)) }  x1 <- x2 <- seq(-5.12, 5.12, by = 0.1)  #run optimization through the genetic algorithm (with constraints) GA <- ga(type = ""real-valued"",           fitness =  function(x) -Rastrigin(x[1], x[2]),          lower = c(-5.12, -5.12), upper = c(5.12, 5.12),           popSize = 50, maxiter = 1000, run = 100)   GA | iter = 1 | Mean = -37.396067 | Best =  -6.441093 GA | iter = 2 | Mean = -29.00421 | Best =  -2.51091 GA | iter = 3 | Mean = -22.89516 | Best =  -2.51091  ....  GA | iter = 90 | Mean = -3.025916e+00 | Best = -9.301074e-06 GA | iter = 91 | Mean = -5.725962e+00 | Best = -9.301074e-06 GA | iter = 92 | Mean = -5.858303e+00 | Best = -9.301074e-06 ....  GA | iter = 250 | Mean = -3.581667e+00 | Best = -1.103990e-06 GA | iter = 251 | Mean = -5.160585e+00 | Best = -1.103990e-06 GA | iter = 252 | Mean = -7.226593e+00 | Best = -1.103990e-06   summary(GA)  ## Solution =  ##                x1           x2 ## [1,] 5.722083e-05 9.223472e-05","I have been reading about a theorem in math called the ""Schema Theorem"" - this theorem is one of the first theorems from the field of evolutionary computing and genetic algorithms, largely responsible for justifying the use of ""genetic algorithms"" to solve optimization problems when the derivative of the objective function is either unknown or not clearly defined. ""Genetic algorithms"" can be used in many different types of optimization problems, such as finding the roots of a polynomial. For example, we could use the ""genetic algorithm"" to find the roots (the zeros) of the following polynomial (this polynomial will be referred to as the ""objective function"", i.e. the objective of the optimization/root finding): f(x, y) = x sin(4x) + 1.1y sin(2y) Essentially, the ""genetic algorithm"" would start by taking random values of ""x"" and ""y"" and record the corresponding value of ""f(x,y)"" for each random combination of ""x"" and ""y"" : the value of ""f(x,y)"" for a given combination of ""x"" and ""y"" is called the ""fitness"". The ""genetic algorithm"" works by taking many such random combinations of ""x"" and ""y"" and recording which combinations produce lower fitness values (i.e. which coordinates of ""x"" and ""y"" correspond to low elevation regions on the f(x,y) surface). The ""genetic algorithm"" then ""combines"" (i.e. ""mutates"") combinations of ""x"" and ""y"" that produced low fitness values, and re-evaluates ""f(x,y)"" at these new ""mutated combinations"" of ""x"" and ""y"". The ""genetic algorithm"" repeats this mutation process many times until it successive differences in ""f(x,y)"" are negligible, or after a predefined number of iterations - this is very useful in problems where evaluating the derivative of the objective function can be extremely time consuming or costly, or the derivative of the objective function is poorly defined (i.e. standard optimization algorithms like gradient descent and newton-raphson can not be performed). The optimization process for the ""genetic algorithm"" can be seen below: An interesting observation about the ""genetic algorithm"" is that it does not ""promise or guarantee"" convergence to the true minimum point of the objective function during the optimization process. But what the ""genetic algorithm"" does guarantee, is that ""the overall fitness of the objective function is guaranteed to improve as the number of iterations increase"". In other words, the ""genetic algorithm"" will reach closer to the true minimum of the objective function after ""m iterations"" compared to ""n iterations"", where m >> n. This idea is expressed in one of the original and fundamental theorems of genetic algorithms, called the ""Schema Theorem"" (see below): Apparently, this theorem states that ""schemas"" (e.g. combinations of ""x"" and ""y"") that produce better fitness values are more likely to ""survive after mutation"" - i.e. the expected number of ""schemas"" that produce better fitness values are likely to increase as the number of iterations increase. Question: I have spent a long time trying to either find a derivation of proof of the Schema Theorem - something which attempts to mathematically explain why the ""genetic algorithm"" is guaranteed to move towards the true optimum of the objection function as the number of iterations increase (rhetorical question: after all, why does the ""genetic algorithm"" not result in worse fitness results?). After all: Why does this inequality hold? I am very curious to see if there is a mathematical proof that justifies one of the most important and earliest theorems in ""genetic algorithms"" and evolutionary computing - does anyone know if the Schema Theorem has a proof? Thanks! Reference: https://en.wikipedia.org/wiki/Holland%27s_schema_theorem https://www.whitman.edu/Documents/Academics/Mathematics/2014/carrjk.pdf https://www.sciencedirect.com/topics/computer-science/schema-theorem (alternate statement of theorem) https://www.researchgate.net/publication/51892214_The_Exact_Schema_Theorem/link/02bfe50f6181790d85000000/download http://dynamics.org/Altenberg/FILES/LeeSTPT.pdf https://en.wikipedia.org/wiki/Genetic_algorithm https://cran.r-project.org/web/packages/GA/vignettes/GA.html Note: An Example of Optimization using the Genetic Algorithm in the R Programming Language library(GA)     #define function to be optimized Rastrigin <- function(x1, x2) {   20 + x1^2 + x2^2 - 10*(cos(2*pi*x1) + cos(2*pi*x2)) }  x1 <- x2 <- seq(-5.12, 5.12, by = 0.1)  #run optimization through the genetic algorithm (with constraints) GA <- ga(type = ""real-valued"",           fitness =  function(x) -Rastrigin(x[1], x[2]),          lower = c(-5.12, -5.12), upper = c(5.12, 5.12),           popSize = 50, maxiter = 1000, run = 100)   GA | iter = 1 | Mean = -37.396067 | Best =  -6.441093 GA | iter = 2 | Mean = -29.00421 | Best =  -2.51091 GA | iter = 3 | Mean = -22.89516 | Best =  -2.51091  ....  GA | iter = 90 | Mean = -3.025916e+00 | Best = -9.301074e-06 GA | iter = 91 | Mean = -5.725962e+00 | Best = -9.301074e-06 GA | iter = 92 | Mean = -5.858303e+00 | Best = -9.301074e-06 ....  GA | iter = 250 | Mean = -3.581667e+00 | Best = -1.103990e-06 GA | iter = 251 | Mean = -5.160585e+00 | Best = -1.103990e-06 GA | iter = 252 | Mean = -7.226593e+00 | Best = -1.103990e-06   summary(GA)  ## Solution =  ##                x1           x2 ## [1,] 5.722083e-05 9.223472e-05",,"['derivatives', 'inequality', 'optimization', 'algorithms', 'computer-science']"
40,Deriving the equation for radial wave function,Deriving the equation for radial wave function,,"I'm trying to solve Schrodinger's equation of an exciton using the separation of variables method: $\psi = RY$ . Here's the equation I've already derived: $$ \frac{2\mu r^2}{\hbar^2}(E+\frac{e^2}{\epsilon r})+\frac{r^2}{R}\frac{\partial^2 R}{\partial r^2}+\frac{r}{R}\frac{\partial R}{\partial r} = -\frac{1}{Y}\frac{\partial^2Y}{\partial\theta^2} $$ Where the radial wave function is $R = R(r)$ and angular part is $Y = Y(\theta)$ . Since the two sides of the equation only depend on a single variable, they are both constants. I've found the angular part is $$ Y_m(\theta) = \frac{1}{\sqrt{2\pi}}e^{im\theta} $$ I'm having trouble solving the left part to obtain the radial equation. How can I do that? Thanks!","I'm trying to solve Schrodinger's equation of an exciton using the separation of variables method: . Here's the equation I've already derived: Where the radial wave function is and angular part is . Since the two sides of the equation only depend on a single variable, they are both constants. I've found the angular part is I'm having trouble solving the left part to obtain the radial equation. How can I do that? Thanks!","\psi = RY 
\frac{2\mu r^2}{\hbar^2}(E+\frac{e^2}{\epsilon r})+\frac{r^2}{R}\frac{\partial^2 R}{\partial r^2}+\frac{r}{R}\frac{\partial R}{\partial r} = -\frac{1}{Y}\frac{\partial^2Y}{\partial\theta^2}
 R = R(r) Y = Y(\theta) 
Y_m(\theta) = \frac{1}{\sqrt{2\pi}}e^{im\theta}
","['derivatives', 'partial-differential-equations', 'mathematical-physics', 'wave-equation']"
41,"Profile decomposition of $f$ satisfying $\lim_{x\to\infty}[f(x+a)-f(x)]=0$, for any $a\geq 0$.","Profile decomposition of  satisfying , for any .",f \lim_{x\to\infty}[f(x+a)-f(x)]=0 a\geq 0,"Let $f\in C[0,\infty)$ , and $\forall\ a\geq 0, \lim_{x\to\infty}[f(x+a)-f(x)]=0.$ Show that there exists continuous $g$ and continously differentiable $h$ , such that $f=g+h$ , $\lim_{x\to\infty}g(x)=0, \lim_{x\to\infty}h'(x)=0$ . Let $h(x)=\int_x^{x+1}f(t)dt$ , then $\lim_{x\to\infty}h'(x)=0$ simutanenously. But the property of $g$ maybe difficult to prove.","Let , and Show that there exists continuous and continously differentiable , such that , . Let , then simutanenously. But the property of maybe difficult to prove.","f\in C[0,\infty) \forall\ a\geq 0, \lim_{x\to\infty}[f(x+a)-f(x)]=0. g h f=g+h \lim_{x\to\infty}g(x)=0, \lim_{x\to\infty}h'(x)=0 h(x)=\int_x^{x+1}f(t)dt \lim_{x\to\infty}h'(x)=0 g","['calculus', 'derivatives', 'continuity']"
42,Connection between Determinant and the Quotient Rule,Connection between Determinant and the Quotient Rule,,"For the function $\frac{f(x)}{g(x)}$ , by the Quotient Rule, $\frac{d}{dx} (\frac{f}{g}) = \dfrac{\frac{df}{dx}g-f \frac{dg}{dx}}{g^2}$ . We can write the numerator as $\begin{vmatrix} \frac{df}{dx} & f \\ \frac{dg}{dx} & g \end{vmatrix}$ . I know how to prove the formula by definition of derivative, but I wonder why determinant appears in the numerator? Is there any mathematical relation between derivative of $\frac fg$ and determinant that gives us $\begin{vmatrix} \frac{df}{dx} & f \\ \frac{dg}{dx} & g \end{vmatrix}$ right away?","For the function , by the Quotient Rule, . We can write the numerator as . I know how to prove the formula by definition of derivative, but I wonder why determinant appears in the numerator? Is there any mathematical relation between derivative of and determinant that gives us right away?",\frac{f(x)}{g(x)} \frac{d}{dx} (\frac{f}{g}) = \dfrac{\frac{df}{dx}g-f \frac{dg}{dx}}{g^2} \begin{vmatrix} \frac{df}{dx} & f \\ \frac{dg}{dx} & g \end{vmatrix} \frac fg \begin{vmatrix} \frac{df}{dx} & f \\ \frac{dg}{dx} & g \end{vmatrix},"['calculus', 'derivatives', 'determinant']"
43,How to find the partial derivatives of the following nested expression?,How to find the partial derivatives of the following nested expression?,,"I want to find the partial derivatives of the expression for $v_3(\boldsymbol{u})$ with respect to $u_1$ , $u_2$ and $u_3$ from the expressions below. Here $\Phi$ denotes the cumulative distribution function of the standard normal probability distribution. I think it should be possible to solve it analytically by employing the chain rule. However, I have a feeling that the expressions quickly will become quite ugly. Any suggestions on how this can be done? $$h(\boldsymbol{u}) = F_{H_s}^{-1}(\boldsymbol{\Phi}(u_1)) = \alpha[-\ln(1-\Phi(u_1))]^{1/\beta}$$ $$t(\boldsymbol{u}) = F_{T_z|H_s}^{-1}(\boldsymbol{\Phi}(u_2)|h(\boldsymbol{u})) = \exp({\mu(h(\boldsymbol{u})) + \sigma(h(\boldsymbol{u})) u_2})$$ $$v_3(\boldsymbol{u}) = F_{V_3|Tz,H_s}^{-1}(\boldsymbol{\Phi}(u_3)|h(\boldsymbol{u}),t(\boldsymbol{u})) = \sqrt{-2m_0(h(\boldsymbol{u}),t(\boldsymbol{u})) \ln\left(-\frac{2\pi}{\widetilde{T}}\sqrt{\frac{m_0(h(\boldsymbol{u}),t(\boldsymbol{u}))}{m_2(h(\boldsymbol{u}),t(\boldsymbol{u}))}}\ln\boldsymbol{\Phi}(u_3)\right)}$$ Here $\mu(h(\boldsymbol{u}))$ and $\sigma(h(\boldsymbol{u}))$ denotes the h-dependent mean and standard deviation of the lognormal distribution, respectively. These are given by: $\mu(h(\boldsymbol{u})) = a_0 + a_1 h ^ {a_2}$ $\sigma(h(\boldsymbol{u})) = b_0 + b_1 \exp(b_2 h)$ where $a_0 = 0.70, a_1 = 0.282, a_2 = 0.167, b_0 = 0.07, b_1 = 0.3449, b_2 = -0.2073$ . Furthermore, $\alpha = 1.76$ and $\beta = 1.59$ .","I want to find the partial derivatives of the expression for with respect to , and from the expressions below. Here denotes the cumulative distribution function of the standard normal probability distribution. I think it should be possible to solve it analytically by employing the chain rule. However, I have a feeling that the expressions quickly will become quite ugly. Any suggestions on how this can be done? Here and denotes the h-dependent mean and standard deviation of the lognormal distribution, respectively. These are given by: where . Furthermore, and .","v_3(\boldsymbol{u}) u_1 u_2 u_3 \Phi h(\boldsymbol{u}) = F_{H_s}^{-1}(\boldsymbol{\Phi}(u_1)) = \alpha[-\ln(1-\Phi(u_1))]^{1/\beta} t(\boldsymbol{u}) = F_{T_z|H_s}^{-1}(\boldsymbol{\Phi}(u_2)|h(\boldsymbol{u})) = \exp({\mu(h(\boldsymbol{u})) + \sigma(h(\boldsymbol{u})) u_2}) v_3(\boldsymbol{u}) = F_{V_3|Tz,H_s}^{-1}(\boldsymbol{\Phi}(u_3)|h(\boldsymbol{u}),t(\boldsymbol{u})) = \sqrt{-2m_0(h(\boldsymbol{u}),t(\boldsymbol{u})) \ln\left(-\frac{2\pi}{\widetilde{T}}\sqrt{\frac{m_0(h(\boldsymbol{u}),t(\boldsymbol{u}))}{m_2(h(\boldsymbol{u}),t(\boldsymbol{u}))}}\ln\boldsymbol{\Phi}(u_3)\right)} \mu(h(\boldsymbol{u})) \sigma(h(\boldsymbol{u})) \mu(h(\boldsymbol{u})) = a_0 + a_1 h ^ {a_2} \sigma(h(\boldsymbol{u})) = b_0 + b_1 \exp(b_2 h) a_0 = 0.70, a_1 = 0.282, a_2 = 0.167, b_0 = 0.07, b_1 = 0.3449, b_2 = -0.2073 \alpha = 1.76 \beta = 1.59","['derivatives', 'probability-distributions', 'partial-derivative', 'nested-radicals', 'reliability']"
44,Continuity of derivative of a continuous monotone function,Continuity of derivative of a continuous monotone function,,"We know that every continuous monotone function defined on [a, b] possesses a finite derivative almost everywhere. What about the continuity of the derivative? Can I say that the derivative of this function is continuous almost everywhere? Or if we define $\tilde f(x) = \liminf_{y \to x-} \frac{f(y)-f(x)}{y-x}$ , what is the continuity of $\tilde f(x)$ ? Is that continuous almost everywhere?","We know that every continuous monotone function defined on [a, b] possesses a finite derivative almost everywhere. What about the continuity of the derivative? Can I say that the derivative of this function is continuous almost everywhere? Or if we define , what is the continuity of ? Is that continuous almost everywhere?",\tilde f(x) = \liminf_{y \to x-} \frac{f(y)-f(x)}{y-x} \tilde f(x),"['real-analysis', 'derivatives', 'continuity']"
45,What are most ways to differentiate the same function?,What are most ways to differentiate the same function?,,"An example of a function that relates to my question: $f(x)=\frac{x^3\left(2x+1\right)^2}{x^2}$ with $x>0$ . That function could be differentiated (at least) 5 different ways using rules . Simplify, expand and differentiate each term: $f(x)=4x^3+4x^2+x$ then $f'(x)=12x^2+8x+1$ Product rule: $f(x)=x(2x+1)(2x+1)$ then $f'(x)=(1)(2x+1)(2x+1)+(x)(2)(2x+1)+(x)(2x+1)(2)=12x^2+8x+1$ Chain rule with product rule : $f(x)=x\cdot(2x+1)^2$ then $f'(x)=(1)(2x+1)^2+(x)(2)(2x+1)(2)=12x^2+8x+1$ Logarithmic differentiation: $\ln\left(f(x)\right)=\ln\left(x\left(2x+1\right)^2\right)$ then $f'(x)=\left(\frac{1}{x}+2\cdot\frac{1}{2x+1}\cdot2\right)\left[x\left(2x+1\right)^2\right]=12x^2+8x+1$ Quotient rule with product rule : $f'(x)=\frac{\left(\left(3x^2\right)\left(2x+1\right)^2+\left(x^3\right)\left(2\left(2x+1\right)2\right)\right)x^2-\left(x^3\left(2x+1\right)^2\right)2x}{\left(x^2\right)^2}=12x^2+8x+1$ (And then, there is the limit definition of the derivative) The purpose of the question is to introduce new ways of differentiating with functions that have been used previously. Also, students should think about which way is most efficient, but that only works if there are multiple (rule-based) ways of differentiating the same function.","An example of a function that relates to my question: with . That function could be differentiated (at least) 5 different ways using rules . Simplify, expand and differentiate each term: then Product rule: then Chain rule with product rule : then Logarithmic differentiation: then Quotient rule with product rule : (And then, there is the limit definition of the derivative) The purpose of the question is to introduce new ways of differentiating with functions that have been used previously. Also, students should think about which way is most efficient, but that only works if there are multiple (rule-based) ways of differentiating the same function.",f(x)=\frac{x^3\left(2x+1\right)^2}{x^2} x>0 f(x)=4x^3+4x^2+x f'(x)=12x^2+8x+1 f(x)=x(2x+1)(2x+1) f'(x)=(1)(2x+1)(2x+1)+(x)(2)(2x+1)+(x)(2x+1)(2)=12x^2+8x+1 f(x)=x\cdot(2x+1)^2 f'(x)=(1)(2x+1)^2+(x)(2)(2x+1)(2)=12x^2+8x+1 \ln\left(f(x)\right)=\ln\left(x\left(2x+1\right)^2\right) f'(x)=\left(\frac{1}{x}+2\cdot\frac{1}{2x+1}\cdot2\right)\left[x\left(2x+1\right)^2\right]=12x^2+8x+1 f'(x)=\frac{\left(\left(3x^2\right)\left(2x+1\right)^2+\left(x^3\right)\left(2\left(2x+1\right)2\right)\right)x^2-\left(x^3\left(2x+1\right)^2\right)2x}{\left(x^2\right)^2}=12x^2+8x+1,['calculus']
46,Visualize Itô differentiation rule,Visualize Itô differentiation rule,,"Please help me to find an idea to visualize $$\displaystyle d{ f(t,x)} = \frac{\partial f(t,x)}{\partial t}dt + \frac{\partial f(t,x)}{\partial x}dx + \frac12 \frac{\partial^2f(t,x)}{\partial x^2} dt$$ To clarify the question first make an example,  when we want to visualize $d(xy)$ (where x,y are function of $t$ ) we can use a rectangle  with $x+\Delta x ,y+\Delta y$ as long and short side,like the picture below $$\Delta(xy)=(x+\Delta x)(y+\Delta y)-xy\\=x\Delta y+y\Delta x+\Delta x\Delta y$$ so ignore term of $\Delta x\Delta y = somethimg(x)\Delta t \times somethimg(y)\Delta t=something (\Delta t)^2 \to 0$ and say $d(xy)=xdy +ydx$ But in the case of Itô, $(dw_t)^2$ tends to $dt$ . Now can you help me to find an idea to show (or visualize like rectangle example or something else ) $$\displaystyle d{ f(t,x)} = \frac{\partial f(t,x)}{\partial t}dt + \frac{\partial f(t,x)}{\partial x}dw_t + \frac12 \frac{\partial^2f(t,x)}{\partial x^2} (dw_t)^2$$","Please help me to find an idea to visualize To clarify the question first make an example,  when we want to visualize (where x,y are function of ) we can use a rectangle  with as long and short side,like the picture below so ignore term of and say But in the case of Itô, tends to . Now can you help me to find an idea to show (or visualize like rectangle example or something else )","\displaystyle d{ f(t,x)}
=
\frac{\partial f(t,x)}{\partial t}dt
+ \frac{\partial f(t,x)}{\partial x}dx
+ \frac12 \frac{\partial^2f(t,x)}{\partial x^2}
dt d(xy) t x+\Delta x ,y+\Delta y \Delta(xy)=(x+\Delta x)(y+\Delta y)-xy\\=x\Delta y+y\Delta x+\Delta x\Delta y \Delta x\Delta y = somethimg(x)\Delta t \times somethimg(y)\Delta t=something (\Delta t)^2 \to 0 d(xy)=xdy +ydx (dw_t)^2 dt \displaystyle d{ f(t,x)}
=
\frac{\partial f(t,x)}{\partial t}dt
+ \frac{\partial f(t,x)}{\partial x}dw_t
+ \frac12 \frac{\partial^2f(t,x)}{\partial x^2}
(dw_t)^2","['derivatives', 'stochastic-calculus', 'alternative-proof', 'stochastic-analysis', 'visualization']"
47,Are most physics books wrong about the covariant derivative and connection?,Are most physics books wrong about the covariant derivative and connection?,,"I have always read in many physics books that a valid way of intuitively introducing the covariant derivative and the connection was the following: (example in GR but same thing for gauge theories) Let $TM$ be the tangent bundle over a manifold $M$ . I define the ""weird"" derivative $\partial^{\mathfrak w}$ of a vector $V$ (only the element of the tangent space obtained by projecting to the second element) at $\gamma(t_o)$ as $$\partial^{\mathfrak w}_WV=\lim_{h\to 0} \frac{V_{\gamma(t_o+h)}-V_{\gamma(t_o)}}{h}$$ where $W_{\gamma(t_o)}=\dot\gamma(t_o)$ . This derivative breaks the properties of a vector, because if the limit is to be intended as ""limit of components"" then transition function (jacobian) loses its meaning, whereas if intended as subtraction of differential operators, independent of coordinates, then it does not make sense because they do not take functions over the same point in $M$ . But all of this because I am exceeding out of the fiber. In general here you read in these books that you cannot sum different vectors because they are belonging to different tangent spaces (fibers), so you introduce a covariant differentiation. But we know that the tangent space is a vector bundle and thus it admits a local trivialization, therefore the fibers are locally isomorphic to $\mathbb R^n$ , so the two spaces are indeed isomorphic. $\textbf{Question}$ : I would say that I can define such a weird derivative in the sense of components in $\mathbb R^n$ , I would simply go out of the fiber and thus I can no longer talk about vectors and change of charts. What do you think? (Btw, in components this derivative would be the ordinary derivative $\partial_\mu$ that we find in the covariant derivative $\nabla_\mu V^\nu=\partial_\mu V^\nu+\Gamma_{\mu\alpha}^\nu V^\alpha$ .)","I have always read in many physics books that a valid way of intuitively introducing the covariant derivative and the connection was the following: (example in GR but same thing for gauge theories) Let be the tangent bundle over a manifold . I define the ""weird"" derivative of a vector (only the element of the tangent space obtained by projecting to the second element) at as where . This derivative breaks the properties of a vector, because if the limit is to be intended as ""limit of components"" then transition function (jacobian) loses its meaning, whereas if intended as subtraction of differential operators, independent of coordinates, then it does not make sense because they do not take functions over the same point in . But all of this because I am exceeding out of the fiber. In general here you read in these books that you cannot sum different vectors because they are belonging to different tangent spaces (fibers), so you introduce a covariant differentiation. But we know that the tangent space is a vector bundle and thus it admits a local trivialization, therefore the fibers are locally isomorphic to , so the two spaces are indeed isomorphic. : I would say that I can define such a weird derivative in the sense of components in , I would simply go out of the fiber and thus I can no longer talk about vectors and change of charts. What do you think? (Btw, in components this derivative would be the ordinary derivative that we find in the covariant derivative .)",TM M \partial^{\mathfrak w} V \gamma(t_o) \partial^{\mathfrak w}_WV=\lim_{h\to 0} \frac{V_{\gamma(t_o+h)}-V_{\gamma(t_o)}}{h} W_{\gamma(t_o)}=\dot\gamma(t_o) M \mathbb R^n \textbf{Question} \mathbb R^n \partial_\mu \nabla_\mu V^\nu=\partial_\mu V^\nu+\Gamma_{\mu\alpha}^\nu V^\alpha,"['derivatives', 'differential-geometry', 'fiber-bundles']"
48,Quadratics: Intuitive relation between discriminant and derivative at roots,Quadratics: Intuitive relation between discriminant and derivative at roots,,"While working with quadratics that have real roots, I realized an interesting fact: The slope of a quadratic at its roots is equal to $\pm \sqrt{D}$ where $D=b^2-4ac$ Proof: $$f(x) = ax^2 + bx +c$$ $$f'(x) = 2ax+b$$ Roots: $$x = \frac{-b\pm\sqrt{b^2-4ac}}{2a}$$ So, if we try to find the slope at any root ( $r$ ): $$f’(r) = \pm \sqrt D$$ where the sign ( $\pm$ ) can be determined by whether the root is on the right of the vertex or the left. If the quadratic has only $1$ root (or $2$ roots that are the same) then it means the quadratic is at a stationary point so the slope must be $0$ . This is backed by the fact that quadratics have only 1 distinct root when $b^2 - 4ac = 0$ . What geometric/intuitive approach can be applied to explain this interesting phenomenon?","While working with quadratics that have real roots, I realized an interesting fact: The slope of a quadratic at its roots is equal to where Proof: Roots: So, if we try to find the slope at any root ( ): where the sign ( ) can be determined by whether the root is on the right of the vertex or the left. If the quadratic has only root (or roots that are the same) then it means the quadratic is at a stationary point so the slope must be . This is backed by the fact that quadratics have only 1 distinct root when . What geometric/intuitive approach can be applied to explain this interesting phenomenon?",\pm \sqrt{D} D=b^2-4ac f(x) = ax^2 + bx +c f'(x) = 2ax+b x = \frac{-b\pm\sqrt{b^2-4ac}}{2a} r f’(r) = \pm \sqrt D \pm 1 2 0 b^2 - 4ac = 0,"['derivatives', 'polynomials', 'roots', 'quadratics', 'slope']"
49,Half Derivative of $\tan(x)$,Half Derivative of,\tan(x),"I know how to find one of many half derivatives of $\sin(x)$ and $\cos(x)$ which are, $${D^{\frac{1}{2}}}\sin(x)=\sin(x+\frac{\pi}{4})$$ $${D^{\frac{1}{2}}}\cos(x)=\cos(x+\frac{\pi}{4})$$ Using these is it possible to find a half derivative for $\tan(x)$ by letting. $$\tan(x)=\frac{\sin(x)}{\cos(x)}$$ Does the Quotient rule apply for half derivatives as well? For example, $$D^{\frac{1}{2}}\tan(x)=\frac{D^{\frac{1}{2}}[\sin(x)]\cos(x)-D^{\frac{1}{2}}[\cos(x)]\sin(x)}{\cos^2(x)}$$ As a side question, does the Product and Chain rule also apply for half derivatives, or some other form of it?","I know how to find one of many half derivatives of $\sin(x)$ and $\cos(x)$ which are, $${D^{\frac{1}{2}}}\sin(x)=\sin(x+\frac{\pi}{4})$$ $${D^{\frac{1}{2}}}\cos(x)=\cos(x+\frac{\pi}{4})$$ Using these is it possible to find a half derivative for $\tan(x)$ by letting. $$\tan(x)=\frac{\sin(x)}{\cos(x)}$$ Does the Quotient rule apply for half derivatives as well? For example, $$D^{\frac{1}{2}}\tan(x)=\frac{D^{\frac{1}{2}}[\sin(x)]\cos(x)-D^{\frac{1}{2}}[\cos(x)]\sin(x)}{\cos^2(x)}$$ As a side question, does the Product and Chain rule also apply for half derivatives, or some other form of it?",,"['calculus', 'trigonometry', 'derivatives']"
50,Linearity of pushforward of tangent vectors and differentiability,Linearity of pushforward of tangent vectors and differentiability,,"Let $f:U\to V$ be a continuous map of open subsets of Euclidean spaces $U\subset\mathbb R^m,V\subset\mathbb R^n$. Suppose: For every differentiable curve $\gamma$ in $U$ based at $p\in U$ the composite $f\circ \gamma$ is a differentiable curve in $V$ based at $fp$. There's a well-defined assignment $\gamma^\prime(0)\mapsto (f\circ \gamma)^\prime(0)$ which is moreover a linear map $\mathbb R^m=\mathrm T_pU\to \mathrm T_{fp}V=\mathbb R^n$. Does it follow that $f$ is differentiable at $p$? Example 3.3 of Kriegl and Michor's The Convenient Setting of Global Analysis provides a non-differentiable function $\mathbb R^2\to \mathbb R^2$ satisfying the first condition, but it does not seem to satisfy the second.","Let $f:U\to V$ be a continuous map of open subsets of Euclidean spaces $U\subset\mathbb R^m,V\subset\mathbb R^n$. Suppose: For every differentiable curve $\gamma$ in $U$ based at $p\in U$ the composite $f\circ \gamma$ is a differentiable curve in $V$ based at $fp$. There's a well-defined assignment $\gamma^\prime(0)\mapsto (f\circ \gamma)^\prime(0)$ which is moreover a linear map $\mathbb R^m=\mathrm T_pU\to \mathrm T_{fp}V=\mathbb R^n$. Does it follow that $f$ is differentiable at $p$? Example 3.3 of Kriegl and Michor's The Convenient Setting of Global Analysis provides a non-differentiable function $\mathbb R^2\to \mathbb R^2$ satisfying the first condition, but it does not seem to satisfy the second.",,"['calculus', 'derivatives', 'differential-geometry']"
51,Hyper smooth and ultra smooth functions,Hyper smooth and ultra smooth functions,,"So I was mixing smooth functions with fractional calculus when I came upon the following idea.  We could have functions in $D^\alpha$ defined as follows: $f(x)\in D^1\iff\frac d{dx}f(x)=f'(x)$ exists. $f(x)\in D^2\iff\frac{d^2}{dx^2}f(x)=f''(x)$ exists. $f(x)\in D^k\iff\frac{d^k}{dx^k}f(x)=f^{(k)}(x)$ exists. $f(x)\in D^\omega\iff\forall k<\omega(f(x)\in D^k)$, which is equivalent to $f(x)$ being a smooth function. Next, we need to define fractional derivatives .  Per this question, I don't mind which fractional derivative you use, so long as it is one of the many well known and accepted fractional derivatives. Now, we may extend our derivatives further to what I will call hyper smooth functions. $f(x)\in D^{\omega+1}\iff\frac d{dt}f^{(t)}(x)=f^{(1,t)}(x)$ exists. $f(x)\in D^{\omega+2}\iff\frac{d^2}{dt^2}f^{(t)}(x)=f^{(2,t)}(x)$ exists. $f(x)\in D^{\omega+k}\iff\frac{d^k}{dt^k}f^{(t)}(x)=f^{(k,t)}(x)$ exists. $f(x)\in D^{\omega2}\iff\forall\alpha<\omega2(f(x)\in D^\alpha)$ $f(x)\in D^{\omega2+k}\iff\frac{d^k}{du^k}f^{(u,t)}(x)=f^{(k,u,t)}(x)$ exists. $f(x)\in D^{\omega3}\iff\forall\alpha<\omega3(f(x)\in D^\alpha)$ And so on, defining ourselves $D^\alpha$ for every $\alpha\le\omega^2$.  A function is called ultra smooth if we have: $f(x)\in D^{\omega^2}\iff\forall\alpha<\omega^2(f(x)\in D^\alpha)$ But are these functions unique?  Is $\{f(x)\in D^\omega\land f(x)\notin D^{\omega+1}\}$ an empty set or not?  It seems rather hard to find examples (and don't forget you are free to use whichever fractional derivative you choose).  In general, can a function be in $D^\alpha$ but not in $D^\beta$ for $\omega\le\alpha<\beta\le\omega^2$? Assuming we enforce a domain restriction that everything must be $\mathbb R\mapsto\mathbb R$, exponential functions are an appropriate example.  Under certain definitions and $a>0$, we have $$\frac{d^t}{dx^t}a^x=a^x(\ln(a))^t$$ However, when $0<a<1$, we find that $\frac d{dt}\frac{d^t}{dx^t}a^x$ does not exist (as a function $\mathbb R\mapsto\mathbb R$) and so this is an example of a function that is smooth but not hyper smooth.  Likewise this pattern can be used to show the existence of functions $f(x)\in D^{\omega k}$ and $f(x)\notin D^{\omega k+1}$. However, I cannot find any examples where $f(x)\in D^{\alpha+1}$ but $f(x)\notin D^{\alpha+2}$. Allowing functions to be $\mathbb C\mapsto\mathbb C$, I cannot find any other examples of this.","So I was mixing smooth functions with fractional calculus when I came upon the following idea.  We could have functions in $D^\alpha$ defined as follows: $f(x)\in D^1\iff\frac d{dx}f(x)=f'(x)$ exists. $f(x)\in D^2\iff\frac{d^2}{dx^2}f(x)=f''(x)$ exists. $f(x)\in D^k\iff\frac{d^k}{dx^k}f(x)=f^{(k)}(x)$ exists. $f(x)\in D^\omega\iff\forall k<\omega(f(x)\in D^k)$, which is equivalent to $f(x)$ being a smooth function. Next, we need to define fractional derivatives .  Per this question, I don't mind which fractional derivative you use, so long as it is one of the many well known and accepted fractional derivatives. Now, we may extend our derivatives further to what I will call hyper smooth functions. $f(x)\in D^{\omega+1}\iff\frac d{dt}f^{(t)}(x)=f^{(1,t)}(x)$ exists. $f(x)\in D^{\omega+2}\iff\frac{d^2}{dt^2}f^{(t)}(x)=f^{(2,t)}(x)$ exists. $f(x)\in D^{\omega+k}\iff\frac{d^k}{dt^k}f^{(t)}(x)=f^{(k,t)}(x)$ exists. $f(x)\in D^{\omega2}\iff\forall\alpha<\omega2(f(x)\in D^\alpha)$ $f(x)\in D^{\omega2+k}\iff\frac{d^k}{du^k}f^{(u,t)}(x)=f^{(k,u,t)}(x)$ exists. $f(x)\in D^{\omega3}\iff\forall\alpha<\omega3(f(x)\in D^\alpha)$ And so on, defining ourselves $D^\alpha$ for every $\alpha\le\omega^2$.  A function is called ultra smooth if we have: $f(x)\in D^{\omega^2}\iff\forall\alpha<\omega^2(f(x)\in D^\alpha)$ But are these functions unique?  Is $\{f(x)\in D^\omega\land f(x)\notin D^{\omega+1}\}$ an empty set or not?  It seems rather hard to find examples (and don't forget you are free to use whichever fractional derivative you choose).  In general, can a function be in $D^\alpha$ but not in $D^\beta$ for $\omega\le\alpha<\beta\le\omega^2$? Assuming we enforce a domain restriction that everything must be $\mathbb R\mapsto\mathbb R$, exponential functions are an appropriate example.  Under certain definitions and $a>0$, we have $$\frac{d^t}{dx^t}a^x=a^x(\ln(a))^t$$ However, when $0<a<1$, we find that $\frac d{dt}\frac{d^t}{dx^t}a^x$ does not exist (as a function $\mathbb R\mapsto\mathbb R$) and so this is an example of a function that is smooth but not hyper smooth.  Likewise this pattern can be used to show the existence of functions $f(x)\in D^{\omega k}$ and $f(x)\notin D^{\omega k+1}$. However, I cannot find any examples where $f(x)\in D^{\alpha+1}$ but $f(x)\notin D^{\alpha+2}$. Allowing functions to be $\mathbb C\mapsto\mathbb C$, I cannot find any other examples of this.",,"['calculus', 'derivatives', 'fractional-calculus']"
52,Construction of $f$ so that $f^{(n)}(0)=(n!)^2$: does this work?,Construction of  so that : does this work?,f f^{(n)}(0)=(n!)^2,"A question ( Are derivatives actually bounded? ) has been asked on Stackexchange as to whether there exists $f\in C^{\infty}$ such that $f^{(n)}(0)=(n!)^2$. Obviously $f$ is not analytic but the respondent cites Borel's theorem as implying an affirmative answer. For fun, I decided to construct one such $f$, and I hit on the following: $g_n(x)=\exp\frac{\exp\frac{1}{n|x|-1}}{-|x|}$ for $0<|x|<\frac{1}{n}$ $f_n(x)=x^nn![1-g_n(x)]$ for $0<|x|<\frac{1}{n}$; anywhere else, $f_n(x)=0$ $f(x)=\sum_{n=1}^{\infty} f_n(x)$ Are there functions with that property which are easier to verify? Because it took me ages to produce what I believe to be a proof that the k-th derivative of $\sum_{n=1}^{\infty}x^nn!g_n(x)$ is a uniformly convergent series. And is $f$ the required function?","A question ( Are derivatives actually bounded? ) has been asked on Stackexchange as to whether there exists $f\in C^{\infty}$ such that $f^{(n)}(0)=(n!)^2$. Obviously $f$ is not analytic but the respondent cites Borel's theorem as implying an affirmative answer. For fun, I decided to construct one such $f$, and I hit on the following: $g_n(x)=\exp\frac{\exp\frac{1}{n|x|-1}}{-|x|}$ for $0<|x|<\frac{1}{n}$ $f_n(x)=x^nn![1-g_n(x)]$ for $0<|x|<\frac{1}{n}$; anywhere else, $f_n(x)=0$ $f(x)=\sum_{n=1}^{\infty} f_n(x)$ Are there functions with that property which are easier to verify? Because it took me ages to produce what I believe to be a proof that the k-th derivative of $\sum_{n=1}^{\infty}x^nn!g_n(x)$ is a uniformly convergent series. And is $f$ the required function?",,"['real-analysis', 'derivatives']"
53,Derivative of remainder function wrt denominator,Derivative of remainder function wrt denominator,,"Given $f(x,y) = x \mathbin{\%} y = x - y \lfloor \frac{x}{y} \rfloor$, I want to find the partials ${{\partial f}\over{\partial y}}$ and ${{\partial f}\over{\partial x}}$ I understand there will be discontinuities in both. Intuitively ${{\partial f}\over{\partial x}} = 1$ where $\frac{x}{y} \notin \mathbb{Z}$ However, ${{\partial f}\over{\partial y}}$ seems more tricky. If I try to use intuition and define it piecewise, it seems like it may be: $$ {{\partial f}\over{\partial y}} = \left\{\begin{aligned} &0 &&: 0 < x < y\\ &\text{undefined} &&: \frac{x}{y} \in \mathbb{Z} \\ &-1 &&: \text{otherwise} \end{aligned} \right.$$ Or something similar. However, if I derive it from the above definition: $$ \begin{aligned} {{\partial f}\over{\partial y}} &= -\frac{\partial}{\partial y} \operatorname{floor}\left(\frac{x}{y}\right)\\[6pt] &= \frac{\partial}{\partial y} \operatorname{floor} \left(\frac{x}{y}\right)\frac{x}{y^{2}}\\ \end{aligned} $$ But it's my understanding that $\frac{\partial}{\partial x}\operatorname{floor}(x) = 0, \forall x \notin \mathbb{Z}$ which implies $\frac{\partial f}{\partial y} = 0$. This just feels like an incomplete picture and I hope I've made some error or poor assumption somewhere. Would really appreciate some insight into alternative definitions of modulo that I could use here.","Given $f(x,y) = x \mathbin{\%} y = x - y \lfloor \frac{x}{y} \rfloor$, I want to find the partials ${{\partial f}\over{\partial y}}$ and ${{\partial f}\over{\partial x}}$ I understand there will be discontinuities in both. Intuitively ${{\partial f}\over{\partial x}} = 1$ where $\frac{x}{y} \notin \mathbb{Z}$ However, ${{\partial f}\over{\partial y}}$ seems more tricky. If I try to use intuition and define it piecewise, it seems like it may be: $$ {{\partial f}\over{\partial y}} = \left\{\begin{aligned} &0 &&: 0 < x < y\\ &\text{undefined} &&: \frac{x}{y} \in \mathbb{Z} \\ &-1 &&: \text{otherwise} \end{aligned} \right.$$ Or something similar. However, if I derive it from the above definition: $$ \begin{aligned} {{\partial f}\over{\partial y}} &= -\frac{\partial}{\partial y} \operatorname{floor}\left(\frac{x}{y}\right)\\[6pt] &= \frac{\partial}{\partial y} \operatorname{floor} \left(\frac{x}{y}\right)\frac{x}{y^{2}}\\ \end{aligned} $$ But it's my understanding that $\frac{\partial}{\partial x}\operatorname{floor}(x) = 0, \forall x \notin \mathbb{Z}$ which implies $\frac{\partial f}{\partial y} = 0$. This just feels like an incomplete picture and I hope I've made some error or poor assumption somewhere. Would really appreciate some insight into alternative definitions of modulo that I could use here.",,"['derivatives', 'modular-arithmetic', 'partial-derivative']"
54,Non-integer order derivative,Non-integer order derivative,,"I do not know much about fractional calculus, except what I have read in a few short posts at MSE and https://en.wikipedia.org/wiki/Fractional_calculus . I know that order of a derivative can be extended from rational values to real values, but all I know is what is written here . So my question is, what is the simplest way to understand and/or define $\dfrac{d^n}{dx^n}f(x),$ for $n\in\mathbb{R}$? For example what is $\dfrac{d^\pi}{dx^\pi}f(x)$? Also, what about complex values ?  What might we say about $\dfrac{d^{s}}{dz^s}f(z)$ for $s\in\mathbb{C}$? And what are some uses for such derivatives?","I do not know much about fractional calculus, except what I have read in a few short posts at MSE and https://en.wikipedia.org/wiki/Fractional_calculus . I know that order of a derivative can be extended from rational values to real values, but all I know is what is written here . So my question is, what is the simplest way to understand and/or define $\dfrac{d^n}{dx^n}f(x),$ for $n\in\mathbb{R}$? For example what is $\dfrac{d^\pi}{dx^\pi}f(x)$? Also, what about complex values ?  What might we say about $\dfrac{d^{s}}{dz^s}f(z)$ for $s\in\mathbb{C}$? And what are some uses for such derivatives?",,"['functional-analysis', 'fractional-calculus']"
55,"Derivatives, discrete and continuous, of $(1/\sqrt{n})\cos (t\log n)$ and $(1/\sqrt{n})\sin (t\log n)$ and Cauchy-Riemann equations","Derivatives, discrete and continuous, of  and  and Cauchy-Riemann equations",(1/\sqrt{n})\cos (t\log n) (1/\sqrt{n})\sin (t\log n),"For any arithmetical function $f(n)$, we define its derivative to be $f'(n)=f(n)\cdot \log n$ for $n\geq 1$ (see for example [1], page 45 or Wikipedia). Fact. The functions $u(n,t)=(1/\sqrt{n})\cos (t\log n)$ and $v(n,t)=(1/\sqrt{n})\sin (t\log n)$, for integers $n\geq 1$ and real $t$, satisfy the modified Cauchy-Riemann    $$\begin{cases} u_n=v_t \\ u_t=-v_n \end{cases}$$   where the derivative with respect to $n$ is taken in the sense of the derivative of an arithmetical function. My question, and please if someone don't understand it, and another user can edit my post, or clarify my words in a comment I am agree, is Question. Is possible (thus could be in the literature or in other case we can define it) to define a derivative $D$ acting on a complex function $f$ (this function $f$ is defined from $u$ and $v$) such that implies the system of previous modified Cauchy-Riemann equations? Or is impossible find a mixture of a discrete and continuos derivative in this way? Thanks in advance. References: [1] Apostol, Introduction to Analytic Number Theory, Springer, page 45. [2] Wikipedia, Arithmetical Function, Cauchy Riemann Equations.","For any arithmetical function $f(n)$, we define its derivative to be $f'(n)=f(n)\cdot \log n$ for $n\geq 1$ (see for example [1], page 45 or Wikipedia). Fact. The functions $u(n,t)=(1/\sqrt{n})\cos (t\log n)$ and $v(n,t)=(1/\sqrt{n})\sin (t\log n)$, for integers $n\geq 1$ and real $t$, satisfy the modified Cauchy-Riemann    $$\begin{cases} u_n=v_t \\ u_t=-v_n \end{cases}$$   where the derivative with respect to $n$ is taken in the sense of the derivative of an arithmetical function. My question, and please if someone don't understand it, and another user can edit my post, or clarify my words in a comment I am agree, is Question. Is possible (thus could be in the literature or in other case we can define it) to define a derivative $D$ acting on a complex function $f$ (this function $f$ is defined from $u$ and $v$) such that implies the system of previous modified Cauchy-Riemann equations? Or is impossible find a mixture of a discrete and continuos derivative in this way? Thanks in advance. References: [1] Apostol, Introduction to Analytic Number Theory, Springer, page 45. [2] Wikipedia, Arithmetical Function, Cauchy Riemann Equations.",,['derivatives']
56,Derivative of $f(x)^{g(x)}$ at points when $f(x)=0$,Derivative of  at points when,f(x)^{g(x)} f(x)=0,"I am interested in understanding the general behavior of the derivative for $$f(x)^{g(x)}$$ at points where $f(x)=0$. For example, if $f^g=x^n$ we have $$\frac{d}{dx}f^g(0)=\begin{cases}0 & n\ge 1 \\ \pm\infty & n<1\end{cases}$$ The general formula $$(f^g)'=f^g\left(g'\ln |f|+g\frac{f'}{f}\right)$$ breaks down when $f=0$, though as the example above shows the derivative may still exist there. I am not sure what the proper assumptions should be. Tentatively, take $f(x)^{g(x)}\ge 0$ for all $x$ in the relevant domain, so that (I believe) we have $$\ln \left(f(x)^{g(x)}\right)=g(x)\ln |f(x)|$$ when $f$ is strictly positive and undefined otherwise. Also, is there any non-trivial example of a well-defined function (meaning ""nice"", as in differentiable almost everywhere) $f^g$ where $f$ takes on negative values and where $g$ is not constant? In other words, for cases of $f<0$ are the only functions worth considering of the form $f^c$ for constants $c$? EDIT: I want the function(s) to be real, although arguments using complex numbers are of course permissible.","I am interested in understanding the general behavior of the derivative for $$f(x)^{g(x)}$$ at points where $f(x)=0$. For example, if $f^g=x^n$ we have $$\frac{d}{dx}f^g(0)=\begin{cases}0 & n\ge 1 \\ \pm\infty & n<1\end{cases}$$ The general formula $$(f^g)'=f^g\left(g'\ln |f|+g\frac{f'}{f}\right)$$ breaks down when $f=0$, though as the example above shows the derivative may still exist there. I am not sure what the proper assumptions should be. Tentatively, take $f(x)^{g(x)}\ge 0$ for all $x$ in the relevant domain, so that (I believe) we have $$\ln \left(f(x)^{g(x)}\right)=g(x)\ln |f(x)|$$ when $f$ is strictly positive and undefined otherwise. Also, is there any non-trivial example of a well-defined function (meaning ""nice"", as in differentiable almost everywhere) $f^g$ where $f$ takes on negative values and where $g$ is not constant? In other words, for cases of $f<0$ are the only functions worth considering of the form $f^c$ for constants $c$? EDIT: I want the function(s) to be real, although arguments using complex numbers are of course permissible.",,['calculus']
57,Partial derivative of a composite function $\mathbb{R}^n \to \mathbb{R}^n$,Partial derivative of a composite function,\mathbb{R}^n \to \mathbb{R}^n,"I am trying to understand a proof but I am stuck on this technical bit: Apart from the small typo highlighted, I don't really see how to get the big formula for the partial derivative of $v_i$ What I keep getting is the following: $$\partial_i v_i(x)=\displaystyle \sum_{j,k=1}^{n}  d_{i,j}(x) \partial_k (w_j(f(x))\partial_i( f_k) +\sum_{j=1}^n w_j(f(x)\partial_i d_{i,j} $$ where $\partial_k (w_j(f(x))=\displaystyle \frac{\partial w_j(f(x))}{\partial f_k}$ Later in the book an expression very similar to the one I get appears...are they the same thing? Is there a huge typo? EDIT : I am thinking it could be that $W\not = w$ for some reason, is it a standard notation for something I don't know? I don't really see why they would have changed to capital letters otherwise... Thank you very much!!","I am trying to understand a proof but I am stuck on this technical bit: Apart from the small typo highlighted, I don't really see how to get the big formula for the partial derivative of $v_i$ What I keep getting is the following: $$\partial_i v_i(x)=\displaystyle \sum_{j,k=1}^{n}  d_{i,j}(x) \partial_k (w_j(f(x))\partial_i( f_k) +\sum_{j=1}^n w_j(f(x)\partial_i d_{i,j} $$ where $\partial_k (w_j(f(x))=\displaystyle \frac{\partial w_j(f(x))}{\partial f_k}$ Later in the book an expression very similar to the one I get appears...are they the same thing? Is there a huge typo? EDIT : I am thinking it could be that $W\not = w$ for some reason, is it a standard notation for something I don't know? I don't really see why they would have changed to capital letters otherwise... Thank you very much!!",,"['derivatives', 'differential-topology']"
58,Derivatives problem - what am I doing wrong?,Derivatives problem - what am I doing wrong?,,"The problem: A shuttle lifts off vertically from a point $4$ miles from the command post.  For the first $20$ seconds of flight, its angle of elevation changes at a constant rate of $3$ degrees per second.  What is the velocity of the shuttle when the angle of elevation is $30$ degrees? I made a right triangle picture and realized that $\tan \theta = \frac x4$.  This means that $ x=4\tan \theta$ which is the same as $ x=4\tan(3t)$ because $\theta=3t$ if the angle is changing at $3$ degrees per second. The velocity will be $\frac {dx}{dt}$ so differentiating the equation gives $\frac {dx}{dt}=12\sec^2 (3t)$. Substituting back gives me $\frac {dx}{dt}=12\sec^2\theta$. Finally, the problem asks for the velocity when the angle of elevation is $30$ degrees, which is $\frac \pi 6$, so evaluating my derivative gives $12\sec^2 \frac \pi 6$ which is $16$. The text says the answer is supposed to be $\frac {4\pi} {45}$, and I can't see my error.  Please help.","The problem: A shuttle lifts off vertically from a point $4$ miles from the command post.  For the first $20$ seconds of flight, its angle of elevation changes at a constant rate of $3$ degrees per second.  What is the velocity of the shuttle when the angle of elevation is $30$ degrees? I made a right triangle picture and realized that $\tan \theta = \frac x4$.  This means that $ x=4\tan \theta$ which is the same as $ x=4\tan(3t)$ because $\theta=3t$ if the angle is changing at $3$ degrees per second. The velocity will be $\frac {dx}{dt}$ so differentiating the equation gives $\frac {dx}{dt}=12\sec^2 (3t)$. Substituting back gives me $\frac {dx}{dt}=12\sec^2\theta$. Finally, the problem asks for the velocity when the angle of elevation is $30$ degrees, which is $\frac \pi 6$, so evaluating my derivative gives $12\sec^2 \frac \pi 6$ which is $16$. The text says the answer is supposed to be $\frac {4\pi} {45}$, and I can't see my error.  Please help.",,"['calculus', 'derivatives']"
59,Find an upper bound for $f(x) = \sin(\sin(x))$.,Find an upper bound for .,f(x) = \sin(\sin(x)),"I've run into this hard calculus problem that I can't seem to solve.  The question is: If $f(x) = \sin(\sin x)$, use a graph to find an upper bound for $|f^{(iv)}(x)|$. I am not sure what I have to do.  Should I find the fourth derivative and then find the maximum of that graph.  Or do I have to do something completely different? Thanks in advance.","I've run into this hard calculus problem that I can't seem to solve.  The question is: If $f(x) = \sin(\sin x)$, use a graph to find an upper bound for $|f^{(iv)}(x)|$. I am not sure what I have to do.  Should I find the fourth derivative and then find the maximum of that graph.  Or do I have to do something completely different? Thanks in advance.",,"['calculus', 'derivatives']"
60,$99$th derivative of $\sin x$,th derivative of,99 \sin x,Can someone help me calculate the $99$th derivative of $\sin(x)$? Calculate $f^{(99)}(x) $ for the function $f(x) = \sin(x) $,Can someone help me calculate the $99$th derivative of $\sin(x)$? Calculate $f^{(99)}(x) $ for the function $f(x) = \sin(x) $,,"['calculus', 'trigonometry', 'derivatives']"
61,Derivative of a constant not making sense,Derivative of a constant not making sense,,"I'm studying calculus on my own and I came across something weird: The derivative of $x^n$ is $n\cdot x^{n-1}$, and the derivative of any constant is $0$. Also, any constant $x = x^1$. However, the derivative of $x^1$ ($x$ being any integer constant) seems to be $1$: $n\cdot x^{n-1} = 1\cdot x^{1-1} = x^0 = 1$. What's failing in my reasoning?","I'm studying calculus on my own and I came across something weird: The derivative of $x^n$ is $n\cdot x^{n-1}$, and the derivative of any constant is $0$. Also, any constant $x = x^1$. However, the derivative of $x^1$ ($x$ being any integer constant) seems to be $1$: $n\cdot x^{n-1} = 1\cdot x^{1-1} = x^0 = 1$. What's failing in my reasoning?",,"['calculus', 'derivatives']"
62,Why is the quotient rule in differentiation necessary?,Why is the quotient rule in differentiation necessary?,,Calculus - Derivatives - Quotient Rule Why is a quotient rule even necessary? Why can't we just consider $\frac{A}{B}$ as $A \cdot B^{-1}$ and use the multiplication formula?,Calculus - Derivatives - Quotient Rule Why is a quotient rule even necessary? Why can't we just consider as and use the multiplication formula?,\frac{A}{B} A \cdot B^{-1},"['calculus', 'derivatives']"
63,"Differentiation of $x^{\sqrt{x}}$, how?","Differentiation of , how?",x^{\sqrt{x}},"The answer is (I think) $x^{\sqrt{x}-0.5} (1+0.5\ln(x))$, but how?","The answer is (I think) $x^{\sqrt{x}-0.5} (1+0.5\ln(x))$, but how?",,"['calculus', 'derivatives', 'faq']"
64,Calculus Derivative—Finding unknown constants [closed],Calculus Derivative—Finding unknown constants [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Find all values of $k$ and $l$ such that   $$\lim_{x \to 0} \frac{k+\cos(lx)}{x^2}=-4.$$ Any help on how to do this would be greatly appreciated.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Find all values of $k$ and $l$ such that   $$\lim_{x \to 0} \frac{k+\cos(lx)}{x^2}=-4.$$ Any help on how to do this would be greatly appreciated.",,"['calculus', 'derivatives', 'constants']"
65,Is $\sin^2(x)$ the same as $\sin x^2$?,Is  the same as ?,\sin^2(x) \sin x^2,I'm working with derivatives and need to know if $\sin^2(x)$ the same as $\sin(x^2)$? I almost don't want to ask because my last question was closed. It was a valid question and so is this one. I've been trying to find the answer on my own but the answers I've discovered are conflicting. Any clarification here is appreciated.,I'm working with derivatives and need to know if $\sin^2(x)$ the same as $\sin(x^2)$? I almost don't want to ask because my last question was closed. It was a valid question and so is this one. I've been trying to find the answer on my own but the answers I've discovered are conflicting. Any clarification here is appreciated.,,['derivatives']
66,How do I differentiate a product $a(x) b(x) c(x)$ of three functions?,How do I differentiate a product  of three functions?,a(x) b(x) c(x),"If I have a function $$f(x)=a(x)\cdot b(x)\cdot c(x),$$ where I cannot simplify any further, how do I differentiate it? For example, how would I differentiate $$f(x)=(x^2+5x+6)\cdot(e^{2x})\cdot(\cos(\pi+x))\ ?$$","If I have a function $$f(x)=a(x)\cdot b(x)\cdot c(x),$$ where I cannot simplify any further, how do I differentiate it? For example, how would I differentiate $$f(x)=(x^2+5x+6)\cdot(e^{2x})\cdot(\cos(\pi+x))\ ?$$",,"['calculus', 'derivatives', 'products']"
67,quick question on an example of the derivative as a linear map.,quick question on an example of the derivative as a linear map.,,"After reading many answers on the subject I feel like I am close to finally understanding why the derivative is a linear map. I think that if someone helps me understand the following example I might ""get it"" So I have $f(x)= e^x$ then $f(x)' = e^x$ is not linear, but instead the function defined by $y \rightarrow e^x y$ is linear, but what is $y$? is it introduced for the sole purpose to have a linear map? Say that I fix x, then for different values of $y$ what am I computing? So I should not call $f(x)'$ the derivative any more if the derivative is a linear map, then what should I call $f(x)'$?","After reading many answers on the subject I feel like I am close to finally understanding why the derivative is a linear map. I think that if someone helps me understand the following example I might ""get it"" So I have $f(x)= e^x$ then $f(x)' = e^x$ is not linear, but instead the function defined by $y \rightarrow e^x y$ is linear, but what is $y$? is it introduced for the sole purpose to have a linear map? Say that I fix x, then for different values of $y$ what am I computing? So I should not call $f(x)'$ the derivative any more if the derivative is a linear map, then what should I call $f(x)'$?",,"['real-analysis', 'derivatives']"
68,Differentiate $\log_{10}x$,Differentiate,\log_{10}x,"My attempt: $\eqalign{   & \log_{10}x = {{\ln x} \over {\ln 10}}  \cr    & u = \ln x  \cr    & v = \ln 10  \cr    & {{du} \over {dx}} = {1 \over x}  \cr    & {{dv} \over {dx}} = 0  \cr    & {v^2} = {(\ln10)^2}  \cr    & {{dy} \over {dx}} = {{\left( {{{\ln 10} \over x}} \right)} \over {2\ln 10}} = {{\ln10} \over x} \times {1 \over {2\ln 10}} = {1 \over {2x}} \cr} $ The right answer is: ${{dy} \over {dx}} = {1 \over {x\ln 10}}$ , where did I go wrong? Thanks!","My attempt: $\eqalign{   & \log_{10}x = {{\ln x} \over {\ln 10}}  \cr    & u = \ln x  \cr    & v = \ln 10  \cr    & {{du} \over {dx}} = {1 \over x}  \cr    & {{dv} \over {dx}} = 0  \cr    & {v^2} = {(\ln10)^2}  \cr    & {{dy} \over {dx}} = {{\left( {{{\ln 10} \over x}} \right)} \over {2\ln 10}} = {{\ln10} \over x} \times {1 \over {2\ln 10}} = {1 \over {2x}} \cr} $ The right answer is: ${{dy} \over {dx}} = {1 \over {x\ln 10}}$ , where did I go wrong? Thanks!",,"['calculus', 'derivatives', 'logarithms']"
69,Is this ambiguity in the use of prime notation to denote a derivative?,Is this ambiguity in the use of prime notation to denote a derivative?,,"Consider the function $f(x)$ and let $g(x)=f(cx)$ . By the definition of derivative $$f'(x)=\frac{df(x)}{dx}=\lim\limits_{h \to 0} \frac{f(x+h)-f(x)}{h}\tag{1}$$ and so the definition of $f'(cx)$ is $$f'(cx)=\frac{df(cx)}{d(cx)}=\lim\limits_{h \to 0} \frac{f(cx+h)-f(cx)}{h}\tag{2}$$ Also $$g'(x) = \lim\limits_{h \to 0} \frac{g(x+h)-g(x)}{h}$$ $$=\lim\limits_{h \to 0} \frac{f(c(x+h))-f(cx)}{h}$$ $$=\lim\limits_{h \to 0} \frac{f(cx+ch)-f(cx)}{h}$$ $$=c\lim\limits_{h \to 0} \frac{f(cx+ch)-f(cx)}{ch}$$ $$=cf'(cx)$$ Hence $$g'(x)=cf'(cx)$$ I am not sure if I am seeing an inexistent ambiguity, but $f'(cx)$ seems like ambiguous notation. As defined above in $(2)$ , $f'(cx)$ means the derivative of $f$ relative to $cx$ , evaluated at a point we call $cx$ , . Note that this is different than the derivative of $f$ relative to $x$ evaluated at a point $cx$ : this derivative is $g'(x)=\frac{df(cx)}{dx}$ . In ""prime"" notation, how do we denote this latter derivative? It would seem to be $f'(cx)$ , but I think either this is incorrect, or the definition given in $(2)$ is somehow non-standard. For example, let $$f(x)=3x^3$$ $$g(x)=f(cx)=3c^3x^3$$ Then $$f'(x)=\frac{df(x)}{dx}=9x^2$$ $$\left.\frac{df(x)}{dx}\right \vert_{x=cx}=9c^2x^2\ \ (=f'(cx)???)$$ $$g'(x)=\frac{df(cx)}{dx}=c\frac{df(cx)}{d(cx)}=c\cdot f'(cx)=c\cdot 9c^2x^2= 9c^3x^2\tag{3}$$ In this example, $f'(cx)=\frac{df(cx)}{d(cx)}=9c^2x^2$ according to definition I gave in $(2)$ . But perhaps more intuitively, it could also be $f'(cx)= \left.\frac{df(x)}{dx}\right \vert_{x=cx}=9c^2x^2$ Note that both uses of $f'(cx)$ lead to the same result in this example. Which use of $f'(cx)$ is the ""correct"" or ""standard"" one?","Consider the function and let . By the definition of derivative and so the definition of is Also Hence I am not sure if I am seeing an inexistent ambiguity, but seems like ambiguous notation. As defined above in , means the derivative of relative to , evaluated at a point we call , . Note that this is different than the derivative of relative to evaluated at a point : this derivative is . In ""prime"" notation, how do we denote this latter derivative? It would seem to be , but I think either this is incorrect, or the definition given in is somehow non-standard. For example, let Then In this example, according to definition I gave in . But perhaps more intuitively, it could also be Note that both uses of lead to the same result in this example. Which use of is the ""correct"" or ""standard"" one?",f(x) g(x)=f(cx) f'(x)=\frac{df(x)}{dx}=\lim\limits_{h \to 0} \frac{f(x+h)-f(x)}{h}\tag{1} f'(cx) f'(cx)=\frac{df(cx)}{d(cx)}=\lim\limits_{h \to 0} \frac{f(cx+h)-f(cx)}{h}\tag{2} g'(x) = \lim\limits_{h \to 0} \frac{g(x+h)-g(x)}{h} =\lim\limits_{h \to 0} \frac{f(c(x+h))-f(cx)}{h} =\lim\limits_{h \to 0} \frac{f(cx+ch)-f(cx)}{h} =c\lim\limits_{h \to 0} \frac{f(cx+ch)-f(cx)}{ch} =cf'(cx) g'(x)=cf'(cx) f'(cx) (2) f'(cx) f cx cx f x cx g'(x)=\frac{df(cx)}{dx} f'(cx) (2) f(x)=3x^3 g(x)=f(cx)=3c^3x^3 f'(x)=\frac{df(x)}{dx}=9x^2 \left.\frac{df(x)}{dx}\right \vert_{x=cx}=9c^2x^2\ \ (=f'(cx)???) g'(x)=\frac{df(cx)}{dx}=c\frac{df(cx)}{d(cx)}=c\cdot f'(cx)=c\cdot 9c^2x^2= 9c^3x^2\tag{3} f'(cx)=\frac{df(cx)}{d(cx)}=9c^2x^2 (2) f'(cx)= \left.\frac{df(x)}{dx}\right \vert_{x=cx}=9c^2x^2 f'(cx) f'(cx),"['calculus', 'derivatives', 'notation']"
70,Why $f(x) = x^2$ has variable derivative but its tangent has constant slope?,Why  has variable derivative but its tangent has constant slope?,f(x) = x^2,"I'm taking Brilliant.org's calculus course, and I'm on the section called The Derivative . My (mis)understanding: A tangent line is a linear function that grazes a point, $a$ , on the graph of a different function. The slope of that tangent line is the instantaneous rate of change at $a$ . The way you find this is by taking the difference ratio, giving you the slope, and plugging in equal values for the input points. So, for the function $f(x) = x^2$ , the slope would be: $$\require{cancel} \frac{b^2 - a^2}{b-a} = \frac{\cancel{(b-a)}(b+a)}{\cancel{b-a}} = b+a$$ Now, considering the case as the difference between $b$ and $a$ gets smaller, or equivalently, as the secant line's two points become one and the same point, thus turning it to a tangent line: $$\lim_{b \rightarrow a} b +a = 2b = 2a$$ Thus, the derivative of the square function is found, $f'(x) = 2x$ . The slope of the tangent is $2x$ , but apparently, the derivative isn't the slope of the tangent, it is only so the other way around (see this Math.SE answer ). I could define any number of functions of the form $g(x) = ax$ , which would be tangent to or intersect the function $f(x)=x^2$ , when $x=a$ . For any of these functions, $g'(x) = a$ , as per the difference ratio. The slopes of these functions would be any arbitrary number $a$ , not $2x$ . So, it seems we're dealing with the slope of one particular tangent. However, I can't see how this tangent line is linear. If it is linear, its slope is a constant, but $2x$ contains a variable. All of this leads me to think that we're talking about the tangent line in a very different sense than the one of ""a linear line that grazes the function's graph"". But I've only been made familiar with the latter sense, and this mysterious sense is very ill-defined in my head. What is it and what does it have to do with tangent lines? The trick with morphing a secant line into a tangent line doesn't really explain this, because one can do that anywhere on the graph and wind up with tangent lines that have differing slopes, none of which are equal to $2x$ .","I'm taking Brilliant.org's calculus course, and I'm on the section called The Derivative . My (mis)understanding: A tangent line is a linear function that grazes a point, , on the graph of a different function. The slope of that tangent line is the instantaneous rate of change at . The way you find this is by taking the difference ratio, giving you the slope, and plugging in equal values for the input points. So, for the function , the slope would be: Now, considering the case as the difference between and gets smaller, or equivalently, as the secant line's two points become one and the same point, thus turning it to a tangent line: Thus, the derivative of the square function is found, . The slope of the tangent is , but apparently, the derivative isn't the slope of the tangent, it is only so the other way around (see this Math.SE answer ). I could define any number of functions of the form , which would be tangent to or intersect the function , when . For any of these functions, , as per the difference ratio. The slopes of these functions would be any arbitrary number , not . So, it seems we're dealing with the slope of one particular tangent. However, I can't see how this tangent line is linear. If it is linear, its slope is a constant, but contains a variable. All of this leads me to think that we're talking about the tangent line in a very different sense than the one of ""a linear line that grazes the function's graph"". But I've only been made familiar with the latter sense, and this mysterious sense is very ill-defined in my head. What is it and what does it have to do with tangent lines? The trick with morphing a secant line into a tangent line doesn't really explain this, because one can do that anywhere on the graph and wind up with tangent lines that have differing slopes, none of which are equal to .",a a f(x) = x^2 \require{cancel} \frac{b^2 - a^2}{b-a} = \frac{\cancel{(b-a)}(b+a)}{\cancel{b-a}} = b+a b a \lim_{b \rightarrow a} b +a = 2b = 2a f'(x) = 2x 2x g(x) = ax f(x)=x^2 x=a g'(x) = a a 2x 2x 2x,"['calculus', 'derivatives', 'terminology', 'tangent-line', 'slope']"
71,Why is derivative of $x$ with respect to $x$ equal to $1$?,Why is derivative of  with respect to  equal to ?,x x 1,"I just started learning the derivatives of inverse function. The first example is based on the fact that $\frac{\mathsf{d}x}{\mathsf{d}x} = 1$ and it is stated that I should know this already. However I don't recall this fact, moreover I don't know how you can solve $\frac{\mathsf{d}x}{\mathsf{d}x}$. Could somebody explain me how do we solve such question and is this rule apply to every function?","I just started learning the derivatives of inverse function. The first example is based on the fact that $\frac{\mathsf{d}x}{\mathsf{d}x} = 1$ and it is stated that I should know this already. However I don't recall this fact, moreover I don't know how you can solve $\frac{\mathsf{d}x}{\mathsf{d}x}$. Could somebody explain me how do we solve such question and is this rule apply to every function?",,"['derivatives', 'inverse-function']"
72,Infinitesimally small time intervals,Infinitesimally small time intervals,,"When saying that in a small time interval $dt$ , the velocity has changed by $d\vec v$ , and so the acceleration $\vec a$ is $d\vec v/dt$ , are we not assuming that $\vec a$ is constant in that small interval $dt$ , otherwise considering a change in acceleration $d\vec a$ , the expression should have been $\vec a = \frac{d\vec v}{dt} - \frac{d\vec a}{2}$ (Again assuming rate of change of acceleration is constant). According to that argument, I can say that $\vec v$ is also constant in that time interval and so $\vec a = \vec 0$ . Can someone point out where exactly I have gone wrong. Also this was just an example, my question is general.","When saying that in a small time interval , the velocity has changed by , and so the acceleration is , are we not assuming that is constant in that small interval , otherwise considering a change in acceleration , the expression should have been (Again assuming rate of change of acceleration is constant). According to that argument, I can say that is also constant in that time interval and so . Can someone point out where exactly I have gone wrong. Also this was just an example, my question is general.",dt d\vec v \vec a d\vec v/dt \vec a dt d\vec a \vec a = \frac{d\vec v}{dt} - \frac{d\vec a}{2} \vec v \vec a = \vec 0,"['kinematics', 'derivatives', 'calculus']"
73,$f(x)=\frac{e^x-1}{x}$ and $f(0)=1$. Finding $f''(0)$ rapidly.,and . Finding  rapidly.,f(x)=\frac{e^x-1}{x} f(0)=1 f''(0),"Let be $f(x)=\frac{e^x-1}{x}$ and $f(0)=1$ . I have to find $f''(0)$ . I tried to solve it with finding the second derivative of the function and after that using the L'Hospital rule more times. But this takes a lot of time, more than I have therefor on my exam. Is there a way to find $f''(0)$ faster?","Let be and . I have to find . I tried to solve it with finding the second derivative of the function and after that using the L'Hospital rule more times. But this takes a lot of time, more than I have therefor on my exam. Is there a way to find faster?",f(x)=\frac{e^x-1}{x} f(0)=1 f''(0) f''(0),"['calculus', 'derivatives']"
74,All partial derivatives are 0.,All partial derivatives are 0.,,"I know that for a function $f$ all of its partial derivatives are $0$. Thus, $\frac{\partial f_i}{\partial x_j} = 0$ for any $i = 1, \dots, m$ and any $j = 1, \dots, n$. Is there any easy way to prove that $f$ is constant? The results seems obvious but I'm having a hard time expressing it in words explicitly why it's true.","I know that for a function $f$ all of its partial derivatives are $0$. Thus, $\frac{\partial f_i}{\partial x_j} = 0$ for any $i = 1, \dots, m$ and any $j = 1, \dots, n$. Is there any easy way to prove that $f$ is constant? The results seems obvious but I'm having a hard time expressing it in words explicitly why it's true.",,"['calculus', 'real-analysis', 'derivatives']"
75,Find the derivative using the chain rule and the quotient rule,Find the derivative using the chain rule and the quotient rule,,$$f(x) = \left(\frac{x}{x+1}\right)^4$$ Find $f'(x)$. Here is my work: $$f'(x) = \frac{4x^3\left(x+1\right)^4-4\left(x+1\right)^3x^4}{\left(x+1\right)^8}$$ $$f'(x) = \frac{4x^3\left(x+1\right)^4-4x^4\left(x+1\right)^3}{\left(x+1\right)^8}$$ I know the final simplified answer to be: $${4x^3\over (x+1)^5}$$ How do I get to the final answer from my last step? Or have I done something wrong in my own work?,$$f(x) = \left(\frac{x}{x+1}\right)^4$$ Find $f'(x)$. Here is my work: $$f'(x) = \frac{4x^3\left(x+1\right)^4-4\left(x+1\right)^3x^4}{\left(x+1\right)^8}$$ $$f'(x) = \frac{4x^3\left(x+1\right)^4-4x^4\left(x+1\right)^3}{\left(x+1\right)^8}$$ I know the final simplified answer to be: $${4x^3\over (x+1)^5}$$ How do I get to the final answer from my last step? Or have I done something wrong in my own work?,,"['calculus', 'derivatives', 'solution-verification']"
76,Simple differentiation from first principles problem,Simple differentiation from first principles problem,,"I know this is really basic, but how do I differentiate this equation from first principles to find $\frac{dy}{dx}$: $$ y = \frac{1}{x} $$ I tried this: $$\begin{align} f'(x) = \frac{dy}{dx} & = \lim_{\delta x\to 0} \left[ \frac{f(x + \delta x) - f(x)}{\delta x} \right] \\ & = \lim_{\delta x\to 0} \left[ \frac{(x+\delta x)^{-1} - x^{-1}}{\delta x} \right] \\ & = \lim_{\delta x\to 0} \left[ \frac{1}{\delta x(x + \delta x )} - \frac{1}{x(\delta x)} \right] \\ & = \lim_{\delta x\to 0} \left[ \frac{1}{x(\delta x)} + \frac{1}{(\delta x)^2} - \frac{1}{x(\delta x)} \right] \\ & = 1 \end{align}$$ which is obviously wrong, since $f'(x) = - \frac{1}{x^2}$. Where am I going wrong?","I know this is really basic, but how do I differentiate this equation from first principles to find $\frac{dy}{dx}$: $$ y = \frac{1}{x} $$ I tried this: $$\begin{align} f'(x) = \frac{dy}{dx} & = \lim_{\delta x\to 0} \left[ \frac{f(x + \delta x) - f(x)}{\delta x} \right] \\ & = \lim_{\delta x\to 0} \left[ \frac{(x+\delta x)^{-1} - x^{-1}}{\delta x} \right] \\ & = \lim_{\delta x\to 0} \left[ \frac{1}{\delta x(x + \delta x )} - \frac{1}{x(\delta x)} \right] \\ & = \lim_{\delta x\to 0} \left[ \frac{1}{x(\delta x)} + \frac{1}{(\delta x)^2} - \frac{1}{x(\delta x)} \right] \\ & = 1 \end{align}$$ which is obviously wrong, since $f'(x) = - \frac{1}{x^2}$. Where am I going wrong?",,"['calculus', 'derivatives']"
77,Derivative of $\frac{d}{dx}\ln (x+\sqrt{x^2+1})$,Derivative of,\frac{d}{dx}\ln (x+\sqrt{x^2+1}),Show that $\frac{d}{dx}\ln (x+\sqrt{x^2+1}) = \frac{1}{\sqrt{x^2+1}}$ So I've done this so far: $$\frac{d}{dx}\ln (x+\sqrt{x^2+1}) = \frac{1+x(x^2+1)^{-0.5}}{x+(x^2+1)^{0.5}}$$ I have tried a few combinations but cannot attain the desired result. Could someone offer just a hint to get me going? Thanks,Show that $\frac{d}{dx}\ln (x+\sqrt{x^2+1}) = \frac{1}{\sqrt{x^2+1}}$ So I've done this so far: $$\frac{d}{dx}\ln (x+\sqrt{x^2+1}) = \frac{1+x(x^2+1)^{-0.5}}{x+(x^2+1)^{0.5}}$$ I have tried a few combinations but cannot attain the desired result. Could someone offer just a hint to get me going? Thanks,,"['calculus', 'derivatives']"
78,The derivative of $ \ln\left(\frac{x+2}{x^3-1}\right)$,The derivative of, \ln\left(\frac{x+2}{x^3-1}\right),"I know it is a simple question, but what would be the next few steps in this equation to find the derivative? $$f(x)= \ln\left(\frac{x+2}{x^3-1}\right)$$","I know it is a simple question, but what would be the next few steps in this equation to find the derivative? $$f(x)= \ln\left(\frac{x+2}{x^3-1}\right)$$",,"['calculus', 'derivatives', 'logarithms']"
79,Why is $\frac{d}{dx}\ln |x|=\frac 1x$,Why is,\frac{d}{dx}\ln |x|=\frac 1x,"Obviously, $x$ is always positive for the $\ln$ function. But the modulus allows $x$ to hold negative values as well. But why isn’t the derivative of the function $=\frac{1}{|x|}$ ? I understand the orignal function isn’t entirely differentiable, but I don’t see why it is possible to completely ignore the modulus.","Obviously, is always positive for the function. But the modulus allows to hold negative values as well. But why isn’t the derivative of the function ? I understand the orignal function isn’t entirely differentiable, but I don’t see why it is possible to completely ignore the modulus.",x \ln x =\frac{1}{|x|},"['calculus', 'derivatives', 'logarithms']"
80,how do you differentiate $\ln(x)$ using the difference quotient. [duplicate],how do you differentiate  using the difference quotient. [duplicate],\ln(x),"This question already has answers here : Proof of the derivative of $\ln(x)$ (5 answers) Closed 4 years ago . So the limit is as $h$ approaches $0$ of $\displaystyle \frac{\ln(x+h)-\ln(x)}{h}$ , which simplifies to $\displaystyle\frac{\ln\left(\frac{x+h}{x}\right)}{h}$ , which simplifies to $\displaystyle\frac{\ln\left(1+ \frac hx\right)}{h}$ . I got stuck here.  , how should I continue?","This question already has answers here : Proof of the derivative of $\ln(x)$ (5 answers) Closed 4 years ago . So the limit is as approaches of , which simplifies to , which simplifies to . I got stuck here.  , how should I continue?",h 0 \displaystyle \frac{\ln(x+h)-\ln(x)}{h} \displaystyle\frac{\ln\left(\frac{x+h}{x}\right)}{h} \displaystyle\frac{\ln\left(1+ \frac hx\right)}{h},['derivatives']
81,How to calculate the derivative of $x^x$?,How to calculate the derivative of ?,x^x,"I'm trying to follow an example in my textbook. $$y=x^x$$ $$\ln (y)=\ln (x) \cdot x$$ We want to calculate the derivative with respect to x The book makes quite a leap here and states that: $$\frac{y'}{y}=\frac{1}{x}\cdot x+1\cdot \ln(x)$$ Since $y=x^x$ this means that: $$y'=x^x(1+\ln(x))$$ Is this correct? If I start from the beginning then: $$y=x^x$$ $$\ln (y)=\ln (x) \cdot x$$ Only if we want to take the derivative this expression isn't useful, we'll have to use the full expression: $$e^{\ln (y)}=e^{\ln (x) \cdot x}$$ Now, if we take the derivative of this with respect to x we find that: $$\frac {1}{y}\cdot e^{\ln (y)}=e^{\ln (x) \cdot x}\cdot (1+ln(x))$$ The left hand expression would simplify to $\frac {y'}{y}$, since $e^{\ln(y)}=y$ and the derivative of $y=y'$ so: $$\frac {y'}{y}=e^{\ln (x) \cdot x}\cdot (1+\ln(x))$$ Which can be written as: $$\frac {y'}{x^x}=x^x\cdot (1+\ln(x))$$ Which simplifies to: $$y'=x^{2x}\cdot (1+\ln(x))$$","I'm trying to follow an example in my textbook. $$y=x^x$$ $$\ln (y)=\ln (x) \cdot x$$ We want to calculate the derivative with respect to x The book makes quite a leap here and states that: $$\frac{y'}{y}=\frac{1}{x}\cdot x+1\cdot \ln(x)$$ Since $y=x^x$ this means that: $$y'=x^x(1+\ln(x))$$ Is this correct? If I start from the beginning then: $$y=x^x$$ $$\ln (y)=\ln (x) \cdot x$$ Only if we want to take the derivative this expression isn't useful, we'll have to use the full expression: $$e^{\ln (y)}=e^{\ln (x) \cdot x}$$ Now, if we take the derivative of this with respect to x we find that: $$\frac {1}{y}\cdot e^{\ln (y)}=e^{\ln (x) \cdot x}\cdot (1+ln(x))$$ The left hand expression would simplify to $\frac {y'}{y}$, since $e^{\ln(y)}=y$ and the derivative of $y=y'$ so: $$\frac {y'}{y}=e^{\ln (x) \cdot x}\cdot (1+\ln(x))$$ Which can be written as: $$\frac {y'}{x^x}=x^x\cdot (1+\ln(x))$$ Which simplifies to: $$y'=x^{2x}\cdot (1+\ln(x))$$",,"['calculus', 'derivatives', 'exponentiation']"
82,Prove that inequality is true for $x>0$: $(e^x-1)\ln(1+x) > x^2$,Prove that inequality is true for :,x>0 (e^x-1)\ln(1+x) > x^2,"I was given a task to prove that inequality is true for x>0: $(e^x-1)\ln(1+x) > x^2$. I've tried to use derivatives to show that the $f(x) = (e^x-1)\ln(1+x)-x^2$ is greater than zero, but has never succeeded. Any help will be appreciated.","I was given a task to prove that inequality is true for x>0: $(e^x-1)\ln(1+x) > x^2$. I've tried to use derivatives to show that the $f(x) = (e^x-1)\ln(1+x)-x^2$ is greater than zero, but has never succeeded. Any help will be appreciated.",,"['real-analysis', 'derivatives', 'inequality', 'logarithms', 'exponential-function']"
83,Prove that $ \lim_{x \to \infty} f'(x) = 0$ [duplicate],Prove that  [duplicate], \lim_{x \to \infty} f'(x) = 0,"This question already has answers here : Proving that $\lim\limits_{x\to\infty}f'(x) = 0$ when $\lim\limits_{x\to\infty}f(x)$ and $\lim\limits_{x\to\infty}f'(x)$ exist (6 answers) Closed 6 years ago . Prove that if $\displaystyle \lim_{x \to \infty} f(x)$ and $\displaystyle \lim_{x \to \infty} f'(x)$ are both real numbers, then $\displaystyle \lim_{x \to \infty} f'(x) = 0$. Attempt Intuitively this makes sense to me. Take $y = x$. This slope is constant but it increases arbitrarily, and it seems that we can't make both the slope and value of $f(x)$ to be real numbers without ""flattening"" out the graph. I tried first saying $\displaystyle \lim_{x \to \infty} f(x) = a$ and $\displaystyle \lim_{x \to \infty} f'(x) = b$. Then we might be able to do something with the L'Hospital's rule.","This question already has answers here : Proving that $\lim\limits_{x\to\infty}f'(x) = 0$ when $\lim\limits_{x\to\infty}f(x)$ and $\lim\limits_{x\to\infty}f'(x)$ exist (6 answers) Closed 6 years ago . Prove that if $\displaystyle \lim_{x \to \infty} f(x)$ and $\displaystyle \lim_{x \to \infty} f'(x)$ are both real numbers, then $\displaystyle \lim_{x \to \infty} f'(x) = 0$. Attempt Intuitively this makes sense to me. Take $y = x$. This slope is constant but it increases arbitrarily, and it seems that we can't make both the slope and value of $f(x)$ to be real numbers without ""flattening"" out the graph. I tried first saying $\displaystyle \lim_{x \to \infty} f(x) = a$ and $\displaystyle \lim_{x \to \infty} f'(x) = b$. Then we might be able to do something with the L'Hospital's rule.",,"['real-analysis', 'derivatives']"
84,Differentiating with respect to $1 - x$,Differentiating with respect to,1 - x,"I am fairly sure this is a silly question, but a Google search was insufficient to find a satisfactory answer. If I differentiate some function of $x$ with respect to $1-x$, what do I get compared to differentiating with respect to $x$? I know I need to use the chain rule to figure this out, but I am stuck on the details.","I am fairly sure this is a silly question, but a Google search was insufficient to find a satisfactory answer. If I differentiate some function of $x$ with respect to $1-x$, what do I get compared to differentiating with respect to $x$? I know I need to use the chain rule to figure this out, but I am stuck on the details.",,"['calculus', 'derivatives', 'chain-rule']"
85,How can I solve this: $-e^{-5x}x^7(5x-8)=0$,How can I solve this:,-e^{-5x}x^7(5x-8)=0,"I'm examining function slope and determine relative extrema of function (local minimum and local maximum) The example is as following: $ y = x^8 e^{-5x} $ To do this I have to determine derivative of this function, which is: $$ y' = 8x^7e^{-5x}+x^8(-5e^{-5x}) = -e^{-5x}x^7(5x-8) $$ Then according to my notes I need to solve equation from derivative: $ -e^{-5x}x^7(5x-8)=0 $ How to solve this equation?","I'm examining function slope and determine relative extrema of function (local minimum and local maximum) The example is as following: $ y = x^8 e^{-5x} $ To do this I have to determine derivative of this function, which is: $$ y' = 8x^7e^{-5x}+x^8(-5e^{-5x}) = -e^{-5x}x^7(5x-8) $$ Then according to my notes I need to solve equation from derivative: $ -e^{-5x}x^7(5x-8)=0 $ How to solve this equation?",,"['calculus', 'derivatives']"
86,Why does the derivative rule $a^x = \ln(a)\cdot a^x$ fail for $e^{-x}$?,Why does the derivative rule  fail for ?,a^x = \ln(a)\cdot a^x e^{-x},In my textbook there is a derivative rule stated as folows: $$f(x)=a^x \implies f'(x)=\ln(a) \cdot  \ a^x$$ But when I try to apply this rule to $e^{-x}$ I get: $$\ln(e) \cdot \  e^{-x} = e^{-x}$$ Which is not correct because the derivative of $e^{-x}$ is $-e^{-x}$. What's going wrong here?,In my textbook there is a derivative rule stated as folows: $$f(x)=a^x \implies f'(x)=\ln(a) \cdot  \ a^x$$ But when I try to apply this rule to $e^{-x}$ I get: $$\ln(e) \cdot \  e^{-x} = e^{-x}$$ Which is not correct because the derivative of $e^{-x}$ is $-e^{-x}$. What's going wrong here?,,"['calculus', 'derivatives']"
87,Prove if $f$ is increasing then $f'(x) \ge 0$,Prove if  is increasing then,f f'(x) \ge 0,"Let $f:[a, b] \rightarrow \mathbb{R}$ where $a, b \in \mathbb{R}$ with $a <b$. Suppose $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$. Suppose $f$ is monotonically increasing on $[a,b]$. Show that $f'(x) \ge 0$ for all $x \in (a,b)$. My attempt: I tried using the Mean Value Theorem, but it doesn't quite seem to work. For example, by the MVT we can conclude that there exists a $c \in (a,b)$ such that $f(b) - f(a) = f'(c) (b-a)$. Which implies that $f'(c) = \frac{f(b) - f(a)}{b-a}$. Now since $f$ is monotonically increasing, $f(b) - f(a) \ge 0$ whenever $b>a$, so $f'(c) \ge 0$. But this only shows for one particular $c \in (a,b)$, and the question asks to show this is true for ALL $x \in (a,b)$. What can I do to complete the proof?","Let $f:[a, b] \rightarrow \mathbb{R}$ where $a, b \in \mathbb{R}$ with $a <b$. Suppose $f$ is continuous on $[a,b]$ and differentiable on $(a,b)$. Suppose $f$ is monotonically increasing on $[a,b]$. Show that $f'(x) \ge 0$ for all $x \in (a,b)$. My attempt: I tried using the Mean Value Theorem, but it doesn't quite seem to work. For example, by the MVT we can conclude that there exists a $c \in (a,b)$ such that $f(b) - f(a) = f'(c) (b-a)$. Which implies that $f'(c) = \frac{f(b) - f(a)}{b-a}$. Now since $f$ is monotonically increasing, $f(b) - f(a) \ge 0$ whenever $b>a$, so $f'(c) \ge 0$. But this only shows for one particular $c \in (a,b)$, and the question asks to show this is true for ALL $x \in (a,b)$. What can I do to complete the proof?",,"['calculus', 'real-analysis', 'derivatives']"
88,Proof: Derivative of $(-1)^{x}$,Proof: Derivative of,(-1)^{x},The derivative for $(-1)^{x}$ is  \begin{equation} \frac d{dx}\left[(-1)^x\right]=i\pi(-1)^{x} \end{equation} But why? What happens with higher order derivatives? Thanks in advance.,The derivative for $(-1)^{x}$ is  \begin{equation} \frac d{dx}\left[(-1)^x\right]=i\pi(-1)^{x} \end{equation} But why? What happens with higher order derivatives? Thanks in advance.,,"['calculus', 'derivatives', 'complex-numbers']"
89,Does $f(x)>g(x)$ imply $\frac{d}{dx}f(x)>\frac{d}{dx}g(x)$?,Does  imply ?,f(x)>g(x) \frac{d}{dx}f(x)>\frac{d}{dx}g(x),Is it true that $f(x)>g(x) \implies \frac{d}{dx}f(x)>\frac{d}{dx}g(x)$? What about $|f(x)|>|g(x)| \implies \frac{d}{dx}|f(x)|>\frac{d}{dx}|g(x)|$?,Is it true that $f(x)>g(x) \implies \frac{d}{dx}f(x)>\frac{d}{dx}g(x)$? What about $|f(x)|>|g(x)| \implies \frac{d}{dx}|f(x)|>\frac{d}{dx}|g(x)|$?,,"['calculus', 'derivatives']"
90,Prove $f(x) = \frac{1}{x}$ is smooth (infinitely differentiable).,Prove  is smooth (infinitely differentiable).,f(x) = \frac{1}{x},"I have never proved that a function is smooth (infinitely differentiable) before.  The only function that comes to mind which is smooth is $g(x) = e^{x}$, because it is defined on all of $\Bbb R$, continuous everywhere, and once you prove that $g'(x) = e^{x}$, you are done in showing that it is infinitely differentiable, i.e., smooth. How would I prove that the function $f(x) = \frac{1}{x}$ is smooth everywhere except at $0$?  It's not hard to show that the derivative at each $x \neq 0$ of $f$ is $-\frac{1}{x^{2}}$.  So, I have that the first derivative exists.  How do I go about showing that derivatives of all orders exist?","I have never proved that a function is smooth (infinitely differentiable) before.  The only function that comes to mind which is smooth is $g(x) = e^{x}$, because it is defined on all of $\Bbb R$, continuous everywhere, and once you prove that $g'(x) = e^{x}$, you are done in showing that it is infinitely differentiable, i.e., smooth. How would I prove that the function $f(x) = \frac{1}{x}$ is smooth everywhere except at $0$?  It's not hard to show that the derivative at each $x \neq 0$ of $f$ is $-\frac{1}{x^{2}}$.  So, I have that the first derivative exists.  How do I go about showing that derivatives of all orders exist?",,"['real-analysis', 'derivatives']"
91,Points on $(x^2 + y^2)^2 = 2x^2 - 2y^2$ with slope of $1$,Points on  with slope of,(x^2 + y^2)^2 = 2x^2 - 2y^2 1,Let the curve in the plane defined by the equation: $(x^2 + y^2)^2 = 2x^2 - 2y^2$ How can i graph the curve in the plane and determine the points of the curve where $\frac{dy}{dx} = 1$. My work: First i found the roots of this equation with a change of variable $z = y^2$ and get: and then i tried to graph the point $ x - y$ and $x + y $ but i stuck i can't graph this and find the point  where the derivative is 1. Some help please.,Let the curve in the plane defined by the equation: $(x^2 + y^2)^2 = 2x^2 - 2y^2$ How can i graph the curve in the plane and determine the points of the curve where $\frac{dy}{dx} = 1$. My work: First i found the roots of this equation with a change of variable $z = y^2$ and get: and then i tried to graph the point $ x - y$ and $x + y $ but i stuck i can't graph this and find the point  where the derivative is 1. Some help please.,,"['calculus', 'derivatives', 'graphing-functions', 'polar-coordinates']"
92,justification of a limit,justification of a limit,,"I encountered something interesting when trying to differentiate $F(x) = c$. Consider: $\lim_{x→0}\frac0x$. I understand that for any $x$, no matter how incredibly small, we will have $0$ as the quotient. But don't things change when one takes matters to infinitesimals? I.e. why is the function $\frac0x = f(x)$, not undefined at $x=0$? I would appreciate a strong logical argument for why the limit stays at $0$.","I encountered something interesting when trying to differentiate $F(x) = c$. Consider: $\lim_{x→0}\frac0x$. I understand that for any $x$, no matter how incredibly small, we will have $0$ as the quotient. But don't things change when one takes matters to infinitesimals? I.e. why is the function $\frac0x = f(x)$, not undefined at $x=0$? I would appreciate a strong logical argument for why the limit stays at $0$.",,"['derivatives', 'infinity']"
93,How to get derivatives from Taylor series [closed],How to get derivatives from Taylor series [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 7 months ago . Improve this question I recently started learning about Taylor series and getting the derivative is confusing from the sum provided. So for example if I had the Taylor series $$f(x) = \sum_{k=0}^\infty (-1)^{k+1}\frac{k!}{(2k)!}(x-4)^k$$ how would I get derivatives from this for example $f ''(4)$ ?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 7 months ago . Improve this question I recently started learning about Taylor series and getting the derivative is confusing from the sum provided. So for example if I had the Taylor series how would I get derivatives from this for example ?",f(x) = \sum_{k=0}^\infty (-1)^{k+1}\frac{k!}{(2k)!}(x-4)^k f ''(4),"['calculus', 'derivatives', 'taylor-expansion']"
94,What is the tangent to this equation at origin?,What is the tangent to this equation at origin?,,"$$x(x^2 + y^2) = a(x^2 - y^2)$$ I was trying to find the equation of the tangent as $$(Y-0) = \frac{dy}{dx}(X - 0)$$ where $$\frac{dy}{dx} = \frac{2ax-3x^2-y^2}{2(x+a)y}$$ So here putting the values of $(x,y)$ as $(0,0)$ makes the derivative non existent. I am stuck at this point and unable to proceed further.",I was trying to find the equation of the tangent as where So here putting the values of as makes the derivative non existent. I am stuck at this point and unable to proceed further.,"x(x^2 + y^2) = a(x^2 - y^2) (Y-0) = \frac{dy}{dx}(X - 0) \frac{dy}{dx} = \frac{2ax-3x^2-y^2}{2(x+a)y} (x,y) (0,0)","['calculus', 'derivatives', 'implicit-differentiation', 'tangent-line', 'slope']"
95,Wrong answer in Thomas Calculus 14th Edition textbook,Wrong answer in Thomas Calculus 14th Edition textbook,,"There is this question on derivatives to which the answer is given as $\frac{43}{75}$ rad/sec in the answers section. This answer appears to be wrong. My Solution $$ \theta+\tan^{-1}\left(\frac{6}{4-x}\right)+\tan^{-1}\left(\frac{3}{x}\right)=\pi $$ which gets reduced to $$ \theta=\pi-\tan^{-1}\left(\frac{3x+12}{4x-x^2-18}\right) $$ Therefore, $$ \frac{d\theta}{dt}=\left(\frac{3x^2+24x-102}{(4x-x^2+18)^2+(3x+12)^2}\right)\frac{dx}{dt} $$ Given $x=4$ and $\frac{dx}{dt}=2\text{ cm/sec}$ , $$ \frac{d\theta}{dt}=-\frac{7}{75}\text{rad/sec} $$ I've got the graph of $\theta$ as a function of $x$ here , which also indicates my answer is correct. Or am I? Kindly help.","There is this question on derivatives to which the answer is given as rad/sec in the answers section. This answer appears to be wrong. My Solution which gets reduced to Therefore, Given and , I've got the graph of as a function of here , which also indicates my answer is correct. Or am I? Kindly help.","\frac{43}{75} 
\theta+\tan^{-1}\left(\frac{6}{4-x}\right)+\tan^{-1}\left(\frac{3}{x}\right)=\pi
 
\theta=\pi-\tan^{-1}\left(\frac{3x+12}{4x-x^2-18}\right)
 
\frac{d\theta}{dt}=\left(\frac{3x^2+24x-102}{(4x-x^2+18)^2+(3x+12)^2}\right)\frac{dx}{dt}
 x=4 \frac{dx}{dt}=2\text{ cm/sec} 
\frac{d\theta}{dt}=-\frac{7}{75}\text{rad/sec}
 \theta x",['derivatives']
96,About the derivative of the absolute value function,About the derivative of the absolute value function,,"For this question, let $f(x) = |x|$ . I found this answer saying that the derivative of the absolute value function is the signum function. In symbols, $$\frac{d}{dx}|x| = \mathrm{sgn}(x).$$ I know that $$f'(x) = \frac{x}{|x|}$$ using the chain rule. Notice that this is well-defined for $x \neq 0$ . However, the definition of the signum function is $$\mathrm{sgn}\,x = \begin{cases}-1 && \text{for } x< 0 \\ 0 &&\text{for }x = 0 \\ 1 && \text{for } x > 0\end{cases}.$$ This will be my question: Are $x/|x|$ and $\mathrm{sgn}\,x$ the same derivative of $|x|$ ?","For this question, let . I found this answer saying that the derivative of the absolute value function is the signum function. In symbols, I know that using the chain rule. Notice that this is well-defined for . However, the definition of the signum function is This will be my question: Are and the same derivative of ?","f(x) = |x| \frac{d}{dx}|x| = \mathrm{sgn}(x). f'(x) = \frac{x}{|x|} x \neq 0 \mathrm{sgn}\,x = \begin{cases}-1 && \text{for } x< 0 \\ 0 &&\text{for }x = 0 \\ 1 && \text{for } x > 0\end{cases}. x/|x| \mathrm{sgn}\,x |x|","['calculus', 'derivatives', 'absolute-value']"
97,Elementary proof of Power Rule For differentiation $f'(x)=rx^{r-1}$ for $f(x)=x^r$,Elementary proof of Power Rule For differentiation  for,f'(x)=rx^{r-1} f(x)=x^r,"I could  not find any elementary proof of the Power Rule for differentiation: Given $x,r \in R$ , $x>0$ and a function $f(x)=x^r$ , then its derivative is $f'(x)=rx^{r-1}$ Defintion :Let a sequence of rational numbers { $r_n$ } tend to $r$ then $x^r=\lim_{r_n \to r}x^{r_n}$ All the proofs I had seen utilized the derivative of $logx$ but I dont think this is  elementary because one has to show that for $e=\text{Euler's number}$ , $t \in R$ and $n \in N$ $\lim_{n \to \infty}(1+\frac{1}{n})^n=\lim_{t \to 0}(1+t)^\frac{1}{t}=e$ which can be proved using the Power Rule Ofcourse the proof for the Power Rule is straight forward if one is familiar with the result of Power Rule if $r$ is a rational number and theorem  regarding  derivative of a sequence of derivatives that are uniformly convergent.","I could  not find any elementary proof of the Power Rule for differentiation: Given , and a function , then its derivative is Defintion :Let a sequence of rational numbers { } tend to then All the proofs I had seen utilized the derivative of but I dont think this is  elementary because one has to show that for , and which can be proved using the Power Rule Ofcourse the proof for the Power Rule is straight forward if one is familiar with the result of Power Rule if is a rational number and theorem  regarding  derivative of a sequence of derivatives that are uniformly convergent.","x,r \in R x>0 f(x)=x^r f'(x)=rx^{r-1} r_n r x^r=\lim_{r_n \to r}x^{r_n} logx e=\text{Euler's number} t \in R n \in N \lim_{n \to \infty}(1+\frac{1}{n})^n=\lim_{t \to 0}(1+t)^\frac{1}{t}=e r","['real-analysis', 'derivatives']"
98,Find the derivative of the function $f(x)=x/(x^2+1)$ at a point a,Find the derivative of the function  at a point a,f(x)=x/(x^2+1),"$$\frac{x}{x^2+1}$$ I have to find the derivative and set a point by myself. So I just set $x=a$ ( is it correct?) and using the limit, $$ \lim_{h\to 0}\frac{f(a+h)-f(a)}{h}  = \lim_{h \to 0} \frac{\frac{a+h}{(a+h)^2 + 1} - \frac{a}{a^2+1}}{h}$$ therefore, the answer is $$\frac{ 1-a^2}{(a^2+1)^2}$$ is it correct?","$$\frac{x}{x^2+1}$$ I have to find the derivative and set a point by myself. So I just set $x=a$ ( is it correct?) and using the limit, $$ \lim_{h\to 0}\frac{f(a+h)-f(a)}{h}  = \lim_{h \to 0} \frac{\frac{a+h}{(a+h)^2 + 1} - \frac{a}{a^2+1}}{h}$$ therefore, the answer is $$\frac{ 1-a^2}{(a^2+1)^2}$$ is it correct?",,"['calculus', 'real-analysis', 'derivatives']"
99,"If $x^4+7x^2y^2+9y^4=24xy^3$,show that $\frac{dy}{dx}=\frac{y}{x}$","If ,show that",x^4+7x^2y^2+9y^4=24xy^3 \frac{dy}{dx}=\frac{y}{x},"If $x^4+7x^2y^2+9y^4=24xy^3$,show that $\frac{dy}{dx}=\frac{y}{x}$ I tried to solve it.But i got stuck after some steps. $x^4+7x^2y^2+9y^4=24xy^3$ $4x^3+7x^2.2y\frac{dy}{dx}+7y^2.2x+36y^3.\frac{dy}{dx}=24x.3y^2\frac{dy}{dx}+24y^3$ $\dfrac{dy}{dx}=\dfrac{24y^3-4x^3-14y^2x}{14x^2y+36y^3-72xy^2}$ How to move ahead?Or there is some other elegant way to solve it.","If $x^4+7x^2y^2+9y^4=24xy^3$,show that $\frac{dy}{dx}=\frac{y}{x}$ I tried to solve it.But i got stuck after some steps. $x^4+7x^2y^2+9y^4=24xy^3$ $4x^3+7x^2.2y\frac{dy}{dx}+7y^2.2x+36y^3.\frac{dy}{dx}=24x.3y^2\frac{dy}{dx}+24y^3$ $\dfrac{dy}{dx}=\dfrac{24y^3-4x^3-14y^2x}{14x^2y+36y^3-72xy^2}$ How to move ahead?Or there is some other elegant way to solve it.",,['derivatives']
