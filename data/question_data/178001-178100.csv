,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Finding the Area of a Torus-like surface,Finding the Area of a Torus-like surface,,"I'm trying to find out the Area of the following surface: Let $C$ be the curve associated  to a regular, simple path $\theta:[0,l]\rightarrow  \Bbb  R^2  $; also assume that $((x'(s))^2+((y'(s))^2=b^2$ and let $S$ be the surface generated by the circles of radius $b$, orthogonal to, and centered in points of the curve $\rho(s)=(\theta(s),0) $. With help from this source . I concluded that a convenient parametrization for $S$ is given by: $$ H(s, t) = ( x(s), y(s), 0) + \sin(t) (0, 0, b) + \cos(t) (b\ y'(s), -b\ x'(s), 0). $$ I intended to use the fact that the area of $S$ is given by the surface integral: $$ \int_S ||T_s \times T_t|| \ dv $$ However,by this approach, the terms of $||T_s \times T_t||$ become really awful. Am I doing something wrong here? What would you recommend? I found this approach , wich uses the Divergence Theorem. However in this case I´m not sure I can use that since I don´t have a vector field.","I'm trying to find out the Area of the following surface: Let $C$ be the curve associated  to a regular, simple path $\theta:[0,l]\rightarrow  \Bbb  R^2  $; also assume that $((x'(s))^2+((y'(s))^2=b^2$ and let $S$ be the surface generated by the circles of radius $b$, orthogonal to, and centered in points of the curve $\rho(s)=(\theta(s),0) $. With help from this source . I concluded that a convenient parametrization for $S$ is given by: $$ H(s, t) = ( x(s), y(s), 0) + \sin(t) (0, 0, b) + \cos(t) (b\ y'(s), -b\ x'(s), 0). $$ I intended to use the fact that the area of $S$ is given by the surface integral: $$ \int_S ||T_s \times T_t|| \ dv $$ However,by this approach, the terms of $||T_s \times T_t||$ become really awful. Am I doing something wrong here? What would you recommend? I found this approach , wich uses the Divergence Theorem. However in this case I´m not sure I can use that since I don´t have a vector field.",,"['real-analysis', 'integration', 'multivariable-calculus', 'parametric']"
1,Challenging Path Integral,Challenging Path Integral,,"Let $c$ be the curve of intersection of the plane defined by $x+y+z = a$ and the cylinder $x^{2} + y^{2} = a^{2}$ ($a > 0$). Evaluate the path integral: $\displaystyle\oint_{c} \sqrt{a^2 + xy}   \mathrm{d}s$ I have first tried to parametrize the curve of intersection with the followng (let $0 \leq t \leq 2\pi$): $x = a\cos(t)$ $y = a\sin(t)$ $z = a - x -y = a - a\cos(t) - a\sin(t)$ Then, we can find that $|c'(t)| = \sqrt{(-a\sin(t))^2 + (a\cos(t))^2 + (a\sin(t) - a\cos(t))^2}$, to then have that: $\displaystyle\oint_{c} \sqrt{a^{2} + xy} \mathrm{d}s = \int_{0}^{2\pi} \sqrt{a^{2}+a^{2}\cos(t)\sin(t)} |c'(t)|\mathrm{d}t = \int_{0}^{2\pi} \sqrt{a^{2}+a^{2}\cos(t)\sin(t)} \sqrt{2a^{2}-2a^{2}\cos(t)\sin(t)} \mathrm{d}t = \sqrt{2}a^{2} \int_{0}^{2\pi} \sqrt{1-\cos^{2}(t)\sin^{2}(t)} \mathrm{d}t$ I've been trying to solve the last integral, but apparently I checked if it could be solved with Wolfram Alpha, and it gives the solution in terms of the elliptic integral of the second kind. On the other hand, Stokes' Theorem cannot be used that easily as we would not have a line integral in this case. What would be the best way to solve this problem to obtain a closed form solution? Perhaps I made a mistake in my calculations. Thank you for the help.","Let $c$ be the curve of intersection of the plane defined by $x+y+z = a$ and the cylinder $x^{2} + y^{2} = a^{2}$ ($a > 0$). Evaluate the path integral: $\displaystyle\oint_{c} \sqrt{a^2 + xy}   \mathrm{d}s$ I have first tried to parametrize the curve of intersection with the followng (let $0 \leq t \leq 2\pi$): $x = a\cos(t)$ $y = a\sin(t)$ $z = a - x -y = a - a\cos(t) - a\sin(t)$ Then, we can find that $|c'(t)| = \sqrt{(-a\sin(t))^2 + (a\cos(t))^2 + (a\sin(t) - a\cos(t))^2}$, to then have that: $\displaystyle\oint_{c} \sqrt{a^{2} + xy} \mathrm{d}s = \int_{0}^{2\pi} \sqrt{a^{2}+a^{2}\cos(t)\sin(t)} |c'(t)|\mathrm{d}t = \int_{0}^{2\pi} \sqrt{a^{2}+a^{2}\cos(t)\sin(t)} \sqrt{2a^{2}-2a^{2}\cos(t)\sin(t)} \mathrm{d}t = \sqrt{2}a^{2} \int_{0}^{2\pi} \sqrt{1-\cos^{2}(t)\sin^{2}(t)} \mathrm{d}t$ I've been trying to solve the last integral, but apparently I checked if it could be solved with Wolfram Alpha, and it gives the solution in terms of the elliptic integral of the second kind. On the other hand, Stokes' Theorem cannot be used that easily as we would not have a line integral in this case. What would be the best way to solve this problem to obtain a closed form solution? Perhaps I made a mistake in my calculations. Thank you for the help.",,"['integration', 'multivariable-calculus', 'parametric', 'line-integrals']"
2,How do we know that $(\frac {\partial \mathbf r}{\partial u} \times \frac {\partial \mathbf r}{\partial v}) $ is a unit vector?,How do we know that  is a unit vector?,(\frac {\partial \mathbf r}{\partial u} \times \frac {\partial \mathbf r}{\partial v}) ,"The two formulas I have for surface integrals are $\int_S \mathbf v \cdot \mathbf n dS$ and $\int_{\gamma} \mathbf v(\mathbf r(u,v)) \cdot \left(\frac {\partial \mathbf r}{\partial u} \times \frac {\partial \mathbf r}{\partial v}\right) dudv$.  If these are the same thing then doesn't that mean that $\left(\frac {\partial \mathbf r}{\partial u} \times \frac {\partial \mathbf r}{\partial v}\right)$ has to be equal to $\mathbf n$, which is a unit vector?  I don't see how for any parametrization we make, that cross product will always be of unit length. Can someone explain to me what I'm missing here?","The two formulas I have for surface integrals are $\int_S \mathbf v \cdot \mathbf n dS$ and $\int_{\gamma} \mathbf v(\mathbf r(u,v)) \cdot \left(\frac {\partial \mathbf r}{\partial u} \times \frac {\partial \mathbf r}{\partial v}\right) dudv$.  If these are the same thing then doesn't that mean that $\left(\frac {\partial \mathbf r}{\partial u} \times \frac {\partial \mathbf r}{\partial v}\right)$ has to be equal to $\mathbf n$, which is a unit vector?  I don't see how for any parametrization we make, that cross product will always be of unit length. Can someone explain to me what I'm missing here?",,['multivariable-calculus']
3,Surface integrals: Find the area of the portion of the cone $x^2+y^2=z^2$ above the $xy$ plane and inside the cylinder $x^2+y^2=ax$,Surface integrals: Find the area of the portion of the cone  above the  plane and inside the cylinder,x^2+y^2=z^2 xy x^2+y^2=ax,"I need to find the area of the portion of the cone $x^2+y^2=z^2$ above the $xy$ plane and inside the cylinder $x^2+y^2=ax$ . For this, I used cylindrical coordinates to parametrize the region: $$x=r\cos \theta$$ $$y=r\sin \theta$$ $$z=z$$ Since $x^2+y^2=z^2$ then $r=z$. And since $x^2+y^2=ax$ then $r=a\cos \theta$. So the final parameterization, for $-\pi/2<\theta<\pi/2$ and $0<r<a\cos \theta$, is: $$x=r\cos \theta$$ $$y=r\sin \theta$$ $$z=r$$ And so, $T_r =(\cos\theta,\sin\theta,1)$ and $ T_{\theta}=(-r\sin\theta ,r \cos\theta, 0)$. Which gives  $T_r \times T_{\theta}=(-r\cos\theta, -rsin\theta, r^2)$ And so the area is given by: $$\int_{-\pi/2}^{\pi/2} \int_0^{a \cos\theta}r(1+r^2)^{1/2} dr d\theta$$ Which seems simple, but gives a hell-difficult integral when integrating respect to $\theta$. So I wanted to ask you: is my approach correct? Is there a simpler way to parametrize this surface?","I need to find the area of the portion of the cone $x^2+y^2=z^2$ above the $xy$ plane and inside the cylinder $x^2+y^2=ax$ . For this, I used cylindrical coordinates to parametrize the region: $$x=r\cos \theta$$ $$y=r\sin \theta$$ $$z=z$$ Since $x^2+y^2=z^2$ then $r=z$. And since $x^2+y^2=ax$ then $r=a\cos \theta$. So the final parameterization, for $-\pi/2<\theta<\pi/2$ and $0<r<a\cos \theta$, is: $$x=r\cos \theta$$ $$y=r\sin \theta$$ $$z=r$$ And so, $T_r =(\cos\theta,\sin\theta,1)$ and $ T_{\theta}=(-r\sin\theta ,r \cos\theta, 0)$. Which gives  $T_r \times T_{\theta}=(-r\cos\theta, -rsin\theta, r^2)$ And so the area is given by: $$\int_{-\pi/2}^{\pi/2} \int_0^{a \cos\theta}r(1+r^2)^{1/2} dr d\theta$$ Which seems simple, but gives a hell-difficult integral when integrating respect to $\theta$. So I wanted to ask you: is my approach correct? Is there a simpler way to parametrize this surface?",,"['real-analysis', 'integration', 'multivariable-calculus', 'parametric']"
4,Is there any diffeomorphism from A to B that $f(A)=B$?,Is there any diffeomorphism from A to B that ?,f(A)=B,"I've been thinking about this problem. $A = \{(x,y) \in \mathbb{R}^2 : x \ge 0 \wedge y \ge 0\}$ $B = \mathbb{R}^2 \setminus \{(x,y) \in \mathbb{R}^2 : x>0 \wedge y>0 \}$ Is there any diffeomorphism $f : \mathbb{R}^2 \rightarrow \mathbb{R}^2 $that $f(A)=B$? My thoughts : We need to ""stretch"" A set on three quadrants and then rotate it for $\frac{\pi}{2}$ angle. That ""stretching"" looks like complex numbers argument multiplication. But, at the beginning of coordinate system derivative of this function is $0$. Well. I'm not really sure about this ""solution"".","I've been thinking about this problem. $A = \{(x,y) \in \mathbb{R}^2 : x \ge 0 \wedge y \ge 0\}$ $B = \mathbb{R}^2 \setminus \{(x,y) \in \mathbb{R}^2 : x>0 \wedge y>0 \}$ Is there any diffeomorphism $f : \mathbb{R}^2 \rightarrow \mathbb{R}^2 $that $f(A)=B$? My thoughts : We need to ""stretch"" A set on three quadrants and then rotate it for $\frac{\pi}{2}$ angle. That ""stretching"" looks like complex numbers argument multiplication. But, at the beginning of coordinate system derivative of this function is $0$. Well. I'm not really sure about this ""solution"".",,"['calculus', 'multivariable-calculus']"
5,"Vector Analysis Flux question using divergence theorem, trouble understanding the vector field","Vector Analysis Flux question using divergence theorem, trouble understanding the vector field",,Let $S$ be the curve cylindrical surface $x^2+y^2=a^2$ for $0 \leq z \leq 2a$.  Calculate flux of the of $\displaystyle \int \vec{r} \cdot \vec{ds}$ over $S$ directly and also verify the answer using the divergence theorem. I normally know how to set these up but im not sure what $\vec{r}$ is? Any help would be much appreciated,Let $S$ be the curve cylindrical surface $x^2+y^2=a^2$ for $0 \leq z \leq 2a$.  Calculate flux of the of $\displaystyle \int \vec{r} \cdot \vec{ds}$ over $S$ directly and also verify the answer using the divergence theorem. I normally know how to set these up but im not sure what $\vec{r}$ is? Any help would be much appreciated,,"['multivariable-calculus', 'vector-analysis', 'surface-integrals']"
6,Calculating Hydrodynamic Interaction Tensor,Calculating Hydrodynamic Interaction Tensor,,"I'm a bit of a newbie when it comes to Tensor calculus. Please excuse me as I learn... Given the Oseen tensor, $\mathbf{T}(\mathbf{R}) = (8\pi \eta R)^{-1} \left[ \mathbf{I} + (\mathbf{R}\mathbf{R}/R^2) \right]$ (1) the velocity perturbation $\mathbf{v}(\mathbf{R})$ caused by a point force $\mathbf{F}$ can be determined, or if you like, $\mathbf{v}(\mathbf{R}) = \mathbf{T}(\mathbf{R}) \cdot \mathbf{F}$ (2) If the single point force $\mathbf{F}$ is distributed uniformly over a spherical surface of radius $a$ , then the corrected (via Taylor series expansion) hydrodynamic interaction tensor is $\mathbf{P}(\mathbf{R}) = \mathbf{T}(\mathbf{R}) + \frac{1}{6} a^2 \nabla^2 \mathbf{T}(\mathbf{R})$ (3) where we have neglected higher order terms. Substituting (1) into (3), we obtain $\mathbf{P}(\mathbf{R}) =  (8\pi \eta R)^{-1} \left\lbrace \left[ \mathbf{I} + (\mathbf{R}\mathbf{R}/R^2) \right] + (a^2/R^2) \left[ \frac{1}{3} \mathbf{I} - (\mathbf{R}\mathbf{R}/R^2) \right] \right\rbrace $ (4) Now, here are my silly questions... How $\frac{1}{6} a^2 \nabla^2 \mathbf{T}(\mathbf{R})$ becomes $(a^2/R^2) \left[ \frac{1}{3} \mathbf{I} - (\mathbf{R}\mathbf{R}/R^2) \right]$ ? I feel stupid for asking this one. Seems like it should be easy enough but I'm going around in circles at the moment. If I wanted to calculate $\left(1 + \frac{1}{6} a^2 \nabla^2  \right)^2\mathbf{T}(\mathbf{R})$ , how would I go about calculating? Do I need to numerically find $\nabla^4$ ? Essentially I'm trying to construct the Rotne-Prager tensor ( $M_{\alpha \beta}^{RP}$ )in this paper: http://authors.library.caltech.edu/3142/1/ICHpof01.pdf Any pointers appreciated... EDIT (03/12/2014): I've managed to calculate the hydrodynamic interaction forces. I've just approached it numerically which answers question 2. Brief steps of what I've done is below for anyone interested (more for my own reference he he) I have assumed a polystyrene bead of radius $a=1 \mu \mathrm{m}$ is suspended in water ( $\eta = 1.002 \times 10^{-3}$ ) and subjected to a force $\mathbf{F} = 1 \times 10^{-9}N \mathbf{\hat{x}}$ . Eq. (2) is then calculated using Eq. (4) and shown in Fig. 1. (source: torrkish.com ) Figure 1. I've compared this with Comsol Multiphysics and it seems right. Now consider the system with two particles $\alpha$ and $\beta$ . Faxen's law states that the force experienced by $\alpha$ due to the velocity perturbation caused by $\beta$ is $\mathbf{F}_{\alpha} = 6 \pi \eta a \left[ \left(1 + \frac{a^2}{6}\nabla^2 \right) \mathbf{v}(\mathbf{R}_{\alpha}) - \mathbf{U}_{\alpha} \right]$ (5) where $\mathbf{U}_{\alpha}$ is the translational velocity of particle $\alpha$ , and $\mathbf{v}(\mathbf{R}_{\alpha})$ is the velocity field in which particle $\alpha$ is immersed. Written more explicitly, this is $\mathbf{F}_{\alpha} = 6 \pi \eta a \mathbf{v}(\mathbf{R}_{\alpha}) + \pi \eta a^3 \nabla^2 \mathbf{v}(\mathbf{R}_{\alpha})$ (6) The omitted term $6 \pi \eta a \mathbf{U}_{\alpha}$ is the force required to move $\alpha$ with velocity $\mathbf{U}_{\alpha}$ . In my case this is an external optical force derived from Maxwell's tensor. The first term in Eq. (6) is a simple scaling of the velocity field in Figure (1b). The second term can be calculated numerically (discretize, etc, etc), and its contribution is shown in Fig. (2). (source: torrkish.com ) Figure 2. Calculating Eq. (6) for all $\mathbf{R}_{\alpha}$ the individual components of $\mathbf{F}$ are shown in Fig. 3 and 4. (source: torrkish.com ) Figure 3. $\mathbf{F} \cdot \mathbf{\hat{x}}$ (source: torrkish.com ) Figure 4 $\mathbf{F} \cdot \mathbf{\hat{y}}$ Obviously, we have violated some overlap rules here. Two particles need to be at least $2a$ distance from each other. Naturally, there was a strong singularity when $r < a$ which is why I have omitted that region. Next we must apply the Method of Reflections which is an iterative process which in summary is: $\mathbf{F}_{\beta}^{(i)} = 6 \pi \eta a \left[ \left(1 + \frac{a^2}{6}\nabla^2 \right) \mathbf{v}_{\alpha}^{(i-1)} \right]$ If we assume $\alpha$ is always positioned at the origin and apply an external force $\mathbf{F} = 1 \times 10^{-9}N \mathbf{\hat{x}}$ , and $\beta$ is placed in various positions, the hydrodynamic interaction forces on $\alpha$ and $\beta$ are shown in Fig. (5). Generally, convergence criteria was met within less than 20 iterations. (source: torrkish.com ) Figure 5. Hydrodynamic force components in the (a) x-direction $\mathrm{log}_{10}(F_x)$ and in the (b) $\mathrm{log}_{10}(F_y)$ y-direction for a particle $\alpha$ centered at the origin as a function of the $(x,y)$ position of particle $\beta$ . That'll do for now. EDIT 9th December 2014 (source: torrkish.com ) Squeeze flow between two 1micron spheres","I'm a bit of a newbie when it comes to Tensor calculus. Please excuse me as I learn... Given the Oseen tensor, (1) the velocity perturbation caused by a point force can be determined, or if you like, (2) If the single point force is distributed uniformly over a spherical surface of radius , then the corrected (via Taylor series expansion) hydrodynamic interaction tensor is (3) where we have neglected higher order terms. Substituting (1) into (3), we obtain (4) Now, here are my silly questions... How becomes ? I feel stupid for asking this one. Seems like it should be easy enough but I'm going around in circles at the moment. If I wanted to calculate , how would I go about calculating? Do I need to numerically find ? Essentially I'm trying to construct the Rotne-Prager tensor ( )in this paper: http://authors.library.caltech.edu/3142/1/ICHpof01.pdf Any pointers appreciated... EDIT (03/12/2014): I've managed to calculate the hydrodynamic interaction forces. I've just approached it numerically which answers question 2. Brief steps of what I've done is below for anyone interested (more for my own reference he he) I have assumed a polystyrene bead of radius is suspended in water ( ) and subjected to a force . Eq. (2) is then calculated using Eq. (4) and shown in Fig. 1. (source: torrkish.com ) Figure 1. I've compared this with Comsol Multiphysics and it seems right. Now consider the system with two particles and . Faxen's law states that the force experienced by due to the velocity perturbation caused by is (5) where is the translational velocity of particle , and is the velocity field in which particle is immersed. Written more explicitly, this is (6) The omitted term is the force required to move with velocity . In my case this is an external optical force derived from Maxwell's tensor. The first term in Eq. (6) is a simple scaling of the velocity field in Figure (1b). The second term can be calculated numerically (discretize, etc, etc), and its contribution is shown in Fig. (2). (source: torrkish.com ) Figure 2. Calculating Eq. (6) for all the individual components of are shown in Fig. 3 and 4. (source: torrkish.com ) Figure 3. (source: torrkish.com ) Figure 4 Obviously, we have violated some overlap rules here. Two particles need to be at least distance from each other. Naturally, there was a strong singularity when which is why I have omitted that region. Next we must apply the Method of Reflections which is an iterative process which in summary is: If we assume is always positioned at the origin and apply an external force , and is placed in various positions, the hydrodynamic interaction forces on and are shown in Fig. (5). Generally, convergence criteria was met within less than 20 iterations. (source: torrkish.com ) Figure 5. Hydrodynamic force components in the (a) x-direction and in the (b) y-direction for a particle centered at the origin as a function of the position of particle . That'll do for now. EDIT 9th December 2014 (source: torrkish.com ) Squeeze flow between two 1micron spheres","\mathbf{T}(\mathbf{R}) = (8\pi \eta R)^{-1} \left[ \mathbf{I} + (\mathbf{R}\mathbf{R}/R^2) \right] \mathbf{v}(\mathbf{R}) \mathbf{F} \mathbf{v}(\mathbf{R}) = \mathbf{T}(\mathbf{R}) \cdot \mathbf{F} \mathbf{F} a \mathbf{P}(\mathbf{R}) = \mathbf{T}(\mathbf{R}) + \frac{1}{6} a^2 \nabla^2 \mathbf{T}(\mathbf{R}) \mathbf{P}(\mathbf{R}) =  (8\pi \eta R)^{-1} \left\lbrace \left[ \mathbf{I} + (\mathbf{R}\mathbf{R}/R^2) \right] + (a^2/R^2) \left[ \frac{1}{3} \mathbf{I} - (\mathbf{R}\mathbf{R}/R^2) \right] \right\rbrace  \frac{1}{6} a^2 \nabla^2 \mathbf{T}(\mathbf{R}) (a^2/R^2) \left[ \frac{1}{3} \mathbf{I} - (\mathbf{R}\mathbf{R}/R^2) \right] \left(1 + \frac{1}{6} a^2 \nabla^2  \right)^2\mathbf{T}(\mathbf{R}) \nabla^4 M_{\alpha \beta}^{RP} a=1 \mu \mathrm{m} \eta = 1.002 \times 10^{-3} \mathbf{F} = 1 \times 10^{-9}N \mathbf{\hat{x}} \alpha \beta \alpha \beta \mathbf{F}_{\alpha} = 6 \pi \eta a \left[ \left(1 + \frac{a^2}{6}\nabla^2 \right) \mathbf{v}(\mathbf{R}_{\alpha}) - \mathbf{U}_{\alpha} \right] \mathbf{U}_{\alpha} \alpha \mathbf{v}(\mathbf{R}_{\alpha}) \alpha \mathbf{F}_{\alpha} = 6 \pi \eta a \mathbf{v}(\mathbf{R}_{\alpha}) + \pi \eta a^3 \nabla^2 \mathbf{v}(\mathbf{R}_{\alpha}) 6 \pi \eta a \mathbf{U}_{\alpha} \alpha \mathbf{U}_{\alpha} \mathbf{R}_{\alpha} \mathbf{F} \mathbf{F} \cdot \mathbf{\hat{x}} \mathbf{F} \cdot \mathbf{\hat{y}} 2a r < a \mathbf{F}_{\beta}^{(i)} = 6 \pi \eta a \left[ \left(1 + \frac{a^2}{6}\nabla^2 \right) \mathbf{v}_{\alpha}^{(i-1)} \right] \alpha \mathbf{F} = 1 \times 10^{-9}N \mathbf{\hat{x}} \beta \alpha \beta \mathrm{log}_{10}(F_x) \mathrm{log}_{10}(F_y) \alpha (x,y) \beta","['multivariable-calculus', 'physics', 'tensors']"
7,Notation laplace operator squared $\Delta^2$,Notation laplace operator squared,\Delta^2,I have the following expression (in a numerical context) $$\Delta_h u(x) = \Delta u(x) + \frac{h^2}{12} \Delta^2 u(x) + O(h^4)$$ The $\Delta$ is the Laplace operator so $\Delta u = u_{xx}+u_{yy}$. But what is $\Delta^2$? In the context it would make sense (but it is not really a strong indication) for it to be $$\Delta^2 u = u_{xxxx}+u_{yyyy}$$ but when I first saw it I just thougth it was $$\Delta^2 u = \Delta (\Delta u) = \Delta (u_{xx}+u_{yy}) = u_{xxxx}+2u_{xxyy}+u_{yyyy}.$$ Which one is correct? Can you provide references where one or the other is used?,I have the following expression (in a numerical context) $$\Delta_h u(x) = \Delta u(x) + \frac{h^2}{12} \Delta^2 u(x) + O(h^4)$$ The $\Delta$ is the Laplace operator so $\Delta u = u_{xx}+u_{yy}$. But what is $\Delta^2$? In the context it would make sense (but it is not really a strong indication) for it to be $$\Delta^2 u = u_{xxxx}+u_{yyyy}$$ but when I first saw it I just thougth it was $$\Delta^2 u = \Delta (\Delta u) = \Delta (u_{xx}+u_{yy}) = u_{xxxx}+2u_{xxyy}+u_{yyyy}.$$ Which one is correct? Can you provide references where one or the other is used?,,"['multivariable-calculus', 'numerical-methods', 'notation', 'laplacian']"
8,Setting up the intergal but do not integrate,Setting up the intergal but do not integrate,,"I'm having a little trouble with this problem. Let D be the solid bounded by y=x, z=1-y^2, x=0, and z=0 1) Sketch the region of integration using 2 and  dimensional sketches to show the region clearly. 2). Setup 6 different integrals for calculating the volume of D, each with a different order of integration. I can do number 1 but for 2 I am having trouble I am thinking I need to setup intergals like this  dydxdz dxdydz dzdydx dzdxdy dydzdx dxdzdy If you could explain how to do go about doing this it will help a WHOLE bunch Of course I don't want you guys to do my work for me but if you could do one order of integration and walk me through that as well it will help because my teacher isn't all that great. Thank you much","I'm having a little trouble with this problem. Let D be the solid bounded by y=x, z=1-y^2, x=0, and z=0 1) Sketch the region of integration using 2 and  dimensional sketches to show the region clearly. 2). Setup 6 different integrals for calculating the volume of D, each with a different order of integration. I can do number 1 but for 2 I am having trouble I am thinking I need to setup intergals like this  dydxdz dxdydz dzdydx dzdxdy dydzdx dxdzdy If you could explain how to do go about doing this it will help a WHOLE bunch Of course I don't want you guys to do my work for me but if you could do one order of integration and walk me through that as well it will help because my teacher isn't all that great. Thank you much",,['multivariable-calculus']
9,A convenient variable change for solving an integral,A convenient variable change for solving an integral,,"I'm trying to compute the following integral: $$ \int_0^1\int_{x^2}^x  (x^2+y^2)^{\frac{-1}{2}} \, dy dx$$ I already noted that this is the region between the parabola $y=x^2$ and the line $y=x$, so it is clear that there should be a convenient variable change to simplify this integral. I wanted to ask for suggestions.","I'm trying to compute the following integral: $$ \int_0^1\int_{x^2}^x  (x^2+y^2)^{\frac{-1}{2}} \, dy dx$$ I already noted that this is the region between the parabola $y=x^2$ and the line $y=x$, so it is clear that there should be a convenient variable change to simplify this integral. I wanted to ask for suggestions.",,"['integration', 'multivariable-calculus']"
10,If $\overline f=f-f'(a)$ then how is $\overline {f'(a)}=0$?,If  then how is ?,\overline f=f-f'(a) \overline {f'(a)}=0,"Below is the definition of a function being differentiable at a point, given in my notes: A function $f:A \rightarrow Y$ is said to be differentiable at $a \in A$ if there is a linear map $T \in L(X,Y) ,$ such that                   lim$_{r \to 0}\frac{\|f(a+r)-f(a)-T\text{r}\|}{\|\text{r}\|}=0$. It is called the derivative of $f$ at $a \in A$.  Note that $f'(a)$ is a linear operator and its value at $x\in X$ is $f'(a)(x)\in Y$ Now I had a proof in which I had to define $\overline f=f-f'(a)$ and then it is given that $\overline {f'(a)}=0$. But I can't understand how is $\overline {f'(a)}=0$? Kindly if anyone can help me with this..","Below is the definition of a function being differentiable at a point, given in my notes: A function $f:A \rightarrow Y$ is said to be differentiable at $a \in A$ if there is a linear map $T \in L(X,Y) ,$ such that                   lim$_{r \to 0}\frac{\|f(a+r)-f(a)-T\text{r}\|}{\|\text{r}\|}=0$. It is called the derivative of $f$ at $a \in A$.  Note that $f'(a)$ is a linear operator and its value at $x\in X$ is $f'(a)(x)\in Y$ Now I had a proof in which I had to define $\overline f=f-f'(a)$ and then it is given that $\overline {f'(a)}=0$. But I can't understand how is $\overline {f'(a)}=0$? Kindly if anyone can help me with this..",,"['multivariable-calculus', 'derivatives', 'linear-transformations']"
11,Pullback of Euclidean metric under spherical coordinates,Pullback of Euclidean metric under spherical coordinates,,"I came across the following problem when I was reading a paper. Let $\Omega=B_2\subset\mathbb{R}^3$, which is the ball of radius 2 centered at origin. Map $F:B_2\backslash \{0\}\to B_2\backslash B_1$ is given via $$ F(x)=(1+\frac{|x|}{2})\frac{x}{|x|} $$ Now under spherical coordinates $(r,\phi,\theta)$, this $F$ becomes $F:(r,\phi,\theta)\mapsto(\frac{1}{2}r+1,\phi,\theta)$. Then since the metric $g$ is of the form $$g = \begin{pmatrix} 1 & 0 & 0\\ 0 & r^2 & 0 \\ 0 & 0 & r^2 \sin^2 \phi \end{pmatrix}$$ I have $$F_{\ast}g=DF^TgDF=\begin{pmatrix} \frac{1}{4} & 0 & 0\\ 0 & r^2 & 0 \\ 0 & 0 & r^2 \sin^2 \phi \end{pmatrix}$$ However from the paper it seems that the final result is not true. So what is going wrong here when I tried to compute $F_{\ast}g$ ? Thank you in advance!","I came across the following problem when I was reading a paper. Let $\Omega=B_2\subset\mathbb{R}^3$, which is the ball of radius 2 centered at origin. Map $F:B_2\backslash \{0\}\to B_2\backslash B_1$ is given via $$ F(x)=(1+\frac{|x|}{2})\frac{x}{|x|} $$ Now under spherical coordinates $(r,\phi,\theta)$, this $F$ becomes $F:(r,\phi,\theta)\mapsto(\frac{1}{2}r+1,\phi,\theta)$. Then since the metric $g$ is of the form $$g = \begin{pmatrix} 1 & 0 & 0\\ 0 & r^2 & 0 \\ 0 & 0 & r^2 \sin^2 \phi \end{pmatrix}$$ I have $$F_{\ast}g=DF^TgDF=\begin{pmatrix} \frac{1}{4} & 0 & 0\\ 0 & r^2 & 0 \\ 0 & 0 & r^2 \sin^2 \phi \end{pmatrix}$$ However from the paper it seems that the final result is not true. So what is going wrong here when I tried to compute $F_{\ast}g$ ? Thank you in advance!",,"['multivariable-calculus', 'differential-geometry']"
12,Normalize gradient,Normalize gradient,,"I want to minimize a function $f \, : \, \mathbb{R}^{N} \, \longrightarrow \, \mathbb{R}$ (with $N \in \mathbb{N}^{\ast}$. In my problem, $N = 315$). I know that $f$ is differentiable on $\mathbb{R}^{N}$ and convex. Numerically, when I compute the gradient of $f$ at a given point, I have noticed that the coordinates of $f$ have different orders of magnitude ($\frac{\partial f}{\partial x_{1}}  \gg \frac{\partial f}{\partial x_{2}}$ at every point). In practice, when I minimize $f$ using a gradient descent ($x_{k+1} \, \leftarrow \, x_{k} - t_{k}\nabla f(x_{k})$, with $t_{k}$ determined using a backtracking line-search), $x_{2}$ almost do not change through the iterations. I have been told that my problem is badly scaled and that I can address this issue by minimizing $\tilde{f}(x) = f(Dx)$ where $D$ is a diagonal matrix with positive coefficients. Doing so, I have : $$ \nabla \tilde{f}(x) = D \nabla f(Dx) $$ for all $x \in \mathbb{R}^{N}$. My question is : how do I choose this matrix $D$ ? Are there (empirical) rules to choose this matrix ?","I want to minimize a function $f \, : \, \mathbb{R}^{N} \, \longrightarrow \, \mathbb{R}$ (with $N \in \mathbb{N}^{\ast}$. In my problem, $N = 315$). I know that $f$ is differentiable on $\mathbb{R}^{N}$ and convex. Numerically, when I compute the gradient of $f$ at a given point, I have noticed that the coordinates of $f$ have different orders of magnitude ($\frac{\partial f}{\partial x_{1}}  \gg \frac{\partial f}{\partial x_{2}}$ at every point). In practice, when I minimize $f$ using a gradient descent ($x_{k+1} \, \leftarrow \, x_{k} - t_{k}\nabla f(x_{k})$, with $t_{k}$ determined using a backtracking line-search), $x_{2}$ almost do not change through the iterations. I have been told that my problem is badly scaled and that I can address this issue by minimizing $\tilde{f}(x) = f(Dx)$ where $D$ is a diagonal matrix with positive coefficients. Doing so, I have : $$ \nabla \tilde{f}(x) = D \nabla f(Dx) $$ for all $x \in \mathbb{R}^{N}$. My question is : how do I choose this matrix $D$ ? Are there (empirical) rules to choose this matrix ?",,"['multivariable-calculus', 'derivatives', 'optimization', 'convex-optimization']"
13,Multiple Integral Substitution Error,Multiple Integral Substitution Error,,"I just started learning about the substitution rule for multiple integrals and I decided to give myself an example problem: Calculate $\iint_R{(x^2 + y^2)dA}$ with $R = \{(x, y) \in \Bbb{R} \ |\ 0 \le x + 2y \le 2 \land 0 \le x - y \le 1\}$ To calculate this, I decided to substitute $x = \frac{1}{3}(u+2v)$ and $y = \frac{1}{3}(u-v)$, (which is the same as $u = x + 2y$ and $v = x - y$). Under this substitution, the region of integral becomes $S = \{(u, v) \in \Bbb{R} \ |\ 0 \le u \le 2 \land 0 \le v \le 1\}$, which we can integrate directly. Now our integral is $$ \begin{align} \iint_R{(x^2+y^2)dA} & = \int_0^1\int_0^2{\left[\frac{1}{9}(u+2v)^2 + \frac{1}{9}(u-v)^2\right]\left|\frac{\partial{(x, y)}}{\partial{(u, v)}}\right|dudv} \\ & = -\frac{1}{27}\int_0^1\int_0^2{\left[(u+2v)^2 + (u-v)^2\right]dudv}. \end{align} $$ Because $\left|\frac{\partial{(x, y)}}{\partial{(u, v)}}\right| = -\frac{1}{3}$. Here is when I run into a problem. The original integrand is positive everywhere, so the volume is positive as well. But the transformed integrand is positive everywhere as well, so when multiplied by a negative constant it must mean that the volume is negative. How can the volume be both positive and negative at the same time? Where was my error?","I just started learning about the substitution rule for multiple integrals and I decided to give myself an example problem: Calculate $\iint_R{(x^2 + y^2)dA}$ with $R = \{(x, y) \in \Bbb{R} \ |\ 0 \le x + 2y \le 2 \land 0 \le x - y \le 1\}$ To calculate this, I decided to substitute $x = \frac{1}{3}(u+2v)$ and $y = \frac{1}{3}(u-v)$, (which is the same as $u = x + 2y$ and $v = x - y$). Under this substitution, the region of integral becomes $S = \{(u, v) \in \Bbb{R} \ |\ 0 \le u \le 2 \land 0 \le v \le 1\}$, which we can integrate directly. Now our integral is $$ \begin{align} \iint_R{(x^2+y^2)dA} & = \int_0^1\int_0^2{\left[\frac{1}{9}(u+2v)^2 + \frac{1}{9}(u-v)^2\right]\left|\frac{\partial{(x, y)}}{\partial{(u, v)}}\right|dudv} \\ & = -\frac{1}{27}\int_0^1\int_0^2{\left[(u+2v)^2 + (u-v)^2\right]dudv}. \end{align} $$ Because $\left|\frac{\partial{(x, y)}}{\partial{(u, v)}}\right| = -\frac{1}{3}$. Here is when I run into a problem. The original integrand is positive everywhere, so the volume is positive as well. But the transformed integrand is positive everywhere as well, so when multiplied by a negative constant it must mean that the volume is negative. How can the volume be both positive and negative at the same time? Where was my error?",,"['integration', 'multivariable-calculus']"
14,Book Request: Taylor's Theorem for functions $f: \Bbb R^n \to \Bbb R^m$,Book Request: Taylor's Theorem for functions,f: \Bbb R^n \to \Bbb R^m,"I'm looking for a resource (e.g. a book, website, or arxiv paper) that goes over the general case of Taylor's theorem, with a full proof and examples.  Do you guys know of any material that covers this.","I'm looking for a resource (e.g. a book, website, or arxiv paper) that goes over the general case of Taylor's theorem, with a full proof and examples.  Do you guys know of any material that covers this.",,"['multivariable-calculus', 'reference-request', 'taylor-expansion']"
15,What happens when the determinant of the Jacobian is zero?,What happens when the determinant of the Jacobian is zero?,,"Let $f \colon \Bbb R^n \rightarrow \Bbb R^n$ be a continuously differentiable function. Let $f'(x)$ denote the Jacobian matrix of $f$ at the point $x \in \Bbb R^n$. I would like to show that if $f$ is one-to-one, then $\exists$ $a \in \Bbb R^n$ such that det$f'(a) \neq 0$. When $n=1$, this result is a consequence of the Mean Value Theorem. For larger $n$, I assume we will need some version of either the Inverse Function Theorem or the Implicit Function Theorem. Thank you for your thoughts!","Let $f \colon \Bbb R^n \rightarrow \Bbb R^n$ be a continuously differentiable function. Let $f'(x)$ denote the Jacobian matrix of $f$ at the point $x \in \Bbb R^n$. I would like to show that if $f$ is one-to-one, then $\exists$ $a \in \Bbb R^n$ such that det$f'(a) \neq 0$. When $n=1$, this result is a consequence of the Mean Value Theorem. For larger $n$, I assume we will need some version of either the Inverse Function Theorem or the Implicit Function Theorem. Thank you for your thoughts!",,"['calculus', 'real-analysis', 'multivariable-calculus']"
16,Lagrange multiplier for more than one constraints.,Lagrange multiplier for more than one constraints.,,"How to minimize $x^TAx$ over the set $D=(x\geq 0, x^TBx=1$ and $(I-A^\dagger A)x=0$), where $A$ is copositive matrix of order $n-1$ and $B$ is strictly copositive matrix of order $n$. If I drop the last constraint from set $D$ then using Lagrange multiplier I am able to minimize $x^TAx$ over set $D$, but if I have third constraint then how to proceed? Set $D$ can be rewritten as $D=( x\in \mathbb{R}^n, x^TBx=1, h_{i}(x)=0, 1\leq i\leq n$) where $h_{i}(x)=\langle (I-A^\dagger A)e^{i},x \rangle=0$","How to minimize $x^TAx$ over the set $D=(x\geq 0, x^TBx=1$ and $(I-A^\dagger A)x=0$), where $A$ is copositive matrix of order $n-1$ and $B$ is strictly copositive matrix of order $n$. If I drop the last constraint from set $D$ then using Lagrange multiplier I am able to minimize $x^TAx$ over set $D$, but if I have third constraint then how to proceed? Set $D$ can be rewritten as $D=( x\in \mathbb{R}^n, x^TBx=1, h_{i}(x)=0, 1\leq i\leq n$) where $h_{i}(x)=\langle (I-A^\dagger A)e^{i},x \rangle=0$",,"['linear-algebra', 'multivariable-calculus', 'numerical-linear-algebra', 'copositivity']"
17,Show that the innerproduct of two vector is a differentiable mapping,Show that the innerproduct of two vector is a differentiable mapping,,"The question is to show that the innerproduct is a differentiable mapping. Define $g: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ such that $ g(x_1,x_2) = <x_1|x_2>$. We use the productmetric on $\mathbb{R}^n \times \mathbb{R}^n$. Now we have to show that $g$ is differentiable in every point $(a_1,a_2) \in \mathbb{R}^n \times \mathbb{R}^n$ and that the total derivative in every point is given by: $ Dg(a_1,a_2)(h_1,h_2) = <a_1|h_2> + <h_1|a_2>, \, h_1,h_2 \in \mathbb{R}^n. $ The productmetric is defined as $d((x_1,x_2),(y_1,y_2)) = d(x_1,y_1) + d(x_2,y_2)$. And using the formula for differentiation: $ \lim_{h \to 0} \frac{ \|f(a+h) - f(a) - Df(a)(h) \|}{\|h\|} = 0$ Plugging in $g$ for $f$ I figured that $g(a_1+h_1,a_2+h_2) - g(a_1,a_2)=\,  <a_1+h_1 | a_2+h_2> - <a_1|a_2> = \, <a_1|h_2> + <h_1|a_2> + <h_1 | h_2>. $ This is the point where I'm stuck. How can I come from here to concluding $Dg(a_1,a_2)(h_1,h_2) = <a_1|h_2> + <h_1|a_2>$? And where do I use the productmetric?","The question is to show that the innerproduct is a differentiable mapping. Define $g: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ such that $ g(x_1,x_2) = <x_1|x_2>$. We use the productmetric on $\mathbb{R}^n \times \mathbb{R}^n$. Now we have to show that $g$ is differentiable in every point $(a_1,a_2) \in \mathbb{R}^n \times \mathbb{R}^n$ and that the total derivative in every point is given by: $ Dg(a_1,a_2)(h_1,h_2) = <a_1|h_2> + <h_1|a_2>, \, h_1,h_2 \in \mathbb{R}^n. $ The productmetric is defined as $d((x_1,x_2),(y_1,y_2)) = d(x_1,y_1) + d(x_2,y_2)$. And using the formula for differentiation: $ \lim_{h \to 0} \frac{ \|f(a+h) - f(a) - Df(a)(h) \|}{\|h\|} = 0$ Plugging in $g$ for $f$ I figured that $g(a_1+h_1,a_2+h_2) - g(a_1,a_2)=\,  <a_1+h_1 | a_2+h_2> - <a_1|a_2> = \, <a_1|h_2> + <h_1|a_2> + <h_1 | h_2>. $ This is the point where I'm stuck. How can I come from here to concluding $Dg(a_1,a_2)(h_1,h_2) = <a_1|h_2> + <h_1|a_2>$? And where do I use the productmetric?",,"['real-analysis', 'multivariable-calculus']"
18,"Two ways to evaluate $\int (\Delta u) v d\Omega$, two different results","Two ways to evaluate , two different results",\int (\Delta u) v d\Omega,"I would like to evaluate the integral $\int (\Delta  u) v d\Omega$, where the domain $\Omega$ is a cylinder. On the boundaries, either the normal derivative $\partial_n u$ is zero or $v$ is zero. An additional assumption is that $u$ and $v$ are axial symmetric, ie. $\frac{\partial u}{\partial \phi} = \frac{\partial v}{\partial \phi} = 0$. The two steps in the derivation below are partial integration of the second derivative, and using cylindrical coordinates for the nabla operator. However, the order seems to matter. Method 1 $\int (\Delta  u) v d\Omega$ Using Green's first identity this becomes $- \int_\Omega \nabla u \cdot \nabla v + \oint_{\partial \Omega} (\partial_n u)v$ where the second term disappears because of the boundary conditions   assumed. With the expression for the gradient in cylindrical   coordinates, $\nabla = \left(\frac{\partial}{\partial \rho} \boldsymbol{\hat{\rho}}, \frac1{\rho}\frac{\partial}{\partial \phi} \boldsymbol{\hat{\phi}}, \frac{\partial}{\partial z} \mathbf{\hat{z}}\right)$, and the dot product in the coordinates defined by the orthogonal (local) unit vectors $\boldsymbol{\hat{\rho}}, \boldsymbol{\hat{\phi}}, \boldsymbol{\hat{z}}$:   $(\rho_1, \phi_1, z_1) \cdot (\rho_2, \phi_2, z_2) = \rho_1 \rho_2 + \phi_1 \phi_2 + z_1 z_2$, the first term becomes: $-2 \pi \int \int d\rho dz \left[ \frac{\partial u}{\partial \rho} \frac{\partial v}{\partial \rho} + \frac1{\rho^2} \frac{\partial u}{\partial \phi}  \frac{\partial v}{\partial \phi} + \frac{\partial u}{\partial z} \frac{\partial u}{\partial z}\right] $ or due to axial symmetry: $-2 \pi \int \int d\rho dz \left[ \frac{\partial u}{\partial \rho} \frac{\partial v}{\partial \rho}  + \frac{\partial u}{\partial z} \frac{\partial u}{\partial z}\right]$ Or: Method 2 $\int (\Delta  u) v d\Omega$ With the Laplacian in cylindrical coordinates, $\Delta =  \frac1{\rho}\frac{\partial}{\partial \rho} \left(\rho  \frac{\partial}{\partial \rho}\right) + \frac1{\rho^2}  \frac{\partial^2}{\partial \phi^2} + \frac{\partial^2}{\partial z^2}$   this gives $2 \pi \int \int d\rho dz \left[ \frac1{\rho}\frac{\partial u}{\partial  \rho} + \frac{\partial^2 u}{\partial \rho^2} +  \frac1{\rho^2} \frac{\partial^2 u}{\partial \phi^2} + \frac{\partial^2  u}{\partial z^2}\right] v$ Applying now partial integration to the terms with a second derivative,   eg. $\int \int d\rho dz \frac{\partial^2 u}{\partial \rho^2}v = \int  dz \left. \frac{\partial u}{\partial \rho}v \right|_0^{\rho_{max}} -  \int \int d\rho dz \frac{\partial u}{\partial \rho} \frac{\partial  v}{\partial \rho} = - \int d\rho dz \frac{\partial u}{\partial \rho}  \frac{\partial v}{\partial \rho}$ where the last step is due to axial   symmetry and assumed boundary conditions. The resulting equation then   is $2 \pi \int \int d\rho dz \left[\frac1{\rho} \frac{\partial  u}{\partial \rho} v - \frac{\partial u}{\partial \rho} \frac{\partial  v}{\partial \rho}  - \frac{\partial u}{\partial z} \frac{\partial  u}{\partial z}\right]$ Where now there is an additional term compared to method 1, and I don't see what I did wrong?","I would like to evaluate the integral $\int (\Delta  u) v d\Omega$, where the domain $\Omega$ is a cylinder. On the boundaries, either the normal derivative $\partial_n u$ is zero or $v$ is zero. An additional assumption is that $u$ and $v$ are axial symmetric, ie. $\frac{\partial u}{\partial \phi} = \frac{\partial v}{\partial \phi} = 0$. The two steps in the derivation below are partial integration of the second derivative, and using cylindrical coordinates for the nabla operator. However, the order seems to matter. Method 1 $\int (\Delta  u) v d\Omega$ Using Green's first identity this becomes $- \int_\Omega \nabla u \cdot \nabla v + \oint_{\partial \Omega} (\partial_n u)v$ where the second term disappears because of the boundary conditions   assumed. With the expression for the gradient in cylindrical   coordinates, $\nabla = \left(\frac{\partial}{\partial \rho} \boldsymbol{\hat{\rho}}, \frac1{\rho}\frac{\partial}{\partial \phi} \boldsymbol{\hat{\phi}}, \frac{\partial}{\partial z} \mathbf{\hat{z}}\right)$, and the dot product in the coordinates defined by the orthogonal (local) unit vectors $\boldsymbol{\hat{\rho}}, \boldsymbol{\hat{\phi}}, \boldsymbol{\hat{z}}$:   $(\rho_1, \phi_1, z_1) \cdot (\rho_2, \phi_2, z_2) = \rho_1 \rho_2 + \phi_1 \phi_2 + z_1 z_2$, the first term becomes: $-2 \pi \int \int d\rho dz \left[ \frac{\partial u}{\partial \rho} \frac{\partial v}{\partial \rho} + \frac1{\rho^2} \frac{\partial u}{\partial \phi}  \frac{\partial v}{\partial \phi} + \frac{\partial u}{\partial z} \frac{\partial u}{\partial z}\right] $ or due to axial symmetry: $-2 \pi \int \int d\rho dz \left[ \frac{\partial u}{\partial \rho} \frac{\partial v}{\partial \rho}  + \frac{\partial u}{\partial z} \frac{\partial u}{\partial z}\right]$ Or: Method 2 $\int (\Delta  u) v d\Omega$ With the Laplacian in cylindrical coordinates, $\Delta =  \frac1{\rho}\frac{\partial}{\partial \rho} \left(\rho  \frac{\partial}{\partial \rho}\right) + \frac1{\rho^2}  \frac{\partial^2}{\partial \phi^2} + \frac{\partial^2}{\partial z^2}$   this gives $2 \pi \int \int d\rho dz \left[ \frac1{\rho}\frac{\partial u}{\partial  \rho} + \frac{\partial^2 u}{\partial \rho^2} +  \frac1{\rho^2} \frac{\partial^2 u}{\partial \phi^2} + \frac{\partial^2  u}{\partial z^2}\right] v$ Applying now partial integration to the terms with a second derivative,   eg. $\int \int d\rho dz \frac{\partial^2 u}{\partial \rho^2}v = \int  dz \left. \frac{\partial u}{\partial \rho}v \right|_0^{\rho_{max}} -  \int \int d\rho dz \frac{\partial u}{\partial \rho} \frac{\partial  v}{\partial \rho} = - \int d\rho dz \frac{\partial u}{\partial \rho}  \frac{\partial v}{\partial \rho}$ where the last step is due to axial   symmetry and assumed boundary conditions. The resulting equation then   is $2 \pi \int \int d\rho dz \left[\frac1{\rho} \frac{\partial  u}{\partial \rho} v - \frac{\partial u}{\partial \rho} \frac{\partial  v}{\partial \rho}  - \frac{\partial u}{\partial z} \frac{\partial  u}{\partial z}\right]$ Where now there is an additional term compared to method 1, and I don't see what I did wrong?",,"['multivariable-calculus', 'polar-coordinates']"
19,Finding the equation of a one sheeted hyperboloid,Finding the equation of a one sheeted hyperboloid,,"Was working on calc 3 homework assignment and couldn't find out how to solve this question. the answer doesn't matter any more,i just really want to find out how to do it since my book seems to skip over it. the problem goes like this. ""A cooling tower for a nuclear reactor is to be constructed in the shape of a hyperboloid of one sheet. The diameter at the base is 280 m and the minimum diameter, 500 m above the base, is 220 m. Find an equation for the tower. (Assume the center is at the origin with axis the z-axis and the minimum diameter is at the center.)"" I know the equation of a hyperboloid is $\frac{x^2}{a^2} +\frac {y^2}{b^2} - \frac{z^2}{c^2} = 1$ but i don't know how to find this with the parameters given. thanks.","Was working on calc 3 homework assignment and couldn't find out how to solve this question. the answer doesn't matter any more,i just really want to find out how to do it since my book seems to skip over it. the problem goes like this. ""A cooling tower for a nuclear reactor is to be constructed in the shape of a hyperboloid of one sheet. The diameter at the base is 280 m and the minimum diameter, 500 m above the base, is 220 m. Find an equation for the tower. (Assume the center is at the origin with axis the z-axis and the minimum diameter is at the center.)"" I know the equation of a hyperboloid is $\frac{x^2}{a^2} +\frac {y^2}{b^2} - \frac{z^2}{c^2} = 1$ but i don't know how to find this with the parameters given. thanks.",,['multivariable-calculus']
20,Rewriting a continuously differentiable function,Rewriting a continuously differentiable function,,"I have the following $i$-th regressor function: $\phi_i(x)$ in which $x$ is a vector with elements $x_1, \ldots, x_n$. I cite from an article: Let $e_i = \hat{x}_i - x_i$ and note that, since $\phi_i(\cdot)$ is continuously differentiable (an earlier made assumption) , we can write: $\phi_i(x)  = \phi_i(\hat{x}_1,\ldots,\hat{x}_{i-1},x_i,\hat{x}_{i+1},\ldots,\hat{x}_n) + \sum_{j=1}^{n} e_j\delta_{ij}(x,e), \qquad   eq. (1)$ for some functions $\delta_{ij}(\cdot)$, with $\delta_{ii}(x,e) = 0$. So a simple example, assume we have: $\phi_2(x) = x_1x_2$, then we get: $x_1x_2 = \hat{x}_1x_2 + e_1\delta_{21}(x,e)$ which can be written as: $x_1x_2 = (x_1+e_1)x_2 + e_1\delta_{21}(x,e) $ So we simply find: $\delta_{21}(x,e)= -x_2$ Now I am wondering, why do we require $\phi_i(x)$ to be continuously differentiable in order for eq. (1) to be true in general?","I have the following $i$-th regressor function: $\phi_i(x)$ in which $x$ is a vector with elements $x_1, \ldots, x_n$. I cite from an article: Let $e_i = \hat{x}_i - x_i$ and note that, since $\phi_i(\cdot)$ is continuously differentiable (an earlier made assumption) , we can write: $\phi_i(x)  = \phi_i(\hat{x}_1,\ldots,\hat{x}_{i-1},x_i,\hat{x}_{i+1},\ldots,\hat{x}_n) + \sum_{j=1}^{n} e_j\delta_{ij}(x,e), \qquad   eq. (1)$ for some functions $\delta_{ij}(\cdot)$, with $\delta_{ii}(x,e) = 0$. So a simple example, assume we have: $\phi_2(x) = x_1x_2$, then we get: $x_1x_2 = \hat{x}_1x_2 + e_1\delta_{21}(x,e)$ which can be written as: $x_1x_2 = (x_1+e_1)x_2 + e_1\delta_{21}(x,e) $ So we simply find: $\delta_{21}(x,e)= -x_2$ Now I am wondering, why do we require $\phi_i(x)$ to be continuously differentiable in order for eq. (1) to be true in general?",,['multivariable-calculus']
21,"Multiple choice question on $f:A\to\mathbb{R}^2, f(x,y)=({x\over 1+x+y},{y\over 1+x+y})$",Multiple choice question on,"f:A\to\mathbb{R}^2, f(x,y)=({x\over 1+x+y},{y\over 1+x+y})","$A=\{(x,y)\in\mathbb{R}^2: x+y\neq -1\}$ $f:A\to\mathbb{R}^2, f(x,y)=({x\over 1+x+y},{y\over 1+x+y})$,Then Jacobian matrix of $f$ does not vanish on $A$ $f$ is infinitely differentiable on $A$ $f$ is injective on $A$ $f(A)=\mathbb{R}^2$ I have calculated $J={(x+y+2)(y-x)\over (x+y+1)^2}$, It will vanish only when $y=x$ or $x+y=-2$ but clearly they are in $A^c$, so $1$ is true. Yes, $f$ is infinitely differentiable  which I understand from the expression but which result I  have to use to  conclude that or how? $f$ is injective  as Non vanishing Jacobian $f$ is not surjective as $({1\over 2},1)$ has no pre-image in $A$ Shall be glad to have reply","$A=\{(x,y)\in\mathbb{R}^2: x+y\neq -1\}$ $f:A\to\mathbb{R}^2, f(x,y)=({x\over 1+x+y},{y\over 1+x+y})$,Then Jacobian matrix of $f$ does not vanish on $A$ $f$ is infinitely differentiable on $A$ $f$ is injective on $A$ $f(A)=\mathbb{R}^2$ I have calculated $J={(x+y+2)(y-x)\over (x+y+1)^2}$, It will vanish only when $y=x$ or $x+y=-2$ but clearly they are in $A^c$, so $1$ is true. Yes, $f$ is infinitely differentiable  which I understand from the expression but which result I  have to use to  conclude that or how? $f$ is injective  as Non vanishing Jacobian $f$ is not surjective as $({1\over 2},1)$ has no pre-image in $A$ Shall be glad to have reply",,['multivariable-calculus']
22,Converting a polar integral to spherical,Converting a polar integral to spherical,,"$$\int_0^{2\pi} \int_0^{\sqrt{2}}\int_r^{\sqrt{4-r^2}}\mathrm{d}z \, r \, \mathrm{d}r \, \mathrm{d}\theta$$ So in spherical this would become: $$\int_0^{2\pi} \int_0^{\pi/4}\int_0^2 \rho^2\sin\phi \, \mathrm{d} \rho \, \mathrm{d}\phi \, \mathrm{d}\theta,$$ correct?","$$\int_0^{2\pi} \int_0^{\sqrt{2}}\int_r^{\sqrt{4-r^2}}\mathrm{d}z \, r \, \mathrm{d}r \, \mathrm{d}\theta$$ So in spherical this would become: $$\int_0^{2\pi} \int_0^{\pi/4}\int_0^2 \rho^2\sin\phi \, \mathrm{d} \rho \, \mathrm{d}\phi \, \mathrm{d}\theta,$$ correct?",,"['multivariable-calculus', 'spherical-coordinates']"
23,Sketching a surface,Sketching a surface,,"If $${\bf F}=2y{\bf i}-z{\bf j}+x^2{\bf k},$$ and $s$ is the surface of the parabolic cylinder $y^2=8x$ in the first octant, bounded by the planes $y=4$ and $z=6$, evaluate $$\int_S{\bf F}\cdot{\bf\hat n}\,dS,$$ where $\bf\hat n$ points in the direction of increasing $x$, by projecting the integral onto the plane $x=0$. I'm trying to draw a sketch to get a feel of the situation but am confused as to what the question is asking. I have sketched $y^2=8x$ in the plane $z=0$ and marked on the points where $y$ and $z$ are bounded.","If $${\bf F}=2y{\bf i}-z{\bf j}+x^2{\bf k},$$ and $s$ is the surface of the parabolic cylinder $y^2=8x$ in the first octant, bounded by the planes $y=4$ and $z=6$, evaluate $$\int_S{\bf F}\cdot{\bf\hat n}\,dS,$$ where $\bf\hat n$ points in the direction of increasing $x$, by projecting the integral onto the plane $x=0$. I'm trying to draw a sketch to get a feel of the situation but am confused as to what the question is asking. I have sketched $y^2=8x$ in the plane $z=0$ and marked on the points where $y$ and $z$ are bounded.",,['multivariable-calculus']
24,Line integrals and path independence,Line integrals and path independence,,"Consider $\textbf{F}(x,y)=\frac{-y}{x^2+y^2}\textbf{i}+\frac{x}{x^2+y^2}\textbf{j}$. Let $C_1$ be the upper half of the unit circle oriented counterclockwise, and let $C_2$ be the lower half of the unit circle oriented clockwise. Compute $\int_{C_1}\textbf{F} \dot \ d\textbf{r}$ and $\int_{C_2}\textbf{F} \dot \ d\textbf{r}$. $C_1:x^2+y^2=1, x=\cos(\theta), y=\sin(\theta), 0 \leq \theta \leq \pi$ $dx = -\sin(\theta), dy = \cos(\theta)$ $\int_{C_1}\textbf{F} \dot \ d\textbf{r} = \int_0^{\pi} \langle \frac{-\sin(\theta)}{\sin^2(\theta)+\cos^2(\theta)}, \frac{\cos(\theta)}{\sin^2(\theta)+\cos^2(\theta)} \rangle \langle -\sin(\theta), \cos(\theta) \rangle \ d\theta$ $=\int_0^{\pi} \sin^2(\theta)+\cos^2(\theta) \ d\theta$ $= \left. \theta \right|_{0}^{\pi}$ $= \pi.$ $C_2: x=\cos(\theta), y=-\sin(\theta), 0 \leq \theta \leq \pi$ $dx=-\sin(\theta), dy=-\cos(\theta)$ $\int_{C_2}\textbf{F} \dot \ d\textbf{r} = \int_0^{\pi} \langle \sin(\theta), \cos(\theta) \rangle \langle -\sin(\theta), -\cos(\theta) \rangle \ d\theta$ $=\int_0^{\pi} -\sin^2(\theta)-\cos^2(\theta) \ d\theta$ $=\left. -\theta \right|_0^{\pi}$ $=-\pi$ Is $\int_C\textbf{F} \dot \ d\textbf{r}$ always independent of path? Why or why not? No, $\int_C\textbf{F} \dot \ d\textbf{r}$ is not always independent of path. Because the vector field is undefined at (0,0), it is not conservative. Therefore, it depends on the path taken. Could someone please tell me if I computed the line integrals correctly? Also, I'm not sure about my last answer about path independence. Is this correct? Thank you.","Consider $\textbf{F}(x,y)=\frac{-y}{x^2+y^2}\textbf{i}+\frac{x}{x^2+y^2}\textbf{j}$. Let $C_1$ be the upper half of the unit circle oriented counterclockwise, and let $C_2$ be the lower half of the unit circle oriented clockwise. Compute $\int_{C_1}\textbf{F} \dot \ d\textbf{r}$ and $\int_{C_2}\textbf{F} \dot \ d\textbf{r}$. $C_1:x^2+y^2=1, x=\cos(\theta), y=\sin(\theta), 0 \leq \theta \leq \pi$ $dx = -\sin(\theta), dy = \cos(\theta)$ $\int_{C_1}\textbf{F} \dot \ d\textbf{r} = \int_0^{\pi} \langle \frac{-\sin(\theta)}{\sin^2(\theta)+\cos^2(\theta)}, \frac{\cos(\theta)}{\sin^2(\theta)+\cos^2(\theta)} \rangle \langle -\sin(\theta), \cos(\theta) \rangle \ d\theta$ $=\int_0^{\pi} \sin^2(\theta)+\cos^2(\theta) \ d\theta$ $= \left. \theta \right|_{0}^{\pi}$ $= \pi.$ $C_2: x=\cos(\theta), y=-\sin(\theta), 0 \leq \theta \leq \pi$ $dx=-\sin(\theta), dy=-\cos(\theta)$ $\int_{C_2}\textbf{F} \dot \ d\textbf{r} = \int_0^{\pi} \langle \sin(\theta), \cos(\theta) \rangle \langle -\sin(\theta), -\cos(\theta) \rangle \ d\theta$ $=\int_0^{\pi} -\sin^2(\theta)-\cos^2(\theta) \ d\theta$ $=\left. -\theta \right|_0^{\pi}$ $=-\pi$ Is $\int_C\textbf{F} \dot \ d\textbf{r}$ always independent of path? Why or why not? No, $\int_C\textbf{F} \dot \ d\textbf{r}$ is not always independent of path. Because the vector field is undefined at (0,0), it is not conservative. Therefore, it depends on the path taken. Could someone please tell me if I computed the line integrals correctly? Also, I'm not sure about my last answer about path independence. Is this correct? Thank you.",,['multivariable-calculus']
25,Line integral parametrization,Line integral parametrization,,"We are given the field $\textbf{F}(x,y)=(x-y)\textbf{i}+xy\textbf{j}$ and C being $\frac{3}{4}$ of a circle of radius $2$ centered at the origin traversed from $(2,0)$ to $(0,-2)$. $$\textbf{F}(x,y) =(x-y)\textbf {i}+(xy)\textbf{j}$$ $$C: x^2+y^2=4, x=2\cos(\theta), y=2\sin(\theta), 0 \leq \theta \leq \frac{3\pi}{2}$$ $$dx= -2\sin(\theta), dy=2\cos(\theta)$$ $$\begin{align}\int\limits_C \textbf{F} \dot \ d\textbf{r} & = \int_0^{\frac{3\pi}{2}} \langle 2\cos(\theta)-2\sin(\theta), (4\cos(\theta)\sin(\theta)\rangle  \langle -2\sin(\theta), 2\cos(\theta) \rangle \ d\theta \\ & =\int_0^{\frac{3\pi}{2}} 4\cos(\theta)\sin(\theta)+4\sin^2(\theta)+8\cos^2(\theta)\sin(\theta) \ d\theta \\ & =\frac{2}{3}+3\pi\end{align}$$ Did I set up and compute this integral correctly? Thank you.","We are given the field $\textbf{F}(x,y)=(x-y)\textbf{i}+xy\textbf{j}$ and C being $\frac{3}{4}$ of a circle of radius $2$ centered at the origin traversed from $(2,0)$ to $(0,-2)$. $$\textbf{F}(x,y) =(x-y)\textbf {i}+(xy)\textbf{j}$$ $$C: x^2+y^2=4, x=2\cos(\theta), y=2\sin(\theta), 0 \leq \theta \leq \frac{3\pi}{2}$$ $$dx= -2\sin(\theta), dy=2\cos(\theta)$$ $$\begin{align}\int\limits_C \textbf{F} \dot \ d\textbf{r} & = \int_0^{\frac{3\pi}{2}} \langle 2\cos(\theta)-2\sin(\theta), (4\cos(\theta)\sin(\theta)\rangle  \langle -2\sin(\theta), 2\cos(\theta) \rangle \ d\theta \\ & =\int_0^{\frac{3\pi}{2}} 4\cos(\theta)\sin(\theta)+4\sin^2(\theta)+8\cos^2(\theta)\sin(\theta) \ d\theta \\ & =\frac{2}{3}+3\pi\end{align}$$ Did I set up and compute this integral correctly? Thank you.",,['multivariable-calculus']
26,Simplifying a Vector Integral,Simplifying a Vector Integral,,"While reading the book - Theory and Applications of Boltzmann Transport Equation by Cercignani (I am not a math student), I found this integral which I am unable to understand. Note that $\xi_i , \xi_l$ and $x_i , x_l$ are vectors. The first term in the simplification comes from the fact that $x_i$ is not equal to $x_l$, and we are differentiating $P_N$ wrt $x_i$ (over which we do not integrate) ; so the differentiation can be taken out of the integral. However the domain of integration has boundaries $|x_i-x_l|= \sigma $ which depend upon $x_i$, so there is some second term. This is the image: My question is: where does the second term come from? This book is mathematically very involved (for e.g. it uses at some places, volume and surface area of $n>3$ dimensional spheres) and its seems that I need to have some background/prerequisite for reading this book. I'll be very grateful if someone can suggest a maths book as prerequisite based on what I told you and which will help me to understand the integration.","While reading the book - Theory and Applications of Boltzmann Transport Equation by Cercignani (I am not a math student), I found this integral which I am unable to understand. Note that $\xi_i , \xi_l$ and $x_i , x_l$ are vectors. The first term in the simplification comes from the fact that $x_i$ is not equal to $x_l$, and we are differentiating $P_N$ wrt $x_i$ (over which we do not integrate) ; so the differentiation can be taken out of the integral. However the domain of integration has boundaries $|x_i-x_l|= \sigma $ which depend upon $x_i$, so there is some second term. This is the image: My question is: where does the second term come from? This book is mathematically very involved (for e.g. it uses at some places, volume and surface area of $n>3$ dimensional spheres) and its seems that I need to have some background/prerequisite for reading this book. I'll be very grateful if someone can suggest a maths book as prerequisite based on what I told you and which will help me to understand the integration.",,"['calculus', 'integration', 'multivariable-calculus', 'reference-request', 'statistical-mechanics']"
27,Prove Green's theorem for circles,Prove Green's theorem for circles,,"So the problem is in the title. The rules are that I can't split the circle into ""rectangles"" and I can't use pull-back. I tried to do something similar to the proof on unit squares. The problem is that to prove Green's theorem for unit squares you don't need to use any kind of coordinate transformation so when I try to use the same principle for circles I fail because I don't know how to proceed. So, here's what I've got so far $\omega=Pdx+Qdy, \space\space\gamma(\phi)=(s_x+R\cos\phi, s_y+R\sin\phi),\space\space\phi\in[0, 2\pi]$ The circle has the radius $R$ and is centered at $(s_x, s_y)$. Using this parametrization, $dx=-R\sin\phi,\space\space dy=R\cos\phi$ Now, the first integral in Green's theorem is the following $\int_\gamma\omega=\int_0^{2\pi}P(\gamma(\phi))(-R\sin\phi)+\int_0^{2\pi}Q(\gamma(\phi))(R\cos\phi)$ As far as I know, I can't calculate anything further than that without knowing $P$ and $Q$. So, the second integral is supposed to be something like this $\int_Dd\omega=\int\int_D(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y})dxdy$ I have no idea where to go next. Since the parametrization for the interior of the circle $D$ would be something like $g(r, \phi)=(s_x+r\cos\phi, s_y+r\sin\phi)$, I don't know what $dxdy$ means. Also, I'm pretty sure I can't just write $\frac{\partial Q}{\partial x}$ and be fine with it since I'm not using $x,y$. I'm pretty sure most of what I've wrote doesn't make sense but I'm still very fuzzy on the whole ""differential form"" thing. If someone could help me out, I'd appreciate it.","So the problem is in the title. The rules are that I can't split the circle into ""rectangles"" and I can't use pull-back. I tried to do something similar to the proof on unit squares. The problem is that to prove Green's theorem for unit squares you don't need to use any kind of coordinate transformation so when I try to use the same principle for circles I fail because I don't know how to proceed. So, here's what I've got so far $\omega=Pdx+Qdy, \space\space\gamma(\phi)=(s_x+R\cos\phi, s_y+R\sin\phi),\space\space\phi\in[0, 2\pi]$ The circle has the radius $R$ and is centered at $(s_x, s_y)$. Using this parametrization, $dx=-R\sin\phi,\space\space dy=R\cos\phi$ Now, the first integral in Green's theorem is the following $\int_\gamma\omega=\int_0^{2\pi}P(\gamma(\phi))(-R\sin\phi)+\int_0^{2\pi}Q(\gamma(\phi))(R\cos\phi)$ As far as I know, I can't calculate anything further than that without knowing $P$ and $Q$. So, the second integral is supposed to be something like this $\int_Dd\omega=\int\int_D(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y})dxdy$ I have no idea where to go next. Since the parametrization for the interior of the circle $D$ would be something like $g(r, \phi)=(s_x+r\cos\phi, s_y+r\sin\phi)$, I don't know what $dxdy$ means. Also, I'm pretty sure I can't just write $\frac{\partial Q}{\partial x}$ and be fine with it since I'm not using $x,y$. I'm pretty sure most of what I've wrote doesn't make sense but I'm still very fuzzy on the whole ""differential form"" thing. If someone could help me out, I'd appreciate it.",,"['integration', 'multivariable-calculus', 'differential-forms']"
28,Using Stokes's theorem to calculate a value of integral,Using Stokes's theorem to calculate a value of integral,,"Use Stokes's theorem to calculate the integral $$I= \int_\Gamma (x^2+2y)dx+(y+z)dy+(z^2+x^2)dz$$ where $\Gamma$ is the boundary of $$\gamma=\left\{ (x,y,z):3x+y+3z=3,x\ge0,y\ge0,z\ge0\right\} $$ Using Stokes's theorem I got $$\int_\Gamma (x^2+2y)dx+(y+z)dy+(z^2+x^2)dz=\int _\gamma -dy\wedge dz-2xdz\wedge dx-2dx\wedge dy$$but I have problem finding parameric representation for $\gamma$ since I don't know the limits of $(x,y,z)$ (I know that the parametric representation for $\gamma $ will be $(u,v,w)\mapsto(u,v,1-u-\frac{v}{3})$ but what are the limits of $u,v,w$  that's a mystery for me). EDIT: To achieve that I used the version for $F=(P,Q,R)$ of Green theorem, i.e $$\int_{\partial\Omega} Pdx+Qdy+Rdz=\iint_{\Omega}\left(\frac{\partial R}{\partial y}-\frac{\partial Q}{\partial z}\right)dy\wedge dz-\left(\frac{\partial P}{\partial z}-\frac{\partial R}{\partial x}\right)dz\wedge dx+\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right)dx\wedge dy.$$ In my case $$\int_\Gamma  Pdx+Qdy+Rdz=\int_\gamma (0-1)dydz-(0-2x)dzdx+(0-2)dxdy.$$ What am I doing wrong?","Use Stokes's theorem to calculate the integral $$I= \int_\Gamma (x^2+2y)dx+(y+z)dy+(z^2+x^2)dz$$ where $\Gamma$ is the boundary of $$\gamma=\left\{ (x,y,z):3x+y+3z=3,x\ge0,y\ge0,z\ge0\right\} $$ Using Stokes's theorem I got $$\int_\Gamma (x^2+2y)dx+(y+z)dy+(z^2+x^2)dz=\int _\gamma -dy\wedge dz-2xdz\wedge dx-2dx\wedge dy$$but I have problem finding parameric representation for $\gamma$ since I don't know the limits of $(x,y,z)$ (I know that the parametric representation for $\gamma $ will be $(u,v,w)\mapsto(u,v,1-u-\frac{v}{3})$ but what are the limits of $u,v,w$  that's a mystery for me). EDIT: To achieve that I used the version for $F=(P,Q,R)$ of Green theorem, i.e $$\int_{\partial\Omega} Pdx+Qdy+Rdz=\iint_{\Omega}\left(\frac{\partial R}{\partial y}-\frac{\partial Q}{\partial z}\right)dy\wedge dz-\left(\frac{\partial P}{\partial z}-\frac{\partial R}{\partial x}\right)dz\wedge dx+\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right)dx\wedge dy.$$ In my case $$\int_\Gamma  Pdx+Qdy+Rdz=\int_\gamma (0-1)dydz-(0-2x)dzdx+(0-2)dxdy.$$ What am I doing wrong?",,"['real-analysis', 'integration']"
29,Evaluating a triple integral explained step by step,Evaluating a triple integral explained step by step,,"Evaluate: $$ \iiint_{D}\sqrt{(1-9z^2)(1-4y^2-9z^2)}\,dx\,dy\,dz$$ where $D$ is the domain: $$D: x^2 +4y^2+9z^2\le1$$ Can someone tell me if my steps are correct? $$\int_{\frac{-1}{3}}^{\frac{1}{3}} \, dz\iint_{\{ \frac{x^2}{1-9z^2}+\frac{y^2}{\frac{1}{4}(1-9z^2)} \le1 \}}\sqrt{(1-9z^2)(1-4y^2-9z^2)} \,dx\,dy $$ The last double integral is composed by the following integrals: $$\int_{-\sqrt{1-9z^2}}^{+\sqrt{1-9z^2}} \, dx \int_{-\frac{1}{2}\sqrt{1-9z^2}}^{+\frac{1}{2}\sqrt{1-9z^2}}\,\sqrt{(1-9z^2)(1-4y^2-9z^2)} dy$$ Since both integral don't depend on $z$ I can write:$$ \iiint_{D}f\,dx\,dy\,dz=2\int_{\frac{-1}{3}}^{\frac{1}{3}}\sqrt{1-9z^2}\sqrt{1-9z^2}\,dz \int_{-\frac{1}{2}\sqrt{1-9z^2}}^{+\frac{1}{2}\sqrt{1-9z^2}}\sqrt{1-4y^2-9z^2}\,dy$$ Here I've already done the first integral which depends on $x$. So we have to calculate:$$\int_{\frac{-1}{2}a}^{\frac{1}{2}a} \sqrt{a^2-4y^2}\, dy$$ where $a^2=1-9z^2$; we can consider $a$ as a constant. Now let $y=\frac{1}{2}a\sin(t)$, we have: $$dy=\frac{1}{2}a\cos(t)dt$$ The last integral I wrote turns into:$$\frac{1}{2}a^2\int_{\frac{-\pi}{2}}^{\frac{+\pi}{2}}\left | \cos(t) \right |\cos(t)dt$$ which can be written as:$$a^2\int_{0}^{\frac{\pi}{2}}\cos^2(t)dt=a^2\left [ \frac{t}{2} +\frac{\sin(t)\cos(t)}{2}\right ]\begin{matrix} \frac{\pi}{2} \\0\end{matrix}=\frac{\pi}{4}a^2$$ Finally:$$ \iiint_{D}f\,dx\,dy\,dz=\frac{\pi}{2}\int_{\frac{-1}{3}}^{\frac{1}{3}}(1-9z^2)^2\,dz$$ In my schoolbook the solution is $\frac{64}{135}$...so where is the mistake in my method?","Evaluate: $$ \iiint_{D}\sqrt{(1-9z^2)(1-4y^2-9z^2)}\,dx\,dy\,dz$$ where $D$ is the domain: $$D: x^2 +4y^2+9z^2\le1$$ Can someone tell me if my steps are correct? $$\int_{\frac{-1}{3}}^{\frac{1}{3}} \, dz\iint_{\{ \frac{x^2}{1-9z^2}+\frac{y^2}{\frac{1}{4}(1-9z^2)} \le1 \}}\sqrt{(1-9z^2)(1-4y^2-9z^2)} \,dx\,dy $$ The last double integral is composed by the following integrals: $$\int_{-\sqrt{1-9z^2}}^{+\sqrt{1-9z^2}} \, dx \int_{-\frac{1}{2}\sqrt{1-9z^2}}^{+\frac{1}{2}\sqrt{1-9z^2}}\,\sqrt{(1-9z^2)(1-4y^2-9z^2)} dy$$ Since both integral don't depend on $z$ I can write:$$ \iiint_{D}f\,dx\,dy\,dz=2\int_{\frac{-1}{3}}^{\frac{1}{3}}\sqrt{1-9z^2}\sqrt{1-9z^2}\,dz \int_{-\frac{1}{2}\sqrt{1-9z^2}}^{+\frac{1}{2}\sqrt{1-9z^2}}\sqrt{1-4y^2-9z^2}\,dy$$ Here I've already done the first integral which depends on $x$. So we have to calculate:$$\int_{\frac{-1}{2}a}^{\frac{1}{2}a} \sqrt{a^2-4y^2}\, dy$$ where $a^2=1-9z^2$; we can consider $a$ as a constant. Now let $y=\frac{1}{2}a\sin(t)$, we have: $$dy=\frac{1}{2}a\cos(t)dt$$ The last integral I wrote turns into:$$\frac{1}{2}a^2\int_{\frac{-\pi}{2}}^{\frac{+\pi}{2}}\left | \cos(t) \right |\cos(t)dt$$ which can be written as:$$a^2\int_{0}^{\frac{\pi}{2}}\cos^2(t)dt=a^2\left [ \frac{t}{2} +\frac{\sin(t)\cos(t)}{2}\right ]\begin{matrix} \frac{\pi}{2} \\0\end{matrix}=\frac{\pi}{4}a^2$$ Finally:$$ \iiint_{D}f\,dx\,dy\,dz=\frac{\pi}{2}\int_{\frac{-1}{3}}^{\frac{1}{3}}(1-9z^2)^2\,dz$$ In my schoolbook the solution is $\frac{64}{135}$...so where is the mistake in my method?",,"['real-analysis', 'integration', 'multivariable-calculus']"
30,Brouwer's fixed theorem using Stokes' theorem,Brouwer's fixed theorem using Stokes' theorem,,"according to Wikipedia, there is a simple way to prove Brouwer's fixed point theorem from Stokes' theorem: see here . So I would like to present the former famous theorem (Brouwer's one) to my Calculus students using the latter famous one (Stokes'theorem)! However, the link uses the formalism of manifolds and differential forms, material that my students don't know. Is there a way to rewrite the proof using only basic Calculus material (basically, everything in Stewart's Calculus is fine). Sorry for the vagueness of the question. What students know: Line integrals of functions/  vector fields in space, Green's theorem (tangential and normal forms), surface integrals of functions/vector fields, Stokes' theorem in space (the curl version). A proof using the Divergence Theorem instead of Stokes'theorem would be acceptable and very welcomed!","according to Wikipedia, there is a simple way to prove Brouwer's fixed point theorem from Stokes' theorem: see here . So I would like to present the former famous theorem (Brouwer's one) to my Calculus students using the latter famous one (Stokes'theorem)! However, the link uses the formalism of manifolds and differential forms, material that my students don't know. Is there a way to rewrite the proof using only basic Calculus material (basically, everything in Stewart's Calculus is fine). Sorry for the vagueness of the question. What students know: Line integrals of functions/  vector fields in space, Green's theorem (tangential and normal forms), surface integrals of functions/vector fields, Stokes' theorem in space (the curl version). A proof using the Divergence Theorem instead of Stokes'theorem would be acceptable and very welcomed!",,['multivariable-calculus']
31,curve integral - intersection between plane and sphere,curve integral - intersection between plane and sphere,,"I am going to calculate the line integral $$ \int_\gamma z^4dx+x^2dy+y^8dz,$$ where $\gamma$ is the intersection  of the plane $y+z=1$ with the sphere $x^2+y^2+z^2=1$, $x \geq 0$, with the orientation given by increasing  $y$. Since $\gamma$ is an intersection curve, I decided to use Stoke's theorem,  applied to  the vector field $(z^4,x^2,y^8)$ and an oriented surface $Y$ with  boundary $\gamma$. But how am I going to parametrize the surface so I can use it with Stoke's thoerem? If I parametrize the surface by $(x(s,t),y(s,t),z(s,t))=(0,t,1-t),$ $x^2+y^2+z^2\leq 1$, I will get the normal vector $(0,0,0)$, but the normal vector is going to point upwards, I think.","I am going to calculate the line integral $$ \int_\gamma z^4dx+x^2dy+y^8dz,$$ where $\gamma$ is the intersection  of the plane $y+z=1$ with the sphere $x^2+y^2+z^2=1$, $x \geq 0$, with the orientation given by increasing  $y$. Since $\gamma$ is an intersection curve, I decided to use Stoke's theorem,  applied to  the vector field $(z^4,x^2,y^8)$ and an oriented surface $Y$ with  boundary $\gamma$. But how am I going to parametrize the surface so I can use it with Stoke's thoerem? If I parametrize the surface by $(x(s,t),y(s,t),z(s,t))=(0,t,1-t),$ $x^2+y^2+z^2\leq 1$, I will get the normal vector $(0,0,0)$, but the normal vector is going to point upwards, I think.",,"['real-analysis', 'integration', 'multivariable-calculus', 'differential-geometry', 'vector-analysis']"
32,Solving a system of integral-partial differential equations,Solving a system of integral-partial differential equations,,"Hi I am a student in electrical engineering. Currently I am facing a difficult problem solving a coupled integral-differential partial equations arising from mean field game. The problem is similar as in http://mfglabs.com/wp-content/uploads/2012/12/cfe.pdf The Bellman cost function is defined as $u(t,R) = \max_{p(s)\geq0,s\geq t} E\left[\int_t^{\infty}-\left[p(s)g_1 - g_2(I(s)+N_0)\right]^2e^{-r(s-t)}ds\right]$ where $g_1,g_2,N_0 > 0$ are constant and $R \geq 0$ $dR = -pdt + \lambda dW$ where $W$ is a Wiener process $I(t) =\frac{d}{dt}\int Rm(t,R)dR $ where $m(t,R)$ is the probability distribution of $R$ at time $t$ For mean field game, I assume $I(t)$ and $p(t)$ are independent. I applied the HJB equation and the Fokker Planck equation to get a system of partial equations as : $\partial_tu(t,R) +\lambda^2\partial^2_{RR}u(t,R) -ru(t,R)+ \max_{p\geq 0}\{-(pg_1 - g_2(I+N_0))^2-p\partial_{R} u(t,R)\}= 0 $ $\partial_t m(t,R) - \partial_R [p(t,R)m(t,R)] = \frac{\lambda^2}{2} \partial^2_{RR}[m(t,R)] $ $\int m(t,R)dR = 1 $ $I$ is calculated as above. The initial condition is known as $m(0,R) = f(R)$ where $f$ are known distribution Solving the Hamiltonian I can get the optimal control $p$. However when I replace $p$ back into the HJB this becomes a system of integral-partial differential equations that I can not solve. The paper suggests using eductive method to find the answer however using Google search returns no result. I want to ask if someone know any directions or (Matlab) methods to approximately solve these equations in order to find $m$ and $u$ ? Thank you very much.","Hi I am a student in electrical engineering. Currently I am facing a difficult problem solving a coupled integral-differential partial equations arising from mean field game. The problem is similar as in http://mfglabs.com/wp-content/uploads/2012/12/cfe.pdf The Bellman cost function is defined as $u(t,R) = \max_{p(s)\geq0,s\geq t} E\left[\int_t^{\infty}-\left[p(s)g_1 - g_2(I(s)+N_0)\right]^2e^{-r(s-t)}ds\right]$ where $g_1,g_2,N_0 > 0$ are constant and $R \geq 0$ $dR = -pdt + \lambda dW$ where $W$ is a Wiener process $I(t) =\frac{d}{dt}\int Rm(t,R)dR $ where $m(t,R)$ is the probability distribution of $R$ at time $t$ For mean field game, I assume $I(t)$ and $p(t)$ are independent. I applied the HJB equation and the Fokker Planck equation to get a system of partial equations as : $\partial_tu(t,R) +\lambda^2\partial^2_{RR}u(t,R) -ru(t,R)+ \max_{p\geq 0}\{-(pg_1 - g_2(I+N_0))^2-p\partial_{R} u(t,R)\}= 0 $ $\partial_t m(t,R) - \partial_R [p(t,R)m(t,R)] = \frac{\lambda^2}{2} \partial^2_{RR}[m(t,R)] $ $\int m(t,R)dR = 1 $ $I$ is calculated as above. The initial condition is known as $m(0,R) = f(R)$ where $f$ are known distribution Solving the Hamiltonian I can get the optimal control $p$. However when I replace $p$ back into the HJB this becomes a system of integral-partial differential equations that I can not solve. The paper suggests using eductive method to find the answer however using Google search returns no result. I want to ask if someone know any directions or (Matlab) methods to approximately solve these equations in order to find $m$ and $u$ ? Thank you very much.",,"['multivariable-calculus', 'partial-differential-equations', 'optimal-control']"
33,Proving extermum by using Taylor series,Proving extermum by using Taylor series,,"Will someone please help me in the following? I am given with the function $f(x,y)=(x+y)^3\sqrt{x^2+y^2}-1+\cos(x+y)$ and need to determine whether $(0,0)$ and $(1,-1)$ are extremum points or not. As for $(0,0)$ after using Taylor expansion for $\cos(x+y)$ around $(0,0)$, we get that $f(x,y)=-\frac{(x+y)^2}{2}+O((x+y)^3) $ and hence we have that $(0,0)$ is a local maximum. As for $(1,-1)$ I think that the same argument will work here as well, but I am not sure about it, since these are two different parts of the question. Am I right? If not, where is my mistake? Hope you'll be able to help. Thanks in advance","Will someone please help me in the following? I am given with the function $f(x,y)=(x+y)^3\sqrt{x^2+y^2}-1+\cos(x+y)$ and need to determine whether $(0,0)$ and $(1,-1)$ are extremum points or not. As for $(0,0)$ after using Taylor expansion for $\cos(x+y)$ around $(0,0)$, we get that $f(x,y)=-\frac{(x+y)^2}{2}+O((x+y)^3) $ and hence we have that $(0,0)$ is a local maximum. As for $(1,-1)$ I think that the same argument will work here as well, but I am not sure about it, since these are two different parts of the question. Am I right? If not, where is my mistake? Hope you'll be able to help. Thanks in advance",,['multivariable-calculus']
34,Surface Area Line integral problem,Surface Area Line integral problem,,"I'm trying to figure out how to solve a surface area with surface and line integrals (showing both methods). The area I'm trying to compute is the area of the shape $$x^2+y^2=9$$ bounded by $z=0$ and $z=y$. (Note: $y \ge 0$). I've started the problem by making a parametrization:  $$ r(u,v) = \langle 3\cos v, 3\sin v, u\rangle $$ from $0 \leq u \leq 3\sin v$ and $-\frac{ \pi}{3} \leq v \leq \pi$. The magnitude of $|r_u \times r_v| = 3$. Not sure where to go from here.","I'm trying to figure out how to solve a surface area with surface and line integrals (showing both methods). The area I'm trying to compute is the area of the shape $$x^2+y^2=9$$ bounded by $z=0$ and $z=y$. (Note: $y \ge 0$). I've started the problem by making a parametrization:  $$ r(u,v) = \langle 3\cos v, 3\sin v, u\rangle $$ from $0 \leq u \leq 3\sin v$ and $-\frac{ \pi}{3} \leq v \leq \pi$. The magnitude of $|r_u \times r_v| = 3$. Not sure where to go from here.",,"['calculus', 'multivariable-calculus', 'surfaces']"
35,How to evaluate this partial derivative on multivariate polynomial?,How to evaluate this partial derivative on multivariate polynomial?,,"This is a polynomial that I found in my research, and I'd like to know a simplified form. I have a very simple degree-$n$ multivariate polynomial $p(x,z_1,\cdots,z_n) = (x + z_1 + \cdots +z_n)^n$. I'd like to apply a set of simple operations to convert $p(x,z_1,\cdots,z_n)$ into a univariate polynomial $q(x)$. \begin{equation*} q(x) = \left[ \left( \prod_{i=1}^n \left(1 - \frac{\partial}{\partial z_i}\right) \right) p(x,z_1,\cdots,z_n) \right] \Bigg|_{z_1=\cdots=z_n=0}. \end{equation*} One can easily see that the conversion is composed of two kinds of operations: Restriction: Setting a certain variable $z_i = 0$ in a multivariate polynomial; that is, $r(x,z_1,\cdots,z_{n-1},z_n)|_{z_n=0} = r(x,z_1,\cdots,z_{n-1},0)$. Differentiation: $(1 - \frac{\partial}{\partial z_i})r(x,z_1,\cdots,z_n) = r(x,z_1,\cdots,z_n) - \frac{\partial}{\partial z_i}r(x,z_1,\cdots,z_n)$. I'd like to know how $q(x)$ looks for any $n$. Is there any closed form for this univariate polynomial $q(x)$? Can I actually expand the definition of $q(x)$ and find a simplified form?","This is a polynomial that I found in my research, and I'd like to know a simplified form. I have a very simple degree-$n$ multivariate polynomial $p(x,z_1,\cdots,z_n) = (x + z_1 + \cdots +z_n)^n$. I'd like to apply a set of simple operations to convert $p(x,z_1,\cdots,z_n)$ into a univariate polynomial $q(x)$. \begin{equation*} q(x) = \left[ \left( \prod_{i=1}^n \left(1 - \frac{\partial}{\partial z_i}\right) \right) p(x,z_1,\cdots,z_n) \right] \Bigg|_{z_1=\cdots=z_n=0}. \end{equation*} One can easily see that the conversion is composed of two kinds of operations: Restriction: Setting a certain variable $z_i = 0$ in a multivariate polynomial; that is, $r(x,z_1,\cdots,z_{n-1},z_n)|_{z_n=0} = r(x,z_1,\cdots,z_{n-1},0)$. Differentiation: $(1 - \frac{\partial}{\partial z_i})r(x,z_1,\cdots,z_n) = r(x,z_1,\cdots,z_n) - \frac{\partial}{\partial z_i}r(x,z_1,\cdots,z_n)$. I'd like to know how $q(x)$ looks for any $n$. Is there any closed form for this univariate polynomial $q(x)$? Can I actually expand the definition of $q(x)$ and find a simplified form?",,"['multivariable-calculus', 'partial-derivative']"
36,A vector analysis question requiring multivariable calculus,A vector analysis question requiring multivariable calculus,,"The question is taken from a Vector-Analysis worksheet as an extension exercise, here is the first part: Let $\underline{v}$ be a vector field $\mathbb{R}^{2} \backslash (0,0)$ of the form \begin{equation} \underline{v}(x,y) = f(r)(x,y) \end{equation} where $r = \sqrt{x^{2}+y^{2}}$ and $f: (0,\infty) \to \mathbb{R}$ is continuously differentiable. Find a scalar potential for $\underline{v}$ in terms of an integral involving $f$, given that $\underline{v}$ is conservative. So far I have that since $\underline{v}$ is conservative, there exists a function $g(x,y)$ such that $\frac{\partial{g}}{\partial{x}} = xf(r)$ and $\frac{\partial{g}}{\partial{y}} = yf(r)$. Now I am a guessing that $g$ will in someway contain $F(t)$ where $F(t) = \int_0^rf(t)\,\mathrm{d}t$ since by the Fundamental Theorem of Calculus $F'(t) = f(r)$. However my knowledge of multivariable differentiation under the integral sign is not so good, and I am unsure how to take the partial derivatives of $F(t)$ with respect to $x$ or $y$. I am also unsure of the significance of $f$ being continuously differentiable. Any tips on where to go from here?","The question is taken from a Vector-Analysis worksheet as an extension exercise, here is the first part: Let $\underline{v}$ be a vector field $\mathbb{R}^{2} \backslash (0,0)$ of the form \begin{equation} \underline{v}(x,y) = f(r)(x,y) \end{equation} where $r = \sqrt{x^{2}+y^{2}}$ and $f: (0,\infty) \to \mathbb{R}$ is continuously differentiable. Find a scalar potential for $\underline{v}$ in terms of an integral involving $f$, given that $\underline{v}$ is conservative. So far I have that since $\underline{v}$ is conservative, there exists a function $g(x,y)$ such that $\frac{\partial{g}}{\partial{x}} = xf(r)$ and $\frac{\partial{g}}{\partial{y}} = yf(r)$. Now I am a guessing that $g$ will in someway contain $F(t)$ where $F(t) = \int_0^rf(t)\,\mathrm{d}t$ since by the Fundamental Theorem of Calculus $F'(t) = f(r)$. However my knowledge of multivariable differentiation under the integral sign is not so good, and I am unsure how to take the partial derivatives of $F(t)$ with respect to $x$ or $y$. I am also unsure of the significance of $f$ being continuously differentiable. Any tips on where to go from here?",,"['multivariable-calculus', 'derivatives', 'vector-analysis']"
37,"What is the 2nd order taylor polynomial of f(x,y)?","What is the 2nd order taylor polynomial of f(x,y)?",,"I'm just computing the 2nd order taylor polynomial for $f(x,y) = tan(x + 3y + \frac{\pi}{4})$ centered at (3,-1) and wondering if I have done this correctly or if anyone has any suggestions on how I can improve my answer (I've never done this before with mulitvariables so just want to be sure I am on the right track): I have that: $$f(3,-1) = tan(\frac{\pi}{4}) = 1$$ $$f_{x}(3,-1) = sec^2(\frac{\pi}{4}) = 2$$ $$f_{y}(3,-1) = 3sec^2(\frac{\pi}{4}) = 6$$ So the gradient vector for $f$ is $(2,6)$ The letting $u = x + 3y + \frac{\pi}{4}$, I get that $\frac{du}{dx}= 1 $ and $\frac{dy}{dx}= 3 $ So, $$f_{x}(u) = sec^2(u)$$ $$f_{xx}(u) = 2tan(u)sec^2(u)\frac{du}{dx}$$ $$f_{xy}(u) = 2tan(u)sec^2(u)\frac{du}{dy}$$ and similarly, $$f_{y}(u) = 3sec^2(u)$$ $$f_{yx}(u) = 6tan(u)sec^2(u)\frac{du}{dx}$$ $$f_{yy}(u) = 6tan(u)sec^2(u)\frac{du}{dy}$$ Substituting back for $u$ and plugging in $(3,-1)$ I get: $$f_{xx}(3,-1) = 2tan(\frac{\pi}{4})sec^2(\frac{\pi}{4})\cdot1 = 4$$ $$f_{xy}(3,-1) = 2tan(\frac{\pi}{4})sec^2(\frac{\pi}{4})\cdot3 = 12$$ and similarly, $$f_{yx}(3,-1) = 6tan(\frac{\pi}{4})sec^2(\frac{\pi}{4})\cdot1 = 12$$ $$f_{yy}(3,-1) = 6tan(\frac{\pi}{4})sec^2(\frac{\pi}{4})\cdot3 = 36$$ So, $$T_{2}((x,y),(3,-1)) = 1+(2,6) \cdot (x-3,y+1) + \frac{1}{2} \bigl( \begin{smallmatrix}    x-3 & y+1\\ \end{smallmatrix} \bigr) \bigl( \begin{smallmatrix}    4 & 12\\   12 & 36  \end{smallmatrix} \bigr) \bigl( \begin{smallmatrix}    x-3 \\   y+1   \end{smallmatrix} \bigr)$$ Performing the calculations I get: $$= 1+2x-6+6y+6+\frac{1}{2}[(x-3)(4x-12+12y+12)+(y+1)(12x-36+36y+36)]$$ $$ = 2x^2 + 18y^2 + 12xy + 2x + 6y + 1$$ Is this correct and have I done the steps correctly? Many thanks in advance!","I'm just computing the 2nd order taylor polynomial for $f(x,y) = tan(x + 3y + \frac{\pi}{4})$ centered at (3,-1) and wondering if I have done this correctly or if anyone has any suggestions on how I can improve my answer (I've never done this before with mulitvariables so just want to be sure I am on the right track): I have that: $$f(3,-1) = tan(\frac{\pi}{4}) = 1$$ $$f_{x}(3,-1) = sec^2(\frac{\pi}{4}) = 2$$ $$f_{y}(3,-1) = 3sec^2(\frac{\pi}{4}) = 6$$ So the gradient vector for $f$ is $(2,6)$ The letting $u = x + 3y + \frac{\pi}{4}$, I get that $\frac{du}{dx}= 1 $ and $\frac{dy}{dx}= 3 $ So, $$f_{x}(u) = sec^2(u)$$ $$f_{xx}(u) = 2tan(u)sec^2(u)\frac{du}{dx}$$ $$f_{xy}(u) = 2tan(u)sec^2(u)\frac{du}{dy}$$ and similarly, $$f_{y}(u) = 3sec^2(u)$$ $$f_{yx}(u) = 6tan(u)sec^2(u)\frac{du}{dx}$$ $$f_{yy}(u) = 6tan(u)sec^2(u)\frac{du}{dy}$$ Substituting back for $u$ and plugging in $(3,-1)$ I get: $$f_{xx}(3,-1) = 2tan(\frac{\pi}{4})sec^2(\frac{\pi}{4})\cdot1 = 4$$ $$f_{xy}(3,-1) = 2tan(\frac{\pi}{4})sec^2(\frac{\pi}{4})\cdot3 = 12$$ and similarly, $$f_{yx}(3,-1) = 6tan(\frac{\pi}{4})sec^2(\frac{\pi}{4})\cdot1 = 12$$ $$f_{yy}(3,-1) = 6tan(\frac{\pi}{4})sec^2(\frac{\pi}{4})\cdot3 = 36$$ So, $$T_{2}((x,y),(3,-1)) = 1+(2,6) \cdot (x-3,y+1) + \frac{1}{2} \bigl( \begin{smallmatrix}    x-3 & y+1\\ \end{smallmatrix} \bigr) \bigl( \begin{smallmatrix}    4 & 12\\   12 & 36  \end{smallmatrix} \bigr) \bigl( \begin{smallmatrix}    x-3 \\   y+1   \end{smallmatrix} \bigr)$$ Performing the calculations I get: $$= 1+2x-6+6y+6+\frac{1}{2}[(x-3)(4x-12+12y+12)+(y+1)(12x-36+36y+36)]$$ $$ = 2x^2 + 18y^2 + 12xy + 2x + 6y + 1$$ Is this correct and have I done the steps correctly? Many thanks in advance!",,"['multivariable-calculus', 'taylor-expansion']"
38,Is it generally difficult to memorize 'multivariable calculus' theorems?,Is it generally difficult to memorize 'multivariable calculus' theorems?,,"There are many weak forms of ""Fubini's theorem"" with strong hypotheses in elementary calculus texts. However, these strong hypotheses are very unnatural and are thus hard to memorize. Compared to that, the full Fubini's theorem (in measure space) is actually relatively easy to memorize in comparison. Furthermore: I am wondering if this is also the case for multivariable calculus. I'm currently studying ""implicit function theorem"", and I find the hypotheses for the theorem to be quite hard to memorize. Are theorems in mutivariable calculus easier to memorize in the context of manifolds ?","There are many weak forms of ""Fubini's theorem"" with strong hypotheses in elementary calculus texts. However, these strong hypotheses are very unnatural and are thus hard to memorize. Compared to that, the full Fubini's theorem (in measure space) is actually relatively easy to memorize in comparison. Furthermore: I am wondering if this is also the case for multivariable calculus. I'm currently studying ""implicit function theorem"", and I find the hypotheses for the theorem to be quite hard to memorize. Are theorems in mutivariable calculus easier to memorize in the context of manifolds ?",,"['real-analysis', 'multivariable-calculus', 'differential-geometry', 'soft-question']"
39,Prove an implicit function solves an O.D.E,Prove an implicit function solves an O.D.E,,"Let $\Phi(u,v)$ differentiable on the plane. a. which condition should $$\Phi(x+az,y+bz)=0\quad(a,b\in\mathbb R)$$ satisfy in order to define a function $z=z(x,y)$ ? b. Prove that $z=z(x,y)$ is a solution for the ode $$a\frac{\partial z}{\partial x}+b\frac{\partial z}{\partial y}=-1$$ About A: According to implicit function theorem we can require $$\Phi(x+az(x,y),y+bz(x,y))=0$$ from there I cannot find any constraints on $\Phi$ . We also require $$\frac{\partial \Phi}{\partial z}\neq 0$$ . but since it's function of $x+az,y+bz$ , the derivative $\Phi^\prime$ is function of $a+b$ so we require $a+b\neq0\Leftrightarrow a\neq -b$ . In these terms I think $z=z(x,y)$ is defined. I'm not sure about the first condition (seems I miss something) About B: If $z=z(x,y)$ so according to the theorem $$\frac{\partial z}{\partial x}=\frac{\frac{\partial \Phi}{\partial x}}{\frac{\partial \Phi}{\partial z}}=\frac{1}{a+b},\frac{\partial z}{\partial x}=\frac{\frac{\partial \Phi}{\partial y}}{\frac{\partial \Phi}{\partial z}}=\frac{1}{a+b}$$ so after substituing I get $$\frac{a+b}{a+b}=1\neq -1$$ Where am I wrong?","Let differentiable on the plane. a. which condition should satisfy in order to define a function ? b. Prove that is a solution for the ode About A: According to implicit function theorem we can require from there I cannot find any constraints on . We also require . but since it's function of , the derivative is function of so we require . In these terms I think is defined. I'm not sure about the first condition (seems I miss something) About B: If so according to the theorem so after substituing I get Where am I wrong?","\Phi(u,v) \Phi(x+az,y+bz)=0\quad(a,b\in\mathbb R) z=z(x,y) z=z(x,y) a\frac{\partial z}{\partial x}+b\frac{\partial z}{\partial y}=-1 \Phi(x+az(x,y),y+bz(x,y))=0 \Phi \frac{\partial \Phi}{\partial z}\neq 0 x+az,y+bz \Phi^\prime a+b a+b\neq0\Leftrightarrow a\neq -b z=z(x,y) z=z(x,y) \frac{\partial z}{\partial x}=\frac{\frac{\partial \Phi}{\partial x}}{\frac{\partial \Phi}{\partial z}}=\frac{1}{a+b},\frac{\partial z}{\partial x}=\frac{\frac{\partial \Phi}{\partial y}}{\frac{\partial \Phi}{\partial z}}=\frac{1}{a+b} \frac{a+b}{a+b}=1\neq -1","['real-analysis', 'multivariable-calculus']"
40,Derivative of a linear transformation $f: \mathbb{R^n} \to \mathbb{R^m} $,Derivative of a linear transformation,f: \mathbb{R^n} \to \mathbb{R^m} ,"In multivariable calculus we have that , $f: \mathbb{R^n} \to \mathbb{R^m}$ is differentiable at $a \in \mathbb{R^n}$, if there exists a linear transformation $\mu : \mathbb{R^n} \to \mathbb{R^m} $ such that $$\lim_{h \to 0} \frac{|f(a+h)-f(a)-\mu(h)|}{|h|} = 0$$ then $\mu$ is the derivative of $f$ at $a$ and we denote it by $Df(a)$. According to this definition it is easy to see that, if $f$ is a linear transformation, then $Df(a)=f$ for all $a \in \mathbb{R^n}$. Having this definition in mind, I am confused by this simple example: Let $f: \mathbb{R} \to \mathbb{R}$ be defined by $f(x)=2x$, is a linear transformation, I can't believe that  $Df(a)=f$, which sends any real $x$ number to $2x$ but clearly $Df(a)=2$, is the Jacobian $1 \times 1$ matrix, I kind of see that, it is just the same thing, this also sends any real $x$ number to $2x$. What is happenning here ?!?! Thanks for your help in advance","In multivariable calculus we have that , $f: \mathbb{R^n} \to \mathbb{R^m}$ is differentiable at $a \in \mathbb{R^n}$, if there exists a linear transformation $\mu : \mathbb{R^n} \to \mathbb{R^m} $ such that $$\lim_{h \to 0} \frac{|f(a+h)-f(a)-\mu(h)|}{|h|} = 0$$ then $\mu$ is the derivative of $f$ at $a$ and we denote it by $Df(a)$. According to this definition it is easy to see that, if $f$ is a linear transformation, then $Df(a)=f$ for all $a \in \mathbb{R^n}$. Having this definition in mind, I am confused by this simple example: Let $f: \mathbb{R} \to \mathbb{R}$ be defined by $f(x)=2x$, is a linear transformation, I can't believe that  $Df(a)=f$, which sends any real $x$ number to $2x$ but clearly $Df(a)=2$, is the Jacobian $1 \times 1$ matrix, I kind of see that, it is just the same thing, this also sends any real $x$ number to $2x$. What is happenning here ?!?! Thanks for your help in advance",,"['calculus', 'multivariable-calculus']"
41,"Prove that $f$ is continuous at $(0, y_0)$. where $f$ is defined on $\Bbb R^2$.",Prove that  is continuous at . where  is defined on .,"f (0, y_0) f \Bbb R^2","Prove that f is continuous at $(0, y_0)$ $f(x, y) = \begin{cases} (1+xy)^{1/x} &\mbox{if } x \neq 0 \\ e^y & \mbox{if } x \equiv 0. \end{cases} $ Thank you!","Prove that f is continuous at $(0, y_0)$ $f(x, y) = \begin{cases} (1+xy)^{1/x} &\mbox{if } x \neq 0 \\ e^y & \mbox{if } x \equiv 0. \end{cases} $ Thank you!",,"['real-analysis', 'multivariable-calculus', 'continuity']"
42,Change to polar coordinates when evaluating limits of functions in two variables?,Change to polar coordinates when evaluating limits of functions in two variables?,,"I have a function in two variables $f(x, y)$ and need to calculate the limit $$ \lim_{(x, y) \rightarrow (2, 3)}{f(x, y)} .$$ If I decide to change to polar coordinates, how can I determine where $r$ tends to? I was thinking, since $r = \sqrt{x^{2} + y^{2}}$, on evaluating $$ \lim_{(x, y) \rightarrow (2, 3)}{r} = \lim_{(x, y) \rightarrow (2, 3)}{ \sqrt{x^{2} + y^{2}}} = \sqrt{13},$$ and then writing $$ \lim_{(x, y) \rightarrow (2, 3)}{f(x, y)} = \lim_{r \rightarrow \sqrt{13}}{f(r \cos{\theta}, r \sin{\theta})} .$$ Is this correct?","I have a function in two variables $f(x, y)$ and need to calculate the limit $$ \lim_{(x, y) \rightarrow (2, 3)}{f(x, y)} .$$ If I decide to change to polar coordinates, how can I determine where $r$ tends to? I was thinking, since $r = \sqrt{x^{2} + y^{2}}$, on evaluating $$ \lim_{(x, y) \rightarrow (2, 3)}{r} = \lim_{(x, y) \rightarrow (2, 3)}{ \sqrt{x^{2} + y^{2}}} = \sqrt{13},$$ and then writing $$ \lim_{(x, y) \rightarrow (2, 3)}{f(x, y)} = \lim_{r \rightarrow \sqrt{13}}{f(r \cos{\theta}, r \sin{\theta})} .$$ Is this correct?",,"['limits', 'multivariable-calculus', 'polar-coordinates']"
43,The Derivative of a General Linear Map,The Derivative of a General Linear Map,,"This question is somewhat abstract compared to the things we've discussed in class, so I'm just making sure I've got the right idea. I'd appreciate any help/suggestions; I'm pretty sure I've got the right answer, but I may be totally incorrect in what I think the question is asking. The question is: Suppose $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is a linear map. What is the derivative of $f$ ? My answer is: Let $f: A \subset \mathbb{R}^n \rightarrow \mathbb{R}^m$ be a linear map where $A$ is an open set. Let $x,y \in \mathbb{R}^n$ and $\alpha \in \mathbb{R}$ . From the def. of a linear map, we know that $f(\alpha x) = \alpha f(x)$ and $f(x + y) = f(x) + f(y)$ . Thus, to be a linear map, no element in the image of $f$ can contain the product of elements in the domain of $f$ (otherwise, $f(x_1,x_2) = x_1x_2 \implies f(\alpha_1x_1,\alpha_2x_2) = \alpha^2x_1x_2 \ne \alpha f(x_1,x_2)$ ). Given the above statement, all linear maps from $\mathbb{R}^n$ to $\mathbb{R}^m$ must have the form... $f(x_1,...,x_n) =$ $\begin{pmatrix}  a_{1,1} x_1 ~ + ...+ ~ a_{1,n} x_n \\ \vdots \\ a_{m,1}x_1 ~ + ... + ~ a_{m,n} x_n \end{pmatrix}$ Where $a_{i,j}$ is the coefficient of the $x_{j}$ term in the $i^{th}$ row. The derivative of this function is the matrix of partial derivatives of $f(x_1,...,x_n)$ . Note, though, that the partial derivative of any row with respect to $x_i$ is just the coefficient of the $x_i$ term (since no other term in the row depends on $x_i$ , thus their partial derivatives are zero). So our matrix of partial derivatives becomes: $\frac{df}{dx} = \begin{pmatrix}  a_{1,1} & ... & a_{1,n} \\ \vdots & \ddots\\ a_{m,1} & ... & a_{m,n} \end{pmatrix}$ (That is, the matrix of coefficients where the $i^{th}$ column contains the $m$ coefficients of each $x_i$ term). Can anyone verify whether or not a) This is actually what the question is looking for (I think the simplicity is making me skeptical) and b) Whether or not I've the correct answer if it is, in fact, what the question is looking for?","This question is somewhat abstract compared to the things we've discussed in class, so I'm just making sure I've got the right idea. I'd appreciate any help/suggestions; I'm pretty sure I've got the right answer, but I may be totally incorrect in what I think the question is asking. The question is: Suppose is a linear map. What is the derivative of ? My answer is: Let be a linear map where is an open set. Let and . From the def. of a linear map, we know that and . Thus, to be a linear map, no element in the image of can contain the product of elements in the domain of (otherwise, ). Given the above statement, all linear maps from to must have the form... Where is the coefficient of the term in the row. The derivative of this function is the matrix of partial derivatives of . Note, though, that the partial derivative of any row with respect to is just the coefficient of the term (since no other term in the row depends on , thus their partial derivatives are zero). So our matrix of partial derivatives becomes: (That is, the matrix of coefficients where the column contains the coefficients of each term). Can anyone verify whether or not a) This is actually what the question is looking for (I think the simplicity is making me skeptical) and b) Whether or not I've the correct answer if it is, in fact, what the question is looking for?","f: \mathbb{R}^n \rightarrow \mathbb{R}^m f f: A \subset \mathbb{R}^n \rightarrow \mathbb{R}^m A x,y \in \mathbb{R}^n \alpha \in \mathbb{R} f(\alpha x) = \alpha f(x) f(x + y) = f(x) + f(y) f f f(x_1,x_2) = x_1x_2 \implies f(\alpha_1x_1,\alpha_2x_2) = \alpha^2x_1x_2 \ne \alpha f(x_1,x_2) \mathbb{R}^n \mathbb{R}^m f(x_1,...,x_n) = \begin{pmatrix} 
a_{1,1} x_1 ~ + ...+ ~ a_{1,n} x_n \\
\vdots \\
a_{m,1}x_1 ~ + ... + ~ a_{m,n} x_n
\end{pmatrix} a_{i,j} x_{j} i^{th} f(x_1,...,x_n) x_i x_i x_i \frac{df}{dx} = \begin{pmatrix} 
a_{1,1} & ... & a_{1,n} \\
\vdots & \ddots\\
a_{m,1} & ... & a_{m,n}
\end{pmatrix} i^{th} m x_i","['multivariable-calculus', 'vector-spaces']"
44,Mean Value Theorem help,Mean Value Theorem help,,"once again. I wish to prove that $$\frac{2}{\pi} = \cos\left( \frac{\pi t}{2}\right) + \sin\left( \frac{\pi}{2} ( 1-t ) \right)$$ for some t in the interval $( 0, 1 )$ given the function $$f( x, y ) = \sin(\pi x ) + \cos(\pi y ).$$ I was told that I could prove it using the mean value theorem, but I am not exactly sure how to use it in this case. Thank you for your help ahead of time.","once again. I wish to prove that $$\frac{2}{\pi} = \cos\left( \frac{\pi t}{2}\right) + \sin\left( \frac{\pi}{2} ( 1-t ) \right)$$ for some t in the interval $( 0, 1 )$ given the function $$f( x, y ) = \sin(\pi x ) + \cos(\pi y ).$$ I was told that I could prove it using the mean value theorem, but I am not exactly sure how to use it in this case. Thank you for your help ahead of time.",,['multivariable-calculus']
45,Multivariable Calculus Vector Fields,Multivariable Calculus Vector Fields,,"I have to prove that if  $f(x,y,z)=f_{a}(x,y,z)+f_b(x,y,z)+f_c(x,y,z)$ is a conservative vector field and and $g(x,y,z)=g_{a}(x,y,z)+g_b(x,y,z)+g_c(x,y,z)$ is also a conservative vector field, then $(cf+dg)(x,y,z)$ is conservative.","I have to prove that if  $f(x,y,z)=f_{a}(x,y,z)+f_b(x,y,z)+f_c(x,y,z)$ is a conservative vector field and and $g(x,y,z)=g_{a}(x,y,z)+g_b(x,y,z)+g_c(x,y,z)$ is also a conservative vector field, then $(cf+dg)(x,y,z)$ is conservative.",,"['multivariable-calculus', 'vector-spaces']"
46,Multivariable calculus along with tensors ...etc to start studying General Relativity,Multivariable calculus along with tensors ...etc to start studying General Relativity,,"I bought Spivak Calculus on Manifolds last time and I was really really disappointed... I opened the first chapters and I understood nothing of what he was saying. But i need to understand multivariable calculus along with tensors and all that stuff to start study General relativity next semester... Which book do you recommend me? (someone that contain everything      (Partial) Differentiation     (Multiple) Integration     Curves and Surfaces in R3     Vector Calculus (Green's Theorem, Stokes' Theorem, Divergence Theorem) ) THANKS","I bought Spivak Calculus on Manifolds last time and I was really really disappointed... I opened the first chapters and I understood nothing of what he was saying. But i need to understand multivariable calculus along with tensors and all that stuff to start study General relativity next semester... Which book do you recommend me? (someone that contain everything      (Partial) Differentiation     (Multiple) Integration     Curves and Surfaces in R3     Vector Calculus (Green's Theorem, Stokes' Theorem, Divergence Theorem) ) THANKS",,"['reference-request', 'multivariable-calculus', 'general-relativity']"
47,Correct substitution for simplifying derivative,Correct substitution for simplifying derivative,,"The stationary points of $$ f(x,y) = (x^2 + y^2)e^{-(x^2 + y^2)} $$ can be seen to be at $(0,0)$ and on the unit circle, specifically the point at $(0,0)$ is a local minimum and points on the unit circle maximums. This is easy to verify by partial differentiation. I tried another approach via the substitution $r = x^2 + y^2$, which gives a simpler differential with respect to $r$, $$ \frac{d}{dr} (re^{-r}) = (1 - r)e^{-r} $$ which clearly gives the stationary points on the unit circle. However, this differential implies the point $r = 0$ has derivative $1$, not $0$ as expected. If instead the substitution $r^2 = x^2 + y^2$ is made, we find the derivative with respect to $r$ is $$ \frac{d}{dr} (r^2e^{-r^2}) = e^{-r^2} (2r-2 r^3) $$ which has roots $r = \pm 1$ and $r = 0$, giving all the stationary points expected. My question is why does the second substitution give all the correct stationary points where as the first does not? And why does the first substitution imply the point $(0,0)$ has a non-zero derivative?","The stationary points of $$ f(x,y) = (x^2 + y^2)e^{-(x^2 + y^2)} $$ can be seen to be at $(0,0)$ and on the unit circle, specifically the point at $(0,0)$ is a local minimum and points on the unit circle maximums. This is easy to verify by partial differentiation. I tried another approach via the substitution $r = x^2 + y^2$, which gives a simpler differential with respect to $r$, $$ \frac{d}{dr} (re^{-r}) = (1 - r)e^{-r} $$ which clearly gives the stationary points on the unit circle. However, this differential implies the point $r = 0$ has derivative $1$, not $0$ as expected. If instead the substitution $r^2 = x^2 + y^2$ is made, we find the derivative with respect to $r$ is $$ \frac{d}{dr} (r^2e^{-r^2}) = e^{-r^2} (2r-2 r^3) $$ which has roots $r = \pm 1$ and $r = 0$, giving all the stationary points expected. My question is why does the second substitution give all the correct stationary points where as the first does not? And why does the first substitution imply the point $(0,0)$ has a non-zero derivative?",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
48,Change of variables for linear differential operators,Change of variables for linear differential operators,,"I am trying to find an expression for a change of variables (invertible and $C^{\infty}$) of a linear differential operator in $\mathbb R ^d \newcommand{\t}{\tilde} \newcommand{\p}{\partial}$. i) case $d = 1$. A linear differential operator has the form $$ A = a_0 + a_1 \dfrac{d}{dx}.$$ Consider the change of variable $ y = f(x) $. I want to find a differential operator $\t A = \t a_0 + \t a_1 \dfrac{d}{dy}$ s.t. $$ \t A( u(y))|_{y=f(x)} = A(u(x)).$$ To obtain this identity, it must be $$ \t a_0(f(x)) + \t a_1(f(x)) \dfrac{d u(y)}{dy} \Bigg|_{y=f(x)} = a_0(x) + a_1(x) \dfrac{du(x)}{dx}.$$ But how should I take $\t a_0, \t a_1$ to obtain this equality? ii) case $ d = 2$. A linear differential operator has the form $$ A= a_{00} + a_{10} \dfrac{\p}{\p x_1} + a_{01} \dfrac{\p}{\p x_2} + a_{11} \dfrac{\p^2}{\p x_2} + a_{20} \dfrac{\p^2}{\p x_1 ^2} + a_{02} \dfrac{\p^2}{\p x_2^2}.$$ Considering  the change of variable $Y = (y_1, y_2) = (f_1(x_1, x_2), f_2(x_1, x_2)) = F(X)$, we will have that the operator $\t A$ expresses $A$ in the new coordinates if $$ \t a_{00}(F(X)) + \dots + \t a_{02}(F(X)) \dfrac{\p^2 u(y_1, y_2))}{\p y_2^2} \Bigg|_{Y= F(X)} \\ =  a_{00}(X) + \dots + a_{02}(X) \dfrac{\p^2 u(x_1, x_2)}{\p x_2^2}.$$ Again, how can I find the coefficients $\t a_{ij}$? iii) general case. Consider in $\mathbb R ^d $ the change of coordinates $$Y = (y_1, \dots, y_d) = (f_1(x_1, \dots, x_d), \dots , f_d(x_1, \dots, x_d)) = F(X).$$ I will the use multi-index notation $$ P = (p_1, \dots, p_d), |P| = p_1 + \dots + p_d,\\ \p ^P_X = \dfrac{\p ^{|P|}}{\p x_1^{p_1} \dots \p x_d^{p_d}}, \p ^P_Y = \dfrac{\p ^{|P|}}{\p y_1^{p_1} \dots \p y_d^{p_d}}.$$ A linear differential operator has the form $$ A = \sum_{|P| \leq m} a_P(X) \p ^P_X. $$ How can I find the coefficients of $$ \t A = \sum_{|P| \leq m} \t a_P(Y) \p ^P_Y $$ so that it is the expression of $A$ in the new coordinates $Y$?","I am trying to find an expression for a change of variables (invertible and $C^{\infty}$) of a linear differential operator in $\mathbb R ^d \newcommand{\t}{\tilde} \newcommand{\p}{\partial}$. i) case $d = 1$. A linear differential operator has the form $$ A = a_0 + a_1 \dfrac{d}{dx}.$$ Consider the change of variable $ y = f(x) $. I want to find a differential operator $\t A = \t a_0 + \t a_1 \dfrac{d}{dy}$ s.t. $$ \t A( u(y))|_{y=f(x)} = A(u(x)).$$ To obtain this identity, it must be $$ \t a_0(f(x)) + \t a_1(f(x)) \dfrac{d u(y)}{dy} \Bigg|_{y=f(x)} = a_0(x) + a_1(x) \dfrac{du(x)}{dx}.$$ But how should I take $\t a_0, \t a_1$ to obtain this equality? ii) case $ d = 2$. A linear differential operator has the form $$ A= a_{00} + a_{10} \dfrac{\p}{\p x_1} + a_{01} \dfrac{\p}{\p x_2} + a_{11} \dfrac{\p^2}{\p x_2} + a_{20} \dfrac{\p^2}{\p x_1 ^2} + a_{02} \dfrac{\p^2}{\p x_2^2}.$$ Considering  the change of variable $Y = (y_1, y_2) = (f_1(x_1, x_2), f_2(x_1, x_2)) = F(X)$, we will have that the operator $\t A$ expresses $A$ in the new coordinates if $$ \t a_{00}(F(X)) + \dots + \t a_{02}(F(X)) \dfrac{\p^2 u(y_1, y_2))}{\p y_2^2} \Bigg|_{Y= F(X)} \\ =  a_{00}(X) + \dots + a_{02}(X) \dfrac{\p^2 u(x_1, x_2)}{\p x_2^2}.$$ Again, how can I find the coefficients $\t a_{ij}$? iii) general case. Consider in $\mathbb R ^d $ the change of coordinates $$Y = (y_1, \dots, y_d) = (f_1(x_1, \dots, x_d), \dots , f_d(x_1, \dots, x_d)) = F(X).$$ I will the use multi-index notation $$ P = (p_1, \dots, p_d), |P| = p_1 + \dots + p_d,\\ \p ^P_X = \dfrac{\p ^{|P|}}{\p x_1^{p_1} \dots \p x_d^{p_d}}, \p ^P_Y = \dfrac{\p ^{|P|}}{\p y_1^{p_1} \dots \p y_d^{p_d}}.$$ A linear differential operator has the form $$ A = \sum_{|P| \leq m} a_P(X) \p ^P_X. $$ How can I find the coefficients of $$ \t A = \sum_{|P| \leq m} \t a_P(Y) \p ^P_Y $$ so that it is the expression of $A$ in the new coordinates $Y$?",,"['calculus', 'multivariable-calculus', 'partial-differential-equations', 'partial-derivative', 'differential-operators']"
49,Show that B is measurable,Show that B is measurable,,"Let $\mathbf{g}:\Delta \subset \mathbb{R}^n \rightarrow D\subset \mathbb{R}^n $ be univalent, $C^{(1)}$ and with the Jacobian different from zero, $\forall t \in \Delta$. Let $B=\mathbf{g}^{-1}(A)$, where $A$ is a measurable (Lebesgue) subset of $D$. Show that B is measurable. There is hint which is to first consider the case when $A\subset L\subset D$, where $L$ is compact.  And also, from another exercise, we proved that for this g, and for a compact $L$, and a number $C$ (proved in yet another exercise), if $B\subset int(L)$, then $V(A)=V[\mathbf{g}(B)]\leq C^n V(B)$. And the last part of the exercise is to find compact sets $L_1 \subset L_2 \subset \dots$ with union $D$. We know that $\mathbf{g}^{-1}$ will also have the Jacobian different from zero. So, I was thinking of using this last theorem this time to $\mathbf{g}^{-1}$, resulting in a different upper bound number $Q$, we would get $V(B)=V[\mathbf{g}^{-1}(A)]\leq Q^n V(A)$... But this doesn't seem to help at all. I have no idea how to prove that it's measurable. The hints seem only help me prove what would be the measure of B (bounds of the measure) instead of trying to prove that B has a measure. I'm probably not understanding something... Any help would be appreciated. Thanks in advance. P.S.: This is an exercise taken from Wendell Fleming's Functions of several variables, page 216, exercise 8. (It uses exercise 6 and 7) http://en.bookfi.org/book/580162","Let $\mathbf{g}:\Delta \subset \mathbb{R}^n \rightarrow D\subset \mathbb{R}^n $ be univalent, $C^{(1)}$ and with the Jacobian different from zero, $\forall t \in \Delta$. Let $B=\mathbf{g}^{-1}(A)$, where $A$ is a measurable (Lebesgue) subset of $D$. Show that B is measurable. There is hint which is to first consider the case when $A\subset L\subset D$, where $L$ is compact.  And also, from another exercise, we proved that for this g, and for a compact $L$, and a number $C$ (proved in yet another exercise), if $B\subset int(L)$, then $V(A)=V[\mathbf{g}(B)]\leq C^n V(B)$. And the last part of the exercise is to find compact sets $L_1 \subset L_2 \subset \dots$ with union $D$. We know that $\mathbf{g}^{-1}$ will also have the Jacobian different from zero. So, I was thinking of using this last theorem this time to $\mathbf{g}^{-1}$, resulting in a different upper bound number $Q$, we would get $V(B)=V[\mathbf{g}^{-1}(A)]\leq Q^n V(A)$... But this doesn't seem to help at all. I have no idea how to prove that it's measurable. The hints seem only help me prove what would be the measure of B (bounds of the measure) instead of trying to prove that B has a measure. I'm probably not understanding something... Any help would be appreciated. Thanks in advance. P.S.: This is an exercise taken from Wendell Fleming's Functions of several variables, page 216, exercise 8. (It uses exercise 6 and 7) http://en.bookfi.org/book/580162",,"['measure-theory', 'multivariable-calculus', 'lebesgue-measure']"
50,Show that $\left| \oint_{\partial D}fdx + gdy \right|^2 \leq (\text{Area}(D))\int_D \left( |\nabla f|^2 + |\nabla g|^2 \right) dx dy.$,Show that,\left| \oint_{\partial D}fdx + gdy \right|^2 \leq (\text{Area}(D))\int_D \left( |\nabla f|^2 + |\nabla g|^2 \right) dx dy.,"Let $\vec{F}=(f,g):\mathbb{R}^2\to \mathbb{R}^2$ be a smooth vector   field such that $|\vec{F}(x,y)|\to 0$ rapidly as $|(x,y)|\to \infty$,   and let $D$ denote a compact domain in $\mathbb{R}^2$ whose boundary   $\gamma$ is a smooth non-self-intersecting closed curve in   $\mathbb{R}^2$. Show that $$\left\vert  \oint_{\gamma}\left(f\,{\rm d}x + g\,{\rm d}y\right) \right\vert^{2}  \leq \left(\text{Area}\left(D\right)\right)\int_D \left(\left\vert\nabla f\right\vert^{2} + \left\vert\nabla g\right\vert^{2} \right)\, {\rm d}x\,{\rm d}y.$$ I start out by noting that $$\oint_{\gamma}fdx + gdy = \int_D (g_x - f_y) \,dx dy.$$ The $\text{Area}(D)$ part is crying out for me to use Cauchy-Schwarz, in the form $$|\langle \varphi,1\rangle_{L^2(D)}|^2 \leq \|\varphi\|^2_{L^2(D)}\|1\|^2_{L^2(D)}=\text{Area(D)} \|\varphi\|^2_{L^2(D)}$$ for some function $\varphi$. Now $$\left(\int_D(g_x-f_y)\,dxdy\right)^2\leq \left(\int_D|g_x-f_y|\,dxdy\right)^2\\[19pt] = \left|\langle g_x-f_y,1\rangle_{L^2(D)}\right|^2$$ and now it would remain to show that $$\int_{D}(g_x-f_y)^2 dx dy \leq \int_{D}f_x^2 + f_y^2 + g_x^2 + g_y^2\,dx dy,$$ but I don't think that is always true. Also I have not used the decay of $\vec{F}$...but I don't see how that's relevant, since $D$ is compact. Any ideas?","Let $\vec{F}=(f,g):\mathbb{R}^2\to \mathbb{R}^2$ be a smooth vector   field such that $|\vec{F}(x,y)|\to 0$ rapidly as $|(x,y)|\to \infty$,   and let $D$ denote a compact domain in $\mathbb{R}^2$ whose boundary   $\gamma$ is a smooth non-self-intersecting closed curve in   $\mathbb{R}^2$. Show that $$\left\vert  \oint_{\gamma}\left(f\,{\rm d}x + g\,{\rm d}y\right) \right\vert^{2}  \leq \left(\text{Area}\left(D\right)\right)\int_D \left(\left\vert\nabla f\right\vert^{2} + \left\vert\nabla g\right\vert^{2} \right)\, {\rm d}x\,{\rm d}y.$$ I start out by noting that $$\oint_{\gamma}fdx + gdy = \int_D (g_x - f_y) \,dx dy.$$ The $\text{Area}(D)$ part is crying out for me to use Cauchy-Schwarz, in the form $$|\langle \varphi,1\rangle_{L^2(D)}|^2 \leq \|\varphi\|^2_{L^2(D)}\|1\|^2_{L^2(D)}=\text{Area(D)} \|\varphi\|^2_{L^2(D)}$$ for some function $\varphi$. Now $$\left(\int_D(g_x-f_y)\,dxdy\right)^2\leq \left(\int_D|g_x-f_y|\,dxdy\right)^2\\[19pt] = \left|\langle g_x-f_y,1\rangle_{L^2(D)}\right|^2$$ and now it would remain to show that $$\int_{D}(g_x-f_y)^2 dx dy \leq \int_{D}f_x^2 + f_y^2 + g_x^2 + g_y^2\,dx dy,$$ but I don't think that is always true. Also I have not used the decay of $\vec{F}$...but I don't see how that's relevant, since $D$ is compact. Any ideas?",,"['integration', 'multivariable-calculus']"
51,Uniform Convergence/continuity,Uniform Convergence/continuity,,"Let $A$ be a compact set of $\mathbb{R}^n$ and $f$ continuous on $A$. Let $F=f_A$, and let $I_0$ be a cube containing $A$. Divide $I_0$ in $2^n$ equal subcubes $I_{1_1},\dots, I_{1_{2^n}}$.On $I_0$ we define $F_0=max f_A$.  We define $A_{k_i}=A\cap cl(I_{k_i})$ and $F_{1} (\mathbf{x})= \begin{cases} max f_{A_{1_i}}(x), & \text{if }x\in I_{1_i}\text{ and } A_{1_i}\text{ not empty}\\ 0, & \text{ otherwise} \end{cases}$. Now, we repeat the same division of subcubes in subsubcubes, and so on, and define, for each new division, $F_{k} (\mathbf{x})= \begin{cases} max f_{A_{k_i}}(x), & \text{if }x\in I_{k_i}\text{ and } A_{k_i}\text{ not empty}\\ 0, & \text{ otherwise} \end{cases}$. The exercise asks us to prove that $F_k$ converges uniformly to $f_A$ if and only if $f(\mathbf{x})=\mathbf{0} \ \ \forall \mathbf{x}\in fr(A)$ Exercise 10, section 5.5, Wendell Fleming's Functions of Several Variables. My first doubt is: Shouldn't the $F_k$ be defined in terms of $ cl(I_{k_i})$ instead of just $I_{k_i}$?Because for a given point in $A$, there will be a certain order where that point will belong to $fr(I_{k_i})$, where $F_k(\mathbf{x})=0 $ My second doubt is how do I solve this? I have no clue as to how to tackle this.  A hint would be just fine. Thanks.","Let $A$ be a compact set of $\mathbb{R}^n$ and $f$ continuous on $A$. Let $F=f_A$, and let $I_0$ be a cube containing $A$. Divide $I_0$ in $2^n$ equal subcubes $I_{1_1},\dots, I_{1_{2^n}}$.On $I_0$ we define $F_0=max f_A$.  We define $A_{k_i}=A\cap cl(I_{k_i})$ and $F_{1} (\mathbf{x})= \begin{cases} max f_{A_{1_i}}(x), & \text{if }x\in I_{1_i}\text{ and } A_{1_i}\text{ not empty}\\ 0, & \text{ otherwise} \end{cases}$. Now, we repeat the same division of subcubes in subsubcubes, and so on, and define, for each new division, $F_{k} (\mathbf{x})= \begin{cases} max f_{A_{k_i}}(x), & \text{if }x\in I_{k_i}\text{ and } A_{k_i}\text{ not empty}\\ 0, & \text{ otherwise} \end{cases}$. The exercise asks us to prove that $F_k$ converges uniformly to $f_A$ if and only if $f(\mathbf{x})=\mathbf{0} \ \ \forall \mathbf{x}\in fr(A)$ Exercise 10, section 5.5, Wendell Fleming's Functions of Several Variables. My first doubt is: Shouldn't the $F_k$ be defined in terms of $ cl(I_{k_i})$ instead of just $I_{k_i}$?Because for a given point in $A$, there will be a certain order where that point will belong to $fr(I_{k_i})$, where $F_k(\mathbf{x})=0 $ My second doubt is how do I solve this? I have no clue as to how to tackle this.  A hint would be just fine. Thanks.",,['multivariable-calculus']
52,Proof of solid angle theorem,Proof of solid angle theorem,,"I have a homework problem to prove about the solid angle. The book says: Let S be a smooth parametric surface and let P be a point such   that each line that starts at P intersects S at most once. The solid angle Ω(S) subteded by S at P is the set of lines starting at P and passing through S . Let S(a) be the   intersection of Ω(S) with the surface of the sphere with center P and radius a . Then the measure of the solid angle (in steardians )   is defined to be $$ |Ω(S)| = \frac{\text{area of }S(a)}{a^2}$$ Apply the Divergence Theorem to the part of Ω(S) between S(a) and S to show that $$ |Ω(S)| = \iint_S \frac{\mathbf r \cdot \mathbf n}{r^3} dS$$ where r is the radius vector from P to any point on S , r = r , and the unit normal vector n is directed away from P . First let me say that it is O.K. for me to ask for some help here according to my university rules. I try to do this problem but have some questions. First, problem says it is the set of lines starting at P but should this be rays ? It seems to me like in the case of a sphere there would be no lines that start at P and intersect the sphere at most once if the point is inside the circle, because each line can go in to directions. So I assume it is rays. After that I have some questions like, how am I supposed to calculate the area of S(a) ? I know that the area of S(a) is equal to $\iint_{S(a)} dS$ but I have to get that as an integral of S , not S(a) . I had the idea that because I know it intersects at most once I could map each point on S onto S(a) , but I don't know how to do that. I thought maybe could just divide by the radius of the sphere a but that clearly doesn't work. I think this is an interesting problem and could be fun to solve but I don't know where to start. Could you please give a hint -- NOT the whole answer? By the way this is Stewart's Calculus 7e on page 1163, the ""Problems Plus"" problem #1. Sorry for my bad English and thanks in advance! :) EDIT I have seen on Wikipedia ( http://en.wikipedia.org/wiki/Solid_angle ) that it says $$Ω = \iint_S \frac{\mathbf r \cdot \hat n}{r^3} dS$$ which means that this is correct, but it does not say how the proof works. It says ""can be calculated as the surface integral"" but does not explain how ""can be calculated"" is true. Could someone elaborate here?","I have a homework problem to prove about the solid angle. The book says: Let S be a smooth parametric surface and let P be a point such   that each line that starts at P intersects S at most once. The solid angle Ω(S) subteded by S at P is the set of lines starting at P and passing through S . Let S(a) be the   intersection of Ω(S) with the surface of the sphere with center P and radius a . Then the measure of the solid angle (in steardians )   is defined to be $$ |Ω(S)| = \frac{\text{area of }S(a)}{a^2}$$ Apply the Divergence Theorem to the part of Ω(S) between S(a) and S to show that $$ |Ω(S)| = \iint_S \frac{\mathbf r \cdot \mathbf n}{r^3} dS$$ where r is the radius vector from P to any point on S , r = r , and the unit normal vector n is directed away from P . First let me say that it is O.K. for me to ask for some help here according to my university rules. I try to do this problem but have some questions. First, problem says it is the set of lines starting at P but should this be rays ? It seems to me like in the case of a sphere there would be no lines that start at P and intersect the sphere at most once if the point is inside the circle, because each line can go in to directions. So I assume it is rays. After that I have some questions like, how am I supposed to calculate the area of S(a) ? I know that the area of S(a) is equal to $\iint_{S(a)} dS$ but I have to get that as an integral of S , not S(a) . I had the idea that because I know it intersects at most once I could map each point on S onto S(a) , but I don't know how to do that. I thought maybe could just divide by the radius of the sphere a but that clearly doesn't work. I think this is an interesting problem and could be fun to solve but I don't know where to start. Could you please give a hint -- NOT the whole answer? By the way this is Stewart's Calculus 7e on page 1163, the ""Problems Plus"" problem #1. Sorry for my bad English and thanks in advance! :) EDIT I have seen on Wikipedia ( http://en.wikipedia.org/wiki/Solid_angle ) that it says $$Ω = \iint_S \frac{\mathbf r \cdot \hat n}{r^3} dS$$ which means that this is correct, but it does not say how the proof works. It says ""can be calculated as the surface integral"" but does not explain how ""can be calculated"" is true. Could someone elaborate here?",,"['multivariable-calculus', 'solid-angle']"
53,Knowing when to use Green/Stokes/Divergence theorem to evaluate line/surface integrals,Knowing when to use Green/Stokes/Divergence theorem to evaluate line/surface integrals,,"$\newcommand{\mbf}{\mathbf}$ Evaluate  $$ \iint \limits_{S} \mbf{F} \cdot d \mbf{S} $$ where $\mbf{F} = 3xy^2 \mbf{i} + 3x^2y \mbf{j} + z^3 \mbf{k}$ and $S$ is the surface of the unit sphere. I have written my solution below -- is it correct?  Also, how does one know when to use Green/Stokes/Divergence theorem to evaluate a surface/line integral? Note that evaluating this surface integral directly yields an ugly integral of $\int \sin^5(x)\, dx$.  We will resort to Gauss' Divergence Theorem to simplify matters. Indeed, by Gauss' Divergence Theorem, we have $$ \iint \limits_{S} \mbf{F} \cdot d \mbf{S} = \iiint \limits_{S_{\text{int}}} (\nabla \cdot \mbf{F}) \, dV. $$ where $S_{\text{int}}$ denotes the interior of the sphere. Now, $\nabla \cdot \mathbf{F} = 3(x^2+y^2+z^2)$.  Now, we are just integrating $$ \iiint \limits_{S_{\text{int}}} 3(x^2+y^2+z^2) \, dV. $$ Let's transform this to spherical coordinates.  We obtain the equivalent integral $$ \int_{0}^{2\pi} \int_{0}^{\pi} \int_{0}^{1} 3 \rho^4 \sin \phi \, d \rho \, d \phi \, d \theta. $$ $$ = \frac{3}{5} \int_{0}^{2 \pi} \int_{0}^{\pi} \sin \phi \, d \phi \, d \theta $$ $$ = \frac{12 \pi}{5}. $$","$\newcommand{\mbf}{\mathbf}$ Evaluate  $$ \iint \limits_{S} \mbf{F} \cdot d \mbf{S} $$ where $\mbf{F} = 3xy^2 \mbf{i} + 3x^2y \mbf{j} + z^3 \mbf{k}$ and $S$ is the surface of the unit sphere. I have written my solution below -- is it correct?  Also, how does one know when to use Green/Stokes/Divergence theorem to evaluate a surface/line integral? Note that evaluating this surface integral directly yields an ugly integral of $\int \sin^5(x)\, dx$.  We will resort to Gauss' Divergence Theorem to simplify matters. Indeed, by Gauss' Divergence Theorem, we have $$ \iint \limits_{S} \mbf{F} \cdot d \mbf{S} = \iiint \limits_{S_{\text{int}}} (\nabla \cdot \mbf{F}) \, dV. $$ where $S_{\text{int}}$ denotes the interior of the sphere. Now, $\nabla \cdot \mathbf{F} = 3(x^2+y^2+z^2)$.  Now, we are just integrating $$ \iiint \limits_{S_{\text{int}}} 3(x^2+y^2+z^2) \, dV. $$ Let's transform this to spherical coordinates.  We obtain the equivalent integral $$ \int_{0}^{2\pi} \int_{0}^{\pi} \int_{0}^{1} 3 \rho^4 \sin \phi \, d \rho \, d \phi \, d \theta. $$ $$ = \frac{3}{5} \int_{0}^{2 \pi} \int_{0}^{\pi} \sin \phi \, d \phi \, d \theta $$ $$ = \frac{12 \pi}{5}. $$",,['multivariable-calculus']
54,Triple integral using spherical coordinates,Triple integral using spherical coordinates,,"The following function is given: $$\iiint_{x^2+y^2+z^2\leq z} \sqrt{x^2+y^2+z^2}dx\,dy\,dz$$ And I have to calculate this integral using spherical coordinates. The substitutions are standard, I think, but I am having a problem with the limits. $$0\leq\phi\leq\pi$$$$0\leq\theta\leq2\pi$$ are the limits for the angles. I am not able to determine the limits for $\rho$ defined as $$\rho=\sqrt{x^2+y^2+z^2}$$ I tried it with $\rho\cos(\phi)$ as the upper limit but it didn't work.","The following function is given: $$\iiint_{x^2+y^2+z^2\leq z} \sqrt{x^2+y^2+z^2}dx\,dy\,dz$$ And I have to calculate this integral using spherical coordinates. The substitutions are standard, I think, but I am having a problem with the limits. $$0\leq\phi\leq\pi$$$$0\leq\theta\leq2\pi$$ are the limits for the angles. I am not able to determine the limits for $\rho$ defined as $$\rho=\sqrt{x^2+y^2+z^2}$$ I tried it with $\rho\cos(\phi)$ as the upper limit but it didn't work.",,['multivariable-calculus']
55,Multivariable calculus- Two tangent circles,Multivariable calculus- Two tangent circles,,"Another question from a midterm: Let $f:\mathbb{R}^3 \to \mathbb{R} $ be differentiable.  It is also given that $f$ is constant on the following two spheres: $ S_1 = \{(x,y,z)|x^2 + y^2 +z^2 =1\} $ and $ S_2 = \{(x,y,z)| (x-1)^2 + (y-1)^2 + (z-1)^2 =1\} $   . A. prove that on every point $(x,y,z)\in S_1 $ we have that $\nabla f(x,y,z)$ is a scalar multiple   of $(x,y,z)$ . B. Prove that there must exist a point on $S_1 \cup S_2 $ on which $\nabla f =0$ . Will someone please help me ? Thanks !","Another question from a midterm: Let $f:\mathbb{R}^3 \to \mathbb{R} $ be differentiable.  It is also given that $f$ is constant on the following two spheres: $ S_1 = \{(x,y,z)|x^2 + y^2 +z^2 =1\} $ and $ S_2 = \{(x,y,z)| (x-1)^2 + (y-1)^2 + (z-1)^2 =1\} $   . A. prove that on every point $(x,y,z)\in S_1 $ we have that $\nabla f(x,y,z)$ is a scalar multiple   of $(x,y,z)$ . B. Prove that there must exist a point on $S_1 \cup S_2 $ on which $\nabla f =0$ . Will someone please help me ? Thanks !",,['multivariable-calculus']
56,"Implicit function theorem, system of equations","Implicit function theorem, system of equations",,"Problem statement Given the function: $$F(x,y,z) = (x^2y^3 + yz^3, xy^2 + y^3z^3) = (0,0)$$ Show that $F(x,y,z)$ is an implicit function $f: \mathbb{R} ^2 \to \mathbb{R}$, that is, $(x,y)^T = f(z)$, at the point $(1,1,-1)$ and find $f'(-1)$ if it exists. Attempt at a solution Define $g(x,y,z) = x^2y^3 + yz^3 = 0$ and $h(x,y,z) = xy^2 + y^3z^3 = 0$. Then $$\begin{align} z_{x_1} = -\frac{g_x}{g_z} &= -\frac{2xy^3}{3yz^2} \\  z_{y_1} = -\frac{g_y}{g_z} &= -\frac{3x^2y^2 + z^3}{3yz^2} \\ z_{x_2} = -\frac{h_x}{h_z} &= -\frac{y^2}{3y^3z^2} \\  z_{y_2} = -\frac{h_y}{h_z} &= -\frac{2xy + 3y^2z^3}{3y^3z^2} \end{align}$$ At the point $(1,1,-1)$ both $g_z$ and $h_z$ are non-zero so $F(x,y,z)$ is an implicit function. And now I'm stuck because plugging $(1,1,-1)$ into the equations of above yields different results for $f'(-1)$ which makes me believe I've done something wrong. Any help is appreciated. EDIT: I think I get it now. I didn't need to find $z_x$ etc as from above but rather find: $$\begin{align} \frac{dz}{dx} &= -\left. \frac{\frac{d(f,g)}{d(x,y)}}{\frac{d(f,g)}{d(z,y)}} \right| _{(1,1,-1)} \\\end{align}$$ Am I now on the right track?","Problem statement Given the function: $$F(x,y,z) = (x^2y^3 + yz^3, xy^2 + y^3z^3) = (0,0)$$ Show that $F(x,y,z)$ is an implicit function $f: \mathbb{R} ^2 \to \mathbb{R}$, that is, $(x,y)^T = f(z)$, at the point $(1,1,-1)$ and find $f'(-1)$ if it exists. Attempt at a solution Define $g(x,y,z) = x^2y^3 + yz^3 = 0$ and $h(x,y,z) = xy^2 + y^3z^3 = 0$. Then $$\begin{align} z_{x_1} = -\frac{g_x}{g_z} &= -\frac{2xy^3}{3yz^2} \\  z_{y_1} = -\frac{g_y}{g_z} &= -\frac{3x^2y^2 + z^3}{3yz^2} \\ z_{x_2} = -\frac{h_x}{h_z} &= -\frac{y^2}{3y^3z^2} \\  z_{y_2} = -\frac{h_y}{h_z} &= -\frac{2xy + 3y^2z^3}{3y^3z^2} \end{align}$$ At the point $(1,1,-1)$ both $g_z$ and $h_z$ are non-zero so $F(x,y,z)$ is an implicit function. And now I'm stuck because plugging $(1,1,-1)$ into the equations of above yields different results for $f'(-1)$ which makes me believe I've done something wrong. Any help is appreciated. EDIT: I think I get it now. I didn't need to find $z_x$ etc as from above but rather find: $$\begin{align} \frac{dz}{dx} &= -\left. \frac{\frac{d(f,g)}{d(x,y)}}{\frac{d(f,g)}{d(z,y)}} \right| _{(1,1,-1)} \\\end{align}$$ Am I now on the right track?",,"['calculus', 'multivariable-calculus']"
57,Find the surface integral of $f=|x|-|y|$ over the part of $z=1-\frac{x^2}{M}-\frac{y^2}{N}$ inside a cylinder.,Find the surface integral of  over the part of  inside a cylinder.,f=|x|-|y| z=1-\frac{x^2}{M}-\frac{y^2}{N},"(a) Find the surface integral of $f=|x|-|y|$ over the part of $z=1-\frac{x^2}{M}-\frac{y^2}{N}$ inside the region $\frac{x^2}{M^2}+\frac{y^2}{N^2}=1$ (b) Find the surface integral of $f=|xy|$ over the part of $z=1-\frac{x^2}{M}-\frac{y^2}{N}$ inside the region $\frac{x^2}{M^2}+\frac{y^2}{N^2}=1$ I am assuming that if I can solve (a) I will be able to use a similar method to solve (b). So for (a), I have tried a change of variables using $x=Mr$cos$t$, $y=Nr$sin$t$ and $z=1-Mr^2$cos$^2t-Nr^2$sin$^2t$. Then I found the Jacobian to be $J=MNr$. So $dxdy=MNrdrdt$. So then, the surface area element $=MNr\sqrt{1+4r^2}drdt$ Then, to avoid the absolute value functions, I split $f$ into a piecewise formula: for $0\leq t\leq \frac{\pi}{2}$, $f=Mr$cos$t-Nr$sin$t$ for $\frac{\pi}{2}\leq t\leq \pi$, $f=-Mr$cos$t-Nr$sin$t$ for $\pi\leq t\leq \frac{3\pi}{2}$, $f=-Mr$cos$t+Nr$sin$t$ for $\frac{3\pi}{2}0\leq t\leq 2\pi$, $f=Mr$cos$t+Nr$sin$t$ and finally I tried to evaluate DESIRED INTEGRAL $=\int_0^\frac{\pi}{2} \int_0^1 (Mr$cos$t-Nr$sin$t)MNr\sqrt{1+4r^2}drdt+$ $+ \int_\frac{\pi}{2}^\pi \int_0^1 (-Mr$cos$t-Nr$sin$t)MNr\sqrt{1+4r^2}drdt+$ $+ \int_\pi^\frac{3\pi}{2} \int_0^1 (-Mr$cos$t+Nr$sin$t)MNr\sqrt{1+4r^2}drdt+$ $+ \int_\frac{3\pi}{2}^2\pi \int_0^1 (Mr$cos$t+Nr$sin$t)MNr\sqrt{1+4r^2}drdt $ which can be done and I have no trouble doing. Does this method work at all? Have I made errors? Or do you have a better idea?","(a) Find the surface integral of $f=|x|-|y|$ over the part of $z=1-\frac{x^2}{M}-\frac{y^2}{N}$ inside the region $\frac{x^2}{M^2}+\frac{y^2}{N^2}=1$ (b) Find the surface integral of $f=|xy|$ over the part of $z=1-\frac{x^2}{M}-\frac{y^2}{N}$ inside the region $\frac{x^2}{M^2}+\frac{y^2}{N^2}=1$ I am assuming that if I can solve (a) I will be able to use a similar method to solve (b). So for (a), I have tried a change of variables using $x=Mr$cos$t$, $y=Nr$sin$t$ and $z=1-Mr^2$cos$^2t-Nr^2$sin$^2t$. Then I found the Jacobian to be $J=MNr$. So $dxdy=MNrdrdt$. So then, the surface area element $=MNr\sqrt{1+4r^2}drdt$ Then, to avoid the absolute value functions, I split $f$ into a piecewise formula: for $0\leq t\leq \frac{\pi}{2}$, $f=Mr$cos$t-Nr$sin$t$ for $\frac{\pi}{2}\leq t\leq \pi$, $f=-Mr$cos$t-Nr$sin$t$ for $\pi\leq t\leq \frac{3\pi}{2}$, $f=-Mr$cos$t+Nr$sin$t$ for $\frac{3\pi}{2}0\leq t\leq 2\pi$, $f=Mr$cos$t+Nr$sin$t$ and finally I tried to evaluate DESIRED INTEGRAL $=\int_0^\frac{\pi}{2} \int_0^1 (Mr$cos$t-Nr$sin$t)MNr\sqrt{1+4r^2}drdt+$ $+ \int_\frac{\pi}{2}^\pi \int_0^1 (-Mr$cos$t-Nr$sin$t)MNr\sqrt{1+4r^2}drdt+$ $+ \int_\pi^\frac{3\pi}{2} \int_0^1 (-Mr$cos$t+Nr$sin$t)MNr\sqrt{1+4r^2}drdt+$ $+ \int_\frac{3\pi}{2}^2\pi \int_0^1 (Mr$cos$t+Nr$sin$t)MNr\sqrt{1+4r^2}drdt $ which can be done and I have no trouble doing. Does this method work at all? Have I made errors? Or do you have a better idea?",,"['integration', 'multivariable-calculus', 'surfaces']"
58,Invariance of Integration on Homotopic Curves,Invariance of Integration on Homotopic Curves,,"All: I'm trying to show that if curves $\gamma, \gamma'$ are homotopic to each other in some region $R$ (open, connected subset) of the plane, and f is differentiable in $R$ , then: $\int_{\gamma}f=\int_{\gamma'}f$ . Please critique this argument and help me finish: first, we construct a homotopy $H(x,t)$ between the two curves  ${\gamma},{\gamma'}$, with $H(x,0)=\gamma, H(x,1)=\gamma'$ then, define $M$ to be the set $H(x,t)$for $t$ in $(0,1)$, so that $M$ is the region bounded by the two curves. Now, we apply Stokes' thm. to get: $\int \int_M df=\int_{\partial M} f=\int_{\gamma}f-\int_{\gamma'}f$ Now, I need to have the integral be equal to zero, to get the equality $\int_{\gamma}f-\int_{\gamma'}f=0$ How can I do so? Is the rest correct? Thanks.","All: I'm trying to show that if curves $\gamma, \gamma'$ are homotopic to each other in some region $R$ (open, connected subset) of the plane, and f is differentiable in $R$ , then: $\int_{\gamma}f=\int_{\gamma'}f$ . Please critique this argument and help me finish: first, we construct a homotopy $H(x,t)$ between the two curves  ${\gamma},{\gamma'}$, with $H(x,0)=\gamma, H(x,1)=\gamma'$ then, define $M$ to be the set $H(x,t)$for $t$ in $(0,1)$, so that $M$ is the region bounded by the two curves. Now, we apply Stokes' thm. to get: $\int \int_M df=\int_{\partial M} f=\int_{\gamma}f-\int_{\gamma'}f$ Now, I need to have the integral be equal to zero, to get the equality $\int_{\gamma}f-\int_{\gamma'}f=0$ How can I do so? Is the rest correct? Thanks.",,"['general-topology', 'multivariable-calculus', 'differential-geometry']"
59,How to calculate hard integral?,How to calculate hard integral?,,"How to calculate the integral $$ \int_D \frac {\prod_{i<j}(a_i-a_j)^2\prod_{i<j}(b_i-b_j)^2}  {\prod_{i,j} (a_i+b_j)^2}\,d\lambda_{2n-1},$$ where $$D:=\{ (a_1,\dots a_n,b_1\dots,b_n):a_1\ge\,a_2\ge\dots \ge a_n\ge 0,b_1\ge\,b_2\ge\dots \ge b_n\ge 0,\sum_i a_i +\sum_j b_j =1\}$$ and $\lambda_{2n-1} $ - the $(2n-1)$dimensional Lebesgue measure?","How to calculate the integral $$ \int_D \frac {\prod_{i<j}(a_i-a_j)^2\prod_{i<j}(b_i-b_j)^2}  {\prod_{i,j} (a_i+b_j)^2}\,d\lambda_{2n-1},$$ where $$D:=\{ (a_1,\dots a_n,b_1\dots,b_n):a_1\ge\,a_2\ge\dots \ge a_n\ge 0,b_1\ge\,b_2\ge\dots \ge b_n\ge 0,\sum_i a_i +\sum_j b_j =1\}$$ and $\lambda_{2n-1} $ - the $(2n-1)$dimensional Lebesgue measure?",,"['multivariable-calculus', 'integration']"
60,Working with projection of areas?,Working with projection of areas?,,"I was recently solving a physics problem which had to do with the momentum imparted by a photon beam to a perfectly absorbing sphere and a perfectly reflecting one. Considering the former and Putting aside the physical part of the question, I needed to do the following:- Find the net momentum per unit area of the beam $I/c$ and multiply this with a small area on my sphere to ultimately integrate over half a sphere to get the net momentum. My efforts Consider a sphere of $radius=r$ placed with the center on origin of the co-ordinate axes. Let the position vector characterizing the sphere be $$\vec r=r \cos \theta \hat z+r \sin \theta \cos \phi \hat x+ r \sin \theta \sin \phi \hat y$$ Where $\theta$ is the angle of the position vector with $z$ axis and $\phi$ the angle of the projection of the position vector in x-y plane with x axis. Now, suppose the beam is falling along the $x$axis. Therefore at a point $(x,y.z)$ the incoming beam will make an angle $\phi$ with the normal at that point. Therefore, for an area $da=r^2 \sin \theta d\theta d\phi$, the momentum imparted will be $\frac{I}{c} da \cos \phi (-\hat x)$, since the area of the beam imparting momentum on the area $da$ is only $da \cos \phi$ normal to the beam. Therefore the net momentum should be $$\int_0^\pi \int_{-\frac \pi 2}^{\frac \pi 2} \frac {r^2I}c \cos \phi \sin \theta d\phi d\theta (-\hat x)$$  But that evaluates to $\frac {4r^2I}c$. But the answer and intuitive understanding ($\pi r^2$ area of the beam is intercepted and absorbed so the net momentum is this area times the momentum per unit area) both gives $\pi r^2I/c$ as the answer. Where did I go wrong? The same problem arises if we consider a reflecting sphere. I guess the problem is in dealing with the projected areas, i.e the projection of $da$ as $da \cos \phi$ normal to the direction of the incoming beam. Sorry if this has a trivial mistake. Can anyone help me with solving the perfectly reflecting case properly? I do not know how to proceed therein appropriately ( in reflection, the laws of reflection need to be followed and the given answer is the same as calculated for the absorption case)","I was recently solving a physics problem which had to do with the momentum imparted by a photon beam to a perfectly absorbing sphere and a perfectly reflecting one. Considering the former and Putting aside the physical part of the question, I needed to do the following:- Find the net momentum per unit area of the beam $I/c$ and multiply this with a small area on my sphere to ultimately integrate over half a sphere to get the net momentum. My efforts Consider a sphere of $radius=r$ placed with the center on origin of the co-ordinate axes. Let the position vector characterizing the sphere be $$\vec r=r \cos \theta \hat z+r \sin \theta \cos \phi \hat x+ r \sin \theta \sin \phi \hat y$$ Where $\theta$ is the angle of the position vector with $z$ axis and $\phi$ the angle of the projection of the position vector in x-y plane with x axis. Now, suppose the beam is falling along the $x$axis. Therefore at a point $(x,y.z)$ the incoming beam will make an angle $\phi$ with the normal at that point. Therefore, for an area $da=r^2 \sin \theta d\theta d\phi$, the momentum imparted will be $\frac{I}{c} da \cos \phi (-\hat x)$, since the area of the beam imparting momentum on the area $da$ is only $da \cos \phi$ normal to the beam. Therefore the net momentum should be $$\int_0^\pi \int_{-\frac \pi 2}^{\frac \pi 2} \frac {r^2I}c \cos \phi \sin \theta d\phi d\theta (-\hat x)$$  But that evaluates to $\frac {4r^2I}c$. But the answer and intuitive understanding ($\pi r^2$ area of the beam is intercepted and absorbed so the net momentum is this area times the momentum per unit area) both gives $\pi r^2I/c$ as the answer. Where did I go wrong? The same problem arises if we consider a reflecting sphere. I guess the problem is in dealing with the projected areas, i.e the projection of $da$ as $da \cos \phi$ normal to the direction of the incoming beam. Sorry if this has a trivial mistake. Can anyone help me with solving the perfectly reflecting case properly? I do not know how to proceed therein appropriately ( in reflection, the laws of reflection need to be followed and the given answer is the same as calculated for the absorption case)",,"['calculus', 'multivariable-calculus', 'definite-integrals', 'surfaces']"
61,"Formal proof of $h(x,y)=f(x)+g(y)$ has a critical point $(x_0,y_0)$ iff $x_0$ is a critical point of f and $y_0$ is a critical point of g",Formal proof of  has a critical point  iff  is a critical point of f and  is a critical point of g,"h(x,y)=f(x)+g(y) (x_0,y_0) x_0 y_0","The question is basically all in the subject, so I'll just copy and paste and move on to what I've tried. Formal proof of $h(x,y)=f(x)+g(y)$ has a critical point $(x_0,y_0)$ iff $x_0$ is a critical point of f and $y_0$ is a critical point of g Extra: $f:\mathbb{R}\rightarrow\mathbb{R}$ and the same for g Firstly: It's obviously true, this took me by surprise because I'd have taken this without proof, I may have even used it before and considered it too trivial to question, f's change is independent of g's because one is in x and the other y. There should be a name for how useful of a property this is. I'm not even sure how to word it, you can think of the partials a the derivative of f along the x axis and derivative of g along the y axis, if one is turning (I sometimes call critical points turning points, it's how I was taught, is this wrong?) and the other isn't it's obviously not a critical point of the surface. It's difficult to explain, I just hope it's not so obscure someone suspects I'm bluffing, or that this is a part of that bluff and so forth, I hope no body things I have an odd numbered bluff! I'm a little thrown off by the if and only if part. I could use the second derivative test (and use the fact that $h_xy$ and $h_yx$ are both zero) but this is actually part 2 of the question, and if that were 0 requires further investigation involving the third derivatives, if that's 0, the fourth, I've proved this by induction before but only for single variables, the question is only worth 4 marks ~ 8 minutes at most. So yes, how would I prove this? additionally (this probably should have gone first) It does say critical, and the definition of that is ""first derivatives being zero"", I can show if this is true then it's a turning point? Then assume both derivatives are zero, and, well not sure, I thought integrate but I'm not entirely sure what I'd be integrating over, it's very hard to go back, I could say ""then the tangent plane is flat"" but then I'd have to prove if the tangent plane is flat that it's a critical point, I'm not certain of this (I know critical point => tangent plane is flat and I can't think of a counter example going the other way, in-fact the tangent plane is defined by the first derivatives, so perhaps this is the proof? It's fairly easy to show) Addendum No wonder I'm so muddled, no one looks at $f(x,y)=x+y$ as the sum of two functions, it's like the obvious limit ($\lim_{x\rightarrow a}(x)=a$) only I could prove that, is it not ""true because of the definition we use?"" and again, what are the both ways, the ""if then"" and ""only then"" parts?","The question is basically all in the subject, so I'll just copy and paste and move on to what I've tried. Formal proof of $h(x,y)=f(x)+g(y)$ has a critical point $(x_0,y_0)$ iff $x_0$ is a critical point of f and $y_0$ is a critical point of g Extra: $f:\mathbb{R}\rightarrow\mathbb{R}$ and the same for g Firstly: It's obviously true, this took me by surprise because I'd have taken this without proof, I may have even used it before and considered it too trivial to question, f's change is independent of g's because one is in x and the other y. There should be a name for how useful of a property this is. I'm not even sure how to word it, you can think of the partials a the derivative of f along the x axis and derivative of g along the y axis, if one is turning (I sometimes call critical points turning points, it's how I was taught, is this wrong?) and the other isn't it's obviously not a critical point of the surface. It's difficult to explain, I just hope it's not so obscure someone suspects I'm bluffing, or that this is a part of that bluff and so forth, I hope no body things I have an odd numbered bluff! I'm a little thrown off by the if and only if part. I could use the second derivative test (and use the fact that $h_xy$ and $h_yx$ are both zero) but this is actually part 2 of the question, and if that were 0 requires further investigation involving the third derivatives, if that's 0, the fourth, I've proved this by induction before but only for single variables, the question is only worth 4 marks ~ 8 minutes at most. So yes, how would I prove this? additionally (this probably should have gone first) It does say critical, and the definition of that is ""first derivatives being zero"", I can show if this is true then it's a turning point? Then assume both derivatives are zero, and, well not sure, I thought integrate but I'm not entirely sure what I'd be integrating over, it's very hard to go back, I could say ""then the tangent plane is flat"" but then I'd have to prove if the tangent plane is flat that it's a critical point, I'm not certain of this (I know critical point => tangent plane is flat and I can't think of a counter example going the other way, in-fact the tangent plane is defined by the first derivatives, so perhaps this is the proof? It's fairly easy to show) Addendum No wonder I'm so muddled, no one looks at $f(x,y)=x+y$ as the sum of two functions, it's like the obvious limit ($\lim_{x\rightarrow a}(x)=a$) only I could prove that, is it not ""true because of the definition we use?"" and again, what are the both ways, the ""if then"" and ""only then"" parts?",,"['calculus', 'multivariable-calculus']"
62,Multivariable calculus: optimizing for shortest path along a curvy plane?,Multivariable calculus: optimizing for shortest path along a curvy plane?,,"I want to write a computer program which can help me spend the least amount of energy and time walking between locations on my university campus. My campus is very hilly, and it is also extremely hot during the early fall, so I would like to optimize for the least energy used. I want my program to answer a few questions: When is walking uphill for a short distance more favorable than walking a longer roundabout path that is flatter or downhill? Walking distances causes internal temperature to rise, but so does standing in the sun for too long. Which path on a certain day with given weather inputs (sunny, hot) would cause the least amount of sweating? (note: Only educated up to single-variable calculus.) This animated image seems to be visually what I am going for: I have a feeling this is a simple calculus question where I need to choose different paths. For each path, I need to find the integral of the angle of elevation and then compare it to the last path I calculated, then the lowest number is my answer. How do I formulate this integral?","I want to write a computer program which can help me spend the least amount of energy and time walking between locations on my university campus. My campus is very hilly, and it is also extremely hot during the early fall, so I would like to optimize for the least energy used. I want my program to answer a few questions: When is walking uphill for a short distance more favorable than walking a longer roundabout path that is flatter or downhill? Walking distances causes internal temperature to rise, but so does standing in the sun for too long. Which path on a certain day with given weather inputs (sunny, hot) would cause the least amount of sweating? (note: Only educated up to single-variable calculus.) This animated image seems to be visually what I am going for: I have a feeling this is a simple calculus question where I need to choose different paths. For each path, I need to find the integral of the angle of elevation and then compare it to the last path I calculated, then the lowest number is my answer. How do I formulate this integral?",,"['calculus', 'multivariable-calculus', 'integration', 'graph-theory', 'optimization']"
63,local parametrization of regular surface,local parametrization of regular surface,,"I am doing excercises of Do Carmo's dg of curves and surfaces Chapter 2.2 and need some help with the following excercise: Show that the set $S=\{(x,y,z)\in R^3;z=x^2-y^2\}$ a regular surface and check (a) and (b) are parametrizations of $S$. Also : answer: which part of S do these parametrizations  cover? (a) $x(u,v)=(u+v,u-v,4uv),(u,v)\in R^2$ (b) $x(u,v)=(u \cosh v, u \sinh v, u^2), (u,v)\in R^2 , u\neq 0$ Part (a) is straightforward. Since S is a regular surface, by prop 2.2.4 it suffices to chek x is differentiable , one-to-one and  and differential is one-to-one. And it's quite straightforward to check these properties. Also, a should cover the whole $S$. However, for (b) although x is differentiable and I can check that dx is one-to-one, x is simply not one-to-one and hence not a homeomorphism. I am wondering if there's any other definition for local coordinates (or parametrization) so that we don't need one-to-one propert. For example , I only need to show it is locally homeomorphic.Also, I am not quite sure which part of $S$ that x in (b) covers?","I am doing excercises of Do Carmo's dg of curves and surfaces Chapter 2.2 and need some help with the following excercise: Show that the set $S=\{(x,y,z)\in R^3;z=x^2-y^2\}$ a regular surface and check (a) and (b) are parametrizations of $S$. Also : answer: which part of S do these parametrizations  cover? (a) $x(u,v)=(u+v,u-v,4uv),(u,v)\in R^2$ (b) $x(u,v)=(u \cosh v, u \sinh v, u^2), (u,v)\in R^2 , u\neq 0$ Part (a) is straightforward. Since S is a regular surface, by prop 2.2.4 it suffices to chek x is differentiable , one-to-one and  and differential is one-to-one. And it's quite straightforward to check these properties. Also, a should cover the whole $S$. However, for (b) although x is differentiable and I can check that dx is one-to-one, x is simply not one-to-one and hence not a homeomorphism. I am wondering if there's any other definition for local coordinates (or parametrization) so that we don't need one-to-one propert. For example , I only need to show it is locally homeomorphic.Also, I am not quite sure which part of $S$ that x in (b) covers?",,"['multivariable-calculus', 'differential-geometry', 'manifolds']"
64,Differentiability in $\mathbb{R}^n$: how to prove that $f'(a)v=g'(a)v$ for all $v \in \mathbb{R}^m$?,Differentiability in : how to prove that  for all ?,\mathbb{R}^n f'(a)v=g'(a)v v \in \mathbb{R}^m,"Let $f,g:U\to\mathbb{R}^n$ be differentiable at point $a\in U$, where $U\subset\mathbb{R}^m$ is an open set. Suppose $f(a)=g(a)$. Prove that $$\lim_{v\to0}\frac{f(a+v)-g(a+v)}{|v|}=0\;\;\;\;[\#]$$ if, and only if, $$f'(a)=g'(a)\;\;\;\;[*]$$ I've proved that $[*]\Rightarrow[\#]$ and I need help for the converse. Definition: Suppose $U$ is an open set in $\mathbb{R}^m$, $f$ maps $U$ into $\mathbb{R}^n$, and $a\in U$. If there exists a linear transformation $T:\mathbb{R}^m\to\mathbb{R}^n$ such that $$f(a+v)-f(a)=Ta+r(h), \;\text{where}\; \lim_{v\to 0}\frac{r(v)}{|v|},$$ then we say that $f$ is differentiable at $a$, and we write $T=f'(a)$. Thanks.","Let $f,g:U\to\mathbb{R}^n$ be differentiable at point $a\in U$, where $U\subset\mathbb{R}^m$ is an open set. Suppose $f(a)=g(a)$. Prove that $$\lim_{v\to0}\frac{f(a+v)-g(a+v)}{|v|}=0\;\;\;\;[\#]$$ if, and only if, $$f'(a)=g'(a)\;\;\;\;[*]$$ I've proved that $[*]\Rightarrow[\#]$ and I need help for the converse. Definition: Suppose $U$ is an open set in $\mathbb{R}^m$, $f$ maps $U$ into $\mathbb{R}^n$, and $a\in U$. If there exists a linear transformation $T:\mathbb{R}^m\to\mathbb{R}^n$ such that $$f(a+v)-f(a)=Ta+r(h), \;\text{where}\; \lim_{v\to 0}\frac{r(v)}{|v|},$$ then we say that $f$ is differentiable at $a$, and we write $T=f'(a)$. Thanks.",,['multivariable-calculus']
65,Taylor Polynomial in Multivariable Case,Taylor Polynomial in Multivariable Case,,"I am doing Problem 9.30 in Rudin's Principles of Mathematical Analysis and have done part (a) and (b) but got stuck on part(c). In part (b) I have finished the proof that: $$f(\mathbf a+\mathbf x)=\sum_{k=0}^{m-1}{1 \over{k!}}\sum (D_{i_1 \dots i_k}f)(\mathbf a)x_{i_1}\dots x_{i_k}+r(\mathbf x),(*)$$ where $$\lim_{x\to 0}{{r(\mathbf x)}\over{|\mathbf x|^{m-1}}}=0.$$ Now I need to show that the equation ($*$) is in fact equivalent to $$\sum{{(D_1^{s_1}\dots D_n^{s_n}f)(\mathbf a)}\over{s_1 !\dots s_n !}}x_1^{s_1}\dots x_n^{s_n}, $$where the summation extends over all ordered $n$-tuples $(s_1,\dots,s_n)$ such that each $s_i$ is a nonnegative integer and $s_1+\dots+s_n \le m-1$. Thanks in advance.","I am doing Problem 9.30 in Rudin's Principles of Mathematical Analysis and have done part (a) and (b) but got stuck on part(c). In part (b) I have finished the proof that: $$f(\mathbf a+\mathbf x)=\sum_{k=0}^{m-1}{1 \over{k!}}\sum (D_{i_1 \dots i_k}f)(\mathbf a)x_{i_1}\dots x_{i_k}+r(\mathbf x),(*)$$ where $$\lim_{x\to 0}{{r(\mathbf x)}\over{|\mathbf x|^{m-1}}}=0.$$ Now I need to show that the equation ($*$) is in fact equivalent to $$\sum{{(D_1^{s_1}\dots D_n^{s_n}f)(\mathbf a)}\over{s_1 !\dots s_n !}}x_1^{s_1}\dots x_n^{s_n}, $$where the summation extends over all ordered $n$-tuples $(s_1,\dots,s_n)$ such that each $s_i$ is a nonnegative integer and $s_1+\dots+s_n \le m-1$. Thanks in advance.",,"['real-analysis', 'multivariable-calculus', 'taylor-expansion']"
66,partial derivative notation question,partial derivative notation question,,"I'm reading a book called Correlated Data Analysis, Analytics, and Applications and I simply don't understand some notation.  The author says, in chapter 2, page 26: A unit deviance is called regular if function $d(y;u)$ is twice continuously differentiable with respect to $(y, \mu)$ on $\Omega  \times \Omega$ and satisfies   $$ \frac{ \partial^2 d}{\partial \mu^2}(y;y) = \frac{ \partial^2 d}{\partial \mu^2}(y;\mu) \Bigg|_{\mu=y} >0,\quad \forall y \in \Omega$$ Um... what does that notation mean?  On the left, I guess I take the second partial derivative then evaluate with y as both variables?  In the right case, I have no idea... Could someone share an example of a function where this would not be true? Thank you. Edit: you can view this in place by downloading this sample pdf (logical page 26, pdf page 4)","I'm reading a book called Correlated Data Analysis, Analytics, and Applications and I simply don't understand some notation.  The author says, in chapter 2, page 26: A unit deviance is called regular if function $d(y;u)$ is twice continuously differentiable with respect to $(y, \mu)$ on $\Omega  \times \Omega$ and satisfies   $$ \frac{ \partial^2 d}{\partial \mu^2}(y;y) = \frac{ \partial^2 d}{\partial \mu^2}(y;\mu) \Bigg|_{\mu=y} >0,\quad \forall y \in \Omega$$ Um... what does that notation mean?  On the left, I guess I take the second partial derivative then evaluate with y as both variables?  In the right case, I have no idea... Could someone share an example of a function where this would not be true? Thank you. Edit: you can view this in place by downloading this sample pdf (logical page 26, pdf page 4)",,"['calculus', 'multivariable-calculus', 'notation']"
67,Partial derivative equality,Partial derivative equality,,"I'm working through Colley's Vector Calculus 3rd edition to reacclimate and introduce myself to all things vectors and differential forms.  The following question is stumping me. Suppose $z=f(x,y)$ has continuous partial derivatives.  Let $x=e^r\cos \theta$ and $y=e^r\sin\theta$.  Show that $$\left(\frac{\partial{z}}{\partial{x}}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\right)^2=e^{-2r}\left[\left(\frac{\partial{z}}{\partial{r}}\right)^2+\left(\frac{\partial{z}}{\partial{\theta}}\right)^2\right]$$ So I almost have the answer but I keep getting a pesky $\cos^2\theta$. $$e^{-2r}\left[\left(\frac{\partial{z}}{\partial{r}}\right)^2+\left(\frac{\partial{z}}{\partial{\theta}}\right)^2\right]=e^{-2r}\left[\left(\frac{\partial{z}}{\partial{x}}\cdot{\frac{\partial{x}}{\partial{r}}}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\cdot{\frac{\partial{y}}{\partial{\theta}}}\right)^2\right]=e^{-2r}\left[\left(\frac{\partial{z}}{\partial{x}}\cdot{e^r\cos\theta}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\cdot{e^r\cos\theta}\right)^2\right]=\cos^2\theta\left[\left(\frac{\partial{z}}{\partial{x}}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\right)\right]^2$$ Where is my error?","I'm working through Colley's Vector Calculus 3rd edition to reacclimate and introduce myself to all things vectors and differential forms.  The following question is stumping me. Suppose $z=f(x,y)$ has continuous partial derivatives.  Let $x=e^r\cos \theta$ and $y=e^r\sin\theta$.  Show that $$\left(\frac{\partial{z}}{\partial{x}}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\right)^2=e^{-2r}\left[\left(\frac{\partial{z}}{\partial{r}}\right)^2+\left(\frac{\partial{z}}{\partial{\theta}}\right)^2\right]$$ So I almost have the answer but I keep getting a pesky $\cos^2\theta$. $$e^{-2r}\left[\left(\frac{\partial{z}}{\partial{r}}\right)^2+\left(\frac{\partial{z}}{\partial{\theta}}\right)^2\right]=e^{-2r}\left[\left(\frac{\partial{z}}{\partial{x}}\cdot{\frac{\partial{x}}{\partial{r}}}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\cdot{\frac{\partial{y}}{\partial{\theta}}}\right)^2\right]=e^{-2r}\left[\left(\frac{\partial{z}}{\partial{x}}\cdot{e^r\cos\theta}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\cdot{e^r\cos\theta}\right)^2\right]=\cos^2\theta\left[\left(\frac{\partial{z}}{\partial{x}}\right)^2+\left(\frac{\partial{z}}{\partial{y}}\right)\right]^2$$ Where is my error?",,"['multivariable-calculus', 'partial-derivative']"
68,Are there any strategy to solve this system of multivariate quadratic equation?,Are there any strategy to solve this system of multivariate quadratic equation?,,"Solve: $$ (\sum_{j=1}^{n}a_{ij}x_{j})(\sum_{j=1}^{n}a_{ij}y_{j})=0, \quad i=1,\cdots, 2n-1\\ \sum_{j=1}^{n}x_{j}y_{j}=0 $$ ,where $a_{ij}$ are known real constants and $x_{j}$ and $y_{j}$ are nonzero unknowns to be solved in $\mathbb{Q}$. The motivation of this question is trying to decompose a set of vectors $a_{i}$ into two hyperplanes with orthogonal normal vectors. P.S. Thanks to S.B.'s comment. In my specific application, $a_{ij}$ are generated so that a nonzero solution is guaranteed to exist and the number of $a_{i}$ always exceeds the dimension of the space in question. I know that they belong to two such subspaces, but I want to know which. Thanks very much!","Solve: $$ (\sum_{j=1}^{n}a_{ij}x_{j})(\sum_{j=1}^{n}a_{ij}y_{j})=0, \quad i=1,\cdots, 2n-1\\ \sum_{j=1}^{n}x_{j}y_{j}=0 $$ ,where $a_{ij}$ are known real constants and $x_{j}$ and $y_{j}$ are nonzero unknowns to be solved in $\mathbb{Q}$. The motivation of this question is trying to decompose a set of vectors $a_{i}$ into two hyperplanes with orthogonal normal vectors. P.S. Thanks to S.B.'s comment. In my specific application, $a_{ij}$ are generated so that a nonzero solution is guaranteed to exist and the number of $a_{i}$ always exceeds the dimension of the space in question. I know that they belong to two such subspaces, but I want to know which. Thanks very much!",,"['algebraic-geometry', 'multivariable-calculus']"
69,Attach term to solution of PDE(perturbation theory),Attach term to solution of PDE(perturbation theory),,"Currently I am struggeling with the following problem: Actually I have a found a solution to the PDE $\Delta \Phi(r,\theta)=f(r,\theta)$ and now I want to include a small extra term given by $kg(r)\Phi(r,\theta)$ a priori to the differential equation where $g(r)=1$, if $|r|>R\in \mathbb{R}$ so ""outside a sphere"" and g(r)=0 inside the sphere. Is there any way to approximatively attach this behavior to the solution to the differential equation a posteriori or do you see any other chance to do this approximatively? It might be worth saying that the solution of the differential equation is rather a complicated function. Since I do not want to busy you with the full calculation I was just wondering whether you know some equations(maybe perturbation theory) in order to do this right.","Currently I am struggeling with the following problem: Actually I have a found a solution to the PDE $\Delta \Phi(r,\theta)=f(r,\theta)$ and now I want to include a small extra term given by $kg(r)\Phi(r,\theta)$ a priori to the differential equation where $g(r)=1$, if $|r|>R\in \mathbb{R}$ so ""outside a sphere"" and g(r)=0 inside the sphere. Is there any way to approximatively attach this behavior to the solution to the differential equation a posteriori or do you see any other chance to do this approximatively? It might be worth saying that the solution of the differential equation is rather a complicated function. Since I do not want to busy you with the full calculation I was just wondering whether you know some equations(maybe perturbation theory) in order to do this right.",,"['calculus', 'real-analysis', 'functional-analysis', 'multivariable-calculus', 'partial-differential-equations']"
70,Double Integration.,Double Integration.,,"I have an integral $$\int_0 ^a\int_0 ^b\int_0 ^a\int_0 ^b \sin(x)\sin(\bar{x})\sin(y)\sin(\bar{y})f(x,\bar{x},y,\bar{y})~dx~dy~d\bar{x}~d\bar{y}$$  where $f= \dfrac{\sin\left(\sqrt{(x-\bar{x})^2+(y-\bar{y})^2}\right)}{(x-\bar{x})^2+(y-\bar{y})^2}$. Under the transformation $$\bar{x}=x+u,~~~\text{and}~~~\bar{y}=y+v$$ the function $f$ becomes $f(u,v)$. This integral results in $$\int_0 ^a\int_0 ^b[(a-u)\cos(u)+\sin(u)][(b-v)\cos(v)+\sin(v)]f(u,v) \, du \, dv$$ Can someone help me in proving this?","I have an integral $$\int_0 ^a\int_0 ^b\int_0 ^a\int_0 ^b \sin(x)\sin(\bar{x})\sin(y)\sin(\bar{y})f(x,\bar{x},y,\bar{y})~dx~dy~d\bar{x}~d\bar{y}$$  where $f= \dfrac{\sin\left(\sqrt{(x-\bar{x})^2+(y-\bar{y})^2}\right)}{(x-\bar{x})^2+(y-\bar{y})^2}$. Under the transformation $$\bar{x}=x+u,~~~\text{and}~~~\bar{y}=y+v$$ the function $f$ becomes $f(u,v)$. This integral results in $$\int_0 ^a\int_0 ^b[(a-u)\cos(u)+\sin(u)][(b-v)\cos(v)+\sin(v)]f(u,v) \, du \, dv$$ Can someone help me in proving this?",,"['integration', 'multivariable-calculus']"
71,Proper change of coordinates,Proper change of coordinates,,"It's a really easy one for you guys. I'm performing a simple cylindrical change of variable from Cartesian coordinates, but I want to write it out properly and I'm stuck with the differential matrices. I won't get into details on everything here but I'll focus on my issue. I consider the diffeomorphism $\Psi$ matching the cylindrical change of coordinates $\Psi : (r, \phi, z) \rightarrow (r\cos{\phi}, r\sin{\phi}, z)$ And I compute its jacobian $D\Psi$ as well as its inverse $(D\Psi)^{-1}$. Now, for any vector field $A : (x, y, z) \rightarrow (A_x(x, y, z), A_y(x, y, z), A_z(x, y, z))$ and $\hat{A}  : (r, \phi, z) \rightarrow (A_r(r, \phi, z), A_\phi(r, \phi, z), A_z(r, \phi, z))$ I would like to set clearly the link between partial derivatives of these two vector fields in a matrix form such that, $D\hat{A}(r, \theta, z) = DA(\Psi(r, \phi, z))\circ D\Psi(r, \phi, z)$, (edit) Where $D$ is the jacobian matrix. My final goal would be to get $\partial_x A$ in function of the partial derivatives $\partial_r, \partial_\phi, \partial_z$ of $\hat{A}$ in order to express the differential operators such as curl or divergence in the new coordinate system, but I'm not sure how to do it. Thus, could someone help me out to clarify the procedure of this change of coordinates. ( edit ) A confusing point I think is that we need to be cautious about the basis in which matrices are expressed. This is why I have trouble apprehending the matrix formulation, I don't know if we can get something like $J(A)_{x, y, x} = T J(\hat{A})_{r, \phi, z}$ where $J$ is the jacobian, and $T$ a matrix to be computed. I'm a bit ashamed to ask it here, but sometimes you just can't figure out basic stuff. Thanks in advance.","It's a really easy one for you guys. I'm performing a simple cylindrical change of variable from Cartesian coordinates, but I want to write it out properly and I'm stuck with the differential matrices. I won't get into details on everything here but I'll focus on my issue. I consider the diffeomorphism $\Psi$ matching the cylindrical change of coordinates $\Psi : (r, \phi, z) \rightarrow (r\cos{\phi}, r\sin{\phi}, z)$ And I compute its jacobian $D\Psi$ as well as its inverse $(D\Psi)^{-1}$. Now, for any vector field $A : (x, y, z) \rightarrow (A_x(x, y, z), A_y(x, y, z), A_z(x, y, z))$ and $\hat{A}  : (r, \phi, z) \rightarrow (A_r(r, \phi, z), A_\phi(r, \phi, z), A_z(r, \phi, z))$ I would like to set clearly the link between partial derivatives of these two vector fields in a matrix form such that, $D\hat{A}(r, \theta, z) = DA(\Psi(r, \phi, z))\circ D\Psi(r, \phi, z)$, (edit) Where $D$ is the jacobian matrix. My final goal would be to get $\partial_x A$ in function of the partial derivatives $\partial_r, \partial_\phi, \partial_z$ of $\hat{A}$ in order to express the differential operators such as curl or divergence in the new coordinate system, but I'm not sure how to do it. Thus, could someone help me out to clarify the procedure of this change of coordinates. ( edit ) A confusing point I think is that we need to be cautious about the basis in which matrices are expressed. This is why I have trouble apprehending the matrix formulation, I don't know if we can get something like $J(A)_{x, y, x} = T J(\hat{A})_{r, \phi, z}$ where $J$ is the jacobian, and $T$ a matrix to be computed. I'm a bit ashamed to ask it here, but sometimes you just can't figure out basic stuff. Thanks in advance.",,"['multivariable-calculus', 'coordinate-systems']"
72,inequalities for the derivatives of the logratihm of the complete symmetric polynomial,inequalities for the derivatives of the logratihm of the complete symmetric polynomial,,"I am interested in demonstrating the following: $$ \frac{\partial^2 log(P_{3,n}) }{\partial x_1 \partial x_2} \leq 0 $$ and $$ \frac{\partial^3 log(P_{3,n}) }{\partial x_1 \partial x_2 \partial x_3} \geq 0 $$ where $P_{3,n}$ is the n-degre complete symmetric polynomial on the variables $x_1, x_2, x_3$. I have checked that the above inequalities hold for $n=2,3,4,5$ and $6$ but I am not able to find a general proof for all $n$ or at least for a set of values of n. Thank you for your help!","I am interested in demonstrating the following: $$ \frac{\partial^2 log(P_{3,n}) }{\partial x_1 \partial x_2} \leq 0 $$ and $$ \frac{\partial^3 log(P_{3,n}) }{\partial x_1 \partial x_2 \partial x_3} \geq 0 $$ where $P_{3,n}$ is the n-degre complete symmetric polynomial on the variables $x_1, x_2, x_3$. I have checked that the above inequalities hold for $n=2,3,4,5$ and $6$ but I am not able to find a general proof for all $n$ or at least for a set of values of n. Thank you for your help!",,['multivariable-calculus']
73,Uniform estimate for multivariate Taylor's formula,Uniform estimate for multivariate Taylor's formula,,"I am wondering how the remainder in multivariate Taylor's formula could be uniformly bounded in a small ball around a point. For instance, let $f:\mathbb R^n\rightarrow\mathbb R^m$ be a function of class $C^2$ around a point $x\in \mathbb R^n$. For $\varepsilon>0$, let $B$ (resp. $\overline B$) denote the open (resp. closed) ball of center $x$ and radius $\varepsilon$. We assume $\varepsilon$ sufficiently small so that $f$ is $C^2$ in $B$. Then, for $y\in B$, define $$R(y)=\frac{\lVert f(y)-f(x)-D_x f(y-x)\rVert}{\lVert y-x\rVert^2}.$$ My question is: can $R(y)$ be uniformly bounded on $B$? For instance, if $n=m=1$ then the mean value theorem asserts that $R(y)\leq \frac{\max_{z\in \overline B}\mid f''(z)\mid}{2}$. Is there a similar bound in higher dimensions? (for instance involving $\max_{z\in \overline B}\frac{\lVert D^2_z f\rVert}{2}$ where $\lVert D^2_z f\rVert=\sup\frac{\lVert D^2_z f(u,v)\rVert}{\lVert u\rVert\lVert v\rVert}$) Thank you!","I am wondering how the remainder in multivariate Taylor's formula could be uniformly bounded in a small ball around a point. For instance, let $f:\mathbb R^n\rightarrow\mathbb R^m$ be a function of class $C^2$ around a point $x\in \mathbb R^n$. For $\varepsilon>0$, let $B$ (resp. $\overline B$) denote the open (resp. closed) ball of center $x$ and radius $\varepsilon$. We assume $\varepsilon$ sufficiently small so that $f$ is $C^2$ in $B$. Then, for $y\in B$, define $$R(y)=\frac{\lVert f(y)-f(x)-D_x f(y-x)\rVert}{\lVert y-x\rVert^2}.$$ My question is: can $R(y)$ be uniformly bounded on $B$? For instance, if $n=m=1$ then the mean value theorem asserts that $R(y)\leq \frac{\max_{z\in \overline B}\mid f''(z)\mid}{2}$. Is there a similar bound in higher dimensions? (for instance involving $\max_{z\in \overline B}\frac{\lVert D^2_z f\rVert}{2}$ where $\lVert D^2_z f\rVert=\sup\frac{\lVert D^2_z f(u,v)\rVert}{\lVert u\rVert\lVert v\rVert}$) Thank you!",,['multivariable-calculus']
74,Calculating the Volume of a function on the sphere,Calculating the Volume of a function on the sphere,,"As part of a question in calculus homework, I have to calculate: $Vol_n(M_C(f))$, when $f:S^{n-1}_+\rightarrow \mathbb{R}$ (when $S^{n-1}_+=\left\{x\in S^n: \forall i\in [1,n], <x,e_i>\geq 0\right\}$) is defined by $f(y)=\frac{1}{<a,y>^n}$, for a constant vector $a\in \mathbb{R}^n$ $M_C(f)=\left\{x\in S^{n-1}_+: f(x)=C\right\}$ and I currently have no idea how to calculate this from $Vol_n(M_C(f))=\int_{M_C(f)}1dx$. Thanks for the help!","As part of a question in calculus homework, I have to calculate: $Vol_n(M_C(f))$, when $f:S^{n-1}_+\rightarrow \mathbb{R}$ (when $S^{n-1}_+=\left\{x\in S^n: \forall i\in [1,n], <x,e_i>\geq 0\right\}$) is defined by $f(y)=\frac{1}{<a,y>^n}$, for a constant vector $a\in \mathbb{R}^n$ $M_C(f)=\left\{x\in S^{n-1}_+: f(x)=C\right\}$ and I currently have no idea how to calculate this from $Vol_n(M_C(f))=\int_{M_C(f)}1dx$. Thanks for the help!",,"['integration', 'functions', 'multivariable-calculus', 'volume']"
75,Need help in integrating $\int_{0}^{2\pi}\int_{0}^{2\pi}\frac{\cos t-\sin s - \cos t \sin s}{\sqrt{3+2(\cos t - \sin s - \cos t \sin s)}^3}dtds$,Need help in integrating,\int_{0}^{2\pi}\int_{0}^{2\pi}\frac{\cos t-\sin s - \cos t \sin s}{\sqrt{3+2(\cos t - \sin s - \cos t \sin s)}^3}dtds,This is not a homework problem. I was trying to use the Gaussian linking integral to compute the linking number of the Hopf link and I get stuck at: $$\int_{0}^{2\pi}\int_{0}^{2\pi}\frac{\cos t-\sin s - \cos t \sin s}{\sqrt{3+2(\cos t - \sin s - \cos t \sin s)}^3}dtds$$ Can anyone suggest some help?,This is not a homework problem. I was trying to use the Gaussian linking integral to compute the linking number of the Hopf link and I get stuck at: $$\int_{0}^{2\pi}\int_{0}^{2\pi}\frac{\cos t-\sin s - \cos t \sin s}{\sqrt{3+2(\cos t - \sin s - \cos t \sin s)}^3}dtds$$ Can anyone suggest some help?,,"['calculus', 'integration', 'multivariable-calculus']"
76,least squares: verify simple derivative,least squares: verify simple derivative,,"What is the derivative of $||Y-X\beta||_2$ w.r.t $X$? I have $(Y-X\beta)^T(Y-X \beta) = Y^TY-Y^TX \beta-\beta^TX^TY+\beta^TX^TX\beta$ which gives the derivative as $-Y\beta^T -Y\beta^T +2X\beta\beta^T$ Is this correct? If not, what is the correct result?","What is the derivative of $||Y-X\beta||_2$ w.r.t $X$? I have $(Y-X\beta)^T(Y-X \beta) = Y^TY-Y^TX \beta-\beta^TX^TY+\beta^TX^TX\beta$ which gives the derivative as $-Y\beta^T -Y\beta^T +2X\beta\beta^T$ Is this correct? If not, what is the correct result?",,"['calculus', 'matrices', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
77,About Jordan volume,About Jordan volume,,"I have a multivariate calculus homework, and I don't understand completely the definition of Jordan volume. Here's my question: Be $R \subset \mathbb{R}^2$ with Jordan area and for each $h>0$ be $C(R,h)\subset \mathbb{R}^3$ defined by: $$C(R,h) := \lbrace (x,y,h) \in \mathbb{R}^3: (x,y)\in R\rbrace$$ Show that $C(R,h)$ have Jordan volume and calculate it. If you could help me I would greatly appreciate.","I have a multivariate calculus homework, and I don't understand completely the definition of Jordan volume. Here's my question: Be $R \subset \mathbb{R}^2$ with Jordan area and for each $h>0$ be $C(R,h)\subset \mathbb{R}^3$ defined by: $$C(R,h) := \lbrace (x,y,h) \in \mathbb{R}^3: (x,y)\in R\rbrace$$ Show that $C(R,h)$ have Jordan volume and calculate it. If you could help me I would greatly appreciate.",,['multivariable-calculus']
78,"Given $\Sigma$ a surface parameterized by $\Phi : D \to \Sigma$, prove a certain formula for $area(\Sigma).$","Given  a surface parameterized by , prove a certain formula for",\Sigma \Phi : D \to \Sigma area(\Sigma).,"Let $\Sigma$ be a surface parameterized by  $\Phi : D \to \Sigma$, and let $$A=\Phi_u \cdot \Phi_u~,~B=\Phi_u \cdot \Phi_v,~ C=\Phi_v \cdot \Phi_v.$$ Prove $$area(\Sigma)=\int\int_D \sqrt{AC-B^2} dudv.$$ What I've done: We already know $$area(\Sigma)=\int\int_D ||\vec{N}||dudv.$$ where $\vec{N}$ is the normal vector $~\Phi_u \times \Phi_v.$ So it suffices to show that $||\vec{N}||=\sqrt{AC-B^2}.$ Now I can do this trivially by ""brute force computation"", i.e., by expanding out the components of $A,B,C$ and $\vec{N},$ and showing that the expressions are equal. I was wondering though if there is a simpler way to do this, without resorting to expanding into components.","Let $\Sigma$ be a surface parameterized by  $\Phi : D \to \Sigma$, and let $$A=\Phi_u \cdot \Phi_u~,~B=\Phi_u \cdot \Phi_v,~ C=\Phi_v \cdot \Phi_v.$$ Prove $$area(\Sigma)=\int\int_D \sqrt{AC-B^2} dudv.$$ What I've done: We already know $$area(\Sigma)=\int\int_D ||\vec{N}||dudv.$$ where $\vec{N}$ is the normal vector $~\Phi_u \times \Phi_v.$ So it suffices to show that $||\vec{N}||=\sqrt{AC-B^2}.$ Now I can do this trivially by ""brute force computation"", i.e., by expanding out the components of $A,B,C$ and $\vec{N},$ and showing that the expressions are equal. I was wondering though if there is a simpler way to do this, without resorting to expanding into components.",,"['integration', 'multivariable-calculus']"
79,Calculating the Solid Angle,Calculating the Solid Angle,,"$\textbf{Problem:}$ Consider we have a class $C^1$ parameterization $\psi:[a_1,b_1]\times[a_2,b_2]\rightarrow\mathbb{R}^3-\{0\}$ for the surface $S$. Also, consider that $S$ is such that the map $\varphi:S\rightarrow S^2$, with $\varphi(x,y,z)=\lVert(x,y,z)\lVert^{-1}(x,y,z)$, is injective. PS: $\lVert(x,y,z)\lVert$ is the 2-norm. The solid angle of $S$ is the area of $\varphi(S)$. I'm asked to show that the solid angle is $$\Bigg|\int_{a_1}^{b_1}\int_{a_2}^{b_2}\frac{1}{\lVert(\psi(s,t))\lVert^3}\psi(s,t)\cdot\big(\psi_s(s,t)\times\psi_t(s,t)\big)dtds\Bigg|. $$ $\textbf{My trial:}$ I started noting that $$\frac{\partial(\varphi\circ\psi)_i}{\partial s}=\frac{1}{\lVert\psi\lVert^3}\Bigg(\lVert\psi\lVert^2\frac{\partial\psi_i}{\partial s}-\psi_i(\psi_s\cdot\psi)\Bigg)$$ for $i=1,2,3$, and analogous for variable $t$. I simplified the notation doing $\psi=\psi(s,t)$, $\psi_s=\bigg(\frac{\partial\psi_1}{\partial s}(s,t),\frac{\partial\psi_2}{\partial s}(s,t),\frac{\partial\psi_3}{\partial s}(s,t)\bigg)$ and $\psi_i$ is the ith coordinate of $\psi(s,t)$. I can parameterize $\varphi(S)$ using the composite $\varphi\circ\psi$, and the following expression gives the area of the parameterized surface $\varphi(S)$:  $$\int_{a_1}^{b_1}\int_{a_2}^{b_2}\Bigg\lVert\Bigg(\frac{\partial(\varphi\circ\psi)_1,}{\partial s}\frac{\partial(\varphi\circ\psi)_2}{\partial s},\frac{\partial(\varphi\circ\psi)_3}{\partial s}\Bigg)\times\Bigg(\frac{\partial(\varphi\circ\psi)_1,}{\partial t}\frac{\partial(\varphi\circ\psi)_2}{\partial t},\frac{\partial(\varphi\circ\psi)_3}{\partial t}\Bigg)\Bigg\lVert dtds $$ I think the absolute value we have is to take care of some orientation issue. Anyway, using the initial relation and doing the calculations in this integral, we get $$\int_{a_1}^{b_1}\int_{a_2}^{b_2}\frac{1}{\lVert\psi\lVert^3}\Big\lVert \lVert\psi\lVert(\psi_s\times\psi_t)-\lVert\psi\lVert^{-1}\Big((\psi_t\cdot\psi)(\psi_s\times\psi)-(\psi_s\cdot\psi)(\psi_t\times\psi)\Big)\Big\lVert dtds= $$ $$=\int_{a_1}^{b_1}\int_{a_2}^{b_2}\frac{1}{\lVert\psi\lVert^3}\Big\lVert \lVert\psi\lVert(\psi_s\times\psi_t)-\lVert\psi\lVert^{-1}\Big(\big((\psi_t\cdot\psi)\psi_s-(\psi_s\cdot\psi)\psi_t\big)\times\psi\Big)\Big\lVert dtds $$ I don't know what to do from here, thanks.","$\textbf{Problem:}$ Consider we have a class $C^1$ parameterization $\psi:[a_1,b_1]\times[a_2,b_2]\rightarrow\mathbb{R}^3-\{0\}$ for the surface $S$. Also, consider that $S$ is such that the map $\varphi:S\rightarrow S^2$, with $\varphi(x,y,z)=\lVert(x,y,z)\lVert^{-1}(x,y,z)$, is injective. PS: $\lVert(x,y,z)\lVert$ is the 2-norm. The solid angle of $S$ is the area of $\varphi(S)$. I'm asked to show that the solid angle is $$\Bigg|\int_{a_1}^{b_1}\int_{a_2}^{b_2}\frac{1}{\lVert(\psi(s,t))\lVert^3}\psi(s,t)\cdot\big(\psi_s(s,t)\times\psi_t(s,t)\big)dtds\Bigg|. $$ $\textbf{My trial:}$ I started noting that $$\frac{\partial(\varphi\circ\psi)_i}{\partial s}=\frac{1}{\lVert\psi\lVert^3}\Bigg(\lVert\psi\lVert^2\frac{\partial\psi_i}{\partial s}-\psi_i(\psi_s\cdot\psi)\Bigg)$$ for $i=1,2,3$, and analogous for variable $t$. I simplified the notation doing $\psi=\psi(s,t)$, $\psi_s=\bigg(\frac{\partial\psi_1}{\partial s}(s,t),\frac{\partial\psi_2}{\partial s}(s,t),\frac{\partial\psi_3}{\partial s}(s,t)\bigg)$ and $\psi_i$ is the ith coordinate of $\psi(s,t)$. I can parameterize $\varphi(S)$ using the composite $\varphi\circ\psi$, and the following expression gives the area of the parameterized surface $\varphi(S)$:  $$\int_{a_1}^{b_1}\int_{a_2}^{b_2}\Bigg\lVert\Bigg(\frac{\partial(\varphi\circ\psi)_1,}{\partial s}\frac{\partial(\varphi\circ\psi)_2}{\partial s},\frac{\partial(\varphi\circ\psi)_3}{\partial s}\Bigg)\times\Bigg(\frac{\partial(\varphi\circ\psi)_1,}{\partial t}\frac{\partial(\varphi\circ\psi)_2}{\partial t},\frac{\partial(\varphi\circ\psi)_3}{\partial t}\Bigg)\Bigg\lVert dtds $$ I think the absolute value we have is to take care of some orientation issue. Anyway, using the initial relation and doing the calculations in this integral, we get $$\int_{a_1}^{b_1}\int_{a_2}^{b_2}\frac{1}{\lVert\psi\lVert^3}\Big\lVert \lVert\psi\lVert(\psi_s\times\psi_t)-\lVert\psi\lVert^{-1}\Big((\psi_t\cdot\psi)(\psi_s\times\psi)-(\psi_s\cdot\psi)(\psi_t\times\psi)\Big)\Big\lVert dtds= $$ $$=\int_{a_1}^{b_1}\int_{a_2}^{b_2}\frac{1}{\lVert\psi\lVert^3}\Big\lVert \lVert\psi\lVert(\psi_s\times\psi_t)-\lVert\psi\lVert^{-1}\Big(\big((\psi_t\cdot\psi)\psi_s-(\psi_s\cdot\psi)\psi_t\big)\times\psi\Big)\Big\lVert dtds $$ I don't know what to do from here, thanks.",,"['multivariable-calculus', 'surfaces', 'solid-angle']"
80,Please tell me what I am doing wrong for this multivariable Calculus Problem,Please tell me what I am doing wrong for this multivariable Calculus Problem,,"Suppose $F =(2x−4y)i +(x+3y)j$. Use Stokes' Theorem to make the following circulation calculations: (a) Find the circulation of $F$ around the circle $C$ of radius $10$ centered at the origin in the $xy$-plane, oriented clockwise as viewed from the positive $z$-axis. Circulation = $\int_CF\cdot dr$ Here is my work: Please tell me the correct answer and what I am doing wrong: $$ \begin{align*} \int_C F\cdot dr&= \int\int_S \text{curl} F · dS & \text{by Stokes' Theorem} \\ &= \int\int\langle 0, 0, 5\rangle \cdot \langle 0, 0, 1\rangle~dA & \text{since the circle lies on }z = 0 \\ &= 5 * (\text{Area of }C) \\ &= 5 * 100π \\ &= 500π. \end{align*} $$","Suppose $F =(2x−4y)i +(x+3y)j$. Use Stokes' Theorem to make the following circulation calculations: (a) Find the circulation of $F$ around the circle $C$ of radius $10$ centered at the origin in the $xy$-plane, oriented clockwise as viewed from the positive $z$-axis. Circulation = $\int_CF\cdot dr$ Here is my work: Please tell me the correct answer and what I am doing wrong: $$ \begin{align*} \int_C F\cdot dr&= \int\int_S \text{curl} F · dS & \text{by Stokes' Theorem} \\ &= \int\int\langle 0, 0, 5\rangle \cdot \langle 0, 0, 1\rangle~dA & \text{since the circle lies on }z = 0 \\ &= 5 * (\text{Area of }C) \\ &= 5 * 100π \\ &= 500π. \end{align*} $$",,['multivariable-calculus']
81,Am I doing this partial derivative correctly so far?,Am I doing this partial derivative correctly so far?,,"If $u=\arctan(xy+z)$, where $$x=s^2+t^2,\;y=9re^{st},\;z=r^2st,$$ find the value of $\frac{\partial u}{\partial s}$ when $r=2,s=1,t=0$. Is my attempt so far correct? $$\frac { ∂u }{ ∂s } =\frac { ∂\tan^{ -1 }(xy+z) }{ ∂s } \\[12pt] =\frac { ∂\tan^{ -1 }(xy+z) }{ ∂(xy+z) } \frac { ∂(xy+z) }{ ∂s } \\[12pt] =\frac { 1 }{ 1+{ (xy+z) }^{ 2 } } \frac { ∂(xy+z) }{ ∂s } $$ Simplifying the factor on the right: $$\frac { ∂(xy+z) }{ ∂s } \\[12pt] =\frac { ∂(xy+z) }{ ∂x } \frac { ∂x }{ ∂s } +\frac { ∂(xy+z) }{ ∂y } \frac { ∂y }{ ∂s } +\frac { ∂(xy+z) }{ ∂z } \frac { ∂z }{ ∂s } $$ Continuation of attempt: http://s16.postimg.org/4lk9voaxx/IMG_20130415_173718.jpg Sorry for poor quality.","If $u=\arctan(xy+z)$, where $$x=s^2+t^2,\;y=9re^{st},\;z=r^2st,$$ find the value of $\frac{\partial u}{\partial s}$ when $r=2,s=1,t=0$. Is my attempt so far correct? $$\frac { ∂u }{ ∂s } =\frac { ∂\tan^{ -1 }(xy+z) }{ ∂s } \\[12pt] =\frac { ∂\tan^{ -1 }(xy+z) }{ ∂(xy+z) } \frac { ∂(xy+z) }{ ∂s } \\[12pt] =\frac { 1 }{ 1+{ (xy+z) }^{ 2 } } \frac { ∂(xy+z) }{ ∂s } $$ Simplifying the factor on the right: $$\frac { ∂(xy+z) }{ ∂s } \\[12pt] =\frac { ∂(xy+z) }{ ∂x } \frac { ∂x }{ ∂s } +\frac { ∂(xy+z) }{ ∂y } \frac { ∂y }{ ∂s } +\frac { ∂(xy+z) }{ ∂z } \frac { ∂z }{ ∂s } $$ Continuation of attempt: http://s16.postimg.org/4lk9voaxx/IMG_20130415_173718.jpg Sorry for poor quality.",,['multivariable-calculus']
82,Useful approximation of the pdf,Useful approximation of the pdf,,"Good day to everyone. In my research work I came out with a function, which looks like this (it is the pdf of some random variable): $$f(x,\rho,\psi)=\frac{2}{\pi }+\sqrt{\frac{2}{\pi }} e^{-\frac{\rho ^2}{4}} \rho  \sum _{k=1}^{\infty } \cos  (2 k x ) \cos  (2 k \psi )\left(I_{k+\frac{1}{2}}\left(\frac{\rho ^2}{4}\right)+I_{k-\frac{1}{2}}\left(\frac{\rho ^2}{4}\right)\right) $$ where $\rho >0, 0<\psi<\frac{\pi}{2}, 0<x<\frac{\pi}{2}$ and $I_k(x)$ - modified Bessel function of the first kind and the summation is over all odd  indices. The thing is that the series converges slowly (because of the cosine terms). So it is hard to use this representation in practice. At first I tried to sum up the series but at last gave up. But I know that in similar problems the pdf is usually assumed to be the Von Mises or wrapped normal . So at first I noticed that  that the factor before the sum compensates the increment of the Bessel functions $\lim_{\rho \to \infty }\sqrt{\frac{2}{\pi }} e^{-\frac{\rho ^2}{4}} \rho  \left(I_{k+\frac{1}{2}}\left(\frac{\rho ^2}{4}\right)+I_{k-\frac{1}{2}}\left(\frac{\rho ^2}{4}\right)\right)  =\frac{4}{\pi }$ . And then, manipulating the Von Mises pdf obtained: $$f_1(x,\rho,\psi)=\frac{e^{\frac{1}{4} \rho^2  \cos  (2 (x -\psi ))}+e^{\frac{1}{4} \rho^2  \cos  (2 (x +\psi ))}}{\pi  I_0\left(\frac{\rho ^2}{4}\right)}$$ The same with wrapped normal. $$f_2(x,\rho,\psi)=\frac{\vartheta _3\left(x -\psi ,e^{-\frac{2}{\rho ^2}}\right)+\vartheta _3\left(x +\psi ,e^{-\frac{2}{\rho ^2}}\right)}{\pi }$$ where $\vartheta _3$ is the Jacobi theta function. Well, but this is just ""manipulating"". May be you can give me a hint how to show it analytically? After that I tried to compare those approximations. There are a lot of criteria to define how close those distributions are, and they all come down to computational procedure with different parameter $\rho, \psi$, which is not vary nice/descriptive. What I really want to find is a strict enough proof that this or that approximation (in analytic form) is superior for different $\rho, \psi$ in application to those pdf's. Do you think it is possible?","Good day to everyone. In my research work I came out with a function, which looks like this (it is the pdf of some random variable): $$f(x,\rho,\psi)=\frac{2}{\pi }+\sqrt{\frac{2}{\pi }} e^{-\frac{\rho ^2}{4}} \rho  \sum _{k=1}^{\infty } \cos  (2 k x ) \cos  (2 k \psi )\left(I_{k+\frac{1}{2}}\left(\frac{\rho ^2}{4}\right)+I_{k-\frac{1}{2}}\left(\frac{\rho ^2}{4}\right)\right) $$ where $\rho >0, 0<\psi<\frac{\pi}{2}, 0<x<\frac{\pi}{2}$ and $I_k(x)$ - modified Bessel function of the first kind and the summation is over all odd  indices. The thing is that the series converges slowly (because of the cosine terms). So it is hard to use this representation in practice. At first I tried to sum up the series but at last gave up. But I know that in similar problems the pdf is usually assumed to be the Von Mises or wrapped normal . So at first I noticed that  that the factor before the sum compensates the increment of the Bessel functions $\lim_{\rho \to \infty }\sqrt{\frac{2}{\pi }} e^{-\frac{\rho ^2}{4}} \rho  \left(I_{k+\frac{1}{2}}\left(\frac{\rho ^2}{4}\right)+I_{k-\frac{1}{2}}\left(\frac{\rho ^2}{4}\right)\right)  =\frac{4}{\pi }$ . And then, manipulating the Von Mises pdf obtained: $$f_1(x,\rho,\psi)=\frac{e^{\frac{1}{4} \rho^2  \cos  (2 (x -\psi ))}+e^{\frac{1}{4} \rho^2  \cos  (2 (x +\psi ))}}{\pi  I_0\left(\frac{\rho ^2}{4}\right)}$$ The same with wrapped normal. $$f_2(x,\rho,\psi)=\frac{\vartheta _3\left(x -\psi ,e^{-\frac{2}{\rho ^2}}\right)+\vartheta _3\left(x +\psi ,e^{-\frac{2}{\rho ^2}}\right)}{\pi }$$ where $\vartheta _3$ is the Jacobi theta function. Well, but this is just ""manipulating"". May be you can give me a hint how to show it analytically? After that I tried to compare those approximations. There are a lot of criteria to define how close those distributions are, and they all come down to computational procedure with different parameter $\rho, \psi$, which is not vary nice/descriptive. What I really want to find is a strict enough proof that this or that approximation (in analytic form) is superior for different $\rho, \psi$ in application to those pdf's. Do you think it is possible?",,"['multivariable-calculus', 'probability-distributions', 'special-functions', 'approximation']"
83,"Using the integral equation, find the eigenvalues and eigenfucntions","Using the integral equation, find the eigenvalues and eigenfucntions",,The integral equation: $$ \int_{-\frac{T}{2}}^{\frac{T}{2}}dt' \phi (t')e^{\Gamma\left | t-t' \right |}  =\lambda  \phi(t) $$ for  $(-\frac{1}{2}T< t < \frac{1}{2}T)$ is useful in photon counting statistics theory. Show that the eigenvalues $\lambda_k$ of the above equation are given by $$ \Gamma \lambda_k  =  \frac{2}{1+u^{2}_k} $$ where the $u_k$ are the roots of the transcendental equation $$ tan(\Gamma T u_k) = \frac{2 u_k}{u^{2}_k - 1} $$ Using this solve for the eigenfunctions associated The $\Gamma$ here is a constant.  I can see that the kernel is an exponential.  I've worked this numerous times using Fredholms technique but repeatidly hit a brick wall.  Am I missing something?,The integral equation: $$ \int_{-\frac{T}{2}}^{\frac{T}{2}}dt' \phi (t')e^{\Gamma\left | t-t' \right |}  =\lambda  \phi(t) $$ for  $(-\frac{1}{2}T< t < \frac{1}{2}T)$ is useful in photon counting statistics theory. Show that the eigenvalues $\lambda_k$ of the above equation are given by $$ \Gamma \lambda_k  =  \frac{2}{1+u^{2}_k} $$ where the $u_k$ are the roots of the transcendental equation $$ tan(\Gamma T u_k) = \frac{2 u_k}{u^{2}_k - 1} $$ Using this solve for the eigenfunctions associated The $\Gamma$ here is a constant.  I can see that the kernel is an exponential.  I've worked this numerous times using Fredholms technique but repeatidly hit a brick wall.  Am I missing something?,,"['multivariable-calculus', 'physics', 'mathematical-physics', 'integral-equations']"
84,satisfy the Euler-Lagrange equation,satisfy the Euler-Lagrange equation,,"Two circles of unit radius, each normal to the line through their centers are a distance d apart.  A soap film is formed between themas shown below; energetic considerations require the filem to assume a shape of minimum surface area. a) show that for any value of d less than a critical value $d_{c}$, there are two surfaces satisfying the appropriate Euler-Lagrange equation, while for d > $d_{c}$ there is no surface.  evaluate $d_{c}$ for a typical d < $d_{c}$, sketch the 2 surfaces.  which has the lesser area? b) show that in a certain range $d_{o}$ < d < $d_{c}$, the minimum surface as given by the Euler-Lagrange equation has an area larger than 2$\pi$, so hat the surface is ""metastable""; that is, the surface is stable against small perturbations but not against arbitrary perturbation.  evaluate $d_{o}$. c) what happens when d is in creased beyond $d_{c}$? Here is an drawing of the surface. Here is what i have so far.","Two circles of unit radius, each normal to the line through their centers are a distance d apart.  A soap film is formed between themas shown below; energetic considerations require the filem to assume a shape of minimum surface area. a) show that for any value of d less than a critical value $d_{c}$, there are two surfaces satisfying the appropriate Euler-Lagrange equation, while for d > $d_{c}$ there is no surface.  evaluate $d_{c}$ for a typical d < $d_{c}$, sketch the 2 surfaces.  which has the lesser area? b) show that in a certain range $d_{o}$ < d < $d_{c}$, the minimum surface as given by the Euler-Lagrange equation has an area larger than 2$\pi$, so hat the surface is ""metastable""; that is, the surface is stable against small perturbations but not against arbitrary perturbation.  evaluate $d_{o}$. c) what happens when d is in creased beyond $d_{c}$? Here is an drawing of the surface. Here is what i have so far.",,"['optimization', 'calculus-of-variations']"
85,Extension of Brouwer's degree to continuous functions.,Extension of Brouwer's degree to continuous functions.,,"I am studying the first chapter of this book: Topological Degree Theory and Applications At page 13 of this document, Definition 1.2.5, it is essentially said that to define the degree of a continuous function $f$ we just approximate it by a function $g$ in $C^2$ and take its degree. Now I can't see how to justify this from the previous proof: we just showed that if $\Omega$ is open and bounded, $f\in C^2(\bar \Omega) $, $p \not \in f(\partial \Omega)$ and $|p'-p|<\operatorname{dist}(p,f(\partial \Omega)$ where $p',p $ are regular values of $f$ then $$\operatorname{deg}(f,\Omega,p)=\operatorname{deg}(f,\Omega, p')$$ While what we actually need is that if $g_1,g_2 \in C^2(\bar \Omega)$ and $\max( {|g_1-f|,|g_2-f|)}< d(p, f(\partial \Omega))$ then $$\operatorname{deg}(g_1,\Omega,p)=\operatorname{deg}(g_2,\Omega, p')$$ Essentially we need an homotopy invariance for smooth functions, or am I wrong? Thanks!","I am studying the first chapter of this book: Topological Degree Theory and Applications At page 13 of this document, Definition 1.2.5, it is essentially said that to define the degree of a continuous function $f$ we just approximate it by a function $g$ in $C^2$ and take its degree. Now I can't see how to justify this from the previous proof: we just showed that if $\Omega$ is open and bounded, $f\in C^2(\bar \Omega) $, $p \not \in f(\partial \Omega)$ and $|p'-p|<\operatorname{dist}(p,f(\partial \Omega)$ where $p',p $ are regular values of $f$ then $$\operatorname{deg}(f,\Omega,p)=\operatorname{deg}(f,\Omega, p')$$ While what we actually need is that if $g_1,g_2 \in C^2(\bar \Omega)$ and $\max( {|g_1-f|,|g_2-f|)}< d(p, f(\partial \Omega))$ then $$\operatorname{deg}(g_1,\Omega,p)=\operatorname{deg}(g_2,\Omega, p')$$ Essentially we need an homotopy invariance for smooth functions, or am I wrong? Thanks!",,"['algebraic-topology', 'multivariable-calculus', 'differential-topology']"
86,Derivative/Jacobian of the matrix logarithm,Derivative/Jacobian of the matrix logarithm,,"I need help finding the Jacobian of the matrix logarithm function, i.e. $\log{M} = R$ defined by $e^R = M = V\begin{bmatrix}e^{\lambda_1} & & \\ & \ddots & \\ & & e^{\lambda_n} \end{bmatrix}V^{-1}$ (where $V\begin{bmatrix} \lambda_1 & & \\ & \ddots & \\ & & \lambda_n \end{bmatrix}V^{-1}$ is the eigendecomposition of $M$). Anyway, I've found some presentations like this one but found them totally impenetrable. If it helps, I don't need the derivative in a completely general situation. In my case $M$ is a $3 \times 3$ rotation matrix, and immediately after taking the logarithm I take the Frobenius norm, so I can shortcut those two operations with $||\log{M}||_F = 2\sqrt{\theta}$ as detailed on Wikipedia . However, I'm not sure if I can extend the shortcut to say $\frac{\partial}{\partial M}||\log{M}||_F = 2\theta^{-\frac{1}{2}}$. And anyway, the calculation of $\theta$ depends on arccosine which has nasty discontinuities so that is not an ideal method.","I need help finding the Jacobian of the matrix logarithm function, i.e. $\log{M} = R$ defined by $e^R = M = V\begin{bmatrix}e^{\lambda_1} & & \\ & \ddots & \\ & & e^{\lambda_n} \end{bmatrix}V^{-1}$ (where $V\begin{bmatrix} \lambda_1 & & \\ & \ddots & \\ & & \lambda_n \end{bmatrix}V^{-1}$ is the eigendecomposition of $M$). Anyway, I've found some presentations like this one but found them totally impenetrable. If it helps, I don't need the derivative in a completely general situation. In my case $M$ is a $3 \times 3$ rotation matrix, and immediately after taking the logarithm I take the Frobenius norm, so I can shortcut those two operations with $||\log{M}||_F = 2\sqrt{\theta}$ as detailed on Wikipedia . However, I'm not sure if I can extend the shortcut to say $\frac{\partial}{\partial M}||\log{M}||_F = 2\theta^{-\frac{1}{2}}$. And anyway, the calculation of $\theta$ depends on arccosine which has nasty discontinuities so that is not an ideal method.",,"['calculus', 'matrices', 'multivariable-calculus', 'euclidean-geometry']"
87,Integration of sine^2 w.r.t. some norm,Integration of sine^2 w.r.t. some norm,,"Let $||x||$ be any norm over $\mathbb R^n$. Let $B_T$ the open ball with radius $T$ w.r.t. to our norm, i.e. all $x\in\mathbb R^n$ such that $||x||<T$. Let $n\in\mathbb N$. How much is:$$\intop_{B_T} \sin^2 \frac {||x||\pi(n+\frac 1 2)} T dx$$ and $$\intop_{B_T} \sin^2 \frac {||x||\pi n} T dx$$ At least, how much is it with $L_2$ norm $||x||=||x||_2$? EDIT: Note that we're actually interested in $L_2$ norm of a function. So maybe can one come with a Fouerier-Transform-Isometry argument to calculate it more easily.","Let $||x||$ be any norm over $\mathbb R^n$. Let $B_T$ the open ball with radius $T$ w.r.t. to our norm, i.e. all $x\in\mathbb R^n$ such that $||x||<T$. Let $n\in\mathbb N$. How much is:$$\intop_{B_T} \sin^2 \frac {||x||\pi(n+\frac 1 2)} T dx$$ and $$\intop_{B_T} \sin^2 \frac {||x||\pi n} T dx$$ At least, how much is it with $L_2$ norm $||x||=||x||_2$? EDIT: Note that we're actually interested in $L_2$ norm of a function. So maybe can one come with a Fouerier-Transform-Isometry argument to calculate it more easily.",,"['complex-analysis', 'functional-analysis', 'measure-theory', 'integration', 'multivariable-calculus']"
88,Multivariable change of variable in integral,Multivariable change of variable in integral,,"Let $f:\mathbb R^n\rightarrow [0,T]$ be surjective and smooth enough (you can assume more assuptions if you want). I'd like to calculate: $$\int _{\mathbb R^n} g(f(x))dx$$ and $$\int _{\mathbb R^n} f(x)g(f(x))dx$$ where $g:[0,T]\rightarrow [0,T]$ smooth enough and increasing, by using change of variable and transform the integral into univariate one over $[0,T]$. How can this be done? I guess it might involve some gradients/determinants.","Let $f:\mathbb R^n\rightarrow [0,T]$ be surjective and smooth enough (you can assume more assuptions if you want). I'd like to calculate: $$\int _{\mathbb R^n} g(f(x))dx$$ and $$\int _{\mathbb R^n} f(x)g(f(x))dx$$ where $g:[0,T]\rightarrow [0,T]$ smooth enough and increasing, by using change of variable and transform the integral into univariate one over $[0,T]$. How can this be done? I guess it might involve some gradients/determinants.",,"['multivariable-calculus', 'integration']"
89,Gradient descent for the Thomson problem,Gradient descent for the Thomson problem,,"I'm trying to solve the Thomson Problem, i.e we have $N$ repelling point charges on a (hyper)sphere of dimension $m$ and we want to determine which configuration gives the lowest energy. We thus want to minimize $E=\sum_{i<j} ||x_i-x_j||^{-s} $ where $s$ is an integer, (generally taken to be equal to 1), and $x_i$ is the $i$th point. I want to apply gradient descent to it (as part of a local optimization routien in a  genetic algorithm), so I need the gradient of $E$, however $E$ depends on $N$ points, and each point has $m-1$ (hyper)spherical components. How could I calculate $\nabla E$ ?","I'm trying to solve the Thomson Problem, i.e we have $N$ repelling point charges on a (hyper)sphere of dimension $m$ and we want to determine which configuration gives the lowest energy. We thus want to minimize $E=\sum_{i<j} ||x_i-x_j||^{-s} $ where $s$ is an integer, (generally taken to be equal to 1), and $x_i$ is the $i$th point. I want to apply gradient descent to it (as part of a local optimization routien in a  genetic algorithm), so I need the gradient of $E$, however $E$ depends on $N$ points, and each point has $m-1$ (hyper)spherical components. How could I calculate $\nabla E$ ?",,"['multivariable-calculus', 'optimization']"
90,Property of derivative of Dirac delta function in $\mathbb{R}^n$,Property of derivative of Dirac delta function in,\mathbb{R}^n,"With reference to Property of Dirac delta function in $\mathbb{R}^n$ , is there a similar formula for $\langle g^*\delta', f \rangle$ (or even $\langle g^*\delta^{(n)}, f \rangle$)? By similar I mean a representation by an integral over $g^{-1}(0)$.","With reference to Property of Dirac delta function in $\mathbb{R}^n$ , is there a similar formula for $\langle g^*\delta', f \rangle$ (or even $\langle g^*\delta^{(n)}, f \rangle$)? By similar I mean a representation by an integral over $g^{-1}(0)$.",,"['multivariable-calculus', 'distribution-theory']"
91,Partial derivatives using variables after a transformation,Partial derivatives using variables after a transformation,,"I have a transformation $$(x'_1,x'_2)=(f(x_1,x_2), g(x_1,x_2))$$ and I wish to find $$\partial x'_1\over \partial x'_2$$ how might I evaluate this? If it is difficult to find a general expression for this, what if we suppose $f,g$ are simply linear combinations of $x_1,x_2$ so something like $ax_1+bx_2$ where $a,b$ are constants?","I have a transformation $$(x'_1,x'_2)=(f(x_1,x_2), g(x_1,x_2))$$ and I wish to find $$\partial x'_1\over \partial x'_2$$ how might I evaluate this? If it is difficult to find a general expression for this, what if we suppose $f,g$ are simply linear combinations of $x_1,x_2$ so something like $ax_1+bx_2$ where $a,b$ are constants?",,"['differential-geometry', 'multivariable-calculus', 'derivatives']"
92,Multivariate Taylor Polynomials,Multivariate Taylor Polynomials,,"Let $f: \mathbb{R}^n \to \mathbb{R}$ and additionally suppose that $f \in \mathcal{C}^{\infty}(\mathbb{R}^n)$. Given a point $a \in \mathbb{R}^n$, let the $l$-th Taylor polynomial of $f$ about $a$, $T_{a,l} : \mathbb{R}^n \to \mathbb{R}$ be given by: $$T_{a,l}(x) = f(a) + \sum_{|z|=1}^l \frac{1}{z!} \frac{ \partial^{z} f}{\partial x^{z}}(a)(x-a)^{z}$$ where $z$ is an $n$-dimensional multi-index. I would like to prove that $$\lim_{h \to 0} \frac{f(a+h)-T_{a,l}(a+h)}{||h||^l}=0$$ Any hints?","Let $f: \mathbb{R}^n \to \mathbb{R}$ and additionally suppose that $f \in \mathcal{C}^{\infty}(\mathbb{R}^n)$. Given a point $a \in \mathbb{R}^n$, let the $l$-th Taylor polynomial of $f$ about $a$, $T_{a,l} : \mathbb{R}^n \to \mathbb{R}$ be given by: $$T_{a,l}(x) = f(a) + \sum_{|z|=1}^l \frac{1}{z!} \frac{ \partial^{z} f}{\partial x^{z}}(a)(x-a)^{z}$$ where $z$ is an $n$-dimensional multi-index. I would like to prove that $$\lim_{h \to 0} \frac{f(a+h)-T_{a,l}(a+h)}{||h||^l}=0$$ Any hints?",,['real-analysis']
93,Can I solve for a unique integral kernel?,Can I solve for a unique integral kernel?,,"Consider, for $\mathbf{v},\mathbf{w} \in \mathbb{R^3}$, $$ f(\mathbf{w}) := \int K(\mathbf{w,\mathbf{v}}) g(\mathbf{v}) \, d\mathbf{v} \, .$$ Is it possible to solve for the integral kernel, $K(\mathbf{w,\mathbf{v}})$, if $f(\mathbf{w})$ and $g(\mathbf{v})$ are known scalar functions and we require $$\int K(\mathbf{w,\mathbf{v}}) \, d\mathbf{v} = 1 \,  ?$$ Follow-up note: these are definite integrals, $\int \rightarrow \int_{a1}^{b1} \int_{a2}^{b2} \int_{a3}^{b3}$ Thank you for any insight.","Consider, for $\mathbf{v},\mathbf{w} \in \mathbb{R^3}$, $$ f(\mathbf{w}) := \int K(\mathbf{w,\mathbf{v}}) g(\mathbf{v}) \, d\mathbf{v} \, .$$ Is it possible to solve for the integral kernel, $K(\mathbf{w,\mathbf{v}})$, if $f(\mathbf{w})$ and $g(\mathbf{v})$ are known scalar functions and we require $$\int K(\mathbf{w,\mathbf{v}}) \, d\mathbf{v} = 1 \,  ?$$ Follow-up note: these are definite integrals, $\int \rightarrow \int_{a1}^{b1} \int_{a2}^{b2} \int_{a3}^{b3}$ Thank you for any insight.",,"['multivariable-calculus', 'integration', 'vector-analysis', 'integral-transforms']"
94,Minimal condition for the existence of a limit in 2 dimensions,Minimal condition for the existence of a limit in 2 dimensions,,"Let $f$ be a continuous function on $\mathbb R^2\setminus \{0\}$. In a standard multivariable calculus course, one learns that even if $x\mapsto f(x,0)$ and $y\mapsto f(0,y)$ extend to be continuous at 0,  $f$ need not do so. Indeed, it is not even sufficient to have $t\mapsto f(at, bt)$ continuous at 0 (with the same value there) for all $a,b\in \mathbb R$. This can be seen by taking $$f(x,y) = \frac{x^2 - y^3}{x^2+y^3},$$ which satisfies $f(at, bt) \rightarrow 1$ as $t\rightarrow 0$ for any $a$ and $b$, but $f(t^{3/2}, t) \rightarrow 0$. I am wondering if there is a sufficient condition on the limits of compositions of $f$ with one-variable functions which guarantees that $f$ extends to be continuous at 0. Is it sufficient to have $f(p(t), q(t))$ continuous at 0 (with the same value at 0) for all polynomials $p(t)$ and $q(t)$ which vanish at the origin? What about for all analytic functions?","Let $f$ be a continuous function on $\mathbb R^2\setminus \{0\}$. In a standard multivariable calculus course, one learns that even if $x\mapsto f(x,0)$ and $y\mapsto f(0,y)$ extend to be continuous at 0,  $f$ need not do so. Indeed, it is not even sufficient to have $t\mapsto f(at, bt)$ continuous at 0 (with the same value there) for all $a,b\in \mathbb R$. This can be seen by taking $$f(x,y) = \frac{x^2 - y^3}{x^2+y^3},$$ which satisfies $f(at, bt) \rightarrow 1$ as $t\rightarrow 0$ for any $a$ and $b$, but $f(t^{3/2}, t) \rightarrow 0$. I am wondering if there is a sufficient condition on the limits of compositions of $f$ with one-variable functions which guarantees that $f$ extends to be continuous at 0. Is it sufficient to have $f(p(t), q(t))$ continuous at 0 (with the same value at 0) for all polynomials $p(t)$ and $q(t)$ which vanish at the origin? What about for all analytic functions?",,"['real-analysis', 'multivariable-calculus']"
95,Showing S is Jordan Measurable and Calculating the Volume,Showing S is Jordan Measurable and Calculating the Volume,,"If S is the solid obtained by intersecting the ball $x^2+y^2+z^2\le4$ and $x^2+z^2\le1$ 1) How do I show that S is Jordan measurable? Can I simply say the following: ""Clearly S is bounded, and the boundary is a union of smooth curves in $R^3$. Since smooth curves have zero content, the boundary is 0."" 2) Also for obtaining the volume of S, I am not sure how to calculate the bounds... Do I need the intersections? Many thanks!","If S is the solid obtained by intersecting the ball $x^2+y^2+z^2\le4$ and $x^2+z^2\le1$ 1) How do I show that S is Jordan measurable? Can I simply say the following: ""Clearly S is bounded, and the boundary is a union of smooth curves in $R^3$. Since smooth curves have zero content, the boundary is 0."" 2) Also for obtaining the volume of S, I am not sure how to calculate the bounds... Do I need the intersections? Many thanks!",,"['measure-theory', 'multivariable-calculus', 'integration']"
96,How can we define the Inner Product of multi-variable functions?,How can we define the Inner Product of multi-variable functions?,,"How can we define the Inner Product of multi-variable functions? For example, what is the value of the inner product of $\nabla f$ and $\nabla g$? $$\langle \nabla f, \nabla g\rangle = ?? $$ Here $\langle\cdot,\cdot\rangle$ is used for the inner product in $L^2$.","How can we define the Inner Product of multi-variable functions? For example, what is the value of the inner product of $\nabla f$ and $\nabla g$? $$\langle \nabla f, \nabla g\rangle = ?? $$ Here $\langle\cdot,\cdot\rangle$ is used for the inner product in $L^2$.",,['multivariable-calculus']
97,Three-dimensional vectors and force systems,Three-dimensional vectors and force systems,,"Full disclosure: this is a homework problem. However, I find myself stuck in the middle. The problem is below As shown, a system of cables suspends a crate weighing W = 350 .    (Part C 1 figure)  The dimensions in the figure are as follows:  h=   22.2 , l = 4.20 , x = 9.50 , theta = 40.0, and phi = 16.0. Determine TA, TD, and TE, the tensions in cable segments CA, CD, and CE,   respectively. I already found that TA = 406, i still need to find TD and TE. I know that CE = CF and that I need to find TD and TE through the Z forces and that the Y forces in these two should equate but I'm not sure about that either. A push in the right direction would be a huge help.","Full disclosure: this is a homework problem. However, I find myself stuck in the middle. The problem is below As shown, a system of cables suspends a crate weighing W = 350 .    (Part C 1 figure)  The dimensions in the figure are as follows:  h=   22.2 , l = 4.20 , x = 9.50 , theta = 40.0, and phi = 16.0. Determine TA, TD, and TE, the tensions in cable segments CA, CD, and CE,   respectively. I already found that TA = 406, i still need to find TD and TE. I know that CE = CF and that I need to find TD and TE through the Z forces and that the Y forces in these two should equate but I'm not sure about that either. A push in the right direction would be a huge help.",,"['multivariable-calculus', 'vector-spaces', '3d', 'vector-analysis']"
98,Singular manifold of the Jacobian,Singular manifold of the Jacobian,,"Suppose I have a map  $f: \mathbb R^{N} \mapsto \mathbb R^{N}$ of multivariate polynomial form of degree $K$: $$ f^i: X \mapsto A^{i}_0 + A^{ij}_1 X^{j} + A^{ijk}_2 X^j X^k + \ldots + A^{i i_1 \cdots i_K}_K X^{i_1} \cdots X^{i_K} $$ (a sum is implied for each repeated index). What can be said about the topology of the manifold defined by $$ J(f) = \det \left( \frac{\partial f^i}{\partial X^j } \right) = 0 .$$ For instance, what can be said of its dimensionality? How does it depend on $N$ and $K$?","Suppose I have a map  $f: \mathbb R^{N} \mapsto \mathbb R^{N}$ of multivariate polynomial form of degree $K$: $$ f^i: X \mapsto A^{i}_0 + A^{ij}_1 X^{j} + A^{ijk}_2 X^j X^k + \ldots + A^{i i_1 \cdots i_K}_K X^{i_1} \cdots X^{i_K} $$ (a sum is implied for each repeated index). What can be said about the topology of the manifold defined by $$ J(f) = \det \left( \frac{\partial f^i}{\partial X^j } \right) = 0 .$$ For instance, what can be said of its dimensionality? How does it depend on $N$ and $K$?",,"['polynomials', 'multivariable-calculus']"
99,why are curl and divergence defined the way they are?,why are curl and divergence defined the way they are?,,"Curl of a given vector field, $F$, at a given point is a vector, $C$, that gives the measure of amount of rotation the vector undergoes  at that point. It has been defined using cross products and determinants. So the curl of a velocity vector is the angular velocity vector. so can I opine that curl of a vector field, $F$, gives a vector, $C$, such that position vector $\times C = F$? If yes then can anyone provide me a derivation of the formula for curl (the vector with which when $R$ is crossed we obtain the vector whose curl we want to determine)? Now similar is the case of divergence of a vector field, at a given point it provides us the amount of how much the vector spreads. Can anyone explain to me why it has been defined as the way it is, i.e. derivative of $x$ component wrt $x$ + derivative of $y$ component wrt $y$ + derivative of $z$ component wrt $z$, or provide me with its derivation if there is any?","Curl of a given vector field, $F$, at a given point is a vector, $C$, that gives the measure of amount of rotation the vector undergoes  at that point. It has been defined using cross products and determinants. So the curl of a velocity vector is the angular velocity vector. so can I opine that curl of a vector field, $F$, gives a vector, $C$, such that position vector $\times C = F$? If yes then can anyone provide me a derivation of the formula for curl (the vector with which when $R$ is crossed we obtain the vector whose curl we want to determine)? Now similar is the case of divergence of a vector field, at a given point it provides us the amount of how much the vector spreads. Can anyone explain to me why it has been defined as the way it is, i.e. derivative of $x$ component wrt $x$ + derivative of $y$ component wrt $y$ + derivative of $z$ component wrt $z$, or provide me with its derivation if there is any?",,"['real-analysis', 'multivariable-calculus', 'definition']"
