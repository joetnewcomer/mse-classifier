,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Extending continuous and uniformly continuous functions,Extending continuous and uniformly continuous functions,,"What do you say about the following statement: Let $f\colon(a,b)\rightarrow \mathbb{R}$ be a uniformly continuous function. Then $f$ can be extended to a uniformly continuous function with domain $[a,b]$. Let $f\colon(a,b)\rightarrow \mathbb{R}$ be a continuous function. Then $f$ can be extended to a continuous function with domain $[a,b]$. So, I think that 2) is true, will be like add the points $a,b$ to domain used the definition limit, but I'm not sure. And 1) is true, too. But I don't know a good explanation.","What do you say about the following statement: Let $f\colon(a,b)\rightarrow \mathbb{R}$ be a uniformly continuous function. Then $f$ can be extended to a uniformly continuous function with domain $[a,b]$. Let $f\colon(a,b)\rightarrow \mathbb{R}$ be a continuous function. Then $f$ can be extended to a continuous function with domain $[a,b]$. So, I think that 2) is true, will be like add the points $a,b$ to domain used the definition limit, but I'm not sure. And 1) is true, too. But I don't know a good explanation.",,"['real-analysis', 'continuity']"
1,A set which is neither meagre nor comeagre in any interval.,A set which is neither meagre nor comeagre in any interval.,,"As the title suggests, I'm interested in a subset of the real line which is neither meagre nor comeagre in any interval. Does anyone have an example? Added. See the comments for some discussion about Bernstein sets, which may provide a solution. I don't know anything about these sets, so a detailed answer or reference in that connection would certainly be welcome. Here are some thoughts of mine and of one of my classmates regarding the problem. My original idea was as follows: Let $\mathcal V$ be a Vitali set (i.e., $\mathcal V$ contains exactly one element from each additive coset of $\mathbb Q$), choose dense subset $S$ of $\mathbb Q$ whose complement $\mathbb Q \setminus S$ in $\mathbb Q$ is also dense, and let $E = \mathcal{V} + S$. Then $E^c = \mathcal{V} + \mathbb{Q}\setminus S$, and it seems like $E$ should have the required property, but I have been unable to prove that this is so. My classmate had another idea. He worked in $[-1,1]\setminus\{0\}$, seeking a set $E$ with the following properties. The complement of $E$ is obtained from $E$ via reflection through the origin, that is, $E^c = -E$. If $E \cap I$ is the intersection of $E$ with some dyadic subinterval $I \subset [-1,1]$ of the $k$th generation, then $E$ is similar to this intersection, in the sense that $E = 2^k(E\cap I) + a$ for some real number $a$; that is, the intersection of $E$ with any dyadic subinterval is a scaled translate of $E$. If I'm not mistaken, it is (relatively) straightforward to verify that any set $E$ satisfying 1. and 2. provides an example. As far as the existence of such a set is concerned, my classmate made an argument, but it was fairly complicated, and I cannot recall it well enough off the top of my head to verify its correctness.","As the title suggests, I'm interested in a subset of the real line which is neither meagre nor comeagre in any interval. Does anyone have an example? Added. See the comments for some discussion about Bernstein sets, which may provide a solution. I don't know anything about these sets, so a detailed answer or reference in that connection would certainly be welcome. Here are some thoughts of mine and of one of my classmates regarding the problem. My original idea was as follows: Let $\mathcal V$ be a Vitali set (i.e., $\mathcal V$ contains exactly one element from each additive coset of $\mathbb Q$), choose dense subset $S$ of $\mathbb Q$ whose complement $\mathbb Q \setminus S$ in $\mathbb Q$ is also dense, and let $E = \mathcal{V} + S$. Then $E^c = \mathcal{V} + \mathbb{Q}\setminus S$, and it seems like $E$ should have the required property, but I have been unable to prove that this is so. My classmate had another idea. He worked in $[-1,1]\setminus\{0\}$, seeking a set $E$ with the following properties. The complement of $E$ is obtained from $E$ via reflection through the origin, that is, $E^c = -E$. If $E \cap I$ is the intersection of $E$ with some dyadic subinterval $I \subset [-1,1]$ of the $k$th generation, then $E$ is similar to this intersection, in the sense that $E = 2^k(E\cap I) + a$ for some real number $a$; that is, the intersection of $E$ with any dyadic subinterval is a scaled translate of $E$. If I'm not mistaken, it is (relatively) straightforward to verify that any set $E$ satisfying 1. and 2. provides an example. As far as the existence of such a set is concerned, my classmate made an argument, but it was fairly complicated, and I cannot recall it well enough off the top of my head to verify its correctness.",,"['real-analysis', 'general-topology', 'descriptive-set-theory']"
2,Banach-Tarski Paradox for the unit interval?,Banach-Tarski Paradox for the unit interval?,,"Does there exist such a map $\phi: [0, 1] \rightarrow S^2$ which determines some sort of a Banach-Tarski decomposition for the unit interval $[0, 1]$? I did read through Stan Wagon's The Banach-Tarski Paradox but the language is somewhat terse. I did read somewhere that Felix Hausdorff proved that one can chop up the unit interval into countably many pieces, slide the pieces around, and fit them together into the interval $[0, 2]$. Not sure of any journals that describe the exact proof. Is it possible to find $\phi$ explicitly perhaps in terms of the middle-third Cantor set?","Does there exist such a map $\phi: [0, 1] \rightarrow S^2$ which determines some sort of a Banach-Tarski decomposition for the unit interval $[0, 1]$? I did read through Stan Wagon's The Banach-Tarski Paradox but the language is somewhat terse. I did read somewhere that Felix Hausdorff proved that one can chop up the unit interval into countably many pieces, slide the pieces around, and fit them together into the interval $[0, 2]$. Not sure of any journals that describe the exact proof. Is it possible to find $\phi$ explicitly perhaps in terms of the middle-third Cantor set?",,"['real-analysis', 'general-topology']"
3,Controlling the Size of an Open Cover of a Set of Measure Zero,Controlling the Size of an Open Cover of a Set of Measure Zero,,"Suppose we have a subset $A\subset\mathbb{R}$ of Lebesgue measure zero contained in a compact interval, say $[0,1]$.  We know that since $A$ has measure zero we can cover $A$ with a countable set of open intervals, say $\{U_i\}$, such that $\mu(\cup_iU_i)\leq \varepsilon$ for any $\varepsilon$.  Now, if we fix some $\varepsilon>0$, can we cover $A$ with a countable set of open intervals, say now $\{V_i\}$, such that $\mu(V_i)\leq\frac{\varepsilon}{2^i}$ for each $i$?  That is, can we control the size of each individual set in some way? I have been thinking about this for a bit, and keep running into having to do things an infinite number of times, or having to choose the wrong indices first.  Any ideas? Thanks!","Suppose we have a subset $A\subset\mathbb{R}$ of Lebesgue measure zero contained in a compact interval, say $[0,1]$.  We know that since $A$ has measure zero we can cover $A$ with a countable set of open intervals, say $\{U_i\}$, such that $\mu(\cup_iU_i)\leq \varepsilon$ for any $\varepsilon$.  Now, if we fix some $\varepsilon>0$, can we cover $A$ with a countable set of open intervals, say now $\{V_i\}$, such that $\mu(V_i)\leq\frac{\varepsilon}{2^i}$ for each $i$?  That is, can we control the size of each individual set in some way? I have been thinking about this for a bit, and keep running into having to do things an infinite number of times, or having to choose the wrong indices first.  Any ideas? Thanks!",,"['real-analysis', 'measure-theory']"
4,Set of Finite Measure: Uncountable disjoint subsets of non-zero measure,Set of Finite Measure: Uncountable disjoint subsets of non-zero measure,,"Suppose $A$ is a set of finite measure.   Is it possible that $A$ can be an uncountable union of disjoint subsets of $A$, each of which has positive measure?","Suppose $A$ is a set of finite measure.   Is it possible that $A$ can be an uncountable union of disjoint subsets of $A$, each of which has positive measure?",,['real-analysis']
5,prove the Riemann-Lebesgue lemma: $\int^b_af(x)\cos(nx)dx\rightarrow 0$ as $n\rightarrow \infty$ for any regulated function $f$,prove the Riemann-Lebesgue lemma:  as  for any regulated function,\int^b_af(x)\cos(nx)dx\rightarrow 0 n\rightarrow \infty f,"I would like to prove the Riemann-Lebesgue lemma, namely that $\int^b_af(x)\cos(nx)dx\rightarrow 0$ as $n\rightarrow \infty$ for any regulated function $f$ . The textbook which I'm working from says that I need to prove 3 things in the following order: For all $a \lt b$ , show that $\int^b_a\cos(nx)dx \rightarrow 0$ as $n \rightarrow \infty$ By considering separately each interval of the partition, show that $\int^b_a\phi(x)\cos(nx)dx\rightarrow 0 $ as $n \rightarrow \infty$ , where $\phi(x)$ is a step function on $[a,b]$ Extend this to all $f \in R[a,b]$ So, here's my proof: $\int^b_a\cos(nx)dx= \frac{1}{n}\sin(nb)-\frac{1}{n}\sin(na) \rightarrow 0$ by the sandwhich rule: $$-1 \leq \sin(nx) \leq 1 \Leftrightarrow \frac{-1}{n} \leq \frac{\sin(nx)}{n} \leq \frac{1}{n} \Leftrightarrow 0 \leq \frac{\sin(nx)}{n} \leq 0 \text{ as } n \rightarrow \infty \Rightarrow \frac{1}{n}\sin(nx)=0$$ $\Rightarrow \int^b_a \cos(nx)dx \rightarrow 0$ as $n \rightarrow \infty$ Let $\phi(x) \in S[a,b]$ be a step function in $[a,b]$ and let $P=\{p_0,\ldots,p_k\}$ be a compatible partition with $\phi(x)$ . Then: $$\int\phi(x)\cos(nx)dx= \frac{1}{n}\phi(x)\Bigl(\sin(nb)-\sin(na)\Bigr)+ \frac{1}{n^2}cos(nx)(p_i-p_{i-1}) \rightarrow 0 \text{ as $n \rightarrow \infty$ for $x\in [p_i,p_i-1)$}$$ using IBP: $\phi(x)=u \Rightarrow (p_i-p_{i-1})dx=du$ and then $\cos(nx)dx=dv \Rightarrow \frac{1}{n}\sin(nx)=v$ . Hence, If I apply the same to every interval, $\int^b_a\phi(x)\cos(nx)dx \rightarrow 0$ as $n \rightarrow \infty$ for each interval of the partition $P$ . Let $\phi_n \in S[a,b]$ be a sequence of step functions converging uniformly to $f$ . Then $$\lim_{n \rightarrow \infty} \int^b_a \phi_n \cos(nx)dx= \int^b_af(x)\cos(nx) \rightarrow 0$$ as $n \rightarrow \infty$ by (2) Is my proof correct? Any help is appreciated!","I would like to prove the Riemann-Lebesgue lemma, namely that as for any regulated function . The textbook which I'm working from says that I need to prove 3 things in the following order: For all , show that as By considering separately each interval of the partition, show that as , where is a step function on Extend this to all So, here's my proof: by the sandwhich rule: as Let be a step function in and let be a compatible partition with . Then: using IBP: and then . Hence, If I apply the same to every interval, as for each interval of the partition . Let be a sequence of step functions converging uniformly to . Then as by (2) Is my proof correct? Any help is appreciated!","\int^b_af(x)\cos(nx)dx\rightarrow 0 n\rightarrow \infty f a \lt b \int^b_a\cos(nx)dx \rightarrow 0 n \rightarrow \infty \int^b_a\phi(x)\cos(nx)dx\rightarrow 0  n \rightarrow \infty \phi(x) [a,b] f \in R[a,b] \int^b_a\cos(nx)dx= \frac{1}{n}\sin(nb)-\frac{1}{n}\sin(na) \rightarrow 0 -1 \leq \sin(nx) \leq 1 \Leftrightarrow \frac{-1}{n} \leq \frac{\sin(nx)}{n} \leq \frac{1}{n} \Leftrightarrow 0 \leq \frac{\sin(nx)}{n} \leq 0 \text{ as } n \rightarrow \infty \Rightarrow \frac{1}{n}\sin(nx)=0 \Rightarrow \int^b_a \cos(nx)dx \rightarrow 0 n \rightarrow \infty \phi(x) \in S[a,b] [a,b] P=\{p_0,\ldots,p_k\} \phi(x) \int\phi(x)\cos(nx)dx= \frac{1}{n}\phi(x)\Bigl(\sin(nb)-\sin(na)\Bigr)+ \frac{1}{n^2}cos(nx)(p_i-p_{i-1}) \rightarrow 0 \text{ as n \rightarrow \infty for x\in [p_i,p_i-1)} \phi(x)=u \Rightarrow (p_i-p_{i-1})dx=du \cos(nx)dx=dv \Rightarrow \frac{1}{n}\sin(nx)=v \int^b_a\phi(x)\cos(nx)dx \rightarrow 0 n \rightarrow \infty P \phi_n \in S[a,b] f \lim_{n \rightarrow \infty} \int^b_a \phi_n \cos(nx)dx= \int^b_af(x)\cos(nx) \rightarrow 0 n \rightarrow \infty",['real-analysis']
6,Find $f'(0)$ if $f(x)+f(2x)=x\space\space\forall x$,Find  if,f'(0) f(x)+f(2x)=x\space\space\forall x,"Find $f'(0)$ if $f(x)+f(2x)=x\space\space\forall x$ If we assume $f'(0)\in\mathbb R$, then obviously, $f'(0)=\frac{1}{3}$. But what if we don't assume the derivative exists? I get this question when I am taking an exam, and I then asked the professor about whether there exist $f$ such that $f'(0)$ doesn't exist but satisfies the condition, but he says he think both exist or not is likely... What I have tried: If $f(x)+f(2x)=x\space\space\forall x\in\mathbb R$, $f(x)=\frac{x}{3}$ is the only nice function I have ever thought up$. If we restrict $dom(f)\in\mathbb R$, I think that any polynomial besides $\frac{x}{3}$ doesn't satisfy the condition, and somethings like $\vert x\vert$ doesn't help neither. On the other hand, I have tried to think about the equivalence statements with $f(x)+f(2x)=x\space\space\forall x$ in order to prove $f$ must be specific kind of problem s.t.$f'(0)$ exists. $f(2x)+f(4x)=2x$, so $f(4x)-f(x)=x$. In general, $$f(2^{2^n}x)-f(x)=x\prod_{k=1}^{n-1}(2^{2^k}+1)\forall n\in\mathbb N, x\in\mathbb R$$ But I don't think this helps. Can we find $f(2x)-f(x)$ by given condition? I havn't get an idea. But while I am thinking of it, I observe that $g(x)=f(2x)-f(x)$ is on its own satisfying $g(2x)+g(x)=x$. Will thinking about $f\circ f\circ f\circ f\circ f\circ\dots$ be useful? $f\biggl(f(x)+f(2x)\biggr)+f\biggl(2\bigl(f(x)+f(2x)\bigr)\biggr)=x\space\space\forall x$ Any help will be appreciate. Thank you!","Find $f'(0)$ if $f(x)+f(2x)=x\space\space\forall x$ If we assume $f'(0)\in\mathbb R$, then obviously, $f'(0)=\frac{1}{3}$. But what if we don't assume the derivative exists? I get this question when I am taking an exam, and I then asked the professor about whether there exist $f$ such that $f'(0)$ doesn't exist but satisfies the condition, but he says he think both exist or not is likely... What I have tried: If $f(x)+f(2x)=x\space\space\forall x\in\mathbb R$, $f(x)=\frac{x}{3}$ is the only nice function I have ever thought up$. If we restrict $dom(f)\in\mathbb R$, I think that any polynomial besides $\frac{x}{3}$ doesn't satisfy the condition, and somethings like $\vert x\vert$ doesn't help neither. On the other hand, I have tried to think about the equivalence statements with $f(x)+f(2x)=x\space\space\forall x$ in order to prove $f$ must be specific kind of problem s.t.$f'(0)$ exists. $f(2x)+f(4x)=2x$, so $f(4x)-f(x)=x$. In general, $$f(2^{2^n}x)-f(x)=x\prod_{k=1}^{n-1}(2^{2^k}+1)\forall n\in\mathbb N, x\in\mathbb R$$ But I don't think this helps. Can we find $f(2x)-f(x)$ by given condition? I havn't get an idea. But while I am thinking of it, I observe that $g(x)=f(2x)-f(x)$ is on its own satisfying $g(2x)+g(x)=x$. Will thinking about $f\circ f\circ f\circ f\circ f\circ\dots$ be useful? $f\biggl(f(x)+f(2x)\biggr)+f\biggl(2\bigl(f(x)+f(2x)\bigr)\biggr)=x\space\space\forall x$ Any help will be appreciate. Thank you!",,"['calculus', 'real-analysis', 'functional-equations']"
7,Theorems that give sufficient condition for a $C^{\infty}$ function to be analytic,Theorems that give sufficient condition for a  function to be analytic,C^{\infty},"What are general theorems that give sufficient criteria for a $C^{\infty}$ function to be analytic? The more general/simple the test, the better. I'm trying to understand in a more thorough way what prevents $C^{\infty}$ functions from being analytic. The canonical example of a $C^{\infty}$ function $f$ which is not analytic at a point is $f(x) = e^{-\frac{1}{x}}$ for $x \in [0, \infty]$ and $f(x) = 0$ elsewhere. This indicates to me that nonzero analytic functions cannot ""flatten out"" too quickly at a point where they are defined. I know this is vague, but can you give me some theorems that make it more clear just how much more general $C^{\infty}$ functions are than analytic ones, and how many more degrees of freedom they have? I would also appreciate examples of $C^{\infty}$ functions that are pathological in some sense, in a way that an analytic function cannot be. For example, is there a $C^{\infty}$ function that fails to be analytic on a dense set?","What are general theorems that give sufficient criteria for a $C^{\infty}$ function to be analytic? The more general/simple the test, the better. I'm trying to understand in a more thorough way what prevents $C^{\infty}$ functions from being analytic. The canonical example of a $C^{\infty}$ function $f$ which is not analytic at a point is $f(x) = e^{-\frac{1}{x}}$ for $x \in [0, \infty]$ and $f(x) = 0$ elsewhere. This indicates to me that nonzero analytic functions cannot ""flatten out"" too quickly at a point where they are defined. I know this is vague, but can you give me some theorems that make it more clear just how much more general $C^{\infty}$ functions are than analytic ones, and how many more degrees of freedom they have? I would also appreciate examples of $C^{\infty}$ functions that are pathological in some sense, in a way that an analytic function cannot be. For example, is there a $C^{\infty}$ function that fails to be analytic on a dense set?",,"['real-analysis', 'analysis', 'big-list', 'analytic-functions']"
8,Solutions of autonomous ODEs are monotonic,Solutions of autonomous ODEs are monotonic,,"Problem. Let $I,J$ be open intervals, $\,f:I\to \mathbb R$, continuous, $\,\varphi :J\to \mathbb R$,  continuously differentiable, with $\varphi[J]\subset I$, and $\varphi$ satisfying $$ \varphi'(t)=f\big(\varphi(t)\big),  \quad \text{for all $t\in J$}. $$ Show that $\varphi$ is monotonic. This becomes rather straight-forward if we further assume the $f$ is continuously differentiable, or even locally Lipschitz continuous, as this implies that IVP for $x'=f(x)$ enjoy uniqueness, and hence if $f\big(\varphi(t_0)\big)=0$, for some $t_0\in J$, then $\varphi$ is constant. In fact something stronger holds in such case: $\varphi$ is either strictly monotonic or constant. This problem is also straight-forward if $f(x)\ne 0$, for all $x\in I$, in which case uniqueness kicks in anew. The hard part is to show monotonicity of $\varphi$ when $f$ is just continuous, and its values include zero.","Problem. Let $I,J$ be open intervals, $\,f:I\to \mathbb R$, continuous, $\,\varphi :J\to \mathbb R$,  continuously differentiable, with $\varphi[J]\subset I$, and $\varphi$ satisfying $$ \varphi'(t)=f\big(\varphi(t)\big),  \quad \text{for all $t\in J$}. $$ Show that $\varphi$ is monotonic. This becomes rather straight-forward if we further assume the $f$ is continuously differentiable, or even locally Lipschitz continuous, as this implies that IVP for $x'=f(x)$ enjoy uniqueness, and hence if $f\big(\varphi(t_0)\big)=0$, for some $t_0\in J$, then $\varphi$ is constant. In fact something stronger holds in such case: $\varphi$ is either strictly monotonic or constant. This problem is also straight-forward if $f(x)\ne 0$, for all $x\in I$, in which case uniqueness kicks in anew. The hard part is to show monotonicity of $\varphi$ when $f$ is just continuous, and its values include zero.",,"['calculus', 'real-analysis', 'analysis', 'ordinary-differential-equations']"
9,Calculate integral $\int_0^1 e^{e^{e^x}} dx$,Calculate integral,\int_0^1 e^{e^{e^x}} dx,I have to calculate integral $$\Large \int_0^1 e^{e^{e^x}} dx$$ Is this even possible?,I have to calculate integral $$\Large \int_0^1 e^{e^{e^x}} dx$$ Is this even possible?,,"['calculus', 'real-analysis', 'integration']"
10,Infinite Series $\sum\limits_{n=1}^\infty\frac{\zeta(3n)}{2^{3n}}$,Infinite Series,\sum\limits_{n=1}^\infty\frac{\zeta(3n)}{2^{3n}},"I want to evaluate $$\sum_{n=1}^\infty\frac{\zeta(3n)}{2^{3n}}$$ let $f(z)=\sum_{n=1}^\infty\frac{1}{2^{3n}}z^{3n}$, then $$\sum_{n=1}^\infty f\left(\frac{1}{n}\right)=\sum_{n=1}^\infty\sum_{m=1}^\infty\frac{1}{2^{3m}}\frac{1}{n^{3m}}\leq\frac{1}{7}\zeta(3)<\infty$$ so we can switch order of summation $$\sum_{n=1}^\infty f\left(\frac{1}{n}\right)=\sum_{n=1}^\infty\frac{\zeta(3n)}{2^{3n}}$$ and now  $$f(z)=\sum_{n=1}^\infty\left(\frac{z^3}{2^3}\right)^n=\frac{z^3}{8-z^3}, f\left(\frac{1}{n}\right)=\frac{1}{8n^3-1}$$ Hence, $$\sum_{n=1}^\infty\frac{\zeta(3n)}{2^{3n}}=\sum_{n=1}^\infty\frac{1}{8n^3-1}$$  Is there an analytic method for evaluating $\sum_{n=1}^\infty\frac{1}{8n^3-1}$?","I want to evaluate $$\sum_{n=1}^\infty\frac{\zeta(3n)}{2^{3n}}$$ let $f(z)=\sum_{n=1}^\infty\frac{1}{2^{3n}}z^{3n}$, then $$\sum_{n=1}^\infty f\left(\frac{1}{n}\right)=\sum_{n=1}^\infty\sum_{m=1}^\infty\frac{1}{2^{3m}}\frac{1}{n^{3m}}\leq\frac{1}{7}\zeta(3)<\infty$$ so we can switch order of summation $$\sum_{n=1}^\infty f\left(\frac{1}{n}\right)=\sum_{n=1}^\infty\frac{\zeta(3n)}{2^{3n}}$$ and now  $$f(z)=\sum_{n=1}^\infty\left(\frac{z^3}{2^3}\right)^n=\frac{z^3}{8-z^3}, f\left(\frac{1}{n}\right)=\frac{1}{8n^3-1}$$ Hence, $$\sum_{n=1}^\infty\frac{\zeta(3n)}{2^{3n}}=\sum_{n=1}^\infty\frac{1}{8n^3-1}$$  Is there an analytic method for evaluating $\sum_{n=1}^\infty\frac{1}{8n^3-1}$?",,"['real-analysis', 'sequences-and-series', 'summation', 'closed-form', 'riemann-zeta']"
11,Possible generalizations of $\sum_{k=1}^n \cos k$ being bounded,Possible generalizations of  being bounded,\sum_{k=1}^n \cos k,"I just read in this month's MAA Math Horizons the problem: Show that $\sum_{k=1}^n \cos k$ is bounded. After a bit of floundering, I realized that the key is to write this as the real part of $\sum_{k=1}^n e^{ik}$  and sum the geometric series. I then realized that this also shows that $\sum_{k=1}^n \cos(ak)$ and $\sum_{k=1}^n \sin(ak)$ are also bounded for any real $a$ not a multiple of $2\pi$, since the denominator of the expression that results is $e^{i a}-1$, and this is nonzero for any such $a$. So I wondered if there is a generalization that doesn't depend on the properties of $e^{ix}$, and came up with this: If $f$ is continuous and periodic with shortest period $p$, $\int_0^p f(x)\, dx = 0$, and $q$ is not a multiple of $p$, is $\sum_{k=1}^n f(qk)$  bounded? If this is false, are there any additional conditions on $f$ or $q$ (such as $f$ having a bounded derivative or $q/p$ being irrational)  that would make it true? (added later, after the comments and the very nice work by Micah) Here is another possible condition: $q/p$ is not an integer and, in every period of $f$, for every $f(x)$, there is at most one other $z$ such that $f(z) = f(x)$. (This would rule out the possibility suggested by Thomas Andrews.)","I just read in this month's MAA Math Horizons the problem: Show that $\sum_{k=1}^n \cos k$ is bounded. After a bit of floundering, I realized that the key is to write this as the real part of $\sum_{k=1}^n e^{ik}$  and sum the geometric series. I then realized that this also shows that $\sum_{k=1}^n \cos(ak)$ and $\sum_{k=1}^n \sin(ak)$ are also bounded for any real $a$ not a multiple of $2\pi$, since the denominator of the expression that results is $e^{i a}-1$, and this is nonzero for any such $a$. So I wondered if there is a generalization that doesn't depend on the properties of $e^{ix}$, and came up with this: If $f$ is continuous and periodic with shortest period $p$, $\int_0^p f(x)\, dx = 0$, and $q$ is not a multiple of $p$, is $\sum_{k=1}^n f(qk)$  bounded? If this is false, are there any additional conditions on $f$ or $q$ (such as $f$ having a bounded derivative or $q/p$ being irrational)  that would make it true? (added later, after the comments and the very nice work by Micah) Here is another possible condition: $q/p$ is not an integer and, in every period of $f$, for every $f(x)$, there is at most one other $z$ such that $f(z) = f(x)$. (This would rule out the possibility suggested by Thomas Andrews.)",,"['calculus', 'real-analysis', 'sequences-and-series']"
12,I need to find all functions $f:\mathbb R \rightarrow \mathbb R$ which are continuous and satisfy $f(x+y)=f(x)+f(y)$,I need to find all functions  which are continuous and satisfy,f:\mathbb R \rightarrow \mathbb R f(x+y)=f(x)+f(y),"I need to find all functions $f:\mathbb R \rightarrow \mathbb R$ such that $f(x+y)=f(x)+f(y)$. I know that there are other questions that are asking the same thing, but I'm trying to figure this out by myself as best as possible. Here is how I started out: Try out some cases: $x=0:$ $$f(0+y)=f(0)+f(y) \iff f(y)=f(0)+f(y) \iff 0=f(0) $$ The same result is for when $y=0$ $x=-y:$ $$f(-y+y)=f(-y)+f(y) \iff f(0)=f(-y)+f(y) \iff 0=f(-y)+f(y)\iff \quad f(-y)=-f(y)$$ I want to extend the result of setting $x=-y$ to numbers other that $-1$, perhaps all real numbers or all rational numbers. I got a little help from reading other solutions on the next part: Let $q=1+1+1+...+1$. Then  $$f(qx)=f((1+1+...+1)x)=f(x+x+...+x)=f(x)+f(x)+...+f(x)=qf(x)$$ I understood this part, but I don't understand why this helps me find all the functions that satisfy the requirement that $f(x+y)=f(x)+f(y)$, but here is how I went on: Thus  $$f(qx)=qf(x)$$ and it should follow that $$f \bigg (\frac {1}{q} x\bigg)= \frac{1}{q}f(x)$$ where $q\not =0$, then it further follows that  $$f \bigg (\frac {p}{q} x\bigg)= \frac{p}{q}f(x)$$ where $\frac{p}{q}$ is rational, and lastly it further follows that  $$f (ax)= af(x)$$ where $a$ is real. Thus functions of the form $f(ax)$ where $a$ is real satisfies the requirement of  $f(x+y)=f(x)+f(y)$. I don't know how much of what I did is correct\incorrect, and any help would be greatly appreciated. Also is there any way that I can say that functions of the form $f(ax)$ where $a$ is real are the only functions that satisfy the requirement of  $f(x+y)=f(x)+f(y)$? Or do other solutions exist? Again, thanks a lot for any help! (Hints would be appreciated, I'll really try to understand the hints!)","I need to find all functions $f:\mathbb R \rightarrow \mathbb R$ such that $f(x+y)=f(x)+f(y)$. I know that there are other questions that are asking the same thing, but I'm trying to figure this out by myself as best as possible. Here is how I started out: Try out some cases: $x=0:$ $$f(0+y)=f(0)+f(y) \iff f(y)=f(0)+f(y) \iff 0=f(0) $$ The same result is for when $y=0$ $x=-y:$ $$f(-y+y)=f(-y)+f(y) \iff f(0)=f(-y)+f(y) \iff 0=f(-y)+f(y)\iff \quad f(-y)=-f(y)$$ I want to extend the result of setting $x=-y$ to numbers other that $-1$, perhaps all real numbers or all rational numbers. I got a little help from reading other solutions on the next part: Let $q=1+1+1+...+1$. Then  $$f(qx)=f((1+1+...+1)x)=f(x+x+...+x)=f(x)+f(x)+...+f(x)=qf(x)$$ I understood this part, but I don't understand why this helps me find all the functions that satisfy the requirement that $f(x+y)=f(x)+f(y)$, but here is how I went on: Thus  $$f(qx)=qf(x)$$ and it should follow that $$f \bigg (\frac {1}{q} x\bigg)= \frac{1}{q}f(x)$$ where $q\not =0$, then it further follows that  $$f \bigg (\frac {p}{q} x\bigg)= \frac{p}{q}f(x)$$ where $\frac{p}{q}$ is rational, and lastly it further follows that  $$f (ax)= af(x)$$ where $a$ is real. Thus functions of the form $f(ax)$ where $a$ is real satisfies the requirement of  $f(x+y)=f(x)+f(y)$. I don't know how much of what I did is correct\incorrect, and any help would be greatly appreciated. Also is there any way that I can say that functions of the form $f(ax)$ where $a$ is real are the only functions that satisfy the requirement of  $f(x+y)=f(x)+f(y)$? Or do other solutions exist? Again, thanks a lot for any help! (Hints would be appreciated, I'll really try to understand the hints!)",,"['real-analysis', 'functional-equations']"
13,"Principles of Mathematical Analysis, Dedekind Cuts, Multiplicative Inverse","Principles of Mathematical Analysis, Dedekind Cuts, Multiplicative Inverse",,"At the top of the page 20 of Rudin's book ''Principles of Mathematical Analysis'' he writes: ''The proofs (of the multiplication axioms) are so similar to the ones given in detail in Step 4 (proof of the addition axioms) that we omit them''. I tried to prove them but I got stuck in the proof of  \begin{equation}\alpha \cdot {\alpha }^{-1}=1^*\end{equation}  where $\alpha$ is positive cut and ${\alpha }^{-1}=\mathbb{Q}_{-}\bigcup\left\{0\right\}\bigcup\left\{t\in \mathbb{Q}:0<t<r\text{ for some }r\in \mathbb{Q}:\frac{1}{r}\notin \alpha\right\}$ is the candidate for the multiplicative inverse of $\alpha$. I have already proved that ${\alpha }^{-1}$ is a cut and $\alpha \cdot {\alpha }^{-1}\le 1^*$. My question is how do we prove the opposite direction similarly to the proof Rudin gives for $\alpha +(-\alpha) \le 0^*$. A proof completely different to that one can be found here: Dedekind cut multiplicative inverse Here is what I have tried thus far: Let $p\in 1^*$. If $p\le 0$ then obviously $p\in \alpha\cdot \alpha^{-1}$. Suppose $0<p<1$ and $q=q(p)\in \mathbb{Q}_{+}$. By the Archimedean Property of Rational numbers \begin{equation}\exists n\in \mathbb{N}:nq\in \alpha\text{ and }(n+1)q\notin \alpha\end{equation}  We must find a $u \in \alpha^{-1}$ such as that $p=(nq)\cdot u$ or equivalenty, $u=\frac{p}{nq}$ In order for $u \in \alpha^{-1}$ we must have that $0<u<r$ and $\frac{1}{r}\notin \alpha$ for some rational $r$. The only reasonable choice for $r$ would be $\frac{1}{(n+1)q}$. But then, \begin{equation}u<r\Leftrightarrow \frac{p}{nq}<\frac{1}{(n+1)q}\Leftrightarrow p<\frac{n}{n+1}\end{equation} which may not be true for some values of $n$ (like $0$). Where can we derive a restriction for these values of $n$? EDIT: Found another proof here: http://mypage.iu.edu/~sgautam/m413.33418.11f/Dedekind.pdf STill nothing similar to Rudin's...","At the top of the page 20 of Rudin's book ''Principles of Mathematical Analysis'' he writes: ''The proofs (of the multiplication axioms) are so similar to the ones given in detail in Step 4 (proof of the addition axioms) that we omit them''. I tried to prove them but I got stuck in the proof of  \begin{equation}\alpha \cdot {\alpha }^{-1}=1^*\end{equation}  where $\alpha$ is positive cut and ${\alpha }^{-1}=\mathbb{Q}_{-}\bigcup\left\{0\right\}\bigcup\left\{t\in \mathbb{Q}:0<t<r\text{ for some }r\in \mathbb{Q}:\frac{1}{r}\notin \alpha\right\}$ is the candidate for the multiplicative inverse of $\alpha$. I have already proved that ${\alpha }^{-1}$ is a cut and $\alpha \cdot {\alpha }^{-1}\le 1^*$. My question is how do we prove the opposite direction similarly to the proof Rudin gives for $\alpha +(-\alpha) \le 0^*$. A proof completely different to that one can be found here: Dedekind cut multiplicative inverse Here is what I have tried thus far: Let $p\in 1^*$. If $p\le 0$ then obviously $p\in \alpha\cdot \alpha^{-1}$. Suppose $0<p<1$ and $q=q(p)\in \mathbb{Q}_{+}$. By the Archimedean Property of Rational numbers \begin{equation}\exists n\in \mathbb{N}:nq\in \alpha\text{ and }(n+1)q\notin \alpha\end{equation}  We must find a $u \in \alpha^{-1}$ such as that $p=(nq)\cdot u$ or equivalenty, $u=\frac{p}{nq}$ In order for $u \in \alpha^{-1}$ we must have that $0<u<r$ and $\frac{1}{r}\notin \alpha$ for some rational $r$. The only reasonable choice for $r$ would be $\frac{1}{(n+1)q}$. But then, \begin{equation}u<r\Leftrightarrow \frac{p}{nq}<\frac{1}{(n+1)q}\Leftrightarrow p<\frac{n}{n+1}\end{equation} which may not be true for some values of $n$ (like $0$). Where can we derive a restriction for these values of $n$? EDIT: Found another proof here: http://mypage.iu.edu/~sgautam/m413.33418.11f/Dedekind.pdf STill nothing similar to Rudin's...",,"['real-analysis', 'elementary-set-theory']"
14,Norm of the linear functional $f\mapsto \int_0^1 f(t)/t dt$,Norm of the linear functional,f\mapsto \int_0^1 f(t)/t dt,"Let $C^1([0,1],\mathbb R)$ be the space of continuously differentiable functions over $[0,1]$ . We equip it with the norm $\|f\|_{C^1} = \|f\|_{\infty}+\|f'\|_{\infty}$ . Let $F$ be the linear subspace $F=\{f \in C^1([0,1],\mathbb R) : f(0)=0 \}$ and define the functional $$\varphi: F\to \mathbb R, \quad f\mapsto \int_0^1 \frac{f(t)}t dt$$ What is the norm of $\varphi$ ? Here are my thoughts: given $f\in F$ , since $f(0)=0$ and $f$ is differentiable at $0$ , $$f(t)\ln(t) = \frac{f(t)-f(0)}{t-0} \times  t\ln(t)\xrightarrow[t\to 0]{} 0 $$ hence integration by parts is licit and yields $$\varphi(f) = -\int_0^1 f'(t) \ln(t) dt$$ from which we derive $|\varphi(f)| \leq \|f'\|_\infty \int_0^1 |\ln(t)| dt = \|f'\|_\infty\leq \|f\|_{C^1}$ . Therefore $\|\varphi\| \leq 1$ . However, this upper bound seems too loose. When experimenting with low-degree polynomials I find that $\displaystyle \frac{|\varphi(f)|}{\|f\|_{C^1}}\leq \frac 12$ . EDIT: Thanks to user Medo I know that $\|\varphi\|>\frac 12$ since for $f:t\mapsto \sin(2t)$ , $\varphi(f) \approx 1.60$ while $\|f\|_{C^1} = 1 + 2 = 3$ , hence a ratio of $\approx 0.53$ .","Let be the space of continuously differentiable functions over . We equip it with the norm . Let be the linear subspace and define the functional What is the norm of ? Here are my thoughts: given , since and is differentiable at , hence integration by parts is licit and yields from which we derive . Therefore . However, this upper bound seems too loose. When experimenting with low-degree polynomials I find that . EDIT: Thanks to user Medo I know that since for , while , hence a ratio of .","C^1([0,1],\mathbb R) [0,1] \|f\|_{C^1} = \|f\|_{\infty}+\|f'\|_{\infty} F F=\{f \in C^1([0,1],\mathbb R) : f(0)=0 \} \varphi: F\to \mathbb R, \quad f\mapsto \int_0^1 \frac{f(t)}t dt \varphi f\in F f(0)=0 f 0 f(t)\ln(t) = \frac{f(t)-f(0)}{t-0} \times  t\ln(t)\xrightarrow[t\to 0]{} 0  \varphi(f) = -\int_0^1 f'(t) \ln(t) dt |\varphi(f)| \leq \|f'\|_\infty \int_0^1 |\ln(t)| dt = \|f'\|_\infty\leq \|f\|_{C^1} \|\varphi\| \leq 1 \displaystyle \frac{|\varphi(f)|}{\|f\|_{C^1}}\leq \frac 12 \|\varphi\|>\frac 12 f:t\mapsto \sin(2t) \varphi(f) \approx 1.60 \|f\|_{C^1} = 1 + 2 = 3 \approx 0.53","['real-analysis', 'functional-analysis']"
15,Hard inequality :$\Big(\frac{1}{a^2+b^2}\Big)^2+\Big(\frac{1}{b^2+c^2}\Big)^2+\Big(\frac{1}{c^2+a^2}\Big)^2\geq \frac{3}{4}$,Hard inequality :,\Big(\frac{1}{a^2+b^2}\Big)^2+\Big(\frac{1}{b^2+c^2}\Big)^2+\Big(\frac{1}{c^2+a^2}\Big)^2\geq \frac{3}{4},"I have a hard problem this is it : Let $a,b,c>0$ such that $a^ab^bc^c=1$ then we have : $$\Big(\frac{1}{a^2+b^2}\Big)^2+\Big(\frac{1}{b^2+c^2}\Big)^2+\Big(\frac{1}{c^2+a^2}\Big)^2\geq \frac{3}{4}$$ I try to use Jensen's ienquality applied to the function $f(x)=\frac{1}{x^2}$ but it doesn't works for all the values . if we apply Am-Gm it's like Jensen's inequality so I forget this ways . Maybe If we apply Karamata's inequality but I didn't found the right majorization . I try also to use Muirhead inequality but without success . So I'm a bit lost with that if you have a hint or a full answer I will be happy to read your work .",I have a hard problem this is it : Let such that then we have : I try to use Jensen's ienquality applied to the function but it doesn't works for all the values . if we apply Am-Gm it's like Jensen's inequality so I forget this ways . Maybe If we apply Karamata's inequality but I didn't found the right majorization . I try also to use Muirhead inequality but without success . So I'm a bit lost with that if you have a hint or a full answer I will be happy to read your work .,"a,b,c>0 a^ab^bc^c=1 \Big(\frac{1}{a^2+b^2}\Big)^2+\Big(\frac{1}{b^2+c^2}\Big)^2+\Big(\frac{1}{c^2+a^2}\Big)^2\geq \frac{3}{4} f(x)=\frac{1}{x^2}","['real-analysis', 'inequality', 'exponentiation']"
16,Stationary point of a smooth function $f:\Bbb R^2\to \Bbb R$,Stationary point of a smooth function,f:\Bbb R^2\to \Bbb R,"Let $f \in C^{\infty}(\mathbb{R^{2}})$ be a function such that: $$\lim_{y \to \pm \infty} f(x,y)=+\infty, \ \forall x \in \mathbb{R},$$ and that $$\lim_{x \to \pm \infty} f(x,y)=-\infty, \ \forall y \in \mathbb{R}.$$ With this hypothesis, does a stationary point necessarily exist? I'm looking for either a proof of the existence or a counterexample.  It sounds false to me, but I didn't find a counterexample.","Let $f \in C^{\infty}(\mathbb{R^{2}})$ be a function such that: $$\lim_{y \to \pm \infty} f(x,y)=+\infty, \ \forall x \in \mathbb{R},$$ and that $$\lim_{x \to \pm \infty} f(x,y)=-\infty, \ \forall y \in \mathbb{R}.$$ With this hypothesis, does a stationary point necessarily exist? I'm looking for either a proof of the existence or a counterexample.  It sounds false to me, but I didn't find a counterexample.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
17,"""Why"" is $[\mathbb{C}:\mathbb{R}] < \infty$?","""Why"" is ?",[\mathbb{C}:\mathbb{R}] < \infty,"Obviously this question is a little open-ended. A lot of complex analysis seems to work primarily because we can view $\mathbb{C}$ as a finite-dimensional $\mathbb{R}$-algebra, and apply analytic and geometric ideas which work only (or at least best) in finite-dimensional real space. When we consider most other fields that we come across in practice (for instance, $\mathbb{F}_q$,$\mathbb{Q}_p$, or $\mathbb{Q}$) generally their algebraic closures are infinite-dimensional extensions. Is there any intuitive reason why the way we construct $\mathbb{R}$ would suggest that we were producing a field whose algebraic closure was a finite-dimensional extension?  Does such a construction generalize to other fields in any way?","Obviously this question is a little open-ended. A lot of complex analysis seems to work primarily because we can view $\mathbb{C}$ as a finite-dimensional $\mathbb{R}$-algebra, and apply analytic and geometric ideas which work only (or at least best) in finite-dimensional real space. When we consider most other fields that we come across in practice (for instance, $\mathbb{F}_q$,$\mathbb{Q}_p$, or $\mathbb{Q}$) generally their algebraic closures are infinite-dimensional extensions. Is there any intuitive reason why the way we construct $\mathbb{R}$ would suggest that we were producing a field whose algebraic closure was a finite-dimensional extension?  Does such a construction generalize to other fields in any way?",,"['real-analysis', 'abstract-algebra', 'complex-analysis', 'soft-question', 'intuition']"
18,Conditions for continuous extension of a function on an open set to its closure,Conditions for continuous extension of a function on an open set to its closure,,"Suppose $U$ is a open set in $\Bbb R^n$, and suppose $f\colon U\to \Bbb R$ is a continuous function. Suppose that $f$ is uniformly continuous on every bounded subset of $U$. Question: Can $f$ be continuously extended to the closure of $U$ in $\Bbb R^n$?","Suppose $U$ is a open set in $\Bbb R^n$, and suppose $f\colon U\to \Bbb R$ is a continuous function. Suppose that $f$ is uniformly continuous on every bounded subset of $U$. Question: Can $f$ be continuously extended to the closure of $U$ in $\Bbb R^n$?",,"['real-analysis', 'analysis', 'continuity']"
19,Nasty examples for different classes of functions,Nasty examples for different classes of functions,,"Let $f: \mathbb{R} \to \mathbb{R}$ be a function. Usually when proving a theorem where $f$ is assumed to be continuous, differentiable, $C^1$ or smooth, it is enough to draw intuition by assuming that $f$ is piecewise smooth (something that one could perhaps draw on a paper without lifting your pencil). What I'm saying is that in all these cases my mental picture is about the same. This works most of the time, but sometimes it of course doesn't. Hence I would like to ask for examples of continuous, differentiable and $C^1$ functions, which would highlight the differences between the different classes. I'm especially interested in how nasty differentiable functions can be compared to continuously differentiable ones. Also if it is the case that the one dimensional case happens to be uninteresting, feel free to expand your answer to functions $\mathbb{R}^n \to \mathbb{R}^m$. The optimal answer would also list some general minimal 'sanity-checks' for different classes of functions, which a proof of a theorem concerning a particular class would have to take into account.","Let $f: \mathbb{R} \to \mathbb{R}$ be a function. Usually when proving a theorem where $f$ is assumed to be continuous, differentiable, $C^1$ or smooth, it is enough to draw intuition by assuming that $f$ is piecewise smooth (something that one could perhaps draw on a paper without lifting your pencil). What I'm saying is that in all these cases my mental picture is about the same. This works most of the time, but sometimes it of course doesn't. Hence I would like to ask for examples of continuous, differentiable and $C^1$ functions, which would highlight the differences between the different classes. I'm especially interested in how nasty differentiable functions can be compared to continuously differentiable ones. Also if it is the case that the one dimensional case happens to be uninteresting, feel free to expand your answer to functions $\mathbb{R}^n \to \mathbb{R}^m$. The optimal answer would also list some general minimal 'sanity-checks' for different classes of functions, which a proof of a theorem concerning a particular class would have to take into account.",,"['real-analysis', 'intuition', 'examples-counterexamples']"
20,Do Wronskians have the intermediate value property?,Do Wronskians have the intermediate value property?,,"I wonder if the following is true: Conjecture: Let $I \subset \Bbb R$ be an open interval and $f, g: I \to \Bbb R$ be differentiable functions. Then the Wronskian $$ W(f,g) =\begin{vmatrix}f &g \\f' & g'\end{vmatrix} = f g' - f'g $$ is a Darboux function. A Darboux function is a real-valued function $f$ which has the “intermediate value property”: for any two values $a$ and $b$ in the domain of $f$ , and any $y$ between $f(a)$ and $f(b)$ , there is some $c$ between $a$ and $b$ with $f(c) = y$ . Motivation and thoughts: In Does there exists two differentiable functions $f, g$ on $I$ such that $W(f, g) (x) >0$ on $A$ and $W(f, g) <0$ on $I\setminus A$? it what proved that If the Wronskian takes both positive and negative values on an interval, then it must be zero somewhere. and my conjecture would be a natural generalization. However, I do not yet see how the case of an arbitrary intermediate value $y$ can be reduced to the special case of $y = 0$ as the intermediate value. The conjecture is (trivially) true if both $f$ and $g$ are continuously differentiable, since then $W(f, g)$ is continuous. So the interesting case is that $f$ and $g$ are just assumed to be differentiable. Derivatives have the Darboux property, that covers the case that $f$ or $g$ is constant, e.g. $W(1, g) = g'$ . Sums and products of Darboux functions are not necessarily Darboux functions (see for example The sum of Darboux is a Darboux function? ). So even if all terms in $f g' - f'g$ have the intermediate value property, there is no immediate way to conclude the conjecture. A (failed) proof attempt: Assume that $w = W(f, g)$ does not take a value $y \in \Bbb R$ , and consider the sets $$ A = \{ x \in I \mid w(x) > y \} \, , \, B = \{ x \in I \mid w(x) < y \} \, . $$ If we can show that both $A$ and $B$ are open then one of them must be empty (since $I$ is connected), and we are done. If $f(x_0) \ne 0$ then we can define $h(x) = y \int_{x_0}^x f(t)^{-2} dt$ in a neighborhood of $x_0$ , and $$  W(f, g) -y = f^2 \left( \frac gf - h\right)' $$ shows that $W(f, g) -y$ does not change its sign near $x_0$ , so that $x_0$ is an interior point of $A$ or of $B$ . A similar argument works if $g(x_0) \ne 0$ . However, other than in my previous answer , one can not exclude the case $f(x_0) = g(x_0) = 0$ . That is where my I am stuck in my current proof attempt.","I wonder if the following is true: Conjecture: Let be an open interval and be differentiable functions. Then the Wronskian is a Darboux function. A Darboux function is a real-valued function which has the “intermediate value property”: for any two values and in the domain of , and any between and , there is some between and with . Motivation and thoughts: In Does there exists two differentiable functions $f, g$ on $I$ such that $W(f, g) (x) >0$ on $A$ and $W(f, g) <0$ on $I\setminus A$? it what proved that If the Wronskian takes both positive and negative values on an interval, then it must be zero somewhere. and my conjecture would be a natural generalization. However, I do not yet see how the case of an arbitrary intermediate value can be reduced to the special case of as the intermediate value. The conjecture is (trivially) true if both and are continuously differentiable, since then is continuous. So the interesting case is that and are just assumed to be differentiable. Derivatives have the Darboux property, that covers the case that or is constant, e.g. . Sums and products of Darboux functions are not necessarily Darboux functions (see for example The sum of Darboux is a Darboux function? ). So even if all terms in have the intermediate value property, there is no immediate way to conclude the conjecture. A (failed) proof attempt: Assume that does not take a value , and consider the sets If we can show that both and are open then one of them must be empty (since is connected), and we are done. If then we can define in a neighborhood of , and shows that does not change its sign near , so that is an interior point of or of . A similar argument works if . However, other than in my previous answer , one can not exclude the case . That is where my I am stuck in my current proof attempt.","I \subset \Bbb R f, g: I \to \Bbb R 
W(f,g) =\begin{vmatrix}f &g \\f' & g'\end{vmatrix} = f g' - f'g
 f a b f y f(a) f(b) c a b f(c) = y y y = 0 f g W(f, g) f g f g W(1, g) = g' f g' - f'g w = W(f, g) y \in \Bbb R 
A = \{ x \in I \mid w(x) > y \} \, , \, B = \{ x \in I \mid w(x) < y \} \, .
 A B I f(x_0) \ne 0 h(x) = y \int_{x_0}^x f(t)^{-2} dt x_0 
 W(f, g) -y = f^2 \left( \frac gf - h\right)'
 W(f, g) -y x_0 x_0 A B g(x_0) \ne 0 f(x_0) = g(x_0) = 0","['real-analysis', 'determinant', 'wronskian']"
21,Prove that $\int\limits_0^1 \bigg | \frac{f''(x)}{f(x)} \bigg| dx \ge \frac{4(M-1)}{M}$,Prove that,\int\limits_0^1 \bigg | \frac{f''(x)}{f(x)} \bigg| dx \ge \frac{4(M-1)}{M},"Let $f:[0,1]\to \mathbb{R}$ be a $C^2$ class function such that $f(0)=f(1)=1$ and $f(x)>1,\forall x\in (0,1)$ . Prove that $$\int\limits_0^1 \bigg | \frac{f''(x)}{f(x)} \bigg| dx \ge \frac{4(M-1)}{M},$$ where $M=\max\limits_{x\in [0,1]}f(x).$ I can't solve this problem, but here are some of the things I have tried/observed: $0$ and $1$ are extreme points of $f$ , but we cannot apply Fermat's theorem since they are the endpoints of $f$ 's domain $$\int\limits_0^1 \bigg | \frac{f''(x)}{f(x)} \bigg| dx \ge \int\limits_0^1 \bigg |\frac{f''(x)}{f'(x)}\cdot \frac{f'(x)}{f(x)}\bigg | dx\ge \bigg|\int\limits_0^1 \frac{f''(x)}{f'(x)}\cdot \frac{f'(x)}{f(x)} dx \bigg|$$ Then, I tried to apply Integration by Parts, but to no avail. $\exists m\in (0,1)$ such that $f(m)=M$ . From Lagrange's MVT on $[0,m]$ and $[m,1]$ , \exists $a\in (0,m), b\in (m,1)$ such that $$M=f(m)=mf'(a) \space \text{and} \space M-1=f(m)-1=(m-1)f'(b).$$ Now, we have that $$\int\limits_0^1 \bigg | \frac{f''(x)}{f(x)} \bigg| dx \ge \frac{1}{M}\int\limits_0^1 |f''(x) |dx \ge \frac{1}{M} \int\limits_a^b |f''(x)|dx \ge \frac{1}{M} \bigg|\int\limits_a^b f''(x) dx \bigg |=$$ $$=\frac{1}{M}(f'(b)-f'(a))$$ If I substitute $f'(b)$ and $f'(a)$ in terms of $m$ and $M$ , I cannot prove the required inequality. EDIT: It should be $M-1=mf'(a)$ . With this, the problem is solved if we proceed the way I did.","Let be a class function such that and . Prove that where I can't solve this problem, but here are some of the things I have tried/observed: and are extreme points of , but we cannot apply Fermat's theorem since they are the endpoints of 's domain Then, I tried to apply Integration by Parts, but to no avail. such that . From Lagrange's MVT on and , \exists such that Now, we have that If I substitute and in terms of and , I cannot prove the required inequality. EDIT: It should be . With this, the problem is solved if we proceed the way I did.","f:[0,1]\to \mathbb{R} C^2 f(0)=f(1)=1 f(x)>1,\forall x\in (0,1) \int\limits_0^1 \bigg | \frac{f''(x)}{f(x)} \bigg| dx \ge \frac{4(M-1)}{M}, M=\max\limits_{x\in [0,1]}f(x). 0 1 f f \int\limits_0^1 \bigg | \frac{f''(x)}{f(x)} \bigg| dx \ge \int\limits_0^1 \bigg |\frac{f''(x)}{f'(x)}\cdot \frac{f'(x)}{f(x)}\bigg | dx\ge \bigg|\int\limits_0^1 \frac{f''(x)}{f'(x)}\cdot \frac{f'(x)}{f(x)} dx \bigg| \exists m\in (0,1) f(m)=M [0,m] [m,1] a\in (0,m), b\in (m,1) M=f(m)=mf'(a) \space \text{and} \space M-1=f(m)-1=(m-1)f'(b). \int\limits_0^1 \bigg | \frac{f''(x)}{f(x)} \bigg| dx \ge \frac{1}{M}\int\limits_0^1 |f''(x) |dx \ge \frac{1}{M} \int\limits_a^b |f''(x)|dx \ge \frac{1}{M} \bigg|\int\limits_a^b f''(x) dx \bigg |= =\frac{1}{M}(f'(b)-f'(a)) f'(b) f'(a) m M M-1=mf'(a)","['real-analysis', 'inequality', 'integral-inequality']"
22,$f(a)-f(b)$ is rational iff $f(a-b) $ is rational,is rational iff  is rational,f(a)-f(b) f(a-b) ,"Prove that the continuous function $f:\mathbb{R} \to \mathbb{R}$ satisfying $f\left(x\right)-f\left(y\right)  \in\mathbb{Q} \iff f\left(x-y\right) \in \mathbb{Q}$ is of the form $  f\left(x\right)=ax+b.$ My Attempt. I tried considering the function $$g\left(x\right)=\frac{f\left(x\right)-f\left(0\right)}{f\left(1\right)-f\left(0\right)}$$ which also satisfies the property $g\left(a\right)-g\left(b\right)  \in\mathbb{Q} \iff g\left(a-b\right) \in \mathbb{Q}$ , Now I am trying to prove that this function g is identity function and then I can prove that $f\left(x\right)=\left(f\left(1\right)-f\left(0\right)\right)x+f\left(0\right).$ And I am done. Also This function has to be identity function because $g\left(0\right)=0$ and $g\left(1\right)=1$ . I tried assuming that the function g is such that $g\left(a\right)\neq a$ for some $a\in \mathbb{R}$ . Then by continuity $g\left(x\right)\neq x$ for some $\delta>0$ neighborhood of $a$ . But I cannot move further. Also Using a previously known result, I was able to prove that f must be monotonic. However I do not want to use any other result which is not known and not trivial. If a function $f $ is continous in $\left[a,b\right]$ and $f\left(a\right)=f\left(b\right)$ then for any $\epsilon >0$ there exists $m,n \in \left[a,b\right] $ such that $f\left(m\right)=f\left(n\right)$ and $m-n=\epsilon$ . In this case choose $\epsilon \in \mathbb{R}-\mathbb{Q} $ and get $m,n \in \left[a,b\right] $ such that $f\left(m\right)-f\left(n\right)=0 \in \mathbb{Q}$ but $m-n \in\mathbb{R}-\mathbb{Q}$ .","Prove that the continuous function satisfying is of the form My Attempt. I tried considering the function which also satisfies the property , Now I am trying to prove that this function g is identity function and then I can prove that And I am done. Also This function has to be identity function because and . I tried assuming that the function g is such that for some . Then by continuity for some neighborhood of . But I cannot move further. Also Using a previously known result, I was able to prove that f must be monotonic. However I do not want to use any other result which is not known and not trivial. If a function is continous in and then for any there exists such that and . In this case choose and get such that but .","f:\mathbb{R} \to \mathbb{R} f\left(x\right)-f\left(y\right)  \in\mathbb{Q} \iff f\left(x-y\right) \in \mathbb{Q}  
f\left(x\right)=ax+b. g\left(x\right)=\frac{f\left(x\right)-f\left(0\right)}{f\left(1\right)-f\left(0\right)} g\left(a\right)-g\left(b\right)  \in\mathbb{Q} \iff g\left(a-b\right) \in \mathbb{Q} f\left(x\right)=\left(f\left(1\right)-f\left(0\right)\right)x+f\left(0\right). g\left(0\right)=0 g\left(1\right)=1 g\left(a\right)\neq a a\in \mathbb{R} g\left(x\right)\neq x \delta>0 a f  \left[a,b\right] f\left(a\right)=f\left(b\right) \epsilon >0 m,n \in \left[a,b\right]  f\left(m\right)=f\left(n\right) m-n=\epsilon \epsilon \in \mathbb{R}-\mathbb{Q}  m,n \in \left[a,b\right]  f\left(m\right)-f\left(n\right)=0 \in \mathbb{Q} m-n \in\mathbb{R}-\mathbb{Q}","['real-analysis', 'continuity', 'functional-equations', 'rational-numbers', 'fixed-point-theorems']"
23,Regarding linear independence on a normed-linear space given a condition,Regarding linear independence on a normed-linear space given a condition,,"Let $(X,\|\cdot\|)$ be a normed linear space and $x_{1}, x_{2}, \ldots, x_{n}$ be $n$ linearly independent vectors in $X$. Show that there exists $\epsilon > 0$ such that if $y_{1}, y_{2}, \cdots, y_{n} \in X$ with $\|y_{i}\| < \epsilon$, $i = 1,2,\ldots,n$, then $x_{1} + y_{1}, x_{2} + y_{2},\ldots,x_{n} + y_{n}$ are also linearly independent vectors in $X$. I've been thinking over this for a few days and I'm not really getting anywhere. I've tried to start with the definition of linear independence for the $x$-vectors and then working towards the linear independence of $x+y$ given the condition on $y$, but I'm not getting anywhere. May someone offer a hint or a solution?","Let $(X,\|\cdot\|)$ be a normed linear space and $x_{1}, x_{2}, \ldots, x_{n}$ be $n$ linearly independent vectors in $X$. Show that there exists $\epsilon > 0$ such that if $y_{1}, y_{2}, \cdots, y_{n} \in X$ with $\|y_{i}\| < \epsilon$, $i = 1,2,\ldots,n$, then $x_{1} + y_{1}, x_{2} + y_{2},\ldots,x_{n} + y_{n}$ are also linearly independent vectors in $X$. I've been thinking over this for a few days and I'm not really getting anywhere. I've tried to start with the definition of linear independence for the $x$-vectors and then working towards the linear independence of $x+y$ given the condition on $y$, but I'm not getting anywhere. May someone offer a hint or a solution?",,"['real-analysis', 'linear-algebra', 'functional-analysis', 'normed-spaces']"
24,Does this tricky series converge?,Does this tricky series converge?,,"$$\sum_2 \frac{\cos(\log{n})}{n\log{n}}$$ The naive attempt is to use dirichlet's test to falsely claim that $\cos(\log{n})$ has bounded partial sums, but I don't think it works. I am also trying a difference of sum and integral type of strategy but am not sure where to go from it. Finally, as A.S pointed out below in the comments, the integral test does not apply, since cos(logn) does not behave monotonically... Any ideas are welcome. Thanks,","$$\sum_2 \frac{\cos(\log{n})}{n\log{n}}$$ The naive attempt is to use dirichlet's test to falsely claim that $\cos(\log{n})$ has bounded partial sums, but I don't think it works. I am also trying a difference of sum and integral type of strategy but am not sure where to go from it. Finally, as A.S pointed out below in the comments, the integral test does not apply, since cos(logn) does not behave monotonically... Any ideas are welcome. Thanks,",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
25,Let $f$ be a bounded measurable function on $E$. Show that there are sequences of simple functions converging uniformly on $E$.,Let  be a bounded measurable function on . Show that there are sequences of simple functions converging uniformly on .,f E E,"Let $f$ be a bounded measurable function on $E$. Show that there are sequences of simple functions on $E$, $\{\phi_n\}$ and $\{\psi_n\}$, such that $\{\phi_n\}$ is increasing and $\{\psi_n\}$ is decreasing and each of these sequences converges uniformly on $E$. Let $f:E \rightarrow \mathbb{R}$ be measurable. There exist simple measurable functions $\phi_n$ on $E$ such that     $$(a) \  \phi_1 \leq \phi_2 \leq \cdots \leq f.$$      $$(b) \ \forall x\in E, \text{ we have that } \phi_n(x) \rightarrow f(x), \text{ as }n \rightarrow \infty$$ Proof: Notice that for each positive integer $n$ and each real number $t$ corresponds a unique integer $k = k_n(t)$ that satisfies $k2^{-n} \leq t < (k+1)2^{-n}$. Define: $$ s_n(t) = \left\{\begin{matrix}  k_n(t)2^{-n} & 0 \leq t < n\\ n & n \leq t \leq \infty \end{matrix}\right. $$ Note that each $s_n$ is a borel function on $[0,\infty]$. In other words, for any open set $V$, $s_n^{-1}(V)$ is a borel set. Now, $$k2^{-n} \leq t  < (k+1)2^{-n}  \Rightarrow t - 2^{-n} < s_n(t) \leq t \text{ if } 0 \leq t\leq n,$$ thus, $0 \leq s_1 \leq s_2 \leq \cdots \leq t,$ and $s_n(t) \rightarrow t$ as $n \rightarrow \infty,$ for every $t\in [0,\infty]$. It follows that the function $\phi_n = s_n\circ f$ satisfy (a) and (b); since $f$ is measurable and $s_n$ is a borel function, then $\phi_n$ is also measurable. To obtain a decreasing function, let $\psi_n = -s_n(-f)$, thus $\phi_n$ and $\psi_n$ are steps functions, $ \phi_n \leq f\leq \psi_n$ and $\psi_n-\phi_n \leq 2^{-n}$ for every integer $n$. My question is how I can obtain uniform convergence? I know that from what i proved, I didn't needed to assume that $f$ is bounded. The following theorem might be useful, namely Dini's Theorem(obtained from wikipedia). If $Y$ is a compact topological space, and $\{f_n\}_{n\in \mathbb{N}}$ is a monotonically increasing sequence (meaning $f_n(x) \leq f_{n+1}(x)$ for all $n$ and $x$) of continuous real-valued functions on $X$ which converges pointwise to a continuous function $f$, then the convergence is uniform. The same conclusion holds if $\{f_n\}_{n\in \mathbb{N}}$ is monotonically decreasing instead of increasing. Is there a way to use this theorem under the assumption that $f$ is bounded to obtain uniform convergence?","Let $f$ be a bounded measurable function on $E$. Show that there are sequences of simple functions on $E$, $\{\phi_n\}$ and $\{\psi_n\}$, such that $\{\phi_n\}$ is increasing and $\{\psi_n\}$ is decreasing and each of these sequences converges uniformly on $E$. Let $f:E \rightarrow \mathbb{R}$ be measurable. There exist simple measurable functions $\phi_n$ on $E$ such that     $$(a) \  \phi_1 \leq \phi_2 \leq \cdots \leq f.$$      $$(b) \ \forall x\in E, \text{ we have that } \phi_n(x) \rightarrow f(x), \text{ as }n \rightarrow \infty$$ Proof: Notice that for each positive integer $n$ and each real number $t$ corresponds a unique integer $k = k_n(t)$ that satisfies $k2^{-n} \leq t < (k+1)2^{-n}$. Define: $$ s_n(t) = \left\{\begin{matrix}  k_n(t)2^{-n} & 0 \leq t < n\\ n & n \leq t \leq \infty \end{matrix}\right. $$ Note that each $s_n$ is a borel function on $[0,\infty]$. In other words, for any open set $V$, $s_n^{-1}(V)$ is a borel set. Now, $$k2^{-n} \leq t  < (k+1)2^{-n}  \Rightarrow t - 2^{-n} < s_n(t) \leq t \text{ if } 0 \leq t\leq n,$$ thus, $0 \leq s_1 \leq s_2 \leq \cdots \leq t,$ and $s_n(t) \rightarrow t$ as $n \rightarrow \infty,$ for every $t\in [0,\infty]$. It follows that the function $\phi_n = s_n\circ f$ satisfy (a) and (b); since $f$ is measurable and $s_n$ is a borel function, then $\phi_n$ is also measurable. To obtain a decreasing function, let $\psi_n = -s_n(-f)$, thus $\phi_n$ and $\psi_n$ are steps functions, $ \phi_n \leq f\leq \psi_n$ and $\psi_n-\phi_n \leq 2^{-n}$ for every integer $n$. My question is how I can obtain uniform convergence? I know that from what i proved, I didn't needed to assume that $f$ is bounded. The following theorem might be useful, namely Dini's Theorem(obtained from wikipedia). If $Y$ is a compact topological space, and $\{f_n\}_{n\in \mathbb{N}}$ is a monotonically increasing sequence (meaning $f_n(x) \leq f_{n+1}(x)$ for all $n$ and $x$) of continuous real-valued functions on $X$ which converges pointwise to a continuous function $f$, then the convergence is uniform. The same conclusion holds if $\{f_n\}_{n\in \mathbb{N}}$ is monotonically decreasing instead of increasing. Is there a way to use this theorem under the assumption that $f$ is bounded to obtain uniform convergence?",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
26,Bounds of the derivatives of the mollifier function,Bounds of the derivatives of the mollifier function,,"The standard mollifier function is defined as follows $$f(x)=\begin{cases} 0 & \text{if } |x| \ge 1\\ \exp \left(-\cfrac{1}{1-x^2}\right) & \text{if } |x|<1.\end{cases}$$ It is well known that $f$ is $C^\infty$ , and $f^{(n)}(x)=0$ for $|x| \ge 1$ . On the interval $x\in (-1,1)$ , the derivative $$\displaystyle f^{(n)}(x)=\frac{P_n(x)}{(1-x^2)^{2n}}\cdot f(x)$$ where $P_n$ is a polynomial function of $x$ defined inductively by $$P_0(x) \equiv 1, \qquad P_1(x)=-2x, \qquad P_{n+1}(x)=P_n'(x)(1-x^2)^2+4nx(1-x^2) P_n(x)-2xP_n(x)$$ Note that $\displaystyle \sup_{|x|<1} f^{(n)}(x)<+\infty$ , since $f^{(n)}(\pm 1)=0$ . So $\displaystyle |f|_n:=\max_{|x|<1} f^{(n)}(x)$ is well defined. Are there some rough/good estimates on the size $|f|_n$ of the derivatives? Thanks!","The standard mollifier function is defined as follows It is well known that is , and for . On the interval , the derivative where is a polynomial function of defined inductively by Note that , since . So is well defined. Are there some rough/good estimates on the size of the derivatives? Thanks!","f(x)=\begin{cases} 0 & \text{if } |x| \ge 1\\ \exp \left(-\cfrac{1}{1-x^2}\right) & \text{if } |x|<1.\end{cases} f C^\infty f^{(n)}(x)=0 |x| \ge 1 x\in (-1,1) \displaystyle f^{(n)}(x)=\frac{P_n(x)}{(1-x^2)^{2n}}\cdot f(x) P_n x P_0(x) \equiv 1, \qquad P_1(x)=-2x, \qquad P_{n+1}(x)=P_n'(x)(1-x^2)^2+4nx(1-x^2) P_n(x)-2xP_n(x) \displaystyle \sup_{|x|<1} f^{(n)}(x)<+\infty f^{(n)}(\pm 1)=0 \displaystyle |f|_n:=\max_{|x|<1} f^{(n)}(x) |f|_n","['real-analysis', 'derivatives']"
27,Proof of fundamental lemma of calculus of variation.,Proof of fundamental lemma of calculus of variation.,,"Suppose $\Omega$ is an open subset of $\mathbb{R}^n$ and let $L^1_\text{Loc}\Omega$ denote all locally integrable functions on $\Omega$ and $C^{\infty}_0\Omega$ for smooth functions whose support lie in $\Omega$. My teacher tell me the following statement: Suppose $f\in L_{Loc}^1\Omega$ and $$\int_\Omega f\varphi=0,\forall\varphi\in C^\infty_0\Omega$$ Then $f=0\text{ a.e.}$ It is known as fundamental lemma of calculus of variation. My teacher told me it suffices to prove this statement holds for the case $f$ is continuous. But I find it's not easy to deduce the lemma from the case $f$ is continuous. Could someone tell me how to do this or how to prove the lemma directly?  Thanks a lot!","Suppose $\Omega$ is an open subset of $\mathbb{R}^n$ and let $L^1_\text{Loc}\Omega$ denote all locally integrable functions on $\Omega$ and $C^{\infty}_0\Omega$ for smooth functions whose support lie in $\Omega$. My teacher tell me the following statement: Suppose $f\in L_{Loc}^1\Omega$ and $$\int_\Omega f\varphi=0,\forall\varphi\in C^\infty_0\Omega$$ Then $f=0\text{ a.e.}$ It is known as fundamental lemma of calculus of variation. My teacher told me it suffices to prove this statement holds for the case $f$ is continuous. But I find it's not easy to deduce the lemma from the case $f$ is continuous. Could someone tell me how to do this or how to prove the lemma directly?  Thanks a lot!",,"['real-analysis', 'calculus-of-variations']"
28,Packing an infinite sequence of disks,Packing an infinite sequence of disks,,"Let $a > 1$ and $Q(a)$ denote the supremum of values of $q$ such that a countably infinite collection of disks, whose areas form an infinitely decreasing geometric progression with the start value $1$ and the common ratio $q$ (i.e. $\{1,\ q,\ q^2,\ q^3,\ \dots\}$), can be packed within a circle of area $a$. Apparently, $0 < Q(a) < 1$ and is monotonically increasing. Can we find a closed form expression for $Q(a)$? Or at least, some non-trivial bounds? Can we find the exact value of $Q(2)$, for example? What properties of the function $Q(a)$ can be proved? Is it continuous, smooth, analytic? What's its asymptotic behavior for $a\to1$ and $a\to\infty$?","Let $a > 1$ and $Q(a)$ denote the supremum of values of $q$ such that a countably infinite collection of disks, whose areas form an infinitely decreasing geometric progression with the start value $1$ and the common ratio $q$ (i.e. $\{1,\ q,\ q^2,\ q^3,\ \dots\}$), can be packed within a circle of area $a$. Apparently, $0 < Q(a) < 1$ and is monotonically increasing. Can we find a closed form expression for $Q(a)$? Or at least, some non-trivial bounds? Can we find the exact value of $Q(2)$, for example? What properties of the function $Q(a)$ can be proved? Is it continuous, smooth, analytic? What's its asymptotic behavior for $a\to1$ and $a\to\infty$?",,"['real-analysis', 'asymptotics', 'closed-form', 'packing-problem', 'combinatorial-geometry']"
29,Failure of Doob-Dynkin lemma in general measurable spaces,Failure of Doob-Dynkin lemma in general measurable spaces,,"The version of the Doob-Dynkin lemma given in my textbook is as follows: Let $f: \Omega_1 \to \Omega_2$ be a function, let $\mathcal{F}$ be a $\sigma$-algebra on $\Omega_2$, and let $\sigma(f)$ be the $\sigma$-algebra on $\Omega_1$ generated by $f$. Denote the Borel $\sigma$-algebra on $\mathbb{R}$ by $B(\mathbb{R})$. Then, $h : (\Omega_1, \sigma(f)) \to (\mathbb{R}, B(\mathbb{R})) $ is measurable if and only if $h = g \circ f$ for some measurable function $g: (\Omega_2, \mathcal{F}) \to (\mathbb{R}, B(\mathbb{R}))  $. My textbook also states that this lemma fails if we replace $(\mathbb{R},B(\mathbb{R}))$ by some other measurable space, but does not provide an example of this failure. As such, I'm trying to come up with such an example on my own, but am not sure where to begin. Any help is appreciated!","The version of the Doob-Dynkin lemma given in my textbook is as follows: Let $f: \Omega_1 \to \Omega_2$ be a function, let $\mathcal{F}$ be a $\sigma$-algebra on $\Omega_2$, and let $\sigma(f)$ be the $\sigma$-algebra on $\Omega_1$ generated by $f$. Denote the Borel $\sigma$-algebra on $\mathbb{R}$ by $B(\mathbb{R})$. Then, $h : (\Omega_1, \sigma(f)) \to (\mathbb{R}, B(\mathbb{R})) $ is measurable if and only if $h = g \circ f$ for some measurable function $g: (\Omega_2, \mathcal{F}) \to (\mathbb{R}, B(\mathbb{R}))  $. My textbook also states that this lemma fails if we replace $(\mathbb{R},B(\mathbb{R}))$ by some other measurable space, but does not provide an example of this failure. As such, I'm trying to come up with such an example on my own, but am not sure where to begin. Any help is appreciated!",,"['real-analysis', 'measure-theory', 'probability-theory']"
30,Restriction of smooth functions.,Restriction of smooth functions.,,"Consider the following question: Suppose that $X$ is a subset of $\mathbb{R}^n$ and $Z$ is a subset of $X$. Show that the restriction to $Z$ of any smooth map on $X$ is a smooth map on $Z$. (Note: A smooth function is defined to be one that has continuous partial derivatives of all orders) What exactly is this question asking? I don't see why this does not follow immediately, and so I must be missing something quite crucial. I don't think the question was intended to prompt an answer of the form Proof: QED.","Consider the following question: Suppose that $X$ is a subset of $\mathbb{R}^n$ and $Z$ is a subset of $X$. Show that the restriction to $Z$ of any smooth map on $X$ is a smooth map on $Z$. (Note: A smooth function is defined to be one that has continuous partial derivatives of all orders) What exactly is this question asking? I don't see why this does not follow immediately, and so I must be missing something quite crucial. I don't think the question was intended to prompt an answer of the form Proof: QED.",,"['real-analysis', 'differential-geometry', 'differential-topology']"
31,Monotonicity of $\ell_p$ norm,Monotonicity of  norm,\ell_p,"Consider a $n$ dimensional space, it is known (Wikipedia) that for $p>r>0$, we have $$ \|x\|_p\leq\|x\|_r\leq n^{(1/r-1/p)}\|x\|_p. $$ I have two questions about the above inequality. $(\bf 1)$. The first is how to show $\|x\|_p\leq\|x\|_r$ when $p,r\leq1$. When $p>r\geq1$, we can define $$f(s)=\|x\|_s,\,\,s\geq1$$ and find out that $$f'(s)=\|x\|_s\left\{-\frac{1}{s^2}\log(\sum_i|x_i|^s)+\frac{1}{s}\frac{\sum_i|x_i|^s\log(|x_i|)}{\sum_i|x_i|^s}\right\}.$$ Then by the concavity of the $\log$ function, we can see that $$\frac{\sum_i|x_i|^s\log(|x_i|)}{\sum_i|x_i|^s}\leq \log\left(\sum_i\frac{|x_i|^s}{\sum_j|x_j|^s}\cdot|x_i|\right).$$ Let $$y_i=\frac{|x_i|^s}{\sum_j|x_j|^s},$$ it is easy to see $\|y\|_{s^*}\leq1$, where $s^*\geq1$ and $1/s+1/s^*=1$. Then, the Hölder's inequality leads to $$\frac{\sum_i|x_i|^s\log(|x_i|)}{\sum_i|x_i|^s}\leq \log\left(\sum_i\frac{|x_i|^s}{\sum_j|x_j|^s}\cdot|x_i|\right)= \log\left(\sum_iy_i\cdot|x_i|\right)\leq\log(\|x\|_s\|y\|_{s^*})\leq\log\|x\|_s.$$ Therefore, we can conclude $f'(s)\leq0$ and $\|x\|_p\leq\|x\|_r$ is satisfied. However, when $p,r<1$, we do not have $s^*\geq1$ and $\|y\|_{s^*}\leq1$. The last step does not work any more. (${\bf 2}$). My second question is how to show $\|x\|_r\leq n^{(1/r-1/p)}\|x\|_p.$ In fact, I was trying to show this by solving the following optimization problem: $$ \max_{\|x\|_p\leq1} \|x\|_r. $$ But seems it is difficult to derive a closed form solution. The objective function is non-smooth. Is there any elegant way to solve the above optimization problem? Can anyone give me a hint? Thanks a lot.","Consider a $n$ dimensional space, it is known (Wikipedia) that for $p>r>0$, we have $$ \|x\|_p\leq\|x\|_r\leq n^{(1/r-1/p)}\|x\|_p. $$ I have two questions about the above inequality. $(\bf 1)$. The first is how to show $\|x\|_p\leq\|x\|_r$ when $p,r\leq1$. When $p>r\geq1$, we can define $$f(s)=\|x\|_s,\,\,s\geq1$$ and find out that $$f'(s)=\|x\|_s\left\{-\frac{1}{s^2}\log(\sum_i|x_i|^s)+\frac{1}{s}\frac{\sum_i|x_i|^s\log(|x_i|)}{\sum_i|x_i|^s}\right\}.$$ Then by the concavity of the $\log$ function, we can see that $$\frac{\sum_i|x_i|^s\log(|x_i|)}{\sum_i|x_i|^s}\leq \log\left(\sum_i\frac{|x_i|^s}{\sum_j|x_j|^s}\cdot|x_i|\right).$$ Let $$y_i=\frac{|x_i|^s}{\sum_j|x_j|^s},$$ it is easy to see $\|y\|_{s^*}\leq1$, where $s^*\geq1$ and $1/s+1/s^*=1$. Then, the Hölder's inequality leads to $$\frac{\sum_i|x_i|^s\log(|x_i|)}{\sum_i|x_i|^s}\leq \log\left(\sum_i\frac{|x_i|^s}{\sum_j|x_j|^s}\cdot|x_i|\right)= \log\left(\sum_iy_i\cdot|x_i|\right)\leq\log(\|x\|_s\|y\|_{s^*})\leq\log\|x\|_s.$$ Therefore, we can conclude $f'(s)\leq0$ and $\|x\|_p\leq\|x\|_r$ is satisfied. However, when $p,r<1$, we do not have $s^*\geq1$ and $\|y\|_{s^*}\leq1$. The last step does not work any more. (${\bf 2}$). My second question is how to show $\|x\|_r\leq n^{(1/r-1/p)}\|x\|_p.$ In fact, I was trying to show this by solving the following optimization problem: $$ \max_{\|x\|_p\leq1} \|x\|_r. $$ But seems it is difficult to derive a closed form solution. The objective function is non-smooth. Is there any elegant way to solve the above optimization problem? Can anyone give me a hint? Thanks a lot.",,"['real-analysis', 'functional-analysis', 'optimization', 'normed-spaces', 'lp-spaces']"
32,taylor series and uniform convergence,taylor series and uniform convergence,,"Maybe is a silly question but I am confused...so I hope someone can help me. Is the convergence of the Taylor series uniform? To be more specific. We know for example that $\displaystyle{ e^x = \sum_{n=0}^{\infty} \frac{x^n}{ n!} \quad}$ , $\displaystyle{ \sin x = \sum_{n=0}^{\infty} \frac{ (-1)^{n+1} }{ (2n+1)!} x^{2n+1} }$ Now the question is: Does these series of functions converges uniformly to $e^x$ and $\sin x$ respectively? And one more question: Every Taylor series converges (pointiwise) for every $x$ so has radius of convergence $\infty$ right? These questions came up when I was studying and saw I my notes that the lecture interchange the summation and the integration of the Taylor series, for example $\displaystyle{ \frac{1}{ 1+ x^2} = \sum_{n=0}^{\infty} (-1)^n x^{2n}, |x|<1 \implies \arctan x = \int_{0}^{x} \sum_{n=0}^{\infty} (-1)^n t^{2n} dt=  \sum_{n=0}^{\infty} (-1)^n \int_{0}^{x} t^{2n} dt }$ and I can understand how can we do this if the convergence in not uniform, which in this example is not since $\displaystyle{ \sum_{n=0}^{\infty} x^n = \frac{1}{1-x} }$ pointwise but not uniform.","Maybe is a silly question but I am confused...so I hope someone can help me. Is the convergence of the Taylor series uniform? To be more specific. We know for example that $\displaystyle{ e^x = \sum_{n=0}^{\infty} \frac{x^n}{ n!} \quad}$ , $\displaystyle{ \sin x = \sum_{n=0}^{\infty} \frac{ (-1)^{n+1} }{ (2n+1)!} x^{2n+1} }$ Now the question is: Does these series of functions converges uniformly to $e^x$ and $\sin x$ respectively? And one more question: Every Taylor series converges (pointiwise) for every $x$ so has radius of convergence $\infty$ right? These questions came up when I was studying and saw I my notes that the lecture interchange the summation and the integration of the Taylor series, for example $\displaystyle{ \frac{1}{ 1+ x^2} = \sum_{n=0}^{\infty} (-1)^n x^{2n}, |x|<1 \implies \arctan x = \int_{0}^{x} \sum_{n=0}^{\infty} (-1)^n t^{2n} dt=  \sum_{n=0}^{\infty} (-1)^n \int_{0}^{x} t^{2n} dt }$ and I can understand how can we do this if the convergence in not uniform, which in this example is not since $\displaystyle{ \sum_{n=0}^{\infty} x^n = \frac{1}{1-x} }$ pointwise but not uniform.",,['real-analysis']
33,Summation of an Infinite Series: $\sum_{n=1}^\infty \frac{4^{2n}}{n^3 \binom{2n}{n}^2} = 8\pi G-14\zeta(3)$,Summation of an Infinite Series:,\sum_{n=1}^\infty \frac{4^{2n}}{n^3 \binom{2n}{n}^2} = 8\pi G-14\zeta(3),I am having trouble proving that $$\sum_{n=1}^\infty \frac{4^{2n}}{n^3 \binom{2n}{n}^2} = 8\pi G-14\zeta(3)$$ I know that $$\frac{2x \ \arcsin(x)}{\sqrt{1-x^2}} = \sum_{n=1}^\infty \frac{(2x)^{2n}}{n \binom{2n}{n}}$$ What should I do if the binomial coefficient is squared?,I am having trouble proving that $$\sum_{n=1}^\infty \frac{4^{2n}}{n^3 \binom{2n}{n}^2} = 8\pi G-14\zeta(3)$$ I know that $$\frac{2x \ \arcsin(x)}{\sqrt{1-x^2}} = \sum_{n=1}^\infty \frac{(2x)^{2n}}{n \binom{2n}{n}}$$ What should I do if the binomial coefficient is squared?,,"['calculus', 'real-analysis', 'sequences-and-series', 'binomial-coefficients', 'summation']"
34,Compute $\lim_{s\to 0} \left(\int_0^1 (\Gamma (x))^s\space\mathrm{dx}\right)^{1/s}$,Compute,\lim_{s\to 0} \left(\int_0^1 (\Gamma (x))^s\space\mathrm{dx}\right)^{1/s},"Compute  $$\lim_{s\to 0} \left(\int_0^1 (\Gamma (x))^s\space\mathrm{dx}\right)^{1/s}$$ This is a problem I thought of these days and I think I know a way although not completely justified. This is what I have Firstly take log $$\lim_{s\to 0} \frac{\ln\left(\int_0^1 (\Gamma (x))^s\space\mathrm{dx}\right)}{s}\space \text{(Unjustified part where considering the numerator tends to 0) }$$ and then apply  l'Hôpital's rule $$\lim_{s\to 0} \frac{\displaystyle \frac{d}{ds}\ln\left(\int_0^1 (\Gamma (x))^s\space\mathrm{dx}\right)}{\displaystyle \frac{d}{ds}s}\space=$$ $$\lim_{s\to 0} \frac{\displaystyle \frac{d}{ds} \left(\int_0^1 (\Gamma (x))^s\space\mathrm{dx}\right)}{\int_0^1 (\Gamma (x))^s\space\mathrm{dx}}\space=$$ and now differentiate under the integral sign $$\lim_{s\to 0} \frac{\displaystyle \int_0^1 \frac{d}{ds}(\Gamma (x))^s\space\mathrm{dx}}{\int_0^1 (\Gamma (x))^s\space\mathrm{dx}}\space=$$ $$\lim_{s\to 0} \frac{\displaystyle \int_0^1 (\Gamma (x))^s \ln (\Gamma(x))\space\mathrm{dx}}{\int_0^1 (\Gamma (x))^s\space\mathrm{dx}}\space=$$ $$\int_0^1 \ln (\Gamma(x))\space\mathrm{dx} \space \text{(Unjustified - I considered $\lim_{s\to 0} \int_0^1 (\Gamma (x))^s=1$ ) }$$ At this point I'm done since we know to compute $\int_0^1 \ln (\Gamma(x))\space\mathrm{dx}$. So, for the problematic part I managed to split $$\lim_{s\to 0} \int_0^1 (\Gamma (x))^s \mathrm{dx}$$ into  $$\lim_{s\to 0} \left(\int_0^{\epsilon} (\Gamma (x))^s \mathrm{dx}+\int_{\epsilon}^{1} (\Gamma (x))^s \mathrm{dx}\right)$$ and then I'm thinking to use the uniform convergence for the first integral to prove that it tends to $0$. Am I on the right way? What would you suggest me to do further? Would you approach the problem in a different manner? Thanks!","Compute  $$\lim_{s\to 0} \left(\int_0^1 (\Gamma (x))^s\space\mathrm{dx}\right)^{1/s}$$ This is a problem I thought of these days and I think I know a way although not completely justified. This is what I have Firstly take log $$\lim_{s\to 0} \frac{\ln\left(\int_0^1 (\Gamma (x))^s\space\mathrm{dx}\right)}{s}\space \text{(Unjustified part where considering the numerator tends to 0) }$$ and then apply  l'Hôpital's rule $$\lim_{s\to 0} \frac{\displaystyle \frac{d}{ds}\ln\left(\int_0^1 (\Gamma (x))^s\space\mathrm{dx}\right)}{\displaystyle \frac{d}{ds}s}\space=$$ $$\lim_{s\to 0} \frac{\displaystyle \frac{d}{ds} \left(\int_0^1 (\Gamma (x))^s\space\mathrm{dx}\right)}{\int_0^1 (\Gamma (x))^s\space\mathrm{dx}}\space=$$ and now differentiate under the integral sign $$\lim_{s\to 0} \frac{\displaystyle \int_0^1 \frac{d}{ds}(\Gamma (x))^s\space\mathrm{dx}}{\int_0^1 (\Gamma (x))^s\space\mathrm{dx}}\space=$$ $$\lim_{s\to 0} \frac{\displaystyle \int_0^1 (\Gamma (x))^s \ln (\Gamma(x))\space\mathrm{dx}}{\int_0^1 (\Gamma (x))^s\space\mathrm{dx}}\space=$$ $$\int_0^1 \ln (\Gamma(x))\space\mathrm{dx} \space \text{(Unjustified - I considered $\lim_{s\to 0} \int_0^1 (\Gamma (x))^s=1$ ) }$$ At this point I'm done since we know to compute $\int_0^1 \ln (\Gamma(x))\space\mathrm{dx}$. So, for the problematic part I managed to split $$\lim_{s\to 0} \int_0^1 (\Gamma (x))^s \mathrm{dx}$$ into  $$\lim_{s\to 0} \left(\int_0^{\epsilon} (\Gamma (x))^s \mathrm{dx}+\int_{\epsilon}^{1} (\Gamma (x))^s \mathrm{dx}\right)$$ and then I'm thinking to use the uniform convergence for the first integral to prove that it tends to $0$. Am I on the right way? What would you suggest me to do further? Would you approach the problem in a different manner? Thanks!",,"['calculus', 'real-analysis', 'limits', 'definite-integrals']"
35,A version of Hardy's inequality involving reciprocals.,A version of Hardy's inequality involving reciprocals.,,"How can one prove for any sequence of positive numbers $a_n, n\ge1,$ we have $$\sum_{n=1}^\infty \frac{n}{a_1+a_2+a_3+\cdots+a_n}\le 2\sum_{n=1}^\infty \frac{1}{a_n}$$ Added later: Apparently, this is a version of Hardy's inequality . The above is the case $p=-1$. (See the wiki for what $p$ is). The case $p=2$ appears here: Proving $A: l_2 \to l_2$ is a bounded operator","How can one prove for any sequence of positive numbers $a_n, n\ge1,$ we have $$\sum_{n=1}^\infty \frac{n}{a_1+a_2+a_3+\cdots+a_n}\le 2\sum_{n=1}^\infty \frac{1}{a_n}$$ Added later: Apparently, this is a version of Hardy's inequality . The above is the case $p=-1$. (See the wiki for what $p$ is). The case $p=2$ appears here: Proving $A: l_2 \to l_2$ is a bounded operator",,"['real-analysis', 'calculus', 'sequences-and-series', 'inequality', 'cauchy-schwarz-inequality']"
36,Applying analysis to solve a line-of-sight problem,Applying analysis to solve a line-of-sight problem,,"This was an optional h.w. problem: You are at the origin in $\mathbb{Z}\times\mathbb{Z}$. There are trees of a fixed finite radius at each point in $\mathbb{Z}\times\mathbb{Z}$ (other than the origin). How far can you see? This was asked in (I think), the context of the Stone-Weierstrass Theorem. So I have been thinking there must be some distance where the diameters of the trees visually converge and you get a polynomial approximation of a visual boundary. Thanks for your kindness to reply.","This was an optional h.w. problem: You are at the origin in $\mathbb{Z}\times\mathbb{Z}$. There are trees of a fixed finite radius at each point in $\mathbb{Z}\times\mathbb{Z}$ (other than the origin). How far can you see? This was asked in (I think), the context of the Stone-Weierstrass Theorem. So I have been thinking there must be some distance where the diameters of the trees visually converge and you get a polynomial approximation of a visual boundary. Thanks for your kindness to reply.",,[]
37,"An atypical integral with arctan, log, and radical","An atypical integral with arctan, log, and radical",,"The following integral appears in More (Almost) Impossible Integrals, Sums, and Series (2023) (evaluation details on pages $301$ - $306$ ), the sequel of (Almost) Impossible Integrals, Sums, and Series (2019) : $$\int_0^1 \arctan \left(\sqrt{\frac{1+x^2}{x(1-x) }}\right) \frac{\log (1+x) }{x \sqrt{1+x^2}}\textrm{d}x$$ $$=\frac{1}{2}\log ^2(\sqrt{2}-1)\pi-\frac{\pi^3}{8}-3\pi\operatorname{Li}_2(1-\sqrt{2}),\tag1$$ where $\operatorname{Li}_2$ represents the Dilogarithm function. Now, in the book, there are two preliminary steps needed before evaluating the preceding integral, which are as follows: $$\int_0^{\pi/2}\frac{\arctan(\sin(x))\log(1+\sin^2(x))}{\sin(x)} \textrm{d}x$$ $$=\frac{5}{24}\pi ^3-\frac{1}{2}\log ^2(\sqrt{2}-1)\pi+4\pi \operatorname{Li}_2(1-\sqrt{2}) \tag2$$ and $$\small  \int_0^{\pi/2}\frac{\arctan(\sin(x))\log(1+\sin^2(x))}{\sin(x)} \textrm{d}x+2 \int_0^1 \arctan \left(\sqrt{\frac{1+x^2}{x(1-x) }}\right) \frac{\log (1+x) }{x \sqrt{1+x^2}} \textrm{d}x$$ $$=\frac{1}{2}\log^2(\sqrt{2}-1)\pi-\frac{\pi^3}{24}-2\pi\operatorname{Li}_2(1-\sqrt{2}). \tag3$$ Question_1: How would we like to go differently in $(1)$ without involving the mentioned preliminary steps in $(2)$ and $(3)$ (maybe in a more direct way)? Question_2: I also find interesting the version with the squared arctan. Ideas for its evaluation? $$\int_0^1 \arctan^2 \left(\sqrt{\frac{1+x^2}{x(1-x) }}\right) \frac{\log (1+x) }{x \sqrt{1+x^2}}\textrm{d}x,$$ or more generally, $$\mathcal{I}=\int_0^1 \arctan^n \left(\sqrt{\frac{1+x^2}{x(1-x) }}\right) \frac{\log (1+x) }{x \sqrt{1+x^2}}\textrm{d}x, \ n\ge2,\ n \in \mathbb{N}.$$","The following integral appears in More (Almost) Impossible Integrals, Sums, and Series (2023) (evaluation details on pages - ), the sequel of (Almost) Impossible Integrals, Sums, and Series (2019) : where represents the Dilogarithm function. Now, in the book, there are two preliminary steps needed before evaluating the preceding integral, which are as follows: and Question_1: How would we like to go differently in without involving the mentioned preliminary steps in and (maybe in a more direct way)? Question_2: I also find interesting the version with the squared arctan. Ideas for its evaluation? or more generally,","301 306 \int_0^1 \arctan \left(\sqrt{\frac{1+x^2}{x(1-x) }}\right) \frac{\log (1+x) }{x \sqrt{1+x^2}}\textrm{d}x =\frac{1}{2}\log ^2(\sqrt{2}-1)\pi-\frac{\pi^3}{8}-3\pi\operatorname{Li}_2(1-\sqrt{2}),\tag1 \operatorname{Li}_2 \int_0^{\pi/2}\frac{\arctan(\sin(x))\log(1+\sin^2(x))}{\sin(x)} \textrm{d}x =\frac{5}{24}\pi ^3-\frac{1}{2}\log ^2(\sqrt{2}-1)\pi+4\pi \operatorname{Li}_2(1-\sqrt{2}) \tag2 \small  \int_0^{\pi/2}\frac{\arctan(\sin(x))\log(1+\sin^2(x))}{\sin(x)} \textrm{d}x+2 \int_0^1 \arctan \left(\sqrt{\frac{1+x^2}{x(1-x) }}\right) \frac{\log (1+x) }{x \sqrt{1+x^2}} \textrm{d}x =\frac{1}{2}\log^2(\sqrt{2}-1)\pi-\frac{\pi^3}{24}-2\pi\operatorname{Li}_2(1-\sqrt{2}). \tag3 (1) (2) (3) \int_0^1 \arctan^2 \left(\sqrt{\frac{1+x^2}{x(1-x) }}\right) \frac{\log (1+x) }{x \sqrt{1+x^2}}\textrm{d}x, \mathcal{I}=\int_0^1 \arctan^n \left(\sqrt{\frac{1+x^2}{x(1-x) }}\right) \frac{\log (1+x) }{x \sqrt{1+x^2}}\textrm{d}x, \ n\ge2,\ n \in \mathbb{N}.","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'definite-integrals']"
38,"Prove or disprove: If $f(x)$ is continuous in $(0,1]$ and $f(x)\to\infty$ as $x\to 0^+$, then $\lim_{n\to\infty}\sum_{k=1}^n f(k/n)$ does not exist.","Prove or disprove: If  is continuous in  and  as , then  does not exist.","f(x) (0,1] f(x)\to\infty x\to 0^+ \lim_{n\to\infty}\sum_{k=1}^n f(k/n)","I'm trying to prove or disprove the following conjecture: If $f(x)$ is continuous in $(0,1]$ and $f(x)\to\infty$ as $x\to 0^+$ then $L=\lim\limits_{n\to\infty}\sum\limits_{k=1}^n f\left(\frac{k}{n}\right)$ does not exist. My attempt I tried proof by contradiction. Asssume $L$ exists. $L=\lim\limits_{n\to\infty}n(\frac{1}{n})\sum\limits_{k=1}^n f\left(\frac{k} {n}\right)=\left(\lim\limits_{n\to\infty}n\right)\int_{0}^1 f(x)dx$ (EDIT: As mentioned by @FShrike in the comments, the previous step is not valid.) $\therefore \int_{0}^1 f(x)dx=0$ There are functions $f(x)$ , continuous in $(0,1]$ , such that $f(x)\to\infty$ as $x\to 0^+$ and $\int_{0}^1 f(x)dx=0$ . For example, $f(x)=-\ln{x}-1$ , in which case $L$ does not exist, by Stirling's approximation . But I do not see why all such functions $f(x)$ would imply that $L$ does not exist. Context: I am interested in geometrical infinite products ( example1 , example2 ). The conjecture in this question, via the substitution $f(x)=-\ln{g(x)}$ , is equivalent to: If $g(x)$ is continuous in $(0,1]$ and $\lim\limits_{x\to 0^+}g(x)=0$ then $\lim\limits_{n\to\infty}\prod\limits_{k=1}^ng\left(\frac{k}{n}\right)$ either equals $0$ or does not exist, which stands in interesting contrast with the fact that infinite products of lengths or areas, that tend to $0$ , can equal a positive number. EDIT2: I'm not sure if this is helpful, but I have noticed that $L_2=\lim\limits_{n\to\infty}\sum\limits_{k=1}^n f\left(\frac{k-1/2}{n}\right)$ can exist. For example, $\lim\limits_{n\to\infty}\sum\limits_{k=1}^n \left(-\ln{\left(\frac{k-1/2}{n}\right)}-1\right)=-\frac{\ln{2}}{2}$ . (Another question of mine yielded methods for dealing with the sum $\sum\limits_{k=1}^n \ln{(k-\frac12)}$ .) I do not understand why replacing $k$ with $k-\frac12$ seems to make the limit existable (if that's a word).","I'm trying to prove or disprove the following conjecture: If is continuous in and as then does not exist. My attempt I tried proof by contradiction. Asssume exists. (EDIT: As mentioned by @FShrike in the comments, the previous step is not valid.) There are functions , continuous in , such that as and . For example, , in which case does not exist, by Stirling's approximation . But I do not see why all such functions would imply that does not exist. Context: I am interested in geometrical infinite products ( example1 , example2 ). The conjecture in this question, via the substitution , is equivalent to: If is continuous in and then either equals or does not exist, which stands in interesting contrast with the fact that infinite products of lengths or areas, that tend to , can equal a positive number. EDIT2: I'm not sure if this is helpful, but I have noticed that can exist. For example, . (Another question of mine yielded methods for dealing with the sum .) I do not understand why replacing with seems to make the limit existable (if that's a word).","f(x) (0,1] f(x)\to\infty x\to 0^+ L=\lim\limits_{n\to\infty}\sum\limits_{k=1}^n f\left(\frac{k}{n}\right) L L=\lim\limits_{n\to\infty}n(\frac{1}{n})\sum\limits_{k=1}^n f\left(\frac{k}
{n}\right)=\left(\lim\limits_{n\to\infty}n\right)\int_{0}^1 f(x)dx \therefore \int_{0}^1 f(x)dx=0 f(x) (0,1] f(x)\to\infty x\to 0^+ \int_{0}^1 f(x)dx=0 f(x)=-\ln{x}-1 L f(x) L f(x)=-\ln{g(x)} g(x) (0,1] \lim\limits_{x\to 0^+}g(x)=0 \lim\limits_{n\to\infty}\prod\limits_{k=1}^ng\left(\frac{k}{n}\right) 0 0 L_2=\lim\limits_{n\to\infty}\sum\limits_{k=1}^n f\left(\frac{k-1/2}{n}\right) \lim\limits_{n\to\infty}\sum\limits_{k=1}^n \left(-\ln{\left(\frac{k-1/2}{n}\right)}-1\right)=-\frac{\ln{2}}{2} \sum\limits_{k=1}^n \ln{(k-\frac12)} k k-\frac12","['real-analysis', 'sequences-and-series', 'limits', 'infinite-product', 'conjectures']"
39,How to prove that $S=\sum_{n=0}^{\infty}\frac{(\sqrt{2}-1)^{2n+1}}{(2n+1)^2}=\frac{\pi^2}{16}-\frac{1}{4}\log^2(\sqrt{2}-1)?$,How to prove that,S=\sum_{n=0}^{\infty}\frac{(\sqrt{2}-1)^{2n+1}}{(2n+1)^2}=\frac{\pi^2}{16}-\frac{1}{4}\log^2(\sqrt{2}-1)?,How to prove that $$S=\displaystyle\sum_{n=0}^{\infty}\frac{(\sqrt{2}-1)^{2n+1}}{(2n+1)^2}=\frac{\pi^2}{16}-\frac{1}{4}\log^2(\sqrt{2}-1)$$ My attempt: We have for $|x|\leq1$ $$\tanh^{-1}(x)=\displaystyle\sum_{n=0}^{\infty}\frac{x^{2n+1}}{2n+1}$$ and : \begin{align*} \displaystyle\int_0^{\sqrt{2}-1}\frac{\tanh^{-1}(x)}{x}\ \mathrm{d}x&=\displaystyle\int_0^{\sqrt{2}-1}\frac{1}{x}\displaystyle\sum_{n=0}^{\infty}\frac{x^{2n+1}}{2n+1}\mathrm{d}x\\ &=\displaystyle\sum_{n=0}^{\infty}\frac{1}{2n+1}\displaystyle\int_0^{\sqrt{2}-1}x^{2n}\mathrm{d}x\\ &=\displaystyle\sum_{n=0}^{\infty}\frac{(\sqrt{2}-1)^{2n+1}}{(2n+1)^2}\\ &=S\\ \end{align*} So : \begin{align*} S&=\displaystyle\sum_{n=0}^{\infty}\frac{(\sqrt{2}-1)^{2n+1}}{(2n+1)^2}\\ &=\displaystyle\int_0^{\sqrt{2}-1}\frac{\tanh^{-1}(x)}{x}\mathrm{d}x\\ &=\displaystyle\int_0^{\sqrt{2}-1}\frac{1}{2x}\left(\log(1+x)-\log(1-x)\right)\mathrm{d}x\\ &=\frac{1}{2}(J_1-J_2) \end{align*} Where: \begin{align*} J_1&=\displaystyle\int_0^{\sqrt{2}-1}\frac{\log(1+x)}{x}\mathrm{d}x\\ &=\displaystyle\sum_{n=0}^{\infty}\frac{(-1)^n}{n+1}\displaystyle\int_0^{\sqrt{2}-1}x^n\mathrm{d}x\\ &=\displaystyle\sum_{n=0}^{\infty}\frac{(-1)^n(\sqrt{2}-1)^{n+1}}{(n+1)^2}\\ \end{align*} And: \begin{align*} J_2&=\displaystyle\int_0^{\sqrt{2}-1}\frac{\log(1-x)}{x}\mathrm{d}x\\ &=\displaystyle\int_0^{\sqrt{2}-1}-\displaystyle\sum_{n=0}^{\infty}\frac{x^n}{n+1}dx\\ &=-\displaystyle\sum_{n=0}^{\infty}\frac{1}{n+1}\left[\frac{x^{n+1}}{n+1}\right]_0^{\sqrt{2}-1}\\ &=-\displaystyle\sum_{n=0}^{\infty}\frac{(\sqrt{2}-1)^{n+1}}{(n+1)^2}\\ \end{align*} Finally we find : $$S=\frac{1}{2}\left(\displaystyle\sum_{n=0}^{\infty}\frac{(\sqrt{2}-1)^{n+1}((-1)^n+1)}{(n+1)^2}\right)$$ But I could not find a way to calculate $S$ . Any help please? and thank's in advance.,How to prove that My attempt: We have for and : So : Where: And: Finally we find : But I could not find a way to calculate . Any help please? and thank's in advance.,"S=\displaystyle\sum_{n=0}^{\infty}\frac{(\sqrt{2}-1)^{2n+1}}{(2n+1)^2}=\frac{\pi^2}{16}-\frac{1}{4}\log^2(\sqrt{2}-1) |x|\leq1 \tanh^{-1}(x)=\displaystyle\sum_{n=0}^{\infty}\frac{x^{2n+1}}{2n+1} \begin{align*}
\displaystyle\int_0^{\sqrt{2}-1}\frac{\tanh^{-1}(x)}{x}\ \mathrm{d}x&=\displaystyle\int_0^{\sqrt{2}-1}\frac{1}{x}\displaystyle\sum_{n=0}^{\infty}\frac{x^{2n+1}}{2n+1}\mathrm{d}x\\
&=\displaystyle\sum_{n=0}^{\infty}\frac{1}{2n+1}\displaystyle\int_0^{\sqrt{2}-1}x^{2n}\mathrm{d}x\\
&=\displaystyle\sum_{n=0}^{\infty}\frac{(\sqrt{2}-1)^{2n+1}}{(2n+1)^2}\\
&=S\\
\end{align*} \begin{align*}
S&=\displaystyle\sum_{n=0}^{\infty}\frac{(\sqrt{2}-1)^{2n+1}}{(2n+1)^2}\\
&=\displaystyle\int_0^{\sqrt{2}-1}\frac{\tanh^{-1}(x)}{x}\mathrm{d}x\\
&=\displaystyle\int_0^{\sqrt{2}-1}\frac{1}{2x}\left(\log(1+x)-\log(1-x)\right)\mathrm{d}x\\
&=\frac{1}{2}(J_1-J_2)
\end{align*} \begin{align*}
J_1&=\displaystyle\int_0^{\sqrt{2}-1}\frac{\log(1+x)}{x}\mathrm{d}x\\
&=\displaystyle\sum_{n=0}^{\infty}\frac{(-1)^n}{n+1}\displaystyle\int_0^{\sqrt{2}-1}x^n\mathrm{d}x\\
&=\displaystyle\sum_{n=0}^{\infty}\frac{(-1)^n(\sqrt{2}-1)^{n+1}}{(n+1)^2}\\
\end{align*} \begin{align*}
J_2&=\displaystyle\int_0^{\sqrt{2}-1}\frac{\log(1-x)}{x}\mathrm{d}x\\
&=\displaystyle\int_0^{\sqrt{2}-1}-\displaystyle\sum_{n=0}^{\infty}\frac{x^n}{n+1}dx\\
&=-\displaystyle\sum_{n=0}^{\infty}\frac{1}{n+1}\left[\frac{x^{n+1}}{n+1}\right]_0^{\sqrt{2}-1}\\
&=-\displaystyle\sum_{n=0}^{\infty}\frac{(\sqrt{2}-1)^{n+1}}{(n+1)^2}\\
\end{align*} S=\frac{1}{2}\left(\displaystyle\sum_{n=0}^{\infty}\frac{(\sqrt{2}-1)^{n+1}((-1)^n+1)}{(n+1)^2}\right) S","['real-analysis', 'calculus', 'integration', 'sequences-and-series']"
40,A guess related to Lebesgue differentiation theorem,A guess related to Lebesgue differentiation theorem,,"When I read Lebesgue differentiation theorem, I suddenly have the following conjecture, which I can't prove or find a counterexample. Let $f\in L_{\mathrm{loc}}^1(\mathbb{R}^n)$. If    $$ \int_{B_r(x)} f(y)dy=0 $$   holds for any $r\geq 1$ and $x\in \mathbb{R}^n$ , then can we say that $f(x)=0$ a.e. ? Please be careful that $r$ is larger than 1 , which prevents us from taking advantage of  Lebesgue differentiation theorem. When $n=1$, this seems to be true.","When I read Lebesgue differentiation theorem, I suddenly have the following conjecture, which I can't prove or find a counterexample. Let $f\in L_{\mathrm{loc}}^1(\mathbb{R}^n)$. If    $$ \int_{B_r(x)} f(y)dy=0 $$   holds for any $r\geq 1$ and $x\in \mathbb{R}^n$ , then can we say that $f(x)=0$ a.e. ? Please be careful that $r$ is larger than 1 , which prevents us from taking advantage of  Lebesgue differentiation theorem. When $n=1$, this seems to be true.",,"['real-analysis', 'measure-theory', 'harmonic-analysis']"
41,"Average number of ways to express number as a sum of $n$ squares goes to zero, why?","Average number of ways to express number as a sum of  squares goes to zero, why?",n,"So I recently stumbled upon this beautiful problem: Show that the number $r_2(n)$ of representations of $n$ as a sum of two squares (of not neccesarily positive integers) has $\pi$ as its arithmetic mean, that is   $$ \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{i=0}^n r_2(i) = \pi$$ Now the proof remarks that every point of the unit two dimensional integer lattice $(x, y)$ is the solution to the equation $x^2+y^2=n$ for some $n$, and then if we look at all the lattice point satisfyting $x^2+y^2 \leq n$ we can bound this quantity by the area of a circle (with radius about $\sqrt{n}$), and thus gain the result. (This is a rough idea of the proof, it is by no means precise) Now we can extend this to the function $r_k(n)$ that counts the number of ways to express $n$ as the sum of $k$ squares. Now we simply take the $k$ dimensional lattice and bound it by $k$-spheres, and gain the average as the $k$-volume of the unit $k$-sphere. This quantity is given by: $$\frac{\pi^{\frac{k}{2}}}{\Gamma(1+\frac{k}{2})}$$ where $\Gamma(n)$ is the gamma function. Now what is interesting about this is that the average tends to zero as the dimension grows to infinity. On the other hand, it is well known that any natural number can be expressed as the sum of four squares. Now since our definition of $r_k(n)$ counts all the ways to gain $n$, including those that allow $0^2$ as one of the summands (I assume this is true since in our lattice-counting proof we include all latice points, including those that have one or more coordinates zero), then it would be the case that solutions obtained for smaller dimension be valid in higher dimension as well (simply by setting on of the coordinates to zero). So we would expect that every number can be expressed in atleast one way for all $k\geq4$, meaning $r_k(n)\geq1$ for all $n$. However, this would mean that the average should be atleast one for all higher dimensions, but this contradicts that the volume goes to zero. Why is this? Is there some intuitive explanation? I have a sense that the infinite nature of the average is probably in some way misleading and my usual intuition that can be applied to finite averages.  I have tried thinking of the limit also as an asymptotic bound of $\sum r_k(n)$ in terms of $c\cdot n$, but that doesnt help either. There is also a possibility that there is some flaw in my proof (well, idea of proof as it is presented here). Maybe it is not the volume that I should be looking at?","So I recently stumbled upon this beautiful problem: Show that the number $r_2(n)$ of representations of $n$ as a sum of two squares (of not neccesarily positive integers) has $\pi$ as its arithmetic mean, that is   $$ \lim_{n\rightarrow \infty }\frac{1}{n}\sum_{i=0}^n r_2(i) = \pi$$ Now the proof remarks that every point of the unit two dimensional integer lattice $(x, y)$ is the solution to the equation $x^2+y^2=n$ for some $n$, and then if we look at all the lattice point satisfyting $x^2+y^2 \leq n$ we can bound this quantity by the area of a circle (with radius about $\sqrt{n}$), and thus gain the result. (This is a rough idea of the proof, it is by no means precise) Now we can extend this to the function $r_k(n)$ that counts the number of ways to express $n$ as the sum of $k$ squares. Now we simply take the $k$ dimensional lattice and bound it by $k$-spheres, and gain the average as the $k$-volume of the unit $k$-sphere. This quantity is given by: $$\frac{\pi^{\frac{k}{2}}}{\Gamma(1+\frac{k}{2})}$$ where $\Gamma(n)$ is the gamma function. Now what is interesting about this is that the average tends to zero as the dimension grows to infinity. On the other hand, it is well known that any natural number can be expressed as the sum of four squares. Now since our definition of $r_k(n)$ counts all the ways to gain $n$, including those that allow $0^2$ as one of the summands (I assume this is true since in our lattice-counting proof we include all latice points, including those that have one or more coordinates zero), then it would be the case that solutions obtained for smaller dimension be valid in higher dimension as well (simply by setting on of the coordinates to zero). So we would expect that every number can be expressed in atleast one way for all $k\geq4$, meaning $r_k(n)\geq1$ for all $n$. However, this would mean that the average should be atleast one for all higher dimensions, but this contradicts that the volume goes to zero. Why is this? Is there some intuitive explanation? I have a sense that the infinite nature of the average is probably in some way misleading and my usual intuition that can be applied to finite averages.  I have tried thinking of the limit also as an asymptotic bound of $\sum r_k(n)$ in terms of $c\cdot n$, but that doesnt help either. There is also a possibility that there is some flaw in my proof (well, idea of proof as it is presented here). Maybe it is not the volume that I should be looking at?",,"['real-analysis', 'geometry', 'number-theory', 'asymptotics', 'average']"
42,The two definitions of a compact set,The two definitions of a compact set,,"In general, $A$ is compact if every open cover of $A$ contains a finite subcover of $A$. In $R$, $A$ is compact if it is closed and bounded. The second is very easy to understand because I can easily come up with an example like $[0,1]$ which is both closed and bounded so it's compact. However, I am very confused at definition (1) because I don't really understand what is meant by a cover and I don't understand how this is really related to a set being closed and bounded? Could someone please explain what is the relationship between (1) and (2)? Thank you.","In general, $A$ is compact if every open cover of $A$ contains a finite subcover of $A$. In $R$, $A$ is compact if it is closed and bounded. The second is very easy to understand because I can easily come up with an example like $[0,1]$ which is both closed and bounded so it's compact. However, I am very confused at definition (1) because I don't really understand what is meant by a cover and I don't understand how this is really related to a set being closed and bounded? Could someone please explain what is the relationship between (1) and (2)? Thank you.",,"['real-analysis', 'general-topology', 'compactness']"
43,Proving $f$ is Lebesgue integrable iff $|f|$ is Lebesgue integrable.,Proving  is Lebesgue integrable iff  is Lebesgue integrable.,f |f|,"How can we prove the statement?  $f$ is Lebesgue integrable if and only if $|f|$ is Lebesgue integrable.  First, suppose $f$ is Lebesgue integrable, by definition, $f$ is measurable and nonnegative, so $f=|f|$ and $\displaystyle \int_E f \, d \mu=\int_E|f| \, d\mu$ implies that $|f|$ is Lebesgue integrable. Second, suppose $|f|$ is Lebesgue integrable, if $f$ is nonnegative, then $f=|f|$, so $f$ is Lebesgue integrable; if $f$ is negative, then how can I prove?","How can we prove the statement?  $f$ is Lebesgue integrable if and only if $|f|$ is Lebesgue integrable.  First, suppose $f$ is Lebesgue integrable, by definition, $f$ is measurable and nonnegative, so $f=|f|$ and $\displaystyle \int_E f \, d \mu=\int_E|f| \, d\mu$ implies that $|f|$ is Lebesgue integrable. Second, suppose $|f|$ is Lebesgue integrable, if $f$ is nonnegative, then $f=|f|$, so $f$ is Lebesgue integrable; if $f$ is negative, then how can I prove?",,"['real-analysis', 'measure-theory']"
44,Lindelöf's Covering Theorem,Lindelöf's Covering Theorem,,"If $A \in \mathbb R^n$ and $F$ be an open covering of $A$. Then there is a countable subcollection of $F$ which also covers $A$. Proof: Let $G=\{A_1,A_2, \cdots\}$ denote the countable collection of all $n-$balls having rational centers and rational radii. this set $G$ will be used to help us extract a countable subcollection of $F$ which covers $A$. Assume $x \in A$. Then there is an open set $S \in F$ such that $x \in S \implies $ there's an $n-$ ball $A_k$ in $G$ such that $x \in A_k \subseteq S$. There are infinitely many $A_k$ but we choose the one with the minimum index , say $m = m(x)$. Then, we have $ x\in A_{m(x)} \subseteq S$. The set of all $n-$ balls $A_{m(x)}$ obtained as $x$ varies over all elements of $A$ is a countable collection of open sets which covers $A$. To get a countable subcollection of open sets which cover $A$, we simply correlate to each set $A_{k(x)}$ one of the sets $S$ of $F$ which contained $A_{k(x)}.$ This completes the proof. $(a)$ I have trouble understanding the last lines of the proof shown in the blue box. What does it mean actually? $(b)$There can be many $A_{k(x)}$ in  $S$. where $S$ is an open set in the collection $F$. In that case, will we define $m = \{\min m(x) : x \in S\}$ and assign $m$ with $S$?However, I don't think this is possible because there can be infinite number of $m(x)$ in $S$ $(c)$ What happens when $x$ belongs to more than one open sets, say $ x \in S,T,U,..$. How are we going to define countability of the open sets in that situation? Thank you for your help.","If $A \in \mathbb R^n$ and $F$ be an open covering of $A$. Then there is a countable subcollection of $F$ which also covers $A$. Proof: Let $G=\{A_1,A_2, \cdots\}$ denote the countable collection of all $n-$balls having rational centers and rational radii. this set $G$ will be used to help us extract a countable subcollection of $F$ which covers $A$. Assume $x \in A$. Then there is an open set $S \in F$ such that $x \in S \implies $ there's an $n-$ ball $A_k$ in $G$ such that $x \in A_k \subseteq S$. There are infinitely many $A_k$ but we choose the one with the minimum index , say $m = m(x)$. Then, we have $ x\in A_{m(x)} \subseteq S$. The set of all $n-$ balls $A_{m(x)}$ obtained as $x$ varies over all elements of $A$ is a countable collection of open sets which covers $A$. To get a countable subcollection of open sets which cover $A$, we simply correlate to each set $A_{k(x)}$ one of the sets $S$ of $F$ which contained $A_{k(x)}.$ This completes the proof. $(a)$ I have trouble understanding the last lines of the proof shown in the blue box. What does it mean actually? $(b)$There can be many $A_{k(x)}$ in  $S$. where $S$ is an open set in the collection $F$. In that case, will we define $m = \{\min m(x) : x \in S\}$ and assign $m$ with $S$?However, I don't think this is possible because there can be infinite number of $m(x)$ in $S$ $(c)$ What happens when $x$ belongs to more than one open sets, say $ x \in S,T,U,..$. How are we going to define countability of the open sets in that situation? Thank you for your help.",,"['real-analysis', 'general-topology', 'elementary-set-theory']"
45,A set is closed if and only if it contains all its limit points.,A set is closed if and only if it contains all its limit points.,,"A set is closed if and only if it contains all its limit points. Proof in book: Suppose $S$ is ""not closed"". We must show that $S$ does not contain all its limit points. Since $S$ is ""not closed"", $S^c$ is ""not open"". Therefore there is at least one element $x$ of $S^c$ such that every ball $B(x,\epsilon)$ contains at least one element of $S$ ... Why is there at least one element $x\in S^c$ such that every open ball contains at least one element of the open set $S$?","A set is closed if and only if it contains all its limit points. Proof in book: Suppose $S$ is ""not closed"". We must show that $S$ does not contain all its limit points. Since $S$ is ""not closed"", $S^c$ is ""not open"". Therefore there is at least one element $x$ of $S^c$ such that every ball $B(x,\epsilon)$ contains at least one element of $S$ ... Why is there at least one element $x\in S^c$ such that every open ball contains at least one element of the open set $S$?",,['real-analysis']
46,$\epsilon$-$\delta$ proof that $\lim_{x \to 1} \sqrt{x} = 1$,- proof that,\epsilon \delta \lim_{x \to 1} \sqrt{x} = 1,"I'm trying to teach myself how to do $\epsilon$-$\delta$ proofs and would like to know if I solved this proof correctly. The answer given (Spivak, but in the solutions book) was very different. Exercise: Prove $\lim_{x \to 1} \sqrt{x} = 1$ using $\epsilon$-$\delta$. My Proof: We have that $0 < |x-1| < \delta $. Also, $|x - 1| = \bigl|(\sqrt{x}-1)(\sqrt{x}+1)\bigr| = |\sqrt{x}-1||\sqrt{x}+1| < \delta$. $\therefore |\sqrt{x}-1|< \frac{\delta}{|\sqrt{x}+1|}$ Now we let $\delta = 1$. Then \begin{array}{l} -1<x-1<1 \\ \therefore  0 < x < 2 \\ \therefore  1 < \sqrt{x} + 1<\sqrt{2} + 1 \\ \therefore  \frac{1}{\sqrt{x} + 1}<1. \end{array} We had that $$|\sqrt{x}-1|< \frac{\delta}{|\sqrt{x}+1|} \therefore |\sqrt{x}-1|<\delta$$ By letting $\delta=\min(1, \epsilon)$, we get that $|\sqrt{x}-1|<\epsilon$ if $0 < |x-1| < \delta $. Thus, $\lim_{x \to 1} \sqrt{x} = 1$. Is my proof correct? Is there a better way to do it (still using $\epsilon-\delta$)?","I'm trying to teach myself how to do $\epsilon$-$\delta$ proofs and would like to know if I solved this proof correctly. The answer given (Spivak, but in the solutions book) was very different. Exercise: Prove $\lim_{x \to 1} \sqrt{x} = 1$ using $\epsilon$-$\delta$. My Proof: We have that $0 < |x-1| < \delta $. Also, $|x - 1| = \bigl|(\sqrt{x}-1)(\sqrt{x}+1)\bigr| = |\sqrt{x}-1||\sqrt{x}+1| < \delta$. $\therefore |\sqrt{x}-1|< \frac{\delta}{|\sqrt{x}+1|}$ Now we let $\delta = 1$. Then \begin{array}{l} -1<x-1<1 \\ \therefore  0 < x < 2 \\ \therefore  1 < \sqrt{x} + 1<\sqrt{2} + 1 \\ \therefore  \frac{1}{\sqrt{x} + 1}<1. \end{array} We had that $$|\sqrt{x}-1|< \frac{\delta}{|\sqrt{x}+1|} \therefore |\sqrt{x}-1|<\delta$$ By letting $\delta=\min(1, \epsilon)$, we get that $|\sqrt{x}-1|<\epsilon$ if $0 < |x-1| < \delta $. Thus, $\lim_{x \to 1} \sqrt{x} = 1$. Is my proof correct? Is there a better way to do it (still using $\epsilon-\delta$)?",,"['calculus', 'real-analysis', 'limits', 'radicals', 'epsilon-delta']"
47,Lim Sup/Inf for real valued functions,Lim Sup/Inf for real valued functions,,"To understand the notion of, say, limit superior for a sequence, is not difficult. Simply consider the set of all upper buonds for the set of all limit points of the sequence, and then simply pick the inf of this set. I said simply, because for a sequence, we only take limits to infinity. Now, for a function we can calculate limits to any point. So, what exactly means $$\limsup_{x\to c} f(x) $$ Wikipedia is unclear about this, and I found nothing in my (tiny) literature. Thanks in advance.","To understand the notion of, say, limit superior for a sequence, is not difficult. Simply consider the set of all upper buonds for the set of all limit points of the sequence, and then simply pick the inf of this set. I said simply, because for a sequence, we only take limits to infinity. Now, for a function we can calculate limits to any point. So, what exactly means $$\limsup_{x\to c} f(x) $$ Wikipedia is unclear about this, and I found nothing in my (tiny) literature. Thanks in advance.",,"['real-analysis', 'sequences-and-series', 'analysis', 'limits', 'functions']"
48,Equivalent definition of absolutely continuous,Equivalent definition of absolutely continuous,,"A function $f$ is absolutely continuous on $[a,b]$ is defined by: for each $\varepsilon>0$, there is a $\delta>0$, for each finite disjoint open interval $\{(c_k,d_k)\}_{k=1}^n$ contained in $[a,b]$, we have $$ \text{if}\,\, \sum_{k=1}^n (d_k-c_k)<\delta, \,\,\text{then}\,\, \sum_{k=1}^n\left|f(d_k)-f(c_k)\right|<\varepsilon. $$ However, in the book I'm reading, it is said that there is a equivalent definition, say $$ \text{if}\,\, \sum_{k=1}^n (d_k-c_k)<\delta,\,\, \text{then}\,\, \left|\sum_{k=1}^n f(d_k)-f(c_k)\right|<\varepsilon. $$ However, I can not prove it. How?","A function $f$ is absolutely continuous on $[a,b]$ is defined by: for each $\varepsilon>0$, there is a $\delta>0$, for each finite disjoint open interval $\{(c_k,d_k)\}_{k=1}^n$ contained in $[a,b]$, we have $$ \text{if}\,\, \sum_{k=1}^n (d_k-c_k)<\delta, \,\,\text{then}\,\, \sum_{k=1}^n\left|f(d_k)-f(c_k)\right|<\varepsilon. $$ However, in the book I'm reading, it is said that there is a equivalent definition, say $$ \text{if}\,\, \sum_{k=1}^n (d_k-c_k)<\delta,\,\, \text{then}\,\, \left|\sum_{k=1}^n f(d_k)-f(c_k)\right|<\varepsilon. $$ However, I can not prove it. How?",,['real-analysis']
49,"Does $\lim \frac{xy}{x+y}$ exist at $(0,0)$?",Does  exist at ?,"\lim \frac{xy}{x+y} (0,0)","Given the function $f(x,y) = \frac{xy}{x+y}$, after my analysis I concluded that the limit at $(0,0)$ does not exists. In short, if we approach to $(0,0)$ through the parabola $y = -x^2 -x$ and $y = x^2 - x$ we find that $f(x,y)$ approaches to $1$ and $-1$ respectively. Therefore the limit does not exists. I think my rationale is right. What do you think? Alternatively, is there another approach for this problem?","Given the function $f(x,y) = \frac{xy}{x+y}$, after my analysis I concluded that the limit at $(0,0)$ does not exists. In short, if we approach to $(0,0)$ through the parabola $y = -x^2 -x$ and $y = x^2 - x$ we find that $f(x,y)$ approaches to $1$ and $-1$ respectively. Therefore the limit does not exists. I think my rationale is right. What do you think? Alternatively, is there another approach for this problem?",,"['real-analysis', 'multivariable-calculus']"
50,Has the Riemann Rearrangement Theorem ever helped in computation rather than just being a warning?,Has the Riemann Rearrangement Theorem ever helped in computation rather than just being a warning?,,"A decent course in elementary analysis will eventually discuss series, absolute convergence, conditional convergence, and the Riemann Rearrangement Theorem. However, in any presentation I've seen, in person or within a text, the discussion sort of ends after Riemann's theorem is given---quite content in proving to the student(s) or the reader that one shouldn't extend finite intuition to infinite settings without providing a proof. I agree that this is an important moral to impart; however, I'm interested in something else: Has the Riemann Rearrangement theorem been used as a computational aid to explicitly calculate a sum? By this vague question, I specifically have in mind that piece of Riemann's Theorem that states an absolutely convergent series is commutatively convergent. So, to be slightly more narrow in scope: Has there been a series $\sum a_k$ which is fairly easy to show absolutely converges; however, the sum itself was computed by a clever choice of bijection $\sigma:\mathbb{N}\rightarrow\mathbb{N}$ and by working with the partial sums of $\sum a_{\sigma(k)}$ ? This is a rather vague question, and I don't expect it to have much of an absolute answer. But I'm interested in any variety of answers, and I'm sure they'd be demonstrative and helpful to future readers.","A decent course in elementary analysis will eventually discuss series, absolute convergence, conditional convergence, and the Riemann Rearrangement Theorem. However, in any presentation I've seen, in person or within a text, the discussion sort of ends after Riemann's theorem is given---quite content in proving to the student(s) or the reader that one shouldn't extend finite intuition to infinite settings without providing a proof. I agree that this is an important moral to impart; however, I'm interested in something else: Has the Riemann Rearrangement theorem been used as a computational aid to explicitly calculate a sum? By this vague question, I specifically have in mind that piece of Riemann's Theorem that states an absolutely convergent series is commutatively convergent. So, to be slightly more narrow in scope: Has there been a series which is fairly easy to show absolutely converges; however, the sum itself was computed by a clever choice of bijection and by working with the partial sums of ? This is a rather vague question, and I don't expect it to have much of an absolute answer. But I'm interested in any variety of answers, and I'm sure they'd be demonstrative and helpful to future readers.",\sum a_k \sigma:\mathbb{N}\rightarrow\mathbb{N} \sum a_{\sigma(k)},"['real-analysis', 'sequences-and-series']"
51,Differentiation under the integral sign and uniform integrability,Differentiation under the integral sign and uniform integrability,,"Let $(X,\mu)$ be a measure space (if it's convenient we can assume $\mu$ is finite).  Let $[a,b]$ be an interval, and suppose we have a function $f : [a,b] \times X \to \mathbb{R}$ such that: For each $x \in X$, we have $f(\cdot, x) \in C^1([a,b])$; For each $t \in [a,b]$, we have $f(t, \cdot), \partial_t f(t, \cdot) \in L^1(\mu)$. (If it helps I'm happy to also assume that $f$ and $\partial_t f$ are  jointly measurable.) Set $F(t) = \int_X f(t,x)\, \mu(dx)$.  The classical ""differentiation under the integral sign"" theorem says that if we assume the hypothesis $$\text{There exists $g \in L^1(\mu)$ such that $\sup_{t \in [a,b]} |\partial_t f(t,x)| \le g(x)$} \tag{DOM}$$ then $F$ is differentiable on $(a,b)$ and $F'(t) = \int_X \partial_t f(t,x) \,\mu(dx)$.  Indeed, it would follow that $F \in C^1([a,b])$. Now, in many situations, one can weaken a ""domination"" hypothesis to uniform integrability.  (For instance, the dominated convergence theorem is extended by the Vitali convergence theorem .) So suppose we replace the hypothesis (DOM) with the following: $$\text{$\{\partial_t f(t,\cdot) : t \in [a,b]\}$ is uniformly integrable with respect to $\mu$} \tag{UI}$$ Does the same conclusion hold? I'd think this would be standard if it's true, but I've never seen it written down.  But I also can't think of a counterexample. I would like to follow the proof of the classical result by proceeding as follows: Fix $t_0 \in (a,b)$ and an arbitrary sequence $t_n \to t_0$ with all $t_n \in (a,b)$.  We have $$F'(t_0) = \lim_{n \to \infty} \int_X \frac{f(t_n, x)-f(t_0, x)}{t_n - t_0}\,\mu(dx)$$ if the limit exists, and we would like to pass the limit under the integral sign.  This would be possible if the sequence of difference quotients $D_n(x) = \frac{f(t_n, x)-f(t_0, x)}{t_n - t_0}$ were uniformly integrable.  By the mean value theorem, we know that for each $x$ there is $t_n^*(x) \in (a,b)$ such that $D_n(x) = \partial_t f(t_n^*(x), x)$.  If we could choose $t_n$ independently of $x$, then $\{D_n\}$ would be dominated by a UI sequence and we would be done.  But of course that will not work in general. Note: there are a couple of inequivalent definitions of uniformly integrable .  I would be happy to adopt the stronger one, in which we assume that $\{\partial_t f(t,\cdot) : t \in [a,b]\}$ is bounded in $L^1$ norm. (In the specific case that I care about, $\mu$ is a probability measure and I can show $\sup_{t \in [a,b]} \|\partial_t f(t, \cdot)\|_{L^p(\mu)} < \infty$ for some $p>1$, which implies (UI) by the so-called ""crystal ball condition"".)","Let $(X,\mu)$ be a measure space (if it's convenient we can assume $\mu$ is finite).  Let $[a,b]$ be an interval, and suppose we have a function $f : [a,b] \times X \to \mathbb{R}$ such that: For each $x \in X$, we have $f(\cdot, x) \in C^1([a,b])$; For each $t \in [a,b]$, we have $f(t, \cdot), \partial_t f(t, \cdot) \in L^1(\mu)$. (If it helps I'm happy to also assume that $f$ and $\partial_t f$ are  jointly measurable.) Set $F(t) = \int_X f(t,x)\, \mu(dx)$.  The classical ""differentiation under the integral sign"" theorem says that if we assume the hypothesis $$\text{There exists $g \in L^1(\mu)$ such that $\sup_{t \in [a,b]} |\partial_t f(t,x)| \le g(x)$} \tag{DOM}$$ then $F$ is differentiable on $(a,b)$ and $F'(t) = \int_X \partial_t f(t,x) \,\mu(dx)$.  Indeed, it would follow that $F \in C^1([a,b])$. Now, in many situations, one can weaken a ""domination"" hypothesis to uniform integrability.  (For instance, the dominated convergence theorem is extended by the Vitali convergence theorem .) So suppose we replace the hypothesis (DOM) with the following: $$\text{$\{\partial_t f(t,\cdot) : t \in [a,b]\}$ is uniformly integrable with respect to $\mu$} \tag{UI}$$ Does the same conclusion hold? I'd think this would be standard if it's true, but I've never seen it written down.  But I also can't think of a counterexample. I would like to follow the proof of the classical result by proceeding as follows: Fix $t_0 \in (a,b)$ and an arbitrary sequence $t_n \to t_0$ with all $t_n \in (a,b)$.  We have $$F'(t_0) = \lim_{n \to \infty} \int_X \frac{f(t_n, x)-f(t_0, x)}{t_n - t_0}\,\mu(dx)$$ if the limit exists, and we would like to pass the limit under the integral sign.  This would be possible if the sequence of difference quotients $D_n(x) = \frac{f(t_n, x)-f(t_0, x)}{t_n - t_0}$ were uniformly integrable.  By the mean value theorem, we know that for each $x$ there is $t_n^*(x) \in (a,b)$ such that $D_n(x) = \partial_t f(t_n^*(x), x)$.  If we could choose $t_n$ independently of $x$, then $\{D_n\}$ would be dominated by a UI sequence and we would be done.  But of course that will not work in general. Note: there are a couple of inequivalent definitions of uniformly integrable .  I would be happy to adopt the stronger one, in which we assume that $\{\partial_t f(t,\cdot) : t \in [a,b]\}$ is bounded in $L^1$ norm. (In the specific case that I care about, $\mu$ is a probability measure and I can show $\sup_{t \in [a,b]} \|\partial_t f(t, \cdot)\|_{L^p(\mu)} < \infty$ for some $p>1$, which implies (UI) by the so-called ""crystal ball condition"".)",,"['real-analysis', 'measure-theory', 'derivatives', 'lebesgue-integral', 'uniform-integrability']"
52,Prove $\int_{0}^{x}f+\int_{0}^{f(x)}f^{-1}=xf(x)\qquad\text{for all $x\geq0$}$ [duplicate],Prove x\geq0 [duplicate],\int_{0}^{x}f+\int_{0}^{f(x)}f^{-1}=xf(x)\qquad\text{for all  },"This question already has answers here : The sum of integrals of a function and its inverse: $\int_{0}^{a}f+\int_{f(0)}^{f(a)}f^{-1}=af(a)$ (5 answers) Closed 8 years ago . Suppose that the function $f:[0,\infty)\rightarrow\mathbb{R}$ is continuous and strictly increasing and that $f:(0,\infty)\rightarrow\mathbb{R}$ is differentiable. Moreover, assume $f(0)=0$. Consider the formula $$\int_{0}^{x}f+\int_{0}^{f(x)}f^{-1}=xf(x)\qquad\text{for all $x\geq0$}$$ Prove this formula. Attempt: Consider that $g(x)=\int_{0}^{x}f+\int_{0}^{f(x)}f^{-1}-xf(x)$ which is continuous on $\mathbb{R}$. Then differentiate $g(x)$, we have $g'(x)=f(x)+xf'(x)-f(x)-xf'(x)=0$. Thus for all $x$, $$g(x)-g(0)=0\Longrightarrow\int_{0}^{x}f+\int_{0}^{f(x)}f^{-1}-xf(x)=0$$ I am not sure my the equality valid not or not. If not, can someone give me a suggestion to modify the proof. Thanks.","This question already has answers here : The sum of integrals of a function and its inverse: $\int_{0}^{a}f+\int_{f(0)}^{f(a)}f^{-1}=af(a)$ (5 answers) Closed 8 years ago . Suppose that the function $f:[0,\infty)\rightarrow\mathbb{R}$ is continuous and strictly increasing and that $f:(0,\infty)\rightarrow\mathbb{R}$ is differentiable. Moreover, assume $f(0)=0$. Consider the formula $$\int_{0}^{x}f+\int_{0}^{f(x)}f^{-1}=xf(x)\qquad\text{for all $x\geq0$}$$ Prove this formula. Attempt: Consider that $g(x)=\int_{0}^{x}f+\int_{0}^{f(x)}f^{-1}-xf(x)$ which is continuous on $\mathbb{R}$. Then differentiate $g(x)$, we have $g'(x)=f(x)+xf'(x)-f(x)-xf'(x)=0$. Thus for all $x$, $$g(x)-g(0)=0\Longrightarrow\int_{0}^{x}f+\int_{0}^{f(x)}f^{-1}-xf(x)=0$$ I am not sure my the equality valid not or not. If not, can someone give me a suggestion to modify the proof. Thanks.",,"['real-analysis', 'proof-verification']"
53,$L^2(\mathbb{R})$ sequence such that $\sum_{n=1}^{\infty}\int_{\mathbb{R}}f_n(x)g(x)d\mu(x)=0$,sequence such that,L^2(\mathbb{R}) \sum_{n=1}^{\infty}\int_{\mathbb{R}}f_n(x)g(x)d\mu(x)=0,"I am currently studying for an analysis qualifying exam, and this problem has been bothering me: Suppose we have a sequence $\{f_n\}$ in $L^2(\mathbb{R})$ such that $\sum_{n=1}^{\infty}||f_n||_2^2<\infty$ and $\sum_{n=1}^{\infty} f_n(x)=0$ for almost every $x\in\mathbb{R}$. Then for every $g\in L^2(\mathbb{R})$, $\sum_{n=1}^{\infty}\int_{\mathbb{R}}f_n(x)g(x)d\mu(x)$ exists and is equal to zero. My first thought was to use Cauchy-Schwarz, but the problem is that the norm of $f_n$ is squared in the sum above. My other thought was to try and use something like the Dominated Convergence Theorem on a function $h_m(x)=\sum_{n=1}^{m}f_n(x)$, but I think I am forgetting something easy about the relationship between $|f_n(x)|$ and $||f_n||_2$. Any pointers would be appreciated. Thank you in advance!","I am currently studying for an analysis qualifying exam, and this problem has been bothering me: Suppose we have a sequence $\{f_n\}$ in $L^2(\mathbb{R})$ such that $\sum_{n=1}^{\infty}||f_n||_2^2<\infty$ and $\sum_{n=1}^{\infty} f_n(x)=0$ for almost every $x\in\mathbb{R}$. Then for every $g\in L^2(\mathbb{R})$, $\sum_{n=1}^{\infty}\int_{\mathbb{R}}f_n(x)g(x)d\mu(x)$ exists and is equal to zero. My first thought was to use Cauchy-Schwarz, but the problem is that the norm of $f_n$ is squared in the sum above. My other thought was to try and use something like the Dominated Convergence Theorem on a function $h_m(x)=\sum_{n=1}^{m}f_n(x)$, but I think I am forgetting something easy about the relationship between $|f_n(x)|$ and $||f_n||_2$. Any pointers would be appreciated. Thank you in advance!",,"['real-analysis', 'lebesgue-integral', 'lp-spaces']"
54,Solve $\lim_{x\to +\infty}\frac{x^x}{(\lfloor x \rfloor)^{\lfloor x \rfloor }}$,Solve,\lim_{x\to +\infty}\frac{x^x}{(\lfloor x \rfloor)^{\lfloor x \rfloor }},"Determine if the following limits exist $$\lim_{x\to +\infty}\dfrac{x^x}{(\lfloor x \rfloor)^{\lfloor x \rfloor }}$$ note that $\lfloor x \rfloor \leq x < \lfloor x \rfloor + 1 \implies x-1 <\lfloor x \rfloor  \leq x$ and $x^x=\exp(x\log x)$ $$\dfrac{1}{x^{\lfloor x \rfloor}} \le \dfrac{1}{\lfloor x \rfloor}< \dfrac{1}{(x-1)^{\lfloor x \rfloor}}$$ $$\dfrac{x^x}{x^{\lfloor x \rfloor}} \le \dfrac{x^x}{\lfloor x \rfloor}< \dfrac{x^x}{(x-1)^{\lfloor x \rfloor}}$$ Edit Let $f(x)=\dfrac{x^x}{(\lfloor x \rfloor)^{\lfloor x \rfloor }}$, already $x^x$ is defined only for $x\geq 0$, then $f$ is defined on $\mathbb{R}^{*}_{+}$, Case 1 : $x\in \mathbb{N}^{*}$  $$\dfrac{x^x}{(\lfloor x \rfloor)^{\lfloor x \rfloor }}=1$$ So a possible limit may be $1$ Case 2 : otherswises if we choose $x=n+0,5$, we have \begin{align*} f(x)&=\exp((n+0,5)\log(n+0,5)-n\log(n)\\ &=\exp(n(ln(n+0,5)-ln(n))+0,5ln(n+05))\\ &\geq \exp(0,5\log(n+05)) \end{align*} or $\exp(0,5\log(n+05))\to \infty$ when $x\to \infty$ $\forall\quad 0<\epsilon<1$, let $W_n=f(n+\epsilon)$ we have $$ \begin{align*}  ln(W_n)&=(n+\epsilon)ln(n+\epsilon)-nln\\ &=n(ln(n+\epsilon)-ln(n))+\epsilon ln(n+\epsilon)\\ &\geq \epsilon ln(n+\epsilon) \end{align*} $$ then $$\lim_{x\to +\infty}W_n=+\infty $$ thus $$\lim_{x\to +\infty}f(x)=+\infty $$ therefore the function $f$ does not have a real limit as $x$ tends to infinity","Determine if the following limits exist $$\lim_{x\to +\infty}\dfrac{x^x}{(\lfloor x \rfloor)^{\lfloor x \rfloor }}$$ note that $\lfloor x \rfloor \leq x < \lfloor x \rfloor + 1 \implies x-1 <\lfloor x \rfloor  \leq x$ and $x^x=\exp(x\log x)$ $$\dfrac{1}{x^{\lfloor x \rfloor}} \le \dfrac{1}{\lfloor x \rfloor}< \dfrac{1}{(x-1)^{\lfloor x \rfloor}}$$ $$\dfrac{x^x}{x^{\lfloor x \rfloor}} \le \dfrac{x^x}{\lfloor x \rfloor}< \dfrac{x^x}{(x-1)^{\lfloor x \rfloor}}$$ Edit Let $f(x)=\dfrac{x^x}{(\lfloor x \rfloor)^{\lfloor x \rfloor }}$, already $x^x$ is defined only for $x\geq 0$, then $f$ is defined on $\mathbb{R}^{*}_{+}$, Case 1 : $x\in \mathbb{N}^{*}$  $$\dfrac{x^x}{(\lfloor x \rfloor)^{\lfloor x \rfloor }}=1$$ So a possible limit may be $1$ Case 2 : otherswises if we choose $x=n+0,5$, we have \begin{align*} f(x)&=\exp((n+0,5)\log(n+0,5)-n\log(n)\\ &=\exp(n(ln(n+0,5)-ln(n))+0,5ln(n+05))\\ &\geq \exp(0,5\log(n+05)) \end{align*} or $\exp(0,5\log(n+05))\to \infty$ when $x\to \infty$ $\forall\quad 0<\epsilon<1$, let $W_n=f(n+\epsilon)$ we have $$ \begin{align*}  ln(W_n)&=(n+\epsilon)ln(n+\epsilon)-nln\\ &=n(ln(n+\epsilon)-ln(n))+\epsilon ln(n+\epsilon)\\ &\geq \epsilon ln(n+\epsilon) \end{align*} $$ then $$\lim_{x\to +\infty}W_n=+\infty $$ thus $$\lim_{x\to +\infty}f(x)=+\infty $$ therefore the function $f$ does not have a real limit as $x$ tends to infinity",,"['calculus', 'real-analysis', 'analysis', 'limits', 'contest-math']"
55,measurability with zero measure,measurability with zero measure,,"Let $f : [0,1] \rightarrow \Bbb R$ is arbitrary function , and $E \subset \{ x \in [0,1] | f'(x)$  exists$\}$. How to prove this statement: If $E$ is measurable with zero measure then $f(E)$ is measurable with zero measure. If $E$ is measurable with zero measure then $m(E)=0$ that $m :$ {measurable} $\rightarrow \Bbb R$, $m^* : P(\Bbb R) \rightarrow \Bbb R$,$A:= \{ x \in [0,1] | f'(x)$ is  be exist$\}$. so $m(E)=m^*(E) <= m^*(A)$","Let $f : [0,1] \rightarrow \Bbb R$ is arbitrary function , and $E \subset \{ x \in [0,1] | f'(x)$  exists$\}$. How to prove this statement: If $E$ is measurable with zero measure then $f(E)$ is measurable with zero measure. If $E$ is measurable with zero measure then $m(E)=0$ that $m :$ {measurable} $\rightarrow \Bbb R$, $m^* : P(\Bbb R) \rightarrow \Bbb R$,$A:= \{ x \in [0,1] | f'(x)$ is  be exist$\}$. so $m(E)=m^*(E) <= m^*(A)$",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
56,Prove the set of points where $f$ is differentiable is dense.,Prove the set of points where  is differentiable is dense.,f,"Let $I\subset \Bbb{R}$ be an open interval and consider a continuous function $f:I\to \Bbb{R}$ satisfying, for all $x\in I$ $$\displaystyle \lim_{h\to 0} \frac{f(x+h)+f(x-h)-2f(x)}{h}=0$$   Prove that the set of points at which $f$ is differentiable is dense in $I$. I know that $$\displaystyle \lim_{h\to 0} \frac{f(x+h)+f(x-h)-2f(x)}{h^2}=f''(x)$$ when $f''(x)$ exists, but it seems to be no use here.","Let $I\subset \Bbb{R}$ be an open interval and consider a continuous function $f:I\to \Bbb{R}$ satisfying, for all $x\in I$ $$\displaystyle \lim_{h\to 0} \frac{f(x+h)+f(x-h)-2f(x)}{h}=0$$   Prove that the set of points at which $f$ is differentiable is dense in $I$. I know that $$\displaystyle \lim_{h\to 0} \frac{f(x+h)+f(x-h)-2f(x)}{h^2}=f''(x)$$ when $f''(x)$ exists, but it seems to be no use here.",,['real-analysis']
57,When differentiability of the product implies differentiability of the individual terms?,When differentiability of the product implies differentiability of the individual terms?,,"Say we have $ h(x)=f(x)\cdot g(x)$ where $f$ and $g$ are continuous and strictly increasing. It follows they are differentiable almost everywhere and so is $h$. We also know that $f>0$ and $g>0$. I'm trying to find a straightforward proof that under these conditions, if $h$ is differentiable everywhere then both $f$ and $g$ are also differentiable everywhere. I have more structure on these functions but I was hoping I did need to impose additional assumptions.","Say we have $ h(x)=f(x)\cdot g(x)$ where $f$ and $g$ are continuous and strictly increasing. It follows they are differentiable almost everywhere and so is $h$. We also know that $f>0$ and $g>0$. I'm trying to find a straightforward proof that under these conditions, if $h$ is differentiable everywhere then both $f$ and $g$ are also differentiable everywhere. I have more structure on these functions but I was hoping I did need to impose additional assumptions.",,['real-analysis']
58,Show function with infinitely many discontinuities is Riemann-integrable,Show function with infinitely many discontinuities is Riemann-integrable,,"Prove that the following function is Riemann-integrable on $[0, 1]$ even though it has infinite many discontinuities on $[0, 1]$ : $$f(x) = \begin{cases} 1 & \text{if } x = \frac1n \text{ where } n = 1, 2, 3, \ldots, \\ 0 & \text{otherwise}.\end{cases}$$ I have a possible proof but don't feel too confident about it. If anyone could tell me if its valid or give me another proof, or hints to another proof, that would be very helpful! Let $\varepsilon > 0$ .  Observe that given $\frac\varepsilon2$ there exists a least natural number $N$ with $\frac{1}{N+1} < \frac\varepsilon2$ . Observe that there are $N$ numbers less than $N + 1$ . Now we want to consider the two intervals $[0, \delta]$ and $[\delta, 1]$ with $0 < \delta < 1$ and $\delta = \frac\varepsilon2$ . Lets first consider the interval $[\delta, 1]$ . Observe that there are exactly $N$ discontinuities on this interval, that is, $N$ elements of the form $\frac1n$ such that $\frac1N > \frac{1}{N+1}$ . This interval is bounded since by construction $f$ is bounded above by 1 and bounded below by 0 and thus it is bounded. We have satisfied the hypothesis of Exercise 6.6 and thus we can conclude that $f$ is Riemann-integrable on $[\delta, 1]$ . Moreover, since $f$ is Riemann-integrable on $[\delta, 1]$ , by Theorem 6.1, we are assured that there exists a partition $P_2$ such that $U(P_2, f) - L(P_2, f) < \frac\varepsilon2$ . Observe that the lower sum is 0 for any given partition, so we have $U(P_2, f) < \frac\varepsilon2$ . Now let us consider the interval $[0, \delta]$ . Let $P_1$ be any partition of this set and observe that $$U(P, f) = \sum_{i = 1}^l M_i \, \Delta x_i < 1 \cdot \big( \frac\varepsilon2 - 0\big) = \frac\varepsilon2$$ since the supremum of the entire interval is 1, it must hold that 1 is an upper bound for any smaller interval. Moreover the length of the interval $[0, \delta]$ is $\delta$ which is defined to be $\frac\varepsilon2$ . Again it still holds that the lower sum is 0. Thus we have $U(P_1, f) < \frac\varepsilon2$ . Now choose $P = P_1 \cup P_2$ to be a common refinement of $P_1$ and $P_2$ and observe that $$U(P, f) - L(P, f) = U(P, f) = U(P_1, f) + U(P_2, f) = \frac\varepsilon2 + \frac\varepsilon2 = \varepsilon$$ Thus by Theorem 6.1 $f$ is Riemann-integrable on $[0, 1]$ .","Prove that the following function is Riemann-integrable on even though it has infinite many discontinuities on : I have a possible proof but don't feel too confident about it. If anyone could tell me if its valid or give me another proof, or hints to another proof, that would be very helpful! Let .  Observe that given there exists a least natural number with . Observe that there are numbers less than . Now we want to consider the two intervals and with and . Lets first consider the interval . Observe that there are exactly discontinuities on this interval, that is, elements of the form such that . This interval is bounded since by construction is bounded above by 1 and bounded below by 0 and thus it is bounded. We have satisfied the hypothesis of Exercise 6.6 and thus we can conclude that is Riemann-integrable on . Moreover, since is Riemann-integrable on , by Theorem 6.1, we are assured that there exists a partition such that . Observe that the lower sum is 0 for any given partition, so we have . Now let us consider the interval . Let be any partition of this set and observe that since the supremum of the entire interval is 1, it must hold that 1 is an upper bound for any smaller interval. Moreover the length of the interval is which is defined to be . Again it still holds that the lower sum is 0. Thus we have . Now choose to be a common refinement of and and observe that Thus by Theorem 6.1 is Riemann-integrable on .","[0, 1] [0, 1] f(x) = \begin{cases} 1 & \text{if } x = \frac1n \text{ where } n = 1, 2, 3, \ldots, \\ 0 & \text{otherwise}.\end{cases} \varepsilon > 0 \frac\varepsilon2 N \frac{1}{N+1} < \frac\varepsilon2 N N + 1 [0, \delta] [\delta, 1] 0 < \delta < 1 \delta = \frac\varepsilon2 [\delta, 1] N N \frac1n \frac1N > \frac{1}{N+1} f f [\delta, 1] f [\delta, 1] P_2 U(P_2, f) - L(P_2, f) < \frac\varepsilon2 U(P_2, f) < \frac\varepsilon2 [0, \delta] P_1 U(P, f) = \sum_{i = 1}^l M_i \, \Delta x_i < 1 \cdot \big( \frac\varepsilon2 - 0\big) = \frac\varepsilon2 [0, \delta] \delta \frac\varepsilon2 U(P_1, f) < \frac\varepsilon2 P = P_1 \cup P_2 P_1 P_2 U(P, f) - L(P, f) = U(P, f) = U(P_1, f) + U(P_2, f) = \frac\varepsilon2 + \frac\varepsilon2 = \varepsilon f [0, 1]","['real-analysis', 'integration']"
59,"Existence of a Strictly Increasing, Continuous Function whose Derivative is 0 a.e. on $\mathbb{R}$","Existence of a Strictly Increasing, Continuous Function whose Derivative is 0 a.e. on",\mathbb{R},"This proof is almost done except for the step of showing that the function's derivative is $0$ a.e. Let $I = \{[p_n, q_n]\}$ denote the set of all closed intervals in $\mathbb{R}$ with rational endpoints $p_i, q_i \in \mathbb{Q}$. Let $\phi_n$ denote a variant of the Cantor Ternary Function (i.e., Devil's Staircase Function) extended to all $\mathbb{R}$ which satisfies $\phi_n(x) = 0$ for all $x < p_i$, $\phi_n(x) = 1/2^n$ for all $x > q_i$ and is non-decreasing, continuous, and of zero derivative a.e. on $[p_i,q_i]$ so that $\phi_n$ satisfies $\phi_n'(x) = 0$ a.e. on $\mathbb{R}$ $\phi_n$ non-decreasing, continuous on $\mathbb{R}$ $\phi_n$ is bounded above by $1/2^n$ $\phi_n (p_n) < \phi_n (q_n)$ (trivial but important later) Now let $\sum \phi_n = \phi$. $\fbox{Claim}$ $\sum \phi_n \rightarrow \phi$ uniformly $\phi$ is strictly increasing and continuous on $\mathbb{R}$ yet also satisfies $\phi'(x) = 0$ a.e. on $\mathbb{R}$. $\fbox{Attempted Proof}$ First since $\forall n \in \mathbb{N}$, we have that $|\phi_n| \le 1/2^n$ with $\sum 1/2^n < \infty$, it follows that $\sum \phi_n \rightarrow \phi$ uniformly (a fact from Real Analysis). Consider that the finite sum of $k$ continuous functions is itself a continuous function.  Since all of the $\phi_n$ are continuous functions, we therefore have that $f_k = \sum_{n=1}^k \phi_n$ is also a continuous function.  Moreover, since (1) implies that $f_k \rightarrow \phi$ unformly, we now have that $\phi$ is also continuous (via another Real Analysis fact that a uniformly convergent sequence of continuous functions converges to another continuous function). To show $\phi$ is strictly increasing, consider $x,y \in \mathbb{R}$ s.t. $x < y$.  Since $\exists k \in \mathbb{N}$ s.t. $x < p_k < q_k < y$, we have that $\phi_k(x) \le \phi_k(p_k) < \phi_k(q_k) \le \phi_k(y)$ implies that $\phi_k(x) < \phi_k(y)$ so that for $N \ge k$ we have $\sum_{n = 1}^N \phi_n(x) < \sum_{n = 1}^N \phi_n(y)$ and more generally $\phi(x) < \phi(y)$.  This shows that indeed $\phi$ is strictly increasing on $\mathbb{R}$. To show that $\phi'(x) = 0$ a.e. on $\mathbb{R}$, we will show that $\phi' = \sum \phi_n' = \sum 0_n$, where each $0_n$ is the derivative function of $\phi_n$ which is $0$ almost everywhere. But I'm not sure how to complete (4)?","This proof is almost done except for the step of showing that the function's derivative is $0$ a.e. Let $I = \{[p_n, q_n]\}$ denote the set of all closed intervals in $\mathbb{R}$ with rational endpoints $p_i, q_i \in \mathbb{Q}$. Let $\phi_n$ denote a variant of the Cantor Ternary Function (i.e., Devil's Staircase Function) extended to all $\mathbb{R}$ which satisfies $\phi_n(x) = 0$ for all $x < p_i$, $\phi_n(x) = 1/2^n$ for all $x > q_i$ and is non-decreasing, continuous, and of zero derivative a.e. on $[p_i,q_i]$ so that $\phi_n$ satisfies $\phi_n'(x) = 0$ a.e. on $\mathbb{R}$ $\phi_n$ non-decreasing, continuous on $\mathbb{R}$ $\phi_n$ is bounded above by $1/2^n$ $\phi_n (p_n) < \phi_n (q_n)$ (trivial but important later) Now let $\sum \phi_n = \phi$. $\fbox{Claim}$ $\sum \phi_n \rightarrow \phi$ uniformly $\phi$ is strictly increasing and continuous on $\mathbb{R}$ yet also satisfies $\phi'(x) = 0$ a.e. on $\mathbb{R}$. $\fbox{Attempted Proof}$ First since $\forall n \in \mathbb{N}$, we have that $|\phi_n| \le 1/2^n$ with $\sum 1/2^n < \infty$, it follows that $\sum \phi_n \rightarrow \phi$ uniformly (a fact from Real Analysis). Consider that the finite sum of $k$ continuous functions is itself a continuous function.  Since all of the $\phi_n$ are continuous functions, we therefore have that $f_k = \sum_{n=1}^k \phi_n$ is also a continuous function.  Moreover, since (1) implies that $f_k \rightarrow \phi$ unformly, we now have that $\phi$ is also continuous (via another Real Analysis fact that a uniformly convergent sequence of continuous functions converges to another continuous function). To show $\phi$ is strictly increasing, consider $x,y \in \mathbb{R}$ s.t. $x < y$.  Since $\exists k \in \mathbb{N}$ s.t. $x < p_k < q_k < y$, we have that $\phi_k(x) \le \phi_k(p_k) < \phi_k(q_k) \le \phi_k(y)$ implies that $\phi_k(x) < \phi_k(y)$ so that for $N \ge k$ we have $\sum_{n = 1}^N \phi_n(x) < \sum_{n = 1}^N \phi_n(y)$ and more generally $\phi(x) < \phi(y)$.  This shows that indeed $\phi$ is strictly increasing on $\mathbb{R}$. To show that $\phi'(x) = 0$ a.e. on $\mathbb{R}$, we will show that $\phi' = \sum \phi_n' = \sum 0_n$, where each $0_n$ is the derivative function of $\phi_n$ which is $0$ almost everywhere. But I'm not sure how to complete (4)?",,"['real-analysis', 'measure-theory']"
60,Zeroes of the third derivative of an iterated sine.,Zeroes of the third derivative of an iterated sine.,,"I've been playing with the functions $$f_n:[0,\pi/2]\to[0,1]\\\begin{cases} f_1&=&\sin\\f_{n+1}&=&\sin\circ f_n\end{cases}.$$ A simple argument proves that $f_n(x)\to 0$ for $x\in [0,\pi/2].$ I thought it was interesting, though, how the functions did that. Near $0,$ we have $\sin\approx \operatorname{id},$ so near $0,$ iterating the sine should be close to iterating the identity function. I wanted to see what happens near zero that the function $f_n$ stops behaving like the identity function and starts behaving so as to allow the convergence of the sequence to zero. I've plotted some iterations and it looks like the functions get ""broken"" near zero. The point of the ""breaking"" seems to go to zero as $n$ goes to infinity. And the ""break"" seems to get more and more ""severe"". I'll post some graphs of the following ""normalized"" functions: $$g_n:[0,\pi/2]\to[0,1]\\g_n=\frac{f_n}{f_n(\pi/2)}$$ These functions approximate the indicator function of the domain. (I've actually seen a proof of this. My teacher showed me when I asked him, but I've already forgotten it. I remember it involved the Stolz–Cesàro theorem.) As you can see, there are points when the functions slow down abruptly. I believe the point where it happens should be the unique zero of the third derivative of $g_n.$ It is easy to see that the first derivatives of the functions are all positive and the second derivatives are all negative. However, writing down a general formula for the third derivative is difficult. I've tried doing it despite the length, but I realized that even if I finish doing it, I'll never see anything in such a long formula. So my question is: can you see a way of proving that the third derivative of $g_n$ has a unique zero in $(0,\pi/2)$ for $n>1?$ And if so, that those zeroes converge to zero as $n\to \infty?$ EDIT: I still don't know how to prove it, but my interpretation of the pictures has changed on further reflection. It seems to me that I was wrong, and the points I'm asking about are not the zeroes of the third derivatives, but their minima (zeroes of the fourth derivatives). They are points were the speed of deceleration is the greatest, not the points where the speed of deceleration is zero. This means that there is more to prove apparently. I'm still unable to handle the third derivatives of these functions, so I can't prove that the third derivatives are negative, which I now think is true. But I can't even dream of trying to write down the formula of the fourth derivative and equate it to zero to find out something about the points. $g_1$ $g_{10}$ $g_{200}$ $g_{1000}$ $g_{10000}$ $g_{100000}$","I've been playing with the functions $$f_n:[0,\pi/2]\to[0,1]\\\begin{cases} f_1&=&\sin\\f_{n+1}&=&\sin\circ f_n\end{cases}.$$ A simple argument proves that $f_n(x)\to 0$ for $x\in [0,\pi/2].$ I thought it was interesting, though, how the functions did that. Near $0,$ we have $\sin\approx \operatorname{id},$ so near $0,$ iterating the sine should be close to iterating the identity function. I wanted to see what happens near zero that the function $f_n$ stops behaving like the identity function and starts behaving so as to allow the convergence of the sequence to zero. I've plotted some iterations and it looks like the functions get ""broken"" near zero. The point of the ""breaking"" seems to go to zero as $n$ goes to infinity. And the ""break"" seems to get more and more ""severe"". I'll post some graphs of the following ""normalized"" functions: $$g_n:[0,\pi/2]\to[0,1]\\g_n=\frac{f_n}{f_n(\pi/2)}$$ These functions approximate the indicator function of the domain. (I've actually seen a proof of this. My teacher showed me when I asked him, but I've already forgotten it. I remember it involved the Stolz–Cesàro theorem.) As you can see, there are points when the functions slow down abruptly. I believe the point where it happens should be the unique zero of the third derivative of $g_n.$ It is easy to see that the first derivatives of the functions are all positive and the second derivatives are all negative. However, writing down a general formula for the third derivative is difficult. I've tried doing it despite the length, but I realized that even if I finish doing it, I'll never see anything in such a long formula. So my question is: can you see a way of proving that the third derivative of $g_n$ has a unique zero in $(0,\pi/2)$ for $n>1?$ And if so, that those zeroes converge to zero as $n\to \infty?$ EDIT: I still don't know how to prove it, but my interpretation of the pictures has changed on further reflection. It seems to me that I was wrong, and the points I'm asking about are not the zeroes of the third derivatives, but their minima (zeroes of the fourth derivatives). They are points were the speed of deceleration is the greatest, not the points where the speed of deceleration is zero. This means that there is more to prove apparently. I'm still unable to handle the third derivatives of these functions, so I can't prove that the third derivatives are negative, which I now think is true. But I can't even dream of trying to write down the formula of the fourth derivative and equate it to zero to find out something about the points. $g_1$ $g_{10}$ $g_{200}$ $g_{1000}$ $g_{10000}$ $g_{100000}$",,"['real-analysis', 'sequences-and-series']"
61,Misunderstanding in Spivak's Proof of the Intermediate Value Theorem,Misunderstanding in Spivak's Proof of the Intermediate Value Theorem,,"I am a student who has taken the basic calculus courses but I am working through Calculus (the fourth edition) by Spivak in my spare time in order to both review the material and to gain a more rigorous understanding of the concepts. I am currently in Chapter 8 where he supplies a proof of the basis of the Intermediate Value Theorem, Theorem 7-1 (pp. 135-136 of the fourth edition). It seems mostly straightforward but I think there is some nuance of it that I don't grasp. I am including in this post the statement Theorem 6-3 since he references it in the proof of Theorem 7-1 as well as part of Theorem 7-1 itself. Theorem 6-3 Suppose $f$ is continuous at $a$, and $f(a) > 0$. Then $f(x) > 0$ for all $x$ in some interval containing $a$; more precisely, there is a number $\delta > 0$ such that $f(x) > 0$ for all $x$ satisfying $|x-a| < \delta$. Similarly, if $f(a) < 0$, then there is a number $\delta > 0$ such that $f(x) < 0$ for all $x$ satisfying $|x-a| < \delta$. Problem 6-16 is also referenced. However,it is the same thing as Theorem 6-3 for one-sided limits. The following proof is the Theorem 7-1 up through the paragraph which I don't fully understand. Theorem 7-1: If $f$ is continuous on $[a,b]$ and $f(a) < 0 < f(b)$, then there is some number $z$ in $[a,b]$ such that $f(x) = 0$. Proof: Define the set $A$ as follows:  $$A = \{x : a \le x\le b, \mbox{ and } f \mbox{ is negative on the interval } [a,x] \}.$$       Clearly $A \ne \emptyset$, since $a$ is in $A$; in fact, there is some $\delta > 0$ such that $A$ contains all points $x$ satisfying $a \le x < a + \delta$; this follows from Problem 6-16, since $f$ is continuous on $[a,b]$ and $f(a)<0$. Similarly, $b$ is an upper bound for $A$ and, in fact, there is a $\delta > 0$ such that all points $x$ satisfying $b-\delta < x \le b$ are upper bounds for $A$; this also follows from Problem 6-16, since $f(b) > 0$. From these remarks it follows that $A$ has a least upper bound $\alpha$ and that $a < \alpha < b$. We now wish to show that $f(\alpha) = 0$, by eliminating the possibilities $f(\alpha) < 0$ and $f(\alpha) > 0$. Suppose first that $f(\alpha) < 0$. By Theorem 6-3, there is a $\delta > 0$ such that $f(x) < 0$ for $\alpha - \delta < x < \alpha + \delta$. Now there is some number $x_0$ in $A$ which satisfies $\alpha - \delta < x_0 < \alpha$ (because otherwise $\alpha$ would not be the least upper bound of $A$). This means that $f$ is negative on the whole interval $[a,x_0]$. But if $x_1$ is a number between $\alpha$ and $\alpha+\delta$, then $f$ is also negative on the whole interval $[x_0,x_1]$. Therefore $f$ is negative on the interval $[a,x_1]$, so $x_1$ is in $A$. But this contradicts the fact that $\alpha$ is an upper bound for $A$; our original assumption that $f(\alpha) < 0$ must be false. The point I don't understand is in the final paragraph of the excerpt. Why does Spivak split the interval $[a,b]$ up into the subintervals $\lbrack a,x_0 \rbrack$ and $[x_0,x_1]$? Isn't it true that, because $\alpha$ is a least upper bound of $A$ and $f$ is continuous on $[a,b]$, it must be true that if $a \le x < \alpha$ then $f(x)<0$? I understand the need to pick out the $x_1$ but not the $x_0$. Especially perplexing is the statement ""there is some number $x_0$ in $A$ which satisfies $\alpha - \delta < x_0 < \alpha$ (because otherwise $\alpha$ would not be the least upper bound of $A$."" Wouldn't it be true that if there were some $x_0$ such that $a \le x_0 < \alpha$ and $f(x_0) \ge 0$ that $x_0$ would be an upper bound for $A$, contradicting that $\alpha$ is the least upper bound of $A$? Thanks for any help.","I am a student who has taken the basic calculus courses but I am working through Calculus (the fourth edition) by Spivak in my spare time in order to both review the material and to gain a more rigorous understanding of the concepts. I am currently in Chapter 8 where he supplies a proof of the basis of the Intermediate Value Theorem, Theorem 7-1 (pp. 135-136 of the fourth edition). It seems mostly straightforward but I think there is some nuance of it that I don't grasp. I am including in this post the statement Theorem 6-3 since he references it in the proof of Theorem 7-1 as well as part of Theorem 7-1 itself. Theorem 6-3 Suppose $f$ is continuous at $a$, and $f(a) > 0$. Then $f(x) > 0$ for all $x$ in some interval containing $a$; more precisely, there is a number $\delta > 0$ such that $f(x) > 0$ for all $x$ satisfying $|x-a| < \delta$. Similarly, if $f(a) < 0$, then there is a number $\delta > 0$ such that $f(x) < 0$ for all $x$ satisfying $|x-a| < \delta$. Problem 6-16 is also referenced. However,it is the same thing as Theorem 6-3 for one-sided limits. The following proof is the Theorem 7-1 up through the paragraph which I don't fully understand. Theorem 7-1: If $f$ is continuous on $[a,b]$ and $f(a) < 0 < f(b)$, then there is some number $z$ in $[a,b]$ such that $f(x) = 0$. Proof: Define the set $A$ as follows:  $$A = \{x : a \le x\le b, \mbox{ and } f \mbox{ is negative on the interval } [a,x] \}.$$       Clearly $A \ne \emptyset$, since $a$ is in $A$; in fact, there is some $\delta > 0$ such that $A$ contains all points $x$ satisfying $a \le x < a + \delta$; this follows from Problem 6-16, since $f$ is continuous on $[a,b]$ and $f(a)<0$. Similarly, $b$ is an upper bound for $A$ and, in fact, there is a $\delta > 0$ such that all points $x$ satisfying $b-\delta < x \le b$ are upper bounds for $A$; this also follows from Problem 6-16, since $f(b) > 0$. From these remarks it follows that $A$ has a least upper bound $\alpha$ and that $a < \alpha < b$. We now wish to show that $f(\alpha) = 0$, by eliminating the possibilities $f(\alpha) < 0$ and $f(\alpha) > 0$. Suppose first that $f(\alpha) < 0$. By Theorem 6-3, there is a $\delta > 0$ such that $f(x) < 0$ for $\alpha - \delta < x < \alpha + \delta$. Now there is some number $x_0$ in $A$ which satisfies $\alpha - \delta < x_0 < \alpha$ (because otherwise $\alpha$ would not be the least upper bound of $A$). This means that $f$ is negative on the whole interval $[a,x_0]$. But if $x_1$ is a number between $\alpha$ and $\alpha+\delta$, then $f$ is also negative on the whole interval $[x_0,x_1]$. Therefore $f$ is negative on the interval $[a,x_1]$, so $x_1$ is in $A$. But this contradicts the fact that $\alpha$ is an upper bound for $A$; our original assumption that $f(\alpha) < 0$ must be false. The point I don't understand is in the final paragraph of the excerpt. Why does Spivak split the interval $[a,b]$ up into the subintervals $\lbrack a,x_0 \rbrack$ and $[x_0,x_1]$? Isn't it true that, because $\alpha$ is a least upper bound of $A$ and $f$ is continuous on $[a,b]$, it must be true that if $a \le x < \alpha$ then $f(x)<0$? I understand the need to pick out the $x_1$ but not the $x_0$. Especially perplexing is the statement ""there is some number $x_0$ in $A$ which satisfies $\alpha - \delta < x_0 < \alpha$ (because otherwise $\alpha$ would not be the least upper bound of $A$."" Wouldn't it be true that if there were some $x_0$ such that $a \le x_0 < \alpha$ and $f(x_0) \ge 0$ that $x_0$ would be an upper bound for $A$, contradicting that $\alpha$ is the least upper bound of $A$? Thanks for any help.",,"['calculus', 'real-analysis']"
62,"Prove that if $\{a_{k}\}$ is a sequence of real numbers such that $\sum_{k=1}^{\infty} \frac{|a_{k}|}{k} = \infty$,","Prove that if  is a sequence of real numbers such that ,",\{a_{k}\} \sum_{k=1}^{\infty} \frac{|a_{k}|}{k} = \infty,"Prove that if $\{a_{k}\}$ is a sequence of real numbers such that $$\sum_{k=1}^{\infty} \frac{|a_{k}|}{k} = \infty$$ and $$\sum_{n=1}^{\infty} \left( \sum_{k=2^{n-1}}^{2^n-1} k(a_k - a_{k+1})^2 \right)^{1/2} < \infty,$$ then $$\int_{0}^{\pi} \left| \sum_{k=1}^{\infty} a_k \sin(kx) \right| \,dx = \infty.$$ My idea to prove Although the condition $$\lim_{k \rightarrow \infty} a_k=0$$ does not appear in the statement of the problem, by the well-known Cantor-Lebesgue theorem, this follows from the fact that the series $$\sum_{k=1}^{\infty} a_k \sin k x$$ is convergent almost everywhere. We note that for the application of this theorem, it would be sufficient if the series (2) were convergent on a set of positive measure. To make reference easier, we list the remaining conditions: $$\sum_{k=1}^{\infty} \frac{\left|a_k\right|}{k}=\infty$$ $$\sum_{n=1}^{\infty}\left(\sum_{k=2^{n-1}}^{2^n-1} k\left|\Delta a_k\right|^2\right)^{1 / 2}<\infty$$ where $$\Delta a_k:=a_k-a_{k+1} \quad(k=1,2, \ldots) .$$ From (4) it follows that the sequence $\left\{a_k\right\}$ has bounded variation, that is, $$\sum_{k=1}^{\infty}\left|\Delta a_k\right|<\infty$$ Really, by the Cauchy inequality, \begin{aligned} \sum_{k=1}^{\infty}\left|\Delta a_k\right| & =\sum_{n=1}^{\infty} \sum_{k=2^{n-1}}^{2^n-1}\left|\Delta a_k\right| \\ & \leq \sum_{n=1}^{\infty}\left(2^{n-1} \sum_{k=2^{n-1}}^{2^n-1}\left|\Delta a_k\right|^2\right)^{1 / 2} \\ & \leq \sum_{n=1}^{\infty}\left(\sum_{k=2^{n-1}}^{2^n-1} k\left|\Delta a_k\right|^2\right)^{1 / 2} \end{aligned} Consider the $n$ th partial sum of (2). By Abel's rearrangement, we obtain $$ \sum_{k=1}^n a_k \sin k x = \sum_{k=1}^n \tilde{D}_k(x) \Delta a_k + a_{n+1} \tilde{D}_n(x) $$ where $\tilde{D}_n(x)$ is the conjugate Dirichlet kernel: \begin{align*} \tilde{D}_n(x) &:= \sum_{k=1}^n \sin k x \\ &= \frac{\cos \frac{x}{2} - \cos \left(n + \frac{1}{2}\right) x}{2 \sin \frac{x}{2}} \quad (n=1,2, \ldots) . \end{align*} Introduce the notation $$\bar{D}_n(x):=-\frac{\cos \left(n+\frac{1}{2}\right) x}{2 \sin \frac{x}{2}} \quad(n=0,1, \ldots) .$$ Then $$\tilde{D}_n(x)=\bar{D}_n(x)-\bar{D}_0(x) \quad\left(n=0,1, \ldots ; \tilde{D}_0(x)=0\right),$$","Prove that if is a sequence of real numbers such that and then My idea to prove Although the condition does not appear in the statement of the problem, by the well-known Cantor-Lebesgue theorem, this follows from the fact that the series is convergent almost everywhere. We note that for the application of this theorem, it would be sufficient if the series (2) were convergent on a set of positive measure. To make reference easier, we list the remaining conditions: where From (4) it follows that the sequence has bounded variation, that is, Really, by the Cauchy inequality, Consider the th partial sum of (2). By Abel's rearrangement, we obtain where is the conjugate Dirichlet kernel: Introduce the notation Then","\{a_{k}\} \sum_{k=1}^{\infty} \frac{|a_{k}|}{k} = \infty \sum_{n=1}^{\infty} \left( \sum_{k=2^{n-1}}^{2^n-1} k(a_k - a_{k+1})^2 \right)^{1/2} < \infty, \int_{0}^{\pi} \left| \sum_{k=1}^{\infty} a_k \sin(kx) \right| \,dx = \infty. \lim_{k \rightarrow \infty} a_k=0 \sum_{k=1}^{\infty} a_k \sin k x \sum_{k=1}^{\infty} \frac{\left|a_k\right|}{k}=\infty \sum_{n=1}^{\infty}\left(\sum_{k=2^{n-1}}^{2^n-1} k\left|\Delta a_k\right|^2\right)^{1 / 2}<\infty \Delta a_k:=a_k-a_{k+1} \quad(k=1,2, \ldots) . \left\{a_k\right\} \sum_{k=1}^{\infty}\left|\Delta a_k\right|<\infty \begin{aligned}
\sum_{k=1}^{\infty}\left|\Delta a_k\right| & =\sum_{n=1}^{\infty} \sum_{k=2^{n-1}}^{2^n-1}\left|\Delta a_k\right| \\
& \leq \sum_{n=1}^{\infty}\left(2^{n-1} \sum_{k=2^{n-1}}^{2^n-1}\left|\Delta a_k\right|^2\right)^{1 / 2} \\
& \leq \sum_{n=1}^{\infty}\left(\sum_{k=2^{n-1}}^{2^n-1} k\left|\Delta a_k\right|^2\right)^{1 / 2}
\end{aligned} n 
\sum_{k=1}^n a_k \sin k x = \sum_{k=1}^n \tilde{D}_k(x) \Delta a_k + a_{n+1} \tilde{D}_n(x)
 \tilde{D}_n(x) \begin{align*}
\tilde{D}_n(x) &:= \sum_{k=1}^n \sin k x \\
&= \frac{\cos \frac{x}{2} - \cos \left(n + \frac{1}{2}\right) x}{2 \sin \frac{x}{2}} \quad (n=1,2, \ldots) .
\end{align*} \bar{D}_n(x):=-\frac{\cos \left(n+\frac{1}{2}\right) x}{2 \sin \frac{x}{2}} \quad(n=0,1, \ldots) . \tilde{D}_n(x)=\bar{D}_n(x)-\bar{D}_0(x) \quad\left(n=0,1, \ldots ; \tilde{D}_0(x)=0\right),","['real-analysis', 'convergence-divergence', 'fourier-series', 'bounded-variation']"
63,Integration of rational functions. Integration techniques. Hermite-Ostragradski method.,Integration of rational functions. Integration techniques. Hermite-Ostragradski method.,,"By the fundamental theorem of algebra a real nonconstant polynomial $Q$ has factorisation into real prime factors $$Q=g_1^{k_1}g_2^{k_2}\cdots g_l^{k_l}$$ The prime factors $g_j$ , all distinct, are first degree polynomials or second degree polynomials without real roots. The exponents are positive integers. The polynomial $Q$ is called square-free if all the exponents $k_j$ equal 1. Exercise: Let $\psi$ be square-free polynomial of degree $d$ and $P$ have degree less than $2d$ and no prime factor in common with $\psi$ . Show that the integral $\int\frac{P}{\psi^2}$ is rational if and only if $\psi$ divides $P'\psi'-P\psi''.$ My sratch work: I have noticed that $$(\frac{P}{\psi'})'=\frac{P'\psi'-P\psi''}{(\psi')^2}$$ So I decided use this pattern in the given integral bu using integration by parts : $$\int\frac{P}{\psi^2}=\int(\frac{P}{\psi'})(\frac{1}{\psi^2}\psi')=\frac{P}{\psi'}(\frac{-1}{\psi})+\int\frac{1}{\psi}\frac{P'\psi'-P\psi''}{(\psi')^2}$$ Meanwhile the first sum in the right is rational we should care about the rightside integral. Lets consider $$\int\frac{1}{\psi}\frac{P'\psi'-P\psi''}{(\psi')^2}=\frac{A}{B}$$ Where A and B are polynomials. So we get $$\frac{1}{\psi}\frac{P'\psi'-P\psi''}{(\psi')^2}=\frac{A'B-B'A}{B^2} $$ but how judge after I couldn't proceed. Maybe, another approach is using Hermite-Ostragradski method of integration. In this case the integral becomes $$\int \frac{P}{\psi^2}=\frac{P_1}{\psi}+\int \frac{P_2}{\psi}$$ The integral on the right hand side is transcendental  function since the denominator $\psi$ is square free and the numerator has lower degree than the denominator. In order integral to be rational the rightside integral should be equal $0$ . So we get an equality $$\int \frac{P}{\psi^2}=\frac{P_1}{\psi}$$ and diferrentiate both side : $$\frac{P}{\psi^2}=\frac{P_1'\psi-P_1\psi'}{\psi^2}$$ However I am still unable to proceed further. Could anybody help me please.","By the fundamental theorem of algebra a real nonconstant polynomial has factorisation into real prime factors The prime factors , all distinct, are first degree polynomials or second degree polynomials without real roots. The exponents are positive integers. The polynomial is called square-free if all the exponents equal 1. Exercise: Let be square-free polynomial of degree and have degree less than and no prime factor in common with . Show that the integral is rational if and only if divides My sratch work: I have noticed that So I decided use this pattern in the given integral bu using integration by parts : Meanwhile the first sum in the right is rational we should care about the rightside integral. Lets consider Where A and B are polynomials. So we get but how judge after I couldn't proceed. Maybe, another approach is using Hermite-Ostragradski method of integration. In this case the integral becomes The integral on the right hand side is transcendental  function since the denominator is square free and the numerator has lower degree than the denominator. In order integral to be rational the rightside integral should be equal . So we get an equality and diferrentiate both side : However I am still unable to proceed further. Could anybody help me please.",Q Q=g_1^{k_1}g_2^{k_2}\cdots g_l^{k_l} g_j Q k_j \psi d P 2d \psi \int\frac{P}{\psi^2} \psi P'\psi'-P\psi''. (\frac{P}{\psi'})'=\frac{P'\psi'-P\psi''}{(\psi')^2} \int\frac{P}{\psi^2}=\int(\frac{P}{\psi'})(\frac{1}{\psi^2}\psi')=\frac{P}{\psi'}(\frac{-1}{\psi})+\int\frac{1}{\psi}\frac{P'\psi'-P\psi''}{(\psi')^2} \int\frac{1}{\psi}\frac{P'\psi'-P\psi''}{(\psi')^2}=\frac{A}{B} \frac{1}{\psi}\frac{P'\psi'-P\psi''}{(\psi')^2}=\frac{A'B-B'A}{B^2}  \int \frac{P}{\psi^2}=\frac{P_1}{\psi}+\int \frac{P_2}{\psi} \psi 0 \int \frac{P}{\psi^2}=\frac{P_1}{\psi} \frac{P}{\psi^2}=\frac{P_1'\psi-P_1\psi'}{\psi^2},"['real-analysis', 'calculus', 'integration', 'indefinite-integrals']"
64,$x\hat{f}(x)\in L^1(\mathbb{R})$ implies $x\hat{f}(x)\in L^\infty(\mathbb{R})$,implies,x\hat{f}(x)\in L^1(\mathbb{R}) x\hat{f}(x)\in L^\infty(\mathbb{R}),Let $\hat{f}(x)=\int_\mathbb{R}f(t)e^{-itx}dt$ be the Fourier transform of the function $f\in L^1(\mathbb{R})\cap L^\infty(\mathbb{R})$ . If $\hat{f}$ has bounded derivative on $\mathbb{R}$ and $$x\hat{f}(x)\in L^1(\mathbb{R})$$ can we say that $$x\hat{f}(x)\in L^\infty(\mathbb{R})?$$,Let be the Fourier transform of the function . If has bounded derivative on and can we say that,\hat{f}(x)=\int_\mathbb{R}f(t)e^{-itx}dt f\in L^1(\mathbb{R})\cap L^\infty(\mathbb{R}) \hat{f} \mathbb{R} x\hat{f}(x)\in L^1(\mathbb{R}) x\hat{f}(x)\in L^\infty(\mathbb{R})?,"['real-analysis', 'fourier-analysis', 'lp-spaces']"
65,"Finding $\displaystyle \lim_{n\to \infty} (x_0 x_1 \cdots x_n)\sqrt{n}$ where $x_{n+1}=x_n^3-x_n^2+1$, $x_0=\frac{1}{2}$","Finding  where ,",\displaystyle \lim_{n\to \infty} (x_0 x_1 \cdots x_n)\sqrt{n} x_{n+1}=x_n^3-x_n^2+1 x_0=\frac{1}{2},"Problem. Let $(x_n)$ be the sequence defined by $x_0=\frac{1}{2}$ and $x_{n+1}=x_n^3-x_n^2+1$ for any $n\in \mathbb{N}\cup \{0\}$ . Find $$ \lim_{n\to \infty} (x_0 x_1 \cdots x_n)\sqrt{n}.$$ According to the answer sheet, this limit equals $1$ . However, I can't manage to solve it. Here is what I've done. Obviously, $x_{n+1}-x_n=(x_n-1)^2(x_n+1)>0$ (it is easy to observe that all the terms of the sequence are positive), so $(x_n)$ is a strictly increasing sequence. Let us now prove by induction on $n$ that $x_n<1$ for all $n\in \mathbb{N}\cup \{0\}$ . The base case is obvious, so suppose it holds for $n$ and prove it for $n+1$ . $x_{n+1}=x_n^2(x_n-1)+1<1$ by the induction hypothesis and we are done. Hence, $(x_n)$ is monotone and bounded, so it is convergent. It is easy to see now that $\displaystyle \lim_{n\to \infty}x_n=1$ . Now I pretty much got stuck. I tried to use the epsilon definition of a limit, trying to exploit $\displaystyle \lim_{n\to \infty}x_n=1$ , but it didn't help. Maybe I should use Stolz–Cesàro on the limit that I want to compute?","Problem. Let be the sequence defined by and for any . Find According to the answer sheet, this limit equals . However, I can't manage to solve it. Here is what I've done. Obviously, (it is easy to observe that all the terms of the sequence are positive), so is a strictly increasing sequence. Let us now prove by induction on that for all . The base case is obvious, so suppose it holds for and prove it for . by the induction hypothesis and we are done. Hence, is monotone and bounded, so it is convergent. It is easy to see now that . Now I pretty much got stuck. I tried to use the epsilon definition of a limit, trying to exploit , but it didn't help. Maybe I should use Stolz–Cesàro on the limit that I want to compute?",(x_n) x_0=\frac{1}{2} x_{n+1}=x_n^3-x_n^2+1 n\in \mathbb{N}\cup \{0\}  \lim_{n\to \infty} (x_0 x_1 \cdots x_n)\sqrt{n}. 1 x_{n+1}-x_n=(x_n-1)^2(x_n+1)>0 (x_n) n x_n<1 n\in \mathbb{N}\cup \{0\} n n+1 x_{n+1}=x_n^2(x_n-1)+1<1 (x_n) \displaystyle \lim_{n\to \infty}x_n=1 \displaystyle \lim_{n\to \infty}x_n=1,"['real-analysis', 'sequences-and-series', 'limits', 'contest-math']"
66,"Prove that $\forall \epsilon > 0$: $\lim_{t\to\infty}t^{-2}\int_{0}^{t}[(f(x))^{1+\epsilon}/f'(x)]\,\mathrm dx =+\infty$",Prove that :,"\forall \epsilon > 0 \lim_{t\to\infty}t^{-2}\int_{0}^{t}[(f(x))^{1+\epsilon}/f'(x)]\,\mathrm dx =+\infty","Let $f: [0,+\infty) \to [0,+\infty)$ be differentiable, $f' > 0$ . Prove that $$\forall \epsilon > 0: \lim_{t\to\infty}\dfrac{1}{t^2}\int_{0}^{t}\dfrac{\left(f(x)\right)^{1+\epsilon}}{f'(x)}\mathrm dx =+\infty$$ I used L'Hôpital's rule and got $$\lim\limits_{t\to\infty}\dfrac{1}{t^2}\int_{0}^{t}\dfrac{\left(f(x)\right)^{1+\epsilon}}{f'(x)}\mathrm dx =\dfrac{1}{2}\displaystyle\lim_{t\to\infty}\dfrac{\left(f(t)\right)^{1+\epsilon}}{tf'(t)}$$ If $\lim\limits_{t\to\infty}f'(t)$ exists, then we can prove the above statement using L'Hôpital's rule. But if $\lim\limits_{t\to\infty}f'(t)$ does not exist, I don't know how to proceed","Let be differentiable, . Prove that I used L'Hôpital's rule and got If exists, then we can prove the above statement using L'Hôpital's rule. But if does not exist, I don't know how to proceed","f: [0,+\infty) \to [0,+\infty) f' > 0 \forall \epsilon > 0: \lim_{t\to\infty}\dfrac{1}{t^2}\int_{0}^{t}\dfrac{\left(f(x)\right)^{1+\epsilon}}{f'(x)}\mathrm dx =+\infty \lim\limits_{t\to\infty}\dfrac{1}{t^2}\int_{0}^{t}\dfrac{\left(f(x)\right)^{1+\epsilon}}{f'(x)}\mathrm dx =\dfrac{1}{2}\displaystyle\lim_{t\to\infty}\dfrac{\left(f(t)\right)^{1+\epsilon}}{tf'(t)} \lim\limits_{t\to\infty}f'(t) \lim\limits_{t\to\infty}f'(t)","['real-analysis', 'analysis']"
67,If $f^2$ and $f^3$ are $C^{\infty}(\mathbb R)$ then $f$ is $C^{\infty}(\mathbb R)$,If  and  are  then  is,f^2 f^3 C^{\infty}(\mathbb R) f C^{\infty}(\mathbb R),"Since it is an exercise from an oral exam, I have added some indications I had. $f : \mathbb R \to \mathbb R$, such that $f^2$ and $f^3$ are $C^{\infty}$, show that $f$ is $C^{\infty}$. The two isolated hypothesis are not sufficient, since there is a problem in zero for the cube root, and since you can take $f$ that takes $1$ and $-1$ without regularity but its square will be constant. How to proceed with the two hypothesis? Here, the indications I have from the candidate who has taken the oral exam: -First, show that $f$ is $C^1$: Suppose $f(0) = 0$ (it is exactly the problem, and by translation we consider that it is in $0$). Suppose they exist and use the Taylor expansions of $f^2$ and $f^3$ up to a non-zero order. Find a polynomial link between both coefficients using equivalents. If they do not exist, $f^2 =o(h^4)$ so $f$ is $C^1$ in $0$ (?). -Use the general Taylor expansion (with integral remain) for $f^3$.  Suppose the coefficients are not all zeros, (change the bounds to $0$ and $1$), show that the parameter integral is $C^{\infty}$, then take the cube root (??).","Since it is an exercise from an oral exam, I have added some indications I had. $f : \mathbb R \to \mathbb R$, such that $f^2$ and $f^3$ are $C^{\infty}$, show that $f$ is $C^{\infty}$. The two isolated hypothesis are not sufficient, since there is a problem in zero for the cube root, and since you can take $f$ that takes $1$ and $-1$ without regularity but its square will be constant. How to proceed with the two hypothesis? Here, the indications I have from the candidate who has taken the oral exam: -First, show that $f$ is $C^1$: Suppose $f(0) = 0$ (it is exactly the problem, and by translation we consider that it is in $0$). Suppose they exist and use the Taylor expansions of $f^2$ and $f^3$ up to a non-zero order. Find a polynomial link between both coefficients using equivalents. If they do not exist, $f^2 =o(h^4)$ so $f$ is $C^1$ in $0$ (?). -Use the general Taylor expansion (with integral remain) for $f^3$.  Suppose the coefficients are not all zeros, (change the bounds to $0$ and $1$), show that the parameter integral is $C^{\infty}$, then take the cube root (??).",,['real-analysis']
68,"If every point is a local maximum, is it a step function?","If every point is a local maximum, is it a step function?",,"What are the functions $f:\mathbb R\to\mathbb R$ such that every point is a local maximum? Certainly, $f(x)=c$ works for every constant. So does $\lfloor x\rfloor$, as does $I_{\{0\}}(x)=\begin{cases}1&x=0\\0&x\ne0\end{cases}$. Another example is the oddly beautiful $e^{-\lfloor1/\lvert x\rvert\rfloor^{-2}}$. Question is, are step functions such as these the only ones? Are there any examples where every point is a strict local maximum? (Thomae's function is a near-miss, since the condition doesn't hold on irrational points. Although, most points are irrational, so I guess the term ""near-miss"" doesn't quite fit.) EDIT: My definition of step-function is the sum of countably many indicator functions of intervals, $\sum_{n=0}^\infty a_nI_{A_n}(x)$. If it were uncountable, any function would be step, as $\{\alpha\}$ is an interval ($[\alpha,\alpha]$). This arose when I was thinking about the topological space with basis $\{(-\infty,a]:a\in\mathbb R\}$. If you call that space $X$, then the continuous functions $f:\mathbb R\to X$ are precisely these. I don't have a formal proof but I'm pretty sure (and this would belong in a different question anyway). Contrast with: Is $f$ constant if every point is local maximum or local minimum of $f$?","What are the functions $f:\mathbb R\to\mathbb R$ such that every point is a local maximum? Certainly, $f(x)=c$ works for every constant. So does $\lfloor x\rfloor$, as does $I_{\{0\}}(x)=\begin{cases}1&x=0\\0&x\ne0\end{cases}$. Another example is the oddly beautiful $e^{-\lfloor1/\lvert x\rvert\rfloor^{-2}}$. Question is, are step functions such as these the only ones? Are there any examples where every point is a strict local maximum? (Thomae's function is a near-miss, since the condition doesn't hold on irrational points. Although, most points are irrational, so I guess the term ""near-miss"" doesn't quite fit.) EDIT: My definition of step-function is the sum of countably many indicator functions of intervals, $\sum_{n=0}^\infty a_nI_{A_n}(x)$. If it were uncountable, any function would be step, as $\{\alpha\}$ is an interval ($[\alpha,\alpha]$). This arose when I was thinking about the topological space with basis $\{(-\infty,a]:a\in\mathbb R\}$. If you call that space $X$, then the continuous functions $f:\mathbb R\to X$ are precisely these. I don't have a formal proof but I'm pretty sure (and this would belong in a different question anyway). Contrast with: Is $f$ constant if every point is local maximum or local minimum of $f$?",,"['real-analysis', 'general-topology', 'functions']"
69,$f=\underset{+\infty}{\mathcal{O}}\bigr(f''\bigl)$ implies that $f=\underset{+\infty}{\mathcal{O}}\bigr(f'\bigl)$.,implies that .,f=\underset{+\infty}{\mathcal{O}}\bigr(f''\bigl) f=\underset{+\infty}{\mathcal{O}}\bigr(f'\bigl),"Let $f\in\mathcal{C}^2(\Bbb{R},\Bbb{R})$ be a positive function such that $f=\underset{+\infty}{\mathcal{O}}\bigr(f''\bigl)$ does it implies that $f=\underset{+\infty}{\mathcal{O}}\bigr(f'\bigl)$? First intuitively I would say that because of the first hypothesis $f''$ is positive but I don't know how can I prove this. More precisely, I want to prove that if there are $N,C$ so that $|f(x)|\leq C|f''(x)|$ for all $x>N$, then there are $M,B$ so that $|f(x)|\leq B|f'(x)|$ for all $x>M$. Perhaps I can use the taylor expression? $ f(x+h)=f(x)+hf'(x)+\frac{h^2}{2}f''(x)+O(h^3)$ and $f(x-h)=f(x)-hf'(x)+\frac{h^2}{2}f''(x)+O(h^3)$. Then we get that $$f''(x)=\lim_{h\to0} \frac{f(x+h) - 2 f(x) + f(x-h)}{h^{2}}.$$ Any ideas?","Let $f\in\mathcal{C}^2(\Bbb{R},\Bbb{R})$ be a positive function such that $f=\underset{+\infty}{\mathcal{O}}\bigr(f''\bigl)$ does it implies that $f=\underset{+\infty}{\mathcal{O}}\bigr(f'\bigl)$? First intuitively I would say that because of the first hypothesis $f''$ is positive but I don't know how can I prove this. More precisely, I want to prove that if there are $N,C$ so that $|f(x)|\leq C|f''(x)|$ for all $x>N$, then there are $M,B$ so that $|f(x)|\leq B|f'(x)|$ for all $x>M$. Perhaps I can use the taylor expression? $ f(x+h)=f(x)+hf'(x)+\frac{h^2}{2}f''(x)+O(h^3)$ and $f(x-h)=f(x)-hf'(x)+\frac{h^2}{2}f''(x)+O(h^3)$. Then we get that $$f''(x)=\lim_{h\to0} \frac{f(x+h) - 2 f(x) + f(x-h)}{h^{2}}.$$ Any ideas?",,"['real-analysis', 'functions']"
70,Proof check of sum of a compact and closed set of real numbers is closed,Proof check of sum of a compact and closed set of real numbers is closed,,"Let $A$ be a closed and $B$ be a closed and bounded set in $\mathbb R$ , then  we have to show that $A+B:=\{a+b:a\in A , b\in B \}$ is closed in $\mathbb R$ . My Proof : Let $\{a_n+b_n\}$ be a convergent sequence in $A+B$ , where $\{a_n\}\in A , \{b_n\}\in B$ , with limit $x$ , we have to show that $x \in A+B$ . Since $B$ is closed-bounded , there is a subsequence $\{b_{r_n}\}$ of $\{b_n\}$ such that $\{b_{r_n}\}$ converges with $\lim \{b_{r_n}\}=l$ (say) ; then since $B$ is closed , $l\in B$. Also $\lim \{a_{r_n}+b_{r_n}\}=x$ , thus $\lim \{a_{r_n}\}=x-l$ , and since $A$ is closed , so $x-l \in A$ , thus $x=(x-l)+l \in A+B$ . Am I correct ?","Let $A$ be a closed and $B$ be a closed and bounded set in $\mathbb R$ , then  we have to show that $A+B:=\{a+b:a\in A , b\in B \}$ is closed in $\mathbb R$ . My Proof : Let $\{a_n+b_n\}$ be a convergent sequence in $A+B$ , where $\{a_n\}\in A , \{b_n\}\in B$ , with limit $x$ , we have to show that $x \in A+B$ . Since $B$ is closed-bounded , there is a subsequence $\{b_{r_n}\}$ of $\{b_n\}$ such that $\{b_{r_n}\}$ converges with $\lim \{b_{r_n}\}=l$ (say) ; then since $B$ is closed , $l\in B$. Also $\lim \{a_{r_n}+b_{r_n}\}=x$ , thus $\lim \{a_{r_n}\}=x-l$ , and since $A$ is closed , so $x-l \in A$ , thus $x=(x-l)+l \in A+B$ . Am I correct ?",,"['real-analysis', 'proof-verification']"
71,"In what sense is Lebesgue integral the ""most general""?","In what sense is Lebesgue integral the ""most general""?",,"[ UPDATE : This question is apparently really easy to misinterpret.  I already know about things like the Henstock-Kurzweil integral, etc.  I'm asking if the Lebesgue integral (i.e. the general measure-theoretic integral) can be precisely characterized as ""the most general integral that can be defined naturally on an arbitrary measurable space."" ] Apologies as I don't have that much of a background in real analysis.  These questions may be stupid from the point of view of someone who knows this stuff; if so, just say so. My layman's understanding of various integrals is that the Lebesgue integral is the ""most general"" integral you can define when your domain is an arbitrary measure space, but that if your domain has some extra structure, like for instance if it's $\mathbb{R}^n$, then you can define integrals for a wider class of functions than the measurable ones, e.g. the Henstock-Kurzweil or Khintchine integrals. [ EDIT : To clarify, by the ""Lebesgue integral"" I mean the general measure-theoretic integral defined for arbitrary Borel-measurable functions on arbitrary measure spaces, rather than just the special case defined when the domain is equipped with the Lebesgue measure. ] My question: is there a theorem saying that no sufficiently natural integral defined on arbitrary measure spaces can (1) satisfy the usual conditions and integral should, (2) agree with the Lebesgue measure for measurable functions, and (3) integrate at least one non-measurable function?  Or, conversely, is this false?  Of course this hinges on the correct definition of ""sufficiently natural,"" but I assume it's not too hard to render that statement into abstract nonsense. Such an integral would, I guess, have to somehow ""detect"" sigma algebras looking like those of $\mathbb{R}^n$ and somehow act on this. [ EDIT 2 : To clarify, by an ""integral"" I mean a function that takes as input $((\Omega, \Sigma, \mu), f)$, where $(\Omega, \Sigma, \mu)$ is any measure space and $f : (\Omega, \Sigma) \to (\mathbb{R}, \mathcal{B})$ is a measurable function, and outputs a number, subject to the obvious conditions. ] UPDATE : The following silly example would satisfy all of my criteria except naturality: Let $(\Omega, \Sigma, \mu)$ be a measure space, let $\mathcal{B}$ denote the Borel measure on $\mathbb{R}$, and let $f : (\Omega, \Sigma) \to (\mathbb{R}, \mathcal{B})$ be a Borel-measurable function.  Define $$\int_\Omega f \, d \mu := \begin{cases} \text{the Khintchine integral} & \text{if } (\Omega, \Sigma, \mu) = (\mathbb{R}, \mathcal{L}, \mu_\text{Lebesgue});\\ \text{the Lebesgue integral} & \text{otherwise.} \end{cases}$$","[ UPDATE : This question is apparently really easy to misinterpret.  I already know about things like the Henstock-Kurzweil integral, etc.  I'm asking if the Lebesgue integral (i.e. the general measure-theoretic integral) can be precisely characterized as ""the most general integral that can be defined naturally on an arbitrary measurable space."" ] Apologies as I don't have that much of a background in real analysis.  These questions may be stupid from the point of view of someone who knows this stuff; if so, just say so. My layman's understanding of various integrals is that the Lebesgue integral is the ""most general"" integral you can define when your domain is an arbitrary measure space, but that if your domain has some extra structure, like for instance if it's $\mathbb{R}^n$, then you can define integrals for a wider class of functions than the measurable ones, e.g. the Henstock-Kurzweil or Khintchine integrals. [ EDIT : To clarify, by the ""Lebesgue integral"" I mean the general measure-theoretic integral defined for arbitrary Borel-measurable functions on arbitrary measure spaces, rather than just the special case defined when the domain is equipped with the Lebesgue measure. ] My question: is there a theorem saying that no sufficiently natural integral defined on arbitrary measure spaces can (1) satisfy the usual conditions and integral should, (2) agree with the Lebesgue measure for measurable functions, and (3) integrate at least one non-measurable function?  Or, conversely, is this false?  Of course this hinges on the correct definition of ""sufficiently natural,"" but I assume it's not too hard to render that statement into abstract nonsense. Such an integral would, I guess, have to somehow ""detect"" sigma algebras looking like those of $\mathbb{R}^n$ and somehow act on this. [ EDIT 2 : To clarify, by an ""integral"" I mean a function that takes as input $((\Omega, \Sigma, \mu), f)$, where $(\Omega, \Sigma, \mu)$ is any measure space and $f : (\Omega, \Sigma) \to (\mathbb{R}, \mathcal{B})$ is a measurable function, and outputs a number, subject to the obvious conditions. ] UPDATE : The following silly example would satisfy all of my criteria except naturality: Let $(\Omega, \Sigma, \mu)$ be a measure space, let $\mathcal{B}$ denote the Borel measure on $\mathbb{R}$, and let $f : (\Omega, \Sigma) \to (\mathbb{R}, \mathcal{B})$ be a Borel-measurable function.  Define $$\int_\Omega f \, d \mu := \begin{cases} \text{the Khintchine integral} & \text{if } (\Omega, \Sigma, \mu) = (\mathbb{R}, \mathcal{L}, \mu_\text{Lebesgue});\\ \text{the Lebesgue integral} & \text{otherwise.} \end{cases}$$",,"['real-analysis', 'measure-theory']"
72,Derivative of a determinant,Derivative of a determinant,,"How can I get that? Is it some application of the Chain Rule considering the determinant as a function of the vector function $f$? Notation $d_{i j}$ is the cofactor of $\frac{\partial f_i}{\partial x_j}$ in the Jacobian matrix and $\partial_i$ stands for the partial derivative with respect to $x_i$ The highlighted part is just because I believe that it's a typo!!! An important consequence of this, which i can obtain with a little bit of work is the following: Let $f$ be an infinitely differentiable function of $x_0,x_1,..,x_n$,  $f: \mathbb{R}^{n+1} \rightarrow \mathbb{R}^n$ Let $A_i$ be the matrix with columns the partial derivatives $f_{x_0},..,f_{x_{i-1}},f_{x_{i+1}},..,f_{x_{n}}$ and let, for all $i\not = j$,  $C_{ij}$ be the  matrix with columns $\displaystyle f_{x_i x_j},f_{x_0},..,f_{x_n}$ excluding $f_{x_i}$ and $f_{x_j}$ Then $\displaystyle \frac{\partial{\det(A_i)}}{{\partial x_i}} = \sum_{j<i}(-1)^j \det(C_{ij})+\sum_{j>i}(-1)^{j-1} \det(C_{ij})$","How can I get that? Is it some application of the Chain Rule considering the determinant as a function of the vector function $f$? Notation $d_{i j}$ is the cofactor of $\frac{\partial f_i}{\partial x_j}$ in the Jacobian matrix and $\partial_i$ stands for the partial derivative with respect to $x_i$ The highlighted part is just because I believe that it's a typo!!! An important consequence of this, which i can obtain with a little bit of work is the following: Let $f$ be an infinitely differentiable function of $x_0,x_1,..,x_n$,  $f: \mathbb{R}^{n+1} \rightarrow \mathbb{R}^n$ Let $A_i$ be the matrix with columns the partial derivatives $f_{x_0},..,f_{x_{i-1}},f_{x_{i+1}},..,f_{x_{n}}$ and let, for all $i\not = j$,  $C_{ij}$ be the  matrix with columns $\displaystyle f_{x_i x_j},f_{x_0},..,f_{x_n}$ excluding $f_{x_i}$ and $f_{x_j}$ Then $\displaystyle \frac{\partial{\det(A_i)}}{{\partial x_i}} = \sum_{j<i}(-1)^j \det(C_{ij})+\sum_{j>i}(-1)^{j-1} \det(C_{ij})$",,"['real-analysis', 'multivariable-calculus']"
73,A real differentiable function is convex if and only if its derivative is monotonically increasing,A real differentiable function is convex if and only if its derivative is monotonically increasing,,"I'm working on a problem in baby Rudin, Chapter 5 Exercise 14 reads: Let $f$ be a differentiable real function defined in $(a,b)$. Prove that $f$ is convex if and only if $f'$ is monotonically increasing. I am trying to prove that $f'$ is monotonically increasing under that assumption that $f$ is convex. I have written a proof, but a friend and I do not agree on the validity of my argument. Here is my argument. First, assume that $f$ is convex. Since $f$ is differentiable and real on $(a,b)$, $f$ is continuous on $(a,b)$. So, for $a<s<u<v<t<b$, by the mean value theorem there exist points $y_1\in[s,u]$ and $y_2\in[v,t]$ such that    $$ f'(y_1)=\frac{f(u)-f(s)}{u-s}\quad\text{and}\quad f'(y_2)=\frac{f(t)-f(v)}{t-v}. $$ Then, by exercise 23 of chapter 4 (proven previously):    $$ \frac{f(u)-f(s)}{u-s}\leq\frac{f(t)-f(v)}{t-v} $$   or,   $$ f'(y_1)\leq f'(y_2). $$   Hence, $f'$ is monotonically increasing. $\hspace{3.5in}\square$ My friend claims that this merely proves that for any two arbitrary intervals $[s,u]$ and $[v,t]$ there are points that satisfy $f'(y_1) \leq f'(y_2)$. He says that what I need to prove is that for any two arbitrary points $y_1\leq y_2$ we have $f'(y_1)\leq f'(y_2)$. (I should mention that he doesn't know how one would do this.) Is he right? Is my proof insufficient? If so, how can I fix it?","I'm working on a problem in baby Rudin, Chapter 5 Exercise 14 reads: Let $f$ be a differentiable real function defined in $(a,b)$. Prove that $f$ is convex if and only if $f'$ is monotonically increasing. I am trying to prove that $f'$ is monotonically increasing under that assumption that $f$ is convex. I have written a proof, but a friend and I do not agree on the validity of my argument. Here is my argument. First, assume that $f$ is convex. Since $f$ is differentiable and real on $(a,b)$, $f$ is continuous on $(a,b)$. So, for $a<s<u<v<t<b$, by the mean value theorem there exist points $y_1\in[s,u]$ and $y_2\in[v,t]$ such that    $$ f'(y_1)=\frac{f(u)-f(s)}{u-s}\quad\text{and}\quad f'(y_2)=\frac{f(t)-f(v)}{t-v}. $$ Then, by exercise 23 of chapter 4 (proven previously):    $$ \frac{f(u)-f(s)}{u-s}\leq\frac{f(t)-f(v)}{t-v} $$   or,   $$ f'(y_1)\leq f'(y_2). $$   Hence, $f'$ is monotonically increasing. $\hspace{3.5in}\square$ My friend claims that this merely proves that for any two arbitrary intervals $[s,u]$ and $[v,t]$ there are points that satisfy $f'(y_1) \leq f'(y_2)$. He says that what I need to prove is that for any two arbitrary points $y_1\leq y_2$ we have $f'(y_1)\leq f'(y_2)$. (I should mention that he doesn't know how one would do this.) Is he right? Is my proof insufficient? If so, how can I fix it?",,"['real-analysis', 'convex-analysis']"
74,$\sin(n)$ subsequence limits set,subsequence limits set,\sin(n),"We were given a challenge by our calculus professor and I've been stuck on it for a while now. Show that the set of subsequence limits of $A_n=\sin(n)$ is $[-1, 1]$ (another way to phrase this would be: $\forall r\in [-1,1]$ there exists a subsequence of $A_n=\sin(n)$ that converges to $r$). What would be a good way to start?","We were given a challenge by our calculus professor and I've been stuck on it for a while now. Show that the set of subsequence limits of $A_n=\sin(n)$ is $[-1, 1]$ (another way to phrase this would be: $\forall r\in [-1,1]$ there exists a subsequence of $A_n=\sin(n)$ that converges to $r$). What would be a good way to start?",,"['real-analysis', 'calculus']"
75,$\int_0^{\pi/2} f(t)\sin t dt =\int_{\pi/2}^\pi f(t)\sin t dt < f(\frac{\pi}{2}) \Rightarrow \int_0^\pi \sqrt{f(t)} dt < \pi \sqrt{f(\frac{\pi}{2})}$,,\int_0^{\pi/2} f(t)\sin t dt =\int_{\pi/2}^\pi f(t)\sin t dt < f(\frac{\pi}{2}) \Rightarrow \int_0^\pi \sqrt{f(t)} dt < \pi \sqrt{f(\frac{\pi}{2})},"Assume $f:[0,\pi]\rightarrow (0,+\infty)$ is continuous. And $$ \int_0^{\pi/2} f(t)\sin t dt =\int_{\pi/2}^\pi f(t)\sin t dt \tag{1} $$ if $$ \int_0^{\pi/2} f(t)\sin t dt < f(\frac{\pi}{2}) \tag{2} $$ then, how to show $$ \int_0^\pi \sqrt{f(t)} dt < \pi \sqrt{f(\frac{\pi}{2})}    ~~~~~~? \tag{3} $$ I guess it when I read some paper. I am not sure it is right. And I don't know how to prove it. Seemly, the Holder, Cauchy, Young inequalities are useless for it. Response to mathworker21 (2023-8-25): I calculated a function liking $\delta^{-2}1_{[0,\delta]}+1_{[\frac{\pi}{4},\frac{\pi}{2})}+c1_{\{\frac{\pi}{2}\}}$ . For convenience, let $$ f(t)=\delta^{-2}1_{[0,\delta]}+1_{[\frac{\pi}{4},\frac{\pi}{2}-\epsilon)}+ k(t-\frac{\pi}{2}+\epsilon) 1_{[\frac{\pi}{2}-\epsilon,  \frac{\pi}{2}]} $$ First, I have $$ \int_0^{\pi/2} f(t)\sin tdt = -\delta^{-2}\cos\delta+\delta^{-2}-\sin\epsilon+\frac{\sqrt 2}{2}+k-k\cos\epsilon   \\ f(\frac{\pi}{2}) = k\epsilon $$ So, $\int_0^{\pi/2} f(t)\sin(t)dt < f(\frac{\pi}{2})$ imply $$ k< \frac{ \delta^{-2}\cos\delta  - \delta^{-2}  +\sin\epsilon  -\frac{\sqrt 2}{2} } { 1-\epsilon-\cos\epsilon } \tag{5} $$ On the other hand, we have $$ \int_0^{\pi/2}\sqrt{f(t)}dt = 1+\frac{\pi}{4}-\epsilon+\frac{2}{3k}(k\epsilon)^{3/2} $$ Therefore, $\int_0^{\pi/2}\sqrt{f(t)}dt=\frac{\pi}{2}\sqrt{f(\frac{\pi}{2})}$ imply $$ \sqrt k = \frac{ 1+\frac{\pi}{4}-\epsilon }{ \frac{\pi}{2}\sqrt\epsilon -\frac{2}{3}\epsilon^{3/2} } \tag{6} $$ However, It is not possible to satisfy both conditions (5) and (6) simultaneously. I use Wolfram Mathematica (a software) to get that when $\epsilon=10^{-10},\delta=10^{-10}$ , (5)  imply $$ k<1.207106781146902863457773*10^{10} $$ (6) imply $$ k=1.291904506901873765116087*10^{10} $$ Of course, I also test other numerical value. For example, $\epsilon=10^{-6},\delta=10^{-6}$ or $\epsilon=10^{-10},\delta=10^{-6}$ and so on. But (5) and (6) can't be satisfied simultaneously. Besides, I also calculated $$ f(t)=\delta^{-2}1_{[0,\delta]}+1_{[\frac{\pi}{4},\frac{\pi}{2}-\epsilon)}+ k^2(t-\frac{\pi}{2}+\epsilon)^2 1_{[\frac{\pi}{2}-\epsilon,  \frac{\pi}{2}]} $$ It also is not the counterexample of statement 2. The Mathematica code: Response to mathworker21 (2023-8-22): I find that $$ f(t) = \delta^{-2}1_{[0,\delta]}(t)+1_{[\frac{\pi}{4},\frac{\pi}{2}]}(t) $$ is not the  counterexample  of Statement $2$ : For every continuous $f : [0,\frac{\pi}{2}] \to (0,+\infty)$ satisfying $\int_0^{\pi/2} f(t)\sin(t)dt < f(\frac{\pi}{2})$ , we have $\int_0^{\pi/2} \sqrt{f(t)}dt < \frac{\pi}{2}\sqrt{f(\frac{\pi}{2})}$ . For this $f(t)$ there always be $\int_0^{\pi/2} \sqrt{f(t)}dt < \frac{\pi}{2}\sqrt{f(\frac{\pi}{2})}$ . However, $$ \int_0^{\pi/2} f(t)\sin(t)dt  =-\delta^{-2}\cos\delta +\delta^{-2}+\cos\frac{\pi}{4} ~~~~~~~ f(\frac{\pi}{2})=1 $$ for any $\delta\in (0,\frac{\pi}{4})$ ,  there is $$ -\delta^{-2}\cos\delta +\delta^{-2}+\cos\frac{\pi}{4} >1 $$ Response to mathworker21 (2023-8-11): Could you explain why it is equivalent to showing $$ \left(\frac{1}{\pi/2}\int_0^{\pi/2} \sqrt{f(t)}dt\right)^2 \le \int_0^{\pi/2} f(t)\sin(t)dt   ~~? \tag{4} $$ In fact, I also don't understand Ryszard Szwarc's comment. Besides, I calculate your example, when $\delta>0$ is sufficiently small, it is not false. For $$ f(t) = \delta^{-1/2}1_{[0,\delta]}(t)+1_{[\frac{\pi}{4},\frac{\pi}{2}]}(t) $$ I have $$ \int_0^{\pi/2} \sqrt{f(t)}dt= \int_0^\delta \sqrt{\frac{1}{\sqrt\delta}}dt + \int_{\pi/4}^{\pi/2} dt =\delta^{3/4}+\frac{\pi}{4} $$ Therefore, the left part of (4) is $$ L=\frac{4}{\pi^2} \delta^{3/2} +\frac{2}{\pi}\delta^{3/4}+\frac{1}{4} $$ On the other hand, the right part of (4)  is $$ R= \int_0^{\pi/2} f(t)\sin t dt = \int_0^\delta \frac{1}{\sqrt\delta} \sin t dt  + \int_{\pi/4}^{\pi/2} \sin t dt = \frac{1}{\sqrt\delta}-\frac{1}{\sqrt\delta}\cos\delta+\cos\frac{\pi}{4} $$ And $$ \lim_{\delta\rightarrow 0^+} L =\frac{1}{4} ~~~~~~ \lim_{\delta\rightarrow 0^+} R =\cos\frac{\pi}{4}=\frac{\sqrt 2}{2} $$ So, when $\delta>0$ is sufficiently small, there is $L\le R$ . But, I use Mathematica to get the grapha of $L-R$ , there is $L>R$ when $\delta$ near $\pi/4$ . $x$ is the $\delta$ PS(2023-8-8): From two aspects, I think it is right. First, by the rearrangement inequality, I feel it is right. But it is not general rearrangement, the maximum value of $f$ should be placed near $\frac{\pi}{2}$ in the rearrangement. But I still can't give a detailed proof up to now. On the other hand, I write a program to verify it. I approximate the integral by summation. There is not counter-example. The Python code is as follows: import math import random  i=0 while i<10000:     i=i+1      #生成一个随机函数  (Generates a random function)     f_list=[]     for i0 in range(0,200):         f_list.append(random.random())      #计算左边积分  (Compute the left-hand integral of (1) )     L=0     for i1 in range(0,100):         L=L+f_list[i1]*math.sin(i1*math.pi/200)*(math.pi/200)      #计算右边积分  (Compute the right-hand integral of (1) )     R=0     for i2 in range(100,200):         R=R+f_list[i2]*math.sin(i2*math.pi/200)*(math.pi/200)      #调整两边积分的大小  (Adjust the size of the integral on both sides of (1) )     d=L/R     for i3 in range(100,200):         f_list[i3]=f_list[i3]*d      #判断是否小于f(pi/2)  (Determine if it is less than f(pi/2))     if L>= f_list[100]:         i=i-1         continue   #跳过循环剩下部分  (Skip the rest of the loop)      #计算最后一个式子中左边的积分  (Calculate the integral on the left of the (3) )     LL=0     for i4 in range(0,200):         LL=LL+math.sqrt(f_list[i4])*(math.pi/200)      #如果是反例，就打印出来  (If it's a counterexample, print it out)     if LL >= math.pi*math.sqrt(f_list[100]):         print(LL,math.pi*math.sqrt(f_list[100]),LL-f_list[100])  print(""End"")","Assume is continuous. And if then, how to show I guess it when I read some paper. I am not sure it is right. And I don't know how to prove it. Seemly, the Holder, Cauchy, Young inequalities are useless for it. Response to mathworker21 (2023-8-25): I calculated a function liking . For convenience, let First, I have So, imply On the other hand, we have Therefore, imply However, It is not possible to satisfy both conditions (5) and (6) simultaneously. I use Wolfram Mathematica (a software) to get that when , (5)  imply (6) imply Of course, I also test other numerical value. For example, or and so on. But (5) and (6) can't be satisfied simultaneously. Besides, I also calculated It also is not the counterexample of statement 2. The Mathematica code: Response to mathworker21 (2023-8-22): I find that is not the  counterexample  of Statement : For every continuous satisfying , we have . For this there always be . However, for any ,  there is Response to mathworker21 (2023-8-11): Could you explain why it is equivalent to showing In fact, I also don't understand Ryszard Szwarc's comment. Besides, I calculate your example, when is sufficiently small, it is not false. For I have Therefore, the left part of (4) is On the other hand, the right part of (4)  is And So, when is sufficiently small, there is . But, I use Mathematica to get the grapha of , there is when near . is the PS(2023-8-8): From two aspects, I think it is right. First, by the rearrangement inequality, I feel it is right. But it is not general rearrangement, the maximum value of should be placed near in the rearrangement. But I still can't give a detailed proof up to now. On the other hand, I write a program to verify it. I approximate the integral by summation. There is not counter-example. The Python code is as follows: import math import random  i=0 while i<10000:     i=i+1      #生成一个随机函数  (Generates a random function)     f_list=[]     for i0 in range(0,200):         f_list.append(random.random())      #计算左边积分  (Compute the left-hand integral of (1) )     L=0     for i1 in range(0,100):         L=L+f_list[i1]*math.sin(i1*math.pi/200)*(math.pi/200)      #计算右边积分  (Compute the right-hand integral of (1) )     R=0     for i2 in range(100,200):         R=R+f_list[i2]*math.sin(i2*math.pi/200)*(math.pi/200)      #调整两边积分的大小  (Adjust the size of the integral on both sides of (1) )     d=L/R     for i3 in range(100,200):         f_list[i3]=f_list[i3]*d      #判断是否小于f(pi/2)  (Determine if it is less than f(pi/2))     if L>= f_list[100]:         i=i-1         continue   #跳过循环剩下部分  (Skip the rest of the loop)      #计算最后一个式子中左边的积分  (Calculate the integral on the left of the (3) )     LL=0     for i4 in range(0,200):         LL=LL+math.sqrt(f_list[i4])*(math.pi/200)      #如果是反例，就打印出来  (If it's a counterexample, print it out)     if LL >= math.pi*math.sqrt(f_list[100]):         print(LL,math.pi*math.sqrt(f_list[100]),LL-f_list[100])  print(""End"")","f:[0,\pi]\rightarrow (0,+\infty) 
\int_0^{\pi/2} f(t)\sin t dt =\int_{\pi/2}^\pi f(t)\sin t dt
\tag{1}
 
\int_0^{\pi/2} f(t)\sin t dt < f(\frac{\pi}{2})
\tag{2}
 
\int_0^\pi \sqrt{f(t)} dt < \pi \sqrt{f(\frac{\pi}{2})}    ~~~~~~?
\tag{3}
 \delta^{-2}1_{[0,\delta]}+1_{[\frac{\pi}{4},\frac{\pi}{2})}+c1_{\{\frac{\pi}{2}\}} 
f(t)=\delta^{-2}1_{[0,\delta]}+1_{[\frac{\pi}{4},\frac{\pi}{2}-\epsilon)}+
k(t-\frac{\pi}{2}+\epsilon) 1_{[\frac{\pi}{2}-\epsilon,  \frac{\pi}{2}]}
 
\int_0^{\pi/2} f(t)\sin tdt =
-\delta^{-2}\cos\delta+\delta^{-2}-\sin\epsilon+\frac{\sqrt 2}{2}+k-k\cos\epsilon   \\
f(\frac{\pi}{2}) = k\epsilon
 \int_0^{\pi/2} f(t)\sin(t)dt < f(\frac{\pi}{2}) 
k< \frac{
\delta^{-2}\cos\delta  - \delta^{-2}  +\sin\epsilon  -\frac{\sqrt 2}{2}
}
{
1-\epsilon-\cos\epsilon
}
\tag{5}
 
\int_0^{\pi/2}\sqrt{f(t)}dt =
1+\frac{\pi}{4}-\epsilon+\frac{2}{3k}(k\epsilon)^{3/2}
 \int_0^{\pi/2}\sqrt{f(t)}dt=\frac{\pi}{2}\sqrt{f(\frac{\pi}{2})} 
\sqrt k = \frac{
1+\frac{\pi}{4}-\epsilon
}{
\frac{\pi}{2}\sqrt\epsilon -\frac{2}{3}\epsilon^{3/2}
}
\tag{6}
 \epsilon=10^{-10},\delta=10^{-10} 
k<1.207106781146902863457773*10^{10}
 
k=1.291904506901873765116087*10^{10}
 \epsilon=10^{-6},\delta=10^{-6} \epsilon=10^{-10},\delta=10^{-6} 
f(t)=\delta^{-2}1_{[0,\delta]}+1_{[\frac{\pi}{4},\frac{\pi}{2}-\epsilon)}+
k^2(t-\frac{\pi}{2}+\epsilon)^2 1_{[\frac{\pi}{2}-\epsilon,  \frac{\pi}{2}]}
 
f(t) = \delta^{-2}1_{[0,\delta]}(t)+1_{[\frac{\pi}{4},\frac{\pi}{2}]}(t)
 2 f : [0,\frac{\pi}{2}] \to (0,+\infty) \int_0^{\pi/2} f(t)\sin(t)dt < f(\frac{\pi}{2}) \int_0^{\pi/2} \sqrt{f(t)}dt < \frac{\pi}{2}\sqrt{f(\frac{\pi}{2})} f(t) \int_0^{\pi/2} \sqrt{f(t)}dt < \frac{\pi}{2}\sqrt{f(\frac{\pi}{2})} 
\int_0^{\pi/2} f(t)\sin(t)dt 
=-\delta^{-2}\cos\delta +\delta^{-2}+\cos\frac{\pi}{4}
~~~~~~~
f(\frac{\pi}{2})=1
 \delta\in (0,\frac{\pi}{4}) 
-\delta^{-2}\cos\delta +\delta^{-2}+\cos\frac{\pi}{4} >1
 
\left(\frac{1}{\pi/2}\int_0^{\pi/2} \sqrt{f(t)}dt\right)^2 \le \int_0^{\pi/2} f(t)\sin(t)dt  
~~?
\tag{4}
 \delta>0 
f(t) = \delta^{-1/2}1_{[0,\delta]}(t)+1_{[\frac{\pi}{4},\frac{\pi}{2}]}(t)
 
\int_0^{\pi/2} \sqrt{f(t)}dt=
\int_0^\delta \sqrt{\frac{1}{\sqrt\delta}}dt
+
\int_{\pi/4}^{\pi/2} dt
=\delta^{3/4}+\frac{\pi}{4}
 
L=\frac{4}{\pi^2} \delta^{3/2} +\frac{2}{\pi}\delta^{3/4}+\frac{1}{4}
 
R=
\int_0^{\pi/2} f(t)\sin t dt
=
\int_0^\delta \frac{1}{\sqrt\delta} \sin t dt 
+
\int_{\pi/4}^{\pi/2} \sin t dt
=
\frac{1}{\sqrt\delta}-\frac{1}{\sqrt\delta}\cos\delta+\cos\frac{\pi}{4}
 
\lim_{\delta\rightarrow 0^+} L =\frac{1}{4}
~~~~~~
\lim_{\delta\rightarrow 0^+} R =\cos\frac{\pi}{4}=\frac{\sqrt 2}{2}
 \delta>0 L\le R L-R L>R \delta \pi/4 x \delta f \frac{\pi}{2}","['real-analysis', 'calculus', 'inequality', 'integral-inequality']"
76,Can we show without using calculator that in a unit right triangle $1 \le ab^{b^2/a^2} + ba^{a^2/b^2} < 1 + \frac{7}{180}$?,Can we show without using calculator that in a unit right triangle ?,1 \le ab^{b^2/a^2} + ba^{a^2/b^2} < 1 + \frac{7}{180},"While working on this related inequality , I found the following inequality about right triangles. Let $a^2 + b^2 = 1$ be the two perpendicular sides of a right triangle then, $$ 1 \le ab^{b^2/a^2} + ba^{a^2/b^2} < 1 + \frac{7}{180} $$ I found this inequality interesting because it is pretty tight in the sense that the upper bound is less than $3.89\%$ higher than the lower bound. Wolfram Alpha gives the upper bound as $1.03888$ and a Monte Carlo simulation gives $1.03888238314$ at $a = 0.18733386311638833$ and $b= 0.9822963013927571$ . Hence I have used $1+\frac{7}{180} = 1.03888888...$ as an elegant estimate for the upper bound. Question : The above upper bound is using brute force methods of calculus, What is the best upper bound that can be obtained using Olympiad level tools and standard well known inequalities? Update 1 : The lower bound follows immediately from Young's inequality. Let $a^2+b^2 = c^2$ . Take $p = \frac{c^2}{b^2}$ and $q = \frac{c^2}{a^2}$ in $(3)$ of Young's inequality . Then, Young's inequality says that $$ \frac{b^2}{c^2}a^{c^2/b^2} + \frac{a^2}{c^2}b^{c^2/a^2}  = \frac{b^2}{c^2}a^{1+a^2/b^2} + \frac{a^2}{c^2}b^{1+b^2/a^2}  = \frac{b^2}{c^2}a^{1+a^2/b^2} + \frac{a^2}{c^2}b^{1+b^2/a^2}\ge ab $$ or equivalently, $\displaystyle ab^{b^2/a^2} + ba^{a^2/b^2} \ge c^2$ . Scaling the right triangle to a unit circle, we have $a^2+b^2=c^2 =1$ and the lower bound follows.","While working on this related inequality , I found the following inequality about right triangles. Let be the two perpendicular sides of a right triangle then, I found this inequality interesting because it is pretty tight in the sense that the upper bound is less than higher than the lower bound. Wolfram Alpha gives the upper bound as and a Monte Carlo simulation gives at and . Hence I have used as an elegant estimate for the upper bound. Question : The above upper bound is using brute force methods of calculus, What is the best upper bound that can be obtained using Olympiad level tools and standard well known inequalities? Update 1 : The lower bound follows immediately from Young's inequality. Let . Take and in of Young's inequality . Then, Young's inequality says that or equivalently, . Scaling the right triangle to a unit circle, we have and the lower bound follows.","a^2 + b^2 = 1 
1 \le ab^{b^2/a^2} + ba^{a^2/b^2} < 1 + \frac{7}{180}
 3.89\% 1.03888 1.03888238314 a = 0.18733386311638833 b= 0.9822963013927571 1+\frac{7}{180} = 1.03888888... a^2+b^2 = c^2 p = \frac{c^2}{b^2} q = \frac{c^2}{a^2} (3) 
\frac{b^2}{c^2}a^{c^2/b^2} + \frac{a^2}{c^2}b^{c^2/a^2} 
= \frac{b^2}{c^2}a^{1+a^2/b^2} + \frac{a^2}{c^2}b^{1+b^2/a^2} 
= \frac{b^2}{c^2}a^{1+a^2/b^2} + \frac{a^2}{c^2}b^{1+b^2/a^2}\ge ab
 \displaystyle ab^{b^2/a^2} + ba^{a^2/b^2} \ge c^2 a^2+b^2=c^2 =1","['real-analysis', 'algebra-precalculus', 'analysis', 'inequality']"
77,An entropy inequality,An entropy inequality,,"Denote $H(p) = -p \log_2 p - (1-p)\log_2(1-p)$ [Shannon entropy]. It is well-known that $H$ is a concave function that increases on $(0,\frac{1}{2})$ and decreases on $(\frac{1}{2}, 1)$ . Let $\beta, \beta_1, \beta_2 \in (0,\frac{1}{2})$ be real numbers such that $ H(\beta_1) + H(\beta_2) = 2 H(\beta).$ Let $r$ be a positive real number such that all values $\beta_1 + r, \beta_2 + r,\beta + r$ are less than $\frac{1}{2}$ . How to prove the following inequality? $$ H(\beta_1 + r - 2\beta_1 r) + H(\beta_2 + r - 2\beta_2 r) \ge 2H(\beta + r - 2\beta r)$$ UPD: Why I know that this inequality holds? Because it follows from rather deep result in Kolmogorov complexity theory . But I hope there is a simpler proof of this fact:)",Denote [Shannon entropy]. It is well-known that is a concave function that increases on and decreases on . Let be real numbers such that Let be a positive real number such that all values are less than . How to prove the following inequality? UPD: Why I know that this inequality holds? Because it follows from rather deep result in Kolmogorov complexity theory . But I hope there is a simpler proof of this fact:),"H(p) = -p \log_2 p - (1-p)\log_2(1-p) H (0,\frac{1}{2}) (\frac{1}{2}, 1) \beta, \beta_1, \beta_2 \in (0,\frac{1}{2})  H(\beta_1) + H(\beta_2) = 2 H(\beta). r \beta_1 + r, \beta_2 + r,\beta + r \frac{1}{2}  H(\beta_1 + r - 2\beta_1 r) + H(\beta_2 + r - 2\beta_2 r) \ge 2H(\beta + r - 2\beta r)","['real-analysis', 'convex-analysis', 'information-theory']"
78,Expression for the value of $\int_0^1 x^{1/x}dx$,Expression for the value of,\int_0^1 x^{1/x}dx,"I'm looking to evaluate $$\int_0^1 x^{1/x} dx$$ So far, I have that $$ \int_0^1 x^\frac{1}{x} dx \Rightarrow \int_0^1 e^{\frac{\ln x}{x}} dx \Rightarrow \int_0^1 \sum_{n \geq 0} \frac{\ln^nx}{x^nn!} dx $$ However, I know that $ \int_0^1 \frac{\ln x}{x} dx $ diverges, so I can't see a justification for switching the integral and sum. Furthermore, substituting $x = e^\frac{-u}{n+1}$ yields that the above is equivalent to (assuming my calculations were right) $$ \sum_{n\geq0} \frac{(-1)^n}{(1-n)^{1+n}} $$ which is nonsensical. This thought process can be used to solve the ""sophomore's dream"" integral, which is $\int_0^1 x^{-x} dx = \sum_{n\geq1} n^{-n}$ , to which I have two questions: Why can't the same process be used? What changes between the two expressions? I suspect is has something to do with switching the integral and sum, but being a power series for $e^x$ I believe it should be justifiable. If the above is true, what is another approach to take for this integral? For some context, I have barely cracked into analysis, so if the answer is rudimentary, this is why. However, its very clear that the original integral will have a finite value, and being a cousin of the sophomore's dream, I suspect it could have a solution of the same form.","I'm looking to evaluate So far, I have that However, I know that diverges, so I can't see a justification for switching the integral and sum. Furthermore, substituting yields that the above is equivalent to (assuming my calculations were right) which is nonsensical. This thought process can be used to solve the ""sophomore's dream"" integral, which is , to which I have two questions: Why can't the same process be used? What changes between the two expressions? I suspect is has something to do with switching the integral and sum, but being a power series for I believe it should be justifiable. If the above is true, what is another approach to take for this integral? For some context, I have barely cracked into analysis, so if the answer is rudimentary, this is why. However, its very clear that the original integral will have a finite value, and being a cousin of the sophomore's dream, I suspect it could have a solution of the same form.","\int_0^1 x^{1/x} dx 
\int_0^1 x^\frac{1}{x} dx \Rightarrow \int_0^1 e^{\frac{\ln x}{x}} dx \Rightarrow \int_0^1 \sum_{n \geq 0} \frac{\ln^nx}{x^nn!} dx
 
\int_0^1 \frac{\ln x}{x} dx
 x = e^\frac{-u}{n+1} 
\sum_{n\geq0} \frac{(-1)^n}{(1-n)^{1+n}}
 \int_0^1 x^{-x} dx = \sum_{n\geq1} n^{-n} e^x","['real-analysis', 'integration']"
79,Functions between polynomial and exponential order,Functions between polynomial and exponential order,,"I'm wondering if there exists a strictly increasing function $f:[0, \infty) \rightarrow \mathbb{R}_+$ which increases faster than any polynomial in the sense that $$ \limsup_{x \rightarrow \infty} \frac{f(x)}{x^k} = \infty \; \; \; \; \; \forall \; \; k > 0  $$ but there exists some $\zeta \in ( 0,1)  $ such that $$ \limsup_{x \rightarrow \infty} \frac{f(x)}{f(\zeta x)} < \infty $$ I tried several obvious cases such as exponentials and functions inbetween polynomial and exponential but had no luck. Does anyone know how to construct such a function or to show it does not exist?",I'm wondering if there exists a strictly increasing function which increases faster than any polynomial in the sense that but there exists some such that I tried several obvious cases such as exponentials and functions inbetween polynomial and exponential but had no luck. Does anyone know how to construct such a function or to show it does not exist?,"f:[0, \infty) \rightarrow \mathbb{R}_+  \limsup_{x \rightarrow \infty} \frac{f(x)}{x^k} = \infty \; \; \; \; \; \forall \; \; k > 0   \zeta \in ( 0,1)    \limsup_{x \rightarrow \infty} \frac{f(x)}{f(\zeta x)} < \infty ","['real-analysis', 'calculus', 'polynomials']"
80,Covering a compact set with balls whose centers do not belong to other balls.,Covering a compact set with balls whose centers do not belong to other balls.,,"Let $K\subset \Bbb R^n$ be a compact set such that each $x\in K$ is associated with a positive number $r_x>0$ . Claim : $K$ can be covered by a family of balls $$ \mathcal B = \{ B(x_i,r_i) : i=1,\dots,k\ \}, $$ where $r_i := r_{x_i}$ , such that for any distinct $i,j \le k$ , we have $$ x_i\notin B(x_j,r_j) \quad\text{and}\quad x_j\notin B(x_i,r_i). $$ Is the claim true without any additional assumptions on $r_x$ 's? At first I thought of using Zorn's lemma to extract a maximal subfamily of balls from $\mathcal F = \{B(x,r_x/2) : x\in K \}$ such that any pair of balls is disjoint. However, enlarging the radii by a factor of $2$ may not be a cover of $K$ so this approach may not work.","Let be a compact set such that each is associated with a positive number . Claim : can be covered by a family of balls where , such that for any distinct , we have Is the claim true without any additional assumptions on 's? At first I thought of using Zorn's lemma to extract a maximal subfamily of balls from such that any pair of balls is disjoint. However, enlarging the radii by a factor of may not be a cover of so this approach may not work.","K\subset \Bbb R^n x\in K r_x>0 K 
\mathcal B = \{ B(x_i,r_i) : i=1,\dots,k\ \},
 r_i := r_{x_i} i,j \le k 
x_i\notin B(x_j,r_j) \quad\text{and}\quad x_j\notin B(x_i,r_i).
 r_x \mathcal F = \{B(x,r_x/2) : x\in K \} 2 K","['real-analysis', 'general-topology', 'measure-theory', 'compactness']"
81,What is this function related with continued fractions?,What is this function related with continued fractions?,,"Playing with continued fractions, I came with the idea of iterating the limit of the simplest one: $$1 + \cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+\cdots}}}}}\ = \Phi$$ And then I thoght about iterating the result: $$\Phi + \cfrac{1}{\Phi+\cfrac{1}{\Phi+\cfrac{1}{\Phi+\cfrac{1}{\Phi+\cfrac{1}{\Phi+\cdots}}}}}\ = ?$$ And after that keep iterating again and again. Essentially, what I am doing is solving this: $$x = a + \frac{1}{x}$$ for different values of $a$. Whose solution is: $$ x = \frac{a \pm \sqrt{a^2 + 4}}{2}$$ And I'm just staying with the positive solution. At the end, I'm just exploring this sequence defined recursively: $$ f(1) = \Phi \\ f(n+1) = \frac{f(n) + \sqrt{f(n)^2 + 4}}{2} $$ When you see the plot of the terms of the sequence, you get this picture: I would like to know which is the function that generates that curve . It really seems to be a continuos function (notice that the picture is a set of points so close, that may seem to be continuous, but it's not). So... What's the function underlying this curve and how can we find it? Many thanks in advance!","Playing with continued fractions, I came with the idea of iterating the limit of the simplest one: $$1 + \cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+\cdots}}}}}\ = \Phi$$ And then I thoght about iterating the result: $$\Phi + \cfrac{1}{\Phi+\cfrac{1}{\Phi+\cfrac{1}{\Phi+\cfrac{1}{\Phi+\cfrac{1}{\Phi+\cdots}}}}}\ = ?$$ And after that keep iterating again and again. Essentially, what I am doing is solving this: $$x = a + \frac{1}{x}$$ for different values of $a$. Whose solution is: $$ x = \frac{a \pm \sqrt{a^2 + 4}}{2}$$ And I'm just staying with the positive solution. At the end, I'm just exploring this sequence defined recursively: $$ f(1) = \Phi \\ f(n+1) = \frac{f(n) + \sqrt{f(n)^2 + 4}}{2} $$ When you see the plot of the terms of the sequence, you get this picture: I would like to know which is the function that generates that curve . It really seems to be a continuos function (notice that the picture is a set of points so close, that may seem to be continuous, but it's not). So... What's the function underlying this curve and how can we find it? Many thanks in advance!",,"['real-analysis', 'sequences-and-series', 'recurrence-relations', 'continued-fractions', 'golden-ratio']"
82,"nice inequality $\sum\limits_{i=1}^{n}\max({a_{1},\cdots,a_{i}})\min{(a_{i},\cdots,a_{n}})\le\frac{n}{2\sqrt{n-1}}\sum\limits_{i=1}^{n}a^2_{i}$",nice inequality,"\sum\limits_{i=1}^{n}\max({a_{1},\cdots,a_{i}})\min{(a_{i},\cdots,a_{n}})\le\frac{n}{2\sqrt{n-1}}\sum\limits_{i=1}^{n}a^2_{i}","Let $n\ge 2$ be a positive integer, and $a_{i}>0,i=1,2,\cdots,n$ . Show that $$\sum_{i=1}^{n}\max({a_{1},a_{2},\cdots,a_{i}})\min{(a_{i},a_{i+1},\cdots,a_{n}})\le\dfrac{n}{2\sqrt{n-1}}\sum_{i=1}^{n}a^2_{i}.$$ maybe use Cauchy-Schwarz inequality,because this form $$n\sum_{i=1}^{n}a^2_{i}\ge(a_{1}+a_{2}+\cdots+a_{n})^2$$","Let be a positive integer, and . Show that maybe use Cauchy-Schwarz inequality,because this form","n\ge 2 a_{i}>0,i=1,2,\cdots,n \sum_{i=1}^{n}\max({a_{1},a_{2},\cdots,a_{i}})\min{(a_{i},a_{i+1},\cdots,a_{n}})\le\dfrac{n}{2\sqrt{n-1}}\sum_{i=1}^{n}a^2_{i}. n\sum_{i=1}^{n}a^2_{i}\ge(a_{1}+a_{2}+\cdots+a_{n})^2","['real-analysis', 'inequality']"
83,Which sets of natural numbers generate fractions which are dense in $\mathbb{R}_{+}$?,Which sets of natural numbers generate fractions which are dense in ?,\mathbb{R}_{+},"This is an extension of this recent question. We assume $0 \notin \mathbb{N}$. Let's say a subset $S \subseteq\mathbb{N}$ is quotient-dense if $$\overline{\left\{\frac{p}{q} :p,q \in S\right\}} = \mathbb{R}_{\geq 0}$$ Can we characterize the quotient-dense subsets of the natural numbers? Some obvious examples are $\mathbb{N}$ itself, and subsets of the form $\{an+b:n \in \mathbb{N}\}$ for fixed natural $a,b$. $S$ must obviously be countably infinite. However, this is not a sufficient condition. If we consider the set $X$ of all powers of two, then $\frac pq = 2^k$ for some $k \in \mathbb{Z}$. The closure of the set is certainly not $\mathbb{R}_{\geq 0}$ The answers in the linked question above show that the set of primes is quotient-dense. One guess is that $S$ is quotient-dense iff $$\sum_{k \in S} \frac{1}{k} = +\infty$$ EDIT: The guess above is wrong, as shown in the comments.","This is an extension of this recent question. We assume $0 \notin \mathbb{N}$. Let's say a subset $S \subseteq\mathbb{N}$ is quotient-dense if $$\overline{\left\{\frac{p}{q} :p,q \in S\right\}} = \mathbb{R}_{\geq 0}$$ Can we characterize the quotient-dense subsets of the natural numbers? Some obvious examples are $\mathbb{N}$ itself, and subsets of the form $\{an+b:n \in \mathbb{N}\}$ for fixed natural $a,b$. $S$ must obviously be countably infinite. However, this is not a sufficient condition. If we consider the set $X$ of all powers of two, then $\frac pq = 2^k$ for some $k \in \mathbb{Z}$. The closure of the set is certainly not $\mathbb{R}_{\geq 0}$ The answers in the linked question above show that the set of primes is quotient-dense. One guess is that $S$ is quotient-dense iff $$\sum_{k \in S} \frac{1}{k} = +\infty$$ EDIT: The guess above is wrong, as shown in the comments.",,"['real-analysis', 'rational-numbers']"
84,$f(nx)\to 0$ as $n\to+\infty$,as,f(nx)\to 0 n\to+\infty,"Let $f:\mathbb R^+\to\mathbb R^+$ be a continuous function, and let $I$ be a subset of $\mathbb R^+$ such that the following property holds: For any $x\in I$, $f(nx)\to 0$ as $n\to+\infty$. Intuitively, if $I$ is 'big' enough, $f$ necessarily tends to $0$ at infinity, but it happens to not always be the case. I am investigating whether it can be said, for various $I$, if $f(x)\to 0$ as $x\to+\infty$. As a first example, consider $I=[0,1]$. Set $\varepsilon>0$, and consider the closed sets $F_n=\{x\in I\mid f(kx)<\varepsilon,\forall k\geq n\}$. Their union is $[0,1]$, and thus, thanks to the property of Baire, one of them has non-empty interior, i.e. $[a,b]\in F_n$ for a certain $n$. It follows that for all $k\geq n$ and $x\in[ka,kb]$, $f(x)<\varepsilon$. But since $b>a$, $\bigcup_{k=n}^\infty [ka,kb]$ contains a half-line, and $\limsup_{x\to\infty} f(x) \leq \varepsilon$. Since the reasoning holds for any $\varepsilon>0$, we conclude that $f$ does, indeed, tend to $0$ at infinity. The same proof actually works for all $I$ with non-empty interior. The result is false in higher dimensions when $I$ contains a neighbourhood of the origin, as a simple counter-example can be constructed using a parabola. What happens when $I\subset \mathbb R$ is smaller? Consider the following sets: $I$ is any measurable set with empty interior, but with Lebesgue measure >0. In that case, one of the aforementioned $F_n$ has positive Lebesgue measure. $I=(0,1)\cap C$, where $C$ is the Cantor set (or another uncountable set). Then, one of the $F_n$ has to be uncountable aswell, $I=\{1/k,k\in\mathbb N\}$ or $I=(0,1)\cap \mathbb Q$, both of these being equivalent. I provided an answer below for this one, and actually for any countable set; such a function $f$ does not necessarily converge to $0$. In any of these cases, can anything be said about the behaviour of $f$ at infinity? Bonus question: what is the minimum condition for $I$ (if there is one) so that $f$ has to converge to $0$? I thought about mimicking the proof of the first example when $I$ has empty interior but positive Lebesgue measure. If there exists an integer $n$ and a non-trivial interval $[a,b]$ such that $|F_n\cap [a,b]|=b-a$, then $f\leq\varepsilon$ on a set that is dense on a half-line, and thus, using continuity, tends to $0$. Sadly, such an interval may not exist, since the closure of $F_n$ could very well be of empty interior, like a generalized Cantor set.","Let $f:\mathbb R^+\to\mathbb R^+$ be a continuous function, and let $I$ be a subset of $\mathbb R^+$ such that the following property holds: For any $x\in I$, $f(nx)\to 0$ as $n\to+\infty$. Intuitively, if $I$ is 'big' enough, $f$ necessarily tends to $0$ at infinity, but it happens to not always be the case. I am investigating whether it can be said, for various $I$, if $f(x)\to 0$ as $x\to+\infty$. As a first example, consider $I=[0,1]$. Set $\varepsilon>0$, and consider the closed sets $F_n=\{x\in I\mid f(kx)<\varepsilon,\forall k\geq n\}$. Their union is $[0,1]$, and thus, thanks to the property of Baire, one of them has non-empty interior, i.e. $[a,b]\in F_n$ for a certain $n$. It follows that for all $k\geq n$ and $x\in[ka,kb]$, $f(x)<\varepsilon$. But since $b>a$, $\bigcup_{k=n}^\infty [ka,kb]$ contains a half-line, and $\limsup_{x\to\infty} f(x) \leq \varepsilon$. Since the reasoning holds for any $\varepsilon>0$, we conclude that $f$ does, indeed, tend to $0$ at infinity. The same proof actually works for all $I$ with non-empty interior. The result is false in higher dimensions when $I$ contains a neighbourhood of the origin, as a simple counter-example can be constructed using a parabola. What happens when $I\subset \mathbb R$ is smaller? Consider the following sets: $I$ is any measurable set with empty interior, but with Lebesgue measure >0. In that case, one of the aforementioned $F_n$ has positive Lebesgue measure. $I=(0,1)\cap C$, where $C$ is the Cantor set (or another uncountable set). Then, one of the $F_n$ has to be uncountable aswell, $I=\{1/k,k\in\mathbb N\}$ or $I=(0,1)\cap \mathbb Q$, both of these being equivalent. I provided an answer below for this one, and actually for any countable set; such a function $f$ does not necessarily converge to $0$. In any of these cases, can anything be said about the behaviour of $f$ at infinity? Bonus question: what is the minimum condition for $I$ (if there is one) so that $f$ has to converge to $0$? I thought about mimicking the proof of the first example when $I$ has empty interior but positive Lebesgue measure. If there exists an integer $n$ and a non-trivial interval $[a,b]$ such that $|F_n\cap [a,b]|=b-a$, then $f\leq\varepsilon$ on a set that is dense on a half-line, and thus, using continuity, tends to $0$. Sadly, such an interval may not exist, since the closure of $F_n$ could very well be of empty interior, like a generalized Cantor set.",,"['real-analysis', 'limits', 'functions']"
85,Group Structure on $\Bbb R$,Group Structure on,\Bbb R,"$(\Bbb R,+)$ is a topological group. Is there any other group structure on $\Bbb R$ such that it is still a topological group and this group is not isomorphic to $(\Bbb R,+)$ ? Refer to Non-isomorphic Group Structures on a Topological Group for the general problem. In that problem I have conjectured that there is no such group however I couldn't prove it.","$(\Bbb R,+)$ is a topological group. Is there any other group structure on $\Bbb R$ such that it is still a topological group and this group is not isomorphic to $(\Bbb R,+)$ ? Refer to Non-isomorphic Group Structures on a Topological Group for the general problem. In that problem I have conjectured that there is no such group however I couldn't prove it.",,"['real-analysis', 'general-topology', 'topological-groups', 'real-numbers']"
86,$f(x)=x$ if $x$ irrational and $f(x)=p\sin\frac1q$ if $x$ rational,if  irrational and  if  rational,f(x)=x x f(x)=p\sin\frac1q x,"Define the real-valued function $f$ on $\mathbb{R}$ by setting $f(x)=x$ if $x$ is irrational, and $f(x)=p\sin\frac1q$ if $x=\frac{p}q$ is written in lowest terms. At what points is $f$ continuous? I'm pretty sure $f$ is continuous at no point. For any point $x$, we can find $\frac{p}q$ that's really close to $x$, but such that the value $p\sin\frac1q$ is nowhere near $x$. For example, if $x=0$, we have $f(x)=0$, and we can choose $y$ as something like $\frac{34567}{10000000000}$ so that its value is really close to $x$, but $\frac{p}{q}$ is unlikely to be close to $x$. But since the value of $\sin\frac{1}{q}$ fluctuates, I'm unable to turn this into a rigorous proof.","Define the real-valued function $f$ on $\mathbb{R}$ by setting $f(x)=x$ if $x$ is irrational, and $f(x)=p\sin\frac1q$ if $x=\frac{p}q$ is written in lowest terms. At what points is $f$ continuous? I'm pretty sure $f$ is continuous at no point. For any point $x$, we can find $\frac{p}q$ that's really close to $x$, but such that the value $p\sin\frac1q$ is nowhere near $x$. For example, if $x=0$, we have $f(x)=0$, and we can choose $y$ as something like $\frac{34567}{10000000000}$ so that its value is really close to $x$, but $\frac{p}{q}$ is unlikely to be close to $x$. But since the value of $\sin\frac{1}{q}$ fluctuates, I'm unable to turn this into a rigorous proof.",,"['real-analysis', 'continuity']"
87,On Vanishing Riemann Sums and Odd Functions,On Vanishing Riemann Sums and Odd Functions,,"Let $ f: [-1,1] \to \mathbb{R} $ be a continuous function. Suppose that the $ n $-th midpoint Riemann sum of $ f $ vanishes for all $ n \in \mathbb{N} $. In other words,   $$ \forall n \in \mathbb{N}: \quad \mathcal{R}^{f}_{n} := \sum_{k=1}^{n} f \left( -1 + \frac{2k - 1}{n} \right) \cdot \frac{2}{n} = 0. $$ Question: Is it necessarily true that $ f $ is an odd function? It is easy to verify that if $ f $ is an odd continuous function, then $ \mathcal{R}^{f}_{n} = 0 $ for all $ n \in \mathbb{N} $. However, is the converse true? This is part of an original research problem, so unfortunately, there is no other source except myself. With someone else, I managed to obtain the following partial result. Theorem If $ f $ is a polynomial function and $ \mathcal{R}^{f}_{n} = 0 $ for all $ n \in \mathbb{N} $, then $ f $ has only odd powers, which immediately implies that $ f $ is an odd function. The proof relies on properties of Bernoulli polynomials and Vandermonde matrices. For the general case, I was thinking that Fourier-analytic tools might help, such as Poisson summation. A Fourier-analytic approach seems promising, but it has limitations and might not be able to fully resolve the question. Would anyone care to offer some insight into the problem? Thanks!","Let $ f: [-1,1] \to \mathbb{R} $ be a continuous function. Suppose that the $ n $-th midpoint Riemann sum of $ f $ vanishes for all $ n \in \mathbb{N} $. In other words,   $$ \forall n \in \mathbb{N}: \quad \mathcal{R}^{f}_{n} := \sum_{k=1}^{n} f \left( -1 + \frac{2k - 1}{n} \right) \cdot \frac{2}{n} = 0. $$ Question: Is it necessarily true that $ f $ is an odd function? It is easy to verify that if $ f $ is an odd continuous function, then $ \mathcal{R}^{f}_{n} = 0 $ for all $ n \in \mathbb{N} $. However, is the converse true? This is part of an original research problem, so unfortunately, there is no other source except myself. With someone else, I managed to obtain the following partial result. Theorem If $ f $ is a polynomial function and $ \mathcal{R}^{f}_{n} = 0 $ for all $ n \in \mathbb{N} $, then $ f $ has only odd powers, which immediately implies that $ f $ is an odd function. The proof relies on properties of Bernoulli polynomials and Vandermonde matrices. For the general case, I was thinking that Fourier-analytic tools might help, such as Poisson summation. A Fourier-analytic approach seems promising, but it has limitations and might not be able to fully resolve the question. Would anyone care to offer some insight into the problem? Thanks!",,"['calculus', 'real-analysis', 'functions', 'fourier-analysis']"
88,Finite number of zeros?,Finite number of zeros?,,"Is it true that any nonzero function $f: \mathbb R \to \mathbb R$ which is either: 1) constant, or 2) a polynomial, or 3) $\exp$, or 4) $\log$, or 5) any finite combination of the above using addition, subtraction, multiplication, division and composition, (and individually considered as functions from $\mathbb R$ to $\mathbb R$), Has a finite number of zeros? Another attempt. I'm actually interested in asymptotic behavior of functions, so a function f(x), as far as I'm concerned, is any expression constructed using the syntax below, such that starting from some point c>0, f(x) is defined for all x>c and takes real values everywhere in this range. A function has the following syntax: F --> real number F --> exp F --> ln F --> -F F --> F + F F --> F(F) Hope this specifies exactly what I mean and excludes everything I DON'T. Any help is appreciated.","Is it true that any nonzero function $f: \mathbb R \to \mathbb R$ which is either: 1) constant, or 2) a polynomial, or 3) $\exp$, or 4) $\log$, or 5) any finite combination of the above using addition, subtraction, multiplication, division and composition, (and individually considered as functions from $\mathbb R$ to $\mathbb R$), Has a finite number of zeros? Another attempt. I'm actually interested in asymptotic behavior of functions, so a function f(x), as far as I'm concerned, is any expression constructed using the syntax below, such that starting from some point c>0, f(x) is defined for all x>c and takes real values everywhere in this range. A function has the following syntax: F --> real number F --> exp F --> ln F --> -F F --> F + F F --> F(F) Hope this specifies exactly what I mean and excludes everything I DON'T. Any help is appreciated.",,['real-analysis']
89,Interesting Identity: $\lim_{m\to\infty}\lim_{n\to\infty}\prod_{k=1}^{\infty}\prod_{j=2}^{m}(1+\phi_{n})^{nk^{-j}\Phi_{j}}=e^{\gamma}$,Interesting Identity:,\lim_{m\to\infty}\lim_{n\to\infty}\prod_{k=1}^{\infty}\prod_{j=2}^{m}(1+\phi_{n})^{nk^{-j}\Phi_{j}}=e^{\gamma},Define the following identities: $$ \phi_{n}:=\frac{4}{\pi}\int_{0}^{\infty}\frac{\coth(nx^{-1})-xn^{-1}}{n(1+x^{2})^{2}}\;dx \qquad \text{and}\qquad  \Phi_{n}:=\frac{\cos(n\pi)}{n} $$ $\forall n\in\mathbb{N}$ . I want to show that : $$ \lim_{m\to\infty}\lim_{n\to\infty}\prod_{k=1}^{\infty}\prod_{j=2}^{m}(1+\phi_{n})^{ nk^{-j}\Phi_{j}}=e^{\gamma} $$ Where $\gamma$ is the Euler - Mascheroni constant. What I did is that I used the linearity of integrals for $\phi_{n}$ $$ \frac{\pi}{4}\phi_{n}=\int_{0}^{\infty}\frac{\coth\left(\frac{n}{x}\right)}{n(1+x^{2})^{2}}\;dx-\int_{0}^{\infty}\frac{x}{n^{2}(1+x^{2})^{2}}\;dx $$ The first one I define it as $I_{1}$ and the second one I define it as $I_{2}$ . Afterwards I defined: $$ \lambda_{n}(x):=\frac{\coth\left(\frac{n}{x}\right)}{n(1+x^{2})^{2}}\leq\left|\frac{1}{n(1+x^{2})^{2}}\right|\leq\left|\frac{1}{2n(1+x^{2})}\right|=:V_{n}(x) $$ so that: $$ \lim_{n\to\infty}\int_{0}^{\infty}V_{n}(x)\;dx=\lim_{n\to\infty}\frac{\pi}{4n}=0 $$ Applying the Lebesgue Dominated Convergence Theorem I get: $$ \lim_{n\to\infty}\int_{0}^{\infty}\lambda_{n}(x)\;dx=0 \tag 1$$ Now note that: $$ I_{2}=\int_{0}^{\infty}\frac{x}{n^{2}(1+x^{2})}\;dx=\frac{1}{2n^{2}}\quad\implies\quad\lim_{n\to\infty}I_{2}=0 \tag 2$$ $(1)$ and $(2)$ verify that $\lim_{n\to\infty}(1+\phi_{n})^{n}\leq\lim_{n\to\infty}(1+I_{1}-\displaystyle I_{2})^{n}=\lim_{n\to\infty}\left(1+\frac{1}{n}-\frac{2}{n\pi^{2}}\right)^{n}=e$ Now all I have to evaluate $\displaystyle\exp\left(\lim_{m\to\infty}\sum_{k=1}^{\infty}\sum_{j=2}^{\infty}\frac{\cos(j\pi)}{k^{j}j}\right)$ but this seems a bit confusing and misleading when I try it which is what I am stuck on.,Define the following identities: . I want to show that : Where is the Euler - Mascheroni constant. What I did is that I used the linearity of integrals for The first one I define it as and the second one I define it as . Afterwards I defined: so that: Applying the Lebesgue Dominated Convergence Theorem I get: Now note that: and verify that Now all I have to evaluate but this seems a bit confusing and misleading when I try it which is what I am stuck on.,"
\phi_{n}:=\frac{4}{\pi}\int_{0}^{\infty}\frac{\coth(nx^{-1})-xn^{-1}}{n(1+x^{2})^{2}}\;dx \qquad
\text{and}\qquad 
\Phi_{n}:=\frac{\cos(n\pi)}{n}
 \forall n\in\mathbb{N} 
\lim_{m\to\infty}\lim_{n\to\infty}\prod_{k=1}^{\infty}\prod_{j=2}^{m}(1+\phi_{n})^{ nk^{-j}\Phi_{j}}=e^{\gamma}
 \gamma \phi_{n} 
\frac{\pi}{4}\phi_{n}=\int_{0}^{\infty}\frac{\coth\left(\frac{n}{x}\right)}{n(1+x^{2})^{2}}\;dx-\int_{0}^{\infty}\frac{x}{n^{2}(1+x^{2})^{2}}\;dx
 I_{1} I_{2} 
\lambda_{n}(x):=\frac{\coth\left(\frac{n}{x}\right)}{n(1+x^{2})^{2}}\leq\left|\frac{1}{n(1+x^{2})^{2}}\right|\leq\left|\frac{1}{2n(1+x^{2})}\right|=:V_{n}(x)
 
\lim_{n\to\infty}\int_{0}^{\infty}V_{n}(x)\;dx=\lim_{n\to\infty}\frac{\pi}{4n}=0
 
\lim_{n\to\infty}\int_{0}^{\infty}\lambda_{n}(x)\;dx=0
\tag 1 
I_{2}=\int_{0}^{\infty}\frac{x}{n^{2}(1+x^{2})}\;dx=\frac{1}{2n^{2}}\quad\implies\quad\lim_{n\to\infty}I_{2}=0
\tag 2 (1) (2) \lim_{n\to\infty}(1+\phi_{n})^{n}\leq\lim_{n\to\infty}(1+I_{1}-\displaystyle I_{2})^{n}=\lim_{n\to\infty}\left(1+\frac{1}{n}-\frac{2}{n\pi^{2}}\right)^{n}=e \displaystyle\exp\left(\lim_{m\to\infty}\sum_{k=1}^{\infty}\sum_{j=2}^{\infty}\frac{\cos(j\pi)}{k^{j}j}\right)","['real-analysis', 'calculus', 'limits']"
90,What uniquely characterizes the germ of a smooth function?,What uniquely characterizes the germ of a smooth function?,,"Let $X$ be the set of all functions $f:\mathbb{R}\rightarrow\mathbb{R}$ which are infinitely differentiable at $0$.  Let us define an equivalence relation $\sim$ on $X$ by saying that $f\sim g$ if there exists a $\delta>0$ such that $f(x)=g(x)$ for all points in the interval $(-\delta,\delta)$. And let $Y$ be the set of equivalence classes of elements of $X$ under $\sim$. My question is, what characterizes a given equivalence class in $Y$?  The values  of $f(0)$ and $f^{(n)}(0)$ for all $n$ aren't enough, because for any given analytic function $f$ there exists an non-analytic infinitely differentiable function $g$ such that $f(0)=g(0)$ and $f^{(n)}=g^{(n)}$ for all $n$ but where it's not the case that $f\sim g$. So what is the minimum information needed to unambiguously specify an element of $Y$? EDIT: It turns out my concept is an existing mathematical concept, known as a germ .  So my question reduces to, what information uniquely characterizes the germ of a smooth function?","Let $X$ be the set of all functions $f:\mathbb{R}\rightarrow\mathbb{R}$ which are infinitely differentiable at $0$.  Let us define an equivalence relation $\sim$ on $X$ by saying that $f\sim g$ if there exists a $\delta>0$ such that $f(x)=g(x)$ for all points in the interval $(-\delta,\delta)$. And let $Y$ be the set of equivalence classes of elements of $X$ under $\sim$. My question is, what characterizes a given equivalence class in $Y$?  The values  of $f(0)$ and $f^{(n)}(0)$ for all $n$ aren't enough, because for any given analytic function $f$ there exists an non-analytic infinitely differentiable function $g$ such that $f(0)=g(0)$ and $f^{(n)}=g^{(n)}$ for all $n$ but where it's not the case that $f\sim g$. So what is the minimum information needed to unambiguously specify an element of $Y$? EDIT: It turns out my concept is an existing mathematical concept, known as a germ .  So my question reduces to, what information uniquely characterizes the germ of a smooth function?",,"['calculus', 'real-analysis', 'taylor-expansion', 'analyticity', 'analytic-functions']"
91,"Characterization of $C^{k,\alpha}$ (functions with Hölder continuous derivatives) through Taylor estimates",Characterization of  (functions with Hölder continuous derivatives) through Taylor estimates,"C^{k,\alpha}","For $k \in \Bbb{N} = \{1,2,3,\dots\}$ and $\alpha \in (0,1)$, let us define $$ C^{k,\alpha} := \{ f : \Bbb{R} \to \Bbb{R} \,:\, f \in C^k \text{ with } f, f', \dots, f^{(k)} \text{ bounded and } [f^{(k)}]_{C^\alpha} < \infty \}, $$ where $$ [g]_{C^\alpha} := \sup_{x,y \in \Bbb{R}, x \neq y} \frac{|g(x)-g(y)|}{|x-y|^\alpha} \, . $$ It is not hard to see by using Taylor's formula for $f \in C^{k,\alpha}$ that for each $x \in \Bbb{R}$, there is a polynomial $P_x$ of degree $k$ (namely, the Taylor polynomial of $f$) satisfying $$ |f(x+h) - P_x (h)| \leq C \cdot |h|^{k+\alpha} \qquad \forall h \in \Bbb{R} \text{ with } |h| \leq 1, \tag{$\circledast$} $$ with a constant $C$ independent of $x,h$ (but depending on $f$). It is claimed in Exercise 13.31 of certain notes by Martin Hairer that also the converse holds, i.e., if $f : \Bbb{R} \to \Bbb{R}$ and if there is a constant $C>0$, such that for each $x$, there is a polynomial $P_x$ of degree at most $k$ such that $(\circledast)$ holds, then $f \in C^{k,\alpha}$. I would like to know a solution to this exercise, and even the case $k=1$ would be interesting to me. To simplify the question a bit, let us assume that we already know $f$ to be $k$-times differentiable, with $f,f', \dots, f^{(k)}$ being bounded (I think this boundedness does not actually follow ""automatically""), so we only need to establish $\alpha$-Hölder continuity of $f^{(k)}$. After the fold, I discuss some of my ideas towards this problem. It might be better, however, to at first not read these ideas, since given that this is an exercise, they seem overly complicated (they refer to Campanato spaces, which are probably not known to everyone doing the exercise). Thus it might be better to start afresh instead of getting spoiled by my stupid thoughts :) My thoughts only apply to the case $k=1$. Here, one can more or less easily show that necessarily $P_x (h) = f(x) + f'(x) h$. Then we get \begin{align*} & |f(x+h) - f(x-h) - 2h \cdot f'(x)| \\ & = |f(x+h) - f(x) - f'(x) h - (f(x-h) - f(x) - f'(x) \cdot (-h))| \\ & \leq 2C |h|^{1+\alpha} \end{align*} for $|h| \leq 1$. But using the fundamental theorem of calculus to write $f(x+h) - f(x-h) = \int_{x-h}^{x+h} f'(s) ds$, and writing $g = f'$, we thus get $$ \left| - \int_{x-h}^{x+h} g(s) - g(x) ds \right| \leq 2C \cdot |h|^{1+\alpha}. \tag{$\dagger$} $$ Now, it is known (see Lemma 0.0.15 in Some notes on Campanato spaces ) that the norm $$ \| g \|_\ast = \|g\|_{L^1} + \sup_{x \in \Bbb{R}, 0 < h \leq C} \,\, \inf_{c \in \Bbb{R}} \,\, h^{-(1 + \alpha)} \int_{x-h}^{x+h} |g(t) - c| \, dt $$ is an equivalent norm for the Campanato space $L^{1,1+\alpha}_C (\Bbb{R})$. Strictly speaking, I am cheating a bit here, since $\Bbb{R}$ is not a bounded domain. Thus, if in $(\dagger)$ above, we would have the absolute value inside the integral instead of outside, we would get a Campanato condition on the function $g = f'$. But it is known (see Wikipedia, or Theorem 0.0.22 of the notes I linked above) that $L^{1, 1+\alpha}_C = C^{0,\alpha}$ is the space of $\alpha$-Hölder continuous functions. Thus, my hope is (I am currently investigating this) that one can adapt the arguments from the lecture notes above to also work in the case where the absolute value is outside of the integral. As I said above, I don't think that this is the right solution, since it seems overly complicated :) I would be happy about any thoughts!","For $k \in \Bbb{N} = \{1,2,3,\dots\}$ and $\alpha \in (0,1)$, let us define $$ C^{k,\alpha} := \{ f : \Bbb{R} \to \Bbb{R} \,:\, f \in C^k \text{ with } f, f', \dots, f^{(k)} \text{ bounded and } [f^{(k)}]_{C^\alpha} < \infty \}, $$ where $$ [g]_{C^\alpha} := \sup_{x,y \in \Bbb{R}, x \neq y} \frac{|g(x)-g(y)|}{|x-y|^\alpha} \, . $$ It is not hard to see by using Taylor's formula for $f \in C^{k,\alpha}$ that for each $x \in \Bbb{R}$, there is a polynomial $P_x$ of degree $k$ (namely, the Taylor polynomial of $f$) satisfying $$ |f(x+h) - P_x (h)| \leq C \cdot |h|^{k+\alpha} \qquad \forall h \in \Bbb{R} \text{ with } |h| \leq 1, \tag{$\circledast$} $$ with a constant $C$ independent of $x,h$ (but depending on $f$). It is claimed in Exercise 13.31 of certain notes by Martin Hairer that also the converse holds, i.e., if $f : \Bbb{R} \to \Bbb{R}$ and if there is a constant $C>0$, such that for each $x$, there is a polynomial $P_x$ of degree at most $k$ such that $(\circledast)$ holds, then $f \in C^{k,\alpha}$. I would like to know a solution to this exercise, and even the case $k=1$ would be interesting to me. To simplify the question a bit, let us assume that we already know $f$ to be $k$-times differentiable, with $f,f', \dots, f^{(k)}$ being bounded (I think this boundedness does not actually follow ""automatically""), so we only need to establish $\alpha$-Hölder continuity of $f^{(k)}$. After the fold, I discuss some of my ideas towards this problem. It might be better, however, to at first not read these ideas, since given that this is an exercise, they seem overly complicated (they refer to Campanato spaces, which are probably not known to everyone doing the exercise). Thus it might be better to start afresh instead of getting spoiled by my stupid thoughts :) My thoughts only apply to the case $k=1$. Here, one can more or less easily show that necessarily $P_x (h) = f(x) + f'(x) h$. Then we get \begin{align*} & |f(x+h) - f(x-h) - 2h \cdot f'(x)| \\ & = |f(x+h) - f(x) - f'(x) h - (f(x-h) - f(x) - f'(x) \cdot (-h))| \\ & \leq 2C |h|^{1+\alpha} \end{align*} for $|h| \leq 1$. But using the fundamental theorem of calculus to write $f(x+h) - f(x-h) = \int_{x-h}^{x+h} f'(s) ds$, and writing $g = f'$, we thus get $$ \left| - \int_{x-h}^{x+h} g(s) - g(x) ds \right| \leq 2C \cdot |h|^{1+\alpha}. \tag{$\dagger$} $$ Now, it is known (see Lemma 0.0.15 in Some notes on Campanato spaces ) that the norm $$ \| g \|_\ast = \|g\|_{L^1} + \sup_{x \in \Bbb{R}, 0 < h \leq C} \,\, \inf_{c \in \Bbb{R}} \,\, h^{-(1 + \alpha)} \int_{x-h}^{x+h} |g(t) - c| \, dt $$ is an equivalent norm for the Campanato space $L^{1,1+\alpha}_C (\Bbb{R})$. Strictly speaking, I am cheating a bit here, since $\Bbb{R}$ is not a bounded domain. Thus, if in $(\dagger)$ above, we would have the absolute value inside the integral instead of outside, we would get a Campanato condition on the function $g = f'$. But it is known (see Wikipedia, or Theorem 0.0.22 of the notes I linked above) that $L^{1, 1+\alpha}_C = C^{0,\alpha}$ is the space of $\alpha$-Hölder continuous functions. Thus, my hope is (I am currently investigating this) that one can adapt the arguments from the lecture notes above to also work in the case where the absolute value is outside of the integral. As I said above, I don't think that this is the right solution, since it seems overly complicated :) I would be happy about any thoughts!",,"['real-analysis', 'derivatives', 'holder-spaces', 'stochastic-pde']"
92,Can a total rational metric space be complete?,Can a total rational metric space be complete?,,"Let's call a metric space $(M,d)$ a total rational metric space if: For every $x,y\in M$, $d(x,y)\in\mathbb{Q}$. For every $x\in M$ and every rational $q\geq0$ there exists a $y\in M$ such that $d(x,y)=q$. Can a total rational metric space be complete?","Let's call a metric space $(M,d)$ a total rational metric space if: For every $x,y\in M$, $d(x,y)\in\mathbb{Q}$. For every $x\in M$ and every rational $q\geq0$ there exists a $y\in M$ such that $d(x,y)=q$. Can a total rational metric space be complete?",,"['real-analysis', 'metric-spaces']"
93,"if $\left | x_{n+1}-\frac{x_{n}^2}{x_{n-1}} \right |\leq 1$, show that $(\frac{x_{n+1}}{x_{n}}) $ convergent","if , show that  convergent",\left | x_{n+1}-\frac{x_{n}^2}{x_{n-1}} \right |\leq 1 (\frac{x_{n+1}}{x_{n}}) ,"Let a real positive number sequence $(x_{n})$ such that $\left | x_{n+1}-\frac{x_{n}^2}{x_{n-1}} \right |\leq 1$ and $\sqrt{x_{1}}\geq \sqrt{x_0+1}$. Show that $(\frac{x_{n+1}}{x_{n}}) $ convergent. I have tried to define a sequence $(y_{n})$ such that $y_{n}=\frac{x_{n+1}}{x_{n}}$. So the preceding inequality equivalent to : $\left |x_{n}( \frac{x_{n+1}}{x_{n}}-\frac{x_{n}}{x_{n-1}}) \right |\leq 1$ $\equiv \left |y_{n}-y_{n-1}\right |\leq \frac {1}{x_{n}}$ I want to show that $(y_{n})$ is contractive sequence, and then it will be a cauchy sequence if and only if convergent sequence. But i stuck at this part. Please give me some clue, thank you.","Let a real positive number sequence $(x_{n})$ such that $\left | x_{n+1}-\frac{x_{n}^2}{x_{n-1}} \right |\leq 1$ and $\sqrt{x_{1}}\geq \sqrt{x_0+1}$. Show that $(\frac{x_{n+1}}{x_{n}}) $ convergent. I have tried to define a sequence $(y_{n})$ such that $y_{n}=\frac{x_{n+1}}{x_{n}}$. So the preceding inequality equivalent to : $\left |x_{n}( \frac{x_{n+1}}{x_{n}}-\frac{x_{n}}{x_{n-1}}) \right |\leq 1$ $\equiv \left |y_{n}-y_{n-1}\right |\leq \frac {1}{x_{n}}$ I want to show that $(y_{n})$ is contractive sequence, and then it will be a cauchy sequence if and only if convergent sequence. But i stuck at this part. Please give me some clue, thank you.",,"['real-analysis', 'sequences-and-series']"
94,Will the Lebesgue integral of a real valued function always be a Riemann sum?,Will the Lebesgue integral of a real valued function always be a Riemann sum?,,"If we have a real valued integral that is Lebesgue integrable but not Riemann integrable, can the value of the Lebesgue integral be given by a Riemann sum by choosing appropriate points in the partitioning process? Example: The indicator function for the irrationals:  $$f(x)=\begin{cases} 1 &\mbox{if } x\not\in \mathbb{Q}\\                      0 &\mbox{if } x \in \mathbb{Q} \end{cases}$$ has Lebesgue integral $1$ on $[0,1]$. This is the same value we would get if we formed the Riemann sum using irrational endpoints.","If we have a real valued integral that is Lebesgue integrable but not Riemann integrable, can the value of the Lebesgue integral be given by a Riemann sum by choosing appropriate points in the partitioning process? Example: The indicator function for the irrationals:  $$f(x)=\begin{cases} 1 &\mbox{if } x\not\in \mathbb{Q}\\                      0 &\mbox{if } x \in \mathbb{Q} \end{cases}$$ has Lebesgue integral $1$ on $[0,1]$. This is the same value we would get if we formed the Riemann sum using irrational endpoints.",,"['real-analysis', 'measure-theory']"
95,Valid proof of Young's inequality?,Valid proof of Young's inequality?,,"Part of an exercise to prove Holder's inequality in Rudin involves proving Young's Inequality... That is, given $\frac{1}{p}+\frac{1}{q} = 1$, prove  $$ab \leqslant \frac{a^p}{p} + \frac{b^q}{q}.$$ Here's my attempt at a proof: Let $$f(x) = \frac{x^p}{p} + \frac{b^q}{q} -bx$$ then, $$f'(x) = x^{p-1} -b$$ so that $f$ attains a minimum at $$x=b^{\frac{1}{p-1}}.$$ Since $\frac{p}{p-1} = q$, this is equivalent to saying that $f$ attains its minimum when $x^p = b^q$. Now, we need show that $$f(b^{\frac{1}{p-1}}) = 0.$$ We compute $$ \begin{align} f(b^{\frac{1}{p-1}}) &= \frac{(b^{\frac{1}{p-1}})^p}{p} + \frac{b^q}{q} - bb^{\frac{1}{p-1}} \\ &= \frac{b^q}{p} + \frac{b^q}{q} - b^{\frac{1}{p-1} +1} \\ &= \frac{b^q(p+q)}{pq} - b^{\frac{1}{p-1} +1} \\ &= b^q - b^{\frac{1}{p-1} +1} \\ &= b^q - b^q \\ & = 0 \end{align}$$ where $b^{\frac{1}{p-1} +1} = b^q$ since $$\frac{1}{p-1} +1 = \frac{1}{p-1} +\frac{p-1}{p-1} = \frac{p}{p-1} = q.$$ Thus, $f(x) = 0$ only when $x^p = b^q$. This is the global minimum of $f$ since $f^{''} \geq 0$ and analysis of concavity. Therefore, if $x > b^{\frac{1}{p-1}}$, $f(x) > 0$. That is, if $x^p > b^q$, the inequality holds. A similar analysis for $g(y) = \frac{a^p}{p} + \frac{y^q}{q} -ay$ shows that $g(y) > 0$ if $y^q > a^p$. Combining these two statements yields that, if $a^p \neq b^q$, the inequality holds, so we're done. Is this a valid proof? If so, if anyone could provide any alternative proofs, I'd be more than interested to see them.","Part of an exercise to prove Holder's inequality in Rudin involves proving Young's Inequality... That is, given $\frac{1}{p}+\frac{1}{q} = 1$, prove  $$ab \leqslant \frac{a^p}{p} + \frac{b^q}{q}.$$ Here's my attempt at a proof: Let $$f(x) = \frac{x^p}{p} + \frac{b^q}{q} -bx$$ then, $$f'(x) = x^{p-1} -b$$ so that $f$ attains a minimum at $$x=b^{\frac{1}{p-1}}.$$ Since $\frac{p}{p-1} = q$, this is equivalent to saying that $f$ attains its minimum when $x^p = b^q$. Now, we need show that $$f(b^{\frac{1}{p-1}}) = 0.$$ We compute $$ \begin{align} f(b^{\frac{1}{p-1}}) &= \frac{(b^{\frac{1}{p-1}})^p}{p} + \frac{b^q}{q} - bb^{\frac{1}{p-1}} \\ &= \frac{b^q}{p} + \frac{b^q}{q} - b^{\frac{1}{p-1} +1} \\ &= \frac{b^q(p+q)}{pq} - b^{\frac{1}{p-1} +1} \\ &= b^q - b^{\frac{1}{p-1} +1} \\ &= b^q - b^q \\ & = 0 \end{align}$$ where $b^{\frac{1}{p-1} +1} = b^q$ since $$\frac{1}{p-1} +1 = \frac{1}{p-1} +\frac{p-1}{p-1} = \frac{p}{p-1} = q.$$ Thus, $f(x) = 0$ only when $x^p = b^q$. This is the global minimum of $f$ since $f^{''} \geq 0$ and analysis of concavity. Therefore, if $x > b^{\frac{1}{p-1}}$, $f(x) > 0$. That is, if $x^p > b^q$, the inequality holds. A similar analysis for $g(y) = \frac{a^p}{p} + \frac{y^q}{q} -ay$ shows that $g(y) > 0$ if $y^q > a^p$. Combining these two statements yields that, if $a^p \neq b^q$, the inequality holds, so we're done. Is this a valid proof? If so, if anyone could provide any alternative proofs, I'd be more than interested to see them.",,"['real-analysis', 'analysis', 'inequality', 'proof-verification', 'young-inequality']"
96,The closed form of $\sum_{n=0}^{\infty} \arcsin\bigl(\frac{1}{e^n}\bigr)$,The closed form of,\sum_{n=0}^{\infty} \arcsin\bigl(\frac{1}{e^n}\bigr),"In my study on some type of integrals I met the series below that I don't how to approach it. Of course, one of the obvious questions is: does it have a closed form? Before answering that, I need to learn how to tackle them, the proper tools to employ. Any help  on this series is very welcome. The use of $\arcsin(x)$ series expansion wasn't fruitful. $$\sum_{n=0}^{\infty} \arcsin\left(\frac{1}{e^n}\right)$$ that more generally can be considered as $$\sum_{n=0}^{\infty} \arcsin\left(x^n\right)$$","In my study on some type of integrals I met the series below that I don't how to approach it. Of course, one of the obvious questions is: does it have a closed form? Before answering that, I need to learn how to tackle them, the proper tools to employ. Any help  on this series is very welcome. The use of $\arcsin(x)$ series expansion wasn't fruitful. $$\sum_{n=0}^{\infty} \arcsin\left(\frac{1}{e^n}\right)$$ that more generally can be considered as $$\sum_{n=0}^{\infty} \arcsin\left(x^n\right)$$",,"['calculus', 'real-analysis', 'sequences-and-series', 'complex-analysis', 'elliptic-integrals']"
97,"If $f'$ is differentiable at $a$ then $f'$ is continuous at $(a-\delta,a+\delta)$",If  is differentiable at  then  is continuous at,"f' a f' (a-\delta,a+\delta)","Is there a counterexample? Proposition: Let $f:\mathbb{R} \longrightarrow \mathbb{R}$ be a differentiable function  such that $f':\mathbb{R} \longrightarrow \mathbb{R}$ is differentiable at $a\in\mathbb{R}$ (possibly differentiable at a single point). Then $f':\mathbb{R} \longrightarrow \mathbb{R}$ is continuous at an interval $(a-\delta,a+\delta)$ for some $\delta>0$. Any hints would be appreciated.","Is there a counterexample? Proposition: Let $f:\mathbb{R} \longrightarrow \mathbb{R}$ be a differentiable function  such that $f':\mathbb{R} \longrightarrow \mathbb{R}$ is differentiable at $a\in\mathbb{R}$ (possibly differentiable at a single point). Then $f':\mathbb{R} \longrightarrow \mathbb{R}$ is continuous at an interval $(a-\delta,a+\delta)$ for some $\delta>0$. Any hints would be appreciated.",,['real-analysis']
98,How to find a homeomorphism $\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ having certain properties,How to find a homeomorphism  having certain properties,\mathbb{R}^{n}\rightarrow\mathbb{R}^{n},"Let $n\ge 2$ and let $C$ be a Cantor space in $\mathbb{R}^{n}$ . That is, $C$ is homeomorphic to the Cantor ternary set . Let $x$ and $y$ be two points in $\mathbb{R}^{n}-C$ , and let $L_{xy}$ be the straight line segment joining them. For a fixed $\varepsilon>0$ we would like to to find a homeomorphism $\phi:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$ such that $\phi(x)=x$ and $\phi(y)=y$ . Outside an $\varepsilon$ -neighborhood of $L_{xy}$ , $\phi$ is the identity, and $|\phi(z)-z|<\varepsilon$ . $\phi(L_{xy})\cap C=\emptyset$ . How do we construct/show existence of such a homeomorphism $\phi?$","Let and let be a Cantor space in . That is, is homeomorphic to the Cantor ternary set . Let and be two points in , and let be the straight line segment joining them. For a fixed we would like to to find a homeomorphism such that and . Outside an -neighborhood of , is the identity, and . . How do we construct/show existence of such a homeomorphism",n\ge 2 C \mathbb{R}^{n} C x y \mathbb{R}^{n}-C L_{xy} \varepsilon>0 \phi:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n} \phi(x)=x \phi(y)=y \varepsilon L_{xy} \phi |\phi(z)-z|<\varepsilon \phi(L_{xy})\cap C=\emptyset \phi?,"['real-analysis', 'general-topology', 'cantor-set']"
99,Abel limit theorem,Abel limit theorem,,"I would like to know if the Abel limit theorem works if the limit is infinite.  Let the series $\sum_{k=0}^\infty a_k x^k$ have radius of convergence 1. Assume further that $\sum_{k=0}^\infty a_k = \infty$ . Is it the case that $\lim_{x\to 1^-} \sum_{k=0}^\infty a_k x^k = \infty$ ? Thank you in advance for any suggestions. I would just like to know if it is true, then I will try think of a proof myself.","I would like to know if the Abel limit theorem works if the limit is infinite.  Let the series $\sum_{k=0}^\infty a_k x^k$ have radius of convergence 1. Assume further that $\sum_{k=0}^\infty a_k = \infty$ . Is it the case that $\lim_{x\to 1^-} \sum_{k=0}^\infty a_k x^k = \infty$ ? Thank you in advance for any suggestions. I would just like to know if it is true, then I will try think of a proof myself.",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'power-series']"
