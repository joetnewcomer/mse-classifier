,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Which is the correct method to find simultaneous limit and why?,Which is the correct method to find simultaneous limit and why?,,"I tried finding the simultaneous limit as $(x,y)\to(0,0)$ for the function $$f(x, y) = \begin{cases} \frac{x^3y}{x^6+y^2}& (x, y)\neq (0, 0) \\ 0& (x, y) = (0, 0) \end{cases}$$ using the method @alfriedman suggested . Let $x=r\cos(\theta)$ and $y=r\sin(\theta)$. We get $$f(r\cos(\theta),r\sin(\theta))=\begin{cases} \frac{r^4\cos^3(\theta)\sin(\theta)}{(r\cos(\theta))^6+(r\sin(\theta))^2},& r\neq 0 \\ 0& r=0 \end{cases}$$ Method I: (Take $r\to 0$ and then fix $\theta$) As we take  $r\to 0$, we get the limit as $\frac{0}{\sin^2(\theta)}$. The limit wouldn't exist if $\sin(\theta)=0$ (say for $\theta=0$ or $\theta=\pi$). This is the reason why the simultaneous limit as $(x,y)\to(0,0)$ doesn't exist. Method 2: (Fix $\theta$ first and then take $r\to 0$) Let us consider $\theta=0$ or $\theta=\pi$. In that case the limit is $0$ (as $r\to 0$) Also, for any other angle $\theta=\theta_o$ such that $\theta_0 \in [0,2\pi)-\{0,\pi\}$ The limit is surely $0$ as $r\to 0$. Hence we see that method 1 tells us that the simultaneous limit doesn't exist as $(x,y)\to(0,0)$ but method 2 tells us that it does exist. Now, I checked it on Wolfram Alpha that the simultaneous limit doesn't exist. So, I'd like to know why method 1 is correct and method 2 is wrong. Any idea?","I tried finding the simultaneous limit as $(x,y)\to(0,0)$ for the function $$f(x, y) = \begin{cases} \frac{x^3y}{x^6+y^2}& (x, y)\neq (0, 0) \\ 0& (x, y) = (0, 0) \end{cases}$$ using the method @alfriedman suggested . Let $x=r\cos(\theta)$ and $y=r\sin(\theta)$. We get $$f(r\cos(\theta),r\sin(\theta))=\begin{cases} \frac{r^4\cos^3(\theta)\sin(\theta)}{(r\cos(\theta))^6+(r\sin(\theta))^2},& r\neq 0 \\ 0& r=0 \end{cases}$$ Method I: (Take $r\to 0$ and then fix $\theta$) As we take  $r\to 0$, we get the limit as $\frac{0}{\sin^2(\theta)}$. The limit wouldn't exist if $\sin(\theta)=0$ (say for $\theta=0$ or $\theta=\pi$). This is the reason why the simultaneous limit as $(x,y)\to(0,0)$ doesn't exist. Method 2: (Fix $\theta$ first and then take $r\to 0$) Let us consider $\theta=0$ or $\theta=\pi$. In that case the limit is $0$ (as $r\to 0$) Also, for any other angle $\theta=\theta_o$ such that $\theta_0 \in [0,2\pi)-\{0,\pi\}$ The limit is surely $0$ as $r\to 0$. Hence we see that method 1 tells us that the simultaneous limit doesn't exist as $(x,y)\to(0,0)$ but method 2 tells us that it does exist. Now, I checked it on Wolfram Alpha that the simultaneous limit doesn't exist. So, I'd like to know why method 1 is correct and method 2 is wrong. Any idea?",,['limits']
1,Finding number of unique dice with n sides with values between x and y that add up to z,Finding number of unique dice with n sides with values between x and y that add up to z,,"I've recently taken an interest in nontransitive dice; specifically corrected Grime dice. I want to see how many other unique dice there are that follow the same criteria of having $5n$ sides (possible by either displaying each value twice on a d10 or four times on a d20) with values between $1$ and $9$ that add up to $25$ (possibly so that I can see if I can find another set of 5 dice with a more consistent pair of cyclical chains than the currently known set later, but that will be another question if I set about it). So I want to find a formula that will tell me how many unique dice fit the criteria I put forth. Through manual work I think I've found that there are $72$ unique dice when $n=5$, $x=1$, $y=9$, and $z=25$; though I'm not 100% certain that I haven't missed any. I'll include the list that I've found so that people can double-check my work + manual method; if there's a way that I could format it to be easier to read while still not taking up too much space either please do suggest an edit: 1, 1, 5, 9, 9 1, 2, 4, 9, 9 1, 3, 3, 9, 9 2, 2, 3, 9, 9 1, 1, 6, 8, 9 1, 2, 5, 8, 9 1, 3, 4, 8, 9 2, 2, 4, 8, 9 2, 3, 3, 8, 9 1, 1, 7, 7, 9 1, 2, 6, 7, 9 1, 3, 5, 7, 9 1, 4, 4, 7, 9 2, 2, 5, 7, 9 2, 3, 4, 7, 9 3, 3, 3, 7, 9 1, 3, 6, 6, 9 1, 4, 5, 6, 9 2, 2, 6, 6, 9 2, 3, 5, 6, 9 2, 4, 4, 6, 9 3, 3, 4, 6, 9 1, 5, 5, 5, 9 2, 4, 5, 5, 9 3, 3, 5, 5, 9 3, 4, 4, 5, 9 4, 4, 4, 4, 9 1, 1, 7, 8, 8 1, 2, 6, 8, 8 1, 3, 5, 8, 8 1, 4, 4, 8, 8 2, 2, 5, 8, 8 2, 3, 4, 8, 8 3, 3, 3, 8, 8 1, 2, 7, 7, 8 1, 3, 6, 7, 8 1, 4, 5, 7, 8 2, 2, 6, 7, 8 2, 3, 5, 7, 8 2, 4, 4, 7, 8 3, 3, 4, 7, 8 1, 4, 6, 6, 8 1, 5, 5, 6, 8 2, 3, 6, 6, 8 2, 4, 5, 6, 8 3, 3, 5, 6, 8 3, 4, 4, 6, 8 2, 5, 5, 5, 8 3, 4, 5, 5, 8 4, 4, 4, 5, 8 1, 3, 7, 7, 7 1, 4, 6, 7, 7 1, 5, 5, 7, 7 2, 2, 7, 7, 7 2, 4, 5, 7, 7 3, 3, 5, 7, 7 3, 4, 4, 7, 7 1, 5, 6, 6, 7 2, 4, 6, 6, 7 2, 5, 5, 6, 7 3, 3, 6, 6, 7 3, 4, 5, 6, 7 4, 4, 4, 6, 7 3, 5, 5, 5, 7 4, 4, 5, 5, 7 1, 6, 6, 6, 6 2, 5, 6, 6, 6 3, 4, 6, 6, 6 3, 5, 5, 6, 6 4, 4, 5, 6, 6 4, 5, 5, 5, 6 5, 5, 5, 5, 5 I'd like it if $n=5$ $x=1$ $y=9$ $z=25$ could be used to demonstrate formulas so that I can see what an easier way to arrive at my conclusion would have been and if I came to the correct conclusion. I'd also like examples using $n=6$ $x=1$ $y=6$ $z=21$ so that I can also see an answer that I didn't (attempt to) manually work out. If I'm using any incorrect tags or I'm not using a tag that I should be using, please let me know.","I've recently taken an interest in nontransitive dice; specifically corrected Grime dice. I want to see how many other unique dice there are that follow the same criteria of having $5n$ sides (possible by either displaying each value twice on a d10 or four times on a d20) with values between $1$ and $9$ that add up to $25$ (possibly so that I can see if I can find another set of 5 dice with a more consistent pair of cyclical chains than the currently known set later, but that will be another question if I set about it). So I want to find a formula that will tell me how many unique dice fit the criteria I put forth. Through manual work I think I've found that there are $72$ unique dice when $n=5$, $x=1$, $y=9$, and $z=25$; though I'm not 100% certain that I haven't missed any. I'll include the list that I've found so that people can double-check my work + manual method; if there's a way that I could format it to be easier to read while still not taking up too much space either please do suggest an edit: 1, 1, 5, 9, 9 1, 2, 4, 9, 9 1, 3, 3, 9, 9 2, 2, 3, 9, 9 1, 1, 6, 8, 9 1, 2, 5, 8, 9 1, 3, 4, 8, 9 2, 2, 4, 8, 9 2, 3, 3, 8, 9 1, 1, 7, 7, 9 1, 2, 6, 7, 9 1, 3, 5, 7, 9 1, 4, 4, 7, 9 2, 2, 5, 7, 9 2, 3, 4, 7, 9 3, 3, 3, 7, 9 1, 3, 6, 6, 9 1, 4, 5, 6, 9 2, 2, 6, 6, 9 2, 3, 5, 6, 9 2, 4, 4, 6, 9 3, 3, 4, 6, 9 1, 5, 5, 5, 9 2, 4, 5, 5, 9 3, 3, 5, 5, 9 3, 4, 4, 5, 9 4, 4, 4, 4, 9 1, 1, 7, 8, 8 1, 2, 6, 8, 8 1, 3, 5, 8, 8 1, 4, 4, 8, 8 2, 2, 5, 8, 8 2, 3, 4, 8, 8 3, 3, 3, 8, 8 1, 2, 7, 7, 8 1, 3, 6, 7, 8 1, 4, 5, 7, 8 2, 2, 6, 7, 8 2, 3, 5, 7, 8 2, 4, 4, 7, 8 3, 3, 4, 7, 8 1, 4, 6, 6, 8 1, 5, 5, 6, 8 2, 3, 6, 6, 8 2, 4, 5, 6, 8 3, 3, 5, 6, 8 3, 4, 4, 6, 8 2, 5, 5, 5, 8 3, 4, 5, 5, 8 4, 4, 4, 5, 8 1, 3, 7, 7, 7 1, 4, 6, 7, 7 1, 5, 5, 7, 7 2, 2, 7, 7, 7 2, 4, 5, 7, 7 3, 3, 5, 7, 7 3, 4, 4, 7, 7 1, 5, 6, 6, 7 2, 4, 6, 6, 7 2, 5, 5, 6, 7 3, 3, 6, 6, 7 3, 4, 5, 6, 7 4, 4, 4, 6, 7 3, 5, 5, 5, 7 4, 4, 5, 5, 7 1, 6, 6, 6, 6 2, 5, 6, 6, 6 3, 4, 6, 6, 6 3, 5, 5, 6, 6 4, 4, 5, 6, 6 4, 5, 5, 5, 6 5, 5, 5, 5, 5 I'd like it if $n=5$ $x=1$ $y=9$ $z=25$ could be used to demonstrate formulas so that I can see what an easier way to arrive at my conclusion would have been and if I came to the correct conclusion. I'd also like examples using $n=6$ $x=1$ $y=6$ $z=21$ so that I can also see an answer that I didn't (attempt to) manually work out. If I'm using any incorrect tags or I'm not using a tag that I should be using, please let me know.",,"['combinatorics', 'multivariable-calculus', 'extremal-combinatorics']"
2,"Multivariable analysis - convergent subsequence of $ x_n=(\sin(n) ,\cos(n), 1+(-1)^n)$",Multivariable analysis - convergent subsequence of," x_n=(\sin(n) ,\cos(n), 1+(-1)^n)","If $ x_n=(\sin(n) ,\cos(n), 1+(-1)^n),$ does the sequence $\{x_n\} \in \mathbb R^3$ have a convergent subsequence? For this problem I'm not sure if a convergent subsequence would exist for this specific sequence since $ 1 + (-1)^n $ isn't bounded and thus doesn't converge which should force $ x_n $ to not have a convergent subsequence? Am I correct or if not what would be the proper subsequence for $x_n$?","If $ x_n=(\sin(n) ,\cos(n), 1+(-1)^n),$ does the sequence $\{x_n\} \in \mathbb R^3$ have a convergent subsequence? For this problem I'm not sure if a convergent subsequence would exist for this specific sequence since $ 1 + (-1)^n $ isn't bounded and thus doesn't converge which should force $ x_n $ to not have a convergent subsequence? Am I correct or if not what would be the proper subsequence for $x_n$?",,"['real-analysis', 'sequences-and-series', 'multivariable-calculus']"
3,Evaluate the flux of the vector field without using divergence theorem,Evaluate the flux of the vector field without using divergence theorem,,"Given $ F(x,y,z)=(z^2-x,-xy,3z)$ and $S$ is the surface of the solid   delimited by the equations $z=4-y^2, x=0, x=3$ and $z=0$, with the   normal vector exterior, evaluate $\iint_S F\cdot n \,ds$ without using   The Divergence Theorem.","Given $ F(x,y,z)=(z^2-x,-xy,3z)$ and $S$ is the surface of the solid   delimited by the equations $z=4-y^2, x=0, x=3$ and $z=0$, with the   normal vector exterior, evaluate $\iint_S F\cdot n \,ds$ without using   The Divergence Theorem.",,"['calculus', 'multivariable-calculus', 'surface-integrals']"
4,"Show that the directional derivatives of $f$ and $g$ exist everywhere, but that there is a $u \neq 0$ for which $h'(0,u)$ does not exist.","Show that the directional derivatives of  and  exist everywhere, but that there is a  for which  does not exist.","f g u \neq 0 h'(0,u)","Let $g : \mathbb{R}^2 \to \mathbb{R}^2$ be defined by the equation $g(x,y) = (x,y + x^2)$. Let  $f : \mathbb{R}^2 \to \mathbb{R}$ be the function defined as $f(x,y) = x^2y/(x^4+y^2)$. Let $h = f \circ g$. Show that the directional derivatives of $f$ and $g$ exist everywhere, but  that there is a $u \neq 0$ for which $h'(0,u)$ does not exist. I have verified the fact that the directional derivatives of $f$ and $g$ exist everywhere. Hints reqd to do the 2nd part.","Let $g : \mathbb{R}^2 \to \mathbb{R}^2$ be defined by the equation $g(x,y) = (x,y + x^2)$. Let  $f : \mathbb{R}^2 \to \mathbb{R}$ be the function defined as $f(x,y) = x^2y/(x^4+y^2)$. Let $h = f \circ g$. Show that the directional derivatives of $f$ and $g$ exist everywhere, but  that there is a $u \neq 0$ for which $h'(0,u)$ does not exist. I have verified the fact that the directional derivatives of $f$ and $g$ exist everywhere. Hints reqd to do the 2nd part.",,"['multivariable-calculus', 'derivatives']"
5,Surface integral over a triangle region,Surface integral over a triangle region,,"Evaluate the integral $\iint xyz\,ds$ where $S$ is the triangle with vertices $(1,0,0)$ , $(0,1,0)$ , and $(0,0,1)$ . Well, I did almost everything, I'm just stuck at finding the boundaries. Firstly, I found the parametrization of the surface: $\alpha(t,k)=(1-t-k,k,t)$ , using the equation of the plane and the vectors based on the points given. So I found the vector parallel to the normal one: $N=(-1,-1,-1)$ Then I computed its norm: $\|N\|=\sqrt 3$ . Substituting at the given equation, I have: $$\iint (kt-kt^2-k^2t)\sqrt 3 \, dk\,dt$$ But now I don't know how I can find the limits of integration.","Evaluate the integral where is the triangle with vertices , , and . Well, I did almost everything, I'm just stuck at finding the boundaries. Firstly, I found the parametrization of the surface: , using the equation of the plane and the vectors based on the points given. So I found the vector parallel to the normal one: Then I computed its norm: . Substituting at the given equation, I have: But now I don't know how I can find the limits of integration.","\iint xyz\,ds S (1,0,0) (0,1,0) (0,0,1) \alpha(t,k)=(1-t-k,k,t) N=(-1,-1,-1) \|N\|=\sqrt 3 \iint (kt-kt^2-k^2t)\sqrt 3 \, dk\,dt","['multivariable-calculus', 'surface-integrals', 'multiple-integral']"
6,Understanding partial derivatives of multi-variable functions,Understanding partial derivatives of multi-variable functions,,"I'm having trouble convincing myself of the validity of the following set of steps in differentiation of a multi-variable function under a constraint: Suppose $f(x,y) = 0$ The above constraint implicitly makes $x$ dependent on $y$. I can do the following operation  \begin{align} \frac{df(x,y)}{dx} = 0 \end{align} Since the derivative is not partial, $y$ is allowed to change with changing $x$ and the constraint $f(x,y) = 0$ can still be satisfied. I know that taking partial derivative wouldn't be legal since that would make $y$ constant and the constraint will no longer be satisfied. How do I prove this formally? That \begin{align} \frac{{\partial f(x,y)}}{{\partial x}} \end{align} is invalid given that $f(x,y) = 0$.","I'm having trouble convincing myself of the validity of the following set of steps in differentiation of a multi-variable function under a constraint: Suppose $f(x,y) = 0$ The above constraint implicitly makes $x$ dependent on $y$. I can do the following operation  \begin{align} \frac{df(x,y)}{dx} = 0 \end{align} Since the derivative is not partial, $y$ is allowed to change with changing $x$ and the constraint $f(x,y) = 0$ can still be satisfied. I know that taking partial derivative wouldn't be legal since that would make $y$ constant and the constraint will no longer be satisfied. How do I prove this formally? That \begin{align} \frac{{\partial f(x,y)}}{{\partial x}} \end{align} is invalid given that $f(x,y) = 0$.",,"['calculus', 'multivariable-calculus', 'derivatives', 'partial-derivative']"
7,"Prove that $||\nabla u||^2+||\nabla v||^2=\frac{0.5}{\sqrt{x^2+y^2}}$ if $x=2uv, y=u^2-v^2$",Prove that  if,"||\nabla u||^2+||\nabla v||^2=\frac{0.5}{\sqrt{x^2+y^2}} x=2uv, y=u^2-v^2","$x,y$ are functions of $u,v$ and $x=2uv, y=u^2-v^2$. There's an inverse map from such that $u,v$ are functions of $x,y$. Prove that $$||\nabla u||^2+||\nabla v||^2=\frac{0.5}{\sqrt{x^2+y^2}}$$ This is an exercise for implicit differentiation. I thought of the following: $$ u_u=v_x 2v+v_y 2v\\ v_v=v_x 2u-v_y 2v\\ u_u=u_x 2v+u_y 2u\\ u_v=u_x 2u-u_y 2v $$ Even if this is correct I don't see how we can factor out the gradients from here.","$x,y$ are functions of $u,v$ and $x=2uv, y=u^2-v^2$. There's an inverse map from such that $u,v$ are functions of $x,y$. Prove that $$||\nabla u||^2+||\nabla v||^2=\frac{0.5}{\sqrt{x^2+y^2}}$$ This is an exercise for implicit differentiation. I thought of the following: $$ u_u=v_x 2v+v_y 2v\\ v_v=v_x 2u-v_y 2v\\ u_u=u_x 2v+u_y 2u\\ u_v=u_x 2u-u_y 2v $$ Even if this is correct I don't see how we can factor out the gradients from here.",,"['multivariable-calculus', 'partial-derivative', 'implicit-differentiation']"
8,Find the sphere with radius $\sqrt 6$ which touches an ellipsoid in a given point?,Find the sphere with radius  which touches an ellipsoid in a given point?,\sqrt 6,"Find the center of a sphere $(x-x_0)^2+(y-y_0)^2+(z-z_0)^2=6$ which touches ellipsoid $3x^2+2y^2+12z^2=42$ in $(2,3,1)$? I thought of finding the normal vector to the tangent plane to the ellipsoid at point $(2,3,1)$. It is: $$ n=\langle-\frac{x}{4\sqrt{42-3x^2-2y^2}},-\frac{y}{4\sqrt{42-3x^2-2y^2}},-1\rangle $$ Then I thought getting normal unit vector: $n_u=\frac{n}{||n||}$. Because the distance from center of the sphere to the point $(2,3,1)$ is its radius it is $\sqrt 6$. So I thought to multiply $n_u$ by $\sqrt 6$ and then plug in the point $(2,3,1)$. I tried doing this but the answer is incorrect. What's wrong with my method?","Find the center of a sphere $(x-x_0)^2+(y-y_0)^2+(z-z_0)^2=6$ which touches ellipsoid $3x^2+2y^2+12z^2=42$ in $(2,3,1)$? I thought of finding the normal vector to the tangent plane to the ellipsoid at point $(2,3,1)$. It is: $$ n=\langle-\frac{x}{4\sqrt{42-3x^2-2y^2}},-\frac{y}{4\sqrt{42-3x^2-2y^2}},-1\rangle $$ Then I thought getting normal unit vector: $n_u=\frac{n}{||n||}$. Because the distance from center of the sphere to the point $(2,3,1)$ is its radius it is $\sqrt 6$. So I thought to multiply $n_u$ by $\sqrt 6$ and then plug in the point $(2,3,1)$. I tried doing this but the answer is incorrect. What's wrong with my method?",,"['multivariable-calculus', 'differential-geometry', 'vectors']"
9,"Calculate the flux of the field $\mathbf{F} = k\left(\frac{\mathbf{r}}{r^3}\right)$ out of an arbitrary closed surface not intersecting $(0,0,0)$",Calculate the flux of the field  out of an arbitrary closed surface not intersecting,"\mathbf{F} = k\left(\frac{\mathbf{r}}{r^3}\right) (0,0,0)","Calculate the flux of the field $$\mathbf{F} = k\left(\frac{\mathbf{r}}{|\mathbf{r}|^3}\right)$$ where $\mathbf{r}=\langle x,y,z\rangle$ out of an arbitrary closed surface not intersecting $(0,0,0)$. My attempt I get  $$\operatorname{div} \mathbf{F} = 0$$ Using Gauss’s theorem I get that the flux crossing an arbitrary surface must be $0$ since no flux is produced. The answer is however $4k\pi$ if the surface envelopes $(0,0,0)$ and otherwise it is $0$. How can this be true? How can any flux pass any surface if no flux is created anywhere? My understanding is obviously flawed, but I can’t pinpoint where.","Calculate the flux of the field $$\mathbf{F} = k\left(\frac{\mathbf{r}}{|\mathbf{r}|^3}\right)$$ where $\mathbf{r}=\langle x,y,z\rangle$ out of an arbitrary closed surface not intersecting $(0,0,0)$. My attempt I get  $$\operatorname{div} \mathbf{F} = 0$$ Using Gauss’s theorem I get that the flux crossing an arbitrary surface must be $0$ since no flux is produced. The answer is however $4k\pi$ if the surface envelopes $(0,0,0)$ and otherwise it is $0$. How can this be true? How can any flux pass any surface if no flux is created anywhere? My understanding is obviously flawed, but I can’t pinpoint where.",,"['multivariable-calculus', 'divergence-operator']"
10,"How does Rudin's rank theorem apply to the map $F(x,y)=x^2+y^2$?",How does Rudin's rank theorem apply to the map ?,"F(x,y)=x^2+y^2","Intuition regarding the ""rank theorem"" in Rudin's Principle of Mathematical Analysis has been asked several times in this site: I'm trying to understand how this theorem applies to the following example. Consider $F:{\bf R}^2\to{\bf R}$ with $$ F(x,y)=x^2+y^2 $$ So here we have $n=2$ and $m=r=1$. Take $a=(1,0)$. Then $A=[2,0]$ and thus $Y_1={\bf R}$ and $Y_2=\{0\}$. Here are my questions : What should be $V$, $H$ and $\varphi$ for this example? What does the rank theorem really say about this example?","Intuition regarding the ""rank theorem"" in Rudin's Principle of Mathematical Analysis has been asked several times in this site: I'm trying to understand how this theorem applies to the following example. Consider $F:{\bf R}^2\to{\bf R}$ with $$ F(x,y)=x^2+y^2 $$ So here we have $n=2$ and $m=r=1$. Take $a=(1,0)$. Then $A=[2,0]$ and thus $Y_1={\bf R}$ and $Y_2=\{0\}$. Here are my questions : What should be $V$, $H$ and $\varphi$ for this example? What does the rank theorem really say about this example?",,['real-analysis']
11,Finding the heat flow across the curved surface of a cylinder,Finding the heat flow across the curved surface of a cylinder,,"I have the following problem: The temperature at a point in a cylinder of radius $a$ and height $h$, and made of material with conductivity $k$, is inversely proportional to the distance from the centre of the cylinder. Find the heat flow across the curved surface of the cylinder. The solution says that $T = \dfrac{\alpha}{\sqrt{x^2 + y^2 + z^2}}$ where $\alpha$ is a constant and hence $\mathbf{F} = \dfrac{\alpha k(x, y, z)}{(x^2 + y^2 + z^2)^{3/2}}$ $= \dfrac{\alpha k(r\cos(\theta), r\sin(\theta), z)}{(r^2 + z^2)^{3/2}}$ How and why did we go from $\sqrt{x^2 + y^2 + z^2}$ to $(x^2 + y^2 + z^2)^{3/2}$? This isn't explained in the solution. In $T = \dfrac{\alpha}{\sqrt{x^2 + y^2 + z^2}}$, presuming $\sqrt{x^2 + y^2 + z^2}$ is meant to be the distance from the centre of the cylinder, shouldn't we have $z = 0$ such that we get $\sqrt{x^2 + y^2}$ as the distance from any point to the centre of the cylinder? This is because the formula for the distance between any two points in a cylinder (and indeed any object in $\mathbb{R}^3$) is $d(P_1, P_2) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2}$. Since we want the distance to the centre of the cylinder, we would have $z_2 = z_1$ such that $z_2 - z_1 = 0$. So shouldn't it actually be $T = \dfrac{\alpha}{\sqrt{x^2 + y^2}}$ where $\alpha$ is a constant? I would greatly appreciate it if people could please take the time to clarify this.","I have the following problem: The temperature at a point in a cylinder of radius $a$ and height $h$, and made of material with conductivity $k$, is inversely proportional to the distance from the centre of the cylinder. Find the heat flow across the curved surface of the cylinder. The solution says that $T = \dfrac{\alpha}{\sqrt{x^2 + y^2 + z^2}}$ where $\alpha$ is a constant and hence $\mathbf{F} = \dfrac{\alpha k(x, y, z)}{(x^2 + y^2 + z^2)^{3/2}}$ $= \dfrac{\alpha k(r\cos(\theta), r\sin(\theta), z)}{(r^2 + z^2)^{3/2}}$ How and why did we go from $\sqrt{x^2 + y^2 + z^2}$ to $(x^2 + y^2 + z^2)^{3/2}$? This isn't explained in the solution. In $T = \dfrac{\alpha}{\sqrt{x^2 + y^2 + z^2}}$, presuming $\sqrt{x^2 + y^2 + z^2}$ is meant to be the distance from the centre of the cylinder, shouldn't we have $z = 0$ such that we get $\sqrt{x^2 + y^2}$ as the distance from any point to the centre of the cylinder? This is because the formula for the distance between any two points in a cylinder (and indeed any object in $\mathbb{R}^3$) is $d(P_1, P_2) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2}$. Since we want the distance to the centre of the cylinder, we would have $z_2 = z_1$ such that $z_2 - z_1 = 0$. So shouldn't it actually be $T = \dfrac{\alpha}{\sqrt{x^2 + y^2}}$ where $\alpha$ is a constant? I would greatly appreciate it if people could please take the time to clarify this.",,"['real-analysis', 'multivariable-calculus', 'mathematical-modeling', 'vector-fields', 'surface-integrals']"
12,Question about total differentials,Question about total differentials,,"Why is it true that if you move from point $(u,v,w)$ to point $(u+du,v+dv,w+dw)$, a scalar function $\phi(u,v,w)$ changes by an amount $$d\phi=\frac{\partial \phi}{\partial u}du+\frac{\partial \phi}{\partial v}dv+\frac{\partial \phi}{\partial w}dw$$","Why is it true that if you move from point $(u,v,w)$ to point $(u+du,v+dv,w+dw)$, a scalar function $\phi(u,v,w)$ changes by an amount $$d\phi=\frac{\partial \phi}{\partial u}du+\frac{\partial \phi}{\partial v}dv+\frac{\partial \phi}{\partial w}dw$$",,['multivariable-calculus']
13,Evaluating $\iiint_V x^2 dxdydz$ Over a Spherical Sector,Evaluating  Over a Spherical Sector,\iiint_V x^2 dxdydz,"Triple Integral of $$I=\iiint_V x^2 dxdydz$$ where $V=\{(x,y,z) | y\le x, x\ge0, y\ge0, z\ge0, x^2+y^2+z^2\le1\}$. And I set it as $$I=\int_0^1 x^2\int_0^x\int_0^{\sqrt{1-x^2-y^2}}dzdydx$$ and first integrate $dz$ then use polar co-ordinate for $dy$ and $dx$, changing them to $rdrd\theta$. Am I setting the integral right?","Triple Integral of $$I=\iiint_V x^2 dxdydz$$ where $V=\{(x,y,z) | y\le x, x\ge0, y\ge0, z\ge0, x^2+y^2+z^2\le1\}$. And I set it as $$I=\int_0^1 x^2\int_0^x\int_0^{\sqrt{1-x^2-y^2}}dzdydx$$ and first integrate $dz$ then use polar co-ordinate for $dy$ and $dx$, changing them to $rdrd\theta$. Am I setting the integral right?",,"['integration', 'multivariable-calculus', 'definite-integrals']"
14,How to find tangent planes? [closed],How to find tangent planes? [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Finding the tangent plane equation is simple for equations with a simple $z$. For example, if $z$ is a function $f(x,y)$ such as $f(x,y)=x^2+y^3$. It's simply: $z-z_0=f_x(x_0,y_0)(x-x_0)+f_y(x_0,y_0)(y-y_0)$ Which in that case would be: $z-z_0=(2x)(x-x_0)+(3y^2)(y-y_0)$ But what if we're dealing with a more complicated $f(x,y,z)$ where values are not necessarily unique along the $z$ axis? Wouldn't we have to add a $f_z$ term?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question Finding the tangent plane equation is simple for equations with a simple $z$. For example, if $z$ is a function $f(x,y)$ such as $f(x,y)=x^2+y^3$. It's simply: $z-z_0=f_x(x_0,y_0)(x-x_0)+f_y(x_0,y_0)(y-y_0)$ Which in that case would be: $z-z_0=(2x)(x-x_0)+(3y^2)(y-y_0)$ But what if we're dealing with a more complicated $f(x,y,z)$ where values are not necessarily unique along the $z$ axis? Wouldn't we have to add a $f_z$ term?",,"['multivariable-calculus', 'linear-approximation']"
15,Differentiating a matrix function [duplicate],Differentiating a matrix function [duplicate],,"This question already has an answer here : Taking derivatives with respect to a matrix (1 answer) Closed 6 years ago . In the book ""Elements of Statistical Learning"", early on the author is discussing linear regression, and naturally discusses the residual sum of squares (RSS) based on the parameter space $\boldsymbol{\beta}$. In the general formulation, $\text{RSS}(\beta) = (\boldsymbol{y} - \boldsymbol{X}\beta)^T(\boldsymbol{y} - \boldsymbol{X}\beta)$ where $\boldsymbol{X}$ is an $N \times p$ matrix and $\beta$ is a $p \times K$ matrix. The author then says to minimize RSS, you differentiate with respect to $\beta$ and get $\boldsymbol{X}^T(\boldsymbol{y} - \boldsymbol{X}\beta) = 0$ My question is, what are the mechanics of differentiating with respect to the matrix $\beta$? I have a B.S. in physics, so I have a reasonably sophisticated math background, but I never covered this in my undergraduate education. I tried looking a bit into ""Matrix calculus"", but it wasn't much help. Is that the correct term? If this is the language used in the remainder of the textbook, what are some good resources somewhat familiar with vector calc and linear algebra to learn ""matrix calc""?","This question already has an answer here : Taking derivatives with respect to a matrix (1 answer) Closed 6 years ago . In the book ""Elements of Statistical Learning"", early on the author is discussing linear regression, and naturally discusses the residual sum of squares (RSS) based on the parameter space $\boldsymbol{\beta}$. In the general formulation, $\text{RSS}(\beta) = (\boldsymbol{y} - \boldsymbol{X}\beta)^T(\boldsymbol{y} - \boldsymbol{X}\beta)$ where $\boldsymbol{X}$ is an $N \times p$ matrix and $\beta$ is a $p \times K$ matrix. The author then says to minimize RSS, you differentiate with respect to $\beta$ and get $\boldsymbol{X}^T(\boldsymbol{y} - \boldsymbol{X}\beta) = 0$ My question is, what are the mechanics of differentiating with respect to the matrix $\beta$? I have a B.S. in physics, so I have a reasonably sophisticated math background, but I never covered this in my undergraduate education. I tried looking a bit into ""Matrix calculus"", but it wasn't much help. Is that the correct term? If this is the language used in the remainder of the textbook, what are some good resources somewhat familiar with vector calc and linear algebra to learn ""matrix calc""?",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'matrix-calculus']"
16,"Existence of $\lim_{(x,y,z)\to(0,0,0)}\frac{\ln(1+x^2y^4z^6)}{(x^2+y^2+z^2)^{\alpha+\frac{1}{2}}}$ for some $\alpha$.",Existence of  for some .,"\lim_{(x,y,z)\to(0,0,0)}\frac{\ln(1+x^2y^4z^6)}{(x^2+y^2+z^2)^{\alpha+\frac{1}{2}}} \alpha","Hy guys! for which values of $\alpha\in\mathbb R$ does this limit exists?  $$\lim_{(x,y,z)\to(0,0,0)}\frac{\ln(1+x^2y^4z^6)}{(x^2+y^2+z^2)^{\alpha+\frac{1}{2}}}$$ I know that for all $\alpha\leq-\frac{1}{2}$ the limit exists, but how can I argue to prove (or disprove) that this limit exists for $\alpha>-\frac{1}{2}$?","Hy guys! for which values of $\alpha\in\mathbb R$ does this limit exists?  $$\lim_{(x,y,z)\to(0,0,0)}\frac{\ln(1+x^2y^4z^6)}{(x^2+y^2+z^2)^{\alpha+\frac{1}{2}}}$$ I know that for all $\alpha\leq-\frac{1}{2}$ the limit exists, but how can I argue to prove (or disprove) that this limit exists for $\alpha>-\frac{1}{2}$?",,"['calculus', 'limits', 'multivariable-calculus']"
17,"Surface area of $x^2 + y^2 = 4, \quad 0 \leq z \leq 3$",Surface area of,"x^2 + y^2 = 4, \quad 0 \leq z \leq 3","I'm asked to calculate the surface area of  $$x^2 + y^2 = 4, \quad 0 \leq z \leq 3$$ using the surface integral. This is my attempt : For symmetry reasons I calculate the surface area when $y>0$ and then multiply by 2. For $y>0$ This surface can be parametrized by  $$(x,z) : (x, \sqrt{4-x^2}, z)$$ I set  $$r(s,t) = (2\cos{s}, 2\sin{s}, t) \quad 0 \leq s \leq 2\pi ,\quad 0 \leq t \leq 3$$ I find the normal to the surface expressed  in $(s,t)$ to be $$(2\cos{s}, 2\sin{s},0)$$ I find the Jacobian of the transformation $(x,z) --> (s,t)$ to be $-2\sin{s}$. Using the definition of surface integral I get:  $$\int\int\sqrt{4\sin^2{s} + 4\cos^2{s}} \cdot -2\sin{s} \quad dsdt$$ $$-4\int\int \sin{s} \quad dsdt$$ $$-4\int_{0}^{\pi}\sin{s}\quad ds \cdot \int_0^3 1\quad dt= -24$$ Now multiplying by 2 (to get both sides) I get -48 (I understand the answer can't be negative). And the answer is supposed to be $12\pi$. What am I doing wrong?","I'm asked to calculate the surface area of  $$x^2 + y^2 = 4, \quad 0 \leq z \leq 3$$ using the surface integral. This is my attempt : For symmetry reasons I calculate the surface area when $y>0$ and then multiply by 2. For $y>0$ This surface can be parametrized by  $$(x,z) : (x, \sqrt{4-x^2}, z)$$ I set  $$r(s,t) = (2\cos{s}, 2\sin{s}, t) \quad 0 \leq s \leq 2\pi ,\quad 0 \leq t \leq 3$$ I find the normal to the surface expressed  in $(s,t)$ to be $$(2\cos{s}, 2\sin{s},0)$$ I find the Jacobian of the transformation $(x,z) --> (s,t)$ to be $-2\sin{s}$. Using the definition of surface integral I get:  $$\int\int\sqrt{4\sin^2{s} + 4\cos^2{s}} \cdot -2\sin{s} \quad dsdt$$ $$-4\int\int \sin{s} \quad dsdt$$ $$-4\int_{0}^{\pi}\sin{s}\quad ds \cdot \int_0^3 1\quad dt= -24$$ Now multiplying by 2 (to get both sides) I get -48 (I understand the answer can't be negative). And the answer is supposed to be $12\pi$. What am I doing wrong?",,"['multivariable-calculus', 'surfaces', 'surface-integrals', 'jacobian']"
18,Question about changing variables in $\iiint_W x+3yz^2dV$,Question about changing variables in,\iiint_W x+3yz^2dV,"I've had a multivariable calculus exam with this problem, I needed to calculate following integral: $$\iiint_W x+3yz^2dV$$   $W = \{ (x,y,z) \in \Bbb R^3 : z^2 \geq 9x^2 + 9y^2, 0 \leq z \leq 3, x \geq 0\}$ My question is, would it make sense to make this variable change?: $x = r \sin \theta$ $y = r \cos \theta$ $z = z$ Boundaries for integrating would be: $0 \leq z \leq 3$ $0 \leq r \leq 1$ $0 \leq \theta \leq \pi$ I've tried it like this, but when I try substituting in the integral and multiplying it by the Jacobian's determinant it doesn't seem right. What would a best change of variables be? Any suggestions or ideas? Thanks!","I've had a multivariable calculus exam with this problem, I needed to calculate following integral: $$\iiint_W x+3yz^2dV$$   $W = \{ (x,y,z) \in \Bbb R^3 : z^2 \geq 9x^2 + 9y^2, 0 \leq z \leq 3, x \geq 0\}$ My question is, would it make sense to make this variable change?: $x = r \sin \theta$ $y = r \cos \theta$ $z = z$ Boundaries for integrating would be: $0 \leq z \leq 3$ $0 \leq r \leq 1$ $0 \leq \theta \leq \pi$ I've tried it like this, but when I try substituting in the integral and multiplying it by the Jacobian's determinant it doesn't seem right. What would a best change of variables be? Any suggestions or ideas? Thanks!",,"['integration', 'analysis', 'multivariable-calculus', 'definite-integrals', 'change-of-variable']"
19,Why write $\mathbb R\times \mathbb R^3\to\mathbb R^3$ instead of $\mathbb R^4\to\mathbb R^3$?,Why write  instead of ?,\mathbb R\times \mathbb R^3\to\mathbb R^3 \mathbb R^4\to\mathbb R^3,"I often see this notation for a vector field $$\mathbf A:\mathbb R\times \mathbb R^3\to\mathbb R^3$$ But isn't $\mathbb R\times \mathbb R^3=\mathbb R^4$, right? So, is there any advantages using $\mathbb R\times \mathbb R^3\rightarrow\mathbb R^3$ instead of $\mathbb{R}^4\rightarrow \mathbb{R}^3$?","I often see this notation for a vector field $$\mathbf A:\mathbb R\times \mathbb R^3\to\mathbb R^3$$ But isn't $\mathbb R\times \mathbb R^3=\mathbb R^4$, right? So, is there any advantages using $\mathbb R\times \mathbb R^3\rightarrow\mathbb R^3$ instead of $\mathbb{R}^4\rightarrow \mathbb{R}^3$?",,"['multivariable-calculus', 'vector-analysis']"
20,"An approximation as accurate as $\mathcal{o}(h^2+k^2)$ for $f(h, k)$ where $\displaystyle f(x,y) = \sqrt{1+4x^2+y^2}$.",An approximation as accurate as  for  where .,"\mathcal{o}(h^2+k^2) f(h, k) \displaystyle f(x,y) = \sqrt{1+4x^2+y^2}","Give an approximation as accurate as $\mathcal{o}(h^2+k^2)$ for $f(h, k)$ where $\displaystyle f(x,y) = \sqrt{1+4x^2+y^2}$. To clarify, I'm asked an expansion of $f$ at $(h, k)$ that's as accurate as $\mathcal{o}(h^2+k^2)$ $\leftarrow$ what's this? I know $\displaystyle f(x,y) = f(x_0,y_0)+\left(\Delta x \frac{\partial f}{\partial x}+\Delta y  \frac{\partial f}{\partial y}\right)+\frac{1}{2}\left( \Delta x \Delta y\frac{\partial^2 f}{\partial x \partial y}+\Delta x \Delta y\frac{\partial^2 f}{\partial x \partial y}\right)+\cdots$ evaluated at $(x, y) = (x_0, y_0)$, where $\Delta x = x-x_0$ and $\Delta y = y-y_0$. Is this correct and how do you apply it? Do I replace $x_0$ with $h$ and $y_0$ with $k$, and it's fine to still have $x$ and $y$ in the expansion for $f(h, k)$? Also, what's the meaning of $\mathcal{o}(h^2+k^2)$ and how does it differ from $f(h^2, k^2)$?","Give an approximation as accurate as $\mathcal{o}(h^2+k^2)$ for $f(h, k)$ where $\displaystyle f(x,y) = \sqrt{1+4x^2+y^2}$. To clarify, I'm asked an expansion of $f$ at $(h, k)$ that's as accurate as $\mathcal{o}(h^2+k^2)$ $\leftarrow$ what's this? I know $\displaystyle f(x,y) = f(x_0,y_0)+\left(\Delta x \frac{\partial f}{\partial x}+\Delta y  \frac{\partial f}{\partial y}\right)+\frac{1}{2}\left( \Delta x \Delta y\frac{\partial^2 f}{\partial x \partial y}+\Delta x \Delta y\frac{\partial^2 f}{\partial x \partial y}\right)+\cdots$ evaluated at $(x, y) = (x_0, y_0)$, where $\Delta x = x-x_0$ and $\Delta y = y-y_0$. Is this correct and how do you apply it? Do I replace $x_0$ with $h$ and $y_0$ with $k$, and it's fine to still have $x$ and $y$ in the expansion for $f(h, k)$? Also, what's the meaning of $\mathcal{o}(h^2+k^2)$ and how does it differ from $f(h^2, k^2)$?",,"['calculus', 'multivariable-calculus', 'taylor-expansion']"
21,How do I verify divergence theorem for given vector field and surface?,How do I verify divergence theorem for given vector field and surface?,,"The vector field is ${\bf{F}}\left( {x,y,z} \right) = x{y^2}{\bf{i}} + y{x^2}{\bf{j}} + e{\bf{k}}$ and the surface $S$ is bounded by $z = \sqrt {{x^2} + {y^2}} $ and $z = 4$.","The vector field is ${\bf{F}}\left( {x,y,z} \right) = x{y^2}{\bf{i}} + y{x^2}{\bf{j}} + e{\bf{k}}$ and the surface $S$ is bounded by $z = \sqrt {{x^2} + {y^2}} $ and $z = 4$.",,['calculus']
22,"Find work done by the force field $F(x,y) = (3y^2 + 2)i + 16xj$ along the upper half of an ellipse",Find work done by the force field  along the upper half of an ellipse,"F(x,y) = (3y^2 + 2)i + 16xj","I'm trying to solve the following problem. (10.13.10 from Apostol Vol. 2.) Let $$ F(x,y) = (3y^2 + 2)i + 16xj $$ be a force field. I want to find the work done by $F(x,y)$ from   $(-1,0)$ to $(1,0)$ along the upper half of the ellipse $b^2 x^2 + y^2 > = b^2$. Here's my solution attempt. The ellipse can be parametrized by $a(t) = \cos(t)i + b\sin(t)$ for $t \in [-\pi, \pi]$. So $$ F(a(t)) = (3(b \sin(t) )^2 + 2)i + 16\cos(t) j $$ $$ a'(t) = -\sin(t) i + b\cos(t) j .$$ Thus the work done is $$ \int_C F ds = \int_{-\pi}^\pi F(a(t)) \cdot a'(t) dt $$ which is equal to $$ \int_{-\pi}^\pi -3b^2 \sin(t)^3 - 2\sin(t) + 16b \cos^2(t) dt. $$ This integral evaluates to $16b\pi$. I checked it both by hand and using Mathematica. However, the solution is supposed to be $4b^2 - 8\pi b + 4$. Where is my solution attempt incorrect?","I'm trying to solve the following problem. (10.13.10 from Apostol Vol. 2.) Let $$ F(x,y) = (3y^2 + 2)i + 16xj $$ be a force field. I want to find the work done by $F(x,y)$ from   $(-1,0)$ to $(1,0)$ along the upper half of the ellipse $b^2 x^2 + y^2 > = b^2$. Here's my solution attempt. The ellipse can be parametrized by $a(t) = \cos(t)i + b\sin(t)$ for $t \in [-\pi, \pi]$. So $$ F(a(t)) = (3(b \sin(t) )^2 + 2)i + 16\cos(t) j $$ $$ a'(t) = -\sin(t) i + b\cos(t) j .$$ Thus the work done is $$ \int_C F ds = \int_{-\pi}^\pi F(a(t)) \cdot a'(t) dt $$ which is equal to $$ \int_{-\pi}^\pi -3b^2 \sin(t)^3 - 2\sin(t) + 16b \cos^2(t) dt. $$ This integral evaluates to $16b\pi$. I checked it both by hand and using Mathematica. However, the solution is supposed to be $4b^2 - 8\pi b + 4$. Where is my solution attempt incorrect?",,"['calculus', 'integration', 'multivariable-calculus', 'line-integrals']"
23,Showing differentiability of product of two functions,Showing differentiability of product of two functions,,"Let $\Omega$ be a nonempty open subset of $\mathbb{R}^n$ and let $f:\Omega\to \mathbb R$ be a differentiable function at $\mathbf{x}_0\in \Omega$ such that $f(\mathbf{x}_0)=0$. If $g$ is continuous at $x_0$, then I have to show that $fg$ on $\Omega$ defined by $fg(\mathbf{x})=f(\mathbf{x})g(\mathbf{x})$ is differentiable at $\mathbf{x}_0$. Any help is appreciated.","Let $\Omega$ be a nonempty open subset of $\mathbb{R}^n$ and let $f:\Omega\to \mathbb R$ be a differentiable function at $\mathbf{x}_0\in \Omega$ such that $f(\mathbf{x}_0)=0$. If $g$ is continuous at $x_0$, then I have to show that $fg$ on $\Omega$ defined by $fg(\mathbf{x})=f(\mathbf{x})g(\mathbf{x})$ is differentiable at $\mathbf{x}_0$. Any help is appreciated.",,"['real-analysis', 'multivariable-calculus', 'derivatives']"
24,Why is this directional derivative equal to $0$?,Why is this directional derivative equal to ?,0,"Let $A \subset \mathbb{R}^n$, with $A$ open, and let $\emptyset\ne S \subset A$.  Let $f\in C^1 (A, \mathbb{R} )$ be a function. Let $\vec{a}$ be a point in $S$, and suppose that $\vec{a}$ is a local extremum point for $f \mid S$. Prove that the gradient vector $( \nabla f ) ( \vec{a} )$ is orthogonal to every vector $\vec{v}$ which is tangent to $S$ at $\vec{a}$. Aside (definition of a vector tangent to a set): Let $\emptyset\ne S\subset\mathbb{R}^n$ and let $\vec{a}\in S$.  A vector $\vec{v} \in \mathbb{R}^n$ is said to be tangent to $S$ at $\vec{a}$ when there exists a differentiable path    $\gamma : I \to \mathbb{R}^n$, where $I \subseteq \mathbb{R}$ is an open interval containing $0$, such that the following conditions are fulfilled: (i) $\gamma (t) \in S$ for every $t \in I$; (ii) $\gamma (0) = \vec{a}$; (iii) $\gamma ' (0) = \vec{v}$. The approach I used is as follows: Let $u:I\to\mathbb{R}$ be a function, such that $u(t) = f(\gamma(t)), t\in I$. Then $u$ is differentiable, with $$u'(t)=\langle (\nabla f(\gamma(t)),\gamma'(t) \rangle$$ $$D_{\vec{v}}(\vec{a})=u'(0)=\langle (\nabla f(\gamma(0)),\gamma'(0) \rangle=\langle \nabla f(\vec{a}), \vec{v} \rangle$$ Now I'm wondering why $D_{\vec{v}}(f(\vec{a}))$ should be equal to $0$. This must somehow be related to the fact that $\vec{a}$ is a local extremum of $f$. But how exactly does this make $D_{\vec{v}}(\vec{a})=0$?","Let $A \subset \mathbb{R}^n$, with $A$ open, and let $\emptyset\ne S \subset A$.  Let $f\in C^1 (A, \mathbb{R} )$ be a function. Let $\vec{a}$ be a point in $S$, and suppose that $\vec{a}$ is a local extremum point for $f \mid S$. Prove that the gradient vector $( \nabla f ) ( \vec{a} )$ is orthogonal to every vector $\vec{v}$ which is tangent to $S$ at $\vec{a}$. Aside (definition of a vector tangent to a set): Let $\emptyset\ne S\subset\mathbb{R}^n$ and let $\vec{a}\in S$.  A vector $\vec{v} \in \mathbb{R}^n$ is said to be tangent to $S$ at $\vec{a}$ when there exists a differentiable path    $\gamma : I \to \mathbb{R}^n$, where $I \subseteq \mathbb{R}$ is an open interval containing $0$, such that the following conditions are fulfilled: (i) $\gamma (t) \in S$ for every $t \in I$; (ii) $\gamma (0) = \vec{a}$; (iii) $\gamma ' (0) = \vec{v}$. The approach I used is as follows: Let $u:I\to\mathbb{R}$ be a function, such that $u(t) = f(\gamma(t)), t\in I$. Then $u$ is differentiable, with $$u'(t)=\langle (\nabla f(\gamma(t)),\gamma'(t) \rangle$$ $$D_{\vec{v}}(\vec{a})=u'(0)=\langle (\nabla f(\gamma(0)),\gamma'(0) \rangle=\langle \nabla f(\vec{a}), \vec{v} \rangle$$ Now I'm wondering why $D_{\vec{v}}(f(\vec{a}))$ should be equal to $0$. This must somehow be related to the fact that $\vec{a}$ is a local extremum of $f$. But how exactly does this make $D_{\vec{v}}(\vec{a})=0$?",,"['real-analysis', 'multivariable-calculus', 'proof-verification', 'vector-analysis']"
25,Integrate outward unit vector over the surface of a sphere,Integrate outward unit vector over the surface of a sphere,,"I'm trying to understand an integration over the surface of a sphere that is used in one of the articles I'm reading. I don't know why I can't understand it as it seems to be a pretty straightforward integration and I have been used to more complex math but anyway. Let $\textbf{r} = \textbf{x - x'}$, $r = |\textbf{r}|$ and $\textbf{n} = \frac{\textbf{r}}{r}$ where $|.|$ is the euclidian norm. The goal is to calculate the following integral for $i, j \in \{1,2,3\}$ : $$I = \int_{A(r)} n_{i}n_{j}\text{d}A$$ Where $A(r)$ denotes a spherical surface of radius $r$. The way I've gone about it is to say that for a spherical surface of radius $r$ we have $$\text{d}A = r\sin(\phi)\text{d}\phi\text{d}\theta$$ with $(\phi, \theta) \in [0,\pi]\times[0,2\pi]$ and since $r_{i}$ or $r_{j}$ are not dependent of the angles we should have $$I = 4\pi r_{i}r_{j}$$ However the article I'm reading has the result $$I = \frac{4\pi r^{2}}{3} \delta_{ij}$$ I've been thinking about it but can't seem to find my mistake. Thanks for your help kind stranger :)","I'm trying to understand an integration over the surface of a sphere that is used in one of the articles I'm reading. I don't know why I can't understand it as it seems to be a pretty straightforward integration and I have been used to more complex math but anyway. Let $\textbf{r} = \textbf{x - x'}$, $r = |\textbf{r}|$ and $\textbf{n} = \frac{\textbf{r}}{r}$ where $|.|$ is the euclidian norm. The goal is to calculate the following integral for $i, j \in \{1,2,3\}$ : $$I = \int_{A(r)} n_{i}n_{j}\text{d}A$$ Where $A(r)$ denotes a spherical surface of radius $r$. The way I've gone about it is to say that for a spherical surface of radius $r$ we have $$\text{d}A = r\sin(\phi)\text{d}\phi\text{d}\theta$$ with $(\phi, \theta) \in [0,\pi]\times[0,2\pi]$ and since $r_{i}$ or $r_{j}$ are not dependent of the angles we should have $$I = 4\pi r_{i}r_{j}$$ However the article I'm reading has the result $$I = \frac{4\pi r^{2}}{3} \delta_{ij}$$ I've been thinking about it but can't seem to find my mistake. Thanks for your help kind stranger :)",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
26,Why are this function's second partial derivatives not continuous?,Why are this function's second partial derivatives not continuous?,,"so we know that for a function's mixed partial derivatives to be symmetrical we need their second partial derivatives to be continuous. In this example f(x,y) =   \begin{array}{l l}     \dfrac{xy(x^2-y^2)}{x^2+y^2} & \quad \text{for $(x,y) \neq (0,0)$}\\     0 & \quad \text{for $(x,y)=(0,0)$}   \end{array} they $Fxy$ and $Fyx$ are not the same in $(0,0)$, but I'm not so sure how to prove their second derivatives are not continous in that point, how would I go about it?","so we know that for a function's mixed partial derivatives to be symmetrical we need their second partial derivatives to be continuous. In this example f(x,y) =   \begin{array}{l l}     \dfrac{xy(x^2-y^2)}{x^2+y^2} & \quad \text{for $(x,y) \neq (0,0)$}\\     0 & \quad \text{for $(x,y)=(0,0)$}   \end{array} they $Fxy$ and $Fyx$ are not the same in $(0,0)$, but I'm not so sure how to prove their second derivatives are not continous in that point, how would I go about it?",,['multivariable-calculus']
27,How to find the area of the surface enclosed between a sphere $x^2+y^2+z^2=4a^2$ and a cylinder $x^2+(y-a)^2=a^2$?,How to find the area of the surface enclosed between a sphere  and a cylinder ?,x^2+y^2+z^2=4a^2 x^2+(y-a)^2=a^2,"Find the area of the surface enclosed between a sphere $x^2+y^2+z^2=4a^2$ and a cylinder $x^2+(y-a)^2=a^2$. The correct answer should be $(8\pi-16)a^2$. This is an illustration: First, there's the following formula to calculate surface area: $$ \int\int_R\sqrt{Z_x^2+Z_y^2+1}\cdot dA $$ In our case it's: $$ \int\int_R \sqrt{\frac{x^2+y^2}{4a^2-x^2-y^2}+1}dA $$ We can move into polar coordinates (and multiply the original operand by $r$ as the Jacobian in polar), then: $$ \int\int_R\ r\sqrt{\frac{r^2}{4a^2-r^2}+1}drd\theta $$ Now the only thing left is to find the bounds of $r$ and $\theta$. The projection of the surface to $xy$ plane will be a circle with the center in $(0,a)$ with the radius $a$ so it's symmetric around $y$ axis. Then $0\le\theta\le\pi$. Because $x^2+y^2=2ay$ then $r=2a\sin\theta$, therefore we now have the integral: $$ \int_0^{\pi}\int_0^{2a\sin\theta}r\sqrt{\frac{r^2}{4a^2-r^2}+1}drd\theta $$ And this is where I'm stuck. How can I integrate $dr$?","Find the area of the surface enclosed between a sphere $x^2+y^2+z^2=4a^2$ and a cylinder $x^2+(y-a)^2=a^2$. The correct answer should be $(8\pi-16)a^2$. This is an illustration: First, there's the following formula to calculate surface area: $$ \int\int_R\sqrt{Z_x^2+Z_y^2+1}\cdot dA $$ In our case it's: $$ \int\int_R \sqrt{\frac{x^2+y^2}{4a^2-x^2-y^2}+1}dA $$ We can move into polar coordinates (and multiply the original operand by $r$ as the Jacobian in polar), then: $$ \int\int_R\ r\sqrt{\frac{r^2}{4a^2-r^2}+1}drd\theta $$ Now the only thing left is to find the bounds of $r$ and $\theta$. The projection of the surface to $xy$ plane will be a circle with the center in $(0,a)$ with the radius $a$ so it's symmetric around $y$ axis. Then $0\le\theta\le\pi$. Because $x^2+y^2=2ay$ then $r=2a\sin\theta$, therefore we now have the integral: $$ \int_0^{\pi}\int_0^{2a\sin\theta}r\sqrt{\frac{r^2}{4a^2-r^2}+1}drd\theta $$ And this is where I'm stuck. How can I integrate $dr$?",,"['integration', 'multivariable-calculus', 'polar-coordinates']"
28,Why $\rho=\cos \phi$ in $x^2+y^2+(z-\frac{1}{2})^2=\frac{1}{4}$ and $0 \le \phi \le \pi/4$?,Why  in  and ?,\rho=\cos \phi x^2+y^2+(z-\frac{1}{2})^2=\frac{1}{4} 0 \le \phi \le \pi/4,"I need to calculate the volume of cone $z=\sqrt{x^2+y^2}$ inside the sphere $x^2+y^2+(z-\frac{1}{2})^2=\frac{1}{4}$ using spherical coordinates, triple integral and $f(\rho, \phi, \theta)$ notation. Intuitively, I'd like to say that the radius of the sphere is $0.5$ and this should be the upper bound of $\rho$. However, we need to use $\rho \le \cos\phi$. The algebraical explanation for this is: $$ x^2+y^2+z^2-z+\frac{1}{4}=\frac{1}{4} \Rightarrow x^2+y^2+z^2=z\\ z=\rho\cos\phi \Rightarrow \rho=z=\rho\cos\phi $$ I don't understand the reasoning behind this though. Is it because $\rho$ doesn't start in the origin so we need to make the adjustment? And secondly, aren't we interested in the angle $\pi/4 \le \phi \le \pi/2$ because this is actually where the cone is situated? This is the illustration:","I need to calculate the volume of cone $z=\sqrt{x^2+y^2}$ inside the sphere $x^2+y^2+(z-\frac{1}{2})^2=\frac{1}{4}$ using spherical coordinates, triple integral and $f(\rho, \phi, \theta)$ notation. Intuitively, I'd like to say that the radius of the sphere is $0.5$ and this should be the upper bound of $\rho$. However, we need to use $\rho \le \cos\phi$. The algebraical explanation for this is: $$ x^2+y^2+z^2-z+\frac{1}{4}=\frac{1}{4} \Rightarrow x^2+y^2+z^2=z\\ z=\rho\cos\phi \Rightarrow \rho=z=\rho\cos\phi $$ I don't understand the reasoning behind this though. Is it because $\rho$ doesn't start in the origin so we need to make the adjustment? And secondly, aren't we interested in the angle $\pi/4 \le \phi \le \pi/2$ because this is actually where the cone is situated? This is the illustration:",,"['integration', 'multivariable-calculus', 'spherical-coordinates']"
29,"How to take $\frac{\partial }{\partial x}\langle x,y \rangle$",How to take,"\frac{\partial }{\partial x}\langle x,y \rangle","I confess this is a silly question. I need to take the partial derivatives of $f(x,y):\mathbb{R}^{2n}\to\mathbb{R}$ of $f(x,y) = \langle x,y\rangle$. That is: $$\frac{\partial }{\partial x}\langle x,y \rangle, \frac{\partial }{\partial y}\langle x,y \rangle$$ But how to view this as a limit? For example: $$\lim_{t\to 0} \frac{\langle x+t,y\rangle-\langle x, t\rangle}{t}$$ I cannot expand this into anything useful. How do I take this gradient?","I confess this is a silly question. I need to take the partial derivatives of $f(x,y):\mathbb{R}^{2n}\to\mathbb{R}$ of $f(x,y) = \langle x,y\rangle$. That is: $$\frac{\partial }{\partial x}\langle x,y \rangle, \frac{\partial }{\partial y}\langle x,y \rangle$$ But how to view this as a limit? For example: $$\lim_{t\to 0} \frac{\langle x+t,y\rangle-\langle x, t\rangle}{t}$$ I cannot expand this into anything useful. How do I take this gradient?",,"['calculus', 'multivariable-calculus', 'derivatives']"
30,"Find partial derivatives of $f(x,y) = g(e^{xy^2}, \sin x+\cos y)$",Find partial derivatives of,"f(x,y) = g(e^{xy^2}, \sin x+\cos y)","Find partial derivatives of $f(x,y) = g(e^{xy^2}, \sin x+\cos y)$, with $g:\mathbb{R}^2 \to \mathbb{R}$ a differentiable function. So I called $u(x,y) = e^{xy^2}$ and $v(x,y)=\sin x+\cos y$ Then, I calculated $\frac{\partial f}{\partial x} = \frac{\partial g}{\partial u}\frac{\partial u}{\partial x} + \frac{\partial g}{\partial v}\frac{\partial v}{\partial x} = y^2e^{xy^2}\frac{\partial g}{\partial u} + \cos x\frac{\partial g}{\partial v}$ And $\frac{\partial f}{\partial y} = \frac{\partial g}{\partial u}\frac{\partial u}{\partial y} + \frac{\partial g}{\partial v}\frac{\partial v}{\partial y} = 2xye^{xy^2}\frac{\partial g}{\partial u} - \sin y\frac{\partial g}{\partial v}$ Now, I don't how to proceed, I have the derivatives in function of $u(x,y)$ and $v(x,y)$, not only on $x$ and $y$. Any help?","Find partial derivatives of $f(x,y) = g(e^{xy^2}, \sin x+\cos y)$, with $g:\mathbb{R}^2 \to \mathbb{R}$ a differentiable function. So I called $u(x,y) = e^{xy^2}$ and $v(x,y)=\sin x+\cos y$ Then, I calculated $\frac{\partial f}{\partial x} = \frac{\partial g}{\partial u}\frac{\partial u}{\partial x} + \frac{\partial g}{\partial v}\frac{\partial v}{\partial x} = y^2e^{xy^2}\frac{\partial g}{\partial u} + \cos x\frac{\partial g}{\partial v}$ And $\frac{\partial f}{\partial y} = \frac{\partial g}{\partial u}\frac{\partial u}{\partial y} + \frac{\partial g}{\partial v}\frac{\partial v}{\partial y} = 2xye^{xy^2}\frac{\partial g}{\partial u} - \sin y\frac{\partial g}{\partial v}$ Now, I don't how to proceed, I have the derivatives in function of $u(x,y)$ and $v(x,y)$, not only on $x$ and $y$. Any help?",,"['multivariable-calculus', 'partial-derivative']"
31,"Given $F(x,y,z)=0$, $\frac{\partial x}{\partial y} \frac{\partial y}{\partial z}\frac{\partial z}{\partial x}$ should be -1, but I got 1","Given ,  should be -1, but I got 1","F(x,y,z)=0 \frac{\partial x}{\partial y} \frac{\partial y}{\partial z}\frac{\partial z}{\partial x}","What went wrong in my calculation? Given $F(x,y,z)=0$ , $F_x\neq 0 , F_y\neq 0, F_z\neq 0$, and $z=f(x,y), y=g(x,z), x=h(y,z)$ $F_x\quad\,+F_y\frac{\partial y}{\partial x} + F_z \frac{\partial z}{\partial x} = 0\\ F_x\frac{\partial x}{\partial y}+F_y\quad\,+F_z\frac{\partial z}{\partial y}=0$ Then we have: $F_y(\frac{\partial x}{\partial y}\frac{\partial y}{\partial x}-1)+F_z(\frac{\partial z}{\partial x}\frac{\partial x}{\partial y}-\frac{\partial z}{\partial y}) =0$ Let $P(x,y)=F(x,y,f(x,y))\\ P_x+P_y\frac{\partial y}{\partial x}=0\\ \frac{\partial y}{\partial x} = -\frac{P_x}{P_y}, similarly, \frac{\partial x}{\partial y} = -\frac{P_y}{P_x}, \frac{\partial x}{\partial y}\frac{\partial y}{\partial x} = 1$ $F_y(1-1)+F_z(\frac{\partial z}{\partial x} \frac{\partial x}{\partial y} - \frac{\partial z}{\partial y}) = 0\\ \frac{\partial z}{\partial x}\frac{\partial x}{\partial y} - \frac{\partial z}{\partial y}=0\\ \frac{\partial z}{\partial x}\frac{\partial x}{\partial y}\frac{\partial y}{\partial z} -\frac{\partial z}{\partial y}\frac{\partial y}{\partial z} = 0\\ \frac{\partial z}{\partial x}\frac{\partial x}{\partial y}\frac{\partial y}{\partial z} = 1$","What went wrong in my calculation? Given $F(x,y,z)=0$ , $F_x\neq 0 , F_y\neq 0, F_z\neq 0$, and $z=f(x,y), y=g(x,z), x=h(y,z)$ $F_x\quad\,+F_y\frac{\partial y}{\partial x} + F_z \frac{\partial z}{\partial x} = 0\\ F_x\frac{\partial x}{\partial y}+F_y\quad\,+F_z\frac{\partial z}{\partial y}=0$ Then we have: $F_y(\frac{\partial x}{\partial y}\frac{\partial y}{\partial x}-1)+F_z(\frac{\partial z}{\partial x}\frac{\partial x}{\partial y}-\frac{\partial z}{\partial y}) =0$ Let $P(x,y)=F(x,y,f(x,y))\\ P_x+P_y\frac{\partial y}{\partial x}=0\\ \frac{\partial y}{\partial x} = -\frac{P_x}{P_y}, similarly, \frac{\partial x}{\partial y} = -\frac{P_y}{P_x}, \frac{\partial x}{\partial y}\frac{\partial y}{\partial x} = 1$ $F_y(1-1)+F_z(\frac{\partial z}{\partial x} \frac{\partial x}{\partial y} - \frac{\partial z}{\partial y}) = 0\\ \frac{\partial z}{\partial x}\frac{\partial x}{\partial y} - \frac{\partial z}{\partial y}=0\\ \frac{\partial z}{\partial x}\frac{\partial x}{\partial y}\frac{\partial y}{\partial z} -\frac{\partial z}{\partial y}\frac{\partial y}{\partial z} = 0\\ \frac{\partial z}{\partial x}\frac{\partial x}{\partial y}\frac{\partial y}{\partial z} = 1$",,"['multivariable-calculus', 'partial-derivative']"
32,Example of a continuous and Gâteaux differentiable function that is not Fréchet differentiable.,Example of a continuous and Gâteaux differentiable function that is not Fréchet differentiable.,,"Is there an example of a function $f:\mathbb{R}^2 \to \mathbb{R}$, with $f(0,0) = 0$, that is Gâteaux differentiable (all directional derivatives exist) and continuous at $(0,0)$, but is not Fréchet differentiable at $(0,0)$? Edit: By Gâteaux differentiable, I use the definition that the Gâteaux derivative is not required to be a linear map, but just all the directional derivatives to exist in all directions.","Is there an example of a function $f:\mathbb{R}^2 \to \mathbb{R}$, with $f(0,0) = 0$, that is Gâteaux differentiable (all directional derivatives exist) and continuous at $(0,0)$, but is not Fréchet differentiable at $(0,0)$? Edit: By Gâteaux differentiable, I use the definition that the Gâteaux derivative is not required to be a linear map, but just all the directional derivatives to exist in all directions.",,"['calculus', 'real-analysis', 'functional-analysis', 'multivariable-calculus', 'frechet-derivative']"
33,"Studying continuity of $f(x,y) = \left( \frac{\sin(x^2+y^2)}{x^2 + y^2},\frac{e^{x^2+y^2}-1}{x^2 + y^2} \right)$",Studying continuity of,"f(x,y) = \left( \frac{\sin(x^2+y^2)}{x^2 + y^2},\frac{e^{x^2+y^2}-1}{x^2 + y^2} \right)","I need to study the continuity of this function: $$f(x,y) = \left( \frac{\sin(x^2+y^2)}{x^2 + y^2},\frac{e^{x^2+y^2}-1}{x^2 + y^2} \right)$$ It's from an exercise of a calculus textbook. As they are continuous functions I know that the only problematic point is the origin... I know from previous exercises that the limit of $\frac{\sin(x)}{x}$ when $x$ approaches $0$ is $1$. But I don't know how to prove that the $y$ coordinate is algo going to $1$. Could you help me with this? Thank you.","I need to study the continuity of this function: $$f(x,y) = \left( \frac{\sin(x^2+y^2)}{x^2 + y^2},\frac{e^{x^2+y^2}-1}{x^2 + y^2} \right)$$ It's from an exercise of a calculus textbook. As they are continuous functions I know that the only problematic point is the origin... I know from previous exercises that the limit of $\frac{\sin(x)}{x}$ when $x$ approaches $0$ is $1$. But I don't know how to prove that the $y$ coordinate is algo going to $1$. Could you help me with this? Thank you.",,"['limits', 'multivariable-calculus', 'continuity']"
34,"$f:U\to\mathbb{R}^n$ is of class $C^1$. If for $a\in U$ ($U$ open), $f'(a)$ is injective, exists ball where $|f(x)-f(y)\ge c|x-y||$","is of class . If for  ( open),  is injective, exists ball where",f:U\to\mathbb{R}^n C^1 a\in U U f'(a) |f(x)-f(y)\ge c|x-y||,"$f:U\to\mathbb{R}^n$ is of class $C^1$ in the open $U\subset  \mathbb{R}^m$. If for some $a\in U$,   $f'(a):\mathbb{R}^m\to\mathbb{R}^n$ is injective, then there exists   $\delta>0$ and $c>0$ such that $B = B(a,\delta)\subset U$ and, for any   $x,y\in B$ we have  $|f(x)-f(y)\ge c|x-y||$. In particular, the   restriction $f_{|_ B}$ is injective. Proof given by book: The function $u\to |f'(a)\cdot u|$ is positive in all the points $u$   of the unit sphere $S^{m-1}$ (what - 1) , which is compact. By the Weiestrass   theorem, there exists $c>0$ such that $|f'(a)\cdot u| \ge 2c$ for all   $u\in S^{m-1}$ (what? - 2) . By linearity, follow that $|f'(a)\cdot  v|\ge 2c\cdot |v|$ for all $v\in \mathbb{R}^m$ (what? - 3) . For all   $x\in U$, we write: $$r(x) = f(x)-f(a) -f'(a)(x-a)$$ Then, for any $x,y\in U$ we have: $$f(x)-f(y) = f'(a)\cdot (x-y) + r(x)-r(y)$$ By $|u+v|\ge |u|-|v|$ follows: $$|f(x)-f(y)|\ge |f'(a)\cdot(x-y)|-|r(x)-r(y)|\ge\\ 2c\cdot  |x-y|-|t(x)-r(y)|$$ Observe that $r$, as defined above, is of class $C^1$, with $r(a) = > 0$. By the continuity of $r'$, there exists $\delta >0$ such that   $|x-a|<\delta \implies x \in U $ and $|r'(x)|<c$. The mean value   inequality applied in $r$ in the convex set $B = B(x,\delta)$ makes   $x,y\in B \implies |f(x)-f(y)|\ge 2c|x-y|-|x-y|$, that is,   $$|f(x)-f(y)|\ge c|x-y|$$ what - 1: couldn't it be $0$? what - 2: if it's already positive, then why do I need Weiertrass theorem? If it's positive then it's always greater than a $c$, I can just make it smaller so it's bigger than $2c$ what - 3: how linearity makes that possible? Also, which Weiertrass theorem is applied here? There are many.","$f:U\to\mathbb{R}^n$ is of class $C^1$ in the open $U\subset  \mathbb{R}^m$. If for some $a\in U$,   $f'(a):\mathbb{R}^m\to\mathbb{R}^n$ is injective, then there exists   $\delta>0$ and $c>0$ such that $B = B(a,\delta)\subset U$ and, for any   $x,y\in B$ we have  $|f(x)-f(y)\ge c|x-y||$. In particular, the   restriction $f_{|_ B}$ is injective. Proof given by book: The function $u\to |f'(a)\cdot u|$ is positive in all the points $u$   of the unit sphere $S^{m-1}$ (what - 1) , which is compact. By the Weiestrass   theorem, there exists $c>0$ such that $|f'(a)\cdot u| \ge 2c$ for all   $u\in S^{m-1}$ (what? - 2) . By linearity, follow that $|f'(a)\cdot  v|\ge 2c\cdot |v|$ for all $v\in \mathbb{R}^m$ (what? - 3) . For all   $x\in U$, we write: $$r(x) = f(x)-f(a) -f'(a)(x-a)$$ Then, for any $x,y\in U$ we have: $$f(x)-f(y) = f'(a)\cdot (x-y) + r(x)-r(y)$$ By $|u+v|\ge |u|-|v|$ follows: $$|f(x)-f(y)|\ge |f'(a)\cdot(x-y)|-|r(x)-r(y)|\ge\\ 2c\cdot  |x-y|-|t(x)-r(y)|$$ Observe that $r$, as defined above, is of class $C^1$, with $r(a) = > 0$. By the continuity of $r'$, there exists $\delta >0$ such that   $|x-a|<\delta \implies x \in U $ and $|r'(x)|<c$. The mean value   inequality applied in $r$ in the convex set $B = B(x,\delta)$ makes   $x,y\in B \implies |f(x)-f(y)|\ge 2c|x-y|-|x-y|$, that is,   $$|f(x)-f(y)|\ge c|x-y|$$ what - 1: couldn't it be $0$? what - 2: if it's already positive, then why do I need Weiertrass theorem? If it's positive then it's always greater than a $c$, I can just make it smaller so it's bigger than $2c$ what - 3: how linearity makes that possible? Also, which Weiertrass theorem is applied here? There are many.",,"['calculus', 'real-analysis', 'functions', 'multivariable-calculus']"
35,Jacobian question -,Jacobian question -,,"I am stuck on a question in my calc III class which is shown above (part a). I completely understand how to find the Jacobian; however I don't understand why the relationship shown is true. How do I find the inverse transformation? In terms of the Jacobian, I got $$J = \frac{1}{2u}$$. Thanks for your time.","I am stuck on a question in my calc III class which is shown above (part a). I completely understand how to find the Jacobian; however I don't understand why the relationship shown is true. How do I find the inverse transformation? In terms of the Jacobian, I got $$J = \frac{1}{2u}$$. Thanks for your time.",,"['calculus', 'multivariable-calculus']"
36,Multivariable limit using polar coordinates?,Multivariable limit using polar coordinates?,,"$$\lim_{(x,y)\to(0,0)} \frac {2\sin (x^2 + y^2) + y^3}{3x^2+3y^2}$$ I am actually very new to polar coordinates so I'm a little bit confused about this part. I substitute this into this: $\frac {2sin (r^2) + (r cos \theta)^3}{3r^2}$ But I am not even sure if it's right and what should I do after this. Does the limit even exist? I need an explanation. Thank you!",I am actually very new to polar coordinates so I'm a little bit confused about this part. I substitute this into this: But I am not even sure if it's right and what should I do after this. Does the limit even exist? I need an explanation. Thank you!,"\lim_{(x,y)\to(0,0)} \frac {2\sin (x^2 + y^2) + y^3}{3x^2+3y^2} \frac {2sin (r^2) + (r cos \theta)^3}{3r^2}","['calculus', 'limits', 'multivariable-calculus']"
37,Green's Theorem for area using polar coordinates,Green's Theorem for area using polar coordinates,,"I've come across a Green's Theorem proof that has me perplexed. Using the area formula: $$A = \frac{1}{2}\int_C xdy - ydx $$ Prove that:$$A = \frac{1}{2}\int_a^b r^2d\theta$$for a region in polar coordinates. I assume a parametrisation is needed, but I'm not sure where to start due to the change in variables. My first thoughts are to change coordinates to $x=rcos\theta$ and $y=rsin\theta$. I also have assumed the $r^2$ is a result of the Jacobian being $r$ and some simplification of the $cos^2\theta + sin^2\theta$ identity. Any help would be appreciated. Thanks!","I've come across a Green's Theorem proof that has me perplexed. Using the area formula: $$A = \frac{1}{2}\int_C xdy - ydx $$ Prove that:$$A = \frac{1}{2}\int_a^b r^2d\theta$$for a region in polar coordinates. I assume a parametrisation is needed, but I'm not sure where to start due to the change in variables. My first thoughts are to change coordinates to $x=rcos\theta$ and $y=rsin\theta$. I also have assumed the $r^2$ is a result of the Jacobian being $r$ and some simplification of the $cos^2\theta + sin^2\theta$ identity. Any help would be appreciated. Thanks!",,"['multivariable-calculus', 'proof-verification', 'polar-coordinates', 'greens-theorem', 'change-of-variable']"
38,Finding a parametric representation for a boundary of $S$,Finding a parametric representation for a boundary of,S,"the question is: Let $\Omega$ denote the conical region $\sqrt{x^2 + y^2} \leq z \leq 2$. Find a parametric representation $x(u,v)$ for $S = \partial \Omega$, the boundary of $\Omega$. (You'll need to split it into two parts). I have no clue how to do this. What does it mean by boundary? Does it mean $z=2$ and $z = \sqrt{x^2 + y^2}$?How would I approach finding a parametric for this?","the question is: Let $\Omega$ denote the conical region $\sqrt{x^2 + y^2} \leq z \leq 2$. Find a parametric representation $x(u,v)$ for $S = \partial \Omega$, the boundary of $\Omega$. (You'll need to split it into two parts). I have no clue how to do this. What does it mean by boundary? Does it mean $z=2$ and $z = \sqrt{x^2 + y^2}$?How would I approach finding a parametric for this?",,"['multivariable-calculus', 'parametrization']"
39,Polar coordinates unit vectors proof [duplicate],Polar coordinates unit vectors proof [duplicate],,This question already has answers here : relationship of polar unit vectors to rectangular (5 answers) Closed 1 year ago . Prove that the unit vectors in polar coordinates are related to those in rectangular coordinates by \begin{align*} \hat{r}&=\hat{x}\cos\phi+\hat{y}\sin\phi\\ \hat{\phi}&=-\hat{x}\sin\phi+\hat{y}\cos\phi. \end{align*} What are $\hat{x}$ and $\hat{y}$ in terms of $\hat{r}$ and $\hat{\phi}$?,This question already has answers here : relationship of polar unit vectors to rectangular (5 answers) Closed 1 year ago . Prove that the unit vectors in polar coordinates are related to those in rectangular coordinates by \begin{align*} \hat{r}&=\hat{x}\cos\phi+\hat{y}\sin\phi\\ \hat{\phi}&=-\hat{x}\sin\phi+\hat{y}\cos\phi. \end{align*} What are $\hat{x}$ and $\hat{y}$ in terms of $\hat{r}$ and $\hat{\phi}$?,,"['multivariable-calculus', 'polar-coordinates']"
40,Injective derivative implies locally injective function,Injective derivative implies locally injective function,,"I am working on a proof for my real analysis class, and got stuck. Let $U$ be an open subset of $\mathbb{R}^n$. Let $f: U \to \mathbb{R}^m$ be a continuously differentiable map, and further suppose that $Df(x_0)$ is injective for some $x_0 \in U$. Show that there exists an open set $U_1 \subset U$ containing $x_0$ such that $f$ restricted to $U_1$ is injective. I have already shown that for $x, y, x_0 \in U$, we have that $||f(x) - f(y) - Df(x_0)(x-y)|| \leq ||x-y||\sup||Df(v)-Df(x_0)||$ (*) by using the mean value inequality. Here I am taking the supremum over $v$, where $v$ is in the line segment connecting $x$ and $y$. I was given a hint to apply this inequality to show that for some open ball $B_r (x_0)$, we have that $||f(x_1) - f(x_2)|| \geq C||x_1 - x_2||$ (**) for some constant $C$. It is clear to me that injectivity follows from the second inequality (**), but I am struggling to show that this inequality is true. The injectivity of $Df(x_0)$ gives us that $Df(x_0)(x-y) \neq 0$ for $x \neq y$, and I tried using this fact to break up the left hand side of (*) by, e.g., using the triangle inequality, but was unable to make much progress.","I am working on a proof for my real analysis class, and got stuck. Let $U$ be an open subset of $\mathbb{R}^n$. Let $f: U \to \mathbb{R}^m$ be a continuously differentiable map, and further suppose that $Df(x_0)$ is injective for some $x_0 \in U$. Show that there exists an open set $U_1 \subset U$ containing $x_0$ such that $f$ restricted to $U_1$ is injective. I have already shown that for $x, y, x_0 \in U$, we have that $||f(x) - f(y) - Df(x_0)(x-y)|| \leq ||x-y||\sup||Df(v)-Df(x_0)||$ (*) by using the mean value inequality. Here I am taking the supremum over $v$, where $v$ is in the line segment connecting $x$ and $y$. I was given a hint to apply this inequality to show that for some open ball $B_r (x_0)$, we have that $||f(x_1) - f(x_2)|| \geq C||x_1 - x_2||$ (**) for some constant $C$. It is clear to me that injectivity follows from the second inequality (**), but I am struggling to show that this inequality is true. The injectivity of $Df(x_0)$ gives us that $Df(x_0)(x-y) \neq 0$ for $x \neq y$, and I tried using this fact to break up the left hand side of (*) by, e.g., using the triangle inequality, but was unable to make much progress.",,['real-analysis']
41,Implicit differentiation of a two variable function,Implicit differentiation of a two variable function,,"Let $f(u,v)$ be a differentiable function of two variables, and let z be a differentiable function of x and y defined implicitly by $f(xz,yz)$ = 0. Prove that $x\frac{\partial z}{\partial x}+y\frac{\partial z}{\partial y} = -z$. I've got no idea how to start this. Any guidance would be appreciated. Thanks in advance!","Let $f(u,v)$ be a differentiable function of two variables, and let z be a differentiable function of x and y defined implicitly by $f(xz,yz)$ = 0. Prove that $x\frac{\partial z}{\partial x}+y\frac{\partial z}{\partial y} = -z$. I've got no idea how to start this. Any guidance would be appreciated. Thanks in advance!",,"['multivariable-calculus', 'implicit-differentiation']"
42,Find the domain and the range of $ \arcsin \frac{2x-y}{x+y}$? [closed],Find the domain and the range of ? [closed], \arcsin \frac{2x-y}{x+y},"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Well, the domain of arcsin is $-1 \leq \arcsin x \leq 1$, so I can write that $-1 \leq  \frac{2x-y}{x+y} \leq 1$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Well, the domain of arcsin is $-1 \leq \arcsin x \leq 1$, so I can write that $-1 \leq  \frac{2x-y}{x+y} \leq 1$",,"['real-analysis', 'functions', 'multivariable-calculus']"
43,When is a linear operator the gradient of a function $f:\mathbb{R}^n \to \mathbb{R}$?,When is a linear operator the gradient of a function ?,f:\mathbb{R}^n \to \mathbb{R},"Let $F(x) = Ax$, where $A \in \mathbb{R}^{n \times n}$ and $x \in \mathbb{R}^n$ Under what condition does there exist a $f$, such that $\nabla f = F$? It is clear to me this happens when $A$ is diagonal matrix. Is there any other conditions on the matrix $A$ such that $f$ exists?","Let $F(x) = Ax$, where $A \in \mathbb{R}^{n \times n}$ and $x \in \mathbb{R}^n$ Under what condition does there exist a $f$, such that $\nabla f = F$? It is clear to me this happens when $A$ is diagonal matrix. Is there any other conditions on the matrix $A$ such that $f$ exists?",,"['linear-algebra', 'functions', 'multivariable-calculus', 'operator-theory', 'convex-analysis']"
44,"What the expected value of $X$, $E(X)$, if the joint pdf $f(x,y) = \frac{e^{-y}}{y} $ for values $0 < x < y,\ 0 < y < \infty$?","What the expected value of , , if the joint pdf  for values ?","X E(X) f(x,y) = \frac{e^{-y}}{y}  0 < x < y,\ 0 < y < \infty","What is $E(X)$ if the joint PDF of $X$ and $Y$ is $$f(x,y) = \begin{cases} \frac{e^{-y}}{y}, &0 < x < y, \ 0 < y < \infty\\ 0, &\text{otherwise}. \end{cases}$$ $$E(X) = \int_{-\infty}^{\infty}x f_X(x)\,dx$$ $$f_X(x)=\int_{-\infty}^{\infty}f(x,y)\,dy = \int_0^\infty \frac{e^{-y}}{y} \,dy$$ But I'm not sure how to compute the integral for $f_X(x).$ So I tried to compute the entire double integral $E(X).$ $$E(X) = \int_{-\infty}^{\infty}x f_X(x)\,dx =\int_{0}^{y}x\int_0^\infty \frac{e^{-y}}{y}\, dy .$$ I remember there is a trick to swap the bounds of integration so that it can make integration easier, but I'm confused how to do this.","What is $E(X)$ if the joint PDF of $X$ and $Y$ is $$f(x,y) = \begin{cases} \frac{e^{-y}}{y}, &0 < x < y, \ 0 < y < \infty\\ 0, &\text{otherwise}. \end{cases}$$ $$E(X) = \int_{-\infty}^{\infty}x f_X(x)\,dx$$ $$f_X(x)=\int_{-\infty}^{\infty}f(x,y)\,dy = \int_0^\infty \frac{e^{-y}}{y} \,dy$$ But I'm not sure how to compute the integral for $f_X(x).$ So I tried to compute the entire double integral $E(X).$ $$E(X) = \int_{-\infty}^{\infty}x f_X(x)\,dx =\int_{0}^{y}x\int_0^\infty \frac{e^{-y}}{y}\, dy .$$ I remember there is a trick to swap the bounds of integration so that it can make integration easier, but I'm confused how to do this.",,"['probability', 'multivariable-calculus']"
45,Variance of squared norm of multivariate normal vector,Variance of squared norm of multivariate normal vector,,"Suppose, that we have a secuence of independent random variables $\{w_i\}_{i=0}^n$ from $\mathcal{N}(0,\mathbb{I}_d)$. We consider the following weighted sum defined for some $\beta \in (0,1)$ $$ \theta = \sum_{i=0}^n (1 - \beta)^i w_i $$ We are interested in the value of $\text{Var}(\|\theta\|_2^2)$. My approach. I have started with calculation of $\mathbb{E}(\|\theta\|_2^2)$. It appears to be $\frac{d}{\beta(2-\beta)}$. Next, I moved to estimation of $$ \mathbb{E}(\|\theta\|_2^4)  =  \mathbb{E}\left(\| \sum_{i=0}^n (1 - \beta)^i w_i \|_2^4\right) =  \sum_{i=0}^n (1 - \beta)^i \mathbb{E}(\|w_i\|_2^4) + \sum_{i,j=0, i\ne j}^n 3(1 - \beta)^{2i+2j} \mathbb{E}(\|w_i\|^2)\mathbb{E}(\|w_i\|^2) $$ Then I have found that $\mathbb{E}(\|w_i\|_2^4) = d^2  + 2d$ and $\mathbb{E}(\|w_i\|^2) = d$. Then I used the following equality $$ \sum_{i,j=0}^n (1 - \beta)^{2i+2j} \approx \frac{1}{(1-(1-\beta))^2}. $$ to obtain for small $\beta$ $$ \mathbb{E}(\|\theta\|_2^4)  \approx  \sum_{i,j=0, i\ne j}^n 3(1 - \beta)^{2i+2j} d^2 \approx \sum_{i,j=0}^n \frac{3d^2}{(1-(1-\beta))^2}. $$ Then $$ \text{Var}(\|\theta\|_2^2) = \mathbb{E}(\|\theta\|_2^4) - (\mathbb{E}(\|\theta\|_2^2))^2 = \frac{2d^2}{(1-(1-\beta))^2}. $$ But computations show that the variance is  $$ \text{Var}(\|\theta\|_2^2) = \frac{2d}{(1-(1-\beta))^2}, $$ which is $d$ times smaller than mine.","Suppose, that we have a secuence of independent random variables $\{w_i\}_{i=0}^n$ from $\mathcal{N}(0,\mathbb{I}_d)$. We consider the following weighted sum defined for some $\beta \in (0,1)$ $$ \theta = \sum_{i=0}^n (1 - \beta)^i w_i $$ We are interested in the value of $\text{Var}(\|\theta\|_2^2)$. My approach. I have started with calculation of $\mathbb{E}(\|\theta\|_2^2)$. It appears to be $\frac{d}{\beta(2-\beta)}$. Next, I moved to estimation of $$ \mathbb{E}(\|\theta\|_2^4)  =  \mathbb{E}\left(\| \sum_{i=0}^n (1 - \beta)^i w_i \|_2^4\right) =  \sum_{i=0}^n (1 - \beta)^i \mathbb{E}(\|w_i\|_2^4) + \sum_{i,j=0, i\ne j}^n 3(1 - \beta)^{2i+2j} \mathbb{E}(\|w_i\|^2)\mathbb{E}(\|w_i\|^2) $$ Then I have found that $\mathbb{E}(\|w_i\|_2^4) = d^2  + 2d$ and $\mathbb{E}(\|w_i\|^2) = d$. Then I used the following equality $$ \sum_{i,j=0}^n (1 - \beta)^{2i+2j} \approx \frac{1}{(1-(1-\beta))^2}. $$ to obtain for small $\beta$ $$ \mathbb{E}(\|\theta\|_2^4)  \approx  \sum_{i,j=0, i\ne j}^n 3(1 - \beta)^{2i+2j} d^2 \approx \sum_{i,j=0}^n \frac{3d^2}{(1-(1-\beta))^2}. $$ Then $$ \text{Var}(\|\theta\|_2^2) = \mathbb{E}(\|\theta\|_2^4) - (\mathbb{E}(\|\theta\|_2^2))^2 = \frac{2d^2}{(1-(1-\beta))^2}. $$ But computations show that the variance is  $$ \text{Var}(\|\theta\|_2^2) = \frac{2d}{(1-(1-\beta))^2}, $$ which is $d$ times smaller than mine.",,"['multivariable-calculus', 'normal-distribution', 'random']"
46,Finding the original function from a Hessian.,Finding the original function from a Hessian.,,"I'm trying to find the original function from the Hessian defined as: $Hf:= \begin{bmatrix}x-2y & x+2y\\x+2y & 2x+2y\end{bmatrix}$ Since the Hessian is symmetric, and the mixed order partials are equivalent, then there exists some $C^2$ function that can be differentiated twice to give this matrix. I'm not sure how I should go about solving this, though, and I should note it should not require integration. Regardless, I did try to approach via integration by integrating $x-2y$ and $2x+2y$ to return the partials of x and y respectively, but these do not result in giving the mixed order derivatives. Is there an alternative approach I should be taking, that does not involve integration? I guess you could logically deduce it...? EDIT: Could I perhaps use a Taylor series expansion somehow? Though I don't think I have enough information...","I'm trying to find the original function from the Hessian defined as: $Hf:= \begin{bmatrix}x-2y & x+2y\\x+2y & 2x+2y\end{bmatrix}$ Since the Hessian is symmetric, and the mixed order partials are equivalent, then there exists some $C^2$ function that can be differentiated twice to give this matrix. I'm not sure how I should go about solving this, though, and I should note it should not require integration. Regardless, I did try to approach via integration by integrating $x-2y$ and $2x+2y$ to return the partials of x and y respectively, but these do not result in giving the mixed order derivatives. Is there an alternative approach I should be taking, that does not involve integration? I guess you could logically deduce it...? EDIT: Could I perhaps use a Taylor series expansion somehow? Though I don't think I have enough information...",,['multivariable-calculus']
47,Harmonic functions are analytic,Harmonic functions are analytic,,"Let $B_1\subset \mathbb{R}^N$ be the unitary ball centered in the origin, and let $u$ be an harmonic function in $B_1$, i.e. $$ -\Delta u = 0 \,\,\,\,\, \mbox{in}\,\,\, B_1. $$ How can i prove that $u$ is analytic? Moreover, how can be proved this results using some estimates on the high order derivative? I know that it must be a classic result, also in several variables, but i cannot find a good reference.","Let $B_1\subset \mathbb{R}^N$ be the unitary ball centered in the origin, and let $u$ be an harmonic function in $B_1$, i.e. $$ -\Delta u = 0 \,\,\,\,\, \mbox{in}\,\,\, B_1. $$ How can i prove that $u$ is analytic? Moreover, how can be proved this results using some estimates on the high order derivative? I know that it must be a classic result, also in several variables, but i cannot find a good reference.",,"['calculus', 'multivariable-calculus', 'partial-differential-equations', 'harmonic-analysis', 'harmonic-functions']"
48,proving a double integral indentity,proving a double integral indentity,,"Numerical integration suggests that the following identity holds for any $\lambda>0$: \begin{equation}\tag{1} \int_0^1\int_0^\infty\frac{3\sqrt{2}}{2\pi\lambda(1-x)\sqrt{x}}\,\exp\left(-\frac{1}{\lambda(1-x)^\frac{3}{2}}\Big((1+y)^\frac{3}{2}-(1+xy)^\frac{3}{2}\Big)\right)\,dy\,dx= \end{equation} \begin{equation}\tag{2} \int_0^1\int_0^\infty\frac{\sqrt{2}}{\pi\sqrt{s(1-s)}}\Big(1+\lambda s^\frac{3}{2}z\Big)^{-\frac{2}{3}}\exp(-z)\,dz\,ds. \end{equation} I've tried proving that the double integrals are equal by performing various substitutions in order to transform one integral into the other without success. For example, using the substitutions $$s=x$$ and $$z=\frac{1}{\lambda(1-x)^\frac{3}{2}}\Big((1+y)^\frac{3}{2}-(1+xy)^\frac{3}{2}\Big)$$ on integral (2) results in $$\int_0^1\int_0^\infty\frac{3\sqrt{2}}{2\pi\lambda(1-x)\sqrt{x}}\,\exp\left(-\frac{1}{\lambda(1-x)^\frac{3}{2}}\Big((1+y)^\frac{3}{2}-(1+xy)^\frac{3}{2}\Big)\right)A\,dy\,dx$$ where $$A=\frac{\sqrt{1+y}-x\sqrt{1+xy}}{1-x}\left(1+\left(\frac{x+xy}{1-x}\right)^\frac{3}{2}-\left(\frac{x+x^2 y}{1-x}\right)^\frac{3}{2}\right)^{-\frac{2}{3}}.$$ I've also tried taking Laplace transforms in $\lambda$ but the resulting double integrals don't seem any easier to deal with. Can anyone suggest a substitution that will work or perhaps another idea on how to prove the above identity?","Numerical integration suggests that the following identity holds for any $\lambda>0$: \begin{equation}\tag{1} \int_0^1\int_0^\infty\frac{3\sqrt{2}}{2\pi\lambda(1-x)\sqrt{x}}\,\exp\left(-\frac{1}{\lambda(1-x)^\frac{3}{2}}\Big((1+y)^\frac{3}{2}-(1+xy)^\frac{3}{2}\Big)\right)\,dy\,dx= \end{equation} \begin{equation}\tag{2} \int_0^1\int_0^\infty\frac{\sqrt{2}}{\pi\sqrt{s(1-s)}}\Big(1+\lambda s^\frac{3}{2}z\Big)^{-\frac{2}{3}}\exp(-z)\,dz\,ds. \end{equation} I've tried proving that the double integrals are equal by performing various substitutions in order to transform one integral into the other without success. For example, using the substitutions $$s=x$$ and $$z=\frac{1}{\lambda(1-x)^\frac{3}{2}}\Big((1+y)^\frac{3}{2}-(1+xy)^\frac{3}{2}\Big)$$ on integral (2) results in $$\int_0^1\int_0^\infty\frac{3\sqrt{2}}{2\pi\lambda(1-x)\sqrt{x}}\,\exp\left(-\frac{1}{\lambda(1-x)^\frac{3}{2}}\Big((1+y)^\frac{3}{2}-(1+xy)^\frac{3}{2}\Big)\right)A\,dy\,dx$$ where $$A=\frac{\sqrt{1+y}-x\sqrt{1+xy}}{1-x}\left(1+\left(\frac{x+xy}{1-x}\right)^\frac{3}{2}-\left(\frac{x+x^2 y}{1-x}\right)^\frac{3}{2}\right)^{-\frac{2}{3}}.$$ I've also tried taking Laplace transforms in $\lambda$ but the resulting double integrals don't seem any easier to deal with. Can anyone suggest a substitution that will work or perhaps another idea on how to prove the above identity?",,"['calculus', 'integration', 'multivariable-calculus', 'definite-integrals']"
49,"Evaluate the$\int_0^1\int_{\sin^{-1} y}^{\pi/2} \cos x\sqrt{1+\cos^2 x}\,dxdy$ and $\int_0^2\int_0^1\int_y^1 \sinh(z^2)\,dzdydx$",Evaluate the and,"\int_0^1\int_{\sin^{-1} y}^{\pi/2} \cos x\sqrt{1+\cos^2 x}\,dxdy \int_0^2\int_0^1\int_y^1 \sinh(z^2)\,dzdydx","Evaluate the following integrals:   \begin{gather} \int_0^1\int_{\sin^{-1} y}^{\pi/2} \cos x\sqrt{1+\cos^2 x}\,dxdy;\\ \int_0^2\int_0^1\int_y^1 \sinh(z^2)\,dzdydx \end{gather} I tried doing integration by parts, got nowhere as nothing seems to reduce. I tried converting $\cos(x)^2$ into $1-\sin(x)^2$ and u-sub, but ended up with $\sqrt{2-u^2}$. Again stuck. Know the hyperbolic equivalents $\sinh x = \frac{e^x-e^{-x}}{2}$, and evaluate the integral with $x=z^2$. Help with number 1? Also maybe links to how to deal with these types of integrals?","Evaluate the following integrals:   \begin{gather} \int_0^1\int_{\sin^{-1} y}^{\pi/2} \cos x\sqrt{1+\cos^2 x}\,dxdy;\\ \int_0^2\int_0^1\int_y^1 \sinh(z^2)\,dzdydx \end{gather} I tried doing integration by parts, got nowhere as nothing seems to reduce. I tried converting $\cos(x)^2$ into $1-\sin(x)^2$ and u-sub, but ended up with $\sqrt{2-u^2}$. Again stuck. Know the hyperbolic equivalents $\sinh x = \frac{e^x-e^{-x}}{2}$, and evaluate the integral with $x=z^2$. Help with number 1? Also maybe links to how to deal with these types of integrals?",,"['integration', 'multivariable-calculus']"
50,Change of the sign of an integral of differential form,Change of the sign of an integral of differential form,,"I'm having troubles with understanding one thing. Consider an integral of some differential form $\omega$, say $n$-form, over some subset $V \subseteq R^n$, which is defined as follows: $$\int_V f(x) \; dx_1 \wedge dx_2 \wedge \ldots \wedge dx_n = \int_V f(x) \;dx_1 dx_2 \ldots dx_n$$ For $\omega$ holds: $$f(x) \; dx_1 \wedge dx_2 \wedge \ldots \wedge dx_n =  -f(x) \; dx_2 \wedge dx_1 \wedge \ldots \wedge dx_n$$ The thing that I don't understand why this doesn't imply that $$\int_V f(x) \;dx_1 dx_2 \ldots dx_n = - \int_V f(x) \;dx_2 dx_1 \ldots dx_n$$ which is obviously wrong, however. Thanks!","I'm having troubles with understanding one thing. Consider an integral of some differential form $\omega$, say $n$-form, over some subset $V \subseteq R^n$, which is defined as follows: $$\int_V f(x) \; dx_1 \wedge dx_2 \wedge \ldots \wedge dx_n = \int_V f(x) \;dx_1 dx_2 \ldots dx_n$$ For $\omega$ holds: $$f(x) \; dx_1 \wedge dx_2 \wedge \ldots \wedge dx_n =  -f(x) \; dx_2 \wedge dx_1 \wedge \ldots \wedge dx_n$$ The thing that I don't understand why this doesn't imply that $$\int_V f(x) \;dx_1 dx_2 \ldots dx_n = - \int_V f(x) \;dx_2 dx_1 \ldots dx_n$$ which is obviously wrong, however. Thanks!",,"['multivariable-calculus', 'differential-geometry']"
51,Use the chain rule to compute ∂z/∂x and ∂z/∂y for 2x^2+y^2+z^2=9. Where exactly do I use it and how?,Use the chain rule to compute ∂z/∂x and ∂z/∂y for 2x^2+y^2+z^2=9. Where exactly do I use it and how?,,"Use the chain rule to compute $\dfrac{\partial z}{\partial x}$ and $\dfrac{\partial z}{\partial y}$ for $2x^2+y^2+z^2=9$. I got the following, however I don't think I'm using the chain rule.  Where exactly do I use it and how? $\dfrac{d}{dy}$ $(2x^2+y^2+z^2)$ = $\dfrac{d}{dy}$ $(9)$. $\dfrac{d}{dy}$$(2x^2) + $$\dfrac{d}{dy}$(y^2) + $\dfrac{\partial z}{\partial y}$$(z^2)$ = $\dfrac{d}{dy}$(9) 0 + 2y + $\dfrac{\partial z}{\partial y}$$(z^2)$ = 0 2y + $\dfrac{\partial z}{\partial y}$(2z) = 0 $\dfrac{\partial z}{\partial y}$(2z) = -2y $\dfrac{\partial z}{\partial y}$= $\dfrac{-2y}{2z}$ = $\dfrac{-y}{z}$ $\dfrac{d}{dx}$ $(2x^2+y^2+z^2)$ = $\dfrac{d}{dx}$ $(9)$ 2$\dfrac{d}{dx}$$(x^2)$+$\dfrac{d}{dx}$$(y^2)$+$\dfrac{\partial z}{\partial x}$$(z^2)$=$\dfrac{d}{dx}$(9). 2(2x)+$\dfrac{d}{dx}$$(y^2)$+$\dfrac{\partial z}{\partial x}$$(z^2)$=$\dfrac{d}{dx}$(9) 4x+ 0 + $\dfrac{\partial z}{\partial x}$$(z^2)$ = 0 4x + $\dfrac{\partial z}{\partial x}$(2z) = 0 $\dfrac{\partial z}{\partial x}$(2z) = -4x $\dfrac{\partial z}{\partial x}$= $\dfrac{-4x}{2z}$ = $\dfrac{-2x}{z}$ My answers of -y/z and -2x/z are correct, but how do I apply the chain rule to the problem.  I also want to apologize up front for the formatting, I just am still trying to figure it out.","Use the chain rule to compute $\dfrac{\partial z}{\partial x}$ and $\dfrac{\partial z}{\partial y}$ for $2x^2+y^2+z^2=9$. I got the following, however I don't think I'm using the chain rule.  Where exactly do I use it and how? $\dfrac{d}{dy}$ $(2x^2+y^2+z^2)$ = $\dfrac{d}{dy}$ $(9)$. $\dfrac{d}{dy}$$(2x^2) + $$\dfrac{d}{dy}$(y^2) + $\dfrac{\partial z}{\partial y}$$(z^2)$ = $\dfrac{d}{dy}$(9) 0 + 2y + $\dfrac{\partial z}{\partial y}$$(z^2)$ = 0 2y + $\dfrac{\partial z}{\partial y}$(2z) = 0 $\dfrac{\partial z}{\partial y}$(2z) = -2y $\dfrac{\partial z}{\partial y}$= $\dfrac{-2y}{2z}$ = $\dfrac{-y}{z}$ $\dfrac{d}{dx}$ $(2x^2+y^2+z^2)$ = $\dfrac{d}{dx}$ $(9)$ 2$\dfrac{d}{dx}$$(x^2)$+$\dfrac{d}{dx}$$(y^2)$+$\dfrac{\partial z}{\partial x}$$(z^2)$=$\dfrac{d}{dx}$(9). 2(2x)+$\dfrac{d}{dx}$$(y^2)$+$\dfrac{\partial z}{\partial x}$$(z^2)$=$\dfrac{d}{dx}$(9) 4x+ 0 + $\dfrac{\partial z}{\partial x}$$(z^2)$ = 0 4x + $\dfrac{\partial z}{\partial x}$(2z) = 0 $\dfrac{\partial z}{\partial x}$(2z) = -4x $\dfrac{\partial z}{\partial x}$= $\dfrac{-4x}{2z}$ = $\dfrac{-2x}{z}$ My answers of -y/z and -2x/z are correct, but how do I apply the chain rule to the problem.  I also want to apologize up front for the formatting, I just am still trying to figure it out.",,['multivariable-calculus']
52,"Taylor Series F"" [closed]","Taylor Series F"" [closed]",,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question I'm looking to derive Taylor’s Series F’’ to 4th order: The final answer is  F’’=(-F(x+2h)+16F(x+h)-30F(x)+16F(x-h)-F(x-2h))/(12h^2) + O(h^4) I'm just not sure how to get -1  16  -30  16  -1, Can anyone give me some guidance?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question I'm looking to derive Taylor’s Series F’’ to 4th order: The final answer is  F’’=(-F(x+2h)+16F(x+h)-30F(x)+16F(x-h)-F(x-2h))/(12h^2) + O(h^4) I'm just not sure how to get -1  16  -30  16  -1, Can anyone give me some guidance?",,['multivariable-calculus']
53,Multiple variable differential equation,Multiple variable differential equation,,Find the general solution to the following differential equation. $\frac{dy}{dx}  =  \frac{4y}{3x}$ For this question I got the solution $y = Ce^{(4/3)^x}$ but this is some how incorrect. How? What is the right answer? Could someone show me their formatting so that I know where I went wrong?,Find the general solution to the following differential equation. $\frac{dy}{dx}  =  \frac{4y}{3x}$ For this question I got the solution $y = Ce^{(4/3)^x}$ but this is some how incorrect. How? What is the right answer? Could someone show me their formatting so that I know where I went wrong?,,"['calculus', 'multivariable-calculus']"
54,"Sketch the vector equation:$ r(t) = (2\cos(t), 2\sin(t), 1)$",Sketch the vector equation:," r(t) = (2\cos(t), 2\sin(t), 1)","Given the parametric equations: $$x = 2\cos(t), \quad y=2\sin(t), \quad z = 1.$$ We know that $x^2 + y^2 = 4\cos^2(t) + 4\sin^2(t) = 4(\cos^2(t) + \sin^2(t)) = 4$. In other problems, I've seen this fact tell us that the graph lies on the cylinder $x^2 + y^2 = 4$. However, in this case, $z$ is a constant. Does this mean that the 3D graph looks like a circle with radius of $2$, at $z = 1$?","Given the parametric equations: $$x = 2\cos(t), \quad y=2\sin(t), \quad z = 1.$$ We know that $x^2 + y^2 = 4\cos^2(t) + 4\sin^2(t) = 4(\cos^2(t) + \sin^2(t)) = 4$. In other problems, I've seen this fact tell us that the graph lies on the cylinder $x^2 + y^2 = 4$. However, in this case, $z$ is a constant. Does this mean that the 3D graph looks like a circle with radius of $2$, at $z = 1$?",,"['multivariable-calculus', 'vector-spaces', 'vectors', 'graphing-functions']"
55,a surface integral problem,a surface integral problem,,"$F=(y-z,z-x,x-y)$ $S$ be the portion of surface defined by  $x^2+y^2+z^2=1$ and $x+y+z\ge 1$. We want to evaluate $\int_S curl(F)\cdot dS$. I have found $curl(F)=(-2,-2,-2)$ and the normal vector of $S$ is $(x,y,z)$. Thus the integral becomes $\int_S -2(x+y+z) dS$. However, if I use the polar coordinate of $S$, then the boundary of $\phi$ and $\theta$ is a big problem. I still consider using Stoke's Theorem.  It then should be $\int_A F\cdot dr$ where $A$ is the circle defined by the unit ball and the plane $x+y+z=1$. However I can not find a way to paramettrize this circle. That is a big problem.","$F=(y-z,z-x,x-y)$ $S$ be the portion of surface defined by  $x^2+y^2+z^2=1$ and $x+y+z\ge 1$. We want to evaluate $\int_S curl(F)\cdot dS$. I have found $curl(F)=(-2,-2,-2)$ and the normal vector of $S$ is $(x,y,z)$. Thus the integral becomes $\int_S -2(x+y+z) dS$. However, if I use the polar coordinate of $S$, then the boundary of $\phi$ and $\theta$ is a big problem. I still consider using Stoke's Theorem.  It then should be $\int_A F\cdot dr$ where $A$ is the circle defined by the unit ball and the plane $x+y+z=1$. However I can not find a way to paramettrize this circle. That is a big problem.",,"['multivariable-calculus', 'stokes-theorem']"
56,Intuition for continuity in $\Bbb R^n$,Intuition for continuity in,\Bbb R^n,"I'm trying to conceptually understand continuity in $\Bbb R^n$. I understand the one-dimensional case where for a function $f: \Bbb R \to \Bbb R$, arbitrarily small deviations of the input ($\delta$) will produce arbitrarily small deviations in the output ($\epsilon$). However, this picture is muddied when I start to consider functions of multiple variables. For example, what does a continuous function $g: \Bbb R^2 \to \Bbb R^2$ look like? Is it just piecewise continuous? For example, if I have a function $g(x,y) = (x+y, 2xy)$, would I just need to consider that an arbitrarily small change in $x$ and $y$, and show that each of these individually would produce an arbitrarily small change in the output? More generally, is there a way that I can intuitively think about it? The picture for $\Bbb R$ is clear, but I think it is less clear for higher dimensions. Also, how should I picture any general function $f:\Bbb R^2 \to \Bbb R^2$?","I'm trying to conceptually understand continuity in $\Bbb R^n$. I understand the one-dimensional case where for a function $f: \Bbb R \to \Bbb R$, arbitrarily small deviations of the input ($\delta$) will produce arbitrarily small deviations in the output ($\epsilon$). However, this picture is muddied when I start to consider functions of multiple variables. For example, what does a continuous function $g: \Bbb R^2 \to \Bbb R^2$ look like? Is it just piecewise continuous? For example, if I have a function $g(x,y) = (x+y, 2xy)$, would I just need to consider that an arbitrarily small change in $x$ and $y$, and show that each of these individually would produce an arbitrarily small change in the output? More generally, is there a way that I can intuitively think about it? The picture for $\Bbb R$ is clear, but I think it is less clear for higher dimensions. Also, how should I picture any general function $f:\Bbb R^2 \to \Bbb R^2$?",,"['multivariable-calculus', 'continuity', 'intuition', 'epsilon-delta']"
57,"Finding $f_x$ and and $f_y$ in a $f(x,y)$ function.",Finding  and and  in a  function.,"f_x f_y f(x,y)","Let  $f(x,y)  =  x^4y + 6xy^3 + 3x^4$. (a) Find  $f_x$. (b) Find  $f_y$. I believe that I should make the equation equal to the function that I am looking for to solve the question. Would I be able to find the answer this way, and if not, how should I?","Let  $f(x,y)  =  x^4y + 6xy^3 + 3x^4$. (a) Find  $f_x$. (b) Find  $f_y$. I believe that I should make the equation equal to the function that I am looking for to solve the question. Would I be able to find the answer this way, and if not, how should I?",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
58,Why to define the tangent plane we need differentiability?,Why to define the tangent plane we need differentiability?,,"Let $f:\mathbb R^n\longrightarrow \mathbb R$. Why to define the tangent plane at $a\in\mathbb R^n$ we need that $f$ is differentible at $a$ (and not only partially differentiable) ? Indeed, if it's partially differentiable, I can define $$z=f(a)+\nabla f(a)\cdot (x-a)$$ that exist since partial derivatives exists. And this is the equation of the tangent plan, no ?","Let $f:\mathbb R^n\longrightarrow \mathbb R$. Why to define the tangent plane at $a\in\mathbb R^n$ we need that $f$ is differentible at $a$ (and not only partially differentiable) ? Indeed, if it's partially differentiable, I can define $$z=f(a)+\nabla f(a)\cdot (x-a)$$ that exist since partial derivatives exists. And this is the equation of the tangent plan, no ?",,"['real-analysis', 'multivariable-calculus']"
59,Multivariable calculus (m substitution),Multivariable calculus (m substitution),,"I have a question involving multi variable calculus: $$\lim_{(x,y)\to(0,0)} \frac{x^4-4y^2}{x^2+2y^2}$$ When proving that limit exists... we sometimes let $y = mx$ to say that we are considering all straight line paths to the origin. Then we write: $$\lim_{x\to 0} \frac{x^4-4(mx)^2}{x^2+2(mx)^2}$$ I know some example of function but I want to understand the inner working of why the limit can be re-written as so?","I have a question involving multi variable calculus: $$\lim_{(x,y)\to(0,0)} \frac{x^4-4y^2}{x^2+2y^2}$$ When proving that limit exists... we sometimes let $y = mx$ to say that we are considering all straight line paths to the origin. Then we write: $$\lim_{x\to 0} \frac{x^4-4(mx)^2}{x^2+2(mx)^2}$$ I know some example of function but I want to understand the inner working of why the limit can be re-written as so?",,"['limits', 'multivariable-calculus']"
60,What happens to $\frac{\partial z}{\partial x}$ and $\frac{\partial z}{\partial y}$ for functions over curves in $\mathbb{R^2}$?,What happens to  and  for functions over curves in ?,\frac{\partial z}{\partial x} \frac{\partial z}{\partial y} \mathbb{R^2},"Suppose we have $f(x,y)=x+y$, and $\operatorname{domain}(f)=\{(x,y) \mid y=2x\}$. Now, what happens to $\dfrac{\partial f}{\partial x}$ at some point in the domain, say $(x_0,y_0)$? Will it be some real number, $0$ or undefined? According to me, this should be undefined. I think so because one can write $f(x,y)=3x$, and conclude $$\dfrac{\partial f}{\partial x}=3$$ or one may write $f(x,y)=\dfrac{3}{2}y$, and conclude $$\dfrac{\partial f}{\partial x}=0$$ However, if the function is defined over a curve, then finding the partial derivatives is meaningless, as keeping one quantity fixed is not possible. So, shouldn't this mean that these functions don't have partial derivatives? The question arose in my mind while thinking of finding $\dfrac{df}{dt}$, for parametrisation $$x=t,y=2t$$ If I use total derivatives, $$\dfrac{df}{dt}=\dfrac{\partial f}{\partial x}\dfrac{dx}{dt}+\dfrac{\partial f}{\partial y}\dfrac{dy}{dt}$$ then I am confused regarding the partials. Shouldn't this operation be undefined for such functions?","Suppose we have $f(x,y)=x+y$, and $\operatorname{domain}(f)=\{(x,y) \mid y=2x\}$. Now, what happens to $\dfrac{\partial f}{\partial x}$ at some point in the domain, say $(x_0,y_0)$? Will it be some real number, $0$ or undefined? According to me, this should be undefined. I think so because one can write $f(x,y)=3x$, and conclude $$\dfrac{\partial f}{\partial x}=3$$ or one may write $f(x,y)=\dfrac{3}{2}y$, and conclude $$\dfrac{\partial f}{\partial x}=0$$ However, if the function is defined over a curve, then finding the partial derivatives is meaningless, as keeping one quantity fixed is not possible. So, shouldn't this mean that these functions don't have partial derivatives? The question arose in my mind while thinking of finding $\dfrac{df}{dt}$, for parametrisation $$x=t,y=2t$$ If I use total derivatives, $$\dfrac{df}{dt}=\dfrac{\partial f}{\partial x}\dfrac{dx}{dt}+\dfrac{\partial f}{\partial y}\dfrac{dy}{dt}$$ then I am confused regarding the partials. Shouldn't this operation be undefined for such functions?",,"['multivariable-calculus', 'partial-derivative']"
61,Continuity of partial derivatives imply differentiability - but I have a counter-example,Continuity of partial derivatives imply differentiability - but I have a counter-example,,"I must be doing something wrong here. Let $f: \mathbb R^2 \to \mathbb R$ be defined as $$f(x,y)= \begin{cases} 3x+4y, &\text{if } xy \neq 0 \\ 0, &\text{if } xy =0 \end{cases}$$ i.e. $f$ is $0$ on the $x-$axis and on the $y-$axis whereas $f(x,y)=3x+4y$ everywhere else. I want the check the differentiability at $(0,0)$. $\bullet$ Clearly, $\ f$ is continuous. $\bullet$ Also, I think that $f_x=0=f_y$, the partial derivatives are just the function restricted to $x-$axis and $y-$axis. I found the picture below on the web, it gives a geometric interpretation of partial derivatives. $\bullet$ So, the partial derivatives exist and continuous. Then, $f$ is differentiable at $(0,0)$. $\bullet$ However, I think that $f$ is not differentiable at the origin, because... Umm... The directional derivative is not continuous. A little direction change may yield a huge change in directional derivative. Is $f$ differentiable at the origin? What is wrong about my reasoning? $$$$ $$$$ Edit: So, isn't $f_x$ defined as the slope of the tangent line to the curve that is given by the intersection of the graph of $f$ and the $xz-$plane? (that is the red line in the above picture)","I must be doing something wrong here. Let $f: \mathbb R^2 \to \mathbb R$ be defined as $$f(x,y)= \begin{cases} 3x+4y, &\text{if } xy \neq 0 \\ 0, &\text{if } xy =0 \end{cases}$$ i.e. $f$ is $0$ on the $x-$axis and on the $y-$axis whereas $f(x,y)=3x+4y$ everywhere else. I want the check the differentiability at $(0,0)$. $\bullet$ Clearly, $\ f$ is continuous. $\bullet$ Also, I think that $f_x=0=f_y$, the partial derivatives are just the function restricted to $x-$axis and $y-$axis. I found the picture below on the web, it gives a geometric interpretation of partial derivatives. $\bullet$ So, the partial derivatives exist and continuous. Then, $f$ is differentiable at $(0,0)$. $\bullet$ However, I think that $f$ is not differentiable at the origin, because... Umm... The directional derivative is not continuous. A little direction change may yield a huge change in directional derivative. Is $f$ differentiable at the origin? What is wrong about my reasoning? $$$$ $$$$ Edit: So, isn't $f_x$ defined as the slope of the tangent line to the curve that is given by the intersection of the graph of $f$ and the $xz-$plane? (that is the red line in the above picture)",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
62,Double integral - changing order of integration,Double integral - changing order of integration,,"I am trying to change the order of integration of the following double integral $$\int _0^{2a} dx\int _{\sqrt{2ax-x^2}}^{\sqrt{4ax}} f(x,y) \ dy .$$ (a>0) I've sketched the domain but am struggling to change it from type 1 domain to type 2 domain. Any help? I've attached an image of the geometry of the domain and I've shaded it. (Well the image shows the case when a=1.)","I am trying to change the order of integration of the following double integral $$\int _0^{2a} dx\int _{\sqrt{2ax-x^2}}^{\sqrt{4ax}} f(x,y) \ dy .$$ (a>0) I've sketched the domain but am struggling to change it from type 1 domain to type 2 domain. Any help? I've attached an image of the geometry of the domain and I've shaded it. (Well the image shows the case when a=1.)",,"['integration', 'multivariable-calculus', 'definite-integrals']"
63,"Finding extremal points on $f(x,y)$",Finding extremal points on,"f(x,y)","This is the equation:  $$f(x,y) = xye^\left(-\frac{1}{2}(x^2 + y^2)\right)$$ This is what I've done: $$\nabla f(x,y) = \begin{bmatrix} (1-x^2)ye^\left(-\frac{1}{2}(x^2 + y^2)\right) \\  (1-y^2)xe^\left(-\frac{1}{2}(x^2 + y^2)\right) \end{bmatrix}$$ Here's the thing I'm worried about, to find when $\nabla f(x,y) = 0$, i set each equation $= 0$. $$\begin{align} (1-x^2)ye^\left(-\frac{1}{2}(x^2 + y^2)\right) &= 0 \\ y e^\left(-\frac{1}{2}(x^2 + y^2)\right) &= x^2ye^\left(-\frac{1}{2}(x^2 + y^2)\right) \\ 1 &= x^2 \\ x &= \pm 1\end{align} $$ Is this legal, or do i lose some solutions when I divide away everything?","This is the equation:  $$f(x,y) = xye^\left(-\frac{1}{2}(x^2 + y^2)\right)$$ This is what I've done: $$\nabla f(x,y) = \begin{bmatrix} (1-x^2)ye^\left(-\frac{1}{2}(x^2 + y^2)\right) \\  (1-y^2)xe^\left(-\frac{1}{2}(x^2 + y^2)\right) \end{bmatrix}$$ Here's the thing I'm worried about, to find when $\nabla f(x,y) = 0$, i set each equation $= 0$. $$\begin{align} (1-x^2)ye^\left(-\frac{1}{2}(x^2 + y^2)\right) &= 0 \\ y e^\left(-\frac{1}{2}(x^2 + y^2)\right) &= x^2ye^\left(-\frac{1}{2}(x^2 + y^2)\right) \\ 1 &= x^2 \\ x &= \pm 1\end{align} $$ Is this legal, or do i lose some solutions when I divide away everything?",,"['multivariable-calculus', 'proof-verification', 'applications', 'maxima-minima']"
64,Checking whether a multivariable function is convex,Checking whether a multivariable function is convex,,"I have a simple question. I have a multi-variable function that I'm supposed to check whether convex or not. I know the definition for convexity as follows: The function $f(x)$ is convex if: $$ f(\lambda x_1 + (1 - \lambda x_2)) \leq \lambda f(x_1) + (1 - \lambda )f(x_2)$$ But this is for the single variable case. How do I generalize it for multi-variable case? The author of this question seems to be showing that the function $f(x,y)$ is convex if, $$ f(\lambda x_1 + (1 - \lambda x_2),\lambda y_1 + (1 - \lambda y_2) ) \leq \lambda f(x_1,y_1) + (1 - \lambda )f(x_2,y_2) $$ But it hasn't been explicitly written anywhere. Is this correct? Please help. EDIT: The two functions should be corrected as: $$ f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda )f(x_2)$$ $$ f(\lambda x_1 + (1 - \lambda) x_2,\lambda y_1 + (1 - \lambda) y_2 ) \leq \lambda f(x_1,y_1) + (1 - \lambda )f(x_2,y_2) $$","I have a simple question. I have a multi-variable function that I'm supposed to check whether convex or not. I know the definition for convexity as follows: The function $f(x)$ is convex if: $$ f(\lambda x_1 + (1 - \lambda x_2)) \leq \lambda f(x_1) + (1 - \lambda )f(x_2)$$ But this is for the single variable case. How do I generalize it for multi-variable case? The author of this question seems to be showing that the function $f(x,y)$ is convex if, $$ f(\lambda x_1 + (1 - \lambda x_2),\lambda y_1 + (1 - \lambda y_2) ) \leq \lambda f(x_1,y_1) + (1 - \lambda )f(x_2,y_2) $$ But it hasn't been explicitly written anywhere. Is this correct? Please help. EDIT: The two functions should be corrected as: $$ f(\lambda x_1 + (1 - \lambda) x_2) \leq \lambda f(x_1) + (1 - \lambda )f(x_2)$$ $$ f(\lambda x_1 + (1 - \lambda) x_2,\lambda y_1 + (1 - \lambda) y_2 ) \leq \lambda f(x_1,y_1) + (1 - \lambda )f(x_2,y_2) $$",,"['multivariable-calculus', 'convex-analysis']"
65,Easy way to check if multi-variable function is convex,Easy way to check if multi-variable function is convex,,"I know that a function with one variable has to have continuous positive second order derivative for every $x$. Is there an easy way like that to check convexity for multi-variable functions? For example let's consider the function $f(x_1,x_2)=\frac{1}{3}x_1^3-4x_1+\frac{1}{3}x_2^3-16x_2$","I know that a function with one variable has to have continuous positive second order derivative for every $x$. Is there an easy way like that to check convexity for multi-variable functions? For example let's consider the function $f(x_1,x_2)=\frac{1}{3}x_1^3-4x_1+\frac{1}{3}x_2^3-16x_2$",,"['multivariable-calculus', 'convex-analysis']"
66,Jacobian of $A (A^\top X A)^{-1} A^\top$,Jacobian of,A (A^\top X A)^{-1} A^\top,"Let $A\in\mathbb{R}^{n\times m}$, $n\geq m$, be a full column rank matrix, and consider the function \begin{align} f&\colon \mathbb{R}^{n\times n} \to \mathbb{R}^{n\times n}\\ & X\mapsto A (A^\top X A)^{-1} A^\top, \end{align} where $\bullet^\top$ denotes transposition. Assuming that $(A^\top X A)^{-1}$ exists, I'm interested in the computation of the Jacobian matrix of $f$, i.e. $$\tag{1}\label{a} \mathbf{J}[f] = \left[\frac{\partial f(X)}{\partial X_{ij}}\right]\in\mathbb{R}^{n^2\times n^2}. $$ I know that there exists a closed form expressions for the Jacobian of the inverse, namely $\mathbf{J}[X^{-1}]=-(X^{-\top} \otimes X^{-1})$ (see e.g. here , page 5). Hence, I wonder whether a similar closed-form expression can be derived for \eqref{a}. Thanks in advance.","Let $A\in\mathbb{R}^{n\times m}$, $n\geq m$, be a full column rank matrix, and consider the function \begin{align} f&\colon \mathbb{R}^{n\times n} \to \mathbb{R}^{n\times n}\\ & X\mapsto A (A^\top X A)^{-1} A^\top, \end{align} where $\bullet^\top$ denotes transposition. Assuming that $(A^\top X A)^{-1}$ exists, I'm interested in the computation of the Jacobian matrix of $f$, i.e. $$\tag{1}\label{a} \mathbf{J}[f] = \left[\frac{\partial f(X)}{\partial X_{ij}}\right]\in\mathbb{R}^{n^2\times n^2}. $$ I know that there exists a closed form expressions for the Jacobian of the inverse, namely $\mathbf{J}[X^{-1}]=-(X^{-\top} \otimes X^{-1})$ (see e.g. here , page 5). Hence, I wonder whether a similar closed-form expression can be derived for \eqref{a}. Thanks in advance.",,"['matrices', 'multivariable-calculus', 'derivatives', 'matrix-calculus', 'jacobian']"
67,Differential of composition of functions,Differential of composition of functions,,"If $\mathbf f : \mathbb R^n \to \mathbb R^m$ is differentiable at $x_0 \in \mathbb R^n$, define the differential operator $$d\mathbf f_{x_0} : \mathbb R^n \to \mathbb R^m $$ as the linear operator that takes a small increment $h \in \mathbb R^n$ and outputs $D\mathbf f(x_0) \cdot h$, where $D\mathbf f(x_0) \in \operatorname {Mat}(m\times n,\mathbb R)$ is the Jacobian matrix of $\mathbf f$ at the point $x_0$. Let $\mathbf f$ be defined as before, and let $\mathbf g : \mathbb R^p \to \mathbb R^q$ be differentiable at $y_0 \in \mathbb R^p$. Call $a_0 \doteq \mathbf f(x_0)$ and $b_0 \doteq \mathbf g(y_0)$. Lastly, let $\mathbf h : \mathbb R^m \times \mathbb R^q \to \mathbb R^s$ be differentiable at $(a_0,b_0)$. How would I write down the differential $d\mathbf s_{(x_0,y_0)}$ of function $\mathbf s : \mathbb R^n \times \mathbb R^p \to \mathbb R^s$ defined as follows, $$\mathbf s : (x,y) \mapsto \mathbf h(\mathbf f(x),\mathbf g(y)) $$ as a composition of the differentials $d\mathbf h_{(a_0,b_0)}$, $d\mathbf f_{x_0}$, and $d\mathbf g_{y_0}$? I know that the Jacobian matrix associated to $d\mathbf s_{(x_0,y_0)}$ should be in $\operatorname{Mat}(s \times pq, \mathbb R)$, but I'm not exactly sure what matrices need be multiplied together to construct it. I guess my brain is having a hard time understanding how to ""merge"" two functions such as $\mathbf f$ and $\mathbf g$ in an ordered couple, so I can't compile my composition diagram correctly.","If $\mathbf f : \mathbb R^n \to \mathbb R^m$ is differentiable at $x_0 \in \mathbb R^n$, define the differential operator $$d\mathbf f_{x_0} : \mathbb R^n \to \mathbb R^m $$ as the linear operator that takes a small increment $h \in \mathbb R^n$ and outputs $D\mathbf f(x_0) \cdot h$, where $D\mathbf f(x_0) \in \operatorname {Mat}(m\times n,\mathbb R)$ is the Jacobian matrix of $\mathbf f$ at the point $x_0$. Let $\mathbf f$ be defined as before, and let $\mathbf g : \mathbb R^p \to \mathbb R^q$ be differentiable at $y_0 \in \mathbb R^p$. Call $a_0 \doteq \mathbf f(x_0)$ and $b_0 \doteq \mathbf g(y_0)$. Lastly, let $\mathbf h : \mathbb R^m \times \mathbb R^q \to \mathbb R^s$ be differentiable at $(a_0,b_0)$. How would I write down the differential $d\mathbf s_{(x_0,y_0)}$ of function $\mathbf s : \mathbb R^n \times \mathbb R^p \to \mathbb R^s$ defined as follows, $$\mathbf s : (x,y) \mapsto \mathbf h(\mathbf f(x),\mathbf g(y)) $$ as a composition of the differentials $d\mathbf h_{(a_0,b_0)}$, $d\mathbf f_{x_0}$, and $d\mathbf g_{y_0}$? I know that the Jacobian matrix associated to $d\mathbf s_{(x_0,y_0)}$ should be in $\operatorname{Mat}(s \times pq, \mathbb R)$, but I'm not exactly sure what matrices need be multiplied together to construct it. I guess my brain is having a hard time understanding how to ""merge"" two functions such as $\mathbf f$ and $\mathbf g$ in an ordered couple, so I can't compile my composition diagram correctly.",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'function-and-relation-composition', 'jacobian']"
68,Computing surface integral on paraboloid,Computing surface integral on paraboloid,,"This question is loosely related to a question I asked earlier today about surface parametrisation. I have the vector field $\boldsymbol{v}=(2z,x,y)$ and want to find the surface integral $$  \int_S (\nabla \times \boldsymbol{v}) \cdot d\boldsymbol{A} $$ where $S$ is the surface of the paraboloid $z=x^2+y^2$ underneath the plane $z=y$, bounded by the curve $C$ formed by the intersection of the paraboloid $z=x^2+y^2$ and the plane $z=y$. I realise that there are simpler surfaces to pick (the disc) or I could use Stokes' Theorem and find the line integral over $C$, but my question specifically concerns the surface described above. I can use the fact that we have a level set $$ f=x^2+y^2-z=0 \implies \nabla f = (2x, 2y, -1)$$ which describes our surface and the normals to it. Then by a well known result, $$   \begin{split} \int_S (\nabla \times \boldsymbol{v}) \cdot d\boldsymbol{A} &= \int_A \frac{ (\nabla \times \boldsymbol{v}) \cdot \nabla f}{\boldsymbol{e}_3 \cdot \nabla f} \, dx \, dy  \\ &=\int_A(1-2x-4y) \, dx \, dy \end{split} $$ but I'm not really sure where to go from here (i.e. how to pick the limits of integration). I was thinking about parameterising the projection of $C$ in the $xy$ plane using polars but this seems completely wrong as I'm considering the surface of the paraboloid, not the disc. But I have no idea how you'd find the limits to use here/how you would even parameterise the paraboloid's surface to do the integral. There are a lot of questions similar to this one on this site, but having read a lot of them, I'm still unsure as to whether a problem like this always gets reduced to a region in the $xy$ plane or if sometimes, you have to do some kind of a 3d parameterisation. The answer for me seems to be no, having looked at the proof of the result I've used above, and also simply due to the fact that the integral only has $dx \, dy$ at the end - no $dz$. I'd be very grateful if anyone could shed some light on this - thanks.","This question is loosely related to a question I asked earlier today about surface parametrisation. I have the vector field $\boldsymbol{v}=(2z,x,y)$ and want to find the surface integral $$  \int_S (\nabla \times \boldsymbol{v}) \cdot d\boldsymbol{A} $$ where $S$ is the surface of the paraboloid $z=x^2+y^2$ underneath the plane $z=y$, bounded by the curve $C$ formed by the intersection of the paraboloid $z=x^2+y^2$ and the plane $z=y$. I realise that there are simpler surfaces to pick (the disc) or I could use Stokes' Theorem and find the line integral over $C$, but my question specifically concerns the surface described above. I can use the fact that we have a level set $$ f=x^2+y^2-z=0 \implies \nabla f = (2x, 2y, -1)$$ which describes our surface and the normals to it. Then by a well known result, $$   \begin{split} \int_S (\nabla \times \boldsymbol{v}) \cdot d\boldsymbol{A} &= \int_A \frac{ (\nabla \times \boldsymbol{v}) \cdot \nabla f}{\boldsymbol{e}_3 \cdot \nabla f} \, dx \, dy  \\ &=\int_A(1-2x-4y) \, dx \, dy \end{split} $$ but I'm not really sure where to go from here (i.e. how to pick the limits of integration). I was thinking about parameterising the projection of $C$ in the $xy$ plane using polars but this seems completely wrong as I'm considering the surface of the paraboloid, not the disc. But I have no idea how you'd find the limits to use here/how you would even parameterise the paraboloid's surface to do the integral. There are a lot of questions similar to this one on this site, but having read a lot of them, I'm still unsure as to whether a problem like this always gets reduced to a region in the $xy$ plane or if sometimes, you have to do some kind of a 3d parameterisation. The answer for me seems to be no, having looked at the proof of the result I've used above, and also simply due to the fact that the integral only has $dx \, dy$ at the end - no $dz$. I'd be very grateful if anyone could shed some light on this - thanks.",,"['integration', 'multivariable-calculus', 'surfaces', 'parametric', 'surface-integrals']"
69,How to show that a set is convex?,How to show that a set is convex?,,"My problem is as follows:  Let f(x,y) be a general concave function (i.e. any concave function), and let S be points (x,y) such that f(x,y) greater than or equal to zero. I understand that this means S is the set of points on or below f(x,y) and >0, and I can show how it is convex in a sketch/graph. I just don't understand how you show it mathematically in a sufficient way using the definition of a convex set. Any help would be greatly appreciated.","My problem is as follows:  Let f(x,y) be a general concave function (i.e. any concave function), and let S be points (x,y) such that f(x,y) greater than or equal to zero. I understand that this means S is the set of points on or below f(x,y) and >0, and I can show how it is convex in a sketch/graph. I just don't understand how you show it mathematically in a sufficient way using the definition of a convex set. Any help would be greatly appreciated.",,"['linear-algebra', 'multivariable-calculus', 'convex-analysis', 'convex-optimization']"
70,"$f: \mathbb R^2 \to \mathbb R$ be a function whose restriction on the graph of any continuous function on open set is continuous , is $f$ continuous?","be a function whose restriction on the graph of any continuous function on open set is continuous , is  continuous?",f: \mathbb R^2 \to \mathbb R f,"Let $f: \mathbb R^2 \to \mathbb R$ be a function such that for every open set $U \subseteq \mathbb R$ and continuous function $h:U \to \mathbb R$ , $f|_{G(h)} : G(h) \to \mathbb R$ is continuous ($G(h):=\{(x,h(x))|x \in U\}$) , then is it true that $f$ is continuous ?","Let $f: \mathbb R^2 \to \mathbb R$ be a function such that for every open set $U \subseteq \mathbb R$ and continuous function $h:U \to \mathbb R$ , $f|_{G(h)} : G(h) \to \mathbb R$ is continuous ($G(h):=\{(x,h(x))|x \in U\}$) , then is it true that $f$ is continuous ?",,['multivariable-calculus']
71,"Show that $(x^3+2y^3)/x^2+y^2$ is continuous at $(0,0)$",Show that  is continuous at,"(x^3+2y^3)/x^2+y^2 (0,0)",I used polar coordinates to solve the problem and reached up to $$ r|\cos^3(\theta) + 2\sin^3(\theta)| $$ I am stuck after that and don't know how to figure out epsilon and delta.,I used polar coordinates to solve the problem and reached up to $$ r|\cos^3(\theta) + 2\sin^3(\theta)| $$ I am stuck after that and don't know how to figure out epsilon and delta.,,"['multivariable-calculus', 'continuity']"
72,Fully prove that curve is of certain type,Fully prove that curve is of certain type,,"I apologize for the generic title. By type I mean that it belongs to a class of curves, such as helixes, circles, lines, etc. To the point however, I stumbled into an exercise where asked to describe which type of curve that has the following parameterization: $$\vec{r} = (a \cos t \sin t)\hat{i} + (a \sin^2 t)\hat{j} + bt\hat{k}, $$ What I did was that I tried to express the $x$- and $y$-components in the form $C_1 \sin f(t)$ and $C_2 \cos f(t)$ respectively. So: $$x=a \cos t \sin t=\frac{a}{2} \sin 2t$$ $$y = a \sin^2 t=a\frac{1-\cos^2 2t}{2} \iff 2y - a = -a\cos 2t$$ Using this, I confirmed that: $$x^2 + \left(y - \frac{a}{2}\right)^2 = \left( \frac{a}{2} \right) ^2$$ This implies that the curve lies on the cylinder with the above equation. However, this is where I get a bit confused as to how to proceed. It is seemingly clear that the equation represents a helix, considering that $z$ is increasing and both $x$ and $y$ draws out projections that are circles in the $xy$-plane. But how do I actually prove this? Technically, it could be the line $x = y = 0$ as well, if I don't proceed to somehow that $x$ and $y$ are ""behaving circular"" (you get what I mean, hopefully). Worth mentioning is that the book doesn't seem to prove this either. Question: Can I in this particular case prove that the curve is a helix? My initial guess was that it had to do with $x,y \in C^0$, along with some other constraint, but I can't really get any further. Any help would be gladly appreciated!","I apologize for the generic title. By type I mean that it belongs to a class of curves, such as helixes, circles, lines, etc. To the point however, I stumbled into an exercise where asked to describe which type of curve that has the following parameterization: $$\vec{r} = (a \cos t \sin t)\hat{i} + (a \sin^2 t)\hat{j} + bt\hat{k}, $$ What I did was that I tried to express the $x$- and $y$-components in the form $C_1 \sin f(t)$ and $C_2 \cos f(t)$ respectively. So: $$x=a \cos t \sin t=\frac{a}{2} \sin 2t$$ $$y = a \sin^2 t=a\frac{1-\cos^2 2t}{2} \iff 2y - a = -a\cos 2t$$ Using this, I confirmed that: $$x^2 + \left(y - \frac{a}{2}\right)^2 = \left( \frac{a}{2} \right) ^2$$ This implies that the curve lies on the cylinder with the above equation. However, this is where I get a bit confused as to how to proceed. It is seemingly clear that the equation represents a helix, considering that $z$ is increasing and both $x$ and $y$ draws out projections that are circles in the $xy$-plane. But how do I actually prove this? Technically, it could be the line $x = y = 0$ as well, if I don't proceed to somehow that $x$ and $y$ are ""behaving circular"" (you get what I mean, hopefully). Worth mentioning is that the book doesn't seem to prove this either. Question: Can I in this particular case prove that the curve is a helix? My initial guess was that it had to do with $x,y \in C^0$, along with some other constraint, but I can't really get any further. Any help would be gladly appreciated!",,"['multivariable-calculus', 'curves']"
73,Different answers for integral of $\sin^3x$,Different answers for integral of,\sin^3x,"I was calculating an integral that I thought it'd be easy however it turned out very weird. I couldn't figure out so I'm looking for some help. $$\int_{-\pi/2}^{\pi/2} \int_{2\cos\theta}^{2}2r{\sqrt{4-r^2}} \,dr\,d\vartheta$$ Then, I solved the inner integral with respect to r. I got the following by simplifying $\int {2\over3}(4-(2\cos\theta)^2)^{3\over2} $ $\int {2\over3}(4-4\cos^2\theta)^{3\over2} $ $\int {2\over3}(4(1-\cos^2\theta))^{3\over2} $ $\int {2\over3}(4\sin^2\theta)^{3\over2} $ ${16\over3}\int(\sin^3\theta) $ Then, with simple substitution, I got $\int(\sin^3\theta) d\theta $ = $1/3\cos^3\theta-\cos\theta+C$. When I plug in my end points $\pi/2$  and  $-\pi/2$, I get zero. However, it can't be zero because it's a legit solid piece. So, I tried to solve with wolphram alpha for $\sin^3\theta$ and my integrand before simplification, I got followings: I also found this one for the integration of $\sin^3x$ vs So, I'm super confused how come $\sin^3x$ got two different answers???They seem the same but how come they are completely different functions? Where do I make mistake to solve the original problem? thank you","I was calculating an integral that I thought it'd be easy however it turned out very weird. I couldn't figure out so I'm looking for some help. $$\int_{-\pi/2}^{\pi/2} \int_{2\cos\theta}^{2}2r{\sqrt{4-r^2}} \,dr\,d\vartheta$$ Then, I solved the inner integral with respect to r. I got the following by simplifying $\int {2\over3}(4-(2\cos\theta)^2)^{3\over2} $ $\int {2\over3}(4-4\cos^2\theta)^{3\over2} $ $\int {2\over3}(4(1-\cos^2\theta))^{3\over2} $ $\int {2\over3}(4\sin^2\theta)^{3\over2} $ ${16\over3}\int(\sin^3\theta) $ Then, with simple substitution, I got $\int(\sin^3\theta) d\theta $ = $1/3\cos^3\theta-\cos\theta+C$. When I plug in my end points $\pi/2$  and  $-\pi/2$, I get zero. However, it can't be zero because it's a legit solid piece. So, I tried to solve with wolphram alpha for $\sin^3\theta$ and my integrand before simplification, I got followings: I also found this one for the integration of $\sin^3x$ vs So, I'm super confused how come $\sin^3x$ got two different answers???They seem the same but how come they are completely different functions? Where do I make mistake to solve the original problem? thank you",,"['integration', 'multivariable-calculus', 'polar-coordinates', 'trigonometric-integrals']"
74,Question on Notation from Spivak's Calculus on Manifolds,Question on Notation from Spivak's Calculus on Manifolds,,"In Spivak's book Calculus on Manifolds, he uses some notation that I'm not entirely sure of. On pg 20 when discussing the derivative of a function $p:R^2 \rightarrow R$ and $p(x,y) = x \cdot y$, he writes $$ Dp(a,b)(x,y) = bx + ay $$ and concludes that $p'(a,b) = (b,a)$. This is where I am a little confused. At one point, he states that $Dp(a,b)$ represents the derivative of $p$ at the point $(a,b)$ (pg 16). He goes on to say that it's common to represent the derivative in this case as the Jacobian, which would be a $1x2$ matrix in this case, as indicated by $p'(a,b) = (b,a)$ (I mean, it's not exactly a matrix, but the point is there). However, when defining $Dp(a,b)(x,y)$, what returns is a scalar value. So I'm not sure how to interpret this. In exercise 2-12 (pg 23), he asks us to show for a bilinear function $f:R^n \times R^m \rightarrow R^p$ that $Df(a,b)(x,y) = f(a,y) + f(x,b)$. Well, now here, the derivative is in $R^p$, so clearly not a scalar. How am I supposed to interpret this? If it is at the point $(a,b)$, then why are both $x$ and $y$ present on the right hand side? What is being expressed in this case?","In Spivak's book Calculus on Manifolds, he uses some notation that I'm not entirely sure of. On pg 20 when discussing the derivative of a function $p:R^2 \rightarrow R$ and $p(x,y) = x \cdot y$, he writes $$ Dp(a,b)(x,y) = bx + ay $$ and concludes that $p'(a,b) = (b,a)$. This is where I am a little confused. At one point, he states that $Dp(a,b)$ represents the derivative of $p$ at the point $(a,b)$ (pg 16). He goes on to say that it's common to represent the derivative in this case as the Jacobian, which would be a $1x2$ matrix in this case, as indicated by $p'(a,b) = (b,a)$ (I mean, it's not exactly a matrix, but the point is there). However, when defining $Dp(a,b)(x,y)$, what returns is a scalar value. So I'm not sure how to interpret this. In exercise 2-12 (pg 23), he asks us to show for a bilinear function $f:R^n \times R^m \rightarrow R^p$ that $Df(a,b)(x,y) = f(a,y) + f(x,b)$. Well, now here, the derivative is in $R^p$, so clearly not a scalar. How am I supposed to interpret this? If it is at the point $(a,b)$, then why are both $x$ and $y$ present on the right hand side? What is being expressed in this case?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives', 'manifolds']"
75,"$\forall r>0$ small enough $f(B(x_0,r))$ doesn't contain a ball around $f(x_0)$",small enough  doesn't contain a ball around,"\forall r>0 f(B(x_0,r)) f(x_0)","Let $k<m$ and $f:\mathbb{R}^k \to \mathbb{R}^m$  be continuously differentiable around $x_0$. with $\operatorname{rank}Df(x_0) = k$. Then $\forall r>0$ small enough $f(B(x_0,r))$ doesn't contain a ball around $f(x_0)$. My attempt is the following: Since $f$ is onto its image $\operatorname{Img}(f) \subset \mathbb{R}^m$ of degree at most $k$. Since $\operatorname{rank}Df(x_0) = k$, around $x_0$ the image of $f$ is a subset of $\mathbb{R}^m$ which is of dimension $k$. That is, around $x_0$ $f$ is injective (by the inverse function theorem). The inverse function theorem applies for functions from $\mathbb{R}^k$ to itself, is this a legitimate use of this theorem, thinking of the image as a $k$ dimensional subset of $\mathbb{R}^m$? If so how do we show this? Letting $U$ be the environment in which $f$ is injective, we take $r>0$ so that $B(x_0,r) \subset U$. If $\exists \delta > 0$ s.t $B(f(x_0),\delta) \subset f(B(x_0,r))$ then since $\dim(B(f(x_0),\delta)) = m$ we get $m \leq k$, in contradiction. I'm not sure the use of the inverse function theorem is correct (due to the fact the image is in $\mathbb{R}^m$). And also the second part, couldn't I have concluded that without $f$ being injective in $U$?","Let $k<m$ and $f:\mathbb{R}^k \to \mathbb{R}^m$  be continuously differentiable around $x_0$. with $\operatorname{rank}Df(x_0) = k$. Then $\forall r>0$ small enough $f(B(x_0,r))$ doesn't contain a ball around $f(x_0)$. My attempt is the following: Since $f$ is onto its image $\operatorname{Img}(f) \subset \mathbb{R}^m$ of degree at most $k$. Since $\operatorname{rank}Df(x_0) = k$, around $x_0$ the image of $f$ is a subset of $\mathbb{R}^m$ which is of dimension $k$. That is, around $x_0$ $f$ is injective (by the inverse function theorem). The inverse function theorem applies for functions from $\mathbb{R}^k$ to itself, is this a legitimate use of this theorem, thinking of the image as a $k$ dimensional subset of $\mathbb{R}^m$? If so how do we show this? Letting $U$ be the environment in which $f$ is injective, we take $r>0$ so that $B(x_0,r) \subset U$. If $\exists \delta > 0$ s.t $B(f(x_0),\delta) \subset f(B(x_0,r))$ then since $\dim(B(f(x_0),\delta)) = m$ we get $m \leq k$, in contradiction. I'm not sure the use of the inverse function theorem is correct (due to the fact the image is in $\mathbb{R}^m$). And also the second part, couldn't I have concluded that without $f$ being injective in $U$?",,"['calculus', 'real-analysis', 'functional-analysis', 'analysis', 'multivariable-calculus']"
76,Proof: Interior of two intersecting sets equals the intersecting interiors of the sets seperately,Proof: Interior of two intersecting sets equals the intersecting interiors of the sets seperately,,"Prove that the following is true: $$\operatorname{int}A∩\operatorname{int}B=\operatorname{int}(A∩B)$$ I'm in general really bad at writing proofs. I do not know where to start even when looking at the definitions given in the textbook. I am well aware of the definition of the interior of sets, but I can't apply it in a meaningful way. My attempt at writing that proof is laughable since I'm doing the proof backwards. We know that $int(A)\subset A$ , thus we have: $$\operatorname{int}(A∩B)=\operatorname{int}A∩\operatorname{int}(B)\\ \operatorname{int}(A\cap B)\subseteq A \cap B$$ The latter is per definition the interior of the set $A \cap B$ . Q.E.D.","Prove that the following is true: I'm in general really bad at writing proofs. I do not know where to start even when looking at the definitions given in the textbook. I am well aware of the definition of the interior of sets, but I can't apply it in a meaningful way. My attempt at writing that proof is laughable since I'm doing the proof backwards. We know that , thus we have: The latter is per definition the interior of the set . Q.E.D.","\operatorname{int}A∩\operatorname{int}B=\operatorname{int}(A∩B) int(A)\subset A \operatorname{int}(A∩B)=\operatorname{int}A∩\operatorname{int}(B)\\
\operatorname{int}(A\cap B)\subseteq A \cap B A \cap B","['multivariable-calculus', 'elementary-set-theory']"
77,Spivak's proof of Inverse Function Theorem.,Spivak's proof of Inverse Function Theorem.,,"This question concerns the Inverse Function Theorem done in the book. Spivak states that $$|D_jf^i(x) - D_jf^i(a)| < 1/2n^2$$ for all $i,j$ and $x \in U$ Here $U$ is a closed rectangle set, $n$ is an integer. The lemma 2.10 (on the previous page) states that Let $A \subset \mathbb{R}^n$ be a rectangle and let $f: A \to \mathbb{R}^n$ be continuously differentiable. If there is a number M such that $|D_jf^i(x)| \leq M$ for all $x$ in the interior of $A$, then $$|f(x) - f(y)| \leq n^2 M|x - y|$$ He define $g(x) := f(x) - x$ and states that, for $x_1, x_2 \in U$, we have $|f(x_1) - x _1 -(f(x_2) - x_2)| \leq \frac{1}{2} |x_1 - x_2|$ and it's ok for me, because $|D_jg^i(x) - D_jg^i(a)| \leq \frac{1}{2n^2} n^2 |x_1 - x_2| = \frac{1}{2} |x_1 - x_2|$ by the lemma, but we can do this only if $|D_jg^i(x)| \leq M$, my doubt is why is this valid?","This question concerns the Inverse Function Theorem done in the book. Spivak states that $$|D_jf^i(x) - D_jf^i(a)| < 1/2n^2$$ for all $i,j$ and $x \in U$ Here $U$ is a closed rectangle set, $n$ is an integer. The lemma 2.10 (on the previous page) states that Let $A \subset \mathbb{R}^n$ be a rectangle and let $f: A \to \mathbb{R}^n$ be continuously differentiable. If there is a number M such that $|D_jf^i(x)| \leq M$ for all $x$ in the interior of $A$, then $$|f(x) - f(y)| \leq n^2 M|x - y|$$ He define $g(x) := f(x) - x$ and states that, for $x_1, x_2 \in U$, we have $|f(x_1) - x _1 -(f(x_2) - x_2)| \leq \frac{1}{2} |x_1 - x_2|$ and it's ok for me, because $|D_jg^i(x) - D_jg^i(a)| \leq \frac{1}{2n^2} n^2 |x_1 - x_2| = \frac{1}{2} |x_1 - x_2|$ by the lemma, but we can do this only if $|D_jg^i(x)| \leq M$, my doubt is why is this valid?",,"['calculus', 'multivariable-calculus', 'proof-explanation']"
78,"Thought Process behind Parametrics in the $(x,y,z)$ space",Thought Process behind Parametrics in the  space,"(x,y,z)","Converting parametrics to a rectangular equation in $2$D is pretty straight-forward, I think: just solve for $t$ and set them equal to each other or do a substitution. $3D$ is confusing me, however.  For example, $r(t) = (t, t, t^2)$ or $r(t) = (t, \sin t, 2\cos t)$ What steps would I take to visualize these as well as make the mathematical connection to the cartesian plane? Of course plotting points is possible, but tedious in $3$D.  Others in my class are able to simply look at these and know what shape they make, something I do not know how to do.","Converting parametrics to a rectangular equation in $2$D is pretty straight-forward, I think: just solve for $t$ and set them equal to each other or do a substitution. $3D$ is confusing me, however.  For example, $r(t) = (t, t, t^2)$ or $r(t) = (t, \sin t, 2\cos t)$ What steps would I take to visualize these as well as make the mathematical connection to the cartesian plane? Of course plotting points is possible, but tedious in $3$D.  Others in my class are able to simply look at these and know what shape they make, something I do not know how to do.",,"['multivariable-calculus', '3d', 'parametric']"
79,"Multivariable integral, probably related to the gamma function","Multivariable integral, probably related to the gamma function",,"Let $x=\left[ x_1,x_2,..,x_n \right]^{T}$ represent the vector of all variables and $D$ be a diagonal matrix, the question is to integrate or give an approximate answer: $\idotsint_{[0,\infty]^{n}} (x^{T}Dx)^{\alpha-1}exp(-x^{T}Dx) \,dx_1 \dots dx_n$","Let $x=\left[ x_1,x_2,..,x_n \right]^{T}$ represent the vector of all variables and $D$ be a diagonal matrix, the question is to integrate or give an approximate answer: $\idotsint_{[0,\infty]^{n}} (x^{T}Dx)^{\alpha-1}exp(-x^{T}Dx) \,dx_1 \dots dx_n$",,"['integration', 'multivariable-calculus', 'improper-integrals', 'gamma-function']"
80,Proving inequality using Lagrange multipliers.,Proving inequality using Lagrange multipliers.,,"I started learing about Lagrange Multipliers and I got the following question: Prove that $\frac{a}{b+c}+\frac{b}{c+a}+\frac{c}{a+b}≥\frac{3}{2}$, for each $a,b,c>0$. I'm not sure how to use Lagrange multipliers for it... any ideas?","I started learing about Lagrange Multipliers and I got the following question: Prove that $\frac{a}{b+c}+\frac{b}{c+a}+\frac{c}{a+b}≥\frac{3}{2}$, for each $a,b,c>0$. I'm not sure how to use Lagrange multipliers for it... any ideas?",,"['calculus', 'multivariable-calculus', 'inequality', 'fractions', 'lagrange-multiplier']"
81,$f$ function with $\nabla f=\bar{0}$,function with,f \nabla f=\bar{0},"Let $f:D\subset\mathbb{R}^2\rightarrow\mathbb{R}$ be a function with $D$ open and connected set such that $\nabla f=\bar{0}$ for all $x\in D$ Proof that $f$ is a constant function. Hi! I have some problems with this exercise. I think that I have the proof, but, in this, I never use the fact that $D$ is a  connected  set. Can anyone help me? Please. My proof: First, since $\nabla f=\left(\displaystyle\frac{\partial f}{\partial x},\displaystyle\frac{\partial f}{\partial y} \right)=(0,0)$, then, $\displaystyle\frac{\partial f}{\partial x}=0$ and $\displaystyle\frac{\partial f}{\partial y} =0$. Hence, the partial derivatives are bounded, i.e., there exist a $K,M\in\mathbb{R}^{+}$ such that $\left|\displaystyle\frac{\partial f}{\partial y}  \right|\leq K$ and $\left|\displaystyle\frac{\partial f}{\partial x}  \right|\leq M$ (I think that anything $K$ and $M$ it works, because the partial derivatives are $0$). Now let $a = (a_1,a_2)$ and $b = (b_1,b_2) \in D$. Then we have: \begin{align*} f(a) - f(b) &= f(a_1,a_2) - f(b_1,b_2) \\ f(a)-f(b) &= f(a_1,a_2) - f(a_1,b_2) + f(a_1,b_2) - f(b_1,b_2) \end{align*} Then, by the triangle inequality: $$|f(a)-f(b)| \leq |f(a_1,a_2) - (a_1,b_2)| + |f(a_1,b_2) - f(b_1,b_2)|$$ And since the partial derivatives exist, we can use the one-dimensional Mean Value Theorem to show that there exists some $c$ such that: $$\frac{f(a_1,a_2)-f(a_1,b_2)}{a_2-b_2} = \displaystyle\frac{\partial f}{\partial y}(a_1,c)$$ And noting how we defined $K$, it follows that $$|f(a_1,a_2) - f(a_1,b_2)| \leq K|a_2 - b_2|$$ And similarly $$|f(a_1,b_2) - f(b_1,b_2)| \leq M |a_1-b_1|$$ And using the statement we got from the triangle inequality, we have that $$|f(a)-f(b)| \leq M|a_1-b_1| + K|a_2 - b_2|$$ And by the Cauchy-Schwarz inequality, we have that $$M|a_1-b_1| + K|a_2 - b_2| \leq  \sqrt{M^2 + K^2}\cdot \sqrt{(a_1-b_1)^2+(a_2-b_2)^2} = \sqrt{M^2 + K^2} \cdot ||a-b||$$ Whereby $$|f(a)-f(b)| \leq \sqrt{M^2 + K^2} \cdot ||a-b||$$ So $f$ is Lipschitz with $L = \sqrt{K^2 + M^2}$. Hence, the function is continuous. But, $M=K=0$ works, we can consider $L=0$ and the nex equality $$0\leq|f(a)-f(b)| \leq 0$$Then $|f(b)-f(a)|=0$ and we can conclude that $f(b)=f(a)$ for all $a,b\in D$. Then, the function is constant. But, in the proof, I never used the connected of $D$. Where is the wrong? Can anyone help me? Thanks.","Let $f:D\subset\mathbb{R}^2\rightarrow\mathbb{R}$ be a function with $D$ open and connected set such that $\nabla f=\bar{0}$ for all $x\in D$ Proof that $f$ is a constant function. Hi! I have some problems with this exercise. I think that I have the proof, but, in this, I never use the fact that $D$ is a  connected  set. Can anyone help me? Please. My proof: First, since $\nabla f=\left(\displaystyle\frac{\partial f}{\partial x},\displaystyle\frac{\partial f}{\partial y} \right)=(0,0)$, then, $\displaystyle\frac{\partial f}{\partial x}=0$ and $\displaystyle\frac{\partial f}{\partial y} =0$. Hence, the partial derivatives are bounded, i.e., there exist a $K,M\in\mathbb{R}^{+}$ such that $\left|\displaystyle\frac{\partial f}{\partial y}  \right|\leq K$ and $\left|\displaystyle\frac{\partial f}{\partial x}  \right|\leq M$ (I think that anything $K$ and $M$ it works, because the partial derivatives are $0$). Now let $a = (a_1,a_2)$ and $b = (b_1,b_2) \in D$. Then we have: \begin{align*} f(a) - f(b) &= f(a_1,a_2) - f(b_1,b_2) \\ f(a)-f(b) &= f(a_1,a_2) - f(a_1,b_2) + f(a_1,b_2) - f(b_1,b_2) \end{align*} Then, by the triangle inequality: $$|f(a)-f(b)| \leq |f(a_1,a_2) - (a_1,b_2)| + |f(a_1,b_2) - f(b_1,b_2)|$$ And since the partial derivatives exist, we can use the one-dimensional Mean Value Theorem to show that there exists some $c$ such that: $$\frac{f(a_1,a_2)-f(a_1,b_2)}{a_2-b_2} = \displaystyle\frac{\partial f}{\partial y}(a_1,c)$$ And noting how we defined $K$, it follows that $$|f(a_1,a_2) - f(a_1,b_2)| \leq K|a_2 - b_2|$$ And similarly $$|f(a_1,b_2) - f(b_1,b_2)| \leq M |a_1-b_1|$$ And using the statement we got from the triangle inequality, we have that $$|f(a)-f(b)| \leq M|a_1-b_1| + K|a_2 - b_2|$$ And by the Cauchy-Schwarz inequality, we have that $$M|a_1-b_1| + K|a_2 - b_2| \leq  \sqrt{M^2 + K^2}\cdot \sqrt{(a_1-b_1)^2+(a_2-b_2)^2} = \sqrt{M^2 + K^2} \cdot ||a-b||$$ Whereby $$|f(a)-f(b)| \leq \sqrt{M^2 + K^2} \cdot ||a-b||$$ So $f$ is Lipschitz with $L = \sqrt{K^2 + M^2}$. Hence, the function is continuous. But, $M=K=0$ works, we can consider $L=0$ and the nex equality $$0\leq|f(a)-f(b)| \leq 0$$Then $|f(b)-f(a)|=0$ and we can conclude that $f(b)=f(a)$ for all $a,b\in D$. Then, the function is constant. But, in the proof, I never used the connected of $D$. Where is the wrong? Can anyone help me? Thanks.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'continuity']"
82,Lagrange Multiplier Three Dimension,Lagrange Multiplier Three Dimension,,"(2). Consider the surface produced by the equation $xy^2z^3 = 2$ . Find the points in this surface closest to the origin? So far here is what I have, Let $$ f(x,y,z) = x^2 + y^2 + z^2 $$ Where f is the square of the distance from the point $(x,y,z)$ to the origin. We want to minimize $f$, so our constraint is $$ g(x,y,z) =  xy^2z^3 - 2 $$ We must solve $$\nabla f = \lambda \nabla g \quad \text{ and } \quad  g = 0$$ By the former we must have $$ \Bigg( \begin{matrix} 2x \\ 2y \\ 2z \\ \end{matrix} \Bigg) =  \lambda \Bigg( \begin{matrix} y^2z^3 \\ 2xyz^3 \\ 3xy^2z^2 \\ \end{matrix} \Bigg) $$ Solving this system we get equations $$ 2x = \lambda \text{ } y^2z^3  $$ $$ $$  $$ 2y = 2\lambda \text{ } xyz^3 $$ $$ $$  $$ 2z = 3\lambda  \text{ } xy^2z^2$$ $$ $$ This is where I start to get stuck. From what I gathered from websites and youtube videos is that I would try to solve it using either cases, or by having the left side equal the same thing. I tried the latter and believe the answers were $$ y^2 = 2x^2 \to y = \pm x\sqrt(2) \text{    and}$$ $$ 2z^2 = 3y^2 \to z = \pm y \sqrt{\frac{3}{2}} $$ I got these solutions by multiplying the first, second, and third equation by yz, xz, xy, respectively. Then having the first equation equal the second and then having the second equation = third equation. So it should look like this: $$ 2xyz_1 = \lambda \text{ } y^3z^4  $$ $$ $$  $$ 2xyz_2 = 2\lambda \text{ } x^2yz^4 $$ $$ $$ $$ 2xyz_3 = 3\lambda  \text{ } x^2y^3z^2$$ $$ $$ But from here I am stuck and I have no idea what to do or even if I am on the right path. Any help would be greatly appreciated.","(2). Consider the surface produced by the equation $xy^2z^3 = 2$ . Find the points in this surface closest to the origin? So far here is what I have, Let $$ f(x,y,z) = x^2 + y^2 + z^2 $$ Where f is the square of the distance from the point $(x,y,z)$ to the origin. We want to minimize $f$, so our constraint is $$ g(x,y,z) =  xy^2z^3 - 2 $$ We must solve $$\nabla f = \lambda \nabla g \quad \text{ and } \quad  g = 0$$ By the former we must have $$ \Bigg( \begin{matrix} 2x \\ 2y \\ 2z \\ \end{matrix} \Bigg) =  \lambda \Bigg( \begin{matrix} y^2z^3 \\ 2xyz^3 \\ 3xy^2z^2 \\ \end{matrix} \Bigg) $$ Solving this system we get equations $$ 2x = \lambda \text{ } y^2z^3  $$ $$ $$  $$ 2y = 2\lambda \text{ } xyz^3 $$ $$ $$  $$ 2z = 3\lambda  \text{ } xy^2z^2$$ $$ $$ This is where I start to get stuck. From what I gathered from websites and youtube videos is that I would try to solve it using either cases, or by having the left side equal the same thing. I tried the latter and believe the answers were $$ y^2 = 2x^2 \to y = \pm x\sqrt(2) \text{    and}$$ $$ 2z^2 = 3y^2 \to z = \pm y \sqrt{\frac{3}{2}} $$ I got these solutions by multiplying the first, second, and third equation by yz, xz, xy, respectively. Then having the first equation equal the second and then having the second equation = third equation. So it should look like this: $$ 2xyz_1 = \lambda \text{ } y^3z^4  $$ $$ $$  $$ 2xyz_2 = 2\lambda \text{ } x^2yz^4 $$ $$ $$ $$ 2xyz_3 = 3\lambda  \text{ } x^2y^3z^2$$ $$ $$ But from here I am stuck and I have no idea what to do or even if I am on the right path. Any help would be greatly appreciated.",,"['multivariable-calculus', 'systems-of-equations', 'linear-programming', 'nonlinear-optimization', 'lagrange-multiplier']"
83,Finding the anti Gradient?,Finding the anti Gradient?,,"I have this question that says $f(x,y,z)$ such that $\nabla f(x,y,z) = <2xy,2yz+x^2,y^2>$ The way I see this is that they are asking for what can you take the partial derivative of with respect $x y$ and $z$ so that you get that $\nabla f(x,y,z)$ My answer would be $f(x,y,z) = (x^2y+y^2z+x^2y,y^2z) $","I have this question that says $f(x,y,z)$ such that $\nabla f(x,y,z) = <2xy,2yz+x^2,y^2>$ The way I see this is that they are asking for what can you take the partial derivative of with respect $x y$ and $z$ so that you get that $\nabla f(x,y,z)$ My answer would be $f(x,y,z) = (x^2y+y^2z+x^2y,y^2z) $",,['multivariable-calculus']
84,Slope of curve of intersection with surface and plane x = 2,Slope of curve of intersection with surface and plane x = 2,,"Let $f(x, y) = x^{2} + y^{2}$. I want to find the slope of the curve of intersection between the surface $f(x, y) = z$ and the plane $x = 2$. I also want to find a direction vector of the tangent line at the point $(2, 0, 4)$. Is the slope just partial $y$? And i'm not really sure how to approach the second part of the problem.","Let $f(x, y) = x^{2} + y^{2}$. I want to find the slope of the curve of intersection between the surface $f(x, y) = z$ and the plane $x = 2$. I also want to find a direction vector of the tangent line at the point $(2, 0, 4)$. Is the slope just partial $y$? And i'm not really sure how to approach the second part of the problem.",,['multivariable-calculus']
85,Extending gradient of a submanifold,Extending gradient of a submanifold,,"Let $M$ be a submanifold of $\Bbb{R^n}$ and $f:M\to \Bbb{R}$ continuously differentiable and such that $f^{-1}[a,b]$ has no critical points. We define $\nabla f (a)$ by $\langle \nabla f(a),v \rangle=T_a f(v)$ for all $v\in T_a M$. Now for all $x\in f^{-1}([a,b])$ we have $\nabla f(x)\ne 0$. How can I extend this in a neighborhood of $f^{-1}([a,b])$ ? It seems thatp I need bump function but I really don't ""see"" how can I do that. Edit: sorry I forgot to say that $M$ is compact so does $f^1[a,b]$","Let $M$ be a submanifold of $\Bbb{R^n}$ and $f:M\to \Bbb{R}$ continuously differentiable and such that $f^{-1}[a,b]$ has no critical points. We define $\nabla f (a)$ by $\langle \nabla f(a),v \rangle=T_a f(v)$ for all $v\in T_a M$. Now for all $x\in f^{-1}([a,b])$ we have $\nabla f(x)\ne 0$. How can I extend this in a neighborhood of $f^{-1}([a,b])$ ? It seems thatp I need bump function but I really don't ""see"" how can I do that. Edit: sorry I forgot to say that $M$ is compact so does $f^1[a,b]$",,"['multivariable-calculus', 'differential-geometry', 'manifolds']"
86,What points on surface is the tangent plane parallel to $xy$- plane?,What points on surface is the tangent plane parallel to - plane?,xy,"At what pts. on the surface $z = x^{2}y + y^{2}x + 3x$ is the tangent plane parallel to the $xy$-plane? So first I define a function $F(x, y, z) =  x^{2}y + y^{2}x + 3x - z$ which has gradient $grad F = (2xy + y^2 + 3, x^{2} + 2yx, -1)$. So the equation of our tangent plane is: $(2xy + y^{2} + 3)(x - x_{0}) + (x^{2} + 2yx)(y - y_{0}) - (z - z_{0}) = 0$. So we get as a normal line: $r(t) = (x_{0} + (2x_{0}y_{0} + y_{0}^{2} + 3)t, y_{0} + (x_{0}^{2} +2y_{0}x_{0})t, z_{0} - t)$. Now I know two planes are parallel if their normal lines are parallel, but I'm not quite sure how to complete the problem.","At what pts. on the surface $z = x^{2}y + y^{2}x + 3x$ is the tangent plane parallel to the $xy$-plane? So first I define a function $F(x, y, z) =  x^{2}y + y^{2}x + 3x - z$ which has gradient $grad F = (2xy + y^2 + 3, x^{2} + 2yx, -1)$. So the equation of our tangent plane is: $(2xy + y^{2} + 3)(x - x_{0}) + (x^{2} + 2yx)(y - y_{0}) - (z - z_{0}) = 0$. So we get as a normal line: $r(t) = (x_{0} + (2x_{0}y_{0} + y_{0}^{2} + 3)t, y_{0} + (x_{0}^{2} +2y_{0}x_{0})t, z_{0} - t)$. Now I know two planes are parallel if their normal lines are parallel, but I'm not quite sure how to complete the problem.",,['multivariable-calculus']
87,Second derivative as a multilinear map,Second derivative as a multilinear map,,"Well, let's assume that $T=(f\circ g)$.  How do I compute $D^2 T(y)(v,w)$? I tried $D^2 T(y)(v,w)=(D_wD_vT)(y)$, i.e. do first the directional derivatives and then compute them at $y$. $D_vT=((Df)\circ g) \circ (Dg )\ (v)$, then I try to apply again the chain rule to $D_wD_vT$. My best (incorrect) effort gives $((D((Df)\circ g))\circ Dg \ (v))\circ D^2g(v,w)=((D^2f)\circ g)\circ Dg )\circ Dg \ (v))\circ D^2g(v,w)$ which is a big mess and this doesn't look like what I'm supposed to get, which is $$D^2f(x)(Dg(y)v,Dg(y)w)+Df(x)D^2g(y)(v,w)$$ How would one proceed? any help would be appreciated. P.S: If you're downvoting this question, at least tell me what's wrong it with  so that I can improve it. Thanks.","Well, let's assume that $T=(f\circ g)$.  How do I compute $D^2 T(y)(v,w)$? I tried $D^2 T(y)(v,w)=(D_wD_vT)(y)$, i.e. do first the directional derivatives and then compute them at $y$. $D_vT=((Df)\circ g) \circ (Dg )\ (v)$, then I try to apply again the chain rule to $D_wD_vT$. My best (incorrect) effort gives $((D((Df)\circ g))\circ Dg \ (v))\circ D^2g(v,w)=((D^2f)\circ g)\circ Dg )\circ Dg \ (v))\circ D^2g(v,w)$ which is a big mess and this doesn't look like what I'm supposed to get, which is $$D^2f(x)(Dg(y)v,Dg(y)w)+Df(x)D^2g(y)(v,w)$$ How would one proceed? any help would be appreciated. P.S: If you're downvoting this question, at least tell me what's wrong it with  so that I can improve it. Thanks.",,"['multivariable-calculus', 'multilinear-algebra']"
88,Old Derivative Problem solved with partial derivatives,Old Derivative Problem solved with partial derivatives,,"Sometimes I get my best understanding when I work older problems with new techniques.  So, I wanted to work the old derivative problem of minimizing the surface area of a cylindrical can to hold 1 liter of oil.  We solved this in single variable calculus by getting the surface area equation to have only one variable and then taking the derivative and setting it =0.  Worked perfectly. Now, I thought, well, since I know partial derivatives, I'm thinking I don't need to get that Surface area equation to have just one variable.  I can leave it as: $$A= 2\pi rh+ 2 \pi r^2$$ Shouldn't I just be able to find the partial derivative with respect to h and a separate partial derivative with respect to r, set them both =0 and solve to get the critical values. hmmmmm...when I do this, I get something weird. (a)  $$\frac {\partial A}{\partial r} = 2\pi h + 4\pi r$$ (b)  $$\frac {\partial A}{\partial h} =  2\pi r$$ Setting $(a) =0$ I get $2\pi h=-4\pi r$ or $h = -2 r$ HUH?  This doesn't make sense.  My height is a negative number! Why doesn't this old calculus problem work using partials?","Sometimes I get my best understanding when I work older problems with new techniques.  So, I wanted to work the old derivative problem of minimizing the surface area of a cylindrical can to hold 1 liter of oil.  We solved this in single variable calculus by getting the surface area equation to have only one variable and then taking the derivative and setting it =0.  Worked perfectly. Now, I thought, well, since I know partial derivatives, I'm thinking I don't need to get that Surface area equation to have just one variable.  I can leave it as: $$A= 2\pi rh+ 2 \pi r^2$$ Shouldn't I just be able to find the partial derivative with respect to h and a separate partial derivative with respect to r, set them both =0 and solve to get the critical values. hmmmmm...when I do this, I get something weird. (a)  $$\frac {\partial A}{\partial r} = 2\pi h + 4\pi r$$ (b)  $$\frac {\partial A}{\partial h} =  2\pi r$$ Setting $(a) =0$ I get $2\pi h=-4\pi r$ or $h = -2 r$ HUH?  This doesn't make sense.  My height is a negative number! Why doesn't this old calculus problem work using partials?",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
89,Use Mean Value Theorem to show $f(y) = f(x) + \nabla f(x)^T(y-x) + \int\limits_0^1 t(y-x)^T\nabla^2 f(x+\xi (y-x))^T(y-x) dt$,Use Mean Value Theorem to show,f(y) = f(x) + \nabla f(x)^T(y-x) + \int\limits_0^1 t(y-x)^T\nabla^2 f(x+\xi (y-x))^T(y-x) dt,"Claim: Given a $C^2$, convex function $f$ and vectors $x,y \in \mathbb{R}^n, t \in [0,1]$ Suppose that $$f(y) = f(x) + \nabla f(x)^T(y-x) + \int\limits_0^1 (\nabla  f(x+t(y-x))^T-\nabla f(x))^T(y-x) dt$$ Then $$f(y) = f(x) + \nabla f(x)^T(y-x) + \int\limits_0^1 t(y-x)^T\nabla^2  f(x+\xi (y-x))^T(y-x) dt$$ where $0\leq \xi \leq t$ I think the proof relies on the mean value theorem Specifically,  $$\nabla  f(x+t(y-x))-\nabla f(x) = \nabla^2  f(x+\xi  (y-x))^T(x+t(y-x)-x) = \nabla^2  f(x+\xi  (y-x))^Tt(y-x) $$ While this has structural similarity compared to the first order mean value theorem, I am not sure how this can be proved and searching the literature up and down I just could not find a so called higher dimensional mean value theorem. Typing in mean value theorem + hessian returns no good results. Can someone please provide a reference to a higher order, generalized, mean value theorem which involves the gradient and the Hessian and was used in this proof?","Claim: Given a $C^2$, convex function $f$ and vectors $x,y \in \mathbb{R}^n, t \in [0,1]$ Suppose that $$f(y) = f(x) + \nabla f(x)^T(y-x) + \int\limits_0^1 (\nabla  f(x+t(y-x))^T-\nabla f(x))^T(y-x) dt$$ Then $$f(y) = f(x) + \nabla f(x)^T(y-x) + \int\limits_0^1 t(y-x)^T\nabla^2  f(x+\xi (y-x))^T(y-x) dt$$ where $0\leq \xi \leq t$ I think the proof relies on the mean value theorem Specifically,  $$\nabla  f(x+t(y-x))-\nabla f(x) = \nabla^2  f(x+\xi  (y-x))^T(x+t(y-x)-x) = \nabla^2  f(x+\xi  (y-x))^Tt(y-x) $$ While this has structural similarity compared to the first order mean value theorem, I am not sure how this can be proved and searching the literature up and down I just could not find a so called higher dimensional mean value theorem. Typing in mean value theorem + hessian returns no good results. Can someone please provide a reference to a higher order, generalized, mean value theorem which involves the gradient and the Hessian and was used in this proof?",,"['real-analysis', 'multivariable-calculus', 'reference-request', 'convex-analysis', 'hessian-matrix']"
90,Parametrization of surface of revolution,Parametrization of surface of revolution,,"A surface of revolution is obtained by picking a curve $C$ on a plane and rotating it around some axis contained on the plane. Choosing the coordinates in a convenient manner, we can pick $C$ to be contained on the plane $xz$ and chose the axis to be the $z$ axis. In that case, if $R_z(\theta) : \mathbb{R}^3\to \mathbb{R}^3$ is a rotation around the $z$ -axis, the surface of revolution will be $$S = \{R_z(\theta)\cdot p \in \mathbb{R}^3 : p\in C, \theta\in [0,2\pi]\}.$$ Now, I want to show that $S$ is a regular surface, in the sense that it satisfies Do Carmo's definition: A regular surface is $S\subset \mathbb{R}^3$ such that for each $p\in S$ there is $V$ open in $S$ , $U\subset \mathbb{R}^2$ open in $\mathbb{R}^2$ and $\mathbf{x}: U\to V$ such that $\mathbf{x}$ is differentiable, $\mathbf{x}$ is injective, $\mathbf{x}$ has injective derivative, that is, $d\mathbf{x}_q : \mathbb{R}^2\to \mathbb{R}^3$ is injective for each $q\in U$ . Now, without thinking too much about domains for a while, a natural choice for $\mathbf{x}$ would be as follows: we parametrize $C$ by $\alpha : I\subset \mathbb{R}\to \mathbb{R}^3$ and define $\mathbf{x} : [0,2\pi]\times I\to S$ $$\mathbf{x}(\theta,t)=R_z(\theta)\cdot \alpha(t).$$ This has two problems: $[0,2\pi]\times I$ is not open. Furthermore, $R_z(0)=R_z(2\pi)$ , hece we would not have injectivity. This problem would be solved if we replace $[0,2\pi]$ by $(0,2\pi)$ and if $I$ is an open interval. I can't identify the open set $V\subset S$ used with this. I mean, I know that this $\mathbf{x}$ should work, but I don't know how to find open $V$ in $S$ for each $p\in S$ so that there is one $\mathbf{x}$ like that. I mean, defining $\mathbf{x}$ isn't that hard. What I'm finding quite complicated is finding the correct domains on $\mathbb{R}^2$ and $S$ , and then proving inside the domains that $1,2,3$ are satisfied. How can this be done in this general context?","A surface of revolution is obtained by picking a curve on a plane and rotating it around some axis contained on the plane. Choosing the coordinates in a convenient manner, we can pick to be contained on the plane and chose the axis to be the axis. In that case, if is a rotation around the -axis, the surface of revolution will be Now, I want to show that is a regular surface, in the sense that it satisfies Do Carmo's definition: A regular surface is such that for each there is open in , open in and such that is differentiable, is injective, has injective derivative, that is, is injective for each . Now, without thinking too much about domains for a while, a natural choice for would be as follows: we parametrize by and define This has two problems: is not open. Furthermore, , hece we would not have injectivity. This problem would be solved if we replace by and if is an open interval. I can't identify the open set used with this. I mean, I know that this should work, but I don't know how to find open in for each so that there is one like that. I mean, defining isn't that hard. What I'm finding quite complicated is finding the correct domains on and , and then proving inside the domains that are satisfied. How can this be done in this general context?","C C xz z R_z(\theta) : \mathbb{R}^3\to \mathbb{R}^3 z S = \{R_z(\theta)\cdot p \in \mathbb{R}^3 : p\in C, \theta\in [0,2\pi]\}. S S\subset \mathbb{R}^3 p\in S V S U\subset \mathbb{R}^2 \mathbb{R}^2 \mathbf{x}: U\to V \mathbf{x} \mathbf{x} \mathbf{x} d\mathbf{x}_q : \mathbb{R}^2\to \mathbb{R}^3 q\in U \mathbf{x} C \alpha : I\subset \mathbb{R}\to \mathbb{R}^3 \mathbf{x} : [0,2\pi]\times I\to S \mathbf{x}(\theta,t)=R_z(\theta)\cdot \alpha(t). [0,2\pi]\times I R_z(0)=R_z(2\pi) [0,2\pi] (0,2\pi) I V\subset S \mathbf{x} V S p\in S \mathbf{x} \mathbf{x} \mathbb{R}^2 S 1,2,3","['real-analysis', 'analysis', 'multivariable-calculus', 'differential-geometry', 'surfaces']"
91,Wanting to know if my set up for these integrals using spherical and cylindrical coords is correct.,Wanting to know if my set up for these integrals using spherical and cylindrical coords is correct.,,"So as usual I've been doing problems out my book for practice, only to find one of the answers pages with this section torn out... I don't expect to get answers for all of them here, or even full answers for a few, but I was hoping maybe some could tell me if the set up on the few I've done is correct, after all, I can check the evaluation on symbolab. At least then when trying other problems I'll have something more to go off when it comes to the set up. If this isn't an ok question or should be broken up into separate ones, just let me know, just hoping to get all the practice I can. Using Cylindrical: $(A)$ $\int\int\int (z) dv$ where E is enclosed by the paraboloid $z=x^2+y^2$ and the plane $z=4$ From the fact that it's a paraboloid I have theta between $0$ and $\pi$, r between $0$ and 2 by plugging the given z value into the paraboloid equation, and z between 4 and $r^2$ which I got from the x,y equation swapping to polar, which is the part I'm not really sure about. $\int^\pi_0\int_0^2\int^4_{r^2}(zr)dzdrd\theta$ $(B)$ $\int\int\int_E (x) dv$ where E is the solid in the first octant that lies under the paraboloid $z=4-x^2-y^2$."" Since it's the first octant I have $0<=\theta<=\pi/2$, r between 0 and 4 from the z equation, and zero through the the z equation for z: $\int^{\pi/2}_0\int^4_0\int^{4-r^2}_0[(4-r^2)r]dzdrd\theta$ Using Spherical: $(C)$ $\int\int\int(xe^{x^2+y^2+z^2})dv$ where E is the portion of the unit ball $x^2+y^2+z^2<=1$ that lies in the first octant. From the first octant part I have theta between 0 and $\pi/2$, and from $\rho^2=x^2+y^2+z^2$ I have $\rho$ between zero and 1. I'm not sure how to determine $\phi$ in this one though going off a similar problem I seen I have it between zero and $\pi/4$. Replacing all the x and y's with the matching cartesian to spherical identities: $\int^{\pi/2}_0\int_0^{\pi/4}\int_0^1(\rho sin(\phi)e^{\rho^2}\rho^2 sin(\phi))d\rho d\phi d\theta$ ...And I think I'll leave it there. Have a couple more I tried in spherical but this seems like enough for one post.","So as usual I've been doing problems out my book for practice, only to find one of the answers pages with this section torn out... I don't expect to get answers for all of them here, or even full answers for a few, but I was hoping maybe some could tell me if the set up on the few I've done is correct, after all, I can check the evaluation on symbolab. At least then when trying other problems I'll have something more to go off when it comes to the set up. If this isn't an ok question or should be broken up into separate ones, just let me know, just hoping to get all the practice I can. Using Cylindrical: $(A)$ $\int\int\int (z) dv$ where E is enclosed by the paraboloid $z=x^2+y^2$ and the plane $z=4$ From the fact that it's a paraboloid I have theta between $0$ and $\pi$, r between $0$ and 2 by plugging the given z value into the paraboloid equation, and z between 4 and $r^2$ which I got from the x,y equation swapping to polar, which is the part I'm not really sure about. $\int^\pi_0\int_0^2\int^4_{r^2}(zr)dzdrd\theta$ $(B)$ $\int\int\int_E (x) dv$ where E is the solid in the first octant that lies under the paraboloid $z=4-x^2-y^2$."" Since it's the first octant I have $0<=\theta<=\pi/2$, r between 0 and 4 from the z equation, and zero through the the z equation for z: $\int^{\pi/2}_0\int^4_0\int^{4-r^2}_0[(4-r^2)r]dzdrd\theta$ Using Spherical: $(C)$ $\int\int\int(xe^{x^2+y^2+z^2})dv$ where E is the portion of the unit ball $x^2+y^2+z^2<=1$ that lies in the first octant. From the first octant part I have theta between 0 and $\pi/2$, and from $\rho^2=x^2+y^2+z^2$ I have $\rho$ between zero and 1. I'm not sure how to determine $\phi$ in this one though going off a similar problem I seen I have it between zero and $\pi/4$. Replacing all the x and y's with the matching cartesian to spherical identities: $\int^{\pi/2}_0\int_0^{\pi/4}\int_0^1(\rho sin(\phi)e^{\rho^2}\rho^2 sin(\phi))d\rho d\phi d\theta$ ...And I think I'll leave it there. Have a couple more I tried in spherical but this seems like enough for one post.",,"['integration', 'multivariable-calculus', 'spherical-coordinates', 'cylindrical-coordinates']"
92,Is there a mistake in the problem? Continuity of a two-variable function.,Is there a mistake in the problem? Continuity of a two-variable function.,,"Let $\phi : \mathbb{R} \rightarrow \mathbb{R}$ be a $C^2$ function, such that $\phi (0) = 0$ and $\phi''(0) \neq 0$. If $$ f(x,y) =  \begin{cases}  \frac{x\phi(y) - y\phi(x)}{x^2+y^2}, &\text{if } (x,y) \neq (0,0) \\  0, &\text{if } (x,y) = (0,0) \end{cases}, $$ then show that $f$ is continuous, but not $C^1$. I have tried solving it. The problem is, that if $\phi(x)=\sqrt{x}$, then $f$ would not be continuous (taking the path $y=2x$, and letting $x \rightarrow 0$ makes the limit $-\infty \neq 0$). Did I not understand the question properly? Is taking $\phi(x) = \sqrt{x}$ as a counter example a mistake? Is there some mistake with the problem? And if not, could I get some sort of hint on solving it, as I have tried a few methods, but I can't seem to solve it. Edit: I may have solved it, but I am not sure if it is correct. Let $\epsilon > 0$. We have \begin{align} |f(x,y) - f(0,0)| &= \frac{|x\phi(y)-y\phi(x)|}{x^2+y^2} \\ &\leq \frac{|x||\phi(y)|+|y||\phi(x)|}{x^2+y^2} \\ &\leq \frac{\sqrt{x^2+y^2}(|\phi(y)| + |\phi(x)|)}{x^2+y^2}, \quad \text{since }|x|,|y|\leq\sqrt{x^2+y^2} \\ &= \frac{|\phi(x)| + |\phi(y)|}{\sqrt{x^2+y^2}}. \end{align} For any of those $x,y \in \mathbb{R}^*$, because $\phi$ is continuous at $0$ with $\phi(0) = 0$, by having $\epsilon_1 = y^2 > 0$ and $\epsilon_2 = x^2 > 0$, there must be some $\delta_1, \delta_2 > 0$, such that if $|x|\leq\delta_1$ and $|y|\leq\delta_2$, then $|\phi(x)|\leq\epsilon_1=y^2$ and $|\phi(y)|\leq\epsilon_2=x^2$. From this, we pick $\delta_m = \min\{\delta_1, \delta_2\}$ and have $|x|,|y|\leq\delta_m$. Continuing \begin{align} \frac{|\phi(x)| + |\phi(y)|}{\sqrt{x^2+y^2}} &\leq \frac{x^2+y^2}{\sqrt{x^2+y^2}} \\ &= \sqrt{x^2+y^2} \\ &\leq |x| + |y| \\ &\leq 2\delta_m \end{align} We pick $\delta = \min\{\epsilon, 2\delta_m\}$, making $f$ continuous at $(0,0)$. Could someone tell me if taking $\epsilon_1 = y^2$ and $\epsilon_2 = x^2$ was a mistake?","Let $\phi : \mathbb{R} \rightarrow \mathbb{R}$ be a $C^2$ function, such that $\phi (0) = 0$ and $\phi''(0) \neq 0$. If $$ f(x,y) =  \begin{cases}  \frac{x\phi(y) - y\phi(x)}{x^2+y^2}, &\text{if } (x,y) \neq (0,0) \\  0, &\text{if } (x,y) = (0,0) \end{cases}, $$ then show that $f$ is continuous, but not $C^1$. I have tried solving it. The problem is, that if $\phi(x)=\sqrt{x}$, then $f$ would not be continuous (taking the path $y=2x$, and letting $x \rightarrow 0$ makes the limit $-\infty \neq 0$). Did I not understand the question properly? Is taking $\phi(x) = \sqrt{x}$ as a counter example a mistake? Is there some mistake with the problem? And if not, could I get some sort of hint on solving it, as I have tried a few methods, but I can't seem to solve it. Edit: I may have solved it, but I am not sure if it is correct. Let $\epsilon > 0$. We have \begin{align} |f(x,y) - f(0,0)| &= \frac{|x\phi(y)-y\phi(x)|}{x^2+y^2} \\ &\leq \frac{|x||\phi(y)|+|y||\phi(x)|}{x^2+y^2} \\ &\leq \frac{\sqrt{x^2+y^2}(|\phi(y)| + |\phi(x)|)}{x^2+y^2}, \quad \text{since }|x|,|y|\leq\sqrt{x^2+y^2} \\ &= \frac{|\phi(x)| + |\phi(y)|}{\sqrt{x^2+y^2}}. \end{align} For any of those $x,y \in \mathbb{R}^*$, because $\phi$ is continuous at $0$ with $\phi(0) = 0$, by having $\epsilon_1 = y^2 > 0$ and $\epsilon_2 = x^2 > 0$, there must be some $\delta_1, \delta_2 > 0$, such that if $|x|\leq\delta_1$ and $|y|\leq\delta_2$, then $|\phi(x)|\leq\epsilon_1=y^2$ and $|\phi(y)|\leq\epsilon_2=x^2$. From this, we pick $\delta_m = \min\{\delta_1, \delta_2\}$ and have $|x|,|y|\leq\delta_m$. Continuing \begin{align} \frac{|\phi(x)| + |\phi(y)|}{\sqrt{x^2+y^2}} &\leq \frac{x^2+y^2}{\sqrt{x^2+y^2}} \\ &= \sqrt{x^2+y^2} \\ &\leq |x| + |y| \\ &\leq 2\delta_m \end{align} We pick $\delta = \min\{\epsilon, 2\delta_m\}$, making $f$ continuous at $(0,0)$. Could someone tell me if taking $\epsilon_1 = y^2$ and $\epsilon_2 = x^2$ was a mistake?",,"['multivariable-calculus', 'continuity']"
93,Measure of a Borel set.,Measure of a Borel set.,,"Let $f: [0,1)\rightarrow [0,1)$ be defined by $$f(x)= \begin{cases}  \hfill 2x   \hfill & x < \frac12 \\ \hfill 2x-1 \hfill & \frac12\le x<1 \\ \end{cases}$$ Suppose $E$ is a Borel subset of $[0,1)$ such that $f^{-1}(E)=E$. Use a density point argument to show that $m(E)=0$ or $m(E)=1$. I have tried to use the symmetry of the preimages of $E$ to imply the measure of $E$ is arbitrarily close to 1 and therefore equal to 1. However, I still stuck on this. Could you help me with this? Thank you very much.","Let $f: [0,1)\rightarrow [0,1)$ be defined by $$f(x)= \begin{cases}  \hfill 2x   \hfill & x < \frac12 \\ \hfill 2x-1 \hfill & \frac12\le x<1 \\ \end{cases}$$ Suppose $E$ is a Borel subset of $[0,1)$ such that $f^{-1}(E)=E$. Use a density point argument to show that $m(E)=0$ or $m(E)=1$. I have tried to use the symmetry of the preimages of $E$ to imply the measure of $E$ is arbitrarily close to 1 and therefore equal to 1. However, I still stuck on this. Could you help me with this? Thank you very much.",,"['real-analysis', 'multivariable-calculus']"
94,Line integral of a conservative vector field,Line integral of a conservative vector field,,"I was trying to integrate the function $$f(x,y)=\left(\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2}\right)$$ from $(1,1)$ to $(1,-1)$ along any curve that does not intersect the line $L=\{(x,y)\in\mathbb{R}^2\:|\:\: y=0, x\geq 0\}$. My approach was to note that $$\frac{\partial f_x}{\partial y}=\frac{\partial f_y}{\partial x}$$ and that $\mathbb{R}^2-L$ is an simply connected subset of $\mathbb{R}^2$. This implies that $$\int f\cdot \mathrm{d}s$$ independs of the path and then integrated it along the circle of radius $\sqrt{2}$ that connects the points $(1,1)$ and $(1,-1)$. I arrived at the value $3\pi/2$ for the integral. However, I could define another simply connected subset of $\mathbb{R}^2$ such that I could integrate $f$ along the line that connects $(1,1)$ to $(1,-1)$. Then I would arrive at the result $-\pi/2$ for the integral. What is wrong here? Thanks!","I was trying to integrate the function $$f(x,y)=\left(\frac{-y}{x^2+y^2},\frac{x}{x^2+y^2}\right)$$ from $(1,1)$ to $(1,-1)$ along any curve that does not intersect the line $L=\{(x,y)\in\mathbb{R}^2\:|\:\: y=0, x\geq 0\}$. My approach was to note that $$\frac{\partial f_x}{\partial y}=\frac{\partial f_y}{\partial x}$$ and that $\mathbb{R}^2-L$ is an simply connected subset of $\mathbb{R}^2$. This implies that $$\int f\cdot \mathrm{d}s$$ independs of the path and then integrated it along the circle of radius $\sqrt{2}$ that connects the points $(1,1)$ and $(1,-1)$. I arrived at the value $3\pi/2$ for the integral. However, I could define another simply connected subset of $\mathbb{R}^2$ such that I could integrate $f$ along the line that connects $(1,1)$ to $(1,-1)$. Then I would arrive at the result $-\pi/2$ for the integral. What is wrong here? Thanks!",,"['calculus', 'multivariable-calculus', 'vector-analysis']"
95,Help with tricky double integral,Help with tricky double integral,,"Consider the region $R$ bounded by the circles   $$x^{2}+y^{2}=Ax$$$$x^{2}+y^{2}=Bx$$$$x^{2}+y^{2}=Cy$$$$x^{2}+y^{2}=Dy$$   where $B>A$ and $D>C$. Use the change of variables \begin{cases} u=\frac{x}{x^2+y^2}\\  v=\frac{y}{x^2+y^2} \end{cases} to evaluate the integral $$\int\int_R  \frac{dxdy}{(x^2+y^2)^3}\quad  (1)$$ My work so far : the Jacobian is $$\begin{bmatrix} \frac{\partial u}{\partial x} & \frac{\partial u}{\partial y}\\   \frac{\partial v}{\partial x}& \frac{\partial v}{\partial y} \end{bmatrix}$$ With $\frac{\partial v}{\partial x}=\frac{\partial u}{\partial y}=\frac{-2xy}{(x^2+y^2)^2} $ and $\frac{\partial u}{\partial x}=\frac{y^2-x^2}{(x^2+y^2)^2}$ and $\frac{\partial v}{\partial y}=\frac{x^2-y^2}{(x^2+y^2)^2}$ : $$J(u,v)=\frac{\partial (x,y)}{\partial (u,v)}=\frac{-1}{(x^2+y^2)^2} \quad(2)$$ My first approach to get the limits is rearranging $x^{2}+y^{2}=Ax$ and hence $\frac{1}{A}=\frac{x}{x^2+y^2}$ hence $1/A$ is a limit and the rest is $1/B$, $1/C$ and $1/D$. My questions: How do I find the limits of integration? How do I express the integral is terms of $u$ and $v$ ? Any tips will be appreciated thanks!!","Consider the region $R$ bounded by the circles   $$x^{2}+y^{2}=Ax$$$$x^{2}+y^{2}=Bx$$$$x^{2}+y^{2}=Cy$$$$x^{2}+y^{2}=Dy$$   where $B>A$ and $D>C$. Use the change of variables \begin{cases} u=\frac{x}{x^2+y^2}\\  v=\frac{y}{x^2+y^2} \end{cases} to evaluate the integral $$\int\int_R  \frac{dxdy}{(x^2+y^2)^3}\quad  (1)$$ My work so far : the Jacobian is $$\begin{bmatrix} \frac{\partial u}{\partial x} & \frac{\partial u}{\partial y}\\   \frac{\partial v}{\partial x}& \frac{\partial v}{\partial y} \end{bmatrix}$$ With $\frac{\partial v}{\partial x}=\frac{\partial u}{\partial y}=\frac{-2xy}{(x^2+y^2)^2} $ and $\frac{\partial u}{\partial x}=\frac{y^2-x^2}{(x^2+y^2)^2}$ and $\frac{\partial v}{\partial y}=\frac{x^2-y^2}{(x^2+y^2)^2}$ : $$J(u,v)=\frac{\partial (x,y)}{\partial (u,v)}=\frac{-1}{(x^2+y^2)^2} \quad(2)$$ My first approach to get the limits is rearranging $x^{2}+y^{2}=Ax$ and hence $\frac{1}{A}=\frac{x}{x^2+y^2}$ hence $1/A$ is a limit and the rest is $1/B$, $1/C$ and $1/D$. My questions: How do I find the limits of integration? How do I express the integral is terms of $u$ and $v$ ? Any tips will be appreciated thanks!!",,"['integration', 'limits', 'multivariable-calculus']"
96,How to change the order of integration when angles and trigonometric functions are involved in limits?,How to change the order of integration when angles and trigonometric functions are involved in limits?,,"Problem: Change the order of integration of $$\int_0^{\pi/2}\int_0^{\cos(\theta)}\ \cos{(\theta)}\ dr\,d\theta$$ Solution: First, I've made a plot of the given region: $$0\leqslant\ r \leqslant \cos(\theta)$$ $$0\leqslant\ \theta \leqslant \pi/2$$ I have tried to define the new limits, $$\int_0^1\int_0^{\cos(\theta)}\ \cos(\theta)\ d\theta\, dr$$ Some suggestions, tips,... to understand how to define limits when angles and trigonometric functions are involved in the original limits?","Problem: Change the order of integration of $$\int_0^{\pi/2}\int_0^{\cos(\theta)}\ \cos{(\theta)}\ dr\,d\theta$$ Solution: First, I've made a plot of the given region: $$0\leqslant\ r \leqslant \cos(\theta)$$ $$0\leqslant\ \theta \leqslant \pi/2$$ I have tried to define the new limits, $$\int_0^1\int_0^{\cos(\theta)}\ \cos(\theta)\ d\theta\, dr$$ Some suggestions, tips,... to understand how to define limits when angles and trigonometric functions are involved in the original limits?",,['multivariable-calculus']
97,A question concerning the directional derivative of a function,A question concerning the directional derivative of a function,,"As I understand it, intuitively the directional derivative of a function $f$, at some point, describes its rate of change as one moves along a direction, specified by some vector $\mathbf{v}$, away from that point. With this in mind I am trying to understand the mathematical formalism a bit deeper, but I'm unsure if I am doing so correctly. This is my understanding so far (apologies for any abuse of notation): Let $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ be some smooth function, and let $c:\mathbb{R}\rightarrow\mathbb{R}^{n}$ be some smooth curve in $\mathbb{R}^{n}$, with coordinates $(x\circ c)(t)\equiv x(t)=(x^{1}(t),\ldots ,x^{n}(t))$. In a sufficiently small neighbourhood of a point $x(0)=a$ we can describe the curve linearly as $$x(t)=a+t\mathbf{v}$$ where $\mathbf{v}$ is the tangent vector to the curve $x(t)$ at the point $x(0)=a=(a^{1},\ldots ,a^{n})$. Given this, we can then determine the rate of change in the function $f$ along the direction defined by $\mathbf{v}$ by evaluating $f$ along the line segment of the curve, $x(t)=a+t\mathbf{v}$, such that $f=(f\circ x)(t)=f(x(t))$. In doing so, we can now consider $f$ as a composite function of $t$, such that, upon taking its derivative with respect to $t$ we will determine its rate of change along $\mathbf{v}$. Indeed, $$\frac{df(x(t))}{dt}\bigg\vert_{t=0}=\sum_{i=1}^{n}\frac{\partial f(x(t))}{\partial x^{i}}\frac{d x^{i}}{dt}\bigg\vert_{t=0}=\nabla f(a)\cdot\mathbf{v}$$ where $\nabla f=\left(\frac{\partial f(x(t))}{\partial x^{1}},\ldots,\frac{\partial f(x(t))}{\partial x^{n}}\right)$ is the gradient of $f$, and $v^{i}=\frac{d x^{i}}{dt}$ are the components of the tangent vector at the point $x(0)=a$. Would this be a correct understanding at all?","As I understand it, intuitively the directional derivative of a function $f$, at some point, describes its rate of change as one moves along a direction, specified by some vector $\mathbf{v}$, away from that point. With this in mind I am trying to understand the mathematical formalism a bit deeper, but I'm unsure if I am doing so correctly. This is my understanding so far (apologies for any abuse of notation): Let $f:\mathbb{R}^{n}\rightarrow\mathbb{R}$ be some smooth function, and let $c:\mathbb{R}\rightarrow\mathbb{R}^{n}$ be some smooth curve in $\mathbb{R}^{n}$, with coordinates $(x\circ c)(t)\equiv x(t)=(x^{1}(t),\ldots ,x^{n}(t))$. In a sufficiently small neighbourhood of a point $x(0)=a$ we can describe the curve linearly as $$x(t)=a+t\mathbf{v}$$ where $\mathbf{v}$ is the tangent vector to the curve $x(t)$ at the point $x(0)=a=(a^{1},\ldots ,a^{n})$. Given this, we can then determine the rate of change in the function $f$ along the direction defined by $\mathbf{v}$ by evaluating $f$ along the line segment of the curve, $x(t)=a+t\mathbf{v}$, such that $f=(f\circ x)(t)=f(x(t))$. In doing so, we can now consider $f$ as a composite function of $t$, such that, upon taking its derivative with respect to $t$ we will determine its rate of change along $\mathbf{v}$. Indeed, $$\frac{df(x(t))}{dt}\bigg\vert_{t=0}=\sum_{i=1}^{n}\frac{\partial f(x(t))}{\partial x^{i}}\frac{d x^{i}}{dt}\bigg\vert_{t=0}=\nabla f(a)\cdot\mathbf{v}$$ where $\nabla f=\left(\frac{\partial f(x(t))}{\partial x^{1}},\ldots,\frac{\partial f(x(t))}{\partial x^{n}}\right)$ is the gradient of $f$, and $v^{i}=\frac{d x^{i}}{dt}$ are the components of the tangent vector at the point $x(0)=a$. Would this be a correct understanding at all?",,"['multivariable-calculus', 'derivatives', 'intuition', 'curves']"
98,Ways to evaluate $\int_0^1 \int_0^1 \frac{1}{1-xy}dxdy = \frac{\pi^2}{6}$ [duplicate],Ways to evaluate  [duplicate],\int_0^1 \int_0^1 \frac{1}{1-xy}dxdy = \frac{\pi^2}{6},"This question already has an answer here : How to evaluate $\int_0^1\int_0^1 \frac{1}{1-xy} \, dy \, dx$ to prove $\sum_{n=1}^{\infty} \frac{1}{n^2}=\frac{\pi^2}{6}$. (1 answer) Closed 7 years ago . A classmate told me this, but he didn't tell me how to evaluate the integral. $$\int_0^1 \int_0^1 \frac{1}{1-xy}dxdy=\int_0^1 \int_0^1\sum_{n=0}^\infty (xy)^n dxdy=\sum_{n=1}^\infty \frac{1}{n^2}=\zeta(2)$$ So if you can evaluate that integral this might be an easy way to solve the Basel problem. I tried substituting $\frac{1}{x}=v$ and $\frac{1}{y}=t$ to get $$\zeta(2)=\int_1^\infty\int_1^\infty\frac{1}{vt-1}-\frac{1}{vt}dvdt=\lim_{N\to\infty}\left(\int_1^N\int_1^N\frac{1}{vt-1}dvdt-\ln(N)^2\right)$$ The areas of the cross sections of $\frac{1}{vt-1}$ are going to be infinite near $(1,1)$ so this seems like a dead end.","This question already has an answer here : How to evaluate $\int_0^1\int_0^1 \frac{1}{1-xy} \, dy \, dx$ to prove $\sum_{n=1}^{\infty} \frac{1}{n^2}=\frac{\pi^2}{6}$. (1 answer) Closed 7 years ago . A classmate told me this, but he didn't tell me how to evaluate the integral. $$\int_0^1 \int_0^1 \frac{1}{1-xy}dxdy=\int_0^1 \int_0^1\sum_{n=0}^\infty (xy)^n dxdy=\sum_{n=1}^\infty \frac{1}{n^2}=\zeta(2)$$ So if you can evaluate that integral this might be an easy way to solve the Basel problem. I tried substituting $\frac{1}{x}=v$ and $\frac{1}{y}=t$ to get $$\zeta(2)=\int_1^\infty\int_1^\infty\frac{1}{vt-1}-\frac{1}{vt}dvdt=\lim_{N\to\infty}\left(\int_1^N\int_1^N\frac{1}{vt-1}dvdt-\ln(N)^2\right)$$ The areas of the cross sections of $\frac{1}{vt-1}$ are going to be infinite near $(1,1)$ so this seems like a dead end.",,"['integration', 'multivariable-calculus', 'definite-integrals', 'riemann-zeta']"
99,How to find the field lines of a vector field?,How to find the field lines of a vector field?,,"I need help finding the field lines of a vector field. I hesitant if the procedure and solution is correct. The vector field is $$\mathbf{F}(x,y)=\frac{-y}{x^2+y^2}\mathbf{\hat x}+\frac{x}{x^2+y^2}\mathbf{\hat{y}}$$ So I should solve the equation $$ \mathbf{F}(\mathbf{r}(t))=\frac{d\mathbf{r}(t)}{dt}, \quad \text{where} \quad \mathbf{r}(t)=x(t)\mathbf{\hat x}+y(t)\mathbf{\hat y} $$ Therefore I have the equations$$ \frac{dx(t)}{dt}=\frac{-y}{x^2+y^2} \tag{1} $$ $$ \frac{dy(t)}{dt}=\frac{x}{x^2+y^2} \tag{2} $$ The first one is $$ \frac{x^2+y^2}{-y}dx=dt $$ $$ \Longrightarrow t=-y-\frac{x^3}{3y}+C_1 \tag{3} $$ And the second equation is $$ \frac{x^2+y^2}{x}dy=dt $$ $$ \Longrightarrow t=x+\frac{y^3}{3x}+C_2 \tag{4} $$ And let $(3)=(4)$ so $$ -y-\frac{x^3}{3y}+C_1= x+\frac{y^3}{3x}+C_2 $$ Let $-(C_1-C_2)=C_3$ so $$ y+\frac{x^3}{3y}+x+\frac{y^3}{3x}+C_3=0 $$ Is this correct? How can i interpret this equation in the $(x,y,z)$-space?","I need help finding the field lines of a vector field. I hesitant if the procedure and solution is correct. The vector field is $$\mathbf{F}(x,y)=\frac{-y}{x^2+y^2}\mathbf{\hat x}+\frac{x}{x^2+y^2}\mathbf{\hat{y}}$$ So I should solve the equation $$ \mathbf{F}(\mathbf{r}(t))=\frac{d\mathbf{r}(t)}{dt}, \quad \text{where} \quad \mathbf{r}(t)=x(t)\mathbf{\hat x}+y(t)\mathbf{\hat y} $$ Therefore I have the equations$$ \frac{dx(t)}{dt}=\frac{-y}{x^2+y^2} \tag{1} $$ $$ \frac{dy(t)}{dt}=\frac{x}{x^2+y^2} \tag{2} $$ The first one is $$ \frac{x^2+y^2}{-y}dx=dt $$ $$ \Longrightarrow t=-y-\frac{x^3}{3y}+C_1 \tag{3} $$ And the second equation is $$ \frac{x^2+y^2}{x}dy=dt $$ $$ \Longrightarrow t=x+\frac{y^3}{3x}+C_2 \tag{4} $$ And let $(3)=(4)$ so $$ -y-\frac{x^3}{3y}+C_1= x+\frac{y^3}{3x}+C_2 $$ Let $-(C_1-C_2)=C_3$ so $$ y+\frac{x^3}{3y}+x+\frac{y^3}{3x}+C_3=0 $$ Is this correct? How can i interpret this equation in the $(x,y,z)$-space?",,['multivariable-calculus']
