,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Obtain a special solution for a differential equation,Obtain a special solution for a differential equation,,"If I have a differential equation $y"" + 4y' + 4y = 2e^{-x} + xe^{-x}$ and $f(x)$ is a special solution to this equation, how can I determine $f(x)$ if I know f(0)=0 and $f'(0)=0$ ?","If I have a differential equation $y"" + 4y' + 4y = 2e^{-x} + xe^{-x}$ and $f(x)$ is a special solution to this equation, how can I determine $f(x)$ if I know f(0)=0 and $f'(0)=0$ ?",,"['linear-algebra', 'ordinary-differential-equations']"
1,Making a Piecewise Function a Single Function,Making a Piecewise Function a Single Function,,Is there a way to turn a piecewise function into one function. For example: $$\ f(x)=\begin{cases} g(x) & \text{if $a≤x<b $} \\ h(x) & \text{if $b≤x≤d$} \end{cases}$$ (Can you use the Heaviside Step Function? $\theta(x))$,Is there a way to turn a piecewise function into one function. For example: $$\ f(x)=\begin{cases} g(x) & \text{if $a≤x<b $} \\ h(x) & \text{if $b≤x≤d$} \end{cases}$$ (Can you use the Heaviside Step Function? $\theta(x))$,,"['calculus', 'ordinary-differential-equations']"
2,Simple ODE with given initial value,Simple ODE with given initial value,,"Find all solutions of $$2x'=3x^{1/3}, \ x(0)=0,\ x(1)=a$$ for which $\lim_{t\rightarrow -\infty}x(t) = 0 $, in respect to $a$ parameter, $a\in\mathbb{R}$. It's quite easy to notice that the solutions are $x(t)=t^{3/2}, \ x(t)=-t^{3/2}$ and$ \ x(t)=0$. Does that mean the only solution that meets this exercise requirements is $x(t)\equiv0$ along with $a=0$?","Find all solutions of $$2x'=3x^{1/3}, \ x(0)=0,\ x(1)=a$$ for which $\lim_{t\rightarrow -\infty}x(t) = 0 $, in respect to $a$ parameter, $a\in\mathbb{R}$. It's quite easy to notice that the solutions are $x(t)=t^{3/2}, \ x(t)=-t^{3/2}$ and$ \ x(t)=0$. Does that mean the only solution that meets this exercise requirements is $x(t)\equiv0$ along with $a=0$?",,['ordinary-differential-equations']
3,A First Order Nonlinear ODE,A First Order Nonlinear ODE,,"Is there a closed form solution for this first order ODE of $x(t)$ where $a(t)$ is as smooth as needed? $$x'(t)=\frac{a(t)}{x(t)}+b$$ Here, $b$ is a constant. But what if $b$ is also a function of $t$? But I do not want a series expansion formula with recursion equations resulting from the Frobenius method.","Is there a closed form solution for this first order ODE of $x(t)$ where $a(t)$ is as smooth as needed? $$x'(t)=\frac{a(t)}{x(t)}+b$$ Here, $b$ is a constant. But what if $b$ is also a function of $t$? But I do not want a series expansion formula with recursion equations resulting from the Frobenius method.",,['ordinary-differential-equations']
4,Solve the differential inequality,Solve the differential inequality,,"Let $\sigma$ be a differentiable function satisfying the differential inequality $$|\sigma'(x)|\le K\sigma(x)$$ for $\infty \le x \le \infty$ where K is a constant.Then, $$\sigma(x) \le \sigma(a)e^{K|x-a|}$$, for any $-\infty \le a \le \infty$ I'm having trouble with the modulus sign and also do I've to deal with cases $a<x$ and $x<a$ separately?","Let $\sigma$ be a differentiable function satisfying the differential inequality $$|\sigma'(x)|\le K\sigma(x)$$ for $\infty \le x \le \infty$ where K is a constant.Then, $$\sigma(x) \le \sigma(a)e^{K|x-a|}$$, for any $-\infty \le a \le \infty$ I'm having trouble with the modulus sign and also do I've to deal with cases $a<x$ and $x<a$ separately?",,['ordinary-differential-equations']
5,what is the solution for this EDO y'y'' = 1,what is the solution for this EDO y'y'' = 1,,"I´ll appreciate any help with this EDO y'y''=1, In some posts on internet theres a trick: multiply on each side y' and integrate but im not capable of doing that Thanks, sorry if my english is bad.","I´ll appreciate any help with this EDO y'y''=1, In some posts on internet theres a trick: multiply on each side y' and integrate but im not capable of doing that Thanks, sorry if my english is bad.",,['ordinary-differential-equations']
6,Help with Differential Equation,Help with Differential Equation,,"Our differential equation is:   $$ y' + 2y/3 = 1-t/2 $$   Consider $y_0$ and find the value for which the solution of our differential equation touches, but does not cross, the $t$-axis. EDIT I've resolved the original discrepancy that spurned this question. However, I'm having trouble answering the actual question above. That is, I am having trouble finding the point at which Y touches, but does not cross, the t-axis and solving for the solution. I have solved for $y=-3t/4+(21/8)+Ce^{-2t/3}$. This is correct, as confirmed by other posters below. Must I input this into some program like Matlab or can I answer this question using only pen and paper?","Our differential equation is:   $$ y' + 2y/3 = 1-t/2 $$   Consider $y_0$ and find the value for which the solution of our differential equation touches, but does not cross, the $t$-axis. EDIT I've resolved the original discrepancy that spurned this question. However, I'm having trouble answering the actual question above. That is, I am having trouble finding the point at which Y touches, but does not cross, the t-axis and solving for the solution. I have solved for $y=-3t/4+(21/8)+Ce^{-2t/3}$. This is correct, as confirmed by other posters below. Must I input this into some program like Matlab or can I answer this question using only pen and paper?",,['ordinary-differential-equations']
7,Matrix with eigenvalues no negatives: What is $\lim_{t\to\infty} e^{tA}$?,Matrix with eigenvalues no negatives: What is ?,\lim_{t\to\infty} e^{tA},"Here's a homework question I've been stuck on for a while. My question is what can you tell about $$\lim_{t\rightarrow\infty}e^{tA}$$ if $A$ is $n\times n$ matrix and you know that every eigenvalue of $A$ has negative real part?. I think it's zero, but I don't know how to prove it!! Thanks!","Here's a homework question I've been stuck on for a while. My question is what can you tell about $$\lim_{t\rightarrow\infty}e^{tA}$$ if $A$ is $n\times n$ matrix and you know that every eigenvalue of $A$ has negative real part?. I think it's zero, but I don't know how to prove it!! Thanks!",,"['linear-algebra', 'ordinary-differential-equations', 'limits']"
8,$y'' + 4y = \sin^3(2x)$ Continuation of question,Continuation of question,y'' + 4y = \sin^3(2x),"I have to find the solution for $y'' + 4y = \sin^3 (2x)$. We can use an identity to transform our equation to: $y'' + 4y = (3/4)\sin(2x) - (1/4) \sin(6x)$.  Our guess for the particular solution would then be: $y_p = A\sin(2x) + B\sin(6x)$. I am, however, having trouble solving for A and B. $y_p = A\sin(2x) + B\sin(6x)4$ $y^{(1)}_p = 2A\cos(2x) + 6B\cos(6x)$ $y^{(2)}_p = -4(A\sin(2x) + 9B\sin(6x))$ So then we plug these back into our non-homogeneous differential equation, $y'' + 4y = sin^3(2x)$, and we get: $-4(A\sin(2x) + 9B\sin(6x)) + 4(A\sin(2x) + B\sin(6x)) = (3/4)\sin(2x) - (1/4)\sin(6x)$ Simplifying we will find that the sines with the A coefficient cancel out. How do we then find what A is if they always cancel out? This is a continuation of: Undetermined Coefficients trouble Is my particular solution guess correct?","I have to find the solution for $y'' + 4y = \sin^3 (2x)$. We can use an identity to transform our equation to: $y'' + 4y = (3/4)\sin(2x) - (1/4) \sin(6x)$.  Our guess for the particular solution would then be: $y_p = A\sin(2x) + B\sin(6x)$. I am, however, having trouble solving for A and B. $y_p = A\sin(2x) + B\sin(6x)4$ $y^{(1)}_p = 2A\cos(2x) + 6B\cos(6x)$ $y^{(2)}_p = -4(A\sin(2x) + 9B\sin(6x))$ So then we plug these back into our non-homogeneous differential equation, $y'' + 4y = sin^3(2x)$, and we get: $-4(A\sin(2x) + 9B\sin(6x)) + 4(A\sin(2x) + B\sin(6x)) = (3/4)\sin(2x) - (1/4)\sin(6x)$ Simplifying we will find that the sines with the A coefficient cancel out. How do we then find what A is if they always cancel out? This is a continuation of: Undetermined Coefficients trouble Is my particular solution guess correct?",,['ordinary-differential-equations']
9,Is the following differential equation exact?,Is the following differential equation exact?,,"I have the following equation: $$ydx-(x+y^2)dy=0. $$ It seems to me the equation is not exact ($My=1, Nx=-1$), but then I don't know how to solve it. So is it exact or not? And if not, how can it be solved?","I have the following equation: $$ydx-(x+y^2)dy=0. $$ It seems to me the equation is not exact ($My=1, Nx=-1$), but then I don't know how to solve it. So is it exact or not? And if not, how can it be solved?",,['ordinary-differential-equations']
10,Is my solution of the following differential equation wrong?,Is my solution of the following differential equation wrong?,,"I had to solve the following differential equation: $(x^2+y)\mathrm{d}x - x \mathrm{d}y=0$. The equation is not exact and so I solved it as a simple linear equation $$ \frac{\mathrm{d}y}{\mathrm{d}x}-\frac{y}{x}=x$$ The solution I got is $\frac{y}{x}=x+c$. However, the solution in the textbook is $-\frac{y}{x}+x=c$. I tried solve it again and again, and yet could not find where I'm wrong. Or is the solution in the book incorrect?","I had to solve the following differential equation: $(x^2+y)\mathrm{d}x - x \mathrm{d}y=0$. The equation is not exact and so I solved it as a simple linear equation $$ \frac{\mathrm{d}y}{\mathrm{d}x}-\frac{y}{x}=x$$ The solution I got is $\frac{y}{x}=x+c$. However, the solution in the textbook is $-\frac{y}{x}+x=c$. I tried solve it again and again, and yet could not find where I'm wrong. Or is the solution in the book incorrect?",,['ordinary-differential-equations']
11,Confused over the solution of partial differential equation $xu_x+u_t=0$,Confused over the solution of partial differential equation,xu_x+u_t=0,"Consider, $$ \displaystyle x\frac{\partial u}{\partial x}+\frac{\partial  u}{\partial t} = 0 $$ with initial values $ t = 0 : \ u(x, 0) = f(x) $ and calculate the  solution $ u(x,t) $ of the above Cauchy problem using the method of  characteristics. And here is the solution, I will point out where i am stuck: We parametrise the initial conditions by $\mathbb n:x_0(\mathbb n)=\mathbb n$, $t_0(\mathbb n)=0, u_0(\mathbb n)=f(\mathbb n)$ Solve the characteristic equations $$     \matrix{      \frac{\partial x(\sigma,\mathbb n)}{\partial \sigma} = x, && x(0,\mathbb n)=n \\       \frac{\partial t(\sigma,\mathbb n)}{\partial \sigma} = 1, && x(0,\mathbb n)=n \\      t(0,\mathbb n)=0: && x(\sigma,\mathbb n)=e^{\sigma}\mathbb n \\      t(\sigma,\mathbb n)=\sigma     }     $$ This is where i am stuck and confused How did they get $x(\sigma,\mathbb n)=e^{\sigma}\mathbb n$? I just cannot see where the $e$ came from, please forgive my stupidity but can someone please tell me how they got this solution? When i integrate i do not get this! I will put the rest of the solution so the reader can follow, I am only stuck with the part mentioned though Calculate $\sigma$ and $\mathbb n$ in terms of $x$ and $t$ (coordinate change) $$ \sigma = t, \mathbb n = xe^-{t} $$ Solve the compatibility condition $ \frac{\partial u}{\partial \sigma} = 0, u(0,\mathbb n) = f(\mathbb n) $. Hence, $u(\sigma)=f(\mathbb n)$ Hence we have $u(x,t)=f(xe^{-t})$","Consider, $$ \displaystyle x\frac{\partial u}{\partial x}+\frac{\partial  u}{\partial t} = 0 $$ with initial values $ t = 0 : \ u(x, 0) = f(x) $ and calculate the  solution $ u(x,t) $ of the above Cauchy problem using the method of  characteristics. And here is the solution, I will point out where i am stuck: We parametrise the initial conditions by $\mathbb n:x_0(\mathbb n)=\mathbb n$, $t_0(\mathbb n)=0, u_0(\mathbb n)=f(\mathbb n)$ Solve the characteristic equations $$     \matrix{      \frac{\partial x(\sigma,\mathbb n)}{\partial \sigma} = x, && x(0,\mathbb n)=n \\       \frac{\partial t(\sigma,\mathbb n)}{\partial \sigma} = 1, && x(0,\mathbb n)=n \\      t(0,\mathbb n)=0: && x(\sigma,\mathbb n)=e^{\sigma}\mathbb n \\      t(\sigma,\mathbb n)=\sigma     }     $$ This is where i am stuck and confused How did they get $x(\sigma,\mathbb n)=e^{\sigma}\mathbb n$? I just cannot see where the $e$ came from, please forgive my stupidity but can someone please tell me how they got this solution? When i integrate i do not get this! I will put the rest of the solution so the reader can follow, I am only stuck with the part mentioned though Calculate $\sigma$ and $\mathbb n$ in terms of $x$ and $t$ (coordinate change) $$ \sigma = t, \mathbb n = xe^-{t} $$ Solve the compatibility condition $ \frac{\partial u}{\partial \sigma} = 0, u(0,\mathbb n) = f(\mathbb n) $. Hence, $u(\sigma)=f(\mathbb n)$ Hence we have $u(x,t)=f(xe^{-t})$",,"['ordinary-differential-equations', 'partial-differential-equations']"
12,The curvature of a Cycloid at its cusps.,The curvature of a Cycloid at its cusps.,,"My lecturer proposed a question to particular result regarding the curvature of a Cycloid (generated by circle of radius 1) at its cusps. Having left it as an open problem, I thought it'd be interesting to share it here and find a hopeful answer that I can share with him. To begin, first consider a curvature function with respect to $t$ , given by $$\kappa(t)=t.$$ Now, observe that as $t\to\infty$ , $\kappa(t)\to\infty$ . For a regular curve to satisfy this curvature function, it would have to more or less, look like this (consider the first quadrant): Now that this has been established, onto the cycloid (with $x(t)=t-\sin t$ and $y=1-\cos t$ ). The cycloid has curvature equation $$\kappa(t)=-\frac{1}{4\vert\sin\frac{t}{2}\vert}.$$ Now, as $t\to0$ (that is, when approaching the cusp at $t=0$ ), $\vert\sin\frac{t}{2}\vert \to 0$ , thus implying that $\kappa \to \infty$ . However, inspecting the behaviour of the curvature of the cycloid, [![][1]][1] one can see that the behaviour of the curve does not behave in the same way as Euler's Spiral. My lecturer wants to know why? Thank you all in advanced for your ideas and help.","My lecturer proposed a question to particular result regarding the curvature of a Cycloid (generated by circle of radius 1) at its cusps. Having left it as an open problem, I thought it'd be interesting to share it here and find a hopeful answer that I can share with him. To begin, first consider a curvature function with respect to , given by Now, observe that as , . For a regular curve to satisfy this curvature function, it would have to more or less, look like this (consider the first quadrant): Now that this has been established, onto the cycloid (with and ). The cycloid has curvature equation Now, as (that is, when approaching the cusp at ), , thus implying that . However, inspecting the behaviour of the curvature of the cycloid, [![][1]][1] one can see that the behaviour of the curve does not behave in the same way as Euler's Spiral. My lecturer wants to know why? Thank you all in advanced for your ideas and help.",t \kappa(t)=t. t\to\infty \kappa(t)\to\infty x(t)=t-\sin t y=1-\cos t \kappa(t)=-\frac{1}{4\vert\sin\frac{t}{2}\vert}. t\to0 t=0 \vert\sin\frac{t}{2}\vert \to 0 \kappa \to \infty,"['ordinary-differential-equations', 'limits', 'differential-geometry', 'curvature']"
13,Cauchy Problem and Region of Validity,Cauchy Problem and Region of Validity,,"I have the Cauchy Problem $$ 2xu_x+(x+y)u_y=2u $$ with data $$ u(x,-x)=\sqrt{x},x>0$$ Omitting details, my answer is $$u(x,y) =\sqrt{x}\left ( \frac{y}{2x}-\frac{1}{2} \right)^{-\frac{1}{3}} $$ or $$u(x,y) =\frac{2^{\frac{1}{3}}x^{\frac{5}{6}}}{\left ( y-x \right)^{\frac{1}{3}} } $$ If you solve the Cauchy problem, do you arrive at the same $u(x,y)$?  What about the region of validity? It seems this becomes invalid when $y=x$ and when $x<0$, so I propose that my region of validity is $x>0$, $y<x$","I have the Cauchy Problem $$ 2xu_x+(x+y)u_y=2u $$ with data $$ u(x,-x)=\sqrt{x},x>0$$ Omitting details, my answer is $$u(x,y) =\sqrt{x}\left ( \frac{y}{2x}-\frac{1}{2} \right)^{-\frac{1}{3}} $$ or $$u(x,y) =\frac{2^{\frac{1}{3}}x^{\frac{5}{6}}}{\left ( y-x \right)^{\frac{1}{3}} } $$ If you solve the Cauchy problem, do you arrive at the same $u(x,y)$?  What about the region of validity? It seems this becomes invalid when $y=x$ and when $x<0$, so I propose that my region of validity is $x>0$, $y<x$",,"['ordinary-differential-equations', 'partial-differential-equations', 'partial-derivative']"
14,Differential equation: $y + t\frac{dy}{dt} = \frac{y}{y^2 - 1} $,Differential equation:,y + t\frac{dy}{dt} = \frac{y}{y^2 - 1} ,"$$y + t\frac{dy}{dt} = \frac{y}{y^2 - 1} \Rightarrow t\frac{dy}{dt} = \frac{-y^3 + 2y}{y^2 - 1} \Rightarrow \frac{1}{t} dt = \frac{y^2 - 1}{-y^3 + 2y}dy \Rightarrow$$ $$\Rightarrow \ln |t| = - \int \frac{y^2 - 1}{y^3 - 2y}dy$$ How to solve the integral in the right part? Had it been $\frac{3y^2 - 2}{y^3 - 2y}$, then the answer would be  $\ln |y^3 - 2y|$.","$$y + t\frac{dy}{dt} = \frac{y}{y^2 - 1} \Rightarrow t\frac{dy}{dt} = \frac{-y^3 + 2y}{y^2 - 1} \Rightarrow \frac{1}{t} dt = \frac{y^2 - 1}{-y^3 + 2y}dy \Rightarrow$$ $$\Rightarrow \ln |t| = - \int \frac{y^2 - 1}{y^3 - 2y}dy$$ How to solve the integral in the right part? Had it been $\frac{3y^2 - 2}{y^3 - 2y}$, then the answer would be  $\ln |y^3 - 2y|$.",,['ordinary-differential-equations']
15,Solve 2 connected ODEs describing a domain,Solve 2 connected ODEs describing a domain,,"This problem confused me for a long time. I have 2 ODEs which describe part of our domain. They are connected at middle: $$ \frac{d^2}{dx^2} u = -a, x<x_0  $$ $$ \frac{d^2}{dx^2} u - \frac{u}{b^2}= 0, x>x_0  $$ The whole domain is defined as $$ -W < x   $$ With the following conditions $$ \lim_{x->-W} u = 0   $$ $$ \lim_{x->+\infty} u = c \;\;or\;\; \lim_{x->L} u = c  $$ u, u' and u'' needs to be continous. a, b, W and c are constants. If not possible with $\infty$ one can replace $+\infty$ with a constant L. Update: As a hint, the following functions are solutions of individual equations $$u_1(x) = \alpha x^2 + \beta x + \gamma  $$     $$u_2(x) = c_1 exp(-x/b) + c_2 exp(x/b) $$ Where $\alpha$, $\beta$, $\gamma$, $c_1$ and $c_2$ are constants related to the constants in the ODEs. Problem is that we usually put $c_2 = 0$ since the exp(x/b) diverges.     If we do so, 2nd derivative of u would not be continous at $x_0$. My numerical simulated solution looks like  $$ \frac{1}{1+exp(x/b)}$$ But it does't match exactly. The form of numerical solution and 2 regions are clear in the following link: http://www.wolframalpha.com/input/?i=1%2F%281%2Bexp%28x%29%29+from+x%3D-10+to+x%3D10","This problem confused me for a long time. I have 2 ODEs which describe part of our domain. They are connected at middle: $$ \frac{d^2}{dx^2} u = -a, x<x_0  $$ $$ \frac{d^2}{dx^2} u - \frac{u}{b^2}= 0, x>x_0  $$ The whole domain is defined as $$ -W < x   $$ With the following conditions $$ \lim_{x->-W} u = 0   $$ $$ \lim_{x->+\infty} u = c \;\;or\;\; \lim_{x->L} u = c  $$ u, u' and u'' needs to be continous. a, b, W and c are constants. If not possible with $\infty$ one can replace $+\infty$ with a constant L. Update: As a hint, the following functions are solutions of individual equations $$u_1(x) = \alpha x^2 + \beta x + \gamma  $$     $$u_2(x) = c_1 exp(-x/b) + c_2 exp(x/b) $$ Where $\alpha$, $\beta$, $\gamma$, $c_1$ and $c_2$ are constants related to the constants in the ODEs. Problem is that we usually put $c_2 = 0$ since the exp(x/b) diverges.     If we do so, 2nd derivative of u would not be continous at $x_0$. My numerical simulated solution looks like  $$ \frac{1}{1+exp(x/b)}$$ But it does't match exactly. The form of numerical solution and 2 regions are clear in the following link: http://www.wolframalpha.com/input/?i=1%2F%281%2Bexp%28x%29%29+from+x%3D-10+to+x%3D10",,"['integration', 'ordinary-differential-equations', 'derivatives', 'partial-differential-equations', 'partial-derivative']"
16,Asymptotic expansion on 3 nonlinear ordinary differential equations,Asymptotic expansion on 3 nonlinear ordinary differential equations,,"The 3 nonlinear differential equations are as follows \begin{equation} \epsilon \frac{dc}{dt}=\alpha I   +  \ c (-K_F - K_D-K_N s-K_P(1-q)), \nonumber \end{equation} \begin{equation} \frac{ds}{dt}= \lambda_b P_C \ \epsilon \ c   (1-s)- \lambda_r (1-q)  \ s,  \nonumber \end{equation} \begin{equation}  \frac{dq}{dt}= K_P (1-q) \frac{P_C}{P_Q} \  \ c - \gamma  \ q,  \nonumber \end{equation} I want to use asymptotic expansion on $c, s$ and $q$. And values of parameters are: $K_F = 6.7 \times 10^{-2},$ $K_N = 6.03 \times 10^{-1}$ $K_P =  2.92 \times 10^{-2}$, $K_D = 4.94 \times 10^{-2}$, $\lambda_b= 0.0087$, $I=1200$ $P_C  =  3 \times 10^{11}$ $P_Q  = 2.304 \times 10^{9}$ $\gamma=2.74 $ $\lambda_{b}=0.0087 $ $\lambda_{r}= 835$ $\alpha=1.14437 \times 10^{-3}$ For initial conditions: \begin{equation} c_0(0)= c(0) = 0.25 \nonumber \end{equation} \begin{equation} s_0(0)= cs(0) = 0.02 \nonumber \nonumber \end{equation} \begin{equation} q_0(0)=q(0) = 0.98 \nonumber \nonumber \end{equation}  and \begin{equation} c_i(0)= 0,    \ i>0\nonumber \end{equation} \begin{equation} s_i(0)= 0, \ i>0 \nonumber \nonumber \end{equation} \begin{equation} q_i(0)=0, i>0. \nonumber \nonumber \end{equation} => i started with the expansions : \begin{equation} c= c_0+ \epsilon c_1 + \epsilon^2 c_2+......... \nonumber \end{equation} \begin{equation} s= s_0+ \epsilon s_1 + \epsilon^2 s_2+......... \nonumber \end{equation} \begin{equation} q= q_0+ \epsilon q_1 + \epsilon^2 q_2+......... \nonumber \end{equation} we are only interseted in up to fisrt power of $\epsilon$.  so, we should get total 6 approximate differential equations to get answer for $\dfrac{dc_0}{dt}, \dfrac{ds_0}{dt}, \dfrac{dq_0}{dt}, \dfrac{dc_1}{dt}, \dfrac{ds_1}{dt}$ and $\dfrac{dq_1}{dt}$ but i think $\dfrac{dc_1}{dt}$ will disappear while expanding and equating the up to first power of $\epsilon$, do i need to go further up to $\epsilon{^2}$ because $\dfrac{dc_1}{dt}$ is very important to find and we need 6 approximate differetial equations in total. what can i do? please some one help  me.","The 3 nonlinear differential equations are as follows \begin{equation} \epsilon \frac{dc}{dt}=\alpha I   +  \ c (-K_F - K_D-K_N s-K_P(1-q)), \nonumber \end{equation} \begin{equation} \frac{ds}{dt}= \lambda_b P_C \ \epsilon \ c   (1-s)- \lambda_r (1-q)  \ s,  \nonumber \end{equation} \begin{equation}  \frac{dq}{dt}= K_P (1-q) \frac{P_C}{P_Q} \  \ c - \gamma  \ q,  \nonumber \end{equation} I want to use asymptotic expansion on $c, s$ and $q$. And values of parameters are: $K_F = 6.7 \times 10^{-2},$ $K_N = 6.03 \times 10^{-1}$ $K_P =  2.92 \times 10^{-2}$, $K_D = 4.94 \times 10^{-2}$, $\lambda_b= 0.0087$, $I=1200$ $P_C  =  3 \times 10^{11}$ $P_Q  = 2.304 \times 10^{9}$ $\gamma=2.74 $ $\lambda_{b}=0.0087 $ $\lambda_{r}= 835$ $\alpha=1.14437 \times 10^{-3}$ For initial conditions: \begin{equation} c_0(0)= c(0) = 0.25 \nonumber \end{equation} \begin{equation} s_0(0)= cs(0) = 0.02 \nonumber \nonumber \end{equation} \begin{equation} q_0(0)=q(0) = 0.98 \nonumber \nonumber \end{equation}  and \begin{equation} c_i(0)= 0,    \ i>0\nonumber \end{equation} \begin{equation} s_i(0)= 0, \ i>0 \nonumber \nonumber \end{equation} \begin{equation} q_i(0)=0, i>0. \nonumber \nonumber \end{equation} => i started with the expansions : \begin{equation} c= c_0+ \epsilon c_1 + \epsilon^2 c_2+......... \nonumber \end{equation} \begin{equation} s= s_0+ \epsilon s_1 + \epsilon^2 s_2+......... \nonumber \end{equation} \begin{equation} q= q_0+ \epsilon q_1 + \epsilon^2 q_2+......... \nonumber \end{equation} we are only interseted in up to fisrt power of $\epsilon$.  so, we should get total 6 approximate differential equations to get answer for $\dfrac{dc_0}{dt}, \dfrac{ds_0}{dt}, \dfrac{dq_0}{dt}, \dfrac{dc_1}{dt}, \dfrac{ds_1}{dt}$ and $\dfrac{dq_1}{dt}$ but i think $\dfrac{dc_1}{dt}$ will disappear while expanding and equating the up to first power of $\epsilon$, do i need to go further up to $\epsilon{^2}$ because $\dfrac{dc_1}{dt}$ is very important to find and we need 6 approximate differetial equations in total. what can i do? please some one help  me.",,"['ordinary-differential-equations', 'asymptotics', 'taylor-expansion', 'systems-of-equations', 'perturbation-theory']"
17,Solving weak 2 body problem,Solving weak 2 body problem,,I tried to solve a physics problem about two body problem where the masses $M$ and $m$ are $M \gg m$. The body $m$ is at radius $R$ from the mass $M$ and is falling down with initial speed $v(0) = 0$. I need to calculate the time of falling. The point is not how can I solve it the problem (I have found a better way). I tried to find the solution it by solving the differential equation  $$\ddot x(t) = \frac{k}{x(t)^2}$$ but had no success. The way I tried is the following: $$ \dot x \ddot x = \frac{k}{x^2} \dot x \\ \frac{1}{2} D[(\dot x)^2] = k \frac{\dot x}{x^2} \\ \int \frac{1}{2} D[(\dot x)^2]\ dt = k \int \frac{\dot x}{x^2}dt\\ \frac{1}{2}(\dot x)^2 = -k \frac{1}{x} + C \\ \dot x = \sqrt{-2k\frac{1}{x} + C} $$ which doesn't seem correct to me. I want to solve it using clever tecniques (by clever I mean multiply or add a nice terms and make calculations almost instant). Could you suggest me something?,I tried to solve a physics problem about two body problem where the masses $M$ and $m$ are $M \gg m$. The body $m$ is at radius $R$ from the mass $M$ and is falling down with initial speed $v(0) = 0$. I need to calculate the time of falling. The point is not how can I solve it the problem (I have found a better way). I tried to find the solution it by solving the differential equation  $$\ddot x(t) = \frac{k}{x(t)^2}$$ but had no success. The way I tried is the following: $$ \dot x \ddot x = \frac{k}{x^2} \dot x \\ \frac{1}{2} D[(\dot x)^2] = k \frac{\dot x}{x^2} \\ \int \frac{1}{2} D[(\dot x)^2]\ dt = k \int \frac{\dot x}{x^2}dt\\ \frac{1}{2}(\dot x)^2 = -k \frac{1}{x} + C \\ \dot x = \sqrt{-2k\frac{1}{x} + C} $$ which doesn't seem correct to me. I want to solve it using clever tecniques (by clever I mean multiply or add a nice terms and make calculations almost instant). Could you suggest me something?,,"['ordinary-differential-equations', 'physics']"
18,What does a number in gradient symbol subscript means?,What does a number in gradient symbol subscript means?,,"While solving some problems I have encountered a subscript in front of a gradient symbol. I'm unable to understand it, I know a superscript of 2 on gradient symbol means Laplacian but what does subscript means ? Can anyone explain ? What does that 3 means ? It is multiplied three times ?","While solving some problems I have encountered a subscript in front of a gradient symbol. I'm unable to understand it, I know a superscript of 2 on gradient symbol means Laplacian but what does subscript means ? Can anyone explain ? What does that 3 means ? It is multiplied three times ?",,"['ordinary-differential-equations', 'functions', 'vector-spaces', 'partial-derivative', 'gradient-flows']"
19,Show solvability of ODE without explicitly calculating solution,Show solvability of ODE without explicitly calculating solution,,Show that  $$  u + u^{(4)} - u^{(2)} = f $$ has a solution $u \in H^4(\mathbb R)$ (without explicitly calculuting it) for every $f \in L^2(\mathbb R)$! What criteria for solvability for such ODE's exist?,Show that  $$  u + u^{(4)} - u^{(2)} = f $$ has a solution $u \in H^4(\mathbb R)$ (without explicitly calculuting it) for every $f \in L^2(\mathbb R)$! What criteria for solvability for such ODE's exist?,,"['analysis', 'functional-analysis', 'ordinary-differential-equations']"
20,Particular solution of an ODE,Particular solution of an ODE,,"How do I show that a particular solution $y_{1}$ of the ODE $$y''-k^{2}y=R(x)$$ $k\neq0$, is given by $$y_{1}=\frac{1}{k}\int_{0}^{x}R(t)\sinh(k(x-t))dt$$ I really have no idea how to do it.","How do I show that a particular solution $y_{1}$ of the ODE $$y''-k^{2}y=R(x)$$ $k\neq0$, is given by $$y_{1}=\frac{1}{k}\int_{0}^{x}R(t)\sinh(k(x-t))dt$$ I really have no idea how to do it.",,['ordinary-differential-equations']
21,What is the inverse function of $\int{ \frac{1}{{\sqrt{x+1}}{x^n}} dx}$?,What is the inverse function of ?,\int{ \frac{1}{{\sqrt{x+1}}{x^n}} dx},"I am trying to solve $$ \frac{dy}{dt} = \alpha ((y+1)^2 - \gamma)^n \hspace{2cm} y(0)=0 $$ Here $y$ is a real-valued, monotonically increasing, positive definite function of $t$ in the interval $(0,\infty)$ and is bounded on every closed interval in $(0,\infty)$; $\alpha$ and $\gamma$ are constants. After a few manipulations, one arrives at this expression: $$ \int_{x_0}^{x(t)}{\frac{1}{{x^n}\sqrt{x+1}} dx} = \beta t $$ for some $\beta$. For integer $n>1$ the integral can also be performed: $$ 2^{1-2n} {2(n-1) \choose n-1} \left( \mathrm{tanh^{-1}}(\sqrt{x+1})-\mathrm{tanh}^{-1}(\sqrt{x_0+1}) \right)  +  \sqrt{x+1}\sum_{k=1}^{n-1}{(-1)^{-k}b_{n,k}\frac{1}{k x^k}} = (-1)^{n}\beta t $$ The coefficients $b_{n,k}$ in the above sum can be represented by $$ b_{n,k}=\frac{{2(n-1) \choose 2k}{2(n-1-k) \choose n-1-k}}{2^{2(n-1-k)}{n-1 \choose k}^2} = \frac{\Gamma (n - \frac12) \Gamma (k+1)}{\Gamma (n) \Gamma (k + \frac12)} $$ I am not certain how to extend this result to non-integer values of $n$ without the use of special functions, e.g. the Beta function, which is suggested by the integral. In any case, the next step in the calculation would be to take the inverse of the LHS - but I have no idea how to do that. However, the form of the sum seems very suggestive; perhaps at the limit of large $n$, $\sqrt{x+1}\sum_{k=1}^{n-1}{(-1)^{-k}b_{n,k}k^{-1}x^{-k}}$ may be approximated by a constant and for low $n$ by some other function, which will enable further calculations. The following also seems to hold: $$ \sqrt{x+1} \sum^{n-1}_{k=1}{(-1)^{-k}\frac{\Gamma(n-\frac12)(k-1)!}{\Gamma(k+\frac12)}x^{-k}} =  (-1)^n \frac{2\sqrt{x+1}}{(2n-1)x^{n}}  {_{2}F_{1} \left( 1,n;n+\frac12;-\frac1x \right)} -\frac{2 \Gamma(n-\frac12)}{\sqrt{\pi} \Gamma(n)} \mathrm{sinh}^{-1} \left( \frac{1}{\sqrt{x}} \right) $$ although I am uncertain to what degree that may lead to an invertible expression for the integral without the use of more ""exotic"" special functions; after all the hypergeometric function in this expression appears to be the dominant term. I found that the late-time behaviour may be approximated by the $\int{dx \frac{1}{x^n}}$; however such an approximation misses the early-time behaviour and does not approach the correct asymptote by simply applying the original initial condition. It would still be very helpful if there exist special exact or approximate formulae for the inverse of the incomplete Beta function, e.g. for special parameter values (the case $n=1$ is already derived here).","I am trying to solve $$ \frac{dy}{dt} = \alpha ((y+1)^2 - \gamma)^n \hspace{2cm} y(0)=0 $$ Here $y$ is a real-valued, monotonically increasing, positive definite function of $t$ in the interval $(0,\infty)$ and is bounded on every closed interval in $(0,\infty)$; $\alpha$ and $\gamma$ are constants. After a few manipulations, one arrives at this expression: $$ \int_{x_0}^{x(t)}{\frac{1}{{x^n}\sqrt{x+1}} dx} = \beta t $$ for some $\beta$. For integer $n>1$ the integral can also be performed: $$ 2^{1-2n} {2(n-1) \choose n-1} \left( \mathrm{tanh^{-1}}(\sqrt{x+1})-\mathrm{tanh}^{-1}(\sqrt{x_0+1}) \right)  +  \sqrt{x+1}\sum_{k=1}^{n-1}{(-1)^{-k}b_{n,k}\frac{1}{k x^k}} = (-1)^{n}\beta t $$ The coefficients $b_{n,k}$ in the above sum can be represented by $$ b_{n,k}=\frac{{2(n-1) \choose 2k}{2(n-1-k) \choose n-1-k}}{2^{2(n-1-k)}{n-1 \choose k}^2} = \frac{\Gamma (n - \frac12) \Gamma (k+1)}{\Gamma (n) \Gamma (k + \frac12)} $$ I am not certain how to extend this result to non-integer values of $n$ without the use of special functions, e.g. the Beta function, which is suggested by the integral. In any case, the next step in the calculation would be to take the inverse of the LHS - but I have no idea how to do that. However, the form of the sum seems very suggestive; perhaps at the limit of large $n$, $\sqrt{x+1}\sum_{k=1}^{n-1}{(-1)^{-k}b_{n,k}k^{-1}x^{-k}}$ may be approximated by a constant and for low $n$ by some other function, which will enable further calculations. The following also seems to hold: $$ \sqrt{x+1} \sum^{n-1}_{k=1}{(-1)^{-k}\frac{\Gamma(n-\frac12)(k-1)!}{\Gamma(k+\frac12)}x^{-k}} =  (-1)^n \frac{2\sqrt{x+1}}{(2n-1)x^{n}}  {_{2}F_{1} \left( 1,n;n+\frac12;-\frac1x \right)} -\frac{2 \Gamma(n-\frac12)}{\sqrt{\pi} \Gamma(n)} \mathrm{sinh}^{-1} \left( \frac{1}{\sqrt{x}} \right) $$ although I am uncertain to what degree that may lead to an invertible expression for the integral without the use of more ""exotic"" special functions; after all the hypergeometric function in this expression appears to be the dominant term. I found that the late-time behaviour may be approximated by the $\int{dx \frac{1}{x^n}}$; however such an approximation misses the early-time behaviour and does not approach the correct asymptote by simply applying the original initial condition. It would still be very helpful if there exist special exact or approximate formulae for the inverse of the incomplete Beta function, e.g. for special parameter values (the case $n=1$ is already derived here).",,"['calculus', 'ordinary-differential-equations', 'summation', 'special-functions', 'inverse']"
22,Help needed in solving a differential equation,Help needed in solving a differential equation,,"Please help me in solving: $$a^2z+\frac{\partial^2z}{\partial x^2}-\frac{\partial^2 z}{\partial y^2}=0$$  ($a$ is a constant) I plugged this in Wolfram Alpha and it outputs that this is a second order linear partial differential equation but unfortunately, I have never solved these type of equations before. I came across this equation while solving some other problem. I tried to look up the methods to solve such equations but they seem too advanced to me. :( Can somebody please please solve this? I am not even sure if it is possible to solve this equation. Any help is appreciated. Thanks!","Please help me in solving: $$a^2z+\frac{\partial^2z}{\partial x^2}-\frac{\partial^2 z}{\partial y^2}=0$$  ($a$ is a constant) I plugged this in Wolfram Alpha and it outputs that this is a second order linear partial differential equation but unfortunately, I have never solved these type of equations before. I came across this equation while solving some other problem. I tried to look up the methods to solve such equations but they seem too advanced to me. :( Can somebody please please solve this? I am not even sure if it is possible to solve this equation. Any help is appreciated. Thanks!",,"['calculus', 'integration', 'ordinary-differential-equations', 'partial-differential-equations']"
23,Diferential equation solution satisfying $y(0) = \pi$,Diferential equation solution satisfying,y(0) = \pi,"I have the following question from a past exam: Show that the differential equation $\frac{dy}{dx} = \cfrac{e^x + x}{\sin y + 2}$ has a solution satisfying $y(0) = \pi$ What I have done: $$\int (\sin y + 2) \; \mathrm{d}y = \int (e^x + x) \; \mathrm{d}x$$ $$-\cos y + 2y = e^x + \frac{x^2}2 + C$$ Sub in $y = \pi, x = 0$ $$-\cos {\pi} + 2\pi = e^0 + 0 + C$$ $$1+2\pi - 1 = C, C = 2\pi$$ $$-\cos y + 2y = e^x + \frac{x^2}2 + 2\pi$$ Now I am not sure what this question actually wants me to do. Any ideas how I can 'show that the differential equation has a solution satisfying$\dots$'? Note: I am apparently meant to solve it implicitly using an 'appropriate' theorem. Perhaps there is a real-analysis way of solving this?(This was for an Analysis past exam, and I seem to have solved it using only calculus)","I have the following question from a past exam: Show that the differential equation $\frac{dy}{dx} = \cfrac{e^x + x}{\sin y + 2}$ has a solution satisfying $y(0) = \pi$ What I have done: $$\int (\sin y + 2) \; \mathrm{d}y = \int (e^x + x) \; \mathrm{d}x$$ $$-\cos y + 2y = e^x + \frac{x^2}2 + C$$ Sub in $y = \pi, x = 0$ $$-\cos {\pi} + 2\pi = e^0 + 0 + C$$ $$1+2\pi - 1 = C, C = 2\pi$$ $$-\cos y + 2y = e^x + \frac{x^2}2 + 2\pi$$ Now I am not sure what this question actually wants me to do. Any ideas how I can 'show that the differential equation has a solution satisfying$\dots$'? Note: I am apparently meant to solve it implicitly using an 'appropriate' theorem. Perhaps there is a real-analysis way of solving this?(This was for an Analysis past exam, and I seem to have solved it using only calculus)",,"['calculus', 'ordinary-differential-equations']"
24,"Prove that $\lim \limits_{n \to \infty} \frac{x^n}{n!} = 0$, $x \in \Bbb R$.","Prove that , .",\lim \limits_{n \to \infty} \frac{x^n}{n!} = 0 x \in \Bbb R,"Why is $$\lim_{n \to \infty} \frac{2^n}{n!}=0\text{ ?}$$ Can we generalize it to any exponent $x \in \Bbb R$? This is to say, is $$\lim_{n \to \infty} \frac{x^n}{n!}=0\text{ ?}$$ This is being repurposed in an effort to cut down on duplicates, see here: Coping with abstract duplicate questions. and here: List of abstract duplicates .","Why is $$\lim_{n \to \infty} \frac{2^n}{n!}=0\text{ ?}$$ Can we generalize it to any exponent $x \in \Bbb R$? This is to say, is $$\lim_{n \to \infty} \frac{x^n}{n!}=0\text{ ?}$$ This is being repurposed in an effort to cut down on duplicates, see here: Coping with abstract duplicate questions. and here: List of abstract duplicates .",,"['limits', 'factorial', 'faq']"
25,Stirling's formula: proof?,Stirling's formula: proof?,,"Suppose we want to show that $$ n! \sim \sqrt{2 \pi} n^{n+(1/2)}e^{-n}$$ Instead we could show that $$\lim_{n \to \infty} \frac{n!}{n^{n+(1/2)}e^{-n}} = C$$ where $C$ is a constant. Maybe $C = \sqrt{2 \pi}$ . What is a good way of doing this? Could we use L'Hopital's Rule ? Or maybe take the log of both sides (e.g., compute the limit of the log of the quantity)? So for example do the following $$\lim_{n \to \infty} \log \left[\frac{n!}{n^{n+(1/2)}e^{-n}} \right] =  \log C$$","Suppose we want to show that Instead we could show that where is a constant. Maybe . What is a good way of doing this? Could we use L'Hopital's Rule ? Or maybe take the log of both sides (e.g., compute the limit of the log of the quantity)? So for example do the following", n! \sim \sqrt{2 \pi} n^{n+(1/2)}e^{-n} \lim_{n \to \infty} \frac{n!}{n^{n+(1/2)}e^{-n}} = C C C = \sqrt{2 \pi} \lim_{n \to \infty} \log \left[\frac{n!}{n^{n+(1/2)}e^{-n}} \right] =  \log C,"['limits', 'asymptotics', 'factorial', 'gamma-function', 'big-list']"
26,Closed form solution for $\sum_{n=1}^\infty\frac{1}{1+\frac{n^2}{1+\frac{1}{\stackrel{\ddots}{1+\frac{1}{1+n^2}}}}}$.,Closed form solution for .,\sum_{n=1}^\infty\frac{1}{1+\frac{n^2}{1+\frac{1}{\stackrel{\ddots}{1+\frac{1}{1+n^2}}}}},"Let $$ \text{S}_k = \sum_{n=1}^\infty\cfrac{1}{1+\cfrac{n^2}{1+\cfrac{1}{\ddots1+\cfrac{1}{1+n^2}}}},\quad\text{$k$ rows in the continued fraction} $$ So for example, the terms of the sum $\text{S}_6$ are $$ \cfrac{1}{1+\cfrac{n^2}{1+\cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+n^2}}}}}} $$ Using a symbolic computation software (Mathematica), I got the following interesting results: $$ \begin{align} \text{S}_4 &= \frac{\pi}{4}\left(\coth(\pi)+\sqrt{3}\coth(\sqrt{3}\pi)\right)-\frac{1}{2}\\ \text{S}_6 &= \frac{\pi}{4}\left(\sqrt{2}\coth\left(\sqrt{2}\pi\right)+\sqrt{\frac{4}{3}}\coth\left(\sqrt{\frac{4}{3}}\pi\right)\right)-\frac{1}{2}\\ \text{S}_8 &= \frac{\pi}{4}\left(\sqrt{\frac{3}{2}}\coth\left(\sqrt{\frac{3}{2}}\pi\right)+\sqrt{\frac{7}{4}}\coth\left(\sqrt{\frac{7}{4}}\pi\right)\right)-\frac{1}{2}\\ \text{S}_{10} &= \frac{\pi}{4}\left(\sqrt{\frac{5}{3}}\coth\left(\sqrt{\frac{5}{3}}\pi\right)+\sqrt{\frac{11}{7}}\coth\left(\sqrt{\frac{11}{7}}\pi\right)\right)-\frac{1}{2}\\ \text{S}_{12} &= \frac{\pi}{4}\left(\sqrt{\frac{8}{5}}\coth\left(\sqrt{\frac{8}{5}}\pi\right)+\sqrt{\frac{18}{11}}\coth\left(\sqrt{\frac{11}{7}}\pi\right)\right)-\frac{1}{2}\\ \text{S}_{14} &= \frac{\pi}{4}\left(\sqrt{\frac{13}{8}}\coth\left(\sqrt{\frac{13}{8}}\pi\right)+\sqrt{\frac{29}{18}}\coth\left(\sqrt{\frac{29}{18}}\pi\right)\right)-\frac{1}{2}.\\ \end{align} $$ The numbers appearing at the first $\coth$ term are easy to guess: they are the famous Fibonacci numbers . The numbers at the second $\coth$ term can also be guessed: they appear to be the Lucas numbers . Those are constructed like the Fibonacci numbers but starting with $2,1$ instead of $0,1$. Hence: Conjecture: $$\text{S}_{2k}=\frac{\pi}{4}\left(\sqrt{\frac{F_k}{F_{k-1}}}\coth\left(\sqrt{\frac{F_k}{F_{k-1}}}\pi\right)+\sqrt{\frac{L_k}{L_{k-1}}}\coth\left(\sqrt{\frac{L_k}{L_{k-1}}}\pi\right)\right)-\frac{1}{2}$$ I have verified this conjecture for many $k$'s and it always work out perfectly. To me this is quite amazing, but I am not able to verify the conjecture. Can anyone prove it? Moreover, if true the conjecture implies that $$\lim_{k\to\infty}\text{S}_{2k}=\frac{\sqrt{\varphi}\pi\coth\left(\sqrt{\varphi}\pi\right)-1}{2}$$ which is also very nice ($\pi$ and $\varphi$ don't meet very often).","Let $$ \text{S}_k = \sum_{n=1}^\infty\cfrac{1}{1+\cfrac{n^2}{1+\cfrac{1}{\ddots1+\cfrac{1}{1+n^2}}}},\quad\text{$k$ rows in the continued fraction} $$ So for example, the terms of the sum $\text{S}_6$ are $$ \cfrac{1}{1+\cfrac{n^2}{1+\cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+\cfrac{1}{1+n^2}}}}}} $$ Using a symbolic computation software (Mathematica), I got the following interesting results: $$ \begin{align} \text{S}_4 &= \frac{\pi}{4}\left(\coth(\pi)+\sqrt{3}\coth(\sqrt{3}\pi)\right)-\frac{1}{2}\\ \text{S}_6 &= \frac{\pi}{4}\left(\sqrt{2}\coth\left(\sqrt{2}\pi\right)+\sqrt{\frac{4}{3}}\coth\left(\sqrt{\frac{4}{3}}\pi\right)\right)-\frac{1}{2}\\ \text{S}_8 &= \frac{\pi}{4}\left(\sqrt{\frac{3}{2}}\coth\left(\sqrt{\frac{3}{2}}\pi\right)+\sqrt{\frac{7}{4}}\coth\left(\sqrt{\frac{7}{4}}\pi\right)\right)-\frac{1}{2}\\ \text{S}_{10} &= \frac{\pi}{4}\left(\sqrt{\frac{5}{3}}\coth\left(\sqrt{\frac{5}{3}}\pi\right)+\sqrt{\frac{11}{7}}\coth\left(\sqrt{\frac{11}{7}}\pi\right)\right)-\frac{1}{2}\\ \text{S}_{12} &= \frac{\pi}{4}\left(\sqrt{\frac{8}{5}}\coth\left(\sqrt{\frac{8}{5}}\pi\right)+\sqrt{\frac{18}{11}}\coth\left(\sqrt{\frac{11}{7}}\pi\right)\right)-\frac{1}{2}\\ \text{S}_{14} &= \frac{\pi}{4}\left(\sqrt{\frac{13}{8}}\coth\left(\sqrt{\frac{13}{8}}\pi\right)+\sqrt{\frac{29}{18}}\coth\left(\sqrt{\frac{29}{18}}\pi\right)\right)-\frac{1}{2}.\\ \end{align} $$ The numbers appearing at the first $\coth$ term are easy to guess: they are the famous Fibonacci numbers . The numbers at the second $\coth$ term can also be guessed: they appear to be the Lucas numbers . Those are constructed like the Fibonacci numbers but starting with $2,1$ instead of $0,1$. Hence: Conjecture: $$\text{S}_{2k}=\frac{\pi}{4}\left(\sqrt{\frac{F_k}{F_{k-1}}}\coth\left(\sqrt{\frac{F_k}{F_{k-1}}}\pi\right)+\sqrt{\frac{L_k}{L_{k-1}}}\coth\left(\sqrt{\frac{L_k}{L_{k-1}}}\pi\right)\right)-\frac{1}{2}$$ I have verified this conjecture for many $k$'s and it always work out perfectly. To me this is quite amazing, but I am not able to verify the conjecture. Can anyone prove it? Moreover, if true the conjecture implies that $$\lim_{k\to\infty}\text{S}_{2k}=\frac{\sqrt{\varphi}\pi\coth\left(\sqrt{\varphi}\pi\right)-1}{2}$$ which is also very nice ($\pi$ and $\varphi$ don't meet very often).",,"['limits', 'fibonacci-numbers', 'continued-fractions', 'experimental-mathematics', 'lucas-numbers']"
27,Does the average primeness of natural numbers tend to zero?,Does the average primeness of natural numbers tend to zero?,,"Note 1 : This questions requires some new definitions, namely ""continuous primeness"" which I have made. Everyone is welcome to improve the definition  without altering the spirit of the question. Click here for a somewhat related question . A number is either prime or composite, hence primality is a binary concept. Instead I wanted to put a value of primality to every number using some function $f$ such that $f(n) = 1$ iff $n$ is a prime otherwise, $0 < f(n) < 1$ and as the number divisors of $n$ increases, $f(n)$ decreases on average. Thus $f(n)$ is a measure of the degree of primeness of $n$ where 1 is a perfect prime and 0 is a hypothetical perfect composite. Hence $\frac{1}{N}\sum_{r \le N} f(r)$ can be interpreted as a measure of average primeness of the first $N$ integers. After trying several definitions and going through the ones in literature , I came up with: Define $f(n) = \dfrac{2s_n}{n-1}$ for $n \ge 2$ , where $s_n$ is the   standard deviation of the divisors of $n$ . One advantage of using standard deviation is that even if two numbers have the same number of divisor their value of $f$ appears to be different hence their measure of primeness will be different. Question 1: Does the average primeness tend to zero? i.e. does the following hold? $$ \lim_{N \to \infty} \frac{1}{N}\sum_{r = 2}^N f(r) = 0 $$ Question 2 : Is $f(n)$ injective over composites? i.e., do there exist composites $3 < m < n$ such that $f(m) = f(n)$ ? My progress $f(4.35\times 10^8) \approx 0.5919$ and decreasing so the limit if it exists must be between 0 and 0.5919. For $2 \le i \le n$ , the minimum value of $f(i)$ occurs at the largest highly composite number $\le n$ . Note 2 : Here standard deviation of $x_1, x_2, \ldots , x_n$ is defined as $\sqrt \frac{\sum_{i=1}^{n} (x-x_i)^2}{n}$ . Also notice that even if we define standard deviation as $\sqrt \frac{\sum_{i=1}^{n} (x-x_i)^2}{n-1}$ our questions remain unaffected because in this case in the definition of $f$ , we will be multiplying with $\sqrt 2$ instead of $2$ to normalize $f$ in the interval $(0,1)$ . Note 3 : Posted this question in MO and got answer for question 1. Indeed the limit tends to zero. Question 2 is still open.","Note 1 : This questions requires some new definitions, namely ""continuous primeness"" which I have made. Everyone is welcome to improve the definition  without altering the spirit of the question. Click here for a somewhat related question . A number is either prime or composite, hence primality is a binary concept. Instead I wanted to put a value of primality to every number using some function such that iff is a prime otherwise, and as the number divisors of increases, decreases on average. Thus is a measure of the degree of primeness of where 1 is a perfect prime and 0 is a hypothetical perfect composite. Hence can be interpreted as a measure of average primeness of the first integers. After trying several definitions and going through the ones in literature , I came up with: Define for , where is the   standard deviation of the divisors of . One advantage of using standard deviation is that even if two numbers have the same number of divisor their value of appears to be different hence their measure of primeness will be different. Question 1: Does the average primeness tend to zero? i.e. does the following hold? Question 2 : Is injective over composites? i.e., do there exist composites such that ? My progress and decreasing so the limit if it exists must be between 0 and 0.5919. For , the minimum value of occurs at the largest highly composite number . Note 2 : Here standard deviation of is defined as . Also notice that even if we define standard deviation as our questions remain unaffected because in this case in the definition of , we will be multiplying with instead of to normalize in the interval . Note 3 : Posted this question in MO and got answer for question 1. Indeed the limit tends to zero. Question 2 is still open.","f f(n) = 1 n 0 < f(n) < 1 n f(n) f(n) n \frac{1}{N}\sum_{r \le N} f(r) N f(n) = \dfrac{2s_n}{n-1} n \ge 2 s_n n f 
\lim_{N \to \infty} \frac{1}{N}\sum_{r = 2}^N f(r) = 0
 f(n) 3 < m < n f(m) = f(n) f(4.35\times 10^8) \approx 0.5919 2 \le i \le n f(i) \le n x_1, x_2, \ldots , x_n \sqrt \frac{\sum_{i=1}^{n} (x-x_i)^2}{n} \sqrt \frac{\sum_{i=1}^{n} (x-x_i)^2}{n-1} f \sqrt 2 2 f (0,1)","['number-theory', 'limits', 'statistics', 'prime-numbers', 'natural-numbers']"
28,About $\lim \left(1+\frac {x}{n}\right)^n$,About,\lim \left(1+\frac {x}{n}\right)^n,I was wondering if it is possible to get a link to a rigorous proof that  $$\displaystyle \lim_{n\to\infty} \left(1+\frac {x}{n}\right)^n=\exp x$$,I was wondering if it is possible to get a link to a rigorous proof that  $$\displaystyle \lim_{n\to\infty} \left(1+\frac {x}{n}\right)^n=\exp x$$,,"['limits', 'exponential-function', 'faq']"
29,Find the value of $\sqrt{10\sqrt{10\sqrt{10...}}}$,Find the value of,\sqrt{10\sqrt{10\sqrt{10...}}},"I found a question that asked to find the limiting value of $$10\sqrt{10\sqrt{10\sqrt{10\sqrt{10\sqrt{...}}}}}$$If you make the substitution $x=10\sqrt{10\sqrt{10\sqrt{10\sqrt{10\sqrt{...}}}}}$ it simplifies to $x=10\sqrt{x}$ which has solutions $x=0,100$. I don't understand how $x=0$ is a possible solution, I know that squaring equations can introduce new, invalid solutions to equations and so you should check the solutions in the original (unsquared) equation, but doing that here doesn't lead to any non-real solutions or contradictions. I was wondering if anyone knows how $x=0$ turns out as a valid solution, is there an algebaric or geometric interpretation? Or is it just a ""special case"" equation? A similar question says to find the limiting value of $\sqrt{6+5\sqrt{6+5\sqrt{6+5\sqrt{...}}}}$, and making a similar substituion for $x$ leads to $$x=\sqrt{6+5x}$$ $$x^2=6+5x$$ which has solutions $x=-1,6$. In this case though, you could substitute $x=-1$ into the first equation, leading to the contradiction $-1=1$ so you could satisfactorily disclude it. Is there any similar reasoning for the first question? I know this might be a stupid question but I'm genuinely curious :)","I found a question that asked to find the limiting value of $$10\sqrt{10\sqrt{10\sqrt{10\sqrt{10\sqrt{...}}}}}$$If you make the substitution $x=10\sqrt{10\sqrt{10\sqrt{10\sqrt{10\sqrt{...}}}}}$ it simplifies to $x=10\sqrt{x}$ which has solutions $x=0,100$. I don't understand how $x=0$ is a possible solution, I know that squaring equations can introduce new, invalid solutions to equations and so you should check the solutions in the original (unsquared) equation, but doing that here doesn't lead to any non-real solutions or contradictions. I was wondering if anyone knows how $x=0$ turns out as a valid solution, is there an algebaric or geometric interpretation? Or is it just a ""special case"" equation? A similar question says to find the limiting value of $\sqrt{6+5\sqrt{6+5\sqrt{6+5\sqrt{...}}}}$, and making a similar substituion for $x$ leads to $$x=\sqrt{6+5x}$$ $$x^2=6+5x$$ which has solutions $x=-1,6$. In this case though, you could substitute $x=-1$ into the first equation, leading to the contradiction $-1=1$ so you could satisfactorily disclude it. Is there any similar reasoning for the first question? I know this might be a stupid question but I'm genuinely curious :)",,"['limits', 'proof-verification', 'radicals', 'infinite-product', 'nested-radicals']"
30,Different sizes of infinity,Different sizes of infinity,,"Correct me if I'm wrong, but this is what they taught us in precal: $$\lim_{x\rightarrow\infty}x=\infty$$ $$\lim_{x\rightarrow\infty}x^{2}=\infty$$ But, we also know that $n^{2}>n$ if $n\notin [0,1]$ Does that mean that some infinities are greater than others? Why don't we explicitly define infinity so that we can show differences in sizes? Thank you in advance!","Correct me if I'm wrong, but this is what they taught us in precal: $$\lim_{x\rightarrow\infty}x=\infty$$ $$\lim_{x\rightarrow\infty}x^{2}=\infty$$ But, we also know that $n^{2}>n$ if $n\notin [0,1]$ Does that mean that some infinities are greater than others? Why don't we explicitly define infinity so that we can show differences in sizes? Thank you in advance!",,"['limits', 'infinity']"
31,Calculation with infinitely many operands,Calculation with infinitely many operands,,"Recently I have come across the equation $$x^{x^{x^{x^{...}}}} = 2$$ which I found out to be solvable using substitution: $$u = x^{x^{x^{x^{...}}}} = 2$$ $$x^u = 2$$ $$x^2 = 2$$ $$x = \sqrt{2}$$ Thinking about that, I tried to apply the same concept to other operations $$ x \cdot x \cdot x \cdot x\space \cdot \space ...\space= 2 $$ $$u = x \cdot x \cdot x \cdot x\space \cdot \space ...\space= 2$$ $$x \cdot u = 2$$ $$2x = 2$$ $$x = 1$$ This solution is rather strange, as $1 \cdot 1 \cdot 1 ...$ should of course always equal $1$, but the equation says something else. Same thing goes for the addition: $$x + x + x + x + \space ...\space= 2$$ $$u = x + x + x + \space...\space= 2$$ $$x + u = 2$$ $$x + 2 = 2$$ $$x = 0 $$ Which basically says that if you add $0$ up often enough you get $2$. And the next problem is, if you define the sum to equal $1$ instead of $2$ you end up with $1 = 2$. So obviously there must be a mistake here. Is there any reason why this approach can be used for infinite powers but not sums and products?","Recently I have come across the equation $$x^{x^{x^{x^{...}}}} = 2$$ which I found out to be solvable using substitution: $$u = x^{x^{x^{x^{...}}}} = 2$$ $$x^u = 2$$ $$x^2 = 2$$ $$x = \sqrt{2}$$ Thinking about that, I tried to apply the same concept to other operations $$ x \cdot x \cdot x \cdot x\space \cdot \space ...\space= 2 $$ $$u = x \cdot x \cdot x \cdot x\space \cdot \space ...\space= 2$$ $$x \cdot u = 2$$ $$2x = 2$$ $$x = 1$$ This solution is rather strange, as $1 \cdot 1 \cdot 1 ...$ should of course always equal $1$, but the equation says something else. Same thing goes for the addition: $$x + x + x + x + \space ...\space= 2$$ $$u = x + x + x + \space...\space= 2$$ $$x + u = 2$$ $$x + 2 = 2$$ $$x = 0 $$ Which basically says that if you add $0$ up often enough you get $2$. And the next problem is, if you define the sum to equal $1$ instead of $2$ you end up with $1 = 2$. So obviously there must be a mistake here. Is there any reason why this approach can be used for infinite powers but not sums and products?",,"['limits', 'infinity']"
32,Limit of $x \log x$ as $x$ tends to $0^+$,Limit of  as  tends to,x \log x x 0^+,"Why is the limit of $x \log x$ as $x$ tends to $0^+$, $0$? The limit of $x$ as $x$ tends to $0$ is $0$. The limit of $\log x$ as $x$ tends to $0^+$ is $-\infty$. The limit of products is the product of each limit, provided each limit exists. Therefore, the limit of $x \log x$ as $x$ tends to $0^+$ should be $0 \times (-\infty)$, which is undefined and not $0$.","Why is the limit of $x \log x$ as $x$ tends to $0^+$, $0$? The limit of $x$ as $x$ tends to $0$ is $0$. The limit of $\log x$ as $x$ tends to $0^+$ is $-\infty$. The limit of products is the product of each limit, provided each limit exists. Therefore, the limit of $x \log x$ as $x$ tends to $0^+$ should be $0 \times (-\infty)$, which is undefined and not $0$.",,['limits']
33,Limit of $(1+ x/n)^n$ when $n$ tends to infinity [duplicate],Limit of  when  tends to infinity [duplicate],(1+ x/n)^n n,This question already has answers here : About $\lim \left(1+\frac {x}{n}\right)^n$ (14 answers) Closed 9 years ago . Does anyone know the exact proof of this limit result? $$\lim_{n\to\infty} \left(1+\frac{x}{n}\right)^n = e^x$$,This question already has answers here : About $\lim \left(1+\frac {x}{n}\right)^n$ (14 answers) Closed 9 years ago . Does anyone know the exact proof of this limit result? $$\lim_{n\to\infty} \left(1+\frac{x}{n}\right)^n = e^x$$,,"['limits', 'proof-writing']"
34,Proof of 1 = 0 by Mathematical Induction on Limits?,Proof of 1 = 0 by Mathematical Induction on Limits?,,"I got stuck with a problem that pop up in my mind while learning limits. I am still a high school student. Define $P(m)$ to be the statement: $\quad \lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{m})=0$ The statement holds for $m = 1$: $\quad \lim\limits_{n\to\infty}\frac{1}{n}=0$. Assume that $P(k)$ holds for some $k$. So put $m = k$: $\quad \lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{k})=0$. We prove $P(k + 1)$: $\quad \lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{k+1}) =\lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{k}+\frac{1}{n})$ $=\lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{k}) +\lim\limits_{n\to\infty}\frac{1}{n}$ $=0+0=0$. It has now been proved by mathematical induction that statement holds for all natural m. If we let $m=n$, then $\lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{n})=0 \tag{*}$. However, $\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{n}=1 \implies \lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{n})=1 \tag{$\dagger$}$. Then $(*) \, \& \, (\dagger)$ yield $1=0$? Can anybody explain this? thanks.","I got stuck with a problem that pop up in my mind while learning limits. I am still a high school student. Define $P(m)$ to be the statement: $\quad \lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{m})=0$ The statement holds for $m = 1$: $\quad \lim\limits_{n\to\infty}\frac{1}{n}=0$. Assume that $P(k)$ holds for some $k$. So put $m = k$: $\quad \lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{k})=0$. We prove $P(k + 1)$: $\quad \lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{k+1}) =\lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{k}+\frac{1}{n})$ $=\lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{k}) +\lim\limits_{n\to\infty}\frac{1}{n}$ $=0+0=0$. It has now been proved by mathematical induction that statement holds for all natural m. If we let $m=n$, then $\lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{n})=0 \tag{*}$. However, $\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{n}=1 \implies \lim\limits_{n\to\infty}(\underbrace{\frac{1}{n}+\frac{1}{n}+\cdots+\frac{1}{n}}_{n})=1 \tag{$\dagger$}$. Then $(*) \, \& \, (\dagger)$ yield $1=0$? Can anybody explain this? thanks.",,"['limits', 'induction', 'fake-proofs']"
35,"Is ""taking a limit"" a function? Is it a procedure? A ternary operation?","Is ""taking a limit"" a function? Is it a procedure? A ternary operation?",,"I was sitting in analysis yesterday and, naturally, we took the limit of some expression. It occurred to me that ""taking the limit"" of some expression abides the rules of a linear transformation $$\lim_{x \rightarrow k}\ c(f(x)+g(x)) = c \lim_{x \rightarrow k} f(x) + c\ \lim_{x \rightarrow k} g(x),$$ and (my group theory is virtually non existent) appears also to be a homomorphism: $$\lim_{x \rightarrow k} (fg)(x) = \lim_{x \rightarrow k} f(x)g(x), $$ etc. Anyway, my real question is, what mathematical construct is the limit?","I was sitting in analysis yesterday and, naturally, we took the limit of some expression. It occurred to me that ""taking the limit"" of some expression abides the rules of a linear transformation and (my group theory is virtually non existent) appears also to be a homomorphism: etc. Anyway, my real question is, what mathematical construct is the limit?","\lim_{x \rightarrow k}\ c(f(x)+g(x)) = c \lim_{x \rightarrow k} f(x) + c\ \lim_{x \rightarrow k} g(x), \lim_{x \rightarrow k} (fg)(x) = \lim_{x \rightarrow k} f(x)g(x), ","['limits', 'terminology']"
36,"If $f(x)=x^2-x-1$ and $f^n(x)=f(f(\cdots f(x)\cdots))$, find all $x$ for which $f^{3n}(x)$ converges.","If  and , find all  for which  converges.",f(x)=x^2-x-1 f^n(x)=f(f(\cdots f(x)\cdots)) x f^{3n}(x),"Let $f:\mathbb{R}\to\mathbb{R}$ be the polynomial defined by $$f(x)=x^2-x-1$$ and let $$g_0(x)=f(x),\quad g_1(x)=f(f(x)),\quad\ldots\quad g_n(x)=f(f(f(\cdots f(x)\cdots)))$$ The positive root of $f(x)$ is the famous golden ratio $$\varphi=\frac{1+\sqrt{5}}{2}.$$ Problem: Find all $x\in\mathbb{R}$ for which the limit   $$\lim_{n\to\infty}g_{3n}(x)$$   exists. The answer appear to be not at all trivial. Based on numerical computations, I conjecture that the limit exists if and only if $$x\in[-\sqrt{2},1+\sqrt{2}]-\mathbb{N}.$$ But for some $x$, the convergence is very strange. For example, look at this plot of the sequence $\{g_{3n}(0.18)\}$ for $0\leq n\leq 200$: EDIT: As pointed out by Zach Effman, there seem to be a fractal structure emerging from this problem. To better visualize it, I looked at the boundedness of the sequence $\{g_n(z)\}$ for $z\in\mathbb{C}$. More precisely, I made a grid of points $z\in\mathbb{C}$, colored black if $\{|g_n(z)|\}$ is bounded at $z$ and the more quickly $\{|g_n(z)|\}$ diverges to infinity the more white the pixel is. A dramatic fractal structure emerges: Does anybody recognize this fractal? Is this a known fractal? The center of this picture reveals a very nice structure:","Let $f:\mathbb{R}\to\mathbb{R}$ be the polynomial defined by $$f(x)=x^2-x-1$$ and let $$g_0(x)=f(x),\quad g_1(x)=f(f(x)),\quad\ldots\quad g_n(x)=f(f(f(\cdots f(x)\cdots)))$$ The positive root of $f(x)$ is the famous golden ratio $$\varphi=\frac{1+\sqrt{5}}{2}.$$ Problem: Find all $x\in\mathbb{R}$ for which the limit   $$\lim_{n\to\infty}g_{3n}(x)$$   exists. The answer appear to be not at all trivial. Based on numerical computations, I conjecture that the limit exists if and only if $$x\in[-\sqrt{2},1+\sqrt{2}]-\mathbb{N}.$$ But for some $x$, the convergence is very strange. For example, look at this plot of the sequence $\{g_{3n}(0.18)\}$ for $0\leq n\leq 200$: EDIT: As pointed out by Zach Effman, there seem to be a fractal structure emerging from this problem. To better visualize it, I looked at the boundedness of the sequence $\{g_n(z)\}$ for $z\in\mathbb{C}$. More precisely, I made a grid of points $z\in\mathbb{C}$, colored black if $\{|g_n(z)|\}$ is bounded at $z$ and the more quickly $\{|g_n(z)|\}$ diverges to infinity the more white the pixel is. A dramatic fractal structure emerges: Does anybody recognize this fractal? Is this a known fractal? The center of this picture reveals a very nice structure:",,"['limits', 'polynomials', 'dynamical-systems', 'fractals', 'chaos-theory']"
37,Which infinity is meant in limits?,Which infinity is meant in limits?,,"For example, when we write $\lim_{x\rightarrow \infty} f(x)$ - which infinity is meant and why? Countable? If uncountable - which and why?","For example, when we write $\lim_{x\rightarrow \infty} f(x)$ - which infinity is meant and why? Countable? If uncountable - which and why?",,"['limits', 'infinity']"
38,On the Paris constant and $\sqrt[k]{1+\sqrt[k]{1+\sqrt[k]{1+\sqrt[k]{1+\dots}}}}$?,On the Paris constant and ?,\sqrt[k]{1+\sqrt[k]{1+\sqrt[k]{1+\sqrt[k]{1+\dots}}}},"In 1987, R. Paris proved that the nested radical expression for $\phi$, $$\phi=\sqrt{1+\sqrt{1+\sqrt{1+\sqrt{1+\dots}}}}$$ approaches $\phi$ at a constant rate. For example, defining $\phi_n$ as using $n = 5, 6, 7$ ""ones"" respectively, then, $$(1/2)(\phi-\phi_5)(2\phi)^5 = 1.0977\dots$$ $$(1/2)(\phi-\phi_6)(2\phi)^6 = 1.0983\dots$$ $$(1/2)(\phi-\phi_7)(2\phi)^7 = 1.0985\dots$$ which is approaching the Paris constant $R = 1.09864196\dots$. It seems it can be generalized. Define, $$x_n=\sqrt[k]{1+\sqrt[k]{1+\sqrt[k]{1+\sqrt[k]{1_n+\dots}}}}\tag{1}$$ for integer $k>1$ and the equations, $$x^k = x+1\tag{2}$$ $$y = \frac{1}{x}+1\tag{3}$$ where $x$ is the root of $(2)$ such that $x = x_n$ as $n \to \infty$ in $(1)$.  Then one can conjecture that, $$\lim_{n\to\infty}(1/2)(x-x_n)(ky)^n = C_k\tag{4}$$ for some constant $C_k$. The Paris constant is simply the case $C_2$. I tested it for increasing large $k$. The sequence of $C_k$ seem to be themselves approaching a constant. The rate is very slow, so for much higher $k = 10^{14},10^{15},10^{16}$, $$C_{10^{14}} =  0.6931471805599500\dots$$ $$C_{10^{15}} =  0.6931471805599457\dots$$ $$C_{10^{16}} =  0.6931471805599454\dots$$ Compare to, $$\ln 2 =  0.6931471805599453\dots$$ Question: Does $C_k \to \ln 2$, as $k \to \infty$? $\color{blue}{Edit,\; Nov.\;25}$ More generally, define, $$x_n=\sqrt[k]{a+\sqrt[k]{a+\sqrt[k]{a+\sqrt[k]{a_n+\dots}}}}\tag{5}$$ for integers $a\ge 1,\;k>1$ and, $$x^k = x+a\tag{6}$$ $$y = \frac{a}{x}+1\tag{7}$$ Then it seems, $$\lim_{n\to\infty}(1/2)(x-x_n)(ky)^n = C_{a,k}\tag{8}$$ The Paris constant is the case $C_{1,2}$. Is it true that as $k \to \infty$, then, $$\lim_{k\to \infty} C_{1,k} =  \ln 2$$ $$\lim_{k\to \infty} C_{2,k} = \tfrac{3}{2} \ln \tfrac{3}{2}$$ $$\lim_{k\to \infty} C_{3,k} = \tfrac{4}{2} \ln \tfrac{4}{3}$$ $$\lim_{k\to \infty} C_{4,k} = \tfrac{5}{2} \ln \tfrac{5}{4}$$ and so on? P.S. The only known closed-form in terms of transcendental constants is $C_{2,2} = \pi^2/8$.","In 1987, R. Paris proved that the nested radical expression for $\phi$, $$\phi=\sqrt{1+\sqrt{1+\sqrt{1+\sqrt{1+\dots}}}}$$ approaches $\phi$ at a constant rate. For example, defining $\phi_n$ as using $n = 5, 6, 7$ ""ones"" respectively, then, $$(1/2)(\phi-\phi_5)(2\phi)^5 = 1.0977\dots$$ $$(1/2)(\phi-\phi_6)(2\phi)^6 = 1.0983\dots$$ $$(1/2)(\phi-\phi_7)(2\phi)^7 = 1.0985\dots$$ which is approaching the Paris constant $R = 1.09864196\dots$. It seems it can be generalized. Define, $$x_n=\sqrt[k]{1+\sqrt[k]{1+\sqrt[k]{1+\sqrt[k]{1_n+\dots}}}}\tag{1}$$ for integer $k>1$ and the equations, $$x^k = x+1\tag{2}$$ $$y = \frac{1}{x}+1\tag{3}$$ where $x$ is the root of $(2)$ such that $x = x_n$ as $n \to \infty$ in $(1)$.  Then one can conjecture that, $$\lim_{n\to\infty}(1/2)(x-x_n)(ky)^n = C_k\tag{4}$$ for some constant $C_k$. The Paris constant is simply the case $C_2$. I tested it for increasing large $k$. The sequence of $C_k$ seem to be themselves approaching a constant. The rate is very slow, so for much higher $k = 10^{14},10^{15},10^{16}$, $$C_{10^{14}} =  0.6931471805599500\dots$$ $$C_{10^{15}} =  0.6931471805599457\dots$$ $$C_{10^{16}} =  0.6931471805599454\dots$$ Compare to, $$\ln 2 =  0.6931471805599453\dots$$ Question: Does $C_k \to \ln 2$, as $k \to \infty$? $\color{blue}{Edit,\; Nov.\;25}$ More generally, define, $$x_n=\sqrt[k]{a+\sqrt[k]{a+\sqrt[k]{a+\sqrt[k]{a_n+\dots}}}}\tag{5}$$ for integers $a\ge 1,\;k>1$ and, $$x^k = x+a\tag{6}$$ $$y = \frac{a}{x}+1\tag{7}$$ Then it seems, $$\lim_{n\to\infty}(1/2)(x-x_n)(ky)^n = C_{a,k}\tag{8}$$ The Paris constant is the case $C_{1,2}$. Is it true that as $k \to \infty$, then, $$\lim_{k\to \infty} C_{1,k} =  \ln 2$$ $$\lim_{k\to \infty} C_{2,k} = \tfrac{3}{2} \ln \tfrac{3}{2}$$ $$\lim_{k\to \infty} C_{3,k} = \tfrac{4}{2} \ln \tfrac{4}{3}$$ $$\lim_{k\to \infty} C_{4,k} = \tfrac{5}{2} \ln \tfrac{5}{4}$$ and so on? P.S. The only known closed-form in terms of transcendental constants is $C_{2,2} = \pi^2/8$.",,"['limits', 'logarithms', 'nested-radicals', 'constants', 'golden-ratio']"
39,"Why can there be an infinite difference between two functions as x grows large, but a ratio of 1?","Why can there be an infinite difference between two functions as x grows large, but a ratio of 1?",,"I learned in grade school that the closer $a$ and $b$ are to one another, the closer $\frac{a}{b}$ is going to be to $1$. For example, $\frac{3}{\pi}$ is pretty close to 1, and $\frac{10^{100}}{42}$ isn't even close to 1. So, why is: $$\lim_{x\to\infty} \frac{x^{2}}{x^{2}+x} = 1$$ But: $$\lim_{x\to\infty}[(x^2+x)-(x^2)] = \infty$$ ? Seems pretty counterintuitive. What's going on here?","I learned in grade school that the closer $a$ and $b$ are to one another, the closer $\frac{a}{b}$ is going to be to $1$. For example, $\frac{3}{\pi}$ is pretty close to 1, and $\frac{10^{100}}{42}$ isn't even close to 1. So, why is: $$\lim_{x\to\infty} \frac{x^{2}}{x^{2}+x} = 1$$ But: $$\lim_{x\to\infty}[(x^2+x)-(x^2)] = \infty$$ ? Seems pretty counterintuitive. What's going on here?",,"['limits', 'soft-question']"
40,How to show that $\lim_{n \to +\infty} n^{\frac{1}{n}} = 1$?,How to show that ?,\lim_{n \to +\infty} n^{\frac{1}{n}} = 1,"I've spent the better part of this day trying to show from first principles that this sequence tends to 1. Could anyone give me an idea of how I can approach this problem? $$ \lim_{n \to +\infty} n^{\frac{1}{n}} $$","I've spent the better part of this day trying to show from first principles that this sequence tends to 1. Could anyone give me an idea of how I can approach this problem? $$ \lim_{n \to +\infty} n^{\frac{1}{n}} $$",,"['analysis', 'limits', 'radicals']"
41,"Using Limits to Determine Big-O, Big-Omega, and Big-Theta","Using Limits to Determine Big-O, Big-Omega, and Big-Theta",,"I am trying to get a concrete answer on using limits to determine if two functions, $f(n)$ and $g(n)$, are Big-$O$, Big-$\Omega$, or Big-$\Theta$.  I have looked at my book, my lecture notes, and have even done some online research but I still haven't found anything that gives me a solid yes or no.  From what I understand about each of the three notations I have come up with 3 sets of rules: $\displaystyle f(n) = O(g(n)) \implies \lim_{n \to \infty}\;\frac{f(n)}{g(n)} = 0$ $\displaystyle f(n) = \Omega(g(n)) \implies \lim_{n \to \infty}\;\frac{f(n)}{g(n)} = \infty$ $\displaystyle f(n) = \Theta(g(n)) \implies 0 < \; \lim_{n \to \infty} \; \frac{f(n)}{g(n)}  \;< \infty$ The reason I am trying to get such a definite answer on this is because for a HW assignment we have to briefly explain why $\;f(n) = O(g(n))$, $\;f(n) = \Omega(g(n)),$ or $\;f(n) = \Theta(g(n))$.  If I can just use those 3 rules above my explanations will be short, sweet, and to the point. Any help or advice would be great appreciated","I am trying to get a concrete answer on using limits to determine if two functions, $f(n)$ and $g(n)$, are Big-$O$, Big-$\Omega$, or Big-$\Theta$.  I have looked at my book, my lecture notes, and have even done some online research but I still haven't found anything that gives me a solid yes or no.  From what I understand about each of the three notations I have come up with 3 sets of rules: $\displaystyle f(n) = O(g(n)) \implies \lim_{n \to \infty}\;\frac{f(n)}{g(n)} = 0$ $\displaystyle f(n) = \Omega(g(n)) \implies \lim_{n \to \infty}\;\frac{f(n)}{g(n)} = \infty$ $\displaystyle f(n) = \Theta(g(n)) \implies 0 < \; \lim_{n \to \infty} \; \frac{f(n)}{g(n)}  \;< \infty$ The reason I am trying to get such a definite answer on this is because for a HW assignment we have to briefly explain why $\;f(n) = O(g(n))$, $\;f(n) = \Omega(g(n)),$ or $\;f(n) = \Theta(g(n))$.  If I can just use those 3 rules above my explanations will be short, sweet, and to the point. Any help or advice would be great appreciated",,"['limits', 'algorithms', 'asymptotics']"
42,"Second derivative ""formula derivation""","Second derivative ""formula derivation""",,"I've been trying to understand how the second order derivative ""formula"" works: $$\lim_{h\to0} \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}$$ So, the rate of change of the rate of change for an arbitrary continuous function. It basically feels right, since it samples ""the after $x+h$ and the before $x-h$"" and the $h^2$ is there (due to the expected /h/h -> /h*h), but I'm having trouble finding the equation on my own. It's is basically a derivative of a derivative, right? Newtonian notation declares as $f''$ and Leibniz's as $\frac{\partial^2{y}}{\partial{x}^2}$ which dissolves into: $$(f')'$$ and $$\frac{\partial{}}{\partial{x}}\frac{\partial{f}}{\partial{x}}$$ So, first derivation shows the rate of change of a function's value relative to input. The second derivative shows the rate of change of the actual rate of change, suggesting information relating to how frequenly it changes. The original one is rather straightforward: $$\frac{\Delta y}{\Delta x} = \lim_{h\to0} \frac{f(x+h) - f(x)}{x + h - x} = \lim_{h\to0} \frac{f(x+h) - f(x)}{h}$$ And can easily be shown that $f'(x) = nx^{n-1} + \dots$ is correct for the more forthcoming of polynomial functions. So, my logic suggests that to get the derivative of a derivative, one only needs to send the derivative function as input to finding the new derivative. I'll drop the $\lim_{h\to0}$ for simplicity: $$f'(x) = \frac{f(x+h) - f(x)}{h}$$ So, the derivative of the derivative should be: $$f''(x) = \lim_{h\to0} \frac{f'(x+h) - f'(x)}{h}$$ $$f''(x) = \lim_{h\to0} \frac{  \frac{ f(x+2h) - f(x+h)}{h} - \frac{ f(x+h) - f(x)}{h}  }{h}$$ $$f''(x) = \lim_{h\to0} \frac{  \frac{ f(x+2h) - f(x+h) - f(x+h) + f(x)}{h}  }{h}$$ $$f''(x) = \lim_{h\to0} \frac{  f(x+2h) - f(x+h) - f(x+h) + f(x)  }{h^2}$$ $$f''(x) = \lim_{h\to0} \frac{  f(x+2h) - 2f(x+h) + f(x)  }{h^2}$$ What am I doing wrong? Perhaps it is the mess of it all, but I just can't see it. Please help.","I've been trying to understand how the second order derivative ""formula"" works: $$\lim_{h\to0} \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}$$ So, the rate of change of the rate of change for an arbitrary continuous function. It basically feels right, since it samples ""the after $x+h$ and the before $x-h$"" and the $h^2$ is there (due to the expected /h/h -> /h*h), but I'm having trouble finding the equation on my own. It's is basically a derivative of a derivative, right? Newtonian notation declares as $f''$ and Leibniz's as $\frac{\partial^2{y}}{\partial{x}^2}$ which dissolves into: $$(f')'$$ and $$\frac{\partial{}}{\partial{x}}\frac{\partial{f}}{\partial{x}}$$ So, first derivation shows the rate of change of a function's value relative to input. The second derivative shows the rate of change of the actual rate of change, suggesting information relating to how frequenly it changes. The original one is rather straightforward: $$\frac{\Delta y}{\Delta x} = \lim_{h\to0} \frac{f(x+h) - f(x)}{x + h - x} = \lim_{h\to0} \frac{f(x+h) - f(x)}{h}$$ And can easily be shown that $f'(x) = nx^{n-1} + \dots$ is correct for the more forthcoming of polynomial functions. So, my logic suggests that to get the derivative of a derivative, one only needs to send the derivative function as input to finding the new derivative. I'll drop the $\lim_{h\to0}$ for simplicity: $$f'(x) = \frac{f(x+h) - f(x)}{h}$$ So, the derivative of the derivative should be: $$f''(x) = \lim_{h\to0} \frac{f'(x+h) - f'(x)}{h}$$ $$f''(x) = \lim_{h\to0} \frac{  \frac{ f(x+2h) - f(x+h)}{h} - \frac{ f(x+h) - f(x)}{h}  }{h}$$ $$f''(x) = \lim_{h\to0} \frac{  \frac{ f(x+2h) - f(x+h) - f(x+h) + f(x)}{h}  }{h}$$ $$f''(x) = \lim_{h\to0} \frac{  f(x+2h) - f(x+h) - f(x+h) + f(x)  }{h^2}$$ $$f''(x) = \lim_{h\to0} \frac{  f(x+2h) - 2f(x+h) + f(x)  }{h^2}$$ What am I doing wrong? Perhaps it is the mess of it all, but I just can't see it. Please help.",,"['limits', 'derivatives']"
43,How to prove that $\lim \limits_{n\rightarrow \infty} \frac{F_{n+1}}{F_n}=\frac{\sqrt{5}+1}{2}$,How to prove that,\lim \limits_{n\rightarrow \infty} \frac{F_{n+1}}{F_n}=\frac{\sqrt{5}+1}{2},How would one prove that $$\lim_{n\rightarrow \infty} \frac{F_{n+1}}{F_n}=\frac{\sqrt{5}+1}{2}=\varphi$$ where $F_n$ is the nth Fibonacci number and $\varphi$ is the Golden Ratio?,How would one prove that $$\lim_{n\rightarrow \infty} \frac{F_{n+1}}{F_n}=\frac{\sqrt{5}+1}{2}=\varphi$$ where $F_n$ is the nth Fibonacci number and $\varphi$ is the Golden Ratio?,,"['limits', 'fibonacci-numbers', 'golden-ratio']"
44,"Is there such thing as a ""smallest positive number that isn't zero""? [duplicate]","Is there such thing as a ""smallest positive number that isn't zero""? [duplicate]",,"This question already has answers here : What is the meaning of infinitesimal? (6 answers) Closed 7 years ago . My brother and I have been discussing whether it would be possible to have a ""smallest positive number"" or not and we have concluded that it's impossible. Here's our reasoning: firstly, my brother discussed how you can always halve something, $(1, 0.5, 0.25, \dotsc)$ . I myself believe that it is impossible because of something I managed to come up with. You can put an infinite amount of zeroes in the decimal place before a number, $(0.1, 0.01, 0.001, \dotsc)$ . I am not entirely sure if our reasoning is correct though. I have been told that there is a smallest number possible but I decided to see for myself.","This question already has answers here : What is the meaning of infinitesimal? (6 answers) Closed 7 years ago . My brother and I have been discussing whether it would be possible to have a ""smallest positive number"" or not and we have concluded that it's impossible. Here's our reasoning: firstly, my brother discussed how you can always halve something, . I myself believe that it is impossible because of something I managed to come up with. You can put an infinite amount of zeroes in the decimal place before a number, . I am not entirely sure if our reasoning is correct though. I have been told that there is a smallest number possible but I decided to see for myself.","(1, 0.5, 0.25, \dotsc) (0.1, 0.01, 0.001, \dotsc)","['limits', 'proof-verification']"
45,A continued fraction involving prime numbers,A continued fraction involving prime numbers,,"What is the limit of the continued fraction $$\cfrac{1}{2+\cfrac{1}{3+\cfrac{1}{5+\cfrac{1}{7+\cfrac{1}{11+\cfrac{1}{13+\cdots}}}}}}\ ?$$ Is the limit algebraic, or expressible in terms of e or $\pi$? What is the fastest way to approximate the limit?","What is the limit of the continued fraction $$\cfrac{1}{2+\cfrac{1}{3+\cfrac{1}{5+\cfrac{1}{7+\cfrac{1}{11+\cfrac{1}{13+\cdots}}}}}}\ ?$$ Is the limit algebraic, or expressible in terms of e or $\pi$? What is the fastest way to approximate the limit?",,"['prime-numbers', 'limits', 'continued-fractions']"
46,What is $\lim_{x \to 0}\frac{\sin\left(\frac 1x\right)}{\sin \left(\frac 1 x\right)}$ ? Does it exist?,What is  ? Does it exist?,\lim_{x \to 0}\frac{\sin\left(\frac 1x\right)}{\sin \left(\frac 1 x\right)},"Does $$\lim_{x \to 0}\;\frac{\sin\left(\frac 1x\right)}{\sin \left(\frac 1 x\right)}$$ exist? I believe the limit should be $1$. Because function being defined at the point is not a condition for limit to exist. This question came in my test and the answer given is limit does not exist. But if we see the graph , it is quite clear the function is exact 1 as $x \to 0$, so the limit should be 0. Even wolfram alpha gives the limit to be 1. But we are playing with infinity, so who knows? Maybe I am missing out on something? So what exactly is the limit and why? Edit: Wolfram alpha's widget (the link to which I have posted above) says the limit is 1. But here wolfram alpha says that the limit doesn't exist on the real line.","Does $$\lim_{x \to 0}\;\frac{\sin\left(\frac 1x\right)}{\sin \left(\frac 1 x\right)}$$ exist? I believe the limit should be $1$. Because function being defined at the point is not a condition for limit to exist. This question came in my test and the answer given is limit does not exist. But if we see the graph , it is quite clear the function is exact 1 as $x \to 0$, so the limit should be 0. Even wolfram alpha gives the limit to be 1. But we are playing with infinity, so who knows? Maybe I am missing out on something? So what exactly is the limit and why? Edit: Wolfram alpha's widget (the link to which I have posted above) says the limit is 1. But here wolfram alpha says that the limit doesn't exist on the real line.",,['limits']
47,An iterative logarithmic transformation of a power series,An iterative logarithmic transformation of a power series,,"Consider the following iterative process. We start with the function having all $1$ 's in its Taylor series expansion: $$f_0(x)=\frac1{1-x}=1+x+x^2+x^3+x^4+O\left(x^5\right).\tag1$$ Then, at each step we apply the following transformation: $$f_{n+1}(x)=x^{-1}\log\left(\frac{f_n(x)}{f_n(0)}\right).\tag2$$ A few initial iterations give us: $$ \begin{array}{l} f_1(x)=1+\frac{x}{2}+\frac{x^2}{3}+\frac{x^3}{4}+\frac{x^4}{5}+O\left(x^5\right), \\ f_2(x)=\frac{1}{2}+\frac{5 x}{24}+\frac{x^2}{8}+\frac{251 x^3}{2880}+\frac{19 x^4}{288}+O\left(x^5\right), \\ f_3(x)=\frac{5}{12}+\frac{47 x}{288}+\frac{2443 x^2}{25920}+\frac{5303 x^3}{82944}+\frac{412589x^4}{8709120}+O\left(x^5\right), \end{array}\tag3 $$ and their initial terms form the following sequence: $$1,\,\frac{1}{2},\,\frac{5}{12},\,\frac{47}{120},\,\frac{12917}{33840},\,\frac{329458703}{874222560},\,\dots,\tag4$$ whose denominators grow pretty quickly, but which appear to slowly converge to a value $$\lambda\stackrel{\color{#aaaaaa}?}\approx0.3678\dots\tag5$$ If we assume that the process with the iterative step $(2)$ converges to a fixed point, we can see that it must have a form: $$f_\omega(x)=-x^{-1}\,W(-c\,x)=c+c^2 x+\frac{3\, c^3\, x^2}{2}+\frac{8\, c^4\, x^3}{3}+\frac{125\, c^5\, x^4}{24}+O\left(x^5\right),\tag6$$ where $W(\cdot)$ is the Lambert 𝑊-function , and $c$ is a coefficient that is not uniquely determined but depends on the choice of the initial function $f_0(x)$ . In our case, $c=\lambda$ . Questions: Does this process actually converge to a fixed point? If yes, then what is a closed-form expression (or another useful description) for $\lambda$ ? Update: An explicit recurrent formula for the coefficients (the parenthesized superscript $m$ in $a_n^{\small(m)}$ is just the second index of the coefficient; the sum $\sum_{\ell=1}^m$ is assumed to be $0$ when $m=0$ ): $$f_n(x)=\sum_{m=0}^\infty a_{n\vphantom{+0}}^{\small(m)}x^m,$$ where $$a_{0\vphantom{+0}}^{\small(m)}=1,\quad a_{n\vphantom{+0}}^{\small(m)}=\frac1{\,a_{n-1}^{\small(0)}\,}\left(a_{n-1}^{\small(m+1)}-\frac1{m+1} \sum_{\ell=1}^m\ell\;a_{n\vphantom{+0}}^{\small(\ell-1)}\,a_{n-1}^{\small(m-\ell+1)}\right).$$ The sequence of coefficients $(4)$ is $\big\{a_{n\vphantom{+0}}^{\small(0)}\big\}$ .","Consider the following iterative process. We start with the function having all 's in its Taylor series expansion: Then, at each step we apply the following transformation: A few initial iterations give us: and their initial terms form the following sequence: whose denominators grow pretty quickly, but which appear to slowly converge to a value If we assume that the process with the iterative step converges to a fixed point, we can see that it must have a form: where is the Lambert 𝑊-function , and is a coefficient that is not uniquely determined but depends on the choice of the initial function . In our case, . Questions: Does this process actually converge to a fixed point? If yes, then what is a closed-form expression (or another useful description) for ? Update: An explicit recurrent formula for the coefficients (the parenthesized superscript in is just the second index of the coefficient; the sum is assumed to be when ): where The sequence of coefficients is .","1 f_0(x)=\frac1{1-x}=1+x+x^2+x^3+x^4+O\left(x^5\right).\tag1 f_{n+1}(x)=x^{-1}\log\left(\frac{f_n(x)}{f_n(0)}\right).\tag2 
\begin{array}{l}
f_1(x)=1+\frac{x}{2}+\frac{x^2}{3}+\frac{x^3}{4}+\frac{x^4}{5}+O\left(x^5\right), \\
f_2(x)=\frac{1}{2}+\frac{5 x}{24}+\frac{x^2}{8}+\frac{251 x^3}{2880}+\frac{19 x^4}{288}+O\left(x^5\right), \\
f_3(x)=\frac{5}{12}+\frac{47 x}{288}+\frac{2443 x^2}{25920}+\frac{5303 x^3}{82944}+\frac{412589x^4}{8709120}+O\left(x^5\right),
\end{array}\tag3
 1,\,\frac{1}{2},\,\frac{5}{12},\,\frac{47}{120},\,\frac{12917}{33840},\,\frac{329458703}{874222560},\,\dots,\tag4 \lambda\stackrel{\color{#aaaaaa}?}\approx0.3678\dots\tag5 (2) f_\omega(x)=-x^{-1}\,W(-c\,x)=c+c^2 x+\frac{3\, c^3\, x^2}{2}+\frac{8\, c^4\, x^3}{3}+\frac{125\, c^5\, x^4}{24}+O\left(x^5\right),\tag6 W(\cdot) c f_0(x) c=\lambda \lambda m a_n^{\small(m)} \sum_{\ell=1}^m 0 m=0 f_n(x)=\sum_{m=0}^\infty a_{n\vphantom{+0}}^{\small(m)}x^m, a_{0\vphantom{+0}}^{\small(m)}=1,\quad a_{n\vphantom{+0}}^{\small(m)}=\frac1{\,a_{n-1}^{\small(0)}\,}\left(a_{n-1}^{\small(m+1)}-\frac1{m+1} \sum_{\ell=1}^m\ell\;a_{n\vphantom{+0}}^{\small(\ell-1)}\,a_{n-1}^{\small(m-\ell+1)}\right). (4) \big\{a_{n\vphantom{+0}}^{\small(0)}\big\}","['limits', 'logarithms', 'power-series', 'closed-form', 'lambert-w']"
48,"1 to the power of infinity, why is it indeterminate? [duplicate]","1 to the power of infinity, why is it indeterminate? [duplicate]",,"This question already has answers here : Why is $1^{\infty}$ considered to be an indeterminate form (9 answers) Closed 11 years ago . I've been taught that $1^\infty$ is undetermined case. Why is it so? Isn't $1*1*1...=1$ whatever times you would multiply it? So if you take a limit, say $\lim_{n\to\infty} 1^n$, doesn't it converge to 1? So why would the limit not exist?","This question already has answers here : Why is $1^{\infty}$ considered to be an indeterminate form (9 answers) Closed 11 years ago . I've been taught that $1^\infty$ is undetermined case. Why is it so? Isn't $1*1*1...=1$ whatever times you would multiply it? So if you take a limit, say $\lim_{n\to\infty} 1^n$, doesn't it converge to 1? So why would the limit not exist?",,"['limits', 'infinity', 'exponentiation']"
49,Does the logarithm function grow slower than any polynomial?,Does the logarithm function grow slower than any polynomial?,,"Does $f(x)=ax^b$ grow faster than $g(x)=\ln x$ for all $a, b > 0$? Can I say that $f(x) > g(x)$ as $x$ approaches infinity? I thought the answer is yes, but this graph appears to be telling a different story. Is the polynomial (the green curve) going to cross the log function (the red curve) and exceed in value for some large value of x? If the answer is yes, does it mean that if I subtract the two functions and set it to zero, the resulting equation will have two roots? What are the roots?","Does $f(x)=ax^b$ grow faster than $g(x)=\ln x$ for all $a, b > 0$? Can I say that $f(x) > g(x)$ as $x$ approaches infinity? I thought the answer is yes, but this graph appears to be telling a different story. Is the polynomial (the green curve) going to cross the log function (the red curve) and exceed in value for some large value of x? If the answer is yes, does it mean that if I subtract the two functions and set it to zero, the resulting equation will have two roots? What are the roots?",,"['limits', 'polynomials', 'logarithms']"
50,Nested Radicals Involving Primes,Nested Radicals Involving Primes,,"How do you evaluate $\sqrt { 2+\sqrt { 3+\sqrt { 5+\sqrt { 7+\sqrt { 11+ \dots }  }  }  }  } $ ? This question appears to be rather difficult as there is no way to perfectly know what $p_{ n }$ is , if $p_{ n }$ denotes the $n$ th prime. It is simple to show that the value above is convergent. Bertrand`s Postulate implies that $p_{ n } \le 2^n$ , which implies that $\sqrt { 2+\sqrt { 3+\sqrt { 5+\sqrt { 7+\sqrt { 11+ \dots }  }  }  }  } \le \sqrt { 2+\sqrt { 4+\sqrt { 8+\sqrt { 16+\sqrt { 32+\dots }  }  }  }  } $ , which is convergent, as seen here . So it is pretty clear that $\sqrt { 2+\sqrt { 3+\sqrt { 5+\sqrt { 7+\sqrt { 11+ \dots }  }  }  }  } $ is convergent. However, in which fashion can you evaluate the value above? If there is no exact way to evaluate it, is it irrational or rational? The value seems to be about $2.10359749633989726261993..$ as seen here . Any help would be appreciated.","How do you evaluate ? This question appears to be rather difficult as there is no way to perfectly know what is , if denotes the th prime. It is simple to show that the value above is convergent. Bertrand`s Postulate implies that , which implies that , which is convergent, as seen here . So it is pretty clear that is convergent. However, in which fashion can you evaluate the value above? If there is no exact way to evaluate it, is it irrational or rational? The value seems to be about as seen here . Any help would be appreciated.",\sqrt { 2+\sqrt { 3+\sqrt { 5+\sqrt { 7+\sqrt { 11+ \dots }  }  }  }  }  p_{ n } p_{ n } n p_{ n } \le 2^n \sqrt { 2+\sqrt { 3+\sqrt { 5+\sqrt { 7+\sqrt { 11+ \dots }  }  }  }  } \le \sqrt { 2+\sqrt { 4+\sqrt { 8+\sqrt { 16+\sqrt { 32+\dots }  }  }  }  }  \sqrt { 2+\sqrt { 3+\sqrt { 5+\sqrt { 7+\sqrt { 11+ \dots }  }  }  }  }  2.10359749633989726261993..,"['number-theory', 'limits', 'convergence-divergence', 'nested-radicals']"
51,limit $\lim_{n\to ∞}\sin(\pi(2+\sqrt3)^n)$,limit,\lim_{n\to ∞}\sin(\pi(2+\sqrt3)^n),"$\lim_{n\to ∞}\sin(\pi(2+\sqrt3)^n)$ I tried to write it as $\sin (n\pi - \theta)$ to get the form $∞-∞$ form within $\sin$ function. But could not proceed after that. How should I do it? Edit:I am sorry, I forgot to mention $n\in \mathbb{N}$","$\lim_{n\to ∞}\sin(\pi(2+\sqrt3)^n)$ I tried to write it as $\sin (n\pi - \theta)$ to get the form $∞-∞$ form within $\sin$ function. But could not proceed after that. How should I do it? Edit:I am sorry, I forgot to mention $n\in \mathbb{N}$",,['limits']
52,Find $\sqrt{4+\sqrt[3]{4+\sqrt[4]{4+\sqrt[5]{4+\cdots}}}}$,Find,\sqrt{4+\sqrt[3]{4+\sqrt[4]{4+\sqrt[5]{4+\cdots}}}},"Find the value of $$\sqrt{4+\sqrt[3]{4+\sqrt[4]{4+\sqrt[5]{4+\cdots}}}}$$ I know how to solve when all surds are of the same order, but what if they are different? Technically, (as some users wanted to know exactly what is to be found), find: $$\lim_{n\to\infty}\sqrt{4+\sqrt[3]{4+\sqrt[4]{4+\sqrt[5]{4+\cdots+\sqrt[n]{4}}}}} $$","Find the value of I know how to solve when all surds are of the same order, but what if they are different? Technically, (as some users wanted to know exactly what is to be found), find:",\sqrt{4+\sqrt[3]{4+\sqrt[4]{4+\sqrt[5]{4+\cdots}}}} \lim_{n\to\infty}\sqrt{4+\sqrt[3]{4+\sqrt[4]{4+\sqrt[5]{4+\cdots+\sqrt[n]{4}}}}} ,"['limits', 'recurrence-relations', 'nested-radicals']"
53,Why does simplifying a function give it another limit [duplicate],Why does simplifying a function give it another limit [duplicate],,This question already has answers here : Why does factoring eliminate a hole in the limit? (16 answers) Closed 8 years ago . I'm asked: $$\lim_{x\to 1} \frac{x^3 - 1}{x^2 + 2x -3}$$ This does obviously not evaluate since the denominator equals $0$. The solution is to: $$\lim_{x\to 1} \frac{(x-1)(x^2+x+1)}{(x-1)(x+3)}$$ $$\lim_{x\to 1} \frac{x^2 + x + 1}{x + 3}$$ $$\frac{1+1+1}{1+3} = \frac{3}{4}$$ My question: what is actually happening? How can simplifying a function give it another limit? Is it a complete other function and if so why would it be relevant to our original question?,This question already has answers here : Why does factoring eliminate a hole in the limit? (16 answers) Closed 8 years ago . I'm asked: $$\lim_{x\to 1} \frac{x^3 - 1}{x^2 + 2x -3}$$ This does obviously not evaluate since the denominator equals $0$. The solution is to: $$\lim_{x\to 1} \frac{(x-1)(x^2+x+1)}{(x-1)(x+3)}$$ $$\lim_{x\to 1} \frac{x^2 + x + 1}{x + 3}$$ $$\frac{1+1+1}{1+3} = \frac{3}{4}$$ My question: what is actually happening? How can simplifying a function give it another limit? Is it a complete other function and if so why would it be relevant to our original question?,,"['limits', 'paradoxes', 'indeterminate-forms']"
54,WolframAlpha says limit exists when it doesn't?,WolframAlpha says limit exists when it doesn't?,,"I was trying to calculate the following limit: $$ \lim_{(x,y)\to (0,0)} \frac{(x^2+y^2)^2}{x^2+y^4} $$ and, feeding it into WolframAlpha, I obtain the following answer, stating the limit is $0$: However, when I try to calculate the limit when $x = 0$ and $y$ approaches 0, the limit is 1... Is the answer given by WolframAlpha wrong? or am I?","I was trying to calculate the following limit: $$ \lim_{(x,y)\to (0,0)} \frac{(x^2+y^2)^2}{x^2+y^4} $$ and, feeding it into WolframAlpha, I obtain the following answer, stating the limit is $0$: However, when I try to calculate the limit when $x = 0$ and $y$ approaches 0, the limit is 1... Is the answer given by WolframAlpha wrong? or am I?",,"['limits', 'multivariable-calculus', 'wolfram-alpha']"
55,"Conjecture: $\lim\limits_{x\to 0}(x!\,x!!\,x!!!\,x!!!!\cdots )^{-1/x}\stackrel?=e$",Conjecture:,"\lim\limits_{x\to 0}(x!\,x!!\,x!!!\,x!!!!\cdots )^{-1/x}\stackrel?=e","Well, it's a conjecture so let me propose it: $$\lim_{x\to 0}(x!\,x!!\,x!!!\,x!!!!\cdots)^{-1/x}\stackrel?=e$$ Where I use desmos notation and $x!! := ((x!)!,x!!!=(((x!)!)!)$ It seems so hard that I haven't any clue to show it. I already know that $$x!^{\frac{-1}{x}}=e^{\gamma}$$ Perhaps we can use the Weierstrass factorization theorem and compute it. So, how to (dis)prove it? Further investigation : If we supposed that the following functions are convex on $(0,1)$ : $$a_1(x)=x!,a_2=x!!,\cdots,a_n=x!!\cdots !$$ Then we can rewrite the conjectured limit as : $$L=\lim_{x\to 0}\left((a_1'(0)x+1)(a_2'(0)x+1)\cdots(a'_n(0)x+1)\cdots\right)^{\frac{-1}{x}}=^?e$$ Where : $$a_1'(0)=\gamma,a_2'(0)=(\gamma-1)\gamma,a_3'(0)=-(\gamma-1)^2\gamma,a_4'(0)=(\gamma-1)^3\gamma,\cdots\tag{I}$$ Update : It seems we have : $$\lim_{x\to 0^+}\left(x!x!!x!!!x!!!!x!!!!!...\right)^{\frac{2^{x}-1}{x^{2}}}=1/2$$ Update $2$ : Using $I$ and the fact that (see Robjohn's answer) : $$\sum_{n=1}^\infty\gamma(1-\gamma)^{n-1}=1$$ And : Let $x_i\in[1-1/n,1]$ where $n\geq M$ two natural numbers large enought then we have : $$\sum_{i=1}^{n}x_i-(n-1)\leq \prod_{i=1}^{n}x_i\leq \sum_{i=1}^{n}x_i-(n-1)+\frac{1}{2n}$$ We have after simplification : $$\left(-x+1\right)^{\left(-\frac{1}{x}\right)}<L<\left(-x+1+\frac{1}{2n}\right)^{\left(-\frac{1}{x}\right)}$$ now let $n\to \infty$ and $x\to 0$ we get the result . Ps:It's a try and I think it should be clearing a little (the LHS seems dubious) and the credit come back to @Robjohn.","Well, it's a conjecture so let me propose it: Where I use desmos notation and It seems so hard that I haven't any clue to show it. I already know that Perhaps we can use the Weierstrass factorization theorem and compute it. So, how to (dis)prove it? Further investigation : If we supposed that the following functions are convex on : Then we can rewrite the conjectured limit as : Where : Update : It seems we have : Update : Using and the fact that (see Robjohn's answer) : And : Let where two natural numbers large enought then we have : We have after simplification : now let and we get the result . Ps:It's a try and I think it should be clearing a little (the LHS seems dubious) and the credit come back to @Robjohn.","\lim_{x\to 0}(x!\,x!!\,x!!!\,x!!!!\cdots)^{-1/x}\stackrel?=e x!! := ((x!)!,x!!!=(((x!)!)!) x!^{\frac{-1}{x}}=e^{\gamma} (0,1) a_1(x)=x!,a_2=x!!,\cdots,a_n=x!!\cdots ! L=\lim_{x\to 0}\left((a_1'(0)x+1)(a_2'(0)x+1)\cdots(a'_n(0)x+1)\cdots\right)^{\frac{-1}{x}}=^?e a_1'(0)=\gamma,a_2'(0)=(\gamma-1)\gamma,a_3'(0)=-(\gamma-1)^2\gamma,a_4'(0)=(\gamma-1)^3\gamma,\cdots\tag{I} \lim_{x\to 0^+}\left(x!x!!x!!!x!!!!x!!!!!...\right)^{\frac{2^{x}-1}{x^{2}}}=1/2 2 I \sum_{n=1}^\infty\gamma(1-\gamma)^{n-1}=1 x_i\in[1-1/n,1] n\geq M \sum_{i=1}^{n}x_i-(n-1)\leq \prod_{i=1}^{n}x_i\leq \sum_{i=1}^{n}x_i-(n-1)+\frac{1}{2n} \left(-x+1\right)^{\left(-\frac{1}{x}\right)}<L<\left(-x+1+\frac{1}{2n}\right)^{\left(-\frac{1}{x}\right)} n\to \infty x\to 0","['limits', 'gamma-function', 'constants']"
56,L'hospital rule for two variable.,L'hospital rule for two variable.,,"How to use L'hospital rule to compute the limit of the given function $$\lim_{(x,y)\to (0,0)} \frac{x^{2}+y^{2}}{x+y}?$$","How to use L'hospital rule to compute the limit of the given function $$\lim_{(x,y)\to (0,0)} \frac{x^{2}+y^{2}}{x+y}?$$",,"['limits', 'multivariable-calculus']"
57,Why does a product $\lim_{n\to\infty} \prod_{i=1}^n (1- \frac{1}{n+2i})$ lead to infinity?,Why does a product  lead to infinity?,\lim_{n\to\infty} \prod_{i=1}^n (1- \frac{1}{n+2i}),"I asked Wolfram Alpha to compute the following limit : $\lim_{n\to\infty} \prod_{i=1}^n (1- \frac{1}{n+2i})$ and the answer was $\infty$ . I do not understand how this can be, as all elements in the product are smaller than 1. For comparison, when I asked about the following limit : $\lim_{n\to\infty} \prod_{i=1}^n (1- \frac{1}{n+i})=1/2$ . What is the real value of these limits?","I asked Wolfram Alpha to compute the following limit : and the answer was . I do not understand how this can be, as all elements in the product are smaller than 1. For comparison, when I asked about the following limit : . What is the real value of these limits?",\lim_{n\to\infty} \prod_{i=1}^n (1- \frac{1}{n+2i}) \infty \lim_{n\to\infty} \prod_{i=1}^n (1- \frac{1}{n+i})=1/2,"['limits', 'products', 'wolfram-alpha']"
58,Simple proof Euler–Mascheroni $\gamma$ constant,Simple proof Euler–Mascheroni  constant,\gamma,I'm searching for a really simple and beautiful proof that the sequence $(u_n)_{n \in \mathbb{N}} = \displaystyle\sum_{k=1}^n \frac{1}{k} - \log(n)$ converges. At first I want to know if my answer is OK. My try: $\lim\limits_{n\to\infty} \left(\sum\limits_{k=1}^n \frac{1}{k} - \log (n)\right) = \lim\limits_{n\to\infty} \left(\sum\limits_{k=1}^n \frac{1}{k} + \sum\limits_{k=1}^{n-1} [\log(k)-\log(k+1)]\right)$ $ = \lim\limits_{n\to\infty} \left(\frac{1}{n} + \sum\limits_{k=1}^{n-1} \left[\log(\frac{k}{k+1})+\frac{1}{k}\right]\right) = \sum\limits_{k=1}^{\infty} \left[\frac{1}{k}-\log(\frac{k+1}{k})\right]$ Now we prove that the last sum converges by the comparison test: $\frac{1}{k}-\log(\frac{k+1}{k}) < \frac{1}{k^2} \Leftrightarrow k<k^2\log(\frac{k+1}{k})+1$ which surely holds for $k\geqslant 1$ As $ \sum\limits_{k=1}^{\infty} \frac{1}{k^2}$ converges $ \Rightarrow \sum\limits_{k=1}^{\infty} \left[\frac{1}{k}-\log(\frac{k+1}{k})\right]$ converges and we name this limit $\gamma$ q.e.d,I'm searching for a really simple and beautiful proof that the sequence converges. At first I want to know if my answer is OK. My try: Now we prove that the last sum converges by the comparison test: which surely holds for As converges converges and we name this limit q.e.d,(u_n)_{n \in \mathbb{N}} = \displaystyle\sum_{k=1}^n \frac{1}{k} - \log(n) \lim\limits_{n\to\infty} \left(\sum\limits_{k=1}^n \frac{1}{k} - \log (n)\right) = \lim\limits_{n\to\infty} \left(\sum\limits_{k=1}^n \frac{1}{k} + \sum\limits_{k=1}^{n-1} [\log(k)-\log(k+1)]\right)  = \lim\limits_{n\to\infty} \left(\frac{1}{n} + \sum\limits_{k=1}^{n-1} \left[\log(\frac{k}{k+1})+\frac{1}{k}\right]\right) = \sum\limits_{k=1}^{\infty} \left[\frac{1}{k}-\log(\frac{k+1}{k})\right] \frac{1}{k}-\log(\frac{k+1}{k}) < \frac{1}{k^2} \Leftrightarrow k<k^2\log(\frac{k+1}{k})+1 k\geqslant 1  \sum\limits_{k=1}^{\infty} \frac{1}{k^2}  \Rightarrow \sum\limits_{k=1}^{\infty} \left[\frac{1}{k}-\log(\frac{k+1}{k})\right] \gamma,"['limits', 'logarithms', 'euler-mascheroni-constant', 'harmonic-numbers']"
59,Is $e$ a coincidence?,Is  a coincidence?,e,"$e$ has many definitions and properties. The one I'm most used to is $$\lim_{n\to \infty}\left(1+\frac{1}{n}\right)^n $$ If someone asked me (and I didn't know about $e$): Is there a constant $c$ such that the equation $\frac{d}{dx}c^x=c^x $ is true for all $x$? Then I'd likely answer that: I doubt it! That would be a crazy coincidence. I'm curious, is it a coincidence that there is a constant that makes this true?","$e$ has many definitions and properties. The one I'm most used to is $$\lim_{n\to \infty}\left(1+\frac{1}{n}\right)^n $$ If someone asked me (and I didn't know about $e$): Is there a constant $c$ such that the equation $\frac{d}{dx}c^x=c^x $ is true for all $x$? Then I'd likely answer that: I doubt it! That would be a crazy coincidence. I'm curious, is it a coincidence that there is a constant that makes this true?",,"['limits', 'derivatives', 'exponential-function']"
60,Are there any paths that will always show if there is a limit?,Are there any paths that will always show if there is a limit?,,I'm trying to do limits in 3D and I'm wondering whether or not there are paths along which the limit of any function at any point can always be found. In my book it isn't clear whether this exists or not; neither is it clear how to choose a path if this does exist. In the book they replace $y$ with $kx$ a lot but sometimes they replace $x$ and $y$ with $0$ separately  and sometimes they use $x^2$. I've seen some people have already asked similar questions but about specific formulas and I can't link that to my exact question. Thanks for any help!,I'm trying to do limits in 3D and I'm wondering whether or not there are paths along which the limit of any function at any point can always be found. In my book it isn't clear whether this exists or not; neither is it clear how to choose a path if this does exist. In the book they replace $y$ with $kx$ a lot but sometimes they replace $x$ and $y$ with $0$ separately  and sometimes they use $x^2$. I've seen some people have already asked similar questions but about specific formulas and I can't link that to my exact question. Thanks for any help!,,"['limits', '3d']"
61,Conjecture: $\lim_{x\rightarrow 0_+} \sum_{n=1}^{\infty} \frac{\sin(n^2 x)}{n} = \frac{\pi}{4}$,Conjecture:,\lim_{x\rightarrow 0_+} \sum_{n=1}^{\infty} \frac{\sin(n^2 x)}{n} = \frac{\pi}{4},"I was playing around with sums the other day, and started fiddling with the function $$ f(x) = \sum_{n=1}^{\infty} \frac{\sin(n^2 x)}{n}\, . $$ Now, obviously this is a very jagged function. (I think the derivative doesn't exist anywhere.) However, it seems to go to a finite, positive limit as $x\rightarrow 0_+$. Furthermore, just looking at the first few decimal places of this limit, it looks like it may be $\pi/4$. It seems plausible that such a limit might go to a ""nice"" number like $\pi/4$, but I can't prove it. A few known things about this problem: 1) $f(x)$ is odd, so $\lim_{x\rightarrow 0_{-}} f(x) = -\lim_{x\rightarrow 0_{+}} f(x)$. 2) $f(x)$ is $2\pi$-periodic (obviously). 3) One possible way that occurs to me to evaluate this limit (if it exists), is to replace it with the following: \begin{align} \lim_{x\rightarrow 0_{+}} f(x) &= \lim_{x\rightarrow 0_{+}} \frac{1}{x}\int_{0}^{x} dy\, f(y)\\ &= \lim_{x\rightarrow 0_{+}} \frac{2}{x}\sum_{n=1}^{\infty} \frac{\sin^2\left(n^2 x/2\right)}{n^3} \end{align} The latter series is smoother and converges more quickly than the original one, so it's better-suited to numerics. If I use $x = 0.0001$ in this series and sum the first $100,000$ terms in Mathematica, I get $0.785393$, whereas $\pi/4 = 0.785398...$ I don't know where to go from there. (I tried the Poisson summation formula to no avail.) Can anyone here prove this conjecture? Or disprove it? Or show that the question is somehow ill-posed?","I was playing around with sums the other day, and started fiddling with the function $$ f(x) = \sum_{n=1}^{\infty} \frac{\sin(n^2 x)}{n}\, . $$ Now, obviously this is a very jagged function. (I think the derivative doesn't exist anywhere.) However, it seems to go to a finite, positive limit as $x\rightarrow 0_+$. Furthermore, just looking at the first few decimal places of this limit, it looks like it may be $\pi/4$. It seems plausible that such a limit might go to a ""nice"" number like $\pi/4$, but I can't prove it. A few known things about this problem: 1) $f(x)$ is odd, so $\lim_{x\rightarrow 0_{-}} f(x) = -\lim_{x\rightarrow 0_{+}} f(x)$. 2) $f(x)$ is $2\pi$-periodic (obviously). 3) One possible way that occurs to me to evaluate this limit (if it exists), is to replace it with the following: \begin{align} \lim_{x\rightarrow 0_{+}} f(x) &= \lim_{x\rightarrow 0_{+}} \frac{1}{x}\int_{0}^{x} dy\, f(y)\\ &= \lim_{x\rightarrow 0_{+}} \frac{2}{x}\sum_{n=1}^{\infty} \frac{\sin^2\left(n^2 x/2\right)}{n^3} \end{align} The latter series is smoother and converges more quickly than the original one, so it's better-suited to numerics. If I use $x = 0.0001$ in this series and sum the first $100,000$ terms in Mathematica, I get $0.785393$, whereas $\pi/4 = 0.785398...$ I don't know where to go from there. (I tried the Poisson summation formula to no avail.) Can anyone here prove this conjecture? Or disprove it? Or show that the question is somehow ill-posed?",,['limits']
62,Understanding a particular evaluation of $\prod\limits_{n=2}^{\infty}\left(1-\frac{1}{n^3}\right)$,Understanding a particular evaluation of,\prod\limits_{n=2}^{\infty}\left(1-\frac{1}{n^3}\right),"I'm having a hard time understanding the following evaluation of the infinite product $$ \prod_{n=2}^{\infty}\left(1-\frac{1}{n^3}\right).$$ In particular, I don't understand how you go from line 2 to line 3. Here $\omega = -\frac{1}{2}+ i \frac{\sqrt{3}}{2}$ , which is a primitive third root of unity. $$\begin{align}\prod_{n=2}^{\infty}\left(1-\frac{1}{n^3}\right)  &=\prod_{n=2}^{\infty}\frac{(n-1)(n^2+n+1)}{n^3} \\  &=\lim_{m\to\infty}\frac{1}{m}\prod_{n=2}^m\frac{(n-\omega)(n-\omega^2)}{n^2} \\  &=\lim_{m\to\infty}\frac{\Gamma(m+1-\omega)\Gamma(m+1-\omega^2)}{m(m!)^2\Gamma(-\omega)\Gamma(-\omega^2)(1-\omega)(1-\omega^2)(-\omega)(-\omega^2)} \\  &=\frac{1}{3\Gamma(-\omega)\Gamma(-\omega^2)} \\ &=\frac{\sin{\pi(-\omega)}}{3\pi} \\  &=\frac{\cosh (\frac{\sqrt{3}\pi}{2})}{3\pi } \end{align}$$ EDIT : I think I'm starting to make sense out of this by writing out the terms. $$ \begin{align} \prod_{n=2}^{m} \frac{(n- \omega)(n-\omega^{2})}{n^{2}} &= \frac{(2-\omega)(3-\omega) \cdots (m- \omega)(2-\omega^{2})(3-\omega^{2})\cdots (m- \omega^{2})}{2^{2} \cdot 3^{2} \cdot \cdots  \cdot m^{2}}  \\ &= \frac{1}{(m!)^{2}} \frac{\Gamma(m+1-\omega)}{(1-\omega)(-\omega)\Gamma(-\omega)} \frac{\Gamma(m+1-\omega^{2})}{(1-\omega^{2})(-\omega^{2})\Gamma(-\omega^{2})} \end{align}$$ Now I have to figure out how to take the limit as $m \to \infty$ .","I'm having a hard time understanding the following evaluation of the infinite product In particular, I don't understand how you go from line 2 to line 3. Here , which is a primitive third root of unity. EDIT : I think I'm starting to make sense out of this by writing out the terms. Now I have to figure out how to take the limit as .", \prod_{n=2}^{\infty}\left(1-\frac{1}{n^3}\right). \omega = -\frac{1}{2}+ i \frac{\sqrt{3}}{2} \begin{align}\prod_{n=2}^{\infty}\left(1-\frac{1}{n^3}\right)  &=\prod_{n=2}^{\infty}\frac{(n-1)(n^2+n+1)}{n^3} \\  &=\lim_{m\to\infty}\frac{1}{m}\prod_{n=2}^m\frac{(n-\omega)(n-\omega^2)}{n^2} \\  &=\lim_{m\to\infty}\frac{\Gamma(m+1-\omega)\Gamma(m+1-\omega^2)}{m(m!)^2\Gamma(-\omega)\Gamma(-\omega^2)(1-\omega)(1-\omega^2)(-\omega)(-\omega^2)} \\  &=\frac{1}{3\Gamma(-\omega)\Gamma(-\omega^2)} \\ &=\frac{\sin{\pi(-\omega)}}{3\pi} \\  &=\frac{\cosh (\frac{\sqrt{3}\pi}{2})}{3\pi } \end{align}  \begin{align} \prod_{n=2}^{m} \frac{(n- \omega)(n-\omega^{2})}{n^{2}} &= \frac{(2-\omega)(3-\omega) \cdots (m- \omega)(2-\omega^{2})(3-\omega^{2})\cdots (m- \omega^{2})}{2^{2} \cdot 3^{2} \cdot \cdots  \cdot m^{2}}  \\ &= \frac{1}{(m!)^{2}} \frac{\Gamma(m+1-\omega)}{(1-\omega)(-\omega)\Gamma(-\omega)} \frac{\Gamma(m+1-\omega^{2})}{(1-\omega^{2})(-\omega^{2})\Gamma(-\omega^{2})} \end{align} m \to \infty,"['limits', 'gamma-function', 'infinite-product']"
63,Limit of infinite composition of sin(x),Limit of infinite composition of sin(x),,"I was playing around on desmos the other day, and noticed that $\sin\left(\sin\left(x\right)\right)$ is basically a version of sin with a lower amplitude (which makes intuitive sense). To me, it seems intuitve that this curve, when composed infinite times, becomes a straight line, as the values at $x=\frac{\pi}{2}\mathbb{Z}$ would move (slowly) towards the values at $x=\pi\mathbb{Z}$ by virtue of them moving away from the peaks, but is there a way to go about properly proving this?","I was playing around on desmos the other day, and noticed that is basically a version of sin with a lower amplitude (which makes intuitive sense). To me, it seems intuitve that this curve, when composed infinite times, becomes a straight line, as the values at would move (slowly) towards the values at by virtue of them moving away from the peaks, but is there a way to go about properly proving this?",\sin\left(\sin\left(x\right)\right) x=\frac{\pi}{2}\mathbb{Z} x=\pi\mathbb{Z},"['limits', 'functions']"
64,Proving that $\lim\limits_{x \to 0}\frac{e^x-1}{x} = 1$,Proving that,\lim\limits_{x \to 0}\frac{e^x-1}{x} = 1,"I was messing around with the definition of the derivative, trying to work out the formulas for the common functions using limits. I hit a roadblock, however, while trying to find the derivative of $e^x$. The process went something like this: $$\begin{align} (e^x)' &= \lim_{h \to 0} \frac{e^{x+h}-e^x}{h} \\ &= \lim_{h \to 0} \frac{e^xe^h-e^x}{h} \\ &= \lim_{h \to 0} e^x\frac{e^{h}-1}{h} \\ &= e^x \lim_{h \to 0}\frac{e^h-1}{h} \end{align} $$ I can show that $\lim_{h\to 0} \frac{e^h-1}{h} = 1$ using L'Hôpital's, but it kind of defeats the purpose of working out the derivative, so I want to prove it in some other way. I've been trying, but I can't work anything out. Could someone give a hint?","I was messing around with the definition of the derivative, trying to work out the formulas for the common functions using limits. I hit a roadblock, however, while trying to find the derivative of $e^x$. The process went something like this: $$\begin{align} (e^x)' &= \lim_{h \to 0} \frac{e^{x+h}-e^x}{h} \\ &= \lim_{h \to 0} \frac{e^xe^h-e^x}{h} \\ &= \lim_{h \to 0} e^x\frac{e^{h}-1}{h} \\ &= e^x \lim_{h \to 0}\frac{e^h-1}{h} \end{align} $$ I can show that $\lim_{h\to 0} \frac{e^h-1}{h} = 1$ using L'Hôpital's, but it kind of defeats the purpose of working out the derivative, so I want to prove it in some other way. I've been trying, but I can't work anything out. Could someone give a hint?",,"['limits', 'derivatives', 'exponential-function']"
65,When can you interchange composition of a limit?,When can you interchange composition of a limit?,,"I've seen some limit problems where you can do this: $$ \lim_{x \to \infty} \exp\left({g(x)}\right) = \exp \left( \lim_{x \to \infty }g(x)\right) . $$ So, I've tried to generalize the result as: $$ \lim_{x \to \infty} f(g(x)) = f \left( \lim_{x \to \infty }g(x) \right) $$ and I was just wondering if the result above is actually true, and on what conditions? Perhaps someone can point me to a theorem.","I've seen some limit problems where you can do this: $$ \lim_{x \to \infty} \exp\left({g(x)}\right) = \exp \left( \lim_{x \to \infty }g(x)\right) . $$ So, I've tried to generalize the result as: $$ \lim_{x \to \infty} f(g(x)) = f \left( \lim_{x \to \infty }g(x) \right) $$ and I was just wondering if the result above is actually true, and on what conditions? Perhaps someone can point me to a theorem.",,"['limits', 'continuity']"
66,limit inferior and superior for sets vs real numbers,limit inferior and superior for sets vs real numbers,,"I am looking for an intuitive explanation of $\liminf$ and $\limsup$ for sequence of sets and how it corresponds to $\liminf$ and $\limsup$ for sets of real numbers. I researched online but cannot find a good comparison. Any link, reference or answer very much appreciated. For example, what is $\liminf$ and $\limsup$ of real number sequences $a_n=(-1)^n$ and $b_n=1/n$. Corresponding to this, what is the $\liminf$ and $\limsup$ of sequence of sets $A_n=\{(−1)^n\}$ and $B_n=\{1/n\}$?","I am looking for an intuitive explanation of $\liminf$ and $\limsup$ for sequence of sets and how it corresponds to $\liminf$ and $\limsup$ for sets of real numbers. I researched online but cannot find a good comparison. Any link, reference or answer very much appreciated. For example, what is $\liminf$ and $\limsup$ of real number sequences $a_n=(-1)^n$ and $b_n=1/n$. Corresponding to this, what is the $\liminf$ and $\limsup$ of sequence of sets $A_n=\{(−1)^n\}$ and $B_n=\{1/n\}$?",,"['measure-theory', 'elementary-set-theory', 'limits', 'limsup-and-liminf']"
67,A Gamma limit $\lim_{n\rightarrow+\infty}\sum_{k=1}^n \left( \Gamma\bigl(\frac{k}{n}\bigr)\right)^{-k}=\frac{e^\gamma}{e^\gamma-1}$,A Gamma limit,\lim_{n\rightarrow+\infty}\sum_{k=1}^n \left( \Gamma\bigl(\frac{k}{n}\bigr)\right)^{-k}=\frac{e^\gamma}{e^\gamma-1},Show that $$\lim_{n\rightarrow+\infty}\sum_{k=1}^n \displaystyle \left( \Gamma\bigl(\frac{k}{n}\bigr)\right)^{-k}=\frac{e^\gamma}{e^\gamma-1}$$ where $\gamma$ is the Euler-Mascheroni Constant . Motivation : One can show that $$\lim_{n\rightarrow+\infty}\displaystyle\left(n-\Gamma\bigl(\frac{1}{n}\bigr)\right)=\gamma.$$ This means that $\Gamma\bigl(\frac{1}{n}\bigr)\sim n$ when $n$ is large. So we have that (even if is not correct) $\Gamma\bigl(\frac{k}{n}\bigr)\sim \frac{n}{k}$ . It implies that $$\sum_{k=1}^{n}\displaystyle\left(\Gamma\bigl(\frac{k}{n}\bigr)\right)^{-k}\sim \sum_{k=1}^{n}(\frac{k}{n})^k.$$ Since the limit of the right sum exists and its value is $\frac{e}{e-1}$ . Numerical calculations show that the limit of the sum involving the Gamma function would be $\frac{e^\gamma}{e^\gamma− 1}$ .,Show that where is the Euler-Mascheroni Constant . Motivation : One can show that This means that when is large. So we have that (even if is not correct) . It implies that Since the limit of the right sum exists and its value is . Numerical calculations show that the limit of the sum involving the Gamma function would be .,\lim_{n\rightarrow+\infty}\sum_{k=1}^n \displaystyle \left( \Gamma\bigl(\frac{k}{n}\bigr)\right)^{-k}=\frac{e^\gamma}{e^\gamma-1} \gamma \lim_{n\rightarrow+\infty}\displaystyle\left(n-\Gamma\bigl(\frac{1}{n}\bigr)\right)=\gamma. \Gamma\bigl(\frac{1}{n}\bigr)\sim n n \Gamma\bigl(\frac{k}{n}\bigr)\sim \frac{n}{k} \sum_{k=1}^{n}\displaystyle\left(\Gamma\bigl(\frac{k}{n}\bigr)\right)^{-k}\sim \sum_{k=1}^{n}(\frac{k}{n})^k. \frac{e}{e-1} \frac{e^\gamma}{e^\gamma− 1},"['limits', 'summation']"
68,Limit of $\log (\log( ... \log((n)^ {(n-1)^ {....}})))$,Limit of,\log (\log( ... \log((n)^ {(n-1)^ {....}}))),"This is a spinoff of this question Defining $$f_0(x) = x$$ $$f_n(x) = \log(f_{(n-1)} (x)) \space (\forall n>0)$$ and $$a_0 = 1$$ $$a_{n+1} = (n+1)^{a_n} \space (\forall n>0)$$ How to calculate $$\lim_{n \to \infty } f_n(a_n) $$ (an ""experiment"" here , but (beware) I think WolframAlpha is using an approximate representation in powers of 10) Edit A table with the first few values (made with aid of hypercalc , as per Gottfried's suggestion in comments) $$\begin{array}{cc}  n & f_n(a_n) \\  0 & 1. \\  1 & 0. \\  2 & -0.366513 \\  3 & -0.239279 \\  4 & -0.0771089 \\  5 & -0.06133128660211943 \\  6 & -0.06133124230008346 \\  7 & -0.06133124230008346 \\  8 & -0.06133124230008346 \end{array}$$","This is a spinoff of this question Defining $$f_0(x) = x$$ $$f_n(x) = \log(f_{(n-1)} (x)) \space (\forall n>0)$$ and $$a_0 = 1$$ $$a_{n+1} = (n+1)^{a_n} \space (\forall n>0)$$ How to calculate $$\lim_{n \to \infty } f_n(a_n) $$ (an ""experiment"" here , but (beware) I think WolframAlpha is using an approximate representation in powers of 10) Edit A table with the first few values (made with aid of hypercalc , as per Gottfried's suggestion in comments) $$\begin{array}{cc}  n & f_n(a_n) \\  0 & 1. \\  1 & 0. \\  2 & -0.366513 \\  3 & -0.239279 \\  4 & -0.0771089 \\  5 & -0.06133128660211943 \\  6 & -0.06133124230008346 \\  7 & -0.06133124230008346 \\  8 & -0.06133124230008346 \end{array}$$",,['limits']
69,Infinite powering by $i$ [duplicate],Infinite powering by  [duplicate],i,This question already has answers here : Complex towers: $i^{i^{i^{...}}}$ (2 answers) Closed 11 years ago . Find the value of: $i^{i^{i^{i^{i^{i^{....\infty}}}}}}$ Simply infinite powering by i's and the limiting value. Thank you for the help.,This question already has answers here : Complex towers: $i^{i^{i^{...}}}$ (2 answers) Closed 11 years ago . Find the value of: $i^{i^{i^{i^{i^{i^{....\infty}}}}}}$ Simply infinite powering by i's and the limiting value. Thank you for the help.,,"['limits', 'complex-numbers', 'exponentiation', 'tetration']"
70,How to find $\lim_{n\to\infty}\frac{1!+2!+\cdots+n!}{n!}$?,How to find ?,\lim_{n\to\infty}\frac{1!+2!+\cdots+n!}{n!},"How to evaluate the following limit? $$\lim_{n\to\infty}\dfrac{1!+2!+\cdots+n!}{n!}$$ For this problem I have two methods. But I'd like to know if there are better methods. My solution 1: Using Stolz-Cesaro Theorem, we have $$\lim_{n\to\infty}\dfrac{1!+2!+\cdots+n!}{n!}=\lim_{n\to\infty}\dfrac{n!}{n!-(n-1)!}=\lim_{n\to\infty}\dfrac{n}{n-1}=1$$ My solution 2: $$1=\dfrac{n!}{n!}<\dfrac{1!+2!+\cdots+n!}{n!}<\dfrac{(n-2)(n-2)!+(n-1)!+n!}{n!}=\dfrac{n-2}{n(n-1)}+\dfrac{1}{n}+1$$","How to evaluate the following limit? $$\lim_{n\to\infty}\dfrac{1!+2!+\cdots+n!}{n!}$$ For this problem I have two methods. But I'd like to know if there are better methods. My solution 1: Using Stolz-Cesaro Theorem, we have $$\lim_{n\to\infty}\dfrac{1!+2!+\cdots+n!}{n!}=\lim_{n\to\infty}\dfrac{n!}{n!-(n-1)!}=\lim_{n\to\infty}\dfrac{n}{n-1}=1$$ My solution 2: $$1=\dfrac{n!}{n!}<\dfrac{1!+2!+\cdots+n!}{n!}<\dfrac{(n-2)(n-2)!+(n-1)!+n!}{n!}=\dfrac{n-2}{n(n-1)}+\dfrac{1}{n}+1$$",,"['limits', 'factorial']"
71,Formulae of the Year 2016,Formulae of the Year 2016,,Decode the following limits to welcome the new year! This is my love limits (Created by me). I hope you Love it. Let $$A_{n}=\dfrac{n}{n^2+1}+\dfrac{n}{n^2+2^2}+\cdots+\dfrac{n}{n^2+n^2}$$ show that  $$\lim_{n\to\infty}\dfrac{1}{n^4\left\{\dfrac{1}{24}-n\left[n\left(\dfrac{\pi}{4}-A_{n}\right)-\dfrac{1}{4}\right]\right\}}=2016$$ can you create some nice other problem (result is 2016)? Happy New  Year To  Everyone .,Decode the following limits to welcome the new year! This is my love limits (Created by me). I hope you Love it. Let $$A_{n}=\dfrac{n}{n^2+1}+\dfrac{n}{n^2+2^2}+\cdots+\dfrac{n}{n^2+n^2}$$ show that  $$\lim_{n\to\infty}\dfrac{1}{n^4\left\{\dfrac{1}{24}-n\left[n\left(\dfrac{\pi}{4}-A_{n}\right)-\dfrac{1}{4}\right]\right\}}=2016$$ can you create some nice other problem (result is 2016)? Happy New  Year To  Everyone .,,['limits']
72,"Why does ${x}^{x^{x^{x^{\,.^{\,.^{\,.}}}}}}$ bifurcate below $\sim0.065$?",Why does  bifurcate below ?,"{x}^{x^{x^{x^{\,.^{\,.^{\,.}}}}}} \sim0.065","When you calculate what ${x}^{x^{x^{x\cdots }}}$ converges to between $0$ and $1$ , before approximately $0.065$ the graph bifurcates. Why does this happen and is there a reason for it happens at that number?","When you calculate what converges to between and , before approximately the graph bifurcates. Why does this happen and is there a reason for it happens at that number?",{x}^{x^{x^{x\cdots }}} 0 1 0.065,"['limits', 'tetration', 'bifurcation']"
73,Is some thing wrong with the epsilon-delta definition of limit??,Is some thing wrong with the epsilon-delta definition of limit??,,"In the epsilon-delta definition of limit which is: For all $\epsilon>0$ there exists a $\delta>0$ such that, whenever $|x-a|<\delta$ then $|f(x)-L|<\epsilon$ . Now since $\epsilon$ is given and it should be proven that there is some $\delta$ for the given $\epsilon$, then I think the rest of the definition should have been: $|f(x)-L|<\epsilon \implies |x-a|<\delta$ . Please tell me whether I'm right or not.","In the epsilon-delta definition of limit which is: For all $\epsilon>0$ there exists a $\delta>0$ such that, whenever $|x-a|<\delta$ then $|f(x)-L|<\epsilon$ . Now since $\epsilon$ is given and it should be proven that there is some $\delta$ for the given $\epsilon$, then I think the rest of the definition should have been: $|f(x)-L|<\epsilon \implies |x-a|<\delta$ . Please tell me whether I'm right or not.",,"['limits', 'continuity', 'definition', 'epsilon-delta']"
74,small o(1) notation,small o(1) notation,,"It's probably a vey silly question, but I'm confused. Does o(1) simply mean $\lim_{n \to \infty} \frac{f(n)}{\epsilon}=0$ for some $n>N$?","It's probably a vey silly question, but I'm confused. Does o(1) simply mean $\lim_{n \to \infty} \frac{f(n)}{\epsilon}=0$ for some $n>N$?",,"['analysis', 'limits']"
75,"Another interesting property of $y=2^{n-1}\prod_{k=0}^n \left(x-\cos{\frac{k\pi}{n}}\right)$: product of arc lengths converges, but to what?","Another interesting property of : product of arc lengths converges, but to what?",y=2^{n-1}\prod_{k=0}^n \left(x-\cos{\frac{k\pi}{n}}\right),"Here is the curve $y=2^{n-1}\prod\limits_{k=0}^n \left(x-\cos{\frac{k\pi}{n}}\right)$ , shown with example $n=8$ , together with the unit circle centred at the origin. Call the arc lengths between neighboring roots $l_1, l_2, l_3, ..., l_n$ . What is the exact value of $L=\lim\limits_{n\to\infty}\prod\limits_{k=1}^n l_k$ ? Desmos suggests that $L$ exists and is approximately $2.94$ . Maybe $\frac{8}{e}$ ? Context I have studied this curve, and found that it has several interesting properties. The curve is tangent to the unit circle at $n$ points, which are uniformly spaced around the circle. The magnitude of the gradient at each root inside the circle is $n$ ; the magnitude of the gradient at $x=\pm1$ is $2n$ . The total area of the regions enclosed by the curve and the x -axis is $1$ . As $n\to\infty$ , the volume of revolution of those regions about the x -axis approaches $\frac{1}{2}$ of the volume of the unit sphere, and the volume of revolution of those regions about the y -axis approaches $\frac{1}{\pi}$ of the volume of the unit sphere. As $n\to\infty$ , if the curve is magnified so that the average area of those regions is always $2$ , then the product of those areas approaches $4\cosh^2{\left(\frac{\sqrt{\pi^2-8}}{2}\right)}\approx6.18$ , as shown here . I recently discovered that the product of arc lengths between neighboring roots seems to converge to a positive number as $n\to\infty$ . Hence, my question. (If you know any other interesting properties of this curve, feel free to add them in the comments.) My attempt The part of the curve inside the circle can be expressed as $y=-\sqrt{1-x^2}\sin{(n\arccos{x})}$ . So $$L=\lim\limits_{n\to\infty}\prod\limits_{k=1}^n \int_{\cos{\frac{k\pi}{n}}}^{\cos{\frac{(k-1)\pi}{n}}}\sqrt{1+\left(n\cos{(n\arccos{x})+\frac{x\sin{(n\arccos{x})}}{\sqrt{1-x^2}}}\right)^2}dx$$ I do not know how to evaluate this limit. I tried taking the log of the product, without success. I tried to approximate each integral as areas of triangles (hoping that that approximation would become equality with the limit) and a rectangle at the bottom, multiplying each triangle's area by $\frac{4}{\pi}$ (which is the ratio of areas under sine or cosine to the area of an inscribed triangle), but that resulted in a different limit. EDIT Further numerical analysis strongly suggests that $L=\frac{8}{e}$ . I noticed that when $n$ doubles, the ratio of the two products is a certain number (which is close to $1$ ), and when $n$ is doubled again, the ratio's distance to $1$ is approximately halved. So then I projected that the product indeed approaches $\frac{8}{e}$ . (I don't have Mathematica; anyone who has it is welcome to confirm this.) I have simplified the expression of $L$ . Letting $x=\cos{\frac{u}{n}}$ , and ignoring the $1$ in the $\sqrt{1+(...)^2}$ (I think this is OK since $n\to\infty$ ), we get $$L=\lim\limits_{n\to\infty}\prod\limits_{k=1}^n \int_{k\pi}^{(k-1)\pi}\sqrt{\left(n\cos{u}+(\sin{u})\cot{\frac{u}{n}}\right)^2}\left(-\frac{1}{n}\sin{\frac{u}{n}}\right)du$$ $$\space{}=\lim\limits_{n\to\infty}\prod\limits_{k=1}^n \int_{(k-1)\pi}^{k\pi}\left|(\cos{u})\sin{\frac{u}{n}}+\frac{1}{n}(\sin{u})\cos{\frac{u}{n}}\right|du$$ So why is this equal to $\frac{8}{e}$ ?","Here is the curve , shown with example , together with the unit circle centred at the origin. Call the arc lengths between neighboring roots . What is the exact value of ? Desmos suggests that exists and is approximately . Maybe ? Context I have studied this curve, and found that it has several interesting properties. The curve is tangent to the unit circle at points, which are uniformly spaced around the circle. The magnitude of the gradient at each root inside the circle is ; the magnitude of the gradient at is . The total area of the regions enclosed by the curve and the x -axis is . As , the volume of revolution of those regions about the x -axis approaches of the volume of the unit sphere, and the volume of revolution of those regions about the y -axis approaches of the volume of the unit sphere. As , if the curve is magnified so that the average area of those regions is always , then the product of those areas approaches , as shown here . I recently discovered that the product of arc lengths between neighboring roots seems to converge to a positive number as . Hence, my question. (If you know any other interesting properties of this curve, feel free to add them in the comments.) My attempt The part of the curve inside the circle can be expressed as . So I do not know how to evaluate this limit. I tried taking the log of the product, without success. I tried to approximate each integral as areas of triangles (hoping that that approximation would become equality with the limit) and a rectangle at the bottom, multiplying each triangle's area by (which is the ratio of areas under sine or cosine to the area of an inscribed triangle), but that resulted in a different limit. EDIT Further numerical analysis strongly suggests that . I noticed that when doubles, the ratio of the two products is a certain number (which is close to ), and when is doubled again, the ratio's distance to is approximately halved. So then I projected that the product indeed approaches . (I don't have Mathematica; anyone who has it is welcome to confirm this.) I have simplified the expression of . Letting , and ignoring the in the (I think this is OK since ), we get So why is this equal to ?","y=2^{n-1}\prod\limits_{k=0}^n \left(x-\cos{\frac{k\pi}{n}}\right) n=8 l_1, l_2, l_3, ..., l_n L=\lim\limits_{n\to\infty}\prod\limits_{k=1}^n l_k L 2.94 \frac{8}{e} n n x=\pm1 2n 1 n\to\infty \frac{1}{2} \frac{1}{\pi} n\to\infty 2 4\cosh^2{\left(\frac{\sqrt{\pi^2-8}}{2}\right)}\approx6.18 n\to\infty y=-\sqrt{1-x^2}\sin{(n\arccos{x})} L=\lim\limits_{n\to\infty}\prod\limits_{k=1}^n \int_{\cos{\frac{k\pi}{n}}}^{\cos{\frac{(k-1)\pi}{n}}}\sqrt{1+\left(n\cos{(n\arccos{x})+\frac{x\sin{(n\arccos{x})}}{\sqrt{1-x^2}}}\right)^2}dx \frac{4}{\pi} L=\frac{8}{e} n 1 n 1 \frac{8}{e} L x=\cos{\frac{u}{n}} 1 \sqrt{1+(...)^2} n\to\infty L=\lim\limits_{n\to\infty}\prod\limits_{k=1}^n \int_{k\pi}^{(k-1)\pi}\sqrt{\left(n\cos{u}+(\sin{u})\cot{\frac{u}{n}}\right)^2}\left(-\frac{1}{n}\sin{\frac{u}{n}}\right)du \space{}=\lim\limits_{n\to\infty}\prod\limits_{k=1}^n \int_{(k-1)\pi}^{k\pi}\left|(\cos{u})\sin{\frac{u}{n}}+\frac{1}{n}(\sin{u})\cos{\frac{u}{n}}\right|du \frac{8}{e}","['limits', 'polynomials', 'circles', 'infinite-product', 'arc-length']"
76,Is WolframAlpha wrong? or am I?,Is WolframAlpha wrong? or am I?,,"Problem : Determine a limit $$\lim_{x\to\infty}xe^{\sin x}$$ exists or not. If it exists, find a limit. Since $-1\le \sin x \le 1$ , I can say that $\displaystyle0<\frac{1}{e}\le e^{\sin x}\le e$ . Multiply $x$ both side, $\displaystyle0<\frac{x}{e}\le xe^{\sin x}\le ex$ Since $\displaystyle\lim_{x\to\infty} \frac{x}{e}=\infty$ , I think $\displaystyle\lim_{x\to\infty}xe^{\sin x}=\infty$ also. But, WA says it is indeterminate form : Am I correct? or Am I wrong? If I'm wrong, where did I make a mistake?","Problem : Determine a limit exists or not. If it exists, find a limit. Since , I can say that . Multiply both side, Since , I think also. But, WA says it is indeterminate form : Am I correct? or Am I wrong? If I'm wrong, where did I make a mistake?",\lim_{x\to\infty}xe^{\sin x} -1\le \sin x \le 1 \displaystyle0<\frac{1}{e}\le e^{\sin x}\le e x \displaystyle0<\frac{x}{e}\le xe^{\sin x}\le ex \displaystyle\lim_{x\to\infty} \frac{x}{e}=\infty \displaystyle\lim_{x\to\infty}xe^{\sin x}=\infty,"['limits', 'wolfram-alpha']"
77,Asymptotic behaviour of sum,Asymptotic behaviour of sum,,"I would like to evaluate the number $c$ given by $$ c = \lim_{m\to\infty} \frac{1}{\log m}\sum_{n=1}^m \frac{1}{n^2 \sin^2(\pi n \tau)} $$ where $\tau = (1+\sqrt{5})/2$. My attempt: my guess was this sum would be dominated by the terms for which $n$ is a Fibonacci number. I considered the sum of this sub series using the relation $F_a \tau = F_{a+1}-(-\tau)^{-a}$, the small angle approximation, and that $F_a\tau^{-a} = 1/\sqrt{5}+\mathrm{O(\tau^{-2a})}$. This yielded $$ \sum_{n=1}^m \frac{1}{n^2 \sin^2(\pi n \tau)} \approx \sum_{a=1}^{\log_\tau m \sqrt{5}} \frac{1}{F_a^2 \sin^2(\pi \tau^{-a})} \approx \sum_{a=1}^{\log_\tau m \sqrt{5}} \frac{5}{\pi^2} = \frac{5}{\pi^2} \frac{\log m \sqrt{5}}{\log \tau} $$ this would imply $c = 5/(\pi^2 \log \tau)$, however this seems about a factor $1.2$ smaller than the correct value, implying I have missed some important terms. Is it possible to obtain an exact expression? Or at least a better a tighter lower (and upper) bound?","I would like to evaluate the number $c$ given by $$ c = \lim_{m\to\infty} \frac{1}{\log m}\sum_{n=1}^m \frac{1}{n^2 \sin^2(\pi n \tau)} $$ where $\tau = (1+\sqrt{5})/2$. My attempt: my guess was this sum would be dominated by the terms for which $n$ is a Fibonacci number. I considered the sum of this sub series using the relation $F_a \tau = F_{a+1}-(-\tau)^{-a}$, the small angle approximation, and that $F_a\tau^{-a} = 1/\sqrt{5}+\mathrm{O(\tau^{-2a})}$. This yielded $$ \sum_{n=1}^m \frac{1}{n^2 \sin^2(\pi n \tau)} \approx \sum_{a=1}^{\log_\tau m \sqrt{5}} \frac{1}{F_a^2 \sin^2(\pi \tau^{-a})} \approx \sum_{a=1}^{\log_\tau m \sqrt{5}} \frac{5}{\pi^2} = \frac{5}{\pi^2} \frac{\log m \sqrt{5}}{\log \tau} $$ this would imply $c = 5/(\pi^2 \log \tau)$, however this seems about a factor $1.2$ smaller than the correct value, implying I have missed some important terms. Is it possible to obtain an exact expression? Or at least a better a tighter lower (and upper) bound?",,"['limits', 'trigonometry', 'summation', 'fibonacci-numbers', 'trigonometric-series']"
78,Prove that $\lfloor0.999\dots\rfloor= ?$ $0$ or $1$?,Prove that   or ?,\lfloor0.999\dots\rfloor= ? 0 1,"I think $\lfloor0.999\dots\rfloor= 1$, as $0.999\dots=1$,but I have doubt, as $\lfloor0.9\rfloor=0$,$\lfloor0.99\rfloor=0$,$\lfloor0.9999999\rfloor=0$, etc.","I think $\lfloor0.999\dots\rfloor= 1$, as $0.999\dots=1$,but I have doubt, as $\lfloor0.9\rfloor=0$,$\lfloor0.99\rfloor=0$,$\lfloor0.9999999\rfloor=0$, etc.",,"['limits', 'decimal-expansion']"
79,Limit of Zeta function,Limit of Zeta function,,I'm looking for a reference for (or an elementary proof of) $$ \lim_{s \rightarrow 1} \left( \zeta(s) - \frac{1}{s-1} \right) = \gamma$$ Thanks for your help.,I'm looking for a reference for (or an elementary proof of) $$ \lim_{s \rightarrow 1} \left( \zeta(s) - \frac{1}{s-1} \right) = \gamma$$ Thanks for your help.,,"['limits', 'special-functions', 'riemann-zeta', 'euler-mascheroni-constant']"
80,Speed of convergence of Riemann sums,Speed of convergence of Riemann sums,,"This question is inspired by a previous question . It was shown that, for all function $f \in \mathcal{C} ([0, 1])$, $$ \lim_{n \to + \infty} \sum_{k=0}^{n} f \left( \frac{k}{n+1} \right) - \sum_{k=0}^{n-1} f \left( \frac{k}{n} \right) = \int_0^1 f (x) \ dx.$$ A stronger statement would be that there exists some constant $a(f)$ such that: $$\sum_{k=0}^{n-1} f \left( \frac{k}{n} \right) = n \int_0^1 f (x) \ dx + a(f) + o(1),$$ or, in other words, that there is an asymptotic development at order $1$ of the Riemann sums: $$\frac{1}{n} \sum_{k=0}^{n-1} f \left( \frac{k}{n} \right) = \int_0^1 f (x) \ dx + \frac{a(f)}{n} + o(n^{-1}).$$ Given $f$, can we always find such a constant $a(f)$? If this is false, can we find a counter-example? If this is true, can $a(f)$ be written explicitely? I have had a quick look at the litterature, but most asymptotics for the Riemann sums involve different meshes, which depend on the function $f$.","This question is inspired by a previous question . It was shown that, for all function $f \in \mathcal{C} ([0, 1])$, $$ \lim_{n \to + \infty} \sum_{k=0}^{n} f \left( \frac{k}{n+1} \right) - \sum_{k=0}^{n-1} f \left( \frac{k}{n} \right) = \int_0^1 f (x) \ dx.$$ A stronger statement would be that there exists some constant $a(f)$ such that: $$\sum_{k=0}^{n-1} f \left( \frac{k}{n} \right) = n \int_0^1 f (x) \ dx + a(f) + o(1),$$ or, in other words, that there is an asymptotic development at order $1$ of the Riemann sums: $$\frac{1}{n} \sum_{k=0}^{n-1} f \left( \frac{k}{n} \right) = \int_0^1 f (x) \ dx + \frac{a(f)}{n} + o(n^{-1}).$$ Given $f$, can we always find such a constant $a(f)$? If this is false, can we find a counter-example? If this is true, can $a(f)$ be written explicitely? I have had a quick look at the litterature, but most asymptotics for the Riemann sums involve different meshes, which depend on the function $f$.",,"['asymptotics', 'riemann-sum']"
81,Why won't a series converge if the limit of the sequence is 0?,Why won't a series converge if the limit of the sequence is 0?,,"Just thinking about it in terms of logic, shouldn't the series of a sequence whose limit as $n$ approaches infinity is 0 converge? I know that the $n$th term test for divergence says that if a series is convergent, then the limit of its sequence is 0 and I also know there are some sequences for which it has been ""proven"" that their series does not converge even though the sequence converges to 0, but I just don't believe these tests. If we stretch $n$ out to infinity and the terms are approaching 0, then how is it possible for the sum of the terms to ""overflow"" and diverge if the terms are becoming negligibly small?","Just thinking about it in terms of logic, shouldn't the series of a sequence whose limit as $n$ approaches infinity is 0 converge? I know that the $n$th term test for divergence says that if a series is convergent, then the limit of its sequence is 0 and I also know there are some sequences for which it has been ""proven"" that their series does not converge even though the sequence converges to 0, but I just don't believe these tests. If we stretch $n$ out to infinity and the terms are approaching 0, then how is it possible for the sum of the terms to ""overflow"" and diverge if the terms are becoming negligibly small?",,"['limits', 'convergence-divergence', 'divergent-series']"
82,Why isn't $\lim \limits_{x\to\infty}\left(1+\frac{1}{x}\right)^{x}$ equal to $1$?,Why isn't  equal to ?,\lim \limits_{x\to\infty}\left(1+\frac{1}{x}\right)^{x} 1,"Given $\lim \limits_{x\to\infty}(1+\frac{1}{x})^{x}$, why can't you reduce it to $\lim \limits_{x\to\infty}(1+0)^{x}$, making the result ""$1$""?  Obviously, it's wrong, as the true value is $e$.  Is it because the $\frac{1}{x}$ is still something even though it's really small?  Then why is $$\lim_{x\to\infty}\left(\frac{1}{x}\right) = 0\text{?}$$ What is the proper way of calculating the limit in this case?","Given $\lim \limits_{x\to\infty}(1+\frac{1}{x})^{x}$, why can't you reduce it to $\lim \limits_{x\to\infty}(1+0)^{x}$, making the result ""$1$""?  Obviously, it's wrong, as the true value is $e$.  Is it because the $\frac{1}{x}$ is still something even though it's really small?  Then why is $$\lim_{x\to\infty}\left(\frac{1}{x}\right) = 0\text{?}$$ What is the proper way of calculating the limit in this case?",,"['limits', 'exponential-function']"
83,"Two trains move towards each other, a bird moves between them. How many trips can the bird make? [duplicate]","Two trains move towards each other, a bird moves between them. How many trips can the bird make? [duplicate]",,"This question already has answers here : 'Bee flying between two trains' problem (9 answers) Closed 4 years ago . The problem statement: Two trains move towards each other at a speed of $34\ km/h$ in the same rectilinear road. A certain bird can fly at a speed of $58\ km/h$ and starts flying from the front of one of the trains to the other, when they're $102\ km$ apart. When the bird reaches the front of the other train, it starts flying back to the first train, and so on. How many of these trips can the bird make before the two trains meet? What is the total distance the bird travels? Commentary: The second question of the problem seems relatively simple, since one only has to notice that the trains will take 1.5 hours to meet, therefore, the bird travels $58\cdot1.5=87 km$ . However, the first question baffles me. How can one calculate how many trips the bird makes? If I'm correct, in order to obtain the time the bird will take to make its first trip, we have to add the bird's speed and the speed at which the distance of the trains is being reduced ( $68\ km/h$ ). This means the bird will take $\frac{102}{126}\approx0.809$ hours to finish the first trip, and the trains will be $\frac{986}{21}\approx 46.95\ km$ apart. If I continue this way (now finding how long will the bird take to travel those 46.95 km), it seems that I'll never stop or that at least it will take a huge amount of trips that cannot be computed by hand. Is there a way to find a 'quick' answer to this problem? Am I making it more complicated than it actually is? Thanks in advance!","This question already has answers here : 'Bee flying between two trains' problem (9 answers) Closed 4 years ago . The problem statement: Two trains move towards each other at a speed of in the same rectilinear road. A certain bird can fly at a speed of and starts flying from the front of one of the trains to the other, when they're apart. When the bird reaches the front of the other train, it starts flying back to the first train, and so on. How many of these trips can the bird make before the two trains meet? What is the total distance the bird travels? Commentary: The second question of the problem seems relatively simple, since one only has to notice that the trains will take 1.5 hours to meet, therefore, the bird travels . However, the first question baffles me. How can one calculate how many trips the bird makes? If I'm correct, in order to obtain the time the bird will take to make its first trip, we have to add the bird's speed and the speed at which the distance of the trains is being reduced ( ). This means the bird will take hours to finish the first trip, and the trains will be apart. If I continue this way (now finding how long will the bird take to travel those 46.95 km), it seems that I'll never stop or that at least it will take a huge amount of trips that cannot be computed by hand. Is there a way to find a 'quick' answer to this problem? Am I making it more complicated than it actually is? Thanks in advance!",34\ km/h 58\ km/h 102\ km 58\cdot1.5=87 km 68\ km/h \frac{102}{126}\approx0.809 \frac{986}{21}\approx 46.95\ km,"['limits', 'arithmetic', 'puzzle']"
84,The limit of the alternating series $x - x^2 + x^4 - x^8 + {x^{16}}-\dotsb$ as $x \to 1$,The limit of the alternating series  as,x - x^2 + x^4 - x^8 + {x^{16}}-\dotsb x \to 1,I want to calculate the limit of this sum : $$\lim\limits_{x \to 1} {\left(x - x^2 + x^4 - x^8 + {x^{16}}-\dotsb\right)}$$ My efforts to solve the problem are described in the self-answer below .,I want to calculate the limit of this sum : $$\lim\limits_{x \to 1} {\left(x - x^2 + x^4 - x^8 + {x^{16}}-\dotsb\right)}$$ My efforts to solve the problem are described in the self-answer below .,,['limits']
85,The limit of $\frac{n+\sqrt{n}+\cdots+\sqrt[n]{n}}{n}$,The limit of,\frac{n+\sqrt{n}+\cdots+\sqrt[n]{n}}{n},How do I compute the following limit or show it doesn't exists? $$\lim_{n\rightarrow\infty}\frac{n+\sqrt{n}+\cdots+\sqrt[n]{n}}{n}$$ I've struggled with this problem for a while now so I would appreciate a complete solution.,How do I compute the following limit or show it doesn't exists? $$\lim_{n\rightarrow\infty}\frac{n+\sqrt{n}+\cdots+\sqrt[n]{n}}{n}$$ I've struggled with this problem for a while now so I would appreciate a complete solution.,,['limits']
86,Definition of the nth derivative? [First post],Definition of the nth derivative? [First post],,If the definition of the derivative is  $$ f^\prime(x) = \lim_{\Delta x \to 0} \dfrac{f(x+\Delta x) - f(x)}{\Delta x} $$ Would it make sense that the nth derivative would be (I know that the 'n' in delta x to the nth power is useless)  $$ f^{(n)}(x)=\lim_{\Delta x \to 0} \sum_{k=0}^{n}(-1)^k{n \choose k}\dfrac{f(x+\Delta x(n-k))}{\Delta x^n} $$ I came to this conclusion using this method $$ f^\prime(x) = \lim_{\Delta x \to 0} \dfrac{f(x+\Delta x) - f(x)}{\Delta x} $$ (this is correct right?) $$ f^{\prime\prime}(x) = \lim_{\Delta x \to 0} \dfrac{f^\prime(x+\Delta x) - f^\prime(x)}{\Delta x}=$$ $$\lim_{\Delta x \to 0}\dfrac{\dfrac{f((x+\Delta x)+\Delta x)-f(x+\Delta x)}{\Delta x}-\dfrac{f(x+\Delta x)-f(x)}{\Delta x}}{\Delta x}=$$ $$\lim_{\Delta x \to 0}\dfrac{f(x+2\Delta x)-2f(x+\Delta x)+f(x)}{\Delta x^2} $$ After following this method a couple of times(I think I used it to the 5th derivative) I noticed the pattern of $$(a-b)^n$$ And that is how i arrived at  $$ f^{(n)}(x)=\lim_{\Delta x \to 0} \sum_{k=0}^{n}(-1)^k{n \choose k}\dfrac{f(x+\Delta x(n-k))}{\Delta x^n} $$ Have I made a fatal error somewhere or does this definition actually follow through? Thanks for your time I really appreciate it. P.S. Any input on using tags will be appreciated.,If the definition of the derivative is  $$ f^\prime(x) = \lim_{\Delta x \to 0} \dfrac{f(x+\Delta x) - f(x)}{\Delta x} $$ Would it make sense that the nth derivative would be (I know that the 'n' in delta x to the nth power is useless)  $$ f^{(n)}(x)=\lim_{\Delta x \to 0} \sum_{k=0}^{n}(-1)^k{n \choose k}\dfrac{f(x+\Delta x(n-k))}{\Delta x^n} $$ I came to this conclusion using this method $$ f^\prime(x) = \lim_{\Delta x \to 0} \dfrac{f(x+\Delta x) - f(x)}{\Delta x} $$ (this is correct right?) $$ f^{\prime\prime}(x) = \lim_{\Delta x \to 0} \dfrac{f^\prime(x+\Delta x) - f^\prime(x)}{\Delta x}=$$ $$\lim_{\Delta x \to 0}\dfrac{\dfrac{f((x+\Delta x)+\Delta x)-f(x+\Delta x)}{\Delta x}-\dfrac{f(x+\Delta x)-f(x)}{\Delta x}}{\Delta x}=$$ $$\lim_{\Delta x \to 0}\dfrac{f(x+2\Delta x)-2f(x+\Delta x)+f(x)}{\Delta x^2} $$ After following this method a couple of times(I think I used it to the 5th derivative) I noticed the pattern of $$(a-b)^n$$ And that is how i arrived at  $$ f^{(n)}(x)=\lim_{\Delta x \to 0} \sum_{k=0}^{n}(-1)^k{n \choose k}\dfrac{f(x+\Delta x(n-k))}{\Delta x^n} $$ Have I made a fatal error somewhere or does this definition actually follow through? Thanks for your time I really appreciate it. P.S. Any input on using tags will be appreciated.,,"['limits', 'derivatives', 'definition']"
87,Why $\lim \limits_ {n\to \infty}\left (\frac{n+3}{n+4}\right)^n \neq 1$?,Why ?,\lim \limits_ {n\to \infty}\left (\frac{n+3}{n+4}\right)^n \neq 1,"Why doesn't $\lim\limits_ {n\to \infty}\ (\frac{n+3}{n+4})^n$ equal $1$? So this is the question. I found it actually it equals $e^{-1}$. I could prove it, using some reordering and canceling. However another way I took was this: $$\lim_ {n\to \infty}\ \left(\frac{n}{n+4}+\frac{3}{n+4}\right)^n$$ with the limit of the first term going to $1$ and the second to $0$. So $(1+0)^n=1$ not $e^{-1}$.","Why doesn't $\lim\limits_ {n\to \infty}\ (\frac{n+3}{n+4})^n$ equal $1$? So this is the question. I found it actually it equals $e^{-1}$. I could prove it, using some reordering and canceling. However another way I took was this: $$\lim_ {n\to \infty}\ \left(\frac{n}{n+4}+\frac{3}{n+4}\right)^n$$ with the limit of the first term going to $1$ and the second to $0$. So $(1+0)^n=1$ not $e^{-1}$.",,['limits']
88,On applying the quadratic formula to a first-degree equation,On applying the quadratic formula to a first-degree equation,,"You're probably thinking, ""Why?""  Please let me explain... It is ( very ) well-known that $$ \forall (a,b,c,x) \in \mathbb{C}^* \times \mathbb{C}^3: ax^2 + bx + c = 0 \Leftrightarrow x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}. $$ For some bizarre reason, I decided to try to solve $ bx + c = 0 $ using this formula by introducing a term $ \alpha x^2 $ and removing it in the limit $ \alpha \to 0 $.  Doing so with L'Hopital's rule, I find these solutions: $$ \displaystyle x_1 = \lim_{\alpha \to 0} {\frac{-b + \sqrt{b^2 - 4c \alpha}}{2 \alpha}} = \lim_{\alpha \to 0} {\frac{-c}{\sqrt{b^2 - 4c \alpha}}} = \frac{-c}{b}, $$ $$ \displaystyle x_2 = \lim_{\alpha \to 0} {\frac{-b - \sqrt{b^2 - 4c \alpha}}{2 \alpha}} = \infty. $$ The first was to be expected, but I still haven't been able to explain the second cleanly (that is, in a way other than ""since $ -c/b $ is gone, it couldn't be a true number""). In addition, carrying out the analogous process one degree lower yields a root at either zero or infinity, depending on the constant.  The latter possibility (which occurs when $ c \neq 0 $) corresponds to the unsolvable case, while the former (in which $ c = 0 $) corresponds to the trivially satisfied one, so a root at zero here appears to have a vastly different meaning from $ x_1 = 0 $ above, where $ x_1 $ gives the location of the unique, genuine root of $ bx + 0 = 0 $ provided $ b \neq 0 $. My question is why a solution at zero can have either of the two meanings just described, and whether the phantom root $ x_2 = \infty $ (obtained by treating the first-degree polynomial $ bx + c $ as a degenerate case of the second-degree one) has a meaningful interpretation. Thank you all in advance, and sorry if my typesetting doesn't render nicely (this is my first experience).","You're probably thinking, ""Why?""  Please let me explain... It is ( very ) well-known that $$ \forall (a,b,c,x) \in \mathbb{C}^* \times \mathbb{C}^3: ax^2 + bx + c = 0 \Leftrightarrow x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}. $$ For some bizarre reason, I decided to try to solve $ bx + c = 0 $ using this formula by introducing a term $ \alpha x^2 $ and removing it in the limit $ \alpha \to 0 $.  Doing so with L'Hopital's rule, I find these solutions: $$ \displaystyle x_1 = \lim_{\alpha \to 0} {\frac{-b + \sqrt{b^2 - 4c \alpha}}{2 \alpha}} = \lim_{\alpha \to 0} {\frac{-c}{\sqrt{b^2 - 4c \alpha}}} = \frac{-c}{b}, $$ $$ \displaystyle x_2 = \lim_{\alpha \to 0} {\frac{-b - \sqrt{b^2 - 4c \alpha}}{2 \alpha}} = \infty. $$ The first was to be expected, but I still haven't been able to explain the second cleanly (that is, in a way other than ""since $ -c/b $ is gone, it couldn't be a true number""). In addition, carrying out the analogous process one degree lower yields a root at either zero or infinity, depending on the constant.  The latter possibility (which occurs when $ c \neq 0 $) corresponds to the unsolvable case, while the former (in which $ c = 0 $) corresponds to the trivially satisfied one, so a root at zero here appears to have a vastly different meaning from $ x_1 = 0 $ above, where $ x_1 $ gives the location of the unique, genuine root of $ bx + 0 = 0 $ provided $ b \neq 0 $. My question is why a solution at zero can have either of the two meanings just described, and whether the phantom root $ x_2 = \infty $ (obtained by treating the first-degree polynomial $ bx + c $ as a degenerate case of the second-degree one) has a meaningful interpretation. Thank you all in advance, and sorry if my typesetting doesn't render nicely (this is my first experience).",,"['polynomials', 'roots', 'infinity', 'limits']"
89,Limits of $f(x)=x-x$,Limits of,f(x)=x-x,"It's obvious that $f(x)=x-x=0$. But what exactly happens here? You have a function $f(x)=x-x$ and you have to calculate the limits when $x\to \infty$ This'll be like this: $$\lim\limits_{x\to \infty}f(x)=\infty - \infty$$ That's an indetermination, and you have to multiply both sides with the conjugate of $f(x)$, which is equal to $x+x$. \begin{align} f(x)&=x-x\\ &=\frac{(x-x)(x+x)}{x+x}\\ &=\frac{x^2-x^2}{x+x} \end{align} If we do the limits now the answer is going to be: $$\lim\limits_{x\to \infty}f(x)=\frac{\infty - \infty}{\infty+\infty}$$ Which it's another type of indetermination(I think). What happens here?Can there be an error multiplying with it's conjugate in both sides? Is there another case like this? Or am I completely wrong?","It's obvious that $f(x)=x-x=0$. But what exactly happens here? You have a function $f(x)=x-x$ and you have to calculate the limits when $x\to \infty$ This'll be like this: $$\lim\limits_{x\to \infty}f(x)=\infty - \infty$$ That's an indetermination, and you have to multiply both sides with the conjugate of $f(x)$, which is equal to $x+x$. \begin{align} f(x)&=x-x\\ &=\frac{(x-x)(x+x)}{x+x}\\ &=\frac{x^2-x^2}{x+x} \end{align} If we do the limits now the answer is going to be: $$\lim\limits_{x\to \infty}f(x)=\frac{\infty - \infty}{\infty+\infty}$$ Which it's another type of indetermination(I think). What happens here?Can there be an error multiplying with it's conjugate in both sides? Is there another case like this? Or am I completely wrong?",,"['limits', 'infinity']"
90,Is this correct? Does no limit exist?,Is this correct? Does no limit exist?,,"I've recently been presented with the following problem : (b) (3 marks) Now consider the function $g: \mathbb{R}^2 \rightarrow \mathbb{R}$ where $$ g(x, y) = \begin{cases} \frac{\sin(2x^2+2y^2)}{x^2+y^2},& (x, y) \neq (0,0) \\ a,& (x, y) = (0,0) \end{cases} $$ For what value(s) of $a$, if any, is $g(x, y)$ continuous at $(0, 0)$? And I believe there is no values of a which satisfy continuity. I've taken two limits which are analogous for the Y variable, which describe 4 approaches to the point in question: $$ \lim_{x,0\to0,0} \frac{\sin(2x^2+2(0)^2)}{x^2+(0)^2} = \lim_{x\to0} \frac{\sin(2x^2)}{x^2} $$ I'll skip the evidence we can use L'Hospitals here, but they both converge to 0 (numerator and denominator), therefore applying the rule for this single variable limit: $$ \lim_{x\to0} \frac{4x\cdot\cos(2x^2)}{2x} = \lim_{x\to0} 2\cdot\cos(2x^2) = 2$$ So on this particular approach, $a = 2$ would make the function continuous. However, note that when you take the approach $x = y$, you yield the following (Utilizing product of limit laws): $$ \lim_{x,x\to0,0} \frac{\sin(2x^2+2(x)^2)}{x^2+(x)^2} = \lim_{x\to0} \frac{\sin(4x^2)}{2x^2} = \frac{1}{2}\cdot \lim_{x\to0}\frac{\sin(4x^2)}{x^2}$$ Again we apply L'Hospitals Rule: $$\frac{1}{2}\cdot\lim_{x\to0} \frac{8x\cdot\cos(4x^2)}{4x} = \frac{1}{2}\cdot\lim_{x\to0}2\cdot\cos(4x^2) = \lim_{x\to0} \cos(4x^2)  = 1 $$ From this we find a separate value that would also make the function continuous at the point 0,0, so there is no limit that exists. Is this right? According to online calculators there is only one limit, 2, but this path wherein x = y seems to hold up being different... Can someone poke a hole in my work for me please so I can realise my error?","I've recently been presented with the following problem : (b) (3 marks) Now consider the function $g: \mathbb{R}^2 \rightarrow \mathbb{R}$ where $$ g(x, y) = \begin{cases} \frac{\sin(2x^2+2y^2)}{x^2+y^2},& (x, y) \neq (0,0) \\ a,& (x, y) = (0,0) \end{cases} $$ For what value(s) of $a$, if any, is $g(x, y)$ continuous at $(0, 0)$? And I believe there is no values of a which satisfy continuity. I've taken two limits which are analogous for the Y variable, which describe 4 approaches to the point in question: $$ \lim_{x,0\to0,0} \frac{\sin(2x^2+2(0)^2)}{x^2+(0)^2} = \lim_{x\to0} \frac{\sin(2x^2)}{x^2} $$ I'll skip the evidence we can use L'Hospitals here, but they both converge to 0 (numerator and denominator), therefore applying the rule for this single variable limit: $$ \lim_{x\to0} \frac{4x\cdot\cos(2x^2)}{2x} = \lim_{x\to0} 2\cdot\cos(2x^2) = 2$$ So on this particular approach, $a = 2$ would make the function continuous. However, note that when you take the approach $x = y$, you yield the following (Utilizing product of limit laws): $$ \lim_{x,x\to0,0} \frac{\sin(2x^2+2(x)^2)}{x^2+(x)^2} = \lim_{x\to0} \frac{\sin(4x^2)}{2x^2} = \frac{1}{2}\cdot \lim_{x\to0}\frac{\sin(4x^2)}{x^2}$$ Again we apply L'Hospitals Rule: $$\frac{1}{2}\cdot\lim_{x\to0} \frac{8x\cdot\cos(4x^2)}{4x} = \frac{1}{2}\cdot\lim_{x\to0}2\cdot\cos(4x^2) = \lim_{x\to0} \cos(4x^2)  = 1 $$ From this we find a separate value that would also make the function continuous at the point 0,0, so there is no limit that exists. Is this right? According to online calculators there is only one limit, 2, but this path wherein x = y seems to hold up being different... Can someone poke a hole in my work for me please so I can realise my error?",,['limits']
91,Why is $ab$ likely to have more divisors than $(a-b)(a+b)$?,Why is  likely to have more divisors than ?,ab (a-b)(a+b),"Consider the two numbers $ab$ and $(a-b)(a+b), \gcd(a,b) = 1, 1 \le b < a$ . On an average, which of these two numbers has more distinct prime factors? All the prime factors of $a$ and $b$ divide $ab$ and similarly all the prime factors of $a-b$ and $a+b$ divide $(a-b)(a+b)$ . So one number does not seem to have an obvious advantage over the other. However if we look at the data than we see that $ab$ dominates. Let $f(x)$ be the average number of distinct prime factors in all such $ab, a \le x$ and $g(x)$ be the average number of distinct prime factors in all such $(a-b)(a+b), a \le x$ . Update : Experimental data for the first $6.1 \times 10^{9}$ pairs of $(a,b)$ shows that $f(x) - g(x) \sim 0.30318$ . Instead of distinct prime factors, if we count the number of divisors than $f(x) - g(x) \sim 0.848$ . Question : Why is $ab$ likely to have more distinct prime factors or divisors than $(a-b)(a+b)$ and what is the limiting value of $f(x) - g(x)$ ?","Consider the two numbers and . On an average, which of these two numbers has more distinct prime factors? All the prime factors of and divide and similarly all the prime factors of and divide . So one number does not seem to have an obvious advantage over the other. However if we look at the data than we see that dominates. Let be the average number of distinct prime factors in all such and be the average number of distinct prime factors in all such . Update : Experimental data for the first pairs of shows that . Instead of distinct prime factors, if we count the number of divisors than . Question : Why is likely to have more distinct prime factors or divisors than and what is the limiting value of ?","ab (a-b)(a+b), \gcd(a,b) = 1, 1 \le b < a a b ab a-b a+b (a-b)(a+b) ab f(x) ab, a \le x g(x) (a-b)(a+b), a \le x 6.1 \times 10^{9} (a,b) f(x) - g(x) \sim 0.30318 f(x) - g(x) \sim 0.848 ab (a-b)(a+b) f(x) - g(x)","['limits', 'number-theory', 'elementary-number-theory', 'prime-numbers', 'divisibility']"
92,$\lim_{x\to 1^-}\sqrt{1-x}\ \left(1+x+x^4+x^9+x^{16}+x^{25}+\cdots\right)=\sqrt{\pi}/2$ is true?,is true?,\lim_{x\to 1^-}\sqrt{1-x}\ \left(1+x+x^4+x^9+x^{16}+x^{25}+\cdots\right)=\sqrt{\pi}/2,"When I was considering limits of various functions, I had the following conjecture. $$\lim_{x \to 1^-}\sqrt{1-x}\ \sum_{k=0}^{\infty}x^{\left(k^2\right)}=\lim_{x\to 1^-}\sqrt{1-x}\ \left(1+x+x^4+x^9+x^{16}+x^{25}+\cdots\right)=\frac{\sqrt{\pi}}{2}$$ This seems true, but I can't prove that. If this is true, how can we prove that? If this is not true, what is the answer?","When I was considering limits of various functions, I had the following conjecture. $$\lim_{x \to 1^-}\sqrt{1-x}\ \sum_{k=0}^{\infty}x^{\left(k^2\right)}=\lim_{x\to 1^-}\sqrt{1-x}\ \left(1+x+x^4+x^9+x^{16}+x^{25}+\cdots\right)=\frac{\sqrt{\pi}}{2}$$ This seems true, but I can't prove that. If this is true, how can we prove that? If this is not true, what is the answer?",,['limits']
93,Why does the limit $\log{2^\pi}/\log{2\pi}$ emerge as a ratio of a certain set of adjacent prime factors?,Why does the limit  emerge as a ratio of a certain set of adjacent prime factors?,\log{2^\pi}/\log{2\pi},"Let $f(n)$ for $n\in\mathbb N$ be a function that increases the prime index of each prime factor of $n$ (with multiplicity) by $1$ . e.g. $f(20)=f(2^2\cdot 5)=f(p_1^2\cdot p_3)=p_{2}^2\cdot p_{4}=3^2\cdot 7=63$ . Let the set $S_n=\{n< s< 2n : f(s)\leq 2n\}$ , for any $n\in\mathbb N$ . If you take the mean of $\frac{f(s)}{s}$ for each element $s \in S_n$ , empirical results for $n \leq 10^8$ strongly suggest that this mean converges: $$\lim_{n\to\infty} \left(\frac{1}{|S_n|}\sum_{s \in S_n} \frac{f(s)}{s}\right)=\frac{\log 2^{\pi}}{\log {2\pi}}\approx 1.18484.$$ $$$$ Can anyone explain why this should be so? My motivation in asking is as part of a larger study of the behavior of $f$ , and this result seems too nice and tidy to ignore without investigation. If anyone can explain the origin of the RHS term, or show that it is not a limit after all, I'll consider this question answered. As a rough picture of what empirical results I'm referring to, here's a plot of this mean as $n$ increases by a factor of $1.1$ each step to nearly $10^8$ : It appears convincingly centered around $\frac{\log{2^\pi}}{\log{2\pi}}$ , which seems much more plausible than any other nearby constants I could find, and it's varying by around $10^{-6}$ at the tail end there. Per request, Mathematica code to generate plot. This was quick 'n dirty and I'm certain it could be optimized to run much faster. Use Alt+. to halt execution. ClearAll[f, dyn];  f[1] = 1; f[n_, k_ : 1] := Times @@ (NextPrime[#1, k]^#2 &) @@@ FactorInteger@n; SetAttributes[f, Listable]; SetAttributes[dyn, HoldAll];  dyn[expr_, symbols_List : {}, interval_ : \[Infinity]] :=   PrintTemporary[   Dynamic[Refresh[expr, TrackedSymbols :> symbols,      UpdateInterval -> interval]]]  tab = {}; disp := Column[{Length@tab, n, ListLinePlot[Last/@tab, ImageSize -> Large],     Grid[Prepend[      Reverse@tab, {Style[""n"", Bold], Style[""S mean"", Bold]}],      Dividers -> All, Alignment -> {Left, Top}]}] dyn[disp, {tab}];  n = 100; While[True,  s = Select[f[Range[n + 3 - Mod[n, 2, 1], 2 n, 2], -1], # > n &];  m = Mean[N[f@#, 8]/# & /@ s];  AppendTo[tab, {n, m}];  n = Ceiling[1.1 n];  ] disp","Let for be a function that increases the prime index of each prime factor of (with multiplicity) by . e.g. . Let the set , for any . If you take the mean of for each element , empirical results for strongly suggest that this mean converges: Can anyone explain why this should be so? My motivation in asking is as part of a larger study of the behavior of , and this result seems too nice and tidy to ignore without investigation. If anyone can explain the origin of the RHS term, or show that it is not a limit after all, I'll consider this question answered. As a rough picture of what empirical results I'm referring to, here's a plot of this mean as increases by a factor of each step to nearly : It appears convincingly centered around , which seems much more plausible than any other nearby constants I could find, and it's varying by around at the tail end there. Per request, Mathematica code to generate plot. This was quick 'n dirty and I'm certain it could be optimized to run much faster. Use Alt+. to halt execution. ClearAll[f, dyn];  f[1] = 1; f[n_, k_ : 1] := Times @@ (NextPrime[#1, k]^#2 &) @@@ FactorInteger@n; SetAttributes[f, Listable]; SetAttributes[dyn, HoldAll];  dyn[expr_, symbols_List : {}, interval_ : \[Infinity]] :=   PrintTemporary[   Dynamic[Refresh[expr, TrackedSymbols :> symbols,      UpdateInterval -> interval]]]  tab = {}; disp := Column[{Length@tab, n, ListLinePlot[Last/@tab, ImageSize -> Large],     Grid[Prepend[      Reverse@tab, {Style[""n"", Bold], Style[""S mean"", Bold]}],      Dividers -> All, Alignment -> {Left, Top}]}] dyn[disp, {tab}];  n = 100; While[True,  s = Select[f[Range[n + 3 - Mod[n, 2, 1], 2 n, 2], -1], # > n &];  m = Mean[N[f@#, 8]/# & /@ s];  AppendTo[tab, {n, m}];  n = Ceiling[1.1 n];  ] disp",f(n) n\in\mathbb N n 1 f(20)=f(2^2\cdot 5)=f(p_1^2\cdot p_3)=p_{2}^2\cdot p_{4}=3^2\cdot 7=63 S_n=\{n< s< 2n : f(s)\leq 2n\} n\in\mathbb N \frac{f(s)}{s} s \in S_n n \leq 10^8 \lim_{n\to\infty} \left(\frac{1}{|S_n|}\sum_{s \in S_n} \frac{f(s)}{s}\right)=\frac{\log 2^{\pi}}{\log {2\pi}}\approx 1.18484.  f n 1.1 10^8 \frac{\log{2^\pi}}{\log{2\pi}} 10^{-6},"['limits', 'elementary-number-theory', 'prime-numbers']"
94,Is the limit of $f(n) = n-n$ zero as $n\rightarrow \infty$?,Is the limit of  zero as ?,f(n) = n-n n\rightarrow \infty,"I have been working on a proof which involves sums and products going to infinity. I am wondering whether the following proof of a limit is valid, and whether that result would allow me to come to another conclusion. What is: $$\lim \limits_{n \to \infty} f(n)\text {, where }f(n) = n-n$$ I have worked this out to be $$\lim \limits_{n \to \infty} n-n = \lim \limits_{n \to \infty} n(1-1) = \lim \limits_{n \to \infty} n\cdot 0 = 0$$ I'm not sure whether this is the correct way of proving this limit, or whether the answer is correct. My math teacher had said that the whole limit raised a red flag in his mind, and he wasn't sure why. If my limit is correct, though, I would like to know whether the following is also valid: $$\lim \limits_{n \to \infty} f(n)\cdot n = 0$$","I have been working on a proof which involves sums and products going to infinity. I am wondering whether the following proof of a limit is valid, and whether that result would allow me to come to another conclusion. What is: $$\lim \limits_{n \to \infty} f(n)\text {, where }f(n) = n-n$$ I have worked this out to be $$\lim \limits_{n \to \infty} n-n = \lim \limits_{n \to \infty} n(1-1) = \lim \limits_{n \to \infty} n\cdot 0 = 0$$ I'm not sure whether this is the correct way of proving this limit, or whether the answer is correct. My math teacher had said that the whole limit raised a red flag in his mind, and he wasn't sure why. If my limit is correct, though, I would like to know whether the following is also valid: $$\lim \limits_{n \to \infty} f(n)\cdot n = 0$$",,"['limits', 'functions']"
95,Why $x<\tan{x}$ while $0<x<\frac{\pi}{2}$?,Why  while ?,x<\tan{x} 0<x<\frac{\pi}{2},"In proof of $\displaystyle\lim_{x\rightarrow0}\frac{\sin{x}}{x}=1$ is assumed that $\sin{x}\leq{x}\leq\tan{x}$ while $0<x<\frac{\pi}{2}$. First comparison is clear, arc length must be greater than sine value, but how about $x\leq\tan{x}$, why tangent is longer than arc?","In proof of $\displaystyle\lim_{x\rightarrow0}\frac{\sin{x}}{x}=1$ is assumed that $\sin{x}\leq{x}\leq\tan{x}$ while $0<x<\frac{\pi}{2}$. First comparison is clear, arc length must be greater than sine value, but how about $x\leq\tan{x}$, why tangent is longer than arc?",,"['trigonometry', 'inequality', 'limits']"
96,Find $\lim\limits_{n \to \infty}\left(1-\frac{1}{2^2}\right)\left(1-\frac{1}{3^2}\right)\cdots\left(1-\frac{1}{n^2}\right)$ [duplicate],Find  [duplicate],\lim\limits_{n \to \infty}\left(1-\frac{1}{2^2}\right)\left(1-\frac{1}{3^2}\right)\cdots\left(1-\frac{1}{n^2}\right),This question already has answers here : Finding Value of the Infinite Product $\prod \Bigl(1-\frac{1}{n^{2}}\Bigr)$ (6 answers) Closed 8 years ago . Find the limit $$\lim_{n \to \infty}\left[\left(1-\frac{1}{2^2}\right)\left(1-\frac{1}{3^2}\right)\cdots\left(1-\frac{1}{n^2}\right)\right]$$ I take log and get $$\lim_{n \to \infty}\sum_{k=2}^{n} \log\left(1-\frac{1}{k^2}\right)$$,This question already has answers here : Finding Value of the Infinite Product $\prod \Bigl(1-\frac{1}{n^{2}}\Bigr)$ (6 answers) Closed 8 years ago . Find the limit $$\lim_{n \to \infty}\left[\left(1-\frac{1}{2^2}\right)\left(1-\frac{1}{3^2}\right)\cdots\left(1-\frac{1}{n^2}\right)\right]$$ I take log and get $$\lim_{n \to \infty}\sum_{k=2}^{n} \log\left(1-\frac{1}{k^2}\right)$$,,"['limits', 'infinite-product']"
97,Closed form for a pair of continued fractions,Closed form for a pair of continued fractions,,"What is $1+\cfrac{1}{2+\cfrac{1}{3+\cfrac{1}{4+\cdots}}}$ ? What is $1+\cfrac{2}{1+\cfrac{3}{1+\cdots}}$ ? It does bear some resemblance to the continued fraction for $e$, which is $2+\cfrac{2}{2+\cfrac{3}{3+\cfrac{4}{4+\cdots}}}$. Another thing I was wondering: can all transcendental numbers be expressed as infinite continued fractions containing only rational numbers? Of course for almost all transcendental numbers there does not exist any method to determine all the numerators and denominators.","What is $1+\cfrac{1}{2+\cfrac{1}{3+\cfrac{1}{4+\cdots}}}$ ? What is $1+\cfrac{2}{1+\cfrac{3}{1+\cdots}}$ ? It does bear some resemblance to the continued fraction for $e$, which is $2+\cfrac{2}{2+\cfrac{3}{3+\cfrac{4}{4+\cdots}}}$. Another thing I was wondering: can all transcendental numbers be expressed as infinite continued fractions containing only rational numbers? Of course for almost all transcendental numbers there does not exist any method to determine all the numerators and denominators.",,"['limits', 'irrational-numbers', 'continued-fractions', 'transcendental-numbers']"
98,Trying to calculate a limit with a finite product and WolframAlpha disagrees with my logic.,Trying to calculate a limit with a finite product and WolframAlpha disagrees with my logic.,,"I want to calculate $$\lim_{N \to \infty} \prod_{j=1}^{N} \frac{2j}{N + j + 1} \,.$$ Here is my logic: For the range of the product operator ( $1 \leq j \leq N $ ) the inequality $2j < N + j + 1$ is always true. Therefore $\frac{2j}{N + j + 1} < 1 $ , and so an upper boundary can be estabilished using the largest term of the product: $$\lim_{N \to \infty} \prod_{j=1}^{N} \frac{2j}{N + j + 1} \leq \lim_{N \to \infty}\frac{2N}{2N + 1} = 1 \,.$$ However, when I use WolframAlpha I get $\infty$ for an answer. I have seen other questions that challenge the reliability of WolframAlpha, so I am tempted to dismiss it. My questions: Is my logic correct or is WolframAlpha correct? (and why?) If the limit is indeed finite, does it go to zero or does it simply coneverge to some value smaller than 1? How can I show it? Thank you very much.","I want to calculate Here is my logic: For the range of the product operator ( ) the inequality is always true. Therefore , and so an upper boundary can be estabilished using the largest term of the product: However, when I use WolframAlpha I get for an answer. I have seen other questions that challenge the reliability of WolframAlpha, so I am tempted to dismiss it. My questions: Is my logic correct or is WolframAlpha correct? (and why?) If the limit is indeed finite, does it go to zero or does it simply coneverge to some value smaller than 1? How can I show it? Thank you very much.","\lim_{N \to \infty} \prod_{j=1}^{N} \frac{2j}{N + j + 1} \,. 1 \leq j \leq N  2j < N + j + 1 \frac{2j}{N + j + 1} < 1  \lim_{N \to \infty} \prod_{j=1}^{N} \frac{2j}{N + j + 1} \leq \lim_{N \to \infty}\frac{2N}{2N + 1} = 1 \,. \infty","['limits', 'products', 'wolfram-alpha']"
99,$\lim_{n \to \infty}\frac{1}{n}\sum_{k = 1}^nf(k)$ where $f(n)$ is the largest prime factor exponent?,where  is the largest prime factor exponent?,\lim_{n \to \infty}\frac{1}{n}\sum_{k = 1}^nf(k) f(n),"Let $f(n)$ the be largest exponent among exponents of the prime factor of $n$ . E.g. $f(80) = 4$ since $80 = 2^4.5$ and the prime factor of $80$ which has the largest exponent is $2$ which occurs with the exponent $4$ . Trivially for all square-free numbers $n, f(n) = 1$ . Experimental data for $n \le 3.5 \times 10^9$ shows that. $$ \mu = \lim_{n \to \infty}\frac{1}{n}\sum_{k = 1}^nf(n) \approx 1.784744 $$ Q 1 : Does this limit exist and does it have a closed form? If all positive integers were square-free, the above mean would have been exactly $1$ . But this is not the case because there are non-squarefree numbers. Thus, we can say that the square-free numbers contribute exactly $1$ to the above mean value while all numbers which have prime factors with an exponent $> 1$ contribute the remaining $0.784744$ . Using the natural density of $k$ -th power free numbers, I can show that $$ \mu \ge \sum_{k = 1}^{\infty}k \Big(\frac{1}{\zeta(k+1)} - \frac{1}{\zeta(k)}\Big) \approx 1.70521 $$ Q 2 : Is it possible to show a weaker result like $\mu < 2$ ?","Let the be largest exponent among exponents of the prime factor of . E.g. since and the prime factor of which has the largest exponent is which occurs with the exponent . Trivially for all square-free numbers . Experimental data for shows that. Q 1 : Does this limit exist and does it have a closed form? If all positive integers were square-free, the above mean would have been exactly . But this is not the case because there are non-squarefree numbers. Thus, we can say that the square-free numbers contribute exactly to the above mean value while all numbers which have prime factors with an exponent contribute the remaining . Using the natural density of -th power free numbers, I can show that Q 2 : Is it possible to show a weaker result like ?","f(n) n f(80) = 4 80 = 2^4.5 80 2 4 n, f(n) = 1 n \le 3.5 \times 10^9 
\mu = \lim_{n \to \infty}\frac{1}{n}\sum_{k = 1}^nf(n) \approx 1.784744
 1 1 > 1 0.784744 k 
\mu \ge \sum_{k = 1}^{\infty}k \Big(\frac{1}{\zeta(k+1)} - \frac{1}{\zeta(k)}\Big) \approx 1.70521
 \mu < 2","['limits', 'number-theory', 'elementary-number-theory', 'analytic-number-theory', 'prime-factorization']"
