,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why $\rho(AABABB)=\rho(ABAABB)$?,Why ?,\rho(AABABB)=\rho(ABAABB),"Let $A,B$ be two matrices, $\rho$ be spectral radius, which is the top eigenvalue of a matrix. I discovered that $$\rho(AABABB)=\rho(ABAABB).$$ But I could not find the reason. By the way, all I had tested were $2\times2$ positive matrices. And I know the fact $\rho(AB)=\rho(BA)$, but it seems that this property is not enough for this statement.","Let $A,B$ be two matrices, $\rho$ be spectral radius, which is the top eigenvalue of a matrix. I discovered that $$\rho(AABABB)=\rho(ABAABB).$$ But I could not find the reason. By the way, all I had tested were $2\times2$ positive matrices. And I know the fact $\rho(AB)=\rho(BA)$, but it seems that this property is not enough for this statement.",,"['matrices', 'functional-analysis', 'eigenvalues-eigenvectors']"
1,Why left multiplication when it comes to Markov chains?,Why left multiplication when it comes to Markov chains?,,"When working with Markov chains and transition matrices $P$ we multiply from the left, meaning that for example $\mu^{(n)} = \mu^{(0)}P^n$ or that the stationary distribution satisfies $\pi = \pi P$. Especially for the stationary distribution this means that $\pi$ is a left eigenvector of the transition matrix $P$. Why do we do this left multiplication? Is it just a convention or are there any other reasons why this is done? I couldn't think of an intuitive explanation. In my opinion, it seems more intuitive to do $\mu^{(n)} = P'^n\mu{(0)}$ and treat $\pi$ as a (normal/right) eigenvector $\pi = P' \pi$. It seems to me that, in this case, $P' = P^t$.","When working with Markov chains and transition matrices $P$ we multiply from the left, meaning that for example $\mu^{(n)} = \mu^{(0)}P^n$ or that the stationary distribution satisfies $\pi = \pi P$. Especially for the stationary distribution this means that $\pi$ is a left eigenvector of the transition matrix $P$. Why do we do this left multiplication? Is it just a convention or are there any other reasons why this is done? I couldn't think of an intuitive explanation. In my opinion, it seems more intuitive to do $\mu^{(n)} = P'^n\mu{(0)}$ and treat $\pi$ as a (normal/right) eigenvector $\pi = P' \pi$. It seems to me that, in this case, $P' = P^t$.",,"['matrices', 'markov-chains']"
2,Projection onto Birkhoff Polytope,Projection onto Birkhoff Polytope,,"Suppose we would like to compute the Euclidean projection of an arbitrary matrix $A$ onto the Birkhoff polytope, the set of doubly-stochastic matrices. Under some conditions on $A$, Sinkhorn's algorithm returns two diagonal matrices $D_1,D_2$ such that $D_1 A D_2$ is a doubly-stochastic matrix (which is unique). How does this matrix compare to the Euclidean projection, and if not, does this solution correspond to the projection under any particular metric? Are there known methods for solving this efficiently besides just formulating the problem as a quadratic program and applying a generic QP algorithm?","Suppose we would like to compute the Euclidean projection of an arbitrary matrix $A$ onto the Birkhoff polytope, the set of doubly-stochastic matrices. Under some conditions on $A$, Sinkhorn's algorithm returns two diagonal matrices $D_1,D_2$ such that $D_1 A D_2$ is a doubly-stochastic matrix (which is unique). How does this matrix compare to the Euclidean projection, and if not, does this solution correspond to the projection under any particular metric? Are there known methods for solving this efficiently besides just formulating the problem as a quadratic program and applying a generic QP algorithm?",,"['matrices', 'projection', 'birkhoff-polytopes']"
3,Matrix which commutes with permutation matrix,Matrix which commutes with permutation matrix,,"I'm trying to show that if $A$ commutes with all $3\times 3$ permutation matrices, then $A$ has to be of the following form:  $ A = \begin{pmatrix} a & b & b \\ b & a & b  \\ b & b & a  \\ \end{pmatrix} $  What i've tried so far: Let $A$ be a general $3 \times 3$-Matrix. We are trying to find $A$ such that $AP = PA$. This equation basically means the following: $AP$ is A with permuted columns. $PA$ is A with permuted rows. This means that $A$ has to be such that permuting columns and rows in the same way leaves the marix looking the same. For example for the permutations which flip $1$ and $2$: $ \begin{pmatrix} d & e & f \\ a & b & c  \\ g & h & i  \\ \end{pmatrix}$ = $ \begin{pmatrix} b & a & c \\ e & d & f  \\ h & g & i  \\ \end{pmatrix}$. If we do the same for flipping $2$ and $3$ and flipping $1$ and $3$ we get three matrix equations for the 9 entries  $a$ to $i$  Resulting in the following equations: $a = i, b = h, g = c, f = d , b = d, a = e, c = f, h = g, b = c, f = h, i = e, g = d$ which means : $ a= e = i$ and $b=c=d=g=h$ which is exactly what I was looking for. The thing now is. If I look at permutations which not only swap two rows/columns (for example $(123) \implies (312)$ ) I get equations which contradict my above equations. What am I doing wrong? Is the proposition even true i.e. provable? Thanks in advance!","I'm trying to show that if $A$ commutes with all $3\times 3$ permutation matrices, then $A$ has to be of the following form:  $ A = \begin{pmatrix} a & b & b \\ b & a & b  \\ b & b & a  \\ \end{pmatrix} $  What i've tried so far: Let $A$ be a general $3 \times 3$-Matrix. We are trying to find $A$ such that $AP = PA$. This equation basically means the following: $AP$ is A with permuted columns. $PA$ is A with permuted rows. This means that $A$ has to be such that permuting columns and rows in the same way leaves the marix looking the same. For example for the permutations which flip $1$ and $2$: $ \begin{pmatrix} d & e & f \\ a & b & c  \\ g & h & i  \\ \end{pmatrix}$ = $ \begin{pmatrix} b & a & c \\ e & d & f  \\ h & g & i  \\ \end{pmatrix}$. If we do the same for flipping $2$ and $3$ and flipping $1$ and $3$ we get three matrix equations for the 9 entries  $a$ to $i$  Resulting in the following equations: $a = i, b = h, g = c, f = d , b = d, a = e, c = f, h = g, b = c, f = h, i = e, g = d$ which means : $ a= e = i$ and $b=c=d=g=h$ which is exactly what I was looking for. The thing now is. If I look at permutations which not only swap two rows/columns (for example $(123) \implies (312)$ ) I get equations which contradict my above equations. What am I doing wrong? Is the proposition even true i.e. provable? Thanks in advance!",,['matrices']
4,"""Inverse"" of tensor product","""Inverse"" of tensor product",,"I am trying to figure out something. I have a 4-tensor $\phi_{i \, j \, k \, \ell}$ and I know that $\phi = A \otimes B$, being $A$ and $B$ two matrices. With indices, I know this: $\phi_{i \, j \, k \, \ell} = A_{k \, i} \, B_{j \, \ell}$ Now, if I know $A$ and $B$ I can create $\phi$. How can I do the inverse? If I know $\phi$, how can I find $A$ and $B$? I know this is not an ""inverse"" tensor product, but I hope I was sufficiently clear :)","I am trying to figure out something. I have a 4-tensor $\phi_{i \, j \, k \, \ell}$ and I know that $\phi = A \otimes B$, being $A$ and $B$ two matrices. With indices, I know this: $\phi_{i \, j \, k \, \ell} = A_{k \, i} \, B_{j \, \ell}$ Now, if I know $A$ and $B$ I can create $\phi$. How can I do the inverse? If I know $\phi$, how can I find $A$ and $B$? I know this is not an ""inverse"" tensor product, but I hope I was sufficiently clear :)",,"['matrices', 'tensors']"
5,"Algebraic and geometric multiplicity, eigenspace and Transition Matrix","Algebraic and geometric multiplicity, eigenspace and Transition Matrix",,"I have a matrix $$ A=\begin{bmatrix}6 & 9 &15 \\ -5&-10 & -21 \\2&5&11\end{bmatrix} $$ The Characteristic Polynomial is $ x^3-7x^2+16x-12 $ From this i have worked out the Eigenvalues to be $2,2,3$ and the corresponding Eigenvectors to be $ \begin{bmatrix} 3\\-3\\1 \end{bmatrix} $ For the value 2, and $ \begin{bmatrix} 1\\-2\\1 \end{bmatrix} $ For the value 3 However im not too sure what is meant by the Alegrabic multiplicity,Eigenspace and the Geometric multiplicity?? The Question further on asks to compute the $ JordanForm $ which i got to be $$ \begin{bmatrix}3 & 0 &0 \\ 0&2 & 1 \\0&0&2\end{bmatrix} $$ But then asks me to find the Transition matrix P by assembling the generalized eigenvectors. i've computed the generalized eigenvectors to be $ \begin{bmatrix}-6 & -3 &1 \\ 0&1 & -2 \\1&0&1\end{bmatrix} $ but im not sure how to find the Transition Matrix P from this??","I have a matrix $$ A=\begin{bmatrix}6 & 9 &15 \\ -5&-10 & -21 \\2&5&11\end{bmatrix} $$ The Characteristic Polynomial is $ x^3-7x^2+16x-12 $ From this i have worked out the Eigenvalues to be $2,2,3$ and the corresponding Eigenvectors to be $ \begin{bmatrix} 3\\-3\\1 \end{bmatrix} $ For the value 2, and $ \begin{bmatrix} 1\\-2\\1 \end{bmatrix} $ For the value 3 However im not too sure what is meant by the Alegrabic multiplicity,Eigenspace and the Geometric multiplicity?? The Question further on asks to compute the $ JordanForm $ which i got to be $$ \begin{bmatrix}3 & 0 &0 \\ 0&2 & 1 \\0&0&2\end{bmatrix} $$ But then asks me to find the Transition matrix P by assembling the generalized eigenvectors. i've computed the generalized eigenvectors to be $ \begin{bmatrix}-6 & -3 &1 \\ 0&1 & -2 \\1&0&1\end{bmatrix} $ but im not sure how to find the Transition Matrix P from this??",,['matrices']
6,Functions space of discrete space: how does taking quotients lead to noncommutativity?,Functions space of discrete space: how does taking quotients lead to noncommutativity?,,"It is pointed out in Geometry from the spectral point of view the following: If one considers a discrete space, say, the two-point space $\{1,2\}$ , after identifying its points $X=\{1,2\}/\sim$ , the algebra of functions $A=C(\{1,2\}/\sim,\mathbb{C})$ is the algebra of matrices $M_2(\mathbb{C})$ with usual matrix product. According to the author, the noncommutativity of this algebra is a result of the relation between the ""points"". (If one takes the quotient one has $X=\{*\}$ , whose algebra is $\mathbb{C}$ . I still have no problem with this apparent ambiguity, i.e. $M_2(\mathbb{C})$ is Morita equivalent to $\mathbb{C}$ , so it will have the same ""noncommutative topology"", so to say). However, concerning Connes' statement, Question: where does the usual matrix product comes from?","It is pointed out in Geometry from the spectral point of view the following: If one considers a discrete space, say, the two-point space , after identifying its points , the algebra of functions is the algebra of matrices with usual matrix product. According to the author, the noncommutativity of this algebra is a result of the relation between the ""points"". (If one takes the quotient one has , whose algebra is . I still have no problem with this apparent ambiguity, i.e. is Morita equivalent to , so it will have the same ""noncommutative topology"", so to say). However, concerning Connes' statement, Question: where does the usual matrix product comes from?","\{1,2\} X=\{1,2\}/\sim A=C(\{1,2\}/\sim,\mathbb{C}) M_2(\mathbb{C}) X=\{*\} \mathbb{C} M_2(\mathbb{C}) \mathbb{C}","['matrices', 'functional-analysis', 'noncommutative-geometry']"
7,Absolute value of all values in a matrix,Absolute value of all values in a matrix,,"How do I express the matlab function abs(M) , on a matrix $M$ , in mathematical terms? I thought about norms or just $|M|$ , but these return scalars, not another matrix of the same size as $M$ . Sorry for the rough explanation, english isn't my primary language.","How do I express the matlab function abs(M) , on a matrix , in mathematical terms? I thought about norms or just , but these return scalars, not another matrix of the same size as . Sorry for the rough explanation, english isn't my primary language.",M |M| M,['matrices']
8,"""Vieta's-formulas"" on matrices","""Vieta's-formulas"" on matrices",,"Let matrices $X $ and $ Y$ satisfy the equality $Z^2+AZ+B=0$ where $A , B$ are real matrices and $\det(X - Y)\ne 0$ Prove $ \operatorname{tr} X + \operatorname{tr} Y  = - \operatorname{tr} A$ $\det X \det Y = \det B $ Proof 1) $(X+Y)(X-Y)+XY-YX+A(X-Y)=0\Rightarrow X+Y+(XY-YX)(X-Y)^{-1}+A=0$ As $ \operatorname{tr} PQ= \operatorname{tr} QP$ we have $\operatorname{tr} (XY-YX)(X-Y)^{-1}=0$ entailing the result. We have $(X+A)X=-B$ and $(Y+A)Y=-B$ . The result follows if $\det XY=0.$ If $\det X\ne 0$ and $\det Y\ne0$ then $X-Y=B(Y^{-1}-X^{-1})$ Assuming that $X$ and $Y$ are permutable, we can easily obtain the result. Is it possible to achieve this result without such an assumption, or is it necessary to look for a counterexample?","Let matrices and satisfy the equality where are real matrices and Prove Proof 1) As we have entailing the result. We have and . The result follows if If and then Assuming that and are permutable, we can easily obtain the result. Is it possible to achieve this result without such an assumption, or is it necessary to look for a counterexample?","X   Y Z^2+AZ+B=0 A , B \det(X - Y)\ne 0  \operatorname{tr} X + \operatorname{tr} Y  = - \operatorname{tr} A \det X \det Y = \det B  (X+Y)(X-Y)+XY-YX+A(X-Y)=0\Rightarrow X+Y+(XY-YX)(X-Y)^{-1}+A=0  \operatorname{tr} PQ= \operatorname{tr} QP \operatorname{tr} (XY-YX)(X-Y)^{-1}=0 (X+A)X=-B (Y+A)Y=-B \det XY=0. \det X\ne 0 \det Y\ne0 X-Y=B(Y^{-1}-X^{-1}) X Y",['matrices']
9,Prove that $I + A^{T}A$ is invertible,Prove that  is invertible,I + A^{T}A,"I'm completely stuck with this seemingly simple problem. I'm trying to prove that matrix $I + A^TA$ is invertible, where $A \in \mathbb{R^{m \times n}}$ . The book where this is from hasn't yet introduced determinants or eigenvalues or other such ""fancier"" results so I should do without those. I've tried to prove that $rank(A) = n$ or that $(I + A^TA)x=0 \implies x=0$ , but none of my approaches seem to lead to any breakthroughs. Any hints? Edit: Based on the replies from Will Jagy and Martin Argerami I was able to see the solution (without the use of eigenvalues). So, here's my formulation for the solution (I think): Suppose that $(I + A^TA)x=0$ where $x$ is a nonzero vector. Then, we also have $x^T(I + A^TA)x=0$ . Because $x^T(I + A^TA)x=x^TIx+x^T(A^TA)x=x^Tx+(Ax)^TAx$ and $x^Tx=\sum_{k = 1}^{n} x^2_k>0$ (for $x \ne 0$ ) and $(Ax)^TAx=\sum_{h=1}^n (\sum_{k=1}^n a_{hk}x_k)^2 \ge 0$ , then we have $x^T(I + A^TA)x=x^Tx+(Ax)^TAx>0$ which is a contradiction. Thus, $x$ cannot be a nonzero vector and $(I + A^TA)x=0 \implies x=0$ which means that $I + A^TA$ is invertible.","I'm completely stuck with this seemingly simple problem. I'm trying to prove that matrix is invertible, where . The book where this is from hasn't yet introduced determinants or eigenvalues or other such ""fancier"" results so I should do without those. I've tried to prove that or that , but none of my approaches seem to lead to any breakthroughs. Any hints? Edit: Based on the replies from Will Jagy and Martin Argerami I was able to see the solution (without the use of eigenvalues). So, here's my formulation for the solution (I think): Suppose that where is a nonzero vector. Then, we also have . Because and (for ) and , then we have which is a contradiction. Thus, cannot be a nonzero vector and which means that is invertible.",I + A^TA A \in \mathbb{R^{m \times n}} rank(A) = n (I + A^TA)x=0 \implies x=0 (I + A^TA)x=0 x x^T(I + A^TA)x=0 x^T(I + A^TA)x=x^TIx+x^T(A^TA)x=x^Tx+(Ax)^TAx x^Tx=\sum_{k = 1}^{n} x^2_k>0 x \ne 0 (Ax)^TAx=\sum_{h=1}^n (\sum_{k=1}^n a_{hk}x_k)^2 \ge 0 x^T(I + A^TA)x=x^Tx+(Ax)^TAx>0 x (I + A^TA)x=0 \implies x=0 I + A^TA,['matrices']
10,Determinant of a sum of two matrices [closed],Determinant of a sum of two matrices [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question For a matrix $X$ with $\det X = 0$ , what should be the constraints on $Y$ such that $\det (X+Y) = 0$ ? On obvious choice would be $Y=\alpha X$ , but can one say something more than this?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question For a matrix with , what should be the constraints on such that ? On obvious choice would be , but can one say something more than this?",X \det X = 0 Y \det (X+Y) = 0 Y=\alpha X,"['matrices', 'determinant']"
11,"What does ""$A^b \bmod c$, where $A$ is a square matrix"" mean? What is the modulus of a matrix?","What does "", where  is a square matrix"" mean? What is the modulus of a matrix?",A^b \bmod c A,"I was reading Wikipedia's ""Modular exponentiation"" entry . It made sense to me until I got to the part about Matrices . What does "" $A^b \bmod c$ , where $A$ is a square matrix"" mean? What is the modulus of a matrix?","I was reading Wikipedia's ""Modular exponentiation"" entry . It made sense to me until I got to the part about Matrices . What does "" , where is a square matrix"" mean? What is the modulus of a matrix?",A^b \bmod c A,"['matrices', 'modular-arithmetic']"
12,equivalence of two conditions on two matrices,equivalence of two conditions on two matrices,,"I would like to ask for your view on two different conditions for two matrices. I have the two matrices mentioned above, say $\Lambda_1$ and $\Lambda_2$ . Their dimensions are, for both matrices, $N\times P$ , and $N>P$ . In some papers I am reading, I found that the following condition is needed $\Lambda_2 \neq \Lambda_1 G$ for all matrices $G$ , with $G$ of dimensions $P\times P$ . Nothing else is said on $G$ . Working on a related problem, I need the condition $\Lambda_2=\Lambda_1C + \tilde{\Lambda}_1$ where $C$ is some matrix of dimensions $P\times P$ . I do not have any specific conditions on $C$ (e.g. it could have full rank or not, it could be nonzero or all zeros, etc...). On the other hand, $\tilde{\Lambda}_1$ is an $N\times P$ matrix which belongs in the orthogonal space of $\Lambda_1$ - I mean that $\Lambda_1^{\prime}\tilde{\Lambda}_1=0$ My question is: how different are the two conditions mentioned above? Are they entirely equivalent, do they imply each other, does one imply the other at all...? Any comment would be welcome.","I would like to ask for your view on two different conditions for two matrices. I have the two matrices mentioned above, say and . Their dimensions are, for both matrices, , and . In some papers I am reading, I found that the following condition is needed for all matrices , with of dimensions . Nothing else is said on . Working on a related problem, I need the condition where is some matrix of dimensions . I do not have any specific conditions on (e.g. it could have full rank or not, it could be nonzero or all zeros, etc...). On the other hand, is an matrix which belongs in the orthogonal space of - I mean that My question is: how different are the two conditions mentioned above? Are they entirely equivalent, do they imply each other, does one imply the other at all...? Any comment would be welcome.",\Lambda_1 \Lambda_2 N\times P N>P \Lambda_2 \neq \Lambda_1 G G G P\times P G \Lambda_2=\Lambda_1C + \tilde{\Lambda}_1 C P\times P C \tilde{\Lambda}_1 N\times P \Lambda_1 \Lambda_1^{\prime}\tilde{\Lambda}_1=0,"['matrices', 'orthogonality']"
13,Obtain B to minimize $\|\boldsymbol{X}-\boldsymbol{B}\|_{\boldsymbol{F}}^{2}$ with constraint on the spectral norm of B to be ‚â§ 1,Obtain B to minimize  with constraint on the spectral norm of B to be ‚â§ 1,\|\boldsymbol{X}-\boldsymbol{B}\|_{\boldsymbol{F}}^{2},I stumbled upon this problem where I have to I want to obtain B such that we Minimize $\|\boldsymbol{X}-\boldsymbol{B}\|_{\boldsymbol{F}}^{2}$ subject to the constraint that the spectral norm of ùë© is less than or equal to one. I tried to proceed using the SVD decomposition and trying to involve some inequalities of norms. but with no result. Any suggestions??,I stumbled upon this problem where I have to I want to obtain B such that we Minimize subject to the constraint that the spectral norm of ùë© is less than or equal to one. I tried to proceed using the SVD decomposition and trying to involve some inequalities of norms. but with no result. Any suggestions??,\|\boldsymbol{X}-\boldsymbol{B}\|_{\boldsymbol{F}}^{2},"['matrices', 'optimization', 'svd', 'matrix-norms']"
14,Connections between numerical calculation of eigenvalues and other topics,Connections between numerical calculation of eigenvalues and other topics,,"In the preface to his book Linear Algebra and Its Applications (2013), Peter Lax says It is with genuine regret that I omit a chapter on the numerical calculation of eigenvalues of self-adjoint matrices. Astonishing connections have been discovered recently between this important subject   and other seemingly unrelated topics. Can anyone comment on Lax's ""astonishing connections"", or suggest papers or course notes?","In the preface to his book Linear Algebra and Its Applications (2013), Peter Lax says It is with genuine regret that I omit a chapter on the numerical calculation of eigenvalues of self-adjoint matrices. Astonishing connections have been discovered recently between this important subject   and other seemingly unrelated topics. Can anyone comment on Lax's ""astonishing connections"", or suggest papers or course notes?",,"['matrices', 'reference-request', 'eigenvalues-eigenvectors', 'soft-question', 'numerical-linear-algebra']"
15,Concavity inequality for the matrix square root,Concavity inequality for the matrix square root,,"Let $A$ , $B$ and $C$ be symmetric, positive semi-definite matrices. Is it true that $$ \|(A + C)^{1/2} - (B + C)^{1/2}\| \leq \|A^{1/2} - B^{1/2}\|,$$ in either the 2 or Frobenius norm? It is clearly true when $A, B$ and $C$ commute, but the general case is less clear to me. In fact, even the particular case $B = 0$ does not seem obvious. Without loss of generality, it is clear that we can assume that $C$ is diagonal. We show that it is sufficient to prove to prove the inequality for the matrix with zeros everywhere except on any position $k$ on the diagonal, $$ (C_k)_{ij} = \begin{cases} 1 & \text{if } i=j=k\\ 0 & \text{otherwise} \end{cases} $$ Clearly,  if the inequality is true for one $C_k$ , it is true for any $C_k$ , by flipping the axes, and also for $C = \alpha C_k$ , for any $\alpha \geq 0$ , because \begin{align} \|(A + \alpha \, C_k)^{1/2} - (B + \alpha C_k)^{1/2}\|  &= \sqrt{\alpha} \|(A/\alpha +  C_k)^{1/2} - (B/\alpha + C_k)^{1/2}\|  \\ &\leq \sqrt{\alpha} \|(A/\alpha)^{1/2} - (B/\alpha)^{1/2}\| = \sqrt{\alpha} \|A^{1/2} - B^{1/2}\| \end{align} Now, a general diagonal $C$ can be decomposed as $C = \sum_{k=1}^{n} \alpha_k C_k$ . Applying the previous inequality (specialized for a matrix $C$ with only one nonzero diagonal element) repeatedly, we can remove the diagonal elements one by one \begin{align} &\|(A + \sum_{k=1}^{n}\alpha_k \, C_k)^{1/2} - (B + \sum_{k=1}^{n}\alpha_k \, C_k)^{1/2}\| \\ &\qquad = \|((A + \sum_{k=1}^{n-1}\alpha_k \, C_k) + \alpha_n C_n)^{1/2} - ((B + \sum_{k=1}^{n-1}\alpha_k \, C_k) + \alpha_n C_n)^{1/2}\| \\ &\qquad \leq \|(A + \sum_{k=1}^{n-1}\alpha_k \, C_k)^{1/2} - (B + \sum_{k=1}^{n-1}\alpha_k \, C_k)^{1/2}\| \\ &\qquad \leq \|(A + \sum_{k=1}^{n-2}\alpha_k \, C_k)^{1/2} - (B + \sum_{k=1}^{n-2}\alpha_k \, C_k)^{1/2}\| \\ &\qquad \leq \dots \leq \sqrt{\alpha} \|A^{1/2} - B^{1/2}\|. \end{align} Here are three ways of proving the inequality in 1 dimension, which I tried to generalize to the multidimensional case without success. Let us write $a$ , $b$ , $c$ instead of $A$ , $B$ , $C$ , to emphasize that we are working in one dimension, and let us assume without loss of generality that $a \leq b$ . Let us write: $$ f(c) = \sqrt{b + c} - \sqrt{a + c} $$ We calculate that the derivative of $f$ is given by $$ f'(c) = \frac{1}{2} \left( \frac{1}{\sqrt{b + c}} - \frac{1}{\sqrt{a + c}} \right) \leq 0, $$ and so $f(c) = f(0) + \int_{0}^{c} f'(x) \, d x  \leq f(0)$ . We have, by the fundamental theorem of calculus and a change of variable \begin{align}     \sqrt{b + c} - \sqrt{a + c} &= \int_{a + c}^{b + c} \frac{1}{2 \sqrt{x}} \, d x = \int_{a}^{b} \frac{1}{2 \sqrt{x + c}} \, d x  \\     &\leq \int_{a}^{b} \frac{1}{2 \sqrt{x}} \, d x = \sqrt{b} - \sqrt{a}. \end{align} Squaring the two sides of the inequality, we obtain $$ a + c - 2 \sqrt{a+ c} \, \sqrt{b + c} + b + c \leq a + b - 2 \sqrt{a} \sqrt{b}. $$ Simplifying and rearranging, $$ c + \sqrt{a} \sqrt{b} \leq \sqrt{a+ c} \, \sqrt{b + c} . $$ Squaring again $$ \require{cancel} \cancel{c^2 + a b} + 2 c \sqrt{a b} \leq \cancel{c^2 + ab} + ac + bc, $$ leading to $$ a  + b - 2 \sqrt{ab} = (\sqrt{b} - \sqrt{a})^2 \geq 0$$ . Numerical experiments suggest that the inequality is true in both the 2 and the Frobenius norm.  (One realization of) the following code prints 0.9998775. import numpy as np import scipy.linalg as la  n, d, ratios = 100000, 3, [] for i in range(n):     A = np.random.randn(d, d)     B = np.random.randn(d, d)     C = .1*np.random.randn(d, d)     A, B, C = A.dot(A.T), B.dot(B.T), C.dot(C.T)     lhs = la.norm(la.sqrtm(A + C) - la.sqrtm(B + C), ord='fro')     rhs = la.norm(la.sqrtm(A) - la.sqrtm(B), ord='fro')     ratios.append(lhs/rhs)  print(np.max(ratios))","Let , and be symmetric, positive semi-definite matrices. Is it true that in either the 2 or Frobenius norm? It is clearly true when and commute, but the general case is less clear to me. In fact, even the particular case does not seem obvious. Without loss of generality, it is clear that we can assume that is diagonal. We show that it is sufficient to prove to prove the inequality for the matrix with zeros everywhere except on any position on the diagonal, Clearly,  if the inequality is true for one , it is true for any , by flipping the axes, and also for , for any , because Now, a general diagonal can be decomposed as . Applying the previous inequality (specialized for a matrix with only one nonzero diagonal element) repeatedly, we can remove the diagonal elements one by one Here are three ways of proving the inequality in 1 dimension, which I tried to generalize to the multidimensional case without success. Let us write , , instead of , , , to emphasize that we are working in one dimension, and let us assume without loss of generality that . Let us write: We calculate that the derivative of is given by and so . We have, by the fundamental theorem of calculus and a change of variable Squaring the two sides of the inequality, we obtain Simplifying and rearranging, Squaring again leading to . Numerical experiments suggest that the inequality is true in both the 2 and the Frobenius norm.  (One realization of) the following code prints 0.9998775. import numpy as np import scipy.linalg as la  n, d, ratios = 100000, 3, [] for i in range(n):     A = np.random.randn(d, d)     B = np.random.randn(d, d)     C = .1*np.random.randn(d, d)     A, B, C = A.dot(A.T), B.dot(B.T), C.dot(C.T)     lhs = la.norm(la.sqrtm(A + C) - la.sqrtm(B + C), ord='fro')     rhs = la.norm(la.sqrtm(A) - la.sqrtm(B), ord='fro')     ratios.append(lhs/rhs)  print(np.max(ratios))","A B C  \|(A + C)^{1/2} - (B + C)^{1/2}\| \leq \|A^{1/2} - B^{1/2}\|, A, B C B = 0 C k 
(C_k)_{ij} = \begin{cases} 1 & \text{if } i=j=k\\ 0 & \text{otherwise} \end{cases}
 C_k C_k C = \alpha C_k \alpha \geq 0 \begin{align}
\|(A + \alpha \, C_k)^{1/2} - (B + \alpha C_k)^{1/2}\| 
&= \sqrt{\alpha} \|(A/\alpha +  C_k)^{1/2} - (B/\alpha + C_k)^{1/2}\|  \\
&\leq \sqrt{\alpha} \|(A/\alpha)^{1/2} - (B/\alpha)^{1/2}\|
= \sqrt{\alpha} \|A^{1/2} - B^{1/2}\|
\end{align} C C = \sum_{k=1}^{n} \alpha_k C_k C \begin{align}
&\|(A + \sum_{k=1}^{n}\alpha_k \, C_k)^{1/2} - (B + \sum_{k=1}^{n}\alpha_k \, C_k)^{1/2}\| \\
&\qquad = \|((A + \sum_{k=1}^{n-1}\alpha_k \, C_k) + \alpha_n C_n)^{1/2} - ((B + \sum_{k=1}^{n-1}\alpha_k \, C_k) + \alpha_n C_n)^{1/2}\| \\
&\qquad \leq \|(A + \sum_{k=1}^{n-1}\alpha_k \, C_k)^{1/2} - (B + \sum_{k=1}^{n-1}\alpha_k \, C_k)^{1/2}\| \\
&\qquad \leq \|(A + \sum_{k=1}^{n-2}\alpha_k \, C_k)^{1/2} - (B + \sum_{k=1}^{n-2}\alpha_k \, C_k)^{1/2}\| \\
&\qquad \leq \dots \leq \sqrt{\alpha} \|A^{1/2} - B^{1/2}\|.
\end{align} a b c A B C a \leq b  f(c) = \sqrt{b + c} - \sqrt{a + c}  f 
f'(c) = \frac{1}{2} \left( \frac{1}{\sqrt{b + c}} - \frac{1}{\sqrt{a + c}} \right) \leq 0,
 f(c) = f(0) + \int_{0}^{c} f'(x) \, d x  \leq f(0) \begin{align}
    \sqrt{b + c} - \sqrt{a + c} &= \int_{a + c}^{b + c} \frac{1}{2 \sqrt{x}} \, d x = \int_{a}^{b} \frac{1}{2 \sqrt{x + c}} \, d x  \\
    &\leq \int_{a}^{b} \frac{1}{2 \sqrt{x}} \, d x = \sqrt{b} - \sqrt{a}.
\end{align} 
a + c - 2 \sqrt{a+ c} \, \sqrt{b + c} + b + c \leq a + b - 2 \sqrt{a} \sqrt{b}.
 
c + \sqrt{a} \sqrt{b} \leq \sqrt{a+ c} \, \sqrt{b + c} .
 
\require{cancel} \cancel{c^2 + a b} + 2 c \sqrt{a b} \leq \cancel{c^2 + ab} + ac + bc,
  a  + b - 2 \sqrt{ab} = (\sqrt{b} - \sqrt{a})^2 \geq 0","['matrices', 'inequality']"
16,Matrix differentiation involving exponential term,Matrix differentiation involving exponential term,,"Let the scalar field $\Phi : \mathbb{R}^{m \times r} \times \mathbb{R}^{r \times n} \to \mathbb{R}$ be defined by $$\Phi(X,Y) := \sum_{i=1}^m \sum_{j=1}^n \exp \left( - \left( \frac{XY-A}{\gamma} \right)_{ij}^2 \right) $$ where $A\in \mathbb{R}^{m\times n}$ and $\gamma \in \mathbb{R}$ are given. I would like to compute the gradients $\nabla_X \Phi$ and $\nabla_Y \Phi$ . Could anyone please help me with the above differentiation please? I would appreciate a lot.",Let the scalar field be defined by where and are given. I would like to compute the gradients and . Could anyone please help me with the above differentiation please? I would appreciate a lot.,"\Phi : \mathbb{R}^{m \times r} \times \mathbb{R}^{r \times n} \to \mathbb{R} \Phi(X,Y) := \sum_{i=1}^m \sum_{j=1}^n \exp \left( - \left( \frac{XY-A}{\gamma} \right)_{ij}^2 \right)  A\in \mathbb{R}^{m\times n} \gamma \in \mathbb{R} \nabla_X \Phi \nabla_Y \Phi","['matrices', 'derivatives', 'matrix-calculus', 'scalar-fields']"
17,Relation between the eigenvalues of matrices conjugated by a homeomorphism.,Relation between the eigenvalues of matrices conjugated by a homeomorphism.,,"Let $A, B$ be $2\times 2 $ matrices satisfying: The eigenvalues $\lambda,\mu$ of $A$ satisfy $|\lambda|<1<|\mu|$ . The eigenvalues $\lambda',\mu'$ of $B$ satisfy $|\lambda'|<1<|\mu'|$ . There exists a homeomorphism $h:\mathbb{R}^2 \to \mathbb{R}^2$ such that $$h(A x) = B h(x), \  \forall \ x \in \mathbb{R}^2.$$ I'm reading the paper ""Generic Singularities of 3D Piecewise Smooth Dynamical Systems"", and the author ""kinda says"" that that 1) + 2) + 3) implies that $$\frac{\log(\lambda)}{\log(\mu)} =\frac{\log(\lambda')}{\log(\mu')}.  $$ My question: Does someone know if 1)+2)+3) $\Rightarrow$ $\frac{\log(\lambda)}{\log(\mu)} =\frac{\log(\lambda')}{\log(\mu')} $ ? Just some commentaries. I might be confusing something. However, my assumption is based on this phrase The references listed on the picture above are: Moreover, on Proposition 9, the author uses (in my view) the fact of existing a conjugation between two diffeomorphisms $\phi$ and $\phi_0$ , to conclude that $P(\phi) = P(\phi_0)$ and then construct a homeomorphism. Can anyone help me? EDIT: the author at no time says that $1) + 2) +3) \Rightarrow \frac{\log(\lambda)}{\log(\mu)} =\frac{\log(\lambda')}{\log(\mu')}$ , I understood what was written in the wrong way, it was my mistake.","Let be matrices satisfying: The eigenvalues of satisfy . The eigenvalues of satisfy . There exists a homeomorphism such that I'm reading the paper ""Generic Singularities of 3D Piecewise Smooth Dynamical Systems"", and the author ""kinda says"" that that 1) + 2) + 3) implies that My question: Does someone know if 1)+2)+3) ? Just some commentaries. I might be confusing something. However, my assumption is based on this phrase The references listed on the picture above are: Moreover, on Proposition 9, the author uses (in my view) the fact of existing a conjugation between two diffeomorphisms and , to conclude that and then construct a homeomorphism. Can anyone help me? EDIT: the author at no time says that , I understood what was written in the wrong way, it was my mistake.","A, B 2\times 2  \lambda,\mu A |\lambda|<1<|\mu| \lambda',\mu' B |\lambda'|<1<|\mu'| h:\mathbb{R}^2 \to \mathbb{R}^2 h(A x) = B h(x), \  \forall \ x \in \mathbb{R}^2. \frac{\log(\lambda)}{\log(\mu)} =\frac{\log(\lambda')}{\log(\mu')}.   \Rightarrow \frac{\log(\lambda)}{\log(\mu)} =\frac{\log(\lambda')}{\log(\mu')}  \phi \phi_0 P(\phi) = P(\phi_0) 1) + 2) +3) \Rightarrow \frac{\log(\lambda)}{\log(\mu)} =\frac{\log(\lambda')}{\log(\mu')}","['matrices', 'dynamical-systems', 'diffeomorphism']"
18,Is the algebraic structure of the full matrix ring preserved by every Lie algebra endomorphism?,Is the algebraic structure of the full matrix ring preserved by every Lie algebra endomorphism?,,"Let $A = \mathcal M_n(k)$ be the full matrix algebra over a field $k$ . If $\phi:A\to A$ is a nonzero endomorphism of $A$ as a Lie algebra, must it automatically be an endomorphism of $A$ as a unital $k$ -algebra? If not, what would be necessary conditions? EDIT As TorstenSchoeneberg pointed out in the comments, a necessary condition is that $\phi$ is the identity on $k$ . Could this also be a sufficient condition?","Let be the full matrix algebra over a field . If is a nonzero endomorphism of as a Lie algebra, must it automatically be an endomorphism of as a unital -algebra? If not, what would be necessary conditions? EDIT As TorstenSchoeneberg pointed out in the comments, a necessary condition is that is the identity on . Could this also be a sufficient condition?",A = \mathcal M_n(k) k \phi:A\to A A A k \phi k,"['matrices', 'lie-algebras', 'ring-homomorphism']"
19,"On the radical of a certain ideal of sixteen variable polynomial ring, generated by the entries of certain matrices","On the radical of a certain ideal of sixteen variable polynomial ring, generated by the entries of certain matrices",,"Consider the polynomial ring $R=\mathbb C[x_1,x_2,...,x_{16}]$, and set $$X=\begin{pmatrix} x_1 &x_2&x_3 &x_4\\ x_5&x_6& x_7&x_8\\x_9&x_{10}&x_{11}&x_{12}\\x_{13}&x_{14}&x_{15}&x_{16}\end{pmatrix}.$$ Now, using these three matrices $$L=\begin{pmatrix}0&-1&0&0\\1&0&0&0\\0&0&0&-1\\0&0&1&0 \end{pmatrix}$$ $$M=\begin{pmatrix}0&0&0&-1\\0&0&-1&0\\0&1&0&0\\1&0&0&0\end{pmatrix}$$ $$N=\begin{pmatrix}0&0&-1&0\\0&0&0&1\\1&0&0&0\\0&-1&0&0\end{pmatrix}$$ we create polynomials $f_i, g_i,$ and $h_i$ in the following way: $$XLX^t-L=\begin{pmatrix} f_1 &f_2&f_3 &f_4\\ f_5&f_6& f_7&f_8\\f_9&f_{10}&f_{11}&f_{12}\\f_{13}&f_{14}&f_{15}&f_{16}\end{pmatrix}$$ $$XMX^t-M=\begin{pmatrix} g_1 &g_2&g_3 &g_4\\ g_5&g_6& g_7&g_8\\g_9&g_{10}&g_{11}&g_{12}\\g_{13}&g_{14}&g_{15}&g_{16}\end{pmatrix}$$ $$XNX^t-N=\begin{pmatrix} h_1 &h_2&h_3 &h_4\\ h_5&h_6& h_7&h_8\\h_9&h_{10}&h_{11}&h_{12}\\h_{13}&h_{14}&h_{15}&h_{16}\end{pmatrix}$$ Finally, let $I = (f_i, g_i, h_i)$ be the ideal generated by these $48$ polynomials. Then how to show that the radical of $I$, i.e. $\sqrt I$, is generated by twelve linear polynomials and one quadratic polynomial ? I have no idea how to approach this problem; may be use Nullstelensatz ... ? Please help NOTE : All the matrices $L,M,N$ are orthogonal , so the three defining equations can be written as $(XL)(LX)^t=(XM)(MX)^t=(XN)(NX)^t=Id$. Now if we can find some pattern in $XL,LX,MX,XM,NX,XN$ then it could be helpful to find the zero set of the ideal $I$ ...  Also $L,M,N$ are skew symmetric matrices and as @Balaji sb noted, $LM=-N$ ... this means $L,M,N$ works as the $i,j,k$ in the Quaternion ring ...","Consider the polynomial ring $R=\mathbb C[x_1,x_2,...,x_{16}]$, and set $$X=\begin{pmatrix} x_1 &x_2&x_3 &x_4\\ x_5&x_6& x_7&x_8\\x_9&x_{10}&x_{11}&x_{12}\\x_{13}&x_{14}&x_{15}&x_{16}\end{pmatrix}.$$ Now, using these three matrices $$L=\begin{pmatrix}0&-1&0&0\\1&0&0&0\\0&0&0&-1\\0&0&1&0 \end{pmatrix}$$ $$M=\begin{pmatrix}0&0&0&-1\\0&0&-1&0\\0&1&0&0\\1&0&0&0\end{pmatrix}$$ $$N=\begin{pmatrix}0&0&-1&0\\0&0&0&1\\1&0&0&0\\0&-1&0&0\end{pmatrix}$$ we create polynomials $f_i, g_i,$ and $h_i$ in the following way: $$XLX^t-L=\begin{pmatrix} f_1 &f_2&f_3 &f_4\\ f_5&f_6& f_7&f_8\\f_9&f_{10}&f_{11}&f_{12}\\f_{13}&f_{14}&f_{15}&f_{16}\end{pmatrix}$$ $$XMX^t-M=\begin{pmatrix} g_1 &g_2&g_3 &g_4\\ g_5&g_6& g_7&g_8\\g_9&g_{10}&g_{11}&g_{12}\\g_{13}&g_{14}&g_{15}&g_{16}\end{pmatrix}$$ $$XNX^t-N=\begin{pmatrix} h_1 &h_2&h_3 &h_4\\ h_5&h_6& h_7&h_8\\h_9&h_{10}&h_{11}&h_{12}\\h_{13}&h_{14}&h_{15}&h_{16}\end{pmatrix}$$ Finally, let $I = (f_i, g_i, h_i)$ be the ideal generated by these $48$ polynomials. Then how to show that the radical of $I$, i.e. $\sqrt I$, is generated by twelve linear polynomials and one quadratic polynomial ? I have no idea how to approach this problem; may be use Nullstelensatz ... ? Please help NOTE : All the matrices $L,M,N$ are orthogonal , so the three defining equations can be written as $(XL)(LX)^t=(XM)(MX)^t=(XN)(NX)^t=Id$. Now if we can find some pattern in $XL,LX,MX,XM,NX,XN$ then it could be helpful to find the zero set of the ideal $I$ ...  Also $L,M,N$ are skew symmetric matrices and as @Balaji sb noted, $LM=-N$ ... this means $L,M,N$ works as the $i,j,k$ in the Quaternion ring ...",,"['matrices', 'algebraic-geometry', 'polynomials', 'commutative-algebra', 'noncommutative-algebra']"
20,rank inequality for Hadamard product,rank inequality for Hadamard product,,"How I can show that $$ \operatorname{rank}(A\circ B) \leq \operatorname{rank}(A)\operatorname{rank}(B) $$ where $A,B$ are rectangular matrices and $\circ$ is the Hadamard product between the two. Similarly, how can we show that  if $A$ has rank d, then $\operatorname{rank}(A\circ A)$ is at most $d+1 \choose 2$ ?","How I can show that $$ \operatorname{rank}(A\circ B) \leq \operatorname{rank}(A)\operatorname{rank}(B) $$ where $A,B$ are rectangular matrices and $\circ$ is the Hadamard product between the two. Similarly, how can we show that  if $A$ has rank d, then $\operatorname{rank}(A\circ A)$ is at most $d+1 \choose 2$ ?",,"['matrices', 'matrix-rank', 'hadamard-product']"
21,What is a gradient matrix?,What is a gradient matrix?,,"I know what the gradient of a function is, but I have this problem which seems to have something else in mind. The problem reads Write the oscillator equation $y''-\cos y=0$ as a system of first-order equations with $x_1=y$ and $x_2=y'$ and find the gradient matrix $\nabla f$, and compute its eigenvalue as a function of $x$.  Draw some typical trajectories in phase space and indicate where you expect divergence, based on eigenvalue analysis. Does this just mean to turn the system into a matrix like $$\begin{pmatrix}x_1\\x_2\end{pmatrix}'=\begin{pmatrix}0&1\\0&0\end{pmatrix}\begin{pmatrix}x_1\\x_2\end{pmatrix}+\begin{pmatrix}0\\-\cos x_1\end{pmatrix}$$ Would the coefficient matrix be the gradient matrix?  Or would it be the fundamental matrix solution?  I've tried googling the term and nothing seems to be what this problem is referring to.","I know what the gradient of a function is, but I have this problem which seems to have something else in mind. The problem reads Write the oscillator equation $y''-\cos y=0$ as a system of first-order equations with $x_1=y$ and $x_2=y'$ and find the gradient matrix $\nabla f$, and compute its eigenvalue as a function of $x$.  Draw some typical trajectories in phase space and indicate where you expect divergence, based on eigenvalue analysis. Does this just mean to turn the system into a matrix like $$\begin{pmatrix}x_1\\x_2\end{pmatrix}'=\begin{pmatrix}0&1\\0&0\end{pmatrix}\begin{pmatrix}x_1\\x_2\end{pmatrix}+\begin{pmatrix}0\\-\cos x_1\end{pmatrix}$$ Would the coefficient matrix be the gradient matrix?  Or would it be the fundamental matrix solution?  I've tried googling the term and nothing seems to be what this problem is referring to.",,"['matrices', 'ordinary-differential-equations']"
22,Solving a large $n \times n$ Lights Out Board,Solving a large  Lights Out Board,n \times n,"My motivation is solving arbitrary Lights Out puzzles with $n^2$ cells with a method given in MathWorld : basically imagine the game as a square grid graph and construct an $n^2 \times n^2$ ""adjacency matrix"" $A$ . Then for initial configuration $b$ , solving $Ax = b$ gives you button presses to solve the game. For a matrix of size $n^2 \times n^2$ , normally Gaussian elimination takes $O((n^2)^3) = O(n^6)$ time, but maybe having a sparse binary matrix (by binary I mean all operations are mod 2, over field $\mathbb Z / 2 \mathbb Z$ ) can make the operation faster. I know the matrix is very sparse because every cell only has at most 4 neighbors, so almost all the matrix entries are zero. Any references to algorithms or software implementations that can solve binary sparse matrices efficiently are appreciated.","My motivation is solving arbitrary Lights Out puzzles with cells with a method given in MathWorld : basically imagine the game as a square grid graph and construct an ""adjacency matrix"" . Then for initial configuration , solving gives you button presses to solve the game. For a matrix of size , normally Gaussian elimination takes time, but maybe having a sparse binary matrix (by binary I mean all operations are mod 2, over field ) can make the operation faster. I know the matrix is very sparse because every cell only has at most 4 neighbors, so almost all the matrix entries are zero. Any references to algorithms or software implementations that can solve binary sparse matrices efficiently are appreciated.",n^2 n^2 \times n^2 A b Ax = b n^2 \times n^2 O((n^2)^3) = O(n^6) \mathbb Z / 2 \mathbb Z,"['matrices', 'reference-request', 'puzzle', 'binary', 'gaussian-elimination']"
23,Apply chain rule and product rule on matrix differentiation,Apply chain rule and product rule on matrix differentiation,,"I have a question on taking the time derivative (i.e. w.r.t. t variable in the following function) of a quadratic function $V=x(t)^TPx(t)$. $t\in R^+$ has the physical meaning of time. $x(t)\in R^n$, $P\in R^{n\times n}$ is a positive definite matrix. x(t) has its own linear dynamics:$\dot{x}(t)=Ax(t),A\in R^{n\times n}$. I ran in to this problem when I was reading materials of Lyapunov function of linear systems, and from the materials I knew the answer should be $\dot{V}=x^T(A^TP+PA)x$. The above correct answer can be obtained by applying the product rule: $\dot{V}=\dot{x}^TPx+x^TP\dot{x}=(Ax)^TPx+x^TP(Ax)=x^T(A^TP+PA)x$. But when I was deriving it, I use the chain rule, and got different answer: $\dot{V}=\frac{\partial V}{\partial x}\dot{x}=\frac{\partial x^TPx}{\partial x}\dot{x}=x^T(P+P^T)(Ax)=x^T(PA+P^TA)x=2x^TPAx$, the last step was becasue of the symmetric property of $P$ being positive definite. Was there anything wrong I did with applying the chain rule? or should product rule being applied before chain rule? Thank you for your answer in advance!","I have a question on taking the time derivative (i.e. w.r.t. t variable in the following function) of a quadratic function $V=x(t)^TPx(t)$. $t\in R^+$ has the physical meaning of time. $x(t)\in R^n$, $P\in R^{n\times n}$ is a positive definite matrix. x(t) has its own linear dynamics:$\dot{x}(t)=Ax(t),A\in R^{n\times n}$. I ran in to this problem when I was reading materials of Lyapunov function of linear systems, and from the materials I knew the answer should be $\dot{V}=x^T(A^TP+PA)x$. The above correct answer can be obtained by applying the product rule: $\dot{V}=\dot{x}^TPx+x^TP\dot{x}=(Ax)^TPx+x^TP(Ax)=x^T(A^TP+PA)x$. But when I was deriving it, I use the chain rule, and got different answer: $\dot{V}=\frac{\partial V}{\partial x}\dot{x}=\frac{\partial x^TPx}{\partial x}\dot{x}=x^T(P+P^T)(Ax)=x^T(PA+P^TA)x=2x^TPAx$, the last step was becasue of the symmetric property of $P$ being positive definite. Was there anything wrong I did with applying the chain rule? or should product rule being applied before chain rule? Thank you for your answer in advance!",,"['matrices', 'derivatives']"
24,Algebraic or Analytic Proof of a Polynomial Identity,Algebraic or Analytic Proof of a Polynomial Identity,,"Let $m$, $n$, and $r$ be integers with $0\leq r \leq \min\{m,n\}$.  Define   $$f_{m,n,r}(q):=\left(\prod_{j=1}^r\,\left(q^m-q^{j-1}\right)\right)\,\left(\sum_{\substack{{j_1,\ldots,j_r\in\mathbb{Z}_{\geq 0}}\\{j_1+j_2+\ldots+j_r\leq n-r}}}\,q^{\sum_{i=1}^r\,i\,j_i}\right)\,,$$   so that   $$f_{n,m,r}(q)=\left(\prod_{j=1}^r\,\left(q^n-q^{j-1}\right)\right)\,\left(\sum_{\substack{{j_1,\ldots,j_r\in\mathbb{Z}_{\geq 0}}\\{j_1+j_2+\ldots+j_r\leq m-r}}}\,q^{\sum_{i=1}^r\,i\,j_i}\right)$$   as polynomials over $\mathbb{Z}$ in the variable $q$.  Prove that $$f_{m,n,r}(q)=f_{n,m,r}(q)\,.$$ Here is a combinatorial proof of this identity.  The polynomial $f_{m,n,r}(q)$ counts the number of $m$-by-$n$ matrices over $\mathbb{F}_q$ of rank $r$, when $q$ is a power of a prime natural number.  Since the transpose map from $\text{Mat}_{m\times n}\left(\mathbb{F}_q\right)\to\text{Mat}_{n\times m}\left(\mathbb{F}_q\right)$ is a bijection that preserves rank, we conclude that the number of $n$-by-$m$ matrices over $\mathbb{F}_q$ of rank $r$ is also $f_{m,n,r}(q)$.  Ergo, $f_{m,n,r}(q)=f_{n,m,r}(q)$ whenever $q$ is a prime power, whence the equality $f_{m,n,r}(q)=f_{n,m,r}(q)$ is indeed an identity in $\mathbb{Z}[q]$.  See here .","Let $m$, $n$, and $r$ be integers with $0\leq r \leq \min\{m,n\}$.  Define   $$f_{m,n,r}(q):=\left(\prod_{j=1}^r\,\left(q^m-q^{j-1}\right)\right)\,\left(\sum_{\substack{{j_1,\ldots,j_r\in\mathbb{Z}_{\geq 0}}\\{j_1+j_2+\ldots+j_r\leq n-r}}}\,q^{\sum_{i=1}^r\,i\,j_i}\right)\,,$$   so that   $$f_{n,m,r}(q)=\left(\prod_{j=1}^r\,\left(q^n-q^{j-1}\right)\right)\,\left(\sum_{\substack{{j_1,\ldots,j_r\in\mathbb{Z}_{\geq 0}}\\{j_1+j_2+\ldots+j_r\leq m-r}}}\,q^{\sum_{i=1}^r\,i\,j_i}\right)$$   as polynomials over $\mathbb{Z}$ in the variable $q$.  Prove that $$f_{m,n,r}(q)=f_{n,m,r}(q)\,.$$ Here is a combinatorial proof of this identity.  The polynomial $f_{m,n,r}(q)$ counts the number of $m$-by-$n$ matrices over $\mathbb{F}_q$ of rank $r$, when $q$ is a power of a prime natural number.  Since the transpose map from $\text{Mat}_{m\times n}\left(\mathbb{F}_q\right)\to\text{Mat}_{n\times m}\left(\mathbb{F}_q\right)$ is a bijection that preserves rank, we conclude that the number of $n$-by-$m$ matrices over $\mathbb{F}_q$ of rank $r$ is also $f_{m,n,r}(q)$.  Ergo, $f_{m,n,r}(q)=f_{n,m,r}(q)$ whenever $q$ is a prime power, whence the equality $f_{m,n,r}(q)=f_{n,m,r}(q)$ is indeed an identity in $\mathbb{Z}[q]$.  See here .",,"['matrices', 'complex-analysis', 'polynomials', 'finite-fields', 'analytic-functions']"
25,Determinant of a large block matrix,Determinant of a large block matrix,,"$\newcommand{\lmt}{\left[\begin{matrix}}$ $\newcommand{\rmt}{\end{matrix}\right]}$ Hi, I was reading through a proof of the number of domino tilings of a $(2n)\times(2n)$ chessboard, and somewhere in the proof was the following unjustified claim: Let $A=\lmt 0&1&0&\cdots&0\\ -1&0&1&\ddots&\vdots\\ 0&-1&0&\ddots&0\\ \vdots&\ddots&\ddots&\ddots&1\\ 0&\cdots&0&-1&0 \rmt$ and $B=\lmt 0&1&0&\cdots&0\\ 1&0&1&\ddots&\vdots\\ 0&1&0&\ddots&0\\ \vdots&\ddots&\ddots&\ddots&1\\ 0&\cdots&0&1&0 \rmt$. In case my notation isn't clear, they have $\pm1$ on the superdiagonal and subdiagonal and $0$ everywhere else. Let $C$ be the matrix with blocks $\lmt -A&I&0&\cdots&0\\ I&-A&I&\ddots&\vdots\\ 0&I&-A&\ddots&0\\ \vdots&\ddots&\ddots&\ddots&I\\ 0&\cdots&0&I&-A \rmt$. Let $p_B$ be the characteristic polynomial of $B$. Then, the claim is that $$ \det C = \det p_B(A). $$ This was stated without proof, so I'm wondering if this follows from a well-known theorem, or if there's a slick proof of it. Also, I'm curious to what extent this claim generalizes. Thanks!","$\newcommand{\lmt}{\left[\begin{matrix}}$ $\newcommand{\rmt}{\end{matrix}\right]}$ Hi, I was reading through a proof of the number of domino tilings of a $(2n)\times(2n)$ chessboard, and somewhere in the proof was the following unjustified claim: Let $A=\lmt 0&1&0&\cdots&0\\ -1&0&1&\ddots&\vdots\\ 0&-1&0&\ddots&0\\ \vdots&\ddots&\ddots&\ddots&1\\ 0&\cdots&0&-1&0 \rmt$ and $B=\lmt 0&1&0&\cdots&0\\ 1&0&1&\ddots&\vdots\\ 0&1&0&\ddots&0\\ \vdots&\ddots&\ddots&\ddots&1\\ 0&\cdots&0&1&0 \rmt$. In case my notation isn't clear, they have $\pm1$ on the superdiagonal and subdiagonal and $0$ everywhere else. Let $C$ be the matrix with blocks $\lmt -A&I&0&\cdots&0\\ I&-A&I&\ddots&\vdots\\ 0&I&-A&\ddots&0\\ \vdots&\ddots&\ddots&\ddots&I\\ 0&\cdots&0&I&-A \rmt$. Let $p_B$ be the characteristic polynomial of $B$. Then, the claim is that $$ \det C = \det p_B(A). $$ This was stated without proof, so I'm wondering if this follows from a well-known theorem, or if there's a slick proof of it. Also, I'm curious to what extent this claim generalizes. Thanks!",,"['matrices', 'determinant']"
26,Transformation that preserves an increasing ratio between vectors,Transformation that preserves an increasing ratio between vectors,,"Consider two vectors $x=(x_1,x_2,\ldots,x_n)$, $y= (y_1,y_2,\ldots,y_n)$ such that all $x_i,y_i>0$ and  \begin{align} \frac{y_1}{x_1}\le \frac{y_2}{x_2}\le\cdots\le \frac{y_n}{x_n} \end{align} Now consider an upper triangular matrix $A$ with elements $a_{ij}\ge 0$ I want to prove (or come up with conditions on $A$ so) that  \begin{align} \frac{[yA]_1}{[xA]_1}\le \frac{[yA]_2}{[xA]_2}\le\cdots\le \frac{[yA]_n}{[xA]_n} \end{align} For example, for $n=3$, \begin{align} A = \left( \begin{array}{ccc} a_{11} & a_{12} & a_{13} \\ 0 & a_{22} & a_{23} \\ 0 & 0 & a_{33} \end{array} \right) \end{align} We would want to show \begin{align} \frac{a_{11}y_1}{a_{11}x_1}\le \frac{a_{12}y_1 + a_{22}y_2}{a_{12}x_1+a_{22}x_2}\le \frac{a_{13}y_1 + a_{23}y_2 + a_{33}y_3}{a_{13}x_1 + a_{23}x_2 + a_{33}x_3} \end{align} I am interested in proving this for general $n$. Update: I'm thinking that a sufficient condition for this to hold is that the column sums are increasing in the column index. That is, for $n=3$, $a_{11} \le a_{12} + a_{22} \le a_{13}+a_{23}+a_{33}$. Is this true?","Consider two vectors $x=(x_1,x_2,\ldots,x_n)$, $y= (y_1,y_2,\ldots,y_n)$ such that all $x_i,y_i>0$ and  \begin{align} \frac{y_1}{x_1}\le \frac{y_2}{x_2}\le\cdots\le \frac{y_n}{x_n} \end{align} Now consider an upper triangular matrix $A$ with elements $a_{ij}\ge 0$ I want to prove (or come up with conditions on $A$ so) that  \begin{align} \frac{[yA]_1}{[xA]_1}\le \frac{[yA]_2}{[xA]_2}\le\cdots\le \frac{[yA]_n}{[xA]_n} \end{align} For example, for $n=3$, \begin{align} A = \left( \begin{array}{ccc} a_{11} & a_{12} & a_{13} \\ 0 & a_{22} & a_{23} \\ 0 & 0 & a_{33} \end{array} \right) \end{align} We would want to show \begin{align} \frac{a_{11}y_1}{a_{11}x_1}\le \frac{a_{12}y_1 + a_{22}y_2}{a_{12}x_1+a_{22}x_2}\le \frac{a_{13}y_1 + a_{23}y_2 + a_{33}y_3}{a_{13}x_1 + a_{23}x_2 + a_{33}x_3} \end{align} I am interested in proving this for general $n$. Update: I'm thinking that a sufficient condition for this to hold is that the column sums are increasing in the column index. That is, for $n=3$, $a_{11} \le a_{12} + a_{22} \le a_{13}+a_{23}+a_{33}$. Is this true?",,"['matrices', 'inequality', 'fractions']"
27,Prove that trace of a matrix is $0$.,Prove that trace of a matrix is .,0,"Let $ n\geq 2 $ and $ A,B,C  \in M_{n}(\mathbb{C}) $ be three matrices so that $$ A^{2}B+BA^{2}=2ABA $$ and $ C=AB-BA $. Prove that $ \mbox{tr}(C^{k})=0,\forall k\in \mathbb{N}. $ I tried solving it by mathematical induction, but it didn't work. The only thing I know is that $ \mbox{tr}(C)=0. $","Let $ n\geq 2 $ and $ A,B,C  \in M_{n}(\mathbb{C}) $ be three matrices so that $$ A^{2}B+BA^{2}=2ABA $$ and $ C=AB-BA $. Prove that $ \mbox{tr}(C^{k})=0,\forall k\in \mathbb{N}. $ I tried solving it by mathematical induction, but it didn't work. The only thing I know is that $ \mbox{tr}(C)=0. $",,"['matrices', 'matrix-equations', 'trace']"
28,A matrix norm inequality,A matrix norm inequality,,"Given a real $m\times n$ matrix $C$, a $m\times m$ diagonal matrix $p$ whose diagonal entries $p_{ii}$ are either 0 or 1, and a $n\times n$ diagonal matrix $q$ whose diagonal entries $q_{ii}$ are either 0 or 1. Let $P(\alpha)=\frac{\exp(i\alpha)}{2}p + \frac{I-p}{2}$, a diagonal matrix whose diagonal elements are either $1/2$ or $\exp(i\alpha)/2$. Let $Q(\alpha)=\frac{\exp(i\alpha)}{2}q + \frac{I-q}{2}$, a diagonal matrix whose diagonal elements are either $1/2$ or $\exp(i\alpha)/2$. Then we can construct the function $$n(\alpha)=\frac{\|P(\alpha) C + C Q(\alpha)\|}{\|C\|}$$ where the norm is the operator norm . The figure below shows all possible $n(\alpha)$ curves for a $7\times 7$ matrix $C$. We are interested in the behaviour of $n(\alpha)$ for $\alpha\in[0,\pi]$. We can prove easily that $n(\alpha)\leq 1$: $$\frac{\|P(\alpha) C + C Q(\alpha)\|}{\|C\|}\leq \frac{\|P(\alpha) C \|+\| C Q(\alpha)\|}{\|C\|}\leq \frac{\|P(\alpha)\|\| C \|+\| C \|\|Q(\alpha)\|}{\|C\|}\\ \leq \frac{\frac{1}{2}\| C \|+\| C \|\frac{1}{2}}{\|C\|} \leq 1$$ In the case where $P(\alpha)=I/2$, we can easily prove that $n(\alpha)$ is a nonincreasing function of $\alpha$: $$n(\alpha+\delta_{\alpha})=\frac{\|C/2 + C Q(\alpha+\delta_{\alpha})\|}{\|C\|}=\frac{\|C (I/2+ Q(\alpha+\delta_{\alpha}))\|}{\|C\|}\\ =\frac{\|C (I/2+ Q(\alpha))(I/2+ Q(\alpha))^{-1}(I/2+ Q(\alpha+\delta_{\alpha}))\|}{\|C\|}\\ \leq n(\alpha)\|(I/2+ Q(\alpha))^{-1}(I/2+ Q(\alpha+\delta_{\alpha}))\|$$ $(I/2+ Q(\alpha))^{-1}(I/2+ Q(\alpha+\delta_{\alpha}))$ is a diagonal matrix with diagonal elements either 1 or $\frac{1+\exp(i(\alpha+\delta_{\alpha}))}{1+\exp(i\alpha)}$. Note $|\frac{1+\exp(i(\alpha+\delta_{\alpha}))}{1+\exp(i\alpha)}|\leq 1$ for relevant parameter values ($\alpha\in[0,\pi],\delta_{\alpha}>0,\alpha+\delta_{\alpha}\leq\pi$), so $\|(I/2+ Q(\alpha))^{-1}(I/2+ Q(\alpha+\delta_{\alpha}))\|\leq 1$ so $n(\alpha+\delta_{\alpha})\leq n(\alpha)$, so $n(\alpha)$ is indeed a non-increasing function of $\alpha$. So, on to the actual question I suspect that $n(\alpha)$ is always a non-increasing function of $\alpha$, not just in the $P=I/2$ case as shown above, but the proof technique used above does not work in the general case. How could I prove this? It also seems like $n(\alpha)\geq \cos(\alpha/2)$. How could I prove this?","Given a real $m\times n$ matrix $C$, a $m\times m$ diagonal matrix $p$ whose diagonal entries $p_{ii}$ are either 0 or 1, and a $n\times n$ diagonal matrix $q$ whose diagonal entries $q_{ii}$ are either 0 or 1. Let $P(\alpha)=\frac{\exp(i\alpha)}{2}p + \frac{I-p}{2}$, a diagonal matrix whose diagonal elements are either $1/2$ or $\exp(i\alpha)/2$. Let $Q(\alpha)=\frac{\exp(i\alpha)}{2}q + \frac{I-q}{2}$, a diagonal matrix whose diagonal elements are either $1/2$ or $\exp(i\alpha)/2$. Then we can construct the function $$n(\alpha)=\frac{\|P(\alpha) C + C Q(\alpha)\|}{\|C\|}$$ where the norm is the operator norm . The figure below shows all possible $n(\alpha)$ curves for a $7\times 7$ matrix $C$. We are interested in the behaviour of $n(\alpha)$ for $\alpha\in[0,\pi]$. We can prove easily that $n(\alpha)\leq 1$: $$\frac{\|P(\alpha) C + C Q(\alpha)\|}{\|C\|}\leq \frac{\|P(\alpha) C \|+\| C Q(\alpha)\|}{\|C\|}\leq \frac{\|P(\alpha)\|\| C \|+\| C \|\|Q(\alpha)\|}{\|C\|}\\ \leq \frac{\frac{1}{2}\| C \|+\| C \|\frac{1}{2}}{\|C\|} \leq 1$$ In the case where $P(\alpha)=I/2$, we can easily prove that $n(\alpha)$ is a nonincreasing function of $\alpha$: $$n(\alpha+\delta_{\alpha})=\frac{\|C/2 + C Q(\alpha+\delta_{\alpha})\|}{\|C\|}=\frac{\|C (I/2+ Q(\alpha+\delta_{\alpha}))\|}{\|C\|}\\ =\frac{\|C (I/2+ Q(\alpha))(I/2+ Q(\alpha))^{-1}(I/2+ Q(\alpha+\delta_{\alpha}))\|}{\|C\|}\\ \leq n(\alpha)\|(I/2+ Q(\alpha))^{-1}(I/2+ Q(\alpha+\delta_{\alpha}))\|$$ $(I/2+ Q(\alpha))^{-1}(I/2+ Q(\alpha+\delta_{\alpha}))$ is a diagonal matrix with diagonal elements either 1 or $\frac{1+\exp(i(\alpha+\delta_{\alpha}))}{1+\exp(i\alpha)}$. Note $|\frac{1+\exp(i(\alpha+\delta_{\alpha}))}{1+\exp(i\alpha)}|\leq 1$ for relevant parameter values ($\alpha\in[0,\pi],\delta_{\alpha}>0,\alpha+\delta_{\alpha}\leq\pi$), so $\|(I/2+ Q(\alpha))^{-1}(I/2+ Q(\alpha+\delta_{\alpha}))\|\leq 1$ so $n(\alpha+\delta_{\alpha})\leq n(\alpha)$, so $n(\alpha)$ is indeed a non-increasing function of $\alpha$. So, on to the actual question I suspect that $n(\alpha)$ is always a non-increasing function of $\alpha$, not just in the $P=I/2$ case as shown above, but the proof technique used above does not work in the general case. How could I prove this? It also seems like $n(\alpha)\geq \cos(\alpha/2)$. How could I prove this?",,"['matrices', 'normed-spaces']"
29,Frobenius norm and submultiplicativity,Frobenius norm and submultiplicativity,,"I read (page 8 here ) that if $A$ and $B$ are rectangular matrices so that the product $AB$ is defined, then  $$(1)\quad||AB||_F^2\leq ||A||_F^2||B||_F^2$$ Does that mean that the inequality above also holds when the number of rows of $A$ is larger than the number of columns of $B$? The justification (Cauchy Swartz): $$||AB||_F^2=\sum_{i=1}^n\sum_{j=1}^k(a_i^\top b_j)^2\leq \sum_{i=1}^n\sum_{j=1}^k||a_i||_2^2||b_j||^2_2=||A||_F^2||B||_F^2$$ does not require $k$ (the number of columns of $B$) to equal $n$ (the number of rows of $A$). Intuitively, you could also add imaginary columns of 0's to $B$, so I can believe the claim. On the other hand, in other places I only see $(1)$  claimed for matrix of the same size and have had a hard time finding it claimed for the more general case (where $A$ $B$ are merely multiplicative)  online.","I read (page 8 here ) that if $A$ and $B$ are rectangular matrices so that the product $AB$ is defined, then  $$(1)\quad||AB||_F^2\leq ||A||_F^2||B||_F^2$$ Does that mean that the inequality above also holds when the number of rows of $A$ is larger than the number of columns of $B$? The justification (Cauchy Swartz): $$||AB||_F^2=\sum_{i=1}^n\sum_{j=1}^k(a_i^\top b_j)^2\leq \sum_{i=1}^n\sum_{j=1}^k||a_i||_2^2||b_j||^2_2=||A||_F^2||B||_F^2$$ does not require $k$ (the number of columns of $B$) to equal $n$ (the number of rows of $A$). Intuitively, you could also add imaginary columns of 0's to $B$, so I can believe the claim. On the other hand, in other places I only see $(1)$  claimed for matrix of the same size and have had a hard time finding it claimed for the more general case (where $A$ $B$ are merely multiplicative)  online.",,"['matrices', 'normed-spaces', 'matrix-norms']"
30,Get bounding rectangle segments of a rotated rectangle (matrix?),Get bounding rectangle segments of a rotated rectangle (matrix?),,"My problem: I have: $x$ , $y$ & $\alpha$ and the aspect ratio $o$ (long): $p$ (short) (red rectangle) I want to have $n$ & $m$ in dependancy of $x, y, \alpha, o, p$ I tried to figure it out with cos , sin and tan but I don't get a solution. My math teacher said something about the matrix of rotation, but I don't know this method. I'm also fine with the points where the red corners are on the black rectangle. Is this possible ? Thanks in advance ;)","My problem: I have: $x$ , $y$ & $\alpha$ and the aspect ratio $o$ (long): $p$ (short) (red rectangle) I want to have $n$ & $m$ in dependancy of $x, y, \alpha, o, p$ I tried to figure it out with cos , sin and tan but I don't get a solution. My math teacher said something about the matrix of rotation, but I don't know this method. I'm also fine with the points where the red corners are on the black rectangle. Is this possible ? Thanks in advance ;)",,"['matrices', 'trigonometry', 'rotations']"
31,Relation between coefficients of a matrix and its eigenvalues,Relation between coefficients of a matrix and its eigenvalues,,"Let $A \in \mathbb{R}^{nxn}$ be a matrix and $\rho(A)$ its largest eigenvalue (or largest module of its eigenvalues). Let $a_{ij}$ be a typical entry of the matrix $A$ at the $i$-th row and $j$-th column such that $a_{ij} \in \{0,1\}$ and $a_{ii} \equiv 0$. If the following condition is satisfied for some constant $\alpha > 0$ $$ \alpha \rho(A) < 1 $$ can we deduce that $$\alpha a_{ij} < 1 $$ for all $i$ and $j$ in $\{1, \ldots n \}$ ? For example call the following matrix A, \begin{pmatrix}     0 & 0 & 1 \\    1 & 0 & 1 \\ 1 & 1 & 0  \end{pmatrix} has eigenvalues  $-0.6180$ and $1.6180$ and $-1.0000$. Since $\rho(A)= 1.618$ it seems to be true for this special case. Can anyone see an obvious counter example? We know from Gershgorin circle theorem that every eigen value of the square matrix $A$ lies in at least one of Gershgorin's disc $D(a_{ii} , R_i)$, where $D(a_{ii} , R_i)$ is a closed disc centered at $a_{ii}$ with radius $R_i = \sum_{ j \neq i } |a_{ij} |$. So we have an estimate of the range of the eigenvalues but it doesn't directly answer my question.","Let $A \in \mathbb{R}^{nxn}$ be a matrix and $\rho(A)$ its largest eigenvalue (or largest module of its eigenvalues). Let $a_{ij}$ be a typical entry of the matrix $A$ at the $i$-th row and $j$-th column such that $a_{ij} \in \{0,1\}$ and $a_{ii} \equiv 0$. If the following condition is satisfied for some constant $\alpha > 0$ $$ \alpha \rho(A) < 1 $$ can we deduce that $$\alpha a_{ij} < 1 $$ for all $i$ and $j$ in $\{1, \ldots n \}$ ? For example call the following matrix A, \begin{pmatrix}     0 & 0 & 1 \\    1 & 0 & 1 \\ 1 & 1 & 0  \end{pmatrix} has eigenvalues  $-0.6180$ and $1.6180$ and $-1.0000$. Since $\rho(A)= 1.618$ it seems to be true for this special case. Can anyone see an obvious counter example? We know from Gershgorin circle theorem that every eigen value of the square matrix $A$ lies in at least one of Gershgorin's disc $D(a_{ii} , R_i)$, where $D(a_{ii} , R_i)$ is a closed disc centered at $a_{ii}$ with radius $R_i = \sum_{ j \neq i } |a_{ij} |$. So we have an estimate of the range of the eigenvalues but it doesn't directly answer my question.",,"['matrices', 'eigenvalues-eigenvectors']"
32,To show that a matrix defines a map from $l^2$ to $l^2$,To show that a matrix defines a map from  to,l^2 l^2,"Let $$M=\begin{bmatrix} 1 &\frac{1}{2}&\frac{1}{3}&\frac{1}{4} \dots\\ 0 &\frac{1}{2}&\frac{1}{3}&\frac{1}{4} \dots\\ 0 & 0 &\frac{1}{3} &\frac{1}{4} \dots\\ \vdots & \vdots &\dots \end{bmatrix}$$ I need to find out if this matrix defines a map from $l^2$ to $l^2$. For that I look at a typical $y(j)$ position, meaning if I multiply $M$ by $x=\left(x(1),x(2),..,x(j),...\right)$, I get $y=\left(y(1),y(2),..,y(j),..\right)$ and for $M$ to be a map each $y(j)$ should make sense and whole of $y$ should make sense. For each $y(j)$ to make sense , $y(j)=\sum_{k=j}^{\infty} \dfrac{1}{k}x(k)$, should converge. So I look at $$\sum_{k=j}^{\infty}|\frac{1}{k} x(k)| \le \sqrt{\sum_{k=j}^{\infty}\frac{1}{k^2}} \times ||x||_2$$ So the series converges absolutely and hence it converges. No problem with this. Now $y$ should be in $l^2$ for everything to fall through. So I look at $$||y||^{2}_2=\sum_{j=1}^{\infty} |\left(\sum_{k=j}^{\infty}\frac{1}{k}x(k)\right)|^2 \le \sum_{j=1}^{\infty}\left(\sum_{k=j}^{\infty}|\frac{1}{k}x(k)|\right)^2 $$ $$\le \sum_{j=1}^{\infty}\{\left(\sum_{k=j}^{\infty}\frac{1}{k^2}\right)\left(\sum_{k=j}^{\infty}|x(k)|^2\right)\} $$ This is where I am stuck . I can always pull out $||x||_2$ but that doesn't help. I have a double series here. To solve this problem it seems to me that i have to show that this is double series converges. Cauchy-Schwarz is the best possible approximation I can get. So I don't think I have to do something with that. I tried to use the fact that the tail of the series goes to zero but that doesn't help either. (since there is a difference between going to zero and actually being zero) Now suppose I multiply the matrix with $ x=(0,...0, j,0,0,0..) $ where $ j $ is at the $ k$ th place, then $ y=(1,1,1,...,1,0,0..0) $ where the last$1$ is at the $ k $ th place. Also $||y||_2=\sqrt {k}$. This doesn't lead to anything.  I think that finally it boils down to choosing $ x $ such that the series on the right diverges. Suppose I multiply by an arbitrary $x=\left(x(1),x(2),..,x(j),..\dots \right)$, (of course $x \in l^2$)then $y=(y(1),y(2),..,y(j),..\dots )$, where $y(j)=\sum_{k=j}^{\infty} \dfrac{x(k)}{k}$. I am unable to find an $x$.  Thanks for the help!!","Let $$M=\begin{bmatrix} 1 &\frac{1}{2}&\frac{1}{3}&\frac{1}{4} \dots\\ 0 &\frac{1}{2}&\frac{1}{3}&\frac{1}{4} \dots\\ 0 & 0 &\frac{1}{3} &\frac{1}{4} \dots\\ \vdots & \vdots &\dots \end{bmatrix}$$ I need to find out if this matrix defines a map from $l^2$ to $l^2$. For that I look at a typical $y(j)$ position, meaning if I multiply $M$ by $x=\left(x(1),x(2),..,x(j),...\right)$, I get $y=\left(y(1),y(2),..,y(j),..\right)$ and for $M$ to be a map each $y(j)$ should make sense and whole of $y$ should make sense. For each $y(j)$ to make sense , $y(j)=\sum_{k=j}^{\infty} \dfrac{1}{k}x(k)$, should converge. So I look at $$\sum_{k=j}^{\infty}|\frac{1}{k} x(k)| \le \sqrt{\sum_{k=j}^{\infty}\frac{1}{k^2}} \times ||x||_2$$ So the series converges absolutely and hence it converges. No problem with this. Now $y$ should be in $l^2$ for everything to fall through. So I look at $$||y||^{2}_2=\sum_{j=1}^{\infty} |\left(\sum_{k=j}^{\infty}\frac{1}{k}x(k)\right)|^2 \le \sum_{j=1}^{\infty}\left(\sum_{k=j}^{\infty}|\frac{1}{k}x(k)|\right)^2 $$ $$\le \sum_{j=1}^{\infty}\{\left(\sum_{k=j}^{\infty}\frac{1}{k^2}\right)\left(\sum_{k=j}^{\infty}|x(k)|^2\right)\} $$ This is where I am stuck . I can always pull out $||x||_2$ but that doesn't help. I have a double series here. To solve this problem it seems to me that i have to show that this is double series converges. Cauchy-Schwarz is the best possible approximation I can get. So I don't think I have to do something with that. I tried to use the fact that the tail of the series goes to zero but that doesn't help either. (since there is a difference between going to zero and actually being zero) Now suppose I multiply the matrix with $ x=(0,...0, j,0,0,0..) $ where $ j $ is at the $ k$ th place, then $ y=(1,1,1,...,1,0,0..0) $ where the last$1$ is at the $ k $ th place. Also $||y||_2=\sqrt {k}$. This doesn't lead to anything.  I think that finally it boils down to choosing $ x $ such that the series on the right diverges. Suppose I multiply by an arbitrary $x=\left(x(1),x(2),..,x(j),..\dots \right)$, (of course $x \in l^2$)then $y=(y(1),y(2),..,y(j),..\dots )$, where $y(j)=\sum_{k=j}^{\infty} \dfrac{x(k)}{k}$. I am unable to find an $x$.  Thanks for the help!!",,"['matrices', 'functional-analysis', 'normed-spaces', 'linear-transformations', 'matrix-calculus']"
33,Determinant of a symmetric Toeplitz matrix,Determinant of a symmetric Toeplitz matrix,,"Let $A_n = (a_{ij})$ be an $n\times n$ matrix such that $a_{ii} = 0, a_{ij} = 2$ when $|j ‚àí i| = 1$, and $a_{ij} = 1$ otherwise. The question is to find the determinant in terms of $n$. I computed the first six terms, depending on $n$, but, unfortunately, no clear relationship was found. Here they are: \begin{align} \det A_1&=0,\\ \det A_2&=-4,\\ \det A_3&=8,\\ \det A_4&=-7,\\ \det A_5&=0,\\ \det A_6&=7. \end{align} Laplace expansion turned out to be useful only if $n$ is known. How can one derive a formula for $\det A_n$ in terms of $n$?","Let $A_n = (a_{ij})$ be an $n\times n$ matrix such that $a_{ii} = 0, a_{ij} = 2$ when $|j ‚àí i| = 1$, and $a_{ij} = 1$ otherwise. The question is to find the determinant in terms of $n$. I computed the first six terms, depending on $n$, but, unfortunately, no clear relationship was found. Here they are: \begin{align} \det A_1&=0,\\ \det A_2&=-4,\\ \det A_3&=8,\\ \det A_4&=-7,\\ \det A_5&=0,\\ \det A_6&=7. \end{align} Laplace expansion turned out to be useful only if $n$ is known. How can one derive a formula for $\det A_n$ in terms of $n$?",,"['matrices', 'determinant']"
34,"Maximal order of elements of $\textrm{SL}(n, \mathbb{Z})$",Maximal order of elements of,"\textrm{SL}(n, \mathbb{Z})","In the case of $\textrm{SL}(2, \mathbb{Z})$, I know that the order of any finite order matrix in this group is at most $6$. This follows from the fact that $\textrm{SL}(2, \mathbb{Z}) \cong \textrm{Mod}(S_{1})$, the mapping class group of the torus. Furthermore, finite order elements of the mapping class group of a surface have order at most $4g + 2$ by a theorem of Wang . This bound is also attained, for example in this case by the matrix {{1, -1}, {1, 0}}. I would like to know if similar bounds hold when $n > 2$, that is: What is the maximal order of a finite order matrix in $\textrm{SL}(n, \mathbb{Z})$ when $n > 2$?","In the case of $\textrm{SL}(2, \mathbb{Z})$, I know that the order of any finite order matrix in this group is at most $6$. This follows from the fact that $\textrm{SL}(2, \mathbb{Z}) \cong \textrm{Mod}(S_{1})$, the mapping class group of the torus. Furthermore, finite order elements of the mapping class group of a surface have order at most $4g + 2$ by a theorem of Wang . This bound is also attained, for example in this case by the matrix {{1, -1}, {1, 0}}. I would like to know if similar bounds hold when $n > 2$, that is: What is the maximal order of a finite order matrix in $\textrm{SL}(n, \mathbb{Z})$ when $n > 2$?",,"['matrices', 'group-theory']"
35,Compare determinants of matrices with different dimensions,Compare determinants of matrices with different dimensions,,"Reading about matrices and determinants I am wondering about the following concept: How valid is to compare the determinants of matrices with different dimensions? e.g. compare a determinant $D1$ derived from a $N\times N$ matrix with the determinant $D2$ derived from a $M\times M$ matrix. I've read that the determinant represents the volume of the $N-$dimensional object that is defined by the elements of the matrix. If this is correct then comparing volumes of different ""objects"" doesn't sound as incorrect. But having in mind that these ""objects"" came from two different spaces (a $N-$dimensional and a $M-$dimensional) how valid is that?","Reading about matrices and determinants I am wondering about the following concept: How valid is to compare the determinants of matrices with different dimensions? e.g. compare a determinant $D1$ derived from a $N\times N$ matrix with the determinant $D2$ derived from a $M\times M$ matrix. I've read that the determinant represents the volume of the $N-$dimensional object that is defined by the elements of the matrix. If this is correct then comparing volumes of different ""objects"" doesn't sound as incorrect. But having in mind that these ""objects"" came from two different spaces (a $N-$dimensional and a $M-$dimensional) how valid is that?",,"['matrices', 'determinant', 'volume']"
36,Matrix Calculus and Matrix Derivatives,Matrix Calculus and Matrix Derivatives,,"Consider a map $f : \mathbb R^{n\times m} \to \mathbb R^{p \times l}$ between matrix spaces, what is the differential of such a mapping? I looked at a really simple example, $\operatorname{id} : \mathbb R^{n\times n} \to \mathbb R^{n\times n}$ given by $\operatorname{id}(X) = X$. Then (in analogy to the the case $f : \mathbb \to \mathbb R$ oder $f : \mathbb R^n \to \mathbb R^n$) we should have $d\operatorname{id}(A) = I$ for all matrices $A$, where $I$ is the identity matrix (and $d\operatorname{id}$ denotes the differential, i.e. the best linear approximation map). Now I read about matrix derivatives, for example on Wikipedia the derivate of a mapping $F : M(n,m) \to M(p,q)$ between matrix spaces is said to be: $$  \frac{\partial\mathbf{F}} {\partial\mathbf{X}}= \begin{bmatrix} \frac{\partial\mathbf{F}}{\partial X_{1,1}}  & \cdots & \frac{\partial \mathbf{F}}{\partial X_{n,1}}\\ \vdots  & \ddots  & \vdots\\ \frac{\partial\mathbf{F}}{\partial X_{1,m}} & \cdots & \frac{\partial \mathbf{F}}{\partial X_{n,m}}\\ \end{bmatrix}  $$ And also in the Matrix Cookbook the basic formula (on page 8) is written as $$  \frac{\partial X_{kl}}{\partial X_{ij}} = \delta_{ik}\delta_{lj} $$ (where $\delta_{ij}$ denotes the Kronecker delta) and this I guess is essentially the derivation formula for the identity map. So If I apply this on the above map $\operatorname{id} : \mathbb R^{n\times n} \to \mathbb R^{n\times n}$ I get an $4\times 4$ matrix $$  \begin{pmatrix}    \frac{\partial X}{\partial x_{11}} & \frac{\partial X}{\partial x_{21}} \\   \frac{\partial X}{\partial x_{12}} & \frac{\partial X}{\partial x_{22}}  \end{pmatrix}  =   \begin{pmatrix}   \frac{\partial \begin{pmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{pmatrix}}{\partial x_{11}} &  \frac{\partial \begin{pmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{pmatrix}}{\partial x_{21}} \\  \frac{\partial \begin{pmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{pmatrix}}{\partial x_{12}} &  \frac{\partial \begin{pmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{pmatrix}}{\partial x_{22}}  \end{pmatrix}   = \begin{pmatrix}       1 & 0 & 0 & 0 \\      0 & 0 & 1 & 0 \\      0 & 1 & 0 & 0 \\      0 & 0 & 0 & 1 \end{pmatrix}           $$ (where I on the last line had not written out the blockmatrices). But this result is quite different from what I would intuitively expect, so what did I wrong? Maybe I am interpreting all these matrix derivatives wrong, could someone please explain?","Consider a map $f : \mathbb R^{n\times m} \to \mathbb R^{p \times l}$ between matrix spaces, what is the differential of such a mapping? I looked at a really simple example, $\operatorname{id} : \mathbb R^{n\times n} \to \mathbb R^{n\times n}$ given by $\operatorname{id}(X) = X$. Then (in analogy to the the case $f : \mathbb \to \mathbb R$ oder $f : \mathbb R^n \to \mathbb R^n$) we should have $d\operatorname{id}(A) = I$ for all matrices $A$, where $I$ is the identity matrix (and $d\operatorname{id}$ denotes the differential, i.e. the best linear approximation map). Now I read about matrix derivatives, for example on Wikipedia the derivate of a mapping $F : M(n,m) \to M(p,q)$ between matrix spaces is said to be: $$  \frac{\partial\mathbf{F}} {\partial\mathbf{X}}= \begin{bmatrix} \frac{\partial\mathbf{F}}{\partial X_{1,1}}  & \cdots & \frac{\partial \mathbf{F}}{\partial X_{n,1}}\\ \vdots  & \ddots  & \vdots\\ \frac{\partial\mathbf{F}}{\partial X_{1,m}} & \cdots & \frac{\partial \mathbf{F}}{\partial X_{n,m}}\\ \end{bmatrix}  $$ And also in the Matrix Cookbook the basic formula (on page 8) is written as $$  \frac{\partial X_{kl}}{\partial X_{ij}} = \delta_{ik}\delta_{lj} $$ (where $\delta_{ij}$ denotes the Kronecker delta) and this I guess is essentially the derivation formula for the identity map. So If I apply this on the above map $\operatorname{id} : \mathbb R^{n\times n} \to \mathbb R^{n\times n}$ I get an $4\times 4$ matrix $$  \begin{pmatrix}    \frac{\partial X}{\partial x_{11}} & \frac{\partial X}{\partial x_{21}} \\   \frac{\partial X}{\partial x_{12}} & \frac{\partial X}{\partial x_{22}}  \end{pmatrix}  =   \begin{pmatrix}   \frac{\partial \begin{pmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{pmatrix}}{\partial x_{11}} &  \frac{\partial \begin{pmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{pmatrix}}{\partial x_{21}} \\  \frac{\partial \begin{pmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{pmatrix}}{\partial x_{12}} &  \frac{\partial \begin{pmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{pmatrix}}{\partial x_{22}}  \end{pmatrix}   = \begin{pmatrix}       1 & 0 & 0 & 0 \\      0 & 0 & 1 & 0 \\      0 & 1 & 0 & 0 \\      0 & 0 & 0 & 1 \end{pmatrix}           $$ (where I on the last line had not written out the blockmatrices). But this result is quite different from what I would intuitively expect, so what did I wrong? Maybe I am interpreting all these matrix derivatives wrong, could someone please explain?",,"['matrices', 'analysis', 'multivariable-calculus', 'differential-geometry', 'matrix-calculus']"
37,"Volume of a parallelepiped, given 8 vertices","Volume of a parallelepiped, given 8 vertices",,"Given the eight vertices $(0,0,0)$, $(3,0,0)$, $(0,5,1)$, $(3,5,1)$, $(2,0,5)$, $(5,0,5)$, $(2,5,6)$, and $(5,5,6)$, find the volume of the parallelepiped. I'm having trouble finding the 1 vertex and 3 vectors needed to find the volume. The closest four vertexes I found so far are $(0,0,0), (3,0,0), (0,5,1), (3,5,1)$...is using those four vertexes correct? Any starting hints to point me in the right direction?","Given the eight vertices $(0,0,0)$, $(3,0,0)$, $(0,5,1)$, $(3,5,1)$, $(2,0,5)$, $(5,0,5)$, $(2,5,6)$, and $(5,5,6)$, find the volume of the parallelepiped. I'm having trouble finding the 1 vertex and 3 vectors needed to find the volume. The closest four vertexes I found so far are $(0,0,0), (3,0,0), (0,5,1), (3,5,1)$...is using those four vertexes correct? Any starting hints to point me in the right direction?",,"['calculus', 'algebra-precalculus', 'matrices', 'geometry', 'trigonometry']"
38,eigenvalues of symmetric matrix sums,eigenvalues of symmetric matrix sums,,"Suppose $A$ and $B$ are symmatric matrices: $A,B \in S^n$. Let $Y=A+B$. What is the relationship between eigenvalues of $Y$ and eigenvalues of $A$ and $B$? Or, Does any nonsingular matrix $P$ exist such that $P^{-1}AP$ and $P^{-1}BP$ are diagonal matrices in the same time? Maybe I'm in the wrong direction. What I need to show in process of my homework problem is: Suppose $A$ and $B$ are positive semidefinite matrices: $A,B \in S^n_+$. And $Tr(A+B)=1$. Let $Y=A-B$. I'm trying to show that ${\parallel Y \parallel}_{2*} \leq 1$. ${\parallel Y \parallel}_{2*} = \sum\limits_{i = 1}^n {|{\lambda _i}(Y)|}$ is the summation of the absoluate values of the eigenvalues of $Y$.","Suppose $A$ and $B$ are symmatric matrices: $A,B \in S^n$. Let $Y=A+B$. What is the relationship between eigenvalues of $Y$ and eigenvalues of $A$ and $B$? Or, Does any nonsingular matrix $P$ exist such that $P^{-1}AP$ and $P^{-1}BP$ are diagonal matrices in the same time? Maybe I'm in the wrong direction. What I need to show in process of my homework problem is: Suppose $A$ and $B$ are positive semidefinite matrices: $A,B \in S^n_+$. And $Tr(A+B)=1$. Let $Y=A-B$. I'm trying to show that ${\parallel Y \parallel}_{2*} \leq 1$. ${\parallel Y \parallel}_{2*} = \sum\limits_{i = 1}^n {|{\lambda _i}(Y)|}$ is the summation of the absoluate values of the eigenvalues of $Y$.",,"['matrices', 'eigenvalues-eigenvectors']"
39,How to solve 29 coupled quadratic equations?,How to solve 29 coupled quadratic equations?,,"I have a set of 29 coupled quadratic equations, with 29 unknown variables. Can anyone offer any advice on how I could go about solving this? 3 days of staring at a wall has so far given me no thoughts on how to do this at all. EDIT: \begin{align} T_1 &= X_1^2 X_2 X_3 X_4 X_5 X_6 \\ T_2 &= X_2^2 X_1 X_3 X_4 X_5 X_6 \\ T_3 &= X_3^2 X_1 X_2 X_4 X_5 X_6 \\ T_4 &= X_4^2 X_1 X_2 X_3 X_5 X_6 \\ T_5 &= X_5^2 X_1 X_2 X_3 X_4 X_6 \\ T_6 &= X_6^2 X_1 X_2 X_3 X_4 X_5 \\ T_7 &= X_1 X_2 X_3 X_4 X_5 X_6 X_7^2 X_8 X_9 X_{10} (1-X_5) \\ T_8 &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8^2 X_9 X_{10} (1-X_5) \\ T_9 &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9^2 X_{10} (1-X_5) \\ T_{10} &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10}^2 (1-X_5) \\ T_{11} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11}^2 X_{12} X_{13} X_{14} X_{15} \\ T_{12} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11} X_{12}^2 X_{13} X_{14} X_{15} \\ T_{13} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13}^2 X_{14} X_{15} \\ T_{14} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14}^2 X_{15} \\ T_{15} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15}^2 \\ T_{16} &= X_1 X_2 X_3 X_4 X_5 X_6 (1-X_6)(1-X_9)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16}^2 X_{17} \\ T_{17} &= X_1 X_2 X_3 X_4 X_5 X_6 (1-X_6)(1-X_9)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17}^2 \\ T_{18} &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17} (1-X_9)(1-X_5)(1-X_{16}) X_{18}^2 X_{19} X_{20} X_{21} \\ T_{19} &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17} (1-X_9)(1-X_5)(1-X_{16}) X_{18} X_{19}^2 X_{20} X_{21} \\ T_{20} &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17} (1-X_9)(1-X_5)(1-X_{16}) X_{18} X_{19} X_{20}^2 X_{21} \\ T_{21} &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17} (1-X_9)(1-X_5)(1-X_{16}) X_{18} X_{19} X_{20} X_{21}^2 \\ T_{22} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_5) X_7 X_8 X_9 X_{10} [(1-X_{17} + (1-X_9)(1-X_7)X_{16}X_{17}] + (1-X_2)\} X_{22}^2 X_{23} \\ T_{23} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_5) X_7 X_8 X_9 X_{10} [(1-X_{17} + (1-X_9)(1-X_7)X_{16}X_{17}] + (1-X_2)\} X_{22} X_{23}^2 \\ T_{24} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_3) + (1-X_5)(1-X_8)X_7 X_8 X_9 X_{10} \} X_{24}^2 X_{25} \\ T_{25} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_3) + (1-X_5)(1-X_8)X_7 X_8 X_9 X_{10} \} X_{24} X_{25}^2 \\ T_{26} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_4) + \{ (1-X_6) + (1-X_{10})(1-X_5))X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15} \}(1-X_{12}) + X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15}X_{16}X_{17}X_{18}X_{19}X_{20}X_{21}(1-X_{20})(1-X_5)(1-X_9)(1-x_16) +(1-X_25)\{(1-X_3) + (1-X_5)X_7 X_8 X_9 X_{10} (1-X_8)  \}X_{24}X_{25} \} X_{26} \\ T_{27} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_4) + \{ (1-X_6) + (1-X_{10})(1-X_5))X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15} \}(1-X_{12}) + X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15}X_{16}X_{17}X_{18}X_{19}X_{20}X_{21}(1-X_{20})(1-X_5)(1-X_9)(1-x_16) +(1-X_25)\{(1-X_3) + (1-X_5)X_7 X_8 X_9 X_{10} (1-X_8)  \}X_{24}X_{25} \} X_{27} \\ T_{28} &= \{ X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_4) + \{ (1-X_6) + (1-X_{10})(1-X_5))X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15} \}(1-X_{12}) + X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15}X_{16}X_{17}X_{18}X_{19}X_{20}X_{21}(1-X_{20})(1-X_5)(1-X_9)(1-x_16)+(1-X_25)\{(1-X_3) + (1-X_5)X_7 X_8 X_9 X_{10} (1-X_8)  \}X_{24}X_{25} \}  \}(1-X_{27}+ X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17} (1-X_9)(1-X_5)(1-X_{16}) X_{18} X_{19} X_{20} X_{21}(1-X_20)  +(1-X_{13})X_{28}X_{29} X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15}    \} X_{28} \\ T_{29} &= \{ X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_4) + \{ (1-X_6) + (1-X_{10})(1-X_5))X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15} \}(1-X_{12}) + X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15}X_{16}X_{17}X_{18}X_{19}X_{20}X_{21}(1-X_{20})(1-X_5)(1-X_9)(1-x_16)+(1-X_25)\{(1-X_3) + (1-X_5)X_7 X_8 X_9 X_{10} (1-X_8)  \}X_{24}X_{25} \}  \}(1-X_{27}+ X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17} (1-X_9)(1-X_5)(1-X_{16}) X_{18} X_{19} X_{20} X_{21}(1-X_20)  +(1-X_{13})X_{28}X_{29} X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15}    \} X_{29} \end{align} Here are the equations, my unknowns are the $X$ terms. The $T$ terms are known. Both $T$ and $X$ terms are real and positive. Thanks for the comments, still attempting to understand what a ‚ÄúGroebner basis‚Äú is so far‚Ä¶ EDIT END","I have a set of 29 coupled quadratic equations, with 29 unknown variables. Can anyone offer any advice on how I could go about solving this? 3 days of staring at a wall has so far given me no thoughts on how to do this at all. EDIT: Here are the equations, my unknowns are the terms. The terms are known. Both and terms are real and positive. Thanks for the comments, still attempting to understand what a ‚ÄúGroebner basis‚Äú is so far‚Ä¶ EDIT END","\begin{align}
T_1 &= X_1^2 X_2 X_3 X_4 X_5 X_6 \\
T_2 &= X_2^2 X_1 X_3 X_4 X_5 X_6 \\
T_3 &= X_3^2 X_1 X_2 X_4 X_5 X_6 \\
T_4 &= X_4^2 X_1 X_2 X_3 X_5 X_6 \\
T_5 &= X_5^2 X_1 X_2 X_3 X_4 X_6 \\
T_6 &= X_6^2 X_1 X_2 X_3 X_4 X_5 \\
T_7 &= X_1 X_2 X_3 X_4 X_5 X_6 X_7^2 X_8 X_9 X_{10} (1-X_5) \\
T_8 &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8^2 X_9 X_{10} (1-X_5) \\
T_9 &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9^2 X_{10} (1-X_5) \\
T_{10} &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10}^2 (1-X_5) \\
T_{11} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11}^2 X_{12} X_{13} X_{14} X_{15} \\
T_{12} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11} X_{12}^2 X_{13} X_{14} X_{15} \\
T_{13} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13}^2 X_{14} X_{15} \\
T_{14} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14}^2 X_{15} \\
T_{15} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15}^2 \\
T_{16} &= X_1 X_2 X_3 X_4 X_5 X_6 (1-X_6)(1-X_9)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16}^2 X_{17} \\
T_{17} &= X_1 X_2 X_3 X_4 X_5 X_6 (1-X_6)(1-X_9)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17}^2 \\
T_{18} &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17} (1-X_9)(1-X_5)(1-X_{16}) X_{18}^2 X_{19} X_{20} X_{21} \\
T_{19} &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17} (1-X_9)(1-X_5)(1-X_{16}) X_{18} X_{19}^2 X_{20} X_{21} \\
T_{20} &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17} (1-X_9)(1-X_5)(1-X_{16}) X_{18} X_{19} X_{20}^2 X_{21} \\
T_{21} &= X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17} (1-X_9)(1-X_5)(1-X_{16}) X_{18} X_{19} X_{20} X_{21}^2 \\
T_{22} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_5) X_7 X_8 X_9 X_{10} [(1-X_{17} + (1-X_9)(1-X_7)X_{16}X_{17}] + (1-X_2)\} X_{22}^2 X_{23} \\
T_{23} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_5) X_7 X_8 X_9 X_{10} [(1-X_{17} + (1-X_9)(1-X_7)X_{16}X_{17}] + (1-X_2)\} X_{22} X_{23}^2 \\
T_{24} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_3) + (1-X_5)(1-X_8)X_7 X_8 X_9 X_{10} \} X_{24}^2 X_{25} \\
T_{25} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_3) + (1-X_5)(1-X_8)X_7 X_8 X_9 X_{10} \} X_{24} X_{25}^2 \\
T_{26} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_4) + \{ (1-X_6) + (1-X_{10})(1-X_5))X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15} \}(1-X_{12}) + X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15}X_{16}X_{17}X_{18}X_{19}X_{20}X_{21}(1-X_{20})(1-X_5)(1-X_9)(1-x_16)
+(1-X_25)\{(1-X_3) + (1-X_5)X_7 X_8 X_9 X_{10} (1-X_8)  \}X_{24}X_{25} \} X_{26} \\
T_{27} &= X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_4) + \{ (1-X_6) + (1-X_{10})(1-X_5))X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15} \}(1-X_{12}) + X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15}X_{16}X_{17}X_{18}X_{19}X_{20}X_{21}(1-X_{20})(1-X_5)(1-X_9)(1-x_16)
+(1-X_25)\{(1-X_3) + (1-X_5)X_7 X_8 X_9 X_{10} (1-X_8)  \}X_{24}X_{25} \} X_{27} \\
T_{28} &= \{ X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_4) + \{ (1-X_6) + (1-X_{10})(1-X_5))X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15} \}(1-X_{12}) + X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15}X_{16}X_{17}X_{18}X_{19}X_{20}X_{21}(1-X_{20})(1-X_5)(1-X_9)(1-x_16)+(1-X_25)\{(1-X_3) + (1-X_5)X_7 X_8 X_9 X_{10} (1-X_8)  \}X_{24}X_{25} \}  \}(1-X_{27}+ X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17} (1-X_9)(1-X_5)(1-X_{16}) X_{18} X_{19} X_{20} X_{21}(1-X_20) 
+(1-X_{13})X_{28}X_{29} X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15}    \} X_{28} \\
T_{29} &= \{ X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_4) + \{ (1-X_6) + (1-X_{10})(1-X_5))X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15} \}(1-X_{12}) + X_7 X_8 X_9 X_{10} X_{11}X_{12} X_{13} X_{14} X_{15}X_{16}X_{17}X_{18}X_{19}X_{20}X_{21}(1-X_{20})(1-X_5)(1-X_9)(1-x_16)+(1-X_25)\{(1-X_3) + (1-X_5)X_7 X_8 X_9 X_{10} (1-X_8)  \}X_{24}X_{25} \}  \}(1-X_{27}+ X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15} X_{16} X_{17} (1-X_9)(1-X_5)(1-X_{16}) X_{18} X_{19} X_{20} X_{21}(1-X_20) 
+(1-X_{13})X_{28}X_{29} X_1 X_2 X_3 X_4 X_5 X_6 \{ (1-X_6) + (1-X_{10})(1-X_5)X_7 X_8 X_9 X_{10} \} X_{11} X_{12} X_{13} X_{14} X_{15}    \} X_{29}
\end{align} X T T X","['matrices', 'polynomials']"
40,"Simple eigenvalue and eigenvector of $\{\sin(|i-j|x)\}_{1\le i,j\le5}$",Simple eigenvalue and eigenvector of,"\{\sin(|i-j|x)\}_{1\le i,j\le5}","Let $M=\begin{bmatrix}   0&\sin(x)&\sin(2x)&\sin(3x)&\sin(4x)   \\\sin(x)&0&\sin(x)&\sin(2x)&\sin(3x)   \\\sin(2x)&\sin(x)&0&\sin(x)&\sin(2x)   \\\sin(3x)&\sin(2x)&\sin(x)&0&\sin(x)   \\\sin(4x)&\sin(3x)&\sin(2x)&\sin(x)&0   \end{bmatrix}$. Question 1. (out of 3) of the exercise asks for an easy eigenvalue and eigenvector, but after having searched for an hour, tried some Chebichev tricks, I haven't found anything. Looking at simple cases (n=1,n=2), I didn't see any logical progression either. By the way : Question 2 is to give its characteristic polynomial, so we are not supposed to use it here I guess.","Let $M=\begin{bmatrix}   0&\sin(x)&\sin(2x)&\sin(3x)&\sin(4x)   \\\sin(x)&0&\sin(x)&\sin(2x)&\sin(3x)   \\\sin(2x)&\sin(x)&0&\sin(x)&\sin(2x)   \\\sin(3x)&\sin(2x)&\sin(x)&0&\sin(x)   \\\sin(4x)&\sin(3x)&\sin(2x)&\sin(x)&0   \end{bmatrix}$. Question 1. (out of 3) of the exercise asks for an easy eigenvalue and eigenvector, but after having searched for an hour, tried some Chebichev tricks, I haven't found anything. Looking at simple cases (n=1,n=2), I didn't see any logical progression either. By the way : Question 2 is to give its characteristic polynomial, so we are not supposed to use it here I guess.",,"['matrices', 'eigenvalues-eigenvectors']"
41,Expressing a summation using matrix algebra,Expressing a summation using matrix algebra,,"Consider the $r \times n$ matrix  $$\begin{pmatrix} X_{11} & X_{12} & \cdots & X_{1n} \\ X_{21} & X_{22} & \cdots & X_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ X_{r1} & X_{r2} & \cdots & X_{rn} \end{pmatrix}\text{.}$$ Define  $$\begin{align*} &\bar{X} = \dfrac{\sum\limits_{i=1}^{r}\sum\limits_{j=1}^{n}X_{ij}}{nr} \\ &\bar{X}_{i} = \dfrac{\sum\limits_{j=1}^{n}X_{ij}}{n}\text{.} \end{align*}$$ I am interested in knowing if there is a possible way to write the summations $$\begin{align*} \hat{v}^{S} &= \sum\limits_{i=1}^{r}\sum\limits_{j=1}^{n}\left(X_{ij}-\bar{X}_{i}\right)^{2} \\ \hat{a}^{S} &= \sum\limits_{i=1}^{r}\left(\bar{X}_i - \bar{X}\right)^{2} \end{align*}$$ in terms of matrix operations (anything one would learn in a first course in linear algebra, such as multiplication of matrices, inverses of matrices, determinants, eigenvalues, etc.). The reason why is because I have to memorize these formulas for the actuarial exam I will be taking soon, and I am not interested in memorizing summations if there is a way to express them in matrix form. There may not be an answer to what I seek, and I might just have to memorize these summations as is, but I thought I would ask in case there is. ETA : I did pass this exam (at least 93% scored) but am still interested in knowing if there is a solution to this problem.","Consider the $r \times n$ matrix  $$\begin{pmatrix} X_{11} & X_{12} & \cdots & X_{1n} \\ X_{21} & X_{22} & \cdots & X_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ X_{r1} & X_{r2} & \cdots & X_{rn} \end{pmatrix}\text{.}$$ Define  $$\begin{align*} &\bar{X} = \dfrac{\sum\limits_{i=1}^{r}\sum\limits_{j=1}^{n}X_{ij}}{nr} \\ &\bar{X}_{i} = \dfrac{\sum\limits_{j=1}^{n}X_{ij}}{n}\text{.} \end{align*}$$ I am interested in knowing if there is a possible way to write the summations $$\begin{align*} \hat{v}^{S} &= \sum\limits_{i=1}^{r}\sum\limits_{j=1}^{n}\left(X_{ij}-\bar{X}_{i}\right)^{2} \\ \hat{a}^{S} &= \sum\limits_{i=1}^{r}\left(\bar{X}_i - \bar{X}\right)^{2} \end{align*}$$ in terms of matrix operations (anything one would learn in a first course in linear algebra, such as multiplication of matrices, inverses of matrices, determinants, eigenvalues, etc.). The reason why is because I have to memorize these formulas for the actuarial exam I will be taking soon, and I am not interested in memorizing summations if there is a way to express them in matrix form. There may not be an answer to what I seek, and I might just have to memorize these summations as is, but I thought I would ask in case there is. ETA : I did pass this exam (at least 93% scored) but am still interested in knowing if there is a solution to this problem.",,"['matrices', 'statistics', 'summation', 'average', 'actuarial-science']"
42,Column or row of a matrix?,Column or row of a matrix?,,"The question is so simple, but I cannot find the answer. Is $M_i$ (usually) the $i^{\text{th}}$ column of matrix $M$? Or the $i^{\text{th}}$ row? Since $M_{ij}$ is the $j^{\text{th}}$ element of the $i^{\text{th}}$ row, I would say $M_i$ is the row. On the other hand we usually work with column vectors and it is therefore unusual to take a row from a matrix and it would be illogical to have a simple notation for something that is used less often. If $M_i$ is the $i^{\text{th}}$ row, how would I get the $i^{\text{th}}$ column? Surely not $(M^T)_i$!","The question is so simple, but I cannot find the answer. Is $M_i$ (usually) the $i^{\text{th}}$ column of matrix $M$? Or the $i^{\text{th}}$ row? Since $M_{ij}$ is the $j^{\text{th}}$ element of the $i^{\text{th}}$ row, I would say $M_i$ is the row. On the other hand we usually work with column vectors and it is therefore unusual to take a row from a matrix and it would be illogical to have a simple notation for something that is used less often. If $M_i$ is the $i^{\text{th}}$ row, how would I get the $i^{\text{th}}$ column? Surely not $(M^T)_i$!",,"['matrices', 'notation']"
43,Extended matrix function,Extended matrix function,,"I have a continuous matrix-valued function $f:\mathbb{R}^d\mapsto {\cal M}_{k\times d}$, with $d<k$, such that $f(x)$ is full rank for all $x\in\mathbb{R}^k$. Can I extend this function to be a continuous function $g:\mathbb{R}^k\mapsto {\cal M}_{k\times k}$ such that $g(x)$ is full rank matrix, the first $d$ columns of $g(x)$ equals to $f(x)$, and the rest columns are orthogonal with all columns of $f(x)$. Note that since $f(x)$ is full rank then we can add $k-d$ columns such that the obtained matrix is full rank (invertible). Then, using the Gram-Schmidt procedure which is a continuous process we can get $g(x)$ if the $k-d$ vectors that we choose is also conttinuous. But, how can I choose this $k-d$ vectors. Thanks for any help","I have a continuous matrix-valued function $f:\mathbb{R}^d\mapsto {\cal M}_{k\times d}$, with $d<k$, such that $f(x)$ is full rank for all $x\in\mathbb{R}^k$. Can I extend this function to be a continuous function $g:\mathbb{R}^k\mapsto {\cal M}_{k\times k}$ such that $g(x)$ is full rank matrix, the first $d$ columns of $g(x)$ equals to $f(x)$, and the rest columns are orthogonal with all columns of $f(x)$. Note that since $f(x)$ is full rank then we can add $k-d$ columns such that the obtained matrix is full rank (invertible). Then, using the Gram-Schmidt procedure which is a continuous process we can get $g(x)$ if the $k-d$ vectors that we choose is also conttinuous. But, how can I choose this $k-d$ vectors. Thanks for any help",,"['matrices', 'multivariable-calculus', 'continuity']"
44,Prove that an eigenvector is the maximum of a symmetric matrix,Prove that an eigenvector is the maximum of a symmetric matrix,,"Let $f : S^{n-1} \rightarrow \mathbb{R}, x \mapsto x^TAx$ ( A is a symmetric matrix), then an eigenvector $\xi$ of A is a local maximum of this function. We are supposed to prove this in 6 steps and I got stuck somewhere.( I have to follow these steps, although it might be easier to prove this slightly different.) (i) Express $S^{n-1}$ in terms of a function $g(x_1,...,x_n)=0$. I did this by saying : $g(x_1,...,x_n) = x_1^2+...+x_n^2-1=0$. (ii) Assume $\xi=e_n$. Then proof that there is a function $\gamma:B_{\epsilon}(0) \subset \mathbb{R}^{n-1} \rightarrow \mathbb{R} $ such that $ g(x_1,...,x_{n-1},\gamma(x_1,...,x_{n-1})=0$ and show that $D\gamma|_{x_1=0,...,x_{n-1}=0}=0$. Well by implicit function theorem, we get at $(0,...,0,1)$ that $g(0,...,0,1)=0$ and $\partial_{x_n}g(0,...,0,1)=2$ this is invertible and therefore there exists such a curve $\gamma$ and by implicit differentiation we get that $D\gamma|_{x_1=0,...,x_{n-1}=0}=0$. (iii) Look at the function $\bar{f}(x_1,...,x_{n-1})=f(x_1,...,x_n,\gamma(x_1,...,x_{n-1}))$ and prove that whenever $\bar{f}$ has a local maximum, then the same is true for $f$. Okay, this is pretty clear, since $\bar{f}$ and $f$ coincide on a local set and since the question of having a local extremum is only a local property, this is true. (iv) Look at $f$ as a map $f:\mathbb{R}^n \rightarrow \mathbb{R}, x \mapsto x^T Ax$ and calculate $\nabla f|_{x=e_n}$. Well $\nabla f(e_n) = e_n^T A$ (v) And now I am supposed to show that $e_n$ is an eigenvector. I have no idea how to do this, but I think I missed something, since the answer to (iv) is not telling me much. Does anybody have an idea, where I am wrong? Also, I am not sure about the fact: Assume $\xi=e_n$, this should have at least some effect on the proof. If something is unclear, please let me know. Does nobody have an idea or a hint/comment?(Maybe you are also wondering about something.)","Let $f : S^{n-1} \rightarrow \mathbb{R}, x \mapsto x^TAx$ ( A is a symmetric matrix), then an eigenvector $\xi$ of A is a local maximum of this function. We are supposed to prove this in 6 steps and I got stuck somewhere.( I have to follow these steps, although it might be easier to prove this slightly different.) (i) Express $S^{n-1}$ in terms of a function $g(x_1,...,x_n)=0$. I did this by saying : $g(x_1,...,x_n) = x_1^2+...+x_n^2-1=0$. (ii) Assume $\xi=e_n$. Then proof that there is a function $\gamma:B_{\epsilon}(0) \subset \mathbb{R}^{n-1} \rightarrow \mathbb{R} $ such that $ g(x_1,...,x_{n-1},\gamma(x_1,...,x_{n-1})=0$ and show that $D\gamma|_{x_1=0,...,x_{n-1}=0}=0$. Well by implicit function theorem, we get at $(0,...,0,1)$ that $g(0,...,0,1)=0$ and $\partial_{x_n}g(0,...,0,1)=2$ this is invertible and therefore there exists such a curve $\gamma$ and by implicit differentiation we get that $D\gamma|_{x_1=0,...,x_{n-1}=0}=0$. (iii) Look at the function $\bar{f}(x_1,...,x_{n-1})=f(x_1,...,x_n,\gamma(x_1,...,x_{n-1}))$ and prove that whenever $\bar{f}$ has a local maximum, then the same is true for $f$. Okay, this is pretty clear, since $\bar{f}$ and $f$ coincide on a local set and since the question of having a local extremum is only a local property, this is true. (iv) Look at $f$ as a map $f:\mathbb{R}^n \rightarrow \mathbb{R}, x \mapsto x^T Ax$ and calculate $\nabla f|_{x=e_n}$. Well $\nabla f(e_n) = e_n^T A$ (v) And now I am supposed to show that $e_n$ is an eigenvector. I have no idea how to do this, but I think I missed something, since the answer to (iv) is not telling me much. Does anybody have an idea, where I am wrong? Also, I am not sure about the fact: Assume $\xi=e_n$, this should have at least some effect on the proof. If something is unclear, please let me know. Does nobody have an idea or a hint/comment?(Maybe you are also wondering about something.)",,"['calculus', 'real-analysis']"
45,the table at the end of Theoretical Computer Science Cheat Sheet,the table at the end of Theoretical Computer Science Cheat Sheet,,"Theoretical Computer Science Cheat Sheet , created by Steve Seiden , is a hodgepodge of well-known mathematical theorems and notions. I can understand (or guess at least) many of them, but I'm not sure about this 10-by-10 table at the end of the document. What is this matrix? The document has no explanation at all, and I'm wondering why the author put in a cheat sheet . Is there any special meaning in computer science or math that this matrix stands for?","Theoretical Computer Science Cheat Sheet , created by Steve Seiden , is a hodgepodge of well-known mathematical theorems and notions. I can understand (or guess at least) many of them, but I'm not sure about this 10-by-10 table at the end of the document. What is this matrix? The document has no explanation at all, and I'm wondering why the author put in a cheat sheet . Is there any special meaning in computer science or math that this matrix stands for?",,"['matrices', 'pattern-recognition', 'combinatorial-designs']"
46,"derivative of log(det(A)) wrt x, where A is matrix that depends on x","derivative of log(det(A)) wrt x, where A is matrix that depends on x",,"I have two large sparse matrices B and C, and I need to calculate $\frac{\rm{d}}{\rm{d}(\log({\lambda}) }\log( \det(B+\lambda C))$. Because B and C are very large I can't directly evaluate the determinant. The paper I'm following implies that the above is equivalent to : $\rm{trace}((B+\lambda C)^{-1}C)$. Although I don't know how the authors got to it and the expression isn't much help, since calculating the inverse is also prohibitively expensive. My question is: Are there any identities that can make either of these calculations computationally tractable, given B and C are large and sparse?","I have two large sparse matrices B and C, and I need to calculate $\frac{\rm{d}}{\rm{d}(\log({\lambda}) }\log( \det(B+\lambda C))$. Because B and C are very large I can't directly evaluate the determinant. The paper I'm following implies that the above is equivalent to : $\rm{trace}((B+\lambda C)^{-1}C)$. Although I don't know how the authors got to it and the expression isn't much help, since calculating the inverse is also prohibitively expensive. My question is: Are there any identities that can make either of these calculations computationally tractable, given B and C are large and sparse?",,"['calculus', 'matrices', 'matrix-equations']"
47,"Is this monotonic in $a,b$?",Is this monotonic in ?,"a,b","I want to claim that $$\det(I+[aA,bB]VV^*[aA,bB]^*)$$ is increasing in both $a$ and $b$. where $V$ is a complex valued matrix so are $A$ and $B$. $a,b$ are real positive and $^*$ denotes conjugate transpose (Hermitian). $[aAbB]$ represents the augmentation of the the matrices by putting them side by side in that order. Also in case the claim is wrong, for what $V$ is it valid? I can see that it holds for real valued vector for example $[aA,bB]=(a,b)$ and $V=(c,d)$ where all values are reals. Also holds for $a$ or $b$ zero in matrix case. I think it may have a solution in eigenvalues of positive semi-definite matrices ( Is $\det(I+aAVV^*A^*)$ increasing function in $a$. ). It holds for $a=b$ as well. PS. NOT home work, my research in information theory. Thank you.","I want to claim that $$\det(I+[aA,bB]VV^*[aA,bB]^*)$$ is increasing in both $a$ and $b$. where $V$ is a complex valued matrix so are $A$ and $B$. $a,b$ are real positive and $^*$ denotes conjugate transpose (Hermitian). $[aAbB]$ represents the augmentation of the the matrices by putting them side by side in that order. Also in case the claim is wrong, for what $V$ is it valid? I can see that it holds for real valued vector for example $[aA,bB]=(a,b)$ and $V=(c,d)$ where all values are reals. Also holds for $a$ or $b$ zero in matrix case. I think it may have a solution in eigenvalues of positive semi-definite matrices ( Is $\det(I+aAVV^*A^*)$ increasing function in $a$. ). It holds for $a=b$ as well. PS. NOT home work, my research in information theory. Thank you.",,"['matrices', 'eigenvalues-eigenvectors']"
48,Derivative of the off-diagonal $L_1$ matrix norm,Derivative of the off-diagonal  matrix norm,L_1,"We define the off-diagonal $L_1$ norm of a matrix as follows: for any $A\in \mathcal{M}_{n,n}$, $$\|A\|_1^{\text{off}} = \sum_{i\ne j}|a_{ij}|.$$ So what is $$\frac{\partial \|A\|_1^{\text{off}}}{\partial A}\;?$$","We define the off-diagonal $L_1$ norm of a matrix as follows: for any $A\in \mathcal{M}_{n,n}$, $$\|A\|_1^{\text{off}} = \sum_{i\ne j}|a_{ij}|.$$ So what is $$\frac{\partial \|A\|_1^{\text{off}}}{\partial A}\;?$$",,"['matrices', 'derivatives', 'matrix-calculus']"
49,"Simple subgroup proof, would love some advice","Simple subgroup proof, would love some advice",,"Let $\mathrm{GL}_2(\mathbb R)$ be the group of invertible $2\times 2$ matrices with real entries. Consider $H$ which is the subset of matrices of the form $$A=\begin{bmatrix}1 & b \\ 0 &1\end{bmatrix}$$ with $b$ an integer. Is $H$ a subgroup of $\mathrm{GL}_2(\mathbb R)$? Explain your answer. well when $b = 0$ we have the identity, and $\det |A| = 1$ so it is invertible and the inverse is simply  $$\begin{bmatrix}1 & -b \\ 0 &1\end{bmatrix}$$ for all $b$ in the integers. I seem a little lost on closure. Clearly $$\begin{bmatrix}1 & -b \\ 0 &1\end{bmatrix}$$ multiplied by $$\begin{bmatrix}1 & a \\ 0 &1\end{bmatrix}$$ where $a$ is in the integers yields $$\begin{bmatrix}1 & -b+a \\ 0 &1\end{bmatrix}$$ and $a - b$ where $a$ and $b$ are integers is an integer. So we got non empty and we got $A^{-1} B$. So we're done?","Let $\mathrm{GL}_2(\mathbb R)$ be the group of invertible $2\times 2$ matrices with real entries. Consider $H$ which is the subset of matrices of the form $$A=\begin{bmatrix}1 & b \\ 0 &1\end{bmatrix}$$ with $b$ an integer. Is $H$ a subgroup of $\mathrm{GL}_2(\mathbb R)$? Explain your answer. well when $b = 0$ we have the identity, and $\det |A| = 1$ so it is invertible and the inverse is simply  $$\begin{bmatrix}1 & -b \\ 0 &1\end{bmatrix}$$ for all $b$ in the integers. I seem a little lost on closure. Clearly $$\begin{bmatrix}1 & -b \\ 0 &1\end{bmatrix}$$ multiplied by $$\begin{bmatrix}1 & a \\ 0 &1\end{bmatrix}$$ where $a$ is in the integers yields $$\begin{bmatrix}1 & -b+a \\ 0 &1\end{bmatrix}$$ and $a - b$ where $a$ and $b$ are integers is an integer. So we got non empty and we got $A^{-1} B$. So we're done?",,"['abstract-algebra', 'group-theory', 'matrices']"
50,"How to prove in a graph $G$, its incidence matrix $A$ is totally unimodular if and only if $G$ is a bipartite graph?","How to prove in a graph , its incidence matrix  is totally unimodular if and only if  is a bipartite graph?",G A G,"I'm learning network and transportation model. The question is not from my homework. I'm just curious about: In a graph $G$, its incidence matrix $A$ is totally unimodular if and only if $G$ is a bipartite graph. Could anyone good at proving give a simple provement? OR Could anyone provide some related materials?","I'm learning network and transportation model. The question is not from my homework. I'm just curious about: In a graph $G$, its incidence matrix $A$ is totally unimodular if and only if $G$ is a bipartite graph. Could anyone good at proving give a simple provement? OR Could anyone provide some related materials?",,"['matrices', 'graph-theory', 'reference-request', 'total-unimodularity']"
51,Diagonally dominant matrix with matrix similarity,Diagonally dominant matrix with matrix similarity,,"Applying similarity transform to a matrix $A$ gives: $$M=P^{-1}AP$$ $M$ and $A$ have same eigenvalues. What is the way to to find $P$ such that $M$ is diagonally dominant case of $A$? $M$ is diagonally dominant if $$|{m_{ii}}| \ge \sum\limits_{j \ne i} | {m_{ij}}|\quad {\rm{for \quad  all}}\quad i,{\mkern 1mu} $$ Note: I want $P$ to be something other than eigenvectors of $A$ EDIT: Some eigenvalues of $A$ might be zero.","Applying similarity transform to a matrix $A$ gives: $$M=P^{-1}AP$$ $M$ and $A$ have same eigenvalues. What is the way to to find $P$ such that $M$ is diagonally dominant case of $A$? $M$ is diagonally dominant if $$|{m_{ii}}| \ge \sum\limits_{j \ne i} | {m_{ij}}|\quad {\rm{for \quad  all}}\quad i,{\mkern 1mu} $$ Note: I want $P$ to be something other than eigenvectors of $A$ EDIT: Some eigenvalues of $A$ might be zero.",,['matrices']
52,Non-negative matrix and inverse,Non-negative matrix and inverse,,"Lately, I¬¥ve been struggling with math homework and came across a question I¬¥m not sure how to answer. I will be glad for any help... Suppose we have matrix $A$ (size $n\times n$) and its inverse (lets call it $B$). They are both non-negative in the sense that all their elements $A_{ij}$ and $B_{ij}\geq 0$, where $1\leq i,j\leq n$. The question is, what can we say about these matrices - everything must be justified. This where I got so far: 1) $A$ is regular (otherwise it wouldn't have and inverse - I don't think I have to justify this statement) Are there any other features? I think I can justify some of them by using minor matrices, but I'm not sure how :-(","Lately, I¬¥ve been struggling with math homework and came across a question I¬¥m not sure how to answer. I will be glad for any help... Suppose we have matrix $A$ (size $n\times n$) and its inverse (lets call it $B$). They are both non-negative in the sense that all their elements $A_{ij}$ and $B_{ij}\geq 0$, where $1\leq i,j\leq n$. The question is, what can we say about these matrices - everything must be justified. This where I got so far: 1) $A$ is regular (otherwise it wouldn't have and inverse - I don't think I have to justify this statement) Are there any other features? I think I can justify some of them by using minor matrices, but I'm not sure how :-(",,"['matrices', 'inverse']"
53,Logistic Sigmoid Function with a vector input,Logistic Sigmoid Function with a vector input,,"For a statistical learning problem (classification), I have the data set $\{ (x_i,y_i) \}_{i=1}^n$ with $x_i \in \mathbb{R}^2$ being the input data and $y_i \in \{0,1\}$ the possible classes. The data is used to compute the log-likelihood for the data, in that equation I have to compute the logistic sigmoid function $$\sigma(x_i) = \frac{1}{e^{-x_i} + 1}$$ My problem is: The input data of $x$ is a matrix $X \in \mathbb{R}^{n \times 2}$, now I am confused how I can compute the $\sigma(x_i)$ for a certain value, since one value of the matrix is a tuple, respectively a vector, one row of this matrix. Any hints on how to approach this problem and compute my $\sigma(x_i)$? My matrix looks like that: $$\begin{pmatrix} 1.55545  & -1.00055\\ -1.24155 & 1.58778\\ 1.28068  & -1.0224\\ \vdots   & \vdots\\ -1.68505 & 0.290898\\ 1.73686  & 0.793386\\  \end{pmatrix}$$ Hence $x_1 = (1.55545, -1.00055)$, but what is then: $$\sigma(1.55545, -1.00055) = \frac{1}{e^{????} + 1}$$ The only thing I have found is the Vector exponential which claims that it can be computed by: $$exp(v) = 1 \cosh(|v|) + \frac{v}{|v|} \sinh(|v|)$$","For a statistical learning problem (classification), I have the data set $\{ (x_i,y_i) \}_{i=1}^n$ with $x_i \in \mathbb{R}^2$ being the input data and $y_i \in \{0,1\}$ the possible classes. The data is used to compute the log-likelihood for the data, in that equation I have to compute the logistic sigmoid function $$\sigma(x_i) = \frac{1}{e^{-x_i} + 1}$$ My problem is: The input data of $x$ is a matrix $X \in \mathbb{R}^{n \times 2}$, now I am confused how I can compute the $\sigma(x_i)$ for a certain value, since one value of the matrix is a tuple, respectively a vector, one row of this matrix. Any hints on how to approach this problem and compute my $\sigma(x_i)$? My matrix looks like that: $$\begin{pmatrix} 1.55545  & -1.00055\\ -1.24155 & 1.58778\\ 1.28068  & -1.0224\\ \vdots   & \vdots\\ -1.68505 & 0.290898\\ 1.73686  & 0.793386\\  \end{pmatrix}$$ Hence $x_1 = (1.55545, -1.00055)$, but what is then: $$\sigma(1.55545, -1.00055) = \frac{1}{e^{????} + 1}$$ The only thing I have found is the Vector exponential which claims that it can be computed by: $$exp(v) = 1 \cosh(|v|) + \frac{v}{|v|} \sinh(|v|)$$",,"['matrices', 'statistics', 'machine-learning']"
54,Second conjugate operators and their representations,Second conjugate operators and their representations,,"We can think about a bounded operator $T\colon c_0\to c_0$ as a double-infinite matrix $[T_{mn}]_{m,n\geq 1}$ which acts on a sequence $a=[a_1, a_2, a_3, \ldots ]\in c_0$ in the same way as usual (finite) matrices act on vectors ($n$-tuples of scalars), i.e. $$ Ta=  [T_{mn}][a_n] = \left[ \sum_{n=1}^\infty T_{mn}a_n\right] $$ Suppose $a=[a_1, a_2, a_3, \ldots ]\in \ell^\infty = (c_0)^{**}$. Does the following formula still hold: $$ T^{**}a=  [T_{mn}][a_n] = \left[ \sum_{n=1}^\infty T_{mn}a_n\right] $$","We can think about a bounded operator $T\colon c_0\to c_0$ as a double-infinite matrix $[T_{mn}]_{m,n\geq 1}$ which acts on a sequence $a=[a_1, a_2, a_3, \ldots ]\in c_0$ in the same way as usual (finite) matrices act on vectors ($n$-tuples of scalars), i.e. $$ Ta=  [T_{mn}][a_n] = \left[ \sum_{n=1}^\infty T_{mn}a_n\right] $$ Suppose $a=[a_1, a_2, a_3, \ldots ]\in \ell^\infty = (c_0)^{**}$. Does the following formula still hold: $$ T^{**}a=  [T_{mn}][a_n] = \left[ \sum_{n=1}^\infty T_{mn}a_n\right] $$",,"['matrices', 'functional-analysis', 'banach-spaces']"
55,Eigenvalues of unitary matrices,Eigenvalues of unitary matrices,,"I have a sparse unitary matrix with complex entries and want to compute all its eigenvalues. Unfortunately, matlab doesn't like this. If I try do enter eigs(A, N) (A the matrix, N its size), it tells me that I should use eig(full(A)) instead. This is awfully slow ... comparred to the computation for self-adjoint sparse matrices. Is there any way to do this quicker?","I have a sparse unitary matrix with complex entries and want to compute all its eigenvalues. Unfortunately, matlab doesn't like this. If I try do enter eigs(A, N) (A the matrix, N its size), it tells me that I should use eig(full(A)) instead. This is awfully slow ... comparred to the computation for self-adjoint sparse matrices. Is there any way to do this quicker?",,"['matrices', 'eigenvalues-eigenvectors', 'matlab', 'unitary-matrices']"
56,rotation matrix for particle track,rotation matrix for particle track,,"Struggling with some equations here. I've got a particle I need to track in an absolute reference frame, but each step I move/rotate it relative to its own reference frame. I need to track it's absolute position while moving it relatively. I'm trying to figure out the proper set of rotation matrices. It propagates along the x-axis and I can describe its current direction with [ux, uy, uz], now I need to rotate it about it's own reference frame by the deflection $\theta$ (deflection from incident direction) and azimuthal rotation $\phi$ (rotation about incident direction), and then find its new position, [ux', uy', uz']. Should I multiply by the rotation matrix, ROT[y,$\theta$], then ROT[x,$\phi$] or the opposite order? I'm just getting myself confused. By way of reference, I'm trying to derive the set of equations on (pdf) page 20 of this document . It's a) not derived very well and b) derived for propagation along the z-axis, not the x-axis. TL;DR - I have a vector in space pointing in an arbitrary direction, described by its projections on the world x,y,z-axis. I need to rotate the vector, in relation to its own reference frame, and find the world axis projections. Essentially this is a ray-tracing problem. Incoming ray needs to point in a new direction - what is that direction?","Struggling with some equations here. I've got a particle I need to track in an absolute reference frame, but each step I move/rotate it relative to its own reference frame. I need to track it's absolute position while moving it relatively. I'm trying to figure out the proper set of rotation matrices. It propagates along the x-axis and I can describe its current direction with [ux, uy, uz], now I need to rotate it about it's own reference frame by the deflection $\theta$ (deflection from incident direction) and azimuthal rotation $\phi$ (rotation about incident direction), and then find its new position, [ux', uy', uz']. Should I multiply by the rotation matrix, ROT[y,$\theta$], then ROT[x,$\phi$] or the opposite order? I'm just getting myself confused. By way of reference, I'm trying to derive the set of equations on (pdf) page 20 of this document . It's a) not derived very well and b) derived for propagation along the z-axis, not the x-axis. TL;DR - I have a vector in space pointing in an arbitrary direction, described by its projections on the world x,y,z-axis. I need to rotate the vector, in relation to its own reference frame, and find the world axis projections. Essentially this is a ray-tracing problem. Incoming ray needs to point in a new direction - what is that direction?",,['matrices']
57,"Given $A_i, B_i \in \mathbb{R}^{k \times d}$, minimize $\sum_{i} \lVert U A_i V^T - B_i \rVert_F^2 $ over orthogonal $U, V$.","Given , minimize  over orthogonal .","A_i, B_i \in \mathbb{R}^{k \times d} \sum_{i} \lVert U A_i V^T - B_i \rVert_F^2  U, V","Given a collection of rectangular matrices $A_i, B_i \in \mathbb{R}^{k \times d}$ for $1 \leq i \leq n$ , I am looking for an analytical solution for orthogonal matrices $U \in \mathbb{R}^{k \times k}$ and $V \in \mathbb{R}^{d \times d}$ with $U^T U = I_k$ and $V^T V = I_d$ that minimize \begin{align} \sum_{i = 1}^n \lVert U A_i V^T - B_i\rVert_F^2. \end{align} In the case that $i=1$ the answer should reduce to something similar to this question after diagonalizing $A$ and $B$ via the SVD. I also understand that it may be the case that there are an infinity of solutions which is fine. That said, I'm finding it tricky to attack this problem. One idea I've had is to linearize the system with the Kronecker product to equivalently(?) minimize \begin{align} \lVert W \, [ \textrm{vec}(A_1), \ldots, \textrm{vec}(A_n)] - [\textrm{vec}(B_1), \ldots, \textrm{vec}(B_n)] \rVert_F^2 \end{align} for orthogonal $W$ subject to the constraint that $W = V \otimes U$ , but I don't know how to impose the latter.","Given a collection of rectangular matrices for , I am looking for an analytical solution for orthogonal matrices and with and that minimize In the case that the answer should reduce to something similar to this question after diagonalizing and via the SVD. I also understand that it may be the case that there are an infinity of solutions which is fine. That said, I'm finding it tricky to attack this problem. One idea I've had is to linearize the system with the Kronecker product to equivalently(?) minimize for orthogonal subject to the constraint that , but I don't know how to impose the latter.","A_i, B_i \in \mathbb{R}^{k \times d} 1 \leq i \leq n U \in \mathbb{R}^{k \times k} V \in \mathbb{R}^{d \times d} U^T U = I_k V^T V = I_d \begin{align}
\sum_{i = 1}^n \lVert U A_i V^T - B_i\rVert_F^2.
\end{align} i=1 A B \begin{align}
\lVert W \, [ \textrm{vec}(A_1), \ldots, \textrm{vec}(A_n)] - [\textrm{vec}(B_1), \ldots, \textrm{vec}(B_n)] \rVert_F^2
\end{align} W W = V \otimes U","['matrices', 'matrix-equations', 'least-squares', 'orthogonal-matrices', 'procrustes-problem']"
58,"Is $PGL(2,\mathbb R)$ isomorphic to $SO(1,2)$?",Is  isomorphic to ?,"PGL(2,\mathbb R) SO(1,2)","Consider the following representation $\rho:GL(2,\mathbb R)\to GL(3,\mathbb R):G\mapsto \hat{G}$ where $G=\begin{pmatrix} \alpha&\beta\\ \gamma&\delta \end{pmatrix}\in GL(2,\mathbb R)$ and $\hat{G}:= \frac{1}{\Delta} \begin{pmatrix} \frac{1}{2}\big(\alpha^2+\beta^2+\gamma^2+\delta^2\big)&\frac{\sqrt{2}}{2}(\alpha\gamma+\beta\delta)&-\frac{1}{2}\big(\alpha^2+\beta^2-\gamma^2-\delta^2\big)\\ \sqrt{2}\, (\alpha\beta+\gamma\delta)&\alpha\delta+\beta\gamma&-\sqrt{2}\, (\alpha\beta-\gamma\delta)\\ -\frac{1}{2}\big(\alpha^2-\beta^2+\gamma^2-\delta^2\big)&-\frac{\sqrt{2}}{2}(\alpha\gamma-\beta\delta)&\frac{1}{2}\big(\alpha^2-\beta^2-\gamma^2+\delta^2\big) \end{pmatrix}\in GL(3,\mathbb R)$ with $\Delta=\det G=\alpha\delta-\beta\gamma$ . The kernel of $\rho$ is the normal subgroup of nonzero scalar transformations of $\mathbb R^2$ , namely $Z=\{\lambda I_2|\lambda\in\mathbb R^\times\}$ . Hence, $\rho$ induces a faithful representation $PGL(2,\mathbb R)\to GL(3,\mathbb R)$ where $PGL(2,\mathbb R)$ is the projective general linear group $PGL(2,\mathbb R)=GL(2,\mathbb R)/Z$ . Note furthermore that $\det \hat{G}=1$ and $\hat{G}^T\eta\hat{G}=\eta$ where $\eta=\begin{pmatrix} -2&0&0\\ 0&1&0\\ 0&0&2 \end{pmatrix}$ , hence $\hat{G}\in SO(1,2)$ [since $\eta$ has signature $(1,2)$ ] where $SO(1,2)$ stands for the proper Lorentz group in $2+1$ dimensions. Whenever $G\in SL(2,\mathbb R)$ , $\hat{G}{}^0{}_{0}=\frac{1}{2}\big(\alpha^2+\beta^2+\gamma^2+\delta^2\big)>0$ , so that $\hat{G}\in SO^+(1,2)$ with $SO^+(1,2)$ the proper orthochronous Lorentz group in $2+1$ dimensions, so that $\rho$ reduces to the well-known isomorphism $PSL(2,\mathbb R)\to SO^+(1,2)$ , where $PSL(2,\mathbb R)=SL(2,\mathbb R)/\{-1,1\}$ is the projective special linear group in 2 dimensions. Question: Does $\rho$ induce an isomorphism $PGL(2,\mathbb R)\to SO(1,2)$ ? If yes, are there some references discussing this isomorphism?","Consider the following representation where and with . The kernel of is the normal subgroup of nonzero scalar transformations of , namely . Hence, induces a faithful representation where is the projective general linear group . Note furthermore that and where , hence [since has signature ] where stands for the proper Lorentz group in dimensions. Whenever , , so that with the proper orthochronous Lorentz group in dimensions, so that reduces to the well-known isomorphism , where is the projective special linear group in 2 dimensions. Question: Does induce an isomorphism ? If yes, are there some references discussing this isomorphism?","\rho:GL(2,\mathbb R)\to GL(3,\mathbb R):G\mapsto \hat{G} G=\begin{pmatrix}
\alpha&\beta\\
\gamma&\delta
\end{pmatrix}\in GL(2,\mathbb R) \hat{G}:=
\frac{1}{\Delta}
\begin{pmatrix}
\frac{1}{2}\big(\alpha^2+\beta^2+\gamma^2+\delta^2\big)&\frac{\sqrt{2}}{2}(\alpha\gamma+\beta\delta)&-\frac{1}{2}\big(\alpha^2+\beta^2-\gamma^2-\delta^2\big)\\
\sqrt{2}\, (\alpha\beta+\gamma\delta)&\alpha\delta+\beta\gamma&-\sqrt{2}\, (\alpha\beta-\gamma\delta)\\
-\frac{1}{2}\big(\alpha^2-\beta^2+\gamma^2-\delta^2\big)&-\frac{\sqrt{2}}{2}(\alpha\gamma-\beta\delta)&\frac{1}{2}\big(\alpha^2-\beta^2-\gamma^2+\delta^2\big)
\end{pmatrix}\in GL(3,\mathbb R) \Delta=\det G=\alpha\delta-\beta\gamma \rho \mathbb R^2 Z=\{\lambda I_2|\lambda\in\mathbb R^\times\} \rho PGL(2,\mathbb R)\to GL(3,\mathbb R) PGL(2,\mathbb R) PGL(2,\mathbb R)=GL(2,\mathbb R)/Z \det \hat{G}=1 \hat{G}^T\eta\hat{G}=\eta \eta=\begin{pmatrix}
-2&0&0\\
0&1&0\\
0&0&2
\end{pmatrix} \hat{G}\in SO(1,2) \eta (1,2) SO(1,2) 2+1 G\in SL(2,\mathbb R) \hat{G}{}^0{}_{0}=\frac{1}{2}\big(\alpha^2+\beta^2+\gamma^2+\delta^2\big)>0 \hat{G}\in SO^+(1,2) SO^+(1,2) 2+1 \rho PSL(2,\mathbb R)\to SO^+(1,2) PSL(2,\mathbb R)=SL(2,\mathbb R)/\{-1,1\} \rho PGL(2,\mathbb R)\to SO(1,2)","['matrices', 'group-theory', 'finite-groups', 'lie-groups', 'group-isomorphism']"
59,"Do the left and right ""parts"" of the matrix used when performing Gaussian Elimination have names?","Do the left and right ""parts"" of the matrix used when performing Gaussian Elimination have names?",,"Given a matrix used to perform Gaussian Elimination like this: $$ \begin{bmatrix} 1 & -3 & | & 1 \\ 2 & -7 & | & 3 \end{bmatrix} $$ Which would be derived from a system of equations like this: $$ \begin{cases} x - 3y = 1 \\ 2x - 7y = 3 \end{cases}  $$ Do the different ""sides"", i.e. the parts that come from the right hand side of the equation and the part that comes from the left hand side of the equation, respectively, have names?","Given a matrix used to perform Gaussian Elimination like this: Which would be derived from a system of equations like this: Do the different ""sides"", i.e. the parts that come from the right hand side of the equation and the part that comes from the left hand side of the equation, respectively, have names?","
\begin{bmatrix}
1 & -3 & | & 1 \\
2 & -7 & | & 3
\end{bmatrix}
 
\begin{cases}
x - 3y = 1 \\
2x - 7y = 3
\end{cases} 
","['matrices', 'terminology', 'gaussian-elimination']"
60,Derivative of trace of a matrix function [$\operatorname{Tr}(X\log(Y))$] w.r.t. a scalar,Derivative of trace of a matrix function [] w.r.t. a scalar,\operatorname{Tr}(X\log(Y)),"$\DeclareMathOperator{\Tr}{Tr}$ I'm trying to find a closed form for $\frac{\partial}{\partial \theta}\Tr(X\log(Y))$ where $X(\theta)$ and $Y(\theta)$ are Hermitian positive definite matrices with trace 1 (i.e. full rank density matrices), parametrized by scalar $\theta$ , that in general don't commute. If it was $\frac{\partial}{\partial \theta}\Tr(X\log(X))$ , I could write down the Taylor expansion of the log and using the cyclic property of the trace rearrange all the terms coming from differentiating $X^n$ and pretend like I was doing single variable calculus; which would give me $\Tr(X'\log(X) - X')$ , where $X'\equiv\frac{\partial X}{\partial \theta}$ . However, if I were to apply the same approach to $\frac{\partial}{\partial \theta}\Tr(X\log(Y))$ , the Taylor expansion gives me a sum of terms like $\Tr(X(Y-1)^n)$ (here $1$ is the identity matrix of the same dimension as $X$ & $Y$ ): $$ \Tr(X\log(Y)) = \sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}\Tr(X(Y-1)^n) $$ When differentiated, the trace in the $n$ -th term produces $$ \Tr(X'(Y-1)^n)+\Tr(XY'(Y-1)^{n-1})+\dots+\Tr(X(Y-1)^k\,Y'\,(Y-1)^{n-k-1})+\dots+\Tr(X(Y-1)^{n-1}Y') $$ The first term can be separated to give $\Tr(X'\log(Y))$ . However, the rest of the terms can't be rearranged in a nice way since $X$ breaks the sort of cyclic symmetry we had in the previous case. This is the point I'm stuck at; is there a different approach with which I can manipulate this expression to obtain a closed form derivative? Thanks in advance. Context: I'm trying to differentiate von Neumann relative entropy in quantum mechanics/information.","I'm trying to find a closed form for where and are Hermitian positive definite matrices with trace 1 (i.e. full rank density matrices), parametrized by scalar , that in general don't commute. If it was , I could write down the Taylor expansion of the log and using the cyclic property of the trace rearrange all the terms coming from differentiating and pretend like I was doing single variable calculus; which would give me , where . However, if I were to apply the same approach to , the Taylor expansion gives me a sum of terms like (here is the identity matrix of the same dimension as & ): When differentiated, the trace in the -th term produces The first term can be separated to give . However, the rest of the terms can't be rearranged in a nice way since breaks the sort of cyclic symmetry we had in the previous case. This is the point I'm stuck at; is there a different approach with which I can manipulate this expression to obtain a closed form derivative? Thanks in advance. Context: I'm trying to differentiate von Neumann relative entropy in quantum mechanics/information.","\DeclareMathOperator{\Tr}{Tr} \frac{\partial}{\partial \theta}\Tr(X\log(Y)) X(\theta) Y(\theta) \theta \frac{\partial}{\partial \theta}\Tr(X\log(X)) X^n \Tr(X'\log(X) - X') X'\equiv\frac{\partial X}{\partial \theta} \frac{\partial}{\partial \theta}\Tr(X\log(Y)) \Tr(X(Y-1)^n) 1 X Y 
\Tr(X\log(Y)) = \sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}\Tr(X(Y-1)^n)
 n 
\Tr(X'(Y-1)^n)+\Tr(XY'(Y-1)^{n-1})+\dots+\Tr(X(Y-1)^k\,Y'\,(Y-1)^{n-k-1})+\dots+\Tr(X(Y-1)^{n-1}Y')
 \Tr(X'\log(Y)) X","['matrices', 'derivatives', 'logarithms', 'matrix-calculus', 'entropy']"
61,Find a rotation matrix with two constraints: Aligns two vectors AND a third vector is perpendicular to a fourth vector when transformed,Find a rotation matrix with two constraints: Aligns two vectors AND a third vector is perpendicular to a fourth vector when transformed,,"I have four vectors $ d,w,c,n \in \mathscr{R}^3$ .  I want to find a rotation matrix $R$ that satisfies these constraints: $w$ is aligned with $d$ after rotation  AND $c$ is perpendicular to $n$ after rotation. You can imagine it in two steps: after a first rotation that aligns $w$ with $d$ , make a second rotation around $d$ so that $c$ becomes perpendicular to $n$ . In a particular problem I am working on $d=[1,1,1]^T$ and $n=[0,0,1]^T$ . I believe the constraints can be expressed with these two equations $$ d \times Rw=0$$ $$ n^T  Rc=0$$ In addition, we must impose a third condition for $R$ being a rotation matrix $$ RR^T=I$$ Is there a nice way to solve for R using matrix algebra? Or how to formulate the two steps above? I tried to play with Kronecker products to form a vectorized version of $R$ , but did not reach a solution. With $S=skew(d)$ being the skew vector for cross product: $$d \times Rw=SRw=0 \rightarrow (w^T \oplus S)vec(R)=0$$ $$n^T  Rc=0 \rightarrow(c^T \oplus n^T)vec(R)=0$$ but could not impose the $RR^T=I$","I have four vectors .  I want to find a rotation matrix that satisfies these constraints: is aligned with after rotation  AND is perpendicular to after rotation. You can imagine it in two steps: after a first rotation that aligns with , make a second rotation around so that becomes perpendicular to . In a particular problem I am working on and . I believe the constraints can be expressed with these two equations In addition, we must impose a third condition for being a rotation matrix Is there a nice way to solve for R using matrix algebra? Or how to formulate the two steps above? I tried to play with Kronecker products to form a vectorized version of , but did not reach a solution. With being the skew vector for cross product: but could not impose the"," d,w,c,n \in \mathscr{R}^3 R w d c n w d d c n d=[1,1,1]^T n=[0,0,1]^T  d \times Rw=0  n^T  Rc=0 R  RR^T=I R S=skew(d) d \times Rw=SRw=0 \rightarrow (w^T \oplus S)vec(R)=0 n^T  Rc=0 \rightarrow(c^T \oplus n^T)vec(R)=0 RR^T=I","['matrices', 'rotations', 'kronecker-product']"
62,Why $\mathcal{S}^n_d$ is a manifold?,Why  is a manifold?,\mathcal{S}^n_d,Let $\mathcal{S}^n_d$ be the set of all $n \times n$ real symmetric matrices of rank $d$ . How can I prove that $\mathcal{S}^n_d$ is a $dn-\binom{d}{2}$ dimensional manifold?,Let be the set of all real symmetric matrices of rank . How can I prove that is a dimensional manifold?,\mathcal{S}^n_d n \times n d \mathcal{S}^n_d dn-\binom{d}{2},"['matrices', 'manifolds', 'smooth-manifolds', 'matrix-decomposition']"
63,Why isn't the matrix logarithm defined as the integral of the inverse function?,Why isn't the matrix logarithm defined as the integral of the inverse function?,,"I just started to play around with the idea of the matrix logarithm. From my research, the simplest way to compute the value of the matrix logarithm is a generalization of the Taylor series of the natural logarithm ‚Äî you just replace the $1$ , with $I$ : $$\ln(A + I) = \sum_{n=1}^{\infty} (-1)^{-n + 1} \frac{A^{n}}{n}$$ Why not define the matrix logarithm, as the generalization of the $\ln(x)$ as the integral of the function $\frac{1}{x}$ instead $?$ $$\ln(x) = \int_{1}^{x} \frac{1}{t} dt = \lim_{n\to\infty} \sum_{i=1}^{n-1}\frac{1}{1 + \frac{(x - 1)i}{n}}\frac{x-1}{n}$$ so why not define the matrix logarithm as such $$\ln(A) = \int_{I}^{A} \frac{1}{x} dx = \lim_{n\to\infty} \sum_{i=1}^{n-1}\frac{1}{I + \frac{(A - I)i}{n}}\frac{A-I}{n}?$$ I know it's a bit weird that the limits of integration are matrices, but I don't see anything wrong with that, especially because we can just use the Riemann sum directly. Do we avoid using that definition because not every matrix is invertiable $?$ Because the two matrices in this sum don't always commute? or do we do use this definition and my reseasrch was incomplete?","I just started to play around with the idea of the matrix logarithm. From my research, the simplest way to compute the value of the matrix logarithm is a generalization of the Taylor series of the natural logarithm ‚Äî you just replace the , with : Why not define the matrix logarithm, as the generalization of the as the integral of the function instead so why not define the matrix logarithm as such I know it's a bit weird that the limits of integration are matrices, but I don't see anything wrong with that, especially because we can just use the Riemann sum directly. Do we avoid using that definition because not every matrix is invertiable Because the two matrices in this sum don't always commute? or do we do use this definition and my reseasrch was incomplete?",1 I \ln(A + I) = \sum_{n=1}^{\infty} (-1)^{-n + 1} \frac{A^{n}}{n} \ln(x) \frac{1}{x} ? \ln(x) = \int_{1}^{x} \frac{1}{t} dt = \lim_{n\to\infty} \sum_{i=1}^{n-1}\frac{1}{1 + \frac{(x - 1)i}{n}}\frac{x-1}{n} \ln(A) = \int_{I}^{A} \frac{1}{x} dx = \lim_{n\to\infty} \sum_{i=1}^{n-1}\frac{1}{I + \frac{(A - I)i}{n}}\frac{A-I}{n}? ?,"['matrices', 'limits', 'logarithms', 'matrix-calculus', 'matrix-analysis']"
64,Algebra of upper triangular matrices,Algebra of upper triangular matrices,,"Let $k$ be a field and $R$ the algebra of $3\times3$ upper triangular matrices $(a_{ij})$ st $a_{11}=a_{22}=a_{33}$ . Find the Jacobson radical $J(R)$ of $R$ Attempt : Using the characterization $y\in J(R)\iff 1-xy\in U(R),\ \forall x\in R$ I found that $$J(R)=\left\{\begin{pmatrix} 0 \ a \ b\\ 0\ 0 \ c \\ 0\ 0 \ 0\end{pmatrix}:a,b,c\in k\right\}$$ Show that every two simple $R-$ modules are isomorphic. Attempt : The left simple $R-$ modules coincide with left simple $R/J(R)-$ modules and $R/J(R)=k$ which is a simple Artinian ring. Hence all simple $R-$ modules are isomorphic. Let $M,N$ two semisimple $R-$ modules with $\dim_kM=\dim_kN<\infty$ . Is it true that $M$ and $N$ are isomorphic? Attempt : Yes. It is $M=V^m,\ N=V^n$ where $V$ is the unique up to isomorphism simple $R-$ module. So $n\dim_kV=\dim_kN=\dim_kM=m\dim_kV<\infty \Rightarrow m=n \Rightarrow M\cong N$ . Let $M,N$ be two $R-$ modules with $\dim_kM=\dim_kN<\infty$ .  Is it true that $M$ and $N$ are isomorphic? Attempt : I suppose they are not since $R$ is not a simple algebra ( $\left\{\begin{pmatrix} 0 \ 0 \ b\\ 0\ 0 \ 0 \\ 0\ 0 \ 0\end{pmatrix}:b\in k\right\}$ is an ideal) but I can't find a counter-example. Is my approach correct? What about $4$ ? Can you give a general example where $R$ is a $k-$ algebra of finite dimension (not simple) and the equivalence $M\cong N\iff \dim_kM=\dim_kN<\infty$ does not hold? Thanks in advance!",Let be a field and the algebra of upper triangular matrices st . Find the Jacobson radical of Attempt : Using the characterization I found that Show that every two simple modules are isomorphic. Attempt : The left simple modules coincide with left simple modules and which is a simple Artinian ring. Hence all simple modules are isomorphic. Let two semisimple modules with . Is it true that and are isomorphic? Attempt : Yes. It is where is the unique up to isomorphism simple module. So . Let be two modules with .  Is it true that and are isomorphic? Attempt : I suppose they are not since is not a simple algebra ( is an ideal) but I can't find a counter-example. Is my approach correct? What about ? Can you give a general example where is a algebra of finite dimension (not simple) and the equivalence does not hold? Thanks in advance!,"k R 3\times3 (a_{ij}) a_{11}=a_{22}=a_{33} J(R) R y\in J(R)\iff 1-xy\in U(R),\ \forall x\in R J(R)=\left\{\begin{pmatrix} 0 \ a \ b\\ 0\ 0 \ c \\ 0\ 0 \ 0\end{pmatrix}:a,b,c\in k\right\} R- R- R/J(R)- R/J(R)=k R- M,N R- \dim_kM=\dim_kN<\infty M N M=V^m,\ N=V^n V R- n\dim_kV=\dim_kN=\dim_kM=m\dim_kV<\infty \Rightarrow m=n \Rightarrow M\cong N M,N R- \dim_kM=\dim_kN<\infty M N R \left\{\begin{pmatrix} 0 \ 0 \ b\\ 0\ 0 \ 0 \\ 0\ 0 \ 0\end{pmatrix}:b\in k\right\} 4 R k- M\cong N\iff \dim_kM=\dim_kN<\infty","['matrices', 'ring-theory', 'solution-verification', 'noncommutative-algebra']"
65,Find the cardinality of the image of $\varphi.$,Find the cardinality of the image of,\varphi.,"Consider the group homomorphism $\varphi : SL_2 (\Bbb Z) \longrightarrow SL_2 (\Bbb Z/ 3 \Bbb Z)$ defined by $$\begin{pmatrix} a & b \\ c & d \end{pmatrix} \mapsto \begin{pmatrix} \overline {a} & \overline {b} \\ \overline {c} & \overline {d} \end{pmatrix}.$$ What is the cardinality of the image of $\varphi$ ? Since $\text {Im} (\varphi)$ is a subgroup of $SL_2 (\Bbb Z/ 3 \Bbb Z)$ so by Lagrange's theorem $\#\ \text {Im} (\varphi)\ \big |\ \#\ SL_2 (\Bbb Z/ 3 \Bbb Z) = 24.$ What I have observed is that $\#\ \text {Im} (\varphi) \geq 7.$ So $\#\ \text {Im} (\varphi) = 8,12\ \text {or}\ 24.$ I have just seen that the image contains at least $10$ elements. So the possibility for cardinality of $\text {Im} (\varphi)$ is $12$ or $24.$",Consider the group homomorphism defined by What is the cardinality of the image of ? Since is a subgroup of so by Lagrange's theorem What I have observed is that So I have just seen that the image contains at least elements. So the possibility for cardinality of is or,"\varphi : SL_2 (\Bbb Z) \longrightarrow SL_2 (\Bbb Z/ 3 \Bbb Z) \begin{pmatrix} a & b \\ c & d \end{pmatrix} \mapsto \begin{pmatrix} \overline {a} & \overline {b} \\ \overline {c} & \overline {d} \end{pmatrix}. \varphi \text {Im} (\varphi) SL_2 (\Bbb Z/ 3 \Bbb Z) \#\ \text {Im} (\varphi)\ \big |\ \#\ SL_2 (\Bbb Z/ 3 \Bbb Z) = 24. \#\ \text {Im} (\varphi) \geq 7. \#\ \text {Im} (\varphi) = 8,12\ \text {or}\ 24. 10 \text {Im} (\varphi) 12 24.","['matrices', 'group-theory', 'group-homomorphism']"
66,Explicit group decomposition,Explicit group decomposition,,"Let $E/F$ be a quadratic extension of non-archimedean fields. Let $p$ its maximal ideal and $O$ its ring of integers. I am interested in the subgroup $A$ of invertible matrices of the form $$ \left( \begin{array}{ccc} O & O & O \\ p & O & O \\ p&p&O \end{array} \right) $$ I would like to understand its index in the subgroup of all invertible matrices with integer coefficients. However, I am stuck with a problem that seems elementary. Indeed, I could think of Bruhat decomposition but this would give double classes more than left/right-classes, and also I don't know how to deal with it in the case of non-split groups. Is there any other way to compute this index (or the volume with respect to a  well-normalised Haar measure?)","Let be a quadratic extension of non-archimedean fields. Let its maximal ideal and its ring of integers. I am interested in the subgroup of invertible matrices of the form I would like to understand its index in the subgroup of all invertible matrices with integer coefficients. However, I am stuck with a problem that seems elementary. Indeed, I could think of Bruhat decomposition but this would give double classes more than left/right-classes, and also I don't know how to deal with it in the case of non-split groups. Is there any other way to compute this index (or the volume with respect to a  well-normalised Haar measure?)","E/F p O A 
\left(
\begin{array}{ccc}
O & O & O \\
p & O & O \\
p&p&O
\end{array}
\right)
","['matrices', 'group-theory', 'field-theory', 'extension-field']"
67,The proof of positive semi-definite for a kernel,The proof of positive semi-definite for a kernel,,"How to prove the following kernel $K$ over $\mathbb R \times \mathbb R$ is positive semi-definite: $$K(x_i, x_j) = e^{-\lambda[\sin(x_i - x_j)]^2},$$ where $\lambda > 0$ . It looks like the gaussian kernel $e^{-\lambda\|x_i - x_j\|^2}$ . How can we link $\sin$ function to some kinds of norm? Or equivalently, how to prove the matrix $A$ defined by $$A_{ij} = e^{-\lambda[\sin(x_i - x_j)]^2}$$ is positive semi-definite for any $\lambda > 0$ and $x_1, \cdots, x_n > 0$ ?","How to prove the following kernel over is positive semi-definite: where . It looks like the gaussian kernel . How can we link function to some kinds of norm? Or equivalently, how to prove the matrix defined by is positive semi-definite for any and ?","K \mathbb R \times \mathbb R K(x_i, x_j) = e^{-\lambda[\sin(x_i - x_j)]^2}, \lambda > 0 e^{-\lambda\|x_i - x_j\|^2} \sin A A_{ij} = e^{-\lambda[\sin(x_i - x_j)]^2} \lambda > 0 x_1, \cdots, x_n > 0","['matrices', 'positive-semidefinite']"
68,Find minimal number of elements in matrix.,Find minimal number of elements in matrix.,,"Consider a $A \in Mat_{n}(\{+1,-1\})$ (square matrix consisted of +1,-1). Now we can make and operation majority , i.e. $a_{i,j} = $ median of his neighborhoods(closest elements around him, i.e. closest 3,5, or 8 elements). If number of elements are equal , then majority return -1 , i.e.: $\begin{pmatrix}   -1& -1 &1\\   -1& 1 &1\\   -1& 1& 1\\ \end{pmatrix} \to \begin{pmatrix}   -1& 1 &1\\   -1& -1 &1\\   1& 1& 1\\ \end{pmatrix}$ medium element go to $-1$ , because of there are equal number of +1/-1 . After this we will have another matrix. Obviously there could be three situations (depends of initial positions of +1/-1): after some number of majority operations there will be 1 matrix, -1 matrix or matrix will cycled. So my question: does there some estimates on number of $1$ ( $-1$ ) to get 1 matrix ( -1 matrix). I've thought that it should be $O(n^{2})$ (actually I thought that $n(n-1)$ is enough for $n>3$ ). For easier understanding I've left an example: $\begin{pmatrix}   -1& -1\\   1& -1 \end{pmatrix} \to \begin{pmatrix}   -1& -1\\   -1& -1 \end{pmatrix} $ Added example for $n=3$ $\begin{pmatrix}   1& 1& 1\\   -1& -1& 1\\   1 & 1 &-1 \end{pmatrix} \to \begin{pmatrix}   -1& 1& 1\\   1& 1& 1\\   -1 & -1 &1 \end{pmatrix} \to \begin{pmatrix}   1& 1& 1\\   1& 1& 1\\   1 & 1 &1 \end{pmatrix}$ EDIT 1 : I've tried to consider such sequence in OEIS.  First of all I've found that for $n =2$ we have $f(2) = 3$ , $n = 3$ we have $f(3) = $ 7 , if $n = 4$ we have $f(4) = 11$ and probably for $n = 5$ $f(5) = 17$ . I've considered some sequences but for some large $n$ this sequences became near the $n/2$ . Which contradict with my logic.","Consider a (square matrix consisted of +1,-1). Now we can make and operation majority , i.e. median of his neighborhoods(closest elements around him, i.e. closest 3,5, or 8 elements). If number of elements are equal , then majority return -1 , i.e.: medium element go to , because of there are equal number of +1/-1 . After this we will have another matrix. Obviously there could be three situations (depends of initial positions of +1/-1): after some number of majority operations there will be 1 matrix, -1 matrix or matrix will cycled. So my question: does there some estimates on number of ( ) to get 1 matrix ( -1 matrix). I've thought that it should be (actually I thought that is enough for ). For easier understanding I've left an example: Added example for EDIT 1 : I've tried to consider such sequence in OEIS.  First of all I've found that for we have , we have 7 , if we have and probably for . I've considered some sequences but for some large this sequences became near the . Which contradict with my logic.","A \in Mat_{n}(\{+1,-1\}) a_{i,j} =  \begin{pmatrix}
  -1& -1 &1\\
  -1& 1 &1\\
  -1& 1& 1\\
\end{pmatrix} \to \begin{pmatrix}
  -1& 1 &1\\
  -1& -1 &1\\
  1& 1& 1\\
\end{pmatrix} -1 1 -1 O(n^{2}) n(n-1) n>3 \begin{pmatrix}
  -1& -1\\
  1& -1
\end{pmatrix} \to \begin{pmatrix}
  -1& -1\\
  -1& -1
\end{pmatrix}  n=3 \begin{pmatrix}
  1& 1& 1\\
  -1& -1& 1\\
  1 & 1 &-1
\end{pmatrix} \to \begin{pmatrix}
  -1& 1& 1\\
  1& 1& 1\\
  -1 & -1 &1
\end{pmatrix} \to \begin{pmatrix}
  1& 1& 1\\
  1& 1& 1\\
  1 & 1 &1
\end{pmatrix} n =2 f(2) = 3 n = 3 f(3) =  n = 4 f(4) = 11 n = 5 f(5) = 17 n n/2","['matrices', 'discrete-mathematics', 'cellular-automata']"
69,Find the Inverse of an Infinite Square Matrix,Find the Inverse of an Infinite Square Matrix,,For my mathematics assignment I am using polynomial interpolation to solve certain problems and I end up with the following scenario: $\begin{bmatrix}... & 0 &0 & 0 & 1\\... &1^3 & 1^2 & 1 & 1\\ ... &2^3 & 2^2 & 2 & 1\\... &3^3 & 3^2 & 3 & 1\\ \unicode{x22F0} & \vdots & \vdots & \vdots & \vdots\end{bmatrix}^{-1} \unicode{x22c5} \begin{bmatrix}a\\b\\c\\d\\ \vdots \end{bmatrix}$ Where the value in the rows of the produced single-column matrix correspond to the coefficients of an infinite-polynomial with decreasing powers. I was wondering is there was a way to either invert this infinite matrix or find what happens to the polynomial as I increase the size of the matrices used (e.g. see if it approaches another function's taylor or power series),For my mathematics assignment I am using polynomial interpolation to solve certain problems and I end up with the following scenario: Where the value in the rows of the produced single-column matrix correspond to the coefficients of an infinite-polynomial with decreasing powers. I was wondering is there was a way to either invert this infinite matrix or find what happens to the polynomial as I increase the size of the matrices used (e.g. see if it approaches another function's taylor or power series),\begin{bmatrix}... & 0 &0 & 0 & 1\\... &1^3 & 1^2 & 1 & 1\\ ... &2^3 & 2^2 & 2 & 1\\... &3^3 & 3^2 & 3 & 1\\ \unicode{x22F0} & \vdots & \vdots & \vdots & \vdots\end{bmatrix}^{-1} \unicode{x22c5} \begin{bmatrix}a\\b\\c\\d\\ \vdots \end{bmatrix},"['matrices', 'polynomials', 'infinite-matrices']"
70,Matrix Inverse is a Uniformly Continuous function for Uniformly Positive Definite matrices?,Matrix Inverse is a Uniformly Continuous function for Uniformly Positive Definite matrices?,,"Definition 1: A collection of $k\times k$ positive semidefinite matrices $\{A_n\}$ is said to be uniformly positive definite if for some $\eta > 0$, $det(A_n) > \eta$. Definition 2: A function $f$ is uniformly continuous on $B$ if for any $\epsilon > 0$, there exists a $\delta > 0$ such that for $x, y \in B$, $d(x,y) < \delta$ implies $d(f(x),f(y)) < \epsilon$. I need to prove that the matrix inverse is a uniformly continuous function for uniformly positive definite matrices, using the sup-metric, $d(M, N) = \max_{ij}|m_{ij} - n_{ij}| = ||M - N||_{\infty}$. The statement is clearly true for scalars $\{a_n\}$, in that if we have $a_n > \eta > 0$ for all $n$, then have $m = \eta^{-1}$ larger than any other value of the function. For any $\epsilon >0$, let $\epsilon^* = \min(\epsilon, m/2)$. We take a $\delta$ such that $0 < \delta < (m - \epsilon^*)^{-1}$, and for that $\delta$ we have $|x - y| < \delta$ implies $|x^{-1} - y^{-1}| < \epsilon$. However, I do not now how to generalize this proof to matrices. Any hint?","Definition 1: A collection of $k\times k$ positive semidefinite matrices $\{A_n\}$ is said to be uniformly positive definite if for some $\eta > 0$, $det(A_n) > \eta$. Definition 2: A function $f$ is uniformly continuous on $B$ if for any $\epsilon > 0$, there exists a $\delta > 0$ such that for $x, y \in B$, $d(x,y) < \delta$ implies $d(f(x),f(y)) < \epsilon$. I need to prove that the matrix inverse is a uniformly continuous function for uniformly positive definite matrices, using the sup-metric, $d(M, N) = \max_{ij}|m_{ij} - n_{ij}| = ||M - N||_{\infty}$. The statement is clearly true for scalars $\{a_n\}$, in that if we have $a_n > \eta > 0$ for all $n$, then have $m = \eta^{-1}$ larger than any other value of the function. For any $\epsilon >0$, let $\epsilon^* = \min(\epsilon, m/2)$. We take a $\delta$ such that $0 < \delta < (m - \epsilon^*)^{-1}$, and for that $\delta$ we have $|x - y| < \delta$ implies $|x^{-1} - y^{-1}| < \epsilon$. However, I do not now how to generalize this proof to matrices. Any hint?",,"['matrices', 'uniform-continuity', 'positive-definite', 'positive-semidefinite']"
71,"Prove that $SL(2,R)$ is a subgroup of $GL(2,R)$.",Prove that  is a subgroup of .,"SL(2,R) GL(2,R)","Let $SL(2,R)=[A \in GL(2,R); det(A)=1]$ . Prove that $SL(2,R)$ is a subgroup of $GL(2,R)$ . Here is what I have using subgroup criteria but I'm not sure if this proof is right. Proof: Clearly $\begin{bmatrix}1&0\\0&1\end{bmatrix}=I_{2} \in SL(2,R)$ . Thus, $SL(2,R)$ is nonempty. I) Closure: Let $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}\in SL(2,R)$ and $B=\begin{bmatrix}a'&b'\\c'&d'\end{bmatrix}\in SL(2,R)$ . Also, $\det(AB)=det(A)*det(B)=1*1=1$ . So $AB\in SL(2,R)$ . II) Inverses: Let $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}\in SL(2,R)$ . Then, $A^{-1}=\frac{1}{det(A)}*\begin{bmatrix}d&-b\\-c&a\end{bmatrix}=\begin{bmatrix}d&-b\\-c&a\end{bmatrix}\in SL(2,R), d\et(A^{-1})=det(A)^{-1}=1^{-1}=1$ . Thus, $A^{-1}\in SL(2,R)$ . Thus, $SL(2,R)$ is a subgroup of $GL(2,R)$ .","Let . Prove that is a subgroup of . Here is what I have using subgroup criteria but I'm not sure if this proof is right. Proof: Clearly . Thus, is nonempty. I) Closure: Let and . Also, . So . II) Inverses: Let . Then, . Thus, . Thus, is a subgroup of .","SL(2,R)=[A \in GL(2,R); det(A)=1] SL(2,R) GL(2,R) \begin{bmatrix}1&0\\0&1\end{bmatrix}=I_{2} \in SL(2,R) SL(2,R) A=\begin{bmatrix}a&b\\c&d\end{bmatrix}\in SL(2,R) B=\begin{bmatrix}a'&b'\\c'&d'\end{bmatrix}\in SL(2,R) \det(AB)=det(A)*det(B)=1*1=1 AB\in SL(2,R) A=\begin{bmatrix}a&b\\c&d\end{bmatrix}\in SL(2,R) A^{-1}=\frac{1}{det(A)}*\begin{bmatrix}d&-b\\-c&a\end{bmatrix}=\begin{bmatrix}d&-b\\-c&a\end{bmatrix}\in SL(2,R), d\et(A^{-1})=det(A)^{-1}=1^{-1}=1 A^{-1}\in SL(2,R) SL(2,R) GL(2,R)","['abstract-algebra', 'matrices', 'group-theory', 'solution-verification']"
72,Can a large number of small matrices be multiplied quickly?,Can a large number of small matrices be multiplied quickly?,,"I know two large matrices can be multiplied faster than one would naively expect.  A large number of the same matrix can be multiplied quickly using repeated squaring.  But what about a large number of small matrices?  Specifically, can I find the product of one million two by two matrices all the entries of which are positive integers less than five quickly (or faster than expected)?","I know two large matrices can be multiplied faster than one would naively expect.  A large number of the same matrix can be multiplied quickly using repeated squaring.  But what about a large number of small matrices?  Specifically, can I find the product of one million two by two matrices all the entries of which are positive integers less than five quickly (or faster than expected)?",,"['matrices', 'algorithms']"
73,kronecker product of three matrices,kronecker product of three matrices,,"Facts: For matrices $A_i\in \mathbb{R}^{n\times n}$ with $i=1, 2, 3$, we have the following equation: $$ A_1\otimes A_2 \otimes A_3 = (A_1\otimes I_{n^2})(I_{n}\otimes A_2 \otimes I_n)(I_{n^2}\otimes A_3), $$ where $I_n$ represents an $n$ by $n$ identity matrix, and $\otimes$ denotes the Kronecker product. My question is what would happen in the equation if matrices $A_i$ are not square matrices, i.e., $A_i\in \mathbb{R}^{m\times n}$, where $m,n$ are not necessarily equal. Is there a way to prove it? Thank you very much! Pulong","Facts: For matrices $A_i\in \mathbb{R}^{n\times n}$ with $i=1, 2, 3$, we have the following equation: $$ A_1\otimes A_2 \otimes A_3 = (A_1\otimes I_{n^2})(I_{n}\otimes A_2 \otimes I_n)(I_{n^2}\otimes A_3), $$ where $I_n$ represents an $n$ by $n$ identity matrix, and $\otimes$ denotes the Kronecker product. My question is what would happen in the equation if matrices $A_i$ are not square matrices, i.e., $A_i\in \mathbb{R}^{m\times n}$, where $m,n$ are not necessarily equal. Is there a way to prove it? Thank you very much! Pulong",,"['matrices', 'kronecker-product']"
74,What are the hyperbolic rotation matrices in 3 and 4 dimensions?,What are the hyperbolic rotation matrices in 3 and 4 dimensions?,,So the hyperbola-preserving transformation in 2 dimensional space is given by the matrix  \begin{pmatrix} \cosh(\phi) & \sinh(\phi) \\ \sinh(\phi) & \cosh(\phi) \end{pmatrix} I'm wondering what such a matrix would be in 3 dimensional space (so that it preserves 2 dimensional hyperboloids) and 4 dimensional space (so that it preserves 3 dimensional hyperboloids). Sources or derivations would be appreciated. Thank you!,So the hyperbola-preserving transformation in 2 dimensional space is given by the matrix  \begin{pmatrix} \cosh(\phi) & \sinh(\phi) \\ \sinh(\phi) & \cosh(\phi) \end{pmatrix} I'm wondering what such a matrix would be in 3 dimensional space (so that it preserves 2 dimensional hyperboloids) and 4 dimensional space (so that it preserves 3 dimensional hyperboloids). Sources or derivations would be appreciated. Thank you!,,"['matrices', 'hyperbolic-geometry']"
75,Skew-symmetric parts of stochastic matrices,Skew-symmetric parts of stochastic matrices,,"It's easy to see that the set $\{W - W^T : W \in \mathbb{R}^{n \times n}\}$ is precisely the set of real skew-symmetric matrices. This continues to be the case if we restrict to (entry-wise) non-negative matrices (i.e., $$\{W - W^T : W \in \mathbb{R}^{n \times n}\} = \{W - W^T : W \in \mathbb{R}^{n \times n}, \text{ each } W_{i,j} \geq 0\} = \{A \in \mathbb{R}^{n \times n} : A \text{ is skew-symmetric}\}).$$ Is there a simple condition if we restrict to non-negative right-stochastic matrices? (i.e., those whose rows sum to $1$)? That is, is there a simple condition $C$ on $A$ such that $$\{W - W^T : W \in \mathbb{R}^{n \times n}, \text{ each } W_{i,j} \geq 0, W \text{ right-stochastic}\} = \{A \in \mathbb{R}^{n \times n} : A \text{ is skew-symmetric}, C \text{ holds}\})?$$ For context, I reduced a problem to the optimization problem $$\min_{W \in \mathbb{R}^{n \times n}} x^T (W - W^T) y$$ where $x, y \in \mathbb{R}^n$ are fixed vectors, subject to $W$ being non-negative and right stochastic, and I'm wondering whether there's a simple equivalent problem of the form $$\min_{A \in \mathbb{R}^{n \times n} \text{skew-symmetric}} x^T A y.$$","It's easy to see that the set $\{W - W^T : W \in \mathbb{R}^{n \times n}\}$ is precisely the set of real skew-symmetric matrices. This continues to be the case if we restrict to (entry-wise) non-negative matrices (i.e., $$\{W - W^T : W \in \mathbb{R}^{n \times n}\} = \{W - W^T : W \in \mathbb{R}^{n \times n}, \text{ each } W_{i,j} \geq 0\} = \{A \in \mathbb{R}^{n \times n} : A \text{ is skew-symmetric}\}).$$ Is there a simple condition if we restrict to non-negative right-stochastic matrices? (i.e., those whose rows sum to $1$)? That is, is there a simple condition $C$ on $A$ such that $$\{W - W^T : W \in \mathbb{R}^{n \times n}, \text{ each } W_{i,j} \geq 0, W \text{ right-stochastic}\} = \{A \in \mathbb{R}^{n \times n} : A \text{ is skew-symmetric}, C \text{ holds}\})?$$ For context, I reduced a problem to the optimization problem $$\min_{W \in \mathbb{R}^{n \times n}} x^T (W - W^T) y$$ where $x, y \in \mathbb{R}^n$ are fixed vectors, subject to $W$ being non-negative and right stochastic, and I'm wondering whether there's a simple equivalent problem of the form $$\min_{A \in \mathbb{R}^{n \times n} \text{skew-symmetric}} x^T A y.$$",,"['matrices', 'optimization', 'stochastic-matrices']"
76,What is a generic matrix,What is a generic matrix,,What is a generic matrix? I try to google this but even can not find a definition. And what is a generic nilpotent matrix? Does generic has some canonical meaning?,What is a generic matrix? I try to google this but even can not find a definition. And what is a generic nilpotent matrix? Does generic has some canonical meaning?,,[]
77,Factor the matrix (scalar $\times A$) into permutations of $A$,Factor the matrix (scalar ) into permutations of,\times A A,"Here's an example of $A . B = scalar \times C$, done with magic squares. The last square does not have a consecutive range of digits. Drop the magic square requirement.  In $2\times2$ matrices we have the following, where a matrix times a scalar is factored into permutations of the original matrix. All entries of the matrix are distinct. $$(\begin{pmatrix} -1 & 0 \\  2 & 1  \\  \end{pmatrix} + a)\cdot (\begin{pmatrix} 1 & 0 \\  -1 & 2  \\ \end{pmatrix}+a) = (2 a + 1)\times(\begin{pmatrix} -1 & 0 \\  1 & 2  \\  \end{pmatrix}+a)$$ Here are examples with $3 \times 3$ matrices.  Can these be canonicalized in some way and all solutions listed? $$ \begin{pmatrix} 3 & 2 & 4 \\  1 & -3 & -1 \\ -4 & 0 & -2 \\ \end{pmatrix} \cdot  \begin{pmatrix} -3 & -1 & 2 \\  -2 & 0 & 3 \\ 4 & 1 & -4 \\ \end{pmatrix} =  \begin{pmatrix} 3 & 1 & -4 \\  -1 & -2 & -3 \\ 4 & 2 & 0 \\ \end{pmatrix}$$ $$ \begin{pmatrix} 4 & -2 & 1 \\  -3 & 2 & -1 \\ -4 & 3 & 0 \\ \end{pmatrix} \cdot  \begin{pmatrix} 1 & -1 & -2 \\  2 & 0 & -4 \\ 3 & 4 & -3 \\ \end{pmatrix} =  \begin{pmatrix} 3 & 0 & -3 \\  -2 & -1 & 1 \\ 2 & 4 & -4 \\ \end{pmatrix}$$ $$ \begin{pmatrix} -1 & 0 & 1 \\  3 & 4 & -3 \\ -2 & -4 & 2 \\ \end{pmatrix} \cdot  \begin{pmatrix} 4 & -1 & -3 \\  0 & -2 & 2 \\ 3 & -4 & 1 \\ \end{pmatrix} =  \begin{pmatrix} -1 & -3 & 4 \\  3 & 1 & -4 \\ -2 & 2 & 0 \\ \end{pmatrix}$$ $$ \begin{pmatrix} 4 & -3 & -4 \\  -1 & 1 & 0 \\ -2 & 2 & 3 \\ \end{pmatrix} \cdot  \begin{pmatrix} -1 & 4 & -2 \\  2 & 0 & -4 \\ -3 & 3 & 1 \\ \end{pmatrix} =  \begin{pmatrix} 2 & 4 & 0 \\  3 & -4 & -2 \\ -3 & 1 & -1 \\ \end{pmatrix}$$ Is there a way to solve ""Factor $A =\begin{pmatrix} -4 & 0 & -3 \\  3 & 4 & -1 \\ 1 & 2 & -2 \\ \end{pmatrix}$ into permutations of $A$."" ? Can examples be made with larger matrices?","Here's an example of $A . B = scalar \times C$, done with magic squares. The last square does not have a consecutive range of digits. Drop the magic square requirement.  In $2\times2$ matrices we have the following, where a matrix times a scalar is factored into permutations of the original matrix. All entries of the matrix are distinct. $$(\begin{pmatrix} -1 & 0 \\  2 & 1  \\  \end{pmatrix} + a)\cdot (\begin{pmatrix} 1 & 0 \\  -1 & 2  \\ \end{pmatrix}+a) = (2 a + 1)\times(\begin{pmatrix} -1 & 0 \\  1 & 2  \\  \end{pmatrix}+a)$$ Here are examples with $3 \times 3$ matrices.  Can these be canonicalized in some way and all solutions listed? $$ \begin{pmatrix} 3 & 2 & 4 \\  1 & -3 & -1 \\ -4 & 0 & -2 \\ \end{pmatrix} \cdot  \begin{pmatrix} -3 & -1 & 2 \\  -2 & 0 & 3 \\ 4 & 1 & -4 \\ \end{pmatrix} =  \begin{pmatrix} 3 & 1 & -4 \\  -1 & -2 & -3 \\ 4 & 2 & 0 \\ \end{pmatrix}$$ $$ \begin{pmatrix} 4 & -2 & 1 \\  -3 & 2 & -1 \\ -4 & 3 & 0 \\ \end{pmatrix} \cdot  \begin{pmatrix} 1 & -1 & -2 \\  2 & 0 & -4 \\ 3 & 4 & -3 \\ \end{pmatrix} =  \begin{pmatrix} 3 & 0 & -3 \\  -2 & -1 & 1 \\ 2 & 4 & -4 \\ \end{pmatrix}$$ $$ \begin{pmatrix} -1 & 0 & 1 \\  3 & 4 & -3 \\ -2 & -4 & 2 \\ \end{pmatrix} \cdot  \begin{pmatrix} 4 & -1 & -3 \\  0 & -2 & 2 \\ 3 & -4 & 1 \\ \end{pmatrix} =  \begin{pmatrix} -1 & -3 & 4 \\  3 & 1 & -4 \\ -2 & 2 & 0 \\ \end{pmatrix}$$ $$ \begin{pmatrix} 4 & -3 & -4 \\  -1 & 1 & 0 \\ -2 & 2 & 3 \\ \end{pmatrix} \cdot  \begin{pmatrix} -1 & 4 & -2 \\  2 & 0 & -4 \\ -3 & 3 & 1 \\ \end{pmatrix} =  \begin{pmatrix} 2 & 4 & 0 \\  3 & -4 & -2 \\ -3 & 1 & -1 \\ \end{pmatrix}$$ Is there a way to solve ""Factor $A =\begin{pmatrix} -4 & 0 & -3 \\  3 & 4 & -1 \\ 1 & 2 & -2 \\ \end{pmatrix}$ into permutations of $A$."" ? Can examples be made with larger matrices?",,"['matrices', 'recreational-mathematics']"
78,Rank of a matrix over a principal ideal domain,Rank of a matrix over a principal ideal domain,,"I apologize if my question is stupid but I'm not very familiar with matrices over a principal ideal domain $R$ (For example, $R=\mathbb{Z}$ or $R=\mathbb{R}[X]$). I was wondering how to define the rank of a matrix $M \in \mathcal{M}_{n}(R)$. If $R$ is a field, then the rank of $M$ is not a problem for me. It is defined as the dimension (over $R$) of the linear space spanned by the columns of $M$. So, the rank of $M$ is the maximum number of linearly independent columns of $M$. Now, if $R$ is a principal ideal domain, I have found a similar definition on Internet. But what is, in that case, the meaning of $\left\langle C_{1},\ldots,C_{n} \right\rangle$ (the linear space spanned by the columns $C_{1},\ldots,C_{n}$ of $C$) ? Of which vector space is it a subspace ? I feel more comfortable by defining the rank of the matrix $M \in \mathcal{M}_{n}(R)$ as the smallest nonnegative integer $s$ such that there exist a $n \times s$ matrix $P$ (with coefficients in $R$) and a $s \times n$ matrix $Q$ (with coefficients in $R$) such that $M=PQ$. What would be a good definition? Can you please enlighten me about the definition with the dimension?","I apologize if my question is stupid but I'm not very familiar with matrices over a principal ideal domain $R$ (For example, $R=\mathbb{Z}$ or $R=\mathbb{R}[X]$). I was wondering how to define the rank of a matrix $M \in \mathcal{M}_{n}(R)$. If $R$ is a field, then the rank of $M$ is not a problem for me. It is defined as the dimension (over $R$) of the linear space spanned by the columns of $M$. So, the rank of $M$ is the maximum number of linearly independent columns of $M$. Now, if $R$ is a principal ideal domain, I have found a similar definition on Internet. But what is, in that case, the meaning of $\left\langle C_{1},\ldots,C_{n} \right\rangle$ (the linear space spanned by the columns $C_{1},\ldots,C_{n}$ of $C$) ? Of which vector space is it a subspace ? I feel more comfortable by defining the rank of the matrix $M \in \mathcal{M}_{n}(R)$ as the smallest nonnegative integer $s$ such that there exist a $n \times s$ matrix $P$ (with coefficients in $R$) and a $s \times n$ matrix $Q$ (with coefficients in $R$) such that $M=PQ$. What would be a good definition? Can you please enlighten me about the definition with the dimension?",,"['matrices', 'ring-theory', 'matrix-rank', 'principal-ideal-domains', 'dimension-theory-algebra']"
79,Homomorphisms from $\mathbb{C}$ to $M_2(\mathbb{R})$ are conjugate,Homomorphisms from  to  are conjugate,\mathbb{C} M_2(\mathbb{R}),"Let $\phi_1$ and $\phi_2$ be two ring homomorphisms from $\mathbb{C}$ to $M_2(\mathbb{R})$. Show that there exists $g\in GL_2(\mathbb{R})$ such that $\phi_2(x) = g\phi_1(x)g^{-1}$ for all $x\in\mathbb{C}$. $\phi(1) = I$ by definition of ring homomorphism.  By additivity, $\phi(-1) = -I$.  $\phi(i)^2 = \phi(i^2) = \phi(-1) = -I$ and hence the minimal polynomial of $\phi(i)$ in field $\mathbb{R}$ is $x^2+1$.  If we consider the rational forms of $\phi(i)$, then we see that there exists $g\in GL_2(\mathbb{R})$ such that $\phi_2(i) = g\phi_1(i)g^{-1}$. If we can show that $\phi(r) = rI$ for all $r\in\mathbb{R}$, then we are done because $1$ and $i$ are a basis for $\mathbb{C}$ over $\mathbb{R}$.  It's easy to see that $\phi(q) = q$ for all $q\in\mathbb{Q}$. I want to use the fact that $\mathbb{Q}$ is dense in $\mathbb{R}$, but I'm stuck here. Any suggestion? Thanks.","Let $\phi_1$ and $\phi_2$ be two ring homomorphisms from $\mathbb{C}$ to $M_2(\mathbb{R})$. Show that there exists $g\in GL_2(\mathbb{R})$ such that $\phi_2(x) = g\phi_1(x)g^{-1}$ for all $x\in\mathbb{C}$. $\phi(1) = I$ by definition of ring homomorphism.  By additivity, $\phi(-1) = -I$.  $\phi(i)^2 = \phi(i^2) = \phi(-1) = -I$ and hence the minimal polynomial of $\phi(i)$ in field $\mathbb{R}$ is $x^2+1$.  If we consider the rational forms of $\phi(i)$, then we see that there exists $g\in GL_2(\mathbb{R})$ such that $\phi_2(i) = g\phi_1(i)g^{-1}$. If we can show that $\phi(r) = rI$ for all $r\in\mathbb{R}$, then we are done because $1$ and $i$ are a basis for $\mathbb{C}$ over $\mathbb{R}$.  It's easy to see that $\phi(q) = q$ for all $q\in\mathbb{Q}$. I want to use the fact that $\mathbb{Q}$ is dense in $\mathbb{R}$, but I'm stuck here. Any suggestion? Thanks.",,"['abstract-algebra', 'matrices']"
80,What is the Laplacian Matrix used for?,What is the Laplacian Matrix used for?,,"You can turn graphs into several matrix forms depending on what data you want to focus on.  Does the Laplacian form have any uses on its own, or does it need to be paired with other things as some intermediary to be of use? just wondering.","You can turn graphs into several matrix forms depending on what data you want to focus on.  Does the Laplacian form have any uses on its own, or does it need to be paired with other things as some intermediary to be of use? just wondering.",,"['matrices', 'graph-theory', 'graph-laplacian']"
81,Help with Autonne-Takagi factorization of a complex symmetric matrix.,Help with Autonne-Takagi factorization of a complex symmetric matrix.,,"Let $A=A_1i+A_2$ with $A$ non singular. Now let $$B =\begin{bmatrix} A_1 & A_2\\  A_2 & -A_1  \end{bmatrix}$$ With $A_1$ , $A_2$ and $B$ symmetric. Is it true that: 1) $B$ is non singular 2) $B \begin{bmatrix} x \\  -y  \end{bmatrix}=\lambda \begin{bmatrix} x \\  -y  \end{bmatrix}$ if and only if $B \begin{bmatrix} x \\  -y  \end{bmatrix}=-\lambda \begin{bmatrix} x \\  -y  \end{bmatrix}$ so the eigenvalues of $B$ appear in $+-$ pairs. 3) Let $\begin{bmatrix} x_1 \\  -y_1  \end{bmatrix},\dots, \begin{bmatrix} x_n \\  -y_n  \end{bmatrix}$ be the orthonormal eigenvectors of $B$ associated with its positive eigenvalues $\lambda_1,\dots,\lambda_n$ . Let $X=\begin{bmatrix} x_1 & \dots & x_n  \end{bmatrix}$ , $Y=\begin{bmatrix} y_1 & \dots & y_n  \end{bmatrix}$ , $\Sigma=diag(\lambda_1,\dots,\lambda_n)$ , $V =\begin{bmatrix} X & Y\\  -Y & X  \end{bmatrix}$ and $\Lambda = \Sigma \oplus (-\Sigma)$ . Then $V$ is real orthogonal and $B = V\Lambda V^T$ . Let $U = X - iY$ . Explain why $U$ is unitary and show that $U\Sigma U ^T=A$ .","Let with non singular. Now let With , and symmetric. Is it true that: 1) is non singular 2) if and only if so the eigenvalues of appear in pairs. 3) Let be the orthonormal eigenvectors of associated with its positive eigenvalues . Let , , , and . Then is real orthogonal and . Let . Explain why is unitary and show that .","A=A_1i+A_2 A B =\begin{bmatrix}
A_1 & A_2\\ 
A_2 & -A_1 
\end{bmatrix} A_1 A_2 B B B \begin{bmatrix}
x \\ 
-y 
\end{bmatrix}=\lambda \begin{bmatrix}
x \\ 
-y 
\end{bmatrix} B \begin{bmatrix}
x \\ 
-y 
\end{bmatrix}=-\lambda \begin{bmatrix}
x \\ 
-y 
\end{bmatrix} B +- \begin{bmatrix}
x_1 \\ 
-y_1 
\end{bmatrix},\dots, \begin{bmatrix}
x_n \\ 
-y_n 
\end{bmatrix} B \lambda_1,\dots,\lambda_n X=\begin{bmatrix}
x_1 & \dots & x_n 
\end{bmatrix} Y=\begin{bmatrix}
y_1 & \dots & y_n 
\end{bmatrix} \Sigma=diag(\lambda_1,\dots,\lambda_n) V =\begin{bmatrix}
X & Y\\ 
-Y & X 
\end{bmatrix} \Lambda = \Sigma \oplus (-\Sigma) V B = V\Lambda V^T U = X - iY U U\Sigma U ^T=A","['matrices', 'factoring', 'symmetry']"
82,Embedding $\mathbb{G}_a$ into $GL_2$,Embedding  into,\mathbb{G}_a GL_2,"Let $k$ be an algebraically closed field of characteristic $p$.  I'd like to find interesting examples of closed embeddings $\mathbb{G}_a(k)\hookrightarrow GL_2(k)$, where $\mathbb{G}_a(k)$ is $(k,+)$.  For any non-zero $a\in k$, we have the two standard embeddings: $$c\longmapsto\begin{pmatrix}1&ac\\0&1\end{pmatrix}\qquad\qquad c\longmapsto\begin{pmatrix}1&0\\ac&1\end{pmatrix}$$ Also, for $p=2$ we have the embedding $$c\longmapsto\begin{pmatrix}1+ac&ac\\ac&1+ac\end{pmatrix}$$ What are other examples of such embeddings, either in arbitrary positive characteristic, or in a specific positive characteristic?  If you prefer to think in terms of coordinate algebras, this problem is equivalent to finding surjective Hopf algebra maps from $k[GL_2]$ to $k[\mathbb{G}_a]$.  Thanks in advance. Edit: Thanks to the comments, we have for any $a,b\in k$, not both $0$, an embedding given by $$c\longmapsto\begin{pmatrix}1-abc&a^2c\\-b^2c&1+abc\end{pmatrix}$$ This example genralizes the previous $3$.  Do all embeddings have this form?","Let $k$ be an algebraically closed field of characteristic $p$.  I'd like to find interesting examples of closed embeddings $\mathbb{G}_a(k)\hookrightarrow GL_2(k)$, where $\mathbb{G}_a(k)$ is $(k,+)$.  For any non-zero $a\in k$, we have the two standard embeddings: $$c\longmapsto\begin{pmatrix}1&ac\\0&1\end{pmatrix}\qquad\qquad c\longmapsto\begin{pmatrix}1&0\\ac&1\end{pmatrix}$$ Also, for $p=2$ we have the embedding $$c\longmapsto\begin{pmatrix}1+ac&ac\\ac&1+ac\end{pmatrix}$$ What are other examples of such embeddings, either in arbitrary positive characteristic, or in a specific positive characteristic?  If you prefer to think in terms of coordinate algebras, this problem is equivalent to finding surjective Hopf algebra maps from $k[GL_2]$ to $k[\mathbb{G}_a]$.  Thanks in advance. Edit: Thanks to the comments, we have for any $a,b\in k$, not both $0$, an embedding given by $$c\longmapsto\begin{pmatrix}1-abc&a^2c\\-b^2c&1+abc\end{pmatrix}$$ This example genralizes the previous $3$.  Do all embeddings have this form?",,"['abstract-algebra', 'group-theory', 'matrices', 'algebraic-groups', 'hopf-algebras']"
83,How to project a symmetric matrix onto the cone of positive semidefinite (PSD) matrices?,How to project a symmetric matrix onto the cone of positive semidefinite (PSD) matrices?,,How would you project a symmetric real matrix onto the cone of all positive semidefinite matrices?,How would you project a symmetric real matrix onto the cone of all positive semidefinite matrices?,,"['matrices', 'convex-optimization', 'projection', 'positive-semidefinite', 'dual-cone']"
84,Polynomials in matrices with integer entries,Polynomials in matrices with integer entries,,"I'm looking for references, if there is any, for this problem: Characterize all elements $a \in M_n(\mathbb{Z})$ for which we have $\mathbb{Q}[a] \cap M_n(\mathbb{Z})=\mathbb{Z}[a].$ Here, by $C[a]$ I mean the ring of polynomials in $a$ with coefficients in a ring $C.$ Thank you","I'm looking for references, if there is any, for this problem: Characterize all elements $a \in M_n(\mathbb{Z})$ for which we have $\mathbb{Q}[a] \cap M_n(\mathbb{Z})=\mathbb{Z}[a].$ Here, by $C[a]$ I mean the ring of polynomials in $a$ with coefficients in a ring $C.$ Thank you",,"['matrices', 'reference-request', 'ring-theory']"
85,"Is there a ""natural"" transitive action of $SL_2(\mathbb{F}_5)$ on a set with 5 elements?","Is there a ""natural"" transitive action of  on a set with 5 elements?",SL_2(\mathbb{F}_5),"I'm really looking for a ""cute"" way of showing that $SL_2(\mathbb{F}_5)$ is a double cover of $A_5$.  The sort of action I am looking for is something like the action of $GL_2(\mathbb{F}_3)$ on $\mathbb{P}^1(\mathbb{F}_3)$, which shows $GL_2(\mathbb{F}_3)$ is a double cover of $S_4$.  Now that's cute.","I'm really looking for a ""cute"" way of showing that $SL_2(\mathbb{F}_5)$ is a double cover of $A_5$.  The sort of action I am looking for is something like the action of $GL_2(\mathbb{F}_3)$ on $\mathbb{P}^1(\mathbb{F}_3)$, which shows $GL_2(\mathbb{F}_3)$ is a double cover of $S_4$.  Now that's cute.",,"['group-theory', 'matrices', 'finite-groups', 'permutations']"
86,FLOSS tool to visualize 2- and 3-space matrix transformations,FLOSS tool to visualize 2- and 3-space matrix transformations,,"I'm looking for a FLOSS application (Windows or Ubuntu but preferably both) that can help me visualize matrix transformations in 2- and 3-space. So I'd like to be able to enter a vector or matrix, see it in 2-space or 3-space, enter a transformation vector or matrix, and see the result. For example, enter a 3x3 matrix, see the parallelepiped it represents, enter a rotation matrix, see the rotated parallelepiped. Bonus points for ability to calculate area/volume, animate, change colours, visualize higher dimensions, wash my socks.","I'm looking for a FLOSS application (Windows or Ubuntu but preferably both) that can help me visualize matrix transformations in 2- and 3-space. So I'd like to be able to enter a vector or matrix, see it in 2-space or 3-space, enter a transformation vector or matrix, and see the result. For example, enter a 3x3 matrix, see the parallelepiped it represents, enter a rotation matrix, see the rotated parallelepiped. Bonus points for ability to calculate area/volume, animate, change colours, visualize higher dimensions, wash my socks.",,"['matrices', 'math-software', 'visualization']"
87,Finding SVD efficiently for $AB^T$,Finding SVD efficiently for,AB^T,"I posted this on cs theory yesterday but did not get an answer and hence I am posting here. I have a low rank matrix given as $AB^T$ where $A,B \in \mathbb{R}^{n \times p}$ and $p \ll n$. (I know $A$ and $B$ separately) One efficient way to get the Singular Value Decomposition is to do a reduced QR on $A$ and $B$ i.e. $A = Q_A R_A$ and $B = Q_B R_B$. The above can be done in $\mathcal{O}(p^2n)$ cost. I could then compute the svd of $R_A R_B^T = U_1 \Sigma V_1^T$ which is $\mathcal{O}(p^3)$. Hence this will give me $$AB^T = (Q_A U_1) \Sigma (Q_B V_1)^T$$ The total cost is $\mathcal{O}(np^2)$. However, I am wondering if there are other efficient ways to go about doing this to reduce the coefficient infront of $p^2n$. If I am right, the coefficient infront of $np^2$ is $3$ if we go about doing QR. The reason why I am interested in minimizing the coefficient is that my $n$ is really ginormous. So if I were to implement it, a cost cutting on the coefficient of the $\mathcal{O}(n)$ could be significant. I am wondering if we could exploit the fact that the left singular vectors must span $A$ and the right singular vectors span $B$ i.e. we know that if $AB^T = U \Sigma V^T$, then $$A = U \alpha \text{ and } B = V \beta$$ and we want $\alpha \beta^T$ to be diagonal and $U$,$V$ needs to be unitary. EDIT What I essentially want is an exact rank $r<p$ approximation to the product $AB^T$ in the form $U_r V_r^T$. I am not looking for approximate or probabilistic algorithms.","I posted this on cs theory yesterday but did not get an answer and hence I am posting here. I have a low rank matrix given as $AB^T$ where $A,B \in \mathbb{R}^{n \times p}$ and $p \ll n$. (I know $A$ and $B$ separately) One efficient way to get the Singular Value Decomposition is to do a reduced QR on $A$ and $B$ i.e. $A = Q_A R_A$ and $B = Q_B R_B$. The above can be done in $\mathcal{O}(p^2n)$ cost. I could then compute the svd of $R_A R_B^T = U_1 \Sigma V_1^T$ which is $\mathcal{O}(p^3)$. Hence this will give me $$AB^T = (Q_A U_1) \Sigma (Q_B V_1)^T$$ The total cost is $\mathcal{O}(np^2)$. However, I am wondering if there are other efficient ways to go about doing this to reduce the coefficient infront of $p^2n$. If I am right, the coefficient infront of $np^2$ is $3$ if we go about doing QR. The reason why I am interested in minimizing the coefficient is that my $n$ is really ginormous. So if I were to implement it, a cost cutting on the coefficient of the $\mathcal{O}(n)$ could be significant. I am wondering if we could exploit the fact that the left singular vectors must span $A$ and the right singular vectors span $B$ i.e. we know that if $AB^T = U \Sigma V^T$, then $$A = U \alpha \text{ and } B = V \beta$$ and we want $\alpha \beta^T$ to be diagonal and $U$,$V$ needs to be unitary. EDIT What I essentially want is an exact rank $r<p$ approximation to the product $AB^T$ in the form $U_r V_r^T$. I am not looking for approximate or probabilistic algorithms.",,['matrices']
88,When does A and exp(B) commuting imply A commutes with B?,When does A and exp(B) commuting imply A commutes with B?,,"Let $A,B$ $\in GL_{n}(\mathbb{C})$ and $[A,B] = AB-BA = 0$ . My question is about the existence of a $b \in M_{n}(\mathbb{C})$ such that $B = \exp(b) $ and $[A,b] =0 $ . Note that in general $[A,\exp(b)]=0$ does not imply $[A,b]=0$ . As an example consider the matrices $A = \begin{bmatrix}1&i\\i&2\end{bmatrix}$ $B= I$ and $b = \begin{bmatrix}2 \pi i&0\\0&0\end{bmatrix} $ . We have $[A, \exp(b)] = 0$ but $[A,b] \neq 0$ . However, we can replace $b$ with $\widetilde{b}=0$ . I believe in general this could have something to do with the locus in $M_{n}(\mathbb{C})$ where $\exp$ is a local isomorphism. This is the locus of matrices $X$ which have $\lambda_{i} - \lambda_{j} \neq 2 \pi i k $ for $k \in \mathbb{Z} - 0 $ for any two eigenvalues $\lambda_{i} , \lambda_{j}$ of $X$ . Note that $\exp$ is still surjective when restricted to this locus. Namely, perhaps if we restrict to preimages of $B$ under the exponential in this locus then perhaps the matrices commute. In particular, in the example above $b$ is not in this locus.","Let and . My question is about the existence of a such that and . Note that in general does not imply . As an example consider the matrices and . We have but . However, we can replace with . I believe in general this could have something to do with the locus in where is a local isomorphism. This is the locus of matrices which have for for any two eigenvalues of . Note that is still surjective when restricted to this locus. Namely, perhaps if we restrict to preimages of under the exponential in this locus then perhaps the matrices commute. In particular, in the example above is not in this locus.","A,B \in GL_{n}(\mathbb{C}) [A,B] = AB-BA = 0 b \in M_{n}(\mathbb{C}) B = \exp(b)  [A,b] =0  [A,\exp(b)]=0 [A,b]=0 A = \begin{bmatrix}1&i\\i&2\end{bmatrix} B= I b = \begin{bmatrix}2 \pi i&0\\0&0\end{bmatrix}  [A, \exp(b)] = 0 [A,b] \neq 0 b \widetilde{b}=0 M_{n}(\mathbb{C}) \exp X \lambda_{i} - \lambda_{j} \neq 2 \pi i k  k \in \mathbb{Z} - 0  \lambda_{i} , \lambda_{j} X \exp B b","['matrices', 'lie-groups', 'matrix-exponential']"
89,Group Structure of the $2\times2$ Matrix Sphere,Group Structure of the  Matrix Sphere,2\times2,"I define the $2\times2$ matrix sphere as the quotient set $$\boxed{\mathbb{S}_{2}:=\{(X,Y)\in M_2(\mathbb{R})^2:X^2+Y^2=I\}/\sim}$$ where $M_2(\mathbb{R})^2=\mathbb{R}^{2\times2}\times\mathbb{R}^{2\times2}$ is the set of pairs of $2\times2$ real matrices and $\sim$ is the equivalence relation $$(A,B)\sim(C,D)\iff\exists P\in GL_2(\mathbb{R}):(C,D)=(PAP^{-1},PBP^{-1})$$ The motivation comes from generalizing the $1$ -sphere $S^1=\{(x,y)\in\mathbb{R}^2:x^2+y^2=1\}$ to $2\times2$ matrices identifying those pairs that are essentially the same since every matrix solution $(A,B)$ of $X^2+Y^2=I$ leads to an infinite amount of solutions as $$\forall P\in GL_2(\mathbb{R}):(PAP^{-1})^2+(PBP^{-1})^2 = P^2(A^2+B^2)P^{-2}=P^2IP^{-2}=I$$ Q: My question then is, analogous to the group structure of $(S^1,\cdot)$ considering it as a subgroup of $\mathbb{C}^\times$ , is there any natural operation $\odot:\mathbb{S}_{2}\times\mathbb{S}_2\to\mathbb{S}_2$ that $(\mathbb{S}_2,\odot)$ is a group? The thing with $\mathbb{S}_2$ is that it is not abelian. So, if we considered the operation $$\odot:[M_2(\mathbb{R})^2/\sim]\times[M_2(\mathbb{R})^2/\sim]\to M_2(\mathbb{R})^2/\sim$$ $$(A,B)\odot(C,D):=(AC-BD,AD+BC)$$ which is an intuitive generalization of the complex product, $\mathbb{S}_2$ is not closed under $\odot$ so $(\mathbb{S}_2,\odot)$ is not a group. In fact, if we define the norm $N:M_2(\mathbb{R})^2\to M_2(\mathbb{R})$ as $N(A,B)=A^2+B^2$ , then $\forall (A,B),(C,D)\in\mathbb{S}_2$ $$N((A,B)\odot(C,D))=N(A,B)N(C,D)\iff ACBD+BDAC=ADBC+BCAD$$ So $N$ is only multiplicative given that condition. I don't know if this helps but it is easy to see that the orthogonal group $\mathcal{O}_2=\{Q\in\mathbb{R}^{2\times2}:Q^TQ=I\}$ acts on $\mathbb{S}_2$ as $$\begin{pmatrix}\alpha& \beta\\ \gamma& \delta\end{pmatrix}(A,B)=(\alpha A+\beta B,\gamma A+\delta B)$$","I define the matrix sphere as the quotient set where is the set of pairs of real matrices and is the equivalence relation The motivation comes from generalizing the -sphere to matrices identifying those pairs that are essentially the same since every matrix solution of leads to an infinite amount of solutions as Q: My question then is, analogous to the group structure of considering it as a subgroup of , is there any natural operation that is a group? The thing with is that it is not abelian. So, if we considered the operation which is an intuitive generalization of the complex product, is not closed under so is not a group. In fact, if we define the norm as , then So is only multiplicative given that condition. I don't know if this helps but it is easy to see that the orthogonal group acts on as","2\times2 \boxed{\mathbb{S}_{2}:=\{(X,Y)\in M_2(\mathbb{R})^2:X^2+Y^2=I\}/\sim} M_2(\mathbb{R})^2=\mathbb{R}^{2\times2}\times\mathbb{R}^{2\times2} 2\times2 \sim (A,B)\sim(C,D)\iff\exists P\in GL_2(\mathbb{R}):(C,D)=(PAP^{-1},PBP^{-1}) 1 S^1=\{(x,y)\in\mathbb{R}^2:x^2+y^2=1\} 2\times2 (A,B) X^2+Y^2=I \forall P\in GL_2(\mathbb{R}):(PAP^{-1})^2+(PBP^{-1})^2 = P^2(A^2+B^2)P^{-2}=P^2IP^{-2}=I (S^1,\cdot) \mathbb{C}^\times \odot:\mathbb{S}_{2}\times\mathbb{S}_2\to\mathbb{S}_2 (\mathbb{S}_2,\odot) \mathbb{S}_2 \odot:[M_2(\mathbb{R})^2/\sim]\times[M_2(\mathbb{R})^2/\sim]\to M_2(\mathbb{R})^2/\sim (A,B)\odot(C,D):=(AC-BD,AD+BC) \mathbb{S}_2 \odot (\mathbb{S}_2,\odot) N:M_2(\mathbb{R})^2\to M_2(\mathbb{R}) N(A,B)=A^2+B^2 \forall (A,B),(C,D)\in\mathbb{S}_2 N((A,B)\odot(C,D))=N(A,B)N(C,D)\iff ACBD+BDAC=ADBC+BCAD N \mathcal{O}_2=\{Q\in\mathbb{R}^{2\times2}:Q^TQ=I\} \mathbb{S}_2 \begin{pmatrix}\alpha& \beta\\ \gamma& \delta\end{pmatrix}(A,B)=(\alpha A+\beta B,\gamma A+\delta B)","['matrices', 'group-theory', 'matrix-equations']"
90,Diagonalization of specific symmetric tridiagonal matrix,Diagonalization of specific symmetric tridiagonal matrix,,"I am wondering if there is a way to get an explicit expression for the eigenvalues (and possibly the eigenvectors) of a symmetric tridiagonal matrix with the following peculiar structure: $$ \left( \begin{array}{ccccc} 0 & k\sqrt{1} & 0 & 0 & \dots \\ k\sqrt{1} & 1 & k\sqrt{2} & 0 & \dots \\ 0 & k\sqrt{2} & 2 & k\sqrt{3} & \dots \\ 0 & 0 & k\sqrt{3} & 3 & \dots \\ \vdots & \vdots & \vdots & \vdots & \ddots \end{array} \right) $$ in the limit where the size of the matrix goes to infinity. The context of this problem is quantum physics, and by some arguments that are shown here , the answer should be something like $\lambda_n = n - k^2$ . I know that the characteristic polinomial can be written recursively as $\lim_{n\to\infty}p_n(\lambda)$ , where $$ p_n(\lambda)=a_n(\lambda) p_{n-1}(\lambda) - b_{n-1}^2 p_{n-2}(\lambda), \;\;\;\;\; p_{0}(\lambda) = 1 \;\;\;\;\; p_{-1}(\lambda) = 0, $$ $a_n(\lambda)$ being the diagonal term, so $a_n=n-1-\lambda$ and $b_n$ being the off diagonal term, thus $b_n=k\sqrt{n}$ , but I am stuck here. Can anyone help me with this? Thanks!","I am wondering if there is a way to get an explicit expression for the eigenvalues (and possibly the eigenvectors) of a symmetric tridiagonal matrix with the following peculiar structure: in the limit where the size of the matrix goes to infinity. The context of this problem is quantum physics, and by some arguments that are shown here , the answer should be something like . I know that the characteristic polinomial can be written recursively as , where being the diagonal term, so and being the off diagonal term, thus , but I am stuck here. Can anyone help me with this? Thanks!","
\left(
\begin{array}{ccccc}
0 & k\sqrt{1} & 0 & 0 & \dots \\
k\sqrt{1} & 1 & k\sqrt{2} & 0 & \dots \\
0 & k\sqrt{2} & 2 & k\sqrt{3} & \dots \\
0 & 0 & k\sqrt{3} & 3 & \dots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{array}
\right)
 \lambda_n = n - k^2 \lim_{n\to\infty}p_n(\lambda) 
p_n(\lambda)=a_n(\lambda) p_{n-1}(\lambda) - b_{n-1}^2 p_{n-2}(\lambda),
\;\;\;\;\;
p_{0}(\lambda) = 1
\;\;\;\;\;
p_{-1}(\lambda) = 0,
 a_n(\lambda) a_n=n-1-\lambda b_n b_n=k\sqrt{n}","['matrices', 'eigenvalues-eigenvectors', 'diagonalization', 'tridiagonal-matrices']"
91,Involution on $2\times 2$ matrices,Involution on  matrices,2\times 2,"Show that the map on $2\times 2$ matrices \begin{eqnarray}  \left( \begin{matrix} a & b\\ c & d \end{matrix} \right)\overset{\Phi}{\mapsto} \left( \begin{matrix} a & b\\ c & d \end{matrix} \right)\cdot \left( \begin{matrix} 2 (a c - b^2) & a d - b c\\ a d - b c & 2(b d - c^2) \end{matrix} \right)^{-1} \cdot \left( \begin{matrix} 0 & 1\\ 1 & 0 \end{matrix} \right) \end{eqnarray} is an involution, that is $\Phi\circ \Phi=\operatorname{Id}$ Notes: For convenience, one could treat $a$ , $b$ , $c$ , $d$ as independent variables, with coefficients in $\mathbb{C}$ . If we consider the form $F(x,y) =a x^3 + 3 b x^2 y + 3 c x y^2 + d$ , then the Hessian determinant is a quadratic form with matrix coefficients (up to a constant( $\left( \begin{matrix} 2 (a c - b^2) & a d - b c\\ a d - b c & 2(b d - c^2) \end{matrix} \right)$ . Its determinant is up to a constant the discriminant of the cubic form $F$ . See also the matrix $\left(\begin{matrix} a& b &c \\ b&c&d\end{matrix}\right)$ and its $2\times 2$ minors. If instead of the inverse on the second factor we consider the adjugate, then we don't have to worry about denominators. Then the composition will be a constant times the matrix. One could consider an alternate map with the third factor on RHS given by $\left( \begin{matrix} 0 & -1\\ 1 & 0 \end{matrix} \right)$ . This is connected to the covariant $t$ for cubics, and  an involution on triplets ( the roots of the cubic).  Note that the square of the map is in this case $-\operatorname{Id}$ . One can show that we have an involution by direct calculation ( RHS could be multiplied by an arbitrary fixed constant). It would be nice to have a proof involving some shortcuts. Maybe everything is just something basic. Any feedback would be appreciated!","Show that the map on matrices is an involution, that is Notes: For convenience, one could treat , , , as independent variables, with coefficients in . If we consider the form , then the Hessian determinant is a quadratic form with matrix coefficients (up to a constant( . Its determinant is up to a constant the discriminant of the cubic form . See also the matrix and its minors. If instead of the inverse on the second factor we consider the adjugate, then we don't have to worry about denominators. Then the composition will be a constant times the matrix. One could consider an alternate map with the third factor on RHS given by . This is connected to the covariant for cubics, and  an involution on triplets ( the roots of the cubic).  Note that the square of the map is in this case . One can show that we have an involution by direct calculation ( RHS could be multiplied by an arbitrary fixed constant). It would be nice to have a proof involving some shortcuts. Maybe everything is just something basic. Any feedback would be appreciated!","2\times 2 \begin{eqnarray} 
\left( \begin{matrix} a & b\\ c & d \end{matrix} \right)\overset{\Phi}{\mapsto} \left( \begin{matrix} a & b\\ c & d \end{matrix} \right)\cdot \left( \begin{matrix} 2 (a c - b^2) & a d - b c\\ a d - b c & 2(b d - c^2) \end{matrix} \right)^{-1} \cdot \left( \begin{matrix} 0 & 1\\ 1 & 0 \end{matrix} \right)
\end{eqnarray} \Phi\circ \Phi=\operatorname{Id} a b c d \mathbb{C} F(x,y) =a x^3 + 3 b x^2 y + 3 c x y^2 + d \left( \begin{matrix} 2 (a c - b^2) & a d - b c\\ a d - b c & 2(b d - c^2) \end{matrix} \right) F \left(\begin{matrix} a& b &c \\
b&c&d\end{matrix}\right) 2\times 2 \left( \begin{matrix} 0 & -1\\ 1 & 0 \end{matrix} \right) t -\operatorname{Id}","['matrices', 'cubics', 'invariant-theory', 'schur-complement', 'roots-of-cubics']"
92,Using Cayley-Hamilton theorem to find the minimal polynomial,Using Cayley-Hamilton theorem to find the minimal polynomial,,"I need to find the minimal polynomial of $\beta = i + \sqrt[3]{2}$ over $\mathbb{Q}$ using Cayley-Hamilton theorem. So far I've found that $\{1, \sqrt[3]{2}, \sqrt[3]{4}, i, \sqrt[3]{2}i, \sqrt[3]{4}i\}$ is a basis of $\mathbb{Q}[\sqrt[3]{2},i]$ as a vector space over $\mathbb{Q}$ . Also, the matrix representing the multiplication by $\beta$ using this basis is $$           A =            \begin{pmatrix}             0& 0& 2&-1& 0& 0\\             1& 0& 0& 0&-1& 0\\             0& 1& 0& 0& 0&-1\\             1& 0& 0& 0& 0& 2\\             0& 1& 0& 1& 0& 0\\             0& 0& 1& 0& 1& 0\\           \end{pmatrix} $$ where the element $$\lambda_{1} + \lambda_{2}\sqrt[3]{2} + \lambda_{3}\sqrt[3]{4} + \lambda_{4}i + \lambda_{5}\sqrt[3]{2}i + \lambda_{6}\sqrt[3]{4}i$$ in $\mathbb{Q}[\sqrt[3]{2},i]$ is represented by the vector $$           \begin{pmatrix}             \lambda_{1}\\             \lambda_{2}\\             \lambda_{3}\\             \lambda_{4}\\             \lambda_{5}\\             \lambda_{6}\\           \end{pmatrix}. $$ I could't figure out how to use the characteristic polynomial of $A$ ( $p(x) = x^{6} + 3x^{4} - 4x^{3} + 3x^{2}$ ) to find a polynomial $f(x)$ in $\mathbb{Q}[x]$ such that $f(\beta) = 0$ .","I need to find the minimal polynomial of over using Cayley-Hamilton theorem. So far I've found that is a basis of as a vector space over . Also, the matrix representing the multiplication by using this basis is where the element in is represented by the vector I could't figure out how to use the characteristic polynomial of ( ) to find a polynomial in such that .","\beta = i + \sqrt[3]{2} \mathbb{Q} \{1, \sqrt[3]{2}, \sqrt[3]{4}, i, \sqrt[3]{2}i, \sqrt[3]{4}i\} \mathbb{Q}[\sqrt[3]{2},i] \mathbb{Q} \beta 
          A = 
          \begin{pmatrix}
            0& 0& 2&-1& 0& 0\\
            1& 0& 0& 0&-1& 0\\
            0& 1& 0& 0& 0&-1\\
            1& 0& 0& 0& 0& 2\\
            0& 1& 0& 1& 0& 0\\
            0& 0& 1& 0& 1& 0\\
          \end{pmatrix}
 \lambda_{1} + \lambda_{2}\sqrt[3]{2} + \lambda_{3}\sqrt[3]{4} + \lambda_{4}i + \lambda_{5}\sqrt[3]{2}i + \lambda_{6}\sqrt[3]{4}i \mathbb{Q}[\sqrt[3]{2},i] 
          \begin{pmatrix}
            \lambda_{1}\\
            \lambda_{2}\\
            \lambda_{3}\\
            \lambda_{4}\\
            \lambda_{5}\\
            \lambda_{6}\\
          \end{pmatrix}.
 A p(x) = x^{6} + 3x^{4} - 4x^{3} + 3x^{2} f(x) \mathbb{Q}[x] f(\beta) = 0","['abstract-algebra', 'matrices']"
93,"Understanding $\,\det(A+B)$",Understanding,"\,\det(A+B)","From this paper on Determinant of sums, where \begin{equation} \det(A+B) = \sum_{r} \sum_{\alpha,\beta} (-1)^{s(\alpha) + s(\beta)} \det(A[\alpha|\beta])\det(B[\alpha|\beta]), \end{equation} the meaning of $A[\alpha|\beta]$ , and how the sum runs over $\alpha, \beta$ is not clear to me. Would appreciate an explanation of this this result.","From this paper on Determinant of sums, where the meaning of , and how the sum runs over is not clear to me. Would appreciate an explanation of this this result.","\begin{equation}
\det(A+B) = \sum_{r} \sum_{\alpha,\beta} (-1)^{s(\alpha) + s(\beta)} \det(A[\alpha|\beta])\det(B[\alpha|\beta]),
\end{equation} A[\alpha|\beta] \alpha, \beta","['matrices', 'notation', 'determinant']"
94,Is there a relationship between the Ricci scalar and the determinant?,Is there a relationship between the Ricci scalar and the determinant?,,"On the one hand the determinant (technically the absolute value of the determinant) represents the ""volume distortion"" experienced by a region after being transformed. On the other hand the scalar curvature represents the amount by which the volume of a small geodesic ball in a Riemannian manifold deviates from that of the standard ball in Euclidean space. Is there a link between the two?  Can the determinant of a matrix be interpreted as a scalar curvature? If I makes a difference, I am mostly interested in the context of general relativity.","On the one hand the determinant (technically the absolute value of the determinant) represents the ""volume distortion"" experienced by a region after being transformed. On the other hand the scalar curvature represents the amount by which the volume of a small geodesic ball in a Riemannian manifold deviates from that of the standard ball in Euclidean space. Is there a link between the two?  Can the determinant of a matrix be interpreted as a scalar curvature? If I makes a difference, I am mostly interested in the context of general relativity.",,"['matrices', 'geometry', 'riemannian-geometry', 'curvature']"
95,A question on irreducible unitary representations of the unitary group $ U(n)$,A question on irreducible unitary representations of the unitary group, U(n),"Let $n$ be a positive integer and $U(n)$ be the group of $n\times n$ unitary matrices. I have two question regarding the irreducible unitary representations of $U(n)$ . Is there any irreducible unitary representation $\pi$ of $U(n)$ such that $1<\dim \pi < n$ , where $\dim \pi$ is the dimension of the corresponding representation space. Let $\pi_1:U(n)\to GL_n(C)$ and $\pi_2:U(n)\to GL_n(C)$ be defined by $$\pi_1(u)=u,~~~\pi_2(u)=\overline u,~u\in U(n).$$ Here $\overline{u}$ is the conjugate matrix of $u$ . Clearly, $\pi_1$ and $\pi_2$ are two irreducible unitary representation  of $U(n)$ where the  dimension of the corresponding representation space is $n$ . Are these all irreducible unitary representation  of $U(n)$ (upto unitary equivalence) where the dimension of the corresponding representation space is $n$ ? I know that there is a Weyl dimension formula for  irreducible unitary representation  of $U(n)$ . But I was not able to solve the above questions using that formula. Also I am very new in this area. Any help or reference will be highly appreciated. Thanks in advance!","Let be a positive integer and be the group of unitary matrices. I have two question regarding the irreducible unitary representations of . Is there any irreducible unitary representation of such that , where is the dimension of the corresponding representation space. Let and be defined by Here is the conjugate matrix of . Clearly, and are two irreducible unitary representation  of where the  dimension of the corresponding representation space is . Are these all irreducible unitary representation  of (upto unitary equivalence) where the dimension of the corresponding representation space is ? I know that there is a Weyl dimension formula for  irreducible unitary representation  of . But I was not able to solve the above questions using that formula. Also I am very new in this area. Any help or reference will be highly appreciated. Thanks in advance!","n U(n) n\times n U(n) \pi U(n) 1<\dim \pi < n \dim \pi \pi_1:U(n)\to GL_n(C) \pi_2:U(n)\to GL_n(C) \pi_1(u)=u,~~~\pi_2(u)=\overline u,~u\in U(n). \overline{u} u \pi_1 \pi_2 U(n) n U(n) n U(n)","['matrices', 'representation-theory', 'unitary-matrices']"
96,Weighted nuclear norm minimization,Weighted nuclear norm minimization,,"Crossposted on Operations Research Stack Exchange The problem. Let $X,A \in\mathbb{R}^{n\times m}$ and let $W\in\mathbb{R}^{nm\times nm}$ be a positive definite matrix. I want to know if there is a closed-form solution to this problem $$ \min_{X} \frac{1}{2}\text{vec}(X-A)^\top W\text{vec}(X-A) + \|X\|_*, $$ where $\|X\|_*$ denotes the nuclear norm of $X$ , and $\text{vec}(X)$ denotes the vectorization operation of the matrix $X$ . A solution in the unweighted case. For $W = I$ (the identity matrix), the problem is known as Singular Value Thresholding (SVT), and corresponds to: $$ \min_{X} \frac{1}{2}\|X-A\|_F^2 + \|X\|_*, $$ where $\|X\|_F$ is the Frobenius norm of $X$ , and a solution is found in this paper (thm. 2.1) , and is nicely expressed as $$X = U\max(0,S-I)V^\top,$$ where $A = USV^\top$ is the SVD of the matrix $A$ . In the weighted case I am not sure whether is possible to express the solution in closed form. This problem is also known as Weighted Singular Value Thresholding (WSVT) and a closed-form solution has not been found yet. Related work. Paper 1 studies the problem of the Weighted Low-Rank Approximation (WLRA) problem which is more difficult than the one proposed in this question (the problem in the question is a convex relaxation of the one in the paper). They have a closed-form solution in the case $W$ can be expressed as $W = W_1 \otimes W_2$ . This was my initial problem, however ... Paper 2 proves that the WLRA problem is NP-hard in general and that is the reason why I moved from WLRA to WSVT (actually my initial problem includes also other constraints, but that is another story). Paper 3 studies the WSVT problem, but they come up with a numerical solution (no analytical solution). Is there really no way one can solve the problem in closed form as for the unweighted SVT problem? Paper 4 studies a related problem (the same as this related question ), where the nuclear norm is weighted by a vector $w$ . In this case, we have an analytical solution. However, I tried to recast my problem in this form with no success. Why bother with a closed form solution? I really need an analytical solution because the original problem is much more complex (includes other convex constraints and another outer optimization over another set of variables). To solve such optimization problem, I believe a closed-form solution for the simpler problem considered in this question would help considerably.","Crossposted on Operations Research Stack Exchange The problem. Let and let be a positive definite matrix. I want to know if there is a closed-form solution to this problem where denotes the nuclear norm of , and denotes the vectorization operation of the matrix . A solution in the unweighted case. For (the identity matrix), the problem is known as Singular Value Thresholding (SVT), and corresponds to: where is the Frobenius norm of , and a solution is found in this paper (thm. 2.1) , and is nicely expressed as where is the SVD of the matrix . In the weighted case I am not sure whether is possible to express the solution in closed form. This problem is also known as Weighted Singular Value Thresholding (WSVT) and a closed-form solution has not been found yet. Related work. Paper 1 studies the problem of the Weighted Low-Rank Approximation (WLRA) problem which is more difficult than the one proposed in this question (the problem in the question is a convex relaxation of the one in the paper). They have a closed-form solution in the case can be expressed as . This was my initial problem, however ... Paper 2 proves that the WLRA problem is NP-hard in general and that is the reason why I moved from WLRA to WSVT (actually my initial problem includes also other constraints, but that is another story). Paper 3 studies the WSVT problem, but they come up with a numerical solution (no analytical solution). Is there really no way one can solve the problem in closed form as for the unweighted SVT problem? Paper 4 studies a related problem (the same as this related question ), where the nuclear norm is weighted by a vector . In this case, we have an analytical solution. However, I tried to recast my problem in this form with no success. Why bother with a closed form solution? I really need an analytical solution because the original problem is much more complex (includes other convex constraints and another outer optimization over another set of variables). To solve such optimization problem, I believe a closed-form solution for the simpler problem considered in this question would help considerably.","X,A \in\mathbb{R}^{n\times m} W\in\mathbb{R}^{nm\times nm} 
\min_{X} \frac{1}{2}\text{vec}(X-A)^\top W\text{vec}(X-A) + \|X\|_*,
 \|X\|_* X \text{vec}(X) X W = I 
\min_{X} \frac{1}{2}\|X-A\|_F^2 + \|X\|_*,
 \|X\|_F X X = U\max(0,S-I)V^\top, A = USV^\top A W W = W_1 \otimes W_2 w","['matrices', 'optimization', 'convex-optimization', 'regularization', 'nuclear-norm']"
97,Finding the order of a 2x2 matrix mod n when n is NOT prime,Finding the order of a 2x2 matrix mod n when n is NOT prime,,"Given a $2\times 2$ matrix $A\in M_{2\times 2}(\mathbb Z)$ , if $\det(A)$ is relatively prime to $n$ , then we can find a $k$ such that $A^k\equiv I\pmod n$ . Let us denote the smallest positive $k$ with this property by $\operatorname{ord}_n(A)$ .  I wish to understand how this changes for a fixed $A$ and variable $n$ .  By the CRT, If $\gcd(m,n)=1$ , $\operatorname{ord}_{mn}(A)=\operatorname{lcm}(\operatorname{ord}_m(A),\operatorname{ord}_n(A))$ .  This reduces the problem to understanding $\operatorname{ord}_q(A)$ where $q=p^k$ is a prime power. One can show $\operatorname{ord}_p(A)$ divides either $p^2-1$ or $p^2-p$ . Further, if you know $A^m=I \pmod{p^k}$ , then $A^m=I+p^k B \pmod{p^{k+1}}$ for some (potentially zero) matrix $B$ , and then $A^{pm}=I \pmod{p^{k+1}}$ , so either $\operatorname{ord}_{p^{k+1}}(A)=\operatorname{ord}_{p^{k}}(A)$ or $\operatorname{ord}_{p^{k+1}}(A)=p\operatorname{ord}_{p^{k}}(A)$ . Are there any other ways one can zero in on this problem in general? Are there good conditions for when $\operatorname{ord}_{p^{k+1}}(A)=\operatorname{ord}_{p^{k}}(A)$ vs $\operatorname{ord}_{p^{k+1}}(A)=p\operatorname{ord}_{p^{k}}(A)$ ? My interest is in a specific matrix $$A=\begin{pmatrix}1 & 7 \\ 1 & 1\end{pmatrix}$$ which is particularly nice, so there may be a more specialized or ad hoc approach here, which I would be interested in as well,","Given a matrix , if is relatively prime to , then we can find a such that . Let us denote the smallest positive with this property by .  I wish to understand how this changes for a fixed and variable .  By the CRT, If , .  This reduces the problem to understanding where is a prime power. One can show divides either or . Further, if you know , then for some (potentially zero) matrix , and then , so either or . Are there any other ways one can zero in on this problem in general? Are there good conditions for when vs ? My interest is in a specific matrix which is particularly nice, so there may be a more specialized or ad hoc approach here, which I would be interested in as well,","2\times 2 A\in M_{2\times 2}(\mathbb Z) \det(A) n k A^k\equiv I\pmod n k \operatorname{ord}_n(A) A n \gcd(m,n)=1 \operatorname{ord}_{mn}(A)=\operatorname{lcm}(\operatorname{ord}_m(A),\operatorname{ord}_n(A)) \operatorname{ord}_q(A) q=p^k \operatorname{ord}_p(A) p^2-1 p^2-p A^m=I \pmod{p^k} A^m=I+p^k B \pmod{p^{k+1}} B A^{pm}=I \pmod{p^{k+1}} \operatorname{ord}_{p^{k+1}}(A)=\operatorname{ord}_{p^{k}}(A) \operatorname{ord}_{p^{k+1}}(A)=p\operatorname{ord}_{p^{k}}(A) \operatorname{ord}_{p^{k+1}}(A)=\operatorname{ord}_{p^{k}}(A) \operatorname{ord}_{p^{k+1}}(A)=p\operatorname{ord}_{p^{k}}(A) A=\begin{pmatrix}1 & 7 \\ 1 & 1\end{pmatrix}","['matrices', 'group-theory', 'elementary-number-theory', 'reference-request']"
98,Difference of positive semi-definite matrices,Difference of positive semi-definite matrices,,"If we have $S$ positive semi-definite matrices $A_1,\dots, A_S$ then what is the largest matrix positive semi definite matrix C such that $A_s -C$ is also psd for all $s=1,\dotsc,S$ ? By largest I mean in terms of a suitable norm.",If we have positive semi-definite matrices then what is the largest matrix positive semi definite matrix C such that is also psd for all ? By largest I mean in terms of a suitable norm.,"S A_1,\dots, A_S A_s -C s=1,\dotsc,S","['matrices', 'matrix-decomposition', 'convex-cone']"
99,Regular singular point of non-linear ODE,Regular singular point of non-linear ODE,,"Consider a system of ordinary differential equations of the form $$ \dot{x}(t) + \frac{1}{t}Ax(t) = B(x(t)) $$ where $x(t) \in \mathbb{R}^n$ , $A \in \mathrm{Mat}_{n\times n}(\mathbb{R})$ is a constant matrix, and $B: \mathbb{R}^n \to \mathbb{R}^n$ is homogeneous of degree $2$ , i.e. $B(\lambda x) = \lambda^2 B(x)$ for $\lambda \in \mathbb{R}$ . What is known about existence of solutions near $t = 0$ ? If it were not for the quadratic term, the point $t = 0$ would be a regular singular point of the ODE and then we could use the Frobenius method. But in all the books I have, regular singular points are only discussed for linear systems.","Consider a system of ordinary differential equations of the form where , is a constant matrix, and is homogeneous of degree , i.e. for . What is known about existence of solutions near ? If it were not for the quadratic term, the point would be a regular singular point of the ODE and then we could use the Frobenius method. But in all the books I have, regular singular points are only discussed for linear systems.","
\dot{x}(t) + \frac{1}{t}Ax(t) = B(x(t))
 x(t) \in \mathbb{R}^n A \in \mathrm{Mat}_{n\times n}(\mathbb{R}) B: \mathbb{R}^n \to \mathbb{R}^n 2 B(\lambda x) = \lambda^2 B(x) \lambda \in \mathbb{R} t = 0 t = 0","['real-analysis', 'matrices', 'ordinary-differential-equations', 'dynamical-systems']"
