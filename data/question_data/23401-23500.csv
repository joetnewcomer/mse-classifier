,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Bijective check with matrix,Bijective check with matrix,,My book doesn't cover the criterion for bijective transformations very well. I just want to check my understanding: is this statement true? Let F be a linear transformation. Let A be the matrix that represents that transformation (which means that that $F(v)=Av$ for any vector $v$). We now have that F is bijective iff $\det(A)\not=0$.,My book doesn't cover the criterion for bijective transformations very well. I just want to check my understanding: is this statement true? Let F be a linear transformation. Let A be the matrix that represents that transformation (which means that that $F(v)=Av$ for any vector $v$). We now have that F is bijective iff $\det(A)\not=0$.,,['linear-algebra']
1,Derivative of a map involving the matrix inverse,Derivative of a map involving the matrix inverse,,"I have $f: U\rightarrow \mathbb{R}$, $f(X):=\operatorname{tr}(X^{-1})$, $U$ contains all matrices $X$, which are  positive definite and symmetric. I want to show that $f$ is differentiable on $U$. To do so, I have to figure out  $f(X+tY)=\operatorname{tr}[(X+tY)^{-1}]=\operatorname{tr}[ (X(I-(-t)X^{-1}Y))^{-1}]$ Now it is: $(X(I-(-t)X^{-1}Y))^{-1}=\sum_{i=0}^{\infty}(-t)^{i}(X^{-1}Y)^{i}X^{-1}$. This is the point where I get stuck.  The first term of the series vanishes with $\operatorname{tr}[X]$, when it comes to the derivation. But how can I fix the rest?","I have $f: U\rightarrow \mathbb{R}$, $f(X):=\operatorname{tr}(X^{-1})$, $U$ contains all matrices $X$, which are  positive definite and symmetric. I want to show that $f$ is differentiable on $U$. To do so, I have to figure out  $f(X+tY)=\operatorname{tr}[(X+tY)^{-1}]=\operatorname{tr}[ (X(I-(-t)X^{-1}Y))^{-1}]$ Now it is: $(X(I-(-t)X^{-1}Y))^{-1}=\sum_{i=0}^{\infty}(-t)^{i}(X^{-1}Y)^{i}X^{-1}$. This is the point where I get stuck.  The first term of the series vanishes with $\operatorname{tr}[X]$, when it comes to the derivation. But how can I fix the rest?",,"['linear-algebra', 'derivatives']"
2,Is there a name for a block-diagonal matrix with blocks of the form $\begin{pmatrix} 0 & a \\ -a & 0 \end{pmatrix}$?,Is there a name for a block-diagonal matrix with blocks of the form ?,\begin{pmatrix} 0 & a \\ -a & 0 \end{pmatrix},"Is there a name for a real square matrix of the form $$\begin{pmatrix} 0 & a_1 \\ -a_1 & 0 \\ & & 0 & a_2 \\ & & -a_2 & 0 \\ & & & & \ddots \\ & & & & & 0 & a_k \\ & & & & & -a_k & 0 \end{pmatrix}$$ optionally with one or more additional rows/columns of 0? For example, analogous to the fact that a real symmetric matrix is orthogonally equivalent to a real diagonal matrix, we can show that a real skew-symmetric matrix is orthogonally equivalent to a matrix of the form above.","Is there a name for a real square matrix of the form $$\begin{pmatrix} 0 & a_1 \\ -a_1 & 0 \\ & & 0 & a_2 \\ & & -a_2 & 0 \\ & & & & \ddots \\ & & & & & 0 & a_k \\ & & & & & -a_k & 0 \end{pmatrix}$$ optionally with one or more additional rows/columns of 0? For example, analogous to the fact that a real symmetric matrix is orthogonally equivalent to a real diagonal matrix, we can show that a real skew-symmetric matrix is orthogonally equivalent to a matrix of the form above.",,"['linear-algebra', 'matrices']"
3,In a finite dimensional vector space a positive operator can have infinitely many square roots,In a finite dimensional vector space a positive operator can have infinitely many square roots,,"""In a finite dimensional vector space a identity operator (which is positive) can have infinitely many square roots"". I see only $2 ^ {C(n,2)}$ when dimension is $n (>1)$ by swapping two basis elements. Where am I wrong ?","""In a finite dimensional vector space a identity operator (which is positive) can have infinitely many square roots"". I see only $2 ^ {C(n,2)}$ when dimension is $n (>1)$ by swapping two basis elements. Where am I wrong ?",,['linear-algebra']
4,What is the name for a vector with a single non-zero component?,What is the name for a vector with a single non-zero component?,,"What do you call a vector with a single component, e.g. $[a, 0, ..., 0]$ or $[0, b, 0, ..., 0]$, where $a, b$ are any non-zero number? I'd like the language to differentiate from a vector with many components, e.g. $[a, b, 0, c, 0]$.","What do you call a vector with a single component, e.g. $[a, 0, ..., 0]$ or $[0, b, 0, ..., 0]$, where $a, b$ are any non-zero number? I'd like the language to differentiate from a vector with many components, e.g. $[a, b, 0, c, 0]$.",,"['linear-algebra', 'vector-spaces', 'convention']"
5,Why is $\det(A - \lambda I)$ zero?,Why is  zero?,\det(A - \lambda I),"I was reading over some notes of mine from the Linear Algebra class I took a while ago in order to prepare myself for Computer Graphics course I'll be taking this Fall and I ran across a sentence that I can't quite grasp. It states that Since we have a non-zero vector $x$ in the nullspace of $A - \lambda I$, then $\det(A - \lambda I) = 0$. From reading Wikipedia page on determinants I got that ... the system has a unique solution exactly when the determinant is nonzero; when the determinant is zero there are either no solutions or many solutions. So if we have vector $x$ in the nullspace of $A-\lambda I$, then we have either 0 solutions or many solutions. But I still don't understand why $\det(A - \lambda I)$ has to be equal zero?","I was reading over some notes of mine from the Linear Algebra class I took a while ago in order to prepare myself for Computer Graphics course I'll be taking this Fall and I ran across a sentence that I can't quite grasp. It states that Since we have a non-zero vector $x$ in the nullspace of $A - \lambda I$, then $\det(A - \lambda I) = 0$. From reading Wikipedia page on determinants I got that ... the system has a unique solution exactly when the determinant is nonzero; when the determinant is zero there are either no solutions or many solutions. So if we have vector $x$ in the nullspace of $A-\lambda I$, then we have either 0 solutions or many solutions. But I still don't understand why $\det(A - \lambda I)$ has to be equal zero?",,"['linear-algebra', 'matrices', 'characteristic-polynomial']"
6,Real Symmetric Matrices,Real Symmetric Matrices,,"Let $A$ be a real, symmetric, $n$ x $n$ matrix. Suppose $A^m=I$ for some $m$. Prove $A^2=I$. I think I want to use the symmetric implies diagonalizability...and take powers from there...correct?","Let $A$ be a real, symmetric, $n$ x $n$ matrix. Suppose $A^m=I$ for some $m$. Prove $A^2=I$. I think I want to use the symmetric implies diagonalizability...and take powers from there...correct?",,['linear-algebra']
7,Pseudo inverse of a product of two matrices with different rank,Pseudo inverse of a product of two matrices with different rank,,"Let $V$ be an $n \times n$ symmetric, positive definite matrix (of rank $n$). Let $X$ be an $n \times p$ matrix of rank $p$. Define $A^- = (A^\top A)^{-1} A^\top$ as the pseudo inverse of $A$ when $A$ is of full column rank. Note that $V^- = V^{-1}$ because $V$ is invertible. I'd like to prove that $$ (VX)^- = X^- V^{-1} $$ but the only theorem I know about the pseudo-inverses of products requires that both of the matrices be of the same rank AND that the second matrix has full row rank. (To wit: If $B$ is an $m \times r$ matrix of rank $r$ and $C$ is an $r \times m$  matrix of rank $r$, then $(BC)^- = C^-B^-$.) There is likely something obvious I'm missing. Any clues?","Let $V$ be an $n \times n$ symmetric, positive definite matrix (of rank $n$). Let $X$ be an $n \times p$ matrix of rank $p$. Define $A^- = (A^\top A)^{-1} A^\top$ as the pseudo inverse of $A$ when $A$ is of full column rank. Note that $V^- = V^{-1}$ because $V$ is invertible. I'd like to prove that $$ (VX)^- = X^- V^{-1} $$ but the only theorem I know about the pseudo-inverses of products requires that both of the matrices be of the same rank AND that the second matrix has full row rank. (To wit: If $B$ is an $m \times r$ matrix of rank $r$ and $C$ is an $r \times m$  matrix of rank $r$, then $(BC)^- = C^-B^-$.) There is likely something obvious I'm missing. Any clues?",,"['linear-algebra', 'matrices', 'inverse']"
8,A function from a matrix to its characteristic polynomial.,A function from a matrix to its characteristic polynomial.,,"Maybe it's a known fact, but I would like to know whether the function sending a square matrix of order $n$ to its characteristic polynomial is continuous. If this is true, there are some resources where I can find this? Thanks in advance.","Maybe it's a known fact, but I would like to know whether the function sending a square matrix of order to its characteristic polynomial is continuous. If this is true, there are some resources where I can find this? Thanks in advance.",n,"['linear-algebra', 'abstract-algebra', 'polynomials', 'continuity', 'characteristic-polynomial']"
9,Proving an invertible matrix which is its own inverse has determinant $1$ or $-1$ [duplicate],Proving an invertible matrix which is its own inverse has determinant  or  [duplicate],1 -1,This question already has answers here : Prove that an involutory matrix has eigenvalues $\pm 1$ (6 answers) Closed 5 years ago . Let A be an invertible $n \times n$ matrix whose inverse is itself. Prove that $\det(A)$ is either $1$ or $-1$. I'm really lost in class. I don't even know where to start. Please help.,This question already has answers here : Prove that an involutory matrix has eigenvalues $\pm 1$ (6 answers) Closed 5 years ago . Let A be an invertible $n \times n$ matrix whose inverse is itself. Prove that $\det(A)$ is either $1$ or $-1$. I'm really lost in class. I don't even know where to start. Please help.,,"['linear-algebra', 'matrices', 'determinant', 'involutions']"
10,Finding trace and determinant of linear operator,Finding trace and determinant of linear operator,,"I've got the following question Consider the linear operator of left multiplication by an $m \times m$ matrix $A$ on the vector space of all $m \times m$ matrices. Determine the trace and determinant of this operator. I'm a bit stuck as to how to even begin, I know this is going to involve eigenvalues/vectors and that if $\lambda_1, \lambda_2, ... ,\lambda_m$ are the $m$ roots of the characteristic polynomial of an $m \times m$ matrix $A$, then: $\det(A) = \lambda_1 ... \lambda_m$ and $\text{trace}(A) = \lambda_1 + ... + \lambda_m$ But obviously not all $m \times m$ matrices have $m$ eigenvalues so I'm really stuck on this question. Thanks!","I've got the following question Consider the linear operator of left multiplication by an $m \times m$ matrix $A$ on the vector space of all $m \times m$ matrices. Determine the trace and determinant of this operator. I'm a bit stuck as to how to even begin, I know this is going to involve eigenvalues/vectors and that if $\lambda_1, \lambda_2, ... ,\lambda_m$ are the $m$ roots of the characteristic polynomial of an $m \times m$ matrix $A$, then: $\det(A) = \lambda_1 ... \lambda_m$ and $\text{trace}(A) = \lambda_1 + ... + \lambda_m$ But obviously not all $m \times m$ matrices have $m$ eigenvalues so I'm really stuck on this question. Thanks!",,['linear-algebra']
11,Intuition about Hyperplane,Intuition about Hyperplane,,"I'm having a hard time understanding hyperplane ideas. So, can anyone explain to me how to easily understand what a Hyperplane is ?","I'm having a hard time understanding hyperplane ideas. So, can anyone explain to me how to easily understand what a Hyperplane is ?",,"['linear-algebra', 'geometry', 'matrices', 'intuition']"
12,"Properties of sum of real symmetric, positive semi-definite matrices","Properties of sum of real symmetric, positive semi-definite matrices",,I have two correlation matrices A and B. They are: Real symmetric (with ones on the diagonal) Positive semi-definite (eigenvalues are $\ge 0$) I want to try to prove that the average of these two matrices $C={1\over2}A + {1\over2}B$ still has the same properties. It was a few years since my linear algebra courses and would like some help along the way here. I am assuming it's possible as I have tested it numerically and for my 5000 randomly generated correlation matrices the property holds (at least within machine precision). Grateful for any kind of help!,I have two correlation matrices A and B. They are: Real symmetric (with ones on the diagonal) Positive semi-definite (eigenvalues are $\ge 0$) I want to try to prove that the average of these two matrices $C={1\over2}A + {1\over2}B$ still has the same properties. It was a few years since my linear algebra courses and would like some help along the way here. I am assuming it's possible as I have tested it numerically and for my 5000 randomly generated correlation matrices the property holds (at least within machine precision). Grateful for any kind of help!,,"['linear-algebra', 'numerical-linear-algebra']"
13,Finding range of a linear transformation,Finding range of a linear transformation,,"Define $T: \mathbb{R}^3 \rightarrow \mathbb{R}^2$ by $T(x,y,z) = (2y + z, x-z)$. Find $\mbox{ker}(T)$ and $\mbox{range}(T)$ I could find the kernel easy enough, and ended up getting $\{(-2x, x, -2x) : x \in \mathbb{R}\}$ but I don't really know how the get the range. In this case isn't the range effectively just the set with elements satisfying the equation $T$? I'm not really sure what the question is wanting to be honest. Some help would be great. Thanks","Define $T: \mathbb{R}^3 \rightarrow \mathbb{R}^2$ by $T(x,y,z) = (2y + z, x-z)$. Find $\mbox{ker}(T)$ and $\mbox{range}(T)$ I could find the kernel easy enough, and ended up getting $\{(-2x, x, -2x) : x \in \mathbb{R}\}$ but I don't really know how the get the range. In this case isn't the range effectively just the set with elements satisfying the equation $T$? I'm not really sure what the question is wanting to be honest. Some help would be great. Thanks",,['linear-algebra']
14,Is the identity matrix an elementary matrix?,Is the identity matrix an elementary matrix?,,"My reasoning is yes, as you can switch row i with row i in the matrix... But I'm not sure if it's a ""legal"" elementary operation to switch a row with itself.","My reasoning is yes, as you can switch row i with row i in the matrix... But I'm not sure if it's a ""legal"" elementary operation to switch a row with itself.",,"['linear-algebra', 'matrices']"
15,What's the difference between Jordan and Schur decomposition?,What's the difference between Jordan and Schur decomposition?,,"They both seems to decompose a square matrix into a upper triangular matrix, but what's the fundamental difference between these two decompositions? Thanks!","They both seems to decompose a square matrix into a upper triangular matrix, but what's the fundamental difference between these two decompositions? Thanks!",,"['linear-algebra', 'matrices']"
16,Show that the minimum eigenvalue of a Hermitian matrix $A$ is less than or equal to the smallest diagonal element of $A$,Show that the minimum eigenvalue of a Hermitian matrix  is less than or equal to the smallest diagonal element of,A A,"I have a following question: Let $A \in  C^{n\times n}$ be Hermitian and $\lambda_\min$ be the smallest eigenvalue of $A$ , i.e., $\lambda_\min = \min\{\lambda_1, \ldots, \lambda_n\}$ . Show that $\lambda_{\min} \leq \min_j a_{j}$ . Hint: use the properties of the Rayleigh quotient. I got started with the problem, as follows: Rayleigh quotient, for any vector $x$ , is given as, $$r(x) = \frac{\langle Ax,x\rangle}{\langle x,x\rangle}.$$ If $x$ is an eigenvector, $r(x) = \lambda$ . For a Hermitian matrix, $r(x)\in\mathbb R$ , the eigenvalues are real. Now, a Hermitian matrix can be diagonalized as: $A = UDU^*$ , where U is a unitary matrix of eigenvectors, and $D$ is a diagonal matrix of eigenvalues of $A$ . I am missing the final link, that I need to solve my problem. Thanks.","I have a following question: Let be Hermitian and be the smallest eigenvalue of , i.e., . Show that . Hint: use the properties of the Rayleigh quotient. I got started with the problem, as follows: Rayleigh quotient, for any vector , is given as, If is an eigenvector, . For a Hermitian matrix, , the eigenvalues are real. Now, a Hermitian matrix can be diagonalized as: , where U is a unitary matrix of eigenvectors, and is a diagonal matrix of eigenvalues of . I am missing the final link, that I need to solve my problem. Thanks.","A \in  C^{n\times n} \lambda_\min A \lambda_\min = \min\{\lambda_1, \ldots, \lambda_n\} \lambda_{\min} \leq \min_j a_{j} x r(x) = \frac{\langle Ax,x\rangle}{\langle x,x\rangle}. x r(x) = \lambda r(x)\in\mathbb R A = UDU^* D A","['linear-algebra', 'numerical-linear-algebra']"
17,What does matrix rank $k$ to precision $\epsilon$ mean?,What does matrix rank  to precision  mean?,k \epsilon,"I read this sentence. Suppose that the matrix $A_{ij}$ of dimension $n_i \times n_j$ has rank $k$ to precision $\epsilon$, then there exists a factorization of $A_{ij}$ of the form:   $A_{ij} = L_i S_{ij} R_j + \text{O}(\epsilon)$. I wonder what does matrix rank $k$ to precision $\epsilon$ mean? Thank you.","I read this sentence. Suppose that the matrix $A_{ij}$ of dimension $n_i \times n_j$ has rank $k$ to precision $\epsilon$, then there exists a factorization of $A_{ij}$ of the form:   $A_{ij} = L_i S_{ij} R_j + \text{O}(\epsilon)$. I wonder what does matrix rank $k$ to precision $\epsilon$ mean? Thank you.",,"['linear-algebra', 'numerical-methods']"
18,What is the term for the projection of a vector onto the unit cube?,What is the term for the projection of a vector onto the unit cube?,,"Normalizing a vector sets its magnitude to $1$ and retains its direction. In three dimensions, it projects the vector onto the unit sphere. Is there a term associated with projecting it onto the unit cube (where at least one coordinate is equal to 1), or clamping to the unit cube?","Normalizing a vector sets its magnitude to $1$ and retains its direction. In three dimensions, it projects the vector onto the unit sphere. Is there a term associated with projecting it onto the unit cube (where at least one coordinate is equal to 1), or clamping to the unit cube?",,"['linear-algebra', 'geometry', 'terminology', 'vector-spaces']"
19,Help with commutative property of matrices problem,Help with commutative property of matrices problem,,"Given matrices $A$ and $B$ , where that $AB = A + B$ , prove $AB = BA$ . I keep coming up with AB = AB. It seems like basic algebra, but for the life of me, I'm getting nowhere :/. Someone help please?","Given matrices and , where that , prove . I keep coming up with AB = AB. It seems like basic algebra, but for the life of me, I'm getting nowhere :/. Someone help please?",A B AB = A + B AB = BA,"['linear-algebra', 'matrices']"
20,"Find extra arbitrary two points for a plane, given the normal and a point that lies on the plane","Find extra arbitrary two points for a plane, given the normal and a point that lies on the plane",,"For a plane, I have the normal $n$, and also a point $P$ that lies on the plane. Now, how am I going to find extra arbitrary two points ($P_1$ and $P_2$) for the plane so that these three points $P$, $P_1$ and $P_2$ completely define the plane? The solution here suggests that one assumes a certain $x$ and $y$ to substitute into the plane equation and find the remaining  $z$. But this method is only suitable for hand calculation; it breaks down for plane $z=0$. As such, it is not suitable for computer implementation. I would need an algorithm that is robust and can handle all the cases, any idea how to construct it? There is a similar question here , and the answer suggests me to use  Gram-Schmidt, but I don't see how it can be applied in my case here.","For a plane, I have the normal $n$, and also a point $P$ that lies on the plane. Now, how am I going to find extra arbitrary two points ($P_1$ and $P_2$) for the plane so that these three points $P$, $P_1$ and $P_2$ completely define the plane? The solution here suggests that one assumes a certain $x$ and $y$ to substitute into the plane equation and find the remaining  $z$. But this method is only suitable for hand calculation; it breaks down for plane $z=0$. As such, it is not suitable for computer implementation. I would need an algorithm that is robust and can handle all the cases, any idea how to construct it? There is a similar question here , and the answer suggests me to use  Gram-Schmidt, but I don't see how it can be applied in my case here.",,"['linear-algebra', 'geometry', 'analytic-geometry']"
21,If every eigenvector of $T$ is also an eigenvector of $T^{*}$ then $T$ is a normal operator,If every eigenvector of  is also an eigenvector of  then  is a normal operator,T T^{*} T,"Let $V$ be a finite-dimensional inner product space over $\mathbb{C}$ and $T: V \to V$ a linear transformation. Show that if every eigenvector of $T$ is also an eigenvector of $T^{*}$ then $T$ is a normal. We need to show that $\forall v \in V,\ TT^*v=T^*Tv$. I started by picking $v$ to be an eigenvector of $T$, hence $Tv=\lambda_1 v$, and $T^*v=\lambda_2 v$. Therefore: $$TT^*v=T(\lambda_2 v)=\lambda_2 Tv=\lambda_1 \lambda_2 v=...=T^*Tv$$ But I'm stuck when $v$ is not an eigenvector. (Also, this question is taken from a chapter about Jordan forms, but I can't see how it relates)","Let $V$ be a finite-dimensional inner product space over $\mathbb{C}$ and $T: V \to V$ a linear transformation. Show that if every eigenvector of $T$ is also an eigenvector of $T^{*}$ then $T$ is a normal. We need to show that $\forall v \in V,\ TT^*v=T^*Tv$. I started by picking $v$ to be an eigenvector of $T$, hence $Tv=\lambda_1 v$, and $T^*v=\lambda_2 v$. Therefore: $$TT^*v=T(\lambda_2 v)=\lambda_2 Tv=\lambda_1 \lambda_2 v=...=T^*Tv$$ But I'm stuck when $v$ is not an eigenvector. (Also, this question is taken from a chapter about Jordan forms, but I can't see how it relates)",,['linear-algebra']
22,Logarithm of a Markov Matrix,Logarithm of a Markov Matrix,,"Start with a Markov matrix $\mathbf{M}$, whose elements are all between $0 \le \mathbf{M}_{ij} \le 1$ and each row sums to one. There is a natural connection with this matrix and the rate matrix $\mathbf{W}$ in the Master Equation $$ \mathbf{M} = \exp( t \mathbf{W} ) $$ Here, given $\mathbf{W}$, the calculation of $\mathbf{M}$ is unambiguous with $t$ since the matrix exponential is unique and converges by the Taylor expansion. What about the other direction? $$ t \mathbf{W} = \log( \mathbf{M} ) $$ Do the properties of the Markov matrix guarantee this is unique and that the alternating sum in the log Taylor series converge? Please provide a reference where this is discussed if possible! Motivation (by request) I've been studying the dynamics assoicated with an Ising model type system under single spin-flip Glauber dynamics. Glauber dynamics gives essentially the $\mathbf{W}$ matrix. If one were to observe the system over a finite time, an approximation to $\mathbf{M}$ could be made. I was interested in when it was permissible to convert between the two. In the reference provided by one of the answers the question boils down to: In probabilistic terms a Markov matrix A is embeddable if it   is obtained by taking a snapshot at a particular time of an autonomous finite   state Markov process that develops continuously in time. On the other hand   a Markov matrix might not be embeddable if it describes the annual changes   in a population that has a strongly seasonal breeding pattern; in such cases   one might construct a more elaborate model that incorporates the seasonal   variations. Embeddability may also fail because the matrix entries are not   accurate; in such cases a regularization technique might yield a very similar   Markov matrix that is embeddable;","Start with a Markov matrix $\mathbf{M}$, whose elements are all between $0 \le \mathbf{M}_{ij} \le 1$ and each row sums to one. There is a natural connection with this matrix and the rate matrix $\mathbf{W}$ in the Master Equation $$ \mathbf{M} = \exp( t \mathbf{W} ) $$ Here, given $\mathbf{W}$, the calculation of $\mathbf{M}$ is unambiguous with $t$ since the matrix exponential is unique and converges by the Taylor expansion. What about the other direction? $$ t \mathbf{W} = \log( \mathbf{M} ) $$ Do the properties of the Markov matrix guarantee this is unique and that the alternating sum in the log Taylor series converge? Please provide a reference where this is discussed if possible! Motivation (by request) I've been studying the dynamics assoicated with an Ising model type system under single spin-flip Glauber dynamics. Glauber dynamics gives essentially the $\mathbf{W}$ matrix. If one were to observe the system over a finite time, an approximation to $\mathbf{M}$ could be made. I was interested in when it was permissible to convert between the two. In the reference provided by one of the answers the question boils down to: In probabilistic terms a Markov matrix A is embeddable if it   is obtained by taking a snapshot at a particular time of an autonomous finite   state Markov process that develops continuously in time. On the other hand   a Markov matrix might not be embeddable if it describes the annual changes   in a population that has a strongly seasonal breeding pattern; in such cases   one might construct a more elaborate model that incorporates the seasonal   variations. Embeddability may also fail because the matrix entries are not   accurate; in such cases a regularization technique might yield a very similar   Markov matrix that is embeddable;",,"['linear-algebra', 'stochastic-processes', 'logarithms']"
23,Dot product of two vectors,Dot product of two vectors,,How does one show that the dot product of two vectors is $ A \cdot B = |A| |B| \cos (\Theta) $?,How does one show that the dot product of two vectors is $ A \cdot B = |A| |B| \cos (\Theta) $?,,['linear-algebra']
24,Orthonormal Eigenbasis,Orthonormal Eigenbasis,,"I am a little apprehensive to ask this question because I have a feeling it's a ""duh"" question but I guess that's the beauty of sites like this (anonymity): I need to find an orthonormal eigenbasis for the $2 \times 2$ matrix $\left(\begin{array}{cc}1&1\\ 1&1\end{array}\right)$.  I calculated that the eigenvalues were $x=0$ and $x=2$ and the corresponding eigenvectors were  $E(0) = \mathrm{span}\left(\begin{array}{r}-1\\1\end{array}\right)$ and $E(2) = \mathrm{span}\left(\begin{array}{c}1\\1\end{array}\right)$. Therefore, an orthonormal eigenbasis would be:  $$\frac{1}{\sqrt{2}}\left(\begin{array}{r}-1\\1\end{array}\right),  \frac{1}{\sqrt{2}}\left(\begin{array}{c}1\\1\end{array}\right).$$ Here my question: Could the eigenvalues for $E(0)$ been $\mathrm{span}\left(\begin{array}{r}1\\-1\end{array}\right)$??  This would make the final answer $\frac{1}{\sqrt{2}}\left(\begin{array}{r}1\\-1\end{array}\right), \frac{1}{\sqrt{2}}\left(\begin{array}{c}1\\1\end{array}\right)$.  Is one answer more correct than the other (or are they both wrong)? Thanks!","I am a little apprehensive to ask this question because I have a feeling it's a ""duh"" question but I guess that's the beauty of sites like this (anonymity): I need to find an orthonormal eigenbasis for the $2 \times 2$ matrix $\left(\begin{array}{cc}1&1\\ 1&1\end{array}\right)$.  I calculated that the eigenvalues were $x=0$ and $x=2$ and the corresponding eigenvectors were  $E(0) = \mathrm{span}\left(\begin{array}{r}-1\\1\end{array}\right)$ and $E(2) = \mathrm{span}\left(\begin{array}{c}1\\1\end{array}\right)$. Therefore, an orthonormal eigenbasis would be:  $$\frac{1}{\sqrt{2}}\left(\begin{array}{r}-1\\1\end{array}\right),  \frac{1}{\sqrt{2}}\left(\begin{array}{c}1\\1\end{array}\right).$$ Here my question: Could the eigenvalues for $E(0)$ been $\mathrm{span}\left(\begin{array}{r}1\\-1\end{array}\right)$??  This would make the final answer $\frac{1}{\sqrt{2}}\left(\begin{array}{r}1\\-1\end{array}\right), \frac{1}{\sqrt{2}}\left(\begin{array}{c}1\\1\end{array}\right)$.  Is one answer more correct than the other (or are they both wrong)? Thanks!",,"['linear-algebra', 'eigenvalues-eigenvectors', 'orthonormal']"
25,How to solve a system of linear equations modulo $8$?,How to solve a system of linear equations modulo ?,8,"I encountered a set of linear equations with modulo in only $2$ variables $$(a_{11}x + a_{12}y) \mod 8 = b_1$$ $$(a_{21}x + a_{22}y) \mod 8 = b_2$$ In the case of simple equations without modulo, the solution of $Ax=b$ is $x=A^{-1}b$, but how to handle this modulo? Any clue or pointers will be very helpful.","I encountered a set of linear equations with modulo in only $2$ variables $$(a_{11}x + a_{12}y) \mod 8 = b_1$$ $$(a_{21}x + a_{22}y) \mod 8 = b_2$$ In the case of simple equations without modulo, the solution of $Ax=b$ is $x=A^{-1}b$, but how to handle this modulo? Any clue or pointers will be very helpful.",,"['linear-algebra', 'modular-arithmetic', 'modules', 'systems-of-equations']"
26,How can angular velocity or angular momentum be a vector?,How can angular velocity or angular momentum be a vector?,,"Rotations in 3 dimensions are not commutative; however they are in the plane. In classical mechanics, are we allowed to say that angular momentum is a vector because particles only rotate along a single axis? Or do we make some kind of argument that you can assign angular velocity to an object because you can approximate it locally as being along a single rotation axis and so its ""locally commutative"" and hence appropriate to call it a vector? If that is the case what happens if we have a particle rotating in a way that isn't differentiable? Is there some physical reason that particles can't rotate in that sort of way? I'm really confused by this and haven't taken physics since high school, any help would be appreciated. :)","Rotations in 3 dimensions are not commutative; however they are in the plane. In classical mechanics, are we allowed to say that angular momentum is a vector because particles only rotate along a single axis? Or do we make some kind of argument that you can assign angular velocity to an object because you can approximate it locally as being along a single rotation axis and so its ""locally commutative"" and hence appropriate to call it a vector? If that is the case what happens if we have a particle rotating in a way that isn't differentiable? Is there some physical reason that particles can't rotate in that sort of way? I'm really confused by this and haven't taken physics since high school, any help would be appreciated. :)",,"['linear-algebra', 'physics']"
27,Inverse for the canonical isomorphism of inner product space with its dual.,Inverse for the canonical isomorphism of inner product space with its dual.,,"Let $V$ be a finite-dimensional vector space equipped with an inner product $\langle\cdot,\cdot\rangle$ . We know that in this case $V$ is isomorphic to $V^*$ via the map $$f:V \to V^\ast, f(v)=\langle v,\cdot\rangle.$$ I can prove that this is injective and surjective, but what I wanted to know is that what is the inverse of this map explicitly? I couldn't find an answer online. Much of the stuff  I found were proving the bijectivity, but not giving an explicit inverse.","Let be a finite-dimensional vector space equipped with an inner product . We know that in this case is isomorphic to via the map I can prove that this is injective and surjective, but what I wanted to know is that what is the inverse of this map explicitly? I couldn't find an answer online. Much of the stuff  I found were proving the bijectivity, but not giving an explicit inverse.","V \langle\cdot,\cdot\rangle V V^* f:V \to V^\ast, f(v)=\langle v,\cdot\rangle.","['linear-algebra', 'abstract-algebra', 'vector-spaces']"
28,Is there a $5 \times 5$ matrix with exactly $2$ fun elements?,Is there a  matrix with exactly  fun elements?,5 \times 5 2,"An element of a matrix is called 'fun' if changing this element changes the determinant. Is there a $5 \times 5$ matrix with exactly $2$ fun elements? My intuition is that there does not exist such a matrix. I have tried using properties of the determinant to no avail. I have also tried to relate eigenvalues and the characteristic polynomial to this problem, but I'm having trouble relating eigenvalues to specific elements of a matrix. Any help or guidance would be appreciated, thank you!","An element of a matrix is called 'fun' if changing this element changes the determinant. Is there a matrix with exactly fun elements? My intuition is that there does not exist such a matrix. I have tried using properties of the determinant to no avail. I have also tried to relate eigenvalues and the characteristic polynomial to this problem, but I'm having trouble relating eigenvalues to specific elements of a matrix. Any help or guidance would be appreciated, thank you!",5 \times 5 2,"['linear-algebra', 'matrices']"
29,"Prove that $\min_\limits{x\in\{-1,1\}^n} \lVert Ax\rVert_\infty≤ C\sqrt{n\log(n)}$",Prove that,"\min_\limits{x\in\{-1,1\}^n} \lVert Ax\rVert_\infty≤ C\sqrt{n\log(n)}",I’m struggling a lot since few hours on this exercice and I really have no idea how to make the log appears. The problem is the following: Let $A$ be a matrix of size $n\times n$ whose entries $a_{ij}$ are such that $\left|a_{ij}\right|\leqslant1$ . Prove that there exists $C>0$ a constant that doesn’t depend on $n$ such that: $\min_\limits{x\in\{-1;1\}^n}$ $\lVert Ax\rVert_{\infty}\leqslant C\sqrt{n\log(n)}$ I did a lot of exercises dealing with inequalities of matrix norm but I really have no idea for this one. I learnt about Jensen inequality but it led me nowhere. Thanks in advance for any kind of help.,I’m struggling a lot since few hours on this exercice and I really have no idea how to make the log appears. The problem is the following: Let be a matrix of size whose entries are such that . Prove that there exists a constant that doesn’t depend on such that: I did a lot of exercises dealing with inequalities of matrix norm but I really have no idea for this one. I learnt about Jensen inequality but it led me nowhere. Thanks in advance for any kind of help.,A n\times n a_{ij} \left|a_{ij}\right|\leqslant1 C>0 n \min_\limits{x\in\{-1;1\}^n} \lVert Ax\rVert_{\infty}\leqslant C\sqrt{n\log(n)},"['linear-algebra', 'matrices', 'inequality']"
30,"Provided that $A$ is a $n \times n$ real matrix with $A^2＝0$, is it true that $\operatorname{rank}(A+A^\text{T})＝2\operatorname{rank}(A)$?","Provided that  is a  real matrix with , is it true that ?",A n \times n A^2＝0 \operatorname{rank}(A+A^\text{T})＝2\operatorname{rank}(A),"Question: Provided that $A$ is a $n×n$ real matrix with $A^2＝0$ , is it true that $\operatorname{rank}(A+A^\text{T})＝2\operatorname{rank}(A)$ ? I believe it’s true. I’ve tried matrix operations, Jordan standard form and dividing $A$ into column vectors, but all failed. Now I’m aware that linear mapping might be a good choice, because $Ax$ and $A^\text{T}x$ are perpendicular, so we can prove that $\ker(A+A^\text{T})=\ker(A)\cap\ker(A^\text{T})$ . Then we need to prove that $\dim(\ker(A+A^\text{T}))=n-2 \operatorname{rank}(A)$ , but I don’t know how.","Question: Provided that is a real matrix with , is it true that ? I believe it’s true. I’ve tried matrix operations, Jordan standard form and dividing into column vectors, but all failed. Now I’m aware that linear mapping might be a good choice, because and are perpendicular, so we can prove that . Then we need to prove that , but I don’t know how.",A n×n A^2＝0 \operatorname{rank}(A+A^\text{T})＝2\operatorname{rank}(A) A Ax A^\text{T}x \ker(A+A^\text{T})=\ker(A)\cap\ker(A^\text{T}) \dim(\ker(A+A^\text{T}))=n-2 \operatorname{rank}(A),"['linear-algebra', 'matrices', 'matrix-rank']"
31,Does anything change if we allow complex roots in system of linear equations?,Does anything change if we allow complex roots in system of linear equations?,,"Say we have two variable, two equation system ( $a_i, b_i, c_i \in \mathbb{R}$ ). $$a_1x + b_1y + c_1 = 0$$ $$a_2x + b_2y + c_2 = 0$$ If equation $2$ is just equation $(1)$ multiplied by a constant, then the system has infinite solution. Otherwise, if $a_1b_2=a_2b_1$ , then the system has no solution. Else, the system has a unique solution. My question is: Does anything change if we allow $x, y$ to take complex values? For example, is it possible for the system to have no solution for real $x, y$ , but have a solution for complex $x, y$ ? What if the system has a unique solution, is it possible that system gets infinite solution when we extend $x, y$ to complex? Sorry for stupid question. Thanks","Say we have two variable, two equation system ( ). If equation is just equation multiplied by a constant, then the system has infinite solution. Otherwise, if , then the system has no solution. Else, the system has a unique solution. My question is: Does anything change if we allow to take complex values? For example, is it possible for the system to have no solution for real , but have a solution for complex ? What if the system has a unique solution, is it possible that system gets infinite solution when we extend to complex? Sorry for stupid question. Thanks","a_i, b_i, c_i \in \mathbb{R} a_1x + b_1y + c_1 = 0 a_2x + b_2y + c_2 = 0 2 (1) a_1b_2=a_2b_1 x, y x, y x, y x, y","['linear-algebra', 'algebra-precalculus', 'complex-numbers', 'systems-of-equations']"
32,What do you call a matrix of the form $\left(\begin{smallmatrix} a & -b \\ b & a \end{smallmatrix}\right)$?,What do you call a matrix of the form ?,\left(\begin{smallmatrix} a & -b \\ b & a \end{smallmatrix}\right),"I'm reviewing a piece of python code that uses the term ""rotations"" for these matrices, but of course that's not quite accurate. What's a good, more accurate term of similar accessibility? Here are my candidates so far: scaling-and-rotation matrix : Clunky, but the best I've got right now. scaled rotation : Less clunky than the above. This term might sound like we're scaling the amount of rotation . conformal matrix : Technically correct, but far fewer people will understand the meaning compared to ""rotation matrix"". Also, it's a bit ingenuous since the term conformal prototypically refers to nonlinear maps that behave infinitesimally like these matrices. angle-preserving matrix : A more accessible form of conformal matrix . Less knowledgeable readers may be distracted trying to figure out which $2\times2$ matrices are angle-preserving. (Readers who understand the term conformal matrix will probably already know this.) [multiplication by a] complex number : Not as accessible as ""rotation"". Also a bit clunky since it identifies a complex number with its representation. sum of a scaling and antisymmetric matrix : Worst suggestion so far, especially since antisymmetric matrices don't show up anywhere in the code or code comments. Is there any standard, accessible term for such linear transformations? I'd also appreciate thoughts on whether I'm mistaken about how accessible and/or clunky the terms above are.","I'm reviewing a piece of python code that uses the term ""rotations"" for these matrices, but of course that's not quite accurate. What's a good, more accurate term of similar accessibility? Here are my candidates so far: scaling-and-rotation matrix : Clunky, but the best I've got right now. scaled rotation : Less clunky than the above. This term might sound like we're scaling the amount of rotation . conformal matrix : Technically correct, but far fewer people will understand the meaning compared to ""rotation matrix"". Also, it's a bit ingenuous since the term conformal prototypically refers to nonlinear maps that behave infinitesimally like these matrices. angle-preserving matrix : A more accessible form of conformal matrix . Less knowledgeable readers may be distracted trying to figure out which matrices are angle-preserving. (Readers who understand the term conformal matrix will probably already know this.) [multiplication by a] complex number : Not as accessible as ""rotation"". Also a bit clunky since it identifies a complex number with its representation. sum of a scaling and antisymmetric matrix : Worst suggestion so far, especially since antisymmetric matrices don't show up anywhere in the code or code comments. Is there any standard, accessible term for such linear transformations? I'd also appreciate thoughts on whether I'm mistaken about how accessible and/or clunky the terms above are.",2\times2,"['linear-algebra', 'matrices', 'complex-numbers', 'terminology']"
33,Is the set of strictly positive polynomials an open set? In case it is a smooth manifold.,Is the set of strictly positive polynomials an open set? In case it is a smooth manifold.,,"Let $I=[-1, 1]$ , $\mathcal{P}^n$ be the set of polynomials of degree $n$ with domain $I$ , $\mathcal{P}^n_+(I, \mathbb{R})$ be the set of real-valued polynomials with domain $I$ that are strictly positive $$ \forall p\in\mathcal{P}^n_+(I, \mathbb{R}) \ \ p(t) >0 \forall t\in I $$ Is this set open with respect to the topology induced by the $C^0$ norm, i.e. $$ \| p \|_{C^0} = \sup_{t\in I} | p(t) | $$ Attempt of proof: In order to prove that $\mathcal{P}_+(I, \mathbb{R})$ is open with respect to the mentioned topology I will try to demonstrate that all its elements has a neighborhood contained in it. First we define a ball as $$ N(p, r) = \left\{q\in\mathcal{P}^n\text{ s.t. } \| p - q \|_{C^0}<r\right\} $$ Any ball is a neighborhood. We desire to prove that $$ p\in \mathcal{P}_+^n(I, \mathbb{R})  \implies \exists r>0\text{ s.t. } N(p, r) \subset \mathcal{P}^n_+(I, \mathbb{R})  $$ Let $p\in\mathcal{P}_+^n$ and the ball $$ N'(p, r) = \left\{p+e\in\mathcal{P}^n\text{ s.t. } \| e \|_{C^0}<r\right\} $$ we can always find a sufficiently small $r>0$ such that $$ \inf_{t\in I} \left\{ p(t) + e(t)\right\} >0 $$ It this proof right? If it is, then as $\mathcal{P}^n$ is a vector space, then $\mathcal{P}_+^n$ is a smooth manifold.","Let , be the set of polynomials of degree with domain , be the set of real-valued polynomials with domain that are strictly positive Is this set open with respect to the topology induced by the norm, i.e. Attempt of proof: In order to prove that is open with respect to the mentioned topology I will try to demonstrate that all its elements has a neighborhood contained in it. First we define a ball as Any ball is a neighborhood. We desire to prove that Let and the ball we can always find a sufficiently small such that It this proof right? If it is, then as is a vector space, then is a smooth manifold.","I=[-1, 1] \mathcal{P}^n n I \mathcal{P}^n_+(I, \mathbb{R}) I 
\forall p\in\mathcal{P}^n_+(I, \mathbb{R}) \ \ p(t) >0 \forall t\in I
 C^0 
\| p \|_{C^0} = \sup_{t\in I} | p(t) |
 \mathcal{P}_+(I, \mathbb{R}) 
N(p, r) = \left\{q\in\mathcal{P}^n\text{ s.t. } \| p - q \|_{C^0}<r\right\}
 
p\in \mathcal{P}_+^n(I, \mathbb{R})  \implies \exists r>0\text{ s.t. } N(p, r) \subset \mathcal{P}^n_+(I, \mathbb{R}) 
 p\in\mathcal{P}_+^n 
N'(p, r) = \left\{p+e\in\mathcal{P}^n\text{ s.t. } \| e \|_{C^0}<r\right\}
 r>0 
\inf_{t\in I} \left\{ p(t) + e(t)\right\} >0
 \mathcal{P}^n \mathcal{P}_+^n","['linear-algebra', 'general-topology', 'functional-analysis', 'polynomials']"
34,Left ideals of $\mathrm{End}_{k}(V)$,Left ideals of,\mathrm{End}_{k}(V),"I'm trying to solve the following problem: $\DeclareMathOperator{\End}{End}$ Let V be a vector space over a field $K$ with $\dim_k V =n$ and consider the ring $R = \End_k(V)$ . If $U$ is a subspace define $I_U = \{ f \in R : U \subset \ker f\}$ . Show that $I_U$ is a left ideal of $R$ and that every left ideal has this form. Showing $I_U$ is left ideal is straightforward. I'm having trouble with the second part. I started taking $I$ left ideal of $R$ . And define $m = \min \{\dim(\ker f) : f \in I\}$ and $U = \ker f $ where $f$ is any element of $I$ with $\dim(\ker f) = m$ . Then I want to show that $I=I_U$ . If $g \in I_U$ I need to construct $h$ such that $g= h \circ f$ . However, I'm having trouble defining $h$ . For the other inclusion, I don't know where to start.","I'm trying to solve the following problem: Let V be a vector space over a field with and consider the ring . If is a subspace define . Show that is a left ideal of and that every left ideal has this form. Showing is left ideal is straightforward. I'm having trouble with the second part. I started taking left ideal of . And define and where is any element of with . Then I want to show that . If I need to construct such that . However, I'm having trouble defining . For the other inclusion, I don't know where to start.",\DeclareMathOperator{\End}{End} K \dim_k V =n R = \End_k(V) U I_U = \{ f \in R : U \subset \ker f\} I_U R I_U I R m = \min \{\dim(\ker f) : f \in I\} U = \ker f  f I \dim(\ker f) = m I=I_U g \in I_U h g= h \circ f h,"['linear-algebra', 'abstract-algebra', 'ring-theory', 'ideals']"
35,Show that $f(t) = \det(A + tB)$ is a line,Show that  is a line,f(t) = \det(A + tB),"Suppose $A,B \in \mathbb{R}^{n \times n}$ where rank $(B) = 1$ . Show that the function $f: \mathbb{R} \to \mathbb{R}$ where $$f(t) = \det(A + tB)$$ is a line , i.e., has the form $f = mt + b$ . So far, I really only have that $f(0) = \det(A) = b$ . I thought about explicitly calculating the derivative as in, $$ \lim_{t\to0} \frac{\det(A + tB) - b}{t}$$ but I'm not sure where I would go next. Also, I am not sure how to use that rank $(B) = 1$ . Any help would be appreciated.","Suppose where rank . Show that the function where is a line , i.e., has the form . So far, I really only have that . I thought about explicitly calculating the derivative as in, but I'm not sure where I would go next. Also, I am not sure how to use that rank . Any help would be appreciated.","A,B \in \mathbb{R}^{n \times n} (B) = 1 f: \mathbb{R} \to \mathbb{R} f(t) = \det(A + tB) f = mt + b f(0) = \det(A) = b  \lim_{t\to0} \frac{\det(A + tB) - b}{t} (B) = 1","['linear-algebra', 'matrices', 'determinant']"
36,$A$ is positive semidefinite $\iff \text{det} (B_K) \geq 0$,is positive semidefinite,A \iff \text{det} (B_K) \geq 0,"Let $A \in \mathbb R^{n \times n}$ a symmetric matrix. Show that $A$ is positive semidefinite $\iff$ all its symmetric minors are $\geq 0$ , that means $\det(B_K) \geq 0$ for all $K \subseteq \{1,\cdots,n\}$ . (Let $K = \{ l_1, \cdots, l_k \} \subseteq \{1,\cdots,n\}$ where $1 \leq l_1 < l_2 < \cdots < l_k \leq n$ . The matrix $B_K \in \mathbb R^{k \times k}$ is the matrix with $(B_K)_{ij}=A_{l_il_j}$ , $1\leq i,j \leq k$ ). $\Longrightarrow$ $A$ is symmetric matrix so by the spectral theorem, we have that $$A=U\begin{pmatrix} \lambda_1 &  & \\  & \ddots \\  & & \lambda_n \end{pmatrix}U^T$$ with $U$ an orthogonal matrix and $\lambda_i \geq 0$ $\forall i$ because A is positive semidefinite. We have that $(B_K)=\begin{bmatrix} u_{l_1} \\ \cdots \\ u_{l_k}  \end{bmatrix}$ $\begin{pmatrix} \lambda_1 &  & \\  & \ddots \\  & & \lambda_n \end{pmatrix}$ $\begin{bmatrix} u_{l_1}^T & \cdots & u_{l_k}^T \end{bmatrix}$ , where $u_{l_i}$ is the $l_i^\text{ th}$ line of $U$ . Since $\begin{bmatrix} u_{l_1} \\ \cdots \\ u_{l_k}  \end{bmatrix} \cdot \begin{bmatrix} u_{l_1}^T & \cdots & u_{l_k}^T \end{bmatrix}=I_k$ then $\det(B_K)=\det\begin{pmatrix} \lambda_1 &  & \\  & \ddots \\  & & \lambda_n \end{pmatrix} = \Pi_{i=1}^n \lambda_i \geq 0$ . Can someone help me for the $\Longleftarrow$ way ?","Let a symmetric matrix. Show that is positive semidefinite all its symmetric minors are , that means for all . (Let where . The matrix is the matrix with , ). is symmetric matrix so by the spectral theorem, we have that with an orthogonal matrix and because A is positive semidefinite. We have that , where is the line of . Since then . Can someone help me for the way ?","A \in \mathbb R^{n \times n} A \iff \geq 0 \det(B_K) \geq 0 K \subseteq \{1,\cdots,n\} K = \{ l_1, \cdots, l_k \} \subseteq \{1,\cdots,n\} 1 \leq l_1 < l_2 < \cdots < l_k \leq n B_K \in \mathbb R^{k \times k} (B_K)_{ij}=A_{l_il_j} 1\leq i,j \leq k \Longrightarrow A A=U\begin{pmatrix}
\lambda_1 &  & \\  & \ddots \\  & & \lambda_n
\end{pmatrix}U^T U \lambda_i \geq 0 \forall i (B_K)=\begin{bmatrix}
u_{l_1} \\
\cdots \\ u_{l_k} 
\end{bmatrix} \begin{pmatrix}
\lambda_1 &  & \\  & \ddots \\  & & \lambda_n
\end{pmatrix} \begin{bmatrix}
u_{l_1}^T &
\cdots & u_{l_k}^T
\end{bmatrix} u_{l_i} l_i^\text{ th} U \begin{bmatrix}
u_{l_1} \\
\cdots \\ u_{l_k} 
\end{bmatrix} \cdot \begin{bmatrix}
u_{l_1}^T &
\cdots & u_{l_k}^T
\end{bmatrix}=I_k \det(B_K)=\det\begin{pmatrix}
\lambda_1 &  & \\  & \ddots \\  & & \lambda_n
\end{pmatrix} = \Pi_{i=1}^n \lambda_i \geq 0 \Longleftarrow","['linear-algebra', 'symmetric-matrices', 'positive-semidefinite', 'orthogonal-matrices']"
37,A $3\times3$ triangular matrix has a repeated eigenvalue if it is the square of a non-triangular matrix,A  triangular matrix has a repeated eigenvalue if it is the square of a non-triangular matrix,3\times3,"After pondering for an interesting answer to a recently asked question , I discovered the following phenomenon. Suppose $A$ is a $3\times3$ matrix whose elements are taken from some field. If $A$ is not upper triangular but $B:=A^2$ is upper triangular, then $B$ has a repeated eigenvalue. Here is a proof outline. Let $A=\pmatrix{a&b&c\\ p&q&r\\ x&y&z}$ . Since $A$ is not upper triangular, $(p,x,y)\ne(0,0,0)$ . The condition that $A^2$ is upper triangular is equivalent to the set of conditions \begin{align} ap+pq+rx&=0,\tag{1}\\ ax+py+xz&=0,\tag{2}\\ bx+qy+yz&=0.\tag{3} \end{align} Suppose both $p$ and $x$ are nonzero. Then $(1)$ and $(3)$ give \begin{equation} \frac{(ap+pq+rx)(a+z)}{p} + \frac{(bx+qy+yz)p}{x} = 0.\tag{4} \end{equation} Substitute $(2)$ into $(4)$ , we obtain $a^2+bp=yr+z^2$ . Hence $B_{11}=B_{33}$ in this case. The other cases where $(p,x,y)$ is $(=0,=0,\ne0),(=0,\ne0,\ne0),\ldots$ etc. can be handled by similar or simpler algebraic manipulations of $(1)-(3)$ , and other equalities between the diagonal elements of $B$ (i.e. $B_{11}=B_{22}$ or $B_{22}=B_{33}$ ) may occur in these cases. However, this proof feels ugly because it uses coordinates too heavily and it has to consider different corner cases separately. Admittedly, proofs of the statement in question probably cannot be entirely coordinate-free, because the assumption that "" $A$ is not triangular"" is already basis dependent, but I still want to see a more conceptual proof that isn't just a series of algebraic manipulations. Any idea? P.S. Since the analogous statement also holds when $A$ is $2\times2$ , I also wonder if it is true for all sizes of $A$ .","After pondering for an interesting answer to a recently asked question , I discovered the following phenomenon. Suppose is a matrix whose elements are taken from some field. If is not upper triangular but is upper triangular, then has a repeated eigenvalue. Here is a proof outline. Let . Since is not upper triangular, . The condition that is upper triangular is equivalent to the set of conditions Suppose both and are nonzero. Then and give Substitute into , we obtain . Hence in this case. The other cases where is etc. can be handled by similar or simpler algebraic manipulations of , and other equalities between the diagonal elements of (i.e. or ) may occur in these cases. However, this proof feels ugly because it uses coordinates too heavily and it has to consider different corner cases separately. Admittedly, proofs of the statement in question probably cannot be entirely coordinate-free, because the assumption that "" is not triangular"" is already basis dependent, but I still want to see a more conceptual proof that isn't just a series of algebraic manipulations. Any idea? P.S. Since the analogous statement also holds when is , I also wonder if it is true for all sizes of .","A 3\times3 A B:=A^2 B A=\pmatrix{a&b&c\\ p&q&r\\ x&y&z} A (p,x,y)\ne(0,0,0) A^2 \begin{align}
ap+pq+rx&=0,\tag{1}\\
ax+py+xz&=0,\tag{2}\\
bx+qy+yz&=0.\tag{3}
\end{align} p x (1) (3) \begin{equation}
\frac{(ap+pq+rx)(a+z)}{p} + \frac{(bx+qy+yz)p}{x} = 0.\tag{4}
\end{equation} (2) (4) a^2+bp=yr+z^2 B_{11}=B_{33} (p,x,y) (=0,=0,\ne0),(=0,\ne0,\ne0),\ldots (1)-(3) B B_{11}=B_{22} B_{22}=B_{33} A A 2\times2 A","['linear-algebra', 'matrices']"
38,Exterior power of lie algebra representation in coordinates,Exterior power of lie algebra representation in coordinates,,"Suppose I have a representation $V = \mathbb{C}^3$ of a group $G$ where elements are of the form $M = \begin{bmatrix}m_1^1 & m_1^2 & m_1^3 \\ m_2^1 & m_2^2 & m_2^3 \\m_3^1 & m_3^2 & m_3^3 \end{bmatrix}$ in some basis $v_1,v_2,v_3$ . If I wish to compute the representation $\Lambda^2 V \subset V \otimes V$ , I could look at the action $g \cdot (v \wedge u) := (gv) \wedge (gu)$ and see that I will get a matrix $N$ of minors of $M$ , i.e. $n_{i,j} = $ minor obtained by removing $i$ th row and $j$ th column. My question is as follows: In the case of a representation of a Lie algebra $\mathfrak{g}$ , is there an analogous interpretation when considering an exterior power of the representation? As an example, using the Lie algebra action $g(v \wedge u) := (gv) \wedge u + v \wedge (gv)$ , with basis $v_1 \wedge v_2$ , $v_1 \wedge v_3$ , and $v_2 \wedge v_3$ , I obtain the following matrix for $\begin{bmatrix}m_1^1+m_2^2 & m_2^3 & -m_1^3 \\ m_3^2 & m_1^1+m_3^3 & m_1^2 \\-m_3^1 & m_2^1 & m_2^2 + m_3^3 \end{bmatrix}$ . Any insight would be appreciated!","Suppose I have a representation of a group where elements are of the form in some basis . If I wish to compute the representation , I could look at the action and see that I will get a matrix of minors of , i.e. minor obtained by removing th row and th column. My question is as follows: In the case of a representation of a Lie algebra , is there an analogous interpretation when considering an exterior power of the representation? As an example, using the Lie algebra action , with basis , , and , I obtain the following matrix for . Any insight would be appreciated!","V = \mathbb{C}^3 G M = \begin{bmatrix}m_1^1 & m_1^2 & m_1^3 \\ m_2^1 & m_2^2 & m_2^3 \\m_3^1 & m_3^2 & m_3^3 \end{bmatrix} v_1,v_2,v_3 \Lambda^2 V \subset V \otimes V g \cdot (v \wedge u) := (gv) \wedge (gu) N M n_{i,j} =  i j \mathfrak{g} g(v \wedge u) := (gv) \wedge u + v \wedge (gv) v_1 \wedge v_2 v_1 \wedge v_3 v_2 \wedge v_3 \begin{bmatrix}m_1^1+m_2^2 & m_2^3 & -m_1^3 \\ m_3^2 & m_1^1+m_3^3 & m_1^2 \\-m_3^1 & m_2^1 & m_2^2 + m_3^3 \end{bmatrix}","['linear-algebra', 'representation-theory', 'lie-algebras']"
39,Do orthogonal projections play a role in diagonalizability?,Do orthogonal projections play a role in diagonalizability?,,"I'm studying Linear Algebra by myself, and the textbook I use is the fourth edition written by Friedberg, Insel, and Spence. For now, I'm trying to get through Section 6.6 that concerns orthogonal projections and the spectral theorem. The following claim embodied in this section really confuses me, and I'm not sure what theorem the authors apply to guarantee diagonalizability of $T$ . Let $V$ be a finite-dimensional inner product space, $W$ be a subspace of $V$ , and $T$ be the orthogonal projection of $V$ on $W$ . We may choose an orthonormal basis $\beta=\{v_1,\ldots,v_n\}$ for $V$ so that $\{v_1,\ldots,v_k\}$ is a basis for $W$ . Then $[T]_\beta$ is a diagonal matrix with $1$ 's as the first $k$ diagonal entries and $0$ 's elsewhere. I have no doubt about the existence of $\beta$ ; in fact, this can be guaranteed by Theorem 6.7. However, I don't know why the authors are confident to say that $[T]_\beta$ is diagonal. They even tell me the explicit form of this matrix representation. Does anyone have an idea? Thank you so much.","I'm studying Linear Algebra by myself, and the textbook I use is the fourth edition written by Friedberg, Insel, and Spence. For now, I'm trying to get through Section 6.6 that concerns orthogonal projections and the spectral theorem. The following claim embodied in this section really confuses me, and I'm not sure what theorem the authors apply to guarantee diagonalizability of . Let be a finite-dimensional inner product space, be a subspace of , and be the orthogonal projection of on . We may choose an orthonormal basis for so that is a basis for . Then is a diagonal matrix with 's as the first diagonal entries and 's elsewhere. I have no doubt about the existence of ; in fact, this can be guaranteed by Theorem 6.7. However, I don't know why the authors are confident to say that is diagonal. They even tell me the explicit form of this matrix representation. Does anyone have an idea? Thank you so much.","T V W V T V W \beta=\{v_1,\ldots,v_n\} V \{v_1,\ldots,v_k\} W [T]_\beta 1 k 0 \beta [T]_\beta","['linear-algebra', 'matrices', 'projection', 'projection-matrices']"
40,$\det(I+A)=1+\operatorname{Tr}(A)$ if $\operatorname{rank}(A)=1$,if,\det(I+A)=1+\operatorname{Tr}(A) \operatorname{rank}(A)=1,"Let $A$ be a complex matrix of rank $1$ . Show that $$\det (I+A) = 1 + \operatorname{Tr}(A)$$ where $\det(X)$ denotes the determinant of $X$ and $\operatorname{Tr}(X)$ denotes the trace of $X$ . Any hint, please. I do not get how to combine the ideas of rank, determinant and trace. Thank you.","Let be a complex matrix of rank . Show that where denotes the determinant of and denotes the trace of . Any hint, please. I do not get how to combine the ideas of rank, determinant and trace. Thank you.",A 1 \det (I+A) = 1 + \operatorname{Tr}(A) \det(X) X \operatorname{Tr}(X) X,"['linear-algebra', 'matrices']"
41,Fuglede's theorem in finite-dimensional vector space,Fuglede's theorem in finite-dimensional vector space,,"Let $V$ be a finite dimensional vector space and $A$ be normal operator on $V$ and $B$ is an operator such that $AB=BA$ . Show that $BA^*=A^*B$ . I guess that this problem should not be so difficult. I have tried different approaches and I got some identities which do not lead to desired equality. So I would be thankful if you show the solution to this problem, please!","Let be a finite dimensional vector space and be normal operator on and is an operator such that . Show that . I guess that this problem should not be so difficult. I have tried different approaches and I got some identities which do not lead to desired equality. So I would be thankful if you show the solution to this problem, please!",V A V B AB=BA BA^*=A^*B,['linear-algebra']
42,The Biggest Step Size with Guaranteed Convergence for Constant Step Size Gradient Descent of a Convex Function with Lipschitz Continuous Gradient,The Biggest Step Size with Guaranteed Convergence for Constant Step Size Gradient Descent of a Convex Function with Lipschitz Continuous Gradient,,"Given a convex function $ f \left( x \right) : \mathbb{R}^{n} \to \mathbb{R} $ with $ L $ - Lipschitz Continuous Gradient. Namely: $$ {\left\| \nabla f \left( x \right) - \nabla f \left( y \right) \right\|}_{2} \leq L {\left\| x - y \right\|}_{2} $$ What is the largest constant step size, $ \alpha $ , one could use in Gradient Descent to minimize the function? In most literature I see $ \alpha = \frac{1}{L} $ yet in some other cases I see $ \alpha = \frac{2}{L} $ . Which one is right? Also, for the case $ f \left( x \right) = \frac{1}{2} {\left\| A x - b \right\|}_{2}^{2} $ what is $ L $ ? Is it the largest Singular Value of $ A $ ?","Given a convex function with - Lipschitz Continuous Gradient. Namely: What is the largest constant step size, , one could use in Gradient Descent to minimize the function? In most literature I see yet in some other cases I see . Which one is right? Also, for the case what is ? Is it the largest Singular Value of ?", f \left( x \right) : \mathbb{R}^{n} \to \mathbb{R}   L   {\left\| \nabla f \left( x \right) - \nabla f \left( y \right) \right\|}_{2} \leq L {\left\| x - y \right\|}_{2}   \alpha   \alpha = \frac{1}{L}   \alpha = \frac{2}{L}   f \left( x \right) = \frac{1}{2} {\left\| A x - b \right\|}_{2}^{2}   L   A ,"['linear-algebra', 'convex-analysis', 'convex-optimization', 'gradient-descent']"
43,$X$ and $Y$ are $n\times n$ matrices such that rank of $X-Y$ is $1$.,and  are  matrices such that rank of  is .,X Y n\times n X-Y 1,"Suppose that $X$ and $Y$ are $n\times n$ complex matrices such that $2Y^2=XY-YX$ and the rank of $X-Y$ is $1$ . Prove that $Y^3=YXY$ . I have shown that, trace $Y^n=0$ for all $n=2,3,4,\ldots$ . Also we can write $X-Y=xy^t$ . From this how to proceed. Please give some idea.","Suppose that and are complex matrices such that and the rank of is . Prove that . I have shown that, trace for all . Also we can write . From this how to proceed. Please give some idea.","X Y n\times n 2Y^2=XY-YX X-Y 1 Y^3=YXY Y^n=0 n=2,3,4,\ldots X-Y=xy^t","['linear-algebra', 'matrix-rank']"
44,When are the eigenvalues of a matrix containing all squared elements irrational/rational?,When are the eigenvalues of a matrix containing all squared elements irrational/rational?,,"Forgive me in advance if any of this is trivial. After looking at many 2x2 matrices it seems that if all of the elements in matrix are unique squared integers then the eigenvalues are irrational. So I tried to investigate this: $\det \begin{pmatrix} \lambda -a^2 & b^2\\ c^2& \lambda -d^2\end{pmatrix}= \lambda^2 -(a^2+d^2)\lambda + (a^2d^2-c^2b^2)$ after applying the quadratic formula this gives a radical of, $\sqrt{a^4+4b^2c^2-2a^2d^2+d^4}$ If the stated observation is true, is there a way to show that this is irrational? Furthermore it looks like on the surface that for 3x3 matrices the eigenvalues for a matrix containing all  unique squared entries that the eigenvalues will also be irrational. Are either of these statements true? Is there a generalization of this for an nxn matrix? Edit: I'm not entirely sure I derived the radical correctly, but I'd still like to have some direction on the questions above also I'd like to examine cases where the eigenvalue is not zero Example: \begin{pmatrix} 2^2 & 4^2\\  3^2 & 6^2 \end{pmatrix} has eigenvalues 40 and 0. Edit 2: still looking for rational eigenvalues of a $3x3$ have been with imposed restrictions and nonzero eigenvalues/entries.","Forgive me in advance if any of this is trivial. After looking at many 2x2 matrices it seems that if all of the elements in matrix are unique squared integers then the eigenvalues are irrational. So I tried to investigate this: after applying the quadratic formula this gives a radical of, If the stated observation is true, is there a way to show that this is irrational? Furthermore it looks like on the surface that for 3x3 matrices the eigenvalues for a matrix containing all  unique squared entries that the eigenvalues will also be irrational. Are either of these statements true? Is there a generalization of this for an nxn matrix? Edit: I'm not entirely sure I derived the radical correctly, but I'd still like to have some direction on the questions above also I'd like to examine cases where the eigenvalue is not zero Example: has eigenvalues 40 and 0. Edit 2: still looking for rational eigenvalues of a have been with imposed restrictions and nonzero eigenvalues/entries.","\det \begin{pmatrix} \lambda -a^2 & b^2\\ c^2& \lambda -d^2\end{pmatrix}= \lambda^2 -(a^2+d^2)\lambda + (a^2d^2-c^2b^2) \sqrt{a^4+4b^2c^2-2a^2d^2+d^4} \begin{pmatrix}
2^2 & 4^2\\ 
3^2 & 6^2
\end{pmatrix} 3x3","['linear-algebra', 'matrices', 'number-theory']"
45,There does not exist a onto ring homomorphism from $M_{n+1 \times n+1}(\mathbb F) \to M_{n \times n}(\mathbb F) $ for any field $\mathbb F.$,There does not exist a onto ring homomorphism from  for any field,M_{n+1 \times n+1}(\mathbb F) \to M_{n \times n}(\mathbb F)  \mathbb F.,"For a positive integer $n$ , I have to show that there does not exist a onto ring homomorphism from $M_{n+1 \times n+1}(\mathbb F) \to M_{n \times n}(\mathbb F) $ for any field $\mathbb F.$ If it exists lets say there is $f:M_{n+1 \times n+1}(\mathbb F) \to M_{n \times n}(\mathbb F)$ surjection and since $1$ maps to $1,$ its kernel is trivial and hence an isomorphism. Now if I can show that $f$ is $\mathbb F$ - linear then we are done by dimension argument. To show that $f$ is $\mathbb F$ -linear enough to show that $f(cI_{n+1})=cI_n$ for each $c \in \mathbb F,$ which I am not able to show. Or may be there is some alternative way to prove it. I need some help to show it.","For a positive integer , I have to show that there does not exist a onto ring homomorphism from for any field If it exists lets say there is surjection and since maps to its kernel is trivial and hence an isomorphism. Now if I can show that is - linear then we are done by dimension argument. To show that is -linear enough to show that for each which I am not able to show. Or may be there is some alternative way to prove it. I need some help to show it.","n M_{n+1 \times n+1}(\mathbb F) \to M_{n \times n}(\mathbb F)  \mathbb F. f:M_{n+1 \times n+1}(\mathbb F) \to M_{n \times n}(\mathbb F) 1 1, f \mathbb F f \mathbb F f(cI_{n+1})=cI_n c \in \mathbb F,","['linear-algebra', 'matrices', 'ring-theory', 'field-theory', 'linear-transformations']"
46,"Simplifying the determinant of the matrix whose $(i,j)$-th entry is $b_i c_j$ for $i=j$ and $-b_i c_j$ for $i\neq j$",Simplifying the determinant of the matrix whose -th entry is  for  and  for,"(i,j) b_i c_j i=j -b_i c_j i\neq j","$A$ is a $n \times n$ real matrix. $A_{ij} = \begin{cases} \phantom{-}b_{i}c_{j} & \text{if } i = j \\  -b_{i}c_{j} & \text{if } i \ne j  \end{cases}$ How to simplify $\det(A)$ ? Update: Can I simplify the determinant with elementary row and column operations as described at http://www.maths.nuigalway.ie/~rquinlan/MA203/section2-5.pdf ? Divide each row by $b_{i}$ (elementary row operation) Divide each column by $c_{j}$ (elementary column operation) $\det(A) = \left(\prod_{i=1}^{n} b_{i} \right) \left(\prod_{j=1}^{n} c_{j} \right) \det(S)$ where $S_{ij} = \begin{cases} + 1 & \text{if } i = j \\  -1 & \text{if } i \ne j  \end{cases}$ So, the problem reduces to finding $\det(S)$ .","is a real matrix. How to simplify ? Update: Can I simplify the determinant with elementary row and column operations as described at http://www.maths.nuigalway.ie/~rquinlan/MA203/section2-5.pdf ? Divide each row by (elementary row operation) Divide each column by (elementary column operation) where So, the problem reduces to finding .","A n \times n A_{ij} = \begin{cases} \phantom{-}b_{i}c_{j} & \text{if } i = j \\ 
-b_{i}c_{j} & \text{if } i \ne j  \end{cases} \det(A) b_{i} c_{j} \det(A) = \left(\prod_{i=1}^{n} b_{i} \right) \left(\prod_{j=1}^{n} c_{j} \right) \det(S) S_{ij} = \begin{cases} + 1 & \text{if } i = j \\ 
-1 & \text{if } i \ne j  \end{cases} \det(S)","['linear-algebra', 'determinant']"
47,Find Jordan Decomposition of $\left(\begin{smallmatrix} 4 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 3 \end{smallmatrix}\right)$ over $\mathbb{F}_5$,Find Jordan Decomposition of  over,\left(\begin{smallmatrix} 4 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 3 \end{smallmatrix}\right) \mathbb{F}_5,"Find the Jordan decomposition of $$ A := \begin{pmatrix} 4 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 3 \end{pmatrix} \in M_3(\mathbb{F}_5), $$ where $\mathbb{F}_5$ is the field modulo 5. What I've done so far The characteristic polynomial is \begin{equation} P_A(t) = (4 - t)(1-t)(3-t) - (1-t) = -t^3 + 8t^2-18t+1 \equiv 4t^3 + 3t^2 + 2t + 1 \mod5. \end{equation} Therefore, $\lambda = 1$ is a zero of $P_A$ , since $4+3+2+1 = 10 \equiv 0 \mod 5$ . By polynomial division one obtains $$ P_A(t) = (t + 4)(4t^2 + 2t + 4) = (t + 4)(t + 4) (4t + 1) \equiv 4 (t + 4)^3 $$ Therefore $\lambda = 1$ is the only eigenvalue of $A$ . To find the eigenspace we calculate the kernel of $A + 4 E_3$ and obtain $$ \text{span}\left( \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix}, \begin{pmatrix} 1 \\ 1 \\ 2 \end{pmatrix} \right) $$ Since $(A + 4 E_3)^2 = 0$ , the kernel of $(A + 4 E_3)^2$ is the whole space. Now, I choose $v := (1, 0, 0) \in \text{ker}(A + 4 E_3)^2$ such that $v \not\in \text{ker}(A + 4 E_3)$ . We calculate $(A + 4E)v = (3, 0, 1)$ and then $$ (A + 4E) \begin{pmatrix} 3 \\ 0 \\ 1\end{pmatrix} = \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix}, $$ but the zero vector can't be a basis vector of our Jordan decomposition. Have I made a mistake in my calculations?","Find the Jordan decomposition of where is the field modulo 5. What I've done so far The characteristic polynomial is Therefore, is a zero of , since . By polynomial division one obtains Therefore is the only eigenvalue of . To find the eigenspace we calculate the kernel of and obtain Since , the kernel of is the whole space. Now, I choose such that . We calculate and then but the zero vector can't be a basis vector of our Jordan decomposition. Have I made a mistake in my calculations?","
A := \begin{pmatrix} 4 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 3 \end{pmatrix}
\in M_3(\mathbb{F}_5),
 \mathbb{F}_5 \begin{equation}
P_A(t) = (4 - t)(1-t)(3-t) - (1-t)
= -t^3 + 8t^2-18t+1
\equiv 4t^3 + 3t^2 + 2t + 1 \mod5.
\end{equation} \lambda = 1 P_A 4+3+2+1 = 10 \equiv 0 \mod 5 
P_A(t)
= (t + 4)(4t^2 + 2t + 4)
= (t + 4)(t + 4) (4t + 1)
\equiv 4 (t + 4)^3
 \lambda = 1 A A + 4 E_3 
\text{span}\left( \begin{pmatrix} 1 \\ 0 \\ 2 \end{pmatrix}, \begin{pmatrix} 1 \\ 1 \\ 2 \end{pmatrix} \right)
 (A + 4 E_3)^2 = 0 (A + 4 E_3)^2 v := (1, 0, 0) \in \text{ker}(A + 4 E_3)^2 v \not\in \text{ker}(A + 4 E_3) (A + 4E)v = (3, 0, 1) 
(A + 4E)
\begin{pmatrix} 3 \\ 0 \\ 1\end{pmatrix}
= \begin{pmatrix} 0 \\ 0 \\ 0 \end{pmatrix},
","['linear-algebra', 'finite-fields', 'jordan-normal-form']"
48,How to find the determinant of this matrix? (A spherical-Cartesian transformation Jacobian matrix),How to find the determinant of this matrix? (A spherical-Cartesian transformation Jacobian matrix),,"I meet a difficult determinant question as the followings: $$ \text{Matrix A is given as:} $$ $$ A=\begin{bmatrix}\frac{\partial x}{\partial r}&\frac{\partial x}{\partial\theta}&\frac{\partial x}{\partial\phi}\\\frac{\partial y}{\partial r}&\frac{\partial y}{\partial\theta}&\frac{\partial y}{\partial\phi}\\\frac{\partial z}{\partial r}&\frac{\partial z}{\partial\theta}&\frac{\partial z}{\partial\phi}\end{bmatrix} $$ $$ \text{where }x=r\sin\theta\cos\phi\text{, }y=r\sin\theta\sin\phi\text{, and }z=r\cos\theta.\text{ Find determinants }\det{(A)}\text{, }\det{(A^{-1})}\text{, and }\det{(A^2)}. $$ I tried to simplify it, but just got: $$ A=\begin{bmatrix}\sin\theta\cos\phi&r\cos\theta\cos\phi&-r\sin\theta\sin\phi\\\sin\theta\sin\phi&r\cos\theta\sin\phi&r\sin\theta\cos\phi\\\cos\theta&-r\sin\theta&0\end{bmatrix} $$ Because it is wired, I have also searched the Internet. But till now all I know is that this is just a spherical-Cartesian transformation formula using Jacobian matrix. (Maybe we can make a breakthough here?) I can only solve $\det{(A)}$ by directly calculating it, $\det{(A)}=r^2\sin\theta$ . However, I think it is still hard to find the inverse matrix, needless to say the huge calculation to get $A^2$ . As I think, there must be some ways to simplify it. Could anyone kindly teach me that whether there is any way to simplify $A$ , so as to calculate the determinant?Thank you!","I meet a difficult determinant question as the followings: I tried to simplify it, but just got: Because it is wired, I have also searched the Internet. But till now all I know is that this is just a spherical-Cartesian transformation formula using Jacobian matrix. (Maybe we can make a breakthough here?) I can only solve by directly calculating it, . However, I think it is still hard to find the inverse matrix, needless to say the huge calculation to get . As I think, there must be some ways to simplify it. Could anyone kindly teach me that whether there is any way to simplify , so as to calculate the determinant?Thank you!","
\text{Matrix A is given as:}
 
A=\begin{bmatrix}\frac{\partial x}{\partial r}&\frac{\partial x}{\partial\theta}&\frac{\partial x}{\partial\phi}\\\frac{\partial y}{\partial r}&\frac{\partial y}{\partial\theta}&\frac{\partial y}{\partial\phi}\\\frac{\partial z}{\partial r}&\frac{\partial z}{\partial\theta}&\frac{\partial z}{\partial\phi}\end{bmatrix}
 
\text{where }x=r\sin\theta\cos\phi\text{, }y=r\sin\theta\sin\phi\text{, and }z=r\cos\theta.\text{ Find determinants }\det{(A)}\text{, }\det{(A^{-1})}\text{, and }\det{(A^2)}.
 
A=\begin{bmatrix}\sin\theta\cos\phi&r\cos\theta\cos\phi&-r\sin\theta\sin\phi\\\sin\theta\sin\phi&r\cos\theta\sin\phi&r\sin\theta\cos\phi\\\cos\theta&-r\sin\theta&0\end{bmatrix}
 \det{(A)} \det{(A)}=r^2\sin\theta A^2 A","['linear-algebra', 'matrices', 'determinant']"
49,Matrices with $A^2+B^2=2AB$,Matrices with,A^2+B^2=2AB,"Let $A, B \in M_3(\mathbb{C})$ so that $$A^2+B^2=2AB.$$ Prove that $$\det(A+B) ^2=8\det(A^2+B^2).$$ My work: Let $A=X+Y$ , $B=X-Y$ with $X, Y \in M_3(\mathbb{C})$ . The condition rewrites as $-2Y^2=[X,Y]$ . Since $Y$ commutes with $Y^2$ it will also commute with $[X, Y] $ , so according to Jacobson's lemma $[X, Y] $ is nilpotent. I am not sure if this helps, but using my notations the conclusion is equivalent to $\det(X^2)=\det(X^2+Y^2)$ and we also have $\det Y=0$ .","Let so that Prove that My work: Let , with . The condition rewrites as . Since commutes with it will also commute with , so according to Jacobson's lemma is nilpotent. I am not sure if this helps, but using my notations the conclusion is equivalent to and we also have .","A, B \in M_3(\mathbb{C}) A^2+B^2=2AB. \det(A+B) ^2=8\det(A^2+B^2). A=X+Y B=X-Y X, Y \in M_3(\mathbb{C}) -2Y^2=[X,Y] Y Y^2 [X, Y]  [X, Y]  \det(X^2)=\det(X^2+Y^2) \det Y=0","['linear-algebra', 'matrices', 'determinant']"
50,Infinity matrix norm is maximum row sum norm,Infinity matrix norm is maximum row sum norm,,I want to prove that the infinity matrix norm is maximum row sum norm. I've shown that for $\|x\|_{\infty}=1$ $$||Ax||_{\infty} =  \max_{i}\left|\sum^n_{j=1}a_{ij}x_j \right| \leq \max_{i}\sum^{n}_{j=1} |a_{ij}|\|x\|_{\infty}= \max_{i}\sum^{n}_{j=1} |a_{ij}|.$$ Now I need to show that there exists vector $x$ with $\|x\|_{\infty}=1$ for which this inequality becomes equality. And I'm stuck here. How do I proceed? What is the correct $x$ ?,I want to prove that the infinity matrix norm is maximum row sum norm. I've shown that for Now I need to show that there exists vector with for which this inequality becomes equality. And I'm stuck here. How do I proceed? What is the correct ?,\|x\|_{\infty}=1 ||Ax||_{\infty} =  \max_{i}\left|\sum^n_{j=1}a_{ij}x_j \right| \leq \max_{i}\sum^{n}_{j=1} |a_{ij}|\|x\|_{\infty}= \max_{i}\sum^{n}_{j=1} |a_{ij}|. x \|x\|_{\infty}=1 x,['linear-algebra']
51,Is the cross product a tensor or pseudotensor?,Is the cross product a tensor or pseudotensor?,,I am confused as to the tensor nature of the cross product. Maybe I'm misunderstanding what I'm reading. But I seem to be getting conflicting messages.,I am confused as to the tensor nature of the cross product. Maybe I'm misunderstanding what I'm reading. But I seem to be getting conflicting messages.,,"['linear-algebra', 'tensors', 'cross-product']"
52,$A+3I$ is nonsingular,is nonsingular,A+3I,"If $A^2=A+2I$, show that $A+3I$ is nonsingular. If $p$ is a polynomial that annihilates A, how does the minimum poly of A and $p$ relate? Can someone help me with this? I’m not quite familiar with annihilating polynomial.","If $A^2=A+2I$, show that $A+3I$ is nonsingular. If $p$ is a polynomial that annihilates A, how does the minimum poly of A and $p$ relate? Can someone help me with this? I’m not quite familiar with annihilating polynomial.",,"['linear-algebra', 'inverse', 'minimal-polynomials']"
53,To find the smallest value of 'k' for the following equation,To find the smallest value of 'k' for the following equation,,"Let $\mathrm a,b$ are positive real numbers such that for $\mathrm a - b = 10$, then the smallest value of the constant $\mathrm k$ for which $\mathrm {\sqrt {x^2 + ax}} - {\sqrt{x^2 + bx}} < k$ for all $\mathrm x>0$, is? I don't get how to approach this problem. Any help would be appreciated.","Let $\mathrm a,b$ are positive real numbers such that for $\mathrm a - b = 10$, then the smallest value of the constant $\mathrm k$ for which $\mathrm {\sqrt {x^2 + ax}} - {\sqrt{x^2 + bx}} < k$ for all $\mathrm x>0$, is? I don't get how to approach this problem. Any help would be appreciated.",,"['linear-algebra', 'functions']"
54,$2n+1$ Subsets whose cardinality is a multiples of 6,Subsets whose cardinality is a multiples of 6,2n+1,"Consider $2n+1$ subsets $S_1,S_2,...,S_{2n+1}$ of $\{ 1,2,...,n\}$. Can anybody show (or provide a counterexample) that if $|S_i|$ is not a multiple of 6 for all $i\in \{ 1,2,...,2n+1\}$, then there are $i\neq j\in \{ 1,2,...,2n+1\}$ such that $|S_i\cap S_j|$ is not a multiple of 6? I found it on a forum without any answers and I believe there are no counterexample. Any help or reference would be appreciated.","Consider $2n+1$ subsets $S_1,S_2,...,S_{2n+1}$ of $\{ 1,2,...,n\}$. Can anybody show (or provide a counterexample) that if $|S_i|$ is not a multiple of 6 for all $i\in \{ 1,2,...,2n+1\}$, then there are $i\neq j\in \{ 1,2,...,2n+1\}$ such that $|S_i\cap S_j|$ is not a multiple of 6? I found it on a forum without any answers and I believe there are no counterexample. Any help or reference would be appreciated.",,"['linear-algebra', 'combinatorics', 'elementary-set-theory', 'reference-request']"
55,What is the 'Algebraic' Dimension of $l^2(\mathbb{N})$?,What is the 'Algebraic' Dimension of ?,l^2(\mathbb{N}),"Let  $l^2(\mathbb{N})$ be the space of all complex sequences that are square-summable. Clearly this is a Hilbert space, and it has a maximal orthonormal system which is equipotent to $\mathbb{N}$. Hence, its 'Hilbert dimension' is $\aleph_0$ (please tell me if other termiology is used). But what is the 'algebraic' dimension of this space? That is, what is the cardinality of its Hamel basis? Is there a way to compute it using only cardinal arithmetic, or does one have to know much more about its algebraic structure? I was reading this article explaining Halmos' counterexample of an inner product space NOT having any complete, orthonormal subset. It is stated there (implicitly, I guess) that the algebraic dimension of $l^2(\mathbb{N})$ is the cardinality of the continuum, but I don't see why that has to be the case; this seems highly non-trivial to me. Could someone please explain this to me?","Let  $l^2(\mathbb{N})$ be the space of all complex sequences that are square-summable. Clearly this is a Hilbert space, and it has a maximal orthonormal system which is equipotent to $\mathbb{N}$. Hence, its 'Hilbert dimension' is $\aleph_0$ (please tell me if other termiology is used). But what is the 'algebraic' dimension of this space? That is, what is the cardinality of its Hamel basis? Is there a way to compute it using only cardinal arithmetic, or does one have to know much more about its algebraic structure? I was reading this article explaining Halmos' counterexample of an inner product space NOT having any complete, orthonormal subset. It is stated there (implicitly, I guess) that the algebraic dimension of $l^2(\mathbb{N})$ is the cardinality of the continuum, but I don't see why that has to be the case; this seems highly non-trivial to me. Could someone please explain this to me?",,"['linear-algebra', 'hilbert-spaces', 'inner-products', 'cardinals']"
56,Relation of trace and determinant,Relation of trace and determinant,,"Let $A$ be a $3\times3$ non-diagonal matrix with $A=A^{-1}$ . Prove that $\det A = \operatorname{tr} A =\pm1$ I have already proved $\det A = \pm1$ , but I have no idea about how to proceed with $\operatorname{tr} A$ . Thank you.","Let be a non-diagonal matrix with . Prove that I have already proved , but I have no idea about how to proceed with . Thank you.",A 3\times3 A=A^{-1} \det A = \operatorname{tr} A =\pm1 \det A = \pm1 \operatorname{tr} A,"['linear-algebra', 'matrices', 'proof-writing', 'determinant', 'trace']"
57,Determinant-free proof that a real $n \times n$ matrix has at least one real eigenvalue when $n$ is odd.,Determinant-free proof that a real  matrix has at least one real eigenvalue when  is odd.,n \times n n,"Is there a determinant-free proof that $\textbf{A} \in \mathbb{R}^{n \times n}$ must have at least one real eigenvalue when $n$ is odd? I have seen a few other definitions of ""the set of eigenvalues"" which do not invoke the determinant, specifically the complement of the resolvent set , or the set of points for which $\lambda\textbf{I}_{n \times n} - \textbf{A}$ is singular.","Is there a determinant-free proof that $\textbf{A} \in \mathbb{R}^{n \times n}$ must have at least one real eigenvalue when $n$ is odd? I have seen a few other definitions of ""the set of eigenvalues"" which do not invoke the determinant, specifically the complement of the resolvent set , or the set of points for which $\lambda\textbf{I}_{n \times n} - \textbf{A}$ is singular.",,"['linear-algebra', 'matrices']"
58,Tensor product and the pure elements,Tensor product and the pure elements,,"From my understanding, one important thing about tensor product is that not all elements are pure elements, that is in $M\otimes N$ not all elements are in the form of $m\otimes n$ ,they are linear combinations of pure elements. My professor mentioned that while doing a problem thinking the tensor product only contains pure elements is a very common mistake. But I don't quite see the importance of this, if we can check some property $P$ for all pure elements, then shouldn't it hold for $M\otimes N$? For example, if $m\otimes n = 0$ for each $m\in M, n\in N$, then $M\otimes N = 0$. Can you give me an example where checking all pure elements is not good enough for $M\otimes N$.","From my understanding, one important thing about tensor product is that not all elements are pure elements, that is in $M\otimes N$ not all elements are in the form of $m\otimes n$ ,they are linear combinations of pure elements. My professor mentioned that while doing a problem thinking the tensor product only contains pure elements is a very common mistake. But I don't quite see the importance of this, if we can check some property $P$ for all pure elements, then shouldn't it hold for $M\otimes N$? For example, if $m\otimes n = 0$ for each $m\in M, n\in N$, then $M\otimes N = 0$. Can you give me an example where checking all pure elements is not good enough for $M\otimes N$.",,"['linear-algebra', 'abstract-algebra', 'tensor-products']"
59,"Prove that $v, Tv, T^2v, ... , T^{m-1}v$ is linearly independent",Prove that  is linearly independent,"v, Tv, T^2v, ... , T^{m-1}v","Suppose $T$ is in $L(V)$ , $m$ is a positive integer, and $v$ in vector space $V$ is such that $(T^{m-1})v \neq 0$ , and $(T^m)v = 0$ . Prove that $[v, Tv, T^2v, ... , T^{m-1}v]$ is linearly independent I get that $(T^j)v \neq 0\ \forall\ j < m$ . Additionally, since $T$ is nilpotent, $V$ has a basis with respect to which the matrix of $T$ has $0$ s on and below the diagonal. However, I'm not sure if these can be used to show linear independence or if they're even relevant to the problem at all. Any help is appreciated!","Suppose is in , is a positive integer, and in vector space is such that , and . Prove that is linearly independent I get that . Additionally, since is nilpotent, has a basis with respect to which the matrix of has s on and below the diagonal. However, I'm not sure if these can be used to show linear independence or if they're even relevant to the problem at all. Any help is appreciated!","T L(V) m v V (T^{m-1})v \neq 0 (T^m)v = 0 [v, Tv, T^2v, ... , T^{m-1}v] (T^j)v \neq 0\ \forall\ j < m T V T 0","['linear-algebra', 'matrices', 'linear-transformations']"
60,complex conjugate of trace of matrices,complex conjugate of trace of matrices,,Is $[Tr(AB)]^{*} = Tr(AB)^{\dagger} = Tr(B^{\dagger}A^{\dagger})$? The second equal sign is trivial but the first equal sign is what I am puzzled about. $*$ means the complex conjugate of a number. $\dagger$ means the complex conjugate of a matrix.,Is $[Tr(AB)]^{*} = Tr(AB)^{\dagger} = Tr(B^{\dagger}A^{\dagger})$? The second equal sign is trivial but the first equal sign is what I am puzzled about. $*$ means the complex conjugate of a number. $\dagger$ means the complex conjugate of a matrix.,,"['linear-algebra', 'matrices']"
61,are there any matrices that act like the identity matrix but are totally different?,are there any matrices that act like the identity matrix but are totally different?,,"Given any matrix $A$ or vector $v$, can you find a matrix $B \neq I$ and also not resembling $I$ (having entries in places other than diagonal) such that $B*A$ and $B*v$ are approximately $A$ and approximately $v$? For me the answer is should obviously be yes.  Also I am aware that $I$ is unique, so the matrix $B$ will never have the same effect as $I$ unless it is equal to $I$ itself.","Given any matrix $A$ or vector $v$, can you find a matrix $B \neq I$ and also not resembling $I$ (having entries in places other than diagonal) such that $B*A$ and $B*v$ are approximately $A$ and approximately $v$? For me the answer is should obviously be yes.  Also I am aware that $I$ is unique, so the matrix $B$ will never have the same effect as $I$ unless it is equal to $I$ itself.",,['linear-algebra']
62,Left/Right Eigenvectors,Left/Right Eigenvectors,,"Let $M$ be a nonsymmetric matrix; suppose the columns of matrix $A$ are the right eigenvectors of $M$ and the rows of matrix $B$ are the left eigenvectors of $M$. In one of the answers to a question on left and right eigenvectors it was claimed that $AB=I$. Is that true, and how would you prove it?","Let $M$ be a nonsymmetric matrix; suppose the columns of matrix $A$ are the right eigenvectors of $M$ and the rows of matrix $B$ are the left eigenvectors of $M$. In one of the answers to a question on left and right eigenvectors it was claimed that $AB=I$. Is that true, and how would you prove it?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant', 'inverse']"
63,Meaning of row space,Meaning of row space,,"I used to think of the row space of an $m \times n$ matrix $A$ as the column space of $A^T$ , and therefore the row vectors are the images of the standard dual basis of $\mathbb{R}^m$ under $A^T$ . But it seems that we can have an interpretation of the row space without introducing the dual space. I think, but am not sure, that the row spaces are those vectors which are 1-1 mapped to vectors in the column space. Is this correct?","I used to think of the row space of an matrix as the column space of , and therefore the row vectors are the images of the standard dual basis of under . But it seems that we can have an interpretation of the row space without introducing the dual space. I think, but am not sure, that the row spaces are those vectors which are 1-1 mapped to vectors in the column space. Is this correct?",m \times n A A^T \mathbb{R}^m A^T,"['linear-algebra', 'matrices', 'vector-spaces']"
64,Find best-fit parabola to the given data,Find best-fit parabola to the given data,,"Find the parabola $At^2+Bt+C$ that best approximates the data set $t= -1,0,1,2,3$ and $b(t) = 5,2,1,2,5$. Would I be using least squares such that $x = (A^TA)^{-1}A^Tb$? Thank you in advance.","Find the parabola $At^2+Bt+C$ that best approximates the data set $t= -1,0,1,2,3$ and $b(t) = 5,2,1,2,5$. Would I be using least squares such that $x = (A^TA)^{-1}A^Tb$? Thank you in advance.",,"['linear-algebra', 'regression', 'least-squares']"
65,"Are $\sin(n\pi x /L)$ a basis of $L^2[0,L]$?",Are  a basis of ?,"\sin(n\pi x /L) L^2[0,L]","I'm studying a book ""Applied linear algebra"" by Sadun. The book deals linear algebra and functional analysis somewhat informally. Here is a quote from the book. I have some questions about the italicized sentences (italicizing is mine, not of the author). What we have done is exhibit three orthogonal bases for the same space ($L^2[0,L]$). One orthogonal basis is the set of functions $\sin(n \pi x/L)$ . A second orthogonal basis is the set of functions $\sin(2n\pi x/L)$ together with the functions $\cos(2n\pi x/L)$. Closely related to the second basis is the third basis $\{\exp(2n\pi xi /L)\}$ with $n$ now ranging from $-\infty$ to $\infty$. These bases are in turn obtained as the eigenfunctions of the three different operators on $L^2[0,L]$. The first operator is $d^2/dx^2$ with Dirichlet boundary conditions, whose eigenvalues are $-n^2\pi^2/L^2$, and whose eigenfunctions are the functions $\sin(n\pi x/L)$ $\cdots$ i) Are $\sin(n\pi x/L)$ really a basis for $L^2[0,L]$? How can a function whose value is nonzero at the boundaries be represented as a linear combination of them? ii) I'm not comfortable with defining an operator with boundary conditions. Are such definitions of an operator with boundary conditions conventional or usual? I think it is more appropriate to define a subspace by the boundary condition and define an operator on the subspace. Then $\sin(n\pi x/L)$ can be a basis for the subspace in which functions vanish at the boundaries. I'm not much familiar with rigorous functional analysis nor linear algebra, but the sentences in the book are somewhat wiered for me.","I'm studying a book ""Applied linear algebra"" by Sadun. The book deals linear algebra and functional analysis somewhat informally. Here is a quote from the book. I have some questions about the italicized sentences (italicizing is mine, not of the author). What we have done is exhibit three orthogonal bases for the same space ($L^2[0,L]$). One orthogonal basis is the set of functions $\sin(n \pi x/L)$ . A second orthogonal basis is the set of functions $\sin(2n\pi x/L)$ together with the functions $\cos(2n\pi x/L)$. Closely related to the second basis is the third basis $\{\exp(2n\pi xi /L)\}$ with $n$ now ranging from $-\infty$ to $\infty$. These bases are in turn obtained as the eigenfunctions of the three different operators on $L^2[0,L]$. The first operator is $d^2/dx^2$ with Dirichlet boundary conditions, whose eigenvalues are $-n^2\pi^2/L^2$, and whose eigenfunctions are the functions $\sin(n\pi x/L)$ $\cdots$ i) Are $\sin(n\pi x/L)$ really a basis for $L^2[0,L]$? How can a function whose value is nonzero at the boundaries be represented as a linear combination of them? ii) I'm not comfortable with defining an operator with boundary conditions. Are such definitions of an operator with boundary conditions conventional or usual? I think it is more appropriate to define a subspace by the boundary condition and define an operator on the subspace. Then $\sin(n\pi x/L)$ can be a basis for the subspace in which functions vanish at the boundaries. I'm not much familiar with rigorous functional analysis nor linear algebra, but the sentences in the book are somewhat wiered for me.",,"['linear-algebra', 'functional-analysis', 'hilbert-spaces', 'self-learning']"
66,Decompose rotation matrix to plane of rotation and angle,Decompose rotation matrix to plane of rotation and angle,,"I would like to decompose an $n$-dimensional orthogonal rotation matrix (restricting to simple rotation with a single plane of rotation) to the two basis vectors of the plane of rotation, and an angle of rotation. The common method is decomposing the rotation matrix to an axis and angle, but this doesn't work in higher dimensions. For example in $\mathbb{R}^3$ given the rotation matrix $R_{xy}=\begin{bmatrix}\cos\theta & -\sin\theta & 0 \\ \sin\theta & \cos\theta & 0 \\ 0 & 0 & 1 \end{bmatrix}$ it's obvious that the plane of rotation is the $xy$-plane spanned by basis vectors $b_0 = (1,0,0)$ and $b_1=(0,1,0)$ and the angle of rotation is $\theta$. However decomposing it mathematically is rather challenging. What is the solution for a general (restricted to a single, but arbitrary plane) rotation matrix in $\mathbb{R}^n$?","I would like to decompose an $n$-dimensional orthogonal rotation matrix (restricting to simple rotation with a single plane of rotation) to the two basis vectors of the plane of rotation, and an angle of rotation. The common method is decomposing the rotation matrix to an axis and angle, but this doesn't work in higher dimensions. For example in $\mathbb{R}^3$ given the rotation matrix $R_{xy}=\begin{bmatrix}\cos\theta & -\sin\theta & 0 \\ \sin\theta & \cos\theta & 0 \\ 0 & 0 & 1 \end{bmatrix}$ it's obvious that the plane of rotation is the $xy$-plane spanned by basis vectors $b_0 = (1,0,0)$ and $b_1=(0,1,0)$ and the angle of rotation is $\theta$. However decomposing it mathematically is rather challenging. What is the solution for a general (restricted to a single, but arbitrary plane) rotation matrix in $\mathbb{R}^n$?",,"['linear-algebra', 'matrices', 'geometry', 'euclidean-geometry', 'rotations']"
67,Making sense of angles in $\mathbb{R^n}$,Making sense of angles in,\mathbb{R^n},"One of the definitions of the dot product of two vectors is the following $$\vec{a} \bullet \vec{b}\ \  \stackrel{\text{def}}{=} \ \ \|\vec{a}\| \,\|\vec{b}\|\cos(\theta)$$ Where $\theta$ denotes the angle between the vectors $\vec{a}$ and $\vec{b}$. But for that angle $\theta$ to 'make sense', both $\vec{a}$ and $\vec{b}$ must lie in the same plane correct? i.e. There must exist some two-dimensional space, e.g. $\mathbb{R^2}$, that both $\vec{a}$ and $\vec{b}$ are an element of. Now we can always find a plane that both $\vec{a}$ and $\vec{b}$ fall in. But now the question arises, are we finding the angle $\theta$ with respect to the plane that they lie in (what I'm trying to say is: Is $\theta$ denoting the angle between $\vec{a}$ and $\vec{b}$ in a plane that they both fall in?), or is it with respect to the basis vectors of of the original vector space that they lie in initially. Here's an example that will hopefully get across what I'm asking: Take $\vec{a}, \vec{b} \in \mathbb{R^4}$. If we can find a plane that both $\vec{a}$ and $\vec{b}$ both lie in, then 'the angle between them' , $\theta$, makes sense. If not then what essentially is being said is $\theta$ is the angle between two vectors in four-dimensional space, and I'm not sure that an angle in $\mathbb{R^4}$ has any sort of meaning. My question then essentially boils down to the following. Are angles only defined (or do they only have meaning) in $\mathbb{R^2}$? If we have two vectors in $\mathbb{R^n}$, is the only way to find the 'angle between them' by first finding a plane (a two-dimensional vector space) that they both lie in, and then solving for the angle with respect to that plane?","One of the definitions of the dot product of two vectors is the following $$\vec{a} \bullet \vec{b}\ \  \stackrel{\text{def}}{=} \ \ \|\vec{a}\| \,\|\vec{b}\|\cos(\theta)$$ Where $\theta$ denotes the angle between the vectors $\vec{a}$ and $\vec{b}$. But for that angle $\theta$ to 'make sense', both $\vec{a}$ and $\vec{b}$ must lie in the same plane correct? i.e. There must exist some two-dimensional space, e.g. $\mathbb{R^2}$, that both $\vec{a}$ and $\vec{b}$ are an element of. Now we can always find a plane that both $\vec{a}$ and $\vec{b}$ fall in. But now the question arises, are we finding the angle $\theta$ with respect to the plane that they lie in (what I'm trying to say is: Is $\theta$ denoting the angle between $\vec{a}$ and $\vec{b}$ in a plane that they both fall in?), or is it with respect to the basis vectors of of the original vector space that they lie in initially. Here's an example that will hopefully get across what I'm asking: Take $\vec{a}, \vec{b} \in \mathbb{R^4}$. If we can find a plane that both $\vec{a}$ and $\vec{b}$ both lie in, then 'the angle between them' , $\theta$, makes sense. If not then what essentially is being said is $\theta$ is the angle between two vectors in four-dimensional space, and I'm not sure that an angle in $\mathbb{R^4}$ has any sort of meaning. My question then essentially boils down to the following. Are angles only defined (or do they only have meaning) in $\mathbb{R^2}$? If we have two vectors in $\mathbb{R^n}$, is the only way to find the 'angle between them' by first finding a plane (a two-dimensional vector space) that they both lie in, and then solving for the angle with respect to that plane?",,"['linear-algebra', 'trigonometry', 'vector-spaces', 'vectors']"
68,Complex ($\mathbb C$) least squares derivation,Complex () least squares derivation,\mathbb C,"I know how to derive the least squares in the real domain. If a tall matrix $A$ and a column vector $b$ are real, then the solution of the least squares problem $Ax = b$ can be derived as: $$\begin{align} \{E(x)\}^2 &= ||Ax - b||^2 \\ &= (Ax-b)^T (Ax-b) \\ &= x^T A^T Ax - x^T A^T b - b^T Ax + b^T b \\ &= x^T A^T Ax - 2 x^T A^T b + b^T b \qquad (\because (Ax)^T b = b^T (Ax)) \end{align}$$ Differentiating both sides with respect to $x$, $$\begin{align} \frac{d \{E(x)\}^2}{dx} &= 2A^T Ax - 2 A^T b \end{align}$$ Setting $\frac{d \{E(x)\}^2}{dx} = 0$ to find when we get the minimum $E(x)$, $$ 2A^T Ax - 2 A^T b = 0 \\ A^T Ax = A^T b \\ x = (A^T A)^{-1} A^T b $$ Now, we turn to the complex-valued situation. Assume $A$ and $b$ are complex, $$\begin{align} \{E(x)\}^2 &= ||Ax - b||^2 \\ &= (Ax-b)^H (Ax-b) \\ &= x^H A^H Ax - x^H A^H b - b^H Ax + b^H b \\ \end{align}$$ Here, I have some problems. First, $x^H A^H b \neq b^H Ax$ unless $(Ax)^H b$ is real. Most of all, I don't know how to differentiate the complex matrices above. How to proceed the derivation? There are plenty of derivations in the real domain in Google, but I couldn't find detailed explanation of the general complex case.","I know how to derive the least squares in the real domain. If a tall matrix $A$ and a column vector $b$ are real, then the solution of the least squares problem $Ax = b$ can be derived as: $$\begin{align} \{E(x)\}^2 &= ||Ax - b||^2 \\ &= (Ax-b)^T (Ax-b) \\ &= x^T A^T Ax - x^T A^T b - b^T Ax + b^T b \\ &= x^T A^T Ax - 2 x^T A^T b + b^T b \qquad (\because (Ax)^T b = b^T (Ax)) \end{align}$$ Differentiating both sides with respect to $x$, $$\begin{align} \frac{d \{E(x)\}^2}{dx} &= 2A^T Ax - 2 A^T b \end{align}$$ Setting $\frac{d \{E(x)\}^2}{dx} = 0$ to find when we get the minimum $E(x)$, $$ 2A^T Ax - 2 A^T b = 0 \\ A^T Ax = A^T b \\ x = (A^T A)^{-1} A^T b $$ Now, we turn to the complex-valued situation. Assume $A$ and $b$ are complex, $$\begin{align} \{E(x)\}^2 &= ||Ax - b||^2 \\ &= (Ax-b)^H (Ax-b) \\ &= x^H A^H Ax - x^H A^H b - b^H Ax + b^H b \\ \end{align}$$ Here, I have some problems. First, $x^H A^H b \neq b^H Ax$ unless $(Ax)^H b$ is real. Most of all, I don't know how to differentiate the complex matrices above. How to proceed the derivation? There are plenty of derivations in the real domain in Google, but I couldn't find detailed explanation of the general complex case.",,"['linear-algebra', 'matrix-calculus', 'least-squares']"
69,Prove that the determinant is a multiple of $17$ without developing it,Prove that the determinant is a multiple of  without developing it,17,"Let, matrix is given as : $$D=\begin{bmatrix} 1 & 1 & 9 \\ 1 & 8 & 7 \\ 1 & 5 & 3\end{bmatrix}$$ Prove that the determinant is a multiple of $17$ without developing it? I saw a resolution by the Jacobi method , but could not apply the methodology in this example.","Let, matrix is given as : $$D=\begin{bmatrix} 1 & 1 & 9 \\ 1 & 8 & 7 \\ 1 & 5 & 3\end{bmatrix}$$ Prove that the determinant is a multiple of $17$ without developing it? I saw a resolution by the Jacobi method , but could not apply the methodology in this example.",,"['linear-algebra', 'matrices', 'divisibility', 'determinant']"
70,Minors of orthonormal matrices are easy to compute,Minors of orthonormal matrices are easy to compute,,"Here is an intriguing fact: Take any $3\times 3$ orthogonal matrix $A$. For example, take $$ A=\begin{pmatrix} 1/\sqrt{14} & \sqrt{2/7} & 3/\sqrt{14} \\  2 \sqrt{13/105} & -5 \sqrt{5/273} & 8/\sqrt{1365}  \\ \sqrt{13/30} & \sqrt{10/39} & -11/\sqrt{390} \end{pmatrix} $$ which has been produced by applying the Gram–Schmidt process to some random choice of vectors. Compute the determinant of any $2\times 2$ submatrix. For example, the one consisting of the four corners: $$ \begin{vmatrix} 1/\sqrt{14}& 3/\sqrt{14} \\ \sqrt{13/30} & -11/\sqrt{390} \end{vmatrix} = -5 \sqrt{5/273} $$ Note that the result is the middle element of $A$. Doing this for any submatrix of any $A$, we always get $\pm$ the entry that completes the submatrix. Why is that?","Here is an intriguing fact: Take any $3\times 3$ orthogonal matrix $A$. For example, take $$ A=\begin{pmatrix} 1/\sqrt{14} & \sqrt{2/7} & 3/\sqrt{14} \\  2 \sqrt{13/105} & -5 \sqrt{5/273} & 8/\sqrt{1365}  \\ \sqrt{13/30} & \sqrt{10/39} & -11/\sqrt{390} \end{pmatrix} $$ which has been produced by applying the Gram–Schmidt process to some random choice of vectors. Compute the determinant of any $2\times 2$ submatrix. For example, the one consisting of the four corners: $$ \begin{vmatrix} 1/\sqrt{14}& 3/\sqrt{14} \\ \sqrt{13/30} & -11/\sqrt{390} \end{vmatrix} = -5 \sqrt{5/273} $$ Note that the result is the middle element of $A$. Doing this for any submatrix of any $A$, we always get $\pm$ the entry that completes the submatrix. Why is that?",,"['linear-algebra', 'matrices']"
71,Show that real field $\mathbb R$ is a vector space of infinite dimensions over the field of rational numbers $\mathbb Q$.,Show that real field  is a vector space of infinite dimensions over the field of rational numbers .,\mathbb R \mathbb Q,"I am doing it in the following way. Is it correct? Set $S = \{1,\pi,\pi^2,\pi^3,...,\pi^n\}$ is LI over $\mathbb Q$ Suppose  $a_0\times 1 + a_1\times\pi + ... a_n\times \pi^n = 0$ where all the a_i's are not $0$. Then $\pi$ is a root of $a_0 + a_1x + ... + a_nx^n = 0$ which is imposible since $\pi$ is a transcendental number. Therefore, S is LI. Hence $\mathbb R$ is of infinite dimension over $\mathbb Q$.","I am doing it in the following way. Is it correct? Set $S = \{1,\pi,\pi^2,\pi^3,...,\pi^n\}$ is LI over $\mathbb Q$ Suppose  $a_0\times 1 + a_1\times\pi + ... a_n\times \pi^n = 0$ where all the a_i's are not $0$. Then $\pi$ is a root of $a_0 + a_1x + ... + a_nx^n = 0$ which is imposible since $\pi$ is a transcendental number. Therefore, S is LI. Hence $\mathbb R$ is of infinite dimension over $\mathbb Q$.",,"['linear-algebra', 'vector-spaces', 'proof-verification']"
72,How to prove SSE and SSR are independent,How to prove SSE and SSR are independent,,"Consider $Y=X\beta+\varepsilon$, where $X$ is n by p, $\beta$ is p by 1 and $\varepsilon$ is n by 1 with covariance matrix = var($\varepsilon$)=$\sigma^2 I$. Give expression for the regression and error sums of squares, find their expected values, and show that they are independent. My work: One has $SSE=Y^{T}Y-\hat{\beta}^{T}X^{T}Y=Y^{T}(I-X(X^{T}X)^{-1}X^{T})Y$, and $SSR=Y^{T}(X(X^{T}X)^{-1}X^{T}-\frac{1}{n}J)Y$. For $SSE$, it is easy to get its distribution. But I have difficulty to get the distribution of $SSR$, I was trying to prove $X(X^{T}X)^{-1}X^{T}-\frac{1}{n}J$ is idempotent. But it seems not easy for me. Also I have difficulty to prove $(I-X(X^{T}X)^{-1}X^{T})(X(X^{T}X)^{-1}X^{T}-\frac{1}{n}J)=0$. Can someone help me here?","Consider $Y=X\beta+\varepsilon$, where $X$ is n by p, $\beta$ is p by 1 and $\varepsilon$ is n by 1 with covariance matrix = var($\varepsilon$)=$\sigma^2 I$. Give expression for the regression and error sums of squares, find their expected values, and show that they are independent. My work: One has $SSE=Y^{T}Y-\hat{\beta}^{T}X^{T}Y=Y^{T}(I-X(X^{T}X)^{-1}X^{T})Y$, and $SSR=Y^{T}(X(X^{T}X)^{-1}X^{T}-\frac{1}{n}J)Y$. For $SSE$, it is easy to get its distribution. But I have difficulty to get the distribution of $SSR$, I was trying to prove $X(X^{T}X)^{-1}X^{T}-\frac{1}{n}J$ is idempotent. But it seems not easy for me. Also I have difficulty to prove $(I-X(X^{T}X)^{-1}X^{T})(X(X^{T}X)^{-1}X^{T}-\frac{1}{n}J)=0$. Can someone help me here?",,"['linear-algebra', 'statistics', 'regression', 'statistical-inference', 'regression-analysis']"
73,An inequality for the dimension of the sum of subspaces,An inequality for the dimension of the sum of subspaces,,"The answer with the most of upvotes on MO is this answer on $\dim(U+V+W)$. Question : 1. Is it nonetheless true that every three vector subspaces $U$, $V$ and $W$ of a vector space $M$ satisfy $$ \dim(U +V + W) \le $$ $$ \dim U + \dim V + \dim W - \dim (U \cap V) - \dim (U \cap W) - \dim (V \cap W) + \dim(U \cap V \cap W) $$ ? 2. And, more generally, that $$\dim(\sum_{i = 1}^{n} U_i) \le \sum_{r=1}^{n} (-1)^{r+1} \sum_{i_1 < i_2 < \dots < i_r} \dim(\bigcap_{s=1}^{r}U_{i_s}) ? $$","The answer with the most of upvotes on MO is this answer on $\dim(U+V+W)$. Question : 1. Is it nonetheless true that every three vector subspaces $U$, $V$ and $W$ of a vector space $M$ satisfy $$ \dim(U +V + W) \le $$ $$ \dim U + \dim V + \dim W - \dim (U \cap V) - \dim (U \cap W) - \dim (V \cap W) + \dim(U \cap V \cap W) $$ ? 2. And, more generally, that $$\dim(\sum_{i = 1}^{n} U_i) \le \sum_{r=1}^{n} (-1)^{r+1} \sum_{i_1 < i_2 < \dots < i_r} \dim(\bigcap_{s=1}^{r}U_{i_s}) ? $$",,['linear-algebra']
74,Does there exist such an invertible matrix?,Does there exist such an invertible matrix?,,"Let $n \geq 1$ and $A = \mathbb{k}[x]$, where $\mathbb{k}$ is a field. Let $a_1, \dots, a_n \in A$ be such that $$Aa_1 + \dots + Aa_n = A.$$ Does there exist an invertible matrix $\|r_{ij}\| \in M_n\left(A\right)$ such that $r_{1j} = a_j$ for all $j = 1, \dots, n$?","Let $n \geq 1$ and $A = \mathbb{k}[x]$, where $\mathbb{k}$ is a field. Let $a_1, \dots, a_n \in A$ be such that $$Aa_1 + \dots + Aa_n = A.$$ Does there exist an invertible matrix $\|r_{ij}\| \in M_n\left(A\right)$ such that $r_{1j} = a_j$ for all $j = 1, \dots, n$?",,"['linear-algebra', 'abstract-algebra']"
75,How to find the limit of this matrix function,How to find the limit of this matrix function,,"Let $A$ be $n\times n$ real symmetric matrix that is positive definite. Let $x\in\mathbb{R^n}, \space x\ne 0$. Prove that the following limit   $$ \lim_{m\to\infty}\dfrac{x^TA^{m+1}x}{x^TA^{m}x} $$   exists and find that limit. My thought: since $A$ is symmetric, its eigenvalues are real. The solution should be related to eigenvalues of $A$. But I don't know how to do it and need help.","Let $A$ be $n\times n$ real symmetric matrix that is positive definite. Let $x\in\mathbb{R^n}, \space x\ne 0$. Prove that the following limit   $$ \lim_{m\to\infty}\dfrac{x^TA^{m+1}x}{x^TA^{m}x} $$   exists and find that limit. My thought: since $A$ is symmetric, its eigenvalues are real. The solution should be related to eigenvalues of $A$. But I don't know how to do it and need help.",,['linear-algebra']
76,reciprocal vectors,reciprocal vectors,,"I don't understand some of the terminology in this  question. I googled reciprocal vectors and got an article on reciprocal lattices, but I'm not sure if that is what they are talking about in this question. Also, when they say that ${\bf A}$, ${\bf B}$, and ${\bf C}$ are defined by ... plus cyclic permutations, again I looked at the wikipedia article on the subject, but I still do not understand the concept. Does anyone have a link for a clear explanation? The vectors ${\bf a}$, ${\bf b}$, and ${\bf c}$ are non-coplanar, and   form a non-orthogonal vector base. The vectors ${\bf A}$, ${\bf B}$,   and ${\bf C}$, defined by $$ {\bf A} = \frac{{\bf b}\times {\bf c}}{{\bf a}\cdot{\bf b}\times  {\bf c}}, $$ plus cyclic permutations, are said to be reciprocal vectors. Show that $$ {\bf a} = \frac{{\bf B}\times {\bf C}}{{\bf A}\cdot{\bf B}\times {\bf  C}}, $$ plus cyclic permutations. thanks","I don't understand some of the terminology in this  question. I googled reciprocal vectors and got an article on reciprocal lattices, but I'm not sure if that is what they are talking about in this question. Also, when they say that ${\bf A}$, ${\bf B}$, and ${\bf C}$ are defined by ... plus cyclic permutations, again I looked at the wikipedia article on the subject, but I still do not understand the concept. Does anyone have a link for a clear explanation? The vectors ${\bf a}$, ${\bf b}$, and ${\bf c}$ are non-coplanar, and   form a non-orthogonal vector base. The vectors ${\bf A}$, ${\bf B}$,   and ${\bf C}$, defined by $$ {\bf A} = \frac{{\bf b}\times {\bf c}}{{\bf a}\cdot{\bf b}\times  {\bf c}}, $$ plus cyclic permutations, are said to be reciprocal vectors. Show that $$ {\bf a} = \frac{{\bf B}\times {\bf C}}{{\bf A}\cdot{\bf B}\times {\bf  C}}, $$ plus cyclic permutations. thanks",,"['linear-algebra', 'vectors']"
77,Multiplication of inverse and non-inverse matrices,Multiplication of inverse and non-inverse matrices,,"I have thought about the combination of multiplication product of invertible and non-invertible matrices: invertible $\cdot$ invertible = invertible non-invertible $\cdot$non-invertible = non-invertible non-invertible $\cdot$invertible = non-invertible invertible $\cdot$non-invertible = non-invertible Is it right? I have thought about from the point of view that non-invertible matrix is row equivalent to a matrix with a zero row there for multiple it from both right and left will produce a matrix with a zero row and a zero column respectfully, and opposite goes for invertible matrices","I have thought about the combination of multiplication product of invertible and non-invertible matrices: invertible $\cdot$ invertible = invertible non-invertible $\cdot$non-invertible = non-invertible non-invertible $\cdot$invertible = non-invertible invertible $\cdot$non-invertible = non-invertible Is it right? I have thought about from the point of view that non-invertible matrix is row equivalent to a matrix with a zero row there for multiple it from both right and left will produce a matrix with a zero row and a zero column respectfully, and opposite goes for invertible matrices",,['linear-algebra']
78,How to define an affine transformation using 2 triangles?,How to define an affine transformation using 2 triangles?,,"I have $2$ triangles ($6$ dots) on a $2D$ plane. The points of the triangles are: a, b, c and x, y, z I would like to find a matrix, using I can transform every point in the 2D space. If I transform a , then the result is x . For b the result is y , and for c the result is z And if there is a given d point, which is halfway from a to b , then after the transformation the result should be between x and y halfway. I've tried to solve it according to NovaDenizen's solution, But the result is wrong. The original triangle: $$ a = \left[ \begin{array}{ccc} -3\\ 0\\ \end{array} \right] $$ $$ b = \left[ \begin{array}{ccc} 0\\ 3\\ \end{array} \right] $$ $$ c = \left[ \begin{array}{ccc} 3\\ 0\\ \end{array} \right] $$ The x, y, z dots: $$ x = \left[ \begin{array}{ccc} 2\\ 3\\ \end{array} \right] $$ $$ y = \left[ \begin{array}{ccc} 3\\ 2\\ \end{array} \right] $$ $$ z = \left[ \begin{array}{ccc} 4\\ 3\\ \end{array} \right] $$ I've created a figure: I tried to transform the (0, 0) point, which is halfway between a and b , but the result was (3, 3.5) instead of (3, 3) The T matrix is: $$\left[ \begin{array}{ccc} 1/3 & 1/6 & 0\\ 0 & -1/2 & 0\\ 3 & 3,5 & 1\\ \end{array} \right]$$","I have $2$ triangles ($6$ dots) on a $2D$ plane. The points of the triangles are: a, b, c and x, y, z I would like to find a matrix, using I can transform every point in the 2D space. If I transform a , then the result is x . For b the result is y , and for c the result is z And if there is a given d point, which is halfway from a to b , then after the transformation the result should be between x and y halfway. I've tried to solve it according to NovaDenizen's solution, But the result is wrong. The original triangle: $$ a = \left[ \begin{array}{ccc} -3\\ 0\\ \end{array} \right] $$ $$ b = \left[ \begin{array}{ccc} 0\\ 3\\ \end{array} \right] $$ $$ c = \left[ \begin{array}{ccc} 3\\ 0\\ \end{array} \right] $$ The x, y, z dots: $$ x = \left[ \begin{array}{ccc} 2\\ 3\\ \end{array} \right] $$ $$ y = \left[ \begin{array}{ccc} 3\\ 2\\ \end{array} \right] $$ $$ z = \left[ \begin{array}{ccc} 4\\ 3\\ \end{array} \right] $$ I've created a figure: I tried to transform the (0, 0) point, which is halfway between a and b , but the result was (3, 3.5) instead of (3, 3) The T matrix is: $$\left[ \begin{array}{ccc} 1/3 & 1/6 & 0\\ 0 & -1/2 & 0\\ 3 & 3,5 & 1\\ \end{array} \right]$$",,"['linear-algebra', 'matrices', 'transformation', 'affine-geometry']"
79,minors and rank of a matrix,minors and rank of a matrix,,"When reading a text, I came across a statement saying ""the rank of an $m\times n$ matrix is $r$ if and only if all $(n-r+1)\times(n-r+1)$. minors vanish"" Could anyone explain what it means by a minor, and how to prove the statement?","When reading a text, I came across a statement saying ""the rank of an $m\times n$ matrix is $r$ if and only if all $(n-r+1)\times(n-r+1)$. minors vanish"" Could anyone explain what it means by a minor, and how to prove the statement?",,"['linear-algebra', 'matrices']"
80,Equation of a curved line that passes through 3 points?,Equation of a curved line that passes through 3 points?,,"I have a screen wherein the upper-leftmost part is at x,y coordinate (0,0). Then I have a curved line that passes through 3 points: (132, 201), (295, 661) and (644, 1085). Now, say I want to find 7 points within that curved line so that the line will be equally divided into 6 segments. How do I go about computing the coordinates of these 7 points?","I have a screen wherein the upper-leftmost part is at x,y coordinate (0,0). Then I have a curved line that passes through 3 points: (132, 201), (295, 661) and (644, 1085). Now, say I want to find 7 points within that curved line so that the line will be equally divided into 6 segments. How do I go about computing the coordinates of these 7 points?",,"['linear-algebra', 'geometry', 'algebraic-geometry']"
81,Why do the concepts of linear algebra apply to differential equations?,Why do the concepts of linear algebra apply to differential equations?,,"A lot of the stuff we do to solve differential equations are taken word for word from linear algebra. The concept of linear independence, determinant of the Wronskian used to determine independence, adding a particular solution to the kernel to get the general solution - all this stuff that I'm used to applying on vectors seems to work with functions. Why?","A lot of the stuff we do to solve differential equations are taken word for word from linear algebra. The concept of linear independence, determinant of the Wronskian used to determine independence, adding a particular solution to the kernel to get the general solution - all this stuff that I'm used to applying on vectors seems to work with functions. Why?",,"['linear-algebra', 'ordinary-differential-equations']"
82,nth derivative of determinant wrt matrix,nth derivative of determinant wrt matrix,,"I'm working on an expression for the nth derivative of a (symmetric) matrix, i.e. \begin{equation}\frac{\partial^{n} \det(A)}{\partial A^{n}}\end{equation} Starting with \begin{equation}\frac{\partial \det(A)}{\partial A}=\det(A) A^{-1}\end{equation} Then naturally the next derivative is  \begin{equation}\frac{\partial^{2}\det(A)}{\partial A^{2}}=\frac{\partial}{\partial A}\left(\det(A)A^{-1}\right)=\det(A)A^{-2}-\det(A)A^{-2}=0\end{equation} I doubt this is right, can someone point out my mistake? I'm actually working on an expression for the nth derivative of $\det(A)^{-1/2}$ but a general formula for the simple case would be fine.","I'm working on an expression for the nth derivative of a (symmetric) matrix, i.e. \begin{equation}\frac{\partial^{n} \det(A)}{\partial A^{n}}\end{equation} Starting with \begin{equation}\frac{\partial \det(A)}{\partial A}=\det(A) A^{-1}\end{equation} Then naturally the next derivative is  \begin{equation}\frac{\partial^{2}\det(A)}{\partial A^{2}}=\frac{\partial}{\partial A}\left(\det(A)A^{-1}\right)=\det(A)A^{-2}-\det(A)A^{-2}=0\end{equation} I doubt this is right, can someone point out my mistake? I'm actually working on an expression for the nth derivative of $\det(A)^{-1/2}$ but a general formula for the simple case would be fine.",,"['linear-algebra', 'determinant', 'matrix-calculus']"
83,"Determine whether A is invertible, and if so, ﬁnd the inverse. (3x3)","Determine whether A is invertible, and if so, ﬁnd the inverse. (3x3)",,"In Exercises 37-38, determine whether $A$ is invertible, and if so, find the inverse. [ Hint: Solve $AX = I$ for $X$ by equating corresponding entries on the two sides. 37. $A = \begin{bmatrix} 1&0&1 \\ 1&1&0 \\ 0&1&1 \end{bmatrix}$ How the heck am I supposed to find an inverse of a 3x3? If I'm supposed to solve for $X$ I'd normally multiply both sides by $A^{-1}$ but this book gave no mention of a formula for more than 4 elements in a matrix... so if I equate $AX$ to $I$ , how do I solve for $X$ ?","In Exercises 37-38, determine whether is invertible, and if so, find the inverse. [ Hint: Solve for by equating corresponding entries on the two sides. 37. How the heck am I supposed to find an inverse of a 3x3? If I'm supposed to solve for I'd normally multiply both sides by but this book gave no mention of a formula for more than 4 elements in a matrix... so if I equate to , how do I solve for ?",A AX = I X A = \begin{bmatrix} 1&0&1 \\ 1&1&0 \\ 0&1&1 \end{bmatrix} X A^{-1} AX I X,['linear-algebra']
84,Elementary proofs of subadditivity of positive index of inertia,Elementary proofs of subadditivity of positive index of inertia,,"Denote the number of positive eigenvalues of a Hermitian matrix by $P(\cdot)$. If $A,B$ Hermitian, show that $$P(A+B)\leq P(A)+P(B).$$ I know there is an elementary proof: Subadditivity of positive index of inertia But my teacher says that this problem has other elementary proofs. Can anybody help? Thank you.","Denote the number of positive eigenvalues of a Hermitian matrix by $P(\cdot)$. If $A,B$ Hermitian, show that $$P(A+B)\leq P(A)+P(B).$$ I know there is an elementary proof: Subadditivity of positive index of inertia But my teacher says that this problem has other elementary proofs. Can anybody help? Thank you.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
85,Proof that $\det(A)=\det(A^T)$ using permutations.,Proof that  using permutations.,\det(A)=\det(A^T),"I'm reading a proof for the identity $\det(A) = \det(A^T)$ and I'm trying to udnerstand why the following rows are equivalent: $$\eqalign{   & \det ({A}) = \sum\limits_{\pi  \in {S_n}} {{\mathop{\rm sgn}} (\pi ) \cdot {a_{\pi (1),1}}} ...{a_{\pi (n),n}}  \cr    & \det ({A^T}) = \sum\limits_{\pi  \in {S_n}} {{\mathop{\rm sgn}} ({\pi ^{ - 1}}) \cdot {a_{1,{\pi ^{ - 1}}(1)}}} ...{a_{n,{\pi ^{ - 1}}(n)}} \cr} $$ Intuitively, I can understand that each term appears in both of the summations. How to understand it algebraically?","I'm reading a proof for the identity and I'm trying to udnerstand why the following rows are equivalent: Intuitively, I can understand that each term appears in both of the summations. How to understand it algebraically?","\det(A) = \det(A^T) \eqalign{
  & \det ({A}) = \sum\limits_{\pi  \in {S_n}} {{\mathop{\rm sgn}} (\pi ) \cdot {a_{\pi (1),1}}} ...{a_{\pi (n),n}}  \cr 
  & \det ({A^T}) = \sum\limits_{\pi  \in {S_n}} {{\mathop{\rm sgn}} ({\pi ^{ - 1}}) \cdot {a_{1,{\pi ^{ - 1}}(1)}}} ...{a_{n,{\pi ^{ - 1}}(n)}} \cr} ","['linear-algebra', 'permutations', 'determinant', 'transpose']"
86,"Besides being symmetric, when will a matrix have ONLY real eigenvalues?","Besides being symmetric, when will a matrix have ONLY real eigenvalues?",,"I realize that when a matrix is symmetric, then it must have all real eigenvalues. However, I am doing research on matrices for my own pleasure and I cannot find a mathematical proof or explanation when a matrix will have all real eigenvalues except for when it is symmetric. I am dealing with matrices such as A below and I want to know what is it about A and its characteristic polynomial that gives it real eigenvalues (0, 0, -2)? Similarly, what is it about matrix B that gives it only one real eigenvalue (0) and the other two complex?","I realize that when a matrix is symmetric, then it must have all real eigenvalues. However, I am doing research on matrices for my own pleasure and I cannot find a mathematical proof or explanation when a matrix will have all real eigenvalues except for when it is symmetric. I am dealing with matrices such as A below and I want to know what is it about A and its characteristic polynomial that gives it real eigenvalues (0, 0, -2)? Similarly, what is it about matrix B that gives it only one real eigenvalue (0) and the other two complex?",,"['linear-algebra', 'matrices']"
87,Finding the additive inverse in a vector space with unusual operations,Finding the additive inverse in a vector space with unusual operations,,"Let $V=\mathbb{R}$. For $u,v\in V$ and $a\in\mathbb{R}$, define vector addition by $$u\boxplus v:=u+v+2$$   and scalar multiplication by   $$a\boxdot u:=au+2a−2.$$   It can be shown that $(V,\boxplus,\boxdot)$ is a vector space over the scalar field $\mathbb{R}$. Find the following... I have found the sum, scalar multiple and the zero vector. I don't know where the x is coming from, let alone how to find the additive inverse. Can someone assist me? Thanks.","Let $V=\mathbb{R}$. For $u,v\in V$ and $a\in\mathbb{R}$, define vector addition by $$u\boxplus v:=u+v+2$$   and scalar multiplication by   $$a\boxdot u:=au+2a−2.$$   It can be shown that $(V,\boxplus,\boxdot)$ is a vector space over the scalar field $\mathbb{R}$. Find the following... I have found the sum, scalar multiple and the zero vector. I don't know where the x is coming from, let alone how to find the additive inverse. Can someone assist me? Thanks.",,['linear-algebra']
88,What properties should a matrix have if all its eigenvalues are real?,What properties should a matrix have if all its eigenvalues are real?,,"Recently, I’m trying to prove all the eigenvalues of a class of matrices are real. The matrices are complex and not hermitian. The problem for me is I don't know any properties for a matrix with all the eigenvalues real. So would you please tell me any sufficient or necessary conditions for such matrices? Thank you!","Recently, I’m trying to prove all the eigenvalues of a class of matrices are real. The matrices are complex and not hermitian. The problem for me is I don't know any properties for a matrix with all the eigenvalues real. So would you please tell me any sufficient or necessary conditions for such matrices? Thank you!",,['linear-algebra']
89,Show that the least squares line must pass through the center of mass,Show that the least squares line must pass through the center of mass,,"My problem: The point $(\bar x, \bar y)$ is the center of mass for the collection of points in Exercise 7. Show that the least squares line must pass through the center of mass. [ Hint : Use a change of variables $z = x - \bar x$ to translate the problem so that the new independent variable has mean 0.] I have already solved Exercise 7: Given a collection of points $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$, let $\mathbf x = (x_1, x_2, \ldots, x_n)^T$, $\mathbf y = (y_1, y_2, \ldots, y_n)^T$, $\bar x = \frac 1n \sum_1^n x_i$, $\bar y = \frac 1n \sum_1^n y_i$ and let $y = c_0 + c_1 y$ be the linear function that gives the best least squares fit to the points. Show that if $\bar x = 0$, then $c_0 = \bar y$ and $c_1 = \frac {\mathbf x^T \mathbf y}{\mathbf x^T \mathbf x}$. It is obvious that if $x = \bar x$ then $y = c_0 + c_1x = \bar y + 0 = \bar y$, however the hint suggests that the problem should be solved in another way. Edit I have found an answer. It makes use of the following theorem: If A is an m x n matrix of rank n , the normal equations $ A^T A \mathbf x = A^T \mathbf b$ have a unique solution $ \hat {\mathbf x} = (A^TA)^{-1}A^T \mathbf b$ and $ \hat {\mathbf x} $ is the unique least squares solution of the system $ A \mathbf x = \mathbf b $. Now let $ \hat {\mathbf x} = \mathbf c = (c_0, c_1)^T, A = \begin{pmatrix}1 & \cdots & 1 \\x_1 & \cdots & x_n \\\end{pmatrix}, \mathbf b = \mathbf y = (y_1, \ldots, y_n)^T $ such that $c = (A^TA)^{-1}A^Ty$, then $$\begin{pmatrix}c_0\\c_1\\\end{pmatrix} = \begin{pmatrix}n & \sum x_i\\\sum x_i & \sum x_i^2\\\end{pmatrix}^{-1} \begin{pmatrix}\sum y_i\\\sum x_iy_i\\\end{pmatrix} $$ which gives values for $c_0$ and $c_1$. These values should be used in the formula $c_1x + c_0$, which, together with $ x = \bar x = \frac 1n \sum x_i$, indeed results in $ \bar y $.","My problem: The point $(\bar x, \bar y)$ is the center of mass for the collection of points in Exercise 7. Show that the least squares line must pass through the center of mass. [ Hint : Use a change of variables $z = x - \bar x$ to translate the problem so that the new independent variable has mean 0.] I have already solved Exercise 7: Given a collection of points $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$, let $\mathbf x = (x_1, x_2, \ldots, x_n)^T$, $\mathbf y = (y_1, y_2, \ldots, y_n)^T$, $\bar x = \frac 1n \sum_1^n x_i$, $\bar y = \frac 1n \sum_1^n y_i$ and let $y = c_0 + c_1 y$ be the linear function that gives the best least squares fit to the points. Show that if $\bar x = 0$, then $c_0 = \bar y$ and $c_1 = \frac {\mathbf x^T \mathbf y}{\mathbf x^T \mathbf x}$. It is obvious that if $x = \bar x$ then $y = c_0 + c_1x = \bar y + 0 = \bar y$, however the hint suggests that the problem should be solved in another way. Edit I have found an answer. It makes use of the following theorem: If A is an m x n matrix of rank n , the normal equations $ A^T A \mathbf x = A^T \mathbf b$ have a unique solution $ \hat {\mathbf x} = (A^TA)^{-1}A^T \mathbf b$ and $ \hat {\mathbf x} $ is the unique least squares solution of the system $ A \mathbf x = \mathbf b $. Now let $ \hat {\mathbf x} = \mathbf c = (c_0, c_1)^T, A = \begin{pmatrix}1 & \cdots & 1 \\x_1 & \cdots & x_n \\\end{pmatrix}, \mathbf b = \mathbf y = (y_1, \ldots, y_n)^T $ such that $c = (A^TA)^{-1}A^Ty$, then $$\begin{pmatrix}c_0\\c_1\\\end{pmatrix} = \begin{pmatrix}n & \sum x_i\\\sum x_i & \sum x_i^2\\\end{pmatrix}^{-1} \begin{pmatrix}\sum y_i\\\sum x_iy_i\\\end{pmatrix} $$ which gives values for $c_0$ and $c_1$. These values should be used in the formula $c_1x + c_0$, which, together with $ x = \bar x = \frac 1n \sum x_i$, indeed results in $ \bar y $.",,"['linear-algebra', 'regression', 'least-squares']"
90,How do you prove that there does not exist a $3 \times 3$ matrix over $\Bbb{Q}$ such as $A^8=I$ and $A^4 \ne I$?,How do you prove that there does not exist a  matrix over  such as  and ?,3 \times 3 \Bbb{Q} A^8=I A^4 \ne I,"I am kind of stuck with the following problem: Prove that there does not exist a $3 \times 3$ matrix over $\Bbb{Q}$ such as $A^8=I$ and $A^4 \ne I$. I already tried some things and got that if $A^8=I$, then $(A^4)^2=I$. And I guess a similar reasoning would work for this one.   I bet that I am missing something obvious but I can't find what is it :(","I am kind of stuck with the following problem: Prove that there does not exist a $3 \times 3$ matrix over $\Bbb{Q}$ such as $A^8=I$ and $A^4 \ne I$. I already tried some things and got that if $A^8=I$, then $(A^4)^2=I$. And I guess a similar reasoning would work for this one.   I bet that I am missing something obvious but I can't find what is it :(",,"['linear-algebra', 'matrices']"
91,Finding inverse of a matrix,Finding inverse of a matrix,,"This question is in my assignment. We are not allowed to use any symbol to represent any elementary row and column operations used in the solution. We must solve it step-by-step. Please help me to check my solution word by word including my spelling and grammar. Question: Find the inverse of $$A=\begin{pmatrix}2& 2& 3\\ 2& 5& 3\\ 1& 0& 8\end{pmatrix}$$ by using only elementary row operations. Solution: We begin by forming the matrix $\begin{pmatrix} A & | & I_3 \end{pmatrix}=\left(\begin{array}{ccc|ccc}2 & 2 & 3 & 1 & 0 & 0\\2 & 5 & 3 & 0 & 1 & 0\\1 & 0 & 8 & 0 & 0 & 1\end{array}\right)$. Interchanging the first and third rows of the matrix $\begin{pmatrix} A & | & I_3 \end{pmatrix}$, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\2 & 5 & 3 & 0 & 1 & 0\\2 & 2 & 3 & 1 & 0 & 0\end{array}\right)$. Adding $(-2)$ times the first row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\2 & 5 & 3 & 0 & 1 & 0\\2 & 2 & 3 & 1 & 0 & 0\end{array}\right)$ to its second row, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 5 & -13 & 0 & 1 & -2\\2 & 2 & 3 & 1 & 0 & 0\end{array}\right)$. Multiplying the second row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 5 & -13 & 0 & 1 & -2\\2 & 2 & 3 & 1 & 0 & 0\end{array}\right)$ by $\frac{1}{5}$, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\2 & 2 & 3 & 1 & 0 & 0\end{array}\right)$. Adding $(-2)$ times the first row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\2 & 2 & 3 & 1 & 0 & 0\end{array}\right)$ to its third row, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\0 & 2 & -13 & 1 & 0 & -2\end{array}\right)$. Adding $(-2)$ times the second row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\0 & 2 & -13 & 1 & 0 & -2\end{array}\right)$ to its third row, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\0 & 0 & -\frac{39}{5} & 1 & -\frac{2}{5} & -\frac{6}{5}\end{array}\right)$. Multiplying the third row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\0 & 0 & -\frac{39}{5} & 1 & -\frac{2}{5} & -\frac{6}{5}\end{array}\right)$ by $(-\frac{5}{39})$, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\0 & 0 & 1 & -\frac{5}{39} & \frac{2}{39} & \frac{2}{13}\end{array}\right)$. Adding $(\frac{13}{5})$ times the third row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\0 & 0 & 1 & -\frac{5}{39} & \frac{2}{39} & \frac{2}{13}\end{array}\right)$ to its second row, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & 0 & -\frac{1}{3} & \frac{1}{3} & 0\\0 & 0 & 1 & -\frac{5}{39} & \frac{2}{39} & \frac{2}{13}\end{array}\right)$. Adding $(-8)$ times the third row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & 0 & -\frac{1}{3} & \frac{1}{3} & 0\\0 & 0 & 1 & -\frac{5}{39} & \frac{2}{39} & \frac{2}{13}\end{array}\right)$ to its first row, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 0 & \frac{40}{39} & -\frac{16}{39} & -\frac{3}{13}\\0 & 1 & 0 & -\frac{1}{3} & \frac{1}{3} & 0\\0 & 0 & 1 & -\frac{5}{39} & \frac{2}{39} & \frac{2}{13}\end{array}\right)$. Thus, $A^{-1}=\begin{pmatrix}\frac{40}{39} & -\frac{16}{39} & -\frac{3}{13}\\ -\frac{1}{3} & \frac{1}{3} & 0\\ -\frac{5}{39} & \frac{2}{39} & \frac{2}{13}\end{pmatrix}$.","This question is in my assignment. We are not allowed to use any symbol to represent any elementary row and column operations used in the solution. We must solve it step-by-step. Please help me to check my solution word by word including my spelling and grammar. Question: Find the inverse of $$A=\begin{pmatrix}2& 2& 3\\ 2& 5& 3\\ 1& 0& 8\end{pmatrix}$$ by using only elementary row operations. Solution: We begin by forming the matrix $\begin{pmatrix} A & | & I_3 \end{pmatrix}=\left(\begin{array}{ccc|ccc}2 & 2 & 3 & 1 & 0 & 0\\2 & 5 & 3 & 0 & 1 & 0\\1 & 0 & 8 & 0 & 0 & 1\end{array}\right)$. Interchanging the first and third rows of the matrix $\begin{pmatrix} A & | & I_3 \end{pmatrix}$, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\2 & 5 & 3 & 0 & 1 & 0\\2 & 2 & 3 & 1 & 0 & 0\end{array}\right)$. Adding $(-2)$ times the first row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\2 & 5 & 3 & 0 & 1 & 0\\2 & 2 & 3 & 1 & 0 & 0\end{array}\right)$ to its second row, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 5 & -13 & 0 & 1 & -2\\2 & 2 & 3 & 1 & 0 & 0\end{array}\right)$. Multiplying the second row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 5 & -13 & 0 & 1 & -2\\2 & 2 & 3 & 1 & 0 & 0\end{array}\right)$ by $\frac{1}{5}$, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\2 & 2 & 3 & 1 & 0 & 0\end{array}\right)$. Adding $(-2)$ times the first row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\2 & 2 & 3 & 1 & 0 & 0\end{array}\right)$ to its third row, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\0 & 2 & -13 & 1 & 0 & -2\end{array}\right)$. Adding $(-2)$ times the second row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\0 & 2 & -13 & 1 & 0 & -2\end{array}\right)$ to its third row, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\0 & 0 & -\frac{39}{5} & 1 & -\frac{2}{5} & -\frac{6}{5}\end{array}\right)$. Multiplying the third row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\0 & 0 & -\frac{39}{5} & 1 & -\frac{2}{5} & -\frac{6}{5}\end{array}\right)$ by $(-\frac{5}{39})$, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\0 & 0 & 1 & -\frac{5}{39} & \frac{2}{39} & \frac{2}{13}\end{array}\right)$. Adding $(\frac{13}{5})$ times the third row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & -\frac{13}{5} & 0 & \frac{1}{5} & -\frac{2}{5}\\0 & 0 & 1 & -\frac{5}{39} & \frac{2}{39} & \frac{2}{13}\end{array}\right)$ to its second row, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & 0 & -\frac{1}{3} & \frac{1}{3} & 0\\0 & 0 & 1 & -\frac{5}{39} & \frac{2}{39} & \frac{2}{13}\end{array}\right)$. Adding $(-8)$ times the third row of the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 8 & 0 & 0 & 1\\0 & 1 & 0 & -\frac{1}{3} & \frac{1}{3} & 0\\0 & 0 & 1 & -\frac{5}{39} & \frac{2}{39} & \frac{2}{13}\end{array}\right)$ to its first row, we obtain the matrix $\left(\begin{array}{ccc|ccc}1 & 0 & 0 & \frac{40}{39} & -\frac{16}{39} & -\frac{3}{13}\\0 & 1 & 0 & -\frac{1}{3} & \frac{1}{3} & 0\\0 & 0 & 1 & -\frac{5}{39} & \frac{2}{39} & \frac{2}{13}\end{array}\right)$. Thus, $A^{-1}=\begin{pmatrix}\frac{40}{39} & -\frac{16}{39} & -\frac{3}{13}\\ -\frac{1}{3} & \frac{1}{3} & 0\\ -\frac{5}{39} & \frac{2}{39} & \frac{2}{13}\end{pmatrix}$.",,"['linear-algebra', 'matrices', 'inverse']"
92,What is meant by linearity of a dot product?,What is meant by linearity of a dot product?,,I would like to know what is meant by linearity of a dot product. Thank you,I would like to know what is meant by linearity of a dot product. Thank you,,['linear-algebra']
93,Relationship between positive definite matrix eigenvalues and the principal axes lengths of ellipsoid,Relationship between positive definite matrix eigenvalues and the principal axes lengths of ellipsoid,,"In many textbooks of pattern recognition I have seen the following statement: If a matrix $A$ in the quadratic form $F(\text{x}) = \text{x}^{T}A\text{x}$ is positive definite, then it follows that the surfaces of constant $F(\text{x})$ are hyperellipsoids, with principal axes having lengths proportional to $\lambda_{k}^{-1/2}$. $A$ is a $n \times n$ matrix, $\lambda_k$ is an eigenvalue of $A$, $\text{x}$ is a non-zero vector and $k = 1, ..., n$. My question is: ""Why are the principal axes of hyperellipsoids proportional to the eigenvalues of $A$? Why are they proportional to $\lambda_{k}^{-1/2}$? And why are the  surfaces of constant $F(\text{x})$  hyperellipsoids?"" Can someone give me a proof? Visualization perhaps? etc. Anything that would make me see that this indeed is the case :) Hope my question is clear :)  Thank you for any help!","In many textbooks of pattern recognition I have seen the following statement: If a matrix $A$ in the quadratic form $F(\text{x}) = \text{x}^{T}A\text{x}$ is positive definite, then it follows that the surfaces of constant $F(\text{x})$ are hyperellipsoids, with principal axes having lengths proportional to $\lambda_{k}^{-1/2}$. $A$ is a $n \times n$ matrix, $\lambda_k$ is an eigenvalue of $A$, $\text{x}$ is a non-zero vector and $k = 1, ..., n$. My question is: ""Why are the principal axes of hyperellipsoids proportional to the eigenvalues of $A$? Why are they proportional to $\lambda_{k}^{-1/2}$? And why are the  surfaces of constant $F(\text{x})$  hyperellipsoids?"" Can someone give me a proof? Visualization perhaps? etc. Anything that would make me see that this indeed is the case :) Hope my question is clear :)  Thank you for any help!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
94,Quadratic Bezier curves representation as implicit quadratic equation,Quadratic Bezier curves representation as implicit quadratic equation,,"A quadratic bezier curve from points P1=(x1, y1) to P3=(x3, y3) with control point P2=(x2, y2) can be represented as parametric quadratic curve P(t) where t is in [0, 1] . $$P(t) = (P_1t + P_2(1 - t))t + (P_2t + P_3(1 - t))(1 - t)          = P_1t^2 + P_2(1 - t)t + P_3(1 - t)^2$$ We can extend the range of t to be all real numbers, from minus infinity to plus infinity, and then we will get a nice curve dividing the plane in two. I read somewhere, that this infinete curve is either a straight line (if P1 , P2 and P3 are colinear) or a parabola. Could you please confirm that? Since division of plane by a parabola or a line can be expressed by quadratic equation of form: Point P=(x, y) is    on one side  if F(P) > 0   on the other if F(P) < 0   on the curve if F(P) == 0 where F(P) = A * x^2 + B * y^2 + C * x * y + D * x + E * y + F How can we, starting from points P1, P2, P3 calculate the numbers A, B, C,D, E, F ? I think something similar is done in this wolfram demostration http://demonstrations.wolfram.com/FocusAndDirectrixInAQuadraticBezierCurve/ Thank you","A quadratic bezier curve from points P1=(x1, y1) to P3=(x3, y3) with control point P2=(x2, y2) can be represented as parametric quadratic curve P(t) where t is in [0, 1] . We can extend the range of t to be all real numbers, from minus infinity to plus infinity, and then we will get a nice curve dividing the plane in two. I read somewhere, that this infinete curve is either a straight line (if P1 , P2 and P3 are colinear) or a parabola. Could you please confirm that? Since division of plane by a parabola or a line can be expressed by quadratic equation of form: Point P=(x, y) is    on one side  if F(P) > 0   on the other if F(P) < 0   on the curve if F(P) == 0 where F(P) = A * x^2 + B * y^2 + C * x * y + D * x + E * y + F How can we, starting from points P1, P2, P3 calculate the numbers A, B, C,D, E, F ? I think something similar is done in this wolfram demostration http://demonstrations.wolfram.com/FocusAndDirectrixInAQuadraticBezierCurve/ Thank you","P(t) = (P_1t + P_2(1 - t))t + (P_2t + P_3(1 - t))(1 - t)
         = P_1t^2 + P_2(1 - t)t + P_3(1 - t)^2","['linear-algebra', 'geometry', 'algebraic-geometry']"
95,Distance of a test point from the center of an ellipsoid,Distance of a test point from the center of an ellipsoid,,"I'm trying to learn about Mahanalobis distance and I'm pretty close to getting the idea. I've learned that the distance has got a lot to do with the properties of an ellipsoid . I have understood so far that: The Mahalanobis distance is simply the distance of the test point $\textbf{x}$ from the center of mass $\textbf{y}$ divided by the width of the ellipsoid in the direction of the test point and is given by the formula: $$D(\textbf{x},\textbf{y})=\sqrt{ (\textbf{x}-\textbf{y})^TC^{-1}(\textbf{x}-\textbf{y})} $$ Now my question is: ""Why does this formula give us the distance of a point $\textbf{x}$ from the center of mass $\textbf{y}$ divided by the width of the ellipsoid in the direction of the test point?"" =) I don't understand or see how this formula describes that distance, could someone help explaining this distance more? How is the plain distance of a point $\textbf{x}$ from the ellipsoid's center of mass $\textbf{y}$ in the direction of the test point found and why? =) I hope my question is clear enough. My question could be analogous with for example ""Why does $c^2 = a^2 + b^2$"" Then you would prove this to me with a geometric proof or something =) Thank you for any help =) P.S. $C$ is the covariance matrix of vector $\textbf{x} = (x_1, ..., x_n)$","I'm trying to learn about Mahanalobis distance and I'm pretty close to getting the idea. I've learned that the distance has got a lot to do with the properties of an ellipsoid . I have understood so far that: The Mahalanobis distance is simply the distance of the test point $\textbf{x}$ from the center of mass $\textbf{y}$ divided by the width of the ellipsoid in the direction of the test point and is given by the formula: $$D(\textbf{x},\textbf{y})=\sqrt{ (\textbf{x}-\textbf{y})^TC^{-1}(\textbf{x}-\textbf{y})} $$ Now my question is: ""Why does this formula give us the distance of a point $\textbf{x}$ from the center of mass $\textbf{y}$ divided by the width of the ellipsoid in the direction of the test point?"" =) I don't understand or see how this formula describes that distance, could someone help explaining this distance more? How is the plain distance of a point $\textbf{x}$ from the ellipsoid's center of mass $\textbf{y}$ in the direction of the test point found and why? =) I hope my question is clear enough. My question could be analogous with for example ""Why does $c^2 = a^2 + b^2$"" Then you would prove this to me with a geometric proof or something =) Thank you for any help =) P.S. $C$ is the covariance matrix of vector $\textbf{x} = (x_1, ..., x_n)$",,"['linear-algebra', 'probability', 'geometry', 'statistics', 'multivariable-calculus']"
96,Calculating the minimal polynomial of this matrix,Calculating the minimal polynomial of this matrix,,"Given $\begin{pmatrix} 1 & 1 & \cdots & 1 \\ 2 & 2 & \cdots & 2 \\ \cdot & \cdot & \cdots & \cdot \\  n & n & \cdots & n \end{pmatrix}$, calculate the minimal polynomial. I know that the characteristic polyomial is: $$P_A(x) = x^{n-1} \cdot (x - \frac{n(n+1)}{2}) $$ But how do I exactly know the minimal polynomial? I am sensing that it is by using the Cayley-Hamilton theory!","Given $\begin{pmatrix} 1 & 1 & \cdots & 1 \\ 2 & 2 & \cdots & 2 \\ \cdot & \cdot & \cdots & \cdot \\  n & n & \cdots & n \end{pmatrix}$, calculate the minimal polynomial. I know that the characteristic polyomial is: $$P_A(x) = x^{n-1} \cdot (x - \frac{n(n+1)}{2}) $$ But how do I exactly know the minimal polynomial? I am sensing that it is by using the Cayley-Hamilton theory!",,"['linear-algebra', 'matrices']"
97,Relation between Normed space and inner product,Relation between Normed space and inner product,,"Below is what i have proved: If $V$ is a normed vector space over $\mathbb{R}$ satisfies parallelogram equality, then there exists an inner product $\langle \bullet,\bullet\rangle$ such that $\langle \bullet,\bullet \rangle = \lVert \bullet \rVert^2$ If $V$ is an inner product space over $\mathbb{R}$ and $\lVert \bullet \rVert \triangleq \sqrt{\langle \bullet \rangle}$, then $\lVert \bullet \rVert$ is a norm and $\langle x,y\rangle = \frac{1}{4}(\lVert x+y\rVert^2 - \lVert x-y\rVert^2)$. I thought defining a norm as square root of inner product $\langle x,x\rangle$ is just one possinle way to define norm on an inner product space. However, the below is the article in wikipedia about impossibility of defining $p-norm$ on an inner product space: ""p ≠ 2 is a normed space but not an inner product space, because this norm does not satisfy the parallelogram equality required of a norm to have an inner product associated with it"" I don't think what i proved above give this conclusion in wikipeia.. What should i prove to conclude this? Plus, it's on wikipedia that $2$-notm is completely natural to define as so, but it's not that natural to take square root to inner product to define norm on an inner product space to me. Why do we specifically use 2-norm? Moreover, what's the point of relating two concepts inner product and norm, literally?","Below is what i have proved: If $V$ is a normed vector space over $\mathbb{R}$ satisfies parallelogram equality, then there exists an inner product $\langle \bullet,\bullet\rangle$ such that $\langle \bullet,\bullet \rangle = \lVert \bullet \rVert^2$ If $V$ is an inner product space over $\mathbb{R}$ and $\lVert \bullet \rVert \triangleq \sqrt{\langle \bullet \rangle}$, then $\lVert \bullet \rVert$ is a norm and $\langle x,y\rangle = \frac{1}{4}(\lVert x+y\rVert^2 - \lVert x-y\rVert^2)$. I thought defining a norm as square root of inner product $\langle x,x\rangle$ is just one possinle way to define norm on an inner product space. However, the below is the article in wikipedia about impossibility of defining $p-norm$ on an inner product space: ""p ≠ 2 is a normed space but not an inner product space, because this norm does not satisfy the parallelogram equality required of a norm to have an inner product associated with it"" I don't think what i proved above give this conclusion in wikipeia.. What should i prove to conclude this? Plus, it's on wikipedia that $2$-notm is completely natural to define as so, but it's not that natural to take square root to inner product to define norm on an inner product space to me. Why do we specifically use 2-norm? Moreover, what's the point of relating two concepts inner product and norm, literally?",,"['linear-algebra', 'normed-spaces', 'inner-products']"
98,Can every symmetric regular complex matrix be decomposed into a product of a matrix times its transpose?,Can every symmetric regular complex matrix be decomposed into a product of a matrix times its transpose?,,"Let $A\in GL(n,\mathbb{C})$ be symmetric. Is there a always a $B\in GL(n,\mathbb{C})$, such that $A=B^TB$?","Let $A\in GL(n,\mathbb{C})$ be symmetric. Is there a always a $B\in GL(n,\mathbb{C})$, such that $A=B^TB$?",,"['linear-algebra', 'matrices']"
99,$n$ by $n$ invertible matrix $A$ has $\text{rank(A)}=n$,by  invertible matrix  has,n n A \text{rank(A)}=n,"I'm wondering why $\text{rank}(A)=n$ means $A$ is invertible. Since invertible means one-to-one and onto, we have to prove that. $\text{rank}(A)=n$ means $\dim(N(A))=0$ which means one-to-one. Now, we have to show onto. $\text{rank}(A)=\dim(R(A))=n$ and it means onto. So we complete the proof. Is that right? [ADDITION] I want to check this: $\text{rank}(A)=\text{rank}(L_A)=\dim(R(L_A))=n$ and it means onto? I mean, range and codomain have same dimension means they are same? I know even though they have same dimension but still their spaces could be different, like row space and column space. But the above statement ""range and codomain have the same dimension means they are same"" is true? I solve it. It is true since $R(L_A)$ is a subspace of $F^n$. Refer to theorem $1.11$ in textbook ""linear algebra"" by friedberg.","I'm wondering why $\text{rank}(A)=n$ means $A$ is invertible. Since invertible means one-to-one and onto, we have to prove that. $\text{rank}(A)=n$ means $\dim(N(A))=0$ which means one-to-one. Now, we have to show onto. $\text{rank}(A)=\dim(R(A))=n$ and it means onto. So we complete the proof. Is that right? [ADDITION] I want to check this: $\text{rank}(A)=\text{rank}(L_A)=\dim(R(L_A))=n$ and it means onto? I mean, range and codomain have same dimension means they are same? I know even though they have same dimension but still their spaces could be different, like row space and column space. But the above statement ""range and codomain have the same dimension means they are same"" is true? I solve it. It is true since $R(L_A)$ is a subspace of $F^n$. Refer to theorem $1.11$ in textbook ""linear algebra"" by friedberg.",,['linear-algebra']
