,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Lim inf with norm and weak convergence,Lim inf with norm and weak convergence,,The following is an real analysis qualifying exam problem that I cannot solve: Suppose $X$ is a Banach space and that $(x_n)$ converges weakly to $x$. Show that $\liminf ||x_n|| \geq ||x||$. Using the Uniform Boundedness Principle I can show that $\sup_{n \in \mathbb{N}} ||x_n -x||$ is finite. Using Alaoglu's Theorem I can show that some subset of $(x_n)$ converges in norm to $x$. I feel like I am close with this but cannot seem to finish the problem.,The following is an real analysis qualifying exam problem that I cannot solve: Suppose $X$ is a Banach space and that $(x_n)$ converges weakly to $x$. Show that $\liminf ||x_n|| \geq ||x||$. Using the Uniform Boundedness Principle I can show that $\sup_{n \in \mathbb{N}} ||x_n -x||$ is finite. Using Alaoglu's Theorem I can show that some subset of $(x_n)$ converges in norm to $x$. I feel like I am close with this but cannot seem to finish the problem.,,"['real-analysis', 'functional-analysis']"
1,Stieltjes Integral meaning.,Stieltjes Integral meaning.,,"Can anybody give a geometrical interpretation of the Stieltjes integral: $$\int_a^bf(\xi)\,d\alpha(\xi)$$ How would we calculate? $$\int_a^b \xi^3\,d\alpha(\xi)$$ for example.","Can anybody give a geometrical interpretation of the Stieltjes integral: $$\int_a^bf(\xi)\,d\alpha(\xi)$$ How would we calculate? $$\int_a^b \xi^3\,d\alpha(\xi)$$ for example.",,"['real-analysis', 'integration']"
2,"Divergence of $\sum\limits_n1/\max(a_n,b_n)$",Divergence of,"\sum\limits_n1/\max(a_n,b_n)","Given two positive and (strictly) monotone increasing sequences, $a_n$ and $b_n$ such that $\displaystyle \sum_{n=1}^{\infty}\dfrac1{a_n}$ and $\displaystyle \sum_{n=1}^{\infty}\dfrac1{b_n}$ diverge, does $\displaystyle \sum_{n=1}^{\infty}\dfrac1{\max(a_n,b_n)}$ diverge? For sequences, where $a_n > b_n$ or $a_n < b_n$ eventually, the result follows immediately. But I am unable to see what happens when $a_n$ and $b_n$ take turns to overtake each other as $n \to \infty$? Essentially this question stems from the question, Osgood condition , where I wanted to consider the function $F(x) = g(x) + h(x)$ and lower bound $\displaystyle \int\dfrac{dy}{F(y)}$ by $\displaystyle \int \dfrac{dy}{2\max(h(y),g(y))}$ in an attempt to show that $\displaystyle \int\dfrac{dy}{F(y)}$ diverges.","Given two positive and (strictly) monotone increasing sequences, $a_n$ and $b_n$ such that $\displaystyle \sum_{n=1}^{\infty}\dfrac1{a_n}$ and $\displaystyle \sum_{n=1}^{\infty}\dfrac1{b_n}$ diverge, does $\displaystyle \sum_{n=1}^{\infty}\dfrac1{\max(a_n,b_n)}$ diverge? For sequences, where $a_n > b_n$ or $a_n < b_n$ eventually, the result follows immediately. But I am unable to see what happens when $a_n$ and $b_n$ take turns to overtake each other as $n \to \infty$? Essentially this question stems from the question, Osgood condition , where I wanted to consider the function $F(x) = g(x) + h(x)$ and lower bound $\displaystyle \int\dfrac{dy}{F(y)}$ by $\displaystyle \int \dfrac{dy}{2\max(h(y),g(y))}$ in an attempt to show that $\displaystyle \int\dfrac{dy}{F(y)}$ diverges.",,['real-analysis']
3,$\int_{0}^{\infty} \frac{\cos x - e^{-x}}{x} \mathrm dx$ Evaluate Integral,Evaluate Integral,\int_{0}^{\infty} \frac{\cos x - e^{-x}}{x} \mathrm dx,Evaluate $$\int_{0}^{\infty} \frac{\cos x - e^{-x}}{x} \ dx$$,Evaluate $$\int_{0}^{\infty} \frac{\cos x - e^{-x}}{x} \ dx$$,,"['calculus', 'real-analysis', 'integration', 'improper-integrals']"
4,"The continuity of $g\circ h$ and $h$, and $h$ surjective, imply the continuity of $g$?","The continuity of  and , and  surjective, imply the continuity of ?",g\circ h h h g,"Structural assumptions: $X$ is a nontrivial connected separable topological space Given: $f(x)=g(h(x))$ . $f:X\to\mathbb R$ is continuous $h:X\to\mathbb R$ is continuous and $h(X)=\mathbb R$ . $g:\mathbb R\to\mathbb R$ . Question: Can we say $g$ is also continuous? Motivation: Composition of continuous functions is also continuous. Does an ""inverse"" argument holds? After doing some research the argument seems to be correct for metric spaces with additional assumptions of one of the function being homeomorphism: If the composition of two functions is continuous and one of those functions is continuous, is the other function continuous as well? Note: superscripts in this proof are indexes not exponents. My test: I think if we let $X$ be path-connected, it is easy to prove that $g$ is continuous. Let $y_1<y_2$ be two real numbers and $h(x_i)=y_i$ , for any real number $y\in (y_1,y_2)$ , consider a sequence $y^n\in (y_1,y_2)$ converging to $y$ . We will show that $g$ is continuous at arbitrary real number $y$ . Let $T$ be the path between $x_1,x_2$ . It follows from the continuity of $h$ that for each $n$ there exists $x^n\in T$ such that $h(x^n)=y$ . Because of path-connectedness, path $T$ is compact. So sequence $x^n$ converges to $x$ where $f(x)=y$ . By the continuity of $f$ , it follows that $f(x^n)$ converges to $f(x)$ and $g(h(x^n))$ converges to $g(h(x))$ . Finally, we conclude that $h(x^n)\to h(x)$ $\implies g(h(x^n))\to g(h(x))$ . However, I think this method only works for path-connected $X$ not connected $X$ . Could you please give me some hints or a counterexample?","Structural assumptions: is a nontrivial connected separable topological space Given: . is continuous is continuous and . . Question: Can we say is also continuous? Motivation: Composition of continuous functions is also continuous. Does an ""inverse"" argument holds? After doing some research the argument seems to be correct for metric spaces with additional assumptions of one of the function being homeomorphism: If the composition of two functions is continuous and one of those functions is continuous, is the other function continuous as well? Note: superscripts in this proof are indexes not exponents. My test: I think if we let be path-connected, it is easy to prove that is continuous. Let be two real numbers and , for any real number , consider a sequence converging to . We will show that is continuous at arbitrary real number . Let be the path between . It follows from the continuity of that for each there exists such that . Because of path-connectedness, path is compact. So sequence converges to where . By the continuity of , it follows that converges to and converges to . Finally, we conclude that . However, I think this method only works for path-connected not connected . Could you please give me some hints or a counterexample?","X f(x)=g(h(x)) f:X\to\mathbb R h:X\to\mathbb R h(X)=\mathbb R g:\mathbb R\to\mathbb R g X g y_1<y_2 h(x_i)=y_i y\in (y_1,y_2) y^n\in (y_1,y_2) y g y T x_1,x_2 h n x^n\in T h(x^n)=y T x^n x f(x)=y f f(x^n) f(x) g(h(x^n)) g(h(x)) h(x^n)\to h(x) \implies g(h(x^n))\to g(h(x)) X X","['real-analysis', 'general-topology', 'continuity', 'connectedness', 'function-and-relation-composition']"
5,"Evaluate $\int_{0}^{\infty} \ln(1+\frac{2\cos x}{x^2} +\frac{1}{x^4}) \, dx$",Evaluate,"\int_{0}^{\infty} \ln(1+\frac{2\cos x}{x^2} +\frac{1}{x^4}) \, dx","Numerical evidence suggests the following $$I=\int_{0}^{\infty} \ln\left(1+\frac{2\cos x}{x^2} +\frac{1}{x^4}\right) \, dx\stackrel{?}{=}4\pi W\left(\frac{1}{2}\right)=4.42\cdots$$ where $W(x)$ is the Lambert $W$ (product log) function. Here’s the context for the problem: I was playing around with other $\ln$ integrals, and evaluated the following: $$\int_{0}^{\infty} \ln \left(1+2x^{-2} \cos(\phi)+x^{-4}\right)\,dx=2\pi\cos \left(\frac{\phi}{2}\right)$$ There are several questions on MSE on similar integrals to the above, such as this one . The above integral can be evaluated in several ways, but I just chose to introduce a $\ln(z)$ integral representation. I was curious to know what the integral would instead be if I replaced $\cos \phi$ with $\cos x$ . $I$ converges because it is bounded above by the integral $\int_{0}^{\infty} \ln (1+2x^{-2}+x^{-4})\,dx=2\pi$ . Here are my attempts: Although I’m much more comfortable with real analysis, I currently have no method that I can apply real analytic techniques or representations on $I$ . I tried instead contour integration: Firstly notice that $$I \stackrel{\text{IBP}}{=}2\int_{0}^{\infty} \frac{2+2x^2 \cos x+x^3 \sin x}{1+x^4 +2x^2 \cos x} \, dx$$ I then converted these trig functions into exponential forms and factorised the denominator to determine where the poles are- they are the four solutions to $x^2+e^{i x} = 0$ and $x^2+e^{-i x}=0$ which are $$x=\pm 2i W\left(\pm \frac{1}{2}\right)$$ I then took a semi-circular arc in the upper half plane to be my contour as the integrand is even, and computed the following residues: $$\text{Res} \left[f(z),\,2i W\left(\frac{1}{2}\right)\right] =-2i W\left(\frac{1}{2}\right)$$ $$\text{Res} \left[f(z),\,-2i W\left(-\frac{1}{2}\right)\right] =2i W\left(-\frac{1}{2}\right)$$ where $f(x)$ is the integrand function above. So we have then that $$\oint_C f(z) \, dz=2I + \int_{\Gamma} f(z) \, dz = -4\pi \left(W\left(-\frac{1}{2}\right) -W\left(\frac{1}{2}\right)\right)$$ where $\int_{\Gamma}$ traces the semi-circular arc path. Here is where I’m stuck however because I’m not sure how to evaluate this arc integral.","Numerical evidence suggests the following where is the Lambert (product log) function. Here’s the context for the problem: I was playing around with other integrals, and evaluated the following: There are several questions on MSE on similar integrals to the above, such as this one . The above integral can be evaluated in several ways, but I just chose to introduce a integral representation. I was curious to know what the integral would instead be if I replaced with . converges because it is bounded above by the integral . Here are my attempts: Although I’m much more comfortable with real analysis, I currently have no method that I can apply real analytic techniques or representations on . I tried instead contour integration: Firstly notice that I then converted these trig functions into exponential forms and factorised the denominator to determine where the poles are- they are the four solutions to and which are I then took a semi-circular arc in the upper half plane to be my contour as the integrand is even, and computed the following residues: where is the integrand function above. So we have then that where traces the semi-circular arc path. Here is where I’m stuck however because I’m not sure how to evaluate this arc integral.","I=\int_{0}^{\infty} \ln\left(1+\frac{2\cos x}{x^2} +\frac{1}{x^4}\right) \, dx\stackrel{?}{=}4\pi W\left(\frac{1}{2}\right)=4.42\cdots W(x) W \ln \int_{0}^{\infty} \ln \left(1+2x^{-2} \cos(\phi)+x^{-4}\right)\,dx=2\pi\cos \left(\frac{\phi}{2}\right) \ln(z) \cos \phi \cos x I \int_{0}^{\infty} \ln (1+2x^{-2}+x^{-4})\,dx=2\pi I I \stackrel{\text{IBP}}{=}2\int_{0}^{\infty} \frac{2+2x^2 \cos x+x^3 \sin x}{1+x^4 +2x^2 \cos x} \, dx x^2+e^{i x} = 0 x^2+e^{-i x}=0 x=\pm 2i W\left(\pm \frac{1}{2}\right) \text{Res} \left[f(z),\,2i W\left(\frac{1}{2}\right)\right] =-2i W\left(\frac{1}{2}\right) \text{Res} \left[f(z),\,-2i W\left(-\frac{1}{2}\right)\right] =2i W\left(-\frac{1}{2}\right) f(x) \oint_C f(z) \, dz=2I + \int_{\Gamma} f(z) \, dz = -4\pi \left(W\left(-\frac{1}{2}\right) -W\left(\frac{1}{2}\right)\right) \int_{\Gamma}","['real-analysis', 'calculus', 'integration', 'complex-analysis', 'analysis']"
6,"Zero derivative implies constant function (No MVT, Rolle's Theorem, etc.)","Zero derivative implies constant function (No MVT, Rolle's Theorem, etc.)",,"I'm working through Foundations of Mathematical Analysis by Johnsonbaugh. The following problem is given before MVT is proven. However, we do know about compactness, completeness, continuity, intermediate value theorem, metric spaces, sequences, etc. I am trying to find a proof that does not use MVT. This question seems similar to mine, but they end up using something equivalent to Rolle's theorem in the answer. Problem: Suppose $f'(x) = 0$ for all $x \in (a,b)$. Prove that $f$ is constant on $(a,b)$. Attempt: Let $x,y \in (a,b)$. We have $$f'(x) = \lim_{z \to x} \frac{f(z)-f(x)}{z-x} = 0$$ and $$f'(y) = \lim_{z \to y} \frac{f(z)-f(y)}{z-y} = 0$$","I'm working through Foundations of Mathematical Analysis by Johnsonbaugh. The following problem is given before MVT is proven. However, we do know about compactness, completeness, continuity, intermediate value theorem, metric spaces, sequences, etc. I am trying to find a proof that does not use MVT. This question seems similar to mine, but they end up using something equivalent to Rolle's theorem in the answer. Problem: Suppose $f'(x) = 0$ for all $x \in (a,b)$. Prove that $f$ is constant on $(a,b)$. Attempt: Let $x,y \in (a,b)$. We have $$f'(x) = \lim_{z \to x} \frac{f(z)-f(x)}{z-x} = 0$$ and $$f'(y) = \lim_{z \to y} \frac{f(z)-f(y)}{z-y} = 0$$",,"['real-analysis', 'derivatives', 'continuity']"
7,Graphical explanation of the difference between $C^1$ and $C^2$ function?,Graphical explanation of the difference between  and  function?,C^1 C^2,"We are all aware of the intuitive (graphical) explanation of the concepts of continuous and differentiable function. Whenever these two concepts are formally defined, the following elementary explanations are given: A continuous function is a function whose graph has no ""holes"" or ""jumps"", and a differentiable function is a function whose graph has no ""corners"". This is a non continuous function: This is a non differentiable continuous function: And this is a differentiable continuous function: Is there a ""graphical"" or intuitive explanation of the difference between a $C^1$ function and a differentiable function with discontinuous derivatives? What about a function that is $C^1$ but not $C^2$ because it does not have second derivatives? Or what about a function that is $C^1$ and has second derivatives but they are not continuous? What about the difference between a $C^1$ function and a $C^{\infty}$ function?","We are all aware of the intuitive (graphical) explanation of the concepts of continuous and differentiable function. Whenever these two concepts are formally defined, the following elementary explanations are given: A continuous function is a function whose graph has no ""holes"" or ""jumps"", and a differentiable function is a function whose graph has no ""corners"". This is a non continuous function: This is a non differentiable continuous function: And this is a differentiable continuous function: Is there a ""graphical"" or intuitive explanation of the difference between a $C^1$ function and a differentiable function with discontinuous derivatives? What about a function that is $C^1$ but not $C^2$ because it does not have second derivatives? Or what about a function that is $C^1$ and has second derivatives but they are not continuous? What about the difference between a $C^1$ function and a $C^{\infty}$ function?",,"['real-analysis', 'graphing-functions', 'intuition', 'visualization', 'elementary-functions']"
8,Let ($x_n$) be a monotone sequence and contain a convergent subsequence. Prove that ($x_n$) is convergent.,Let () be a monotone sequence and contain a convergent subsequence. Prove that () is convergent.,x_n x_n,"Let ($x_n$) be a monotone sequence and contain a convergent subsequence. Prove that ($x_n$) is convergent. I know that by the Bolzano-Weierstrass Theorem, every bounded sequence has a convergent subsequence. But I need some hints as to how to prove the question above. Thanks for your help!","Let ($x_n$) be a monotone sequence and contain a convergent subsequence. Prove that ($x_n$) is convergent. I know that by the Bolzano-Weierstrass Theorem, every bounded sequence has a convergent subsequence. But I need some hints as to how to prove the question above. Thanks for your help!",,['real-analysis']
9,Continuous function and Hausdorff space,Continuous function and Hausdorff space,,"The question is: $f: X \to Y$ is continuous bijection, which of the following is   correct: I. if $X$ is Hausdorff space then $Y$ is Hausdorff space. II. if $X$ is compact and $Y$ is Hausdorff space, then $f^{-1}$ exist. I think I is correct since $X$, the Hausdorff is separable, then image should be separable. I'm not sure about II. Besides, can you show me some example and counterexample of Hausdorff space? Thank you.","The question is: $f: X \to Y$ is continuous bijection, which of the following is   correct: I. if $X$ is Hausdorff space then $Y$ is Hausdorff space. II. if $X$ is compact and $Y$ is Hausdorff space, then $f^{-1}$ exist. I think I is correct since $X$, the Hausdorff is separable, then image should be separable. I'm not sure about II. Besides, can you show me some example and counterexample of Hausdorff space? Thank you.",,['real-analysis']
10,Approximations for the partial sums of exponential series,Approximations for the partial sums of exponential series,,"Though the question here ( Partial sums of exponential series - Stack Exchange ) is similar, it is more specialized and I rather need a general approximation for an arbitrary partial sum. Essentially, I am trying to approximate the probability mass function of a particular random variable and I ended up with a Poisson random variable's CDF in the mix. Hence, for my purpose, I need to figure out a reasonable approximation of the sum: $\displaystyle\sum_{k = 0}^{r} \frac{\lambda^k}{k!}$ OR the tail, i.e. $\displaystyle\sum_{k = r}^{\infty} \frac{\lambda^k}{k!}$ Does someone know some approximations for this? Also, if there exist conditions for those approximations to be valid, I'd like to know them as well. Thanks in advance! Addendum: There appears to be a closed form expression for such a partial sum: $\displaystyle\sum_{k = 0}^{r} \frac{\lambda^k}{k!} = e^\lambda \frac{\Gamma(r + 1, \lambda)}{\Gamma(r + 1)}$, where $\Gamma(a, x)$ is defined as: $\displaystyle \Gamma(a, x) = \int_x^\infty t^{a - 1} e^{-t} \,dt$ and $\displaystyle \Gamma(a) = \Gamma(a, 0)$. Is there a simple closed form approximation for the Gamma function? At the end of the day, somehow or the other, I either end up with a summation sign or an integral. I just want to be able to pin down this partial sum as a numeric quantity, that is reasonably approximate.","Though the question here ( Partial sums of exponential series - Stack Exchange ) is similar, it is more specialized and I rather need a general approximation for an arbitrary partial sum. Essentially, I am trying to approximate the probability mass function of a particular random variable and I ended up with a Poisson random variable's CDF in the mix. Hence, for my purpose, I need to figure out a reasonable approximation of the sum: $\displaystyle\sum_{k = 0}^{r} \frac{\lambda^k}{k!}$ OR the tail, i.e. $\displaystyle\sum_{k = r}^{\infty} \frac{\lambda^k}{k!}$ Does someone know some approximations for this? Also, if there exist conditions for those approximations to be valid, I'd like to know them as well. Thanks in advance! Addendum: There appears to be a closed form expression for such a partial sum: $\displaystyle\sum_{k = 0}^{r} \frac{\lambda^k}{k!} = e^\lambda \frac{\Gamma(r + 1, \lambda)}{\Gamma(r + 1)}$, where $\Gamma(a, x)$ is defined as: $\displaystyle \Gamma(a, x) = \int_x^\infty t^{a - 1} e^{-t} \,dt$ and $\displaystyle \Gamma(a) = \Gamma(a, 0)$. Is there a simple closed form approximation for the Gamma function? At the end of the day, somehow or the other, I either end up with a summation sign or an integral. I just want to be able to pin down this partial sum as a numeric quantity, that is reasonably approximate.",,"['real-analysis', 'approximation']"
11,$\cos(x)$ and $\arccos(x)$ couple limit,and  couple limit,\cos(x) \arccos(x),Find the value of the following limit: $$\lim_{n\to\infty} \frac {\cos 1 \cdot \arccos \frac{1}{n}+\cos\frac {1}{2} \cdot \arccos \frac{1}{(n-1)}+ \cdots +\cos \frac{1}{n} \cdot  \arccos{1}}{n}$$,Find the value of the following limit: $$\lim_{n\to\infty} \frac {\cos 1 \cdot \arccos \frac{1}{n}+\cos\frac {1}{2} \cdot \arccos \frac{1}{(n-1)}+ \cdots +\cos \frac{1}{n} \cdot  \arccos{1}}{n}$$,,"['calculus', 'real-analysis', 'trigonometry', 'limits']"
12,"About example of two function which convolution is discontinuous on the ""big"" set of points","About example of two function which convolution is discontinuous on the ""big"" set of points",,"I want to ask about example of real valued functions defined on the real line such that their convolution exist in every point and is discontinuous on a ""large"" set, for example on  each point of some interval or in dense subset of $\mathbb{R}$ or maybe on the whole $\mathbb{R}$. My question is related to paper Mikusinski, Ryll-Nardzewski, Sur le produit de composition, http://matwbn.icm.edu.pl/ksiazki/sm/sm12/sm1213.pdf The authors consider convolution of integrable functions which are zero for $x\leq 0$. On page 52 the above paper is given example of  two integrable functions  such that their convolution is discontinuous at a point.  Autors say also, but there is no proof of this statement, that it is possible by condensation of singularities to construct integrable functions such that their product is discontinuous  everywhere (on $\mathbb{R_+}$). Thanks.","I want to ask about example of real valued functions defined on the real line such that their convolution exist in every point and is discontinuous on a ""large"" set, for example on  each point of some interval or in dense subset of $\mathbb{R}$ or maybe on the whole $\mathbb{R}$. My question is related to paper Mikusinski, Ryll-Nardzewski, Sur le produit de composition, http://matwbn.icm.edu.pl/ksiazki/sm/sm12/sm1213.pdf The authors consider convolution of integrable functions which are zero for $x\leq 0$. On page 52 the above paper is given example of  two integrable functions  such that their convolution is discontinuous at a point.  Autors say also, but there is no proof of this statement, that it is possible by condensation of singularities to construct integrable functions such that their product is discontinuous  everywhere (on $\mathbb{R_+}$). Thanks.",,"['real-analysis', 'analysis', 'functional-analysis']"
13,Opposite of a contraction mapping,Opposite of a contraction mapping,,"I am taking Real Analysis and we recently went over the Banach Fixed-point Theorem, also commonly known as the Contraction Mapping Theorem which states: If $(X,d)$ is a complete metric space, and $f:X\to X$ is a contraction, that is $f$ satisfies $$d(f(x),f(y)) \leq L d(x,y)$$ for every $x,y \in X$ and some fixed $L<1$, then $f$ has exactly one fixed point, i.e. there exists a unique $z \in X$ such that $f(z)=z$. I was thinking about a similar statement for what I can only guess are called expansion mappings. Suppose $(X,d)$ is a complete metric space and $f:X\to X$ satisfies $$ d(f(x),f(y)) \geq L d(x,y)$$ for some fixed $L>1$ and every $x,y \in X$ with $x\neq y$. Does $f$ necessarily have exactly one fixed point? I could not come up with a counterexample using real functions, though I haven't really had time (due to homework) to put much more thought into it. I googled ""expansion mapping"" and some other similar terms but there does not seem to be any useful source on the topic that I could find. I think this notion of expansion mapping is a natural one to consider after considering contraction mappings, so I don't know why there doesn't seem to be any available research on the topic. I have a few good ideas for how I would go about trying to prove this that might help. Firstly we note that $f$ must be injective, otherwise we would have two distinct points which get closer (distance zero) after applying the mapping which would be a contradiction. Thus $f$ is left invertible. If I had to guess, I would say that the left inverse of an expansion mapping must be a contraction mapping (with reciprocal constant $\frac{1}{L}$?). Then that contraction must have a fixed point by the Banach Fixed-point Theorem. Perhaps it can be shown that this must also be a fixed point of $f$ itself, I haven't taken the time to see whether fixed points are preserved using only one-sided invertibility. Any thoughts, ideas, research, or proofs on the topic of expansion mappings are welcome.","I am taking Real Analysis and we recently went over the Banach Fixed-point Theorem, also commonly known as the Contraction Mapping Theorem which states: If $(X,d)$ is a complete metric space, and $f:X\to X$ is a contraction, that is $f$ satisfies $$d(f(x),f(y)) \leq L d(x,y)$$ for every $x,y \in X$ and some fixed $L<1$, then $f$ has exactly one fixed point, i.e. there exists a unique $z \in X$ such that $f(z)=z$. I was thinking about a similar statement for what I can only guess are called expansion mappings. Suppose $(X,d)$ is a complete metric space and $f:X\to X$ satisfies $$ d(f(x),f(y)) \geq L d(x,y)$$ for some fixed $L>1$ and every $x,y \in X$ with $x\neq y$. Does $f$ necessarily have exactly one fixed point? I could not come up with a counterexample using real functions, though I haven't really had time (due to homework) to put much more thought into it. I googled ""expansion mapping"" and some other similar terms but there does not seem to be any useful source on the topic that I could find. I think this notion of expansion mapping is a natural one to consider after considering contraction mappings, so I don't know why there doesn't seem to be any available research on the topic. I have a few good ideas for how I would go about trying to prove this that might help. Firstly we note that $f$ must be injective, otherwise we would have two distinct points which get closer (distance zero) after applying the mapping which would be a contradiction. Thus $f$ is left invertible. If I had to guess, I would say that the left inverse of an expansion mapping must be a contraction mapping (with reciprocal constant $\frac{1}{L}$?). Then that contraction must have a fixed point by the Banach Fixed-point Theorem. Perhaps it can be shown that this must also be a fixed point of $f$ itself, I haven't taken the time to see whether fixed points are preserved using only one-sided invertibility. Any thoughts, ideas, research, or proofs on the topic of expansion mappings are welcome.",,"['real-analysis', 'metric-spaces', 'examples-counterexamples', 'fixed-point-theorems', 'contraction-mapping']"
14,Problem when proving the series $1+\frac 13-\frac 12+\frac 15+\frac17-\frac 14 +\frac 19+\frac 1{11}-\frac 16+...$ converges to $\frac 32\log 2.$,Problem when proving the series  converges to,1+\frac 13-\frac 12+\frac 15+\frac17-\frac 14 +\frac 19+\frac 1{11}-\frac 16+... \frac 32\log 2.,"Prove that the series $$1+\frac 13-\frac 12+\frac 15+\frac17-\frac 14 +\frac 19+\frac 1{11}-\frac 16+\cdots$$ converges to $\frac 32\log 2.$ I tried solving the problem as follows: The series given is $$1+\frac 13-\frac 12+\frac 15+\frac17-\frac 14 +\frac 19+\frac 1{11}-\frac 16+\cdots.$$ We write the given series as $\sum u_n$ where $\{u_n\}$ is a sequence defined by $u_n=\frac{1}{4n-3}+\frac{1}{4n-1}-\frac{1}{2n},\forall n\in\Bbb N.$ We consider the partial sum of the given  series $t_n=u_1+u_2+u_3+...+u_n$ ( consisting of $n$ terms). Then, $$t_{3n}=u_1+u_2+\cdots +u_{3n}=(1+\frac 13-\frac 12)+(\frac 15+\frac17-\frac 14) +(\frac 19+\frac 1{11}-\frac 16)+\cdots+(\frac{1}{12n-3}+\frac{1}{12n-1}-\frac{1}{6n})=(1+\frac 13+\frac 15+\frac 17+...+\frac{1}{12n-1})-(\frac 12+\frac 14+\frac 16+\cdots +\frac 1{6n})\tag 1.$$ (Till now, we have grouped all the odd terms (positive terms) together and the negative terms( even terms) together in the partial sum, above. ) Next, we add and subtract $\frac{1}{2}+\frac{1}{4}+\frac{1}{6}+...+\frac{1}{12n}$ in the right hand side of the above equality $(1)$ and rearrange, to get the partial sum $t_{3n}$ of the series as, $$t_{3n}=(1+\frac 13+\frac 15+\frac 17+...+\frac{1}{12n-1})-(\frac 12+\frac 14+\frac 16+\cdots +\frac 1{6n})=(1+\frac 12+\frac 13+\cdots +\frac{1}{12n-1})-(\frac 12+\frac 14+\frac 16+\cdots +\frac {1}{12n})-(\frac 12+\frac 14+\frac 16+\cdots +\frac 1{6n})=(1+\frac 12+\frac 13+\cdots +\frac{1}{12n-1})-2(\frac 12+\frac 14+\frac 16+\cdots +\frac {1}{6n})-(\frac1{6n+2}+\frac{1}{6n+4}+\cdots+\frac{1}{12n})=(1+\frac 12+\frac 13+\cdots +\frac{1}{12n-1})-( 1+\frac 12+\frac 13+\cdots +\frac {1}{3n})-(\frac1{6n+2}+\frac{1}{6n+4}+\cdots+\frac{1}{12n})\tag 2$$ We know that, $\lim(1+\frac 12+\frac 13+\cdots+ \frac 1n-\log n)=\gamma.$ Let, $1+\frac 12+\frac 13+\cdots+ \frac 1n-\log n=\gamma_n\tag 3$ then, $\lim\gamma_n=\gamma.$ Using $(3),$ the  equality $(2)$ can be written as, $$t_{3n}=(1+\frac 12+\frac 13+\cdots +\frac{1}{12n-1})-( 1+\frac 12+\frac 13+\cdots +\frac {1}{3n})-(\frac1{6n+2}+\frac{1}{6n+4}+\cdots+\frac{1}{12n})=\log(12n-1)+\gamma_{12n-1}-\log(3n)-\gamma_{3n}-S,$$ where $S=\frac1{6n+2}+\frac{1}{6n+4}+\cdots+\frac{1}{12n}.$ We also, note that, $\lim S=0.$ We now proceed to evaluate the limit of $t_{3n}=\log(12n-1)+\gamma_{12n-1}-\log(3n)-\gamma_{3n}-S=\log(4-\frac{1}{3n})+\gamma_{12n-1}-\gamma_{3n}+S.$ We observe, $$\lim t_{3n}=\log 4=2\log 2.$$ As, $t_{3n+1}=t_{3n}+u_{3n+1}$ and $t_{3n+2}=t_{3n+1}+u_{3n+2},$ we have, $$\lim t_{3n+1}=\lim t_{3n+2}=\lim t_{3n}=2\log 2.$$ So, $\lim t_n=2\log 2.$ Hence, the series $$1+\frac 13-\frac 12+\frac 15+\frac17-\frac 14 +\frac 19+\frac 1{11}-\frac 16+\cdots$$ converges to $2\log 2.$ However, as it turns out, this gives the value $2\log 2$ contrary to what was required to be established, i.e $\frac 32\log 2.$ The problem, is precisely, I find no mistake in my solution. I want to know where did things go wrong. Specifically, where is the mistake in this solution? A New Issue As pointed out, in the comment, by @metamorphy, it seems that the evaluation of the limit .i.e $\lim S=0$ is incorrect. It should be, as follows: $$S=\frac1{6n+2}+\frac{1}{6n+4}+\cdots+\frac{1}{12n}\implies 2S=\frac1{3n+1}+\frac1{3n+2}+\dots+\frac1{6n}\\=\left(1+\frac12+\dots+\frac1{6n}\right)-\left(1+\frac12+\dots+\frac1{3n}\right)\\=\big(\gamma_{6n}+\log(6n)\big)-\big(\gamma_{3n}+\log(3n)\big).$$ This means, $\lim 2S=\log 2,$ or $\lim S=\frac 12 \log 2.$ But here's the problem, what went wrong with this reasoning, i.e the reason by which, I concluded $\lim S$ is $0$ which is: $$S=\frac1{6n+2}+\frac{1}{6n+4}+\cdots+\frac{1}{12n},$$ so, limit of each of the terms $\lim\frac{1}{6n+r}=0$ , such that $r\in\{2,4,...,6n\}.$ This, means $\lim S=0.$ I want to know the mistake in this second limit evaluation explicitly ?","Prove that the series converges to I tried solving the problem as follows: The series given is We write the given series as where is a sequence defined by We consider the partial sum of the given  series ( consisting of terms). Then, (Till now, we have grouped all the odd terms (positive terms) together and the negative terms( even terms) together in the partial sum, above. ) Next, we add and subtract in the right hand side of the above equality and rearrange, to get the partial sum of the series as, We know that, Let, then, Using the  equality can be written as, where We also, note that, We now proceed to evaluate the limit of We observe, As, and we have, So, Hence, the series converges to However, as it turns out, this gives the value contrary to what was required to be established, i.e The problem, is precisely, I find no mistake in my solution. I want to know where did things go wrong. Specifically, where is the mistake in this solution? A New Issue As pointed out, in the comment, by @metamorphy, it seems that the evaluation of the limit .i.e is incorrect. It should be, as follows: This means, or But here's the problem, what went wrong with this reasoning, i.e the reason by which, I concluded is which is: so, limit of each of the terms , such that This, means I want to know the mistake in this second limit evaluation explicitly ?","1+\frac 13-\frac 12+\frac 15+\frac17-\frac 14 +\frac 19+\frac 1{11}-\frac 16+\cdots \frac 32\log 2. 1+\frac 13-\frac 12+\frac 15+\frac17-\frac 14 +\frac 19+\frac 1{11}-\frac 16+\cdots. \sum u_n \{u_n\} u_n=\frac{1}{4n-3}+\frac{1}{4n-1}-\frac{1}{2n},\forall n\in\Bbb N. t_n=u_1+u_2+u_3+...+u_n n t_{3n}=u_1+u_2+\cdots +u_{3n}=(1+\frac 13-\frac 12)+(\frac 15+\frac17-\frac 14) +(\frac 19+\frac 1{11}-\frac 16)+\cdots+(\frac{1}{12n-3}+\frac{1}{12n-1}-\frac{1}{6n})=(1+\frac 13+\frac 15+\frac 17+...+\frac{1}{12n-1})-(\frac 12+\frac 14+\frac 16+\cdots +\frac 1{6n})\tag 1. \frac{1}{2}+\frac{1}{4}+\frac{1}{6}+...+\frac{1}{12n} (1) t_{3n} t_{3n}=(1+\frac 13+\frac 15+\frac 17+...+\frac{1}{12n-1})-(\frac 12+\frac 14+\frac 16+\cdots +\frac 1{6n})=(1+\frac 12+\frac 13+\cdots +\frac{1}{12n-1})-(\frac 12+\frac 14+\frac 16+\cdots +\frac {1}{12n})-(\frac 12+\frac 14+\frac 16+\cdots +\frac 1{6n})=(1+\frac 12+\frac 13+\cdots +\frac{1}{12n-1})-2(\frac 12+\frac 14+\frac 16+\cdots +\frac {1}{6n})-(\frac1{6n+2}+\frac{1}{6n+4}+\cdots+\frac{1}{12n})=(1+\frac 12+\frac 13+\cdots +\frac{1}{12n-1})-( 1+\frac 12+\frac 13+\cdots +\frac {1}{3n})-(\frac1{6n+2}+\frac{1}{6n+4}+\cdots+\frac{1}{12n})\tag 2 \lim(1+\frac 12+\frac 13+\cdots+ \frac 1n-\log n)=\gamma. 1+\frac 12+\frac 13+\cdots+ \frac 1n-\log n=\gamma_n\tag 3 \lim\gamma_n=\gamma. (3), (2) t_{3n}=(1+\frac 12+\frac 13+\cdots +\frac{1}{12n-1})-( 1+\frac 12+\frac 13+\cdots +\frac {1}{3n})-(\frac1{6n+2}+\frac{1}{6n+4}+\cdots+\frac{1}{12n})=\log(12n-1)+\gamma_{12n-1}-\log(3n)-\gamma_{3n}-S, S=\frac1{6n+2}+\frac{1}{6n+4}+\cdots+\frac{1}{12n}. \lim S=0. t_{3n}=\log(12n-1)+\gamma_{12n-1}-\log(3n)-\gamma_{3n}-S=\log(4-\frac{1}{3n})+\gamma_{12n-1}-\gamma_{3n}+S. \lim t_{3n}=\log 4=2\log 2. t_{3n+1}=t_{3n}+u_{3n+1} t_{3n+2}=t_{3n+1}+u_{3n+2}, \lim t_{3n+1}=\lim t_{3n+2}=\lim t_{3n}=2\log 2. \lim t_n=2\log 2. 1+\frac 13-\frac 12+\frac 15+\frac17-\frac 14 +\frac 19+\frac 1{11}-\frac 16+\cdots 2\log 2. 2\log 2 \frac 32\log 2. \lim S=0 S=\frac1{6n+2}+\frac{1}{6n+4}+\cdots+\frac{1}{12n}\implies 2S=\frac1{3n+1}+\frac1{3n+2}+\dots+\frac1{6n}\\=\left(1+\frac12+\dots+\frac1{6n}\right)-\left(1+\frac12+\dots+\frac1{3n}\right)\\=\big(\gamma_{6n}+\log(6n)\big)-\big(\gamma_{3n}+\log(3n)\big). \lim 2S=\log 2, \lim S=\frac 12 \log 2. \lim S 0 S=\frac1{6n+2}+\frac{1}{6n+4}+\cdots+\frac{1}{12n}, \lim\frac{1}{6n+r}=0 r\in\{2,4,...,6n\}. \lim S=0.","['real-analysis', 'sequences-and-series']"
15,"Is every ""almost everywhere derivative"" Henstock–Kurzweil integrable?","Is every ""almost everywhere derivative"" Henstock–Kurzweil integrable?",,"It is well known that the Henstock–Kurzweil integral fixes a lot of issues with trying to integrate derivatives. The second fundamental theorem of calculus for this integral states: Given that $f : [a,b] \rightarrow \mathbb{R}$ is a continuous function. If $f$ is differentiable co-countably everywhere (in other words: differentiable everywhere except for possibly a countable set of points), then: $f'$ is Henstock-Kurzweil integrable $\int_a^bf'(x) dx = f(b) - f(a)$ My question is what happens if you replace ""co-countably everywhere"" with ""almost everywhere""? Clearly the second statement no longer holds (the Cantor function provides a counter-example), but what about the first statement? If $f$ is continuous everywhere and differentiable almost everywhere, is $f'$ necessarily Henstock-Kurzweil integrable?","It is well known that the Henstock–Kurzweil integral fixes a lot of issues with trying to integrate derivatives. The second fundamental theorem of calculus for this integral states: Given that is a continuous function. If is differentiable co-countably everywhere (in other words: differentiable everywhere except for possibly a countable set of points), then: is Henstock-Kurzweil integrable My question is what happens if you replace ""co-countably everywhere"" with ""almost everywhere""? Clearly the second statement no longer holds (the Cantor function provides a counter-example), but what about the first statement? If is continuous everywhere and differentiable almost everywhere, is necessarily Henstock-Kurzweil integrable?","f : [a,b] \rightarrow \mathbb{R} f f' \int_a^bf'(x) dx = f(b) - f(a) f f'","['real-analysis', 'integration', 'analysis', 'derivatives', 'gauge-integral']"
16,Binomial analogue of Riemann sum for definite integral,Binomial analogue of Riemann sum for definite integral,,I found this interesting relationship which is an analogue of the Riemann sum for definite integral. $$ \lim_{n \to \infty}\frac{1}{n}\sum_{r = 1}^n f\Big(\frac{r}{n}\Big) = \int_{0}^{1}f(x) dx $$ $$ \lim_{n \to \infty}\frac{1}{2^n}\sum_{r = 1}^n {n \choose  r}f\Big(\frac{r}{n}\Big) = f\Big(\frac{1}{2}\Big) $$ Application . $$ \lim_{n \to \infty}\frac{1}{2^n}\sum_{r = 1}^n {n \choose r}\Gamma\Big(\frac{r}{n}\Big) = \sqrt{\pi} $$ $$ \lim_{n \to \infty}\frac{1}{2^n}\sum_{r = 1}^n {n \choose r}\arcsin\Big(\frac{r}{n}\Big) = \frac{\pi}{6} $$ Question : Any reference to this in mathematics literature? A generalization of this problem is posted here.,I found this interesting relationship which is an analogue of the Riemann sum for definite integral. Application . Question : Any reference to this in mathematics literature? A generalization of this problem is posted here.,"
\lim_{n \to \infty}\frac{1}{n}\sum_{r = 1}^n f\Big(\frac{r}{n}\Big) = \int_{0}^{1}f(x) dx
  \lim_{n \to \infty}\frac{1}{2^n}\sum_{r = 1}^n {n \choose
 r}f\Big(\frac{r}{n}\Big) = f\Big(\frac{1}{2}\Big)  
\lim_{n \to \infty}\frac{1}{2^n}\sum_{r = 1}^n {n \choose r}\Gamma\Big(\frac{r}{n}\Big) = \sqrt{\pi}
 
\lim_{n \to \infty}\frac{1}{2^n}\sum_{r = 1}^n {n \choose r}\arcsin\Big(\frac{r}{n}\Big) = \frac{\pi}{6}
","['real-analysis', 'calculus', 'number-theory', 'limits', 'binomial-coefficients']"
17,Difference of convexity and strict convexity,Difference of convexity and strict convexity,,"I know what is a function convex and what is a strict convex function. But I was wondering what is concretely the difference, haw can we differentiate both on a graph ? For example, I know that a convex function is always upper that any tangent, i.e. $f(x)\geq f(y)+f'(y)(x-y)$ for all $x$ and all $y$. I unfortunately don't thing that it make sense to says that it's strictly upper it's tangent since for $x=y$ we always have $f(x)=f(y)+f'(y)(x-y)$. So, is there a way to distinguish convexity and strict convexity just by watching the graph of a function ? And if yes, how ? What can be the specific characteristic of a strict convex function that a convex function doesn't have ? (despite the strict inequality at the definition).","I know what is a function convex and what is a strict convex function. But I was wondering what is concretely the difference, haw can we differentiate both on a graph ? For example, I know that a convex function is always upper that any tangent, i.e. $f(x)\geq f(y)+f'(y)(x-y)$ for all $x$ and all $y$. I unfortunately don't thing that it make sense to says that it's strictly upper it's tangent since for $x=y$ we always have $f(x)=f(y)+f'(y)(x-y)$. So, is there a way to distinguish convexity and strict convexity just by watching the graph of a function ? And if yes, how ? What can be the specific characteristic of a strict convex function that a convex function doesn't have ? (despite the strict inequality at the definition).",,"['real-analysis', 'convex-analysis']"
18,Showing that all monotone functions are integrable,Showing that all monotone functions are integrable,,"I am given the following proof: Theorem. All monotone functions are integrable. Proof. Without loss of generality, assume that $f$ is increasing on an interval $\left[a, b \right]$.  Thus, $f(a) \le f(x) \le f(b)$, and $f$ is bounded on $\left[a, b \right]$.  Given $\varepsilon >0$, there exists $k > 0$ such that \begin{equation*} k \left[f(b) - f(a) \right] < \varepsilon. \end{equation*} Let $P = \left\lbrace x_0, x_1, \dots, x_n \right\rbrace$ be a partition of $\left[a, b \right]$ such that $\Delta x_i \le k$ for all $i$. Since $f$ is increasing, it follows that \begin{equation*} m_i = f(x_{i-1}) \ \text{and} \ M_i = f(x_i), \quad i = 1, 2, \dots, n. \end{equation*} Where $m_i$ is the greatest lower bound of $f$ on $\left[ x_{i-1}, x_i \right]$, and $M_i$ is the least upper bound of $f$ on $\left[ x_{i-1}, x_i \right]$. $U(f, P) - L(f, P) = \sum_{i=1}^n \left[ f(x_i) - f(x_{i-1}) \right] \Delta x_i$ $\le k \sum_{i=1}^n \left[ f(x_i) - f(x_{i-1}) \right] (*)$ $= k \left[f(b) - f(a) \right] $ $< \varepsilon.$ By Theorem 7.1.9 $f$ is integrable on $\left[ a, b \right]$. In the case that $f$ is monotone decreasing, we may use the same argument on $-f$. I am just wondering if somebody could explain how we get from the line marked by (*) to the line after that.","I am given the following proof: Theorem. All monotone functions are integrable. Proof. Without loss of generality, assume that $f$ is increasing on an interval $\left[a, b \right]$.  Thus, $f(a) \le f(x) \le f(b)$, and $f$ is bounded on $\left[a, b \right]$.  Given $\varepsilon >0$, there exists $k > 0$ such that \begin{equation*} k \left[f(b) - f(a) \right] < \varepsilon. \end{equation*} Let $P = \left\lbrace x_0, x_1, \dots, x_n \right\rbrace$ be a partition of $\left[a, b \right]$ such that $\Delta x_i \le k$ for all $i$. Since $f$ is increasing, it follows that \begin{equation*} m_i = f(x_{i-1}) \ \text{and} \ M_i = f(x_i), \quad i = 1, 2, \dots, n. \end{equation*} Where $m_i$ is the greatest lower bound of $f$ on $\left[ x_{i-1}, x_i \right]$, and $M_i$ is the least upper bound of $f$ on $\left[ x_{i-1}, x_i \right]$. $U(f, P) - L(f, P) = \sum_{i=1}^n \left[ f(x_i) - f(x_{i-1}) \right] \Delta x_i$ $\le k \sum_{i=1}^n \left[ f(x_i) - f(x_{i-1}) \right] (*)$ $= k \left[f(b) - f(a) \right] $ $< \varepsilon.$ By Theorem 7.1.9 $f$ is integrable on $\left[ a, b \right]$. In the case that $f$ is monotone decreasing, we may use the same argument on $-f$. I am just wondering if somebody could explain how we get from the line marked by (*) to the line after that.",,"['real-analysis', 'integration']"
19,How can I show a supremum of a set is also its limit point?,How can I show a supremum of a set is also its limit point?,,It just seems like I can't wrap my head around this. For a set $A$ how can I show that the supremum of that set $\sup(A)$ is also a limit point of said set? Can I just set $$a=\sup(A)$$ $$b\le a \: \:  \forall \: b \in A$$ and then show that there's a point different from $a$ in any $\epsilon$ -neighbourhood for any $\epsilon>0$ ? Or should I use a completely different approach?,It just seems like I can't wrap my head around this. For a set how can I show that the supremum of that set is also a limit point of said set? Can I just set and then show that there's a point different from in any -neighbourhood for any ? Or should I use a completely different approach?,A \sup(A) a=\sup(A) b\le a \: \:  \forall \: b \in A a \epsilon \epsilon>0,"['real-analysis', 'general-topology', 'supremum-and-infimum']"
20,Uniformly bounded derivative implies uniform convergence,Uniformly bounded derivative implies uniform convergence,,"Let $f_n$ be a sequence of differentiable functions on $[a, b] \subset \mathbb{R}$. Suppose $\lim_{n \rightarrow \infty} f_n(x) = f(x)$ exists for all $x \in [a, b]$, and the derivatives $|f_n'(x)| < M$ are uniformly bounded over $n$ and $x$ Prove that $f_n$ converges to $f$ uniformly. The closest I can get is to try to adapt Arzela-Ascoli somehow, but it is not working. I can conclude the existence of a subsequence that converges uniformly, but this doesn't guarantee the original sequence converges uniformly.","Let $f_n$ be a sequence of differentiable functions on $[a, b] \subset \mathbb{R}$. Suppose $\lim_{n \rightarrow \infty} f_n(x) = f(x)$ exists for all $x \in [a, b]$, and the derivatives $|f_n'(x)| < M$ are uniformly bounded over $n$ and $x$ Prove that $f_n$ converges to $f$ uniformly. The closest I can get is to try to adapt Arzela-Ascoli somehow, but it is not working. I can conclude the existence of a subsequence that converges uniformly, but this doesn't guarantee the original sequence converges uniformly.",,['real-analysis']
21,Computing $\lim_{n\to \infty}{\frac{5\cdot9\cdot13\cdot\dots.\cdot(4n+1)}{7\cdot11\cdot15\cdot\dots.\cdot(4n+3)}}$,Computing,\lim_{n\to \infty}{\frac{5\cdot9\cdot13\cdot\dots.\cdot(4n+1)}{7\cdot11\cdot15\cdot\dots.\cdot(4n+3)}},"Let $\{a_n\}_{n\ge1}^{\infty}=\bigg\{\cfrac{5\cdot9\cdot13\cdot\dots.\cdot(4n+1)}{7\cdot11\cdot15\cdot\dots.\cdot(4n+3)}\bigg\}$. Find $\lim_{n\to \infty}{a_n}$. I.) In the first step I studied monotony: $a_{n+1}-a_{n}=\cfrac{5\cdot9\cdot13\cdot\dots.\cdot(4n+1)}{7\cdot11\cdot15\cdot\dots.\cdot(4n+3)}\cdot\cfrac{-2}{4n+7}<0$, $\{a_n\}$ is decreasing. II.) In the second step I studied boundary. $$1>a_{1}=\cfrac{5}{7}>a_{2}=\cfrac{45}{77}>\dots>a_{n}>0$$ III.) In the last step I know that $\{a_n\}$ converges to $a\in\mathbb R$. $$a_{n+1}=a_{n}\cdot\cfrac{4n+1}{4n+3}$$ Taking the limit as $n\to\infty$: $$a=a$$ No conclusion. But if I apply Cesaro-Stolz? IV.) Let $\{x_n\}_{n\ge1}^{\infty}=\{5\cdot9\cdot13\cdot\dots.\cdot(4n+1)\}$ and $\{y_n\}_{n\ge1}^{\infty}=\{7\cdot11\cdot15\cdot\dots.\cdot(4n+3)\}$. Then $$\lim_{n\to \infty}{\cfrac{x_{n+1}-x_{n}}{y_{n+1}-y_{n}}=\lim_{n\to \infty}{\cfrac{5\cdot9\cdot13\cdot\dots.\cdot(4n+1)^2}{7\cdot11\cdot15\cdot\dots.\cdot(4n+5)}=?}}$$ If you have a simple solution, I would appreciate it. Thank you!","Let $\{a_n\}_{n\ge1}^{\infty}=\bigg\{\cfrac{5\cdot9\cdot13\cdot\dots.\cdot(4n+1)}{7\cdot11\cdot15\cdot\dots.\cdot(4n+3)}\bigg\}$. Find $\lim_{n\to \infty}{a_n}$. I.) In the first step I studied monotony: $a_{n+1}-a_{n}=\cfrac{5\cdot9\cdot13\cdot\dots.\cdot(4n+1)}{7\cdot11\cdot15\cdot\dots.\cdot(4n+3)}\cdot\cfrac{-2}{4n+7}<0$, $\{a_n\}$ is decreasing. II.) In the second step I studied boundary. $$1>a_{1}=\cfrac{5}{7}>a_{2}=\cfrac{45}{77}>\dots>a_{n}>0$$ III.) In the last step I know that $\{a_n\}$ converges to $a\in\mathbb R$. $$a_{n+1}=a_{n}\cdot\cfrac{4n+1}{4n+3}$$ Taking the limit as $n\to\infty$: $$a=a$$ No conclusion. But if I apply Cesaro-Stolz? IV.) Let $\{x_n\}_{n\ge1}^{\infty}=\{5\cdot9\cdot13\cdot\dots.\cdot(4n+1)\}$ and $\{y_n\}_{n\ge1}^{\infty}=\{7\cdot11\cdot15\cdot\dots.\cdot(4n+3)\}$. Then $$\lim_{n\to \infty}{\cfrac{x_{n+1}-x_{n}}{y_{n+1}-y_{n}}=\lim_{n\to \infty}{\cfrac{5\cdot9\cdot13\cdot\dots.\cdot(4n+1)^2}{7\cdot11\cdot15\cdot\dots.\cdot(4n+5)}=?}}$$ If you have a simple solution, I would appreciate it. Thank you!",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits', 'infinite-product']"
22,Essential Supremum with the continuous function?,Essential Supremum with the continuous function?,,"I have a problem when I read about the Essential Supremum of a measurable function.  Let $f: E\longrightarrow \mathbb{R}$ is a measurable function respect $E$ is Lebesuge measurable set and the Lebesgue measure. Let $$ \operatorname{ess sup} f = \inf\{z: f\leq z\;\text{almost everywhere}\}$$ We always have $\operatorname{ess sup} f \leq \sup f$, but when $f$ is continuous, why $\operatorname{ess sup} f = \sup f$? I assume that $\operatorname{ess sup} f < \sup f = \alpha < \infty$, then exist $z$ such that $f\leq z$ almost everywhere and $z < \sup f = \alpha$, then the set $A = \{x\in E: f(x) > z\}$ is a null set, so the set $B = \{x\in E: \alpha > f(x) > z\}$ is also a null set, but $B = f^{-1}\big((z,\alpha)\big)$ is an open set in $E$. But since a set $B$ is open in $E$ if and only if exist an open set $V \subset \mathbb{R}$ such that  $$ B = V\cap E$$ Now, I can't find any contradiction?, for exmaple, if $E = \mathbb{R}$ then I have a contradiction since $B$ is open and not empty which can't be a null set. But if $E$ is a null set of $\mathbb{R}$, why is contradiction?","I have a problem when I read about the Essential Supremum of a measurable function.  Let $f: E\longrightarrow \mathbb{R}$ is a measurable function respect $E$ is Lebesuge measurable set and the Lebesgue measure. Let $$ \operatorname{ess sup} f = \inf\{z: f\leq z\;\text{almost everywhere}\}$$ We always have $\operatorname{ess sup} f \leq \sup f$, but when $f$ is continuous, why $\operatorname{ess sup} f = \sup f$? I assume that $\operatorname{ess sup} f < \sup f = \alpha < \infty$, then exist $z$ such that $f\leq z$ almost everywhere and $z < \sup f = \alpha$, then the set $A = \{x\in E: f(x) > z\}$ is a null set, so the set $B = \{x\in E: \alpha > f(x) > z\}$ is also a null set, but $B = f^{-1}\big((z,\alpha)\big)$ is an open set in $E$. But since a set $B$ is open in $E$ if and only if exist an open set $V \subset \mathbb{R}$ such that  $$ B = V\cap E$$ Now, I can't find any contradiction?, for exmaple, if $E = \mathbb{R}$ then I have a contradiction since $B$ is open and not empty which can't be a null set. But if $E$ is a null set of $\mathbb{R}$, why is contradiction?",,['real-analysis']
23,Prove that $\lim_{n\to\infty}\frac1{n}\int_0^{n}xf(x)dx=0$.,Prove that .,\lim_{n\to\infty}\frac1{n}\int_0^{n}xf(x)dx=0,"Let $f$ be a continuous, nonnegative, real-valued function and $$\int_0^{\infty}f(x)dx<\infty.$$ Prove that $$\lim_{n\to\infty}\frac1{n}\int_0^{n}xf(x)dx=0.$$ A start: If $\lim\limits_{n\to\infty}\int_0^{n}xf(x)dx$ is finite, then it's obvious. Otherwise, perform L'Hopital's rule, we get $\lim nf(n)$, which we want to show is $0$.","Let $f$ be a continuous, nonnegative, real-valued function and $$\int_0^{\infty}f(x)dx<\infty.$$ Prove that $$\lim_{n\to\infty}\frac1{n}\int_0^{n}xf(x)dx=0.$$ A start: If $\lim\limits_{n\to\infty}\int_0^{n}xf(x)dx$ is finite, then it's obvious. Otherwise, perform L'Hopital's rule, we get $\lim nf(n)$, which we want to show is $0$.",,"['real-analysis', 'analysis', 'integration', 'limits', 'contest-math']"
24,Compute $\lim\limits_{n\to\infty} \left[\ln\left(\frac{1}{0!}+\frac{1}{1!}+\cdots+\frac{1}{n!}\right)\right]^n$,Compute,\lim\limits_{n\to\infty} \left[\ln\left(\frac{1}{0!}+\frac{1}{1!}+\cdots+\frac{1}{n!}\right)\right]^n,"Compute $$\lim_{n\to\infty} \left[\ln\left(\frac{1}{0!}+\frac{1}{1!}+\cdots+\frac{1}{n!}\right)\right]^n$$ If you have some nice proofs and you're willing to share them, then I thank you and you definitely  have my upvote!","Compute $$\lim_{n\to\infty} \left[\ln\left(\frac{1}{0!}+\frac{1}{1!}+\cdots+\frac{1}{n!}\right)\right]^n$$ If you have some nice proofs and you're willing to share them, then I thank you and you definitely  have my upvote!",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
25,Is the set of polynomials of degree less than or equal to $n$ closed?,Is the set of polynomials of degree less than or equal to  closed?,n,"This question is in relation to the space $C(I)$, $I = [a, b]$. Define $P_n =\{ a_0+\dots+a_nx^n \mid a_i \in \mathbb{R}\}$ (any or all $a_i$ could be zero); clearly $P_n \subset C(I)$. The norm I'm using is $\lVert f\rVert_I = \sup_I |f(x)|$. Is $P_n$ closed under $\lVert\cdot\rVert_I$? I am almost sure the answer is ""yes"", but I can't seem to prove it. My first instinct was to biject  $P_n$ to $\mathbb{R}^{n+1}$, using coefficients as coordinates, and prove that sequences of degree-$n$ polynomials converge to degree-$n$ polynomials, but I can't prove that the metric $\lVert\cdot\rVert_I$ is equivalent to the standard metric on $\mathbb{R}^{n+1}$. Next, intuitively given a function $f \in C(I), \notin P_n$ I should be able to find some $\epsilon > 0$ such that there are no low-degree polynomials ""nearby"", then that function was not a limit point of $P_n$ so $P_n$ is closed. Again I have no idea how to prove this. What should I do?","This question is in relation to the space $C(I)$, $I = [a, b]$. Define $P_n =\{ a_0+\dots+a_nx^n \mid a_i \in \mathbb{R}\}$ (any or all $a_i$ could be zero); clearly $P_n \subset C(I)$. The norm I'm using is $\lVert f\rVert_I = \sup_I |f(x)|$. Is $P_n$ closed under $\lVert\cdot\rVert_I$? I am almost sure the answer is ""yes"", but I can't seem to prove it. My first instinct was to biject  $P_n$ to $\mathbb{R}^{n+1}$, using coefficients as coordinates, and prove that sequences of degree-$n$ polynomials converge to degree-$n$ polynomials, but I can't prove that the metric $\lVert\cdot\rVert_I$ is equivalent to the standard metric on $\mathbb{R}^{n+1}$. Next, intuitively given a function $f \in C(I), \notin P_n$ I should be able to find some $\epsilon > 0$ such that there are no low-degree polynomials ""nearby"", then that function was not a limit point of $P_n$ so $P_n$ is closed. Again I have no idea how to prove this. What should I do?",,"['real-analysis', 'general-topology', 'functional-analysis', 'polynomials']"
26,Why metrizable group requires continuity of inverse?,Why metrizable group requires continuity of inverse?,,"A metrizable group is a metric space $(G,d)$ with a binary operation $\cdot$ such $(G,(\cdot))$ is a group and maps $(\cdot):G\times G\to G$ and $f:G\to G$ given by $(\cdot)(x,y)=xy$ and $f(x)=x^{-1}$ are continuous respect $d$. Why requires definition that $f$ be continuous? Is it possible to have continuity of $(\cdot)$ without continuity of $f$?","A metrizable group is a metric space $(G,d)$ with a binary operation $\cdot$ such $(G,(\cdot))$ is a group and maps $(\cdot):G\times G\to G$ and $f:G\to G$ given by $(\cdot)(x,y)=xy$ and $f(x)=x^{-1}$ are continuous respect $d$. Why requires definition that $f$ be continuous? Is it possible to have continuity of $(\cdot)$ without continuity of $f$?",,"['real-analysis', 'abstract-algebra', 'general-topology', 'group-theory', 'topological-groups']"
27,Value of $\lim_{n\to\infty}{(1+\frac{2n^2+\cos{n}}{n^3+n})^n}$,Value of,\lim_{n\to\infty}{(1+\frac{2n^2+\cos{n}}{n^3+n})^n},"How should one go about computing $$\lim_{n\to\infty}{\left(1+\frac{2n^2+\cos{n}}{n^3+n}\right)^n}\quad?$$ What surprised me about this is that  $$\lim_{n\to\infty}{\left(1+\frac{2n^2+\cos{n}}{n^3+n}\right)^\frac{n^3+n}{2n^2+\cos{n}}}=1$$(according to wolfram ), instead of $e$, which is what I expected. Could someone comment on that also?","How should one go about computing $$\lim_{n\to\infty}{\left(1+\frac{2n^2+\cos{n}}{n^3+n}\right)^n}\quad?$$ What surprised me about this is that  $$\lim_{n\to\infty}{\left(1+\frac{2n^2+\cos{n}}{n^3+n}\right)^\frac{n^3+n}{2n^2+\cos{n}}}=1$$(according to wolfram ), instead of $e$, which is what I expected. Could someone comment on that also?",,['real-analysis']
28,Convergence in $L_{\infty}$ norm implies uniform convergence,Convergence in  norm implies uniform convergence,L_{\infty},"I'm trying to prove the following claim: $f_n \in C_c$, $C_c$ being the set of continuous functions with compact support, then $\mathrm{lim}_{n \rightarrow \infty} || f_n - f||_{\infty} = 0$ implies $f_n(x) \rightarrow f(x)$ uniformly. So, according to my understanding, $$ \mathrm{lim}_{n \rightarrow \infty} || f_n - f||_{\infty} = 0 $$  $$ \Leftrightarrow $$ $$ \forall \varepsilon > 0 \exists N: n > N \Rightarrow |f_n(x) - f(x)| \leq || f_n - f||_{\infty} < \varepsilon$$ $\mu$-almost everywhere on $X$. Now my problem is that I don't see how uniform convergence follows from pointwise convergence $\mu$ almost everywhere. Can someone give me a hint? I guess I have to use the fact that they have compact support but I don't see how to put this together. Thanks for your help!","I'm trying to prove the following claim: $f_n \in C_c$, $C_c$ being the set of continuous functions with compact support, then $\mathrm{lim}_{n \rightarrow \infty} || f_n - f||_{\infty} = 0$ implies $f_n(x) \rightarrow f(x)$ uniformly. So, according to my understanding, $$ \mathrm{lim}_{n \rightarrow \infty} || f_n - f||_{\infty} = 0 $$  $$ \Leftrightarrow $$ $$ \forall \varepsilon > 0 \exists N: n > N \Rightarrow |f_n(x) - f(x)| \leq || f_n - f||_{\infty} < \varepsilon$$ $\mu$-almost everywhere on $X$. Now my problem is that I don't see how uniform convergence follows from pointwise convergence $\mu$ almost everywhere. Can someone give me a hint? I guess I have to use the fact that they have compact support but I don't see how to put this together. Thanks for your help!",,"['real-analysis', 'measure-theory']"
29,Can $\ln$ be written as a ratio of polynomials?,Can  be written as a ratio of polynomials?,\ln,"Is it possible that $\ln(x)=\frac{p(x)}{q(x)}$ for all $x>0,$ where $p$ and $q$ are polynomials with real coefficients? I think the answer is no. Suppose two such polynomials did exist. Take the limit as $x$ goes to infinity. This gives that $\deg(p)>\deg(q),$ as $\lim_{x\to\infty}\ln(x)=\infty.$ Let $m=\deg(p)$ and $n=\deg(q).$ Differentiate both sides to get $$\frac{1}{x}=\frac{p'(x)q(x)-p(x)q'(x)}{q(x)^2}.$$ Rearrange (this step is valid as $x>0$ and $q(x)^2>0$ by hypothesis) to get $q(x)^2=xp'(x)q(x)-xp(x)q'(x).$ Let the leading coefficient of $p$ be $a$ and the leading coefficient of $q$ be b. The leading coefficient of $xp'(x)q(x)$ then, is $amb.$ Similarly, the leading coefficient of $-xp(x)q'(x)$ is $-bna.$ Suppose their sum were $0.$ Then, $ab(m-n)=0.$ But, $ab≠0$ (as $a$ and $b$ are leading coefficients). So, $m-n=0.$ This contradicts $m>n.$ Hence, the coefficient of $x^{m+n}$ in the RHS is non-zero. Now, compare degrees to get $2n=m+n.$ This contradicts $m>n.$ Is my approach right? What other methods can we use to show this?","Is it possible that for all where and are polynomials with real coefficients? I think the answer is no. Suppose two such polynomials did exist. Take the limit as goes to infinity. This gives that as Let and Differentiate both sides to get Rearrange (this step is valid as and by hypothesis) to get Let the leading coefficient of be and the leading coefficient of be b. The leading coefficient of then, is Similarly, the leading coefficient of is Suppose their sum were Then, But, (as and are leading coefficients). So, This contradicts Hence, the coefficient of in the RHS is non-zero. Now, compare degrees to get This contradicts Is my approach right? What other methods can we use to show this?","\ln(x)=\frac{p(x)}{q(x)} x>0, p q x \deg(p)>\deg(q), \lim_{x\to\infty}\ln(x)=\infty. m=\deg(p) n=\deg(q). \frac{1}{x}=\frac{p'(x)q(x)-p(x)q'(x)}{q(x)^2}. x>0 q(x)^2>0 q(x)^2=xp'(x)q(x)-xp(x)q'(x). p a q xp'(x)q(x) amb. -xp(x)q'(x) -bna. 0. ab(m-n)=0. ab≠0 a b m-n=0. m>n. x^{m+n} 2n=m+n. m>n.","['real-analysis', 'calculus', 'contest-math']"
30,Are there any non-constant differentiable functions $f : \mathbb R \to \mathbb R$ where each $t \in \mathbb R$ has $f(t)f(f'(t))=1$?,Are there any non-constant differentiable functions  where each  has ?,f : \mathbb R \to \mathbb R t \in \mathbb R f(t)f(f'(t))=1,"Partition 1st differentiable $f : \mathbb R \to \mathbb R$ , by the number of $t \in \mathbb R$ where $f(t)f(f'(t))=1$ . My main question is Are there any non-constant functions where every $t \in \mathbb R$ satisfies the property? A few things I've noticed: I was able to generate all of the countable equivalence classes using $\text {exp, sin,}+,\circ,\cdot$ . The property is volatile with respect to adding a constant $c$ to a given function $f$ . I'm 99% sure $f$ cannot both be a finite-degree polynomial and have the property of the main question.","Partition 1st differentiable , by the number of where . My main question is Are there any non-constant functions where every satisfies the property? A few things I've noticed: I was able to generate all of the countable equivalence classes using . The property is volatile with respect to adding a constant to a given function . I'm 99% sure cannot both be a finite-degree polynomial and have the property of the main question.","f : \mathbb R \to \mathbb R t \in \mathbb R f(t)f(f'(t))=1 t \in \mathbb R \text {exp, sin,}+,\circ,\cdot c f f","['real-analysis', 'calculus', 'derivatives']"
31,Prove that the diophantine equation $(xz+1)(yz+1)=az^{k}+1$ has infinitely many solutions in positive integers.,Prove that the diophantine equation  has infinitely many solutions in positive integers.,(xz+1)(yz+1)=az^{k}+1,"Given two positive integers $a$ and $k>3$ : From experimental data, it appears the diophantine equation $(xz+1)(yz+1)=az^{k}+1$ has infinitely many solutions in positive integers $x,y, z$ . To motivate the question, it can easily be shown that if $k <3$ ,  the given diophantine equation has no solutions in positive integers $x, y ,z$ with $z>a$ . Proof: $(xz+1)(yz+1)=az^{k}+1$ may be simplified to $xyz^{2}+(x+y)z=az^{k}$ . If $k=1$ , this reduces to $xyz+x+y=a$ . Its clear that $a>z$ therefore there are no positive integral solutions in $x$ and $y$ when $z>a$ . if $k=2$ , we have the reduced equation $xyz+x+y=az$ . We have $z$ | $x+y$ , $z \le(x+y) \le xy$ .  Therefore $LHS=xyz+x+y>z^{2}$ . Because $RHS=az$ , we must have $a>z$ thus there are no solutions in positive integers $x ,y$ when $z>a$ . I would like to prove that given two positive integers $a$ and $k>3 $ , the diophantine equation $(xz+1)(yz+1)=az^{k}+1$ has  infinitely many positive integer solutions $x, y, z$ . I do not know how to start the proof.","Given two positive integers and : From experimental data, it appears the diophantine equation has infinitely many solutions in positive integers . To motivate the question, it can easily be shown that if ,  the given diophantine equation has no solutions in positive integers with . Proof: may be simplified to . If , this reduces to . Its clear that therefore there are no positive integral solutions in and when . if , we have the reduced equation . We have | , .  Therefore . Because , we must have thus there are no solutions in positive integers when . I would like to prove that given two positive integers and , the diophantine equation has  infinitely many positive integer solutions . I do not know how to start the proof.","a k>3 (xz+1)(yz+1)=az^{k}+1 x,y, z k <3 x, y ,z z>a (xz+1)(yz+1)=az^{k}+1 xyz^{2}+(x+y)z=az^{k} k=1 xyz+x+y=a a>z x y z>a k=2 xyz+x+y=az z x+y z \le(x+y) \le xy LHS=xyz+x+y>z^{2} RHS=az a>z x ,y z>a a k>3  (xz+1)(yz+1)=az^{k}+1 x, y, z","['real-analysis', 'elementary-number-theory', 'diophantine-equations']"
32,A power series $\sum_{n = 0}^\infty a_nx^n$ such that $\sum_{n=0}^\infty a_n= +\infty$ but $\lim_{x \to 1} \sum_{n = 0}^\infty a_nx^n \ne \infty$,A power series  such that  but,\sum_{n = 0}^\infty a_nx^n \sum_{n=0}^\infty a_n= +\infty \lim_{x \to 1} \sum_{n = 0}^\infty a_nx^n \ne \infty,"Let's consider the power series $\sum_{n = 0}^{\infty} a_nx^n $ with radius of convergence $1$ . Moreover let's suppose that : $\sum_{n = 0}^{\infty} a_n= +\infty$ . Then I would like to find a sequence $(a_n)_{n \in \mathbb{N}} \subseteq \mathbb{R}^{\mathbb{N}}$ that respect the above condition and such that : $$\lim_{x \to 1, x < 1} \sum_{n = 0}^\infty a_nx^n \ne +\infty$$ First I've noticed that $a_n$ can't be a positive sequence, since if it was the case we would have for all $N$ : $$\lim_{x \to 1} \sum_{n = 0}^\infty a_nx^n \geq \sum_{n = 0}^N a_n$$ Hence we need some of the $a_n$ to be negative. Moreover I need to use the assumption that the sum at $x = 1$ diverges, because if the sum at $x = 1$ converges then Abel's theorem says that the limit at $x \to 1$ and the sum of the power series at $x = 1$ are equal. Thank you.","Let's consider the power series with radius of convergence . Moreover let's suppose that : . Then I would like to find a sequence that respect the above condition and such that : First I've noticed that can't be a positive sequence, since if it was the case we would have for all : Hence we need some of the to be negative. Moreover I need to use the assumption that the sum at diverges, because if the sum at converges then Abel's theorem says that the limit at and the sum of the power series at are equal. Thank you.","\sum_{n = 0}^{\infty} a_nx^n  1 \sum_{n = 0}^{\infty} a_n= +\infty (a_n)_{n \in \mathbb{N}} \subseteq \mathbb{R}^{\mathbb{N}} \lim_{x \to 1, x < 1} \sum_{n = 0}^\infty a_nx^n \ne +\infty a_n N \lim_{x \to 1} \sum_{n = 0}^\infty a_nx^n \geq \sum_{n = 0}^N a_n a_n x = 1 x = 1 x \to 1 x = 1","['real-analysis', 'calculus', 'summation', 'power-series']"
33,"An Euler type sum: $\sum_{n=1}^{\infty}\frac{H_n^{(2)}}{n\cdot 4^n}{2n \choose n}$, where $H_n^{(2)}=\sum\limits_{k=1}^{n}\frac{1}{k^2}$","An Euler type sum: , where",\sum_{n=1}^{\infty}\frac{H_n^{(2)}}{n\cdot 4^n}{2n \choose n} H_n^{(2)}=\sum\limits_{k=1}^{n}\frac{1}{k^2},"I've been trying to compute the following series for quite a while : $$\sum_{n=1}^{\infty}\frac{H_n^{(2)}}{n\cdot 4^n}{2n \choose n}$$ where $H_n^{(2)}=\sum\limits_{k=1}^{n}\frac{1}{k^2}$ are the generalized harmonic numbers of order 2. I've already successfully computed the value of a lot of similar-looking series involving harmonic numbers and central binomial coefficients (by using Abel's transformation, finding the generating function...), and they all had closed forms in terms of the Riemann Zeta function, so I'm confident this one too has a nice closed form ; but it's a tough guy that resists my previous methods. By numerical test, I suspect that the value is $\frac{3}{2}\zeta(3)$. Any idea for derivation ?","I've been trying to compute the following series for quite a while : $$\sum_{n=1}^{\infty}\frac{H_n^{(2)}}{n\cdot 4^n}{2n \choose n}$$ where $H_n^{(2)}=\sum\limits_{k=1}^{n}\frac{1}{k^2}$ are the generalized harmonic numbers of order 2. I've already successfully computed the value of a lot of similar-looking series involving harmonic numbers and central binomial coefficients (by using Abel's transformation, finding the generating function...), and they all had closed forms in terms of the Riemann Zeta function, so I'm confident this one too has a nice closed form ; but it's a tough guy that resists my previous methods. By numerical test, I suspect that the value is $\frac{3}{2}\zeta(3)$. Any idea for derivation ?",,"['real-analysis', 'calculus', 'sequences-and-series', 'summation', 'harmonic-numbers']"
34,"Topologically, is there a definition of differentiability that is dependent on the underlying topology, similar to continuity?","Topologically, is there a definition of differentiability that is dependent on the underlying topology, similar to continuity?",,"I'm studying Analysis on Manifolds by Munkres, and at page 199, it is given that Let $S$ be a subset of $\mathbb{R}^k$; let $f: S \to \mathbb{R}^n$. We   say that $f$ is of class $C^r$, on $S$ if $f$ may be extended to a   function $g: U \to \mathbb{R}^n$ that is of class $C^r$ on an open set   $U$ of $\mathbb{R}^k$ containing $S$. It is clear from this definition that, even if we were working on a subspace $M$ of $\mathbb{R}^n$ (or on the set $M$ with different topology other subspace topology), we still consider the opens sets of $M$ as a subset of $\mathbb{R}^n$, and show the differentiability according to that. However, for example, if we were to show the continuity of a function $f : \mathbb{R}^k \to M$, we would consider open sets of $M$, as open sets of $M$, i.e not $\mathbb{R}^n$. In this sense, the continuity of a function is depends on the topology of the domain & codomain of that function, whereas the differentiability does not, as far as I have seen. So my question is that, is there any definition of differentiability that is dependent on the underlying topologies of domain & codomain ? Clarification For example, normally, for $f: A \to \mathbb{R}^m$, the concept differentiability is defined for $x \in Int(A)$, but the very definition of interior needs the definition of what we mean by an open set , which is dependent on the underlying topology, so say (trivially) $A = \{1,2\}$, then with the discrete topology both $1,2 \in Int(A)$, but can we define differentiability in this space ? Or let say, $A = (0,1]$ as a subspace of $[0,1]$ with the standard topology (subspace topology inherited from $\mathbb{R}$).Now if we consider our bigger space as $[0,1]$, then we should be able to define differentiability at $x = 1$ because $(0,1]$ is open in $[0,1]$, hence $1\in Int[0,1]$.","I'm studying Analysis on Manifolds by Munkres, and at page 199, it is given that Let $S$ be a subset of $\mathbb{R}^k$; let $f: S \to \mathbb{R}^n$. We   say that $f$ is of class $C^r$, on $S$ if $f$ may be extended to a   function $g: U \to \mathbb{R}^n$ that is of class $C^r$ on an open set   $U$ of $\mathbb{R}^k$ containing $S$. It is clear from this definition that, even if we were working on a subspace $M$ of $\mathbb{R}^n$ (or on the set $M$ with different topology other subspace topology), we still consider the opens sets of $M$ as a subset of $\mathbb{R}^n$, and show the differentiability according to that. However, for example, if we were to show the continuity of a function $f : \mathbb{R}^k \to M$, we would consider open sets of $M$, as open sets of $M$, i.e not $\mathbb{R}^n$. In this sense, the continuity of a function is depends on the topology of the domain & codomain of that function, whereas the differentiability does not, as far as I have seen. So my question is that, is there any definition of differentiability that is dependent on the underlying topologies of domain & codomain ? Clarification For example, normally, for $f: A \to \mathbb{R}^m$, the concept differentiability is defined for $x \in Int(A)$, but the very definition of interior needs the definition of what we mean by an open set , which is dependent on the underlying topology, so say (trivially) $A = \{1,2\}$, then with the discrete topology both $1,2 \in Int(A)$, but can we define differentiability in this space ? Or let say, $A = (0,1]$ as a subspace of $[0,1]$ with the standard topology (subspace topology inherited from $\mathbb{R}$).Now if we consider our bigger space as $[0,1]$, then we should be able to define differentiability at $x = 1$ because $(0,1]$ is open in $[0,1]$, hence $1\in Int[0,1]$.",,"['real-analysis', 'general-topology', 'derivatives', 'continuity', 'differential-topology']"
35,Zero limit implies zero sequence,Zero limit implies zero sequence,,"Let $0 < \alpha_1 < \alpha_2 < \cdots < \alpha_k < 1$ , and $a_1,\ldots , a_k \in \mathbb{R}$ . Suppose that $$\lim_{n\to + \infty} \sum_{p = 1}^k a_p \sin (n \pi \alpha_p)=0$$ Show that $a_1=a_2=\cdots=a_k=0$ . I tried to use the same method as Cantor Lemma which you can find a related problem on MSE : Convergence of series of functions: $f_n(x)=u_n\sin(nx)$ But I failed, it seems to be more difficult.","Let , and . Suppose that Show that . I tried to use the same method as Cantor Lemma which you can find a related problem on MSE : Convergence of series of functions: $f_n(x)=u_n\sin(nx)$ But I failed, it seems to be more difficult.","0 < \alpha_1 < \alpha_2 < \cdots < \alpha_k < 1 a_1,\ldots , a_k \in \mathbb{R} \lim_{n\to + \infty} \sum_{p = 1}^k a_p \sin (n \pi \alpha_p)=0 a_1=a_2=\cdots=a_k=0",['real-analysis']
36,"How to prove that $\,\,f\equiv 0,$ without using Weierstrass theorem?",How to prove that  without using Weierstrass theorem?,"\,\,f\equiv 0,","Let $\,f:[0,1] \to \mathbb{R}$ continuous. Show that: If   $$\int_0 ^1 x^k f(x)\, dx=0,$$   for all $k\in\mathbb N$, then $f\equiv 0$. I know that it can be proved using Weierstrass Theorem, but I'd like to know whether it is possible to avoid using it?","Let $\,f:[0,1] \to \mathbb{R}$ continuous. Show that: If   $$\int_0 ^1 x^k f(x)\, dx=0,$$   for all $k\in\mathbb N$, then $f\equiv 0$. I know that it can be proved using Weierstrass Theorem, but I'd like to know whether it is possible to avoid using it?",,"['real-analysis', 'integration']"
37,"sufficient condition for a polynomial to have roots in $[0,1]$",sufficient condition for a polynomial to have roots in,"[0,1]","Question is to check : which of the following is sufficient condition for a polynomial $f(x)=a_0 +a_1x+a_2x^2+\dots +a_nx^n\in \mathbb{R}[x] $ to have a root in $[0,1]$. $a_0 <0$ and $a_0+a_1+a_2+\dots +a_n >0$ $a_0+\frac{a_1}{2}+\frac{a_2}{3}+\dots +\frac{a_n}{n+1}=0$ $\frac{a_0}{1.2}+\frac{a_1}{2.3}+\dots+\frac{a_n}{(n+1).(n+2)} =0$ First of all i tried by considering degree $1$ polynomial and then degree $2$ polynomial and then degree $3$ polnomial hoping to see some patern but could not make it out. And then, I saw that $a_0= f(0)$ and $f(1)=a_0+a_1+a_2+\dots +a_n$. So, if $f(0)<0$ and $f(1)>0$ it would be sufficient for $f$ to have root in $[0,1]$ In first case we have $a_0 <0$ i.e., $f(0)<0$ and $f(1)>1>0$. So, first condition should be implying existence of a root in $[0,1]$ for second case, let $f(x)$ be a linear polynomial i.e., $f(x)=a_0+a_1x$ Now, $a_0+\frac{a_1}{2}=0$ implies $0\leq x=\frac{-a_0}{a_1}=\frac{1}{2}< 1$ So, this might be possibly give existence in case of linear polynomials. Now, $\frac{a_0}{1.2}+\frac{a_1}{2.3}=0$implies $0\leq x=\frac{-a_0}{a_1}=\frac{1}{3}< 1$ So, this might be possibly give existence in case of linear polynomials. So, for linear polynomials all the three conditions imply existence of a root in $[0,1]$. But, i guess this can not be generalized for higher degree polynomial. I think there should be some ""neat idea"" than checking for roots and all. I am sure about first case but I have no idea how to consider the other two cases. please provide some hints to proceed further.","Question is to check : which of the following is sufficient condition for a polynomial $f(x)=a_0 +a_1x+a_2x^2+\dots +a_nx^n\in \mathbb{R}[x] $ to have a root in $[0,1]$. $a_0 <0$ and $a_0+a_1+a_2+\dots +a_n >0$ $a_0+\frac{a_1}{2}+\frac{a_2}{3}+\dots +\frac{a_n}{n+1}=0$ $\frac{a_0}{1.2}+\frac{a_1}{2.3}+\dots+\frac{a_n}{(n+1).(n+2)} =0$ First of all i tried by considering degree $1$ polynomial and then degree $2$ polynomial and then degree $3$ polnomial hoping to see some patern but could not make it out. And then, I saw that $a_0= f(0)$ and $f(1)=a_0+a_1+a_2+\dots +a_n$. So, if $f(0)<0$ and $f(1)>0$ it would be sufficient for $f$ to have root in $[0,1]$ In first case we have $a_0 <0$ i.e., $f(0)<0$ and $f(1)>1>0$. So, first condition should be implying existence of a root in $[0,1]$ for second case, let $f(x)$ be a linear polynomial i.e., $f(x)=a_0+a_1x$ Now, $a_0+\frac{a_1}{2}=0$ implies $0\leq x=\frac{-a_0}{a_1}=\frac{1}{2}< 1$ So, this might be possibly give existence in case of linear polynomials. Now, $\frac{a_0}{1.2}+\frac{a_1}{2.3}=0$implies $0\leq x=\frac{-a_0}{a_1}=\frac{1}{3}< 1$ So, this might be possibly give existence in case of linear polynomials. So, for linear polynomials all the three conditions imply existence of a root in $[0,1]$. But, i guess this can not be generalized for higher degree polynomial. I think there should be some ""neat idea"" than checking for roots and all. I am sure about first case but I have no idea how to consider the other two cases. please provide some hints to proceed further.",,['real-analysis']
38,"Upper semi continuous, lower semi continuous","Upper semi continuous, lower semi continuous",,"which of the followings are true? $X$ be a topological space,  $f_n:X\rightarrow \mathbb{R}$ is sequence of lower semi continuous functions then the $\sup\{f_n\}=f$ is also lower semi continuous. every continuous real valued function on $X$ is lower semi continuous. A real valued function on $X$ is continuous iff it is both USC and LSC. I read in my measure theory course and recall that $3$ and $1$ is true though I can not remember the proofs now, but could any one just give me hint how to handle $2$? Thank you.","which of the followings are true? $X$ be a topological space,  $f_n:X\rightarrow \mathbb{R}$ is sequence of lower semi continuous functions then the $\sup\{f_n\}=f$ is also lower semi continuous. every continuous real valued function on $X$ is lower semi continuous. A real valued function on $X$ is continuous iff it is both USC and LSC. I read in my measure theory course and recall that $3$ and $1$ is true though I can not remember the proofs now, but could any one just give me hint how to handle $2$? Thank you.",,"['real-analysis', 'semicontinuous-functions']"
39,Bounded convergence theorem,Bounded convergence theorem,,"Suppose that $f_n$ is a sequence of measurable functions that are all bounded by M, supported on a set E of finite measure, and $f_n(x)\to f(x)$ a.e. x as $n\to \infty$. Then f is measurable, bounded, supported on E for a.e. x, and $\int |f_n-f|\to 0 $ as $n\to\infty$ At here, I understand the restriction of E to be finite measure is because we need to use Egorov theroem. But what is the reason to assume $f_n$ to be bounded? I check the Egorov theorem, it does not require the squence that converge to $f$  to be bounded in order to find a smaller set differs from E by $\epsilon$ and converges uniformly to $f$ on that set.","Suppose that $f_n$ is a sequence of measurable functions that are all bounded by M, supported on a set E of finite measure, and $f_n(x)\to f(x)$ a.e. x as $n\to \infty$. Then f is measurable, bounded, supported on E for a.e. x, and $\int |f_n-f|\to 0 $ as $n\to\infty$ At here, I understand the restriction of E to be finite measure is because we need to use Egorov theroem. But what is the reason to assume $f_n$ to be bounded? I check the Egorov theorem, it does not require the squence that converge to $f$  to be bounded in order to find a smaller set differs from E by $\epsilon$ and converges uniformly to $f$ on that set.",,"['real-analysis', 'lebesgue-integral']"
40,What kind of object is the second derivative of a function $f:\mathbb R^n\to\mathbb R^m$?,What kind of object is the second derivative of a function ?,f:\mathbb R^n\to\mathbb R^m,"I wonder what is the meaning of the second derivative or what kind of object it is when we have a function $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$. The first derivative is the Jacobian matrix, but then, what is the second derivative? How can I treat them when I write $f''$ or $D^2 f$? Thanks a lot for your help!","I wonder what is the meaning of the second derivative or what kind of object it is when we have a function $f:\mathbb{R}^n \rightarrow \mathbb{R}^m$. The first derivative is the Jacobian matrix, but then, what is the second derivative? How can I treat them when I write $f''$ or $D^2 f$? Thanks a lot for your help!",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives', 'vector-analysis']"
41,Continuous Functions and Cauchy Sequences,Continuous Functions and Cauchy Sequences,,"We know that if a function $f: A \mapsto \mathbb{R}$, $A \subseteq \mathbb{R}$, is uniformly continuous on $A$ then, if $(x_n)$ is a Cauchy sequence in $A$, then $(f(x_n))$ is also a Cauchy sequence. I would like an example of continuous function $g: A \mapsto \mathbb{R}$ such that for a Cauchy sequence $(x_n)$ in $A$, it is not true that $f(x_n)$ is a Cauchy sequence. Thanks for your help.","We know that if a function $f: A \mapsto \mathbb{R}$, $A \subseteq \mathbb{R}$, is uniformly continuous on $A$ then, if $(x_n)$ is a Cauchy sequence in $A$, then $(f(x_n))$ is also a Cauchy sequence. I would like an example of continuous function $g: A \mapsto \mathbb{R}$ such that for a Cauchy sequence $(x_n)$ in $A$, it is not true that $f(x_n)$ is a Cauchy sequence. Thanks for your help.",,"['real-analysis', 'sequences-and-series', 'continuity', 'examples-counterexamples', 'cauchy-sequences']"
42,"Show $f(x) = x^3 - \sin^2{x} \tan{x} < 0$ on $(0, \frac{\pi}{2})$",Show  on,"f(x) = x^3 - \sin^2{x} \tan{x} < 0 (0, \frac{\pi}{2})","This is the last of a homework problem set from Principles of Mathematical Analysis (Ch. 8 #18(a)) that I've been working/stuck on for a few days: Define $f(x) = x^3 - \sin^2{x}\tan{x}.$   Find out whether it is positive or negative for all $x \in (0, \frac{\pi}{2})$, or whether it changes sign. Prove your answer. I've thought of a couple possible ways to solve it but have gotten stuck each time. Power series: This would be super easy, since the polynomial is gone and all of the other terms are negative. The problem: I'd have to calculate the $\tan$ power series and show various properties of the Bernoulli numbers. Big hassle. Straight up approximation: I've spent most of the time on this method, but I always overestimate, causing me to lose the strict inequality. This was using various trig identities and just basic stuff like $\sin{x} < x < \tan{x}$ on $(0, \frac{\pi}{2})$. Integrating and more approximating. Running into the same difficulty as above; overestimation. I'm kind of just running around in circles at this point, probably missing something simple. Can I get a hint?","This is the last of a homework problem set from Principles of Mathematical Analysis (Ch. 8 #18(a)) that I've been working/stuck on for a few days: Define $f(x) = x^3 - \sin^2{x}\tan{x}.$   Find out whether it is positive or negative for all $x \in (0, \frac{\pi}{2})$, or whether it changes sign. Prove your answer. I've thought of a couple possible ways to solve it but have gotten stuck each time. Power series: This would be super easy, since the polynomial is gone and all of the other terms are negative. The problem: I'd have to calculate the $\tan$ power series and show various properties of the Bernoulli numbers. Big hassle. Straight up approximation: I've spent most of the time on this method, but I always overestimate, causing me to lose the strict inequality. This was using various trig identities and just basic stuff like $\sin{x} < x < \tan{x}$ on $(0, \frac{\pi}{2})$. Integrating and more approximating. Running into the same difficulty as above; overestimation. I'm kind of just running around in circles at this point, probably missing something simple. Can I get a hint?",,"['calculus', 'real-analysis', 'trigonometry']"
43,Continuity from below for Lebesgue outer measure,Continuity from below for Lebesgue outer measure,,"Let $\{E_n\}$ be an increasing sequence of subsets of $\mathbb{R}^n$, measurable or not. Then  $$m^* \bigg( \bigcup_{n=1}^{\infty}E_n \bigg)=\lim_{n\rightarrow\infty}m^*E_n$$","Let $\{E_n\}$ be an increasing sequence of subsets of $\mathbb{R}^n$, measurable or not. Then  $$m^* \bigg( \bigcup_{n=1}^{\infty}E_n \bigg)=\lim_{n\rightarrow\infty}m^*E_n$$",,"['real-analysis', 'measure-theory']"
44,What properties are used to assert that there is always a number between two given numbers?,What properties are used to assert that there is always a number between two given numbers?,,"What properties of ""numbers"" are used to assert that for given numbers $a$ and $b$, $a≠b$, there exists a number $x$ such that $a < x < b$ ? In the texts I've read, this seems to be assumed without explanation in discussions that are otherwise quite careful about such things. For example, in Spivak's Calculus (4E, p. 123) this fact is used to demonstrate that the function $f(x) = x^2$ does not take on its maximum on the interval (0,1) because for any $0 ≤ y < 1$ there is an $x$ such that $y < x < 1$. Up to that point, the only properties of ""numbers"" (whatever they may be) that have been defined are those of an ordered field, and it is not clear to me that this property can be derived from those. I gather this amounts to the ""numbers"" in question having ""dense order"". (Correct me if I'm wrong.) But I'm unclear where that comes from. Also note: up to the point where this question arises, there has been no mention of what ""numbers"" are (i.e., whether they are $\mathbb Q$ or $\mathbb R$), only that they have the properties of an ordered field.","What properties of ""numbers"" are used to assert that for given numbers $a$ and $b$, $a≠b$, there exists a number $x$ such that $a < x < b$ ? In the texts I've read, this seems to be assumed without explanation in discussions that are otherwise quite careful about such things. For example, in Spivak's Calculus (4E, p. 123) this fact is used to demonstrate that the function $f(x) = x^2$ does not take on its maximum on the interval (0,1) because for any $0 ≤ y < 1$ there is an $x$ such that $y < x < 1$. Up to that point, the only properties of ""numbers"" (whatever they may be) that have been defined are those of an ordered field, and it is not clear to me that this property can be derived from those. I gather this amounts to the ""numbers"" in question having ""dense order"". (Correct me if I'm wrong.) But I'm unclear where that comes from. Also note: up to the point where this question arises, there has been no mention of what ""numbers"" are (i.e., whether they are $\mathbb Q$ or $\mathbb R$), only that they have the properties of an ordered field.",,['real-analysis']
45,Differentiability and decay of magnitude of fourier series coefficients,Differentiability and decay of magnitude of fourier series coefficients,,I want to know the answer/references for the question on decay of Fourier series coefficients and the differentiability of a function. Does the magitude of fourier series coefficients {$a_k$} of a differentiable function in $L^2 (\mathbb{R})$ (or in any suitable space) decay as fast as or faster than $k^{-1}$. I want to know if there any such theorem ? Also about the converse statement. ?,I want to know the answer/references for the question on decay of Fourier series coefficients and the differentiability of a function. Does the magitude of fourier series coefficients {$a_k$} of a differentiable function in $L^2 (\mathbb{R})$ (or in any suitable space) decay as fast as or faster than $k^{-1}$. I want to know if there any such theorem ? Also about the converse statement. ?,,"['real-analysis', 'fourier-series']"
46,"How much a càdlàg (i.e., right-continuous with left limits) function can jump?","How much a càdlàg (i.e., right-continuous with left limits) function can jump?",,"I have changed the title (replaced ""well-behaved"" by ""càdlàg""), since it seems that ""a well-behaved function"" might be interpreted as ""a function of bounded variation"" (rather than ""a càdlàg function"", which I actually meant). Let $f:[0,1] \to {\bf R}$ have a following property: $f$ is continuous except at the points $x_{k,n} = \frac{{2k - 1}}{{2^n }}$, $k=1,\ldots,2^{n-1}$, $n=1,2,3,\ldots$, where $\lim _{x \downarrow x_{k,n} } f(x) = f(x_{k,n} )$ but $f(x_{k,n} ) - \lim _{x \uparrow x_{k,n} } f(x) = a_n  > 0$. Can such a function exist if $a_n > 1/ \log(n)$ for all sufficiently large $n$?","I have changed the title (replaced ""well-behaved"" by ""càdlàg""), since it seems that ""a well-behaved function"" might be interpreted as ""a function of bounded variation"" (rather than ""a càdlàg function"", which I actually meant). Let $f:[0,1] \to {\bf R}$ have a following property: $f$ is continuous except at the points $x_{k,n} = \frac{{2k - 1}}{{2^n }}$, $k=1,\ldots,2^{n-1}$, $n=1,2,3,\ldots$, where $\lim _{x \downarrow x_{k,n} } f(x) = f(x_{k,n} )$ but $f(x_{k,n} ) - \lim _{x \uparrow x_{k,n} } f(x) = a_n  > 0$. Can such a function exist if $a_n > 1/ \log(n)$ for all sufficiently large $n$?",,['real-analysis']
47,"If $\{x_n\}$ is positive, decreasing and $\sum x_n=\infty,$ then $\sum x_ne^{-\frac{x_{n}}{x_{n+1}}}=\infty$","If  is positive, decreasing and  then","\{x_n\} \sum x_n=\infty, \sum x_ne^{-\frac{x_{n}}{x_{n+1}}}=\infty","The problem is Suppose that $\{x_n\}$ is positive, monotonic decreasing and $\sum_{n=1}^\infty x_n=+\infty$ . Prove that $$\sum_{n=1}^\infty x_ne^{-\frac{x_{n}}{x_{n+1}}}=+\infty.$$ This is a past examination problem of Analysis . I tried many ways, but failed. One trival idea is to prove $\frac{x_{n}}{x_{n+1}}$ is bounded above, but I failed to show it, maybe it is not ture.  Hope to find some hints here, thanks in advance.","The problem is Suppose that is positive, monotonic decreasing and . Prove that This is a past examination problem of Analysis . I tried many ways, but failed. One trival idea is to prove is bounded above, but I failed to show it, maybe it is not ture.  Hope to find some hints here, thanks in advance.",\{x_n\} \sum_{n=1}^\infty x_n=+\infty \sum_{n=1}^\infty x_ne^{-\frac{x_{n}}{x_{n+1}}}=+\infty. \frac{x_{n}}{x_{n+1}},"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'divergent-series']"
48,How to progress in Real Analysis?,How to progress in Real Analysis?,,"I am currently reading Abbott's Understanding Analysis and also trying to solve the problems. However, I get stuck on some problems and wonder why is that the case because people say that Abbott's book is just introductory. I know that getting stuck for a while is a part of learning real analysis but how should one go further? Should I aim at solving all the exercises before moving on to the next section or chapter? Or should I solve as many as I can within a certain time frame and move on to next sections? (Of course I'll get back to the remaining exercises and try to solve them) I do not read the latter sections unless and until I feel like I have understood the preceding ones. I'd like to know how you progressed in the process of learning analysis. Thanks :)","I am currently reading Abbott's Understanding Analysis and also trying to solve the problems. However, I get stuck on some problems and wonder why is that the case because people say that Abbott's book is just introductory. I know that getting stuck for a while is a part of learning real analysis but how should one go further? Should I aim at solving all the exercises before moving on to the next section or chapter? Or should I solve as many as I can within a certain time frame and move on to next sections? (Of course I'll get back to the remaining exercises and try to solve them) I do not read the latter sections unless and until I feel like I have understood the preceding ones. I'd like to know how you progressed in the process of learning analysis. Thanks :)",,"['real-analysis', 'self-learning', 'advice']"
49,Compute $\sum\limits_{n=1}^\infty\frac{1}{(n(n+1))^p}$ where $p\geq 1$,Compute  where,\sum\limits_{n=1}^\infty\frac{1}{(n(n+1))^p} p\geq 1,"I was recently told to compute some integral, and the result turned out to be a scalar multiple of the series $$\sum\limits_{n=1}^\infty\frac{1}{(n(n+1))^p},$$ where $p\geq 1$ . I know it converges by comparison for $$\dfrac{1}{(n(n+1))^p}\leq\dfrac{1}{n(n+1)}<\dfrac{1}{n^2},$$ and we know thanks to Euler that $$\sum\limits_{n=1}^\infty\frac1{n^2}=\frac{\pi^2}6.$$ I managed to work out the cases where $p=1$ and $p=2$ . With $p=1$ being a telescoping sum, and my solution for $p=2$ being $$\frac13\pi^2-3,$$ which I obtained based on Euler's solution to the Basel Problem. I see no way to generalize the results to values to arbitrary values of $p$ however. Any advice on where to start would be much appreciated. Also, in absence of another formula, is the series itself a valid answer? Given that it converges of course.","I was recently told to compute some integral, and the result turned out to be a scalar multiple of the series where . I know it converges by comparison for and we know thanks to Euler that I managed to work out the cases where and . With being a telescoping sum, and my solution for being which I obtained based on Euler's solution to the Basel Problem. I see no way to generalize the results to values to arbitrary values of however. Any advice on where to start would be much appreciated. Also, in absence of another formula, is the series itself a valid answer? Given that it converges of course.","\sum\limits_{n=1}^\infty\frac{1}{(n(n+1))^p}, p\geq 1 \dfrac{1}{(n(n+1))^p}\leq\dfrac{1}{n(n+1)}<\dfrac{1}{n^2}, \sum\limits_{n=1}^\infty\frac1{n^2}=\frac{\pi^2}6. p=1 p=2 p=1 p=2 \frac13\pi^2-3, p","['real-analysis', 'sequences-and-series']"
50,"Prove that if $a+b+c+d=4$, then $(a^2+3)(b^2+3)(c^2+3)(d^2+3)\geq256$","Prove that if , then",a+b+c+d=4 (a^2+3)(b^2+3)(c^2+3)(d^2+3)\geq256,"Given $a,b,c,d$ such that $a + b + c + d = 4$ show that $$(a^2 + 3)(b^2 + 3)(c^2 + 3)(d^2 + 3) \geq 256$$ What I have tried so far is using CBS: $(a^2 + 3)(b^2 + 3) \geq (a\sqrt{3} + b\sqrt{3})^2 = 3(a + b)^2$ $(c^2 + 3)(d^2 + 3) \geq 3(c + d)^2$ $(a^2 + b^2)(c^2 + d^2) \geq (ac + bd)^2$ Then, we have: $(a^2 + 3)(b^2 + 3)(c^2 + 3)(d^2 + 3) \geq 9(a + b)^2(c + d)^2$ . Thus, we have to prove that $9(a + b)^2(c + d)^2 \geq 256$ . Then, I used the following substitution: $c + d = t$ and $a + b = 4 - t$ . We assume wlog that $a \leq b \leq c \leq d$ . Then, $4 = a + b + c + d \leq 2(c + d) = 2t$ . Thus, $t \geq 2$ . Then, what we have to prove is: $9t^2(4 - t)^2 \geq 256$ . We can rewrite this as: $(3t(4 - t) - 16)(3t(4 - t) + 16) \geq 0$ , or $(3t^2 + 2t + 16)(3t^2 - 12t - 16) \geq 0$ , at which point I got stuck.","Given such that show that What I have tried so far is using CBS: Then, we have: . Thus, we have to prove that . Then, I used the following substitution: and . We assume wlog that . Then, . Thus, . Then, what we have to prove is: . We can rewrite this as: , or , at which point I got stuck.","a,b,c,d a + b + c + d = 4 (a^2 + 3)(b^2 + 3)(c^2 + 3)(d^2 + 3) \geq 256 (a^2 + 3)(b^2 + 3) \geq (a\sqrt{3} + b\sqrt{3})^2 = 3(a + b)^2 (c^2 + 3)(d^2 + 3) \geq 3(c + d)^2 (a^2 + b^2)(c^2 + d^2) \geq (ac + bd)^2 (a^2 + 3)(b^2 + 3)(c^2 + 3)(d^2 + 3) \geq 9(a + b)^2(c + d)^2 9(a + b)^2(c + d)^2 \geq 256 c + d = t a + b = 4 - t a \leq b \leq c \leq d 4 = a + b + c + d \leq 2(c + d) = 2t t \geq 2 9t^2(4 - t)^2 \geq 256 (3t(4 - t) - 16)(3t(4 - t) + 16) \geq 0 (3t^2 + 2t + 16)(3t^2 - 12t - 16) \geq 0","['real-analysis', 'inequality', 'a.m.-g.m.-inequality', 'cauchy-schwarz-inequality', 'tangent-line-method']"
51,Does this variant on Rolle's theorem have a name?,Does this variant on Rolle's theorem have a name?,,"The following seems to be true: Suppose $f : (a,b) \rightarrow \mathbb{R}$ is continuously differentiable. Then $$|x \in (a,b):f(x) = 0| \leq |x\in (a,b):f'(x) = 0|+1.$$ (By $|x \in X : P|$, I just mean the number of $x \in X$ satisfying $P$. This can be viewed as shorthand for the more long-winded $|\{x \in X : P\}|$.) I'm having trouble formalizing the details, but the proof is basically by Rolle's theorem. Since $f$ is continuous, there are two cases: The zeroes of $f$ are isolated from each other. There exists a non-empty open interval on which $f$ is zero. In the second case, both the LHS and RHS are $|\mathbb{R}|$, so we're done. So assume the zeros of $f$ are isolated from each other. Then we repeatedly apply Rolle's theorem to get a stationary point of $f$ for each root of $f$ beyond the first. That's the idea. Anyway, just wondering if this result has a name? It's basically the same as Rolle's theorem, but perhaps bit easier for young people to understand and use. For instance, suppose we wish to prove that $\sqrt{2}$ refers to exactly one real number. Define $f(x)=x^2-2.$ We want to show that $f$ has precisely one root in $(0,\infty)$. We get a lower bound on the cardinality its set of roots by computing $f(0) = -2$ and $f(2) = 2$, hence by IVT, we have: $$1 \leq |x \in (0,\infty) : x^2 -2 = 0|$$ For an upper bound, use the above theorem to get $$|x \in (0,\infty) : x^2 -2 = 0| \leq |x \in (0,\infty) : 2x = 0|+1 = 0+1 = 1$$ Ergo: $$|x \in (0,\infty) : x^2 = 2| = 1$$","The following seems to be true: Suppose $f : (a,b) \rightarrow \mathbb{R}$ is continuously differentiable. Then $$|x \in (a,b):f(x) = 0| \leq |x\in (a,b):f'(x) = 0|+1.$$ (By $|x \in X : P|$, I just mean the number of $x \in X$ satisfying $P$. This can be viewed as shorthand for the more long-winded $|\{x \in X : P\}|$.) I'm having trouble formalizing the details, but the proof is basically by Rolle's theorem. Since $f$ is continuous, there are two cases: The zeroes of $f$ are isolated from each other. There exists a non-empty open interval on which $f$ is zero. In the second case, both the LHS and RHS are $|\mathbb{R}|$, so we're done. So assume the zeros of $f$ are isolated from each other. Then we repeatedly apply Rolle's theorem to get a stationary point of $f$ for each root of $f$ beyond the first. That's the idea. Anyway, just wondering if this result has a name? It's basically the same as Rolle's theorem, but perhaps bit easier for young people to understand and use. For instance, suppose we wish to prove that $\sqrt{2}$ refers to exactly one real number. Define $f(x)=x^2-2.$ We want to show that $f$ has precisely one root in $(0,\infty)$. We get a lower bound on the cardinality its set of roots by computing $f(0) = -2$ and $f(2) = 2$, hence by IVT, we have: $$1 \leq |x \in (0,\infty) : x^2 -2 = 0|$$ For an upper bound, use the above theorem to get $$|x \in (0,\infty) : x^2 -2 = 0| \leq |x \in (0,\infty) : 2x = 0|+1 = 0+1 = 1$$ Ergo: $$|x \in (0,\infty) : x^2 = 2| = 1$$",,"['calculus', 'real-analysis', 'reference-request', 'terminology']"
52,Sum of two sequences of functions converging in measure still converges in measure,Sum of two sequences of functions converging in measure still converges in measure,,"Suppose $f_n\to f$ in measure and $g_n\to g$ in measure. Can I claim that $(f_n+g_n)\to f+g$ in measure? Attempt at the proof: Since we know that $f_n$ and $g_n$ converge in measure respectively, we know that for all $\epsilon>0$, we have that $m \left(x\in D: \left|f_n(x)-f(x)\right|\geq \epsilon/2 \right)\to 0$ and similarly for $g_n$. Then $\{f_n(x)+g_n(x)-f(x)-g(x):x\in D\}\subset \{f_n(x)-f(x):x\in D\}\cup \{g_n(x)-g(x):x\in D\}$. And so the measures of these sets would be $\epsilon/2+\epsilon/2=\epsilon$? (I don't know) I assume some form of the triangle inequality is supposed to be used here? How to continue?","Suppose $f_n\to f$ in measure and $g_n\to g$ in measure. Can I claim that $(f_n+g_n)\to f+g$ in measure? Attempt at the proof: Since we know that $f_n$ and $g_n$ converge in measure respectively, we know that for all $\epsilon>0$, we have that $m \left(x\in D: \left|f_n(x)-f(x)\right|\geq \epsilon/2 \right)\to 0$ and similarly for $g_n$. Then $\{f_n(x)+g_n(x)-f(x)-g(x):x\in D\}\subset \{f_n(x)-f(x):x\in D\}\cup \{g_n(x)-g(x):x\in D\}$. And so the measures of these sets would be $\epsilon/2+\epsilon/2=\epsilon$? (I don't know) I assume some form of the triangle inequality is supposed to be used here? How to continue?",,"['real-analysis', 'measure-theory', 'convergence-divergence', 'lebesgue-measure']"
53,Contiuous function on a closed bounded interval is uniformly continuous. Don't understand the proof.,Contiuous function on a closed bounded interval is uniformly continuous. Don't understand the proof.,,"I'm self studying real analysis from Wade's ""An Introduction to Real Analysis"" and I've come across a proof that I don't understand. I was hoping that some might be able to walk me through it. The theorem is as follows Theorem. Suppose that $I$ is a closed, bounded interval. If $f:\rightarrow\mathbb{R}$ is continuous on $I$, then $f$ is uniformly continuous on $I$. Proof. Suppose to the contrary that $f$ is continuous but not uniformly continuous on $I$. Then there is an $\varepsilon_0>0$ and points $x_n, y_n \in I$ such that $|x_n-y_n|<\frac{1}{n}$ and $$|f(x_n)-f(y_n)|\geq \varepsilon_0\;\;\;\;n\in\mathbb{N}$$ By the Bolzano-Weierstrass Theorem and the Comparison Theorem the sequence $\{x_n\}$ has a subsequence, say $x_{n_k}$, which converges as $k\rightarrow\infty$, to some $x \in I$. Similarly the sequence $\{y_{n_k}\}_{k\in\mathbb{N}}$ has a convergent subsequence say $y_{n_{k_j}}$, which converges as $j\rightarrow \infty$, to some $y \in I$. Since $x_{n_{k_j}} \rightarrow x$ as $j\rightarrow \infty$ and $f$ is continuous it follows from above that $|f(x)-f(y)|\geq \varepsilon_0$; that is $f(x)\neq f(y)$. But $|x_n-y_n|<\frac{1}{n}$ for all $n \in \mathbb{N}$ so the Squeeze Theorem implies $x=y$. Therefore,  $f(x)=f(y)$, a contradiction. Why in this proof do we need to take a sub-subsequence, why wont subsequences suffice? I have seen slightly different proofs of this theorem which use only subsequences and the triangle inequality. If someone could help me I would be most grateful.","I'm self studying real analysis from Wade's ""An Introduction to Real Analysis"" and I've come across a proof that I don't understand. I was hoping that some might be able to walk me through it. The theorem is as follows Theorem. Suppose that $I$ is a closed, bounded interval. If $f:\rightarrow\mathbb{R}$ is continuous on $I$, then $f$ is uniformly continuous on $I$. Proof. Suppose to the contrary that $f$ is continuous but not uniformly continuous on $I$. Then there is an $\varepsilon_0>0$ and points $x_n, y_n \in I$ such that $|x_n-y_n|<\frac{1}{n}$ and $$|f(x_n)-f(y_n)|\geq \varepsilon_0\;\;\;\;n\in\mathbb{N}$$ By the Bolzano-Weierstrass Theorem and the Comparison Theorem the sequence $\{x_n\}$ has a subsequence, say $x_{n_k}$, which converges as $k\rightarrow\infty$, to some $x \in I$. Similarly the sequence $\{y_{n_k}\}_{k\in\mathbb{N}}$ has a convergent subsequence say $y_{n_{k_j}}$, which converges as $j\rightarrow \infty$, to some $y \in I$. Since $x_{n_{k_j}} \rightarrow x$ as $j\rightarrow \infty$ and $f$ is continuous it follows from above that $|f(x)-f(y)|\geq \varepsilon_0$; that is $f(x)\neq f(y)$. But $|x_n-y_n|<\frac{1}{n}$ for all $n \in \mathbb{N}$ so the Squeeze Theorem implies $x=y$. Therefore,  $f(x)=f(y)$, a contradiction. Why in this proof do we need to take a sub-subsequence, why wont subsequences suffice? I have seen slightly different proofs of this theorem which use only subsequences and the triangle inequality. If someone could help me I would be most grateful.",,['real-analysis']
54,Regularity of the function $|x|^ax$,Regularity of the function,|x|^ax,"Assuming $x \in \mathbb{R}$, what can we say about the regularity class ($C, C^1, C^2, ..., \text{or}\ C^\infty$) of the following function (also with respect to $a \in \mathbb{R}$)? $$f(x)=|x|^ax$$","Assuming $x \in \mathbb{R}$, what can we say about the regularity class ($C, C^1, C^2, ..., \text{or}\ C^\infty$) of the following function (also with respect to $a \in \mathbb{R}$)? $$f(x)=|x|^ax$$",,"['calculus', 'real-analysis', 'analysis', 'limits', 'functions']"
55,Continuous function on closed unit ball,Continuous function on closed unit ball,,"Take a continuous mapping $f: \bar{B^{n}} \rightarrow \bar{B^{n}}$, where $\bar{B^{n}}$ is a closed unit ball in $\mathbb{R}^{n}$. Assume that $f(x) \neq x$ for every $x \in \bar{B^{n}}$. Define another function $r$ by following the directed line segment from $f(x)$ through $x$ to its intersection with $\partial B^{n}$, and let the intersection point be $r(x)$. Is it immediately evident that $r$ is a continuous function? Is so how does it follow? Thanks.","Take a continuous mapping $f: \bar{B^{n}} \rightarrow \bar{B^{n}}$, where $\bar{B^{n}}$ is a closed unit ball in $\mathbb{R}^{n}$. Assume that $f(x) \neq x$ for every $x \in \bar{B^{n}}$. Define another function $r$ by following the directed line segment from $f(x)$ through $x$ to its intersection with $\partial B^{n}$, and let the intersection point be $r(x)$. Is it immediately evident that $r$ is a continuous function? Is so how does it follow? Thanks.",,"['real-analysis', 'functional-analysis']"
56,Existence of a map $f: \mathbb{Z}\rightarrow \mathbb{Q}$,Existence of a map,f: \mathbb{Z}\rightarrow \mathbb{Q},"Question is to check which option holds true : There exist a map $f: \mathbb{Z}\rightarrow \mathbb{Q}$ such that is bijective and increasing is onto and decreasing is bijective and satifies $f(n)\geq 0$ if $n\leq 0$ has uncountable image. First of all any subset of $\mathbb{Q}$ is countable so there is no point in looking for last option. Now, As both $\mathbb{Z}$ and $\mathbb{Q}$ are countable, there could be a possible bijective function.. Now, the first problem is i could not think of a bijection (I am very sure this exist) and second problem is even if i find some function will that old first or third possibilities. Please just do not give an answer but please give some hint and give some time to think about. Thank you :)","Question is to check which option holds true : There exist a map $f: \mathbb{Z}\rightarrow \mathbb{Q}$ such that is bijective and increasing is onto and decreasing is bijective and satifies $f(n)\geq 0$ if $n\leq 0$ has uncountable image. First of all any subset of $\mathbb{Q}$ is countable so there is no point in looking for last option. Now, As both $\mathbb{Z}$ and $\mathbb{Q}$ are countable, there could be a possible bijective function.. Now, the first problem is i could not think of a bijection (I am very sure this exist) and second problem is even if i find some function will that old first or third possibilities. Please just do not give an answer but please give some hint and give some time to think about. Thank you :)",,[]
57,Every real number lies between 2 consecutive integers,Every real number lies between 2 consecutive integers,,Is it possible to prove that every real number must between two integers without using the completeness property?,Is it possible to prove that every real number must between two integers without using the completeness property?,,[]
58,"Bounded variation and $\int_a^b |F'(x)|dx=T_F([a,b])$ implies absolutely continuous",Bounded variation and  implies absolutely continuous,"\int_a^b |F'(x)|dx=T_F([a,b])","If $F$ is of bounded variation defined on $[a,b]$, and $F$ satisfies $$\int_{a}^b |F'(x)|dx=T_F([a,b])$$ where $T_F([a,b])$ is the total variation. How to prove that $F$ is absolutely continuous? My Attempt: I used the inequality, that $P_F([a,x])$ and $N_F([a,x])$ (positive and negative variation )are both monotonic non-decreasing function, and thus their derivative exists a.e. Thus  $$\int_{a}^b |F'(x)|dx\le \int_a^b |P_F'([a,x])|dx+\int_a^b|N_F'([a,x])|dx\le P_f([a,b])+N_F([a,b])=T_F([a,b])$$ Then by the condition, the middle inequality should all be replaced by equality. But I cannot derive any useful information from the equalities, since the annoying absolute value cannot be diminished. I tried to prove  $$F(x)=F(a)+\int_{a}^x F'(t)dt$$ but this doesn't work. Applying the definition of absolute continuity also failed to give me a clearer view. Thanks for your attention!","If $F$ is of bounded variation defined on $[a,b]$, and $F$ satisfies $$\int_{a}^b |F'(x)|dx=T_F([a,b])$$ where $T_F([a,b])$ is the total variation. How to prove that $F$ is absolutely continuous? My Attempt: I used the inequality, that $P_F([a,x])$ and $N_F([a,x])$ (positive and negative variation )are both monotonic non-decreasing function, and thus their derivative exists a.e. Thus  $$\int_{a}^b |F'(x)|dx\le \int_a^b |P_F'([a,x])|dx+\int_a^b|N_F'([a,x])|dx\le P_f([a,b])+N_F([a,b])=T_F([a,b])$$ Then by the condition, the middle inequality should all be replaced by equality. But I cannot derive any useful information from the equalities, since the annoying absolute value cannot be diminished. I tried to prove  $$F(x)=F(a)+\int_{a}^x F'(t)dt$$ but this doesn't work. Applying the definition of absolute continuity also failed to give me a clearer view. Thanks for your attention!",,"['real-analysis', 'bounded-variation']"
59,Find all differentiable functions $f$ such that $f\circ f=f$,Find all differentiable functions  such that,f f\circ f=f,"I want to find all differentiable functions $f:\mathbb R \to \mathbb R$ such that $f\circ f=f$, My attempt since $f$ is differentiable, $f'(f(x))f'(x)=f'(x)$ Now if $f'(x)\neq0$($f'=0$ means constant and they satisfy) then $f'(f(x))=1$, so we are looking for functions which satisfy $f'(f(x))=1$, How to proceed in this case? ($f(x)=x$ is also an obvious solution)","I want to find all differentiable functions $f:\mathbb R \to \mathbb R$ such that $f\circ f=f$, My attempt since $f$ is differentiable, $f'(f(x))f'(x)=f'(x)$ Now if $f'(x)\neq0$($f'=0$ means constant and they satisfy) then $f'(f(x))=1$, so we are looking for functions which satisfy $f'(f(x))=1$, How to proceed in this case? ($f(x)=x$ is also an obvious solution)",,"['real-analysis', 'functional-equations']"
60,A curve whose image has positive measure,A curve whose image has positive measure,,"It is well-known that there are continuous curves $f:I \to \mathbb R^2$ (where $I \subset \mathbb R$ is an interval) whose image have positive measure (e.g Peano curve). I have read somewhere that if we require the curve to be differentiable evrywhere then this cannot happen; but if we require it to be almost everywhere differentiable, then it can happen! How could one proceed to prove the first statement and give a counterexample for the second?","It is well-known that there are continuous curves $f:I \to \mathbb R^2$ (where $I \subset \mathbb R$ is an interval) whose image have positive measure (e.g Peano curve). I have read somewhere that if we require the curve to be differentiable evrywhere then this cannot happen; but if we require it to be almost everywhere differentiable, then it can happen! How could one proceed to prove the first statement and give a counterexample for the second?",,"['real-analysis', 'differential-geometry']"
61,Lower semicontinuous function as the limit of an increasing sequence of continuous functions,Lower semicontinuous function as the limit of an increasing sequence of continuous functions,,"Let $ f:\mathbb{R}^m \rightarrow (-\infty,\infty] $ be lower semicontinuous and bounded from below. Set $f_k(x) = \inf\{f(y)+k d( x,y ): y\in \mathbb{R}^m\} $ , where $d(x,y)$ is a metric. It is easy to see that each $f_k$ is continuous and $f_1  \leq f_2\leq ...\leq f \\$. However, I don't know how to prove that $ \lim_{k \rightarrow \infty}f_k(x) = f(x) $ for every $x\in\mathbb{R}^m $.","Let $ f:\mathbb{R}^m \rightarrow (-\infty,\infty] $ be lower semicontinuous and bounded from below. Set $f_k(x) = \inf\{f(y)+k d( x,y ): y\in \mathbb{R}^m\} $ , where $d(x,y)$ is a metric. It is easy to see that each $f_k$ is continuous and $f_1  \leq f_2\leq ...\leq f \\$. However, I don't know how to prove that $ \lim_{k \rightarrow \infty}f_k(x) = f(x) $ for every $x\in\mathbb{R}^m $.",,"['real-analysis', 'semicontinuous-functions']"
62,Is Koch snowflake a continuous curve?,Is Koch snowflake a continuous curve?,,"For Koch snowflake , does there exits a continuous map from $[0,1]$ to it? The actural construction of the map may be impossible, but how to claim the existence of such a continuous map? Or can we conside the limit of a sequence of continuous map, but this sequence of continuous maps may not have continuous limit.","For Koch snowflake , does there exits a continuous map from $[0,1]$ to it? The actural construction of the map may be impossible, but how to claim the existence of such a continuous map? Or can we conside the limit of a sequence of continuous map, but this sequence of continuous maps may not have continuous limit.",,['real-analysis']
63,Dini's Theorem and tests for uniform convergence,Dini's Theorem and tests for uniform convergence,,"Suppose $f_n$ is a sequence of functions defined a set $K$ with pointwise limit function $f$. I am confused about the following. If the following conditions are satisfied: $f_n$  is continuous on $K$ for all $n$. The pointwise limit $f$ is continuous on $K$. $K$ is a compact interval (i.e., a closed and bounded interval in $\mathbb{R}$). The convergence of $f_n$ to $f$ is increasing or decreasing. Then does this imply that $f_n$ is uniformly convergent to $f$? Now a different problem: If one of these conditions is not satisfied, does this imply that $f_n$  is not uniformly convergent to $f$? If $f_n$ is not uniformly convergent to f, does this mean that one these four conditions doesn't hold? In general: what is the logical relationship between uniform convergence and these four conditions? Please, I need answers to all the above questions because this theorem always confuses me when I solve the problems and I don't know how to use it properly. Thanks for your help in advance.","Suppose $f_n$ is a sequence of functions defined a set $K$ with pointwise limit function $f$. I am confused about the following. If the following conditions are satisfied: $f_n$  is continuous on $K$ for all $n$. The pointwise limit $f$ is continuous on $K$. $K$ is a compact interval (i.e., a closed and bounded interval in $\mathbb{R}$). The convergence of $f_n$ to $f$ is increasing or decreasing. Then does this imply that $f_n$ is uniformly convergent to $f$? Now a different problem: If one of these conditions is not satisfied, does this imply that $f_n$  is not uniformly convergent to $f$? If $f_n$ is not uniformly convergent to f, does this mean that one these four conditions doesn't hold? In general: what is the logical relationship between uniform convergence and these four conditions? Please, I need answers to all the above questions because this theorem always confuses me when I solve the problems and I don't know how to use it properly. Thanks for your help in advance.",,"['real-analysis', 'analysis']"
64,Uniform semi-continuity,Uniform semi-continuity,,"Background It is a standard and important fact in basic calculus/real analysis that a continuous function on a compact metric space is in fact uniformly continuous.  That is, suppose $(X,d)$ is a compact metric space and $f\colon X \to\mathbb R$ is such that for every $x\in X$ and $\varepsilon>0$ there exists $\delta>0$ such that $d(x,y)<\delta$ implies $|f(x)-f(y)|<\varepsilon$.  Then in fact, such a $\delta$ can be chosen independently of $x$. Question Does a similar statement hold regarding semi -continuous functions?  For concreteness, let's consider upper semi-continuous functions, so suppose $(X,d)$ is compact and $f\colon X \to\mathbb R$ has the property that for every $x\in X$ and $\varepsilon >0$ there exists $\delta >0$ such that $d(x,y)<\delta$ implies $f(y) < f(x)+\varepsilon$.  (Note the asymmetry of $x$ and $y$ in this definition.)  Then is it true that $\delta=\delta(\varepsilon)$ can be chosen independently of $x$? Reformulation Given $\delta, \epsilon > 0$, consider the set $$ X_\delta^\epsilon := \lbrace x\in X \mid f(y) < f(x) + \epsilon \text{ for every } y\in B(x,\delta) \rbrace. $$ Then $f$ is upper semi-continuous if and only if $\displaystyle\bigcup_{\delta>0} X_\delta^\epsilon = X$ for every $\epsilon > 0$, and $f$ is uniformly upper semi-continuous if and only if this union stabilises -- that is, if for every $\epsilon > 0$ there exists $\delta>0$ such that $X_\delta^\epsilon = X$.","Background It is a standard and important fact in basic calculus/real analysis that a continuous function on a compact metric space is in fact uniformly continuous.  That is, suppose $(X,d)$ is a compact metric space and $f\colon X \to\mathbb R$ is such that for every $x\in X$ and $\varepsilon>0$ there exists $\delta>0$ such that $d(x,y)<\delta$ implies $|f(x)-f(y)|<\varepsilon$.  Then in fact, such a $\delta$ can be chosen independently of $x$. Question Does a similar statement hold regarding semi -continuous functions?  For concreteness, let's consider upper semi-continuous functions, so suppose $(X,d)$ is compact and $f\colon X \to\mathbb R$ has the property that for every $x\in X$ and $\varepsilon >0$ there exists $\delta >0$ such that $d(x,y)<\delta$ implies $f(y) < f(x)+\varepsilon$.  (Note the asymmetry of $x$ and $y$ in this definition.)  Then is it true that $\delta=\delta(\varepsilon)$ can be chosen independently of $x$? Reformulation Given $\delta, \epsilon > 0$, consider the set $$ X_\delta^\epsilon := \lbrace x\in X \mid f(y) < f(x) + \epsilon \text{ for every } y\in B(x,\delta) \rbrace. $$ Then $f$ is upper semi-continuous if and only if $\displaystyle\bigcup_{\delta>0} X_\delta^\epsilon = X$ for every $\epsilon > 0$, and $f$ is uniformly upper semi-continuous if and only if this union stabilises -- that is, if for every $\epsilon > 0$ there exists $\delta>0$ such that $X_\delta^\epsilon = X$.",,['real-analysis']
65,"Are there continuous functions $f,g:\mathbb{R}\longrightarrow\mathbb{R}$ such that for any $x$, $f(g(x))=\sin x$ and $g(f(x))=\cos x$?","Are there continuous functions  such that for any ,  and ?","f,g:\mathbb{R}\longrightarrow\mathbb{R} x f(g(x))=\sin x g(f(x))=\cos x","Is it possible to find continuous functions $f,g : \mathbb{R}\longrightarrow \mathbb{R}$ such that for any $x$ , $f(g(x)) = \sin x$ and $g(f(x)) = \cos x$ ? (The question came up to me while thinking on another problem. but I couldn’t solve it)","Is it possible to find continuous functions such that for any , and ? (The question came up to me while thinking on another problem. but I couldn’t solve it)","f,g : \mathbb{R}\longrightarrow \mathbb{R} x f(g(x)) = \sin x g(f(x)) = \cos x","['real-analysis', 'functions', 'functional-equations']"
66,Finding $f$ such that $f(f(f(f...(x)))) = x$,Finding  such that,f f(f(f(f...(x)))) = x,"I would like to find the set of continuous functions $f_n(x)$ , where $f_n(x):\mathbb{R}\to \mathbb{R}$ satisfies $$f_n(f_n(f_n(f_n...(x)))) = x$$ where there are $n$ iterations of $f(x)$ . For example $f_1(x)$ would be the solution to $f_1(x)=x$ . $f_2(x)$ would be the solution to $f_2(f_2(x)) = x$ . For $f_1(x)$ , the only solution is $f_1(x)=x$ . For $f_2(x)$ , the solutions are involutions . For $f_3(x)$ , the only answer is $f_3(x)=x$ . For all other $f_n(x)$ , one solution is $f_n(x) = x$ . My question: For $n \ge 3$ , is $f_n(x) = x$ the only solution? If not, what are the solutions? Edit: @MattSamuel said that any involution works for an even $n$ . This is because $f_n(f_n(x))$ can be replaced with $x$ . For example, $$f_2(f_2(f_2(f_2(f_2(f_2(x)))))) = f_2(f_2(f_2(f_2(x)))) = f_2(f_2(x)) = x$$ However, this does not necessarily mean that involutions are the only set of solutions for $f_{2k}(x)$ .","I would like to find the set of continuous functions , where satisfies where there are iterations of . For example would be the solution to . would be the solution to . For , the only solution is . For , the solutions are involutions . For , the only answer is . For all other , one solution is . My question: For , is the only solution? If not, what are the solutions? Edit: @MattSamuel said that any involution works for an even . This is because can be replaced with . For example, However, this does not necessarily mean that involutions are the only set of solutions for .",f_n(x) f_n(x):\mathbb{R}\to \mathbb{R} f_n(f_n(f_n(f_n...(x)))) = x n f(x) f_1(x) f_1(x)=x f_2(x) f_2(f_2(x)) = x f_1(x) f_1(x)=x f_2(x) f_3(x) f_3(x)=x f_n(x) f_n(x) = x n \ge 3 f_n(x) = x n f_n(f_n(x)) x f_2(f_2(f_2(f_2(f_2(f_2(x)))))) = f_2(f_2(f_2(f_2(x)))) = f_2(f_2(x)) = x f_{2k}(x),"['real-analysis', 'functional-equations']"
67,"""Counterexample"" for the Inverse function theorem","""Counterexample"" for the Inverse function theorem",,"In a lecture we stated the theorem as follows: Let $\Omega\subseteq\mathbb{R}^n$ be an open set and $f:\Omega\to\mathbb{R}^n$ a $\mathscr{C}^1(\Omega)$ function. If $|J_f(a)|\ne0$ for some $a\in\Omega$ then there exists $\delta>0$ such that $g:=f\vert_{B(a,\delta)}$ is injective and ... This only is a sufficient condition, so is there any function whose jacobian has determinant $0$ at every point but still is injective? If the determinant only vanished on one single point something similar to $f(x)=x^3$ at $x=0$ in $\mathbb{R}$ would do the trick, but if $|f'(x)|=0$ for every $x\in\Omega\subseteq\mathbb{R}$ then $f$ is constant and not injective. Does the same hold in $\mathbb{R}^n$ ? Thanks","In a lecture we stated the theorem as follows: Let be an open set and a function. If for some then there exists such that is injective and ... This only is a sufficient condition, so is there any function whose jacobian has determinant at every point but still is injective? If the determinant only vanished on one single point something similar to at in would do the trick, but if for every then is constant and not injective. Does the same hold in ? Thanks","\Omega\subseteq\mathbb{R}^n f:\Omega\to\mathbb{R}^n \mathscr{C}^1(\Omega) |J_f(a)|\ne0 a\in\Omega \delta>0 g:=f\vert_{B(a,\delta)} 0 f(x)=x^3 x=0 \mathbb{R} |f'(x)|=0 x\in\Omega\subseteq\mathbb{R} f \mathbb{R}^n","['real-analysis', 'examples-counterexamples', 'inverse-function-theorem']"
68,"Cardinality of the set of Riemann integrable functions on [0,1]","Cardinality of the set of Riemann integrable functions on [0,1]",,"What is the cardinality of the set $\mathcal{R}[0,1]$ of all Riemann integrable real functions on [0,1]? I expect it to be $2^\mathfrak{c}$. A function is Riemann integrable if and only if it is continuous almost everywhere and bounded. Since a set of measure 0 can be uncountable, I assume one can construct $2^\mathfrak{c}$ subsets of $[0,1]$ that have measure 0 yet are discontinuity sets of real functions. But then I also realize that there can be measure zero sets which are never discontinuity sets of a real function. So, I am stuck at this point and have no idea how to proceed.","What is the cardinality of the set $\mathcal{R}[0,1]$ of all Riemann integrable real functions on [0,1]? I expect it to be $2^\mathfrak{c}$. A function is Riemann integrable if and only if it is continuous almost everywhere and bounded. Since a set of measure 0 can be uncountable, I assume one can construct $2^\mathfrak{c}$ subsets of $[0,1]$ that have measure 0 yet are discontinuity sets of real functions. But then I also realize that there can be measure zero sets which are never discontinuity sets of a real function. So, I am stuck at this point and have no idea how to proceed.",,"['real-analysis', 'riemann-integration']"
69,On the summation $\sum \limits_{n=1}^{\infty} \arctan \left ( \frac{1}{n^3+n^2+n+1} \right )$,On the summation,\sum \limits_{n=1}^{\infty} \arctan \left ( \frac{1}{n^3+n^2+n+1} \right ),"Here is a problem that I ran into. I seriously doubt if there is a closed form but you never know. Evaluate the series $$\mathcal{S} = \sum_{n=1}^\infty \arctan \left ( \frac 1 {n^3+n^2+n+1} \right) $$ I searched in vain to attack it using telescopic summation but I failed miserably. Then I remembered the following technique. Since ${\rm Im} \log (1+ix) = \arctan x$ we can express the sum as follows \begin{align*} \sum_{n=1}^\infty \arctan \left ( \frac{1}{n^3+n^2+n+1} \right ) &= \sum_{n=1}^\infty \arctan \left [ \frac{1}{(n+1)(n^2+1)} \right ] \\   &= \sum_{n=1}^\infty \operatorname{Im} \left [ \log \left ( 1 + \frac{i}{(n+1)(n^2+1)} \right ) \right ] \\   &= \operatorname{Im} \log \left [ \prod_{n=1}^\infty \left ( 1 + \frac{i}{(n+1)(n^2+1)} \right ) \right ]  \end{align*} I tried to combine it with the famous Euler product $$ \frac{\sin \pi z}{\pi z} = \prod_{n=1}^{\infty} \left( 1 - \frac{z^2}{n^2} \right) \tag{1} $$ but I see no connection. So, is there a possible way to evaluate it?","Here is a problem that I ran into. I seriously doubt if there is a closed form but you never know. Evaluate the series $$\mathcal{S} = \sum_{n=1}^\infty \arctan \left ( \frac 1 {n^3+n^2+n+1} \right) $$ I searched in vain to attack it using telescopic summation but I failed miserably. Then I remembered the following technique. Since ${\rm Im} \log (1+ix) = \arctan x$ we can express the sum as follows \begin{align*} \sum_{n=1}^\infty \arctan \left ( \frac{1}{n^3+n^2+n+1} \right ) &= \sum_{n=1}^\infty \arctan \left [ \frac{1}{(n+1)(n^2+1)} \right ] \\   &= \sum_{n=1}^\infty \operatorname{Im} \left [ \log \left ( 1 + \frac{i}{(n+1)(n^2+1)} \right ) \right ] \\   &= \operatorname{Im} \log \left [ \prod_{n=1}^\infty \left ( 1 + \frac{i}{(n+1)(n^2+1)} \right ) \right ]  \end{align*} I tried to combine it with the famous Euler product $$ \frac{\sin \pi z}{\pi z} = \prod_{n=1}^{\infty} \left( 1 - \frac{z^2}{n^2} \right) \tag{1} $$ but I see no connection. So, is there a possible way to evaluate it?",,"['real-analysis', 'sequences-and-series']"
70,How to prove that the both definition of completeness of $\mathbb{R}$ are equivalent?,How to prove that the both definition of completeness of  are equivalent?,\mathbb{R},"In the definition of completeness of a set, in particular $\mathbb{R}$, I have seen the following definitions: Dedekind: Every non-empty bounded of subset has a least upper bound (with respect to the natural order). Cauchy: Every Cauchy sequence converges. However, how can one prove that both of these definitions are equivalent ?","In the definition of completeness of a set, in particular $\mathbb{R}$, I have seen the following definitions: Dedekind: Every non-empty bounded of subset has a least upper bound (with respect to the natural order). Cauchy: Every Cauchy sequence converges. However, how can one prove that both of these definitions are equivalent ?",,"['real-analysis', 'cauchy-sequences']"
71,"If $ \int fg = 0 $ for all compactly supported continuous g, then f = 0 a.e.?","If  for all compactly supported continuous g, then f = 0 a.e.?", \int fg = 0 ,"I was wondering whether the following statement is true and, if so, how it can be shown: If $ f \in L^{1}_{Loc}(\mathbb{R}^n) $ and if for all compactly supported continuous functions $ g: \mathbb{R}^n \to \mathbb{C} $ we have that the Lebesgue integral of $ f $ multiplied by $ g $ equals zero, i.e. $$ \int_{\mathbb{R}^n} f(x)g(x) \mathrm{d}x = 0 , $$ then $ f(x) = 0 $ almost everywhere. I would be very grateful for any answers or hints! N.B. I am aware that this question has already been addressed. In If $f\in L^1_{loc}(\mathbb{R})$ and $\int f\varphi=0$ for all $\varphi$ continuous with compact support, then $f=0$ a.e. , I am not quite sure about how to create a sequence of compactly supported continuous functions such that $ \varphi_n\to \frac{f}{|f|+1} $. This particular question may have its answer in If $f\in L^1(\mathbb{R})$ is such that $\int_{\mathbb{R}}f\phi=0$ for all continuous compactly supported $\phi$, then $f\equiv 0$. , however here I am unsure about the meaning of a ""regularizing sequence""; why does $ \phi_n\ast f\to f $ in $L^1$ sense if $ \phi_n(x) = n\phi(nx) $, where $ \phi\in \mathcal C^\infty_c(\Bbb R) $ with $ \phi\ge 0 $ and $ \int_{\Bbb R}\phi(x)dx=1 $? Once again, any answer would be much appreciated!","I was wondering whether the following statement is true and, if so, how it can be shown: If $ f \in L^{1}_{Loc}(\mathbb{R}^n) $ and if for all compactly supported continuous functions $ g: \mathbb{R}^n \to \mathbb{C} $ we have that the Lebesgue integral of $ f $ multiplied by $ g $ equals zero, i.e. $$ \int_{\mathbb{R}^n} f(x)g(x) \mathrm{d}x = 0 , $$ then $ f(x) = 0 $ almost everywhere. I would be very grateful for any answers or hints! N.B. I am aware that this question has already been addressed. In If $f\in L^1_{loc}(\mathbb{R})$ and $\int f\varphi=0$ for all $\varphi$ continuous with compact support, then $f=0$ a.e. , I am not quite sure about how to create a sequence of compactly supported continuous functions such that $ \varphi_n\to \frac{f}{|f|+1} $. This particular question may have its answer in If $f\in L^1(\mathbb{R})$ is such that $\int_{\mathbb{R}}f\phi=0$ for all continuous compactly supported $\phi$, then $f\equiv 0$. , however here I am unsure about the meaning of a ""regularizing sequence""; why does $ \phi_n\ast f\to f $ in $L^1$ sense if $ \phi_n(x) = n\phi(nx) $, where $ \phi\in \mathcal C^\infty_c(\Bbb R) $ with $ \phi\ge 0 $ and $ \int_{\Bbb R}\phi(x)dx=1 $? Once again, any answer would be much appreciated!",,"['real-analysis', 'functional-analysis', 'lebesgue-integral', 'lebesgue-measure', 'almost-everywhere']"
72,Proof of Lemma 8.5.14 in Terence Tao Analysis I,Proof of Lemma 8.5.14 in Terence Tao Analysis I,,"Lemma 8.5.14. Let X be a partially ordered set with ordering relation $\leq$, and let $x_0$ be an element of $X$. Then there is a well-ordered subset $Y$ of $X$ which has $x_0$ as its minimal element, and which has no strict upper bound. Proof. The intuition behind this lemma is that one is trying to perform the following algorithm: we initalize $Y:=\{x_0\}$. If $Y$ has no strict upper bound, then we are done; otherwise, we choose a strict upper bound and add it to $Y$ . Then we look again to see if $Y$ has a strict upper bound or not. If not, we are done; otherwise we choose another strict upper bound and add it to $Y$ . We continue this algorithm “infinitely often” until we exhaust all the strict upper bounds; the axiom of choice comes in because infinitely many choices are involved. This is however not a rigorous proof because it is quite difficult to precisely pin down what it means to perform an algorithm “infinitely often”. Instead, what we will do is that we will isolate a collection of “partially completed” sets $Y$, which we shall call good sets, and then take the union of all these good sets to obtain a “completed” object $Y_{\infty}$ which will indeed have no strict upper bound. We now begin the rigorous proof. Suppose for sake of contradiction that every well-ordered subset $Y$ of $X$ which has $x_0$ as its minimal element has at least one strict upper bound. Using the axiom of choice (in the form of Proposition 8.4.7), we can thus assign a strict upper bound $s(Y)\in X $ to each well-ordered subset $Y$ of $X$ which has $x_0$ as its minimal element. Let us define a special class of subsets $Y$ of $X$. We say that a subset $Y$ of $X$ is good iff it is well-ordered, contains $x_0$ as its minimal element, and obeys the property that $x=s\left(\{y\in Y:y<x\}\right)$ for all $x \in Y\backslash \{x_0\}$. Note that if $x \in Y\backslash \{x_0\}$ then the set $\{y \in Y :y<x\}$ is a subset of $X$ which is well-ordered and contains $x_0$ as its minimal element. Let $\Omega:=\{Y \subseteq X: Y\, \text{is good}\}$ be the collection of all good subsets of $X$. This collection is not empty, since the subset $\{x_0\}$ of $X$ is clearly good (why?). We make the following important observation: if $Y$ and $Y^\prime$ are two good subsets of $X$, then every element of $Y^{\prime}\backslash Y$ is a strict upper bound for $Y$ , and every element of $Y\backslash Y^{\prime}$ is a strict upper bound for $Y^{\prime}$. In particular, given any two good sets $Y$ and $Y^\prime$, at least one of $Y^{\prime}\backslash Y$ and $Y \backslash Y^{\prime}$ must be empty (since they are both strict upper bounds of each other). In other words, $\Omega$ is totally ordered by set inclusion: given any two good sets $Y$ and $Y^\prime$, either $Y \subseteq Y^\prime$ or $Y^\prime \subseteq Y$. Can anyone help me to understand ""if $Y$ and $Y^\prime$ are two good subsets of $X$, then every element of $Y^{\prime}\backslash Y$ is a strict upper bound for $Y$ , and every element of $Y\backslash Y^{\prime}$ is a strict upper bound for $Y^{\prime}$. ""","Lemma 8.5.14. Let X be a partially ordered set with ordering relation $\leq$, and let $x_0$ be an element of $X$. Then there is a well-ordered subset $Y$ of $X$ which has $x_0$ as its minimal element, and which has no strict upper bound. Proof. The intuition behind this lemma is that one is trying to perform the following algorithm: we initalize $Y:=\{x_0\}$. If $Y$ has no strict upper bound, then we are done; otherwise, we choose a strict upper bound and add it to $Y$ . Then we look again to see if $Y$ has a strict upper bound or not. If not, we are done; otherwise we choose another strict upper bound and add it to $Y$ . We continue this algorithm “infinitely often” until we exhaust all the strict upper bounds; the axiom of choice comes in because infinitely many choices are involved. This is however not a rigorous proof because it is quite difficult to precisely pin down what it means to perform an algorithm “infinitely often”. Instead, what we will do is that we will isolate a collection of “partially completed” sets $Y$, which we shall call good sets, and then take the union of all these good sets to obtain a “completed” object $Y_{\infty}$ which will indeed have no strict upper bound. We now begin the rigorous proof. Suppose for sake of contradiction that every well-ordered subset $Y$ of $X$ which has $x_0$ as its minimal element has at least one strict upper bound. Using the axiom of choice (in the form of Proposition 8.4.7), we can thus assign a strict upper bound $s(Y)\in X $ to each well-ordered subset $Y$ of $X$ which has $x_0$ as its minimal element. Let us define a special class of subsets $Y$ of $X$. We say that a subset $Y$ of $X$ is good iff it is well-ordered, contains $x_0$ as its minimal element, and obeys the property that $x=s\left(\{y\in Y:y<x\}\right)$ for all $x \in Y\backslash \{x_0\}$. Note that if $x \in Y\backslash \{x_0\}$ then the set $\{y \in Y :y<x\}$ is a subset of $X$ which is well-ordered and contains $x_0$ as its minimal element. Let $\Omega:=\{Y \subseteq X: Y\, \text{is good}\}$ be the collection of all good subsets of $X$. This collection is not empty, since the subset $\{x_0\}$ of $X$ is clearly good (why?). We make the following important observation: if $Y$ and $Y^\prime$ are two good subsets of $X$, then every element of $Y^{\prime}\backslash Y$ is a strict upper bound for $Y$ , and every element of $Y\backslash Y^{\prime}$ is a strict upper bound for $Y^{\prime}$. In particular, given any two good sets $Y$ and $Y^\prime$, at least one of $Y^{\prime}\backslash Y$ and $Y \backslash Y^{\prime}$ must be empty (since they are both strict upper bounds of each other). In other words, $\Omega$ is totally ordered by set inclusion: given any two good sets $Y$ and $Y^\prime$, either $Y \subseteq Y^\prime$ or $Y^\prime \subseteq Y$. Can anyone help me to understand ""if $Y$ and $Y^\prime$ are two good subsets of $X$, then every element of $Y^{\prime}\backslash Y$ is a strict upper bound for $Y$ , and every element of $Y\backslash Y^{\prime}$ is a strict upper bound for $Y^{\prime}$. """,,['real-analysis']
73,Self study Control Theory,Self study Control Theory,,"Please forgive the long setup but I think it is relevant to my question. I am a third year Electrical Engineering student (before dismissing me a an engineer please read the rest of the question) and I am planning on doing graduate studies in Control Theory. I find it really brings together pure math and some sort of distant application which is enough for me. As such I've taken the usual engineering math courses  (Calculus, Linear Algebra, Complex Analysis, Dynamical Systems, a whole ton of Fourier analysis, PDEs, Probabilities and such) where they proceeded to completely disregard any rigor. The only thing close to rigorous math that I actually did was in our Algorithms course which was fascinating  (P=NP, Graphs, etc) and actually satisfyingly rigorous. Anyways, I am now at a point where I want to strengthen my actual math knowledge and especially work towards a really good knowledge of Differential Geometry, Complex Analysis and Topology. As such I began studying the basics: real analysis with Chapman Pugh which I am really enjoying. However I would have appreciated some input on what you think is the best way to proceed from here. My plan was next to do Topology with Munkres, Abstract Algebra with Dummit (perhaps not everything but at the very least a good coverage of group theory) and sometime after Smooth Manifolds by Lee and Papa Rudin. What do you think?","Please forgive the long setup but I think it is relevant to my question. I am a third year Electrical Engineering student (before dismissing me a an engineer please read the rest of the question) and I am planning on doing graduate studies in Control Theory. I find it really brings together pure math and some sort of distant application which is enough for me. As such I've taken the usual engineering math courses  (Calculus, Linear Algebra, Complex Analysis, Dynamical Systems, a whole ton of Fourier analysis, PDEs, Probabilities and such) where they proceeded to completely disregard any rigor. The only thing close to rigorous math that I actually did was in our Algorithms course which was fascinating  (P=NP, Graphs, etc) and actually satisfyingly rigorous. Anyways, I am now at a point where I want to strengthen my actual math knowledge and especially work towards a really good knowledge of Differential Geometry, Complex Analysis and Topology. As such I began studying the basics: real analysis with Chapman Pugh which I am really enjoying. However I would have appreciated some input on what you think is the best way to proceed from here. My plan was next to do Topology with Munkres, Abstract Algebra with Dummit (perhaps not everything but at the very least a good coverage of group theory) and sometime after Smooth Manifolds by Lee and Papa Rudin. What do you think?",,"['real-analysis', 'general-topology', 'differential-geometry', 'dynamical-systems', 'control-theory']"
74,Fourier Transform of $\ln(f(t))$,Fourier Transform of,\ln(f(t)),"I want to compute Fourier transform of $\ln(f(t))$ maybe in a sense of distributions? Where we can assume that: $f(t) > 0$ $f(t) \in L^1$ $f(t)$ is continuous $\lim_{t \to \infty} f(t)=0$ and  $\lim_{t \to -\infty} f(t)=0$ Denote Fourier transform of $f(t)$ by $\mathcal{F}(f(t))=F(\omega)$ I am also fine with some other restrictions. For example: $f(t)$ is analytical (i.e. all derivatives exist) My main question are: 1) is the above set of condition enough to guarantee existence of Fourier transform? If not, what are the requirements? 2)  what is the Fourier transform? Edit Based on the suggestion of Mattos \begin{align*} &\int_{-\infty}^\infty  \ln(f(t)) e^{-i \omega t} dt=  \ln(f(t))\frac{e^{-j\omega t}}{-j\omega} \Big|_{t=-\infty}^{t=\infty}-\int_{-\infty}^\infty \frac{e^{-j\omega t}}{-j\omega} \frac{f'(t)}{f(t)} dt\\ & =\ln(f(t))\frac{e^{-jwt}}{-jw} \Big|_{t=-\infty}^{t=\infty} +\frac{1}{j \omega}\mathcal{F} \left(\frac{f'(t)}{f(t)} \right) \end{align*} But, now how to compute $\ln(f(t))\frac{e^{-jwt}}{-jw} \Big|_{t=-\infty}^{t=\infty}$. Possible Solution to 2): I might have solution but not sure. It relays on using the following property. \begin{align*} \mathcal{F} \left( \frac{d}{dt} g(t) \right)=(j\omega) \mathcal{F}(g(t)) \end{align*} Now take $g(t)=\ln(f(t))$ so we have \begin{align*} \mathcal{F} \left( \frac{d}{dt} \ln(f(t)) \right)=(j\omega) \mathcal{F}(\ln(f(t)))\\ \mathcal{F} \left(  \frac{f'(t)}{f(t)}\right)=(j\omega) \mathcal{F}(\ln(f(t)))\\ \frac{\mathcal{F} \left(  \frac{f'(t)}{f(t)}\right)}{(j\omega)}= \mathcal{F}(\ln(f(t))) \end{align*} So, of course this solution requires that $f'(t)$ exists.  Is this reasoning correct? Is there any technicality that I missed? Here is the example I tried Let $f(t)=e^{-t^2}$ then $\ln(f(t))=-t^2$,  \begin{align*} \mathcal{F}(\ln(f(t)))=\mathcal{F}(-t^2)=2\pi\delta^{(2)}(\omega) \end{align*} On the other hand, \begin{align*} \frac{1}{j \omega} \mathcal{F} \left( \frac{f'(t)}{f(t)} \right)&=\frac{1}{j \omega} \mathcal{F} \left( \frac{-2te^{-t^2}}{e^{-t^2}} \right)=\frac{1}{j \omega} \mathcal{F} \left( -2t \right)=-2 \frac{1}{j \omega} (2 \pi j) \delta^{(1)}(\omega)\\ &=  -(4 \pi ) \frac{1}{ \omega}\delta^{(1)}(\omega)=(2 \pi ) \delta^{(2)}(\omega) \end{align*} the last equality uses $\omega \delta^{(2)}(\omega)=-2\delta^{(1)}(\omega)$ So, this is an example when my approach works. Thank you for any help, I really appreciate it.","I want to compute Fourier transform of $\ln(f(t))$ maybe in a sense of distributions? Where we can assume that: $f(t) > 0$ $f(t) \in L^1$ $f(t)$ is continuous $\lim_{t \to \infty} f(t)=0$ and  $\lim_{t \to -\infty} f(t)=0$ Denote Fourier transform of $f(t)$ by $\mathcal{F}(f(t))=F(\omega)$ I am also fine with some other restrictions. For example: $f(t)$ is analytical (i.e. all derivatives exist) My main question are: 1) is the above set of condition enough to guarantee existence of Fourier transform? If not, what are the requirements? 2)  what is the Fourier transform? Edit Based on the suggestion of Mattos \begin{align*} &\int_{-\infty}^\infty  \ln(f(t)) e^{-i \omega t} dt=  \ln(f(t))\frac{e^{-j\omega t}}{-j\omega} \Big|_{t=-\infty}^{t=\infty}-\int_{-\infty}^\infty \frac{e^{-j\omega t}}{-j\omega} \frac{f'(t)}{f(t)} dt\\ & =\ln(f(t))\frac{e^{-jwt}}{-jw} \Big|_{t=-\infty}^{t=\infty} +\frac{1}{j \omega}\mathcal{F} \left(\frac{f'(t)}{f(t)} \right) \end{align*} But, now how to compute $\ln(f(t))\frac{e^{-jwt}}{-jw} \Big|_{t=-\infty}^{t=\infty}$. Possible Solution to 2): I might have solution but not sure. It relays on using the following property. \begin{align*} \mathcal{F} \left( \frac{d}{dt} g(t) \right)=(j\omega) \mathcal{F}(g(t)) \end{align*} Now take $g(t)=\ln(f(t))$ so we have \begin{align*} \mathcal{F} \left( \frac{d}{dt} \ln(f(t)) \right)=(j\omega) \mathcal{F}(\ln(f(t)))\\ \mathcal{F} \left(  \frac{f'(t)}{f(t)}\right)=(j\omega) \mathcal{F}(\ln(f(t)))\\ \frac{\mathcal{F} \left(  \frac{f'(t)}{f(t)}\right)}{(j\omega)}= \mathcal{F}(\ln(f(t))) \end{align*} So, of course this solution requires that $f'(t)$ exists.  Is this reasoning correct? Is there any technicality that I missed? Here is the example I tried Let $f(t)=e^{-t^2}$ then $\ln(f(t))=-t^2$,  \begin{align*} \mathcal{F}(\ln(f(t)))=\mathcal{F}(-t^2)=2\pi\delta^{(2)}(\omega) \end{align*} On the other hand, \begin{align*} \frac{1}{j \omega} \mathcal{F} \left( \frac{f'(t)}{f(t)} \right)&=\frac{1}{j \omega} \mathcal{F} \left( \frac{-2te^{-t^2}}{e^{-t^2}} \right)=\frac{1}{j \omega} \mathcal{F} \left( -2t \right)=-2 \frac{1}{j \omega} (2 \pi j) \delta^{(1)}(\omega)\\ &=  -(4 \pi ) \frac{1}{ \omega}\delta^{(1)}(\omega)=(2 \pi ) \delta^{(2)}(\omega) \end{align*} the last equality uses $\omega \delta^{(2)}(\omega)=-2\delta^{(1)}(\omega)$ So, this is an example when my approach works. Thank you for any help, I really appreciate it.",,"['real-analysis', 'functional-analysis', 'fourier-analysis']"
75,Does the series $1 + \frac{1}{2} - \frac{1}{3} + \frac{1}{4} + \frac{1}{5} - \frac{1}{6} + \dots$ converge?,Does the series  converge?,1 + \frac{1}{2} - \frac{1}{3} + \frac{1}{4} + \frac{1}{5} - \frac{1}{6} + \dots,"Does the following variant of the harmonic series converge? If it diverges (which I think it does), can I know if it diverges to $\infty$ or has no limit? Note that the series is not alternating in the classical sense of the word. $$1 + \frac{1}{2} -\frac{1}{3} + \frac{1}{4} + \frac{1}{5} - \frac{1}{6} + \dots$$ The generic term of the series would have to be something like, $$a_n = \left\{\begin{array}{ll} -\frac{1}{n}, & \text{if } 3 \mid n  \\ \;\, \,\, \frac{1}{n}, & \text{otherwise}\end{array}\right.$$ I'm not sure if it's very helpful. The terms divisible by 3 are negative, and the others are positive. Is there a way to decide and prove whether an alternating series of this sort (e.g. with a period other than 2) converges? Or one where terms are positive or negative according to some other rule? Almost all convergence tests I've come across are generally limited to either simple alternating series or where all the terms are positive.","Does the following variant of the harmonic series converge? If it diverges (which I think it does), can I know if it diverges to $\infty$ or has no limit? Note that the series is not alternating in the classical sense of the word. $$1 + \frac{1}{2} -\frac{1}{3} + \frac{1}{4} + \frac{1}{5} - \frac{1}{6} + \dots$$ The generic term of the series would have to be something like, $$a_n = \left\{\begin{array}{ll} -\frac{1}{n}, & \text{if } 3 \mid n  \\ \;\, \,\, \frac{1}{n}, & \text{otherwise}\end{array}\right.$$ I'm not sure if it's very helpful. The terms divisible by 3 are negative, and the others are positive. Is there a way to decide and prove whether an alternating series of this sort (e.g. with a period other than 2) converges? Or one where terms are positive or negative according to some other rule? Almost all convergence tests I've come across are generally limited to either simple alternating series or where all the terms are positive.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
76,"Show that any continuous $f:[0,1] \rightarrow [0,1]$ has a fixed point $\zeta$",Show that any continuous  has a fixed point,"f:[0,1] \rightarrow [0,1] \zeta","Be a continuous function $f:[0,1] \rightarrow [0,1]$. Show that there is a $\zeta \in [0,1]$ with $f(\zeta)=\zeta$ ($\zeta$ is called fixed point). Consider the function $g:[0,1] \rightarrow [-1,1]$, $g(x):= f(x)-x$. $g$ is continuous. Because of $f(0),f(1) \in [0,1]$ is $g(0)\geq0$ and $g(1)\leq 0$. Because $f(0)$ has a value between $0$ and $1$, $f(0)\geq 0$. $g(0) = f(0)-0= f(0) \geq 0 - 0 = 0$. Because $f(1)$ has a value between $0$ and $1$, $ f(1) \leq 1 $. $g(1)=f(1)-1 \leq 1 - 1 = 0$ $\Leftrightarrow g(0)\geq 0$ and $g(1)\leq 0 $ After the IVT: $\exists \zeta \in [0,1]:g(\zeta)=0 \Leftrightarrow f(\zeta) = \zeta $ $\zeta$ is a fixed point of $f$. $\Box$ My questions are: Is this proof done in the correct way or have I missed something? Is there something I can improve?","Be a continuous function $f:[0,1] \rightarrow [0,1]$. Show that there is a $\zeta \in [0,1]$ with $f(\zeta)=\zeta$ ($\zeta$ is called fixed point). Consider the function $g:[0,1] \rightarrow [-1,1]$, $g(x):= f(x)-x$. $g$ is continuous. Because of $f(0),f(1) \in [0,1]$ is $g(0)\geq0$ and $g(1)\leq 0$. Because $f(0)$ has a value between $0$ and $1$, $f(0)\geq 0$. $g(0) = f(0)-0= f(0) \geq 0 - 0 = 0$. Because $f(1)$ has a value between $0$ and $1$, $ f(1) \leq 1 $. $g(1)=f(1)-1 \leq 1 - 1 = 0$ $\Leftrightarrow g(0)\geq 0$ and $g(1)\leq 0 $ After the IVT: $\exists \zeta \in [0,1]:g(\zeta)=0 \Leftrightarrow f(\zeta) = \zeta $ $\zeta$ is a fixed point of $f$. $\Box$ My questions are: Is this proof done in the correct way or have I missed something? Is there something I can improve?",,"['real-analysis', 'analysis']"
77,"Increasing, bounded and continuous is uniformly continuous","Increasing, bounded and continuous is uniformly continuous",,"Let $f:(x,y)\to \mathbb{R}$ be increasing, bounded and continuous on $(x,y)$. Prove that $f$ is uniformly continuous on $(x,y)$. Since it is bounded then there exists an $M\in \mathbb{R}$ such that $|f(x,y)|<M$ and since it is continuous then $$\forall \epsilon > 0, \forall x \in X, \exists \delta > 0 : |x - y| < \delta \implies |f(x) - f(y)| < \epsilon.$$  To show it is uniformly continuous I must show: $$\text{there exists} \ \epsilon >0 \ \forall \ \delta \ \exists \  (x_0,x)\in I:\{|x -x_0|<\delta \implies |f(x)-f(x_0)|\le \epsilon\}$$ but I am not sure how I can show that?","Let $f:(x,y)\to \mathbb{R}$ be increasing, bounded and continuous on $(x,y)$. Prove that $f$ is uniformly continuous on $(x,y)$. Since it is bounded then there exists an $M\in \mathbb{R}$ such that $|f(x,y)|<M$ and since it is continuous then $$\forall \epsilon > 0, \forall x \in X, \exists \delta > 0 : |x - y| < \delta \implies |f(x) - f(y)| < \epsilon.$$  To show it is uniformly continuous I must show: $$\text{there exists} \ \epsilon >0 \ \forall \ \delta \ \exists \  (x_0,x)\in I:\{|x -x_0|<\delta \implies |f(x)-f(x_0)|\le \epsilon\}$$ but I am not sure how I can show that?",,['real-analysis']
78,Can the the radius of convergence increase due to composition of two power series?,Can the the radius of convergence increase due to composition of two power series?,,"When composing power series, is the radius of convergence the minimum of that of the individual series, or is it like for multiplication and addition of power series where the resultant radius of convergence may be larger than for the individual series? If the radius of convergence is the minimum as described, then is the interval of convergence the intersection of the two individual intervals of convergence, or is behaviour at the end-points not guaranteed (just like it isn't when differentiating a power series to obtain another)?","When composing power series, is the radius of convergence the minimum of that of the individual series, or is it like for multiplication and addition of power series where the resultant radius of convergence may be larger than for the individual series? If the radius of convergence is the minimum as described, then is the interval of convergence the intersection of the two individual intervals of convergence, or is behaviour at the end-points not guaranteed (just like it isn't when differentiating a power series to obtain another)?",,"['calculus', 'real-analysis', 'complex-analysis', 'analysis', 'power-series']"
79,How to prove that $\lim(\underset{k\rightarrow\infty}{\lim}(\cos(|n!\pi x|)^{2k}))=\chi_\mathbb{Q}$ [duplicate],How to prove that  [duplicate],\lim(\underset{k\rightarrow\infty}{\lim}(\cos(|n!\pi x|)^{2k}))=\chi_\mathbb{Q},"This question already has answers here : Closed 11 years ago . Possible Duplicate: How is this called? Rationals and irrationals Please help me prove, that $$\underset{n\rightarrow\infty}{\lim}\left(\underset{k\rightarrow\infty}{\lim}(\cos(|n!\pi x|)^{2k})\right)=\begin{cases} 1 & \iff x\in\mathbb{Q}\\ 0 & \iff x\notin\mathbb{Q} \end{cases}$$ Seems very complicated, but it's on calc I. I've tried use series expansions of cos, but it don't lead to answer. Thanks in advance! Edit Please don't use too much advanced techniques.","This question already has answers here : Closed 11 years ago . Possible Duplicate: How is this called? Rationals and irrationals Please help me prove, that $$\underset{n\rightarrow\infty}{\lim}\left(\underset{k\rightarrow\infty}{\lim}(\cos(|n!\pi x|)^{2k})\right)=\begin{cases} 1 & \iff x\in\mathbb{Q}\\ 0 & \iff x\notin\mathbb{Q} \end{cases}$$ Seems very complicated, but it's on calc I. I've tried use series expansions of cos, but it don't lead to answer. Thanks in advance! Edit Please don't use too much advanced techniques.",,['real-analysis']
80,"If $f$ is a positive, monotone decreasing function, prove that $\int_0^1xf(x)^2dx \int_0^1f(x)dx\le \int_0^1f(x)^2dx \int_0^1xf(x)dx$","If  is a positive, monotone decreasing function, prove that",f \int_0^1xf(x)^2dx \int_0^1f(x)dx\le \int_0^1f(x)^2dx \int_0^1xf(x)dx,"If $f$ is a positive, monotone decreasing function, prove that $\int_0^1xf(x)^2dx \int_0^1f(x)dx\le \int_0^1f(x)^2dx \int_0^1xf(x)dx$","If $f$ is a positive, monotone decreasing function, prove that $\int_0^1xf(x)^2dx \int_0^1f(x)dx\le \int_0^1f(x)^2dx \int_0^1xf(x)dx$",,"['calculus', 'real-analysis', 'integration', 'inequality']"
81,"Sufficient conditions to conclude that $\lim_{a \to 0^{+}} \int_{0}^{\infty} f(x) e^{-ax} \, dx = \int_{0}^{\infty} f(x) \, dx$",Sufficient conditions to conclude that,"\lim_{a \to 0^{+}} \int_{0}^{\infty} f(x) e^{-ax} \, dx = \int_{0}^{\infty} f(x) \, dx","What are sufficient conditions to conclude that $$ \lim_{a \to 0^{+}} \int_{0}^{\infty} f(x) e^{-ax} \, dx = \int_{0}^{\infty} f(x) \, dx \ ?$$ For example, for $a>0$, $$ \int_{0}^{\infty} J_{0}(x) e^{-ax} \, dx = \frac{1}{\sqrt{1+a^{2}}} \, ,$$ where $J_{0}(x)$ is the Bessel function of the first kind of order zero. But I've seen it stated in a couple places without any justification that $$ \int_{0}^{\infty} J_{0}(x) \, dx = \lim_{a \to 0^{+}} \int_{0}^{\infty} J_{0}(x) e^{-ax} \, dx =  \lim_{a \to 0^{+}} \frac{1}{\sqrt{1+a^{2}}} =  1 .$$ EDIT : In user12014's answer, it is assumed that $ \int_{0}^{\infty} f(x) \, dx$ converges absolutely. But in the example above, $ \int_{0}^{\infty} J_{0}(x) \, dx$ does not converge absolutely. And there are other examples like $$ \int_{0}^{\infty} \frac{\sin x}{x} \, dx  = \lim_{a \to 0^{+}} \int_{0}^{\infty} \frac{\sin x}{x}e^{-ax} \, dx =  \lim_{a \to 0^{+}} \arctan \left(\frac{1}{a} \right) = \frac{\pi}{2} $$ and $$ \int_{0}^{\infty} \text{Ci}(x) \, dx = \lim_{a \to 0^{+}} \int_{0}^{\infty} \text{Ci}(x) e^{-ax} \, dx =  - \lim_{a \to 0^{+}} \frac{\log(1+a^{2})}{2a} =0 \, ,$$ where $\text{Ci}(x)$ is the cosine integral. SECOND EDIT : Combining Daniel Fischer's answer below with his answer to my follow-up question shows that if  $\int_{0}^{\infty} f(x) \, dx$ exists as an improper Riemann integral, then $$\lim_{a \to 0^{+}} \int_{0}^{\infty} f(x) e^{-ax} \, dx = \int_{0}^{\infty} f(x) \, dx.$$","What are sufficient conditions to conclude that $$ \lim_{a \to 0^{+}} \int_{0}^{\infty} f(x) e^{-ax} \, dx = \int_{0}^{\infty} f(x) \, dx \ ?$$ For example, for $a>0$, $$ \int_{0}^{\infty} J_{0}(x) e^{-ax} \, dx = \frac{1}{\sqrt{1+a^{2}}} \, ,$$ where $J_{0}(x)$ is the Bessel function of the first kind of order zero. But I've seen it stated in a couple places without any justification that $$ \int_{0}^{\infty} J_{0}(x) \, dx = \lim_{a \to 0^{+}} \int_{0}^{\infty} J_{0}(x) e^{-ax} \, dx =  \lim_{a \to 0^{+}} \frac{1}{\sqrt{1+a^{2}}} =  1 .$$ EDIT : In user12014's answer, it is assumed that $ \int_{0}^{\infty} f(x) \, dx$ converges absolutely. But in the example above, $ \int_{0}^{\infty} J_{0}(x) \, dx$ does not converge absolutely. And there are other examples like $$ \int_{0}^{\infty} \frac{\sin x}{x} \, dx  = \lim_{a \to 0^{+}} \int_{0}^{\infty} \frac{\sin x}{x}e^{-ax} \, dx =  \lim_{a \to 0^{+}} \arctan \left(\frac{1}{a} \right) = \frac{\pi}{2} $$ and $$ \int_{0}^{\infty} \text{Ci}(x) \, dx = \lim_{a \to 0^{+}} \int_{0}^{\infty} \text{Ci}(x) e^{-ax} \, dx =  - \lim_{a \to 0^{+}} \frac{\log(1+a^{2})}{2a} =0 \, ,$$ where $\text{Ci}(x)$ is the cosine integral. SECOND EDIT : Combining Daniel Fischer's answer below with his answer to my follow-up question shows that if  $\int_{0}^{\infty} f(x) \, dx$ exists as an improper Riemann integral, then $$\lim_{a \to 0^{+}} \int_{0}^{\infty} f(x) e^{-ax} \, dx = \int_{0}^{\infty} f(x) \, dx.$$",,"['real-analysis', 'integration', 'limits', 'laplace-transform']"
82,"If $A$ is compact and $B$ is closed, show $d(A,B)$ is achieved [duplicate]","If  is compact and  is closed, show  is achieved [duplicate]","A B d(A,B)","This question already has answers here : $A$ and $B$ disjoint, $A$ compact, and $B$ closed implies there is positive distance between both sets. (2 answers) Closed 6 years ago . Let $A, B$ be subsets of a metric space $X$. If $A$ is compact and $B$ is closed, show that the distance between $A$ and $B$ is achieved. Attempt at a proof: Let $A$ be compact and $B$ be closed. Let $m=d(A,B)=\inf_{b\in B} d(A,B)$. Then, there are two possibilities: (a) $\exists b\in B$, $d(a,b)=m$. If this is the case, we're done. (b) $\forall b\in B$, $d(a,b)>m$. In this case, there exists a sequence $\{b_n\}\subseteq B:$ $d(a,b_n)\rightarrow m$ as $n\rightarrow\infty$ by definition of infinum. Then there exists a subsequence $\{b_{n_k}\}$: $d(b_{n_1},a)>d(b_{n_2},a)>...$ which is monotonic decreasing. Then note that $d(b_{n_k},a)<d(b_{n_1},a)<\infty$. So it's a bounded sequence. Now I want to show that it has a convergent subsequence that converges to $b\in B$ and then I want to do the same for $A$. And then finally to show that $d(a,b)=m$ in fact. Any clues to how to get there?","This question already has answers here : $A$ and $B$ disjoint, $A$ compact, and $B$ closed implies there is positive distance between both sets. (2 answers) Closed 6 years ago . Let $A, B$ be subsets of a metric space $X$. If $A$ is compact and $B$ is closed, show that the distance between $A$ and $B$ is achieved. Attempt at a proof: Let $A$ be compact and $B$ be closed. Let $m=d(A,B)=\inf_{b\in B} d(A,B)$. Then, there are two possibilities: (a) $\exists b\in B$, $d(a,b)=m$. If this is the case, we're done. (b) $\forall b\in B$, $d(a,b)>m$. In this case, there exists a sequence $\{b_n\}\subseteq B:$ $d(a,b_n)\rightarrow m$ as $n\rightarrow\infty$ by definition of infinum. Then there exists a subsequence $\{b_{n_k}\}$: $d(b_{n_1},a)>d(b_{n_2},a)>...$ which is monotonic decreasing. Then note that $d(b_{n_k},a)<d(b_{n_1},a)<\infty$. So it's a bounded sequence. Now I want to show that it has a convergent subsequence that converges to $b\in B$ and then I want to do the same for $A$. And then finally to show that $d(a,b)=m$ in fact. Any clues to how to get there?",,"['real-analysis', 'metric-spaces']"
83,Can we derive a norm and an inner product from a metric?,Can we derive a norm and an inner product from a metric?,,"Given an inner product on a vector space, I can always define a norm and a metric (and a topology using that metric). Is the converse true? That is, given a metric on a vector space, can I define an inner product with it? And what about a metric space that is not a vector space?","Given an inner product on a vector space, I can always define a norm and a metric (and a topology using that metric). Is the converse true? That is, given a metric on a vector space, can I define an inner product with it? And what about a metric space that is not a vector space?",,"['real-analysis', 'vector-spaces', 'metric-spaces', 'normed-spaces', 'inner-products']"
84,Infinite products $f(x) = \prod_{n=1}^{\infty}(1-x^n)$ and $g(x) = \prod_{n=1}^{\infty}(1+x^n)$,Infinite products  and,f(x) = \prod_{n=1}^{\infty}(1-x^n) g(x) = \prod_{n=1}^{\infty}(1+x^n),"Consider the functions $ f(x) = \prod_{n=1}^{\infty}(1-x^n) $ and $ g(x) = \prod_{n=1}^{\infty}(1+x^n) $ $f(x)$ is defined for $x\in[-1,1]$ and $g(x)$ is defined for $x\in[-1,0]$ . I was wondering if there was a different/better way to express these functions, such as a closed-form expression/power series. Desmos screenshot ( $f(x)$ in red and $g(x)$ in blue) Edit: As pointed out by donaastor the Pentagonal Number Theorem can be used to express $f(x)$ as a power series. Edit 2: The Euler function $\phi(x)$ seems similar to $f(x)$ but with a different domain. For $\phi(x)$ the domain is $x\in(-\infty,-1)\cup(1,\infty)$ .","Consider the functions and is defined for and is defined for . I was wondering if there was a different/better way to express these functions, such as a closed-form expression/power series. Desmos screenshot ( in red and in blue) Edit: As pointed out by donaastor the Pentagonal Number Theorem can be used to express as a power series. Edit 2: The Euler function seems similar to but with a different domain. For the domain is .","
f(x) = \prod_{n=1}^{\infty}(1-x^n)
 
g(x) = \prod_{n=1}^{\infty}(1+x^n)
 f(x) x\in[-1,1] g(x) x\in[-1,0] f(x) g(x) f(x) \phi(x) f(x) \phi(x) x\in(-\infty,-1)\cup(1,\infty)","['real-analysis', 'functions', 'polynomials', 'special-functions', 'infinite-product']"
85,About a chain rule for Wronskians,About a chain rule for Wronskians,,"The Wronskian of $(n-1)$ times differentiable functions $f_1, \ldots, f_n$ is defined as the determinant $$ W(f_1, \ldots, f_n)(x) = \begin{vmatrix} f_1(x) & f_2(x) & \cdots & f_n(x) \\ f_1'(x) & f_2'(x) & \cdots & f_n'(x) \\ \vdots & \vdots & \ddots & \vdots \\ f_1^{(n-1)}(x) & f_2^{(n-1)}(x) & \cdots & f_n^{(n-1)}(x) \end{vmatrix} $$ and used e.g. in the context of linear differential equations. While working on Wronskian of functions $\sin(nx), n=1,2,...,k$. I “discovered” the following chain rule for Wronskians: Let $I, J \subset \Bbb R$ be intervals, $g:I \to J$ and $f_1, \ldots, f_n: J \to \Bbb R$ be $(n-1)$ times differentiable functions. Then $$  W(f_1 \circ g, \ldots, f_n \circ g)(x) = W(f_1, \ldots f_n)(g(x)) \cdot (g'(x))^{n(n-1)/2} \, . $$ It may be surprising (it was to me!) that only the first derivative of $g$ occurs on the right hand side. That is a consequence of Faà di Bruno's formula for the derivatives of a composite function. This is surely a known identity, but I haven't found a reference so far. I searched for “Wronskian” in connection with ”chain rule”, “Faà di Bruno's formula“, or “Bell polynomials” and checked the Wikipedia and Wolfram Mathworld pages about those topics. So what I am asking for is a reference for that formula. Or perhaps it is a direct consequence of some other well-known identity for Wronskians? For the sake of completeness I'll provide a sketch of my proof of the above identity. Faà di Bruno's formula states that $$  \frac{d^k}{dx^k}f_l(g(x)) = \sum_{j=1}^k f_l^{(j)}(g(x)) B_{k, j}(g'(x), g''(x), \ldots, g^{(k-j+1)}(x)) $$ where $B_{k,j}$ are the Bell polynomials . This can be written as a matrix product $$  \Bigl( (f_l \circ g)^{(k)}(x)\Bigr)_{k, l} = B(x) \cdot \Bigl( f_l^{(j)}(g(x))\Bigr)_{j, l} $$ where $B(x)$ is the triangular matrix $$ \begin{pmatrix}  1 & 0 & 0 & \cdots & 0 \\  0 & b_{1, 1}(x) & 0 &\cdots & 0 \\  0 & b_{2, 1}(x) & b_{2, 2}(x)& \cdots & 0 \\  \vdots & \vdots & \vdots & \ddots & \vdots \\  0 & b_{n-1, 1}(x) & b_{n-1, 2}(x) & \cdots & b_{n-1, n-1}(x) \end{pmatrix} $$ with $$  b_{k, j}(x) = B_{k, j}(g'(x), g''(x), \ldots, g^{(k-j+1)}(x)) \, . $$ It follows that $$ W(f_1 \circ g, \ldots, f_n \circ g)(x) = \det(B(x)) \cdot W(f_1, \ldots ,f_n)(g(x)) \, . $$ The diagonal entries of $B(x)$ are $B_{k,k}(g'(x)) = (g'(x))^k $ , so that $$ \det(B(x)) = \prod_{k=1}^{n-1} (g'(x))^k = (g'(x))^{n(n-1)/2} $$ and that gives exactly the desired result. (It looks easy once you have a proof, but it took me a while to figure this out :)","The Wronskian of times differentiable functions is defined as the determinant and used e.g. in the context of linear differential equations. While working on Wronskian of functions $\sin(nx), n=1,2,...,k$. I “discovered” the following chain rule for Wronskians: Let be intervals, and be times differentiable functions. Then It may be surprising (it was to me!) that only the first derivative of occurs on the right hand side. That is a consequence of Faà di Bruno's formula for the derivatives of a composite function. This is surely a known identity, but I haven't found a reference so far. I searched for “Wronskian” in connection with ”chain rule”, “Faà di Bruno's formula“, or “Bell polynomials” and checked the Wikipedia and Wolfram Mathworld pages about those topics. So what I am asking for is a reference for that formula. Or perhaps it is a direct consequence of some other well-known identity for Wronskians? For the sake of completeness I'll provide a sketch of my proof of the above identity. Faà di Bruno's formula states that where are the Bell polynomials . This can be written as a matrix product where is the triangular matrix with It follows that The diagonal entries of are , so that and that gives exactly the desired result. (It looks easy once you have a proof, but it took me a while to figure this out :)","(n-1) f_1, \ldots, f_n 
W(f_1, \ldots, f_n)(x) = \begin{vmatrix}
f_1(x) & f_2(x) & \cdots & f_n(x) \\
f_1'(x) & f_2'(x) & \cdots & f_n'(x) \\
\vdots & \vdots & \ddots & \vdots \\
f_1^{(n-1)}(x) & f_2^{(n-1)}(x) & \cdots & f_n^{(n-1)}(x)
\end{vmatrix}
 I, J \subset \Bbb R g:I \to J f_1, \ldots, f_n: J \to \Bbb R (n-1) 
 W(f_1 \circ g, \ldots, f_n \circ g)(x) = W(f_1, \ldots f_n)(g(x)) \cdot (g'(x))^{n(n-1)/2} \, .
 g 
 \frac{d^k}{dx^k}f_l(g(x)) = \sum_{j=1}^k f_l^{(j)}(g(x)) B_{k, j}(g'(x), g''(x), \ldots, g^{(k-j+1)}(x))
 B_{k,j} 
 \Bigl( (f_l \circ g)^{(k)}(x)\Bigr)_{k, l} = B(x) \cdot \Bigl( f_l^{(j)}(g(x))\Bigr)_{j, l}
 B(x) 
\begin{pmatrix}
 1 & 0 & 0 & \cdots & 0 \\
 0 & b_{1, 1}(x) & 0 &\cdots & 0 \\
 0 & b_{2, 1}(x) & b_{2, 2}(x)& \cdots & 0 \\
 \vdots & \vdots & \vdots & \ddots & \vdots \\
 0 & b_{n-1, 1}(x) & b_{n-1, 2}(x) & \cdots & b_{n-1, n-1}(x)
\end{pmatrix}
 
 b_{k, j}(x) = B_{k, j}(g'(x), g''(x), \ldots, g^{(k-j+1)}(x)) \, .
 
W(f_1 \circ g, \ldots, f_n \circ g)(x) = \det(B(x)) \cdot W(f_1, \ldots ,f_n)(g(x)) \, .
 B(x) B_{k,k}(g'(x)) = (g'(x))^k  
\det(B(x)) = \prod_{k=1}^{n-1} (g'(x))^k = (g'(x))^{n(n-1)/2}
","['real-analysis', 'reference-request', 'chain-rule', 'wronskian']"
86,Proof of a few equations involving $\int_{\alpha}^{\infty}\frac{1}{t\left(e^{t}\pm1\right)}dt$,Proof of a few equations involving,\int_{\alpha}^{\infty}\frac{1}{t\left(e^{t}\pm1\right)}dt,"I derived these formulas with the Laurent series and Euler-Maclaurin summation formula. I can demonstrate this later if anyone's curious.  I'm wondering if there is another way.  I'm also interested in finding generalized formulas. $$\lim\limits_{\alpha\to 0}\left[\frac{\ln\left(\alpha\right)}{2}+\int_{\alpha}^{\infty}\frac{1}{t\left(e^{t}+1\right)}dt\right]=\frac{1}{2}\left(\ln\left(\pi\right)-\ln\left(2\right)-\gamma\right)$$ $$\lim\limits_{\alpha\to 0}\left[\frac1\alpha+\frac{\ln\left(\alpha\right)}{2}-\int_{\alpha}^{\infty}\frac{1}{t\left(e^{t}-1\right)}dt\right]=\frac{1}{2}\left(\ln\left(\pi\right)+\ln\left(2\right)-\gamma\right)$$ A manipulation of these equations yields $$\lim\limits_{s\to -1}\left[\frac{1}{\ln|s|}+\left(-\frac{1}{s+1}+\frac{1}{2}\right)\ln|\ln|s||+\int_{s}^{\infty}\frac{\ln\left(\ln\left(u\right)\right)}{\left(u+1\right)^{2}}du\right]=\frac{1}{2}\left(\ln\left(\pi\right)-3\ln\left(2\right)-\gamma\right)$$ $$\lim\limits_{s\to 1}\left[\frac{1}{\ln\left(s\right)}+\left(\frac{1}{s-1}+\frac{1}{2}\right)\ln\left(\ln\left(s\right)\right)-\int_{s}^{\infty}\frac{\ln\left(\ln\left(u\right)\right)}{\left(u-1\right)^{2}}du\right]=\frac{1}{2}\left(\ln\left(\pi\right)+\ln\left(2\right)-\gamma\right)$$ Here is my elementary method of deriving these: Begin with the Euler-Maclaurin summation formula: $$ \begin{align} \frac{1}{h}\int_a^b f(t)dt &=\sum_{k=0}^n f(kh+a)-\left(\frac{f(a)+f(b)}{2}\right) \\&-\sum_{k=1}^n \frac{h^{2k-1}B_{2k}}{(2k)!} \left(f^{(2k-1)}(b)-f^{(2k-1)}(a)\right) \\&-R \end{align} $$ where $h=\frac{b-a}{n}$ and $R$ is the remainder term. Letting $n=\frac{b-a}{x}$ and rearranging we get $$ \begin{align} \sum_{k=1}^{(b-a)/x} \frac{x^{2k-1}B_{2k}}{(2k)!} \left(f^{(2k-1)}(b)-f^{(2k-1)}(a)\right) &=\sum_{k=0}^{(b-a)/x} f(kx+a)-\left(\frac{f(a)+f(b)}{2}\right) \\&-\frac{1}{x}\int_a^b f(t)dt \\&-R \end{align} $$ Limiting $b\to 0$ and $a\to -\infty$ , we have $$ \begin{align} \lim\limits_{\substack{%      a \to -\infty\\      b \to 0}} \sum_{k=1}^{-a/x} \frac{x^{2k-1}B_{2k}}{(2k)!} \left(f^{(2k-1)}(b)-f^{(2k-1)}(a)\right) &=\lim\limits_{\substack{%      a \to -\infty\\      b \to 0}}\left(\sum_{k=0}^{-a/x} f(kx+a)-\left(\frac{f(a)+f(b)}{2}\right)-\frac{1}{x}\int_a^b f(t)dt\right) \end{align} $$ The remainder disappears as $n\to\infty$ . Now make a variable substitution in the limit $a\to -ax$ . $$ \begin{align} \lim\limits_{\substack{%      a \to \infty\\      b \to 0}}\sum_{k=1}^{a} \frac{x^{2k-1}B_{2k}}{(2k)!} \left(f^{(2k-1)}(b)-f^{(2k-1)}(-ax)\right) &=\lim\limits_{\substack{%      a \to \infty\\      b \to 0}}\left(\sum_{k=0}^{a} f((k-a)x)-\left(\frac{f(-ax)+f(b)}{2}\right)-\frac{1}{x}\int_{-ax}^b f(t)dt\right) \\&=\lim\limits_{\substack{%      a \to \infty\\      b \to 0}}\left(\sum_{k=0}^{a} f(-kx)-\left(\frac{f(-ax)+f(b)}{2}\right)-\frac{1}{x}\int_{-ax}^b f(t)dt\right) \end{align} $$ Now use the following hint. $$\frac{1}{z(e^z-1)}=\frac{1}{z^2}-\frac{1}{2z}+\sum_{k=1}^\infty\frac{B_{2k}}{(2k)!}z^{2k-2}$$ which when we integrate we get $$\begin{align} \int_x^\infty\frac{1}{z(e^z-1)}dz &=K-\left(-\frac{1}{x}-\frac{\ln{|x|}}{2}+\sum_{k=1}^\infty\frac{B_{2k}}{(2k)!(2k-1)}x^{2k-1}\right) \end{align}$$ Where $K$ stands for the integral evaluated at $\infty$ . Let $f(x)=\text{Ei}\left(x\right)-\ln\left|x\right|-\gamma$ . Note that $\lim\limits_{t\to 0}f^{(m)}(t)=\frac1m$ and $\lim\limits_{t\to -\infty}f^{(m)}(t)=0$ for $m\ge1$ . Further note that $\lim\limits_{t\to 0} f(t) = 0$ so we may substitute the sum: $\sum_{k=0}^{a} f(-kx)=\sum_{k=1}^{a} f(-kx)$ . Now we have $$ \begin{align} \sum_{k=1}^\infty\frac{B_{2k}}{(2k)!(2k-1)}x^{2k-1} &=\lim\limits_{\substack{%      a \to \infty\\      b \to 0}}\left(\sum_{k=1}^{a} f(-kx)-\left(\frac{f(-ax)+f(b)}{2}\right)-\frac{1}{x}\int_{-ax}^b f(t)dt\right) \\&=\lim\limits_{\substack{%      a \to \infty\\      b \to 0}}\left(\sum_{k=1}^{a}\text{Ei}(-kx)-\sum_{k=1}^{a}\ln\left|-kx\right|-\sum_{k=1}^{a}\gamma-\left(\frac{f(-ax)+f(b)}{2}\right)-\frac{1}{x}\int_{-ax}^b f(t)dt\right) \\&=-\int_x^\infty\frac{1}{z(e^z-1)}dz+\lim\limits_{\substack{%      a \to \infty\\      b \to 0}}\left(-a\ln|x|-\ln|a!|-a\gamma-\left(\frac{f(-ax)+f(b)}{2}\right)-\frac{1}{x}\int_{-ax}^b f(t)dt\right) \\&=-\int_x^\infty\frac{1}{z(e^z-1)}dz+\frac{1}{x}+\frac{\ln\left|x\right|}{2}+\frac{1}{2}\left(\gamma-\ln\left(2\pi\right)\right) \end{align} $$ (The limit is tricky which is why I left out some steps). Therefore $$\begin{align} \int_x^\infty\frac{1}{z(e^z-1)}dz=\frac12 (\gamma-\ln(2\pi))+\frac{1}{x}+\frac{\ln{|x|}}{2}-\sum_{k=1}^\infty\frac{B_{2k}}{(2k)!(2k-1)}x^{2k-1} \end{align}$$ We can derive the other equation with $f(x)=-\text{Ei}(x)+2\text{Ei}(2x)-\ln|4x|-\gamma$ .","I derived these formulas with the Laurent series and Euler-Maclaurin summation formula. I can demonstrate this later if anyone's curious.  I'm wondering if there is another way.  I'm also interested in finding generalized formulas. A manipulation of these equations yields Here is my elementary method of deriving these: Begin with the Euler-Maclaurin summation formula: where and is the remainder term. Letting and rearranging we get Limiting and , we have The remainder disappears as . Now make a variable substitution in the limit . Now use the following hint. which when we integrate we get Where stands for the integral evaluated at . Let . Note that and for . Further note that so we may substitute the sum: . Now we have (The limit is tricky which is why I left out some steps). Therefore We can derive the other equation with .","\lim\limits_{\alpha\to 0}\left[\frac{\ln\left(\alpha\right)}{2}+\int_{\alpha}^{\infty}\frac{1}{t\left(e^{t}+1\right)}dt\right]=\frac{1}{2}\left(\ln\left(\pi\right)-\ln\left(2\right)-\gamma\right) \lim\limits_{\alpha\to 0}\left[\frac1\alpha+\frac{\ln\left(\alpha\right)}{2}-\int_{\alpha}^{\infty}\frac{1}{t\left(e^{t}-1\right)}dt\right]=\frac{1}{2}\left(\ln\left(\pi\right)+\ln\left(2\right)-\gamma\right) \lim\limits_{s\to -1}\left[\frac{1}{\ln|s|}+\left(-\frac{1}{s+1}+\frac{1}{2}\right)\ln|\ln|s||+\int_{s}^{\infty}\frac{\ln\left(\ln\left(u\right)\right)}{\left(u+1\right)^{2}}du\right]=\frac{1}{2}\left(\ln\left(\pi\right)-3\ln\left(2\right)-\gamma\right) \lim\limits_{s\to 1}\left[\frac{1}{\ln\left(s\right)}+\left(\frac{1}{s-1}+\frac{1}{2}\right)\ln\left(\ln\left(s\right)\right)-\int_{s}^{\infty}\frac{\ln\left(\ln\left(u\right)\right)}{\left(u-1\right)^{2}}du\right]=\frac{1}{2}\left(\ln\left(\pi\right)+\ln\left(2\right)-\gamma\right) 
\begin{align}
\frac{1}{h}\int_a^b f(t)dt
&=\sum_{k=0}^n f(kh+a)-\left(\frac{f(a)+f(b)}{2}\right)
\\&-\sum_{k=1}^n \frac{h^{2k-1}B_{2k}}{(2k)!} \left(f^{(2k-1)}(b)-f^{(2k-1)}(a)\right)
\\&-R
\end{align}
 h=\frac{b-a}{n} R n=\frac{b-a}{x} 
\begin{align}
\sum_{k=1}^{(b-a)/x} \frac{x^{2k-1}B_{2k}}{(2k)!} \left(f^{(2k-1)}(b)-f^{(2k-1)}(a)\right)
&=\sum_{k=0}^{(b-a)/x} f(kx+a)-\left(\frac{f(a)+f(b)}{2}\right)
\\&-\frac{1}{x}\int_a^b f(t)dt
\\&-R
\end{align}
 b\to 0 a\to -\infty 
\begin{align}
\lim\limits_{\substack{%
     a \to -\infty\\
     b \to 0}}
\sum_{k=1}^{-a/x} \frac{x^{2k-1}B_{2k}}{(2k)!} \left(f^{(2k-1)}(b)-f^{(2k-1)}(a)\right)
&=\lim\limits_{\substack{%
     a \to -\infty\\
     b \to 0}}\left(\sum_{k=0}^{-a/x} f(kx+a)-\left(\frac{f(a)+f(b)}{2}\right)-\frac{1}{x}\int_a^b f(t)dt\right)
\end{align}
 n\to\infty a\to -ax 
\begin{align}
\lim\limits_{\substack{%
     a \to \infty\\
     b \to 0}}\sum_{k=1}^{a} \frac{x^{2k-1}B_{2k}}{(2k)!} \left(f^{(2k-1)}(b)-f^{(2k-1)}(-ax)\right)
&=\lim\limits_{\substack{%
     a \to \infty\\
     b \to 0}}\left(\sum_{k=0}^{a} f((k-a)x)-\left(\frac{f(-ax)+f(b)}{2}\right)-\frac{1}{x}\int_{-ax}^b f(t)dt\right)
\\&=\lim\limits_{\substack{%
     a \to \infty\\
     b \to 0}}\left(\sum_{k=0}^{a} f(-kx)-\left(\frac{f(-ax)+f(b)}{2}\right)-\frac{1}{x}\int_{-ax}^b f(t)dt\right)
\end{align}
 \frac{1}{z(e^z-1)}=\frac{1}{z^2}-\frac{1}{2z}+\sum_{k=1}^\infty\frac{B_{2k}}{(2k)!}z^{2k-2} \begin{align}
\int_x^\infty\frac{1}{z(e^z-1)}dz
&=K-\left(-\frac{1}{x}-\frac{\ln{|x|}}{2}+\sum_{k=1}^\infty\frac{B_{2k}}{(2k)!(2k-1)}x^{2k-1}\right)
\end{align} K \infty f(x)=\text{Ei}\left(x\right)-\ln\left|x\right|-\gamma \lim\limits_{t\to 0}f^{(m)}(t)=\frac1m \lim\limits_{t\to -\infty}f^{(m)}(t)=0 m\ge1 \lim\limits_{t\to 0} f(t) = 0 \sum_{k=0}^{a} f(-kx)=\sum_{k=1}^{a} f(-kx) 
\begin{align}
\sum_{k=1}^\infty\frac{B_{2k}}{(2k)!(2k-1)}x^{2k-1}
&=\lim\limits_{\substack{%
     a \to \infty\\
     b \to 0}}\left(\sum_{k=1}^{a} f(-kx)-\left(\frac{f(-ax)+f(b)}{2}\right)-\frac{1}{x}\int_{-ax}^b f(t)dt\right)
\\&=\lim\limits_{\substack{%
     a \to \infty\\
     b \to 0}}\left(\sum_{k=1}^{a}\text{Ei}(-kx)-\sum_{k=1}^{a}\ln\left|-kx\right|-\sum_{k=1}^{a}\gamma-\left(\frac{f(-ax)+f(b)}{2}\right)-\frac{1}{x}\int_{-ax}^b f(t)dt\right)
\\&=-\int_x^\infty\frac{1}{z(e^z-1)}dz+\lim\limits_{\substack{%
     a \to \infty\\
     b \to 0}}\left(-a\ln|x|-\ln|a!|-a\gamma-\left(\frac{f(-ax)+f(b)}{2}\right)-\frac{1}{x}\int_{-ax}^b f(t)dt\right)
\\&=-\int_x^\infty\frac{1}{z(e^z-1)}dz+\frac{1}{x}+\frac{\ln\left|x\right|}{2}+\frac{1}{2}\left(\gamma-\ln\left(2\pi\right)\right)
\end{align}
 \begin{align}
\int_x^\infty\frac{1}{z(e^z-1)}dz=\frac12 (\gamma-\ln(2\pi))+\frac{1}{x}+\frac{\ln{|x|}}{2}-\sum_{k=1}^\infty\frac{B_{2k}}{(2k)!(2k-1)}x^{2k-1}
\end{align} f(x)=-\text{Ei}(x)+2\text{Ei}(2x)-\ln|4x|-\gamma","['real-analysis', 'integration', 'closed-form']"
87,Two powerful alternating sums $\sum_{n=1}^\infty\frac{(-1)^nH_nH_n^{(2)}}{n^2}$ and $\sum_{n=1}^\infty\frac{(-1)^nH_n^3}{n^2}$,Two powerful alternating sums  and,\sum_{n=1}^\infty\frac{(-1)^nH_nH_n^{(2)}}{n^2} \sum_{n=1}^\infty\frac{(-1)^nH_n^3}{n^2},"where $H_n$ is the harmonic number and can be defined as: $H_n=1+\frac12+\frac13+...+\frac1n$ $H_n^{(2)}=1+\frac1{2^2}+\frac1{3^2}+...+\frac1{n^2}$ these two sums are already solved by Cornel using summation manipulation and can be also found in his newly released book "" (almost) impossible integrals, sums and series "". I was able to evaluate them using integration and some harmonic identities. \begin{align} \sum_{n=1}^\infty\frac{(-1)^nH_nH_n^{(2)}}{n^2}&=4\operatorname{Li}_5\left(\frac12\right)+4\ln2\operatorname{Li}_4\left(\frac12\right)-\frac23\ln^32\zeta(2)+\frac74\ln^22\zeta(3)\\&\quad-\frac{15}{16}\zeta(2)\zeta(3)-\frac{23}8\zeta(5)+\frac2{15}\ln^52 \end{align} \begin{align} \sum_{n=1}^\infty\frac{(-1)^nH_n^3}{n^2}&=-6\operatorname{Li}_5\left(\frac12\right)-6\ln2\operatorname{Li}_4\left(\frac12\right)+\ln^32\zeta(2)-\frac{21}{8}\ln^22\zeta(3)\\&\quad+\frac{27}{16}\zeta(2)\zeta(3)+\frac94\zeta(5)-\frac15\ln^52 \end{align} The point of posting these two sums is to use them as a reference in our solutions if needed.","where is the harmonic number and can be defined as: these two sums are already solved by Cornel using summation manipulation and can be also found in his newly released book "" (almost) impossible integrals, sums and series "". I was able to evaluate them using integration and some harmonic identities. The point of posting these two sums is to use them as a reference in our solutions if needed.","H_n H_n=1+\frac12+\frac13+...+\frac1n H_n^{(2)}=1+\frac1{2^2}+\frac1{3^2}+...+\frac1{n^2} \begin{align}
\sum_{n=1}^\infty\frac{(-1)^nH_nH_n^{(2)}}{n^2}&=4\operatorname{Li}_5\left(\frac12\right)+4\ln2\operatorname{Li}_4\left(\frac12\right)-\frac23\ln^32\zeta(2)+\frac74\ln^22\zeta(3)\\&\quad-\frac{15}{16}\zeta(2)\zeta(3)-\frac{23}8\zeta(5)+\frac2{15}\ln^52
\end{align} \begin{align}
\sum_{n=1}^\infty\frac{(-1)^nH_n^3}{n^2}&=-6\operatorname{Li}_5\left(\frac12\right)-6\ln2\operatorname{Li}_4\left(\frac12\right)+\ln^32\zeta(2)-\frac{21}{8}\ln^22\zeta(3)\\&\quad+\frac{27}{16}\zeta(2)\zeta(3)+\frac94\zeta(5)-\frac15\ln^52
\end{align}","['real-analysis', 'integration', 'sequences-and-series', 'harmonic-numbers', 'polylogarithm']"
88,"Given $f$ is continuous and $f(x)=f(e^{t}x)$ for all $x\in\mathbb{R}$ and $t\ge0$, show that $f$ is constant function","Given  is continuous and  for all  and , show that  is constant function",f f(x)=f(e^{t}x) x\in\mathbb{R} t\ge0 f,"This question was asked in ISI BStat / BMath 2018 entrance exam : Let $f: \mathbb{R} \to \mathbb{R}$ be a continuous function such that   for all $x\in\mathbb{R}$ and $t\ge 0$ , $$f(x)=f(e^{t}x)$$ Show that $f$ is a constant function. My attempt: Suppose that $f$ is not a constant function. Then $f(0)\ne f(x_0)$ for some $x_0 \in \mathbb{R}$ . We eliminate the possibilities that $x_0>0$ and $x_0<0$ , thus proving that our assumption was wrong. Case 1: ( $x_0>0$ ). Let $k$ be any real number between $f(0)$ and $f(x_0)$ (not inclusive). Then by the intermediate value theorem,  there exists $y_0 \in (0, x_0)$ such that $f(y_0)=k$ . But $f(y_0)=f\left( e^{\ln \left( \frac{x_0}{y_0}\right)  } y_0\right) = f(x_0)$ which contradicts our assumption that $f(y_0)$ was between $f(0)$ and $f(x_0)$ . Case 2: ( $x_0<0$ ). Let $k$ be any real number between $f(0)$ and $f(x_0)$ (not inclusive). Then by the intermediate value theorem,  there exists $y_0 \in (x_0, 0)$ such that $f(y_0)=k$ . But $f(x_0)=f\left( e^{\ln \left( \frac{y_0}{x_0}\right)  } x_0\right) = f(y_0)$ , a contradiction again. Is this proof correct? I was probably looking for a direct proof if there's any. Alternative proofs are welcome.","This question was asked in ISI BStat / BMath 2018 entrance exam : Let be a continuous function such that   for all and , Show that is a constant function. My attempt: Suppose that is not a constant function. Then for some . We eliminate the possibilities that and , thus proving that our assumption was wrong. Case 1: ( ). Let be any real number between and (not inclusive). Then by the intermediate value theorem,  there exists such that . But which contradicts our assumption that was between and . Case 2: ( ). Let be any real number between and (not inclusive). Then by the intermediate value theorem,  there exists such that . But , a contradiction again. Is this proof correct? I was probably looking for a direct proof if there's any. Alternative proofs are welcome.","f: \mathbb{R} \to \mathbb{R} x\in\mathbb{R} t\ge 0 f(x)=f(e^{t}x) f f f(0)\ne f(x_0) x_0 \in \mathbb{R} x_0>0 x_0<0 x_0>0 k f(0) f(x_0) y_0 \in (0, x_0) f(y_0)=k f(y_0)=f\left( e^{\ln \left( \frac{x_0}{y_0}\right)  } y_0\right) = f(x_0) f(y_0) f(0) f(x_0) x_0<0 k f(0) f(x_0) y_0 \in (x_0, 0) f(y_0)=k f(x_0)=f\left( e^{\ln \left( \frac{y_0}{x_0}\right)  } x_0\right) = f(y_0)","['real-analysis', 'proof-verification', 'continuity', 'alternative-proof']"
89,Prove that $\int_0^x \frac{\sin t}{t} dt > \arctan x $ for $x>0$,Prove that  for,\int_0^x \frac{\sin t}{t} dt > \arctan x  x>0,I'm finding some bounds for the Si function defined as $$ \operatorname{Si}(x) := \int_0^x\frac{\sin t}{t}dt. $$ I observed from WolframAlpha that the inequality $$ \operatorname{Si}(x)>\arctan(x) $$ holds for $x>0$. I tried to show this analytically but failed and could not find any references regarding this. Could someone help me with this?,I'm finding some bounds for the Si function defined as $$ \operatorname{Si}(x) := \int_0^x\frac{\sin t}{t}dt. $$ I observed from WolframAlpha that the inequality $$ \operatorname{Si}(x)>\arctan(x) $$ holds for $x>0$. I tried to show this analytically but failed and could not find any references regarding this. Could someone help me with this?,,"['real-analysis', 'calculus', 'inequality']"
90,"What is the limit of the average value of the first $n$ terms of $(1, 2, 1, 1, 1, 2, 1, 1, 2, 1, ...)$ as $n\to\infty$?",What is the limit of the average value of the first  terms of  as ?,"n (1, 2, 1, 1, 1, 2, 1, 1, 2, 1, ...) n\to\infty","There exists a sequence $a_n$ that begins $(1, 2, 1, 1, 1, 2, 1, 1, 2, 1, ...)$. It is fully defined on the OEIS at A293630 , but I will give a simple explanation here. The sequence starts $1, 2$. The next part is generated by looking at the last term in the current sequence (currently $2$), and adding the rest of the sequence to the end that many times (resulting in $1, 2, 1, 1$). This continues $(1, 2, 1, 1, 1, 2, 1), (1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2)...$ $a_n$ represents the $n$th term of the final, infinite sequence. It appears that $$\lim_{n\to\infty} \frac{\sum_{k=1}^{n} a_k}{n} = 1.2752618420911721359284772047801515149347600371...$$ While empirical evidence holds this to be true up to an absurd amount of terms, I have not been successful in even proving that the limit exists. I have found no relation of this number to other constants. Is there a way I could either prove the existence of the limit, prove this is the value of the limit, or find the significance of the number? Edit: Now that a proof has been provided by Sangchul Lee , I am looking for what this value may represent and what causes this value in particular to appear. An acquaintance with the same interest has put a bounty up for answers that may present an explanation for this.","There exists a sequence $a_n$ that begins $(1, 2, 1, 1, 1, 2, 1, 1, 2, 1, ...)$. It is fully defined on the OEIS at A293630 , but I will give a simple explanation here. The sequence starts $1, 2$. The next part is generated by looking at the last term in the current sequence (currently $2$), and adding the rest of the sequence to the end that many times (resulting in $1, 2, 1, 1$). This continues $(1, 2, 1, 1, 1, 2, 1), (1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 2)...$ $a_n$ represents the $n$th term of the final, infinite sequence. It appears that $$\lim_{n\to\infty} \frac{\sum_{k=1}^{n} a_k}{n} = 1.2752618420911721359284772047801515149347600371...$$ While empirical evidence holds this to be true up to an absurd amount of terms, I have not been successful in even proving that the limit exists. I have found no relation of this number to other constants. Is there a way I could either prove the existence of the limit, prove this is the value of the limit, or find the significance of the number? Edit: Now that a proof has been provided by Sangchul Lee , I am looking for what this value may represent and what causes this value in particular to appear. An acquaintance with the same interest has put a bounty up for answers that may present an explanation for this.",,"['real-analysis', 'sequences-and-series', 'limits']"
91,(Fundamental) Solution of the Helmholtz equation,(Fundamental) Solution of the Helmholtz equation,,The fundamental solution of the Helmholtz equation in $\mathbb{R}^3$ $$(\Delta+k^2)u=-\delta \tag{1}$$ is well known:  $$u(x)=\frac{e^{\pm ik|x|}}{4\pi |x|}$$ solves the Helmholtz equation in distributional sense. The usual ansatz to obtain fundamental solutions is to Fourier transform both sides. Then $(1)$ becomes $$(-|x|^2+k^2)\hat{u}(x)=-1 \implies \hat{u}(x)=\frac{1}{k^2-|x|^2}.$$ The problem now arises is that we want to inverse Fourier transform $\hat{u}$ to obtain the solution. But $\hat{u}$ has singularities on the sphere of radius $k$. How do we proceed in a rigorous distributional way to obtain the above fundamental solution? What do people usually do and is there any book about such problems?,The fundamental solution of the Helmholtz equation in $\mathbb{R}^3$ $$(\Delta+k^2)u=-\delta \tag{1}$$ is well known:  $$u(x)=\frac{e^{\pm ik|x|}}{4\pi |x|}$$ solves the Helmholtz equation in distributional sense. The usual ansatz to obtain fundamental solutions is to Fourier transform both sides. Then $(1)$ becomes $$(-|x|^2+k^2)\hat{u}(x)=-1 \implies \hat{u}(x)=\frac{1}{k^2-|x|^2}.$$ The problem now arises is that we want to inverse Fourier transform $\hat{u}$ to obtain the solution. But $\hat{u}$ has singularities on the sphere of radius $k$. How do we proceed in a rigorous distributional way to obtain the above fundamental solution? What do people usually do and is there any book about such problems?,,"['real-analysis', 'functional-analysis', 'analysis', 'partial-differential-equations', 'distribution-theory']"
92,To what function does this series converge?,To what function does this series converge?,,"I have the following series expansion $$f(x) = \sum_{n=1}^\infty a_n x^{b_n},$$ where $a=\{a_n\}_{n=1}^\infty$ is such that $\sum_{n=1}^\infty |a_n| < \infty$ and $b=\{b_n\}_{n=1}^\infty$ such that $b_n\in (0,1/2)$ for all $n\geq 1$ and $\lim_n b_n = 0$. Can I find a choice of $a$ and $b$ such that I have a closed form for $f$? For example: $$f(x) = \sum_{n=1}^\infty \frac{(-1)^n}{n!} x^{1/n}, \mbox{ or } f(x) = \sum_{n=1}^\infty \frac{1}{n!} x^{1/n}$$ remind of the exponential but not quite. Any ideas? Thank you for your support!","I have the following series expansion $$f(x) = \sum_{n=1}^\infty a_n x^{b_n},$$ where $a=\{a_n\}_{n=1}^\infty$ is such that $\sum_{n=1}^\infty |a_n| < \infty$ and $b=\{b_n\}_{n=1}^\infty$ such that $b_n\in (0,1/2)$ for all $n\geq 1$ and $\lim_n b_n = 0$. Can I find a choice of $a$ and $b$ such that I have a closed form for $f$? For example: $$f(x) = \sum_{n=1}^\infty \frac{(-1)^n}{n!} x^{1/n}, \mbox{ or } f(x) = \sum_{n=1}^\infty \frac{1}{n!} x^{1/n}$$ remind of the exponential but not quite. Any ideas? Thank you for your support!",,"['calculus', 'real-analysis', 'sequences-and-series', 'functions', 'convergence-divergence']"
93,Integral $ \int_{0}^{\infty} \ln x\left[\ln \left( \frac{x+1}{2} \right) - \frac{1}{x+1} - \psi \left( \frac{x+1}{2} \right) \right] \mathrm{d}x $,Integral, \int_{0}^{\infty} \ln x\left[\ln \left( \frac{x+1}{2} \right) - \frac{1}{x+1} - \psi \left( \frac{x+1}{2} \right) \right] \mathrm{d}x ,"Prove That : $$ \int_{0}^{\infty} \ln x\left[\ln \left( \dfrac{x+1}{2} \right) - \dfrac{1}{x+1} - \psi \left( \dfrac{x+1}{2} \right) \right] \mathrm{d}x = \dfrac{\ln^2 2}{2}+\ln2\cdot\ln\pi-1 $$ where $\psi(z)$ denotes the Digamma Function . This integral arose from my attempt to find an alternate solution to Problem 5 , i.e, $$ {\large\int}_0^\infty\frac{\ln\left(x^2+1\right)\,\arctan x}{e^{\pi x}-1}dx=\frac{\ln^22}2+\ln2\cdot\ln\pi-1 $$ Here's my try : We have the identity, $$ \displaystyle \int_0^\infty \frac{\ln y}{(y+a)^2 + b^2}\,\mathrm{d}y \; = \; \tfrac{1}{2b}\,\tan^{-1}\tfrac{b}{a}\,\ln(a^2+b^2) $$ Since substituting $y \mapsto \dfrac{a^2+b^2}{y}$ gives, $$\displaystyle \int_0^\infty \frac{\ln y}{(y+a)^2+b^2} \, \mathrm{d}y =\ln(a^2+b^2)\int_0^\infty \frac{dy}{y^2+2ay+a^2+b^2} - \int_0^\infty \frac{\ln y}{(y+a)^2+b^2} \, \mathrm{d}y $$ $$ \implies \displaystyle \int_0^\infty \frac{\ln y}{(y+a)^2 + b^2}\,\mathrm{d}y \; = \; \tfrac{1}{2b}\,\tan^{-1}\tfrac{b}{a}\,\ln(a^2+b^2)  $$ Putting $a=1$ and $b=x$, we have, $ \displaystyle \int_0^\infty \frac{2x \ln y}{(y+1)^2 + x^2}\,\mathrm{d}y \; = \; \,\,\ln(1+x^2) \ \tan^{-1}x \tag{1} $ Now, we have to prove, $$\displaystyle {\large\int}_0^\infty\frac{\ln\left(x^2+1\right)\,\tan^{-1}x}{e^{\pi x}-1} \mathrm{d}x=\frac{\ln^22}2+\ln2\cdot\ln\pi-1$$ Let, $$\displaystyle \text{I} = {\large\int}_0^\infty\frac{\ln\left(x^2+1\right)\,\tan^{-1}x}{e^{\pi x}-1} \mathrm{d}x $$ $$\displaystyle = \int_{0}^{\infty} \int_{0}^{\infty} \frac{2x \ln y}{[(y+1)^2 + x^2][e^{\pi x} - 1]} \mathrm{d}x \ \mathrm{d}y \quad (\text{From 1}) \tag{2}$$ The inner integral is of the form, $$ \displaystyle \text{J} = \int_{0}^{\infty} \dfrac{x}{(x^2+a^2)(e^{\pi x} - 1)}  \ \mathrm{d}x \ ; \ a = (y+1)$$ I have proved here that, $\displaystyle \int_{0}^{\infty} \dfrac{\log(1-e^{-2a\pi x})}{1+x^2} \mathrm{d}x = \pi \left[\dfrac{1}{2} \log (2a\pi ) + a(\log a - 1) - \log(\Gamma(a+1)) \right] \tag{3}$ Differentiating both sides w.r.t. $a$, substituting $ a \mapsto \frac{a}{2} $ and $ x \mapsto \frac{x}{a} $, we get, $\displaystyle \int_{0}^{\infty} \dfrac{x}{(x^2+a^2)(e^{\pi x} - 1)}  \ \mathrm{d}x = \dfrac{1}{2} \left[ \dfrac{1}{a} + \ln \left( \dfrac{a}{2} \right) - \psi \left( \dfrac{a}{2} + 1 \right) \right] \tag{4}$ Putting $(4)$ in $(2)$, we have, $$ \displaystyle \text{I} = \int_{0}^{\infty} \left[ \dfrac{\ln y}{y+1} + \ln y \ln \left( \dfrac{y+1}{2} \right) - \ln y \ \psi \left( \dfrac{y+1}{2} + 1 \right) \right]  \mathrm{d}y $$ $ = \displaystyle \int_{0}^{\infty} \ln x\left[\ln \left( \dfrac{x+1}{2} \right) - \dfrac{1}{x+1} - \psi \left( \dfrac{x+1}{2} \right) \right] \mathrm{d}x \tag{*}$ Since the original question has already been proved in the link, so $(*)$ must be equal to the stated closed form. It also matches numerically. I'm looking for  some method to evaluate $(*)$ independent of Problem 5. Any help will be greatly appreciated.","Prove That : $$ \int_{0}^{\infty} \ln x\left[\ln \left( \dfrac{x+1}{2} \right) - \dfrac{1}{x+1} - \psi \left( \dfrac{x+1}{2} \right) \right] \mathrm{d}x = \dfrac{\ln^2 2}{2}+\ln2\cdot\ln\pi-1 $$ where $\psi(z)$ denotes the Digamma Function . This integral arose from my attempt to find an alternate solution to Problem 5 , i.e, $$ {\large\int}_0^\infty\frac{\ln\left(x^2+1\right)\,\arctan x}{e^{\pi x}-1}dx=\frac{\ln^22}2+\ln2\cdot\ln\pi-1 $$ Here's my try : We have the identity, $$ \displaystyle \int_0^\infty \frac{\ln y}{(y+a)^2 + b^2}\,\mathrm{d}y \; = \; \tfrac{1}{2b}\,\tan^{-1}\tfrac{b}{a}\,\ln(a^2+b^2) $$ Since substituting $y \mapsto \dfrac{a^2+b^2}{y}$ gives, $$\displaystyle \int_0^\infty \frac{\ln y}{(y+a)^2+b^2} \, \mathrm{d}y =\ln(a^2+b^2)\int_0^\infty \frac{dy}{y^2+2ay+a^2+b^2} - \int_0^\infty \frac{\ln y}{(y+a)^2+b^2} \, \mathrm{d}y $$ $$ \implies \displaystyle \int_0^\infty \frac{\ln y}{(y+a)^2 + b^2}\,\mathrm{d}y \; = \; \tfrac{1}{2b}\,\tan^{-1}\tfrac{b}{a}\,\ln(a^2+b^2)  $$ Putting $a=1$ and $b=x$, we have, $ \displaystyle \int_0^\infty \frac{2x \ln y}{(y+1)^2 + x^2}\,\mathrm{d}y \; = \; \,\,\ln(1+x^2) \ \tan^{-1}x \tag{1} $ Now, we have to prove, $$\displaystyle {\large\int}_0^\infty\frac{\ln\left(x^2+1\right)\,\tan^{-1}x}{e^{\pi x}-1} \mathrm{d}x=\frac{\ln^22}2+\ln2\cdot\ln\pi-1$$ Let, $$\displaystyle \text{I} = {\large\int}_0^\infty\frac{\ln\left(x^2+1\right)\,\tan^{-1}x}{e^{\pi x}-1} \mathrm{d}x $$ $$\displaystyle = \int_{0}^{\infty} \int_{0}^{\infty} \frac{2x \ln y}{[(y+1)^2 + x^2][e^{\pi x} - 1]} \mathrm{d}x \ \mathrm{d}y \quad (\text{From 1}) \tag{2}$$ The inner integral is of the form, $$ \displaystyle \text{J} = \int_{0}^{\infty} \dfrac{x}{(x^2+a^2)(e^{\pi x} - 1)}  \ \mathrm{d}x \ ; \ a = (y+1)$$ I have proved here that, $\displaystyle \int_{0}^{\infty} \dfrac{\log(1-e^{-2a\pi x})}{1+x^2} \mathrm{d}x = \pi \left[\dfrac{1}{2} \log (2a\pi ) + a(\log a - 1) - \log(\Gamma(a+1)) \right] \tag{3}$ Differentiating both sides w.r.t. $a$, substituting $ a \mapsto \frac{a}{2} $ and $ x \mapsto \frac{x}{a} $, we get, $\displaystyle \int_{0}^{\infty} \dfrac{x}{(x^2+a^2)(e^{\pi x} - 1)}  \ \mathrm{d}x = \dfrac{1}{2} \left[ \dfrac{1}{a} + \ln \left( \dfrac{a}{2} \right) - \psi \left( \dfrac{a}{2} + 1 \right) \right] \tag{4}$ Putting $(4)$ in $(2)$, we have, $$ \displaystyle \text{I} = \int_{0}^{\infty} \left[ \dfrac{\ln y}{y+1} + \ln y \ln \left( \dfrac{y+1}{2} \right) - \ln y \ \psi \left( \dfrac{y+1}{2} + 1 \right) \right]  \mathrm{d}y $$ $ = \displaystyle \int_{0}^{\infty} \ln x\left[\ln \left( \dfrac{x+1}{2} \right) - \dfrac{1}{x+1} - \psi \left( \dfrac{x+1}{2} \right) \right] \mathrm{d}x \tag{*}$ Since the original question has already been proved in the link, so $(*)$ must be equal to the stated closed form. It also matches numerically. I'm looking for  some method to evaluate $(*)$ independent of Problem 5. Any help will be greatly appreciated.",,"['real-analysis', 'definite-integrals', 'improper-integrals', 'special-functions']"
94,Why is that the extended real line $\mathbb{\overline R}$ do not enjoy widespread use as $\mathbb{R}$?,Why is that the extended real line  do not enjoy widespread use as ?,\mathbb{\overline R} \mathbb{R},"Let  $\mathbb{\overline R}$ denote the extended real line. In a course on topology, I have heard people say that compact Hausdorff space is a space which topologists love and  $\mathbb{\overline R}$ seems to carry a lot of those good properties. So that's why I don't quite understand why we insist working with $\mathbb{R}$ instead of  $\mathbb{\overline R}$? In undergrad analysis and topology, you almost never heard about $\mathbb{\overline R}$. What you hear instead is you hear your professor telling you that $\infty$ is not a number. But when we start talking about measure theory, which I had a hell of a time with, it seems to be standard to say that $m(\mathbb{R}) = +\infty$. To me it would seem to ease a lot of the troubles with the reals by simply capping it off at the end. Maybe I exaggerated, the real has its problems (for example our computer cannot represent every real, the physical world is quantized but we use the reals to represent physical quantities anyways...), but at least this seems to me be the logical step towards having a more ""comfortable"" space to work with. So can someone give a strong reason why $\mathbb{\overline R}$ do not enjoy as widespread of use as $\mathbb{R}$?","Let  $\mathbb{\overline R}$ denote the extended real line. In a course on topology, I have heard people say that compact Hausdorff space is a space which topologists love and  $\mathbb{\overline R}$ seems to carry a lot of those good properties. So that's why I don't quite understand why we insist working with $\mathbb{R}$ instead of  $\mathbb{\overline R}$? In undergrad analysis and topology, you almost never heard about $\mathbb{\overline R}$. What you hear instead is you hear your professor telling you that $\infty$ is not a number. But when we start talking about measure theory, which I had a hell of a time with, it seems to be standard to say that $m(\mathbb{R}) = +\infty$. To me it would seem to ease a lot of the troubles with the reals by simply capping it off at the end. Maybe I exaggerated, the real has its problems (for example our computer cannot represent every real, the physical world is quantized but we use the reals to represent physical quantities anyways...), but at least this seems to me be the logical step towards having a more ""comfortable"" space to work with. So can someone give a strong reason why $\mathbb{\overline R}$ do not enjoy as widespread of use as $\mathbb{R}$?",,"['real-analysis', 'general-topology', 'real-numbers']"
95,How many closed subsets of $\mathbb R$ are there up to homeomorphism?,How many closed subsets of  are there up to homeomorphism?,\mathbb R,"I know there are lists of convex subsets of $\mathbb{R}$ up to homeomorphism, and closed convex subsets of $\mathbb{R}^2$ up to homeomorphism, but what about just closed subsets in general of $\mathbb{R}$?","I know there are lists of convex subsets of $\mathbb{R}$ up to homeomorphism, and closed convex subsets of $\mathbb{R}^2$ up to homeomorphism, but what about just closed subsets in general of $\mathbb{R}$?",,"['real-analysis', 'general-topology']"
96,Lipschitz space-filling maps,Lipschitz space-filling maps,,"First, some preliminaries and context. Let $f \colon [0,1]\to[0,1]^2$ be a space-filling curve . If we put on $[0,1]$ and $[0,1]^2$ the standard Euclidean metrics induced by $\mathbb{R}$ and $\mathbb{R}^2$ respectively, it is somewhat well known that $f$ can't be Lipschitz, because Lipschitz maps don't increase Hausdorff dimension , and we have $\dim_H([0,1])=1$ and $\dim_H([0,1]^2)=2$ , where $\dim_H$ denotes the Hausdorff dimension. To hope for such a map to be Lipschitz we can put on $[0,1]$ the metric given by $|\cdot|^{\frac12}$ , where $|\cdot|$ denotes the standard metric. In this case, since Hausdorff dimension relies on the metric, we have $\widetilde{\dim}_H([0,1])=\dim_H([0,1]^2)=2$ , where $\widetilde{\dim}_H$ denotes the Hausdorff dimension with respect to the metric $|\cdot|^{\frac12}$ on $[0,1]$ . Hence, it is a priori possible to find a Lipschitz space-filling curve $f$ . The fact that $\widetilde{\dim}_H([0,1])=2$ is an easy calculation using the fact that $\dim_H([0,1])=1$ . I should be able to prove, using some nice properties of dyadic numbers that the Hilbert curve is indeed Lipschitz in this case (with $L=1$ ). I couldn't extend such proof to other curves which do not have any nice dyadic structure, for instance the Peano curve .  I could attempt to write down the proof, but for the time being this is not helpful to the purpose of my question so I will postpone until someone requires it. I have two questions: Does anybody have a proof of the fact that a particular space-filling curve is Lipschitz in the sense described above? I would be happy to see any proof, elementary or not. Is it true that any space-filling curve is Lipschitz? If yes, why? If not, is there a counterexample? I'm interested in this because such a map is a typical example of a Lipschitz map which does not have any biLipschitz piece. Any comment or partial answer is very welcome.","First, some preliminaries and context. Let be a space-filling curve . If we put on and the standard Euclidean metrics induced by and respectively, it is somewhat well known that can't be Lipschitz, because Lipschitz maps don't increase Hausdorff dimension , and we have and , where denotes the Hausdorff dimension. To hope for such a map to be Lipschitz we can put on the metric given by , where denotes the standard metric. In this case, since Hausdorff dimension relies on the metric, we have , where denotes the Hausdorff dimension with respect to the metric on . Hence, it is a priori possible to find a Lipschitz space-filling curve . The fact that is an easy calculation using the fact that . I should be able to prove, using some nice properties of dyadic numbers that the Hilbert curve is indeed Lipschitz in this case (with ). I couldn't extend such proof to other curves which do not have any nice dyadic structure, for instance the Peano curve .  I could attempt to write down the proof, but for the time being this is not helpful to the purpose of my question so I will postpone until someone requires it. I have two questions: Does anybody have a proof of the fact that a particular space-filling curve is Lipschitz in the sense described above? I would be happy to see any proof, elementary or not. Is it true that any space-filling curve is Lipschitz? If yes, why? If not, is there a counterexample? I'm interested in this because such a map is a typical example of a Lipschitz map which does not have any biLipschitz piece. Any comment or partial answer is very welcome.","f \colon [0,1]\to[0,1]^2 [0,1] [0,1]^2 \mathbb{R} \mathbb{R}^2 f \dim_H([0,1])=1 \dim_H([0,1]^2)=2 \dim_H [0,1] |\cdot|^{\frac12} |\cdot| \widetilde{\dim}_H([0,1])=\dim_H([0,1]^2)=2 \widetilde{\dim}_H |\cdot|^{\frac12} [0,1] f \widetilde{\dim}_H([0,1])=2 \dim_H([0,1])=1 L=1","['real-analysis', 'geometric-measure-theory', 'lipschitz-functions']"
97,Why is $\lim_{n \to +\infty }{\sqrt[n]{a_1 a_2 \cdots a_n}} =\lim_{n \to +\infty}{a_n}$,Why is,\lim_{n \to +\infty }{\sqrt[n]{a_1 a_2 \cdots a_n}} =\lim_{n \to +\infty}{a_n},"Solving some problems regarding limits and sequence convergence, i stumbled upon a task, and it's solution relies on, and i quote: ""We now use a well-known theorem : $$\lim_{n \to +\infty }{\sqrt[n]{a_1 a_2 \ldots a_n}} = \lim_{n \to +\infty}{a_n}$$ This isn't really intuitive (at least to me) and I don't know how to prove it. The original task was to find the limit of  $$\lim_{n \to +\infty }{\sqrt[n]{\bigg{(}1+\frac{1}{1}\bigg{)} \bigg{(}1+\frac{1}{2}\bigg{)}^2 \ldots \bigg{(}1+\frac{1}{n}\bigg{)}^n}} $$ which of course, using the expression above is just $e$.","Solving some problems regarding limits and sequence convergence, i stumbled upon a task, and it's solution relies on, and i quote: ""We now use a well-known theorem : $$\lim_{n \to +\infty }{\sqrt[n]{a_1 a_2 \ldots a_n}} = \lim_{n \to +\infty}{a_n}$$ This isn't really intuitive (at least to me) and I don't know how to prove it. The original task was to find the limit of  $$\lim_{n \to +\infty }{\sqrt[n]{\bigg{(}1+\frac{1}{1}\bigg{)} \bigg{(}1+\frac{1}{2}\bigg{)}^2 \ldots \bigg{(}1+\frac{1}{n}\bigg{)}^n}} $$ which of course, using the expression above is just $e$.",,"['real-analysis', 'sequences-and-series', 'limits']"
98,Does $\sum |\sin n| / n$ converge? [duplicate],Does  converge? [duplicate],\sum |\sin n| / n,"This question already has answers here : Covergence test of $\sum_{n\geq 1}{\frac{|\sin n|}{n}}$ (3 answers) Closed 9 years ago . How can I prove if the following series converges? $$\sum_{n\geqslant1} \frac{|\sin n|}{n}$$ I can't use differential or integral calculus. I've tried using Dirichlet and Cauchy tests, but they didn't get me anywhere.","This question already has answers here : Covergence test of $\sum_{n\geq 1}{\frac{|\sin n|}{n}}$ (3 answers) Closed 9 years ago . How can I prove if the following series converges? $$\sum_{n\geqslant1} \frac{|\sin n|}{n}$$ I can't use differential or integral calculus. I've tried using Dirichlet and Cauchy tests, but they didn't get me anywhere.",,"['real-analysis', 'sequences-and-series']"
99,"Is the space $B([a,b])$ separable?",Is the space  separable?,"B([a,b])","Let $a$, $b$ be two real numbers such that $a < b$, and let $B([a,b])$ denote the metric space consisting of all (real or complex-valued) functions $x=x(t)$, $y=y(t)$ that are bounded on the closed interval $[a,b]$ with the metric $d$ defined as follows:  $$ d(x,y) \colon= \sup_{a\leq t \leq b} \ |x(t) - y(t)|.$$ Then how to determine whether or not this space is separable? By definition, a metric space $X$ is said to be separable if it has a countable dense subset, that is, if there is a countable subset $M$ of $X$ such that $\bar{M} = X$.","Let $a$, $b$ be two real numbers such that $a < b$, and let $B([a,b])$ denote the metric space consisting of all (real or complex-valued) functions $x=x(t)$, $y=y(t)$ that are bounded on the closed interval $[a,b]$ with the metric $d$ defined as follows:  $$ d(x,y) \colon= \sup_{a\leq t \leq b} \ |x(t) - y(t)|.$$ Then how to determine whether or not this space is separable? By definition, a metric space $X$ is said to be separable if it has a countable dense subset, that is, if there is a countable subset $M$ of $X$ such that $\bar{M} = X$.",,"['real-analysis', 'general-topology', 'analysis', 'functional-analysis', 'metric-spaces']"
