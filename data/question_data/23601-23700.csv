,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,What is Direct Sum Decomposition,What is Direct Sum Decomposition,,Suppose that $V$ is a finite dimension inner product space and $W$ is a subspace of $V$. Then we know that $V = W \oplus W^{\perp}$. What is this $\oplus$ operator? Is it equivalent to union $\bigcup$ or intersection $\bigcap$?,Suppose that $V$ is a finite dimension inner product space and $W$ is a subspace of $V$. Then we know that $V = W \oplus W^{\perp}$. What is this $\oplus$ operator? Is it equivalent to union $\bigcup$ or intersection $\bigcap$?,,"['linear-algebra', 'direct-sum']"
1,Best way to find Reduced Row Echelon Form (rref) of a matrix?,Best way to find Reduced Row Echelon Form (rref) of a matrix?,,"I'm sitting here doing rref problems and many of them seem so tedious. Any tricks out there to achieve rref with less effort or am I stuck with rewriting the matrix for every 2/3 operations? I know TI calculators can do it, but I'm gonna have to do this on my midterm, so I must learn how to do this the most efficient way possible. Thanks.","I'm sitting here doing rref problems and many of them seem so tedious. Any tricks out there to achieve rref with less effort or am I stuck with rewriting the matrix for every 2/3 operations? I know TI calculators can do it, but I'm gonna have to do this on my midterm, so I must learn how to do this the most efficient way possible. Thanks.",,"['linear-algebra', 'matrices']"
2,If two Normal operators commute then the product is normal,If two Normal operators commute then the product is normal,,"Let $T$ and $S$ be two normal operators in a infinite dimensional inner-product complex vector space. If $ST=TS$, I want to show that $TS$ is normal. For the finite-dimensional case, it went down to showing $T^*=f(T)$ for some polynomial $f$. But I have no clue on what to do on the infinite dimensional case. This is an exercise of Hoffman's Linear Algebra.","Let $T$ and $S$ be two normal operators in a infinite dimensional inner-product complex vector space. If $ST=TS$, I want to show that $TS$ is normal. For the finite-dimensional case, it went down to showing $T^*=f(T)$ for some polynomial $f$. But I have no clue on what to do on the infinite dimensional case. This is an exercise of Hoffman's Linear Algebra.",,['linear-algebra']
3,Do entries in augmented columns count as pivot?,Do entries in augmented columns count as pivot?,,"I am in a basic linear algebra course, and we are learning to solve linear equations with augmented matrices. We learned that when an augmented matrix is in row echelon form or reduced echelon form, you can tell if the system has one, infinitely many, or no solutions by looking if there is a pivot in every column or a pivot in every row. When you look for pivots, do entries in the augmented column count?","I am in a basic linear algebra course, and we are learning to solve linear equations with augmented matrices. We learned that when an augmented matrix is in row echelon form or reduced echelon form, you can tell if the system has one, infinitely many, or no solutions by looking if there is a pivot in every column or a pivot in every row. When you look for pivots, do entries in the augmented column count?",,"['linear-algebra', 'matrices', 'matrix-equations']"
4,"If $A^3B = BA^3$, then $AB = BA$.","If , then .",A^3B = BA^3 AB = BA,"Let $A$ be a Hermitian matrix. Suppose there exists a matrix $B$ such that $A^3B = BA^3$ . Show that $AB = BA$ . I was trying to use the fact that since $A$ is Hermitian, there exists a unitary matrix $U$ such that $UDU^* = A$ , thus $UD^3U^* B = BUD^3U^*$ , so $UD^3U^* B - BUD^3U^* = 0$ , $D^3U^* BU - U^*BUD^3= 0$ (multiplying both sides by $U^*$ and $U$ . Let $U^* BU = C$ , then we have $D^3C = C D^3$ , but I get stuck from here...","Let be a Hermitian matrix. Suppose there exists a matrix such that . Show that . I was trying to use the fact that since is Hermitian, there exists a unitary matrix such that , thus , so , (multiplying both sides by and . Let , then we have , but I get stuck from here...",A B A^3B = BA^3 AB = BA A U UDU^* = A UD^3U^* B = BUD^3U^* UD^3U^* B - BUD^3U^* = 0 D^3U^* BU - U^*BUD^3= 0 U^* U U^* BU = C D^3C = C D^3,['linear-algebra']
5,Prove that orthogonal projections in a Hilbert space satisfy $pq=0$ iff $p+q\le I$,Prove that orthogonal projections in a Hilbert space satisfy  iff,pq=0 p+q\le I,"Assume that $p$ and $q$ are (orthogonal) projections on Hilbert space $\mathcal{H}$. I want to prove: $pq=0$ iff $p+q\leq1$ I had the following in mind: Assume $pq=0$. Then $qp=0$, hence $p+q$ is a projection. One has the theorem that if e and f are two projections, then $e\leq f$ iff $ef=f$. But if we take $e=p+q$ and $f=1$, then $p+q\leq p+q$ which is true. Hence $p+q<1$. For the other direction I thought about an estimate of $||pqx||$, but I don't know how to go further. Anyone who can help me with this? Furthermore, if there are any corrections about the other directions proof, please let me know. Thanks","Assume that $p$ and $q$ are (orthogonal) projections on Hilbert space $\mathcal{H}$. I want to prove: $pq=0$ iff $p+q\leq1$ I had the following in mind: Assume $pq=0$. Then $qp=0$, hence $p+q$ is a projection. One has the theorem that if e and f are two projections, then $e\leq f$ iff $ef=f$. But if we take $e=p+q$ and $f=1$, then $p+q\leq p+q$ which is true. Hence $p+q<1$. For the other direction I thought about an estimate of $||pqx||$, but I don't know how to go further. Anyone who can help me with this? Furthermore, if there are any corrections about the other directions proof, please let me know. Thanks",,"['linear-algebra', 'operator-theory', 'hilbert-spaces', 'projection']"
6,How to prove that the dimension of a hyperplane is n-1,How to prove that the dimension of a hyperplane is n-1,,"The hyperplane $H$ defined by $$H:=\{x\in\mathbb {R}^n:a^Tx=b\}$$ is the set that has dimension $n-1$, my question is why or how can we prove that its dimension is $n-1$? Thank you to every one who provide any help or if possible the proof for that.","The hyperplane $H$ defined by $$H:=\{x\in\mathbb {R}^n:a^Tx=b\}$$ is the set that has dimension $n-1$, my question is why or how can we prove that its dimension is $n-1$? Thank you to every one who provide any help or if possible the proof for that.",,['linear-algebra']
7,$3$-linear functions on $\mathbb{R}^3$,-linear functions on,3 \mathbb{R}^3,"I know that any bilinear function $\Phi$ can be presented in a unique way as a sum $$\Phi = S + A,$$ where $S$ is a symmetric and $A$ is skew-symmetric bilinear functions. Does a similar statement hold for $3$-linear functions on $\mathbb{R}^3$?","I know that any bilinear function $\Phi$ can be presented in a unique way as a sum $$\Phi = S + A,$$ where $S$ is a symmetric and $A$ is skew-symmetric bilinear functions. Does a similar statement hold for $3$-linear functions on $\mathbb{R}^3$?",,[]
8,Maximum square cells in a rectangle,Maximum square cells in a rectangle,,"I know this sounds like bin packing but it's a bit different so please read the question to the end. Given a rectangle of known width and height, I need to divide it into smaller rectangles using column and rows. I'm essentially creating a grid(Think of iPhone's home screen, it's a grid with square icons). Now the problem(constrain): Cells' have a minimum size(say height > 10 and width > 10) The proportion of the cell needs to be as close to a square(1:1) as possible All cells' dimension need to be uniform in the grid. I'm solving for the number of columns and rows needed to satisfy the requirements above. Example: A rectangle of width=3; height=3 and with a cell minimum size of width >= 1; height >= 1 Becomes: In this example, column and row is 3 while all cells have a width and height of 1 UPDATE: A very rough solution that works some of the time : size = max(cellMinWidth, cellMinHeight) columns = floor(width/size) rows = floor(height/size) This fails when the difference between the width and height of the rectangle is big.","I know this sounds like bin packing but it's a bit different so please read the question to the end. Given a rectangle of known width and height, I need to divide it into smaller rectangles using column and rows. I'm essentially creating a grid(Think of iPhone's home screen, it's a grid with square icons). Now the problem(constrain): Cells' have a minimum size(say height > 10 and width > 10) The proportion of the cell needs to be as close to a square(1:1) as possible All cells' dimension need to be uniform in the grid. I'm solving for the number of columns and rows needed to satisfy the requirements above. Example: A rectangle of width=3; height=3 and with a cell minimum size of width >= 1; height >= 1 Becomes: In this example, column and row is 3 while all cells have a width and height of 1 UPDATE: A very rough solution that works some of the time : size = max(cellMinWidth, cellMinHeight) columns = floor(width/size) rows = floor(height/size) This fails when the difference between the width and height of the rectangle is big.",,"['linear-algebra', 'geometry']"
9,A Question about Non-Conservative Vector Fields,A Question about Non-Conservative Vector Fields,,"In my multivariable calculus class, we spent some time discussing the vector field that was the gradient of arctan(y/x). This field was shown to be non-conservative in closed regions which enclosed the origin. Our instructor also hinted at the idea that any non-conservative vector field could be described as the sum of a conservative gradient field and a multiple of (-y,x)/(x^2+y^2), which is the gradient of the function mentioned above. This idea reminded me of algebra, where, in a linear map, the image of any set and the image of sums of members of the set and the kernel are identical. My question now is, is there any sort of relationship between these two notions of kernel in an algebraic structure and conservative vector fields? Furthermore, does it make sense to talk about the dimension of the gradient operator, as a sort of local linear map, in this context?","In my multivariable calculus class, we spent some time discussing the vector field that was the gradient of arctan(y/x). This field was shown to be non-conservative in closed regions which enclosed the origin. Our instructor also hinted at the idea that any non-conservative vector field could be described as the sum of a conservative gradient field and a multiple of (-y,x)/(x^2+y^2), which is the gradient of the function mentioned above. This idea reminded me of algebra, where, in a linear map, the image of any set and the image of sums of members of the set and the kernel are identical. My question now is, is there any sort of relationship between these two notions of kernel in an algebraic structure and conservative vector fields? Furthermore, does it make sense to talk about the dimension of the gradient operator, as a sort of local linear map, in this context?",,"['linear-algebra', 'complex-analysis', 'multivariable-calculus', 'vector-fields']"
10,Find high powers of a matrix with the Cayley Hamilton theorem,Find high powers of a matrix with the Cayley Hamilton theorem,,Let A =  \begin{bmatrix} 0 & 1 & 0\\ 0 & 0 & 1\\ -1 & -1 &-1\\ \end{bmatrix} Compute $A^{10000} + A^{9998}$ I know this should be done by the Cayley-Hamilton theorem. I get as characteristic polynomial $-A^3 - A^2 - A - I = 0$ but I don't see how to calculate $A^{10000} + A^{9998}$ from there. I hope someone can help me out!,Let A =  \begin{bmatrix} 0 & 1 & 0\\ 0 & 0 & 1\\ -1 & -1 &-1\\ \end{bmatrix} Compute $A^{10000} + A^{9998}$ I know this should be done by the Cayley-Hamilton theorem. I get as characteristic polynomial $-A^3 - A^2 - A - I = 0$ but I don't see how to calculate $A^{10000} + A^{9998}$ from there. I hope someone can help me out!,,"['linear-algebra', 'matrices']"
11,Existence of a linear transformation in an infinite dimension vector space.,Existence of a linear transformation in an infinite dimension vector space.,,"If $V$ and $W$ are vector spaces, $\beta=\{v_1, \ldots , v_n\}$ is a finite a basis for $V$ and $\{w_1, \ldots , w_n\}\subset W$, we know there is an unique linear transformation $T:V\rightarrow W$  such that $T(v_i)=w_i$ for $i=1, 2, \ldots , n$ Is this valid when $V$ is not finite-dimensional?","If $V$ and $W$ are vector spaces, $\beta=\{v_1, \ldots , v_n\}$ is a finite a basis for $V$ and $\{w_1, \ldots , w_n\}\subset W$, we know there is an unique linear transformation $T:V\rightarrow W$  such that $T(v_i)=w_i$ for $i=1, 2, \ldots , n$ Is this valid when $V$ is not finite-dimensional?",,"['linear-algebra', 'vector-spaces', 'transformation']"
12,Hard problems book in linear algebra,Hard problems book in linear algebra,,Could you suggest me a book where I can find hard problems in Linear Algebra for an undergraduate student? Thanks in advance.,Could you suggest me a book where I can find hard problems in Linear Algebra for an undergraduate student? Thanks in advance.,,"['linear-algebra', 'reference-request', 'big-list', 'book-recommendation']"
13,Linear Independence easy question,Linear Independence easy question,,"I have these vectors $B = \{u, v, w\}$ with $$u = (-1, 1, -1),\, v = (19, 10, -9),\, w = (-1, x, y)$$ And i want to prove that these vectors are linearly independent. I have no problem to proove that three vectors without unknown variables are linearly indepedent but i have difficulties in this that i have two unknown variables $(x, y)$. I find that $\det(B)$ is $-28x - 29y -1$ but i do not know how this helps. The actual question is to prove that B forms a basis in R3 PS. New to Linear Algebra, dont shoot !","I have these vectors $B = \{u, v, w\}$ with $$u = (-1, 1, -1),\, v = (19, 10, -9),\, w = (-1, x, y)$$ And i want to prove that these vectors are linearly independent. I have no problem to proove that three vectors without unknown variables are linearly indepedent but i have difficulties in this that i have two unknown variables $(x, y)$. I find that $\det(B)$ is $-28x - 29y -1$ but i do not know how this helps. The actual question is to prove that B forms a basis in R3 PS. New to Linear Algebra, dont shoot !",,['linear-algebra']
14,"If N is elementary nilpotent matrix, show that N Transpose is similar to N","If N is elementary nilpotent matrix, show that N Transpose is similar to N",,"If $N$ is a $k \times k$ elementary nilpotent matrix, i.e. $N^k = 0$ but $N^{k-1} \ne 0$, then show that $N^\top$ is similar to $N$.  Now use the Jordan form to prove that every complex $n \times n$ matrix is similar to its transpose. I have figured out the second part, and am struggling with connecting nilpotency to being similar to the transpose.","If $N$ is a $k \times k$ elementary nilpotent matrix, i.e. $N^k = 0$ but $N^{k-1} \ne 0$, then show that $N^\top$ is similar to $N$.  Now use the Jordan form to prove that every complex $n \times n$ matrix is similar to its transpose. I have figured out the second part, and am struggling with connecting nilpotency to being similar to the transpose.",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
15,Where M is a matrix calculate a formula for M^n,Where M is a matrix calculate a formula for M^n,,Let $$M = \begin{bmatrix} -7 & 8 \\ -8 & -7 \end{bmatrix}.$$ Find formulas for the entries of $M^n$ where $n$ is a positive integer.  (Your formulas should not contain complex numbers.)  Your answer should be in the form of a matrix. I diagonalized to the form $M = P D P^{-1}$  and $M^n = P D^n P^{-1}$ where $P$ is my matrix of eigenvectors and $D$ is my matrix of eigenvalues. My final answer after diagonalization was $$M^n = \begin{bmatrix} .5((-7+8i)^n+(-7-8i)^n) & (i/2)(-(-7+8i)^n+(-7-8i)^n) \\ (.5/i)(-(-7+8i)^n+(-7-8i)^n) & .5((-7+8i)^n+(-7-8i)^n) \end{bmatrix}$$ I can't seem to find an answer not in terms of complex numbers.  Can someone show me what I'm missing?,Let $$M = \begin{bmatrix} -7 & 8 \\ -8 & -7 \end{bmatrix}.$$ Find formulas for the entries of $M^n$ where $n$ is a positive integer.  (Your formulas should not contain complex numbers.)  Your answer should be in the form of a matrix. I diagonalized to the form $M = P D P^{-1}$  and $M^n = P D^n P^{-1}$ where $P$ is my matrix of eigenvectors and $D$ is my matrix of eigenvalues. My final answer after diagonalization was $$M^n = \begin{bmatrix} .5((-7+8i)^n+(-7-8i)^n) & (i/2)(-(-7+8i)^n+(-7-8i)^n) \\ (.5/i)(-(-7+8i)^n+(-7-8i)^n) & .5((-7+8i)^n+(-7-8i)^n) \end{bmatrix}$$ I can't seem to find an answer not in terms of complex numbers.  Can someone show me what I'm missing?,,"['linear-algebra', 'diagonalization']"
16,Nilpotent linear operators,Nilpotent linear operators,,"Suppose that $T : V \to V$ is a linear operator on an $n$ -dimensional vector space $V$ . (a) Show that for all $i$ , $\ker T^i \subset \ker T^{i+1}$ . (b) Show that if $\ker T^k = \ker T^{k+1}$ , then $\ker T^k = \ker T^{k+j}$ for all $j \geq 1$ . (c) Show that if $T^k=0$ for some $k$ , then $T^n=0$ . My question is about (c), I do not understand what the question is asking, I was told ""if some power $k$ of the operator is the zero operator, then the smallest such power must be no larger than $n$ "", if so, can someone help me with this? Thanks in advance.","Suppose that is a linear operator on an -dimensional vector space . (a) Show that for all , . (b) Show that if , then for all . (c) Show that if for some , then . My question is about (c), I do not understand what the question is asking, I was told ""if some power of the operator is the zero operator, then the smallest such power must be no larger than "", if so, can someone help me with this? Thanks in advance.",T : V \to V n V i \ker T^i \subset \ker T^{i+1} \ker T^k = \ker T^{k+1} \ker T^k = \ker T^{k+j} j \geq 1 T^k=0 k T^n=0 k n,"['linear-algebra', 'vector-spaces', 'operator-theory']"
17,Notation for null vector with one entry = 1,Notation for null vector with one entry = 1,,"Is there a common notation for a vector which has all elements equal to 0 except for one, which is equal to 1? I was considering using a Kronecker delta, but the standard use of two subscripts, $\delta_{ij}$, seems unnecessary since it is a vector and therefore a rank 1 tensor, whereas the two indices suggest a rank 2 tensor. Any thoughts?","Is there a common notation for a vector which has all elements equal to 0 except for one, which is equal to 1? I was considering using a Kronecker delta, but the standard use of two subscripts, $\delta_{ij}$, seems unnecessary since it is a vector and therefore a rank 1 tensor, whereas the two indices suggest a rank 2 tensor. Any thoughts?",,"['linear-algebra', 'notation']"
18,"Define constant c so an equation system either has none, one or infinite solutions","Define constant c so an equation system either has none, one or infinite solutions",,"I have the following equation system matrix: $$\left[\begin{array}{ccc|c}   c & 1 & 1 & 1 \\   1 & c & 1 & 1 \\   1 & 1 & c & 1 \end{array}\right]$$ From this one I'm supposed to be able to define the constant $c$, for creating an equation with either no solutions, one solution or infinite solutions. I have successfully gotten the row reduce matrix: $$\left[\begin{array}{ccc|c}   1 & 0 & 0 & \frac{1}{c+2} \\   0 & 1 & 0 & \frac{1}{c+2} \\   0 & 0 & 1 & \frac{1}{c+2} \end{array}\right]$$ But I'm not sure where to go from this. Is it possible just from the this form to see which definitions of $c$ gives the different solutions?","I have the following equation system matrix: $$\left[\begin{array}{ccc|c}   c & 1 & 1 & 1 \\   1 & c & 1 & 1 \\   1 & 1 & c & 1 \end{array}\right]$$ From this one I'm supposed to be able to define the constant $c$, for creating an equation with either no solutions, one solution or infinite solutions. I have successfully gotten the row reduce matrix: $$\left[\begin{array}{ccc|c}   1 & 0 & 0 & \frac{1}{c+2} \\   0 & 1 & 0 & \frac{1}{c+2} \\   0 & 0 & 1 & \frac{1}{c+2} \end{array}\right]$$ But I'm not sure where to go from this. Is it possible just from the this form to see which definitions of $c$ gives the different solutions?",,[]
19,Geometric visualization of covector?,Geometric visualization of covector?,,How could I geometrically visualize a linear functional?,How could I geometrically visualize a linear functional?,,"['linear-algebra', 'analytic-geometry', 'visualization']"
20,Prove $T$ has at most two distinct eigenvalues,Prove  has at most two distinct eigenvalues,T,"The question is from Axler's Linear Algebra text. The $\mathcal{L}(V)$ stands for the space of linear operators on the vector space $V$. Suppose that V is a complex vector space with dim $V=n$ and $T \in \mathcal{L}(V)$ is such that     $$\text{null} \ T^{n-2} \neq \text{null} \ T^{n-1}  $$     Prove that $T$ has at most two distinct eigenvalues. I fist thought of solving this by contradiction. That is, I thought, suppose there were three distinct eigenvalues. Then, there would be an equation like  $$(x-\lambda_1I)^{d_1}(x-\lambda_2I)^{d_2}(x-\lambda_3I)^{d_3} $$ where the $d_i$'s are positive integers that sum to dim $V$. Call this polynomial $q(x)$ the characteristic poly. Thus, by Cayley's theorem, $$q(T)=(T-\lambda_1I)^{d_1}(T-\lambda_2I)^{d_2}(T-\lambda_3I)^{d_3}=0 $$ Then multiplying out and setting dim $V = d_1+d_2+d_3 = n$, I could then get a poly with the various powers of $T$ (this is a little tricky to write down). In particular, I wanted to see the powers $n-2$ and $n-1$. I thought, ok, so now, rewrite the poly in terms of each of these and using the fact that $\text{null} \ T^{n-2} \neq \text{null} \ T^{n-1}$ you get some vector $v \in V$ such that, $$T^{n-1}v = (\text{poly}_1)v = 0 $$  but  $$T^{n-2}v = (\text{poly}_2)v = k \neq 0 $$ I can think of some interesting things about $(\text{poly})_1$ and $(\text{poly})_2$, in particular, each has the monic term $T^n$. At this point, I'm not sure any of this helped. Well, anyways, I'm sure someone has a much better approach! Thanks in advance to anyone who read this.","The question is from Axler's Linear Algebra text. The $\mathcal{L}(V)$ stands for the space of linear operators on the vector space $V$. Suppose that V is a complex vector space with dim $V=n$ and $T \in \mathcal{L}(V)$ is such that     $$\text{null} \ T^{n-2} \neq \text{null} \ T^{n-1}  $$     Prove that $T$ has at most two distinct eigenvalues. I fist thought of solving this by contradiction. That is, I thought, suppose there were three distinct eigenvalues. Then, there would be an equation like  $$(x-\lambda_1I)^{d_1}(x-\lambda_2I)^{d_2}(x-\lambda_3I)^{d_3} $$ where the $d_i$'s are positive integers that sum to dim $V$. Call this polynomial $q(x)$ the characteristic poly. Thus, by Cayley's theorem, $$q(T)=(T-\lambda_1I)^{d_1}(T-\lambda_2I)^{d_2}(T-\lambda_3I)^{d_3}=0 $$ Then multiplying out and setting dim $V = d_1+d_2+d_3 = n$, I could then get a poly with the various powers of $T$ (this is a little tricky to write down). In particular, I wanted to see the powers $n-2$ and $n-1$. I thought, ok, so now, rewrite the poly in terms of each of these and using the fact that $\text{null} \ T^{n-2} \neq \text{null} \ T^{n-1}$ you get some vector $v \in V$ such that, $$T^{n-1}v = (\text{poly}_1)v = 0 $$  but  $$T^{n-2}v = (\text{poly}_2)v = k \neq 0 $$ I can think of some interesting things about $(\text{poly})_1$ and $(\text{poly})_2$, in particular, each has the monic term $T^n$. At this point, I'm not sure any of this helped. Well, anyways, I'm sure someone has a much better approach! Thanks in advance to anyone who read this.",,"['linear-algebra', 'vector-spaces', 'eigenvalues-eigenvectors']"
21,"Linear Algebra, Parseval's Identity","Linear Algebra, Parseval's Identity",,"How does one go about proving Parseval's identity? Let ${v_1, v_2, ..., v_n}$ be an orthonormal basis for a a finite-dimensional inner product space $V$ over some field $F$. For any $x, y$ in $V$, prove:   $\langle x, y \rangle$ $=$ $\sum\limits_{i = 1}^{n} \langle x, v_i \rangle \overline {\langle y, v_i \rangle}$. Attempt: using $ x = \sum\limits_{i = 1}^{n} \langle x, v_i \rangle v_i$ and $ y = \sum\limits_{i = 1}^{n} \langle y, v_i \rangle v_i$ But I don't really know how to proceed from here.","How does one go about proving Parseval's identity? Let ${v_1, v_2, ..., v_n}$ be an orthonormal basis for a a finite-dimensional inner product space $V$ over some field $F$. For any $x, y$ in $V$, prove:   $\langle x, y \rangle$ $=$ $\sum\limits_{i = 1}^{n} \langle x, v_i \rangle \overline {\langle y, v_i \rangle}$. Attempt: using $ x = \sum\limits_{i = 1}^{n} \langle x, v_i \rangle v_i$ and $ y = \sum\limits_{i = 1}^{n} \langle y, v_i \rangle v_i$ But I don't really know how to proceed from here.",,['linear-algebra']
22,Proving a diagonal matrix exists for linear operators with complemented invariant subspaces,Proving a diagonal matrix exists for linear operators with complemented invariant subspaces,,"I came across this problem one of my practice worksheets and I was stumped as to how I would go about solving this. Let $T : V \rightarrow V$ be a linear operator on a finite dimensional vector space $V$ over $\Bbb C$. Assume that $T$ has the following property: for each invariant subspace $U \subset V$ , there exists an invariant subspace $W \subset V$ such that $V = U \oplus W$. Show that $T$ has a diagonal matrix with respect to some basis of $V$. One thing I know is that as a direct result of the last assumption, that we have $\dim V$ linearly independent vectors that make up the bases of $V$ and $U$, but I'm not sure how exactly to go from here to showing that these vectors are eigenvectors of T. Any help would be greatly appreciated. Thanks!","I came across this problem one of my practice worksheets and I was stumped as to how I would go about solving this. Let $T : V \rightarrow V$ be a linear operator on a finite dimensional vector space $V$ over $\Bbb C$. Assume that $T$ has the following property: for each invariant subspace $U \subset V$ , there exists an invariant subspace $W \subset V$ such that $V = U \oplus W$. Show that $T$ has a diagonal matrix with respect to some basis of $V$. One thing I know is that as a direct result of the last assumption, that we have $\dim V$ linearly independent vectors that make up the bases of $V$ and $U$, but I'm not sure how exactly to go from here to showing that these vectors are eigenvectors of T. Any help would be greatly appreciated. Thanks!",,"['linear-algebra', 'diagonalization']"
23,How to prove that $ E:=ABC D $ is also positive definite?,How to prove that  is also positive definite?, E:=ABC D ,"Now I think this is true: Let $A$, $B$, $C$ and $D$ be symmetric, positive definite matrices and suppose that $E:=ABCD $ is symmetric.    How might I prove that $E$ is also positive definite? the similar question  can see: How to prove that $D := ABC$ is also positive definite? @Landscape  this not true,  Really? Thank you everyone","Now I think this is true: Let $A$, $B$, $C$ and $D$ be symmetric, positive definite matrices and suppose that $E:=ABCD $ is symmetric.    How might I prove that $E$ is also positive definite? the similar question  can see: How to prove that $D := ABC$ is also positive definite? @Landscape  this not true,  Really? Thank you everyone",,"['linear-algebra', 'matrices', 'quadratic-forms']"
24,Kernel of Differential Operator,Kernel of Differential Operator,,Suppose we have a linear differential equation: $$\left[D^{n}+a_{1}D^{n-1}+\dots+a_{n}D^{0}\right]y(t)=0$$ Where $y$ is an analytic function. How can we prove that the kernel of the differential operator $$\left[D^{n}+a_{1}D^{n-1}+\dots+a_{n}D^{0}\right]$$ has dimension $n$? Thanks a lot.,Suppose we have a linear differential equation: $$\left[D^{n}+a_{1}D^{n-1}+\dots+a_{n}D^{0}\right]y(t)=0$$ Where $y$ is an analytic function. How can we prove that the kernel of the differential operator $$\left[D^{n}+a_{1}D^{n-1}+\dots+a_{n}D^{0}\right]$$ has dimension $n$? Thanks a lot.,,"['linear-algebra', 'ordinary-differential-equations']"
25,Describe all matrices similar to a certain matrix.,Describe all matrices similar to a certain matrix.,,"Math people: I assigned this problem as homework to my students (from Strang's ""Linear Algebra and its Applications"", 4th edition): Describe in words all matrices that are similar to $$\begin{bmatrix}1& 0\\ 0& -1\end{bmatrix}$$ and find two of them. Square matrices $A$ and $B$ are defined to be ""similar"" if there exists square invertible $M$ with $A = M^{-1}BM$ (or vice versa, since this is an equivalence relation).  The answer to the problem is not in the text, and I am embarrassed to admit I am having trouble solving it.  The problem looked easy when I first saw it. The given matrix induces a reflection in the $x_2$-coordinate, but I don't see how the geometry helps.  A similar matrix has to have the same eigenvalues, trace, and determinant, so its trace is $0$ and its determinant is $-1$.  I spent a fair amount of time on it, with little progress, and I can spend my time more productively.  This problem is #2 in the problem set, which suggests that maybe there is an easy solution. I would settle for a hint that leads me to a solution. EDIT: Thanks to Thomas (?) for rendering my matrix in $\LaTeX$. Stefan (STack Exchange FAN)","Math people: I assigned this problem as homework to my students (from Strang's ""Linear Algebra and its Applications"", 4th edition): Describe in words all matrices that are similar to $$\begin{bmatrix}1& 0\\ 0& -1\end{bmatrix}$$ and find two of them. Square matrices $A$ and $B$ are defined to be ""similar"" if there exists square invertible $M$ with $A = M^{-1}BM$ (or vice versa, since this is an equivalence relation).  The answer to the problem is not in the text, and I am embarrassed to admit I am having trouble solving it.  The problem looked easy when I first saw it. The given matrix induces a reflection in the $x_2$-coordinate, but I don't see how the geometry helps.  A similar matrix has to have the same eigenvalues, trace, and determinant, so its trace is $0$ and its determinant is $-1$.  I spent a fair amount of time on it, with little progress, and I can spend my time more productively.  This problem is #2 in the problem set, which suggests that maybe there is an easy solution. I would settle for a hint that leads me to a solution. EDIT: Thanks to Thomas (?) for rendering my matrix in $\LaTeX$. Stefan (STack Exchange FAN)",,"['linear-algebra', 'matrices']"
26,Eigenvalues of block matrix from the eigenvalues of one block,Eigenvalues of block matrix from the eigenvalues of one block,,Give a matrix which can be decomposed into $4$ blocks $$B = \left[\begin{matrix}A &I \\ -I &0\end{matrix}\right]$$ where $I$ denotes the identity matrix and $0$ is a zero matrix. It's easy to compute the eigenvalues of $A$ . Is there a simple way to compute all the eigenvalues of $B$ ?,Give a matrix which can be decomposed into blocks where denotes the identity matrix and is a zero matrix. It's easy to compute the eigenvalues of . Is there a simple way to compute all the eigenvalues of ?,4 B = \left[\begin{matrix}A &I \\ -I &0\end{matrix}\right] I 0 A B,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant', 'block-matrices']"
27,Arf invariant for quadratic forms,Arf invariant for quadratic forms,,"I am reading a paper in which they define the Arf invariant as follows: Let $V$ be a vector space of dimension $2n$ over $\mathbb{Z}_2$ and $f$ is a non-degenerate bilinear form $V$. We call $q$ a quadratic form on $(V,f)$ if $$q(x+y)=q(x)+q(y)+f(x,y).$$ Then we define the Arf invariant of $q$ as $$\operatorname{Arf}(q)=\frac{1}{|V|}\sum_{\alpha \in V} (-1)^{q(\alpha)}.$$ Now they state that there are exactly $2^{2n-1}+2^{n-1}$ quadratic forms on $(V,f)$ with Arf invarian $1$ and $2^{2n-1}-2^{n-1}$ quadratic forms with Arf invariant $-1$. I don't understand why but if the statement is true, since there are exactly $2^{2n}$ quadratic forms on $(V,f)$, so each quadratic form always has Arf invariant $1$ or $-1$. It's not true by the definition. Some one can help me? Thanks a lot!","I am reading a paper in which they define the Arf invariant as follows: Let $V$ be a vector space of dimension $2n$ over $\mathbb{Z}_2$ and $f$ is a non-degenerate bilinear form $V$. We call $q$ a quadratic form on $(V,f)$ if $$q(x+y)=q(x)+q(y)+f(x,y).$$ Then we define the Arf invariant of $q$ as $$\operatorname{Arf}(q)=\frac{1}{|V|}\sum_{\alpha \in V} (-1)^{q(\alpha)}.$$ Now they state that there are exactly $2^{2n-1}+2^{n-1}$ quadratic forms on $(V,f)$ with Arf invarian $1$ and $2^{2n-1}-2^{n-1}$ quadratic forms with Arf invariant $-1$. I don't understand why but if the statement is true, since there are exactly $2^{2n}$ quadratic forms on $(V,f)$, so each quadratic form always has Arf invariant $1$ or $-1$. It's not true by the definition. Some one can help me? Thanks a lot!",,"['linear-algebra', 'combinatorics']"
28,Why is $\mathbb{R}^2$ not a subset and /or a subspace of $\mathbb{R}^3$?,Why is  not a subset and /or a subspace of ?,\mathbb{R}^2 \mathbb{R}^3,"One thing this suggests--at least to me--is that the x-y plane and $\mathbb{R}^2$ are not necessarily equivalent.  For example, I could define the following: $X = \left\{ \begin{bmatrix} x\\y\\z\end{bmatrix} x,y \in \mathbb{R} \land z = 0\right\}$.  Am I wrong to think, one, that this is a subset of $\mathbb{R}^3$?  As I write this it occurs to me that while scalar multiplication is closed under the above rules, addition doesn't pass the smell test for a subspace... so, OK, it's certainly not a subspace.  I would welcome any insight readers of this query can provide.","One thing this suggests--at least to me--is that the x-y plane and $\mathbb{R}^2$ are not necessarily equivalent.  For example, I could define the following: $X = \left\{ \begin{bmatrix} x\\y\\z\end{bmatrix} x,y \in \mathbb{R} \land z = 0\right\}$.  Am I wrong to think, one, that this is a subset of $\mathbb{R}^3$?  As I write this it occurs to me that while scalar multiplication is closed under the above rules, addition doesn't pass the smell test for a subspace... so, OK, it's certainly not a subspace.  I would welcome any insight readers of this query can provide.",,"['linear-algebra', 'vector-spaces']"
29,A theorem concerning unique linear mapping between vector spaces: What does it say?,A theorem concerning unique linear mapping between vector spaces: What does it say?,,"Theorem (from Schaum's Linear Algebra) Let $V$ and $U$ be vector spaces and $\{v_1, \ldots, v_n\}$ be a basis on $V$. Let $\{u_1,\ldots, u_n\}$ be arbitrary vectors in $U$. Then there exists a unique linear mapping $F: V \to U$ such that $F(v_i) = u_i$. I omit the proof, it is not so hard. But I don't understand what this theorem means or what does it imply, how one can use it to deduce more results. Can anybody briefly explain it? Thanks in advance!","Theorem (from Schaum's Linear Algebra) Let $V$ and $U$ be vector spaces and $\{v_1, \ldots, v_n\}$ be a basis on $V$. Let $\{u_1,\ldots, u_n\}$ be arbitrary vectors in $U$. Then there exists a unique linear mapping $F: V \to U$ such that $F(v_i) = u_i$. I omit the proof, it is not so hard. But I don't understand what this theorem means or what does it imply, how one can use it to deduce more results. Can anybody briefly explain it? Thanks in advance!",,['linear-algebra']
30,Factorization of an invertible symmetric matrix,Factorization of an invertible symmetric matrix,,"Given any invertible symmetric matrix: $A=\begin{bmatrix}a&b&c\\ b&d&e\\ c&e&f\end{bmatrix}$ over the complex number, Can be it factored as $A=T^\top T$? where $T^\top$ is the transpose matrix of $T$, for some invertible matrix $T$. Any suggestions are welcome! Thanks!","Given any invertible symmetric matrix: $A=\begin{bmatrix}a&b&c\\ b&d&e\\ c&e&f\end{bmatrix}$ over the complex number, Can be it factored as $A=T^\top T$? where $T^\top$ is the transpose matrix of $T$, for some invertible matrix $T$. Any suggestions are welcome! Thanks!",,"['linear-algebra', 'matrices']"
31,Show that norm of matrix $A$ is given by the square root of the largest eigenvalue of $A^tA$,Show that norm of matrix  is given by the square root of the largest eigenvalue of,A A^tA,The norm is defined as $\|A\|=\sup\{ \|A v \| : \|v\|=1\}$ . I want to show it is equal to the square root of the largest eigenvalue of $A^tA$ . I do not know why it is an eigenvalue of a product of $A^tA$ not simply an eigenvalue of $A$ . How to proceed?,The norm is defined as . I want to show it is equal to the square root of the largest eigenvalue of . I do not know why it is an eigenvalue of a product of not simply an eigenvalue of . How to proceed?,\|A\|=\sup\{ \|A v \| : \|v\|=1\} A^tA A^tA A,"['linear-algebra', 'matrices']"
32,Eigen Value of a Linear Map,Eigen Value of a Linear Map,,Let $V$ be the vector space of all continuous functions from $\mathbb{R}$ into $\mathbb{R}$ and let $T\colon V \rightarrow V$ be a linear map defined by $T(f)(x)=\int^{x}_{0}f(t)dt$. How can we prove $T$ has no eigenvalue?,Let $V$ be the vector space of all continuous functions from $\mathbb{R}$ into $\mathbb{R}$ and let $T\colon V \rightarrow V$ be a linear map defined by $T(f)(x)=\int^{x}_{0}f(t)dt$. How can we prove $T$ has no eigenvalue?,,"['linear-algebra', 'eigenvalues-eigenvectors']"
33,Modular Arithmetic over a Matrix,Modular Arithmetic over a Matrix,,"What are the rules for modular arithmetic when multiplying two matrices?  I want to calulate $C = AB \mod{n}.$  Aside from the obvious way of performing the modulo after the multiplication, when and where can i safely perform the modulo during the multiplication algorithm? [1] Normally: $C_{ij}=\displaystyle\sum\limits_{k=0}^m A_{ik}B_{kj}$ Can I take each of these summands $A_{ik}B_{kj} \mod{n}$, as follows? [2] $C_{ij}=\displaystyle\sum\limits_{k=0}^m [ A_{ik}B_{kj}\pmod{n} ]$ Here is an example: $A = \left(\begin{array}{cc} 9 & 2 \\ 10 & 10 \\ \end{array}\right)$ $B = \left(\begin{array}{cc} 7 & 3 \\ 1 & 6 \\ \end{array}\right)$ $C = \left(\begin{array}{cc} 65 & 39 \\ 80 & 90 \\ \end{array}\right)$ $C \equiv \left(\begin{array}{cc} 2 & 4 \\ 3 & 6 \\ \end{array}\right) \mod{7}$ edit: using [2] $C \mod 7= \left(\begin{array}{cc} 2 & 11 \\ 3 & 6 \\ \end{array}\right)$ This doesn't result in the same matrix.","What are the rules for modular arithmetic when multiplying two matrices?  I want to calulate $C = AB \mod{n}.$  Aside from the obvious way of performing the modulo after the multiplication, when and where can i safely perform the modulo during the multiplication algorithm? [1] Normally: $C_{ij}=\displaystyle\sum\limits_{k=0}^m A_{ik}B_{kj}$ Can I take each of these summands $A_{ik}B_{kj} \mod{n}$, as follows? [2] $C_{ij}=\displaystyle\sum\limits_{k=0}^m [ A_{ik}B_{kj}\pmod{n} ]$ Here is an example: $A = \left(\begin{array}{cc} 9 & 2 \\ 10 & 10 \\ \end{array}\right)$ $B = \left(\begin{array}{cc} 7 & 3 \\ 1 & 6 \\ \end{array}\right)$ $C = \left(\begin{array}{cc} 65 & 39 \\ 80 & 90 \\ \end{array}\right)$ $C \equiv \left(\begin{array}{cc} 2 & 4 \\ 3 & 6 \\ \end{array}\right) \mod{7}$ edit: using [2] $C \mod 7= \left(\begin{array}{cc} 2 & 11 \\ 3 & 6 \\ \end{array}\right)$ This doesn't result in the same matrix.",,"['linear-algebra', 'modular-arithmetic']"
34,Does the sign of the characteristic polynomial have any meaning?,Does the sign of the characteristic polynomial have any meaning?,,"The characteristic polynomial of a matrix $A \in \mathbb{C}^{n \times n}$, $p_A (\lambda) = \det(A-\lambda \cdot E)$ can be used to find the eigenvalues of the linear function $\varphi:\mathbb{C}^n \rightarrow \mathbb{C}^n, \varphi(x) := A \cdot x$, as the eigenvalues are the roots of $p_A(\lambda)$. So, for finding the eigenvalues, the sign of the characteristic polynomial isn't important. At the moment, this is to only application of the characteristic polynomial that I know. Do other applications of the characteristic polynomial exist, where the sign of it is important? Can I make any statements about the matrix $A$ when I know the sign of its characteristic polynomial?","The characteristic polynomial of a matrix $A \in \mathbb{C}^{n \times n}$, $p_A (\lambda) = \det(A-\lambda \cdot E)$ can be used to find the eigenvalues of the linear function $\varphi:\mathbb{C}^n \rightarrow \mathbb{C}^n, \varphi(x) := A \cdot x$, as the eigenvalues are the roots of $p_A(\lambda)$. So, for finding the eigenvalues, the sign of the characteristic polynomial isn't important. At the moment, this is to only application of the characteristic polynomial that I know. Do other applications of the characteristic polynomial exist, where the sign of it is important? Can I make any statements about the matrix $A$ when I know the sign of its characteristic polynomial?",,"['linear-algebra', 'matrices', 'polynomials']"
35,Invariant subspaces if $f$ is defined by more than one matrix,Invariant subspaces if  is defined by more than one matrix,f,"Suppose there is a group-homomorphism $f: G \rightarrow \operatorname{GL}(n,K)$ with $a\mapsto A, b\mapsto B$ for a group $G$ which is generated by two elements $a,b$ and matrices $A,B \in \operatorname{GL}(n,K)$. How can you calculate here the $f$-invariant subspaces of $K^n$? My problem here is, that there are two matrices $A$ and $B$. If $f$ would be defined just by one matrix $A$ than I can get the $f$-invariant subspaces by calculating the generalized eigenspaces (for n=2 just the normal eigenspaces) of $A$. What do I have to do when there is a second matrix $B$ and why? Could someone help me please.","Suppose there is a group-homomorphism $f: G \rightarrow \operatorname{GL}(n,K)$ with $a\mapsto A, b\mapsto B$ for a group $G$ which is generated by two elements $a,b$ and matrices $A,B \in \operatorname{GL}(n,K)$. How can you calculate here the $f$-invariant subspaces of $K^n$? My problem here is, that there are two matrices $A$ and $B$. If $f$ would be defined just by one matrix $A$ than I can get the $f$-invariant subspaces by calculating the generalized eigenspaces (for n=2 just the normal eigenspaces) of $A$. What do I have to do when there is a second matrix $B$ and why? Could someone help me please.",,"['linear-algebra', 'abstract-algebra', 'representation-theory']"
36,Inverse of symmetric matrix $M = A A^\top$,Inverse of symmetric matrix,M = A A^\top,"I have a matrix, generated by the product of a non-square matrix with its own transpose: $$M = A A^\top.$$ I need the inverse of $M$, assuming $\det(M) \neq 0$. Given the nature of the matrix $M$, are there any specialised computational methods to find its inverse, prioritising precision over speed?  Gauss-Jordan is prone to error, and I hope to find something nicer than and with comparable precision to the adj/det method. $M$ will either have be around $70 \times 70$, or $1000 \times 1000$ in size. I've had a quick read of the Matrix Cookbook and of this page , but (at the present time of 1 am) I'm struggling to see how it could help me. In case it helps, I'm actually trying to calculate: $$B = (A A^\top)^{-1} A.$$","I have a matrix, generated by the product of a non-square matrix with its own transpose: $$M = A A^\top.$$ I need the inverse of $M$, assuming $\det(M) \neq 0$. Given the nature of the matrix $M$, are there any specialised computational methods to find its inverse, prioritising precision over speed?  Gauss-Jordan is prone to error, and I hope to find something nicer than and with comparable precision to the adj/det method. $M$ will either have be around $70 \times 70$, or $1000 \times 1000$ in size. I've had a quick read of the Matrix Cookbook and of this page , but (at the present time of 1 am) I'm struggling to see how it could help me. In case it helps, I'm actually trying to calculate: $$B = (A A^\top)^{-1} A.$$",,"['linear-algebra', 'matrices', 'algorithms', 'inverse']"
37,"Functions that generate ""easy"" matrices of full rank","Functions that generate ""easy"" matrices of full rank",,"While explaining how to invert matrices I once used this ill-fated example  $A=\begin{pmatrix} 1&2&3\\4&5&6 \\7&8&9 \end{pmatrix}$ which can not be inverted ($\det(A)=0$). That got me thinking, given a matrix of size $N$, what are some good functions that map to the elements such that: The elements are integers The elements are ""small"" (for hand calculation) The matrix is always invertible (optional) the function has a random component, but still satisfies (3) Let $A_{ij} = f(j + (i-1)N)$. In the example above $f(n) = n$.","While explaining how to invert matrices I once used this ill-fated example  $A=\begin{pmatrix} 1&2&3\\4&5&6 \\7&8&9 \end{pmatrix}$ which can not be inverted ($\det(A)=0$). That got me thinking, given a matrix of size $N$, what are some good functions that map to the elements such that: The elements are integers The elements are ""small"" (for hand calculation) The matrix is always invertible (optional) the function has a random component, but still satisfies (3) Let $A_{ij} = f(j + (i-1)N)$. In the example above $f(n) = n$.",,"['linear-algebra', 'matrices', 'education']"
38,Understanding why the roots of homogeneous difference equation must be eigenvalues,Understanding why the roots of homogeneous difference equation must be eigenvalues,,"There is some obvious relationship between the root solutions to a homogeneous difference equation (as a recurrence relation) and eigenvalues which I'm trying to see. I have read over the wiki article 3.2, 3.4 and the eigenvalues ($\lambda$ ) are hinted at as the roots, but I'm still not sure why these must be eigenvalues of some matrix, say $A_0$, and what the meaning of $A_0$ may be. It seems that to solve a homogeneous linear difference equation we find the ""characteristic polynomial"" by simply factoring one difference equation. However, typically to find the ""characteristic polynomial"" I would solve the characteristic equation for some matrix, $A_0 = \begin{pmatrix} 1 & 0 & 0\\  0 & -2 & 0  \\  0 & 0 & 3  \\   \end{pmatrix}$ $(A_0 - \lambda I)\mathbf x = \mathbf 0$, then solve for the determinant equal to $0$, and then solve for each $\lambda$ e.g. $ \det(A_0 - \lambda I)  = 0$ $(1 - \lambda)(2 + \lambda)(3 - \lambda) = 0$ Now suppose this also happens to be a solution to some linear difference equation, and so here the characteristic polynomial is $\lambda^3 - 2\lambda^2 - 5\lambda + 6 = 0$, and the difference equation is.  $y_{k+3} - 2y_{k+2} - 5y_{k+1} + 6y_k = 0 $. Then, for example, $\lambda = 3$ is a solution for all k. Now, given we have found this solution to this difference equation, how can we explain some special relationship to $A_0$, other than $\lambda = 3$ happens to be an eigenvalue of $A_0$? Is there any meaning to make of $A_0$? (cf. 4.8, Linear Algebra 4th, D. Lay )","There is some obvious relationship between the root solutions to a homogeneous difference equation (as a recurrence relation) and eigenvalues which I'm trying to see. I have read over the wiki article 3.2, 3.4 and the eigenvalues ($\lambda$ ) are hinted at as the roots, but I'm still not sure why these must be eigenvalues of some matrix, say $A_0$, and what the meaning of $A_0$ may be. It seems that to solve a homogeneous linear difference equation we find the ""characteristic polynomial"" by simply factoring one difference equation. However, typically to find the ""characteristic polynomial"" I would solve the characteristic equation for some matrix, $A_0 = \begin{pmatrix} 1 & 0 & 0\\  0 & -2 & 0  \\  0 & 0 & 3  \\   \end{pmatrix}$ $(A_0 - \lambda I)\mathbf x = \mathbf 0$, then solve for the determinant equal to $0$, and then solve for each $\lambda$ e.g. $ \det(A_0 - \lambda I)  = 0$ $(1 - \lambda)(2 + \lambda)(3 - \lambda) = 0$ Now suppose this also happens to be a solution to some linear difference equation, and so here the characteristic polynomial is $\lambda^3 - 2\lambda^2 - 5\lambda + 6 = 0$, and the difference equation is.  $y_{k+3} - 2y_{k+2} - 5y_{k+1} + 6y_k = 0 $. Then, for example, $\lambda = 3$ is a solution for all k. Now, given we have found this solution to this difference equation, how can we explain some special relationship to $A_0$, other than $\lambda = 3$ happens to be an eigenvalue of $A_0$? Is there any meaning to make of $A_0$? (cf. 4.8, Linear Algebra 4th, D. Lay )",,"['linear-algebra', 'recurrence-relations', 'eigenvalues-eigenvectors']"
39,Difference between congruence and similarity transformations,Difference between congruence and similarity transformations,,"I am trying to understand the difference between a ""congruence"" or ""similarity"" transformation for two $n \times n$ matrices (which for the sake of simplicity, we'll assume are real). From what I can glean, a similarity transformation represents a change of basis from one orthogonal basis in $\mathbf{R}^n$ to another. My understanding is that a congruence transformation is an isometry, and so, it seems it would represent some geometrical operation like a (rigid) rotation, reflection, etc which preserves angles ad distances (but not necessarily orientation). If someone can tell me if this is correct or correct any mistakes in my interpretation, I'd be most appreciative. Thanks in advance.","I am trying to understand the difference between a ""congruence"" or ""similarity"" transformation for two $n \times n$ matrices (which for the sake of simplicity, we'll assume are real). From what I can glean, a similarity transformation represents a change of basis from one orthogonal basis in $\mathbf{R}^n$ to another. My understanding is that a congruence transformation is an isometry, and so, it seems it would represent some geometrical operation like a (rigid) rotation, reflection, etc which preserves angles ad distances (but not necessarily orientation). If someone can tell me if this is correct or correct any mistakes in my interpretation, I'd be most appreciative. Thanks in advance.",,"['linear-algebra', 'matrices']"
40,Dimension of spaces of bi/linear maps,Dimension of spaces of bi/linear maps,,"For $V$  a finite dimensional vector space over a field $\mathbb{K}$, I have encountered the claim that $$ \dim(\mathrm{Hom}(V,V)) = \dim(\mathrm{Hom}(V \times V, \mathbb{K})) $$ where $\mathrm{Hom}(V,V)$ denote the vector spaces, respectively, of all linear maps from $V$ to $V$ and all bilinear maps from $V\times V$ to the ground field $\mathbb{K}$. I'm sure I'm overlooking something elementary, but I don't see this. There is a theorem that, in general, for any finite-dimensional vector spaces $V$ and $W$ that $$ \dim(\mathrm{Hom}(V,W)) = \dim(V)\dim(W) $$ But, $\dim(V \times W) = \dim(V) + \dim(W)$ and therefore $$ \dim(\mathrm{Hom}(V \times V, \mathbb{K})) = (\dim(V) + \dim(V))\cdot \dim(K) = 2\dim(V)\cdot 1 $$ which is obviously not equal to $\dim(\mathrm{Hom}(V,V)) = \dim(V)\cdot\dim(V)$ Where is my mistake?","For $V$  a finite dimensional vector space over a field $\mathbb{K}$, I have encountered the claim that $$ \dim(\mathrm{Hom}(V,V)) = \dim(\mathrm{Hom}(V \times V, \mathbb{K})) $$ where $\mathrm{Hom}(V,V)$ denote the vector spaces, respectively, of all linear maps from $V$ to $V$ and all bilinear maps from $V\times V$ to the ground field $\mathbb{K}$. I'm sure I'm overlooking something elementary, but I don't see this. There is a theorem that, in general, for any finite-dimensional vector spaces $V$ and $W$ that $$ \dim(\mathrm{Hom}(V,W)) = \dim(V)\dim(W) $$ But, $\dim(V \times W) = \dim(V) + \dim(W)$ and therefore $$ \dim(\mathrm{Hom}(V \times V, \mathbb{K})) = (\dim(V) + \dim(V))\cdot \dim(K) = 2\dim(V)\cdot 1 $$ which is obviously not equal to $\dim(\mathrm{Hom}(V,V)) = \dim(V)\cdot\dim(V)$ Where is my mistake?",,['linear-algebra']
41,What does it mean for an action to be an $F$-linear transformation?,What does it mean for an action to be an -linear transformation?,F,"I'm working on a problem from Dummit & Foote's Abstract Algebra and I can't figure out what exactly I'm being asked to prove. I hate to ask this here, because it seems that I should've been able to find an answer on my own. The problem is from Section 13.2 #19 and it reads: Let $K$ be an extension of $F$ of degree $n$. (a) For any $\alpha\in K$ prove that $\alpha$ acting by left multiplication on $K$     is an $F$-linear transformation of $K$. I have searched and searched through the text and around on the internet and I cannot find an explicit definition of an ""$F$-linear transformation"".  What does this mean? If there is a definition for this in the book (or on the net) could anyone direct me to it?","I'm working on a problem from Dummit & Foote's Abstract Algebra and I can't figure out what exactly I'm being asked to prove. I hate to ask this here, because it seems that I should've been able to find an answer on my own. The problem is from Section 13.2 #19 and it reads: Let $K$ be an extension of $F$ of degree $n$. (a) For any $\alpha\in K$ prove that $\alpha$ acting by left multiplication on $K$     is an $F$-linear transformation of $K$. I have searched and searched through the text and around on the internet and I cannot find an explicit definition of an ""$F$-linear transformation"".  What does this mean? If there is a definition for this in the book (or on the net) could anyone direct me to it?",,"['linear-algebra', 'abstract-algebra']"
42,"Possible Jordan forms for a nilpotent $A$, given the ranks of $A$ and $A^2$","Possible Jordan forms for a nilpotent , given the ranks of  and",A A A^2,"Could you help me to solve this exercise? Let $A$ be an $8\times8$ nilpotent matrix over $\mathbb{C}$ with $\mathrm{rank}(A)=5$ and $\mathrm{rank}(A^2)=2$. List all possible Jordan canonical forms for $A$ and show that knowledge of $\mathrm{rank}(A^3)$ would allow one to determine the Jordan canonical form. This is what I have done: $A$ is nilpotent so the characteristic polynomial is $x^8$ and the minumum polynomial is $x^n$ with $3\leq n\leq8$. But now I don't know how to continue, any idea?","Could you help me to solve this exercise? Let $A$ be an $8\times8$ nilpotent matrix over $\mathbb{C}$ with $\mathrm{rank}(A)=5$ and $\mathrm{rank}(A^2)=2$. List all possible Jordan canonical forms for $A$ and show that knowledge of $\mathrm{rank}(A^3)$ would allow one to determine the Jordan canonical form. This is what I have done: $A$ is nilpotent so the characteristic polynomial is $x^8$ and the minumum polynomial is $x^n$ with $3\leq n\leq8$. But now I don't know how to continue, any idea?",,"['linear-algebra', 'matrices']"
43,Matrix such that $B^2 + B - I = 0$,Matrix such that,B^2 + B - I = 0,The following is an old exam question I'm trying solve but I just can't. Any help would be much appreciated. The teachers has provided answers to all questions on the exam besides this one.. A quadratic matrix $B$ satisfies the equation $B^2 + B - I = 0$ where $I$ is an identity matrix with dimensions $N\times N$ and where $0$ is the null matrix with the dimensions $N\times N$. a) What size is the matrix $B$? b) Show that $I + B$ is the inverse matrix of $B$. c) Show that $B^3 = -I + 2B$.,The following is an old exam question I'm trying solve but I just can't. Any help would be much appreciated. The teachers has provided answers to all questions on the exam besides this one.. A quadratic matrix $B$ satisfies the equation $B^2 + B - I = 0$ where $I$ is an identity matrix with dimensions $N\times N$ and where $0$ is the null matrix with the dimensions $N\times N$. a) What size is the matrix $B$? b) Show that $I + B$ is the inverse matrix of $B$. c) Show that $B^3 = -I + 2B$.,,"['linear-algebra', 'matrices']"
44,SVD-like matrix decomposition based on any basis?,SVD-like matrix decomposition based on any basis?,,"Let's say I have a point $\mathbf{x}$ in $n$-dimensional space. For any basis $(\mathbf{u}_1, ..., \mathbf{u}_n)$, $\mathbf{x}$ can be written as a linear combination of this basis. $\mathbf{x} = x_1 \mathbf{u}_1 + x_2 \mathbf{u}_2 + ... + x_n \mathbf{u}_n$ where each $x_i$ is a projection of $\mathbf{x}$ onto $\mathbf{u}_i$. Now I want to generalize it to a matrix $\mathbf{X}$ in $\mathbb{R}^{m\times n}$. The Singular Value Decomposition (SVD) guarantees that any matrx $\mathbf{X}$ of rank $r$ can be written as  $$\mathbf{X} = \sum_{i=1}^r \sigma_i \mathbf{u}_i \mathbf{v}^T_i$$ where $\mathbf{u}_1, ..., \mathbf{u}_r\in\mathbb{R}^m$ are orthonormal (each has length 1 and every pair is orthogonal) and $\mathbf{v}_1, ..., \mathbf{v}_r\in\mathbb{R}^n$ are also orthonormal. Each pair $\mathbf{u}_i$ and $\mathbf{v}_i$ form a pair of left and right singular vectgors with singular value $\sigma_i$. Note that $\mathbf{u}_1, ..., \mathbf{u}_r$ is not the only orthonormal vector set in $\mathbb{R}^m$. In fact, any $r$ vectors picked from an orthonormal basis $\mathbf{u}'_1, ..., \mathbf{u}'_m\in\mathbb{R}^m$ can be a candidate. The same holds for the right singular vectors $\mathbf{v}_i$'s. Then the question is, can I write up the SVD-like decomposition using every orthonormal basis other than left / right singular vectors? (Then the SVD can be regarded as a special case of this composition which uses left / right singular vectors.) I mean can I write something like $$\mathbf{X} = \sum_{i=1}^r \alpha_i \mathbf{s}_i \mathbf{t}^T_i$$ for every orthonormal vectors $\mathbf{s}_1, ..., \mathbf{s}_r\in\mathbb{R}^m$ and  $\mathbf{t}_1, ..., \mathbf{t}_r\in\mathbb{R}^n$? If that's possible, how can I compute such $\alpha_i$'s?","Let's say I have a point $\mathbf{x}$ in $n$-dimensional space. For any basis $(\mathbf{u}_1, ..., \mathbf{u}_n)$, $\mathbf{x}$ can be written as a linear combination of this basis. $\mathbf{x} = x_1 \mathbf{u}_1 + x_2 \mathbf{u}_2 + ... + x_n \mathbf{u}_n$ where each $x_i$ is a projection of $\mathbf{x}$ onto $\mathbf{u}_i$. Now I want to generalize it to a matrix $\mathbf{X}$ in $\mathbb{R}^{m\times n}$. The Singular Value Decomposition (SVD) guarantees that any matrx $\mathbf{X}$ of rank $r$ can be written as  $$\mathbf{X} = \sum_{i=1}^r \sigma_i \mathbf{u}_i \mathbf{v}^T_i$$ where $\mathbf{u}_1, ..., \mathbf{u}_r\in\mathbb{R}^m$ are orthonormal (each has length 1 and every pair is orthogonal) and $\mathbf{v}_1, ..., \mathbf{v}_r\in\mathbb{R}^n$ are also orthonormal. Each pair $\mathbf{u}_i$ and $\mathbf{v}_i$ form a pair of left and right singular vectgors with singular value $\sigma_i$. Note that $\mathbf{u}_1, ..., \mathbf{u}_r$ is not the only orthonormal vector set in $\mathbb{R}^m$. In fact, any $r$ vectors picked from an orthonormal basis $\mathbf{u}'_1, ..., \mathbf{u}'_m\in\mathbb{R}^m$ can be a candidate. The same holds for the right singular vectors $\mathbf{v}_i$'s. Then the question is, can I write up the SVD-like decomposition using every orthonormal basis other than left / right singular vectors? (Then the SVD can be regarded as a special case of this composition which uses left / right singular vectors.) I mean can I write something like $$\mathbf{X} = \sum_{i=1}^r \alpha_i \mathbf{s}_i \mathbf{t}^T_i$$ for every orthonormal vectors $\mathbf{s}_1, ..., \mathbf{s}_r\in\mathbb{R}^m$ and  $\mathbf{t}_1, ..., \mathbf{t}_r\in\mathbb{R}^n$? If that's possible, how can I compute such $\alpha_i$'s?",,['linear-algebra']
45,Theta transposes to x,Theta transposes to x,,"$$h(x) = \sum_{i = 0}^n \theta_i x_i = \theta^T x$$ I understand the above equation apart from the last bit on the right side. I think you have to read it Theta transposes to X. What does it mean? Does someone maybe have an example how it is used? Since I'm not an English native speaker, can you write out, how I would speak it correctly when I read the last part of the equation?","$$h(x) = \sum_{i = 0}^n \theta_i x_i = \theta^T x$$ I understand the above equation apart from the last bit on the right side. I think you have to read it Theta transposes to X. What does it mean? Does someone maybe have an example how it is used? Since I'm not an English native speaker, can you write out, how I would speak it correctly when I read the last part of the equation?",,"['linear-algebra', 'notation']"
46,Finding the correct pivot in Smith normal form,Finding the correct pivot in Smith normal form,,"I have been working through Smith normal form examples and I am wondering if I am finding the correct pivot in order to carry out the calculation. Let $V \subset \mathbb{Z}$ be an Abelian group with relation matrix $A$. $$ A = \begin{pmatrix} 2 & -6 & 0 \\ 0 & 2 & -6 \\ -6 & 0 &  2 \end{pmatrix}  $$ Question 1: For the case of entries in $\mathbb{Z}$, is the first step always to bring smallest integer to 1-1 position in the matrix? So we don't divide by 2 in this case right? (I was trying to do something similar to the matrix A  in the Wikipedia article but I have no idea how to make the first pivot 1 and still get $SNF(xI-A) = \begin{pmatrix}1&0  \\0 &(x-1)^2 \end{pmatrix}$ Applying my logic this is what I get for the Smith normal form (SNF) of the original problem $$\begin{pmatrix} 2 & -6 & 0 \\ 0 & 2 & -6 \\ -6 & 0 &  2 \end{pmatrix}  \sim \begin{pmatrix} 2 & -6 & 0 \\ 0 & 2 & -6 \\ 0 & -18 &  2 \end{pmatrix} \sim \begin{pmatrix} 2 & 0 & 0 \\ 0 & 2 & -6 \\ 0 & -18 &  2 \end{pmatrix} \sim \begin{pmatrix} 2 & 0 & 0 \\ 0 & 2 & -6 \\ 0 & 0 &  52 \end{pmatrix}  \sim \begin{pmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 &  52 \end{pmatrix}$$ Question 2: Is $V = \mathbb{Z}/2\mathbb{Z} \oplus \mathbb{Z}/2\mathbb{Z} \oplus \mathbb{Z} / 52\mathbb{Z}$ based on the above Smith normal form?","I have been working through Smith normal form examples and I am wondering if I am finding the correct pivot in order to carry out the calculation. Let $V \subset \mathbb{Z}$ be an Abelian group with relation matrix $A$. $$ A = \begin{pmatrix} 2 & -6 & 0 \\ 0 & 2 & -6 \\ -6 & 0 &  2 \end{pmatrix}  $$ Question 1: For the case of entries in $\mathbb{Z}$, is the first step always to bring smallest integer to 1-1 position in the matrix? So we don't divide by 2 in this case right? (I was trying to do something similar to the matrix A  in the Wikipedia article but I have no idea how to make the first pivot 1 and still get $SNF(xI-A) = \begin{pmatrix}1&0  \\0 &(x-1)^2 \end{pmatrix}$ Applying my logic this is what I get for the Smith normal form (SNF) of the original problem $$\begin{pmatrix} 2 & -6 & 0 \\ 0 & 2 & -6 \\ -6 & 0 &  2 \end{pmatrix}  \sim \begin{pmatrix} 2 & -6 & 0 \\ 0 & 2 & -6 \\ 0 & -18 &  2 \end{pmatrix} \sim \begin{pmatrix} 2 & 0 & 0 \\ 0 & 2 & -6 \\ 0 & -18 &  2 \end{pmatrix} \sim \begin{pmatrix} 2 & 0 & 0 \\ 0 & 2 & -6 \\ 0 & 0 &  52 \end{pmatrix}  \sim \begin{pmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 &  52 \end{pmatrix}$$ Question 2: Is $V = \mathbb{Z}/2\mathbb{Z} \oplus \mathbb{Z}/2\mathbb{Z} \oplus \mathbb{Z} / 52\mathbb{Z}$ based on the above Smith normal form?",,"['linear-algebra', 'abstract-algebra', 'smith-normal-form']"
47,Linear algebra: rank,Linear algebra: rank,,"Let $A:E\rightarrow F$ be a Linear Transformation between finite dimensional vector spaces, with $\mathrm{Rank}(A)=r$ and $\dim E=n$, $\dim F=m$. Prove that there are basis in $E$ and $F$ such that the matrix of $A$ has $a_{11}=\cdots=a_{rr}=1$ and $a_{ij}=0$ everywhere else, as entries. I thought in the change of basis $ap=qa'$ where $a'$ would be the matrix we want but as I got no information about $a$, $p$, $q$ then this is not a way out definetely. Then as the rank is the maximum number of independent columns and rows I thought I could just erase the ones that are linear dependent but this doesn't guarantee me that the transformation would be the same transformation without the deleted linear dependent columns and rows. A hint would be apreciated, Thanks in advance.","Let $A:E\rightarrow F$ be a Linear Transformation between finite dimensional vector spaces, with $\mathrm{Rank}(A)=r$ and $\dim E=n$, $\dim F=m$. Prove that there are basis in $E$ and $F$ such that the matrix of $A$ has $a_{11}=\cdots=a_{rr}=1$ and $a_{ij}=0$ everywhere else, as entries. I thought in the change of basis $ap=qa'$ where $a'$ would be the matrix we want but as I got no information about $a$, $p$, $q$ then this is not a way out definetely. Then as the rank is the maximum number of independent columns and rows I thought I could just erase the ones that are linear dependent but this doesn't guarantee me that the transformation would be the same transformation without the deleted linear dependent columns and rows. A hint would be apreciated, Thanks in advance.",,['linear-algebra']
48,Proving that $\det{(A^i_j})= \sqrt{ |\det{(G)}|}$,Proving that,\det{(A^i_j})= \sqrt{ |\det{(G)}|},"Let $V$ be an $n$-dimensional vector space and let $(v_1, \dots, v_n)$ denote any oriented basis for $V$. Also, let $g$ be an inner product on $V$ and let $G$ denote the Gram matrix of inner products $G = [g(v_i, v_j)]$. I am trying to show that if $v_j = A^k_je_k$, where $e_k$ denotes a basis that is orthonormal with respect to g, then $\det{(A^i_j)} = \sqrt{G}$. I believe I have found a useful intermediate result, but I'm not really sure how to close the deal. For vectors $v_i$ and $v_j$ we have: $$ g(v_i, v_j) = g(A^k_i e_k, A^r_j e_r) = A^k_iA^r_j \delta_{kr} = \sum\limits_{m=1}^n A^m_iA^m_j = \langle A_i | A_j\rangle $$ where $A_k$ denotes the $k^{th}$ column of $A$ and $\langle\cdot | \cdot\rangle$ denotes the standard Euclidean inner product. Therefore, the matrix $G$ is given by $$ G = [\langle A_i|A_j \rangle] $$ At this point, I'm not sure what to do. I'm thinking there's some essential fact I need to know in order to continue. So, my question is, am I on the right track and if so what should my next step be? Edit: I updated this question to change the assumption that the $e_i$ are the standard basis vectors to the assumption that the $e_i$ are orthonormal with respect to $g$","Let $V$ be an $n$-dimensional vector space and let $(v_1, \dots, v_n)$ denote any oriented basis for $V$. Also, let $g$ be an inner product on $V$ and let $G$ denote the Gram matrix of inner products $G = [g(v_i, v_j)]$. I am trying to show that if $v_j = A^k_je_k$, where $e_k$ denotes a basis that is orthonormal with respect to g, then $\det{(A^i_j)} = \sqrt{G}$. I believe I have found a useful intermediate result, but I'm not really sure how to close the deal. For vectors $v_i$ and $v_j$ we have: $$ g(v_i, v_j) = g(A^k_i e_k, A^r_j e_r) = A^k_iA^r_j \delta_{kr} = \sum\limits_{m=1}^n A^m_iA^m_j = \langle A_i | A_j\rangle $$ where $A_k$ denotes the $k^{th}$ column of $A$ and $\langle\cdot | \cdot\rangle$ denotes the standard Euclidean inner product. Therefore, the matrix $G$ is given by $$ G = [\langle A_i|A_j \rangle] $$ At this point, I'm not sure what to do. I'm thinking there's some essential fact I need to know in order to continue. So, my question is, am I on the right track and if so what should my next step be? Edit: I updated this question to change the assumption that the $e_i$ are the standard basis vectors to the assumption that the $e_i$ are orthonormal with respect to $g$",,['linear-algebra']
49,How to show that the following eigenvectors have to be orthogonal? [duplicate],How to show that the following eigenvectors have to be orthogonal? [duplicate],,"This question already has answers here : Eigenvectors of real symmetric matrices are orthogonal (7 answers) Closed 3 years ago . I have the following problem: Suppose that $A$ is a symmetric matrix, with $A$ = $A^{T}$ . Suppose $\vec{v}$ and $\vec{w}$ are eigenvectors of $A$ associated with distinct eigenvalues. Show that $\vec{v}$ and $\vec{w}$ must be orthogonal. (Hint: Show that a$\vec{v}$ $\cdot$ $\vec{w}$ = $\vec{v}$ $\cdot$ b$\vec{w}$.) I am unsure how to approach this, even with the hint taken into account. I tried to use the fact that orthogonal complement of Im(A) is in Ker of A transpose, and since they are equal it is also in ker A, but that didn't get me anywhere (I am probably thinking in the wrong direction). Thanks in advance for your hints!","This question already has answers here : Eigenvectors of real symmetric matrices are orthogonal (7 answers) Closed 3 years ago . I have the following problem: Suppose that $A$ is a symmetric matrix, with $A$ = $A^{T}$ . Suppose $\vec{v}$ and $\vec{w}$ are eigenvectors of $A$ associated with distinct eigenvalues. Show that $\vec{v}$ and $\vec{w}$ must be orthogonal. (Hint: Show that a$\vec{v}$ $\cdot$ $\vec{w}$ = $\vec{v}$ $\cdot$ b$\vec{w}$.) I am unsure how to approach this, even with the hint taken into account. I tried to use the fact that orthogonal complement of Im(A) is in Ker of A transpose, and since they are equal it is also in ker A, but that didn't get me anywhere (I am probably thinking in the wrong direction). Thanks in advance for your hints!",,[]
50,Basis for Kernel and Image of the following T,Basis for Kernel and Image of the following T,,"I am working on this practice problem, and I was wondering if I could get some help. I have a $T$:$\mathbb{R^{2x2}}\to \mathbf{P_{2}}$, that is, from 2x2 matrices to polynomials of degree at most 2. The transformation is given as following:  $$T\left(\begin{bmatrix} a & b\\  c & d \end{bmatrix}\right) = a-c+2d+(b+2c-d)t+(a-c+3d)t^{2}.$$ To get the basis of kernel of $T$, I solved a system of equations needed to get the 'O' element in the $\mathbf{P_{2}}$ -- $a-c+2d=0$, $b+2c-d=0$ and $a-c+3d=0$. As a result, I got the basis of the kernel equal to $$\begin{bmatrix} 1 & -2\\  1 & 0\\  \end{bmatrix}.$$ When it comes to image, if I understand correctly, I need to factor out all the variables separately, to see what is it that they span. So I got $a(1+t^{2})+b(t)+c(-t^{2}+2t-1)+d(3t^{2}-t+2)$. So would I be correct in saying that these three polynomials (without the coefficients $a$, $b$, $c$, and $d$) form the basis for the image $T$? Thank you!","I am working on this practice problem, and I was wondering if I could get some help. I have a $T$:$\mathbb{R^{2x2}}\to \mathbf{P_{2}}$, that is, from 2x2 matrices to polynomials of degree at most 2. The transformation is given as following:  $$T\left(\begin{bmatrix} a & b\\  c & d \end{bmatrix}\right) = a-c+2d+(b+2c-d)t+(a-c+3d)t^{2}.$$ To get the basis of kernel of $T$, I solved a system of equations needed to get the 'O' element in the $\mathbf{P_{2}}$ -- $a-c+2d=0$, $b+2c-d=0$ and $a-c+3d=0$. As a result, I got the basis of the kernel equal to $$\begin{bmatrix} 1 & -2\\  1 & 0\\  \end{bmatrix}.$$ When it comes to image, if I understand correctly, I need to factor out all the variables separately, to see what is it that they span. So I got $a(1+t^{2})+b(t)+c(-t^{2}+2t-1)+d(3t^{2}-t+2)$. So would I be correct in saying that these three polynomials (without the coefficients $a$, $b$, $c$, and $d$) form the basis for the image $T$? Thank you!",,['linear-algebra']
51,Pasting Together Fibers of a Vector Bundle,Pasting Together Fibers of a Vector Bundle,,"Everyone:   Please forgive that I do not yet know LaTex, bro, and my English ( I am from UCV in Venezuela). I think I understand  concept of bundles almost well, and that, once a vector bundle with a fiber is known/given, that we can define a new fiber pointwise, in manipulating each of the fibers, e.g., we may change the fiber from being R (over itself) to being R(+)R, or from R to R(x)R ; RxR*, (dual)etc.     What I  not too clear on, is on how one put together all the new fibers coherently into a bundle, i.e., how one construct new trivializations and transition functions to turn the space with altered fiber into a new bundle. I am particularly interest in the quotient bundle, if someone  knows. I imagine we use initial charts, trivialization to construct the altered bundle, but I don't see fully how, other than I pretty sure we use multilinear algebra and functoriality somehow.   Would be great if someone knew about how to do this for general fiber bundles. Thanks You from Caracas.","Everyone:   Please forgive that I do not yet know LaTex, bro, and my English ( I am from UCV in Venezuela). I think I understand  concept of bundles almost well, and that, once a vector bundle with a fiber is known/given, that we can define a new fiber pointwise, in manipulating each of the fibers, e.g., we may change the fiber from being R (over itself) to being R(+)R, or from R to R(x)R ; RxR*, (dual)etc.     What I  not too clear on, is on how one put together all the new fibers coherently into a bundle, i.e., how one construct new trivializations and transition functions to turn the space with altered fiber into a new bundle. I am particularly interest in the quotient bundle, if someone  knows. I imagine we use initial charts, trivialization to construct the altered bundle, but I don't see fully how, other than I pretty sure we use multilinear algebra and functoriality somehow.   Would be great if someone knew about how to do this for general fiber bundles. Thanks You from Caracas.",,['linear-algebra']
52,Determining Coefficients of a Finite Degree Polynomial $f$ from the Sequence $\{f(k)\}_{k \in \mathbb{N}}$,Determining Coefficients of a Finite Degree Polynomial  from the Sequence,f \{f(k)\}_{k \in \mathbb{N}},"Suppose $f$ is an unknown polynomial of degree $n$ (in one indeterminate) but the sequence $\{ f(k) \}_{k \in \mathbb{N}}$ is given. It is a nice exercise to show that one needs only the first $n+1$ terms of the sequence to determine the coefficients of $f$. That is, simply solve the matrix equation $A\mathbf{x} = b$, where $\mathbf{b} = (f(0), \dots, f(n))^{\top}$, $A$ is the Vandermonde matrix of $(i^{j})_{i,j = 0, \dots, n}$ and $\mathbf{x} = (c_{0}, \dots, c_{n})^{\top}$ (the unknown coefficients of $f$). Question : Is there a closed form expression for the coefficients of a finite degree polynomial $f$ in terms of the sequence $\{ f(k) \}_{k \in \mathbb{N}}$ that doesn't involve matrix inversion or differentiation or explicitly calculating the polynomial in question? ( Motivation ) The Ehrhart polynomial counts the number of integer lattice points intersecting a dilate of a polytope and can be calculated by the residue of an associated complex rational function (see M. Beck's articles on the subject). Some of the coefficients of the Ehrhart polynomial can be related to an $n$-volume, a relative area and the euler characteristic of said polytope. However, computing coefficients of the Ehrhart polynomial is not a particularly easy task. Having simple formulas for them, say in terms of the residues above would be nice to have at one's disposal. A reasonable starting point is answering the question above. Thanks!","Suppose $f$ is an unknown polynomial of degree $n$ (in one indeterminate) but the sequence $\{ f(k) \}_{k \in \mathbb{N}}$ is given. It is a nice exercise to show that one needs only the first $n+1$ terms of the sequence to determine the coefficients of $f$. That is, simply solve the matrix equation $A\mathbf{x} = b$, where $\mathbf{b} = (f(0), \dots, f(n))^{\top}$, $A$ is the Vandermonde matrix of $(i^{j})_{i,j = 0, \dots, n}$ and $\mathbf{x} = (c_{0}, \dots, c_{n})^{\top}$ (the unknown coefficients of $f$). Question : Is there a closed form expression for the coefficients of a finite degree polynomial $f$ in terms of the sequence $\{ f(k) \}_{k \in \mathbb{N}}$ that doesn't involve matrix inversion or differentiation or explicitly calculating the polynomial in question? ( Motivation ) The Ehrhart polynomial counts the number of integer lattice points intersecting a dilate of a polytope and can be calculated by the residue of an associated complex rational function (see M. Beck's articles on the subject). Some of the coefficients of the Ehrhart polynomial can be related to an $n$-volume, a relative area and the euler characteristic of said polytope. However, computing coefficients of the Ehrhart polynomial is not a particularly easy task. Having simple formulas for them, say in terms of the residues above would be nice to have at one's disposal. A reasonable starting point is answering the question above. Thanks!",,"['linear-algebra', 'abstract-algebra', 'polynomials', 'interpolation']"
53,Solving a linear equation given the solution of another,Solving a linear equation given the solution of another,,"Suppose I have a matrix $S$ having a one-dimensional nullspace $\{ e \}$ such that $S + ee^\top$ is a positive definite symmetric matrix. Now let $b \in Range(S)$ and suppose I solve the equation $(S + ee^\top)x = b$ is there anyway I can derive the solution $x'$ of the equation $Sx' = b$?   I was trying a Sherman Morrison Woodbury type formula, but this fails since the denominator is $0.$ Any help would be appreciated.","Suppose I have a matrix $S$ having a one-dimensional nullspace $\{ e \}$ such that $S + ee^\top$ is a positive definite symmetric matrix. Now let $b \in Range(S)$ and suppose I solve the equation $(S + ee^\top)x = b$ is there anyway I can derive the solution $x'$ of the equation $Sx' = b$?   I was trying a Sherman Morrison Woodbury type formula, but this fails since the denominator is $0.$ Any help would be appreciated.",,"['linear-algebra', 'matrices']"
54,Is a scalar presented as a matrix or not here?,Is a scalar presented as a matrix or not here?,,"In the linear algebra course I am taking, the inner product of 2 vectors $\langle u, v \rangle$ is defined as being a scalar; however, it is also viewed as being a product of 2 matrices as $u^Tv$ , as vectors are said to be a specific type of matrix, and the product of 2 matrices is defined as being a matrix. This seems to suggest a scalar is a matrix; yet matrix-scalar multiplication doesn't make sense if the scalar is a matrix. So my question is how to reconcile how in one case a scalar seems like it is a matrix, and on the other hand seems like it isn't, based on the way these concepts are expounded in the specific course I am taking. Is it the case that a scalar is indeed a matrix, but matrix-matrix multiplication is just defined in a unique way in the case of one of the factors being a scalar, which differs from the standard view?","In the linear algebra course I am taking, the inner product of 2 vectors is defined as being a scalar; however, it is also viewed as being a product of 2 matrices as , as vectors are said to be a specific type of matrix, and the product of 2 matrices is defined as being a matrix. This seems to suggest a scalar is a matrix; yet matrix-scalar multiplication doesn't make sense if the scalar is a matrix. So my question is how to reconcile how in one case a scalar seems like it is a matrix, and on the other hand seems like it isn't, based on the way these concepts are expounded in the specific course I am taking. Is it the case that a scalar is indeed a matrix, but matrix-matrix multiplication is just defined in a unique way in the case of one of the factors being a scalar, which differs from the standard view?","\langle u, v \rangle u^Tv","['linear-algebra', 'matrices', 'vectors', 'inner-products']"
55,"$f$ and $h\circ f$ are linear and $f$ is surjective, is $h$ linear?","and  are linear and  is surjective, is  linear?",f h\circ f f h,"I have a very easy question but I can't find the solution. Let $V,U,W$ be three $\mathbb{R}$ -vector spaces and let $f: V \rightarrow U$ be a surjective linear map and $g: V \rightarrow W$ a linear map. Now, define $h : U \rightarrow W$ such that $h \circ f =g$ . I want to know if $h$ is linear or not. Let $u_1,u_2 \in U$ such that $u_1= f(v_1)$ and $u_2= f(v_2)$ and let $c \in \mathbb{R}$ . Then, \begin{equation} \begin{aligned} h(cu_1+u_2) &= h(cf(v_1)+f(v_2)) \\ &= h(f(cv_1+v_2)) \\ &=g(cv_1+v_2) \\ &=cg(v_1)+g(v_2) \\ &= c h(f(v_1))+h(f(v_2)) \\ &= c h(u_1) +h(u_2) \end{aligned} \end{equation} Then, I want to conclude than $h : U \rightarrow W$ is linear. But now, take $f$ and $g$ such that $\text{ker} (f) \neq \text{ker} (g)$ (and suppose the kernel are not reduce to $0$ ). Then, there exists $v \in V$ such that $v \in \text{ker} (f)$ and $v \not\in \text{ker} (g)$ and so $$h(0) = h(f(v))=g(v) \neq 0$$ and so $h$ is not linear ... Can you explain to me what the problem is with my reasoning? My first calculation tells me that in any case $h$ is linear but the second says that when $\text{ker} (f) \neq \text{ker} (g)$ then $h$ is not linear ...","I have a very easy question but I can't find the solution. Let be three -vector spaces and let be a surjective linear map and a linear map. Now, define such that . I want to know if is linear or not. Let such that and and let . Then, Then, I want to conclude than is linear. But now, take and such that (and suppose the kernel are not reduce to ). Then, there exists such that and and so and so is not linear ... Can you explain to me what the problem is with my reasoning? My first calculation tells me that in any case is linear but the second says that when then is not linear ...","V,U,W \mathbb{R} f: V \rightarrow U g: V \rightarrow W h : U \rightarrow W h \circ f =g h u_1,u_2 \in U u_1= f(v_1) u_2= f(v_2) c \in \mathbb{R} \begin{equation}
\begin{aligned}
h(cu_1+u_2) &= h(cf(v_1)+f(v_2)) \\
&= h(f(cv_1+v_2)) \\
&=g(cv_1+v_2) \\
&=cg(v_1)+g(v_2) \\
&= c h(f(v_1))+h(f(v_2)) \\
&= c h(u_1) +h(u_2)
\end{aligned}
\end{equation} h : U \rightarrow W f g \text{ker} (f) \neq \text{ker} (g) 0 v \in V v \in \text{ker} (f) v \not\in \text{ker} (g) h(0) = h(f(v))=g(v) \neq 0 h h \text{ker} (f) \neq \text{ker} (g) h",['linear-algebra']
56,Show that the order of a finite subgroup of $\mathrm{SL}_n(\mathbb{Z})$ divides the order of $\mathrm{SL}_n(\mathbb{F_3})$,Show that the order of a finite subgroup of  divides the order of,\mathrm{SL}_n(\mathbb{Z}) \mathrm{SL}_n(\mathbb{F_3}),"This problem comes from Johns Hopkins University Spring 2020 algebra qualifying. Let $G$ be a finite subgroup of $\mathrm{SL}_n(\mathbb{Z})$ . Prove that the order of $G$ divides $$ \frac{1}{2}\left(3^n-1\right)\left(3^n-3\right) \ldots\left(3^n-3^{n-1}\right) . $$ Hint: Use reduction modulo 3. My idea: Assuming $\pi_p:\mathrm{SL}_n(\mathbb{Z}) \rightarrow \mathrm{SL}_n(\mathbb{F_3})$ performed by mod 3 ,so it is sufficient to prove $\pi_p$ provides a injection between finite subgroups  of $\mathrm{SL}_n(\mathbb{Z})$ to $\mathrm{SL}_n(\mathbb{F_3})$ , for which I cannot give a proof. I have found it have been solved on https://en.wikipedia.org/wiki/Congruence_subgroup ,where it provides a huge paper concerning some advanced method. But I havn't learn algebraic number theory. So I wonder some simple method for graduates level students.","This problem comes from Johns Hopkins University Spring 2020 algebra qualifying. Let be a finite subgroup of . Prove that the order of divides Hint: Use reduction modulo 3. My idea: Assuming performed by mod 3 ,so it is sufficient to prove provides a injection between finite subgroups  of to , for which I cannot give a proof. I have found it have been solved on https://en.wikipedia.org/wiki/Congruence_subgroup ,where it provides a huge paper concerning some advanced method. But I havn't learn algebraic number theory. So I wonder some simple method for graduates level students.","G \mathrm{SL}_n(\mathbb{Z}) G 
\frac{1}{2}\left(3^n-1\right)\left(3^n-3\right) \ldots\left(3^n-3^{n-1}\right) .
 \pi_p:\mathrm{SL}_n(\mathbb{Z}) \rightarrow \mathrm{SL}_n(\mathbb{F_3}) \pi_p \mathrm{SL}_n(\mathbb{Z}) \mathrm{SL}_n(\mathbb{F_3})","['linear-algebra', 'abstract-algebra', 'group-theory']"
57,Every matrix can be changed to a symmetric matrix through elementary column operations,Every matrix can be changed to a symmetric matrix through elementary column operations,,"The following question is given in a section 2 lecture of linear algebra. The first section is about polynomial, so the lectures just started to talk about determinants and matrices. Let $A$ be an $n\times n$ matrix over a number field $F$ . Then there exists an invertible matrix $R$ such that $AR$ is symmetric. I know that this question can be (elegantly) eliminated using Jordan canonical form. But since the question is left to who just learn linear algebra, I don’t think Jordan form is necessarily required. Then the question can be interpreted as the following: Let $A$ be an $n\times n$ matrix over a number field $F$ . Then $A$ can be changed to a symmetric matrix through elementary column operations. The Jordan form method only establishes the existence of some invertible matrix satisfying this property, which (I think) makes it unclear how to relate it with row/column operations. I think it may be dealt with by induction. Am I right? It is not very clear to me how to complete the inductive steps. Any help is sincerely appreciated.","The following question is given in a section 2 lecture of linear algebra. The first section is about polynomial, so the lectures just started to talk about determinants and matrices. Let be an matrix over a number field . Then there exists an invertible matrix such that is symmetric. I know that this question can be (elegantly) eliminated using Jordan canonical form. But since the question is left to who just learn linear algebra, I don’t think Jordan form is necessarily required. Then the question can be interpreted as the following: Let be an matrix over a number field . Then can be changed to a symmetric matrix through elementary column operations. The Jordan form method only establishes the existence of some invertible matrix satisfying this property, which (I think) makes it unclear how to relate it with row/column operations. I think it may be dealt with by induction. Am I right? It is not very clear to me how to complete the inductive steps. Any help is sincerely appreciated.",A n\times n F R AR A n\times n F A,['linear-algebra']
58,Why matrix representation of a linear transformation doesn't encode choice of basis for its range and domain?,Why matrix representation of a linear transformation doesn't encode choice of basis for its range and domain?,,"A linear transformation T is defined to be: $T: V \mapsto W$ , $Tv_k= \sum A_{i,j} *w_j$ , where $v_i$ 's are basises for $v$ and $w_j$ 's are basises for $W$ . A matrix representation of $T$ only encodes the $A_{i,j}$ leaving the choice of basis for domain and range( $v_i$ 's and $v_j$ 's). To me it sounds like a big flaw. Choice of basis for $V$ and $W$ affects $T$ 's characteristics like being Identity of not. For example if $V$ and $W$ are both $\mathbb{R^2}$ , $v_1 = (1,0)$ , $v_2 =(1,0)$ and $w_1 =  (0, 1)$ and $w_2 = (1,0)$ , let \begin{equation} T = \begin{bmatrix} 0 & 1\\ 1& 0 \end{bmatrix} . \end{equation} $T$ is going to map elements of $V$ to same elements of $W$ . Thus, $T$ is identity while it doesn't look like a identity matrix. Similarly, an Identity matrix where choice of basis for its domain and range are not the same, is not going to be identity. This was annoying me for a while until I pinpointed the problem.","A linear transformation T is defined to be: , , where 's are basises for and 's are basises for . A matrix representation of only encodes the leaving the choice of basis for domain and range( 's and 's). To me it sounds like a big flaw. Choice of basis for and affects 's characteristics like being Identity of not. For example if and are both , , and and , let is going to map elements of to same elements of . Thus, is identity while it doesn't look like a identity matrix. Similarly, an Identity matrix where choice of basis for its domain and range are not the same, is not going to be identity. This was annoying me for a while until I pinpointed the problem.","T: V \mapsto W Tv_k= \sum A_{i,j} *w_j v_i v w_j W T A_{i,j} v_i v_j V W T V W \mathbb{R^2} v_1 = (1,0) v_2 =(1,0) w_1 =  (0, 1) w_2 = (1,0) \begin{equation}
T = \begin{bmatrix}
0 & 1\\
1& 0
\end{bmatrix} .
\end{equation} T V W T","['linear-algebra', 'matrices']"
59,A linear algebra problem about linear subspace of $GL_n$,A linear algebra problem about linear subspace of,GL_n,"Assume $A_1,\dots,A_n$ are fixed $n\times n$ real matrices and satisfy that for any nonzero vector $v ∈ \mathbb R^n$ , the vectors $A_1v,\dots,A_nv$ form a basis for $\mathbb R^n$ . Find the integers $n$ such that the matrices $A_1,\dots,A_n$ exist. List examples of matrices $A_1,\dots, A_n$ for those $n$ . I found this problem in a book about linear algebra. I've solved the problem for $n=1,2,4$ and odd numbers before I posted the problem. At first I thought this problem is just about the matrices, but when observing the examples for $n=1,2,4$ I noticed that the problem has something to do with the product structure of $\mathbb R^n$ and this insight allows me to solve this problem.","Assume are fixed real matrices and satisfy that for any nonzero vector , the vectors form a basis for . Find the integers such that the matrices exist. List examples of matrices for those . I found this problem in a book about linear algebra. I've solved the problem for and odd numbers before I posted the problem. At first I thought this problem is just about the matrices, but when observing the examples for I noticed that the problem has something to do with the product structure of and this insight allows me to solve this problem.","A_1,\dots,A_n n\times n v ∈ \mathbb R^n A_1v,\dots,A_nv \mathbb R^n n A_1,\dots,A_n A_1,\dots, A_n n n=1,2,4 n=1,2,4 \mathbb R^n","['linear-algebra', 'matrices', 'problem-solving']"
60,Linear algebra book for early phd student,Linear algebra book for early phd student,,"I understand this is a frequently asked question, but I'm posing it again, since I am not sure about the extent to which other people who have asked this wanted the same thing as I do. I'm an early phd student in an engineering discipline. I am looking for a book on LA aimed at advanced ugrads/early grads to help me cover my gaps. I've taken an introductory course in LA whose exposition focused more on linear systems etc, as well as an abstract algebra course focusing on group theory. Now I want an LA book focusing more on the vector space aspects of it instead of matrix theory. The particular topics of focus are linear spaces and transformations , eigenvalues and eigendecompositions , inner product spaces, matrix norms and quadratic forms , as well as the geometric interpretation of these notions. Despite not being a math student, I'm not looking for an applied book (like Strang). My area of research requires an excellent command of the aforementioned topics, so I want a theoretically sound, proof-based exposition that doesn't delve too much into functional analysis/operator theory. Also, challenging problems are definitely a plus.","I understand this is a frequently asked question, but I'm posing it again, since I am not sure about the extent to which other people who have asked this wanted the same thing as I do. I'm an early phd student in an engineering discipline. I am looking for a book on LA aimed at advanced ugrads/early grads to help me cover my gaps. I've taken an introductory course in LA whose exposition focused more on linear systems etc, as well as an abstract algebra course focusing on group theory. Now I want an LA book focusing more on the vector space aspects of it instead of matrix theory. The particular topics of focus are linear spaces and transformations , eigenvalues and eigendecompositions , inner product spaces, matrix norms and quadratic forms , as well as the geometric interpretation of these notions. Despite not being a math student, I'm not looking for an applied book (like Strang). My area of research requires an excellent command of the aforementioned topics, so I want a theoretically sound, proof-based exposition that doesn't delve too much into functional analysis/operator theory. Also, challenging problems are definitely a plus.",,"['linear-algebra', 'reference-request']"
61,Proofs of Karpelevich's results about eigenvalues of nonnegative matrices.,Proofs of Karpelevich's results about eigenvalues of nonnegative matrices.,,"Are there any books or papers written in English that contain proofs of the results obtained in the following paper? F.I. Karpelevich, On the characteristic roots of matrices with nonnegative elements , Izv. Akad. Nauk SSSR Ker. Mat. , 1951, v.15, issue 4, 361-383. The statement without proof of Karpelevich's main result can be found in theorem 5.1 of the paper On $p$ th roots of stochastic matrices by Higham and Lin. According to the authors, more details of Karpelevich's results can be found in Minc's book Nonnegative Matrices , but again, without proofs.","Are there any books or papers written in English that contain proofs of the results obtained in the following paper? F.I. Karpelevich, On the characteristic roots of matrices with nonnegative elements , Izv. Akad. Nauk SSSR Ker. Mat. , 1951, v.15, issue 4, 361-383. The statement without proof of Karpelevich's main result can be found in theorem 5.1 of the paper On th roots of stochastic matrices by Higham and Lin. According to the authors, more details of Karpelevich's results can be found in Minc's book Nonnegative Matrices , but again, without proofs.",p,"['linear-algebra', 'reference-request', 'nonnegative-matrices']"
62,Proof of Cayley-Hamilton using Krylov subspaces,Proof of Cayley-Hamilton using Krylov subspaces,,"I came up with another proof of the Cayley-Hamilton Theorem.  Is this new?  The proof is by induction over the dimension of the underlying vector space. Let $v \in \mathbb F^n \setminus \{0\}$ .  Consider the Krylov subspaces $$ K_j = \text{span} \{v, Av, \dots, A^{j-1} v\} .$$ Let $$j_0 = \min\{j \ge 1 : K_j = K_{j+1}\} .$$ Case 1: $j_0 < n$ .  Then $K_{j_0}$ is an invariant subspace for $A$ , so with respect to a basis whose first $j_0$ elements are in $K_{j_0}$ , the matrix is a block upper triangular matrix.  Now the result follows by the inductive hypothesis on each of the diagonal blocks. Case 2: $j_0 = n$ .  Then $K_n = \mathbb F^n$ , and $\{v, Av,\dots,A^{n-1}v\}$ is a basis of $\mathbb F^n$ .  It follows that there exists $a_0, a_1, \dots, a_{n-1} \in \mathbb F$ such that $$ A^n v = -a_0 v - a_1 Av - a_2 A^2 v - \cdots - a_{n-1} A^{n-1} v .$$ That is, setting $$p(\lambda) = \lambda^n + a_{n-1}\lambda^{n-1} + \cdots + a_0,$$ we have $$ p(A) v = 0 .$$ For any vector $w \in \mathbb F^n$ , we have that $w = q(A) v$ for some polynomial $q$ .  Thus $$ p(A) w = p(A) q(A) v = q(A) p(A) v = 0 .$$ Hence $$ p(A) = 0 .$$ Finally with respect to the basis $\{v, Av,\dots,A^{n-1}v\}$ , the matrix $A$ has the form of the companion matrix: $$ \begin{bmatrix} 0 & 0 & 0 & \cdots & 0 & 0 & -a_0 \\                    1 & 0 & 0 & \cdots & 0 & 0 & -a_1 \\                    0 & 1 & 0 & \cdots & 0 & 0 & -a_2 \\                    \vdots & \vdots & \vdots & & \vdots & \vdots & \vdots \\                    0 & 0 & 0 & \cdots & 0 & 0 & -a_{n-3} \\                    0 & 0 & 0 & \cdots & 1 & 0 & -a_{n-2} \\                    0 & 0 & 0 & \cdots & 0 & 1 & -a_{n-1} \end{bmatrix} ,$$ and it is well known that the characteristic polynomial of the companion matrix is given by $$ p(\lambda) = \lambda^n + a_{n-1}\lambda^{n-1} + \cdots + a_0. $$","I came up with another proof of the Cayley-Hamilton Theorem.  Is this new?  The proof is by induction over the dimension of the underlying vector space. Let .  Consider the Krylov subspaces Let Case 1: .  Then is an invariant subspace for , so with respect to a basis whose first elements are in , the matrix is a block upper triangular matrix.  Now the result follows by the inductive hypothesis on each of the diagonal blocks. Case 2: .  Then , and is a basis of .  It follows that there exists such that That is, setting we have For any vector , we have that for some polynomial .  Thus Hence Finally with respect to the basis , the matrix has the form of the companion matrix: and it is well known that the characteristic polynomial of the companion matrix is given by","v \in \mathbb F^n \setminus \{0\}  K_j = \text{span} \{v, Av, \dots, A^{j-1} v\} . j_0 = \min\{j \ge 1 : K_j = K_{j+1}\} . j_0 < n K_{j_0} A j_0 K_{j_0} j_0 = n K_n = \mathbb F^n \{v, Av,\dots,A^{n-1}v\} \mathbb F^n a_0, a_1, \dots, a_{n-1} \in \mathbb F  A^n v = -a_0 v - a_1 Av - a_2 A^2 v - \cdots - a_{n-1} A^{n-1} v . p(\lambda) = \lambda^n + a_{n-1}\lambda^{n-1} + \cdots + a_0,  p(A) v = 0 . w \in \mathbb F^n w = q(A) v q  p(A) w = p(A) q(A) v = q(A) p(A) v = 0 .  p(A) = 0 . \{v, Av,\dots,A^{n-1}v\} A  \begin{bmatrix} 0 & 0 & 0 & \cdots & 0 & 0 & -a_0 \\
                   1 & 0 & 0 & \cdots & 0 & 0 & -a_1 \\
                   0 & 1 & 0 & \cdots & 0 & 0 & -a_2 \\
                   \vdots & \vdots & \vdots & & \vdots & \vdots & \vdots \\
                   0 & 0 & 0 & \cdots & 0 & 0 & -a_{n-3} \\
                   0 & 0 & 0 & \cdots & 1 & 0 & -a_{n-2} \\
                   0 & 0 & 0 & \cdots & 0 & 1 & -a_{n-1}
\end{bmatrix} ,  p(\lambda) = \lambda^n + a_{n-1}\lambda^{n-1} + \cdots + a_0. ","['linear-algebra', 'reference-request', 'cayley-hamilton']"
63,Proof of $BAC-CAB$ identity missing step,Proof of  identity missing step,BAC-CAB,"I'm stuck on one step of the proof for the identity: $$ \vec{A}\times(\vec{B}\times\vec{C}) = \vec{B}(\vec{A}\cdot\vec{C}) - \vec{C}(\vec{A}\cdot\vec{B})$$ So far, the proof follows as: We know that $\vec{B}\times\vec{C}$ gives a vector perpendicular to both $\vec{B}$ & $\vec{C}$ , and that $ \vec{A}\times(\vec{B}\times\vec{C})$ gives a vector perpendicular to both $\vec{A}$ & $(\vec{B}\times\vec{C})$ . Therefore, the vector $\vec{A}\times(\vec{B}\times\vec{C})$ must lie in the plane containing both $\vec{B}$ & $\vec{C}$ . Provided $\vec{B}$ & $\vec{C}$ are not parallel (if they were, $\vec{A}\times(\vec{B}\times\vec{C}) = 0$ regardless), vectors $\vec{B}$ & $\vec{C}$ span the 2D plane containing them both. Therefore, we can  express any vector in the plane as a linear combination of both $\vec{B}$ and $\vec{C}$ and so we can write: $$\vec{A}\times(\vec{B}\times\vec{C}) = \alpha\vec{B} + \beta\vec{C} \tag{1}$$ Taking the scalar product of both sides with $\vec{A}$ : $$\vec{A} \cdot (\vec{A}\times(\vec{B}\times\vec{C})) = \vec{A} \cdot (\alpha\vec{B} + \beta\vec{C}) = 0$$ So, $$\alpha(\vec{A} \cdot \vec{B}) + \beta(\vec{A} \cdot\vec{C}) = 0$$ Now writing, $$\lambda = \frac{\alpha}{\vec{A} \cdot\vec{C}} = -\frac{\beta}{\vec{A} \cdot \vec{B}}$$ and substituting $\alpha$ and $\beta$ back into (1) we get: $$ \vec{A}\times(\vec{B}\times\vec{C}) = \lambda(\vec{B}(\vec{A}\cdot\vec{C}) - \vec{C}(\vec{A}\cdot\vec{B}))$$ I am able to show $\lambda = 1$ with particular choices of unit vectors for $\vec{A}, \vec{B}, \vec{C}$ but I am unable to prove that $\lambda$ is independent of the magnitude of vectors (i.e. $\lambda = 1$ for all choices of $\vec{A}, \vec{B}, \vec{C}$ ). This is the step that I am struggling with. Any suggestions?","I'm stuck on one step of the proof for the identity: So far, the proof follows as: We know that gives a vector perpendicular to both & , and that gives a vector perpendicular to both & . Therefore, the vector must lie in the plane containing both & . Provided & are not parallel (if they were, regardless), vectors & span the 2D plane containing them both. Therefore, we can  express any vector in the plane as a linear combination of both and and so we can write: Taking the scalar product of both sides with : So, Now writing, and substituting and back into (1) we get: I am able to show with particular choices of unit vectors for but I am unable to prove that is independent of the magnitude of vectors (i.e. for all choices of ). This is the step that I am struggling with. Any suggestions?"," \vec{A}\times(\vec{B}\times\vec{C}) = \vec{B}(\vec{A}\cdot\vec{C}) - \vec{C}(\vec{A}\cdot\vec{B}) \vec{B}\times\vec{C} \vec{B} \vec{C}  \vec{A}\times(\vec{B}\times\vec{C}) \vec{A} (\vec{B}\times\vec{C}) \vec{A}\times(\vec{B}\times\vec{C}) \vec{B} \vec{C} \vec{B} \vec{C} \vec{A}\times(\vec{B}\times\vec{C}) = 0 \vec{B} \vec{C} \vec{B} \vec{C} \vec{A}\times(\vec{B}\times\vec{C}) = \alpha\vec{B} + \beta\vec{C} \tag{1} \vec{A} \vec{A} \cdot (\vec{A}\times(\vec{B}\times\vec{C})) = \vec{A} \cdot (\alpha\vec{B} + \beta\vec{C}) = 0 \alpha(\vec{A} \cdot \vec{B}) + \beta(\vec{A} \cdot\vec{C}) = 0 \lambda = \frac{\alpha}{\vec{A} \cdot\vec{C}} = -\frac{\beta}{\vec{A} \cdot \vec{B}} \alpha \beta  \vec{A}\times(\vec{B}\times\vec{C}) = \lambda(\vec{B}(\vec{A}\cdot\vec{C}) - \vec{C}(\vec{A}\cdot\vec{B})) \lambda = 1 \vec{A}, \vec{B}, \vec{C} \lambda \lambda = 1 \vec{A}, \vec{B}, \vec{C}","['linear-algebra', 'vectors', 'proof-explanation']"
64,Is there a nontrivial homomorphism $\mathbb{Q} \to SL_n(\mathbb{Z})$?,Is there a nontrivial homomorphism ?,\mathbb{Q} \to SL_n(\mathbb{Z}),"I am curious about whether there is a nontrivial group homomorphism $\mathbb{Q} \to SL_n(\mathbb{Z})$ for some $n$ . It's not hard to find such a homomorphism $\mathbb{Q} \to SL_2(\mathbb{Q})$ ; we can take the map given by $x \mapsto \left(\begin{smallmatrix} 1 & x \\ 0 & 1 \end{smallmatrix}\right)$ , but I don't see any obvious map into $SL_n(\mathbb{Z})$ . Another, weaker, question of interest is whether some $SL_n(\mathbb{Z})$ has an element $A \neq I$ for which $A$ has a $k$ -th root in $SL_n(\mathbb{Z})$ for every $k$ . In general, for a group $G$ , the condition that $G$ has an element with $k$ -th roots for every $k$ is strictly weaker than the condition that there is a nontrivial homomorphism $\mathbb{Q} \to G$ , so these questions may have different answers.","I am curious about whether there is a nontrivial group homomorphism for some . It's not hard to find such a homomorphism ; we can take the map given by , but I don't see any obvious map into . Another, weaker, question of interest is whether some has an element for which has a -th root in for every . In general, for a group , the condition that has an element with -th roots for every is strictly weaker than the condition that there is a nontrivial homomorphism , so these questions may have different answers.",\mathbb{Q} \to SL_n(\mathbb{Z}) n \mathbb{Q} \to SL_2(\mathbb{Q}) x \mapsto \left(\begin{smallmatrix} 1 & x \\ 0 & 1 \end{smallmatrix}\right) SL_n(\mathbb{Z}) SL_n(\mathbb{Z}) A \neq I A k SL_n(\mathbb{Z}) k G G k k \mathbb{Q} \to G,"['linear-algebra', 'matrices', 'group-theory', 'group-homomorphism']"
65,Show that there is a conjugation class that is contained in the subset of triangular matrices,Show that there is a conjugation class that is contained in the subset of triangular matrices,,"The question goes like this: Let $p$ be a prime, $G$ a subgroup of $GL(n, \mathbb{F}_p)$ such that $|G| = p^k$ for some $k \ge 1$ . Show that there exists an element $g \in GL(n, \mathbb{F}_p)$ such that $G^g$ is contained in the subgroup of $GL(n, \mathbb{F}_p)$ composed of upper triangular matrices with ones in the diagonal entries. Here $G^g = \{x^{-1} g x \mid x \in G\}$ . I think this must have something to do with the class equation, since $|G| = p^k$ must imply there is some conjugacy class with just one element. I'm not sure. EDIT: a previous question says that there is a common eigenvector $v$ with eigenvalue 1 for all matrices in $G$ , and there is a hint to use induction over $n$ , applying the induction hypothesis on $\mathbb{F}_p^n / \langle v \rangle$ .","The question goes like this: Let be a prime, a subgroup of such that for some . Show that there exists an element such that is contained in the subgroup of composed of upper triangular matrices with ones in the diagonal entries. Here . I think this must have something to do with the class equation, since must imply there is some conjugacy class with just one element. I'm not sure. EDIT: a previous question says that there is a common eigenvector with eigenvalue 1 for all matrices in , and there is a hint to use induction over , applying the induction hypothesis on .","p G GL(n, \mathbb{F}_p) |G| = p^k k \ge 1 g \in GL(n, \mathbb{F}_p) G^g GL(n, \mathbb{F}_p) G^g = \{x^{-1} g x \mid x \in G\} |G| = p^k v G n \mathbb{F}_p^n / \langle v \rangle","['linear-algebra', 'group-theory']"
66,Connection Between Bézout's Identity and Linear Algebra,Connection Between Bézout's Identity and Linear Algebra,,"Today I looked at Bézout's Identity and I became reminded a bit of linear algebra. Let me explain. Bézout's Identity says that for coprime integers $a, b$ , there exists integers $x,y$ such that $ax+by=1$ . So in essence $a$ and $b$ can generate any integer $n$ through the linear combination $a(nx)+b(ny)=n$ . I thought this reminiscent of the notion of linear independence and a spanning set, in that coprime integers can be thought of as independent, and two coprime integers $\{a,b\}$ can be said to span the integers. It seems to me that this bit of number theory generalizes to some algebraic structure. Is my intuition off? My knowledge of abstract algebra is to the extent of elementary group theory.","Today I looked at Bézout's Identity and I became reminded a bit of linear algebra. Let me explain. Bézout's Identity says that for coprime integers , there exists integers such that . So in essence and can generate any integer through the linear combination . I thought this reminiscent of the notion of linear independence and a spanning set, in that coprime integers can be thought of as independent, and two coprime integers can be said to span the integers. It seems to me that this bit of number theory generalizes to some algebraic structure. Is my intuition off? My knowledge of abstract algebra is to the extent of elementary group theory.","a, b x,y ax+by=1 a b n a(nx)+b(ny)=n \{a,b\}","['linear-algebra', 'abstract-algebra', 'number-theory']"
67,"Given the equation $\alpha \mathbf{v} + \mathbf{v}\times\mathbf{a} = \mathbf{b}$, solve for $\mathbf{v}$.","Given the equation , solve for .",\alpha \mathbf{v} + \mathbf{v}\times\mathbf{a} = \mathbf{b} \mathbf{v},"I'm reading a textbook at the moment that provides the following linear equation, $$ \alpha \mathbf{v} + \mathbf{v}\times\mathbf{a} = \mathbf{b}, $$ and asks to solve for $\mathbf{v}$ . The form of $\mathbf{v}$ is given as $$ \mathbf{v} = \frac{\alpha^2 \mathbf{b} - \alpha (\mathbf{b} \times \mathbf{a}) + (\mathbf{a}\cdot\mathbf{b})\mathbf{a}}{\alpha(\alpha^2+\lvert \mathbf{a} \rvert^2)}. $$ It's easy enough to verify that this is the correct solution. However, I can't figure out how I'd solve for $\mathbf{v}$ if given just the original equation. Are there any general approaches to solving this kind of equation systematically? Edit: $\mathbf{a}, \mathbf{b}$ and $\mathbf{v}$ are all vectors, whereas $\alpha$ is a scalar such that $\alpha \neq 0$ .","I'm reading a textbook at the moment that provides the following linear equation, and asks to solve for . The form of is given as It's easy enough to verify that this is the correct solution. However, I can't figure out how I'd solve for if given just the original equation. Are there any general approaches to solving this kind of equation systematically? Edit: and are all vectors, whereas is a scalar such that .","
\alpha \mathbf{v} + \mathbf{v}\times\mathbf{a} = \mathbf{b},
 \mathbf{v} \mathbf{v} 
\mathbf{v} = \frac{\alpha^2 \mathbf{b} - \alpha (\mathbf{b} \times \mathbf{a}) + (\mathbf{a}\cdot\mathbf{b})\mathbf{a}}{\alpha(\alpha^2+\lvert \mathbf{a} \rvert^2)}.
 \mathbf{v} \mathbf{a}, \mathbf{b} \mathbf{v} \alpha \alpha \neq 0",['linear-algebra']
68,matrix expression for $e^{iA}Be^{iA}$ in terms of anticommutators?,matrix expression for  in terms of anticommutators?,e^{iA}Be^{iA},"I'm familiar with the expression $$e^{-iA}Be^{iA} = \sum_{n=0}^{\infty} \frac{i^n}{n!}[..[B,A],\dots A]_{n \; \rm times}$$ for square matrices $A$ and $B$ and was wondering if equivalently $$e^{iA}Be^{iA} = \sum_{n=0}^{\infty} \frac{i^n}{n!}\{..\{B,A\},\dots A\}_{n \; \rm times},$$ or something similar?",I'm familiar with the expression for square matrices and and was wondering if equivalently or something similar?,"e^{-iA}Be^{iA} = \sum_{n=0}^{\infty} \frac{i^n}{n!}[..[B,A],\dots A]_{n \; \rm times} A B e^{iA}Be^{iA} = \sum_{n=0}^{\infty} \frac{i^n}{n!}\{..\{B,A\},\dots A\}_{n \; \rm times},","['linear-algebra', 'matrices', 'functional-analysis', 'matrix-exponential']"
69,Find the rank of $T^2$,Find the rank of,T^2,"Question: Let $\mathbb{C}^{11}$ is a vector space over $\mathbb{C}$ and $T:\mathbb{C}^{11}\to \mathbb{C}^{11}$ is a linear transformation. If dimension of Kernel $T=4$ , dimension of Kernel $T^3=9$ and dimension of Kernel $T^4=11$ . Then the dimension of Kernel $T^2=$ ............ Since $T$ is a linear operator, $T^2, T^3,T^4$ will also be linear operators and there will be matrices associated with these linear operators, say $[T]$ represents the matrix related to the linear operator $T$ . By rank-nullity theorem, we get $rank(T)+nullity(T)=dim(\mathbb{C}^{11})=11$ . So, rank $(T)=7$ and similarly,  we  can get rank $(T^3)=2$ and rank $(T^4)=0$ . Therefore, $T$ is nilpotent. Again by rank-nullity theorem, $nullity(T^2)=dim(\mathbb{C}^{11})-rank(T^2)=11-rank(T^2)$ . Now the main problem is reduced to find the rank of $T^2$ . We know that $T$ is nilpotent. Now, let $B_{11 \times 11}=T^2$ and $B^2=T^4=0$ , then the rank of $B$ can be found using this fact Matrix algebra: If $A^2=0$, Proof rank(A) $\le \frac{n}{2}$ . We get, rank $(T^2)\leq \frac{11}{2}$ . In this way we can tell possibilities of the dimension of Kernel of $T^2$ . Can we only find the possibilities not the exact rank of $T^2$ with the given data?","Question: Let is a vector space over and is a linear transformation. If dimension of Kernel , dimension of Kernel and dimension of Kernel . Then the dimension of Kernel ............ Since is a linear operator, will also be linear operators and there will be matrices associated with these linear operators, say represents the matrix related to the linear operator . By rank-nullity theorem, we get . So, rank and similarly,  we  can get rank and rank . Therefore, is nilpotent. Again by rank-nullity theorem, . Now the main problem is reduced to find the rank of . We know that is nilpotent. Now, let and , then the rank of can be found using this fact Matrix algebra: If $A^2=0$, Proof rank(A) $\le \frac{n}{2}$ . We get, rank . In this way we can tell possibilities of the dimension of Kernel of . Can we only find the possibilities not the exact rank of with the given data?","\mathbb{C}^{11} \mathbb{C} T:\mathbb{C}^{11}\to \mathbb{C}^{11} T=4 T^3=9 T^4=11 T^2= T T^2, T^3,T^4 [T] T rank(T)+nullity(T)=dim(\mathbb{C}^{11})=11 (T)=7 (T^3)=2 (T^4)=0 T nullity(T^2)=dim(\mathbb{C}^{11})-rank(T^2)=11-rank(T^2) T^2 T B_{11 \times 11}=T^2 B^2=T^4=0 B (T^2)\leq \frac{11}{2} T^2 T^2","['linear-algebra', 'matrices', 'linear-transformations', 'matrix-rank']"
70,"If $u_i\cdot u_j<0$ for all $u_0,...,u_n\in\Bbb R^n$, then the $u_i$ cannot lie in the same halfspace?","If  for all , then the  cannot lie in the same halfspace?","u_i\cdot u_j<0 u_0,...,u_n\in\Bbb R^n u_i","Given a set of $n+1$ vectors $u_0,...,u_n\in\Bbb R^n$ with pair-wise negative inner product, that is, $u_i\cdot u_j<0$ for $i\not=j$ . Question: what is a quick and clean way to see that the $u_i$ cannot all lie in the same half-space, that is, that there is no $y\in\Bbb R^n \setminus \{ 0 \}$ with $y\cdot u_i\ge 0$ for all $i\in\{0,...,n\}$ ?","Given a set of vectors with pair-wise negative inner product, that is, for . Question: what is a quick and clean way to see that the cannot all lie in the same half-space, that is, that there is no with for all ?","n+1 u_0,...,u_n\in\Bbb R^n u_i\cdot u_j<0 i\not=j u_i y\in\Bbb R^n \setminus \{ 0 \} y\cdot u_i\ge 0 i\in\{0,...,n\}","['linear-algebra', 'geometry', 'euclidean-geometry', 'inner-products', 'discrete-geometry']"
71,Matrix norms and the matrix transpose,Matrix norms and the matrix transpose,,"There are three parts to this question, and I'm not sure how they link together to provide answers. $A$ is a linear mapping from Euclidean space $X$ to Euclidean space $U$ , and the norm $\| \cdot \|$ is the Euclidean norm for matrices. (i) Show that $\|A^{T}\| = \|A\|$ . (ii) Let $v \in \mathbb{R}^{n}$ be a unit vector, and $\sigma u = Av$ , with $\sigma = \|Av\|$ . Therefore, $u\in \mathbb{R}^{m}$ is also a unit vector. Does it follow that $\sigma v = A^{T}u$ ? (iii) Now if $v$ is as above, but $\sigma = \|A\|$ . Show that $\sigma v = A^{T}u$ .","There are three parts to this question, and I'm not sure how they link together to provide answers. is a linear mapping from Euclidean space to Euclidean space , and the norm is the Euclidean norm for matrices. (i) Show that . (ii) Let be a unit vector, and , with . Therefore, is also a unit vector. Does it follow that ? (iii) Now if is as above, but . Show that .",A X U \| \cdot \| \|A^{T}\| = \|A\| v \in \mathbb{R}^{n} \sigma u = Av \sigma = \|Av\| u\in \mathbb{R}^{m} \sigma v = A^{T}u v \sigma = \|A\| \sigma v = A^{T}u,"['linear-algebra', 'transpose', 'matrix-norms']"
72,QR decompositon for singular matrices,QR decompositon for singular matrices,,"I read in textbook that every $m$ by $n$ matrix with independent columns can be factored into $A=QR$ . The columns of $Q$ are orthonormal, and $R$ is upper triangular and invertible. I don't fully understand why $R$ is invertible when $A$ has independent columns. My thought is: $A=QR\rightarrow Q^TA=R$ (multiply both sides by $Q^T$ , and $Q^TQ=I$ ) If $A$ is not square but has right inverse $A^{-1}$ , then $Q^TAA^{-1}Q=I=RA^{-1}Q$ So $A^{-1}Q$ is the inverse of $R$ . But having right inverse requires independent rows, and I don't see how independent columns are related to it. To clear my doubt, I looked at some other materials, and saw the following statement: Every invertible matrix has a $QR$ decomposition, where $R$ is invertible. As  a  side  note,  it  bears  mentioning  that  this  result  holds  even  if  the  matrix  is  not invertible: Every matrix has a $QR$ decomposition, though $R$ may not always be invertible. It confused me even more. If a matrix $A$ does not have independent columns, how will its $QR$ decomposition be like? As far as I know, $Q$ has pairwise orthogonal columns, all of unit length, and its shape is the same as $A$ . But if $A$ doesn't have independent columns in the first place, how do we find the corresponding $Q$ ? I am stuck and hope someone can help me make it clear. Any help is appreciated! Thanks in advance.","I read in textbook that every by matrix with independent columns can be factored into . The columns of are orthonormal, and is upper triangular and invertible. I don't fully understand why is invertible when has independent columns. My thought is: (multiply both sides by , and ) If is not square but has right inverse , then So is the inverse of . But having right inverse requires independent rows, and I don't see how independent columns are related to it. To clear my doubt, I looked at some other materials, and saw the following statement: Every invertible matrix has a decomposition, where is invertible. As  a  side  note,  it  bears  mentioning  that  this  result  holds  even  if  the  matrix  is  not invertible: Every matrix has a decomposition, though may not always be invertible. It confused me even more. If a matrix does not have independent columns, how will its decomposition be like? As far as I know, has pairwise orthogonal columns, all of unit length, and its shape is the same as . But if doesn't have independent columns in the first place, how do we find the corresponding ? I am stuck and hope someone can help me make it clear. Any help is appreciated! Thanks in advance.",m n A=QR Q R R A A=QR\rightarrow Q^TA=R Q^T Q^TQ=I A A^{-1} Q^TAA^{-1}Q=I=RA^{-1}Q A^{-1}Q R QR R QR R A QR Q A A Q,"['linear-algebra', 'matrices', 'inverse', 'orthogonality', 'orthonormal']"
73,How does one find the axis of rotation for a pure rotation matrix when said matrix is also symmetric?,How does one find the axis of rotation for a pure rotation matrix when said matrix is also symmetric?,,"I'm a programmer working on an advanced C++ 3D mathematics library. Now, things have been going well, in fact, basically everything about the library has been fully implemented, but one final bit of code still alludes me: finding the axis of rotation for a pure rotation matrix when said matrix is also symmetric. I've got a nice bit of math going when it comes to non-symmetric matrices Given a non-symmetric 3x3 pure rotation matrix [M]  M = { { a, b, c },       { d, e, f },       { g, h, i } }  det(M) = 1  M * T(M) = T(M) * M = I  M =/= T(M)  an eigenvector [u] which sits along the axis of rotation can be found   u = { h - f,       c - g,       d - b }  such that its normal is axis of rotation [r] of the matrix  r = u / |u| but this math breaks the moment you give it a symmetric matrix, as the 'h - f', 'c - g', and 'd - b' portions will all resolve to zero, which obviously isn't the normal vector I want. Now, I do understand linear algebra, but only a little bit. I've been researching this problem for a few days now, and while there are resources that talk about this, most of them either don't address the problem I'm having, or explain it in a way that my scrub brain simply can't keep up with. They tell me to do things like 'diagonalize M and solve for u', but I've not a clue what that actually entails doing, let alone in a generalized way, let alone (even more) teaching my C++ library to do it in a generalized way given any symmetric pure rotation matrix. So ya, that's my plight. Hoping that one of y'all could help bail me out on this one and show me how to solve this problem. :D Btw, again, this is needed for writing code, so if your answer could be written in a way that addresses that need and also the fact that I'm a linear algebra noobie, that would be awesome. Thanks in advance!","I'm a programmer working on an advanced C++ 3D mathematics library. Now, things have been going well, in fact, basically everything about the library has been fully implemented, but one final bit of code still alludes me: finding the axis of rotation for a pure rotation matrix when said matrix is also symmetric. I've got a nice bit of math going when it comes to non-symmetric matrices Given a non-symmetric 3x3 pure rotation matrix [M]  M = { { a, b, c },       { d, e, f },       { g, h, i } }  det(M) = 1  M * T(M) = T(M) * M = I  M =/= T(M)  an eigenvector [u] which sits along the axis of rotation can be found   u = { h - f,       c - g,       d - b }  such that its normal is axis of rotation [r] of the matrix  r = u / |u| but this math breaks the moment you give it a symmetric matrix, as the 'h - f', 'c - g', and 'd - b' portions will all resolve to zero, which obviously isn't the normal vector I want. Now, I do understand linear algebra, but only a little bit. I've been researching this problem for a few days now, and while there are resources that talk about this, most of them either don't address the problem I'm having, or explain it in a way that my scrub brain simply can't keep up with. They tell me to do things like 'diagonalize M and solve for u', but I've not a clue what that actually entails doing, let alone in a generalized way, let alone (even more) teaching my C++ library to do it in a generalized way given any symmetric pure rotation matrix. So ya, that's my plight. Hoping that one of y'all could help bail me out on this one and show me how to solve this problem. :D Btw, again, this is needed for writing code, so if your answer could be written in a way that addresses that need and also the fact that I'm a linear algebra noobie, that would be awesome. Thanks in advance!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
74,Embedding of $\mathrm{SU}(2)$ in $\mathrm{SU}(3)$,Embedding of  in,\mathrm{SU}(2) \mathrm{SU}(3),"There is an embedding of $\mathfrak{su}(2)$ in $\mathfrak{su}(3)$ given by sending the standard basis $$ \begin{pmatrix}i/2&0\\0&-i/2\end{pmatrix},\quad \begin{pmatrix}0&-1/2\\1/2&0\end{pmatrix},\quad\begin{pmatrix}0&-i/2\\-i/2&0\end{pmatrix} $$ of $\mathfrak{su}(2)$ to $$\begin{pmatrix}i\\&0\\&&-i\end{pmatrix},\quad\frac{1}{\sqrt{2}}\begin{pmatrix}&-1\\1&&-1\\&1\end{pmatrix},\quad-\frac{i}{\sqrt{2}}\begin{pmatrix}&1\\1&&1\\&1\end{pmatrix}. $$ in $\mathfrak{su}(3)$ . Since $\mathrm{SU}(2)$ is simply connected, this lifts to a Lie group homomorphism $$ \varphi:\mathrm{SU}(2)\longrightarrow\mathrm{SU}(3). $$ Question. Is there an explicit formula for this map? We can express any matrix in $\mathrm{SU}(2)$ as $$ \begin{pmatrix}\alpha&-\bar{\beta}\\ \beta&\bar{\alpha}\end{pmatrix} $$ where $\alpha,\beta\in\mathbb{C}$ and $|\alpha|^2+|\beta|^2=1$ . Are there explicit expressions for the coordinates of $\varphi(\begin{smallmatrix}\alpha&-\bar{\beta}\\ \beta&\bar{\alpha}\end{smallmatrix})$ in terms of $\alpha,\beta$ ? Using the exponential map, we can find special cases, such as $$ \varphi\begin{pmatrix}\alpha&0\\0&\bar{\alpha}\end{pmatrix}=\begin{pmatrix}\alpha^2\\&1\\&&\bar{\alpha}^{2}\end{pmatrix} $$ but I am having some difficulties finding the general formula.","There is an embedding of in given by sending the standard basis of to in . Since is simply connected, this lifts to a Lie group homomorphism Question. Is there an explicit formula for this map? We can express any matrix in as where and . Are there explicit expressions for the coordinates of in terms of ? Using the exponential map, we can find special cases, such as but I am having some difficulties finding the general formula.","\mathfrak{su}(2) \mathfrak{su}(3) 
\begin{pmatrix}i/2&0\\0&-i/2\end{pmatrix},\quad \begin{pmatrix}0&-1/2\\1/2&0\end{pmatrix},\quad\begin{pmatrix}0&-i/2\\-i/2&0\end{pmatrix}
 \mathfrak{su}(2) \begin{pmatrix}i\\&0\\&&-i\end{pmatrix},\quad\frac{1}{\sqrt{2}}\begin{pmatrix}&-1\\1&&-1\\&1\end{pmatrix},\quad-\frac{i}{\sqrt{2}}\begin{pmatrix}&1\\1&&1\\&1\end{pmatrix}.
 \mathfrak{su}(3) \mathrm{SU}(2) 
\varphi:\mathrm{SU}(2)\longrightarrow\mathrm{SU}(3).
 \mathrm{SU}(2) 
\begin{pmatrix}\alpha&-\bar{\beta}\\ \beta&\bar{\alpha}\end{pmatrix}
 \alpha,\beta\in\mathbb{C} |\alpha|^2+|\beta|^2=1 \varphi(\begin{smallmatrix}\alpha&-\bar{\beta}\\ \beta&\bar{\alpha}\end{smallmatrix}) \alpha,\beta 
\varphi\begin{pmatrix}\alpha&0\\0&\bar{\alpha}\end{pmatrix}=\begin{pmatrix}\alpha^2\\&1\\&&\bar{\alpha}^{2}\end{pmatrix}
","['linear-algebra', 'group-theory', 'differential-geometry', 'lie-groups', 'lie-algebras']"
75,The rank of a symmetric matrix equals the number of nonzero eigenvalues.,The rank of a symmetric matrix equals the number of nonzero eigenvalues.,,"I am wondering why the rank of a symmetric matrix equals its number of nonzero   eigenvalues. I have tried showing it like this: A symmetrix matrix A can be written: $$A=PDP^T$$ , where P is an orthogonal matrix. It is not difficult to see that for a vector x: $PDP^Tx=0 \leftrightarrow DP^Tx=0$ ,  since P is invertible. So what we need to show is that dimension of the nullspace of $DP^T$ equals the number of eigenvalues with value zero. Do you see how to do this?","I am wondering why the rank of a symmetric matrix equals its number of nonzero   eigenvalues. I have tried showing it like this: A symmetrix matrix A can be written: , where P is an orthogonal matrix. It is not difficult to see that for a vector x: ,  since P is invertible. So what we need to show is that dimension of the nullspace of equals the number of eigenvalues with value zero. Do you see how to do this?",A=PDP^T PDP^Tx=0 \leftrightarrow DP^Tx=0 DP^T,"['linear-algebra', 'matrices', 'symmetric-matrices']"
76,Why is the expectation of a trace equal to the trace of the expectation?,Why is the expectation of a trace equal to the trace of the expectation?,,Some textbooks use the property $$\mathbb{E}\left[\operatorname{tr}\left(X\right)\right]=\operatorname{tr}\left(\mathbb{E}\left[X\right]\right)$$ But why? I would really appreciate it if someone could prove this.,Some textbooks use the property But why? I would really appreciate it if someone could prove this.,\mathbb{E}\left[\operatorname{tr}\left(X\right)\right]=\operatorname{tr}\left(\mathbb{E}\left[X\right]\right),"['linear-algebra', 'statistics', 'proof-writing']"
77,$k\sum v_i v_i^T-\big(\sum v_i\big)\big(\sum v_i^T\big)\succeq 0$,,k\sum v_i v_i^T-\big(\sum v_i\big)\big(\sum v_i^T\big)\succeq 0,"My professor claimed that $$k\sum_{i=1}^k v_i v_i^T-\Big(\sum_{i=1}^k v_i\Big)\Big(\sum_{i=1}^k v_i^T\Big)\succeq 0,$$ holds for any family of vectors $\{v_1,\dots,v_k\}$ , and can be shown using the Cauchy Schwarz inequality on the quadratic form. I'm unsure whether it's necessary assume: $k$ is a positive integer, and $v_i$ are vectors of ones and zeros such that $\sum_{i=1}^k v_i=\vec{1}$ . I don't think need to assume this due to the claim that it holds for any family of vectors. In trying to prove that the above is positive semidefinite, I get the quadratic form $$\begin{align} k\sum_{i=1}^k x^T v_i v_i^T x-x^T \Big(\sum_{i=1}^k v_i\Big)\Big(\sum_{i=1}^k v_i^T \Big)x &= k\sum_{i=1}^k x^T v_i v_i^T x-|\langle \sum_{i=1}^k v_i, x\rangle|^2\\ &\geq  k\sum_{i=1}^k x^T v_i v_i^T x-\|x\|^2 \bigg\|\sum_{i=1}^k v_i\bigg\|^2\\ &\equiv k\sum_{i=1}^k x^T v_i v_i^T x-x^Tx n\\ &= x^T\big(k\sum_{i=1}^k v_i v_i^T-n\mathbb{I}\big)x\\ \end{align}$$ where $n\geq k$ . I do not think this matrix in the parentheses is positive semidefinite, since its diagonals are negative. Can someone help me prove the claim of my professor?","My professor claimed that holds for any family of vectors , and can be shown using the Cauchy Schwarz inequality on the quadratic form. I'm unsure whether it's necessary assume: is a positive integer, and are vectors of ones and zeros such that . I don't think need to assume this due to the claim that it holds for any family of vectors. In trying to prove that the above is positive semidefinite, I get the quadratic form where . I do not think this matrix in the parentheses is positive semidefinite, since its diagonals are negative. Can someone help me prove the claim of my professor?","k\sum_{i=1}^k v_i v_i^T-\Big(\sum_{i=1}^k v_i\Big)\Big(\sum_{i=1}^k v_i^T\Big)\succeq 0, \{v_1,\dots,v_k\} k v_i \sum_{i=1}^k v_i=\vec{1} \begin{align}
k\sum_{i=1}^k x^T v_i v_i^T x-x^T \Big(\sum_{i=1}^k v_i\Big)\Big(\sum_{i=1}^k v_i^T \Big)x
&=
k\sum_{i=1}^k x^T v_i v_i^T x-|\langle \sum_{i=1}^k v_i, x\rangle|^2\\
&\geq 
k\sum_{i=1}^k x^T v_i v_i^T x-\|x\|^2 \bigg\|\sum_{i=1}^k v_i\bigg\|^2\\
&\equiv
k\sum_{i=1}^k x^T v_i v_i^T x-x^Tx n\\
&=
x^T\big(k\sum_{i=1}^k v_i v_i^T-n\mathbb{I}\big)x\\
\end{align} n\geq k","['linear-algebra', 'convex-optimization', 'quadratic-forms', 'positive-semidefinite']"
78,Looking for a different type of Linear Algebra book,Looking for a different type of Linear Algebra book,,"Are there any good linear algebra books with lots of (mathematical, preferably algebraic or geometric-flavored) applications? E.g. I'm not so interested in the typical engineering-style applications or even really analysis-style (not that I would be upset by interesting ones) applications since I feel like those are very commonly covered many books, but if it contained computing homology or graph theory or combinatorics or etc... that would be awesome! I'm comparing against things like Axler, Hoffman/Kunze, Strang, Friedberg/Insel/Spence, which all seem to have very same-y treatments of linear algebra with no super exciting exercises to keep young math students excited!","Are there any good linear algebra books with lots of (mathematical, preferably algebraic or geometric-flavored) applications? E.g. I'm not so interested in the typical engineering-style applications or even really analysis-style (not that I would be upset by interesting ones) applications since I feel like those are very commonly covered many books, but if it contained computing homology or graph theory or combinatorics or etc... that would be awesome! I'm comparing against things like Axler, Hoffman/Kunze, Strang, Friedberg/Insel/Spence, which all seem to have very same-y treatments of linear algebra with no super exciting exercises to keep young math students excited!",,"['linear-algebra', 'reference-request', 'soft-question', 'book-recommendation']"
79,A coend in the category of vector spaces,A coend in the category of vector spaces,,"Let $Vect_k$ denote the category of (not necessarily finite-dimensional) $k$ -vector spaces. Clearly, this category is closed symmetric monoidal with internal hom $[X,Y]=Hom_k(X,Y)$ . Is it true that the coend $\int^{X\in Vect_k}\, [X,X]$ doesn't exist? I'm pretty sure that it doesn't, since in general there is no trace $tr_X:[X,X] \to k$ , but I want to be sure. What makes me uncertain is that I read that the category of $k$ -vector spaces is cocomplete..","Let denote the category of (not necessarily finite-dimensional) -vector spaces. Clearly, this category is closed symmetric monoidal with internal hom . Is it true that the coend doesn't exist? I'm pretty sure that it doesn't, since in general there is no trace , but I want to be sure. What makes me uncertain is that I read that the category of -vector spaces is cocomplete..","Vect_k k [X,Y]=Hom_k(X,Y) \int^{X\in Vect_k}\, [X,X] tr_X:[X,X] \to k k","['linear-algebra', 'abstract-algebra', 'category-theory', 'limits-colimits', 'monoidal-categories']"
80,"The set $H=\{(x,y)\in \Bbb{R^2}:\;3x+2y=5 \},$ is a closed subset of $\Bbb{R^2}$",The set  is a closed subset of,"H=\{(x,y)\in \Bbb{R^2}:\;3x+2y=5 \}, \Bbb{R^2}","Kindly check if my proof is correct. Thanks for your time and efforts. MY PROOF For all $n\in \Bbb{N},$ let $(x_n,y_n)\in H$ such that $(x_n,y_n)\to (x,y),$ as $n\to \infty.$ We show that $(x,y)\in H.$ However, $(x_n,y_n)\in H$ implies $3x_n+2y_n=5,$ for all $n\in \Bbb{N}.$ As $n\to \infty, $ $$ 3x+2y=\lim\limits_{n\to\infty}(3x_n+2y_n)=\lim\limits_{n\to\infty}5=5 .     $$ Hence, $(x,y)\in H$ which implies that the set, $H=\{(x,y)\in \Bbb{R^2}:\;3x+2y=5 \},$ is a closed subset of $\Bbb{R^2}$","Kindly check if my proof is correct. Thanks for your time and efforts. MY PROOF For all let such that as We show that However, implies for all As Hence, which implies that the set, is a closed subset of","n\in \Bbb{N}, (x_n,y_n)\in H (x_n,y_n)\to (x,y), n\to \infty. (x,y)\in H. (x_n,y_n)\in H 3x_n+2y_n=5, n\in \Bbb{N}. n\to \infty,   3x+2y=\lim\limits_{n\to\infty}(3x_n+2y_n)=\lim\limits_{n\to\infty}5=5 .      (x,y)\in H H=\{(x,y)\in \Bbb{R^2}:\;3x+2y=5 \}, \Bbb{R^2}","['linear-algebra', 'analysis']"
81,Matrix logarithm not in Lie algebra,Matrix logarithm not in Lie algebra,,"In Hall's Lie Groups, Lie Algebras, and Representations: An Elementary Introduction , he defines the Lie algebra of a matrix Lie group $G$ as the set $\mathfrak{g}$ of all matrices $X$ such that $e^{tX}\in G$ for all $t\in\mathbb{R}$ . Here the exponential of matrices is defined using the power series. Similarly, he defines the logarithm of a matrix $A$ using power series: \begin{equation} \log A=\sum_{m=1}^{\infty}(-1)^{m+1}\frac{(A-I)^m}{m}. \end{equation} It is known that this series converges when $\|A-I\|<1$ , where $\|\cdot\|$ is the Hilbert-Schmidt norm. Now, in Exercise 3.7, we are asked the following question: Given an $A$ in a matrix Lie group such that $\|A-I\|<1$ (so that the series above converges), is it always true that $\log A$ is in $\mathfrak{g}$ ? Prove or give a counterexample. My idea is the following: We know that the exponential map $exp:\mathfrak{g}\to G$ is a local diffeomorphism between a small neighbourhood $U$ of $0$ in $\mathfrak{g}$ and a small neighbourhood $V$ of $I$ in $G$ . However, $V$ may be very small such that it may not contain some $A$ that satisfies $\|A-I\|<1$ (that is, although $A$ is already closed to $I$ , it may still not in $V$ ). In this case, $\log A$ may not necessarily inside $\mathfrak{g}$ . But then when I tried to find some counterexamples, they are all outside the radius $1$ ball of $I$ (i.e., these examples $A$ are such that $\|A-I\|>1$ ). Thus, I am lost again. Any hint, suggestion, comment and answer are much appreciated.","In Hall's Lie Groups, Lie Algebras, and Representations: An Elementary Introduction , he defines the Lie algebra of a matrix Lie group as the set of all matrices such that for all . Here the exponential of matrices is defined using the power series. Similarly, he defines the logarithm of a matrix using power series: It is known that this series converges when , where is the Hilbert-Schmidt norm. Now, in Exercise 3.7, we are asked the following question: Given an in a matrix Lie group such that (so that the series above converges), is it always true that is in ? Prove or give a counterexample. My idea is the following: We know that the exponential map is a local diffeomorphism between a small neighbourhood of in and a small neighbourhood of in . However, may be very small such that it may not contain some that satisfies (that is, although is already closed to , it may still not in ). In this case, may not necessarily inside . But then when I tried to find some counterexamples, they are all outside the radius ball of (i.e., these examples are such that ). Thus, I am lost again. Any hint, suggestion, comment and answer are much appreciated.","G \mathfrak{g} X e^{tX}\in G t\in\mathbb{R} A \begin{equation}
\log A=\sum_{m=1}^{\infty}(-1)^{m+1}\frac{(A-I)^m}{m}.
\end{equation} \|A-I\|<1 \|\cdot\| A \|A-I\|<1 \log A \mathfrak{g} exp:\mathfrak{g}\to G U 0 \mathfrak{g} V I G V A \|A-I\|<1 A I V \log A \mathfrak{g} 1 I A \|A-I\|>1","['linear-algebra', 'matrices', 'lie-groups', 'lie-algebras']"
82,Prove: $ \operatorname{Ker}(T)^\perp= \operatorname{Im}(T^*)$,Prove:, \operatorname{Ker}(T)^\perp= \operatorname{Im}(T^*),Let $T:V\to V$ Prove: $ \operatorname{Ker}(T)^\perp= \operatorname{Im}(T^*)$ If $v\in  \operatorname{Im}(T^*)$ so  $\exists w\in V:T^*w=v$ but how can I continue from here? If $v\in  \operatorname{Ker}(T)^\perp$ what does it say?,Let $T:V\to V$ Prove: $ \operatorname{Ker}(T)^\perp= \operatorname{Im}(T^*)$ If $v\in  \operatorname{Im}(T^*)$ so  $\exists w\in V:T^*w=v$ but how can I continue from here? If $v\in  \operatorname{Ker}(T)^\perp$ what does it say?,,['linear-algebra']
83,How To Find Minimal Polynomial,How To Find Minimal Polynomial,,"$$A=\left(\begin{array}{ccccc} 4 & 1 & 0 & 0 & 0 \\ 0 & 4 & 0 & 0 & 0 \\ 0 & 0 & 4 & 0 & 0 \\ 0 & 0 & 0 & 9 & 0 \\ 0 & 0 & 0 & 0 & 9 \end{array}\right)$$ I know that the characteristic polynomial is $(\lambda-4)^3(\lambda-9)^2$ I know that the minimal polynomial can be a least $(\lambda-4)(\lambda-9)$ and $(\lambda-4)^3(\lambda-9)^2$ at most. The matrix is $\text{diagonal}(J_2(4),J_1(4),J_1(9),J_1(9))$ How can I continue?","$$A=\left(\begin{array}{ccccc} 4 & 1 & 0 & 0 & 0 \\ 0 & 4 & 0 & 0 & 0 \\ 0 & 0 & 4 & 0 & 0 \\ 0 & 0 & 0 & 9 & 0 \\ 0 & 0 & 0 & 0 & 9 \end{array}\right)$$ I know that the characteristic polynomial is $(\lambda-4)^3(\lambda-9)^2$ I know that the minimal polynomial can be a least $(\lambda-4)(\lambda-9)$ and $(\lambda-4)^3(\lambda-9)^2$ at most. The matrix is $\text{diagonal}(J_2(4),J_1(4),J_1(9),J_1(9))$ How can I continue?",,"['linear-algebra', 'matrices', 'minimal-polynomials']"
84,"$W$ be a $d$-dimensional subspace of $\mathbb R^n$. Then how to show that $|\{(x_1,...,x_n) \in W : x_i \in \{0,1\}\}| \le 2^d$?",be a -dimensional subspace of . Then how to show that ?,"W d \mathbb R^n |\{(x_1,...,x_n) \in W : x_i \in \{0,1\}\}| \le 2^d","Let $W$ be a $d$-dimensional subspace of $\mathbb R^n$. Then how to show that $|\{(x_1,...,x_n) \in W : x_i \in \{0,1\}\}| \le 2^d$ ? I can see that we are done if we can find an injective function from $\{(x_1,...,x_n) \in W : x_i \in \{0,1\}\}$ to $\mathbb F_2^d $ , but I can't quite see any such natural injection.","Let $W$ be a $d$-dimensional subspace of $\mathbb R^n$. Then how to show that $|\{(x_1,...,x_n) \in W : x_i \in \{0,1\}\}| \le 2^d$ ? I can see that we are done if we can find an injective function from $\{(x_1,...,x_n) \in W : x_i \in \{0,1\}\}$ to $\mathbb F_2^d $ , but I can't quite see any such natural injection.",,['linear-algebra']
85,Misconception on Jordan Canonical Form,Misconception on Jordan Canonical Form,,"Say we have matrix $$M= \begin{pmatrix}  2 & 0 & 1 & -3\\ 0 & 2 & 4 & 8\\ 0 & 0 & 2 & 0\\ 0 & 0 & 0 & 3 \end{pmatrix}\DeclareMathOperator{\Id}{Id}$$ It follows that $$\chi_{M}=(x-2)^{3}(x-3)$$ I found that $\ker(M-2\Id)=\{(1,0,0,0)^{T},(0,1,0,0)^{T}\}$, so dimension 2 . Similarly, we get dimension of $\ker(M-3\Id)=1$. I want to focus on $ker(M-2\Id)$: Looking at $ker(M-2Id)^{2}$, we get a basis of $\{(1,0,0,0)^{T},(0,1,0,0)^{T}, (0,0,1,0)^{T}\}$, so dimension 3 . In our notes, we have written down: Find a vector $v \in \ker(M-2\Id)^{2}$, such that $v \notin \ker(M-2\Id)$. It follows that $Mv \in \ker(M-2\Id)$ ( First question, should this not be $(M-2\Id)v \in \ker(M-2\Id)?).$ How does this help us in terms of the Jordan Form? I'm not sure how invariant subspaces fit into all of this either, other than the fact  $\ker(M-2\Id)\subset \ker(M-2\Id)^{2}$. An intuitive explanation would be of great assistance.","Say we have matrix $$M= \begin{pmatrix}  2 & 0 & 1 & -3\\ 0 & 2 & 4 & 8\\ 0 & 0 & 2 & 0\\ 0 & 0 & 0 & 3 \end{pmatrix}\DeclareMathOperator{\Id}{Id}$$ It follows that $$\chi_{M}=(x-2)^{3}(x-3)$$ I found that $\ker(M-2\Id)=\{(1,0,0,0)^{T},(0,1,0,0)^{T}\}$, so dimension 2 . Similarly, we get dimension of $\ker(M-3\Id)=1$. I want to focus on $ker(M-2\Id)$: Looking at $ker(M-2Id)^{2}$, we get a basis of $\{(1,0,0,0)^{T},(0,1,0,0)^{T}, (0,0,1,0)^{T}\}$, so dimension 3 . In our notes, we have written down: Find a vector $v \in \ker(M-2\Id)^{2}$, such that $v \notin \ker(M-2\Id)$. It follows that $Mv \in \ker(M-2\Id)$ ( First question, should this not be $(M-2\Id)v \in \ker(M-2\Id)?).$ How does this help us in terms of the Jordan Form? I'm not sure how invariant subspaces fit into all of this either, other than the fact  $\ker(M-2\Id)\subset \ker(M-2\Id)^{2}$. An intuitive explanation would be of great assistance.",,"['linear-algebra', 'jordan-normal-form']"
86,"Show that arbitrary $A$ and $A^T$ have same eigenvalue, algebraic and geometric multiplicity","Show that arbitrary  and  have same eigenvalue, algebraic and geometric multiplicity",A A^T,"Show that an arbitrary $n \times n$ matrix $A$ and its transpose $A^T$   have the same eigenvalues, algebraic multiplicity and geometric   multiplicity. I'm not sure if I did it correctly and especially how to show that they have same geometric multiplicity? same eigen values Assume $A$ and $A^T$ have same eigenvalues, then they have the same chracteristic polynomial. So we need to show that $p_A(\lambda)=\det(A-\lambda I)$ is same as $p_{A^T}(\lambda)=\det(A^T-\lambda I)$. So we have $$p_{A^T}(\lambda)=\det(A^T-\lambda I) = \det(A^T-\lambda I^T) = \det\left((A-\lambda I)^T\right) = \det(A-\lambda I)=p_A(\lambda)$$ We see their characteristic polynomials are same so their eigenvalues are same as well. same algebraic multiplicity I'm not sure if this is a correct reason proof but: Because the characteristic polynomials are same, we have that the algebraic multiplicities of the eigenvalues of $A$ nd $A^T$ sre the same. same geometric multiplicity I don't know? :/","Show that an arbitrary $n \times n$ matrix $A$ and its transpose $A^T$   have the same eigenvalues, algebraic multiplicity and geometric   multiplicity. I'm not sure if I did it correctly and especially how to show that they have same geometric multiplicity? same eigen values Assume $A$ and $A^T$ have same eigenvalues, then they have the same chracteristic polynomial. So we need to show that $p_A(\lambda)=\det(A-\lambda I)$ is same as $p_{A^T}(\lambda)=\det(A^T-\lambda I)$. So we have $$p_{A^T}(\lambda)=\det(A^T-\lambda I) = \det(A^T-\lambda I^T) = \det\left((A-\lambda I)^T\right) = \det(A-\lambda I)=p_A(\lambda)$$ We see their characteristic polynomials are same so their eigenvalues are same as well. same algebraic multiplicity I'm not sure if this is a correct reason proof but: Because the characteristic polynomials are same, we have that the algebraic multiplicities of the eigenvalues of $A$ nd $A^T$ sre the same. same geometric multiplicity I don't know? :/",,"['linear-algebra', 'matrices', 'proof-writing', 'eigenvalues-eigenvectors', 'transpose']"
87,"Proof of ""every finite dimensional vector space has a finite basis""","Proof of ""every finite dimensional vector space has a finite basis""",,"Finite dimensional implies the existence of a finite set that spans the vector space. Let V be one such vector space and let S be a finite set that spans V. The text I am following has a theorem that Theorem 1: Any minimal spanning set of V is a basis of V. Since we have a spanning set to begin with, we can keep on removing the linearly dependent vectors till we are left with a minimal spanning set which should then be a basis for that vector space. However, the text I am following has the following theorem. Theorem 2: Every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space Then the text mentions the following corollary Corollary to Theorem 2: Every finite dimensional vector space has a finite basis The proof is not given for the corollary. Is it really that straight forward? Does it involve something like the empty set of basis vectors, which by definition, is the basis of the set {0}, can be extended to a basis of V? That would then imply V has a basis. I feel that something is missing. All we know up until this point is the if a basis exists, then it is a minimal spanning set, maximal linearly independent set, and that any two sets basis vectors must have the same number of elements (which is where motivation to define dimension will start to emerge). We have not yet shown that a finite dimensional vector space has a basis and hence, we cannot assume that V has a finite basis. So my question is how can we prove Theorem 2 without referring to any finite list of basis of V? Line of proof for Theorem 2 given in the text: Let W be a subspace of V with basis vectors $\{w_1,w_2,...,w_k\}$ . Choose a vector $v_{k+1}$ from V-W. Then the set $\{w_1,w_2,...,w_k,v_{k+1}\}$ is linearly independent. Let the span of this new set be $W_1$ . Then choose any vector from V- $W_1$ , say $v_{k+2}$ , and add it to the set of linearly independent vectors to get the new set $\{w_1,w_2,...,w_k,v_{k+1},v_{k+2}\}$ . We can keep on going like this but can the process go on forever? This is where the text simply mentions that this process has to terminate because ""the vector space is finite dimensional."" To me, this is the statement that does not make sense. All we know is There is a finite set of vectors, say S, which spans V, and we know that There is a subset W of V with some basis, say $\{w_1,w_2,...,w_k\}$ . How can we use just the above facts (and maybe also some of the aforementioned theorems about basis vectors if they existed) to prove Theorem 2? I would greatly appreciate feedback to the above query.","Finite dimensional implies the existence of a finite set that spans the vector space. Let V be one such vector space and let S be a finite set that spans V. The text I am following has a theorem that Theorem 1: Any minimal spanning set of V is a basis of V. Since we have a spanning set to begin with, we can keep on removing the linearly dependent vectors till we are left with a minimal spanning set which should then be a basis for that vector space. However, the text I am following has the following theorem. Theorem 2: Every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space Then the text mentions the following corollary Corollary to Theorem 2: Every finite dimensional vector space has a finite basis The proof is not given for the corollary. Is it really that straight forward? Does it involve something like the empty set of basis vectors, which by definition, is the basis of the set {0}, can be extended to a basis of V? That would then imply V has a basis. I feel that something is missing. All we know up until this point is the if a basis exists, then it is a minimal spanning set, maximal linearly independent set, and that any two sets basis vectors must have the same number of elements (which is where motivation to define dimension will start to emerge). We have not yet shown that a finite dimensional vector space has a basis and hence, we cannot assume that V has a finite basis. So my question is how can we prove Theorem 2 without referring to any finite list of basis of V? Line of proof for Theorem 2 given in the text: Let W be a subspace of V with basis vectors . Choose a vector from V-W. Then the set is linearly independent. Let the span of this new set be . Then choose any vector from V- , say , and add it to the set of linearly independent vectors to get the new set . We can keep on going like this but can the process go on forever? This is where the text simply mentions that this process has to terminate because ""the vector space is finite dimensional."" To me, this is the statement that does not make sense. All we know is There is a finite set of vectors, say S, which spans V, and we know that There is a subset W of V with some basis, say . How can we use just the above facts (and maybe also some of the aforementioned theorems about basis vectors if they existed) to prove Theorem 2? I would greatly appreciate feedback to the above query.","\{w_1,w_2,...,w_k\} v_{k+1} \{w_1,w_2,...,w_k,v_{k+1}\} W_1 W_1 v_{k+2} \{w_1,w_2,...,w_k,v_{k+1},v_{k+2}\} \{w_1,w_2,...,w_k\}",['linear-algebra']
88,Proof or reference to the Weyl inequalities?,Proof or reference to the Weyl inequalities?,,"Does anyone know a proof or reference to the following result? Suppose that $A, B$ are both $m \times n$ real matrices. Then for all $1 \leq k \leq \min\{m, n\}$,    $$|\sigma_k(A) - \sigma_k(B)| \leq \|A - B\|.$$ I think these are called the Weyl inequalities, and I remember learning a proof of this result using the minimax characterization of these singular values but I can't reconstruct the proof. Anyone know it or have a reference to it?","Does anyone know a proof or reference to the following result? Suppose that $A, B$ are both $m \times n$ real matrices. Then for all $1 \leq k \leq \min\{m, n\}$,    $$|\sigma_k(A) - \sigma_k(B)| \leq \|A - B\|.$$ I think these are called the Weyl inequalities, and I remember learning a proof of this result using the minimax characterization of these singular values but I can't reconstruct the proof. Anyone know it or have a reference to it?",,"['linear-algebra', 'reference-request', 'singular-values']"
89,Do Real Symmetric Matrices have 'n' linearly independent eigenvectors? [duplicate],Do Real Symmetric Matrices have 'n' linearly independent eigenvectors? [duplicate],,"This question already has answers here : Eigenvectors of real symmetric matrices are orthogonal (7 answers) Closed 3 years ago . I know that Real Symmetric Matrices have real eigenvalues and the vectors corresponding to each distinct eigenvalue is orthogonal from this answer . But what if the matrix has repeated eigenvalues? Does it have linearly independent (and orthogonal) eigenvectors? How to prove that? PS: In the answer I referred to has another answer which might have answered this question. I'm not sure if it answered my question since I didn't understand it. If it did answer my question, can anyone please explain it? Thanks!","This question already has answers here : Eigenvectors of real symmetric matrices are orthogonal (7 answers) Closed 3 years ago . I know that Real Symmetric Matrices have real eigenvalues and the vectors corresponding to each distinct eigenvalue is orthogonal from this answer . But what if the matrix has repeated eigenvalues? Does it have linearly independent (and orthogonal) eigenvectors? How to prove that? PS: In the answer I referred to has another answer which might have answered this question. I'm not sure if it answered my question since I didn't understand it. If it did answer my question, can anyone please explain it? Thanks!",,"['linear-algebra', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
90,What is the use of Null Space?,What is the use of Null Space?,,"This answer explains what is null space very effectively. In brief Null Space is the set of vectors which have 0 effect on the system when applied. So, what is the use of finding null-space? Is it just that it gives us what not to use and whether the matrix is invertible or not or is there a better use for null space? May be something like, ""we know adding null-space-vectors won't change the system but improves the stability of the system?"" (I'm just guessing) Any practical examples (like the ones given in the answer referred) are greatly appreciated. Theoretical ones will also be helpful. Thanks","This answer explains what is null space very effectively. In brief Null Space is the set of vectors which have 0 effect on the system when applied. So, what is the use of finding null-space? Is it just that it gives us what not to use and whether the matrix is invertible or not or is there a better use for null space? May be something like, ""we know adding null-space-vectors won't change the system but improves the stability of the system?"" (I'm just guessing) Any practical examples (like the ones given in the answer referred) are greatly appreciated. Theoretical ones will also be helpful. Thanks",,"['linear-algebra', 'vector-spaces', 'examples-counterexamples']"
91,Minimizing matrix norm via similarity transformation,Minimizing matrix norm via similarity transformation,,"What algorithms or references can I look into in order to find a transformation $T$ that minimizes the norm of a given matrix $A$ with no particular properties? $$\min_T \| T A T^{-1} \|$$ The norm I want to minimize is the $2$-norm, but the Frobenius norm also works because it bounds (tightly) the $2$-norm. Thus, it should yield similar $T$. Matrix $A$ is real in some applications and imaginary in others. Either solution would help.","What algorithms or references can I look into in order to find a transformation $T$ that minimizes the norm of a given matrix $A$ with no particular properties? $$\min_T \| T A T^{-1} \|$$ The norm I want to minimize is the $2$-norm, but the Frobenius norm also works because it bounds (tightly) the $2$-norm. Thus, it should yield similar $T$. Matrix $A$ is real in some applications and imaginary in others. Either solution would help.",,"['linear-algebra', 'optimization', 'linear-transformations']"
92,"What are the differences between equivalence, row equivalence and similarity in matrices?","What are the differences between equivalence, row equivalence and similarity in matrices?",,"Two matrices being equivalent, row equivalent or similar are introduced in different linear Algebra text books. Each book has a different perspective and different level of difficulty. As a result, I get confused when trying to set apart each quality in terms of the definition, characteristics, ... Also, intuitively, I feel there are mathematical relationships between them. May someone help with this?","Two matrices being equivalent, row equivalent or similar are introduced in different linear Algebra text books. Each book has a different perspective and different level of difficulty. As a result, I get confused when trying to set apart each quality in terms of the definition, characteristics, ... Also, intuitively, I feel there are mathematical relationships between them. May someone help with this?",,"['linear-algebra', 'matrices']"
93,Can real non-symmetric matrices have real eigenvalues? [closed],Can real non-symmetric matrices have real eigenvalues? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question From the spectrum theorem, we know real symmetric matrices have real eigenvalues. But can real non-symmetric matrix have real eigenvalues? What are the necessary and sufficient conditions for a real matrix to have real eigenvalues?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question From the spectrum theorem, we know real symmetric matrices have real eigenvalues. But can real non-symmetric matrix have real eigenvalues? What are the necessary and sufficient conditions for a real matrix to have real eigenvalues?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
94,Ways to find the orthogonal projection matrix,Ways to find the orthogonal projection matrix,,"I'm a bit lost trying to find the projection matrix for an orthogonal projection onto a plane defined by the normal vector $n = (1, 1, 1)^T$. Then I can find the basis C of the plain $C = ( (-1,0,1)^T (0,-1,1)^T)$. Now i should be able to find the projection Matrix with $A(A^TA)^{-1}A^T$ Where $A:=\begin{bmatrix}  -1 & 0\\ 0 & -1\\ 1 & 1\end{bmatrix}$. Then my the projection matrix will look like this? $A:=\begin{bmatrix} 2/3 & -1/3 & -1/3\\ -1/3 & 2/3 & -1/3\\ -1/3 & -1/3 & 2/3\end{bmatrix}$ Is this correct? To which basis is this projection matrix? How can I change the matrix to a different basis? There should be another way to find the matrix. Something like add to my basis $C$ a vector from my basis $B$ (which should not be the standard basis) in $\mathbb{R^3}$, find the projection of the basis ( I only need to do this for the added basis vector from $B$ since the rest is already on the plane). But how can I find the projection of the added basis vector? After that what would be the matrix from basis B to B? The coefficients of the linear combinations $c_1,c_2,P(b_i)$ in B?","I'm a bit lost trying to find the projection matrix for an orthogonal projection onto a plane defined by the normal vector $n = (1, 1, 1)^T$. Then I can find the basis C of the plain $C = ( (-1,0,1)^T (0,-1,1)^T)$. Now i should be able to find the projection Matrix with $A(A^TA)^{-1}A^T$ Where $A:=\begin{bmatrix}  -1 & 0\\ 0 & -1\\ 1 & 1\end{bmatrix}$. Then my the projection matrix will look like this? $A:=\begin{bmatrix} 2/3 & -1/3 & -1/3\\ -1/3 & 2/3 & -1/3\\ -1/3 & -1/3 & 2/3\end{bmatrix}$ Is this correct? To which basis is this projection matrix? How can I change the matrix to a different basis? There should be another way to find the matrix. Something like add to my basis $C$ a vector from my basis $B$ (which should not be the standard basis) in $\mathbb{R^3}$, find the projection of the basis ( I only need to do this for the added basis vector from $B$ since the rest is already on the plane). But how can I find the projection of the added basis vector? After that what would be the matrix from basis B to B? The coefficients of the linear combinations $c_1,c_2,P(b_i)$ in B?",,"['linear-algebra', 'projection-matrices']"
95,Why is the following operator invertible,Why is the following operator invertible,,"Suppose that $T : X \rightarrow X$ is an isometry between two Banach spaces. Then if $|\lambda| < 1$, then $T^{-1} - \frac{1}{\lambda}I_X$ is invertible. Can someone explain this step as it is critical step in one of my proofs.","Suppose that $T : X \rightarrow X$ is an isometry between two Banach spaces. Then if $|\lambda| < 1$, then $T^{-1} - \frac{1}{\lambda}I_X$ is invertible. Can someone explain this step as it is critical step in one of my proofs.",,['linear-algebra']
96,Prove that idempotent operator $E$ is self-adjoint if and only if $EE^∗$ = $E^∗E$,Prove that idempotent operator  is self-adjoint if and only if  =,E EE^∗ E^∗E,"Let $V$ be a finite-dimensional inner product space, and let $E$ be an idempotent linear operator on $V$, i.e., $E^2 = E$. Prove that E is self-adjoint if and only if $EE^* = E^*E$. Are there any simpler answers to the question that the answers provided here Normal, idempotent operator implies self-adjointness. . Both answers seem to be correct but contain logical steps that I can't comprehend e.g $(I−E)Ex=0 \Rightarrow (I−E^∗)Ex=0 $ and $v^\ast E^\ast Ev=0 \Rightarrow Ev=0$","Let $V$ be a finite-dimensional inner product space, and let $E$ be an idempotent linear operator on $V$, i.e., $E^2 = E$. Prove that E is self-adjoint if and only if $EE^* = E^*E$. Are there any simpler answers to the question that the answers provided here Normal, idempotent operator implies self-adjointness. . Both answers seem to be correct but contain logical steps that I can't comprehend e.g $(I−E)Ex=0 \Rightarrow (I−E^∗)Ex=0 $ and $v^\ast E^\ast Ev=0 \Rightarrow Ev=0$",,['linear-algebra']
97,Example of element of double dual that is not an evaluation map,Example of element of double dual that is not an evaluation map,,"It's well known that if $V$ is a vector space over a field $F$, then there is a natural injection from $V$ to the double dual $V^{**}$, which associates to every $v \in V$ the evaluation map $\phi \mapsto \phi(v)$, where $\phi: V \to F$ is an arbitrary functional in $V^*$. It's also well known that this injection is an isomorphism if $V$ is finite-dimensional, as any finite-dimensional vector space has the same dimension as its dual. My question is this: are there any nice, readily understood examples of infinite -dimensional vector spaces $V$ for which an element of $V^{**}$ that is not an evaluation map can be explicitly constructed (at least with the axiom of choice)? I find infinite-dimensional double dual spaces hard even to think about.","It's well known that if $V$ is a vector space over a field $F$, then there is a natural injection from $V$ to the double dual $V^{**}$, which associates to every $v \in V$ the evaluation map $\phi \mapsto \phi(v)$, where $\phi: V \to F$ is an arbitrary functional in $V^*$. It's also well known that this injection is an isomorphism if $V$ is finite-dimensional, as any finite-dimensional vector space has the same dimension as its dual. My question is this: are there any nice, readily understood examples of infinite -dimensional vector spaces $V$ for which an element of $V^{**}$ that is not an evaluation map can be explicitly constructed (at least with the axiom of choice)? I find infinite-dimensional double dual spaces hard even to think about.",,"['linear-algebra', 'abstract-algebra', 'functional-analysis', 'examples-counterexamples', 'dual-spaces']"
98,Diagonalization of symmetric matrix,Diagonalization of symmetric matrix,,"Suppose we have a symmetric matrix $A \in \mathbb R^{n \times n}$ (for example, the matrix corresponding to a quadratic form) which we want to diagonalize. Now the usual way to do this is to find an orthonormal basis of $\mathbb R^n$ constisting of eigenvectors of $A$ (the spectral theorem always guarantees the existence of such) and the resulting matrix $Q \in O_n(\mathbb R)$ is such that $QAQ^{-1}$ is diagonal. However, sometimes the task is to diagonalize a quadratic form, but not the way I just described, but by performing simultaneous column and row transformations (we can do that due to Sylvester's inertia theorem). Now my questions: Are these procedures completely different from each other or are they doing the same thing? Why don't I use the spectral theorem to diagonalize a quadratic form or why don't I use the $2$nd procedure to diagonalize a general symmetric matrix? I suppose this has to do with the type of transformation, i.e. orthogonal or not, but I would like to have some good explanation.","Suppose we have a symmetric matrix $A \in \mathbb R^{n \times n}$ (for example, the matrix corresponding to a quadratic form) which we want to diagonalize. Now the usual way to do this is to find an orthonormal basis of $\mathbb R^n$ constisting of eigenvectors of $A$ (the spectral theorem always guarantees the existence of such) and the resulting matrix $Q \in O_n(\mathbb R)$ is such that $QAQ^{-1}$ is diagonal. However, sometimes the task is to diagonalize a quadratic form, but not the way I just described, but by performing simultaneous column and row transformations (we can do that due to Sylvester's inertia theorem). Now my questions: Are these procedures completely different from each other or are they doing the same thing? Why don't I use the spectral theorem to diagonalize a quadratic form or why don't I use the $2$nd procedure to diagonalize a general symmetric matrix? I suppose this has to do with the type of transformation, i.e. orthogonal or not, but I would like to have some good explanation.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'linear-transformations', 'orthogonality']"
99,"If $f:X \rightarrow \Bbb{K}$ is a linear functional,then $\textbf{Ker}(f)$ is a maximal subspace of $X$?","If  is a linear functional,then  is a maximal subspace of ?",f:X \rightarrow \Bbb{K} \textbf{Ker}(f) X,"Let $f$ be a linear functional on vector space $X$. $K$ is a Field which can be $\Bbb{R}$ or $\Bbb{C}$. Let $Z(f)$ denote the kernel of $f$ The Maximal subspace of $f$ : $Z$ is said to be a maximal subspace of $X$ if for any subspace $Z_{1}$ with $Z \subset Z_{1} \subset X$ then either $Z = Z_{1}$ or $X = Z_{1}$ definition 2 : $Z$ is a maximal subspace of $X$ $\textbf{iff}$ $\textbf{span}(Z \cup \{a\}) = X$ for any $a \in X\setminus Z$. For $f$ a linear functional on $X$. Now I was thinking to prove that $Z(f)$ is a maximal subspace of $X$. For proving I thought of this- Suppose $Z \neq Z_{1}$ then we need to show that $Z_{1} = X$,but how do I proceed with this? $\textbf{EDIT}:$ Also as pointed out in the comments whether $X$ is finite dimensional or infinite dimensional? So it would be interesting to see if those two case go similarly or are there any different treatment or consequences in the proof of both the cases? Any help is great.","Let $f$ be a linear functional on vector space $X$. $K$ is a Field which can be $\Bbb{R}$ or $\Bbb{C}$. Let $Z(f)$ denote the kernel of $f$ The Maximal subspace of $f$ : $Z$ is said to be a maximal subspace of $X$ if for any subspace $Z_{1}$ with $Z \subset Z_{1} \subset X$ then either $Z = Z_{1}$ or $X = Z_{1}$ definition 2 : $Z$ is a maximal subspace of $X$ $\textbf{iff}$ $\textbf{span}(Z \cup \{a\}) = X$ for any $a \in X\setminus Z$. For $f$ a linear functional on $X$. Now I was thinking to prove that $Z(f)$ is a maximal subspace of $X$. For proving I thought of this- Suppose $Z \neq Z_{1}$ then we need to show that $Z_{1} = X$,but how do I proceed with this? $\textbf{EDIT}:$ Also as pointed out in the comments whether $X$ is finite dimensional or infinite dimensional? So it would be interesting to see if those two case go similarly or are there any different treatment or consequences in the proof of both the cases? Any help is great.",,"['linear-algebra', 'functional-analysis']"
