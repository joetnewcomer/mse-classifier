,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Exterior power of dual space,Exterior power of dual space,,"Let $V$ be a vector space with basis $e_1, \ldots, e_n$ and $V^*$ be its dual space with dual basis $e_1^*, \ldots, e_n^*$. Let $k$ be an integer between $1$ and $n$. Why $\wedge^{n-k}V=\wedge^{k}V^*$? Thank you very much.","Let $V$ be a vector space with basis $e_1, \ldots, e_n$ and $V^*$ be its dual space with dual basis $e_1^*, \ldots, e_n^*$. Let $k$ be an integer between $1$ and $n$. Why $\wedge^{n-k}V=\wedge^{k}V^*$? Thank you very much.",,[]
1,Proving that the set of polynomials is closed under addition,Proving that the set of polynomials is closed under addition,,"I'm working through Axler's Linear Algebra Done Right (third edition). On p30-31 we are told: P(F) is the set of all polynomials with coefficients in F And then it is left to the reader to verify that $P(F)$ is a vector space over F. I'm having a bit of trouble showing that $P(F)$ is closed under addition. This is not a problem if I can assume that $P(F)$ is the set of all polynomials $p(x)$ with coefficients in $F$ and $x \in F$ . In other words I have no problem showing that $p_{1}(x) + p_{2}(x) \in P(F)$ . But Axler's definition of $P(F)$ states that it is ""the set of all polynomials with coefficients in F."" Does this include $p_{1}(x)$ and $p_{2}(z)$ for $x,z \in F$ ? If so then I'm having trouble showing that $p_{1}(x)+p_{2}(z) \in P(F)$ for $x,z \in F$ . I'm still wrapping my head around vector spaces of functions. Is there a convention with respect to the functions' arguments that I'm not aware of? (E.g., when discussing vector spaces of functions is it assumed that the argument of the functions is the same for all functions in that space? I.e. $p_{1}(x), p_{2}(x), p_{3}(x)...$ or can we assume the space includes $p_{1}(x), p_{2}(y), p_{3}(z)...$ ?) Any guidance here would be appreciated.","I'm working through Axler's Linear Algebra Done Right (third edition). On p30-31 we are told: P(F) is the set of all polynomials with coefficients in F And then it is left to the reader to verify that is a vector space over F. I'm having a bit of trouble showing that is closed under addition. This is not a problem if I can assume that is the set of all polynomials with coefficients in and . In other words I have no problem showing that . But Axler's definition of states that it is ""the set of all polynomials with coefficients in F."" Does this include and for ? If so then I'm having trouble showing that for . I'm still wrapping my head around vector spaces of functions. Is there a convention with respect to the functions' arguments that I'm not aware of? (E.g., when discussing vector spaces of functions is it assumed that the argument of the functions is the same for all functions in that space? I.e. or can we assume the space includes ?) Any guidance here would be appreciated.","P(F) P(F) P(F) p(x) F x \in F p_{1}(x) + p_{2}(x) \in P(F) P(F) p_{1}(x) p_{2}(z) x,z \in F p_{1}(x)+p_{2}(z) \in P(F) x,z \in F p_{1}(x), p_{2}(x), p_{3}(x)... p_{1}(x), p_{2}(y), p_{3}(z)...","['linear-algebra', 'polynomials', 'vector-spaces']"
2,Are all nilpotent matrices strictly upper triangularizable (over the complex and real fields)?,Are all nilpotent matrices strictly upper triangularizable (over the complex and real fields)?,,"According to this question, strictly upper triangular matrices are nilpotent (over any field). Does it also hold tha nilpotent matrices $N$ are strictly upper triangularizable? (i.e. they are conjugate that an strictly upper triangular matrix $N = JUJ^{-1}$ with $J$ and $U$ both complex). In the complex case, it seems trivial to me that the Jordan canonical form has zeros on its diagonal and is thus strictly upper triangular. However, I have not seen this characterization of complex nilpotent matrices so I might be missing something. This would be related to the Jordan decomposition of matrices/endomorphisms into a semisimple and nilpotent part. For the complex case, semisimple is the same as diagonalizable, and, if my intuition is correct, the nilpotent part is the same as strictly upper triangularizable. This perfectly relates to the Jordan canonical form of complex matrices, which implies that all complex matrices are triangularizable. Also, what about the real case? Does my statement about nilpotent matrices being strictly upper triangularizable still hold? The Jordan canonical form should be the same (with given signature), although they might not be conjugate via a real matrix, so I'm not sure whether it is still true. For the real case, the semisimple part is conjugate to a block decomposition where you might also have some two-dimensional blocks which are non-scalar/not proportional to the identity (as per this question), so ""kind of"" diagonalizable. Does something similar hold for real nilpotent matrices - ""kind of"" strictly upper triangularizable? Hope my questions were clear, thanks!","According to this question, strictly upper triangular matrices are nilpotent (over any field). Does it also hold tha nilpotent matrices are strictly upper triangularizable? (i.e. they are conjugate that an strictly upper triangular matrix with and both complex). In the complex case, it seems trivial to me that the Jordan canonical form has zeros on its diagonal and is thus strictly upper triangular. However, I have not seen this characterization of complex nilpotent matrices so I might be missing something. This would be related to the Jordan decomposition of matrices/endomorphisms into a semisimple and nilpotent part. For the complex case, semisimple is the same as diagonalizable, and, if my intuition is correct, the nilpotent part is the same as strictly upper triangularizable. This perfectly relates to the Jordan canonical form of complex matrices, which implies that all complex matrices are triangularizable. Also, what about the real case? Does my statement about nilpotent matrices being strictly upper triangularizable still hold? The Jordan canonical form should be the same (with given signature), although they might not be conjugate via a real matrix, so I'm not sure whether it is still true. For the real case, the semisimple part is conjugate to a block decomposition where you might also have some two-dimensional blocks which are non-scalar/not proportional to the identity (as per this question), so ""kind of"" diagonalizable. Does something similar hold for real nilpotent matrices - ""kind of"" strictly upper triangularizable? Hope my questions were clear, thanks!",N N = JUJ^{-1} J U,"['linear-algebra', 'matrices', 'complex-numbers', 'real-numbers']"
3,Is any symmetric matrix of which its diagonalvector equals its eigenvalues a diagonal matrix?,Is any symmetric matrix of which its diagonalvector equals its eigenvalues a diagonal matrix?,,"Let $A$ be an $n \times n$ a real, positive semidefinite and symmetric matrix. Suppose we know that the diagonal of this matrix is equal to its eigenvalues. (So if we have eigenvalues $\lambda_1, ... \lambda_n$ , then $diag(A)$ is a permutation of $(\lambda_1, ..., \lambda_n)$ .) Can we prove from these assumptions that $A$ is a diagonal matrix? I.e. all non-diagonal entries are $0$ ? And if so, how? It's equal to this questio n, only the counterexample given there is not symmetric. Ideas so far: Proving it the other way around is easier. If a matrix is diagonal and $n \times n$ , then its entries are equal to its eigenvalues. But this doesn't help with proving it the other way around. I know that since $A$ is real, and positive semidefinite, we can do a diagonalization by eigendecomposition to find $A = VBV^{T}$ with $V$ an orthonormal matrix and $B$ a diagonal matrix with eigenvalues on the diagonal. I've tried to prove from here that the $diag(A)$ being a permutation of $diag(B)$ must mean that $V$ is a permutation matrix, i.e. $V$ 's orthonormal rows consist of $n-1$ zeros and a single 1, but I couldn't get the math working in cases when there are two equal eigenvalues on $B$ 's diagonal.","Let be an a real, positive semidefinite and symmetric matrix. Suppose we know that the diagonal of this matrix is equal to its eigenvalues. (So if we have eigenvalues , then is a permutation of .) Can we prove from these assumptions that is a diagonal matrix? I.e. all non-diagonal entries are ? And if so, how? It's equal to this questio n, only the counterexample given there is not symmetric. Ideas so far: Proving it the other way around is easier. If a matrix is diagonal and , then its entries are equal to its eigenvalues. But this doesn't help with proving it the other way around. I know that since is real, and positive semidefinite, we can do a diagonalization by eigendecomposition to find with an orthonormal matrix and a diagonal matrix with eigenvalues on the diagonal. I've tried to prove from here that the being a permutation of must mean that is a permutation matrix, i.e. 's orthonormal rows consist of zeros and a single 1, but I couldn't get the math working in cases when there are two equal eigenvalues on 's diagonal.","A n \times n \lambda_1, ... \lambda_n diag(A) (\lambda_1, ..., \lambda_n) A 0 n \times n A A = VBV^{T} V B diag(A) diag(B) V V n-1 B",['linear-algebra']
4,Nonnegativity of the determinant of a commuting matrix,Nonnegativity of the determinant of a commuting matrix,,Let $A\in M_n(\mathbb{R})$ such that $A^2=-I_n$ and $AB=BA$ for some $B\in M_n(\mathbb{R})$ . Prove that $\det(B)\geq0$ . All the information I could extract from the relation $A^2=-I_n$ are as follows: $(a)$ $A$ is not diagonalizable. $(b)$ $\det(A)=1$ . $(c)$ $n$ must be even. Now how to conclude that $\det(B)$ is nonnegative using these $3$ informations alongwith $AB=BA$ is not clear to me. Any help is appreciated.,Let such that and for some . Prove that . All the information I could extract from the relation are as follows: is not diagonalizable. . must be even. Now how to conclude that is nonnegative using these informations alongwith is not clear to me. Any help is appreciated.,A\in M_n(\mathbb{R}) A^2=-I_n AB=BA B\in M_n(\mathbb{R}) \det(B)\geq0 A^2=-I_n (a) A (b) \det(A)=1 (c) n \det(B) 3 AB=BA,"['linear-algebra', 'matrices', 'contest-math']"
5,Polar decomposition of a general matrix,Polar decomposition of a general matrix,,"How can I calculate the polar decomposition for a general matrix? For example for this simple one: $$     \begin{pmatrix}     a & -b  \\     b & a  \\     \end{pmatrix} $$ I know how to calculate it for a matrix with numbers, via eigenvalues, eigenvectors. I have been searching for the answer on the internet for a while but I don't fully understand it.","How can I calculate the polar decomposition for a general matrix? For example for this simple one: I know how to calculate it for a matrix with numbers, via eigenvalues, eigenvectors. I have been searching for the answer on the internet for a while but I don't fully understand it.","
    \begin{pmatrix}
    a & -b  \\
    b & a  \\
    \end{pmatrix}
","['linear-algebra', 'matrix-decomposition']"
6,To find the inverse of a special kind of matrix.,To find the inverse of a special kind of matrix.,,"In a matrix analysis problem, I encountered the following special kind of matrix $$       \begin{bmatrix}     0 & 1 & a & a & a & a \\     1 & 0 & a& a& a& a \\     a& a &0 & 1& a& a \\     a& a &1 & 0 & a& a \\     a & a & a & a &0 & 1\\     a & a & a & a &1 & 0     \end{bmatrix} $$ where $a$ is a positive integer.But this matrix is not a circulant matrix. It is not in my knowledge if this is any known form. We can also search for inverses for the general form of the matrix for even order.","In a matrix analysis problem, I encountered the following special kind of matrix where is a positive integer.But this matrix is not a circulant matrix. It is not in my knowledge if this is any known form. We can also search for inverses for the general form of the matrix for even order.","  
    \begin{bmatrix}
    0 & 1 & a & a & a & a \\
    1 & 0 & a& a& a& a \\
    a& a &0 & 1& a& a \\
    a& a &1 & 0 & a& a \\
    a & a & a & a &0 & 1\\
    a & a & a & a &1 & 0
    \end{bmatrix}
 a","['linear-algebra', 'matrices', 'analysis', 'inverse', 'matrix-analysis']"
7,Basis of infinite Vector Space $\mathbb R^{\infty}$ [duplicate],Basis of infinite Vector Space  [duplicate],\mathbb R^{\infty},"This question already has answers here : Is there a constructive way to exhibit a basis for $\mathbb{R}^\mathbb{N}$? (3 answers) Closed 6 years ago . I have a question about one example in Linear Algebra. Let $\mathbb R^∞$ be the vector space of infinite sequences $(\alpha_1, \alpha_2, \alpha_3, \ldots )$ of real numbers. Scalar multiplication are defined in the natural way: the sum of $(\alpha_1 , \alpha_2 , \alpha_3 , \ldots )$ and $(\beta_1,\beta_2,\beta_3,\ldots)$ is $(\alpha_1 +\beta_1, \alpha_2 + \beta_2, \alpha_3 + \beta_3,\ldots)$ the product of $(\alpha_1,\alpha_2,\alpha_3,\ldots)$ by a scalar $\lambda$ is the sequence $(\lambda \alpha_1, \lambda\alpha_2, \lambda\alpha_3, \ldots )$ . There exists infinite linear independent set of vectors $(e_1, e_2, e_3, \ldots)$ \begin{align} e_1 &= (1, 0, 0, \ldots)\\ e_2 &= (0, 1, 0, \ldots)\\ & \,\,\,\vdots \end{align} The problem is that this set (lets call it $X$ ) is not a basis of this vector space. Because for example $v = (1, 1, 1, \ldots)$ cannot be written as a linear combination of set $X$ (Linear combination must be a finite sum). My task is to add ""some vectors"" to the set $X$ to create a basis of that vector space. If I add $v$ its not basis ( $\langle X, v\rangle \ne\mathbb R^∞$ ) Is there any proof that the process of adding vectors to set $X$ is not finite? Or is it possible to create a basis with adding vectors to $X$ ? Thanks for answers","This question already has answers here : Is there a constructive way to exhibit a basis for $\mathbb{R}^\mathbb{N}$? (3 answers) Closed 6 years ago . I have a question about one example in Linear Algebra. Let be the vector space of infinite sequences of real numbers. Scalar multiplication are defined in the natural way: the sum of and is the product of by a scalar is the sequence . There exists infinite linear independent set of vectors The problem is that this set (lets call it ) is not a basis of this vector space. Because for example cannot be written as a linear combination of set (Linear combination must be a finite sum). My task is to add ""some vectors"" to the set to create a basis of that vector space. If I add its not basis ( ) Is there any proof that the process of adding vectors to set is not finite? Or is it possible to create a basis with adding vectors to ? Thanks for answers","\mathbb R^∞ (\alpha_1, \alpha_2, \alpha_3, \ldots ) (\alpha_1 , \alpha_2 , \alpha_3 , \ldots ) (\beta_1,\beta_2,\beta_3,\ldots) (\alpha_1 +\beta_1, \alpha_2 + \beta_2, \alpha_3 + \beta_3,\ldots) (\alpha_1,\alpha_2,\alpha_3,\ldots) \lambda (\lambda \alpha_1, \lambda\alpha_2, \lambda\alpha_3, \ldots ) (e_1, e_2, e_3, \ldots) \begin{align}
e_1 &= (1, 0, 0, \ldots)\\
e_2 &= (0, 1, 0, \ldots)\\
& \,\,\,\vdots
\end{align} X v = (1, 1, 1, \ldots) X X v \langle X, v\rangle \ne\mathbb R^∞ X X","['linear-algebra', 'vector-spaces', 'axiom-of-choice']"
8,"If $N$ is nilpotent, prove that $\det(I+N)=1$","If  is nilpotent, prove that",N \det(I+N)=1,"If $N$ is nilpotent, prove that $\det(I+N)=1$. My attempt: Since $N$ is nilpotent we have $$\det(N)=0.$$ That is, there exists a non-zero vector $v$ such that $$Nv=0.$$ Now consider $$(I+N)v=Iv+Nv=Iv+0=v.$$ Thus $$(I+N)v=v.$$ That is, $1$ is an eigenvalue of $I+N$. With this, how can we say $\det(I+N)=1$?","If $N$ is nilpotent, prove that $\det(I+N)=1$. My attempt: Since $N$ is nilpotent we have $$\det(N)=0.$$ That is, there exists a non-zero vector $v$ such that $$Nv=0.$$ Now consider $$(I+N)v=Iv+Nv=Iv+0=v.$$ Thus $$(I+N)v=v.$$ That is, $1$ is an eigenvalue of $I+N$. With this, how can we say $\det(I+N)=1$?",,"['linear-algebra', 'matrices', 'determinant']"
9,Why does $\ker(T) = \{0\} \Leftrightarrow T$ is injective,Why does  is injective,\ker(T) = \{0\} \Leftrightarrow T,"Let $T$ be linear transformation from $V$ to $W$. I know how to prove the result that nullity$(T) = 0$ if and only if $T$ is an injective linear transformation. But I still don't intuitively understand why the kernel only containing the zero vector means that $T$ is injective, and vice versa. In contrast, the relation between the image of $T$ and condition of being surjective is easy to see, since in order to map to all of the elements of $W$ the image of $T$ must have the same dimension as $W$. This can intuitively be seen with a diagram of the mapping from $V$ to $W$, for example. I can't really imagine a diagram that plainly shows the injective condition. In short, what about nullity$(T) = 0$ imples that $T$ is a one-to-one function?","Let $T$ be linear transformation from $V$ to $W$. I know how to prove the result that nullity$(T) = 0$ if and only if $T$ is an injective linear transformation. But I still don't intuitively understand why the kernel only containing the zero vector means that $T$ is injective, and vice versa. In contrast, the relation between the image of $T$ and condition of being surjective is easy to see, since in order to map to all of the elements of $W$ the image of $T$ must have the same dimension as $W$. This can intuitively be seen with a diagram of the mapping from $V$ to $W$, for example. I can't really imagine a diagram that plainly shows the injective condition. In short, what about nullity$(T) = 0$ imples that $T$ is a one-to-one function?",,"['linear-algebra', 'linear-transformations']"
10,Find the inverse of a lower triangular matrix of ones,Find the inverse of a lower triangular matrix of ones,,"Find the inverse of the matrix $A=(a_{ij})\in M_n$ where $$ \begin{cases} a_{ij}=1, &i\geq j,\\ a_{ij}=0, &i<j. \end{cases} $$ The only method for finding inverses that I was taught was by finding the adjugate matrix. So $A^{-1}=\frac{1}{\det A}\operatorname{adj(A)}$ $$A=\begin{pmatrix} 1 & 0 &0 &\ldots &0\\ 1 & 1 & 0 &\ldots &0\\ 1 & 1 & 1 &\ldots &0\\ \vdots &\vdots & \vdots &\ddots & \vdots\\ 1 & 1 &1 & \ldots &1 \end{pmatrix}$$ This is a triangular matrix so $\det A=1.$ To find the adjugate I first need to find the cofactor matirx$(C)$ of $A$. $$C=\begin{pmatrix} 1&-1&0&0&\ldots &0\\ 0 & 1& -1 &0&\ldots & 0\\ 0 & 0 & 1 & -1&\ldots &0\\ 0 & 0 & 0 &1 &\ldots &0\\ \vdots &\vdots &\vdots &\vdots&\ddots & \vdots\\ 0 & 0 &0 &0 &\ldots &1\end{pmatrix}$$ $$C^T=\operatorname{adj}(A)=\begin{pmatrix} 1 & 0 & 0 & 0&\ldots &0\\ -1 & 1& 0 & 0 &\ldots &0\\ 0&-1&1&0&\ldots &0\\ 0& 0 &-1 &1 &\ldots&0\\ \vdots &\vdots &\vdots &\vdots &\ddots&\vdots\\  0&0&0&0&\ldots &1\end{pmatrix}=A^{-1}$$ Is this correct? Also, can I leave it like that or should I somehow write it more formally?","Find the inverse of the matrix $A=(a_{ij})\in M_n$ where $$ \begin{cases} a_{ij}=1, &i\geq j,\\ a_{ij}=0, &i<j. \end{cases} $$ The only method for finding inverses that I was taught was by finding the adjugate matrix. So $A^{-1}=\frac{1}{\det A}\operatorname{adj(A)}$ $$A=\begin{pmatrix} 1 & 0 &0 &\ldots &0\\ 1 & 1 & 0 &\ldots &0\\ 1 & 1 & 1 &\ldots &0\\ \vdots &\vdots & \vdots &\ddots & \vdots\\ 1 & 1 &1 & \ldots &1 \end{pmatrix}$$ This is a triangular matrix so $\det A=1.$ To find the adjugate I first need to find the cofactor matirx$(C)$ of $A$. $$C=\begin{pmatrix} 1&-1&0&0&\ldots &0\\ 0 & 1& -1 &0&\ldots & 0\\ 0 & 0 & 1 & -1&\ldots &0\\ 0 & 0 & 0 &1 &\ldots &0\\ \vdots &\vdots &\vdots &\vdots&\ddots & \vdots\\ 0 & 0 &0 &0 &\ldots &1\end{pmatrix}$$ $$C^T=\operatorname{adj}(A)=\begin{pmatrix} 1 & 0 & 0 & 0&\ldots &0\\ -1 & 1& 0 & 0 &\ldots &0\\ 0&-1&1&0&\ldots &0\\ 0& 0 &-1 &1 &\ldots&0\\ \vdots &\vdots &\vdots &\vdots &\ddots&\vdots\\  0&0&0&0&\ldots &1\end{pmatrix}=A^{-1}$$ Is this correct? Also, can I leave it like that or should I somehow write it more formally?",,"['linear-algebra', 'matrices', 'inverse']"
11,Variance of dot product,Variance of dot product,,When reading a proof I came across the following step: $$\operatorname{Var}(x^Ty) = x^T\operatorname{Var}(y)x$$ $x$ and $y$ are column vectors. How can you derive this?,When reading a proof I came across the following step: $$\operatorname{Var}(x^Ty) = x^T\operatorname{Var}(y)x$$ $x$ and $y$ are column vectors. How can you derive this?,,"['linear-algebra', 'statistics']"
12,"difference between hyperplane and plane, examples, pictures","difference between hyperplane and plane, examples, pictures",,What is the difference between these two objects? In 2D? In 3D? In 4D? It would be great if anyone can give me some examples distinguish the two concepts and pictures.,What is the difference between these two objects? In 2D? In 3D? In 4D? It would be great if anyone can give me some examples distinguish the two concepts and pictures.,,"['linear-algebra', 'geometry', 'vector-spaces']"
13,"For an orthogonal matrix $Q$, why does $QQ^T = I$? [duplicate]","For an orthogonal matrix , why does ? [duplicate]",Q QQ^T = I,"This question already has answers here : Why is inverse of orthogonal matrix is its transpose? (5 answers) Why, if a matrix $Q$ is orthogonal, then $Q^T Q = I$? [duplicate] (6 answers) Closed 2 years ago . In my linear algebra text (Strang), an orthogonal matrix is defined to be a square matrix whose columns are orthonormal. In other words, an orthogonal matrix is a matrix $Q = [q_1 \cdots q_n]$ where each $q_i$ is a unit column vector of length $n$ with $$q_i^Tq_j = \begin{cases} 0 & \text{ when }i \neq j \\ 1 & \text{ when } i = j \end{cases}.$$ My book says that an orthogonal matrix $Q$ has the properties $Q^TQ = I$ and $QQ^T = I$, from which it follows that $Q^T = Q^{-1}$. I can see how (1) follows from orthonormal columns, but I don't see why (2) is necessarily true. Can anyone provide some insight?","This question already has answers here : Why is inverse of orthogonal matrix is its transpose? (5 answers) Why, if a matrix $Q$ is orthogonal, then $Q^T Q = I$? [duplicate] (6 answers) Closed 2 years ago . In my linear algebra text (Strang), an orthogonal matrix is defined to be a square matrix whose columns are orthonormal. In other words, an orthogonal matrix is a matrix $Q = [q_1 \cdots q_n]$ where each $q_i$ is a unit column vector of length $n$ with $$q_i^Tq_j = \begin{cases} 0 & \text{ when }i \neq j \\ 1 & \text{ when } i = j \end{cases}.$$ My book says that an orthogonal matrix $Q$ has the properties $Q^TQ = I$ and $QQ^T = I$, from which it follows that $Q^T = Q^{-1}$. I can see how (1) follows from orthonormal columns, but I don't see why (2) is necessarily true. Can anyone provide some insight?",,"['linear-algebra', 'matrices', 'orthogonality', 'orthonormal', 'orthogonal-matrices']"
14,"Prove that, at least one of the matrices $A+B$ and $A-B$ has to be singular","Prove that, at least one of the matrices  and  has to be singular",A+B A-B,"Problem: Let $A$ and $B$ be real orthogonal matrices, $n$x$n$, where $n$ is an odd number. Prove that, at least one of the matrices $A+B$ and $A-B$ has to be singular. What have I done so far: -Since  matrices $A$ and $B$ are real orthogonal, it means that their determinants are $-1$ or $+1$ First I observed matrix $A+B$ : $$A+B=AI+BI=ABB^T+BAA^T=AB(B^T+A^T)=AB(A+B)^T\Rightarrow$$ $$det(A+B)=det(AB)det(A+B)^T=det(A)det(B)det(A+B)$$ So, if $detA=detB$ we have $det(A+B)=det(A+B)$ which tells me nothing.  But, if  $detA=-detB$ we have $det(A+B)=-det(A+B)$ which can only hapen if $det(A+B)=0$ making $A+B$ singular. Then, I tried the same for $A-B$ : $$A-B=AI-BI=ABB^T-BAA^T=AB(B^T-A^T)=AB(A-B)^T\Rightarrow$$ $$det(A-B)=det(AB)det(A-B)^T=det(A)det(B)det(A-B)$$ So, if $detA=-detB$ we have $det(A-B)=-det(A-B)$ which can happen if $det(A-B)=0$ making $A-B$ singular. Is this correct or should I do matrix $A-B$ differently? I am also a little confused why does it have to be said that $n$ is an odd number? What should that tell me? Any help is greatly appreciated.","Problem: Let $A$ and $B$ be real orthogonal matrices, $n$x$n$, where $n$ is an odd number. Prove that, at least one of the matrices $A+B$ and $A-B$ has to be singular. What have I done so far: -Since  matrices $A$ and $B$ are real orthogonal, it means that their determinants are $-1$ or $+1$ First I observed matrix $A+B$ : $$A+B=AI+BI=ABB^T+BAA^T=AB(B^T+A^T)=AB(A+B)^T\Rightarrow$$ $$det(A+B)=det(AB)det(A+B)^T=det(A)det(B)det(A+B)$$ So, if $detA=detB$ we have $det(A+B)=det(A+B)$ which tells me nothing.  But, if  $detA=-detB$ we have $det(A+B)=-det(A+B)$ which can only hapen if $det(A+B)=0$ making $A+B$ singular. Then, I tried the same for $A-B$ : $$A-B=AI-BI=ABB^T-BAA^T=AB(B^T-A^T)=AB(A-B)^T\Rightarrow$$ $$det(A-B)=det(AB)det(A-B)^T=det(A)det(B)det(A-B)$$ So, if $detA=-detB$ we have $det(A-B)=-det(A-B)$ which can happen if $det(A-B)=0$ making $A-B$ singular. Is this correct or should I do matrix $A-B$ differently? I am also a little confused why does it have to be said that $n$ is an odd number? What should that tell me? Any help is greatly appreciated.",,"['linear-algebra', 'matrices']"
15,"Any two Singer cyclic subgroups of GL(n,q) are conjugate","Any two Singer cyclic subgroups of GL(n,q) are conjugate",,"Cyclic subgroups of $\operatorname{GL}(n,q)$ of order $q^n - 1$ are called Singer cyclic subgroups . The following statement seems to be well-known: Any two Singer cyclic subgroups of $\operatorname{GL}(n,q)$ are conjugate. I don't know how to prove this in general. Sometimes, $q^n - 1$ is prime such that Sylow can be used (Example: $q=2, n=3$). Some more background: Elements of order $q^n - 1$ (and thus Singer cyclic subgroups) always exist: Let $\alpha$ be a primitive element of $\operatorname{GF}(q^n)$ and look at the $\operatorname{GF}(q)$-linear map $\operatorname{GF}(q^n) \to \operatorname{GF}(q^n)$ given by $x \mapsto \alpha x$. $q^n - 1$ is the largest element order in $\operatorname{GL}(n,q)$, as by Cayley-Hamilton, the $\operatorname{GF}(q)$ vector space spanned by $I,A,A^2,A^3,\ldots$ has at most dimension $n$. The elements of order $q^n - 1$ are not necessarily conjugate. For example, in $\operatorname{GL}(3,2)$ there are two conjugacy classes of elements of order $7$. However, the generated Singer cyclic subgroups are conjugate.","Cyclic subgroups of $\operatorname{GL}(n,q)$ of order $q^n - 1$ are called Singer cyclic subgroups . The following statement seems to be well-known: Any two Singer cyclic subgroups of $\operatorname{GL}(n,q)$ are conjugate. I don't know how to prove this in general. Sometimes, $q^n - 1$ is prime such that Sylow can be used (Example: $q=2, n=3$). Some more background: Elements of order $q^n - 1$ (and thus Singer cyclic subgroups) always exist: Let $\alpha$ be a primitive element of $\operatorname{GF}(q^n)$ and look at the $\operatorname{GF}(q)$-linear map $\operatorname{GF}(q^n) \to \operatorname{GF}(q^n)$ given by $x \mapsto \alpha x$. $q^n - 1$ is the largest element order in $\operatorname{GL}(n,q)$, as by Cayley-Hamilton, the $\operatorname{GF}(q)$ vector space spanned by $I,A,A^2,A^3,\ldots$ has at most dimension $n$. The elements of order $q^n - 1$ are not necessarily conjugate. For example, in $\operatorname{GL}(3,2)$ there are two conjugacy classes of elements of order $7$. However, the generated Singer cyclic subgroups are conjugate.",,"['linear-algebra', 'matrices', 'group-theory', 'finite-groups', 'finite-fields']"
16,Anybody knows a proof of Uniqueness of the Reduced Echelon Form Theorem?,Anybody knows a proof of Uniqueness of the Reduced Echelon Form Theorem?,,"The book has no proof showing each matrix is row equivalent to one and only one reduced echelon matrix. Does anybody know how to prove this theorem? ""Theorem  Uniqueness of the Reduced Echelon Form Each matrix is row equivalent to one and only one reduced echelon matrix"" Source: Linear Algebra and Its Applications, David, C. Lay. [EDIT I think the following can be a proof that each echelon matrix is reduced to only  one reduced echelon matrix, but how to show a matrix that is not in echelon form is reduced to only one reduced echelon matrix?] In a $m×n$ matrix in echelon form of a linear system for some positive integers m, n, let the leading entries $(■)$ have any nonzero value, and the starred entries $(☆)$ have any value including zero. Leading entries $■$s in $R_1$ and $R_2$ in an echelon matrix can become leading 1 in a reduced echelon matrix through dividing them by $■$, and the entry ☆ in $R_1$ above $■$ in $R_2$ can be $0$ by subtracting a multiple of $■$. So $R_1$ and $R_2$ in a matrix in echelon form becomes as follows: $\begin{array}{rcl}  R_1\space & [■ ☆\cdots ☆☆☆☆]\\ R_2\space & [0 ■\cdots ☆☆☆☆]\end{array} \qquad ~ \begin{array}{rcl} R_1\space & [1 0\cdots ☆☆☆☆]\\R_2 &[0 1\cdots ☆☆☆☆]  \end{array}$ For all integers k with $2≤k<m$, $R_k$, $R_{k+1}$ in the echelon matrix can be expressed as $R_{k}\space$ $[0 \cdots 0 ■☆☆\cdots ☆]$ $R_{k+1}$     $[0 \cdots 0 0 ■☆\cdots ☆]$. Subtracting a multiple of leading entry of $R_{k+1}$ from  $R_k$ can make the entry above leading $■$ in $R_{k+1}$ be zero, and the leading $■$s in $R_k$, $R_{k+1}$ can be 1 through dividing the rows by leading entry $■$s. So the rows in echelon matrix become the following in reduced $m×n$ echelon matrix: $\begin{array}{rcl}  R_{k}\space & [0 \cdots 0 ■☆☆\cdots ☆]\\   R_{k+1} &    [0 \cdots 0 0 ■☆\cdots ☆]\\  \end{array} \qquad  \begin{array}{rcl}  R_{k} & [0 \cdots 0 1 0 ☆\cdots ☆]\\ R_{k+1} & [0 \cdots 0 0 1 ☆\cdots ☆]\\  \end{array}$ Hence, it's found that leading 1s in reduced echelon form of $m×n$ matrix of a linear system correspond to the locations of the leading non-zero values in a $m×n$ matrix in echelon form of the linear system.","The book has no proof showing each matrix is row equivalent to one and only one reduced echelon matrix. Does anybody know how to prove this theorem? ""Theorem  Uniqueness of the Reduced Echelon Form Each matrix is row equivalent to one and only one reduced echelon matrix"" Source: Linear Algebra and Its Applications, David, C. Lay. [EDIT I think the following can be a proof that each echelon matrix is reduced to only  one reduced echelon matrix, but how to show a matrix that is not in echelon form is reduced to only one reduced echelon matrix?] In a $m×n$ matrix in echelon form of a linear system for some positive integers m, n, let the leading entries $(■)$ have any nonzero value, and the starred entries $(☆)$ have any value including zero. Leading entries $■$s in $R_1$ and $R_2$ in an echelon matrix can become leading 1 in a reduced echelon matrix through dividing them by $■$, and the entry ☆ in $R_1$ above $■$ in $R_2$ can be $0$ by subtracting a multiple of $■$. So $R_1$ and $R_2$ in a matrix in echelon form becomes as follows: $\begin{array}{rcl}  R_1\space & [■ ☆\cdots ☆☆☆☆]\\ R_2\space & [0 ■\cdots ☆☆☆☆]\end{array} \qquad ~ \begin{array}{rcl} R_1\space & [1 0\cdots ☆☆☆☆]\\R_2 &[0 1\cdots ☆☆☆☆]  \end{array}$ For all integers k with $2≤k<m$, $R_k$, $R_{k+1}$ in the echelon matrix can be expressed as $R_{k}\space$ $[0 \cdots 0 ■☆☆\cdots ☆]$ $R_{k+1}$     $[0 \cdots 0 0 ■☆\cdots ☆]$. Subtracting a multiple of leading entry of $R_{k+1}$ from  $R_k$ can make the entry above leading $■$ in $R_{k+1}$ be zero, and the leading $■$s in $R_k$, $R_{k+1}$ can be 1 through dividing the rows by leading entry $■$s. So the rows in echelon matrix become the following in reduced $m×n$ echelon matrix: $\begin{array}{rcl}  R_{k}\space & [0 \cdots 0 ■☆☆\cdots ☆]\\   R_{k+1} &    [0 \cdots 0 0 ■☆\cdots ☆]\\  \end{array} \qquad  \begin{array}{rcl}  R_{k} & [0 \cdots 0 1 0 ☆\cdots ☆]\\ R_{k+1} & [0 \cdots 0 0 1 ☆\cdots ☆]\\  \end{array}$ Hence, it's found that leading 1s in reduced echelon form of $m×n$ matrix of a linear system correspond to the locations of the leading non-zero values in a $m×n$ matrix in echelon form of the linear system.",,"['linear-algebra', 'matrices']"
17,What does echelon mean?,What does echelon mean?,,"When you solve a system of linear equations, you write down the augmented matrix and reduce this to reduced row echelon form. What is the meaning of the word echelon?","When you solve a system of linear equations, you write down the augmented matrix and reduce this to reduced row echelon form. What is the meaning of the word echelon?",,"['linear-algebra', 'matrices', 'terminology']"
18,Gradient of the TV norm of an image,Gradient of the TV norm of an image,,"Context: I am trying to implement an algorithm for X-ray image reconstruction called ADS-POCS that minimizes the TV norm as well as reconstructs the image. After separating the reconstruction into 2 steps, namely data reconstruction and TV norm minimization, the second part is solved by a steepest descend. The image is a 3D image (relevant for the TV-norm). Problem: The paper defines $\vec{f}$ as a $1\times N_i$ vector of voxels. Later, defines the operator $\nabla_{\vec{f}}$ as $\nabla_{\vec{f}} Q(\vec f)=\sum_{i} \frac{\partial}{\partial f_i} Q(\vec f) \vec \delta_i$, being $\delta_i$ 1 in the $i$ voxel and  0 elsewhere. eventually, the algorithm to minimize the TV norm of the image is defined as a gradient descend loop where the update is defined as: $\vec f   =\vec f  -\alpha \cdot \nabla_{\vec{f}} ||\vec f ||_{TV}$ My problem is that I dont know how to compute the $\nabla_{\vec{f}} ||\vec f ||_{TV}$ term. I know how would I compute $||\vec f ||_{TV}$, but I feel like the $\nabla_{\vec{f}}$ is actually derivation the norm itself. If this were the 2-norm , for example,  I'd know how to derive it (e.g. see here ), but the TV-norm has the absolute value function, and its also dependent in neighboring voxels, while the 2-norm isn't. The TV norm can be written as (if I'm not wrong with my maths notation) $||\vec f||_{TV} =\sum_i ||\nabla \vec f_i||_{2}$ Being my mayor in ElecEng, I feel like there are some maths here that I'm missing in order to understand and code this""gradient of the TV-norm"" operator. So, how can I compute that term? How can I get the gradient of the TV-norm? Disclaimer: due to my little math knowledge, I am unaware if this is a too specific problem or it has a more generic mathematical explanation. If the question is too specific to help anyone else, please inform me and Ill delete/edit my question. The paper: Image reconstruction in circular cone-beam computed tomography by constrained, total-variation minimization Emil Y Sidky and Xiaochuan Pan","Context: I am trying to implement an algorithm for X-ray image reconstruction called ADS-POCS that minimizes the TV norm as well as reconstructs the image. After separating the reconstruction into 2 steps, namely data reconstruction and TV norm minimization, the second part is solved by a steepest descend. The image is a 3D image (relevant for the TV-norm). Problem: The paper defines $\vec{f}$ as a $1\times N_i$ vector of voxels. Later, defines the operator $\nabla_{\vec{f}}$ as $\nabla_{\vec{f}} Q(\vec f)=\sum_{i} \frac{\partial}{\partial f_i} Q(\vec f) \vec \delta_i$, being $\delta_i$ 1 in the $i$ voxel and  0 elsewhere. eventually, the algorithm to minimize the TV norm of the image is defined as a gradient descend loop where the update is defined as: $\vec f   =\vec f  -\alpha \cdot \nabla_{\vec{f}} ||\vec f ||_{TV}$ My problem is that I dont know how to compute the $\nabla_{\vec{f}} ||\vec f ||_{TV}$ term. I know how would I compute $||\vec f ||_{TV}$, but I feel like the $\nabla_{\vec{f}}$ is actually derivation the norm itself. If this were the 2-norm , for example,  I'd know how to derive it (e.g. see here ), but the TV-norm has the absolute value function, and its also dependent in neighboring voxels, while the 2-norm isn't. The TV norm can be written as (if I'm not wrong with my maths notation) $||\vec f||_{TV} =\sum_i ||\nabla \vec f_i||_{2}$ Being my mayor in ElecEng, I feel like there are some maths here that I'm missing in order to understand and code this""gradient of the TV-norm"" operator. So, how can I compute that term? How can I get the gradient of the TV-norm? Disclaimer: due to my little math knowledge, I am unaware if this is a too specific problem or it has a more generic mathematical explanation. If the question is too specific to help anyone else, please inform me and Ill delete/edit my question. The paper: Image reconstruction in circular cone-beam computed tomography by constrained, total-variation minimization Emil Y Sidky and Xiaochuan Pan",,"['linear-algebra', 'normed-spaces']"
19,What does vector mean in Linear Algebra?,What does vector mean in Linear Algebra?,,"I have just started reading Linear Algebra and there are some basic things I cannot understand. I read some answers on this site and also tried to search in some books but I didn't find a clear answer. Here are few words form my textbook : 1. Here by VECTOR we do not mean the vector quantity which we have defined in vector algebra as a directed line segment. 2. Matrices having a single row or column are referred to as vectors. 3. I also watched a video in which at approximately 3:55 he says that a point in two dimensional real coordinate space is written in matrix form in LINEAR ALGEBRA. Now I'm confused! Since in the video it seems as a vector in LINEAR ALGEBRA is just same as a point in 1, 2, 3...n real coordinate spaces. But in my text book its written that vector is not the vector quantity! And I also read in a book in which it was written that we are using LINEAR word instead of VECTOR to avoid confusion! So is it really the difference in words? Why its not written clearly what the vector really is.","I have just started reading Linear Algebra and there are some basic things I cannot understand. I read some answers on this site and also tried to search in some books but I didn't find a clear answer. Here are few words form my textbook : 1. Here by VECTOR we do not mean the vector quantity which we have defined in vector algebra as a directed line segment. 2. Matrices having a single row or column are referred to as vectors. 3. I also watched a video in which at approximately 3:55 he says that a point in two dimensional real coordinate space is written in matrix form in LINEAR ALGEBRA. Now I'm confused! Since in the video it seems as a vector in LINEAR ALGEBRA is just same as a point in 1, 2, 3...n real coordinate spaces. But in my text book its written that vector is not the vector quantity! And I also read in a book in which it was written that we are using LINEAR word instead of VECTOR to avoid confusion! So is it really the difference in words? Why its not written clearly what the vector really is.",,"['linear-algebra', 'vector-spaces']"
20,SVM : Why can we set 1 in the hyperplane equation?,SVM : Why can we set 1 in the hyperplane equation?,,"I am reading the Wikipedia article about SVM and there is something I don't understand. When they say: These hyperplanes can be described by the equations $$ wx - b=1 $$  and $$wx - b=-1$$ I was wondering where does the +1 and -1 come from? I found two papers which explain that this is an arbitrary choice: We can write the following equations for the support hyperplanes: $$w^T x  = b + \delta    $$     $$w^T x = b − \delta   $$    We now note that we have over-parameterized the problem:    if we scale w, b and $\delta$ by a constant factor $\alpha$,    the equations for x are still satisfied. To remove this  ambiguity we will require that $\delta$ = 1, this sets the scale of the  problem , i.e. if we measure distance in millimeters or meters Source but I don't understand what he means when he says ""this sets the scale of the problem"" and Note that if the equation   $f(x) = wx + b$ defines a discriminant function (so that the output is > $sgn(f(x))$), then   the hyperplane $cwx + cb$ defines the same discriminant function for any $c > 0$. Thus we have the freedom to choose the scaling of $w$ so that $min_{x_i} |wx_i + b| = 1$. Source but I don't understand why he introduces  $min_{x_i} |wx_i + b| = 1$. My understanding is that we can do it because variables $w$, $b$ and $\delta$ are kind of linked together. Can we change the Wikipedia definition and say These hyperplanes can be described by the equations  $$ wx - b= 2 $$    and   $$wx - b=- 2$$ or is this incorrect and so we must say that : These hyperplanes can be described by the equations  $$ 2wx - 2b= 2 $$    and   $$2wx - 2b=- 2$$ Could you clarify this for me?","I am reading the Wikipedia article about SVM and there is something I don't understand. When they say: These hyperplanes can be described by the equations $$ wx - b=1 $$  and $$wx - b=-1$$ I was wondering where does the +1 and -1 come from? I found two papers which explain that this is an arbitrary choice: We can write the following equations for the support hyperplanes: $$w^T x  = b + \delta    $$     $$w^T x = b − \delta   $$    We now note that we have over-parameterized the problem:    if we scale w, b and $\delta$ by a constant factor $\alpha$,    the equations for x are still satisfied. To remove this  ambiguity we will require that $\delta$ = 1, this sets the scale of the  problem , i.e. if we measure distance in millimeters or meters Source but I don't understand what he means when he says ""this sets the scale of the problem"" and Note that if the equation   $f(x) = wx + b$ defines a discriminant function (so that the output is > $sgn(f(x))$), then   the hyperplane $cwx + cb$ defines the same discriminant function for any $c > 0$. Thus we have the freedom to choose the scaling of $w$ so that $min_{x_i} |wx_i + b| = 1$. Source but I don't understand why he introduces  $min_{x_i} |wx_i + b| = 1$. My understanding is that we can do it because variables $w$, $b$ and $\delta$ are kind of linked together. Can we change the Wikipedia definition and say These hyperplanes can be described by the equations  $$ wx - b= 2 $$    and   $$wx - b=- 2$$ or is this incorrect and so we must say that : These hyperplanes can be described by the equations  $$ 2wx - 2b= 2 $$    and   $$2wx - 2b=- 2$$ Could you clarify this for me?",,"['linear-algebra', 'geometry', 'vectors']"
21,Is there a term for two subsets whose intersection is {0}?,Is there a term for two subsets whose intersection is {0}?,,"In linear algebra I've just recently learned about the ""direct sum"", which can be defined as the sum of two vector spaces whose intersection is the null vector. Basically, I'm wondering if there's a formal term in abstract algebra to define algebraic substructures whose intersection is the identity element, as it seems like it'd be a, in the case of vector spaces, useful way to differentiate between subspaces that can be directly summed and subspaces that can't be. I'm specifically asking this because algebraic substructures whose intersection is the identity element seem like they could have some interesting interacting properties that differ from the interacting properties of any two substructures (and that the term disjoint wouldn't work in the case of any algebraic substructures).","In linear algebra I've just recently learned about the ""direct sum"", which can be defined as the sum of two vector spaces whose intersection is the null vector. Basically, I'm wondering if there's a formal term in abstract algebra to define algebraic substructures whose intersection is the identity element, as it seems like it'd be a, in the case of vector spaces, useful way to differentiate between subspaces that can be directly summed and subspaces that can't be. I'm specifically asking this because algebraic substructures whose intersection is the identity element seem like they could have some interesting interacting properties that differ from the interacting properties of any two substructures (and that the term disjoint wouldn't work in the case of any algebraic substructures).",,"['linear-algebra', 'vector-spaces', 'terminology']"
22,Prove that $\det(I-CD)=\det(I-DC) $,Prove that,\det(I-CD)=\det(I-DC) ,"Let $C$ and $D$ be matrices such that $DC$ and $CD$ are square matrices of the same dimension. How can one prove that $\det(I-CD)=\det(I-DC)$ ? This is my approach to the question. I am not sure whether it is a correct one. Let $ x $ be the eigenvalue of the matrix CD with the eigenvector $\textbf{v}$ . Then $CD\textbf{v}=x\textbf{v}$ . Now let $\textbf{y} = D\textbf{v}$ . Then $ DC\textbf{y}=DCD\textbf{v}=xD\textbf{v} =x\textbf{y}$ . This shows that $\textbf{y}$ is the eigenvector of $DC$ with the eigenvalue $x$ . Now $DC\textbf{y}=xD\textbf{v}$ is non-zero if and only if $x$ and $D\textbf{v}$ are non-zero. This implies that $DC$ and $CD$ have the same non-zero eigenvalues, which also means that $\phi (CD, x)= \phi (DC, x) $ provided $x \neq 0$ . Considering the case where $x=1$ we have the required results, because $ \det(I -CD) =\phi (CD, 1)= \phi (DC, 1) = \det(I -DC)$ .","Let and be matrices such that and are square matrices of the same dimension. How can one prove that ? This is my approach to the question. I am not sure whether it is a correct one. Let be the eigenvalue of the matrix CD with the eigenvector . Then . Now let . Then . This shows that is the eigenvector of with the eigenvalue . Now is non-zero if and only if and are non-zero. This implies that and have the same non-zero eigenvalues, which also means that provided . Considering the case where we have the required results, because .","C D DC CD \det(I-CD)=\det(I-DC)  x  \textbf{v} CD\textbf{v}=x\textbf{v} \textbf{y} = D\textbf{v}  DC\textbf{y}=DCD\textbf{v}=xD\textbf{v} =x\textbf{y} \textbf{y} DC x DC\textbf{y}=xD\textbf{v} x D\textbf{v} DC CD \phi (CD, x)= \phi (DC, x)  x \neq 0 x=1  \det(I -CD) =\phi (CD, 1)= \phi (DC, 1) = \det(I -DC)","['linear-algebra', 'proof-verification']"
23,Matrix product notation,Matrix product notation,,My lecturer has used some notation that I've never seen before: it is a (matrix) product symbol with a left-to-right arrow over the top. Does anybody know what this means? Thanks in advance. Edit: It looks like this:,My lecturer has used some notation that I've never seen before: it is a (matrix) product symbol with a left-to-right arrow over the top. Does anybody know what this means? Thanks in advance. Edit: It looks like this:,,"['linear-algebra', 'matrices', 'notation', 'products']"
24,To show that $\operatorname{Rank}(\mathbf{A}-\mathbf{I})=\operatorname{Nullity}(\mathbf{A})$,To show that,\operatorname{Rank}(\mathbf{A}-\mathbf{I})=\operatorname{Nullity}(\mathbf{A}),"Problem is: Let $\mathbf{A}$ be $n\times n$ matrix with real entries such that $\mathbf{A}^{2} = \mathbf{A}$. If $\mathbf{I}$ denotes the identity matrix, then how do I prove the result: $$\operatorname{Rank}(\mathbf{A}-\mathbf{I})=\operatorname{Nullity}(\mathbf{A})$$ I have trivially shown by taking $\mathbf{A}=\mathbf{I}$, but in general I do not know how to prove this, thanks for help.","Problem is: Let $\mathbf{A}$ be $n\times n$ matrix with real entries such that $\mathbf{A}^{2} = \mathbf{A}$. If $\mathbf{I}$ denotes the identity matrix, then how do I prove the result: $$\operatorname{Rank}(\mathbf{A}-\mathbf{I})=\operatorname{Nullity}(\mathbf{A})$$ I have trivially shown by taking $\mathbf{A}=\mathbf{I}$, but in general I do not know how to prove this, thanks for help.",,['linear-algebra']
25,Exponential of matrix with negative entries only on the diagonal,Exponential of matrix with negative entries only on the diagonal,,"Suppose I have a matrix $A$ with real entries such that the off-diagonal entries of $A$ are positive or zero. (The diagonal entries may be positive, negative or zero.) From doing a few examples in Python, it looks like the following might be true of the matrix exponential $e^A$: The entries of $e^A$ are all real and non-negative (both on and off the diagonal), and If an entry of $A$ is non-zero, the corresponding entry of $e^A$ will be positive. (For zero entries of $A$, the corresponding entry in $e^A$ might be zero or positive.) Are these things indeed the case? How can this be shown? Is there a result that will allow me to predict which elements of $e^A$ will be positive, depending on which elements of $A$ are non-zero?","Suppose I have a matrix $A$ with real entries such that the off-diagonal entries of $A$ are positive or zero. (The diagonal entries may be positive, negative or zero.) From doing a few examples in Python, it looks like the following might be true of the matrix exponential $e^A$: The entries of $e^A$ are all real and non-negative (both on and off the diagonal), and If an entry of $A$ is non-zero, the corresponding entry of $e^A$ will be positive. (For zero entries of $A$, the corresponding entry in $e^A$ might be zero or positive.) Are these things indeed the case? How can this be shown? Is there a result that will allow me to predict which elements of $e^A$ will be positive, depending on which elements of $A$ are non-zero?",,['linear-algebra']
26,What's the proof stategy for: Hermitian matrix has orthogonal eigenvectors for distinct eigenvalues?,What's the proof stategy for: Hermitian matrix has orthogonal eigenvectors for distinct eigenvalues?,,"Linear Algebra (2015 5 ed) by Lay, p. 397. Theorem 7.1 involves only real numbers. Let: $A^* =  \bar{A}^T $ . $v_i$ and $v_j$ be two eigenvectors of an Hermitian matrix H. Suppose that their respective eigenvalues i and j are different, i.e. $\lambda_i \neq \lambda_j$ . This means $Hv_i = \lambda_iv_i$ and $Hv_j  = \lambda_jv_j\ \  ...(3)$ . Take the Hermitian conjugate of $Hv_i = \lambda_iv_i$ : $\begin{align} (Hv_i)^* & = (\lambda_iv_i)^* \\ \implies v_i^* H^* & = v_i^* \lambda_i^* \\ \implies v_i^* H & = v_i^* \lambda_i \ \ (as\ H\ is\ hermitian)\end{align}$ Right-multiply the previous equation by $\color{green}{v_j}: \qquad  v_i^*H \color{green}{v_j} = v_i^* \lambda_i \color{green}{v_j} \qquad ...(4)$ Left-multiply (3) by $\color{orangered}{v_i^*} : \quad  \color{orangered}{v_i^*}Hv_j = \color{orangered}{v_i^*} \lambda_jv_j  \qquad ...(5)$ . Equate the RHS of (4) and (5): $\quad  v_i^* \lambda_i  \color{green}{v_j}  = \color{orangered}{v_i^*} \lambda_jv_j \qquad  \blacksquare$ . I understand, and ask not about, the algebra. What's the proof strategy? For example, how can you divine when to take the Hermitian conjugate, what to multiply, and when to left- or right-multiply?","Linear Algebra (2015 5 ed) by Lay, p. 397. Theorem 7.1 involves only real numbers. Let: . and be two eigenvectors of an Hermitian matrix H. Suppose that their respective eigenvalues i and j are different, i.e. . This means and . Take the Hermitian conjugate of : Right-multiply the previous equation by Left-multiply (3) by . Equate the RHS of (4) and (5): . I understand, and ask not about, the algebra. What's the proof strategy? For example, how can you divine when to take the Hermitian conjugate, what to multiply, and when to left- or right-multiply?","A^* =  \bar{A}^T  v_i v_j \lambda_i \neq \lambda_j Hv_i = \lambda_iv_i Hv_j
 = \lambda_jv_j\ \  ...(3) Hv_i = \lambda_iv_i \begin{align} (Hv_i)^* & = (\lambda_iv_i)^* \\ \implies v_i^* H^* & = v_i^* \lambda_i^* \\ \implies v_i^* H & = v_i^* \lambda_i \ \ (as\ H\ is\ hermitian)\end{align} \color{green}{v_j}: \qquad
 v_i^*H \color{green}{v_j} = v_i^* \lambda_i \color{green}{v_j} \qquad
...(4) \color{orangered}{v_i^*} : \quad
 \color{orangered}{v_i^*}Hv_j = \color{orangered}{v_i^*} \lambda_jv_j
 \qquad ...(5) \quad  v_i^* \lambda_i
 \color{green}{v_j}  = \color{orangered}{v_i^*} \lambda_jv_j \qquad
 \blacksquare",['linear-algebra']
27,How do I find a basis for the subspace of $\mathbb{R}^4$ spanned by the four vectors below:,How do I find a basis for the subspace of  spanned by the four vectors below:,\mathbb{R}^4,"$v_1=(1, 1, 2, 4), v_2=(2, -1, -5, 2),v_3 = (1, -1, -4, 0), v_4=(2, 1, 1, 6).$ I started with writing down the matrix and through row reduction ended with the solution $x_1=4/3 + (1/3)x_3$ and $x_2 = 1/3 - (2/3)x_3$, $x_3$ is free. I think the basis is supposed to be $\{v_1, v_2\}$, but I'm not sure if this is correct. They are linearly independent, but how do the two vectors generate $\mathbb{R}^4$?","$v_1=(1, 1, 2, 4), v_2=(2, -1, -5, 2),v_3 = (1, -1, -4, 0), v_4=(2, 1, 1, 6).$ I started with writing down the matrix and through row reduction ended with the solution $x_1=4/3 + (1/3)x_3$ and $x_2 = 1/3 - (2/3)x_3$, $x_3$ is free. I think the basis is supposed to be $\{v_1, v_2\}$, but I'm not sure if this is correct. They are linearly independent, but how do the two vectors generate $\mathbb{R}^4$?",,['linear-algebra']
28,Proving the product of two non singular matrices is also non singular.,Proving the product of two non singular matrices is also non singular.,,"I am having trouble with a proof for linear algebra. Could somebody explain to me how to prove that if $A$ and $B$ are both $n\times n$ non singular matrices, that their product $AB$ is also non singular. A place to start would be helpful. Thank you for your time.","I am having trouble with a proof for linear algebra. Could somebody explain to me how to prove that if $A$ and $B$ are both $n\times n$ non singular matrices, that their product $AB$ is also non singular. A place to start would be helpful. Thank you for your time.",,['linear-algebra']
29,How to prove this matrix inequality $\det(A+B)\ge 2^n\sqrt{\det(A)\det(B)}$,How to prove this matrix inequality,\det(A+B)\ge 2^n\sqrt{\det(A)\det(B)},"Question: Let $A_{n\times n}$ and $B_{n\times n}$ be positive Hermitian matrices. Show that $$\det(A+B)\ge 2^n\sqrt{\det(A)\det(B)}.$$ I know that $$\det(A+B)\ge \det(A)+\det(B)$$ But my problem is that I can't,(maybe this is an old reslut,and also I can't find it), Thank you very much!","Question: Let and be positive Hermitian matrices. Show that I know that But my problem is that I can't,(maybe this is an old reslut,and also I can't find it), Thank you very much!",A_{n\times n} B_{n\times n} \det(A+B)\ge 2^n\sqrt{\det(A)\det(B)}. \det(A+B)\ge \det(A)+\det(B),"['linear-algebra', 'matrices', 'inequality']"
30,Symmetric Tridiagonal Matrix has distinct eigenvalues. [closed],Symmetric Tridiagonal Matrix has distinct eigenvalues. [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Show that the rank of $ n\times n$ symmetric tridiagonal matrix is at least $n-1$, and prove that it has $n$ distinct eigenvalues.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Show that the rank of $ n\times n$ symmetric tridiagonal matrix is at least $n-1$, and prove that it has $n$ distinct eigenvalues.",,"['linear-algebra', 'matrices']"
31,"Is $\{\sin x,\cos x\}$ independent?",Is  independent?,"\{\sin x,\cos x\}","Is $\{\sin x,\cos x\}$ linearly independent in $\mathbb{R}^n$ ? I thought they were not because I can write $\cos x=\sin (x+\pi/2)$ . My professor on the other hand said it was independent and his proof is as follows: If $\{\sin x,\cos\}$ is independent in $[0,2\pi]$ , then it will be independent on all of $\mathbb{R}.$ (He didn't prove this either.) $a\cos x+b \sin x=0  ,\forall \vec{x}\in[0,2\pi].$ $x=0\implies a\cdot1+b\cdot0=0 \implies a=0$ . $x=\pi /2\implies a\cdot 0+b \cdot 1 \implies b=0$ . So $\{\sin x, \cos x\}$ is independent on $[0,2\pi],$ and thus independent everywhere. $QED$ . Is there something I am missing or not understanding...? Why is this set independent, when I can express an element of the set as a linear combination?","Is linearly independent in ? I thought they were not because I can write . My professor on the other hand said it was independent and his proof is as follows: If is independent in , then it will be independent on all of (He didn't prove this either.) . . So is independent on and thus independent everywhere. . Is there something I am missing or not understanding...? Why is this set independent, when I can express an element of the set as a linear combination?","\{\sin x,\cos x\} \mathbb{R}^n \cos x=\sin (x+\pi/2) \{\sin x,\cos\} [0,2\pi] \mathbb{R}. a\cos x+b \sin x=0  ,\forall \vec{x}\in[0,2\pi]. x=0\implies a\cdot1+b\cdot0=0 \implies a=0 x=\pi /2\implies a\cdot 0+b \cdot 1 \implies b=0 \{\sin x, \cos x\} [0,2\pi], QED",['linear-algebra']
32,Finding a basis for the plane with the equation,Finding a basis for the plane with the equation,,"I know the conditions of being a basis.  The vectors in set should be linearly independent and they should span the vector space. So while finding a basis for the equation $y = z$, it's easy to see that the $x$ is free variable and if we call it as $x = s$, $y$ and $z$ becomes $t$ for example. And the basis are $(1,0,0)$ and $(0,1,1)$ However, i could not apply the same logic to $x - 2y + 5z =0$ Is there any free variable in this equation? How can I find the basis?","I know the conditions of being a basis.  The vectors in set should be linearly independent and they should span the vector space. So while finding a basis for the equation $y = z$, it's easy to see that the $x$ is free variable and if we call it as $x = s$, $y$ and $z$ becomes $t$ for example. And the basis are $(1,0,0)$ and $(0,1,1)$ However, i could not apply the same logic to $x - 2y + 5z =0$ Is there any free variable in this equation? How can I find the basis?",,['linear-algebra']
33,Jordan decomposition of $A^T$ given that of $A$,Jordan decomposition of  given that of,A^T A,Suppose I have the Jordan normal form of a matrix $A$. The decomposition involves the Jordan matrix $J$ and a similarity matrix $P$ such that $P^{-1}.J.P = A$. My question: is it possible to find the similarity matrix of $A^T$ given that we know that of $A$? Thank you in advance.,Suppose I have the Jordan normal form of a matrix $A$. The decomposition involves the Jordan matrix $J$ and a similarity matrix $P$ such that $P^{-1}.J.P = A$. My question: is it possible to find the similarity matrix of $A^T$ given that we know that of $A$? Thank you in advance.,,['linear-algebra']
34,Does the Linear Dependence Lemma imply that one of at least TWO possible vectors can be removed from a list without changing the span?,Does the Linear Dependence Lemma imply that one of at least TWO possible vectors can be removed from a list without changing the span?,,"Does the Linear Dependence Lemma imply that there are always at least TWO possible vectors of which one can be removed from a linearly dependent list without changing the span? The Linear Dependence Lemma as presented in Axler, Linear Algebra Done Right (3rd Edition): It seems to me that there exist at least 2 such $j$ , because if $v_{j}$ is in the span of the other vectors in the list then it can be written as a linear combination of those others, (i.e., $v_{j}=a_{1}v_{1}+a_{2}v_{2} +...+ a_{m}v_{m}$ ). And if so, then by rearranging terms between RHS and LHS you could always specify at least one other $v$ in terms of $v_{j}$ (and assuming $v_{j} \neq 0$ then at least one $a$ is also $\neq 0$ ). In which case you could remove that $v$ from the list without changing the span. So there should always be at least 2 candidates for removal from a linearly dependent list without changing the span, right? Perhaps this is obvious, but I want to make sure I'm not missing something. Any and all feedback is welcome. Thanks!","Does the Linear Dependence Lemma imply that there are always at least TWO possible vectors of which one can be removed from a linearly dependent list without changing the span? The Linear Dependence Lemma as presented in Axler, Linear Algebra Done Right (3rd Edition): It seems to me that there exist at least 2 such , because if is in the span of the other vectors in the list then it can be written as a linear combination of those others, (i.e., ). And if so, then by rearranging terms between RHS and LHS you could always specify at least one other in terms of (and assuming then at least one is also ). In which case you could remove that from the list without changing the span. So there should always be at least 2 candidates for removal from a linearly dependent list without changing the span, right? Perhaps this is obvious, but I want to make sure I'm not missing something. Any and all feedback is welcome. Thanks!",j v_{j} v_{j}=a_{1}v_{1}+a_{2}v_{2} +...+ a_{m}v_{m} v v_{j} v_{j} \neq 0 a \neq 0 v,"['linear-algebra', 'vector-spaces']"
35,Matrix determinant effect,Matrix determinant effect,,"Determinant of the matrix $$A= \begin{bmatrix}a&b&c\\d&e&f\\g&h&i\end{bmatrix}$$ is $$\det A=4.$$ So what is the determinant of \begin{bmatrix}3a&3b&3c\\-d&-e&-f\\g-a&h-b&i-c\end{bmatrix} I found that the row operations that were done were $3R1, -1.R2,$ and the last one doesn't matter. So is the determinant $3\times 4\times(-1)= -12$ or we have to do the inverse $4\times 1/3 \times 1/(-1)?$",Determinant of the matrix is So what is the determinant of I found that the row operations that were done were and the last one doesn't matter. So is the determinant or we have to do the inverse,"A= \begin{bmatrix}a&b&c\\d&e&f\\g&h&i\end{bmatrix} \det A=4. \begin{bmatrix}3a&3b&3c\\-d&-e&-f\\g-a&h-b&i-c\end{bmatrix} 3R1, -1.R2, 3\times 4\times(-1)= -12 4\times 1/3 \times 1/(-1)?","['linear-algebra', 'determinant']"
36,Prove that all $2 \times 2$ orthogonal matrices can be expressed as rotation or reflection,Prove that all  orthogonal matrices can be expressed as rotation or reflection,2 \times 2,"Let A be some $2 \times 2$ matrix with real entries. Prove that $A^T$$A$ = $I$ if and only if $A$ is the rotation matrix or the reflection matrix. My Progress: It can be shown that if $A$ is either the rotation or reflection matrix, then $A^T A = I$ holds by matrix multiplication. Where I get suck is showing that if $A$ is a $2 \times 2$ orthogonal matrix, then $A$ must either be the Rotation or Reflection Matrix. I suppose that since $A$ is orthogonal, it is distance preserving - and the only $2 \times 2$ matrices that preserve distance are the Rotation and Reflection Matrices, but this isn't really a proof.","Let A be some matrix with real entries. Prove that = if and only if is the rotation matrix or the reflection matrix. My Progress: It can be shown that if is either the rotation or reflection matrix, then holds by matrix multiplication. Where I get suck is showing that if is a orthogonal matrix, then must either be the Rotation or Reflection Matrix. I suppose that since is orthogonal, it is distance preserving - and the only matrices that preserve distance are the Rotation and Reflection Matrices, but this isn't really a proof.",2 \times 2 A^TA I A A A^T A = I A 2 \times 2 A A 2 \times 2,"['linear-algebra', 'matrices']"
37,Prove that $\text{rank}(X^TX)=\text{rank}(X)$,Prove that,\text{rank}(X^TX)=\text{rank}(X),"Prove, for real $X$, that $\text{rank}(X^TX)=\text{rank}(X)$. Could anyone please help me with this problem? I've tried to use full-rank factorization and rank-related theorems mentioned in my book but still failed to solve this. I am learning linear algebra by myself and my book has no solutions manual so I find it really hard to get used to solve linear algebra problems. Many thanks in advance for your help!","Prove, for real $X$, that $\text{rank}(X^TX)=\text{rank}(X)$. Could anyone please help me with this problem? I've tried to use full-rank factorization and rank-related theorems mentioned in my book but still failed to solve this. I am learning linear algebra by myself and my book has no solutions manual so I find it really hard to get used to solve linear algebra problems. Many thanks in advance for your help!",,"['linear-algebra', 'matrix-rank']"
38,Proving or disproving product of two stochastic matrices is stochastic,Proving or disproving product of two stochastic matrices is stochastic,,Let $P$ and $Q$ be two stochastic matrices. Does the product $PQ$ have to be stochastic? Prove or disprove. What Im thinking is that since matrix multiplication is only defined for two matrices $A$ and $B$ where $A$ has the same amount of columns as $B$ has rows and vice versa. Therefore the product of any two stochastic matrices $P$ and $Q$ can not be stochastic. This is not much of a proof though.,Let $P$ and $Q$ be two stochastic matrices. Does the product $PQ$ have to be stochastic? Prove or disprove. What Im thinking is that since matrix multiplication is only defined for two matrices $A$ and $B$ where $A$ has the same amount of columns as $B$ has rows and vice versa. Therefore the product of any two stochastic matrices $P$ and $Q$ can not be stochastic. This is not much of a proof though.,,"['linear-algebra', 'matrices', 'proof-writing', 'stochastic-matrices']"
39,Is the multiplicity of an eigenvalue equal to the dimension of its associated eigenspace?,Is the multiplicity of an eigenvalue equal to the dimension of its associated eigenspace?,,"This is a silly question, but if someone could provide a short proof as to why if it is true, or a counterexample and short explanation if it's false I would appreciate it. EDIT: By multiplicity I mean, when solving the characteristic polynomial, the roots (i.e the eigenvalues) can be repeated, so I was referring to the algebraic multiplicity.","This is a silly question, but if someone could provide a short proof as to why if it is true, or a counterexample and short explanation if it's false I would appreciate it. EDIT: By multiplicity I mean, when solving the characteristic polynomial, the roots (i.e the eigenvalues) can be repeated, so I was referring to the algebraic multiplicity.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
40,Geometric interpretation of Gauss elimination,Geometric interpretation of Gauss elimination,,Solution of a set of linear equations is finding a point of intersection of all planes represented by those equations. But how can we relate it to gauss elimination method ? Suppose we have equations like- x + 3y = 4 (A) 2x - 6y = 8 (B) We are trying ti find common value of x and y that will satisfy both equations. So we do B-2A and find the value of y. But how can we explain B-2A geometrically in context of solving these equations ?,Solution of a set of linear equations is finding a point of intersection of all planes represented by those equations. But how can we relate it to gauss elimination method ? Suppose we have equations like- x + 3y = 4 (A) 2x - 6y = 8 (B) We are trying ti find common value of x and y that will satisfy both equations. So we do B-2A and find the value of y. But how can we explain B-2A geometrically in context of solving these equations ?,,['linear-algebra']
41,Must any nth order homogeneous ODE have n solutions?,Must any nth order homogeneous ODE have n solutions?,,"I am quite confused about ordinary differential equations and the number of solutions they have. In particular, it seems that an nth order homogeneous differential equation has n solutions, not more or less. I cannot figure out why this would be so. I have read the following two posts: here and here . The first didn't really seem to have any conclusive answers and the second was quite technical for me to understand. Perhaps there is an explanation in easy terms? My current 'understanding' so far is as as follows: The general solution to any homogeneous linear ODE is a linear combination of all possible solutions. In the case of an ODE of form $ay''+by'+cy=0$ where there are two equal roots to the auxillary equation, i.e. $b^2-4ac=0$, then I know the extra solution is $xe^{\lambda x}$ (although I have no idea why someone ever realised that this would work. Is there an underlying reasoning motivating this choice?) Now if I accept this form and plug it in, I can see that indeed $xe^{\lambda x}$ is a solution iff $b^2-4ac=0$. Therefore in the case of two equal roots of the auxillary equation, we necessarily have that two of the solutions are $e^{\lambda x}$ and $xe^{\lambda x}$ so the general solution is their linear combination. And if the two roots of the auxillary equation are not equal, $xe^{\lambda x}$ is not a solution. Of course I have not shown that there are only two solutions (could there be more?), nor have I shown that there have to be at least n solutions for an nth order ODE. I have just demonstrated that for a second order linear homogeneous ODE, $xe^{\lambda x}$ is a solution if there is only one root of the auxillary equatiuon, and it isn't otherwise. I think my dilemma boils down to the following questions which I would be grateful if someone could answer, or indicate where to look for further information: a)Need an nth order homogeneous ODE have n solutions (can it ever have more or less)? b)Need an nth order linear homogeneous ODE have n solutions (can it ever have more or less)? c)Are there any 'special cases'? Thank you in advance. EDIT: I have just thought a bit about 'function spaces' (i'm not sure if this is the right term. I am not too familiar with this but from my vague understanding, I think that n liearly independent solutions will span an n dimensional function space, so if it is the case that there must be n linearly independent solutions then this implies that the solution space of the ODE must span n dimensions? I am not sure why this is the case though, or what differential equations have to do with dimensions...","I am quite confused about ordinary differential equations and the number of solutions they have. In particular, it seems that an nth order homogeneous differential equation has n solutions, not more or less. I cannot figure out why this would be so. I have read the following two posts: here and here . The first didn't really seem to have any conclusive answers and the second was quite technical for me to understand. Perhaps there is an explanation in easy terms? My current 'understanding' so far is as as follows: The general solution to any homogeneous linear ODE is a linear combination of all possible solutions. In the case of an ODE of form $ay''+by'+cy=0$ where there are two equal roots to the auxillary equation, i.e. $b^2-4ac=0$, then I know the extra solution is $xe^{\lambda x}$ (although I have no idea why someone ever realised that this would work. Is there an underlying reasoning motivating this choice?) Now if I accept this form and plug it in, I can see that indeed $xe^{\lambda x}$ is a solution iff $b^2-4ac=0$. Therefore in the case of two equal roots of the auxillary equation, we necessarily have that two of the solutions are $e^{\lambda x}$ and $xe^{\lambda x}$ so the general solution is their linear combination. And if the two roots of the auxillary equation are not equal, $xe^{\lambda x}$ is not a solution. Of course I have not shown that there are only two solutions (could there be more?), nor have I shown that there have to be at least n solutions for an nth order ODE. I have just demonstrated that for a second order linear homogeneous ODE, $xe^{\lambda x}$ is a solution if there is only one root of the auxillary equatiuon, and it isn't otherwise. I think my dilemma boils down to the following questions which I would be grateful if someone could answer, or indicate where to look for further information: a)Need an nth order homogeneous ODE have n solutions (can it ever have more or less)? b)Need an nth order linear homogeneous ODE have n solutions (can it ever have more or less)? c)Are there any 'special cases'? Thank you in advance. EDIT: I have just thought a bit about 'function spaces' (i'm not sure if this is the right term. I am not too familiar with this but from my vague understanding, I think that n liearly independent solutions will span an n dimensional function space, so if it is the case that there must be n linearly independent solutions then this implies that the solution space of the ODE must span n dimensions? I am not sure why this is the case though, or what differential equations have to do with dimensions...",,"['linear-algebra', 'ordinary-differential-equations', 'fundamental-solution']"
42,Show that orthogonal matrices are diagonalizable,Show that orthogonal matrices are diagonalizable,,"I want to prove that all orthogonal matrices are diagonalizable over $C$. I know that a matrix is orthogonal if $Q^TQ = QQ^T = I$ and $Q^T = Q^{-1}$, and that a matrix $A$ is diagonalizable if $A = PDP^{-1}$ where $D$ is a diagonal matrix. How can I start this proof?","I want to prove that all orthogonal matrices are diagonalizable over $C$. I know that a matrix is orthogonal if $Q^TQ = QQ^T = I$ and $Q^T = Q^{-1}$, and that a matrix $A$ is diagonalizable if $A = PDP^{-1}$ where $D$ is a diagonal matrix. How can I start this proof?",,"['linear-algebra', 'matrices']"
43,Optimal approximation of quadratic form,Optimal approximation of quadratic form,,"Let $\mathbf{x}\in\Bbb{R}^n$ and $A\in\Bbb{S}_{++}^n$, where $\Bbb{S}_{++}^n$ denotes the space of symmetric positive definite $n\times n$ real matrices. Also, let $Q\colon\Bbb{R}^n\to\Bbb{R}_{+}$ be the quadratic form given by $$ Q(\mathbf{x})=\mathbf{x}^\top A\mathbf{x}\geq0. $$ I would like to approximate $Q(\mathbf{x})$ by a scalar multiple of the squared Euclidean norm of $\mathbf{x}$, that is $$ Q(\mathbf{x})\approx c \lVert\mathbf{x}\rVert^2,\quad c>0. $$ If $A$ is a multiple of the identity matrix (of order $n$), i.e. $A=aI_n$, $a>0$, then $c=a$ and $Q(\mathbf{x})=a\lVert\mathbf{x}\rVert^2$. In this case we have no approximation but a strict equality. On the other hand, is $A\neq aI_n$, we could approximate the quadratic form using the mean of eigenvalues of $A$, since it holds that $$ \lambda_{min}(A)\lVert\mathbf{x}\rVert^2\leq Q(\mathbf{x})\leq\lambda_{max}(A)\lVert\mathbf{x}\rVert^2, $$ that is, $$ Q(\mathbf{x})\approx\frac{1}{n}\sum_{i=1}^{n}\lambda_{i}(A)\lVert\mathbf{x}\rVert^2, $$ and thus $c=\frac{1}{n}\sum_{i=1}^{n}\lambda_{i}(A)$. Is there any way of finding an optimal $c$ such that the approximation of $Q(\mathbf{x})$ is optimal (by satisfying some criterion)?","Let $\mathbf{x}\in\Bbb{R}^n$ and $A\in\Bbb{S}_{++}^n$, where $\Bbb{S}_{++}^n$ denotes the space of symmetric positive definite $n\times n$ real matrices. Also, let $Q\colon\Bbb{R}^n\to\Bbb{R}_{+}$ be the quadratic form given by $$ Q(\mathbf{x})=\mathbf{x}^\top A\mathbf{x}\geq0. $$ I would like to approximate $Q(\mathbf{x})$ by a scalar multiple of the squared Euclidean norm of $\mathbf{x}$, that is $$ Q(\mathbf{x})\approx c \lVert\mathbf{x}\rVert^2,\quad c>0. $$ If $A$ is a multiple of the identity matrix (of order $n$), i.e. $A=aI_n$, $a>0$, then $c=a$ and $Q(\mathbf{x})=a\lVert\mathbf{x}\rVert^2$. In this case we have no approximation but a strict equality. On the other hand, is $A\neq aI_n$, we could approximate the quadratic form using the mean of eigenvalues of $A$, since it holds that $$ \lambda_{min}(A)\lVert\mathbf{x}\rVert^2\leq Q(\mathbf{x})\leq\lambda_{max}(A)\lVert\mathbf{x}\rVert^2, $$ that is, $$ Q(\mathbf{x})\approx\frac{1}{n}\sum_{i=1}^{n}\lambda_{i}(A)\lVert\mathbf{x}\rVert^2, $$ and thus $c=\frac{1}{n}\sum_{i=1}^{n}\lambda_{i}(A)$. Is there any way of finding an optimal $c$ such that the approximation of $Q(\mathbf{x})$ is optimal (by satisfying some criterion)?",,"['linear-algebra', 'optimization', 'eigenvalues-eigenvectors']"
44,On the importance of order for bases in finite dimensional vector spaces,On the importance of order for bases in finite dimensional vector spaces,,"I am reading Tapp's Introduction to Matrix Groups for Undergraduates and he writes: Let $V$ be an $n$-dimensional (left) vector space over $\mathbb K$. Then $V$ is isomorphic to $\mathbb K^n$. In fact, there are many isomorphisms from $V$ to $\mathbb K^n$. For any ordered basis $v_1, \dots, v_n$ of $V$ the following is an isomorphism: $(c_1 v_1 + \dots + c_n v_n) \mapsto (c_1, \dots, c_n)$ My question is: (1) Are all finite dimensional bases in linear algebra courses in general automatically assumed to be ordered because writing $b_1, \dots, b_n$ is an order? (2) It seems to me that the order is irrelevant in the sense that the isomorphism above could just as well map $(c_1 v_1 + \dots + c_n v_n) \mapsto (c_n, \dots, c_1)$. Hence my question: is it possible to give an isomorphism without using an ordered basis?","I am reading Tapp's Introduction to Matrix Groups for Undergraduates and he writes: Let $V$ be an $n$-dimensional (left) vector space over $\mathbb K$. Then $V$ is isomorphic to $\mathbb K^n$. In fact, there are many isomorphisms from $V$ to $\mathbb K^n$. For any ordered basis $v_1, \dots, v_n$ of $V$ the following is an isomorphism: $(c_1 v_1 + \dots + c_n v_n) \mapsto (c_1, \dots, c_n)$ My question is: (1) Are all finite dimensional bases in linear algebra courses in general automatically assumed to be ordered because writing $b_1, \dots, b_n$ is an order? (2) It seems to me that the order is irrelevant in the sense that the isomorphism above could just as well map $(c_1 v_1 + \dots + c_n v_n) \mapsto (c_n, \dots, c_1)$. Hence my question: is it possible to give an isomorphism without using an ordered basis?",,['linear-algebra']
45,Linear algebra - Memorising proper definitions of homomorphism types,Linear algebra - Memorising proper definitions of homomorphism types,,"I am reading a book about linear algebra. On the basis of this book, I worked out the terminology below. Problem: To me, it looks like Wikipedia defines homomorphism differently. Apart from that: Do you agree with the following definitions of the homomorphism subtypes? If so, is there a trick to memorise them? Let $V_1, V_2$ be vector spaces over a common field. We consider a function $f : V_1 \rightarrow V_2$. Now, $f$ is a homomorphism iff $f$ is linear (linear-algebra-linear, not calculus-linear). epimorphism = homomorphism + surjective monomorphism = homomorphism + injective isomorphism = epimorphism + monomorphism endomorphism = homomorphism + (domain = codomain) automorphism = endomorphism + isomorphism The article Algebra homomorphism enumerates (in its first sentence) homogeneity and additivity but also a third property. The third property seems to be missing in my definition (definition based on the book). By the way, should I use the term map instead of function ?","I am reading a book about linear algebra. On the basis of this book, I worked out the terminology below. Problem: To me, it looks like Wikipedia defines homomorphism differently. Apart from that: Do you agree with the following definitions of the homomorphism subtypes? If so, is there a trick to memorise them? Let $V_1, V_2$ be vector spaces over a common field. We consider a function $f : V_1 \rightarrow V_2$. Now, $f$ is a homomorphism iff $f$ is linear (linear-algebra-linear, not calculus-linear). epimorphism = homomorphism + surjective monomorphism = homomorphism + injective isomorphism = epimorphism + monomorphism endomorphism = homomorphism + (domain = codomain) automorphism = endomorphism + isomorphism The article Algebra homomorphism enumerates (in its first sentence) homogeneity and additivity but also a third property. The third property seems to be missing in my definition (definition based on the book). By the way, should I use the term map instead of function ?",,"['linear-algebra', 'vector-spaces', 'abstract-algebra']"
46,"Geometrically, what is the span of vectors?","Geometrically, what is the span of vectors?",,"Simple question from a calc 3 beginner. Visually I cannot imagine the span of two vectors, what does this necessarily mean? For example my text mentions if two vectors are parallel their span is a line, otherwise a plane. Can anyone elaborate?","Simple question from a calc 3 beginner. Visually I cannot imagine the span of two vectors, what does this necessarily mean? For example my text mentions if two vectors are parallel their span is a line, otherwise a plane. Can anyone elaborate?",,"['linear-algebra', 'vector-spaces', 'vectors']"
47,Calculating the determinant gives $(a^2+b^2+c^2+d^2)^2$?,Calculating the determinant gives ?,(a^2+b^2+c^2+d^2)^2,"I need to calculate the following determinant in order to prove the following equality: $$\det\begin{pmatrix} a & b & c & d \\ -b & a & -d & c \\ -c & d & a & -b \\ -d & -c & b & a \end{pmatrix} = (a^2+b^2+c^2+d^2)^2.$$ I tried using Gauss-algorithm to get an easier matrix, but I'm not sure if I did it correctly. Calling the $4$ lines $I$, $II$, $III$ and $IV$, I did: (1) $II \cdot a$ (2) $III \cdot a$ (3) $IV \cdot a$ After this I did: (4) $II' + I \cdot b$ (5) $III' + I \cdot c$ (6) $IV' + I \cdot d$ So finally I got the following matrix: $$\begin{pmatrix} a & b & c & d \\ 0 & a^2+b^2 & bc-ad & ac+bd \\ 0 & ad+bc & a^2+c^2 & cd-ab \\ 0 & bd-ac & ab-cd & a^2+d^2 \end{pmatrix}.$$ I thought this would make the determinant a bit easier, unfortunately I must have done something wrong. Is multiplication with single lines allowed as I have done it? Thank you very much.","I need to calculate the following determinant in order to prove the following equality: $$\det\begin{pmatrix} a & b & c & d \\ -b & a & -d & c \\ -c & d & a & -b \\ -d & -c & b & a \end{pmatrix} = (a^2+b^2+c^2+d^2)^2.$$ I tried using Gauss-algorithm to get an easier matrix, but I'm not sure if I did it correctly. Calling the $4$ lines $I$, $II$, $III$ and $IV$, I did: (1) $II \cdot a$ (2) $III \cdot a$ (3) $IV \cdot a$ After this I did: (4) $II' + I \cdot b$ (5) $III' + I \cdot c$ (6) $IV' + I \cdot d$ So finally I got the following matrix: $$\begin{pmatrix} a & b & c & d \\ 0 & a^2+b^2 & bc-ad & ac+bd \\ 0 & ad+bc & a^2+c^2 & cd-ab \\ 0 & bd-ac & ab-cd & a^2+d^2 \end{pmatrix}.$$ I thought this would make the determinant a bit easier, unfortunately I must have done something wrong. Is multiplication with single lines allowed as I have done it? Thank you very much.",,"['linear-algebra', 'determinant']"
48,Notation for Subspaces,Notation for Subspaces,,"Is there a proper notation for denoting subspaces? For example, if $U$ is a subspace of some vector space $V$. I would usually just write ""the subspace $U \subseteq V$"" but I'm wondering is there is a proper, more compact, notation.","Is there a proper notation for denoting subspaces? For example, if $U$ is a subspace of some vector space $V$. I would usually just write ""the subspace $U \subseteq V$"" but I'm wondering is there is a proper, more compact, notation.",,"['linear-algebra', 'notation']"
49,Show that $\dim(U + V) = \dim(U) + \dim(V) - \dim(U \cap V)$ [duplicate],Show that  [duplicate],\dim(U + V) = \dim(U) + \dim(V) - \dim(U \cap V),"This question already has answers here : Given two subspaces $U,W$ of vector space $V$, how to show that $\dim(U)+\dim(W)=\dim(U+W)+\dim(U\cap W)$ (4 answers) Closed 10 years ago . Let $W$ be a vector space and let $U$ and $V$ be finite dimensional subspaces. Not sure how to go about solving this.","This question already has answers here : Given two subspaces $U,W$ of vector space $V$, how to show that $\dim(U)+\dim(W)=\dim(U+W)+\dim(U\cap W)$ (4 answers) Closed 10 years ago . Let $W$ be a vector space and let $U$ and $V$ be finite dimensional subspaces. Not sure how to go about solving this.",,['linear-algebra']
50,Jacobian matrix and Hessian matrix identity,Jacobian matrix and Hessian matrix identity,,"I am trying understand the following identity in two dimensions: $$ H_{XY} = J^TH_{xy}J$$ Here $x,y$ and $X,Y$ are different coordinates for $\mathbb R^2$, the $J$ is the Jacobian matrix and the $H$ are the Hessian matrices in the coordinates. My thoughts are the following: the $J$ is locally a coordinate transformation from $XY$ to $xy$ coordinates. At least is how I thought of Jacobians until today. The problem is that then $J^T$ should be $J^{-1}$. I checked and Jacobians are not necessarily unitary. Now I wonder: how do I understand the equation above and in general the Jacobian matrix? In particular why is it $J^T$ and not $J^{-1}$?","I am trying understand the following identity in two dimensions: $$ H_{XY} = J^TH_{xy}J$$ Here $x,y$ and $X,Y$ are different coordinates for $\mathbb R^2$, the $J$ is the Jacobian matrix and the $H$ are the Hessian matrices in the coordinates. My thoughts are the following: the $J$ is locally a coordinate transformation from $XY$ to $xy$ coordinates. At least is how I thought of Jacobians until today. The problem is that then $J^T$ should be $J^{-1}$. I checked and Jacobians are not necessarily unitary. Now I wonder: how do I understand the equation above and in general the Jacobian matrix? In particular why is it $J^T$ and not $J^{-1}$?",,"['linear-algebra', 'multivariable-calculus']"
51,What is a the intuition behind a parametric equation?,What is a the intuition behind a parametric equation?,,"I have always used equations for the line (y=a + bx) in R2. Recently I came upon this thing called parametric equations. I cannot grasp the difference between them and the equations for lines that I used before. Of course, I can't understand the plane and the parametric equations for it. Any good example or reference for having a good intuition about them. I'm a first year student in economics, if possible, an example in economics will be of great help. I recently saw a video on khan academy about parametric equations where both x and y depend on time, time being the parameter. Why not just make a 3d graph with 3 variables: x, y and t instead of a parametric equation in 2d?","I have always used equations for the line (y=a + bx) in R2. Recently I came upon this thing called parametric equations. I cannot grasp the difference between them and the equations for lines that I used before. Of course, I can't understand the plane and the parametric equations for it. Any good example or reference for having a good intuition about them. I'm a first year student in economics, if possible, an example in economics will be of great help. I recently saw a video on khan academy about parametric equations where both x and y depend on time, time being the parameter. Why not just make a 3d graph with 3 variables: x, y and t instead of a parametric equation in 2d?",,"['linear-algebra', 'euclidean-geometry', 'plane-curves']"
52,Does the product of positive definite matrices have a positive trace,Does the product of positive definite matrices have a positive trace,,"Let $ A_{1}, A_{2}, \ldots, A_{m}$ be a real symmetric semi-positive  definite matrices,  I want to know whether $ tr (A_{1} \cdot A_{2} \cdots A_{m} ) \geq 0$ ? When $m=2$, it seems a rather standard problem and has a ""yes"" answer.","Let $ A_{1}, A_{2}, \ldots, A_{m}$ be a real symmetric semi-positive  definite matrices,  I want to know whether $ tr (A_{1} \cdot A_{2} \cdots A_{m} ) \geq 0$ ? When $m=2$, it seems a rather standard problem and has a ""yes"" answer.",,"['linear-algebra', 'matrices']"
53,Linear algebra - Dimension theorem.,Linear algebra - Dimension theorem.,,"Suppose we have a vector space $V$, and $U$, $W$ subspaces of  $V$. Dimension theorem states: $$ \dim(U+W)=\dim U+ \dim W - \dim (U\cap W).$$ My question is: Why is $U \cap W$ necessary in this theorem?","Suppose we have a vector space $V$, and $U$, $W$ subspaces of  $V$. Dimension theorem states: $$ \dim(U+W)=\dim U+ \dim W - \dim (U\cap W).$$ My question is: Why is $U \cap W$ necessary in this theorem?",,"['linear-algebra', 'vector-spaces']"
54,Showing that $(A_{ij})=\left(\frac1{1+x_i+x_j}\right)$ is positive semidefinite,Showing that  is positive semidefinite,(A_{ij})=\left(\frac1{1+x_i+x_j}\right),"Consider the matrix $A$ whose coefficients are $A_{ij} = \frac{1}{1+x_i+x_j} $ where we have  $ x_i \geq 0$ and $ x_j \geq 0$  for $ i,j=1,2,\dots,n$.  How can I prove that this matrix is positive semidefinite for arbitrary $n$? Using first principal minors, I proved that this is positive semi-definite for $n=2$ but I could not generalize it.","Consider the matrix $A$ whose coefficients are $A_{ij} = \frac{1}{1+x_i+x_j} $ where we have  $ x_i \geq 0$ and $ x_j \geq 0$  for $ i,j=1,2,\dots,n$.  How can I prove that this matrix is positive semidefinite for arbitrary $n$? Using first principal minors, I proved that this is positive semi-definite for $n=2$ but I could not generalize it.",,"['linear-algebra', 'matrices', 'inner-products', 'positive-definite', 'positive-semidefinite']"
55,Proving that $\|A\|$ is finite.,Proving that  is finite.,\|A\|,"Let $|v|$ be the Euclidean norm on $\mathbb{R^n} $. For  $A\in \mathrm{Mat}_{n\times n}(\mathbb{R})$ we define $\displaystyle \|A\|:= \sup_{\large v\in \mathbb{R^n},\,v \neq 0}\frac{|Av|}{|v|}$. How to show that $\|A\|$ is finite for every $A$? It would be very helpful if someone could give hints. I think I should show that $\|-\|$ is bounded,but I don't know how...","Let $|v|$ be the Euclidean norm on $\mathbb{R^n} $. For  $A\in \mathrm{Mat}_{n\times n}(\mathbb{R})$ we define $\displaystyle \|A\|:= \sup_{\large v\in \mathbb{R^n},\,v \neq 0}\frac{|Av|}{|v|}$. How to show that $\|A\|$ is finite for every $A$? It would be very helpful if someone could give hints. I think I should show that $\|-\|$ is bounded,but I don't know how...",,['linear-algebra']
56,What does the double-lined capital $\mathbb{E}$ (not the sigma) stand for?,What does the double-lined capital  (not the sigma) stand for?,\mathbb{E},"I've encountered this symbol that looks like a capital $\mathbb{E}$ (with double vertical lines), which I am not familiar with, and I have no idea what to search for to find what it means, so apologies if it is something trivial. The context in which it is written is as follows: $R=\sum^\mathcal{T}_{t=1}\lambda^{t-1}\mathbb{E}[r^t]$ What does the $\mathbb{E}$ stand for? Update Some more context: $\mathcal{T}$ is the set of timeslots over which something is happening. $t\in\mathcal{T}$ (i.e. each timeslot). $\lambda$ is a discount factor raised to the timeslot its related to. $r^t$ is a reward collected at time $t$, and $R$ is supposedly calculating the total discounted reward over all timeslots in $\mathcal{T}$. I haven't got much more information (trying to understand this thing myself).","I've encountered this symbol that looks like a capital $\mathbb{E}$ (with double vertical lines), which I am not familiar with, and I have no idea what to search for to find what it means, so apologies if it is something trivial. The context in which it is written is as follows: $R=\sum^\mathcal{T}_{t=1}\lambda^{t-1}\mathbb{E}[r^t]$ What does the $\mathbb{E}$ stand for? Update Some more context: $\mathcal{T}$ is the set of timeslots over which something is happening. $t\in\mathcal{T}$ (i.e. each timeslot). $\lambda$ is a discount factor raised to the timeslot its related to. $r^t$ is a reward collected at time $t$, and $R$ is supposedly calculating the total discounted reward over all timeslots in $\mathcal{T}$. I haven't got much more information (trying to understand this thing myself).",,['linear-algebra']
57,Intuitively (concretely actually) what happens when you multiply a matrix by its transpose?,Intuitively (concretely actually) what happens when you multiply a matrix by its transpose?,,The construct $A^TA$ for $A$ any $m \times n$ matrix seems to appear often in formulae and results. For example I was reading that square root of eigenvalues of $A^TA$ (an $n \times n$ matrix) are singular values of $A$. If I do it on paper it seems like the diagonal contains squares and the off diagonals contain all possible quadratic combinations (and of course symmetric). What is a good way to think of this construct or how is it used other than in that singular value statement?,The construct $A^TA$ for $A$ any $m \times n$ matrix seems to appear often in formulae and results. For example I was reading that square root of eigenvalues of $A^TA$ (an $n \times n$ matrix) are singular values of $A$. If I do it on paper it seems like the diagonal contains squares and the off diagonals contain all possible quadratic combinations (and of course symmetric). What is a good way to think of this construct or how is it used other than in that singular value statement?,,"['linear-algebra', 'matrices']"
58,Why is every irreducible matrix with period 1 primitive?,Why is every irreducible matrix with period 1 primitive?,,"In a certain text on Perron-Frobenius theory, it is postulated that every irreducible nonnegative matrix with period $1$ is primitive and this proposition is said to be obvious. However, when I tried to prove the theorem by myself, I found that I was unable to come up with a formal proof. Intuitively, of course, I am sure that the theorem holds (and the converse statement is in fact easy to prove), but I am unable to prove it. According to the text, the theorem should be obvious from the graph-theoretic interpretation of these notions. For a given nonnegative matrix, it is possible to construct a digraph as follows: There is an edge from the $i$-th vertex to the $j$-th vertex if and only if the entry $(i,j)$ of the matrix is positive. Thus, the matrix is irreducible if and only if its digraph is strongly connected. The period is defined to be the greatest common divisor of lengths of cycles (more precisely, the closed paths) of the graph. And finally, the matrix is said to be primitive if there is a positive integer $n$ such that for each pair of vertices of the graph there is a path of length $n$ interconnecting these vertices. The theorem to be proved can thus be restated as follows: For every strongly connected digraph with the greatest common divisor of the lengths of closed paths equal to $1$, there is a positive integer $n$ such that for every pair of vertices of the digraph there is a path of length $n$ interconnecting these vertices. It seems to me that the theorem might be proved by means of number theory, but I have not been able to find a proof up to now. To be more specific, I am looking for a proof without the use of the Perron-Frobenius theorem (the proposition is used in the text to prove the Perron-Frobenius theorem). Any ideas? Thank you in advance.","In a certain text on Perron-Frobenius theory, it is postulated that every irreducible nonnegative matrix with period $1$ is primitive and this proposition is said to be obvious. However, when I tried to prove the theorem by myself, I found that I was unable to come up with a formal proof. Intuitively, of course, I am sure that the theorem holds (and the converse statement is in fact easy to prove), but I am unable to prove it. According to the text, the theorem should be obvious from the graph-theoretic interpretation of these notions. For a given nonnegative matrix, it is possible to construct a digraph as follows: There is an edge from the $i$-th vertex to the $j$-th vertex if and only if the entry $(i,j)$ of the matrix is positive. Thus, the matrix is irreducible if and only if its digraph is strongly connected. The period is defined to be the greatest common divisor of lengths of cycles (more precisely, the closed paths) of the graph. And finally, the matrix is said to be primitive if there is a positive integer $n$ such that for each pair of vertices of the graph there is a path of length $n$ interconnecting these vertices. The theorem to be proved can thus be restated as follows: For every strongly connected digraph with the greatest common divisor of the lengths of closed paths equal to $1$, there is a positive integer $n$ such that for every pair of vertices of the digraph there is a path of length $n$ interconnecting these vertices. It seems to me that the theorem might be proved by means of number theory, but I have not been able to find a proof up to now. To be more specific, I am looking for a proof without the use of the Perron-Frobenius theorem (the proposition is used in the text to prove the Perron-Frobenius theorem). Any ideas? Thank you in advance.",,"['linear-algebra', 'graph-theory', 'markov-chains']"
59,"If $\alpha_{1},\ldots,\alpha_{m}$ are vectors different from zero vector, then there is a linear functional $f$ such that $f(\alpha_{i})\neq 0$","If  are vectors different from zero vector, then there is a linear functional  such that","\alpha_{1},\ldots,\alpha_{m} f f(\alpha_{i})\neq 0","I am self-studying Hoffman and Kunze's book Linear Algebra . This is exercise 14 from page 106. Let $\mathbb{F}$ be a field of characteristic zero and let $V$ be a   finite-dimensional vector space over $\mathbb{F}$. If   $\alpha_{1},\ldots,\alpha_{m}$ are finitely many vectors vectors in   $V$, each different from zero vector, prove that there is a linear   functional $f$ such that $f(\alpha_{i})\neq 0, i=0,\ldots,m.$ I thought that prove by induction on $m$ would work. I showed the base case, but I got stuck on inductive step. I would appreciate your help.","I am self-studying Hoffman and Kunze's book Linear Algebra . This is exercise 14 from page 106. Let $\mathbb{F}$ be a field of characteristic zero and let $V$ be a   finite-dimensional vector space over $\mathbb{F}$. If   $\alpha_{1},\ldots,\alpha_{m}$ are finitely many vectors vectors in   $V$, each different from zero vector, prove that there is a linear   functional $f$ such that $f(\alpha_{i})\neq 0, i=0,\ldots,m.$ I thought that prove by induction on $m$ would work. I showed the base case, but I got stuck on inductive step. I would appreciate your help.",,[]
60,Finding a basis for the columnspace of a matrix,Finding a basis for the columnspace of a matrix,,"Find a linearly independent set of vectors that spans the same subspace of $R^4$ as that spanned by the vectors -   $$ \begin{bmatrix} 2 \\ -4 \\ -1 \\ -2 \\ \end{bmatrix} , \begin{bmatrix} 7 \\ -2 \\ 7 \\ 2 \\ \end{bmatrix} , \begin{bmatrix} 1 \\ 2 \\ 3 \\ 2 \\ \end{bmatrix} , \begin{bmatrix} 3 \\ -2 \\ 2 \\ 0 \\ \end{bmatrix} $$ So I start by reducing the matrix of these vectors to echelon form - $$ \begin{bmatrix} 1 & -7 & -3 & -2 \\ 0 & 21 & 7 &  7 \\ 0 & -30 & -10 & -10 \\ 0 & -12 & -4 & -4 \\ \end{bmatrix} $$ $$ \begin{bmatrix} 1 & -7 & -3 & -2 \\ 0 & 3 & 1 &  1 \\ 0 & -3 & -1 & -1 \\ 0 & -3 & -1 & -1 \\ \end{bmatrix} $$ $$ \begin{bmatrix} 1 & -7 & -3 & -2 \\ 0 & 3 & 1 &  1 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ \end{bmatrix} $$ Then I get $x_2 = -\frac{1}{3}x_3 - \frac{1}{3}x_4$ and $x_1 = \frac{2}{3}x_3 - \frac{1}{3}x_4$ So this gives me the linearly independent vectors below that span the same subspaces as the original vectors - $$ \begin{bmatrix} 1 \\ 0 \\ \frac {2}{3} \\ -\frac {1}{3}\\ \end{bmatrix} , \begin{bmatrix} 0 \\ 1 \\ -\frac {1}{3} \\ -\frac {1}{3} \\ \end{bmatrix}$$ But when I enter this answer I am told it is incorrect...anyone able to see where Im going wrong?","Find a linearly independent set of vectors that spans the same subspace of $R^4$ as that spanned by the vectors -   $$ \begin{bmatrix} 2 \\ -4 \\ -1 \\ -2 \\ \end{bmatrix} , \begin{bmatrix} 7 \\ -2 \\ 7 \\ 2 \\ \end{bmatrix} , \begin{bmatrix} 1 \\ 2 \\ 3 \\ 2 \\ \end{bmatrix} , \begin{bmatrix} 3 \\ -2 \\ 2 \\ 0 \\ \end{bmatrix} $$ So I start by reducing the matrix of these vectors to echelon form - $$ \begin{bmatrix} 1 & -7 & -3 & -2 \\ 0 & 21 & 7 &  7 \\ 0 & -30 & -10 & -10 \\ 0 & -12 & -4 & -4 \\ \end{bmatrix} $$ $$ \begin{bmatrix} 1 & -7 & -3 & -2 \\ 0 & 3 & 1 &  1 \\ 0 & -3 & -1 & -1 \\ 0 & -3 & -1 & -1 \\ \end{bmatrix} $$ $$ \begin{bmatrix} 1 & -7 & -3 & -2 \\ 0 & 3 & 1 &  1 \\ 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 \\ \end{bmatrix} $$ Then I get $x_2 = -\frac{1}{3}x_3 - \frac{1}{3}x_4$ and $x_1 = \frac{2}{3}x_3 - \frac{1}{3}x_4$ So this gives me the linearly independent vectors below that span the same subspaces as the original vectors - $$ \begin{bmatrix} 1 \\ 0 \\ \frac {2}{3} \\ -\frac {1}{3}\\ \end{bmatrix} , \begin{bmatrix} 0 \\ 1 \\ -\frac {1}{3} \\ -\frac {1}{3} \\ \end{bmatrix}$$ But when I enter this answer I am told it is incorrect...anyone able to see where Im going wrong?",,"['linear-algebra', 'matrices']"
61,Linear combination of natural numbers with positive coefficients,Linear combination of natural numbers with positive coefficients,,"I'm searching for a reference for the following result, so as to avoid writing a full proof in a paper.  Alternatively, if a one-liner exists, I'd be glad to know it! Theorem: Let $a, b$ be two positive integers.  Then there is a finite set $N$ of positive integers smaller than $\mbox{lcm}(a, b)$ such that: $$\{k_1a + k_2b \mid k_1, k_2 \in \mathbb{N}\} = \{\mbox{lcm}(a, b) + k\times\gcd(a, b) \mid k \in \mathbb{N}\} \cup N.$$ I'm also interested in its generalization to any number of integers: Say a set of integers is a linear set with $n$ periods if it can be written as: $$\{c_0 + \sum_{i=1}^n k_i \times c_i \mid k_i \in \mathbb{N}\}.$$ Then: Theorem: Any linear set is the union of a finite set and a linear set with one period. Thanks!","I'm searching for a reference for the following result, so as to avoid writing a full proof in a paper.  Alternatively, if a one-liner exists, I'd be glad to know it! Theorem: Let $a, b$ be two positive integers.  Then there is a finite set $N$ of positive integers smaller than $\mbox{lcm}(a, b)$ such that: $$\{k_1a + k_2b \mid k_1, k_2 \in \mathbb{N}\} = \{\mbox{lcm}(a, b) + k\times\gcd(a, b) \mid k \in \mathbb{N}\} \cup N.$$ I'm also interested in its generalization to any number of integers: Say a set of integers is a linear set with $n$ periods if it can be written as: $$\{c_0 + \sum_{i=1}^n k_i \times c_i \mid k_i \in \mathbb{N}\}.$$ Then: Theorem: Any linear set is the union of a finite set and a linear set with one period. Thanks!",,"['linear-algebra', 'reference-request']"
62,A specific type of orthogonal bases,A specific type of orthogonal bases,,"In $\mathbb{R}^n$, can we find an orthogonal basis $v_1,v_2,\ldots,v_n$ with $\|v_i\|=1$ such that $$\|\bar{v}_1\|=\|\bar{v}_2\|= \cdots =\|\bar{v}_n\|$$ where $\bar{v}_i\in\mathbb{R}^m$ is the vector formed by the first $m$ coordinates of $v_i$ with respect to the standard basis and $m$ is a given integer $m<n$.","In $\mathbb{R}^n$, can we find an orthogonal basis $v_1,v_2,\ldots,v_n$ with $\|v_i\|=1$ such that $$\|\bar{v}_1\|=\|\bar{v}_2\|= \cdots =\|\bar{v}_n\|$$ where $\bar{v}_i\in\mathbb{R}^m$ is the vector formed by the first $m$ coordinates of $v_i$ with respect to the standard basis and $m$ is a given integer $m<n$.",,"['linear-algebra', 'inner-products']"
63,"If the product of an invertible symmetric matrix and some other matrix is symmetric, is that other matrix also symmetric?","If the product of an invertible symmetric matrix and some other matrix is symmetric, is that other matrix also symmetric?",,"The thought came from the following problem: Let $V$ be a Euclidean space.  Let $T$ be an inner product on $V$.  Let $f$ be a linear transformation $f:V \to V$ such that $T(x,f(y))=T(f(x),y)$ for $x,y\in V$.  Let $v_1,\dots,v_n$ be an orthonormal basis, and let $A=(a_{ij})$ be the matrix of $f$ with respect to this basis. The goal here is to prove that the $A$ is symmetric.  I can prove this easily enough by saying: Since $T$ is an inner product, $T(v_i,v_j)=\delta_{ij}$. \begin{align*} T(A v_j,v_i)&=T(\sum_{k=1}^n a_{kj} v_k,v_i)\\ &=T(a_{1j} v_1,v_i) + \dots + T(a_{nj} v_n,v_i)\\ &=a_{1j} T(v_1,v_i) + \dots + a_{nj} T(v_n,v_i)\tag{bilinearity}\\ &=a_{ij}\tag{$T(v_i,v_j)=\delta_{ij}$}\\ \end{align*} By the same logic, \begin{align*} T(A v_j,v_i)&=T(v_j,A v_i)\\ &=T(v_j,\sum_{k=1}^n a_{ki} v_k)\\ &=T(v_j,a_{1i} v_1)+\dots+T(v_j,a_{ni} v_n)\\ &=a_{1i} T(v_j,v_1)+\dots+a_{ni} T(v_j,v_n)\\ &= a_{ji}\\ \end{align*} By hypothesis, $T(A v_j,v_i)=T(v_j,A v_i)$, therefore $a_{ij}=T(A v_j,v_i)=T(v_j,T v_i)=a_{ji}$. I had this other idea though, that since $T$ is an inner product, its matrix is positive definite. $T(x,f(y))=T(f(x),y)$ in matrix notation is $x^T T A y=(A x)^T T y$ \begin{align*} x^T T A y &= (A x)^T T y\\ &=x^T A^T T y\\ TA &= A^T T\\ (TA)^T &= (A^T T)^T\\ A^T T^T &= T^T A\\ TA &= A^T T^T\tag{T is symmetric}\\ &= (TA)^T\tag{transpose of matrix product}\\ \end{align*} This is where I got stuck.  We know that $T$ and $TA$ are both symmetric matrices.  Clearly $T^{-1}$ is symmetric.  If it can be shown that $T^{-1}$ and $AT$ commute, that would show it.","The thought came from the following problem: Let $V$ be a Euclidean space.  Let $T$ be an inner product on $V$.  Let $f$ be a linear transformation $f:V \to V$ such that $T(x,f(y))=T(f(x),y)$ for $x,y\in V$.  Let $v_1,\dots,v_n$ be an orthonormal basis, and let $A=(a_{ij})$ be the matrix of $f$ with respect to this basis. The goal here is to prove that the $A$ is symmetric.  I can prove this easily enough by saying: Since $T$ is an inner product, $T(v_i,v_j)=\delta_{ij}$. \begin{align*} T(A v_j,v_i)&=T(\sum_{k=1}^n a_{kj} v_k,v_i)\\ &=T(a_{1j} v_1,v_i) + \dots + T(a_{nj} v_n,v_i)\\ &=a_{1j} T(v_1,v_i) + \dots + a_{nj} T(v_n,v_i)\tag{bilinearity}\\ &=a_{ij}\tag{$T(v_i,v_j)=\delta_{ij}$}\\ \end{align*} By the same logic, \begin{align*} T(A v_j,v_i)&=T(v_j,A v_i)\\ &=T(v_j,\sum_{k=1}^n a_{ki} v_k)\\ &=T(v_j,a_{1i} v_1)+\dots+T(v_j,a_{ni} v_n)\\ &=a_{1i} T(v_j,v_1)+\dots+a_{ni} T(v_j,v_n)\\ &= a_{ji}\\ \end{align*} By hypothesis, $T(A v_j,v_i)=T(v_j,A v_i)$, therefore $a_{ij}=T(A v_j,v_i)=T(v_j,T v_i)=a_{ji}$. I had this other idea though, that since $T$ is an inner product, its matrix is positive definite. $T(x,f(y))=T(f(x),y)$ in matrix notation is $x^T T A y=(A x)^T T y$ \begin{align*} x^T T A y &= (A x)^T T y\\ &=x^T A^T T y\\ TA &= A^T T\\ (TA)^T &= (A^T T)^T\\ A^T T^T &= T^T A\\ TA &= A^T T^T\tag{T is symmetric}\\ &= (TA)^T\tag{transpose of matrix product}\\ \end{align*} This is where I got stuck.  We know that $T$ and $TA$ are both symmetric matrices.  Clearly $T^{-1}$ is symmetric.  If it can be shown that $T^{-1}$ and $AT$ commute, that would show it.",,['linear-algebra']
64,Show that $V = \mbox{ker}(f) \oplus \mbox{im}(f)$ for a linear map with $f \circ f = f$,Show that  for a linear map with,V = \mbox{ker}(f) \oplus \mbox{im}(f) f \circ f = f,"Question: Let $V$ be a $K$-Vectorspace and $f: V \rightarrow V$ be linear. It holds that $f \circ f = f$. Show that $V = \mbox{ker}(f) \oplus \mbox{im}(f)$. My attempt: So i guess that the $\oplus$ denotes a direct sum which means i have to show that (i) $V = \mbox{ker}(f) + \mbox{im}(f)$ and (ii) $\mbox{ker}(f) \cap \mbox{im}(f) = \{0\}$. I tried to do (ii) first: Let $v \in \mbox{im}(f) \cap \mbox{ker}(f)$ $\Rightarrow \exists u: f(u)=v \wedge f(v) = 0$ (can i put a ""Rightarrow"" here?) $(f \circ f)(u)=f(f(u)) = f(v) = 0$ As for (i) i am having difficulty with an approach to showing that $V = \mbox{ker}(f) + \mbox{im}(f)$. Should I even be trying to do this in the first place? if so, any advice as to how?","Question: Let $V$ be a $K$-Vectorspace and $f: V \rightarrow V$ be linear. It holds that $f \circ f = f$. Show that $V = \mbox{ker}(f) \oplus \mbox{im}(f)$. My attempt: So i guess that the $\oplus$ denotes a direct sum which means i have to show that (i) $V = \mbox{ker}(f) + \mbox{im}(f)$ and (ii) $\mbox{ker}(f) \cap \mbox{im}(f) = \{0\}$. I tried to do (ii) first: Let $v \in \mbox{im}(f) \cap \mbox{ker}(f)$ $\Rightarrow \exists u: f(u)=v \wedge f(v) = 0$ (can i put a ""Rightarrow"" here?) $(f \circ f)(u)=f(f(u)) = f(v) = 0$ As for (i) i am having difficulty with an approach to showing that $V = \mbox{ker}(f) + \mbox{im}(f)$. Should I even be trying to do this in the first place? if so, any advice as to how?",,['linear-algebra']
65,Why is the transpose related to the dual space?,Why is the transpose related to the dual space?,,"A matrix with $m$ rows and $n$ columns in the real numbers is a map from $M : \mathbb{R}^n \to \mathbb{R}^m$ ; the transpose of this matrix is then a map $M^T: \mathbb{R}^m \to \mathbb{R}^n$ . However, it seems like the transpose is related to the dual space, i.e., here $M^T: \mathbb{R}^{m*} \to \mathbb{R}^{n*}.$ Intuitively, why is it natural to associate the transpose with the dual space? It doesn't seem to follow with our intuition when dealing with matrices.","A matrix with rows and columns in the real numbers is a map from ; the transpose of this matrix is then a map . However, it seems like the transpose is related to the dual space, i.e., here Intuitively, why is it natural to associate the transpose with the dual space? It doesn't seem to follow with our intuition when dealing with matrices.",m n M : \mathbb{R}^n \to \mathbb{R}^m M^T: \mathbb{R}^m \to \mathbb{R}^n M^T: \mathbb{R}^{m*} \to \mathbb{R}^{n*}.,['linear-algebra']
66,When does $\det\left(\begin{smallmatrix}1&1&1\\x_1&x_2&x_3\\x_2&x_3&x_1\end{smallmatrix}\right)=0$ imply $x_1=x_2=x_3$?,When does  imply ?,\det\left(\begin{smallmatrix}1&1&1\\x_1&x_2&x_3\\x_2&x_3&x_1\end{smallmatrix}\right)=0 x_1=x_2=x_3,"Original question. Does the condition $$\det\begin{pmatrix}1&1&1\\x_1&x_2&x_3\\x_2&x_3&x_1\end{pmatrix}=0$$ imply $x_1=x_2=x_3$ ? a) Is this true if we assume $x_{1,2,3}\in\mathbb R$ ? b) Is this true if we assume $x_{1,2,3}\in\mathbb C$ ? I believe that this is true for real numbers and false for complex numbers. I posted my arguments in the answer below. (But still I would be interested in any alternative solutions and additional insights.) We might ask about an arbitrary field - but I would not be surprised it the generalized question might be difficult. Bonus question. For which fields the above condition holds? If this is difficult to answer, are there are at least some sufficient or necessary conditions? Some stuff which immediately comes into mind - and perhaps some of these might be reasonable ways to look at this problem. Let us denote $T(x_1,x_2,x_3)=(x_2,x_3,x_1)$ . The condition about the determinant basically says that the vectors $\vec u=(1,1,1)$ , $\vec x$ and $T(\vec x)$ are linearly dependent. Clearly, $T$ is a linear isomorphism and $T^3=id$ . If we are working over complex numbers, then the minimal polynomial it $m(t)=t^3-1$ and the eigenvalues are the roots of this polynomial: $1$ , $\omega$ , $\omega^2$ . For real numbers, the condition about determinant says that the points $(x_1,x_2)$ , $(x_2,x_3)$ , $(x_3,x_1)$ are collinear. We can forget the linear structure and simply view this as a problem about a polynomial in three variables - we're asking when $x_1x_2+x_2x_3+x_3x_1-x_1^2-x_2^2-x_3^2=0$ . I will add that this question originated from some discussion in chat - although in that discussion it was just an auxiliary result.","Original question. Does the condition imply ? a) Is this true if we assume ? b) Is this true if we assume ? I believe that this is true for real numbers and false for complex numbers. I posted my arguments in the answer below. (But still I would be interested in any alternative solutions and additional insights.) We might ask about an arbitrary field - but I would not be surprised it the generalized question might be difficult. Bonus question. For which fields the above condition holds? If this is difficult to answer, are there are at least some sufficient or necessary conditions? Some stuff which immediately comes into mind - and perhaps some of these might be reasonable ways to look at this problem. Let us denote . The condition about the determinant basically says that the vectors , and are linearly dependent. Clearly, is a linear isomorphism and . If we are working over complex numbers, then the minimal polynomial it and the eigenvalues are the roots of this polynomial: , , . For real numbers, the condition about determinant says that the points , , are collinear. We can forget the linear structure and simply view this as a problem about a polynomial in three variables - we're asking when . I will add that this question originated from some discussion in chat - although in that discussion it was just an auxiliary result.","\det\begin{pmatrix}1&1&1\\x_1&x_2&x_3\\x_2&x_3&x_1\end{pmatrix}=0 x_1=x_2=x_3 x_{1,2,3}\in\mathbb R x_{1,2,3}\in\mathbb C T(x_1,x_2,x_3)=(x_2,x_3,x_1) \vec u=(1,1,1) \vec x T(\vec x) T T^3=id m(t)=t^3-1 1 \omega \omega^2 (x_1,x_2) (x_2,x_3) (x_3,x_1) x_1x_2+x_2x_3+x_3x_1-x_1^2-x_2^2-x_3^2=0","['linear-algebra', 'determinant', 'symmetric-polynomials']"
67,Trace of power of a real matrix,Trace of power of a real matrix,,Suppose that $A \in M_n(\mathbb{R})$ . Prove that there is a $k \in \mathbb{Z}_{\geq 1}$ such that $Tr (A^k) \geq 0$ .,Suppose that . Prove that there is a such that .,A \in M_n(\mathbb{R}) k \in \mathbb{Z}_{\geq 1} Tr (A^k) \geq 0,"['linear-algebra', 'matrices', 'trace']"
68,Show that matrix of $T$ has at least dim range $T$ nonzero entries.,Show that matrix of  has at least dim range  nonzero entries.,T T,"Suppose $V$ and $W$ are finite-dimensional and $T \in \mathcal{L}(V, W)$ . Show that with respect to each choice of bases of $V$ and $W$ , the matrix of $T$ has at least dim range $T$ nonzero entries. I have no idea on how to solve this.","Suppose and are finite-dimensional and . Show that with respect to each choice of bases of and , the matrix of has at least dim range nonzero entries. I have no idea on how to solve this.","V W T \in \mathcal{L}(V, W) V W T T","['linear-algebra', 'matrices', 'linear-transformations']"
69,Sum of squared inner product of vector with spokes around unit circle is constant,Sum of squared inner product of vector with spokes around unit circle is constant,,"Let $v$ be any vector in the plane, and $\{w_i\}$ be $n>2$ vectors evenly spaced around the unit circle. Then it seems true that $$\sum_{i=1}^n (v\cdot w_i)^2 = k \|v\|^2$$ where $k$ is a constant independent of $v$ . My intuition is that the left-hand side, as a function of the argument $\theta$ of $v$ , has ""too much symmetry"" for its low frequency. Is there some slick proof of this identity, using e.g. the algebraic structure of the $n$ roots of unity?","Let be any vector in the plane, and be vectors evenly spaced around the unit circle. Then it seems true that where is a constant independent of . My intuition is that the left-hand side, as a function of the argument of , has ""too much symmetry"" for its low frequency. Is there some slick proof of this identity, using e.g. the algebraic structure of the roots of unity?",v \{w_i\} n>2 \sum_{i=1}^n (v\cdot w_i)^2 = k \|v\|^2 k v \theta v n,"['linear-algebra', 'plane-geometry']"
70,We have $n$ real numbers around the circle and among any consecutive 3 one is AM of the other two. Then all the numbers are the same or $3\mid n$.,We have  real numbers around the circle and among any consecutive 3 one is AM of the other two. Then all the numbers are the same or .,n 3\mid n,"There are $n$ real numbers around the circle and among any consecutive 3 one is arithmetic mean of the other two. Prove that all the numbers are the same or $3\mid n$. Hint was to use a linear algebra. It is obviously that if among some three consecutive numbers some two are the same then all are three the same: Say we have $$(a,a,b)\implies b ={a+a\over 2}= a\;\;\;{\rm or}\;\;\;a ={a+b\over 2} \implies a=b$$ But then all the numbers are the same. So we can assume that among any consecutive 3 there are all different. Any way, if all the number are $a_1,a_2,....,a_n$ then for any three consecutive (inidices are modulo $n$) we have $$a_{i-1}+a_i+a_{i+1} \equiv_3 0$$","There are $n$ real numbers around the circle and among any consecutive 3 one is arithmetic mean of the other two. Prove that all the numbers are the same or $3\mid n$. Hint was to use a linear algebra. It is obviously that if among some three consecutive numbers some two are the same then all are three the same: Say we have $$(a,a,b)\implies b ={a+a\over 2}= a\;\;\;{\rm or}\;\;\;a ={a+b\over 2} \implies a=b$$ But then all the numbers are the same. So we can assume that among any consecutive 3 there are all different. Any way, if all the number are $a_1,a_2,....,a_n$ then for any three consecutive (inidices are modulo $n$) we have $$a_{i-1}+a_i+a_{i+1} \equiv_3 0$$",,"['linear-algebra', 'combinatorics', 'vector-spaces', 'algebraic-combinatorics', 'hamel-basis']"
71,Reconstructing vectors from inner products,Reconstructing vectors from inner products,,"Suppose that I have two unknown vectors $v_1,v_2 \in \mathbb{R}^3$, and a known vector $v_{3} \in \mathbb{R}^3$. I know all inner products $\langle v_i, v_j \rangle$, $1 \leq i,j \leq 3$. Is there a closed-form (or least painful iterative) way to reconstruct $v_1$ and $v_2$? intuitively it looks like there might be two solutions since any $v_1$ and $v_2$ can be reflected through the known $v_3$ to get the same inner products. Nevertheless getting them both would be helpful. Note that the problem is not generally rotation invariant as $v_3$ is known.","Suppose that I have two unknown vectors $v_1,v_2 \in \mathbb{R}^3$, and a known vector $v_{3} \in \mathbb{R}^3$. I know all inner products $\langle v_i, v_j \rangle$, $1 \leq i,j \leq 3$. Is there a closed-form (or least painful iterative) way to reconstruct $v_1$ and $v_2$? intuitively it looks like there might be two solutions since any $v_1$ and $v_2$ can be reflected through the known $v_3$ to get the same inner products. Nevertheless getting them both would be helpful. Note that the problem is not generally rotation invariant as $v_3$ is known.",,"['linear-algebra', 'inner-products']"
72,Calculate eigenvalues and eigenvectors,Calculate eigenvalues and eigenvectors,,"I have this matrix below, and I need to find the eigenvalues.\begin{bmatrix}         2 & -1 & 0 \\         5 & 2 & 4 \\         0 & 1 & 2 \\         \end{bmatrix} This is what I have done so far: I used $\det(A-λI)=0$ and reached this form \begin{bmatrix}         2-λ & -1 & 0 \\         5 & 2-λ & 4 \\         0 & 1 & 2-λ \\        \end{bmatrix} I have done some simplifications: $(2-λ)[(2-λ)(2-λ)-4]-5(-(2-λ))=0$ $(2-λ)[(2-λ)(2-λ)-4]-5(-1)=0$ $(2-λ)[4-4λ+λ^2-4+5]=0$ $(2-λ)[λ^2-4λ+5]=0$ $λ^2(2-λ)-4λ(2-λ)+5(2-λ)=0$ $2λ^2-λ^3-8λ+4λ^2+10-5λ=0$ $-λ^3+6λ^2-13λ+10=0$ or $-λ(λ^2-6λ+13)+10=0$ $-λ (λ-(3+{\sqrt 22})) (λ+(3-{\sqrt 22}))+10=0$ Am I doing it right and if so: I checked the answer on Symbolab and it was $2,2-i,2+i$, How so? and what  is $i$? And is the matrix will be like this when I want to calculate the eigenvector for $2-i$ ??\begin{matrix}         i & -1 & 0 \\         5 & i & 4 \\         0 & 1 & i \\        \end{matrix}","I have this matrix below, and I need to find the eigenvalues.\begin{bmatrix}         2 & -1 & 0 \\         5 & 2 & 4 \\         0 & 1 & 2 \\         \end{bmatrix} This is what I have done so far: I used $\det(A-λI)=0$ and reached this form \begin{bmatrix}         2-λ & -1 & 0 \\         5 & 2-λ & 4 \\         0 & 1 & 2-λ \\        \end{bmatrix} I have done some simplifications: $(2-λ)[(2-λ)(2-λ)-4]-5(-(2-λ))=0$ $(2-λ)[(2-λ)(2-λ)-4]-5(-1)=0$ $(2-λ)[4-4λ+λ^2-4+5]=0$ $(2-λ)[λ^2-4λ+5]=0$ $λ^2(2-λ)-4λ(2-λ)+5(2-λ)=0$ $2λ^2-λ^3-8λ+4λ^2+10-5λ=0$ $-λ^3+6λ^2-13λ+10=0$ or $-λ(λ^2-6λ+13)+10=0$ $-λ (λ-(3+{\sqrt 22})) (λ+(3-{\sqrt 22}))+10=0$ Am I doing it right and if so: I checked the answer on Symbolab and it was $2,2-i,2+i$, How so? and what  is $i$? And is the matrix will be like this when I want to calculate the eigenvector for $2-i$ ??\begin{matrix}         i & -1 & 0 \\         5 & i & 4 \\         0 & 1 & i \\        \end{matrix}",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
73,Relation between rank and number of distinct eigenvalues [duplicate],Relation between rank and number of distinct eigenvalues [duplicate],,"This question already has answers here : What is the Relationship between the Rank and the Number of Eigenvalues? (4 answers) Closed 4 years ago . $3 \times 3$ matrix B has eigenvalues 0, 1 and 2. Find the rank of B. I understand that $0$ being an eigenvalue implies that rank of B is less than 3. The solution is here (right at the top).  It says that rank of B is 2 because the number of distinct non zero eigenvalues is 2. This thread says that the only information that the rank gives is the about the eigenvalue $0$ and its corresponding eigenvectors. What am I missing? Edit What I am really looking for is an explicit answer to this: ""Is the rank of a matrix equal to the number of distinct non zero eigenvalues of that matrix?"" Yes/No/Yes for some special cases","This question already has answers here : What is the Relationship between the Rank and the Number of Eigenvalues? (4 answers) Closed 4 years ago . matrix B has eigenvalues 0, 1 and 2. Find the rank of B. I understand that being an eigenvalue implies that rank of B is less than 3. The solution is here (right at the top).  It says that rank of B is 2 because the number of distinct non zero eigenvalues is 2. This thread says that the only information that the rank gives is the about the eigenvalue and its corresponding eigenvectors. What am I missing? Edit What I am really looking for is an explicit answer to this: ""Is the rank of a matrix equal to the number of distinct non zero eigenvalues of that matrix?"" Yes/No/Yes for some special cases",3 \times 3 0 0,"['linear-algebra', 'matrices']"
74,Why aren't vectors curved?,Why aren't vectors curved?,,"I don't understand why vectors can't be curved. For example when specifying the angle vector for an object in cylindrical/spherical coordinates, I think a curved vector would make more sense.","I don't understand why vectors can't be curved. For example when specifying the angle vector for an object in cylindrical/spherical coordinates, I think a curved vector would make more sense.",,"['linear-algebra', 'vector-spaces', 'vectors', 'intuition']"
75,Showing two matrix blocks are similar,Showing two matrix blocks are similar,,"Let $A \in M_n$ and $B,C \in M_m$ . Prove that if $$H= \begin{bmatrix}     A&0  \\     0 & B   \end{bmatrix}$$ is similar to $$K = \begin{bmatrix}     A&0  \\     0 & C   \end{bmatrix}$$ then $B$ is similar to $C$ . I am not sure how I would do this I know that if $H$ is similar to $K$ then for some non-singular matrix $S$ then $S^{-1} H S=K$ .",Let and . Prove that if is similar to then is similar to . I am not sure how I would do this I know that if is similar to then for some non-singular matrix then .,"A \in M_n B,C \in M_m H= \begin{bmatrix}
    A&0  \\
    0 & B
  \end{bmatrix} K = \begin{bmatrix}
    A&0  \\
    0 & C
  \end{bmatrix} B C H K S S^{-1} H S=K","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'block-matrices', 'similar-matrices']"
76,How to solve this determinant,How to solve this determinant,,"Question Statment:- Show that   \begin{align*} \begin{vmatrix} (a+b)^2 & ca & bc \\  ca & (b+c)^2 & ab \\ bc & ab & (c+a)^2 \\  \end{vmatrix} =2abc(a+b+c)^3 \end{align*} My Attempt:- $$\begin{aligned} &\begin{vmatrix} \\(a+b)^2 & ca & bc \\  \\ca & (b+c)^2 & ab \\ \\bc & ab & (c+a)^2 \\\ \end{vmatrix}\\ =&\begin{vmatrix} \\a^2+b^2+2ab & ca & bc \\  \\ca & b^2+c^2+2bc & ab \\ \\bc & ab & c^2+a^2+2ac \\\ \end{vmatrix}\\ =&\dfrac{1}{abc}\begin{vmatrix} \\ca^2+cb^2+2abc & ca^2 & b^2c \\  \\ac^2 & ab^2+ac^2+2abc & ab^2 \\ \\bc^2 & a^2b & bc^2+a^2b+2abc \\\ \end{vmatrix}\\\\ &\qquad (C_1\rightarrow cC_1, C_2\rightarrow aC_2, C_3\rightarrow bC_3)\\\\ =&\dfrac{2}{abc}\times\begin{vmatrix} \\ca^2+cb^2+abc & ca^2 & b^2c \\  \\ab^2+ac^2+abc & ab^2+ac^2+2abc & ab^2 \\ \\bc^2+a^2b+abc & a^2b & bc^2+a^2b+2abc \\\ \end{vmatrix}\\\\ &\qquad (C_1\rightarrow C_1+C_2+C_3)\\\\ =&\dfrac{2abc}{abc}\left(\begin{vmatrix} \\a^2+b^2 & a^2 & b^2 \\  \\b^2+c^2 & b^2+c^2+2bc & b^2 \\ \\c^2+a^2 & a^2 & c^2+a^2+2ac \\\ \end{vmatrix}+ \begin{vmatrix} \\1 & ca^2 & b^2c \\  \\1 & ab^2+ac^2+2abc & ab^2 \\ \\1 & a^2b & bc^2+a^2b+2abc \\\ \end{vmatrix}\right) \end{aligned}$$ The second determinant in the last step can be simplified to  \begin{vmatrix} \\1 & ca^2 & b^2c \\  \\0 & ab^2+ac^2+2abc-ca^2 & ab^2-b^2c \\ \\0 & a^2b-ca^2 & bc^2+a^2b+2abc-b^2c \\\ \end{vmatrix} I couldn't proceed further with this, so your help will be appreciated and if any other simpler way is possible please do post it too.","Question Statment:- Show that   \begin{align*} \begin{vmatrix} (a+b)^2 & ca & bc \\  ca & (b+c)^2 & ab \\ bc & ab & (c+a)^2 \\  \end{vmatrix} =2abc(a+b+c)^3 \end{align*} My Attempt:- $$\begin{aligned} &\begin{vmatrix} \\(a+b)^2 & ca & bc \\  \\ca & (b+c)^2 & ab \\ \\bc & ab & (c+a)^2 \\\ \end{vmatrix}\\ =&\begin{vmatrix} \\a^2+b^2+2ab & ca & bc \\  \\ca & b^2+c^2+2bc & ab \\ \\bc & ab & c^2+a^2+2ac \\\ \end{vmatrix}\\ =&\dfrac{1}{abc}\begin{vmatrix} \\ca^2+cb^2+2abc & ca^2 & b^2c \\  \\ac^2 & ab^2+ac^2+2abc & ab^2 \\ \\bc^2 & a^2b & bc^2+a^2b+2abc \\\ \end{vmatrix}\\\\ &\qquad (C_1\rightarrow cC_1, C_2\rightarrow aC_2, C_3\rightarrow bC_3)\\\\ =&\dfrac{2}{abc}\times\begin{vmatrix} \\ca^2+cb^2+abc & ca^2 & b^2c \\  \\ab^2+ac^2+abc & ab^2+ac^2+2abc & ab^2 \\ \\bc^2+a^2b+abc & a^2b & bc^2+a^2b+2abc \\\ \end{vmatrix}\\\\ &\qquad (C_1\rightarrow C_1+C_2+C_3)\\\\ =&\dfrac{2abc}{abc}\left(\begin{vmatrix} \\a^2+b^2 & a^2 & b^2 \\  \\b^2+c^2 & b^2+c^2+2bc & b^2 \\ \\c^2+a^2 & a^2 & c^2+a^2+2ac \\\ \end{vmatrix}+ \begin{vmatrix} \\1 & ca^2 & b^2c \\  \\1 & ab^2+ac^2+2abc & ab^2 \\ \\1 & a^2b & bc^2+a^2b+2abc \\\ \end{vmatrix}\right) \end{aligned}$$ The second determinant in the last step can be simplified to  \begin{vmatrix} \\1 & ca^2 & b^2c \\  \\0 & ab^2+ac^2+2abc-ca^2 & ab^2-b^2c \\ \\0 & a^2b-ca^2 & bc^2+a^2b+2abc-b^2c \\\ \end{vmatrix} I couldn't proceed further with this, so your help will be appreciated and if any other simpler way is possible please do post it too.",,"['linear-algebra', 'determinant']"
77,Why all irreducible representations appear in the regular representation?,Why all irreducible representations appear in the regular representation?,,"Let $G$ be a finite group and $R$ the regular representation. That is, as a vector space $R = F(G)$ is the free vector space with basis $G$ . If the basis is $\{e_g : g \in G\}$ the action is defined by $$g \cdot e_{g'}=e_{gg'}$$ and extended by linearity. Now, in the book I'm studying the author states the following corolary: Corollary 2.18 : Any irreducible representation $V$ of $G$ over an algebraically closed field of characteristic $0$ appears in the regular representation $\dim V$ times . The ""proof"" for this is a little argument before the statement: We know tha the character of $R$ is simply $$\chi_R(g)=\begin{cases}0, & g\neq e, \\ |G|, & g= e\end{cases}$$ Thus, we see first of all that $R$ is not irreducible if $G\neq \{e\}$ . In fact, if we set $R = \bigoplus V_i^{\oplus a_i}$ , with $V_i$ distinct irreducibles, then: $$a_i = (\chi_{V_i},\chi_R)=\dfrac{1}{|G|}\chi_{V_i}(e)|G|=\dim V_i.$$ All I get from that is: if we decompose $R$ into a direct sum of irreducible representations, the multiplicites are the dimensions. But what guarantees that any irreducible representation of $G$ appears in that decomposition of $R$ ? Why all irreducible representations of $G$ appear in the direct sum decomposition of the regular representation?","Let be a finite group and the regular representation. That is, as a vector space is the free vector space with basis . If the basis is the action is defined by and extended by linearity. Now, in the book I'm studying the author states the following corolary: Corollary 2.18 : Any irreducible representation of over an algebraically closed field of characteristic appears in the regular representation times . The ""proof"" for this is a little argument before the statement: We know tha the character of is simply Thus, we see first of all that is not irreducible if . In fact, if we set , with distinct irreducibles, then: All I get from that is: if we decompose into a direct sum of irreducible representations, the multiplicites are the dimensions. But what guarantees that any irreducible representation of appears in that decomposition of ? Why all irreducible representations of appear in the direct sum decomposition of the regular representation?","G R R = F(G) G \{e_g : g \in G\} g \cdot e_{g'}=e_{gg'} V G 0 \dim V R \chi_R(g)=\begin{cases}0, & g\neq e, \\ |G|, & g= e\end{cases} R G\neq \{e\} R = \bigoplus V_i^{\oplus a_i} V_i a_i = (\chi_{V_i},\chi_R)=\dfrac{1}{|G|}\chi_{V_i}(e)|G|=\dim V_i. R G R G","['linear-algebra', 'abstract-algebra', 'group-theory', 'finite-groups', 'representation-theory']"
78,finding eigenvectors given eigenvalues,finding eigenvectors given eigenvalues,,"i have the matrix: $A = \begin{bmatrix}8 & -2\\-2 & 5\end{bmatrix}$ i want to find its eigenvectors and eigenvalues. by the characteristic equation: $\textrm{det}(A - \lambda I) = 0$ expanding the determinant: $\begin{bmatrix}8 - \lambda & -2\\-2 & 5-\lambda\end{bmatrix} = \lambda^2 - 13\lambda + 36 = 0$ using the quadratic formula, $\lambda = 9$ or $\lambda = 4$, so the two eigenvalues are $\{9,4\}$. when i try to get the eigenvectors, i run into problems. i plugin $\lambda = 9$ into the characteristic polynomial equation: $\begin{bmatrix}-1 & -2\\-2 & -4\end{bmatrix}\begin{bmatrix}v_1\\v_2\end{bmatrix} = 0$ resulting in: $-v_1-2v_2 = 0$ $-2v_1 - 4v_2 = 0$ How can this be solved to get the eigenvector: $[v_1 v_2]^T$? I just get $v_1 = v_2 = 0$ which does not help.","i have the matrix: $A = \begin{bmatrix}8 & -2\\-2 & 5\end{bmatrix}$ i want to find its eigenvectors and eigenvalues. by the characteristic equation: $\textrm{det}(A - \lambda I) = 0$ expanding the determinant: $\begin{bmatrix}8 - \lambda & -2\\-2 & 5-\lambda\end{bmatrix} = \lambda^2 - 13\lambda + 36 = 0$ using the quadratic formula, $\lambda = 9$ or $\lambda = 4$, so the two eigenvalues are $\{9,4\}$. when i try to get the eigenvectors, i run into problems. i plugin $\lambda = 9$ into the characteristic polynomial equation: $\begin{bmatrix}-1 & -2\\-2 & -4\end{bmatrix}\begin{bmatrix}v_1\\v_2\end{bmatrix} = 0$ resulting in: $-v_1-2v_2 = 0$ $-2v_1 - 4v_2 = 0$ How can this be solved to get the eigenvector: $[v_1 v_2]^T$? I just get $v_1 = v_2 = 0$ which does not help.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
79,Show that a field extension $L/K$ is separable iff the trace form is non-degenerate.,Show that a field extension  is separable iff the trace form is non-degenerate.,L/K,Let $L$ be a finite field extension of $K$. I have the following question: Show that $L/K$ is separable if and only if the bilinear trace form $\text{Tr}_{L/K}:L\times L \to K$ is non-degenerate. A proof or reference will be great. Thank you!,Let $L$ be a finite field extension of $K$. I have the following question: Show that $L/K$ is separable if and only if the bilinear trace form $\text{Tr}_{L/K}:L\times L \to K$ is non-degenerate. A proof or reference will be great. Thank you!,,"['linear-algebra', 'extension-field', 'bilinear-form']"
80,"How many 1's can a regular 0,1-matrix contain?","How many 1's can a regular 0,1-matrix contain?",,"A matrix of order $n$ has all of its entries in $\{0,1\}$. What is the maximum number of $1$ in the matrix for which the matrix is non singular.","A matrix of order $n$ has all of its entries in $\{0,1\}$. What is the maximum number of $1$ in the matrix for which the matrix is non singular.",,"['linear-algebra', 'matrices']"
81,How to verify whether R^2 is a subspace of the complex vector space C^2?,How to verify whether R^2 is a subspace of the complex vector space C^2?,,"It's an exercise of the book Linear Algebra Done Right. I'm not clear about how to prove these problems, would you please offer me some suggestion about how to improve this kind of ability, thanks a lot.","It's an exercise of the book Linear Algebra Done Right. I'm not clear about how to prove these problems, would you please offer me some suggestion about how to improve this kind of ability, thanks a lot.",,['linear-algebra']
82,Rank of a Matrix Sum [duplicate],Rank of a Matrix Sum [duplicate],,"This question already has answers here : Show $\operatorname{rank}(A) + \operatorname{rank}(B) \ge \operatorname{rank}(A+B)$ [duplicate] (2 answers) Closed 8 years ago . I have $3\times3$ matrices such that $S=A+B$ . I know there is an inequality connecting rank of the matrices $A$ , $B$ and its sum $S$ . Could you write down that here? It will be a great help for me. Means equation  or inequality connecting $\operatorname{rank}(S)$ , $\operatorname{rank}(A)$ and $\operatorname{rank}(B)$ .","This question already has answers here : Show $\operatorname{rank}(A) + \operatorname{rank}(B) \ge \operatorname{rank}(A+B)$ [duplicate] (2 answers) Closed 8 years ago . I have matrices such that . I know there is an inequality connecting rank of the matrices , and its sum . Could you write down that here? It will be a great help for me. Means equation  or inequality connecting , and .",3\times3 S=A+B A B S \operatorname{rank}(S) \operatorname{rank}(A) \operatorname{rank}(B),"['linear-algebra', 'matrices', 'inequality', 'matrix-rank']"
83,Volume of ellipsoid using Linear Algebra,Volume of ellipsoid using Linear Algebra,,Can someone tell me how to find the volume of an ellipsoid of dimension $\mathbb{R}^3$ by using linear algebra? I know the formula is $\frac{4}{3}\pi abc$. I am given the equation $$\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$$,Can someone tell me how to find the volume of an ellipsoid of dimension $\mathbb{R}^3$ by using linear algebra? I know the formula is $\frac{4}{3}\pi abc$. I am given the equation $$\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$$,,"['linear-algebra', 'geometry', 'multivariable-calculus', 'volume']"
84,Proving equivalence relations,Proving equivalence relations,,"I just started my abstract algebra class and I am struggling with the concept of equivalence relations.  I know that in order to prove equivalence relations, I have to prove the reflexive, symmetric, and transitive properties.  However, I don't know how to go about starting the actual proof or solution.  I have these examples and any help would be appreciated. I have to show which of the following are equivalence relations on the set of real numbers and, if they are not, why. $a\sim b$ iff $|a|=|b|$ $a\sim b$ iff $a\leq b$ $a\sim b$ iff $|a-b| \leq 1$ Thank you for any help!","I just started my abstract algebra class and I am struggling with the concept of equivalence relations.  I know that in order to prove equivalence relations, I have to prove the reflexive, symmetric, and transitive properties.  However, I don't know how to go about starting the actual proof or solution.  I have these examples and any help would be appreciated. I have to show which of the following are equivalence relations on the set of real numbers and, if they are not, why. $a\sim b$ iff $|a|=|b|$ $a\sim b$ iff $a\leq b$ $a\sim b$ iff $|a-b| \leq 1$ Thank you for any help!",,"['linear-algebra', 'abstract-algebra', 'inequality', 'proof-verification', 'equivalence-relations']"
85,Prove null $T^k$ = null $T$ and range $T^k$ = range $T$,Prove null  = null  and range  = range,T^k T T^k T,"I'm trying to prove that if $T$ is a normal operator, then null $T^k$ = null $T$ and range $T^k$ = range $T$. Showing null $T$ $\subset$ null $T^k$ is simple, so I'm working on the other inclusion. So far I've been able to deduce that for a vector $v \in$ null $T^k$ we have $TT^\star v = T^\star Tv$ $\implies$ $T^k T^\star v = T^\star T^k v$ $\implies$ $T^k T^\star v = 0$ $\implies$ $T^\star v \in$ null $T^k$. I'm not sure if this is useful though, and I'm stuck on where I should go from here.","I'm trying to prove that if $T$ is a normal operator, then null $T^k$ = null $T$ and range $T^k$ = range $T$. Showing null $T$ $\subset$ null $T^k$ is simple, so I'm working on the other inclusion. So far I've been able to deduce that for a vector $v \in$ null $T^k$ we have $TT^\star v = T^\star Tv$ $\implies$ $T^k T^\star v = T^\star T^k v$ $\implies$ $T^k T^\star v = 0$ $\implies$ $T^\star v \in$ null $T^k$. I'm not sure if this is useful though, and I'm stuck on where I should go from here.",,['linear-algebra']
86,Minimal Spanning Set vs Basis of a vector space,Minimal Spanning Set vs Basis of a vector space,,I read the following in my textbook: Find as small a set of vectors that span the row space of $A$ as you can. Such a set is called a minimal spanning set. Is this terminology synonymous with the basis of the vector space? A basis is also made up of the largest set of linearly independent vectors that span a vector space.,I read the following in my textbook: Find as small a set of vectors that span the row space of $A$ as you can. Such a set is called a minimal spanning set. Is this terminology synonymous with the basis of the vector space? A basis is also made up of the largest set of linearly independent vectors that span a vector space.,,"['linear-algebra', 'terminology']"
87,Prove：$x^d-1 \mid x^n-1$ iff $d \mid n$.,Prove： iff .,x^d-1 \mid x^n-1 d \mid n,"Consider the polynomial ring $F\left[x\right]$ over a field $F$ . Let $d$ and $n$ be two nonnegative integers. Prove： $x^d-1 \mid x^n-1$ iff $d \mid n$ . my tries: necessity, Let $n=d t+r$ , $0\le r<d$ since $x^d-1 \mid x^n-1$ , so, $x^n-1=\left(x^d-1\right)\left(x^{\text{dt}+r-d}+\dots+1\right)$ ... so,,, to prove $r=0$ ? I don't know, and I can't go on. How to do it.","Consider the polynomial ring over a field . Let and be two nonnegative integers. Prove： iff . my tries: necessity, Let , since , so, ... so,,, to prove ? I don't know, and I can't go on. How to do it.",F\left[x\right] F d n x^d-1 \mid x^n-1 d \mid n n=d t+r 0\le r<d x^d-1 \mid x^n-1 x^n-1=\left(x^d-1\right)\left(x^{\text{dt}+r-d}+\dots+1\right) r=0,"['linear-algebra', 'polynomials']"
88,How does one obtain the Jordan normal form of a matrix $A$ by studying $XI-A$?,How does one obtain the Jordan normal form of a matrix  by studying ?,A XI-A,"In our lecture notes, there's the following example problem. Find a Jordan normal form matrix that is similar to the following. $$A=\begin{bmatrix}2 & 0 & 0 & 0\\-1 & 1 & 0 & 0\\0 & -1 & 0 & -1\\1 & 1 & 1 & 2\end{bmatrix}$$ The solution begins by demonstrating that the matrix $XI-A$ is equivalent to a matrix of the form $$B=\begin{bmatrix}-1 & 0 & 0 & 0\\0 & 1 & 0 & 0\\0 & 0 & -(X-1) & 0\\0 & 0 & 0 & (X-2)(X-1)^2\end{bmatrix}$$ From here, the notes immediately deduce that the Jordan normal form of $A$ is $$J=\begin{bmatrix}1 & 0 & 0 & 0\\0 & 1 & 0 & 0\\0 & 1 & 1 & 0\\0 & 0 & 0 & 2\end{bmatrix}.$$ I'm struggling to understand this final step - how do we get from $B$ to $J$? I get that $J$ consists of three elementary Jordan matrices, but how are these matrices actually found?","In our lecture notes, there's the following example problem. Find a Jordan normal form matrix that is similar to the following. $$A=\begin{bmatrix}2 & 0 & 0 & 0\\-1 & 1 & 0 & 0\\0 & -1 & 0 & -1\\1 & 1 & 1 & 2\end{bmatrix}$$ The solution begins by demonstrating that the matrix $XI-A$ is equivalent to a matrix of the form $$B=\begin{bmatrix}-1 & 0 & 0 & 0\\0 & 1 & 0 & 0\\0 & 0 & -(X-1) & 0\\0 & 0 & 0 & (X-2)(X-1)^2\end{bmatrix}$$ From here, the notes immediately deduce that the Jordan normal form of $A$ is $$J=\begin{bmatrix}1 & 0 & 0 & 0\\0 & 1 & 0 & 0\\0 & 1 & 1 & 0\\0 & 0 & 0 & 2\end{bmatrix}.$$ I'm struggling to understand this final step - how do we get from $B$ to $J$? I get that $J$ consists of three elementary Jordan matrices, but how are these matrices actually found?",,"['linear-algebra', 'matrices', 'modules', 'jordan-normal-form']"
89,Tests for positive definiteness of nonsymmetric matrices,Tests for positive definiteness of nonsymmetric matrices,,Do tests for positive definiteness for nonsymmetric matrices exist?  More specifically I am working with bidiagonal upper/lower triangular matrices with positive eigenvalues and I need to check to see if they are positive definite or not.,Do tests for positive definiteness for nonsymmetric matrices exist?  More specifically I am working with bidiagonal upper/lower triangular matrices with positive eigenvalues and I need to check to see if they are positive definite or not.,,"['linear-algebra', 'matrices', 'positive-definite']"
90,Invariant Factors vs. Elementary Divisors,Invariant Factors vs. Elementary Divisors,,"I have been studying Cooperstein's Advanced Linear Algebra for about seven months now and I am having problems understanding how to find the elementary divisors of a linear operator and how to find the invariant factors of a linear operator as well.  I feel as though if I'm having trouble with these then I won't be able to move forward.  I understand that the invariant factors comprise the characteristic polynomial and can find them using determinants and eigenvalues (I get that), but the text builds the theory up using direct sums, minimal polynomials of T, and T-cyclic subgroups and I'm just not seeing it. As for an example,  Let $T$ be an element of $L(\mathbf R^4,\mathbf R^4)$ be the operator given by $$ T(v) = \begin{pmatrix} -3&2&2&-4\\-3&1&4&-4\\-2&0&3&-2\\-1&0&2&-1\end{pmatrix}(v) $$ Determine the elementary divisors and the invariant factors of $T$. (how do I write the correct matrix in LaTeX?)","I have been studying Cooperstein's Advanced Linear Algebra for about seven months now and I am having problems understanding how to find the elementary divisors of a linear operator and how to find the invariant factors of a linear operator as well.  I feel as though if I'm having trouble with these then I won't be able to move forward.  I understand that the invariant factors comprise the characteristic polynomial and can find them using determinants and eigenvalues (I get that), but the text builds the theory up using direct sums, minimal polynomials of T, and T-cyclic subgroups and I'm just not seeing it. As for an example,  Let $T$ be an element of $L(\mathbf R^4,\mathbf R^4)$ be the operator given by $$ T(v) = \begin{pmatrix} -3&2&2&-4\\-3&1&4&-4\\-2&0&3&-2\\-1&0&2&-1\end{pmatrix}(v) $$ Determine the elementary divisors and the invariant factors of $T$. (how do I write the correct matrix in LaTeX?)",,"['linear-algebra', 'invariant-theory']"
91,Show that $\operatorname{rank}(A^2+A+I_3)=1$,Show that,\operatorname{rank}(A^2+A+I_3)=1,"If $A \in M_3(\mathbb{R}), A \ne I_3 $ and $A^3=I_3$ Show that $\operatorname{rank}(A^2+A+I_3)=1$ . What I have reached so far is that $\operatorname{rank}(A-I_3)+\operatorname{rank}(A^2+A+I_3)\le 3$ using Sylvester theorem, and also it can be quite easily proven that $\operatorname{rank}(A^2+A+I_3) \ne 0$ , if that helps in any way, but I have no idea what should I do now. I've seen this problem statement in the archives of a contest, but no official solution is provided.","If and Show that . What I have reached so far is that using Sylvester theorem, and also it can be quite easily proven that , if that helps in any way, but I have no idea what should I do now. I've seen this problem statement in the archives of a contest, but no official solution is provided.","A \in M_3(\mathbb{R}), A \ne I_3  A^3=I_3 \operatorname{rank}(A^2+A+I_3)=1 \operatorname{rank}(A-I_3)+\operatorname{rank}(A^2+A+I_3)\le 3 \operatorname{rank}(A^2+A+I_3) \ne 0","['linear-algebra', 'matrices', 'contest-math']"
92,Transpose of the differential operator,Transpose of the differential operator,,"Is the differential operator ${d\over dx}$ antisymmetric? If so, what does it even mean to take it's transpose? Thank you.","Is the differential operator ${d\over dx}$ antisymmetric? If so, what does it even mean to take it's transpose? Thank you.",,"['linear-algebra', 'differential-operators']"
93,"If the covariance matrix is $\Sigma$, the covariance after projecting in $u$ is $u^T \Sigma u$. Why?","If the covariance matrix is , the covariance after projecting in  is . Why?",\Sigma u u^T \Sigma u,"I read in this answer that: If covariance matrix is $\Sigma$, the covariance after projecting in   $u$ is $u^T \Sigma u$. I fail to see this, how do I get the covariance of a set of points after projecting those points along the direction $u$ as a function of $u$ and $\Sigma$ ?","I read in this answer that: If covariance matrix is $\Sigma$, the covariance after projecting in   $u$ is $u^T \Sigma u$. I fail to see this, how do I get the covariance of a set of points after projecting those points along the direction $u$ as a function of $u$ and $\Sigma$ ?",,"['linear-algebra', 'statistics']"
94,Classification of simple modules for algebra of upper triangular matrices?,Classification of simple modules for algebra of upper triangular matrices?,,"I've been refreshing my linear algebra, and this is a question of curiosity I have. Let $U:=U_n(F)$ be the algebra of upper triangular $n\times n$ matrices over a field $F$. Is there a classification of all simple $U$-modules (up to isomorphism of course)? I've been researching around, but didn't find any relevant results on first look. I'd also be happy for a reference showing such classification if one exists. Thank you.","I've been refreshing my linear algebra, and this is a question of curiosity I have. Let $U:=U_n(F)$ be the algebra of upper triangular $n\times n$ matrices over a field $F$. Is there a classification of all simple $U$-modules (up to isomorphism of course)? I've been researching around, but didn't find any relevant results on first look. I'd also be happy for a reference showing such classification if one exists. Thank you.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'reference-request', 'modules']"
95,How do we show every linear transformation which is not bijective is the difference of bijective linear transforms?,How do we show every linear transformation which is not bijective is the difference of bijective linear transforms?,,"I have been reviewing some ideas about vector spaces and came upon a surprising fact. I am not quite sure how to begin the argument because the problem requires one to construct two bijective linear transformations whose difference is equal to a given linear transformation. Let $V$ be a vector space over a filed $F$.  Suppose $\phi:V \rightarrow V$ is a linear transformation that is not a bijection. How do we show $\exists f,g :V \rightarrow V$ that are both bjiective linear transformations such that $\phi = f - g$. I tried proving the fact using contradiction but have not been able to get to far so I am wondering if there is a standard constructive proof that applies directly.","I have been reviewing some ideas about vector spaces and came upon a surprising fact. I am not quite sure how to begin the argument because the problem requires one to construct two bijective linear transformations whose difference is equal to a given linear transformation. Let $V$ be a vector space over a filed $F$.  Suppose $\phi:V \rightarrow V$ is a linear transformation that is not a bijection. How do we show $\exists f,g :V \rightarrow V$ that are both bjiective linear transformations such that $\phi = f - g$. I tried proving the fact using contradiction but have not been able to get to far so I am wondering if there is a standard constructive proof that applies directly.",,"['linear-algebra', 'vector-spaces']"
96,Correspondences between linear transformations and matrices,Correspondences between linear transformations and matrices,,"EDIT: I value all the answers given to me in this question, and upon reading them and thinking about the question for some time, I have come up with my own answer and explanation for this question. I have typed this up and submitted it as an answer here, so that people may read it and can hopefully let me know if my current understanding is fully correct and useful. And furthermore if it is incorrect I would very much appreciate corrections and amendments to my answer. Thank you. I am a bit confused about representing linear transformations as matrices, I will try to explain my confusion here, and hopefully someone can clear it up. Note: Let $V$ and $V'$ be finite dimensional vector spaces over an arbitrary field $F$ such that $\operatorname{dim}(V) = n$ and $\operatorname{dim}(V') = m$, $V$ has basis $B$, and $V'$ has basis $B'$. Now we want to create an isomorphism: $f: \operatorname{Hom}(V,V') \rightarrow M_{m\times n}(F)$ According to notes we map  $$ h \mapsto [h]^{B'}_{B} = [P^{-1}_{B'}hP_{B}] =  \left( \begin{array}{cc} P^{-1}_{B'}\circ h\circ P_B \begin{pmatrix} 1\\ 0 \\ \vdots \\ 0\end{pmatrix} &\cdots &P^{-1}_{B'}\circ h\circ P_B\begin{pmatrix} 0\\ 0 \\ \vdots\\ 1\end{pmatrix} \end{array} \right)$$ Where $P_B : F^n \rightarrow V$ is defined by $$\begin{pmatrix} a_1 \\ \vdots \\ a_n \end{pmatrix} \mapsto a_1v_1 + ... + a_nv_n$$ Now, I'm quite confused as to where this mapping came from, and furthermore I'm confused as to exactly what matrix we are mapping $h$ to, and why is this matrix called the matrix of $h$ with respect to $B$ and $B'$ ($[h]^{B'}_{B}$). I apologize if the question isn't quite clear and thank you in advance for any answers.","EDIT: I value all the answers given to me in this question, and upon reading them and thinking about the question for some time, I have come up with my own answer and explanation for this question. I have typed this up and submitted it as an answer here, so that people may read it and can hopefully let me know if my current understanding is fully correct and useful. And furthermore if it is incorrect I would very much appreciate corrections and amendments to my answer. Thank you. I am a bit confused about representing linear transformations as matrices, I will try to explain my confusion here, and hopefully someone can clear it up. Note: Let $V$ and $V'$ be finite dimensional vector spaces over an arbitrary field $F$ such that $\operatorname{dim}(V) = n$ and $\operatorname{dim}(V') = m$, $V$ has basis $B$, and $V'$ has basis $B'$. Now we want to create an isomorphism: $f: \operatorname{Hom}(V,V') \rightarrow M_{m\times n}(F)$ According to notes we map  $$ h \mapsto [h]^{B'}_{B} = [P^{-1}_{B'}hP_{B}] =  \left( \begin{array}{cc} P^{-1}_{B'}\circ h\circ P_B \begin{pmatrix} 1\\ 0 \\ \vdots \\ 0\end{pmatrix} &\cdots &P^{-1}_{B'}\circ h\circ P_B\begin{pmatrix} 0\\ 0 \\ \vdots\\ 1\end{pmatrix} \end{array} \right)$$ Where $P_B : F^n \rightarrow V$ is defined by $$\begin{pmatrix} a_1 \\ \vdots \\ a_n \end{pmatrix} \mapsto a_1v_1 + ... + a_nv_n$$ Now, I'm quite confused as to where this mapping came from, and furthermore I'm confused as to exactly what matrix we are mapping $h$ to, and why is this matrix called the matrix of $h$ with respect to $B$ and $B'$ ($[h]^{B'}_{B}$). I apologize if the question isn't quite clear and thank you in advance for any answers.",,"['linear-algebra', 'matrices']"
97,Question about the nature of coordinate systems,Question about the nature of coordinate systems,,"I couldn't really think of a good one line title for my question, so I will try to elaborate. From what I have sort of gathered over my years, if you want to locate an arbitrary point in an n-dimensional coordinate space, you need at least n coordinates.  This was sort of implied in my Linear Algebra class about linear coordinates, I think ... but I sort of felt that there was a further implication that this requirement applies to all types of coordinate systems, including angle-based ones like Polar coordinates. For example, if I wanted to describe a 3D point, I can choose whatever coordinate system I want, but that coordinate system must require at least 3 points to be able to uniquely identify my point.  That's why cartesian $(x,y,z)$ , cylindrical $(r,\theta,z)$ , and spherical coordinates $(r,\theta,\phi)$ work, as well as a coordinate system relying on a linear combination of three linearly independent vectors (cartesian being a special case of this where the three vectors are orthogonal unit vectors).  Furthermore, if a linear combination coordinate system requires four or more coordinates but only describes points on one 3D space, that coordinate system has redundancies and may be reduced to only needing three coordinates. I was then taught, in Physics, that a point in that 3D space was an abstract entity, and that the coordinate system is irrelevant to the nature of the point -- to describe it, all you needed was to chose a set of bases.  Any bases will do -- as long as there are three of them. I sort of took this generalization to other spaces besides just the n-dimensional real number space.  For example, the human color space, I figured, has 3 dimensions, and that's why all of the current color systems (RGB, HSV, HSL, the varies CIE perceptional color spaces) all require three coordinates, and you can't describe all colors with only two coordinates, and a coordinate system with four coordinates is redundant. Also, there is the space of all possible rectangles.  A rectangle can be uniquely described by its height and width, but there are many other ways, all of which require at least two numbers. And all ""redundant"" systems may be reduced to a non-redundant one (similar to how vectors that aren't linearly independent can be reduced to a set of linearly independent vectors that can combine to form the original ones) Thank you if you are still reading at this point; I know that that was kinda long-winded.  I have two questions, that are somewhat related. Is what I am thinking true?  Did I make too many vast generalizations?  What is wrong about this line of thought, if anything?  I'm sure I made too many jumps somewhere. One thing that has been troubling me is the ""trilateration"" ""coordinate system"" used to find points on a 2D plane.  Coordinate system is defined as: There are three reference points at arbitrary locations on the plane.  The coordinate is given as $(r_1,r_2,r_3)$ , where $r_n$ is the distance of the given point from the $n$ th anchor point. This bugs me because to locate a 2D point using this coordinate system, you need three coordinates.  Should it not be that, because this is a 2D plane, you only need two coordinates?  How does something like this exist?  What makes it fundamentally different from the other coordinate systems/spaces I have mentioned previously? (Cartesian, polar, color space, rectangles, etc.)","I couldn't really think of a good one line title for my question, so I will try to elaborate. From what I have sort of gathered over my years, if you want to locate an arbitrary point in an n-dimensional coordinate space, you need at least n coordinates.  This was sort of implied in my Linear Algebra class about linear coordinates, I think ... but I sort of felt that there was a further implication that this requirement applies to all types of coordinate systems, including angle-based ones like Polar coordinates. For example, if I wanted to describe a 3D point, I can choose whatever coordinate system I want, but that coordinate system must require at least 3 points to be able to uniquely identify my point.  That's why cartesian , cylindrical , and spherical coordinates work, as well as a coordinate system relying on a linear combination of three linearly independent vectors (cartesian being a special case of this where the three vectors are orthogonal unit vectors).  Furthermore, if a linear combination coordinate system requires four or more coordinates but only describes points on one 3D space, that coordinate system has redundancies and may be reduced to only needing three coordinates. I was then taught, in Physics, that a point in that 3D space was an abstract entity, and that the coordinate system is irrelevant to the nature of the point -- to describe it, all you needed was to chose a set of bases.  Any bases will do -- as long as there are three of them. I sort of took this generalization to other spaces besides just the n-dimensional real number space.  For example, the human color space, I figured, has 3 dimensions, and that's why all of the current color systems (RGB, HSV, HSL, the varies CIE perceptional color spaces) all require three coordinates, and you can't describe all colors with only two coordinates, and a coordinate system with four coordinates is redundant. Also, there is the space of all possible rectangles.  A rectangle can be uniquely described by its height and width, but there are many other ways, all of which require at least two numbers. And all ""redundant"" systems may be reduced to a non-redundant one (similar to how vectors that aren't linearly independent can be reduced to a set of linearly independent vectors that can combine to form the original ones) Thank you if you are still reading at this point; I know that that was kinda long-winded.  I have two questions, that are somewhat related. Is what I am thinking true?  Did I make too many vast generalizations?  What is wrong about this line of thought, if anything?  I'm sure I made too many jumps somewhere. One thing that has been troubling me is the ""trilateration"" ""coordinate system"" used to find points on a 2D plane.  Coordinate system is defined as: There are three reference points at arbitrary locations on the plane.  The coordinate is given as , where is the distance of the given point from the th anchor point. This bugs me because to locate a 2D point using this coordinate system, you need three coordinates.  Should it not be that, because this is a 2D plane, you only need two coordinates?  How does something like this exist?  What makes it fundamentally different from the other coordinate systems/spaces I have mentioned previously? (Cartesian, polar, color space, rectangles, etc.)","(x,y,z) (r,\theta,z) (r,\theta,\phi) (r_1,r_2,r_3) r_n n","['linear-algebra', 'coordinate-systems']"
98,Are vectors in one dimension scalar?,Are vectors in one dimension scalar?,,"A physics textbook that I use mentions that ""If a physical quantity has magnitude as well as direction but doesn't add up according to the triangle rule, it will not be called a vector quantity"". The paragraph further explains that electric current is not a vector quantity because there is no meaning of triangle law there. While I understand why electric current is a scalar quantity, I am unable to comprehend how definite the rule is. For all I think, in the case of a right angled triangle, the rule is just calculating the hypotenuse (resultant vector). In dimensions two and above, it would make complete sense. However, in one dimension  there isn't any angle involved and each point depends on only one real number. The two directions: left and right can be dealt with two sets of real numbers, i.e., negative and positive. I want to know why vectors in one dimension can't be written as scalars. Why they must be written in the form of $x\hat{\imath}+0\hat {\jmath}+0\hat k$ when they can be mentioned simply as the scalar number on the $x$ axis.","A physics textbook that I use mentions that ""If a physical quantity has magnitude as well as direction but doesn't add up according to the triangle rule, it will not be called a vector quantity"". The paragraph further explains that electric current is not a vector quantity because there is no meaning of triangle law there. While I understand why electric current is a scalar quantity, I am unable to comprehend how definite the rule is. For all I think, in the case of a right angled triangle, the rule is just calculating the hypotenuse (resultant vector). In dimensions two and above, it would make complete sense. However, in one dimension  there isn't any angle involved and each point depends on only one real number. The two directions: left and right can be dealt with two sets of real numbers, i.e., negative and positive. I want to know why vectors in one dimension can't be written as scalars. Why they must be written in the form of when they can be mentioned simply as the scalar number on the axis.",x\hat{\imath}+0\hat {\jmath}+0\hat k x,"['linear-algebra', 'vectors']"
99,How to prove a trace matrix is positive (semi) definite?,How to prove a trace matrix is positive (semi) definite?,,"Assume $X,A,B,C$ are all positive (semi) definite matrices with the same dimension. Define $$ S = \begin{bmatrix} \mbox{tr}(XAXA) & \mbox{tr}(XAXB) & \mbox{tr}(XAXC) \\ \mbox{tr}(XAXB) & \mbox{tr}(XBXB) & \mbox{tr}(XBXC) \\ \mbox{tr}(XAXC) & \mbox{tr}(XBXC) & \mbox{tr}(XCXC) \end{bmatrix}. $$ Can we show that $S$ is a positive (semi) definite matrix? I think probably mathematical induction can be applied since the original problem itself is of $p>3$ dimension, but I still have no specific idea. To validate if it is the truth, I run the R code below. Here I simulated different positive definite matrices $X,A,B,C$ for $1000$ times and the results showed that all the $S$ matrix is positive definite. library(clusterGeneration) K <- 50 ss <- 0 for(i in 1:1000){   set.seed(i)   X <- genPositiveDefMat(K)$Sigma   A <- genPositiveDefMat(K)$Sigma   B <- genPositiveDefMat(K)$Sigma   C <- genPositiveDefMat(K)$Sigma   S <- matrix(0, nrow = 3, ncol = 3)   S[1,1] = tr(X%*%A%*%X%*%A)   S[1,2] = S[2,1] = tr(X%*%A%*%X%*%B)   S[1,3] = S[3,1] = tr(X%*%A%*%X%*%C)   S[2,2] = tr(X%*%B%*%X%*%B)   S[3,2] = S[2,3] = tr(X%*%B%*%X%*%C)   S[3,3] = tr(X%*%C%*%X%*%C)   ss <- ss + sum(eigen(S)$values > 0) } ss # The result ss = 3000 shows that all S is positive definite Motivation This question comes from the expectation of the 2nd order derivative of a specific log-likelihood function. I just simplified it to a $3$ -dimensional problem with $A$ , $B$ , $C$ matrices.","Assume are all positive (semi) definite matrices with the same dimension. Define Can we show that is a positive (semi) definite matrix? I think probably mathematical induction can be applied since the original problem itself is of dimension, but I still have no specific idea. To validate if it is the truth, I run the R code below. Here I simulated different positive definite matrices for times and the results showed that all the matrix is positive definite. library(clusterGeneration) K <- 50 ss <- 0 for(i in 1:1000){   set.seed(i)   X <- genPositiveDefMat(K)$Sigma   A <- genPositiveDefMat(K)$Sigma   B <- genPositiveDefMat(K)$Sigma   C <- genPositiveDefMat(K)$Sigma   S <- matrix(0, nrow = 3, ncol = 3)   S[1,1] = tr(X%*%A%*%X%*%A)   S[1,2] = S[2,1] = tr(X%*%A%*%X%*%B)   S[1,3] = S[3,1] = tr(X%*%A%*%X%*%C)   S[2,2] = tr(X%*%B%*%X%*%B)   S[3,2] = S[2,3] = tr(X%*%B%*%X%*%C)   S[3,3] = tr(X%*%C%*%X%*%C)   ss <- ss + sum(eigen(S)$values > 0) } ss # The result ss = 3000 shows that all S is positive definite Motivation This question comes from the expectation of the 2nd order derivative of a specific log-likelihood function. I just simplified it to a -dimensional problem with , , matrices.","X,A,B,C 
S = \begin{bmatrix}
\mbox{tr}(XAXA) & \mbox{tr}(XAXB) & \mbox{tr}(XAXC) \\
\mbox{tr}(XAXB) & \mbox{tr}(XBXB) & \mbox{tr}(XBXC) \\
\mbox{tr}(XAXC) & \mbox{tr}(XBXC) & \mbox{tr}(XCXC)
\end{bmatrix}.
 S p>3 X,A,B,C 1000 S 3 A B C","['linear-algebra', 'probability', 'matrices', 'induction', 'positive-definite']"
