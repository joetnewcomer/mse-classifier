,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is the zero matrix upper and lower triangular as well as diagonal?,Is the zero matrix upper and lower triangular as well as diagonal?,,"From what I can tell from the definitions of a lower-triangular, upper-triangular, and diagonal matrices, I've come to the conclusion that the zero matrix is in the set of all of each type of matrix.  Is this correct?","From what I can tell from the definitions of a lower-triangular, upper-triangular, and diagonal matrices, I've come to the conclusion that the zero matrix is in the set of all of each type of matrix.  Is this correct?",,"['linear-algebra', 'matrices']"
1,Trace minimization with constraints,Trace minimization with constraints,,"For positive semi-definite matrices $A,B$, how can I find an $X$ that minimizes $\text{Trace}(AX^TBX$) under 'either' one of these constraints: a) Sum of squares of Euclidean-distances between pairs of rows in $X$ is a constant $\nu$. or b) $X$ is orthogonal. Not a hw problem- though the style of writing might suggest so.  All the matrices have real entries and $A,B$ are square while $X$ is rectangular. Thanks. This is what I have: Define $B=F^{T}F$. Define $Y=FX$. You get the above problem as  \begin{align} \min_{Y}~ \text{trace}(AY^{T}Y) \end{align} But now I want $X^*$ that minimizes the original problem. This is what is confusing me!","For positive semi-definite matrices $A,B$, how can I find an $X$ that minimizes $\text{Trace}(AX^TBX$) under 'either' one of these constraints: a) Sum of squares of Euclidean-distances between pairs of rows in $X$ is a constant $\nu$. or b) $X$ is orthogonal. Not a hw problem- though the style of writing might suggest so.  All the matrices have real entries and $A,B$ are square while $X$ is rectangular. Thanks. This is what I have: Define $B=F^{T}F$. Define $Y=FX$. You get the above problem as  \begin{align} \min_{Y}~ \text{trace}(AY^{T}Y) \end{align} But now I want $X^*$ that minimizes the original problem. This is what is confusing me!",,"['linear-algebra', 'matrices', 'optimization', 'qcqp']"
2,Product of positive definite and seminegative definite matrices,Product of positive definite and seminegative definite matrices,,Let $A$ a spd (symmetric positive definite) matrix and $B$ a symmetric seminegative definite matrix. Is tr $AB \leq 0$ and more general is $AB$ seminegative definite? I know that tr $AB \leq 0$ follows from $AB$ seminegative definite since the eigenvalues $\lambda$ of $AB$ are nonpositve and hence tr $AB=\sum_{\lambda \in spec\ A} \lambda \leq 0$. But I don't know how to find something out about the definitness of $AB$. I think in general there is nothing you can say about the eigenvalues of $AB$. Thanks in advance!,Let $A$ a spd (symmetric positive definite) matrix and $B$ a symmetric seminegative definite matrix. Is tr $AB \leq 0$ and more general is $AB$ seminegative definite? I know that tr $AB \leq 0$ follows from $AB$ seminegative definite since the eigenvalues $\lambda$ of $AB$ are nonpositve and hence tr $AB=\sum_{\lambda \in spec\ A} \lambda \leq 0$. But I don't know how to find something out about the definitness of $AB$. I think in general there is nothing you can say about the eigenvalues of $AB$. Thanks in advance!,,"['linear-algebra', 'matrices', 'numerical-linear-algebra']"
3,"Why, if we have more columns than rows, can the columns not be linearly independent?","Why, if we have more columns than rows, can the columns not be linearly independent?",,"Why, if we have more columns than rows, can the columns not be linearly independent? For example, suppose I have a set of vectors $\{ v_1, v_2, v_3, v_4\}$, $\forall v_i \in \mathbb{R}^3$.","Why, if we have more columns than rows, can the columns not be linearly independent? For example, suppose I have a set of vectors $\{ v_1, v_2, v_3, v_4\}$, $\forall v_i \in \mathbb{R}^3$.",,['linear-algebra']
4,"Is there such a thing as ""quadratic independence"" (and higher generalizations of linear independence)?","Is there such a thing as ""quadratic independence"" (and higher generalizations of linear independence)?",,"The notion of linear independence is very well-known and well-understood. However, is there a way to generalize the definition to other types of independence -- such as perhaps ""quadratic independence"", ""polynomial independence"", ""harmonic independence"", etc.? (Sorry if linear-algebra isn't a good tag; I couldn't think of a better one.)","The notion of linear independence is very well-known and well-understood. However, is there a way to generalize the definition to other types of independence -- such as perhaps ""quadratic independence"", ""polynomial independence"", ""harmonic independence"", etc.? (Sorry if linear-algebra isn't a good tag; I couldn't think of a better one.)",,['linear-algebra']
5,Find the maximum of $\operatorname{Tr}(RZ)$ over all orthogonal matrices $R$,Find the maximum of  over all orthogonal matrices,\operatorname{Tr}(RZ) R,"Say I have the following maximization. $$ \max_{R: R^T R=I_n} \operatorname{Tr}(RZ),$$ where $R$ is an $n\times n$ orthogonal transformational, and the SVD of $Z$ is written as $Z = USV^T$ . I'm trying to find the optimal $R^*$ which intuitively I know is equal to $VU^T$ where $$\operatorname{Tr}(RZ)=\operatorname{Tr}(VU^T USV^T)=\operatorname{Tr}(S).$$ I know this is the max since it is the sum of all the singular values of $Z$ . However, I'm having trouble coming up with a mathematical proof justifying my intuition. Any thoughts?","Say I have the following maximization. where is an orthogonal transformational, and the SVD of is written as . I'm trying to find the optimal which intuitively I know is equal to where I know this is the max since it is the sum of all the singular values of . However, I'm having trouble coming up with a mathematical proof justifying my intuition. Any thoughts?"," \max_{R: R^T R=I_n} \operatorname{Tr}(RZ), R n\times n Z Z = USV^T R^* VU^T \operatorname{Tr}(RZ)=\operatorname{Tr}(VU^T USV^T)=\operatorname{Tr}(S). Z","['linear-algebra', 'optimization', 'trace', 'svd', 'orthogonality']"
6,Algorithm to find an orthogonal basis (orthogonal to a given vector),Algorithm to find an orthogonal basis (orthogonal to a given vector),,"Let $K$ be a given integer, with $K$ even (and ""large""). Let $\mathbf{v} \in \mathbb{R}^{K \times 1}$ be a given non-zero (column) vector. Write a (possibly efficient) algorithm to construct a matrix $\mathbf{B} \in \mathbb{R}^{K \times K-1}$ such as that: $(1)$ the column-vectors of $\mathbf{B}$ are orthogonal to each other; $(2)$ $\mathbf{B^T v} = \mathbf{0}$, where $^T$ denotes transpose and $\mathbf{0}$ a ((K-1)-dimensional) vector of all zeros. Denoting with $\mathbf{b}_i$, $i=1,\dots,K-1$, the $i$-th column of $\mathbf{B}$, the first requirement can be rewritten as: $\mathbf{b}_i^T \mathbf{b}_j = 0$ for $i \neq j$. Essentially, the problem asks to write an algorithm to find an orthogonal basis to the orthogonal complement of the (mono-dimensional) space spanned by $\mathbf{v}$. The algorithm can be written in pseudo-code (or in MATLAB-like or in C-like code). The algorithm takes in input the vector $\mathbf{v}$ and the integer $K$; its output is the matrix $\mathbf{B}$. Note: the algorithm cannot do a ""random trial-and-error"", i.e., generate a random vector, try if it is orthogonal to $\mathbf{v}$ and to the previously found columns of $\mathbf{B}$, discard if it not, memorize it if it is. This is an explicitly forbidden brute-force approach. However, it is indeed allowed to do this as an initializing stage, i.e., for the first column of the matrix or as a ""random guess"" at the start, if any need to do so should arise. ""Normality"", i.e., finding an orthonormal basis, is not required. Input checking (e.g. checking if $K$ is an even integer) is not required. EDIT: My thoughts and previous attempts: The problem is essentially an implementation of Gram-Schmidts orthonormalization process. However, it cannot simply be used as stated, because Gram-Schimdts' theorem assumes to start with a basis (which we do not have). What we can actually construct is a spanning set of vectors, i.e. $\mathbf{v}, e_1, \dots, e_K$, where $e_i$ denotes the $i$-th canonical base vector. I have already implemented Gram-Schmidts, paying attention to numerical issues. The problem is a slight generalization of the process, in which you do not start with a basis, but with a spanning set, find a suitable basis in the set (I don't know how to do it), which must contain $\mathbf{v}$ as its first element, and then apply Gram-Schimdts process. P.S. Any help is appreciated, I am no master of linear algebra, especially numerical implementations. Thanks.","Let $K$ be a given integer, with $K$ even (and ""large""). Let $\mathbf{v} \in \mathbb{R}^{K \times 1}$ be a given non-zero (column) vector. Write a (possibly efficient) algorithm to construct a matrix $\mathbf{B} \in \mathbb{R}^{K \times K-1}$ such as that: $(1)$ the column-vectors of $\mathbf{B}$ are orthogonal to each other; $(2)$ $\mathbf{B^T v} = \mathbf{0}$, where $^T$ denotes transpose and $\mathbf{0}$ a ((K-1)-dimensional) vector of all zeros. Denoting with $\mathbf{b}_i$, $i=1,\dots,K-1$, the $i$-th column of $\mathbf{B}$, the first requirement can be rewritten as: $\mathbf{b}_i^T \mathbf{b}_j = 0$ for $i \neq j$. Essentially, the problem asks to write an algorithm to find an orthogonal basis to the orthogonal complement of the (mono-dimensional) space spanned by $\mathbf{v}$. The algorithm can be written in pseudo-code (or in MATLAB-like or in C-like code). The algorithm takes in input the vector $\mathbf{v}$ and the integer $K$; its output is the matrix $\mathbf{B}$. Note: the algorithm cannot do a ""random trial-and-error"", i.e., generate a random vector, try if it is orthogonal to $\mathbf{v}$ and to the previously found columns of $\mathbf{B}$, discard if it not, memorize it if it is. This is an explicitly forbidden brute-force approach. However, it is indeed allowed to do this as an initializing stage, i.e., for the first column of the matrix or as a ""random guess"" at the start, if any need to do so should arise. ""Normality"", i.e., finding an orthonormal basis, is not required. Input checking (e.g. checking if $K$ is an even integer) is not required. EDIT: My thoughts and previous attempts: The problem is essentially an implementation of Gram-Schmidts orthonormalization process. However, it cannot simply be used as stated, because Gram-Schimdts' theorem assumes to start with a basis (which we do not have). What we can actually construct is a spanning set of vectors, i.e. $\mathbf{v}, e_1, \dots, e_K$, where $e_i$ denotes the $i$-th canonical base vector. I have already implemented Gram-Schmidts, paying attention to numerical issues. The problem is a slight generalization of the process, in which you do not start with a basis, but with a spanning set, find a suitable basis in the set (I don't know how to do it), which must contain $\mathbf{v}$ as its first element, and then apply Gram-Schimdts process. P.S. Any help is appreciated, I am no master of linear algebra, especially numerical implementations. Thanks.",,"['linear-algebra', 'inner-products', 'numerical-linear-algebra', 'orthogonality']"
7,Calculate the angle between two vectors,Calculate the angle between two vectors,,"I come from Stack Overflow and I thought my question was more related to this forum. The problem is I'm not a mathematician, so please excuse me if my question is dumb. I'm trying to get the angle between two vectors. As numbers of posts says, here or here , I tried this solution. But my angle must be ""oriented"": If th angle between u⃗ and v⃗ is θ, the angle between v⃗ and u⃗ must be -θ . Is there a mathematical solution to this? Edit : Here's the formula I implemented for the points $a = (x_1, y_1)$ and $b = (x_2, y_2)$ representing the vectors: $$ \mathrm{angle} = \arccos \left(\frac{x_1 \cdot x_2 + y_1 \cdot y_2}{\sqrt{x_1^2+y_1^2} \cdot \sqrt{x_2^2+y_2^2}} \right) $$","I come from Stack Overflow and I thought my question was more related to this forum. The problem is I'm not a mathematician, so please excuse me if my question is dumb. I'm trying to get the angle between two vectors. As numbers of posts says, here or here , I tried this solution. But my angle must be ""oriented"": If th angle between u⃗ and v⃗ is θ, the angle between v⃗ and u⃗ must be -θ . Is there a mathematical solution to this? Edit : Here's the formula I implemented for the points and representing the vectors:","a = (x_1, y_1) b = (x_2, y_2) 
\mathrm{angle} = \arccos \left(\frac{x_1 \cdot x_2 + y_1 \cdot y_2}{\sqrt{x_1^2+y_1^2} \cdot \sqrt{x_2^2+y_2^2}} \right)
","['linear-algebra', 'vector-spaces']"
8,Invariance of eigenvalues of a product of square matrices under cyclic permutation,Invariance of eigenvalues of a product of square matrices under cyclic permutation,,"I recently came across this proposition that the eigenvalues of a product of square matrices are invariant under cyclic permutation of the product order. Is there perhaps some group theoretic way of proving this proposition? I've tried a few cases and it seems to be true, but a direct proof has proven elusive. Or is a proof not so simple?","I recently came across this proposition that the eigenvalues of a product of square matrices are invariant under cyclic permutation of the product order. Is there perhaps some group theoretic way of proving this proposition? I've tried a few cases and it seems to be true, but a direct proof has proven elusive. Or is a proof not so simple?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
9,Prove in full detail that the set is a vector space,Prove in full detail that the set is a vector space,,"So I'm doing a review test and I have this problem: Prove in full detail, with the standard operations in $\Bbb R^2$ , that the set $\{(x,2x): x \ \text{is a real number}\}$ is a vector space. Attempt: Given: $(x_1, 2x_1) \in \mathbb{R}^2$ and $(x_2, 2x_2) \in \mathbb{R}^2$ Addition: $(x_1, 2x_1) + (x_2, 2x_2) = (x_1 + x_2, 2x_1 + 2x_2) \in \mathbb{R}^2$ $ = (x_1 + x_2, 2(x_1 + x_2)) \in \mathbb{R}^2$ $ ≃ (x, 2x) \in \mathbb{R}^2$ Thus the set is closed under addition Scalar multiplication: $c(x_1, 2x_1) = (cx_1, 2(cx_1)) \in \mathbb{R}^2$ $ ≃ (x, 2x) \in \mathbb{R}^2$ Thus the set is closed under scalar multiplication Are these operations enough to prove that the set is a vector space? Or do I have to go through each of the following (or in other words do I have to to the same thing for each property in the definition):","So I'm doing a review test and I have this problem: Prove in full detail, with the standard operations in , that the set is a vector space. Attempt: Given: and Addition: Thus the set is closed under addition Scalar multiplication: Thus the set is closed under scalar multiplication Are these operations enough to prove that the set is a vector space? Or do I have to go through each of the following (or in other words do I have to to the same thing for each property in the definition):","\Bbb R^2 \{(x,2x): x \ \text{is a real number}\} (x_1, 2x_1) \in \mathbb{R}^2 (x_2, 2x_2) \in \mathbb{R}^2 (x_1, 2x_1) + (x_2, 2x_2) = (x_1 + x_2, 2x_1 + 2x_2) \in \mathbb{R}^2  = (x_1 + x_2, 2(x_1 + x_2)) \in \mathbb{R}^2  ≃ (x, 2x) \in \mathbb{R}^2 c(x_1, 2x_1) = (cx_1, 2(cx_1)) \in \mathbb{R}^2  ≃ (x, 2x) \in \mathbb{R}^2",['linear-algebra']
10,How to show that a linear map is surjective?,How to show that a linear map is surjective?,,"Sorry if this is somewhat a duplicate.  The answers I see deal with functions in general rather than linear maps. Let $T$ be a linear map from $U$ to $V$. I understand that by definition a linear map is injective if every element in the range gets mapped there by a unique vector from the domain.  This is easy to show by choosing two vectors $u$ and $v$ in $U$, and showing that if $T(u)=T(v)$, then $u=v$. But for a surjective linear map, it does not seem like there is something simple like this we can do? We have to show that range$(T)=V$.  How is this done? EDIT: As a concrete example, suppose we have $T\in L(F^\infty \rightarrow F^\infty)$ defined by $T(x_1,x_2,x_3,\dots) = (x_2,x_3, \dots)$.  How can we show this is surjective?  Is it enough to: Suppose $w\in W$, where $w=(w_1, w_2, \dots)$.  Then let $u=(a, w_1, w_2)$ for some $a\in F$.  And that's all we need?","Sorry if this is somewhat a duplicate.  The answers I see deal with functions in general rather than linear maps. Let $T$ be a linear map from $U$ to $V$. I understand that by definition a linear map is injective if every element in the range gets mapped there by a unique vector from the domain.  This is easy to show by choosing two vectors $u$ and $v$ in $U$, and showing that if $T(u)=T(v)$, then $u=v$. But for a surjective linear map, it does not seem like there is something simple like this we can do? We have to show that range$(T)=V$.  How is this done? EDIT: As a concrete example, suppose we have $T\in L(F^\infty \rightarrow F^\infty)$ defined by $T(x_1,x_2,x_3,\dots) = (x_2,x_3, \dots)$.  How can we show this is surjective?  Is it enough to: Suppose $w\in W$, where $w=(w_1, w_2, \dots)$.  Then let $u=(a, w_1, w_2)$ for some $a\in F$.  And that's all we need?",,['linear-algebra']
11,Structure on $\mathbb{R}$ such that homomorphisms $\mathbb{R} \to \mathbb{R}$ are exactly polynomials?,Structure on  such that homomorphisms  are exactly polynomials?,\mathbb{R} \mathbb{R} \to \mathbb{R},"If we consider $\mathbb{R}$ as a vector space over the field $\mathbb{R}$ , then the maps $\mathbb{R} \to \mathbb{R}$ preserving this structure are exactly the linear maps $x \mapsto ax$ . In contrast, if we think of $\mathbb{R}$ as an affine space instead, the maps preserving this structure are exactly the affine maps $x \mapsto ax + b$ . Is it possible to ""forget"" even more structure, and have a structure on $\mathbb{R}$ such that the maps $\mathbb{R} \to \mathbb{R}$ preserving this structure are exactly the polynomials? I'll leave it up to you what ring to take coefficients of polynomials from. If anyone could do it for $\mathbb{N}$ , $\mathbb{Z}$ , $\mathbb{Q}$ , $\mathbb{C}$ instead, I'll be equally happy.","If we consider as a vector space over the field , then the maps preserving this structure are exactly the linear maps . In contrast, if we think of as an affine space instead, the maps preserving this structure are exactly the affine maps . Is it possible to ""forget"" even more structure, and have a structure on such that the maps preserving this structure are exactly the polynomials? I'll leave it up to you what ring to take coefficients of polynomials from. If anyone could do it for , , , instead, I'll be equally happy.",\mathbb{R} \mathbb{R} \mathbb{R} \to \mathbb{R} x \mapsto ax \mathbb{R} x \mapsto ax + b \mathbb{R} \mathbb{R} \to \mathbb{R} \mathbb{N} \mathbb{Z} \mathbb{Q} \mathbb{C},"['linear-algebra', 'polynomials', 'model-theory']"
12,Can elementary row operations be done by both left and right multiplication?,Can elementary row operations be done by both left and right multiplication?,,"So I know that interchanging two rows of a matrix $A$ can be done by left-multiplying it by some permutation matrix $P$ to get $PA$ . Similarly, interchanging two columns can be done by right-multiplication, $AP$ . My question is: Can you interchange rows by right -multiplication (of some matrix, not necessarily a permutation matrix)? And interchange columns by left-multiplication? In general, can every elementary row (or column) operation be represented as both a left- and right-multiplication? Edit: if $A$ is invertible, and we want to find a matrix $B$ such that $AB = PA$ , then we can solve for $B$ by multiplying both sides by $A^{-1}$ to get $B = A^{-1}PA$ . So this is how we can interchange two rows by right-multiplication. Similarly for interchanging columns. But what if $A$ isn't invertible? Can we characterize the cases when it is or isn't possible?","So I know that interchanging two rows of a matrix can be done by left-multiplying it by some permutation matrix to get . Similarly, interchanging two columns can be done by right-multiplication, . My question is: Can you interchange rows by right -multiplication (of some matrix, not necessarily a permutation matrix)? And interchange columns by left-multiplication? In general, can every elementary row (or column) operation be represented as both a left- and right-multiplication? Edit: if is invertible, and we want to find a matrix such that , then we can solve for by multiplying both sides by to get . So this is how we can interchange two rows by right-multiplication. Similarly for interchanging columns. But what if isn't invertible? Can we characterize the cases when it is or isn't possible?",A P PA AP A B AB = PA B A^{-1} B = A^{-1}PA A,"['linear-algebra', 'matrices']"
13,Is there any intuition why the following matrix is positive semidefinite?,Is there any intuition why the following matrix is positive semidefinite?,,"I have the following 8 by 8 square matrix, which is positive semidefinite: \begin{bmatrix}3&1&1&-1&1&-1&-1&-3\\1&3&-1&1&-1&1&-3&-1& \\ 1&-1&3&1&-1&-3&1&-1 \\  -1&1&1&3&-3&-1&-1&1 \\ 1&-1&-1&-3&3&1&1&-1 \\  -1&1&-3&-1&1&3&-1&1 \\ -1&-3&1&-1&1&-1&3&1  \\ -3&-1&-1&1&-1&1&1&3  \end{bmatrix} I wonder if there is an intuitive argument why this matrix is positive semidefinite. This matrix has some interesting properties: It is symmetric, diagonal elements have the largest values, and antidiagonal have the smallest values. The sum of each row and column is $0$ . I also notice it is symmetric in the sense that first column is the inverse of the last column, the second column is the inverse of the 7th column. If it helps, I computed the eigenvalues, which are $0,0,0,0,0,8,8$ and $8$ .","I have the following 8 by 8 square matrix, which is positive semidefinite: I wonder if there is an intuitive argument why this matrix is positive semidefinite. This matrix has some interesting properties: It is symmetric, diagonal elements have the largest values, and antidiagonal have the smallest values. The sum of each row and column is . I also notice it is symmetric in the sense that first column is the inverse of the last column, the second column is the inverse of the 7th column. If it helps, I computed the eigenvalues, which are and .","\begin{bmatrix}3&1&1&-1&1&-1&-1&-3\\1&3&-1&1&-1&1&-3&-1&
\\ 1&-1&3&1&-1&-3&1&-1 \\  -1&1&1&3&-3&-1&-1&1
\\ 1&-1&-1&-3&3&1&1&-1 \\  -1&1&-3&-1&1&3&-1&1
\\ -1&-3&1&-1&1&-1&3&1  \\ -3&-1&-1&1&-1&1&1&3
 \end{bmatrix} 0 0,0,0,0,0,8,8 8","['linear-algebra', 'matrices', 'symmetric-matrices', 'positive-semidefinite']"
14,$AB-BA$ invertible and $A^2+B^2 = AB$ then $3$ divides $n$ [duplicate],invertible and  then  divides  [duplicate],AB-BA A^2+B^2 = AB 3 n,"This question already has answers here : Square matrices satisfying certain relations must have dimension divisible by $3$ (3 answers) Closed 3 years ago . Let $A, B \in M_n(\mathbb{R})$ such that : $AB-BA$ invertible and $A^2+B^2 = AB$ then prove that : $3 \mid n$ . I tried to manipulate or find some factorisation in order to be able to use efficiently eigenvalues...  but I dind't find anything. Thank you !",This question already has answers here : Square matrices satisfying certain relations must have dimension divisible by $3$ (3 answers) Closed 3 years ago . Let such that : invertible and then prove that : . I tried to manipulate or find some factorisation in order to be able to use efficiently eigenvalues...  but I dind't find anything. Thank you !,"A, B \in M_n(\mathbb{R}) AB-BA A^2+B^2 = AB 3 \mid n","['linear-algebra', 'vector-spaces']"
15,When does an inner product induce a norm?,When does an inner product induce a norm?,,"When we consider a vector space $V$ over some field $F$ , I know that when the $F=\mathbb{R}$ or $ =\mathbb{C}$ , by setting $\|x\|=\left\langle{x,x}\right\rangle^\frac{1}{2}$ we get a norm. However, since the inner product is a function with its image in $F$ , what happens if we consider any $V$ over the rational numbers? For example, if we take $\mathbb{Q}^2$ over $\mathbb{Q}$ with the dot product, then $v=(1,1)$ has norm $\sqrt{2}$ , which is not rational. How can one obtain a norm from a given inner product in such cases?","When we consider a vector space over some field , I know that when the or , by setting we get a norm. However, since the inner product is a function with its image in , what happens if we consider any over the rational numbers? For example, if we take over with the dot product, then has norm , which is not rational. How can one obtain a norm from a given inner product in such cases?","V F F=\mathbb{R}  =\mathbb{C} \|x\|=\left\langle{x,x}\right\rangle^\frac{1}{2} F V \mathbb{Q}^2 \mathbb{Q} v=(1,1) \sqrt{2}","['linear-algebra', 'normed-spaces', 'inner-products']"
16,Why the null space of pseudo inverse equals the null space of the matrix transpose?,Why the null space of pseudo inverse equals the null space of the matrix transpose?,,"The pseudoinverse $A^+$ of A is the matrix for which $x = A^+Ax$ for all x in the row space of A. The nullspace of $A^+$ is the nullspace of $A^T$. I don't understand this cause the above seems to imply that $A^+=A^T$ which doesn't make sense as $x = A^+Ax$ while $A^TA$ gives a matrix which is not an identity matrix. Here is the source -Page 2 on ""Finding the pseudo Inverse""","The pseudoinverse $A^+$ of A is the matrix for which $x = A^+Ax$ for all x in the row space of A. The nullspace of $A^+$ is the nullspace of $A^T$. I don't understand this cause the above seems to imply that $A^+=A^T$ which doesn't make sense as $x = A^+Ax$ while $A^TA$ gives a matrix which is not an identity matrix. Here is the source -Page 2 on ""Finding the pseudo Inverse""",,"['linear-algebra', 'matrices', 'pseudoinverse']"
17,What is the relation between $\det(A^TA)$ and $\det(AA^T)$ for an $m\times n$ matrix $A$?,What is the relation between  and  for an  matrix ?,\det(A^TA) \det(AA^T) m\times n A,"Let $A$ be an $m\times n$ real matrix , and $\det(\cdot)$ the determinant . It is rather trivial that when $A$ is of size $1\times 1$ , then $\det(A^TA)=\det(AA^T)$ . In general, what is the relation between $\det(A^TA)$ and $\det(AA^T)$ ?","Let be an real matrix , and the determinant . It is rather trivial that when is of size , then . In general, what is the relation between and ?",A m\times n \det(\cdot) A 1\times 1 \det(A^TA)=\det(AA^T) \det(A^TA) \det(AA^T),"['linear-algebra', 'matrices', 'determinant']"
18,Can we find the inverse for a vector?,Can we find the inverse for a vector?,,"Can we invert a vector like we do with matrices, and why? I didn't see in any linear algebra course the concept of the ""vector inverse"", and I was wondering if there is any such thing, and if not, why.","Can we invert a vector like we do with matrices, and why? I didn't see in any linear algebra course the concept of the ""vector inverse"", and I was wondering if there is any such thing, and if not, why.",,"['linear-algebra', 'matrices', 'inverse', 'vectors']"
19,"Reason for thinking of vector as ""row"" and ""column"" vectors in linear algebra","Reason for thinking of vector as ""row"" and ""column"" vectors in linear algebra",,"Consider the $n$-tuple $(x_1,\ldots,x_n)$ with entries in some field $K$. What is the reason for perceiving this tuple either as a row vector, $$ [x_1,\ldots,x_n]$$or as a column vector $$\left[\begin{array}{c}x_1\\\vdots\\x_n\end{array}\right]$$? To clarify further: All the answers on this site, that I looked up, like this , this or this one deal with the question, which objects should be thought of as row respectively column vectors - opposed to that is what I'm asking: What's reason for making this distinction in the first place ? Because the thing is, I could define matrix multiplication, like this $$ \left[\begin{array}{cc} a & b  \\ c & d   \end{array}\right](x_1,x_2):=(ax_1+bx_2,cx_1+dx_2), $$so I only ever need to deal with ""tuple-vectors"", not row or column vectors; no need to ever talk about row or column vectors. So the thing is, that everything related to coordinates could be done, in the case of vectors, in row or column terms. So if that is possible, why is nobody doing it like that ? The only benefit that I see from making the row-column distinction - in contrast to using tuples $(x_1,\ldots,x_n)$ which are neither ""row"" nor ""column""-type - is to be gained on a notational level , so that it is easier to remember, for example, how to do matrix-vector multiplication, by ""moving"" along the rows of the matrix while ""moving"" along the  column of vector. But that seems a shallow reason, to put up with distinguishing between ""row"" and ""column""-vectors all the time. I hope there's something deeper than that.","Consider the $n$-tuple $(x_1,\ldots,x_n)$ with entries in some field $K$. What is the reason for perceiving this tuple either as a row vector, $$ [x_1,\ldots,x_n]$$or as a column vector $$\left[\begin{array}{c}x_1\\\vdots\\x_n\end{array}\right]$$? To clarify further: All the answers on this site, that I looked up, like this , this or this one deal with the question, which objects should be thought of as row respectively column vectors - opposed to that is what I'm asking: What's reason for making this distinction in the first place ? Because the thing is, I could define matrix multiplication, like this $$ \left[\begin{array}{cc} a & b  \\ c & d   \end{array}\right](x_1,x_2):=(ax_1+bx_2,cx_1+dx_2), $$so I only ever need to deal with ""tuple-vectors"", not row or column vectors; no need to ever talk about row or column vectors. So the thing is, that everything related to coordinates could be done, in the case of vectors, in row or column terms. So if that is possible, why is nobody doing it like that ? The only benefit that I see from making the row-column distinction - in contrast to using tuples $(x_1,\ldots,x_n)$ which are neither ""row"" nor ""column""-type - is to be gained on a notational level , so that it is easier to remember, for example, how to do matrix-vector multiplication, by ""moving"" along the rows of the matrix while ""moving"" along the  column of vector. But that seems a shallow reason, to put up with distinguishing between ""row"" and ""column""-vectors all the time. I hope there's something deeper than that.",,['linear-algebra']
20,Notation: subscript vs. superscript for coordinate vector fields,Notation: subscript vs. superscript for coordinate vector fields,,"Some books write the coordinate vector fields with a subscript as $$\frac{\partial}{\partial x_i}$$ while some write it with a superscript as $$\frac{\partial}{\partial x^i}.$$ Is there a conceptual reason for this distinction? I.e. in some texts I have seen, a supercript is to indicate the components of covectors, and a subscript for vectors.","Some books write the coordinate vector fields with a subscript as $$\frac{\partial}{\partial x_i}$$ while some write it with a superscript as $$\frac{\partial}{\partial x^i}.$$ Is there a conceptual reason for this distinction? I.e. in some texts I have seen, a supercript is to indicate the components of covectors, and a subscript for vectors.",,"['linear-algebra', 'differential-geometry', 'notation', 'vector-analysis']"
21,Why is SVD on $X$ preferred to eigendecomposition of $XX^\top$ in PCA?,Why is SVD on  preferred to eigendecomposition of  in PCA?,X XX^\top,"In this post J.M. has mentioned that ... In fact, using the SVD to perform PCA makes much better sense numerically than forming the covariance matrix to begin with, since the formation of $XX^\top$ can cause loss of precision. This is detailed in books on numerical linear algebra, but I'll leave you with an example of a matrix that can be stable SVD'd, but forming $XX^\top$ can be disastrous ... Could you elaborate on why the calculation of $XX^\top$ is disastrous for the matrix given in that post? I calculated $XX^\top$ for the numbers it specifies. I got a $3\times3$ matrix with all $1$'s.","In this post J.M. has mentioned that ... In fact, using the SVD to perform PCA makes much better sense numerically than forming the covariance matrix to begin with, since the formation of $XX^\top$ can cause loss of precision. This is detailed in books on numerical linear algebra, but I'll leave you with an example of a matrix that can be stable SVD'd, but forming $XX^\top$ can be disastrous ... Could you elaborate on why the calculation of $XX^\top$ is disastrous for the matrix given in that post? I calculated $XX^\top$ for the numbers it specifies. I got a $3\times3$ matrix with all $1$'s.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'svd', 'principal-component-analysis']"
22,Minimal polynominal: geometric meaning,Minimal polynominal: geometric meaning,,I am currently studying Chapter 6 of Hoffman & Kunze's Linear Algebra which deals with characteristic values and triangulation and diagonalization theorems. The chapter makes heavy use of the concept of the minimal polynomial which it defines as the monic polynomial of the smallest degree that annihilates a linear transformation. I am currently finding the proofs in the book that use this definition of the minimal polynomial to be very opaque: just chains of calculations with polynomials that turn out to give the right answer in the end. I was therefore wondering if there is a more geometric view of the minimal polynomial? What information about the linear transformation is it carrying and why? I guess the answer would be simpler in the case of an algebraically closed field but is there an answer which also works for a general field?,I am currently studying Chapter 6 of Hoffman & Kunze's Linear Algebra which deals with characteristic values and triangulation and diagonalization theorems. The chapter makes heavy use of the concept of the minimal polynomial which it defines as the monic polynomial of the smallest degree that annihilates a linear transformation. I am currently finding the proofs in the book that use this definition of the minimal polynomial to be very opaque: just chains of calculations with polynomials that turn out to give the right answer in the end. I was therefore wondering if there is a more geometric view of the minimal polynomial? What information about the linear transformation is it carrying and why? I guess the answer would be simpler in the case of an algebraically closed field but is there an answer which also works for a general field?,,['linear-algebra']
23,Commutators of tensor product of Pauli matrices,Commutators of tensor product of Pauli matrices,,"Given tensor product of rank-2 Pauli matrices $\sigma^a$. Each $\sigma^a$ is related to the generator of SU(2) Lie algebra. We know they satisfy $$[\sigma^a, \sigma^b ] = 2 i \epsilon^{abc} \sigma^c$$ Do you know any equality/identity to simplify: $$ [\sigma^a \otimes \sigma^c, \sigma^b \otimes \sigma^d] = ? $$ also $$ [\sigma^a \otimes \sigma^c  \otimes \sigma^e, \sigma^b \otimes \sigma^d  \otimes \sigma^f] = ? $$ $$ [\sigma^a \otimes \sigma^c  \otimes \sigma^e \otimes \sigma^g, \sigma^b \otimes \sigma^d  \otimes \sigma^f \otimes \sigma^h] = ? $$ so that the final answers have no commutators? Commutator is defined by default as $$ [A,B]:=AB-BA $$","Given tensor product of rank-2 Pauli matrices $\sigma^a$. Each $\sigma^a$ is related to the generator of SU(2) Lie algebra. We know they satisfy $$[\sigma^a, \sigma^b ] = 2 i \epsilon^{abc} \sigma^c$$ Do you know any equality/identity to simplify: $$ [\sigma^a \otimes \sigma^c, \sigma^b \otimes \sigma^d] = ? $$ also $$ [\sigma^a \otimes \sigma^c  \otimes \sigma^e, \sigma^b \otimes \sigma^d  \otimes \sigma^f] = ? $$ $$ [\sigma^a \otimes \sigma^c  \otimes \sigma^e \otimes \sigma^g, \sigma^b \otimes \sigma^d  \otimes \sigma^f \otimes \sigma^h] = ? $$ so that the final answers have no commutators? Commutator is defined by default as $$ [A,B]:=AB-BA $$",,"['linear-algebra', 'abstract-algebra', 'lie-algebras', 'tensor-products']"
24,"Find the Jordan normal form of a nilpotent matrix $N$ given the dimensions of the kernels of $N, N^2, N^3$",Find the Jordan normal form of a nilpotent matrix  given the dimensions of the kernels of,"N N, N^2, N^3","Let $N\in \text{Mat}(10 \times 10,\mathbb{C})$ be nilpotent. Furthermore let $\text{dim} \ker N =3 $, $\text{dim} \ker N^2=6$ and $\text{dim} \ker N^3=7$. What is the Jordan Normal Form? The only thing I know is that there have to be three blocks, since $\text{dim} \ker N = 3$. Thank you very much in advance for your help.","Let $N\in \text{Mat}(10 \times 10,\mathbb{C})$ be nilpotent. Furthermore let $\text{dim} \ker N =3 $, $\text{dim} \ker N^2=6$ and $\text{dim} \ker N^3=7$. What is the Jordan Normal Form? The only thing I know is that there have to be three blocks, since $\text{dim} \ker N = 3$. Thank you very much in advance for your help.",,"['linear-algebra', 'matrices', 'jordan-normal-form']"
25,Proof of Spectral Theorem,Proof of Spectral Theorem,,"There are a number of results called Spectral Theorems . This question deals with the Linear Algebra result on normal operators, which has the self-adjoint case as a particular case. In class, we saw the spectral theorem for self-adjoint operators, and the teacher attempted a sketch of a proof which nobody in the class understood. Writing my thesis, I hit into the normal operator case, so I was looking for a complete proof of it. How is it proven?","There are a number of results called Spectral Theorems . This question deals with the Linear Algebra result on normal operators, which has the self-adjoint case as a particular case. In class, we saw the spectral theorem for self-adjoint operators, and the teacher attempted a sketch of a proof which nobody in the class understood. Writing my thesis, I hit into the normal operator case, so I was looking for a complete proof of it. How is it proven?",,"['linear-algebra', 'spectral-theory']"
26,Eigenvalues and Spectrum,Eigenvalues and Spectrum,,"In algebra, I learned that if $\lambda$ is an eigenvalue of a linear operator $T$ , I can have \begin{equation} Tx = \lambda x \tag{1} \end{equation} for some $x\neq 0$ , which is equivalent to $\lambda I-T$ not being invertible. In functional analysis, it is said that if $\lambda$ is an element of a spectrum of the linear operator $T$ , then $\lambda I - T$ is not invertible. However, my Professor never mentioned $(1)$ . Is the definition/concept in functional analysis the same as $(1)$ in linear algebra? Can I use $(1)$ in functional analysis too? Does it depend on which spaces we are in? For example, suppose $\lambda$ is in the spectrum of $T$ , where $T$ is a linear operator on $E$ , a Banach space. I want to show $\lambda^n$ is in the spectrum of $T^n$ . Would this problem is equivalent to showing if $\lambda$ is an eigenvalue of a linear operator $T$ , then $\lambda^n$ is an eigenvalue of $T^n$ ? Thank you.","In algebra, I learned that if is an eigenvalue of a linear operator , I can have for some , which is equivalent to not being invertible. In functional analysis, it is said that if is an element of a spectrum of the linear operator , then is not invertible. However, my Professor never mentioned . Is the definition/concept in functional analysis the same as in linear algebra? Can I use in functional analysis too? Does it depend on which spaces we are in? For example, suppose is in the spectrum of , where is a linear operator on , a Banach space. I want to show is in the spectrum of . Would this problem is equivalent to showing if is an eigenvalue of a linear operator , then is an eigenvalue of ? Thank you.","\lambda T \begin{equation}
Tx = \lambda x
\tag{1}
\end{equation} x\neq 0 \lambda I-T \lambda T \lambda I - T (1) (1) (1) \lambda T T E \lambda^n T^n \lambda T \lambda^n T^n","['linear-algebra', 'functional-analysis', 'eigenvalues-eigenvectors', 'spectral-theory']"
27,An uncountable linearly independent set,An uncountable linearly independent set,,"I've been taking a course in linear algebra and one of the first things we defined was linear independence. It made me wonder how big a linearly independent set can be, in particular whether we can find uncountable ones. After asking my professor he said I could consider the set $S$ of sequences whose entries lie in some arbitrary field $\mathbb{F}$ . $S$ is not linearly independent so I think he is hinting that its basis is uncountable. Since then I've managed to find an uncountable linearly independent set (and prove that it is so): the set of sequence of the form $x^n,\;0<x<1$. But the problem my professor gave me still escapes me. Any ideas how I could show that the set $S$ has uncountable dimension? (thank you @egreg for your kind answer, it's made the concept clearer. It doesn't really answer my question sadly)","I've been taking a course in linear algebra and one of the first things we defined was linear independence. It made me wonder how big a linearly independent set can be, in particular whether we can find uncountable ones. After asking my professor he said I could consider the set $S$ of sequences whose entries lie in some arbitrary field $\mathbb{F}$ . $S$ is not linearly independent so I think he is hinting that its basis is uncountable. Since then I've managed to find an uncountable linearly independent set (and prove that it is so): the set of sequence of the form $x^n,\;0<x<1$. But the problem my professor gave me still escapes me. Any ideas how I could show that the set $S$ has uncountable dimension? (thank you @egreg for your kind answer, it's made the concept clearer. It doesn't really answer my question sadly)",,"['linear-algebra', 'elementary-set-theory']"
28,Calculate the vector normal to the plane by given points,Calculate the vector normal to the plane by given points,,"How can one calculate the vector normal to the plane that is determined by given points? For example, given three points $P_1(5,0,0)$, $P_2(0,0,5)$ and $P_3(10,0,5)$, calculate the vector normal to the plane containing these three points. The compute the normal is by vector product. $$ a = \left(\begin{matrix} x_2-x_1\\y_2-y_1\\z_2-z_1 \end{matrix} \right) \qquad b = \left(\begin{matrix} x_3-x_1\\y_3-y_1\\z_3-z_1 \end{matrix} \right) $$ therefore $a = -5i+5k$, and $b=5i+5k$ $$ a\times b = \left|\begin{matrix}     i & j & k \\     -5 & 0 & 5 \\     5 & 0 & -5 \\ \end{matrix}\right| $$ The questions are: 1) Are A and B is given, that means no matter which three point i use, the $a$ and $b$ is still using this to calucate? 2) How to get $a$ , $b$ and $a\times b$ ? Thank you","How can one calculate the vector normal to the plane that is determined by given points? For example, given three points $P_1(5,0,0)$, $P_2(0,0,5)$ and $P_3(10,0,5)$, calculate the vector normal to the plane containing these three points. The compute the normal is by vector product. $$ a = \left(\begin{matrix} x_2-x_1\\y_2-y_1\\z_2-z_1 \end{matrix} \right) \qquad b = \left(\begin{matrix} x_3-x_1\\y_3-y_1\\z_3-z_1 \end{matrix} \right) $$ therefore $a = -5i+5k$, and $b=5i+5k$ $$ a\times b = \left|\begin{matrix}     i & j & k \\     -5 & 0 & 5 \\     5 & 0 & -5 \\ \end{matrix}\right| $$ The questions are: 1) Are A and B is given, that means no matter which three point i use, the $a$ and $b$ is still using this to calucate? 2) How to get $a$ , $b$ and $a\times b$ ? Thank you",,"['linear-algebra', 'analytic-geometry', 'cross-product']"
29,Textbook that brings together linear algebra and PDEs?,Textbook that brings together linear algebra and PDEs?,,"I'm looking for a textbook that goes into as much detail as possible about the parallels between linear algebra in finite, countable, and continuous ""spaces."" Specific topics that I'm trying to get a better (more general) understanding of are, for example: The relationship between Hermitian matrices and self-adjoint operators in general; How the orthogonality and completeness relations can be described in a way to include ""normalization with the Dirac delta function"" without making it a special case; How to understand ""basis vectors"" (such as with Dirac normalization) that don't appear to be ""part of"" the vector space that the normalized functions live in. I hope my terminology make sense. The whole point is that I'm just trying to learn this stuff, and I don't quite know what the correct terminology is :-) I've had basic courses in linear algebra and applied PDEs, and I see a lot of parallels, but in the books I have these parallels are (sometimes) mentioned but hardly ever emphasised. I think that, for example, a book which aims to teach partial differential equations ""from a linear algebra perspective"" might be what I'm looking for. Any suggestions appreciated.","I'm looking for a textbook that goes into as much detail as possible about the parallels between linear algebra in finite, countable, and continuous ""spaces."" Specific topics that I'm trying to get a better (more general) understanding of are, for example: The relationship between Hermitian matrices and self-adjoint operators in general; How the orthogonality and completeness relations can be described in a way to include ""normalization with the Dirac delta function"" without making it a special case; How to understand ""basis vectors"" (such as with Dirac normalization) that don't appear to be ""part of"" the vector space that the normalized functions live in. I hope my terminology make sense. The whole point is that I'm just trying to learn this stuff, and I don't quite know what the correct terminology is :-) I've had basic courses in linear algebra and applied PDEs, and I see a lot of parallels, but in the books I have these parallels are (sometimes) mentioned but hardly ever emphasised. I think that, for example, a book which aims to teach partial differential equations ""from a linear algebra perspective"" might be what I'm looking for. Any suggestions appreciated.",,"['linear-algebra', 'reference-request', 'partial-differential-equations']"
30,A simple proof for the relationship between the eigenvalues of a positive definite matrix and its Cholesky decomposition,A simple proof for the relationship between the eigenvalues of a positive definite matrix and its Cholesky decomposition,,"Can anyone present to me an elegant elementary proof of the relationship between the eigenvalues of a positive definite matrix and its Cholesky decomposition? More formally, suppose $\mathbf{A}$ is an $n\times n$ positive definite matrix and let $\mathbf{A} = \mathbf{R}^\top \mathbf{R}$ be its Cholesky decomposition. Establish the relationship between the eigenvalues of $\mathbf{A}$ and that of $\mathbf{R}$ . EDIT (Additional remarks): My question specifically wants to find, if possible, an equation or function, say $f$ , that relates the eigenvalues, i.e., $f\left(\lambda_i(\mathbf{R})\right) = \lambda_i(\mathbf{A})$ , with uniqueness up to order being considered if necessary.","Can anyone present to me an elegant elementary proof of the relationship between the eigenvalues of a positive definite matrix and its Cholesky decomposition? More formally, suppose is an positive definite matrix and let be its Cholesky decomposition. Establish the relationship between the eigenvalues of and that of . EDIT (Additional remarks): My question specifically wants to find, if possible, an equation or function, say , that relates the eigenvalues, i.e., , with uniqueness up to order being considered if necessary.",\mathbf{A} n\times n \mathbf{A} = \mathbf{R}^\top \mathbf{R} \mathbf{A} \mathbf{R} f f\left(\lambda_i(\mathbf{R})\right) = \lambda_i(\mathbf{A}),"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'positive-definite', 'cholesky-decomposition']"
31,Second Derivative of a Determinant,Second Derivative of a Determinant,,"How does one evaluate the second derivative of the determinant of a square matrix? Jacobi's formula tells us how to evaluate the first derivative but I can't find anything for the second. This is my attempt: We can start using the partial derivative formulation of Jacobi's formula, assuming A is invertible: \begin{equation}\frac{\partial}{\partial \alpha}\det A = (\det A) \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right)\end{equation}         Taking a second derivative:         \begin{equation}\frac{\partial^2}{\partial \alpha^2}\det A = \frac{\partial}{\partial \alpha}\left[(\det A) \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right)\right]\end{equation}         Applying product rule:         \begin{equation}= \frac{\partial}{\partial \alpha}(\det A) \cdot \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) +     (\det A) \frac{\partial}{\partial \alpha} \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right)\end{equation} Replacing $\frac{\partial}{\partial\alpha}\det A$ with Jacobi's formula:         \begin{equation}= (\det A) \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) \cdot \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) +(\det A) \text{tr}\left( \frac{\partial}{\partial \alpha}\left(A^{-1} \frac{\partial}{\partial \alpha} A \right)\right)\end{equation}         Factoring out $\det(A)$:         \begin{equation}= (\det A) \left[\text{tr}^2\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) +      \text{tr}\left( \frac{\partial}{\partial \alpha}\left(A^{-1} \frac{\partial}{\partial \alpha} A \right)\right)\right]\end{equation}          Another product rule:         \begin{equation}= (\det A) \left[\text{tr}^2\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) +      \text{tr}\left(\frac{\partial}{\partial \alpha}A^{-1} \frac{\partial}{\partial \alpha} A +      A^{-1} \frac{\partial^2}{\partial \alpha^2} A \right)\right]\end{equation}          Finally, using $A_{\alpha}$ to denote the partial of A wrt to $\alpha$ we have         \begin{equation}\frac{\partial^2}{\partial \alpha^2}\det A= \det(A) \left[\text{tr}^2\left( A^{-1} A_{\alpha} \right) +      \text{tr}\left(A^{-1}_{\alpha} A_{\alpha}\right) + \text{tr} \left( A^{-1} A_{\alpha^2} \right)\right]\end{equation}          The second trace actually reduces to N, for an NxN matrix:          \begin{equation}\text{tr}\left(A^{-1}_{\alpha} A_{\alpha}\right)=\text{tr}(I)\end{equation}          \begin{equation}=\sum_{diagonals}1\end{equation}          \begin{equation}=N\end{equation} \begin{equation}\therefore\frac{\partial^2}{\partial \alpha^2}\det A= \det(A) \left[\text{tr}^2\left( A^{-1} A_{\alpha} \right) +      \text{tr} \left( A^{-1} A_{\alpha^2} \right)+N\right]\end{equation} I'm not overly confident with the matrix calculus, but the result is nice enough to seem plausible. Is there another method, or is this proof valid? Thanks!","How does one evaluate the second derivative of the determinant of a square matrix? Jacobi's formula tells us how to evaluate the first derivative but I can't find anything for the second. This is my attempt: We can start using the partial derivative formulation of Jacobi's formula, assuming A is invertible: \begin{equation}\frac{\partial}{\partial \alpha}\det A = (\det A) \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right)\end{equation}         Taking a second derivative:         \begin{equation}\frac{\partial^2}{\partial \alpha^2}\det A = \frac{\partial}{\partial \alpha}\left[(\det A) \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right)\right]\end{equation}         Applying product rule:         \begin{equation}= \frac{\partial}{\partial \alpha}(\det A) \cdot \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) +     (\det A) \frac{\partial}{\partial \alpha} \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right)\end{equation} Replacing $\frac{\partial}{\partial\alpha}\det A$ with Jacobi's formula:         \begin{equation}= (\det A) \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) \cdot \text{tr}\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) +(\det A) \text{tr}\left( \frac{\partial}{\partial \alpha}\left(A^{-1} \frac{\partial}{\partial \alpha} A \right)\right)\end{equation}         Factoring out $\det(A)$:         \begin{equation}= (\det A) \left[\text{tr}^2\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) +      \text{tr}\left( \frac{\partial}{\partial \alpha}\left(A^{-1} \frac{\partial}{\partial \alpha} A \right)\right)\right]\end{equation}          Another product rule:         \begin{equation}= (\det A) \left[\text{tr}^2\left( A^{-1} \frac{\partial}{\partial \alpha} A \right) +      \text{tr}\left(\frac{\partial}{\partial \alpha}A^{-1} \frac{\partial}{\partial \alpha} A +      A^{-1} \frac{\partial^2}{\partial \alpha^2} A \right)\right]\end{equation}          Finally, using $A_{\alpha}$ to denote the partial of A wrt to $\alpha$ we have         \begin{equation}\frac{\partial^2}{\partial \alpha^2}\det A= \det(A) \left[\text{tr}^2\left( A^{-1} A_{\alpha} \right) +      \text{tr}\left(A^{-1}_{\alpha} A_{\alpha}\right) + \text{tr} \left( A^{-1} A_{\alpha^2} \right)\right]\end{equation}          The second trace actually reduces to N, for an NxN matrix:          \begin{equation}\text{tr}\left(A^{-1}_{\alpha} A_{\alpha}\right)=\text{tr}(I)\end{equation}          \begin{equation}=\sum_{diagonals}1\end{equation}          \begin{equation}=N\end{equation} \begin{equation}\therefore\frac{\partial^2}{\partial \alpha^2}\det A= \det(A) \left[\text{tr}^2\left( A^{-1} A_{\alpha} \right) +      \text{tr} \left( A^{-1} A_{\alpha^2} \right)+N\right]\end{equation} I'm not overly confident with the matrix calculus, but the result is nice enough to seem plausible. Is there another method, or is this proof valid? Thanks!",,"['linear-algebra', 'matrices', 'determinant', 'partial-derivative']"
32,Eigenvalues of Matrix vs Eigenvalues of Operator,Eigenvalues of Matrix vs Eigenvalues of Operator,,"I'm having some trouble reconciling the concept of eigenvalues of operators with eigenvalues of matrices: Say you have an $n\times n$ matrix $A$. It represents a linear operator $T:V\to V$ with respect to some basis $\{e_i\}$ in the background. Now my understanding is that 1.) Whatever the basis is, it has no effect on the eigenvalues of $A$. I.e. solutions to $\text{det}(A-\lambda I)=0$ gives the same solutions regardless if we have $\{e_i\}$ or $\{e_i'\}$ as the basis in the background, so long as we keep the entries of $A$ the same in both cases. 2.) The eigenvalues of $A$ are the same as the eigenvalues of $T$ as long as we use the basis $\{e_i\}$ in which $A$ represents $T$. However, if we keep the entries of $A$ the same, and change the basis in the background, then $A$ represents a different linear operator $T'$. This seems contradictory since  \begin{align} \{\text{eigenvalues of} \ T\}&=\{\text{eigenvalues of} \ A \ \text{with respect to basis} \{e_i\}\} \ \ \text{by 2}\\ &=\{\text{eigenvalues of} \ A \ \text{with respect to basis} \{e_i'\}\} \ \ \text{by 1} \\ &=\{\text{eigenvalues of} \ T'\} \ \ \text{by 2} \end{align} But there's no reason the two operators $T$ and $T'$ should have the same eigenvalues. Can someone point out what's wrong here? Any help would be appreciated.","I'm having some trouble reconciling the concept of eigenvalues of operators with eigenvalues of matrices: Say you have an $n\times n$ matrix $A$. It represents a linear operator $T:V\to V$ with respect to some basis $\{e_i\}$ in the background. Now my understanding is that 1.) Whatever the basis is, it has no effect on the eigenvalues of $A$. I.e. solutions to $\text{det}(A-\lambda I)=0$ gives the same solutions regardless if we have $\{e_i\}$ or $\{e_i'\}$ as the basis in the background, so long as we keep the entries of $A$ the same in both cases. 2.) The eigenvalues of $A$ are the same as the eigenvalues of $T$ as long as we use the basis $\{e_i\}$ in which $A$ represents $T$. However, if we keep the entries of $A$ the same, and change the basis in the background, then $A$ represents a different linear operator $T'$. This seems contradictory since  \begin{align} \{\text{eigenvalues of} \ T\}&=\{\text{eigenvalues of} \ A \ \text{with respect to basis} \{e_i\}\} \ \ \text{by 2}\\ &=\{\text{eigenvalues of} \ A \ \text{with respect to basis} \{e_i'\}\} \ \ \text{by 1} \\ &=\{\text{eigenvalues of} \ T'\} \ \ \text{by 2} \end{align} But there's no reason the two operators $T$ and $T'$ should have the same eigenvalues. Can someone point out what's wrong here? Any help would be appreciated.",,['linear-algebra']
33,Calculating SVD by hand: resolving sign ambiguities in the range vectors.,Calculating SVD by hand: resolving sign ambiguities in the range vectors.,,"When calculating the SVD of the matrix $$A = \begin{bmatrix}3&1&1\\-1&3&1\end{bmatrix}$$ I followed these steps $$A A^{T} = \begin{bmatrix}3&1&1\\-1&3&1\end{bmatrix} \begin{bmatrix}3&-1\\1&3\\1&1\end{bmatrix} = \begin{bmatrix}11&1\\1&11\end{bmatrix}$$ $$\det(A A^{T} - \lambda I) = (11-\lambda)^{2} - 1 = 0$$ Hence, the eigenvalues are $\lambda_{1} = 12$ and $\lambda_{2} = 10$. When $\lambda_{1} = 12$: $$ \begin{bmatrix}11-\lambda_{1}&1\\1&11-\lambda_{1}\end{bmatrix} \begin{bmatrix}x_{1}\\x_{2}\end{bmatrix} = \begin{bmatrix}0\\0\end{bmatrix}$$ $$x_{1} = x_{2} \implies u_{1} = \begin{bmatrix}t\\t\end{bmatrix}$$ And for $\lambda_{2} = 10$: $$x_{1} = -x_{2} \implies u_{2} = \begin {bmatrix}t\\-t\end{bmatrix}$$ Now $$U = \begin {bmatrix} u_{1}&u_{2} \end{bmatrix}$$ $u_{1}$ and $u_{2}$ are orthonormal. So the for $u_{1} = \begin{bmatrix}t\\t\end{bmatrix}$ , $u_{2} = \begin{bmatrix}t\\-t\end{bmatrix}$ I know $\left| t  \right| = \frac{1}{\sqrt{2}}$ and $u_{1}.u_{2}=0$. My question how can we decide about the sign? For example I think both $U=  \begin{bmatrix}\frac{1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &\frac{-1}{\sqrt{2}} \end{bmatrix}$ and $U=\begin{bmatrix}\frac{-1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \end{bmatrix}$ could be answers. Then Which one should I choose? ====== Update1: Based on answers posted I rewrite: $u_{1} = sgn (t_1) \begin{bmatrix} \frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}\end{bmatrix}$ $u_{2} = sgn (t_2) \begin{bmatrix} \frac{1}{\sqrt{2}}\\ \frac{-1}{\sqrt{2}}\end{bmatrix}$ $$U= \begin{bmatrix} \frac{1}{\sqrt{2}}& \frac{-1}{\sqrt{2}} \\  \frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\end{bmatrix} \begin{bmatrix} \operatorname{sgn} (t_1)&0 \\ 0& \operatorname{sgn} (t_2) \end{bmatrix}$$ ====== Update2: I continued by calculation of $V$ as follow: $ A^{T} A = \begin{bmatrix}3&-1\\1&\\1&1\end{bmatrix} \begin{bmatrix}3&1&1\\-1&3&1\end{bmatrix}  = \begin{bmatrix}10&0&2\\0&10&4\\2&4&2\end{bmatrix}$ $det( A^{T} A- \lambda I)=0$ $\lambda_{1} = 12 , v_1 =  sgn(t_3) \begin{bmatrix}t_{3}\\ 2t_{3} \\ t_{3} \end{bmatrix}$ $\lambda_{2} = 10 ,  V_{2} = sgn(t_4) \begin{bmatrix}t_{4}\\ -0.5t_{4} \\ 0 \end{bmatrix}$ $\lambda_{3} = 0 ,  V_{3} = sgn(t_5) \begin{bmatrix}t_{5}\\ 2t_{5} \\ -5t_{5} \end{bmatrix}$ $V= \begin{bmatrix}\frac{1}{\sqrt{6}} &\frac{2}{\sqrt{5}} &\frac{1}{\sqrt{30}}\\ \frac{2}{\sqrt{6}}&\frac{-1}{\sqrt{5}}&\frac{2}{\sqrt{30}}\\ \frac{1}{\sqrt{6}}& 0& \frac{-5}{\sqrt{30}}\end{bmatrix}\begin{bmatrix} \operatorname{sgn} (t_3)&0&0 \\ 0& \operatorname{sgn} (t_4)&0\\ 0&0& \operatorname{sgn} (t_5) \end{bmatrix}$ I try to check if all possible answers for U and V are valid : $A = U\Sigma V^{*}$  $A = \begin{bmatrix} \frac{1}{\sqrt{2}}& \frac{-1}{\sqrt{2}} \\  \frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\end{bmatrix} \begin{bmatrix} \operatorname{sgn} (t_1)&0 \\ 0& \operatorname{sgn} (t_2) \end{bmatrix} \Sigma  (\begin{bmatrix}\frac{1}{\sqrt{6}} &\frac{2}{\sqrt{5}} &\frac{1}{\sqrt{30}}\\ \frac{2}{\sqrt{6}}&\frac{-1}{\sqrt{5}}&\frac{2}{\sqrt{30}}\\ \frac{1}{\sqrt{6}}& 0& \frac{-5}{\sqrt{30}}\end{bmatrix} \begin{bmatrix} \operatorname{sgn} (t_3)&0&0 \\ 0& \operatorname{sgn} (t_4)&0\\ 0&0& \operatorname{sgn} (t_5) \end{bmatrix} )^{*} $  $A = \begin{bmatrix} \frac{1}{\sqrt{2}}& \frac{-1}{\sqrt{2}} \\  \frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\end{bmatrix} \begin{bmatrix} \operatorname{sgn} (t_1)&0 \\ 0& \operatorname{sgn} (t_2) \end{bmatrix} \Sigma  \begin{bmatrix} \operatorname{sgn} (t_3)&0&0 \\ 0& \operatorname{sgn} (t_4)&0\\ 0&0& \operatorname{sgn} (t_5) \end{bmatrix} \begin{bmatrix}\frac{1}{\sqrt{6}} &\frac{2}{\sqrt{5}} &\frac{1}{\sqrt{30}}\\ \frac{2}{\sqrt{6}}&\frac{-1}{\sqrt{5}}&\frac{2}{\sqrt{30}}\\ \frac{1}{\sqrt{6}}& 0& \frac{-5}{\sqrt{30}}\end{bmatrix}^{*} $ When I assigned $U=  \begin{bmatrix}\frac{-1}{\sqrt{2}} &\frac{-1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \end{bmatrix}$  and $V= \begin{bmatrix}\frac{-1}{\sqrt{6}} &\frac{-2}{\sqrt{5}} &\frac{-1}{\sqrt{30}}\\ \frac{-2}{\sqrt{6}}&\frac{1}{\sqrt{5}} & \frac{-2}{\sqrt{30}} \\ \frac{-1}{\sqrt{6}}& 0& \frac{5}{\sqrt{30}}\end{bmatrix}$ and $\Sigma = \begin{bmatrix}\sqrt{20}&0&0\\ 0&\sqrt{10}&0\end{bmatrix}  $in $A = U\Sigma V^{*}$ I got the A. But when I updated U as $U = \begin{bmatrix}\frac{1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &\frac{-1}{\sqrt{2}} \end{bmatrix}$, it produced -A. This probably means certain version of $U$ and $V$ will reproduce A. I haven't  figured how should I choose them.","When calculating the SVD of the matrix $$A = \begin{bmatrix}3&1&1\\-1&3&1\end{bmatrix}$$ I followed these steps $$A A^{T} = \begin{bmatrix}3&1&1\\-1&3&1\end{bmatrix} \begin{bmatrix}3&-1\\1&3\\1&1\end{bmatrix} = \begin{bmatrix}11&1\\1&11\end{bmatrix}$$ $$\det(A A^{T} - \lambda I) = (11-\lambda)^{2} - 1 = 0$$ Hence, the eigenvalues are $\lambda_{1} = 12$ and $\lambda_{2} = 10$. When $\lambda_{1} = 12$: $$ \begin{bmatrix}11-\lambda_{1}&1\\1&11-\lambda_{1}\end{bmatrix} \begin{bmatrix}x_{1}\\x_{2}\end{bmatrix} = \begin{bmatrix}0\\0\end{bmatrix}$$ $$x_{1} = x_{2} \implies u_{1} = \begin{bmatrix}t\\t\end{bmatrix}$$ And for $\lambda_{2} = 10$: $$x_{1} = -x_{2} \implies u_{2} = \begin {bmatrix}t\\-t\end{bmatrix}$$ Now $$U = \begin {bmatrix} u_{1}&u_{2} \end{bmatrix}$$ $u_{1}$ and $u_{2}$ are orthonormal. So the for $u_{1} = \begin{bmatrix}t\\t\end{bmatrix}$ , $u_{2} = \begin{bmatrix}t\\-t\end{bmatrix}$ I know $\left| t  \right| = \frac{1}{\sqrt{2}}$ and $u_{1}.u_{2}=0$. My question how can we decide about the sign? For example I think both $U=  \begin{bmatrix}\frac{1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &\frac{-1}{\sqrt{2}} \end{bmatrix}$ and $U=\begin{bmatrix}\frac{-1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \end{bmatrix}$ could be answers. Then Which one should I choose? ====== Update1: Based on answers posted I rewrite: $u_{1} = sgn (t_1) \begin{bmatrix} \frac{1}{\sqrt{2}}\\ \frac{1}{\sqrt{2}}\end{bmatrix}$ $u_{2} = sgn (t_2) \begin{bmatrix} \frac{1}{\sqrt{2}}\\ \frac{-1}{\sqrt{2}}\end{bmatrix}$ $$U= \begin{bmatrix} \frac{1}{\sqrt{2}}& \frac{-1}{\sqrt{2}} \\  \frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\end{bmatrix} \begin{bmatrix} \operatorname{sgn} (t_1)&0 \\ 0& \operatorname{sgn} (t_2) \end{bmatrix}$$ ====== Update2: I continued by calculation of $V$ as follow: $ A^{T} A = \begin{bmatrix}3&-1\\1&\\1&1\end{bmatrix} \begin{bmatrix}3&1&1\\-1&3&1\end{bmatrix}  = \begin{bmatrix}10&0&2\\0&10&4\\2&4&2\end{bmatrix}$ $det( A^{T} A- \lambda I)=0$ $\lambda_{1} = 12 , v_1 =  sgn(t_3) \begin{bmatrix}t_{3}\\ 2t_{3} \\ t_{3} \end{bmatrix}$ $\lambda_{2} = 10 ,  V_{2} = sgn(t_4) \begin{bmatrix}t_{4}\\ -0.5t_{4} \\ 0 \end{bmatrix}$ $\lambda_{3} = 0 ,  V_{3} = sgn(t_5) \begin{bmatrix}t_{5}\\ 2t_{5} \\ -5t_{5} \end{bmatrix}$ $V= \begin{bmatrix}\frac{1}{\sqrt{6}} &\frac{2}{\sqrt{5}} &\frac{1}{\sqrt{30}}\\ \frac{2}{\sqrt{6}}&\frac{-1}{\sqrt{5}}&\frac{2}{\sqrt{30}}\\ \frac{1}{\sqrt{6}}& 0& \frac{-5}{\sqrt{30}}\end{bmatrix}\begin{bmatrix} \operatorname{sgn} (t_3)&0&0 \\ 0& \operatorname{sgn} (t_4)&0\\ 0&0& \operatorname{sgn} (t_5) \end{bmatrix}$ I try to check if all possible answers for U and V are valid : $A = U\Sigma V^{*}$  $A = \begin{bmatrix} \frac{1}{\sqrt{2}}& \frac{-1}{\sqrt{2}} \\  \frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\end{bmatrix} \begin{bmatrix} \operatorname{sgn} (t_1)&0 \\ 0& \operatorname{sgn} (t_2) \end{bmatrix} \Sigma  (\begin{bmatrix}\frac{1}{\sqrt{6}} &\frac{2}{\sqrt{5}} &\frac{1}{\sqrt{30}}\\ \frac{2}{\sqrt{6}}&\frac{-1}{\sqrt{5}}&\frac{2}{\sqrt{30}}\\ \frac{1}{\sqrt{6}}& 0& \frac{-5}{\sqrt{30}}\end{bmatrix} \begin{bmatrix} \operatorname{sgn} (t_3)&0&0 \\ 0& \operatorname{sgn} (t_4)&0\\ 0&0& \operatorname{sgn} (t_5) \end{bmatrix} )^{*} $  $A = \begin{bmatrix} \frac{1}{\sqrt{2}}& \frac{-1}{\sqrt{2}} \\  \frac{1}{\sqrt{2}}& \frac{1}{\sqrt{2}}\end{bmatrix} \begin{bmatrix} \operatorname{sgn} (t_1)&0 \\ 0& \operatorname{sgn} (t_2) \end{bmatrix} \Sigma  \begin{bmatrix} \operatorname{sgn} (t_3)&0&0 \\ 0& \operatorname{sgn} (t_4)&0\\ 0&0& \operatorname{sgn} (t_5) \end{bmatrix} \begin{bmatrix}\frac{1}{\sqrt{6}} &\frac{2}{\sqrt{5}} &\frac{1}{\sqrt{30}}\\ \frac{2}{\sqrt{6}}&\frac{-1}{\sqrt{5}}&\frac{2}{\sqrt{30}}\\ \frac{1}{\sqrt{6}}& 0& \frac{-5}{\sqrt{30}}\end{bmatrix}^{*} $ When I assigned $U=  \begin{bmatrix}\frac{-1}{\sqrt{2}} &\frac{-1}{\sqrt{2}} \\\frac{-1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \end{bmatrix}$  and $V= \begin{bmatrix}\frac{-1}{\sqrt{6}} &\frac{-2}{\sqrt{5}} &\frac{-1}{\sqrt{30}}\\ \frac{-2}{\sqrt{6}}&\frac{1}{\sqrt{5}} & \frac{-2}{\sqrt{30}} \\ \frac{-1}{\sqrt{6}}& 0& \frac{5}{\sqrt{30}}\end{bmatrix}$ and $\Sigma = \begin{bmatrix}\sqrt{20}&0&0\\ 0&\sqrt{10}&0\end{bmatrix}  $in $A = U\Sigma V^{*}$ I got the A. But when I updated U as $U = \begin{bmatrix}\frac{1}{\sqrt{2}} &\frac{1}{\sqrt{2}} \\\frac{1}{\sqrt{2}} &\frac{-1}{\sqrt{2}} \end{bmatrix}$, it produced -A. This probably means certain version of $U$ and $V$ will reproduce A. I haven't  figured how should I choose them.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'svd']"
34,What is the name of the 3D matrices?,What is the name of the 3D matrices?,,The name of a variable in the $\mathbb{R}$ is called scalar. Multiple scalars form a vector: $\mathbb{R}^n$ Two or more vectors form together a matrix: $\mathbb{R}^{n \times m }$ But what is the name of the 3 dimensional array of elments? ($\mathbb{R}^{n \times m \times k }$),The name of a variable in the $\mathbb{R}$ is called scalar. Multiple scalars form a vector: $\mathbb{R}^n$ Two or more vectors form together a matrix: $\mathbb{R}^{n \times m }$ But what is the name of the 3 dimensional array of elments? ($\mathbb{R}^{n \times m \times k }$),,"['linear-algebra', 'matrices', 'vectors']"
35,Any left ideal of $M_n(\mathbb{F})$ is principal,Any left ideal of  is principal,M_n(\mathbb{F}),"I'm working on the following problem: Let $A$ be the ring of $n \times n$ matrices over a field $\mathbb{F}$ . (a) Show that for any subspace $V$ of $\mathbb{F}^n$ , the set $I_V$ of matrices whose kernel contains $V$ is a left ideal of $A$ . (b) Show that every left ideal of $A$ is principal. I've done part $a)$ , but would like to know if you can prove $b)$ directly from $a)$ . It seems to me that given the left ideal $J$ , it should be the case that if $V$ is the intersection of the kernels of matrices in $J$ , then we should have $J = I_V$ . I can show that $I_V$ is principal, and certainly $J$ is contained in $I_V$ , but I can't show the other direction. I think you can prove $b)$ by considering the subspace $W$ of $\mathbb{F}^n$ consisting of the rows of elements of $J$ , which is of dimension $k \leq n$ say, and then showing that $J$ is generated by any matrix whose first $k$ rows are some basis for $W$ and whose final rows are all $0$ . But it seems that we should be able to do the problem just using $a)$ , and I'd like to know how to do it!","I'm working on the following problem: Let be the ring of matrices over a field . (a) Show that for any subspace of , the set of matrices whose kernel contains is a left ideal of . (b) Show that every left ideal of is principal. I've done part , but would like to know if you can prove directly from . It seems to me that given the left ideal , it should be the case that if is the intersection of the kernels of matrices in , then we should have . I can show that is principal, and certainly is contained in , but I can't show the other direction. I think you can prove by considering the subspace of consisting of the rows of elements of , which is of dimension say, and then showing that is generated by any matrix whose first rows are some basis for and whose final rows are all . But it seems that we should be able to do the problem just using , and I'd like to know how to do it!",A n \times n \mathbb{F} V \mathbb{F}^n I_V V A A a) b) a) J V J J = I_V I_V J I_V b) W \mathbb{F}^n J k \leq n J k W 0 a),"['linear-algebra', 'abstract-algebra', 'ring-theory', 'ideals']"
36,Eigenvalues of a row-reduced matrix on the main diagonal still?,Eigenvalues of a row-reduced matrix on the main diagonal still?,,"It is widely known that if a matrix is given in upper triangular form, then one can just read off the eigenvalues (and their algebraic multiplicity) on the main diagonal of the matrix. My question is:  what if I get a non-upper triangular matrix to start, and I then put it into row-echelon form - not the row-reduced echelon form with all 1's in the pivot variables.  Can I spot any of the eigenvalues of the original matrix from this upper triangular matrix? Thanks,","It is widely known that if a matrix is given in upper triangular form, then one can just read off the eigenvalues (and their algebraic multiplicity) on the main diagonal of the matrix. My question is:  what if I get a non-upper triangular matrix to start, and I then put it into row-echelon form - not the row-reduced echelon form with all 1's in the pivot variables.  Can I spot any of the eigenvalues of the original matrix from this upper triangular matrix? Thanks,",,"['linear-algebra', 'matrices']"
37,Isomorphism between $E_8$ lattice and lattice defined by Extended Hamming Code,Isomorphism between  lattice and lattice defined by Extended Hamming Code,E_8,"I have read that the following two lattices are isomorphic, and of course it seems believable, but it would be nice to have a sketch of how to construct the bijection. Let $C$ be some extended $(8,4,4)$ Hamming Code. For example, let it be generated by: $$G = \left(\begin{array}{cccc} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 1 & 1 & 0 & 1 \\ 1 & 0 & 1 & 1 \\ 0 & 1 & 1 & 1 \\ {\bf 1} & {\bf 1} & {\bf 1} & {\bf 0}\end{array}\right)$$ Let $L$ be the lattice defined as containing all points in $\mathbb{Z}^8$ such that reduction modulo $2$ component-by-component ($\phi$) gives you a codeword, or in other words, $L = \{x \in \mathbb{Z}^8: \phi(x) \in C\}$. It is well-known and pretty easy to show that the minimum norm is $2$. These minimum norm vectors of norm $2$ corresponding to weight $0$ codewords have a $\pm 2$ in one of the $8$ entries, so there are $2 \times 8 = 16$ of these. The vectors of norm $2$ corresponding to weight $4$ codewords have a $\pm 1$ in four digits. There are $14$ weight $4$ codewords, so there are $14 \times 2^4$ of these. Then there are $2^4 + 14 \times 2^4 = 15 \times 16 = 240$ vectors of minimum norm $2$. Let $E$ by the $E_8$ lattice with the even construction so that the parity is even and all points are either all integral or all half-integral. By a similar argument, there are also $240$ vectors of minimum norm $\sqrt{2}$. But these look different. For example, any vectors with a $1$ in two of the eight positions has minimum norm $2$, and each of these you can permute the signs to give ${{8}\choose{2}} \times 4$ of them. Any half-integer vector of minimum norm $2$ must have all $\frac{1}{2}$ with an even number of positive (or negative) entries, so there are ${{8}\choose{2}} \times 4 + \left({{8}\choose{8}} + {{8}\choose{6}} + {{8}\choose{4}} + {{8}\choose{2}} + {{8}\choose{0}}\right) = 112 + 2^7 = 240$ total minimum norm $\sqrt{2}$. My problem is that the vectors of minimum norm in $L$ and $E$ are totally different, even after rescaling. In $L$, there are vectors with $1$ and $4$ nonzero entries, while in $E$ there are vectors with $2$ and $8$ nonzero entries. Plus, the way that the minimum norm vectors are counted doesn't give a lot of insight. How do you get from $E$ to $L$ (and vice-versa)?","I have read that the following two lattices are isomorphic, and of course it seems believable, but it would be nice to have a sketch of how to construct the bijection. Let $C$ be some extended $(8,4,4)$ Hamming Code. For example, let it be generated by: $$G = \left(\begin{array}{cccc} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 1 & 1 & 0 & 1 \\ 1 & 0 & 1 & 1 \\ 0 & 1 & 1 & 1 \\ {\bf 1} & {\bf 1} & {\bf 1} & {\bf 0}\end{array}\right)$$ Let $L$ be the lattice defined as containing all points in $\mathbb{Z}^8$ such that reduction modulo $2$ component-by-component ($\phi$) gives you a codeword, or in other words, $L = \{x \in \mathbb{Z}^8: \phi(x) \in C\}$. It is well-known and pretty easy to show that the minimum norm is $2$. These minimum norm vectors of norm $2$ corresponding to weight $0$ codewords have a $\pm 2$ in one of the $8$ entries, so there are $2 \times 8 = 16$ of these. The vectors of norm $2$ corresponding to weight $4$ codewords have a $\pm 1$ in four digits. There are $14$ weight $4$ codewords, so there are $14 \times 2^4$ of these. Then there are $2^4 + 14 \times 2^4 = 15 \times 16 = 240$ vectors of minimum norm $2$. Let $E$ by the $E_8$ lattice with the even construction so that the parity is even and all points are either all integral or all half-integral. By a similar argument, there are also $240$ vectors of minimum norm $\sqrt{2}$. But these look different. For example, any vectors with a $1$ in two of the eight positions has minimum norm $2$, and each of these you can permute the signs to give ${{8}\choose{2}} \times 4$ of them. Any half-integer vector of minimum norm $2$ must have all $\frac{1}{2}$ with an even number of positive (or negative) entries, so there are ${{8}\choose{2}} \times 4 + \left({{8}\choose{8}} + {{8}\choose{6}} + {{8}\choose{4}} + {{8}\choose{2}} + {{8}\choose{0}}\right) = 112 + 2^7 = 240$ total minimum norm $\sqrt{2}$. My problem is that the vectors of minimum norm in $L$ and $E$ are totally different, even after rescaling. In $L$, there are vectors with $1$ and $4$ nonzero entries, while in $E$ there are vectors with $2$ and $8$ nonzero entries. Plus, the way that the minimum norm vectors are counted doesn't give a lot of insight. How do you get from $E$ to $L$ (and vice-versa)?",,"['linear-algebra', 'abstract-algebra', 'combinatorics', 'coding-theory']"
38,Show the existence of a polynomial $f$ such that $T^{-1} = f(T)$ for an invertible linear operator $T.$,Show the existence of a polynomial  such that  for an invertible linear operator,f T^{-1} = f(T) T.,Came across this question and have had difficulty approaching it so any help is greatly appreciated. Let $T$ be an invertible linear operator on a finite dimensional vector space $V$ over a field $F$. Prove that there exists a polynomial $f$ over $F$ such that $T^{-1} = f(T)$.,Came across this question and have had difficulty approaching it so any help is greatly appreciated. Let $T$ be an invertible linear operator on a finite dimensional vector space $V$ over a field $F$. Prove that there exists a polynomial $f$ over $F$ such that $T^{-1} = f(T)$.,,"['linear-algebra', 'polynomials']"
39,Countable/uncountable basis of vector space,Countable/uncountable basis of vector space,,"I've stumbled upon this exercise in algebra book, in chapter dealing with vector spaces' dimensions. Prove that basis of the field of real numbers $\mathbb R$ as vector   space over the field of rational numbers $\mathbb Q$ is not countable. Book doesn't cover terms ""countable"" and ""uncountable"" and I'm not able to put web info about these topics into something useful for this exercise. updated: After receiving great answer by Asaf Karagila, which I unfortunately cannot fully comprehend, I've come with my own idea of proof. Let's count all real numbers in this situation. We have countable basis, and any vector of vector space $\mathbb R$ can have only finite subset of coefficients in it not equal to zero. We can pick different combination of finite coefficients which will not be equal to zero with countable number of ways (Asaf Karagila's fact III). So, already we have countable number of different ways to choose single vector. But more than that, in every choosen finite set of the coefficients every one of it will be different from vector to vector. Every coefficient can be one different value taken from countable $\mathbb Q$ set, and we have finite number of coefficients for any finite set choosen earlier. To sum up, we have countable number of ways to choose set of coefficients for any vector, and finite number of countable sequent ways to choose concrete values for each coefficient in basis representation of vector. We've got Cartesian product of   (finite + 1) = finite   countable sets, which is countable set again (property of countable set). So, we conclude that there are countable number of different real numbers, and that's wrong, so the basis of vector space $\mathbb R$ cannot be countable. I have very strong doubts about it though. Please correct me and point to unclear parts of it.","I've stumbled upon this exercise in algebra book, in chapter dealing with vector spaces' dimensions. Prove that basis of the field of real numbers $\mathbb R$ as vector   space over the field of rational numbers $\mathbb Q$ is not countable. Book doesn't cover terms ""countable"" and ""uncountable"" and I'm not able to put web info about these topics into something useful for this exercise. updated: After receiving great answer by Asaf Karagila, which I unfortunately cannot fully comprehend, I've come with my own idea of proof. Let's count all real numbers in this situation. We have countable basis, and any vector of vector space $\mathbb R$ can have only finite subset of coefficients in it not equal to zero. We can pick different combination of finite coefficients which will not be equal to zero with countable number of ways (Asaf Karagila's fact III). So, already we have countable number of different ways to choose single vector. But more than that, in every choosen finite set of the coefficients every one of it will be different from vector to vector. Every coefficient can be one different value taken from countable $\mathbb Q$ set, and we have finite number of coefficients for any finite set choosen earlier. To sum up, we have countable number of ways to choose set of coefficients for any vector, and finite number of countable sequent ways to choose concrete values for each coefficient in basis representation of vector. We've got Cartesian product of   (finite + 1) = finite   countable sets, which is countable set again (property of countable set). So, we conclude that there are countable number of different real numbers, and that's wrong, so the basis of vector space $\mathbb R$ cannot be countable. I have very strong doubts about it though. Please correct me and point to unclear parts of it.",,"['linear-algebra', 'abstract-algebra', 'elementary-set-theory']"
40,Is there a mistake in a GRE preparation book?,Is there a mistake in a GRE preparation book?,,"Preface: I am preparing for the GRE Math subject test, and one of the books that I use is this one by Charles Rambo. I have currently been solving through the “Linear Algebra #1” section of the book, and there is one problem that I think has an incorrect answer in the solutions section. Let me state the problem as it was drawn in the book: Let $V$ be the set of all possible polynomials $p(x)$ with coefficients in $\mathbb{R}$ . And let the linear operators $T$ and $S$ be defined over $V$ as follows: $$ T(p(x)) = xp(x) \\ S(p(x)) = \frac{d}{dx}p(x) = p’(x)  $$ We denote $ST$ and $TS$ as the expected compositions $S\circ T(p(x))$ and $T \circ S(p(x))$ respectively. Which of the following is true? $ST = 0$ $ST = T$ $ST = TS$ $ST - TS$ is the identity map of $V$ $ST + TS$ is the identity map of $V$ My attempt: In my opinion, the correct option is (4), since: $ST = \frac{d}{dx}\left[xp(x)\right] = x’p(x) + xp’(x) = p(x)+xp’(x)$ $TS = x \cdot \frac{d}{dx}p(x) = xp’(x)$ $F(p(x)) = (ST-TS)(p(x))$ is a transformation defined over $V$ , then $F(p(x)) = (ST - TS)(p(x)) = p(x)+xp’(x)-xp’(x) = p(x)$ , hence $F : V \mapsto V$ , in other terms $F$ forms an identity map over $V$ . Book’s solution: In fact, the only thing written in the book about this problem (page 84 in the pdf) was that the answer is (5), and that one had to use the product rule to obtain the correct answer (no explicit solution given). And so I wonder whether there is any fallacy in my logic above, or it is the book, which is mistaken. Any help will be greatly appreciated. Thank you in advance!","Preface: I am preparing for the GRE Math subject test, and one of the books that I use is this one by Charles Rambo. I have currently been solving through the “Linear Algebra #1” section of the book, and there is one problem that I think has an incorrect answer in the solutions section. Let me state the problem as it was drawn in the book: Let be the set of all possible polynomials with coefficients in . And let the linear operators and be defined over as follows: We denote and as the expected compositions and respectively. Which of the following is true? is the identity map of is the identity map of My attempt: In my opinion, the correct option is (4), since: is a transformation defined over , then , hence , in other terms forms an identity map over . Book’s solution: In fact, the only thing written in the book about this problem (page 84 in the pdf) was that the answer is (5), and that one had to use the product rule to obtain the correct answer (no explicit solution given). And so I wonder whether there is any fallacy in my logic above, or it is the book, which is mistaken. Any help will be greatly appreciated. Thank you in advance!","V p(x) \mathbb{R} T S V 
T(p(x)) = xp(x)
\\
S(p(x)) = \frac{d}{dx}p(x) = p’(x) 
 ST TS S\circ T(p(x)) T \circ S(p(x)) ST = 0 ST = T ST = TS ST - TS V ST + TS V ST = \frac{d}{dx}\left[xp(x)\right] = x’p(x) + xp’(x) = p(x)+xp’(x) TS = x \cdot \frac{d}{dx}p(x) = xp’(x) F(p(x)) = (ST-TS)(p(x)) V F(p(x)) = (ST - TS)(p(x)) = p(x)+xp’(x)-xp’(x) = p(x) F : V \mapsto V F V","['linear-algebra', 'solution-verification', 'linear-transformations', 'gre-exam']"
41,Why eigenvectors with the highest eigenvalues maximize the variance in PCA?,Why eigenvectors with the highest eigenvalues maximize the variance in PCA?,,"I'm learning Principal Component Analysis (PCA) and came to know that eigenvectors of the covariance matrix of the data are the principal components, which maximizes the variance of the projected data. I understand the intuition behind why we need the variance of projected data as large as possible. From this answer, I don't understand the following line: The unit vector $u$ which maximizes variance $u^TΣu$ is nothing but the eigenvector with the largest eigenvalue. I know how the variance of projected data points is $u^TΣu$ from this answer. But I don't understand why this will be maxed when $u$ is selected as eigenvectors of $u^TΣu$ with the highest eigenvalues? Intuitively I see eigenvectors as the vectors which stay fixed in their direction under the given linear transformation (values may scale, which are known as eigenvalues). Source: This answer. and this video. I can't relate why vectors with a fixed direction under given linear transformation give the highest variance? Any intuitive explanation will help! Thanks.","I'm learning Principal Component Analysis (PCA) and came to know that eigenvectors of the covariance matrix of the data are the principal components, which maximizes the variance of the projected data. I understand the intuition behind why we need the variance of projected data as large as possible. From this answer, I don't understand the following line: The unit vector which maximizes variance is nothing but the eigenvector with the largest eigenvalue. I know how the variance of projected data points is from this answer. But I don't understand why this will be maxed when is selected as eigenvectors of with the highest eigenvalues? Intuitively I see eigenvectors as the vectors which stay fixed in their direction under the given linear transformation (values may scale, which are known as eigenvalues). Source: This answer. and this video. I can't relate why vectors with a fixed direction under given linear transformation give the highest variance? Any intuitive explanation will help! Thanks.",u u^TΣu u^TΣu u u^TΣu,"['linear-algebra', 'eigenvalues-eigenvectors', 'linear-transformations', 'machine-learning', 'principal-component-analysis']"
42,How many cows are there?,How many cows are there?,,"In a field, the grass increases in a constant rate. $17$ cows can eat all the grass in the field in $30$ days. $19$ cows can eat all the grass in $24$ days. Suppose,  a group of cows  started eating grass for $6$ days. Then $4$ cows are sold. The rest cows took $2$ more days to eat the remaining grass. What is the number of cows in the group? My trying: A cow eats a certain amount of grass in one day, call it $c$.  The field grows by a certain amount each day, call it $g$. The field has some initial amount of grass: $i$ \begin{equation} \begin{aligned} i + 30g - 17\cdot30c &= 0\\ i+24g-19\cdot24c&=0 \end{aligned} \end{equation} Solving these two equations we get , $g = 9c$ . That means It takes 9 cows to eat one day's growth in one day.","In a field, the grass increases in a constant rate. $17$ cows can eat all the grass in the field in $30$ days. $19$ cows can eat all the grass in $24$ days. Suppose,  a group of cows  started eating grass for $6$ days. Then $4$ cows are sold. The rest cows took $2$ more days to eat the remaining grass. What is the number of cows in the group? My trying: A cow eats a certain amount of grass in one day, call it $c$.  The field grows by a certain amount each day, call it $g$. The field has some initial amount of grass: $i$ \begin{equation} \begin{aligned} i + 30g - 17\cdot30c &= 0\\ i+24g-19\cdot24c&=0 \end{aligned} \end{equation} Solving these two equations we get , $g = 9c$ . That means It takes 9 cows to eat one day's growth in one day.",,"['linear-algebra', 'algebra-precalculus', 'word-problem']"
43,Orthogonal Projection onto the Unit Simplex,Orthogonal Projection onto the Unit Simplex,,"The Unit Simplex is defined by: $$ \mathcal{S} = \left\{ x \in \mathbb{{R}^{n}} \mid x \succeq 0, \, \boldsymbol{1}^{T} x = 1 \right\} $$ Orthogonal Projection onto the Unit Simplex is defined by: $$ \begin{alignat*}{3} \arg \min_{x} & \quad & \frac{1}{2} \left\| x - y \right\|_{2}^{2} \\ \text{subject to} & \quad & x \succeq 0 \\ & \quad & \boldsymbol{1}^{T} x = 1 \end{alignat*} $$ How could one solve this convex optimization problem?","The Unit Simplex is defined by: $$ \mathcal{S} = \left\{ x \in \mathbb{{R}^{n}} \mid x \succeq 0, \, \boldsymbol{1}^{T} x = 1 \right\} $$ Orthogonal Projection onto the Unit Simplex is defined by: $$ \begin{alignat*}{3} \arg \min_{x} & \quad & \frac{1}{2} \left\| x - y \right\|_{2}^{2} \\ \text{subject to} & \quad & x \succeq 0 \\ & \quad & \boldsymbol{1}^{T} x = 1 \end{alignat*} $$ How could one solve this convex optimization problem?",,"['linear-algebra', 'optimization', 'convex-optimization', 'matlab']"
44,How to resolve the sign issue in a SVD problem?,How to resolve the sign issue in a SVD problem?,,"Question: When performing a simple Singular Value Decomposition , how can I know that my sign choice for the eigenvectors of the left- and right-singular matrices will result in the correct matrix without just guessing and checking? If it makes things easier, feel free to restrict your answers to just real-valued or real-valued, square matrices. Context Consider the matrix $$A=\begin{pmatrix}2&-4\\4&4\end{pmatrix}$$ which has the left-singular matrix $$AA^T=\begin{pmatrix}20&-8\\-8&32\end{pmatrix}$$ and the right-singular matrix $$A^TA=\begin{pmatrix}20&8\\8&32\end{pmatrix}$$ The eigenvalues for both matrices are $36$ and $16$ (meaning the singular values of $A$ are $6$ and $4$, respectively). The normalized left-singular eigenvectors are $$\textbf{u}_{36}=\frac{1}{\sqrt{5}}\begin{pmatrix}1\\-2\end{pmatrix}\ \ \ \textbf{u}_{16}=\frac{1}{\sqrt{5}}\begin{pmatrix}2\\1\end{pmatrix}$$ and the normalized right-singular eigenvectors are $$\textbf{v}_{36}=\frac{1}{\sqrt{5}}\begin{pmatrix}1\\2\end{pmatrix}\ \ \ \textbf{v}_{16}=\frac{1}{\sqrt{5}}\begin{pmatrix}-2\\1\end{pmatrix}$$ With these in hand, we can construct the SVD which should look like this: $$A=U\Sigma V^T=\frac{1}{5}\begin{pmatrix}1&2\\-2&1\end{pmatrix}\begin{pmatrix}6&0\\0&4\end{pmatrix}\begin{pmatrix}1&2\\-2&1\end{pmatrix}$$ However, if you actually perform the matrix multiplication, the result is $$U\Sigma V^T=\begin{pmatrix}-2&4\\-4&-4\end{pmatrix}= -A \neq A$$ Since the normalized eigenvectors are unique only up to a sign, one resolution to this problem is to choose $$\textbf{u}_{36}=\frac{1}{\sqrt{5}}\begin{pmatrix}-1\\2\end{pmatrix} \ \ \ \ \textbf{v}_{16}=\frac{1}{\sqrt{5}}\begin{pmatrix}2\\-1\end{pmatrix}$$ which produces the correct SVD $$U\Sigma V^T=\frac{1}{5}\begin{pmatrix}-1&2\\2&1\end{pmatrix}\begin{pmatrix}6&0\\0&4\end{pmatrix}\begin{pmatrix}1&2\\2&-1\end{pmatrix}=\begin{pmatrix}2&-4\\4&4\end{pmatrix}=A$$ This begs the question: How was I supposed to know that I had chosen the wrong sign convention for my eigenvectors without checking it by hand? I have a suspicion that the correct sign convention corresponds to the sum of the components of the eigenvectors being positive (and if they sum to zero then the topmost component should be made positive), but this seems like a pretty arbitrary condition despite it holding for several examples that I have checked.","Question: When performing a simple Singular Value Decomposition , how can I know that my sign choice for the eigenvectors of the left- and right-singular matrices will result in the correct matrix without just guessing and checking? If it makes things easier, feel free to restrict your answers to just real-valued or real-valued, square matrices. Context Consider the matrix $$A=\begin{pmatrix}2&-4\\4&4\end{pmatrix}$$ which has the left-singular matrix $$AA^T=\begin{pmatrix}20&-8\\-8&32\end{pmatrix}$$ and the right-singular matrix $$A^TA=\begin{pmatrix}20&8\\8&32\end{pmatrix}$$ The eigenvalues for both matrices are $36$ and $16$ (meaning the singular values of $A$ are $6$ and $4$, respectively). The normalized left-singular eigenvectors are $$\textbf{u}_{36}=\frac{1}{\sqrt{5}}\begin{pmatrix}1\\-2\end{pmatrix}\ \ \ \textbf{u}_{16}=\frac{1}{\sqrt{5}}\begin{pmatrix}2\\1\end{pmatrix}$$ and the normalized right-singular eigenvectors are $$\textbf{v}_{36}=\frac{1}{\sqrt{5}}\begin{pmatrix}1\\2\end{pmatrix}\ \ \ \textbf{v}_{16}=\frac{1}{\sqrt{5}}\begin{pmatrix}-2\\1\end{pmatrix}$$ With these in hand, we can construct the SVD which should look like this: $$A=U\Sigma V^T=\frac{1}{5}\begin{pmatrix}1&2\\-2&1\end{pmatrix}\begin{pmatrix}6&0\\0&4\end{pmatrix}\begin{pmatrix}1&2\\-2&1\end{pmatrix}$$ However, if you actually perform the matrix multiplication, the result is $$U\Sigma V^T=\begin{pmatrix}-2&4\\-4&-4\end{pmatrix}= -A \neq A$$ Since the normalized eigenvectors are unique only up to a sign, one resolution to this problem is to choose $$\textbf{u}_{36}=\frac{1}{\sqrt{5}}\begin{pmatrix}-1\\2\end{pmatrix} \ \ \ \ \textbf{v}_{16}=\frac{1}{\sqrt{5}}\begin{pmatrix}2\\-1\end{pmatrix}$$ which produces the correct SVD $$U\Sigma V^T=\frac{1}{5}\begin{pmatrix}-1&2\\2&1\end{pmatrix}\begin{pmatrix}6&0\\0&4\end{pmatrix}\begin{pmatrix}1&2\\2&-1\end{pmatrix}=\begin{pmatrix}2&-4\\4&4\end{pmatrix}=A$$ This begs the question: How was I supposed to know that I had chosen the wrong sign convention for my eigenvectors without checking it by hand? I have a suspicion that the correct sign convention corresponds to the sum of the components of the eigenvectors being positive (and if they sum to zero then the topmost component should be made positive), but this seems like a pretty arbitrary condition despite it holding for several examples that I have checked.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'svd']"
45,Proving property of Fibonacci sequence with group theory,Proving property of Fibonacci sequence with group theory,,"I stumbled upon this problem on a chapter about group theory, and I can't seem to get a grasp on it: Let $p$ be a prime and $F_1 = F  _2 = 1, F_{n +2} = F_{n +1} + F_n$ be the Fibonacci sequence. Show that $F_{2p(p^2−1)}$ is divisible by $p$. The book gives the following hint: Look at the group of $2×2$ matrices mod $p$ with determinant $± 1$ I know I have to use lagrange theorem at some point, but how do I stablish the link between the fibonacci sequence and the matrices? I mean, I know that there's a special $2×2$ matrix such that it yields the $n^{th}$ fibonacci number when taking the $n^{th}$ power, I even used it to obtain a general formula for the numbers during my linear algebra course, but how can I use that to prove this?","I stumbled upon this problem on a chapter about group theory, and I can't seem to get a grasp on it: Let $p$ be a prime and $F_1 = F  _2 = 1, F_{n +2} = F_{n +1} + F_n$ be the Fibonacci sequence. Show that $F_{2p(p^2−1)}$ is divisible by $p$. The book gives the following hint: Look at the group of $2×2$ matrices mod $p$ with determinant $± 1$ I know I have to use lagrange theorem at some point, but how do I stablish the link between the fibonacci sequence and the matrices? I mean, I know that there's a special $2×2$ matrix such that it yields the $n^{th}$ fibonacci number when taking the $n^{th}$ power, I even used it to obtain a general formula for the numbers during my linear algebra course, but how can I use that to prove this?",,"['linear-algebra', 'group-theory']"
46,"Why does the fact that ""$Tv$ is orthogonal to $v$ for all $v$ implies T is the zero operator"" break down for real inner product spaces?","Why does the fact that "" is orthogonal to  for all  implies T is the zero operator"" break down for real inner product spaces?",Tv v v,"Here's the awkwardly named theorem 7.14 (for which I can't think of a good name either) appearing in Axler's Linear Algebra Done Right, 3rd edition, p 210: The proof is algebraic, and I can't glean from it any intuition about why this theorem breaks down over $\mathbf{R}$, as the author claimed. What's so special about $\mathbf{C}$ that allows an inner product to be written in the above form, which seems impossible for the inner product over $\mathbf{R}$? I would also appreciate alternative (and more ""intuitive"", or perhaps  elementary) proofs for this result.","Here's the awkwardly named theorem 7.14 (for which I can't think of a good name either) appearing in Axler's Linear Algebra Done Right, 3rd edition, p 210: The proof is algebraic, and I can't glean from it any intuition about why this theorem breaks down over $\mathbf{R}$, as the author claimed. What's so special about $\mathbf{C}$ that allows an inner product to be written in the above form, which seems impossible for the inner product over $\mathbf{R}$? I would also appreciate alternative (and more ""intuitive"", or perhaps  elementary) proofs for this result.",,"['linear-algebra', 'complex-analysis', 'inner-products', 'proof-explanation']"
47,Eigenvalues of the principal submatrix of a Hermitian matrix,Eigenvalues of the principal submatrix of a Hermitian matrix,,"This question aims at creating an "" abstract duplicate "" of various questions that can be reduced to the following: Let $A$ be an $n\times n$ Hermitian matrix and $B$ be an $r\times r$ principal submatrix of $A$. How are the eigenvalues of $A$ and $B$ related? Here are some questions on this site that can be viewed as duplicates of this question: Eigenvalues of $MA$ versus eigenvalues of $A$ for orthogonal projection $M$ Relationship of eigenvalues of a diagonal matrix D and $\mathbf{VDV}^{T}$, where V is a semi-orthogonal matrix","This question aims at creating an "" abstract duplicate "" of various questions that can be reduced to the following: Let $A$ be an $n\times n$ Hermitian matrix and $B$ be an $r\times r$ principal submatrix of $A$. How are the eigenvalues of $A$ and $B$ related? Here are some questions on this site that can be viewed as duplicates of this question: Eigenvalues of $MA$ versus eigenvalues of $A$ for orthogonal projection $M$ Relationship of eigenvalues of a diagonal matrix D and $\mathbf{VDV}^{T}$, where V is a semi-orthogonal matrix",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'faq']"
48,What is an intuitive explanation to why elimination retains solution to the original system of equations?,What is an intuitive explanation to why elimination retains solution to the original system of equations?,,"I've studied linear algebra before, however, I wanted to come back to the foundations and understand it again from the beginning. I was looking the following inoffensive linear equations: $$ x - 2y = 1 $$ $$ 3x + 2y = 11 $$ and after elimination one has: $$ x - 2y = 1 $$ $$ 8y = 8 $$ the claim is that both systems of equations have the same solution. Geometrically: if you've never seen this before it nearly seems like magic! We managed to combine different equations and still retain that the solutions is preserved. The new equations seems to be completely different and the lines that we care about don't even have same gradients. Basically, at first glance, the problem seems like it dramatically changed! So even though, the system seems to have changed a lot, in reality, it didn't change that much since they still intersect at the same point. My question is, what is the intuition to why manipulating system of equations in this way and combining them retains the original solution. The intuition/justification that I used to think of was that, if we combine equations, in principal, the ""total"" information that we had at the beginning of the system is preserved as long we are combining different equations and we don't discard one any of them. Basically, combining two equations implicitly retains the information that we had about the old equation. However, we can ""forget"" about the old form of the new equation because that information is preserved even though the equation changed. i.e. its ok to combine equation 1 and 2 to form 2' and discard 2, since 2' AND 1 contains all the information of the original system. This is sort of the intuition that I use but I wasn't sure if that was a good way to think about it or if people maybe had better intuitions or justification to why elimination worked.","I've studied linear algebra before, however, I wanted to come back to the foundations and understand it again from the beginning. I was looking the following inoffensive linear equations: $$ x - 2y = 1 $$ $$ 3x + 2y = 11 $$ and after elimination one has: $$ x - 2y = 1 $$ $$ 8y = 8 $$ the claim is that both systems of equations have the same solution. Geometrically: if you've never seen this before it nearly seems like magic! We managed to combine different equations and still retain that the solutions is preserved. The new equations seems to be completely different and the lines that we care about don't even have same gradients. Basically, at first glance, the problem seems like it dramatically changed! So even though, the system seems to have changed a lot, in reality, it didn't change that much since they still intersect at the same point. My question is, what is the intuition to why manipulating system of equations in this way and combining them retains the original solution. The intuition/justification that I used to think of was that, if we combine equations, in principal, the ""total"" information that we had at the beginning of the system is preserved as long we are combining different equations and we don't discard one any of them. Basically, combining two equations implicitly retains the information that we had about the old equation. However, we can ""forget"" about the old form of the new equation because that information is preserved even though the equation changed. i.e. its ok to combine equation 1 and 2 to form 2' and discard 2, since 2' AND 1 contains all the information of the original system. This is sort of the intuition that I use but I wasn't sure if that was a good way to think about it or if people maybe had better intuitions or justification to why elimination worked.",,"['linear-algebra', 'intuition', 'gaussian-elimination']"
49,Find this Determinant,Find this Determinant,,"I have to find this determinant, call it $D$  \begin{vmatrix}     \frac12 & \frac1{3}& \frac1{4} & \dots  & \frac1{n+1} \\     \frac1{3} & \frac14 & \frac15 & \dots  & \frac1{n+2} \\     \vdots & \vdots & \vdots & \ddots & \vdots \\     \frac1{n+1} & \frac1{n+2} & \frac1{n+3} & \dots  & \frac1{2n} \end{vmatrix} As there are no zeros in there, despite being a symmetric matrix, finding this determinant is tough for me. Is there any tricks etc. I do not know any softwares to find this determinant. I tried to make a pattern by calculating for $n=1,2,3,\dots$ $n=1 \hspace{5cm}D_1=\frac12\\ n=2 \hspace{5cm} D_2=\frac12\frac14-\frac13\frac13\\ \\n=3 \hspace{5cm} D_3=\frac12\frac14\frac16-\frac13\frac13\frac16-\frac12\frac15\frac15-\frac14\frac14\frac14+\frac13\frac14\frac15+\frac13\frac14\frac15$ What I spot from here is, to get $D_2$, (even case) We get $\frac12\frac14$ by multiplying diagonal entries and then subtracting $\frac1x\frac1x$ where $x=\frac{2+4}2$ Next, to get $D_3$ (odd case),  we  get our first term i.e. $\frac12\frac14\frac16$ by multiplying diagonal entries and to obtain the rest we follow this pattern that subtract $\frac1x\frac1y\frac1z$, where first we fix $x=2$ and make $y=z=\frac{4+6}2=5$, similarly next we subtract by fixing $x=6$ and $y=z=\frac{2+4}2=3$ and then by fixing $x=4$ and $y=z=\frac{4+4}2=4$, and to get terms that get added we add terms of the form $\frac1x\frac1y\frac1z$ by putting $x=\frac{2+4}2 , y=\frac{4+4}2=4,z= \frac{4+6}2=5$, but we do it $2$ times. Now out of curiosity I had to calculate $n=4,5$. Here are them- For $n=4 \text{(even case)} \hspace{3cm} D_4=\frac1{2.4.6.8}-\frac1{2.5.5.8}-\frac1{2.4.7.7}-\frac1{2.6.6.6}\frac1{3.3.6.8}-\frac1{3.4.6.7}-\frac1{5.5.3.7}-\frac1{3.4.6.7}-\frac1{4.4.4.8}-\frac1{4.5.5.6}-\frac1{3.5.5.7}-\frac1{4.5.5.6}-\frac1{4.5.5.6}+\frac1{2.5.6.7}+\frac1{2.5.6.7}+\frac1{4.5.5.6}+\frac1{3.3.7.7}+\frac1{3.4.5.8}+\frac1{3.6.6.5}+\frac1{3.4.5.8}+\frac1{4.4.5.7}+\frac1{4.4.6.6}+\frac1{3.6.6.5}+\frac1{5.5.5.5}+\frac1{4.4.5.7}$. This helps somewhat in recognizing a pattern in even case, but to be sure one has to find $n=6$ case too. I guess $n=5$ case will be enough to recognize a pattern, if the one above mentioned in $n=3$ works, as by that, $D_5$ should come out to be $D_5=\frac1{2.4.6.8.10}- {^5C_2} \text{terms of the form} \frac1{x_1x_2x_3x_4x_5}, \text{where any three terms say} x_1,x_2,x_3$  are fixed out of $2,4,6,8,10$ and $x_4,x_5$ takes values of mean of the other two remaining and similarly for positive terms, but they seem very less number of terms as there were already $12$ negative terms in expansion of $D_4$, so here some more negative terms will appear and their pattern can be known by only finding them, after a few steps, may be upto $n=11$ or $12$, we can see a general pattern appearing. I am sure after calculating all this that there is a pattern, but may be too complex to find by hand, as calculations gets huge, and it is also probably a hammer to kill an ant, as I am not aware of any other trick, I worked all this out, may be someone can find a quicker solution?","I have to find this determinant, call it $D$  \begin{vmatrix}     \frac12 & \frac1{3}& \frac1{4} & \dots  & \frac1{n+1} \\     \frac1{3} & \frac14 & \frac15 & \dots  & \frac1{n+2} \\     \vdots & \vdots & \vdots & \ddots & \vdots \\     \frac1{n+1} & \frac1{n+2} & \frac1{n+3} & \dots  & \frac1{2n} \end{vmatrix} As there are no zeros in there, despite being a symmetric matrix, finding this determinant is tough for me. Is there any tricks etc. I do not know any softwares to find this determinant. I tried to make a pattern by calculating for $n=1,2,3,\dots$ $n=1 \hspace{5cm}D_1=\frac12\\ n=2 \hspace{5cm} D_2=\frac12\frac14-\frac13\frac13\\ \\n=3 \hspace{5cm} D_3=\frac12\frac14\frac16-\frac13\frac13\frac16-\frac12\frac15\frac15-\frac14\frac14\frac14+\frac13\frac14\frac15+\frac13\frac14\frac15$ What I spot from here is, to get $D_2$, (even case) We get $\frac12\frac14$ by multiplying diagonal entries and then subtracting $\frac1x\frac1x$ where $x=\frac{2+4}2$ Next, to get $D_3$ (odd case),  we  get our first term i.e. $\frac12\frac14\frac16$ by multiplying diagonal entries and to obtain the rest we follow this pattern that subtract $\frac1x\frac1y\frac1z$, where first we fix $x=2$ and make $y=z=\frac{4+6}2=5$, similarly next we subtract by fixing $x=6$ and $y=z=\frac{2+4}2=3$ and then by fixing $x=4$ and $y=z=\frac{4+4}2=4$, and to get terms that get added we add terms of the form $\frac1x\frac1y\frac1z$ by putting $x=\frac{2+4}2 , y=\frac{4+4}2=4,z= \frac{4+6}2=5$, but we do it $2$ times. Now out of curiosity I had to calculate $n=4,5$. Here are them- For $n=4 \text{(even case)} \hspace{3cm} D_4=\frac1{2.4.6.8}-\frac1{2.5.5.8}-\frac1{2.4.7.7}-\frac1{2.6.6.6}\frac1{3.3.6.8}-\frac1{3.4.6.7}-\frac1{5.5.3.7}-\frac1{3.4.6.7}-\frac1{4.4.4.8}-\frac1{4.5.5.6}-\frac1{3.5.5.7}-\frac1{4.5.5.6}-\frac1{4.5.5.6}+\frac1{2.5.6.7}+\frac1{2.5.6.7}+\frac1{4.5.5.6}+\frac1{3.3.7.7}+\frac1{3.4.5.8}+\frac1{3.6.6.5}+\frac1{3.4.5.8}+\frac1{4.4.5.7}+\frac1{4.4.6.6}+\frac1{3.6.6.5}+\frac1{5.5.5.5}+\frac1{4.4.5.7}$. This helps somewhat in recognizing a pattern in even case, but to be sure one has to find $n=6$ case too. I guess $n=5$ case will be enough to recognize a pattern, if the one above mentioned in $n=3$ works, as by that, $D_5$ should come out to be $D_5=\frac1{2.4.6.8.10}- {^5C_2} \text{terms of the form} \frac1{x_1x_2x_3x_4x_5}, \text{where any three terms say} x_1,x_2,x_3$  are fixed out of $2,4,6,8,10$ and $x_4,x_5$ takes values of mean of the other two remaining and similarly for positive terms, but they seem very less number of terms as there were already $12$ negative terms in expansion of $D_4$, so here some more negative terms will appear and their pattern can be known by only finding them, after a few steps, may be upto $n=11$ or $12$, we can see a general pattern appearing. I am sure after calculating all this that there is a pattern, but may be too complex to find by hand, as calculations gets huge, and it is also probably a hammer to kill an ant, as I am not aware of any other trick, I worked all this out, may be someone can find a quicker solution?",,"['linear-algebra', 'matrices', 'determinant']"
50,Solving a system of non-linear equations,Solving a system of non-linear equations,,"Let $$(\star)\begin{cases}  \begin{vmatrix}  x&y\\  z&x\\  \end{vmatrix}=1,   \\   \begin{vmatrix}  y&z\\  x&y\\  \end{vmatrix}=2,  \\   \begin{vmatrix}  z&x\\  y&z\\  \end{vmatrix}=3.   \end{cases}$$ Solving the above system of three non-linear equations with three unknowns. I have a try. Let$$A=\begin{bmatrix}  1&  1/2& -1/2\\   1/2&  1& -1/2\\   -1/2& -1/2& -1  \end{bmatrix}$$ We have $$(x,y,z)A\begin{pmatrix} x\\  y\\  z \end{pmatrix}=0.$$ There must be a orthogonal matrix $T$,such that $T^{-1}A T=diag \begin{Bmatrix} \frac{1}{2},\frac{\sqrt{33}+1}{4},-\frac{\sqrt{33}-1}{4} \end{Bmatrix}.$ $$\begin{pmatrix} x\\  y\\  z \end{pmatrix}=T\begin{pmatrix} x^{'}\\  y^{'}\\  z^{'} \end{pmatrix}\Longrightarrow\frac{1}{2} {x'}^{2}+\frac{\sqrt{33}+1}{4} {y'}^{2}-\frac{\sqrt{33}-1}{4}{z'}^{2}=0.$$ But even if we find a $\begin{pmatrix} x_0^{'}\\  y_0^{'}\\  z_0^{'} \end{pmatrix} $ satisfying $\frac{1}{2} {x_0'}^{2}+\frac{\sqrt{33}+1}{4} {y_0'}^{2}-\frac{\sqrt{33}-1}{4}{z_0'}^{2}=0,\begin{pmatrix} x_0\\  y_0\\  z_0 \end{pmatrix}=T\begin{pmatrix} x_0^{'}\\  y_0^{'}\\  z_0^{'} \end{pmatrix}$ may not be the solution of $(\star)$ If you have some  good ideas,please give me some hints. Any help would be appreciated!","Let $$(\star)\begin{cases}  \begin{vmatrix}  x&y\\  z&x\\  \end{vmatrix}=1,   \\   \begin{vmatrix}  y&z\\  x&y\\  \end{vmatrix}=2,  \\   \begin{vmatrix}  z&x\\  y&z\\  \end{vmatrix}=3.   \end{cases}$$ Solving the above system of three non-linear equations with three unknowns. I have a try. Let$$A=\begin{bmatrix}  1&  1/2& -1/2\\   1/2&  1& -1/2\\   -1/2& -1/2& -1  \end{bmatrix}$$ We have $$(x,y,z)A\begin{pmatrix} x\\  y\\  z \end{pmatrix}=0.$$ There must be a orthogonal matrix $T$,such that $T^{-1}A T=diag \begin{Bmatrix} \frac{1}{2},\frac{\sqrt{33}+1}{4},-\frac{\sqrt{33}-1}{4} \end{Bmatrix}.$ $$\begin{pmatrix} x\\  y\\  z \end{pmatrix}=T\begin{pmatrix} x^{'}\\  y^{'}\\  z^{'} \end{pmatrix}\Longrightarrow\frac{1}{2} {x'}^{2}+\frac{\sqrt{33}+1}{4} {y'}^{2}-\frac{\sqrt{33}-1}{4}{z'}^{2}=0.$$ But even if we find a $\begin{pmatrix} x_0^{'}\\  y_0^{'}\\  z_0^{'} \end{pmatrix} $ satisfying $\frac{1}{2} {x_0'}^{2}+\frac{\sqrt{33}+1}{4} {y_0'}^{2}-\frac{\sqrt{33}-1}{4}{z_0'}^{2}=0,\begin{pmatrix} x_0\\  y_0\\  z_0 \end{pmatrix}=T\begin{pmatrix} x_0^{'}\\  y_0^{'}\\  z_0^{'} \end{pmatrix}$ may not be the solution of $(\star)$ If you have some  good ideas,please give me some hints. Any help would be appreciated!",,"['linear-algebra', 'matrices']"
51,Prove that the real and imaginary parts of an eigenvector are linearly independent.,Prove that the real and imaginary parts of an eigenvector are linearly independent.,,"Say we have a 2 by 2 matrix $A$ with real entries and $A$ has a complex eigenvector $V = a+bi$ with corresponding complex eigenvalue $\lambda$. How do I prove that the vectors $\mathrm{Re}(V) = a$ and $\mathrm{Im}(V) = b$ are linearly independent? This is a common fact that is used to produce real solutions for a system of differential equations with complex eigenvalues and vectors. Assumptions and Facts: We know that $\overline{V} = a-bi$ is also an eigenvector with eigenvalue $\overline{\lambda}$. We obviously have that $V$ and $\overline{V}$ are linearly independent. (For one thing, eigenvectors of distinct eingenvalues must be LI.) I have started a few ways by trying to find a contradiction, assuming that there is a nonzero $k \in \mathbb{R}$ such $\mathrm{Re}(V)$ = $k\mathrm{Im}(V)$. I tried using the definitions of $\mathrm{Re}(V)$ and $\mathrm{Im}(V)$ in terms of $V$ and $\overline{V}$ but that didn't get me anywhere. I tried starting with the fact that $V$ and $\overline{V}$ are LI to show this directly implies that $V$ and $\overline{V}$ line must be LI. I also couldn't work this through. Please help me out!","Say we have a 2 by 2 matrix $A$ with real entries and $A$ has a complex eigenvector $V = a+bi$ with corresponding complex eigenvalue $\lambda$. How do I prove that the vectors $\mathrm{Re}(V) = a$ and $\mathrm{Im}(V) = b$ are linearly independent? This is a common fact that is used to produce real solutions for a system of differential equations with complex eigenvalues and vectors. Assumptions and Facts: We know that $\overline{V} = a-bi$ is also an eigenvector with eigenvalue $\overline{\lambda}$. We obviously have that $V$ and $\overline{V}$ are linearly independent. (For one thing, eigenvectors of distinct eingenvalues must be LI.) I have started a few ways by trying to find a contradiction, assuming that there is a nonzero $k \in \mathbb{R}$ such $\mathrm{Re}(V)$ = $k\mathrm{Im}(V)$. I tried using the definitions of $\mathrm{Re}(V)$ and $\mathrm{Im}(V)$ in terms of $V$ and $\overline{V}$ but that didn't get me anywhere. I tried starting with the fact that $V$ and $\overline{V}$ are LI to show this directly implies that $V$ and $\overline{V}$ line must be LI. I also couldn't work this through. Please help me out!",,"['linear-algebra', 'matrices']"
52,Why isn't the quotient space $V/V = \{ V \}$?,Why isn't the quotient space ?,V/V = \{ V \},"If $W \subset V$, then one defines the quotient space, $$V/W = \{ v + W : v \in V \}$$ So why isn't this right? $$V/V = \{v + V : v \in V \} = \{V \}$$? I read that $V/V = \{ 0 \}$? Why can't the whole set $V/V$ be partition, by $V$ itself?","If $W \subset V$, then one defines the quotient space, $$V/W = \{ v + W : v \in V \}$$ So why isn't this right? $$V/V = \{v + V : v \in V \} = \{V \}$$? I read that $V/V = \{ 0 \}$? Why can't the whole set $V/V$ be partition, by $V$ itself?",,['linear-algebra']
53,Finding a dual basis,Finding a dual basis,,"This is one of my homework questions - I'm pretty sure I understand part of it. Let $V=\Bbb R^3$, and define $f_1, f_2, f_3 \in V^*$ as follows: $$f_1(x,y,z) = x - 2y;\quad f_2(x,y,z) = x + y + z; \quad f_3(x,y,z) = y - 3z.$$ Prove that $\{f_1, f_2, f_3\}$ is a basis for $V^*$ (they are linearly independent, so this part is true), and then find a basis for $V$ for which it is the dual basis.  The textbook does a horrible job as explaining dual bases in general.  Can someone explain me the methods behind formulating the dual basis here? Thanks.","This is one of my homework questions - I'm pretty sure I understand part of it. Let $V=\Bbb R^3$, and define $f_1, f_2, f_3 \in V^*$ as follows: $$f_1(x,y,z) = x - 2y;\quad f_2(x,y,z) = x + y + z; \quad f_3(x,y,z) = y - 3z.$$ Prove that $\{f_1, f_2, f_3\}$ is a basis for $V^*$ (they are linearly independent, so this part is true), and then find a basis for $V$ for which it is the dual basis.  The textbook does a horrible job as explaining dual bases in general.  Can someone explain me the methods behind formulating the dual basis here? Thanks.",,"['linear-algebra', 'dual-spaces']"
54,"Given a square matrix A of order n, prove $\operatorname{rank}(A^n) = \operatorname{rank}(A^{n+1})$","Given a square matrix A of order n, prove",\operatorname{rank}(A^n) = \operatorname{rank}(A^{n+1}),"Given $A\in F^{n \times n}$ prove: $$\operatorname{rank}(A^n) = \operatorname{rank}(A^{n+1})$$ $\operatorname{rank}(A^{n+1}) \leq \operatorname{rank}(A^n)$ is easy, just from: How to prove $\text{Rank}(AB)\leq \min(\text{Rank}(A), \text{Rank}(B))$? But how can I prove the other direction? or should I do it otherwise?","Given $A\in F^{n \times n}$ prove: $$\operatorname{rank}(A^n) = \operatorname{rank}(A^{n+1})$$ $\operatorname{rank}(A^{n+1}) \leq \operatorname{rank}(A^n)$ is easy, just from: How to prove $\text{Rank}(AB)\leq \min(\text{Rank}(A), \text{Rank}(B))$? But how can I prove the other direction? or should I do it otherwise?",,"['linear-algebra', 'matrices']"
55,prove that a function is an inner product,prove that a function is an inner product,,"I would appreciate some assistance in answering the following problems.  We are moving so quickly through our advanced linear algebra material, I can't wrap my head around the key concepts.  Thank you. Let $V$ be the space of all continuously differentiable real valued functions on $[a, b]$. (i)  Define       $$\langle f,g\rangle = \int_a^bf(t)g(t) \, dt + \int_a^bf'(t)g'(t) \, dt.$$       Prove that $\langle , \rangle$  is an inner product on $V$. (ii)  Define that  $||f|| = \int_a^b|f(t)| \, dt + \int_a^b|f'(t)| \, dt$.  Prove that this defines a norm on V.","I would appreciate some assistance in answering the following problems.  We are moving so quickly through our advanced linear algebra material, I can't wrap my head around the key concepts.  Thank you. Let $V$ be the space of all continuously differentiable real valued functions on $[a, b]$. (i)  Define       $$\langle f,g\rangle = \int_a^bf(t)g(t) \, dt + \int_a^bf'(t)g'(t) \, dt.$$       Prove that $\langle , \rangle$  is an inner product on $V$. (ii)  Define that  $||f|| = \int_a^b|f(t)| \, dt + \int_a^b|f'(t)| \, dt$.  Prove that this defines a norm on V.",,"['linear-algebra', 'normed-spaces', 'inner-products']"
56,Total Derivative and Multilinear Functions,Total Derivative and Multilinear Functions,,"So I'm starting to work through Spivak's Calculus on Manifolds and I'm having a little trouble verifying some of the claims made in the book problems. To review: Given a function $\mathbf{f}:\mathbb{R}^{n}\to\mathbb{R}^m$ , we say that $\mathbf{f}$ is differentiable at a point $\mathbf{a}=(a^{1},\ldots,a^{n})\in\mathbb{R}^n$ (considered as a $1\times n$ matrix) if there exists a linear transformation, $D\mathbf{f}(\mathbf{a}):\mathbb{R}^n\to\mathbb{R}^m$ , (considered as an $m\times n$ matrix) such that $$\lim\limits_{\mathbf{h}\to\mathbf{a}}\dfrac{\|\mathbf{f}(\mathbf{a}+\mathbf{h})-\mathbf{f}(\mathbf{a})-D\mathbf{f}(\mathbf{a})(\mathbf{h})\|}{\|\mathbf{h}\|}=0.$$ $D\mathbf{f}(\mathbf{a})$ is called the total derivative or Jacobian matrix of $\mathbf{f}$ at $\mathbf{a}$ and is unique. We are then asked to prove that if $\mathbf{f}:\mathbb{R}^n\times\mathbb{R}^m\to\mathbb{R}^p$ is bilinear, then $$\lim\limits_{(\mathbf{h},\mathbf{k})\to\mathbf{0}}\dfrac{\|\mathbf{f}(\mathbf{h},\mathbf{k})\|}{\|(\mathbf{h},\mathbf{k})\|}=0.$$ The best approach I could think of was that $\|\mathbf{f}(\mathbf{h},\mathbf{k})\|=\|\mathbf{h}\|\cdot\|\mathbf{k}\|\cdot\|\mathbf{f}(\hat{\mathbf{h}},\hat{\mathbf{k}})\|$ , where $\hat{\mathbf{x}}=\frac{\mathbf{x}}{\|\mathbf{x}\|}$ . We then have that $\|\mathbf{h}\|\cdot\|\mathbf{k}\|\leq\|\mathbf{h}\|^{2}+\|\mathbf{k}\|^{2}=\|(\mathbf{h},\mathbf{k})\|^{2}$ , which gives us $$\dfrac{\|\mathbf{f}(\mathbf{h},\mathbf{k})\|}{\|(\mathbf{h},\mathbf{k})\|}\leq\|(\mathbf{h},\mathbf{k})\|\cdot\|\mathbf{f}(\hat{\mathbf{h}},\hat{\mathbf{k}})\|,$$ where $\|\hat{\mathbf{h}}\|=\|\hat{\mathbf{k}}\|=1$ . And since $\|\mathbf{f}(\mathbf{x},\mathbf{y})\|\leq\|\mathbf{f}(\hat{\mathbf{x}},\mathbf{y})\|\cdot\|\mathbf{x}\|$ (similarly for $\mathbf{y}$ ) then $f$ is a bounded ( continuous ) linear transformation and $$\|\mathbf{f}\|_{x}=\sup\{\|\mathbf{f}(\mathbf{x},\mathbf{y})\|:\|\mathbf{x}\|=1\}<\infty$$ $$\|\mathbf{f}\|_{y}=\sup\{\|\mathbf{f}(\mathbf{x},\mathbf{y})\|:\|\mathbf{y}\|=1\}<\infty,$$ which I was hoping would imply that $\|\mathbf{f}(\hat{\mathbf{h}},\hat{\mathbf{k}})\|<\infty$ , and so we'd have the result we're looking for. But I am stuck here. Assuming this result, I was able to complete the problem and prove that $$D\mathbf{f}(\mathbf{a},\mathbf{b})(\mathbf{x},\mathbf{y})=\mathbf{f}(\mathbf{a},\mathbf{y})+\mathbf{f}(\mathbf{x},\mathbf{b}).$$ The author then proceeds to ask us to prove the following: Given a multilinear function $\mathbf{f}:\mathbb{R}^{n_1}\times\cdots\times\mathbb{R}^{n_k}\to\mathbb{R}^p$ , show that for $\mathbf{h}=(\mathbf{h}_1,\dots,\mathbf{h}_k)$ , with $\mathbf{h}_i\in\mathbb{R}^{n_i}$ , we have $$\lim\limits_{\mathbf{h}\to\mathbf{0}}\dfrac{\|\mathbf{f}(\mathbf{a}_1,\ldots,\mathbf{a}_{i-1},\mathbf{h}_i,\mathbf{a}_{i+1}\ldots,\mathbf{a_{j-1}},\mathbf{h}_j,\mathbf{a}_{j+1},\ldots,\mathbf{a}_k)\|}{\|\mathbf{h}\|}=0,$$ for $i\neq j$ . Use this to prove that $$D\mathbf{f}(\mathbf{a}_1,\ldots,\mathbf{a}_k)(\mathbf{x}_1,\ldots,\mathbf{x}_k)=\sum_{i=1}^{k}{\mathbf{f}(\mathbf{a}_1,\ldots,\mathbf{a}_{i-1},\mathbf{x}_i,\mathbf{a}_{i+1}\ldots,\mathbf{a}_k)}.$$ Using the hint from the book of considering the bilinear function, $\mathbf{g}(\mathbf{x},\mathbf{y})=\mathbf{f}(\mathbf{a}_1,\ldots,\mathbf{x},\ldots,\mathbf{y},\ldots,\mathbf{a}_k)$ , I was able to show the first part of the problem assuming the above result (which I still can't prove). However I'm now also stuck on how to prove the rest of the question. For example, consider the specific case with three arguments. Using the fact that $\mathbf{f}$ is multilinear, we have $$\mathbf{f}(\mathbf{a}+\mathbf{h_1},\mathbf{b}+\mathbf{h_2},\mathbf{c}+\mathbf{h_3})-\mathbf{f}(\mathbf{a},\mathbf{b},\mathbf{c})=$$ $$\mathbf{f}(\mathbf{a},\mathbf{b},\mathbf{h_3})+\mathbf{f}(\mathbf{a},\mathbf{h_2},\mathbf{c})+\mathbf{f}(\mathbf{h_1},\mathbf{b},\mathbf{c}) +$$ $$+\mathbf{f}(\mathbf{a},\mathbf{h_2},\mathbf{h_3})+\mathbf{f}(\mathbf{h_1},\mathbf{b},\mathbf{h_3})+\mathbf{f}(\mathbf{h_1},\mathbf{h_2},\mathbf{c})+$$ $$+\mathbf{f}(\mathbf{h_1},\mathbf{h_2},\mathbf{h_3}).$$ The first three terms are what should be $D\mathbf{f}(\mathbf{a},\mathbf{b},\mathbf{c})$ while the next three will disappear in the limit given the first part of the proof. The trouble is how do I control the limit $\lim\limits_{\mathbf{h}\to\mathbf{0}}\frac{\|\mathbf{f}(\mathbf{h_1},\mathbf{h_2},\mathbf{h_3})\|}{\|(\mathbf{h_1},\mathbf{h_2},\mathbf{h_3})\|}$ ? This trouble arises in the general case as well, any time the number of $\mathbf{h_i}$ is more than two in any single term. E.g. how would I control the term $\mathbf{f}(\mathbf{a_1},\mathbf{a_2},\mathbf{h_3},\mathbf{h_4})$ , which has two "" $h$ "" and "" $a$ "" terms each? Thanks for any help. My analysis skills are a little rusty as I've been focusing on passing my algebra qual.","So I'm starting to work through Spivak's Calculus on Manifolds and I'm having a little trouble verifying some of the claims made in the book problems. To review: Given a function , we say that is differentiable at a point (considered as a matrix) if there exists a linear transformation, , (considered as an matrix) such that is called the total derivative or Jacobian matrix of at and is unique. We are then asked to prove that if is bilinear, then The best approach I could think of was that , where . We then have that , which gives us where . And since (similarly for ) then is a bounded ( continuous ) linear transformation and which I was hoping would imply that , and so we'd have the result we're looking for. But I am stuck here. Assuming this result, I was able to complete the problem and prove that The author then proceeds to ask us to prove the following: Given a multilinear function , show that for , with , we have for . Use this to prove that Using the hint from the book of considering the bilinear function, , I was able to show the first part of the problem assuming the above result (which I still can't prove). However I'm now also stuck on how to prove the rest of the question. For example, consider the specific case with three arguments. Using the fact that is multilinear, we have The first three terms are what should be while the next three will disappear in the limit given the first part of the proof. The trouble is how do I control the limit ? This trouble arises in the general case as well, any time the number of is more than two in any single term. E.g. how would I control the term , which has two "" "" and "" "" terms each? Thanks for any help. My analysis skills are a little rusty as I've been focusing on passing my algebra qual.","\mathbf{f}:\mathbb{R}^{n}\to\mathbb{R}^m \mathbf{f} \mathbf{a}=(a^{1},\ldots,a^{n})\in\mathbb{R}^n 1\times n D\mathbf{f}(\mathbf{a}):\mathbb{R}^n\to\mathbb{R}^m m\times n \lim\limits_{\mathbf{h}\to\mathbf{a}}\dfrac{\|\mathbf{f}(\mathbf{a}+\mathbf{h})-\mathbf{f}(\mathbf{a})-D\mathbf{f}(\mathbf{a})(\mathbf{h})\|}{\|\mathbf{h}\|}=0. D\mathbf{f}(\mathbf{a}) \mathbf{f} \mathbf{a} \mathbf{f}:\mathbb{R}^n\times\mathbb{R}^m\to\mathbb{R}^p \lim\limits_{(\mathbf{h},\mathbf{k})\to\mathbf{0}}\dfrac{\|\mathbf{f}(\mathbf{h},\mathbf{k})\|}{\|(\mathbf{h},\mathbf{k})\|}=0. \|\mathbf{f}(\mathbf{h},\mathbf{k})\|=\|\mathbf{h}\|\cdot\|\mathbf{k}\|\cdot\|\mathbf{f}(\hat{\mathbf{h}},\hat{\mathbf{k}})\| \hat{\mathbf{x}}=\frac{\mathbf{x}}{\|\mathbf{x}\|} \|\mathbf{h}\|\cdot\|\mathbf{k}\|\leq\|\mathbf{h}\|^{2}+\|\mathbf{k}\|^{2}=\|(\mathbf{h},\mathbf{k})\|^{2} \dfrac{\|\mathbf{f}(\mathbf{h},\mathbf{k})\|}{\|(\mathbf{h},\mathbf{k})\|}\leq\|(\mathbf{h},\mathbf{k})\|\cdot\|\mathbf{f}(\hat{\mathbf{h}},\hat{\mathbf{k}})\|, \|\hat{\mathbf{h}}\|=\|\hat{\mathbf{k}}\|=1 \|\mathbf{f}(\mathbf{x},\mathbf{y})\|\leq\|\mathbf{f}(\hat{\mathbf{x}},\mathbf{y})\|\cdot\|\mathbf{x}\| \mathbf{y} f \|\mathbf{f}\|_{x}=\sup\{\|\mathbf{f}(\mathbf{x},\mathbf{y})\|:\|\mathbf{x}\|=1\}<\infty \|\mathbf{f}\|_{y}=\sup\{\|\mathbf{f}(\mathbf{x},\mathbf{y})\|:\|\mathbf{y}\|=1\}<\infty, \|\mathbf{f}(\hat{\mathbf{h}},\hat{\mathbf{k}})\|<\infty D\mathbf{f}(\mathbf{a},\mathbf{b})(\mathbf{x},\mathbf{y})=\mathbf{f}(\mathbf{a},\mathbf{y})+\mathbf{f}(\mathbf{x},\mathbf{b}). \mathbf{f}:\mathbb{R}^{n_1}\times\cdots\times\mathbb{R}^{n_k}\to\mathbb{R}^p \mathbf{h}=(\mathbf{h}_1,\dots,\mathbf{h}_k) \mathbf{h}_i\in\mathbb{R}^{n_i} \lim\limits_{\mathbf{h}\to\mathbf{0}}\dfrac{\|\mathbf{f}(\mathbf{a}_1,\ldots,\mathbf{a}_{i-1},\mathbf{h}_i,\mathbf{a}_{i+1}\ldots,\mathbf{a_{j-1}},\mathbf{h}_j,\mathbf{a}_{j+1},\ldots,\mathbf{a}_k)\|}{\|\mathbf{h}\|}=0, i\neq j D\mathbf{f}(\mathbf{a}_1,\ldots,\mathbf{a}_k)(\mathbf{x}_1,\ldots,\mathbf{x}_k)=\sum_{i=1}^{k}{\mathbf{f}(\mathbf{a}_1,\ldots,\mathbf{a}_{i-1},\mathbf{x}_i,\mathbf{a}_{i+1}\ldots,\mathbf{a}_k)}. \mathbf{g}(\mathbf{x},\mathbf{y})=\mathbf{f}(\mathbf{a}_1,\ldots,\mathbf{x},\ldots,\mathbf{y},\ldots,\mathbf{a}_k) \mathbf{f} \mathbf{f}(\mathbf{a}+\mathbf{h_1},\mathbf{b}+\mathbf{h_2},\mathbf{c}+\mathbf{h_3})-\mathbf{f}(\mathbf{a},\mathbf{b},\mathbf{c})= \mathbf{f}(\mathbf{a},\mathbf{b},\mathbf{h_3})+\mathbf{f}(\mathbf{a},\mathbf{h_2},\mathbf{c})+\mathbf{f}(\mathbf{h_1},\mathbf{b},\mathbf{c}) + +\mathbf{f}(\mathbf{a},\mathbf{h_2},\mathbf{h_3})+\mathbf{f}(\mathbf{h_1},\mathbf{b},\mathbf{h_3})+\mathbf{f}(\mathbf{h_1},\mathbf{h_2},\mathbf{c})+ +\mathbf{f}(\mathbf{h_1},\mathbf{h_2},\mathbf{h_3}). D\mathbf{f}(\mathbf{a},\mathbf{b},\mathbf{c}) \lim\limits_{\mathbf{h}\to\mathbf{0}}\frac{\|\mathbf{f}(\mathbf{h_1},\mathbf{h_2},\mathbf{h_3})\|}{\|(\mathbf{h_1},\mathbf{h_2},\mathbf{h_3})\|} \mathbf{h_i} \mathbf{f}(\mathbf{a_1},\mathbf{a_2},\mathbf{h_3},\mathbf{h_4}) h a","['linear-algebra', 'differential-geometry', 'multivariable-calculus']"
57,An elementary way to show that the determinant is non zero,An elementary way to show that the determinant is non zero,,"Show that the determinant of the matrix \begin{pmatrix} a && -c && -b \\   b && a - 2c && -c -2b \\ c && b && a -2c  \end{pmatrix} is non zero for all integers $a,b,c$ where $abc \ne 0$ There is an interesting way to do this by using integral domains. It is easy to see that the polynomial $t^3 + 2t + 1$ is prime in $\mathbb{Z}[t]$ so the ring $\mathbb{Z}[t]/\langle t^3+2t+1\rangle$ is an integral domain and isomorphic to the ring $\{a + bu + cu^2 \vert \text{$a,b,c$ are integers}\}$ where $u$ is a root of the equation $t^3 +2t + 1 =0$ . Now consider integers $a,b,c$ and $x,y,z$ such that $abc \ne  0$ and $(a +bu + cu^2)(x + yu + zu^2) = 0$ . These are element of an integral domain so we must have that $x = y = z = 0$ since at least one of $a,b$ and $c$ are nonzero. But expanding the above equation we get a system of linear equations in terms of $x,y$ and $z$ . $$ \begin{aligned}  ax- cy -bz &= 0 \\  bx+ (a-2c)y + (-c-2b)z &= 0\\  c x+b y+ (a-2c)z &= 0  \end{aligned}  $$ If the determinant is $0$ then this equation will have a nontrivial solution which is not possible. This strategy requires the use of integral domains which is an abstract tool. Are there any elementary ways to solve this? It gets messy when we try to do row operations or try to expand the determinant. Update : After  Carl Schildkraut's and JimmyK4542's answer. A curious observation: We can also prove that the element $t^3 + rt + 1$ is prime in $\mathbb{Z}[t]$ for $r \ne 0, -2$ . Therefore by similar arguments we can say that the determinant of the matrix \begin{pmatrix} a && -c && -b \\   b && a - rc && -c -rb \\ c && b && a -rc  \end{pmatrix} is non zero for all integers $a,b,c,r$ where $abc \ne 0$ and $r \ne 0,-2$ .",Show that the determinant of the matrix is non zero for all integers where There is an interesting way to do this by using integral domains. It is easy to see that the polynomial is prime in so the ring is an integral domain and isomorphic to the ring where is a root of the equation . Now consider integers and such that and . These are element of an integral domain so we must have that since at least one of and are nonzero. But expanding the above equation we get a system of linear equations in terms of and . If the determinant is then this equation will have a nontrivial solution which is not possible. This strategy requires the use of integral domains which is an abstract tool. Are there any elementary ways to solve this? It gets messy when we try to do row operations or try to expand the determinant. Update : After  Carl Schildkraut's and JimmyK4542's answer. A curious observation: We can also prove that the element is prime in for . Therefore by similar arguments we can say that the determinant of the matrix is non zero for all integers where and .,"\begin{pmatrix} a && -c && -b \\
  b && a - 2c && -c -2b \\ c && b && a -2c
 \end{pmatrix} a,b,c abc \ne 0 t^3 + 2t + 1 \mathbb{Z}[t] \mathbb{Z}[t]/\langle t^3+2t+1\rangle \{a + bu + cu^2 \vert \text{a,b,c are integers}\} u t^3 +2t + 1 =0 a,b,c x,y,z abc \ne  0 (a +bu + cu^2)(x + yu + zu^2) = 0 x = y = z = 0 a,b c x,y z 
\begin{aligned} 
ax- cy -bz &= 0 \\ 
bx+ (a-2c)y + (-c-2b)z &= 0\\ 
c x+b y+ (a-2c)z &= 0 
\end{aligned} 
 0 t^3 + rt + 1 \mathbb{Z}[t] r \ne 0, -2 \begin{pmatrix} a && -c && -b \\
  b && a - rc && -c -rb \\ c && b && a -rc
 \end{pmatrix} a,b,c,r abc \ne 0 r \ne 0,-2","['linear-algebra', 'abstract-algebra', 'matrices', 'elementary-number-theory', 'determinant']"
58,Example of $V^* \otimes V^*$ not isomorphic to $(V \otimes V)^*$,Example of  not isomorphic to,V^* \otimes V^* (V \otimes V)^*,"There is always an injection between $V^* \otimes V^*$ and $(V \otimes V)^*$ given by $$ f(v^* \otimes w^*)(x \otimes y)=v^*(x)w^*(y), $$ where $x,y \in V$. I've been given to understand that in infinite dimension it is not surjective. Does anybody can explain me why it is the case?  Does anybody have a concrete and simple example where $V^* \otimes V^*$ is not isomorphic to $(V \otimes V)^*$ Edit. This problem is involved in basic theory of Hopf Algebra since the fact that in infinite dimension $V^* \otimes V^*$ is only a proper subset of $(V \otimes V)^*$  is the key point for which the dual of a co-algebra is an algebra, but in general the dual of an algebra is not a co-algebra. Second Edit The answer given here doesn't seem to answer my question as the answer of Mariano does. So even if the question may seem a duplicate, the answer is not. And as Hardmath notes "" the proposed duplicate does not restrict to the tensor product of a vector space with itself, and indeed the answer given there involves one factor being the dual of the other factor. It doesn't address the problem here. ""","There is always an injection between $V^* \otimes V^*$ and $(V \otimes V)^*$ given by $$ f(v^* \otimes w^*)(x \otimes y)=v^*(x)w^*(y), $$ where $x,y \in V$. I've been given to understand that in infinite dimension it is not surjective. Does anybody can explain me why it is the case?  Does anybody have a concrete and simple example where $V^* \otimes V^*$ is not isomorphic to $(V \otimes V)^*$ Edit. This problem is involved in basic theory of Hopf Algebra since the fact that in infinite dimension $V^* \otimes V^*$ is only a proper subset of $(V \otimes V)^*$  is the key point for which the dual of a co-algebra is an algebra, but in general the dual of an algebra is not a co-algebra. Second Edit The answer given here doesn't seem to answer my question as the answer of Mariano does. So even if the question may seem a duplicate, the answer is not. And as Hardmath notes "" the proposed duplicate does not restrict to the tensor product of a vector space with itself, and indeed the answer given there involves one factor being the dual of the other factor. It doesn't address the problem here. """,,"['linear-algebra', 'functional-analysis', 'tensor-products', 'hopf-algebras', 'coalgebras']"
59,How to calculate this determinant? [duplicate],How to calculate this determinant? [duplicate],,"This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 7 years ago . How to calculate this determinant? $$A=\begin{bmatrix}n-1&k&k&k&\ldots& k\\k&n-1&k&k&\ldots &k\\\ldots&\ldots&\ldots &&\ldots\\\\k&k&k&k&\ldots &n-1\\ \end{bmatrix}_{n\times n}$$ where $n,k\in \Bbb N$ are fixed. I tried for $n=3$ and got the characteristic polynomial as $(x-2-k)^2(x-2+2k).$ How to find it for general $n\in \Bbb N$?","This question already has answers here : Determinant of a rank $1$ update of a scalar matrix, or characteristic polynomial of a rank $1$ matrix (2 answers) Closed 7 years ago . How to calculate this determinant? $$A=\begin{bmatrix}n-1&k&k&k&\ldots& k\\k&n-1&k&k&\ldots &k\\\ldots&\ldots&\ldots &&\ldots\\\\k&k&k&k&\ldots &n-1\\ \end{bmatrix}_{n\times n}$$ where $n,k\in \Bbb N$ are fixed. I tried for $n=3$ and got the characteristic polynomial as $(x-2-k)^2(x-2+2k).$ How to find it for general $n\in \Bbb N$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
60,What is an Eigenbasis and how do I calculate it with the information below.,What is an Eigenbasis and how do I calculate it with the information below.,,"I have the matrix $$A = \begin{bmatrix} 4 & 2 & 2\\ 2 & 4 & 2\\ 2 & 2 & 4 \end{bmatrix}$$ I've calculated the Eigenvalues and Eigenvectors as follows with help in a previous question: $\lambda = 2$ or $8$ $$E_{\lambda = 2} = \left\{ \begin{bmatrix} v_1\\ v_2\\ v_3\\ \end{bmatrix} = a\begin{bmatrix} -1\\ 1\\ 0\\ \end{bmatrix} + b \begin{bmatrix} -1\\ 0\\ 1 \end{bmatrix} \mid a, b \in \mathbb R \right\}$$ $$E_{\lambda = 8} = \left\{ \begin{bmatrix} v_1\\ v_2\\ v_3\\ \end{bmatrix} = a \begin{bmatrix} 1\\ 1\\ 1\\ \end{bmatrix} \mid \mathbb R\right\}$$ Is the Eigenbasis simply: $$E_{\lambda = 2} = \operatorname{span}\left(\begin{bmatrix} -1\\ 1\\ 0\\ \end{bmatrix} \begin{bmatrix} -1\\ 0\\ 1\\ \end{bmatrix} \right) $$ $$E_{\lambda = 8} = \operatorname{span}\left(\begin{bmatrix} 1\\ 1\\ 1\\ \end{bmatrix}\right)$$","I have the matrix $$A = \begin{bmatrix} 4 & 2 & 2\\ 2 & 4 & 2\\ 2 & 2 & 4 \end{bmatrix}$$ I've calculated the Eigenvalues and Eigenvectors as follows with help in a previous question: $\lambda = 2$ or $8$ $$E_{\lambda = 2} = \left\{ \begin{bmatrix} v_1\\ v_2\\ v_3\\ \end{bmatrix} = a\begin{bmatrix} -1\\ 1\\ 0\\ \end{bmatrix} + b \begin{bmatrix} -1\\ 0\\ 1 \end{bmatrix} \mid a, b \in \mathbb R \right\}$$ $$E_{\lambda = 8} = \left\{ \begin{bmatrix} v_1\\ v_2\\ v_3\\ \end{bmatrix} = a \begin{bmatrix} 1\\ 1\\ 1\\ \end{bmatrix} \mid \mathbb R\right\}$$ Is the Eigenbasis simply: $$E_{\lambda = 2} = \operatorname{span}\left(\begin{bmatrix} -1\\ 1\\ 0\\ \end{bmatrix} \begin{bmatrix} -1\\ 0\\ 1\\ \end{bmatrix} \right) $$ $$E_{\lambda = 8} = \operatorname{span}\left(\begin{bmatrix} 1\\ 1\\ 1\\ \end{bmatrix}\right)$$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations', 'proof-explanation']"
61,Find eigenvalues of unspecified matrix,Find eigenvalues of unspecified matrix,,"Find all possible eigenvalues of a $2\times 2$ matrix $A$ satisfying   $$\det(A^2)I-2\det(A)A+A^2=0.$$ Well, if $Av=\lambda v$ then $$\det(A^2)v-2\det(A)\lambda v+\lambda^2 v=(\det(A)^2-2\det(A)\lambda+\lambda^2) v=(\det(A)-\lambda)^2v=0$$ so $\lambda=\det(A)$. Can we go further and find $\det(A)$? Or perhaps a set of possible values? We have $$0=\det(\det(A^2)I-2\det(A)A+A^2)=(\det(\det(A)I-A))^2$$ so $$\det(A-\det(A)I)=0.$$ Can we solve for $\det(A)$? Edit: We even have $$(A-\det(A)I)^2=0$$ which is a set of 4 equations in 4 unknowns, so perhaps this fully determines $A$. But how?","Find all possible eigenvalues of a $2\times 2$ matrix $A$ satisfying   $$\det(A^2)I-2\det(A)A+A^2=0.$$ Well, if $Av=\lambda v$ then $$\det(A^2)v-2\det(A)\lambda v+\lambda^2 v=(\det(A)^2-2\det(A)\lambda+\lambda^2) v=(\det(A)-\lambda)^2v=0$$ so $\lambda=\det(A)$. Can we go further and find $\det(A)$? Or perhaps a set of possible values? We have $$0=\det(\det(A^2)I-2\det(A)A+A^2)=(\det(\det(A)I-A))^2$$ so $$\det(A-\det(A)I)=0.$$ Can we solve for $\det(A)$? Edit: We even have $$(A-\det(A)I)^2=0$$ which is a set of 4 equations in 4 unknowns, so perhaps this fully determines $A$. But how?",,"['linear-algebra', 'matrices']"
62,Decompose a 2D arbitrary transform into only scaling and rotation,Decompose a 2D arbitrary transform into only scaling and rotation,,"Related to: Newly Developed With Details - Describing orthographic projection using simple 2D transformations Given an arbitrary 2D linear transform (which may include shear, i.e. the vectors or the basis may not form a straight angle), it is possible to decompose it into transforms that are only horizontal/vertical scaling (i.e. matrices of the form $\pmatrix{s_x&0\\0&s_y}$) and pure rotation (i.e. matrices of the form $\pmatrix{\cos \theta&-\sin\theta\\\sin\theta&\cos\theta}$). The minimal number of steps to do so is probably 3: Rotate it so that the next scaling step will give it the correct shape. Scale it to give it the proper shape. Rotate it into the final position. In other words, it seems to be always possible to find parameters $\theta, s_x, s_y, \phi$ such that: $$M = \pmatrix{m_{00}&m_{01}\\m_{10}&m_{11}} = \pmatrix{\cos\phi&-\sin\phi\\\sin\phi&\cos\phi} \pmatrix{s_x&0\\0&s_y} \pmatrix{\cos\theta&-\sin\theta\\\sin\theta&\cos\theta}$$ I don't know if it's always possible to do so. I have the hunch that it is. I could not figure out how; I got a big system of nonlinear equations when I tried by myself. That decomposition has 4 d.o.f. as does the original linear transform, which hints that if it's correct, then it's minimal (you can't do it with 2 rotations or 2 scalings, and you don't have enough d.o.f. with 1 rotation + 1 scaling). I tried with Inkscape and trial and error, and I could not find any rhomboid I couldn't transform into a square, but I might have not tried hard enough. My questions: Is that decomposition always possible? Already answered by Rahul in a comment (thanks!): it's called Singular Value Decomposition ) If so, how to calculate $\theta, s_x, s_y, \phi$? If not, how can it be done in the least number of steps? (Mooted by the answer to question 1). (New question) I found a solution with 4 (sometimes 5) steps, posted in the linked question but it was somewhat convoluted. However, the SVD seems to be complex to calculate. Should I just go with my 4-5 step solution, or is there a simple enough way to perform the decomposition? I've found this answer: https://scicomp.stackexchange.com/questions/8899/robust-algorithm-for-2x2-svd but it looked kind of overkill and I don't understand how to use the code for this case. I also didn't understand how to use the analytical result for the 2x2 case from the Wikipedia page, so if it's easier, an explanation on using it would help too.","Related to: Newly Developed With Details - Describing orthographic projection using simple 2D transformations Given an arbitrary 2D linear transform (which may include shear, i.e. the vectors or the basis may not form a straight angle), it is possible to decompose it into transforms that are only horizontal/vertical scaling (i.e. matrices of the form $\pmatrix{s_x&0\\0&s_y}$) and pure rotation (i.e. matrices of the form $\pmatrix{\cos \theta&-\sin\theta\\\sin\theta&\cos\theta}$). The minimal number of steps to do so is probably 3: Rotate it so that the next scaling step will give it the correct shape. Scale it to give it the proper shape. Rotate it into the final position. In other words, it seems to be always possible to find parameters $\theta, s_x, s_y, \phi$ such that: $$M = \pmatrix{m_{00}&m_{01}\\m_{10}&m_{11}} = \pmatrix{\cos\phi&-\sin\phi\\\sin\phi&\cos\phi} \pmatrix{s_x&0\\0&s_y} \pmatrix{\cos\theta&-\sin\theta\\\sin\theta&\cos\theta}$$ I don't know if it's always possible to do so. I have the hunch that it is. I could not figure out how; I got a big system of nonlinear equations when I tried by myself. That decomposition has 4 d.o.f. as does the original linear transform, which hints that if it's correct, then it's minimal (you can't do it with 2 rotations or 2 scalings, and you don't have enough d.o.f. with 1 rotation + 1 scaling). I tried with Inkscape and trial and error, and I could not find any rhomboid I couldn't transform into a square, but I might have not tried hard enough. My questions: Is that decomposition always possible? Already answered by Rahul in a comment (thanks!): it's called Singular Value Decomposition ) If so, how to calculate $\theta, s_x, s_y, \phi$? If not, how can it be done in the least number of steps? (Mooted by the answer to question 1). (New question) I found a solution with 4 (sometimes 5) steps, posted in the linked question but it was somewhat convoluted. However, the SVD seems to be complex to calculate. Should I just go with my 4-5 step solution, or is there a simple enough way to perform the decomposition? I've found this answer: https://scicomp.stackexchange.com/questions/8899/robust-algorithm-for-2x2-svd but it looked kind of overkill and I don't understand how to use the code for this case. I also didn't understand how to use the analytical result for the 2x2 case from the Wikipedia page, so if it's easier, an explanation on using it would help too.",,"['linear-algebra', 'matrix-decomposition']"
63,determinant inequality $\det(A^2+AB+B^2)\geq\det(AB-BA)$,determinant inequality,\det(A^2+AB+B^2)\geq\det(AB-BA),"$A,B$ are two $2\times 2$ real matrices, then   $$\det(A^2+AB+B^2)\geq\det(AB-BA)$$ The inequality is  equivalent to the following problem: Let $X=A+\dfrac{B}{2},Y=-\dfrac{B}{2}$ $$\det[(X-Y)(X+Y)-2(X^2+Y^2)]≥4\det(XY-YX)$$ http://www.artofproblemsolving.com/Forum/viewtopic.php?f=353&t=588819","$A,B$ are two $2\times 2$ real matrices, then   $$\det(A^2+AB+B^2)\geq\det(AB-BA)$$ The inequality is  equivalent to the following problem: Let $X=A+\dfrac{B}{2},Y=-\dfrac{B}{2}$ $$\det[(X-Y)(X+Y)-2(X^2+Y^2)]≥4\det(XY-YX)$$ http://www.artofproblemsolving.com/Forum/viewtopic.php?f=353&t=588819",,"['linear-algebra', 'inequality', 'determinant']"
64,Vector Space vs Subspace,Vector Space vs Subspace,,"Can someone explain the difference between a subspace and a vector space? I realize that a vector space has 10 axioms that define how vectors can be added and subtracted. I also realize that a subspace is closed under multiplication, addition, and contains the zero vector. My problem is that I fundamentally don't understand the difference between them. Perhaps you guys could show me some examples of both a vector space and subspace. I'm a visual learner. Thanks. :)","Can someone explain the difference between a subspace and a vector space? I realize that a vector space has 10 axioms that define how vectors can be added and subtracted. I also realize that a subspace is closed under multiplication, addition, and contains the zero vector. My problem is that I fundamentally don't understand the difference between them. Perhaps you guys could show me some examples of both a vector space and subspace. I'm a visual learner. Thanks. :)",,"['linear-algebra', 'matrices']"
65,Is every symmetric positive semi-definite matrix a covariance of some multivariate distribution?,Is every symmetric positive semi-definite matrix a covariance of some multivariate distribution?,,"One can easily prove that every covariance matrix is positive semi-definite . I come across many claims that the converse is also true; that is, Every symmetric positive semi-definite matrix is a covariance marix of some multivariate distribution. Is it true? If it is, how can we prove it?","One can easily prove that every covariance matrix is positive semi-definite . I come across many claims that the converse is also true; that is, Every symmetric positive semi-definite matrix is a covariance marix of some multivariate distribution. Is it true? If it is, how can we prove it?",,"['linear-algebra', 'probability', 'matrices', 'statistics']"
66,Are positive definite matrices necessarily diagonalizable and when does the famous eigenvalue criterion apply?,Are positive definite matrices necessarily diagonalizable and when does the famous eigenvalue criterion apply?,,"I mean in $\mathbb{C}$ positive definite matrices seem to be self-adjoint. For matrices over real vector spaces this seems to be wrong, but is it still true that they are diagonalizable? Then everyone knows a result similar to this: When a matrix has only positive eigenvalues then it is positive definite. Is this result always true, and do we also have the converse in general?","I mean in $\mathbb{C}$ positive definite matrices seem to be self-adjoint. For matrices over real vector spaces this seems to be wrong, but is it still true that they are diagonalizable? Then everyone knows a result similar to this: When a matrix has only positive eigenvalues then it is positive definite. Is this result always true, and do we also have the converse in general?",,['linear-algebra']
67,What is the difference between diagonalization and orthogonal diagonalization?,What is the difference between diagonalization and orthogonal diagonalization?,,"I am confused about the following. When you diagonalize a $n\times n$ matrix $A$, you write $A$ as $PDP^{-1}$ with $P$ being orthogonal. Because if $P$ wasn't orthogonal, it wouldn't be invertable. Then why don't we call this ""orthogonal diagonalization""? When you diagonalize a $n\times n$ symmetric matrix $A$ (so $A = A^T$), you write $A$ as $PDP^T$, because $P^{-1}= P^T$. But if $P^{-1}= P^T$, doesn't that imply that $P^TP=I$ and thus that P is orthonormal? Then why don't we call this ""orthonormal diagonalization""?","I am confused about the following. When you diagonalize a $n\times n$ matrix $A$, you write $A$ as $PDP^{-1}$ with $P$ being orthogonal. Because if $P$ wasn't orthogonal, it wouldn't be invertable. Then why don't we call this ""orthogonal diagonalization""? When you diagonalize a $n\times n$ symmetric matrix $A$ (so $A = A^T$), you write $A$ as $PDP^T$, because $P^{-1}= P^T$. But if $P^{-1}= P^T$, doesn't that imply that $P^TP=I$ and thus that P is orthonormal? Then why don't we call this ""orthonormal diagonalization""?",,['linear-algebra']
68,Splicing together Short Exact Sequences,Splicing together Short Exact Sequences,,"Given an exact sequence of vector spaces $$\cdots\longrightarrow V_{i-1}\longrightarrow V_{i}\longrightarrow V_{i+1}\longrightarrow\cdots$$ I want to show that it is the same as having a collection of short ones such that $$0\longrightarrow K_i \longrightarrow V_i \longrightarrow K_{i+1}\longrightarrow0$$ So to start I want to show exactness at an arbitrary $V_i$, so I space them suggestively: $$\begin{array}{c} 0&\rightarrow &K_{i-1}&\rightarrow &V_{i-1}&\rightarrow &K_{i}&\rightarrow&0\\ &&&&0&\rightarrow&K_i&\rightarrow &V_i&\rightarrow&K_{i+1}&\rightarrow&0\\ &&&&&&&&0&\rightarrow&K_{i+1}&\rightarrow&V_{i+1}&\rightarrow&K_{i+2}&\rightarrow&0 \end{array}$$ I drop inclusions down among the corresponding $K_i$'s, and then compose until I get a function from $V_{i-1}$ to $V_i$ and one from $V_i$ to $V_{i+1}$.  I check that the image of the first composite mess is the kernel of the second composite mess, which indeed it is. Question: Am I done?  Is showing exactness at one such $V_i$ enough?  The question now prompts me to worry about the case were the orginal sequence isn't infinite in both directions...I'm not sure how that case is different?","Given an exact sequence of vector spaces $$\cdots\longrightarrow V_{i-1}\longrightarrow V_{i}\longrightarrow V_{i+1}\longrightarrow\cdots$$ I want to show that it is the same as having a collection of short ones such that $$0\longrightarrow K_i \longrightarrow V_i \longrightarrow K_{i+1}\longrightarrow0$$ So to start I want to show exactness at an arbitrary $V_i$, so I space them suggestively: $$\begin{array}{c} 0&\rightarrow &K_{i-1}&\rightarrow &V_{i-1}&\rightarrow &K_{i}&\rightarrow&0\\ &&&&0&\rightarrow&K_i&\rightarrow &V_i&\rightarrow&K_{i+1}&\rightarrow&0\\ &&&&&&&&0&\rightarrow&K_{i+1}&\rightarrow&V_{i+1}&\rightarrow&K_{i+2}&\rightarrow&0 \end{array}$$ I drop inclusions down among the corresponding $K_i$'s, and then compose until I get a function from $V_{i-1}$ to $V_i$ and one from $V_i$ to $V_{i+1}$.  I check that the image of the first composite mess is the kernel of the second composite mess, which indeed it is. Question: Am I done?  Is showing exactness at one such $V_i$ enough?  The question now prompts me to worry about the case were the orginal sequence isn't infinite in both directions...I'm not sure how that case is different?",,"['linear-algebra', 'abstract-algebra', 'vector-spaces']"
69,Determining the left inverse of a non-square matrix,Determining the left inverse of a non-square matrix,,"I understand that non-square matrices do not have an inverse, that is, both a left inverse and a right inverse. Yet, I am fairly certain that it is possible for a non-square matrix to have either a left inverse or (exclusively) right inverse. Take the example where, I want to determine the matrix P for which, $$P \left[ \begin{array}{cc} 1 & 1 \\ 1 & i \\ 0 & 1+i \\ \end{array} \right] \left[ \begin{array}{c} \lambda_{1} \\ \lambda_{2} \\ \end{array} \right] = \left[ \begin{array}{c} 1 \\ 0 \\ i \\ \end{array} \right]$$ It is clear that $P$ must be a $3 \times 3$ matrix, since the column matrix on the right side is $3 \times 1$. How can I determine what this matrix $P$, the left inverse of $\left[ \begin{array}{cc} 1 & 1 \\ 1 & i \\ 0 & 1+i \\ \end{array} \right]$, is? The standard methods I know for inverting an $n \times n$ (square) matrix seem not to be working. Thank you.","I understand that non-square matrices do not have an inverse, that is, both a left inverse and a right inverse. Yet, I am fairly certain that it is possible for a non-square matrix to have either a left inverse or (exclusively) right inverse. Take the example where, I want to determine the matrix P for which, $$P \left[ \begin{array}{cc} 1 & 1 \\ 1 & i \\ 0 & 1+i \\ \end{array} \right] \left[ \begin{array}{c} \lambda_{1} \\ \lambda_{2} \\ \end{array} \right] = \left[ \begin{array}{c} 1 \\ 0 \\ i \\ \end{array} \right]$$ It is clear that $P$ must be a $3 \times 3$ matrix, since the column matrix on the right side is $3 \times 1$. How can I determine what this matrix $P$, the left inverse of $\left[ \begin{array}{cc} 1 & 1 \\ 1 & i \\ 0 & 1+i \\ \end{array} \right]$, is? The standard methods I know for inverting an $n \times n$ (square) matrix seem not to be working. Thank you.",,"['linear-algebra', 'matrices']"
70,Constructing idempotent matrices,Constructing idempotent matrices,,Is there a general method for constructing an idempotent matrix if we are given the values of the diagonal entries?,Is there a general method for constructing an idempotent matrix if we are given the values of the diagonal entries?,,"['linear-algebra', 'matrices']"
71,How to prove that $AB$ is a projection if $(AB)(BA)=AB$?,How to prove that  is a projection if ?,AB (AB)(BA)=AB,"I was trying to solve the following problem: Assume $A,B\in M_n\left( \mathbb{C} \right)$ ,satisfy $$AB^2A=AB.$$ I need to proof $$\left( AB \right) ^2=AB.$$ I tried to use some equivalent substitution of matrices, but I did not succeed. I also tried to find some counterexamples of matrices, such as 2nd order matrices, but I did not succeed either. I don't know if this is a right problem or a wrong problem. I hope to solve this problem. Thanks!","I was trying to solve the following problem: Assume ,satisfy I need to proof I tried to use some equivalent substitution of matrices, but I did not succeed. I also tried to find some counterexamples of matrices, such as 2nd order matrices, but I did not succeed either. I don't know if this is a right problem or a wrong problem. I hope to solve this problem. Thanks!","A,B\in M_n\left( \mathbb{C} \right) AB^2A=AB. \left( AB \right) ^2=AB.","['linear-algebra', 'matrices', 'projection', 'idempotents']"
72,Understanding L1 and L2 norms,Understanding L1 and L2 norms,,"I am not a mathematics student but somehow have to know about L1 and L2 norms. I am looking for some appropriate sources to learn these things and know they work and what are their differences. I am asking this question since there are lots of stuff on this matter in the internet and I am looking for a simple applicable one to understand this. To add more, I want to understand why/how/when these norms are used in optimization problems? And maybe, how these cost functions are solved.","I am not a mathematics student but somehow have to know about L1 and L2 norms. I am looking for some appropriate sources to learn these things and know they work and what are their differences. I am asking this question since there are lots of stuff on this matter in the internet and I am looking for a simple applicable one to understand this. To add more, I want to understand why/how/when these norms are used in optimization problems? And maybe, how these cost functions are solved.",,"['linear-algebra', 'optimization', 'least-squares']"
73,Does the unique map on the zero space have determinant 1?,Does the unique map on the zero space have determinant 1?,,"The trivial vector space over any field $K$, consisting of only the zero vector, admits exactly one endomorphism, let's call it $z$, sending $0$ to itself. It is the identity map, so it should have determinant $1$. On the face of it, the zero map should have determinant $0$. But this is usually argued via $\lambda z = z$ for all $\lambda \in K$, so $\det z = \det (\lambda z) = \lambda^n \det z$, i.e. $(\lambda^n - 1)\det z = 0$. Normally that's enough to conclude that $\det z = 0$, but of course $n = 0$ in this case, so $\lambda^n = 1$ for all $\lambda$, and we learn nothing. Despite being the zero map, it's full rank and has trivial kernel. There are no nonzero vectors, so it has no eigenvectors, so it has no eigenvalues, so their product is $1$. On the other hand, the determinant is meant to be multilinear, and so should map the zero matrix to zero. But should we say that $z$ is represented by a zero matrix, given that its matrix representation is $0\times 0$ and doesn't have any entries at all? I can't help but feel like this is all very silly, but clearly the answer can't be anything other than $1$. Is there anything wrong with giving this answer? Does it cause any problems with any other typical properties of the determinant? Does it simplify any definitions or theorems?","The trivial vector space over any field $K$, consisting of only the zero vector, admits exactly one endomorphism, let's call it $z$, sending $0$ to itself. It is the identity map, so it should have determinant $1$. On the face of it, the zero map should have determinant $0$. But this is usually argued via $\lambda z = z$ for all $\lambda \in K$, so $\det z = \det (\lambda z) = \lambda^n \det z$, i.e. $(\lambda^n - 1)\det z = 0$. Normally that's enough to conclude that $\det z = 0$, but of course $n = 0$ in this case, so $\lambda^n = 1$ for all $\lambda$, and we learn nothing. Despite being the zero map, it's full rank and has trivial kernel. There are no nonzero vectors, so it has no eigenvectors, so it has no eigenvalues, so their product is $1$. On the other hand, the determinant is meant to be multilinear, and so should map the zero matrix to zero. But should we say that $z$ is represented by a zero matrix, given that its matrix representation is $0\times 0$ and doesn't have any entries at all? I can't help but feel like this is all very silly, but clearly the answer can't be anything other than $1$. Is there anything wrong with giving this answer? Does it cause any problems with any other typical properties of the determinant? Does it simplify any definitions or theorems?",,"['linear-algebra', 'determinant']"
74,Eigenvalue problems for matrices over finite fields,Eigenvalue problems for matrices over finite fields,,"Suppose I have a symmetric matrix $A$ with entries in a finite field. In particular, I have the case in mind where $A \in \{0,1\}^{n \times n}$ and want to treat the entries as elements of $\mbox{GF}(2)$ . How much is known about the eigenvalue problem in this case? Is there a spectral theorem? Are there fast algorithms for computing eigenvectors?","Suppose I have a symmetric matrix with entries in a finite field. In particular, I have the case in mind where and want to treat the entries as elements of . How much is known about the eigenvalue problem in this case? Is there a spectral theorem? Are there fast algorithms for computing eigenvectors?","A A \in \{0,1\}^{n \times n} \mbox{GF}(2)","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'finite-fields']"
75,System of linear equations having a real solution has also a rational solution.,System of linear equations having a real solution has also a rational solution.,,"I saw this question Let $A ∈ M_{m\times n}(\mathbb{Q})$ and $b ∈ \mathbb{Q}^m$. Suppose   that the system of linear equations $Ax = b$ has a solution in   $\mathbb{R}^n$. Does it necessarily have a solution in $\mathbb{Q}^n$? and I thought I'd give an interesting, possibly wrong, approach to solving it. I'm not sure if such things can be done, if not maybe you can help me refine. I considered the form of the equality as $$ A^{(1)}x_1+\cdots+A^{(n)}x_n=b, $$ where $A^{(i)}$ is a column vector of $A$. I then noticed that for $x_i\in\mathbb{R\setminus Q}=\mathbb{T}$ then, and this is where I think I'm doing something forbidden, each $x$ has the represenation $$ x_1  =  k_{11}\tau_1+\cdots+k_{1p}\tau_p \\  x_2= k_{21}\tau_1+\cdots+k_{2p}\tau_p \\ \vdots \hspace{4cm} \vdots \\ x_n=k_{n1}\tau_1+\cdots+k_{np}\tau_p,$$ where $\tau_i$ is a distinct irrational number, $k_{ij}\in\mathbb{R}$, and $p$ is the number of such distinct irrational numbers. I wound this out, but there may be a discrepancy with $p$ and $m$. I feel this method can lead me to the answer, but I'm not sure where to go from here. EDIT$^1$: I end up getting something like this, I believe, after substitution: $$ A(k^{(1)}\tau_1+\cdots+k^{(p)}\tau_p)=b$$ Here, $k^{(i)}$ is the vector $$\begin{pmatrix} k_{1i} \\ \vdots \\ k_{ni} \end{pmatrix}.$$ EDIT$^2$: I think there is no discrepancy with $p$ and $m$ because $A\in M_{m\times n}(\mathbb{Q})$, $K\in M_{n\times p}(\mathbb{R})$, and $\tau\in M_{p\times 1}(\mathbb{T})$, so $$ (m\times n)\cdot (n\times p)\cdot (p\times 1) = m\times 1. $$","I saw this question Let $A ∈ M_{m\times n}(\mathbb{Q})$ and $b ∈ \mathbb{Q}^m$. Suppose   that the system of linear equations $Ax = b$ has a solution in   $\mathbb{R}^n$. Does it necessarily have a solution in $\mathbb{Q}^n$? and I thought I'd give an interesting, possibly wrong, approach to solving it. I'm not sure if such things can be done, if not maybe you can help me refine. I considered the form of the equality as $$ A^{(1)}x_1+\cdots+A^{(n)}x_n=b, $$ where $A^{(i)}$ is a column vector of $A$. I then noticed that for $x_i\in\mathbb{R\setminus Q}=\mathbb{T}$ then, and this is where I think I'm doing something forbidden, each $x$ has the represenation $$ x_1  =  k_{11}\tau_1+\cdots+k_{1p}\tau_p \\  x_2= k_{21}\tau_1+\cdots+k_{2p}\tau_p \\ \vdots \hspace{4cm} \vdots \\ x_n=k_{n1}\tau_1+\cdots+k_{np}\tau_p,$$ where $\tau_i$ is a distinct irrational number, $k_{ij}\in\mathbb{R}$, and $p$ is the number of such distinct irrational numbers. I wound this out, but there may be a discrepancy with $p$ and $m$. I feel this method can lead me to the answer, but I'm not sure where to go from here. EDIT$^1$: I end up getting something like this, I believe, after substitution: $$ A(k^{(1)}\tau_1+\cdots+k^{(p)}\tau_p)=b$$ Here, $k^{(i)}$ is the vector $$\begin{pmatrix} k_{1i} \\ \vdots \\ k_{ni} \end{pmatrix}.$$ EDIT$^2$: I think there is no discrepancy with $p$ and $m$ because $A\in M_{m\times n}(\mathbb{Q})$, $K\in M_{n\times p}(\mathbb{R})$, and $\tau\in M_{p\times 1}(\mathbb{T})$, so $$ (m\times n)\cdot (n\times p)\cdot (p\times 1) = m\times 1. $$",,"['linear-algebra', 'abstract-algebra', 'matrices', 'rational-numbers']"
76,Solving linear matrix equations of the form $X = AXA^T + C$,Solving linear matrix equations of the form,X = AXA^T + C,"I'm trying to solve this linear matrix equation: $$X = AXA^T + C$$ In particular, $$ X = \begin{bmatrix} 1.5 & 1 \\ -0.7 & 0 \end{bmatrix} X \begin{bmatrix} 1.5 & -0.7 \\ 1 & 0 \end{bmatrix} + \begin{bmatrix} 1 & 0.5 \\ 0.5 & 0.25 \end{bmatrix}. $$ I started by writing $X$ in open form. $$X = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$$ The equation becomes $$ \begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} 1.5 & 1 \\ -0.7 & 0 \end{bmatrix} \begin{bmatrix} a & b \\ c & d \end{bmatrix} \begin{bmatrix} 1.5 & -0.7 \\ 1 & 0 \end{bmatrix} + \begin{bmatrix} 1 & 0.5 \\ 0.5 & 0.25 \end{bmatrix} \\ \begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} 1.5a+c & 1.5b+d \\ -0.7a & -0.7b \end{bmatrix} \begin{bmatrix} 1.5 & -0.7 \\ 1 & 0 \end{bmatrix} + \begin{bmatrix} 1 & 0.5 \\ 0.5 & 0.25 \end{bmatrix} \\ \begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} 2.25a+1.5b+1.5c+d & -1.05a-0.7c \\ -1.05a-0.7b & 0.49a \end{bmatrix} + \begin{bmatrix} 1 & 0.5 \\ 0.5 & 0.25 \end{bmatrix} \\ \begin{bmatrix} a-1 & b-0.5 \\ c-0.5 & d-0.25 \end{bmatrix} = \begin{bmatrix} 2.25a+1.5b+1.5c+d & -1.05a-0.7c \\ -1.05a-0.7b & 0.49a \end{bmatrix} \\ \begin{bmatrix} -1 & -0.5 \\ -0.5 & -0.25 \end{bmatrix} = \begin{bmatrix} 1.25a+1.5b+1.5c+d & -1.05a-b-0.7c \\ -1.05a-0.7b-c & 0.49a-d \end{bmatrix} \\ $$ Equating cells of RHS and LHS matrices I wrote the linear equation set $$ \begin{bmatrix} 1.25 & 1.5 & 1.5 & 1 \\ -1.05 & -1 & -0.7 & 0 \\ -1.05 & -0.7 & -1 & 0 \\ 0.49 & 0 & 0 & -1 \\ \end{bmatrix} \begin{bmatrix} a \\ b \\ c \\ d \end{bmatrix} = \begin{bmatrix} -1 \\ -0.5 \\ -0.5 \\ -0.25 \end{bmatrix} $$ I solved this set of equations using two different software tool to find exact same $a$ , $b$ , $c$ and $d$ values: $$X = \begin{bmatrix} a & b \\ c & d \end{bmatrix} =  \begin{bmatrix} 18.8802 & -11.3672 \\ -11.3672 & 9.5013 \end{bmatrix}$$ Now remember the equation above: $$ \begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} 1.5 & 1 \\ -0.7 & 0 \end{bmatrix} \begin{bmatrix} a & b \\ c & d \end{bmatrix} \begin{bmatrix} 1.5 & -0.7 \\ 1 & 0 \end{bmatrix} + \begin{bmatrix} 1 & 0.5 \\ 0.5 & 0.25 \end{bmatrix} $$ When I put the $a$ , $b$ , $c$ and $d$ values in the RHS, I don't find the $X$ matrix on the LHS. $$ \begin{bmatrix} 1.5 & 1 \\ -0.7 & 0 \end{bmatrix} \begin{bmatrix} 18.8802 & -11.3672 \\ -11.3672 & 9.5013 \end{bmatrix} \begin{bmatrix} 1.5 & -0.7 \\ 1 & 0 \end{bmatrix} + \begin{bmatrix} 1 & 0.5 \\ 0.5 & 0.25 \end{bmatrix} = \begin{bmatrix} 163.4421 & -332.7002 \\ -332.7002 & 857.9770 \end{bmatrix} $$ What am I doing wrong in my calculations?","I'm trying to solve this linear matrix equation: In particular, I started by writing in open form. The equation becomes Equating cells of RHS and LHS matrices I wrote the linear equation set I solved this set of equations using two different software tool to find exact same , , and values: Now remember the equation above: When I put the , , and values in the RHS, I don't find the matrix on the LHS. What am I doing wrong in my calculations?","X = AXA^T + C 
X = \begin{bmatrix} 1.5 & 1 \\ -0.7 & 0 \end{bmatrix}
X
\begin{bmatrix} 1.5 & -0.7 \\ 1 & 0 \end{bmatrix}
+ \begin{bmatrix} 1 & 0.5 \\ 0.5 & 0.25 \end{bmatrix}.
 X X = \begin{bmatrix} a & b \\ c & d \end{bmatrix} 
\begin{bmatrix} a & b \\ c & d \end{bmatrix}
= \begin{bmatrix} 1.5 & 1 \\ -0.7 & 0 \end{bmatrix}
\begin{bmatrix} a & b \\ c & d \end{bmatrix}
\begin{bmatrix} 1.5 & -0.7 \\ 1 & 0 \end{bmatrix}
+ \begin{bmatrix} 1 & 0.5 \\ 0.5 & 0.25 \end{bmatrix} \\
\begin{bmatrix} a & b \\ c & d \end{bmatrix}
= \begin{bmatrix} 1.5a+c & 1.5b+d \\ -0.7a & -0.7b \end{bmatrix}
\begin{bmatrix} 1.5 & -0.7 \\ 1 & 0 \end{bmatrix}
+ \begin{bmatrix} 1 & 0.5 \\ 0.5 & 0.25 \end{bmatrix} \\
\begin{bmatrix} a & b \\ c & d \end{bmatrix}
= \begin{bmatrix} 2.25a+1.5b+1.5c+d & -1.05a-0.7c \\ -1.05a-0.7b & 0.49a \end{bmatrix}
+ \begin{bmatrix} 1 & 0.5 \\ 0.5 & 0.25 \end{bmatrix} \\
\begin{bmatrix} a-1 & b-0.5 \\ c-0.5 & d-0.25 \end{bmatrix}
= \begin{bmatrix} 2.25a+1.5b+1.5c+d & -1.05a-0.7c \\ -1.05a-0.7b & 0.49a \end{bmatrix} \\
\begin{bmatrix} -1 & -0.5 \\ -0.5 & -0.25 \end{bmatrix}
= \begin{bmatrix} 1.25a+1.5b+1.5c+d & -1.05a-b-0.7c \\ -1.05a-0.7b-c & 0.49a-d \end{bmatrix} \\
 
\begin{bmatrix}
1.25 & 1.5 & 1.5 & 1 \\
-1.05 & -1 & -0.7 & 0 \\
-1.05 & -0.7 & -1 & 0 \\
0.49 & 0 & 0 & -1 \\
\end{bmatrix}
\begin{bmatrix}
a \\
b \\
c \\
d
\end{bmatrix} =
\begin{bmatrix}
-1 \\
-0.5 \\
-0.5 \\
-0.25
\end{bmatrix}
 a b c d X = \begin{bmatrix} a & b \\ c & d \end{bmatrix} = 
\begin{bmatrix} 18.8802 & -11.3672 \\ -11.3672 & 9.5013 \end{bmatrix} 
\begin{bmatrix} a & b \\ c & d \end{bmatrix}
= \begin{bmatrix} 1.5 & 1 \\ -0.7 & 0 \end{bmatrix}
\begin{bmatrix} a & b \\ c & d \end{bmatrix}
\begin{bmatrix} 1.5 & -0.7 \\ 1 & 0 \end{bmatrix}
+ \begin{bmatrix} 1 & 0.5 \\ 0.5 & 0.25 \end{bmatrix}
 a b c d X 
\begin{bmatrix} 1.5 & 1 \\ -0.7 & 0 \end{bmatrix}
\begin{bmatrix} 18.8802 & -11.3672 \\ -11.3672 & 9.5013 \end{bmatrix}
\begin{bmatrix} 1.5 & -0.7 \\ 1 & 0 \end{bmatrix}
+ \begin{bmatrix} 1 & 0.5 \\ 0.5 & 0.25 \end{bmatrix}
= \begin{bmatrix} 163.4421 & -332.7002 \\ -332.7002 & 857.9770 \end{bmatrix}
","['linear-algebra', 'matrices', 'solution-verification', 'problem-solving', 'matrix-equations']"
77,The connection between decomposition of $\ker(f_\phi g_\phi)$ and the Chinese remainder theorem,The connection between decomposition of  and the Chinese remainder theorem,\ker(f_\phi g_\phi),"Original problem Suppose $K$ is a field, $f,g\in K[x]$ and $\gcd(f,g)=1$. $V$ is a linear space based on $K$ and $\phi\in\operatorname{End}(V)$. Notation $f_\phi$ and $g_\phi$ denote linear transformations $f(\phi)$ and $g(\phi)$. Try to show that $$\ker(f_\phi g_\phi)=\ker(f_\phi)\oplus\ker(g_\phi)$$ The sketch of the proof Notice that sub-ring $K[\phi]\subseteq\operatorname{End}(V)$ is commutative, and $f_\phi,g_\phi\in K[\phi]$, so it's easy to show that $\ker(f_\phi),\ker (g_\phi)\subseteq\ker(f_\phi g_\phi)$. $\because\gcd(f,g)=1$, $\therefore\exists u,v\in K[x]: uf+vg=1\implies u_\phi f_\phi+v_\phi g_\phi=I_V$, where $I_V$ is the identity operator of $V$. $\forall x\in\ker(f_\phi g_\phi)$, plug $x$ into the preceding equation, we have $x=u_\phi f_\phi(x)+v_\phi g_\phi(x)$, and it's not quite difficult to show that $u_\phi f_\phi(x)\in\ker(g_\phi)$ and $v_\phi g_\phi(x)\in\ker(f_\phi)$, so $x\in\ker(f_\phi)+\ker(g_\phi)$. $\forall x\in\ker(f_\phi)\cap\ker(g_\phi)$, plug $x$ into that equation and we have $x=0$, so $\ker(f_\phi)\cap\ker(g_\phi)=0$, Q.E.D Thoughts It seems that the proposition is quite similar to Chinese Remainder Theorem, say: $$\mathbb Z/mn\mathbb Z\cong\mathbb Z/m\mathbb Z\oplus\mathbb Z/n\mathbb Z$$ if $\gcd(m,n)=1$. Can we find the deeper relationship between these two propositions? Or just an unique aspect? Thanks!","Original problem Suppose $K$ is a field, $f,g\in K[x]$ and $\gcd(f,g)=1$. $V$ is a linear space based on $K$ and $\phi\in\operatorname{End}(V)$. Notation $f_\phi$ and $g_\phi$ denote linear transformations $f(\phi)$ and $g(\phi)$. Try to show that $$\ker(f_\phi g_\phi)=\ker(f_\phi)\oplus\ker(g_\phi)$$ The sketch of the proof Notice that sub-ring $K[\phi]\subseteq\operatorname{End}(V)$ is commutative, and $f_\phi,g_\phi\in K[\phi]$, so it's easy to show that $\ker(f_\phi),\ker (g_\phi)\subseteq\ker(f_\phi g_\phi)$. $\because\gcd(f,g)=1$, $\therefore\exists u,v\in K[x]: uf+vg=1\implies u_\phi f_\phi+v_\phi g_\phi=I_V$, where $I_V$ is the identity operator of $V$. $\forall x\in\ker(f_\phi g_\phi)$, plug $x$ into the preceding equation, we have $x=u_\phi f_\phi(x)+v_\phi g_\phi(x)$, and it's not quite difficult to show that $u_\phi f_\phi(x)\in\ker(g_\phi)$ and $v_\phi g_\phi(x)\in\ker(f_\phi)$, so $x\in\ker(f_\phi)+\ker(g_\phi)$. $\forall x\in\ker(f_\phi)\cap\ker(g_\phi)$, plug $x$ into that equation and we have $x=0$, so $\ker(f_\phi)\cap\ker(g_\phi)=0$, Q.E.D Thoughts It seems that the proposition is quite similar to Chinese Remainder Theorem, say: $$\mathbb Z/mn\mathbb Z\cong\mathbb Z/m\mathbb Z\oplus\mathbb Z/n\mathbb Z$$ if $\gcd(m,n)=1$. Can we find the deeper relationship between these two propositions? Or just an unique aspect? Thanks!",,"['linear-algebra', 'ring-theory', 'modules']"
78,Minimum and Maximum eigenvalue inequality from a positive definite matrix.,Minimum and Maximum eigenvalue inequality from a positive definite matrix.,,"I got a positive definite matrix $B,$ that is, $V(x) = x^TBx > 0$ for any vector $ x \neq 0.$  I want to show that $ \lambda_\min \|x\|_2^2 \leq V(x) \leq \lambda_\max \|x\|_2^2$ for any $x \neq 0,$ where $\lambda_\min$ and $ \lambda_\max$ are defined by $$ \lambda_\min = \min \lbrace | \lambda|: \lambda \text{ is an eigenvalue of } B \rbrace$$ and  $$ \lambda_\max = \max \lbrace | \lambda|: \lambda \text{ is an eigenvalue of } B  \rbrace$$  Any hint please?","I got a positive definite matrix $B,$ that is, $V(x) = x^TBx > 0$ for any vector $ x \neq 0.$  I want to show that $ \lambda_\min \|x\|_2^2 \leq V(x) \leq \lambda_\max \|x\|_2^2$ for any $x \neq 0,$ where $\lambda_\min$ and $ \lambda_\max$ are defined by $$ \lambda_\min = \min \lbrace | \lambda|: \lambda \text{ is an eigenvalue of } B \rbrace$$ and  $$ \lambda_\max = \max \lbrace | \lambda|: \lambda \text{ is an eigenvalue of } B  \rbrace$$  Any hint please?",,"['linear-algebra', 'matrices']"
79,Measure of Image of Linear Map,Measure of Image of Linear Map,,"I am trying to work my way through the proof of the change of variables theorem for Lebesgue integrals. A key lemma in this context is as follows: If $T:\mathbb{R}^n \rightarrow \mathbb{R}^n$ is a linear map and $A \subset \mathbb{R}^n$ is Lebesgue measurable then $\lambda(T(A)) = |\det T \  | \cdot \lambda (A)$, where $\lambda(X)$ denotes the Lebesgue measure of $X$. Can anyone provide a reference for a proof of this lemma that clearly references the facts from linear algebra that are necessary to effect the proof? The source I have for this lemma refers to German texts that I am incapable of reading and I have been unsuccessful at finding an alternate proof. Added For the Benefit of Future Readers : In addition to the excellent references I received in response to this question, I have managed to find an additional reference that also provides a good proof of this fact: Aliprantis and Burkinshaw's Principles of Real Analysis, Third Edition, Lemma 40.4 pp 389-390.","I am trying to work my way through the proof of the change of variables theorem for Lebesgue integrals. A key lemma in this context is as follows: If $T:\mathbb{R}^n \rightarrow \mathbb{R}^n$ is a linear map and $A \subset \mathbb{R}^n$ is Lebesgue measurable then $\lambda(T(A)) = |\det T \  | \cdot \lambda (A)$, where $\lambda(X)$ denotes the Lebesgue measure of $X$. Can anyone provide a reference for a proof of this lemma that clearly references the facts from linear algebra that are necessary to effect the proof? The source I have for this lemma refers to German texts that I am incapable of reading and I have been unsuccessful at finding an alternate proof. Added For the Benefit of Future Readers : In addition to the excellent references I received in response to this question, I have managed to find an additional reference that also provides a good proof of this fact: Aliprantis and Burkinshaw's Principles of Real Analysis, Third Edition, Lemma 40.4 pp 389-390.",,"['linear-algebra', 'reference-request', 'measure-theory']"
80,Orthogonal Projection,Orthogonal Projection,,"Seems like I still don't get it , I think I am missing something important. Let $V$ be an $n$ dimensional inner product space ($n \geq 1$), and $T\colon\mathbf{V}\to\mathbf{V}$ be a linear transformation such that: $T^2 = T$ $||T(a)|| \leq ||a||$ for every vector $a$ in $\mathbf{V}$; Prove that a subspace $U \subseteq V$ exist, such that $T$ is the orthogonal projection on $U$. Now, I know these things: The fact hat $T^2 = T$ guarantees that $T$ is indeed a projection, so I need to prove that T is an orthogonal projection (I guess this is where $||T(a)|| \leq ||a||$ kicks in). To do this I can prove that: For every $v$ in $ImT^{\perp}$, $T(v) = 0$ Alternatively, I can prove that for every $v$ in $ImT$ and $u$ in $KerT$, $(v,u)=0$. $T$ is self-adjoint (according to Wikipedia) The matrix $A = [T]_{E}$ when $E$ is an orthonormal basis, is hermitian (this is equivalent to the previous point). What else? I've been thinking about it for quite some time now, and I'm pretty sure there is something big I'm missing, again . I just don't know how to use the data to prove any of these things. Thanks!","Seems like I still don't get it , I think I am missing something important. Let $V$ be an $n$ dimensional inner product space ($n \geq 1$), and $T\colon\mathbf{V}\to\mathbf{V}$ be a linear transformation such that: $T^2 = T$ $||T(a)|| \leq ||a||$ for every vector $a$ in $\mathbf{V}$; Prove that a subspace $U \subseteq V$ exist, such that $T$ is the orthogonal projection on $U$. Now, I know these things: The fact hat $T^2 = T$ guarantees that $T$ is indeed a projection, so I need to prove that T is an orthogonal projection (I guess this is where $||T(a)|| \leq ||a||$ kicks in). To do this I can prove that: For every $v$ in $ImT^{\perp}$, $T(v) = 0$ Alternatively, I can prove that for every $v$ in $ImT$ and $u$ in $KerT$, $(v,u)=0$. $T$ is self-adjoint (according to Wikipedia) The matrix $A = [T]_{E}$ when $E$ is an orthonormal basis, is hermitian (this is equivalent to the previous point). What else? I've been thinking about it for quite some time now, and I'm pretty sure there is something big I'm missing, again . I just don't know how to use the data to prove any of these things. Thanks!",,"['linear-algebra', 'linear-transformations', 'projection']"
81,How to maximize the product of scalar products?,How to maximize the product of scalar products?,,"Given $N$ unit vectors $v_1,\ldots,v_N$ of $\mathbb R^d$ , I was curious about finding an explicit maximizer for the product mapping $$ f(x) := \prod_{j=1}^N \langle x,v_j\rangle,\qquad x\in\mathbb R^d, $$ assuming that, say, $\|x\|=1$ . If one replaces the product by a sum, then the Cauchy-Schwarz inequality would directly yield that the maximum is reached at the unit vector proportional to the sum $\sum_jv_j$ , but for the product I was not able to find such a simple solution. I've tried to use the tensor product formalism to write $f(x) = \langle x^{\otimes N},\otimes_{j=1}^N v_j\rangle$ , so that we see that maximizing $f$ boils down to minimize $$ g(x):=\|x^{\otimes N}-\otimes_{j=1}^N v_j\|^2, $$ and thus the solution is the first tensor component of the ""orthogonal projection"" of $\otimes_{j=1}^N v_j$ onto the subset $\Delta := \{x^{\otimes N}: x\in\mathbb R^d\}\subset(\mathbb R^d)^{\otimes N}$ . Unfortunately, $\Delta$ is not a vector space and is not even convex, hence the quotation marks. I don't know how to continue (and gradient derivations did not bring me anywhere). Any ideas? I'm pretty sure that clever people have worked on how to approximate a tensor product by a tensor product of the same vector, but I guess I didn't give the good keywords to Google.","Given unit vectors of , I was curious about finding an explicit maximizer for the product mapping assuming that, say, . If one replaces the product by a sum, then the Cauchy-Schwarz inequality would directly yield that the maximum is reached at the unit vector proportional to the sum , but for the product I was not able to find such a simple solution. I've tried to use the tensor product formalism to write , so that we see that maximizing boils down to minimize and thus the solution is the first tensor component of the ""orthogonal projection"" of onto the subset . Unfortunately, is not a vector space and is not even convex, hence the quotation marks. I don't know how to continue (and gradient derivations did not bring me anywhere). Any ideas? I'm pretty sure that clever people have worked on how to approximate a tensor product by a tensor product of the same vector, but I guess I didn't give the good keywords to Google.","N v_1,\ldots,v_N \mathbb R^d 
f(x) := \prod_{j=1}^N \langle x,v_j\rangle,\qquad x\in\mathbb R^d,
 \|x\|=1 \sum_jv_j f(x) = \langle x^{\otimes N},\otimes_{j=1}^N v_j\rangle f 
g(x):=\|x^{\otimes N}-\otimes_{j=1}^N v_j\|^2,
 \otimes_{j=1}^N v_j \Delta := \{x^{\otimes N}: x\in\mathbb R^d\}\subset(\mathbb R^d)^{\otimes N} \Delta","['linear-algebra', 'differential-geometry', 'optimization', 'tensor-products']"
82,"If $A,B \in \mathcal{M_2}(\mathbb{R})$ and $A^2+B^2=AB$, does it follow that $A$ and $B$ commute?","If  and , does it follow that  and  commute?","A,B \in \mathcal{M_2}(\mathbb{R}) A^2+B^2=AB A B","Let $A,B \in \mathcal{M_2}(\mathbb{R})$ such that $A^2+B^2=AB$ . Is it necessary that $AB=BA$ ? I could easily show that such matrices have the property that $(AB-BA)^2=O_2$ (this was actually the question I was asked, then I started wondering if it would be true that the matrices actually commute) by considering the matrix $M=A+\epsilon B$ and computing the determinat  of $M \cdot \overline{M}$ ( $\epsilon \in \mathbb{C}\setminus \mathbb{R}$ is a cubic root of unity), but that's all I got. I tried to find some counterexamples, but I have a hard time finding any matrices with that property. EDIT: To see that $(AB-BA)^2=O_2$ we do the following : by direct computations $$|\det M|^2=\det M \det \overline{M}=\epsilon^2 \det(AB-BA)$$ and this is a real number if and only if $\det(AB-BA)=0$ . From the Cayley-Hamilton theorem we now get that $(AB-BA)^2=O_2$ .","Let such that . Is it necessary that ? I could easily show that such matrices have the property that (this was actually the question I was asked, then I started wondering if it would be true that the matrices actually commute) by considering the matrix and computing the determinat  of ( is a cubic root of unity), but that's all I got. I tried to find some counterexamples, but I have a hard time finding any matrices with that property. EDIT: To see that we do the following : by direct computations and this is a real number if and only if . From the Cayley-Hamilton theorem we now get that .","A,B \in \mathcal{M_2}(\mathbb{R}) A^2+B^2=AB AB=BA (AB-BA)^2=O_2 M=A+\epsilon B M \cdot \overline{M} \epsilon \in \mathbb{C}\setminus \mathbb{R} (AB-BA)^2=O_2 |\det M|^2=\det M \det \overline{M}=\epsilon^2 \det(AB-BA) \det(AB-BA)=0 (AB-BA)^2=O_2","['linear-algebra', 'matrices']"
83,Are these parallel theorems from Set Theory and Linear Algebra connected through Category Theory?,Are these parallel theorems from Set Theory and Linear Algebra connected through Category Theory?,,"From Set Theory and Linear Algebra, we have these two theorems: Given two finite sets of the same cardinality $X$ and $Y$ and a function $f:X\rightarrow Y$, the following are equivalent: $f$ is a bijection $f$ is an injection $f$ is a surjection Given two finite dimensional vector spaces of the same dimension $V$ and $W$ and a linear map $T:V\rightarrow W$, the following are equivalent: $f$ is an isomorphism $f$ is a monomorphism $f$ is an epimorphism The parallel nature of these theorems suggests they are related. And I have an inkling that Category Theory can make the relationship explicit. Functors preserves isomorphisms in general, so if I can find a combination of functors which preserve epimorphisms and monomorphisms in a convenient way, we can actually show that the first theorem implies the second. I know there is an adjoint pair $G\dashv F$ where $F$ is the forgetful functor from Vect $_K$ to Set and $G$ is the functor which sends a set to the free vector space generated by it. And I also know that both $F$ and $G$ are faithful (but not in general full). But I am having difficulty in getting these facts to work together nicely. Any help in making my ideas rigorous, or an explanation why I'm barking up the wrong tree, is appreciated. EDIT If we considered the free v.s. functor from FinSet to FinVect $_K$, we will then have a faithful functor. Since faithful functors reflect monomorphisms and epimorphisms, if we can find a parallel mono (or epic) map that is in the range of the free v.s. functor for every mono (or epic) map we encounter, we'd be able to show the 1st theorem implies the 2nd. Using the fact that vector spaces of the same dimension are isomorphic, we can conclude that the free v.s. functor is also dense. Hence if $V$ and $W$ are vector spaces of the same dimension, and $m:V\hookrightarrow W$ we'd be done if we can find an $f:X\rightarrow Y$ which makes the following diagram commute: $\hskip2in$ I'm sure a clever choice of bases on $V$ and $W$ and subsequent choices for the isomorphisms would make this possible, but these choices essentially use the 2nd theorem. And this approach doesn't seem to port over to epimorphisms.","From Set Theory and Linear Algebra, we have these two theorems: Given two finite sets of the same cardinality $X$ and $Y$ and a function $f:X\rightarrow Y$, the following are equivalent: $f$ is a bijection $f$ is an injection $f$ is a surjection Given two finite dimensional vector spaces of the same dimension $V$ and $W$ and a linear map $T:V\rightarrow W$, the following are equivalent: $f$ is an isomorphism $f$ is a monomorphism $f$ is an epimorphism The parallel nature of these theorems suggests they are related. And I have an inkling that Category Theory can make the relationship explicit. Functors preserves isomorphisms in general, so if I can find a combination of functors which preserve epimorphisms and monomorphisms in a convenient way, we can actually show that the first theorem implies the second. I know there is an adjoint pair $G\dashv F$ where $F$ is the forgetful functor from Vect $_K$ to Set and $G$ is the functor which sends a set to the free vector space generated by it. And I also know that both $F$ and $G$ are faithful (but not in general full). But I am having difficulty in getting these facts to work together nicely. Any help in making my ideas rigorous, or an explanation why I'm barking up the wrong tree, is appreciated. EDIT If we considered the free v.s. functor from FinSet to FinVect $_K$, we will then have a faithful functor. Since faithful functors reflect monomorphisms and epimorphisms, if we can find a parallel mono (or epic) map that is in the range of the free v.s. functor for every mono (or epic) map we encounter, we'd be able to show the 1st theorem implies the 2nd. Using the fact that vector spaces of the same dimension are isomorphic, we can conclude that the free v.s. functor is also dense. Hence if $V$ and $W$ are vector spaces of the same dimension, and $m:V\hookrightarrow W$ we'd be done if we can find an $f:X\rightarrow Y$ which makes the following diagram commute: $\hskip2in$ I'm sure a clever choice of bases on $V$ and $W$ and subsequent choices for the isomorphisms would make this possible, but these choices essentially use the 2nd theorem. And this approach doesn't seem to port over to epimorphisms.",,['linear-algebra']
84,Determinant identity: $\det M \det N = \det M_{ii} \det M_{jj} - \det M_{ij}\det M_{ji}$,Determinant identity:,\det M \det N = \det M_{ii} \det M_{jj} - \det M_{ij}\det M_{ji},"Let $M$ be a (real) $n \times n$ matrix. For $1 \leq i, j \leq n$ we denote by $M_{ij}$ the $(n-1) \times (n-1)$ matrix that we get when the $i$th row and $j$th column of $M$ are removed. Now, consider fixed $i$ and $j$ with $i\neq j$. Let $N$ be the $(n-2) \times (n-2)$ matrix that we get when removing both the $i$th and $j$th row and the $i$th and $j$th column from $M$. Then the following identity holds: $$ \det M \det N = \det M_{ii}\det M_{jj} - \det M_{ij} \det M_{ji}. $$ We were able to prove this looking at all different terms that can occur on both sides when evaluating the determinant as a sum of $n!$ terms (for instance, terms containing $a_{ji}a_{ij}$ are counted both $1$ time in the LHS and $1$ time in the RHS). We are looking for a slick proof that does not involve writing out the determinant. Any suggestions or approaches to this problem are appreciated!","Let $M$ be a (real) $n \times n$ matrix. For $1 \leq i, j \leq n$ we denote by $M_{ij}$ the $(n-1) \times (n-1)$ matrix that we get when the $i$th row and $j$th column of $M$ are removed. Now, consider fixed $i$ and $j$ with $i\neq j$. Let $N$ be the $(n-2) \times (n-2)$ matrix that we get when removing both the $i$th and $j$th row and the $i$th and $j$th column from $M$. Then the following identity holds: $$ \det M \det N = \det M_{ii}\det M_{jj} - \det M_{ij} \det M_{ji}. $$ We were able to prove this looking at all different terms that can occur on both sides when evaluating the determinant as a sum of $n!$ terms (for instance, terms containing $a_{ji}a_{ij}$ are counted both $1$ time in the LHS and $1$ time in the RHS). We are looking for a slick proof that does not involve writing out the determinant. Any suggestions or approaches to this problem are appreciated!",,"['linear-algebra', 'matrices', 'determinant']"
85,Proof for '$AB = I$ then $BA = I$' without Motivation?,Proof for ' then ' without Motivation?,AB = I BA = I,"I have read this question page ( If $AB = I$ then $BA = I$ ) by Dilawar and saw that most of proofs are using the fact that the algebra of matrices and linear operators are isomorphic. But from a simple view, matrix is only an structured set of scalars, and the fact that the dot product of the i-th row of $A$ and the j-th column of $B$ equals to the Kronecker delta is just a componentwise algebraic informarion. Then I started to wonder if there's any ""brutal"" proof that does not visit the ""higher"" domain of algebraic structures and just uses the simple componentwise algebraic operations to prove that  the dot product of the i-th row of $B$ and the j-th column of $A$ equals to the Kronecker delta from the given condition. (Proof that even a ""machine"" can do) Should we think a matrix as more than a mere 'number box' to show $BA=I$?","I have read this question page ( If $AB = I$ then $BA = I$ ) by Dilawar and saw that most of proofs are using the fact that the algebra of matrices and linear operators are isomorphic. But from a simple view, matrix is only an structured set of scalars, and the fact that the dot product of the i-th row of $A$ and the j-th column of $B$ equals to the Kronecker delta is just a componentwise algebraic informarion. Then I started to wonder if there's any ""brutal"" proof that does not visit the ""higher"" domain of algebraic structures and just uses the simple componentwise algebraic operations to prove that  the dot product of the i-th row of $B$ and the j-th column of $A$ equals to the Kronecker delta from the given condition. (Proof that even a ""machine"" can do) Should we think a matrix as more than a mere 'number box' to show $BA=I$?",,"['linear-algebra', 'soft-question']"
86,Recovering eigenvectors from SVD,Recovering eigenvectors from SVD,,"I am dealing with a problem similar to principal component analysis. Aka, I have a matrix and I want to recover the 'most efficient basis' to explain the matrix variability. With a square matrix these are the eigenvectors, weighted by the eigenvalues. Originally, I was dealing with square matrices, and I used eigendecomposition to recover the eigenvectors, as explained above. Now however, I am dealing with rectangular matrices and using SVD to recover the efficient basis, i.e. A=USV' where the vectors of U are the recovered basis weighted by S, the singular values. In my particular application, the sign of the eigenvalues/singular values makes a difference. Here is my question: with eigendecomposition and square matrices, the eigenvalues will be positive/negative. With SVD, the singular values re contrained to be the absolute value of the eigenvalues, i.e. $s_i$ = | $\lambda_i$ |. Is there anyway to recover the 'sign' of the eigenvalues through SVD? Thanks!","I am dealing with a problem similar to principal component analysis. Aka, I have a matrix and I want to recover the 'most efficient basis' to explain the matrix variability. With a square matrix these are the eigenvectors, weighted by the eigenvalues. Originally, I was dealing with square matrices, and I used eigendecomposition to recover the eigenvectors, as explained above. Now however, I am dealing with rectangular matrices and using SVD to recover the efficient basis, i.e. A=USV' where the vectors of U are the recovered basis weighted by S, the singular values. In my particular application, the sign of the eigenvalues/singular values makes a difference. Here is my question: with eigendecomposition and square matrices, the eigenvalues will be positive/negative. With SVD, the singular values re contrained to be the absolute value of the eigenvalues, i.e. = | |. Is there anyway to recover the 'sign' of the eigenvalues through SVD? Thanks!",s_i \lambda_i,"['linear-algebra', 'eigenvalues-eigenvectors', 'svd']"
87,"How can a matrix be Hermitian, unitary, and diagonal all at once?","How can a matrix be Hermitian, unitary, and diagonal all at once?",,"I was given the following problem in class, and I'm not really sure how to begin this proof. Describe all $3 \times 3$ matrices that are simultaneously Hermitian, unitary, and diagonal. How many such matrices are there? Here's what I have so far. A Hermitian matrix is a complex matrix that is equal to its conjugate transpose: $$A \text{ is Hermitian} \Leftrightarrow A=A^*$$ A unitary Matrix is a complex matrix whose conjugate transpose equals its inverse: $$A \text{ is Unitary} \Leftrightarrow A^*=A^{-1}$$ A diagonal matrix is a matrix  where the entries outside the main diagonal are all zero. $$A \text{ is Diagonal} \Leftrightarrow a_{i,j}=0 \rightarrow i\ne j \  \forall i,j \in \{1,2,..,n\} $$ To satisfy all conditions we can say that a $3 \times 3$ matrix $A$ is simultaneously Hermitian, unitary, and diagonal when: $$A = A* = A^{-1}  \text{ where } a_{i,j}=0 \rightarrow i\ne j \  \forall i,j \in \{1,2,3\}$$","I was given the following problem in class, and I'm not really sure how to begin this proof. Describe all matrices that are simultaneously Hermitian, unitary, and diagonal. How many such matrices are there? Here's what I have so far. A Hermitian matrix is a complex matrix that is equal to its conjugate transpose: A unitary Matrix is a complex matrix whose conjugate transpose equals its inverse: A diagonal matrix is a matrix  where the entries outside the main diagonal are all zero. To satisfy all conditions we can say that a matrix is simultaneously Hermitian, unitary, and diagonal when:","3 \times 3 A \text{ is Hermitian} \Leftrightarrow A=A^* A \text{ is Unitary} \Leftrightarrow A^*=A^{-1} A \text{ is Diagonal} \Leftrightarrow a_{i,j}=0 \rightarrow i\ne j \  \forall i,j \in \{1,2,..,n\}  3 \times 3 A A = A* = A^{-1}  \text{ where } a_{i,j}=0 \rightarrow i\ne j \  \forall i,j \in \{1,2,3\}","['linear-algebra', 'matrices', 'unitary-matrices']"
88,Geometric interpretation of normal and anti-hermitian matrices,Geometric interpretation of normal and anti-hermitian matrices,,"How do I interpret following types of matrices as special types of transformations? I mean what are the transformative properties of following types of matrices, from $\mathbb{R}^n $ to $ \mathbb{R}^n$, or $\mathbb{C^n}$ to $\mathbb{C^n}$? Normal and Anti Hermitian Matrices ? ADDED I expect answer something like this for orthogonal matrices Quoting Wikipedia: As a linear transformation, an orthogonal matrix preserves the dot product of vectors, and therefore acts as an isometry of Euclidean space, such as a rotation or reflection. In other words, it is a unitary transformation. ADDED I managed to collect some more information: Unitary matrices as Linear Maps preserve inner products , may be not necessarily for reals only, as orthogonal ones(???).","How do I interpret following types of matrices as special types of transformations? I mean what are the transformative properties of following types of matrices, from $\mathbb{R}^n $ to $ \mathbb{R}^n$, or $\mathbb{C^n}$ to $\mathbb{C^n}$? Normal and Anti Hermitian Matrices ? ADDED I expect answer something like this for orthogonal matrices Quoting Wikipedia: As a linear transformation, an orthogonal matrix preserves the dot product of vectors, and therefore acts as an isometry of Euclidean space, such as a rotation or reflection. In other words, it is a unitary transformation. ADDED I managed to collect some more information: Unitary matrices as Linear Maps preserve inner products , may be not necessarily for reals only, as orthogonal ones(???).",,['linear-algebra']
89,Do eigenvectors always form a basis?,Do eigenvectors always form a basis?,,Suppose we have a $n \times n $ matrix  over $\Bbb R$ . Is it necessary that we should have $n$ linearly independent eigenvectors associated with eigenvalues so that they form a basis? Can you give a proof or counterexample? How about if you have the same question over complex numbers?,Suppose we have a matrix  over . Is it necessary that we should have linearly independent eigenvectors associated with eigenvalues so that they form a basis? Can you give a proof or counterexample? How about if you have the same question over complex numbers?,n \times n  \Bbb R n,"['linear-algebra', 'proof-writing', 'eigenvalues-eigenvectors']"
90,Solving systems of linear equations when matrix of coefficients is singular,Solving systems of linear equations when matrix of coefficients is singular,,"When attempting to solve a problem of type $AX=B$ where $A$ is the matrix of coefficients, $X$ contains the variables and $B$ is the right hand side, it turned out the $A$ was singular. As a result I could not premultiply both side by $A^{-1}$. In such a case, how would I solve for the variables without having to use augmented matrices? Here is the question: $$\begin{pmatrix} 1&2&1&1&1 \\ 2&1&2&1&1 \\ 1&2&3&1&1 \\ 2&2&1&1&3 \\3&3&5&2&2 \end{pmatrix}\begin{pmatrix} u \\ v \\ w \\ x \\ y \end{pmatrix}=\begin{pmatrix} 6.3 \\ 6.7 \\ 7.7 \\ 9.8 \\10.9 \end{pmatrix}$$ Since $A$ is singular I cannot do $X=A^{-1}B$ which is the method I would typically use to solve for the unknowns. In later parts of the book I am studying they elude to a method with which one can use to solve for $X$, but do not further elaborate. Does anyone have an idea on how to solve this problem (or any problem in general where A is singular) without the use augmented matrices or is that the only way?","When attempting to solve a problem of type $AX=B$ where $A$ is the matrix of coefficients, $X$ contains the variables and $B$ is the right hand side, it turned out the $A$ was singular. As a result I could not premultiply both side by $A^{-1}$. In such a case, how would I solve for the variables without having to use augmented matrices? Here is the question: $$\begin{pmatrix} 1&2&1&1&1 \\ 2&1&2&1&1 \\ 1&2&3&1&1 \\ 2&2&1&1&3 \\3&3&5&2&2 \end{pmatrix}\begin{pmatrix} u \\ v \\ w \\ x \\ y \end{pmatrix}=\begin{pmatrix} 6.3 \\ 6.7 \\ 7.7 \\ 9.8 \\10.9 \end{pmatrix}$$ Since $A$ is singular I cannot do $X=A^{-1}B$ which is the method I would typically use to solve for the unknowns. In later parts of the book I am studying they elude to a method with which one can use to solve for $X$, but do not further elaborate. Does anyone have an idea on how to solve this problem (or any problem in general where A is singular) without the use augmented matrices or is that the only way?",,"['linear-algebra', 'matrices']"
91,Incremental calculation of inverse of a matrix,Incremental calculation of inverse of a matrix,,"Does there exist a fast way to calculate the inverse of an $N \times N$ matrix, if we know the inverse of the $(N-1) \times (N-1)$ sub-matrix? For example, if $A$ is a $1000 \times 1000$ invertible matrix for which the inverse is known, and $B$ is a $1001 \times 1001$ matrix obtained by adding a new row and column to $A$, what is the best approach for calculating inverse of $B$?","Does there exist a fast way to calculate the inverse of an $N \times N$ matrix, if we know the inverse of the $(N-1) \times (N-1)$ sub-matrix? For example, if $A$ is a $1000 \times 1000$ invertible matrix for which the inverse is known, and $B$ is a $1001 \times 1001$ matrix obtained by adding a new row and column to $A$, what is the best approach for calculating inverse of $B$?",,"['linear-algebra', 'matrices', 'computer-science', 'numerical-methods']"
92,Proof of: $AB=0 \Rightarrow Rank(A)+Rank(B) \leq n$,Proof of:,AB=0 \Rightarrow Rank(A)+Rank(B) \leq n,"As the title says,  am searching for a proof of If $A,B \in \mathbb{R}^{n\times n}$ and   $AB=0$ then $\mathrm{rank}(A)+\mathrm{rank}(B) \leq n$ I am doing this as preparation for an upcoming exam and can't figure a way to start. Please just post small hints as answers. I will try to go from there. Thank you ftiaronsem","As the title says,  am searching for a proof of If $A,B \in \mathbb{R}^{n\times n}$ and   $AB=0$ then $\mathrm{rank}(A)+\mathrm{rank}(B) \leq n$ I am doing this as preparation for an upcoming exam and can't figure a way to start. Please just post small hints as answers. I will try to go from there. Thank you ftiaronsem",,"['linear-algebra', 'matrices', 'inequality', 'matrix-rank']"
93,Are translations of a polynomial linearly independent?,Are translations of a polynomial linearly independent?,,"I've been wondering about the following question: Suppose that $P$ is a polynomial of degree $n$ with complex coefficients. Assume that $a_0, a_1, \dots, a_n \in \mathbb{C}$ are distinct. Are the polynomials $$P(x + a_0), P(x + a_1), \dots, P(x + a_n)$$ linearly independent?","I've been wondering about the following question: Suppose that $P$ is a polynomial of degree $n$ with complex coefficients. Assume that $a_0, a_1, \dots, a_n \in \mathbb{C}$ are distinct. Are the polynomials $$P(x + a_0), P(x + a_1), \dots, P(x + a_n)$$ linearly independent?",,"['linear-algebra', 'polynomials']"
94,No solutions to a matrix inequality?,No solutions to a matrix inequality?,,"If $A$ is a stochastic matrix, then $A$ is entry-wise nonnegative and $Ae = e$ , i.e., $(1,e)$ is a right eigenpair for $A$ . Is it true that there exists a vector $b$ such that $$(A - I)x \geq b$$ has no solutions in $x$ ?  If so, is there a simple proof? Motivation: I've been trying to construct an answer to another question using linear programming duality (as the OP implies he is interested in).  If my reasoning is correct, this is the only step I need to complete the argument.  I feel like this should be an easy question to answer, but I've been working on it for a while with no success.","If is a stochastic matrix, then is entry-wise nonnegative and , i.e., is a right eigenpair for . Is it true that there exists a vector such that has no solutions in ?  If so, is there a simple proof? Motivation: I've been trying to construct an answer to another question using linear programming duality (as the OP implies he is interested in).  If my reasoning is correct, this is the only step I need to complete the argument.  I feel like this should be an easy question to answer, but I've been working on it for a while with no success.","A A Ae = e (1,e) A b (A - I)x \geq b x","['linear-algebra', 'stochastic-matrices']"
95,Is every square matrix a sum of a diagonal matrix and a nilpotent matrix,Is every square matrix a sum of a diagonal matrix and a nilpotent matrix,,"The question is in the title. I am working with complex matrices. This is true for $2\times 2$ matrices, but becomes complicated already for $3\times 3$ matrices if we try to brute force it. By brute force, I mean taking an arbitrary $3\times 3$ matrix and imposing conditions on the diagonal entries so that the matrix is nilpotent. A follow up question for matrices which can be expressed as a sum of diagonal and nilpotent matrices is whether there are infinitely many ways of doing this? This feels unlikely because the final answer seems to depend on solving $n$ independent polynomial equations in $n$ variables, but I might be wrong.","The question is in the title. I am working with complex matrices. This is true for matrices, but becomes complicated already for matrices if we try to brute force it. By brute force, I mean taking an arbitrary matrix and imposing conditions on the diagonal entries so that the matrix is nilpotent. A follow up question for matrices which can be expressed as a sum of diagonal and nilpotent matrices is whether there are infinitely many ways of doing this? This feels unlikely because the final answer seems to depend on solving independent polynomial equations in variables, but I might be wrong.",2\times 2 3\times 3 3\times 3 n n,"['linear-algebra', 'matrices']"
96,Proof and Intuition of the Determinant Formula?,Proof and Intuition of the Determinant Formula?,,"[I did notice similar questions were asked here before, but I couldn't find a satisfactory answer for me to grasp as a beginner, so I chose to post this question] I'm just starting to teach myself linear algebra with Linear Algebra and Group Theory . The book starts with the concept of determinant with the definition of even & odd permutations by giving the example of a {3 by 3 array} with the following equation: $$\begin{vmatrix} a_{11}\ \ a_{12}\ \ ...\ \ a_{1k}\ \ ...\ \ a_{1n}\\a_{21}\ \ a_{22}\ \ ...\ \ a_{2k}\ \ ...\ \ a_{2n}\\...\ \ ...\ \ ...\ \ ...\ \ ...\ \ ...\\a_{n1}\ \ a_{n2}\ \ ...\ \ a_{nk}\ \ ...\ \ a_{nn}\end{vmatrix}=\displaystyle\sum_{(p_1, p_2, ..., p_n)}(-1)^{[p_1, p_2, ..., p_n]}a_{1p_1}a_{2p_2}...a_{np_n}$$ where $[p_1, p_2, ..., p_n] $ is the number of inversions of permutation $p_1, p_2, ..., p_n$ So because there is no justification given in the book for the above equation for { $n$ by $n$ array} and the concept of determinant feels a bit odd at the first place, I tried to investigate it myself: [Step 1]: I started with a {2 by 2 array} first, by taking a system of two equations in two unknowns: $$(eq1):\ a_{11}x_1+a_{12}x_2=b_1\\(eq2):\ a_{21}x_1+a_{22}x_2=b_2$$ $$A=\begin{Vmatrix} a_{11}\ \ a_{12}\\a_{21}\ \ a_{22}\end{Vmatrix}$$ with the determinant $detA$ [Step 2]: I then tried to eliminate { $x_2$ in $eq1$ } and { $x_1$ in $eq2$ } by doing the following: $$(eq1 \cdot a_{22})-(eq2 \cdot a_{12})=(a_{11}a_{22}-a_{12}a_{21})x_1\\(eq2 \cdot a_{11})-(eq1 \cdot a_{21})=(a_{11}a_{22}-a_{12}a_{21})x_2$$ [Step 3]: I noticed that the two coefficients above both give me the determinant of the array, so I then postulated the following statement: the determinant is {the coefficient of unknown $x_k$ in $k$ th row} after eliminating other unknowns from the $k$ th row by {multiplication} and {subtracting other rows} in the array. that is to say, if I have a $n$ th order array $$N=\begin{Vmatrix} a_{11}\ \ a_{12}\ \ ...\ \ a_{1k}\ \ ...\ \ a_{1n}\\a_{21}\ \ a_{22}\ \ ...\ \ a_{2k}\ \ ...\ \ a_{2n}\\...\ \ ...\ \ ...\ \ ...\ \ ...\ \ ...\\a_{n1}\ \ a_{n2}\ \ ...\ \ a_{nk}\ \ ...\ \ a_{nn}\end{Vmatrix}$$ I can eventually transfer $N$ into $$\begin{Vmatrix} detN & 0 & ... & 0 & ... & 0\\0 & detN & ... & 0 & ... & 0\\... & ... & ... & ... & ... & ...\\0 & 0 & ... & 0 & ... & detN\end{Vmatrix}$$ [Step 4]: I tested my statement with a {3 by 3 array} and it seems to work. And the idea of odd & even permutation seems to become more intuitive as it has to do with the order of subtracting depending on the row of the unknowns. So here comes my questions if my guess is right, how do I construct the permuatation formula at the beginning of the question for { $n$ by $n$ array} without defining determinant by using a set of formalistic operation at the first place ? I've seen multiple answers talking about the geometric intuition of determinant (and I roughly get the idea). How does the intuition of permutation connects, or transfers into the geometric intuition ? [Note: I have never studied abstract algebra, so answers without using notations in abstract algebra will be much appreciated :) ] ----------------------------------------------------------------------- EDIT: I think I figured out my question 2 (the geometric intuition).... correct me if I am wrong So again using a {2 by 2 array} as an example: [Step 1]: Assume again I have the following equations and array $$(eq1):\ a_{11}x_1+a_{12}x_2=b_1\\(eq2):\ a_{21}x_1+a_{22}x_2=b_2$$ $$A=\begin{vmatrix} a_{11}\ \ a_{12}\\a_{21}\ \ a_{22}\end{vmatrix}$$ with the determinant $detA$ [Step 2]: I can immediately transfer the array into $$\begin{Vmatrix} detA & 0\\0 & detA\end{Vmatrix}$$ [Step 3]: Becasue the above array is the coefficient of $x_1$ and $x_2$ , I can write the unknowns down as a vector $$\begin{bmatrix} x_1\\x_2\end{bmatrix}$$ which makes the { $detA$ array} a linear transformation when multiply with this vector","[I did notice similar questions were asked here before, but I couldn't find a satisfactory answer for me to grasp as a beginner, so I chose to post this question] I'm just starting to teach myself linear algebra with Linear Algebra and Group Theory . The book starts with the concept of determinant with the definition of even & odd permutations by giving the example of a {3 by 3 array} with the following equation: where is the number of inversions of permutation So because there is no justification given in the book for the above equation for { by array} and the concept of determinant feels a bit odd at the first place, I tried to investigate it myself: [Step 1]: I started with a {2 by 2 array} first, by taking a system of two equations in two unknowns: with the determinant [Step 2]: I then tried to eliminate { in } and { in } by doing the following: [Step 3]: I noticed that the two coefficients above both give me the determinant of the array, so I then postulated the following statement: the determinant is {the coefficient of unknown in th row} after eliminating other unknowns from the th row by {multiplication} and {subtracting other rows} in the array. that is to say, if I have a th order array I can eventually transfer into [Step 4]: I tested my statement with a {3 by 3 array} and it seems to work. And the idea of odd & even permutation seems to become more intuitive as it has to do with the order of subtracting depending on the row of the unknowns. So here comes my questions if my guess is right, how do I construct the permuatation formula at the beginning of the question for { by array} without defining determinant by using a set of formalistic operation at the first place ? I've seen multiple answers talking about the geometric intuition of determinant (and I roughly get the idea). How does the intuition of permutation connects, or transfers into the geometric intuition ? [Note: I have never studied abstract algebra, so answers without using notations in abstract algebra will be much appreciated :) ] ----------------------------------------------------------------------- EDIT: I think I figured out my question 2 (the geometric intuition).... correct me if I am wrong So again using a {2 by 2 array} as an example: [Step 1]: Assume again I have the following equations and array with the determinant [Step 2]: I can immediately transfer the array into [Step 3]: Becasue the above array is the coefficient of and , I can write the unknowns down as a vector which makes the { array} a linear transformation when multiply with this vector","\begin{vmatrix} a_{11}\ \ a_{12}\ \ ...\ \ a_{1k}\ \ ...\ \ a_{1n}\\a_{21}\ \ a_{22}\ \ ...\ \ a_{2k}\ \ ...\ \ a_{2n}\\...\ \ ...\ \ ...\ \ ...\ \ ...\ \ ...\\a_{n1}\ \ a_{n2}\ \ ...\ \ a_{nk}\ \ ...\ \ a_{nn}\end{vmatrix}=\displaystyle\sum_{(p_1, p_2, ..., p_n)}(-1)^{[p_1, p_2, ..., p_n]}a_{1p_1}a_{2p_2}...a_{np_n} [p_1, p_2, ..., p_n]  p_1, p_2, ..., p_n n n (eq1):\ a_{11}x_1+a_{12}x_2=b_1\\(eq2):\ a_{21}x_1+a_{22}x_2=b_2 A=\begin{Vmatrix} a_{11}\ \ a_{12}\\a_{21}\ \ a_{22}\end{Vmatrix} detA x_2 eq1 x_1 eq2 (eq1 \cdot a_{22})-(eq2 \cdot a_{12})=(a_{11}a_{22}-a_{12}a_{21})x_1\\(eq2 \cdot a_{11})-(eq1 \cdot a_{21})=(a_{11}a_{22}-a_{12}a_{21})x_2 x_k k k n N=\begin{Vmatrix} a_{11}\ \ a_{12}\ \ ...\ \ a_{1k}\ \ ...\ \ a_{1n}\\a_{21}\ \ a_{22}\ \ ...\ \ a_{2k}\ \ ...\ \ a_{2n}\\...\ \ ...\ \ ...\ \ ...\ \ ...\ \ ...\\a_{n1}\ \ a_{n2}\ \ ...\ \ a_{nk}\ \ ...\ \ a_{nn}\end{Vmatrix} N \begin{Vmatrix} detN & 0 & ... & 0 & ... & 0\\0 & detN & ... & 0 & ... & 0\\... & ... & ... & ... & ... & ...\\0 & 0 & ... & 0 & ... & detN\end{Vmatrix} n n (eq1):\ a_{11}x_1+a_{12}x_2=b_1\\(eq2):\ a_{21}x_1+a_{22}x_2=b_2 A=\begin{vmatrix} a_{11}\ \ a_{12}\\a_{21}\ \ a_{22}\end{vmatrix} detA \begin{Vmatrix} detA & 0\\0 & detA\end{Vmatrix} x_1 x_2 \begin{bmatrix} x_1\\x_2\end{bmatrix} detA","['linear-algebra', 'determinant', 'intuition']"
97,Find matrix $A\in \mathcal{M}_n (\mathbb{N})$ such that $A^k =\left( \sum_{i=1}^{k}10^{i-1} \right)A$.,Find matrix  such that .,A\in \mathcal{M}_n (\mathbb{N}) A^k =\left( \sum_{i=1}^{k}10^{i-1} \right)A,"I was watching this video by Flammable Maths about why $$ \begin{pmatrix} 3 &4\\ 6&8 \end{pmatrix}^2 = \begin{pmatrix} 33 &44\\ 66&88 \end{pmatrix} $$ In the video, it is left as a challenge for the viewer to see if you can generalize the result as follows: Given some $k \in \mathbb{N}\cap[2,\infty), $ can you find a matrix $A\in \mathcal{M}_{n \times n} (\mathbb{N})$ such that $A^k =\left( \sum_{i=1}^{k}10^{i-1} \right)A$ ? I attempted a solution to this problem and did the following. I supposed (with the intention of hopefully simplifying calculations) that $A$ is diagonalizable. This means that I can write the equation we want as $$ PD^{k} P^{-1}= \left( \sum_{i=1}^{k}10^{i-1} \right)PD P^{-1} $$ Now, taking the determinant on both sides I get that \begin{align*} &|P||D|^k|P^{-1}| =  \left( \sum_{i=1}^{k}10^{i-1} \right)^n |P||D| |P^{-1}|\\ \implies & \left(\prod_{j=1}^n \lambda_j\right)^k =  \left( \sum_{i=1}^{k}10^{i-1} \right)^n\left(\prod_{j=1}^n \lambda_j\right)\\ \implies & \prod_{j=1}^n \lambda_j^{k-1} =  \left( \sum_{i=1}^{k}10^{i-1} \right)^n \end{align*} where the $\lambda_j$ 's are the eigenvalues of $A$ . From here I think that if I find a set of eigenvalues that satisfy the above equation I can reconstruct a matrix which satisfies our original intended equation, however, I'm not sure if this is a good way to approach this problem. Does anyone know a better way to solve this? Or does anyone have any other ideas on how to tackle it? Ideally, I would like to find some patter or family of matrices which satisfy the desired property, buy any and all suggestions would be greatly appreciated. Thank you very much! Edit: As pointed out by levap in the comments, it's impossible to find a solution of a matrix made up of strictly positive integers for $k \ge 3$ . However, to not get rid of the possibility of other interesting solutions and/or observations, I'll clarify that other types of solutions with matrices in $\mathcal{M}_n (\mathbb{Z})$ , $\mathcal{M}_n (\mathbb{Q})$ or even in $\mathcal{M}_n (\mathbb{R})$ will happily be considered for the bounty if you think they're similar to the original problem. In short, if you find something you think is interesting, even if it's not too similar to $\begin{pmatrix} 3 &4\\ 6&8 \end{pmatrix}$ , please post them nevertheless. Thank you!","I was watching this video by Flammable Maths about why In the video, it is left as a challenge for the viewer to see if you can generalize the result as follows: Given some can you find a matrix such that ? I attempted a solution to this problem and did the following. I supposed (with the intention of hopefully simplifying calculations) that is diagonalizable. This means that I can write the equation we want as Now, taking the determinant on both sides I get that where the 's are the eigenvalues of . From here I think that if I find a set of eigenvalues that satisfy the above equation I can reconstruct a matrix which satisfies our original intended equation, however, I'm not sure if this is a good way to approach this problem. Does anyone know a better way to solve this? Or does anyone have any other ideas on how to tackle it? Ideally, I would like to find some patter or family of matrices which satisfy the desired property, buy any and all suggestions would be greatly appreciated. Thank you very much! Edit: As pointed out by levap in the comments, it's impossible to find a solution of a matrix made up of strictly positive integers for . However, to not get rid of the possibility of other interesting solutions and/or observations, I'll clarify that other types of solutions with matrices in , or even in will happily be considered for the bounty if you think they're similar to the original problem. In short, if you find something you think is interesting, even if it's not too similar to , please post them nevertheless. Thank you!","
\begin{pmatrix}
3 &4\\
6&8
\end{pmatrix}^2 = \begin{pmatrix}
33 &44\\
66&88
\end{pmatrix}
 k \in \mathbb{N}\cap[2,\infty),  A\in \mathcal{M}_{n \times n} (\mathbb{N}) A^k =\left( \sum_{i=1}^{k}10^{i-1} \right)A A 
PD^{k} P^{-1}= \left( \sum_{i=1}^{k}10^{i-1} \right)PD P^{-1}
 \begin{align*}
&|P||D|^k|P^{-1}| =  \left( \sum_{i=1}^{k}10^{i-1} \right)^n |P||D| |P^{-1}|\\
\implies & \left(\prod_{j=1}^n \lambda_j\right)^k =  \left( \sum_{i=1}^{k}10^{i-1} \right)^n\left(\prod_{j=1}^n \lambda_j\right)\\
\implies & \prod_{j=1}^n \lambda_j^{k-1} =  \left( \sum_{i=1}^{k}10^{i-1} \right)^n
\end{align*} \lambda_j A k \ge 3 \mathcal{M}_n (\mathbb{Z}) \mathcal{M}_n (\mathbb{Q}) \mathcal{M}_n (\mathbb{R}) \begin{pmatrix}
3 &4\\
6&8
\end{pmatrix}","['linear-algebra', 'matrices', 'matrix-equations']"
98,Reference: Continuity of Eigenvectors,Reference: Continuity of Eigenvectors,,"I am looking for an appropriate reference for the following fact. For each $X \in \mathbb{R}^{n \times n}_{\text{sym}}$ (symmetric matrix),   there exist $\varepsilon, L > 0$, such that   for all $H \in \mathbb{R}^{n \times n}_{\text{sym}}$ with $\|H\| \le \varepsilon$   the following holds: There are orthogonal matrices $P, Q$, such that   \begin{equation*} 	Q^\top \, X \, Q = \Lambda(X) \end{equation*}   and   \begin{equation*} 	P^\top \, (X + H) \, P = \Lambda(X + H) \end{equation*}   and   \begin{equation*} 	\| P - Q \| \le L \, \| H \|. \end{equation*} Here, $\lambda(X)$ is the diagonal matrix containing the eigenvalues of $X$. That is, $P$ and $Q$ are bases of eigenvectors of $X$ and $X + H$, respectively. Here, it is crucial that we can choose $Q$ in dependence of $H$. This result can be found in this article , see Lemma 4.3. I feel, however, that a reference from 2003 is not appropriate for this ""simple"" fact.","I am looking for an appropriate reference for the following fact. For each $X \in \mathbb{R}^{n \times n}_{\text{sym}}$ (symmetric matrix),   there exist $\varepsilon, L > 0$, such that   for all $H \in \mathbb{R}^{n \times n}_{\text{sym}}$ with $\|H\| \le \varepsilon$   the following holds: There are orthogonal matrices $P, Q$, such that   \begin{equation*} 	Q^\top \, X \, Q = \Lambda(X) \end{equation*}   and   \begin{equation*} 	P^\top \, (X + H) \, P = \Lambda(X + H) \end{equation*}   and   \begin{equation*} 	\| P - Q \| \le L \, \| H \|. \end{equation*} Here, $\lambda(X)$ is the diagonal matrix containing the eigenvalues of $X$. That is, $P$ and $Q$ are bases of eigenvectors of $X$ and $X + H$, respectively. Here, it is crucial that we can choose $Q$ in dependence of $H$. This result can be found in this article , see Lemma 4.3. I feel, however, that a reference from 2003 is not appropriate for this ""simple"" fact.",,"['linear-algebra', 'reference-request', 'eigenvalues-eigenvectors', 'perturbation-theory']"
99,How do you transpose tensors?,How do you transpose tensors?,,"We transpose a matrix $A$ by replacing $A_{ij}$ with $A_{ji}$ , for all $i$ and $j$ . However, in case $A$ has more than two dimensions (that is, it is a tensor), I don't know how to apply the transpose operation. If A has dimensions $3\times 3 \times 8$ , then what will replace $A_{ijk}$ ? If $A$ has shape $3\times 3\times 8\times 8$ , then what will replace $A_{ijkl}$ ?","We transpose a matrix by replacing with , for all and . However, in case has more than two dimensions (that is, it is a tensor), I don't know how to apply the transpose operation. If A has dimensions , then what will replace ? If has shape , then what will replace ?",A A_{ij} A_{ji} i j A 3\times 3 \times 8 A_{ijk} A 3\times 3\times 8\times 8 A_{ijkl},"['linear-algebra', 'matrices', 'tensors', 'transpose']"
