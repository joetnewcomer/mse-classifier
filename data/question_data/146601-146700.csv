,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Limit to $e^2$.,Limit to .,e^2,"I have to show $$ \lim_{x \to \infty} {\left(\frac{x^2 + 1}{1 - x^2}\right)}^{x^2} = e^2, $$ but I don't get the trick to see it, I suppose I can use something like $$ {\left(\lim_{x \to \infty} {\left(1 + \frac1x\right)}^x\right)}^2 = e^2, $$ but I do not see how I can apply to the problem. On the other hand, $$ \frac{x^2 + 1}{1 - x^2} = 1 + \frac{2 x^2}{1 - x^2}. $$ Any hint?","I have to show but I don't get the trick to see it, I suppose I can use something like but I do not see how I can apply to the problem. On the other hand, Any hint?","
\lim_{x \to \infty} {\left(\frac{x^2 + 1}{1 - x^2}\right)}^{x^2} = e^2,
 
{\left(\lim_{x \to \infty} {\left(1 + \frac1x\right)}^x\right)}^2 = e^2,
 
\frac{x^2 + 1}{1 - x^2} = 1 + \frac{2 x^2}{1 - x^2}.
",['limits']
1,What are the steps in breaking down the exponent in this limit analysis?,What are the steps in breaking down the exponent in this limit analysis?,,"I'm trying to understand the reasoning in the following step of a limit analysis: $$\lim_{n \to \infty} n\left(\left[1- \frac{1+c}{\frac{n}{\ln(n)}} \right]^{\frac{n}{\ln (n)}}\right)^{(n-1)\ln n/n} = \lim_{n \to \infty} ne^{-[(1+c)\ln(n)]}$$ I understand the ""inner"" part; $\lim_{n \to \infty} [1-\frac{1+c}{\frac{n}{\ln (n)}}]^{\frac{n}{\ln n}} = e^{-(1+c)}.$ And I sort of see that outer exponent $((n-1)\ln n) /n = (\ln n) - (\ln n / n)$ and the second part goes to 0, but it's not clear to me what rules actually justify ""bringing the limit to the exponent"". What are the actual steps involved in deducing this limit? More generally, these types of asymptotic analysis show up in comp sci all the time and I feel there is a bag of tricks that I am missing.","I'm trying to understand the reasoning in the following step of a limit analysis: I understand the ""inner"" part; And I sort of see that outer exponent and the second part goes to 0, but it's not clear to me what rules actually justify ""bringing the limit to the exponent"". What are the actual steps involved in deducing this limit? More generally, these types of asymptotic analysis show up in comp sci all the time and I feel there is a bag of tricks that I am missing.",\lim_{n \to \infty} n\left(\left[1- \frac{1+c}{\frac{n}{\ln(n)}} \right]^{\frac{n}{\ln (n)}}\right)^{(n-1)\ln n/n} = \lim_{n \to \infty} ne^{-[(1+c)\ln(n)]} \lim_{n \to \infty} [1-\frac{1+c}{\frac{n}{\ln (n)}}]^{\frac{n}{\ln n}} = e^{-(1+c)}. ((n-1)\ln n) /n = (\ln n) - (\ln n / n),"['calculus', 'limits']"
2,"Let $x_0$ be a transcendental number, $x_{n+1}=\frac{3-x_n}{x_n^2+3x_n-2}$. What is the limit of $x_n$?","Let  be a transcendental number, . What is the limit of ?",x_0 x_{n+1}=\frac{3-x_n}{x_n^2+3x_n-2} x_n,"Let $x_0$ be a transcendental number, $$x_{n+1}=\frac{3-x_n}{x_{n}^{2}+3x_{n}-2}$$ What is the limit of $x_{n}$ ? Choose $x_0=\pi$ , and is seems that the limit of $x_n$ is $-1$ . But what is the proof for this $\pi$ and other numbers? Let $$f(x)=\frac{3-x}{x^{2}+3x-2}$$ The following may be helpful. $$f'(x)=\frac{(x-7)(x+1)}{(x^{2}+3x-2)^2}$$ $$f(x)-x=\frac{-(x-1)(x+1)(x+3)}{x^{2}+3x-2}$$ $$f(x)+1=\frac{(x+1)^{2}}{x^{2}+3x-2}$$ .","Let be a transcendental number, What is the limit of ? Choose , and is seems that the limit of is . But what is the proof for this and other numbers? Let The following may be helpful. .",x_0 x_{n+1}=\frac{3-x_n}{x_{n}^{2}+3x_{n}-2} x_{n} x_0=\pi x_n -1 \pi f(x)=\frac{3-x}{x^{2}+3x-2} f'(x)=\frac{(x-7)(x+1)}{(x^{2}+3x-2)^2} f(x)-x=\frac{-(x-1)(x+1)(x+3)}{x^{2}+3x-2} f(x)+1=\frac{(x+1)^{2}}{x^{2}+3x-2},"['calculus', 'limits']"
3,How to solve $\lim_{n \to \infty}\frac{1}{\sqrt[3]{n^3+n+1}-\sqrt{n^2-n+2}}$ without L'Hopital?,How to solve  without L'Hopital?,\lim_{n \to \infty}\frac{1}{\sqrt[3]{n^3+n+1}-\sqrt{n^2-n+2}},$\lim_{n \to \infty}\frac{1}{\sqrt[3]{n^3+n+1}-\sqrt{n^2-n+2}}$ $\lim_{n \to \infty}\frac{1}{\sqrt[6]{(n^3+n+1)^2}-\sqrt[6]{(n^2-n+2)^3}}$ but because this limit is still the type of $\frac{1}{\infty-\infty}$ I tried to do this: $\lim_{n \to \infty}\frac{\sqrt[6]{(n^3+n+1)^2}+\sqrt[6]{(n^2-n+2)^3}}{(n^3+n+1)^2-(n^2-n+2)^3} = \lim_{n \to \infty}\frac{\sqrt[6]{(n^3+n+1)^2}+\sqrt[6]{(n^2-n+2)^3}}{3n^5-7n^4+15n^3-17n^2+14n-7}$ I'm totally stuck here. I would divide the fraction by $3n^5$ and then the solution is $0$ . Not the correct answer. Did I miss something?,but because this limit is still the type of I tried to do this: I'm totally stuck here. I would divide the fraction by and then the solution is . Not the correct answer. Did I miss something?,\lim_{n \to \infty}\frac{1}{\sqrt[3]{n^3+n+1}-\sqrt{n^2-n+2}} \lim_{n \to \infty}\frac{1}{\sqrt[6]{(n^3+n+1)^2}-\sqrt[6]{(n^2-n+2)^3}} \frac{1}{\infty-\infty} \lim_{n \to \infty}\frac{\sqrt[6]{(n^3+n+1)^2}+\sqrt[6]{(n^2-n+2)^3}}{(n^3+n+1)^2-(n^2-n+2)^3} = \lim_{n \to \infty}\frac{\sqrt[6]{(n^3+n+1)^2}+\sqrt[6]{(n^2-n+2)^3}}{3n^5-7n^4+15n^3-17n^2+14n-7} 3n^5 0,"['calculus', 'limits', 'radicals', 'limits-without-lhopital']"
4,Proof Check: $ \lim_{x \rightarrow 0} \frac{x-\sin x}{x^2} $,Proof Check:, \lim_{x \rightarrow 0} \frac{x-\sin x}{x^2} ,"The limit can be rewritten as $$ \lim_{x \rightarrow 0} \frac{x-\sin x}{x^2} = \lim_{x \rightarrow 0} \frac{1}{x} \left[ 1- \frac{\sin x}{x} \right] $$ Recall the inequality, $$ \cos x < \frac{\sin x}{x} < 1$$ holds for $ x\in (-\pi/2, \pi/2) $ . This provides the new inequalities $$ 0 < \frac{1}{x} \left[ 1- \frac{\sin x}{x} \right] < \frac{1-\cos x}{x},  \text{ with } x>0 $$ $$ \frac{1-\cos x}{x} < \frac{1}{x} \left[1- \frac{\sin x}{x} \right]< 0,  \text{ with } x<0 $$ Applying the Squeeze Theorem, we get $$ \lim_{x \rightarrow 0^{\pm}}  \frac{1}{x} \left[ 1- \frac{\sin x}{x} \right] = 0$$ Note: $ \lim_{ x \rightarrow 0} \frac{1-\cos x}{x} = 0 $","The limit can be rewritten as Recall the inequality, holds for . This provides the new inequalities Applying the Squeeze Theorem, we get Note:"," \lim_{x \rightarrow 0} \frac{x-\sin x}{x^2} = \lim_{x \rightarrow 0} \frac{1}{x} \left[ 1- \frac{\sin x}{x} \right]   \cos x < \frac{\sin x}{x} < 1  x\in (-\pi/2, \pi/2)   0 < \frac{1}{x} \left[ 1- \frac{\sin x}{x} \right] < \frac{1-\cos x}{x},  \text{ with } x>0   \frac{1-\cos x}{x} < \frac{1}{x} \left[1- \frac{\sin x}{x} \right]< 0,  \text{ with } x<0   \lim_{x \rightarrow 0^{\pm}}  \frac{1}{x} \left[ 1- \frac{\sin x}{x} \right] = 0  \lim_{ x \rightarrow 0} \frac{1-\cos x}{x} = 0 ","['calculus', 'limits', 'limits-without-lhopital']"
5,"Soft question, limit of $n\mathbb{Z}$","Soft question, limit of",n\mathbb{Z},"I was trying to think of countable subsets of $\mathbb{R}$ that were, in a sense, small. By that I mean how 'spread out' the terms were. I know this isn't at all concise, but for example, $3\mathbb{Z}$ is 'smaller' (in the manner described above) than say $2\mathbb{Z}$ . So this got me thinking, looking at specifically $n\mathbb{Z}$ , is there anything meaningful to talk about $\lim_{n\to\infty}{n\mathbb{Z}}$ ? Firstly I thought, does this even make sense? Is it just a meaningless thing to consider? What made intuitive sense is, if we consider the extended reals, that the limit is $\{-\infty,+\infty\}$ , but that just seemed somehow not quite right. Is there any resources available, or anything to talk about limits, or limits alike?","I was trying to think of countable subsets of that were, in a sense, small. By that I mean how 'spread out' the terms were. I know this isn't at all concise, but for example, is 'smaller' (in the manner described above) than say . So this got me thinking, looking at specifically , is there anything meaningful to talk about ? Firstly I thought, does this even make sense? Is it just a meaningless thing to consider? What made intuitive sense is, if we consider the extended reals, that the limit is , but that just seemed somehow not quite right. Is there any resources available, or anything to talk about limits, or limits alike?","\mathbb{R} 3\mathbb{Z} 2\mathbb{Z} n\mathbb{Z} \lim_{n\to\infty}{n\mathbb{Z}} \{-\infty,+\infty\}","['limits', 'soft-question']"
6,"Proof of continuity and the limit $ f(x,y) = \frac{1}{y^2-x^2}\int_x^y\ln(e+e^t)dt$",Proof of continuity and the limit," f(x,y) = \frac{1}{y^2-x^2}\int_x^y\ln(e+e^t)dt","I have to prove the continuity and find the limit $(x,y)\to(1,1)$ of $f:D\to R$ , where $$D=\{(x,y)\in R^2:|x|\neq|y|\}$$ and $$ f(x,y) = \frac{1}{y^2-x^2}\int_x^y\ln(e+e^t)dt.$$ I don't know whether my way of thinking is correct and I am a little stuck on the limit. Continuity I have had a few thoughts: The multiplication of continuous functions is continuous, thus if $\int g(x)$ is differentiable and the fraction is continuous, it should be continuous. To be integrable, the integral of an absolute value of the function (the derivative) should have a finite limit. My steps $$ f(x,y) = \frac{1}{y+x}\cdot\frac{1}{y-x}\int_x^y\ln(e+e^t)dt$$ Since we can bound the integral by: $$\int_x^y\ln(e+e^t)dt<\int_x^y\ln(e^t)dt=\int_x^ytdt$$ Which is convergent on a given interval, thus $g(x)$ is integrable and continuous Additionally, from the intermediate value theorem we have: $$\frac{1}{y-x}\int_x^y\ln(e+e^t)dt=\ln(e+e^c)~~~~\text{for some}~~c\in(x,y)$$ Therefore: $$ f(x,y)=\frac{1}{y+x}\cdot\ln(e+e^c) $$ Since $|x|\neq|y|$ the function does not take zero value in the denominator, thus this multiplication of functions is continuous. I am still learning and I would really appreciate if you would point out my mistakes or present a more accurate solution . Limit I have also tried spheric coordinates, but I can't see nothing helpful in this case: $$ \frac{1}{y^2-x^2} = \frac{1}{r^2(\sin^2\theta-\cos^2\theta )} $$ One of my thoughts was that I would have to find the limit of the integral and of the fraction or calculate the integral and combine with the fraction, however I decided to test the fraction first. $$ h(x,y)_{(x,y)\to(1,1)} = \frac{1}{y^2-x^2}$$ I check two sequences: $a_n=(1, \frac{1}{n})$ and $ b_n=(\frac{1}{n},1)$ with $n\to\infty$ and get: $$ \lim_{n\to -\infty} a_n=\lim_{n\to \infty}\frac{n^2}{n^2-1}= \infty\\ \lim_{n\to \infty} b_n=\lim_{n\to \infty}\frac{n^2}{1-n^2} = -\infty~~~~\text{for big}~~n$$ For this function the limit does not exist since we can show two sequences with different limits. Again, I am not sure whether my solution presents the correct way of solving. I would like to ask you for some guidance.","I have to prove the continuity and find the limit of , where and I don't know whether my way of thinking is correct and I am a little stuck on the limit. Continuity I have had a few thoughts: The multiplication of continuous functions is continuous, thus if is differentiable and the fraction is continuous, it should be continuous. To be integrable, the integral of an absolute value of the function (the derivative) should have a finite limit. My steps Since we can bound the integral by: Which is convergent on a given interval, thus is integrable and continuous Additionally, from the intermediate value theorem we have: Therefore: Since the function does not take zero value in the denominator, thus this multiplication of functions is continuous. I am still learning and I would really appreciate if you would point out my mistakes or present a more accurate solution . Limit I have also tried spheric coordinates, but I can't see nothing helpful in this case: One of my thoughts was that I would have to find the limit of the integral and of the fraction or calculate the integral and combine with the fraction, however I decided to test the fraction first. I check two sequences: and with and get: For this function the limit does not exist since we can show two sequences with different limits. Again, I am not sure whether my solution presents the correct way of solving. I would like to ask you for some guidance.","(x,y)\to(1,1) f:D\to R D=\{(x,y)\in R^2:|x|\neq|y|\}  f(x,y) = \frac{1}{y^2-x^2}\int_x^y\ln(e+e^t)dt. \int g(x)  f(x,y) = \frac{1}{y+x}\cdot\frac{1}{y-x}\int_x^y\ln(e+e^t)dt \int_x^y\ln(e+e^t)dt<\int_x^y\ln(e^t)dt=\int_x^ytdt g(x) \frac{1}{y-x}\int_x^y\ln(e+e^t)dt=\ln(e+e^c)~~~~\text{for some}~~c\in(x,y)  f(x,y)=\frac{1}{y+x}\cdot\ln(e+e^c)  |x|\neq|y|  \frac{1}{y^2-x^2} = \frac{1}{r^2(\sin^2\theta-\cos^2\theta )}   h(x,y)_{(x,y)\to(1,1)} = \frac{1}{y^2-x^2} a_n=(1, \frac{1}{n})  b_n=(\frac{1}{n},1) n\to\infty  \lim_{n\to -\infty} a_n=\lim_{n\to \infty}\frac{n^2}{n^2-1}= \infty\\ \lim_{n\to \infty} b_n=\lim_{n\to \infty}\frac{n^2}{1-n^2} = -\infty~~~~\text{for big}~~n","['real-analysis', 'integration', 'limits', 'multivariable-calculus', 'continuity']"
7,Limit within a limit,Limit within a limit,,Hey can someone please show me a step by step procedure to show that the below is possible. I can't seem to figure out how to reduce a limit within a limit to a single limit and the replacing of the variable 'a' and 'b' with 'c' (Someone mentioned iterated limits but I'm not sure how to exactly show that) $$\lim_{a\to 0}\dfrac{\lim\limits_{b\to 0}\dfrac{{f(x+a+b)-2f(x+b)+f(x)}}{b}}{a}=\lim_{c\to 0}\frac{{{f(x+2c)-2f(x+c)+f(x)}}}{c^2}$$ This idea is used when proving the Grunwald-Letkinov derivative ( https://en.wikipedia.org/wiki/GrünwaldLetnikov_derivative ) and they mentioned that this step is done by assuming that both 'a' and 'b' converge synchronously and can be justified by MVT. If possible could you explain that? and how would I use MVT to justify this.,Hey can someone please show me a step by step procedure to show that the below is possible. I can't seem to figure out how to reduce a limit within a limit to a single limit and the replacing of the variable 'a' and 'b' with 'c' (Someone mentioned iterated limits but I'm not sure how to exactly show that) This idea is used when proving the Grunwald-Letkinov derivative ( https://en.wikipedia.org/wiki/GrünwaldLetnikov_derivative ) and they mentioned that this step is done by assuming that both 'a' and 'b' converge synchronously and can be justified by MVT. If possible could you explain that? and how would I use MVT to justify this.,\lim_{a\to 0}\dfrac{\lim\limits_{b\to 0}\dfrac{{f(x+a+b)-2f(x+b)+f(x)}}{b}}{a}=\lim_{c\to 0}\frac{{{f(x+2c)-2f(x+c)+f(x)}}}{c^2},"['real-analysis', 'calculus', 'algebra-precalculus', 'limits', 'derivatives']"
8,$\lim_{x\rightarrow 0}(\ln x^{2})^{2x}$,,\lim_{x\rightarrow 0}(\ln x^{2})^{2x},"$\lim_{x\rightarrow 0}(\ln x^{2})^{2x}$ I felt the only approach to find the limits is taking log on both sides $y=\lim_{x\rightarrow 0}(\ln x^{2})^{2x}$ $\ln y=\lim_{x\rightarrow 0}2x \ln(\ln x^{2})$ Near $0$ , $\ln(\ln x^{2})$ will be $\ln(- \infty)$ . which is undefined. So shall i conclude that the limit does not exist ?","I felt the only approach to find the limits is taking log on both sides Near , will be . which is undefined. So shall i conclude that the limit does not exist ?",\lim_{x\rightarrow 0}(\ln x^{2})^{2x} y=\lim_{x\rightarrow 0}(\ln x^{2})^{2x} \ln y=\lim_{x\rightarrow 0}2x \ln(\ln x^{2}) 0 \ln(\ln x^{2}) \ln(- \infty),"['limits', 'limits-without-lhopital']"
9,Limit of ratio of a combinatorial expression?,Limit of ratio of a combinatorial expression?,,"The following function takes as arguments $n$ and $\left\{n_{i}\right\} = \left\{n_{0}, n_{1}, n_{2}\right\}$ : $$ \operatorname{f}\left(n,\left\{n_i\right\}\right) = \sum_{j = 0}^{n}\binom{n}{j}n_{0}^{\,\,\left(n - j\right)\left(n - j - 1\right)/2}\,\,\,\,\,\, n_{1}^{\left(n - j\right)\,j}\,\, n_{2}^{\,\,j\,\left(j - 1\right)/2}  $$ Is there any hope of calculating the following limit $$ \lim_{n \to \infty}\frac{\operatorname{f}\left(n,\left\{n_{i}\right\}\right)} {\operatorname{f}\left(n,\left\{m_{i}\right\}\right)} $$ in terms of $n_{0}, n_{1}, n_{2}$ and $m_{0},m_{1},m_{2}\ ?$ . The limit needs to be calculated for a given $n_{0}, n_{1}, n_{2}$ and $m_{0},m_{1},m_{2}$ i.e. they are constant.",The following function takes as arguments and : Is there any hope of calculating the following limit in terms of and . The limit needs to be calculated for a given and i.e. they are constant.,"n \left\{n_{i}\right\} = \left\{n_{0}, n_{1}, n_{2}\right\} 
\operatorname{f}\left(n,\left\{n_i\right\}\right) =
\sum_{j = 0}^{n}\binom{n}{j}n_{0}^{\,\,\left(n - j\right)\left(n - j - 1\right)/2}\,\,\,\,\,\,
n_{1}^{\left(n - j\right)\,j}\,\,
n_{2}^{\,\,j\,\left(j - 1\right)/2} 
 
\lim_{n \to \infty}\frac{\operatorname{f}\left(n,\left\{n_{i}\right\}\right)}
{\operatorname{f}\left(n,\left\{m_{i}\right\}\right)}
 n_{0}, n_{1}, n_{2} m_{0},m_{1},m_{2}\ ? n_{0}, n_{1}, n_{2} m_{0},m_{1},m_{2}","['combinatorics', 'limits', 'summation']"
10,Applying squeeze/sandwich theorem to find $\lim_\limits{n\to\infty} \frac{1}{n^2} \sum_\limits{k=n}^{5n} k$,Applying squeeze/sandwich theorem to find,\lim_\limits{n\to\infty} \frac{1}{n^2} \sum_\limits{k=n}^{5n} k,"$$\lim_{n\to\infty}\Bigl( \frac{n}{n^2}+\frac{n+1}{n^2}+\cdots+\frac{5n}{n^2}\Bigr)$$ I know how to directly compute it, but I am required to use squeeze theorem. Let $S_n$ be the sequence concerned. I tried $(4n+1)\frac{n}{n^2}<S_n<(4n+1)\frac{5n}{n^2}$ , but it gives me $4<\lim S_n<20$ . What should I do to get appropriate bounds so that squeeze theorem can be applied?","I know how to directly compute it, but I am required to use squeeze theorem. Let be the sequence concerned. I tried , but it gives me . What should I do to get appropriate bounds so that squeeze theorem can be applied?",\lim_{n\to\infty}\Bigl( \frac{n}{n^2}+\frac{n+1}{n^2}+\cdots+\frac{5n}{n^2}\Bigr) S_n (4n+1)\frac{n}{n^2}<S_n<(4n+1)\frac{5n}{n^2} 4<\lim S_n<20,"['real-analysis', 'sequences-and-series', 'limits']"
11,Limit of a fraction at infinity when all terms go to zero,Limit of a fraction at infinity when all terms go to zero,,"Suppose $Y = \sum_{j = 1}^{(n-1)!} \prod_{i = 1}^{n-1} y_{ij}$ and $Z = \sum_{j = 1}^{(n-1)!} \prod_{i = 1}^{n-1} z_{ij}$ . Also suppose $0 < x < 1$ , $-1 < y_{ij} < 1$ , and $-1 < z_{ij} < 1$ for all $i$ and $j$ , and $x \geq |y_{ij}|$ and $x \geq |z_{ij}|$ for all $i$ and $j$ , but it is such that each product has at least one $|y_{ij}| < x$ (and $|z_{ij}| < x$ ). I believe the following limit is true. $ \lim_{n \rightarrow \infty}\frac{x^{n-1} + Y}{x^{n-1} + Y + Z} = 1 $ Loosely speaking, I believe the reason is because all other terms converge to zero faster than $x^{n-1}$ , so we are left with $ \lim_{n \rightarrow \infty}\frac{x^{n-1} + Y}{x^{n-1} + Y + Z} = \frac{x^{n-1} + 0}{x^{n-1} + 0 + 0} = 1 $ . How would one formally prove this? Edit: Suppose, instead, all the $y_{ij}$ and $z_{ij}$ are weakly positive and $Y$ and $Z$ are such that $Y = \sum_{j = 1}^{n!}(-1)^j \prod_{i = 1}^{n} y_{ij}$ and $Z = \sum_{j = 1}^{n!} (-1)^j \prod_{i = 1}^{n} z_{ij}$ . Then is the above limit equal to $1$ ? If so, how could one show this?","Suppose and . Also suppose , , and for all and , and and for all and , but it is such that each product has at least one (and ). I believe the following limit is true. Loosely speaking, I believe the reason is because all other terms converge to zero faster than , so we are left with . How would one formally prove this? Edit: Suppose, instead, all the and are weakly positive and and are such that and . Then is the above limit equal to ? If so, how could one show this?","Y = \sum_{j = 1}^{(n-1)!} \prod_{i = 1}^{n-1} y_{ij} Z = \sum_{j = 1}^{(n-1)!} \prod_{i = 1}^{n-1} z_{ij} 0 < x < 1 -1 < y_{ij} < 1 -1 < z_{ij} < 1 i j x \geq |y_{ij}| x \geq |z_{ij}| i j |y_{ij}| < x |z_{ij}| < x 
\lim_{n \rightarrow \infty}\frac{x^{n-1} + Y}{x^{n-1} + Y + Z} = 1
 x^{n-1} 
\lim_{n \rightarrow \infty}\frac{x^{n-1} + Y}{x^{n-1} + Y + Z} = \frac{x^{n-1} + 0}{x^{n-1} + 0 + 0} = 1
 y_{ij} z_{ij} Y Z Y = \sum_{j = 1}^{n!}(-1)^j \prod_{i = 1}^{n} y_{ij} Z = \sum_{j = 1}^{n!} (-1)^j \prod_{i = 1}^{n} z_{ij} 1","['real-analysis', 'calculus', 'limits', 'convergence-divergence']"
12,Confusion in understanding epsilon delta definition of limit with a discontinuous function,Confusion in understanding epsilon delta definition of limit with a discontinuous function,,"To understand the definition (from Wikipedia) I have taken a function $$ f(x) = \begin{cases} 5 \quad &\text{if $x \le 2,$} \\ 5 \quad &\text{if $x=3$,} \\ 5 \quad &\text{if $x\ge 4$,} \end{cases} $$ I also see that $\forall \epsilon \gt 0, \exists \delta=2 \gt 0, \forall x \in D, 0 \lt |x-3| \lt \delta \implies |f(x) - 5| \lt \epsilon$ where $D = \mathbb{R} \setminus \{(2,3) \cup (3,4)\}$ With this, it seems the limit for $f(x)$ exists at $x=3$ even tough there is a big gap around $x=3$ . Wondering if I'm misinterpreting the definition! Is there any bound on the gap in the neighborhood of $x=c$ , that I'm not able to gather from the definition of limit.","To understand the definition (from Wikipedia) I have taken a function I also see that where With this, it seems the limit for exists at even tough there is a big gap around . Wondering if I'm misinterpreting the definition! Is there any bound on the gap in the neighborhood of , that I'm not able to gather from the definition of limit.","
f(x) = \begin{cases} 5 \quad &\text{if x \le 2,} \\ 5 \quad &\text{if x=3,} \\ 5 \quad &\text{if x\ge 4,} \end{cases}
 \forall \epsilon \gt 0, \exists \delta=2 \gt 0, \forall x \in D, 0 \lt |x-3| \lt \delta \implies |f(x) - 5| \lt \epsilon D = \mathbb{R} \setminus \{(2,3) \cup (3,4)\} f(x) x=3 x=3 x=c","['real-analysis', 'limits', 'epsilon-delta']"
13,"Show that the function is continuous on $[-1,1]$",Show that the function is continuous on,"[-1,1]","$f(x)=\mid{x}\mid$ Let $a\in(-1,1)$ $\mid f(x)-f(a)\mid=\mid\mid x\mid-\mid a\mid\mid\leq\mid x-a\mid$ Let $\epsilon>0$ be given and define $\delta=\epsilon$ , whenever $\mid x-a\mid<\delta,\space \mid f(x)-f(a)\mid<\epsilon.$ $\therefore f(x)$ is continuous on the interval $(-1,1)$ Also, $\lim_{x\to-1^+}f(x)=1=\lim_{x\to-1}f(x)$ and $\lim_{x\to1^-}f(x)=1=\lim_{x\to1}f(x)$ $\therefore f(x) $ is continuous on the right side of $-1$ and on the left side of $1$ . Thus, we conclude that $f(x)=\mid x\mid$ is continuous on the interval $[-1,1]$ . Is my proof correct?","Let Let be given and define , whenever is continuous on the interval Also, and is continuous on the right side of and on the left side of . Thus, we conclude that is continuous on the interval . Is my proof correct?","f(x)=\mid{x}\mid a\in(-1,1) \mid f(x)-f(a)\mid=\mid\mid x\mid-\mid a\mid\mid\leq\mid x-a\mid \epsilon>0 \delta=\epsilon \mid x-a\mid<\delta,\space \mid f(x)-f(a)\mid<\epsilon. \therefore f(x) (-1,1) \lim_{x\to-1^+}f(x)=1=\lim_{x\to-1}f(x) \lim_{x\to1^-}f(x)=1=\lim_{x\to1}f(x) \therefore f(x)  -1 1 f(x)=\mid x\mid [-1,1]",['limits']
14,Limit of a convex function,Limit of a convex function,,"I would need a check on the following exercise: Let $f:\mathbb{R} \rightarrow \mathbb{R}$ a convex function. Prove that $\lim_{x \rightarrow \infty} f(x)$ and $\lim_{x \rightarrow - \infty} f(x)$ exist Show that if both the limits are finite, then $f$ is constant. My attempt: i ) I know that if $f$ is convex, then $$f(t x_1 + (1-t)x_2)< t f(x_1) + (1-t) f(x_2)$$ If I fix an arbitrary $N>0$ , then I have that for $x>x_2 \colon \quad f(x)>N$ , thanks to the convexity, therefore this proves the limit to $+ \infty$ is $+\infty$ . The same argument applies to $\lim_{x \rightarrow -\infty}f(x)$ : it suffices to note that for $x<x_1 \colon \quad f(x)>N$ . ii) Graphically it's obvious, but I have some problem in make it formal. If the limit is finite, say $L$ , then for every $\varepsilon >0$ there exists an $M(\varepsilon)$ such that for $$x>M(\varepsilon) \colon \quad |f(x)-L|\leq \varepsilon$$ Assume $f (x) \ne c$ . By definition of convexity, it has to hold (for $t \in [0,1]$ ) $$t f(M)+(1-t)f(M+1) \leq f(t M + (1-t)(M+1))$$ Now, by definition of limit, $f(M)$ and $f(M+1)$ are less than $L-\varepsilon$ . Also, the argument in the rhs of the inequality can be simplified: $$L-\varepsilon <t f(M)+(1-t)f(M+1) \leq f(t M + (1-t)(M+1)) = f(M-t)$$ Therefore $$L-\varepsilon < f(M-t)$$ , which is a contradiction because $M-t<M$ and hence it can be greater than $L-\varepsilon$ . So $f$ has to be equal to $c$ . Indeed in this case, it is still (trivially) convex, and the limits are of course finite.","I would need a check on the following exercise: Let a convex function. Prove that and exist Show that if both the limits are finite, then is constant. My attempt: i ) I know that if is convex, then If I fix an arbitrary , then I have that for , thanks to the convexity, therefore this proves the limit to is . The same argument applies to : it suffices to note that for . ii) Graphically it's obvious, but I have some problem in make it formal. If the limit is finite, say , then for every there exists an such that for Assume . By definition of convexity, it has to hold (for ) Now, by definition of limit, and are less than . Also, the argument in the rhs of the inequality can be simplified: Therefore , which is a contradiction because and hence it can be greater than . So has to be equal to . Indeed in this case, it is still (trivially) convex, and the limits are of course finite.","f:\mathbb{R} \rightarrow \mathbb{R} \lim_{x \rightarrow \infty} f(x) \lim_{x \rightarrow - \infty} f(x) f f f(t x_1 + (1-t)x_2)< t f(x_1) + (1-t) f(x_2) N>0 x>x_2 \colon \quad f(x)>N + \infty +\infty \lim_{x \rightarrow -\infty}f(x) x<x_1 \colon \quad f(x)>N L \varepsilon >0 M(\varepsilon) x>M(\varepsilon) \colon \quad |f(x)-L|\leq \varepsilon f (x) \ne c t \in [0,1] t f(M)+(1-t)f(M+1) \leq f(t M + (1-t)(M+1)) f(M) f(M+1) L-\varepsilon L-\varepsilon <t f(M)+(1-t)f(M+1) \leq f(t M + (1-t)(M+1)) = f(M-t) L-\varepsilon < f(M-t) M-t<M L-\varepsilon f c","['real-analysis', 'limits', 'proof-writing', 'solution-verification', 'convexity-inequality']"
15,What can we say about this function when it approaches zero from above?,What can we say about this function when it approaches zero from above?,,"Define the function $f:(0,\infty)\rightarrow \mathbb{R}$ by $$f(x)=\sum\limits_{n=1}^\infty\frac{1}{2^n}\cdot \frac{1}{\left(\frac{1}{n}-x\right)^2}I_{\{x: x \ne \frac{1}{n}\}}(x)$$ . What can we say about this function as it approaches zero? I think it is pretty clear that $\limsup\limits_{x \rightarrow 0^+}f(x)=\infty$ , this is because no matter how close we are to zero, we can choose an $n^*$ closer to zero, and then approach it. But what about $\liminf\limits_{x \rightarrow 0^+}f(x)$ ?, is it possible to see what this is? As we get closer to zero, the tail in the sum has elements of $(1/n-x)^2$ that becomes very small, but $2^n$ becomes very big, so they might cancel them out?","Define the function by . What can we say about this function as it approaches zero? I think it is pretty clear that , this is because no matter how close we are to zero, we can choose an closer to zero, and then approach it. But what about ?, is it possible to see what this is? As we get closer to zero, the tail in the sum has elements of that becomes very small, but becomes very big, so they might cancel them out?","f:(0,\infty)\rightarrow \mathbb{R} f(x)=\sum\limits_{n=1}^\infty\frac{1}{2^n}\cdot \frac{1}{\left(\frac{1}{n}-x\right)^2}I_{\{x: x \ne \frac{1}{n}\}}(x) \limsup\limits_{x \rightarrow 0^+}f(x)=\infty n^* \liminf\limits_{x \rightarrow 0^+}f(x) (1/n-x)^2 2^n","['calculus', 'limits', 'limsup-and-liminf']"
16,Two different definitions of limits.,Two different definitions of limits.,,"There are two definitions of limits that I know of. Definition $(1)$ : Let $X$ be  subset of $\mathbf{R}^n$ , and $x_0$ a point in $\overline{X}$ . A function $f\colon X\rightarrow\mathbf{R}^m$ has the limit $a$ at $x_0$ if for all $\varepsilon>0$ , there exists $\delta>0$ such that for all $x\in X$ , we have $$ |x-x_0|<\delta \implies |f(x)-a|<\varepsilon.$$ Definition $(2)$ is the exact same, except with $$ 0<|x-x_0|<\delta \implies |f(x)-a|<\varepsilon.$$ I am not exactly sure how to reconcile these two definitions. With the first definition, limits work well with composition. It also has the interesting property, that if you take a limit of a function like $\lim_{x\rightarrow 0}\text{sgn}(x)$ , the limit does not exist since $\text{sgn}(0)=0$ , as of course $|0-0|<\delta$ for any $\delta>0$ . Thus, it seems like for $x_0\in X$ , the limit in $(1)$ exists at $x_0$ if and only if $f$ is continuous at $x_0$ (is this correct?). In $(2)$ however, the limit exists at a jump discontinuity since we are disregarding the point $x_0$ , and there are plenty of examples where the limit exists at $x$ while the function $f$ is not continuous at $x$ . The $\text{sgn}$ function mentioned previously fits the bill. Another interesting distinction that I thought of is when taking limits of both sides of an equation. Suppose that the domains of both $f$ and $g$ is $X$ and $x_0\in\overline{X}\setminus X$ . Then under definitions $(1)$ and $(2)$ , if $f(x)=g(x)$ for all $x$ in a neighborhood of $x_0$ , we have $$\lim_{x\rightarrow x_0}f(x)=\lim_{x\rightarrow x_0}g(x),$$ as $x_0\not\in X$ , so the distinction between the two definitions does not show up. Of course, this is provided the limit exists. However, if we do the same example with $x_0\in X$ , where $f(x)=g(x)$ for all $x$ in a neighborhood of $x_0$ excluding $x_0$ , under definition $(2)$ we again have the same result, but under definition $(1)$ a jump discontinuity at $x_0$ could imply that $$\lim_{x\rightarrow x_0}f(x)\neq\lim_{x\rightarrow x_0}g(x).$$ This seems somewhat significant. Is there a way to fit these together, or in general, what is going on? And how does the distinction get ""erased""? I don't think many books dwell on this at all.","There are two definitions of limits that I know of. Definition : Let be  subset of , and a point in . A function has the limit at if for all , there exists such that for all , we have Definition is the exact same, except with I am not exactly sure how to reconcile these two definitions. With the first definition, limits work well with composition. It also has the interesting property, that if you take a limit of a function like , the limit does not exist since , as of course for any . Thus, it seems like for , the limit in exists at if and only if is continuous at (is this correct?). In however, the limit exists at a jump discontinuity since we are disregarding the point , and there are plenty of examples where the limit exists at while the function is not continuous at . The function mentioned previously fits the bill. Another interesting distinction that I thought of is when taking limits of both sides of an equation. Suppose that the domains of both and is and . Then under definitions and , if for all in a neighborhood of , we have as , so the distinction between the two definitions does not show up. Of course, this is provided the limit exists. However, if we do the same example with , where for all in a neighborhood of excluding , under definition we again have the same result, but under definition a jump discontinuity at could imply that This seems somewhat significant. Is there a way to fit these together, or in general, what is going on? And how does the distinction get ""erased""? I don't think many books dwell on this at all.","(1) X \mathbf{R}^n x_0 \overline{X} f\colon X\rightarrow\mathbf{R}^m a x_0 \varepsilon>0 \delta>0 x\in X  |x-x_0|<\delta \implies |f(x)-a|<\varepsilon. (2)  0<|x-x_0|<\delta \implies |f(x)-a|<\varepsilon. \lim_{x\rightarrow 0}\text{sgn}(x) \text{sgn}(0)=0 |0-0|<\delta \delta>0 x_0\in X (1) x_0 f x_0 (2) x_0 x f x \text{sgn} f g X x_0\in\overline{X}\setminus X (1) (2) f(x)=g(x) x x_0 \lim_{x\rightarrow x_0}f(x)=\lim_{x\rightarrow x_0}g(x), x_0\not\in X x_0\in X f(x)=g(x) x x_0 x_0 (2) (1) x_0 \lim_{x\rightarrow x_0}f(x)\neq\lim_{x\rightarrow x_0}g(x).","['real-analysis', 'limits']"
17,"Prove that limit of function does not exist, if and only if sequence $f(s_n)$ is not convergent.","Prove that limit of function does not exist, if and only if sequence  is not convergent.",f(s_n),"I don't understand the hint (in bold) to prove this problem. Problem : Let $f: D \rightarrow  \mathbb{R}$ and let $c$ be an accumulation point of D. Then the following are equivalent: (a) $f$ does not have a limit at $c$ . (b) There exists a sequence $(s_n)$ in $D$ with each $s_n \ne c$ such that $(s_n)$ converges to $c$ , but $f((s_n))$ is not convergent in $\mathbb{R}$ . Hint : To prove that $(a) \Rightarrow (b)$ , suppose that (b) is false. Let $s_n$ be a sequence with $s_n \rightarrow c$ . Before we can use Theorem 20.8, we must show that given any sequence $(t_n)$ in $D$ with $t_n \Rightarrow c$ , we have limit $f(t_n)=L$ . We only know from the negation of (b) that $(f(t_n))$ is convergent. To see that $\lim f(t_n) = L$ , consider the sequence $(u_n) = (s_1, t_1, s_2, t_2...)$ and note that $(f(s_n))$ and $(f(t_n))$ are both subsequences of $(f(u_n))$ . My question: The hint wants for any sequence $(t_n)$ . But, how does $(u_n)$ help showing $(t_n)$ is any sequence? What do I need $(u_n)$ for? The proof is here under Theorem 2.","I don't understand the hint (in bold) to prove this problem. Problem : Let and let be an accumulation point of D. Then the following are equivalent: (a) does not have a limit at . (b) There exists a sequence in with each such that converges to , but is not convergent in . Hint : To prove that , suppose that (b) is false. Let be a sequence with . Before we can use Theorem 20.8, we must show that given any sequence in with , we have limit . We only know from the negation of (b) that is convergent. To see that , consider the sequence and note that and are both subsequences of . My question: The hint wants for any sequence . But, how does help showing is any sequence? What do I need for? The proof is here under Theorem 2.","f: D \rightarrow  \mathbb{R} c f c (s_n) D s_n \ne c (s_n) c f((s_n)) \mathbb{R} (a) \Rightarrow (b) s_n s_n \rightarrow c (t_n) D t_n \Rightarrow c f(t_n)=L (f(t_n)) \lim f(t_n) = L (u_n) = (s_1, t_1, s_2, t_2...) (f(s_n)) (f(t_n)) (f(u_n)) (t_n) (u_n) (t_n) (u_n)","['real-analysis', 'limits', 'proof-writing', 'sequence-of-function']"
18,Is there a function that grows slower than the factorial function and whose derivative grows faster than the function itself? [closed],Is there a function that grows slower than the factorial function and whose derivative grows faster than the function itself? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Is there a function that grows slower than the factorial function and whose derivative grows faster than the function itself? In symbols, is there a function $f(x)$ that satisfies $f(x) = o(x!)$ and $\lim_{x \to \infty} f(x)/f'(x) = 0$ ?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Is there a function that grows slower than the factorial function and whose derivative grows faster than the function itself? In symbols, is there a function that satisfies and ?",f(x) f(x) = o(x!) \lim_{x \to \infty} f(x)/f'(x) = 0,"['calculus', 'limits', 'functions', 'derivatives', 'factorial']"
19,Find limit of volume of n dimensional box in different unit.,Find limit of volume of n dimensional box in different unit.,,Suppose we have a n dimensional box with length in each dimension = $50$ $cms$ . If we find the volumne where n tends to infinity we get its magnitude as infinity. But if we convert it to meters before finding the volume we get the magnitude to be $0$ . So does the box in the limiting case have any volume or not?,Suppose we have a n dimensional box with length in each dimension = . If we find the volumne where n tends to infinity we get its magnitude as infinity. But if we convert it to meters before finding the volume we get the magnitude to be . So does the box in the limiting case have any volume or not?,50 cms 0,"['sequences-and-series', 'limits']"
20,Computing a limit of a piece-wise function.,Computing a limit of a piece-wise function.,,"I'm a student studying maths, I'm taking a real analysis course and I'm looking through some old exams questions. I've arrived at a number of questions that ask me to take the limit of a number of piece-wise functions as $x \rightarrow 2$ . Here is the first one. $$f(x)= \begin{cases}        \frac{x^2-4}{x-2} & \text{if }x \neq 2  \\       6 & \text{if }x = 2 \\     \end{cases} $$ I know if I do some algebra we get... $$\frac{x^2-4}{x-2} = \frac{(x+2)(x-2)}{x-2}= x+2$$ So we get. $$f(x)= \begin{cases}        x+2 & \text{if }x \neq 2  \\       6 & \text{if }x = 2 \\     \end{cases} $$ so if we take the limit $$\lim_{x\to2}f(x) = ???$$ I'm not sure weather the answer is 4 or 6. They both seem intuitively correct so perhaps the answer might even be neither. I'm very interested to know! Thanks for your time!","I'm a student studying maths, I'm taking a real analysis course and I'm looking through some old exams questions. I've arrived at a number of questions that ask me to take the limit of a number of piece-wise functions as . Here is the first one. I know if I do some algebra we get... So we get. so if we take the limit I'm not sure weather the answer is 4 or 6. They both seem intuitively correct so perhaps the answer might even be neither. I'm very interested to know! Thanks for your time!","x \rightarrow 2 f(x)= \begin{cases} 
      \frac{x^2-4}{x-2} & \text{if }x \neq 2  \\
      6 & \text{if }x = 2 \\ 
   \end{cases}
 \frac{x^2-4}{x-2} = \frac{(x+2)(x-2)}{x-2}= x+2 f(x)= \begin{cases} 
      x+2 & \text{if }x \neq 2  \\
      6 & \text{if }x = 2 \\ 
   \end{cases}
 \lim_{x\to2}f(x) = ???","['real-analysis', 'limits', 'piecewise-continuity']"
21,Limit Laws for a Partial Sum of an Infinite Series [duplicate],Limit Laws for a Partial Sum of an Infinite Series [duplicate],,"This question already has answers here : Can't we compute $\lim_{n \to \infty}\frac{1+ \cdots +n}{n^2}=\lim_{n\to\infty}\frac{1}{n^2}+\cdots +\lim_{n\to\infty}\frac{n}{n^2}=0$? (5 answers) Closed 4 years ago . I came across a question in my High School Calculus textbook: Find $\lim_{n \to \infty} a_n$ where $a_n = \frac{1}{n^2} + \frac{2}{n^2} + \frac{3}{n^2} + ... + \frac{n}{n^2}$ My approach was to simply distribute the limit to each term in the partial sum. I believed this would be legitimate according to the limit laws. Once I evaluated the individual limits, the limit of the partial sum came out to equal $0$ : $$\begin{aligned} \lim_{n \to \infty} a_n &= \lim_{n \to \infty}(\frac{1}{n^2} + \frac{2}{n^2} + \frac{3}{n^2} + ... + \frac{n}{n^2}) \\ &= \lim_{n \to \infty}(\frac{1}{n^2}) + \lim_{n \to \infty}(\frac{2}{n^2}) + \lim_{n \to \infty}(\frac{3}{n^2}) + ... + \lim_{n \to \infty}(\frac{n}{n^2}) \\ &= 0+0+0+...+0 \\ &=0 \end{aligned}$$ However, the textbook first combined the fractions and simplified it, only to find that the limit of the partial sum equals $1/2$ : $$\begin{aligned} a_n &= \frac{1+2+3+...+n}{n^2} \\ &= \frac{\frac{n(n+1)}{2}}{n^2} \\ &= \frac{1}{2}(\frac{n+1}{n}) \\ &= \frac{1}{2}(1+\frac{1}{n}) \end{aligned}$$ $$\therefore \lim_{n \to \infty}a_n = \frac{1}{2}$$ Both solutions seemed to be legitimate. However, after pondering on the question for a bit, I starting to think that maybe the limit laws don't apply to partial sums... As $n\rightarrow\infty$ , the number of terms also approaches infinity, so then distributing the limit an ""infinite"" number of times might be wrong, considering how weird infinity is. But I'm not so convinced about that. Is there some sort of restriction on how the limit can be distributed? Is there something I'm missing that completely invalidates my approach to the problem?","This question already has answers here : Can't we compute $\lim_{n \to \infty}\frac{1+ \cdots +n}{n^2}=\lim_{n\to\infty}\frac{1}{n^2}+\cdots +\lim_{n\to\infty}\frac{n}{n^2}=0$? (5 answers) Closed 4 years ago . I came across a question in my High School Calculus textbook: Find where My approach was to simply distribute the limit to each term in the partial sum. I believed this would be legitimate according to the limit laws. Once I evaluated the individual limits, the limit of the partial sum came out to equal : However, the textbook first combined the fractions and simplified it, only to find that the limit of the partial sum equals : Both solutions seemed to be legitimate. However, after pondering on the question for a bit, I starting to think that maybe the limit laws don't apply to partial sums... As , the number of terms also approaches infinity, so then distributing the limit an ""infinite"" number of times might be wrong, considering how weird infinity is. But I'm not so convinced about that. Is there some sort of restriction on how the limit can be distributed? Is there something I'm missing that completely invalidates my approach to the problem?","\lim_{n \to \infty} a_n a_n = \frac{1}{n^2} + \frac{2}{n^2} + \frac{3}{n^2} + ... + \frac{n}{n^2} 0 \begin{aligned}
\lim_{n \to \infty} a_n &= \lim_{n \to \infty}(\frac{1}{n^2} + \frac{2}{n^2} + \frac{3}{n^2} + ... + \frac{n}{n^2})
\\
&= \lim_{n \to \infty}(\frac{1}{n^2}) + \lim_{n \to \infty}(\frac{2}{n^2}) + \lim_{n \to \infty}(\frac{3}{n^2}) + ... + \lim_{n \to \infty}(\frac{n}{n^2})
\\
&= 0+0+0+...+0
\\
&=0
\end{aligned} 1/2 \begin{aligned}
a_n &= \frac{1+2+3+...+n}{n^2}
\\
&= \frac{\frac{n(n+1)}{2}}{n^2}
\\
&= \frac{1}{2}(\frac{n+1}{n})
\\
&= \frac{1}{2}(1+\frac{1}{n})
\end{aligned} \therefore \lim_{n \to \infty}a_n = \frac{1}{2} n\rightarrow\infty","['calculus', 'sequences-and-series', 'limits']"
22,"Differentiability properties of $\psi(x)\cos(\phi(x)),\,\, \psi(x)\sin(\phi(x))$ at $x=0$",Differentiability properties of  at,"\psi(x)\cos(\phi(x)),\,\, \psi(x)\sin(\phi(x)) x=0","Let $\psi:[0,\infty) \to [0,\infty)$ be a  smooth strictly increasing function  satisfying $\psi(0)=0$ , $\psi'(0)>0$ and let $\phi:(0,\infty) \to \mathbb R$ be smooth. Suppose that $\lim_{x \to 0^+}\phi'(x)\psi(x)=0$ . Define $$f_1(x)=\psi(x)\cos(\phi(x)), \, \,\,\,f_2(x)=\psi(x)\sin(\phi(x))$$ on $(0,\infty)$ , and extend them continuously to zero by setting $f_i(x)=0$ . Can the following properties hold simultaneously? $\,f_i$ are infinitely (right) differentiable at $x=0$ . $\,$ All the (right) derivatives of $f_i$ of even order vanish at zero. At least one of the $\,f_i'(0)$ is non-zero. Comment: The assumptions imply that $\alpha:=\lim_{x \to 0^+} \phi(x)$ exists. Indeed, $$ \frac{f_1(x)-f_1(0)}{x}=\frac{\psi(x)-\psi(0)}{x}\cos(\phi(x))\Rightarrow \\ \cos(\phi(x))=\frac{f_1(x)-f_1(0)}{x} \frac{1}{\frac{\psi(x)-\psi(0)}{x}} \Rightarrow \\  \lim_{x \to 0^+} \cos(\phi(x))=\frac{f_1'(0)}{\psi'(0)}, $$ and similarly $\lim_{x \to 0^+} \sin(\phi(x))=\frac{f_2'(0)}{\psi'(0)}$ . So, both $\lim_{x \to 0^+} \cos(\phi(x)), \lim_{x \to 0^+} \sin(\phi(x))$ exist, and hence so does $\lim_{x \to 0^+} \phi(x)$ . Now, a direct calculation shows that $$ f_1'(x)=\begin{cases} \psi'(x)\cos(\phi(x))-\psi(x)\phi'(x)\sin(\phi(x)) & \text{if $x > 0$} \\ \psi'(0)\cdot  \cos(\alpha)  & \text{if $x=0$}\end{cases}$$ Now, I am not sure how to proceed from here. for $x>0$ , we have $$  f_1''(x)=\psi''(x)\cos(\phi(x))-2\psi'(x)\phi'(x)\sin(\phi(x))-\psi(x)\phi''(x)\sin(\phi(x))-\psi(x)(\phi'(x))^2\cos(\phi(x)),$$ but since we don't know whether $\phi'(x),\phi''(x)$ have limits when $x \to 0$ , it is not clear to me what to do next.","Let be a  smooth strictly increasing function  satisfying , and let be smooth. Suppose that . Define on , and extend them continuously to zero by setting . Can the following properties hold simultaneously? are infinitely (right) differentiable at . All the (right) derivatives of of even order vanish at zero. At least one of the is non-zero. Comment: The assumptions imply that exists. Indeed, and similarly . So, both exist, and hence so does . Now, a direct calculation shows that Now, I am not sure how to proceed from here. for , we have but since we don't know whether have limits when , it is not clear to me what to do next.","\psi:[0,\infty) \to [0,\infty) \psi(0)=0 \psi'(0)>0 \phi:(0,\infty) \to \mathbb R \lim_{x \to 0^+}\phi'(x)\psi(x)=0 f_1(x)=\psi(x)\cos(\phi(x)), \, \,\,\,f_2(x)=\psi(x)\sin(\phi(x)) (0,\infty) f_i(x)=0 \,f_i x=0 \, f_i \,f_i'(0) \alpha:=\lim_{x \to 0^+} \phi(x)  \frac{f_1(x)-f_1(0)}{x}=\frac{\psi(x)-\psi(0)}{x}\cos(\phi(x))\Rightarrow \\ \cos(\phi(x))=\frac{f_1(x)-f_1(0)}{x} \frac{1}{\frac{\psi(x)-\psi(0)}{x}} \Rightarrow \\ 
\lim_{x \to 0^+} \cos(\phi(x))=\frac{f_1'(0)}{\psi'(0)},  \lim_{x \to 0^+} \sin(\phi(x))=\frac{f_2'(0)}{\psi'(0)} \lim_{x \to 0^+} \cos(\phi(x)), \lim_{x \to 0^+} \sin(\phi(x)) \lim_{x \to 0^+} \phi(x)  f_1'(x)=\begin{cases} \psi'(x)\cos(\phi(x))-\psi(x)\phi'(x)\sin(\phi(x)) & \text{if x > 0} \\ \psi'(0)\cdot  \cos(\alpha)  & \text{if x=0}\end{cases} x>0   f_1''(x)=\psi''(x)\cos(\phi(x))-2\psi'(x)\phi'(x)\sin(\phi(x))-\psi(x)\phi''(x)\sin(\phi(x))-\psi(x)(\phi'(x))^2\cos(\phi(x)), \phi'(x),\phi''(x) x \to 0","['real-analysis', 'calculus', 'limits', 'derivatives', 'singularity']"
23,Two different answers for limit at zero,Two different answers for limit at zero,,"The following problem arises when calculating the result of Theorem 2 (part (4)) in Takács (1962) Introduction to the Theory of Queues (page 211). Calculate $$\lim_{s\to 0^{+}} \left[     \frac{ 2\bigl( \Pi_0'(s) \bigr)^2 }{ \bigl( \Pi_0(s) \bigr)^3 }     -     \frac{ \Pi_0''(s)}{ \bigl( \Pi_0(s) \bigr)^2 } \right] $$ given $$\lim_{s \to 0^{+}} s^{n+1} \Pi_0^{(n)}(s) = (-1)^n n!\,\mathrm e^{-\lambda\alpha}$$ for all non-negative integers $n$ . Remark : The function $\Pi_0(s)$ is the Laplace transform of $$P_0(t) = \exp\left( -\lambda\int_{0}^{t}[1-H(x)]\,\mathrm dx \right)$$ for a cumulative distribution function $H(x)$ on the non-negative reals, and $\alpha$ is the mean of $H(x)$ . My question : I can obtain two different answers for the limit, the second being the negative of the first. What did I do wrong? Solution 1 (obtains the same result as Takács, 1962) \begin{align*}     \frac{ 2\bigl( \Pi_0'(s) \bigr)^2 }{ \bigl( \Pi_0(s) \bigr)^3 }     -     \frac{ \Pi_0''(s) }{ \bigl( \Pi_0(s) \bigr)^2 } =     \frac{ 2 \Pi_0(s) \bigl( s^2 \Pi_0'(s) \bigr)^2 }{ \bigl( s \Pi_0(s) \bigr)^4 }     -     \frac{ s^3 \Pi_0''(s) }{ s \bigl( s \Pi_0(s) \bigr)^2 } \end{align*} so \begin{align*} \lim_{s\to 0^{+}} \left[     \frac{ 2\bigl( \Pi_0'(s) \bigr)^2 }{ \bigl( \Pi_0(s) \bigr)^3 }     -     \frac{ \Pi_0''(s) }{ \bigl( \Pi_0(s) \bigr)^2 } \right] &= \lim_{s\to 0^{+}} \left[     \frac{ 2 \Pi_0(s) \bigl( -e^{-\lambda\alpha} \bigr)^2 }{ \bigl( e^{-\lambda\alpha} \bigr)^4 }     -     \frac{ 2e^{-\lambda\alpha} }{ s \bigl( e^{-\lambda\alpha} \bigr)^2 } \right] \\ &= \lim_{s\to 0^{+}} 2e^{2\lambda\alpha} \left[     \Pi_0(s)     -     \frac{ e^{-\lambda\alpha} }{ s } \right] \end{align*} Solution 2 \begin{align*}     \frac{ 2\bigl( \Pi_0'(s) \bigr)^2 }{ \bigl( \Pi_0(s) \bigr)^3 }     -     \frac{ \Pi_0''(s) }{ \bigl( \Pi_0(s) \bigr)^2 } =     \frac{ 2\bigl( s^2 \Pi_0'(s) \bigr)^2 }{ s \bigl( s \Pi_0(s) \bigr)^3 }     -     \frac{ \Pi_0(s) s^3 \Pi_0''(s) }{ \bigl( s \Pi_0(s) \bigr)^3 } \end{align*} so \begin{align*} \lim_{s\to 0^{+}} \left[     \frac{ 2\bigl( \Pi_0'(s) \bigr)^2 }{ \bigl( \Pi_0(s) \bigr)^3 }     -     \frac{ \Pi_0''(s) }{ \bigl( \Pi_0(s) \bigr)^2 } \right] &= \lim_{s\to 0^{+}} \left[     \frac{ 2\bigl( -e^{-\lambda\alpha} \bigr)^2 }{ s \bigl( e^{-\lambda\alpha} \bigr)^3 }     -     \frac{ 2 \Pi_0(s) e^{-\lambda\alpha} }{ \bigl( e^{-\lambda\alpha} \bigr)^3 } \right] \\ &= \lim_{s\to 0^{+}} 2e^{2\lambda\alpha} \left[     \frac{ e^{-\lambda\alpha} }{ s }     -     \Pi_0(s) \right] \end{align*} Assuming that I haven't done something silly with the algebra, my guess is that has to do with even versus odd powers of $s$ vis-a-vis $-s$ . In the first answer, after multiplying by powers of $s$ , the denominators are even powers ( $4$ and $2$ ). But in the second answer the denominators are odd powers ( $3$ and $3$ ). So in some sense, in the first answer I could replace $s$ with $-s$ and everything is the same, but in the second answer I have a "" $-$ "" left over. Many thanks in advance.","The following problem arises when calculating the result of Theorem 2 (part (4)) in Takács (1962) Introduction to the Theory of Queues (page 211). Calculate given for all non-negative integers . Remark : The function is the Laplace transform of for a cumulative distribution function on the non-negative reals, and is the mean of . My question : I can obtain two different answers for the limit, the second being the negative of the first. What did I do wrong? Solution 1 (obtains the same result as Takács, 1962) so Solution 2 so Assuming that I haven't done something silly with the algebra, my guess is that has to do with even versus odd powers of vis-a-vis . In the first answer, after multiplying by powers of , the denominators are even powers ( and ). But in the second answer the denominators are odd powers ( and ). So in some sense, in the first answer I could replace with and everything is the same, but in the second answer I have a "" "" left over. Many thanks in advance.","\lim_{s\to 0^{+}} \left[
    \frac{ 2\bigl( \Pi_0'(s) \bigr)^2 }{ \bigl( \Pi_0(s) \bigr)^3 }
    -
    \frac{ \Pi_0''(s)}{ \bigl( \Pi_0(s) \bigr)^2 }
\right]
 \lim_{s \to 0^{+}} s^{n+1} \Pi_0^{(n)}(s) = (-1)^n n!\,\mathrm e^{-\lambda\alpha} n \Pi_0(s) P_0(t) = \exp\left( -\lambda\int_{0}^{t}[1-H(x)]\,\mathrm dx \right) H(x) \alpha H(x) \begin{align*}
    \frac{ 2\bigl( \Pi_0'(s) \bigr)^2 }{ \bigl( \Pi_0(s) \bigr)^3 }
    -
    \frac{ \Pi_0''(s) }{ \bigl( \Pi_0(s) \bigr)^2 }
=
    \frac{ 2 \Pi_0(s) \bigl( s^2 \Pi_0'(s) \bigr)^2 }{ \bigl( s \Pi_0(s) \bigr)^4 }
    -
    \frac{ s^3 \Pi_0''(s) }{ s \bigl( s \Pi_0(s) \bigr)^2 }
\end{align*} \begin{align*}
\lim_{s\to 0^{+}} \left[
    \frac{ 2\bigl( \Pi_0'(s) \bigr)^2 }{ \bigl( \Pi_0(s) \bigr)^3 }
    -
    \frac{ \Pi_0''(s) }{ \bigl( \Pi_0(s) \bigr)^2 }
\right]
&=
\lim_{s\to 0^{+}} \left[
    \frac{ 2 \Pi_0(s) \bigl( -e^{-\lambda\alpha} \bigr)^2 }{ \bigl( e^{-\lambda\alpha} \bigr)^4 }
    -
    \frac{ 2e^{-\lambda\alpha} }{ s \bigl( e^{-\lambda\alpha} \bigr)^2 }
\right]
\\ &=
\lim_{s\to 0^{+}} 2e^{2\lambda\alpha} \left[
    \Pi_0(s)
    -
    \frac{ e^{-\lambda\alpha} }{ s }
\right]
\end{align*} \begin{align*}
    \frac{ 2\bigl( \Pi_0'(s) \bigr)^2 }{ \bigl( \Pi_0(s) \bigr)^3 }
    -
    \frac{ \Pi_0''(s) }{ \bigl( \Pi_0(s) \bigr)^2 }
=
    \frac{ 2\bigl( s^2 \Pi_0'(s) \bigr)^2 }{ s \bigl( s \Pi_0(s) \bigr)^3 }
    -
    \frac{ \Pi_0(s) s^3 \Pi_0''(s) }{ \bigl( s \Pi_0(s) \bigr)^3 }
\end{align*} \begin{align*}
\lim_{s\to 0^{+}} \left[
    \frac{ 2\bigl( \Pi_0'(s) \bigr)^2 }{ \bigl( \Pi_0(s) \bigr)^3 }
    -
    \frac{ \Pi_0''(s) }{ \bigl( \Pi_0(s) \bigr)^2 }
\right]
&=
\lim_{s\to 0^{+}} \left[
    \frac{ 2\bigl( -e^{-\lambda\alpha} \bigr)^2 }{ s \bigl( e^{-\lambda\alpha} \bigr)^3 }
    -
    \frac{ 2 \Pi_0(s) e^{-\lambda\alpha} }{ \bigl( e^{-\lambda\alpha} \bigr)^3 }
\right]
\\ &=
\lim_{s\to 0^{+}} 2e^{2\lambda\alpha} \left[
    \frac{ e^{-\lambda\alpha} }{ s }
    -
    \Pi_0(s)
\right]
\end{align*} s -s s 4 2 3 3 s -s -","['real-analysis', 'limits']"
24,Limits in two variable with two parameters - Is there any strategy?,Limits in two variable with two parameters - Is there any strategy?,,"I have this exercise: ""Find for which positive, real $a$ and $b$ , each of these limits exist"". $$ \lim_{(x,y) \to (0,0)}\frac{|x|^a\cdot|y|^b}{x^{88}+y^{12}}\qquad\lim_{x^2+y^2 \to +\infty}\frac{|x|^a\cdot|y|^b}{x^{88}+y^{12}} $$ In ""1-D"" I used to eliminate one of the parameters and solve for the other one, or I was on search of indecision form to apply De l'Hospital… How can I move in ""2-D"" case? Is there any ""common"" strategy to follow?","I have this exercise: ""Find for which positive, real and , each of these limits exist"". In ""1-D"" I used to eliminate one of the parameters and solve for the other one, or I was on search of indecision form to apply De l'Hospital… How can I move in ""2-D"" case? Is there any ""common"" strategy to follow?","a b 
\lim_{(x,y) \to (0,0)}\frac{|x|^a\cdot|y|^b}{x^{88}+y^{12}}\qquad\lim_{x^2+y^2 \to +\infty}\frac{|x|^a\cdot|y|^b}{x^{88}+y^{12}}
","['real-analysis', 'calculus', 'limits', 'analysis', 'multivariable-calculus']"
25,"Prove that $\lim\limits_{x\to\infty}xf(x)=0$ where $f$ is an integrable function over $(0,\infty)$.",Prove that  where  is an integrable function over .,"\lim\limits_{x\to\infty}xf(x)=0 f (0,\infty)","Please check if the solution to the following problem is correct or not. If there is any flaw then please help me to correct it. Problem: Let $f:(0,\infty)\to(0,\infty)$ be a decreasing function, satisfying $$I=\int\limits_{0}^{\infty}f(x) \, \mathrm{d} x<\infty.$$ Prove that $\lim\limits_{x\to\infty}xf(x)=0$ . Solution: Let us assume that $f$ be a decreasing function such that $\lim\limits_{x\to\infty}xf(x)=l \, ,l>0$ . Since $xf(x)>0$ for $x>0$ , $\lim\limits_{x\to\infty}xf(x)$ cannot be negative. Case 1. Let $l=\infty$ . Then given any $M>0 \, \exists a=\frac{M}{f(a)}>0$ such that for any $x>a$ , $xf(x)>M$ . Then $$g(t)=\int\limits_{0}^{t}f(x) \, \mathrm{d} x=\int\limits_{a}^{t}f(x) \, \mathrm{d} x+\int\limits_{0}^{a}f(x) \, \mathrm{d} x>\int\limits_{0}^{a}f(a)\, \mathrm{d} x=af(a)=M.$$ Hence given any $M>0$ $\exists a=a(M)>0$ such that for any $t>a$ , $g(t)>M$ . Hence $\lim\limits_{t\to\infty}g(t)=\infty$ . Thus $I=\infty$ . Case 2. Let $0<l<\infty$ . Then given any $\epsilon>0 \, \exists \delta=\frac{\epsilon}{f(\delta)}>0$ such that for any $x>a$ , $|xf(x)-l|<\epsilon$ . Then $$g(t)=\int\limits_{0}^{t}f(x) \, \mathrm{d} x=\int\limits_{\delta}^{t}f(x) \, \mathrm{d} x+\int\limits_{0}^{\delta}f(x) \, \mathrm{d} x>\int\limits_{0}^{\delta}f(\delta) \, \mathrm{d} x=\delta f(\delta)=\epsilon.$$ Hence given any $\epsilon>0$ $\exists \delta=\delta(\epsilon)>0$ such that for any $t>\delta$ , $g(t)>\epsilon$ . Hence $\lim\limits_{t\to\infty}g(t)=\infty$ . Thus $I=\infty$ . Hence in both the cases we get a contradiction to the fact that $I<\infty$ . Thus $\lim\limits_{x\to\infty}xf(x)=0$ . Thank you.","Please check if the solution to the following problem is correct or not. If there is any flaw then please help me to correct it. Problem: Let be a decreasing function, satisfying Prove that . Solution: Let us assume that be a decreasing function such that . Since for , cannot be negative. Case 1. Let . Then given any such that for any , . Then Hence given any such that for any , . Hence . Thus . Case 2. Let . Then given any such that for any , . Then Hence given any such that for any , . Hence . Thus . Hence in both the cases we get a contradiction to the fact that . Thus . Thank you.","f:(0,\infty)\to(0,\infty) I=\int\limits_{0}^{\infty}f(x) \, \mathrm{d} x<\infty. \lim\limits_{x\to\infty}xf(x)=0 f \lim\limits_{x\to\infty}xf(x)=l \, ,l>0 xf(x)>0 x>0 \lim\limits_{x\to\infty}xf(x) l=\infty M>0 \, \exists a=\frac{M}{f(a)}>0 x>a xf(x)>M g(t)=\int\limits_{0}^{t}f(x) \, \mathrm{d} x=\int\limits_{a}^{t}f(x) \, \mathrm{d} x+\int\limits_{0}^{a}f(x) \, \mathrm{d} x>\int\limits_{0}^{a}f(a)\, \mathrm{d} x=af(a)=M. M>0 \exists a=a(M)>0 t>a g(t)>M \lim\limits_{t\to\infty}g(t)=\infty I=\infty 0<l<\infty \epsilon>0 \, \exists \delta=\frac{\epsilon}{f(\delta)}>0 x>a |xf(x)-l|<\epsilon g(t)=\int\limits_{0}^{t}f(x) \, \mathrm{d} x=\int\limits_{\delta}^{t}f(x) \, \mathrm{d} x+\int\limits_{0}^{\delta}f(x) \, \mathrm{d} x>\int\limits_{0}^{\delta}f(\delta) \, \mathrm{d} x=\delta f(\delta)=\epsilon. \epsilon>0 \exists \delta=\delta(\epsilon)>0 t>\delta g(t)>\epsilon \lim\limits_{t\to\infty}g(t)=\infty I=\infty I<\infty \lim\limits_{x\to\infty}xf(x)=0","['real-analysis', 'limits', 'improper-integrals', 'solution-verification']"
26,Cluster point and epsilon delta question.,Cluster point and epsilon delta question.,,"Questions Let $D=\{\frac{1}{n}:n\in\mathbb N\}\subseteq\mathbb R$ . (a) Show that $0$ is a cluster point of D (b) Let $f: x \mapsto1$ where $x\in D$ . Show that $\lim_{x\to\infty} f(x) = 1$ . (c) Let $g: \frac{1}{n} \mapsto k$ where $\frac{1}{n}\in D$ and $k\in\mathbb N_0$ is the largest number such that $2^k | n$ . Show that $\lim_{x\to\infty}g(x)$ does not exist My answers (a) Theorem: Let $D\subseteq\mathbb R$ and let $x\in\mathbb R$ . $x$ is a cluster point if and only if $\exists (x_k)_{k\in\mathbb N}$ such that $x_k\in D\setminus{x}$ for all $k\in\mathbb N$ and $\lim_{k\to\infty}x_k=x$ . Let $x_n=\frac{1}{n}$ where $n\in\mathbb N$ be a sequence. We can define D in terms of $x_n$ . i.e $D=\{x_n\}$ . So $x_n\in D$ $\space\forall n\in\mathbb N$ . Proving that $0\notin D$ : Assume $0\in D$ $\Leftrightarrow \space \exists n\in\mathbb N$ such that $\frac{1}{n}=0 \space$ $\Leftrightarrow 1=0$ $\therefore$ Contradiction $\Rightarrow 0\notin D$ $\Rightarrow D\setminus\{0\}=D$ Therefore $x_n\in D\setminus\{0\}$ $\forall \epsilon\gt 0\space$ $\space\exists N_{\epsilon}\in\mathbb N$ where $N_{\epsilon}=\frac{1}{\epsilon}$ : $\forall n\gt N_{\epsilon}\space$ $\Rightarrow |x_n-0|=|\frac{1}{n}-0|=\frac{1}{n}\lt\frac{1}{N_{\epsilon}}=\epsilon$ Hence $\lim_{n\to\infty}x_n=0$ $\Rightarrow$ By the theorem provided we have shown that $x=0$ is a cluster point. (b) Given $\epsilon\gt0$ . We can choose any $\delta$ with $0\lt |x-0|\lt\delta$ we get $|f(x)-1|=|1-1|=0\lt\epsilon$ Hence $\lim_{x\to 0}f(x)=1$ (c) I have tried for a while now -struggling to see how to tackle this. Comments This is a nasty question which was posed to me on my Analysis I course. Would be great if anyone could check my work, maybe even give alternative proofs for (a) and (b). And it would be nice if someone could help me with (c) :)","Questions Let . (a) Show that is a cluster point of D (b) Let where . Show that . (c) Let where and is the largest number such that . Show that does not exist My answers (a) Theorem: Let and let . is a cluster point if and only if such that for all and . Let where be a sequence. We can define D in terms of . i.e . So . Proving that : Assume such that Contradiction Therefore where : Hence By the theorem provided we have shown that is a cluster point. (b) Given . We can choose any with we get Hence (c) I have tried for a while now -struggling to see how to tackle this. Comments This is a nasty question which was posed to me on my Analysis I course. Would be great if anyone could check my work, maybe even give alternative proofs for (a) and (b). And it would be nice if someone could help me with (c) :)",D=\{\frac{1}{n}:n\in\mathbb N\}\subseteq\mathbb R 0 f: x \mapsto1 x\in D \lim_{x\to\infty} f(x) = 1 g: \frac{1}{n} \mapsto k \frac{1}{n}\in D k\in\mathbb N_0 2^k | n \lim_{x\to\infty}g(x) D\subseteq\mathbb R x\in\mathbb R x \exists (x_k)_{k\in\mathbb N} x_k\in D\setminus{x} k\in\mathbb N \lim_{k\to\infty}x_k=x x_n=\frac{1}{n} n\in\mathbb N x_n D=\{x_n\} x_n\in D \space\forall n\in\mathbb N 0\notin D 0\in D \Leftrightarrow \space \exists n\in\mathbb N \frac{1}{n}=0 \space \Leftrightarrow 1=0 \therefore \Rightarrow 0\notin D \Rightarrow D\setminus\{0\}=D x_n\in D\setminus\{0\} \forall \epsilon\gt 0\space \space\exists N_{\epsilon}\in\mathbb N N_{\epsilon}=\frac{1}{\epsilon} \forall n\gt N_{\epsilon}\space \Rightarrow |x_n-0|=|\frac{1}{n}-0|=\frac{1}{n}\lt\frac{1}{N_{\epsilon}}=\epsilon \lim_{n\to\infty}x_n=0 \Rightarrow x=0 \epsilon\gt0 \delta 0\lt |x-0|\lt\delta |f(x)-1|=|1-1|=0\lt\epsilon \lim_{x\to 0}f(x)=1,"['real-analysis', 'limits', 'analysis', 'solution-verification', 'epsilon-delta']"
27,Knowing the limit of $f'(x)$ find the limit of $f(x)$,Knowing the limit of  find the limit of,f'(x) f(x),"We have that $f$ is differentiable on $(a, +\infty)$ with $a>0$ . I want to show that if $\displaystyle{\lim_{x\rightarrow +\infty}f'(x)=\ell}$ , then there are the following cases: If $\ell>0$ then $$\lim_{x\rightarrow +\infty}f(x)=+\infty\ \text{ and } \ \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=\ell$$ If $\ell<0$ then $$\lim_{x\rightarrow +\infty}f(x)=-\infty\ \text{ and } \ \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=\ell$$ If $\ell=0$ then $$\lim_{x\rightarrow +\infty}f(x)=? \ \text{ and } \ \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=\ell$$ $$$$ Do we use the fact that $$f(x+1)-f(x)=\int_x^{x+1}f'(u)\,du$$ to find the limits of the function $f$ ?","We have that is differentiable on with . I want to show that if , then there are the following cases: If then If then If then Do we use the fact that to find the limits of the function ?","f (a, +\infty) a>0 \displaystyle{\lim_{x\rightarrow +\infty}f'(x)=\ell} \ell>0 \lim_{x\rightarrow +\infty}f(x)=+\infty\ \text{ and } \ \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=\ell \ell<0 \lim_{x\rightarrow +\infty}f(x)=-\infty\ \text{ and } \ \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=\ell \ell=0 \lim_{x\rightarrow +\infty}f(x)=? \ \text{ and } \ \lim_{x\rightarrow +\infty}\frac{f(x)}{x}=\ell  f(x+1)-f(x)=\int_x^{x+1}f'(u)\,du f","['limits', 'analysis', 'functions']"
28,Evaluating $\lim_{x\to 0}\left(\frac{e}{(1+x)^{1/x}}\right)^{1/x}$,Evaluating,\lim_{x\to 0}\left(\frac{e}{(1+x)^{1/x}}\right)^{1/x},"I need to evaluate the following limit, which is of the indeterminate form "" $1^{\infty}$ "". $$\lim_{x\to 0}\left(\frac{e}{(1+x)^{1/x}}\right)^{1/x}$$ The answer of this limit should be $\sqrt{e}$ . I have tried it many times (using both binomial expansion and exponential expansion) but it is not working out. I have even tried online limit calculator like mathway but it says ""I'm unable to solve this"". Then I tried MathPortal's Limit Calculator . This gives the answer correctly but doesn't shows steps. Then I tried WolframAlpha, but you can only access its step-by-step solution if you pro member (which is paid subscription) and I'm not one. So if anybody can help me solve this limit it would be highly appreciative. NOTE: I am a high school student.","I need to evaluate the following limit, which is of the indeterminate form "" "". The answer of this limit should be . I have tried it many times (using both binomial expansion and exponential expansion) but it is not working out. I have even tried online limit calculator like mathway but it says ""I'm unable to solve this"". Then I tried MathPortal's Limit Calculator . This gives the answer correctly but doesn't shows steps. Then I tried WolframAlpha, but you can only access its step-by-step solution if you pro member (which is paid subscription) and I'm not one. So if anybody can help me solve this limit it would be highly appreciative. NOTE: I am a high school student.",1^{\infty} \lim_{x\to 0}\left(\frac{e}{(1+x)^{1/x}}\right)^{1/x} \sqrt{e},"['algebra-precalculus', 'limits']"
29,Limit with infinitely many (removable) discontinuities,Limit with infinitely many (removable) discontinuities,,"The question is about $ \lim_{x\to0}\frac{\sin\frac{\pi}{x}}{\sin\frac{\pi}{x}}$ It is obvious that for all $x$ in the domain the function value is $1$ . In DESMOS it shows a horizontal line $y=1$ . But...when you look closer around $x=0$ , the number of holes in the graph also increases. And when you look at an infinite small neighborhood around $x=0$ , there are infinitely many holes in the graph. That's a lot of discontinuities. So with that observation, does the limit then exist? I would like to hear your input on this. Of course, through a piece wise function one can ""remove"" these discontinuities, but if you do that in an infinite close neighborhood around $0$ one can wonder, ""what's left""? Thanks for your input.","The question is about It is obvious that for all in the domain the function value is . In DESMOS it shows a horizontal line . But...when you look closer around , the number of holes in the graph also increases. And when you look at an infinite small neighborhood around , there are infinitely many holes in the graph. That's a lot of discontinuities. So with that observation, does the limit then exist? I would like to hear your input on this. Of course, through a piece wise function one can ""remove"" these discontinuities, but if you do that in an infinite close neighborhood around one can wonder, ""what's left""? Thanks for your input.","
\lim_{x\to0}\frac{\sin\frac{\pi}{x}}{\sin\frac{\pi}{x}} x 1 y=1 x=0 x=0 0",['limits']
30,On the sums $\sum\limits_{i=0}^n\frac{i}{n}\ln(\frac{i}{n}) $ and $\sum\limits_{i=0}^n(-1)^i\frac{i}{n}\ln(\frac{i}{n}) $,On the sums  and,\sum\limits_{i=0}^n\frac{i}{n}\ln(\frac{i}{n})  \sum\limits_{i=0}^n(-1)^i\frac{i}{n}\ln(\frac{i}{n}) ,"I was thinking about unimodal sequences, and the two which occurred to me are $\binom{n}{i}$ and $\dfrac{i}{n}\ln(\dfrac{i}{n}) $ , both for $i=0$ to $n$ (for the second, its value is $0$ at $i=0$ ). For the first, it is well known that $\sum_{i=0}^n  \binom{n}{i} =2^n $ and $\sum_{i=0}^n  (-1)^i\binom{n}{i} =0 $ . I naturally wondered about the corresponding results for $A_n =\sum_{i=0}^n\dfrac{i}{n}\ln(\dfrac{i}{n}) $ and $A_n^{\pm} =\sum_{i=0}^n(-1)^i\dfrac{i}{n}\ln(\dfrac{i}{n}) $ . Here's what I have shown. $$A_n = -\dfrac{n}{4}+\dfrac{\ln(n)}{12n}+\dfrac1{4n}+O\left(\dfrac1{n^2}\right) $$ $$A_{2n}^{\pm} =\dfrac{3\ln(n)}{8n}+O\left(\dfrac1{n}\right) $$ $$A_{2n+1}^{\pm} =\dfrac{\ln(n)}{8n}+O\left(\dfrac1{n}\right) $$ I have verified these computationally. My proofs, as they often are, are fairly messy, especially for $A_{n}^{\pm} $ , so my questions are (ya gotta have a question) How well known are these results? Are there reasonably simple proofs of them? Is there a simple proof that $A_{n}^{\pm} \to 0$ as $n \to \infty$ ?","I was thinking about unimodal sequences, and the two which occurred to me are and , both for to (for the second, its value is at ). For the first, it is well known that and . I naturally wondered about the corresponding results for and . Here's what I have shown. I have verified these computationally. My proofs, as they often are, are fairly messy, especially for , so my questions are (ya gotta have a question) How well known are these results? Are there reasonably simple proofs of them? Is there a simple proof that as ?","\binom{n}{i} \dfrac{i}{n}\ln(\dfrac{i}{n})
 i=0 n 0 i=0 \sum_{i=0}^n 
\binom{n}{i}
=2^n
 \sum_{i=0}^n 
(-1)^i\binom{n}{i}
=0
 A_n
=\sum_{i=0}^n\dfrac{i}{n}\ln(\dfrac{i}{n})
 A_n^{\pm}
=\sum_{i=0}^n(-1)^i\dfrac{i}{n}\ln(\dfrac{i}{n})
 A_n
= -\dfrac{n}{4}+\dfrac{\ln(n)}{12n}+\dfrac1{4n}+O\left(\dfrac1{n^2}\right)
 A_{2n}^{\pm}
=\dfrac{3\ln(n)}{8n}+O\left(\dfrac1{n}\right)
 A_{2n+1}^{\pm}
=\dfrac{\ln(n)}{8n}+O\left(\dfrac1{n}\right)
 A_{n}^{\pm}
 A_{n}^{\pm}
\to 0 n \to \infty","['limits', 'summation', 'logarithms']"
31,"If sequence converges only uniformly, could any of the mean serieses converge absolutly?","If sequence converges only uniformly, could any of the mean serieses converge absolutly?",,"TIL that if a series converges absolutely, the arithmetic, geometric, and harmonic means' serieses also converge to the same limit. Mathematically speaking: If $$\lim_{n \to \infty} a_n = L $$ Then: $$\lim_{n \to \infty} \frac{a_1 + a_2 + ... + a_n}{n} = \lim_{n \to \infty} \sqrt[n]{a_1 a_2 a_3 ... a_n} = \lim_{n \to \infty} \frac{n}{a_1^{-1}+a_2^{-1}+\ldots+a_n^{-1}} = L$$ Partial proofs: Prove convergence of the sequence $(z_1+z_2+\cdots + z_n)/n$ of Cesaro means On Cesàro convergence: If $ x_n \to x $ then $ z_n = \frac{x_1 + \dots +x_n}{n} \to x $ Would this be true if $L = \infty$ ? If not, I'm seeking an example that will satisfy: $$lim_{n \to \infty} a_n = \infty $$ While: $$lim_{n \to \infty} \frac{a_1 + a_2 + ... + a_n}{n} = S $$","TIL that if a series converges absolutely, the arithmetic, geometric, and harmonic means' serieses also converge to the same limit. Mathematically speaking: If Then: Partial proofs: Prove convergence of the sequence $(z_1+z_2+\cdots + z_n)/n$ of Cesaro means On Cesàro convergence: If $ x_n \to x $ then $ z_n = \frac{x_1 + \dots +x_n}{n} \to x $ Would this be true if ? If not, I'm seeking an example that will satisfy: While:",\lim_{n \to \infty} a_n = L  \lim_{n \to \infty} \frac{a_1 + a_2 + ... + a_n}{n} = \lim_{n \to \infty} \sqrt[n]{a_1 a_2 a_3 ... a_n} = \lim_{n \to \infty} \frac{n}{a_1^{-1}+a_2^{-1}+\ldots+a_n^{-1}} = L L = \infty lim_{n \to \infty} a_n = \infty  lim_{n \to \infty} \frac{a_1 + a_2 + ... + a_n}{n} = S ,"['real-analysis', 'sequences-and-series', 'limits', 'means']"
32,"Given the sequence $(a_n)_{n \ge 1}$ with $a_1=2$ and $a_{n+1} = \frac{n^2-1}{a_n} + 2$ for $n \ge 1$, find the following limits.","Given the sequence  with  and  for , find the following limits.",(a_n)_{n \ge 1} a_1=2 a_{n+1} = \frac{n^2-1}{a_n} + 2 n \ge 1,"I have the sequence $(a_n)_{n \ge 1}$ , such that: $$a_1 = 2, \hspace{1.5cm} a_{n+1} = \dfrac{n^2-1}{a_n}+2 \hspace{.25cm}, \forall n \ge 1$$ And I have to find $2$ limits: $$\lim\limits_{n \to \infty} \dfrac{a_n}{n} \hspace{3.5cm} \lim\limits_{n \to \infty} \dfrac{\sum\limits_{k=1}^{n}a_k^3}{n^4}$$ The second one completly put me in the dark. I don't see any trick that I could use. For the first one, I tried using Stolz-Cesaro: $$\lim\limits_{n \to \infty} \dfrac{a_{n+1}-a_n}{n+1-n} = \lim\limits_{n \to \infty} (a_{n+1}-a_n) = \lim\limits_{n \to \infty} \bigg ( \dfrac{n^2-1}{a_n} - a_n \bigg )$$ And I got stuck. I don't think I can find a closed form for $a_n$ , so I really don't know what should I do to find these $2$ limits.","I have the sequence , such that: And I have to find limits: The second one completly put me in the dark. I don't see any trick that I could use. For the first one, I tried using Stolz-Cesaro: And I got stuck. I don't think I can find a closed form for , so I really don't know what should I do to find these limits.","(a_n)_{n \ge 1} a_1 = 2, \hspace{1.5cm} a_{n+1} = \dfrac{n^2-1}{a_n}+2 \hspace{.25cm}, \forall n \ge 1 2 \lim\limits_{n \to \infty} \dfrac{a_n}{n} \hspace{3.5cm} \lim\limits_{n \to \infty} \dfrac{\sum\limits_{k=1}^{n}a_k^3}{n^4} \lim\limits_{n \to \infty} \dfrac{a_{n+1}-a_n}{n+1-n} = \lim\limits_{n \to \infty} (a_{n+1}-a_n) = \lim\limits_{n \to \infty} \bigg ( \dfrac{n^2-1}{a_n} - a_n \bigg ) a_n 2","['real-analysis', 'calculus']"
33,Evaluate $\lim\limits_{n \to \infty}\sum\limits_{k=1}^{n}\frac{\sqrt[k]{k}}{\sqrt{n^2+n-nk}}$,Evaluate,\lim\limits_{n \to \infty}\sum\limits_{k=1}^{n}\frac{\sqrt[k]{k}}{\sqrt{n^2+n-nk}},$$\lim_{n \to  \infty}\sum_{k=1}^{n}\frac{\sqrt[k]{k}}{\sqrt{n^2+n-nk}}$$ How to consider it?,How to consider it?,"\lim_{n \to
 \infty}\sum_{k=1}^{n}\frac{\sqrt[k]{k}}{\sqrt{n^2+n-nk}}",['limits']
34,Evaluating Convergence (Uniform),Evaluating Convergence (Uniform),,"Hi Guys was attempting this question and was wondering if I was doing the question correctly? Determine whether or not the sequence of functions is uniformly convergent:- $$g_n:(0,1)\to \mathbb{R}$$ $$g_n(x) = \frac{n^3+1}{n^3x^2+1}, x\in(0,1)$$ Checking point wise convergence first $$\lim_{n\to \infty}g_n(x) =  \lim_{n\to \infty}\frac{n^3+1}{n^3x^2+1}$$ Dividing by $n^3$ gives the following :- $$\lim_{n\to \infty}g_n(x) =  \lim_{n\to \infty}\frac{1+\frac{1}{n^3}}{x^2+\frac{1}{n^3}}$$ Taking the Limit as n $\to \infty$ gives the following $$\lim_{n\to \infty}g_n(x) =  \frac{1+\frac{1}{\infty^3}}{x^2+\frac{1}{\infty^3}}$$ $$\lim_{n\to \infty}g_n(x) =  \frac{1+0}{x^2+1} = \frac{1}{x^2}$$ Therefore by point wise convergence the sequence of functions converges to the previous function. In order to determine the uniform convergence we must analyze the follwing $$M_n =  sup|f_n(x)-f(x)|,x\in \mathbb{R}$$ $$|f_n(x)-f(x)|$$ $$|\frac{n^3+1}{n^3x^2+1} - \frac{1}{x^2}|$$ $$\frac{(n^3x^2+x^2)-(n^3x^2+1)}{(n^3x^2+1)(x^2)}$$ $$|\frac{x^2-1}{(n^3x^2+1)(x^2)}|$$ The mod gives $$\frac{x^2+1}{(n^3x^2+1)(x^2)}$$ is it accurate to say the following when checking to see uniform convergence $$\frac{x^2+1}{(n^3x^2+1)(x^2)} < \frac{1}{n^3}$$ $$\lim{n \to \infty} $$ Therefore I can conlclude that $$sup|f_n(x)-f(x)|\to 0$$ Therefore the function is uniformly convergent? Oh am i wrong in my evaluation?",Hi Guys was attempting this question and was wondering if I was doing the question correctly? Determine whether or not the sequence of functions is uniformly convergent:- Checking point wise convergence first Dividing by gives the following :- Taking the Limit as n gives the following Therefore by point wise convergence the sequence of functions converges to the previous function. In order to determine the uniform convergence we must analyze the follwing The mod gives is it accurate to say the following when checking to see uniform convergence Therefore I can conlclude that Therefore the function is uniformly convergent? Oh am i wrong in my evaluation?,"g_n:(0,1)\to \mathbb{R} g_n(x) = \frac{n^3+1}{n^3x^2+1}, x\in(0,1) \lim_{n\to \infty}g_n(x) =  \lim_{n\to \infty}\frac{n^3+1}{n^3x^2+1} n^3 \lim_{n\to \infty}g_n(x) =  \lim_{n\to \infty}\frac{1+\frac{1}{n^3}}{x^2+\frac{1}{n^3}} \to \infty \lim_{n\to \infty}g_n(x) =  \frac{1+\frac{1}{\infty^3}}{x^2+\frac{1}{\infty^3}} \lim_{n\to \infty}g_n(x) =  \frac{1+0}{x^2+1} = \frac{1}{x^2} M_n =  sup|f_n(x)-f(x)|,x\in \mathbb{R} |f_n(x)-f(x)| |\frac{n^3+1}{n^3x^2+1} - \frac{1}{x^2}| \frac{(n^3x^2+x^2)-(n^3x^2+1)}{(n^3x^2+1)(x^2)} |\frac{x^2-1}{(n^3x^2+1)(x^2)}| \frac{x^2+1}{(n^3x^2+1)(x^2)} \frac{x^2+1}{(n^3x^2+1)(x^2)} < \frac{1}{n^3} \lim{n \to \infty}  sup|f_n(x)-f(x)|\to 0","['limits', 'functions', 'convergence-divergence', 'uniform-convergence', 'sequence-of-function']"
35,Part 2: Does the arithmetic mean of sides right triangles to the mean of their hypotenuse converge?,Part 2: Does the arithmetic mean of sides right triangles to the mean of their hypotenuse converge?,,"A primitive Pythagorean triplet is a triplet $a^2 + b^2 = c^2$ be where $a,b,c$ have no common factors and is generated by $a = r^2 - s^2, b = 2rs, c = r^2 + s^2$ where $r > s \ge 1, \gcd(r,s) = 1$ and exactly one of the two numbers $r$ and $s$ is even. Clearly as $r$ increases, the number of primitive triplets formed for a given $r$ increases since the number of $s$ satisfying the above conditions increases. Claim: Let $c_1,c_2,\ldots$ be the hypotenuse and $b_1,b_2,\ldots $ be the corresponding longer of the two orthogonal sides formed for Pythagorean triangles for all $r \le x$ then as $x \to \infty$ , $$\frac{b_1 + b_2 + b_3 + \cdots}{c_1 + c_2 + c_3 + \cdots} = \sqrt{2} - \frac{1}{2}$$ Can this claim be proved or disproved? The difference between this question and the related question : Part 1: Does the arithmetic mean of sides right triangles to the mean of their hypotenuse converge? is that here the triangles are in sequenced in ascending order of $r$ and $s$ where as in the related question, they are sequenced in ascending order of the hypotenuse and depending on the choice of sequencing, the limiting value differs.","A primitive Pythagorean triplet is a triplet be where have no common factors and is generated by where and exactly one of the two numbers and is even. Clearly as increases, the number of primitive triplets formed for a given increases since the number of satisfying the above conditions increases. Claim: Let be the hypotenuse and be the corresponding longer of the two orthogonal sides formed for Pythagorean triangles for all then as , Can this claim be proved or disproved? The difference between this question and the related question : Part 1: Does the arithmetic mean of sides right triangles to the mean of their hypotenuse converge? is that here the triangles are in sequenced in ascending order of and where as in the related question, they are sequenced in ascending order of the hypotenuse and depending on the choice of sequencing, the limiting value differs.","a^2 + b^2 = c^2 a,b,c a = r^2 - s^2, b = 2rs, c = r^2 + s^2 r > s \ge 1, \gcd(r,s) = 1 r s r r s c_1,c_2,\ldots b_1,b_2,\ldots  r \le x x \to \infty \frac{b_1 + b_2 + b_3 + \cdots}{c_1 + c_2 + c_3 + \cdots} = \sqrt{2} - \frac{1}{2} r s","['real-analysis', 'sequences-and-series', 'geometry', 'number-theory', 'limits']"
36,"Prove that $\lim_{x\to0, y\to0}\frac{x^3+y^3}{x^2+y^2}=0$",Prove that,"\lim_{x\to0, y\to0}\frac{x^3+y^3}{x^2+y^2}=0","Prove that $\lim_{x\to0, y\to0}\frac{x^3+y^3}{x^2+y^2}=0$ My Try",Prove that My Try,"\lim_{x\to0, y\to0}\frac{x^3+y^3}{x^2+y^2}=0","['calculus', 'limits', 'multivariable-calculus', 'epsilon-delta']"
37,"Prove that if $a_n$ is increasing and $\lim_{n \to \infty} a_n = L$, then $L > a_n$ for all n.","Prove that if  is increasing and , then  for all n.",a_n \lim_{n \to \infty} a_n = L L > a_n,"Prove that if $a_n$ is increasing and $\lim_{n \to \infty} a_n = L$ , then $L > a_n$ for all n. I am given this definition of an increasing sequence $$\text{A sequence } a_n \text{ is increasing if for any } m \text{ and } n \text{ with } n > m, \text{ we have } a_n > a_m$$ along with the definition of a limit $$\text{For every } \epsilon > 0, \text{there exists an } N \text{ such that for every } n \geq N, \left|a_n - L\right| < \epsilon$$ What I came up with: Suppose $L \leq a_n$ . Let $\epsilon = a_m - L$ . Then, $$\left|a_n - L\right| < \epsilon$$ $$a_n - L < \epsilon$$ $$a_n - L < a_m - L$$ $$a_n < a_m$$ Which is supposed to prove by contradiction that $L > a_n$ . This seems very wrong to me though, and so any help would be appreciated.","Prove that if is increasing and , then for all n. I am given this definition of an increasing sequence along with the definition of a limit What I came up with: Suppose . Let . Then, Which is supposed to prove by contradiction that . This seems very wrong to me though, and so any help would be appreciated.","a_n \lim_{n \to \infty} a_n = L L > a_n \text{A sequence } a_n \text{ is increasing if for any } m \text{ and } n \text{ with } n > m, \text{ we have } a_n > a_m \text{For every } \epsilon > 0, \text{there exists an } N \text{ such that for every } n \geq N, \left|a_n - L\right| < \epsilon L \leq a_n \epsilon = a_m - L \left|a_n - L\right| < \epsilon a_n - L < \epsilon a_n - L < a_m - L a_n < a_m L > a_n","['real-analysis', 'limits', 'proof-verification']"
38,Is $\infty + (\infty/\infty)$ indeterminate?,Is  indeterminate?,\infty + (\infty/\infty),"I know $ (\infty/\infty)$ is indeterminate, but it can't be less than $0$ . So can you assume $\infty + (\infty/\infty)$ is determinate because $\infty + n$ where $n\ge 0$ is still $\infty$ ? The equation this question is based off of is $$\lim_{n \to \infty} \frac{n \log n + n}{\log n}.$$ This is in the context of big O notation. Would this be form be valid to use to determine the numerator's function is big Omega of the denominator? Or should l'hopitals rule be used to find a determinate and defined limit?","I know is indeterminate, but it can't be less than . So can you assume is determinate because where is still ? The equation this question is based off of is This is in the context of big O notation. Would this be form be valid to use to determine the numerator's function is big Omega of the denominator? Or should l'hopitals rule be used to find a determinate and defined limit?", (\infty/\infty) 0 \infty + (\infty/\infty) \infty + n n\ge 0 \infty \lim_{n \to \infty} \frac{n \log n + n}{\log n}.,"['limits', 'infinity', 'indeterminate-forms']"
39,Proving whether or not a limit is correct.,Proving whether or not a limit is correct.,,"I am new to calculus. In class, we are learning how to formally prove a limit is correct. Say f(x)=x+2          Say someone claims the limit as x approaches 2 is 3. We were asked to check (formally) whether this is correct or not. I started by asking what values of x would keep the absolute value of ""f(x)-3"" less than some number, epsilon. I got that for this to hold true, the absolute value of ""x-1"" has to be less than epsilon. Then I said, well lets suppose that the limit is 3. Now lets suppose epsilon is 0.1. For the limit to be correct, there must be some range around 2 such that all f(x) lie within 0.1 of 3. But from previous work, I know that this will only happen as long as x is between 0.9 and 1.1. Since I have shown that there exists some epsilon for which there is no range around 2 which works, the limit can not be 3. Is this approach correct?","I am new to calculus. In class, we are learning how to formally prove a limit is correct. Say f(x)=x+2          Say someone claims the limit as x approaches 2 is 3. We were asked to check (formally) whether this is correct or not. I started by asking what values of x would keep the absolute value of ""f(x)-3"" less than some number, epsilon. I got that for this to hold true, the absolute value of ""x-1"" has to be less than epsilon. Then I said, well lets suppose that the limit is 3. Now lets suppose epsilon is 0.1. For the limit to be correct, there must be some range around 2 such that all f(x) lie within 0.1 of 3. But from previous work, I know that this will only happen as long as x is between 0.9 and 1.1. Since I have shown that there exists some epsilon for which there is no range around 2 which works, the limit can not be 3. Is this approach correct?",,"['calculus', 'limits']"
40,"Spivak's Calculus, chapter 5 question 42 without being ""completely fallacious""","Spivak's Calculus, chapter 5 question 42 without being ""completely fallacious""",,"I'm wondering is this proof will fix the issue of being ""completely fallacious"" - or is it still circular? - Consider the function $f(x)=x^2.$ The aim is to prove its continuity using interval notation. Let $a\in \mathbb{R}.$ We need to check that given $\epsilon>0$ there exists $\delta>0$ such that for all $x$ satisfying $a-\delta<x<a+\delta$ one has $a^2-\epsilon<x^2<a^2+\epsilon.$ To understand what must be the value of $\delta,$ we observe that $$ x^2-a^2=(x-a)(x+a). $$ If we know that $|x-a|<\delta,$ then $|x|<\delta+|a|$ and $$ |x^2-a^2|=|x-a||x+a|<\delta(|x|+|a|)<\delta(2|a|+\delta). $$ Take $\delta=\min(1,\frac{\epsilon}{2a+1})$ and let $a-\delta<x<a+\delta.$ Then $$ x^2=a^2+x^2-a^2<a^2+\delta(2|a|+\delta)\leq a^2+\delta(2|a|+1)\leq a^2+\epsilon $$ and $$ x^2=a^2+x^2-a^2>a^2-\delta(2|a|+\delta)\geq a^2-\delta(2|a|+1)\geq a^2-\epsilon $$ Finally, if $a-\delta<x<a+\delta,$ then $a^2-\epsilon<x^2<a^2+\epsilon.$","I'm wondering is this proof will fix the issue of being ""completely fallacious"" - or is it still circular? - Consider the function The aim is to prove its continuity using interval notation. Let We need to check that given there exists such that for all satisfying one has To understand what must be the value of we observe that If we know that then and Take and let Then and Finally, if then","f(x)=x^2. a\in \mathbb{R}. \epsilon>0 \delta>0 x a-\delta<x<a+\delta a^2-\epsilon<x^2<a^2+\epsilon. \delta, 
x^2-a^2=(x-a)(x+a).
 |x-a|<\delta, |x|<\delta+|a| 
|x^2-a^2|=|x-a||x+a|<\delta(|x|+|a|)<\delta(2|a|+\delta).
 \delta=\min(1,\frac{\epsilon}{2a+1}) a-\delta<x<a+\delta. 
x^2=a^2+x^2-a^2<a^2+\delta(2|a|+\delta)\leq a^2+\delta(2|a|+1)\leq a^2+\epsilon
 
x^2=a^2+x^2-a^2>a^2-\delta(2|a|+\delta)\geq a^2-\delta(2|a|+1)\geq a^2-\epsilon
 a-\delta<x<a+\delta, a^2-\epsilon<x^2<a^2+\epsilon.","['real-analysis', 'calculus']"
41,Non existing limit,Non existing limit,,"I was having some fun with limits, until I encountered this: $$ \lim_{x\to0} \frac{\log(\vert x \vert (1+x^2)^{1/3}-\sin{x})}{\log(\vert x \vert)} $$ To me the limit in this case does not exists, so I tried computing it from right and left. Let's start from right. $$ \lim_{x\to0^+} \frac{\log(x (1+x^2)^{1/3}-\sin{x})}{\log(x)} $$ Here I would try to use Hopital theorem, but I want to avoid that if possible. I tried using Taylor polynomial, and I simplify everything getting to the following limit: $$ \lim_{x\to0} \frac{\log(\frac{x^3}{2})}{\log(x)}$$ Applying Hopital I get that this limit is $3$ , however I do not find the same using Taylor expansion. Is it right that this limit is $3$ ?","I was having some fun with limits, until I encountered this: To me the limit in this case does not exists, so I tried computing it from right and left. Let's start from right. Here I would try to use Hopital theorem, but I want to avoid that if possible. I tried using Taylor polynomial, and I simplify everything getting to the following limit: Applying Hopital I get that this limit is , however I do not find the same using Taylor expansion. Is it right that this limit is ?", \lim_{x\to0} \frac{\log(\vert x \vert (1+x^2)^{1/3}-\sin{x})}{\log(\vert x \vert)}   \lim_{x\to0^+} \frac{\log(x (1+x^2)^{1/3}-\sin{x})}{\log(x)}   \lim_{x\to0} \frac{\log(\frac{x^3}{2})}{\log(x)} 3 3,"['limits', 'proof-verification']"
42,Limit of composite function at isolated points,Limit of composite function at isolated points,,"Paul's Online Notes ( link ) writes: I have some doubts about the above claim. In particular, I'm not sure if it's correct if $b$ is an isolated point of the domain of $f$ . I have tried to come up with the counterexample below. Is my counterexample mistaken? And if it isn't, how can we fix the above claim so that it becomes correct? Counterexample. Define $f:\left\{ 2\right\} \rightarrow\mathbb{R}$ by $f\left(2\right)=3$ . Define $g:\mathbb{R}\rightarrow\mathbb{R}$ by $g\left(x\right)=x+1$ Then $\lim_{x\rightarrow1}g\left(x\right)=2$ $f\left(x\right)$ is continuous at $2$ (because $2$ is an isolated point of the domain of $f$ ) $f\left(\lim_{x\rightarrow1}g\left(x\right)\right)=f\left(2\right)=3$ . However, $\lim_{x\rightarrow1}f\left(g\left(x\right)\right)$ does not exist. (The composite function $f\circ g$ is not defined for any $x\neq 1$ .) And so, contrary to the above claim, $$\lim_{x\rightarrow1}f\left(g\left(x\right)\right)\neq f\left(\lim_{x\rightarrow1}g\left(x\right)\right).$$ In response to Hyperion and Rick's comments, here is a screenshot from Abbott (2015, p. 122, Understanding Analysis ). See especially the highlighted paragraph:","Paul's Online Notes ( link ) writes: I have some doubts about the above claim. In particular, I'm not sure if it's correct if is an isolated point of the domain of . I have tried to come up with the counterexample below. Is my counterexample mistaken? And if it isn't, how can we fix the above claim so that it becomes correct? Counterexample. Define by . Define by Then is continuous at (because is an isolated point of the domain of ) . However, does not exist. (The composite function is not defined for any .) And so, contrary to the above claim, In response to Hyperion and Rick's comments, here is a screenshot from Abbott (2015, p. 122, Understanding Analysis ). See especially the highlighted paragraph:",b f f:\left\{ 2\right\} \rightarrow\mathbb{R} f\left(2\right)=3 g:\mathbb{R}\rightarrow\mathbb{R} g\left(x\right)=x+1 \lim_{x\rightarrow1}g\left(x\right)=2 f\left(x\right) 2 2 f f\left(\lim_{x\rightarrow1}g\left(x\right)\right)=f\left(2\right)=3 \lim_{x\rightarrow1}f\left(g\left(x\right)\right) f\circ g x\neq 1 \lim_{x\rightarrow1}f\left(g\left(x\right)\right)\neq f\left(\lim_{x\rightarrow1}g\left(x\right)\right).,"['real-analysis', 'calculus']"
43,"$\lim_{n \to \infty}\frac{d(x_{n+1}, x_\star)}{d(x_n, x_\star)} = 0$, show that $d(x_n, x_\star)\leq K\varepsilon^n \quad \text{for all }n\geq 0$",", show that","\lim_{n \to \infty}\frac{d(x_{n+1}, x_\star)}{d(x_n, x_\star)} = 0 d(x_n, x_\star)\leq K\varepsilon^n \quad \text{for all }n\geq 0","Let $(\mathcal{X}, d)$ be a metric space and $(x_n)_{n = 0}^\infty$ a sequence in $\mathcal{X}$ converging to $x_\star \in \mathcal{X}$ , meaning that $\lim_{n \to \infty}d(x_n, x_\star) = 0$ We suppose that $$ \lim_{n \to \infty}\frac{d(x_{n+1}, x_\star)}{d(x_n, x_\star)} = 0 $$ Show that, for all $\varepsilon \in (0,1)$ , there exists $K = K(\varepsilon)\in (0,\infty)$ s.t. $$ d(x_n, x_\star)\leq K\varepsilon^n \quad \text{for all }n\geq 0  $$ answer : We suppose that $$ \lim_{n \to \infty}\frac{d(x_{n+1}, x_\star)}{d(x_n, x_\star)} = 0 $$ $\forall \varepsilon \in (0,1), \exists n_0 \in \mathbb{N}, \forall n\geq n_0,\frac{d(x_{n+1}, x_\star)}{d(x_n, x_\star)} \leq \varepsilon$ \newline Then $d(x_{n +1}, x_\star)\leq \varepsilon d(x_n, x_\star)$ \newline However $\forall n > n_0, \exists k\in \mathbb{N}^\star$ , $n = n_0 + k$ . And then: \begin{align*} d(x_n, x_\star) &= d(x_{n_0 + k}, x_\star)\\ &\leq \varepsilon d(x_{n_0 + k-1}, x_\star) \qquad (1)\\ &\leq \varepsilon^2 d(x_{n_0 + k-2}, x_\star) \qquad (2)\\ &\leq \varepsilon^{(k_0)} d(x_{n_0}, x_\star)\cdot \frac{\varepsilon^{(k_0)}}{\varepsilon^{(k_0)}} \qquad (3)\\ &\leq \varepsilon^n\underbrace{\frac{d(x_{n_0}, x_\star)}{\varepsilon^{(n_0)}}}_{K_1} \end{align*} And for $n \leq n_0$ : The set $\Big\{\frac{d(x_{n_0}, x^\star)}{\varepsilon^{(n_0)}}; n <n_0\Big\}$ is finite and therefore it has a maximum noted $K_2$ Then for $n \leq n_0$ : $d(x_n, x_\star) = \frac{d(x_{n}, x^\star)}{\varepsilon^{n}}\varepsilon^n \leq K_2\varepsilon^n$ We take $K(\varepsilon) = \max (K_1, K_2)$ and we have the result: \newline $\forall n \in \mathbb{N}$ if $n \leq n_0$ ; $d(x_n, x_\star) \leq \varepsilon^n K_2 \leq \varepsilon^n K(\varepsilon)$ else ; $d(x_n, x_\star) \leq \varepsilon^n K_1 \leq \varepsilon^n K(\varepsilon)$ $\rightarrow \forall n \in \mathbb{N} d(x_n, x^\star) \leq \varepsilon^n K(\varepsilon)$ My question How can we justify that the inequality still remains valid when going from (1 : "" $\leq \varepsilon d(x_{n_0 + k-1}, x_\star)$ "" to (2: "" $\leq \varepsilon^2 d(x_{n_0 + k-2}, x_\star)$ "")","Let be a metric space and a sequence in converging to , meaning that We suppose that Show that, for all , there exists s.t. answer : We suppose that \newline Then \newline However , . And then: And for : The set is finite and therefore it has a maximum noted Then for : We take and we have the result: \newline if ; else ; My question How can we justify that the inequality still remains valid when going from (1 : "" "" to (2: "" "")","(\mathcal{X}, d) (x_n)_{n = 0}^\infty \mathcal{X} x_\star \in \mathcal{X} \lim_{n \to \infty}d(x_n, x_\star) = 0 
\lim_{n \to \infty}\frac{d(x_{n+1}, x_\star)}{d(x_n, x_\star)} = 0
 \varepsilon \in (0,1) K = K(\varepsilon)\in (0,\infty) 
d(x_n, x_\star)\leq K\varepsilon^n \quad \text{for all }n\geq 0 
 
\lim_{n \to \infty}\frac{d(x_{n+1}, x_\star)}{d(x_n, x_\star)} = 0
 \forall \varepsilon \in (0,1), \exists n_0 \in \mathbb{N}, \forall n\geq n_0,\frac{d(x_{n+1}, x_\star)}{d(x_n, x_\star)} \leq \varepsilon d(x_{n +1}, x_\star)\leq \varepsilon d(x_n, x_\star) \forall n > n_0, \exists k\in \mathbb{N}^\star n = n_0 + k \begin{align*}
d(x_n, x_\star) &= d(x_{n_0 + k}, x_\star)\\
&\leq \varepsilon d(x_{n_0 + k-1}, x_\star) \qquad (1)\\
&\leq \varepsilon^2 d(x_{n_0 + k-2}, x_\star) \qquad (2)\\
&\leq \varepsilon^{(k_0)} d(x_{n_0}, x_\star)\cdot \frac{\varepsilon^{(k_0)}}{\varepsilon^{(k_0)}} \qquad (3)\\
&\leq \varepsilon^n\underbrace{\frac{d(x_{n_0}, x_\star)}{\varepsilon^{(n_0)}}}_{K_1}
\end{align*} n \leq n_0 \Big\{\frac{d(x_{n_0}, x^\star)}{\varepsilon^{(n_0)}}; n <n_0\Big\} K_2 n \leq n_0 d(x_n, x_\star) = \frac{d(x_{n}, x^\star)}{\varepsilon^{n}}\varepsilon^n \leq K_2\varepsilon^n K(\varepsilon) = \max (K_1, K_2) \forall n \in \mathbb{N} n \leq n_0 d(x_n, x_\star) \leq \varepsilon^n K_2 \leq \varepsilon^n K(\varepsilon) d(x_n, x_\star) \leq \varepsilon^n K_1 \leq \varepsilon^n K(\varepsilon) \rightarrow \forall n \in \mathbb{N} d(x_n, x^\star) \leq \varepsilon^n K(\varepsilon) \leq \varepsilon d(x_{n_0 + k-1}, x_\star) \leq \varepsilon^2 d(x_{n_0 + k-2}, x_\star)","['sequences-and-series', 'limits']"
44,Commutation of limit and integration,Commutation of limit and integration,,"I'm a physics student and perhaps do not have an in depth exposure to analysis. In the following calculation $$ \tilde{\Phi}(\Omega)= \lim_{\omega \rightarrow0} \int_{-\infty}^{\infty} d\tau\:e^{-i\Omega\tau} \text{cos}\big(\omega g^{-1}e^{-g \tau}\big) $$ it turns out that when I carry out the integral and evaluate $|\tilde{\Phi}(\Omega)|^2$ , then take the limit $\omega $ going to zero, I get a finite result (The finite result is well verified, it is from a paper with whose author I have gone through the calculation). However, if I take the limit $\omega$ going to zero first then $\tilde{\Phi}(\Omega)=\delta(\Omega)$ and $|\tilde{\Phi}(\Omega)|^2$ is the Dirac delta function squared. Pertaining to the corresponding interpretation in terms of physics, the finite result is what is the 'correct' computation. However, I wanted to know if this is the correct way mathematically too, what exactly is the subtelty that has gone into this, and are there similar examples of the same? For instance I know that when I just have two limits, the order can be important. For instance if $$ L = \lim_{ x,y \rightarrow{0}} \frac{f(x)}{g(y)} $$ where both $f(x)$ and $g(y)$ approach zero when $x,y$ approach zero then suppose I took the $x$ limit first. Then that would lead to $L$ being zero.  But the commutation of a limit and an integral is something that I can't quite wrap my head around.","I'm a physics student and perhaps do not have an in depth exposure to analysis. In the following calculation it turns out that when I carry out the integral and evaluate , then take the limit going to zero, I get a finite result (The finite result is well verified, it is from a paper with whose author I have gone through the calculation). However, if I take the limit going to zero first then and is the Dirac delta function squared. Pertaining to the corresponding interpretation in terms of physics, the finite result is what is the 'correct' computation. However, I wanted to know if this is the correct way mathematically too, what exactly is the subtelty that has gone into this, and are there similar examples of the same? For instance I know that when I just have two limits, the order can be important. For instance if where both and approach zero when approach zero then suppose I took the limit first. Then that would lead to being zero.  But the commutation of a limit and an integral is something that I can't quite wrap my head around.","
\tilde{\Phi}(\Omega)= \lim_{\omega \rightarrow0} \int_{-\infty}^{\infty} d\tau\:e^{-i\Omega\tau} \text{cos}\big(\omega g^{-1}e^{-g \tau}\big)
 |\tilde{\Phi}(\Omega)|^2 \omega  \omega \tilde{\Phi}(\Omega)=\delta(\Omega) |\tilde{\Phi}(\Omega)|^2 
L = \lim_{ x,y \rightarrow{0}} \frac{f(x)}{g(y)}
 f(x) g(y) x,y x L","['limits', 'analysis']"
45,Limit Of A Sequence Involving Factorial Functions,Limit Of A Sequence Involving Factorial Functions,,"Find the limit of the sequence $$\frac{c^n}{n!^{\frac{1}{k}}}$$ , $(k>0, c>0)$ Now when $0<c<1$ we get $$0<\frac{c^n}{n!^{\frac{1}{k}}}< \frac{1}{n!^{\frac{1}{k}}}$$ So by Sandwich Theorem we get the limit of the sequenc equal to $0$ But when $c>1$ i do not understand how to move?","Find the limit of the sequence , Now when we get So by Sandwich Theorem we get the limit of the sequenc equal to But when i do not understand how to move?","\frac{c^n}{n!^{\frac{1}{k}}} (k>0, c>0) 0<c<1 0<\frac{c^n}{n!^{\frac{1}{k}}}< \frac{1}{n!^{\frac{1}{k}}} 0 c>1",['limits']
46,Solving an integral with limits of polygamma functions,Solving an integral with limits of polygamma functions,,"I've been trying to solve the following integral: $$I_n=\int_0^1\frac1x\ln^n(x)\ln^{8-n}(1-x)~\mathrm dx$$ for $n\in[2,6]$ . It can be computed as a limit of derivatives of the Beta function: $$I_n=\lim_{x\to0^+}\frac{\partial^8}{\partial x^n\partial y^{8-n}}B(x,y)\bigg|_{y=1}$$ the derivatives of which can be given by WolframAlpha . However, I'm having difficulty completing the last step and taking the limit myself. I've tried tackling this manually by taking the constant term in the Laurent expansion of each part, which isn't very difficult since WolframAlpha factors expressions nicely. Unfortunately though there is a lot of steps and I am prone to error, and I don't know if I can get WolframAlpha to give me the answer. Is there a way to get WolframAlpha to give me the answer to these problems? If not I would appreciate it if someone could provide me the solution to these integrals, preferably in terms of the Riemann zeta function or the polygamma function.","I've been trying to solve the following integral: for . It can be computed as a limit of derivatives of the Beta function: the derivatives of which can be given by WolframAlpha . However, I'm having difficulty completing the last step and taking the limit myself. I've tried tackling this manually by taking the constant term in the Laurent expansion of each part, which isn't very difficult since WolframAlpha factors expressions nicely. Unfortunately though there is a lot of steps and I am prone to error, and I don't know if I can get WolframAlpha to give me the answer. Is there a way to get WolframAlpha to give me the answer to these problems? If not I would appreciate it if someone could provide me the solution to these integrals, preferably in terms of the Riemann zeta function or the polygamma function.","I_n=\int_0^1\frac1x\ln^n(x)\ln^{8-n}(1-x)~\mathrm dx n\in[2,6] I_n=\lim_{x\to0^+}\frac{\partial^8}{\partial x^n\partial y^{8-n}}B(x,y)\bigg|_{y=1}","['limits', 'definite-integrals', 'wolfram-alpha', 'beta-function', 'polygamma']"
47,Find $\lim_{x \to +\infty}(x^{{1}/{x}}-1)^{{1}/{\ln x}}$,Find,\lim_{x \to +\infty}(x^{{1}/{x}}-1)^{{1}/{\ln x}},"\begin{align*} \lim_{x \to +\infty}\left(x^{\frac{1}{x}}-1\right)^{\frac{1}{\ln x}}&=\exp \lim_{x \to +\infty}\frac{\ln\left( x^{\frac{1}{x}}-1\right)}{\ln x}\\ &=\exp \lim_{x \to +\infty}\frac{\ln\left(e^{\frac{\ln x}{x}}-1\right)}{\ln x}\\ &=\exp \lim_{x \to +\infty}\frac{\ln\left({\frac{\ln x}{x}}\right)}{\ln x}\\ &=\exp \lim_{x \to +\infty}\frac{\ln\ln x-\ln x}{\ln x}\\ &=\exp \lim_{x \to +\infty}\left(\frac{\ln\ln x}{\ln x}-1\right)\\ &=\exp(-1)\\ &=\frac{1}{e}, \end{align*} where, namely, the process from the 2nd line to the 3rd line, we used theses facts that \begin{align*} e^{\frac{\ln x}{x}}-1&=\frac{\ln x}{x}+\frac{1}{2!}\left(\frac{\ln x}{x}\right)^2+\frac{1}{3!}\left(\frac{\ln x}{x}\right)^3+\cdots\\ &=\frac{\ln x}{x}\left[1+\frac{1}{2!}\cdot\frac{\ln x}{x}+\frac{1}{3!}\left(\frac{\ln x}{x}\right)^2+\cdots\right]\\ &=\frac{\ln x}{x}\left[1+O\left(\frac{\ln x}{x}\right)\right], \end{align*} thus \begin{align*} \lim_{x \to +\infty}\frac{\ln \left(e^{\frac{\ln x}{x}}-1\right)}{\ln x}&=\lim_{x \to +\infty}\frac{\ln\left(\frac{\ln x}{x}\right)}{\ln x}+\lim_{x \to +\infty}\frac{\ln\left[1+O\left(\frac{\ln x}{x}\right)\right]}{\ln x}\\ &=\lim_{x \to +\infty}\frac{\ln\left(\frac{\ln x}{x}\right)}{\ln x}+\lim_{x \to +\infty}\frac{O\left(\frac{\ln x}{x}\right)}{\ln x}\\ &=\lim_{x \to +\infty}\frac{\ln\left(\frac{\ln x}{x}\right)}{\ln x}+\lim_{x \to +\infty}\left[\frac{O\left(\frac{\ln x}{x}\right)}{\frac{\ln x}{x}}\cdot \frac{1}{x}\right]\\ &=\lim_{x \to +\infty}\frac{\ln\left(\frac{\ln x}{x}\right)}{\ln x}. \end{align*} Please correct me if I'm wrong! Hope you see other solutions.","where, namely, the process from the 2nd line to the 3rd line, we used theses facts that thus Please correct me if I'm wrong! Hope you see other solutions.","\begin{align*}
\lim_{x \to +\infty}\left(x^{\frac{1}{x}}-1\right)^{\frac{1}{\ln x}}&=\exp \lim_{x \to +\infty}\frac{\ln\left( x^{\frac{1}{x}}-1\right)}{\ln x}\\
&=\exp \lim_{x \to +\infty}\frac{\ln\left(e^{\frac{\ln x}{x}}-1\right)}{\ln x}\\
&=\exp \lim_{x \to +\infty}\frac{\ln\left({\frac{\ln x}{x}}\right)}{\ln x}\\
&=\exp \lim_{x \to +\infty}\frac{\ln\ln x-\ln x}{\ln x}\\
&=\exp \lim_{x \to +\infty}\left(\frac{\ln\ln x}{\ln x}-1\right)\\
&=\exp(-1)\\
&=\frac{1}{e},
\end{align*} \begin{align*}
e^{\frac{\ln x}{x}}-1&=\frac{\ln x}{x}+\frac{1}{2!}\left(\frac{\ln x}{x}\right)^2+\frac{1}{3!}\left(\frac{\ln x}{x}\right)^3+\cdots\\
&=\frac{\ln x}{x}\left[1+\frac{1}{2!}\cdot\frac{\ln x}{x}+\frac{1}{3!}\left(\frac{\ln x}{x}\right)^2+\cdots\right]\\
&=\frac{\ln x}{x}\left[1+O\left(\frac{\ln x}{x}\right)\right],
\end{align*} \begin{align*}
\lim_{x \to +\infty}\frac{\ln \left(e^{\frac{\ln x}{x}}-1\right)}{\ln x}&=\lim_{x \to +\infty}\frac{\ln\left(\frac{\ln x}{x}\right)}{\ln x}+\lim_{x \to +\infty}\frac{\ln\left[1+O\left(\frac{\ln x}{x}\right)\right]}{\ln x}\\
&=\lim_{x \to +\infty}\frac{\ln\left(\frac{\ln x}{x}\right)}{\ln x}+\lim_{x \to +\infty}\frac{O\left(\frac{\ln x}{x}\right)}{\ln x}\\
&=\lim_{x \to +\infty}\frac{\ln\left(\frac{\ln x}{x}\right)}{\ln x}+\lim_{x \to +\infty}\left[\frac{O\left(\frac{\ln x}{x}\right)}{\frac{\ln x}{x}}\cdot \frac{1}{x}\right]\\
&=\lim_{x \to +\infty}\frac{\ln\left(\frac{\ln x}{x}\right)}{\ln x}.
\end{align*}","['limits', 'proof-verification']"
48,Taylor polynomials respect derivatives,Taylor polynomials respect derivatives,,"I want to prove that the derivative of the $n$ th order Taylor polynomial is the $n-1$ th order Taylor polynomial of the derivative. More specifically: Suppose $f: \mathbb{R} \to \mathbb{R}$ is $n$ times differentiable about $x_0$ and $T_n(x)$ a polynomail of degree at most $n$ . Then if $$ \lim_{x\to x_0}\frac{f(x) - T_n(x)}{(x-x_0)^n} = 0 $$ then $$ \lim_{x\to x_0}\frac{f'(x) - T'_n(x)}{(x-x_0)^{n-1}} = 0 $$ $\textbf{I am looking for a proof of this fact that is as direct as possible}$ . The fact looks like a simple corollary of l'hopitals rule. However I don't think this is legitimate since l'hopitals rule assumes that the limit $\frac{f'}{g'}$ exists, and this is what we are trying to prove. I think this property should be true by the following argument: The hypothesis implies $T_n(x)$ is the $n$ th order Taylor polynomial of $f$ about $x_0$ . Hence $$ T_n(x) = \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k. $$ This implies $$T'_n(x) = \sum_{k=0}^{n-1} \frac{f^{(k+1)}(x_0)}{k!}(x-x_0)^k = \sum_{k=0}^{n-1} \frac{g^{(k)}(x_0)}{k!}(x-x_0)^k$$ where $g(x) = f'(x)$ . Hence $T'_n(x)$ is the $n-1$ th order Taylor polynomial of $g(x) = f'(x)$ . Again it is well known that this implies $$\lim_{x\to x_0}\frac{f'(x) - T'_n(x)}{(x-x_0)^{n-1}} = 0$$","I want to prove that the derivative of the th order Taylor polynomial is the th order Taylor polynomial of the derivative. More specifically: Suppose is times differentiable about and a polynomail of degree at most . Then if then . The fact looks like a simple corollary of l'hopitals rule. However I don't think this is legitimate since l'hopitals rule assumes that the limit exists, and this is what we are trying to prove. I think this property should be true by the following argument: The hypothesis implies is the th order Taylor polynomial of about . Hence This implies where . Hence is the th order Taylor polynomial of . Again it is well known that this implies","n n-1 f: \mathbb{R} \to \mathbb{R} n x_0 T_n(x) n 
\lim_{x\to x_0}\frac{f(x) - T_n(x)}{(x-x_0)^n}
= 0
 
\lim_{x\to x_0}\frac{f'(x) - T'_n(x)}{(x-x_0)^{n-1}}
= 0
 \textbf{I am looking for a proof of this fact that is as direct as possible} \frac{f'}{g'} T_n(x) n f x_0 
T_n(x)
= \sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!}(x-x_0)^k.
 T'_n(x) = \sum_{k=0}^{n-1} \frac{f^{(k+1)}(x_0)}{k!}(x-x_0)^k = \sum_{k=0}^{n-1} \frac{g^{(k)}(x_0)}{k!}(x-x_0)^k g(x) = f'(x) T'_n(x) n-1 g(x) = f'(x) \lim_{x\to x_0}\frac{f'(x) - T'_n(x)}{(x-x_0)^{n-1}} = 0","['calculus', 'limits', 'polynomials', 'taylor-expansion']"
49,Evaluation of undefined limit,Evaluation of undefined limit,,"I am supposed to evaluate this limit. $$\lim_{x\rightarrow 0} \, \frac{\sqrt[3]{x} \ln(\ln x)}{\sqrt[3]{(2x+3)\ln x}}$$ I tried to solve it as two limits, in the way that: $$\lim_{x\rightarrow 0} \, \frac{\sqrt[3]{x}}{\sqrt[3]{2x+3}}$$ $$\lim_{x\rightarrow 0} \, \frac{\ln(\ln x)}{\sqrt[3]{\ln x}}$$ so that the first one is zero, but the second one is not difined for zero. Can anyone help me to continue? Thanks.","I am supposed to evaluate this limit. I tried to solve it as two limits, in the way that: so that the first one is zero, but the second one is not difined for zero. Can anyone help me to continue? Thanks.","\lim_{x\rightarrow 0} \, \frac{\sqrt[3]{x} \ln(\ln x)}{\sqrt[3]{(2x+3)\ln x}} \lim_{x\rightarrow 0} \, \frac{\sqrt[3]{x}}{\sqrt[3]{2x+3}} \lim_{x\rightarrow 0} \, \frac{\ln(\ln x)}{\sqrt[3]{\ln x}}","['real-analysis', 'calculus', 'limits', 'limits-without-lhopital']"
50,Why does $f(x_n)\rightarrow 0$ imply that a subsequence converges to zero of $f$?,Why does  imply that a subsequence converges to zero of ?,f(x_n)\rightarrow 0 f,"Let $f$ be a separable and irreducible polynomial of degree $d\geq 1$ with coefficients in a local field of characteristic $p$ , say $K= \mathbb F_p((T))$ and $f\in K[X]$ . Assume there is a sequence $(y_n)_{n\in \mathbb N}$ in the separable closure $K^s$ satisfying $$ \lim_{n \rightarrow \infty} f(y_n) = 0. $$ I want to show that there is a subsequence $(y_{n_{k}})_{k\in \mathbb N}$ converging to a zero $x$ of $f$ in $K^s$ . I don't know much about non-archimedian analysis and I wonder if the proof can be stated similar to the real case?","Let be a separable and irreducible polynomial of degree with coefficients in a local field of characteristic , say and . Assume there is a sequence in the separable closure satisfying I want to show that there is a subsequence converging to a zero of in . I don't know much about non-archimedian analysis and I wonder if the proof can be stated similar to the real case?","f d\geq 1 p K= \mathbb F_p((T)) f\in K[X] (y_n)_{n\in \mathbb N} K^s 
\lim_{n \rightarrow \infty} f(y_n) = 0.
 (y_{n_{k}})_{k\in \mathbb N} x f K^s","['limits', 'algebraic-number-theory', 'irreducible-polynomials', 'local-field', 'nonarchimedian-analysis']"
51,finding limit of a multivariable function,finding limit of a multivariable function,,"I want to find the limit of $\dfrac{x^2 +y^4}{y^2+x^4}$ as $(x,y) \to (\infty, \infty)$ . I tried two different paths: $y=x$ : We have the limit of the above function $\lim (x^2+x^4)/(x^4+x^2)=1$ , $y=2x$ : We have $\lim (x^2+16x^4)/(x^4+4x^2)=16\neq1$ . So I concluded that the limit does not exist. Is there a better way to find the limit?","I want to find the limit of as . I tried two different paths: : We have the limit of the above function , : We have . So I concluded that the limit does not exist. Is there a better way to find the limit?","\dfrac{x^2 +y^4}{y^2+x^4} (x,y) \to (\infty, \infty) y=x \lim (x^2+x^4)/(x^4+x^2)=1 y=2x \lim (x^2+16x^4)/(x^4+4x^2)=16\neq1","['calculus', 'limits', 'multivariable-calculus', 'functions']"
52,"compute the limit $\lim_{n\rightarrow\infty} \int_{0}^{\frac \pi 2} \frac{\sin^2(nx)}{1+x} \,dx$",compute the limit,"\lim_{n\rightarrow\infty} \int_{0}^{\frac \pi 2} \frac{\sin^2(nx)}{1+x} \,dx",I've tried using Taylor expansion but that didn't really work out. I'm really stuck and don't know where to begin. I even tried putting it on wolfram alpha but he couldn't solve it either.,I've tried using Taylor expansion but that didn't really work out. I'm really stuck and don't know where to begin. I even tried putting it on wolfram alpha but he couldn't solve it either.,,"['integration', 'limits']"
53,Maximum of function of three variables on the unit cube,Maximum of function of three variables on the unit cube,,"I am looking to prove that the function $$f(x,y,z) = \frac{x(1-x)y(1-y)z(1-z)}{1 - (1-xy)z}$$ attains it's maximum at a point within the unit cube $[0,1] \times [0,1] \times [0,1]$ and NOT on the boundary. Wolfram Alpha confirms that the max is attained not on the boundary, but actually within the cube. I have been able to show that there is a local maximum which gives this value inside the cube, but I can't see how to show that the values at the boundary don't go to infinity. For example, if we approach the vertex $(x,y,z) = (0,0,1)$ we get $0$ over $0$ . How do I know that the function doesn't go to infinity as we tend to this vertex? Many thanks in advance.","I am looking to prove that the function attains it's maximum at a point within the unit cube and NOT on the boundary. Wolfram Alpha confirms that the max is attained not on the boundary, but actually within the cube. I have been able to show that there is a local maximum which gives this value inside the cube, but I can't see how to show that the values at the boundary don't go to infinity. For example, if we approach the vertex we get over . How do I know that the function doesn't go to infinity as we tend to this vertex? Many thanks in advance.","f(x,y,z) = \frac{x(1-x)y(1-y)z(1-z)}{1 - (1-xy)z} [0,1] \times [0,1] \times [0,1] (x,y,z) = (0,0,1) 0 0","['real-analysis', 'calculus', 'limits']"
54,Is my evaluation of this limit correct/sufficient?,Is my evaluation of this limit correct/sufficient?,,"$g(x) = \begin{cases} \frac{1}{b} \text{ if } x^2 = \frac{a}{b} \in \mathbb{Q} \text{ in lowest terms}\\ 0 \text{ if } x^2 \notin \mathbb{Q} \end{cases}$ Evaluate $\displaystyle{\lim_{x \to 0} g(x)}$ Here's what I've done to evaluate the limit. Is this correct/sufficient to show that the limit as $x$ approaches $0$ is $0$ ? $\displaystyle{\lim_{x \to 0} 0 = 0}$ $\displaystyle{\lim_{x \to 0} x^2 = 0}$ $\forall x \in \mathbb{Q} \text{, } 0<x<1 \text{, } 0 < \frac{1}{b} \leq \frac{a}{b} = x^2$ $\Rightarrow$ by the Squeeze Theorem, the limit of $g(x)$ as $x \rightarrow 0^+$ is $0$ Edit: Fixed formatting","Evaluate Here's what I've done to evaluate the limit. Is this correct/sufficient to show that the limit as approaches is ? by the Squeeze Theorem, the limit of as is Edit: Fixed formatting","g(x) = \begin{cases} \frac{1}{b} \text{ if } x^2 = \frac{a}{b} \in \mathbb{Q} \text{ in lowest terms}\\ 0 \text{ if } x^2 \notin \mathbb{Q} \end{cases} \displaystyle{\lim_{x \to 0} g(x)} x 0 0 \displaystyle{\lim_{x \to 0} 0 = 0} \displaystyle{\lim_{x \to 0} x^2 = 0} \forall x \in \mathbb{Q} \text{, } 0<x<1 \text{, } 0 < \frac{1}{b} \leq \frac{a}{b} = x^2 \Rightarrow g(x) x \rightarrow 0^+ 0","['limits', 'proof-verification']"
55,Absolute Value within an Epsilon Delta Problem,Absolute Value within an Epsilon Delta Problem,,"I'm having trouble with an epsilon-delta proof. I'm asked to prove the limit as $x$ goes to $-2$ of $f(x)=|x^2-9|/(x^2+3x+1)$ is $-5$ , so we have $|(|x^2-9|/(x^2+3x+1))-5|<ϵ$ . I've seen that $|x^2-9|$ is $|x-3||x+3|$ but I'm not really sure how to proceed to  ""obtain"" the desired $|x-a|=|x+2|<δ$ . Am I supposed to place a limitation on either $|x-3|$ or $|x+3|$ ? Or is there some other way to handle this problem? Thanks in advance.","I'm having trouble with an epsilon-delta proof. I'm asked to prove the limit as goes to of is , so we have . I've seen that is but I'm not really sure how to proceed to  ""obtain"" the desired . Am I supposed to place a limitation on either or ? Or is there some other way to handle this problem? Thanks in advance.",x -2 f(x)=|x^2-9|/(x^2+3x+1) -5 |(|x^2-9|/(x^2+3x+1))-5|<ϵ |x^2-9| |x-3||x+3| |x-a|=|x+2|<δ |x-3| |x+3|,"['real-analysis', 'limits', 'epsilon-delta']"
56,"Given $\lim_{n \rightarrow \infty} a_n a_{n+1} = L$, how to show: $\lim_{n \rightarrow \infty} a_n a_{n+3} = L$","Given , how to show:",\lim_{n \rightarrow \infty} a_n a_{n+1} = L \lim_{n \rightarrow \infty} a_n a_{n+3} = L,let $\{ a_n \}$ be a sequence where for each $n \in \mathbb N$ $ a_n \neq 0 $ and where $\lim_{n \rightarrow \infty} a_n a_{n+1} = L$ with $L \neq 0$ I want to prove that $\lim_{n \rightarrow \infty} a_n a_{n+3} = L$ and that $\lim_{n \rightarrow \infty} a_n a_{n+2} \neq -1$ Any ideas? Thanks! Edit : Intuitively it's clear but I am looking for a real regorous proof..,let be a sequence where for each and where with I want to prove that and that Any ideas? Thanks! Edit : Intuitively it's clear but I am looking for a real regorous proof..,\{ a_n \} n \in \mathbb N  a_n \neq 0  \lim_{n \rightarrow \infty} a_n a_{n+1} = L L \neq 0 \lim_{n \rightarrow \infty} a_n a_{n+3} = L \lim_{n \rightarrow \infty} a_n a_{n+2} \neq -1,"['sequences-and-series', 'limits', 'limsup-and-liminf']"
57,Limits: factoring out $x$ from $\lim_\limits{x\to +\infty}\left(\frac{5-x^3}{8x+2}\right)$,Limits: factoring out  from,x \lim_\limits{x\to +\infty}\left(\frac{5-x^3}{8x+2}\right),"So my teacher said that I cannot use arithmetic operation to factor out $x$ from this type of equation, saying that it's because it's composed only by addition and subtraction. But I don't understand clearly, because I get the right answer (according to the book): $$\lim_{x\to+\infty}\left(\frac{5-x^3}{8x+2}\right) =\lim_{x\to\infty}\frac{x\times\left(\frac{5}{x}-x^2\right)}{x\times\left(8+\frac{2}{x}\right)} =\lim_{x\to\infty}\frac{\frac{5}{x}-x^2}{8+\frac{2}{x}} =\frac{\lim_\limits{x\to\infty}\left(\frac{5}{x}-x^2\right)}{\lim_\limits{x\to\infty}\left(8+\frac{2}{x}\right)} =\frac{\lim_\limits{x\to\infty}\left(\frac{5}{x}\right)-\lim_\limits{x\to\infty}\left(x^2\right)}{\lim_\limits{x\to\infty}\left(8\right)+\lim_\limits{x\to\infty}\left(\frac{2}{x}\right)} =\frac{0-\infty}{8+0} =\frac{-\infty}{8}$$ Applying the infinity property: $\frac{-\infty}{-c}=\infty$ $=-\infty$ Can someone explain to me why I can't factor $x$ out?","So my teacher said that I cannot use arithmetic operation to factor out from this type of equation, saying that it's because it's composed only by addition and subtraction. But I don't understand clearly, because I get the right answer (according to the book): Applying the infinity property: Can someone explain to me why I can't factor out?","x \lim_{x\to+\infty}\left(\frac{5-x^3}{8x+2}\right)
=\lim_{x\to\infty}\frac{x\times\left(\frac{5}{x}-x^2\right)}{x\times\left(8+\frac{2}{x}\right)}
=\lim_{x\to\infty}\frac{\frac{5}{x}-x^2}{8+\frac{2}{x}}
=\frac{\lim_\limits{x\to\infty}\left(\frac{5}{x}-x^2\right)}{\lim_\limits{x\to\infty}\left(8+\frac{2}{x}\right)}
=\frac{\lim_\limits{x\to\infty}\left(\frac{5}{x}\right)-\lim_\limits{x\to\infty}\left(x^2\right)}{\lim_\limits{x\to\infty}\left(8\right)+\lim_\limits{x\to\infty}\left(\frac{2}{x}\right)}
=\frac{0-\infty}{8+0}
=\frac{-\infty}{8} \frac{-\infty}{-c}=\infty =-\infty x","['calculus', 'limits', 'proof-writing', 'limits-without-lhopital']"
58,Does $\lim_{x\rightarrow0}\left(\frac{f(x) - f(0)}{x}\right) = \lim_{\frac{x}{3}\rightarrow0}\left(\frac{f(x) - f(0)}{x}\right)$?,Does ?,\lim_{x\rightarrow0}\left(\frac{f(x) - f(0)}{x}\right) = \lim_{\frac{x}{3}\rightarrow0}\left(\frac{f(x) - f(0)}{x}\right),"Given that $ f'(0) = 3$ , I need to solve $$\lim_{x\rightarrow0}\left(\frac{f(3x) - f(0)}{x}\right)$$ Because I know that $\lim_{x\rightarrow0}\left(\frac{f(x) - f(0)}{x}\right) = 3$ , I made the substitution $3x = h$ . Thus, $$\lim_{x\rightarrow0}\left(\frac{f(3x) - f(0)}{x}\right) = 3 \lim_{\frac{h}{3}\rightarrow0}\left(\frac{f(h) - f(0)}{h}\right)$$ If, in fact the following is true, then I can conclude that $\lim_{x\rightarrow0}\left(\frac{f(3x) - f(0)}{x}\right) = 9$ , and my work is done. $$\lim_{x\rightarrow0}\left(\frac{f(x) - f(0)}{x}\right) = \lim_{\frac{x}{3}\rightarrow0}\left(\frac{f(x) - f(0)}{x}\right)\,?$$","Given that , I need to solve Because I know that , I made the substitution . Thus, If, in fact the following is true, then I can conclude that , and my work is done."," f'(0) = 3 \lim_{x\rightarrow0}\left(\frac{f(3x) - f(0)}{x}\right) \lim_{x\rightarrow0}\left(\frac{f(x) - f(0)}{x}\right) = 3 3x = h \lim_{x\rightarrow0}\left(\frac{f(3x) - f(0)}{x}\right) = 3 \lim_{\frac{h}{3}\rightarrow0}\left(\frac{f(h) - f(0)}{h}\right) \lim_{x\rightarrow0}\left(\frac{f(3x) - f(0)}{x}\right) = 9 \lim_{x\rightarrow0}\left(\frac{f(x) - f(0)}{x}\right) = \lim_{\frac{x}{3}\rightarrow0}\left(\frac{f(x) - f(0)}{x}\right)\,?","['calculus', 'limits', 'derivatives']"
59,Double limit $\lim_{x\rightarrow0} \lim_{y\rightarrow x} \frac{y^2 f(x) - x^2 f(y)}{(1-\cos x)\sin (x-y)}$,Double limit,\lim_{x\rightarrow0} \lim_{y\rightarrow x} \frac{y^2 f(x) - x^2 f(y)}{(1-\cos x)\sin (x-y)},"Let $I$ be an open interval containing $0$ , and $f: I \rightarrow R$ be differentiable and $f'$ be continuous. Compute: $$\lim_{x\rightarrow0} \lim_{y\rightarrow x} \frac{y^2 f(x) - x^2 f(y)}{(1-\cos x)\sin (x-y)}$$ This is my work: $$\lim_{x\rightarrow0} \lim_{y\rightarrow x} \frac{y^2 x^2 (\frac{f(x)}{x^2}-\frac{f(y)}{y^2})(x-y)}{(1-\cos x)(x-y)\sin(x-y)}$$ $$ \lim_{x\rightarrow0} \frac{x^4}{1 - \cos x} \frac{xf'(x) - 2f(x)}{x^3}$$ $$ \lim_{x\rightarrow0}\frac{x}{1-\cos x} (xf'(x) - 2f(x)) $$ But it doesn't feel right because I'm ""missing"" one power of x to have $\frac{x}{1-\cos x}$ converge. Can anyone help me?","Let be an open interval containing , and be differentiable and be continuous. Compute: This is my work: But it doesn't feel right because I'm ""missing"" one power of x to have converge. Can anyone help me?",I 0 f: I \rightarrow R f' \lim_{x\rightarrow0} \lim_{y\rightarrow x} \frac{y^2 f(x) - x^2 f(y)}{(1-\cos x)\sin (x-y)} \lim_{x\rightarrow0} \lim_{y\rightarrow x} \frac{y^2 x^2 (\frac{f(x)}{x^2}-\frac{f(y)}{y^2})(x-y)}{(1-\cos x)(x-y)\sin(x-y)}  \lim_{x\rightarrow0} \frac{x^4}{1 - \cos x} \frac{xf'(x) - 2f(x)}{x^3}  \lim_{x\rightarrow0}\frac{x}{1-\cos x} (xf'(x) - 2f(x))  \frac{x}{1-\cos x},"['real-analysis', 'calculus', 'limits']"
60,References for solid of revolution of a region which crosses the axis of revolution?,References for solid of revolution of a region which crosses the axis of revolution?,,"Same question as this but for surface area instead of volume: Volume of revolution on an area crossing the axis 1. Is this correct to compute the volume of the solid of revolution? To compute the volume of the solid of revolution obtained by revolving the area of the region (just the regular use of the word 'region' and not region in topology ) between functions $f$ and $g$ and between $x=a$ and $x=b$ around $x$ -axis, where $a<b$ and where the region crosses the $x$ -axis, we use the function $h := \max\{|f|,|g|\}$ $$V = \pi \int_{a}^{b} (h(x))^2 dx = \pi \int_{a}^{b} (\max\{|f(x)|,|g(x)|\})^2 dx$$ $$= \pi \sum_{i=0}^{n-1}\int_{p_i}^{p_{i+1}} (\max\{|f(x)|,|g(x)|\})^2 dx$$ over some partition $\{p_0,...p_{n}\}$ of $[a,b]$ where $h(x)$ changes from one element of the partition to the next. 2. To compute the surface area of the solid of revolution, what $h$ do we use? To compute the surface area of the solid of revolution obtained by revolving the area of the region (just the regular use of the word 'region' and not region in topology ) between functions $f$ and $g$ and between $x=a$ and $x=b$ around $x$ -axis, where $a<b$ and where the region crosses the $x$ -axis, what $h$ do we use ? $$SA = 2 \pi \int_{a}^{b} h(x)\sqrt{1+(h'(x))^2} dx = 2 \pi \int_{a}^{b} ? \sqrt{1+(\frac{d}{dx}?)^2} dx $$ Is it still $\max$ ? Of course, I'm assuming overlap is still a problem in surface areas of solids of revolution just as it is a problem in computing volumes of solids of of revolution. If there's an answer out there, then you don't have to justify the answer: please just link to where I can find the answer, and I'll understand it on my own. 3. Where can I find examples or even definitions of these? I actually couldn't find any examples in Calculus by James Stewart for either question. To those who have used Stewart, do you happen to know if, in the book, there are any or that probably there aren't any (because you, like me, have tried looking)? To those who haven't used Stewart, where can I find examples please? Here's one example: Why does wolfram answer as such in this example for surface area and volume of revolution on an area crossing the axis?","Same question as this but for surface area instead of volume: Volume of revolution on an area crossing the axis 1. Is this correct to compute the volume of the solid of revolution? To compute the volume of the solid of revolution obtained by revolving the area of the region (just the regular use of the word 'region' and not region in topology ) between functions and and between and around -axis, where and where the region crosses the -axis, we use the function over some partition of where changes from one element of the partition to the next. 2. To compute the surface area of the solid of revolution, what do we use? To compute the surface area of the solid of revolution obtained by revolving the area of the region (just the regular use of the word 'region' and not region in topology ) between functions and and between and around -axis, where and where the region crosses the -axis, what do we use ? Is it still ? Of course, I'm assuming overlap is still a problem in surface areas of solids of revolution just as it is a problem in computing volumes of solids of of revolution. If there's an answer out there, then you don't have to justify the answer: please just link to where I can find the answer, and I'll understand it on my own. 3. Where can I find examples or even definitions of these? I actually couldn't find any examples in Calculus by James Stewart for either question. To those who have used Stewart, do you happen to know if, in the book, there are any or that probably there aren't any (because you, like me, have tried looking)? To those who haven't used Stewart, where can I find examples please? Here's one example: Why does wolfram answer as such in this example for surface area and volume of revolution on an area crossing the axis?","f g x=a x=b x a<b x h := \max\{|f|,|g|\} V = \pi \int_{a}^{b} (h(x))^2 dx = \pi \int_{a}^{b} (\max\{|f(x)|,|g(x)|\})^2 dx = \pi \sum_{i=0}^{n-1}\int_{p_i}^{p_{i+1}} (\max\{|f(x)|,|g(x)|\})^2 dx \{p_0,...p_{n}\} [a,b] h(x) h f g x=a x=b x a<b x h SA = 2 \pi \int_{a}^{b} h(x)\sqrt{1+(h'(x))^2} dx = 2 \pi \int_{a}^{b} ? \sqrt{1+(\frac{d}{dx}?)^2} dx  \max","['real-analysis', 'calculus']"
61,Show that recursive sequence is decreasing,Show that recursive sequence is decreasing,,"I'm required to show that the above series is decreasing. However, I encounter a problem when I realize that in the inductive step, I have  a term for a(n) in both the numerator and denominator, which makes it difficult to show that a(n+1) > a(n+2). Any Help would be appreciated.","I'm required to show that the above series is decreasing. However, I encounter a problem when I realize that in the inductive step, I have  a term for a(n) in both the numerator and denominator, which makes it difficult to show that a(n+1) > a(n+2). Any Help would be appreciated.",,['sequences-and-series']
62,Can we multiply both sides of a limit equation?,Can we multiply both sides of a limit equation?,,"Compute $\lim_{x \to -1} f(x)$ for a function $f: \mathbb R \to \mathbb R$ such that $$4 = \lim_{x \to -1} \frac{f(x)+2}{x+1} - \frac{x}{x^2-1} \tag{1}$$ $$ = \lim_{x \to -1} \frac{f(x)+2}{x+1} - \frac{\frac{x}{x-1}}{x+1}$$ $$ = \lim_{x \to -1} \frac{f(x)+2 - \frac{x}{x-1}}{x+1}$$ Solution 1: My approach is that the numerator $f(x)+2 - \frac{x}{x-1}$ must approach zero as $x \to -1$ because the denominator approaches zero as $x \to -1$ and so $\lim_{x \to -1} f(x) = -\frac{3}{2}$ . Are we allowed to do the following, which seems to be the required solution, instead ? This does not seem very rigorous, and I have a feeling there are obvious counterexamples. Of course, we can use $\varepsilon-\delta$ to check our answer, but I would like to know if and how this can be generalised for any function $f: \mathbb R \to \mathbb R$ . Solution 2: Observe $$0 = \lim_{x \to -1} x+1$$ Then $$4 = \lim_{x \to -1} \frac{f(x)+2 - \frac{x}{x-1}}{x+1}$$ $$\implies 0 = 0 \cdot 4 = (\lim_{x \to -1} x+1) \cdot 4 = \lim_{x \to -1} \frac{f(x)+2 - \frac{x}{x-1}}{x+1} \lim_{x \to -1} x+1$$ $$ = \lim_{x \to -1} \frac{f(x)+2 - \frac{x}{x-1}}{x+1} x+1 = \lim_{x \to -1} (f(x)+2 - \frac{x}{x-1})$$ $$ = \lim_{x \to -1} f(x) + \lim_{x \to -1} 2 - \lim_{x \to -1} \frac{x}{x-1} = \lim_{x \to -1} f(x)+2 - \frac12 = \lim_{x \to -1} f(x)+ \frac32$$ $$\implies - \frac32 = \lim_{x \to -1} f(x) \tag{2}$$ I suspect the preceding solution is not rigorous, and this is a case where we have only a guess and must check our answer by proving $(1)$ , by computation since $\varepsilon-\delta$ is actually not yet allowed, assuming $(2)$ , so if one does then preceding solution, then one must follow up with a computation. I'm not quite sure what the problem is, but it might be that we don't know $\lim_{x \to -1} f(x)$ exists in the first place. Is the fact that the domain and range are both $\mathbb R$ relevant? I think of a counterexample like $f: \{7,8,10\} \to \{1,2\}$ or $f: C \to \mathbb Q^c$ where $C$ is the Cantor set.","Compute for a function such that Solution 1: My approach is that the numerator must approach zero as because the denominator approaches zero as and so . Are we allowed to do the following, which seems to be the required solution, instead ? This does not seem very rigorous, and I have a feeling there are obvious counterexamples. Of course, we can use to check our answer, but I would like to know if and how this can be generalised for any function . Solution 2: Observe Then I suspect the preceding solution is not rigorous, and this is a case where we have only a guess and must check our answer by proving , by computation since is actually not yet allowed, assuming , so if one does then preceding solution, then one must follow up with a computation. I'm not quite sure what the problem is, but it might be that we don't know exists in the first place. Is the fact that the domain and range are both relevant? I think of a counterexample like or where is the Cantor set.","\lim_{x \to -1} f(x) f: \mathbb R \to \mathbb R 4 = \lim_{x \to -1} \frac{f(x)+2}{x+1} - \frac{x}{x^2-1} \tag{1}  = \lim_{x \to -1} \frac{f(x)+2}{x+1} - \frac{\frac{x}{x-1}}{x+1}  = \lim_{x \to -1} \frac{f(x)+2 - \frac{x}{x-1}}{x+1} f(x)+2 - \frac{x}{x-1} x \to -1 x \to -1 \lim_{x \to -1} f(x) = -\frac{3}{2} \varepsilon-\delta f: \mathbb R \to \mathbb R 0 = \lim_{x \to -1} x+1 4 = \lim_{x \to -1} \frac{f(x)+2 - \frac{x}{x-1}}{x+1} \implies 0 = 0 \cdot 4 = (\lim_{x \to -1} x+1) \cdot 4 = \lim_{x \to -1} \frac{f(x)+2 - \frac{x}{x-1}}{x+1} \lim_{x \to -1} x+1  = \lim_{x \to -1} \frac{f(x)+2 - \frac{x}{x-1}}{x+1} x+1 = \lim_{x \to -1} (f(x)+2 - \frac{x}{x-1})  = \lim_{x \to -1} f(x) + \lim_{x \to -1} 2 - \lim_{x \to -1} \frac{x}{x-1} = \lim_{x \to -1} f(x)+2 - \frac12 = \lim_{x \to -1} f(x)+ \frac32 \implies - \frac32 = \lim_{x \to -1} f(x) \tag{2} (1) \varepsilon-\delta (2) \lim_{x \to -1} f(x) \mathbb R f: \{7,8,10\} \to \{1,2\} f: C \to \mathbb Q^c C","['real-analysis', 'calculus']"
63,"Proof verification for $\lim \inf x_n + \lim \sup y_n \le \lim \sup(x_n + y_n)$ for bounded $x_n, y_n$",Proof verification for  for bounded,"\lim \inf x_n + \lim \sup y_n \le \lim \sup(x_n + y_n) x_n, y_n","Let $\{x_n\}$ and $\{y_n\}$ denote two bounded sequences. Prove that: $$ \lim \inf x_n + \lim \sup y_n \le \lim \sup(x_n + y_n) \\ $$ We know that both $x_n$ and $y_n$ are bounded hence is their sum: $$ m \le x_n + y_n < M $$ Using that fact we may choose a subsequence in order to satisfy the following: $$ \lim(x_{n_k} + y_{n_k}) = \lim\sup(x_n + y_n) \tag1 $$ Since $x_{n_k}$ is bounded (as far as $x_n$ is) lets choose a convergent subsequence with indices $n^\prime_k \ge n_k$ such that: $$ \exists \lim x_{n^\prime_k} $$ Now consider a sequence $y_{n^\prime_k}$ (note the index is $n^\prime_k$ ), since it is bounded we may choose a convergent subsequence from $y_{n^\prime_k}$ with indices $n^{\prime\prime}_k \ge n^\prime_k$ such that: $$ \exists\lim y_{n^{\prime\prime}_k} $$ Since $\{x_{n^{\prime\prime}_k}\}$ is a subsequence of $\{x_{n^\prime_k}\}$ it is convergent to the same limit. Also $\{y_{n^{\prime\prime}_k}\}$ is a subsequence of $\{y_{n^\prime_k}\}$ and we've chosen $\{y_{n^{\prime\prime}_k}\}$ to be convergent. Based on that and on $(1)$ we may write: $$ \lim(x_{n^{\prime\prime}_k} + y_{n^{\prime\prime}_k}) = \lim(x_{n_k} + y_{n_k}) = \lim\sup (x_n + y_n)  \tag 2 $$ By definition of limsup and liminf: $$ \lim x_{n^{\prime\prime}_k} \ge \lim\inf x_n \\ \lim y_{n^{\prime\prime}_k} \le \lim\sup y_n $$ Or (multiply second inequality by $-1$ ): $$ \lim x_{n^{\prime\prime}_k} \ge \lim\inf x_n \\ -\lim y_{n^{\prime\prime}_k} \ge -\lim\sup y_n $$ Subtract the inequalities: $$ \lim x_{n^{\prime\prime}_k} + \lim y_{n^{\prime\prime}_k} \ge \lim\inf x_n + \lim\sup y_n \tag3 $$ Limit of sum is just a sum of limits so: $$ \lim(x_{n^{\prime\prime}_k} + y_{n^{\prime\prime}_k}) =\lim x_{n^{\prime\prime}_k} + \lim y_{n^{\prime\prime}_k} $$ So now using $(2)$ and $(3)$ we conclude that: $$ \lim \sup(x_n + y_n) \ge \lim\inf x_n + \lim\sup y_n $$ Is this argument enough to consider the proof complete?","Let and denote two bounded sequences. Prove that: We know that both and are bounded hence is their sum: Using that fact we may choose a subsequence in order to satisfy the following: Since is bounded (as far as is) lets choose a convergent subsequence with indices such that: Now consider a sequence (note the index is ), since it is bounded we may choose a convergent subsequence from with indices such that: Since is a subsequence of it is convergent to the same limit. Also is a subsequence of and we've chosen to be convergent. Based on that and on we may write: By definition of limsup and liminf: Or (multiply second inequality by ): Subtract the inequalities: Limit of sum is just a sum of limits so: So now using and we conclude that: Is this argument enough to consider the proof complete?","\{x_n\} \{y_n\} 
\lim \inf x_n + \lim \sup y_n \le \lim \sup(x_n + y_n) \\
 x_n y_n 
m \le x_n + y_n < M
 
\lim(x_{n_k} + y_{n_k}) = \lim\sup(x_n + y_n) \tag1
 x_{n_k} x_n n^\prime_k \ge n_k 
\exists \lim x_{n^\prime_k}
 y_{n^\prime_k} n^\prime_k y_{n^\prime_k} n^{\prime\prime}_k \ge n^\prime_k 
\exists\lim y_{n^{\prime\prime}_k}
 \{x_{n^{\prime\prime}_k}\} \{x_{n^\prime_k}\} \{y_{n^{\prime\prime}_k}\} \{y_{n^\prime_k}\} \{y_{n^{\prime\prime}_k}\} (1) 
\lim(x_{n^{\prime\prime}_k} + y_{n^{\prime\prime}_k}) = \lim(x_{n_k} + y_{n_k}) = \lim\sup (x_n + y_n)  \tag 2
 
\lim x_{n^{\prime\prime}_k} \ge \lim\inf x_n \\
\lim y_{n^{\prime\prime}_k} \le \lim\sup y_n
 -1 
\lim x_{n^{\prime\prime}_k} \ge \lim\inf x_n \\
-\lim y_{n^{\prime\prime}_k} \ge -\lim\sup y_n
 
\lim x_{n^{\prime\prime}_k} + \lim y_{n^{\prime\prime}_k} \ge \lim\inf x_n + \lim\sup y_n \tag3
 
\lim(x_{n^{\prime\prime}_k} + y_{n^{\prime\prime}_k}) =\lim x_{n^{\prime\prime}_k} + \lim y_{n^{\prime\prime}_k}
 (2) (3) 
\lim \sup(x_n + y_n) \ge \lim\inf x_n + \lim\sup y_n
","['calculus', 'sequences-and-series', 'limits', 'proof-verification', 'limsup-and-liminf']"
64,"Infinite balls, infinite groups with finite number of balls of each value","Infinite balls, infinite groups with finite number of balls of each value",,"You have N balls of value 0, N of value 1, N of value 2, etc., N of each value up to infinity. They are separated into a group of X balls with total value 1, another group of X with total value 2, another of X with value 3, etc., one different group of X for each total value up to infinity. What is the smallest N for X=4 to be possible? What is the general equation for any value X?","You have N balls of value 0, N of value 1, N of value 2, etc., N of each value up to infinity. They are separated into a group of X balls with total value 1, another group of X with total value 2, another of X with value 3, etc., one different group of X for each total value up to infinity. What is the smallest N for X=4 to be possible? What is the general equation for any value X?",,"['sequences-and-series', 'combinatorics', 'limits']"
65,Finding a limit involving roots without derivatives,Finding a limit involving roots without derivatives,,"I need to find the following limit $$ \lim_{x\to-1}\frac{1+x^{1/7}}{1+x^{1/5}} $$ using no derivatives. I've tried attempting to rationalize or divide by certain polynomials, but nothing has worked. It's simple using L'Hopital's rule, but that involves derivatives. (I'm also wondering if the technique used to evaluate that limit could be extended to $$ \lim_{x\to-1}\frac{1+x^{1/m}}{1+x^{1/n}} = \frac{n}{m} \qquad m,n\text{ odd} $$ but that's not the main question.)","I need to find the following limit using no derivatives. I've tried attempting to rationalize or divide by certain polynomials, but nothing has worked. It's simple using L'Hopital's rule, but that involves derivatives. (I'm also wondering if the technique used to evaluate that limit could be extended to but that's not the main question.)","
\lim_{x\to-1}\frac{1+x^{1/7}}{1+x^{1/5}}
 
\lim_{x\to-1}\frac{1+x^{1/m}}{1+x^{1/n}} = \frac{n}{m} \qquad m,n\text{ odd}
","['real-analysis', 'calculus', 'limits', 'limits-without-lhopital']"
66,"Let $(a_n), (b_n),$ be bounded, then prove that $c_n$ converges and give its value.","Let  be bounded, then prove that  converges and give its value.","(a_n), (b_n), c_n","Could I get some feedback on the following proof, I feel becoming a good mathematician is through constant feedback and improvement of your work, I tried to make it short, but well-readable. We are given that $a_n$ and $b_n$ are bounded sequences, and also: $$ (n-1)a_n \leq n^2 c_n \leq (n+1)b_n$$ Prove that $c_n$ converges and give the value of the limit. First of all, we know that these sequences are bounded, so surely we have that: $$L \geq a_n \land b_n \leq U$$ For some lower bound $L$ and some upper bound $U$ in $\mathbb{R}$ . We now apply this: $$ (n-1) L \leq (n-1)a_n \leq n^2 c_n \leq (n+1)b_n \leq  (n+1)U$$ $$ \frac{L}{n}-\frac{L}{n^2}=\frac{(n-1) L}{n^2}  \leq  c_n\leq \frac{(n+1) U}{n^2} =\frac{U}{n}+\frac{U}{n^2}$$ Both these sequence converge to zero, so we have that in the limit: $$0 \leq \lim_{n\rightarrow \infty} c_n \leq 0$$ By the squeeze theorem we have that $\lim_{n\rightarrow \infty} c_n =0$","Could I get some feedback on the following proof, I feel becoming a good mathematician is through constant feedback and improvement of your work, I tried to make it short, but well-readable. We are given that and are bounded sequences, and also: Prove that converges and give the value of the limit. First of all, we know that these sequences are bounded, so surely we have that: For some lower bound and some upper bound in . We now apply this: Both these sequence converge to zero, so we have that in the limit: By the squeeze theorem we have that",a_n b_n  (n-1)a_n \leq n^2 c_n \leq (n+1)b_n c_n L \geq a_n \land b_n \leq U L U \mathbb{R}  (n-1) L \leq (n-1)a_n \leq n^2 c_n \leq (n+1)b_n \leq  (n+1)U  \frac{L}{n}-\frac{L}{n^2}=\frac{(n-1) L}{n^2}  \leq  c_n\leq \frac{(n+1) U}{n^2} =\frac{U}{n}+\frac{U}{n^2} 0 \leq \lim_{n\rightarrow \infty} c_n \leq 0 \lim_{n\rightarrow \infty} c_n =0,"['real-analysis', 'sequences-and-series']"
67,"Show that $\lim_n \sum_{k=1}^n\frac{B_k}{k!}\,\frac{n^\underline{k-1}}{n^{k-1}}=\sum_{k=1}^\infty\frac{B_k}{k!}$",Show that,"\lim_n \sum_{k=1}^n\frac{B_k}{k!}\,\frac{n^\underline{k-1}}{n^{k-1}}=\sum_{k=1}^\infty\frac{B_k}{k!}","I need to show that $$\lim_{n\to\infty} \sum_{k=1}^n\frac{B_k}{k!}\,\frac{n^\underline{k-1}}{n^{k-1}}=\sum_{k=1}^\infty\frac{B_k}{k!}$$ where $n^\underline{k-1}:=\prod_{j=0}^{k-2}(n-j)$ is a falling factorial and the $B_k$ are the Bernoulli numbers, and I know that the RHS of above converges to $1/(e-1)$ . I had two attempts: 1) First I tried to use the dominated convergence theorem setting $a_n(k):=\frac{B_k}{k!}\,\frac{n^\underline{k-1}}{n^{k-1}}\chi_{[1,n]}(k)$ , then clearly $\lim_n a_n(k)=B_k/k!$ for each $k\in\Bbb N_{\ge 1}$ , however I dont know if $\sum_{k=1}^\infty|B_k|/k!$ converges, and I dont know any absolutely convergent series that dominates, so Im stuck at this step. 2) A more elementary approach $$\left|\sum_{k=1}^\infty a_n(k)-\sum_{k=1}^\infty\frac{B_k}{k!}\right|\le\left|\sum_{k=1}^M(a_n(k)-B_k/k!)\right|+\sum_{k=M+1}^n\left|1-\frac{n^\underline{k-1}}{n^{k-1}}\right|+\left|\sum_{k=n+1}^\infty\frac{B_k}{k!}\right|$$ such that $|B_k/k!|<1$ for $k\ge M+1$ . Then taking limits above we have that $$\lim_{n\to\infty}\left|\sum_{k=1}^\infty\left(a_n(k)-\frac{B_k}{k!}\right)\right|\le\lim_{n\to\infty}\sum_{k=M+1}^n\left(1-\frac{n^\underline{k-1}}{n^{k-1}}\right)$$ for any fixed enough large $M$ . Then if I can show that for each $\epsilon>0$ there is some $M\in\Bbb N$ such that $$\lim_{n\to\infty}\sum_{k=M+1}^n\left(1-\frac{n^\underline{k-1}}{n^{k-1}}\right)<\epsilon$$ then Im done. However it is not clear how to accomplish (or if it is possible) this task. I thought about use the Stirling approximation on $n^\underline{k-1}/n^{k-1}$ , however it is not clear that I can apply an asymptotic expression inside a series, so Im again stuck. There is some easy way (the more elementary the better) to show the converge of the limit of the title? Thank you.","I need to show that where is a falling factorial and the are the Bernoulli numbers, and I know that the RHS of above converges to . I had two attempts: 1) First I tried to use the dominated convergence theorem setting , then clearly for each , however I dont know if converges, and I dont know any absolutely convergent series that dominates, so Im stuck at this step. 2) A more elementary approach such that for . Then taking limits above we have that for any fixed enough large . Then if I can show that for each there is some such that then Im done. However it is not clear how to accomplish (or if it is possible) this task. I thought about use the Stirling approximation on , however it is not clear that I can apply an asymptotic expression inside a series, so Im again stuck. There is some easy way (the more elementary the better) to show the converge of the limit of the title? Thank you.","\lim_{n\to\infty} \sum_{k=1}^n\frac{B_k}{k!}\,\frac{n^\underline{k-1}}{n^{k-1}}=\sum_{k=1}^\infty\frac{B_k}{k!} n^\underline{k-1}:=\prod_{j=0}^{k-2}(n-j) B_k 1/(e-1) a_n(k):=\frac{B_k}{k!}\,\frac{n^\underline{k-1}}{n^{k-1}}\chi_{[1,n]}(k) \lim_n a_n(k)=B_k/k! k\in\Bbb N_{\ge 1} \sum_{k=1}^\infty|B_k|/k! \left|\sum_{k=1}^\infty a_n(k)-\sum_{k=1}^\infty\frac{B_k}{k!}\right|\le\left|\sum_{k=1}^M(a_n(k)-B_k/k!)\right|+\sum_{k=M+1}^n\left|1-\frac{n^\underline{k-1}}{n^{k-1}}\right|+\left|\sum_{k=n+1}^\infty\frac{B_k}{k!}\right| |B_k/k!|<1 k\ge M+1 \lim_{n\to\infty}\left|\sum_{k=1}^\infty\left(a_n(k)-\frac{B_k}{k!}\right)\right|\le\lim_{n\to\infty}\sum_{k=M+1}^n\left(1-\frac{n^\underline{k-1}}{n^{k-1}}\right) M \epsilon>0 M\in\Bbb N \lim_{n\to\infty}\sum_{k=M+1}^n\left(1-\frac{n^\underline{k-1}}{n^{k-1}}\right)<\epsilon n^\underline{k-1}/n^{k-1}","['real-analysis', 'limits', 'convergence-divergence']"
68,Trying to prove $e$'s irrationality,Trying to prove 's irrationality,e,"Knowing that $\lim\limits_{x\to\ 0}\ $$\frac{\sin(x)}{x}$$= 1$ , $\frac{1}{n+1}<n!r_n<\frac{1}{n}$ , where $r_n=e- \sum _{ k=0 }^{ \ n}{ \frac { 1 }{k!}} $ By studying $\lim\limits_{n\to\infty}\ n\sin(2πn!r_n)$ I have to show that : $$\lim\limits_{n\to\infty}\ n\sin(2πn!e)= 2π,$$ and then prove that $e$ is irrational ?","Knowing that , , where By studying I have to show that : and then prove that is irrational ?","\lim\limits_{x\to\ 0}\ \frac{\sin(x)}{x}= 1 \frac{1}{n+1}<n!r_n<\frac{1}{n} r_n=e- \sum _{ k=0 }^{ \ n}{ \frac { 1 }{k!}}  \lim\limits_{n\to\infty}\ n\sin(2πn!r_n) \lim\limits_{n\to\infty}\ n\sin(2πn!e)= 2π, e","['limits', 'exponential-function', 'irrational-numbers']"
69,Showing lim xn = c,Showing lim xn = c,,"Can anybody help with with (3) My Solution for (1) and (2) Put $g(x):=f(x)-x$ , which is still continuous on $[a,b]$ and differentiable on $(a,b)$ . Observe that $x\in [a,b]$ is a fixed point for $f$ iff $g(x)=0$ . Now if $u<v$ are two distinct fixed points we can apply Rolle's theorem (since $g$ is differentiable on $(u,v)\subseteq (a,b)$ ) and find some $\xi\in (u,v)$ such that $g'(\xi)=0$ , i.e. $f'(\xi)=1$ , contradiction. So there is at most one fixed point. The fact that at least one exists is very well-known: if $g(x)\neq 0$ for any $x\in [a,b]$ then $g(x)$ (by continuity) has always the same sign, but $f(a)\ge a$ and $f(b)\le b$ , so $g(a)\ge 0$ and $g(b)\le 0$ , contradiction. Thus for some $x$ we have $g(x)=0$ and $x$ is the desired fixed point.","Can anybody help with with (3) My Solution for (1) and (2) Put , which is still continuous on and differentiable on . Observe that is a fixed point for iff . Now if are two distinct fixed points we can apply Rolle's theorem (since is differentiable on ) and find some such that , i.e. , contradiction. So there is at most one fixed point. The fact that at least one exists is very well-known: if for any then (by continuity) has always the same sign, but and , so and , contradiction. Thus for some we have and is the desired fixed point.","g(x):=f(x)-x [a,b] (a,b) x\in [a,b] f g(x)=0 u<v g (u,v)\subseteq (a,b) \xi\in (u,v) g'(\xi)=0 f'(\xi)=1 g(x)\neq 0 x\in [a,b] g(x) f(a)\ge a f(b)\le b g(a)\ge 0 g(b)\le 0 x g(x)=0 x","['real-analysis', 'limits', 'fixed-points']"
70,"If $\frac{a_n}{a_{n-1}}=\frac{3n-1}{3n}$ and $a_0=1$, find $\lim\limits_{n \to \infty}a_n$","If  and , find",\frac{a_n}{a_{n-1}}=\frac{3n-1}{3n} a_0=1 \lim\limits_{n \to \infty}a_n,"Problem Assume that $\dfrac{a_n}{a_{n-1}}=\dfrac{3n-1}{3n}(n=1,2,\cdots)$ and $a_0=1$ . Find $\lim\limits_{n \to \infty}a_n$ . Attempt Since \begin{align*} a_n=\frac{a_n}{a_0}=\prod_{k=1}^n\frac{a_k}{a_{k-1}}&=\prod_{k=1}^n\frac{3k-1}{3k}=\prod_{k=1}^n\left(1-\frac{1}{3k}\right)=\exp\left[\sum_{k=1}^n\ln\left(1-\frac{1}{3k}\right)\right], \end{align*} hence $$\lim_{n \to \infty}a_n=\exp\left[\sum_{k=1}^{\infty}\ln\left(1-\frac{1}{3k}\right)\right]. $$ But $\sum\limits_{k=1}^{\infty}\ln\left(1-\dfrac{1}{3k}\right)$ is divergent. I'm confused with this result.",Problem Assume that and . Find . Attempt Since hence But is divergent. I'm confused with this result.,"\dfrac{a_n}{a_{n-1}}=\dfrac{3n-1}{3n}(n=1,2,\cdots) a_0=1 \lim\limits_{n \to \infty}a_n \begin{align*}
a_n=\frac{a_n}{a_0}=\prod_{k=1}^n\frac{a_k}{a_{k-1}}&=\prod_{k=1}^n\frac{3k-1}{3k}=\prod_{k=1}^n\left(1-\frac{1}{3k}\right)=\exp\left[\sum_{k=1}^n\ln\left(1-\frac{1}{3k}\right)\right],
\end{align*} \lim_{n \to \infty}a_n=\exp\left[\sum_{k=1}^{\infty}\ln\left(1-\frac{1}{3k}\right)\right].
 \sum\limits_{k=1}^{\infty}\ln\left(1-\dfrac{1}{3k}\right)","['real-analysis', 'sequences-and-series', 'limits']"
71,A question about limits of algebraic functions,A question about limits of algebraic functions,,"I am a high school math teacher, and I recently put the following question on one of my assessments: Find the numbers a and b such that $\lim\limits_{x \to 0}\frac{\sqrt{ax+b}-4}{x}=1$ . The concept I was testing was for students to realize that, since the limit exists but the denominator approaches 0, the numerator must also approach 0, i.e. $\lim\limits_{x \to 0}{\sqrt{ax+b}-4}=0$ . Instead, some of my students made a different argument which seems a bit off to me.  I don't want to tell them that their thinking is correct without being able to supply a justification for it.  Note that, due to constraints in time and curriculum, we don't cover the definition of a limit (but I will gladly take an epsilon-delta proof if someone can supply one).  I'm thinking, more likely, that there is a counter-example and if anyone can supply one I will be grateful. The student argument goes like this: Consider $\lim\limits_{x \to 0}\frac{x}{x}$ We know that that limit has a value of 1. Note that the limit of the denominator is zero, i.e. , $\lim\limits_{x \to 0}{x} = 0$ Since $\lim\limits_{x \to 0}\frac{\sqrt{ax+b}-4}{x}$ has the same value as $\lim\limits_{x \to 0}\frac{x}{x}$ , and since the denominators of both expressions are equal and their limits are equal, we can say that the limit of the numerators must be equal, i.e. $\lim\limits_{x \to 0}{\sqrt{ax+b}-4} = \lim\limits_{x \to 0}{x}$ . From there, they arrive at the same result which I had intended by solving the limit on the right side of the equation.  But something about this argument bothers me and I'm not certain what it is; perhaps it's because it is so specific in that the limit has to be going to zero, both denominators are just the identity function, and the original limit of the algebraic expressions is 1. If I had made the original limit equal to 2, then could they have  used their argument only changing the considered function from $\frac{x}{x}$ to $\frac{2x}{x}$ ?  I feel like...maybe? Anyway, can anyone provide a counter-example to this specific example of a limit approaching zero of a fractional expression whose denominators are identical, and where both approach zero, and the limit of the entire fractional expression is 1?  Is there a counter-example, or can it be proven that this will always work?  And then what about extrapolating it to when the limit of the original is an arbitrary constant k?  And then when the denominators are unequal functions but both approach zero? Thank you everyone for your help!  I'm very big on rigorous explanations in my math classes (to the point where they can be made...I do an optional epsilon-delta lesson after school for those who are interested) and I'd like to either be able to tell my students that they are correct, or show them a counter-example.  Thank you!","I am a high school math teacher, and I recently put the following question on one of my assessments: Find the numbers a and b such that . The concept I was testing was for students to realize that, since the limit exists but the denominator approaches 0, the numerator must also approach 0, i.e. . Instead, some of my students made a different argument which seems a bit off to me.  I don't want to tell them that their thinking is correct without being able to supply a justification for it.  Note that, due to constraints in time and curriculum, we don't cover the definition of a limit (but I will gladly take an epsilon-delta proof if someone can supply one).  I'm thinking, more likely, that there is a counter-example and if anyone can supply one I will be grateful. The student argument goes like this: Consider We know that that limit has a value of 1. Note that the limit of the denominator is zero, i.e. , Since has the same value as , and since the denominators of both expressions are equal and their limits are equal, we can say that the limit of the numerators must be equal, i.e. . From there, they arrive at the same result which I had intended by solving the limit on the right side of the equation.  But something about this argument bothers me and I'm not certain what it is; perhaps it's because it is so specific in that the limit has to be going to zero, both denominators are just the identity function, and the original limit of the algebraic expressions is 1. If I had made the original limit equal to 2, then could they have  used their argument only changing the considered function from to ?  I feel like...maybe? Anyway, can anyone provide a counter-example to this specific example of a limit approaching zero of a fractional expression whose denominators are identical, and where both approach zero, and the limit of the entire fractional expression is 1?  Is there a counter-example, or can it be proven that this will always work?  And then what about extrapolating it to when the limit of the original is an arbitrary constant k?  And then when the denominators are unequal functions but both approach zero? Thank you everyone for your help!  I'm very big on rigorous explanations in my math classes (to the point where they can be made...I do an optional epsilon-delta lesson after school for those who are interested) and I'd like to either be able to tell my students that they are correct, or show them a counter-example.  Thank you!",\lim\limits_{x \to 0}\frac{\sqrt{ax+b}-4}{x}=1 \lim\limits_{x \to 0}{\sqrt{ax+b}-4}=0 \lim\limits_{x \to 0}\frac{x}{x} \lim\limits_{x \to 0}{x} = 0 \lim\limits_{x \to 0}\frac{\sqrt{ax+b}-4}{x} \lim\limits_{x \to 0}\frac{x}{x} \lim\limits_{x \to 0}{\sqrt{ax+b}-4} = \lim\limits_{x \to 0}{x} \frac{x}{x} \frac{2x}{x},"['calculus', 'limits']"
72,Prove the limit superior of a bounded sequence converges,Prove the limit superior of a bounded sequence converges,,"Let $(a_n)_{n=1}^\infty$ be a bounded sequence and $b_n = \sup\{a_k\ |\ k \geq n\}$ . Prove $b_n$ converges. This is the limit superior of $(a_n) := \limsup\ a_n$ . Wanted to see if my proof made sense. Since $a_n$ is bounded, we know $\exists\ M \in \mathbb{R}$ such that $|a_n| \leq M\ \forall n \in \mathbb{N}$ . $(b_n)$ converges to $x \in \mathbb{R}$ if $\forall \epsilon > 0,    \exists\ N \in\ \mathbb{N}\ \forall n \in \mathbb{N}, n \geq N    \implies |b_n-x| < \epsilon$ . Since $(a_n)$ is bounded, by completeness of $\mathbb{R}$ and non empty by assumption, we know that since it has an upper bound, it has a least upperbound, that is the supremum. So we know past some $K$ , $a_k$ = $S$ , where $S$ denotes the supremum of all elements in the sequence. So for sequence is eventually constant, that is constant for $n \geq K$ . And so converges to $S$ .","Let be a bounded sequence and . Prove converges. This is the limit superior of . Wanted to see if my proof made sense. Since is bounded, we know such that . converges to if . Since is bounded, by completeness of and non empty by assumption, we know that since it has an upper bound, it has a least upperbound, that is the supremum. So we know past some , = , where denotes the supremum of all elements in the sequence. So for sequence is eventually constant, that is constant for . And so converges to .","(a_n)_{n=1}^\infty b_n = \sup\{a_k\ |\ k \geq n\} b_n (a_n) := \limsup\ a_n a_n \exists\ M \in \mathbb{R} |a_n| \leq M\ \forall n \in \mathbb{N} (b_n) x \in \mathbb{R} \forall \epsilon > 0,
   \exists\ N \in\ \mathbb{N}\ \forall n \in \mathbb{N}, n \geq N
   \implies |b_n-x| < \epsilon (a_n) \mathbb{R} K a_k S S n \geq K S","['real-analysis', 'sequences-and-series', 'limits', 'proof-verification', 'supremum-and-infimum']"
73,"For $f(x,y) = \frac{xy-1}{x^2 y^2-1}$, what is the limit as $(x,y)$ goes to $(1,1)$?","For , what is the limit as  goes to ?","f(x,y) = \frac{xy-1}{x^2 y^2-1} (x,y) (1,1)","For $f(x,y) = \frac{xy-1}{x^2 y^2-1}$, what is the limit as $(x,y)$ goes to $(1,1)$? Since the denominator can be factored into $(xy-1)(xy+1)$ and then the $(xy-1)$'s in both the numerator and denominator can be cancelled, we are left with $f(x,y) = \frac{1}{xy+1}$ and substituting $x=1$ and $y=1$, the limit becomes $\frac{1}{2}$. Is this solution right? We had this question on our test todayin Multivariable Caclulus, and some people put $1/2$ while others put $DNE$. Who is right?","For $f(x,y) = \frac{xy-1}{x^2 y^2-1}$, what is the limit as $(x,y)$ goes to $(1,1)$? Since the denominator can be factored into $(xy-1)(xy+1)$ and then the $(xy-1)$'s in both the numerator and denominator can be cancelled, we are left with $f(x,y) = \frac{1}{xy+1}$ and substituting $x=1$ and $y=1$, the limit becomes $\frac{1}{2}$. Is this solution right? We had this question on our test todayin Multivariable Caclulus, and some people put $1/2$ while others put $DNE$. Who is right?",,"['limits', 'multivariable-calculus']"
74,Limiting the expected number of geometrically distributed variables I need to look at,Limiting the expected number of geometrically distributed variables I need to look at,,"I am given a series $X_1, X_2, \dots X_n$ of geometrically distributed (with $p = 0.5$), independent random variables, for some $n$. Since there seem to be multiple definitions around: Think of each $X_i$ as the number of coin flips until you see ""heads"". Let $\hat{X} = \max_i(X_i)$. What I want is an upper bound (in $n$) on the expected number of geometrically distributed random variables (with $p = 0.5$) $X'_1, X'_2, X'_m$ that I need to look at before I see a value of at least $\hat{X} + 1$. In other words: I draw $n$ such random variables and determine the maximum. Now, in the second step, I keep on drawing such random variables until I see a value larger than the previously determined variables. How many variables do I expect to draw in the second step? My assumption is: $m \leq 2n$ or $m \le 3n$. I have two approaches of which I feel like they should show this, but for both I'm not sure whether they hold / how to correctly bound $m$. Approach 1 : Symmetry and Handwaving I feel like there should be symmetry at play here. Say I wasn't looking for a value larger than $\hat{X}$, but at least as large as $\hat{X}$. Concatenate both series together to form $X_1, X_2, \dots X_n, X'_1, X'_2, \dots, X_m$. Now, the values of the individual $X_i$ (resp. $X'_i$) are indepentent of my choice of $n$ - so why should the distance ""to the left"" from $X_n$ to $\hat{X}$ be larger than the distance ""to the right"" from $X_n$ to the next element of at least the same value as $\hat{X}$? For symmetry reasons, we should expect $m = n$ here. Now I'm not looking for an element of the same value, but of strictly greater value. Since $P[X_i = \hat{X}] = 2 \cdot P[X_i = \hat{X} + 1]$, the values I'm looking for should be halve as densely distributed… thus we should expect $m = 2n$ … right? At this point I'm waving my hands really hard and hope for the best. Is this approach valid? Would you (as a reviewer / reader) accept this in a paper, if I used it to prove some property of some data structure? Approach 2 : Actually do the Maths One more rigorous approach I thought of is this: From this answer I know that the expected maximum of $n$ such $X_i$ (let's call it $M(\{X_i\})$) is $$E(M(\{X_1, \dots X_n\})) = \sum_{k \geq 0}\left( 1 - \left(1 - \frac{1}{2^k}\right)^n\right)$$ I feel like if I find an $m$ such that $E(M(\{X'_1, \dots X'_m\})) - E(M(\{X_1, \dots X_n\})) \ge 1$, then I should have proven my point - right? I chose $m = 3n$. The resulting sum from the above can then be simplified to: $$\sum_{k \ge 0}\left( \left(1 - \frac{1}{2^k} \right)^n - \left(1 - \frac{1}{2^k} \right)^{3n} \right) \ge 1$$ However, at this point, I seem to not have paid attention in my calculus classes, or just don't see how to bound this series. Does anybody see a way of showing the above? I would be happy with any $m = cn$ for a constant $c$. I plotted it for $m = 3n$ (and $0 \le k \le 1000$), and the left side is well above $1$ for all $n \geq 1$. Even if I could show that: Is my approach (""if I chose $m$ such that the expected values differ by at least $1$, I have found the upper bound I'm looking for"") valid? Thanks a lot!","I am given a series $X_1, X_2, \dots X_n$ of geometrically distributed (with $p = 0.5$), independent random variables, for some $n$. Since there seem to be multiple definitions around: Think of each $X_i$ as the number of coin flips until you see ""heads"". Let $\hat{X} = \max_i(X_i)$. What I want is an upper bound (in $n$) on the expected number of geometrically distributed random variables (with $p = 0.5$) $X'_1, X'_2, X'_m$ that I need to look at before I see a value of at least $\hat{X} + 1$. In other words: I draw $n$ such random variables and determine the maximum. Now, in the second step, I keep on drawing such random variables until I see a value larger than the previously determined variables. How many variables do I expect to draw in the second step? My assumption is: $m \leq 2n$ or $m \le 3n$. I have two approaches of which I feel like they should show this, but for both I'm not sure whether they hold / how to correctly bound $m$. Approach 1 : Symmetry and Handwaving I feel like there should be symmetry at play here. Say I wasn't looking for a value larger than $\hat{X}$, but at least as large as $\hat{X}$. Concatenate both series together to form $X_1, X_2, \dots X_n, X'_1, X'_2, \dots, X_m$. Now, the values of the individual $X_i$ (resp. $X'_i$) are indepentent of my choice of $n$ - so why should the distance ""to the left"" from $X_n$ to $\hat{X}$ be larger than the distance ""to the right"" from $X_n$ to the next element of at least the same value as $\hat{X}$? For symmetry reasons, we should expect $m = n$ here. Now I'm not looking for an element of the same value, but of strictly greater value. Since $P[X_i = \hat{X}] = 2 \cdot P[X_i = \hat{X} + 1]$, the values I'm looking for should be halve as densely distributed… thus we should expect $m = 2n$ … right? At this point I'm waving my hands really hard and hope for the best. Is this approach valid? Would you (as a reviewer / reader) accept this in a paper, if I used it to prove some property of some data structure? Approach 2 : Actually do the Maths One more rigorous approach I thought of is this: From this answer I know that the expected maximum of $n$ such $X_i$ (let's call it $M(\{X_i\})$) is $$E(M(\{X_1, \dots X_n\})) = \sum_{k \geq 0}\left( 1 - \left(1 - \frac{1}{2^k}\right)^n\right)$$ I feel like if I find an $m$ such that $E(M(\{X'_1, \dots X'_m\})) - E(M(\{X_1, \dots X_n\})) \ge 1$, then I should have proven my point - right? I chose $m = 3n$. The resulting sum from the above can then be simplified to: $$\sum_{k \ge 0}\left( \left(1 - \frac{1}{2^k} \right)^n - \left(1 - \frac{1}{2^k} \right)^{3n} \right) \ge 1$$ However, at this point, I seem to not have paid attention in my calculus classes, or just don't see how to bound this series. Does anybody see a way of showing the above? I would be happy with any $m = cn$ for a constant $c$. I plotted it for $m = 3n$ (and $0 \le k \le 1000$), and the left side is well above $1$ for all $n \geq 1$. Even if I could show that: Is my approach (""if I chose $m$ such that the expected values differ by at least $1$, I have found the upper bound I'm looking for"") valid? Thanks a lot!",,"['calculus', 'probability', 'sequences-and-series', 'limits', 'probability-distributions']"
75,Hierarchy of functions by asymptotic growth,Hierarchy of functions by asymptotic growth,,"I am ordering the following function in order of non-decreasing asymptotic growth. $f(n) \in \mathcal{O} \big(g(n)\big) \in \mathcal{O} \big(h(n)\big)$... etc. I believe I have most of the order correct, but there is one function I'm a bit lost on. The order so far is $\log_{2}n, \quad n^{\frac{1}{3}}, \quad n^{5}, \quad 10^{n},  \quad n^{n}$ The only function I need to figure out is $2^{\sqrt{\log_{2}n}}$ I'm certain that $2^{\sqrt{\log_{2}n}}$ should be in between $\quad \log_{2}n \quad$ and $\quad n^{\frac{1}{3}} \quad$. Unlike the other terms, I am not able to tell from just looking at it, and I'm little lost trying to verify this without a calculator (I'm not allowed to use calculator on my test). It would also help me to know what type of function this is. Is this classified as a logarithmic, or an exponential perhaps? I know that without the square root the function would be linear.","I am ordering the following function in order of non-decreasing asymptotic growth. $f(n) \in \mathcal{O} \big(g(n)\big) \in \mathcal{O} \big(h(n)\big)$... etc. I believe I have most of the order correct, but there is one function I'm a bit lost on. The order so far is $\log_{2}n, \quad n^{\frac{1}{3}}, \quad n^{5}, \quad 10^{n},  \quad n^{n}$ The only function I need to figure out is $2^{\sqrt{\log_{2}n}}$ I'm certain that $2^{\sqrt{\log_{2}n}}$ should be in between $\quad \log_{2}n \quad$ and $\quad n^{\frac{1}{3}} \quad$. Unlike the other terms, I am not able to tell from just looking at it, and I'm little lost trying to verify this without a calculator (I'm not allowed to use calculator on my test). It would also help me to know what type of function this is. Is this classified as a logarithmic, or an exponential perhaps? I know that without the square root the function would be linear.",,"['limits', 'logarithms', 'asymptotics', 'computational-complexity']"
76,"When does the limit $|\cos(n)|^{f(n)}$ converges as $n \rightarrow \infty, n \in \mathbb{N}$?",When does the limit  converges as ?,"|\cos(n)|^{f(n)} n \rightarrow \infty, n \in \mathbb{N}","Here we go with a not-so-trivial problem: Inspired by another problem that I myself asked here , I came with this more general formulation: Let be the sequence $a(n) = |\cos(n)|^{f(n)}$. Then, when does the sequence $|\cos(n)|^{f(n)}$ converges as $n \rightarrow \infty$, for $n \in \mathbb{N}$? And the only thing that we may vary is $f(n)$. The only constrain for $f(n)$ will be that $f(n)$ increases monotonically and is unbounded . It is easy to see that the numbers that belong to the open interval $(k\pi, (k+1)\pi)$ tend to $0$ as $f(n) \rightarrow \infty$. And on the other side, all natural numbers will always belong to these intervals (if not, that would be equivalent to claim the rationality of $\pi$). Then, all members of the sequence will tend to $0$... All?? Not necessarily, because by Dirichlet's Approximation Theorem , you can always find an integer that is arbitrarily close to a multiple of $\pi$. And the closer you are to a multiple of $\pi$, the closer would be the following and related function to 1: $$ \lim_{x \rightarrow k\pi} |\cos(x)| = 1 $$ On the other hand, if instead of considering functions with real numbers, we focus on our sequence over the natural numbers, we can see that not all the integers approximate to a multiple of $\pi$ in the same manner. There is an integer sequence that every member of the sequence is closer to a multiple of $\pi$ that the previous member ( https://oeis.org/A046947 ). This integer sequence increases exponentially and let be $b(n)$ that sequence. I've seen computationally, that $b(n) \approx \pi^n$. So... As I can see this, here there are to different opposing forces acting one against the other to make converge or diverge our sequence. On one side, the convergence speed of the members of the open intervals $(k\pi, (k+1)\pi)$ towards $0$ ($k\in\mathbb{N}$), which we can control through playing with $f(n)$; and on the other side, the convergence speed of the sequence $b(n)$ to a bigger multiple of $\pi$. but this ""process"" is fixed and we cannot alter it. It seems that: $$ f(n) = 2n \implies \nexists  \lim_{n \rightarrow \infty} |\cos(n)|^{f(n)}  $$ This problem seem to be solved here in my previous MSE question. This shows an example of $f(n)$ that makes this sequence diverge, but I strongly think that other expressions for $f(n)$ can make this sequence converge to $0$. My intuition tells me that if $f(n)$ is linear, the sequence will diverge always, but I have that feeling that tells me that if $f(n)$ is exponential , like $f(n) = a^n$, then the sequence will converge if $a > a_0$, being $a_0$ some constant (maybe $\pi$?). So the final question to answer would be: Let $f(n)$ be a monotonically increasing and unbounded sequence whose expression is known. How must be $f(n)$ so: $$ \lim_{n \rightarrow \infty} |\cos(n)|^{f(n)} = 0? $$ Many thanks in advance!","Here we go with a not-so-trivial problem: Inspired by another problem that I myself asked here , I came with this more general formulation: Let be the sequence $a(n) = |\cos(n)|^{f(n)}$. Then, when does the sequence $|\cos(n)|^{f(n)}$ converges as $n \rightarrow \infty$, for $n \in \mathbb{N}$? And the only thing that we may vary is $f(n)$. The only constrain for $f(n)$ will be that $f(n)$ increases monotonically and is unbounded . It is easy to see that the numbers that belong to the open interval $(k\pi, (k+1)\pi)$ tend to $0$ as $f(n) \rightarrow \infty$. And on the other side, all natural numbers will always belong to these intervals (if not, that would be equivalent to claim the rationality of $\pi$). Then, all members of the sequence will tend to $0$... All?? Not necessarily, because by Dirichlet's Approximation Theorem , you can always find an integer that is arbitrarily close to a multiple of $\pi$. And the closer you are to a multiple of $\pi$, the closer would be the following and related function to 1: $$ \lim_{x \rightarrow k\pi} |\cos(x)| = 1 $$ On the other hand, if instead of considering functions with real numbers, we focus on our sequence over the natural numbers, we can see that not all the integers approximate to a multiple of $\pi$ in the same manner. There is an integer sequence that every member of the sequence is closer to a multiple of $\pi$ that the previous member ( https://oeis.org/A046947 ). This integer sequence increases exponentially and let be $b(n)$ that sequence. I've seen computationally, that $b(n) \approx \pi^n$. So... As I can see this, here there are to different opposing forces acting one against the other to make converge or diverge our sequence. On one side, the convergence speed of the members of the open intervals $(k\pi, (k+1)\pi)$ towards $0$ ($k\in\mathbb{N}$), which we can control through playing with $f(n)$; and on the other side, the convergence speed of the sequence $b(n)$ to a bigger multiple of $\pi$. but this ""process"" is fixed and we cannot alter it. It seems that: $$ f(n) = 2n \implies \nexists  \lim_{n \rightarrow \infty} |\cos(n)|^{f(n)}  $$ This problem seem to be solved here in my previous MSE question. This shows an example of $f(n)$ that makes this sequence diverge, but I strongly think that other expressions for $f(n)$ can make this sequence converge to $0$. My intuition tells me that if $f(n)$ is linear, the sequence will diverge always, but I have that feeling that tells me that if $f(n)$ is exponential , like $f(n) = a^n$, then the sequence will converge if $a > a_0$, being $a_0$ some constant (maybe $\pi$?). So the final question to answer would be: Let $f(n)$ be a monotonically increasing and unbounded sequence whose expression is known. How must be $f(n)$ so: $$ \lim_{n \rightarrow \infty} |\cos(n)|^{f(n)} = 0? $$ Many thanks in advance!",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'trigonometric-series']"
77,Probability of choosing the second best candidate in the secretary problem,Probability of choosing the second best candidate in the secretary problem,,"I'm trying to research unexplored properties of the secretary problem , specifically the probability of choosing the k'th best candidate instead of the best one. Could be thought of as trying to calculate the PDF of the result of the optimal policy of skipping $n/e$. I've come up with the following infinite sum which represents the probability of choosing $k=2$, in a method very similar to the original problem's proof. There is also a slightly longer and more algebra-heavy proof for a general k. Using a Riemann approximation of an integral doesn't seem to work in this case, due to the $n-i$ expression stuck inside the sum. By running this calculation, I see that it does in fact converge to $e^{-2} \sim 0.135$, which does in fact match the empirical results. But how do I go about proving it? $$r=n/e$$ $$\lim_{n\rightarrow\infty} \frac{r-1}{n(n-1)} \cdot \sum_{i=r}^{n-1}{\frac{n-i}{i-1}} \approx e^{-2}$$ EDIT: Maybe the following simplification can help? $$ = \lim_{n\rightarrow\infty} \frac{1}{e} ( \frac{1}{n} \cdot \sum_{i=r}^{n-1}{\frac{n-i}{i-1}}) \approx e^{-2} \Leftrightarrow \lim_{n\rightarrow\infty} \frac{1}{n} \cdot \sum_{i=r}^{n-1}{\frac{n-i}{i-1}} \approx e^{-1}$$ Screenshot of how the sum is arrived at P.S. The formula for a general k, if anyone's interested: $$\lim_{n\rightarrow\infty} \frac{r-1}{n} \cdot \sum_{i=r}^{n-k+1}{\frac{1}{i-1} \cdot \prod_{j=0}^{k-2}{\frac{n-i-j}{n-j-1}}}$$ If anyone can help me find what the general k converges to, I will be much obliged. I suspect it converges to $e^{-k}$.","I'm trying to research unexplored properties of the secretary problem , specifically the probability of choosing the k'th best candidate instead of the best one. Could be thought of as trying to calculate the PDF of the result of the optimal policy of skipping $n/e$. I've come up with the following infinite sum which represents the probability of choosing $k=2$, in a method very similar to the original problem's proof. There is also a slightly longer and more algebra-heavy proof for a general k. Using a Riemann approximation of an integral doesn't seem to work in this case, due to the $n-i$ expression stuck inside the sum. By running this calculation, I see that it does in fact converge to $e^{-2} \sim 0.135$, which does in fact match the empirical results. But how do I go about proving it? $$r=n/e$$ $$\lim_{n\rightarrow\infty} \frac{r-1}{n(n-1)} \cdot \sum_{i=r}^{n-1}{\frac{n-i}{i-1}} \approx e^{-2}$$ EDIT: Maybe the following simplification can help? $$ = \lim_{n\rightarrow\infty} \frac{1}{e} ( \frac{1}{n} \cdot \sum_{i=r}^{n-1}{\frac{n-i}{i-1}}) \approx e^{-2} \Leftrightarrow \lim_{n\rightarrow\infty} \frac{1}{n} \cdot \sum_{i=r}^{n-1}{\frac{n-i}{i-1}} \approx e^{-1}$$ Screenshot of how the sum is arrived at P.S. The formula for a general k, if anyone's interested: $$\lim_{n\rightarrow\infty} \frac{r-1}{n} \cdot \sum_{i=r}^{n-k+1}{\frac{1}{i-1} \cdot \prod_{j=0}^{k-2}{\frac{n-i-j}{n-j-1}}}$$ If anyone can help me find what the general k converges to, I will be much obliged. I suspect it converges to $e^{-k}$.",,"['calculus', 'probability', 'sequences-and-series', 'limits', 'game-theory']"
78,When can the limit of a function be equal to the function of the limit? [closed],When can the limit of a function be equal to the function of the limit? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question When can the limit of a function be equal to the function of a limit? For example what are the conditions that the following holds true? $$ \lim_{x\to a} \sqrt{f(x)} = \sqrt{\lim_{x\to a} f(x)} $$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question When can the limit of a function be equal to the function of a limit? For example what are the conditions that the following holds true? $$ \lim_{x\to a} \sqrt{f(x)} = \sqrt{\lim_{x\to a} f(x)} $$",,"['calculus', 'limits', 'continuity']"
79,"$\lim_{(x,y) \to (0,0)}\frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}} = 0 \Longleftrightarrow \frac{a}{c} + \frac{b}{d} > 1$",,"\lim_{(x,y) \to (0,0)}\frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}} = 0 \Longleftrightarrow \frac{a}{c} + \frac{b}{d} > 1","Let $a,b,c,d$ be positive real numbers. Show that the limit   $$\lim_{(x,y) \to (0,0)}\frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}} = 0$$   if only if   $$\frac{a}{c} + \frac{b}{d} > 1.$$ This question seems to be basically about algebraic manipulation, but I cannot seem to find a good way. Can anybody help me?","Let $a,b,c,d$ be positive real numbers. Show that the limit   $$\lim_{(x,y) \to (0,0)}\frac{|x|^{a}|y|^{b}}{|x|^{c} + |y|^{d}} = 0$$   if only if   $$\frac{a}{c} + \frac{b}{d} > 1.$$ This question seems to be basically about algebraic manipulation, but I cannot seem to find a good way. Can anybody help me?",,"['real-analysis', 'limits']"
80,Show that $\lim\limits_{n \to \infty}\sin n^2$ does not exist.,Show that  does not exist.,\lim\limits_{n \to \infty}\sin n^2,"Problem Show that $\lim\limits_{n \to \infty}\sin n^2$ does not exist, where $n=1,2,\cdots.$ Proof Assume that $\lim\limits_{n \to \infty}\sin n^2$ exists, then since $\cos (2n^2)=1-2\sin^2 n^2,$ $\lim\limits_{n \to \infty}\cos(2n^2)$ also exists. It's easy to know $\lim\limits_{n \to \infty}\sin n^2 \neq 0$, even though the limit exists. Hence, $$\lim\limits_{n \to \infty}\sin (2n)^2=\lim\limits_{n \to \infty}\sin n^2 \neq 0.\tag1$$ Since $$\sin(2n)^2=\sin 2(2n^2)=2\sin(2n^2)\cos(2n^2),\tag2$$ hence $\lim\limits_{n \to \infty}\cos (2n^2) \neq 0.$ Otherwise, if $\lim\limits_{n \to \infty}\cos (2n^2) =0,$ then according to $(2)$, we may obtain $\lim\limits_{n \to \infty}\sin(2n^2) = 0,$ which contradicts $(1)$. Now, notice that $\sin (2n^2)=\dfrac{\sin(2n)^2}{2\cos(2n^2)}$, for the reason that $\lim\limits_{n \to \infty}\sin n^2 \neq 0$ and $\lim\limits_{n \to \infty}\cos (2n^2) \neq 0$, we may claim that $\lim\limits_{n \to \infty}\sin (2n^2) $ exists but dose not equal $0$. Since $\sin (2n^2)=2\sin n^2\cos n^2$, hence $\cos n^2=\dfrac{\sin(2n^2)}{2\sin n^2}.$ But we have known that $\lim\limits_{n \to \infty}\sin (2n^2) $ and $\lim\limits_{n \to \infty}\sin n^2 $ both exist and $\lim\limits_{n \to \infty}\sin n^2 \neq 0 $. Thus, $\lim\limits_{n \to \infty} \cos n^2$ exists. Therefore, $\lim\limits_{n \to \infty} \sin (n+1)^2$ and $\lim\limits_{n \to \infty} \cos (n+1)^2$ both exist as well. Since $\sin(2n+1)=\sin[(n+1)^2-n^2]=\sin(n+1)^2\cos n^2-\cos(n+1)^2\sin n^2$，hence $\lim\limits_{ n \to \infty}\sin(2n+1)$ exists, which is absurd. As a result, the assumption at the beginning that $\lim\limits_{n \to \infty}\sin n^2$ exists is false. We are done. NOTE Among the proof above， we apply two facts that $\lim\limits_{n \to \infty} \neq 0,$ and $\lim\limits_{n \to \infty}\sin (2n+1)$ does not exist, which has not been proven. If necessary, I'm pleased to complement the proofs of them. Lemma 1 $\lim\limits_{n \to \infty}\sin(2n+1)\neq 0.$ Proof Assume that $\lim\limits_{n \to \infty}\sin (2n+1)=0.$ Then $\lim\limits_{n \to \infty}|\sin (2n+1)|=0$ and $\lim\limits_{n \to \infty}|\cos (2n+1)|=1.$ Thus, \begin{align*}  \lim\limits_{n \to \infty}|\sin (2n+1)|&=\lim\limits_{n \to \infty}|\sin (2n+3)|\\ &\geq  \lim\limits_{n \to \infty}|\cos (2n+1)\sin 2|- \lim\limits_{n \to \infty}|\sin (2n+1)\cos 2|\\ &= \lim\limits_{n \to \infty}|\cos(2n+1)\sin 2|\\ &=\sin2 \neq 0, \end{align*} which contradicts. Lemma 2 $\lim\limits_{n \to \infty}\sin (2n+1)$ does not exist. Proof Assume that $\lim\limits_{n \to \infty}\sin (2n+1)$ exists. Take two arcs with the middle points $\dfrac{\pi}{2},\dfrac{3\pi}{2}$ respectively and the length $\dfrac{2\pi}{3}$ from the unit circle. Notice the common difference of $\{2n+1\}$ is $2$, which is smaller than $\dfrac{2\pi}{3}$. Hence, there are infinitely many terms locating on the arc with the middle point $\dfrac{\pi}{2}$ and the length $\dfrac{2\pi}{3}$. For such terms $\{2n_k+1\}$, we have $$\sin(2n_k+1) \geq \sin \left(\frac{\pi}{2}-\frac{\pi}{3}\right)=\sin\frac{\pi}{6}=\frac{1}{2}.$$ Therefore, $$\lim\limits_{n \to \infty}\sin (2n+1)=\lim\limits_{n \to \infty}\sin (2n_k+1)\geq \frac{1}{2}.\tag3$$  Similarily, there also exists another subsequence $\{2m_k+1\}$ locating on the arc with the middle point $\dfrac{3\pi}{2}$ and the length $\dfrac{2\pi}{3}$. Notice that $$\sin(2m_k+1) \leq \sin \left(\frac{3\pi}{2}+\frac{\pi}{3}\right)=\sin\frac{11\pi}{6}=-\frac{1}{2}.$$ Therefore, $$\lim\limits_{n \to \infty}\sin (2n+1)=\lim\limits_{n \to \infty}\sin (2m_k+1)\leq -\frac{1}{2}.\tag4$$ As a result, $(3)$ and $(4)$ contradict each other. Lemma 3 $\lim\limits_{n \to \infty} \sin n^2 \neq 0.$ Proof Assume that $\lim\limits_{n \to \infty} \sin n^2 = 0.$ Notice that $\sin(n\pi)=0$ for all $n=1,2,\cdots.$ and the continuity of $y=\sin x$. Thus, for any given $\varepsilon>0$, there exists a $N \in \mathbb{N_+}$, when $k \geq N$, we may choose a positive integer $n_k$ such that $$|k^2-n_k\pi|<\frac{\varepsilon}{2}.$$ Without loss of generality, we may assume $\{n_k\}$ is rigorously increasing. Then $$|(2k+1)-(n_{k+1}-n_k)\pi|=|(k+1)^2-n_{k+1}\pi-(k^2-n_k\pi)|<\varepsilon.$$ Thus, $$\lim_{k \to \infty}\sin(2k+1)=\lim_{k \to \infty}\sin(n_{k+1}-n_k)\pi=0,$$ which contradicts Lemma 1. PLEASE CORRECT ME IF I'M WRONG! HOPE TO SEE MORE ELEGANT SOLUTIONS!","Problem Show that $\lim\limits_{n \to \infty}\sin n^2$ does not exist, where $n=1,2,\cdots.$ Proof Assume that $\lim\limits_{n \to \infty}\sin n^2$ exists, then since $\cos (2n^2)=1-2\sin^2 n^2,$ $\lim\limits_{n \to \infty}\cos(2n^2)$ also exists. It's easy to know $\lim\limits_{n \to \infty}\sin n^2 \neq 0$, even though the limit exists. Hence, $$\lim\limits_{n \to \infty}\sin (2n)^2=\lim\limits_{n \to \infty}\sin n^2 \neq 0.\tag1$$ Since $$\sin(2n)^2=\sin 2(2n^2)=2\sin(2n^2)\cos(2n^2),\tag2$$ hence $\lim\limits_{n \to \infty}\cos (2n^2) \neq 0.$ Otherwise, if $\lim\limits_{n \to \infty}\cos (2n^2) =0,$ then according to $(2)$, we may obtain $\lim\limits_{n \to \infty}\sin(2n^2) = 0,$ which contradicts $(1)$. Now, notice that $\sin (2n^2)=\dfrac{\sin(2n)^2}{2\cos(2n^2)}$, for the reason that $\lim\limits_{n \to \infty}\sin n^2 \neq 0$ and $\lim\limits_{n \to \infty}\cos (2n^2) \neq 0$, we may claim that $\lim\limits_{n \to \infty}\sin (2n^2) $ exists but dose not equal $0$. Since $\sin (2n^2)=2\sin n^2\cos n^2$, hence $\cos n^2=\dfrac{\sin(2n^2)}{2\sin n^2}.$ But we have known that $\lim\limits_{n \to \infty}\sin (2n^2) $ and $\lim\limits_{n \to \infty}\sin n^2 $ both exist and $\lim\limits_{n \to \infty}\sin n^2 \neq 0 $. Thus, $\lim\limits_{n \to \infty} \cos n^2$ exists. Therefore, $\lim\limits_{n \to \infty} \sin (n+1)^2$ and $\lim\limits_{n \to \infty} \cos (n+1)^2$ both exist as well. Since $\sin(2n+1)=\sin[(n+1)^2-n^2]=\sin(n+1)^2\cos n^2-\cos(n+1)^2\sin n^2$，hence $\lim\limits_{ n \to \infty}\sin(2n+1)$ exists, which is absurd. As a result, the assumption at the beginning that $\lim\limits_{n \to \infty}\sin n^2$ exists is false. We are done. NOTE Among the proof above， we apply two facts that $\lim\limits_{n \to \infty} \neq 0,$ and $\lim\limits_{n \to \infty}\sin (2n+1)$ does not exist, which has not been proven. If necessary, I'm pleased to complement the proofs of them. Lemma 1 $\lim\limits_{n \to \infty}\sin(2n+1)\neq 0.$ Proof Assume that $\lim\limits_{n \to \infty}\sin (2n+1)=0.$ Then $\lim\limits_{n \to \infty}|\sin (2n+1)|=0$ and $\lim\limits_{n \to \infty}|\cos (2n+1)|=1.$ Thus, \begin{align*}  \lim\limits_{n \to \infty}|\sin (2n+1)|&=\lim\limits_{n \to \infty}|\sin (2n+3)|\\ &\geq  \lim\limits_{n \to \infty}|\cos (2n+1)\sin 2|- \lim\limits_{n \to \infty}|\sin (2n+1)\cos 2|\\ &= \lim\limits_{n \to \infty}|\cos(2n+1)\sin 2|\\ &=\sin2 \neq 0, \end{align*} which contradicts. Lemma 2 $\lim\limits_{n \to \infty}\sin (2n+1)$ does not exist. Proof Assume that $\lim\limits_{n \to \infty}\sin (2n+1)$ exists. Take two arcs with the middle points $\dfrac{\pi}{2},\dfrac{3\pi}{2}$ respectively and the length $\dfrac{2\pi}{3}$ from the unit circle. Notice the common difference of $\{2n+1\}$ is $2$, which is smaller than $\dfrac{2\pi}{3}$. Hence, there are infinitely many terms locating on the arc with the middle point $\dfrac{\pi}{2}$ and the length $\dfrac{2\pi}{3}$. For such terms $\{2n_k+1\}$, we have $$\sin(2n_k+1) \geq \sin \left(\frac{\pi}{2}-\frac{\pi}{3}\right)=\sin\frac{\pi}{6}=\frac{1}{2}.$$ Therefore, $$\lim\limits_{n \to \infty}\sin (2n+1)=\lim\limits_{n \to \infty}\sin (2n_k+1)\geq \frac{1}{2}.\tag3$$  Similarily, there also exists another subsequence $\{2m_k+1\}$ locating on the arc with the middle point $\dfrac{3\pi}{2}$ and the length $\dfrac{2\pi}{3}$. Notice that $$\sin(2m_k+1) \leq \sin \left(\frac{3\pi}{2}+\frac{\pi}{3}\right)=\sin\frac{11\pi}{6}=-\frac{1}{2}.$$ Therefore, $$\lim\limits_{n \to \infty}\sin (2n+1)=\lim\limits_{n \to \infty}\sin (2m_k+1)\leq -\frac{1}{2}.\tag4$$ As a result, $(3)$ and $(4)$ contradict each other. Lemma 3 $\lim\limits_{n \to \infty} \sin n^2 \neq 0.$ Proof Assume that $\lim\limits_{n \to \infty} \sin n^2 = 0.$ Notice that $\sin(n\pi)=0$ for all $n=1,2,\cdots.$ and the continuity of $y=\sin x$. Thus, for any given $\varepsilon>0$, there exists a $N \in \mathbb{N_+}$, when $k \geq N$, we may choose a positive integer $n_k$ such that $$|k^2-n_k\pi|<\frac{\varepsilon}{2}.$$ Without loss of generality, we may assume $\{n_k\}$ is rigorously increasing. Then $$|(2k+1)-(n_{k+1}-n_k)\pi|=|(k+1)^2-n_{k+1}\pi-(k^2-n_k\pi)|<\varepsilon.$$ Thus, $$\lim_{k \to \infty}\sin(2k+1)=\lim_{k \to \infty}\sin(n_{k+1}-n_k)\pi=0,$$ which contradicts Lemma 1. PLEASE CORRECT ME IF I'M WRONG! HOPE TO SEE MORE ELEGANT SOLUTIONS!",,['limits']
81,"Given info regarding $f$ show it is ""$1-1$"" and more","Given info regarding  show it is """" and more",f 1-1,"If $f:\mathbb R\to\mathbb R$ one function with $f(\mathbb R)=\mathbb R$ which is differentiable and for it applies $f'(x)\neq0, \forall x\in \mathbb R$ . Also, $C_f$ (the graph of $f$ ) passes through the points $A(1,2)$ and $B(0,1).$ I) Show that $f$ is a "" $1-1$ "" function. II) Solve the equation: $f^{-1}(f(1+\ln x)-1)=0$ . III) Show that there is at least one point $M$ of $C_f$ such as that the tangent is perpendicular to $ε:x+y-1=0$ . IV) If $f'$ is continuous then find $\lim\limits_{x\to -\infty}{e^x \over f(x)}$ . Personal work: I) Because $f$ is differentiable then it will be continuous. $\forall x \in \mathbb R$ it applies $f'(x)\neq0 \Rightarrow f(x)\neq c$ because if $f'(x)=0$ , then $f(x)=c.$ ΙΙ) Because $C_f$ passes through the points $A(1,2)$ and $B(0,1)$ then it will apply $$f(0)=1, f(1)=2$$","If one function with which is differentiable and for it applies . Also, (the graph of ) passes through the points and I) Show that is a "" "" function. II) Solve the equation: . III) Show that there is at least one point of such as that the tangent is perpendicular to . IV) If is continuous then find . Personal work: I) Because is differentiable then it will be continuous. it applies because if , then ΙΙ) Because passes through the points and then it will apply","f:\mathbb R\to\mathbb R f(\mathbb R)=\mathbb R f'(x)\neq0, \forall x\in \mathbb R C_f f A(1,2) B(0,1). f 1-1 f^{-1}(f(1+\ln x)-1)=0 M C_f ε:x+y-1=0 f' \lim\limits_{x\to -\infty}{e^x \over f(x)} f \forall x \in \mathbb R f'(x)\neq0 \Rightarrow f(x)\neq c f'(x)=0 f(x)=c. C_f A(1,2) B(0,1) f(0)=1, f(1)=2","['real-analysis', 'limits', 'derivatives']"
82,Exchange a limit with a $\limsup$ or $\liminf$,Exchange a limit with a  or,\limsup \liminf,"Let $f_{n}:\,\mathbb{R}\rightarrow\mathbb{R}$ a sequence of $C^{1}\left(\mathbb{R}\right)$ functions such that $$\left|f_{n}\left(x\right)\right|\leq M,\ \forall x\in\mathbb{R},\,\forall n\in\mathbb{N}$$and assume that $$\lim_{n\rightarrow\infty}f_{n}\left(x\right)=f\left(x\right)$$ for all $x$ in $\mathbb{R}$. Can I conclude that $$\lim_{n\rightarrow\infty}\limsup_{h\rightarrow0}\frac{f_{n}\left(x+h\right)-f_{n}\left(x\right)}{h}=\limsup_{h\rightarrow0}\lim_{n\rightarrow\infty}\frac{f_{n}\left(x+h\right)-f_{n}\left(x\right)}{h}$$ $$=\limsup_{h\rightarrow0}\frac{f\left(x+h\right)-f\left(x\right)}{h}?$$ I know that there are some theorems that allow to switch two limits, like the dominated convergence theorem, but what can I say in this situation? I have not more information about these functions. Note that I'm interested also in the case that the limit is not convergent. In other words: is it true that $$\lim_{n\rightarrow\infty}\limsup_{h\rightarrow0}\frac{f_{n}\left(x+h\right)-f_{n}\left(x\right)}{h}=L\Leftrightarrow\limsup_{h\rightarrow0}\frac{f\left(x+h\right)-f\left(x\right)}{h}=L,\,L\in\mathbb{R}$$ or $$\lim_{n\rightarrow\infty}\limsup_{h\rightarrow0}\frac{f_{n}\left(x+h\right)-f_{n}\left(x\right)}{h}\text{ does not exist}\Leftrightarrow\limsup_{h\rightarrow0}\frac{f\left(x+h\right)-f\left(x\right)}{h}\text{ does not exist}$$ or $$\lim_{n\rightarrow\infty}\limsup_{h\rightarrow0}\frac{f_{n}\left(x+h\right)-f_{n}\left(x\right)}{h}=\infty\,(-\infty)\Leftrightarrow\limsup_{h\rightarrow0}\frac{f\left(x+h\right)-f\left(x\right)}{h}=\infty\,(-\infty)?$$","Let $f_{n}:\,\mathbb{R}\rightarrow\mathbb{R}$ a sequence of $C^{1}\left(\mathbb{R}\right)$ functions such that $$\left|f_{n}\left(x\right)\right|\leq M,\ \forall x\in\mathbb{R},\,\forall n\in\mathbb{N}$$and assume that $$\lim_{n\rightarrow\infty}f_{n}\left(x\right)=f\left(x\right)$$ for all $x$ in $\mathbb{R}$. Can I conclude that $$\lim_{n\rightarrow\infty}\limsup_{h\rightarrow0}\frac{f_{n}\left(x+h\right)-f_{n}\left(x\right)}{h}=\limsup_{h\rightarrow0}\lim_{n\rightarrow\infty}\frac{f_{n}\left(x+h\right)-f_{n}\left(x\right)}{h}$$ $$=\limsup_{h\rightarrow0}\frac{f\left(x+h\right)-f\left(x\right)}{h}?$$ I know that there are some theorems that allow to switch two limits, like the dominated convergence theorem, but what can I say in this situation? I have not more information about these functions. Note that I'm interested also in the case that the limit is not convergent. In other words: is it true that $$\lim_{n\rightarrow\infty}\limsup_{h\rightarrow0}\frac{f_{n}\left(x+h\right)-f_{n}\left(x\right)}{h}=L\Leftrightarrow\limsup_{h\rightarrow0}\frac{f\left(x+h\right)-f\left(x\right)}{h}=L,\,L\in\mathbb{R}$$ or $$\lim_{n\rightarrow\infty}\limsup_{h\rightarrow0}\frac{f_{n}\left(x+h\right)-f_{n}\left(x\right)}{h}\text{ does not exist}\Leftrightarrow\limsup_{h\rightarrow0}\frac{f\left(x+h\right)-f\left(x\right)}{h}\text{ does not exist}$$ or $$\lim_{n\rightarrow\infty}\limsup_{h\rightarrow0}\frac{f_{n}\left(x+h\right)-f_{n}\left(x\right)}{h}=\infty\,(-\infty)\Leftrightarrow\limsup_{h\rightarrow0}\frac{f\left(x+h\right)-f\left(x\right)}{h}=\infty\,(-\infty)?$$",,"['calculus', 'real-analysis', 'limits', 'limsup-and-liminf']"
83,Elementary doubt about differentiability of a two variables function,Elementary doubt about differentiability of a two variables function,,"Given the function $$ g(x,y) = \begin{cases} \frac{xy^3}{2x^4+y^4}, \,\,\, \text{if}\,\, (x,y)\neq(0,0)\,\\  0, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \text{else}          \end{cases} $$ check whether $g$ is continuous and differentiable at $(0,0)$ or not. Considering the two restrictions $y  = \pm x$, it is seen $g$ is not continuous at $(0,0)$. How about differentiability? Is it related to continuity? The two partial derivatives are both $0$ at $(0,0)$. Still I'm not sure $g$ is differentiable at $0$, since I should consider any directional derivative. Thanks for your patience and help.","Given the function $$ g(x,y) = \begin{cases} \frac{xy^3}{2x^4+y^4}, \,\,\, \text{if}\,\, (x,y)\neq(0,0)\,\\  0, \,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \text{else}          \end{cases} $$ check whether $g$ is continuous and differentiable at $(0,0)$ or not. Considering the two restrictions $y  = \pm x$, it is seen $g$ is not continuous at $(0,0)$. How about differentiability? Is it related to continuity? The two partial derivatives are both $0$ at $(0,0)$. Still I'm not sure $g$ is differentiable at $0$, since I should consider any directional derivative. Thanks for your patience and help.",,"['limits', 'multivariable-calculus']"
84,"Functions with bounded derivatives, closed under composition","Functions with bounded derivatives, closed under composition",,"This is a follow-on of sorts to this question , but is self-contained. Let $F_1 := \{f \in C^\infty(\mathbb{R}) \mid \|\frac{df}{dx}\|_\infty \le c_1\}$ ($c_i > 0$ throughout). Given $f, g \in F_1$, we have by the chain rule that $g \circ f \in F_1$ if $\|\frac{dg}{df}\|_\infty \|\frac{df}{dx}\|_\infty \le c_1$ and this is automatically true if $c_1 \le 1$. Now let $F_2 := \{f \in F_1 \mid \|\frac{d^2f}{dx^2}\|_\infty \le c_2\}$. Given $f, g \in F_2$, we have by the generalized chain rule ( Faà di Bruno's formula ) that $g \circ f \in F_2$ if it is in $F_1$ and $\|\frac{d^2g}{df^2}\|_\infty \|\frac{df}{dx}\|_\infty^2 + \|\frac{dg}{df}\|_\infty \|\frac{d^2f}{dx^2}\|_\infty \le c_2$ and this is automatically true if $c_1, c_2$ satisfy the corresponding inequality $c_2 c_1^2 + c_1 c_2 \le c_2$, i.e. if $c_1^2 + c_1 \le 1$ (in which case $c_1 \le 1$ also). In general, let $F_n := \{f \in F_{n-1} \mid \|f^{(n)}\|_\infty \le c_{n} \}$. Given $f, g \in F_n$, we have by the generalized chain rule that $g \circ f \in F_n$ if it is in $F_{n-1}$ and $\sum_{\pi\in\Pi(n)} \|g^{(|\pi|)}\|_\infty \prod_{B\in\pi} \|f^{(|B|)}\|_\infty \le c_n$ (notation from the Wikipedia article ) and this is automatically true if the $c_i$ satisfy the corresponding inequalities $\sum_{\pi \in \Pi(i)} c_{(|\pi|)} \prod_{B \in \pi} c_{(|B|)} \le c_i \forall i \in \{1, ..., n\}$. My question then is: can $F_\infty := \bigcap_{n \in \mathbb{N}} F_n$ be closed under composition without being trivial? In other words: Does there exist a sequence of values $c_i > 0$ which satisfy the inequalities $\sum_{\pi \in \Pi(i)} c_{(|\pi|)} \prod_{B \in \pi} c_{(|B|)} \le c_i \forall i \in \mathbb{N}$?","This is a follow-on of sorts to this question , but is self-contained. Let $F_1 := \{f \in C^\infty(\mathbb{R}) \mid \|\frac{df}{dx}\|_\infty \le c_1\}$ ($c_i > 0$ throughout). Given $f, g \in F_1$, we have by the chain rule that $g \circ f \in F_1$ if $\|\frac{dg}{df}\|_\infty \|\frac{df}{dx}\|_\infty \le c_1$ and this is automatically true if $c_1 \le 1$. Now let $F_2 := \{f \in F_1 \mid \|\frac{d^2f}{dx^2}\|_\infty \le c_2\}$. Given $f, g \in F_2$, we have by the generalized chain rule ( Faà di Bruno's formula ) that $g \circ f \in F_2$ if it is in $F_1$ and $\|\frac{d^2g}{df^2}\|_\infty \|\frac{df}{dx}\|_\infty^2 + \|\frac{dg}{df}\|_\infty \|\frac{d^2f}{dx^2}\|_\infty \le c_2$ and this is automatically true if $c_1, c_2$ satisfy the corresponding inequality $c_2 c_1^2 + c_1 c_2 \le c_2$, i.e. if $c_1^2 + c_1 \le 1$ (in which case $c_1 \le 1$ also). In general, let $F_n := \{f \in F_{n-1} \mid \|f^{(n)}\|_\infty \le c_{n} \}$. Given $f, g \in F_n$, we have by the generalized chain rule that $g \circ f \in F_n$ if it is in $F_{n-1}$ and $\sum_{\pi\in\Pi(n)} \|g^{(|\pi|)}\|_\infty \prod_{B\in\pi} \|f^{(|B|)}\|_\infty \le c_n$ (notation from the Wikipedia article ) and this is automatically true if the $c_i$ satisfy the corresponding inequalities $\sum_{\pi \in \Pi(i)} c_{(|\pi|)} \prod_{B \in \pi} c_{(|B|)} \le c_i \forall i \in \{1, ..., n\}$. My question then is: can $F_\infty := \bigcap_{n \in \mathbb{N}} F_n$ be closed under composition without being trivial? In other words: Does there exist a sequence of values $c_i > 0$ which satisfy the inequalities $\sum_{\pi \in \Pi(i)} c_{(|\pi|)} \prod_{B \in \pi} c_{(|B|)} \le c_i \forall i \in \mathbb{N}$?",,"['real-analysis', 'sequences-and-series', 'functional-analysis', 'limits', 'inequality']"
85,"Prob. 10, Sec. 3.5, in Bartle & Sherbert's INTRO TO REAL ANALYSIS: If $x_1 < x_2$ are arbitrary real numbers and . . .","Prob. 10, Sec. 3.5, in Bartle & Sherbert's INTRO TO REAL ANALYSIS: If  are arbitrary real numbers and . . .",x_1 < x_2,"Here is Prob. 10, Sec. 3.5, in the book Introduction to Real Analysis by Rovert G. Bartle and Donald R. Sherbert, 4th edition: If $x_1 < x_2$ are arbitrary real numbers and $x_n \colon= \frac{1}{2} \left( x_{n-2} + x_{n-1} \right)$ for $n > 2$, show that $\left( x_n \right)$ is convergent. What is its limit? My Attempt: As $x_1 < x_2$, so    $$ x_1 < \frac{1}{2} \left( x_1 + x_2 \right) < x_2, $$   that is,    $$ x_1 < x_3 < x_2. \tag{1} $$   And also    $$ \left\lvert x_3 - x_2 \right\rvert = x_2 - x_3 = \frac{1}{2} \left( x_2 - x_1 \right). \tag{2} $$ Now as $x_3 < x_2$, so    $$ x_3 < \frac{1}{2} \left( x_3 + x_2 \right) < x_2, $$   that is,    $$ x_3 < x_4 < x_2. \tag{3} $$    And also    $$ \left\lvert x_4 - x_3 \right\rvert = x_4 - x_3 = \frac{1}{2} \left( x_2 - x_3 \right) = \frac{1}{2} \left\lvert x_2 - x_3 \right\rvert = \frac{1}{2^2} \left( x_2 - x_1 \right). \tag{4} $$ Now as $x_3 < x_4$, so    $$ x_3 < \frac{1}{2} \left( x_3 + x_4 \right) < x_4, $$   that is,    $$ x_3 < x_5 < x_4. \tag{5} $$   And also    $$ \left\lvert x_5 - x_4 \right\rvert = x_4 - x_5 = \frac{1}{2} \left( x_4 - x_3 \right) = \frac{1}{2} \left\lvert  x_4 - x_3 \right\rvert = \frac{1}{2^3} \left( x_2 - x_1 \right). \tag{6} $$ Now as $x_5 < x_4$, so    $$ x_5 < \frac{1}{2} \left( x_5 + x_4 \right) < x_4, $$   that is,    $$ x_5 < x_6 < x_4. \tag{7} $$   And also    $$ \left\lvert x_6 - x_5 \right\rvert = x_6 - x_5 = \frac{1}{2} \left( x_4 - x_5 \right) = \frac{1}{2} \left\lvert x_5 - x_4 \right\rvert = \frac{1}{2^4} \left( x_2 - x_1 \right). \tag{8} $$ From (1), (3), (5), and (7), we get    $$ x_1 < x_3 < x_5 < x_6 < x_4 < x_2. \tag{9} $$ Now let $k \in \mathbb{N}$ such that $k \geq 3$, and suppose that    $$ x_1 < x_3 < x_5 < \cdots < x_{2k-1} < x_{2k} < x_{2k-2} < \cdots < x_4 < x_2. \tag{10} $$   And suppose also that    $$ \left\lvert x_{2k} - x_{2k-1} \right\vert = \frac{1}{ 2^{2k-2} } \left( x_2 - x_1 \right). \tag{11} $$ Then as $x_{2k-1} < x_{2k}$, so    $$ x_{2k-1} < \frac{1}{2} \left( x_{2k-1} + x_{2k} \right) < x_{2k}, $$   that is,    $$ x_{2k-1} < x_{2k+1} < x_{2k }. \tag{12} $$   And also    $$ \left\lvert x_{2k+1} - x_{2k} \right\rvert = x_{2k}  - x_{2k+1} = \frac{1}{2} \left( x_{2k} - x_{2k-1} \right) = \frac{1}{2} \left\lvert  x_{2k} - x_{2k-1} \right\rvert = \frac{1}{ 2^{2k-1} } \left( x_2 - x_1 \right). \tag{13}  $$ Now as $x_{2k+1} < x_{2k}$, so    $$ x_{2k+1} < \frac{1}{2} \left( x_{2k+1} + x_{2k} \right) < x_{2k}, $$   that is,    $$ x_{2k+1} < x_{2k+2} < x_{2k}. \tag{14} $$ From  (10), (12), and (14), we can conclude that    $$ x_1 < x_3 < x_5 < \cdots < x_{2k-1} < x_{2k+1} < x_{2k+2} < x_{2k} < \cdots < x_6 < x_4 < x_2 \tag{15} $$   for all $k \in \mathbb{N}$. [I'm not really sure what to do with this relation though.] Also from (11) and (13), we can conclude that    $$ \left\lvert x_{n+1} - x_n \right\rvert = \frac{1}{2^{n-1} } \left( x_2 - x_1 \right). \tag{A} $$   for all $n \in \mathbb{N}$ such that $n \geq 2$. So for any natural numbers $m$ and $n$ such that $m > n$, we obtain   $$ \begin{align} \left\lvert x_m - x_n \right\rvert &\leq \left\lvert x_m - x_{m-1} \right\rvert + \cdots + \left\lvert x_{n+1} - x_n \right\rvert \\ &= \left( \frac{1}{2^{m-2} } + \cdots + \frac{1}{2^{n-1} } \right) \left( x_2 - x_1 \right) \\ &= \left( \frac{1}{2^{n-1} } + \frac{1}{2^{n} } \cdots + \frac{1}{2^{m-2} } \right) \left( x_2 - x_1 \right) \\ &= \frac{1}{2^{n-1}} \left( 1 + \frac{1}{2} + \cdots + \frac{1}{2^{m-n-1}} \right) \left( x_2 - x_1 \right) \\ &= \frac{1}{2^{n-1}} \frac{ 1 - \frac{1}{2^{m-n}} }{ 1 - \frac{1}{2} } \left( x_2 - x_1 \right) \\ &= \frac{1}{2^{n-2}} \left( 1 - \frac{1}{2^{m-n}} \right) \left( x_2 - x_1 \right) \\ &< \frac{1}{2^{n-2}}  \left( x_2 - x_1 \right). \tag{B} \end{align} $$ So, given $\varepsilon > 0$, if we choose a natural number $N$ so that    $$ N > \frac{ 4 \left(x_2 - x_1 \right)  }{ \varepsilon }, $$ then    $$ 2^N > N > \frac{ 4 \left(x_2 - x_1 \right)  }{ \varepsilon }, $$   and so    $$ \varepsilon > \frac{ 4 \left( x_2 - x_1 \right) }{ 2^N}.$$   and then we see from (B) that, whenever $m$ and $n$ are any natural numbers such that $m > n > N$, we have    $$ \left\lvert x_m - x_n \right\rvert < \frac{1}{2^{n-2}} \left( x_2 - x_1 \right) = \frac{4 \left( x_2 - x_1 \right) }{ 2^n} < \frac{4 \left( x_2 - x_1 \right)}{2^N} < \varepsilon. $$ Thus our sequence is a Cauchy sequence of real numbers. Therefore, this sequence is convergent. Let $x$ be the limit of this sequence. Then the subsequences    $\left( x_{2k-1} \right)_{k \in \mathbb{N}}$ and $\left( x_{2k } \right)_{k \in \mathbb{N}}$ also converge to this same limit $x$. Now $$ x_3 = \frac{x_2 + x_1 }{2}, $$   and so $$ x_4 = \frac{x_3 + x_2}{2} = \frac{ \frac{x_2 + x_1 }{2} + x_2  }{2} = \frac{ 3x_2 +  x_1 }{4},$$   and hence $$ x_5 = \frac{ x_4 + x_3 }{2} = \frac{ \frac{ 3x_2 +  x_1 }{4} + \frac{x_2 + x_1}{2} }{2} = \frac{ 5 x_2 + 3 x_1 }{8}. $$ Thence $$ x_6 = \frac{ x_5 + x_4 }{2} = \frac{ \frac{ 5 x_2 + 3 x_1 }{8} + \frac{ 3 x_2 +  x_1 }{4} }{2} = \frac{ 11 x_2 + 5 x_1 }{16}, $$   and so    $$ x_7 = \frac{ x_6 + x_5 }{2} = \frac{ \frac{ 11 x_2 + 5 x_1 }{16} + \frac{ 5 x_2 + 3 x_1 }{8} }{2} = \frac{ 21 x_2 + 11 x_1 }{32}. $$   Then    $$ x_8 = \frac{ x_7 + x_6 }{2} = \frac{ \frac{ 21 x_2 + 11 x_1 }{32} + \frac{ 11 x_2 + 5 x_1 }{ 16 } }{2} = \frac{ 43 x_2 + 21 x_1 }{ 64}, $$   and so    $$ x_9 = \frac{ x_8 + x_7 }{2} = \frac{ \frac{ 43 x_2 + 21 x_1 }{64} + \frac{ 21 x_2 + 11 x_1 }{32} }{2} = \frac{ 85 x_2 + 43 x_1 }{128}. $$ Generalising from these formulas for $x_3$ through $x_9$, we suppose that $k \in \mathbb{N}$ such that $k > 1$ and that    $$ x_{2k - 1} = \frac{ \frac{ 2^{2k-2} - 1}{3} x_2  +  \frac{ 2^{2k-3} + 1}{3}  x_1  }{ 2^{2k-3} } = \frac{  \left( 2^{2k-2} - 1 \right) x_2 + \left( 2^{2k-3} + 1 \right) x_1   }{ 3 \times 2^{2k-3} }, $$   and    $$ x_{2k} = \frac{ \frac{ 2^{2k-1} + 1}{3} x_2 + \frac{ 2^{2k-2} - 1}{3}  x_1  }{2^{2k-2}} = \frac{  \left( 2^{2k-1} + 1 \right) x_2 + \left( 2^{2k-2} - 1 \right)  x_1  }{ 3 \times 2^{2k-2} }. $$ Then we find that    $$ \begin{align} x_{2k+1} &= \frac{ x_{2k} + x_{2k-1} }{2} \\  &= \frac{ \frac{  \left( 2^{2k-1} + 1 \right) x_2 + \left( 2^{2k-2} - 1 \right)  x_1  }{ 3 \times 2^{2k-2} } +  \frac{  \left( 2^{2k-2} - 1 \right) x_2 + \left( 2^{2k-3} + 1 \right) x_1   }{ 3 \times 2^{2k-3} } }{2} \\ &= \frac{ \left( 2^{2k-1} + 1 \right) x_2 + \left( 2^{2k-2} - 1 \right)  x_1  + 2 \left( 2^{2k-2} - 1 \right) x_2 + 2 \left( 2^{2k-3} + 1 \right) x_1   }{ 3 \times 2^{2k-1} } \\ &= \frac{ \left( 2^{2k} - 1 \right) x_2 + \left( 2^{2k-1} + 1  \right) x_1  }{ 3 \times 2^{2k-1} },   \end{align} $$    and hence    $$ \begin{align}   x_{2k+2} &= \frac{ x_{2k+1} + x_{2k} }{2} \\ &= \frac{ \frac{ \left( 2^{2k} - 1 \right) x_2 + \left( 2^{2k-1} + 1  \right) x_1  }{ 3 \times 2^{2k-1} }  + \frac{  \left( 2^{2k-1} + 1 \right) x_2 + \left( 2^{2k-2} - 1 \right)  x_1  }{ 3 \times 2^{2k-2} } }{2}  \\ &= \frac{ \left( 2^{2k} - 1 \right) x_2 + \left( 2^{2k-1} + 1  \right) x_1   + 2 \left( 2^{2k-1} + 1 \right) x_2 + 2 \left( 2^{2k-2} - 1 \right)  x_1  }{ 3 \times 2^{2k}  } \\ &= \frac{ \left( 2^{2k+1} + 1 \right) x_2 + \left( 2^{2k} - 1 \right)x_1 }{ 3 \times 2^{2k} }.  \end{align} $$ Thus the induction is complete, and so we can conclude that, for every natural number $k > 1$, we have    $$ x_{2k-1} = \frac{  \left( 2^{2k-2} - 1 \right) x_2 + \left( 2^{2k-3} + 1 \right) x_1   }{ 3 \times 2^{2k-3} } =  \frac{1}{3} \left[  \left( 2 - \frac{1}{2^{2k-3}} \right) x_2 + \left( 1 + \frac{1}{2^{2k-3}} \right) x_1 \right], \tag{C} $$   and    $$ x_{2k} = \frac{  \left( 2^{2k-1} + 1 \right) x_2 + \left( 2^{2k-2} - 1 \right)  x_1  }{ 3 \times 2^{2k-2} } = \frac{1}{3} \left[ \left( 2 +  \frac{1}{2^{2k-2}} \right) x_2 + \left( 1 - \frac{1}{ 2^{2k-2} } \right) x_1  \right]. \tag{D} $$ Now as    $$ \lim_{k \to \infty} \frac{ 1 }{2^{2k-2} } = 0 = \lim_{k \to \infty} \frac{1}{2^{2k-3}}, $$   so from formulas (C) and (D), we can conclude that    $$ \lim_{k \to \infty} x_{2k-1} = \frac{ 2x_2 +  x_1 }{3} = \lim_{ k \to \infty} x_{2k}. $$   Therefore,    $$ \lim_{n \to \infty} x_n = \frac{ 2x_2 +  x_1 }{3}. $$ Is what I've done correct? I know I have protracted my solution way way beyond the solution expected, but what is the shorter approach to it, I wonder?","Here is Prob. 10, Sec. 3.5, in the book Introduction to Real Analysis by Rovert G. Bartle and Donald R. Sherbert, 4th edition: If $x_1 < x_2$ are arbitrary real numbers and $x_n \colon= \frac{1}{2} \left( x_{n-2} + x_{n-1} \right)$ for $n > 2$, show that $\left( x_n \right)$ is convergent. What is its limit? My Attempt: As $x_1 < x_2$, so    $$ x_1 < \frac{1}{2} \left( x_1 + x_2 \right) < x_2, $$   that is,    $$ x_1 < x_3 < x_2. \tag{1} $$   And also    $$ \left\lvert x_3 - x_2 \right\rvert = x_2 - x_3 = \frac{1}{2} \left( x_2 - x_1 \right). \tag{2} $$ Now as $x_3 < x_2$, so    $$ x_3 < \frac{1}{2} \left( x_3 + x_2 \right) < x_2, $$   that is,    $$ x_3 < x_4 < x_2. \tag{3} $$    And also    $$ \left\lvert x_4 - x_3 \right\rvert = x_4 - x_3 = \frac{1}{2} \left( x_2 - x_3 \right) = \frac{1}{2} \left\lvert x_2 - x_3 \right\rvert = \frac{1}{2^2} \left( x_2 - x_1 \right). \tag{4} $$ Now as $x_3 < x_4$, so    $$ x_3 < \frac{1}{2} \left( x_3 + x_4 \right) < x_4, $$   that is,    $$ x_3 < x_5 < x_4. \tag{5} $$   And also    $$ \left\lvert x_5 - x_4 \right\rvert = x_4 - x_5 = \frac{1}{2} \left( x_4 - x_3 \right) = \frac{1}{2} \left\lvert  x_4 - x_3 \right\rvert = \frac{1}{2^3} \left( x_2 - x_1 \right). \tag{6} $$ Now as $x_5 < x_4$, so    $$ x_5 < \frac{1}{2} \left( x_5 + x_4 \right) < x_4, $$   that is,    $$ x_5 < x_6 < x_4. \tag{7} $$   And also    $$ \left\lvert x_6 - x_5 \right\rvert = x_6 - x_5 = \frac{1}{2} \left( x_4 - x_5 \right) = \frac{1}{2} \left\lvert x_5 - x_4 \right\rvert = \frac{1}{2^4} \left( x_2 - x_1 \right). \tag{8} $$ From (1), (3), (5), and (7), we get    $$ x_1 < x_3 < x_5 < x_6 < x_4 < x_2. \tag{9} $$ Now let $k \in \mathbb{N}$ such that $k \geq 3$, and suppose that    $$ x_1 < x_3 < x_5 < \cdots < x_{2k-1} < x_{2k} < x_{2k-2} < \cdots < x_4 < x_2. \tag{10} $$   And suppose also that    $$ \left\lvert x_{2k} - x_{2k-1} \right\vert = \frac{1}{ 2^{2k-2} } \left( x_2 - x_1 \right). \tag{11} $$ Then as $x_{2k-1} < x_{2k}$, so    $$ x_{2k-1} < \frac{1}{2} \left( x_{2k-1} + x_{2k} \right) < x_{2k}, $$   that is,    $$ x_{2k-1} < x_{2k+1} < x_{2k }. \tag{12} $$   And also    $$ \left\lvert x_{2k+1} - x_{2k} \right\rvert = x_{2k}  - x_{2k+1} = \frac{1}{2} \left( x_{2k} - x_{2k-1} \right) = \frac{1}{2} \left\lvert  x_{2k} - x_{2k-1} \right\rvert = \frac{1}{ 2^{2k-1} } \left( x_2 - x_1 \right). \tag{13}  $$ Now as $x_{2k+1} < x_{2k}$, so    $$ x_{2k+1} < \frac{1}{2} \left( x_{2k+1} + x_{2k} \right) < x_{2k}, $$   that is,    $$ x_{2k+1} < x_{2k+2} < x_{2k}. \tag{14} $$ From  (10), (12), and (14), we can conclude that    $$ x_1 < x_3 < x_5 < \cdots < x_{2k-1} < x_{2k+1} < x_{2k+2} < x_{2k} < \cdots < x_6 < x_4 < x_2 \tag{15} $$   for all $k \in \mathbb{N}$. [I'm not really sure what to do with this relation though.] Also from (11) and (13), we can conclude that    $$ \left\lvert x_{n+1} - x_n \right\rvert = \frac{1}{2^{n-1} } \left( x_2 - x_1 \right). \tag{A} $$   for all $n \in \mathbb{N}$ such that $n \geq 2$. So for any natural numbers $m$ and $n$ such that $m > n$, we obtain   $$ \begin{align} \left\lvert x_m - x_n \right\rvert &\leq \left\lvert x_m - x_{m-1} \right\rvert + \cdots + \left\lvert x_{n+1} - x_n \right\rvert \\ &= \left( \frac{1}{2^{m-2} } + \cdots + \frac{1}{2^{n-1} } \right) \left( x_2 - x_1 \right) \\ &= \left( \frac{1}{2^{n-1} } + \frac{1}{2^{n} } \cdots + \frac{1}{2^{m-2} } \right) \left( x_2 - x_1 \right) \\ &= \frac{1}{2^{n-1}} \left( 1 + \frac{1}{2} + \cdots + \frac{1}{2^{m-n-1}} \right) \left( x_2 - x_1 \right) \\ &= \frac{1}{2^{n-1}} \frac{ 1 - \frac{1}{2^{m-n}} }{ 1 - \frac{1}{2} } \left( x_2 - x_1 \right) \\ &= \frac{1}{2^{n-2}} \left( 1 - \frac{1}{2^{m-n}} \right) \left( x_2 - x_1 \right) \\ &< \frac{1}{2^{n-2}}  \left( x_2 - x_1 \right). \tag{B} \end{align} $$ So, given $\varepsilon > 0$, if we choose a natural number $N$ so that    $$ N > \frac{ 4 \left(x_2 - x_1 \right)  }{ \varepsilon }, $$ then    $$ 2^N > N > \frac{ 4 \left(x_2 - x_1 \right)  }{ \varepsilon }, $$   and so    $$ \varepsilon > \frac{ 4 \left( x_2 - x_1 \right) }{ 2^N}.$$   and then we see from (B) that, whenever $m$ and $n$ are any natural numbers such that $m > n > N$, we have    $$ \left\lvert x_m - x_n \right\rvert < \frac{1}{2^{n-2}} \left( x_2 - x_1 \right) = \frac{4 \left( x_2 - x_1 \right) }{ 2^n} < \frac{4 \left( x_2 - x_1 \right)}{2^N} < \varepsilon. $$ Thus our sequence is a Cauchy sequence of real numbers. Therefore, this sequence is convergent. Let $x$ be the limit of this sequence. Then the subsequences    $\left( x_{2k-1} \right)_{k \in \mathbb{N}}$ and $\left( x_{2k } \right)_{k \in \mathbb{N}}$ also converge to this same limit $x$. Now $$ x_3 = \frac{x_2 + x_1 }{2}, $$   and so $$ x_4 = \frac{x_3 + x_2}{2} = \frac{ \frac{x_2 + x_1 }{2} + x_2  }{2} = \frac{ 3x_2 +  x_1 }{4},$$   and hence $$ x_5 = \frac{ x_4 + x_3 }{2} = \frac{ \frac{ 3x_2 +  x_1 }{4} + \frac{x_2 + x_1}{2} }{2} = \frac{ 5 x_2 + 3 x_1 }{8}. $$ Thence $$ x_6 = \frac{ x_5 + x_4 }{2} = \frac{ \frac{ 5 x_2 + 3 x_1 }{8} + \frac{ 3 x_2 +  x_1 }{4} }{2} = \frac{ 11 x_2 + 5 x_1 }{16}, $$   and so    $$ x_7 = \frac{ x_6 + x_5 }{2} = \frac{ \frac{ 11 x_2 + 5 x_1 }{16} + \frac{ 5 x_2 + 3 x_1 }{8} }{2} = \frac{ 21 x_2 + 11 x_1 }{32}. $$   Then    $$ x_8 = \frac{ x_7 + x_6 }{2} = \frac{ \frac{ 21 x_2 + 11 x_1 }{32} + \frac{ 11 x_2 + 5 x_1 }{ 16 } }{2} = \frac{ 43 x_2 + 21 x_1 }{ 64}, $$   and so    $$ x_9 = \frac{ x_8 + x_7 }{2} = \frac{ \frac{ 43 x_2 + 21 x_1 }{64} + \frac{ 21 x_2 + 11 x_1 }{32} }{2} = \frac{ 85 x_2 + 43 x_1 }{128}. $$ Generalising from these formulas for $x_3$ through $x_9$, we suppose that $k \in \mathbb{N}$ such that $k > 1$ and that    $$ x_{2k - 1} = \frac{ \frac{ 2^{2k-2} - 1}{3} x_2  +  \frac{ 2^{2k-3} + 1}{3}  x_1  }{ 2^{2k-3} } = \frac{  \left( 2^{2k-2} - 1 \right) x_2 + \left( 2^{2k-3} + 1 \right) x_1   }{ 3 \times 2^{2k-3} }, $$   and    $$ x_{2k} = \frac{ \frac{ 2^{2k-1} + 1}{3} x_2 + \frac{ 2^{2k-2} - 1}{3}  x_1  }{2^{2k-2}} = \frac{  \left( 2^{2k-1} + 1 \right) x_2 + \left( 2^{2k-2} - 1 \right)  x_1  }{ 3 \times 2^{2k-2} }. $$ Then we find that    $$ \begin{align} x_{2k+1} &= \frac{ x_{2k} + x_{2k-1} }{2} \\  &= \frac{ \frac{  \left( 2^{2k-1} + 1 \right) x_2 + \left( 2^{2k-2} - 1 \right)  x_1  }{ 3 \times 2^{2k-2} } +  \frac{  \left( 2^{2k-2} - 1 \right) x_2 + \left( 2^{2k-3} + 1 \right) x_1   }{ 3 \times 2^{2k-3} } }{2} \\ &= \frac{ \left( 2^{2k-1} + 1 \right) x_2 + \left( 2^{2k-2} - 1 \right)  x_1  + 2 \left( 2^{2k-2} - 1 \right) x_2 + 2 \left( 2^{2k-3} + 1 \right) x_1   }{ 3 \times 2^{2k-1} } \\ &= \frac{ \left( 2^{2k} - 1 \right) x_2 + \left( 2^{2k-1} + 1  \right) x_1  }{ 3 \times 2^{2k-1} },   \end{align} $$    and hence    $$ \begin{align}   x_{2k+2} &= \frac{ x_{2k+1} + x_{2k} }{2} \\ &= \frac{ \frac{ \left( 2^{2k} - 1 \right) x_2 + \left( 2^{2k-1} + 1  \right) x_1  }{ 3 \times 2^{2k-1} }  + \frac{  \left( 2^{2k-1} + 1 \right) x_2 + \left( 2^{2k-2} - 1 \right)  x_1  }{ 3 \times 2^{2k-2} } }{2}  \\ &= \frac{ \left( 2^{2k} - 1 \right) x_2 + \left( 2^{2k-1} + 1  \right) x_1   + 2 \left( 2^{2k-1} + 1 \right) x_2 + 2 \left( 2^{2k-2} - 1 \right)  x_1  }{ 3 \times 2^{2k}  } \\ &= \frac{ \left( 2^{2k+1} + 1 \right) x_2 + \left( 2^{2k} - 1 \right)x_1 }{ 3 \times 2^{2k} }.  \end{align} $$ Thus the induction is complete, and so we can conclude that, for every natural number $k > 1$, we have    $$ x_{2k-1} = \frac{  \left( 2^{2k-2} - 1 \right) x_2 + \left( 2^{2k-3} + 1 \right) x_1   }{ 3 \times 2^{2k-3} } =  \frac{1}{3} \left[  \left( 2 - \frac{1}{2^{2k-3}} \right) x_2 + \left( 1 + \frac{1}{2^{2k-3}} \right) x_1 \right], \tag{C} $$   and    $$ x_{2k} = \frac{  \left( 2^{2k-1} + 1 \right) x_2 + \left( 2^{2k-2} - 1 \right)  x_1  }{ 3 \times 2^{2k-2} } = \frac{1}{3} \left[ \left( 2 +  \frac{1}{2^{2k-2}} \right) x_2 + \left( 1 - \frac{1}{ 2^{2k-2} } \right) x_1  \right]. \tag{D} $$ Now as    $$ \lim_{k \to \infty} \frac{ 1 }{2^{2k-2} } = 0 = \lim_{k \to \infty} \frac{1}{2^{2k-3}}, $$   so from formulas (C) and (D), we can conclude that    $$ \lim_{k \to \infty} x_{2k-1} = \frac{ 2x_2 +  x_1 }{3} = \lim_{ k \to \infty} x_{2k}. $$   Therefore,    $$ \lim_{n \to \infty} x_n = \frac{ 2x_2 +  x_1 }{3}. $$ Is what I've done correct? I know I have protracted my solution way way beyond the solution expected, but what is the shorter approach to it, I wonder?",,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
86,"Is it true that $\lim_{n\to\infty}\sum_{i=1}^nx_{i,n}=\sum_{i=1}^\infty x_{i,\infty}$?",Is it true that ?,"\lim_{n\to\infty}\sum_{i=1}^nx_{i,n}=\sum_{i=1}^\infty x_{i,\infty}","Suppose that $\{x_{i,n}:1\le i\le n,n\ge1\}$ is a real-valued triangular array such that the limit $\lim_{n\to\infty}\sum_{i=1}^nx_{i,n}$ exists and $x_{i,n}\to x_{i,\infty}$ for each $i\ge1$ as $n\to\infty$. Is it true that   $ \lim_{n\to\infty}\sum_{i=1}^nx_{i,n}=\sum_{i=1}^\infty x_{i,\infty}? $   If not, when can we say that it is true? It is quite easy to construct an example where this is the case (for instance, $x_{i,n}=a_ia_n$, where $\sum_{i=1}^\infty a_i$ converges and $\lim_{n\to\infty} a_n$ exists). I do not think that this is always the case even though I am struggling to construct a counterexample. It seems that we might be able to use the dominated convergence theorem, but we actually have just one limit and I do not think that the dominated convergence theorem is applicable in this situation. Any help is much appreciated!","Suppose that $\{x_{i,n}:1\le i\le n,n\ge1\}$ is a real-valued triangular array such that the limit $\lim_{n\to\infty}\sum_{i=1}^nx_{i,n}$ exists and $x_{i,n}\to x_{i,\infty}$ for each $i\ge1$ as $n\to\infty$. Is it true that   $ \lim_{n\to\infty}\sum_{i=1}^nx_{i,n}=\sum_{i=1}^\infty x_{i,\infty}? $   If not, when can we say that it is true? It is quite easy to construct an example where this is the case (for instance, $x_{i,n}=a_ia_n$, where $\sum_{i=1}^\infty a_i$ converges and $\lim_{n\to\infty} a_n$ exists). I do not think that this is always the case even though I am struggling to construct a counterexample. It seems that we might be able to use the dominated convergence theorem, but we actually have just one limit and I do not think that the dominated convergence theorem is applicable in this situation. Any help is much appreciated!",,"['sequences-and-series', 'limits', 'convergence-divergence']"
87,Find the value of $\lim \limits_{x \to \pi /3} \frac{(1-\cos6x)^{1/2}}{\sqrt 2 (\frac \pi3 - x)}$,Find the value of,\lim \limits_{x \to \pi /3} \frac{(1-\cos6x)^{1/2}}{\sqrt 2 (\frac \pi3 - x)},"The question from my textbook requires us to find the value of the below limit: $$\lim \limits_{x \to \pi /3} \frac{(1-\cos6x)^{1/2}}{\sqrt 2 (\pi/3 - x)}$$ On using the trigonometric identity $\cos2x = 1-2\sin^2x$ , the above limit reduces to: $$\lim \limits_{x \to \pi /3} \frac{\lvert \sin3x \rvert}{\pi/3 - x}$$ On substituting $\frac \pi3 - x$ by $y$ the above limit becomes: $$\lim \limits_{y \to 0} \frac{\lvert \sin3y \rvert}y$$ In this case, the left hand and right hand limits are unequal, being equal to $-3$ and $3$ respectively. Thus, $$\lim \limits_{x \to \pi /3} \frac{(1-\cos6x)^{1/2}}{\sqrt 2 (\pi/3 - x)} \text{ does not exist.}$$ But my textbook gives the answer as $3$ . It was apparently because it considered $(1-\cos6x)^{1/2}$ as equal to $\sin 3x$ and not as equal to $\lvert \sin3x \rvert$ . But since $x \to \pi/3$ , shouldn't $(\sin^2 3x)^{1/2}$ equal $\lvert \sin3x \rvert$ ? Is my argument incorrect? Any help would be appreciated.","The question from my textbook requires us to find the value of the below limit: On using the trigonometric identity , the above limit reduces to: On substituting by the above limit becomes: In this case, the left hand and right hand limits are unequal, being equal to and respectively. Thus, But my textbook gives the answer as . It was apparently because it considered as equal to and not as equal to . But since , shouldn't equal ? Is my argument incorrect? Any help would be appreciated.",\lim \limits_{x \to \pi /3} \frac{(1-\cos6x)^{1/2}}{\sqrt 2 (\pi/3 - x)} \cos2x = 1-2\sin^2x \lim \limits_{x \to \pi /3} \frac{\lvert \sin3x \rvert}{\pi/3 - x} \frac \pi3 - x y \lim \limits_{y \to 0} \frac{\lvert \sin3y \rvert}y -3 3 \lim \limits_{x \to \pi /3} \frac{(1-\cos6x)^{1/2}}{\sqrt 2 (\pi/3 - x)} \text{ does not exist.} 3 (1-\cos6x)^{1/2} \sin 3x \lvert \sin3x \rvert x \to \pi/3 (\sin^2 3x)^{1/2} \lvert \sin3x \rvert,"['limits', 'proof-verification']"
88,Does this $a$ exists and how to calculate it if it exists?,Does this  exists and how to calculate it if it exists?,a,"I was trying to solve some problem from a question here on MSE by first trying to find something about simplified version, and, if I calculated correctly I obtained: $$\lim_{n \to + \infty} (\sqrt{a})^{3^n} \cdot \prod_{k=2}^{n} (\dfrac {a}{k})^{3^{n-k+1}}=1$$ Even if my calculations are not right this is interesting in itself and what I would like to know is does such an $a$ exists? That is, even though I obtained this result and am trying to find closed form for $a$ (or an approximation) I really do not know even if there exists an $a$ that would satisfy this limit problem. Does it exists? How to calculate it if it exists?","I was trying to solve some problem from a question here on MSE by first trying to find something about simplified version, and, if I calculated correctly I obtained: $$\lim_{n \to + \infty} (\sqrt{a})^{3^n} \cdot \prod_{k=2}^{n} (\dfrac {a}{k})^{3^{n-k+1}}=1$$ Even if my calculations are not right this is interesting in itself and what I would like to know is does such an $a$ exists? That is, even though I obtained this result and am trying to find closed form for $a$ (or an approximation) I really do not know even if there exists an $a$ that would satisfy this limit problem. Does it exists? How to calculate it if it exists?",,['limits']
89,Does this hold:$\lim_{n \to \infty}{\ln n\over e^n}\int_{0}^{n}{\sinh{x}\over \ln(1+x)}\mathrm dx={1\over 2}$,Does this hold:,\lim_{n \to \infty}{\ln n\over e^n}\int_{0}^{n}{\sinh{x}\over \ln(1+x)}\mathrm dx={1\over 2},"Given the limit $$\lim_{n \to \infty}{\ln n\over e^n}\int_{0}^{n}{\sinh{x}\over \ln(1+x)}\mathrm dx={1\over 2}\tag1$$ How did I came across this integral $(1)$? I was observing this integral $(2)$ $$\int_{0}^{n}{\sinh{x}\over \ln(1+x)}\mathrm dx=C\tag2$$ I notice that  $$C\approx e^n$$ With a few lucky guess, I found it to be $$\ln C\approx n-\ln(\ln n)-\ln2$$ and simplifies it to $(1)$ Does the limit in $(1)$ hold?","Given the limit $$\lim_{n \to \infty}{\ln n\over e^n}\int_{0}^{n}{\sinh{x}\over \ln(1+x)}\mathrm dx={1\over 2}\tag1$$ How did I came across this integral $(1)$? I was observing this integral $(2)$ $$\int_{0}^{n}{\sinh{x}\over \ln(1+x)}\mathrm dx=C\tag2$$ I notice that  $$C\approx e^n$$ With a few lucky guess, I found it to be $$\ln C\approx n-\ln(\ln n)-\ln2$$ and simplifies it to $(1)$ Does the limit in $(1)$ hold?",,['calculus']
90,A function with $\lim_{x \to \pm \infty} f(f(x))=\pm \infty$,A function with,\lim_{x \to \pm \infty} f(f(x))=\pm \infty,"This is regarding my other problem : Let $f: \mathbb{R} \to \mathbb{R}$ such that  $\displaystyle \lim_{x \to \infty} f(f(x))= \infty$ and $\displaystyle \lim_{x \to -\infty} f(f(x))= -\infty$ and $f$ has the intermediate value property (it is not necessarily continuous). Prove that $\displaystyle\lim_{x \to \infty}f(x)$ and $\displaystyle\lim_{x \to -\infty}f(x)$ exist and are infinite. I eventually found the official solution, but I don't quite understand it. It goes like this, after proving the limits exist: Suppose $\lim_{x \to \infty}f(x)=a \in \mathbb{R}$. Let $M=\{x \in \mathbb{R} \mid f(x)=a \}$. If $M$ were unbounded above, then there is $(x_n) \to \infty$ such that $f(x_n)=a$. But we know $\lim_{n \to \infty}f(f(x_n))=\infty$ and so we would have $f(a)=\infty$, which can't be. So $M$ is either empty, or bounded above. However it is, there is $\delta>0$ such that $f(x) \neq a, \: \forall x \geq \delta$. Since f has the intermediate value property, it follows that $f(x)<a, \: \forall x \geq \delta$ or $f(x)>a, \forall x \geq \delta$. Without loss of generality, suppose $f(x)<a, \: \forall x \geq \delta$. We will prove $\lim_{x \nearrow a}f(x)=\infty$. Let $(y_n)$ be strictly increasing and convergent to $a$. Then there is $n_0$ such that $f(\delta)<y_n<a, \: \forall n \geq n_0$. But $\lim_{x \to \infty}f(x)=a$, so by the intermediate value theorem, the set $$M_n=\{x \in (\delta, \infty) \mid f(x)=y_n \}$$ is not empty. If it were unbounded above, we would get a contradiction as we did before. Thus it is bounded and let $x_n=\sup M_n \in \mathbb{R}$. Then $x_n \in M_n$, so $f(x_n)=y_n$. Then $(x_n)$ is strictly increasing, because $(y_n)$ is strictly increasing ... This is the point which bothers me in their solution. They don't prove why $x_n \in M_n$, which I have been trying for a while, but couldn't do it. This is the source of my recent statement from here","This is regarding my other problem : Let $f: \mathbb{R} \to \mathbb{R}$ such that  $\displaystyle \lim_{x \to \infty} f(f(x))= \infty$ and $\displaystyle \lim_{x \to -\infty} f(f(x))= -\infty$ and $f$ has the intermediate value property (it is not necessarily continuous). Prove that $\displaystyle\lim_{x \to \infty}f(x)$ and $\displaystyle\lim_{x \to -\infty}f(x)$ exist and are infinite. I eventually found the official solution, but I don't quite understand it. It goes like this, after proving the limits exist: Suppose $\lim_{x \to \infty}f(x)=a \in \mathbb{R}$. Let $M=\{x \in \mathbb{R} \mid f(x)=a \}$. If $M$ were unbounded above, then there is $(x_n) \to \infty$ such that $f(x_n)=a$. But we know $\lim_{n \to \infty}f(f(x_n))=\infty$ and so we would have $f(a)=\infty$, which can't be. So $M$ is either empty, or bounded above. However it is, there is $\delta>0$ such that $f(x) \neq a, \: \forall x \geq \delta$. Since f has the intermediate value property, it follows that $f(x)<a, \: \forall x \geq \delta$ or $f(x)>a, \forall x \geq \delta$. Without loss of generality, suppose $f(x)<a, \: \forall x \geq \delta$. We will prove $\lim_{x \nearrow a}f(x)=\infty$. Let $(y_n)$ be strictly increasing and convergent to $a$. Then there is $n_0$ such that $f(\delta)<y_n<a, \: \forall n \geq n_0$. But $\lim_{x \to \infty}f(x)=a$, so by the intermediate value theorem, the set $$M_n=\{x \in (\delta, \infty) \mid f(x)=y_n \}$$ is not empty. If it were unbounded above, we would get a contradiction as we did before. Thus it is bounded and let $x_n=\sup M_n \in \mathbb{R}$. Then $x_n \in M_n$, so $f(x_n)=y_n$. Then $(x_n)$ is strictly increasing, because $(y_n)$ is strictly increasing ... This is the point which bothers me in their solution. They don't prove why $x_n \in M_n$, which I have been trying for a while, but couldn't do it. This is the source of my recent statement from here",,"['calculus', 'real-analysis', 'limits', 'proof-explanation', 'supremum-and-infimum']"
91,Multivariable delta-epsilon limit proofs,Multivariable delta-epsilon limit proofs,,"The Question asks: Find the delta epsilon limit if it exist $$ \lim \limits_{(x, y) \to (0, 0)} \ \frac{x^2 y^3}{2x^2 + y^2} = 1$$ What I've done so far: $$ \mid {\frac{x^2 y^3}{2x^2 + y^2} - 1 \mid} < \epsilon \quad \quad \quad \quad0 < \sqrt{x^2 + y^2} < \delta $$ Now, since $x ^2 \leq 2x^2 + y^ \implies \frac{x^2}{2x^2 + y^2} \leq 1$ Then: $$ \frac{x^2 \mid y^3 \mid}{2x^2 + y^2} \leq \mid y^3 \mid = \sqrt{y^2}^3 = \quad ...$$ Here is where I am stuck, I can't make $\sqrt{y^2}^3$ into $\sqrt{ x^2 + y^2} $. How should I proceed further? Using Stewart's Calculus (8th ed) 14.2 Limit proof method","The Question asks: Find the delta epsilon limit if it exist $$ \lim \limits_{(x, y) \to (0, 0)} \ \frac{x^2 y^3}{2x^2 + y^2} = 1$$ What I've done so far: $$ \mid {\frac{x^2 y^3}{2x^2 + y^2} - 1 \mid} < \epsilon \quad \quad \quad \quad0 < \sqrt{x^2 + y^2} < \delta $$ Now, since $x ^2 \leq 2x^2 + y^ \implies \frac{x^2}{2x^2 + y^2} \leq 1$ Then: $$ \frac{x^2 \mid y^3 \mid}{2x^2 + y^2} \leq \mid y^3 \mid = \sqrt{y^2}^3 = \quad ...$$ Here is where I am stuck, I can't make $\sqrt{y^2}^3$ into $\sqrt{ x^2 + y^2} $. How should I proceed further? Using Stewart's Calculus (8th ed) 14.2 Limit proof method",,"['limits', 'multivariable-calculus', 'epsilon-delta']"
92,How do you get to $e^x$ from $e$'s definition?,How do you get to  from 's definition?,e^x e,"$$e=\lim_{n\to\infty}\left(1+\frac 1n\right)^n$$ This is based on Bernoulli's compound interest definition. But let's say we want to find $e^x$ now. $$e^x=\lim_{n\to\infty}\left(1+\frac 1n\right)^{nx}$$ Let $m = nx$ so $n = m/x$. As $n$ goes to infinity, so does $m$, so: $$e^x=\lim_{m\to\infty}\left(1+\frac xm\right)^{m}$$ This now looks like the definition you usually see but apparently this is an invalid proof of the fact because $n$ is supposed to be an integer(??) and $m/x$ may not be. How are you supposed to get from one to the other then?","$$e=\lim_{n\to\infty}\left(1+\frac 1n\right)^n$$ This is based on Bernoulli's compound interest definition. But let's say we want to find $e^x$ now. $$e^x=\lim_{n\to\infty}\left(1+\frac 1n\right)^{nx}$$ Let $m = nx$ so $n = m/x$. As $n$ goes to infinity, so does $m$, so: $$e^x=\lim_{m\to\infty}\left(1+\frac xm\right)^{m}$$ This now looks like the definition you usually see but apparently this is an invalid proof of the fact because $n$ is supposed to be an integer(??) and $m/x$ may not be. How are you supposed to get from one to the other then?",,"['calculus', 'limits', 'exponential-function', 'definition', 'proof-explanation']"
93,Cesaro-Stolz limit,Cesaro-Stolz limit,,"We have $x_{n+1}=x_n+\dfrac{\sqrt n}{x_n}$ with $x_1=1$ and we have to compute $\displaystyle\lim_{n\to\infty} \dfrac{x_n}{n^{3/4}}$. I already proved that $x_n$ is strictly increasing and tends to infinity and I know that we can use the Cesaro-Stolz Lemma to solve it but it hasn't been successful for me. I tried to apply it for $\displaystyle\lim_{n\to\infty} \dfrac{(x_n)^4}{n^3}$, but got stuck.","We have $x_{n+1}=x_n+\dfrac{\sqrt n}{x_n}$ with $x_1=1$ and we have to compute $\displaystyle\lim_{n\to\infty} \dfrac{x_n}{n^{3/4}}$. I already proved that $x_n$ is strictly increasing and tends to infinity and I know that we can use the Cesaro-Stolz Lemma to solve it but it hasn't been successful for me. I tried to apply it for $\displaystyle\lim_{n\to\infty} \dfrac{(x_n)^4}{n^3}$, but got stuck.",,"['real-analysis', 'sequences-and-series', 'limits', 'recurrence-relations']"
94,Proving the product rule for limits,Proving the product rule for limits,,"Proof: $\lim_{x \to a} f(x)g(x) = \lim_{x \to a} f(x) \lim_{x \to a} g(x)$ Let $L_1 = \lim_{x \to a} f(x)$ and $L_2 = \lim_{x \to a} g(x)$. We assume $L_1, L_2 \neq 0$ (for scenarios where $L_1$ or $L_2$ are $0$, the result is trivially true). Let $\epsilon > 0$. We need a $\delta$ such that $|f(x)g(x) - L_1L_2| < \epsilon$ whenever $0 < |x-a| < \delta$. We can rewrite $$\begin{align}|f(x)g(x) - L_1L_2| &= |(f(x)-L_1)(g(x)-L_2) + L_1(g(x)-L_2) + L_2(f(x)-L_1)| \\ &\leq |f(x)-L_1||g(x)-L_2| + |L_1||g(x)-L_2| + |L_2||f(x)-L_1| \\  &< \epsilon\end{align}$$ by triangle inequality. Let $\delta_1 > 0$ such that $|f(x)-L_1| < \sqrt{\frac{\epsilon}{3}}$ whenever $0 < |x-a| < \delta_1$. Let $\delta_2 > 0$ such that $|g(x)-L_2| < \sqrt{\frac{\epsilon}{3}}$ whenever $0 < |x-a| < \delta_2$. Let $\delta_3 > 0$ such that $|g(x)-L_2| < \frac{\epsilon}{3|L_1|}$ whenever $0 < |x-a| < \delta_3$. Let $\delta_4 > 0$ such that $|f(x)-L_1| < \frac{\epsilon}{3|L_2|}$ whenever $0 < |x-a| < \delta_4$. Suppose that $0 < |x-a| < \delta$ where $\delta = \min(\delta_1, \delta_2, \delta_3, \delta_4)$. Then we have: $$\begin{align}|f(x)-L_1||g(x)-L_2| &+ |L_1||g(x)-L_2| + |L_2||f(x)-L_1| \\&< \sqrt{\frac{\epsilon}{3}} \cdot \sqrt{\frac{\epsilon}{3}} + |L_1|\frac{\epsilon}{3|L_1|} + |L_2|\frac{\epsilon}{3|L_2|} \\&= \epsilon \end{align}$$ Is this a correct proof?","Proof: $\lim_{x \to a} f(x)g(x) = \lim_{x \to a} f(x) \lim_{x \to a} g(x)$ Let $L_1 = \lim_{x \to a} f(x)$ and $L_2 = \lim_{x \to a} g(x)$. We assume $L_1, L_2 \neq 0$ (for scenarios where $L_1$ or $L_2$ are $0$, the result is trivially true). Let $\epsilon > 0$. We need a $\delta$ such that $|f(x)g(x) - L_1L_2| < \epsilon$ whenever $0 < |x-a| < \delta$. We can rewrite $$\begin{align}|f(x)g(x) - L_1L_2| &= |(f(x)-L_1)(g(x)-L_2) + L_1(g(x)-L_2) + L_2(f(x)-L_1)| \\ &\leq |f(x)-L_1||g(x)-L_2| + |L_1||g(x)-L_2| + |L_2||f(x)-L_1| \\  &< \epsilon\end{align}$$ by triangle inequality. Let $\delta_1 > 0$ such that $|f(x)-L_1| < \sqrt{\frac{\epsilon}{3}}$ whenever $0 < |x-a| < \delta_1$. Let $\delta_2 > 0$ such that $|g(x)-L_2| < \sqrt{\frac{\epsilon}{3}}$ whenever $0 < |x-a| < \delta_2$. Let $\delta_3 > 0$ such that $|g(x)-L_2| < \frac{\epsilon}{3|L_1|}$ whenever $0 < |x-a| < \delta_3$. Let $\delta_4 > 0$ such that $|f(x)-L_1| < \frac{\epsilon}{3|L_2|}$ whenever $0 < |x-a| < \delta_4$. Suppose that $0 < |x-a| < \delta$ where $\delta = \min(\delta_1, \delta_2, \delta_3, \delta_4)$. Then we have: $$\begin{align}|f(x)-L_1||g(x)-L_2| &+ |L_1||g(x)-L_2| + |L_2||f(x)-L_1| \\&< \sqrt{\frac{\epsilon}{3}} \cdot \sqrt{\frac{\epsilon}{3}} + |L_1|\frac{\epsilon}{3|L_1|} + |L_2|\frac{\epsilon}{3|L_2|} \\&= \epsilon \end{align}$$ Is this a correct proof?",,"['calculus', 'limits', 'proof-verification', 'proof-writing', 'epsilon-delta']"
95,Show that $a_n = f(0) + f(1) + ... + f(n-1) - \int^n_0 f(x) dx$ is a Cauchy sequence,Show that  is a Cauchy sequence,a_n = f(0) + f(1) + ... + f(n-1) - \int^n_0 f(x) dx,"Suppose $f(x)$ is continuous and decreasing on $[0, \infty]$, and $f(n) \rightarrow 0$. Define $\{a_n\}$ by $$a_n = f(0) + f(1) + ... + f(n-1) - \int^n_0 f(x) dx$$ (a) Prove $\{a_n\}$ is a Cauchy sequence directly from the definition. (b) Evaluate $\lim a_n$ if $f(x) = e^{-x}$. I started out by examining $a_{n+1} - a_n$: $$a_{n+1} - a_n = f(n) - \int_n^{n+1} f(x)dx$$ Then, by the Triangle Inequality: $$|a_m - a_n| \leq |a_{n+1} - a_n| + |a_{n+2} - a_{n+1}| + ... + |a_{m} - a_{m-1}|$$ and so, $$|a_m - a_n| \leq \sum_{k = n}^m f(k) - \int_n^{m} f(x)dx$$ And I am lost afterwards. Am I on the right track? How can I use this expression to also prove the limit for $f(x) = e^{-x}$?","Suppose $f(x)$ is continuous and decreasing on $[0, \infty]$, and $f(n) \rightarrow 0$. Define $\{a_n\}$ by $$a_n = f(0) + f(1) + ... + f(n-1) - \int^n_0 f(x) dx$$ (a) Prove $\{a_n\}$ is a Cauchy sequence directly from the definition. (b) Evaluate $\lim a_n$ if $f(x) = e^{-x}$. I started out by examining $a_{n+1} - a_n$: $$a_{n+1} - a_n = f(n) - \int_n^{n+1} f(x)dx$$ Then, by the Triangle Inequality: $$|a_m - a_n| \leq |a_{n+1} - a_n| + |a_{n+2} - a_{n+1}| + ... + |a_{m} - a_{m-1}|$$ and so, $$|a_m - a_n| \leq \sum_{k = n}^m f(k) - \int_n^{m} f(x)dx$$ And I am lost afterwards. Am I on the right track? How can I use this expression to also prove the limit for $f(x) = e^{-x}$?",,"['real-analysis', 'sequences-and-series', 'limits', 'cauchy-sequences']"
96,Prove that the sequence is bounded and subsequences converge,Prove that the sequence is bounded and subsequences converge,,"Sequence is defined in the following way: $x_1 = \frac{2}{3}$, $x_{2n+1} = \frac{x_{2n}}{3} + \frac{2}{3} $, $x_{2n} = \frac{x_{2n-1}}{3}$ I need to show that the sequence is bounded, i.e  $0<x_{n}<1$, and it does not converge.  This is what I have done so far: I have separated odd and even elements as the following subsequences: $x_{2n+1} = \frac{x_{2n-1}}{9} + \frac{2}{3} $ $x_{2n} = \frac{x_{2n-2}}{9} + \frac{2}{9} $ And I have found that $x_{2n+1} \to \frac{3}{4} $ and  $x_{2n} \to \frac{1}{4} $ I know that $x_n$ is divergent, as the subsequential limits are not equal, but I have trouble showing that: $x_n$ is bounded from above. Do I need to look at subsequences separately and show that they are monotone or do I need to somehow estimate $x_n$? Showing that $x_{2n}$ and $x_{2n+1}$ are convergent","Sequence is defined in the following way: $x_1 = \frac{2}{3}$, $x_{2n+1} = \frac{x_{2n}}{3} + \frac{2}{3} $, $x_{2n} = \frac{x_{2n-1}}{3}$ I need to show that the sequence is bounded, i.e  $0<x_{n}<1$, and it does not converge.  This is what I have done so far: I have separated odd and even elements as the following subsequences: $x_{2n+1} = \frac{x_{2n-1}}{9} + \frac{2}{3} $ $x_{2n} = \frac{x_{2n-2}}{9} + \frac{2}{9} $ And I have found that $x_{2n+1} \to \frac{3}{4} $ and  $x_{2n} \to \frac{1}{4} $ I know that $x_n$ is divergent, as the subsequential limits are not equal, but I have trouble showing that: $x_n$ is bounded from above. Do I need to look at subsequences separately and show that they are monotone or do I need to somehow estimate $x_n$? Showing that $x_{2n}$ and $x_{2n+1}$ are convergent",,"['real-analysis', 'sequences-and-series', 'limits']"
97,Trying to understand discontinuities and domains,Trying to understand discontinuities and domains,,"I am really struggling with this concept and I feel like no math book clarifies it sufficiently. First off: I assume that if a one-sided limit exists then it is a real value and not $\pm \infty$. And a two-sided limit exists if both one-sided limits exist (i.e. both limits are real values) and are also equal to each other in value. My understanding of continuity falls into three cases / mental notes: A function $f$ is considered continuous (i.e. continuous everywhere) iff it is continuous at every point in its domain. For a real endpoint $a$ of a domain interval: The function is continuous at $a$ if the one-sided limit $L$ exists and $L = f(a)$ (if the endpoint is on the left side of the interval, we refer to the one-sided limit from the right, and if the endpoint is on the right side of the interval, we refer to the one-sided limit from the left). Moreover $a$ as well as the infinitely-close $x$ value to $a$ must also be in the domain. For a real point $a$ within a domain interval, i.e. not the endpoints. The function is continuous at $a$ if the two-sided limit $L$ exists and $L = f(a)$. Moreover $a$ as well as the infinitely-close $x$ values on both sides of $a$ must also be in the domain. If this is true so far then: Removable Singularities: An undefined (not in the domain) or vertically-shifted (in the domain) single point $a$ where the two-sided limit at $x=a$ nevertheless exists. This would be considered a discontinuity in the function either way due to definition 3 above. The two-sided limit $L$ exists, but $L \neq f(a)$ (although I am unsure if it makes sense to use $\neq$ if we're talking about $f(a)$ when undefined at $a$). Jump Discontinuities This is any value of $x=a$ in the domain where the left-sided limit and right-sided limit exist, but they approach different real values ($f(a)$ itself may or may not be defined). Now here's where I start to get fuzzy. If $f(a)$ is defined, the function is discontinuous by definition 3 since the two-sided limit does not exist. However, if $f(a)$ were undefined, then this would weirdly enough be continuous since we move into definition 2 for two intervals. Since $a$ is not part of the domain, we've just split things into two separate intervals, both of which would be considered continuous on their own based on endpoint continuity definition. Is this right? Infinite/Essential Discontinuities (are these also Nonremovable Singularities?) This is where I am even fuzzier. I believe these exist only when $f(a)$ is undefined, i.e. not in the domain, when at least one of the one-sided limits on either side does not exist (either because it approaches $\pm \infty$ or because it's oscillating forever and never settles down anywhere). I'm even less certain about functions that are only defined in one direction but still have an asymptote. For example if we defined $f(x) = 1/x$ for $x>0$ only, would the asymptote at $x=0$ be considered an infinite discontinuity? But let's say we had $f(x) = 1/x$ where $x \neq 0$. Would we say there is an infinite discontinuity at $x=0$ since we approach infinity in at least one direction, and yet this would not affect the overall continuity of $f$ since $0$ is not part of the domain? I'm really unclear on how all of this works.","I am really struggling with this concept and I feel like no math book clarifies it sufficiently. First off: I assume that if a one-sided limit exists then it is a real value and not $\pm \infty$. And a two-sided limit exists if both one-sided limits exist (i.e. both limits are real values) and are also equal to each other in value. My understanding of continuity falls into three cases / mental notes: A function $f$ is considered continuous (i.e. continuous everywhere) iff it is continuous at every point in its domain. For a real endpoint $a$ of a domain interval: The function is continuous at $a$ if the one-sided limit $L$ exists and $L = f(a)$ (if the endpoint is on the left side of the interval, we refer to the one-sided limit from the right, and if the endpoint is on the right side of the interval, we refer to the one-sided limit from the left). Moreover $a$ as well as the infinitely-close $x$ value to $a$ must also be in the domain. For a real point $a$ within a domain interval, i.e. not the endpoints. The function is continuous at $a$ if the two-sided limit $L$ exists and $L = f(a)$. Moreover $a$ as well as the infinitely-close $x$ values on both sides of $a$ must also be in the domain. If this is true so far then: Removable Singularities: An undefined (not in the domain) or vertically-shifted (in the domain) single point $a$ where the two-sided limit at $x=a$ nevertheless exists. This would be considered a discontinuity in the function either way due to definition 3 above. The two-sided limit $L$ exists, but $L \neq f(a)$ (although I am unsure if it makes sense to use $\neq$ if we're talking about $f(a)$ when undefined at $a$). Jump Discontinuities This is any value of $x=a$ in the domain where the left-sided limit and right-sided limit exist, but they approach different real values ($f(a)$ itself may or may not be defined). Now here's where I start to get fuzzy. If $f(a)$ is defined, the function is discontinuous by definition 3 since the two-sided limit does not exist. However, if $f(a)$ were undefined, then this would weirdly enough be continuous since we move into definition 2 for two intervals. Since $a$ is not part of the domain, we've just split things into two separate intervals, both of which would be considered continuous on their own based on endpoint continuity definition. Is this right? Infinite/Essential Discontinuities (are these also Nonremovable Singularities?) This is where I am even fuzzier. I believe these exist only when $f(a)$ is undefined, i.e. not in the domain, when at least one of the one-sided limits on either side does not exist (either because it approaches $\pm \infty$ or because it's oscillating forever and never settles down anywhere). I'm even less certain about functions that are only defined in one direction but still have an asymptote. For example if we defined $f(x) = 1/x$ for $x>0$ only, would the asymptote at $x=0$ be considered an infinite discontinuity? But let's say we had $f(x) = 1/x$ where $x \neq 0$. Would we say there is an infinite discontinuity at $x=0$ since we approach infinity in at least one direction, and yet this would not affect the overall continuity of $f$ since $0$ is not part of the domain? I'm really unclear on how all of this works.",,"['calculus', 'limits', 'continuity', 'terminology', 'definition']"
98,"Convergence radius of power series $\sum_{n=0}^\infty c_nz^n $ where $c_0 = 0$, $c_1 = 1$, $c_n =\frac{c_{n-1} + c_{n-2}}{2}$","Convergence radius of power series  where , ,",\sum_{n=0}^\infty c_nz^n  c_0 = 0 c_1 = 1 c_n =\frac{c_{n-1} + c_{n-2}}{2},"In my homework I'm supposed to find the convergence radius of the power set in the title. I'm allowed to assume $C = \lim_{n\to\infty} {c_{n+1}\over c_n}$ exists and we should try to figure out that $C$ has to be. I found $${c_{n+1}\over {c_n}} = {1\over 2} + {c_{n-1}\over{2c_n}}  $$ so for $n\to\infty$: $$C = {1\over2} + {1\over{2C}}  \Leftrightarrow C=1$$ which doesn't really help me further. Investigating $c_n - c_{n-1}$ didn't help me out either. Any tips on what I might be missing here? I know how to find the radius once I've found the limit of the sequence, but can't seem to find it.","In my homework I'm supposed to find the convergence radius of the power set in the title. I'm allowed to assume $C = \lim_{n\to\infty} {c_{n+1}\over c_n}$ exists and we should try to figure out that $C$ has to be. I found $${c_{n+1}\over {c_n}} = {1\over 2} + {c_{n-1}\over{2c_n}}  $$ so for $n\to\infty$: $$C = {1\over2} + {1\over{2C}}  \Leftrightarrow C=1$$ which doesn't really help me further. Investigating $c_n - c_{n-1}$ didn't help me out either. Any tips on what I might be missing here? I know how to find the radius once I've found the limit of the sequence, but can't seem to find it.",,"['sequences-and-series', 'limits', 'convergence-divergence', 'power-series']"
99,Finding radius and interval of convergence of unique series,Finding radius and interval of convergence of unique series,,"Here is the problem: Find the radius and the interval of convergence of $$\sum_{n\geq 1}\frac{2\cdot 4\cdot 6\cdots (2n)\,x^{n}}{1\cdot 3\cdot 5\cdots (2n-1)}.$$ I thought that I could write the numerator as $n!2^n$, and the denominator as $\frac{(2n)!}{(n!2^n)}$, but I am not sure that either of these are correct.  If you have any advice on how to solve the problem, it would be much appreciated! :) Thanks!","Here is the problem: Find the radius and the interval of convergence of $$\sum_{n\geq 1}\frac{2\cdot 4\cdot 6\cdots (2n)\,x^{n}}{1\cdot 3\cdot 5\cdots (2n-1)}.$$ I thought that I could write the numerator as $n!2^n$, and the denominator as $\frac{(2n)!}{(n!2^n)}$, but I am not sure that either of these are correct.  If you have any advice on how to solve the problem, it would be much appreciated! :) Thanks!",,"['sequences-and-series', 'limits', 'factorial']"
