,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How can I solve $y'' + \left(4x-\tfrac{2}{x}\right)y' + 4x^2y= 3xe^{x^2}$?,How can I solve ?,y'' + \left(4x-\tfrac{2}{x}\right)y' + 4x^2y= 3xe^{x^2},"The DE is  $$y'' + \left(4x-\frac{2}{x}\right)y' + 4x^2y= 3xe^{x^2}.$$ I've been told to use $t = x^2$ along with change of variable to solve it, but it's clear that's not possible due to the singularity on $x = 0$ on the term $\frac{2}{x}$. I tried using  Frobenius method, but not luck with the solution. I would appreciate any help on how to solve it. Thanks in advance.","The DE is  $$y'' + \left(4x-\frac{2}{x}\right)y' + 4x^2y= 3xe^{x^2}.$$ I've been told to use $t = x^2$ along with change of variable to solve it, but it's clear that's not possible due to the singularity on $x = 0$ on the term $\frac{2}{x}$. I tried using  Frobenius method, but not luck with the solution. I would appreciate any help on how to solve it. Thanks in advance.",,"['ordinary-differential-equations', 'frobenius-method']"
1,"Deriving inequality from a Lyapunov test of $x' = -x + y^3, \space y' = -y + ax^3$",Deriving inequality from a Lyapunov test of,"x' = -x + y^3, \space y' = -y + ax^3","Exercise : Consider the dynamical system :   $$x' = -x +y^3$$   $$y' = -y+ax^3$$   with $(x,y) \in \mathbb R^2$. Find the stationary point of the system and study their stability for every value of $a$. Show that if $a=1$ and $V(x,y) \leq R$, where $R>0$ is a constant and $V(x,y) = \frac{1}{2}x^2 + \frac{1}{2}y^2$, then the derivative $V'(x,y)$ along the solutions of the dynamical system, satisfies the inequality $V'(x,y) \leq -2(1-R)V(x,y)$ and finally, estimate a stability area of $(0,0)$ for the case of $a=1$ using the Lyapunov Functional given. Attempt / Question : For the first part, I've elaborated a complete solution, as one can easily find the stationary points from solving the system : $$\begin{cases} -x+y^3 \space\space=0\\ -y + ax^3=0\end{cases}$$ which yields $3$ different stationary points $(x,y)$. For the stability and the kind of the stationary points, it's enough to find the eigenvalues for each stationary point for the linearisation matrix (jacobian), which is : $$J(x,y) = \begin{bmatrix} -1 & 3y^2 \\ 3ax^2 & -1 \end{bmatrix}$$ For the part of the question in which I have an issue now, regarding the next segment of the problem : The functional derivative, is given as : $$\dot{V}(x,y) = \nabla V(x)f(x,y) = V_xf_x + V_yf_y = 2x(-x+y^3)+2y(-y+x^3) $$ $$\Leftrightarrow$$ $$\dot{V}(x,y) = -2x^2 + 2xy^3 - 2y^2 + 2yx^3$$ $$\Leftrightarrow$$ $$\dot{V}(x,y) = -2x^2 - 2y^2 + 2xy(y^2 + x^2)=-4V(x,y) + 4xyV(x,y)= 4V(x,y)(xy-1)$$ From this point on though, how would one proceed to show the inequality asked ? $$\dot{V}(x,y) \leq -2(1-R)V(x,y)$$ For the final part, I guess you just want to set a domain for the constant $R$ such that the functional is negative, but correct me if I'm wrong.","Exercise : Consider the dynamical system :   $$x' = -x +y^3$$   $$y' = -y+ax^3$$   with $(x,y) \in \mathbb R^2$. Find the stationary point of the system and study their stability for every value of $a$. Show that if $a=1$ and $V(x,y) \leq R$, where $R>0$ is a constant and $V(x,y) = \frac{1}{2}x^2 + \frac{1}{2}y^2$, then the derivative $V'(x,y)$ along the solutions of the dynamical system, satisfies the inequality $V'(x,y) \leq -2(1-R)V(x,y)$ and finally, estimate a stability area of $(0,0)$ for the case of $a=1$ using the Lyapunov Functional given. Attempt / Question : For the first part, I've elaborated a complete solution, as one can easily find the stationary points from solving the system : $$\begin{cases} -x+y^3 \space\space=0\\ -y + ax^3=0\end{cases}$$ which yields $3$ different stationary points $(x,y)$. For the stability and the kind of the stationary points, it's enough to find the eigenvalues for each stationary point for the linearisation matrix (jacobian), which is : $$J(x,y) = \begin{bmatrix} -1 & 3y^2 \\ 3ax^2 & -1 \end{bmatrix}$$ For the part of the question in which I have an issue now, regarding the next segment of the problem : The functional derivative, is given as : $$\dot{V}(x,y) = \nabla V(x)f(x,y) = V_xf_x + V_yf_y = 2x(-x+y^3)+2y(-y+x^3) $$ $$\Leftrightarrow$$ $$\dot{V}(x,y) = -2x^2 + 2xy^3 - 2y^2 + 2yx^3$$ $$\Leftrightarrow$$ $$\dot{V}(x,y) = -2x^2 - 2y^2 + 2xy(y^2 + x^2)=-4V(x,y) + 4xyV(x,y)= 4V(x,y)(xy-1)$$ From this point on though, how would one proceed to show the inequality asked ? $$\dot{V}(x,y) \leq -2(1-R)V(x,y)$$ For the final part, I guess you just want to set a domain for the constant $R$ such that the functional is negative, but correct me if I'm wrong.",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory', 'lyapunov-functions']"
2,Proving a differentiable function exists.,Proving a differentiable function exists.,,"Let $A=\{(x,y)\in\mathbb{R^2}:x>0\}$. Let $f:A\longrightarrow{\mathbb{R}}$ differentiable satisfying: $$x\frac{\partial f}{\partial x}+y\frac{\partial f}{\partial y}=0$$ How can I prove the function $h:\mathbb{R}\rightarrow\mathbb{R}$ differentiable so that $f(x,y)=h(\frac{y}{x})$ exists?","Let $A=\{(x,y)\in\mathbb{R^2}:x>0\}$. Let $f:A\longrightarrow{\mathbb{R}}$ differentiable satisfying: $$x\frac{\partial f}{\partial x}+y\frac{\partial f}{\partial y}=0$$ How can I prove the function $h:\mathbb{R}\rightarrow\mathbb{R}$ differentiable so that $f(x,y)=h(\frac{y}{x})$ exists?",,"['ordinary-differential-equations', 'multivariable-calculus']"
3,How to find the asymptotic behaviour of $(y'')^2=y'+y$ as $x$ tends to $\infty$?,How to find the asymptotic behaviour of  as  tends to ?,(y'')^2=y'+y x \infty,"Can someone help me find the asymptotic behavior of $(y'')^2=y'+y$ as $x$ tends to $\infty$? I tried neglecting the $y'$ term in the equation and ended up with $y\sim x^4/144$. The derivative of $x^4/144$ is indeed neglegible as $x$ goes to  $\infty$ . But when I tried to find the full asymptotic behaviour and plugeg the series $x^4/144+\sum_{n=-3}^\infty \frac{1}{x^n}$, into the equation and equate powers, I found there is no free parameter at all. I also tried different substitution such as $\log$, but none of them seems to be working out. Substituiting exponential clearly fail because not all the terms are multiplied by the same number of $y$ or its deriatives.","Can someone help me find the asymptotic behavior of $(y'')^2=y'+y$ as $x$ tends to $\infty$? I tried neglecting the $y'$ term in the equation and ended up with $y\sim x^4/144$. The derivative of $x^4/144$ is indeed neglegible as $x$ goes to  $\infty$ . But when I tried to find the full asymptotic behaviour and plugeg the series $x^4/144+\sum_{n=-3}^\infty \frac{1}{x^n}$, into the equation and equate powers, I found there is no free parameter at all. I also tried different substitution such as $\log$, but none of them seems to be working out. Substituiting exponential clearly fail because not all the terms are multiplied by the same number of $y$ or its deriatives.",,"['ordinary-differential-equations', 'asymptotics']"
4,System of equations and equilibrium points?,System of equations and equilibrium points?,,"Consider the system of DE $$\frac{dx}{dt}=-axy+b$$ $$\frac{dy}{dt}=axy-cy$$ Where a,b,c are positive constants. Show that the system has a unique equilibrium point. Show that any solution in the system that starts close enough to it's equilibrium point tends finally to the equilibrium point when $t$ tends to infinity. I did 1. The equilibrium point is $\overline x=(\frac{c}{a},\frac{b}{c})$ , and to prove uniqueness I assumed there was another equilibrium point $x^* $ and after calculation I got $x^*=\overline x.$ And I'm not quite sure what should I do to solve 2. Can someone help me please?","Consider the system of DE Where a,b,c are positive constants. Show that the system has a unique equilibrium point. Show that any solution in the system that starts close enough to it's equilibrium point tends finally to the equilibrium point when tends to infinity. I did 1. The equilibrium point is , and to prove uniqueness I assumed there was another equilibrium point and after calculation I got And I'm not quite sure what should I do to solve 2. Can someone help me please?","\frac{dx}{dt}=-axy+b \frac{dy}{dt}=axy-cy t \overline x=(\frac{c}{a},\frac{b}{c}) x^*  x^*=\overline x.","['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
5,Perturbing a dynamical system,Perturbing a dynamical system,,"Let $$x'=1+y-x^2-y^2+af_1(x,y)$$ $$y'=1-x-x^2-y^2+af_2(x,y)$$ where $a>0$ is very small. What is a perturbation $af(x,y)$ that preserves periodic orbits but makes it asymptotically stable? What I've done: So we perturb the system $$x'=1+y-x^2-y^2$$ $$y'=1-x-x^2-y^2$$ A periodic orbit of this is given by $(x,y)=(\cos t,-\sin t)$ (found this earlier), so we must have $$-\sin t=-\sin t+af_1(\cos t,-\sin t)$$ $$-\cos t=-\cos t+af_2(\cos t,-\sin t)$$ So we want $$f_1(\cos t,-\sin t)=f_2(\cos t,-\sin t)=0$$ However, how do I find such $f_1,f_2$ s.t. we obtain asymptotic stability? I am supposed to determine asymptotic stability by finding the Floquet exponents. Edit: Following LutzL's answer below I want to compute the Floquet multipliers. For that we have to compute the Jacobian of $F_1+\lambda(F_2-F_1)$, then plugging in our periodic solution $(\cos t,-\sin t)$ in the Jacobian $J$, we find the multiplier to be $$m=\exp\left(\int_0^{2\pi}\text{tr }J dt\right)$$ I found $$J_{11}=\frac{\partial x'}{\partial x}=-2x+\lambda(1-x^2-y^2)-2\lambda x(x-1)$$ $$J_{22}=\frac{\partial y'}{\partial y}=-2y+\lambda(1-x^2-y^2)-2\lambda y(y-1)$$ But plugging in our periodic solution $(\cos t,-\sin t)$ and integrating from $0$ to $2\pi$ gives multiplier 1, wherea I need $<1$ for asymptotic stability.","Let $$x'=1+y-x^2-y^2+af_1(x,y)$$ $$y'=1-x-x^2-y^2+af_2(x,y)$$ where $a>0$ is very small. What is a perturbation $af(x,y)$ that preserves periodic orbits but makes it asymptotically stable? What I've done: So we perturb the system $$x'=1+y-x^2-y^2$$ $$y'=1-x-x^2-y^2$$ A periodic orbit of this is given by $(x,y)=(\cos t,-\sin t)$ (found this earlier), so we must have $$-\sin t=-\sin t+af_1(\cos t,-\sin t)$$ $$-\cos t=-\cos t+af_2(\cos t,-\sin t)$$ So we want $$f_1(\cos t,-\sin t)=f_2(\cos t,-\sin t)=0$$ However, how do I find such $f_1,f_2$ s.t. we obtain asymptotic stability? I am supposed to determine asymptotic stability by finding the Floquet exponents. Edit: Following LutzL's answer below I want to compute the Floquet multipliers. For that we have to compute the Jacobian of $F_1+\lambda(F_2-F_1)$, then plugging in our periodic solution $(\cos t,-\sin t)$ in the Jacobian $J$, we find the multiplier to be $$m=\exp\left(\int_0^{2\pi}\text{tr }J dt\right)$$ I found $$J_{11}=\frac{\partial x'}{\partial x}=-2x+\lambda(1-x^2-y^2)-2\lambda x(x-1)$$ $$J_{22}=\frac{\partial y'}{\partial y}=-2y+\lambda(1-x^2-y^2)-2\lambda y(y-1)$$ But plugging in our periodic solution $(\cos t,-\sin t)$ and integrating from $0$ to $2\pi$ gives multiplier 1, wherea I need $<1$ for asymptotic stability.",,"['ordinary-differential-equations', 'dynamical-systems', 'perturbation-theory', 'stability-in-odes', 'stability-theory']"
6,What numerical techniques are used to find eigenfunctions and eigenvalues of a differential operator?,What numerical techniques are used to find eigenfunctions and eigenvalues of a differential operator?,,"Of course numerical techniques won't give an expression for eigenfunction, but I am looking for a vector of values of the eigenfunction in a given range. Also, do Scilab or Octave have any built-in functions for finding them? For instance, I was trying to solve the Schrodinger equation for the hydrogen atom and I ended up with the following eigenvalue equation, $$-\bigg(\frac12\frac{d^2}{dr^2}+\frac1{r}\bigg)\psi=E\psi$$ with the initial condition that $\psi(0.0001)=0.0001$ and $\psi'(0.0001)=1$ I am starting a bit away from origin because, I have $r$ in the denominator. I just want to find the eigenvalues and plot it's eigenfunctions.","Of course numerical techniques won't give an expression for eigenfunction, but I am looking for a vector of values of the eigenfunction in a given range. Also, do Scilab or Octave have any built-in functions for finding them? For instance, I was trying to solve the Schrodinger equation for the hydrogen atom and I ended up with the following eigenvalue equation, $$-\bigg(\frac12\frac{d^2}{dr^2}+\frac1{r}\bigg)\psi=E\psi$$ with the initial condition that $\psi(0.0001)=0.0001$ and $\psi'(0.0001)=1$ I am starting a bit away from origin because, I have $r$ in the denominator. I just want to find the eigenvalues and plot it's eigenfunctions.",,"['ordinary-differential-equations', 'numerical-methods', 'eigenvalues-eigenvectors', 'eigenfunctions']"
7,"""Existence and Uniqueness"" Theorems for first order IVP: two or just one?","""Existence and Uniqueness"" Theorems for first order IVP: two or just one?",,"Let's say that I've got the following IVP: $\frac{dy}{dx} = f(x,y)$ $y(x_0) = y_0$ And I want conditions that guarantee existence and uniqueness of its solution. On the one hand I've got the Picard–Lindelöf theorem. It asks that there exists a rectangle $R = [a,b] \times [c,d]$, containing $(x_0, y_0)$ as an interior point, where $f$ is continuous in $x$ and Lipschitz continuous in $y$. On the other hand I've got a theorem, which I've encountered in many undergraduate text books, that requires $f$ and $\frac{\partial f}{\partial y}$ to be continuous in the aforementioned rectangle. Are these two different theorems? It seems to me that the hypotheses of the first one are implied by those of the second one. But in that case, why would some authors prefer this more restrictive form of the theorem? Could it be just so that students don't need to learn the concept of Lipschitz continuity?","Let's say that I've got the following IVP: $\frac{dy}{dx} = f(x,y)$ $y(x_0) = y_0$ And I want conditions that guarantee existence and uniqueness of its solution. On the one hand I've got the Picard–Lindelöf theorem. It asks that there exists a rectangle $R = [a,b] \times [c,d]$, containing $(x_0, y_0)$ as an interior point, where $f$ is continuous in $x$ and Lipschitz continuous in $y$. On the other hand I've got a theorem, which I've encountered in many undergraduate text books, that requires $f$ and $\frac{\partial f}{\partial y}$ to be continuous in the aforementioned rectangle. Are these two different theorems? It seems to me that the hypotheses of the first one are implied by those of the second one. But in that case, why would some authors prefer this more restrictive form of the theorem? Could it be just so that students don't need to learn the concept of Lipschitz continuity?",,['ordinary-differential-equations']
8,Integrating a function of two variables in exact ODE,Integrating a function of two variables in exact ODE,,"In exact ODE of the form  $$M(x,y)\,dx+N(x,y)\,dy=0$$  To get the solution : we may integrate $M$ with respect to $x$ or integrate $N$ with respect to $y.$ I want to know why when integrating for example $M$ w.r.t. $x,$ we consider $y$ as a constant that can be taken outside the integration sign .. is this a general rule? when integrating wrt a specific variable we have to consider all other variables as constants? or this happens here because we are integrating partial derivatives of a function? But I think that the solution to the exact ODE is a relation between x and y . This means that y depends on x or  that x depends on y.. So when integrating wrt x , how can we consider y independent on x ?","In exact ODE of the form  $$M(x,y)\,dx+N(x,y)\,dy=0$$  To get the solution : we may integrate $M$ with respect to $x$ or integrate $N$ with respect to $y.$ I want to know why when integrating for example $M$ w.r.t. $x,$ we consider $y$ as a constant that can be taken outside the integration sign .. is this a general rule? when integrating wrt a specific variable we have to consider all other variables as constants? or this happens here because we are integrating partial derivatives of a function? But I think that the solution to the exact ODE is a relation between x and y . This means that y depends on x or  that x depends on y.. So when integrating wrt x , how can we consider y independent on x ?",,"['integration', 'ordinary-differential-equations']"
9,Is skateboard vibration model correct?,Is skateboard vibration model correct?,,"My friend and I rode a skateboard yesterday. After a certain speed the board began to wobble and I fell down. In order to avoid a similar outcome in the future, I want to model the ""vibration"" that occurred mathematically so that I can analyze the effect of different wheels, suspensions or even foot placement. I have created the following model, which is a variation of the harmonic oscillator: Model of vertical wheel displacement: Variable declarations: $w_n(t):=$Position of the nth wheel $m_n(t):=$Weight of the nth wheel in kg $F_n(t):=$Surface irregularity force on the nth wheel in newton $k_n,v_n\in \mathbb{R}^+:=$Coefficient of nth wheel The vertical displacement of the wheels is then modelled by: $m_n\ddot{w}_n=-k_n\cdot w_n-v_n\cdot \dot{w}+F_n$ Picture (orange part is modelled by the above equation): Model of board vibration I simulate the wobbling as rotation of the board in radian. It is dependent on the displacement of the wheels: The larger the displacement of two adjacent wheels (front view) $w_1,w_2$, the larger the wobbling of the board. Because the rider can counteract this wobbling by exerting a force himself, we have to include him in the model as well. This counterforce is not random but depends on the previous vibration of the board (vibration) at an earlier time $t-\alpha$, so we add the function $\lambda(b(t-\alpha)$. Since the rotation is centered around a pivot which is also dampened, we add the dampening term $v_b\cdot \dot{b}$. To simplify calculations, it is possible to set $\lambda(b(t-\alpha)$=$b(t-\alpha)$. Additional Variable declarations: $\lambda(b(t-\alpha):=$ Counterforce of the rider in newtons. $b(t):=$ Position of the board in radian. $\mu,\alpha\in \mathbb{R}^+:=$Coefficients The rotation of the board is then modelled by: $m_b\ddot{b}(t)=\lambda(b(t-\alpha)\cdot sin(b)+\mu (w_1-w_2)-v_b\cdot \dot{b}$ I have drawn a picture again, so you can see what I mean: Question: Do you think this is a valid model to model high speed ""board wobble""? Perhaps, you can come up with a better approach to solve the problem? I can tape my phone under the board (so I have an accelerometer, a gyroscope, a magnetometer, a gravity sensor, a linear acceleration sensor, a rotation sensor and a pressure sensor at my disposal). Thank you for your help.","My friend and I rode a skateboard yesterday. After a certain speed the board began to wobble and I fell down. In order to avoid a similar outcome in the future, I want to model the ""vibration"" that occurred mathematically so that I can analyze the effect of different wheels, suspensions or even foot placement. I have created the following model, which is a variation of the harmonic oscillator: Model of vertical wheel displacement: Variable declarations: $w_n(t):=$Position of the nth wheel $m_n(t):=$Weight of the nth wheel in kg $F_n(t):=$Surface irregularity force on the nth wheel in newton $k_n,v_n\in \mathbb{R}^+:=$Coefficient of nth wheel The vertical displacement of the wheels is then modelled by: $m_n\ddot{w}_n=-k_n\cdot w_n-v_n\cdot \dot{w}+F_n$ Picture (orange part is modelled by the above equation): Model of board vibration I simulate the wobbling as rotation of the board in radian. It is dependent on the displacement of the wheels: The larger the displacement of two adjacent wheels (front view) $w_1,w_2$, the larger the wobbling of the board. Because the rider can counteract this wobbling by exerting a force himself, we have to include him in the model as well. This counterforce is not random but depends on the previous vibration of the board (vibration) at an earlier time $t-\alpha$, so we add the function $\lambda(b(t-\alpha)$. Since the rotation is centered around a pivot which is also dampened, we add the dampening term $v_b\cdot \dot{b}$. To simplify calculations, it is possible to set $\lambda(b(t-\alpha)$=$b(t-\alpha)$. Additional Variable declarations: $\lambda(b(t-\alpha):=$ Counterforce of the rider in newtons. $b(t):=$ Position of the board in radian. $\mu,\alpha\in \mathbb{R}^+:=$Coefficients The rotation of the board is then modelled by: $m_b\ddot{b}(t)=\lambda(b(t-\alpha)\cdot sin(b)+\mu (w_1-w_2)-v_b\cdot \dot{b}$ I have drawn a picture again, so you can see what I mean: Question: Do you think this is a valid model to model high speed ""board wobble""? Perhaps, you can come up with a better approach to solve the problem? I can tape my phone under the board (so I have an accelerometer, a gyroscope, a magnetometer, a gravity sensor, a linear acceleration sensor, a rotation sensor and a pressure sensor at my disposal). Thank you for your help.",,['ordinary-differential-equations']
10,Solving a first order nonlinear nonhomogeneous ODE,Solving a first order nonlinear nonhomogeneous ODE,,"Edit: from the answers, I have learnt that the differential equation can be solved by expressing it as being a hypergeometric differential equation. My question now is that, how many a function in the form of $x(1-x)^2y''(x)+(1-x)^2y'(x)+ay(x)=0$ be transformed into the form of $\eta(1-\eta)f''(\eta)+(b-c\eta)f'(\eta)+df(\eta)=0$? Original question: How may one solve a differential equation in the form of: $\frac{dy}{dx}=P(x) -ky^2$ I have attempted at reducing it into a second order homogeneous equation in the form of $\frac{d^2u}{dx^2}=-kP(t)u$ by making the substitution of $ky= \frac{\frac{du}{dx}}{u}$ However, I am still unable to solve this. Are there any methods for solving either equation? If it helps, P(x) is the derivative of: $f(x)=\frac{a-be^{cx+d}}{1-e^{cx+d}}$ where a,b,c,d are constants Additionally, y=0 when x=0, and y=0 as x$\rightarrow$ infinity A numerical approach to solving the equation with randomly chosen values for constants substituted in gives the following graph: Link which looks like (maybe) a chi-square distribution....?","Edit: from the answers, I have learnt that the differential equation can be solved by expressing it as being a hypergeometric differential equation. My question now is that, how many a function in the form of $x(1-x)^2y''(x)+(1-x)^2y'(x)+ay(x)=0$ be transformed into the form of $\eta(1-\eta)f''(\eta)+(b-c\eta)f'(\eta)+df(\eta)=0$? Original question: How may one solve a differential equation in the form of: $\frac{dy}{dx}=P(x) -ky^2$ I have attempted at reducing it into a second order homogeneous equation in the form of $\frac{d^2u}{dx^2}=-kP(t)u$ by making the substitution of $ky= \frac{\frac{du}{dx}}{u}$ However, I am still unable to solve this. Are there any methods for solving either equation? If it helps, P(x) is the derivative of: $f(x)=\frac{a-be^{cx+d}}{1-e^{cx+d}}$ where a,b,c,d are constants Additionally, y=0 when x=0, and y=0 as x$\rightarrow$ infinity A numerical approach to solving the equation with randomly chosen values for constants substituted in gives the following graph: Link which looks like (maybe) a chi-square distribution....?",,['ordinary-differential-equations']
11,Computing how long it takes for something to decrease with differential equations of calculus,Computing how long it takes for something to decrease with differential equations of calculus,,"​When a condenser discharges electricity, the instantaneous rate of change of the voltage is proportional to the voltage in the condenser. Suppose you have a discharging condenser and the instantaneous rate of change of the voltage is 1/100 of the voltage (in volts per second). How many seconds does it take for the voltage to decrease by 90%? What I have gotten so far: v'[x] = instantaneous rate of change of the voltage v[x] = voltage in the condenser v'[x] = r v[x] v[x] = k e^(rx) I don't fully understand why v[x] = k e^(rx). I just know it is true. How do you solve this question?","​When a condenser discharges electricity, the instantaneous rate of change of the voltage is proportional to the voltage in the condenser. Suppose you have a discharging condenser and the instantaneous rate of change of the voltage is 1/100 of the voltage (in volts per second). How many seconds does it take for the voltage to decrease by 90%? What I have gotten so far: v'[x] = instantaneous rate of change of the voltage v[x] = voltage in the condenser v'[x] = r v[x] v[x] = k e^(rx) I don't fully understand why v[x] = k e^(rx). I just know it is true. How do you solve this question?",,"['calculus', 'ordinary-differential-equations']"
12,Difference between critically damped systems and overdamped systems,Difference between critically damped systems and overdamped systems,,"Consider a spring system whose equation is given by $$my''+\mu y'+ky=0$$ and let $D=\mu^2-4mk$. Now there are three cases and I am considering the cases that $D=0$ and $D>0$: When $D=0$, the solution is of the form $y=(a+bt)e^{rt}$. (Critically damped) When $D>0$, the solution is of the form $y=c_1e^{r_1t}+c_2e^{r_2t}$. (Overdamped) While I understand that these two cases are very different and the function $y=(a+bt)e^{rt}$ is very different from the function $y=c_1e^{r_1t}+c_2e^{r_2t}$, it also seems to me that the two functions have very similar graphs (in particular, very similar end behaviours). Question Why are the graphs of the solution in these two cases so similar (while in the other case $D<0$, the graph is very different)? How to tell from the graph whether we have a critically damped system or an overdamped system?","Consider a spring system whose equation is given by $$my''+\mu y'+ky=0$$ and let $D=\mu^2-4mk$. Now there are three cases and I am considering the cases that $D=0$ and $D>0$: When $D=0$, the solution is of the form $y=(a+bt)e^{rt}$. (Critically damped) When $D>0$, the solution is of the form $y=c_1e^{r_1t}+c_2e^{r_2t}$. (Overdamped) While I understand that these two cases are very different and the function $y=(a+bt)e^{rt}$ is very different from the function $y=c_1e^{r_1t}+c_2e^{r_2t}$, it also seems to me that the two functions have very similar graphs (in particular, very similar end behaviours). Question Why are the graphs of the solution in these two cases so similar (while in the other case $D<0$, the graph is very different)? How to tell from the graph whether we have a critically damped system or an overdamped system?",,"['ordinary-differential-equations', 'graphing-functions']"
13,Exact period of simple pendulum.,Exact period of simple pendulum.,,"Edit : Here is in depth derivation. Suppose the pendulum is composed of a string of length $L$ and has a point mass of mass $m$ at the end of the string. Say we incline it at an angle $\theta_0 \in (0,\pi)$ counterclockwise from horizontal (counterclockwise counted positive and clockwise counted negative).  Let the mass at the vertical position posses $0$ potential energy.  Then it posses $mg(L-L\cos \theta_0)$ amount of Potential Energy at the signed angle of $\theta_0$ . At any angle the mass posses a Kinetic energy of $\frac{1}{2}mv^2=\frac{1}{2}m \left(L\frac{d\theta}{dt}\right)^2$ and a potential energy of $mg(L-L\cos \theta)$ . By conservation of mechanical energy, $$\frac{1}{2}m\left(L\frac{d\theta}{dt}\right)^2+mgL(1-\cos \theta)=mgL(1-\cos \theta_0)$$ As the pendulum counterclockwise from an angle of $-\theta_0$ to $\theta_0$ , $\frac{d\theta}{dt} \geq 0$ so, $$\frac{d\theta}{dt}=\sqrt{\frac{2g}{L}(\cos \theta-\cos \theta_0)}$$ This motion is half the cycle (to show this look at the equation counterclockwise motion from $\theta_0$ to $-\theta_0$ ), so it takes half the period to occur. From which we find, $$T=2\sqrt{\frac{L}{2g}} \int_{-\theta_0}^{\theta_0} \frac{1}{\sqrt{\cos \theta-\cos \theta_0}} d\theta$$ As the integrand is even we get, $$=4\sqrt{\frac{L}{2g}}\int_{0}^{\theta_0} \frac{1}{\sqrt{\cos \theta-\cos \theta_0}} d\theta$$ Now we make the substitution $\sin x=\dfrac{\sin \frac{\theta}{2}}{\sin \frac{\theta_0}{2}}$ . $x \in [0,\frac{\pi}{2}]$ and $\theta \in [0,\theta_0]$ correspond together, so let $x \in \left[0,\frac{\pi}{2}\right]$ . Then note the identities, $$1-2\sin^2 \left(\frac{\theta}{2}\right)=\cos \theta$$ $$1-2\sin^2 \left(\frac{\theta_0}{2} \right)=\cos \theta_0$$ Give, $$\sqrt{\cos \theta-\cos \theta_0}=\sqrt{2} \sin \frac{\theta_0}{2} \cos x$$ (If we let $\theta_0 \in (0,\pi]$ As $\cos x$ is nonnegative for $x \in \left[0,\frac{\pi}{2} \right]$ ). Also note the identity, $$\cos \frac {\theta}{2}=\sqrt{1-\sin^2 \frac{\theta}{2}}$$ For $0 \leq \theta \leq \theta_0 \leq \pi$ . The identities we found together convert the earlier expression we found for the period into, $$T=4 \sqrt{\frac{L}{2g}} \sqrt{2} \int_{0}^{\frac{\pi}{2}} \frac{1}{\sqrt{1-k^2 \sin^2 x}} dx$$ $$=4\sqrt{\frac{L}{g}} \int_{0}^{\frac{\pi}{2}} \frac{1}{\sqrt{1-k^2 \sin^2 x}} dx$$ Where $k=\sin (\frac{\theta_0}{2})$ . We also have the binomial series expansion, $$(1-k^2\sin^2 x)^{-\frac{1}{2}}=\sum_{n=0}^{ \infty} {-\frac{1}{2} \choose n} (-1)^n k^{2n} \sin^{2n} x$$ A standard exercise in many books is to show for integers $n \geq 2$ , $$\int_{0}^{\frac{\pi}{2}} \sin^{n} x dx=\frac{n-1}{n} \int_{0}^{\frac{\pi}{2}} \sin^{n-2} x dx$$ Hence showing for $n \geq 1$ , $$\int_{0}^{\frac{\pi}{2}} \sin^{2n} x dx=\frac{1 \cdot 3 \cdot 5 \cdots (2n-1)}{2 \cdot 4 \cdot 6 \cdots 2n} \frac{\pi}{2}$$ Using this gives, $$T=2\pi \sqrt{\frac{L}{g}}\left(1+ \sum_{n=1}^{\infty} \frac{1 \cdot 3 \cdot 5 \cdots (2n-1)}{2 \cdot 4 \cdot 6 \cdots 2n} {-\frac{1}{2} \choose n} (-1)^n k^{2n} \right)$$ Also a famous result for $n \geq 1$ is, $$(-1)^n {-\frac{1}{2} \choose n}=\frac{1 \cdot 3 \cdot 5 \cdots (2n-1)}{2 \cdot 4 \cdot 6 \cdots 2n}$$ So the exact period is, $$T=2\pi \sqrt{\frac{L}{g}}\left(1+ \sum_{n=1}^{\infty}\left( \frac{1 \cdot 3 \cdot 5 \cdots (2n-1)}{2 \cdot 4 \cdot 6 \cdots 2n} \right)^2 k^{2n} \right)$$ As claimed. The equation that models a simple pendulum is, $$-g\sin \theta=L \theta''$$ Where the derivative above is a time derivative. I read in my book that the period of of the pendulum starting from an angle of $\theta(0)=\theta_0$ is exactly, $$T=2\pi\sqrt{\frac{L}{g}}\left[1+\left(\frac{1}{2}\right)^2 \sin^2 \left(\frac{\theta_0}{2}\right)+\left(\frac{1 \cdot 3}{2 \cdot 4} \right)^2 \sin^4 \left(\frac{\theta_0}{2}\right)+\cdots \right]$$ My question is how to get it? Here's something I tried use $\sin (\theta)=\theta-\frac{\theta^3}{3}+\cdots$ to come up with a solution though I see if I include anything other than one other term I am lost. With one term I can get the first term in the period. Here's another thing I tried to do, take Laplace transforms on both sides to get: $$-g \int_{0}^{\infty} e^{-st} \sin (\theta(t))dt=L(s^2F(s)-s\theta(0)-\theta'(0))$$ But again it seems like there is no hope to solve that integral.","Edit : Here is in depth derivation. Suppose the pendulum is composed of a string of length and has a point mass of mass at the end of the string. Say we incline it at an angle counterclockwise from horizontal (counterclockwise counted positive and clockwise counted negative).  Let the mass at the vertical position posses potential energy.  Then it posses amount of Potential Energy at the signed angle of . At any angle the mass posses a Kinetic energy of and a potential energy of . By conservation of mechanical energy, As the pendulum counterclockwise from an angle of to , so, This motion is half the cycle (to show this look at the equation counterclockwise motion from to ), so it takes half the period to occur. From which we find, As the integrand is even we get, Now we make the substitution . and correspond together, so let . Then note the identities, Give, (If we let As is nonnegative for ). Also note the identity, For . The identities we found together convert the earlier expression we found for the period into, Where . We also have the binomial series expansion, A standard exercise in many books is to show for integers , Hence showing for , Using this gives, Also a famous result for is, So the exact period is, As claimed. The equation that models a simple pendulum is, Where the derivative above is a time derivative. I read in my book that the period of of the pendulum starting from an angle of is exactly, My question is how to get it? Here's something I tried use to come up with a solution though I see if I include anything other than one other term I am lost. With one term I can get the first term in the period. Here's another thing I tried to do, take Laplace transforms on both sides to get: But again it seems like there is no hope to solve that integral.","L m \theta_0 \in (0,\pi) 0 mg(L-L\cos \theta_0) \theta_0 \frac{1}{2}mv^2=\frac{1}{2}m \left(L\frac{d\theta}{dt}\right)^2 mg(L-L\cos \theta) \frac{1}{2}m\left(L\frac{d\theta}{dt}\right)^2+mgL(1-\cos \theta)=mgL(1-\cos \theta_0) -\theta_0 \theta_0 \frac{d\theta}{dt} \geq 0 \frac{d\theta}{dt}=\sqrt{\frac{2g}{L}(\cos \theta-\cos \theta_0)} \theta_0 -\theta_0 T=2\sqrt{\frac{L}{2g}} \int_{-\theta_0}^{\theta_0} \frac{1}{\sqrt{\cos \theta-\cos \theta_0}} d\theta =4\sqrt{\frac{L}{2g}}\int_{0}^{\theta_0} \frac{1}{\sqrt{\cos \theta-\cos \theta_0}} d\theta \sin x=\dfrac{\sin \frac{\theta}{2}}{\sin \frac{\theta_0}{2}} x \in [0,\frac{\pi}{2}] \theta \in [0,\theta_0] x \in \left[0,\frac{\pi}{2}\right] 1-2\sin^2 \left(\frac{\theta}{2}\right)=\cos \theta 1-2\sin^2 \left(\frac{\theta_0}{2} \right)=\cos \theta_0 \sqrt{\cos \theta-\cos \theta_0}=\sqrt{2} \sin \frac{\theta_0}{2} \cos x \theta_0 \in (0,\pi] \cos x x \in \left[0,\frac{\pi}{2} \right] \cos \frac {\theta}{2}=\sqrt{1-\sin^2 \frac{\theta}{2}} 0 \leq \theta \leq \theta_0 \leq \pi T=4 \sqrt{\frac{L}{2g}} \sqrt{2} \int_{0}^{\frac{\pi}{2}} \frac{1}{\sqrt{1-k^2 \sin^2 x}} dx =4\sqrt{\frac{L}{g}} \int_{0}^{\frac{\pi}{2}} \frac{1}{\sqrt{1-k^2 \sin^2 x}} dx k=\sin (\frac{\theta_0}{2}) (1-k^2\sin^2 x)^{-\frac{1}{2}}=\sum_{n=0}^{ \infty} {-\frac{1}{2} \choose n} (-1)^n k^{2n} \sin^{2n} x n \geq 2 \int_{0}^{\frac{\pi}{2}} \sin^{n} x dx=\frac{n-1}{n} \int_{0}^{\frac{\pi}{2}} \sin^{n-2} x dx n \geq 1 \int_{0}^{\frac{\pi}{2}} \sin^{2n} x dx=\frac{1 \cdot 3 \cdot 5 \cdots (2n-1)}{2 \cdot 4 \cdot 6 \cdots 2n} \frac{\pi}{2} T=2\pi \sqrt{\frac{L}{g}}\left(1+ \sum_{n=1}^{\infty} \frac{1 \cdot 3 \cdot 5 \cdots (2n-1)}{2 \cdot 4 \cdot 6 \cdots 2n} {-\frac{1}{2} \choose n} (-1)^n k^{2n} \right) n \geq 1 (-1)^n {-\frac{1}{2} \choose n}=\frac{1 \cdot 3 \cdot 5 \cdots (2n-1)}{2 \cdot 4 \cdot 6 \cdots 2n} T=2\pi \sqrt{\frac{L}{g}}\left(1+ \sum_{n=1}^{\infty}\left( \frac{1 \cdot 3 \cdot 5 \cdots (2n-1)}{2 \cdot 4 \cdot 6 \cdots 2n} \right)^2 k^{2n} \right) -g\sin \theta=L \theta'' \theta(0)=\theta_0 T=2\pi\sqrt{\frac{L}{g}}\left[1+\left(\frac{1}{2}\right)^2 \sin^2 \left(\frac{\theta_0}{2}\right)+\left(\frac{1 \cdot 3}{2 \cdot 4} \right)^2 \sin^4 \left(\frac{\theta_0}{2}\right)+\cdots \right] \sin (\theta)=\theta-\frac{\theta^3}{3}+\cdots -g \int_{0}^{\infty} e^{-st} \sin (\theta(t))dt=L(s^2F(s)-s\theta(0)-\theta'(0))","['calculus', 'ordinary-differential-equations', 'physics']"
14,Solve the differential equation $y''+4y=g(t)$ using Laplace transformation,Solve the differential equation  using Laplace transformation,y''+4y=g(t),"How can i solve the following differential equation using Laplace Transformation? $$y'' + 4y = g(t)$$  $$Y(0)=0; Y'(0)=0$$ where:  $$  g(t) = \begin{cases}  0; &  0 \leq t <5  \\  (t-5)/5; &  5 \leq t <10 \\ 1; &  t \geq 10 \\ \end{cases}, $$","How can i solve the following differential equation using Laplace Transformation? $$y'' + 4y = g(t)$$  $$Y(0)=0; Y'(0)=0$$ where:  $$  g(t) = \begin{cases}  0; &  0 \leq t <5  \\  (t-5)/5; &  5 \leq t <10 \\ 1; &  t \geq 10 \\ \end{cases}, $$",,['ordinary-differential-equations']
15,Unique solution of Cauchy problem,Unique solution of Cauchy problem,,"Let , $a,b,c,d \in \Bbb R$ such that $c^2+d^2 \not =0$. Then the Cauchy problem $au_x+bu_y=e^{x+y}$ , $x,y\in \Bbb R$ with $u(x,y)=0 $ on $cx+dy=0$ has a unique solution if (A) $ac+bd \not=0$. (B) $ad-bc \not =0$. (C) $ac-bd\not=0$ (D) $ad+bc \not=0$ Using Lagranges equations we get , $bx-ay=C_1$ and $u-\frac{a}{a+b}e^{x+y}=C_2$. Then the solution becomes $\displaystyle u(x,y)=\frac{a}{a+b}e^{x+y}+\phi(bx-ay)$. Then how I can proceed further to find out the answer.","Let , $a,b,c,d \in \Bbb R$ such that $c^2+d^2 \not =0$. Then the Cauchy problem $au_x+bu_y=e^{x+y}$ , $x,y\in \Bbb R$ with $u(x,y)=0 $ on $cx+dy=0$ has a unique solution if (A) $ac+bd \not=0$. (B) $ad-bc \not =0$. (C) $ac-bd\not=0$ (D) $ad+bc \not=0$ Using Lagranges equations we get , $bx-ay=C_1$ and $u-\frac{a}{a+b}e^{x+y}=C_2$. Then the solution becomes $\displaystyle u(x,y)=\frac{a}{a+b}e^{x+y}+\phi(bx-ay)$. Then how I can proceed further to find out the answer.",,"['ordinary-differential-equations', 'partial-differential-equations', 'cauchy-problem']"
16,How to solve this nonlinear and non-homogeneous differential equations?,How to solve this nonlinear and non-homogeneous differential equations?,,"Rephrase the question, as the first was erroneously written. Hello everyone and thank you in advance for your attention,this is the nonlinear differential equations: $$ y''+ a[\sin(y+b)]=f(x) $$ where $a,b  \in \mathbb{R}$, $y=y(x)$ and $f:\mathbb{R} \to \mathbb{R}$. I already solved the homogeneous equation (which it is a Lineard's non-linear differential equation ), but cannot apply the method of Lagrange (variation of parameters)as it is done with linear differential equations,how can i solve this non homogeneous equation? Thank you in advance for any idea or proposed solution. For completeness, and help your readers, i add that to solve the homogeneous equation $$ y''+ a[\sin(y+b)]=0 $$  i took this change of variable: $$y'=Z(y)  \implies y''=Z'Z$$ but now it does not seem that such a change of variable can me solve the non-homogeneous equation,and i got stuck, unable to find the right change of variable or the right way to solve it. It could bring to a Abel nonlinear differential equation?","Rephrase the question, as the first was erroneously written. Hello everyone and thank you in advance for your attention,this is the nonlinear differential equations: $$ y''+ a[\sin(y+b)]=f(x) $$ where $a,b  \in \mathbb{R}$, $y=y(x)$ and $f:\mathbb{R} \to \mathbb{R}$. I already solved the homogeneous equation (which it is a Lineard's non-linear differential equation ), but cannot apply the method of Lagrange (variation of parameters)as it is done with linear differential equations,how can i solve this non homogeneous equation? Thank you in advance for any idea or proposed solution. For completeness, and help your readers, i add that to solve the homogeneous equation $$ y''+ a[\sin(y+b)]=0 $$  i took this change of variable: $$y'=Z(y)  \implies y''=Z'Z$$ but now it does not seem that such a change of variable can me solve the non-homogeneous equation,and i got stuck, unable to find the right change of variable or the right way to solve it. It could bring to a Abel nonlinear differential equation?",,"['calculus', 'ordinary-differential-equations', 'power-series', 'trigonometric-integrals', 'nonlinear-analysis']"
17,Exponential map and matrices: comparison of two pictures,Exponential map and matrices: comparison of two pictures,,"Let $M$ be a Riemannian manifold with the Levi-Civita connection. For $x \in M$ and $X \in T_x(M)$ we can consider geodesic $\gamma$ with the properties that $\gamma(0)=x$ and $\gamma'(0)=X$. Such geodesic exists and is unique: existence is guaranteed provided we take vectors $X$ in some suitable neighborhood of $0$ in $T_x(M)$. If we define $y:=\gamma(1)$ we arrive at the definition of exponential map $\exp_x:T_xM \to M$ (defined only on some small neighborhood of $0$ in $T_xM$). From the other hand, we can consider $GL(n,\mathbb{R})$ as an open set in $\mathbb{R}^{n^2}$: therefore it becomes manifold on its own right. I would like to understand why in this situation $\exp_{I}(A)=e^{A}$ where on the left hand side is the exponential map from riemannian manifold at identity matrix and the right hand side is just the exponential of the matrix. Here we identify $T_I(Gl(n))$ with $\mathbb{R}^{n^2}$.","Let $M$ be a Riemannian manifold with the Levi-Civita connection. For $x \in M$ and $X \in T_x(M)$ we can consider geodesic $\gamma$ with the properties that $\gamma(0)=x$ and $\gamma'(0)=X$. Such geodesic exists and is unique: existence is guaranteed provided we take vectors $X$ in some suitable neighborhood of $0$ in $T_x(M)$. If we define $y:=\gamma(1)$ we arrive at the definition of exponential map $\exp_x:T_xM \to M$ (defined only on some small neighborhood of $0$ in $T_xM$). From the other hand, we can consider $GL(n,\mathbb{R})$ as an open set in $\mathbb{R}^{n^2}$: therefore it becomes manifold on its own right. I would like to understand why in this situation $\exp_{I}(A)=e^{A}$ where on the left hand side is the exponential map from riemannian manifold at identity matrix and the right hand side is just the exponential of the matrix. Here we identify $T_I(Gl(n))$ with $\mathbb{R}^{n^2}$.",,"['ordinary-differential-equations', 'differential-geometry', 'lie-groups', 'lie-algebras', 'exponentiation']"
18,Is it possible to use the multiple scales technique to a set of coupled ODEs?,Is it possible to use the multiple scales technique to a set of coupled ODEs?,,"I need to solve a set of coupled ODEs. The ODEs are well indicated for using the multiple scales method (generalised oscillator equations), I have just never done such a thing on a ODEs set. Is it possible? How should I adapt the steps? Please note: Since there is a good chance I haven't been lucky enough finding in literature and google, I'll be really glad for an online reference as well. EDIT: Please, provide minimal working example or a reference containing it.","I need to solve a set of coupled ODEs. The ODEs are well indicated for using the multiple scales method (generalised oscillator equations), I have just never done such a thing on a ODEs set. Is it possible? How should I adapt the steps? Please note: Since there is a good chance I haven't been lucky enough finding in literature and google, I'll be really glad for an online reference as well. EDIT: Please, provide minimal working example or a reference containing it.",,"['ordinary-differential-equations', 'reference-request', 'dynamical-systems', 'perturbation-theory']"
19,The period of the periodic equilibrium of an ODE,The period of the periodic equilibrium of an ODE,,"I have a nonlinear ODE  $$ \dot{x}(t)=a(t)-\mbox{min}(x(t),b) $$ where $b>0$ and the input $a(t)$ is positive, bounded, and periodic with period $1$. How can I show that any periodic equilibrium of the ODE must also have period 1, i.e., the same as the input? (A solution $x^*(t)$ is a periodic equilibrium if there exists $p>0$ such that $x^*(p+t)=x^*(t)$ for all $t\geq0$.) I know that given $\int_{0}^{1}a(s)ds<b$ then a periodic solution exists and numerical solutions confirm that the period is 1. But I'm not sure how to show this given that it's hard to solve the ODE even after specifiying a $a(t)$ (although a piecewise solution can be constructed by solving it assuming $x(t)>b$ or $x(t)\leq b$ and then attaching the solutions).","I have a nonlinear ODE  $$ \dot{x}(t)=a(t)-\mbox{min}(x(t),b) $$ where $b>0$ and the input $a(t)$ is positive, bounded, and periodic with period $1$. How can I show that any periodic equilibrium of the ODE must also have period 1, i.e., the same as the input? (A solution $x^*(t)$ is a periodic equilibrium if there exists $p>0$ such that $x^*(p+t)=x^*(t)$ for all $t\geq0$.) I know that given $\int_{0}^{1}a(s)ds<b$ then a periodic solution exists and numerical solutions confirm that the period is 1. But I'm not sure how to show this given that it's hard to solve the ODE even after specifiying a $a(t)$ (although a piecewise solution can be constructed by solving it assuming $x(t)>b$ or $x(t)\leq b$ and then attaching the solutions).",,"['ordinary-differential-equations', 'fourier-series', 'dynamical-systems', 'nonlinear-system', 'periodic-functions']"
20,Solving a second-order nonlinear ODE with a singularity on x=0,Solving a second-order nonlinear ODE with a singularity on x=0,,"I'm doing some reasearch on electromagnetic nanostructures and I have to solve this differential equation (the exact values of the constants don't matter, I just want all the possible solutions of y(x) given some values to these constants). $$ \frac{d^2 y}{dx^2}=-\frac{1}{x}\frac{dy}{dx}+\frac{\sin(2y)}{2} (\frac{1}{x^2}+\frac{K}{A})-\frac{D}{A}\frac{\sin(y)}{x}+\frac{\mu HM}{2A}\sin(y) $$ from x=0 till x=R, with the boundary conditions $$ y(0)=0,\ \frac{dy}{dx}(R)=\frac{-D}{2A} $$ I believe you can not find an analitic solution to this equation, so I've been trying to use numerical methods like the shooting method (given the boundary conditions, I found it appropiate). The thing is that the singularity on x=0 doesn't let me find the solutions. I obtain different results depending on how many steps I take in the method. I also posted this on Computational Science and Physics StackExchange, but for now I couldn't fix it.","I'm doing some reasearch on electromagnetic nanostructures and I have to solve this differential equation (the exact values of the constants don't matter, I just want all the possible solutions of y(x) given some values to these constants). $$ \frac{d^2 y}{dx^2}=-\frac{1}{x}\frac{dy}{dx}+\frac{\sin(2y)}{2} (\frac{1}{x^2}+\frac{K}{A})-\frac{D}{A}\frac{\sin(y)}{x}+\frac{\mu HM}{2A}\sin(y) $$ from x=0 till x=R, with the boundary conditions $$ y(0)=0,\ \frac{dy}{dx}(R)=\frac{-D}{2A} $$ I believe you can not find an analitic solution to this equation, so I've been trying to use numerical methods like the shooting method (given the boundary conditions, I found it appropiate). The thing is that the singularity on x=0 doesn't let me find the solutions. I obtain different results depending on how many steps I take in the method. I also posted this on Computational Science and Physics StackExchange, but for now I couldn't fix it.",,"['ordinary-differential-equations', 'numerical-methods']"
21,Forward Euler Pendulum divergence,Forward Euler Pendulum divergence,,"I have implemented a simple forward euler simulation of the pendulum model: $$ \ddot{\phi} = -g \sin \phi $$ The solution builds up energy (plot shows $\phi$ w.r.t. time), as it was expected. But after a while it switches from an oscillating behavior to a monotone divergence. Is that expected or a bug in my implementation? If it is expected, why does it happen (coincidentally around $\phi=\pi$)","I have implemented a simple forward euler simulation of the pendulum model: $$ \ddot{\phi} = -g \sin \phi $$ The solution builds up energy (plot shows $\phi$ w.r.t. time), as it was expected. But after a while it switches from an oscillating behavior to a monotone divergence. Is that expected or a bug in my implementation? If it is expected, why does it happen (coincidentally around $\phi=\pi$)",,"['integration', 'ordinary-differential-equations', 'numerical-methods']"
22,"What is meant by a ""discontinuity of $1$""?","What is meant by a ""discontinuity of ""?",1,"I am confused by the solution to part $(\mathrm{c})$ of the following question for which I have typed out the full question and solutions for context: The equation for a driven, damped harmonic oscillator is $$\frac{d^2y}{dt^2}+2\frac{dy}{dt}+(1+k^2)y=f(t)$$ $(\mathrm{a})$ If the initial conditions are $y=0$ and $\dfrac{dy}{dt}=0$ at $t=0$ , show that the Green's function, valid for $t\ge 0$ , is $$G(t,T)=\begin{cases}A(T)e^{-t}\cos(kt)+B(T)e^{-t}\sin(kt)\quad\text{for}\quad 0\lt t\lt T \\C(T)e^{-t}\cos(kt)+D(T)e^{-t}\sin(kt)\quad\text{for}\quad t\gt T\end{cases}$$ Part $(\mathrm{a})$ solution: For $t\lt T$ , $$\frac{\partial^2G(t,T)}{\partial t^2}+2\frac{\partial G(t,T)}{\partial t}+(1+k^2)G(t,T)=\delta(t-T)=0$$ With a trial solution $G(t,T)\propto e^{mt}$ , we have $$m^2+2m+(1+k^2)=0$$ $$\implies m=\frac{-2\pm\sqrt{4-4(1+k^2)}}{2}=-1\pm\sqrt{-k^2}=-1\pm ik$$ Hence $$\begin{align}G(t,T)&=ae^{-1+ik}+be^{-1-ik}\\&=e^{-t}\Big(a(T)e^{ikt}+b(T)e^{-ikt}\Big)\\&=e^{-t}\Big(A(T)\cos(kt)+B(T)\sin(kt)\Big)\\&=A(T)e^{-t}\cos(kt)+B(T)e^{-t}\sin(kt)\end{align}$$ and similarly for $t\gt T$ , with $A\to C$ and $B\to D$ . $(\mathrm{b})$ Show that $\mathrm{A}=\mathrm{B}=0$ and so $G(t,T)=0$ for $t\lt T$ . Part $(\mathrm{b})$ solution: $G(0,T)=0\implies A=0$ , $\dfrac{\partial G}{\partial t}\bigg|_{t=0}=B(T)\cos(0)=0\implies B(T)=0$ . Hence $G(t,T)=0$ for $t\lt T$ $(\mathrm{c})$ By matching $G(t,T)$ at $t=T$ , and requiring $\dfrac{dG}{dt}$ to have a discontinuity of $1$ there, show that, for $t\gt T$ $$G(t,T)=\frac{e^{T-t}}{k}\bigg(\cos(kT)\sin(kt)-\sin(kT)\cos(kt)\bigg)$$ Part $(\mathrm{c})$ solution: Continuity of of $G$ at $t=T$ gives $$\color{blue}{C(T)e^{-T}\cos(kT)+D(T)e^{-T}\sin(kT)=0}$$ Discontinuity of $1$ in $\dfrac{\partial G(t,T)}{\partial t}$ at $t=T$ gives $\color{red}{[-kC(T)e^{-t}\sin(kt)+kD(T)e^{-t}\cos(kt)]_{t=T}-[C(T)e^{-T}\cos(kT)+D(T)e^{-T}\sin(kT)]_{t=T}=1}$ I understand why the part marked blue is correct as it was explained to me in this previous question but I have no idea about the logic behind the red equation. Specifically I don't understand why subtracting those two terms must be equal to $1$ . I notice that the second term on the LHS of the red equation is actually the blue equation and hence equal to zero. I also know from the same previous question why a first derivative must change by $1$ . But what is meant by a ""discontinuity of $1$ "" and why does the red equation take that form? Put in another way, if I wrote ""discontinuity of $7$ "" instead; What do those three words mean? Does it mean that there is a region on the $x$ -axis consisting of $7$ units where the function is not present? Or does it mean that there is a region on the $y$ -axis where the function is not there? Any hints or tips is greatly appreciated, thank you.","I am confused by the solution to part of the following question for which I have typed out the full question and solutions for context: The equation for a driven, damped harmonic oscillator is If the initial conditions are and at , show that the Green's function, valid for , is Part solution: For , With a trial solution , we have Hence and similarly for , with and . Show that and so for . Part solution: , . Hence for By matching at , and requiring to have a discontinuity of there, show that, for Part solution: Continuity of of at gives Discontinuity of in at gives I understand why the part marked blue is correct as it was explained to me in this previous question but I have no idea about the logic behind the red equation. Specifically I don't understand why subtracting those two terms must be equal to . I notice that the second term on the LHS of the red equation is actually the blue equation and hence equal to zero. I also know from the same previous question why a first derivative must change by . But what is meant by a ""discontinuity of "" and why does the red equation take that form? Put in another way, if I wrote ""discontinuity of "" instead; What do those three words mean? Does it mean that there is a region on the -axis consisting of units where the function is not present? Or does it mean that there is a region on the -axis where the function is not there? Any hints or tips is greatly appreciated, thank you.","(\mathrm{c}) \frac{d^2y}{dt^2}+2\frac{dy}{dt}+(1+k^2)y=f(t) (\mathrm{a}) y=0 \dfrac{dy}{dt}=0 t=0 t\ge 0 G(t,T)=\begin{cases}A(T)e^{-t}\cos(kt)+B(T)e^{-t}\sin(kt)\quad\text{for}\quad 0\lt t\lt T \\C(T)e^{-t}\cos(kt)+D(T)e^{-t}\sin(kt)\quad\text{for}\quad t\gt T\end{cases} (\mathrm{a}) t\lt T \frac{\partial^2G(t,T)}{\partial t^2}+2\frac{\partial G(t,T)}{\partial t}+(1+k^2)G(t,T)=\delta(t-T)=0 G(t,T)\propto e^{mt} m^2+2m+(1+k^2)=0 \implies m=\frac{-2\pm\sqrt{4-4(1+k^2)}}{2}=-1\pm\sqrt{-k^2}=-1\pm ik \begin{align}G(t,T)&=ae^{-1+ik}+be^{-1-ik}\\&=e^{-t}\Big(a(T)e^{ikt}+b(T)e^{-ikt}\Big)\\&=e^{-t}\Big(A(T)\cos(kt)+B(T)\sin(kt)\Big)\\&=A(T)e^{-t}\cos(kt)+B(T)e^{-t}\sin(kt)\end{align} t\gt T A\to C B\to D (\mathrm{b}) \mathrm{A}=\mathrm{B}=0 G(t,T)=0 t\lt T (\mathrm{b}) G(0,T)=0\implies A=0 \dfrac{\partial G}{\partial t}\bigg|_{t=0}=B(T)\cos(0)=0\implies B(T)=0 G(t,T)=0 t\lt T (\mathrm{c}) G(t,T) t=T \dfrac{dG}{dt} 1 t\gt T G(t,T)=\frac{e^{T-t}}{k}\bigg(\cos(kT)\sin(kt)-\sin(kT)\cos(kt)\bigg) (\mathrm{c}) G t=T \color{blue}{C(T)e^{-T}\cos(kT)+D(T)e^{-T}\sin(kT)=0} 1 \dfrac{\partial G(t,T)}{\partial t} t=T \color{red}{[-kC(T)e^{-t}\sin(kt)+kD(T)e^{-t}\cos(kt)]_{t=T}-[C(T)e^{-T}\cos(kT)+D(T)e^{-T}\sin(kT)]_{t=T}=1} 1 1 1 7 x 7 y","['ordinary-differential-equations', 'derivatives', 'proof-explanation', 'greens-function', 'continuity']"
23,Lagrange’s first-order linear partial DE,Lagrange’s first-order linear partial DE,,"Show that the general solution of the partial DE $$u_{x} + (a(x)y + b(x)u)u_{y}=c(x)y + d(x)u  $$ is of the form  $$\phi \left ( \frac{yv_{1}(x)-uw_{1}(x)}{Z(x)}, \frac{yv_{2}(x)-uw_{2}(x)}{Z(x)}\right )=0$$ Where $$W(x) = c_{1}w_{1}(x)+c_{2}w_{2}(x) , V(x) = c_{1}v_{1}(x)+c_{2}v_{2}(x)$$ is the general solution  of the system of equations   $$\frac{dW}{dx}=aW+bV , \frac{dV}{dx}=cW+dV and Z=w_{1}v_{2}-w_{2}v_{1}$$ and $$Z=w_{1}v_{2}-w_{2}v_{1}.$$ Hence , solve the following partial PDEs: $$ (i) \  u_{x}+(-y+2u)u_{y}=4y+u$$ $$ (ii)\  u_{x}+\frac{2y+u}{x}u_{y}=\frac{4y+2u}{x}$$ And the answer using part one is supposed to be as follows: $$ (i) \  \phi (e^{-3x(y+u)},e^{3x}(u-2y))=0 $$ $$ (ii) \  \phi ((2y+u)/x^{4} ,2y-u)=0 $$ Thank You...","Show that the general solution of the partial DE $$u_{x} + (a(x)y + b(x)u)u_{y}=c(x)y + d(x)u  $$ is of the form  $$\phi \left ( \frac{yv_{1}(x)-uw_{1}(x)}{Z(x)}, \frac{yv_{2}(x)-uw_{2}(x)}{Z(x)}\right )=0$$ Where $$W(x) = c_{1}w_{1}(x)+c_{2}w_{2}(x) , V(x) = c_{1}v_{1}(x)+c_{2}v_{2}(x)$$ is the general solution  of the system of equations   $$\frac{dW}{dx}=aW+bV , \frac{dV}{dx}=cW+dV and Z=w_{1}v_{2}-w_{2}v_{1}$$ and $$Z=w_{1}v_{2}-w_{2}v_{1}.$$ Hence , solve the following partial PDEs: $$ (i) \  u_{x}+(-y+2u)u_{y}=4y+u$$ $$ (ii)\  u_{x}+\frac{2y+u}{x}u_{y}=\frac{4y+2u}{x}$$ And the answer using part one is supposed to be as follows: $$ (i) \  \phi (e^{-3x(y+u)},e^{3x}(u-2y))=0 $$ $$ (ii) \  \phi ((2y+u)/x^{4} ,2y-u)=0 $$ Thank You...",,"['ordinary-differential-equations', 'partial-differential-equations']"
24,On a cauchy problem and the max interval on which the solution is defined.,On a cauchy problem and the max interval on which the solution is defined.,,"The problem in question is $y' = (y^2-1) xe^x$ where the initial condition is $y(1) = 2$.  I am also asked to prove that the supremum of the max interval on which the solution is defined is less than infinity. My solution attempt : I recognize this as a differential equation where I can separate variables I know that the function $y$ which is a solution to my Cauchy problem can be found by solving $$\int_2^y \frac{1}{t^2 - 1} dt = \int_1^x s e^s ds \implies 1/2 ( -\ln|y -1| + \ln |y+1| - \ln |3|) = xe^x - e^x$$ which gives $$\ln \sqrt{\frac{|y+1|}{3|y-1|}} = xe^x - e^x \implies \frac{|y+1|}{3|y-1|} = e^{2xe^x - 2e^x}$$ at this point I think I can forgo the absolute values since I know that My solution must be defined in an interval that includes in the range of the solution the number $2$. Then I have $$y = \frac{-1 - 3e^{2xe^x - 2e^x}}{1 -3e^{2xe^x - 2e^x}}$$ which seems to imply that the denominator must be negative and obviously not zero, so I get $$- \ln(3) \le 2xe^x - 2e^x$$ this is true for $x > M$ where $M$ is a big enough number but this contradicts the second question because the interval on which $y$ is defined would be as large as wanted towards $+ \infty$. If I may I kindly ask where my mistakes are and how could they be solved.","The problem in question is $y' = (y^2-1) xe^x$ where the initial condition is $y(1) = 2$.  I am also asked to prove that the supremum of the max interval on which the solution is defined is less than infinity. My solution attempt : I recognize this as a differential equation where I can separate variables I know that the function $y$ which is a solution to my Cauchy problem can be found by solving $$\int_2^y \frac{1}{t^2 - 1} dt = \int_1^x s e^s ds \implies 1/2 ( -\ln|y -1| + \ln |y+1| - \ln |3|) = xe^x - e^x$$ which gives $$\ln \sqrt{\frac{|y+1|}{3|y-1|}} = xe^x - e^x \implies \frac{|y+1|}{3|y-1|} = e^{2xe^x - 2e^x}$$ at this point I think I can forgo the absolute values since I know that My solution must be defined in an interval that includes in the range of the solution the number $2$. Then I have $$y = \frac{-1 - 3e^{2xe^x - 2e^x}}{1 -3e^{2xe^x - 2e^x}}$$ which seems to imply that the denominator must be negative and obviously not zero, so I get $$- \ln(3) \le 2xe^x - 2e^x$$ this is true for $x > M$ where $M$ is a big enough number but this contradicts the second question because the interval on which $y$ is defined would be as large as wanted towards $+ \infty$. If I may I kindly ask where my mistakes are and how could they be solved.",,"['real-analysis', 'ordinary-differential-equations']"
25,Solution of $\frac{xdx - ydy}{xdy - ydx} = \sqrt{\frac{1 - y^{2} + x^{2}}{x^{2} - y^{2}}}$ [duplicate],Solution of  [duplicate],\frac{xdx - ydy}{xdy - ydx} = \sqrt{\frac{1 - y^{2} + x^{2}}{x^{2} - y^{2}}},"This question already has answers here : solve$\frac{xdx+ydy}{xdy-ydx}=\sqrt{\frac{a^2-x^2-y^2}{x^2+y^2}}$ (2 answers) Closed 7 years ago . Find the solution of $$\frac{xdx - ydy}{xdy - ydx} = \sqrt{\frac{1 - y^{2} + x^{2}}{x^{2} - y^{2}}}$$ I was able to bring it down to $$\frac{d(x^2-y^2)}{\sqrt{1+x^2-y^2}}=2\left(\frac{x.d(y/x)}{\sqrt{1-(y/x)^2}}\right)$$ Any help would be greatly appreciated. Thanks in advance! $$$$ Edit: An answer exists here , but I'm trying to solve the question $without$ using trigonometric or hyperbolic substitution (I was told it could be done without both, and that the solution was quite neat).  $$$$ EDIT: Also the question that is mentioned as a possible duplicate is $\textbf{different}$ from mine.","This question already has answers here : solve$\frac{xdx+ydy}{xdy-ydx}=\sqrt{\frac{a^2-x^2-y^2}{x^2+y^2}}$ (2 answers) Closed 7 years ago . Find the solution of $$\frac{xdx - ydy}{xdy - ydx} = \sqrt{\frac{1 - y^{2} + x^{2}}{x^{2} - y^{2}}}$$ I was able to bring it down to $$\frac{d(x^2-y^2)}{\sqrt{1+x^2-y^2}}=2\left(\frac{x.d(y/x)}{\sqrt{1-(y/x)^2}}\right)$$ Any help would be greatly appreciated. Thanks in advance! $$$$ Edit: An answer exists here , but I'm trying to solve the question $without$ using trigonometric or hyperbolic substitution (I was told it could be done without both, and that the solution was quite neat).  $$$$ EDIT: Also the question that is mentioned as a possible duplicate is $\textbf{different}$ from mine.",,"['calculus', 'integration', 'ordinary-differential-equations']"
26,Discontinuous solutions of a linear ode with singularities,Discontinuous solutions of a linear ode with singularities,,"I've came across the following ode in my studies $$a(t)\dot{x}(t) = A(t)x(t),$$ where $x\in \mathbb{R}^m$, $A(t),a(t)$ are analytic and $a(t) \geq 0$. I would like to understand what happens to a solution when it passes through a moment $t_0$ when $a(t_0) = 0$. In particular for other reasons it seems that there might be a weak solution with a jump discontinuity, but I am not able to verify this directly. So I was wondering, if maybe someone stumbled upon this type of ode's and could give me some references about known results for this type of equations. Or maybe some advise on how one could study it's solutions. Thank you.","I've came across the following ode in my studies $$a(t)\dot{x}(t) = A(t)x(t),$$ where $x\in \mathbb{R}^m$, $A(t),a(t)$ are analytic and $a(t) \geq 0$. I would like to understand what happens to a solution when it passes through a moment $t_0$ when $a(t_0) = 0$. In particular for other reasons it seems that there might be a weak solution with a jump discontinuity, but I am not able to verify this directly. So I was wondering, if maybe someone stumbled upon this type of ode's and could give me some references about known results for this type of equations. Or maybe some advise on how one could study it's solutions. Thank you.",,['ordinary-differential-equations']
27,How to integrate of the derivative of a variable?,How to integrate of the derivative of a variable?,,"How to calculate integrals of the type $$ \int \frac{d\phi}{dt}d\phi, $$ $$ \int \phi \, d\left(\frac{d\phi}{dt}\right) $$ and $$ \int \sin\left(\frac{d\theta}{dt}\right)\,d\theta $$ ? Thanks","How to calculate integrals of the type $$ \int \frac{d\phi}{dt}d\phi, $$ $$ \int \phi \, d\left(\frac{d\phi}{dt}\right) $$ and $$ \int \sin\left(\frac{d\theta}{dt}\right)\,d\theta $$ ? Thanks",,"['integration', 'ordinary-differential-equations', 'derivatives']"
28,Dimension of the kernel of a differentiation linear transformation,Dimension of the kernel of a differentiation linear transformation,,"The set of all polynomials in a single variable $x$ forms a vector space $P$ of infinite dimension. Differentiation is a   linear transformation on this vector space: $\frac{d}{dx}: P → P, p(x) → p'(x)$. (a) What is the dimension of the kernel of $\frac{d}{dx}$   as a linear transformation on $P$ ? (b) The linear transformation $\frac{d}{dx} + 2x$ acts on $P$ as $p(x) → p'(x) + 2xp(x)$. What is the dimension of its kernel? I do know what dimensions and kernels of matrices are but this question is confusing me and I don't really understand it. Would really appreciate some help.","The set of all polynomials in a single variable $x$ forms a vector space $P$ of infinite dimension. Differentiation is a   linear transformation on this vector space: $\frac{d}{dx}: P → P, p(x) → p'(x)$. (a) What is the dimension of the kernel of $\frac{d}{dx}$   as a linear transformation on $P$ ? (b) The linear transformation $\frac{d}{dx} + 2x$ acts on $P$ as $p(x) → p'(x) + 2xp(x)$. What is the dimension of its kernel? I do know what dimensions and kernels of matrices are but this question is confusing me and I don't really understand it. Would really appreciate some help.",,"['linear-algebra', 'ordinary-differential-equations']"
29,Closed Form Solutions of the Second Order Linear ODEs with Non-Constant Coefficients,Closed Form Solutions of the Second Order Linear ODEs with Non-Constant Coefficients,,"I am studying about the linear odes with non-constant coefficients. I know the first order linear ode with non-constant coefficient $$y^{'}(x)+f(x)y(x)=0 \tag{1}$$ has a general solution of the form $$y=Ce^{-\int f(x) dx} \tag{2}$$ However, I am more interested in the case of linear second order odes with non-constant coefficients $$y^{''}(x)+g(x)y^{'}(x)+f(x)y(x)=0 \tag{3}$$ I know that this equation does not have a closed form solution like $(2)$. However, I am interested in special cases of that. Questions $1$. Consider $(3)$, when $g(x)=0$, then we have $$y^{''}(x)+f(x)y(x)=0 \tag{4}$$ Is Eq.$(4)$ a famous well-known equation? If YES , what is its name? $2$. Does $(4)$ have a closed form solution like $(2)$? $3$. Can you name or give me a list of well-known linear second order odes with non-constant coefficients which are not polynomial? For example, I know Cauchy-Euler , Airy , Bessel , Chebyshev , Laguerre and Legendre equations whose coefficients are polynomials. But I don't know any well-known equation with non-polynomial coefficients.","I am studying about the linear odes with non-constant coefficients. I know the first order linear ode with non-constant coefficient $$y^{'}(x)+f(x)y(x)=0 \tag{1}$$ has a general solution of the form $$y=Ce^{-\int f(x) dx} \tag{2}$$ However, I am more interested in the case of linear second order odes with non-constant coefficients $$y^{''}(x)+g(x)y^{'}(x)+f(x)y(x)=0 \tag{3}$$ I know that this equation does not have a closed form solution like $(2)$. However, I am interested in special cases of that. Questions $1$. Consider $(3)$, when $g(x)=0$, then we have $$y^{''}(x)+f(x)y(x)=0 \tag{4}$$ Is Eq.$(4)$ a famous well-known equation? If YES , what is its name? $2$. Does $(4)$ have a closed form solution like $(2)$? $3$. Can you name or give me a list of well-known linear second order odes with non-constant coefficients which are not polynomial? For example, I know Cauchy-Euler , Airy , Bessel , Chebyshev , Laguerre and Legendre equations whose coefficients are polynomials. But I don't know any well-known equation with non-polynomial coefficients.",,['ordinary-differential-equations']
30,How to start an eigenvalue problem,How to start an eigenvalue problem,,"I am stuck on this problem : This is an eigenvalue problem  $$\phi''+ \lambda^2 x(x+2)^2 \phi =0\\\phi(1)=0\\ \phi(0)=0$$ I forget  this kind of problems... Please give me a hint or a clue, cause I don't know how to start. Thanks in advance. My first idea was to get basis like this $\phi_n(x)=x\left(1-x\right)^n $  and $$\phi(x)=\sum_{n=1}^{\infty} a_n \phi_n(x)$$ am I on a right track?","I am stuck on this problem : This is an eigenvalue problem  $$\phi''+ \lambda^2 x(x+2)^2 \phi =0\\\phi(1)=0\\ \phi(0)=0$$ I forget  this kind of problems... Please give me a hint or a clue, cause I don't know how to start. Thanks in advance. My first idea was to get basis like this $\phi_n(x)=x\left(1-x\right)^n $  and $$\phi(x)=\sum_{n=1}^{\infty} a_n \phi_n(x)$$ am I on a right track?",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'lebesgue-integral']"
31,What is the motivation behind the Bessel function of second kind,What is the motivation behind the Bessel function of second kind,,"I am studying Bessel function and found the good reference by G.N. Watson At some point in page 58 he introduces the following expression due to Hankel: \begin{eqnarray}  \lim_{\nu \to n} \frac{J_{\nu}(x) - (-1)^{n} J_{-\nu}(x)}{\nu-n}  \end{eqnarray} This made perfect sense since $J_{\nu}$ and $J_{-\nu}$ are not linearly independent when $\nu = n \in \mathbb{Z}$.  Then the author goes into great detail studying this equation. I was convinced that he would get to the equation: \begin{eqnarray*}   Y_{\nu}(x) =  \frac{ \cos \nu \pi  \; J_{\nu}(x) - J_{-\nu}(x)}{\sin \nu \pi}. \end{eqnarray*} but.....no.  He decided to introduce this equation as a new approach from Hankel with no motivation and let the other development there hanging on the air. Any one can help me understand what could have motivated Hankel to introduce this equation? I know it is good and satisfies the requirements....but.... Thanks. Update: It is easy to see (shown in Watson's book) that the first equation here implies \begin{equation}   \lim_{\nu \to n }  \left [ \frac{\partial J_{\nu}}{\partial \nu}        - (-1)^n \frac{\partial J_{-\nu}}{\partial \nu}       \right ] \quad \quad (1). \end{equation} On the other hand by applying L'Hôpital's rule we find from the second equation above the following chain: (I avoid the argument $x$ on the functions to simplify notations) \begin{eqnarray*}   Y_{\nu}(x) &=& \frac{ \cos \nu \pi  \; J_{\nu}(x) - J_{-\nu}(x)}{\sin \nu \pi} \\  &=& \lim_{\nu  \to n}      \frac{- \pi \sin \nu \pi J_{\nu} + \cos \nu \pi \frac{\partial}{\partial \nu}         J_{\nu} - \frac{\partial}{\partial\nu}{J_{-\nu}} }{\pi \cos \nu \pi} \\ &=& \lim_{\nu \to n } \frac{1}{\pi} \left [ \frac{\partial J_{\nu}}{\partial \nu}        - (-1)^n \frac{\partial J_{-\nu}}{\partial \nu}       \right ]  \end{eqnarray*} So, up to a factor of $1/\pi$, we are back to equation (1). I have no problem accepting equation (1) as a motivated linearly  independent function of $J_{\nu}(x)$. My question specifically is how to go back from equation (1) to the representation $Y_{\nu}(x)$.  It is something like ""inverse""  L'Hôpital rule. Is there a way to do this? According to Watson, the formula above for $Y_{\nu}(x)$ was given by Weber after a small modification of a similar formula given by Hankel (which did not work for half integer numbers). Watson says that Weber got this formula as a limit from an integral representation. ??","I am studying Bessel function and found the good reference by G.N. Watson At some point in page 58 he introduces the following expression due to Hankel: \begin{eqnarray}  \lim_{\nu \to n} \frac{J_{\nu}(x) - (-1)^{n} J_{-\nu}(x)}{\nu-n}  \end{eqnarray} This made perfect sense since $J_{\nu}$ and $J_{-\nu}$ are not linearly independent when $\nu = n \in \mathbb{Z}$.  Then the author goes into great detail studying this equation. I was convinced that he would get to the equation: \begin{eqnarray*}   Y_{\nu}(x) =  \frac{ \cos \nu \pi  \; J_{\nu}(x) - J_{-\nu}(x)}{\sin \nu \pi}. \end{eqnarray*} but.....no.  He decided to introduce this equation as a new approach from Hankel with no motivation and let the other development there hanging on the air. Any one can help me understand what could have motivated Hankel to introduce this equation? I know it is good and satisfies the requirements....but.... Thanks. Update: It is easy to see (shown in Watson's book) that the first equation here implies \begin{equation}   \lim_{\nu \to n }  \left [ \frac{\partial J_{\nu}}{\partial \nu}        - (-1)^n \frac{\partial J_{-\nu}}{\partial \nu}       \right ] \quad \quad (1). \end{equation} On the other hand by applying L'Hôpital's rule we find from the second equation above the following chain: (I avoid the argument $x$ on the functions to simplify notations) \begin{eqnarray*}   Y_{\nu}(x) &=& \frac{ \cos \nu \pi  \; J_{\nu}(x) - J_{-\nu}(x)}{\sin \nu \pi} \\  &=& \lim_{\nu  \to n}      \frac{- \pi \sin \nu \pi J_{\nu} + \cos \nu \pi \frac{\partial}{\partial \nu}         J_{\nu} - \frac{\partial}{\partial\nu}{J_{-\nu}} }{\pi \cos \nu \pi} \\ &=& \lim_{\nu \to n } \frac{1}{\pi} \left [ \frac{\partial J_{\nu}}{\partial \nu}        - (-1)^n \frac{\partial J_{-\nu}}{\partial \nu}       \right ]  \end{eqnarray*} So, up to a factor of $1/\pi$, we are back to equation (1). I have no problem accepting equation (1) as a motivated linearly  independent function of $J_{\nu}(x)$. My question specifically is how to go back from equation (1) to the representation $Y_{\nu}(x)$.  It is something like ""inverse""  L'Hôpital rule. Is there a way to do this? According to Watson, the formula above for $Y_{\nu}(x)$ was given by Weber after a small modification of a similar formula given by Hankel (which did not work for half integer numbers). Watson says that Weber got this formula as a limit from an integral representation. ??",,"['calculus', 'ordinary-differential-equations', 'special-functions', 'math-history', 'bessel-functions']"
32,Best way to go about solving specific tricky 2.ODE,Best way to go about solving specific tricky 2.ODE,,"I've been working on this equation for a while now. Find the particular solution of: $$y''-4y'+y=te^t+t$$ My first instinct was to use the method of undetermined coefficients, solving for $te^t$ and $t$ separately. So I tried this:$$y_{p1}=ate^t+be^t,  y_{p2}=at+b$$ I then worked out what these would be for $y''$ and $y'$ and put them back in the original equation $y''-4y+y$. This resulted in:$$y_{p1}:-2a-2at-2b=t,y_{p2}:a=t(1+3a)$$ So normally I'd be rid of the $t$'s and found a value for the $a$'s. Am I just doing this wrong, or can I not use undetermined coefficients here? Anyways, I also tried variation of parameters and ended up with this abomination: $$y_h=c_1e^{2t+\sqrt{3}t}+c_2e^{2t-\sqrt{3}t}$$ Which gives: $$V_1'=-V_2'e^{2\sqrt{3}}$$ and $$V_2'(-e^{2\sqrt{3}t}+(2+\sqrt{3})e^{2t+\sqrt{3}t})=te^t+t$$ So how would you solve this task? Have I been doing it wrong? Question has been answered: WORK: $$y_{p1}=ate^t+be^t,  y_{p2}=at+b$$ $$y_{p1}:-2a-2at-2b=t,y_{p2}:a=t(1+3a)$$ For $y_{p1}$:$$(-2a)t+(-2a-2b)=t$$$$-2at=t,-2a-2b=0$$$$a=-1/2,b=1/2$$ For $y_{p2}$:$$-4a+at+b=t$$$$at=t,-4a+b=0$$$$a=1,b=4$$ Making $$y_p=y_{p1}+y_{p2} = -1/2te^t+1/2e^t+t+4$$ Thank you very much for your help.","I've been working on this equation for a while now. Find the particular solution of: $$y''-4y'+y=te^t+t$$ My first instinct was to use the method of undetermined coefficients, solving for $te^t$ and $t$ separately. So I tried this:$$y_{p1}=ate^t+be^t,  y_{p2}=at+b$$ I then worked out what these would be for $y''$ and $y'$ and put them back in the original equation $y''-4y+y$. This resulted in:$$y_{p1}:-2a-2at-2b=t,y_{p2}:a=t(1+3a)$$ So normally I'd be rid of the $t$'s and found a value for the $a$'s. Am I just doing this wrong, or can I not use undetermined coefficients here? Anyways, I also tried variation of parameters and ended up with this abomination: $$y_h=c_1e^{2t+\sqrt{3}t}+c_2e^{2t-\sqrt{3}t}$$ Which gives: $$V_1'=-V_2'e^{2\sqrt{3}}$$ and $$V_2'(-e^{2\sqrt{3}t}+(2+\sqrt{3})e^{2t+\sqrt{3}t})=te^t+t$$ So how would you solve this task? Have I been doing it wrong? Question has been answered: WORK: $$y_{p1}=ate^t+be^t,  y_{p2}=at+b$$ $$y_{p1}:-2a-2at-2b=t,y_{p2}:a=t(1+3a)$$ For $y_{p1}$:$$(-2a)t+(-2a-2b)=t$$$$-2at=t,-2a-2b=0$$$$a=-1/2,b=1/2$$ For $y_{p2}$:$$-4a+at+b=t$$$$at=t,-4a+b=0$$$$a=1,b=4$$ Making $$y_p=y_{p1}+y_{p2} = -1/2te^t+1/2e^t+t+4$$ Thank you very much for your help.",,['ordinary-differential-equations']
33,Change of Variables in a Second Order Linear Homogeneous Differential Equation,Change of Variables in a Second Order Linear Homogeneous Differential Equation,,"Consider the differential equation $$\frac{d}{dx} \left( x \frac{dy(x)}{dx}\right) + \frac{\lambda}{x} y(x) = 0$$ This a Sturm-Liouville problem where $\lambda \in \mathbb{R}$ corresponds to the (eventual) eigenvalues of the SL operator. To solve the differential equation we preform the following change of variables $$v(x) = \ln (x) \implies \frac{dy(x)}{dx} = \frac{dv(x)}{dx} \frac{dy(v)}{dv} = \frac{1}{x} \frac{dy(v)}{dv}$$ Plugging in and multiplying through by $x \not = 0$ we find that $$ \frac{d^2 y(v)}{dv^2} + \lambda y(x) = 0$$ The change of variables in the case of the derivative happens naturally in the definition of the derivative and derivation of the chain rule, but how do the variables change in the case of $y(x)$? As an example (not related to the exercise at hand) if $y(x) = \chi_{[0,1]}$ (the indicator on the unit interval) then clearly  $$ y(e) = 0, \ y(v(e)) = y(1) = 1$$ So we cannot have $y(x) = y(v(x))$. However in a suitable domain we could have $$ y(x) = y(\ln(e^x)) = y(v(e^x))$$ At this point I am convinced that there is some fundamental error in my understanding of the change of variables method.","Consider the differential equation $$\frac{d}{dx} \left( x \frac{dy(x)}{dx}\right) + \frac{\lambda}{x} y(x) = 0$$ This a Sturm-Liouville problem where $\lambda \in \mathbb{R}$ corresponds to the (eventual) eigenvalues of the SL operator. To solve the differential equation we preform the following change of variables $$v(x) = \ln (x) \implies \frac{dy(x)}{dx} = \frac{dv(x)}{dx} \frac{dy(v)}{dv} = \frac{1}{x} \frac{dy(v)}{dv}$$ Plugging in and multiplying through by $x \not = 0$ we find that $$ \frac{d^2 y(v)}{dv^2} + \lambda y(x) = 0$$ The change of variables in the case of the derivative happens naturally in the definition of the derivative and derivation of the chain rule, but how do the variables change in the case of $y(x)$? As an example (not related to the exercise at hand) if $y(x) = \chi_{[0,1]}$ (the indicator on the unit interval) then clearly  $$ y(e) = 0, \ y(v(e)) = y(1) = 1$$ So we cannot have $y(x) = y(v(x))$. However in a suitable domain we could have $$ y(x) = y(\ln(e^x)) = y(v(e^x))$$ At this point I am convinced that there is some fundamental error in my understanding of the change of variables method.",,['ordinary-differential-equations']
34,Solving an ODE using Picard's Iteration Method,Solving an ODE using Picard's Iteration Method,,"Find the exact solution of the IVP $y'=y^2$, $y(0)=1$ Starting with $y_0(x)=1$, apply Picard's method to calculate $y_1(x),y_2(x),y_3(x)$, and compare these results with the exact solution. Solving this IVP with separation of variables, I get that $y(x)=\frac{1}{1-x}$. Now using Picard's method ($y_n(x)=y_0+\int_0^xf[t,y_{n-1}(t)]dt$), i get \begin{align} y_0(x) & =1 \\ y_1(x) & =1+x \\ y_2(x) & =1+x+x^2+\frac{x^3}{3} \\ y_3(x) & =1+x+x^2+x^3+\frac{2x^4}{3}+\frac{x^5}{3}+\frac{x^6}{9}+\frac{x^7}{63} \end{align} By now, I would have expected to see some pattern, but I don't really see anything, and continuing to iterate I don't think will get me anywhere.","Find the exact solution of the IVP $y'=y^2$, $y(0)=1$ Starting with $y_0(x)=1$, apply Picard's method to calculate $y_1(x),y_2(x),y_3(x)$, and compare these results with the exact solution. Solving this IVP with separation of variables, I get that $y(x)=\frac{1}{1-x}$. Now using Picard's method ($y_n(x)=y_0+\int_0^xf[t,y_{n-1}(t)]dt$), i get \begin{align} y_0(x) & =1 \\ y_1(x) & =1+x \\ y_2(x) & =1+x+x^2+\frac{x^3}{3} \\ y_3(x) & =1+x+x^2+x^3+\frac{2x^4}{3}+\frac{x^5}{3}+\frac{x^6}{9}+\frac{x^7}{63} \end{align} By now, I would have expected to see some pattern, but I don't really see anything, and continuing to iterate I don't think will get me anywhere.",,"['ordinary-differential-equations', 'initial-value-problems']"
35,Solving constrained Euler-Lagrange equations with Lagrange Multipliers (Geodesics),Solving constrained Euler-Lagrange equations with Lagrange Multipliers (Geodesics),,"I'm trying to solve a calculus of variations geodesics problem using Lagrange Multipliers, showing that the geodesics of a sphere are the so-called great circles. I am using a constrained Lagrangian $$\int_{a}^{b}\dot{x}^2+\dot{y}^2+\dot{z}^2+\lambda(t)G(x(t),y(t),z(t))dt$$ where G(x,y,z) is the sphere $$x^2+y^2+z^2 = 1$$ Computing the Euler-Lagrange equations, I get the three equations: $$\ddot{\vec{r}}=\frac{\lambda(t)}{2}\nabla G$$ and the constraint remains: $$\sqrt{x^2+y^2+z^2}=1\ \text{ or equally }\ x^2+y^2+z^2=1$$ Applied to the circle, we find: $$\ddot{\mathbf{r}}=\lambda(t)\mathbf{r}$$ So now we have four equations, an unknown lambda function, and 3 variables. How do I determine lambda and simplify it enough to solve in Mathematica? This is very unfamiliar material for me, so help is appreciated. Thanks $$$$ $\textbf{Edit: expanding on the solution provided by Qmechanic}$ $$$$ Differentiating the constraint twice yields the familiar $$\mathbf{\ddot{r}}\cdot\hat{\mathbf{r}}=-\frac{v^2}{r}$$ And taking the dot product of the EL equations with $\mathbf{\hat{r}}$ and substituting yields $$-\frac{v^2}{r^2}=\lambda(t)$$ Substituting this back into the original equation for $\lambda(t)$, we find that $$\ddot{\mathbf{r}}=-\frac{v^2}{r}\mathbf{\hat{r}}$$ This allows an infinite number of geodesics, but choosing the parameterization that leaves v constant, this simplifies to $$\mathbf{\ddot{r}}=-\mathbf{r}$$ which can be solved for the Cartesian coordinates: $$\begin{matrix} 	x=C_1 \sin (t)+C_2 \cos (t) \\ 	y=C_2 \sin (t)+C_4 \cos (t) \\ 	z=C_3 \sin (t)+C_6 \cos (t) \\ \end{matrix}$$ Which, with appropriate initial conditions, forms a great circle between the two points.","I'm trying to solve a calculus of variations geodesics problem using Lagrange Multipliers, showing that the geodesics of a sphere are the so-called great circles. I am using a constrained Lagrangian $$\int_{a}^{b}\dot{x}^2+\dot{y}^2+\dot{z}^2+\lambda(t)G(x(t),y(t),z(t))dt$$ where G(x,y,z) is the sphere $$x^2+y^2+z^2 = 1$$ Computing the Euler-Lagrange equations, I get the three equations: $$\ddot{\vec{r}}=\frac{\lambda(t)}{2}\nabla G$$ and the constraint remains: $$\sqrt{x^2+y^2+z^2}=1\ \text{ or equally }\ x^2+y^2+z^2=1$$ Applied to the circle, we find: $$\ddot{\mathbf{r}}=\lambda(t)\mathbf{r}$$ So now we have four equations, an unknown lambda function, and 3 variables. How do I determine lambda and simplify it enough to solve in Mathematica? This is very unfamiliar material for me, so help is appreciated. Thanks $$$$ $\textbf{Edit: expanding on the solution provided by Qmechanic}$ $$$$ Differentiating the constraint twice yields the familiar $$\mathbf{\ddot{r}}\cdot\hat{\mathbf{r}}=-\frac{v^2}{r}$$ And taking the dot product of the EL equations with $\mathbf{\hat{r}}$ and substituting yields $$-\frac{v^2}{r^2}=\lambda(t)$$ Substituting this back into the original equation for $\lambda(t)$, we find that $$\ddot{\mathbf{r}}=-\frac{v^2}{r}\mathbf{\hat{r}}$$ This allows an infinite number of geodesics, but choosing the parameterization that leaves v constant, this simplifies to $$\mathbf{\ddot{r}}=-\mathbf{r}$$ which can be solved for the Cartesian coordinates: $$\begin{matrix} 	x=C_1 \sin (t)+C_2 \cos (t) \\ 	y=C_2 \sin (t)+C_4 \cos (t) \\ 	z=C_3 \sin (t)+C_6 \cos (t) \\ \end{matrix}$$ Which, with appropriate initial conditions, forms a great circle between the two points.",,"['ordinary-differential-equations', 'optimization', 'vector-analysis', 'geodesic', 'euler-lagrange-equation']"
36,Solve system inhomogeneous differential equations with variable coefficients,Solve system inhomogeneous differential equations with variable coefficients,,"Given the system of differential equations $$\frac{d\vec{y}}{dx} = \begin{pmatrix}         0 & 1 \\         -1 & 0 \\         \end{pmatrix}\vec{y} \ + \begin{pmatrix}         sin(wx) \\         0\\         \end{pmatrix} \ \ \ \ (w \neq \pm1) $$ There are two questions which I can't answer. 1. How can I find the general solution? 2. How can I find the periodic solutions (in general). I've tried to solve the following system$$\frac{d\vec{y}}{dx} = \begin{pmatrix}         0 & 1 \\         -1 & 0 \\         \end{pmatrix}\vec{y} $$ to find the complementary solution, which was  $$\vec{y} = c_1\begin{pmatrix}         cos(x) \\         -sin(x)\\         \end{pmatrix} \ +c_2\begin{pmatrix}         sin(x) \\         cos(x)\\         \end{pmatrix} $$ So how do I proceed?","Given the system of differential equations $$\frac{d\vec{y}}{dx} = \begin{pmatrix}         0 & 1 \\         -1 & 0 \\         \end{pmatrix}\vec{y} \ + \begin{pmatrix}         sin(wx) \\         0\\         \end{pmatrix} \ \ \ \ (w \neq \pm1) $$ There are two questions which I can't answer. 1. How can I find the general solution? 2. How can I find the periodic solutions (in general). I've tried to solve the following system$$\frac{d\vec{y}}{dx} = \begin{pmatrix}         0 & 1 \\         -1 & 0 \\         \end{pmatrix}\vec{y} $$ to find the complementary solution, which was  $$\vec{y} = c_1\begin{pmatrix}         cos(x) \\         -sin(x)\\         \end{pmatrix} \ +c_2\begin{pmatrix}         sin(x) \\         cos(x)\\         \end{pmatrix} $$ So how do I proceed?",,['ordinary-differential-equations']
37,More elegant way of verifying the solution of $y'=\frac{-(x+2)+\sqrt{x^2+4x+4y}}{2}$?,More elegant way of verifying the solution of ?,y'=\frac{-(x+2)+\sqrt{x^2+4x+4y}}{2},"The question asks to verify that for any value of $c$, $y=c^2+cx+2c+1$ satisfies the solution. $$y'=\frac{-(x+2)+\sqrt{x^2+4x+4y}}{2}\tag{1}$$ I am aware we can just prove this by direct substitution. However, I think it's no coincidence that the author specifically put the DE in the form of a solution to a quadratic equation. $(1)$ can be interpreted as a solution of the quadractic equation $$(y')^2+(x+2)y'+(1-y)=0$$ Which can be factored to $$(y'-c)(y'+c+x+2)=0\tag{2}$$ The first factor gives $y'=c$, which comes from the solution we were already given $(y=c^2+cx+2c+1)$. The second factor we just added when we constructed the quadratic equation, and it's not necessarily a solution of $(1)$. However, I don't think we can conclude that $y=c^2+cx+2c+1$ is a solution of $(1)$ just because $y'=c$ is a solution of $(2)$. Is there any way to complete the question from here, or is another approach neccesary?","The question asks to verify that for any value of $c$, $y=c^2+cx+2c+1$ satisfies the solution. $$y'=\frac{-(x+2)+\sqrt{x^2+4x+4y}}{2}\tag{1}$$ I am aware we can just prove this by direct substitution. However, I think it's no coincidence that the author specifically put the DE in the form of a solution to a quadratic equation. $(1)$ can be interpreted as a solution of the quadractic equation $$(y')^2+(x+2)y'+(1-y)=0$$ Which can be factored to $$(y'-c)(y'+c+x+2)=0\tag{2}$$ The first factor gives $y'=c$, which comes from the solution we were already given $(y=c^2+cx+2c+1)$. The second factor we just added when we constructed the quadratic equation, and it's not necessarily a solution of $(1)$. However, I don't think we can conclude that $y=c^2+cx+2c+1$ is a solution of $(1)$ just because $y'=c$ is a solution of $(2)$. Is there any way to complete the question from here, or is another approach neccesary?",,"['calculus', 'ordinary-differential-equations']"
38,Why is Laplace Transform used for ODEs,Why is Laplace Transform used for ODEs,,"This part is taken from differential equations with applications and historical George simmons. According to the given information , there are another integral transformation.I wonder why is the Laplace transform ($a=0 , b=\infty , K(p,x)=e^{-px}$) begin used for solving ODEs.Are there any advantages or other particular reasons for using this combination ?","This part is taken from differential equations with applications and historical George simmons. According to the given information , there are another integral transformation.I wonder why is the Laplace transform ($a=0 , b=\infty , K(p,x)=e^{-px}$) begin used for solving ODEs.Are there any advantages or other particular reasons for using this combination ?",,"['ordinary-differential-equations', 'laplace-transform']"
39,How to construct an exact but non separable differential equation?,How to construct an exact but non separable differential equation?,,"So I can prove that all separable differential equations are exact, and I can intuitively figure out that not all exact differential equations are necessarily separable, but I'm having a hard time constructing a counterexample? Is a differential equation still considered inseparable if you cannot initially separate it, but do so using an integrating factor? Sorry for asking rudimentary questions, I'm just starting to learn diff eq's. Thank you in advance.","So I can prove that all separable differential equations are exact, and I can intuitively figure out that not all exact differential equations are necessarily separable, but I'm having a hard time constructing a counterexample? Is a differential equation still considered inseparable if you cannot initially separate it, but do so using an integrating factor? Sorry for asking rudimentary questions, I'm just starting to learn diff eq's. Thank you in advance.",,"['calculus', 'ordinary-differential-equations']"
40,Simple system of two nonhomogeneous ordinary differential equations solved by elimination. (3.1-15),Simple system of two nonhomogeneous ordinary differential equations solved by elimination. (3.1-15),,"My differential equations textbook states to use the ""elimination method"" to crack this for $x$ and $y$. The final answer uses $t$ as the independent variable which both $x$ and $y$ are dependent on. I was able to solve this for $x(t)$ but it is $y(t)$ where I am having difficulty duplicating the answer in text. The system consists of the following two linear ordinary differential equations written in linear differential operator forms: $$\begin{align*} &(1) \: D(x + y) = x + t &\\ &(2) \: D^2y = Dx & \end{align*}$$ The textbook states that the general solution for both $x$ and $y$ are: $$\begin{align*} & x(t) = \frac{1}{2}t^2 + c_1t + c_2 &\\ & y(t) = \frac{1}{6}t^3 + \frac{1}{2}c_1t^2 + (c_2 - c_1)t + c_3 &\\ \end{align*}$$ Again I had no problem deriving $x(t)$. It is my solution for $y(t)$ which almost but not fully agrees with the answer in the text. The following steps shows my derivation for $y(t)$. I start by differentiating the known solution for $x$: $$\begin{align*} & x = \frac{1}{2}t^2 + c_1t + c_2 &\\ & x' = t + c_1 &\\ \end{align*}$$ We now integrate twice to find $y$ with substitutions (shown in parenthesis) along the way for $x'$ and $x$ while combining arbitrary constants as needed yielding: $$\begin{align*} & \: y'' = x' &\\ & \: y'' = (t + c_1) &\\ & \: y' = x + c &\\ & \: y' = (\frac{1}{2}t^2 + c_1t + c_2)+ c &\\ & \: y' = \frac{1}{2}t^2 + c_1t + c_2 \\ & \: y = \frac{1}{6}t^3 + \frac{1}{2}c_1t^2 + c_2t + c_3 & \end{align*}$$ Again my textbook states that the general solution for $y$ should look like this: $$\begin{align*} & y(t) = \frac{1}{6}t^3 + \frac{1}{2}c_1t^2 + (c_2 - c_1)t + c_3 &\\ \end{align*}$$ But I get this: $$\begin{align*} & \: y(t) = \frac{1}{6}t^3 + \frac{1}{2}c_1t^2 + c_2t + c_3 & \end{align*}$$ Where did the $-c_1t$ come from?","My differential equations textbook states to use the ""elimination method"" to crack this for $x$ and $y$. The final answer uses $t$ as the independent variable which both $x$ and $y$ are dependent on. I was able to solve this for $x(t)$ but it is $y(t)$ where I am having difficulty duplicating the answer in text. The system consists of the following two linear ordinary differential equations written in linear differential operator forms: $$\begin{align*} &(1) \: D(x + y) = x + t &\\ &(2) \: D^2y = Dx & \end{align*}$$ The textbook states that the general solution for both $x$ and $y$ are: $$\begin{align*} & x(t) = \frac{1}{2}t^2 + c_1t + c_2 &\\ & y(t) = \frac{1}{6}t^3 + \frac{1}{2}c_1t^2 + (c_2 - c_1)t + c_3 &\\ \end{align*}$$ Again I had no problem deriving $x(t)$. It is my solution for $y(t)$ which almost but not fully agrees with the answer in the text. The following steps shows my derivation for $y(t)$. I start by differentiating the known solution for $x$: $$\begin{align*} & x = \frac{1}{2}t^2 + c_1t + c_2 &\\ & x' = t + c_1 &\\ \end{align*}$$ We now integrate twice to find $y$ with substitutions (shown in parenthesis) along the way for $x'$ and $x$ while combining arbitrary constants as needed yielding: $$\begin{align*} & \: y'' = x' &\\ & \: y'' = (t + c_1) &\\ & \: y' = x + c &\\ & \: y' = (\frac{1}{2}t^2 + c_1t + c_2)+ c &\\ & \: y' = \frac{1}{2}t^2 + c_1t + c_2 \\ & \: y = \frac{1}{6}t^3 + \frac{1}{2}c_1t^2 + c_2t + c_3 & \end{align*}$$ Again my textbook states that the general solution for $y$ should look like this: $$\begin{align*} & y(t) = \frac{1}{6}t^3 + \frac{1}{2}c_1t^2 + (c_2 - c_1)t + c_3 &\\ \end{align*}$$ But I get this: $$\begin{align*} & \: y(t) = \frac{1}{6}t^3 + \frac{1}{2}c_1t^2 + c_2t + c_3 & \end{align*}$$ Where did the $-c_1t$ come from?",,"['ordinary-differential-equations', 'multivariable-calculus', 'systems-of-equations']"
41,"Second order ODE, first derivative missing","Second order ODE, first derivative missing",,"I have the following second order equation, where the first derivative is missing, and I am asked to find its general solution: $$6x^{2}yy''=3x(3y^{2}+2)+2(3y^{2}+2)^3$$ I don't know how to solve it. I have tried with a $u(x)=3y^{2}+2$ substitution but it doesn't seem useful... Is there any method for this kind of equation whithout $y'$?","I have the following second order equation, where the first derivative is missing, and I am asked to find its general solution: $$6x^{2}yy''=3x(3y^{2}+2)+2(3y^{2}+2)^3$$ I don't know how to solve it. I have tried with a $u(x)=3y^{2}+2$ substitution but it doesn't seem useful... Is there any method for this kind of equation whithout $y'$?",,['ordinary-differential-equations']
42,Differential equation of non standard form.,Differential equation of non standard form.,,Solve the differential equation: $$\frac{dy}{dx}=\frac{x^{2}-y^{2}}{x^{2}(y^{2}+1)}$$ I tried to convert it into an exact differential but I failed to do so. I also tried to bring the equation into standard forms but again failed. Please help me. Here's the exact question:,Solve the differential equation: $$\frac{dy}{dx}=\frac{x^{2}-y^{2}}{x^{2}(y^{2}+1)}$$ I tried to convert it into an exact differential but I failed to do so. I also tried to bring the equation into standard forms but again failed. Please help me. Here's the exact question:,,"['calculus', 'ordinary-differential-equations']"
43,Turning an initial value problem into an integral equation,Turning an initial value problem into an integral equation,,"Show that the initial value problem(IVP) $$y'' + y^2 − 1 = 0, \qquad y(0) = 0, \qquad y'(0) = 1$$ is equivalent to the integral equation $$y(t) = t − \int_{0}^{t} \int_{0}^{T}(y^2(s) − 1)dsdT$$","Show that the initial value problem(IVP) $$y'' + y^2 − 1 = 0, \qquad y(0) = 0, \qquad y'(0) = 1$$ is equivalent to the integral equation $$y(t) = t − \int_{0}^{t} \int_{0}^{T}(y^2(s) − 1)dsdT$$",,['ordinary-differential-equations']
44,Irrational flow on torus,Irrational flow on torus,,"I have an interesting dynamical systems problem that has had me stumped for a few hours now, so I'm hoping I can get some help. The problem is concerned with flows on the torus. The model is given by $\dot {\theta}_1=\omega_1$ and $\dot {\theta}_2=\omega_2$, where $\theta_{1,2}$ are the phases of the oscillators and $\omega_{1,2}$ are the natural frequencies. The problem is to show if $\frac{\omega_1}{\omega_2}$ is irrational, then every trajectory is dense. The only idea I've come up with is contradiction, since it seems clear that the trajectories would have to be dense since they're quasiperiodic. Any help would be greatly appreciated. Thanks","I have an interesting dynamical systems problem that has had me stumped for a few hours now, so I'm hoping I can get some help. The problem is concerned with flows on the torus. The model is given by $\dot {\theta}_1=\omega_1$ and $\dot {\theta}_2=\omega_2$, where $\theta_{1,2}$ are the phases of the oscillators and $\omega_{1,2}$ are the natural frequencies. The problem is to show if $\frac{\omega_1}{\omega_2}$ is irrational, then every trajectory is dense. The only idea I've come up with is contradiction, since it seems clear that the trajectories would have to be dense since they're quasiperiodic. Any help would be greatly appreciated. Thanks",,"['ordinary-differential-equations', 'dynamical-systems']"
45,obtaining easy differential equation solution,obtaining easy differential equation solution,,"I passed by a question of a differential equation and i need help solving it thought its easy but am new with differential equations. Let S be the solution of the differential equation : $xy' -y= \frac{-x^2}{x^2+1}$. Deduce S. i guess the first thing to do is to work out on  $xy' -y= 0$, is that true?","I passed by a question of a differential equation and i need help solving it thought its easy but am new with differential equations. Let S be the solution of the differential equation : $xy' -y= \frac{-x^2}{x^2+1}$. Deduce S. i guess the first thing to do is to work out on  $xy' -y= 0$, is that true?",,"['ordinary-differential-equations', 'differential']"
46,Differential Equations Complex Eigenvalue functions,Differential Equations Complex Eigenvalue functions,,"Show that a function of the form $x(t) = K_1 \cos\beta t + K_2 \sin\beta t$ Can be written as $x(t) = K\cos(Bt-\phi)$ Where $K = \sqrt {K_1^2 + K_2^2}$ I know that linear systems with complex coefficients are sometimes expressed in this form, however I'm not sure if/how that would be useful to solve this problem. Any suggestions on how to approach this would be greatly appreciated.","Show that a function of the form $x(t) = K_1 \cos\beta t + K_2 \sin\beta t$ Can be written as $x(t) = K\cos(Bt-\phi)$ Where $K = \sqrt {K_1^2 + K_2^2}$ I know that linear systems with complex coefficients are sometimes expressed in this form, however I'm not sure if/how that would be useful to solve this problem. Any suggestions on how to approach this would be greatly appreciated.",,"['ordinary-differential-equations', 'trigonometry', 'complex-numbers', 'eigenfunctions']"
47,Show that the Wronskian of a fundamental solution set of solutions is a constant.,Show that the Wronskian of a fundamental solution set of solutions is a constant.,,"Show that the Wronskian of a fundamental solution set of solutions for a L.D.E of the form: $ a_0(x)y^{(n)} + a_2(x)y^{(n-2)} + ... + a_n(x)y = 0$, (there is no derivative term of the order n-1) is a constant: Consider,  $W(x) = det(\Omega(x)) = \begin{vmatrix} y_1(x) ... y_{n-1}(x) \\ y_1'(x) ... y_{n-1}'(x) \\ ............ \\ y_1^{n-2}(x) ... y_{n-1}^{n-2}(x) \end{vmatrix} $. I wanted to use some facts about the determinant to show that the Wronskian is constant, namely: any determinant of a matrix with two rows exactly the same is $0$. However, I am not sure if the row before the last row of $W(x)$ is the same as the last row.","Show that the Wronskian of a fundamental solution set of solutions for a L.D.E of the form: $ a_0(x)y^{(n)} + a_2(x)y^{(n-2)} + ... + a_n(x)y = 0$, (there is no derivative term of the order n-1) is a constant: Consider,  $W(x) = det(\Omega(x)) = \begin{vmatrix} y_1(x) ... y_{n-1}(x) \\ y_1'(x) ... y_{n-1}'(x) \\ ............ \\ y_1^{n-2}(x) ... y_{n-1}^{n-2}(x) \end{vmatrix} $. I wanted to use some facts about the determinant to show that the Wronskian is constant, namely: any determinant of a matrix with two rows exactly the same is $0$. However, I am not sure if the row before the last row of $W(x)$ is the same as the last row.",,['ordinary-differential-equations']
48,Still getting wrong answer after trying to solve $x''(t)+4x(t)=t^2$ where $x(0)=1$ and $x'(0)=2$,Still getting wrong answer after trying to solve  where  and,x''(t)+4x(t)=t^2 x(0)=1 x'(0)=2,"I am trying to solve this differential equation: $$x''(t)+4x(t)=t^2,x(0)=1,x'(0)=2$$ The answer should be: $$x(t)=\frac{1}{4}t^2-\frac{1}{8}+\frac{9}{8}\cos{2t}+\sin{2t}$$ Which is also verified by WolframAlpha. But instead I got by calculating by hand: $$x(t)=\frac{1}{4}t^2-\frac{1}{4}+\frac{9}{8}\cos{2t}+\sin{2t}$$ Here's how I got it.  The Laplace transform of the differential equation is: $$s^2X(s)-sx(0)-x'(0)+4X(s)=\frac{2}{s^3}$$ Since $x(0)=1$ and $x'(0)=2$: $$s^2X(s)-s-2+4X(s)=\frac{2}{s^3}$$ Solving for $X(s)$: $$X(s)=\frac{s^4+2s^3+2}{s^3(s^2+4)}=\frac{s^4+2s^3+2}{s^3(s+j2)(s-j2)}$$ Breaking it up into partial fractions: $$X(s)=\frac{A}{s^3}+\frac{B}{s^2}+\frac{C}{s}+\frac{D}{s+j2}+\frac{E}{s-j2}$$ Solving for the coefficients: $$ \begin{align} A &= s^3X(s)|_{s=0} = \frac{1}{2} \\ D &= (s+j2)X(s)|_{s=-j2} = \frac{9}{16}+j\frac{1}{2} \\ E &= (s-j2)X(s)|_{s=j2} = \frac{9}{16}-j\frac{1}{2} \\ \end{align} $$ But this part is where I think I calculated wrong but couldn't figure out why: $$ \begin{align} B &= [\frac{d}{ds}s^3X(s)]_{s=0} = \frac{d}{ds}(\frac{s^4+2s^3+2}{s^2+4})|_{s=0}\\ &=\frac{(4s^3+6s^2)(s^2+4)-(s^4+2s^3+2)(2s)}{(s^2+4)^2}|_{s=0} \\ &=\frac{2s^5+2s^4+16s^3+24s^2-4s}{s^4+8s^2+16}|_{s=0} = 0  \end{align} $$ $$ \begin{align} C &= [\frac{d^2}{ds^2}s^3X(s)]_{s=0}=\frac{d}{ds}\frac{2s^5+2s^4+16s^3+24s^2-4s}{s^4+8s^2+16}|_{s=0} \\ &= \frac{(10s^4+8s^3+48s^2+48s-4)(s^4+8s^2+16)-(2s^5+2s^4+16s^3+24s^2-4s)(4s^3+16s)}{(s^4+8s^2+16)^2}|_{s=0} \\ &= \frac{(-4)(16)}{(16)^2} = \frac{-1}{4} \end{align} $$ Substituting in the coefficients: $$X(s)=\frac{1}{2}\cdot\frac{1}{s^3}-\frac{1}{4}\cdot\frac{1}{s}+(\frac{9}{16}+j\frac{1}{2})\cdot\frac{1}{s+j2}+(\frac{9}{16}-j\frac{1}{2})\cdot\frac{1}{s-j2}$$ Then do inverse Laplace transform: $$ \begin{align} x(t) &= \frac{1}{4}t^2-\frac{1}{4}+(\frac{9}{16}+j\frac{1}{2})e^{-j2t}+(\frac{9}{16}-j\frac{1}{2})e^{j2t}\\ &= \frac{1}{4}t^2-\frac{1}{4}+\frac{9}{8}(\frac{e^{j2t}+e^{-j2t}}{2})+(\frac{e^{j2t}-e^{-j2t}}{j2})\\ &=\frac{1}{4}t^2-\frac{1}{4}+\frac{9}{8}\cos{2t}+\sin{2t} \end{align} $$","I am trying to solve this differential equation: $$x''(t)+4x(t)=t^2,x(0)=1,x'(0)=2$$ The answer should be: $$x(t)=\frac{1}{4}t^2-\frac{1}{8}+\frac{9}{8}\cos{2t}+\sin{2t}$$ Which is also verified by WolframAlpha. But instead I got by calculating by hand: $$x(t)=\frac{1}{4}t^2-\frac{1}{4}+\frac{9}{8}\cos{2t}+\sin{2t}$$ Here's how I got it.  The Laplace transform of the differential equation is: $$s^2X(s)-sx(0)-x'(0)+4X(s)=\frac{2}{s^3}$$ Since $x(0)=1$ and $x'(0)=2$: $$s^2X(s)-s-2+4X(s)=\frac{2}{s^3}$$ Solving for $X(s)$: $$X(s)=\frac{s^4+2s^3+2}{s^3(s^2+4)}=\frac{s^4+2s^3+2}{s^3(s+j2)(s-j2)}$$ Breaking it up into partial fractions: $$X(s)=\frac{A}{s^3}+\frac{B}{s^2}+\frac{C}{s}+\frac{D}{s+j2}+\frac{E}{s-j2}$$ Solving for the coefficients: $$ \begin{align} A &= s^3X(s)|_{s=0} = \frac{1}{2} \\ D &= (s+j2)X(s)|_{s=-j2} = \frac{9}{16}+j\frac{1}{2} \\ E &= (s-j2)X(s)|_{s=j2} = \frac{9}{16}-j\frac{1}{2} \\ \end{align} $$ But this part is where I think I calculated wrong but couldn't figure out why: $$ \begin{align} B &= [\frac{d}{ds}s^3X(s)]_{s=0} = \frac{d}{ds}(\frac{s^4+2s^3+2}{s^2+4})|_{s=0}\\ &=\frac{(4s^3+6s^2)(s^2+4)-(s^4+2s^3+2)(2s)}{(s^2+4)^2}|_{s=0} \\ &=\frac{2s^5+2s^4+16s^3+24s^2-4s}{s^4+8s^2+16}|_{s=0} = 0  \end{align} $$ $$ \begin{align} C &= [\frac{d^2}{ds^2}s^3X(s)]_{s=0}=\frac{d}{ds}\frac{2s^5+2s^4+16s^3+24s^2-4s}{s^4+8s^2+16}|_{s=0} \\ &= \frac{(10s^4+8s^3+48s^2+48s-4)(s^4+8s^2+16)-(2s^5+2s^4+16s^3+24s^2-4s)(4s^3+16s)}{(s^4+8s^2+16)^2}|_{s=0} \\ &= \frac{(-4)(16)}{(16)^2} = \frac{-1}{4} \end{align} $$ Substituting in the coefficients: $$X(s)=\frac{1}{2}\cdot\frac{1}{s^3}-\frac{1}{4}\cdot\frac{1}{s}+(\frac{9}{16}+j\frac{1}{2})\cdot\frac{1}{s+j2}+(\frac{9}{16}-j\frac{1}{2})\cdot\frac{1}{s-j2}$$ Then do inverse Laplace transform: $$ \begin{align} x(t) &= \frac{1}{4}t^2-\frac{1}{4}+(\frac{9}{16}+j\frac{1}{2})e^{-j2t}+(\frac{9}{16}-j\frac{1}{2})e^{j2t}\\ &= \frac{1}{4}t^2-\frac{1}{4}+\frac{9}{8}(\frac{e^{j2t}+e^{-j2t}}{2})+(\frac{e^{j2t}-e^{-j2t}}{j2})\\ &=\frac{1}{4}t^2-\frac{1}{4}+\frac{9}{8}\cos{2t}+\sin{2t} \end{align} $$",,"['ordinary-differential-equations', 'laplace-transform', 'partial-fractions']"
49,Solve: $x''(t)-2x'(t) + x(t) = 2 \sin(3t)$,Solve:,x''(t)-2x'(t) + x(t) = 2 \sin(3t),"It is asked to solve the ODE $x''(t)-2x'(t) + x(t) = 2 \sin(3t)$ for $x(0)=10, \; x'(0)=0$ It is equivalent to the first order system in two variables $$\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ -1 & 2 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix}0 \\ 1 \end{bmatrix} 2 \sin(3t), \; \begin{bmatrix}x_0 \\ y_0 \end{bmatrix} = \begin{bmatrix} 10 \\ 0 \end{bmatrix}$$ If $$A = \begin{bmatrix} 0 & 1 \\ -1 & 2 \end{bmatrix} , B = \begin{bmatrix}0 \\ 1 \end{bmatrix}$$ the solution for this ODE of first order is $$ e^{tA} \begin{bmatrix} x_0 \\ y_0 \end{bmatrix} + \int_0^t e^{(t-s)A}2B\sin(3s)ds $$ I know we could look for the characteristic polynomial of the equation and try to find a solution which combines sines and cosines terms, but since I am studying resolution of second order systems using first order ones, I would like to check if this is a good way of solving it. The exponential of the matrix $tA$, for example, doesn't seems to have a good form (except if I wrote something wrong). So, what is the better way of solving it? Thanks in advance! Edit: $$A = \begin{bmatrix} 0 & 1 \\ -1 & 2 \end{bmatrix} = \begin{bmatrix} 1&-1 \\ 1&0 \end{bmatrix} \begin{bmatrix} 1&1 \\0 &1 \end{bmatrix} \begin{bmatrix}0 &1 \\ -1&1 \end{bmatrix}$$ $$\Rightarrow e^{t\begin{bmatrix} 1& 1\\0 &1 \end{bmatrix}} = e^{t\begin{bmatrix} 1&0 \\ 0&1 \end{bmatrix} + \begin{bmatrix} 0&t \\ 0&0 \end{bmatrix}}=\begin{bmatrix}e^t &0 \\0 &e^t \end{bmatrix} \begin{bmatrix} 1&t \\0 &1 \end{bmatrix}$$","It is asked to solve the ODE $x''(t)-2x'(t) + x(t) = 2 \sin(3t)$ for $x(0)=10, \; x'(0)=0$ It is equivalent to the first order system in two variables $$\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ -1 & 2 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix}0 \\ 1 \end{bmatrix} 2 \sin(3t), \; \begin{bmatrix}x_0 \\ y_0 \end{bmatrix} = \begin{bmatrix} 10 \\ 0 \end{bmatrix}$$ If $$A = \begin{bmatrix} 0 & 1 \\ -1 & 2 \end{bmatrix} , B = \begin{bmatrix}0 \\ 1 \end{bmatrix}$$ the solution for this ODE of first order is $$ e^{tA} \begin{bmatrix} x_0 \\ y_0 \end{bmatrix} + \int_0^t e^{(t-s)A}2B\sin(3s)ds $$ I know we could look for the characteristic polynomial of the equation and try to find a solution which combines sines and cosines terms, but since I am studying resolution of second order systems using first order ones, I would like to check if this is a good way of solving it. The exponential of the matrix $tA$, for example, doesn't seems to have a good form (except if I wrote something wrong). So, what is the better way of solving it? Thanks in advance! Edit: $$A = \begin{bmatrix} 0 & 1 \\ -1 & 2 \end{bmatrix} = \begin{bmatrix} 1&-1 \\ 1&0 \end{bmatrix} \begin{bmatrix} 1&1 \\0 &1 \end{bmatrix} \begin{bmatrix}0 &1 \\ -1&1 \end{bmatrix}$$ $$\Rightarrow e^{t\begin{bmatrix} 1& 1\\0 &1 \end{bmatrix}} = e^{t\begin{bmatrix} 1&0 \\ 0&1 \end{bmatrix} + \begin{bmatrix} 0&t \\ 0&0 \end{bmatrix}}=\begin{bmatrix}e^t &0 \\0 &e^t \end{bmatrix} \begin{bmatrix} 1&t \\0 &1 \end{bmatrix}$$",,"['linear-algebra', 'ordinary-differential-equations', 'exponential-function']"
50,How is implicit ODE different from explicit ODE?,How is implicit ODE different from explicit ODE?,,"I've seen the wikipedia article ; there it has been written as : Let F be a given function of x, y, and derivatives of y. Then an equation of the form $$F\left(x,y,y',y'',\cdots,y^{(n-1)}\right) = y^{(n)}$$ is called an explicit ordinary differential equation of order n. More generally, an implicit ordinary differential equation of order n takes the form: $$F\left(x,y,y',y'',\cdots,y^{(n-1)},y^{(n)}\right) = 0$$ Can anyone give me a good example to distinguish these two definitions; I know implicit & explicit function but having problem in visualising this here.","I've seen the wikipedia article ; there it has been written as : Let F be a given function of x, y, and derivatives of y. Then an equation of the form is called an explicit ordinary differential equation of order n. More generally, an implicit ordinary differential equation of order n takes the form: Can anyone give me a good example to distinguish these two definitions; I know implicit & explicit function but having problem in visualising this here.","F\left(x,y,y',y'',\cdots,y^{(n-1)}\right) = y^{(n)} F\left(x,y,y',y'',\cdots,y^{(n-1)},y^{(n)}\right) = 0",[]
51,Is there any general method to solve a Second order ODE with variable coefficient of the form $a(x)y''(x)+b(x)y'(x)+c(x)y(x)=0$,Is there any general method to solve a Second order ODE with variable coefficient of the form,a(x)y''(x)+b(x)y'(x)+c(x)y(x)=0,Is there any general method to solve a Second order ODE with variable coefficient of the form $a(x)y''(x)+b(x)y'(x)+c(x)y(x)=0$,Is there any general method to solve a Second order ODE with variable coefficient of the form $a(x)y''(x)+b(x)y'(x)+c(x)y(x)=0$,,['ordinary-differential-equations']
52,Finding strict Liapunov function,Finding strict Liapunov function,,"Find a strict Liapunov function for the equilibrium point (0, 0) of $$x'   = −2x − y^ 2$$ $$y'  = −y − x^2 .$$ Find δ > 0 as large as possible so that the open disk of radius δ and center (0, 0) is contained in the basin of (0, 0). My attempt was $V = \frac{ax^2+by^2}{2}$, which gives $$ \dot{V}= -2ax^2 -by^2 -axy^2 - bx^2y$$ But I couldnt find the open set. Any hint? Thanks!","Find a strict Liapunov function for the equilibrium point (0, 0) of $$x'   = −2x − y^ 2$$ $$y'  = −y − x^2 .$$ Find δ > 0 as large as possible so that the open disk of radius δ and center (0, 0) is contained in the basin of (0, 0). My attempt was $V = \frac{ax^2+by^2}{2}$, which gives $$ \dot{V}= -2ax^2 -by^2 -axy^2 - bx^2y$$ But I couldnt find the open set. Any hint? Thanks!",,"['ordinary-differential-equations', 'lyapunov-functions']"
53,Differential Equations in Milnor's Topology from the Differential Viewpoint,Differential Equations in Milnor's Topology from the Differential Viewpoint,,"On page $23$ Milnor states: Let $\varphi$ : $\mathbb{R}^n \rightarrow \mathbb{R}$ be a smooth function which satisfies $$\begin{cases} \varphi(x) > 0, & {\rm for}\,\|x\| < 1 \\ \varphi(x) = 0, &{\rm for}\,\|x\| \geq 1\end{cases}$$   Given any fixed unit vector $c \in \mathbb{S} ^{n-1} $, consider the differential equations    $$\frac{dx_i}{dt} = c_{i} \cdot \varphi(x_1,x_2,\dots,x_n);  \hspace{10 mm} i = 1, \dots , n.$$   For any $ \hat x \in \mathbb{R}^n$ these equations have a unique solution $x = x(t)$, defined for all real numbers which satisfies the initial condition    $$x(0)= \hat x.$$   We will use the notation $x(t) = F_{t}( \hat x)$ for this solution. Then clearly 1) $F_{t}$ is defined for all $t$ and $ \hat x $ and depends smoothly on $t$ and $ \hat x $, 2) $F_{0}( \hat x ) =  \hat x $, 3) $F_{s+t}( \hat x ) = F_{s} \circ F_{t}( \hat x )$. 2) is quite clear, however I don't understand why 1) and 3) are valid. Could anybody explain that? Moreover on page $24$ Milnor states that clearly, with suitable choice of $c$ and $t$, the diffeomorphism $F_{t}$ will carry the origin to any desired point in the open unit ball. Is that really 'clear'? Wouldn't it be necessary to show that $x(t) \rightarrow c$ when $t \rightarrow \infty $? If it is, how? Thanks in advance. Source: http://www.maths.ed.ac.uk/~aar/papers/milnortop.pdf","On page $23$ Milnor states: Let $\varphi$ : $\mathbb{R}^n \rightarrow \mathbb{R}$ be a smooth function which satisfies $$\begin{cases} \varphi(x) > 0, & {\rm for}\,\|x\| < 1 \\ \varphi(x) = 0, &{\rm for}\,\|x\| \geq 1\end{cases}$$   Given any fixed unit vector $c \in \mathbb{S} ^{n-1} $, consider the differential equations    $$\frac{dx_i}{dt} = c_{i} \cdot \varphi(x_1,x_2,\dots,x_n);  \hspace{10 mm} i = 1, \dots , n.$$   For any $ \hat x \in \mathbb{R}^n$ these equations have a unique solution $x = x(t)$, defined for all real numbers which satisfies the initial condition    $$x(0)= \hat x.$$   We will use the notation $x(t) = F_{t}( \hat x)$ for this solution. Then clearly 1) $F_{t}$ is defined for all $t$ and $ \hat x $ and depends smoothly on $t$ and $ \hat x $, 2) $F_{0}( \hat x ) =  \hat x $, 3) $F_{s+t}( \hat x ) = F_{s} \circ F_{t}( \hat x )$. 2) is quite clear, however I don't understand why 1) and 3) are valid. Could anybody explain that? Moreover on page $24$ Milnor states that clearly, with suitable choice of $c$ and $t$, the diffeomorphism $F_{t}$ will carry the origin to any desired point in the open unit ball. Is that really 'clear'? Wouldn't it be necessary to show that $x(t) \rightarrow c$ when $t \rightarrow \infty $? If it is, how? Thanks in advance. Source: http://www.maths.ed.ac.uk/~aar/papers/milnortop.pdf",,"['ordinary-differential-equations', 'differential-geometry']"
54,Check my general solution to the differential equation?,Check my general solution to the differential equation?,,"Given differential equation: $$y' = \frac{(2xy^{3}+4x)}{(x^{2}y^{2}+y^{2})}$$ This is the general solution that I got for the above differential equation: $$\frac{1}{3} \ln{\lvert y^3+2\rvert}=\ln{\lvert x^2+1\rvert}+\ln C$$ Please check if it is correct? And for the solution satisfying y(1)=0, this is what I got: $$(y^3+2)^{\frac{1}{3}}=(x^2+1)+C$$","Given differential equation: $$y' = \frac{(2xy^{3}+4x)}{(x^{2}y^{2}+y^{2})}$$ This is the general solution that I got for the above differential equation: $$\frac{1}{3} \ln{\lvert y^3+2\rvert}=\ln{\lvert x^2+1\rvert}+\ln C$$ Please check if it is correct? And for the solution satisfying y(1)=0, this is what I got: $$(y^3+2)^{\frac{1}{3}}=(x^2+1)+C$$",,"['calculus', 'linear-algebra', 'ordinary-differential-equations']"
55,How does one find the Laplace transform for the product of the Dirac delta function and a continuous function?,How does one find the Laplace transform for the product of the Dirac delta function and a continuous function?,,"As an example, what is the Laplace transform for the following: $$g(t)=\delta(t-2\pi) cos t$$ I've worked through a few examples that required finding $\mathcal{L}\{\delta(t-t_0)\}=e^{-st_0}$, but I'm completely stumped when it comes to finding $\mathcal{L}\{\delta(t-t_0)f({t})\}$.  Is there a general formula for such a case?","As an example, what is the Laplace transform for the following: $$g(t)=\delta(t-2\pi) cos t$$ I've worked through a few examples that required finding $\mathcal{L}\{\delta(t-t_0)\}=e^{-st_0}$, but I'm completely stumped when it comes to finding $\mathcal{L}\{\delta(t-t_0)f({t})\}$.  Is there a general formula for such a case?",,"['ordinary-differential-equations', 'laplace-transform', 'dirac-delta']"
56,Lotka-Volterra model with two predators,Lotka-Volterra model with two predators,,"In this, Lotka-Volterra model, we have two predators: $$\frac{dp}{dt} = ap\left(1-\frac{p}{K}\right) - (b_1q_1+b_2q_2)p$$   $$\frac{dq_1}{dt}=e_1b_1pq_1-m_1q_1$$   $$\frac{dq_2}{dt}=e_2b_2pq_2-m_2q_2.$$ Where $p$ is the prey, $q_1$ is the first predator and $q_2$ is the second predator. Also, $bpq$ is the interaction rate between the species, $m$ is the mortality rate of the predators, $K$ is the carrying capacitance. We also have that $a=0.2, \ K = 1.7, \ b_1 = 0.1, \ b_2 = 0.2, \ m_1=m_2=0.1, \ e_1 = 1.0,$ and $e_2 =2.0.$ We also, have $p(0) = q_2(0) = 1.7$ and $q_1(0) =1.0$. I solved this system using Euler's method. I noticed that $\text{predator}_1$ vanishes with time thus two predators cannot co-exist in this model. My question is, how do I vary the characteristics of the second predator in order to see if there is an equilibrium? Also, what do I vary, in this system, in order to see if there is chaos between the two species?","In this, Lotka-Volterra model, we have two predators: $$\frac{dp}{dt} = ap\left(1-\frac{p}{K}\right) - (b_1q_1+b_2q_2)p$$   $$\frac{dq_1}{dt}=e_1b_1pq_1-m_1q_1$$   $$\frac{dq_2}{dt}=e_2b_2pq_2-m_2q_2.$$ Where $p$ is the prey, $q_1$ is the first predator and $q_2$ is the second predator. Also, $bpq$ is the interaction rate between the species, $m$ is the mortality rate of the predators, $K$ is the carrying capacitance. We also have that $a=0.2, \ K = 1.7, \ b_1 = 0.1, \ b_2 = 0.2, \ m_1=m_2=0.1, \ e_1 = 1.0,$ and $e_2 =2.0.$ We also, have $p(0) = q_2(0) = 1.7$ and $q_1(0) =1.0$. I solved this system using Euler's method. I noticed that $\text{predator}_1$ vanishes with time thus two predators cannot co-exist in this model. My question is, how do I vary the characteristics of the second predator in order to see if there is an equilibrium? Also, what do I vary, in this system, in order to see if there is chaos between the two species?",,"['ordinary-differential-equations', 'numerical-methods', 'dynamical-systems']"
57,Energy Method to show uniqueness of solution of PDE,Energy Method to show uniqueness of solution of PDE,,"In my notes there is the following example about the energy method. $$u_{tt}(x, t)-u_{xxtt}(x, t)-u_{xx}(x, t)=0, 0<x<1, t>0 \\ u(x, 0)=0 \\ u_t(x, 0)=0 \\ u_x(0, t)=0 \\ u_x(1, t)=0$$ $$\int_0^1(u_tu_{tt}-u_tu_{xxtt}-u_tu_{xx})dx=0 \tag 1$$ $$\int_0^1 u_tu_{tt}dx=\int_0^1\frac{1}{2}(u_t^2)_tdx=\frac{d}{dt}\int_0^1 \frac{1}{2}u_t^2dx$$ $$\int_0^1 u_t u_{xxtt}dx=-\int_0^1 u_{tx}u_{xtt}dx+[u_t u_{xtt}]_0^1=-\int_0^1\frac{1}{2}(u_{tx}^2)_tdx$$ $$\int_0^1 u_t u_{xx}dx=-\int_0^1 u_{tx}u_x dx+[u_t u_x]_0^1=-\frac{1}{2} \frac{d}{dt} \int_0^1 u_x^2dx$$ $$(1) \Rightarrow \frac{d}{dt}\int_0^1 \frac{1}{2}u_t^2dx+\frac{d}{dt}\frac{1}{2}\int_0^1 u_{tx}^2dx+\frac{d}{dt} \frac{1}{2} \int_0^1 u_x^2dx=0$$ The energy of the system is $$E(t)=\frac{1}{2}\int_0^1 (u_t^2(x, t)+u_{tx}^2(x, t)+u_{x}^2(x, t))dx$$ I wanted to apply this at the following : $$w_{tt}(x, t)-w_{xxt}(x, t)-w_{xx}(x, t)=0, 0<x<1, t>0 \\ w(x, 0)=0 \\ w_t(x, 0)=0 \\ w(0, t)=0 \\ w(1, t)=0$$ $$\int_0^1(w_tw_{tt}-w_tw_{xxt}-w_tw_{xx})dx=0 \tag 1$$ $$\int_0^1 w_tw_{tt}dx=\int_0^1 \frac{d}{dt}\left (\frac{1}{2}w_t^2\right )dx=\frac{d}{dt}\int_0^1 \frac{1}{2}w_t^2dx$$ $$\int_0^1 w_t w_{xxt}dx=-\int_0^1 w_{tx}\frac{d}{dx}[w_{xt}]dx=[w_t w_{xt}]_0^1-\int_0^1 w_{xt}^2dx=-\int_0^1 w_{xt}^2dx$$ $$\int_0^1 w_t w_{xx}dx=\int_0^1 w_t \frac{d}{dx}[w_x]dx=[w_t w_x]_0^1-\int_0^1 w_{tx}w_xdx=-\int_0^1 w_{tx} w_x dx=-\int_0^1 \frac{d}{dt} \left (\frac{1}{2} w_x^2\right )dx=-\frac{1}{2} \frac{d}{dt} \int_0^1 w_x^2 dx$$ How could we find of the method in this case?? EDIT1 : When we have the problem $$v_{tt}(x, t)-v_{xt}(x, t)=0, x \in \mathbb{R}, t>0 \\ v(x, 0)=0, x \in \mathbb{R} \\ v_t(x, 0)=0, x \in \mathbb{R}$$ which are the limits of the integral?? In this case $x \in \mathbb{R}$, do we have to take the integral on $\mathbb{R}$ ?? EDIT2 : $$v_{tt}-v_{xt}=f $$ The characteristic curves are $$x=x_0 \text{ AND } x+t=x_0+t_0$$ $$\int_{x_0}^{x_0+t_0-t}(v_tv_{tt}-v_tv_{xt})dx=0$$ $$\int_{x_0}^{x_0+t_0-t}v_tv_{tt}dx=\int_{x_0}^{x_0+t_0-t}\frac{\partial}{\partial{t}}\left (\frac{1}{2}v_t^2\right )dx=\frac{d}{dt}\int_{x_0}^{x_0+t_0-t}\frac{1}{2}v_t^2dx$$ $$\int_{x_0}^{x_0+t_0-t}v_tv_{xt}dx=\int_{x_0}^{x_0+t_0-t}v_t\left [\frac{\partial}{\partial{x}}v_t\right ]dx=[v_t^2]_{x=x_0}^{x_0+t_0-t}-\int_{x_0}^{x_0+t_0-t}v_{xt}v_tdx \Rightarrow \int_{x_0}^{x_0+t_0-t}v_tv_{xt}dx=\frac{1}{2}\left (v_t^2(x_0+t_0-t, t)-v_t^2(x_0, t)\right )$$ Is this correct so far?? How could we find the energy??","In my notes there is the following example about the energy method. $$u_{tt}(x, t)-u_{xxtt}(x, t)-u_{xx}(x, t)=0, 0<x<1, t>0 \\ u(x, 0)=0 \\ u_t(x, 0)=0 \\ u_x(0, t)=0 \\ u_x(1, t)=0$$ $$\int_0^1(u_tu_{tt}-u_tu_{xxtt}-u_tu_{xx})dx=0 \tag 1$$ $$\int_0^1 u_tu_{tt}dx=\int_0^1\frac{1}{2}(u_t^2)_tdx=\frac{d}{dt}\int_0^1 \frac{1}{2}u_t^2dx$$ $$\int_0^1 u_t u_{xxtt}dx=-\int_0^1 u_{tx}u_{xtt}dx+[u_t u_{xtt}]_0^1=-\int_0^1\frac{1}{2}(u_{tx}^2)_tdx$$ $$\int_0^1 u_t u_{xx}dx=-\int_0^1 u_{tx}u_x dx+[u_t u_x]_0^1=-\frac{1}{2} \frac{d}{dt} \int_0^1 u_x^2dx$$ $$(1) \Rightarrow \frac{d}{dt}\int_0^1 \frac{1}{2}u_t^2dx+\frac{d}{dt}\frac{1}{2}\int_0^1 u_{tx}^2dx+\frac{d}{dt} \frac{1}{2} \int_0^1 u_x^2dx=0$$ The energy of the system is $$E(t)=\frac{1}{2}\int_0^1 (u_t^2(x, t)+u_{tx}^2(x, t)+u_{x}^2(x, t))dx$$ I wanted to apply this at the following : $$w_{tt}(x, t)-w_{xxt}(x, t)-w_{xx}(x, t)=0, 0<x<1, t>0 \\ w(x, 0)=0 \\ w_t(x, 0)=0 \\ w(0, t)=0 \\ w(1, t)=0$$ $$\int_0^1(w_tw_{tt}-w_tw_{xxt}-w_tw_{xx})dx=0 \tag 1$$ $$\int_0^1 w_tw_{tt}dx=\int_0^1 \frac{d}{dt}\left (\frac{1}{2}w_t^2\right )dx=\frac{d}{dt}\int_0^1 \frac{1}{2}w_t^2dx$$ $$\int_0^1 w_t w_{xxt}dx=-\int_0^1 w_{tx}\frac{d}{dx}[w_{xt}]dx=[w_t w_{xt}]_0^1-\int_0^1 w_{xt}^2dx=-\int_0^1 w_{xt}^2dx$$ $$\int_0^1 w_t w_{xx}dx=\int_0^1 w_t \frac{d}{dx}[w_x]dx=[w_t w_x]_0^1-\int_0^1 w_{tx}w_xdx=-\int_0^1 w_{tx} w_x dx=-\int_0^1 \frac{d}{dt} \left (\frac{1}{2} w_x^2\right )dx=-\frac{1}{2} \frac{d}{dt} \int_0^1 w_x^2 dx$$ How could we find of the method in this case?? EDIT1 : When we have the problem $$v_{tt}(x, t)-v_{xt}(x, t)=0, x \in \mathbb{R}, t>0 \\ v(x, 0)=0, x \in \mathbb{R} \\ v_t(x, 0)=0, x \in \mathbb{R}$$ which are the limits of the integral?? In this case $x \in \mathbb{R}$, do we have to take the integral on $\mathbb{R}$ ?? EDIT2 : $$v_{tt}-v_{xt}=f $$ The characteristic curves are $$x=x_0 \text{ AND } x+t=x_0+t_0$$ $$\int_{x_0}^{x_0+t_0-t}(v_tv_{tt}-v_tv_{xt})dx=0$$ $$\int_{x_0}^{x_0+t_0-t}v_tv_{tt}dx=\int_{x_0}^{x_0+t_0-t}\frac{\partial}{\partial{t}}\left (\frac{1}{2}v_t^2\right )dx=\frac{d}{dt}\int_{x_0}^{x_0+t_0-t}\frac{1}{2}v_t^2dx$$ $$\int_{x_0}^{x_0+t_0-t}v_tv_{xt}dx=\int_{x_0}^{x_0+t_0-t}v_t\left [\frac{\partial}{\partial{x}}v_t\right ]dx=[v_t^2]_{x=x_0}^{x_0+t_0-t}-\int_{x_0}^{x_0+t_0-t}v_{xt}v_tdx \Rightarrow \int_{x_0}^{x_0+t_0-t}v_tv_{xt}dx=\frac{1}{2}\left (v_t^2(x_0+t_0-t, t)-v_t^2(x_0, t)\right )$$ Is this correct so far?? How could we find the energy??",,"['ordinary-differential-equations', 'partial-differential-equations']"
58,First order differential equation with non constant coefficients,First order differential equation with non constant coefficients,,"I have the following system : $$\begin{cases}(t^2+1)x'(t)=tx+y+2t^2+1\\(t^2+1)y'(t)=-x+ty+3t\end{cases}$$ How can it be solved ? What I have tried so far : polynomials of the first, second degree as solutions - didn't work One can notice that if we use $X=\begin{bmatrix}x\\y\end{bmatrix},A=\begin{bmatrix}t&1\\-1&t\end{bmatrix},B=\begin{bmatrix}2t^2-1\\3t\end{bmatrix}$ then the system becomes $(t^2+1)X'=AX+B$, and $t^2+1=\det A$. I'm pretty sure that this last result is supposed to help, but I haven't been able to find a way to use it.","I have the following system : $$\begin{cases}(t^2+1)x'(t)=tx+y+2t^2+1\\(t^2+1)y'(t)=-x+ty+3t\end{cases}$$ How can it be solved ? What I have tried so far : polynomials of the first, second degree as solutions - didn't work One can notice that if we use $X=\begin{bmatrix}x\\y\end{bmatrix},A=\begin{bmatrix}t&1\\-1&t\end{bmatrix},B=\begin{bmatrix}2t^2-1\\3t\end{bmatrix}$ then the system becomes $(t^2+1)X'=AX+B$, and $t^2+1=\det A$. I'm pretty sure that this last result is supposed to help, but I haven't been able to find a way to use it.",,"['calculus', 'ordinary-differential-equations']"
59,Picard iteration for a system,Picard iteration for a system,,"I have the following system: $$y'(t)=x^2(t)-x(t)$$ $$x'(t)=y(t)$$ It comes from the second order ode $$x''(t)=x^2(t)'x(t)$$ I am asked to do the first four Picard iterations starting from the solution  $$\phi_0 (t)= \bigg(\frac{-1}{2},0 \bigg)$$ I can do Picard iterations for a simple first order ode, but I am not able to generalize it to a system where the two equations depend on each other, and I cant find any examples or theory that tells the algorithm to help me in this case.","I have the following system: $$y'(t)=x^2(t)-x(t)$$ $$x'(t)=y(t)$$ It comes from the second order ode $$x''(t)=x^2(t)'x(t)$$ I am asked to do the first four Picard iterations starting from the solution  $$\phi_0 (t)= \bigg(\frac{-1}{2},0 \bigg)$$ I can do Picard iterations for a simple first order ode, but I am not able to generalize it to a system where the two equations depend on each other, and I cant find any examples or theory that tells the algorithm to help me in this case.",,['ordinary-differential-equations']
60,Differential equation of 2nd order,Differential equation of 2nd order,,"I have a differential equation $x''(t)=x(t)^2-x(t)$. The exercise is as follows: Let $x'(0)=0$. Then the solution $x(t)$ only depends on the initial position $x(0)$.  Show that there is exactly one value of $x(0)$ for which the solution $x(t)$ is non-constant, yet tens to a finite value as $t$ tends to infinity. Calculate this $x(0)$, as well as the limiting value $x(+\infty)$. What happens to $x(t)$ as $t$ goes to infinity for other values of $x(0)$? This is the question. My intention was to simply solve the reduced system \begin{align*} x'(t)&=y(t)\\ y'(t)&=x(t)^2-x(t) \end{align*} with barrows formula. But I'm not sure how to do that when the equations depend on each other. Any help on how to do this or maybe if there is an easier approach?","I have a differential equation $x''(t)=x(t)^2-x(t)$. The exercise is as follows: Let $x'(0)=0$. Then the solution $x(t)$ only depends on the initial position $x(0)$.  Show that there is exactly one value of $x(0)$ for which the solution $x(t)$ is non-constant, yet tens to a finite value as $t$ tends to infinity. Calculate this $x(0)$, as well as the limiting value $x(+\infty)$. What happens to $x(t)$ as $t$ goes to infinity for other values of $x(0)$? This is the question. My intention was to simply solve the reduced system \begin{align*} x'(t)&=y(t)\\ y'(t)&=x(t)^2-x(t) \end{align*} with barrows formula. But I'm not sure how to do that when the equations depend on each other. Any help on how to do this or maybe if there is an easier approach?",,['ordinary-differential-equations']
61,Solve $x^2 y''+(-2x-x^3)y'+5y=0$,Solve,x^2 y''+(-2x-x^3)y'+5y=0,"Ok so for me I am having trouble solving this equation to get $y=C_1y_1+C_2y_2$, but I'm having trouble dealing with the (-2x-x^3) part.  Usually I would isolate $y''$, then make $y=x^m$, then go from there, but I'm having trouble dealing with the $(-2x-x^3)$ part.  Thanks! When I tried that approach, I got $[x^{3/2}x^{x^2/2}x^{(\sqrt{x^2+6x-11})/2}, x^{3/2}x^{x^2/2}x^{-(\sqrt{x^2+6x-11})/2}]$ form a basis, but it just seems too complicated when I need to solve $y_1(1)=-4, y'_1(1)=5, y_2(1)=-5,y'_2(1)=5$ The point is to find the Wronskian, $w(x)=y_1(x)y'_2(x)-y_2(x)y'_1(x)$ for x>0","Ok so for me I am having trouble solving this equation to get $y=C_1y_1+C_2y_2$, but I'm having trouble dealing with the (-2x-x^3) part.  Usually I would isolate $y''$, then make $y=x^m$, then go from there, but I'm having trouble dealing with the $(-2x-x^3)$ part.  Thanks! When I tried that approach, I got $[x^{3/2}x^{x^2/2}x^{(\sqrt{x^2+6x-11})/2}, x^{3/2}x^{x^2/2}x^{-(\sqrt{x^2+6x-11})/2}]$ form a basis, but it just seems too complicated when I need to solve $y_1(1)=-4, y'_1(1)=5, y_2(1)=-5,y'_2(1)=5$ The point is to find the Wronskian, $w(x)=y_1(x)y'_2(x)-y_2(x)y'_1(x)$ for x>0",,"['calculus', 'ordinary-differential-equations']"
62,"Given the differential equation $\frac{dy}{dt}+a(t)y=f(t)$ , show that every solution tends to 0 as t approaches infinity.","Given the differential equation  , show that every solution tends to 0 as t approaches infinity.",\frac{dy}{dt}+a(t)y=f(t),Given the differential equation $\frac{dy}{dt}+a(t)y=f(t)$ with a(t) and f(t) continuous for: $-\infty<t<\infty$ $a(t) \ge c>0$ $lim_{t_->0}f(t)=0$ Show that every solution tends to 0 as t approaches infinity. $$\frac{dy}{dt}+a(t)y=f(t)$$ $$μ(t)=e^{\int a(t)dt}$$ $$μ(t)=e^{\frac{1}{2}a^2(t)}$$ Multiplying both sides of the equation by $μ(t)$: $$\frac{dy}{dt}+a(t)y=f(t)$$  $$μ(t)[\frac{dy}{dt}+a(t)y]=μ(t)[f(t)]$$  $$e^{\frac{1}{2}a^2(t)}[\frac{dy}{dt}+a(t)y]=e^{\frac{1}{2}a^2(t)}[f(t)]$$  $$\frac{d}{dt}e^{\frac{1}{2}a^2(t)}y=e^{\frac{1}{2}a^2(t)}[f(t)]$$  $$e^{\frac{1}{2}a^2(t)}y=\int e^{\frac{1}{2}a^2(t)}[f(t)]dt$$ Did I make a mistake in the integrating factor step? I am not sure how to proceed..,Given the differential equation $\frac{dy}{dt}+a(t)y=f(t)$ with a(t) and f(t) continuous for: $-\infty<t<\infty$ $a(t) \ge c>0$ $lim_{t_->0}f(t)=0$ Show that every solution tends to 0 as t approaches infinity. $$\frac{dy}{dt}+a(t)y=f(t)$$ $$μ(t)=e^{\int a(t)dt}$$ $$μ(t)=e^{\frac{1}{2}a^2(t)}$$ Multiplying both sides of the equation by $μ(t)$: $$\frac{dy}{dt}+a(t)y=f(t)$$  $$μ(t)[\frac{dy}{dt}+a(t)y]=μ(t)[f(t)]$$  $$e^{\frac{1}{2}a^2(t)}[\frac{dy}{dt}+a(t)y]=e^{\frac{1}{2}a^2(t)}[f(t)]$$  $$\frac{d}{dt}e^{\frac{1}{2}a^2(t)}y=e^{\frac{1}{2}a^2(t)}[f(t)]$$  $$e^{\frac{1}{2}a^2(t)}y=\int e^{\frac{1}{2}a^2(t)}[f(t)]dt$$ Did I make a mistake in the integrating factor step? I am not sure how to proceed..,,['ordinary-differential-equations']
63,Second Order Differential Equation inhomogeneous,Second Order Differential Equation inhomogeneous,,$$y''(s) - \frac{s^2}{c^2}y(s) - \frac{g}{s\cdot c^2}= 0$$ I am getting confused with what to do with the $$\frac{-g}{s\cdot c^2}$$ part,$$y''(s) - \frac{s^2}{c^2}y(s) - \frac{g}{s\cdot c^2}= 0$$ I am getting confused with what to do with the $$\frac{-g}{s\cdot c^2}$$ part,,['ordinary-differential-equations']
64,periodic solution of $x''-\ (1-\ x^2-\ (x')^2)\ x'+x=0$,periodic solution of,x''-\ (1-\ x^2-\ (x')^2)\ x'+x=0,Assume differential equation $$x''-\ (1-\ x^2-\ (x')^2)\ x'+x=0$$ I want to discusse about non-constant periodic solution of it. Can someone give a hint that how to start to think. And does it have periodic solution. My tries: I changed it to system of differential eqution below$$ x_1'=x_2$$ $$x_2'=-x_1+x_2-(x_1^2+x_2^2)x_2$$ I know that if it has perodic solution there exist $t_1$ and $t_2$ such that $x_1(t_1)=0$ and $x_2(t_2)=0$,Assume differential equation $$x''-\ (1-\ x^2-\ (x')^2)\ x'+x=0$$ I want to discusse about non-constant periodic solution of it. Can someone give a hint that how to start to think. And does it have periodic solution. My tries: I changed it to system of differential eqution below$$ x_1'=x_2$$ $$x_2'=-x_1+x_2-(x_1^2+x_2^2)x_2$$ I know that if it has perodic solution there exist $t_1$ and $t_2$ such that $x_1(t_1)=0$ and $x_2(t_2)=0$,,"['analysis', 'ordinary-differential-equations', 'dynamical-systems', 'systems-of-equations']"
65,a question regarding wronskian,a question regarding wronskian,,"I was working on following problem: Let $y_1$ and $y_2$ be solutions of $$x^2y'' + y' + (\sin x)y = 0$$ satisfying $$y_1(0) = 0, y_1'(0)=1,y_2(0) = 1, y_2'(0)=0 $$ . I worked like following: since wronskian $W$ is given as $$W = ce^{-\int-\frac{1}{x^2}\,dx}~,$$ wronskian is not zero except the point $x=0$ . Thus, The two given solutions are L.I. and hence $y_1$ and $y_2$ do not have common zeroes. Am I right in concluding this? Kindly rectify if somehwere I went wrong. Thanks for giving time.","I was working on following problem: Let and be solutions of satisfying . I worked like following: since wronskian is given as wronskian is not zero except the point . Thus, The two given solutions are L.I. and hence and do not have common zeroes. Am I right in concluding this? Kindly rectify if somehwere I went wrong. Thanks for giving time.","y_1 y_2 x^2y'' + y' + (\sin x)y = 0 y_1(0) = 0, y_1'(0)=1,y_2(0) = 1, y_2'(0)=0  W W = ce^{-\int-\frac{1}{x^2}\,dx}~, x=0 y_1 y_2","['linear-algebra', 'ordinary-differential-equations', 'wronskian']"
66,Can one apply a WKB method to an inhomogeneous first order differential equation in order to find the asymptotic expansion of the solution?,Can one apply a WKB method to an inhomogeneous first order differential equation in order to find the asymptotic expansion of the solution?,,"Consider \begin{equation} \varepsilon \frac{dy}{dx} = Q(x)y + R(x) \end{equation} where $\varepsilon$ is a small parameter.  Can one apply a WKB method to find an asymptotic expansion for the solution? I expect to obtain \begin{equation} y(x) \sim - \sum_{n = 0}^\infty \left(\frac{\varepsilon}{Q(x)} \frac{d}{dx}\right)^n \frac{R(x)}{Q(x)} \end{equation} (I am aware that this is a divergent sum), but I have been unsuccessful in recovering this form by using WKB methods.","Consider \begin{equation} \varepsilon \frac{dy}{dx} = Q(x)y + R(x) \end{equation} where $\varepsilon$ is a small parameter.  Can one apply a WKB method to find an asymptotic expansion for the solution? I expect to obtain \begin{equation} y(x) \sim - \sum_{n = 0}^\infty \left(\frac{\varepsilon}{Q(x)} \frac{d}{dx}\right)^n \frac{R(x)}{Q(x)} \end{equation} (I am aware that this is a divergent sum), but I have been unsuccessful in recovering this form by using WKB methods.",,"['ordinary-differential-equations', 'asymptotics', 'approximation']"
67,Solution to non-autonomous system of ODEs,Solution to non-autonomous system of ODEs,,"I have the following system of ordinary differential equations with $\gamma(t),M(t),$ and $\beta(t)$ strictly positive and smooth. $\dot{g_1}(t) = \gamma(t)M(t){g_2}(t)$ $\dot{g_2}(t) = \beta(t)\frac{1}{M(t)}{g_1}(t)$ with initial conditions ${g_1}(0)=1$ ${g_2}(0)=1$ $0\le t \le 1$ How would I go about finding a non-trivial solution? Editing to add that $\gamma(t), M(t),$ and $\beta(t)$ are finite on $[0,1]$ 2nd edit: Corrected typo in domain of $t$","I have the following system of ordinary differential equations with $\gamma(t),M(t),$ and $\beta(t)$ strictly positive and smooth. $\dot{g_1}(t) = \gamma(t)M(t){g_2}(t)$ $\dot{g_2}(t) = \beta(t)\frac{1}{M(t)}{g_1}(t)$ with initial conditions ${g_1}(0)=1$ ${g_2}(0)=1$ $0\le t \le 1$ How would I go about finding a non-trivial solution? Editing to add that $\gamma(t), M(t),$ and $\beta(t)$ are finite on $[0,1]$ 2nd edit: Corrected typo in domain of $t$",,['ordinary-differential-equations']
68,Differential equation separate the variables,Differential equation separate the variables,,"I have to find all solutions to the following differential equation: $$\frac{\text{d}x}{\text{d}t} = t \sin(x).$$ I know I can use the technique of separation by variables, but I'm having some trouble. This is where I am so far: Let $g(x) = \sin(x)$ and $h(t) = t$. Then $\frac{\text{d}x}{\text{d}t} = g(x)h(t)$. Let $$G(x) = \int_{x_0}^x \frac{1}{\sin(x)} \text{d} x \qquad \text{and}\qquad H(t) = \int_{t_0}^t t \,\text{d}t.$$ Then, $$G(x) = \int_{x_0}^x \csc(x) \,\text{d}x = -\ln |\csc(x) + \cot(x)| \Big|_{x_0}^x \qquad \text{and} \qquad H(t) = \frac{t^2 - t_0^2}{2}.$$ I know that now $x(t) = G^{-1}(H(t))$, but I have no idea how to compute $G^{-1}$, since the expression for $G$ seems complicated, which makes me think I'm on the wrong track with this problem.","I have to find all solutions to the following differential equation: $$\frac{\text{d}x}{\text{d}t} = t \sin(x).$$ I know I can use the technique of separation by variables, but I'm having some trouble. This is where I am so far: Let $g(x) = \sin(x)$ and $h(t) = t$. Then $\frac{\text{d}x}{\text{d}t} = g(x)h(t)$. Let $$G(x) = \int_{x_0}^x \frac{1}{\sin(x)} \text{d} x \qquad \text{and}\qquad H(t) = \int_{t_0}^t t \,\text{d}t.$$ Then, $$G(x) = \int_{x_0}^x \csc(x) \,\text{d}x = -\ln |\csc(x) + \cot(x)| \Big|_{x_0}^x \qquad \text{and} \qquad H(t) = \frac{t^2 - t_0^2}{2}.$$ I know that now $x(t) = G^{-1}(H(t))$, but I have no idea how to compute $G^{-1}$, since the expression for $G$ seems complicated, which makes me think I'm on the wrong track with this problem.",,['real-analysis']
69,"Using $\, \,y(x_0)=y_0\,\,$ for differential equation",Using  for differential equation,"\, \,y(x_0)=y_0\,\,","I don't know what to do next, I'am trying to use the given point but as you can see tan(-pi/2) won't give me the answer that I loking for...","I don't know what to do next, I'am trying to use the given point but as you can see tan(-pi/2) won't give me the answer that I loking for...",,"['real-analysis', 'ordinary-differential-equations']"
70,Show that the solution of an initial value problem is always less than a given constant,Show that the solution of an initial value problem is always less than a given constant,,My try is that $$\frac{dy}{dt} =(y-3)e^{\cos ty}$$ $$\frac{dy}{y-3}= e^{\cos ty}dt$$ $$\ln (y-3)=-\frac{e^{\cos ty}}{\sin ty} +c$$ my steps is correct or I made mistakes ? please help to solve this problem,My try is that $$\frac{dy}{dt} =(y-3)e^{\cos ty}$$ $$\frac{dy}{y-3}= e^{\cos ty}dt$$ $$\ln (y-3)=-\frac{e^{\cos ty}}{\sin ty} +c$$ my steps is correct or I made mistakes ? please help to solve this problem,,"['calculus', 'ordinary-differential-equations', 'inequality']"
71,Second order ODE $y''+p(t)y'+q(t)y=0$,Second order ODE,y''+p(t)y'+q(t)y=0,"Let consider ordinary differential equation of the form $$t^2y''+3ty'+y=0$$ This is equivalent to $$y''+\frac{3}{t}y'+\frac{1}{t^2}y = 0$$ which looks better. But how does one find the solutions here? I guessed one of them is $y(t)=\frac{1}{t}$, but guessing shouldn't be the method here. I feel as though I needed a smart substitution. Any hints?","Let consider ordinary differential equation of the form $$t^2y''+3ty'+y=0$$ This is equivalent to $$y''+\frac{3}{t}y'+\frac{1}{t^2}y = 0$$ which looks better. But how does one find the solutions here? I guessed one of them is $y(t)=\frac{1}{t}$, but guessing shouldn't be the method here. I feel as though I needed a smart substitution. Any hints?",,['ordinary-differential-equations']
72,Is there a unique solution to this simple differential equation?,Is there a unique solution to this simple differential equation?,,"I am trying to establish uniqueness for a solution to a bigger problem, and it boils down to whether or not the following differential equation has a unique solution: $$f'(t)⋅(f(t)-t)=K$$ Clearly, one solution to this differential equation is $f(t)=t+K$. Are there other solutions to this differential equation?","I am trying to establish uniqueness for a solution to a bigger problem, and it boils down to whether or not the following differential equation has a unique solution: $$f'(t)⋅(f(t)-t)=K$$ Clearly, one solution to this differential equation is $f(t)=t+K$. Are there other solutions to this differential equation?",,['ordinary-differential-equations']
73,Changes of variables to get an Elliptic Integral of the First Kind,Changes of variables to get an Elliptic Integral of the First Kind,,I'm working with a non-linear second order ODE which has an analytical solution in terms of the Jacobi elliptical function $sn(u|k^2)$. The equation is $y''=y(\gamma - \frac{y^2}{2})$ where $\gamma$ is a constant and should be less than 0 for the specific problem I am dealing with. I want to solve the equation by hand so I am using a first integral to make it a first order non linear ODE because it does not depend on $y'$. Doing that I get: $$\frac{dy}{dx}= \pm \sqrt{y^2 \left( y-\frac{y^2}{4} \right) + C_1}$$ Where $C_1$ is an integration constant. I can separate both sides of this equation and I get the next integral: $$\int_0^y \frac{dr}{\sqrt{r^2(\gamma-\frac{r^2}{4}) + C_1}}$$ Which I know that can be transformed into an elliptic integral because that is the result I get if I introduce it into Mathematica (the elliptic integral of the first kind with $\phi=i arcsinh(\alpha y)$) where $i$ is the imaginary unit and $\alpha$ is some constant which depends upon $\gamma$ and $C_1$. What I want to know is which changes of variables should I perform to get the corresponding elliptic integral: $$\int_0^{iarcsinh(\alpha y)} \frac{dt}{\sqrt{1 - k^2 sin^2(t)}}$$ I saw this thread: Evaluating the elliptic integral $\int_{-\pi}^\pi\frac{dx}{\sqrt{(t-2\cos x)^2-4}}$ and I tried to perform some change similar to the ones that are proposed there but with no result. Thanks.,I'm working with a non-linear second order ODE which has an analytical solution in terms of the Jacobi elliptical function $sn(u|k^2)$. The equation is $y''=y(\gamma - \frac{y^2}{2})$ where $\gamma$ is a constant and should be less than 0 for the specific problem I am dealing with. I want to solve the equation by hand so I am using a first integral to make it a first order non linear ODE because it does not depend on $y'$. Doing that I get: $$\frac{dy}{dx}= \pm \sqrt{y^2 \left( y-\frac{y^2}{4} \right) + C_1}$$ Where $C_1$ is an integration constant. I can separate both sides of this equation and I get the next integral: $$\int_0^y \frac{dr}{\sqrt{r^2(\gamma-\frac{r^2}{4}) + C_1}}$$ Which I know that can be transformed into an elliptic integral because that is the result I get if I introduce it into Mathematica (the elliptic integral of the first kind with $\phi=i arcsinh(\alpha y)$) where $i$ is the imaginary unit and $\alpha$ is some constant which depends upon $\gamma$ and $C_1$. What I want to know is which changes of variables should I perform to get the corresponding elliptic integral: $$\int_0^{iarcsinh(\alpha y)} \frac{dt}{\sqrt{1 - k^2 sin^2(t)}}$$ I saw this thread: Evaluating the elliptic integral $\int_{-\pi}^\pi\frac{dx}{\sqrt{(t-2\cos x)^2-4}}$ and I tried to perform some change similar to the ones that are proposed there but with no result. Thanks.,,"['ordinary-differential-equations', 'special-functions', 'elliptic-functions', 'elliptic-integrals']"
74,Is it possible to write the curl in terms of the infinitesimal rotation tensor?,Is it possible to write the curl in terms of the infinitesimal rotation tensor?,,"Is it possible to write the curl in terms of the infinitesimal rotation tensor?  Basically, we can write the curl as a matrix operator $$    curl=\begin{bmatrix} 0 & -\partial z & \partial y\\\partial z & 0 & -\partial x\\-\partial y & \partial x & 0\end{bmatrix} $$ and we can write the infinitesimal rotation tensor as a matrix operator (modulo a 1/2 constant) $$    \nabla-\nabla^T = \begin{bmatrix}         0 & -(\partial x-\partial y) & \partial z-\partial x\\         \partial x-\partial y & 0 & -(\partial y-\partial z)\\         -(\partial z-\partial x) & \partial y-\partial z & 0     \end{bmatrix}. $$ The axial vector to the infinitesimal rotation tensor is $$     \begin{bmatrix}         \partial y-\partial z\\         \partial z-\partial x\\         \partial x-\partial y     \end{bmatrix}, $$ which looks kind of like the curl, except that this seems kind of sloppy since we have a vector with a bunch of differential operators inside of it with no clear way on how to apply it. As such, again, is there a way to write the curl in terms of the infinitesimal rotation tensor?","Is it possible to write the curl in terms of the infinitesimal rotation tensor?  Basically, we can write the curl as a matrix operator $$    curl=\begin{bmatrix} 0 & -\partial z & \partial y\\\partial z & 0 & -\partial x\\-\partial y & \partial x & 0\end{bmatrix} $$ and we can write the infinitesimal rotation tensor as a matrix operator (modulo a 1/2 constant) $$    \nabla-\nabla^T = \begin{bmatrix}         0 & -(\partial x-\partial y) & \partial z-\partial x\\         \partial x-\partial y & 0 & -(\partial y-\partial z)\\         -(\partial z-\partial x) & \partial y-\partial z & 0     \end{bmatrix}. $$ The axial vector to the infinitesimal rotation tensor is $$     \begin{bmatrix}         \partial y-\partial z\\         \partial z-\partial x\\         \partial x-\partial y     \end{bmatrix}, $$ which looks kind of like the curl, except that this seems kind of sloppy since we have a vector with a bunch of differential operators inside of it with no clear way on how to apply it. As such, again, is there a way to write the curl in terms of the infinitesimal rotation tensor?",,"['ordinary-differential-equations', 'multivariable-calculus', 'differential-geometry', 'classical-mechanics']"
75,How to prove that solution to ODE in spherical coordinate is equivalent to the ODE in cartesian coordinates if it is a thin shell,How to prove that solution to ODE in spherical coordinate is equivalent to the ODE in cartesian coordinates if it is a thin shell,,"Solving a diffusion-type ODE across a spherical shell, the equation is: $$\frac{d}{dr}\left(r^2\frac{df}{dr}\right)=0\tag{1}$$ with boundary conditions $f(r_1)=f_1$ and $f(r_2)=f_2$. The solution is: $$f(r)=\frac{f_1-f_2}{\frac{r}{r_1}-\frac{r}{r_2}}+\frac{f_1r_1-f_2r_2}{r_1-r_2}\tag{2}$$ Now I know intuitively that if the overall thickness of the shell $\Delta r=r_2-r_1\ll r_1$, then this can be reduced to Cartesian coordinates. In that case the ODE is: $$\frac{d^2f}{dr^2}=0\tag{3}$$ With the same boundary conditions, then the solution is: $$f(r)=f_1+(f_2-f_1)\frac{r-r_1}{r_2-r_1}\tag{4}$$ All fine so far.  What I want to do though is prove that Equation 2 reduces to Equation 4 when $\Delta r\ll r_1$, but I can't seem to figure out how it should be done. My approach has been to substitute $r_1 + \Delta r$ for $r_2$ and substitute $r_1+\xi$ for $r$, and then use the geometric series $\frac{1}{1-x}\approx 1+x$ (for $x\ll 1$) type of substitution to get $r$ up into the numerator, but the terms don't cancel and I can't get it to work out. Is there another approach I should be using?  Some trick or technique I'm not remembering?","Solving a diffusion-type ODE across a spherical shell, the equation is: $$\frac{d}{dr}\left(r^2\frac{df}{dr}\right)=0\tag{1}$$ with boundary conditions $f(r_1)=f_1$ and $f(r_2)=f_2$. The solution is: $$f(r)=\frac{f_1-f_2}{\frac{r}{r_1}-\frac{r}{r_2}}+\frac{f_1r_1-f_2r_2}{r_1-r_2}\tag{2}$$ Now I know intuitively that if the overall thickness of the shell $\Delta r=r_2-r_1\ll r_1$, then this can be reduced to Cartesian coordinates. In that case the ODE is: $$\frac{d^2f}{dr^2}=0\tag{3}$$ With the same boundary conditions, then the solution is: $$f(r)=f_1+(f_2-f_1)\frac{r-r_1}{r_2-r_1}\tag{4}$$ All fine so far.  What I want to do though is prove that Equation 2 reduces to Equation 4 when $\Delta r\ll r_1$, but I can't seem to figure out how it should be done. My approach has been to substitute $r_1 + \Delta r$ for $r_2$ and substitute $r_1+\xi$ for $r$, and then use the geometric series $\frac{1}{1-x}\approx 1+x$ (for $x\ll 1$) type of substitution to get $r$ up into the numerator, but the terms don't cancel and I can't get it to work out. Is there another approach I should be using?  Some trick or technique I'm not remembering?",,"['calculus', 'ordinary-differential-equations', 'spherical-coordinates']"
76,Transforming equations of the form $ax''+b(x^2-1)x'+cx=0$ into van der Pol equations,Transforming equations of the form  into van der Pol equations,ax''+b(x^2-1)x'+cx=0,"Show that every equation of the form $$ax'' + b(x^2 - 1) x' + cx = 0$$ where $a, b, c > 0$ can be transformed into a van der Pol equation by a change in the independent variable. I am unable to find this replacement. If anyone could help me or give a hint I would be grateful.","Show that every equation of the form $$ax'' + b(x^2 - 1) x' + cx = 0$$ where $a, b, c > 0$ can be transformed into a van der Pol equation by a change in the independent variable. I am unable to find this replacement. If anyone could help me or give a hint I would be grateful.",,"['ordinary-differential-equations', 'dynamical-systems']"
77,Motivation and Derivation of the Riccati Equation Transformation,Motivation and Derivation of the Riccati Equation Transformation,,Given a Riccati Equation which is differential equation of the form: $$ \frac{dy}{dx} = a_0 (x) + a_1 (x)y + a_2 (x)y^2  $$ It is well known that the transformation: $$ y = -\frac{1}{a_2(x)} \frac{\frac{du}{dx}}{u} $$ Can be substituted into this equation to transform it into a second order linear differential equation (which after simplifying the algebra is) $$ a_0(x)a_2(x) u - (a_1(x)a_2(x) + 1) \frac{du}{dx} + a_2(x)\frac{d^2u}{du^2} = 0 $$ My question is: How does one derive this transformation from scratch. Since although it is easy to prove that the transformation works it does not reveal how it was found.,Given a Riccati Equation which is differential equation of the form: $$ \frac{dy}{dx} = a_0 (x) + a_1 (x)y + a_2 (x)y^2  $$ It is well known that the transformation: $$ y = -\frac{1}{a_2(x)} \frac{\frac{du}{dx}}{u} $$ Can be substituted into this equation to transform it into a second order linear differential equation (which after simplifying the algebra is) $$ a_0(x)a_2(x) u - (a_1(x)a_2(x) + 1) \frac{du}{dx} + a_2(x)\frac{d^2u}{du^2} = 0 $$ My question is: How does one derive this transformation from scratch. Since although it is easy to prove that the transformation works it does not reveal how it was found.,,"['calculus', 'analysis', 'ordinary-differential-equations', 'derivatives', 'problem-solving']"
78,derivative after changing variable,derivative after changing variable,,"I have just studied a lesson about derivative of a function but I still confuse in the following case. Suppose that I have a function: $$ f(x) = 2x^2 + 3x + 1$$  and I want to calculate $\frac{d}{dx}f(x)$ and it is $$ \frac{d}{dx}f(x) = 4x + 3$$ I try to change the variable $x$ such that $x=e^u$, and then $$f(e^u) = 2e^{2u} + 3e^u + 1\quad\quad\quad (1)$$ and I have  $$\frac{d}{du}f(e^u) = 4e^{2u}+3e^u = e^u(4e^u + 3)=\frac{dx}{du}\frac{df}{dx}\quad\quad\quad (2)$$ But I just want to take the derivative by $x$, not by $u$, that means $$\frac{d}{dx}f(e^u) = 4e^u + 3 \quad\quad\quad (3)$$ So, I confuse between $(2)$ and $(3)$. Which is correct? Because the transformation is needed for a calculation in next step (using variable $u$) of my exercise. Maybe my question is stupid but I appreciate if anyone can explain clearly the difference in two ways of taking derivative by $x$ and $u$.","I have just studied a lesson about derivative of a function but I still confuse in the following case. Suppose that I have a function: $$ f(x) = 2x^2 + 3x + 1$$  and I want to calculate $\frac{d}{dx}f(x)$ and it is $$ \frac{d}{dx}f(x) = 4x + 3$$ I try to change the variable $x$ such that $x=e^u$, and then $$f(e^u) = 2e^{2u} + 3e^u + 1\quad\quad\quad (1)$$ and I have  $$\frac{d}{du}f(e^u) = 4e^{2u}+3e^u = e^u(4e^u + 3)=\frac{dx}{du}\frac{df}{dx}\quad\quad\quad (2)$$ But I just want to take the derivative by $x$, not by $u$, that means $$\frac{d}{dx}f(e^u) = 4e^u + 3 \quad\quad\quad (3)$$ So, I confuse between $(2)$ and $(3)$. Which is correct? Because the transformation is needed for a calculation in next step (using variable $u$) of my exercise. Maybe my question is stupid but I appreciate if anyone can explain clearly the difference in two ways of taking derivative by $x$ and $u$.",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
79,Solving an ODE studying the asympotic behaviour,Solving an ODE studying the asympotic behaviour,,"I've seen that in some cases, one tries to study the behaviour of ODE at infinity and at zero, in order to find simplify the equation. For example: $$y''=\left(1-\frac{a}{x}+\frac{b(b+1)}{x^2}\right)y$$ In the limit $x\to\infty$: $$y''\approx y \implies y(x)=Ae^{x}+Be^{-x}$$ In the limit $x\to 0$: $$y''\approx \frac{b(b+1)}{x^2}y \implies y(x)=Cx^{b+1}+Dx^{-b}$$ In this particular problem, we want $A=D=0$. So we try to find solutions of the type: $y(x)=x^{b+1}e^{-x}u(x)$ If we plug this in the original equation: $$xu''+2(b+1-x)u'+(a-2(b+1))u=0$$ We have transformed our problem into the associated Laguerre equation. But how could we know the $u(x)$ ODE would have a solution? Is this method always true and why isn't it usually taught (I've only seen it in Quantum Mechanics)?","I've seen that in some cases, one tries to study the behaviour of ODE at infinity and at zero, in order to find simplify the equation. For example: $$y''=\left(1-\frac{a}{x}+\frac{b(b+1)}{x^2}\right)y$$ In the limit $x\to\infty$: $$y''\approx y \implies y(x)=Ae^{x}+Be^{-x}$$ In the limit $x\to 0$: $$y''\approx \frac{b(b+1)}{x^2}y \implies y(x)=Cx^{b+1}+Dx^{-b}$$ In this particular problem, we want $A=D=0$. So we try to find solutions of the type: $y(x)=x^{b+1}e^{-x}u(x)$ If we plug this in the original equation: $$xu''+2(b+1-x)u'+(a-2(b+1))u=0$$ We have transformed our problem into the associated Laguerre equation. But how could we know the $u(x)$ ODE would have a solution? Is this method always true and why isn't it usually taught (I've only seen it in Quantum Mechanics)?",,['ordinary-differential-equations']
80,"Where to look for ""standard"" ODE solutions?","Where to look for ""standard"" ODE solutions?",,"I remember one day having stumbled upon a nice online resource where one could look for solutions to very general ODEs (or at least the literature names thereof), but unfortunately I forgot its location... So, where can one go looking to e.g. figure out that $$\frac{d}{dr}\left[(1-r^2)\frac{d}{dr}f(r)\right] + l(l+1)f(r) = 0$$ is the Legendre differential equation , especially if one doesn't know about this particular representation?","I remember one day having stumbled upon a nice online resource where one could look for solutions to very general ODEs (or at least the literature names thereof), but unfortunately I forgot its location... So, where can one go looking to e.g. figure out that $$\frac{d}{dr}\left[(1-r^2)\frac{d}{dr}f(r)\right] + l(l+1)f(r) = 0$$ is the Legendre differential equation , especially if one doesn't know about this particular representation?",,"['ordinary-differential-equations', 'online-resources']"
81,Lyapunov function for $x'' + \epsilon x^2x' + x = 0$,Lyapunov function for,x'' + \epsilon x^2x' + x = 0,"I am studying the book by Holmes and Guckenheimer, and am relatively new to all this Lyapunov functions, so I am trying to do all the exercises in it. I am currently stuck with exercise 1.3.1. It is about the equation $x'' + \epsilon x^2 x' + x = 0$, with $\epsilon > 0$, which has the particularity that when linearized the eigenvalues have null real part, so we cannot apply the theorem that says there is an homeomorphism between flows. The exercise asks to prove via a Lyapunov function that the point $x=0, x'=0$ is assimptotically stable. My first guess was a very simple function $V(x,y) = \frac{x^2}{2} + \frac{y^2}{2}$ where $y=x'$, and I got $V'(x,y) = -\epsilon x^2y^2$ which is not enough (the derivative is along the curves) to prove assimptotic stability. Then I tried something along the lines of a previous example in the book, $V(x,y) = \frac{x^2}{2} + \alpha \frac{y^2}{2} + \gamma xy$, but now $V' = xy(\cdots) + \gamma(y^2 - x^2)$ so I cannot get any stability from here. Any advice will be very appreciated.","I am studying the book by Holmes and Guckenheimer, and am relatively new to all this Lyapunov functions, so I am trying to do all the exercises in it. I am currently stuck with exercise 1.3.1. It is about the equation $x'' + \epsilon x^2 x' + x = 0$, with $\epsilon > 0$, which has the particularity that when linearized the eigenvalues have null real part, so we cannot apply the theorem that says there is an homeomorphism between flows. The exercise asks to prove via a Lyapunov function that the point $x=0, x'=0$ is assimptotically stable. My first guess was a very simple function $V(x,y) = \frac{x^2}{2} + \frac{y^2}{2}$ where $y=x'$, and I got $V'(x,y) = -\epsilon x^2y^2$ which is not enough (the derivative is along the curves) to prove assimptotic stability. Then I tried something along the lines of a previous example in the book, $V(x,y) = \frac{x^2}{2} + \alpha \frac{y^2}{2} + \gamma xy$, but now $V' = xy(\cdots) + \gamma(y^2 - x^2)$ so I cannot get any stability from here. Any advice will be very appreciated.",,"['ordinary-differential-equations', 'dynamical-systems', 'lyapunov-functions']"
82,Find the solution of the differential equation $\frac{dy}{dx}=x^2y$ satisfying $y(0)=5$,Find the solution of the differential equation  satisfying,\frac{dy}{dx}=x^2y y(0)=5,"I'm not entirely sure I'm on the right track, and my teacher seemed to fly through this section, so I don't really understand what I'm doing. Anyway, here is the problem again: Find the solution of the differential equation $\frac{dy}{dx}=x^2y$ satisfying $y(0)=5$ By rote I know to do the following... $$\frac{dy}{dx}=x^2y$$ $$\int\frac{dy}{y}=\int x^2dx$$ $$ln|y|=\frac{x^3}{3}+C$$ So then using the initial value condition... $$ln|5|=\frac{(0)^3}{3}+C$$ So $C=ln|5|$  Next I used this value for $C$ to solve for y: $$ln|y|=\frac{x^3}{3}+ln|5|$$ $$ln|y|=\frac{x^3+ln|5|}{3}$$ $$|y|=e^\frac{x^3+ln|5|}{3}$$ So I got here and wasn't sure if it were right and didn't know what to do with the absolute values.  Am I on the right track?","I'm not entirely sure I'm on the right track, and my teacher seemed to fly through this section, so I don't really understand what I'm doing. Anyway, here is the problem again: Find the solution of the differential equation $\frac{dy}{dx}=x^2y$ satisfying $y(0)=5$ By rote I know to do the following... $$\frac{dy}{dx}=x^2y$$ $$\int\frac{dy}{y}=\int x^2dx$$ $$ln|y|=\frac{x^3}{3}+C$$ So then using the initial value condition... $$ln|5|=\frac{(0)^3}{3}+C$$ So $C=ln|5|$  Next I used this value for $C$ to solve for y: $$ln|y|=\frac{x^3}{3}+ln|5|$$ $$ln|y|=\frac{x^3+ln|5|}{3}$$ $$|y|=e^\frac{x^3+ln|5|}{3}$$ So I got here and wasn't sure if it were right and didn't know what to do with the absolute values.  Am I on the right track?",,"['calculus', 'ordinary-differential-equations']"
83,Differential equation in $\mathbb{R}^n$,Differential equation in,\mathbb{R}^n,"Given the following ODE for $x$ in $\mathbb{R^n}$ $$ \textbf{x'}(t)=\frac{4\textbf{x}}{|\textbf{x}|}$$ How can I solve it given that $\textbf{x}$ is a function of time? The magnitude of $\textbf{x}$ is really confusing me. I know it should end up being some sort of exponential, right? Thank you for the help/suggestions :)","Given the following ODE for $x$ in $\mathbb{R^n}$ $$ \textbf{x'}(t)=\frac{4\textbf{x}}{|\textbf{x}|}$$ How can I solve it given that $\textbf{x}$ is a function of time? The magnitude of $\textbf{x}$ is really confusing me. I know it should end up being some sort of exponential, right? Thank you for the help/suggestions :)",,['ordinary-differential-equations']
84,Simple Frobenius problem without recurrence relation?,Simple Frobenius problem without recurrence relation?,,"I am just learning frobenius method in my 'math methods in physics' class. The first problem i am trying to solve is $$ x^2y''-xy'+n^2y=0$$ (where n is a constant). I know that i have to plug in the maclaurin expansion of y which results in me getting this summation $$y(z) = \sum_{k = 0}^{ \infty} a_kx^{(k+s)}[n^2-(k+s)+(k+s)(k+s-1)] = 0$$ [ by asserting that $$ a_0 \neq 0 $$ is non-zero i solved for s and got $$ s=1 \pm \sqrt{1-n^2} $$ Now i am stuck on how to continue to get the coefficients a_1, a_2, and so on. I remember seeing an example that solved for a recurrence relation but since every term had the same power of x when we plugged it back in we cant do that method. How do i continue?","I am just learning frobenius method in my 'math methods in physics' class. The first problem i am trying to solve is $$ x^2y''-xy'+n^2y=0$$ (where n is a constant). I know that i have to plug in the maclaurin expansion of y which results in me getting this summation $$y(z) = \sum_{k = 0}^{ \infty} a_kx^{(k+s)}[n^2-(k+s)+(k+s)(k+s-1)] = 0$$ [ by asserting that $$ a_0 \neq 0 $$ is non-zero i solved for s and got $$ s=1 \pm \sqrt{1-n^2} $$ Now i am stuck on how to continue to get the coefficients a_1, a_2, and so on. I remember seeing an example that solved for a recurrence relation but since every term had the same power of x when we plugged it back in we cant do that method. How do i continue?",,"['sequences-and-series', 'ordinary-differential-equations', 'power-series']"
85,Maximal unique solution to an IVP.,Maximal unique solution to an IVP.,,"In class we learned the existence and uniqueness theorems for differential equations. The weaker Picard-Lindelof states that for any IVP, $$ \begin{cases} x'(t) = f(t, x(t))\\ x(t_0) = x_0 \end{cases} $$ where $f$ is continuous in the first argument and locally Lipschitz in the second, there is a unique solution in some neighborhood around $t_0$ (in a open interval $I_0$ containing $t_0$). This result was extended to: there is a maximal unique solution to all IVP with the above form (Basically means there is biggest possible interval on which the solution is unique). More precisely, it means if $x:(a, b) \to \mathbb{R}^n$ is the maximal solution and $y:(a', b') \to \mathbb{R}^n$ is any other solution to the same IVP, then $(a', b') \subset (a, b)$ and $x = y$ on $(a', b')$. My question is: can we find the maximal interval where there is a unique solution for any given IVP (also for any function $f$)?","In class we learned the existence and uniqueness theorems for differential equations. The weaker Picard-Lindelof states that for any IVP, $$ \begin{cases} x'(t) = f(t, x(t))\\ x(t_0) = x_0 \end{cases} $$ where $f$ is continuous in the first argument and locally Lipschitz in the second, there is a unique solution in some neighborhood around $t_0$ (in a open interval $I_0$ containing $t_0$). This result was extended to: there is a maximal unique solution to all IVP with the above form (Basically means there is biggest possible interval on which the solution is unique). More precisely, it means if $x:(a, b) \to \mathbb{R}^n$ is the maximal solution and $y:(a', b') \to \mathbb{R}^n$ is any other solution to the same IVP, then $(a', b') \subset (a, b)$ and $x = y$ on $(a', b')$. My question is: can we find the maximal interval where there is a unique solution for any given IVP (also for any function $f$)?",,"['calculus', 'real-analysis', 'analysis', 'ordinary-differential-equations']"
86,Existence of solution of singular ODE,Existence of solution of singular ODE,,"Suppose $f$ is a Lipschitz continuous function defined on $\mathbb{R}$ . How can one prove that the following ODE admits at least one solution. \begin{equation} y'' + \frac{1}{x}y' + f(y) = 0 \end{equation} with $y(0) = y_0, y'(0) = 0$ .",Suppose is a Lipschitz continuous function defined on . How can one prove that the following ODE admits at least one solution. with .,"f \mathbb{R} \begin{equation}
y'' + \frac{1}{x}y' + f(y) = 0
\end{equation} y(0) = y_0, y'(0) = 0",['ordinary-differential-equations']
87,Show that System $(I)$ is stable iff $X(t)$ is bounded.,Show that System  is stable iff  is bounded.,(I) X(t),"I have a theorem : For a linear homogeneous system: $$\dfrac{dx}{dt}=A(t)x \tag{I}$$ Where $A(t)=(a_{ij}(t))_{n \times n} \in C(\mathbb{R}^+,\mathbb{R}^{n \times n})$ Suppose that $X(t)$ be the fundamental matrix solution of the following reference system $(I)$. Let $K(t,s)=X(t)X^{-1}(s)$ be the Cauchy matrix of the following reference system $(I)$. Prove that: a/ System $(I)$ is stable iff $X(t)$ is bounded, it means $\exists M>0$ such that $$\|X(t)\| \le M, \forall t \ge 0 $$ b/ System $(I)$ is asymptotically stable iff  $$\lim_{t \to +\infty}X(t)=0$$ ================================================================= I have stuck when I try to show this theorem. I have tried using the definition of stable, asymptotically stable. But I have no solution. ================================================================= Ps : (Or) if somebody knows/reads this theorem (book/pdf/djvu...) then you can post it. Can anyone help me! Any help will be appreciated! Thanks/","I have a theorem : For a linear homogeneous system: $$\dfrac{dx}{dt}=A(t)x \tag{I}$$ Where $A(t)=(a_{ij}(t))_{n \times n} \in C(\mathbb{R}^+,\mathbb{R}^{n \times n})$ Suppose that $X(t)$ be the fundamental matrix solution of the following reference system $(I)$. Let $K(t,s)=X(t)X^{-1}(s)$ be the Cauchy matrix of the following reference system $(I)$. Prove that: a/ System $(I)$ is stable iff $X(t)$ is bounded, it means $\exists M>0$ such that $$\|X(t)\| \le M, \forall t \ge 0 $$ b/ System $(I)$ is asymptotically stable iff  $$\lim_{t \to +\infty}X(t)=0$$ ================================================================= I have stuck when I try to show this theorem. I have tried using the definition of stable, asymptotically stable. But I have no solution. ================================================================= Ps : (Or) if somebody knows/reads this theorem (book/pdf/djvu...) then you can post it. Can anyone help me! Any help will be appreciated! Thanks/",,"['ordinary-differential-equations', 'dynamical-systems', 'control-theory']"
88,Differential equation based on chemical kinetics,Differential equation based on chemical kinetics,,"In chemical kinetics, the law of mass action gives us reaction rates of the form $$r=k x^a y^b$$ where $r$ is the time derivative of either $x$ or $y$ times a constant $$r=-\frac{dx}{\beta dt}=-\frac{dy}{\gamma dt}$$ Here's an example $$r=k x^2 y= -\frac{dx}{2dt} = -\frac{dy}{dt}$$ In general, how would I go about solving these differential equations for $x(t)$ and $y(t)$?","In chemical kinetics, the law of mass action gives us reaction rates of the form $$r=k x^a y^b$$ where $r$ is the time derivative of either $x$ or $y$ times a constant $$r=-\frac{dx}{\beta dt}=-\frac{dy}{\gamma dt}$$ Here's an example $$r=k x^2 y= -\frac{dx}{2dt} = -\frac{dy}{dt}$$ In general, how would I go about solving these differential equations for $x(t)$ and $y(t)$?",,"['ordinary-differential-equations', 'chemistry']"
89,Showing that a given PDE is a solution to harmonic motion via transform,Showing that a given PDE is a solution to harmonic motion via transform,,"OK, here's an problem that should, by all accounts, be pretty simple, but I want to make sure I am approaching this correctly. Given: $$\frac{\partial ^2u}{\partial t^2}=c^2\frac{\partial ^2u}{\partial x^2}$$ use the change of variables $\alpha = x+ct$, $\beta = x-ct$ to transform the above equation into $$\frac{\partial ^2u}{\partial \alpha \partial \beta}=0$$ This should be straightforward enough. So, using the chain rule:  $$ \frac{\partial u}{\partial t}= \frac{\partial u}{\partial \alpha}\frac{\partial \alpha}{\partial t}+ \frac{\partial u}{\partial \beta}\frac{\partial \beta}{\partial t} \text{and } \frac{\partial u}{\partial x}= \frac{\partial u}{\partial \alpha}\frac{\partial \alpha}{\partial x}+ \frac{\partial u}{\partial \beta}\frac{\partial \beta}{\partial x}$$ we take the partial derivatives with respect to t and x in the alpha and beta expressions: $$\frac{\partial \alpha}{\partial t}=c, \frac{\partial \alpha}{\partial x} = 1,\frac{\partial \beta}{\partial t}=-c,\frac{\partial \beta}{\partial x}=1$$ Plug these back into the above PDEs:  $$ \frac{\partial u}{\partial t}=c \frac{\partial u}{\partial \alpha}-c \frac{\partial u}{\partial \beta} \text{and } \frac{\partial u}{\partial x}= \frac{\partial u}{\partial \alpha}+ \frac{\partial u}{\partial \beta}$$ taking another derivative with respect to beta:  $$ \frac{\partial^2 u}{\partial t \partial \beta}=c \frac{\partial^2 u}{\partial \alpha \partial  \beta}-c \frac{\partial^2 u}{\partial \beta^2} \text{and } \frac{\partial^2 u}{\partial x \partial \beta}= \frac{\partial^2 u}{\partial \alpha \partial \beta}+ \frac{\partial^2 u}{\partial \beta^2}$$ I saw there are two terms that are $\frac{\partial^2 u}{\partial \alpha \partial \beta}$ and rearranging things a bit, and noting they are equal: $$ c \frac{\partial^2 u}{\partial \alpha \partial  \beta}=c \frac{\partial^2 u}{\partial \beta^2}+\frac{\partial^2 u}{\partial t \partial \beta} \text{and } \frac{\partial^2 u}{\partial \alpha \partial \beta}=\frac{\partial^2 u}{\partial \beta^2}-\frac{\partial^2 u}{\partial x \partial \beta}  $$ The problem I am having is that I feel I messed up a substitution someplace. Because I end up with $$-c\frac{\partial^2 u}{\partial x \partial \beta}=\frac{\partial^2 u}{\partial t \partial \beta}$$ and the terms aren't going to zero. Or there's some stupidly simple step I missed. I guess what I am asking is if I am right so far.","OK, here's an problem that should, by all accounts, be pretty simple, but I want to make sure I am approaching this correctly. Given: $$\frac{\partial ^2u}{\partial t^2}=c^2\frac{\partial ^2u}{\partial x^2}$$ use the change of variables $\alpha = x+ct$, $\beta = x-ct$ to transform the above equation into $$\frac{\partial ^2u}{\partial \alpha \partial \beta}=0$$ This should be straightforward enough. So, using the chain rule:  $$ \frac{\partial u}{\partial t}= \frac{\partial u}{\partial \alpha}\frac{\partial \alpha}{\partial t}+ \frac{\partial u}{\partial \beta}\frac{\partial \beta}{\partial t} \text{and } \frac{\partial u}{\partial x}= \frac{\partial u}{\partial \alpha}\frac{\partial \alpha}{\partial x}+ \frac{\partial u}{\partial \beta}\frac{\partial \beta}{\partial x}$$ we take the partial derivatives with respect to t and x in the alpha and beta expressions: $$\frac{\partial \alpha}{\partial t}=c, \frac{\partial \alpha}{\partial x} = 1,\frac{\partial \beta}{\partial t}=-c,\frac{\partial \beta}{\partial x}=1$$ Plug these back into the above PDEs:  $$ \frac{\partial u}{\partial t}=c \frac{\partial u}{\partial \alpha}-c \frac{\partial u}{\partial \beta} \text{and } \frac{\partial u}{\partial x}= \frac{\partial u}{\partial \alpha}+ \frac{\partial u}{\partial \beta}$$ taking another derivative with respect to beta:  $$ \frac{\partial^2 u}{\partial t \partial \beta}=c \frac{\partial^2 u}{\partial \alpha \partial  \beta}-c \frac{\partial^2 u}{\partial \beta^2} \text{and } \frac{\partial^2 u}{\partial x \partial \beta}= \frac{\partial^2 u}{\partial \alpha \partial \beta}+ \frac{\partial^2 u}{\partial \beta^2}$$ I saw there are two terms that are $\frac{\partial^2 u}{\partial \alpha \partial \beta}$ and rearranging things a bit, and noting they are equal: $$ c \frac{\partial^2 u}{\partial \alpha \partial  \beta}=c \frac{\partial^2 u}{\partial \beta^2}+\frac{\partial^2 u}{\partial t \partial \beta} \text{and } \frac{\partial^2 u}{\partial \alpha \partial \beta}=\frac{\partial^2 u}{\partial \beta^2}-\frac{\partial^2 u}{\partial x \partial \beta}  $$ The problem I am having is that I feel I messed up a substitution someplace. Because I end up with $$-c\frac{\partial^2 u}{\partial x \partial \beta}=\frac{\partial^2 u}{\partial t \partial \beta}$$ and the terms aren't going to zero. Or there's some stupidly simple step I missed. I guess what I am asking is if I am right so far.",,"['ordinary-differential-equations', 'partial-differential-equations', 'harmonic-functions']"
90,"Initial Value, First Order Differential Equation: Weird natural log separation","Initial Value, First Order Differential Equation: Weird natural log separation",,"Solve the initial value first order differential equation problem: $y' = \displaystyle\frac{y^5}{x(1+y^4)},\ y(1) = 1$ \begin{align} \frac{1+y^4}{y^5}dy &= \frac 1x dx\\ \left(\frac 1{y^5} + \frac 1y\right)dy &= \frac 1x dx\\ -\frac 1 {4y^4} + \text{ln}|y| &= \text{ln}|x| + C_1  \end{align} This is where I get stuck. How do I solve for $y$ at this point? Wolfram Alpha gave has this following step, which I do not understand at all: I tried the raising everything as exponents of $e$, but that seems to be a dead end: \begin{align} -\frac 1 {4y^4} + \text{ln}|y| &= \text{ln}|x| + C_1\\ e^{\left(\text{ln} y - \frac 1 {4y^4}\right)} &= e^{\text{ln} x + C_1}\\ y\cdot e^{\left(- \frac 1 {4y^4}\right)} &= x\cdot C_2,\ \text{where $C_2 = e^{C_1}$} \end{align} P.S. Natural logarithms don't seem to be working: \ln |y| produces $\ln |y|$. I used \text{ln}|y| for $\text{ln}|y|$ instead. Is this a bug?","Solve the initial value first order differential equation problem: $y' = \displaystyle\frac{y^5}{x(1+y^4)},\ y(1) = 1$ \begin{align} \frac{1+y^4}{y^5}dy &= \frac 1x dx\\ \left(\frac 1{y^5} + \frac 1y\right)dy &= \frac 1x dx\\ -\frac 1 {4y^4} + \text{ln}|y| &= \text{ln}|x| + C_1  \end{align} This is where I get stuck. How do I solve for $y$ at this point? Wolfram Alpha gave has this following step, which I do not understand at all: I tried the raising everything as exponents of $e$, but that seems to be a dead end: \begin{align} -\frac 1 {4y^4} + \text{ln}|y| &= \text{ln}|x| + C_1\\ e^{\left(\text{ln} y - \frac 1 {4y^4}\right)} &= e^{\text{ln} x + C_1}\\ y\cdot e^{\left(- \frac 1 {4y^4}\right)} &= x\cdot C_2,\ \text{where $C_2 = e^{C_1}$} \end{align} P.S. Natural logarithms don't seem to be working: \ln |y| produces $\ln |y|$. I used \text{ln}|y| for $\text{ln}|y|$ instead. Is this a bug?",,['ordinary-differential-equations']
91,Bessel function values,Bessel function values,,"Given $$J_m(x)=\sum_{n=0}^{\infty}{{(-1)^n}\over{n!(n+m)!}}\left(\frac{x}{2}\right)^{m+2n},$$ where $m=0,1,2,\ldots$ and $x\ge0$. Need to show $$\left|J_m(x)\right|\le1.$$","Given $$J_m(x)=\sum_{n=0}^{\infty}{{(-1)^n}\over{n!(n+m)!}}\left(\frac{x}{2}\right)^{m+2n},$$ where $m=0,1,2,\ldots$ and $x\ge0$. Need to show $$\left|J_m(x)\right|\le1.$$",,"['ordinary-differential-equations', 'special-functions']"
92,differential equation with parameter,differential equation with parameter,,"I've got problem with such an example: Given equation $\frac{dx}{dt} = -x + x^7$ with initial condition $x(0) = \lambda$, where $x=x(t, \lambda)$. Find $\frac{ \partial x(t, \lambda)}{\partial \lambda} \mid _{\lambda = 0}$. What I've got now is: We can write: $x(t, \lambda) = \int_{0}^{t} -x + x^7 ds + C(t,\lambda)$ but since $x(0) = \lambda$, we've got $\lambda = x(0, \lambda) = \int_{0}^{0} -x + x^7 ds + C(0,\lambda) = C(0, \lambda)$ and so  $x(t, \lambda) = \lambda + \int_{0}^{t} -x + x^7 ds$ Now we can apply $\frac{ \partial }{\partial \lambda}$ to both sides getting: $\frac{ \partial x(t, \lambda)}{\partial \lambda} = 1 + \int_{0}^{t}{ \frac{ \partial}{\partial \lambda}(-x(t, \lambda)) + \frac{ \partial}{\partial \lambda}(x(t, \lambda)^7) }ds$. But now I have no idea how can I calculate value of that integral. Does anyone have idea how to move further with that solution ? Or maybe that is wrong path and there exists easier way to solve that problem? Thanks in advance for all the help","I've got problem with such an example: Given equation $\frac{dx}{dt} = -x + x^7$ with initial condition $x(0) = \lambda$, where $x=x(t, \lambda)$. Find $\frac{ \partial x(t, \lambda)}{\partial \lambda} \mid _{\lambda = 0}$. What I've got now is: We can write: $x(t, \lambda) = \int_{0}^{t} -x + x^7 ds + C(t,\lambda)$ but since $x(0) = \lambda$, we've got $\lambda = x(0, \lambda) = \int_{0}^{0} -x + x^7 ds + C(0,\lambda) = C(0, \lambda)$ and so  $x(t, \lambda) = \lambda + \int_{0}^{t} -x + x^7 ds$ Now we can apply $\frac{ \partial }{\partial \lambda}$ to both sides getting: $\frac{ \partial x(t, \lambda)}{\partial \lambda} = 1 + \int_{0}^{t}{ \frac{ \partial}{\partial \lambda}(-x(t, \lambda)) + \frac{ \partial}{\partial \lambda}(x(t, \lambda)^7) }ds$. But now I have no idea how can I calculate value of that integral. Does anyone have idea how to move further with that solution ? Or maybe that is wrong path and there exists easier way to solve that problem? Thanks in advance for all the help",,['ordinary-differential-equations']
93,How can one calculate this diferential equation,How can one calculate this diferential equation,,$y\ dy = 4x(y^2+1)^2\ dx \text{ and } y(0) = 1$ I'm trying: $\dfrac{y\ dy}{ (y^2+1)^2 }= 4x\ dx$ but I can't figure out what to do now.,$y\ dy = 4x(y^2+1)^2\ dx \text{ and } y(0) = 1$ I'm trying: $\dfrac{y\ dy}{ (y^2+1)^2 }= 4x\ dx$ but I can't figure out what to do now.,,"['calculus', 'integration', 'ordinary-differential-equations']"
94,Integrating velocity field to get position,Integrating velocity field to get position,,"I feel silly for simply being brainstuck, but consider the following integral, physically it would be the solution of $\mathbf{p} = \tfrac{d\mathbf{v}}{dt}$ - the  position of a given particle in space with respect to the time and a velocity vector field. $$\mathbf{p}(x,y) = \int_a^b{\mathbf{v}(x,y)}dt$$ However I have no idea how to describe the $x$ and $y$ components of the velocity vector in t. Or how to convert dt to dx & dy? Say for example $\mathbf{v} = \left \langle 3x, xy  \right \rangle$ Which would result in: $$\mathbf{p}(x,y) = \int_a^b{3x}dt\cdot\mathbf{i}+\int_a^b{xy}dt\cdot\mathbf{j}$$ Buth how then to continue? I should be possible to calculate this right? I know the speed vector at each point in space, so over a given time period I should be able to get the new position right?","I feel silly for simply being brainstuck, but consider the following integral, physically it would be the solution of $\mathbf{p} = \tfrac{d\mathbf{v}}{dt}$ - the  position of a given particle in space with respect to the time and a velocity vector field. $$\mathbf{p}(x,y) = \int_a^b{\mathbf{v}(x,y)}dt$$ However I have no idea how to describe the $x$ and $y$ components of the velocity vector in t. Or how to convert dt to dx & dy? Say for example $\mathbf{v} = \left \langle 3x, xy  \right \rangle$ Which would result in: $$\mathbf{p}(x,y) = \int_a^b{3x}dt\cdot\mathbf{i}+\int_a^b{xy}dt\cdot\mathbf{j}$$ Buth how then to continue? I should be possible to calculate this right? I know the speed vector at each point in space, so over a given time period I should be able to get the new position right?",,"['ordinary-differential-equations', 'multivariable-calculus', 'physics']"
95,Help with generating functions.,Help with generating functions.,,"Background. Let $P_0(y)=2y-3$ and define recursively $$P_{n+1}(y)=4y\cdot P_n'(y)+(5-4y)\cdot P_n(y).$$ I would like to know as many properties of $P_n$ as I can. For example, it can be shown that each $P_n$ has only real simple positive zeros and that $P_n$ and $P_{n+1}$ strictly interlace for every $n$. It can also be shown that the recursive strict Turan Inequality is satisfied, for $y>0$, $$T_n(y):=P_{n+1}(y)^2-P_n(y)\cdot P_{n+2}(y)>0.$$ Empirical evidence (Mathematica) indicates that $T_n(y)$ is an increasing polynomial with only one real zero at $0$. My goal is to find a good estimate of $T_n(\pi)$ and show that $T_n(y)>T_n(\pi)$ for $y>\pi$. In an effort to establish that goal it would be nice to find a generating function for the $\{P_n\}$. Problem. In an attempt to find such generating function I have made a terrible error, but I can't seem to find what my error is. Please show me the error of my ways. Define the formal power series, $$f(y,t):= \sum_{n=0}^\infty \frac{P_n(y)}{n!}t^n.$$ we see then that $$f_t=\frac{d}{dt}f(y,t)=\sum_{n=1}^\infty \frac{P_n(y)}{(n-1)!}t^{n-1}=\sum_{n=0}^\infty \frac{P_{n+1}(y)}{n!}t^n.$$ Using the differential equation above we have, $$\sum_{n=0}^\infty \frac{P_{n+1}(y)}{n!}t^n=\sum_{n=0}^\infty \frac{4y\cdot P_n'(y)+(5-4y)\cdot P_n(y)}{n!}t^n.$$ So we arrive at $$f_t=4y\cdot f_y+(5-4y)\cdot f.$$ A quick check verifies that $f(y,t)=e^{5t+y}$. And so we have the generating function $$e^{5t+y}=\sum_{n=0}^\infty \frac{P_n(y)}{n!}t^n,$$ but this implies, differentiate with respect to $y$, that $$e^{5t+y}=\sum_{n=0}^\infty \frac{P'_n(y)}{n!}t^n,$$ and so $$P_n(y)=P'_n(y)$$ for every $n$ and $y$. How can that be? $\{P_n\}$ is a sequence of polynomials??? I was motivated to try generating functions by reading ""Rainville - Special Functions - Page 188"" and the derivations for Hermite polynomials.","Background. Let $P_0(y)=2y-3$ and define recursively $$P_{n+1}(y)=4y\cdot P_n'(y)+(5-4y)\cdot P_n(y).$$ I would like to know as many properties of $P_n$ as I can. For example, it can be shown that each $P_n$ has only real simple positive zeros and that $P_n$ and $P_{n+1}$ strictly interlace for every $n$. It can also be shown that the recursive strict Turan Inequality is satisfied, for $y>0$, $$T_n(y):=P_{n+1}(y)^2-P_n(y)\cdot P_{n+2}(y)>0.$$ Empirical evidence (Mathematica) indicates that $T_n(y)$ is an increasing polynomial with only one real zero at $0$. My goal is to find a good estimate of $T_n(\pi)$ and show that $T_n(y)>T_n(\pi)$ for $y>\pi$. In an effort to establish that goal it would be nice to find a generating function for the $\{P_n\}$. Problem. In an attempt to find such generating function I have made a terrible error, but I can't seem to find what my error is. Please show me the error of my ways. Define the formal power series, $$f(y,t):= \sum_{n=0}^\infty \frac{P_n(y)}{n!}t^n.$$ we see then that $$f_t=\frac{d}{dt}f(y,t)=\sum_{n=1}^\infty \frac{P_n(y)}{(n-1)!}t^{n-1}=\sum_{n=0}^\infty \frac{P_{n+1}(y)}{n!}t^n.$$ Using the differential equation above we have, $$\sum_{n=0}^\infty \frac{P_{n+1}(y)}{n!}t^n=\sum_{n=0}^\infty \frac{4y\cdot P_n'(y)+(5-4y)\cdot P_n(y)}{n!}t^n.$$ So we arrive at $$f_t=4y\cdot f_y+(5-4y)\cdot f.$$ A quick check verifies that $f(y,t)=e^{5t+y}$. And so we have the generating function $$e^{5t+y}=\sum_{n=0}^\infty \frac{P_n(y)}{n!}t^n,$$ but this implies, differentiate with respect to $y$, that $$e^{5t+y}=\sum_{n=0}^\infty \frac{P'_n(y)}{n!}t^n,$$ and so $$P_n(y)=P'_n(y)$$ for every $n$ and $y$. How can that be? $\{P_n\}$ is a sequence of polynomials??? I was motivated to try generating functions by reading ""Rainville - Special Functions - Page 188"" and the derivations for Hermite polynomials.",,"['ordinary-differential-equations', 'polynomials', 'generating-functions', 'recursion']"
96,1D Green's function: from interval to infinite line,1D Green's function: from interval to infinite line,,"Let's consider two problems for diffusion equation. The first one: $$ u_t = a^2u_{xx},\qquad 0<x<l,\quad 0<t\leq T $$ $$ u(x,0) = \phi(x), \qquad 0 \leq x \leq l  $$ \begin{equation} u(0,t)=0,\quad u(l,t)=0, \quad 0 \leq t \leq T \end{equation} and the second one: $$ u_t = a^2u_{xx},\qquad -\infty <x<+\infty,\quad t>0 $$ $$ u(x,0) = \phi(x), \qquad -\infty < x < +\infty $$ For both these cases there are well known expressions for Green's function: $$ G_1(x,\xi,t) = \frac{2}{l}\sum_{n=1}^{\infty} \exp\left({-\left(\frac{\pi n}{l}\right)^2}a^2t\right)\sin\frac{\pi nx}{l}\sin\frac{\pi n\xi}{l} $$ $$ G_2(x,\xi,t) = \frac{1}{\sqrt{4\pi a^2 t}}\exp{\left(-\cfrac{(x-\xi)^2}{4a^2t}\right)} $$ Thus we obtain solutions for first (finite) problem: \begin{equation} u(x,t)=\int\limits_0^lG_1(x,\xi,t)\phi(\xi)\,d\xi \end{equation} and for the second (infinite): \begin{equation} u(x,t)=\int\limits_{-\infty}^{+\infty}G_2(x,\xi,t)\phi(\xi)\,d\xi \end{equation} Is there any way to obtain Green's function for infinite case from Green's function for range $[0,l]$ (for example by stating  $\quad l\rightarrow+\infty$, but I have not achieved any success with this idea)? Or may be second solution from first?","Let's consider two problems for diffusion equation. The first one: $$ u_t = a^2u_{xx},\qquad 0<x<l,\quad 0<t\leq T $$ $$ u(x,0) = \phi(x), \qquad 0 \leq x \leq l  $$ \begin{equation} u(0,t)=0,\quad u(l,t)=0, \quad 0 \leq t \leq T \end{equation} and the second one: $$ u_t = a^2u_{xx},\qquad -\infty <x<+\infty,\quad t>0 $$ $$ u(x,0) = \phi(x), \qquad -\infty < x < +\infty $$ For both these cases there are well known expressions for Green's function: $$ G_1(x,\xi,t) = \frac{2}{l}\sum_{n=1}^{\infty} \exp\left({-\left(\frac{\pi n}{l}\right)^2}a^2t\right)\sin\frac{\pi nx}{l}\sin\frac{\pi n\xi}{l} $$ $$ G_2(x,\xi,t) = \frac{1}{\sqrt{4\pi a^2 t}}\exp{\left(-\cfrac{(x-\xi)^2}{4a^2t}\right)} $$ Thus we obtain solutions for first (finite) problem: \begin{equation} u(x,t)=\int\limits_0^lG_1(x,\xi,t)\phi(\xi)\,d\xi \end{equation} and for the second (infinite): \begin{equation} u(x,t)=\int\limits_{-\infty}^{+\infty}G_2(x,\xi,t)\phi(\xi)\,d\xi \end{equation} Is there any way to obtain Green's function for infinite case from Green's function for range $[0,l]$ (for example by stating  $\quad l\rightarrow+\infty$, but I have not achieved any success with this idea)? Or may be second solution from first?",,"['ordinary-differential-equations', 'partial-differential-equations', 'mathematical-physics']"
97,Explicit form of Poincare's map for the spring-mass-damper,Explicit form of Poincare's map for the spring-mass-damper,,"Problem: Write in explicit form Poincare's map for $\ddot x+\delta\dot x+\omega_0^2x=\gamma\cos\omega t$. Find the stationary points and examine their stability. An attempt at a solution: The characteristic equation of the homogenous equation is $z^2+\delta z+\omega_0^2=0$ Its roots are $$z_{1,2}=\frac{-\delta\pm\sqrt{\delta^2-4\omega_0^2}}{2}$$ The solution to the homogenous differential equation is thus $$x(t)=Ae^{z_1t}+Be^{z_2t}$$ Let us now use the method of undetermined coefficients to find a particular solution as follows: We will be looking for a solution in the form $$x_p(t)=a\gamma\cos\omega t + b\gamma\sin\omega t$$ Then $$\dot x_p(t)=-a\omega\gamma\sin\omega t + b\omega\gamma\cos\omega t,$$ $$\ddot x_p(t)=-a\omega^2\gamma\cos\omega t - b\omega^2\gamma\sin\omega t$$ Substituting we get $$-a\omega^2\gamma\cos\omega t - b\omega^2\gamma\sin\omega t + \delta(-a\omega\gamma\sin\omega t + b\omega\gamma\cos\omega t)+\omega_0^2(a\gamma\cos\omega t + b\gamma\sin\omega t)=\gamma\cos\omega t$$ Equating the coefficients, we obtain the following system: $$-a\omega^2+b\delta\omega+\omega_0^2a=1$$ $$-b\omega^2-a\delta\omega+\omega_0^2b=0$$ Therefore, $$a=\frac{\omega_0^2-\omega^2}{(\omega_0^2-\omega^2)^2+(\delta\omega)^2}\land b=\frac{(\delta\omega)^2}{(\omega_0^2-\omega^2)^2+(\delta\omega)^2}$$ Now the general solution is $$Ae^{z_1t}+Be^{z_2t}+a\gamma\cos\omega t + b\gamma\sin\omega t$$ Let us consider the initial conditions $x(0)=x_0, \dot x(0)=y_0$. Using the initial conditions, we get $$A=x_0-a\gamma-B, \quad B=\frac{y_0-x_0z_1+a\gamma z_1-b\gamma\omega}{z_2-z_1}$$ Replace $\theta$ with $\omega t$, now lets write the equation as an autonomous system as such: $$\dot x=y$$ $$\dot y=-\delta y-\omega^2_0x+\gamma\cos\theta$$ $$\dot\theta=\omega (\text{mod}\quad 2\pi)$$ WLOG we may consider $\theta(0)=0$. Then the flow is $\phi^t(x_0,y_0,0)=(x(t),y(t),\omega t)$. The section is $\Gamma^0=\{(x,y,\theta):\theta=0\}$. Finally Poincare's map is $P(x_0,y_0)=(x(\frac{2\pi}{\omega}),y(\frac{2\pi}{\omega}))$. I want to examine the stability of all periodic solutions corresponding to fixed points of Poincare's map. Finding the fixed points seems beyond me now. I need help with that. Furthermore, how could I go about finding fundamental matrixes for the respctive periodical solutions and the respective monodromy matrices? Any help would be appreciated.","Problem: Write in explicit form Poincare's map for $\ddot x+\delta\dot x+\omega_0^2x=\gamma\cos\omega t$. Find the stationary points and examine their stability. An attempt at a solution: The characteristic equation of the homogenous equation is $z^2+\delta z+\omega_0^2=0$ Its roots are $$z_{1,2}=\frac{-\delta\pm\sqrt{\delta^2-4\omega_0^2}}{2}$$ The solution to the homogenous differential equation is thus $$x(t)=Ae^{z_1t}+Be^{z_2t}$$ Let us now use the method of undetermined coefficients to find a particular solution as follows: We will be looking for a solution in the form $$x_p(t)=a\gamma\cos\omega t + b\gamma\sin\omega t$$ Then $$\dot x_p(t)=-a\omega\gamma\sin\omega t + b\omega\gamma\cos\omega t,$$ $$\ddot x_p(t)=-a\omega^2\gamma\cos\omega t - b\omega^2\gamma\sin\omega t$$ Substituting we get $$-a\omega^2\gamma\cos\omega t - b\omega^2\gamma\sin\omega t + \delta(-a\omega\gamma\sin\omega t + b\omega\gamma\cos\omega t)+\omega_0^2(a\gamma\cos\omega t + b\gamma\sin\omega t)=\gamma\cos\omega t$$ Equating the coefficients, we obtain the following system: $$-a\omega^2+b\delta\omega+\omega_0^2a=1$$ $$-b\omega^2-a\delta\omega+\omega_0^2b=0$$ Therefore, $$a=\frac{\omega_0^2-\omega^2}{(\omega_0^2-\omega^2)^2+(\delta\omega)^2}\land b=\frac{(\delta\omega)^2}{(\omega_0^2-\omega^2)^2+(\delta\omega)^2}$$ Now the general solution is $$Ae^{z_1t}+Be^{z_2t}+a\gamma\cos\omega t + b\gamma\sin\omega t$$ Let us consider the initial conditions $x(0)=x_0, \dot x(0)=y_0$. Using the initial conditions, we get $$A=x_0-a\gamma-B, \quad B=\frac{y_0-x_0z_1+a\gamma z_1-b\gamma\omega}{z_2-z_1}$$ Replace $\theta$ with $\omega t$, now lets write the equation as an autonomous system as such: $$\dot x=y$$ $$\dot y=-\delta y-\omega^2_0x+\gamma\cos\theta$$ $$\dot\theta=\omega (\text{mod}\quad 2\pi)$$ WLOG we may consider $\theta(0)=0$. Then the flow is $\phi^t(x_0,y_0,0)=(x(t),y(t),\omega t)$. The section is $\Gamma^0=\{(x,y,\theta):\theta=0\}$. Finally Poincare's map is $P(x_0,y_0)=(x(\frac{2\pi}{\omega}),y(\frac{2\pi}{\omega}))$. I want to examine the stability of all periodic solutions corresponding to fixed points of Poincare's map. Finding the fixed points seems beyond me now. I need help with that. Furthermore, how could I go about finding fundamental matrixes for the respctive periodical solutions and the respective monodromy matrices? Any help would be appreciated.",,"['ordinary-differential-equations', 'dynamical-systems']"
98,Integral solution of a differential equation,Integral solution of a differential equation,,"I'm having difficulty verifying that $$K(x)=\int_{1}^{\infty}\frac{e^{-xt}}{\sqrt{t^{2}-1}}\, dt$$ satisfies the differential equation $$f^{\prime\prime}\left(x\right)+\frac{1}{x}f^{\prime}\left(x\right)-f\left(x\right)=0$$ I've tried evaluating each of the terms in the differential equation using $K(x)$ in place of $f(x)$ and then summing them as in the differential equation to see if it equaled zero, but when I do that, I always wind up with the exact same sum that I started with.  Can anyone help?","I'm having difficulty verifying that $$K(x)=\int_{1}^{\infty}\frac{e^{-xt}}{\sqrt{t^{2}-1}}\, dt$$ satisfies the differential equation $$f^{\prime\prime}\left(x\right)+\frac{1}{x}f^{\prime}\left(x\right)-f\left(x\right)=0$$ I've tried evaluating each of the terms in the differential equation using $K(x)$ in place of $f(x)$ and then summing them as in the differential equation to see if it equaled zero, but when I do that, I always wind up with the exact same sum that I started with.  Can anyone help?",,"['ordinary-differential-equations', 'definite-integrals']"
99,The number of non-trivial polynomial solutions of the differential equation $x^3y'(x)=y(x^2)$,The number of non-trivial polynomial solutions of the differential equation,x^3y'(x)=y(x^2),I came across the following problem that says: The number of non-trivial polynomial solutions of the differential equation  $x^3y'(x)=y(x^2)$ is which of the following? $(1)0\space (2)1 \space (3)3 (4)\infty.$ Can someone point me in the right direction? Thanks in advance for your time.,I came across the following problem that says: The number of non-trivial polynomial solutions of the differential equation  $x^3y'(x)=y(x^2)$ is which of the following? $(1)0\space (2)1 \space (3)3 (4)\infty.$ Can someone point me in the right direction? Thanks in advance for your time.,,['ordinary-differential-equations']
