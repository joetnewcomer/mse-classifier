,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Determinant of hermitian matrix,Determinant of hermitian matrix,,"Let $M=A+iB$ be a complex $n \times n$ Hermitian matrix. First of all we know that $$(\det M)^2=\det \begin{pmatrix} A & -B \\ B & A \end{pmatrix}.$$ Also $\det \begin{pmatrix} A & -B \\ B & A \end{pmatrix}$ is a polynomial in $n^2$ variables of degree $2n$. Is it true that $\det M$ is a polynomial, say D, of degree $n$ in this $n^2$ variables such that $D^2(M)=\det \begin{pmatrix} A & -B \\ B & A \end{pmatrix}$? The explicit calculations for $n=1,2,3$ suggest so, yet I can't find the information if this is true neither proof this.","Let $M=A+iB$ be a complex $n \times n$ Hermitian matrix. First of all we know that $$(\det M)^2=\det \begin{pmatrix} A & -B \\ B & A \end{pmatrix}.$$ Also $\det \begin{pmatrix} A & -B \\ B & A \end{pmatrix}$ is a polynomial in $n^2$ variables of degree $2n$. Is it true that $\det M$ is a polynomial, say D, of degree $n$ in this $n^2$ variables such that $D^2(M)=\det \begin{pmatrix} A & -B \\ B & A \end{pmatrix}$? The explicit calculations for $n=1,2,3$ suggest so, yet I can't find the information if this is true neither proof this.",,"['linear-algebra', 'matrices', 'determinant']"
1,"Revisit ""How can I visualize the nuclear norm ball""","Revisit ""How can I visualize the nuclear norm ball""",,"Revisiting How can I visualize the nuclear norm ball? Two eigenvalues are reproduced as following: $$ s_{1,2}=\frac{1}{\sqrt{2}}\sqrt{x^2+2y^2+z^2\pm|x+z|\sqrt{(x-z)^2+4y^2}}. $$ According to the following (from a paper) If a symmetric matrix: $$ A=\left( \begin{array}{cc} x & y\\ y & z\end{array} \right)$$ is rank $1$ , then $y=\sqrt{xz}$ , which comes from the fact that $vv^T$ is rank $1$ and any rank $1$ matrix can be represented in this form. $$\left[\begin{array}{cc} v_1\\ v_2\end{array}\right]\left[\begin{array}{cc} v_1 & v_2\end{array}\right]=\left( \begin{array}{cc} v_1^2 & v_1v_2\\ v_1v_2 & v_2^2\end{array} \right)$$ My question: how to explain the red circle in figure (b) is the $2\times 2$ symmetric unit-Euclidean-norm rank $1$ matrix? This is a circle in $3$ -D, how to get the equation of this circle through the rank $1$ matrix provided above? I believe just replace $y=\pm\sqrt{xz}$ in $s_{1,2}$ and can get the answer. So I choose the larger one of $s_{1,2}$ : $$ s_{\max}=\frac{1}{\sqrt{2}}\sqrt{x^2+2y^2+z^2 + |x+z|\sqrt{(x-z)^2+4y^2}}= \sqrt{2(x+z)^2}=1 (\text{unit norm})$$","Revisiting How can I visualize the nuclear norm ball? Two eigenvalues are reproduced as following: According to the following (from a paper) If a symmetric matrix: is rank , then , which comes from the fact that is rank and any rank matrix can be represented in this form. My question: how to explain the red circle in figure (b) is the symmetric unit-Euclidean-norm rank matrix? This is a circle in -D, how to get the equation of this circle through the rank matrix provided above? I believe just replace in and can get the answer. So I choose the larger one of :"," s_{1,2}=\frac{1}{\sqrt{2}}\sqrt{x^2+2y^2+z^2\pm|x+z|\sqrt{(x-z)^2+4y^2}}.   A=\left( \begin{array}{cc} x & y\\ y & z\end{array} \right) 1 y=\sqrt{xz} vv^T 1 1 \left[\begin{array}{cc} v_1\\ v_2\end{array}\right]\left[\begin{array}{cc} v_1 & v_2\end{array}\right]=\left( \begin{array}{cc} v_1^2 & v_1v_2\\ v_1v_2 & v_2^2\end{array} \right) 2\times 2 1 3 1 y=\pm\sqrt{xz} s_{1,2} s_{1,2}  s_{\max}=\frac{1}{\sqrt{2}}\sqrt{x^2+2y^2+z^2 + |x+z|\sqrt{(x-z)^2+4y^2}}= \sqrt{2(x+z)^2}=1 (\text{unit norm})","['linear-algebra', 'matrices', 'functional-analysis', 'normed-spaces', 'nuclear-norm']"
2,Products of adjugate matrices,Products of adjugate matrices,,"Let $S$ and $A$ be a symmetric and a skew-symmetric $n \times n$ matrix over $\mathbb{R}$, respectively. When calculating (numerically) the product $S^{-1} A S^{-1}$ I keep getting the factor $\det S$ in the denominator, while I would expect to get the square $$S^{-1} A S^{-1} = \frac{(\text{adj }S) A (\text{adj }S)}{(\det S)^2},$$ where $\text{adj }S$ is the adjugate of $S$. Is there a way to prove that the combination $(\text{adj }S) A (\text{adj }S)$ already contains a factor of $\det S$?","Let $S$ and $A$ be a symmetric and a skew-symmetric $n \times n$ matrix over $\mathbb{R}$, respectively. When calculating (numerically) the product $S^{-1} A S^{-1}$ I keep getting the factor $\det S$ in the denominator, while I would expect to get the square $$S^{-1} A S^{-1} = \frac{(\text{adj }S) A (\text{adj }S)}{(\det S)^2},$$ where $\text{adj }S$ is the adjugate of $S$. Is there a way to prove that the combination $(\text{adj }S) A (\text{adj }S)$ already contains a factor of $\det S$?",,['linear-algebra']
3,How to find whether the line is inside the polygon or outside.,How to find whether the line is inside the polygon or outside.,,I have a polygon How can i prove whether the black color line lies outside the polygon or inside the polygon . Given the coordinates of the black line and all the vertices of the polygon.,I have a polygon How can i prove whether the black color line lies outside the polygon or inside the polygon . Given the coordinates of the black line and all the vertices of the polygon.,,"['linear-algebra', 'computational-geometry']"
4,Geometrical meaning of the Column Space,Geometrical meaning of the Column Space,,"Suppose I have $2$ planes in $R^3$ and they form a system $Ax=b$. I know the NullSpace of $A$ represents geometrically the vectors that form the intersection between the 2 planes shifted to the origin. I also know that the Row Space of A represents the span of the normal vectors to the 2 planes. But i was looking for some geometrical meaning for the Column Space of $A$. They contain all the vectors $b$ that make $Ax=b$ have at least one solution, that i know. What I'm trying to see is what does the vectors from the Column Space mean in relating to the $2$ planes in $R^3$ or relating to the possible shiftings the system of planes could suffer to still yield a solution.","Suppose I have $2$ planes in $R^3$ and they form a system $Ax=b$. I know the NullSpace of $A$ represents geometrically the vectors that form the intersection between the 2 planes shifted to the origin. I also know that the Row Space of A represents the span of the normal vectors to the 2 planes. But i was looking for some geometrical meaning for the Column Space of $A$. They contain all the vectors $b$ that make $Ax=b$ have at least one solution, that i know. What I'm trying to see is what does the vectors from the Column Space mean in relating to the $2$ planes in $R^3$ or relating to the possible shiftings the system of planes could suffer to still yield a solution.",,['linear-algebra']
5,Is $A^2=I \implies A=\pm I$ necessarily true?,Is  necessarily true?,A^2=I \implies A=\pm I,$A$ is $n\times n$ matrix. How to prove whether  it is true or false $$A^2=I \implies A=\pm I$$ I was trying on $2\times 2$ case...multiplying general entries and then equating them to the identity requirements...but it is not a proof...,$A$ is $n\times n$ matrix. How to prove whether  it is true or false $$A^2=I \implies A=\pm I$$ I was trying on $2\times 2$ case...multiplying general entries and then equating them to the identity requirements...but it is not a proof...,,"['linear-algebra', 'matrices', 'examples-counterexamples']"
6,Are these two equalities equivalent: $ABA=0$ and $BA=0$? [closed],Are these two equalities equivalent:  and ? [closed],ABA=0 BA=0,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question There are two equalities as follows: $ABA=0$ , $BA=0$ where $A$ and $B$ are two $n\times n$ matrices. Are the mentioned equalities the same? if not, why? Thanks in advance.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question There are two equalities as follows: , where and are two matrices. Are the mentioned equalities the same? if not, why? Thanks in advance.",ABA=0 BA=0 A B n\times n,"['linear-algebra', 'matrices', 'examples-counterexamples', 'matrix-equations']"
7,Does a one-to-one linear transformation from $\mathbb R^4$ to $\mathbb R^3$ exist?,Does a one-to-one linear transformation from  to  exist?,\mathbb R^4 \mathbb R^3,"Is it possible to have a one-to-one (injective) linear transformation: $$f: \mathbb R^4 \to \mathbb R^3$$ If so, is it possible to prove that using dimension theorem ?","Is it possible to have a one-to-one (injective) linear transformation: $$f: \mathbb R^4 \to \mathbb R^3$$ If so, is it possible to prove that using dimension theorem ?",,"['linear-algebra', 'linear-transformations']"
8,Proving two matrices are equal,Proving two matrices are equal,,A friend and I are having some trouble with a linear algebra problem: Let $A$ and $B$ be square matrices with dimensions $n\times n$ Prove or disprove: If $A^2=B^2$ then $A=B$ or $A=-B$ It seems to be true but the rest of my class insists it's false - I can't find an example where this isn't the case - can someone shed some light on this? Thanks!,A friend and I are having some trouble with a linear algebra problem: Let $A$ and $B$ be square matrices with dimensions $n\times n$ Prove or disprove: If $A^2=B^2$ then $A=B$ or $A=-B$ It seems to be true but the rest of my class insists it's false - I can't find an example where this isn't the case - can someone shed some light on this? Thanks!,,"['linear-algebra', 'matrices']"
9,An operator $T:\mathbb{R}^4\to \mathbb{R}^4$ such that $T$ has no (real) eigenvalues.,An operator  such that  has no (real) eigenvalues.,T:\mathbb{R}^4\to \mathbb{R}^4 T,Give an example of an operator $T:\mathbb{R}^4\to \mathbb{R}^4$ such that $T$ has no (real) eigenvalues. How can I find this operator? Thanks for your help.,Give an example of an operator $T:\mathbb{R}^4\to \mathbb{R}^4$ such that $T$ has no (real) eigenvalues. How can I find this operator? Thanks for your help.,,['linear-algebra']
10,Why is this matrix neither positive nor negative semi-definite?,Why is this matrix neither positive nor negative semi-definite?,,"After some search here and on Google, I couldn't find a way to determine the definiteness of this matrix: \begin{bmatrix}0&1\\1&0\end{bmatrix} My understanding is that it should be negative semi-definite since all principal minors are $\leq 0$. However, in the sample solutions of the book I am working through it stated that it is neither positive nor negative semi-definite. A general procedure (for positive and negative semi-definiteness) would be really helpful. Why is that?","After some search here and on Google, I couldn't find a way to determine the definiteness of this matrix: \begin{bmatrix}0&1\\1&0\end{bmatrix} My understanding is that it should be negative semi-definite since all principal minors are $\leq 0$. However, in the sample solutions of the book I am working through it stated that it is neither positive nor negative semi-definite. A general procedure (for positive and negative semi-definiteness) would be really helpful. Why is that?",,"['linear-algebra', 'matrices', 'positive-definite']"
11,Are strictly upper triangular matrices nilpotent?,Are strictly upper triangular matrices nilpotent?,,An $n\times n$ matrix $A$ is called nilpotent if $A^m = 0$ for some $m\ge1$. Show that every triangular matrix with zeros on the main diagonal is nilpotent.,An $n\times n$ matrix $A$ is called nilpotent if $A^m = 0$ for some $m\ge1$. Show that every triangular matrix with zeros on the main diagonal is nilpotent.,,['linear-algebra']
12,"Linear Algebra: If $A^3 = I$, does $A$ have to be $I$?","Linear Algebra: If , does  have to be ?",A^3 = I A I,"So it's been a while since I've taken Linear Algebra, but my friend asked me a question, that I couldn't answer. If a matrix $A$ exists such that $A^3 = I$, does $A$ have to equal the identity matrix $I$? My first instinct was to say no, but... (edited out my incorrect math) EDIT: thanks guys for the awesome examples EDIT2: Followup question: Is there a way to solve for all possibilities of A if given A^3 = I?","So it's been a while since I've taken Linear Algebra, but my friend asked me a question, that I couldn't answer. If a matrix $A$ exists such that $A^3 = I$, does $A$ have to equal the identity matrix $I$? My first instinct was to say no, but... (edited out my incorrect math) EDIT: thanks guys for the awesome examples EDIT2: Followup question: Is there a way to solve for all possibilities of A if given A^3 = I?",,"['linear-algebra', 'numerical-linear-algebra', 'multilinear-algebra', 'symplectic-linear-algebra']"
13,Prove that this vector space is not finite dimensional. [duplicate],Prove that this vector space is not finite dimensional. [duplicate],,"This question already has answers here : Is there a quick proof as to why the vector space of $\mathbb{R}$ over $\mathbb{Q}$ is infinite-dimensional? (7 answers) Closed 9 years ago . Let $V$ be the set of real numbers. Regard V as a vector space over the field of rational numbers $F$ with the usual operations. Prove that this vector space is not finite dimensional. My attempt: Let $\beta$ be the basis of $V$ such that $\beta =\{\alpha_1, \alpha_2, ..., \alpha_n\}$. $\quad\therefore\;\forall\;\alpha\in span(\beta),\;\alpha = c_1\alpha_1 + c_2\alpha_2 + ..... c_n\alpha_n$ where $c_1, c_2, ...., c_n \in F$. Now I have to show there exists $\alpha \in V$ such that $\alpha \notin span(\beta)$ but I cant figure out which $\alpha$ will not belong in $span(\beta)$.","This question already has answers here : Is there a quick proof as to why the vector space of $\mathbb{R}$ over $\mathbb{Q}$ is infinite-dimensional? (7 answers) Closed 9 years ago . Let $V$ be the set of real numbers. Regard V as a vector space over the field of rational numbers $F$ with the usual operations. Prove that this vector space is not finite dimensional. My attempt: Let $\beta$ be the basis of $V$ such that $\beta =\{\alpha_1, \alpha_2, ..., \alpha_n\}$. $\quad\therefore\;\forall\;\alpha\in span(\beta),\;\alpha = c_1\alpha_1 + c_2\alpha_2 + ..... c_n\alpha_n$ where $c_1, c_2, ...., c_n \in F$. Now I have to show there exists $\alpha \in V$ such that $\alpha \notin span(\beta)$ but I cant figure out which $\alpha$ will not belong in $span(\beta)$.",,"['linear-algebra', 'vector-spaces']"
14,$ABCD = I$ then $B^{-1} =?$,then,ABCD = I B^{-1} =?,"I got this question in a practice book. A,B,C and D are $n\times n$ matrices with non-zero determinant. $ABCD = I$ , then $B^{-1}$ = ? The answer to this was $B^{-1}= CDA$. How was that answer arrived at ?","I got this question in a practice book. A,B,C and D are $n\times n$ matrices with non-zero determinant. $ABCD = I$ , then $B^{-1}$ = ? The answer to this was $B^{-1}= CDA$. How was that answer arrived at ?",,"['linear-algebra', 'matrices']"
15,Is the identity matrix the only matrix which is its own inverse?,Is the identity matrix the only matrix which is its own inverse?,,"I just gave a proof for this question . Here's my follow up question: Let $A \in \ \mathbb{M}_n(\mathbb{F})$ where F is a field and there exists $n\in N$  where $A^n$= I. In the case where n=1,2, $A^1$=I and $A^2$=I. Here's my question: In general, if A is it's own inverse, then does it necessarily follow A=I? In other words, is I the only matrix which is it's own inverse? My gut reaction is to say no, but it would probably be fairly tedious to construct a matrix multiplication formula which produces the subset $S\subset \mathbb{M}_n(\mathbb{R})$ where S = {A | AA =I }. Is there such a subset in general? We know the set's nonempty since $I\in S$. Are there any others?","I just gave a proof for this question . Here's my follow up question: Let $A \in \ \mathbb{M}_n(\mathbb{F})$ where F is a field and there exists $n\in N$  where $A^n$= I. In the case where n=1,2, $A^1$=I and $A^2$=I. Here's my question: In general, if A is it's own inverse, then does it necessarily follow A=I? In other words, is I the only matrix which is it's own inverse? My gut reaction is to say no, but it would probably be fairly tedious to construct a matrix multiplication formula which produces the subset $S\subset \mathbb{M}_n(\mathbb{R})$ where S = {A | AA =I }. Is there such a subset in general? We know the set's nonempty since $I\in S$. Are there any others?",,"['linear-algebra', 'abstract-algebra', 'matrices']"
16,Find all invertible $n\times n$ matrices $A$ such that $A^2 + A = 0$,Find all invertible  matrices  such that,n\times n A A^2 + A = 0,"This was a question on one of our practice midterms: Find all invertible $n \times n$ matrices $A$ such that $$A^2 + A = 0.$$ I was told to expand $A^2$ and then solve, but that seems like a really ugly (and hard-to-generalize) solution... are there any better ones?","This was a question on one of our practice midterms: Find all invertible $n \times n$ matrices $A$ such that $$A^2 + A = 0.$$ I was told to expand $A^2$ and then solve, but that seems like a really ugly (and hard-to-generalize) solution... are there any better ones?",,"['linear-algebra', 'matrices']"
17,Let $A_{n\times n}$ be a real matrix. Is it true that $I+A^TA$ is invertible?,Let  be a real matrix. Is it true that  is invertible?,A_{n\times n} I+A^TA,Let $A_{n\times n}$ be a real matrix. Is it true that $I+A^TA$ is always invertible?,Let be a real matrix. Is it true that is always invertible?,A_{n\times n} I+A^TA,"['linear-algebra', 'matrices', 'inverse']"
18,For what values of $a$ will the lines $ay+3x=4$ and $2y+4x=3$ have no intersection points?,For what values of  will the lines  and  have no intersection points?,a ay+3x=4 2y+4x=3,"Multiple choice question: For what values of $a$ will the lines $ay+3x=4$ and $2y+4x=3$ have no intersection points? A)  2 B) 1.5 C) 8/3 D) -0.5 E) -2 I tried rearranging them into $y=mx+b$ form and solve, but I will end up with a literal answer like this $$a=\frac{(4-3x)}{y}$$ How do you work this out? I think you need to make the gradient the same but how do you do that?","Multiple choice question: For what values of $a$ will the lines $ay+3x=4$ and $2y+4x=3$ have no intersection points? A)  2 B) 1.5 C) 8/3 D) -0.5 E) -2 I tried rearranging them into $y=mx+b$ form and solve, but I will end up with a literal answer like this $$a=\frac{(4-3x)}{y}$$ How do you work this out? I think you need to make the gradient the same but how do you do that?",,"['linear-algebra', 'systems-of-equations']"
19,Finding eigenvalues shortcut,Finding eigenvalues shortcut,,"I am being asked to find the eigenvalues for this matrix. It mentions that some tricks can be used instead of having to use $det(A-\lambda I)$. I understand how to do it that way, but what is a shortcut I can use for this matrix? Thanks. Solution is $(\lambda -2)^2 \lambda ^ 2$ $\lambda = 0$ and $\lambda = 2.$ $$         \begin{bmatrix}         1 & 0 & 1 & 0\\         0 & 1 & 0 & 1\\         1 & 0 & 1 & 0\\         0 & 1 & 0 & 1\\         \end{bmatrix} $$","I am being asked to find the eigenvalues for this matrix. It mentions that some tricks can be used instead of having to use $det(A-\lambda I)$. I understand how to do it that way, but what is a shortcut I can use for this matrix? Thanks. Solution is $(\lambda -2)^2 \lambda ^ 2$ $\lambda = 0$ and $\lambda = 2.$ $$         \begin{bmatrix}         1 & 0 & 1 & 0\\         0 & 1 & 0 & 1\\         1 & 0 & 1 & 0\\         0 & 1 & 0 & 1\\         \end{bmatrix} $$",,['linear-algebra']
20,Is there an easy way to compute the determinant of matrix with 1's on diagonal and a's on anti-diagonal?,Is there an easy way to compute the determinant of matrix with 1's on diagonal and a's on anti-diagonal?,,\begin{bmatrix} 1 & 0 & 0 & 0 & 0 & a \\ 0 & 1 & 0 & 0 & a & 0 \\0 & 0 & 1 & a & 0 & a \\0 & 0 & a & 1 & 0 & a \\0 & a & 0 & 0 & 1 & 0 \\a & 0 & 0 & 0 & 0 & 1 \\ \end{bmatrix} Thanks,\begin{bmatrix} 1 & 0 & 0 & 0 & 0 & a \\ 0 & 1 & 0 & 0 & a & 0 \\0 & 0 & 1 & a & 0 & a \\0 & 0 & a & 1 & 0 & a \\0 & a & 0 & 0 & 1 & 0 \\a & 0 & 0 & 0 & 0 & 1 \\ \end{bmatrix} Thanks,,"['linear-algebra', 'matrices', 'determinant']"
21,How to prove that a nilpotent matrix is not invertible?,How to prove that a nilpotent matrix is not invertible?,,"Let $B_{n\times n}$ be a nilpotent matrix. How should I go around proving that $B$ is not invertible? I was thinking this: if $B*B = 0$ , then if I rank $B$ to echelon form, I will always have two or more rows that are the same, and then because $B$ is square, it is not possible to find another matrix $(B^{-1})$ that will transform $B$ to $I_{n}$ ... But I don't know how to write it formally.","Let be a nilpotent matrix. How should I go around proving that is not invertible? I was thinking this: if , then if I rank to echelon form, I will always have two or more rows that are the same, and then because is square, it is not possible to find another matrix that will transform to ... But I don't know how to write it formally.",B_{n\times n} B B*B = 0 B B (B^{-1}) B I_{n},"['linear-algebra', 'matrices']"
22,which vectors are perpendicular to each other? [closed],which vectors are perpendicular to each other? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question which vectors are perpendicular to each other? $\vec a = (1, -2, 3)$, $\vec b = (5, 4, 1)$, $\vec c = (1, 0, -5)$ Do i just take the dot product of 2 of them. If the dot product they are at $90^\circ$? But how do i know if there perpendicular?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question which vectors are perpendicular to each other? $\vec a = (1, -2, 3)$, $\vec b = (5, 4, 1)$, $\vec c = (1, 0, -5)$ Do i just take the dot product of 2 of them. If the dot product they are at $90^\circ$? But how do i know if there perpendicular?",,"['linear-algebra', 'vector-spaces']"
23,"Area of a parallelogram, vertices $(-1,-1), (4,1), (5,3), (10,5)$.","Area of a parallelogram, vertices .","(-1,-1), (4,1), (5,3), (10,5)","I need to find the area of a parallelogram with vertices $(-1,-1), (4,1), (5,3), (10,5)$. If I denote $A=(-1,-1)$, $B=(4,1)$, $C=(5,3)$, $D=(10,5)$, then I see that $\overrightarrow{AB}=(5,2)=\overrightarrow{CD}$. Similarly $\overrightarrow{AC}=\overrightarrow{BD}$. So I see that these points indeed form a parallelogram. It is assignment from linear algebra class. I wasn't sure if I had to like use a matrix or something.","I need to find the area of a parallelogram with vertices $(-1,-1), (4,1), (5,3), (10,5)$. If I denote $A=(-1,-1)$, $B=(4,1)$, $C=(5,3)$, $D=(10,5)$, then I see that $\overrightarrow{AB}=(5,2)=\overrightarrow{CD}$. Similarly $\overrightarrow{AC}=\overrightarrow{BD}$. So I see that these points indeed form a parallelogram. It is assignment from linear algebra class. I wasn't sure if I had to like use a matrix or something.",,"['linear-algebra', 'geometry', 'determinant', 'area', 'cross-product']"
24,There are three lights which can be in one of three states. Can we get the system of lights into a specific state?,There are three lights which can be in one of three states. Can we get the system of lights into a specific state?,,"If there is someone who can come up with a better title for this please edit the title. There are three lights in a line.  Each light can be in one of three states:  off, light red, and dark red.  There is a cycle of states:  OFF, then LIGHT RED, then DARK RED, then back to OFF. There are three switches which control the lights like so: Switch A - advances the cycle for the first two lights Switch B - advances the cycle for the all three lights Switch C - advances the cycle for the last two lights. If we start with all three lights in the off state can the switches be pushed in some order so that the three lights in the line are in:  OFF-LIGHT RED-DARK RED? I'm trying to model this with linear algebra.  Where A,B,C are the lights in a row and we push A x times, B y times, and C z times.  Of course the numbers are mod 3 because after 3 pushes we wrap back to the off state. Any suggestions?","If there is someone who can come up with a better title for this please edit the title. There are three lights in a line.  Each light can be in one of three states:  off, light red, and dark red.  There is a cycle of states:  OFF, then LIGHT RED, then DARK RED, then back to OFF. There are three switches which control the lights like so: Switch A - advances the cycle for the first two lights Switch B - advances the cycle for the all three lights Switch C - advances the cycle for the last two lights. If we start with all three lights in the off state can the switches be pushed in some order so that the three lights in the line are in:  OFF-LIGHT RED-DARK RED? I'm trying to model this with linear algebra.  Where A,B,C are the lights in a row and we push A x times, B y times, and C z times.  Of course the numbers are mod 3 because after 3 pushes we wrap back to the off state. Any suggestions?",,"['linear-algebra', 'modular-arithmetic']"
25,Proving $A^2 = 0$ given $A^5 = 0$ [duplicate],Proving  given  [duplicate],A^2 = 0 A^5 = 0,This question already has answers here : Show that for a $2\times 2$ matrix $A^2=0$ (2 answers) Closed 9 years ago . I have a class question where I must prove $A^2 = 0$ given $A^5 = 0$ with A being a 2x2 matrix.   I though that I could simply say that as $A^5 = 0$ then $A^2 \cdot A^3 = 0 \implies A^2 = 0$ as $A^2 = A\cdot A$ and $A^3 = A\cdot A\cdot A$ $\implies A = 0 \implies A^2 = 0$. Could someone verify that my proof is valid and not just a circular argument (which I feel it may be)  Thanks,This question already has answers here : Show that for a $2\times 2$ matrix $A^2=0$ (2 answers) Closed 9 years ago . I have a class question where I must prove $A^2 = 0$ given $A^5 = 0$ with A being a 2x2 matrix.   I though that I could simply say that as $A^5 = 0$ then $A^2 \cdot A^3 = 0 \implies A^2 = 0$ as $A^2 = A\cdot A$ and $A^3 = A\cdot A\cdot A$ $\implies A = 0 \implies A^2 = 0$. Could someone verify that my proof is valid and not just a circular argument (which I feel it may be)  Thanks,,['linear-algebra']
26,"If matrix $AB=A$, does it mean B must be an identity matrix?","If matrix , does it mean B must be an identity matrix?",AB=A,"If matrix multiplication $AB=A$, does it mean $B$ must be an identity matrix? If not, why? What conditions? $A$ is not a zero matrix.","If matrix multiplication $AB=A$, does it mean $B$ must be an identity matrix? If not, why? What conditions? $A$ is not a zero matrix.",,"['linear-algebra', 'matrices', 'matrix-equations', 'matrix-calculus']"
27,Is $T$ singular or nonsingular for $T(A)=AB - BA$?,Is  singular or nonsingular for ?,T T(A)=AB - BA,Let $B$ be a complex $n\times n$ matrix. Prove or disprove: The linear operator $T$ on the space of all $n\times n$ matrices defined by $ T(A) = AB - BA $ is singular.,Let $B$ be a complex $n\times n$ matrix. Prove or disprove: The linear operator $T$ on the space of all $n\times n$ matrices defined by $ T(A) = AB - BA $ is singular.,,"['linear-algebra', 'matrices']"
28,Factorise the determinant $\det\Bigl(\begin{smallmatrix} a^3+a^2 & a & 1 \\ b^3+b^2 & b & 1 \\ c^3+c^2 & c &1\end{smallmatrix}\Bigr)$,Factorise the determinant,\det\Bigl(\begin{smallmatrix} a^3+a^2 & a & 1 \\ b^3+b^2 & b & 1 \\ c^3+c^2 & c &1\end{smallmatrix}\Bigr),Factorise the determinant $\det\begin{pmatrix} a^3+a^2 & a & 1 \\ b^3+b^2 & b & 1 \\ c^3+c^2 & c &1\end{pmatrix}$. My textbook only provides two simple examples. Really have no idea how to do this type of questions..,Factorise the determinant $\det\begin{pmatrix} a^3+a^2 & a & 1 \\ b^3+b^2 & b & 1 \\ c^3+c^2 & c &1\end{pmatrix}$. My textbook only provides two simple examples. Really have no idea how to do this type of questions..,,"['linear-algebra', 'determinant']"
29,"If $A$ is symmetric, then the matrix exponential $e^{A}$ is positive definite","If  is symmetric, then the matrix exponential  is positive definite",A e^{A},"This is a homework problem, but I feel like I'm struggling with not knowing facts from linear algebra.  Apparently this is supposed to be an easy question but I hit a brick wall at the following point.  Can anyone give me a tip or tell me if I'm going down the wrong road? My approach: Let $A$ be a symmetric $n\times n$ matrix, and then suppose $x$ is a real-valued column vector of dimension $n$ with not all entries equal to $0$.  I need to show that $x^Te^{A}x> 0$. (based on looking up the definition of positive definite on wikipedia) Then $\begin{eqnarray*} x^{T}(e^{A})x &=& x^{T}(\sum_{n=0}^{\infty}\frac{A^{n}}{n!})x\\ &=& \sum_{n=0}^{\infty}\frac{x^{T}A^{n}x}{n!}\\ &=& \sum_{n=0}^{\infty}\frac{x^{T}A^{n}x}{n!})\\ \end{eqnarray*}$ As noted below, from here on is incorrect: $\begin{eqnarray*} &=& \sum_{n=0}^{\infty}\frac{(x^{T}Ax)^{n}}{n!\|x\|^{2(n-1)}})\\ &=& \sum_{n=0}^{\infty}\frac{(x^{T}A^{T}x)^{n}}{n!\|x\|^{2(n-1)}}\\ &=& \sum_{n=0}^{\infty}\frac{((Ax)^{T}x)^{n}}{n!\|x\|^{2(n-1)}}\\ &=& \sum_{n=0}^{\infty}\frac{((Ax)^{T}(x^{T})^{T})^{n}}{n!\|x\|^{2(n-1)}}\\ &=& \sum_{n=0}^{\infty}\frac{((x^{T}Ax)^{T})^{n}}{n!\|x\|^{2(n-1)}}\\ \end{eqnarray*}$ As you can see this brings me no closer to getting $x^Tx$ somewhere, which I may assume is greater than $0$. So my conclusion is that using only the fact that $A$ is symmetric is not enough.  Is there some result about symmetric matrices that I should use?","This is a homework problem, but I feel like I'm struggling with not knowing facts from linear algebra.  Apparently this is supposed to be an easy question but I hit a brick wall at the following point.  Can anyone give me a tip or tell me if I'm going down the wrong road? My approach: Let $A$ be a symmetric $n\times n$ matrix, and then suppose $x$ is a real-valued column vector of dimension $n$ with not all entries equal to $0$.  I need to show that $x^Te^{A}x> 0$. (based on looking up the definition of positive definite on wikipedia) Then $\begin{eqnarray*} x^{T}(e^{A})x &=& x^{T}(\sum_{n=0}^{\infty}\frac{A^{n}}{n!})x\\ &=& \sum_{n=0}^{\infty}\frac{x^{T}A^{n}x}{n!}\\ &=& \sum_{n=0}^{\infty}\frac{x^{T}A^{n}x}{n!})\\ \end{eqnarray*}$ As noted below, from here on is incorrect: $\begin{eqnarray*} &=& \sum_{n=0}^{\infty}\frac{(x^{T}Ax)^{n}}{n!\|x\|^{2(n-1)}})\\ &=& \sum_{n=0}^{\infty}\frac{(x^{T}A^{T}x)^{n}}{n!\|x\|^{2(n-1)}}\\ &=& \sum_{n=0}^{\infty}\frac{((Ax)^{T}x)^{n}}{n!\|x\|^{2(n-1)}}\\ &=& \sum_{n=0}^{\infty}\frac{((Ax)^{T}(x^{T})^{T})^{n}}{n!\|x\|^{2(n-1)}}\\ &=& \sum_{n=0}^{\infty}\frac{((x^{T}Ax)^{T})^{n}}{n!\|x\|^{2(n-1)}}\\ \end{eqnarray*}$ As you can see this brings me no closer to getting $x^Tx$ somewhere, which I may assume is greater than $0$. So my conclusion is that using only the fact that $A$ is symmetric is not enough.  Is there some result about symmetric matrices that I should use?",,"['linear-algebra', 'ordinary-differential-equations', 'positive-definite', 'symmetric-matrices', 'matrix-exponential']"
30,What are the eigenvalues of a tridiagonal Toeplitz matrix?,What are the eigenvalues of a tridiagonal Toeplitz matrix?,,"Consider a matrix $M \in \mathbb{R}^{n \times n}$ in the form: $$ M = \begin{bmatrix}         \alpha & \beta  &  0     & \cdots & 0                 \\        \gamma & \alpha & \beta  & \cdots & 0                 \\         0     & \gamma & \alpha & \cdots & \vdots            \\        \vdots & \vdots & \ddots & \ddots & \beta             \\         0     &  0     & \cdots & \gamma & \operatorname{\alpha}        \end{bmatrix} $$ Then a closed form expression for the eigenvalues is $$\mu_k = \alpha + 2 \beta \sqrt{\frac{\gamma}{\beta}} \cos \left( \frac{k \pi}{n+1} \right)$$ However, this also requires that $\beta \neq 0$ , since the corresponding eigenvectors contain a division by $\beta$ . Therefore, I am wondering if there exists a separate general formula for the case where $\beta = 0$ . This simplifies the above matrix $M$ to: $$ \begin{bmatrix}  \alpha & 0 & 0 & \cdots & 0 \\    \gamma & \alpha & 0 & \cdots & 0 \\    0 & \gamma & \alpha & \cdots & \vdots \\    \vdots  & \vdots  & \ddots & \ddots & 0 \\    0 & 0 & \cdots & \gamma & \operatorname{\alpha}  \end{bmatrix}$$ Of course, it is simple enough to compute this manually for smaller matrices, but I would be interested in knowing what the general solution is in this case. Note: the corresponding eigenvectors in the case where $\beta \neq 0$ are: $$v_k = \Big{(} \sqrt{ \frac{\gamma}{\beta}} \sin \big{(} \frac{ k \pi}{n+1} \big{)}, \big{(} \sqrt{ \frac{\gamma}{\beta}} \space \big{)}^2 \sin \big{(} \frac{ 2k \pi}{n+1} \big{)}, \cdots, \big{(} \sqrt{\frac{\gamma}{\beta}} \space \big{)}^n \sin \big{(} \frac{ nk \pi}{n+1} \big{)} \Big{)}^T$$","Consider a matrix in the form: Then a closed form expression for the eigenvalues is However, this also requires that , since the corresponding eigenvectors contain a division by . Therefore, I am wondering if there exists a separate general formula for the case where . This simplifies the above matrix to: Of course, it is simple enough to compute this manually for smaller matrices, but I would be interested in knowing what the general solution is in this case. Note: the corresponding eigenvectors in the case where are:","M \in \mathbb{R}^{n \times n}  M = \begin{bmatrix} 
       \alpha & \beta  &  0     & \cdots & 0                 \\
       \gamma & \alpha & \beta  & \cdots & 0                 \\
        0     & \gamma & \alpha & \cdots & \vdots            \\
       \vdots & \vdots & \ddots & \ddots & \beta             \\
        0     &  0     & \cdots & \gamma & \operatorname{\alpha}
       \end{bmatrix}  \mu_k = \alpha + 2 \beta \sqrt{\frac{\gamma}{\beta}} \cos \left( \frac{k \pi}{n+1} \right) \beta \neq 0 \beta \beta = 0 M 
\begin{bmatrix} 
\alpha & 0 & 0 & \cdots & 0 \\
   \gamma & \alpha & 0 & \cdots & 0 \\
   0 & \gamma & \alpha & \cdots & \vdots \\
   \vdots  & \vdots  & \ddots & \ddots & 0 \\
   0 & 0 & \cdots & \gamma & \operatorname{\alpha}
 \end{bmatrix} \beta \neq 0 v_k = \Big{(} \sqrt{ \frac{\gamma}{\beta}} \sin \big{(} \frac{ k \pi}{n+1} \big{)}, \big{(} \sqrt{ \frac{\gamma}{\beta}} \space \big{)}^2 \sin \big{(} \frac{ 2k \pi}{n+1} \big{)}, \cdots, \big{(} \sqrt{\frac{\gamma}{\beta}} \space \big{)}^n \sin \big{(} \frac{ nk \pi}{n+1} \big{)} \Big{)}^T","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'tridiagonal-matrices', 'toeplitz-matrices']"
31,Why linear maps act like matrix multiplication?,Why linear maps act like matrix multiplication?,,"In Linear Algebra Done Right , it said Suppose $T \in \mathcal{L}(V,W)$ and $v \in V$ . Suppose $v_1,...,v_n$ is a basis of $V$ and $w_1,...,w_m$ is a basis of $W$ . Then $$M(Tv) = M(T)M(v)$$ $M(T)$ is the m-by-n matrix whose entries $A_{j,k}$ are defined by $Tv_k = A_{1,k}w_1 + ... + A_{m,k}w_m$ suppose $T \in \mathcal{L}(V,W)$ and $v_1,...,v_n$ is a basis of $V$ and $w_1,...,w_m$ is a basis of $W$ . $M(v)$ is the matrix of vector $v$ . I generally follow the following proof: Suppose $v = c_1v_1 + ... + c_nv_n$ , where $c_1,...,c_n \in \mathbb{F}$ . Thus $$Tv = c_1Tv_1 +...+c_nTv_n$$ Hence \begin{equation} \begin{split} M(Tv) &= c_1M(Tv_1) + ...+ c_nM(Tv_n)\\ & = c_1M(T)_{.,1} +...+c_nM(T)_{.,n} \\ & = M(T)M(v) \end{split} \end{equation} But I have questions on the meaning of the proof. The book said it means each m-by-n matrix $A$ induces a linear map from $\mathbb{F}^{n,1}$ to $\mathbb{F}^{m,1}$ . The result can be used to think of every linear map as a matrix multiplication map after suitable relabeling via the isomorphisms given by $M$ . Is the shape of $M(Tv)$ m by 1, $M(T)$ m by n, and $M(v)$ n by 1? What is meant by suitable relabeling via the isomorphisms given by $M$ ? Does it just mean $M(T)$ is a isomorphism linear map between $M(v)$ and $M(Tv)$ ?","In Linear Algebra Done Right , it said Suppose and . Suppose is a basis of and is a basis of . Then is the m-by-n matrix whose entries are defined by suppose and is a basis of and is a basis of . is the matrix of vector . I generally follow the following proof: Suppose , where . Thus Hence But I have questions on the meaning of the proof. The book said it means each m-by-n matrix induces a linear map from to . The result can be used to think of every linear map as a matrix multiplication map after suitable relabeling via the isomorphisms given by . Is the shape of m by 1, m by n, and n by 1? What is meant by suitable relabeling via the isomorphisms given by ? Does it just mean is a isomorphism linear map between and ?","T \in \mathcal{L}(V,W) v \in V v_1,...,v_n V w_1,...,w_m W M(Tv) = M(T)M(v) M(T) A_{j,k} Tv_k = A_{1,k}w_1 + ... + A_{m,k}w_m T \in \mathcal{L}(V,W) v_1,...,v_n V w_1,...,w_m W M(v) v v = c_1v_1 + ... + c_nv_n c_1,...,c_n \in \mathbb{F} Tv = c_1Tv_1 +...+c_nTv_n \begin{equation}
\begin{split}
M(Tv) &= c_1M(Tv_1) + ...+ c_nM(Tv_n)\\
& = c_1M(T)_{.,1} +...+c_nM(T)_{.,n} \\
& = M(T)M(v)
\end{split}
\end{equation} A \mathbb{F}^{n,1} \mathbb{F}^{m,1} M M(Tv) M(T) M(v) M M(T) M(v) M(Tv)","['linear-algebra', 'matrices', 'linear-transformations']"
32,Is rank of submatrix less than or equal to rank of matrix?,Is rank of submatrix less than or equal to rank of matrix?,,"OK, so I realize this might be a stupid question but an answer can certainly help me in my matrix theory class, I need to know if in general the rank of a submatrix is less than or equal to the rank of the larger matrix? Is it true in general?","OK, so I realize this might be a stupid question but an answer can certainly help me in my matrix theory class, I need to know if in general the rank of a submatrix is less than or equal to the rank of the larger matrix? Is it true in general?",,"['linear-algebra', 'matrices', 'matrix-rank']"
33,"If $A=AA^{\top}$, show that $A^2=A$","If , show that",A=AA^{\top} A^2=A,"I've been working trying to understand the following question: Let n be a positive integer, let $F$ be a field, and let $A \in \mathrm{Mat}(n,F)$ satisfy the condition $A=AA^{\top}$. Show that $A^2=A$. I haven't made much progress since my knowledge is pretty basic but I ran across this link and was wondering if this example was essentially the same? Example","I've been working trying to understand the following question: Let n be a positive integer, let $F$ be a field, and let $A \in \mathrm{Mat}(n,F)$ satisfy the condition $A=AA^{\top}$. Show that $A^2=A$. I haven't made much progress since my knowledge is pretty basic but I ran across this link and was wondering if this example was essentially the same? Example",,"['linear-algebra', 'matrices']"
34,Why is $\cos(\alpha+\beta) = \cos(\alpha)\cos(\beta)−\sin(\alpha)\sin(\beta)$?,Why is ?,\cos(\alpha+\beta) = \cos(\alpha)\cos(\beta)−\sin(\alpha)\sin(\beta),I already posted a question about transformation matrices and rotation . But I'm not satisfied with the answer. They simply said Composition of functions corresponds to multiplication of matrices. I think I understand the concept but I'm still confused why exactly $$\cos(\alpha + \beta)=\cos(\alpha)\cos(\beta)−\sin(\alpha)\sin(\beta)$$ Is there a lemma or formula I have to use or does it simply derive from distributivity of matrix multiplication? I can't get my head around it.,I already posted a question about transformation matrices and rotation . But I'm not satisfied with the answer. They simply said Composition of functions corresponds to multiplication of matrices. I think I understand the concept but I'm still confused why exactly Is there a lemma or formula I have to use or does it simply derive from distributivity of matrix multiplication? I can't get my head around it.,\cos(\alpha + \beta)=\cos(\alpha)\cos(\beta)−\sin(\alpha)\sin(\beta),"['linear-algebra', 'matrices', 'rotations', 'transformation']"
35,Is $A$ the $2 × 2$ identity matrix?,Is  the  identity matrix?,A 2 × 2,"If $A$ is a $2 × 2$ complex matrix that is invertible and diagonalizable, and such that $A$ and $A^2$ have the same characteristic polynomial, then $A$ is the $2 × 2$ identity matrix. My claim: Eigenvalues of $A^2$ are square of eigenvalue of $A$ $$\lambda=\lambda^2$$ Since invertible $\lambda=1$ hence similar to identity matrix. But only matrix similar to identity is identity itself. But answer is given as FALSE. please explain me why i'm wrong","If is a complex matrix that is invertible and diagonalizable, and such that and have the same characteristic polynomial, then is the identity matrix. My claim: Eigenvalues of are square of eigenvalue of Since invertible hence similar to identity matrix. But only matrix similar to identity is identity itself. But answer is given as FALSE. please explain me why i'm wrong",A 2 × 2 A A^2 A 2 × 2 A^2 A \lambda=\lambda^2 \lambda=1,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'fake-proofs', 'characteristic-polynomial']"
36,Is a vector space a subspace of itself?,Is a vector space a subspace of itself?,,"We know that a subspace (of a vector space $V$ ) is a vector space that follows the same addition and multiplication rules as $V$ , but is a vector space a subspace of itself? Also, I'm getting confused doing the practice questions, on when we prove that something is a vector space by using the subspace test and when we prove V1 - V10, which are the ten axioms of vector spaces. So for example in $\Bbb R^2$ , we have that $\vec{x} + \vec{y} = \vec{y} + \vec {x}$ , etc..","We know that a subspace (of a vector space ) is a vector space that follows the same addition and multiplication rules as , but is a vector space a subspace of itself? Also, I'm getting confused doing the practice questions, on when we prove that something is a vector space by using the subspace test and when we prove V1 - V10, which are the ten axioms of vector spaces. So for example in , we have that , etc..",V V \Bbb R^2 \vec{x} + \vec{y} = \vec{y} + \vec {x},"['linear-algebra', 'vector-spaces']"
37,A 2x2 matrix $M$ exists. Suppose $M^3=0$ show that (I want proof) $M^2=0$,A 2x2 matrix  exists. Suppose  show that (I want proof),M M^3=0 M^2=0,"I'm sure I've done this before in abstract algebra. Regardless it's escaped me now. I have proved that for $T:U\rightarrow V$, with $dim(U)=m$ and $dim(V)=n$ that $rank(T)\le m$ which is obvious, but I have proved it none the less. I want to.... this is where I get stuck. I'm not quite sure how to say it. Suppose we have a $T$ for example that takes $U\subset\mathbb{R}^3\rightarrow V\subset\mathbb{R}^3$, we could still have a $T$ that maps a plane to something (a plane, line or point) in this case rather than $rank(T)\le 3$ I can state $rank(T)\le 2$. (Correct my notation here, I don't like writing subset, I mean number of dimensions of the space!) To show this I can say $F:P\subset\mathbb{R}^2\rightarrow U\subset\mathbb{R}^3$. Then let $G:P\rightarrow V$ and that $G=TF$, now $rank(G)\le dim(P) = 2$ Now, if $U,P,Q,R$ are spaces of dimension $\le 2$ and P,Q,R are subspaces of U $A:U\rightarrow P$ $B=A:P\rightarrow Q$ $C=A:Q\rightarrow R$ We can now say $A^2=BA$ then $A^3=CBA$ We know that for $A$ rank(A)$\le 2$, this thus provides an upper-bound for rank($A^3$) I want to show now that if I keep applying a linear transformation that the rank is a monotonically decreasing ($\le$) sequence. I am unsure on a proof that the rank of a transformation cannot be greater than the domain's dimensions. Although this is trivial. (NOTE: I have proven that if a set of n vectors, R, span a vector space V, and you take a set W of m linearly independent vectors in V that m$\le$n) If rank(A)=2 then rank($A^2$)=2 and so rank($A^3$)=2 if rank(A)=1 then rank($A^2$)=0 and rank($A^3$)=0 if rank(A)=0 then rank($A^2$)=0 and rank($A^3$)=0 But again, I can't prove this, or at least I am unsure of how to write it. something to clarify: For a map $T:U\rightarrow V$ how do we distinguish (using notation) whether or not we actually use all dim(U)? For example we might have a T that maps a plane in R^3 to a line in R^3, this can be expressed as a composition of maps, one that takes a 2 dimensional vector space (coordinates of the plane in 3 space) to a 3 dimensional point in T's domain, which T then maps to a line. We have rank - the dimensions of the image - to make this distinction on the map's target (the rank of T is 1 if it maps something to a line, but the dimensions of the target is 3) How do we write this? What do I say to describe this? How do I distinguish between the number of vectors in a basis and the number of dimensions the space that basis is in has? Sorry for the length of this! I tried to talk though my problem in the hope of seeing it myself, no such luck, thanks.","I'm sure I've done this before in abstract algebra. Regardless it's escaped me now. I have proved that for $T:U\rightarrow V$, with $dim(U)=m$ and $dim(V)=n$ that $rank(T)\le m$ which is obvious, but I have proved it none the less. I want to.... this is where I get stuck. I'm not quite sure how to say it. Suppose we have a $T$ for example that takes $U\subset\mathbb{R}^3\rightarrow V\subset\mathbb{R}^3$, we could still have a $T$ that maps a plane to something (a plane, line or point) in this case rather than $rank(T)\le 3$ I can state $rank(T)\le 2$. (Correct my notation here, I don't like writing subset, I mean number of dimensions of the space!) To show this I can say $F:P\subset\mathbb{R}^2\rightarrow U\subset\mathbb{R}^3$. Then let $G:P\rightarrow V$ and that $G=TF$, now $rank(G)\le dim(P) = 2$ Now, if $U,P,Q,R$ are spaces of dimension $\le 2$ and P,Q,R are subspaces of U $A:U\rightarrow P$ $B=A:P\rightarrow Q$ $C=A:Q\rightarrow R$ We can now say $A^2=BA$ then $A^3=CBA$ We know that for $A$ rank(A)$\le 2$, this thus provides an upper-bound for rank($A^3$) I want to show now that if I keep applying a linear transformation that the rank is a monotonically decreasing ($\le$) sequence. I am unsure on a proof that the rank of a transformation cannot be greater than the domain's dimensions. Although this is trivial. (NOTE: I have proven that if a set of n vectors, R, span a vector space V, and you take a set W of m linearly independent vectors in V that m$\le$n) If rank(A)=2 then rank($A^2$)=2 and so rank($A^3$)=2 if rank(A)=1 then rank($A^2$)=0 and rank($A^3$)=0 if rank(A)=0 then rank($A^2$)=0 and rank($A^3$)=0 But again, I can't prove this, or at least I am unsure of how to write it. something to clarify: For a map $T:U\rightarrow V$ how do we distinguish (using notation) whether or not we actually use all dim(U)? For example we might have a T that maps a plane in R^3 to a line in R^3, this can be expressed as a composition of maps, one that takes a 2 dimensional vector space (coordinates of the plane in 3 space) to a 3 dimensional point in T's domain, which T then maps to a line. We have rank - the dimensions of the image - to make this distinction on the map's target (the rank of T is 1 if it maps something to a line, but the dimensions of the target is 3) How do we write this? What do I say to describe this? How do I distinguish between the number of vectors in a basis and the number of dimensions the space that basis is in has? Sorry for the length of this! I tried to talk though my problem in the hope of seeing it myself, no such luck, thanks.",,"['linear-algebra', 'notation']"
38,Show that $\lambda$ is eigenvalue of a normal $A$ if and only if $\bar \lambda$ is eigenvalue of $A^*$,Show that  is eigenvalue of a normal  if and only if  is eigenvalue of,\lambda A \bar \lambda A^*,I want to show that $\lambda$ is an eigenvalue of a normal matrix $A$ if and only if $\overline{\lambda}$ is an eigenvalue of $A^{*}$. I am trying to show it for a while and I guess there are some simple ideas to deal with it but I am just stuck. I know that a matrix is normal if and only if it is unitarily diagonalizable but I don't want to use this property cause I believe there must be other easier solution. Any advice?,I want to show that $\lambda$ is an eigenvalue of a normal matrix $A$ if and only if $\overline{\lambda}$ is an eigenvalue of $A^{*}$. I am trying to show it for a while and I guess there are some simple ideas to deal with it but I am just stuck. I know that a matrix is normal if and only if it is unitarily diagonalizable but I don't want to use this property cause I believe there must be other easier solution. Any advice?,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
39,"$V$ is isomorphic to $V^{\ast\ast}$, the double dual space of $V$.","is isomorphic to , the double dual space of .",V V^{\ast\ast} V,"Prove that for any vector space $V$ the map sending $v$ in $V$ to (evaluation at $v$) $E_v$ in $V^{**}$  such that $E_v(\phi) = \phi(v)$ for $\phi$ in $V^*$ , is injective. Derive from this that if $\dim V < \infty$, its double dual $V^{**}$  is naturally isomorphic to $V$. Here $V^*$ is the dual space of $V$.","Prove that for any vector space $V$ the map sending $v$ in $V$ to (evaluation at $v$) $E_v$ in $V^{**}$  such that $E_v(\phi) = \phi(v)$ for $\phi$ in $V^*$ , is injective. Derive from this that if $\dim V < \infty$, its double dual $V^{**}$  is naturally isomorphic to $V$. Here $V^*$ is the dual space of $V$.",,['linear-algebra']
40,very basic question about coordinate geometry,very basic question about coordinate geometry,,"I am taking a beginner course in machine learning and have confused myself horribly about something. The idea is that there is a line which splits a 2D plane into two distinct regions as shown in the figure: Now the instructor says that anything below the line is greater than $0$ and anything above is less than $0$. Now, it is not clear to me why that should be? Is there an intuition behind why anything below this line should be greater than zero (according to the line equation) and anything above should be less than 0.","I am taking a beginner course in machine learning and have confused myself horribly about something. The idea is that there is a line which splits a 2D plane into two distinct regions as shown in the figure: Now the instructor says that anything below the line is greater than $0$ and anything above is less than $0$. Now, it is not clear to me why that should be? Is there an intuition behind why anything below this line should be greater than zero (according to the line equation) and anything above should be less than 0.",,"['linear-algebra', 'geometry', 'coordinate-systems', 'machine-learning']"
41,"What is the non-trivial, general solution of these equal ratios? [closed]","What is the non-trivial, general solution of these equal ratios? [closed]",,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Provide non-trivial solution of the following: $$\frac{a}{b+c}=\frac{b}{c+a}=\frac{c}{a+b}$$ $a=?, b=?, c=?$ The solution should be general.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Provide non-trivial solution of the following: $$\frac{a}{b+c}=\frac{b}{c+a}=\frac{c}{a+b}$$ $a=?, b=?, c=?$ The solution should be general.",,"['linear-algebra', 'algebra-precalculus', 'systems-of-equations', 'fractions']"
42,"How to show that the matrix exponential is invertible for non-diagonalizable matrix A,","How to show that the matrix exponential is invertible for non-diagonalizable matrix A,",,"I have shown the easy case, when A is diagonalizable. But I am stuck on the case when A is not. So, I put A in its Jordan canonical form.  Then say A = $SJS^{-1}$. Then $e^A = Se^JS^{-1}$, where $e^J$ is an upper triangular matrix with the Jordan blocks exponentiated. Now for each Jordan block, I have that $$e^{J_i} = e^{\lambda_i I + N}$$ where N is elementary nilpotent.  But since scalar matrices commute with all matrices, it commutes with N. Then  $$e^{J_i} = e^{\lambda_i I + N} = e^{\lambda_i}e^{N}$$ where $e^{\lambda_i}$ is a diagonal matrix with non-zero diagonal, hence it is invertible / has non-zero determinant.  What can I do with the $e^N$ factor?  I know that it has a finite expansion, since it is nilpotent: $$e^N = I + N + ... + \frac {N^k}{k!}$$ I'm not sure where to go from here... Thanks,","I have shown the easy case, when A is diagonalizable. But I am stuck on the case when A is not. So, I put A in its Jordan canonical form.  Then say A = $SJS^{-1}$. Then $e^A = Se^JS^{-1}$, where $e^J$ is an upper triangular matrix with the Jordan blocks exponentiated. Now for each Jordan block, I have that $$e^{J_i} = e^{\lambda_i I + N}$$ where N is elementary nilpotent.  But since scalar matrices commute with all matrices, it commutes with N. Then  $$e^{J_i} = e^{\lambda_i I + N} = e^{\lambda_i}e^{N}$$ where $e^{\lambda_i}$ is a diagonal matrix with non-zero diagonal, hence it is invertible / has non-zero determinant.  What can I do with the $e^N$ factor?  I know that it has a finite expansion, since it is nilpotent: $$e^N = I + N + ... + \frac {N^k}{k!}$$ I'm not sure where to go from here... Thanks,",,"['linear-algebra', 'matrices', 'power-series', 'exponential-function', 'jordan-normal-form']"
43,Intuition for the Product of Vector and Matrices: $x^TAx $,Intuition for the Product of Vector and Matrices:,x^TAx ,"When I took linear algebra, I had no trouble with the mechanical multiplication of matrices.  Given the time to write things out and mumble a bit about $i$ th and $j$ th rows, I can do the products no problem.  However, as I expand my interests and read more advanced texts, the pause to mumble and scratch is becoming a significant barrier to my comprehension. So, I ask those more experienced how best to build intuition for matrix multiplication, especially large or arbitrary matrices.  Are there any good tricks or rules of thumb that I've missed?  Does it just come with constant exposure/repetition?  How would you go about quickly interpreting (for example) the statement: $$x^TAx $$ where $A$ is a $n$ by $n$ matrix and $x$ is a $n\times 1$ matrix?","When I took linear algebra, I had no trouble with the mechanical multiplication of matrices.  Given the time to write things out and mumble a bit about th and th rows, I can do the products no problem.  However, as I expand my interests and read more advanced texts, the pause to mumble and scratch is becoming a significant barrier to my comprehension. So, I ask those more experienced how best to build intuition for matrix multiplication, especially large or arbitrary matrices.  Are there any good tricks or rules of thumb that I've missed?  Does it just come with constant exposure/repetition?  How would you go about quickly interpreting (for example) the statement: where is a by matrix and is a matrix?",i j x^TAx  A n n x n\times 1,"['linear-algebra', 'matrices', 'soft-question']"
44,"If $AA^T$ is a diagonal matrix, what can be said about $A^TA$?","If  is a diagonal matrix, what can be said about ?",AA^T A^TA,"I am trying to answer this question and any method I can think of requires a knowledge of $A^TA$ given that $AA^T=D$, where $D$ is diagonal and $A$ is a square matrix. I could not find anything useful in MSE or elsewhere and I was unable to do any progress by myself.","I am trying to answer this question and any method I can think of requires a knowledge of $A^TA$ given that $AA^T=D$, where $D$ is diagonal and $A$ is a square matrix. I could not find anything useful in MSE or elsewhere and I was unable to do any progress by myself.",,"['linear-algebra', 'matrices', 'matrix-congruences']"
45,Does $\forall x\ne 0: x^TAx>0$ means all eigenvalues of $A$ are real?,Does  means all eigenvalues of  are real?,\forall x\ne 0: x^TAx>0 A,"Let $A\in\mathbb{R}^{n\times n}$ Does $\forall x\ne 0,x\in \mathbb{R}^n: x^TAx>0$ means $A$ has only real eigenvalues (roots of the characteristic polynomial are all real)?","Let $A\in\mathbb{R}^{n\times n}$ Does $\forall x\ne 0,x\in \mathbb{R}^n: x^TAx>0$ means $A$ has only real eigenvalues (roots of the characteristic polynomial are all real)?",,['linear-algebra']
46,Some aspects of inner products in $\mathbb R^3$,Some aspects of inner products in,\mathbb R^3,"I read several descriptions about inner products recently due to an exercise ( here it is no longer active so I like to ask again). I am still very confused about this concept. Any clarification will be greatly appreciated. The very first definition of inner product I encountered is $\langle     v, w\rangle =v_1w_1+v_2w_2+v_3w_3$. Or in general, it is a function defined on $\mathbb R^3\times\mathbb R^3$ into $\mathbb R$ which satisfies certain positivity and linearity conditions. I have no problem in understanding this definition. Then people talk about the set of inner products in $\mathbb R^3$. It seems that an inner product can be represented by a matrix. Why? And how? This does not make sense to me because if we write inner product in vector notation, then we have $\langle v, w\rangle=v^Tw$. There is no matrix in this notation, is there? Moreover, what does it mean by a set of inner products? Does that mean a set of functions (since inner product can be treated as a function as indicated in 1)? What is the natural topology on the set of inner products? Can I say that the topology is the sub product topology of $\mathbb R^3\times \mathbb R^3$?","I read several descriptions about inner products recently due to an exercise ( here it is no longer active so I like to ask again). I am still very confused about this concept. Any clarification will be greatly appreciated. The very first definition of inner product I encountered is $\langle     v, w\rangle =v_1w_1+v_2w_2+v_3w_3$. Or in general, it is a function defined on $\mathbb R^3\times\mathbb R^3$ into $\mathbb R$ which satisfies certain positivity and linearity conditions. I have no problem in understanding this definition. Then people talk about the set of inner products in $\mathbb R^3$. It seems that an inner product can be represented by a matrix. Why? And how? This does not make sense to me because if we write inner product in vector notation, then we have $\langle v, w\rangle=v^Tw$. There is no matrix in this notation, is there? Moreover, what does it mean by a set of inner products? Does that mean a set of functions (since inner product can be treated as a function as indicated in 1)? What is the natural topology on the set of inner products? Can I say that the topology is the sub product topology of $\mathbb R^3\times \mathbb R^3$?",,"['linear-algebra', 'analysis', 'differential-geometry', 'self-learning']"
47,Nullspace that spans $\mathbb{R}^n$?,Nullspace that spans ?,\mathbb{R}^n,"My professor said that if for a $n \times n$ matrix $A$, $\text{null}(A) = \mathbb{R}^n$, then $A = 0_{n}$. Why is this true? I understand what its saying - if everything times this matrix is zero, then the matrix has to be zero. The intuition is simple enough with numbers, but could someone explain why this is true with matrices? Thanks","My professor said that if for a $n \times n$ matrix $A$, $\text{null}(A) = \mathbb{R}^n$, then $A = 0_{n}$. Why is this true? I understand what its saying - if everything times this matrix is zero, then the matrix has to be zero. The intuition is simple enough with numbers, but could someone explain why this is true with matrices? Thanks",,"['linear-algebra', 'matrices', 'vector-spaces']"
48,How to come up with a counter example in linear algebra,How to come up with a counter example in linear algebra,,This came up in a problem I was working on. Problem :Let $V$ be an $n$ dimensional vector space over a field $F$. Let $T:V\rightarrow V$ be a linear operator and let $W$ be a $T$ invariant subspace of $V$. Prove or give a counter example to the following: Let $U$ be another subspace of $V$ such that $V=W\oplus U$. Then $U$ is also $T$ invariant. Solution Well this is false. I came up with a counter example after spending an embarrassing amount of time guessing and checking. Consider the operator  $$ \left(\begin{array}{ccc}1 & -1 &  \\0 & 1 &\end{array}\right)$$ This does the job because $V=\langle e_1 \rangle \oplus \langle e_2 \rangle$ and the last summand isn't $T$ invertible. My question is I feel like I've spent too much time on this. I absolutely used no linear algebra knowledge to come up with this. I remotely remember my professor working on $\mathbb{R}^2$ to pull a counter example out of his hat on an unrelated problem. So I just kept plugging away numbers until something worked. Even my grandmother who evidently has never taken linear algebra could have done that. So what I ask is: How would you smart people do this  problem?. How do you come up with a counter example for this particular problem. Can you explain the Linear Algebra behind your thought process?. What geometric intuition helps here? (and in general for counterexamples of this nature). I hope my question makes sense. Thank you for your comments and answers.,This came up in a problem I was working on. Problem :Let $V$ be an $n$ dimensional vector space over a field $F$. Let $T:V\rightarrow V$ be a linear operator and let $W$ be a $T$ invariant subspace of $V$. Prove or give a counter example to the following: Let $U$ be another subspace of $V$ such that $V=W\oplus U$. Then $U$ is also $T$ invariant. Solution Well this is false. I came up with a counter example after spending an embarrassing amount of time guessing and checking. Consider the operator  $$ \left(\begin{array}{ccc}1 & -1 &  \\0 & 1 &\end{array}\right)$$ This does the job because $V=\langle e_1 \rangle \oplus \langle e_2 \rangle$ and the last summand isn't $T$ invertible. My question is I feel like I've spent too much time on this. I absolutely used no linear algebra knowledge to come up with this. I remotely remember my professor working on $\mathbb{R}^2$ to pull a counter example out of his hat on an unrelated problem. So I just kept plugging away numbers until something worked. Even my grandmother who evidently has never taken linear algebra could have done that. So what I ask is: How would you smart people do this  problem?. How do you come up with a counter example for this particular problem. Can you explain the Linear Algebra behind your thought process?. What geometric intuition helps here? (and in general for counterexamples of this nature). I hope my question makes sense. Thank you for your comments and answers.,,"['linear-algebra', 'examples-counterexamples']"
49,how this equation is a linear equation?,how this equation is a linear equation?,,"How is the equation $x_1+5x_2-\sqrt{(2x_3)} = 1$  a linear equation? The answer given in the book is, ""The Equation is linear"". How can an equation involving a square root like the above equation be a linear equation? here is the cutting of the book,","How is the equation $x_1+5x_2-\sqrt{(2x_3)} = 1$  a linear equation? The answer given in the book is, ""The Equation is linear"". How can an equation involving a square root like the above equation be a linear equation? here is the cutting of the book,",,['linear-algebra']
50,"Minimal polynomial, determinants and invertibility","Minimal polynomial, determinants and invertibility",,"I need to prove the following: Let $A$ be a square matrix over a field. If the matrix $A$ is invertible, then the minimal polynomial $m_A$ satisfies $m_A(0) \neq 0$ . There is one definition I am unsure of or need help making more clear. I will proceed with proof by contraposition: We must show that if $m_A(0) = 0$ then $A$ is not invertible. By definition of minimum polynomial of $A$ we have: $m_A(x) = x^r - \lambda_{r-1} x^{r-1} - \ldots - \lambda_1 x + \det(A)$ . Not sure about the determinant term here So, $m_A(0) = \det(A) = 0$ . We know $\det(A) = 0 \implies A$ is not invertible.","I need to prove the following: Let be a square matrix over a field. If the matrix is invertible, then the minimal polynomial satisfies . There is one definition I am unsure of or need help making more clear. I will proceed with proof by contraposition: We must show that if then is not invertible. By definition of minimum polynomial of we have: . Not sure about the determinant term here So, . We know is not invertible.",A A m_A m_A(0) \neq 0 m_A(0) = 0 A A m_A(x) = x^r - \lambda_{r-1} x^{r-1} - \ldots - \lambda_1 x + \det(A) m_A(0) = \det(A) = 0 \det(A) = 0 \implies A,"['linear-algebra', 'polynomials', 'determinant']"
51,Compactness of the set of all unitary matrices in $M_2(\mathbb{C})$,Compactness of the set of all unitary matrices in,M_2(\mathbb{C}),Is the set of all unitary matrices in $M_2(\mathbb{C})$ is compact? I can show that as determinant map is continuous so unitary matrices are closed but how to show they are bounded? Please help.,Is the set of all unitary matrices in $M_2(\mathbb{C})$ is compact? I can show that as determinant map is continuous so unitary matrices are closed but how to show they are bounded? Please help.,,"['linear-algebra', 'general-topology', 'matrices']"
52,"If $A$ and $B$ are positive-definite matrices, is $AB$ positive-definite?","If  and  are positive-definite matrices, is  positive-definite?",A B AB,"I've managed to prove that if $A$ and $B$ are positive definite then $AB$ has only positive eigenvalues. To prove $AB$ is positive definite, I also need to prove $(AB)^\ast = AB$ (so $AB$ is Hermitian). Is this statement true? If not, does anyone have a counterexample? Thanks, Josh","I've managed to prove that if $A$ and $B$ are positive definite then $AB$ has only positive eigenvalues. To prove $AB$ is positive definite, I also need to prove $(AB)^\ast = AB$ (so $AB$ is Hermitian). Is this statement true? If not, does anyone have a counterexample? Thanks, Josh",,"['linear-algebra', 'matrices']"
53,classification up to similarity of complex n-by-n matrices,classification up to similarity of complex n-by-n matrices,,Classify up to similarity all 3 x 3 complex matrices $A$ such that $A^n$ = $I$.,Classify up to similarity all 3 x 3 complex matrices $A$ such that $A^n$ = $I$.,,[]
54,Determinant of a matrix of the form $M = I + xx^t$,Determinant of a matrix of the form,M = I + xx^t,"$$M=\begin{pmatrix} 1+x_1^2 & x_1x_2 &...&x_1x_n \\ x_2x_1 & 1+x_2^2 &...&x_2x_n \\...&...& &...& \\x_nx_1 & x_nx_2& ...&1+x_n^2&\end{pmatrix}.$$ So I noticed that $M$ is a symmetric matrix and $ M=I+(x_1, x_2,...,x_n)^T(x_1, x_2,...,x_n)$ . That's all I can get.",So I noticed that is a symmetric matrix and . That's all I can get.,"M=\begin{pmatrix} 1+x_1^2 & x_1x_2 &...&x_1x_n \\ x_2x_1 & 1+x_2^2 &...&x_2x_n \\...&...& &...& \\x_nx_1 & x_nx_2& ...&1+x_n^2&\end{pmatrix}. M  M=I+(x_1, x_2,...,x_n)^T(x_1, x_2,...,x_n)","['linear-algebra', 'determinant']"
55,Showing that linear subset is not a subspace of the Vector space $V$,Showing that linear subset is not a subspace of the Vector space,V,"I am given the following $V = \mathbb R^4$ $W = \{(w,x,y,z)\in \mathbb R^4|w+2x-4y+2 = 0\}$ I have to prove or disprove that $W$ is a subspace of $V$. Now, my linear algebra is fairly weak as I haven't taken it in almost 4 years but for a subspace to exist I believe that: 1) The $0$ vector must exist under $W$ 2) Scalar addition must be closed under $W$ 3) Scalar multiplication must be closed under $W$ I don't think the first condition is true because if I were to take the vector, there is no way I can get the zero vector back. Is that correct or am I doing something very wrong?","I am given the following $V = \mathbb R^4$ $W = \{(w,x,y,z)\in \mathbb R^4|w+2x-4y+2 = 0\}$ I have to prove or disprove that $W$ is a subspace of $V$. Now, my linear algebra is fairly weak as I haven't taken it in almost 4 years but for a subspace to exist I believe that: 1) The $0$ vector must exist under $W$ 2) Scalar addition must be closed under $W$ 3) Scalar multiplication must be closed under $W$ I don't think the first condition is true because if I were to take the vector, there is no way I can get the zero vector back. Is that correct or am I doing something very wrong?",,['linear-algebra']
56,What is the point of a $1\times 1$ matrix?,What is the point of a  matrix?,1\times 1,"Surely a $1\times 1$ matrix can only 'produce' vectors with 1 entry, and can take as input also only one entry vectors. So, is there any use for $1\times 1$ matrices? Since to me they do the same like scalars, only worse.","Surely a $1\times 1$ matrix can only 'produce' vectors with 1 entry, and can take as input also only one entry vectors. So, is there any use for $1\times 1$ matrices? Since to me they do the same like scalars, only worse.",,"['linear-algebra', 'matrices']"
57,An $n\times n$ nilpotent matrix is nilpotent of degree at most $n$,An  nilpotent matrix is nilpotent of degree at most,n\times n n,"The following is a common linear algebra fact that has been asked for specific choices of $n\in\mathbb N$ in the past (see 1 , 2 , 3 , 4 , 5 among others.) $\DeclareMathOperator{\kk}{\mathbb k}$ Suppose that $A$ is a $n\times n$ matrix with coefficients in a field $\kk$ and that it is nilpotent, that is, there exists a non-negative integer $k$ such that $A^k=0$ . Then in fact $A^n=0$ . Disclaimer : For the sake of having an authoritative post to reference and use to close future (abstract) duplicates this post was edited accordingly. This was done with particular interest in the highest voted (but not accepted) elementary proof which may be missed by students, who may also be quick to appeal to the Jordan normal form (or the minimal polynomial say), which require unnecessary machinery and obscure the proof. For example, this post implies immediately that the characteristic polynomial of $A$ is $X^n$ over any field and hence that the minimal polynomial is $X^r$ for some $1\leqslant r\leqslant n$ which is mentioned in the accepted answer.","The following is a common linear algebra fact that has been asked for specific choices of in the past (see 1 , 2 , 3 , 4 , 5 among others.) Suppose that is a matrix with coefficients in a field and that it is nilpotent, that is, there exists a non-negative integer such that . Then in fact . Disclaimer : For the sake of having an authoritative post to reference and use to close future (abstract) duplicates this post was edited accordingly. This was done with particular interest in the highest voted (but not accepted) elementary proof which may be missed by students, who may also be quick to appeal to the Jordan normal form (or the minimal polynomial say), which require unnecessary machinery and obscure the proof. For example, this post implies immediately that the characteristic polynomial of is over any field and hence that the minimal polynomial is for some which is mentioned in the accepted answer.",n\in\mathbb N \DeclareMathOperator{\kk}{\mathbb k} A n\times n \kk k A^k=0 A^n=0 A X^n X^r 1\leqslant r\leqslant n,"['linear-algebra', 'matrices', 'nilpotence']"
58,Tensor product in multilinear algebra,Tensor product in multilinear algebra,,"In the book by Halmos ($FDVS$) the tensor product of two vector spaces U and V is defined as the dual of the vector space of all the bilinear forms on the direct sum of U and V. Is there a generalised form of this for the direct sums of more than two vector spaces? Is there a relation between the space of all multilinear forms on the direct sum of $V_1$,$V_2$,$V_3$,...,$V_k$ with their tensor product. Please explain without invoking other algebraic objects such as modules,rings etc and by using the concepts regarding vector spaces only (as the book assumes no such background either, it is unlikely that any reader of that book will benefit from such an exposition). Everywhere I searched, I found the explanation in terms of those concepts only and being unfamiliar to those I couldn't get them at all. Thanks in advance.","In the book by Halmos ($FDVS$) the tensor product of two vector spaces U and V is defined as the dual of the vector space of all the bilinear forms on the direct sum of U and V. Is there a generalised form of this for the direct sums of more than two vector spaces? Is there a relation between the space of all multilinear forms on the direct sum of $V_1$,$V_2$,$V_3$,...,$V_k$ with their tensor product. Please explain without invoking other algebraic objects such as modules,rings etc and by using the concepts regarding vector spaces only (as the book assumes no such background either, it is unlikely that any reader of that book will benefit from such an exposition). Everywhere I searched, I found the explanation in terms of those concepts only and being unfamiliar to those I couldn't get them at all. Thanks in advance.",,"['linear-algebra', 'tensor-products', 'multilinear-algebra', 'bilinear-form']"
59,Non-commuting matrix exponentials,Non-commuting matrix exponentials,,"Reading this book , I came across the following formula $$e^A e^B = e^{A+B}e^{\frac{1}{2}[A,B]}$$ where $A$ and $B$ are two matrices and $[A,B] := AB-BA$ . I tried to find a proof, without success. It is impossible to use the binomial theorem since $A$ and $B$ do not commute. I've thought about developping the product in a power series, but I'm not sure the Cauchy product is allowed when $[A,B] \ne 0$ . I thought that $[A,B^k]$ could be useful so I searched the general expression $$[A,B^n] = \sum_{i=0}^{n-1} B^i [A,B] B^{n-i-1}$$ Does anyone know a proof of the formula above? If so, is it possible to provide some hints? Or a full proof?","Reading this book , I came across the following formula where and are two matrices and . I tried to find a proof, without success. It is impossible to use the binomial theorem since and do not commute. I've thought about developping the product in a power series, but I'm not sure the Cauchy product is allowed when . I thought that could be useful so I searched the general expression Does anyone know a proof of the formula above? If so, is it possible to provide some hints? Or a full proof?","e^A e^B = e^{A+B}e^{\frac{1}{2}[A,B]} A B [A,B] := AB-BA A B [A,B] \ne 0 [A,B^k] [A,B^n] = \sum_{i=0}^{n-1} B^i [A,B] B^{n-i-1}","['linear-algebra', 'matrices', 'matrix-exponential']"
60,Are vector subspaces of $\mathbb{R}^n$ always closed?,Are vector subspaces of  always closed?,\mathbb{R}^n,"Suppose $S$ is any proper vector subspace of $\mathbb{R}^n$. Is $S$ a closed set in the usual topology on $\mathbb{R}^n$? Geometrically, I think it is clear that $S$ must be closed in $\mathbb{R}^n$ for $1\leq n\leq 3$. If $x\in\mathbb{R}^n\setminus S$, then we can always find the shortest distance from $x$ to the point, line, or plane that is $S$ by taking the projection of $x$ onto $S$. Choosing $\epsilon$ to be less than this distance, $B_\epsilon(x)$ is an open ball around $x$ disjoint from $S$, so $S$ is closed. I believe that the idea should carry over for higher dimensions, but I'm not sure how to make a more rigorous argument. How could this claim be proven without geometric handwaving? Thanks.","Suppose $S$ is any proper vector subspace of $\mathbb{R}^n$. Is $S$ a closed set in the usual topology on $\mathbb{R}^n$? Geometrically, I think it is clear that $S$ must be closed in $\mathbb{R}^n$ for $1\leq n\leq 3$. If $x\in\mathbb{R}^n\setminus S$, then we can always find the shortest distance from $x$ to the point, line, or plane that is $S$ by taking the projection of $x$ onto $S$. Choosing $\epsilon$ to be less than this distance, $B_\epsilon(x)$ is an open ball around $x$ disjoint from $S$, so $S$ is closed. I believe that the idea should carry over for higher dimensions, but I'm not sure how to make a more rigorous argument. How could this claim be proven without geometric handwaving? Thanks.",,"['linear-algebra', 'topological-vector-spaces']"
61,Linearly independent functions,Linearly independent functions,,"Show that the set consisting of the functions $$x, e^x, e^{-x}$$ on $\mathbb R$ is linearly independent. So I have the equation $$ax + be^x + ce^{-x} = 0$$ and I want to show that this is only satisfied when $a = b = c = 0$ Letting x = 0, $b + c = 0$ Letting x = 1, $a + be + ce^{-1} = 0$ Letting x = -1, $-a + be^{-1} + ce = 0$ Using these equations as columns of a matrix I have $\begin{bmatrix} 0 & 1 & 1 \\ 1 & e & e^{-1} \\ -1 & e^{-1} & e \end{bmatrix}$ $\begin{bmatrix} 1 & e & e^{-1} \\ 0 & 1 & 1 \\ -1 & e^{-1} & e \end{bmatrix}$ $\begin{bmatrix} 1 & e & e^{-1} \\ 0 & 1 & 1 \\ 0 & e^{-1}+e & e^{-1}+e \end{bmatrix}$ But if I now subtract (e^{-1}+e) times row 2 from row 3 I will get all zeros in the last row meaning linear dependence. So what have I done wrong?","Show that the set consisting of the functions $$x, e^x, e^{-x}$$ on $\mathbb R$ is linearly independent. So I have the equation $$ax + be^x + ce^{-x} = 0$$ and I want to show that this is only satisfied when $a = b = c = 0$ Letting x = 0, $b + c = 0$ Letting x = 1, $a + be + ce^{-1} = 0$ Letting x = -1, $-a + be^{-1} + ce = 0$ Using these equations as columns of a matrix I have $\begin{bmatrix} 0 & 1 & 1 \\ 1 & e & e^{-1} \\ -1 & e^{-1} & e \end{bmatrix}$ $\begin{bmatrix} 1 & e & e^{-1} \\ 0 & 1 & 1 \\ -1 & e^{-1} & e \end{bmatrix}$ $\begin{bmatrix} 1 & e & e^{-1} \\ 0 & 1 & 1 \\ 0 & e^{-1}+e & e^{-1}+e \end{bmatrix}$ But if I now subtract (e^{-1}+e) times row 2 from row 3 I will get all zeros in the last row meaning linear dependence. So what have I done wrong?",,['linear-algebra']
62,Normal and Lower triangular matrix implies diagonal matrix,Normal and Lower triangular matrix implies diagonal matrix,,A lower triangular complex matrix $A$ satisfies $AA^*=A^*A$. I would like to show that $A$ is diagonal. I know there exists a unitary matrix $P$ such that $PAP^*$ is diagonal. But I don't know how to show $A$ itself is diagonal.,A lower triangular complex matrix $A$ satisfies $AA^*=A^*A$. I would like to show that $A$ is diagonal. I know there exists a unitary matrix $P$ such that $PAP^*$ is diagonal. But I don't know how to show $A$ itself is diagonal.,,['linear-algebra']
63,Why are these examples striking?,Why are these examples striking?,,"The question is from an exercise in Gilbert Strang's Linear Algebra and its Applications . The powers $A^k$ approach zero if all $|\lambda_i|<1$, and they blow up if any $|\lambda_i|>1$. Peter Lax gives four striking examples in his book Linear Algebra .   $$A = \left(   \begin{array}{cc}      3& 2 \\      1& 4 \\   \end{array} \right)\qquad B =  \left(   \begin{array}{cc}      3 & 2  \\      -5 & -3  \\   \end{array} \right)\qquad C =  \left(   \begin{array}{cc}      5& 7 \\      -3& -4  \\   \end{array} \right)\qquad D =  \left(   \begin{array}{cc}      5& 6.9  \\      -3& -4 \\   \end{array} \right)$$   $$\|A^{1024}\|>10^{700}\qquad B^{1024}=I\qquad C^{1024}=-C\qquad \|D^{1024}\|<10^{-78}$$   Find the eigenvalues $\lambda=e^{i\theta}$ of $B$ and $C$ to show that $B^4=I$ and $C^3=-I$. Here is my question: Why are these examples so special? Is it because that all of them contain the number ""1024""? Or such examples are hard to construct?","The question is from an exercise in Gilbert Strang's Linear Algebra and its Applications . The powers $A^k$ approach zero if all $|\lambda_i|<1$, and they blow up if any $|\lambda_i|>1$. Peter Lax gives four striking examples in his book Linear Algebra .   $$A = \left(   \begin{array}{cc}      3& 2 \\      1& 4 \\   \end{array} \right)\qquad B =  \left(   \begin{array}{cc}      3 & 2  \\      -5 & -3  \\   \end{array} \right)\qquad C =  \left(   \begin{array}{cc}      5& 7 \\      -3& -4  \\   \end{array} \right)\qquad D =  \left(   \begin{array}{cc}      5& 6.9  \\      -3& -4 \\   \end{array} \right)$$   $$\|A^{1024}\|>10^{700}\qquad B^{1024}=I\qquad C^{1024}=-C\qquad \|D^{1024}\|<10^{-78}$$   Find the eigenvalues $\lambda=e^{i\theta}$ of $B$ and $C$ to show that $B^4=I$ and $C^3=-I$. Here is my question: Why are these examples so special? Is it because that all of them contain the number ""1024""? Or such examples are hard to construct?",,[]
64,doubt regarding a step proof of Cauchy-Schwarz inequality. Is it valid?,doubt regarding a step proof of Cauchy-Schwarz inequality. Is it valid?,,"I'm quite new to math proofs,I can't understand why the part where we set $\alpha=\|\boldsymbol{v}\|^2$ e $\beta=-\boldsymbol{u} \cdot \boldsymbol{v}$ works , why are proofs like those possible and valid? By setting alpha and beta to these values aren't we proving the theorem just for the case where $\alpha=\|\boldsymbol{v}\|^2$ e $\beta=-\boldsymbol{u} \cdot \boldsymbol{v}$ , the proof isn't generalizing enough or am I missing something? Theorem : If $(V, \cdot)$ is an euclidean vector space (real), then $\forall \boldsymbol{u}, \boldsymbol{v} \in V$ , we have: $|\boldsymbol{u} \cdot \boldsymbol{v}| \leq\|\boldsymbol{u}\|\|\boldsymbol{v}\|$ ,  Cauchy-Schwarz inequality. Proof. Let us first prove the Cauchy-Schwarz inequality. It is clear that the inequality is verified if at least one of the two vectors is null. We therefore assume that they are both nonzero. Let us consider $\boldsymbol{w}=\alpha \boldsymbol{u}+\beta \boldsymbol{v}, \operatorname{with} \alpha, \beta \in \mathbb{R}$ , $$ \boldsymbol{w} \cdot \boldsymbol{w}=(\alpha \boldsymbol{u}+\beta \boldsymbol{v}) \cdot(\alpha \boldsymbol{u}+\beta \boldsymbol{v})=\alpha^2\|\boldsymbol{u}\|^2+\beta^2\|\boldsymbol{v}\|^2+2 \alpha \beta \boldsymbol{u} \cdot \boldsymbol{v} \geq 0 . $$ *** then taking $\alpha=\|\boldsymbol{v}\|^2$ e $\beta=-\boldsymbol{u} \cdot \boldsymbol{v}$ , we get $$ \|v\|^4\|\boldsymbol{u}\|^2+(\boldsymbol{u} \cdot \boldsymbol{v})^2\|v\|^2-2\|v\|^2(\boldsymbol{u} \cdot \boldsymbol{v})^2=\|\boldsymbol{v}\|^2\left(\|\boldsymbol{v}\|^2\|\boldsymbol{u}\|^2-(\boldsymbol{u} \cdot \boldsymbol{v})^2\right) \geq 0 . $$ Since $v \neq 0$ , we can divide by $\|v\|^2$ , and get the inequality $$ \|\boldsymbol{v}\|^2\|\boldsymbol{u}\|^2 \geq(\boldsymbol{u} \cdot \boldsymbol{v})^2, \Longrightarrow|\boldsymbol{u} \cdot \boldsymbol{v}| \leq\|\boldsymbol{u}\|\|\boldsymbol{v}\| . $$","I'm quite new to math proofs,I can't understand why the part where we set e works , why are proofs like those possible and valid? By setting alpha and beta to these values aren't we proving the theorem just for the case where e , the proof isn't generalizing enough or am I missing something? Theorem : If is an euclidean vector space (real), then , we have: ,  Cauchy-Schwarz inequality. Proof. Let us first prove the Cauchy-Schwarz inequality. It is clear that the inequality is verified if at least one of the two vectors is null. We therefore assume that they are both nonzero. Let us consider , *** then taking e , we get Since , we can divide by , and get the inequality","\alpha=\|\boldsymbol{v}\|^2 \beta=-\boldsymbol{u} \cdot \boldsymbol{v} \alpha=\|\boldsymbol{v}\|^2 \beta=-\boldsymbol{u} \cdot \boldsymbol{v} (V, \cdot) \forall \boldsymbol{u}, \boldsymbol{v} \in V |\boldsymbol{u} \cdot \boldsymbol{v}| \leq\|\boldsymbol{u}\|\|\boldsymbol{v}\| \boldsymbol{w}=\alpha \boldsymbol{u}+\beta \boldsymbol{v}, \operatorname{with} \alpha, \beta \in \mathbb{R} 
\boldsymbol{w} \cdot \boldsymbol{w}=(\alpha \boldsymbol{u}+\beta \boldsymbol{v}) \cdot(\alpha \boldsymbol{u}+\beta \boldsymbol{v})=\alpha^2\|\boldsymbol{u}\|^2+\beta^2\|\boldsymbol{v}\|^2+2 \alpha \beta \boldsymbol{u} \cdot \boldsymbol{v} \geq 0 .
 \alpha=\|\boldsymbol{v}\|^2 \beta=-\boldsymbol{u} \cdot \boldsymbol{v} 
\|v\|^4\|\boldsymbol{u}\|^2+(\boldsymbol{u} \cdot \boldsymbol{v})^2\|v\|^2-2\|v\|^2(\boldsymbol{u} \cdot \boldsymbol{v})^2=\|\boldsymbol{v}\|^2\left(\|\boldsymbol{v}\|^2\|\boldsymbol{u}\|^2-(\boldsymbol{u} \cdot \boldsymbol{v})^2\right) \geq 0 .
 v \neq 0 \|v\|^2 
\|\boldsymbol{v}\|^2\|\boldsymbol{u}\|^2 \geq(\boldsymbol{u} \cdot \boldsymbol{v})^2, \Longrightarrow|\boldsymbol{u} \cdot \boldsymbol{v}| \leq\|\boldsymbol{u}\|\|\boldsymbol{v}\| .
","['linear-algebra', 'proof-writing', 'proof-explanation']"
65,Can $y=10^{-x}$ be converted into an equivalent $y=\mathrm{e}^{-kx}$?,Can  be converted into an equivalent ?,y=10^{-x} y=\mathrm{e}^{-kx},"I was dealing with the values: | Digits | Expression | Value                 | |--------|------------|-----------------------| | 1      | 10⁻¹       | 0.1                   | | 2      | 10⁻²       | 0.01                  | | 3      | 10⁻³       | 0.001                 | | 4      | 10⁻⁴       | 0.0001                | | 5      | 10⁻⁵       | 0.00001               | | 6      | 10⁻⁶       | 0.000001              | | 7      | 10⁻⁷       | 0.0000001             | | 8      | 10⁻⁸       | 0.00000001            | | 9      | 10⁻⁹       | 0.000000001           | | 10     | 10⁻¹⁰      | 0.0000000001          | | 11     | 10⁻¹¹      | 0.00000000001         | | 12     | 10⁻¹²      | 0.000000000001        | | 13     | 10⁻¹³      | 0.0000000000001       | | 14     | 10⁻¹⁴      | 0.00000000000001      | | 15     | 10⁻¹⁵      | 0.000000000000001     | And then I plotted the results in Excel on a log scale: Now, I already know the formula for this graph, it's: $$ y = 10^{-x} $$ But was curious to see how well an ""exponential"" trendline would fit, and it fits very well : The $R^2$ is $1$ , even for $15$ decimal places. So it seems that: $$y = 10^{-x} ↔ y = e^{-2.30258509299405x} $$ The question So I have to wonder: is there an algebraic transformation of: $$y = 10^{-x} → y = e^{-kx} $$ Where does the constant $k$ come from? Does it have an expression? Or is this all a very interesting coincidence?","I was dealing with the values: | Digits | Expression | Value                 | |--------|------------|-----------------------| | 1      | 10⁻¹       | 0.1                   | | 2      | 10⁻²       | 0.01                  | | 3      | 10⁻³       | 0.001                 | | 4      | 10⁻⁴       | 0.0001                | | 5      | 10⁻⁵       | 0.00001               | | 6      | 10⁻⁶       | 0.000001              | | 7      | 10⁻⁷       | 0.0000001             | | 8      | 10⁻⁸       | 0.00000001            | | 9      | 10⁻⁹       | 0.000000001           | | 10     | 10⁻¹⁰      | 0.0000000001          | | 11     | 10⁻¹¹      | 0.00000000001         | | 12     | 10⁻¹²      | 0.000000000001        | | 13     | 10⁻¹³      | 0.0000000000001       | | 14     | 10⁻¹⁴      | 0.00000000000001      | | 15     | 10⁻¹⁵      | 0.000000000000001     | And then I plotted the results in Excel on a log scale: Now, I already know the formula for this graph, it's: But was curious to see how well an ""exponential"" trendline would fit, and it fits very well : The is , even for decimal places. So it seems that: The question So I have to wonder: is there an algebraic transformation of: Where does the constant come from? Does it have an expression? Or is this all a very interesting coincidence?", y = 10^{-x}  R^2 1 15 y = 10^{-x} ↔ y = e^{-2.30258509299405x}  y = 10^{-x} → y = e^{-kx}  k,['linear-algebra']
66,"Are Jordan ""Formable"" matrices closed under multiplication?","Are Jordan ""Formable"" matrices closed under multiplication?",,"After learning about Jordan Canonical Form, I began thinking about if you have two matrices $A, B \in M_n(\mathbb{R}),$ whether or not their product $AB$ will also have a Jordan Canonical Form. Trivially, if we consider polynomials in $ M_n(\mathbb{C})$ then our polynomial will always have roots in the field. So, from what I understand, this is equivalent to asking whether or not the characteristic polynomial of $AB$ will have real roots. I began playing around with the $2 \times 2$ case and if we have that $AB$ has a negative determinant then we can't have that the eigenvalues of $AB$ are in $\mathbb{C},$ since they must multiply to the determinant, but since the complex eigenvalues always come in a conjugate pair, that would mean that their product is always positive. I am not sure how I can break down the case if the determinant is positive. Finally, my own intuition says matrices which have Jordan form will be closed under multiplication. This is because if we think about what matrices with strictly, real eigenvalues do, they simply reflect and stretch space. So I don't believe it is possible to find two matrices whose product matrix will be a rotation in the plane. Also, I couldn't find this question asked elsewhere, but if it has an answer, I'd be happy to read it.","After learning about Jordan Canonical Form, I began thinking about if you have two matrices whether or not their product will also have a Jordan Canonical Form. Trivially, if we consider polynomials in then our polynomial will always have roots in the field. So, from what I understand, this is equivalent to asking whether or not the characteristic polynomial of will have real roots. I began playing around with the case and if we have that has a negative determinant then we can't have that the eigenvalues of are in since they must multiply to the determinant, but since the complex eigenvalues always come in a conjugate pair, that would mean that their product is always positive. I am not sure how I can break down the case if the determinant is positive. Finally, my own intuition says matrices which have Jordan form will be closed under multiplication. This is because if we think about what matrices with strictly, real eigenvalues do, they simply reflect and stretch space. So I don't believe it is possible to find two matrices whose product matrix will be a rotation in the plane. Also, I couldn't find this question asked elsewhere, but if it has an answer, I'd be happy to read it.","A, B \in M_n(\mathbb{R}), AB  M_n(\mathbb{C}) AB 2 \times 2 AB AB \mathbb{C},","['linear-algebra', 'abstract-algebra']"
67,Can the Sum of Two Tensor Products Be Written as a Single Tensor Product?,Can the Sum of Two Tensor Products Be Written as a Single Tensor Product?,,"In general, if I have $|\Psi\rangle = (|\Psi_{1_1}\rangle \otimes |\Psi_{1_2}\rangle + |\Psi_{2_1}\rangle \otimes |\Psi_{2_2}\rangle)$,  can I find $|\Psi_{3_1}\rangle$ and $|\Psi_{3_2}\rangle$, such that $|\Psi\rangle = |\Psi_{3_1}\rangle \otimes |\Psi_{3_2}\rangle$? Here $\otimes$ means tensor product and $|\Psi\rangle$ and means a vector. No assumption is made about any relationship between the $|\Psi_{i_j}\rangle$, except that they are all the same dimension and their components are complex numbers. The motivation is the quantum double slit experiment, where the wave state, $|\Psi\rangle$, between the slits and the detector, is the sum of two interfering waves, and $|\Psi\rangle$ is still in a ""pure state"", which means that $|\Psi\rangle$ can also be written as a tensor product","In general, if I have $|\Psi\rangle = (|\Psi_{1_1}\rangle \otimes |\Psi_{1_2}\rangle + |\Psi_{2_1}\rangle \otimes |\Psi_{2_2}\rangle)$,  can I find $|\Psi_{3_1}\rangle$ and $|\Psi_{3_2}\rangle$, such that $|\Psi\rangle = |\Psi_{3_1}\rangle \otimes |\Psi_{3_2}\rangle$? Here $\otimes$ means tensor product and $|\Psi\rangle$ and means a vector. No assumption is made about any relationship between the $|\Psi_{i_j}\rangle$, except that they are all the same dimension and their components are complex numbers. The motivation is the quantum double slit experiment, where the wave state, $|\Psi\rangle$, between the slits and the detector, is the sum of two interfering waves, and $|\Psi\rangle$ is still in a ""pure state"", which means that $|\Psi\rangle$ can also be written as a tensor product",,"['linear-algebra', 'tensor-products']"
68,How to calculate the determinant of this n by n matrix?,How to calculate the determinant of this n by n matrix?,,"Find the determinant of this n by n matrix. $$ \begin{pmatrix} 0 & x_1 & x_2 & \cdots& x_k \\ x_1 & 1 & 0 & \cdots & 0 \\ x_2 & 0 & 1& \cdots & 0 \\ \vdots& \vdots& \vdots& \ddots  & \vdots\\ x_k & 0 & 0 & \cdots& 1 \\ \end{pmatrix} $$ where, $$ k=n-1 $$. I am new to matrices and determinants, but this is what I did: I developed the determinant using the second column: $$ (-1)^2*x_1  \begin{pmatrix} x_1  & 0 & \cdots & 0 \\ x_2  & 1& \cdots & 0 \\ \vdots& \vdots& \ddots& \vdots \\ x_k  & 0 & \cdots& 1 \\ \end{pmatrix} + (-1)^3 *1 \begin{pmatrix} 0 & x_2 & \cdots & x_k \\ x_2  & 1& \cdots & 0 \\ \vdots& \vdots& \ddots& \vdots \\ x_k  & 0 & \cdots& 1 \\\end{pmatrix} $$ the first determinant is triangular, so its equal to $ x_1 $ but this is where I got stuck. I don't know what to do with the second determinant. Any help is appriciated. Thanks","Find the determinant of this n by n matrix. $$ \begin{pmatrix} 0 & x_1 & x_2 & \cdots& x_k \\ x_1 & 1 & 0 & \cdots & 0 \\ x_2 & 0 & 1& \cdots & 0 \\ \vdots& \vdots& \vdots& \ddots  & \vdots\\ x_k & 0 & 0 & \cdots& 1 \\ \end{pmatrix} $$ where, $$ k=n-1 $$. I am new to matrices and determinants, but this is what I did: I developed the determinant using the second column: $$ (-1)^2*x_1  \begin{pmatrix} x_1  & 0 & \cdots & 0 \\ x_2  & 1& \cdots & 0 \\ \vdots& \vdots& \ddots& \vdots \\ x_k  & 0 & \cdots& 1 \\ \end{pmatrix} + (-1)^3 *1 \begin{pmatrix} 0 & x_2 & \cdots & x_k \\ x_2  & 1& \cdots & 0 \\ \vdots& \vdots& \ddots& \vdots \\ x_k  & 0 & \cdots& 1 \\\end{pmatrix} $$ the first determinant is triangular, so its equal to $ x_1 $ but this is where I got stuck. I don't know what to do with the second determinant. Any help is appriciated. Thanks",,"['linear-algebra', 'matrices', 'determinant']"
69,Nth power of the following matrix,Nth power of the following matrix,,"The matrix is  $ \begin{pmatrix}0&1\\-1&0\end{pmatrix}^n $  for  $n=2 \implies \left(\begin{matrix}-1 & 0\\ 0 &-1\end{matrix}\right)$ for $n=3 \implies \begin{pmatrix}1&0\\0&1\end{pmatrix}$ for $n=4 \implies \begin{pmatrix}-1&0\\0&-1\end{pmatrix}$ so I assume that for every $n=2k $, where k is a natural number and bigger than $0$ the matrix will be  $\begin{pmatrix}-1&0\\0&-1\end{pmatrix}$ and for every $n=2k+1$ where k is a natural number and bigger than $0 $the matrix will be $\begin{pmatrix}1&0\\0&1\end{pmatrix}$ How can I prove it? probably with induction and how can I get easily the inverses of the matrices?","The matrix is  $ \begin{pmatrix}0&1\\-1&0\end{pmatrix}^n $  for  $n=2 \implies \left(\begin{matrix}-1 & 0\\ 0 &-1\end{matrix}\right)$ for $n=3 \implies \begin{pmatrix}1&0\\0&1\end{pmatrix}$ for $n=4 \implies \begin{pmatrix}-1&0\\0&-1\end{pmatrix}$ so I assume that for every $n=2k $, where k is a natural number and bigger than $0$ the matrix will be  $\begin{pmatrix}-1&0\\0&-1\end{pmatrix}$ and for every $n=2k+1$ where k is a natural number and bigger than $0 $the matrix will be $\begin{pmatrix}1&0\\0&1\end{pmatrix}$ How can I prove it? probably with induction and how can I get easily the inverses of the matrices?",,"['linear-algebra', 'matrices']"
70,Short proof that $X^2 = X \Rightarrow X^{100} = X$,Short proof that,X^2 = X \Rightarrow X^{100} = X,"Given that a matrix $X$ satisfies $X^2 = X$ it is clear that $X^{100}=X$ by repeated multiplication of $X$. Algebraically, we might write: $$X^{100} = (X^2)^{50}=X^{50}=(X^2)^{25}=X^{25}=X(X^2)^{12} = \dots = (X^2)^2 = X $$ But this seems like too much work for such a simple fact. Is there a short algebraic proof?","Given that a matrix $X$ satisfies $X^2 = X$ it is clear that $X^{100}=X$ by repeated multiplication of $X$. Algebraically, we might write: $$X^{100} = (X^2)^{50}=X^{50}=(X^2)^{25}=X^{25}=X(X^2)^{12} = \dots = (X^2)^2 = X $$ But this seems like too much work for such a simple fact. Is there a short algebraic proof?",,['linear-algebra']
71,Is determinant uniformly continuous?,Is determinant uniformly continuous?,,The determinant map $\det$ sending an $n\times n$ real matrix to its determiant is continuous since it's a polynomial in the coefficients. Is it also uniformly continuous?,The determinant map $\det$ sending an $n\times n$ real matrix to its determiant is continuous since it's a polynomial in the coefficients. Is it also uniformly continuous?,,"['linear-algebra', 'matrices', 'determinant', 'uniform-continuity']"
72,Normalizing a matrix,Normalizing a matrix,,"I came across a step in an numerical algebra algorithm that says ""Normalize the rows of matrix A such that they are unit-norm. Call U the normalized matrix."" I do something like this: for i=1:no_of_rows  U(i,:)= A(i,:)./ norm(A(i,:)) end My question is what norm should I use? Will $2$ norm as below : $$\|X\|_2=\sqrt{\sum_{k=1}^n|x_k|^2}$$ work here? Do I need to satisfy UU*=I ?","I came across a step in an numerical algebra algorithm that says ""Normalize the rows of matrix A such that they are unit-norm. Call U the normalized matrix."" I do something like this: for i=1:no_of_rows  U(i,:)= A(i,:)./ norm(A(i,:)) end My question is what norm should I use? Will $2$ norm as below : $$\|X\|_2=\sqrt{\sum_{k=1}^n|x_k|^2}$$ work here? Do I need to satisfy UU*=I ?",,"['linear-algebra', 'matrices', 'normed-spaces', 'orthonormal']"
73,Different approaches to evaluate this determinant,Different approaches to evaluate this determinant,,"How to evaluate this determinant $$\det\begin{bmatrix} a& b&b &\cdots&b\\ c  &d &0&\cdots&0\\c&0&d&\ddots&\vdots\\\vdots &\vdots&\ddots&\ddots& 0\\c&0&\cdots&0&d \end{bmatrix}?$$ I am looking for the different approaches.","How to evaluate this determinant $$\det\begin{bmatrix} a& b&b &\cdots&b\\ c  &d &0&\cdots&0\\c&0&d&\ddots&\vdots\\\vdots &\vdots&\ddots&\ddots& 0\\c&0&\cdots&0&d \end{bmatrix}?$$ I am looking for the different approaches.",,"['linear-algebra', 'matrices', 'determinant']"
74,surjective linear map from R to R²,surjective linear map from R to R²,,How to quickly and clearly argue/show that there is or is not a linear surjective map $\phi$ $\phi: \mathbb{R} \to \mathbb{R}²$,How to quickly and clearly argue/show that there is or is not a linear surjective map $\phi$ $\phi: \mathbb{R} \to \mathbb{R}²$,,"['linear-algebra', 'functions']"
75,Polynomial fitting - how to fit and what is _polynomial fitting_,Polynomial fitting - how to fit and what is _polynomial fitting_,,I don't understand what is polynomial fitting . Can anyone explain to me how to fit a curve to given points?,I don't understand what is polynomial fitting . Can anyone explain to me how to fit a curve to given points?,,"['linear-algebra', 'polynomials', 'regression']"
76,How do I find the determinant of this matrix?,How do I find the determinant of this matrix?,,"I'm preparing for an exam currently, and I came across this question: I have noticed that A can be constructed from the matrix on the left by a series of row operations, so I had the idea maybe to express A as a product of elementary matrices as well as the matrix on the left and, maybe there was some fact about the determinants of elementary matrices? So then I could just use some properties of determinants, as well as knowing that the determinant of the matrix on the left is 2 to figure out $det(A)$ Entirely confused though because I do not know what the determinant of elementary matrices are, and if I should be multiplying all the elementary matrices together etc etc? Any help would be hugely appreciated.","I'm preparing for an exam currently, and I came across this question: I have noticed that A can be constructed from the matrix on the left by a series of row operations, so I had the idea maybe to express A as a product of elementary matrices as well as the matrix on the left and, maybe there was some fact about the determinants of elementary matrices? So then I could just use some properties of determinants, as well as knowing that the determinant of the matrix on the left is 2 to figure out Entirely confused though because I do not know what the determinant of elementary matrices are, and if I should be multiplying all the elementary matrices together etc etc? Any help would be hugely appreciated.",det(A),['linear-algebra']
77,Largest value of determinant,Largest value of determinant,,"If $\alpha,\beta,\gamma \in [-3,10].$ Then largest value of the determinant $$\begin{vmatrix}3\alpha^2&\beta^2+\alpha\beta+\alpha^2&\gamma^2+\alpha\gamma+\alpha^2\\\\ \alpha^2+\alpha\beta+\beta^2& 3\beta^2&\gamma^2+\beta\gamma+\beta^2\\\\ \alpha^2+\alpha\gamma+\gamma^2& \beta^2+\beta\gamma+\gamma^2&3\gamma^2\end{vmatrix}$$ Try: I am trying to break that determinant into product of 2 determinants but not able to break it. Could someone help me in this question? Thanks.",If Then largest value of the determinant Try: I am trying to break that determinant into product of 2 determinants but not able to break it. Could someone help me in this question? Thanks.,"\alpha,\beta,\gamma \in [-3,10]. \begin{vmatrix}3\alpha^2&\beta^2+\alpha\beta+\alpha^2&\gamma^2+\alpha\gamma+\alpha^2\\\\
\alpha^2+\alpha\beta+\beta^2& 3\beta^2&\gamma^2+\beta\gamma+\beta^2\\\\
\alpha^2+\alpha\gamma+\gamma^2& \beta^2+\beta\gamma+\gamma^2&3\gamma^2\end{vmatrix}","['linear-algebra', 'determinant']"
78,Definition of span,Definition of span,,"On an old midterm exam, my professor requested the students prove that The span of $S$ (where $S$ is a subset of a vector space $V$ ) is equal to all vectors that can be expressed as linear combinations of the elements in $S$ . Does this make any sense? He's requesting we show that the span of $S$ equals what I believe to be the definition of span. Is there possibly some other definition of span that I should be aware of?","On an old midterm exam, my professor requested the students prove that The span of (where is a subset of a vector space ) is equal to all vectors that can be expressed as linear combinations of the elements in . Does this make any sense? He's requesting we show that the span of equals what I believe to be the definition of span. Is there possibly some other definition of span that I should be aware of?",S S V S S,['linear-algebra']
79,"If $f^2=f$, then $f$ is of constant rank.","If , then  is of constant rank.",f^2=f f,"Let $f$ be a continuous function from $[0,1]$ to set of $n\times n$ matrices i.e. $M(n\times n,\mathbb{R})$ such that $f(t)^2=f(t)$ for all $t$. Then  $f(t)$ has a constant rank for all $t$. The only thing that I was able to guess conclude here that $f(t)$ has two eigenvalues, namely $0$ and $1$ with the minimal polynomial $x^2-x$, as if the minimal polynomial is $x$ or $x-1$, we are done. Now what to do afterwards?","Let $f$ be a continuous function from $[0,1]$ to set of $n\times n$ matrices i.e. $M(n\times n,\mathbb{R})$ such that $f(t)^2=f(t)$ for all $t$. Then  $f(t)$ has a constant rank for all $t$. The only thing that I was able to guess conclude here that $f(t)$ has two eigenvalues, namely $0$ and $1$ with the minimal polynomial $x^2-x$, as if the minimal polynomial is $x$ or $x-1$, we are done. Now what to do afterwards?",,"['real-analysis', 'linear-algebra', 'minimal-polynomials']"
80,The general linear group is closed,The general linear group is closed,,"In algebraic geometry I learned that $GL(n, \mathbb R)$ is Zariski-closed (every linear algebraic group is a closed subgroup of it, and it is itself linear algebraic), and as the Zariski topology is coarser than the usual topology it must be closed in the standard topology, but by the defining condition $\det(A) \ne 0$ it is open as well, which is not possible in the standard topology, as this space is connected. So, something is wrong here, but I do not see it? Could anyone please explain this to me?","In algebraic geometry I learned that $GL(n, \mathbb R)$ is Zariski-closed (every linear algebraic group is a closed subgroup of it, and it is itself linear algebraic), and as the Zariski topology is coarser than the usual topology it must be closed in the standard topology, but by the defining condition $\det(A) \ne 0$ it is open as well, which is not possible in the standard topology, as this space is connected. So, something is wrong here, but I do not see it? Could anyone please explain this to me?",,"['linear-algebra', 'algebraic-geometry', 'differential-topology', 'lie-groups', 'topological-groups']"
81,$A\in M_n(\mathbb{R})$ is symmetric s.t. $A^{10}=I.$ Prove $A^2=I$,is symmetric s.t.  Prove,A\in M_n(\mathbb{R}) A^{10}=I. A^2=I,"Let $A\in M_n(\mathbb{R})$ be symmetric, such that $A^{10}=I.$ Prove $A^2=I$ My thoughts: Since $A$ is symmetric, $A^2$ is symmetric, so there exists an orthogonal $P\in M_n(\mathbb{R})$ such that $D=P^{-1}A^2P$ is a diagonal matrix. I tried to work with that in order to find the ""right"" diagonal matrix such that after power manipulations, I could prove that $A^{10}$ is similar to $A^2$ and conclude the result, but got stuck. Any help is appreciated.","Let $A\in M_n(\mathbb{R})$ be symmetric, such that $A^{10}=I.$ Prove $A^2=I$ My thoughts: Since $A$ is symmetric, $A^2$ is symmetric, so there exists an orthogonal $P\in M_n(\mathbb{R})$ such that $D=P^{-1}A^2P$ is a diagonal matrix. I tried to work with that in order to find the ""right"" diagonal matrix such that after power manipulations, I could prove that $A^{10}$ is similar to $A^2$ and conclude the result, but got stuck. Any help is appreciated.",,"['linear-algebra', 'matrices', 'diagonalization', 'symmetric-matrices']"
82,Shortest distance between two lines in 3-dimensional space [closed],Shortest distance between two lines in 3-dimensional space [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Can someone explain to me how to solve this question? Find the shortest distance between the lines  $L_1 = \left\{t \begin{bmatrix} 1\\ 1\\ 1\end{bmatrix} : t \in \mathbb{R}\right\}$ and $L_2 = \left\{s \begin{bmatrix} 1\\ 2\\ 3\end{bmatrix} + \begin{bmatrix} 1\\ 0\\ 0\end{bmatrix}: s \in \mathbb{R}\right\}$ Thanks","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Can someone explain to me how to solve this question? Find the shortest distance between the lines  $L_1 = \left\{t \begin{bmatrix} 1\\ 1\\ 1\end{bmatrix} : t \in \mathbb{R}\right\}$ and $L_2 = \left\{s \begin{bmatrix} 1\\ 2\\ 3\end{bmatrix} + \begin{bmatrix} 1\\ 0\\ 0\end{bmatrix}: s \in \mathbb{R}\right\}$ Thanks",,"['linear-algebra', 'optimization', 'convex-optimization', 'least-squares']"
83,"If $A$ is an $m\times n$ matrix, $B$ is an $n\times m$ matrix and $n<m$, then $AB$ is not invertible.","If  is an  matrix,  is an  matrix and , then  is not invertible.",A m\times n B n\times m n<m AB,"The question was given in the early chapters of Linear Algebra by Hoffman & Kunze, so I am trying to give a proof with only the tools given to me so far - which are mainly row reduction and knowledge of matrix multiplication, row reduced echelon forms, row equivalence and linear independence. I attempted a proof as per the following: Consider $A$ as a collection (not sure if this would be the ideal expression) of $1 \times n$ row vectors, and $B$ as a collection of $n \times 1$ column vectors. Then we have that: $$ A=\begin{bmatrix} r_1 \\ \vdots \\ r_m \end{bmatrix},\ B=\begin{bmatrix} c_1 & \cdots & c_m \end{bmatrix}. $$   Thus it follows that:   $$ AB =\begin{bmatrix} r_1\cdot c_1 & \cdots & r_1\cdot c_m \\ \vdots & ~ & \vdots \\ r_m\cdot c_1 & \cdots & r_m \cdot c_m  \end{bmatrix} $$   Clearly, by inspection, the rows are linearly dependent. Since the rows of $AB$ are linearly dependent, it naturally follows that the reduced row echelon form of $AB$ contains zero rows. Hence, $AB$ is not invertible. Would this be a mathematically sufficient proof?","The question was given in the early chapters of Linear Algebra by Hoffman & Kunze, so I am trying to give a proof with only the tools given to me so far - which are mainly row reduction and knowledge of matrix multiplication, row reduced echelon forms, row equivalence and linear independence. I attempted a proof as per the following: Consider $A$ as a collection (not sure if this would be the ideal expression) of $1 \times n$ row vectors, and $B$ as a collection of $n \times 1$ column vectors. Then we have that: $$ A=\begin{bmatrix} r_1 \\ \vdots \\ r_m \end{bmatrix},\ B=\begin{bmatrix} c_1 & \cdots & c_m \end{bmatrix}. $$   Thus it follows that:   $$ AB =\begin{bmatrix} r_1\cdot c_1 & \cdots & r_1\cdot c_m \\ \vdots & ~ & \vdots \\ r_m\cdot c_1 & \cdots & r_m \cdot c_m  \end{bmatrix} $$   Clearly, by inspection, the rows are linearly dependent. Since the rows of $AB$ are linearly dependent, it naturally follows that the reduced row echelon form of $AB$ contains zero rows. Hence, $AB$ is not invertible. Would this be a mathematically sufficient proof?",,['linear-algebra']
84,Unable to understand the proof of two isomorphic finite-dimensional vector spaces having the same dimension,Unable to understand the proof of two isomorphic finite-dimensional vector spaces having the same dimension,,"Theorem: Two finite-dimensional vector spaces are isomorphic if and only if they have the same dimension. I can understand how to prove that if they are isomorphic then they have the same dimension. Yet for the other direction I cannot totally understand. To quote Axler's Linear Algebra Done Right, 2nd edition page 55: To prove the other direction, suppose $V$ and $W$ are finite-dimensional vector spaces with the same dimension. Let $(v_1,...,v_n)$ be a basis of $V$ and $(w_1,...w_n)$ be a basis of $W$. Let $T$ be the linear map from $V$ to $W$ defined by $$T(a_1v_1+...+a_nv_n)=a_1w_1+...+a_nw_n \ (*)$$ Then $T$ is surjective because $(w_1,...,w_n)$ spans $W$, and $T$ is injective because $(w_1,...,w_n)$ is linearly independent. Because $T$ is injective and surjective, $T$ is invertible. I cannot understand how on earth can we define $T$ that satisfy (*) above. I don't think Axler has given a proof that this can be defined. Could somebody help me on this please?","Theorem: Two finite-dimensional vector spaces are isomorphic if and only if they have the same dimension. I can understand how to prove that if they are isomorphic then they have the same dimension. Yet for the other direction I cannot totally understand. To quote Axler's Linear Algebra Done Right, 2nd edition page 55: To prove the other direction, suppose $V$ and $W$ are finite-dimensional vector spaces with the same dimension. Let $(v_1,...,v_n)$ be a basis of $V$ and $(w_1,...w_n)$ be a basis of $W$. Let $T$ be the linear map from $V$ to $W$ defined by $$T(a_1v_1+...+a_nv_n)=a_1w_1+...+a_nw_n \ (*)$$ Then $T$ is surjective because $(w_1,...,w_n)$ spans $W$, and $T$ is injective because $(w_1,...,w_n)$ is linearly independent. Because $T$ is injective and surjective, $T$ is invertible. I cannot understand how on earth can we define $T$ that satisfy (*) above. I don't think Axler has given a proof that this can be defined. Could somebody help me on this please?",,"['linear-algebra', 'vector-spaces', 'vector-space-isomorphism']"
85,$A^2=AB+BA$. Prove that $\det(AB-BA)=0$,. Prove that,A^2=AB+BA \det(AB-BA)=0,"Let $A,B$ be two $3\times 3$ matrices with complex entries, such that $A^2=AB+BA$. Prove that $\det(AB-BA)=0$ Nice problem, and I want to find a solution. $AB-BA=A^2-2BA=(A-2B)A$ so if $|A|=0$ we have done, if $|A| \not=0$ I can't prove.","Let $A,B$ be two $3\times 3$ matrices with complex entries, such that $A^2=AB+BA$. Prove that $\det(AB-BA)=0$ Nice problem, and I want to find a solution. $AB-BA=A^2-2BA=(A-2B)A$ so if $|A|=0$ we have done, if $|A| \not=0$ I can't prove.",,"['linear-algebra', 'matrices']"
86,Can two unknowns of two *unrelated* linear equations be determined?,Can two unknowns of two *unrelated* linear equations be determined?,,"This question is based on the storm caused by this twitter post . Since a twitter discussion is not an objective question, I'll simply write out the key elements of the problem. Movie earning estimates were reported as roughly \$15 million in total revenue across about 2 million transactions where the rental option was \$6 and the sale purchase option was \$15 dollars. Emphasizing that the given number of transactions and revenue are rough estimates, can this be solved algebraically? That is, can the two ""rough"" equations simply be solved to determine the two unknowns $r$ and $s$ (rentals and sales)? To be clear, $r$ and $s$ represent the number of transactions. If solving these equations is valid, then what is the graphical explanation for why these two seemingly unrelated equations (one is number of transactions the other is revenue) can be solved?","This question is based on the storm caused by this twitter post . Since a twitter discussion is not an objective question, I'll simply write out the key elements of the problem. Movie earning estimates were reported as roughly \$15 million in total revenue across about 2 million transactions where the rental option was \$6 and the sale purchase option was \$15 dollars. Emphasizing that the given number of transactions and revenue are rough estimates, can this be solved algebraically? That is, can the two ""rough"" equations simply be solved to determine the two unknowns and (rentals and sales)? To be clear, and represent the number of transactions. If solving these equations is valid, then what is the graphical explanation for why these two seemingly unrelated equations (one is number of transactions the other is revenue) can be solved?",r s r s,"['linear-algebra', 'algebra-precalculus', 'systems-of-equations']"
87,What does this theorem in linear algebra actually mean?,What does this theorem in linear algebra actually mean?,,"I've just began the study of linear transformations, and I'm still trying to grasp the concepts fully. One theorem in my textbook is as follows: Let $V$ and $W$ be vector spaces over the field $F$ , and suppose that $(v_1, v_2, \ldots, v_n)$ is a basis for $V$ . For $w_1, w_2, \ldots, w_n$ in $W$ , there exists exactly one linear transformation $T: V \rightarrow W$ such that $T(v_i) = w_i$ for $i=1,2,\ldots,n$ . The author doesn't explain it, but gives the proof right away (which I understand). But I'm trying to figure out what this theorem actually states, and why it is so important? So in words it means: if I have a basis for my domain, and a basis for my codomain, then there exists just one linear transformation that links both of them. So let's say I have a linear map $T: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ , with $T(1,0) = (1,4)$ and $T(1,1)=(2,5)$ . So because $(1,0)$ and $(1,1)$ is a basis for my domain, it is an implication of the theorem that $(1,4)$ and $(2,5)$ is automatically a basis for my codomain?","I've just began the study of linear transformations, and I'm still trying to grasp the concepts fully. One theorem in my textbook is as follows: Let and be vector spaces over the field , and suppose that is a basis for . For in , there exists exactly one linear transformation such that for . The author doesn't explain it, but gives the proof right away (which I understand). But I'm trying to figure out what this theorem actually states, and why it is so important? So in words it means: if I have a basis for my domain, and a basis for my codomain, then there exists just one linear transformation that links both of them. So let's say I have a linear map , with and . So because and is a basis for my domain, it is an implication of the theorem that and is automatically a basis for my codomain?","V W F (v_1, v_2, \ldots, v_n) V w_1, w_2, \ldots, w_n W T: V \rightarrow W T(v_i) = w_i i=1,2,\ldots,n T: \mathbb{R}^2 \rightarrow \mathbb{R}^2 T(1,0) = (1,4) T(1,1)=(2,5) (1,0) (1,1) (1,4) (2,5)","['linear-algebra', 'linear-transformations']"
88,Let $A$ be an $n\times n$ invertible complex matrix such that $A^7 = A^*$. Show that $A^8 = I$.,Let  be an  invertible complex matrix such that . Show that .,A n\times n A^7 = A^* A^8 = I,"Let $A$ be an $n\times n$ invertible complex matrix such that $A^7 = A^*$ (where $*$ denotes conjugate transpose). Show that $A^8 = I$. Here are my thoughts so far: I was able to show that all the eigenvalues of $A$ satisfy $\lambda^8 = 1$. I tried writing $A$ in Jordan canonical form $A = PJP^{-1}$ so that $A^* = (P^{-1})^*J^*P^*$ and $A^7 = PJ^7P^{-1}$. I was hoping to conclude $J^* = J^7$ but this isn't necessarily true as they may not be Jordan matrices; they would be if $J$ was diagonal, but if I knew that $J$ was diagonal, I would be done. As $A^{49} = (A^7)^7 = (A^*)^7 = (A^7)^* = (A^*)^* = A$, $A^{48} = I$ so the minimal polynomial of $A$ divides $x^{48}-1$; note that $x^8 - 1$ is a factor of $x^{48}-1$. If $A^8 = PJP^{-1}$ is the Jordan normal form of $A^8$, then $J = I + N$ and $J^6 = I$ by the previous point so $(I+N)^6 = I$ but I can't directly deduce from this equality that $N = 0$. Any hints are very much appreciated.","Let $A$ be an $n\times n$ invertible complex matrix such that $A^7 = A^*$ (where $*$ denotes conjugate transpose). Show that $A^8 = I$. Here are my thoughts so far: I was able to show that all the eigenvalues of $A$ satisfy $\lambda^8 = 1$. I tried writing $A$ in Jordan canonical form $A = PJP^{-1}$ so that $A^* = (P^{-1})^*J^*P^*$ and $A^7 = PJ^7P^{-1}$. I was hoping to conclude $J^* = J^7$ but this isn't necessarily true as they may not be Jordan matrices; they would be if $J$ was diagonal, but if I knew that $J$ was diagonal, I would be done. As $A^{49} = (A^7)^7 = (A^*)^7 = (A^7)^* = (A^*)^* = A$, $A^{48} = I$ so the minimal polynomial of $A$ divides $x^{48}-1$; note that $x^8 - 1$ is a factor of $x^{48}-1$. If $A^8 = PJP^{-1}$ is the Jordan normal form of $A^8$, then $J = I + N$ and $J^6 = I$ by the previous point so $(I+N)^6 = I$ but I can't directly deduce from this equality that $N = 0$. Any hints are very much appreciated.",,"['linear-algebra', 'matrices']"
89,How do you quickly find the eigenvalues of this matrix?,How do you quickly find the eigenvalues of this matrix?,,"I have a final exam tomorrow, am sure a 3x3 eigen value problem like the one below is there. But I find it very hard to find eigen values without zeros in the matrix Show me how you do it quickly so that I can apply it tomorrow; thanks","I have a final exam tomorrow, am sure a 3x3 eigen value problem like the one below is there. But I find it very hard to find eigen values without zeros in the matrix Show me how you do it quickly so that I can apply it tomorrow; thanks",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
90,multiplying a matrix by a row vector,multiplying a matrix by a row vector,,Is multiplying a matrix by a row vector the same as multiplying it by a column vector? Or are there any differences between the two?,Is multiplying a matrix by a row vector the same as multiplying it by a column vector? Or are there any differences between the two?,,"['linear-algebra', 'matrices', 'vectors']"
91,"If a matrix commutes with all diagonal matrices, must the matrix itself be diagonal?","If a matrix commutes with all diagonal matrices, must the matrix itself be diagonal?",,"I'm new to stackexchange so feel free to correct my style/format/logic etc. The question is this: let's say $A$ is a square matrix of size $n$. I would like to show that $AD = DA$, for any diagonal matrix $D$ also of size $n$, if and only if $A$ is also diagonal. I think I have some of the proof but am not very confident in it. $\Leftarrow $:  If A is diagonal, it is not too hard to show that $AD = DA$, because multiplying two diagonal matrices just amounts to multiplying the corresponding diagonal entries. $\Rightarrow$:  (i) $DA$ is found by multiplying each row in A with the corresponding entry along the diagonal in $D$. $AD$ is found by multiplying each column in $A$ with the corresponding entry along the diagonal in $D$. Since $AD = DA$, this product has to be symmetric. (ii) Now suppose $A$ weren't a diagonal matrix. Then if we make the entries along the diagonal in $D$ all different, $AD$ won't be symmetric anymore (?). This contradicts (i), so we have shown both ways. Does this work? [edited]","I'm new to stackexchange so feel free to correct my style/format/logic etc. The question is this: let's say $A$ is a square matrix of size $n$. I would like to show that $AD = DA$, for any diagonal matrix $D$ also of size $n$, if and only if $A$ is also diagonal. I think I have some of the proof but am not very confident in it. $\Leftarrow $:  If A is diagonal, it is not too hard to show that $AD = DA$, because multiplying two diagonal matrices just amounts to multiplying the corresponding diagonal entries. $\Rightarrow$:  (i) $DA$ is found by multiplying each row in A with the corresponding entry along the diagonal in $D$. $AD$ is found by multiplying each column in $A$ with the corresponding entry along the diagonal in $D$. Since $AD = DA$, this product has to be symmetric. (ii) Now suppose $A$ weren't a diagonal matrix. Then if we make the entries along the diagonal in $D$ all different, $AD$ won't be symmetric anymore (?). This contradicts (i), so we have shown both ways. Does this work? [edited]",,['linear-algebra']
92,Unitary invariance,Unitary invariance,,"Why is it that for any non-negative matrix $M$ and unitary matrix $U$, we have $$\sqrt{UMU^\dagger}=U\sqrt{M}U^\dagger$$? This question has to do with Problem 2c from this sheet . I think I am allowed to assume the ""fact"" but I'd like to know why.","Why is it that for any non-negative matrix $M$ and unitary matrix $U$, we have $$\sqrt{UMU^\dagger}=U\sqrt{M}U^\dagger$$? This question has to do with Problem 2c from this sheet . I think I am allowed to assume the ""fact"" but I'd like to know why.",,"['quantum-mechanics', 'linear-algebra']"
93,"Show that the matrix $A^2 + I$ is invertible for all matrices $A$, where $A$ is an $n \times n$ symmetric matrix.","Show that the matrix  is invertible for all matrices , where  is an  symmetric matrix.",A^2 + I A A n \times n,"I'm a little stuck on this problem. I know that since $A$ is symmetric, $A=A^{T}$. I'm also pretty sure that $AA^{T}$ is invertible. Therefore $A^2$ would be invertible. I'm not really sure how to account for the identity matrix (In). Thanks in advance!","I'm a little stuck on this problem. I know that since $A$ is symmetric, $A=A^{T}$. I'm also pretty sure that $AA^{T}$ is invertible. Therefore $A^2$ would be invertible. I'm not really sure how to account for the identity matrix (In). Thanks in advance!",,"['linear-algebra', 'matrices']"
94,Eigenvector and its corresponding eigenvalue,Eigenvector and its corresponding eigenvalue,,"For the following square matrix: $$ \left( \begin{array}{ccc} 3 & 0 & 1 \\  -4 & 1 & 2 \\  -6 & 0 & -2 \end{array} \right)$$ Decide which, if any, of the following vectors are eigenvectors of   that matrix and give the corresponding eigenvalue. $ \left( \begin{array}{ccc} 2  \\ 2  \\  -1  \end{array} \right)$ $ \left( \begin{array}{ccc}  -1  \\ 0  \\ 2  \end{array} \right)$ $ \left( \begin{array}{ccc}  -1  \\ 1  \\ 3  \end{array} \right)$ $ \left( \begin{array}{ccc} 0  \\ 1  \\ 0  \end{array} \right)$$ \left( \begin{array}{ccc} 3 \\ 2  \\ 1   \end{array} \right)$ If I've understood correctly, I must multiply the matrix by each vector first. If the result is a multiple of that vector, then it's an eigenvector. Only the fourth vector is so. But how should I calculate its corresponding eigenvalue?","For the following square matrix: $$ \left( \begin{array}{ccc} 3 & 0 & 1 \\  -4 & 1 & 2 \\  -6 & 0 & -2 \end{array} \right)$$ Decide which, if any, of the following vectors are eigenvectors of   that matrix and give the corresponding eigenvalue. $ \left( \begin{array}{ccc} 2  \\ 2  \\  -1  \end{array} \right)$ $ \left( \begin{array}{ccc}  -1  \\ 0  \\ 2  \end{array} \right)$ $ \left( \begin{array}{ccc}  -1  \\ 1  \\ 3  \end{array} \right)$ $ \left( \begin{array}{ccc} 0  \\ 1  \\ 0  \end{array} \right)$$ \left( \begin{array}{ccc} 3 \\ 2  \\ 1   \end{array} \right)$ If I've understood correctly, I must multiply the matrix by each vector first. If the result is a multiple of that vector, then it's an eigenvector. Only the fourth vector is so. But how should I calculate its corresponding eigenvalue?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
95,Eigenvalue of a matrix,Eigenvalue of a matrix,,"Let $A$ be an $n\times n$ matrix and let $I$ be the $n\times n$ identity matrix. Show that if $A^{2} = I$, and $A \neq I$, then $\lambda =-1$ is an eigenvalue of $A$. This problem doesn't seem that too hard to solve, but I am stuck near the end. Here is what I have done so far. Since $A^{2}=I$, then by definition $Ax=\lambda x$, where $x$ is an eigenvector of $A$ and $\lambda$ is an eigenvalue of $A$. It follows that $x=Ix=A^{2}x=A(Ax)=A(\lambda x)= \lambda(Ax)=\lambda^{2}x$. (Now I was going to use the fact that since $A \neq I$, that $x\neq 0$, so we get that $\lambda = 1$ or $-1$). This is where I am stuck.","Let $A$ be an $n\times n$ matrix and let $I$ be the $n\times n$ identity matrix. Show that if $A^{2} = I$, and $A \neq I$, then $\lambda =-1$ is an eigenvalue of $A$. This problem doesn't seem that too hard to solve, but I am stuck near the end. Here is what I have done so far. Since $A^{2}=I$, then by definition $Ax=\lambda x$, where $x$ is an eigenvector of $A$ and $\lambda$ is an eigenvalue of $A$. It follows that $x=Ix=A^{2}x=A(Ax)=A(\lambda x)= \lambda(Ax)=\lambda^{2}x$. (Now I was going to use the fact that since $A \neq I$, that $x\neq 0$, so we get that $\lambda = 1$ or $-1$). This is where I am stuck.",,['linear-algebra']
96,Determinant called Grammian,Determinant called Grammian,,"Famously, if functions $f_1,f_2,…,f_n$, each of which possesses a derivative of order $n-1$, are linearly independent on the interval $I$, if  $$ \det\left( \begin{array}{ccccc} f_1 & f_2 & f_3 &… &f_n \\ f'_1 & f'_2 & f'_3 &... &f'_n \\ ⋮ & ⋮ & ⋮ &⋮ &⋮ \\ f_1^{(n-1)} & f_2^{(n-1)} & f_3^{(n-1)} &... &f_n^{(n-1)} \end{array} \right) $$ called Wronskian of $f_1,f_2,…,f_n$ ,is not zero for at least one point in the interval $I$. Equivalently, if functions $f_1,f_2,…,f_n$ possess at least $n-1$ derivatives and are linearly dependent on $I$ then $W(f_1,f_2,…,f_n)(x)=0$ for every $x\in I$. So this equivalent statement gives just a necessary condition for dependency of above functions on the interval. Fortunately, there is necessary and sufficient condition for dependency of a set of functions $f_1(x),f_2(x),…,f_n(x), x\in I$: A set of functions  $f_1(x),f_2(x),…,f_n(x), x\in I$ is linearly dependent on $I$ iff the determinant below is identically zero on $I$:   $$ \det\left( \begin{array}{ccccc} \int_{a}^{b} f_1^2 dx& \int_{a}^{b} f_1f_2 dx&… &\int_{a}^{b}f_1f_ndx \\ \int_{a}^{b}f_2f_1dx & \int_{a}^{b}f_2^2 dx &... &\int_{a}^{b}f_2f_ndx \\ ⋮ & ⋮ & ⋮ &⋮ \\ \int_{a}^{b}f_nf_1dx & \int_{a}^{b}f_nf_2dx&... &\int_{a}^{b}f_n^2dx \end{array} \right) $$ It seems to be a great practical Theorem, but I couldn't find its proof. I really appreciate your help.","Famously, if functions $f_1,f_2,…,f_n$, each of which possesses a derivative of order $n-1$, are linearly independent on the interval $I$, if  $$ \det\left( \begin{array}{ccccc} f_1 & f_2 & f_3 &… &f_n \\ f'_1 & f'_2 & f'_3 &... &f'_n \\ ⋮ & ⋮ & ⋮ &⋮ &⋮ \\ f_1^{(n-1)} & f_2^{(n-1)} & f_3^{(n-1)} &... &f_n^{(n-1)} \end{array} \right) $$ called Wronskian of $f_1,f_2,…,f_n$ ,is not zero for at least one point in the interval $I$. Equivalently, if functions $f_1,f_2,…,f_n$ possess at least $n-1$ derivatives and are linearly dependent on $I$ then $W(f_1,f_2,…,f_n)(x)=0$ for every $x\in I$. So this equivalent statement gives just a necessary condition for dependency of above functions on the interval. Fortunately, there is necessary and sufficient condition for dependency of a set of functions $f_1(x),f_2(x),…,f_n(x), x\in I$: A set of functions  $f_1(x),f_2(x),…,f_n(x), x\in I$ is linearly dependent on $I$ iff the determinant below is identically zero on $I$:   $$ \det\left( \begin{array}{ccccc} \int_{a}^{b} f_1^2 dx& \int_{a}^{b} f_1f_2 dx&… &\int_{a}^{b}f_1f_ndx \\ \int_{a}^{b}f_2f_1dx & \int_{a}^{b}f_2^2 dx &... &\int_{a}^{b}f_2f_ndx \\ ⋮ & ⋮ & ⋮ &⋮ \\ \int_{a}^{b}f_nf_1dx & \int_{a}^{b}f_nf_2dx&... &\int_{a}^{b}f_n^2dx \end{array} \right) $$ It seems to be a great practical Theorem, but I couldn't find its proof. I really appreciate your help.",,"['linear-algebra', 'ordinary-differential-equations', 'determinant', 'inner-products']"
97,Is there a name for the matrix $X(X^tX)^{-1}X^{t}$?,Is there a name for the matrix ?,X(X^tX)^{-1}X^{t},"In my work, I have repeatedly stumbled across the matrix (with a generic matrix $X$ of dimensions $m\times n$ with $m>n$ given) $\Lambda=X(X^tX)^{-1}X^{t}$. It can be characterized by the following: (1) If $v$ is in the span of the column vectors of $X$, then $\Lambda v=v$. (2) If $v$ is orthogonal to the span of the column vectors of $X$, then $\Lambda v = 0$. (we assume that $X$ has full rank). I find this matrix neat, but for my work (in statistics) I need more intuition behind it. What does it mean in a probability context? We are deriving properties of linear regressions, where each row in $X$ is an observation. Is this matrix known, and if so in what context (statistics would be optimal but if it is a celebrated operation in differential geometry, I'd be curious to hear as well)?","In my work, I have repeatedly stumbled across the matrix (with a generic matrix $X$ of dimensions $m\times n$ with $m>n$ given) $\Lambda=X(X^tX)^{-1}X^{t}$. It can be characterized by the following: (1) If $v$ is in the span of the column vectors of $X$, then $\Lambda v=v$. (2) If $v$ is orthogonal to the span of the column vectors of $X$, then $\Lambda v = 0$. (we assume that $X$ has full rank). I find this matrix neat, but for my work (in statistics) I need more intuition behind it. What does it mean in a probability context? We are deriving properties of linear regressions, where each row in $X$ is an observation. Is this matrix known, and if so in what context (statistics would be optimal but if it is a celebrated operation in differential geometry, I'd be curious to hear as well)?",,"['linear-algebra', 'statistics']"
98,Vector Spaces: Finding a basis and Dimension,Vector Spaces: Finding a basis and Dimension,,"I could really use some step-by-step help on these two problems please. Thank You in advance. 1.)   Let $V = \{{\bf{A|A}}$ is an $n \times n$ matrix, $n$ fixed,  det$({\bf{A}}) = 0$ }. Is $V$, with the usual addition and scalar multiplication, a vector space? Give reason. If yes, find the  dimension and a basis for $V$. 2.)   Let $V = \{f(x)|f(x) = (ax + b)e^{-x},\; a,b\; \in\; \mathbb{R}\}$. Is $V$, with the usual addition and scalar multiplication, a vector space? Give reason. If yes, find the dimension and basis for $V$.","I could really use some step-by-step help on these two problems please. Thank You in advance. 1.)   Let $V = \{{\bf{A|A}}$ is an $n \times n$ matrix, $n$ fixed,  det$({\bf{A}}) = 0$ }. Is $V$, with the usual addition and scalar multiplication, a vector space? Give reason. If yes, find the  dimension and a basis for $V$. 2.)   Let $V = \{f(x)|f(x) = (ax + b)e^{-x},\; a,b\; \in\; \mathbb{R}\}$. Is $V$, with the usual addition and scalar multiplication, a vector space? Give reason. If yes, find the dimension and basis for $V$.",,['linear-algebra']
99,"Explanation for a solution: Howard Anton, Elementary Linear Algebra","Explanation for a solution: Howard Anton, Elementary Linear Algebra",,"I am currently working with the 1st edition of Howard Anton's ""Elementary Linear Algebra"". I tried the following problem: Excercise Set 1.2 (p. 17), Problem 12: For which values of $a$ will the following system have no solutions? Exactly one solution? Infinitely many solutions? $$\begin{array}{rccccl}  x &+& 2y &-& 3z &=& 4 \\  3x &-& y &+& 5z &=& 2 \\ 4x &+& y &+& (a^2 - 14)z &=& a + 2\end{array}$$ By using Gauss-Jordan-Elimination (and Gaussian for a double check), I found the following solution set: $$\begin{align*} x &= \frac{8}{7} + \frac{-a+4}{a^2-16} \\  y &= \frac{10}{7} + \frac{2a-8}{a^2-16} \\ z &= \frac{a-4}{a^2-16}\end{align*}$$ The solution in the textbook says, that the system has no solution if $a=-4$ an one solution if $a\neq\pm4$ . This part I understand, since $z=\frac{a-4}{a^2-16}$ as well as others terms in the formulas for $y$ and $z$ are not defined for $a=-4$ but the formulas for $x,y,z$ will yield unambiguous values for $a\neq\pm4$ . However, the solution also says, that the system has infinitely many solutions for $a=4$ and this is the point, which I don't understand. Isn't for instance $z=\frac{a-4}{a^2-16}$ still undefined for $a=-4$ or can I simply put in $z = \frac{0}{0} = 0$ . And if I can, isn't $z=0$ still a unique value. I can find no room for different values of $x,y,z$ to satisfy the system of equations. Can somebody explain this to me? Thanks in advance!","I am currently working with the 1st edition of Howard Anton's ""Elementary Linear Algebra"". I tried the following problem: Excercise Set 1.2 (p. 17), Problem 12: For which values of will the following system have no solutions? Exactly one solution? Infinitely many solutions? By using Gauss-Jordan-Elimination (and Gaussian for a double check), I found the following solution set: The solution in the textbook says, that the system has no solution if an one solution if . This part I understand, since as well as others terms in the formulas for and are not defined for but the formulas for will yield unambiguous values for . However, the solution also says, that the system has infinitely many solutions for and this is the point, which I don't understand. Isn't for instance still undefined for or can I simply put in . And if I can, isn't still a unique value. I can find no room for different values of to satisfy the system of equations. Can somebody explain this to me? Thanks in advance!","a \begin{array}{rccccl} 
x &+& 2y &-& 3z &=& 4 \\ 
3x &-& y &+& 5z &=& 2 \\
4x &+& y &+& (a^2 - 14)z &=& a + 2\end{array} \begin{align*} x &= \frac{8}{7} + \frac{-a+4}{a^2-16} \\ 
y &= \frac{10}{7} + \frac{2a-8}{a^2-16} \\
z &= \frac{a-4}{a^2-16}\end{align*} a=-4 a\neq\pm4 z=\frac{a-4}{a^2-16} y z a=-4 x,y,z a\neq\pm4 a=4 z=\frac{a-4}{a^2-16} a=-4 z = \frac{0}{0} = 0 z=0 x,y,z","['linear-algebra', 'systems-of-equations']"
