,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,When is the solution to an ODE $\dot x = f(x)$ uniformly continuous?,When is the solution to an ODE  uniformly continuous?,\dot x = f(x),"In many applications, it is highly desirable to know when the solution $x(t)$ to an ODE $\dot x = f(x)$, is uniformly continuous. However, there seems to be very few words written on conditions on $f$ when this is true. In general, the uniform continuity of $x(t)$ is given by the Caratheodory existence theorem ( https://en.wikipedia.org/wiki/Carath%C3%A9odory%27s_existence_theorem ). But this is a very general theorem that works for discontinuous functions $f$, and the conditions are a bit difficult to check. In any case, the existence theorem provides conditions on $f$ such that $x(t)$ is absolutely continuous, hence uniformly continuous. Is there a way to conclude uniform continuity without resorting to absolute continuity? Can we conclude uniform continuity of the solution directly from Cauchy-Lipschitz or Peano's existence theorem? What are some ""nice"" conditions on $f$ that allows us to immediately conclude $x(t)$ is uniformly continuous? (i.e. suppose $f$ is Lipschitz, then is $x(t)$ u.c.?)","In many applications, it is highly desirable to know when the solution $x(t)$ to an ODE $\dot x = f(x)$, is uniformly continuous. However, there seems to be very few words written on conditions on $f$ when this is true. In general, the uniform continuity of $x(t)$ is given by the Caratheodory existence theorem ( https://en.wikipedia.org/wiki/Carath%C3%A9odory%27s_existence_theorem ). But this is a very general theorem that works for discontinuous functions $f$, and the conditions are a bit difficult to check. In any case, the existence theorem provides conditions on $f$ such that $x(t)$ is absolutely continuous, hence uniformly continuous. Is there a way to conclude uniform continuity without resorting to absolute continuity? Can we conclude uniform continuity of the solution directly from Cauchy-Lipschitz or Peano's existence theorem? What are some ""nice"" conditions on $f$ that allows us to immediately conclude $x(t)$ is uniformly continuous? (i.e. suppose $f$ is Lipschitz, then is $x(t)$ u.c.?)",,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'continuity']"
1,Show that if y satisfies $y''+y=\sin^{2017}{x}\cos x$ then $y$ is a periodic function.,Show that if y satisfies  then  is a periodic function.,y''+y=\sin^{2017}{x}\cos x y,"Show that if y satisfies $y''+y=\sin^{2017} x \cdot \cos x$ then $y$ is a periodic function. To deal with this problem, the first thing I do is solving $y''+y=0$. Then I get $y=c_1 \cos x +c_2 \sin x$, and I get stuck here. Any suggestion? Thanks in advance!","Show that if y satisfies $y''+y=\sin^{2017} x \cdot \cos x$ then $y$ is a periodic function. To deal with this problem, the first thing I do is solving $y''+y=0$. Then I get $y=c_1 \cos x +c_2 \sin x$, and I get stuck here. Any suggestion? Thanks in advance!",,"['ordinary-differential-equations', 'periodic-functions']"
2,How to solve a differential equation with sign function?,How to solve a differential equation with sign function?,,"I was modeling the motion of a spring oscillator with dry friction and thus I met with this equation $$kx+\operatorname{sgn} (x′)⋅|f|+mx''=0,$$ where $x$ stands for the position (positive for stretching the spring and negative for compressing it) and $k,|f|$ and $m$ are constants. I just want to solve the equation without involving more physics things like the law of energy conservation.","I was modeling the motion of a spring oscillator with dry friction and thus I met with this equation $$kx+\operatorname{sgn} (x′)⋅|f|+mx''=0,$$ where $x$ stands for the position (positive for stretching the spring and negative for compressing it) and $k,|f|$ and $m$ are constants. I just want to solve the equation without involving more physics things like the law of energy conservation.",,['ordinary-differential-equations']
3,Question on an interval that makes the solution unique,Question on an interval that makes the solution unique,,"http://www.math.uiuc.edu/~tyson/existence.pdf (example 5) In the example, the solution for the interval is $\delta_2 = |x_0|$. I'm confused on how they got that. What was the algebraic step to solve for $\delta_2$? I mean I can reason it out, and it makes sense. However, how do I actually solve for it algebraically? Thank you!","http://www.math.uiuc.edu/~tyson/existence.pdf (example 5) In the example, the solution for the interval is $\delta_2 = |x_0|$. I'm confused on how they got that. What was the algebraic step to solve for $\delta_2$? I mean I can reason it out, and it makes sense. However, how do I actually solve for it algebraically? Thank you!",,['ordinary-differential-equations']
4,Calculating stability and order of implicit midpoint scheme,Calculating stability and order of implicit midpoint scheme,,"Consider solving $y'(t) = f(t,y(t))$ by the implicit midpoint method: $$ y_{n+1} = y_n + h \cdot f \left(t_n + \frac{h}{2},\frac{y(t_n) + y(t_{n+1})}{2} \right). $$ I want to determine the order and regions of stability for this method. Order. My first idea was to do the following: Substitute the exact solution. \begin{align*} &y(t_{n+1}) - \left[ y(t_n) + h \cdot f \left(t_n + \frac{h}{2},\frac{y(t_n) + y(t_{n+1})}{2} \right) \right] \\ =& [ y(t_n) + hy '(t_n) + \frac{1}{2} h^2 y''(t_n) + O(h^3) ] \\ - & [ y(t_n) + h \left[ y'(t_n) + \frac{1}{2} h y''(t_n) + O(h^2) \right] \end{align*} which is $O(h^3)$. But I don't think I can substitute in for the last part $f(t,y(t)) = y'(t)$ since $\frac{y(t_n) + y(t_{n+1})}{2} \neq y \left( t_n +  \frac{h}{2} \right)$ . . . It's only an approximation. Stability. Apply to the test problem $y'(t) = f(t,y(t)) = \lambda y(t)$. This is done here ( Determine a stability region? ), but again $f(\cdot, y(\cdot))$ is not of the form $f(t, y(t))$, so I don't know why the solution is valid. In particular, why is the test problem $y' =f(t,y(t)) =  \lambda y(t)$ applied to $f(t_{n+1/2} , (y_n + y_{n+1})/2)$ equal to $\lambda (y_n + y_{n+1})/2$? The questions are similar and I probably have some misconception on numerics that is (hopefully) easy to clarify. EDIT: Should I just think about $f \left(t_n + \frac{h}{2},\frac{y(t_n) + y(t_{n+1})}{2} \right)$ as $f(t_{n+1/2} , y_{n+1/2})$? If so I think my question is answered, but I would appreciate someone wiser in the field taking a look.","Consider solving $y'(t) = f(t,y(t))$ by the implicit midpoint method: $$ y_{n+1} = y_n + h \cdot f \left(t_n + \frac{h}{2},\frac{y(t_n) + y(t_{n+1})}{2} \right). $$ I want to determine the order and regions of stability for this method. Order. My first idea was to do the following: Substitute the exact solution. \begin{align*} &y(t_{n+1}) - \left[ y(t_n) + h \cdot f \left(t_n + \frac{h}{2},\frac{y(t_n) + y(t_{n+1})}{2} \right) \right] \\ =& [ y(t_n) + hy '(t_n) + \frac{1}{2} h^2 y''(t_n) + O(h^3) ] \\ - & [ y(t_n) + h \left[ y'(t_n) + \frac{1}{2} h y''(t_n) + O(h^2) \right] \end{align*} which is $O(h^3)$. But I don't think I can substitute in for the last part $f(t,y(t)) = y'(t)$ since $\frac{y(t_n) + y(t_{n+1})}{2} \neq y \left( t_n +  \frac{h}{2} \right)$ . . . It's only an approximation. Stability. Apply to the test problem $y'(t) = f(t,y(t)) = \lambda y(t)$. This is done here ( Determine a stability region? ), but again $f(\cdot, y(\cdot))$ is not of the form $f(t, y(t))$, so I don't know why the solution is valid. In particular, why is the test problem $y' =f(t,y(t)) =  \lambda y(t)$ applied to $f(t_{n+1/2} , (y_n + y_{n+1})/2)$ equal to $\lambda (y_n + y_{n+1})/2$? The questions are similar and I probably have some misconception on numerics that is (hopefully) easy to clarify. EDIT: Should I just think about $f \left(t_n + \frac{h}{2},\frac{y(t_n) + y(t_{n+1})}{2} \right)$ as $f(t_{n+1/2} , y_{n+1/2})$? If so I think my question is answered, but I would appreciate someone wiser in the field taking a look.",,"['ordinary-differential-equations', 'numerical-methods']"
5,How should I factorize a differential equation from the first order but with higher degrees?,How should I factorize a differential equation from the first order but with higher degrees?,,My question is the following: How should I factorize a differential equation from the first order but with higher degrees? Imagine I have: $$x^2(y')^2+xyy'-6y^2 = 0$$ Is it okay if I replace $y'$ with $p$ and just solve the equation for $p$ with $x$ and $y$ being constants? Thanks in advance!,My question is the following: How should I factorize a differential equation from the first order but with higher degrees? Imagine I have: $$x^2(y')^2+xyy'-6y^2 = 0$$ Is it okay if I replace $y'$ with $p$ and just solve the equation for $p$ with $x$ and $y$ being constants? Thanks in advance!,,['ordinary-differential-equations']
6,Flow of vector field,Flow of vector field,,"For my PDE class I have to find the flow of the following vector field $$\mathbf{F}: \mathbb{R}^2-\{0\} \to \mathbb{R}^2, \mathbf{F}(x_1,x_2) = \frac{1}{r}(-x_2, x_1)$$ where $$r = \frac{1}{\sqrt{x_1^2 + x_2^2}}$$ I know that I can find the flow of this vector field by setting $$\mathbf{\dot{x}}(t) = \mathbf{F}(\mathbf{x}(t)) $$ which yields the equations $$ \dot{x_1} = \frac{1}{r} (-x_2) $$ and $$ \dot{x_2} = \frac{1}{r} x_1 $$ or in matrix notation  $$ \dot{\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}} = \begin{pmatrix} 0 & -\frac{1}{r} \\ \frac{1}{r} & 0 \end{pmatrix}\begin{pmatrix} x_1 \\ x_2 \end{pmatrix} $$ Normally, I would solve this quite easily with an exponential ansatz. But here, $r$ itself depends on $x$ and $y$ which makes the matrix nonlinear. Thus I have no idea how to approach this?","For my PDE class I have to find the flow of the following vector field $$\mathbf{F}: \mathbb{R}^2-\{0\} \to \mathbb{R}^2, \mathbf{F}(x_1,x_2) = \frac{1}{r}(-x_2, x_1)$$ where $$r = \frac{1}{\sqrt{x_1^2 + x_2^2}}$$ I know that I can find the flow of this vector field by setting $$\mathbf{\dot{x}}(t) = \mathbf{F}(\mathbf{x}(t)) $$ which yields the equations $$ \dot{x_1} = \frac{1}{r} (-x_2) $$ and $$ \dot{x_2} = \frac{1}{r} x_1 $$ or in matrix notation  $$ \dot{\begin{pmatrix} x_1 \\ x_2 \end{pmatrix}} = \begin{pmatrix} 0 & -\frac{1}{r} \\ \frac{1}{r} & 0 \end{pmatrix}\begin{pmatrix} x_1 \\ x_2 \end{pmatrix} $$ Normally, I would solve this quite easily with an exponential ansatz. But here, $r$ itself depends on $x$ and $y$ which makes the matrix nonlinear. Thus I have no idea how to approach this?",,"['ordinary-differential-equations', 'vector-analysis']"
7,For what $r$ does the Lorenz attractor exist?,For what  does the Lorenz attractor exist?,r,"The Lorenz equations are given by: $$\begin{aligned} \dot X &= \sigma(Y-X)\\ \dot Y &= rX - Y - XZ\\ \dot Z &= X Y - bZ \end{aligned}$$ I know that for $r\gt r_H$ all trajectories go over to a strange attractor, but this does not necessarily mean that the attractor exists for only $r\gt r_H$ . For what values of $r$ does this attractor exist and why?","The Lorenz equations are given by: I know that for all trajectories go over to a strange attractor, but this does not necessarily mean that the attractor exists for only . For what values of does this attractor exist and why?",\begin{aligned} \dot X &= \sigma(Y-X)\\ \dot Y &= rX - Y - XZ\\ \dot Z &= X Y - bZ \end{aligned} r\gt r_H r\gt r_H r,"['ordinary-differential-equations', 'dynamical-systems', 'chaos-theory']"
8,"Fourier Series, Sturm-Liouville Problem - What is the connection that I'm missing?","Fourier Series, Sturm-Liouville Problem - What is the connection that I'm missing?",,"Fourier Series So in my book we are talking about Fourier Series. $$f(x) = \frac{1}{2}a_0+\sum_{n=1}^\infty\left[a_n\sin{\left(\frac{n\pi x}{L}\right)}+b_n\cos{\left(\frac{n\pi x}{L}\right)}\right]$$ It says the represents ""fairly nice"" periodic functions of period $L$. Sturm Liouville Problem A sturm liouville problem is one where $$(p(x)y')'+q(x)y+\lambda w(x)y=0 \qquad p(x),w(x)>0 \qquad \text{on} \qquad x_0 \leq x \leq x_1$$ with Boundary Conditions that allow self-adjointness i.e. Dirichlet, Neumann, Singular POint, Periodic or Radiation. Solution of Sturm Liouville Problem When solving the SL problem, we normally look at  three different values for $\lambda$ ($0,>0,<0$), find the eigenvalues (which are all positive and real), find the eigenfunctions (which are all real) and so if I pick any eigenvalue and its respective eigenfunction, that is a solution to the SL problem (i.e. the Differential Equation with those eigenvalues substituted to lambda has a solution, the eigenfunction, that respects the Boundary Conditions). Connection between the two My book then briefly says that ""$\sin{\left(\frac{n\pi x}{L}\right)}$ and $\cos{\left(\frac{n\pi x}{L}\right)}$ are eigenfunctions of the SL problem $y''+\lambda y=0$ on $[-L,L]$ with periodic BCs. And that $$\left\{1, \sin{\left(\frac{n\pi x}{L}\right)}, \cos{\left(\frac{n\pi x}{L}\right)}\right\}$$ form an orthogonal basis for the space of $2L$ -periodic ""nice"" functions."" My trial So I tried, using $w(x)=1$, $q(x)=0$, $p(x)=1$, on $[-L,L]$ with $y(-L)=y(L)$ and $y'(-L)=y'(L)$ $\lambda =0$ gives $y=Ax+B$ and applying the conditions gives $A=0$. However doesn't tell us anything about B. $\lambda=-\mu^2$ ($\mu\neq 0$) gives $y=Ae^{\mu x}+Be^{-\mu x}$ and applying the conditions gives only the trivial solution $y\equiv 0$ $\lambda=\mu^2$ ($\mu\neq 0$) gives $y =A\sin{(\mu x)}+B\cos{(\mu x)}$ and applying the conditions gives$\mu=\frac{n\pi}{L}$ only. However I don't know how to continue to connect the two. I got another solution from the case $\lambda =0$, how do I unify everything? But in general my question is What is the connection between fourier series and SL problems? How can I show that sin and cos are eigenfunctions of that particular sturm liouville problem? And finally, if that is the case, what are the sturm-liouville's problems associated with sine fourier series and cosine fourier series?","Fourier Series So in my book we are talking about Fourier Series. $$f(x) = \frac{1}{2}a_0+\sum_{n=1}^\infty\left[a_n\sin{\left(\frac{n\pi x}{L}\right)}+b_n\cos{\left(\frac{n\pi x}{L}\right)}\right]$$ It says the represents ""fairly nice"" periodic functions of period $L$. Sturm Liouville Problem A sturm liouville problem is one where $$(p(x)y')'+q(x)y+\lambda w(x)y=0 \qquad p(x),w(x)>0 \qquad \text{on} \qquad x_0 \leq x \leq x_1$$ with Boundary Conditions that allow self-adjointness i.e. Dirichlet, Neumann, Singular POint, Periodic or Radiation. Solution of Sturm Liouville Problem When solving the SL problem, we normally look at  three different values for $\lambda$ ($0,>0,<0$), find the eigenvalues (which are all positive and real), find the eigenfunctions (which are all real) and so if I pick any eigenvalue and its respective eigenfunction, that is a solution to the SL problem (i.e. the Differential Equation with those eigenvalues substituted to lambda has a solution, the eigenfunction, that respects the Boundary Conditions). Connection between the two My book then briefly says that ""$\sin{\left(\frac{n\pi x}{L}\right)}$ and $\cos{\left(\frac{n\pi x}{L}\right)}$ are eigenfunctions of the SL problem $y''+\lambda y=0$ on $[-L,L]$ with periodic BCs. And that $$\left\{1, \sin{\left(\frac{n\pi x}{L}\right)}, \cos{\left(\frac{n\pi x}{L}\right)}\right\}$$ form an orthogonal basis for the space of $2L$ -periodic ""nice"" functions."" My trial So I tried, using $w(x)=1$, $q(x)=0$, $p(x)=1$, on $[-L,L]$ with $y(-L)=y(L)$ and $y'(-L)=y'(L)$ $\lambda =0$ gives $y=Ax+B$ and applying the conditions gives $A=0$. However doesn't tell us anything about B. $\lambda=-\mu^2$ ($\mu\neq 0$) gives $y=Ae^{\mu x}+Be^{-\mu x}$ and applying the conditions gives only the trivial solution $y\equiv 0$ $\lambda=\mu^2$ ($\mu\neq 0$) gives $y =A\sin{(\mu x)}+B\cos{(\mu x)}$ and applying the conditions gives$\mu=\frac{n\pi}{L}$ only. However I don't know how to continue to connect the two. I got another solution from the case $\lambda =0$, how do I unify everything? But in general my question is What is the connection between fourier series and SL problems? How can I show that sin and cos are eigenfunctions of that particular sturm liouville problem? And finally, if that is the case, what are the sturm-liouville's problems associated with sine fourier series and cosine fourier series?",,"['ordinary-differential-equations', 'partial-differential-equations', 'fourier-analysis', 'fourier-series', 'sturm-liouville']"
9,Non-linear first order differential equations,Non-linear first order differential equations,,What technique should I use to solve the following first-order nonlinear ordinary differential equation: $$y'(x)+\log (y(x))=-x-1$$ Thanks in advance,What technique should I use to solve the following first-order nonlinear ordinary differential equation: $$y'(x)+\log (y(x))=-x-1$$ Thanks in advance,,['ordinary-differential-equations']
10,From second order nonhomogeneous ODE to hypergeometric equation,From second order nonhomogeneous ODE to hypergeometric equation,,"I need to solve the following differential equation: $$y''(x)+\left(a_1+a_2e^{-\gamma x}\right) y'(x)+ \left(b_1+b_2e^{-\gamma x}+b_3 e^{-2\gamma x}\right) y(x)=0 $$ It is linear, the expression is plain and clean, but the coefficient are not constant, which constitutes a big problem. What is the approach I have to use? Does someone know how to solve it? Unfortunately I didn't find anything in the literature but I hope this community will surprise me. EDIT: From this point on I continued thanks to Kiryl Pesotski's suggestion. If we apply the substitution $z=e^{-\gamma x}$, we have that: $$\frac{d}{dx}y(x)=\frac{dz}{dx}\frac{d}{dz}y(-\log(z)/\gamma)=-\gamma e^{-\gamma x}\frac{d}{dz}y(-\log(z)/\gamma)=-\gamma z \frac{d}{dz}y(-\log(z)/\gamma)$$ where, for simplicity, we can rename $y(-\log(z)/\gamma)$ as $y(z)$. Then: $$\frac{d^2}{dx^2}y(x)=\frac{dz}{dx}\frac{d}{dz}\left(-\gamma z y'(z)\right)=\cdots =\gamma^2 z(y'(z)+zy''(z))$$ The ode becomes: $$z^2 y''(z)+\frac{1}{\gamma}zy'(z)\left(\gamma-a_1-a_2z\right)+\frac{1}{\gamma^2}y(z)(b_1+b_2z+b_3z^2)=0$$ Now I exploit the second suggestion because I am supposed to get an hypergeometric equation. The substitution $y(z)=x^{-\frac{\gamma-a_1}{2\gamma}}e^{-\frac{a_2}{\gamma^2}}g(\xi)$ with $\xi=\sqrt{\frac{b_1+\gamma^2}{\gamma^2}-\frac{b_3}{\gamma^2}}z$ has the following first derivative: $$\frac{d}{dz}y=\frac{1}{\gamma z}\frac{\gamma-a_1}{\gamma}x^{-\frac{\gamma-a_1}{2\gamma}-1}e^{-\frac{a_2}{\gamma^2}}g(\xi)+x^{-\frac{\gamma-a_1}{2\gamma}}e^{-\frac{a_2}{\gamma^2}}g'(\xi)\sqrt{\frac{b_1+\gamma^2}{\gamma^2}-\frac{b_3}{\gamma^2}}$$ However, if I have not made any mistake, I don't understand how from virtue of this substitution I can get to the final solution. Can someone know how to complete the passages and get the hypergeometric function solution of this equation?","I need to solve the following differential equation: $$y''(x)+\left(a_1+a_2e^{-\gamma x}\right) y'(x)+ \left(b_1+b_2e^{-\gamma x}+b_3 e^{-2\gamma x}\right) y(x)=0 $$ It is linear, the expression is plain and clean, but the coefficient are not constant, which constitutes a big problem. What is the approach I have to use? Does someone know how to solve it? Unfortunately I didn't find anything in the literature but I hope this community will surprise me. EDIT: From this point on I continued thanks to Kiryl Pesotski's suggestion. If we apply the substitution $z=e^{-\gamma x}$, we have that: $$\frac{d}{dx}y(x)=\frac{dz}{dx}\frac{d}{dz}y(-\log(z)/\gamma)=-\gamma e^{-\gamma x}\frac{d}{dz}y(-\log(z)/\gamma)=-\gamma z \frac{d}{dz}y(-\log(z)/\gamma)$$ where, for simplicity, we can rename $y(-\log(z)/\gamma)$ as $y(z)$. Then: $$\frac{d^2}{dx^2}y(x)=\frac{dz}{dx}\frac{d}{dz}\left(-\gamma z y'(z)\right)=\cdots =\gamma^2 z(y'(z)+zy''(z))$$ The ode becomes: $$z^2 y''(z)+\frac{1}{\gamma}zy'(z)\left(\gamma-a_1-a_2z\right)+\frac{1}{\gamma^2}y(z)(b_1+b_2z+b_3z^2)=0$$ Now I exploit the second suggestion because I am supposed to get an hypergeometric equation. The substitution $y(z)=x^{-\frac{\gamma-a_1}{2\gamma}}e^{-\frac{a_2}{\gamma^2}}g(\xi)$ with $\xi=\sqrt{\frac{b_1+\gamma^2}{\gamma^2}-\frac{b_3}{\gamma^2}}z$ has the following first derivative: $$\frac{d}{dz}y=\frac{1}{\gamma z}\frac{\gamma-a_1}{\gamma}x^{-\frac{\gamma-a_1}{2\gamma}-1}e^{-\frac{a_2}{\gamma^2}}g(\xi)+x^{-\frac{\gamma-a_1}{2\gamma}}e^{-\frac{a_2}{\gamma^2}}g'(\xi)\sqrt{\frac{b_1+\gamma^2}{\gamma^2}-\frac{b_3}{\gamma^2}}$$ However, if I have not made any mistake, I don't understand how from virtue of this substitution I can get to the final solution. Can someone know how to complete the passages and get the hypergeometric function solution of this equation?",,"['ordinary-differential-equations', 'hypergeometric-function']"
11,Why does the complex gain include the initial amplitude?,Why does the complex gain include the initial amplitude?,,"While solving a differential equation like $(D^2+4)x=A\cos\omega t$, for example, the complex gain is said to be $\dfrac{1}{4-\omega^2}A$. Is there any deeper reason why it is defined this way, as opposed to simply $\dfrac{1}{4-\omega^2}$, which is the actual ratio of the solution $x(t)$ to the input signal $A\cos\omega t$? Or is the input signal itself defined as $\cos\omega t$, rather than as $A\cos\omega t$? If so, why?","While solving a differential equation like $(D^2+4)x=A\cos\omega t$, for example, the complex gain is said to be $\dfrac{1}{4-\omega^2}A$. Is there any deeper reason why it is defined this way, as opposed to simply $\dfrac{1}{4-\omega^2}$, which is the actual ratio of the solution $x(t)$ to the input signal $A\cos\omega t$? Or is the input signal itself defined as $\cos\omega t$, rather than as $A\cos\omega t$? If so, why?",,"['calculus', 'ordinary-differential-equations', 'signal-processing']"
12,"Solving the matrix differential equation $\frac{d M} {dt} = \kappa \hspace{1mm} \max(0,(M_1-M) -\gamma I )$",Solving the matrix differential equation,"\frac{d M} {dt} = \kappa \hspace{1mm} \max(0,(M_1-M) -\gamma I )","I have a weird equation that I want to solve, $$\frac{d M} {dt} = \kappa  \hspace{1mm} \max(0,(M_1-M) -\gamma I )$$ where, $M, M_1, I$ are all $2 \times 2$ matrices. With the $\max$ function I want to make sure the matrix $M$ does not shrink or become smaller, so the $\max$ condition I am imagining is that the condition for the argument matrix to be positive definite.","I have a weird equation that I want to solve, $$\frac{d M} {dt} = \kappa  \hspace{1mm} \max(0,(M_1-M) -\gamma I )$$ where, $M, M_1, I$ are all $2 \times 2$ matrices. With the $\max$ function I want to make sure the matrix $M$ does not shrink or become smaller, so the $\max$ condition I am imagining is that the condition for the argument matrix to be positive definite.",,"['linear-algebra', 'ordinary-differential-equations', 'matrix-calculus']"
13,"Differentiable functions $f:\Bbb R \rightarrow \Bbb R^n$ satisfying the differential equation $f' = Af$, where $A\in \Bbb R^{n,n}$","Differentiable functions  satisfying the differential equation , where","f:\Bbb R \rightarrow \Bbb R^n f' = Af A\in \Bbb R^{n,n}","Let $A \in \Bbb R^{n,n}$. I want to find all differentiable functions $f:\Bbb R \rightarrow \Bbb R^n$ satisfying the differential equation $f' = Af$. Please can anyone lend a hand here?","Let $A \in \Bbb R^{n,n}$. I want to find all differentiable functions $f:\Bbb R \rightarrow \Bbb R^n$ satisfying the differential equation $f' = Af$. Please can anyone lend a hand here?",,"['linear-algebra', 'matrices', 'ordinary-differential-equations']"
14,Local center manifold,Local center manifold,,"Consider the system $$ x'=x^2, \quad  y'=y $$ and find the local center manifold of the equilibrium $(0,0)$. The linearization matrix is $$ A=\begin{pmatrix}0 & 0\\0 & 1\end{pmatrix}, $$ hence the eigenvalues are $\lambda_1=0$ and $\lambda_2=1$. The center space is spanned by the eigenvector belonging to $\lambda_1$, say $v=(1,0)$. Hence the $x$-axis should be the center space. But what is the center manifold? Since the center manifold $W_c$ should be tangent to the x-axis in point (0,0), my idea would be to write $$ W_c=\left\{(x,y): y=h(x)\right\}, \text{with }h(0)=0, h'(0)=0 $$ and then to make some ansatz like second order approximation: $$ h(x)=ax²+O(\lvert x\rvert³). $$ Do we then get that $h(x)=0$ is a second order approximation of $W_c$, i.e. $W_c$ is the x-axis?","Consider the system $$ x'=x^2, \quad  y'=y $$ and find the local center manifold of the equilibrium $(0,0)$. The linearization matrix is $$ A=\begin{pmatrix}0 & 0\\0 & 1\end{pmatrix}, $$ hence the eigenvalues are $\lambda_1=0$ and $\lambda_2=1$. The center space is spanned by the eigenvector belonging to $\lambda_1$, say $v=(1,0)$. Hence the $x$-axis should be the center space. But what is the center manifold? Since the center manifold $W_c$ should be tangent to the x-axis in point (0,0), my idea would be to write $$ W_c=\left\{(x,y): y=h(x)\right\}, \text{with }h(0)=0, h'(0)=0 $$ and then to make some ansatz like second order approximation: $$ h(x)=ax²+O(\lvert x\rvert³). $$ Do we then get that $h(x)=0$ is a second order approximation of $W_c$, i.e. $W_c$ is the x-axis?",,"['ordinary-differential-equations', 'manifolds', 'dynamical-systems']"
15,Meaning of the inverse of a differential operator,Meaning of the inverse of a differential operator,,Consider the Poisson's equation $$\nabla^2\phi(\textbf{x})=-\rho(\textbf{x})/\epsilon_0.$$ What is the meaning of the inverse operator in the following $$\phi(\textbf{x})=-\frac{1}{\nabla^2}\frac{\rho(\textbf{x})}{\epsilon_0}.$$ How do I show that $\frac{1}{\nabla^2}$ is equivalent to an integral operator acting on $\rho(\textbf{x})$?,Consider the Poisson's equation $$\nabla^2\phi(\textbf{x})=-\rho(\textbf{x})/\epsilon_0.$$ What is the meaning of the inverse operator in the following $$\phi(\textbf{x})=-\frac{1}{\nabla^2}\frac{\rho(\textbf{x})}{\epsilon_0}.$$ How do I show that $\frac{1}{\nabla^2}$ is equivalent to an integral operator acting on $\rho(\textbf{x})$?,,"['ordinary-differential-equations', 'operator-theory', 'fourier-transform', 'greens-function', 'integral-operators']"
16,Doubt about Cauchy-Lipshitz theorem use,Doubt about Cauchy-Lipshitz theorem use,,"I'll show my doubt with the Cauchy problem $\begin{cases} y'=1+y^2  \\ y(0)=0 \end{cases}$. I know the solution is $y(t)=tg(t)$, but let's say I don't know the explicit solution. If I look at $1+y^2$, then I have all the hypothesis for the application of Cauchy-Lipshitz Theorem in $[0, 2\pi]$ for instance, because it is lipshitz there (or not?). So I have a unique solution in $[0,2\pi]$, but now if I wonder if I can prolong the solution, how can I understand that in fact I cannot prolong the solution to $2\pi$ but at best to $\pi/2$ without solving  the differential equation? I mean, where do I commit a mistake in using the Cauchy-Lipshitz Theorem? If you have some other (not simple like mine) examples that have the same problem, I would like to see how to solve them in the right way. Thanks for the help.","I'll show my doubt with the Cauchy problem $\begin{cases} y'=1+y^2  \\ y(0)=0 \end{cases}$. I know the solution is $y(t)=tg(t)$, but let's say I don't know the explicit solution. If I look at $1+y^2$, then I have all the hypothesis for the application of Cauchy-Lipshitz Theorem in $[0, 2\pi]$ for instance, because it is lipshitz there (or not?). So I have a unique solution in $[0,2\pi]$, but now if I wonder if I can prolong the solution, how can I understand that in fact I cannot prolong the solution to $2\pi$ but at best to $\pi/2$ without solving  the differential equation? I mean, where do I commit a mistake in using the Cauchy-Lipshitz Theorem? If you have some other (not simple like mine) examples that have the same problem, I would like to see how to solve them in the right way. Thanks for the help.",,"['ordinary-differential-equations', 'cauchy-problem']"
17,A question about the PDE: $u_t-\Delta u=au-bvu$,A question about the PDE:,u_t-\Delta u=au-bvu,"Consider the following system of partial differential equations (see here for more details): I just want to know how the author got that expression after multiplying by $u^-$ and integrating over $\Omega$ in the proof of Lemma 2.3. I think the function $u^-=\min(0,u)$ is not differentiable  in time.","Consider the following system of partial differential equations (see here for more details): I just want to know how the author got that expression after multiplying by $u^-$ and integrating over $\Omega$ in the proof of Lemma 2.3. I think the function $u^-=\min(0,u)$ is not differentiable  in time.",,"['functional-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'lp-spaces']"
18,Solving a homogenous differential equation with two complex eigenvalues,Solving a homogenous differential equation with two complex eigenvalues,,"A physical scenario in which a spring is hanging with a weight of mass $m$ on it yelds the following differential equation for the position $z(t)$ : $$mz'' = mg - f(z - z_0)$$ In order to solve for $z$ we first need to solve the homogenous equation: $$mz'' + fz = 0$$ Assume $z(t) = e^{\lambda t}$ : $$\lambda^2 m e^{\lambda t} + fe^{\lambda t} = 0$$ $$\lambda_{1,2} = \mp i\sqrt{\frac{f}{m}}$$ Having two eigenvalues, the general solution of the homogenous differential equation is a linear combination of the two: $$z_{\rm hom.}(t) = c_1e^{-i\sqrt{\frac{f}{m}}t} + c_2e^{i\sqrt{\frac{f}{m}}t}$$ We're in the context of physics and this is an oscillation, so we want to transform this result using trigonometric functions to make it more explicit. According to the solution supplied to me this is how it goes: \begin{align} z_{\rm hom.}(t) &= c_1e^{i\sqrt{\frac{f}{m}}t} + c_2e^{-i\sqrt{\frac{f}{m}}t} \\ &= c_1\left(\cos\left(\sqrt{\frac{f}{m}}t\right) + i\sin\left(\sqrt{\frac{f}{m}}t\right)\right) + c_2\left(\cos\left(\sqrt{\frac{f}{m}}t\right) - i\sin\left(\sqrt{\frac{f}{m}}t\right)\right) \\ &= (c_1+c_2)\cos\left(\sqrt{\frac{f}{m}}t\right) + (c_1-c_2)i\sin\left(\sqrt{\frac{f}{m}}t\right) \end{align} And here it gets funny: let $c_1+c_2 = A\cos(\phi)$ and $(c_1-c_2)i = A\sin(\phi)$ : \begin{align} z_{\rm hom.}(t) &= A\cos(\phi)\cos\left(\sqrt{\frac{f}{m}}t\right) + A\sin(\phi)\sin\left(\sqrt{\frac{f}{m}}t\right) \\ &= A\cos\left(\sqrt{\frac{f}{m}}t - \phi\right) \end{align} So, my question is: why can we say that $c_1+c_2 = A\cos(\phi)$ and $(c_1-c_2)i = A\sin(\phi)$ , with the same $A$ and $\phi$ ? It is awfully convenient, but how come $c_1$ and $c_2$ are related just this way? I'm guessing it has to do with the fact that the two eigenvalues $\lambda_{1,2}$ are complex conjugates, but exactly how escapes me. Any help would be appreciated!","A physical scenario in which a spring is hanging with a weight of mass on it yelds the following differential equation for the position : In order to solve for we first need to solve the homogenous equation: Assume : Having two eigenvalues, the general solution of the homogenous differential equation is a linear combination of the two: We're in the context of physics and this is an oscillation, so we want to transform this result using trigonometric functions to make it more explicit. According to the solution supplied to me this is how it goes: And here it gets funny: let and : So, my question is: why can we say that and , with the same and ? It is awfully convenient, but how come and are related just this way? I'm guessing it has to do with the fact that the two eigenvalues are complex conjugates, but exactly how escapes me. Any help would be appreciated!","m z(t) mz'' = mg - f(z - z_0) z mz'' + fz = 0 z(t) = e^{\lambda t} \lambda^2 m e^{\lambda t} + fe^{\lambda t} = 0 \lambda_{1,2} = \mp i\sqrt{\frac{f}{m}} z_{\rm hom.}(t) = c_1e^{-i\sqrt{\frac{f}{m}}t} + c_2e^{i\sqrt{\frac{f}{m}}t} \begin{align} z_{\rm hom.}(t) &= c_1e^{i\sqrt{\frac{f}{m}}t} + c_2e^{-i\sqrt{\frac{f}{m}}t} \\
&= c_1\left(\cos\left(\sqrt{\frac{f}{m}}t\right) + i\sin\left(\sqrt{\frac{f}{m}}t\right)\right) + c_2\left(\cos\left(\sqrt{\frac{f}{m}}t\right) - i\sin\left(\sqrt{\frac{f}{m}}t\right)\right) \\
&= (c_1+c_2)\cos\left(\sqrt{\frac{f}{m}}t\right) + (c_1-c_2)i\sin\left(\sqrt{\frac{f}{m}}t\right) \end{align} c_1+c_2 = A\cos(\phi) (c_1-c_2)i = A\sin(\phi) \begin{align} z_{\rm hom.}(t) &= A\cos(\phi)\cos\left(\sqrt{\frac{f}{m}}t\right) + A\sin(\phi)\sin\left(\sqrt{\frac{f}{m}}t\right) \\ &= A\cos\left(\sqrt{\frac{f}{m}}t - \phi\right) \end{align} c_1+c_2 = A\cos(\phi) (c_1-c_2)i = A\sin(\phi) A \phi c_1 c_2 \lambda_{1,2}","['ordinary-differential-equations', 'complex-numbers', 'eigenvalues-eigenvectors']"
19,Euler's method proof of the limit formula for $e$,Euler's method proof of the limit formula for,e,"While using Euler's method for $\frac{dy}{dx}=y$ given $y(0)=1$ I noticed that the approximate solution is $(1+1/s)^{sx}$ where s is the interval size. Plug in one for $x$ and you get: $$e^{1}=\lim_{s\to\infty}\,(1+1/s)^{s}$$ you get $e$. This has to be $e$ because that is that is the solution to the differential equation. Is this a new proof that $$e=\lim_{s\to\infty}\,(1+1/s)^{s}$$ List item","While using Euler's method for $\frac{dy}{dx}=y$ given $y(0)=1$ I noticed that the approximate solution is $(1+1/s)^{sx}$ where s is the interval size. Plug in one for $x$ and you get: $$e^{1}=\lim_{s\to\infty}\,(1+1/s)^{s}$$ you get $e$. This has to be $e$ because that is that is the solution to the differential equation. Is this a new proof that $$e=\lim_{s\to\infty}\,(1+1/s)^{s}$$ List item",,"['calculus', 'ordinary-differential-equations', 'limits']"
20,Determining stability of ODE,Determining stability of ODE,,"I'm working on a prey-predator model. I'm using the following system of differential equations for it: \begin{align} x'&=-a_1x+a_2xy+a_3xz\\ y'&=b_1y-b_2xy\\ z'&=c_1z-c_2xz \end{align} Where $a_i, b_i, c_i >0$. One of the stationary points is $P=(\frac{b_1}{b_2},\frac{a_1}{a_2},0)$. Question :  How can I determine the stability of this point $P$? Attempt : First I wrote the equation as: \begin{align} \frac{\mathrm d \underline{v}}{\mathrm d t}=\underline{F}(\underline{v}), \hspace{10pt} \underline{v}=(x,y,z)\end{align} I looked at the linearized ODE, and I found: \begin{align} \frac{\mathrm d \underline{\hat{v}}}{\mathrm t}= \begin{pmatrix}  0 & \dfrac{a_2b_1}{b_2} & \dfrac{a_3b_1}{b_2}\\ -\dfrac{a_1b_2}{a_2} & 0 & 0 \\ 0 & 0 &c_1-\dfrac{c_2b_1}{b_2}\\ \end{pmatrix} \underline{\hat{v}} \end{align} The eigenvalues are $c_1-\dfrac{c_2b_1}{b_2}, \pm i \sqrt[]{a_1b_1}$. My problem is the pair with zero real part. I have learned a theorem that only says something when all eigenvalues have negative real part (then it's stable) or at least one has positive eigenvalue (then it's unstable). My other approach was Lyapunov's theorem. I found (with a little bit puzzling) the following Lyapunov function: \begin{align}V(x,y,z) = a_2y+ b_2x  -a_1\left(1+\ln (y)-\ln\left(\frac{a_1}{a_2}\right)\right)-b_1\left(1+\ln(x)-\ln\left(\frac{b_1}{b_2}\right)\right)  \end{align} Differentiating it brought me eventually to this: \begin{align} V'=a_3b_2xz-a_3b_1z\end{align} Now the only thing that I can say is that $V'$ is both positive and negative in every neighborhood of $P$, so the theorem doesn't say something about that case. What I also thought about is to Taylor approximate the function $F$ till the second order term, like it can be done with one-dimensional ODE if the first derivative is equal to zero. But then here I get a matrix in a matrix (a tensor?) which is pretty vague. I can not visualise what is going on there, but if someone can explain how to do it that way, then you are welcome. I have read many articles about this problem on internet. I couldn't find something useful/understandable. Some are talking about manifolds, but I have not learned that yet. What I understand is that it is something like a solution curve. I really couldn't sleep well because of this problem. Your help is appreciated! Thanks in advance. Update I have realised that the function $V$ that I have used missed an imortant requirment, it was $0$ in all points $(\frac{b_1}{b_2},\frac{a_1}{a_2},z)$, so that was not a Lyapunov function. My new approach was to pick: \begin{align} V(x,y,z)=\left(x-\frac{b_1}{b_2}\right)^2+\left( y-\frac{a_1}{a_2}\right)^2+z^2\end{align} Okay, this one is Lyapunov for sure. I differentiate w.r.t. $t$ and get: \begin{align} V'=(-a_1x+a_2xy+a_3xz)\left(x-\frac{b_1}{b_2}\right) + (b_1y-b_2xy)\left( y-\frac{a_1}{a_2}\right) +z(c_1z-c_2xz) \end{align} This is zero in $(x,y,z)=P$ but that is also a saddle point, which means that is both negative and positive in every neighborhood of $P$. I also tried some Lyapunov functions of the form: \begin{align} V(x,y,z)=f(x-\frac{b_1}{b_2})+g(y-\frac{a_1}{a_2})+h(z)\end{align}  where $f,g,z$ are all even functions that has minimum in zero. Examples that I have tried are: $-\cos(x), -e^{-x^2}$ and they also made $V'$ have a saddle point in $P$. I also transformed everything to cilindar coordinates, but the equation that I have got was not so beautiful. I believe there is a way to prove the stability of this point. Can you guys help me? I really don't know what to do after this.","I'm working on a prey-predator model. I'm using the following system of differential equations for it: \begin{align} x'&=-a_1x+a_2xy+a_3xz\\ y'&=b_1y-b_2xy\\ z'&=c_1z-c_2xz \end{align} Where $a_i, b_i, c_i >0$. One of the stationary points is $P=(\frac{b_1}{b_2},\frac{a_1}{a_2},0)$. Question :  How can I determine the stability of this point $P$? Attempt : First I wrote the equation as: \begin{align} \frac{\mathrm d \underline{v}}{\mathrm d t}=\underline{F}(\underline{v}), \hspace{10pt} \underline{v}=(x,y,z)\end{align} I looked at the linearized ODE, and I found: \begin{align} \frac{\mathrm d \underline{\hat{v}}}{\mathrm t}= \begin{pmatrix}  0 & \dfrac{a_2b_1}{b_2} & \dfrac{a_3b_1}{b_2}\\ -\dfrac{a_1b_2}{a_2} & 0 & 0 \\ 0 & 0 &c_1-\dfrac{c_2b_1}{b_2}\\ \end{pmatrix} \underline{\hat{v}} \end{align} The eigenvalues are $c_1-\dfrac{c_2b_1}{b_2}, \pm i \sqrt[]{a_1b_1}$. My problem is the pair with zero real part. I have learned a theorem that only says something when all eigenvalues have negative real part (then it's stable) or at least one has positive eigenvalue (then it's unstable). My other approach was Lyapunov's theorem. I found (with a little bit puzzling) the following Lyapunov function: \begin{align}V(x,y,z) = a_2y+ b_2x  -a_1\left(1+\ln (y)-\ln\left(\frac{a_1}{a_2}\right)\right)-b_1\left(1+\ln(x)-\ln\left(\frac{b_1}{b_2}\right)\right)  \end{align} Differentiating it brought me eventually to this: \begin{align} V'=a_3b_2xz-a_3b_1z\end{align} Now the only thing that I can say is that $V'$ is both positive and negative in every neighborhood of $P$, so the theorem doesn't say something about that case. What I also thought about is to Taylor approximate the function $F$ till the second order term, like it can be done with one-dimensional ODE if the first derivative is equal to zero. But then here I get a matrix in a matrix (a tensor?) which is pretty vague. I can not visualise what is going on there, but if someone can explain how to do it that way, then you are welcome. I have read many articles about this problem on internet. I couldn't find something useful/understandable. Some are talking about manifolds, but I have not learned that yet. What I understand is that it is something like a solution curve. I really couldn't sleep well because of this problem. Your help is appreciated! Thanks in advance. Update I have realised that the function $V$ that I have used missed an imortant requirment, it was $0$ in all points $(\frac{b_1}{b_2},\frac{a_1}{a_2},z)$, so that was not a Lyapunov function. My new approach was to pick: \begin{align} V(x,y,z)=\left(x-\frac{b_1}{b_2}\right)^2+\left( y-\frac{a_1}{a_2}\right)^2+z^2\end{align} Okay, this one is Lyapunov for sure. I differentiate w.r.t. $t$ and get: \begin{align} V'=(-a_1x+a_2xy+a_3xz)\left(x-\frac{b_1}{b_2}\right) + (b_1y-b_2xy)\left( y-\frac{a_1}{a_2}\right) +z(c_1z-c_2xz) \end{align} This is zero in $(x,y,z)=P$ but that is also a saddle point, which means that is both negative and positive in every neighborhood of $P$. I also tried some Lyapunov functions of the form: \begin{align} V(x,y,z)=f(x-\frac{b_1}{b_2})+g(y-\frac{a_1}{a_2})+h(z)\end{align}  where $f,g,z$ are all even functions that has minimum in zero. Examples that I have tried are: $-\cos(x), -e^{-x^2}$ and they also made $V'$ have a saddle point in $P$. I also transformed everything to cilindar coordinates, but the equation that I have got was not so beautiful. I believe there is a way to prove the stability of this point. Can you guys help me? I really don't know what to do after this.",,"['ordinary-differential-equations', 'systems-of-equations', 'nonlinear-system', 'stability-in-odes']"
21,How to solve $\dot{\mathbf{v}} = - \frac{GM}{r^3}\mathbf{r}$ using differential eq-n solver in MATLAB,How to solve  using differential eq-n solver in MATLAB,\dot{\mathbf{v}} = - \frac{GM}{r^3}\mathbf{r},"I need to solve second order system with boundary conditions with help of differential equation solver. The system is $$\dot{\mathbf{v}} = - \frac{GM}{r^3}\mathbf{r}.$$ I tried to use MATLAB dsolve command like this: syms x(t) y(t)    eqns = [diff(x,t,2) == -1*x/sqrt(x*x+y*y)^3, diff(y,t,2) == -1*y/sqrt(x*x+y*y)^3];    Dx = diff(x,t); Dy = diff(y,t); cond = [x(0) == 10, y(0) == 0, Dx(0) == 0, Dy(0) == 1];    dsolve(eqns,cond) Warning: Explicit solution could not be found. EDIT: I think i need to use ode45 instead, something like this: f = @(t,y)[y(3); y(4); -y(1)/sqrt(y(1)*y(1)+y(2)*y(2))^3; -y(2)/sqrt(y(1)*y(1)+y(2)*y(2))^3]; [ts,ys]=ode45(f, [0, 100], [10; 0; 0; 0.1]); plot(ys(:,1), ys(:,2)); And it kinda works: Very good tutorial which helped me a lot: Using Matlab for Higher Order ODEs and Systems of ODEs","I need to solve second order system with boundary conditions with help of differential equation solver. The system is $$\dot{\mathbf{v}} = - \frac{GM}{r^3}\mathbf{r}.$$ I tried to use MATLAB dsolve command like this: syms x(t) y(t)    eqns = [diff(x,t,2) == -1*x/sqrt(x*x+y*y)^3, diff(y,t,2) == -1*y/sqrt(x*x+y*y)^3];    Dx = diff(x,t); Dy = diff(y,t); cond = [x(0) == 10, y(0) == 0, Dx(0) == 0, Dy(0) == 1];    dsolve(eqns,cond) Warning: Explicit solution could not be found. EDIT: I think i need to use ode45 instead, something like this: f = @(t,y)[y(3); y(4); -y(1)/sqrt(y(1)*y(1)+y(2)*y(2))^3; -y(2)/sqrt(y(1)*y(1)+y(2)*y(2))^3]; [ts,ys]=ode45(f, [0, 100], [10; 0; 0; 0.1]); plot(ys(:,1), ys(:,2)); And it kinda works: Very good tutorial which helped me a lot: Using Matlab for Higher Order ODEs and Systems of ODEs",,"['ordinary-differential-equations', 'systems-of-equations', 'matlab']"
22,Is a function still homogeneous if it factors out the absolute value of $t$ or a non-integer exponent of $t$?,Is a function still homogeneous if it factors out the absolute value of  or a non-integer exponent of ?,t t,"Most texts define a homogeneous function as one where $f(tx,ty)=t^kf(x,y)$ for some constant $k$. But I have always been confused about this definition for two reasons. Many texts provide an example such as $\sqrt{x^2+y^2}$ and say that it is homogeneous of degree 1. But $\sqrt{(tx)^2+(ty)^2}=\sqrt{t^2(x^2+y^2)}=|t|\sqrt{x^2+y^2}$, so does this mean a function is still homogeneous if it factors out the absolute value of $t$? Every definition I've seen doesn't specify that $k$ has to be a whole number, but I haven't seen any examples where $k=\frac{1}{2}$ for example. Can $k$ be any real number? I am wondering if the answer to the first question is simply that when solving a homogeneous differential equation, the sign of $t$ is irrelevant because it is taken care of by the constant in the general solution, or something similar. And for the second question, I am wondering if $k$ is implied to be a whole number, or if we don't see examples where $k=\frac{1}{2}$ simply because they are too hard to work with.","Most texts define a homogeneous function as one where $f(tx,ty)=t^kf(x,y)$ for some constant $k$. But I have always been confused about this definition for two reasons. Many texts provide an example such as $\sqrt{x^2+y^2}$ and say that it is homogeneous of degree 1. But $\sqrt{(tx)^2+(ty)^2}=\sqrt{t^2(x^2+y^2)}=|t|\sqrt{x^2+y^2}$, so does this mean a function is still homogeneous if it factors out the absolute value of $t$? Every definition I've seen doesn't specify that $k$ has to be a whole number, but I haven't seen any examples where $k=\frac{1}{2}$ for example. Can $k$ be any real number? I am wondering if the answer to the first question is simply that when solving a homogeneous differential equation, the sign of $t$ is irrelevant because it is taken care of by the constant in the general solution, or something similar. And for the second question, I am wondering if $k$ is implied to be a whole number, or if we don't see examples where $k=\frac{1}{2}$ simply because they are too hard to work with.",,"['ordinary-differential-equations', 'homogeneous-equation']"
23,First derivative equation with $\cosh^{-1}$ set equal to zero solving,First derivative equation with  set equal to zero solving,\cosh^{-1},This is the equation I'm left with when I took the derivative of a function. I would like to optimise so I'm trying to find a min/max by setting equal to zero. I have been having trouble solving. $$\frac{2\text{arcosh}(y)}{\sqrt{y^2-1}}+2y-4=0$$,This is the equation I'm left with when I took the derivative of a function. I would like to optimise so I'm trying to find a min/max by setting equal to zero. I have been having trouble solving. $$\frac{2\text{arcosh}(y)}{\sqrt{y^2-1}}+2y-4=0$$,,"['algebra-precalculus', 'ordinary-differential-equations', 'hyperbolic-functions']"
24,Solving an ODE from a PDE,Solving an ODE from a PDE,,"Solve this differential equation:   $$z^2\frac{dR}{R}+\frac{z\,dz}{1+z^2}=\frac{dS}{S}\tag1$$ I tried separating the variables but could not. Also tried to show that LHS is of the form of the differential of a product of functions but could not. Originally this is coming from a PDE  $$z\frac{\partial n}{\partial x}dx+\frac{n\,dz}{1+z^2}=\frac{1}{z}\cdot \frac{\partial n}{\partial y}dy\tag2$$ where $z=\frac{dy}{dx}$ and $n=n(x,y)$ and $y=y(x)$. The first equation can be obtained from $(2)$ by substituting $n=R(x)S(y)$. Please tell me how to solve the differential equations. Any suggestions for a general approach for solution of the PDE is also welcome. ADDENDUM: The equation $(2)$ has been obtained from solving the Euler Lagrange equation  for a ray of light travelling through a medium with refractive index $n(x,y)$. Now here, $L=L(y,y',x)=n(x,y(x))\sqrt{1+y'^2}$ where $y'=\frac{dy}{dx}$ So we solve $\frac{d}{dx}\left(\frac{\partial L}{\partial y'}\right)=\frac{\partial L}{\partial y}$ and accordingly get equation $(2)$.","Solve this differential equation:   $$z^2\frac{dR}{R}+\frac{z\,dz}{1+z^2}=\frac{dS}{S}\tag1$$ I tried separating the variables but could not. Also tried to show that LHS is of the form of the differential of a product of functions but could not. Originally this is coming from a PDE  $$z\frac{\partial n}{\partial x}dx+\frac{n\,dz}{1+z^2}=\frac{1}{z}\cdot \frac{\partial n}{\partial y}dy\tag2$$ where $z=\frac{dy}{dx}$ and $n=n(x,y)$ and $y=y(x)$. The first equation can be obtained from $(2)$ by substituting $n=R(x)S(y)$. Please tell me how to solve the differential equations. Any suggestions for a general approach for solution of the PDE is also welcome. ADDENDUM: The equation $(2)$ has been obtained from solving the Euler Lagrange equation  for a ray of light travelling through a medium with refractive index $n(x,y)$. Now here, $L=L(y,y',x)=n(x,y(x))\sqrt{1+y'^2}$ where $y'=\frac{dy}{dx}$ So we solve $\frac{d}{dx}\left(\frac{\partial L}{\partial y'}\right)=\frac{\partial L}{\partial y}$ and accordingly get equation $(2)$.",,"['calculus', 'integration', 'ordinary-differential-equations', 'partial-differential-equations']"
25,Second order nonlinear differential equation $y'' = C/y^{1/2}$,Second order nonlinear differential equation,y'' = C/y^{1/2},"$y'' = \frac{C}{\sqrt{y}}$ How can this differential equation be solved? I would love to add details about an attempt, but I have no idea where to even start.","$y'' = \frac{C}{\sqrt{y}}$ How can this differential equation be solved? I would love to add details about an attempt, but I have no idea where to even start.",,"['ordinary-differential-equations', 'nonlinear-system']"
26,How do they got the solution for this $2\times 2$ matrix?,How do they got the solution for this  matrix?,2\times 2,"For $y'=\frac{1}{x}\begin{pmatrix}0 &1 \\ 2 & -1\end{pmatrix}y$ the solutions are given by $y_1=\begin{pmatrix}x \\ x\end{pmatrix},y_2=\begin{pmatrix}x^{-2} \\ -2x^{-2}\end{pmatrix}$ with $x\in \Bbb R,y\in \Bbb R^2$ I would like to know how they got this solutions. I tried the following:$$det\frac{1}{x}\begin{pmatrix}0-\lambda &1 \\ 2 & -(1+\lambda)\end{pmatrix}=\frac{1}{x}(\lambda^2+\lambda-2)$$ $\Rightarrow \lambda_{1,2}=-\frac{1}{2}+/-\frac{3}{2}\Rightarrow\lambda_1=-2 , \lambda_2=1$ Than $$ker\begin{pmatrix}-1&1 \\ 2 & -2\end{pmatrix}\Rightarrow v_1=\begin{pmatrix}1 \\ 1\end{pmatrix}\Rightarrow y_1=e^x\begin{pmatrix}1 \\ 1\end{pmatrix}$$ also $$ker\begin{pmatrix}2&1 \\ 2 & 1\end{pmatrix}\Rightarrow v_1=\begin{pmatrix}1 \\ -2\end{pmatrix}\Rightarrow y_2=e^{-2x}\begin{pmatrix}1 \\ -2\end{pmatrix}$$ I guess this way is wrong Edit: $y_1'=\frac{1}{x}y_2 $ and $y_2'=\frac{1}{x}(2y_1-y_2)$","For $y'=\frac{1}{x}\begin{pmatrix}0 &1 \\ 2 & -1\end{pmatrix}y$ the solutions are given by $y_1=\begin{pmatrix}x \\ x\end{pmatrix},y_2=\begin{pmatrix}x^{-2} \\ -2x^{-2}\end{pmatrix}$ with $x\in \Bbb R,y\in \Bbb R^2$ I would like to know how they got this solutions. I tried the following:$$det\frac{1}{x}\begin{pmatrix}0-\lambda &1 \\ 2 & -(1+\lambda)\end{pmatrix}=\frac{1}{x}(\lambda^2+\lambda-2)$$ $\Rightarrow \lambda_{1,2}=-\frac{1}{2}+/-\frac{3}{2}\Rightarrow\lambda_1=-2 , \lambda_2=1$ Than $$ker\begin{pmatrix}-1&1 \\ 2 & -2\end{pmatrix}\Rightarrow v_1=\begin{pmatrix}1 \\ 1\end{pmatrix}\Rightarrow y_1=e^x\begin{pmatrix}1 \\ 1\end{pmatrix}$$ also $$ker\begin{pmatrix}2&1 \\ 2 & 1\end{pmatrix}\Rightarrow v_1=\begin{pmatrix}1 \\ -2\end{pmatrix}\Rightarrow y_2=e^{-2x}\begin{pmatrix}1 \\ -2\end{pmatrix}$$ I guess this way is wrong Edit: $y_1'=\frac{1}{x}y_2 $ and $y_2'=\frac{1}{x}(2y_1-y_2)$",,['ordinary-differential-equations']
27,Elementary solution to the ODE $u''+(x/x+h)u=0$,Elementary solution to the ODE,u''+(x/x+h)u=0,"I want to solve the following ordinary differential equation $$u''(x)+\frac{x}{x+h}u(x)=0,$$ where $h>0$ is a constant. I would like to know which method for solution is appropriate for the above equation and if the solution can be expressed by elementary functions? The best I could accomplish is a messy looking series representation of the solution. So I am looking for more clever ways to attack the equation. Best wishes","I want to solve the following ordinary differential equation $$u''(x)+\frac{x}{x+h}u(x)=0,$$ where $h>0$ is a constant. I would like to know which method for solution is appropriate for the above equation and if the solution can be expressed by elementary functions? The best I could accomplish is a messy looking series representation of the solution. So I am looking for more clever ways to attack the equation. Best wishes",,"['analysis', 'ordinary-differential-equations']"
28,Period of the nonlinear ODE $y''+\cos(y) = 0$,Period of the nonlinear ODE,y''+\cos(y) = 0,"Suppose an equation $$ y'' + \cos(y) = 0; y(0) = y'(0) = 0 $$ Numerical solution suggest periodic behavior. But how to derive it analytically (a systematic approximate is also acceptable)? Attempt 1 Write the equation as $y''= - \cos(y)$, together with initial condition, we have $y(0) = 0, y'(0) = 0, y''(0) = -cos(y(0)) = -1$ and so on. So the Taylor expansion approximation is $y = -1/2 x^2 + ...$, but this won't tell the qualitative periodic behavior, not to mention quantitative ones. Attempt 2 With the familiar trick $y''(x) = dy'/dy \cdot dy/dx = 1/2 dy'^2 /dx$, the equation can be reduced to first order (initial condition used): $y'^2 + 2 sin(y) = 0$. But I don't know how to proceed with this either. Numerical solution is presented in figure below, the blue curve (get out of plot range around x > 3) is the Taylor series approximation $-1/2x^2$, the yellow and green curves overlap a lot, one is the numerical solution, another is an eyeball fitting with this function ($T\sim 7.4$): $$ \frac{\pi}{2} (\cos(2\pi x/T )-1) $$","Suppose an equation $$ y'' + \cos(y) = 0; y(0) = y'(0) = 0 $$ Numerical solution suggest periodic behavior. But how to derive it analytically (a systematic approximate is also acceptable)? Attempt 1 Write the equation as $y''= - \cos(y)$, together with initial condition, we have $y(0) = 0, y'(0) = 0, y''(0) = -cos(y(0)) = -1$ and so on. So the Taylor expansion approximation is $y = -1/2 x^2 + ...$, but this won't tell the qualitative periodic behavior, not to mention quantitative ones. Attempt 2 With the familiar trick $y''(x) = dy'/dy \cdot dy/dx = 1/2 dy'^2 /dx$, the equation can be reduced to first order (initial condition used): $y'^2 + 2 sin(y) = 0$. But I don't know how to proceed with this either. Numerical solution is presented in figure below, the blue curve (get out of plot range around x > 3) is the Taylor series approximation $-1/2x^2$, the yellow and green curves overlap a lot, one is the numerical solution, another is an eyeball fitting with this function ($T\sim 7.4$): $$ \frac{\pi}{2} (\cos(2\pi x/T )-1) $$",,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system']"
29,Is a differential equation with a periodic solution considered to have multiple solutions?,Is a differential equation with a periodic solution considered to have multiple solutions?,,"This is for homework, but it's not the question that the homework is asking. If we have a differential equation with some periodic solution, for example $(y')^2 + y^2 = 1$ can be solved by $y = \sin t$, then do we consider the solutions of the form $y = \sin(t + 2\pi n)\ \forall n \in \mathbb{Z}$ to be distinct solutions for the initial value $y(0) = 0$? In general, are shifted periodic solutions different solutions, or are they the same solution written in a different way?","This is for homework, but it's not the question that the homework is asking. If we have a differential equation with some periodic solution, for example $(y')^2 + y^2 = 1$ can be solved by $y = \sin t$, then do we consider the solutions of the form $y = \sin(t + 2\pi n)\ \forall n \in \mathbb{Z}$ to be distinct solutions for the initial value $y(0) = 0$? In general, are shifted periodic solutions different solutions, or are they the same solution written in a different way?",,"['ordinary-differential-equations', 'initial-value-problems']"
30,A question regarding existence and uniqueness in IVP,A question regarding existence and uniqueness in IVP,,"Consider the IVP $$y'(t)=f(y(t)), \ \ \ \ y(0)=a \in \mathbb{R}$$ $$f : \mathbb{R} \rightarrow \mathbb{R}$$ Which of the following is/are true $(A)$ There exists a continuous function $f : \mathbb{R} \rightarrow \mathbb{R}$ and $a \in \mathbb{R}$ such that the above problem does not have a solution in any nbd of $0$ . $(B)$ The problem has unique solution for every $a \in \mathbb{R}$ when $f$ is Lipschitz continuous $(C)$ When $f$ is twice continuously differentiable the maximal interval of existence for the above IVP is $\mathbb{R}$ $(D)$ The maximum interval of existence  for the IVP is $\mathbb{R}$ when $f$ is bounded and continuously differentiable. It is obvious to me that $A \ \& \ B$ are false from traditional existance and uniqueness theorems (viz Picards Theorem). I am not sure about $C \ \& \ D$ and this is really bugging me. Please could anyone shed light on this. PS: Multiple correct options are allowed.",Consider the IVP Which of the following is/are true There exists a continuous function and such that the above problem does not have a solution in any nbd of . The problem has unique solution for every when is Lipschitz continuous When is twice continuously differentiable the maximal interval of existence for the above IVP is The maximum interval of existence  for the IVP is when is bounded and continuously differentiable. It is obvious to me that are false from traditional existance and uniqueness theorems (viz Picards Theorem). I am not sure about and this is really bugging me. Please could anyone shed light on this. PS: Multiple correct options are allowed.,"y'(t)=f(y(t)), \ \ \ \ y(0)=a \in \mathbb{R} f : \mathbb{R} \rightarrow \mathbb{R} (A) f : \mathbb{R} \rightarrow \mathbb{R} a \in \mathbb{R} 0 (B) a \in \mathbb{R} f (C) f \mathbb{R} (D) \mathbb{R} f A \ \& \ B C \ \& \ D","['ordinary-differential-equations', 'initial-value-problems']"
31,Differential Operator McLaurin Series,Differential Operator McLaurin Series,,The Non-Homogeneous Differential Equation $$Ly=R$$ Where  $$ L=F(D)= \sum_i a_iD^i $$ Is solved with the way below. $$ Ly=R \Rightarrow y=\frac{R}{L}=\frac{R}{F(D)}$$ Then we expand the qiotient $ \frac{1}{F(D)}$ into Taylor Series in the neighborhood of 0 (McLaurin). In order to expand F(D) we have to give $D=\frac{d}{dx} $ a value. How can we give a differential operator a value?,The Non-Homogeneous Differential Equation $$Ly=R$$ Where  $$ L=F(D)= \sum_i a_iD^i $$ Is solved with the way below. $$ Ly=R \Rightarrow y=\frac{R}{L}=\frac{R}{F(D)}$$ Then we expand the qiotient $ \frac{1}{F(D)}$ into Taylor Series in the neighborhood of 0 (McLaurin). In order to expand F(D) we have to give $D=\frac{d}{dx} $ a value. How can we give a differential operator a value?,,"['calculus', 'ordinary-differential-equations', 'taylor-expansion', 'fractional-calculus']"
32,Functions with the property $\frac{d}{dx}(f(x)\cdot f(x))=f(2x)$,Functions with the property,\frac{d}{dx}(f(x)\cdot f(x))=f(2x),"I've noticed the derivative of $\sin^2 x$, $$\frac{d}{dx}\sin^2(x)=\sin 2x$$ and the same applies to $\sinh^2 x$, that is  $$\frac{d}{dx}\sinh^2(x)=\sinh(2x)$$ I was wondering about the other functions of that property. (I posted a similar question about a week ago but I had an incorrect premise.) So these functions must satisfy $$(f^2(x))'=f(2x)\rightarrow 2f'(x)f(x)=f(2x)$$ The only way I could think of solving these was the power series method which is something I've never been good with. $$f(x)=\sum_{n=0}^{\infty} C_nx^n,$$ $$f'(x)=\sum_{n=0}^{\infty}(n+1)C_{n+1}x^n$$ plugging this $$2\sum_{n=0}^{\infty}(n+1)C_nC_{n+1}x^{2n}=\sum_{n=0}^{\infty}C_n(2x)^n.$$ and now I don't know what to do with that $x^{2n}$ term. Could someone help me find the general form of $f(x)$?","I've noticed the derivative of $\sin^2 x$, $$\frac{d}{dx}\sin^2(x)=\sin 2x$$ and the same applies to $\sinh^2 x$, that is  $$\frac{d}{dx}\sinh^2(x)=\sinh(2x)$$ I was wondering about the other functions of that property. (I posted a similar question about a week ago but I had an incorrect premise.) So these functions must satisfy $$(f^2(x))'=f(2x)\rightarrow 2f'(x)f(x)=f(2x)$$ The only way I could think of solving these was the power series method which is something I've never been good with. $$f(x)=\sum_{n=0}^{\infty} C_nx^n,$$ $$f'(x)=\sum_{n=0}^{\infty}(n+1)C_{n+1}x^n$$ plugging this $$2\sum_{n=0}^{\infty}(n+1)C_nC_{n+1}x^{2n}=\sum_{n=0}^{\infty}C_n(2x)^n.$$ and now I don't know what to do with that $x^{2n}$ term. Could someone help me find the general form of $f(x)$?",,"['calculus', 'ordinary-differential-equations', 'trigonometric-series']"
33,Phase plane for guerrilla vs. conventional warfare [closed],Phase plane for guerrilla vs. conventional warfare [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question For a class, I'm modeling a battle between a conventional force, $C(t)$, and a guerrilla force, $G(t)$. I already know that the time derivative of $C(t)=-gG$, where $g$ is the effectiveness of the guerrilla army, and the time derivative of $G(t)=-cCG$. Then I take the ratio to get $dC/dG = g/cC$. Separating these and integrating, I have $gG = 0.5cC^2 +K$. I'm not sure what $K$ is supposed to be, but I include it since I integrated. Now I must plot the phase plane, and I'm quite lost on how to do this. Does it have to do with plotting the slope field of $dC/dG=1/C$? I also must derive a condition that will tell who wins and loses.","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question For a class, I'm modeling a battle between a conventional force, $C(t)$, and a guerrilla force, $G(t)$. I already know that the time derivative of $C(t)=-gG$, where $g$ is the effectiveness of the guerrilla army, and the time derivative of $G(t)=-cCG$. Then I take the ratio to get $dC/dG = g/cC$. Separating these and integrating, I have $gG = 0.5cC^2 +K$. I'm not sure what $K$ is supposed to be, but I include it since I integrated. Now I must plot the phase plane, and I'm quite lost on how to do this. Does it have to do with plotting the slope field of $dC/dG=1/C$? I also must derive a condition that will tell who wins and loses.",,"['ordinary-differential-equations', 'mathematical-modeling']"
34,First order nonlinear differential equation,First order nonlinear differential equation,,I have problem determining the type of this first order differential equation can someone help me how to start solving it? The equation is: $$2x^2y\ln(y)y'=y+xy'.$$ Thank you!,I have problem determining the type of this first order differential equation can someone help me how to start solving it? The equation is: $$2x^2y\ln(y)y'=y+xy'.$$ Thank you!,,['ordinary-differential-equations']
35,Linearization of nonlinear system and the behavior of the linearized system,Linearization of nonlinear system and the behavior of the linearized system,,Consider the nonlinear system: \begin{cases} x' = x^2 + y \\\\ y' = x - y + a \\\\ \end{cases} where $ a $ is a parameter. $ a) $ Find all equilibrium points and compute the linearized equation at each. For this question I solve \begin{cases} x^2 + y = 0 \\\\ x - y + a = 0 \\\\ \end{cases} to give me $ \displaystyle  x = \frac{-1 \pm \sqrt{1 - 4a}}{2} $ and $ \displaystyle  y = \frac{-1 \pm \sqrt{1 - 4a}}{2} + a $ as equilibrium points. The linearized system I got is \begin{cases} x' = y \\\\ y' = x - y \\\\ \end{cases} $ b) $ Describe the behavior of the linearized system at each equilibrium point? Can someone help me with this one? The linearized system doesn't depend on $ a $ and so why does it ask for the behavior at each equilibrium point?,Consider the nonlinear system: \begin{cases} x' = x^2 + y \\\\ y' = x - y + a \\\\ \end{cases} where $ a $ is a parameter. $ a) $ Find all equilibrium points and compute the linearized equation at each. For this question I solve \begin{cases} x^2 + y = 0 \\\\ x - y + a = 0 \\\\ \end{cases} to give me $ \displaystyle  x = \frac{-1 \pm \sqrt{1 - 4a}}{2} $ and $ \displaystyle  y = \frac{-1 \pm \sqrt{1 - 4a}}{2} + a $ as equilibrium points. The linearized system I got is \begin{cases} x' = y \\\\ y' = x - y \\\\ \end{cases} $ b) $ Describe the behavior of the linearized system at each equilibrium point? Can someone help me with this one? The linearized system doesn't depend on $ a $ and so why does it ask for the behavior at each equilibrium point?,,"['ordinary-differential-equations', 'nonlinear-system']"
36,Convolution and Laplace for ODE,Convolution and Laplace for ODE,,"I've got the equation which I am supposed to use laplace transforms to solve: $$ y'' + y = e^t + tsin(t), ~~~~~y(0) = 3/2,~~~~~y'(0) = 5/2 $$ Using laplace transforms, I come up with the equation: $$ Y(s) = {1 \over (s-1)(s^2+1)} + {2s \over (s^2+1)^3} + {3s+5 \over 2(s^2+1)}$$ I can use partial fraction and inverse laplace to solve most if it: $$ y(t) = \frac{1}{2}e^t +cos(t) + 2sin(t) + L^{-1} \left[\frac{2s}{(s^2+1)^3} \right]$$ I've been trying to use convolution: $$ L^{-1} \left[ \frac{s}{(s^2+1)^2} \right] = \frac{t}{2}sin(t) ~~~~~L^{-1} \left[ \frac{2}{(s^2+1)} \right] = 2sin(t) $$ $$ \left(2sin(t) * \frac{t}{2} sin(t)\right) = \int_0^t \frac{\tau}{2}sin(\tau) \cdot 2sin(t-\tau) d\tau$$ I just can't seem to get the two terms I'm missing. Is there a better way to approach the laplace or convolution? Or do I just need to make sure I'm doing the integral correctly? Symbolab answer to check for correctness: $$y=-\frac{1}{4}t^2\cos \left(t\right)+2\sin \left(t\right)+\frac{e^t}{2}+\frac{1}{4}t\sin \left(t\right)+\cos \left(t\right)$$","I've got the equation which I am supposed to use laplace transforms to solve: $$ y'' + y = e^t + tsin(t), ~~~~~y(0) = 3/2,~~~~~y'(0) = 5/2 $$ Using laplace transforms, I come up with the equation: $$ Y(s) = {1 \over (s-1)(s^2+1)} + {2s \over (s^2+1)^3} + {3s+5 \over 2(s^2+1)}$$ I can use partial fraction and inverse laplace to solve most if it: $$ y(t) = \frac{1}{2}e^t +cos(t) + 2sin(t) + L^{-1} \left[\frac{2s}{(s^2+1)^3} \right]$$ I've been trying to use convolution: $$ L^{-1} \left[ \frac{s}{(s^2+1)^2} \right] = \frac{t}{2}sin(t) ~~~~~L^{-1} \left[ \frac{2}{(s^2+1)} \right] = 2sin(t) $$ $$ \left(2sin(t) * \frac{t}{2} sin(t)\right) = \int_0^t \frac{\tau}{2}sin(\tau) \cdot 2sin(t-\tau) d\tau$$ I just can't seem to get the two terms I'm missing. Is there a better way to approach the laplace or convolution? Or do I just need to make sure I'm doing the integral correctly? Symbolab answer to check for correctness: $$y=-\frac{1}{4}t^2\cos \left(t\right)+2\sin \left(t\right)+\frac{e^t}{2}+\frac{1}{4}t\sin \left(t\right)+\cos \left(t\right)$$",,"['ordinary-differential-equations', 'laplace-transform', 'convolution']"
37,Adaptive Adam-Bashforth method,Adaptive Adam-Bashforth method,,"Let's say we have 3 time points that are not equally spaced: $t_{i-1}, t_i$ and $t_{i+1}$. Here, $t_i - t_{i-1} = h_{prev}, \space \space t_{i+1} - t_i = h$ I need to derive an Adam-Bashforth scheme, with a basic integral relation: $y(t_{i+1}) = y(t_i) + \int_{t_i}^{t_{i+1}}f(t,y)dt$ Note that $f(t,y)$ is simply a linear interpolant based on $t_{i-1}$ and $t_i$. OK, so I know that whatever I come up with needs to look like the following: $y_{{i+1}} = y_{{i}} + \alpha f_i + \beta f_{i-1}$ and I understand that the constants $\alpha$ and $\beta$ should depend on $h_{prev}$ and $h$. So this question is basically asking me to find these constants. Normally, what I would do is set up the interpolant for each constant and then integrate from -1 to 1. So to solve for $\alpha$ using the lagrange interpolant, for example: $L_i = \frac{(t-t_{i-1})}{(t_i-t_{i-1})}$ $L_i(t_i + hs) = \frac{(t_i + hs - (t_i - h))}{h_{prev}}$ Did I set up the interpolant right for $\alpha$? Because if I integrate this with respect to $s$ from -1 to 1 I find $\alpha = 2h/h_{prev}$. Does that seem right? I'd appreciate any help.","Let's say we have 3 time points that are not equally spaced: $t_{i-1}, t_i$ and $t_{i+1}$. Here, $t_i - t_{i-1} = h_{prev}, \space \space t_{i+1} - t_i = h$ I need to derive an Adam-Bashforth scheme, with a basic integral relation: $y(t_{i+1}) = y(t_i) + \int_{t_i}^{t_{i+1}}f(t,y)dt$ Note that $f(t,y)$ is simply a linear interpolant based on $t_{i-1}$ and $t_i$. OK, so I know that whatever I come up with needs to look like the following: $y_{{i+1}} = y_{{i}} + \alpha f_i + \beta f_{i-1}$ and I understand that the constants $\alpha$ and $\beta$ should depend on $h_{prev}$ and $h$. So this question is basically asking me to find these constants. Normally, what I would do is set up the interpolant for each constant and then integrate from -1 to 1. So to solve for $\alpha$ using the lagrange interpolant, for example: $L_i = \frac{(t-t_{i-1})}{(t_i-t_{i-1})}$ $L_i(t_i + hs) = \frac{(t_i + hs - (t_i - h))}{h_{prev}}$ Did I set up the interpolant right for $\alpha$? Because if I integrate this with respect to $s$ from -1 to 1 I find $\alpha = 2h/h_{prev}$. Does that seem right? I'd appreciate any help.",,"['integration', 'analysis', 'ordinary-differential-equations', 'numerical-methods', 'lagrange-interpolation']"
38,how to do this ODE transformation?,how to do this ODE transformation?,,"Could someone please show me how the book did the following transformation? I am not able to reproduce the result shown. I show one attempt, but I tried few others. Here is screen shot of the part of the book page showing the transformation used The book link at google books is this and the page is page number 3 in the introduction: \begin{equation} \frac{d^{2}y}{dx^{2}}\left(  y+x\right)  +\frac{dy}{dx}\left(  \frac{dy} {dx}-1\right)  =0 \tag{1} \end{equation} Since $y=u-v\left(  u\right)  $ then, \begin{equation} \frac{dy}{dx}=\frac{du}{dx}-\frac{dv}{du}\frac{du}{dx} \tag{2} \end{equation} And% \begin{align} \frac{d^{2}y}{dx^{2}} &  =\frac{d^{2}u}{dx^{2}}-\frac{d}{dx}\left(  \frac {dv}{du}\frac{du}{dx}\right)  \nonumber\\ &  =\frac{d^{2}u}{dx^{2}}-\left(  \frac{d^{2}v}{du^{2}}\left(  \frac{du} {dx}\right)  ^{2}+\frac{dv}{du}\frac{d^{2}u}{dx^{2}}\right)  \tag{3} \end{align} Now Let $\frac{dv}{du}=v^{\prime},\frac{d^{2}v}{du^{2}}=v^{\prime\prime}$, then (2) and (3) becomes \begin{align*} \frac{dy}{dx} &  =\frac{du}{dx}-v^{\prime}\frac{du}{dx}\\ \frac{d^{2}y}{dx^{2}} &  =\frac{d^{2}u}{dx^{2}}-\left(  v^{\prime\prime }\left(  \frac{du}{dx}\right)  ^{2}+v^{\prime}\frac{d^{2}u}{dx^{2}}\right) \end{align*} Substituting the above two equations back into (1), and using the given $x=u+v,y=u-v$ results in \begin{align*} \left[  \frac{d^{2}u}{dx^{2}}-\left(  v^{\prime\prime}\left(  \frac{du} {dx}\right)  ^{2}+v^{\prime}\frac{d^{2}u}{dx^{2}}\right)  \right]  \left( u-v+u+v\right)  +\left(  \frac{du}{dx}-v^{\prime}\frac{du}{dx}\right)  \left( \frac{du}{dx}-v^{\prime}\frac{du}{dx}-1\right)   &  =0\\ \left[  \frac{d^{2}u}{dx^{2}}-\left(  v^{\prime\prime}\left(  \frac{du} {dx}\right)  ^{2}+v^{\prime}\frac{d^{2}u}{dx^{2}}\right)  \right]  2u+\left( \frac{du}{dx}\right)  ^{2}-v^{\prime}\left(  \frac{du}{dx}\right)  ^{2} -\frac{du}{dx}-v^{\prime}\left(  \frac{du}{dx}\right)  ^{2}+v^{\prime2}\left( \frac{du}{dx}\right)  ^{2}+v^{\prime}\frac{du}{dx} &  =0\\ 2u\frac{d^{2}u}{dx^{2}}-2uv^{\prime\prime}\left(  \frac{du}{dx}\right) ^{2}-2uv^{\prime}\frac{d^{2}u}{dx^{2}}+\left(  \frac{du}{dx}\right) ^{2}-v^{\prime}\left(  \frac{du}{dx}\right)  ^{2}-\frac{du}{dx}-v^{\prime }\left(  \frac{du}{dx}\right)  ^{2}+v^{\prime2}\left(  \frac{du}{dx}\right) ^{2}+v^{\prime}\frac{du}{dx} &  =0 \end{align*} I do not know how to make the above same as book result.","Could someone please show me how the book did the following transformation? I am not able to reproduce the result shown. I show one attempt, but I tried few others. Here is screen shot of the part of the book page showing the transformation used The book link at google books is this and the page is page number 3 in the introduction: \begin{equation} \frac{d^{2}y}{dx^{2}}\left(  y+x\right)  +\frac{dy}{dx}\left(  \frac{dy} {dx}-1\right)  =0 \tag{1} \end{equation} Since $y=u-v\left(  u\right)  $ then, \begin{equation} \frac{dy}{dx}=\frac{du}{dx}-\frac{dv}{du}\frac{du}{dx} \tag{2} \end{equation} And% \begin{align} \frac{d^{2}y}{dx^{2}} &  =\frac{d^{2}u}{dx^{2}}-\frac{d}{dx}\left(  \frac {dv}{du}\frac{du}{dx}\right)  \nonumber\\ &  =\frac{d^{2}u}{dx^{2}}-\left(  \frac{d^{2}v}{du^{2}}\left(  \frac{du} {dx}\right)  ^{2}+\frac{dv}{du}\frac{d^{2}u}{dx^{2}}\right)  \tag{3} \end{align} Now Let $\frac{dv}{du}=v^{\prime},\frac{d^{2}v}{du^{2}}=v^{\prime\prime}$, then (2) and (3) becomes \begin{align*} \frac{dy}{dx} &  =\frac{du}{dx}-v^{\prime}\frac{du}{dx}\\ \frac{d^{2}y}{dx^{2}} &  =\frac{d^{2}u}{dx^{2}}-\left(  v^{\prime\prime }\left(  \frac{du}{dx}\right)  ^{2}+v^{\prime}\frac{d^{2}u}{dx^{2}}\right) \end{align*} Substituting the above two equations back into (1), and using the given $x=u+v,y=u-v$ results in \begin{align*} \left[  \frac{d^{2}u}{dx^{2}}-\left(  v^{\prime\prime}\left(  \frac{du} {dx}\right)  ^{2}+v^{\prime}\frac{d^{2}u}{dx^{2}}\right)  \right]  \left( u-v+u+v\right)  +\left(  \frac{du}{dx}-v^{\prime}\frac{du}{dx}\right)  \left( \frac{du}{dx}-v^{\prime}\frac{du}{dx}-1\right)   &  =0\\ \left[  \frac{d^{2}u}{dx^{2}}-\left(  v^{\prime\prime}\left(  \frac{du} {dx}\right)  ^{2}+v^{\prime}\frac{d^{2}u}{dx^{2}}\right)  \right]  2u+\left( \frac{du}{dx}\right)  ^{2}-v^{\prime}\left(  \frac{du}{dx}\right)  ^{2} -\frac{du}{dx}-v^{\prime}\left(  \frac{du}{dx}\right)  ^{2}+v^{\prime2}\left( \frac{du}{dx}\right)  ^{2}+v^{\prime}\frac{du}{dx} &  =0\\ 2u\frac{d^{2}u}{dx^{2}}-2uv^{\prime\prime}\left(  \frac{du}{dx}\right) ^{2}-2uv^{\prime}\frac{d^{2}u}{dx^{2}}+\left(  \frac{du}{dx}\right) ^{2}-v^{\prime}\left(  \frac{du}{dx}\right)  ^{2}-\frac{du}{dx}-v^{\prime }\left(  \frac{du}{dx}\right)  ^{2}+v^{\prime2}\left(  \frac{du}{dx}\right) ^{2}+v^{\prime}\frac{du}{dx} &  =0 \end{align*} I do not know how to make the above same as book result.",,"['ordinary-differential-equations', 'substitution']"
39,"Investigate the stability of the origin for $\dot x=-y^2$, $ \dot y = -y + x^2 + xy$","Investigate the stability of the origin for ,",\dot x=-y^2  \dot y = -y + x^2 + xy,"I need to determine the stability of the origin of the following system of differential equations $$\pmatrix{\dot x \\ \dot y} = \pmatrix{-y^2 \\ -y + x^2 + xy},$$ using the Local Center Manifold Theorem as defined below. Local Center Manifold Theorem : Let ${\bf f} \in C^r(E)$, where $E$ is an open subset of $\mathbb R^n$ containing the origin and $r \ge 1$. Suppose that ${\bf f}({\bf 0}) = {\bf 0}$ and that $D{\bf f}({\bf 0})$ has $c$ eigenvalues with zero real parts and $s$ eigenvalues with negative real parts, where $c+s = n$. The system $\dot {\bf x} = {\bf f}({\bf x})$ then can be written in the diagonal form $$\begin{cases}\dot{\bf x} &= C{\bf x} + {\bf F}({\bf x}, {\bf y}) \\ \dot{\bf y} & = P{\bf y} + {\bf G}({\bf x}, {\bf y}),\end{cases}$$ where $({\bf x}, {\bf y}) \in \mathbb R^c \times \mathbb R^s$, $C$ is a square matrix with $c$ eigenvalues having zero real parts, $P$ is a square matrix with $s$ eigenvalues with negative real parts, and ${\bf F}({\bf 0}) = {\bf G}({\bf 0}) = {\bf 0}, D{\bf F}({\bf 0}) = D{\bf G}({\bf 0}) = O$; furthermore, there exists $\delta > 0$ and a function ${\bf h} \in C^r(N_\delta({\bf 0}))$ that defines the local center manifold $$W^c_{\text{loc}}({\bf 0}) = \{({\bf x}, {\bf y}) \in \mathbb R^c \times \mathbb R^s : {\bf y} = {\bf h}({\bf x}) \text{ for } |{\bf x}| < \delta\}$$ and satisfies $$D{\bf h}({\bf x})[C{\bf x} + {\bf F}({\bf x}, {\bf h}({\bf x}))] - P{\bf h}({\bf x}) - {\bf G}({\bf x}, {\bf h}({\bf x})) = 0$$ for $|{\bf x}| < \delta$; and the flow on the center manifold $W^c({\bf 0})$ is defined by the system of differential equations $$\dot {\bf x} = C{\bf x} + {\bf F}({\bf x}, {\bf h}({\bf x}))$$ for all ${\bf x} \in \mathbb R^c$ with $|{\bf x}| < \delta$. I have begun to attempt to solve the problem, namely by letting $f({\bf x}) = \pmatrix{-y^2 \\ -y + x^2 + xy}.$ Then, $$D{\bf f}({\bf x}) = \pmatrix{0 & -2y\\2x + y & x - 1} \implies D{\bf f}({\bf 0}) = \pmatrix{0 & -2\\0 & -1},$$ which has eigenvalues of $0$ and $-1$. Thus, following the above theorem, $C = O$ and $P = [-1]$, as well as $F(x,y) = -y^2$ and $G(x,y) = x^2 + xy$ (just take the nonlinear part of the original systems). Then the system is now in diagonal form. Here's where I am unfortunately stuck . I'm trying to follow examples in my textbook like the one found here on the first page . Almost magically, they come up with the next part which is the function $h$ with no explanation of how they come up with the terms that they do. I understand that we omit the linear terms $1$ and $x$ because this would mean $(x,h(x))$ is tangent to the origin, as I have discovered here . But how do I know what terms to include or even when to stop? I've been looking at some examples and sometimes they have an $xy$ term in there in addition to the powers of $x$, especially when the dimension of the system gets higher. The reason being is that the next step is to plug $h(x) = ax^2 + bx^3 + \cdots$ into the equation we have above, i.e., $D{\bf h}({\bf x})[C{\bf x} + {\bf F}({\bf x}, {\bf h}({\bf x}))] - P{\bf h}({\bf x}) - {\bf G}({\bf x}, {\bf h}({\bf x})) = 0$ to find the unknown coefficients. Short of just guessing can anyone provide some insight on how we come up with the form of $h$? (short of just guessing).","I need to determine the stability of the origin of the following system of differential equations $$\pmatrix{\dot x \\ \dot y} = \pmatrix{-y^2 \\ -y + x^2 + xy},$$ using the Local Center Manifold Theorem as defined below. Local Center Manifold Theorem : Let ${\bf f} \in C^r(E)$, where $E$ is an open subset of $\mathbb R^n$ containing the origin and $r \ge 1$. Suppose that ${\bf f}({\bf 0}) = {\bf 0}$ and that $D{\bf f}({\bf 0})$ has $c$ eigenvalues with zero real parts and $s$ eigenvalues with negative real parts, where $c+s = n$. The system $\dot {\bf x} = {\bf f}({\bf x})$ then can be written in the diagonal form $$\begin{cases}\dot{\bf x} &= C{\bf x} + {\bf F}({\bf x}, {\bf y}) \\ \dot{\bf y} & = P{\bf y} + {\bf G}({\bf x}, {\bf y}),\end{cases}$$ where $({\bf x}, {\bf y}) \in \mathbb R^c \times \mathbb R^s$, $C$ is a square matrix with $c$ eigenvalues having zero real parts, $P$ is a square matrix with $s$ eigenvalues with negative real parts, and ${\bf F}({\bf 0}) = {\bf G}({\bf 0}) = {\bf 0}, D{\bf F}({\bf 0}) = D{\bf G}({\bf 0}) = O$; furthermore, there exists $\delta > 0$ and a function ${\bf h} \in C^r(N_\delta({\bf 0}))$ that defines the local center manifold $$W^c_{\text{loc}}({\bf 0}) = \{({\bf x}, {\bf y}) \in \mathbb R^c \times \mathbb R^s : {\bf y} = {\bf h}({\bf x}) \text{ for } |{\bf x}| < \delta\}$$ and satisfies $$D{\bf h}({\bf x})[C{\bf x} + {\bf F}({\bf x}, {\bf h}({\bf x}))] - P{\bf h}({\bf x}) - {\bf G}({\bf x}, {\bf h}({\bf x})) = 0$$ for $|{\bf x}| < \delta$; and the flow on the center manifold $W^c({\bf 0})$ is defined by the system of differential equations $$\dot {\bf x} = C{\bf x} + {\bf F}({\bf x}, {\bf h}({\bf x}))$$ for all ${\bf x} \in \mathbb R^c$ with $|{\bf x}| < \delta$. I have begun to attempt to solve the problem, namely by letting $f({\bf x}) = \pmatrix{-y^2 \\ -y + x^2 + xy}.$ Then, $$D{\bf f}({\bf x}) = \pmatrix{0 & -2y\\2x + y & x - 1} \implies D{\bf f}({\bf 0}) = \pmatrix{0 & -2\\0 & -1},$$ which has eigenvalues of $0$ and $-1$. Thus, following the above theorem, $C = O$ and $P = [-1]$, as well as $F(x,y) = -y^2$ and $G(x,y) = x^2 + xy$ (just take the nonlinear part of the original systems). Then the system is now in diagonal form. Here's where I am unfortunately stuck . I'm trying to follow examples in my textbook like the one found here on the first page . Almost magically, they come up with the next part which is the function $h$ with no explanation of how they come up with the terms that they do. I understand that we omit the linear terms $1$ and $x$ because this would mean $(x,h(x))$ is tangent to the origin, as I have discovered here . But how do I know what terms to include or even when to stop? I've been looking at some examples and sometimes they have an $xy$ term in there in addition to the powers of $x$, especially when the dimension of the system gets higher. The reason being is that the next step is to plug $h(x) = ax^2 + bx^3 + \cdots$ into the equation we have above, i.e., $D{\bf h}({\bf x})[C{\bf x} + {\bf F}({\bf x}, {\bf h}({\bf x}))] - P{\bf h}({\bf x}) - {\bf G}({\bf x}, {\bf h}({\bf x})) = 0$ to find the unknown coefficients. Short of just guessing can anyone provide some insight on how we come up with the form of $h$? (short of just guessing).",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
40,"With regards to PDEs, what is the difference between an initial condition and a boundary condition?","With regards to PDEs, what is the difference between an initial condition and a boundary condition?",,"With regards to PDEs, what is the difference between an initial condition and a boundary condition? I am given the following examples. An example of an initial condition is $u(x,0) = \sin(x)$ $\forall$ $x$. An example of a boundary condition is $u(0,t) = e^{-t}$ $\forall$ $t > 0$. However, these examples are not telling me anything general about the difference between an initial condition and a boundary condition for PDEs. I would like someone to explain this difference in a general way. Thank you.","With regards to PDEs, what is the difference between an initial condition and a boundary condition? I am given the following examples. An example of an initial condition is $u(x,0) = \sin(x)$ $\forall$ $x$. An example of a boundary condition is $u(0,t) = e^{-t}$ $\forall$ $t > 0$. However, these examples are not telling me anything general about the difference between an initial condition and a boundary condition for PDEs. I would like someone to explain this difference in a general way. Thank you.",,"['ordinary-differential-equations', 'partial-differential-equations']"
41,Find the solution of the Cauchy problem $u_t-uu_x=0$.,Find the solution of the Cauchy problem .,u_t-uu_x=0,"For the Cauchy problem $u_t-uu_x=0, ~x\in \mathbb{R}, t>0$ with $u(x,0)=x,~x\in \mathbb{R} $, which of the following statements is true? the solution $u$ exists for all $t>0$. the solution $u$ exists for $t<\frac{1}{2}$ and breaks down at $t=\frac{1}{2}$. the solution $u$ exists for $t<1$ and breaks down at $t=1$ the solution $u$ exists for $t<2$ and breaks down at $t=2$. My work I used the method of characteristics to find out the solution. Given equation is $u_t-uu_x=0$, then  $$\frac{du}{ds}=\frac{\partial u }{\partial x}\frac{dx}{ds}+\frac{\partial u }{\partial t}\frac{dt}{ds}$$.  $\dfrac{dt}{ds}=1$ , letting $t(0)=0$ , we have $t=s$ $\dfrac{du}{ds}=0$ , letting $u(0)=f(x_0)$ , we have $u(x,t)=f(x_0)$ $\dfrac{dx}{ds}=-u$ , letting $x(0)=x_0$ , we have $x=-us+x_0$ , Therefore $u(x,t)=f(x_0)=f(x+ut)$. Now applying the initial condition $u(x,0)=f(x)=x$ we get $u(x,t)=x+ut$. Now the problem is from the solution I am unable to conclude anything from the option given precisely. It seems the option 1 is true but I don't know for sure. How can I solve this? Please help.Thanks.","For the Cauchy problem $u_t-uu_x=0, ~x\in \mathbb{R}, t>0$ with $u(x,0)=x,~x\in \mathbb{R} $, which of the following statements is true? the solution $u$ exists for all $t>0$. the solution $u$ exists for $t<\frac{1}{2}$ and breaks down at $t=\frac{1}{2}$. the solution $u$ exists for $t<1$ and breaks down at $t=1$ the solution $u$ exists for $t<2$ and breaks down at $t=2$. My work I used the method of characteristics to find out the solution. Given equation is $u_t-uu_x=0$, then  $$\frac{du}{ds}=\frac{\partial u }{\partial x}\frac{dx}{ds}+\frac{\partial u }{\partial t}\frac{dt}{ds}$$.  $\dfrac{dt}{ds}=1$ , letting $t(0)=0$ , we have $t=s$ $\dfrac{du}{ds}=0$ , letting $u(0)=f(x_0)$ , we have $u(x,t)=f(x_0)$ $\dfrac{dx}{ds}=-u$ , letting $x(0)=x_0$ , we have $x=-us+x_0$ , Therefore $u(x,t)=f(x_0)=f(x+ut)$. Now applying the initial condition $u(x,0)=f(x)=x$ we get $u(x,t)=x+ut$. Now the problem is from the solution I am unable to conclude anything from the option given precisely. It seems the option 1 is true but I don't know for sure. How can I solve this? Please help.Thanks.",,"['ordinary-differential-equations', 'partial-differential-equations', 'cauchy-problem']"
42,Prove that $f(x) = a\ln(x) + b$ if $f(xy) = f(x) + f(y)$,Prove that  if,f(x) = a\ln(x) + b f(xy) = f(x) + f(y),"I saw this question and was convinced that the statement is true. However I attempted to solve the problem by derivating the equation with respect to $x$: $$\dfrac{\partial f(xy)}{\partial x}y = \dfrac{d f(x)}{d x}.$$ Now if I take the derivative w.r.t. $y$: $$\frac{\partial^2 f(xy)}{\partial x\partial y}xy + \dfrac{\partial f(xy)}{\partial x} = 0.$$ Let $g(x,y) = \dfrac{\partial f(xy)}{\partial x}$, then: $$\partial_y (g) xy + g = 0$$ I'm stuck here, ny help is very useful for me, thanks!","I saw this question and was convinced that the statement is true. However I attempted to solve the problem by derivating the equation with respect to $x$: $$\dfrac{\partial f(xy)}{\partial x}y = \dfrac{d f(x)}{d x}.$$ Now if I take the derivative w.r.t. $y$: $$\frac{\partial^2 f(xy)}{\partial x\partial y}xy + \dfrac{\partial f(xy)}{\partial x} = 0.$$ Let $g(x,y) = \dfrac{\partial f(xy)}{\partial x}$, then: $$\partial_y (g) xy + g = 0$$ I'm stuck here, ny help is very useful for me, thanks!",,['ordinary-differential-equations']
43,"If $a,b$ are linearly independent functions on an interval $I$, are they linearly independent on any interval $J$ contained in $I$?","If  are linearly independent functions on an interval , are they linearly independent on any interval  contained in ?","a,b I J I","Let's say $~a,~ b~$ are linearly independent functions on an interval $~I~$ . Are they linearly independent on any interval $~J~$ contained in $~I~$ ? If so, how do I prove it? Let's say $~a,~ b~$ are instead linearly dependent functions on an interval $~I~$ . Are they linearly dependent on any interval $~J~$ contained in $~I~$ ? If so, how do I prove it? I have a feeling I'm supposed to use the Wronskian determinant for these but I'm not sure how to apply it.","Let's say are linearly independent functions on an interval . Are they linearly independent on any interval contained in ? If so, how do I prove it? Let's say are instead linearly dependent functions on an interval . Are they linearly dependent on any interval contained in ? If so, how do I prove it? I have a feeling I'm supposed to use the Wronskian determinant for these but I'm not sure how to apply it.","~a,~ b~ ~I~ ~J~ ~I~ ~a,~ b~ ~I~ ~J~ ~I~","['ordinary-differential-equations', 'wronskian']"
44,Integrating factor formula derivation plus-minus problem,Integrating factor formula derivation plus-minus problem,,"I'm trying to derive the formula for the solution of a first order ODE, $y = e^{-\mu(x)}\int e^{\mu(x)}q(x)dx$ with $\mu(x) = \int p(x)dx$ for the form $y'+p(x)y = q(x)$. Here's what I did: Rewriting the general form $y'+py = q$, we get \begin{equation} \left[q(x)-p(x)y\right]dx = dy.\label{eqn:int-fact2} \end{equation} Now state $M = q(x) - p(x) y$, $N=1$ and assume left and right hand side to be a constant: \begin{equation} \frac{dM}{dy} = \frac{dN}{dx} = C\label{eqn:int-fact-MN} \end{equation} Use integrating factor $\mu(x)$ to ensure exactness: \begin{equation} \frac{d[\mu(x) \cdot M]}{dy} = \frac{d[\mu (x) \cdot N]}{dx} \Rightarrow \mu(x) \frac{dM}{dy} = \mu(x) \frac{dN}{dx} + \frac{d\mu(x)}{dx} N. \end{equation} \begin{equation} \mu(x)\left[-p(x)\right] = \frac{d\mu(x)}{dx} \Rightarrow -p(x)dx = \frac{d\mu(x)}{\mu(x)}. \end{equation} \begin{equation} \ln \mu(x) = -\int p(x)dx \Rightarrow \boxed{\mu(x) = e^{-\int p(x)dx}} \end{equation} \begin{equation} \int \left[q(x)-p(x)y\right]dx =\int dy, \end{equation} \begin{equation} \int e^{-\int p(x)dx}dy = e^{-\int p(x)dx}y + C(x)  \end{equation} \begin{equation} p(x)ye^{-\int p(x)dx} + C'(x)= q(x)e^{-\int p(x)dx} + p(x) y e^{-\int p(x)dx} \end{equation} \begin{equation} C'(x)= q(x)e^{-\int p(x)dx} \Rightarrow C(x) = \int q(x)e^{-\int p(x)dx}dx + C. \end{equation} Choose $C=0$ to get: \begin{equation} e^{-\int p(x)dx}y + \int q(x)e^{-\int p(x)dx}dx = 0  \end{equation} \begin{equation} e^{-\int p(x)dx}y  = - \int q(x)e^{-\int p(x)dx}dx \Rightarrow \boxed{y = - e^{\int p(x)dx}\int q(x)e^{-\int p(x)dx}dx} \end{equation} So why am I ending up with ye olde $+\mapsto-$ and $-\mapsto+$? Where is the error?","I'm trying to derive the formula for the solution of a first order ODE, $y = e^{-\mu(x)}\int e^{\mu(x)}q(x)dx$ with $\mu(x) = \int p(x)dx$ for the form $y'+p(x)y = q(x)$. Here's what I did: Rewriting the general form $y'+py = q$, we get \begin{equation} \left[q(x)-p(x)y\right]dx = dy.\label{eqn:int-fact2} \end{equation} Now state $M = q(x) - p(x) y$, $N=1$ and assume left and right hand side to be a constant: \begin{equation} \frac{dM}{dy} = \frac{dN}{dx} = C\label{eqn:int-fact-MN} \end{equation} Use integrating factor $\mu(x)$ to ensure exactness: \begin{equation} \frac{d[\mu(x) \cdot M]}{dy} = \frac{d[\mu (x) \cdot N]}{dx} \Rightarrow \mu(x) \frac{dM}{dy} = \mu(x) \frac{dN}{dx} + \frac{d\mu(x)}{dx} N. \end{equation} \begin{equation} \mu(x)\left[-p(x)\right] = \frac{d\mu(x)}{dx} \Rightarrow -p(x)dx = \frac{d\mu(x)}{\mu(x)}. \end{equation} \begin{equation} \ln \mu(x) = -\int p(x)dx \Rightarrow \boxed{\mu(x) = e^{-\int p(x)dx}} \end{equation} \begin{equation} \int \left[q(x)-p(x)y\right]dx =\int dy, \end{equation} \begin{equation} \int e^{-\int p(x)dx}dy = e^{-\int p(x)dx}y + C(x)  \end{equation} \begin{equation} p(x)ye^{-\int p(x)dx} + C'(x)= q(x)e^{-\int p(x)dx} + p(x) y e^{-\int p(x)dx} \end{equation} \begin{equation} C'(x)= q(x)e^{-\int p(x)dx} \Rightarrow C(x) = \int q(x)e^{-\int p(x)dx}dx + C. \end{equation} Choose $C=0$ to get: \begin{equation} e^{-\int p(x)dx}y + \int q(x)e^{-\int p(x)dx}dx = 0  \end{equation} \begin{equation} e^{-\int p(x)dx}y  = - \int q(x)e^{-\int p(x)dx}dx \Rightarrow \boxed{y = - e^{\int p(x)dx}\int q(x)e^{-\int p(x)dx}dx} \end{equation} So why am I ending up with ye olde $+\mapsto-$ and $-\mapsto+$? Where is the error?",,['ordinary-differential-equations']
45,Can we approximate delayed-differential equations with higher-order-ordinary-differential-equations?,Can we approximate delayed-differential equations with higher-order-ordinary-differential-equations?,,"I noticed that the most simple numerical approximation of a higher order-differential equation has the same form as the numerical approximation of a delayed first-order differential equation. This leads me to the following hypothesis: Hypothesis: Delayed first-order differential equations can be approximated by higher-order ordinary differential equations. I wanted to investigate to what extent this is true so I came up with a method to find approximations of delayed differential equations. However, my initial tests seem to indicate that this approach doesn't work. I hope someone can explain why, and if there is a better, similar approach. Here is my approach: Take the simple delayed first-order differential equation for arbitrary function $f$: $$\dot x(t+1)=f(x(t))$$ By taking $\Delta x =1$, this is numerically approximated by the difference equation: $$x_{t+2}-x_{t+1}=f(x_t)$$ adding to both sides $x_t-x_{t+1}$, gives: $$x_{t+2}-2x_{t+1}+x_t=f(x_t) - (x_{t+1}-x_t)$$ Which, if we again take $\Delta x =1$, is the numerical approximation of  $$\ddot x (t)=f(x(t))-\dot x(t)$$ Hence we might approximate a simple first-order delayed equation by a higher order non-delayed equation:  $$\dot x(t+1)=f(x(t)), \quad \text {is approximated by}\quad   \ddot x(t)+\dot x(t)=f(x(t)) $$   By the same approach we could show that    $$\dot x(t+2)=f(x(t)), \quad \text {is approximated by}\quad   \dddot x(t)+2\ddot x(t) +\dot x(t)=f(x(t))$$   $$\dot x(t+3)=f(x(t)), \quad \text {is approximated by}\quad   \ddddot x(t)+3\dddot x(t) -3\ddot x(t)+\dot x(t)=f(x(t))$$   And so forth... However, I've been using $\Delta x =1$ here. If we take $\Delta x=1/n$, and gradually increment $n$ upwards, a similar pattern to the one for larger delays occurs: $$\dot x(t+1)=f(x(t))$$ is approximated for $n=2$ by: $$2\cdot (x(t+1+\frac {1}{2})-x(t+1))=f(x(t))$$ using the same approach as above, but $\Delta x = \frac{1}{2}$ instead of $\Delta x = 1$, and a lot of tedious algebra, one can show that this is equivalent to the approximation of $$2^{-2}\dddot x(t)+2^{-1}\cdot 2 \ddot x(t)+2^{-0}\dot x(t)$$ Hence we might approach a closer and closer approximation of $\dot x(t+1)=f(x(t))$, by taking increasingly larger $n$, and smaller $\Delta x=\frac{1}{n}$, as follows: using $n=2$ (resulting in $\Delta x=\frac {1}{2} $):   $$\dot x(t+1)=f(x(t)), \quad \text {is approximated by}\quad   n^{-n}\cdot \dddot x(t)+n^{-n+1}\cdot 2 \ddot x(t)+n^{-n+2} \cdot \dot x(t)$$   and using $n=3$:   $$\dot x(t+1)=f(x(t)), \quad \text {is approximated by}\quad $$    $$n^{-n}\cdot \ddddot x(t) +n^{-n+1}\cdot 3\dddot x(t)+n^{-n+2} \cdot 3\ddot x(t)+ n^{-n+3}\cdot \dot x(t)$$ And so forth... The form of this approximation is the same as for $\Delta x =1$, but with higher delays, except for the coefficients of $n$. The most important point I noticed about this approximation is that as we increase $n$, increasingly higher order derivatives are added, but the coefficients of those derivatives seem to decrease hyperexponentially (i.e. $n^{-n}$). This made me hope that perhaps the approximations would converge quickly to the delayed equation as $\Delta x$ decreases. However, I did some tests on the delayed equation $\dot x(t)=x(t-1)$, and $\dot x(t) = x^2(t-1)$, where my approach fails miserably. increasing $n$ actually deteriorates the approximation. On the one hand (in hindsight) this makes sense to me, since increasing the order of the derivative should increase the rate of growth of $x$ for large $t$, but it still bugs me that it doesn't work, despite the fact that the difference equations for the two are so similar. So my question is: Why doesn't my approach work, given that the difference-equation approximations of delayed-differential-equations, and higher-order-differential equations have the same form? and more importantly, is there a different but similar approach to approximate delayed-differential-equations?","I noticed that the most simple numerical approximation of a higher order-differential equation has the same form as the numerical approximation of a delayed first-order differential equation. This leads me to the following hypothesis: Hypothesis: Delayed first-order differential equations can be approximated by higher-order ordinary differential equations. I wanted to investigate to what extent this is true so I came up with a method to find approximations of delayed differential equations. However, my initial tests seem to indicate that this approach doesn't work. I hope someone can explain why, and if there is a better, similar approach. Here is my approach: Take the simple delayed first-order differential equation for arbitrary function $f$: $$\dot x(t+1)=f(x(t))$$ By taking $\Delta x =1$, this is numerically approximated by the difference equation: $$x_{t+2}-x_{t+1}=f(x_t)$$ adding to both sides $x_t-x_{t+1}$, gives: $$x_{t+2}-2x_{t+1}+x_t=f(x_t) - (x_{t+1}-x_t)$$ Which, if we again take $\Delta x =1$, is the numerical approximation of  $$\ddot x (t)=f(x(t))-\dot x(t)$$ Hence we might approximate a simple first-order delayed equation by a higher order non-delayed equation:  $$\dot x(t+1)=f(x(t)), \quad \text {is approximated by}\quad   \ddot x(t)+\dot x(t)=f(x(t)) $$   By the same approach we could show that    $$\dot x(t+2)=f(x(t)), \quad \text {is approximated by}\quad   \dddot x(t)+2\ddot x(t) +\dot x(t)=f(x(t))$$   $$\dot x(t+3)=f(x(t)), \quad \text {is approximated by}\quad   \ddddot x(t)+3\dddot x(t) -3\ddot x(t)+\dot x(t)=f(x(t))$$   And so forth... However, I've been using $\Delta x =1$ here. If we take $\Delta x=1/n$, and gradually increment $n$ upwards, a similar pattern to the one for larger delays occurs: $$\dot x(t+1)=f(x(t))$$ is approximated for $n=2$ by: $$2\cdot (x(t+1+\frac {1}{2})-x(t+1))=f(x(t))$$ using the same approach as above, but $\Delta x = \frac{1}{2}$ instead of $\Delta x = 1$, and a lot of tedious algebra, one can show that this is equivalent to the approximation of $$2^{-2}\dddot x(t)+2^{-1}\cdot 2 \ddot x(t)+2^{-0}\dot x(t)$$ Hence we might approach a closer and closer approximation of $\dot x(t+1)=f(x(t))$, by taking increasingly larger $n$, and smaller $\Delta x=\frac{1}{n}$, as follows: using $n=2$ (resulting in $\Delta x=\frac {1}{2} $):   $$\dot x(t+1)=f(x(t)), \quad \text {is approximated by}\quad   n^{-n}\cdot \dddot x(t)+n^{-n+1}\cdot 2 \ddot x(t)+n^{-n+2} \cdot \dot x(t)$$   and using $n=3$:   $$\dot x(t+1)=f(x(t)), \quad \text {is approximated by}\quad $$    $$n^{-n}\cdot \ddddot x(t) +n^{-n+1}\cdot 3\dddot x(t)+n^{-n+2} \cdot 3\ddot x(t)+ n^{-n+3}\cdot \dot x(t)$$ And so forth... The form of this approximation is the same as for $\Delta x =1$, but with higher delays, except for the coefficients of $n$. The most important point I noticed about this approximation is that as we increase $n$, increasingly higher order derivatives are added, but the coefficients of those derivatives seem to decrease hyperexponentially (i.e. $n^{-n}$). This made me hope that perhaps the approximations would converge quickly to the delayed equation as $\Delta x$ decreases. However, I did some tests on the delayed equation $\dot x(t)=x(t-1)$, and $\dot x(t) = x^2(t-1)$, where my approach fails miserably. increasing $n$ actually deteriorates the approximation. On the one hand (in hindsight) this makes sense to me, since increasing the order of the derivative should increase the rate of growth of $x$ for large $t$, but it still bugs me that it doesn't work, despite the fact that the difference equations for the two are so similar. So my question is: Why doesn't my approach work, given that the difference-equation approximations of delayed-differential-equations, and higher-order-differential equations have the same form? and more importantly, is there a different but similar approach to approximate delayed-differential-equations?",,"['ordinary-differential-equations', 'delay-differential-equations']"
46,Do I need to study ODE in order to understand Fourier series and Fourier Transform?,Do I need to study ODE in order to understand Fourier series and Fourier Transform?,,"I'm a self study in math, computer vision and machine learning. I just finished partial derivative and has no background in Ordinary differential equation(ODE). Do I need to study ODE in order to understand Fourier series and Fourier Transform? The Fourier series and Fourier Transform appears in my computer vision class and I feel unfamiliar with some notations. maybe ODE will also benefit when I study deep learning too?","I'm a self study in math, computer vision and machine learning. I just finished partial derivative and has no background in Ordinary differential equation(ODE). Do I need to study ODE in order to understand Fourier series and Fourier Transform? The Fourier series and Fourier Transform appears in my computer vision class and I feel unfamiliar with some notations. maybe ODE will also benefit when I study deep learning too?",,"['ordinary-differential-equations', 'fourier-series', 'fourier-transform']"
47,Dividing an ODE by $x$,Dividing an ODE by,x,"When solving ODEs in class, the lecturer frequently divides the equations by $x$, usually in order to get it to its normalized form. But $x$ might be zero sometimes, How do we handle this operation, and what are its implications? Example: $x\cdot y' = y$ Divided by x: $y' = \frac{y}{x}$ Thank you.","When solving ODEs in class, the lecturer frequently divides the equations by $x$, usually in order to get it to its normalized form. But $x$ might be zero sometimes, How do we handle this operation, and what are its implications? Example: $x\cdot y' = y$ Divided by x: $y' = \frac{y}{x}$ Thank you.",,['ordinary-differential-equations']
48,Which solvable ODE's correspond to algebraic curves?,Which solvable ODE's correspond to algebraic curves?,,"This is a follow-up to my previous question: Are there periodic functions satisfying a quadratic differential equation? Let $P(u,v)$ be a bivariate polynomial. Then its zero set $\{(u,v): P(u,v)=0\}$ is an algebraic curve. It might make it easier to assume in what follows that the curve has no singularities. If we substitute $y$ and $y'$ into $P(u,v)$ to take the places of $u$ and $v$ respectively, then we get a possibly non-linear first-order ODE. I am curious about how many ODEs of this type have solutions. 1. How many/which first order ODE's formed by substituting into a bivariate polynomial have solutions? 2. Does any solution to such an ODE parametrize the algebraic curve that characterizes the defining ODE? EDIT: I realize now that the answer to this question is obviously yes, because it follows directly from the fact that $y$ and $y'$ are both functions of the same single variable (say $t$ ) and that they satisfy the equation $P(y,y')=0$ , so if they exist they must parametrize the algebraic curve, as a result of their definition. Some examples might help. For second-degree algebraic curves, we have that the equation of the unit circle corresponds to a first order ODE which has both sine and cosine as solutions, and that these two functions parametrize the unit circle as well: $$(y')^2 + y^2 -1=0  $$ Likewise, the hyperbolic sine and hyperbolic cosine parametrize the unit hyperbola, and both correspond to solutions of a first order ODE whose form is that of the unit hyperbola: $$(y')^2 - y^2  -1=0$$ Finally, smooth cubic curves in Weierstrass normal form can be parametrized by the Weierstrass $\wp$ functions, and the (complex) differential equations characterizing the Weierstrass $\wp$ functions have the form of a smooth cuvic curve in Weierstrass normal form: $$(\wp')^2 = 4[\wp^2] -g_2 \wp -g_3  $$ Then my question is essentially: how far does this go? For examples, if I were to choose the algebraic curve $$ v^4 - v^3 = u^5 +u^2 -7$$ would the first-order ODE $$(y')^4 - (y')^3 = y^5 + y^2 -7$$ have a solution? And would the solution to this ODE (if it existed) parametrize the original algebraic curve $v^4 - v^3 = u^5 + u^2 -7$ ? This example is completely arbitrary, but hopefully it makes clear to some extent the level of generality I am interested in. Note: I am not sure how to properly tag this question. This question is related but addresses second order ODE's -- however, I am not interested in second order ODE's, only first order: Algebraic Curves and Second Order Differential Equations I think that this question is probably the most related, assuming that epicycloids can be defined by first-order ODEs (I don't know either way). Proof that Epicycloids are Algebraic Curves?","This is a follow-up to my previous question: Are there periodic functions satisfying a quadratic differential equation? Let be a bivariate polynomial. Then its zero set is an algebraic curve. It might make it easier to assume in what follows that the curve has no singularities. If we substitute and into to take the places of and respectively, then we get a possibly non-linear first-order ODE. I am curious about how many ODEs of this type have solutions. 1. How many/which first order ODE's formed by substituting into a bivariate polynomial have solutions? 2. Does any solution to such an ODE parametrize the algebraic curve that characterizes the defining ODE? EDIT: I realize now that the answer to this question is obviously yes, because it follows directly from the fact that and are both functions of the same single variable (say ) and that they satisfy the equation , so if they exist they must parametrize the algebraic curve, as a result of their definition. Some examples might help. For second-degree algebraic curves, we have that the equation of the unit circle corresponds to a first order ODE which has both sine and cosine as solutions, and that these two functions parametrize the unit circle as well: Likewise, the hyperbolic sine and hyperbolic cosine parametrize the unit hyperbola, and both correspond to solutions of a first order ODE whose form is that of the unit hyperbola: Finally, smooth cubic curves in Weierstrass normal form can be parametrized by the Weierstrass functions, and the (complex) differential equations characterizing the Weierstrass functions have the form of a smooth cuvic curve in Weierstrass normal form: Then my question is essentially: how far does this go? For examples, if I were to choose the algebraic curve would the first-order ODE have a solution? And would the solution to this ODE (if it existed) parametrize the original algebraic curve ? This example is completely arbitrary, but hopefully it makes clear to some extent the level of generality I am interested in. Note: I am not sure how to properly tag this question. This question is related but addresses second order ODE's -- however, I am not interested in second order ODE's, only first order: Algebraic Curves and Second Order Differential Equations I think that this question is probably the most related, assuming that epicycloids can be defined by first-order ODEs (I don't know either way). Proof that Epicycloids are Algebraic Curves?","P(u,v) \{(u,v): P(u,v)=0\} y y' P(u,v) u v y y' t P(y,y')=0 (y')^2 + y^2 -1=0   (y')^2 - y^2  -1=0 \wp \wp (\wp')^2 = 4[\wp^2] -g_2 \wp -g_3    v^4 - v^3 = u^5 +u^2 -7 (y')^4 - (y')^3 = y^5 + y^2 -7 v^4 - v^3 = u^5 + u^2 -7","['ordinary-differential-equations', 'algebraic-geometry', 'algebraic-curves', 'parametric']"
49,Why the separable equation $\frac{dy}{dt}=\frac{-t}{y}$ has no equilibrium solution?,Why the separable equation  has no equilibrium solution?,\frac{dy}{dt}=\frac{-t}{y},"I am given a separable equation with initial value$$y^{'} = -\frac{t}{y} \qquad y(0)=1$$. This is an example from the book however I am not sure why this equation has no equilibrium solution. I do know that an equilibrium solution is a value of $y$ for which $\frac{dy}{dt}=0$ and this happens when $t=0$. From what I understood, the only time this equation has equilibrium solution is when $y=1$ but the equation suggests 2 values for $y$ which can't be possible for the given IVP. Is that why it has no equilibrium solution? Please help.","I am given a separable equation with initial value$$y^{'} = -\frac{t}{y} \qquad y(0)=1$$. This is an example from the book however I am not sure why this equation has no equilibrium solution. I do know that an equilibrium solution is a value of $y$ for which $\frac{dy}{dt}=0$ and this happens when $t=0$. From what I understood, the only time this equation has equilibrium solution is when $y=1$ but the equation suggests 2 values for $y$ which can't be possible for the given IVP. Is that why it has no equilibrium solution? Please help.",,['ordinary-differential-equations']
50,Are there other good ways to look at this differential equation or is this it?,Are there other good ways to look at this differential equation or is this it?,,"\begin{align} \tan \frac \alpha 2 & = u \\[4pt] \alpha & = 2\arctan u \\[4pt] d\alpha & = \frac{2\,du}{1+u^2} \\[4pt] \sin\alpha & = \sin(2\arctan u) = 2(\sin\arctan u) (\cos \arctan u) \\[4pt] & = 2\,\frac u {\sqrt{1+u^2}} \cdot \frac 1 {\sqrt{1+u^2}} = \frac{2u}{1+u^2} \end{align} $$ \frac{\hspace{6cm}}{} \qquad \S \qquad \frac{\hspace{6cm}}{} $$ The well known substitution above is for the moment my preferred approach to solving the differential equation $$ \frac{d\alpha}{\sin\alpha} = \frac{d\beta}{\sin\beta}. \tag{differential equation} $$ One gets $$ \frac{d\alpha}{\sin\alpha} = \frac{\left( \dfrac{2\,du}{1+u^2} \right)}{\left( \dfrac{2u}{1+u^2} \right)} = \frac{du} u $$ and writing $v = \tan \dfrac\beta 2$, we then have $$ \frac{du} u = \frac {dv} v $$ so that $\log u = \log v + \text{constant}$ and so $u = v\times \text{constant}$, and finally $$ \tan \frac\alpha 2 = \text{constant} \times \tan \frac\beta 2. \tag{solution} $$ So my question is whether there are other ways to approach this that are either better or otherwise worth some attention.","\begin{align} \tan \frac \alpha 2 & = u \\[4pt] \alpha & = 2\arctan u \\[4pt] d\alpha & = \frac{2\,du}{1+u^2} \\[4pt] \sin\alpha & = \sin(2\arctan u) = 2(\sin\arctan u) (\cos \arctan u) \\[4pt] & = 2\,\frac u {\sqrt{1+u^2}} \cdot \frac 1 {\sqrt{1+u^2}} = \frac{2u}{1+u^2} \end{align} $$ \frac{\hspace{6cm}}{} \qquad \S \qquad \frac{\hspace{6cm}}{} $$ The well known substitution above is for the moment my preferred approach to solving the differential equation $$ \frac{d\alpha}{\sin\alpha} = \frac{d\beta}{\sin\beta}. \tag{differential equation} $$ One gets $$ \frac{d\alpha}{\sin\alpha} = \frac{\left( \dfrac{2\,du}{1+u^2} \right)}{\left( \dfrac{2u}{1+u^2} \right)} = \frac{du} u $$ and writing $v = \tan \dfrac\beta 2$, we then have $$ \frac{du} u = \frac {dv} v $$ so that $\log u = \log v + \text{constant}$ and so $u = v\times \text{constant}$, and finally $$ \tan \frac\alpha 2 = \text{constant} \times \tan \frac\beta 2. \tag{solution} $$ So my question is whether there are other ways to approach this that are either better or otherwise worth some attention.",,"['ordinary-differential-equations', 'substitution']"
51,particular solution. tan function as RHS,particular solution. tan function as RHS,,How would you discover the particular solution for $$y'' + y = \tan(x)$$ Not really sure where to start with this one. Would it involve a trigonometric identity?,How would you discover the particular solution for $$y'' + y = \tan(x)$$ Not really sure where to start with this one. Would it involve a trigonometric identity?,,['ordinary-differential-equations']
52,How to understand the change of basis in a differential equation,How to understand the change of basis in a differential equation,,"Let $w:\mathbb{R} \times \mathbb{R}^n \rightarrow Mat(n,\mathbb{R})$ be a smooth function, $R_{ij}$ be a fixed skew-symmetric $n\times n$ real matrix, and $A\in\mathbb{R}$. Consider the equation $$\frac{\partial w}{\partial t} - \sum_{i=1}^n\left(\frac{\partial}{\partial x^i} + \frac{1}{4}\sum_j {R}_{ij} x^j\right)^2 w + Aw = 0.$$ My question is, if one wants to solve for $w$, why is it sufficient to take $R_{ij}$ to be block-diagonal, consisting of $2\times 2$ skew-symmetric blocks? I know that a skew-symmetric matrix can always be block-diagonalised this way, but am having a hard time writing down rigorously what the solution to the equation would be if $R$ was changed to a block-diagonal form, say $P^{-1} RP$. I'm looking for a proof that shows something like, if $w$ solves the above equation, then $P^{-1}wP$ solves the equation with $R$ replaced by $P^{-1}RP$. Thanks for any help.","Let $w:\mathbb{R} \times \mathbb{R}^n \rightarrow Mat(n,\mathbb{R})$ be a smooth function, $R_{ij}$ be a fixed skew-symmetric $n\times n$ real matrix, and $A\in\mathbb{R}$. Consider the equation $$\frac{\partial w}{\partial t} - \sum_{i=1}^n\left(\frac{\partial}{\partial x^i} + \frac{1}{4}\sum_j {R}_{ij} x^j\right)^2 w + Aw = 0.$$ My question is, if one wants to solve for $w$, why is it sufficient to take $R_{ij}$ to be block-diagonal, consisting of $2\times 2$ skew-symmetric blocks? I know that a skew-symmetric matrix can always be block-diagonalised this way, but am having a hard time writing down rigorously what the solution to the equation would be if $R$ was changed to a block-diagonal form, say $P^{-1} RP$. I'm looking for a proof that shows something like, if $w$ solves the above equation, then $P^{-1}wP$ solves the equation with $R$ replaced by $P^{-1}RP$. Thanks for any help.",,"['linear-algebra', 'ordinary-differential-equations']"
53,Are S.I. Euler and Verlet the same thing?,Are S.I. Euler and Verlet the same thing?,,"Verlet as given by Wikipedia : set $\vec x_1=\vec x_0+\vec v_0\,\Delta t+\frac12 A(\vec x_0)\,\Delta t^2$ for ''n=1,2,...'' iterate $\vec x_{n+1}=2 \vec x_n- \vec x_{n-1}+ A(\vec x_n)\,\Delta t^2.$ S.I. Euler as given by Wikipedia : $v_{n+1} = v_n + g(t_n, x_n) \, \Delta t\\ x_{n+1} = x_n + f(t_n, v_{n+1}) \, \Delta t$ Starting with S.I. Euler (and simplifying the notation): $$ x_{n+1} = x_n + hv_{n+1} \implies v_n = \frac{x_n - x_{n-1}}{h}\\ x_{n+1} = x_n + hv_{n+1} = x_n + h(v_n + ha_n) = x_n + h\left(\frac{x_n - x_{n-1}}{h} + ha_n\right) = 2x_n - x_{n-1} + h^2a_n $$ OK so they're the same thing, cool. That matches my tests (some simple projectile stuff + wind resistance + weird acceleration functions) where Verlet and SI Euler perform to within floating point error. However the Verlet page says, "" The global error of all Euler methods is of order one, whereas the global error of [Verlet] is, similar to the midpoint method, of order two. "" It also says, "" The global error in position, in contrast, is $O(\Delta t^{2})$ and the global error in velocity is $O(\Delta t^{2})$. "" The SI Euler page says "" The semi-implicit Euler is a first-order integrator, just as the standard Euler method. "" How can they have different orders when they are the same method and seem to produce identical results?","Verlet as given by Wikipedia : set $\vec x_1=\vec x_0+\vec v_0\,\Delta t+\frac12 A(\vec x_0)\,\Delta t^2$ for ''n=1,2,...'' iterate $\vec x_{n+1}=2 \vec x_n- \vec x_{n-1}+ A(\vec x_n)\,\Delta t^2.$ S.I. Euler as given by Wikipedia : $v_{n+1} = v_n + g(t_n, x_n) \, \Delta t\\ x_{n+1} = x_n + f(t_n, v_{n+1}) \, \Delta t$ Starting with S.I. Euler (and simplifying the notation): $$ x_{n+1} = x_n + hv_{n+1} \implies v_n = \frac{x_n - x_{n-1}}{h}\\ x_{n+1} = x_n + hv_{n+1} = x_n + h(v_n + ha_n) = x_n + h\left(\frac{x_n - x_{n-1}}{h} + ha_n\right) = 2x_n - x_{n-1} + h^2a_n $$ OK so they're the same thing, cool. That matches my tests (some simple projectile stuff + wind resistance + weird acceleration functions) where Verlet and SI Euler perform to within floating point error. However the Verlet page says, "" The global error of all Euler methods is of order one, whereas the global error of [Verlet] is, similar to the midpoint method, of order two. "" It also says, "" The global error in position, in contrast, is $O(\Delta t^{2})$ and the global error in velocity is $O(\Delta t^{2})$. "" The SI Euler page says "" The semi-implicit Euler is a first-order integrator, just as the standard Euler method. "" How can they have different orders when they are the same method and seem to produce identical results?",,"['ordinary-differential-equations', 'numerical-methods', 'eulers-method']"
54,Can someone explain where this comes from for the cauchy-euler equation in the case of double roots,Can someone explain where this comes from for the cauchy-euler equation in the case of double roots,,"I am not sure how the following was derived in my textbook given by I understand that the solution we are seeking is $y = x^r$ and the fact that we obtained the first solution as $y = c_1 x^{r_1}$ and that we are looking for a 2nd solution, by taking the derivative with respect to $r$. However, it is a bit puzzling as to how equation (3.8) was arrived at, and in particular how $$L[\frac{\partial}{\partial r} x^r] = L[x^r\log{x}]$$ Would someone mind explaining please? Explanations are greatly appreciated. Thank you.","I am not sure how the following was derived in my textbook given by I understand that the solution we are seeking is $y = x^r$ and the fact that we obtained the first solution as $y = c_1 x^{r_1}$ and that we are looking for a 2nd solution, by taking the derivative with respect to $r$. However, it is a bit puzzling as to how equation (3.8) was arrived at, and in particular how $$L[\frac{\partial}{\partial r} x^r] = L[x^r\log{x}]$$ Would someone mind explaining please? Explanations are greatly appreciated. Thank you.",,['ordinary-differential-equations']
55,Can I find a function of $y$ that satisfies the relation $\dfrac{df(y)}{dx} = y^2(3y'+1)$,Can I find a function of  that satisfies the relation,y \dfrac{df(y)}{dx} = y^2(3y'+1),"Suppose we have an unknown function $y=y(x)$  , is it possible to find a function $f(y)$ such that: $$\dfrac{df}{dx}= y^2\left(3\dfrac{dy}{dx}+1 \right)$$? EDIT: of course if there is no $1$ in the RHS, the solution will be $f(y) = y^3$ I appreciate any help Thank you","Suppose we have an unknown function $y=y(x)$  , is it possible to find a function $f(y)$ such that: $$\dfrac{df}{dx}= y^2\left(3\dfrac{dy}{dx}+1 \right)$$? EDIT: of course if there is no $1$ in the RHS, the solution will be $f(y) = y^3$ I appreciate any help Thank you",,['ordinary-differential-equations']
56,Proving $J_n(x)N_{n+1}(x)-J_{n+1}(x)N_n(x)=-\dfrac{2}{\pi x}$: Part $1$ of $3$,Proving : Part  of,J_n(x)N_{n+1}(x)-J_{n+1}(x)N_n(x)=-\dfrac{2}{\pi x} 1 3,This is the first part of a proof that $J_n(x)N_{n+1}(x)-J_{n+1}(x)N_n(x)=-\dfrac{2}{\pi x}$: Write Bessel's equation $$x^2y^{\prime\prime}+xy^{\prime} + (x^2 - p^2)y=0\tag{1}$$ with $y=J_p$ and again with $y=J_{-p}$; multiply   the $J_p$ equation by $J_{−p}$ and the $J_{−p}$ equation by $J_p$ and subtract to get $$\frac{\mathrm{d}}{\mathrm{d}x}\left[x\left(J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}\right)\right]=0\tag{2}$$ So I wrote $(1)$ with $y=J_p$: $$x^2J_p^{\prime\prime}+xJ_p^{\prime} + (x^2 - p^2)J_p=0\tag{3}$$ and again with $y=J_{-p}$: $$x^2J_{-p}^{\prime\prime}+xJ_{-p}^{\prime} + (x^2 - p^2)J_{-p}=0\tag{4}$$ Multiplying $(3)$ by $J_{-p}$: $$x^2J_p^{\prime\prime}J_{-p}+xJ_p^{\prime}J_{-p} + (x^2 - p^2)J_pJ_{-p}=0\tag{5}$$ Multiplying $(4)$ by $J_{p}$: $$x^2J_{-p}^{\prime\prime}J_{p}+xJ_{-p}^{\prime}J_{p} + (x^2 - p^2)J_{-p}J_{p}=0\tag{6}$$ Subtracting $(6)$ from $(5)$ gives: $$x^2\left[J_p^{\prime\prime}J_{-p}-J_pJ_{-p}^{\prime\prime}\right]+x\left[J_p^{\prime}J_{-p}-J_pJ_{-p}^{\prime}\right]=0\tag{7}$$ Rewriting the second term of $(7)$ and cancelling the $x$'s as $x\ne 0$ gives $$x\left[J_p^{\prime\prime}J_{-p}-J_pJ_{-p}^{\prime\prime}\right]+\left[J_pJ_{-p}\right]^{\prime}=0\tag{8}$$ but I'm not sure if this is helping. Could anyone please give me some hints or advice on how I can obtain equation $(2)$? $$\bbox[#AFA]{\frac{\mathrm{d}}{\mathrm{d}x}\left[x\left(J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}\right)\right]=0}\tag{2}$$ Many thanks. EDIT: In response to the answer given by @okrzysik \begin{equation} x[(J_{p}'' J_{-p} + J_{p}' J_{-p}') - (J_{p} J_{-p}'' + J_{p}' J_{-p}')] + [J_{p}' J_{-p} - J_{p} J_{-p}'] = 0. \end{equation} \begin{equation} \implies x[(J_{p}'J_{-p})' - (J_{-p}'J_{p})'] + [J_{p}' J_{-p} - J_{p} J_{-p}'] = 0. \end{equation} I'm still unsure how to show that this is equal to $$\bbox[#AFF]{\frac{\mathrm{d}}{\mathrm{d}x}\left[x\left(J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}\right)\right]=0}\tag{2}$$ Could anyone please assist me on these final steps? Thanks again.,This is the first part of a proof that $J_n(x)N_{n+1}(x)-J_{n+1}(x)N_n(x)=-\dfrac{2}{\pi x}$: Write Bessel's equation $$x^2y^{\prime\prime}+xy^{\prime} + (x^2 - p^2)y=0\tag{1}$$ with $y=J_p$ and again with $y=J_{-p}$; multiply   the $J_p$ equation by $J_{−p}$ and the $J_{−p}$ equation by $J_p$ and subtract to get $$\frac{\mathrm{d}}{\mathrm{d}x}\left[x\left(J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}\right)\right]=0\tag{2}$$ So I wrote $(1)$ with $y=J_p$: $$x^2J_p^{\prime\prime}+xJ_p^{\prime} + (x^2 - p^2)J_p=0\tag{3}$$ and again with $y=J_{-p}$: $$x^2J_{-p}^{\prime\prime}+xJ_{-p}^{\prime} + (x^2 - p^2)J_{-p}=0\tag{4}$$ Multiplying $(3)$ by $J_{-p}$: $$x^2J_p^{\prime\prime}J_{-p}+xJ_p^{\prime}J_{-p} + (x^2 - p^2)J_pJ_{-p}=0\tag{5}$$ Multiplying $(4)$ by $J_{p}$: $$x^2J_{-p}^{\prime\prime}J_{p}+xJ_{-p}^{\prime}J_{p} + (x^2 - p^2)J_{-p}J_{p}=0\tag{6}$$ Subtracting $(6)$ from $(5)$ gives: $$x^2\left[J_p^{\prime\prime}J_{-p}-J_pJ_{-p}^{\prime\prime}\right]+x\left[J_p^{\prime}J_{-p}-J_pJ_{-p}^{\prime}\right]=0\tag{7}$$ Rewriting the second term of $(7)$ and cancelling the $x$'s as $x\ne 0$ gives $$x\left[J_p^{\prime\prime}J_{-p}-J_pJ_{-p}^{\prime\prime}\right]+\left[J_pJ_{-p}\right]^{\prime}=0\tag{8}$$ but I'm not sure if this is helping. Could anyone please give me some hints or advice on how I can obtain equation $(2)$? $$\bbox[#AFA]{\frac{\mathrm{d}}{\mathrm{d}x}\left[x\left(J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}\right)\right]=0}\tag{2}$$ Many thanks. EDIT: In response to the answer given by @okrzysik \begin{equation} x[(J_{p}'' J_{-p} + J_{p}' J_{-p}') - (J_{p} J_{-p}'' + J_{p}' J_{-p}')] + [J_{p}' J_{-p} - J_{p} J_{-p}'] = 0. \end{equation} \begin{equation} \implies x[(J_{p}'J_{-p})' - (J_{-p}'J_{p})'] + [J_{p}' J_{-p} - J_{p} J_{-p}'] = 0. \end{equation} I'm still unsure how to show that this is equal to $$\bbox[#AFF]{\frac{\mathrm{d}}{\mathrm{d}x}\left[x\left(J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}\right)\right]=0}\tag{2}$$ Could anyone please assist me on these final steps? Thanks again.,,"['ordinary-differential-equations', 'bessel-functions']"
57,How to get the coordinates of the center of the ellipse after approximation,How to get the coordinates of the center of the ellipse after approximation,,"I create an algorithm recognizing ellipses in images. I have five coordinates (points) possible ellipse. (8.8) (7.4) (6.3) (3.6) and (2.2) I use the formula of the conical section of the second order: $Ax ^ 2 + Bxy + Cy ^ 2 + Dx + Ey + F = 0$ And determines the type of conic section by the value of the discriminant. $B ^ 2-4AC$ Where, if the sign of the value < 0 , then this ellipse. On the basis of the coordinate values I find A, B, C, D, E, F . (Calculates them here ) A = 0.0763889  B = -0.0902778  C = 0.0763889  D = -0.312500  E = -0.312500  F = 1.00000 But I do not know what to do next calculations. How do I need to get the coordinates of the center of the ellipse, the length of its two axes, and rotation about the coordinate axes? Thank you!","I create an algorithm recognizing ellipses in images. I have five coordinates (points) possible ellipse. (8.8) (7.4) (6.3) (3.6) and (2.2) I use the formula of the conical section of the second order: $Ax ^ 2 + Bxy + Cy ^ 2 + Dx + Ey + F = 0$ And determines the type of conic section by the value of the discriminant. $B ^ 2-4AC$ Where, if the sign of the value < 0 , then this ellipse. On the basis of the coordinate values I find A, B, C, D, E, F . (Calculates them here ) A = 0.0763889  B = -0.0902778  C = 0.0763889  D = -0.312500  E = -0.312500  F = 1.00000 But I do not know what to do next calculations. How do I need to get the coordinates of the center of the ellipse, the length of its two axes, and rotation about the coordinate axes? Thank you!",,"['ordinary-differential-equations', 'discrete-mathematics', 'conic-sections', 'quadratics']"
58,Help visualizing solutions to the (1D) wave equation.,Help visualizing solutions to the (1D) wave equation.,,"I know that the one-dimensional wave equation can be written as $$ \frac{\partial^2 u}{\partial t^2} = c^2 \frac{\partial ^2 u}{\partial x^2}$$ and has solutions of the form $$ u = F(x+ct) + G(x-ct)$$ I'm having trouble developing a proper intuition about the meaning of the solution, though. I superficially understand that it's the sum of two functions ""travelling"" in different directions with time, but that doesn't help me be able to really visualize what solutions look like. More specifically, I'd like to develop an intuitive or visual understanding of what solutions to the wave equation have in common, and what separates them from functions that aren't solutions.","I know that the one-dimensional wave equation can be written as $$ \frac{\partial^2 u}{\partial t^2} = c^2 \frac{\partial ^2 u}{\partial x^2}$$ and has solutions of the form $$ u = F(x+ct) + G(x-ct)$$ I'm having trouble developing a proper intuition about the meaning of the solution, though. I superficially understand that it's the sum of two functions ""travelling"" in different directions with time, but that doesn't help me be able to really visualize what solutions look like. More specifically, I'd like to develop an intuitive or visual understanding of what solutions to the wave equation have in common, and what separates them from functions that aren't solutions.",,"['ordinary-differential-equations', 'wave-equation']"
59,How to solve a differential equation with a distributional free term?,How to solve a differential equation with a distributional free term?,,"I tried to solve this type of differential equation $$y'' + y = \delta + \delta' .$$ I tried using the Laplace Transform, but I'm stuck at that $\delta$ (Dirac function). The only thing I know is the solution $$(\cos (t) + \sin (t)) \, u(t)$$ with $u(t)$ being Heaviside's step function. EDIT : $$y'' + y = \delta + \delta' .$$ not $$y'' + y' = \delta + \delta' .$$","I tried to solve this type of differential equation $$y'' + y = \delta + \delta' .$$ I tried using the Laplace Transform, but I'm stuck at that $\delta$ (Dirac function). The only thing I know is the solution $$(\cos (t) + \sin (t)) \, u(t)$$ with $u(t)$ being Heaviside's step function. EDIT : $$y'' + y = \delta + \delta' .$$ not $$y'' + y' = \delta + \delta' .$$",,"['ordinary-differential-equations', 'laplace-transform', 'distribution-theory', 'dirac-delta']"
60,Differential equation exercise.,Differential equation exercise.,,"I am tasked with solving \begin{cases}  y''(t) &=& \frac{(y(t)')^2}{y} - 2\frac{y'(t)}{y^4(t)} \\ y(0)   &=& -1 \\  y'(0)  &=& -2 \end{cases} I proceed by setting $v(s) = y' (y^{-1}(s))$ reducing the problem to \begin{cases}  v'(s) &=& \frac{v(s)}{s} - \frac{2}{s^4} \\  v(-1) &=& -2  \end{cases} This is a first order linear ODE, after multiplying by $\frac{1}{|s|}$ (I drop the absolute value and change the sign since $s$ will be in a negative interval). I obtain $$-\frac{v'(s)}{s} + \frac{v}{s^{2}} = \frac{2}{s^5}$$ This gives me $$\frac{v(s)}{s} = \frac{1}{(-2s^4)} + \frac{5}{2} \implies y'(t) =  \frac{-1 + 5y(t)^4}{2y(t)^3}$$ And I can't quite manage to integrate the reciprocal of this to get my $y(t)$. Are my calculations correct? How should I do this?  I am utilizing this method because it's the one I am expected to use at the exam (I have already been told that it can make things more difficult).","I am tasked with solving \begin{cases}  y''(t) &=& \frac{(y(t)')^2}{y} - 2\frac{y'(t)}{y^4(t)} \\ y(0)   &=& -1 \\  y'(0)  &=& -2 \end{cases} I proceed by setting $v(s) = y' (y^{-1}(s))$ reducing the problem to \begin{cases}  v'(s) &=& \frac{v(s)}{s} - \frac{2}{s^4} \\  v(-1) &=& -2  \end{cases} This is a first order linear ODE, after multiplying by $\frac{1}{|s|}$ (I drop the absolute value and change the sign since $s$ will be in a negative interval). I obtain $$-\frac{v'(s)}{s} + \frac{v}{s^{2}} = \frac{2}{s^5}$$ This gives me $$\frac{v(s)}{s} = \frac{1}{(-2s^4)} + \frac{5}{2} \implies y'(t) =  \frac{-1 + 5y(t)^4}{2y(t)^3}$$ And I can't quite manage to integrate the reciprocal of this to get my $y(t)$. Are my calculations correct? How should I do this?  I am utilizing this method because it's the one I am expected to use at the exam (I have already been told that it can make things more difficult).",,"['real-analysis', 'ordinary-differential-equations']"
61,Stuck trying to solve a PDE by method of characteristics,Stuck trying to solve a PDE by method of characteristics,,"I've been trying to solve the inhomogeneous PDE IVP $u_t+c u_x = e^{2x}$; $u(x,0)=f(x)$, but got stuck. Would appreciate some help. Here's what I did by trying to use the method of characteristics: $\frac{dt}{ds}=1$, $\frac{dx}{ds}=c$, $\frac{dz}{ds}=e^{2x}$, where $s$ is the parametrization variable for a characteristic curve lying in the surface $u(x,t)$. So I get $t(s) = s+C_1$, $x(s) = cs+C_2$, $z(s) = e^{2x}+C_3$. We can thus see that $ct-x = C_1-C_2 = D_1$, where $C_i$ and $D_i$ are some constants. But here's where I'm stuck since I have no idea what should be done next. That is, how can we now relate $ct-x$ to $z(x,t)$ to build the surface? I was thinking that, maybe, do something like this: $z-te^{2x}=C_3-C_1=D_2$, so $z(x,t) = D_2 + te^{2(ct-D_1)}$ (after substituting). But is this the solution then and what's remaining is to apply the initial condition? $z(x,t) = u(x,t)$, $u(x,0) = D_2 = f(x)$, so $u(x,t) = f(x) + te^{2(ct-D_1)}$. Unfortunately, this does not match the solution from WolframAlpha.","I've been trying to solve the inhomogeneous PDE IVP $u_t+c u_x = e^{2x}$; $u(x,0)=f(x)$, but got stuck. Would appreciate some help. Here's what I did by trying to use the method of characteristics: $\frac{dt}{ds}=1$, $\frac{dx}{ds}=c$, $\frac{dz}{ds}=e^{2x}$, where $s$ is the parametrization variable for a characteristic curve lying in the surface $u(x,t)$. So I get $t(s) = s+C_1$, $x(s) = cs+C_2$, $z(s) = e^{2x}+C_3$. We can thus see that $ct-x = C_1-C_2 = D_1$, where $C_i$ and $D_i$ are some constants. But here's where I'm stuck since I have no idea what should be done next. That is, how can we now relate $ct-x$ to $z(x,t)$ to build the surface? I was thinking that, maybe, do something like this: $z-te^{2x}=C_3-C_1=D_2$, so $z(x,t) = D_2 + te^{2(ct-D_1)}$ (after substituting). But is this the solution then and what's remaining is to apply the initial condition? $z(x,t) = u(x,t)$, $u(x,0) = D_2 = f(x)$, so $u(x,t) = f(x) + te^{2(ct-D_1)}$. Unfortunately, this does not match the solution from WolframAlpha.",,"['ordinary-differential-equations', 'partial-differential-equations']"
62,A criterion for area preserving dynamical system,A criterion for area preserving dynamical system,,"In my investigation of dynamical systems I was met with this seemingly easy question I could not find an answer to: If we have a two dimensional system of autonomous ODEs viewed as a 2D dynamical system: $\dot{x} = f(x,y)$ $\dot{y} = g(x,y)$ Is there a simple way to check if this system is area preserving or not, possibly via the Jacobi matrix? I realize this may seem easy and ignorant of me to ask but I just need to know if there is a definitive simple answer. I assume f,g and infinitely differentiable.","In my investigation of dynamical systems I was met with this seemingly easy question I could not find an answer to: If we have a two dimensional system of autonomous ODEs viewed as a 2D dynamical system: Is there a simple way to check if this system is area preserving or not, possibly via the Jacobi matrix? I realize this may seem easy and ignorant of me to ask but I just need to know if there is a definitive simple answer. I assume f,g and infinitely differentiable.","\dot{x} = f(x,y) \dot{y} = g(x,y)","['ordinary-differential-equations', 'dynamical-systems', 'area', 'classical-mechanics']"
63,Differential Equations: Solve $(x^2-1){dy\over dx} + 2xy = x$,Differential Equations: Solve,(x^2-1){dy\over dx} + 2xy = x,I managed to get to $$y = {x^2+2C\over 2x^2-2}$$ Not sure if this is right. Help would greatly be appreciated.,I managed to get to $$y = {x^2+2C\over 2x^2-2}$$ Not sure if this is right. Help would greatly be appreciated.,,"['calculus', 'ordinary-differential-equations']"
64,Convolution: How to construct it for a given function?,Convolution: How to construct it for a given function?,,"While working on my thesis my advisor handed me an unfinished paper which states the following: First , define the operators   \begin{align*} A_i &:= -\operatorname{div}(\sigma_i\nabla) \\ A_e &:= -\operatorname{div}(\sigma_e\nabla) \\ C &:= A_i + A_e \\ G &:\approx C^{-1} \\ R &:= \mathrm{Id} - CG \end{align*}   where the $\sigma$'s are tensors for an internal- and an external-""influence/action"". Second , consider the function    $$ \xi(t) := R \left[A_i u(t) - C v(t) + \epsilon^{-1}r(t) \right] \quad \text{on} \quad [0,T]\subset \mathbb{R}$$   and the differential equation   $$ \dot r (t) = \xi(t) -\epsilon^{-1}r(t) \quad \text{also on} \quad [0,T]\subset \mathbb{R}$$    where $u$, $v$, $r$, and $\xi$ are elements of the Sobolev space   $$ W^{1,2}\left(0,T,H^1(\Omega),H^1(\Omega)^*\right) := \left\{ \varphi \, \Big| \, \varphi \in L^2\left(0,T,H^1(\Omega)\right),\, \dot\varphi \in L^2\left(0,T,H^1(\Omega)^*\right) \right\}.$$ Thus , we can write   \begin{equation}  r(t) = \int_{\tau = 0} ^t \exp\left( -\tfrac{t-\tau}{\epsilon} \right) \xi (\tau)\, d\tau \quad \text{f.a.a} \quad t \in [0,T].\hspace{80pt} (1) \end{equation}   Further, using    $$          \delta_t(x) := \begin{cases} 				      + \infty & \text{if } x = t \\ 						0 			& \text{otherwise,} 	\end{cases} \quad\quad\quad \int_{-\infty}^{+\infty} \delta_t(x) \, dx = 1 $$   we can write   $$ \dot{r} (t) = \big( \xi * \left( \delta _t - \mu \right) \big) (t). $$ I can see that with $$ \mu (t) := \left\{ \begin{array}{ll} 				     \epsilon^{-1}\exp\left(-\tfrac{t}{\epsilon} \right) & \mbox{for } t \geq 0 \\ 					0   & \mbox{otherwise} 	\end{array}\right.$$ $(1)$ is a convolution $$r (t) = (\xi * \mu)(t)=\int_{-\infty}^{+\infty}\xi(\tau)\mu(t-\tau)\,d\tau$$ But, why does $r = \xi * \mu $ ? How is this calculated? or how is such a function $\mu$ constructed? Is there a standard method to express any given function as a convolution? Or a differential equation as a convolution?","While working on my thesis my advisor handed me an unfinished paper which states the following: First , define the operators   \begin{align*} A_i &:= -\operatorname{div}(\sigma_i\nabla) \\ A_e &:= -\operatorname{div}(\sigma_e\nabla) \\ C &:= A_i + A_e \\ G &:\approx C^{-1} \\ R &:= \mathrm{Id} - CG \end{align*}   where the $\sigma$'s are tensors for an internal- and an external-""influence/action"". Second , consider the function    $$ \xi(t) := R \left[A_i u(t) - C v(t) + \epsilon^{-1}r(t) \right] \quad \text{on} \quad [0,T]\subset \mathbb{R}$$   and the differential equation   $$ \dot r (t) = \xi(t) -\epsilon^{-1}r(t) \quad \text{also on} \quad [0,T]\subset \mathbb{R}$$    where $u$, $v$, $r$, and $\xi$ are elements of the Sobolev space   $$ W^{1,2}\left(0,T,H^1(\Omega),H^1(\Omega)^*\right) := \left\{ \varphi \, \Big| \, \varphi \in L^2\left(0,T,H^1(\Omega)\right),\, \dot\varphi \in L^2\left(0,T,H^1(\Omega)^*\right) \right\}.$$ Thus , we can write   \begin{equation}  r(t) = \int_{\tau = 0} ^t \exp\left( -\tfrac{t-\tau}{\epsilon} \right) \xi (\tau)\, d\tau \quad \text{f.a.a} \quad t \in [0,T].\hspace{80pt} (1) \end{equation}   Further, using    $$          \delta_t(x) := \begin{cases} 				      + \infty & \text{if } x = t \\ 						0 			& \text{otherwise,} 	\end{cases} \quad\quad\quad \int_{-\infty}^{+\infty} \delta_t(x) \, dx = 1 $$   we can write   $$ \dot{r} (t) = \big( \xi * \left( \delta _t - \mu \right) \big) (t). $$ I can see that with $$ \mu (t) := \left\{ \begin{array}{ll} 				     \epsilon^{-1}\exp\left(-\tfrac{t}{\epsilon} \right) & \mbox{for } t \geq 0 \\ 					0   & \mbox{otherwise} 	\end{array}\right.$$ $(1)$ is a convolution $$r (t) = (\xi * \mu)(t)=\int_{-\infty}^{+\infty}\xi(\tau)\mu(t-\tau)\,d\tau$$ But, why does $r = \xi * \mu $ ? How is this calculated? or how is such a function $\mu$ constructed? Is there a standard method to express any given function as a convolution? Or a differential equation as a convolution?",,"['ordinary-differential-equations', 'convolution']"
65,First-order nonlinear ODE similar to Bernoulli DE,First-order nonlinear ODE similar to Bernoulli DE,,"I know that the Bernoulli equations, i.e. equations in the form $$ y' + p(x) y + q(x) y^{\alpha}=0$$ Can be easily solved with a change of variables. But what about equations in the form $$y'  + q(x) y^{\alpha} + p(x)=0$$ Is there any easy way to solve those too?","I know that the Bernoulli equations, i.e. equations in the form $$ y' + p(x) y + q(x) y^{\alpha}=0$$ Can be easily solved with a change of variables. But what about equations in the form $$y'  + q(x) y^{\alpha} + p(x)=0$$ Is there any easy way to solve those too?",,['ordinary-differential-equations']
66,Differential Equation involving Lambert W function,Differential Equation involving Lambert W function,,"I was wondering whether there is an explicit solution to the following differential equation $$f'(x) = g'(t)\left(f(t)\left(\frac{a}{g(t)} -1 \right)-\frac{a}{g(t) \lambda}\left( 1+ W_{-1}(-e^{-1-\lambda f(t)}  \right)    \right) $$ All the functions are well-behaved, $W_{-1}$ is the -1 branch of the Lambert W function. I am not sure how to solve this one. Thanks a lot for any help!","I was wondering whether there is an explicit solution to the following differential equation $$f'(x) = g'(t)\left(f(t)\left(\frac{a}{g(t)} -1 \right)-\frac{a}{g(t) \lambda}\left( 1+ W_{-1}(-e^{-1-\lambda f(t)}  \right)    \right) $$ All the functions are well-behaved, $W_{-1}$ is the -1 branch of the Lambert W function. I am not sure how to solve this one. Thanks a lot for any help!",,"['ordinary-differential-equations', 'lambert-w']"
67,Symmetry of Green's function on the general case,Symmetry of Green's function on the general case,,"Let's consider the differential equation $$\nabla\cdot[p(\mathbf{r})\nabla u(\mathbf{r})]-s(\mathbf{r})u(\mathbf{r})=-f(\mathbf{r}).$$ I want to show that the Green's function is symmetric, so that $G(\mathbf{r}_1,\mathbf{r}_2)=G(\mathbf{r}_2,\mathbf{r}_1)$. I tried one argument similar to that used with the Helmholtz equation. In that case the equation is: $$\nabla^2\psi+k^2\psi=f,$$ In that case the Green's function satisfies $$\nabla^2G(\mathbf{r},\mathbf{r}_0)+k^2G(\mathbf{r},\mathbf{r}_0)=\delta(\mathbf{r}-\mathbf{r}_0)$$ If we then write this equation for $G(\mathbf{r},\mathbf{r}_i)$ with $i=1,2$ and then we multiply the first by $G(\mathbf{r},\mathbf{r}_2)$, multiply the second by $G(\mathbf{r},\mathbf{r}_1)$ and subtract, we get emplyoing Green's second identity that: $$G(\mathbf{r}_1,\mathbf{r}_2)-G(\mathbf{r}_2,\mathbf{r}_1)=\int_{S}[G(\mathbf{r},\mathbf{r}_2)\nabla G(\mathbf{r},\mathbf{r}_1)-G(\mathbf{r},\mathbf{r}_1)\nabla G(\mathbf{r},\mathbf{r}_2)]\cdot \operatorname{d\mathbf{S}}$$ Thus, considering Dirichlet or Neumann boundary conditions on $S$ we see that the integral vanishes so that $G(\mathbf{r}_1,\mathbf{r}_2)=G(\mathbf{r}_2,\mathbf{r}_1)$. Now, I tried to mimic this to the more general case, but the $p$ function stops us from applying Green's second identity. I've been trying this for some time, but I haven't got any far. So, how can I show the symmetry of the Green function in this case? Any help is appreciated!","Let's consider the differential equation $$\nabla\cdot[p(\mathbf{r})\nabla u(\mathbf{r})]-s(\mathbf{r})u(\mathbf{r})=-f(\mathbf{r}).$$ I want to show that the Green's function is symmetric, so that $G(\mathbf{r}_1,\mathbf{r}_2)=G(\mathbf{r}_2,\mathbf{r}_1)$. I tried one argument similar to that used with the Helmholtz equation. In that case the equation is: $$\nabla^2\psi+k^2\psi=f,$$ In that case the Green's function satisfies $$\nabla^2G(\mathbf{r},\mathbf{r}_0)+k^2G(\mathbf{r},\mathbf{r}_0)=\delta(\mathbf{r}-\mathbf{r}_0)$$ If we then write this equation for $G(\mathbf{r},\mathbf{r}_i)$ with $i=1,2$ and then we multiply the first by $G(\mathbf{r},\mathbf{r}_2)$, multiply the second by $G(\mathbf{r},\mathbf{r}_1)$ and subtract, we get emplyoing Green's second identity that: $$G(\mathbf{r}_1,\mathbf{r}_2)-G(\mathbf{r}_2,\mathbf{r}_1)=\int_{S}[G(\mathbf{r},\mathbf{r}_2)\nabla G(\mathbf{r},\mathbf{r}_1)-G(\mathbf{r},\mathbf{r}_1)\nabla G(\mathbf{r},\mathbf{r}_2)]\cdot \operatorname{d\mathbf{S}}$$ Thus, considering Dirichlet or Neumann boundary conditions on $S$ we see that the integral vanishes so that $G(\mathbf{r}_1,\mathbf{r}_2)=G(\mathbf{r}_2,\mathbf{r}_1)$. Now, I tried to mimic this to the more general case, but the $p$ function stops us from applying Green's second identity. I've been trying this for some time, but I haven't got any far. So, how can I show the symmetry of the Green function in this case? Any help is appreciated!",,"['calculus', 'ordinary-differential-equations', 'vector-analysis', 'mathematical-physics', 'greens-function']"
68,Differential equation and exact solutions,Differential equation and exact solutions,,"Given a differential equation $y'(t)=f(t,y(t))$, where f satisfies the condition $(u-v)(f(t,u)-f(t,v))\le0$ for all $u$ and $v$. If $U(t)$ and $V(t)$ are exact solutions, I want to show that $|U(t)-V(t)|\le|U(0)-V(0)|$. Can I prove it in this way: if $U(0)$ and $V(0)$ are exact solutions, then they are equal. If $U(0)$ and $V(0)$ are not exact solutions, then they are larger than $|U(t)-V(t)|$. And also how can I use this result, to prove that two numerical solutions $u_n$ and $v_n$ generated by implicit Euler satisfy $|u_n-v_n|\le|u_0-v_0|$ for all $n\ge0$.","Given a differential equation $y'(t)=f(t,y(t))$, where f satisfies the condition $(u-v)(f(t,u)-f(t,v))\le0$ for all $u$ and $v$. If $U(t)$ and $V(t)$ are exact solutions, I want to show that $|U(t)-V(t)|\le|U(0)-V(0)|$. Can I prove it in this way: if $U(0)$ and $V(0)$ are exact solutions, then they are equal. If $U(0)$ and $V(0)$ are not exact solutions, then they are larger than $|U(t)-V(t)|$. And also how can I use this result, to prove that two numerical solutions $u_n$ and $v_n$ generated by implicit Euler satisfy $|u_n-v_n|\le|u_0-v_0|$ for all $n\ge0$.",,"['ordinary-differential-equations', 'numerical-methods']"
69,Solve a system of second order differential equations,Solve a system of second order differential equations,,"I have a system of second order differential equations which is $$m_1x_1''=-k_1x_1-k_2(x_1-x_2)\\m_2x_2''=-k_2(x_2-x_1)$$ where $(x_1(0),x_1'(0),x_1(0),x_2'(0))=(1,0,2,0)$ and $(m_1,m_2,k_1,k_2)=(1,1,6,4)$. This problem is asked to solve by using computer, but I want to know to do it by hand. I have no idea how to start solving it by hand. Can someone give me a hint to start? Thanks","I have a system of second order differential equations which is $$m_1x_1''=-k_1x_1-k_2(x_1-x_2)\\m_2x_2''=-k_2(x_2-x_1)$$ where $(x_1(0),x_1'(0),x_1(0),x_2'(0))=(1,0,2,0)$ and $(m_1,m_2,k_1,k_2)=(1,1,6,4)$. This problem is asked to solve by using computer, but I want to know to do it by hand. I have no idea how to start solving it by hand. Can someone give me a hint to start? Thanks",,"['ordinary-differential-equations', 'dynamical-systems']"
70,Phase portrait of ODE in polar coordinates,Phase portrait of ODE in polar coordinates,,"Given the system of ODEs in polar coordinates, $$r' = r(1-r^2)(4-r^2)$$ $$\theta'=2-r^2,$$ one can determine its equilibrium points and limit cycles as follows: $\gamma_1:= \begin{cases} r = 0,\\ \theta = 2t\end{cases}$, $\gamma_2:= \begin{cases} r = 1,\\ \theta = t\end{cases}$, $\gamma_3:= \begin{cases} r = 2,\\ \theta = -2t\end{cases}$. $\gamma_1$ corresponds to $(0,0)$ in the $xy$-plane, and $\gamma_2$ and $\gamma_3$ correspond to circles. Now I need to sketch the phase portrait of this system and, based on this sketch, determine the stability of the equilibrium points and limit cycles. Does one need to solve this system explicitly in order to sketch the phase portrait, or is there a neater way to do it, without solving the system? I've also tried with solving for the ODE in terms of $\frac{d\theta}{dt}$, but it doesn't appear to be an equation which is easy to plot either. Also, do you think that I've found all the possible limit cycles, or maybe missed something? I'm new to this kind of analysis.","Given the system of ODEs in polar coordinates, $$r' = r(1-r^2)(4-r^2)$$ $$\theta'=2-r^2,$$ one can determine its equilibrium points and limit cycles as follows: $\gamma_1:= \begin{cases} r = 0,\\ \theta = 2t\end{cases}$, $\gamma_2:= \begin{cases} r = 1,\\ \theta = t\end{cases}$, $\gamma_3:= \begin{cases} r = 2,\\ \theta = -2t\end{cases}$. $\gamma_1$ corresponds to $(0,0)$ in the $xy$-plane, and $\gamma_2$ and $\gamma_3$ correspond to circles. Now I need to sketch the phase portrait of this system and, based on this sketch, determine the stability of the equilibrium points and limit cycles. Does one need to solve this system explicitly in order to sketch the phase portrait, or is there a neater way to do it, without solving the system? I've also tried with solving for the ODE in terms of $\frac{d\theta}{dt}$, but it doesn't appear to be an equation which is easy to plot either. Also, do you think that I've found all the possible limit cycles, or maybe missed something? I'm new to this kind of analysis.",,"['real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'polar-coordinates', 'stability-in-odes']"
71,How do we find the solution of the problem in interlaced form?,How do we find the solution of the problem in interlaced form?,,"We consider the differnetial equation $$(x-xy(x))+(y(x)+x^2)y'(x)=0$$ I have found the integrating factor $\mu (x,y)=\sqrt{x^2+y^2}$. Using this I have to find the solution of the problem in interlaced form. Could you give me some hints what I am supposed to do? I got stuck right now... Multiplying by the integrating factor we get $$\sqrt{x^2+y^2}(x-xy(x))dx+\sqrt{x^2+y^2}(y(x)+x^2)dy=0$$ or not? How could we continue? $$$$ EDIT: Is it maybe as follows? $$y'=F(y,x) \\ \Rightarrow (x-xy(x))+(y(x)+x^2)F(y,x)=0 \\ \Rightarrow y(x)(1-x)+x^2+x+x^2F(y,x)=0 \\ \Rightarrow y(x)=-\frac{x^2F(y,x)-x-x^2}{1-x}$$","We consider the differnetial equation $$(x-xy(x))+(y(x)+x^2)y'(x)=0$$ I have found the integrating factor $\mu (x,y)=\sqrt{x^2+y^2}$. Using this I have to find the solution of the problem in interlaced form. Could you give me some hints what I am supposed to do? I got stuck right now... Multiplying by the integrating factor we get $$\sqrt{x^2+y^2}(x-xy(x))dx+\sqrt{x^2+y^2}(y(x)+x^2)dy=0$$ or not? How could we continue? $$$$ EDIT: Is it maybe as follows? $$y'=F(y,x) \\ \Rightarrow (x-xy(x))+(y(x)+x^2)F(y,x)=0 \\ \Rightarrow y(x)(1-x)+x^2+x+x^2F(y,x)=0 \\ \Rightarrow y(x)=-\frac{x^2F(y,x)-x-x^2}{1-x}$$",,['ordinary-differential-equations']
72,Deriving heat equation from brownian motion,Deriving heat equation from brownian motion,,"Today my prof gave me an equation of random walk: $$p(x_i,t+\Delta t)=\frac{1}{2}(p(x_i-\Delta t)+p(x_i+\Delta t))-p(x_i,t)$$ Using this he get$$P_t=P_{xx}$$ when $\Delta t<<1$ But how and what's actually the meaning of the probability equation? e.g. one half of particles goes left and another half goes right, while nothing left in the middle. But why divided by 2 an how does this actually make sense?","Today my prof gave me an equation of random walk: $$p(x_i,t+\Delta t)=\frac{1}{2}(p(x_i-\Delta t)+p(x_i+\Delta t))-p(x_i,t)$$ Using this he get$$P_t=P_{xx}$$ when $\Delta t<<1$ But how and what's actually the meaning of the probability equation? e.g. one half of particles goes left and another half goes right, while nothing left in the middle. But why divided by 2 an how does this actually make sense?",,"['real-analysis', 'probability', 'ordinary-differential-equations', 'brownian-motion']"
73,INV problem for $ \frac{dy}{dt} = 2y+3\cos4t $ [closed],INV problem for  [closed], \frac{dy}{dt} = 2y+3\cos4t ,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Consider the differential equation $$ \frac{dy}{dt} = 2y+3\cos4t $$ For what initial values $y(0)$ = $y_0$ are the solutions bounded for all t? My approach is to calculate a general solution,which is $y = ke^{2t} - \frac{3}{5}cos4t + \frac{3}{5}sin4t$. But I don't know how to make it bounded. Please help me to solve this question. Thank you!","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Consider the differential equation $$ \frac{dy}{dt} = 2y+3\cos4t $$ For what initial values $y(0)$ = $y_0$ are the solutions bounded for all t? My approach is to calculate a general solution,which is $y = ke^{2t} - \frac{3}{5}cos4t + \frac{3}{5}sin4t$. But I don't know how to make it bounded. Please help me to solve this question. Thank you!",,['ordinary-differential-equations']
74,"x'(t)=g(t)tan(x), initial conditions solution bounded","x'(t)=g(t)tan(x), initial conditions solution bounded",,"Solve $x'(t)=g(t) \tan(x)$ and for which initial conditions (if any) are the solution bounded? Sketch the solution. I solve the differential equation: $ x(t)= \arcsin( e^{G(t)+C})$ with $ \int g(t)\, dt = G(t) + C$ If $x(0)=x_0$, then $ x_0=x(0)= \arcsin( e^{G(0)+C})$. $\iff C=\ln(\sin(x_0))-G(0)$ It becomes $ x(t)= \arcsin(e^{G(t) - G(0)} \sin(x_0))$ How should I answer the second question about boundedness of the solution? In its domain arcsin satisfies: $ - \pi/2<\arcsin(t)<\pi/2 $. For all initial value $x_0$ it is $-1 \le \sin(x_0)\le 1$. Does anyone know what I should do for this question?","Solve $x'(t)=g(t) \tan(x)$ and for which initial conditions (if any) are the solution bounded? Sketch the solution. I solve the differential equation: $ x(t)= \arcsin( e^{G(t)+C})$ with $ \int g(t)\, dt = G(t) + C$ If $x(0)=x_0$, then $ x_0=x(0)= \arcsin( e^{G(0)+C})$. $\iff C=\ln(\sin(x_0))-G(0)$ It becomes $ x(t)= \arcsin(e^{G(t) - G(0)} \sin(x_0))$ How should I answer the second question about boundedness of the solution? In its domain arcsin satisfies: $ - \pi/2<\arcsin(t)<\pi/2 $. For all initial value $x_0$ it is $-1 \le \sin(x_0)\le 1$. Does anyone know what I should do for this question?",,['ordinary-differential-equations']
75,Finding x given that $\lim_{n\to\infty } x^{x^{n-1}}=5$,Finding x given that,\lim_{n\to\infty } x^{x^{n-1}}=5,"Given a sequence $\{a_n\}^\infty_{n=1}$ defined in the following way: $ a_1=x, \ a_2=x^x, \  a_3=x^{x^{x}}, a_4=x^{x^{x^{x}}} ... \ (x>0)$ what would be the value of $x$ if  $\lim_{n\to\infty } a_n=5$","Given a sequence $\{a_n\}^\infty_{n=1}$ defined in the following way: $ a_1=x, \ a_2=x^x, \  a_3=x^{x^{x}}, a_4=x^{x^{x^{x}}} ... \ (x>0)$ what would be the value of $x$ if  $\lim_{n\to\infty } a_n=5$",,"['calculus', 'sequences-and-series', 'ordinary-differential-equations', 'limits']"
76,Waves Equation solution using Parallelogram law,Waves Equation solution using Parallelogram law,,"Given this problem : $$(1) \text{ }\text{ }\text{ }   U_{tt} -c^2U_{xx} =0$$ $0 < x < ∞, t > 0$ $$ (2)\text{ }\text{ }\text{ }U(x,0)=f(x) $$ $$(3)\text{ }\text{ }\text{ } U_{t}(x,0) = g(x) $$ $$(4)\text{ }\text{ }\text{ }U(0,t)=0 $$ Let's say I'm looking for a solution at $$ U(A) $$  how can I use the Parallelogram law to solve this? ?","Given this problem : $$(1) \text{ }\text{ }\text{ }   U_{tt} -c^2U_{xx} =0$$ $0 < x < ∞, t > 0$ $$ (2)\text{ }\text{ }\text{ }U(x,0)=f(x) $$ $$(3)\text{ }\text{ }\text{ } U_{t}(x,0) = g(x) $$ $$(4)\text{ }\text{ }\text{ }U(0,t)=0 $$ Let's say I'm looking for a solution at $$ U(A) $$  how can I use the Parallelogram law to solve this? ?",,['ordinary-differential-equations']
77,Stupid Algebra Mistake Somewhere in an Easy Bernoulli DE,Stupid Algebra Mistake Somewhere in an Easy Bernoulli DE,,"This is a straightforward DE but I know I am making some simple mistake somewhere in the middle of this problem. An extra set of educated eyes would be appreciated, thanks. Solve the differential equation $$\frac{dy}{dx} = y - y^{3}$$ $\frac{dy}{dx} - y = y^3$ substitute $u = y^{-2}$ ; $y= u^{-1/2}$; $\frac{dy}{dx}=\frac{-1}{2} u^{-3/2}\frac{du}{dx} $   yielding $\frac{-1}{2}u^{-3/2}\frac{du}{dx} -u^{-1/2} = -u^{-3/2}$ Multiply this equation by $-2u^{3/2}$ yielding $\frac{du}{dx} + 2u = 2$ This is now a simple 1st order linear equation. Setting $\mu=e^{\int2dx} = e^{2x}$ giving $2ue^{2x} = \int2e^{2x}dx$ ||$2ue^{2x}=e^{2x} $ || $2y^{-2}e^{2x}=e^{2x} $ finally finishing with $y^{2}= 2 +c$ The posted solution is $y^2 = 4 +\frac {e^{2x}}{c}$ if it helps","This is a straightforward DE but I know I am making some simple mistake somewhere in the middle of this problem. An extra set of educated eyes would be appreciated, thanks. Solve the differential equation $$\frac{dy}{dx} = y - y^{3}$$ $\frac{dy}{dx} - y = y^3$ substitute $u = y^{-2}$ ; $y= u^{-1/2}$; $\frac{dy}{dx}=\frac{-1}{2} u^{-3/2}\frac{du}{dx} $   yielding $\frac{-1}{2}u^{-3/2}\frac{du}{dx} -u^{-1/2} = -u^{-3/2}$ Multiply this equation by $-2u^{3/2}$ yielding $\frac{du}{dx} + 2u = 2$ This is now a simple 1st order linear equation. Setting $\mu=e^{\int2dx} = e^{2x}$ giving $2ue^{2x} = \int2e^{2x}dx$ ||$2ue^{2x}=e^{2x} $ || $2y^{-2}e^{2x}=e^{2x} $ finally finishing with $y^{2}= 2 +c$ The posted solution is $y^2 = 4 +\frac {e^{2x}}{c}$ if it helps",,['ordinary-differential-equations']
78,Orthogonal trajectories - why is it necessary to isolate the parameter,Orthogonal trajectories - why is it necessary to isolate the parameter,,"For orthogonal trajectory, I realized that I need to express the parameter of the given family of curves in terms of x and y, in order to get the right answer. e.g. in $y = kx$, $k$ is the parameter I was talking about in the preceding sentence above. 1) If I solve the orthogonal trajectory problem treating the parameter as if it is a constant. What will that constant mean in my final answer? 2) If I solve the problem properly (i.e. express $k$ in terms of $x$ and $y$), does $k$ changes as I move along any one of the orthogonal trajectories? Is this why I need to isolate for $k$ in the first place? Thanks for the help","For orthogonal trajectory, I realized that I need to express the parameter of the given family of curves in terms of x and y, in order to get the right answer. e.g. in $y = kx$, $k$ is the parameter I was talking about in the preceding sentence above. 1) If I solve the orthogonal trajectory problem treating the parameter as if it is a constant. What will that constant mean in my final answer? 2) If I solve the problem properly (i.e. express $k$ in terms of $x$ and $y$), does $k$ changes as I move along any one of the orthogonal trajectories? Is this why I need to isolate for $k$ in the first place? Thanks for the help",,"['calculus', 'ordinary-differential-equations']"
79,Trapezoidal rule - truncation error,Trapezoidal rule - truncation error,,"I am trying to prove that when solving numerically diff. eq.: $$ y'(t)=f(t,y(t)), \hspace{0.5cm}  y(t_{0})=y_{0} $$ using trapezoidal rule, namely: $$ y_{n+1}=y_{n} + \frac{h}{2} \left( f(t_{n},y_{n}) + f(t_{n+1},y_{n+1}) \right) $$ we get LTE (local truncation error) proportional to $C h^{3}$, $C=const$. Let's denote LTE in $n+1$-th step as: $$ e_{n+1}= \phi(t_{n+1}) - y_{n+1} $$ where $\phi(t_{n})$ represents the actual solution of our problem at $t=t_{n}$ and we assume that at the $n$-th step our approximating solution is exact with the real one ($\phi(t_{n}) = y_{n}$). I know that the most common method starts at representing $\phi(t)$ as the Taylor series about $t_{n+1}=t_{n}+h$ and using the fact that $\phi'(t_{n}) = f(t_{n},\phi(t_{n}))$. In this particular example I used two Taylor representations: $\phi(t_{n+1})=\phi(t_{n}+h)$ and $\phi(t_{n+1}-h)=\phi(t_{n})$. Combining them resulted in the formula: $$ \phi(t_{n+1})=\phi(t_{n}) + \frac{h}{2}(\phi'(t_{n})+\phi'(t_{n+1})) + \frac{h^{2}}{4}(\phi''(t_{n})-\phi''(t_{n+1})) + \frac{h^{3}}{12}(\phi'''(\xi_{n})+\phi'''(\eta_{n+1})) $$ where: $$ t_{n}<\xi_{n}<t_{n}+h $$ $$ t_{n}<\eta_{n+1}<t_{n}+h $$ So plugging this obtained formula for $\phi(t_{n+1})$ into the equation  $$ e_{n+1} = \phi(t_{n+1}) - y_{n+1} = \phi(t_{n+1}) -y_{n} - \frac{h}{2} \left( f(t_{n},y_{n}) + f(t_{n+1},y_{n+1}) \right) $$ and assuming that $\phi(t_{n})=y_{n}$ resulted in: $$ e_{n+1} = \frac{h^{2}}{4}(\phi''(t_{n})-\phi''(t_{n+1})) + \frac{h^{3}}{12}(\phi'''(\xi_{n})+\phi'''(\eta_{n+1})) $$ So it would be $e_{n+1}=O(h^{2})$, not $O(h^{3})$ (and we know that it is $O(h^{3})$). I saw in the literature that assuming correctness of approximation at $t_{n}$ means that trapezoidal rule can be represented as: $$ y_{n+1} = y_{n} + \frac{h}{2} \left( f(t_{n},\phi(t_{n})) + f(t_{n+1},\phi(t_{n+1})) \right) $$ but it doesn't make sense to me because of the correctness of the term $\phi(t_{n+1})$. I hope that I presented my problem clearly, thanks in advance for any help  with solving it!","I am trying to prove that when solving numerically diff. eq.: $$ y'(t)=f(t,y(t)), \hspace{0.5cm}  y(t_{0})=y_{0} $$ using trapezoidal rule, namely: $$ y_{n+1}=y_{n} + \frac{h}{2} \left( f(t_{n},y_{n}) + f(t_{n+1},y_{n+1}) \right) $$ we get LTE (local truncation error) proportional to $C h^{3}$, $C=const$. Let's denote LTE in $n+1$-th step as: $$ e_{n+1}= \phi(t_{n+1}) - y_{n+1} $$ where $\phi(t_{n})$ represents the actual solution of our problem at $t=t_{n}$ and we assume that at the $n$-th step our approximating solution is exact with the real one ($\phi(t_{n}) = y_{n}$). I know that the most common method starts at representing $\phi(t)$ as the Taylor series about $t_{n+1}=t_{n}+h$ and using the fact that $\phi'(t_{n}) = f(t_{n},\phi(t_{n}))$. In this particular example I used two Taylor representations: $\phi(t_{n+1})=\phi(t_{n}+h)$ and $\phi(t_{n+1}-h)=\phi(t_{n})$. Combining them resulted in the formula: $$ \phi(t_{n+1})=\phi(t_{n}) + \frac{h}{2}(\phi'(t_{n})+\phi'(t_{n+1})) + \frac{h^{2}}{4}(\phi''(t_{n})-\phi''(t_{n+1})) + \frac{h^{3}}{12}(\phi'''(\xi_{n})+\phi'''(\eta_{n+1})) $$ where: $$ t_{n}<\xi_{n}<t_{n}+h $$ $$ t_{n}<\eta_{n+1}<t_{n}+h $$ So plugging this obtained formula for $\phi(t_{n+1})$ into the equation  $$ e_{n+1} = \phi(t_{n+1}) - y_{n+1} = \phi(t_{n+1}) -y_{n} - \frac{h}{2} \left( f(t_{n},y_{n}) + f(t_{n+1},y_{n+1}) \right) $$ and assuming that $\phi(t_{n})=y_{n}$ resulted in: $$ e_{n+1} = \frac{h^{2}}{4}(\phi''(t_{n})-\phi''(t_{n+1})) + \frac{h^{3}}{12}(\phi'''(\xi_{n})+\phi'''(\eta_{n+1})) $$ So it would be $e_{n+1}=O(h^{2})$, not $O(h^{3})$ (and we know that it is $O(h^{3})$). I saw in the literature that assuming correctness of approximation at $t_{n}$ means that trapezoidal rule can be represented as: $$ y_{n+1} = y_{n} + \frac{h}{2} \left( f(t_{n},\phi(t_{n})) + f(t_{n+1},\phi(t_{n+1})) \right) $$ but it doesn't make sense to me because of the correctness of the term $\phi(t_{n+1})$. I hope that I presented my problem clearly, thanks in advance for any help  with solving it!",,"['ordinary-differential-equations', 'numerical-methods']"
80,rayleigh quotient of eigenvalue problem (sturm liouville theory and partial differential equations),rayleigh quotient of eigenvalue problem (sturm liouville theory and partial differential equations),,"I am reading ""A First Course in Partial Differential Equations with Complex Variables and Transform Methods"" (Weinberger, p. 168). if we have the eigenvalue problem $$ (pu')'- qu + \lambda \rho u = 0 $$ $$ u(0) = 0 $$ $$ p(1)u'(1) + au(1) = 0, a \ge 0 $$ we find that the eigenvalues are defined by the minimum principles $$ \lambda_k = \min_{\phi \in S_k} \frac{\int_{0}^1 (p\phi'^2 + q\phi^2) dx + a\phi(1)^2} {\int_{0}^1 \rho \phi^2 dx} $$ where $ S_k $ is the set of all continuous and piecewise continuously differentiable functions satisfying $$ \phi(0) = 0 $$ $$ \int_{0}^1 \rho \phi u_j dx = 0 $$ for $ j = 1, ..., (k-1) $ Note that the eigenvalues are arranged in increasing order. Could you please explain how the eigenvalues $ \lambda_k $ of the problem are given by minima of the aforementioned Rayleigh quotient.","I am reading ""A First Course in Partial Differential Equations with Complex Variables and Transform Methods"" (Weinberger, p. 168). if we have the eigenvalue problem $$ (pu')'- qu + \lambda \rho u = 0 $$ $$ u(0) = 0 $$ $$ p(1)u'(1) + au(1) = 0, a \ge 0 $$ we find that the eigenvalues are defined by the minimum principles $$ \lambda_k = \min_{\phi \in S_k} \frac{\int_{0}^1 (p\phi'^2 + q\phi^2) dx + a\phi(1)^2} {\int_{0}^1 \rho \phi^2 dx} $$ where $ S_k $ is the set of all continuous and piecewise continuously differentiable functions satisfying $$ \phi(0) = 0 $$ $$ \int_{0}^1 \rho \phi u_j dx = 0 $$ for $ j = 1, ..., (k-1) $ Note that the eigenvalues are arranged in increasing order. Could you please explain how the eigenvalues $ \lambda_k $ of the problem are given by minima of the aforementioned Rayleigh quotient.",,"['ordinary-differential-equations', 'partial-differential-equations', 'eigenvalues-eigenvectors', 'eigenfunctions', 'sturm-liouville']"
81,The dimension of splitting subbundles in the definition of a hyperbolic set.,The dimension of splitting subbundles in the definition of a hyperbolic set.,,"Let $f:M\rightarrow M$ be a diffeomorphism and $\Lambda$ be a hyperbolic set, where $M$ is a compact Riemannian manifold without boundary. In the definition of the hyperbolic set $\Lambda$, the tangent space over $\Lambda$ splits into two subbundles $T_{x}M=E_x^s\bigoplus E_x^u$ for all $x\in\Lambda$. Then there are conclusions that the dimensions of $E_x^s$ and $E_x^u$ are locally constant and those subspaces change conituously. Thus, is it possible that the dimension of $E_x^s$(or $E_x^u$) changes as $x\in\Lambda$ varies ? Thanks for any help.","Let $f:M\rightarrow M$ be a diffeomorphism and $\Lambda$ be a hyperbolic set, where $M$ is a compact Riemannian manifold without boundary. In the definition of the hyperbolic set $\Lambda$, the tangent space over $\Lambda$ splits into two subbundles $T_{x}M=E_x^s\bigoplus E_x^u$ for all $x\in\Lambda$. Then there are conclusions that the dimensions of $E_x^s$ and $E_x^u$ are locally constant and those subspaces change conituously. Thus, is it possible that the dimension of $E_x^s$(or $E_x^u$) changes as $x\in\Lambda$ varies ? Thanks for any help.",,"['ordinary-differential-equations', 'dynamical-systems']"
82,How to find the boundary conditions of the differential equation?,How to find the boundary conditions of the differential equation?,,"I have a very complicated differential equation that can not be solved analytically and I show a on the simple example: Example 1: $$y''(x)-y(x)=0\tag{1}$$ with boundary conditions:   $$y(1)=1,y'(2)=1\tag{2}$$  solution is: $$y(x)=\frac{e^{-x-1} \left(e^{2 x}+e^{2 x+1}-e^3+e^4\right)}{1+e^2}\tag{3}$$ then  using the substitution of      $$v(x)=\frac{y(x)}{x}$$ I have a NEW equation: $$x v''(x)+2 v'(x)-x v(x)=0\tag{4}$$ How to find the boundary conditions of the NEW  differential equation? $$v(?)=?,v'(?)=?$$ EDITED! $$v(x)= \frac{y(x)}{x}\tag{5}$$ $$v(1)=\frac{y(1)}{1}=\frac{1}{1}=1$$ $$v'(x)= \frac{y'(x)x-y(x)}{x^2}=\frac{y'(2)*2-y(x)}{2^2}= \frac{1*2-y(x)}{2^2}$$ I don't have a $y(2)=?$ Assume $y(x)$ is $y(1)=1$ then: $$v'(2)= \frac{1*2-1}{2^2}=1/4$$ The new boundary conditions are: $$v(1)=1,v'(2)=1/4\tag{6}$$Solution with New equation$(4)$ and new boundary conditions $(6)$ is: $$v(x)=\frac{e^{-x-1} \left(3 e^{2 x}+e^{2 x+1}-e^3+e^4\right)}{\left(3+e^2\right) x}$$  then we substitute to $(5)$  and check solutions is equal: $$\frac{e^{-x-1} \left(3 e^{2 x}+e^{2 x+1}-e^3+e^4\right)}{\left(3+e^2\right)}\neq\frac{e^{-x-1} \left(e^{2 x}+e^{2 x+1}-e^3+e^4\right)}{1+e^2}$$ Is NOT !! . Example 2: $$y''(x)-y(x)=0$$ with boundary conditions:   $$y'(1)=1,y'(2)=1$$ How to find the boundary conditions of the NEW differential equation?","I have a very complicated differential equation that can not be solved analytically and I show a on the simple example: Example 1: $$y''(x)-y(x)=0\tag{1}$$ with boundary conditions:   $$y(1)=1,y'(2)=1\tag{2}$$  solution is: $$y(x)=\frac{e^{-x-1} \left(e^{2 x}+e^{2 x+1}-e^3+e^4\right)}{1+e^2}\tag{3}$$ then  using the substitution of      $$v(x)=\frac{y(x)}{x}$$ I have a NEW equation: $$x v''(x)+2 v'(x)-x v(x)=0\tag{4}$$ How to find the boundary conditions of the NEW  differential equation? $$v(?)=?,v'(?)=?$$ EDITED! $$v(x)= \frac{y(x)}{x}\tag{5}$$ $$v(1)=\frac{y(1)}{1}=\frac{1}{1}=1$$ $$v'(x)= \frac{y'(x)x-y(x)}{x^2}=\frac{y'(2)*2-y(x)}{2^2}= \frac{1*2-y(x)}{2^2}$$ I don't have a $y(2)=?$ Assume $y(x)$ is $y(1)=1$ then: $$v'(2)= \frac{1*2-1}{2^2}=1/4$$ The new boundary conditions are: $$v(1)=1,v'(2)=1/4\tag{6}$$Solution with New equation$(4)$ and new boundary conditions $(6)$ is: $$v(x)=\frac{e^{-x-1} \left(3 e^{2 x}+e^{2 x+1}-e^3+e^4\right)}{\left(3+e^2\right) x}$$  then we substitute to $(5)$  and check solutions is equal: $$\frac{e^{-x-1} \left(3 e^{2 x}+e^{2 x+1}-e^3+e^4\right)}{\left(3+e^2\right)}\neq\frac{e^{-x-1} \left(e^{2 x}+e^{2 x+1}-e^3+e^4\right)}{1+e^2}$$ Is NOT !! . Example 2: $$y''(x)-y(x)=0$$ with boundary conditions:   $$y'(1)=1,y'(2)=1$$ How to find the boundary conditions of the NEW differential equation?",,['ordinary-differential-equations']
83,Finding order and degree of a differential equation,Finding order and degree of a differential equation,,"The question was Find the sum of degree and order of the given DE (differential equation)$$ \frac{d}{dx} \left(\frac{dy}{dx}\right)^3=0 $$ So we have that $$ \left(\frac{dy}{dx}\right)^3=c $$ whee c is some constant. For knowing the order and degree of a DE, it should not contain any arbitrary constant. $$ 3\left(\frac{dy}{dx}\right)^2\left(\frac{d^2y}{dx^2}\right)=0 $$ But this is not a polynomial form so degree shold not be defined. But the answer says that degree is 3 and order is 1.","The question was Find the sum of degree and order of the given DE (differential equation)$$ \frac{d}{dx} \left(\frac{dy}{dx}\right)^3=0 $$ So we have that $$ \left(\frac{dy}{dx}\right)^3=c $$ whee c is some constant. For knowing the order and degree of a DE, it should not contain any arbitrary constant. $$ 3\left(\frac{dy}{dx}\right)^2\left(\frac{d^2y}{dx^2}\right)=0 $$ But this is not a polynomial form so degree shold not be defined. But the answer says that degree is 3 and order is 1.",,"['ordinary-differential-equations', 'derivatives']"
84,Show an equilibrium $x=0$ is asymptotically stable,Show an equilibrium  is asymptotically stable,x=0,"Merry Xmas everyone! Hope you all had a great day:) I'm currently getting stuck on the following problem, due to the inability to use one of a key hypothesis given. Here is the problem: Given the system $\dot{x} = Ax + h(x)$ in $R^{n}$ where $A =  n\times n$ matrix such that all the eigenvalues of $A$ have real part $< 0$, and $h : R^n\rightarrow R^n$ satisfies $h(0) = 0$ and $ \lim_{x\rightarrow 0}\ \frac{|h(x)|}{|x|} = 0$ (this condition is equivalent to $h'(0) = 0$). Prove that the equilibrium $x = 0$ is asymptotically stable. My question: I can't see how the condition $h'(0) = 0$ comes into play. My idea is to use Variation of Parameters formula, but that formula doesn't have anything to do with $h'(0) = 0$. Can somebody please help me with the proof, or at least a new approach?","Merry Xmas everyone! Hope you all had a great day:) I'm currently getting stuck on the following problem, due to the inability to use one of a key hypothesis given. Here is the problem: Given the system $\dot{x} = Ax + h(x)$ in $R^{n}$ where $A =  n\times n$ matrix such that all the eigenvalues of $A$ have real part $< 0$, and $h : R^n\rightarrow R^n$ satisfies $h(0) = 0$ and $ \lim_{x\rightarrow 0}\ \frac{|h(x)|}{|x|} = 0$ (this condition is equivalent to $h'(0) = 0$). Prove that the equilibrium $x = 0$ is asymptotically stable. My question: I can't see how the condition $h'(0) = 0$ comes into play. My idea is to use Variation of Parameters formula, but that formula doesn't have anything to do with $h'(0) = 0$. Can somebody please help me with the proof, or at least a new approach?",,['ordinary-differential-equations']
85,Eigenvalue problem of S-L DE $y''-2xy'+\lambda y=0$.,Eigenvalue problem of S-L DE .,y''-2xy'+\lambda y=0,"Suppose we have a Sturm-Liouville differential equation $$y''-2xy'+\lambda y=0.$$ The equation has a polynomial solution in the case $\lambda =4$. So if we write the equation in self-adjoint form, we get: $$\frac{d}{dx}\bigg(e^{-x^2}\frac{dy}{dx}\bigg)+4e^{-x^2}y=0. \qquad(2)$$ This implies that the polynomial solution and $\lambda=4$ are the eigenfunction and eigenvalue of the corresponding eigenvalue problem, $$\hat{L}[y]=4\omega(x)y. \qquad(3)$$ My question is about the implication, I do not understand how from equation $(2)$ we can imply that equation $(2)$ is actually the eigenfunction of equation $(3)$. I would appreciate any help or suggestion. Thank you.","Suppose we have a Sturm-Liouville differential equation $$y''-2xy'+\lambda y=0.$$ The equation has a polynomial solution in the case $\lambda =4$. So if we write the equation in self-adjoint form, we get: $$\frac{d}{dx}\bigg(e^{-x^2}\frac{dy}{dx}\bigg)+4e^{-x^2}y=0. \qquad(2)$$ This implies that the polynomial solution and $\lambda=4$ are the eigenfunction and eigenvalue of the corresponding eigenvalue problem, $$\hat{L}[y]=4\omega(x)y. \qquad(3)$$ My question is about the implication, I do not understand how from equation $(2)$ we can imply that equation $(2)$ is actually the eigenfunction of equation $(3)$. I would appreciate any help or suggestion. Thank you.",,"['ordinary-differential-equations', 'sturm-liouville']"
86,Indicial equation of $4xy''+2y'+y=0$?,Indicial equation of ?,4xy''+2y'+y=0,"We consider $$4xy''+2y'+y=0$$ and search for a power series solution $$y=\sum_{r=0}^{\infty} a_rx^{r+m},$$ where we assume $a_0\neq 0$. By differentiating and substituting into the differential equation we get $$\sum_{r=0}^{\infty} 2a_r(r+m)(2r+2m-1)x^{r+m-1}+\sum_{r=0}^{\infty} a_rx^{r+m}=0.$$ The lowest power of $x$ is $x^m$ whose coefficient gives the indicial equation $$2m(2m-1)a_0+a_0=0.$$ But shouldn't the roots of the indicial equation be $m=0$ or $m=1/2$? I would appreciate any help or suggestion on this. Thank you.","We consider $$4xy''+2y'+y=0$$ and search for a power series solution $$y=\sum_{r=0}^{\infty} a_rx^{r+m},$$ where we assume $a_0\neq 0$. By differentiating and substituting into the differential equation we get $$\sum_{r=0}^{\infty} 2a_r(r+m)(2r+2m-1)x^{r+m-1}+\sum_{r=0}^{\infty} a_rx^{r+m}=0.$$ The lowest power of $x$ is $x^m$ whose coefficient gives the indicial equation $$2m(2m-1)a_0+a_0=0.$$ But shouldn't the roots of the indicial equation be $m=0$ or $m=1/2$? I would appreciate any help or suggestion on this. Thank you.",,['ordinary-differential-equations']
87,When does an initial value problem have exactly two solutions.,When does an initial value problem have exactly two solutions.,,My book have examples of an initial value problem which have unique solution and some which have infinite solutions.But I want to know when an initial value problem have two solutions. Is possible or not. Thanks in advance,My book have examples of an initial value problem which have unique solution and some which have infinite solutions.But I want to know when an initial value problem have two solutions. Is possible or not. Thanks in advance,,"['ordinary-differential-equations', 'initial-value-problems']"
88,What exactly is Green's Function and why can I use it to solve harmonic oscillator problems?,What exactly is Green's Function and why can I use it to solve harmonic oscillator problems?,,"My question: What exactly is Green's Function and why can I use it to solve   harmonic oscillator problems? In other words, how is Green's function   connected to physics problems like the harmonic oscillator? My question is motivated by a bonus question on my worksheet: A undamped, harmonic oscillating mass $m$ with frequency $\omega_0$ is excited by an external time dependent impulse $F(t)$: $$F(t)=  \begin{cases} 0,  & \text{if $t \le 0$} \\ f_0\sin{\Omega t}, & \text{if $0\le t \le T=\frac{2\pi}{\Omega}$}\\ 0, & \text{if $t \ge T$} \end{cases}$$ find $x(t)$ of the driven harmonic oscillation by using Green's Function. Can someone explain to me what Green's function is and how I can use it to solve this question? If some of the moderators think this question is better suited for physics stack exchange, please let me know.","My question: What exactly is Green's Function and why can I use it to solve   harmonic oscillator problems? In other words, how is Green's function   connected to physics problems like the harmonic oscillator? My question is motivated by a bonus question on my worksheet: A undamped, harmonic oscillating mass $m$ with frequency $\omega_0$ is excited by an external time dependent impulse $F(t)$: $$F(t)=  \begin{cases} 0,  & \text{if $t \le 0$} \\ f_0\sin{\Omega t}, & \text{if $0\le t \le T=\frac{2\pi}{\Omega}$}\\ 0, & \text{if $t \ge T$} \end{cases}$$ find $x(t)$ of the driven harmonic oscillation by using Green's Function. Can someone explain to me what Green's function is and how I can use it to solve this question? If some of the moderators think this question is better suited for physics stack exchange, please let me know.",,"['ordinary-differential-equations', 'physics', 'greens-function']"
89,Show that all level sets of the Hamiltonian are bounded,Show that all level sets of the Hamiltonian are bounded,,"Consider the system $$ \dot{x}=y,~~~~~~~~~~\dot{y}=x^2-x^3. $$ Its Hamiltonian is $$ H(x,y)=\frac{y^2}{2}-\frac{x^3}{3}+\frac{x^4}{4}. $$ Show that all level sets of the Hamiltonian are bounded. As far as I see, it is meant to show that the set $$ \left\{(x,y)\in\mathbb{R}^2: H(x,y)=c\right\} $$ is a bounded set. My computation gives that $H(x,y)=c\Leftrightarrow y=\sqrt{2c+\frac{2}{3}x^3-\frac{1}{2}x^4}$, i.e. $$ \left\{(x,y)\in\mathbb{R}^2: H(x,y)=c\right\}=\left\{(x,y): y=\sqrt{2c+\frac{2}{3}x^3-\frac{1}{2}x^4}\right\} $$ How can I see that this is a bounded set?","Consider the system $$ \dot{x}=y,~~~~~~~~~~\dot{y}=x^2-x^3. $$ Its Hamiltonian is $$ H(x,y)=\frac{y^2}{2}-\frac{x^3}{3}+\frac{x^4}{4}. $$ Show that all level sets of the Hamiltonian are bounded. As far as I see, it is meant to show that the set $$ \left\{(x,y)\in\mathbb{R}^2: H(x,y)=c\right\} $$ is a bounded set. My computation gives that $H(x,y)=c\Leftrightarrow y=\sqrt{2c+\frac{2}{3}x^3-\frac{1}{2}x^4}$, i.e. $$ \left\{(x,y)\in\mathbb{R}^2: H(x,y)=c\right\}=\left\{(x,y): y=\sqrt{2c+\frac{2}{3}x^3-\frac{1}{2}x^4}\right\} $$ How can I see that this is a bounded set?",,"['ordinary-differential-equations', 'hamilton-equations']"
90,Singular Perturbation Approx. for $\epsilon y'' + \frac{2 \epsilon}{t} y'-y=0$,Singular Perturbation Approx. for,\epsilon y'' + \frac{2 \epsilon}{t} y'-y=0,"Use singular perturbation techniques to find the leading order uniform approximation to the solution to the boundary value problem $$\epsilon y'' + \frac{2 \epsilon}{t} y'-y=0$$ $0<t<1$ and $y(0)=0  ,  y'(1)=1$ This has a boundary layer near $t=1$ I am having trouble figuring out how to compute the inner approximation. I am used to doing these problems with $t=0$ as the boundary layer. Using these methods, I get the outer approximation to be $y_0=0$, which I know is correct, but I'm at a loss on how to proceed with the inner approximation. I would appreciate if someone could work out the inner layer so that I could see how it is done.","Use singular perturbation techniques to find the leading order uniform approximation to the solution to the boundary value problem $$\epsilon y'' + \frac{2 \epsilon}{t} y'-y=0$$ $0<t<1$ and $y(0)=0  ,  y'(1)=1$ This has a boundary layer near $t=1$ I am having trouble figuring out how to compute the inner approximation. I am used to doing these problems with $t=0$ as the boundary layer. Using these methods, I get the outer approximation to be $y_0=0$, which I know is correct, but I'm at a loss on how to proceed with the inner approximation. I would appreciate if someone could work out the inner layer so that I could see how it is done.",,"['ordinary-differential-equations', 'boundary-value-problem', 'perturbation-theory']"
91,Laplace Transform of equation,Laplace Transform of equation,,I'm having trouble with the laplace transform: $\mathcal{L} \lbrace \sqrt{\frac{t}{\pi}}\cos(2 t) \rbrace$ The problem gives me the transform identity $\mathcal{L} \lbrace \frac{\cos(2 t)}{\sqrt{\pi t}} \rbrace = \frac{e^{-2/s}}{\sqrt{s}}$ but i'm not sure/confused as to why that would help me,I'm having trouble with the laplace transform: $\mathcal{L} \lbrace \sqrt{\frac{t}{\pi}}\cos(2 t) \rbrace$ The problem gives me the transform identity $\mathcal{L} \lbrace \frac{\cos(2 t)}{\sqrt{\pi t}} \rbrace = \frac{e^{-2/s}}{\sqrt{s}}$ but i'm not sure/confused as to why that would help me,,"['ordinary-differential-equations', 'laplace-transform']"
92,Given Square Matrices,Given Square Matrices,,Given square matrices $A$ and $B$. The matrix $B$ is the square root of matrix $A$ if $A=B^2$. Question: Under what condition is $√B$ real and unique? I'm not sure where to start with this? What does is mean under what condition is the square root real and unique?,Given square matrices $A$ and $B$. The matrix $B$ is the square root of matrix $A$ if $A=B^2$. Question: Under what condition is $√B$ real and unique? I'm not sure where to start with this? What does is mean under what condition is the square root real and unique?,,"['linear-algebra', 'ordinary-differential-equations', 'discrete-mathematics']"
93,The transformation of $\frac{\partial y}{\partial x}=\frac{Q}{P}$ to $\frac{dy}{Q}=\frac{dx}{P}$,The transformation of  to,\frac{\partial y}{\partial x}=\frac{Q}{P} \frac{dy}{Q}=\frac{dx}{P},"I'm studying a method used to solve PDE's and at one point in the argument we have the following: $$\frac{\partial y}{\partial x}=\frac{Q}{P}$$ which is then later re-written as $$\frac{dy}{Q}=\frac{dx}{P}$$ I sort of see how re-arranging fractions would make this true but I am unconvinced and something seems odd about the partials becoming ""not partial"". Can someone try to explain why this should be true?","I'm studying a method used to solve PDE's and at one point in the argument we have the following: $$\frac{\partial y}{\partial x}=\frac{Q}{P}$$ which is then later re-written as $$\frac{dy}{Q}=\frac{dx}{P}$$ I sort of see how re-arranging fractions would make this true but I am unconvinced and something seems odd about the partials becoming ""not partial"". Can someone try to explain why this should be true?",,"['ordinary-differential-equations', 'derivatives', 'partial-differential-equations', 'partial-derivative']"
94,Advice on Mathematical Modeling with Differential Equations,Advice on Mathematical Modeling with Differential Equations,,"I am on my fourth year studying in a bachelor program in applied mathematics and computer science and plan to write a term paper on mathematical modeling using differential equations. This will be the basis for my bachelor thesis. However, I am not sure where to start with. I read about some models of dynamic systems but still cannot make up my mind. What I am looking for is a model of a certain phenomenon which requires a minimum of initial knowledge in disciplines other than mathematics. It has to be something that has already been studied extensively with tangible results, yet something that contains open problems which I can think on by myself after a certain point. The term paper will most probably be an introduction of a mathematical model with a focus on the available results while the final thesis could deal with modifying the model to make it more complex and accurate. As mentioned above, the model has to deal mostly with differential equations and preferably with the use of eigenvalues in differential operators. Any kind of advice on this matter, like book recommendations, references or personal knowledge and experience would be very much appreciated.","I am on my fourth year studying in a bachelor program in applied mathematics and computer science and plan to write a term paper on mathematical modeling using differential equations. This will be the basis for my bachelor thesis. However, I am not sure where to start with. I read about some models of dynamic systems but still cannot make up my mind. What I am looking for is a model of a certain phenomenon which requires a minimum of initial knowledge in disciplines other than mathematics. It has to be something that has already been studied extensively with tangible results, yet something that contains open problems which I can think on by myself after a certain point. The term paper will most probably be an introduction of a mathematical model with a focus on the available results while the final thesis could deal with modifying the model to make it more complex and accurate. As mentioned above, the model has to deal mostly with differential equations and preferably with the use of eigenvalues in differential operators. Any kind of advice on this matter, like book recommendations, references or personal knowledge and experience would be very much appreciated.",,"['ordinary-differential-equations', 'reference-request', 'eigenvalues-eigenvectors', 'mathematical-physics', 'mathematical-modeling']"
95,"In the weak formulation of the Poisson equation, why is the boundary condition included in the integration of the weighted residual?","In the weak formulation of the Poisson equation, why is the boundary condition included in the integration of the weighted residual?",,"In the weak formulation of the Poisson equation $\nabla^2u = g$ with boundary conditions $u = \bar{u}$ on $\Gamma_e$ and $\frac{\partial{u}}{\partial{n}} = \bar{q}$ on $\Gamma_n$, why is the integration of the weighted residual expressed as  $$I = \int_\Omega w(\nabla^2u-g) d\Omega - \int_{\Gamma_e} w\frac{\partial{u}}{\partial{n}}d\Gamma$$ and not just $$I = \int_\Omega w(\nabla^2u-g) d\Omega$$","In the weak formulation of the Poisson equation $\nabla^2u = g$ with boundary conditions $u = \bar{u}$ on $\Gamma_e$ and $\frac{\partial{u}}{\partial{n}} = \bar{q}$ on $\Gamma_n$, why is the integration of the weighted residual expressed as  $$I = \int_\Omega w(\nabla^2u-g) d\Omega - \int_{\Gamma_e} w\frac{\partial{u}}{\partial{n}}d\Gamma$$ and not just $$I = \int_\Omega w(\nabla^2u-g) d\Omega$$",,"['ordinary-differential-equations', 'partial-differential-equations', 'numerical-methods', 'finite-element-method', 'poissons-equation']"
96,Explicit Runge-Kutta with Butcher Tableau not stricty positive,Explicit Runge-Kutta with Butcher Tableau not stricty positive,,"I have a hard time figuring out how explicit Runge-Kutta methods work, so this is a ""what do I misunderstand?"" kind of question. Let $S$ be a system, $Y$ its variables vector and $f(t, Y)$ the function returning the derivative of each variables at a given time and a given state of the system. All explicit Runge-Kutta methods imply to compute the value of $f$ for multiple $(t, Y)$ conditions, and then to perform a linear combination of all these derivatives in order to obtain a ""better"" derivative. The linear combination to perform is defined by a vector which can be found at the bottom of the Butcher tableau of the method. Lets call this vector $B$. My question is : as negative values can be found in some $B$ vectors (like those used by the Runge-Kutta-Fehlberg method ) it is possible that the final derivative will be negative whereas all values produced by f are positive, and vice versa . It seems that in some circumstances, the explicit Runge-Kutta methods including negative coefficients in their $B$ vector can produce irrelevant results by changing the sign of the final derivative. So how comes those methods can be used to produce meaningful results?","I have a hard time figuring out how explicit Runge-Kutta methods work, so this is a ""what do I misunderstand?"" kind of question. Let $S$ be a system, $Y$ its variables vector and $f(t, Y)$ the function returning the derivative of each variables at a given time and a given state of the system. All explicit Runge-Kutta methods imply to compute the value of $f$ for multiple $(t, Y)$ conditions, and then to perform a linear combination of all these derivatives in order to obtain a ""better"" derivative. The linear combination to perform is defined by a vector which can be found at the bottom of the Butcher tableau of the method. Lets call this vector $B$. My question is : as negative values can be found in some $B$ vectors (like those used by the Runge-Kutta-Fehlberg method ) it is possible that the final derivative will be negative whereas all values produced by f are positive, and vice versa . It seems that in some circumstances, the explicit Runge-Kutta methods including negative coefficients in their $B$ vector can produce irrelevant results by changing the sign of the final derivative. So how comes those methods can be used to produce meaningful results?",,"['ordinary-differential-equations', 'numerical-methods']"
97,"Finding curves whose tangents intersect with the x-axis at $(\frac{x}{2},0)$",Finding curves whose tangents intersect with the x-axis at,"(\frac{x}{2},0)","I have to find the family of curves in $\mathbb{R}^2$ with this property: The tangent in an arbitrary point on the curve does intersect with the x-axis in $(\frac{x}{2}, 0)$. I think I have to make a differential equation first, but how do I do that?","I have to find the family of curves in $\mathbb{R}^2$ with this property: The tangent in an arbitrary point on the curve does intersect with the x-axis in $(\frac{x}{2}, 0)$. I think I have to make a differential equation first, but how do I do that?",,"['calculus', 'ordinary-differential-equations', 'differential-geometry', 'plane-curves']"
98,Why is there only one solution to this ODE?,Why is there only one solution to this ODE?,,"Suppose $f:\mathbb{R}^2\rightarrow \mathbb{R}$ is continuous such that $f(x,y)<0$ if $xy>0$ and $f(x,y)>0$ if $xy<0$. Show that the following initial value problem has an unique solution \begin{align} y'=f(x,y),\quad y(0)=0 \end{align} One solution is obviously $y(x)\equiv 0$, since $f(x,0)=0$, $\forall x\in\mathbb{R}$ because of continuity. I have problems to argue that there can't be another solution. Usually one knows uniqueness if the RHS is locally Lipschitz continuous which is not the case here. How can one prove the uniqueness? Best wishes","Suppose $f:\mathbb{R}^2\rightarrow \mathbb{R}$ is continuous such that $f(x,y)<0$ if $xy>0$ and $f(x,y)>0$ if $xy<0$. Show that the following initial value problem has an unique solution \begin{align} y'=f(x,y),\quad y(0)=0 \end{align} One solution is obviously $y(x)\equiv 0$, since $f(x,0)=0$, $\forall x\in\mathbb{R}$ because of continuity. I have problems to argue that there can't be another solution. Usually one knows uniqueness if the RHS is locally Lipschitz continuous which is not the case here. How can one prove the uniqueness? Best wishes",,['ordinary-differential-equations']
99,Elimination of quantifiers in the strucure of polynomials and in the structure of exponentials,Elimination of quantifiers in the strucure of polynomials and in the structure of exponentials,,"I am looking at the elimination of quantifiers. In my notes there is the following: $L=\{+, ' , T, 0, 1\}$ ($""=""$ is meant to be included in $L$) First-order Logic: $Q_1 x_1 \dots Q_m x_m \ \ [\phi ]$  where $Q_1, \dots , Q_m$ are the quantifiers ($\exists , \forall$),  $\phi$ is a boolean combination of atomic formulae ( $t_1 R t_2$ (where $t_1, t_2$ are terms, $R$ is a predicate), $R(t)$ (where $R$ is a predicate) ) The terms can be built from the elements of the language. Terms are for example: $0$, $x$, $x+y$, $x'$, $(x+y)'+x'+1+1$ A sentence without quantifiers has no variables, for example $1=0$, $1'=0$. Reduction of sentences to sentences without quantifiers. Let $\mathcal{A}$ be a structure in that we interpret $L$, for example $\text{Exp}(\mathbb{C})$. Elimination of quantifiers:  For each formula $\psi$ there is a formula $\psi_0$ without quantifiers such that in the strucutre $\mathcal{A}$ the following is a tautology: $$\exists x \ \  \psi (x) \leftrightarrow \psi_0$$ i.e., $$\mathcal{A} \models \exists x \ \ \psi (x) \leftrightarrow \psi_0$$ It suffices to mention only the existential formulas since $$\neg \exists x \ \ \psi (x)\equiv \forall x \ \ \neg \psi (x) \\ \neg \forall x \ \ \psi (x)\equiv \exists  x \ \ \neg \psi (x)$$ $$L=\{+, -, ', T, 0, 1\}$$ $$\models \exists x \ \ \left (x=a \land x=b\right ) \leftrightarrow a=b \\ \exists x \ \ \left (x+y=a \land x-y=b\right ) \leftrightarrow 2y=a-b$$ A term $t$ is of the form $$t\equiv t(x)+t_0$$ that means that $t(x)$ is a term that contains $x$, that is constructed by the language $L$, and $t_0$ is constructed by the language $L$ and it doesn't contain $x$. $$$$ $$$$ Let $T(x)$ be the predicate that $x$ is non-constant. $$$$ Consider the expression $$\exists x \left (x'+x=1 \land T(x) \right ) \tag 1$$  Since the solution of the differential equation $x'+x=1$ is $x(t)=1-Ce^{-t}$, we can eliminate the quantifier at $(1)$ in the structure $\text{Exp}(\mathbb{C})$, so in this case it is $$\exists x \left (x'+x=1 \land T(x) \right ) \leftrightarrow 0=0$$ but we cannot eliminate the quantifiers in the structure of polynomials, so in this case it is $$\exists x \left (x'+x=1 \land T(x) \right ) \leftrightarrow 1=0$$ Is this correct? $$$$ Consider the expression $$\exists x \left (x'+x=t \land T(x) \right ) \tag 2$$  Since the solution of the differential equation $x'+x=t$ is $x(t)=Ce^{-t}+t-1$, we cannot eliminate the quantifier at $(2)$ neither in the structure of polynomials nor in the structure $\text{Exp}(\mathbb{C})$. So in both cases it is $$\exists x \left (x'+x=t \land T(x) \right ) \leftrightarrow 1=0$$ Is this correct?","I am looking at the elimination of quantifiers. In my notes there is the following: $L=\{+, ' , T, 0, 1\}$ ($""=""$ is meant to be included in $L$) First-order Logic: $Q_1 x_1 \dots Q_m x_m \ \ [\phi ]$  where $Q_1, \dots , Q_m$ are the quantifiers ($\exists , \forall$),  $\phi$ is a boolean combination of atomic formulae ( $t_1 R t_2$ (where $t_1, t_2$ are terms, $R$ is a predicate), $R(t)$ (where $R$ is a predicate) ) The terms can be built from the elements of the language. Terms are for example: $0$, $x$, $x+y$, $x'$, $(x+y)'+x'+1+1$ A sentence without quantifiers has no variables, for example $1=0$, $1'=0$. Reduction of sentences to sentences without quantifiers. Let $\mathcal{A}$ be a structure in that we interpret $L$, for example $\text{Exp}(\mathbb{C})$. Elimination of quantifiers:  For each formula $\psi$ there is a formula $\psi_0$ without quantifiers such that in the strucutre $\mathcal{A}$ the following is a tautology: $$\exists x \ \  \psi (x) \leftrightarrow \psi_0$$ i.e., $$\mathcal{A} \models \exists x \ \ \psi (x) \leftrightarrow \psi_0$$ It suffices to mention only the existential formulas since $$\neg \exists x \ \ \psi (x)\equiv \forall x \ \ \neg \psi (x) \\ \neg \forall x \ \ \psi (x)\equiv \exists  x \ \ \neg \psi (x)$$ $$L=\{+, -, ', T, 0, 1\}$$ $$\models \exists x \ \ \left (x=a \land x=b\right ) \leftrightarrow a=b \\ \exists x \ \ \left (x+y=a \land x-y=b\right ) \leftrightarrow 2y=a-b$$ A term $t$ is of the form $$t\equiv t(x)+t_0$$ that means that $t(x)$ is a term that contains $x$, that is constructed by the language $L$, and $t_0$ is constructed by the language $L$ and it doesn't contain $x$. $$$$ $$$$ Let $T(x)$ be the predicate that $x$ is non-constant. $$$$ Consider the expression $$\exists x \left (x'+x=1 \land T(x) \right ) \tag 1$$  Since the solution of the differential equation $x'+x=1$ is $x(t)=1-Ce^{-t}$, we can eliminate the quantifier at $(1)$ in the structure $\text{Exp}(\mathbb{C})$, so in this case it is $$\exists x \left (x'+x=1 \land T(x) \right ) \leftrightarrow 0=0$$ but we cannot eliminate the quantifiers in the structure of polynomials, so in this case it is $$\exists x \left (x'+x=1 \land T(x) \right ) \leftrightarrow 1=0$$ Is this correct? $$$$ Consider the expression $$\exists x \left (x'+x=t \land T(x) \right ) \tag 2$$  Since the solution of the differential equation $x'+x=t$ is $x(t)=Ce^{-t}+t-1$, we cannot eliminate the quantifier at $(2)$ neither in the structure of polynomials nor in the structure $\text{Exp}(\mathbb{C})$. So in both cases it is $$\exists x \left (x'+x=t \land T(x) \right ) \leftrightarrow 1=0$$ Is this correct?",,"['ordinary-differential-equations', 'logic', 'first-order-logic', 'quantifiers', 'quantifier-elimination']"
