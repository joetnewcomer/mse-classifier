,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,I need help finding the solution about $z=0$ of the following differential equation.,I need help finding the solution about  of the following differential equation.,z=0,"The differential equation is $$\frac{d^2g}{dz^2}-\frac{l(l+1)}{z(z-1)}g=0.$$ After using the Frobenius method I have gotten to indicial roots $r=1,0$. The recursion formula for $r=0$ is $$n(n+1)c_{n+1} = [n(n-1)-l(l+1)]c_n.$$ And for $r = 1$ the recursion is $$(n+1)(n+2)c_{n+1}=[k(k+1)-l(l+1)]c_n.$$ Now my problem is that I cannot obtain the first independent solution whose first few terms are $$u_0=z, u_1=z(1-z), u_2=z(z-1)(2z-1), \dots,$$ where $$g_1(z)=\sum_{n=0}^{\infty}c_n^l u_l(z).$$ I will be very grateful to anyone that can explain how to obtain the above solution.","The differential equation is $$\frac{d^2g}{dz^2}-\frac{l(l+1)}{z(z-1)}g=0.$$ After using the Frobenius method I have gotten to indicial roots $r=1,0$. The recursion formula for $r=0$ is $$n(n+1)c_{n+1} = [n(n-1)-l(l+1)]c_n.$$ And for $r = 1$ the recursion is $$(n+1)(n+2)c_{n+1}=[k(k+1)-l(l+1)]c_n.$$ Now my problem is that I cannot obtain the first independent solution whose first few terms are $$u_0=z, u_1=z(1-z), u_2=z(z-1)(2z-1), \dots,$$ where $$g_1(z)=\sum_{n=0}^{\infty}c_n^l u_l(z).$$ I will be very grateful to anyone that can explain how to obtain the above solution.",,"['ordinary-differential-equations', 'mathematical-physics', 'general-relativity']"
1,Particular integral of a fifth order linear ODE?,Particular integral of a fifth order linear ODE?,,"I am presented with the differential equation $y^{(5)}-y^{(1)}=x$ Finding the complementary function was OK. I obtained solutions to the characteristic equation of $\lambda = 0, \pm 1, \pm i$ so the complementary function is $y_{CF}=a_0+a_1e^x+a_2e^{-x}+a_3e^{i}+a_4e^{-i}$ Now for the particular integral, my first thought would be to use a trial form $y_{PI}=b_1x+b_0$ However this clearly cannot be the solution as, plugging it in, I get $-b_1=x$. Trying $b_2x^2+b_1x+b_0$ I get $-2b_2x-b_1=x$ which works for $b_1=0, b_2=-1/2$ and I suppose the constant $b_0$ doesn't really matter because it is amalgamated into the constant in the complementary function anyway. However I am finding it quite strange that the form of the particular integral was $b_2x^2+b_1x+b_0$, and not the general form for a linear forcing term of $b_1x+b_0$ that I have been taught to use. I was wondering if there is a rule particular integrals when you have different orders of ODE? I can see here that my first trial would fail because there is no $y$ term on the RHS. So I suppose this trial form would also in fact fail for a second order ODE of form $ay''+by'=x$. Do you just have to make observations like this in guessing the particular integral? I have never come across this before in class.","I am presented with the differential equation $y^{(5)}-y^{(1)}=x$ Finding the complementary function was OK. I obtained solutions to the characteristic equation of $\lambda = 0, \pm 1, \pm i$ so the complementary function is $y_{CF}=a_0+a_1e^x+a_2e^{-x}+a_3e^{i}+a_4e^{-i}$ Now for the particular integral, my first thought would be to use a trial form $y_{PI}=b_1x+b_0$ However this clearly cannot be the solution as, plugging it in, I get $-b_1=x$. Trying $b_2x^2+b_1x+b_0$ I get $-2b_2x-b_1=x$ which works for $b_1=0, b_2=-1/2$ and I suppose the constant $b_0$ doesn't really matter because it is amalgamated into the constant in the complementary function anyway. However I am finding it quite strange that the form of the particular integral was $b_2x^2+b_1x+b_0$, and not the general form for a linear forcing term of $b_1x+b_0$ that I have been taught to use. I was wondering if there is a rule particular integrals when you have different orders of ODE? I can see here that my first trial would fail because there is no $y$ term on the RHS. So I suppose this trial form would also in fact fail for a second order ODE of form $ay''+by'=x$. Do you just have to make observations like this in guessing the particular integral? I have never come across this before in class.",,"['ordinary-differential-equations', 'homogeneous-equation']"
2,How do I use the convolution theorem to solve an initial value problem?,How do I use the convolution theorem to solve an initial value problem?,,"I'm not quite sure how to use the convolution theorem for this problem. My attempt is that I took the laplace transform of both sides, however I'm confused as to what to do after. Thanks!","I'm not quite sure how to use the convolution theorem for this problem. My attempt is that I took the laplace transform of both sides, however I'm confused as to what to do after. Thanks!",,"['ordinary-differential-equations', 'laplace-transform', 'convolution', 'initial-value-problems']"
3,Transform a logistic equation into a linear ODE using substitution,Transform a logistic equation into a linear ODE using substitution,,"Considering the logistic equation: $$\frac {dP}{dt} = k(t)P\left(1- \frac{P}C\right)$$ where $k(t)$ represents a growth rate, and $C$ is carrying capacity. How would you use the substitution: $$u = \frac1P$$ to transform the logistic equation into a linear ODE?","Considering the logistic equation: $$\frac {dP}{dt} = k(t)P\left(1- \frac{P}C\right)$$ where $k(t)$ represents a growth rate, and $C$ is carrying capacity. How would you use the substitution: $$u = \frac1P$$ to transform the logistic equation into a linear ODE?",,"['calculus', 'integration', 'ordinary-differential-equations']"
4,"If differentials aren't supposed to be treated like fractions, how come we do just that in Differential Equations?","If differentials aren't supposed to be treated like fractions, how come we do just that in Differential Equations?",,"For years I've heard that differentials aren't fractions and multiplying $\frac {d y}{d x} $ with $d x$ to ""cancel out the denominator"" isn't the right view, yet I've never been shown this ""right view"" and in my ODE class we always do exactly what we were told was wrong. What exactly are the properties and proper usages of differentials? How come we're told they don't technically ""cancel out"" in fractions, yet nearly everything we've been exposed to counters that?","For years I've heard that differentials aren't fractions and multiplying $\frac {d y}{d x} $ with $d x$ to ""cancel out the denominator"" isn't the right view, yet I've never been shown this ""right view"" and in my ODE class we always do exactly what we were told was wrong. What exactly are the properties and proper usages of differentials? How come we're told they don't technically ""cancel out"" in fractions, yet nearly everything we've been exposed to counters that?",,"['calculus', 'ordinary-differential-equations']"
5,Ignoring absolute values in differential equation,Ignoring absolute values in differential equation,,Consider the differential equation $\frac{dy}{dx} = \frac{3y - 1}{x}$. It is separable and we get $\ln|3y - 1| = C + \ln(|x|^3)$. On wolfram the solution is reported as $y(x) = c_1 x^3 + \frac{1}{3}$ but wouldn't this ignore solutions like $y(x) = \frac{|x|^3 + 1}{3}$?,Consider the differential equation $\frac{dy}{dx} = \frac{3y - 1}{x}$. It is separable and we get $\ln|3y - 1| = C + \ln(|x|^3)$. On wolfram the solution is reported as $y(x) = c_1 x^3 + \frac{1}{3}$ but wouldn't this ignore solutions like $y(x) = \frac{|x|^3 + 1}{3}$?,,"['integration', 'ordinary-differential-equations']"
6,Peano existence theorem on manifolds,Peano existence theorem on manifolds,,"Does the Peano existence theorem apply to continuous vector fields on smooth manifolds? For locally Lipschitz vector fields on smooth manifolds, the Picard-Lindelöf theorem can be applied in local charts to obtain local solutions, and then the uniqueness portion of this theorem guarantees that the local solutions agree where they overlap and thus can patch together to form global solutions. The Peano theorem guarantees only existence and not uniqueness. Thus I think there is no guarantee that local solutions agree where they overlap, and thus no guarantee that they can patch together to form global solutions. Is it possible to ""choose"" particular local solutions in charts so that they do agree where they overlap, and thus patch together to form global solutions? Edit: As John B pointed out with his answer, my original question wasn't worded very well since the Peano theorem is a local statement. Here is a modified question which captures the essence of what I want to know. Suppose we have a continuous vector field on a compact manifold. Is this vector field complete, in the sense that any integral curve has maximal interval of existence equal to all of $\mathbb{R}$?","Does the Peano existence theorem apply to continuous vector fields on smooth manifolds? For locally Lipschitz vector fields on smooth manifolds, the Picard-Lindelöf theorem can be applied in local charts to obtain local solutions, and then the uniqueness portion of this theorem guarantees that the local solutions agree where they overlap and thus can patch together to form global solutions. The Peano theorem guarantees only existence and not uniqueness. Thus I think there is no guarantee that local solutions agree where they overlap, and thus no guarantee that they can patch together to form global solutions. Is it possible to ""choose"" particular local solutions in charts so that they do agree where they overlap, and thus patch together to form global solutions? Edit: As John B pointed out with his answer, my original question wasn't worded very well since the Peano theorem is a local statement. Here is a modified question which captures the essence of what I want to know. Suppose we have a continuous vector field on a compact manifold. Is this vector field complete, in the sense that any integral curve has maximal interval of existence equal to all of $\mathbb{R}$?",,"['ordinary-differential-equations', 'reference-request', 'differential-topology']"
7,Equation for the distance traveled by a falling object,Equation for the distance traveled by a falling object,,"The velocity of a falling object (or the derivative with respect to time of the distance traveled by the falling object) is given by $$ \dot{s} = \frac{rm}{k}\left(\frac{e^{rt}-e^{-rt}}{e^{rt}+e^{-rt}}\right), $$ where $r = \sqrt{gk/m}$. Here, $g$ is acceleration due to gravity, $m$ is the mass of the object, and $k$ is a constant that captures air resistance. I need to integrate this to obtain an exact formula for $s$.  Can anyone give me some guidance for how to do this?  I'm not sure what strategy is appropriate. The answer should come out to be $$ s(t) = \frac{V^2}{g}\ln\left(\cosh\frac{gt}{V}\right), $$ where $V=\sqrt{\frac{mg}{k}}$ is the terminal velocity.","The velocity of a falling object (or the derivative with respect to time of the distance traveled by the falling object) is given by $$ \dot{s} = \frac{rm}{k}\left(\frac{e^{rt}-e^{-rt}}{e^{rt}+e^{-rt}}\right), $$ where $r = \sqrt{gk/m}$. Here, $g$ is acceleration due to gravity, $m$ is the mass of the object, and $k$ is a constant that captures air resistance. I need to integrate this to obtain an exact formula for $s$.  Can anyone give me some guidance for how to do this?  I'm not sure what strategy is appropriate. The answer should come out to be $$ s(t) = \frac{V^2}{g}\ln\left(\cosh\frac{gt}{V}\right), $$ where $V=\sqrt{\frac{mg}{k}}$ is the terminal velocity.",,"['integration', 'ordinary-differential-equations', 'derivatives', 'dynamical-systems']"
8,Differential equation of the form $y^2y'' = a$,Differential equation of the form,y^2y'' = a,"Can anyone help me in solving the differential equation of the form $$y^2y'' = a$$ where $y$ is a function of $x$ and $a$ is a constant. I am new to solving differential equations and just out of curiosity I tried solving this, but I couldn't. It has been keeping me busy all the time and I wanted a solution to it. Can anyone please help me?","Can anyone help me in solving the differential equation of the form $$y^2y'' = a$$ where $y$ is a function of $x$ and $a$ is a constant. I am new to solving differential equations and just out of curiosity I tried solving this, but I couldn't. It has been keeping me busy all the time and I wanted a solution to it. Can anyone please help me?",,['ordinary-differential-equations']
9,Heat Equation Mixed Boundaries Case: Fourier Coefficients,Heat Equation Mixed Boundaries Case: Fourier Coefficients,,"I was working on the heat equation: $$\mu_t= \alpha \mu_{xx}$$ $$\mu(0,t)=0$$ $$\mu_x(L,t)=0$$ $$\mu(x,0)=f(x)$$ I arrived at the following solution: $$\mu=\sum_{m=1}^{\infty }b_me^{\frac{-\alpha ^2(2m-1)^2\pi^2}{4L^2}t}\sin(\frac{(2m-1)\pi x}{2L})$$ So $\mu(x,0)=f(x)=\sum_{m=1}^{\infty }b_m\sin(\frac{(2m-1)\pi x}{2L})$ I am now stuck since I have no clue how to extend f(x) to a sine fourier series with new L = 2*L. So how do I calculate $b_m$ ? Thanks!","I was working on the heat equation: $$\mu_t= \alpha \mu_{xx}$$ $$\mu(0,t)=0$$ $$\mu_x(L,t)=0$$ $$\mu(x,0)=f(x)$$ I arrived at the following solution: $$\mu=\sum_{m=1}^{\infty }b_me^{\frac{-\alpha ^2(2m-1)^2\pi^2}{4L^2}t}\sin(\frac{(2m-1)\pi x}{2L})$$ So $\mu(x,0)=f(x)=\sum_{m=1}^{\infty }b_m\sin(\frac{(2m-1)\pi x}{2L})$ I am now stuck since I have no clue how to extend f(x) to a sine fourier series with new L = 2*L. So how do I calculate $b_m$ ? Thanks!",,"['ordinary-differential-equations', 'fourier-series']"
10,Prove equality concerning differentiable function,Prove equality concerning differentiable function,,"Let $f(x)$ be a function that is n-times differentiable. Prove the following equality( without L'Hospital): $$(x^{n-1}\cdot f(\frac{1}{x}) )^{(n)}=\frac{(-1)^{n}}{x^{n+1}}\cdot f^{(n)}(\frac{1}{x})\\$$ So, I wanted to prove it by induction, for $n=0$ the statement is obviously true. So let's assume it's true for some $n \in \mathbb{N_0}$ $\\$ Now we need to check for $n+1$ but I'm not sure how to proceed. $$(x^{n}\cdot f(\frac{1}{x}) )^{(n+1)}$$ I thought about taking the first derivative of the product, then separately observe the n-th derivative of the two expressions but haven't reached any useful conclusions. Any kind of hint would be appreciated, thanks in advance!","Let $f(x)$ be a function that is n-times differentiable. Prove the following equality( without L'Hospital): $$(x^{n-1}\cdot f(\frac{1}{x}) )^{(n)}=\frac{(-1)^{n}}{x^{n+1}}\cdot f^{(n)}(\frac{1}{x})\\$$ So, I wanted to prove it by induction, for $n=0$ the statement is obviously true. So let's assume it's true for some $n \in \mathbb{N_0}$ $\\$ Now we need to check for $n+1$ but I'm not sure how to proceed. $$(x^{n}\cdot f(\frac{1}{x}) )^{(n+1)}$$ I thought about taking the first derivative of the product, then separately observe the n-th derivative of the two expressions but haven't reached any useful conclusions. Any kind of hint would be appreciated, thanks in advance!",,"['calculus', 'ordinary-differential-equations']"
11,Nonhomogeneous heat equation solution of $u_t - \Delta u = \rm div f$,Nonhomogeneous heat equation solution of,u_t - \Delta u = \rm div f,"For 3D, I have some doubts of the nonhomogeneous heat equation with initial value zero. i.e if $u_t - \Delta u = f$, $$u(x,t)=c\int_{0}^{t} \int_{\mathbb R^3} \frac{1}{(t-s)^{3/2}} e^{\frac{-|x-y|^2}{4(t-s)}}f(y,s)dyds$$. Now if I change $f$ to $\rm div f$, here I mean the divergence. Then, what will $u$ be? Thanks.","For 3D, I have some doubts of the nonhomogeneous heat equation with initial value zero. i.e if $u_t - \Delta u = f$, $$u(x,t)=c\int_{0}^{t} \int_{\mathbb R^3} \frac{1}{(t-s)^{3/2}} e^{\frac{-|x-y|^2}{4(t-s)}}f(y,s)dyds$$. Now if I change $f$ to $\rm div f$, here I mean the divergence. Then, what will $u$ be? Thanks.",,"['real-analysis', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'heat-equation']"
12,First Order ODE with Delta through Integrating factors,First Order ODE with Delta through Integrating factors,,"I'm trying to solve a differential equation similar to the one shown as a first solution here: Inverse Fourier transform of $ \frac{1}{a+jw} $ The DE is the following: $y' + 2y = \delta(x)$ I'm trying to solve without using Laplace but got stuck here: $y=e^{-2x}\int^{}e^{2x}\delta(x)dx$ There should be some way to prove that $\int^{}e^{2x}\delta(x) = H(x)$ but I just can't see it. As far as I know $\int^{}e^{2x}\delta(x) = 1$, so there should be a way to shift the delta inside the integral as $\delta(x-a)$ when $a \neq 0$ so $H(x)$ makes sense.","I'm trying to solve a differential equation similar to the one shown as a first solution here: Inverse Fourier transform of $ \frac{1}{a+jw} $ The DE is the following: $y' + 2y = \delta(x)$ I'm trying to solve without using Laplace but got stuck here: $y=e^{-2x}\int^{}e^{2x}\delta(x)dx$ There should be some way to prove that $\int^{}e^{2x}\delta(x) = H(x)$ but I just can't see it. As far as I know $\int^{}e^{2x}\delta(x) = 1$, so there should be a way to shift the delta inside the integral as $\delta(x-a)$ when $a \neq 0$ so $H(x)$ makes sense.",,"['integration', 'ordinary-differential-equations', 'dirac-delta']"
13,Why we cannot apply Componendo and Dividendo in this question,Why we cannot apply Componendo and Dividendo in this question,,Please see the step enclosed in yellow ring in the above image. My question is why we cannot apply Componendo-Dividendo so the integration becomes easy. The correct answer to the question is (x - y)^2 = Cxy × e^(-y/x) but I do not get this answer by applying Componend-Dividendo. Why am I not getting correct answer? Please help. Here is what I did :,Please see the step enclosed in yellow ring in the above image. My question is why we cannot apply Componendo-Dividendo so the integration becomes easy. The correct answer to the question is (x - y)^2 = Cxy × e^(-y/x) but I do not get this answer by applying Componend-Dividendo. Why am I not getting correct answer? Please help. Here is what I did :,,"['ordinary-differential-equations', 'ratio']"
14,Reduction of order for third order ode,Reduction of order for third order ode,,"This question is similar to this question, however I do not understand the method I should be using. I need to solve $$x^3y'''+7x^2y''+xy'-16y =0 \,\,\,\,\,\,\, \text{knowing that} \,\,\,\,\,\ y_1 = x^2$$ Okay so normally for second order ODE I simply use the formula $$y_2  =y_1\int \frac{e^{-\int p(x)dx}}{y_1^2}dx$$ where $p(x)$ comes from $$y''+p(x)y'+q(x)y = 0$$ I know the formula comes from the intuition that the second solution has the following relation with the first one: $y_2 = v(x)y_1$ where $v(x)$ is the function to be found. However how do I find a second solution for the equation above? Edit : Trial Trying $y_2 = vy_1$ I get $y_2' = v'y_1+vy_1'$ and $y_2''=v''y_1+v'y_1'+v'y_1'+vy_1'' = v''y_1+2v'y_1'+vy_1''$ and also $y_2''' = v'''y_1+v''y_1'+2(v''y_1'+v'y_1'')+v'y_1'''+v''y_1''=v'''y_1+v''(3y_1'+y_1'')+v'(2y_1''+y_1''')$ but they are all positive and if I sub them into the main equation I will just get a very big amount of terms. Indeed we would have: $$x^3y_2'''+7x^2y_2''+xy_2'-16y_2 =3(v'''y_1+v''(3y_1'+y_1'')+v'(2y_1''+y_1'''))+7x^2(v''y_1+2v'y_1'+vy_1'')+x(v'y_1+vy_1')-16(vy_1)$$","This question is similar to this question, however I do not understand the method I should be using. I need to solve Okay so normally for second order ODE I simply use the formula where comes from I know the formula comes from the intuition that the second solution has the following relation with the first one: where is the function to be found. However how do I find a second solution for the equation above? Edit : Trial Trying I get and and also but they are all positive and if I sub them into the main equation I will just get a very big amount of terms. Indeed we would have:","x^3y'''+7x^2y''+xy'-16y =0 \,\,\,\,\,\,\, \text{knowing that} \,\,\,\,\,\ y_1 = x^2 y_2  =y_1\int \frac{e^{-\int p(x)dx}}{y_1^2}dx p(x) y''+p(x)y'+q(x)y = 0 y_2 = v(x)y_1 v(x) y_2 = vy_1 y_2' = v'y_1+vy_1' y_2''=v''y_1+v'y_1'+v'y_1'+vy_1'' = v''y_1+2v'y_1'+vy_1'' y_2''' = v'''y_1+v''y_1'+2(v''y_1'+v'y_1'')+v'y_1'''+v''y_1''=v'''y_1+v''(3y_1'+y_1'')+v'(2y_1''+y_1''') x^3y_2'''+7x^2y_2''+xy_2'-16y_2 =3(v'''y_1+v''(3y_1'+y_1'')+v'(2y_1''+y_1'''))+7x^2(v''y_1+2v'y_1'+vy_1'')+x(v'y_1+vy_1')-16(vy_1)","['ordinary-differential-equations', 'reduction-of-order-ode']"
15,Freefall motion with air resistance ordinary differential equation,Freefall motion with air resistance ordinary differential equation,,"I'm stuck in finding the general solution for this DE because I'm not sure what technique I should use: $$\frac{dv}{dx}=\frac{g}{v}-\frac{k}{m}$$ Where g, k and m are constants. I'm not sure if I can really use substitution method here. I've also tried long division but I always end up having a higher degree of the numerator. Partial fractions also doesn't seem to work as the denominator is not factorable when considered as a separable equation.","I'm stuck in finding the general solution for this DE because I'm not sure what technique I should use: $$\frac{dv}{dx}=\frac{g}{v}-\frac{k}{m}$$ Where g, k and m are constants. I'm not sure if I can really use substitution method here. I've also tried long division but I always end up having a higher degree of the numerator. Partial fractions also doesn't seem to work as the denominator is not factorable when considered as a separable equation.",,['ordinary-differential-equations']
16,Find the general solution of $(x + \sin{x} +\sin{y})\ \textrm{d}{x} + \cos{y}\ \textrm{d}{y}=0$,Find the general solution of,(x + \sin{x} +\sin{y})\ \textrm{d}{x} + \cos{y}\ \textrm{d}{y}=0,"I have tried to make this equation seperable, homogeneous, exact and use integrating factors and nothing seems to be working. Could you tell me what form to use?","I have tried to make this equation seperable, homogeneous, exact and use integrating factors and nothing seems to be working. Could you tell me what form to use?",,['ordinary-differential-equations']
17,Trouble point in differential equations,Trouble point in differential equations,,"I have the following differential equation (which is a simple one...but I am new to it). $$(x^2+2xy)y'=2(xy+y^2)$$ where $y(1)=2$ $$y'=\dfrac{2xy+2y^2}{2xy+x^2}=\dfrac{2 \left(\dfrac{y}{x}+\dfrac{y^2}{x^2}\right)}{2\dfrac{y}{x}+1}=\dfrac{2u+2u^2}{2u+1}$$ $$y=ux$$ $$u+xu'=\dfrac{2u+2u^2}{2u+1}$$ $$u'=\dfrac{1}{x}\left(\dfrac{2u+2u^2}{2u+1}-u\right)=\dfrac{1}{x}\dfrac{u}{2u+1}$$ Finally we get, separating variables  $$2u+\ln |u| = \ln c|x|$$, where $c>0$ How am I supposed to find $u(x)$ from this equation? Finding $u$ would have my problem solved. Thank you!","I have the following differential equation (which is a simple one...but I am new to it). $$(x^2+2xy)y'=2(xy+y^2)$$ where $y(1)=2$ $$y'=\dfrac{2xy+2y^2}{2xy+x^2}=\dfrac{2 \left(\dfrac{y}{x}+\dfrac{y^2}{x^2}\right)}{2\dfrac{y}{x}+1}=\dfrac{2u+2u^2}{2u+1}$$ $$y=ux$$ $$u+xu'=\dfrac{2u+2u^2}{2u+1}$$ $$u'=\dfrac{1}{x}\left(\dfrac{2u+2u^2}{2u+1}-u\right)=\dfrac{1}{x}\dfrac{u}{2u+1}$$ Finally we get, separating variables  $$2u+\ln |u| = \ln c|x|$$, where $c>0$ How am I supposed to find $u(x)$ from this equation? Finding $u$ would have my problem solved. Thank you!",,"['ordinary-differential-equations', 'derivatives', 'logarithms', 'absolute-value', 'homogeneous-equation']"
18,How to apply Calculus of Variations to this problem?,How to apply Calculus of Variations to this problem?,,"I have some doubts on how to apply the calculus of variations to find a parametric curve which minimizes a certain functional. Let $\gamma: [0,1]\longrightarrow\mathbb{R}^2$ be a curve, and let $\gamma(t)=(x(t),y(t))$. Let \begin{equation} F(\gamma) :=\int_0^1\alpha x(t)+\beta y(t)\  \text{dt} \end{equation} I'd like to find a curve $\gamma$ that minimizes this functional, for some specific values $\alpha,\beta$. The problem is that the functional depends separately on the components of $\gamma$, and i'm unsure if (and eventually how) I can apply the calculus of variations to this case.","I have some doubts on how to apply the calculus of variations to find a parametric curve which minimizes a certain functional. Let $\gamma: [0,1]\longrightarrow\mathbb{R}^2$ be a curve, and let $\gamma(t)=(x(t),y(t))$. Let \begin{equation} F(\gamma) :=\int_0^1\alpha x(t)+\beta y(t)\  \text{dt} \end{equation} I'd like to find a curve $\gamma$ that minimizes this functional, for some specific values $\alpha,\beta$. The problem is that the functional depends separately on the components of $\gamma$, and i'm unsure if (and eventually how) I can apply the calculus of variations to this case.",,"['ordinary-differential-equations', 'calculus-of-variations']"
19,Simple Homogeneous ODE,Simple Homogeneous ODE,,$$y'-\frac{1-2x}{y}=1$$ If we look at the homogeneous part can we use $y_{h}=e^{-\int p(x)dx}$ or because it is in the form of $y'+p(x)y^{-1}=0$ we can not?,$$y'-\frac{1-2x}{y}=1$$ If we look at the homogeneous part can we use $y_{h}=e^{-\int p(x)dx}$ or because it is in the form of $y'+p(x)y^{-1}=0$ we can not?,,['ordinary-differential-equations']
20,Upward projectile motion differential equations,Upward projectile motion differential equations,,"So I have this question here which says: ""An object of mass m is thrown vertically upward from the surface of the earth. It is subject to a constant gravitational field and air resistance proportional to the square of the object's velocity. If $g > 0$ is the constant acceleration due to gravity and $b > 0$ is the coefficient of quadratic air resistance, then Newton's second law gives $\frac{dv}{dt}=-g-\frac{b}{m}v^2$; where v(t) = $\frac{dz}{dt}$ is the object's velocity as a function of time $t=  0$. (Here we assume that when the object is moving upward, its velocity is positive). Solve the differential equation to find $v(t)$ and the height $z(t)$ of the object as it travels upward. Find the maximum height of the object, if its initial velocity is $v_0 > 0$."" This is obviously separable and I get $\frac{dv}{-g-\frac{b}{m}v^2}=dt$ Which I feel I did correctly. I should then be able to integrate and then solve... If I integrate, I get... $\int_{v_o}^v \frac{dv}{-g-\frac{b}{m}v^2}$ = $\int_{0}^t dt$ I assume my bounds are correct. If I try to integrate tough, I get a really nasty substitution problem and end up with arc tan so I'm a little skeptical of if that's the right process... Any guidance?","So I have this question here which says: ""An object of mass m is thrown vertically upward from the surface of the earth. It is subject to a constant gravitational field and air resistance proportional to the square of the object's velocity. If $g > 0$ is the constant acceleration due to gravity and $b > 0$ is the coefficient of quadratic air resistance, then Newton's second law gives $\frac{dv}{dt}=-g-\frac{b}{m}v^2$; where v(t) = $\frac{dz}{dt}$ is the object's velocity as a function of time $t=  0$. (Here we assume that when the object is moving upward, its velocity is positive). Solve the differential equation to find $v(t)$ and the height $z(t)$ of the object as it travels upward. Find the maximum height of the object, if its initial velocity is $v_0 > 0$."" This is obviously separable and I get $\frac{dv}{-g-\frac{b}{m}v^2}=dt$ Which I feel I did correctly. I should then be able to integrate and then solve... If I integrate, I get... $\int_{v_o}^v \frac{dv}{-g-\frac{b}{m}v^2}$ = $\int_{0}^t dt$ I assume my bounds are correct. If I try to integrate tough, I get a really nasty substitution problem and end up with arc tan so I'm a little skeptical of if that's the right process... Any guidance?",,"['ordinary-differential-equations', 'projectile-motion']"
21,"What is the solution to $xy''+2y'+xy=0$ where $y(0),y(\infty)<\infty$",What is the solution to  where,"xy''+2y'+xy=0 y(0),y(\infty)<\infty","I was hoping someone might be able to explain to me how I find the family of solutions to the following: $$xy''+2y'+xy=0$$ where $y=y(x)$ and $y$ is bounded at both $0$ and infinity. I have been attempting to recast it as $$(xy)''+xy = 0$$ and transforming it via $z=xy$, however my solutions aren't making any sense! It's been about 5 years since I've worked with an equation like this so I apologize for the triviality of this problem. My workings: If one makes the transformation $z=xy$ we get $$z''+z=0$$ with $z(0)=0\times y(0)=0$, which gives $$z=A\sin(x)+B\cos(x)$$ $B=0$ by $z(0)=0$. At this point I was expecting to find a way to create an infinite family of solutions, I can if I restrict x to a finite domain, however I wish to solve over an infinite one.","I was hoping someone might be able to explain to me how I find the family of solutions to the following: $$xy''+2y'+xy=0$$ where $y=y(x)$ and $y$ is bounded at both $0$ and infinity. I have been attempting to recast it as $$(xy)''+xy = 0$$ and transforming it via $z=xy$, however my solutions aren't making any sense! It's been about 5 years since I've worked with an equation like this so I apologize for the triviality of this problem. My workings: If one makes the transformation $z=xy$ we get $$z''+z=0$$ with $z(0)=0\times y(0)=0$, which gives $$z=A\sin(x)+B\cos(x)$$ $B=0$ by $z(0)=0$. At this point I was expecting to find a way to create an infinite family of solutions, I can if I restrict x to a finite domain, however I wish to solve over an infinite one.",,"['ordinary-differential-equations', 'sturm-liouville', 'nonlinear-analysis']"
22,$F$ can be written as a sum iff differential equation is solvable,can be written as a sum iff differential equation is solvable,F,"Let $U \subseteq \mathbb R^3$ be a star shaped and $F: U \to \mathbb R^3$ a continuably differentiable vector field. How can I now show that $F$ can be written as a sum $$F=G+H$$ where $$rot \, G=0 \, , \, div \, H =0$$ if and only if the differential equation $\Delta u=div \, F$ has a solution $u:U \to \mathbb R$. A little help or a hint is much appreciated.","Let $U \subseteq \mathbb R^3$ be a star shaped and $F: U \to \mathbb R^3$ a continuably differentiable vector field. How can I now show that $F$ can be written as a sum $$F=G+H$$ where $$rot \, G=0 \, , \, div \, H =0$$ if and only if the differential equation $\Delta u=div \, F$ has a solution $u:U \to \mathbb R$. A little help or a hint is much appreciated.",,['real-analysis']
23,Help Understanding Piecewise Defined Laplace Transformations,Help Understanding Piecewise Defined Laplace Transformations,,"I understand that I need to convert to Heaviside functions and I think I'm doing that correctly, however my answer is not what it's supposed to be. Below is one of the problems I am working on, with my workings and interpretation of what the  graph should look like. Could someone please explain how I am meant to get my answer to look like that.","I understand that I need to convert to Heaviside functions and I think I'm doing that correctly, however my answer is not what it's supposed to be. Below is one of the problems I am working on, with my workings and interpretation of what the  graph should look like. Could someone please explain how I am meant to get my answer to look like that.",,"['ordinary-differential-equations', 'laplace-transform']"
24,solving inhomogenous $2$ x $2$ ODE?,solving inhomogenous  x  ODE?,2 2,"Let $y'=\begin{pmatrix}1 & 2 \\ 3 & 6\end{pmatrix}y+\begin{pmatrix}x \\ sin(x)\end{pmatrix}$ with $y_0=\begin{pmatrix}0 \\ 0\end{pmatrix}$ Now I want to solve it, but don't know how to continue: $1)$ solve the homogeous ODE $y'=Ay$ $\Rightarrow y_{hom}(x)=e^{0x} \cdot c_1\begin{pmatrix}-2 \\ 1\end{pmatrix}+c_2 \cdot e^{7x}\begin{pmatrix}1 \\ 3\end{pmatrix}=c_1\begin{pmatrix}-2 \\ 1\end{pmatrix}+c_2 \cdot e^{7x}\begin{pmatrix}1 \\ 3\end{pmatrix}$ $2)$ solve particular ODE $y'=Ay+b$ with variation of parameters: $y_{par}=Y_{hom}\cdot c(x)=\frac{1}{7}\begin{pmatrix}e^{7x}+6 & 3(e^{7x}-1)\\2(e^{7x}-1) & 6e^{7x}+1\end{pmatrix}\begin{pmatrix}\dot c_1(x) \\ \dot c_2(x)\end{pmatrix}=\begin{pmatrix}\ x \\ \ sin(x)\end{pmatrix}$ Now I solved for $c_1$ and $c_2$ , but I guess it's wrong because I couldn't solve it without calculator and than I checked it , it was wrong :/ $c_1=\dfrac{\mathrm{e}^{-7x}\left(-1029\sin\left(x\right)+\mathrm{e}^{7x}\left(7350\cos\left(x\right)+1225x^2\right)-147\cos\left(x\right)+\left(2100x-300\right)\mathrm{e}^{14x}\right)}{17150}+C$ $c_2=\dfrac{\mathrm{e}^{-7x}\left(1029\sin\left(x\right)+\mathrm{e}^{7x}\left(1225\cos\left(x\right)-1225x^2\right)+147\cos\left(x\right)+\left(350x-50\right)\mathrm{e}^{14x}\right)}{8575}+C$ Is this way correct or is there another one ? Thanks in advance","Let with Now I want to solve it, but don't know how to continue: solve the homogeous ODE solve particular ODE with variation of parameters: Now I solved for and , but I guess it's wrong because I couldn't solve it without calculator and than I checked it , it was wrong :/ Is this way correct or is there another one ? Thanks in advance",y'=\begin{pmatrix}1 & 2 \\ 3 & 6\end{pmatrix}y+\begin{pmatrix}x \\ sin(x)\end{pmatrix} y_0=\begin{pmatrix}0 \\ 0\end{pmatrix} 1) y'=Ay \Rightarrow y_{hom}(x)=e^{0x} \cdot c_1\begin{pmatrix}-2 \\ 1\end{pmatrix}+c_2 \cdot e^{7x}\begin{pmatrix}1 \\ 3\end{pmatrix}=c_1\begin{pmatrix}-2 \\ 1\end{pmatrix}+c_2 \cdot e^{7x}\begin{pmatrix}1 \\ 3\end{pmatrix} 2) y'=Ay+b y_{par}=Y_{hom}\cdot c(x)=\frac{1}{7}\begin{pmatrix}e^{7x}+6 & 3(e^{7x}-1)\\2(e^{7x}-1) & 6e^{7x}+1\end{pmatrix}\begin{pmatrix}\dot c_1(x) \\ \dot c_2(x)\end{pmatrix}=\begin{pmatrix}\ x \\ \ sin(x)\end{pmatrix} c_1 c_2 c_1=\dfrac{\mathrm{e}^{-7x}\left(-1029\sin\left(x\right)+\mathrm{e}^{7x}\left(7350\cos\left(x\right)+1225x^2\right)-147\cos\left(x\right)+\left(2100x-300\right)\mathrm{e}^{14x}\right)}{17150}+C c_2=\dfrac{\mathrm{e}^{-7x}\left(1029\sin\left(x\right)+\mathrm{e}^{7x}\left(1225\cos\left(x\right)-1225x^2\right)+147\cos\left(x\right)+\left(350x-50\right)\mathrm{e}^{14x}\right)}{8575}+C,['ordinary-differential-equations']
25,Solving second-order differential equation arising from a wave equation,Solving second-order differential equation arising from a wave equation,,"I'm solving a scalar Wave Equation $$\left(\frac{1}{c^2}\frac{\partial^2}{\partial t^2}-\nabla^2\right)u(\vec{r},t)=0$$ under the assumption that $u$ only depends on the magnitude of the position vector: $u(\vec{r},t)=u(r,t)$. Assuming that $u(r,t)=T(t)\Psi(r)$, we have $$\frac{\Psi(r)}{c^2}T''(t)=T(t)\nabla^2\Psi(r) \implies\frac{T''(t)}{c^2T(t)}=\frac{1}{\Psi(r)}\nabla^2\Psi(r)=-k^2$$ for some (possibly complex) constant $k$. The differential equation for $T(t)$ is easily solved, we have $$T(t) = Ae^{i\omega t}+Be^{-i\omega t}$$ for arbitrary values of $A,B$ and $\omega=ck$. The spatial equation can be rewritten as $$\left(\nabla^2+k^2\right)\Psi(r) = 0$$ Now, since $\Psi(r)$ only depends on the radial distance, we can plug in the Laplacian in spherical coordinates to obtain an ODE for $\Psi(r)$. $$ \frac{1}{r^2}\frac{d}{dr}\left(r^2\frac{d}{dr}\Psi(r)\right)+k^2\Psi(r) = 0 $$ Calculating the derivatives and rearranging, we have $$r^2\Psi''(r)+2r\Psi'(r)+k^2r^2\Psi(r) = 0$$ Initially, I thought that this would require Bessel Functions, as it looks very similar to the Bessel Differential Equation. However, I was told that this can be solved using only elementary functions and I was given the hint to write $\Psi(r) = r^af(r)$ for some other function of $f(r)$ and an unknown power $a$. With this, we have the following expressions for the derivatives that appear in our ODE: \begin{align} \Psi'(r) &=r^af'(r)+ar^{a-1}f(r)\\ \Psi''(r) &= r^af''(r)+2ar^{a-1}f'(r)+a(a-1)r^{a-2}f(r) \end{align} Plugging this into our ODE becomes quite messy, but we have \begin{align} r^{a+2}f''(r)+2ar^{a+1}f'(r)+a(a-1)r^af(r)+2r^{a+1}f'(r)+2ar^af(r)+k^2r^{a+2}f(r) = 0 \end{align} However, this is still a second order differential equation that seems much more complicated than the one we had before, so I'm not sure what this accomplished. Could anybody give me a hint on how to proceed here?","I'm solving a scalar Wave Equation $$\left(\frac{1}{c^2}\frac{\partial^2}{\partial t^2}-\nabla^2\right)u(\vec{r},t)=0$$ under the assumption that $u$ only depends on the magnitude of the position vector: $u(\vec{r},t)=u(r,t)$. Assuming that $u(r,t)=T(t)\Psi(r)$, we have $$\frac{\Psi(r)}{c^2}T''(t)=T(t)\nabla^2\Psi(r) \implies\frac{T''(t)}{c^2T(t)}=\frac{1}{\Psi(r)}\nabla^2\Psi(r)=-k^2$$ for some (possibly complex) constant $k$. The differential equation for $T(t)$ is easily solved, we have $$T(t) = Ae^{i\omega t}+Be^{-i\omega t}$$ for arbitrary values of $A,B$ and $\omega=ck$. The spatial equation can be rewritten as $$\left(\nabla^2+k^2\right)\Psi(r) = 0$$ Now, since $\Psi(r)$ only depends on the radial distance, we can plug in the Laplacian in spherical coordinates to obtain an ODE for $\Psi(r)$. $$ \frac{1}{r^2}\frac{d}{dr}\left(r^2\frac{d}{dr}\Psi(r)\right)+k^2\Psi(r) = 0 $$ Calculating the derivatives and rearranging, we have $$r^2\Psi''(r)+2r\Psi'(r)+k^2r^2\Psi(r) = 0$$ Initially, I thought that this would require Bessel Functions, as it looks very similar to the Bessel Differential Equation. However, I was told that this can be solved using only elementary functions and I was given the hint to write $\Psi(r) = r^af(r)$ for some other function of $f(r)$ and an unknown power $a$. With this, we have the following expressions for the derivatives that appear in our ODE: \begin{align} \Psi'(r) &=r^af'(r)+ar^{a-1}f(r)\\ \Psi''(r) &= r^af''(r)+2ar^{a-1}f'(r)+a(a-1)r^{a-2}f(r) \end{align} Plugging this into our ODE becomes quite messy, but we have \begin{align} r^{a+2}f''(r)+2ar^{a+1}f'(r)+a(a-1)r^af(r)+2r^{a+1}f'(r)+2ar^af(r)+k^2r^{a+2}f(r) = 0 \end{align} However, this is still a second order differential equation that seems much more complicated than the one we had before, so I'm not sure what this accomplished. Could anybody give me a hint on how to proceed here?",,"['ordinary-differential-equations', 'partial-differential-equations', 'wave-equation']"
26,Solving the differential equation $7x(x-y)dy = 2(x^2+6xy-5y^2)dx$,Solving the differential equation,7x(x-y)dy = 2(x^2+6xy-5y^2)dx,"How do I solve the differential equation $$7x(x-y)dy = 2(x^2+6xy-5y^2)dx$$ Is it homogeneous? I have tried taking the variables from the LHS and applying them to the RHS, making $\frac{dy}{dx}$ subject and ending up with: $$\frac{dy}{dx} = \frac{2(x^2+6xy-5y^2)}{7x(x-y)}$$ After simplifying the numerator: $$\frac{dy}{dx} = \frac{2[(x-y)(x+5y)+2xy]}{7x(x-y)}$$ $$\frac{dy}{dx} = \frac{2(x+5y)}{7x} + \frac{4y}{7(x-y)}$$ I have no idea how to proceed from here. Is the methodology correct so far? Any input would be appreciated.","How do I solve the differential equation $$7x(x-y)dy = 2(x^2+6xy-5y^2)dx$$ Is it homogeneous? I have tried taking the variables from the LHS and applying them to the RHS, making $\frac{dy}{dx}$ subject and ending up with: $$\frac{dy}{dx} = \frac{2(x^2+6xy-5y^2)}{7x(x-y)}$$ After simplifying the numerator: $$\frac{dy}{dx} = \frac{2[(x-y)(x+5y)+2xy]}{7x(x-y)}$$ $$\frac{dy}{dx} = \frac{2(x+5y)}{7x} + \frac{4y}{7(x-y)}$$ I have no idea how to proceed from here. Is the methodology correct so far? Any input would be appreciated.",,['ordinary-differential-equations']
27,Understanding Proof in Teschl (dynamical systems),Understanding Proof in Teschl (dynamical systems),,"Let $\dot{x} = f(x)$ be a dynamical system on $\mathbb{R}^n$ with   corresponding flow $\phi(t,x)$ and let $U \subseteq \mathbb{R}^n$ be   Lebesgue measurable. Then $$\frac{d}{dt}\int_{\phi(t,U)}dx =  \int_{\phi(t,U)}\text{div}(f(x))dx$$ Proof. By the change of variable formula for multiple integrals we have $$\int_{\phi(t,U)}dx = \int_{U}\det(D\phi_t(x))dx$$ This I do not really understand. I mean, we could define a mapping $$U \to \phi(t,x) \qquad x \mapsto\phi(t,x)$$ for which $U$ has to be open (we want a  $C^1$-diffeomorphism). But wouldn't we get then $$\int_{\phi(t,U)}dx = \int_{U}|\det(D\phi_t(x))|dx$$ instead of the above? I do not quite see why the determinant should be positive.","Let $\dot{x} = f(x)$ be a dynamical system on $\mathbb{R}^n$ with   corresponding flow $\phi(t,x)$ and let $U \subseteq \mathbb{R}^n$ be   Lebesgue measurable. Then $$\frac{d}{dt}\int_{\phi(t,U)}dx =  \int_{\phi(t,U)}\text{div}(f(x))dx$$ Proof. By the change of variable formula for multiple integrals we have $$\int_{\phi(t,U)}dx = \int_{U}\det(D\phi_t(x))dx$$ This I do not really understand. I mean, we could define a mapping $$U \to \phi(t,x) \qquad x \mapsto\phi(t,x)$$ for which $U$ has to be open (we want a  $C^1$-diffeomorphism). But wouldn't we get then $$\int_{\phi(t,U)}dx = \int_{U}|\det(D\phi_t(x))|dx$$ instead of the above? I do not quite see why the determinant should be positive.",,"['real-analysis', 'ordinary-differential-equations', 'dynamical-systems']"
28,Exact Differential Equations: Why does this working rule work?,Exact Differential Equations: Why does this working rule work?,,"Let $M$ and $N$ be some functions of $x$ and $y$. Consider the differential equation $$Mdx+Ndy=0.$$ Suppose that this equation is exact, meaning that $$\frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}.$$ Then the author of my textbook suggests the following working rule: $$\int M\text{(treating y as a constant)} dx+ \int N\text{(taking only those terms in N that do not contain x)} dy=c$$ where $c$ is a constant of integration. I want to know why this works in general.","Let $M$ and $N$ be some functions of $x$ and $y$. Consider the differential equation $$Mdx+Ndy=0.$$ Suppose that this equation is exact, meaning that $$\frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}.$$ Then the author of my textbook suggests the following working rule: $$\int M\text{(treating y as a constant)} dx+ \int N\text{(taking only those terms in N that do not contain x)} dy=c$$ where $c$ is a constant of integration. I want to know why this works in general.",,"['ordinary-differential-equations', 'multivariable-calculus']"
29,Forming second order differential equation from given solutions using the Wronskian. (and some general help),Forming second order differential equation from given solutions using the Wronskian. (and some general help),,"For  $$y''+p(x)y'+q(x)y=0 \space\space\space\space\space\space\space\space\space\space\space  (\star)$$  we have  $$W'(x)=-p(x)W(x) \space\space\space\space\space\space\space\space\space\space\space (\dagger)$$  where $W(x)$ is the Wronskian defined as  $W(x)=y_1y_2'-y_1'y_2$  for linearly indepedent solutions $y_1$ and $y_2$ of $(\star)$. The question asks us to use this to find $p(x)$ and $q(x)$ such that  $y_1(x)=1+\cos x$ and $y_2=\sin x$. Solving $(\dagger)$ for $W(x)$ we  have $W(x)=W_0\int^x_{x_0}-p(u) \space du$. Here's where I'm having issues; my notes have that $W_0=W(x_0)$ for   some $x_0$, now it says we can pick $x_0$ as whatever we like and I'm   fine with that, but what was bothering me (and the question goes onto   ask) is that if we can pick $x_0$ as whatever we like, what happens at   $x_0=\pi$? We would get $W=0$ which is confusing me as Abel's theorem   has that $W(x)=0$ or $W(x)\ne0$ for all $x$- Have I misunderstood? Further, if we can have $W_0$ as whatever we like by picking    convenient $x_0$, we can surely just pick $W_0=1$ then putting in the    given $y_1$ and $y_2$ we have $(1+\cos x)\sin x+\sin x\cos x=\sin   2x+\sin x=\exp({\int^x_{x_0}-p(u)\space du})$ $$\implies p(x)=-\frac   d{dx}(\log(\sin 2x+\sin x)=-\frac{2\cos 2x+\cos x}{\sin 2x+\sin x}$$   where does $q(x)$ come into things? Setting $q(x)=0$ with this $p(x)$   does not work. I think I just haven't quite got my head around some of this stuff yet so an answer or general help is much appreciated. Thank you.","For  $$y''+p(x)y'+q(x)y=0 \space\space\space\space\space\space\space\space\space\space\space  (\star)$$  we have  $$W'(x)=-p(x)W(x) \space\space\space\space\space\space\space\space\space\space\space (\dagger)$$  where $W(x)$ is the Wronskian defined as  $W(x)=y_1y_2'-y_1'y_2$  for linearly indepedent solutions $y_1$ and $y_2$ of $(\star)$. The question asks us to use this to find $p(x)$ and $q(x)$ such that  $y_1(x)=1+\cos x$ and $y_2=\sin x$. Solving $(\dagger)$ for $W(x)$ we  have $W(x)=W_0\int^x_{x_0}-p(u) \space du$. Here's where I'm having issues; my notes have that $W_0=W(x_0)$ for   some $x_0$, now it says we can pick $x_0$ as whatever we like and I'm   fine with that, but what was bothering me (and the question goes onto   ask) is that if we can pick $x_0$ as whatever we like, what happens at   $x_0=\pi$? We would get $W=0$ which is confusing me as Abel's theorem   has that $W(x)=0$ or $W(x)\ne0$ for all $x$- Have I misunderstood? Further, if we can have $W_0$ as whatever we like by picking    convenient $x_0$, we can surely just pick $W_0=1$ then putting in the    given $y_1$ and $y_2$ we have $(1+\cos x)\sin x+\sin x\cos x=\sin   2x+\sin x=\exp({\int^x_{x_0}-p(u)\space du})$ $$\implies p(x)=-\frac   d{dx}(\log(\sin 2x+\sin x)=-\frac{2\cos 2x+\cos x}{\sin 2x+\sin x}$$   where does $q(x)$ come into things? Setting $q(x)=0$ with this $p(x)$   does not work. I think I just haven't quite got my head around some of this stuff yet so an answer or general help is much appreciated. Thank you.",,['ordinary-differential-equations']
30,Determine the stability of a system,Determine the stability of a system,,"For the system   $$x'=y(1+r^2),~~y'=-x(1+r^2),$$   where $r=x^2+y^2,$ determine whether this is asymptotically stable or not. My approach: Writing in terms of Polar coordinates, $x=r \cos \theta,~y=r \sin \theta$ and simplifying, I got  $$r'=0,~~\theta'=-1-r^2.$$ By integrating,  $$r=r_0,~~\theta=\theta_0 -(r_0^2 + 1)t.$$ From this it's clear these form a family of periodic orbits with each being a circle in the phase plane. So they are orbitally stable. But what can I say about the asymptotic stability ?","For the system   $$x'=y(1+r^2),~~y'=-x(1+r^2),$$   where $r=x^2+y^2,$ determine whether this is asymptotically stable or not. My approach: Writing in terms of Polar coordinates, $x=r \cos \theta,~y=r \sin \theta$ and simplifying, I got  $$r'=0,~~\theta'=-1-r^2.$$ By integrating,  $$r=r_0,~~\theta=\theta_0 -(r_0^2 + 1)t.$$ From this it's clear these form a family of periodic orbits with each being a circle in the phase plane. So they are orbitally stable. But what can I say about the asymptotic stability ?",,['ordinary-differential-equations']
31,How to guess a particular solution for a non-homogeneous differential equation with purely imaginary eigenvalues.,How to guess a particular solution for a non-homogeneous differential equation with purely imaginary eigenvalues.,,"Take for example  $$y''+5y'+4y=e^{-x}$$ solving the characteristic we have $\lambda_1=-1,\lambda_2=-4$ thus $$y_c=c_1e^{-x}+c_2e^{-4x}$$ in this case we can guess the particular solution to be $$y_p=axe^{-x}$$ My question, how can we guess a particular solution of the form  $$y''+4y=\cos(2x)$$ I tried $y_p=x(c_1\cos(2x)+c_2\sin(2x))$ but it doesn't work. What is the idea behind such type of problems. If you have any reference please include it here.","Take for example  $$y''+5y'+4y=e^{-x}$$ solving the characteristic we have $\lambda_1=-1,\lambda_2=-4$ thus $$y_c=c_1e^{-x}+c_2e^{-4x}$$ in this case we can guess the particular solution to be $$y_p=axe^{-x}$$ My question, how can we guess a particular solution of the form  $$y''+4y=\cos(2x)$$ I tried $y_p=x(c_1\cos(2x)+c_2\sin(2x))$ but it doesn't work. What is the idea behind such type of problems. If you have any reference please include it here.",,['ordinary-differential-equations']
32,interpreting eigenvectors/values of linear systems,interpreting eigenvectors/values of linear systems,,"I have two related questions: If I have a system of linear equations, written in matrix form: $Ax = b$ then this system has a non-trivial solution only if $det(A) \neq 0$. but what is the significance of the eigenvalues/eigenvectors of A for finding the solutions? Related, if I have a system of homogeneous ordinary differential equations: $x' = Ax$ then what is the interpretation of eigenvalues/eigenvectors of A in finding the solutions of this system of equations? pointers to explanations or concise explanations to this would help.","I have two related questions: If I have a system of linear equations, written in matrix form: $Ax = b$ then this system has a non-trivial solution only if $det(A) \neq 0$. but what is the significance of the eigenvalues/eigenvectors of A for finding the solutions? Related, if I have a system of homogeneous ordinary differential equations: $x' = Ax$ then what is the interpretation of eigenvalues/eigenvectors of A in finding the solutions of this system of equations? pointers to explanations or concise explanations to this would help.",,"['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'intuition']"
33,Application of Gronwall's lemma,Application of Gronwall's lemma,,"Use Gronwall's lemma to prove that the IVP $$\frac{dy}{dt}=e^{\sin t}y(t),~y(0)=y_{0}~~~(*),$$ with $y_0$ being given, has an infinite interval of existence for its solutions. My approach: Gronwall's lemma states that Suppose that $g(t)$ is a continuous real-valued function that satisfies $g(t) \geq 0$ and    $$g(t) \leq C+K \int_{0}^{t} g(s)~ds$$   for all $t \in [0,a],$ where $C$ and $K$ are positive constants.  It then follows that    $$g(t) \leq C e^{Kt},~~\textrm{for all}~t \in [0,a].$$ Solving $(*)$ we get  $$y(t)=y_0 \cdot e^{ \int_{0}^{t} e^{\sin s}~ds}.$$ How can I set up an inequality for this and apply Gronwall's to conclude what's being asked ? What I'm understanding is existence of an infinite interval means that the solution exists for all values of $t.$ Any help is much appreciated.","Use Gronwall's lemma to prove that the IVP $$\frac{dy}{dt}=e^{\sin t}y(t),~y(0)=y_{0}~~~(*),$$ with $y_0$ being given, has an infinite interval of existence for its solutions. My approach: Gronwall's lemma states that Suppose that $g(t)$ is a continuous real-valued function that satisfies $g(t) \geq 0$ and    $$g(t) \leq C+K \int_{0}^{t} g(s)~ds$$   for all $t \in [0,a],$ where $C$ and $K$ are positive constants.  It then follows that    $$g(t) \leq C e^{Kt},~~\textrm{for all}~t \in [0,a].$$ Solving $(*)$ we get  $$y(t)=y_0 \cdot e^{ \int_{0}^{t} e^{\sin s}~ds}.$$ How can I set up an inequality for this and apply Gronwall's to conclude what's being asked ? What I'm understanding is existence of an infinite interval means that the solution exists for all values of $t.$ Any help is much appreciated.",,['ordinary-differential-equations']
34,Explicit solution of Initial Value Problem for inviscid Burgers equation,Explicit solution of Initial Value Problem for inviscid Burgers equation,,"I've been working on solving PDEs with conditions recently and have come across this question: Obtain the explicit solution $u(x,t)$ to the IVP:   $$u_t+uu_x=2t,   \quad -\infty < x <\infty, \quad t>0,$$   $$u(x,0)=x, \quad -\infty<x<\infty.$$   Casting your answer as a function of $x$ and $t$ only. I have had issues getting a solution in just terms of $x$ and $t$, and have been worried that I am misunderstanding how to tackle this. Any guidance is appreciated!","I've been working on solving PDEs with conditions recently and have come across this question: Obtain the explicit solution $u(x,t)$ to the IVP:   $$u_t+uu_x=2t,   \quad -\infty < x <\infty, \quad t>0,$$   $$u(x,0)=x, \quad -\infty<x<\infty.$$   Casting your answer as a function of $x$ and $t$ only. I have had issues getting a solution in just terms of $x$ and $t$, and have been worried that I am misunderstanding how to tackle this. Any guidance is appreciated!",,"['ordinary-differential-equations', 'partial-differential-equations', 'initial-value-problems']"
35,To find a singular integral to $(xy^{\prime}-y)^{2}=x^{2}(x^{2}-y^{2})$,To find a singular integral to,(xy^{\prime}-y)^{2}=x^{2}(x^{2}-y^{2}),I am doing a self study in differential equations and came across the following problem in this post $$(xy^{\prime}-y)^{2}=x^{2}(x^{2}-y^{2})$$ to find singular integral of this ODE. The solution given there isn't helpful at all. Is there a standard method to solve these type of equations. Any help would be great!,I am doing a self study in differential equations and came across the following problem in this post $$(xy^{\prime}-y)^{2}=x^{2}(x^{2}-y^{2})$$ to find singular integral of this ODE. The solution given there isn't helpful at all. Is there a standard method to solve these type of equations. Any help would be great!,,['ordinary-differential-equations']
36,Some questions about Peano and Picard,Some questions about Peano and Picard,,"I have some question regarding the Peano and Picard theorems. It started with this exercise: Prove that there exists a solution near the initial value for $y^{\prime}=(x-y)^{5/4},\,y(4)=4$ and state if the solution near that point is unique. I believe that we can't really conclude by using Peano's theorem the existence of a solution and that's because we can't find any rectangle under $y=x$ that contains the point $(4,4)$ in which $(x-y)^{5/4}$ is continuous. I know I am wrong, but can someone explain me why? The confusion started when I read at some online notes here that the problem  $y^{\prime}=(y-x)^{1/2},\,y(a)=b$ is guaranteed to have a solution ONLY if $a>b$, it states that at $a=b$ any rectangle will ""catch"" points for which $x>y$ and thus we can't take $a\geq b$ .  ( http://web.math.rochester.edu/people/faculty/edummit/docs/calc2_5_introduction_to_differential_equations.pdf at page $4$) According to this, I shouldn't be able to prove that there exists a solution near point (4,4) since $a=b=4$. Can someone explain to me why I am wrong? So after all, is there any rectangle which contains $(4,4)$ in which the function is continuous? If so would be it like this? http://sketchtoy.com/67691139 ? I am so confused, I am not sure if I even explained it properly :S . Thanks for reading my (probably) stupid question.","I have some question regarding the Peano and Picard theorems. It started with this exercise: Prove that there exists a solution near the initial value for $y^{\prime}=(x-y)^{5/4},\,y(4)=4$ and state if the solution near that point is unique. I believe that we can't really conclude by using Peano's theorem the existence of a solution and that's because we can't find any rectangle under $y=x$ that contains the point $(4,4)$ in which $(x-y)^{5/4}$ is continuous. I know I am wrong, but can someone explain me why? The confusion started when I read at some online notes here that the problem  $y^{\prime}=(y-x)^{1/2},\,y(a)=b$ is guaranteed to have a solution ONLY if $a>b$, it states that at $a=b$ any rectangle will ""catch"" points for which $x>y$ and thus we can't take $a\geq b$ .  ( http://web.math.rochester.edu/people/faculty/edummit/docs/calc2_5_introduction_to_differential_equations.pdf at page $4$) According to this, I shouldn't be able to prove that there exists a solution near point (4,4) since $a=b=4$. Can someone explain to me why I am wrong? So after all, is there any rectangle which contains $(4,4)$ in which the function is continuous? If so would be it like this? http://sketchtoy.com/67691139 ? I am so confused, I am not sure if I even explained it properly :S . Thanks for reading my (probably) stupid question.",,['ordinary-differential-equations']
37,Prove $xy'+ay=f(x)$ has only one bounded solution,Prove  has only one bounded solution,xy'+ay=f(x),"Let $xy'+ay=f(x)$, where $a>0$ and $f(x)$ is a continuous and bounded function on $(0,\infty)$ that satisfies $\lim\limits_{x\to0^+}f(x)=b$. Prove that the equation has only one solution which is bounded on $(0,\infty)$. I was only able to prove that there can't be more than one bounded solution: suppose there are two solutions $y_1,y_2$ which are bounded $|y_1(x)|\leq M,|y_2(x)|\leq K$. Then $y_1-y_2$ solves the homogeneous equation $g'+\frac{a}{x}g=0$. But the general solution to this equation is $g(x)=Cx^{-a}$. Since $a>0$ we have $\lim\limits_{x\to0^+}g(x)=C\lim\limits_{x\to0^+}x^{-|a|}=\infty$, therefore the solutions are not bounded on $(0,\infty)$ which contradicts the fact that $|y_1-y_2|\leq|y_1|+|y_2|\leq M+K$. However I fail to prove that such (bounded) solution exists (I tried to prove that the integral which we obtain when finding the solution to the original equation is bounded, but I didn't succeed).","Let $xy'+ay=f(x)$, where $a>0$ and $f(x)$ is a continuous and bounded function on $(0,\infty)$ that satisfies $\lim\limits_{x\to0^+}f(x)=b$. Prove that the equation has only one solution which is bounded on $(0,\infty)$. I was only able to prove that there can't be more than one bounded solution: suppose there are two solutions $y_1,y_2$ which are bounded $|y_1(x)|\leq M,|y_2(x)|\leq K$. Then $y_1-y_2$ solves the homogeneous equation $g'+\frac{a}{x}g=0$. But the general solution to this equation is $g(x)=Cx^{-a}$. Since $a>0$ we have $\lim\limits_{x\to0^+}g(x)=C\lim\limits_{x\to0^+}x^{-|a|}=\infty$, therefore the solutions are not bounded on $(0,\infty)$ which contradicts the fact that $|y_1-y_2|\leq|y_1|+|y_2|\leq M+K$. However I fail to prove that such (bounded) solution exists (I tried to prove that the integral which we obtain when finding the solution to the original equation is bounded, but I didn't succeed).",,['ordinary-differential-equations']
38,Forward Euler Method for $dx/dt=2x-4+4t$,Forward Euler Method for,dx/dt=2x-4+4t,"I have the initial value problem $\frac{dx}{dt}=2x-4+4t$ with initial condition $x(0)=3/2$ and i need to use the forward euler method with a time step of $0.1$ up to $t=0.5$. I have that the equation for the forward euler method $x_{n+1}=x_n +dt*(2*(x_n)-4+4*(t_n))$. Where $dt=0.1$. From the intial condition $x=3/2$ when $t=0$ but if i apply these conditions for $x_1$ I get: $x_1=x_0+0.1(2*x_0-4+4*t_0)$, $x_1=1.5+0.1(2*1.5-4+4*0)$, $x_1=1.5-0.1$, $x_1=1.4$. But surely that can't be right as for $t=0$ there are two values of $x$: $x_0=1.5, x_1=1.4$. Can anyone help me?","I have the initial value problem $\frac{dx}{dt}=2x-4+4t$ with initial condition $x(0)=3/2$ and i need to use the forward euler method with a time step of $0.1$ up to $t=0.5$. I have that the equation for the forward euler method $x_{n+1}=x_n +dt*(2*(x_n)-4+4*(t_n))$. Where $dt=0.1$. From the intial condition $x=3/2$ when $t=0$ but if i apply these conditions for $x_1$ I get: $x_1=x_0+0.1(2*x_0-4+4*t_0)$, $x_1=1.5+0.1(2*1.5-4+4*0)$, $x_1=1.5-0.1$, $x_1=1.4$. But surely that can't be right as for $t=0$ there are two values of $x$: $x_0=1.5, x_1=1.4$. Can anyone help me?",,"['ordinary-differential-equations', 'numerical-methods']"
39,How can Banach fixed point theorem be used to prove that there is a unique solution to differential equation?,How can Banach fixed point theorem be used to prove that there is a unique solution to differential equation?,,"I couple of months ago my professor said in class that the Banach fixed Point theorem could be used to prove the uniqueness of a solution to a differential equation. (My memory can be wrong, please correct me if so.). If the above is correct. How can it be used to prove the uniqueness of the solution to $y=y'$?","I couple of months ago my professor said in class that the Banach fixed Point theorem could be used to prove the uniqueness of a solution to a differential equation. (My memory can be wrong, please correct me if so.). If the above is correct. How can it be used to prove the uniqueness of the solution to $y=y'$?",,['ordinary-differential-equations']
40,Interpreting a Butcher Table,Interpreting a Butcher Table,,"I have the following adaptive Adams-Bashforth scheme for estimating an ODE: $y_{{i+1}} = y_{{i}} + \alpha f_i + \beta f_{i-1}$ where, if we have 3 time points that are not equally spaced: $t_{i-1}, t_i$ and $t_{i+1}$, then $t_i - t_{i-1} = h_{prev}, \space \space t_{i+1} - t_i = h$ I am asked to calculate an alternative $y^*_{i+1}$, given a $y_i$, using the third order Runge-Kutta method defined by the Butcher table: 0    |   (1/2)  | (1/2)     1    |  -1      2 --------------------------------          |  (1/6)  (2/3)  (1/6) I have no idea if I wrote the explicit formula for the Runge-Kutta method correctly, however. I dont have experience working with Butcher tables which is why I'm not confident. Here's what I got: $y_{i+1} = y_i + h/6 (k_1 + 4k_2 +k_3)$ where $k_1 = f(t_i,y_i)$ $k_2 = f(t_i + h/2, \space y_i + \frac{h}{2} k_1)$ $k_3 = f(t_i + h/2, \space y_i + \frac{h}{2} k_2)$ I'm hoping someone can show me if/where I went wrong. Part of what's throwing me off is that we're dealing with an adaptive step size, but I'm not sure either way. I'd really appreciate the help.","I have the following adaptive Adams-Bashforth scheme for estimating an ODE: $y_{{i+1}} = y_{{i}} + \alpha f_i + \beta f_{i-1}$ where, if we have 3 time points that are not equally spaced: $t_{i-1}, t_i$ and $t_{i+1}$, then $t_i - t_{i-1} = h_{prev}, \space \space t_{i+1} - t_i = h$ I am asked to calculate an alternative $y^*_{i+1}$, given a $y_i$, using the third order Runge-Kutta method defined by the Butcher table: 0    |   (1/2)  | (1/2)     1    |  -1      2 --------------------------------          |  (1/6)  (2/3)  (1/6) I have no idea if I wrote the explicit formula for the Runge-Kutta method correctly, however. I dont have experience working with Butcher tables which is why I'm not confident. Here's what I got: $y_{i+1} = y_i + h/6 (k_1 + 4k_2 +k_3)$ where $k_1 = f(t_i,y_i)$ $k_2 = f(t_i + h/2, \space y_i + \frac{h}{2} k_1)$ $k_3 = f(t_i + h/2, \space y_i + \frac{h}{2} k_2)$ I'm hoping someone can show me if/where I went wrong. Part of what's throwing me off is that we're dealing with an adaptive step size, but I'm not sure either way. I'd really appreciate the help.",,"['analysis', 'ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
41,"Gradients ""backward flow"" calculation rulls","Gradients ""backward flow"" calculation rulls",,"I am reading article Hacker's guide to Neural Networks. . Becoming Becoming a Backprop Ninja section The quote: lets just use variables such as a,b,c,x, and refer to their gradients as da,db,dc,dx respectively. Again, we think of the variables as the “forward flow” and their gradients as “backward flow” along every wire. Our first example was the * gate: x = a * b; // and given gradient on x (dx), we saw that in backprop we would compute: da = b * dx; I have rewatched Calculus lectures about derivatives and gradient but still don't understand this simple part. My reasonings: The partial derivative of x with respect to a is equal to b . dx/da = b (1) During gradient based optimisation we have dx - that is x gradient(vector that points to the maximum and is the sum of partial derivatives vectors). This gradient tells how mutch should we adjust independent variables (that is b here) to get to the maximum. From (1): da = dx/b So here we need to take gradient of x that came from cost function and divide it into b. Then we can adjust b in that direction. So why does the author say about da = b * dx; and not da = dx/b here? Also if you see some mistakes in my reasoning please tell me.","I am reading article Hacker's guide to Neural Networks. . Becoming Becoming a Backprop Ninja section The quote: lets just use variables such as a,b,c,x, and refer to their gradients as da,db,dc,dx respectively. Again, we think of the variables as the “forward flow” and their gradients as “backward flow” along every wire. Our first example was the * gate: x = a * b; // and given gradient on x (dx), we saw that in backprop we would compute: da = b * dx; I have rewatched Calculus lectures about derivatives and gradient but still don't understand this simple part. My reasonings: The partial derivative of x with respect to a is equal to b . dx/da = b (1) During gradient based optimisation we have dx - that is x gradient(vector that points to the maximum and is the sum of partial derivatives vectors). This gradient tells how mutch should we adjust independent variables (that is b here) to get to the maximum. From (1): da = dx/b So here we need to take gradient of x that came from cost function and divide it into b. Then we can adjust b in that direction. So why does the author say about da = b * dx; and not da = dx/b here? Also if you see some mistakes in my reasoning please tell me.",,"['ordinary-differential-equations', 'derivatives', 'optimization', 'gradient-flows', 'gradient-descent']"
42,How to solve the differential equation: $yy'^2-2xy'+y=0$,How to solve the differential equation:,yy'^2-2xy'+y=0,Solve the differential equation: $yy'^2-2xy'+y=0$ (*) I have $(*)\Leftrightarrow (yy')^2-2xyy'+y^2=0\\ \Leftrightarrow z'^2-4xz+4z^2 =0 \ \ (z=y^2)$ I can't know how to continue. Could you give me some hints. Thanks for helping.,Solve the differential equation: $yy'^2-2xy'+y=0$ (*) I have $(*)\Leftrightarrow (yy')^2-2xyy'+y^2=0\\ \Leftrightarrow z'^2-4xz+4z^2 =0 \ \ (z=y^2)$ I can't know how to continue. Could you give me some hints. Thanks for helping.,,['ordinary-differential-equations']
43,How to show that trajectories are either ellipses or lines,How to show that trajectories are either ellipses or lines,,"Equations of motion for a pendulum came out to be $$x(t) = c_1\cos(\omega t)+c_2 \sin(\omega t)$$ $$y(t) = c_3\cos(\omega t)+c_4 \sin(\omega t)$$ How can it be shown that the trajectories of these solutions are either ellipses or lines? I think one can work directly with the two ODEs, rewrite them as a decoupled system of four linear ODEs, and then analyze their eigenvalues and sketch the phase portrait based on that. But in this case, a simpler approach is needed. Here's what I've done so far: $x(t) = A\cos(\omega t + \delta)$ can be rewritten as $x(\tau) = A\cos(\omega\tau)$, where $\tau := t+\frac{\delta}{\omega}$. The question is now how to rewrite $y$ in such a way is to be able to substitute $x$ into $y$ and get a suitable equation for trajectories. I took $y(\tau) = B\sin(\omega \tau)$, hence we get, after rearrangement and division, the following equation $$ \frac{y^2}{B^2} + \frac{x^2}{A^2}=1 $$, which is clearly an ellipse. But what about lines? If $B=0$ then $y^2 + \frac{B^2}{A^2}x^2=y^2=1$, so $y=\pm 1$, which are horizontal lines. What about lines in general? Would appreciate your feedback.","Equations of motion for a pendulum came out to be $$x(t) = c_1\cos(\omega t)+c_2 \sin(\omega t)$$ $$y(t) = c_3\cos(\omega t)+c_4 \sin(\omega t)$$ How can it be shown that the trajectories of these solutions are either ellipses or lines? I think one can work directly with the two ODEs, rewrite them as a decoupled system of four linear ODEs, and then analyze their eigenvalues and sketch the phase portrait based on that. But in this case, a simpler approach is needed. Here's what I've done so far: $x(t) = A\cos(\omega t + \delta)$ can be rewritten as $x(\tau) = A\cos(\omega\tau)$, where $\tau := t+\frac{\delta}{\omega}$. The question is now how to rewrite $y$ in such a way is to be able to substitute $x$ into $y$ and get a suitable equation for trajectories. I took $y(\tau) = B\sin(\omega \tau)$, hence we get, after rearrangement and division, the following equation $$ \frac{y^2}{B^2} + \frac{x^2}{A^2}=1 $$, which is clearly an ellipse. But what about lines? If $B=0$ then $y^2 + \frac{B^2}{A^2}x^2=y^2=1$, so $y=\pm 1$, which are horizontal lines. What about lines in general? Would appreciate your feedback.",,"['geometry', 'ordinary-differential-equations', 'physics']"
44,Second derivative test inconclusive,Second derivative test inconclusive,,"I have the function $f(x,y) = x^2y(4-x-y)$ and $T = \{(x,y) | x\ge 0, y \ge 0, x+y \le 6\}$. With the second derivative test I have found that $(2, 1)$ is a relative maximum. By calculating the first derivatives you will see that $x=0$ and $y \in [0, 6]$ are critical points (correct me if I am wrong). So by using those numbers, the second derivative test is inconclusive. My questions are: What can I do if the second derivative test is inconclusive? Does this function have a relative minimum?","I have the function $f(x,y) = x^2y(4-x-y)$ and $T = \{(x,y) | x\ge 0, y \ge 0, x+y \le 6\}$. With the second derivative test I have found that $(2, 1)$ is a relative maximum. By calculating the first derivatives you will see that $x=0$ and $y \in [0, 6]$ are critical points (correct me if I am wrong). So by using those numbers, the second derivative test is inconclusive. My questions are: What can I do if the second derivative test is inconclusive? Does this function have a relative minimum?",,"['ordinary-differential-equations', 'multivariable-calculus']"
45,Checking validity of Green's Function,Checking validity of Green's Function,,"I have the following ODE: $$\left[\frac{d^2}{dt^2}+2\gamma\frac{d}{dt}+\gamma^2\right]x(t) = f(t),$$ where $f(t)$ is an arbitrary driving force. I have already obtained the causal Green's function, $G(t,t')$, which is: $$G(t,t') = \begin{cases} 0 &\text{, if }t\leq t'\\ (t-t')e^{-\gamma\,(t-t')} &\text{, if } t>t' \end{cases}$$ Is there any way for me to check if the Green's function I calculated is correct? I know that the Green's function should satisfy the ODE for which $f(t)$ is a delta function (an impulse centred at $t=t'$. I have tried substituting back my Green's function into the relevant ODE (modifying the RHS), but I'm not sure what to expect? I think I got $0$, but I'm not entirely confident of that answer, nor what to make of it. EDIT: Fixed a small typo for the solution for $G(t,t')$. Okay, here's what I got: Let $\displaystyle G=H(t-t')e^{-\gamma(t-t')}$. I need to compute the derivatives for $t-t'>0$: $$\begin{align} \frac{dG}{dt} &= \frac{dH}{dt}e^{-\gamma(t-t')}-H(t-t')\gamma e^{-\gamma(t-t')} \\ &= \delta(t-t')e^{-\gamma(t-t')}-\gamma G(t-t') \\ \frac{d^2G}{dt^2}&= \frac{d\delta}{dt}e^{-\gamma(t-t')}-\gamma\delta(t-t')e^{-\gamma(t-t')}-\gamma\frac{dG}{dt} \\ &= \frac{d\delta}{dt}e^{-\gamma(t-t')}-2\gamma\delta(t-t')e^{-\gamma(t-t')}+\gamma^2G \end{align}$$ Putting them altogether, I get: $$\text{L.H.S.}=\frac{d\delta}{dt}e^{-\gamma(t-t')}$$ I don't see how this is equal to $\delta(t-t')$..","I have the following ODE: $$\left[\frac{d^2}{dt^2}+2\gamma\frac{d}{dt}+\gamma^2\right]x(t) = f(t),$$ where $f(t)$ is an arbitrary driving force. I have already obtained the causal Green's function, $G(t,t')$, which is: $$G(t,t') = \begin{cases} 0 &\text{, if }t\leq t'\\ (t-t')e^{-\gamma\,(t-t')} &\text{, if } t>t' \end{cases}$$ Is there any way for me to check if the Green's function I calculated is correct? I know that the Green's function should satisfy the ODE for which $f(t)$ is a delta function (an impulse centred at $t=t'$. I have tried substituting back my Green's function into the relevant ODE (modifying the RHS), but I'm not sure what to expect? I think I got $0$, but I'm not entirely confident of that answer, nor what to make of it. EDIT: Fixed a small typo for the solution for $G(t,t')$. Okay, here's what I got: Let $\displaystyle G=H(t-t')e^{-\gamma(t-t')}$. I need to compute the derivatives for $t-t'>0$: $$\begin{align} \frac{dG}{dt} &= \frac{dH}{dt}e^{-\gamma(t-t')}-H(t-t')\gamma e^{-\gamma(t-t')} \\ &= \delta(t-t')e^{-\gamma(t-t')}-\gamma G(t-t') \\ \frac{d^2G}{dt^2}&= \frac{d\delta}{dt}e^{-\gamma(t-t')}-\gamma\delta(t-t')e^{-\gamma(t-t')}-\gamma\frac{dG}{dt} \\ &= \frac{d\delta}{dt}e^{-\gamma(t-t')}-2\gamma\delta(t-t')e^{-\gamma(t-t')}+\gamma^2G \end{align}$$ Putting them altogether, I get: $$\text{L.H.S.}=\frac{d\delta}{dt}e^{-\gamma(t-t')}$$ I don't see how this is equal to $\delta(t-t')$..",,"['ordinary-differential-equations', 'greens-function']"
46,How do I verify the energy conservation rate for the total energy?,How do I verify the energy conservation rate for the total energy?,,"It's given differential equation $\dot{x}=p$ $\dot{p}=-x^3+x$ How do I verify the energy conservation rate for the total energy? $H(x,p)=\frac{x^4}{4}-\frac{x^2}{2}+\frac{p^2}{2}+\frac{1}{2}$? What follows for the solution curves of the differential equation? I have found on internet that if I want to check  energy conservation, I need to verify that $0=\frac{dH}{dt}=\frac{dH}{dx}\frac{dx}{dt}+\frac{dH}{dp}\frac{dp}{dt}$ I'm not sure if this equation  is good, but if it is, what is my $t$ here?","It's given differential equation $\dot{x}=p$ $\dot{p}=-x^3+x$ How do I verify the energy conservation rate for the total energy? $H(x,p)=\frac{x^4}{4}-\frac{x^2}{2}+\frac{p^2}{2}+\frac{1}{2}$? What follows for the solution curves of the differential equation? I have found on internet that if I want to check  energy conservation, I need to verify that $0=\frac{dH}{dt}=\frac{dH}{dx}\frac{dx}{dt}+\frac{dH}{dp}\frac{dp}{dt}$ I'm not sure if this equation  is good, but if it is, what is my $t$ here?",,['ordinary-differential-equations']
47,How do I calculate an approximation of the solution of the begining values at the point t = 1 with Euler method?,How do I calculate an approximation of the solution of the begining values at the point t = 1 with Euler method?,,"I have already posted question where I was asking about sketching Euler method. The explicit Euler method for numerically solving the begining values of differential equation $x′=f(t,x),x(t_0)=x_0$ on the interval $I = [t_0, T]$ is given by $x_{k+1}=x_k+hf(t_k,x_k),k=0,…,N−1$  with $h = (T - t_0) / N, N ∈ N. $ $X_k$ is an approximation of the exact solution $x(t)$ of the begining values at time $t_k: = t_0 + kh, k = 0, ..., N.$ By linear interpolation between the points $(t_k, x_k)$ and $(t_{k + 1}, x_{k + 1}), k = 0, ..., N -1,$ we obtain a approximation solution $x_h(t)$. I need to calculate an approximation of the solution of the begining values at the point $t = 1$ $x'=-t/x, x(0)=1$ I need to use h = 0.5. Specify $x_h (1)$ and calculate the error, that is difference $x_h (1) -x (1)$, where $x (1)$ is the value of the exact solution. I really don't know how to start. How can I calulate $x$ or $x_h$ at all?","I have already posted question where I was asking about sketching Euler method. The explicit Euler method for numerically solving the begining values of differential equation $x′=f(t,x),x(t_0)=x_0$ on the interval $I = [t_0, T]$ is given by $x_{k+1}=x_k+hf(t_k,x_k),k=0,…,N−1$  with $h = (T - t_0) / N, N ∈ N. $ $X_k$ is an approximation of the exact solution $x(t)$ of the begining values at time $t_k: = t_0 + kh, k = 0, ..., N.$ By linear interpolation between the points $(t_k, x_k)$ and $(t_{k + 1}, x_{k + 1}), k = 0, ..., N -1,$ we obtain a approximation solution $x_h(t)$. I need to calculate an approximation of the solution of the begining values at the point $t = 1$ $x'=-t/x, x(0)=1$ I need to use h = 0.5. Specify $x_h (1)$ and calculate the error, that is difference $x_h (1) -x (1)$, where $x (1)$ is the value of the exact solution. I really don't know how to start. How can I calulate $x$ or $x_h$ at all?",,"['ordinary-differential-equations', 'numerical-methods', 'eulers-method']"
48,How do I explain Euler method?,How do I explain Euler method?,,"The explicit Euler method for numerically solving the begining values of differential equation $x′=f(t,x),x(t_0)=x_0$ on the interval $I = [t_0, T]$ is given by $x_{k+1}=x_k+hf(t_k,x_k),k=0,…,N−1$  with $h = (T - t_0) / N, N ∈ N. $ $X_k$ is an approximation of the exact solution $x(t)$ of the begining values at time $t_k: = t_0 + kh, k = 0, ..., N.$ By linear interpolation between the points $(t_k, x_k)$ and $(t_{k + 1}, x_{k + 1}), k = 0, ..., N -1,$ we obtain a approximation solution $x_h(t)$. Can someone tell if this sketch is good?","The explicit Euler method for numerically solving the begining values of differential equation $x′=f(t,x),x(t_0)=x_0$ on the interval $I = [t_0, T]$ is given by $x_{k+1}=x_k+hf(t_k,x_k),k=0,…,N−1$  with $h = (T - t_0) / N, N ∈ N. $ $X_k$ is an approximation of the exact solution $x(t)$ of the begining values at time $t_k: = t_0 + kh, k = 0, ..., N.$ By linear interpolation between the points $(t_k, x_k)$ and $(t_{k + 1}, x_{k + 1}), k = 0, ..., N -1,$ we obtain a approximation solution $x_h(t)$. Can someone tell if this sketch is good?",,['ordinary-differential-equations']
49,y'' + y = -sin(x),y'' + y = -sin(x),,"$y'' + y = -\sin(x)$ $y(0) = 0 $ $y'(0) = 0$ I first solved for the homogeneous solution to get: $y(x) = c_1 \sin(x) + c_2 \cos(x)$ then took the derivative of that: $y'(x) = c_1 \cos (x) - c_2 \sin(x)$ This is where I am not sure where to go... $y(0) = c_1 \sin(0) + c_2 \cos(0)       =  0 + c_2 $ so would c2 equal zero? y'(0) = c1 cos(0) - c2 sin(0)       =  c1 - 0 then c2 is zero too? I don't think that's right, but I'm not sure what else to do.  And where to go from there.","$y'' + y = -\sin(x)$ $y(0) = 0 $ $y'(0) = 0$ I first solved for the homogeneous solution to get: $y(x) = c_1 \sin(x) + c_2 \cos(x)$ then took the derivative of that: $y'(x) = c_1 \cos (x) - c_2 \sin(x)$ This is where I am not sure where to go... $y(0) = c_1 \sin(0) + c_2 \cos(0)       =  0 + c_2 $ so would c2 equal zero? y'(0) = c1 cos(0) - c2 sin(0)       =  c1 - 0 then c2 is zero too? I don't think that's right, but I'm not sure what else to do.  And where to go from there.",,['ordinary-differential-equations']
50,Solve the differential equation $\frac{dy}{dx} + 3yx = 0$ for the values $x = 0$ when $y = 1$ - Solution Review,Solve the differential equation  for the values  when  - Solution Review,\frac{dy}{dx} + 3yx = 0 x = 0 y = 1,"Solve the differential equation $\frac{dy}{dx} + 3yx = 0$; $x = 0$ when $y = 1$. I solved this DE using the integration factor method. However, online calculators are giving me a different answer, where they instead used the separation of variables method. Please review my solution and indicate if/where my reasoning is false, why it is false, how to fix it, and what the correct reasoning should be. Thank you. My solution is as follows. $\dfrac{dy}{dx} + 3xy = 0$ $e^{ \int 3x } dx = e^{ \frac{3x^2}{2} }$ $ \dfrac{dy}{dx} e^{ \frac{3x^2}{2} } + 3yxe^{ \frac{3x^2}{2} } = 0$ $ \dfrac{d}{dx} \left( y e^{ \frac{3x^2}{2}} \right) = \dfrac{dy}{dx} e^{ \frac{3x^2}{2} } + 3xye^{ \frac{3x^2}{2}} $ $ \therefore \dfrac{d}{dx} \left( y e^{ \frac{3x^2}{2}} \right) = 0$ $ \displaystyle\int \dfrac{d}{dx} \left( y e^{ \frac{3x^2}{2}} \right) dx + C = 0$ $ \Rightarrow y e^{ \frac{3x^2}{2}} + C = 0$ We want to solve for y. We know that $x = 0$. $ \therefore ye^0 + C = 0$ $ \Rightarrow y = -C$","Solve the differential equation $\frac{dy}{dx} + 3yx = 0$; $x = 0$ when $y = 1$. I solved this DE using the integration factor method. However, online calculators are giving me a different answer, where they instead used the separation of variables method. Please review my solution and indicate if/where my reasoning is false, why it is false, how to fix it, and what the correct reasoning should be. Thank you. My solution is as follows. $\dfrac{dy}{dx} + 3xy = 0$ $e^{ \int 3x } dx = e^{ \frac{3x^2}{2} }$ $ \dfrac{dy}{dx} e^{ \frac{3x^2}{2} } + 3yxe^{ \frac{3x^2}{2} } = 0$ $ \dfrac{d}{dx} \left( y e^{ \frac{3x^2}{2}} \right) = \dfrac{dy}{dx} e^{ \frac{3x^2}{2} } + 3xye^{ \frac{3x^2}{2}} $ $ \therefore \dfrac{d}{dx} \left( y e^{ \frac{3x^2}{2}} \right) = 0$ $ \displaystyle\int \dfrac{d}{dx} \left( y e^{ \frac{3x^2}{2}} \right) dx + C = 0$ $ \Rightarrow y e^{ \frac{3x^2}{2}} + C = 0$ We want to solve for y. We know that $x = 0$. $ \therefore ye^0 + C = 0$ $ \Rightarrow y = -C$",,['ordinary-differential-equations']
51,How do I find the particular solution for $ y''' + y'' + y' + y = \cos t + e^{-t}$?,How do I find the particular solution for ?, y''' + y'' + y' + y = \cos t + e^{-t},I tried solving it as $A\cos t + B\sin t + Ce^{-t}$ but I got $0$ in the end after I differentiate and substitute into the equation.,I tried solving it as $A\cos t + B\sin t + Ce^{-t}$ but I got $0$ in the end after I differentiate and substitute into the equation.,,['ordinary-differential-equations']
52,How to integrate Newton's law of cooling?,How to integrate Newton's law of cooling?,,"I have been given the differential equation $T'(t)=k(T(t)-A)$, where $T$=temperature, $t$=time, $A$=the constant temperature of the surroundings and $k$ is constant. How do I find $T(t)$ expressed with $T_o$ = temperature when $t=0$,  $A$ and $k$. It says to use the substitution with $u(t)=T(t)-A$. Thanks :)","I have been given the differential equation $T'(t)=k(T(t)-A)$, where $T$=temperature, $t$=time, $A$=the constant temperature of the surroundings and $k$ is constant. How do I find $T(t)$ expressed with $T_o$ = temperature when $t=0$,  $A$ and $k$. It says to use the substitution with $u(t)=T(t)-A$. Thanks :)",,"['calculus', 'ordinary-differential-equations']"
53,substitution to homogeneous equation,substitution to homogeneous equation,,"By making substitution  $y=zx^n$ choosing a convenient value of n  show that the following differential equations can be transformed into equations with separable variables, and thereby solve this: $dy/dx=\frac{1-xy^2}{2x^2y}$ My attempt: $F=2x^2dy=(1-xy^2)dx$ $2x^2dy-(1-xy^2)dx=0$ Substitutions: $y=zx^n$ and $ dy=x^ndz$ $2x^{2+2n}zdz+(x^{2n+1}z^2-1)dx=0$ $F_{xy}=F_{yx}$ After taking derivative we get $n=-1/2$ Which is the correct answer  My question is that  $dy/dx=\frac{1-xy^2}{2x^2y}$ the -1 and 2 are just before $xy^2$ and $x^2y$ is n related to them I mean for every problem do I have to go through my way to get that or by dividing them can i get that?  Because in the other problems of my book it is still the case.. Problems are $y'=\frac{2+3xy^2}{4x^2y}$ $ n=3/4$ $y'=\frac{y-xy^2}{x+x^2y}$ $n=-1$","By making substitution  $y=zx^n$ choosing a convenient value of n  show that the following differential equations can be transformed into equations with separable variables, and thereby solve this: $dy/dx=\frac{1-xy^2}{2x^2y}$ My attempt: $F=2x^2dy=(1-xy^2)dx$ $2x^2dy-(1-xy^2)dx=0$ Substitutions: $y=zx^n$ and $ dy=x^ndz$ $2x^{2+2n}zdz+(x^{2n+1}z^2-1)dx=0$ $F_{xy}=F_{yx}$ After taking derivative we get $n=-1/2$ Which is the correct answer  My question is that  $dy/dx=\frac{1-xy^2}{2x^2y}$ the -1 and 2 are just before $xy^2$ and $x^2y$ is n related to them I mean for every problem do I have to go through my way to get that or by dividing them can i get that?  Because in the other problems of my book it is still the case.. Problems are $y'=\frac{2+3xy^2}{4x^2y}$ $ n=3/4$ $y'=\frac{y-xy^2}{x+x^2y}$ $n=-1$",,['ordinary-differential-equations']
54,How to arrange this equation to find the solution to this exact differential equation?,How to arrange this equation to find the solution to this exact differential equation?,,"Is there any rule for simplifying equations of this type, because I tried separating the variables and it doesn't work: $x {\operatorname{d}\!y\over\operatorname{d}\!x} =2xe^x - y + 6x^2$","Is there any rule for simplifying equations of this type, because I tried separating the variables and it doesn't work: $x {\operatorname{d}\!y\over\operatorname{d}\!x} =2xe^x - y + 6x^2$",,['ordinary-differential-equations']
55,Finding solution of Differential Equation $\frac{dy}{dt}=y(1-\frac{y}{4})$ given $y=4$ when $t=0$,Finding solution of Differential Equation  given  when,\frac{dy}{dt}=y(1-\frac{y}{4}) y=4 t=0,"I am given the Differential Equation: $$\frac{dy}{dt}=y(1-\frac{y}{4})$$ First I was asked to find the constant solutions which are: $$y=0,4$$ Now I am asked to use those solutions to find the solution to the DE with the initial values $y=4$ when $t=0$. I started by dividing both sides by $y(1-\frac{y}{4})$: $$\frac{1}{y(1-\frac{y}{4})}\frac{dy}{dt}=1$$ Then integrating both sides, and applying the substitution rule to the left side: $$\int\frac{1}{y-\frac{y^2}{4}}dy=\int1dt$$ $$log(|y|)+\frac{4}{y}=t+C$$ But now I'm not sure how to rearrange the equation to make $y$ the subject.","I am given the Differential Equation: $$\frac{dy}{dt}=y(1-\frac{y}{4})$$ First I was asked to find the constant solutions which are: $$y=0,4$$ Now I am asked to use those solutions to find the solution to the DE with the initial values $y=4$ when $t=0$. I started by dividing both sides by $y(1-\frac{y}{4})$: $$\frac{1}{y(1-\frac{y}{4})}\frac{dy}{dt}=1$$ Then integrating both sides, and applying the substitution rule to the left side: $$\int\frac{1}{y-\frac{y^2}{4}}dy=\int1dt$$ $$log(|y|)+\frac{4}{y}=t+C$$ But now I'm not sure how to rearrange the equation to make $y$ the subject.",,"['integration', 'ordinary-differential-equations']"
56,Finding the general solution of $xy''+y'+xy=0$ using reduction of order.,Finding the general solution of  using reduction of order.,xy''+y'+xy=0,"Use reduction of order to show that the general solution of $$xy''+y'+xy=0$$ can be written as $$y=c_1J_0(x)+c_2J_0(x) \int^x_1 \frac{1}{s[J_0(x)]^2} ds.$$ Show that if $c_2 \neq 0, y \approx c_2 \ln(x)$ as $x \rightarrow 0^+.$ Notes: First we need to transform the original ODE into Bessel's ODE $x^2y''+xy'+x^2y=0$. As $J_0(x)$ (a 0 order Bessel function of the first kind) is a solution to Bessel's ODE we can write it as the second solution as  \begin{align*} y_2&=J_0(x)u(x) \\ y'_2&=J_0'(x)u(x)+J_0(x)u'(x) \\ y''_2 &=J_0^{\prime\prime}u+2J_0^\prime u^\prime+J_0u^{\prime\prime} \end{align*} We can be substituted into $x^2y''+xy'+x^2y=0$ to find the value of $u(x)$ From there my main issues are finding $J_0'(x)$ and $J_0''(x)$, as well as finding the definite integral  $\int^x_1 \frac{1}{s[J_0(x)]^2} ds$. How does reduction of order give a definite integral for $u(x)$?","Use reduction of order to show that the general solution of $$xy''+y'+xy=0$$ can be written as $$y=c_1J_0(x)+c_2J_0(x) \int^x_1 \frac{1}{s[J_0(x)]^2} ds.$$ Show that if $c_2 \neq 0, y \approx c_2 \ln(x)$ as $x \rightarrow 0^+.$ Notes: First we need to transform the original ODE into Bessel's ODE $x^2y''+xy'+x^2y=0$. As $J_0(x)$ (a 0 order Bessel function of the first kind) is a solution to Bessel's ODE we can write it as the second solution as  \begin{align*} y_2&=J_0(x)u(x) \\ y'_2&=J_0'(x)u(x)+J_0(x)u'(x) \\ y''_2 &=J_0^{\prime\prime}u+2J_0^\prime u^\prime+J_0u^{\prime\prime} \end{align*} We can be substituted into $x^2y''+xy'+x^2y=0$ to find the value of $u(x)$ From there my main issues are finding $J_0'(x)$ and $J_0''(x)$, as well as finding the definite integral  $\int^x_1 \frac{1}{s[J_0(x)]^2} ds$. How does reduction of order give a definite integral for $u(x)$?",,"['ordinary-differential-equations', 'bessel-functions']"
57,What method should I apply to solve this differential equation?,What method should I apply to solve this differential equation?,,"What method should I use to solve the following differential equation? $$\frac{dx}{dt}+x=\frac{dy}{dt}$$ Where $x,y$ are functions of $t\in[0,1].$ I know I should know how to do this, but it has been a while since I had to solve any ODE's.","What method should I use to solve the following differential equation? $$\frac{dx}{dt}+x=\frac{dy}{dt}$$ Where $x,y$ are functions of $t\in[0,1].$ I know I should know how to do this, but it has been a while since I had to solve any ODE's.",,"['real-analysis', 'ordinary-differential-equations', 'derivatives']"
58,Find the explicit solution for the following differential equation.,Find the explicit solution for the following differential equation.,,"I have the following differential equation $$\frac{dx}{dt} = x^2-4$$ Separating the variables, I get $$\frac{dx}{x^2-4} = dt$$ Let us write it in partial form $$\frac{dx}{(x-2)(x+2)} = dt$$ $$\frac{dx}{4(x-2)} - \frac{dx}{4(x+2)} = dt $$ $$ \frac{dx}{(x-2)} - \frac{dx}{(x+2)} = 4dt $$ $$ \ln{|x-2|} - \ln{|x+2|} + C_1 = 4t + C_2 $$ Let $C_2 - C_1 = C$ $$ \ln{|\frac{x-2}{x+2}|} = 4t + C $$ $$e^{\ln{|\frac{x-2}{x+2}|}} = e^{4t+C}$$ $$e^{\ln{|\frac{x-2}{x+2}|}} = e^{4t}e^C$$ Let $e^C = C$ Since it is a constant $$\frac{x-2}{x+2} = Ce^{4t}$$ Let $x(0) = x_0$ $$\frac{x_0-2}{x_0+2} = C$$ Substituting for C $$\frac{x-2}{x+2} = \frac{(x_0-2)e^{4t}}{x_0+2}$$ I am rather stuck in here. The solutions manual to this question gives: $$x(t) =\frac{2[x_0 + 2 + (x_0 - 2)]e^{4t}}{x_0 + 2 - (x_0 - 2)}$$ The solutions manual does not elaborate on how it came to the solution above. How do I approach the problem? Any hints? Source: Differential Equations and Boundary Value Problems: Computing and Modeling (5th Edition)  by C. Henry Edwards (Author), David E. Penney (Author), David T. Calvis (Author) Question 5 Chapter 2.2 NOTE: $x(0)$ is not given at all so this is not a mistake. Hence, we simply do $x(0) = x_0$.","I have the following differential equation $$\frac{dx}{dt} = x^2-4$$ Separating the variables, I get $$\frac{dx}{x^2-4} = dt$$ Let us write it in partial form $$\frac{dx}{(x-2)(x+2)} = dt$$ $$\frac{dx}{4(x-2)} - \frac{dx}{4(x+2)} = dt $$ $$ \frac{dx}{(x-2)} - \frac{dx}{(x+2)} = 4dt $$ $$ \ln{|x-2|} - \ln{|x+2|} + C_1 = 4t + C_2 $$ Let $C_2 - C_1 = C$ $$ \ln{|\frac{x-2}{x+2}|} = 4t + C $$ $$e^{\ln{|\frac{x-2}{x+2}|}} = e^{4t+C}$$ $$e^{\ln{|\frac{x-2}{x+2}|}} = e^{4t}e^C$$ Let $e^C = C$ Since it is a constant $$\frac{x-2}{x+2} = Ce^{4t}$$ Let $x(0) = x_0$ $$\frac{x_0-2}{x_0+2} = C$$ Substituting for C $$\frac{x-2}{x+2} = \frac{(x_0-2)e^{4t}}{x_0+2}$$ I am rather stuck in here. The solutions manual to this question gives: $$x(t) =\frac{2[x_0 + 2 + (x_0 - 2)]e^{4t}}{x_0 + 2 - (x_0 - 2)}$$ The solutions manual does not elaborate on how it came to the solution above. How do I approach the problem? Any hints? Source: Differential Equations and Boundary Value Problems: Computing and Modeling (5th Edition)  by C. Henry Edwards (Author), David E. Penney (Author), David T. Calvis (Author) Question 5 Chapter 2.2 NOTE: $x(0)$ is not given at all so this is not a mistake. Hence, we simply do $x(0) = x_0$.",,['ordinary-differential-equations']
59,Extremising a functional with boundary conditions (Euler-Lagrange),Extremising a functional with boundary conditions (Euler-Lagrange),,"I need to determine all functions $ u(x) $ that extremise the functional: $$ I[u]= \int_{-\infty}^\infty \left[\frac{(u')^2}{2}+(1-\cos u)\right] \, dx $$ subject to the boundary conditions $$ \lim_{x \to -\infty} u(x)=0 $$ and $$ \lim_{x \to \infty} u(x) = 2\pi $$ I used the standard approach for finding the stationary points of a functional; that is, attempting to solve the Euler-Lagrange equation, but assuming I've attempted this correctly I arrive at  $$ \frac{d^2u}{dx^2} = \sin u $$ which I believe is not (easily) directly solvable, so I'm presuming there's either another way to approach this problem or I've messed up somewhere. A point in the right direction would be great, thanks in advance","I need to determine all functions $ u(x) $ that extremise the functional: $$ I[u]= \int_{-\infty}^\infty \left[\frac{(u')^2}{2}+(1-\cos u)\right] \, dx $$ subject to the boundary conditions $$ \lim_{x \to -\infty} u(x)=0 $$ and $$ \lim_{x \to \infty} u(x) = 2\pi $$ I used the standard approach for finding the stationary points of a functional; that is, attempting to solve the Euler-Lagrange equation, but assuming I've attempted this correctly I arrive at  $$ \frac{d^2u}{dx^2} = \sin u $$ which I believe is not (easily) directly solvable, so I'm presuming there's either another way to approach this problem or I've messed up somewhere. A point in the right direction would be great, thanks in advance",,"['ordinary-differential-equations', 'functional-equations', 'calculus-of-variations', 'boundary-value-problem', 'euler-lagrange-equation']"
60,Solve This Ordinary Differential Equation,Solve This Ordinary Differential Equation,,"How to solve this ordinary differential equation: $$ (y^{3}+4e^{x}y)dx + (2e^{x}+3y^2)dy = 0 $$ This is from the book Fundamentals of Differential Equations by Nagle, Saff and Sniders. The eight edition on the page 77 problem 12. I appreciate any hints.","How to solve this ordinary differential equation: $$ (y^{3}+4e^{x}y)dx + (2e^{x}+3y^2)dy = 0 $$ This is from the book Fundamentals of Differential Equations by Nagle, Saff and Sniders. The eight edition on the page 77 problem 12. I appreciate any hints.",,['ordinary-differential-equations']
61,Method of characteristics Quasilinear PDE,Method of characteristics Quasilinear PDE,,"I have the following PDE: $h_t + 3h^2 \theta h_\theta + h^3 = 0$ Therefore the characteristic equations are: $\frac{dt}{1} = \frac{d\theta}{3h^2\theta} = \frac{dh}{-h^3}$. Using 2 and 3, we can obtain: $h^3\, d\theta + 3h^2\theta \, dh = 0 \Rightarrow \phi = h^3\theta$ Using 1 and 2, we can obtain $dt = \frac{dh}{-h^3} \Rightarrow d\left(t-\frac{1}{2h^2}\right) = 0 \Rightarrow \psi = t - \frac{1}{2h^2}$ Hence, $F = \left(h^3\theta,t - \frac{1}{2h^2}\right) = 0$. Then via the implicit function theorem, $h = \frac{1}{\theta^{1/3}} f\left(t - \frac{1}{2h^2}\right)^{1/3}$. However, when I substitute this calculated $h$ solution into my PDE I get $\frac{f'}{3\theta^{1/3}f^{2/3}}$ clearly $\neq 0$. Clearly I have made a mistake somewhere, but I am not sure where? Everything seems sound to me. Any pointers would be greatly appreciated. :)","I have the following PDE: $h_t + 3h^2 \theta h_\theta + h^3 = 0$ Therefore the characteristic equations are: $\frac{dt}{1} = \frac{d\theta}{3h^2\theta} = \frac{dh}{-h^3}$. Using 2 and 3, we can obtain: $h^3\, d\theta + 3h^2\theta \, dh = 0 \Rightarrow \phi = h^3\theta$ Using 1 and 2, we can obtain $dt = \frac{dh}{-h^3} \Rightarrow d\left(t-\frac{1}{2h^2}\right) = 0 \Rightarrow \psi = t - \frac{1}{2h^2}$ Hence, $F = \left(h^3\theta,t - \frac{1}{2h^2}\right) = 0$. Then via the implicit function theorem, $h = \frac{1}{\theta^{1/3}} f\left(t - \frac{1}{2h^2}\right)^{1/3}$. However, when I substitute this calculated $h$ solution into my PDE I get $\frac{f'}{3\theta^{1/3}f^{2/3}}$ clearly $\neq 0$. Clearly I have made a mistake somewhere, but I am not sure where? Everything seems sound to me. Any pointers would be greatly appreciated. :)",,"['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
62,Periodic solutions / Level sets,Periodic solutions / Level sets,,"I have a doubt that really bothers me. I Always thought that if in ODE I find a conserved quantity whose level sets are compact, than the solutions have to be periodic. However, in the following system of ODE, things seem different: consider $$x'=-xy,\quad y'=x^2;$$ than one can easily check that $F(x, y)=x^2 + y^2$ is a conserved quantity, but since $y'\geq 0$ , then $y$ admits limit for $t\to\infty$ , and actually $x$ admits it too. So how can these solutions be periodic? If nothing is wrong, then in general we can't conclude that if level sets are compact solutions are periodic, so, in this case, what conditions could allow us to get to such a conclusion? Thank you in advance.","I have a doubt that really bothers me. I Always thought that if in ODE I find a conserved quantity whose level sets are compact, than the solutions have to be periodic. However, in the following system of ODE, things seem different: consider than one can easily check that is a conserved quantity, but since , then admits limit for , and actually admits it too. So how can these solutions be periodic? If nothing is wrong, then in general we can't conclude that if level sets are compact solutions are periodic, so, in this case, what conditions could allow us to get to such a conclusion? Thank you in advance.","x'=-xy,\quad y'=x^2; F(x, y)=x^2 + y^2 y'\geq 0 y t\to\infty x","['ordinary-differential-equations', 'limits', 'periodic-functions']"
63,Converting two nonlinear DEs into a system of four first order ODEs,Converting two nonlinear DEs into a system of four first order ODEs,,"From a from a spring pendulum system, I was able to derive the equations $$ mr''=m(\theta')^2 +mg\cos(\theta)-k(r-l) $$ and  $$r^2\theta''= 2rr'\theta'=gr\sin(\theta) $$ where $r$ is a function of time denoting the length of the spring in the picture ,so r(t)=l+x(t) where $x(t)$ measures how much the spring has been stretched from its equilibrium length $l$; $\theta$ is a function of time which measures the angle shown above; $m$ is the mass of the object at the end of the spring; $g$ is the gravitational constant; and $k$ is the spring constant. My question: I've been told that I can turn these two nonlinear equations into  a system of four first order ODEs using the newly defined terms $\rho_{\theta}=mr^2\theta'$ and $\rho_{r}=mr'$. I'm curious to know if there is a general method or systematic approach to convert nonlinear systems into simpler systems like I want to do above or is it more of an ad hoc approach. In the former case, will someone provide a reference? In the latter, will someone provide a hint for this specific example?","From a from a spring pendulum system, I was able to derive the equations $$ mr''=m(\theta')^2 +mg\cos(\theta)-k(r-l) $$ and  $$r^2\theta''= 2rr'\theta'=gr\sin(\theta) $$ where $r$ is a function of time denoting the length of the spring in the picture ,so r(t)=l+x(t) where $x(t)$ measures how much the spring has been stretched from its equilibrium length $l$; $\theta$ is a function of time which measures the angle shown above; $m$ is the mass of the object at the end of the spring; $g$ is the gravitational constant; and $k$ is the spring constant. My question: I've been told that I can turn these two nonlinear equations into  a system of four first order ODEs using the newly defined terms $\rho_{\theta}=mr^2\theta'$ and $\rho_{r}=mr'$. I'm curious to know if there is a general method or systematic approach to convert nonlinear systems into simpler systems like I want to do above or is it more of an ad hoc approach. In the former case, will someone provide a reference? In the latter, will someone provide a hint for this specific example?",,"['ordinary-differential-equations', 'dynamical-systems']"
64,Sensitivity equations with discontinuities,Sensitivity equations with discontinuities,,"I'm working on solving pharmacometric problems using ordinary differential equations of low dimensionality. For example, a common simple model looks like this: $$\dot{A} = -KA$$ where $A$ is the amount of a drug in blood plasma, and $K$ is the elimination rate, in units of 1/time. It is desired to know how $A$ varies with changes in $K$. Define $G = \partial A/\partial K$. Then we can write a ""sensitivity equation"": $$\dot{G} = -KG - A$$ This is all well known, but what is not well known is how to handle discontinuities at particular time points, such as doses $A = A + D$, or simply setting $A$ to zero $A = 0$. What do we do with $G$ at those time points? I've tentatively concluded that adding something to $A$ does not affect $G$, but multiplying $A$ by some factor, like $0$, should also be applied to $G$. I would appreciate any insight I may be lacking on this. For what it's worth, here is some documentation on CVODES, by Serban and Hindmarsh:","I'm working on solving pharmacometric problems using ordinary differential equations of low dimensionality. For example, a common simple model looks like this: $$\dot{A} = -KA$$ where $A$ is the amount of a drug in blood plasma, and $K$ is the elimination rate, in units of 1/time. It is desired to know how $A$ varies with changes in $K$. Define $G = \partial A/\partial K$. Then we can write a ""sensitivity equation"": $$\dot{G} = -KG - A$$ This is all well known, but what is not well known is how to handle discontinuities at particular time points, such as doses $A = A + D$, or simply setting $A$ to zero $A = 0$. What do we do with $G$ at those time points? I've tentatively concluded that adding something to $A$ does not affect $G$, but multiplying $A$ by some factor, like $0$, should also be applied to $G$. I would appreciate any insight I may be lacking on this. For what it's worth, here is some documentation on CVODES, by Serban and Hindmarsh:",,['ordinary-differential-equations']
65,A hot metal block in the cool sea,A hot metal block in the cool sea,,"(I just want to double check that my answer is correct.) A metal block at the boiling temperature of water is submerged in the sea where the water temperature is $20$°C. If after $15$ minutes the temperature of the block falls to $68$°C, how long will its temperature stay at between $60.5$ and $59.5$°C? Using the formula $$T=T_a+(T_0-T_a)e^{-kt}$$ where $T_a$ is the ambient temperature, $T_0$ is the initial temperature of the object and $T$ is its temperature at a given time $t$, I got $20$ minutes. Is that correct?","(I just want to double check that my answer is correct.) A metal block at the boiling temperature of water is submerged in the sea where the water temperature is $20$°C. If after $15$ minutes the temperature of the block falls to $68$°C, how long will its temperature stay at between $60.5$ and $59.5$°C? Using the formula $$T=T_a+(T_0-T_a)e^{-kt}$$ where $T_a$ is the ambient temperature, $T_0$ is the initial temperature of the object and $T$ is its temperature at a given time $t$, I got $20$ minutes. Is that correct?",,['ordinary-differential-equations']
66,Uniqueness of the solution of an ODE with zero integral mean and convex $f$,Uniqueness of the solution of an ODE with zero integral mean and convex,f,"Consider the problem on $(0,1)$   $$u(x)'=f(u(x))$$   $$\int_0^1u(x)dx=0$$   where $f$ is $C^1$ on the whole real line, strictly convex, has its minimum in $0$, and $f(0)<0$.   Prove that the solution is unique. I tried two ways: to reduce the problem to a Cauchy problem, showing that the condition on the mean, together with the properties of $f$, implies some 'initial-value' condition; or to apply the contraction principle (i.e. showing that the Fredholm operator associated to $f$ is a contraction). But I have not been able to follow any of these ways. Can someone give a help, also explaining, if possible, useful strategies to apply in similar cases (i.e. 'modified' Cauchy problems)? Thank you in advance.","Consider the problem on $(0,1)$   $$u(x)'=f(u(x))$$   $$\int_0^1u(x)dx=0$$   where $f$ is $C^1$ on the whole real line, strictly convex, has its minimum in $0$, and $f(0)<0$.   Prove that the solution is unique. I tried two ways: to reduce the problem to a Cauchy problem, showing that the condition on the mean, together with the properties of $f$, implies some 'initial-value' condition; or to apply the contraction principle (i.e. showing that the Fredholm operator associated to $f$ is a contraction). But I have not been able to follow any of these ways. Can someone give a help, also explaining, if possible, useful strategies to apply in similar cases (i.e. 'modified' Cauchy problems)? Thank you in advance.",,"['ordinary-differential-equations', 'convex-analysis']"
67,Loxodromics (curves with fixed angle with meridians) of a revolution surface,Loxodromics (curves with fixed angle with meridians) of a revolution surface,,"I am trying to find the curves with a fixed angle $\phi$ with meridians for the revolution surface given by the revolution of the graph of $z=\frac{1}{x}$ around the $z$ axis. The surface is easily parametrized as $$(x,\theta)\mapsto(x \cos \theta, x\sin \theta,\frac{1}{x}) \qquad x\in \mathbb R_{>0},\theta \in (0,2\pi)$$ Now, some tedious computations (I hope they are correct) show that the first and the second fundamental form for this parametrization are $$G_{I}=\begin{pmatrix}1+\frac{1}{x^4} &0 \\ 0 & x^2 \end{pmatrix} \qquad G_{II}=\frac{1}{\sqrt{x^4+1}}\begin{pmatrix}\frac{1}{x} & 0 \\ 0 & -x \end{pmatrix}$$ Recall that the angle $\phi$ between two $C^1$ curves $\gamma_1,\gamma_2$ is given by $$\cos \phi=\frac{\gamma_1'^TG_I \gamma_2'}{\sqrt{\gamma_1'^T G_I \gamma_1'}\sqrt{\gamma_2'^TG_I\gamma_2'}}$$ Since we are looking for curves $(x(t),\theta(t))^T$ with constant angle with meridians, we want that $$\frac{\begin{pmatrix}1 & 0 \end{pmatrix}\begin{pmatrix} 1+\frac{1}{x^4(t)} &0 \\ 0 & x^2(t)\end{pmatrix}\begin{pmatrix} x'(t) \\ \theta'(t) \end{pmatrix}}{\sqrt{\begin{pmatrix} x(t) & \theta(t)\end{pmatrix}\begin{pmatrix}1+\frac{1}{x^4(t)} &0 \\ 0 & x^2(t) \end{pmatrix}\begin{pmatrix} x'(t)\\ \theta'(t) \end{pmatrix}}\sqrt{\begin{pmatrix} 1 & 0\end{pmatrix}\begin{pmatrix}1+\frac{1}{x^4(t)} &0 \\ 0 & x^2(t) \end{pmatrix}\begin{pmatrix} 1 \\ 0\end{pmatrix}}}=K$$ which, dropping the dependency on $t$, yields the differential equation $$x'\left (1+\frac{1}{x^4}\right )=K\sqrt{1+\frac{1}{x^4}}\sqrt{x'^2\left ( 1+\frac{1}{x^4}\right)+x^2\theta'^2}$$ Not looking very good. Through a few manipulations, I brought this to the form $$x'(t)=K\sqrt{x'^2+\frac{x^6\theta'^2}{x^4+1}}$$ Since this is an exercise from an exam, I would expect the solution to have a nice closed form, but maybe I am mistaken. Is there any elementary way of solving such differential equation or of finding an explicit equation for the loxodromics?","I am trying to find the curves with a fixed angle $\phi$ with meridians for the revolution surface given by the revolution of the graph of $z=\frac{1}{x}$ around the $z$ axis. The surface is easily parametrized as $$(x,\theta)\mapsto(x \cos \theta, x\sin \theta,\frac{1}{x}) \qquad x\in \mathbb R_{>0},\theta \in (0,2\pi)$$ Now, some tedious computations (I hope they are correct) show that the first and the second fundamental form for this parametrization are $$G_{I}=\begin{pmatrix}1+\frac{1}{x^4} &0 \\ 0 & x^2 \end{pmatrix} \qquad G_{II}=\frac{1}{\sqrt{x^4+1}}\begin{pmatrix}\frac{1}{x} & 0 \\ 0 & -x \end{pmatrix}$$ Recall that the angle $\phi$ between two $C^1$ curves $\gamma_1,\gamma_2$ is given by $$\cos \phi=\frac{\gamma_1'^TG_I \gamma_2'}{\sqrt{\gamma_1'^T G_I \gamma_1'}\sqrt{\gamma_2'^TG_I\gamma_2'}}$$ Since we are looking for curves $(x(t),\theta(t))^T$ with constant angle with meridians, we want that $$\frac{\begin{pmatrix}1 & 0 \end{pmatrix}\begin{pmatrix} 1+\frac{1}{x^4(t)} &0 \\ 0 & x^2(t)\end{pmatrix}\begin{pmatrix} x'(t) \\ \theta'(t) \end{pmatrix}}{\sqrt{\begin{pmatrix} x(t) & \theta(t)\end{pmatrix}\begin{pmatrix}1+\frac{1}{x^4(t)} &0 \\ 0 & x^2(t) \end{pmatrix}\begin{pmatrix} x'(t)\\ \theta'(t) \end{pmatrix}}\sqrt{\begin{pmatrix} 1 & 0\end{pmatrix}\begin{pmatrix}1+\frac{1}{x^4(t)} &0 \\ 0 & x^2(t) \end{pmatrix}\begin{pmatrix} 1 \\ 0\end{pmatrix}}}=K$$ which, dropping the dependency on $t$, yields the differential equation $$x'\left (1+\frac{1}{x^4}\right )=K\sqrt{1+\frac{1}{x^4}}\sqrt{x'^2\left ( 1+\frac{1}{x^4}\right)+x^2\theta'^2}$$ Not looking very good. Through a few manipulations, I brought this to the form $$x'(t)=K\sqrt{x'^2+\frac{x^6\theta'^2}{x^4+1}}$$ Since this is an exercise from an exam, I would expect the solution to have a nice closed form, but maybe I am mistaken. Is there any elementary way of solving such differential equation or of finding an explicit equation for the loxodromics?",,"['ordinary-differential-equations', 'differential-geometry', 'surfaces', 'curves']"
68,ordinary differential equation of third order using substitution,ordinary differential equation of third order using substitution,,How one can solve ODE in the following form? $$y''' y +(y'')^{2} =0$$ It looks like some kind of substitution. I did try some substitution but they were useless. thanks in advance.,How one can solve ODE in the following form? $$y''' y +(y'')^{2} =0$$ It looks like some kind of substitution. I did try some substitution but they were useless. thanks in advance.,,['ordinary-differential-equations']
69,Establish the differential equation of SHM from energy conservation principle.,Establish the differential equation of SHM from energy conservation principle.,,I really don't know where to start. $$ \frac{1}{2} mv^2 + \frac{1}{2} kx^2 = E $$ is the conservation of energy equation. But how should I proceed and how will I get the differential equation of Simple Harmonic Motion (SHM) from that. Can somebody help please?,I really don't know where to start. $$ \frac{1}{2} mv^2 + \frac{1}{2} kx^2 = E $$ is the conservation of energy equation. But how should I proceed and how will I get the differential equation of Simple Harmonic Motion (SHM) from that. Can somebody help please?,,['ordinary-differential-equations']
70,How to justify differentiating an asymptotic series in WKB method,How to justify differentiating an asymptotic series in WKB method,,"Given a second-order linear ordinary differential equation, \begin{equation} \epsilon^2 \frac{d^2y}{dx^2} = Q(x) y(x), \tag{1}\label{1} \end{equation} where $\epsilon$ is regarded as a small positive number, a typical explanation of the WKB method (e.g. Ch. 10 of Bender and Orszag, Advanced Mathematical Methods for Scientists and Engineers, 1978) starts with writing $y(x)$ as \begin{equation} y(x) = \exp\left(\frac{1}{\delta}S(x,\delta)\right), \tag{2}\label{2} \end{equation}  and assuming that $S(x,\delta)$ has an asymptotic series in $\delta$, \begin{align} S(x,\delta) \sim& \sum_{n=0}^{\infty} \delta^n S_n(x) & \text{ as } \delta\rightarrow&0+. \tag{3}\label{3} \end{align} Then, by substituting eqs. (\ref{2}) and (\ref{3}) into eq. (\ref{1}) and dividing both sides by $\exp(S/\delta)$, it is claimed that \begin{align} \frac{\epsilon^2}{\delta^2}  \left[\sum_{n=0}^{\infty} \delta^n \frac{d S_n}{dx}\right]^2 +\frac{\epsilon^2}{\delta} \sum_{n=0}^{\infty} \delta^n \frac{d^2 S_n}{dx^2} \sim& Q(x) & \text{ as } \delta\rightarrow&0+. \tag{4}\label{4} \end{align} From here, the argument proceeds that we can set $\delta = \epsilon$ from dominant balance, and that we can equate the like powers of $\epsilon$ on both sides to yield differential equations for the coefficient functions $\{S_n(x)\}$ as \begin{align} \left(\frac{dS_0}{dx}\right)^2 =& Q(x),  \tag{5}\label{5}\\ 2 \frac{dS_0}{dx}\frac{dS_1}{dx}+\frac{d^2S_0}{dx^2}=&0, \tag{6}\label{6}\\ \dots \end{align} and these equations are solved one after another. Here is my question. How is it justified to differentiate the asymptotic series in eq. (\ref{3}) to derive eq. (\ref{4})? I wondered that maybe we can use the fact that $y(x)$ is a solution of eq. (\ref{1}) in some way, but I don't find a way so far. I read that the derivatives of both sides of an asymptotic relation does not always construct another asymptotic relation, and I guess that even if  \begin{align} f(x,\epsilon)\sim& g_0(x) + g_1(x) \epsilon + g_2(x) \epsilon^2 +\cdots& \text{ as } \epsilon\rightarrow& 0, \tag{7}\label{7} \end{align}  uniformly in some domain of $x$, it does not necessarily follow that  \begin{align} \frac{df}{dx}(x,\epsilon)  \sim& \frac{dg_0}{dx}(x)  + \frac{dg_1}{dx}(x)\epsilon + \frac{dg_2}{dx}(x) \epsilon^2 +\cdots& \text{ as }\epsilon \rightarrow& 0. \tag{8}\label{8} \end{align} Or, does it follow (under some conditions)?","Given a second-order linear ordinary differential equation, \begin{equation} \epsilon^2 \frac{d^2y}{dx^2} = Q(x) y(x), \tag{1}\label{1} \end{equation} where $\epsilon$ is regarded as a small positive number, a typical explanation of the WKB method (e.g. Ch. 10 of Bender and Orszag, Advanced Mathematical Methods for Scientists and Engineers, 1978) starts with writing $y(x)$ as \begin{equation} y(x) = \exp\left(\frac{1}{\delta}S(x,\delta)\right), \tag{2}\label{2} \end{equation}  and assuming that $S(x,\delta)$ has an asymptotic series in $\delta$, \begin{align} S(x,\delta) \sim& \sum_{n=0}^{\infty} \delta^n S_n(x) & \text{ as } \delta\rightarrow&0+. \tag{3}\label{3} \end{align} Then, by substituting eqs. (\ref{2}) and (\ref{3}) into eq. (\ref{1}) and dividing both sides by $\exp(S/\delta)$, it is claimed that \begin{align} \frac{\epsilon^2}{\delta^2}  \left[\sum_{n=0}^{\infty} \delta^n \frac{d S_n}{dx}\right]^2 +\frac{\epsilon^2}{\delta} \sum_{n=0}^{\infty} \delta^n \frac{d^2 S_n}{dx^2} \sim& Q(x) & \text{ as } \delta\rightarrow&0+. \tag{4}\label{4} \end{align} From here, the argument proceeds that we can set $\delta = \epsilon$ from dominant balance, and that we can equate the like powers of $\epsilon$ on both sides to yield differential equations for the coefficient functions $\{S_n(x)\}$ as \begin{align} \left(\frac{dS_0}{dx}\right)^2 =& Q(x),  \tag{5}\label{5}\\ 2 \frac{dS_0}{dx}\frac{dS_1}{dx}+\frac{d^2S_0}{dx^2}=&0, \tag{6}\label{6}\\ \dots \end{align} and these equations are solved one after another. Here is my question. How is it justified to differentiate the asymptotic series in eq. (\ref{3}) to derive eq. (\ref{4})? I wondered that maybe we can use the fact that $y(x)$ is a solution of eq. (\ref{1}) in some way, but I don't find a way so far. I read that the derivatives of both sides of an asymptotic relation does not always construct another asymptotic relation, and I guess that even if  \begin{align} f(x,\epsilon)\sim& g_0(x) + g_1(x) \epsilon + g_2(x) \epsilon^2 +\cdots& \text{ as } \epsilon\rightarrow& 0, \tag{7}\label{7} \end{align}  uniformly in some domain of $x$, it does not necessarily follow that  \begin{align} \frac{df}{dx}(x,\epsilon)  \sim& \frac{dg_0}{dx}(x)  + \frac{dg_1}{dx}(x)\epsilon + \frac{dg_2}{dx}(x) \epsilon^2 +\cdots& \text{ as }\epsilon \rightarrow& 0. \tag{8}\label{8} \end{align} Or, does it follow (under some conditions)?",,"['ordinary-differential-equations', 'asymptotics', 'mathematical-physics', 'perturbation-theory', 'formal-power-series']"
71,How to show global instability of a system?,How to show global instability of a system?,,"Consider the system: $ \dot x_1 = x_2 $ $ \dot x_2 = {x_1}^2 $ It is obviously unstable, I say that because by inspection the derivative of $x_2$ can never be negative. How can we concretely show that, in the general case? If we were to prove stability we can use Lyapunov, but we cannot use that in the opposite way.","Consider the system: $ \dot x_1 = x_2 $ $ \dot x_2 = {x_1}^2 $ It is obviously unstable, I say that because by inspection the derivative of $x_2$ can never be negative. How can we concretely show that, in the general case? If we were to prove stability we can use Lyapunov, but we cannot use that in the opposite way.",,"['ordinary-differential-equations', 'dynamical-systems', 'control-theory']"
72,Monotonic property of solution of a linear (second-order) differential equation,Monotonic property of solution of a linear (second-order) differential equation,,"Consider the following differential equation \begin{equation} y''+q(t) y=0 , \end{equation} where q(t) is a continuous function $\leq 0\;\; \forall t \in \mathbb{R}$. I'm trying to prove that each non-constant solution such that $y(0)=0$ is strictly monotone. I have tried integrating between 0 and t but in this way I can't show anything useful in fact by using the weighted average theorem I get \begin{equation} y'(t)=y'(0)-y(\xi_t)\int_{0}^{t}q(x)dx . \end{equation} I'm wondering if I should continue to prove that $y'>$ or $<0$ in a dense subset of $\mathbb{R}$ or if I should use a different approach.","Consider the following differential equation \begin{equation} y''+q(t) y=0 , \end{equation} where q(t) is a continuous function $\leq 0\;\; \forall t \in \mathbb{R}$. I'm trying to prove that each non-constant solution such that $y(0)=0$ is strictly monotone. I have tried integrating between 0 and t but in this way I can't show anything useful in fact by using the weighted average theorem I get \begin{equation} y'(t)=y'(0)-y(\xi_t)\int_{0}^{t}q(x)dx . \end{equation} I'm wondering if I should continue to prove that $y'>$ or $<0$ in a dense subset of $\mathbb{R}$ or if I should use a different approach.",,"['ordinary-differential-equations', 'monotone-functions', 'monotone-class-theorem']"
73,"PDE $u_t+u^2u_x=0$ how do we deduce/know that the second shock line starts at the $(x=3,t=3)$ point",PDE  how do we deduce/know that the second shock line starts at the  point,"u_t+u^2u_x=0 (x=3,t=3)","I am analyzing solution the below PDE. This is the example 1.14 from the textbook (p.19 http://people.uncw.edu/hermanr/pde1/PDEbook/PDE_Main.pdf ) My question how do we deduce/know that the second shock line starts at the point  $(x=3,t=3)$? $$u_t+u^2u_x=0, \ \ |x|< \infty , \ \ t>0 $$ with boundary condition  $$u(x,0)= \left\{\begin{matrix} 0 &   x\leq 0,  \\  1 & 0<x<2,  \\  0 & x \geq 2 \end{matrix}\right.$$ There is a picture of the characteristic lines","I am analyzing solution the below PDE. This is the example 1.14 from the textbook (p.19 http://people.uncw.edu/hermanr/pde1/PDEbook/PDE_Main.pdf ) My question how do we deduce/know that the second shock line starts at the point  $(x=3,t=3)$? $$u_t+u^2u_x=0, \ \ |x|< \infty , \ \ t>0 $$ with boundary condition  $$u(x,0)= \left\{\begin{matrix} 0 &   x\leq 0,  \\  1 & 0<x<2,  \\  0 & x \geq 2 \end{matrix}\right.$$ There is a picture of the characteristic lines",,"['ordinary-differential-equations', 'partial-differential-equations']"
74,Prove using induction the following equation is true.,Prove using induction the following equation is true.,,If $$(1-x^2)\frac{dy}{dx} - xy - 1 = 0$$ Using induction prove the following for any positive integer n$$(1-x^2)\frac{d^{n+2}y}{dx^{n+2}} - (2n+3)x\frac{d^{n+1}y}{dx^{n+1}} - (n+1)^2\frac{d^ny}{dx^n} = 0$$ I know Leibtniz can be used to solve it easier but I need the proof to use induction.,If $$(1-x^2)\frac{dy}{dx} - xy - 1 = 0$$ Using induction prove the following for any positive integer n$$(1-x^2)\frac{d^{n+2}y}{dx^{n+2}} - (2n+3)x\frac{d^{n+1}y}{dx^{n+1}} - (n+1)^2\frac{d^ny}{dx^n} = 0$$ I know Leibtniz can be used to solve it easier but I need the proof to use induction.,,"['calculus', 'ordinary-differential-equations', 'derivatives', 'induction', 'implicit-differentiation']"
75,Find the solutions to this ODE for an arbitrary $\lambda$,Find the solutions to this ODE for an arbitrary,\lambda,"For $\lambda \in \mathbb{R}$, consider the boundary value problem $$x^2\frac{d^2y}{dx^2}+2x\frac{dy}{dx}+\lambda y=0, \quad x\in [1,2], \qquad y(1)=y(2)=0$$ Which of the following statements is true? there exist a $\lambda_0 \in \mathbb{R}$ such that problem $P_{\lambda}$ has a non trivial solution for $\lambda>\lambda_0$. $\{\lambda \in \mathbb{R}: P_{\lambda}$ has a non trivial solution$\}$ is a dense subset of $\mathbb{R}$. For any continuous function $f:[1,2] \to \mathbb{R}$ with $f(x)\neq 0$ for some $x \in [1,2]$ there exist a solution u of Problem for some   $\lambda \in \mathbb{R}$ such that $\int\limits_1 ^2 fu \neq 0$ there exist a $\lambda \in \mathbb{R}$ such that Problem $P_{\lambda}$  has two linearly independent solutions. As- c I know (4) is incorrect as Wronskian of the system comes out 0. But how to look for others?","For $\lambda \in \mathbb{R}$, consider the boundary value problem $$x^2\frac{d^2y}{dx^2}+2x\frac{dy}{dx}+\lambda y=0, \quad x\in [1,2], \qquad y(1)=y(2)=0$$ Which of the following statements is true? there exist a $\lambda_0 \in \mathbb{R}$ such that problem $P_{\lambda}$ has a non trivial solution for $\lambda>\lambda_0$. $\{\lambda \in \mathbb{R}: P_{\lambda}$ has a non trivial solution$\}$ is a dense subset of $\mathbb{R}$. For any continuous function $f:[1,2] \to \mathbb{R}$ with $f(x)\neq 0$ for some $x \in [1,2]$ there exist a solution u of Problem for some   $\lambda \in \mathbb{R}$ such that $\int\limits_1 ^2 fu \neq 0$ there exist a $\lambda \in \mathbb{R}$ such that Problem $P_{\lambda}$  has two linearly independent solutions. As- c I know (4) is incorrect as Wronskian of the system comes out 0. But how to look for others?",,['ordinary-differential-equations']
76,Finding the inverse laplace transform using complex analysis.,Finding the inverse laplace transform using complex analysis.,,I've been able to prove simple laplace transforms like $\dfrac {1}{(s+a)} $ quite easily but what about $\dfrac {1}{(s+a)^3+b^2} $ this does not seem easy to do since you cannot easily compute the residues of the denominator. How can this be done???,I've been able to prove simple laplace transforms like $\dfrac {1}{(s+a)} $ quite easily but what about $\dfrac {1}{(s+a)^3+b^2} $ this does not seem easy to do since you cannot easily compute the residues of the denominator. How can this be done???,,"['integration', 'complex-analysis', 'ordinary-differential-equations', 'definite-integrals', 'laplace-transform']"
77,Find the value of $y(1)$ of the ODE $y'+y=|x|$.,Find the value of  of the ODE .,y(1) y'+y=|x|,"Let $y$ be the solution of $$y'+y=|x|$$ for $x\in\mathbb{R}$ and $y(-1)=0$. Then the value of $y(1)$ is $\frac{2}{e}-\frac{2}{e^2}$ $\frac{2}{e}-2e^2$ $2-\frac{2}{e}$ $2-2e$ I don't know what to do with the absolute value function in this problem. So I started like the regular first order equation by calculating integrating factor, and got $$ye^x=\int |x|e^x dx+C.$$ Now I got stuck as how to tackle the absolute value function? Help me to solve this. Thanks!","Let $y$ be the solution of $$y'+y=|x|$$ for $x\in\mathbb{R}$ and $y(-1)=0$. Then the value of $y(1)$ is $\frac{2}{e}-\frac{2}{e^2}$ $\frac{2}{e}-2e^2$ $2-\frac{2}{e}$ $2-2e$ I don't know what to do with the absolute value function in this problem. So I started like the regular first order equation by calculating integrating factor, and got $$ye^x=\int |x|e^x dx+C.$$ Now I got stuck as how to tackle the absolute value function? Help me to solve this. Thanks!",,['ordinary-differential-equations']
78,Solve a nonlinear system of coupled differential equations,Solve a nonlinear system of coupled differential equations,,"I have this system of differential equations which describes the motion of a missile launcher model with 5 degrees of freedom: (1)$$(m_w +m_v +m_p)\ddot{y}_w - (m_v + m_p)h_v\ddot{\vartheta}_w \sin(\vartheta_w)  + m_p \xi_p \ddot{\vartheta}_v \cos(\vartheta_v) \\ - (m_v + m_p)h_v\dot{\vartheta}^2_w \cos(\vartheta_w)  - m_p\xi_p \dot{\vartheta}^2_v \sin(\vartheta_v) + m_p \dot{\xi}_p \dot{\vartheta}_v \cos(\vartheta_v)  +k_{w11}\lambda_{w11} \\ + k_{w12}\lambda_{w12} -c_{w11}\dot{\lambda}_{w11} -c_{w12}\dot{\lambda}_{w12} = -(m_w +m_v +m_p)g$$ (2)$$[I_w+I_{p_{zp}}+I_v+(m_v+m_p)h^2_v]\ddot{\vartheta}_w  + [I_{p_{zp}}+m_p h_v \xi_p(\cos\vartheta_w\sin\vartheta_v \\ - \sin\vartheta_w\cos\vartheta_v)]\ddot\vartheta_v  - (m_v + m_p)h_v\ddot{y}_w \sin\vartheta_w - m_p h_v \ddot{\xi}_p(\cos\vartheta_w\cos\vartheta_v \\ + \sin\vartheta_w\sin\vartheta_v)  + 2m_p h_v \dot{\xi}_p \dot{\vartheta}_w \sin\vartheta_w\cos\vartheta_v \\ -2m_p h_v \dot{\xi}_p \dot{\vartheta}_v \sin\vartheta_w\cos\vartheta_v  +m_p h_v \xi_p \dot{\vartheta}^2_v(\cos\vartheta_w\cos\vartheta_v + \sin\vartheta_w\sin\vartheta_v)\\ +k_{w11}l_{w1}\lambda_{w11} + k_{w12}l_{w2}\lambda_{w12} - k_{w31}\lambda_{w31}  -c_{w11}l_{w1}\dot{\lambda}_{w11}+c_{w12}l_{w2}\dot{\lambda}_{w12} \\ + c_{w31}\dot{\lambda}_{w31} = (m_v + m_p)gh_v\sin(\vartheta_w+\vartheta_{wst})$$ (3)$$(I_v + I_{p_{zp}} + m_p\xi^2_p)\ddot{\vartheta}_v + m_p \xi_p \ddot{y}_w\cos\vartheta_v  + [I_v + I_{p_{zp}} + m_p h_v \xi_p(\cos\vartheta_w\sin\vartheta_v \\ - \sin\vartheta_w\cos\vartheta_v)]\ddot{\vartheta}_w + 2m_p\xi_p\dot{\xi}_p\dot{\vartheta}_v  + m_p h_v\xi_p\dot{\vartheta}^2_w(\sin\vartheta_w\sin\vartheta_v \\ + \cos\vartheta_w\cos\vartheta_v)+k_{w31}\lambda_{w31}-c_{w31}\dot{\lambda}_{w31} =  -m_pg\xi_p\cos(\vartheta_v + \vartheta_{vst})$$ (4)$$ m_p\ddot{\xi}_p-m_ph_v\ddot{\vartheta}_v(\cos\vartheta_w\cos\vartheta_v  + \sin\vartheta_w\sin\vartheta_v) \\ + m_p\ddot{y}_w\sin\vartheta_w + m_ph_v\dot{\vartheta}^2_w(\sin\vartheta_w\cos\vartheta_v - \cos\vartheta_w\sin\vartheta_v) \\ + m_p\xi_p\dot{\vartheta}^2_v = -m_pg\sin(\vartheta_v + \vartheta_{vst}) + P_p$$ (5)$$I_{p_{xp}}\ddot{\varphi}_p = M_p$$. The scheme of the model is this: scheme of the launcher . I know all the coefficients but I want to see the trend in the first 2 seconds of the variables $y_w, \vartheta_w, \vartheta_v, \xi_p, \phi_p$. Unfortunately I tried with Matlab ode45 but the system is coupled and nonlinear, can you suggest me a numerical method, a code, or anything that may help me solve this system? After understanding the procedure I can do some more advanced calculations but I need to figure out the approach to solve it first. Thanks, really thanks, to anybody who wants to help. Loris EDIT: same equations but with numbers and with linearised sines and cosines so that you can clearly see where is the difficulty: (1.1)$$\ddot{y}_w = 0.0917\ddot{\vartheta_w}\vartheta_w -0.1667\xi_p\ddot{\vartheta_w} +0.0917\dot{\vartheta_w}^2 +      0.1667\xi_p\dot{\vartheta_v}^2\vartheta_v -0.1667\dot{\xi_p}\dot{\vartheta_v} -4166.67(2y_w +0.3916\vartheta_w)+     4.1667*(2\dot{y}_w +0.3916\dot{\vartheta_w}) + 9.81$$ (2.1)$$\ddot{\vartheta_w} = -0.3217\ddot{\vartheta_v} -0.4531\xi_p\ddot{\vartheta_v}(\vartheta_v-\vartheta_w)     +1.2082\ddot{y}_w\vartheta_w +0.4531\ddot{\xi}_p(1+\vartheta_w\vartheta_v)      -0.9602\dot{\xi}_p\vartheta_w(\vartheta_w-\vartheta_v) -0.4531\xi_p\dot{\vartheta_v}^2(1+\vartheta_w\vartheta_v)     -4.393\cdot 10^4y_w -1.72\cdot 10^4\vartheta_w +366(\vartheta_v-\vartheta_w) +21.5\dot{y}_w +21.78\dot{\vartheta_w} +36.6(\dot{\vartheta_w}-\dot{\vartheta_v}) +11.8529\vartheta_w$$ (3.1)$$\ddot{\vartheta_v} = -3.9734(\xi_p^2\ddot{\vartheta_v}+\xi_p\ddot{y}_w) -\ddot{\vartheta_w}     -0.8197\xi_p(\vartheta_v-\vartheta_w)\ddot{\vartheta_w} -7.9468\xi_p\dot{\xi}_p\dot{\vartheta_v} -0.8197\xi_p\dot{\vartheta_w}^2(1+\vartheta_w\vartheta_v)     -662.23(\vartheta_v-\vartheta_w) -66.223(\dot{\vartheta_v}-\dot{\vartheta_w}) +38.98\xi_p$$ (4.1) $$\ddot{\xi}_p = 0.2063\ddot{\vartheta_v}(1+\vartheta_w\vartheta_v) -\ddot{y}_w\vartheta_w     -0.2063\dot{\vartheta_w}^2(\vartheta_w-\vartheta_v) -\xi_p\dot{\vartheta_v}^2 -9.81\vartheta_v +333$$ (5.1) $$\ddot{\phi}_p = 81$$","I have this system of differential equations which describes the motion of a missile launcher model with 5 degrees of freedom: (1)$$(m_w +m_v +m_p)\ddot{y}_w - (m_v + m_p)h_v\ddot{\vartheta}_w \sin(\vartheta_w)  + m_p \xi_p \ddot{\vartheta}_v \cos(\vartheta_v) \\ - (m_v + m_p)h_v\dot{\vartheta}^2_w \cos(\vartheta_w)  - m_p\xi_p \dot{\vartheta}^2_v \sin(\vartheta_v) + m_p \dot{\xi}_p \dot{\vartheta}_v \cos(\vartheta_v)  +k_{w11}\lambda_{w11} \\ + k_{w12}\lambda_{w12} -c_{w11}\dot{\lambda}_{w11} -c_{w12}\dot{\lambda}_{w12} = -(m_w +m_v +m_p)g$$ (2)$$[I_w+I_{p_{zp}}+I_v+(m_v+m_p)h^2_v]\ddot{\vartheta}_w  + [I_{p_{zp}}+m_p h_v \xi_p(\cos\vartheta_w\sin\vartheta_v \\ - \sin\vartheta_w\cos\vartheta_v)]\ddot\vartheta_v  - (m_v + m_p)h_v\ddot{y}_w \sin\vartheta_w - m_p h_v \ddot{\xi}_p(\cos\vartheta_w\cos\vartheta_v \\ + \sin\vartheta_w\sin\vartheta_v)  + 2m_p h_v \dot{\xi}_p \dot{\vartheta}_w \sin\vartheta_w\cos\vartheta_v \\ -2m_p h_v \dot{\xi}_p \dot{\vartheta}_v \sin\vartheta_w\cos\vartheta_v  +m_p h_v \xi_p \dot{\vartheta}^2_v(\cos\vartheta_w\cos\vartheta_v + \sin\vartheta_w\sin\vartheta_v)\\ +k_{w11}l_{w1}\lambda_{w11} + k_{w12}l_{w2}\lambda_{w12} - k_{w31}\lambda_{w31}  -c_{w11}l_{w1}\dot{\lambda}_{w11}+c_{w12}l_{w2}\dot{\lambda}_{w12} \\ + c_{w31}\dot{\lambda}_{w31} = (m_v + m_p)gh_v\sin(\vartheta_w+\vartheta_{wst})$$ (3)$$(I_v + I_{p_{zp}} + m_p\xi^2_p)\ddot{\vartheta}_v + m_p \xi_p \ddot{y}_w\cos\vartheta_v  + [I_v + I_{p_{zp}} + m_p h_v \xi_p(\cos\vartheta_w\sin\vartheta_v \\ - \sin\vartheta_w\cos\vartheta_v)]\ddot{\vartheta}_w + 2m_p\xi_p\dot{\xi}_p\dot{\vartheta}_v  + m_p h_v\xi_p\dot{\vartheta}^2_w(\sin\vartheta_w\sin\vartheta_v \\ + \cos\vartheta_w\cos\vartheta_v)+k_{w31}\lambda_{w31}-c_{w31}\dot{\lambda}_{w31} =  -m_pg\xi_p\cos(\vartheta_v + \vartheta_{vst})$$ (4)$$ m_p\ddot{\xi}_p-m_ph_v\ddot{\vartheta}_v(\cos\vartheta_w\cos\vartheta_v  + \sin\vartheta_w\sin\vartheta_v) \\ + m_p\ddot{y}_w\sin\vartheta_w + m_ph_v\dot{\vartheta}^2_w(\sin\vartheta_w\cos\vartheta_v - \cos\vartheta_w\sin\vartheta_v) \\ + m_p\xi_p\dot{\vartheta}^2_v = -m_pg\sin(\vartheta_v + \vartheta_{vst}) + P_p$$ (5)$$I_{p_{xp}}\ddot{\varphi}_p = M_p$$. The scheme of the model is this: scheme of the launcher . I know all the coefficients but I want to see the trend in the first 2 seconds of the variables $y_w, \vartheta_w, \vartheta_v, \xi_p, \phi_p$. Unfortunately I tried with Matlab ode45 but the system is coupled and nonlinear, can you suggest me a numerical method, a code, or anything that may help me solve this system? After understanding the procedure I can do some more advanced calculations but I need to figure out the approach to solve it first. Thanks, really thanks, to anybody who wants to help. Loris EDIT: same equations but with numbers and with linearised sines and cosines so that you can clearly see where is the difficulty: (1.1)$$\ddot{y}_w = 0.0917\ddot{\vartheta_w}\vartheta_w -0.1667\xi_p\ddot{\vartheta_w} +0.0917\dot{\vartheta_w}^2 +      0.1667\xi_p\dot{\vartheta_v}^2\vartheta_v -0.1667\dot{\xi_p}\dot{\vartheta_v} -4166.67(2y_w +0.3916\vartheta_w)+     4.1667*(2\dot{y}_w +0.3916\dot{\vartheta_w}) + 9.81$$ (2.1)$$\ddot{\vartheta_w} = -0.3217\ddot{\vartheta_v} -0.4531\xi_p\ddot{\vartheta_v}(\vartheta_v-\vartheta_w)     +1.2082\ddot{y}_w\vartheta_w +0.4531\ddot{\xi}_p(1+\vartheta_w\vartheta_v)      -0.9602\dot{\xi}_p\vartheta_w(\vartheta_w-\vartheta_v) -0.4531\xi_p\dot{\vartheta_v}^2(1+\vartheta_w\vartheta_v)     -4.393\cdot 10^4y_w -1.72\cdot 10^4\vartheta_w +366(\vartheta_v-\vartheta_w) +21.5\dot{y}_w +21.78\dot{\vartheta_w} +36.6(\dot{\vartheta_w}-\dot{\vartheta_v}) +11.8529\vartheta_w$$ (3.1)$$\ddot{\vartheta_v} = -3.9734(\xi_p^2\ddot{\vartheta_v}+\xi_p\ddot{y}_w) -\ddot{\vartheta_w}     -0.8197\xi_p(\vartheta_v-\vartheta_w)\ddot{\vartheta_w} -7.9468\xi_p\dot{\xi}_p\dot{\vartheta_v} -0.8197\xi_p\dot{\vartheta_w}^2(1+\vartheta_w\vartheta_v)     -662.23(\vartheta_v-\vartheta_w) -66.223(\dot{\vartheta_v}-\dot{\vartheta_w}) +38.98\xi_p$$ (4.1) $$\ddot{\xi}_p = 0.2063\ddot{\vartheta_v}(1+\vartheta_w\vartheta_v) -\ddot{y}_w\vartheta_w     -0.2063\dot{\vartheta_w}^2(\vartheta_w-\vartheta_v) -\xi_p\dot{\vartheta_v}^2 -9.81\vartheta_v +333$$ (5.1) $$\ddot{\phi}_p = 81$$",,"['ordinary-differential-equations', 'systems-of-equations']"
79,General solution of a nonlinear differential equation,General solution of a nonlinear differential equation,,"Nonlinear differential equation gone beyond my field of expertise but I'd like to know the details of a problem and to do that I should know the general solution of the following nonlinear differential equation: $$y'(x) = \alpha\beta e^{-\frac{x}{\gamma}} - \delta \sqrt{y(x)}$$ with $\alpha, \gamma, \delta > 0$ and $0 \leq \beta \leq 1$. Since $x$ represents the time is it also possible to assume $x \geq 0$. I tried to solve it with Wolfram Alpha and Wolfram Mathematica but I didn't get any result due to computational time excedeed. Is it possible to find an analytical form of $y(x)$?","Nonlinear differential equation gone beyond my field of expertise but I'd like to know the details of a problem and to do that I should know the general solution of the following nonlinear differential equation: $$y'(x) = \alpha\beta e^{-\frac{x}{\gamma}} - \delta \sqrt{y(x)}$$ with $\alpha, \gamma, \delta > 0$ and $0 \leq \beta \leq 1$. Since $x$ represents the time is it also possible to assume $x \geq 0$. I tried to solve it with Wolfram Alpha and Wolfram Mathematica but I didn't get any result due to computational time excedeed. Is it possible to find an analytical form of $y(x)$?",,['ordinary-differential-equations']
80,Definition of a derivative of differential form,Definition of a derivative of differential form,,"While reading a paper I encountered the following: Let $(\mathbf{q,p}) \in \mathbb{R}^{2n}$ be canonical coordinates and   let $H: \mathbb{R}^{2n} \to \mathbb{R}$ be a smooth function. The continuous-time Hamiltonian system $\mathbf{q}_t = + \nabla_p H(q, p)$ $\mathbf{p}_t = - \nabla_q H(q, p)$ preserves exactly the symplectic form $\mathbf{\omega = dp \wedge dq}$, that is $(d/dt)\omega = 0$ Here $\mathbf{p} , \mathbf{q} \in \mathbb{R}^n$ How is $(d/dt)\omega $ even defined? I have seen exterior derivatives of differential forms, but this is evidently something else. Also, how can we prove that (for the given Hamiltonian system) $(d/dt)\omega = 0$? Any help will be greatly appreciated.","While reading a paper I encountered the following: Let $(\mathbf{q,p}) \in \mathbb{R}^{2n}$ be canonical coordinates and   let $H: \mathbb{R}^{2n} \to \mathbb{R}$ be a smooth function. The continuous-time Hamiltonian system $\mathbf{q}_t = + \nabla_p H(q, p)$ $\mathbf{p}_t = - \nabla_q H(q, p)$ preserves exactly the symplectic form $\mathbf{\omega = dp \wedge dq}$, that is $(d/dt)\omega = 0$ Here $\mathbf{p} , \mathbf{q} \in \mathbb{R}^n$ How is $(d/dt)\omega $ even defined? I have seen exterior derivatives of differential forms, but this is evidently something else. Also, how can we prove that (for the given Hamiltonian system) $(d/dt)\omega = 0$? Any help will be greatly appreciated.",,"['ordinary-differential-equations', 'differential-forms', 'hamilton-equations']"
81,Why does a trajectory take infinite time to reach a critical point?,Why does a trajectory take infinite time to reach a critical point?,,"I have to prove that: For an ODE system $x'=F(x,y),y'=G(x,y)$ where $F,G$ are smooth, any trajectory, which doesn't start at a critical point, cannot reach a critical point in finite time. I have this counter example. Let $F=0,G=3y^2$. Consider a trajectory $(x(t),y(t))=(0,t^3)$ that starts at $t=-1$. It seems to me that at $t=0$, it will reach a critical point $(0,0)$. What is wrong with my example? How should I actually prove the statement required? Thanks.","I have to prove that: For an ODE system $x'=F(x,y),y'=G(x,y)$ where $F,G$ are smooth, any trajectory, which doesn't start at a critical point, cannot reach a critical point in finite time. I have this counter example. Let $F=0,G=3y^2$. Consider a trajectory $(x(t),y(t))=(0,t^3)$ that starts at $t=-1$. It seems to me that at $t=0$, it will reach a critical point $(0,0)$. What is wrong with my example? How should I actually prove the statement required? Thanks.",,['ordinary-differential-equations']
82,Clairaut's form of $(x\frac{dy}{dx}-y)(y\frac{dy}{dx}+x)=a^2\frac{dy}{dx}$,Clairaut's form of,(x\frac{dy}{dx}-y)(y\frac{dy}{dx}+x)=a^2\frac{dy}{dx},Question is to find the Clairaut's form of differential equation  $$(x\frac{dy}{dx}-y)(y\frac{dy}{dx}+x)=a^2\frac{dy}{dx}$$ I know clairaut's equation is of the form $y=x\frac{dy}{dx}+f(\frac{dy}{dx})$ but i was not able to reduce given equation to this form. Any Hint would be Sufficient !,Question is to find the Clairaut's form of differential equation  $$(x\frac{dy}{dx}-y)(y\frac{dy}{dx}+x)=a^2\frac{dy}{dx}$$ I know clairaut's equation is of the form $y=x\frac{dy}{dx}+f(\frac{dy}{dx})$ but i was not able to reduce given equation to this form. Any Hint would be Sufficient !,,['ordinary-differential-equations']
83,Proving $J_n(x)N_{n+1}(x)-J_{n+1}(x)N_n(x)=-\dfrac{2}{\pi x}$: Part $2$ of $3$,Proving : Part  of,J_n(x)N_{n+1}(x)-J_{n+1}(x)N_n(x)=-\dfrac{2}{\pi x} 2 3,"The following question is the second part to this previous question : Prove that $$J_p(x)J_{-p}^{\prime}(x)-J_{-p}(x)J_{p}^{\prime}(x)=-\frac{2}{\pi x}\sin(p\pi)\tag{1}$$ from $$\frac{\mathrm{d}}{\mathrm{d}x}\left[x\left(J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}\right)\right]=0\qquad\longleftarrow\text{(Proved in Part 1)}$$ it follows that $$J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}=\frac{c}{x}\tag{2}$$ To find $c$ use $$\fbox{$J_p(x)=\sum_{n=0}^\infty\frac{(-1)^n}{\Gamma(n+1)\Gamma(n+1+p)}\left(\frac{x}{2}\right)^{2n+p}$}\tag{3}$$ for each of the $4$ functions and pick out the $\dfrac{1}{x}$ terms in the products. Then use $$\fbox{$\Gamma(p)\Gamma(1-p)=\frac{\pi}{\sin(p\pi)}$}\tag{4}$$ Writing out the $4$ functions: $$J_p(x)=\sum_{n=0}^\infty\frac{(-1)^n\cdot x^{2n+p}}{\Gamma(n+1)\Gamma(n+1+p)\cdot 2^{2n+p}}\tag{a}$$ $$J_p^{\prime}(x)=\sum_{n=0}^\infty\frac{(-1)^n(2n+p)\cdot x^{2n+p}}{\Gamma(n+1)\Gamma(n+1+p)\cdot 2^{2n+p}}\cdot\frac{1}{x}\tag{b}$$ $$J_{-p}(x)=\sum_{n=0}^\infty\frac{(-1)^n\cdot x^{2n-p}}{\Gamma(n+1)\Gamma(n+1-p)\cdot 2^{2n-p}}\tag{c}$$ $$J_{-p}^{\prime}(x)=\sum_{n=0}^\infty\frac{(-1)^n(2n-p)\cdot x^{2n-p}}{\Gamma(n+1)\Gamma(n+1-p)\cdot 2^{2n-p}}\cdot\frac{1}{x}\tag{d}$$ Insertion of $(\mathrm{a})$, $(\mathrm{b})$, $(\mathrm{c})$, $(\mathrm{d})$ into $(2)$ gives $$\bbox[#AFF]{ \sum_{n=0}^\infty\frac{(-1)^n\cdot x^{2n+p}}{\Gamma(n+1)\Gamma(n+1+p)\cdot 2^{2n+p}}\cdot\sum_{n=0}^\infty\frac{(-1)^n(2n-p)\cdot x^{2n-p}}{\Gamma(n+1)\Gamma(n+1-p)\cdot 2^{2n-p}}\cdot\frac{1}{x}\quad-\\\sum_{n=0}^\infty\frac{(-1)^n\cdot x^{2n-p}}{\Gamma(n+1)\Gamma(n+1-p)\cdot 2^{2n-p}}\cdot\sum_{n=0}^\infty\frac{(-1)^n(2n+p)\cdot x^{2n+p}}{\Gamma(n+1)\Gamma(n+1+p)\cdot 2^{2n+p}}\cdot\frac{1}{x}=\frac{c}{x} }$$ So unless I'm mistaken this means that the highlighted equation above requires that  $$J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}=c\tag{5}$$ If this is the case then insertion of $(5)$ into $(2)$ implies that $$J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}=\frac{J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}}{x}\implies x=1$$ I'm not sure if what I have done so far is correct, also I have no idea how to make use of $$\fbox{$\Gamma(p)\Gamma(1-p)=\frac{\pi}{\sin(p\pi)}$}$$ Could anyone please provide some feedback, hints or tips on how I can prove that  $$\fbox{$\color{red}{J_p(x)J_{-p}^{\prime}(x)-J_{-p}(x)J_{p}^{\prime}(x)=-\frac{2}{\pi x}\sin(p\pi)}$}$$ Much appreciated.","The following question is the second part to this previous question : Prove that $$J_p(x)J_{-p}^{\prime}(x)-J_{-p}(x)J_{p}^{\prime}(x)=-\frac{2}{\pi x}\sin(p\pi)\tag{1}$$ from $$\frac{\mathrm{d}}{\mathrm{d}x}\left[x\left(J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}\right)\right]=0\qquad\longleftarrow\text{(Proved in Part 1)}$$ it follows that $$J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}=\frac{c}{x}\tag{2}$$ To find $c$ use $$\fbox{$J_p(x)=\sum_{n=0}^\infty\frac{(-1)^n}{\Gamma(n+1)\Gamma(n+1+p)}\left(\frac{x}{2}\right)^{2n+p}$}\tag{3}$$ for each of the $4$ functions and pick out the $\dfrac{1}{x}$ terms in the products. Then use $$\fbox{$\Gamma(p)\Gamma(1-p)=\frac{\pi}{\sin(p\pi)}$}\tag{4}$$ Writing out the $4$ functions: $$J_p(x)=\sum_{n=0}^\infty\frac{(-1)^n\cdot x^{2n+p}}{\Gamma(n+1)\Gamma(n+1+p)\cdot 2^{2n+p}}\tag{a}$$ $$J_p^{\prime}(x)=\sum_{n=0}^\infty\frac{(-1)^n(2n+p)\cdot x^{2n+p}}{\Gamma(n+1)\Gamma(n+1+p)\cdot 2^{2n+p}}\cdot\frac{1}{x}\tag{b}$$ $$J_{-p}(x)=\sum_{n=0}^\infty\frac{(-1)^n\cdot x^{2n-p}}{\Gamma(n+1)\Gamma(n+1-p)\cdot 2^{2n-p}}\tag{c}$$ $$J_{-p}^{\prime}(x)=\sum_{n=0}^\infty\frac{(-1)^n(2n-p)\cdot x^{2n-p}}{\Gamma(n+1)\Gamma(n+1-p)\cdot 2^{2n-p}}\cdot\frac{1}{x}\tag{d}$$ Insertion of $(\mathrm{a})$, $(\mathrm{b})$, $(\mathrm{c})$, $(\mathrm{d})$ into $(2)$ gives $$\bbox[#AFF]{ \sum_{n=0}^\infty\frac{(-1)^n\cdot x^{2n+p}}{\Gamma(n+1)\Gamma(n+1+p)\cdot 2^{2n+p}}\cdot\sum_{n=0}^\infty\frac{(-1)^n(2n-p)\cdot x^{2n-p}}{\Gamma(n+1)\Gamma(n+1-p)\cdot 2^{2n-p}}\cdot\frac{1}{x}\quad-\\\sum_{n=0}^\infty\frac{(-1)^n\cdot x^{2n-p}}{\Gamma(n+1)\Gamma(n+1-p)\cdot 2^{2n-p}}\cdot\sum_{n=0}^\infty\frac{(-1)^n(2n+p)\cdot x^{2n+p}}{\Gamma(n+1)\Gamma(n+1+p)\cdot 2^{2n+p}}\cdot\frac{1}{x}=\frac{c}{x} }$$ So unless I'm mistaken this means that the highlighted equation above requires that  $$J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}=c\tag{5}$$ If this is the case then insertion of $(5)$ into $(2)$ implies that $$J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}=\frac{J_pJ_{-p}^{\prime}-J_{-p}J_{p}^{\prime}}{x}\implies x=1$$ I'm not sure if what I have done so far is correct, also I have no idea how to make use of $$\fbox{$\Gamma(p)\Gamma(1-p)=\frac{\pi}{\sin(p\pi)}$}$$ Could anyone please provide some feedback, hints or tips on how I can prove that  $$\fbox{$\color{red}{J_p(x)J_{-p}^{\prime}(x)-J_{-p}(x)J_{p}^{\prime}(x)=-\frac{2}{\pi x}\sin(p\pi)}$}$$ Much appreciated.",,"['ordinary-differential-equations', 'bessel-functions']"
84,Showing that the omega and the alpha limits are disjoint or have just one common point,Showing that the omega and the alpha limits are disjoint or have just one common point,,"Let $f:\mathbb R^2\rightarrow \mathbb R^2$ be a $C^1$ function and $x'=f(x)$. Suppose that there are finites points $x_i\in \mathbb R^2$, such that $f(x_i)=0$. Given $y$, such that $f(y)\neq 0$, and the flux through $y$ is not periodic, then the $\omega$-limit and the $\alpha$-limit are disjoint, or both are equal to $\{x_0\}$, and moreover $f(x_0)=0$. I'm trying to show that if they are not disjoint, then there is one point, by supposing that there is another point in one of them, and then getting an absurd. But I can't get it done, hope that you have some tips to me. Thanks in advance.","Let $f:\mathbb R^2\rightarrow \mathbb R^2$ be a $C^1$ function and $x'=f(x)$. Suppose that there are finites points $x_i\in \mathbb R^2$, such that $f(x_i)=0$. Given $y$, such that $f(y)\neq 0$, and the flux through $y$ is not periodic, then the $\omega$-limit and the $\alpha$-limit are disjoint, or both are equal to $\{x_0\}$, and moreover $f(x_0)=0$. I'm trying to show that if they are not disjoint, then there is one point, by supposing that there is another point in one of them, and then getting an absurd. But I can't get it done, hope that you have some tips to me. Thanks in advance.",,"['analysis', 'ordinary-differential-equations']"
85,How to solve this 1st order ODE,How to solve this 1st order ODE,,"$$ \frac{dy}{dx} = \frac{2x^2 + 3y^2 - 7}{3x^2 + 4y^2 + 8}$$ This does not satisfy the exactness and I can't find any integrating factor to transform it. I can't make it homogeneous too. Thanks in advance for the replies. edit: The question was written incorrectly by the instructor, so in this version there is no analytic solution for it.","$$ \frac{dy}{dx} = \frac{2x^2 + 3y^2 - 7}{3x^2 + 4y^2 + 8}$$ This does not satisfy the exactness and I can't find any integrating factor to transform it. I can't make it homogeneous too. Thanks in advance for the replies. edit: The question was written incorrectly by the instructor, so in this version there is no analytic solution for it.",,['ordinary-differential-equations']
86,Help with a linearly dependent proof on differential equations,Help with a linearly dependent proof on differential equations,,Show that any two solutions $y_1$ and $y_2$ of the equation $y' + p(x)y = 0$ are linearly dependent. How do I prove this question?,Show that any two solutions $y_1$ and $y_2$ of the equation $y' + p(x)y = 0$ are linearly dependent. How do I prove this question?,,['ordinary-differential-equations']
87,Find an expression for $\frac{dy}{dx}$ in terms of $x$ and $y$ and verify that $P$ is a stationary point.,Find an expression for  in terms of  and  and verify that  is a stationary point.,\frac{dy}{dx} x y P,"A curve is defined by the equation $$2y+e^{2x}y^2=x^2+\frac{2}{e}$$ Find an expression for $\frac{dy}{dx}$ in terms of $x$ and $y$ \begin{align} 2y+e^{2x}y^2 & = x^2+\frac{2}{e} \\ 2\frac{dy}{dx}+2e^{2x}y^2+2e^{2x}y\frac{dy}{dx} & = 2x \\  \frac{dy}{dx}+e^{2x}y^2+e^{2x}y\frac{dy}{dx} & = x \\  \frac{dy}{dx}\left(1+e^{2x}y\right) & = x-e^{2x}y^2 \\  \frac{dy}{dx} & = \frac{x-e^{2x}y^2}{1+e^{2x}y} \\  \end{align} Verify that $P$ $(1, \frac{1}{e})$ is a stationary point on the curve. Stationary point when $\frac{dy}{dx}=0$ $$\frac{x-e^{2x}y^2}{1+e^{2x}y}$$ \begin{align} & = \frac{(1)-e^{2(1)}(\frac{1}{e})^2}{1+e^{2(1)}(\frac{1}{e})} \\  & = \frac{1-e^{2}e^{-2}}{1+e^{2}e^{-1}} \\  & = \frac{1}{1+e} \\  \end{align} $$\frac{1}{1+e}\neq0$$ Thanks","A curve is defined by the equation $$2y+e^{2x}y^2=x^2+\frac{2}{e}$$ Find an expression for $\frac{dy}{dx}$ in terms of $x$ and $y$ \begin{align} 2y+e^{2x}y^2 & = x^2+\frac{2}{e} \\ 2\frac{dy}{dx}+2e^{2x}y^2+2e^{2x}y\frac{dy}{dx} & = 2x \\  \frac{dy}{dx}+e^{2x}y^2+e^{2x}y\frac{dy}{dx} & = x \\  \frac{dy}{dx}\left(1+e^{2x}y\right) & = x-e^{2x}y^2 \\  \frac{dy}{dx} & = \frac{x-e^{2x}y^2}{1+e^{2x}y} \\  \end{align} Verify that $P$ $(1, \frac{1}{e})$ is a stationary point on the curve. Stationary point when $\frac{dy}{dx}=0$ $$\frac{x-e^{2x}y^2}{1+e^{2x}y}$$ \begin{align} & = \frac{(1)-e^{2(1)}(\frac{1}{e})^2}{1+e^{2(1)}(\frac{1}{e})} \\  & = \frac{1-e^{2}e^{-2}}{1+e^{2}e^{-1}} \\  & = \frac{1}{1+e} \\  \end{align} $$\frac{1}{1+e}\neq0$$ Thanks",,['ordinary-differential-equations']
88,$c_1\cosh(x)+c_2\sinh(x)=A\cosh(x+y)$ always true?,always true?,c_1\cosh(x)+c_2\sinh(x)=A\cosh(x+y),"My question: Can I rewrite $c_1\cosh(x)+c_2\sinh(x)$, which is a solution to a differential equation as $$A\cosh(x+x_0)$$ introducing the new constants of integration $A$ and $x_0$? How can I deal with $c_1=0$? This is how I can set up a relationship between the coefficients: $A\cosh(x+x_0)=A\cosh(x)\cosh(x_0)+A\sinh(x)\sin(x_0)$ Compare coefficients: $c_1=A\cosh(x_0)$ and $c_2=A\sinh(x_0)$. Now square and subtract both equations:  $$c_1^2-c_2^2=A^2 \implies A=\pm\sqrt{c_1^2-c_2^2}.$$ Now devide second equation by first equation: $$\tanh(x_0)=c_2/c_1 \implies x_0=\tanh^{-1}(c_2/c_1 )$$","My question: Can I rewrite $c_1\cosh(x)+c_2\sinh(x)$, which is a solution to a differential equation as $$A\cosh(x+x_0)$$ introducing the new constants of integration $A$ and $x_0$? How can I deal with $c_1=0$? This is how I can set up a relationship between the coefficients: $A\cosh(x+x_0)=A\cosh(x)\cosh(x_0)+A\sinh(x)\sin(x_0)$ Compare coefficients: $c_1=A\cosh(x_0)$ and $c_2=A\sinh(x_0)$. Now square and subtract both equations:  $$c_1^2-c_2^2=A^2 \implies A=\pm\sqrt{c_1^2-c_2^2}.$$ Now devide second equation by first equation: $$\tanh(x_0)=c_2/c_1 \implies x_0=\tanh^{-1}(c_2/c_1 )$$",,"['ordinary-differential-equations', 'functions']"
89,Help with solving ODE,Help with solving ODE,,"This is actually a part of an SDE problem, which I am having problem to solve. However i think it requires only ODE knowledge to solve. I have $$\mu(u) = x^3 + 3 \int_t^u a \mu(s) ds + 3 \sigma^2 \int_t^u xe^{a(s-t)}ds$$ Now taking derivatives w.r.t $u$ we obtain the following ODE for $\mu$ $$\dot{\mu}(u) = 3a\mu(u) + 3\sigma^2xe^{a(u-t)}$$ And plugging in $u=t$ both integrals cancel, so $$\mu(t) = x^3$$ The solution to the ODE should be: $$\mu(u) = x^3e^{3a(u-t)}+ \frac{3\sigma^2x}{2a}e^{3a(u-t)}-\frac{3\sigma^2x}{2a}e^{a(u-t)}$$ Now what i have done i first i solve the homogenous equation $$d\mu_h = 3a\mu_h ds$$ I get $$\mu_h = x^3e^{3a(u-t)}$$ Now when solving the rest of the equation i get: $$d\mu = 3\sigma^2xe^{a(u-t)}ds$$ Which could be solved quite straight-forward: $$\mu = 3\sigma^2x \int_t^ue^{a(u-s)}ds = 3\sigma^2x[-\frac{e^{a(u-s)}}{a}]_t^u = 3\sigma^2x \frac{1}{a}(e^{a(u-t)}-1)$$ Now putting them together i do not get the same solution as asked for. i.e. $$\mu_h + \mu = x^3e^{3a(u-t)} + 3\sigma^2x \frac{1}{a}(e^{a(u-t)}-1)$$ Which is not the same as: $$\mu(u) = x^3e^{3a(u-t)}+ \frac{3\sigma^2x}{2a}e^{3a(u-t)}-\frac{3\sigma^2x}{2a}e^{a(u-t)}$$ Does anyone know where the error in reasoning is?","This is actually a part of an SDE problem, which I am having problem to solve. However i think it requires only ODE knowledge to solve. I have $$\mu(u) = x^3 + 3 \int_t^u a \mu(s) ds + 3 \sigma^2 \int_t^u xe^{a(s-t)}ds$$ Now taking derivatives w.r.t $u$ we obtain the following ODE for $\mu$ $$\dot{\mu}(u) = 3a\mu(u) + 3\sigma^2xe^{a(u-t)}$$ And plugging in $u=t$ both integrals cancel, so $$\mu(t) = x^3$$ The solution to the ODE should be: $$\mu(u) = x^3e^{3a(u-t)}+ \frac{3\sigma^2x}{2a}e^{3a(u-t)}-\frac{3\sigma^2x}{2a}e^{a(u-t)}$$ Now what i have done i first i solve the homogenous equation $$d\mu_h = 3a\mu_h ds$$ I get $$\mu_h = x^3e^{3a(u-t)}$$ Now when solving the rest of the equation i get: $$d\mu = 3\sigma^2xe^{a(u-t)}ds$$ Which could be solved quite straight-forward: $$\mu = 3\sigma^2x \int_t^ue^{a(u-s)}ds = 3\sigma^2x[-\frac{e^{a(u-s)}}{a}]_t^u = 3\sigma^2x \frac{1}{a}(e^{a(u-t)}-1)$$ Now putting them together i do not get the same solution as asked for. i.e. $$\mu_h + \mu = x^3e^{3a(u-t)} + 3\sigma^2x \frac{1}{a}(e^{a(u-t)}-1)$$ Which is not the same as: $$\mu(u) = x^3e^{3a(u-t)}+ \frac{3\sigma^2x}{2a}e^{3a(u-t)}-\frac{3\sigma^2x}{2a}e^{a(u-t)}$$ Does anyone know where the error in reasoning is?",,['ordinary-differential-equations']
90,How can I use Bessel's equation to solve the Lengthening Pendulum differential equation?,How can I use Bessel's equation to solve the Lengthening Pendulum differential equation?,,"Taking a small extract of this previous bounty question of mine: It can be shown that the differential equation: $$\fbox{$y^{\prime\prime}+\left(\frac{1-2a}{x}\right)y^{\prime}+\left[\left(bcx^{c-1}\right)^2+\frac{a^2-p^2c^2}{x^2}\right]y=0$}\tag{1}$$ has the solution $$\fbox{$y=x^aZ_p\left(bx^c\right)$}\tag{2}$$ where $Z_p$ stands for $J_p$ or $N_p$ or any linear combination of them, and $a,b,c,p$ are   constants. To see how to use this, let us “solve” the differential equation: $$y^{\prime\prime}+9xy=0\tag{3}$$ If $(3)$ is of the type $(1)$, then we must have $$1-2a=0$$ $$2(c-1)=1$$ $$(bc)^2=9$$  $$a^2-p^2c^2=0$$ from these $4$ equations we find   $$a=\dfrac12$$ $$c=\dfrac32$$ $$b=2$$ $$p=\dfrac{a}{c}=\dfrac13$$ Then the solution of $(3)$ is $$y=x^{1/2}Z_{1/3}\left(2x^{3/2}\right)$$ This means that the general solution of $(3)$ is $$y=x^{1/2}\left[AJ_{1/3}\left(2x^{3/2}\right)+BN_{1/3}\left(2x^{3/2}\right)\right]$$ where $A$ and $B$ are arbitrary constants. The differential equation of a lengthening pendulum is $$l\dfrac{d^2\theta}{dl^2}+2\dfrac{d\theta}{dl}+\dfrac{g}{v^2}\theta=0\qquad\quad\tag{4}\longleftarrow\text{proved in this former question}$$ I have to solve this differential equation for $\theta$ by comparing $(4)$ with $(1)$ in the same manner as used to find the solution to $(3)$ So here is my attempt: First I begin by writing $(1)$ in terms of the new variables $\theta$ and $l$ $$\frac{\mathrm{d}^2\theta}{\mathrm{d}l^2}+\left(\frac{1-2a}{l}\right)\frac{\mathrm{d}\theta}{\mathrm{d}l}+\left[\left(b\,c\,l^{c-1}\right)^2+\frac{a^2-p^2c^2}{l^2}\right]\theta=0\tag{5}$$ Now I compare $(4)$ with $(5)$ to obtain  $$l=1$$ $$\frac{1-2a}{l}=2\implies a=-\frac12\qquad\text{since $l=1$}$$ $$\left(b\,c\,l^{c-1}\right)^2+\frac{a^2-p^2c^2}{l^2}=\frac{g}{v^2}\tag{6}$$ Substituting $a=\frac12$ and $l=1$ into $(6)$ $$\implies b^2c^2+\frac14 -p^2c^2=\frac{g}{v^2}\tag{7}$$ Now if $l=1$ we must have $$c=1$$ Substituting $c=1$ into $(7)$ $$\implies b^2+\frac14 -p^2=\frac{g}{v^2}\tag{8}$$ But I am unable to proceed from here and I have made a mistake anyway as I can tell you that the correct answer is $$\theta=l^{-1/2}Z_1\left(\frac{2{g}^{1/2}}{v}l^{1/2}\right)$$ Comparison with  $$\fbox{$y=x^aZ_p\left(bx^c\right)$}\tag{2}$$ shows that $$a=-\frac12, \quad c=\frac12, \quad b=\frac{2{g}^{1/2}}{v},\quad p=1$$ Could someone please help me find the correct values of $a,b,c,p$? Any hints or advice is well appreciated.","Taking a small extract of this previous bounty question of mine: It can be shown that the differential equation: $$\fbox{$y^{\prime\prime}+\left(\frac{1-2a}{x}\right)y^{\prime}+\left[\left(bcx^{c-1}\right)^2+\frac{a^2-p^2c^2}{x^2}\right]y=0$}\tag{1}$$ has the solution $$\fbox{$y=x^aZ_p\left(bx^c\right)$}\tag{2}$$ where $Z_p$ stands for $J_p$ or $N_p$ or any linear combination of them, and $a,b,c,p$ are   constants. To see how to use this, let us “solve” the differential equation: $$y^{\prime\prime}+9xy=0\tag{3}$$ If $(3)$ is of the type $(1)$, then we must have $$1-2a=0$$ $$2(c-1)=1$$ $$(bc)^2=9$$  $$a^2-p^2c^2=0$$ from these $4$ equations we find   $$a=\dfrac12$$ $$c=\dfrac32$$ $$b=2$$ $$p=\dfrac{a}{c}=\dfrac13$$ Then the solution of $(3)$ is $$y=x^{1/2}Z_{1/3}\left(2x^{3/2}\right)$$ This means that the general solution of $(3)$ is $$y=x^{1/2}\left[AJ_{1/3}\left(2x^{3/2}\right)+BN_{1/3}\left(2x^{3/2}\right)\right]$$ where $A$ and $B$ are arbitrary constants. The differential equation of a lengthening pendulum is $$l\dfrac{d^2\theta}{dl^2}+2\dfrac{d\theta}{dl}+\dfrac{g}{v^2}\theta=0\qquad\quad\tag{4}\longleftarrow\text{proved in this former question}$$ I have to solve this differential equation for $\theta$ by comparing $(4)$ with $(1)$ in the same manner as used to find the solution to $(3)$ So here is my attempt: First I begin by writing $(1)$ in terms of the new variables $\theta$ and $l$ $$\frac{\mathrm{d}^2\theta}{\mathrm{d}l^2}+\left(\frac{1-2a}{l}\right)\frac{\mathrm{d}\theta}{\mathrm{d}l}+\left[\left(b\,c\,l^{c-1}\right)^2+\frac{a^2-p^2c^2}{l^2}\right]\theta=0\tag{5}$$ Now I compare $(4)$ with $(5)$ to obtain  $$l=1$$ $$\frac{1-2a}{l}=2\implies a=-\frac12\qquad\text{since $l=1$}$$ $$\left(b\,c\,l^{c-1}\right)^2+\frac{a^2-p^2c^2}{l^2}=\frac{g}{v^2}\tag{6}$$ Substituting $a=\frac12$ and $l=1$ into $(6)$ $$\implies b^2c^2+\frac14 -p^2c^2=\frac{g}{v^2}\tag{7}$$ Now if $l=1$ we must have $$c=1$$ Substituting $c=1$ into $(7)$ $$\implies b^2+\frac14 -p^2=\frac{g}{v^2}\tag{8}$$ But I am unable to proceed from here and I have made a mistake anyway as I can tell you that the correct answer is $$\theta=l^{-1/2}Z_1\left(\frac{2{g}^{1/2}}{v}l^{1/2}\right)$$ Comparison with  $$\fbox{$y=x^aZ_p\left(bx^c\right)$}\tag{2}$$ shows that $$a=-\frac12, \quad c=\frac12, \quad b=\frac{2{g}^{1/2}}{v},\quad p=1$$ Could someone please help me find the correct values of $a,b,c,p$? Any hints or advice is well appreciated.",,"['ordinary-differential-equations', 'proof-verification', 'bessel-functions']"
91,Definition of Uniform Convergence for vector-valued functions,Definition of Uniform Convergence for vector-valued functions,,"Birkhoff and Rota's 'Ordinary Differential Equations' proves that Picard iteration works not just for scalar differential equations but for the systems of scalar DEs that arise from vector-valued differential equations as well.  (That is, DEs of the form $\frac{d\mathbf{f}}{dt} = \mathbf{F}(\mathbf{f},t)$, where $\mathbf{f}$ and $\mathbf{F}$ are vector-valued.)  The proof relies on the notion of uniform convergence, but I'm unable to find a definition for uniform convergence for vector-valued functions.  By analogy with Wikipedia's definition for uniform convergence for functions whose range is simply in $\mathbb{R}$, I've gone ahead and made my own definition: Let $f_i:\mathbb{R}^n \to \mathbb{R}^m$ be functions such that $f_i$ is defined for all $i \in \mathbb{N}$.  We say that $(f_i)_{i \in \mathbb{N}}$ is uniformly convergent with limit $f:\mathbb{R}^n \to \mathbb{R}^m$ if for every $\epsilon > 0$, there exists a natural number $K$ such that for all $\mathbf{x} \in \mathbb{R}^n$ and all $k \geq K$ we have $\|f(\mathbf{x})_k - f(\mathbf{x})\| < \epsilon$. Is this correct, or close to correct?","Birkhoff and Rota's 'Ordinary Differential Equations' proves that Picard iteration works not just for scalar differential equations but for the systems of scalar DEs that arise from vector-valued differential equations as well.  (That is, DEs of the form $\frac{d\mathbf{f}}{dt} = \mathbf{F}(\mathbf{f},t)$, where $\mathbf{f}$ and $\mathbf{F}$ are vector-valued.)  The proof relies on the notion of uniform convergence, but I'm unable to find a definition for uniform convergence for vector-valued functions.  By analogy with Wikipedia's definition for uniform convergence for functions whose range is simply in $\mathbb{R}$, I've gone ahead and made my own definition: Let $f_i:\mathbb{R}^n \to \mathbb{R}^m$ be functions such that $f_i$ is defined for all $i \in \mathbb{N}$.  We say that $(f_i)_{i \in \mathbb{N}}$ is uniformly convergent with limit $f:\mathbb{R}^n \to \mathbb{R}^m$ if for every $\epsilon > 0$, there exists a natural number $K$ such that for all $\mathbf{x} \in \mathbb{R}^n$ and all $k \geq K$ we have $\|f(\mathbf{x})_k - f(\mathbf{x})\| < \epsilon$. Is this correct, or close to correct?",,"['real-analysis', 'ordinary-differential-equations']"
92,What to do when the integrating factor is a function of both x and y?,What to do when the integrating factor is a function of both x and y?,,"I have to solve the following differential equation: $$(\cos^2x + y \sin 2x) \frac{dy}{dx} + y^2 =0$$ using an integrating factor. An integrating factor that is a function of just $x$ or just $y$ won't work, so we need to find an integrating factor which is a function of both $x$ and $y$. I have followed the instructions from this other post Finding integrating factor when IF will be a function of x and y which suggests looking for a separable solution: $$u(x,y)=e^{\int G(x) dx} e^{\int F(y) dy}$$ where $G(x)=\frac{\delta u/\delta x}{u}$ and $F(y)=\frac{\delta u/\delta y}{u}$. However, I get stuck at this point: $$G(x)(\cos^2 x + y \sin 2x) - F(y) y^2 = 2y + 2 \sin 2x - 2y \cos 2x$$ and I can't find a solution for $G(x)$ and $F(y)$ that would work. Any ideas? Thanks in advance!","I have to solve the following differential equation: $$(\cos^2x + y \sin 2x) \frac{dy}{dx} + y^2 =0$$ using an integrating factor. An integrating factor that is a function of just $x$ or just $y$ won't work, so we need to find an integrating factor which is a function of both $x$ and $y$. I have followed the instructions from this other post Finding integrating factor when IF will be a function of x and y which suggests looking for a separable solution: $$u(x,y)=e^{\int G(x) dx} e^{\int F(y) dy}$$ where $G(x)=\frac{\delta u/\delta x}{u}$ and $F(y)=\frac{\delta u/\delta y}{u}$. However, I get stuck at this point: $$G(x)(\cos^2 x + y \sin 2x) - F(y) y^2 = 2y + 2 \sin 2x - 2y \cos 2x$$ and I can't find a solution for $G(x)$ and $F(y)$ that would work. Any ideas? Thanks in advance!",,['ordinary-differential-equations']
93,Solve first order partial diferential equation,Solve first order partial diferential equation,,"Consider the Cauchy problem: $$\left\{\begin{array}{lll} x^2\partial_x u+y^2\partial_yu=u^2\\ u(x,2x)=1 \end{array}\right.$$ It is easy to show that the characteristic equations are given by: $$\frac{dx}{x^2}=\frac{dy}{y^2}=\frac{dz}{px^2+qy^2}=\frac{dp}{2pu-2xp}=\frac{dq}{2qu-2yq}=dt$$ By the Lagrange-Charpit's method, we need a first integral $\Psi$ different to $\Phi=x^2p+y^2q-u^2$. How I can find  $\Psi$? Maybe we should get directly the characteristic curves of the previous system? Many thanks!","Consider the Cauchy problem: $$\left\{\begin{array}{lll} x^2\partial_x u+y^2\partial_yu=u^2\\ u(x,2x)=1 \end{array}\right.$$ It is easy to show that the characteristic equations are given by: $$\frac{dx}{x^2}=\frac{dy}{y^2}=\frac{dz}{px^2+qy^2}=\frac{dp}{2pu-2xp}=\frac{dq}{2qu-2yq}=dt$$ By the Lagrange-Charpit's method, we need a first integral $\Psi$ different to $\Phi=x^2p+y^2q-u^2$. How I can find  $\Psi$? Maybe we should get directly the characteristic curves of the previous system? Many thanks!",,"['analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
94,Variation of parameters exercise -- harmonic motion,Variation of parameters exercise -- harmonic motion,,"I am self-studying differential equations using MIT's publicly available materials.     One of the assignments asks us to show that the general solution of the inhomogeneous DE $y'' + k^2y = R(x)$ is given by     \begin{align} 	y & = \frac{1}{k}\left[\int_a^t\sin k(x - t)R(t)dt\right] + c_1\sin kx + c_2 \cos kx. 	\end{align} (The problem, like many of those assigned in the course, comes from Birkhoff and Rota.) It is clear that a basis of solutions to the corresponding homogeneous equation is given by $\cos kx$ and $\sin kx$.  So the general solution is given by \begin{align} y = y_p + c_1\sin kx + c_2\cos kx \end{align} where $y_p$ is a particular solution of the inhomogenous equation.  Thus we are to establish that $y_p = \frac{1}{k}\left[\int_a^t\sin k(x - t)R(t)dt\right].$ The Wronskian of our basis of solutions is $k$.  So by variation of parameters, we obtain \begin{align} 	y_p & = \cos kx\int_a^x\frac{-R(t)\sin kt}{k}dt + \sin kx\int_a^x\frac{R(t)\cos kt}{k}dt\\ 	& = \frac{1}{k}\left[\cos kx \int_a^x -R(t)\sin ktdt + \sin kx\int_a^xR(t)\cos ktdt\right] \end{align} which is where I get stuck.  I have two questions: 1)  What's up with the limits of integration in the purported solution?  As far as I know, it doesn't make a lot of sense to integrate with respect to $t$ while $t$ is also a limit of integration. 2)  Assuming the preceding is a typical Birkhoffian/Rotarian typo, how would one move from from \begin{align} \frac{1}{k}\left[\cos kx \int_a^x -R(t)\sin ktdt + \sin kx\int_a^xR(t)\cos ktdt\right] \end{align} to something that in some way resembles \begin{align} \frac{1}{k}\left[\int_a^t\sin k(x - t)R(t)dt\right]? \end{align} I had thought of applying integration by parts to the first expression in hopes that I could take advantage of the Pythagorean identity to obtain some helpful simplification, but no dice. Any help would be appreciated.","I am self-studying differential equations using MIT's publicly available materials.     One of the assignments asks us to show that the general solution of the inhomogeneous DE $y'' + k^2y = R(x)$ is given by     \begin{align} 	y & = \frac{1}{k}\left[\int_a^t\sin k(x - t)R(t)dt\right] + c_1\sin kx + c_2 \cos kx. 	\end{align} (The problem, like many of those assigned in the course, comes from Birkhoff and Rota.) It is clear that a basis of solutions to the corresponding homogeneous equation is given by $\cos kx$ and $\sin kx$.  So the general solution is given by \begin{align} y = y_p + c_1\sin kx + c_2\cos kx \end{align} where $y_p$ is a particular solution of the inhomogenous equation.  Thus we are to establish that $y_p = \frac{1}{k}\left[\int_a^t\sin k(x - t)R(t)dt\right].$ The Wronskian of our basis of solutions is $k$.  So by variation of parameters, we obtain \begin{align} 	y_p & = \cos kx\int_a^x\frac{-R(t)\sin kt}{k}dt + \sin kx\int_a^x\frac{R(t)\cos kt}{k}dt\\ 	& = \frac{1}{k}\left[\cos kx \int_a^x -R(t)\sin ktdt + \sin kx\int_a^xR(t)\cos ktdt\right] \end{align} which is where I get stuck.  I have two questions: 1)  What's up with the limits of integration in the purported solution?  As far as I know, it doesn't make a lot of sense to integrate with respect to $t$ while $t$ is also a limit of integration. 2)  Assuming the preceding is a typical Birkhoffian/Rotarian typo, how would one move from from \begin{align} \frac{1}{k}\left[\cos kx \int_a^x -R(t)\sin ktdt + \sin kx\int_a^xR(t)\cos ktdt\right] \end{align} to something that in some way resembles \begin{align} \frac{1}{k}\left[\int_a^t\sin k(x - t)R(t)dt\right]? \end{align} I had thought of applying integration by parts to the first expression in hopes that I could take advantage of the Pythagorean identity to obtain some helpful simplification, but no dice. Any help would be appreciated.",,['ordinary-differential-equations']
95,Trouble understanding definition of an attracting set,Trouble understanding definition of an attracting set,,"From Wiggins' book, ""Let $\cal{M}$ be a trapping region. Then $A=\cap_{t>0}\phi(t,\cal{M})$ is called an attracting set"". Then he gives an example: $\dot{x}=x-x^3$ $\dot{y}=-y$, and claims that the closed interval $[-1,1]$ is an attracting set with some appropriate trapping region (namely an ellipse $\cal{M}$ that surrounds all three critical points of the system. But I cannot relate this to the definition of an attracting set; I mean, if $t$ is arbitrarily large, then shouldn't all points be either at $(-1,0),(0,0)$ or $(1,0)$? How is the containment $[-1,1] \subset \cap_{t>0}\phi(t,\cal{M})$ seen? I have attached a diagram that he has provided below.","From Wiggins' book, ""Let $\cal{M}$ be a trapping region. Then $A=\cap_{t>0}\phi(t,\cal{M})$ is called an attracting set"". Then he gives an example: $\dot{x}=x-x^3$ $\dot{y}=-y$, and claims that the closed interval $[-1,1]$ is an attracting set with some appropriate trapping region (namely an ellipse $\cal{M}$ that surrounds all three critical points of the system. But I cannot relate this to the definition of an attracting set; I mean, if $t$ is arbitrarily large, then shouldn't all points be either at $(-1,0),(0,0)$ or $(1,0)$? How is the containment $[-1,1] \subset \cap_{t>0}\phi(t,\cal{M})$ seen? I have attached a diagram that he has provided below.",,"['ordinary-differential-equations', 'dynamical-systems']"
96,"Solving differential equation using Laplace transform, problem finding inverse","Solving differential equation using Laplace transform, problem finding inverse",,"Given $$y'' + 4y' + 5y =  H(t-3)e^{-2t}, t>0, y(0) = 1, y'(0)=2 $$ To solve this diff. equation using Laplace transform. Seems very straightforward. On one side, we have $$\mathscr{L}\{y''+4y'+5y\} = \mathscr{L}\{y''\} + 4\mathscr{L}\{y'\} +5\mathscr{L}\{y\}=\\ 	\left [s^2\mathscr{L}\{y\}-sy(0)-y'(0)\right ] + 4\left [s\mathscr{L}\{y\}-y(0)\right ] + 5\mathscr{L}\{y\} =\\ 	\mathscr{L}\{y\}\left (s^2+4s+5\right ) -sy(0) -4y(0) -y'(0) = \mathscr{L}\{y\}\left (s^2+4s+5\right ) -s-6 $$ and on the other $$\mathscr{L}\{H(t-3)e^{-2t}\} = \int_0^\infty e^{-(s+2)t}H(t-3)\mbox{d}t = \int_0^3 0\mbox{d}t + \int_3^\infty e^{-(s+2)t}\mbox{d}t = -\frac{1}{s+2}e^{-(s+2)t}\bigg\vert_3^\infty = \frac{e^{-3(s+2)}}{s+2} $$ Which ultimately yields: $$\mathscr{L}\{y\} = \frac{s+6}{s^2+4s+5} +\frac{e^{-3(s+2)}}{(s+2)(s^2+4s+5)} $$ Since $\mathscr{L}$ is a linear operator, I take its inverse is also linear. Finding the inverse of the first summand is a piece of cake, however, how do we do the second one? $$\mathscr{L}^{-1}\left\lbrace\frac{e^{-3(s+2)}}{(s+2)(s^2+4s+5)} \right\rbrace $$ Oh, missed a useful theorem: $$\forall r>0: \mathscr{L}\{f(t-r)\} = e^{-sr}\mathscr{L}\{f(t)\} $$","Given $$y'' + 4y' + 5y =  H(t-3)e^{-2t}, t>0, y(0) = 1, y'(0)=2 $$ To solve this diff. equation using Laplace transform. Seems very straightforward. On one side, we have $$\mathscr{L}\{y''+4y'+5y\} = \mathscr{L}\{y''\} + 4\mathscr{L}\{y'\} +5\mathscr{L}\{y\}=\\ 	\left [s^2\mathscr{L}\{y\}-sy(0)-y'(0)\right ] + 4\left [s\mathscr{L}\{y\}-y(0)\right ] + 5\mathscr{L}\{y\} =\\ 	\mathscr{L}\{y\}\left (s^2+4s+5\right ) -sy(0) -4y(0) -y'(0) = \mathscr{L}\{y\}\left (s^2+4s+5\right ) -s-6 $$ and on the other $$\mathscr{L}\{H(t-3)e^{-2t}\} = \int_0^\infty e^{-(s+2)t}H(t-3)\mbox{d}t = \int_0^3 0\mbox{d}t + \int_3^\infty e^{-(s+2)t}\mbox{d}t = -\frac{1}{s+2}e^{-(s+2)t}\bigg\vert_3^\infty = \frac{e^{-3(s+2)}}{s+2} $$ Which ultimately yields: $$\mathscr{L}\{y\} = \frac{s+6}{s^2+4s+5} +\frac{e^{-3(s+2)}}{(s+2)(s^2+4s+5)} $$ Since $\mathscr{L}$ is a linear operator, I take its inverse is also linear. Finding the inverse of the first summand is a piece of cake, however, how do we do the second one? $$\mathscr{L}^{-1}\left\lbrace\frac{e^{-3(s+2)}}{(s+2)(s^2+4s+5)} \right\rbrace $$ Oh, missed a useful theorem: $$\forall r>0: \mathscr{L}\{f(t-r)\} = e^{-sr}\mathscr{L}\{f(t)\} $$",,"['complex-analysis', 'ordinary-differential-equations', 'laplace-transform']"
97,Show that all the solutions of the given differential equation are bounded,Show that all the solutions of the given differential equation are bounded,,"Let $ f:[0,\infty)\rightarrow \Bbb R$ be a bounded and continuous function. Show that every solution of the differential equation $$y''+2y'+5y=f(t,)\quad t\ge 0$$ is bounded on $[0,\infty)$. By using the method of solution of Non-Homogeneous differential equation, I got the solution as $$y(t)=e^{-t}(C_1\cos 2t+C_2\sin 2t)+\frac{1}{D^2+2D+5}f(t)$$ I understand that complementary function of the equation is bounded and can have maximum value as $(C_1+C_2)$ but I have no idea about Particular integral function $i.e. \frac{1}{D^2+2D+5}f(t)$ Any help is appreciated.Thanx in advance.","Let $ f:[0,\infty)\rightarrow \Bbb R$ be a bounded and continuous function. Show that every solution of the differential equation $$y''+2y'+5y=f(t,)\quad t\ge 0$$ is bounded on $[0,\infty)$. By using the method of solution of Non-Homogeneous differential equation, I got the solution as $$y(t)=e^{-t}(C_1\cos 2t+C_2\sin 2t)+\frac{1}{D^2+2D+5}f(t)$$ I understand that complementary function of the equation is bounded and can have maximum value as $(C_1+C_2)$ but I have no idea about Particular integral function $i.e. \frac{1}{D^2+2D+5}f(t)$ Any help is appreciated.Thanx in advance.",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'functions']"
98,Converting higher-order ODE to first order ODE,Converting higher-order ODE to first order ODE,,"Given $$ y''' + 2y'' -5y' = 2y + 5y^3 $$ convert to a system of first order equations. My question is do we need to make substitutions for $y$ and $y^3$ or are we only concerned with the derivatives, if so then let $x_{1} = y$ , $x_{2} = y'$ , $x_{3} = y''$ then we get $x_1' = x_2$ , $x_2' = x_3$ , $x_3' = 5x_2 - 2x_3 + 2y + 5y^3$ but this doesn't have the same form of other similar problems I've solved. Do I let $x_4 = y$ and $x_5 = y^3$ or do I leave the y's in since they aren't derivatives?","Given convert to a system of first order equations. My question is do we need to make substitutions for and or are we only concerned with the derivatives, if so then let , , then we get , , but this doesn't have the same form of other similar problems I've solved. Do I let and or do I leave the y's in since they aren't derivatives?","
y''' + 2y'' -5y' = 2y + 5y^3
 y y^3 x_{1} = y x_{2} = y' x_{3} = y'' x_1' = x_2 x_2' = x_3 x_3' = 5x_2 - 2x_3 + 2y + 5y^3 x_4 = y x_5 = y^3",['ordinary-differential-equations']
99,$\omega$-limit set of a point $x \in X$,-limit set of a point,\omega x \in X,"I would like to verify whether the following definition of the $\omega$-limit set of a point $x \in X$ is correct: $$\omega(x) = \{ y \in M : \exists \text{ sequence }\{t_j\}, \text{ where } t_j \rightarrow \infty, \text { such that } \varphi_{t_j} (x) \rightarrow y \text{ as } j \rightarrow \infty \}.$$ The context is ODEs, so $M$ is the phase space.","I would like to verify whether the following definition of the $\omega$-limit set of a point $x \in X$ is correct: $$\omega(x) = \{ y \in M : \exists \text{ sequence }\{t_j\}, \text{ where } t_j \rightarrow \infty, \text { such that } \varphi_{t_j} (x) \rightarrow y \text{ as } j \rightarrow \infty \}.$$ The context is ODEs, so $M$ is the phase space.",,"['sequences-and-series', 'ordinary-differential-equations', 'limits', 'definition']"
