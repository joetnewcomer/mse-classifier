,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,If $F:\mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$ if of class $C^{1}$ ...,If  if of class  ...,F:\mathbb{R}^{m} \rightarrow \mathbb{R}^{m} C^{1},"I'm studying differentiation and came across this question: If $F:\mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$ is of class $C^{1}$ such that $\Vert f^{\prime}(x)v\Vert \geqslant 2\Vert v\Vert$. Then F is a diffeomorphism of  $\mathbb{R}^{m}$ in $\mathbb{R}^{m}$. I can not take the condition of $F$ to show that $F$ is injective. If you could help me, thanks.","I'm studying differentiation and came across this question: If $F:\mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$ is of class $C^{1}$ such that $\Vert f^{\prime}(x)v\Vert \geqslant 2\Vert v\Vert$. Then F is a diffeomorphism of  $\mathbb{R}^{m}$ in $\mathbb{R}^{m}$. I can not take the condition of $F$ to show that $F$ is injective. If you could help me, thanks.",,"['real-analysis', 'analysis']"
1,lebesgue density theorem for smooth manifold (revised),lebesgue density theorem for smooth manifold (revised),,"My question is about lebesgue density theorem: Let $\mathcal{H}^s$ be $s-$dimensional Hausdorff measure. If $A\subset \mathbb{R}^{n}$ with $0<\mathcal{H}^s(A)<\infty,$ then for $\mathcal{H}^{s}$ almost all $x\in A,$ $$\limsup_{r \rightarrow 0}\frac{\mathcal{H}^{s}(A\cap B(x,r))}{\beta_s r^s}\leq 1,$$ where $\beta_s$ is the $s-$dimensional Hausdorff measure of $s-$dimensional unit ball. Do we have the above inequality for all $x\in A$, if we assume that $A$ is a subset of a $C^{1}-$manifold? Thank you so much","My question is about lebesgue density theorem: Let $\mathcal{H}^s$ be $s-$dimensional Hausdorff measure. If $A\subset \mathbb{R}^{n}$ with $0<\mathcal{H}^s(A)<\infty,$ then for $\mathcal{H}^{s}$ almost all $x\in A,$ $$\limsup_{r \rightarrow 0}\frac{\mathcal{H}^{s}(A\cap B(x,r))}{\beta_s r^s}\leq 1,$$ where $\beta_s$ is the $s-$dimensional Hausdorff measure of $s-$dimensional unit ball. Do we have the above inequality for all $x\in A$, if we assume that $A$ is a subset of a $C^{1}-$manifold? Thank you so much",,['analysis']
2,A question on fubini theorem,A question on fubini theorem,,"Just read the last two lines of pg2 of the notes http://www.cims.nyu.edu/~chou/notes/harmonic , why the last integral is supported on $\{|f(x)| > \alpha\}$ but not $\{|f(x)|\geq\alpha\}$? Since we can define $f(x,\alpha)=1.\alpha^{p-1}$ which is supported on the set $\{(x,\alpha)\in (R^n,R)|0\leq\alpha\leq f(x)\}$ , fubini theorem should come in here . Can anyone give me a more detailed explanation of the proof by using fubini ? Thanks in advance .","Just read the last two lines of pg2 of the notes http://www.cims.nyu.edu/~chou/notes/harmonic , why the last integral is supported on $\{|f(x)| > \alpha\}$ but not $\{|f(x)|\geq\alpha\}$? Since we can define $f(x,\alpha)=1.\alpha^{p-1}$ which is supported on the set $\{(x,\alpha)\in (R^n,R)|0\leq\alpha\leq f(x)\}$ , fubini theorem should come in here . Can anyone give me a more detailed explanation of the proof by using fubini ? Thanks in advance .",,"['real-analysis', 'analysis', 'measure-theory']"
3,Properties of Eigenfunctions of a Kernel,Properties of Eigenfunctions of a Kernel,,"I'm a newbie and may be this question is bit simple for you but pardon me if it's too simple and provide me some references. I've and Kernel function $K(x,y)$ $f(x)=(Kg)(x)=\int_{\Omega}K(x,y)g(y)dy$ $\Omega$, a compact set of $\Bbb R^2$. Let's assume that $K$ maps from one Hilbert space to another; let's say $L^2(\Omega)$ and it has an orthonormal basis of eigenpairs$(\lambda_i,f_i)_{i \in N }$. My question is: is there any general theory regarding the nature of $K(x,y)$ in the following cases If I need all the eigenfunctions $(f_i(x))$ such that  for   all  $f_i$ the partial derivatives of $f_i$ wrt $x_1$ and $x_2$ will  be same.$\partial_{x_1} f_i(x)=\partial_{x_2} f_i(x) , \forall i \in \Bbb N $ This's simple. I only need the Eigenfunctions to have nice regularity. Here I need only some reference in some books or papers. Lastly I've tried here writing Latex but it seems the same code as it is does not work here.Any tricks? Arwin","I'm a newbie and may be this question is bit simple for you but pardon me if it's too simple and provide me some references. I've and Kernel function $K(x,y)$ $f(x)=(Kg)(x)=\int_{\Omega}K(x,y)g(y)dy$ $\Omega$, a compact set of $\Bbb R^2$. Let's assume that $K$ maps from one Hilbert space to another; let's say $L^2(\Omega)$ and it has an orthonormal basis of eigenpairs$(\lambda_i,f_i)_{i \in N }$. My question is: is there any general theory regarding the nature of $K(x,y)$ in the following cases If I need all the eigenfunctions $(f_i(x))$ such that  for   all  $f_i$ the partial derivatives of $f_i$ wrt $x_1$ and $x_2$ will  be same.$\partial_{x_1} f_i(x)=\partial_{x_2} f_i(x) , \forall i \in \Bbb N $ This's simple. I only need the Eigenfunctions to have nice regularity. Here I need only some reference in some books or papers. Lastly I've tried here writing Latex but it seems the same code as it is does not work here.Any tricks? Arwin",,"['real-analysis', 'analysis', 'functional-analysis', 'integral-transforms']"
4,An existence Theorem,An existence Theorem,,"Data: $\Omega \subset \mathbb{R}^{n}$ is an open connected (may be unbounded) set, and locally $\partial \Omega$ is aLipschitz graph. $S \subset \partial \Omega$ is measurabel and $H^{n-1}(S)>0.$ The Dirichlet data on $S$ are given by non-negative function $u^0 \in ^{1}_{Loc}(\Omega)$ with $\nabla u^0 \in L^{2}(\Omega)$. The given force function $Q$ is non-negative and measurable. Consider the convex set  \begin{equation} K:=\{ v \in L^{1}_{Loc}(\Omega): \nabla v \in L^{2}(\Omega) \quad \mbox{and} \quad v=u^0 \quad \mbox{on} S\}. \end{equation} We are looking for an absolute minimum of the functional \begin{equation} J(v):= \int_{\Omega}(|\nabla v|^{2} + \chi(\{v>0\})Q^2) \end{equation} in the class $K$. I wish someone would remake this theorem explaining the details. I'll be very grateful. Existence theorem. If $J(u_0)< \infty$ then exist an absolute minimum $u \in K$ of the functional $J$. Since $J$ is non-negative there is a minimal sequence $u_k, k \in \mathbb{N}$. Then $\nabla u_k$ are bounded in $L^{2}(\Omega)$ and since $H^{n-1}(S)$ is positive $u_k -u^0$ are bounded in $L^2(B_r \cap \Omega)$ for large $R$. Therefore there is an $u \in K$, such that for a subsequence \begin{equation} \nabla u_k \rightarrow \nabla u \quad \mbox{weakly in} \quad L^2(\Omega), \ u_k \rightarrow u \quad \mbox{almost everywhere in} \  \Omega. \end{equation} Moreover there is a function $\gamma \in L^\infty(\Omega)$ with $0 \le \gamma \le 1$, such that \begin{equation} \chi(\{u_k>0\})  \rightarrow \gamma \quad \mbox{weakly star in} \quad L^\infty(\Omega). \end{equation} Then for $R>0$ \begin{eqnarray} \int_{B_R \cap \Omega}(|\nabla u|^2 + \gamma \min({Q,R})^2 &\le& \liminf_{k} \int_{B_R \cap \Omega}(|\nabla u_k|^2  + \lim_{k}\int_{B_R \cap \Omega} \chi(\{u_k>0\}) \min(Q,R)^2 \\  &\le & \lim_{k} J(u_k). \\ \end{eqnarray} Letting $R\rightarrow \infty$, and since $\gamma = 1$ almost everywhere in $\{u>0\}$  we conclude \begin{equation} J(u) \le \int_{\Omega}(|\nabla u|^2 + \gamma Q^2) \le \lim_{k} J(u_k). \end{equation} The details can be found in the article Alt, H. M. and Caffarelli, L. A. Existence and regularity for a minimum problem with free boundary. J. Reine Angew. Math., 325, (1981), 105–144. in the page 3","Data: $\Omega \subset \mathbb{R}^{n}$ is an open connected (may be unbounded) set, and locally $\partial \Omega$ is aLipschitz graph. $S \subset \partial \Omega$ is measurabel and $H^{n-1}(S)>0.$ The Dirichlet data on $S$ are given by non-negative function $u^0 \in ^{1}_{Loc}(\Omega)$ with $\nabla u^0 \in L^{2}(\Omega)$. The given force function $Q$ is non-negative and measurable. Consider the convex set  \begin{equation} K:=\{ v \in L^{1}_{Loc}(\Omega): \nabla v \in L^{2}(\Omega) \quad \mbox{and} \quad v=u^0 \quad \mbox{on} S\}. \end{equation} We are looking for an absolute minimum of the functional \begin{equation} J(v):= \int_{\Omega}(|\nabla v|^{2} + \chi(\{v>0\})Q^2) \end{equation} in the class $K$. I wish someone would remake this theorem explaining the details. I'll be very grateful. Existence theorem. If $J(u_0)< \infty$ then exist an absolute minimum $u \in K$ of the functional $J$. Since $J$ is non-negative there is a minimal sequence $u_k, k \in \mathbb{N}$. Then $\nabla u_k$ are bounded in $L^{2}(\Omega)$ and since $H^{n-1}(S)$ is positive $u_k -u^0$ are bounded in $L^2(B_r \cap \Omega)$ for large $R$. Therefore there is an $u \in K$, such that for a subsequence \begin{equation} \nabla u_k \rightarrow \nabla u \quad \mbox{weakly in} \quad L^2(\Omega), \ u_k \rightarrow u \quad \mbox{almost everywhere in} \  \Omega. \end{equation} Moreover there is a function $\gamma \in L^\infty(\Omega)$ with $0 \le \gamma \le 1$, such that \begin{equation} \chi(\{u_k>0\})  \rightarrow \gamma \quad \mbox{weakly star in} \quad L^\infty(\Omega). \end{equation} Then for $R>0$ \begin{eqnarray} \int_{B_R \cap \Omega}(|\nabla u|^2 + \gamma \min({Q,R})^2 &\le& \liminf_{k} \int_{B_R \cap \Omega}(|\nabla u_k|^2  + \lim_{k}\int_{B_R \cap \Omega} \chi(\{u_k>0\}) \min(Q,R)^2 \\  &\le & \lim_{k} J(u_k). \\ \end{eqnarray} Letting $R\rightarrow \infty$, and since $\gamma = 1$ almost everywhere in $\{u>0\}$  we conclude \begin{equation} J(u) \le \int_{\Omega}(|\nabla u|^2 + \gamma Q^2) \le \lim_{k} J(u_k). \end{equation} The details can be found in the article Alt, H. M. and Caffarelli, L. A. Existence and regularity for a minimum problem with free boundary. J. Reine Angew. Math., 325, (1981), 105–144. in the page 3",,"['analysis', 'functional-analysis', 'partial-differential-equations']"
5,"""Green Globs"" question","""Green Globs"" question",,"When I was in high school geometry, we had a fun little game on the computer called Green Globs (the website for the software is http://www.greenglobs.net/index.html ).  A number of targets (globs) are randomly produced, and basically, the point of the game is to enter math equations to destroy targets in the least equations possible; writing an equation that intersects a glob means that it is destroyed.  I haven't thought too much about this generalization but I thought it might be fun to examine, and I'm sure some work has been done in some field on a similar type question.  The question I would eventually answer is the following : ""Let $R >0$ be a positive real number.  Let $n >0$ be an integer. Fix some diameter $d <R$ and consider $n$ balls $\{B_i\}_{i=1}^n$ of diameter $d$ placed randomly inside the ball $B(0,R)$ of radius $R$ centered at the origin, such that each ball $B_i$ is contained entirely inside $B(0,R)$. (Intersections of the $B_i$ are acceptable).  Fix some number $w$ denoting width. Given $R,n,d,w$ as above, what is the average number of lines of width $w$ necessary so that at least one line intersects each ball in at least one point?"" Now, I'm sure that in it's full generality as above, the question is quite hard.  Perhaps examining specific cases are easier or have already been done.  Do you know of any literature on this subject or something similiar?","When I was in high school geometry, we had a fun little game on the computer called Green Globs (the website for the software is http://www.greenglobs.net/index.html ).  A number of targets (globs) are randomly produced, and basically, the point of the game is to enter math equations to destroy targets in the least equations possible; writing an equation that intersects a glob means that it is destroyed.  I haven't thought too much about this generalization but I thought it might be fun to examine, and I'm sure some work has been done in some field on a similar type question.  The question I would eventually answer is the following : ""Let $R >0$ be a positive real number.  Let $n >0$ be an integer. Fix some diameter $d <R$ and consider $n$ balls $\{B_i\}_{i=1}^n$ of diameter $d$ placed randomly inside the ball $B(0,R)$ of radius $R$ centered at the origin, such that each ball $B_i$ is contained entirely inside $B(0,R)$. (Intersections of the $B_i$ are acceptable).  Fix some number $w$ denoting width. Given $R,n,d,w$ as above, what is the average number of lines of width $w$ necessary so that at least one line intersects each ball in at least one point?"" Now, I'm sure that in it's full generality as above, the question is quite hard.  Perhaps examining specific cases are easier or have already been done.  Do you know of any literature on this subject or something similiar?",,"['geometry', 'analysis', 'recreational-mathematics', 'computational-mathematics']"
6,"Convergence of sequence in $C^{k, \alpha}$ composed with $C^\infty$ function",Convergence of sequence in  composed with  function,"C^{k, \alpha} C^\infty","Is it true that if a sequence $u_n \to u$ in $C^{k, \alpha}$ norm, and if you have a function $f \in C^\infty$, then $f(u_n) \to f(u)$ in $C^{k,  \alpha}$? I think so, since this is true for ordinary $C^k$ space so the ""norm part"" of the $C^{k, \alpha}$ norm converges, but I am not sure how to show that the seminorm part of the $C^{k, \alpha}$ norm converges. And I guess if this works for Hölder space, it'll work for parabolic Hölder space too. Parabolic Hölder space is defined as follows. The seminorm is defined $$[u]_{\alpha} = \sup_{(x,t), (y,s) \in Q} \frac{|u(x,t) - u(y,s)|}{(|x-y|^2 + |t-s|)^{\frac{\alpha}{2}}},$$ and norm on the parabolic Hölder space $C^{k, \alpha}$ is defined $$\lVert{u}\rVert_{\widetilde{C}^{k, \alpha}(\overline{Q})} = \sum_{i+2j \leq k} \lVert{\frac{\partial^{i+j}u}{\partial x^i \partial t^j}}\rVert_{C(\overline{Q})} + \sum_{i+2j = k} \bigg[\frac{\partial^{i+j}u}{\partial x^i \partial t^j}\bigg]_\alpha.$$","Is it true that if a sequence $u_n \to u$ in $C^{k, \alpha}$ norm, and if you have a function $f \in C^\infty$, then $f(u_n) \to f(u)$ in $C^{k,  \alpha}$? I think so, since this is true for ordinary $C^k$ space so the ""norm part"" of the $C^{k, \alpha}$ norm converges, but I am not sure how to show that the seminorm part of the $C^{k, \alpha}$ norm converges. And I guess if this works for Hölder space, it'll work for parabolic Hölder space too. Parabolic Hölder space is defined as follows. The seminorm is defined $$[u]_{\alpha} = \sup_{(x,t), (y,s) \in Q} \frac{|u(x,t) - u(y,s)|}{(|x-y|^2 + |t-s|)^{\frac{\alpha}{2}}},$$ and norm on the parabolic Hölder space $C^{k, \alpha}$ is defined $$\lVert{u}\rVert_{\widetilde{C}^{k, \alpha}(\overline{Q})} = \sum_{i+2j \leq k} \lVert{\frac{\partial^{i+j}u}{\partial x^i \partial t^j}}\rVert_{C(\overline{Q})} + \sum_{i+2j = k} \bigg[\frac{\partial^{i+j}u}{\partial x^i \partial t^j}\bigg]_\alpha.$$",,"['analysis', 'functional-analysis', 'convergence-divergence', 'holder-spaces']"
7,Two questions about ultraweak and ultrastrong topology from Dixmier,Two questions about ultraweak and ultrastrong topology from Dixmier,,"You could reference Dixmier's book on Von Neumann Algebras p.42 Theorem 1 and its proof to know the entirety of the context.  Otherwise, the most relevant things are below: Let $M$ be an ultraweakly closed subspace of $B(H)$ and $K$ a convex subset of $M$.  Then he claims $K$ is ultraweakly closed if and only if $K\cap M_r$ is ultraweakly closed for all balls $M_r$ of radius $r$ in $M$ centered at $0$.  Former implies latter is clear.  What about the other way?  He doesn't really make an attempt to prove this and instead references some outside fact and a seemingly unrelated subclaim.  I would not mind if an entirely new proof were suggested in the answers, although a filling in of the missing details would be equally appreciated. He then uses this fact to prove that if $\phi$ is a linear form on $M$, continuous in norm, for which its restriction to the unit ball is continuous in ultrastrong topology, then $\phi$ is actually ultraweakly continuous.  Along the way, he concludes (and I agree with this given 1.) that $\phi^{-1}({0})$ is ultraweakly closed.  I just don't see why this implies the conclusion, even though he asserts it without further comment in the next step.  Thanks!","You could reference Dixmier's book on Von Neumann Algebras p.42 Theorem 1 and its proof to know the entirety of the context.  Otherwise, the most relevant things are below: Let $M$ be an ultraweakly closed subspace of $B(H)$ and $K$ a convex subset of $M$.  Then he claims $K$ is ultraweakly closed if and only if $K\cap M_r$ is ultraweakly closed for all balls $M_r$ of radius $r$ in $M$ centered at $0$.  Former implies latter is clear.  What about the other way?  He doesn't really make an attempt to prove this and instead references some outside fact and a seemingly unrelated subclaim.  I would not mind if an entirely new proof were suggested in the answers, although a filling in of the missing details would be equally appreciated. He then uses this fact to prove that if $\phi$ is a linear form on $M$, continuous in norm, for which its restriction to the unit ball is continuous in ultrastrong topology, then $\phi$ is actually ultraweakly continuous.  Along the way, he concludes (and I agree with this given 1.) that $\phi^{-1}({0})$ is ultraweakly closed.  I just don't see why this implies the conclusion, even though he asserts it without further comment in the next step.  Thanks!",,"['analysis', 'functional-analysis', 'operator-theory', 'operator-algebras']"
8,Seemingly obvious density question with bounds on higher partial derivatives,Seemingly obvious density question with bounds on higher partial derivatives,,"I have the following two linear spaces of functions: $\mathcal{A}(\mathbb{R}^n)$ contains all $f \in C^{\infty}(\mathbb{R}^{2n})$ such that for each multi-indices $\alpha, \beta$ there is a constant $C_{\alpha, \beta} > 0$ with $$|\partial_x^{\alpha} \partial_{\xi}^{\beta} f(x, \xi)| \leq C_{\alpha, \beta} (1+ |x| + |\xi|)^{-|\alpha| - |\beta|}.$$  And $\mathcal{B}(\mathbb{R}^{n+m})$ in the same way contains all functions $f \in C^{\infty}(\mathbb{R}^{2n + 2m})$ such that $$|\partial_{x}^{\alpha} \partial_{\xi}^{\beta} \partial_{y}^{\gamma} \partial_{\eta}^{\delta} f(x, \xi, y, \eta)| \leq C_{\alpha, \beta, \gamma, \delta} (1 + |x| + |\xi|)^{-|\alpha| - |\beta|} (1 + |y| + |\eta|)^{-|\gamma| - |\delta|}.$$ Here $\mathcal{A}(\mathbb{R}^n)$ is given the Fréchet topology of optimal constants, i.e. the topology generated by the seminorms $$\|f\|_{\alpha, \beta, K} := \sup_{(x, \xi) \in K}|((1 + |x| + |\xi|)^{|\alpha| + |\beta|}) \partial_x^{\alpha} \partial_{\xi}^{\beta} f(x, \xi)|$$ for each $K \subset \mathbb{R}^{2n}$ compact and multi-indices $\alpha, \beta$. Analogously for $\mathcal{B}$. QUESTION: Is $\mathcal{A}(\mathbb{R}^n) \otimes \mathcal{A}(\mathbb{R}^m)$ dense in $\mathcal{B}(\mathbb{R}^{n+m})$? IDEAS: I think the answer is yes as would anyone by looking at it. But I don't have a clear argument. I suspect ultimately a reduction to the density $C_c^{\infty}(\mathbb{R}^{n}) \otimes C_c^{\infty}(\mathbb{R}^m) \subset C_c^{\infty}(\mathbb{R}^{m+n})$ by a clever application of bump functions will do the trick. But if anyone has an idea or a good direct proof I'd be delighted. EDIT: Answered question about topology.","I have the following two linear spaces of functions: $\mathcal{A}(\mathbb{R}^n)$ contains all $f \in C^{\infty}(\mathbb{R}^{2n})$ such that for each multi-indices $\alpha, \beta$ there is a constant $C_{\alpha, \beta} > 0$ with $$|\partial_x^{\alpha} \partial_{\xi}^{\beta} f(x, \xi)| \leq C_{\alpha, \beta} (1+ |x| + |\xi|)^{-|\alpha| - |\beta|}.$$  And $\mathcal{B}(\mathbb{R}^{n+m})$ in the same way contains all functions $f \in C^{\infty}(\mathbb{R}^{2n + 2m})$ such that $$|\partial_{x}^{\alpha} \partial_{\xi}^{\beta} \partial_{y}^{\gamma} \partial_{\eta}^{\delta} f(x, \xi, y, \eta)| \leq C_{\alpha, \beta, \gamma, \delta} (1 + |x| + |\xi|)^{-|\alpha| - |\beta|} (1 + |y| + |\eta|)^{-|\gamma| - |\delta|}.$$ Here $\mathcal{A}(\mathbb{R}^n)$ is given the Fréchet topology of optimal constants, i.e. the topology generated by the seminorms $$\|f\|_{\alpha, \beta, K} := \sup_{(x, \xi) \in K}|((1 + |x| + |\xi|)^{|\alpha| + |\beta|}) \partial_x^{\alpha} \partial_{\xi}^{\beta} f(x, \xi)|$$ for each $K \subset \mathbb{R}^{2n}$ compact and multi-indices $\alpha, \beta$. Analogously for $\mathcal{B}$. QUESTION: Is $\mathcal{A}(\mathbb{R}^n) \otimes \mathcal{A}(\mathbb{R}^m)$ dense in $\mathcal{B}(\mathbb{R}^{n+m})$? IDEAS: I think the answer is yes as would anyone by looking at it. But I don't have a clear argument. I suspect ultimately a reduction to the density $C_c^{\infty}(\mathbb{R}^{n}) \otimes C_c^{\infty}(\mathbb{R}^m) \subset C_c^{\infty}(\mathbb{R}^{m+n})$ by a clever application of bump functions will do the trick. But if anyone has an idea or a good direct proof I'd be delighted. EDIT: Answered question about topology.",,"['general-topology', 'analysis']"
9,analysis limit question,analysis limit question,,Let f be an integrable function on $\mathbb{R}$. Show that $\lim_{t\rightarrow 0} \int_{\mathbb{R}}|f(x + t) -f(x)|dx = 0$. I can make it work once it is shown to be true for $f\in C_c(\mathbb{R})$ but I am having trouble proving this case.,Let f be an integrable function on $\mathbb{R}$. Show that $\lim_{t\rightarrow 0} \int_{\mathbb{R}}|f(x + t) -f(x)|dx = 0$. I can make it work once it is shown to be true for $f\in C_c(\mathbb{R})$ but I am having trouble proving this case.,,['analysis']
10,What is this norm?,What is this norm?,,"If $A(t)$ denotes for each fixed $t$ a (smooth) surface in $\mathbb{R}^n$, what is the norm on the space $$L^2\left(\cup_{t \in [0,T]} A(t)\times \{t\}\right)?$$ Is it $$\lVert f \rVert^2 = \int_0^T{\lVert f \rVert^2_{L^2(A(t))}}$$ (the usual Bochner space thing) or is it $$\lVert f \rVert^2 = \int_0^T\int_{A(t)}{f^2(x,t)}\;\mathrm{d}x\;\mathrm{d}t$$ If it's the latter can I change the order of integration?","If $A(t)$ denotes for each fixed $t$ a (smooth) surface in $\mathbb{R}^n$, what is the norm on the space $$L^2\left(\cup_{t \in [0,T]} A(t)\times \{t\}\right)?$$ Is it $$\lVert f \rVert^2 = \int_0^T{\lVert f \rVert^2_{L^2(A(t))}}$$ (the usual Bochner space thing) or is it $$\lVert f \rVert^2 = \int_0^T\int_{A(t)}{f^2(x,t)}\;\mathrm{d}x\;\mathrm{d}t$$ If it's the latter can I change the order of integration?",,"['analysis', 'functional-analysis']"
11,What is $\mathcal{C}(S^{1})$? (Where $S^1$ denotes unit circle),What is ? (Where  denotes unit circle),\mathcal{C}(S^{1}) S^1,"What is $\mathcal{C}(S^{1})$ (Continuous function on a unit circle)? (Where $S^1$ denotes unit circle) I saw this in a proof of showing Fourier Basis $S:=\{1,\sqrt{2}\cos{nx},\sqrt{2}\sin{nx}\}$ is an orthonormal basis of $L^{2}[-\pi ,\pi]$ The proof says $\mathcal{C}(S^{1})$ is equivalent to $C^{*}[-\pi,\pi]=\{f\in C[-\pi,\pi]:f(-\pi)=f(\pi)\}$ then used the facts S is dense in $\mathcal{C}(S^{1})$ $C^{*}[-\pi,\pi]$ is dense in $C[-\pi,\pi]$ and $C[-\pi,\pi]$ is dense in $L^{2}[-\pi ,\pi]$","What is $\mathcal{C}(S^{1})$ (Continuous function on a unit circle)? (Where $S^1$ denotes unit circle) I saw this in a proof of showing Fourier Basis $S:=\{1,\sqrt{2}\cos{nx},\sqrt{2}\sin{nx}\}$ is an orthonormal basis of $L^{2}[-\pi ,\pi]$ The proof says $\mathcal{C}(S^{1})$ is equivalent to $C^{*}[-\pi,\pi]=\{f\in C[-\pi,\pi]:f(-\pi)=f(\pi)\}$ then used the facts S is dense in $\mathcal{C}(S^{1})$ $C^{*}[-\pi,\pi]$ is dense in $C[-\pi,\pi]$ and $C[-\pi,\pi]$ is dense in $L^{2}[-\pi ,\pi]$",,"['analysis', 'fourier-analysis', 'hilbert-spaces']"
12,weak lower semicontinuity of some functionals.,weak lower semicontinuity of some functionals.,,"Let $(\nu_{j} - \phi)$ be is a bounded sequence in $W^{1,p}_{0}(\Omega)$. By reflexivity, there is a function $u \in W^{1,p}(\Omega)$ such that, up to a subsequence $$ \nu_{j} \rightarrow u \ \mbox{weakly} \ \mbox{in} \ W^{1,p}(\Omega), \nu_{j} \rightarrow u \mbox{in} \ L^{p}(\Omega), \nu_{j} \rightarrow u \ \mbox{a.e in} \ \Omega.$$ I know that from lower semicontiuity of norms we obtain $$ \int_{\Omega} | \nabla u |^{p} dx \le \liminf_{j} \int_{\Omega} | \nabla  \nu_{j} |^{p} dx. $$  But I don't undertand  it. If I had $\nabla u(x) \rightarrow \nabla \nu_{j}(x)$ I could use Fatou's lemma.  I would like to know this in the hope of showing \begin{equation} \int_{\Omega} \dfrac{1}{2} \langle A(x) \nabla u(x), \nabla u(x)\rangle \le \liminf_{j} \int_{\Omega} \dfrac{1}{2} \langle A(x) \nabla u_j(x), \nabla u_j(x)\rangle \end{equation} or \begin{equation} \int_{\{u>0\}} \dfrac{1}{2} \langle A(x) \nabla u(x), \nabla u(x)\rangle \le \liminf_{j} \int_{\{u_j >0\}} \dfrac{1}{2} \langle A(x) \nabla u_j(x), \nabla u_j(x)\rangle \end{equation} where $A$ is a continuous matrix.","Let $(\nu_{j} - \phi)$ be is a bounded sequence in $W^{1,p}_{0}(\Omega)$. By reflexivity, there is a function $u \in W^{1,p}(\Omega)$ such that, up to a subsequence $$ \nu_{j} \rightarrow u \ \mbox{weakly} \ \mbox{in} \ W^{1,p}(\Omega), \nu_{j} \rightarrow u \mbox{in} \ L^{p}(\Omega), \nu_{j} \rightarrow u \ \mbox{a.e in} \ \Omega.$$ I know that from lower semicontiuity of norms we obtain $$ \int_{\Omega} | \nabla u |^{p} dx \le \liminf_{j} \int_{\Omega} | \nabla  \nu_{j} |^{p} dx. $$  But I don't undertand  it. If I had $\nabla u(x) \rightarrow \nabla \nu_{j}(x)$ I could use Fatou's lemma.  I would like to know this in the hope of showing \begin{equation} \int_{\Omega} \dfrac{1}{2} \langle A(x) \nabla u(x), \nabla u(x)\rangle \le \liminf_{j} \int_{\Omega} \dfrac{1}{2} \langle A(x) \nabla u_j(x), \nabla u_j(x)\rangle \end{equation} or \begin{equation} \int_{\{u>0\}} \dfrac{1}{2} \langle A(x) \nabla u(x), \nabla u(x)\rangle \le \liminf_{j} \int_{\{u_j >0\}} \dfrac{1}{2} \langle A(x) \nabla u_j(x), \nabla u_j(x)\rangle \end{equation} where $A$ is a continuous matrix.",,"['analysis', 'functional-analysis']"
13,Calculus on complete fields of positive characteristic,Calculus on complete fields of positive characteristic,,"Are there complete fields of positive characteristic with non-trivial absolute value? What does calculus on them looks like? I'm aware that they have to be non-archimedean, and that the bulk of results about power series convergence will carry over, but surely there have to be some extremely weird results e.g. local expansions will be ambiguous.","Are there complete fields of positive characteristic with non-trivial absolute value? What does calculus on them looks like? I'm aware that they have to be non-archimedean, and that the bulk of results about power series convergence will carry over, but surely there have to be some extremely weird results e.g. local expansions will be ambiguous.",,"['abstract-algebra', 'analysis']"
14,"Doubt about function value (expected undefined, but Wolframalpha says otherwise)","Doubt about function value (expected undefined, but Wolframalpha says otherwise)",,"I have the function $f(x) = 1 + \frac{12x+4}{\left( x+1 \right)^2} \cdot \left( \frac{12}{12x+4} - \frac{2}{x+1} \right)$ (actually the derivate of another function, but that shouldn't matter). Since $12x+4=0$ for $x=\frac{-1}{3}$ I would have said the function value is undefined at that point - but Wolframalpha says it's 28! Now what I think might have happened is that WA secretly showed me the limit of $x\to\frac{-1}{3}$ and just didn't bother to tell me. Or, am I missing something else?","I have the function $f(x) = 1 + \frac{12x+4}{\left( x+1 \right)^2} \cdot \left( \frac{12}{12x+4} - \frac{2}{x+1} \right)$ (actually the derivate of another function, but that shouldn't matter). Since $12x+4=0$ for $x=\frac{-1}{3}$ I would have said the function value is undefined at that point - but Wolframalpha says it's 28! Now what I think might have happened is that WA secretly showed me the limit of $x\to\frac{-1}{3}$ and just didn't bother to tell me. Or, am I missing something else?",,"['analysis', 'functions']"
15,How to prove that $\lim_{\lambda \rightarrow 0} f*h_\lambda (x)=f(x) $ a.e.,How to prove that  a.e.,\lim_{\lambda \rightarrow 0} f*h_\lambda (x)=f(x) ,"Assume that $h_\lambda(x)=\frac{1}{\pi} \frac{\lambda}{\lambda^2+x^2}$, for $\lambda>0$, $x \in \mathbb{R}$. I know that if $f\in L^p$ then $\lim_{\lambda \rightarrow 0} \|f*h_\lambda -f\|_p =0$, for $1\leq p< \infty$   ( Rudin, Real and complex analysis, Thr.9.10). How to prove that if $f\in L^1$ then $$\lim_{\lambda \rightarrow 0} f*h_\lambda (x)=f(x) \textrm{ a.e. ?}$$","Assume that $h_\lambda(x)=\frac{1}{\pi} \frac{\lambda}{\lambda^2+x^2}$, for $\lambda>0$, $x \in \mathbb{R}$. I know that if $f\in L^p$ then $\lim_{\lambda \rightarrow 0} \|f*h_\lambda -f\|_p =0$, for $1\leq p< \infty$   ( Rudin, Real and complex analysis, Thr.9.10). How to prove that if $f\in L^1$ then $$\lim_{\lambda \rightarrow 0} f*h_\lambda (x)=f(x) \textrm{ a.e. ?}$$",,"['analysis', 'limits', 'convergence-divergence']"
16,Shrinking Map and Fixed Point via Iteration Method,Shrinking Map and Fixed Point via Iteration Method,,"Let $T$ be a map from a compact metric space $X$ into itself satisfying $ d(Tx,Ty) < d(x,y)$ for all distinct $x,y$ in $X$. It is true that $T$ has a unique fixed point. Fix $x_0 \in X$ any point, and define the recursive sequence $\{ x_n \}$ by $Tx_0 = x_1$, and $T^{n} x_0 = x_n $. If this sequence converges, it is true that it converges to the unique fixed point in $X$. However, I am having trouble showing that the sequence does converge, i.e., is Cauchy. I can show the sequence is Cauchy if the mapping had been a contraction, but the technique there doesn't generalize well. Edit: I came up with a simpler proof than from anything else I could find online, but it's not quite complete. Suppose for the sake of contradiction that $\{T^nx_0\}$ did not converge to the fixed point $\alpha$. Then $\exists \epsilon$ and a subseq $\{ T^{n(k)} x_0 \}$ s.t. for each $n(k)$, $d(T^{n(k)} x_0 , \alpha ) > \epsilon $. By compactness, $ \{ T^{n(k)} x_0 \}$ admits a convergent subseq. I want to say this subseq, say $\{ T^{n_k(m)} x_0\}_{m \in \mathbb{N}}$ and converging to $\beta$, must actually converge to $\alpha$ by continuity of $T$, but can can this be done? It's true that $T(T^{n_k(m))} x_0) \rightarrow T(\beta)$, but can I say $T^{n_k(m) + 1} x_0 \rightarrow \beta $, so that $\beta$ is actually the fixed point $\alpha$?","Let $T$ be a map from a compact metric space $X$ into itself satisfying $ d(Tx,Ty) < d(x,y)$ for all distinct $x,y$ in $X$. It is true that $T$ has a unique fixed point. Fix $x_0 \in X$ any point, and define the recursive sequence $\{ x_n \}$ by $Tx_0 = x_1$, and $T^{n} x_0 = x_n $. If this sequence converges, it is true that it converges to the unique fixed point in $X$. However, I am having trouble showing that the sequence does converge, i.e., is Cauchy. I can show the sequence is Cauchy if the mapping had been a contraction, but the technique there doesn't generalize well. Edit: I came up with a simpler proof than from anything else I could find online, but it's not quite complete. Suppose for the sake of contradiction that $\{T^nx_0\}$ did not converge to the fixed point $\alpha$. Then $\exists \epsilon$ and a subseq $\{ T^{n(k)} x_0 \}$ s.t. for each $n(k)$, $d(T^{n(k)} x_0 , \alpha ) > \epsilon $. By compactness, $ \{ T^{n(k)} x_0 \}$ admits a convergent subseq. I want to say this subseq, say $\{ T^{n_k(m)} x_0\}_{m \in \mathbb{N}}$ and converging to $\beta$, must actually converge to $\alpha$ by continuity of $T$, but can can this be done? It's true that $T(T^{n_k(m))} x_0) \rightarrow T(\beta)$, but can I say $T^{n_k(m) + 1} x_0 \rightarrow \beta $, so that $\beta$ is actually the fixed point $\alpha$?",,['analysis']
17,(RESOLVED) Interpreting a holomorphic function,(RESOLVED) Interpreting a holomorphic function,,"The equation for and electric field is given by $E=−∇ψ$ where $\psi$ is the potential, and in this case $ψ=−Q\ln r$ where $Q$ is just some constant. I have found its harmonic conjugate to be $−Qθ+c$ where $c$ is some constant. What does it say about the field? I know that if I calculate the field directly from $E=−∇ψ$, I get $E=Q/r$ pointing radially outwards, but I am not sure how to interpret the harmonic conjugate found (is it right?). Please help! Thank you. Added : I can see that $\psi$ is only dependent on $r$ and its h.c. only dependent on $\theta$ means that they are orthogonal, but what else about $E$ can I tell from the h.c. of $\psi$? Perhaps I have made a mistake but should the h.c. have a factor $1\over r$ in the term with the $\theta$? Update : Thank you to everyone who read this question. I believe I have figured it out now. The h.c. I have obtained is correct. The correct interpretation follows directly from the CR equations! I don't know why I didn't see this earlier...","The equation for and electric field is given by $E=−∇ψ$ where $\psi$ is the potential, and in this case $ψ=−Q\ln r$ where $Q$ is just some constant. I have found its harmonic conjugate to be $−Qθ+c$ where $c$ is some constant. What does it say about the field? I know that if I calculate the field directly from $E=−∇ψ$, I get $E=Q/r$ pointing radially outwards, but I am not sure how to interpret the harmonic conjugate found (is it right?). Please help! Thank you. Added : I can see that $\psi$ is only dependent on $r$ and its h.c. only dependent on $\theta$ means that they are orthogonal, but what else about $E$ can I tell from the h.c. of $\psi$? Perhaps I have made a mistake but should the h.c. have a factor $1\over r$ in the term with the $\theta$? Update : Thank you to everyone who read this question. I believe I have figured it out now. The h.c. I have obtained is correct. The correct interpretation follows directly from the CR equations! I don't know why I didn't see this earlier...",,"['analysis', 'complex-analysis']"
18,"$\max_{y} \min_{x} f(x,y)$ as motif for exploring mathematics",as motif for exploring mathematics,"\max_{y} \min_{x} f(x,y)","It's been several years since my undergraduate math days, and I would like to spend a bit of time refreshing and then tackling a few things I never completely mastered. Rather than proceeding topic by topic (e.g. analysis, algebra, topology, etc) I wanted to choose a motif to keep as a frame of reference -- that way, for example,  when I return to diff. geom. I can ask questions about what that topic has to say about my particular motif and hopefully not get completely lost in all the abstraction. I've chosen a particular motif, and was hoping people could comment on whether this motif seems general and interesting enough that the major branches and theorems of math might have something to conclude about this topic, and furthermore, I would be extremely grateful if people could suggest various little puzzles around this motif that would eventually tie in to the various deep theorems. The motif I am interested in exploring is the relationship between $\displaystyle\max_{y} \min_{x} f(x,y)$  and $\displaystyle \min_{x} \max_{y} f(x,y)$ I like this motif because 1) it feels like a lot of interesting real word problems involve mins of maxes or vice versa, and 2) it just seems really easy to pose interesting questions about this topic. For example: under what conditions of f does $\displaystyle\max_{y} \min_{x} f(x,y) = \min_{x} \max_{y} f(x,y)$ Assuming $f \in C^{\infty}$ or $f$ is convex, etc, what kinds of bounds can we place on $\displaystyle\max_{y} \min_{x} f(x,y) - \min_{x} \max_{y} f(x,y)$ in some compact, convex, etc region $A$? Thinking a f(x,y) as a discrete, lookup table (i.e. matrix) with random iid entries, what can we say about the probability distribution of $\displaystyle\max_{y} \min_{x} f(x,y) - \min_{x} \max_{y} f(x,y)$ The above questions are not meant to be deep issues for professionals, but more at the level of textbook exercises.  Currently they feel mostly linked to analysis with maybe a small touch of topology, but my intuition feels like I could keep going and find things that would feel like algebra, certainly geometry, and so forth.  What do people think? -- is there enough meat here, or is this motif too trivial in a way that I'm not really seeing (of course, I imagine there is a reasonable chance that the entire methodology will seem confusing -- my apologies)","It's been several years since my undergraduate math days, and I would like to spend a bit of time refreshing and then tackling a few things I never completely mastered. Rather than proceeding topic by topic (e.g. analysis, algebra, topology, etc) I wanted to choose a motif to keep as a frame of reference -- that way, for example,  when I return to diff. geom. I can ask questions about what that topic has to say about my particular motif and hopefully not get completely lost in all the abstraction. I've chosen a particular motif, and was hoping people could comment on whether this motif seems general and interesting enough that the major branches and theorems of math might have something to conclude about this topic, and furthermore, I would be extremely grateful if people could suggest various little puzzles around this motif that would eventually tie in to the various deep theorems. The motif I am interested in exploring is the relationship between $\displaystyle\max_{y} \min_{x} f(x,y)$  and $\displaystyle \min_{x} \max_{y} f(x,y)$ I like this motif because 1) it feels like a lot of interesting real word problems involve mins of maxes or vice versa, and 2) it just seems really easy to pose interesting questions about this topic. For example: under what conditions of f does $\displaystyle\max_{y} \min_{x} f(x,y) = \min_{x} \max_{y} f(x,y)$ Assuming $f \in C^{\infty}$ or $f$ is convex, etc, what kinds of bounds can we place on $\displaystyle\max_{y} \min_{x} f(x,y) - \min_{x} \max_{y} f(x,y)$ in some compact, convex, etc region $A$? Thinking a f(x,y) as a discrete, lookup table (i.e. matrix) with random iid entries, what can we say about the probability distribution of $\displaystyle\max_{y} \min_{x} f(x,y) - \min_{x} \max_{y} f(x,y)$ The above questions are not meant to be deep issues for professionals, but more at the level of textbook exercises.  Currently they feel mostly linked to analysis with maybe a small touch of topology, but my intuition feels like I could keep going and find things that would feel like algebra, certainly geometry, and so forth.  What do people think? -- is there enough meat here, or is this motif too trivial in a way that I'm not really seeing (of course, I imagine there is a reasonable chance that the entire methodology will seem confusing -- my apologies)",,"['analysis', 'soft-question', 'education']"
19,Notation for limit points of a minimizing sequence: $\arg \inf$,Notation for limit points of a minimizing sequence:,\arg \inf,"Could you tell me what is the accepted notation for the set of limit points of a minimizing sequence. For example, if I have a function $f(x)$ and a sequence $x_t$ such that $\lim f(x_t) = \inf f(x)$ I want a notation for the (possibly empty) set of limit points of all such sequences. I thought of using $\arg \inf$ but it is a bit misleading because no argument may actually achieve the $\inf$. Thanks for your help.","Could you tell me what is the accepted notation for the set of limit points of a minimizing sequence. For example, if I have a function $f(x)$ and a sequence $x_t$ such that $\lim f(x_t) = \inf f(x)$ I want a notation for the (possibly empty) set of limit points of all such sequences. I thought of using $\arg \inf$ but it is a bit misleading because no argument may actually achieve the $\inf$. Thanks for your help.",,"['analysis', 'notation', 'optimization']"
20,"What is the constant $e$, fundamentally? [duplicate]","What is the constant , fundamentally? [duplicate]",e,"This question already has answers here : Closed 12 years ago . Possible Duplicate: Why is the number e so important in mathematics? Intuitive Understanding of the constant “e” The number $e$ is important in many respects. If you ask anyone why it is important, you will get multiple answers. Some say the defining property is that the derivative of $e^x$ is $e^x$. Some say that it is due to it being the limit $(1+\frac{1}{n})^n$, some define it using the integral of $\frac{1}{x}$. So far my personal intuition has been that all of those explanations are somewhat secondary. That is, rather than describing some fundamental property of $e$, they just demonstrate the consequences. I understand that this opinion is somewhat subjective and the question is vague, but I hope someone here might understand what I mean and, perhaps, provide a ""better"" answer to the simple question of ""what is $e$""? Intuitively, I feel there are several directions which might provide a ""satisfying"" response. Firstly, perhaps $e$ can be expressed as an elegant and very important general invariant? Something like ""for each isomorphism $f$ between the additive and the multiplicative groups of $\mathbb{R}$, $e=f(f(x)/f'(x))$"". If only it were clear why such (or a similar) invariant must play an important role and participate in all those multiple well-known equations, it would clear things up a lot. Secondly, perhaps there is a nice ""information-theoretical"" explanation for $e$ playing, in some sense, the same role in the multiplicative group as $1$ does in the additive group of $\mathbb{R}$. I.e. with some generalizations $1$ can be viewed as a generator of $(\mathbb{R},+)$ (because any $a = 1\cdot a$), and although any other nonzero number would also work, $1$ is the most ""reasonable"" choice. Similarly, it seems that $e$ is the best choice for a generator in the multiplicative group. If it can be shown to be the ""most reasonable"" choice, perhaps it would become obvious (or, at least, ""natural"") for $e$ to pop up everywhere where infinite multiplications are involved. Finally, maybe the question I am asking is better phrased as ""what properties of the field of real numbers can be stripped away for $e$ to still remain a fundamentally important object""? For example, is there a similarly-important ""$e$"" in any continuous group? Thanks.","This question already has answers here : Closed 12 years ago . Possible Duplicate: Why is the number e so important in mathematics? Intuitive Understanding of the constant “e” The number $e$ is important in many respects. If you ask anyone why it is important, you will get multiple answers. Some say the defining property is that the derivative of $e^x$ is $e^x$. Some say that it is due to it being the limit $(1+\frac{1}{n})^n$, some define it using the integral of $\frac{1}{x}$. So far my personal intuition has been that all of those explanations are somewhat secondary. That is, rather than describing some fundamental property of $e$, they just demonstrate the consequences. I understand that this opinion is somewhat subjective and the question is vague, but I hope someone here might understand what I mean and, perhaps, provide a ""better"" answer to the simple question of ""what is $e$""? Intuitively, I feel there are several directions which might provide a ""satisfying"" response. Firstly, perhaps $e$ can be expressed as an elegant and very important general invariant? Something like ""for each isomorphism $f$ between the additive and the multiplicative groups of $\mathbb{R}$, $e=f(f(x)/f'(x))$"". If only it were clear why such (or a similar) invariant must play an important role and participate in all those multiple well-known equations, it would clear things up a lot. Secondly, perhaps there is a nice ""information-theoretical"" explanation for $e$ playing, in some sense, the same role in the multiplicative group as $1$ does in the additive group of $\mathbb{R}$. I.e. with some generalizations $1$ can be viewed as a generator of $(\mathbb{R},+)$ (because any $a = 1\cdot a$), and although any other nonzero number would also work, $1$ is the most ""reasonable"" choice. Similarly, it seems that $e$ is the best choice for a generator in the multiplicative group. If it can be shown to be the ""most reasonable"" choice, perhaps it would become obvious (or, at least, ""natural"") for $e$ to pop up everywhere where infinite multiplications are involved. Finally, maybe the question I am asking is better phrased as ""what properties of the field of real numbers can be stripped away for $e$ to still remain a fundamentally important object""? For example, is there a similarly-important ""$e$"" in any continuous group? Thanks.",,"['calculus', 'analysis', 'lie-groups']"
21,Proving uniform continuity on an interval,Proving uniform continuity on an interval,,"If I know that $f:(0,\infty)\rightarrow \Bbb R$ is uniformly continuous on the intervals $[a,\infty)$  and $(0,a]$, where $a$ is in $(0,\infty)$, how can I prove that it is uniformly continuous on $(0,\infty)$? I know the general definition of uniform continuity using epsilon-delta, but I am not sure how to apply it to the above. Thanks Edit: I meant Uniformly continuous on the first two intervals","If I know that $f:(0,\infty)\rightarrow \Bbb R$ is uniformly continuous on the intervals $[a,\infty)$  and $(0,a]$, where $a$ is in $(0,\infty)$, how can I prove that it is uniformly continuous on $(0,\infty)$? I know the general definition of uniform continuity using epsilon-delta, but I am not sure how to apply it to the above. Thanks Edit: I meant Uniformly continuous on the first two intervals",,"['real-analysis', 'analysis']"
22,Intuition about moment function derivation [OR] derivatives involving a time varying integration domain,Intuition about moment function derivation [OR] derivatives involving a time varying integration domain,,"$$ 	m_{{pq}}(t)=\iint\limits_{R(t)}h(x,y) dx dy  $$ where $	R(t)$ the domain of integration is time varying (In fact it is the only one which is time varying). And $$ 	h(x,y) = x^p y^q f(x,y) dx dy   $$ In general, $f(x,y)$ is a 2 dimensional function projected onto the power basis. To provide more context, just imagine that $f(x,y)$ is a binary image with only 1 white object of some random shape having a contour C(t). With respect to time, the white object displaces itself in the image and thus the meaning of time varying domain. The function $f(x,y)$ does not vary with respect to time. (The intensities of the object int the image are going to be constant). If I need to differentiate the first equation $m_{{pq}}(t)$ to get the derivative w.r.t time, we can write $$     \begin{equation}     \dot{m}_{pq}=\oint\limits_{C(t)} h(x,y)\; \dot{\textbf{x}}^T \textbf{n}\,dl     \label{eq:momentderiv}		     \end{equation} $$ where $\dot{\textbf{x}}$ is the velocity of the point $\textbf{x}$ in the x,y image plane $\textit{dl}$ is the infinitesimal increment along contour C(t) $\textbf{n}$ is the unit normal vector to the contour at any point $\textbf{x}$ on it. I am able to interpret the dot product between $\dot{\textbf{x}}$ and $\textbf{n}$ as the area of the infinitesimal surface between the contours at $C(t)$ and $C(t+dt)$. What i am not able to see is how exactly can the second equation be formally derived from the first equation? Further, by using the Green's theorem, the second equation (derivative of moment) above can be written in terms of divergence as $$ \begin{equation} 	\dot{m}_{pq}= \iint\limits_{R(t)}\, \,div(h(x,y)\cdot\dot{\textbf{x}}) \,dx\,dy  \label{eq:momentgreen} \end{equation} $$ I thought i had grasped the Green's theorem well but i dont see how this equation was obtained from the earlier moment derivative? I did a formal course before many years before without actually understanding it thoroughly.I have been quite out of touch with multivariable calculus for long.To be comfortable with interpreting and solving these kind of equations, what readings do you suggest? Books? articles? I wish to develop intuition  as well as mathematical techniques to solve these kind of problems that often appear in my domain.","$$ 	m_{{pq}}(t)=\iint\limits_{R(t)}h(x,y) dx dy  $$ where $	R(t)$ the domain of integration is time varying (In fact it is the only one which is time varying). And $$ 	h(x,y) = x^p y^q f(x,y) dx dy   $$ In general, $f(x,y)$ is a 2 dimensional function projected onto the power basis. To provide more context, just imagine that $f(x,y)$ is a binary image with only 1 white object of some random shape having a contour C(t). With respect to time, the white object displaces itself in the image and thus the meaning of time varying domain. The function $f(x,y)$ does not vary with respect to time. (The intensities of the object int the image are going to be constant). If I need to differentiate the first equation $m_{{pq}}(t)$ to get the derivative w.r.t time, we can write $$     \begin{equation}     \dot{m}_{pq}=\oint\limits_{C(t)} h(x,y)\; \dot{\textbf{x}}^T \textbf{n}\,dl     \label{eq:momentderiv}		     \end{equation} $$ where $\dot{\textbf{x}}$ is the velocity of the point $\textbf{x}$ in the x,y image plane $\textit{dl}$ is the infinitesimal increment along contour C(t) $\textbf{n}$ is the unit normal vector to the contour at any point $\textbf{x}$ on it. I am able to interpret the dot product between $\dot{\textbf{x}}$ and $\textbf{n}$ as the area of the infinitesimal surface between the contours at $C(t)$ and $C(t+dt)$. What i am not able to see is how exactly can the second equation be formally derived from the first equation? Further, by using the Green's theorem, the second equation (derivative of moment) above can be written in terms of divergence as $$ \begin{equation} 	\dot{m}_{pq}= \iint\limits_{R(t)}\, \,div(h(x,y)\cdot\dot{\textbf{x}}) \,dx\,dy  \label{eq:momentgreen} \end{equation} $$ I thought i had grasped the Green's theorem well but i dont see how this equation was obtained from the earlier moment derivative? I did a formal course before many years before without actually understanding it thoroughly.I have been quite out of touch with multivariable calculus for long.To be comfortable with interpreting and solving these kind of equations, what readings do you suggest? Books? articles? I wish to develop intuition  as well as mathematical techniques to solve these kind of problems that often appear in my domain.",,"['analysis', 'multivariable-calculus', 'intuition', 'integration', 'image-processing']"
23,Mathematical indicator for the flatness of a curve [closed],Mathematical indicator for the flatness of a curve [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question I am currently working on a computer science project where I have to evaluate charts. The charts are simple lines in a $x$ - $y$ coordinate-system, given by CSV files. The flatter the curve, the better for me. Now I am looking for an indicator for the ""flatness"" of this curves. My first idea was to calculate the first derivative of the function and then calculate the average between two points.  If this value is near $0$, then the function is pretty flat. Is that a good idea? Is there any better solution?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question I am currently working on a computer science project where I have to evaluate charts. The charts are simple lines in a $x$ - $y$ coordinate-system, given by CSV files. The flatter the curve, the better for me. Now I am looking for an indicator for the ""flatness"" of this curves. My first idea was to calculate the first derivative of the function and then calculate the average between two points.  If this value is near $0$, then the function is pretty flat. Is that a good idea? Is there any better solution?",,['analysis']
24,Dirac Delta distribution of discrete variables,Dirac Delta distribution of discrete variables,,"There exists the following relation for the Besselfunctions $j_{l}(\alpha\,x)$, $j_{l}(\beta\,x)$ with $\alpha$, $\beta$ $\in \mathbb{R}$  and  $x\in \mathbb{R}$ $$\int_{0}^{\infty}dx \, x^{2}\, j_{l}(\alpha\,x)\, j_{l}(\beta\,x) = \frac{\pi}{\beta}\, \delta(\alpha^{2}-\beta^{2}). $$ Now, assume $\alpha_{n} , \beta_{n} \in \mathbb{Z}$ , i.e. are discrete numbers, then the above equation can be rewritten in terms of $$\int_{0}^{\infty}dx \, x^{2}\, j_{l}(\alpha_{n}\,x)\, j_{l}(\beta_{n}\,x) =  \int_{0}^{\infty} \,dy \, dz \, \delta(y-\alpha_{n}) \, \delta(z-\beta_{n}) dx \, x^{2}\, j_{l}(y\,x)\, j_{l}(z\,x)$$ $$= \int_{0}^{\infty} \,dy \, dz \, \delta(y-\alpha_{n}) \, \delta(z-\beta_{n}) \frac{\pi}{z}\, \delta(y^{2}-z^{2})$$ $$=\frac{\pi}{\beta_{n}}\, \delta(\alpha_{n}^{2}-\beta_{n}^{2}). $$ Question: I don't understand how the Delta Distribution which is supposed to be continuous can be understood in terms of discrete variables, since the Dirac Delta within the theory of generalized functions is always used with some test function which is integrated over which is not possible in this case.","There exists the following relation for the Besselfunctions $j_{l}(\alpha\,x)$, $j_{l}(\beta\,x)$ with $\alpha$, $\beta$ $\in \mathbb{R}$  and  $x\in \mathbb{R}$ $$\int_{0}^{\infty}dx \, x^{2}\, j_{l}(\alpha\,x)\, j_{l}(\beta\,x) = \frac{\pi}{\beta}\, \delta(\alpha^{2}-\beta^{2}). $$ Now, assume $\alpha_{n} , \beta_{n} \in \mathbb{Z}$ , i.e. are discrete numbers, then the above equation can be rewritten in terms of $$\int_{0}^{\infty}dx \, x^{2}\, j_{l}(\alpha_{n}\,x)\, j_{l}(\beta_{n}\,x) =  \int_{0}^{\infty} \,dy \, dz \, \delta(y-\alpha_{n}) \, \delta(z-\beta_{n}) dx \, x^{2}\, j_{l}(y\,x)\, j_{l}(z\,x)$$ $$= \int_{0}^{\infty} \,dy \, dz \, \delta(y-\alpha_{n}) \, \delta(z-\beta_{n}) \frac{\pi}{z}\, \delta(y^{2}-z^{2})$$ $$=\frac{\pi}{\beta_{n}}\, \delta(\alpha_{n}^{2}-\beta_{n}^{2}). $$ Question: I don't understand how the Delta Distribution which is supposed to be continuous can be understood in terms of discrete variables, since the Dirac Delta within the theory of generalized functions is always used with some test function which is integrated over which is not possible in this case.",,['analysis']
25,Optimal tax rate,Optimal tax rate,,"Suppose you have two countries A and B, with a tax rate $T_A$ and $T_B$, respectively. The tax is redistributed to all people equally. Hence if you live in A and you make $I$ as income then you will finally  receive $$I*(1-T_A) + \overline{I}*T_A$$  where $\overline{I}$ is the average income in $A$.  The country A wants to choose an optimal rate, in order to do this the decision is taken at the median income. But the people can migrate if the new rate makes them poorer than if they were living in $B$. Of course this migration to B comes at a cost $M$, hence if the median income choose as new rate $T$ the people in A such that  $$I*(1-T) + \overline{I}*T < I*(1-T_B) + \overline{I}*T_B -M$$  will leave A for B. And symmetrically  the people in B such that  $$I*(1-T_B) + \overline{I}*T_B < I*(1-T) + \overline{I}*T -M$$ will leave B to A. Which changes the configuration of incomes in A and hence the decision of the median income, since his income depends on the average income. My question is how can we find the tax rate which will optimize the income of the median income after migration? I have thought of a dynamical approach, but it looks hard to show that we converge to an equilibrium. Are there is general tools for this kind of problem?","Suppose you have two countries A and B, with a tax rate $T_A$ and $T_B$, respectively. The tax is redistributed to all people equally. Hence if you live in A and you make $I$ as income then you will finally  receive $$I*(1-T_A) + \overline{I}*T_A$$  where $\overline{I}$ is the average income in $A$.  The country A wants to choose an optimal rate, in order to do this the decision is taken at the median income. But the people can migrate if the new rate makes them poorer than if they were living in $B$. Of course this migration to B comes at a cost $M$, hence if the median income choose as new rate $T$ the people in A such that  $$I*(1-T) + \overline{I}*T < I*(1-T_B) + \overline{I}*T_B -M$$  will leave A for B. And symmetrically  the people in B such that  $$I*(1-T_B) + \overline{I}*T_B < I*(1-T) + \overline{I}*T -M$$ will leave B to A. Which changes the configuration of incomes in A and hence the decision of the median income, since his income depends on the average income. My question is how can we find the tax rate which will optimize the income of the median income after migration? I have thought of a dynamical approach, but it looks hard to show that we converge to an equilibrium. Are there is general tools for this kind of problem?",,"['analysis', 'optimization', 'economics']"
26,Extending a convolution operator from $L^p(\mathbb{R}^d)$ to $L^p(\mathbb{R}^d;L^q(\Omega))$,Extending a convolution operator from  to,L^p(\mathbb{R}^d) L^p(\mathbb{R}^d;L^q(\Omega)),"Let $1<p,q<\infty$ and $\Omega$ some $\sigma$-finite measure space. Let $T$ denote a bounded convolution operator on $L^p(\mathbb{R}^d)$ with scalar valued kernel $K$ which is locally integrable on $\mathbb{R}^d\setminus\{0\}$. Define the associated $L^q(\Omega)$-valued extension $\vec{T}$ via $$(\vec{T}u)(x,\cdot)=\int_{\mathbb{R}^d}K(x-y)u(y,\cdot)dy$$ for sufficiently well-behaved $u\in L^p(\mathbb{R}^d;L^q(\Omega))$. Analogously, one can define a Banach space valued extension of $T$ for any Banach space $X$ via $$(\vec{T}u)(x)=\int_{\mathbb{R}^d}K(x-y)u(y)dy$$ for well-behaved $u\in L^p(\mathbb{R}^d;X)$. I'm looking for sufficient conditions for the $L^q(\Omega)$-valued extension $\vec{T}$ to extend to a continuous operator on $L^p(\mathbb{R}^d;L^q(\Omega))$. The cases $q=p$ and $q=2$ work without any assumptions on $T$ besides continuity on $L^p(\mathbb{R}^d)$ by Fubini and Marcinkiewicz-Zygmund, respectively. The case $q$ between $2$ and $p$ then follows by interpolation. Not taking the convolution structure into account, that's the best one can get in general. If $K$ is in fact in $L^1(\mathbb{R}^d)$ then $\vec{T}$ is continuous for any $p,q\in[1,\infty]$ by Young's inequality. This actually works for any Banach space valued extension of $T$, but $K\in L^1(\mathbb{R}^d)$ is quite restrictive. If $K$ satisfies a Hörmander condition, then $\vec{T}$ is continuous for any $p,q\in (1,\infty)$ and again this actually works for any Banach space valued extension of $T$, provided $\vec{T}$ is continuous on $L^p(\mathbb{R}^d;X)$ for some $p\in (1,\infty)$. Now the above conditions either exploit the structure of $L^q(\Omega)$ without exploiting the structure of $T$, i.e. that $T$ is a bounded convolution operator or they rely mainly on the structure of $T$ without taking the structure of $L^q(\Omega)$ into account. This leaves me with the vague feeling that one might be able to do better, i.e. that there are conditions on the convolution kernel $K$ which are less restrictive than the Hörmander condition or at least different but which still ensure that one can extend the bounded operator $T$ on $L^p(\mathbb{R}^d)$ to a bounded operator $\vec{T}$ on $L^p(\mathbb{R}^d;L^q(\Omega))$ for any $q\in (1,\infty)$. Any ideas or references would be highly appreciated. Of course the same holds true for counterexamples illustrating that one cannot do (significantly) better, if such counterexamples exist. In fact, I even wonder whether there is an example of a convolution operator which is bounded on $L^p(\mathbb{R}^d)$ for $1<p<\infty$ but does not admit a bounded extension to $L^p(\mathbb{R}^d;L^q(\Omega))$ for every $q\in(1,\infty)$.","Let $1<p,q<\infty$ and $\Omega$ some $\sigma$-finite measure space. Let $T$ denote a bounded convolution operator on $L^p(\mathbb{R}^d)$ with scalar valued kernel $K$ which is locally integrable on $\mathbb{R}^d\setminus\{0\}$. Define the associated $L^q(\Omega)$-valued extension $\vec{T}$ via $$(\vec{T}u)(x,\cdot)=\int_{\mathbb{R}^d}K(x-y)u(y,\cdot)dy$$ for sufficiently well-behaved $u\in L^p(\mathbb{R}^d;L^q(\Omega))$. Analogously, one can define a Banach space valued extension of $T$ for any Banach space $X$ via $$(\vec{T}u)(x)=\int_{\mathbb{R}^d}K(x-y)u(y)dy$$ for well-behaved $u\in L^p(\mathbb{R}^d;X)$. I'm looking for sufficient conditions for the $L^q(\Omega)$-valued extension $\vec{T}$ to extend to a continuous operator on $L^p(\mathbb{R}^d;L^q(\Omega))$. The cases $q=p$ and $q=2$ work without any assumptions on $T$ besides continuity on $L^p(\mathbb{R}^d)$ by Fubini and Marcinkiewicz-Zygmund, respectively. The case $q$ between $2$ and $p$ then follows by interpolation. Not taking the convolution structure into account, that's the best one can get in general. If $K$ is in fact in $L^1(\mathbb{R}^d)$ then $\vec{T}$ is continuous for any $p,q\in[1,\infty]$ by Young's inequality. This actually works for any Banach space valued extension of $T$, but $K\in L^1(\mathbb{R}^d)$ is quite restrictive. If $K$ satisfies a Hörmander condition, then $\vec{T}$ is continuous for any $p,q\in (1,\infty)$ and again this actually works for any Banach space valued extension of $T$, provided $\vec{T}$ is continuous on $L^p(\mathbb{R}^d;X)$ for some $p\in (1,\infty)$. Now the above conditions either exploit the structure of $L^q(\Omega)$ without exploiting the structure of $T$, i.e. that $T$ is a bounded convolution operator or they rely mainly on the structure of $T$ without taking the structure of $L^q(\Omega)$ into account. This leaves me with the vague feeling that one might be able to do better, i.e. that there are conditions on the convolution kernel $K$ which are less restrictive than the Hörmander condition or at least different but which still ensure that one can extend the bounded operator $T$ on $L^p(\mathbb{R}^d)$ to a bounded operator $\vec{T}$ on $L^p(\mathbb{R}^d;L^q(\Omega))$ for any $q\in (1,\infty)$. Any ideas or references would be highly appreciated. Of course the same holds true for counterexamples illustrating that one cannot do (significantly) better, if such counterexamples exist. In fact, I even wonder whether there is an example of a convolution operator which is bounded on $L^p(\mathbb{R}^d)$ for $1<p<\infty$ but does not admit a bounded extension to $L^p(\mathbb{R}^d;L^q(\Omega))$ for every $q\in(1,\infty)$.",,"['analysis', 'functional-analysis', 'harmonic-analysis']"
27,Uniformly continuous functions,Uniformly continuous functions,,"Given a uniformly continuous function $f(x)$ on the real numbers $\Bbb R$, then by the definition of uniform continuity this means: for any $\epsilon>0$ there exists $\delta >0$ such that $|f(x)-f(y)|<\epsilon$ whenever $|x-y|<\delta$. My question is: Given $\epsilon_{n} =\frac{1}{3^{n}}$ (for example), and $y_{n}=x_{n}+w_{n}$ so that $|x_{n} -y_{n}|=|w_{n}|$,  then we know that there is $\delta_{n}>0$, how can we find the corresponding $\delta_{n}$? I don't know if my question makes sense.","Given a uniformly continuous function $f(x)$ on the real numbers $\Bbb R$, then by the definition of uniform continuity this means: for any $\epsilon>0$ there exists $\delta >0$ such that $|f(x)-f(y)|<\epsilon$ whenever $|x-y|<\delta$. My question is: Given $\epsilon_{n} =\frac{1}{3^{n}}$ (for example), and $y_{n}=x_{n}+w_{n}$ so that $|x_{n} -y_{n}|=|w_{n}|$,  then we know that there is $\delta_{n}>0$, how can we find the corresponding $\delta_{n}$? I don't know if my question makes sense.",,['calculus']
28,How can I get this tricky sum of a Hadamard product of generating functions?,How can I get this tricky sum of a Hadamard product of generating functions?,,"I am interested in the sum of a Hadamard product of generating functions. If we are given $n$ functions, where $0 < i \leq n$, of the form: $$f_i(x_i) = \sum_{j=0}^m{c_{i,j} x_i^j}.$$ The hadamard product is defined as: $$g(x) = \sum_{j=0}^m{( \prod_{i=0}^n {c_{i,j}})x^j}.$$ It's essentially the same as going through all the generating functions simultaneously and multiplying all the $j$th coefficients together to obtain a new coefficient. Some given information I have polynomials; I think of them as finite length generating functions.  I know that all of the coefficients are natural numbers.  I'd like the sum of the resulting coefficients. There's a trick.  Calculating the Hadamard product generates an extremely complicated expression.  I would like to avoid dealing with this expression explicitly, if possible. I want to know ways that I could do this.","I am interested in the sum of a Hadamard product of generating functions. If we are given $n$ functions, where $0 < i \leq n$, of the form: $$f_i(x_i) = \sum_{j=0}^m{c_{i,j} x_i^j}.$$ The hadamard product is defined as: $$g(x) = \sum_{j=0}^m{( \prod_{i=0}^n {c_{i,j}})x^j}.$$ It's essentially the same as going through all the generating functions simultaneously and multiplying all the $j$th coefficients together to obtain a new coefficient. Some given information I have polynomials; I think of them as finite length generating functions.  I know that all of the coefficients are natural numbers.  I'd like the sum of the resulting coefficients. There's a trick.  Calculating the Hadamard product generates an extremely complicated expression.  I would like to avoid dealing with this expression explicitly, if possible. I want to know ways that I could do this.",,"['analysis', 'sequences-and-series', 'functions']"
29,An interesting question on function iteration,An interesting question on function iteration,,"$f$ is a continuous, strictly increasing function and $f(x+1) = f(x)+1$ We can know that $$\forall x \in \mathbb{R},\lim_{n\rightarrow + \infty} \frac{f^{n}(x)}{n}$$ exists, and its value doesn't rely on x Is this proposition right?(Probably right) And how can I prove it? I have got to know that we can't write down the exact value of limitation probably, so we may try some indirect approaches(which make this question seem thorny) Note: $f^{n}(x) =f( f^{n-1}(x))$","is a continuous, strictly increasing function and We can know that exists, and its value doesn't rely on x Is this proposition right?(Probably right) And how can I prove it? I have got to know that we can't write down the exact value of limitation probably, so we may try some indirect approaches(which make this question seem thorny) Note:","f f(x+1) = f(x)+1 \forall x \in \mathbb{R},\lim_{n\rightarrow + \infty} \frac{f^{n}(x)}{n} f^{n}(x) =f( f^{n-1}(x))",['analysis']
30,Problems in mathematical analysis exam,Problems in mathematical analysis exam,,"I want to ask some problems that are asked in exam (actually the example of exam that I'm preparing in this post it is Mathematica analysis) I want some suggestions to my answer (some are obtained by reading other posts in here that I just aggregate) I want to know if there are some suggestions and improvements and corrections. Thank you. Let $(a_n)$ be a positive real sequence such that $\lim_{n\rightarrow \infty}\frac{a_{n+2}}{a_n}=\frac{1}{2}$ . Prove that $\lim_{n\rightarrow \infty} a_n=0$ . My first sol If $\lim_{n\rightarrow \infty}\frac{a_{n+2}}{a_n}=\frac{1}{2}$ we have $$\left|\dfrac{a_{n+2}}{a_n}-\dfrac{1}{2}\right|<\varepsilon,\forall n\ge N_0, \forall \varepsilon >0.$$ Let us choose $\varepsilon =1/4$ then for $n\ge N$ we have $\dfrac{a_{n+2}}{a_n}-\dfrac{1}{2}<\dfrac{1}{4}$ , so $a_{n+2}<\dfrac{3}{4}a_n$ for all such $n$ . Since $\lim_{m\rightarrow \infty}(3/4)^m=0$ (not sure if I could use it here withput proof), and by mathematical induction we have for all $k>N$ $$a_{2k}<\left(\dfrac{3}{4}\right)^{k-N}a_{2N},$$ $$a_{2k+1}<\left(\dfrac{3}{4}\right)^{k-N}a_{2N+1}.$$ Therefore, for all $n\ge 2N+M+1$ , $a_n<\varepsilon$ as $(3/4)^{m}<\varepsilon/\max(a_{2N},a_{2N+1}), \forall \varepsilon>0, \forall m\ge M$ $\blacksquare$ My second sol By the ratio test we have both $\displaystyle \sum_{k\ge 1}a_{2k+1}$ , $\displaystyle \sum_{k\ge 1}a_{2k}$ converges. If we let $b_k=a_{2k+1}$ and $c_k=a_{2k}$ , then we have $b_k,c_k\rightarrow 0$ as $k\rightarrow \infty$ . That is, $|a_{2k+1}|<\varepsilon$ and $|a_{2k}|<\varepsilon$ for all $\varepsilon$ and $n\ge N$ , which means $|a_n|<\varepsilon$ $\blacksquare$ Let $(a_n)$ be a positive real sequence such that $\sum_{n\ge 1} na_n<\infty$ . Prove that $\sum_{m\le n}a_m $ is a Cauchy sequence. I think it can be done by comparison test that $a_n\le na_n$ $\forall n\ge 1$ , thus $\sum_{m\ge 1}a_m$ converges and so $\sum_{m\le n} a_m$ is a Cauchy sequence (not sure if this is okay since every convergent sequence is Cauchy.) This is something like if we let $s_n=\sum_{m\le n}a_m$ , we have $\lim_{n\rightarrow\infty} s_n=c,\exists c$ , and use the definiton again. Let $f:\mathbb R\rightarrow \mathbb R$ is continuous (does this mean it is continuous all over $\mathbb R?$ ) such that $f(0)>0$ . Prove that $\exists \varepsilon>0$ and $\delta>0$ such that $f(x)>\varepsilon$ whenever $x$ satisfying $|x|<\delta$ . Since $f$ is continuous at $0$ , $\forall\varepsilon,\exists \delta>0$ , $|f(x)-f(0)|<\varepsilon$ whenever $|x|<\delta$ . As $f(0)>0$ we let $f(0)=c,\exists c>0$ , thus $f(x)>c-\varepsilon$ . So we choose $\varepsilon=c/2$ so that $\exists\delta'>0$ we have $f(x)>c/2$ for any $|x|<\delta$ $\blacksquare$ Let $f:\mathbb R\rightarrow\mathbb R$ is continuous at $a\in\mathbb R$ . Let $g(x)=(x-a)f(x)$ . Determine whether $g$ is differentiable at $a$ and explain why. Since $f$ is continuous at $a$ , we have that $|f(x)-f(a)|<\varepsilon$ for any $\varepsilon>0$ and some $\delta=\delta(\varepsilon)>0$ and for any $|x-a|<\delta$ . For $h\neq 0$ , we have $$\dfrac{g(a+h)-g(a)}{h}=\dfrac{hf(a+h)-0}{h}=f(a+h).$$ For any $|h|<\delta$ , we have $|f(a+h)-f(a)|<\varepsilon$ so the limit above (as $h\to 0$ ) exists and equals $f(a)$ $\blacksquare$","I want to ask some problems that are asked in exam (actually the example of exam that I'm preparing in this post it is Mathematica analysis) I want some suggestions to my answer (some are obtained by reading other posts in here that I just aggregate) I want to know if there are some suggestions and improvements and corrections. Thank you. Let be a positive real sequence such that . Prove that . My first sol If we have Let us choose then for we have , so for all such . Since (not sure if I could use it here withput proof), and by mathematical induction we have for all Therefore, for all , as My second sol By the ratio test we have both , converges. If we let and , then we have as . That is, and for all and , which means Let be a positive real sequence such that . Prove that is a Cauchy sequence. I think it can be done by comparison test that , thus converges and so is a Cauchy sequence (not sure if this is okay since every convergent sequence is Cauchy.) This is something like if we let , we have , and use the definiton again. Let is continuous (does this mean it is continuous all over ) such that . Prove that and such that whenever satisfying . Since is continuous at , , whenever . As we let , thus . So we choose so that we have for any Let is continuous at . Let . Determine whether is differentiable at and explain why. Since is continuous at , we have that for any and some and for any . For , we have For any , we have so the limit above (as ) exists and equals","(a_n) \lim_{n\rightarrow \infty}\frac{a_{n+2}}{a_n}=\frac{1}{2} \lim_{n\rightarrow \infty} a_n=0 \lim_{n\rightarrow \infty}\frac{a_{n+2}}{a_n}=\frac{1}{2} \left|\dfrac{a_{n+2}}{a_n}-\dfrac{1}{2}\right|<\varepsilon,\forall n\ge N_0, \forall \varepsilon >0. \varepsilon =1/4 n\ge N \dfrac{a_{n+2}}{a_n}-\dfrac{1}{2}<\dfrac{1}{4} a_{n+2}<\dfrac{3}{4}a_n n \lim_{m\rightarrow \infty}(3/4)^m=0 k>N a_{2k}<\left(\dfrac{3}{4}\right)^{k-N}a_{2N}, a_{2k+1}<\left(\dfrac{3}{4}\right)^{k-N}a_{2N+1}. n\ge 2N+M+1 a_n<\varepsilon (3/4)^{m}<\varepsilon/\max(a_{2N},a_{2N+1}), \forall \varepsilon>0, \forall m\ge M \blacksquare \displaystyle \sum_{k\ge 1}a_{2k+1} \displaystyle \sum_{k\ge 1}a_{2k} b_k=a_{2k+1} c_k=a_{2k} b_k,c_k\rightarrow 0 k\rightarrow \infty |a_{2k+1}|<\varepsilon |a_{2k}|<\varepsilon \varepsilon n\ge N |a_n|<\varepsilon \blacksquare (a_n) \sum_{n\ge 1} na_n<\infty \sum_{m\le n}a_m  a_n\le na_n \forall n\ge 1 \sum_{m\ge 1}a_m \sum_{m\le n} a_m s_n=\sum_{m\le n}a_m \lim_{n\rightarrow\infty} s_n=c,\exists c f:\mathbb R\rightarrow \mathbb R \mathbb R? f(0)>0 \exists \varepsilon>0 \delta>0 f(x)>\varepsilon x |x|<\delta f 0 \forall\varepsilon,\exists \delta>0 |f(x)-f(0)|<\varepsilon |x|<\delta f(0)>0 f(0)=c,\exists c>0 f(x)>c-\varepsilon \varepsilon=c/2 \exists\delta'>0 f(x)>c/2 |x|<\delta \blacksquare f:\mathbb R\rightarrow\mathbb R a\in\mathbb R g(x)=(x-a)f(x) g a f a |f(x)-f(a)|<\varepsilon \varepsilon>0 \delta=\delta(\varepsilon)>0 |x-a|<\delta h\neq 0 \dfrac{g(a+h)-g(a)}{h}=\dfrac{hf(a+h)-0}{h}=f(a+h). |h|<\delta |f(a+h)-f(a)|<\varepsilon h\to 0 f(a) \blacksquare","['real-analysis', 'analysis', 'contest-math', 'problem-solving']"
31,Invariance of domain for smooth functions [duplicate],Invariance of domain for smooth functions [duplicate],,This question already has an answer here : Is there some elementary proof of invariance of domain? (1 answer) Closed 5 years ago . Let $f \colon U \to \mathbb R^n$ ($U \subset \mathbb R^n$ open) be of class $C^1$ and injective. Apparently there is an easy proof to show that $f(U)$ is open. In general it follows from the Invariance of domain theorem. Does someone know that proof?,This question already has an answer here : Is there some elementary proof of invariance of domain? (1 answer) Closed 5 years ago . Let $f \colon U \to \mathbb R^n$ ($U \subset \mathbb R^n$ open) be of class $C^1$ and injective. Apparently there is an easy proof to show that $f(U)$ is open. In general it follows from the Invariance of domain theorem. Does someone know that proof?,,['analysis']
32,Solving a first order non linear PDE with the method of characteristics,Solving a first order non linear PDE with the method of characteristics,,"We have to find the function $u(x,y)$ for the following system: $u_xu_y = xy$ $u(x,y) = y+1$ for $x=y$ Using the method of characterstics I get: $F(x,y,u,u_x,u_y) = u_xu_y-xy = 0$ Defining $p = u_x$ and $q = u_y$ we get: $F(x,y,u,p,q) = pq-xy = 0$ I use a parametrisation for $s = 0$: $x(t) = y(t)$ $y(t) = t$ $u(t) = t+1$ We are supposed to find p and q out of the following system: $F(x,y,u,p,q) = 0$ $u_t = px_t+qy_t$ By filling in what we know we get: $pq-t^2 = 0$ $p+q = 1$ I am not quite sure how to find p and q out of this system. The characteristic differential equations are: $x_s = F_p = q$, for $s=0 : x=t$ $y_s = F_q = p$, for $s=0 : y=t$ $u_s = pF_p+qF_q = 2pq$, for $s=0 : u=t+1$ $p_s=-F_x-pF_u = y$, for $s=0 : p= ?$ $q_s = -F_y-qF_u = x$, for $s=0 : q= ?$ Help would be much appreciated.","We have to find the function $u(x,y)$ for the following system: $u_xu_y = xy$ $u(x,y) = y+1$ for $x=y$ Using the method of characterstics I get: $F(x,y,u,u_x,u_y) = u_xu_y-xy = 0$ Defining $p = u_x$ and $q = u_y$ we get: $F(x,y,u,p,q) = pq-xy = 0$ I use a parametrisation for $s = 0$: $x(t) = y(t)$ $y(t) = t$ $u(t) = t+1$ We are supposed to find p and q out of the following system: $F(x,y,u,p,q) = 0$ $u_t = px_t+qy_t$ By filling in what we know we get: $pq-t^2 = 0$ $p+q = 1$ I am not quite sure how to find p and q out of this system. The characteristic differential equations are: $x_s = F_p = q$, for $s=0 : x=t$ $y_s = F_q = p$, for $s=0 : y=t$ $u_s = pF_p+qF_q = 2pq$, for $s=0 : u=t+1$ $p_s=-F_x-pF_u = y$, for $s=0 : p= ?$ $q_s = -F_y-qF_u = x$, for $s=0 : q= ?$ Help would be much appreciated.",,"['analysis', 'partial-differential-equations', 'characteristics']"
33,Let $g:\mathbb{R}\to\mathbb{R}$ be a measurable function such that $g(x+y) =g(x)+g(y).$ Then $g(x) = g(1)x$ . [closed],Let  be a measurable function such that  Then  . [closed],g:\mathbb{R}\to\mathbb{R} g(x+y) =g(x)+g(y). g(x) = g(1)x,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $g:\mathbb{R}\to\mathbb{R}$ be a measurable function such that    $$g(x+y) =g(x)+g(y).$$   How to prove that $g(x) = cx$ for some $c\in \mathbb{R}?$ The main thing to do here relies upon the fact that such function should be continuous and therefore by natural argument the answer will follow. Using this Additivity + Measurability $\implies$ Continuity Therefore I found out that there is nothing missing in this question.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $g:\mathbb{R}\to\mathbb{R}$ be a measurable function such that    $$g(x+y) =g(x)+g(y).$$   How to prove that $g(x) = cx$ for some $c\in \mathbb{R}?$ The main thing to do here relies upon the fact that such function should be continuous and therefore by natural argument the answer will follow. Using this Additivity + Measurability $\implies$ Continuity Therefore I found out that there is nothing missing in this question.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
34,Do we have $\int f dxdy = \int fdydx$ or $\int f dxdy = -\int f dydx$?,Do we have  or ?,\int f dxdy = \int fdydx \int f dxdy = -\int f dydx,"If $f : \mathbb R^2 \to \mathbb R$ is an integrable function, then do we have $$  \int f dxdy = \int f dydx $$ or $$  \int f dxdy = -\int f dydx? $$ (I am leaving the domain of integration as it does not matter for the question, but suppose we have some compactum $[a,b]\times[c,d]$ as this make integration theory easier). To give some context. I am currently reading about differential forms and integration, and if we have a $2$-form $\omega = f\cdot (dx \wedge dy)$, then $$  \int f dx\wedge dy = -\int f dy \wedge dx $$ and the notation $\int f dx \wedge dy$ could be understood as shorthand for $\int f dxdy$. So then the second interpretation would be true. On the other side, the ""ordinary"" integral as interpreted as the Riemann-integral where $dx$ and $dy$ are sometimes understood as the ""infinitesimal"" sides of the rectangles in which the domain of integrations gets partitioned. But then $dxdy = dydx$, or $\Delta x \cdot \Delta y = \Delta y \cdot \Delta x$. I know this language is a little bit ""hand-wavy"" as it is not clear what infinitesimal objects are, and so some authors leave them out of the integral expression and just write $\int f$, but at least this is the intuitive way and in some way resembles what is done in Riemann integration. Also, another argument in favour of the first (i.e. interchanging does not changes sign) is the Fubini theorem and multiple integrals (see wikipedia ), i.e. if Fubini applies we have $$  \int f dxdy = \int \left(\int f dx\right) dy $$ and  $$  \int \left(\int f dx \right) dy = \int \left(\int f dy\right) dx $$ which would yield $\int f dxdy = \int f dydx$ by comparing those expressions. So there are arguments to support both interpretations, but what is the right interpretation?","If $f : \mathbb R^2 \to \mathbb R$ is an integrable function, then do we have $$  \int f dxdy = \int f dydx $$ or $$  \int f dxdy = -\int f dydx? $$ (I am leaving the domain of integration as it does not matter for the question, but suppose we have some compactum $[a,b]\times[c,d]$ as this make integration theory easier). To give some context. I am currently reading about differential forms and integration, and if we have a $2$-form $\omega = f\cdot (dx \wedge dy)$, then $$  \int f dx\wedge dy = -\int f dy \wedge dx $$ and the notation $\int f dx \wedge dy$ could be understood as shorthand for $\int f dxdy$. So then the second interpretation would be true. On the other side, the ""ordinary"" integral as interpreted as the Riemann-integral where $dx$ and $dy$ are sometimes understood as the ""infinitesimal"" sides of the rectangles in which the domain of integrations gets partitioned. But then $dxdy = dydx$, or $\Delta x \cdot \Delta y = \Delta y \cdot \Delta x$. I know this language is a little bit ""hand-wavy"" as it is not clear what infinitesimal objects are, and so some authors leave them out of the integral expression and just write $\int f$, but at least this is the intuitive way and in some way resembles what is done in Riemann integration. Also, another argument in favour of the first (i.e. interchanging does not changes sign) is the Fubini theorem and multiple integrals (see wikipedia ), i.e. if Fubini applies we have $$  \int f dxdy = \int \left(\int f dx\right) dy $$ and  $$  \int \left(\int f dx \right) dy = \int \left(\int f dy\right) dx $$ which would yield $\int f dxdy = \int f dydx$ by comparing those expressions. So there are arguments to support both interpretations, but what is the right interpretation?",,"['integration', 'analysis', 'differential-geometry', 'differential-forms', 'infinitesimals']"
35,Convergence of Series Whose Terms are Defined Recursively,Convergence of Series Whose Terms are Defined Recursively,,"My recursively defined sequence $(a_n)_{n\in\mathbb{N}}$ is given trough $$a_1 = 1, \quad a_2=\frac{1}{2}\quad a_{n+2}=a_{n}a_{n+1}\quad \text{for } n\geq1$$ and I have to show that the series $$\sum_{n=1}^{\infty}a_n$$ converges. I have 4 questions to my approach(es) I did to deal with that: I noticed that $a_n$ can be represented as $$a_n = 2^{-f_{n-1}}$$ whereas $f_n$ is the $n$-th Fibonacci number. For this claim I did this proof by induction: $$ n=1: a_1 = 1 = 2^0 = 2^{f_0} = 2^{-f_0} = 2^{-f_{1-1}} \\  n=2: a_2 = \frac{1}{2} = 2^{-1} = 2^{-f_1} =  2^{-f_{2-1}}$$ and by multiplying the premises $A(n)$ and $A(n+1)$ in order to get $A(n+2)$ $$""n+1\to n+2"":\quad a_n \cdot a_{n+1}  = 2^{-f_{n-1}} \cdot 2^{-f_n} \quad\Longleftrightarrow\\  a_{n+2} = a_n \cdot a_{n+1} = 2^{-f_{n-1}} \cdot 2^{-f_n} = 2^{-(f_n + f_{n-1})} = 2^{-f_{n+1}}.$$ ...is that valid? Given that $a_n = 2^{-f_{n-1}}$ is true I proceeded with the ratio test $$q := \lim_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right| = \lim_{n\to\infty} \left|\frac{2^{-f_{n}}}{2^{-f_{n-1}}}\right| = \lim_{n\to\infty} \frac{2^{f_{n-1}}}{2^{f_{n}}} =\\ 	= \lim_{n\to\infty} \frac{1}{2^{f_{n} - f_{n-1}}} = \lim_{n\to\infty}\frac{1}{2^{(f_{n-2} + f_{n-1}) - f_{n-1}}} = \lim_{n\to\infty}\frac{1}{2^{f_{n-2}}} < 1.$$ Is that sufficient to show the convergence? Follow-up question: given the $a_n = 2^{-f_{n-1}}$ term - is there a better way than this ratio test to show convergence of $\sum a_n$ now that I have this representation? Another approach would be to find a sequence $(b_n)_{n\in\mathbb{N}}$ with $$\sum_{n=1}^{\infty}b_n < \infty \quad\wedge\quad |b_n| > |a_n|\quad\forall n\geq1.$$ For what it's worth I struggle to find such a series - am I missing something? Is there something immediate that should come to mind? $a_n$ and $b_n$ have to be zero sequences, how can I do the estimate in its recursively defined form? Another ""please correct me if I'm wrong""-question: I did an approach where I proved that $a_{n+1}< a_{n}$ for all $n\geq1$ (which is similar to question 1: $$n=1:\quad a_1 = 1 \geq \frac{1}{2} = a_2\\ n=2: a_3 = a_1a_2 = 1 \cdot \frac{1}{2} = \frac{1}{2},\quad\therefore a_2 = \frac{1}{2} \geq \frac{1}{2} = a_3$$ and $$""n+1 \to n+2"":\quad a_n \cdot a_{n+1} \geq a_{n+1} \cdot a_{n+2} \Longleftrightarrow\\ a_{n+2} = a_n \cdot a_{n+1} \geq a_{n+1} \cdot a_{n+2} =  a_{(n+1)+2} = a_{n+3}$$ respectively) and after that I did the ratio test $$q := \lim_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right| = \lim_{n\to\infty} \left|\frac{a_{n+2}}{a_{n+1}}\right| = \lim_{n\to\infty} \left|\frac{a_n a_{n+1}}{a_{n+1}}\right| = \lim_{n\to\infty}\left|a_{n}\right| \stackrel{*}{<} 1.$$ (with (*) denoting that $a_{n+1}<a_{n}$ and $a_2 <1$). Is that a sufficient answer to my problem? ...thank you so much, guys!","My recursively defined sequence $(a_n)_{n\in\mathbb{N}}$ is given trough $$a_1 = 1, \quad a_2=\frac{1}{2}\quad a_{n+2}=a_{n}a_{n+1}\quad \text{for } n\geq1$$ and I have to show that the series $$\sum_{n=1}^{\infty}a_n$$ converges. I have 4 questions to my approach(es) I did to deal with that: I noticed that $a_n$ can be represented as $$a_n = 2^{-f_{n-1}}$$ whereas $f_n$ is the $n$-th Fibonacci number. For this claim I did this proof by induction: $$ n=1: a_1 = 1 = 2^0 = 2^{f_0} = 2^{-f_0} = 2^{-f_{1-1}} \\  n=2: a_2 = \frac{1}{2} = 2^{-1} = 2^{-f_1} =  2^{-f_{2-1}}$$ and by multiplying the premises $A(n)$ and $A(n+1)$ in order to get $A(n+2)$ $$""n+1\to n+2"":\quad a_n \cdot a_{n+1}  = 2^{-f_{n-1}} \cdot 2^{-f_n} \quad\Longleftrightarrow\\  a_{n+2} = a_n \cdot a_{n+1} = 2^{-f_{n-1}} \cdot 2^{-f_n} = 2^{-(f_n + f_{n-1})} = 2^{-f_{n+1}}.$$ ...is that valid? Given that $a_n = 2^{-f_{n-1}}$ is true I proceeded with the ratio test $$q := \lim_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right| = \lim_{n\to\infty} \left|\frac{2^{-f_{n}}}{2^{-f_{n-1}}}\right| = \lim_{n\to\infty} \frac{2^{f_{n-1}}}{2^{f_{n}}} =\\ 	= \lim_{n\to\infty} \frac{1}{2^{f_{n} - f_{n-1}}} = \lim_{n\to\infty}\frac{1}{2^{(f_{n-2} + f_{n-1}) - f_{n-1}}} = \lim_{n\to\infty}\frac{1}{2^{f_{n-2}}} < 1.$$ Is that sufficient to show the convergence? Follow-up question: given the $a_n = 2^{-f_{n-1}}$ term - is there a better way than this ratio test to show convergence of $\sum a_n$ now that I have this representation? Another approach would be to find a sequence $(b_n)_{n\in\mathbb{N}}$ with $$\sum_{n=1}^{\infty}b_n < \infty \quad\wedge\quad |b_n| > |a_n|\quad\forall n\geq1.$$ For what it's worth I struggle to find such a series - am I missing something? Is there something immediate that should come to mind? $a_n$ and $b_n$ have to be zero sequences, how can I do the estimate in its recursively defined form? Another ""please correct me if I'm wrong""-question: I did an approach where I proved that $a_{n+1}< a_{n}$ for all $n\geq1$ (which is similar to question 1: $$n=1:\quad a_1 = 1 \geq \frac{1}{2} = a_2\\ n=2: a_3 = a_1a_2 = 1 \cdot \frac{1}{2} = \frac{1}{2},\quad\therefore a_2 = \frac{1}{2} \geq \frac{1}{2} = a_3$$ and $$""n+1 \to n+2"":\quad a_n \cdot a_{n+1} \geq a_{n+1} \cdot a_{n+2} \Longleftrightarrow\\ a_{n+2} = a_n \cdot a_{n+1} \geq a_{n+1} \cdot a_{n+2} =  a_{(n+1)+2} = a_{n+3}$$ respectively) and after that I did the ratio test $$q := \lim_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right| = \lim_{n\to\infty} \left|\frac{a_{n+2}}{a_{n+1}}\right| = \lim_{n\to\infty} \left|\frac{a_n a_{n+1}}{a_{n+1}}\right| = \lim_{n\to\infty}\left|a_{n}\right| \stackrel{*}{<} 1.$$ (with (*) denoting that $a_{n+1}<a_{n}$ and $a_2 <1$). Is that a sufficient answer to my problem? ...thank you so much, guys!",,"['sequences-and-series', 'analysis', 'convergence-divergence', 'fibonacci-numbers']"
36,What should I learn next for pure math?,What should I learn next for pure math?,,"I can do basic single variable calculus which is essentially all you do at A Level in the UK. I also just read ""What is mathematics?"" by Richard Courant which I found very good. I would like to know where to go after I finish my proof writing and basic discrete maths books I am currently reading. Should I study real analysis? What book? Linear algebra? I would appreciate advice on where to go next and what book would be good. I am currently 14 and my end goal would to become a pure mathematician. The reason I ask this question is because I was trying to create a structure and I thought about the Gerard t'Hooft physics one or even the Pure mathematician or statiscian plan but they don't seem particularly specific in what you are meant to do where i.e they call a topic calculus and then a later one vector calculus and then analysis but as a pure mathematician I want to go straight into analysis, of reals obviously. In conclusion, I am asking this because I am going to go by steps now and choose the next topic as it comes. Thank you in advance. EDIT: Would this work: In order... Elementary Discrete Maths, Real Analysis, Linear Algebra, ODE's, Probability, Fourier Analysis, Complex Analysis, PDE's, Graduate stuff which I will get to when needed. I'm not sure if probability is necessary but I think it'll be interesting.","I can do basic single variable calculus which is essentially all you do at A Level in the UK. I also just read ""What is mathematics?"" by Richard Courant which I found very good. I would like to know where to go after I finish my proof writing and basic discrete maths books I am currently reading. Should I study real analysis? What book? Linear algebra? I would appreciate advice on where to go next and what book would be good. I am currently 14 and my end goal would to become a pure mathematician. The reason I ask this question is because I was trying to create a structure and I thought about the Gerard t'Hooft physics one or even the Pure mathematician or statiscian plan but they don't seem particularly specific in what you are meant to do where i.e they call a topic calculus and then a later one vector calculus and then analysis but as a pure mathematician I want to go straight into analysis, of reals obviously. In conclusion, I am asking this because I am going to go by steps now and choose the next topic as it comes. Thank you in advance. EDIT: Would this work: In order... Elementary Discrete Maths, Real Analysis, Linear Algebra, ODE's, Probability, Fourier Analysis, Complex Analysis, PDE's, Graduate stuff which I will get to when needed. I'm not sure if probability is necessary but I think it'll be interesting.",,"['analysis', 'soft-question', 'self-learning', 'advice']"
37,Integration by Parts and Leibniz Rule for Differentiation under the Integral Sign,Integration by Parts and Leibniz Rule for Differentiation under the Integral Sign,,"Basically a friend of mine and I have had this hot debate for a little too long, I contend that these two tools are not only logically unconnected but they require different assumptions (I believe one requires a continuously differentiable function and another requires it to simply be continuous). We've even gone through the proofs and disagree on how the assumptions are used. I don't see the connection...  Maybe I'm wrong, maybe they are equivalent (you have one as a tool if and only if you have the other). Anyway, any fresh perspective would be welcomed and any deeper discussion on either appreciated, thanks.","Basically a friend of mine and I have had this hot debate for a little too long, I contend that these two tools are not only logically unconnected but they require different assumptions (I believe one requires a continuously differentiable function and another requires it to simply be continuous). We've even gone through the proofs and disagree on how the assumptions are used. I don't see the connection...  Maybe I'm wrong, maybe they are equivalent (you have one as a tool if and only if you have the other). Anyway, any fresh perspective would be welcomed and any deeper discussion on either appreciated, thanks.",,"['calculus', 'analysis', 'integration', 'derivatives']"
38,"Is there a function which has limit only given $a_1,a_2...a_n....$ infinite points.",Is there a function which has limit only given  infinite points.,"a_1,a_2...a_n....","Find a function which has limit only in advanced marked points $(a_1,a_2,....a_n)$ . Here is my example. $g(x) = \begin{cases} 1,  & \text{if}~x\in\mathbb{Q} \\ 0, & \text{if}~x\in\Bbb{R}\backslash \Bbb{Q} \end{cases}$ $f(x) = (x-a_1)(x-a_2)...(x-a_n)g(x)$ But the problem I can't solve is that,is there a function which has limit only in advanced  given $a_1,a_2...a_n....$ infinite points.","Find a function which has limit only in advanced marked points . Here is my example. But the problem I can't solve is that,is there a function which has limit only in advanced  given infinite points.","(a_1,a_2,....a_n) g(x) =
\begin{cases}
1,  & \text{if}~x\in\mathbb{Q} \\
0, & \text{if}~x\in\Bbb{R}\backslash \Bbb{Q}
\end{cases} f(x) = (x-a_1)(x-a_2)...(x-a_n)g(x) a_1,a_2...a_n....","['real-analysis', 'calculus', 'analysis']"
39,How can I show $f*g = g*f$ for Lebesgue integral?,How can I show  for Lebesgue integral?,f*g = g*f,"I want to show that $(f*g)(x) = (g*f)(x)$ for almost every $x\in\mathbb R^n$ in  Lebesgue integration case, where  * is convolution on $\mathbb R^n$, that is $$ (f*g) (x)=\int_{\mathbb R^n} f(x-y)g(y) dy $$","I want to show that $(f*g)(x) = (g*f)(x)$ for almost every $x\in\mathbb R^n$ in  Lebesgue integration case, where  * is convolution on $\mathbb R^n$, that is $$ (f*g) (x)=\int_{\mathbb R^n} f(x-y)g(y) dy $$",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral']"
40,Does $\lim\limits_{x\rightarrow \infty}f'(x)$ exist?,Does  exist?,\lim\limits_{x\rightarrow \infty}f'(x),Let $f: \Bbb R \rightarrow \Bbb R$ be differentiable with $f'$ uniformly continuous. Suppose $\lim\limits_{x\rightarrow \infty}f(x)=L$ for some $L$. Does $\lim\limits_{x\rightarrow \infty}f'(x)$ exist? I have no idea about this problem. Could you please give a hint? Thank you very much for your help.,Let $f: \Bbb R \rightarrow \Bbb R$ be differentiable with $f'$ uniformly continuous. Suppose $\lim\limits_{x\rightarrow \infty}f(x)=L$ for some $L$. Does $\lim\limits_{x\rightarrow \infty}f'(x)$ exist? I have no idea about this problem. Could you please give a hint? Thank you very much for your help.,,"['calculus', 'real-analysis', 'analysis', 'limits']"
41,Analysis question on Integration bounds,Analysis question on Integration bounds,,I have to find all functions f(t) such that $\int_x^{x^2} f(t) d t=\int_1^x f(t) dt$ I think the solutions are all the functions of the form a/(x+b) because the logarithm would divide the two powers and ln(1) is 0 so that would go away.. Not sure if there are any others though..,I have to find all functions f(t) such that $\int_x^{x^2} f(t) d t=\int_1^x f(t) dt$ I think the solutions are all the functions of the form a/(x+b) because the logarithm would divide the two powers and ln(1) is 0 so that would go away.. Not sure if there are any others though..,,"['integration', 'analysis']"
42,Evaluate : $\int_0^{\frac{\pi}{2}} \frac{\ln(\sec ^2 x)\sec^2x }{(\sec ^2 x+1)\tan x}\mathrm dx$,Evaluate :,\int_0^{\frac{\pi}{2}} \frac{\ln(\sec ^2 x)\sec^2x }{(\sec ^2 x+1)\tan x}\mathrm dx,"I need to evaluate : $$I=\int_0^{\frac{\pi}{2}} \frac{\ln(\sec ^2 x)\sec^2x \mathrm dx}{(\sec ^2 x+1)\tan x}$$ By substituting $x\to \frac{\pi}{2} - x$ , I get : $$I =\int_0^{\frac{\pi}{2}} \frac{\ln(\csc^2 x)\csc^2x \mathrm dx}{(\csc ^2 x+1)\cot x}$$ But I do not know how to proceed with this. I also tried the substitution $\tan x = u$ which transform $I$ to : $$I = \int_0^\infty \frac{\ln(u^2 +1)\mathrm du}{u^3+ 2u}$$ But still I cannot proceed further from here because I have never solved such type of integrals with the limited techniques I know.","I need to evaluate : By substituting , I get : But I do not know how to proceed with this. I also tried the substitution which transform to : But still I cannot proceed further from here because I have never solved such type of integrals with the limited techniques I know.",I=\int_0^{\frac{\pi}{2}} \frac{\ln(\sec ^2 x)\sec^2x \mathrm dx}{(\sec ^2 x+1)\tan x} x\to \frac{\pi}{2} - x I =\int_0^{\frac{\pi}{2}} \frac{\ln(\csc^2 x)\csc^2x \mathrm dx}{(\csc ^2 x+1)\cot x} \tan x = u I I = \int_0^\infty \frac{\ln(u^2 +1)\mathrm du}{u^3+ 2u},"['real-analysis', 'integration', 'analysis', 'definite-integrals', 'improper-integrals']"
43,"If $a_1,\ldots,a_n>0$ and $a_1+\cdots+a_n<\frac{1}{2}$, then $(1+a_1)\cdots(1+a_n)<2$.","If  and , then .","a_1,\ldots,a_n>0 a_1+\cdots+a_n<\frac{1}{2} (1+a_1)\cdots(1+a_n)<2","Assume that $a_1,a_2,\ldots,a_n>0$ and $a_1+a_2+\cdots+a_n<\frac{1}{2}$, and prove that $$(1+a_1)(1+a_2)\cdots(1+a_n)<2$$ I've tried Hölder's inequality (the same result can easily be derived using AM-GM). I've found out that it's sufficient to prove that  $$ \left(\frac{2n+1}{2n}\right)^n<2. $$ (I've created this sign for myself to use informally while searching for a proof. Proving that one of the signs holds will prove my inequality). Does anyone see how one could prove this (if it holds, of course)? However, there must be a way to prove the inequality by using induction at first rather than the Holder's inequality or AM-GM. Thanks. And I'm sorry. But I'd forgotten to add that neither logs nor calculus can be used.","Assume that $a_1,a_2,\ldots,a_n>0$ and $a_1+a_2+\cdots+a_n<\frac{1}{2}$, and prove that $$(1+a_1)(1+a_2)\cdots(1+a_n)<2$$ I've tried Hölder's inequality (the same result can easily be derived using AM-GM). I've found out that it's sufficient to prove that  $$ \left(\frac{2n+1}{2n}\right)^n<2. $$ (I've created this sign for myself to use informally while searching for a proof. Proving that one of the signs holds will prove my inequality). Does anyone see how one could prove this (if it holds, of course)? However, there must be a way to prove the inequality by using induction at first rather than the Holder's inequality or AM-GM. Thanks. And I'm sorry. But I'd forgotten to add that neither logs nor calculus can be used.",,"['calculus', 'analysis', 'inequality', 'induction', 'exponential-function']"
44,Show that a certain set of positive real numbers must be finite or countable,Show that a certain set of positive real numbers must be finite or countable,,"Let $B$ be a set of positive real numbers with the property that   adding together any finite subset of elements from $B$ always gives a sum of $2$ or less. Show that $B$ must be finite or at most countable. $B$ = {$x \in R:x>0\}$, $x_1,x_2...x_n \in B$ such that $x_1+x_2+...+x_n \le 2$. Question: for any $a,b$ $(a,b)$~$R$, but $B$ is $(0,+\infty)$ so why $B$ is not uncountable (taking as $a = 0$, and letting $b$->$\infty$)? And why for $B$ being countable doesn't contradict: for any $a,b$ $(a,b)$~$R$? P.S. I read Showing a set is finite or countable and understood it.","Let $B$ be a set of positive real numbers with the property that   adding together any finite subset of elements from $B$ always gives a sum of $2$ or less. Show that $B$ must be finite or at most countable. $B$ = {$x \in R:x>0\}$, $x_1,x_2...x_n \in B$ such that $x_1+x_2+...+x_n \le 2$. Question: for any $a,b$ $(a,b)$~$R$, but $B$ is $(0,+\infty)$ so why $B$ is not uncountable (taking as $a = 0$, and letting $b$->$\infty$)? And why for $B$ being countable doesn't contradict: for any $a,b$ $(a,b)$~$R$? P.S. I read Showing a set is finite or countable and understood it.",,"['real-analysis', 'analysis']"
45,Radius of Convergence of $\sum\ z^{n!}$ [duplicate],Radius of Convergence of  [duplicate],\sum\ z^{n!},"This question already has answers here : What is the radius of convergence of $\sum z^{n!}$? (2 answers) Closed 3 years ago . Does anyone know how to find the radius of convergence of the series $\sum\ z^{n!}$ , where $z$ is a complex number? I tried to use the definition: $$\frac{1}{R}=\limsup\left|\frac{a_{n+1}}{a_{n}}\right|$$ but I wasn't successful.","This question already has answers here : What is the radius of convergence of $\sum z^{n!}$? (2 answers) Closed 3 years ago . Does anyone know how to find the radius of convergence of the series , where is a complex number? I tried to use the definition: but I wasn't successful.",\sum\ z^{n!} z \frac{1}{R}=\limsup\left|\frac{a_{n+1}}{a_{n}}\right|,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'analysis', 'power-series']"
46,Continuous bijection from non-compact set to compact set,Continuous bijection from non-compact set to compact set,,"So, I was wondering if one could find a continuous bijection from a non-compact set to a compact set. I had an exam today where one of the questions asked me to find a continuous bijection (if there is one) from $\mathbb{R}$ to $\left[0,1\right]$ which obviously does not exist because $f$ has to be monotonic. That generalization sort of popped up in my head, and at some point I thought I had a proof (I've tried to prove that for every compact $X$ in the codomain, $f^{-1}(X)$ is compact), which proved to be wrong. So the question is, is that fact even true, at least  maybe by adding some more conditions?","So, I was wondering if one could find a continuous bijection from a non-compact set to a compact set. I had an exam today where one of the questions asked me to find a continuous bijection (if there is one) from to which obviously does not exist because has to be monotonic. That generalization sort of popped up in my head, and at some point I thought I had a proof (I've tried to prove that for every compact in the codomain, is compact), which proved to be wrong. So the question is, is that fact even true, at least  maybe by adding some more conditions?","\mathbb{R} \left[0,1\right] f X f^{-1}(X)","['general-topology', 'analysis']"
47,Translation from Dutch [closed],Translation from Dutch [closed],,"Closed. This question is off-topic . It is not currently accepting answers. Closed 5 years ago . This question is not about mathematics, within the scope defined in the help center . Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Improve this question Google Translator cannot help me with this translation. May you translate it?","Closed. This question is off-topic . It is not currently accepting answers. Closed 5 years ago . This question is not about mathematics, within the scope defined in the help center . Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Improve this question Google Translator cannot help me with this translation. May you translate it?",,"['analysis', 'translation-request']"
48,Prove that $( 1 + n^{-2}) ^n \to 1$.,Prove that .,( 1 + n^{-2}) ^n \to 1,"I need to prove that $\left(1 + \frac 1 {n^2} \right)^n \to 1$. I tried to use Bernoulli's inequality, but that is not very useful since in the original sequence there is a plus sign. I then tried to use the Sandwich Theorem by finding two sequences which would make bounds for the original one. The lower bound is obvious, the upper bound not so much. I tried using the sequence $\left( \frac 1 {n+1} \right) ^{\frac 1 n}$, but I could not show that this sequence is bigger than the original one for all $n$. Could anyone help me with this?","I need to prove that $\left(1 + \frac 1 {n^2} \right)^n \to 1$. I tried to use Bernoulli's inequality, but that is not very useful since in the original sequence there is a plus sign. I then tried to use the Sandwich Theorem by finding two sequences which would make bounds for the original one. The lower bound is obvious, the upper bound not so much. I tried using the sequence $\left( \frac 1 {n+1} \right) ^{\frac 1 n}$, but I could not show that this sequence is bigger than the original one for all $n$. Could anyone help me with this?",,"['sequences-and-series', 'analysis', 'limits', 'convergence-divergence', 'exponential-function']"
49,"Prove that if $n$ is an integer, then $n^2 + n^3$ is an even number","Prove that if  is an integer, then  is an even number",n n^2 + n^3,"I am trying to work through some of the problems in Stephen Lay's Introduction to Analysis with Proof before my Real Analysis class in the fall term starts, and I was just wondering if I could get some feedback as to whether or not I have completed this proof properly The question: Prove that if $n$ is an integer, then $n^2 + n^3$ is an even number","I am trying to work through some of the problems in Stephen Lay's Introduction to Analysis with Proof before my Real Analysis class in the fall term starts, and I was just wondering if I could get some feedback as to whether or not I have completed this proof properly The question: Prove that if $n$ is an integer, then $n^2 + n^3$ is an even number",,['analysis']
50,Proof: Every convergent sequence of real numbers is bounded,Proof: Every convergent sequence of real numbers is bounded,,"(Taken from an old exam - no homework:) Prove that every convergent sequence of real numbers is bounded. It would be cruel if our professor would take a task like that in the exam because I rather prefer calculating than doing theory (in an exam where you haven't got much time to describe things..). But I'll be honest and say I dislike theory in general! :-) Anyway, for the task, I really got no idea how to proof that but I'll give it a try... We know that if a sequence is convergent it has exactly ONE limit where the sequences (sry my English here) accumulate. So the sequence will have an upper bound or lower bound, based on the limit, thus it will be surely bounded if it's convergent sequence. Even if my proof is wrong (most likely..) I'd like to see a proof like mine, where you avoid using (complicated) maths symbols. Just using words any human can understand would be awesome! :-) Of course a more formal proof is welcome too but not necessarily preferred ^.^","(Taken from an old exam - no homework:) Prove that every convergent sequence of real numbers is bounded. It would be cruel if our professor would take a task like that in the exam because I rather prefer calculating than doing theory (in an exam where you haven't got much time to describe things..). But I'll be honest and say I dislike theory in general! :-) Anyway, for the task, I really got no idea how to proof that but I'll give it a try... We know that if a sequence is convergent it has exactly ONE limit where the sequences (sry my English here) accumulate. So the sequence will have an upper bound or lower bound, based on the limit, thus it will be surely bounded if it's convergent sequence. Even if my proof is wrong (most likely..) I'd like to see a proof like mine, where you avoid using (complicated) maths symbols. Just using words any human can understand would be awesome! :-) Of course a more formal proof is welcome too but not necessarily preferred ^.^",,"['calculus', 'sequences-and-series', 'analysis', 'convergence-divergence', 'proof-explanation']"
51,"Show set of all differentiable functions $f\colon [0,1]\to\mathbb R$ is uncountable.",Show set of all differentiable functions  is uncountable.,"f\colon [0,1]\to\mathbb R","Show set of all differentiable functions $f\colon [0,1]\to\mathbb R$ is uncountable. I think we have to use the Schröder-Bernstein theorem which states that if $|A| \leq |B|$, and $|A| \geq |B|$ then $|A|=|B|$. Need some help.","Show set of all differentiable functions $f\colon [0,1]\to\mathbb R$ is uncountable. I think we have to use the Schröder-Bernstein theorem which states that if $|A| \leq |B|$, and $|A| \geq |B|$ then $|A|=|B|$. Need some help.",,"['real-analysis', 'analysis']"
52,Prove that the following is a constant function,Prove that the following is a constant function,,"Let $f : R \rightarrow R $ $\lvert f(x)-f(y)  \rvert \le (x-y)^2, \forall x,y \in R $ Any sort of help is appreciated! I know I am not suppose to ask for the entire solution, so I will ask for strong hints.","Let $f : R \rightarrow R $ $\lvert f(x)-f(y)  \rvert \le (x-y)^2, \forall x,y \in R $ Any sort of help is appreciated! I know I am not suppose to ask for the entire solution, so I will ask for strong hints.",,['analysis']
53,Expanding $\frac{1}{1-z-z^2}$ to a power series.,Expanding  to a power series.,\frac{1}{1-z-z^2},"How would you expand the analytic function $$\frac{1}{1-z-z^2}$$ to a series of the form $$\sum_{k=0}^\infty a_k z^k \, \, ?$$","How would you expand the analytic function $$\frac{1}{1-z-z^2}$$ to a series of the form $$\sum_{k=0}^\infty a_k z^k \, \, ?$$",,"['combinatorics', 'complex-analysis', 'analysis', 'power-series', 'generating-functions']"
54,Find the sum $\sum_{k=2}^n \frac{n!}{(n-k)!(k-2)!}.$,Find the sum,\sum_{k=2}^n \frac{n!}{(n-k)!(k-2)!}.,"Find the following sum $$\sum_{k=2}^n \frac{n!}{(n-k)!(k-2)!}.$$I found that ,  $$\sum_{k=2}^n \frac{n!}{(n-k)!(k-2)!}=n!\left[\frac{1}{(n-2)!0!}+\frac{1}{(n-3)!1!}+\cdots +\frac{1}{0!(n-2)!}\right]$$From here how I proceed ?","Find the following sum $$\sum_{k=2}^n \frac{n!}{(n-k)!(k-2)!}.$$I found that ,  $$\sum_{k=2}^n \frac{n!}{(n-k)!(k-2)!}=n!\left[\frac{1}{(n-2)!0!}+\frac{1}{(n-3)!1!}+\cdots +\frac{1}{0!(n-2)!}\right]$$From here how I proceed ?",,['real-analysis']
55,Riemann Zeta formula,Riemann Zeta formula,,"can anyone check if this formula is plausible ?? $$ \frac{1}{\zeta (s)} = \sum_{n=0}^{\infty}\frac{ (-\pi)^{n}(s-1)s}{2n!(s+2n)(s+2n+1)} $$ according to the authors this formula would be valid only $ 0 < \Re(s) <1/2 $ the paper may be found at http://arxiv.org/pdf/0709.1389.pdf The authors claim ""Our representation of $\zeta^{−1}(s)$ for $\Re(s) ∈ (0, 1/2)$ - which seems to an anonymous referee "" much too simple "" to be true is analogical to the following - rather simple - series representations of $\zeta(s)$ $$\zeta(s) = \frac{1}{1-2^{1-s}}\sum_{n=1}^\infty\frac{(-1)^{n-1}}{n^s} \text{ for } \Re(s) >0, s\neq 1$$","can anyone check if this formula is plausible ?? according to the authors this formula would be valid only the paper may be found at http://arxiv.org/pdf/0709.1389.pdf The authors claim ""Our representation of for - which seems to an anonymous referee "" much too simple "" to be true is analogical to the following - rather simple - series representations of"," \frac{1}{\zeta (s)} = \sum_{n=0}^{\infty}\frac{ (-\pi)^{n}(s-1)s}{2n!(s+2n)(s+2n+1)}   0 < \Re(s) <1/2  \zeta^{−1}(s) \Re(s) ∈ (0, 1/2) \zeta(s) \zeta(s) = \frac{1}{1-2^{1-s}}\sum_{n=1}^\infty\frac{(-1)^{n-1}}{n^s} \text{ for } \Re(s) >0, s\neq 1","['analysis', 'riemann-zeta', 'zeta-functions']"
56,Union of Uncountably Infinite Sets,Union of Uncountably Infinite Sets,,"How does one notationally describe the set which is the union of uncountably many other sets.  For instance, for each x such that a < x < b, where a and b are real numbers, if there is assigned a set $N_x$,  how does one describe the union of all $N_x$ for all real x, a < x < b?","How does one notationally describe the set which is the union of uncountably many other sets.  For instance, for each x such that a < x < b, where a and b are real numbers, if there is assigned a set $N_x$,  how does one describe the union of all $N_x$ for all real x, a < x < b?",,"['analysis', 'set-theory', 'notation']"
57,A convergent sequence $\{a_n\}$ and divergent sequence $\{b_n\}$ such that $\{a_n+b_n\}$ is convergent,A convergent sequence  and divergent sequence  such that  is convergent,\{a_n\} \{b_n\} \{a_n+b_n\},"Give an example of a convergent sequence $\{a_n\}$ and divergent sequence $\{b_n\}$ such that $\{a_n+b_n\}$ is a convergent series. I've been trying to solve this question for a couple days now and have been struggling, if anyone could give me a hint or show me how you got your answer as I feel this isn't solvable but the question says that I must have an example. Thank you in advance, Math Student :)","Give an example of a convergent sequence $\{a_n\}$ and divergent sequence $\{b_n\}$ such that $\{a_n+b_n\}$ is a convergent series. I've been trying to solve this question for a couple days now and have been struggling, if anyone could give me a hint or show me how you got your answer as I feel this isn't solvable but the question says that I must have an example. Thank you in advance, Math Student :)",,"['real-analysis', 'sequences-and-series', 'analysis']"
58,Prove that the limit of a real sequence (if it exists) can't be a complex number,Prove that the limit of a real sequence (if it exists) can't be a complex number,,"I was wondering whether a sequence, all of whose terms are real numbers can converge (if convergence is at all possible) to a complex number, or more generally to any other number field. Actually I think that answer to this this question should be answered at the outset before trying to prove any of the limit theorems, but unfortunately neither our analysis class professor and neither the tutor cared to prove this. I have tried to prove this but each time I find that my argument becomes circular. Is there any way to prove this?","I was wondering whether a sequence, all of whose terms are real numbers can converge (if convergence is at all possible) to a complex number, or more generally to any other number field. Actually I think that answer to this this question should be answered at the outset before trying to prove any of the limit theorems, but unfortunately neither our analysis class professor and neither the tutor cared to prove this. I have tried to prove this but each time I find that my argument becomes circular. Is there any way to prove this?",,[]
59,Prove that the set of all algebraic numbers is countable.,Prove that the set of all algebraic numbers is countable.,,"I'm a student in Korea. If I make a mistake in grammar, please indicate. Recently, I'm studying the book 'Principles of Mathematical Analysis' So, I tried to solve the exercise #2 in chapter 2. 'A complex number $z$ is said to be algebraic if there are integer $a_0, \dots, a_n$ ,not all zero, such that  $a_0 z^n + a_1 z^{n-1} + \dots + a_n =0$ ' The hint is 'For every positive integer $N$ there are only finitely many equation with $n+|a_0|+...+|a_n|=N$' Of course, I have searched this exercise on this site. But there were different methods. I want to prove this exercise by using the hint. please help me.","I'm a student in Korea. If I make a mistake in grammar, please indicate. Recently, I'm studying the book 'Principles of Mathematical Analysis' So, I tried to solve the exercise #2 in chapter 2. 'A complex number $z$ is said to be algebraic if there are integer $a_0, \dots, a_n$ ,not all zero, such that  $a_0 z^n + a_1 z^{n-1} + \dots + a_n =0$ ' The hint is 'For every positive integer $N$ there are only finitely many equation with $n+|a_0|+...+|a_n|=N$' Of course, I have searched this exercise on this site. But there were different methods. I want to prove this exercise by using the hint. please help me.",,['analysis']
60,compact set always contains its supremum and infimum,compact set always contains its supremum and infimum,,"Let $K$ be a compact subset of $\mathbb R$. Prove that $\sup K$ and $\inf K$ exist and are in $K$. My approach: As $K$ is compact, it is bounded. So $\sup K$ and $\inf K$ exists. The reason is that: Since $K$ is compact, there exist $k_1, \cdots , k_n \in \mathbb R$ such that $$K \subset \bigcup_{j=1}^n  (-k_j,k_j)$$ If $N = \max\{k_1,\cdots, k_n\}$, then $K$ is a subset of $(-N,N)$. Hence $K$ is bounded.  since $K$ is bounded by $-N$ and $N$, $\sup K$ and $\inf K$ exists. Is this good enough? Is boundedness guaranteed the existence of supremum and infimum?","Let $K$ be a compact subset of $\mathbb R$. Prove that $\sup K$ and $\inf K$ exist and are in $K$. My approach: As $K$ is compact, it is bounded. So $\sup K$ and $\inf K$ exists. The reason is that: Since $K$ is compact, there exist $k_1, \cdots , k_n \in \mathbb R$ such that $$K \subset \bigcup_{j=1}^n  (-k_j,k_j)$$ If $N = \max\{k_1,\cdots, k_n\}$, then $K$ is a subset of $(-N,N)$. Hence $K$ is bounded.  since $K$ is bounded by $-N$ and $N$, $\sup K$ and $\inf K$ exists. Is this good enough? Is boundedness guaranteed the existence of supremum and infimum?",,"['real-analysis', 'analysis', 'compactness', 'supremum-and-infimum']"
61,"Proving it doesn't exist a homeomorphism between $\mathbb R$ and $\mathbb R^n$, $n>1$.","Proving it doesn't exist a homeomorphism between  and , .",\mathbb R \mathbb R^n n>1,"I have to prove that for $n \ge 2$, there doesn't exist a homeomorphism between $\mathbb R$ and $\mathbb R^n$. Could anyone give me a hint on how could I prove this?","I have to prove that for $n \ge 2$, there doesn't exist a homeomorphism between $\mathbb R$ and $\mathbb R^n$. Could anyone give me a hint on how could I prove this?",,"['analysis', 'metric-spaces']"
62,Use Cauchy product to find a power series represenitation of $1 \over {(1-x)^3}$,Use Cauchy product to find a power series represenitation of,1 \over {(1-x)^3},"Use Cauchy product to find a power series represenitation of   $$1 \over {(1-x)^3}$$ which is valid in the interval $(-1,1)$. Is it right to use the product of $1 \over {1-x}$ and $1 \over {(1-x)^2}$ if I know thier expantion (HOW).. Is there another way?","Use Cauchy product to find a power series represenitation of   $$1 \over {(1-x)^3}$$ which is valid in the interval $(-1,1)$. Is it right to use the product of $1 \over {1-x}$ and $1 \over {(1-x)^2}$ if I know thier expantion (HOW).. Is there another way?",,"['sequences-and-series', 'analysis', 'power-series']"
63,Derivative of a sequence,Derivative of a sequence,,"Just a quick question, can you derive sequences like any normal funtion ? For example, if $a_n=\frac{1}{n}$ then $\frac{d}{dn}(a_n)=-\frac{1}{n^2}$, is this ok ?","Just a quick question, can you derive sequences like any normal funtion ? For example, if $a_n=\frac{1}{n}$ then $\frac{d}{dn}(a_n)=-\frac{1}{n^2}$, is this ok ?",,['analysis']
64,Is the set of all positive real numbers dense in $\mathbb{R}$,Is the set of all positive real numbers dense in,\mathbb{R},"I am working on a problem I found that asks whether the set $S = \{x \in \mathbb{R} \mid x \ge 0\}$ is dense in $\mathbb{R}$. The theorem I have been using states the following: ""$S$ is dense in $\mathbb{R} \iff \forall a,b \in \mathbb{R}$ with $a < b$, then $\exists x \in S$ such that $x \in (a,b)$."" Now my logic from what I have so far is basically that for any $a$ and $b$ you give me, I can find the midpoint between $a$ and $b$, which will consistently give me a positive real number. This feels like I'm just constructing sentences and not really ""proving"" it, however. I am a bit confused and would like any words of advice.","I am working on a problem I found that asks whether the set $S = \{x \in \mathbb{R} \mid x \ge 0\}$ is dense in $\mathbb{R}$. The theorem I have been using states the following: ""$S$ is dense in $\mathbb{R} \iff \forall a,b \in \mathbb{R}$ with $a < b$, then $\exists x \in S$ such that $x \in (a,b)$."" Now my logic from what I have so far is basically that for any $a$ and $b$ you give me, I can find the midpoint between $a$ and $b$, which will consistently give me a positive real number. This feels like I'm just constructing sentences and not really ""proving"" it, however. I am a bit confused and would like any words of advice.",,"['analysis', 'proof-writing']"
65,Show inequality for positive real numbers,Show inequality for positive real numbers,,"If $x,y$ are positive real numbers then we have that $$ \frac{1}{\sqrt{x+y}}<\frac{1}{\sqrt{x}}+\frac{1}{\sqrt{y}}$$ right? But how can we show that? I have tried the following but I don't think that this is the way we should go. \begin{align*}\frac{1}{\sqrt{x + y}} < \frac{1}{\sqrt{x}} + \frac{1}{\sqrt{y}}=\frac{\sqrt{y}+\sqrt{x}}{\sqrt{xy}} & \iff \left(\frac{1}{\sqrt{x + y}}\right)^2 < \left(\frac{\sqrt{y}+\sqrt{x}}{\sqrt{xy}}\right)^2 \\ & \iff \frac{1}{x + y} <\frac{x+y+2\sqrt{xy}}{xy} \\ & \iff  xy<(x+y)(x+y+2\sqrt{xy}) \\ & \iff xy<x^2+xy+2x\sqrt{xy}+xy+y^2+2y\sqrt{xy}\\ & \iff 0<x^2+2(x+y)\sqrt{xy}+xy+y^2\end{align*}",If are positive real numbers then we have that right? But how can we show that? I have tried the following but I don't think that this is the way we should go.,"x,y  \frac{1}{\sqrt{x+y}}<\frac{1}{\sqrt{x}}+\frac{1}{\sqrt{y}} \begin{align*}\frac{1}{\sqrt{x + y}} < \frac{1}{\sqrt{x}} + \frac{1}{\sqrt{y}}=\frac{\sqrt{y}+\sqrt{x}}{\sqrt{xy}} & \iff \left(\frac{1}{\sqrt{x + y}}\right)^2 < \left(\frac{\sqrt{y}+\sqrt{x}}{\sqrt{xy}}\right)^2 \\ & \iff \frac{1}{x + y} <\frac{x+y+2\sqrt{xy}}{xy} \\ & \iff  xy<(x+y)(x+y+2\sqrt{xy}) \\ & \iff xy<x^2+xy+2x\sqrt{xy}+xy+y^2+2y\sqrt{xy}\\ & \iff 0<x^2+2(x+y)\sqrt{xy}+xy+y^2\end{align*}","['calculus', 'analysis', 'inequality']"
66,To evaluate $\lim_{n\to\infty} \frac{10^n}{n!} .$,To evaluate,\lim_{n\to\infty} \frac{10^n}{n!} .,I am having a lot of trouble evaluating  $$\lim_{n\to\infty} \dfrac{10^n}{n!} .$$ I know intuitively that $n!$ is much larger and this expression should go to zero but I just can't figure out how to prove it.,I am having a lot of trouble evaluating  $$\lim_{n\to\infty} \dfrac{10^n}{n!} .$$ I know intuitively that $n!$ is much larger and this expression should go to zero but I just can't figure out how to prove it.,,"['sequences-and-series', 'analysis', 'limits', 'factorial', 'infinity']"
67,Find the number of distinct real roots of $(x-a)^3+(x-b)^3+(x-c)^3=0$ [duplicate],Find the number of distinct real roots of  [duplicate],(x-a)^3+(x-b)^3+(x-c)^3=0,"This question already has answers here : How many real roots does $(x-a)^3+(x-b)^3+(x-c)^3$ have? (3 answers) Closed 10 years ago . Problem :Find the number of distinct real roots of $(x-a)^3+(x-b)^3+(x-c)^3=0$ where $a,b,c$ are distinct real numbers Solution : $(x-a)^3+(x-b)^3+(x-c)^3=0$ $3x^3-3x^2(a+b+c)+3x(a^2+b^2+c^2)-a^3-b^3-c^3=0$ By Descartes rule of sign,number of positive real roots $=3$ But are they distinct $?$ Answer :- number of distinct real roots $ =1$","This question already has answers here : How many real roots does $(x-a)^3+(x-b)^3+(x-c)^3$ have? (3 answers) Closed 10 years ago . Problem :Find the number of distinct real roots of where are distinct real numbers Solution : By Descartes rule of sign,number of positive real roots But are they distinct Answer :- number of distinct real roots","(x-a)^3+(x-b)^3+(x-c)^3=0 a,b,c (x-a)^3+(x-b)^3+(x-c)^3=0 3x^3-3x^2(a+b+c)+3x(a^2+b^2+c^2)-a^3-b^3-c^3=0 =3 ?  =1","['real-analysis', 'analysis']"
68,Why does $\sin(x) - \sin(y)=2 \cos(\frac{x+y}{2}) \sin(\frac{x-y}{2})$?,Why does ?,\sin(x) - \sin(y)=2 \cos(\frac{x+y}{2}) \sin(\frac{x-y}{2}),Why does this equality hold? $\sin x - \sin y = 2 \cos(\frac{x+y}{2}) \sin(\frac{x-y}{2})$ . My professor was saying that since (i) $\sin(A+B)=\sin A \cos B+ \sin B \cos A$ and (ii) $\sin(A-B) = \sin A \cos B - \sin B \cos A$ we just let $A=\frac{x+y}{2}$ and $B=\frac{x-y}{2}$ . But I tried to write this out and could not figure it out. Any help would be appreciated,Why does this equality hold? . My professor was saying that since (i) and (ii) we just let and . But I tried to write this out and could not figure it out. Any help would be appreciated,\sin x - \sin y = 2 \cos(\frac{x+y}{2}) \sin(\frac{x-y}{2}) \sin(A+B)=\sin A \cos B+ \sin B \cos A \sin(A-B) = \sin A \cos B - \sin B \cos A A=\frac{x+y}{2} B=\frac{x-y}{2},"['real-analysis', 'analysis', 'trigonometry']"
69,Prove that the function is bounded: $f(x) = \frac{1}{x^{2}+1}$,Prove that the function is bounded:,f(x) = \frac{1}{x^{2}+1},"Prove that the function is bounded: $$f(x) = \frac{1}{x^{2}+1}$$ I'd like to know several (easy) ways of proofing this and I found 2 ways but actually they are so similar, it might just be the same way.. :p Every convergent function is bounded. If function has infimum and supremum then it's bounded. Is it actually bounded if it just has one of both? 1. $$\lim_{x\rightarrow\infty}\frac{1}{x^{2}+1}= 0$$ $$\lim_{x\rightarrow-\infty}\frac{1}{x^{2}+1}= 0$$ $\Rightarrow 0$ is limit of function so it's convergent and thus bounded. But what if we had 2 different values, for $\pm\infty$? 2.Calculate supremum:$$\lim_{x\rightarrow0}\frac{1}{x^{2}+1}= \frac{1}{0+1}=1$$ Calculate infimum: $$\lim_{x\rightarrow\infty}\frac{1}{x^{2}+1}= 0$$ Thus function is bounded. Are both ways correct? If you answer, please answer to all my questions.","Prove that the function is bounded: $$f(x) = \frac{1}{x^{2}+1}$$ I'd like to know several (easy) ways of proofing this and I found 2 ways but actually they are so similar, it might just be the same way.. :p Every convergent function is bounded. If function has infimum and supremum then it's bounded. Is it actually bounded if it just has one of both? 1. $$\lim_{x\rightarrow\infty}\frac{1}{x^{2}+1}= 0$$ $$\lim_{x\rightarrow-\infty}\frac{1}{x^{2}+1}= 0$$ $\Rightarrow 0$ is limit of function so it's convergent and thus bounded. But what if we had 2 different values, for $\pm\infty$? 2.Calculate supremum:$$\lim_{x\rightarrow0}\frac{1}{x^{2}+1}= \frac{1}{0+1}=1$$ Calculate infimum: $$\lim_{x\rightarrow\infty}\frac{1}{x^{2}+1}= 0$$ Thus function is bounded. Are both ways correct? If you answer, please answer to all my questions.",,"['calculus', 'analysis', 'functions', 'convergence-divergence', 'supremum-and-infimum']"
70,continuous and strictly increasing implies differentiable,continuous and strictly increasing implies differentiable,,"I am not sure if this is true, but intuitively it seems that if a function is strictly increasing and it is also continuous...it is differentiable. It may be because there are no bumps like in the absolute value.","I am not sure if this is true, but intuitively it seems that if a function is strictly increasing and it is also continuous...it is differentiable. It may be because there are no bumps like in the absolute value.",,['analysis']
71,Determine the minimum of $\frac{\int_0^1{x^2\left( f'\left( x \right) \right) ^2 dx}}{\int_0^1{x^2\left( f\left( x \right) \right) ^2dx}}$ [closed],Determine the minimum of  [closed],\frac{\int_0^1{x^2\left( f'\left( x \right) \right) ^2 dx}}{\int_0^1{x^2\left( f\left( x \right) \right) ^2dx}},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question For all non-zero continuously differentiable function $f:[0,1]\to\mathbb R,f(1)=0$ , determine the minimum of $$\dfrac{\displaystyle \int_0^1{x^2\left( f'\left( x \right) \right) ^2\mathrm dx}}{\displaystyle \int_0^1{x^2\left( f\left( x \right) \right) ^2\mathrm dx}}$$ I totally have no idea how to start. Can anyone help?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question For all non-zero continuously differentiable function , determine the minimum of I totally have no idea how to start. Can anyone help?","f:[0,1]\to\mathbb R,f(1)=0 \dfrac{\displaystyle \int_0^1{x^2\left( f'\left( x \right) \right) ^2\mathrm dx}}{\displaystyle \int_0^1{x^2\left( f\left( x \right) \right) ^2\mathrm dx}}","['real-analysis', 'calculus', 'functional-analysis', 'analysis', 'maxima-minima']"
72,What Is Bigger $100^{100}$or $\sqrt{99^{99} \cdot 101^{101}}$,What Is Bigger or,100^{100} \sqrt{99^{99} \cdot 101^{101}},Hello every what is bigger $100^{100}$ or $\sqrt{99^{99} \cdot 101^{101}}$ ? I tried to square up and I got $100^{200}$ or $99^{99} \cdot 101^{101}$ and I don't have an idea how to continue.,Hello every what is bigger or ? I tried to square up and I got or and I don't have an idea how to continue.,100^{100} \sqrt{99^{99} \cdot 101^{101}} 100^{200} 99^{99} \cdot 101^{101},"['algebra-precalculus', 'analysis', 'inequality', 'jensen-inequality', 'number-comparison']"
73,Converting an infinite series to a definite integral,Converting an infinite series to a definite integral,,"The expression that I have is, $$\lim_{n \to \infty}\sum_{j=0}^n\left(\frac{1}{\sqrt{n^2 + j}} + \frac{1}{\sqrt{n^2 - j}}\right).$$ Original expression: $$\lim_{n \to \infty}\sum_{j=-n}^n \left(\frac{1}{\sqrt{n^2 - j}}\right).$$ I have prove that this summation is equal to '2' What I have tried, Taylor expanding the root and then summing the terms trying to convert the summation into an integral but this doesn't seem to work because of the $\frac{j}{n^2}$ term which comes when one tries to factor out the $n^2 $ from the denominator. I had posted this problem Converting an infinite summation to an integral which looks similar but doesnt not have the root expression. THis was by accident but I decided not to delete the question because people had answered already.","The expression that I have is, Original expression: I have prove that this summation is equal to '2' What I have tried, Taylor expanding the root and then summing the terms trying to convert the summation into an integral but this doesn't seem to work because of the term which comes when one tries to factor out the from the denominator. I had posted this problem Converting an infinite summation to an integral which looks similar but doesnt not have the root expression. THis was by accident but I decided not to delete the question because people had answered already.",\lim_{n \to \infty}\sum_{j=0}^n\left(\frac{1}{\sqrt{n^2 + j}} + \frac{1}{\sqrt{n^2 - j}}\right). \lim_{n \to \infty}\sum_{j=-n}^n \left(\frac{1}{\sqrt{n^2 - j}}\right). \frac{j}{n^2} n^2 ,"['sequences-and-series', 'analysis']"
74,Show that $\mathbb{R^{n}}$ is not compact by definition,Show that  is not compact by definition,\mathbb{R^{n}},"I want to show by definition that $\mathbb{R^{n}}$ is not compact, so I did this: Let ${\Omega}$ be an open cover of $\mathbb{R^n}$ and suppose that $\mathbb{R^n}$ is compact, then there exists a finite subcollection of ${\Omega}_{i=1}^{k}$ such that $\mathbb{R^{n}}$ is covered by $\Omega_{i=1}^{k}$ . So, ${\cup}_{i=1}^{k}\Omega_{i}$ $\supseteq \mathbb{R^{n}}$ . From this part, I don't know how to continue the proof, I think the last argument is a contradiction but I'm not quite sure. Can you help me? Please.","I want to show by definition that is not compact, so I did this: Let be an open cover of and suppose that is compact, then there exists a finite subcollection of such that is covered by . So, . From this part, I don't know how to continue the proof, I think the last argument is a contradiction but I'm not quite sure. Can you help me? Please.",\mathbb{R^{n}} {\Omega} \mathbb{R^n} \mathbb{R^n} {\Omega}_{i=1}^{k} \mathbb{R^{n}} \Omega_{i=1}^{k} {\cup}_{i=1}^{k}\Omega_{i} \supseteq \mathbb{R^{n}},"['general-topology', 'analysis', 'manifolds', 'compactness']"
75,$f:S^1\rightarrow S^1$ injective but not surjective,injective but not surjective,f:S^1\rightarrow S^1,"I thought this question was trivial, but I actually can't answer it, I hope I'm not missing something important. Let $S^1:=\{z\in \mathbb{C} \textit{ such that } |z|=1\}$. Can there be an injective continuous function $f:S^1\rightarrow S^1$ which is not surjective? Then this question generalizes to: Consider $M_n$ a n-dimensional differential compact manifold. Can there be $f:M_n\rightarrow M_n$ continuous and injective but not surjective?","I thought this question was trivial, but I actually can't answer it, I hope I'm not missing something important. Let $S^1:=\{z\in \mathbb{C} \textit{ such that } |z|=1\}$. Can there be an injective continuous function $f:S^1\rightarrow S^1$ which is not surjective? Then this question generalizes to: Consider $M_n$ a n-dimensional differential compact manifold. Can there be $f:M_n\rightarrow M_n$ continuous and injective but not surjective?",,"['real-analysis', 'geometry', 'analysis', 'functions', 'differential-geometry']"
76,The sequence $a_n=\frac{2n+1}{(n-1)^2}$ decreases monotonically,The sequence  decreases monotonically,a_n=\frac{2n+1}{(n-1)^2},How could we show that the sequence $a_n=\frac{2n+1}{(n-1)^2}$ decreases monotonically? When we take the quotient $\frac{a_n}{a_{n+1}}$ we get $\frac{n^2(2n+1)}{(n-1)^2(2n+3)}$. Howcan we conclude that this quotient is $\geq 1$ ?,How could we show that the sequence $a_n=\frac{2n+1}{(n-1)^2}$ decreases monotonically? When we take the quotient $\frac{a_n}{a_{n+1}}$ we get $\frac{n^2(2n+1)}{(n-1)^2(2n+3)}$. Howcan we conclude that this quotient is $\geq 1$ ?,,"['real-analysis', 'sequences-and-series', 'analysis', 'inequality']"
77,Analysis: Prove divergence of sequence $(n!)^{\frac2n}$,Analysis: Prove divergence of sequence,(n!)^{\frac2n},I am trying to prove that the sequence $$a_n = (n!)^{\frac2n}$$ tends to infinity as $ n \to \infty $. I've tried different methods but I haven't really got anywhere. Any solutions/hints?,I am trying to prove that the sequence $$a_n = (n!)^{\frac2n}$$ tends to infinity as $ n \to \infty $. I've tried different methods but I haven't really got anywhere. Any solutions/hints?,,"['real-analysis', 'sequences-and-series', 'analysis', 'infinity', 'divergent-series']"
78,Proof that a discrete metric is indeed a metric space,Proof that a discrete metric is indeed a metric space,,"QUESTION Let X be any set and $d : X \times X \to \mathbf{R}$ be given by $$ d(x,y) = \begin{cases} 0,  & \text{if $x = y$} \\ 1, & \text{if $x \neq y$}  \\ \end{cases}$$ Show that $d$ is a metric on $X$. REMARKS I'm interested in getting to understand this question. First, I assume we're discussing what is known as the Discrete Metric here. But I'm finding it a bit ""too easy"" to prove the axioms. And so I get the impression that I'm doing it wrong. For example; when looking at the axioms: $d(x,y) \ge 0$ $d(x,y) = 0 ,\text{iff}: x=y$ $d(x,y) = d(y,x)$ Both appear to be self-explanatory. The definition of the metric space does make it clear and I find myself doing nothing more than re-stating the definition of the metric to ""prove"" these points. $4.$ Triangle Inequality: $d(x,y) \le d(x,z) + d(z,y)$ This I showed by considering a number of different cases (but not all of them - should I do all possible computations of the $x=/\neq y = /\neq z$ combination? ) and find this to be true in each case. I guess all I want to know - is, is the proof of this as simple as it looks? Thanks","QUESTION Let X be any set and $d : X \times X \to \mathbf{R}$ be given by $$ d(x,y) = \begin{cases} 0,  & \text{if $x = y$} \\ 1, & \text{if $x \neq y$}  \\ \end{cases}$$ Show that $d$ is a metric on $X$. REMARKS I'm interested in getting to understand this question. First, I assume we're discussing what is known as the Discrete Metric here. But I'm finding it a bit ""too easy"" to prove the axioms. And so I get the impression that I'm doing it wrong. For example; when looking at the axioms: $d(x,y) \ge 0$ $d(x,y) = 0 ,\text{iff}: x=y$ $d(x,y) = d(y,x)$ Both appear to be self-explanatory. The definition of the metric space does make it clear and I find myself doing nothing more than re-stating the definition of the metric to ""prove"" these points. $4.$ Triangle Inequality: $d(x,y) \le d(x,z) + d(z,y)$ This I showed by considering a number of different cases (but not all of them - should I do all possible computations of the $x=/\neq y = /\neq z$ combination? ) and find this to be true in each case. I guess all I want to know - is, is the proof of this as simple as it looks? Thanks",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces']"
79,Addition of points on Metric Space,Addition of points on Metric Space,,"Well, I was not quite aware that addition of points is not defined in metric spaces but is defined only on linear spaces and others. Could anyone elaborate why is this?  Is the addition of intervals (closed or open) defined? I am confused as to how addition of points is undefined on metric space but is defined on linear space. I would appreciate if anyone clears this misconception/confusion.","Well, I was not quite aware that addition of points is not defined in metric spaces but is defined only on linear spaces and others. Could anyone elaborate why is this?  Is the addition of intervals (closed or open) defined? I am confused as to how addition of points is undefined on metric space but is defined on linear space. I would appreciate if anyone clears this misconception/confusion.",,"['analysis', 'metric-spaces']"
80,Does every uncountable subset of $\mathbb R$ have a bounded countable subset?,Does every uncountable subset of  have a bounded countable subset?,\mathbb R,"I can just figure out that arbitrary uncountable set has a countable subset, which is trivial. However, I don't know whether I can get a bounded one.","I can just figure out that arbitrary uncountable set has a countable subset, which is trivial. However, I don't know whether I can get a bounded one.",,"['analysis', 'elementary-set-theory']"
81,Convergence of sequence $x_{n+1}=\frac{x_n^2+2}{2x_{n}-1}$,Convergence of sequence,x_{n+1}=\frac{x_n^2+2}{2x_{n}-1},"I got a problem as follows, I have stucked for a week and I got no progress... May I recieve some guidance? Define a sequence $\{x_n\}$ by $x_1=c$ and $\displaystyle x_{n+1}=\frac{x_{n}^{2}+2}{2x_n - 1}$ for $n\in\mathbb{N}$ , where $c\neq \frac{1}{2}$ is a constant. Prove that $\{x_n\}$ is convergent and find its limit. (Hint: Consider $\displaystyle c>\frac{1}{2}$ and $\displaystyle c<\frac{1}{2}$ separately.) I can kind of guess $\displaystyle\lim_{n\rightarrow\infty}x_n = 2$ , and find the recurrence relationship between $|x_{n+1} - 2|$ and $|x_n - 2|$ , but I failed to proceed. Nevertheless, I tried with the Cauchy convergence criterion by considering $|x_{n+1} - x_n|$ and I end up with nothing. Thank you in advance for your kind reply.","I got a problem as follows, I have stucked for a week and I got no progress... May I recieve some guidance? Define a sequence by and for , where is a constant. Prove that is convergent and find its limit. (Hint: Consider and separately.) I can kind of guess , and find the recurrence relationship between and , but I failed to proceed. Nevertheless, I tried with the Cauchy convergence criterion by considering and I end up with nothing. Thank you in advance for your kind reply.",\{x_n\} x_1=c \displaystyle x_{n+1}=\frac{x_{n}^{2}+2}{2x_n - 1} n\in\mathbb{N} c\neq \frac{1}{2} \{x_n\} \displaystyle c>\frac{1}{2} \displaystyle c<\frac{1}{2} \displaystyle\lim_{n\rightarrow\infty}x_n = 2 |x_{n+1} - 2| |x_n - 2| |x_{n+1} - x_n|,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'recurrence-relations']"
82,"Prove that $S=\{(x,y):|x+y|\leq 1, |xy|\leq 1\}$ is a compact set",Prove that  is a compact set,"S=\{(x,y):|x+y|\leq 1, |xy|\leq 1\}","In my analysis test today, I was asked the question Prove that the set $$S=\{(x,y):|x+y|\leq 1, |xy|\leq 1\}$$ is compact in $\mathbb R^2$ . Now, of course, to prove compactness, we need to show that $S$ is closed and bounded, and then use the Heine Borel Theorem. The bounded part is easy to show, since the set is contained in a ball of radius $2$ . It was the closed part that sucked my blood out of me. I know that it's intuitively clear since the set contains the boundary. But, that's not an answer you write in your analysis test. What I did was to write $$S=S_1\cap S_2$$ where \begin{align*} S_1&=\{(x,y):|x+y|\leq 1\}\\ S_2&=\{(x,y):|xy|\leq 1\} \end{align*} and then tried to prove that $S_1^\prime$ and $S_2^\prime$ are open (which proves that $S_1$ and $S_2$ are closed, and hence their intersection is closed). In the case of $S_1^\prime$ , the distance between an arbitrary point and the lines $|x+y|\leq 1$ was easy to calculate explicitly; in the case of $S_2^\prime$ , not really so. Anyways, I feel, I wasn't rigorous enough, and there must be a neater way to solve this. Especially, I spent some time to find a continuous function $f$ such that the preimage of $f$ under some closed set is $S$ , but couldn't find it. I'm quite sure, there must be a function which does the job. Is there a better (than explicitly calculating distances) way to slove this? Here's a graph of $S$ , in case you are interested.","In my analysis test today, I was asked the question Prove that the set is compact in . Now, of course, to prove compactness, we need to show that is closed and bounded, and then use the Heine Borel Theorem. The bounded part is easy to show, since the set is contained in a ball of radius . It was the closed part that sucked my blood out of me. I know that it's intuitively clear since the set contains the boundary. But, that's not an answer you write in your analysis test. What I did was to write where and then tried to prove that and are open (which proves that and are closed, and hence their intersection is closed). In the case of , the distance between an arbitrary point and the lines was easy to calculate explicitly; in the case of , not really so. Anyways, I feel, I wasn't rigorous enough, and there must be a neater way to solve this. Especially, I spent some time to find a continuous function such that the preimage of under some closed set is , but couldn't find it. I'm quite sure, there must be a function which does the job. Is there a better (than explicitly calculating distances) way to slove this? Here's a graph of , in case you are interested.","S=\{(x,y):|x+y|\leq 1, |xy|\leq 1\} \mathbb R^2 S 2 S=S_1\cap S_2 \begin{align*}
S_1&=\{(x,y):|x+y|\leq 1\}\\
S_2&=\{(x,y):|xy|\leq 1\}
\end{align*} S_1^\prime S_2^\prime S_1 S_2 S_1^\prime |x+y|\leq 1 S_2^\prime f f S S","['real-analysis', 'calculus', 'analysis', 'continuity', 'compactness']"
83,If $t_n=\frac{1}{2n+1}-\frac{1}{2n+2}+\frac{1}{2n+3}-\frac{1}{2n+4}+\cdots +\frac{1}{4n-1}-\frac{1}{4n}$. Find $\lim_{n \to \infty} nt_n$,If . Find,t_n=\frac{1}{2n+1}-\frac{1}{2n+2}+\frac{1}{2n+3}-\frac{1}{2n+4}+\cdots +\frac{1}{4n-1}-\frac{1}{4n} \lim_{n \to \infty} nt_n,If $t_n=\frac{1}{2n+1}-\frac{1}{2n+2}+\frac{1}{2n+3}-\frac{1}{2n+4}+\cdots +\frac{1}{4n-1}-\frac{1}{4n}$ Find $\lim_{n \to \infty} nt_n$ First attempt: $t_n$ is positive(grouping two terms and performing subtraction we will get it) so is $nt_n$ . Now can we prove it is monotonically decreasing? If so then $\lim_{n \to \infty} nt_n=\lim_{n \to \infty}[(\frac{1}{2+1/n}-\frac{1}{2+2/n})+(\frac{1}{2+3/n}-\frac{1}{2+4/n})+\cdots +(\frac{1}{4-1/n}-\frac{1}{4})]$ and each of these terms will go to zero so is the limit. Second attempt: I was trying to use Riemann summation $t_n=\frac{1}{2n+1}-\frac{1}{2n+2}+\frac{1}{2n+3}-\frac{1}{2n+4}+\cdots +\frac{1}{4n-1}-\frac{1}{4n}= \frac{1}{2n+1}+\frac{1}{2n+2}+\frac{1}{2n+3}+\frac{1}{2n+4}+\cdots +\frac{1}{4n-1}+\frac{1}{4n}-2[\frac{1}{2n+2}+\frac{1}{2n+4}+\cdots \frac{1}{4n}]\Rightarrow \lim \frac 1n [nt_n]=\int_0^2\frac{dx}{2+x}-\int_0^1\frac{dx}{1+x}=\ln4-\ln 2-\ln 2=0$ So $\lim t_n=0$ So what will happen with $\lim nt_n$ Edit: As I got the answer is not $0$ because of the flaw. So can we have different approaches even with Riemann Sum to have the answer?,If Find First attempt: is positive(grouping two terms and performing subtraction we will get it) so is . Now can we prove it is monotonically decreasing? If so then and each of these terms will go to zero so is the limit. Second attempt: I was trying to use Riemann summation So So what will happen with Edit: As I got the answer is not because of the flaw. So can we have different approaches even with Riemann Sum to have the answer?,"t_n=\frac{1}{2n+1}-\frac{1}{2n+2}+\frac{1}{2n+3}-\frac{1}{2n+4}+\cdots +\frac{1}{4n-1}-\frac{1}{4n} \lim_{n \to \infty} nt_n t_n nt_n \lim_{n \to \infty} nt_n=\lim_{n \to \infty}[(\frac{1}{2+1/n}-\frac{1}{2+2/n})+(\frac{1}{2+3/n}-\frac{1}{2+4/n})+\cdots +(\frac{1}{4-1/n}-\frac{1}{4})] t_n=\frac{1}{2n+1}-\frac{1}{2n+2}+\frac{1}{2n+3}-\frac{1}{2n+4}+\cdots +\frac{1}{4n-1}-\frac{1}{4n}=
\frac{1}{2n+1}+\frac{1}{2n+2}+\frac{1}{2n+3}+\frac{1}{2n+4}+\cdots +\frac{1}{4n-1}+\frac{1}{4n}-2[\frac{1}{2n+2}+\frac{1}{2n+4}+\cdots \frac{1}{4n}]\Rightarrow \lim \frac 1n [nt_n]=\int_0^2\frac{dx}{2+x}-\int_0^1\frac{dx}{1+x}=\ln4-\ln 2-\ln 2=0 \lim t_n=0 \lim nt_n 0","['real-analysis', 'calculus', 'sequences-and-series', 'analysis', 'proof-verification']"
84,"Why can we say ""Let $(x_n)$ be a Cauchy sequence in $X$""?","Why can we say ""Let  be a Cauchy sequence in ""?",(x_n) X,"I am very new to analysis. To show a space $X$ is complete, they always begin with, such as, ""Let $(x_n)$ be a Cauchy sequence in $X$"". But I am confused that why we can must find a Cauchy sequence in $X$, without knowing any property of $X$. In other words, there must exist Cauchy sequence in any space $X$, why? I know Cauchy sequences are those whose elements become closer and closer in terms of a norm, as the label goes larger and larger. While I think nothing guarantees such a sequence exists in arbitrary space. For example from a textbook: (Please pay attention to my remarks) To show a linear subspace $M$ of a Banach space $(X, \|\cdot\|)$ is complete if and only if it is closed. Complete $\Rightarrow$ Closed: Let $x \in \overline{M}$, then there is a sequence $(x_n)$ in $M$ such that $\|x_n - x\| \to 0$ as $n \to \infty$. (Remark: Why is there such a sequence? Why does it guarantee that we can find such a sequence?) Since $(x_n)$ converges, it is Cauchy. (Remark: This is true if $M$ is a normed linear space. A subspace of a normed space must be normed with the same norm?) Completeness of $M$ guarantees the existence of an element $y \in M$ such that $\|x_n - y\| \to 0$ as $n \to \infty$. By uniqueness of limits, $x = y$. Hence $x \in M$ and consequently $M$ is closed. (Remark: Which one is the definition of closed set: Its complement set is open, or, the limit points of all its sequences are contained in it? If the later, $(x_n)$ is ""any"" sequence rather than ""a"" sequence in step 1, and consequently $x$ here is every limit point?) Complete $\Leftarrow$ Closed: Let $(x_n)$ be a Cauchy sequence in $M$. (Remark: Why does such a Cauchy sequence exist and can be found? I mean, who can guarantee this? Indeed, here it means ""a"" Cauchy sequence or ""any"" Cauchy sequence?) Then $(x_n)$ is a Cauchy sequence in $X$. Since $X$ is Banach and thus complete, there is an element $x \in X$ such that $\|x_n - x\| \to 0$ as $n \to \infty$. But then $x \in M$ since $M$ is closed. (Remark: Is this by definition of closed set I mentioned?) Hence $M$ is complete. (Remark: It seems not ""a"" but ""every"" Cauchy sequence in step 1?) As I said, I am very new to analysis. I am really hoping someone can help me to review and answer my remarks carefully. Thank you in advance for your patience.","I am very new to analysis. To show a space $X$ is complete, they always begin with, such as, ""Let $(x_n)$ be a Cauchy sequence in $X$"". But I am confused that why we can must find a Cauchy sequence in $X$, without knowing any property of $X$. In other words, there must exist Cauchy sequence in any space $X$, why? I know Cauchy sequences are those whose elements become closer and closer in terms of a norm, as the label goes larger and larger. While I think nothing guarantees such a sequence exists in arbitrary space. For example from a textbook: (Please pay attention to my remarks) To show a linear subspace $M$ of a Banach space $(X, \|\cdot\|)$ is complete if and only if it is closed. Complete $\Rightarrow$ Closed: Let $x \in \overline{M}$, then there is a sequence $(x_n)$ in $M$ such that $\|x_n - x\| \to 0$ as $n \to \infty$. (Remark: Why is there such a sequence? Why does it guarantee that we can find such a sequence?) Since $(x_n)$ converges, it is Cauchy. (Remark: This is true if $M$ is a normed linear space. A subspace of a normed space must be normed with the same norm?) Completeness of $M$ guarantees the existence of an element $y \in M$ such that $\|x_n - y\| \to 0$ as $n \to \infty$. By uniqueness of limits, $x = y$. Hence $x \in M$ and consequently $M$ is closed. (Remark: Which one is the definition of closed set: Its complement set is open, or, the limit points of all its sequences are contained in it? If the later, $(x_n)$ is ""any"" sequence rather than ""a"" sequence in step 1, and consequently $x$ here is every limit point?) Complete $\Leftarrow$ Closed: Let $(x_n)$ be a Cauchy sequence in $M$. (Remark: Why does such a Cauchy sequence exist and can be found? I mean, who can guarantee this? Indeed, here it means ""a"" Cauchy sequence or ""any"" Cauchy sequence?) Then $(x_n)$ is a Cauchy sequence in $X$. Since $X$ is Banach and thus complete, there is an element $x \in X$ such that $\|x_n - x\| \to 0$ as $n \to \infty$. But then $x \in M$ since $M$ is closed. (Remark: Is this by definition of closed set I mentioned?) Hence $M$ is complete. (Remark: It seems not ""a"" but ""every"" Cauchy sequence in step 1?) As I said, I am very new to analysis. I am really hoping someone can help me to review and answer my remarks carefully. Thank you in advance for your patience.",,"['real-analysis', 'analysis', 'metric-spaces', 'cauchy-sequences', 'complete-spaces']"
85,$f:\mathbb R \rightarrow \mathbb R$ be differentiable function such that $f'(x)$ is continuous and $f(x+1)=f(x)+1$ for all $x \in\mathbb R$.,be differentiable function such that  is continuous and  for all .,f:\mathbb R \rightarrow \mathbb R f'(x) f(x+1)=f(x)+1 x \in\mathbb R,"I was thinking about the problem: Let $f:\mathbb R \rightarrow \mathbb R$ be differentiable function such that $f'(x)$ is continuous and $f(x+1)=f(x)+1$ for all $x \in\mathbb R$ . Then which of the following options is correct? (a) $f'(x)$ must be bounded, (b) $f(x)$ must be bounded, (c) Both $f(x)$ and $f'(x)$ must be bounded, (d) Both $f(x)$ and $f'(x)$ must be unbounded. My attempts: If I take $f(x)$ to be of the simplest form that is $f(x)=x,$ so that the given condition is satisfied then we see that $f'(x)=1,$ which is bounded. So I think that choice (a) is the correct option. Am I going in the right direction? I want a proof in a more generalized way. Please help. Thanks in advance for your time.","I was thinking about the problem: Let be differentiable function such that is continuous and for all . Then which of the following options is correct? (a) must be bounded, (b) must be bounded, (c) Both and must be bounded, (d) Both and must be unbounded. My attempts: If I take to be of the simplest form that is so that the given condition is satisfied then we see that which is bounded. So I think that choice (a) is the correct option. Am I going in the right direction? I want a proof in a more generalized way. Please help. Thanks in advance for your time.","f:\mathbb R \rightarrow \mathbb R f'(x) f(x+1)=f(x)+1 x \in\mathbb R f'(x) f(x) f(x) f'(x) f(x) f'(x) f(x) f(x)=x, f'(x)=1,","['real-analysis', 'analysis']"
86,"If $f(x)$ and $g(x)$ are Riemann integrable and $f(x)\leq h(x)\leq g(x)$, must $h(x)$ be Riemann integrable?","If  and  are Riemann integrable and , must  be Riemann integrable?",f(x) g(x) f(x)\leq h(x)\leq g(x) h(x),Let $f$ and $g$ be Riemann integrable (real) functions and $$f(x)\leq h(x)\leq g(x).$$ Is it true that $h(x)$ is Riemann integrable? Can someone post a proof (if there is)? Thanks.,Let $f$ and $g$ be Riemann integrable (real) functions and $$f(x)\leq h(x)\leq g(x).$$ Is it true that $h(x)$ is Riemann integrable? Can someone post a proof (if there is)? Thanks.,,"['analysis', 'integration']"
87,Question about definition of Sequences in Analysis I by Tao.,Question about definition of Sequences in Analysis I by Tao.,,"Here's the definition of a sequence as laid out in the text: Let $m$ be an integer. A sequence $(a_n)_{n=m}^\infty$ of rational numbers is any function from the set $\{n \in \mathbf{Z} : n \geq m\}$ to $\mathbf{Q}$ . I can make sense out of this definition, but I was under the impression that a sequence has an ordering to it. I do not see any order implied on the ""outputs"" in the definition. Am I missing something?","Here's the definition of a sequence as laid out in the text: Let be an integer. A sequence of rational numbers is any function from the set to . I can make sense out of this definition, but I was under the impression that a sequence has an ordering to it. I do not see any order implied on the ""outputs"" in the definition. Am I missing something?",m (a_n)_{n=m}^\infty \{n \in \mathbf{Z} : n \geq m\} \mathbf{Q},"['real-analysis', 'sequences-and-series', 'analysis', 'functions', 'definition']"
88,Example of sequence $\langle x_n\rangle$ with $x_n>0$ such that series $\sum x_n$ is convergent but $\langle nx_n\rangle$ is NOT a null sequence. [closed],Example of sequence  with  such that series  is convergent but  is NOT a null sequence. [closed],\langle x_n\rangle x_n>0 \sum x_n \langle nx_n\rangle,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Please provide an example of  sequence $\langle x_n\rangle$ of positive terms such that series $\sum x_n$ is convergent but sequence $\langle nx_n\rangle$ is NOT  a null sequence. I try hard but could not  find one such. Please help.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Please provide an example of  sequence of positive terms such that series is convergent but sequence is NOT  a null sequence. I try hard but could not  find one such. Please help.",\langle x_n\rangle \sum x_n \langle nx_n\rangle,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'examples-counterexamples']"
89,"Total number of distinct $x\in[0,1]$ for which $\int_{0}^{x}\frac{t^2}{1+t^4}dt=2x-1$",Total number of distinct  for which,"x\in[0,1] \int_{0}^{x}\frac{t^2}{1+t^4}dt=2x-1","Find total number of distinct $x\in[0,1]$ for which $$\int_{0}^{x}\frac{t^2}{1+t^4}dt=2x-1$$ My multiple attempts are as follows:- Attempt $1$ : $$t^2=\tan\theta$$ $$2t\dfrac{dt}{d\theta}=\sec^2\theta$$ $$dt=\dfrac{\sec^2\theta}{2\sqrt{\tan\theta}}d\theta$$ $$\int_{0}^{\tan^{-1}x^2}\dfrac{\tan\theta}{1+\tan^2\theta}\dfrac{\sec^2\theta}{2\sqrt{\tan\theta}}d\theta=2x-1$$ $$\int_{0}^{\tan^{-1}x^2}\sqrt{\tan\theta}d\theta=4x-2$$ $$\int_{0}^{\tan^{-1}x^2}\dfrac{\sqrt{\sin\theta}}{\sqrt{\cos\theta}}=4x-2$$ $$\int_{0}^{\tan^{-1}x^2}\dfrac{\sin\theta}{\sqrt{\sin\theta}\sqrt{\cos\theta}}=4x-2$$ $$\cos\theta=y$$ $$\int_{1}^{\frac{1}{\sqrt{1+x^4}}}-\dfrac{dy}{\sqrt{y}\sqrt{\sqrt{1-y^2}}}=4x-2$$ Now it is unsolvable from here. Attempt $2$ : It seems like this integral is unsolvable, so let's apply Riemann sum $$\lim_{h\to0}h(f(0)+f(h)+f(2h)+f(3h)\cdots\cdots f(nh))=2x-1$$ $$nh=x$$ $$\lim_{h\to0}h\sum_{r=0}^{n}f(rh)=2x-1$$ $$\lim_{h\to0}h\sum_{r=0}^{n}\dfrac{r^2h^2}{1+r^4h^4}=2x-1$$ If we put the limit $h\rightarrow 0$ , then $0\cdot(0+0+0\cdots\cdots)$ where inside the bracket we have indeterminate form as $n\rightarrow \infty$ Now I was not getting how to proceed further.","Find total number of distinct for which My multiple attempts are as follows:- Attempt : Now it is unsolvable from here. Attempt : It seems like this integral is unsolvable, so let's apply Riemann sum If we put the limit , then where inside the bracket we have indeterminate form as Now I was not getting how to proceed further.","x\in[0,1] \int_{0}^{x}\frac{t^2}{1+t^4}dt=2x-1 1 t^2=\tan\theta 2t\dfrac{dt}{d\theta}=\sec^2\theta dt=\dfrac{\sec^2\theta}{2\sqrt{\tan\theta}}d\theta \int_{0}^{\tan^{-1}x^2}\dfrac{\tan\theta}{1+\tan^2\theta}\dfrac{\sec^2\theta}{2\sqrt{\tan\theta}}d\theta=2x-1 \int_{0}^{\tan^{-1}x^2}\sqrt{\tan\theta}d\theta=4x-2 \int_{0}^{\tan^{-1}x^2}\dfrac{\sqrt{\sin\theta}}{\sqrt{\cos\theta}}=4x-2 \int_{0}^{\tan^{-1}x^2}\dfrac{\sin\theta}{\sqrt{\sin\theta}\sqrt{\cos\theta}}=4x-2 \cos\theta=y \int_{1}^{\frac{1}{\sqrt{1+x^4}}}-\dfrac{dy}{\sqrt{y}\sqrt{\sqrt{1-y^2}}}=4x-2 2 \lim_{h\to0}h(f(0)+f(h)+f(2h)+f(3h)\cdots\cdots f(nh))=2x-1 nh=x \lim_{h\to0}h\sum_{r=0}^{n}f(rh)=2x-1 \lim_{h\to0}h\sum_{r=0}^{n}\dfrac{r^2h^2}{1+r^4h^4}=2x-1 h\rightarrow 0 0\cdot(0+0+0\cdots\cdots) n\rightarrow \infty","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'analysis']"
90,Integrating $\int\limits_{0}^{\infty} e^{-x^3}dx $,Integrating,\int\limits_{0}^{\infty} e^{-x^3}dx ,I have seen ways to evaluate this integral using the upper and lower incomplete gamma functions. I want to know if there are ways to calculate this integral using change of variables or tricks similar to the evaluation of $$ \int_{0}^{\infty} e^{-x^2}dx $$ using double integrals. Thanks in advance,I have seen ways to evaluate this integral using the upper and lower incomplete gamma functions. I want to know if there are ways to calculate this integral using change of variables or tricks similar to the evaluation of using double integrals. Thanks in advance, \int_{0}^{\infty} e^{-x^2}dx ,"['real-analysis', 'calculus', 'integration', 'analysis', 'definite-integrals']"
91,Does $\lim_{n\to\infty} \sum^{n^2}_{k=n}\frac{1}{k}$ exist?,Does  exist?,\lim_{n\to\infty} \sum^{n^2}_{k=n}\frac{1}{k},"Does $\lim_{n\to\infty}\sum^{n^2}_{k=n}\frac{1}{k}$ exist? MY TRIAL \begin{align} \lim_{n\to\infty}\sum^{n^2}_{k=n}\frac{1}{k}=\int^{n^2}_{n}\frac{1}{t} \,\mathrm{d}t=\ln n^2- \ln n=\ln n\end{align} Please, am I right? If no, can anyone show me the right answer? Thanks!","Does $\lim_{n\to\infty}\sum^{n^2}_{k=n}\frac{1}{k}$ exist? MY TRIAL \begin{align} \lim_{n\to\infty}\sum^{n^2}_{k=n}\frac{1}{k}=\int^{n^2}_{n}\frac{1}{t} \,\mathrm{d}t=\ln n^2- \ln n=\ln n\end{align} Please, am I right? If no, can anyone show me the right answer? Thanks!",,"['calculus', 'real-analysis', 'integration', 'analysis', 'definite-integrals']"
92,Decide if $\mathbb{R} \subset \mathbb{C}$ is open or closed.,Decide if  is open or closed.,\mathbb{R} \subset \mathbb{C},"The solution states that the ball of radius $\epsilon >0$ around a real number $x$ always contains the non-real number $x+i\epsilon/2$. I don't understand the answer, for every number $x \in \mathbb{R}$ there is an open ball, right? For every $x \in \mathbb{R}$ there is an $r>0$ such that I can form an open ball $B_r(x)\subset \mathbb{R}$.","The solution states that the ball of radius $\epsilon >0$ around a real number $x$ always contains the non-real number $x+i\epsilon/2$. I don't understand the answer, for every number $x \in \mathbb{R}$ there is an open ball, right? For every $x \in \mathbb{R}$ there is an $r>0$ such that I can form an open ball $B_r(x)\subset \mathbb{R}$.",,"['analysis', 'multivariable-calculus']"
93,Prove $ \frac {x_1 + \cdots + x_n}{n} \ge \sqrt[n]{x_1 x_2 \cdots x_n}$,Prove, \frac {x_1 + \cdots + x_n}{n} \ge \sqrt[n]{x_1 x_2 \cdots x_n},"I am studying computer science in the first term. I have to proof the following inequality: $$ \frac {x_1 + \cdots+ x_n}{n} \ge \sqrt[n]{x_1 x_2 \cdots x_n}$$ $x$ can be any positive real Number:  $ x \in \mathbb{R},x \gt0 $ I try to bring it in a form that is like Bernoulli's Inequality, but then realized that this is pretty much nonsense. So now I am having a hard time to prove it through induction. Beginning of the induction is clear: $n \rightarrow 1$ $ \large\frac {x_1}{1} \ge x_1$ Next Step: $n \rightarrow n+1$ For the start, lets write the left side of the inequality as $$x_a(n) = \frac{1}{n} \sum_{k=1}^n x_k$$  $$x_g(n) =  \prod_{k=1}^n x_k^\frac{1}{n}$$ So now we have to prove: $$x_a(n+1) = \frac{1}{n+1} \sum_{k=1}^{n+1} x_k$$  $$x_g(n+1) =  \prod_{k=1}^{n+1} x_k^\frac{1}{n+1}$$ $x_a(n+1) \ge x_g(n+1)$ Can you just give me a strategy on that? Thanks.","I am studying computer science in the first term. I have to proof the following inequality: $$ \frac {x_1 + \cdots+ x_n}{n} \ge \sqrt[n]{x_1 x_2 \cdots x_n}$$ $x$ can be any positive real Number:  $ x \in \mathbb{R},x \gt0 $ I try to bring it in a form that is like Bernoulli's Inequality, but then realized that this is pretty much nonsense. So now I am having a hard time to prove it through induction. Beginning of the induction is clear: $n \rightarrow 1$ $ \large\frac {x_1}{1} \ge x_1$ Next Step: $n \rightarrow n+1$ For the start, lets write the left side of the inequality as $$x_a(n) = \frac{1}{n} \sum_{k=1}^n x_k$$  $$x_g(n) =  \prod_{k=1}^n x_k^\frac{1}{n}$$ So now we have to prove: $$x_a(n+1) = \frac{1}{n+1} \sum_{k=1}^{n+1} x_k$$  $$x_g(n+1) =  \prod_{k=1}^{n+1} x_k^\frac{1}{n+1}$$ $x_a(n+1) \ge x_g(n+1)$ Can you just give me a strategy on that? Thanks.",,"['analysis', 'inequality']"
94,Prove $\frac{\pi}{4} = \sum_{n = 0}^{\infty}\frac{(-1)^{n}}{2n + 1}$ using Dominated or Monotone Convergence,Prove  using Dominated or Monotone Convergence,\frac{\pi}{4} = \sum_{n = 0}^{\infty}\frac{(-1)^{n}}{2n + 1},Is there a way to prove that $$\frac{\pi}{4} = \sum_{n = 0}^{\infty}\frac{(-1)^{n}}{2n + 1}$$ via the Dominated or Monotone Convergence Theorem?,Is there a way to prove that $$\frac{\pi}{4} = \sum_{n = 0}^{\infty}\frac{(-1)^{n}}{2n + 1}$$ via the Dominated or Monotone Convergence Theorem?,,"['real-analysis', 'sequences-and-series', 'analysis']"
95,"Find, with proof, the following limit","Find, with proof, the following limit",,"$$\lim_{x \to \infty} \, \cos \left(\dfrac{1}{x}\right)^{x} $$ So with this type of limit, does the value cos(1/x) take priority of the power of x as $x \rightarrow \infty$ ? I checked it on wolfram and found the limit to be 1; So would you realise that cos(1/x) as $x \rightarrow \infty$ goes to 1, and $1^{x}$ as $x \rightarrow \infty$ is just 1? Any help on the correct approach would be appreciated.","$$\lim_{x \to \infty} \, \cos \left(\dfrac{1}{x}\right)^{x} $$ So with this type of limit, does the value cos(1/x) take priority of the power of x as $x \rightarrow \infty$ ? I checked it on wolfram and found the limit to be 1; So would you realise that cos(1/x) as $x \rightarrow \infty$ goes to 1, and $1^{x}$ as $x \rightarrow \infty$ is just 1? Any help on the correct approach would be appreciated.",,"['real-analysis', 'analysis', 'limits']"
96,Contradict the Contraction Mapping Theorem,Contradict the Contraction Mapping Theorem,,"I am trying to show that the function $f(x) = 2\pi+x-\tan^{-1}x$ is contractive but has no fixed points. Finally I wish to conclude that it does not contradict the contraction mapping theorem. $f$ is contractive: by the mean value theorem we have: \begin{align}                                                                                                                                                                 |f(x)-f(y)| =& |f'(x)(x-y)|                                                                                                                                                   \end{align} It is sufficient to show that $f'(x) < 1$: \begin{align}                                                                                                                                                                 f'(x) =& 1 - \frac{1}{1+x^2} \\                                                                                                                                                     =& \frac{x^2}{1+x^2}                                                                                                                                                    \end{align} $f$ has no fixed points: Suppose $f$ does have a fixed point. Then $\lim_{x_n\to\infty} = x:$ \begin{align}                                                                                                                                                                 x_{n+1} =& 2\pi + x_n - \arctan(x_n) \\                                                                                                                                       \lim x_{n+1} =& \lim 2\pi + x_n - \arctan(x_n) \\                                                                                                                             x =& 2\pi +x - \arctan(x) \\                                                                                                                                                  0 =& 2\pi - \arctan(x)                                                                                                                                                        \end{align} This is a contradiction as $\frac{-\pi}{2} < \arctan(x) < \frac{\pi}{2}$ for all $x$. Thus $f$ does not have a fixed point. Now I am stuck on how to show that this does not contradict the contraction mapping theorem which says: Let $C$ be a closed subset of the real line. If $F$ is a contractive mapping of $C$ into $C$ then $F$ has a unique fixed point. Morever, this fixed point is the limit of every sequence obtained from $x_n+1 = F(x)$ starting with $x_0\in C$. For $f(x) = 2\pi+x-\arctan(x)$ the domain is all reals and so is the codomain. I don't see another premise that is violated by $f(x)$. Thanks for all the help.","I am trying to show that the function $f(x) = 2\pi+x-\tan^{-1}x$ is contractive but has no fixed points. Finally I wish to conclude that it does not contradict the contraction mapping theorem. $f$ is contractive: by the mean value theorem we have: \begin{align}                                                                                                                                                                 |f(x)-f(y)| =& |f'(x)(x-y)|                                                                                                                                                   \end{align} It is sufficient to show that $f'(x) < 1$: \begin{align}                                                                                                                                                                 f'(x) =& 1 - \frac{1}{1+x^2} \\                                                                                                                                                     =& \frac{x^2}{1+x^2}                                                                                                                                                    \end{align} $f$ has no fixed points: Suppose $f$ does have a fixed point. Then $\lim_{x_n\to\infty} = x:$ \begin{align}                                                                                                                                                                 x_{n+1} =& 2\pi + x_n - \arctan(x_n) \\                                                                                                                                       \lim x_{n+1} =& \lim 2\pi + x_n - \arctan(x_n) \\                                                                                                                             x =& 2\pi +x - \arctan(x) \\                                                                                                                                                  0 =& 2\pi - \arctan(x)                                                                                                                                                        \end{align} This is a contradiction as $\frac{-\pi}{2} < \arctan(x) < \frac{\pi}{2}$ for all $x$. Thus $f$ does not have a fixed point. Now I am stuck on how to show that this does not contradict the contraction mapping theorem which says: Let $C$ be a closed subset of the real line. If $F$ is a contractive mapping of $C$ into $C$ then $F$ has a unique fixed point. Morever, this fixed point is the limit of every sequence obtained from $x_n+1 = F(x)$ starting with $x_0\in C$. For $f(x) = 2\pi+x-\arctan(x)$ the domain is all reals and so is the codomain. I don't see another premise that is violated by $f(x)$. Thanks for all the help.",,"['analysis', 'fixed-point-theorems']"
97,"Proving $C([0,1])$ Is Not Complete Under $L_1$ Without A Counter Example",Proving  Is Not Complete Under  Without A Counter Example,"C([0,1]) L_1","I'd like to show that $C([0,1])$ (that is, the set of functions $\{f:[0,1]\rightarrow \mathbb{R} \, \textrm{ and } \, f \, \textrm{is continuous} \}$ is not a complete mertric space under the $L_1$ distance function: $$ d(f,g) = \int_0^1 |f(x)-g(x)|dx $$ I can find counter examples (for example, here ) but would rather prove it using definitions and principles so that I do not have to rely on committing specific degenerate sequences to memory. Since all compact metric spaces are complete, I have to figure that the place to start is to show that $C([0,1])$ is not compact and that somehow an infinite cover allows for a divergent Cauchy sequence. However, I don't how to show this (or if it's even the right approach to take).","I'd like to show that $C([0,1])$ (that is, the set of functions $\{f:[0,1]\rightarrow \mathbb{R} \, \textrm{ and } \, f \, \textrm{is continuous} \}$ is not a complete mertric space under the $L_1$ distance function: $$ d(f,g) = \int_0^1 |f(x)-g(x)|dx $$ I can find counter examples (for example, here ) but would rather prove it using definitions and principles so that I do not have to rely on committing specific degenerate sequences to memory. Since all compact metric spaces are complete, I have to figure that the place to start is to show that $C([0,1])$ is not compact and that somehow an infinite cover allows for a divergent Cauchy sequence. However, I don't how to show this (or if it's even the right approach to take).",,"['real-analysis', 'analysis', 'functional-analysis']"
98,How to determine whether an integral is convergent,How to determine whether an integral is convergent,,"I missed up the last lecture and can't understand how to determine whether an integral with parameters is convergent or divergent? For example: For which values of the parameters $p,q \in [0,\infty)$, the following integral is convergent $$  \int_{0}^{+\infty} \frac{dx}{x^p + x^q}. $$ Any help would be greatly appreciated.","I missed up the last lecture and can't understand how to determine whether an integral with parameters is convergent or divergent? For example: For which values of the parameters $p,q \in [0,\infty)$, the following integral is convergent $$  \int_{0}^{+\infty} \frac{dx}{x^p + x^q}. $$ Any help would be greatly appreciated.",,"['analysis', 'convergence-divergence', 'improper-integrals']"
99,"Looking for simple function: Passes through 0, sqrt like but never reaches 100","Looking for simple function: Passes through 0, sqrt like but never reaches 100",,"It's in the title. I am looking for simple function that passes through 0, square root like, never reaches 100, but comes closer and closer to it. I'm sure this is very basic, nothing fancy. But I don't know it now. Thanks a lot!","It's in the title. I am looking for simple function that passes through 0, square root like, never reaches 100, but comes closer and closer to it. I'm sure this is very basic, nothing fancy. But I don't know it now. Thanks a lot!",,"['analysis', 'functions']"
