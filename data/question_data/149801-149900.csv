,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,$E$ is closed $\iff\partial E$ (boundary of set $E$) $\subseteq E$,is closed  (boundary of set ),E \iff\partial E E \subseteq E,I am studying topology of euclidean space from William Wade's text book. I saw this question. But I cannot come up with any ideas. Please show me the solution in an instructive an clear way. Thank you for yourhelp. $E$ is closed $\iff\partial E$ (boundary of set $E$) $\subseteq E$,I am studying topology of euclidean space from William Wade's text book. I saw this question. But I cannot come up with any ideas. Please show me the solution in an instructive an clear way. Thank you for yourhelp. $E$ is closed $\iff\partial E$ (boundary of set $E$) $\subseteq E$,,"['real-analysis', 'general-topology', 'analysis']"
1,Regarding $\lim_{n \to \infty} n^{\frac{1}{n}}$,Regarding,\lim_{n \to \infty} n^{\frac{1}{n}},"Suppose $\lim_{n \to \infty} n^{\frac{1}{n}} = l \in \mathbb{R}$. The function $f(x) = x^n$ is continuous, then $$l^n=\left (\lim_{n \to \infty} n^{\frac{1}{n}} \right)^n=\lim_{n \to \infty} \left ( \left (n^{\frac{1}{n}} \right)^n \right ) =\lim_{n \to \infty} n = \infty.$$ Then it follows that $l = \infty $. This is false, but I couldn't find which step is wrong. Could you please help me? Thank you for your time.","Suppose $\lim_{n \to \infty} n^{\frac{1}{n}} = l \in \mathbb{R}$. The function $f(x) = x^n$ is continuous, then $$l^n=\left (\lim_{n \to \infty} n^{\frac{1}{n}} \right)^n=\lim_{n \to \infty} \left ( \left (n^{\frac{1}{n}} \right)^n \right ) =\lim_{n \to \infty} n = \infty.$$ Then it follows that $l = \infty $. This is false, but I couldn't find which step is wrong. Could you please help me? Thank you for your time.",,['analysis']
2,Is this function Lipschitz continuous?,Is this function Lipschitz continuous?,,"Let  $\mu \in \mathbb R^d$ be given. Is the function $f:\mathbb R^d \to \mathbb R^d$ defined as $f(x) := \exp(-\|x- \mu\|) (\mu - x)$  Lipschitz continuous? More specifically, for any $x, y \in \mathbb R^d$, is there a $D\in \mathbb R$ such that $$ | \exp(-\|x- \mu\|) (\mu - x) - \exp(-\|y- \mu\|) (\mu - y)] | \leq D\|x-y\|? $$ $\| \|$ is the Euclidean norm. Thanks!","Let  $\mu \in \mathbb R^d$ be given. Is the function $f:\mathbb R^d \to \mathbb R^d$ defined as $f(x) := \exp(-\|x- \mu\|) (\mu - x)$  Lipschitz continuous? More specifically, for any $x, y \in \mathbb R^d$, is there a $D\in \mathbb R$ such that $$ | \exp(-\|x- \mu\|) (\mu - x) - \exp(-\|y- \mu\|) (\mu - y)] | \leq D\|x-y\|? $$ $\| \|$ is the Euclidean norm. Thanks!",,"['analysis', 'multivariable-calculus', 'metric-spaces']"
3,proving a limit of a function,proving a limit of a function,,Let $h:\mathbb R\rightarrow\mathbb R$ a function and $\alpha\in\mathbb R\backslash\{0\}$. I want to proove that if $\lim_{x\rightarrow 0}\frac{h(x)}{x}=c$ then $\lim_{x\rightarrow 0}\frac{h(\alpha x)}{x}=\alpha c$. I've tried to define $\xi(x):=\frac{h(x)}{x}$ and so $\xi(\alpha x)=\frac{h(\alpha x)}{\alpha x}$ what is equivalent to $\alpha\cdot \xi(\alpha x)=\frac{h(\alpha x)}{x}$. So it's $\lim_{x\rightarrow 0}\frac{h(\alpha x)}{x}=\alpha\cdot\lim_{x\rightarrow 0}\xi(\alpha x)$. But now I am stuck. Anybody could help? Thanks a lot!,Let $h:\mathbb R\rightarrow\mathbb R$ a function and $\alpha\in\mathbb R\backslash\{0\}$. I want to proove that if $\lim_{x\rightarrow 0}\frac{h(x)}{x}=c$ then $\lim_{x\rightarrow 0}\frac{h(\alpha x)}{x}=\alpha c$. I've tried to define $\xi(x):=\frac{h(x)}{x}$ and so $\xi(\alpha x)=\frac{h(\alpha x)}{\alpha x}$ what is equivalent to $\alpha\cdot \xi(\alpha x)=\frac{h(\alpha x)}{x}$. So it's $\lim_{x\rightarrow 0}\frac{h(\alpha x)}{x}=\alpha\cdot\lim_{x\rightarrow 0}\xi(\alpha x)$. But now I am stuck. Anybody could help? Thanks a lot!,,"['real-analysis', 'analysis']"
4,"Find the upper and lower limits of $xf(x)$, as $x\rightarrow \infty$","Find the upper and lower limits of , as",xf(x) x\rightarrow \infty,"Define $$f(x)=\int_{x}^{x+1}\sin(t^2)dt$$ Find the upper and lower limits $xf(x)$, as $x\rightarrow \infty$. I find the answer as $+1, -1$ since $|\sin(x)| \le 1$. (Of course I calculated that function) Is that right or did I miss something? ========================================================== I solved this way. $2xf(x)=\cos(x^2)-\cos[(x+1)^2]+r(x)$  where $r(x)=\frac{\cos(x+1)^2}{x+1}-2x\int_{x^2}^{(x+1)^2}\frac{cos(u)}{4u^{3/2}}du$ Therefore $xf(x)=\frac{1}{2}{\cos(x^2)-\cos(x+1)^2}+\frac{r(x)}{2}$ Using trigonomeric formula: $2\sin(a)\sin(b)=\cos(a-b)-\cos(a+b)$ Rewrite $xf(x)=Â±\sin(x^2+x+\frac{1}{2})\sin(x+\frac{1}{2})+\frac{r(x)}{2}$. As $x\rightarrow \infty, r(x) \rightarrow 0$. Suppose $x^2=2k\pi$ for integer $k$. To achieve $Â±1$, we have to show that $x+\frac{1}{2} \rightarrow 2n\pi+\frac{\pi}{2}$ for some $n$ as $x\rightarrow \infty$. For each $n$, there exists $k$ such that $\sqrt{2\pi k}+\frac{1}{2} < 2n\pi+\frac{\pi}{2} <\sqrt{2\pi (k+1)} +\frac{1}{2} $ Distance between $\sqrt{2\pi k}+\frac{1}{2}$ and $2n\pi+\frac{\pi}{2}$ is at most $\sqrt{2\pi (k+1)} +\frac{1}{2} -\sqrt{2\pi k}+\frac{1}{2}$. As $k \rightarrow \infty$ the distance becomes arbitrary small. Therefore $ x \rightarrow \infty$, $xf(x)=Â±\sin(2n\pi+\frac{\pi}{2})=Â±1$.","Define $$f(x)=\int_{x}^{x+1}\sin(t^2)dt$$ Find the upper and lower limits $xf(x)$, as $x\rightarrow \infty$. I find the answer as $+1, -1$ since $|\sin(x)| \le 1$. (Of course I calculated that function) Is that right or did I miss something? ========================================================== I solved this way. $2xf(x)=\cos(x^2)-\cos[(x+1)^2]+r(x)$  where $r(x)=\frac{\cos(x+1)^2}{x+1}-2x\int_{x^2}^{(x+1)^2}\frac{cos(u)}{4u^{3/2}}du$ Therefore $xf(x)=\frac{1}{2}{\cos(x^2)-\cos(x+1)^2}+\frac{r(x)}{2}$ Using trigonomeric formula: $2\sin(a)\sin(b)=\cos(a-b)-\cos(a+b)$ Rewrite $xf(x)=Â±\sin(x^2+x+\frac{1}{2})\sin(x+\frac{1}{2})+\frac{r(x)}{2}$. As $x\rightarrow \infty, r(x) \rightarrow 0$. Suppose $x^2=2k\pi$ for integer $k$. To achieve $Â±1$, we have to show that $x+\frac{1}{2} \rightarrow 2n\pi+\frac{\pi}{2}$ for some $n$ as $x\rightarrow \infty$. For each $n$, there exists $k$ such that $\sqrt{2\pi k}+\frac{1}{2} < 2n\pi+\frac{\pi}{2} <\sqrt{2\pi (k+1)} +\frac{1}{2} $ Distance between $\sqrt{2\pi k}+\frac{1}{2}$ and $2n\pi+\frac{\pi}{2}$ is at most $\sqrt{2\pi (k+1)} +\frac{1}{2} -\sqrt{2\pi k}+\frac{1}{2}$. As $k \rightarrow \infty$ the distance becomes arbitrary small. Therefore $ x \rightarrow \infty$, $xf(x)=Â±\sin(2n\pi+\frac{\pi}{2})=Â±1$.",,['analysis']
5,Power series related problem,Power series related problem,,"I came across a problem that says: It is given that $\sum_{n=0}^{\infty}a_{n}z^{n}$ converges at $z=3+4i.$ Then the radius of convergence of the power series $\sum_{n=0}^{\infty}a_{n}z^{n}$ is (a)$\leq 5$ (b)$\geq 5$ (c)$<5$ (d)$>5$. We know if a power series $\sum_{n=0}^{\infty}a_{n}z^{n}$ converges for $z=z_{0},$ then it is absolutely convergent for every $z=z_{1},$ when $|z_{1}|<|z_{0}|.$ Using this property, i can conclude that $(a)$ is the correct choice as equality sign occurs keeping in mind that the given series converges at $|3+4i|=5.$ Am i going in the right direction? Please help. Thanks in advance for your time.","I came across a problem that says: It is given that $\sum_{n=0}^{\infty}a_{n}z^{n}$ converges at $z=3+4i.$ Then the radius of convergence of the power series $\sum_{n=0}^{\infty}a_{n}z^{n}$ is (a)$\leq 5$ (b)$\geq 5$ (c)$<5$ (d)$>5$. We know if a power series $\sum_{n=0}^{\infty}a_{n}z^{n}$ converges for $z=z_{0},$ then it is absolutely convergent for every $z=z_{1},$ when $|z_{1}|<|z_{0}|.$ Using this property, i can conclude that $(a)$ is the correct choice as equality sign occurs keeping in mind that the given series converges at $|3+4i|=5.$ Am i going in the right direction? Please help. Thanks in advance for your time.",,['real-analysis']
6,Questions about $f: \mathbb{R} \rightarrow \mathbb{R}$ with bounded derivative,Questions about  with bounded derivative,f: \mathbb{R} \rightarrow \mathbb{R},"I came across a problem that says: Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a function. If $|f'|$ is bounded, then which of the following option(s) is/are true? (a) The function $f$ is bounded. (b) The limit $\lim_{x\to\infty}f(x)$ exists. (c) The function $f$ is uniformly continuous. (d) The set $\{x \mid f(x)=0\}$ is compact. I am stuck on this problem. Please help. Thanks in advance for your time.","I came across a problem that says: Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a function. If $|f'|$ is bounded, then which of the following option(s) is/are true? (a) The function $f$ is bounded. (b) The limit $\lim_{x\to\infty}f(x)$ exists. (c) The function $f$ is uniformly continuous. (d) The set $\{x \mid f(x)=0\}$ is compact. I am stuck on this problem. Please help. Thanks in advance for your time.",,"['real-analysis', 'analysis']"
7,"Let $f: ~\mathbb R\rightarrow \mathbb R$ be a continuous function such that $\int_{-1}^{x}f(t)dt=0$ for all $x \in [-1,1]$",Let  be a continuous function such that  for all,"f: ~\mathbb R\rightarrow \mathbb R \int_{-1}^{x}f(t)dt=0 x \in [-1,1]","I was thinking about the problem that says: Let $f: ~\mathbb R\rightarrow \mathbb R$ be a continuous function such that $\int_{-1}^{x}f(t)dt=0$ for all $x \in [-1,1]$. Then which of the following option(s) is/are correct? (A) $f$ is identically $0$, (B) $f$ is a non-zero odd function, (C) $f$ is a non-zero even function, (D) $f$ is a non-zero periodic function. Please help. Thank you in advance for your time.","I was thinking about the problem that says: Let $f: ~\mathbb R\rightarrow \mathbb R$ be a continuous function such that $\int_{-1}^{x}f(t)dt=0$ for all $x \in [-1,1]$. Then which of the following option(s) is/are correct? (A) $f$ is identically $0$, (B) $f$ is a non-zero odd function, (C) $f$ is a non-zero even function, (D) $f$ is a non-zero periodic function. Please help. Thank you in advance for your time.",,"['real-analysis', 'analysis']"
8,Function whose integral is equal to the function,Function whose integral is equal to the function,,"Let $\lambda \in \mathbb{R}$ be a constant, $\lambda \neq 0$. Is there a function $f \in C[0,1]$, $f \neq 0$, that satisfies the following relation: $$\lambda f(s) = \int_0^s f(t) \, dt$$ Attempt at a solution: Applying Fundamental Theorem of Calculus, one gets $ \lambda f(s) = F(s) - F(0)$, which implies that $f(0) = 0$. It follows that $f$ cannot be differentiable, because if it were, then taking the dervative on both sides gives $\lambda f'(s) = f(s)$, which gives together with the boundary condition $f(s) = \exp[\frac{s}{\lambda}] - 1$, but this function does not satisfy the relation. Hence if such a function exists, then it cannot be differentiable on $[0,1]$.","Let $\lambda \in \mathbb{R}$ be a constant, $\lambda \neq 0$. Is there a function $f \in C[0,1]$, $f \neq 0$, that satisfies the following relation: $$\lambda f(s) = \int_0^s f(t) \, dt$$ Attempt at a solution: Applying Fundamental Theorem of Calculus, one gets $ \lambda f(s) = F(s) - F(0)$, which implies that $f(0) = 0$. It follows that $f$ cannot be differentiable, because if it were, then taking the dervative on both sides gives $\lambda f'(s) = f(s)$, which gives together with the boundary condition $f(s) = \exp[\frac{s}{\lambda}] - 1$, but this function does not satisfy the relation. Hence if such a function exists, then it cannot be differentiable on $[0,1]$.",,"['calculus', 'analysis', 'integration', 'functions']"
9,Use Cauchy's Multiplication Theorem and the Binomial Theorem to prove $\exp(x+y)=\exp(x)\exp(y)$,Use Cauchy's Multiplication Theorem and the Binomial Theorem to prove,\exp(x+y)=\exp(x)\exp(y),"I am to use Cauchy's Multiplication Theorem and the Binomial Theorem in order to prove $\exp(x+y)=\exp(x)\exp(y) $ but I have no idea where to begin. All I can think of doing is setting $\exp(x)$ as the sum to infinity of $(x^n)/n!$ and similarly for $\exp(y)$, $(y^n)/n!$","I am to use Cauchy's Multiplication Theorem and the Binomial Theorem in order to prove $\exp(x+y)=\exp(x)\exp(y) $ but I have no idea where to begin. All I can think of doing is setting $\exp(x)$ as the sum to infinity of $(x^n)/n!$ and similarly for $\exp(y)$, $(y^n)/n!$",,"['sequences-and-series', 'analysis', 'exponential-function', 'exponential-sum']"
10,"How do I prove that $f(x)=(\cos2\pi x,\sin2\pi x)$ is an open map?",How do I prove that  is an open map?,"f(x)=(\cos2\pi x,\sin2\pi x)","How do I prove that $f:\mathbb R\to \mathbb S^1$, $f(x)=(\cos2\pi x,\sin2\pi x)$ is an open map? I'm thinking about a simple solution. Maybe it's a hard question I need help here Thanks","How do I prove that $f:\mathbb R\to \mathbb S^1$, $f(x)=(\cos2\pi x,\sin2\pi x)$ is an open map? I'm thinking about a simple solution. Maybe it's a hard question I need help here Thanks",,"['real-analysis', 'general-topology', 'analysis']"
11,Epigraph of a function.,Epigraph of a function.,,"I hope you can give me some suggestions on convex functions. the function $f:(0,\infty)\rightarrow \mathbb{R}$ given by $f(x)=\dfrac{1}{x}$ is convex and continuous, but its epigraph is closed in $\mathbb{R}^{2}$ ?.","I hope you can give me some suggestions on convex functions. the function $f:(0,\infty)\rightarrow \mathbb{R}$ given by $f(x)=\dfrac{1}{x}$ is convex and continuous, but its epigraph is closed in $\mathbb{R}^{2}$ ?.",,"['real-analysis', 'analysis', 'optimization']"
12,Continuously Differentiable Curves in $\mathbb{R}^{d}$ and their Lebesgue Measure,Continuously Differentiable Curves in  and their Lebesgue Measure,\mathbb{R}^{d},"Show that the image of the curve $\Gamma\in\mathscr{C}^{1}\left([a,b]\to\mathbb{R}^{d}\right)$ has d-dimensional Lebesgue measure zero (of course, $d\geq2$). This can be proved using the absolute continuity of $\Gamma'$ (since $[a,b]$ is compact and $\Gamma'$ is assumed continuous, hence in $L^{1}([a,b])$) together with the fundamental theorem of calculus to obtain an $\epsilon$-small cover of $\Gamma$ by balls. But I am trying to prove this using more elementary means (i.e. without integration).  Intuitively, since $\Gamma$ is smooth, we ought to (for fine enough partitions) be able to cover $\Gamma$ by boxes which arise from its tangent line.  And by taking the partition of $[a,b]$ to be finer and finer, the ""tangent box"" cover ought to also get smaller and smaller. More rigorously, the vector-version of the mean value theorem can be applied: $$|\Gamma(t_{i-1})-\Gamma(t_{i})|\leq(t_{i}-t_{i-1})|\Gamma'(t_{i}^{\star})|\leq M_{i}\Delta t$$ where $t_{i}^{\star}\in(t_{i-1},t_{i})$ and $M_{i}=\sup_{t\in[t_{i-1},t_{i}]}|\Gamma'(t)|$ which exists and is finite since $\Gamma'$ is continuous. But to me, it's not quite clear how to rigorously construct a cover by boxes from here. NOTE: In the proof I mentioned (using integration), essentially you sum the left hand side over all partition intervals of uniform length $\delta$ (which depends on the $\Gamma'$), and ""integrate"" the right hand side. Actually, to be more specific, for each $\epsilon>0$ there exists a $\delta>0$ such that $||P||<\delta$ implies $\int_{t_{i-1}}^{t_{i}}|\Gamma'(t)|dt<\epsilon$ (e.g. absolute continuity).  This allows you to define numbers $\epsilon_{i}=\sup_{t,\bar{t}\in[t_{i-1},t_{i}]}|\Gamma(t)-\Gamma(\bar{t})|\leq\epsilon$ so that $\sum_{i=1}^{\#P}\epsilon_{i}\leq||\Gamma'||_{L^{1}([a,b])}$.  Then you can use these $\epsilon_{i}$ to put balls at each point $\Gamma(t_{i})$ of radius (say) $2\epsilon_{i}$, thus giving you an $\epsilon$-small cover.  Again though, this is harder to establish without integration theory.","Show that the image of the curve $\Gamma\in\mathscr{C}^{1}\left([a,b]\to\mathbb{R}^{d}\right)$ has d-dimensional Lebesgue measure zero (of course, $d\geq2$). This can be proved using the absolute continuity of $\Gamma'$ (since $[a,b]$ is compact and $\Gamma'$ is assumed continuous, hence in $L^{1}([a,b])$) together with the fundamental theorem of calculus to obtain an $\epsilon$-small cover of $\Gamma$ by balls. But I am trying to prove this using more elementary means (i.e. without integration).  Intuitively, since $\Gamma$ is smooth, we ought to (for fine enough partitions) be able to cover $\Gamma$ by boxes which arise from its tangent line.  And by taking the partition of $[a,b]$ to be finer and finer, the ""tangent box"" cover ought to also get smaller and smaller. More rigorously, the vector-version of the mean value theorem can be applied: $$|\Gamma(t_{i-1})-\Gamma(t_{i})|\leq(t_{i}-t_{i-1})|\Gamma'(t_{i}^{\star})|\leq M_{i}\Delta t$$ where $t_{i}^{\star}\in(t_{i-1},t_{i})$ and $M_{i}=\sup_{t\in[t_{i-1},t_{i}]}|\Gamma'(t)|$ which exists and is finite since $\Gamma'$ is continuous. But to me, it's not quite clear how to rigorously construct a cover by boxes from here. NOTE: In the proof I mentioned (using integration), essentially you sum the left hand side over all partition intervals of uniform length $\delta$ (which depends on the $\Gamma'$), and ""integrate"" the right hand side. Actually, to be more specific, for each $\epsilon>0$ there exists a $\delta>0$ such that $||P||<\delta$ implies $\int_{t_{i-1}}^{t_{i}}|\Gamma'(t)|dt<\epsilon$ (e.g. absolute continuity).  This allows you to define numbers $\epsilon_{i}=\sup_{t,\bar{t}\in[t_{i-1},t_{i}]}|\Gamma(t)-\Gamma(\bar{t})|\leq\epsilon$ so that $\sum_{i=1}^{\#P}\epsilon_{i}\leq||\Gamma'||_{L^{1}([a,b])}$.  Then you can use these $\epsilon_{i}$ to put balls at each point $\Gamma(t_{i})$ of radius (say) $2\epsilon_{i}$, thus giving you an $\epsilon$-small cover.  Again though, this is harder to establish without integration theory.",,"['analysis', 'measure-theory']"
13,Measure on Baire space,Measure on Baire space,,"Inspired by the first parenthetical sentence of Joel's answer to this question , I have the following question: is there any useful notion of measurability in the Baire space $\omega^\omega$? Some initial thoughts: Any countably additive, translation-invariant measure has to give measure 0 to any set of the form $$\lbrace f: \sigma\prec f\rbrace.$$ The same holds if we ask for a finitely additive translation-invariant measure which gives finite measure to the whole space. There are several natural surjections $\omega^\omega\rightarrow 2^\omega$, and the latter space has a nice measure theory; so we could, for any such surjection $s$, define a ""measure"" given by $m_s(X)=m(s[X])$. However, it's unclear to me that any of these would be particularly interesting. Even short of a decent ""measure"" on $\omega^\omega$ (and indeed, I believe none exists), we might still be able to talk about subsets of $\omega^\omega$ being ""measurable"" or ""non-measurable"" in terms of what sorts of pathologies they admit. For example, call $X\subseteq\omega^\omega$ definably measurable if $L(\mathbb{R}, X)\models $ ""Every set is measurable."" This is, however, a terrible notion of measurability: there are measure zero (and hence measurable) sets $X\subseteq 2^\omega$ such that $L(\mathbb{R}, X)$ contains a non-measurable set, so this is too restrictive. Thoughts?","Inspired by the first parenthetical sentence of Joel's answer to this question , I have the following question: is there any useful notion of measurability in the Baire space $\omega^\omega$? Some initial thoughts: Any countably additive, translation-invariant measure has to give measure 0 to any set of the form $$\lbrace f: \sigma\prec f\rbrace.$$ The same holds if we ask for a finitely additive translation-invariant measure which gives finite measure to the whole space. There are several natural surjections $\omega^\omega\rightarrow 2^\omega$, and the latter space has a nice measure theory; so we could, for any such surjection $s$, define a ""measure"" given by $m_s(X)=m(s[X])$. However, it's unclear to me that any of these would be particularly interesting. Even short of a decent ""measure"" on $\omega^\omega$ (and indeed, I believe none exists), we might still be able to talk about subsets of $\omega^\omega$ being ""measurable"" or ""non-measurable"" in terms of what sorts of pathologies they admit. For example, call $X\subseteq\omega^\omega$ definably measurable if $L(\mathbb{R}, X)\models $ ""Every set is measurable."" This is, however, a terrible notion of measurability: there are measure zero (and hence measurable) sets $X\subseteq 2^\omega$ such that $L(\mathbb{R}, X)$ contains a non-measurable set, so this is too restrictive. Thoughts?",,"['analysis', 'measure-theory', 'set-theory', 'descriptive-set-theory', 'the-baire-space']"
14,Inequality for logarithms,Inequality for logarithms,,"I conjecture the following inequality is true $$\ln x \le (x - 1)\ln\frac{x}{x-1}$$ for all $x > 1$, but I cannot give a proof. I will appreciate if someone can provide one.","I conjecture the following inequality is true $$\ln x \le (x - 1)\ln\frac{x}{x-1}$$ for all $x > 1$, but I cannot give a proof. I will appreciate if someone can provide one.",,"['analysis', 'inequality', 'logarithms']"
15,An example of topological space in which each singleton is not in  $G_\delta$,An example of topological space in which each singleton is not in,G_\delta,Does there exist a compact Hausdorff space such that each singleton is not in  $\cal{G_\delta}$? Maybe not difficult example.,Does there exist a compact Hausdorff space such that each singleton is not in  $\cal{G_\delta}$? Maybe not difficult example.,,"['general-topology', 'analysis']"
16,Finding minimum of multidimensional function,Finding minimum of multidimensional function,,"My calculus knowledge is pretty limited, but unfortunately I need to solve a problem of the following kind: I'm given a 2 dimensional function $f(x,y)$ from $\mathbb{R}^2$ to $\mathbb{R}$ and I want to know, where it attains its minimum value over $\mathbb{R}\times(a,b)$. Put differently I want to find an $x$ value and a $y\in(a,b)$ such that $f(x,y) \leq f(x',y')$ for all x' in $\mathbb{R}$ and all $y \in (a,b)$. I'll have to take the partial derivative of $f$ w.r.t $x$, but  I don't understand how y will come into play.","My calculus knowledge is pretty limited, but unfortunately I need to solve a problem of the following kind: I'm given a 2 dimensional function $f(x,y)$ from $\mathbb{R}^2$ to $\mathbb{R}$ and I want to know, where it attains its minimum value over $\mathbb{R}\times(a,b)$. Put differently I want to find an $x$ value and a $y\in(a,b)$ such that $f(x,y) \leq f(x',y')$ for all x' in $\mathbb{R}$ and all $y \in (a,b)$. I'll have to take the partial derivative of $f$ w.r.t $x$, but  I don't understand how y will come into play.",,"['calculus', 'analysis', 'optimization']"
17,Intersection of all compact sets of measure 1 in a measure space,Intersection of all compact sets of measure 1 in a measure space,,"The problem: Let $X = [0,1]$ and $\mathcal{B}$ be the Borel subsets of $X$. Let $\mu:\mathcal{B}\to[0,1]$ be a probability measure on $X$. Suppose that $\mu$ is regular, i.e., for all $B\in\mathcal{B}$ we have   $$\mu(B) = \inf\{\mu(O) | B\subset O, O\ \operatorname{is\ open}\}$$   and   $$\mu(B) = \sup\{\mu(K) | K\subset B, K\ \operatorname{is\ compact}\}.$$ Let $K = \cap_\alpha K_\alpha$ where $\{K_\alpha\}_\alpha$ is the collection of all compact subsets of $X$ with $\mu(K_\alpha) = 1$. Show that for every open set $O\subset X$ with $K\subset O$ there is $\alpha$ such that $K\subset K_\alpha\subset O$. Now we know that each $K_\alpha$ satisfies $K\subset K_\alpha$, by definition, so all we need to show is that for any open set $O\subset X$ with $K\subset O$, there exists some $\alpha$ such that $K_\alpha \subset O$. It is fairly easy to show that there is some compact set $J$ satisfying $$ K\subset J\subset O,$$ But what I'm not seeing is how we may conclude $\mu(J) = 1$. I should add, part (ii) of this problem asks us to show $\mu(K) = 1$, which is fairly easy from the part above, but it means for this part I cannot use it combined with monotonicity of the measure to show $\mu(J) = 1$, which is about the only idea I've come up with. Anyone have a hint at why $J$ must have measure 1, simply because it contains $K$, before knowing $K$ has measure 1? Thanks!","The problem: Let $X = [0,1]$ and $\mathcal{B}$ be the Borel subsets of $X$. Let $\mu:\mathcal{B}\to[0,1]$ be a probability measure on $X$. Suppose that $\mu$ is regular, i.e., for all $B\in\mathcal{B}$ we have   $$\mu(B) = \inf\{\mu(O) | B\subset O, O\ \operatorname{is\ open}\}$$   and   $$\mu(B) = \sup\{\mu(K) | K\subset B, K\ \operatorname{is\ compact}\}.$$ Let $K = \cap_\alpha K_\alpha$ where $\{K_\alpha\}_\alpha$ is the collection of all compact subsets of $X$ with $\mu(K_\alpha) = 1$. Show that for every open set $O\subset X$ with $K\subset O$ there is $\alpha$ such that $K\subset K_\alpha\subset O$. Now we know that each $K_\alpha$ satisfies $K\subset K_\alpha$, by definition, so all we need to show is that for any open set $O\subset X$ with $K\subset O$, there exists some $\alpha$ such that $K_\alpha \subset O$. It is fairly easy to show that there is some compact set $J$ satisfying $$ K\subset J\subset O,$$ But what I'm not seeing is how we may conclude $\mu(J) = 1$. I should add, part (ii) of this problem asks us to show $\mu(K) = 1$, which is fairly easy from the part above, but it means for this part I cannot use it combined with monotonicity of the measure to show $\mu(J) = 1$, which is about the only idea I've come up with. Anyone have a hint at why $J$ must have measure 1, simply because it contains $K$, before knowing $K$ has measure 1? Thanks!",,"['real-analysis', 'analysis']"
18,Proof of the classical div-curl-lemma,Proof of the classical div-curl-lemma,,"let $1 = \frac{1}{p} + \frac{1}{q}$ as usual. Let $f \in L^p, g \in L^q$ be vector fields from $\mathbb R^n$ to itself. Assume $div f = 0$ and there exists a function $G$ s.t. $\nabla G = g$. Then $f \cdot g \in \mathcal H^1$ is a Hardy space function. Do you know where I can find a proof of this conclusion? I am aware of a paper by Coifman et al. ""Compensated compactness and Hardy spaces"", but I am not granted access to this journal. Hence I am looking for an alternative resource.","let $1 = \frac{1}{p} + \frac{1}{q}$ as usual. Let $f \in L^p, g \in L^q$ be vector fields from $\mathbb R^n$ to itself. Assume $div f = 0$ and there exists a function $G$ s.t. $\nabla G = g$. Then $f \cdot g \in \mathcal H^1$ is a Hardy space function. Do you know where I can find a proof of this conclusion? I am aware of a paper by Coifman et al. ""Compensated compactness and Hardy spaces"", but I am not granted access to this journal. Hence I am looking for an alternative resource.",,"['analysis', 'reference-request', 'harmonic-analysis']"
19,Why is differentiability defined on open interval?,Why is differentiability defined on open interval?,,"If I have a subset $A \subseteq \mathbb{R}$ , a function $f: A \to \mathbb{R}$ , and a point $a \in A$ , then (according to the definitions I came across) for $f$ to be differentiable at $a$ , then there must exist some open interval $I$ so that $a \in I$ and $I \subseteq A$ . I tried understanding why this requirement exists, by first discarding it and seeing if it naturally arises from the other bits of the definition. NOTATION: For any $x \in X\subseteq \mathbb{R}$ , I define $D[x;X] \subseteq \mathbb{R}$ as: $$D[x;X] = \{ \ h \in \mathbb{R} \setminus \{ 0 \} \ | \ x + h \in X \ \}$$ and for any $\phi: X \to S$ such that $S \subseteq \mathbb{R}$ , I define $\mathrm{Q}^{\phi}_{x} : D[x;X] \to \mathbb{R}$ as: $$\mathrm{Q}^{\phi}_{x}(h) = \frac{\phi(x+h) - \phi(x)}{h}$$ DEFINITION: First I will fix the point $a \in A$ , and then define the derivative of $f$ at $a$ as: $$f'(a) = \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h)$$ REFLECTION: If $f'(a)$ exists, then $\lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h)$ exists. For that to exist, $0\in\mathbb{R}$ has to be an accumulation point of $D[a;A]$ (every punctured neighbourhood of $0\in\mathbb{R}$ contains at least one point from $D[a;A]$ ). From that, it follows that $a\in A$ must be an accumulation point of $A$ . But now I am stuck, I am missing some sort of step or requirement or something, that would allow me to say: $$a_{0} \ \text{is an accumulation point of} \ A \ \ \wedge \ \ \text{something else} \implies \text{there must exist some open interval $I$ so that $a_{0} \in I$ and $I \subseteq A$.}$$ Now, I could always restrict the definition of limits to require that the domain of a function includes within it a punctured neighbourhood of the limit point. But that kind of feels like cheating? What is it that I am missing to make this step? EDIT: I thought maybe some common properties of derivatives cannot be proven with this definition. For example: $$f'(a) = \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) \implies \lim_{ x \to a } f(x) = f(a)$$ But sure enough, if I consider the function $I: D[a;A] \to \mathbb{R}; \ \  I(h) = h$ , I can deduce that $\lim_{ h \to 0 } I(h) = 0$ , so I can use the limit composition to say that: $$ \begin{align}  &\lim_{ h \to 0 } I(h) = 0 \ \ \text{and} \ \ \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) = f'(a) &\implies& \begin{aligned} \lim_{ h \to 0 } \left[ I(h) \cdot \mathrm{Q}^{f}_{a}(h) \right] &= \lim_{ h \to 0 } \left[ h \cdot \frac{f(a+h)-f(a)}{h} \right] \\ &= \lim_{ h \to 0 } \left[ f(a+h)-f(a) \right] \\ &= 0 \cdot f'(a) \\ &= 0 \\ \end{aligned} \\ &&\implies& \lim_{ x \to a } f(x) = a \\ \end{align} $$ In a similar vein, Hans Lundmark suggested that you need extra assumptions to prove the chain rule, but that doesn't seem to be true either (I might be wrong please check my proofðŸ™). For any subsets $A,B \subseteq \mathbb{R}$ , and for any functions $f: A \to \mathbb{R}$ and $g: B \to \mathbb{R}$ such that $f[A] \subseteq B$ , the composite function $g \circ f: A \to \mathbb{R}$ is well defined, which I will denote as $u$ . I will now: Fix the point $a \in A$ and assume that $f'(a) \in \mathbb{R}$ exists. Define $b \in B$ as $b = f(a)$ and assume that $g'(b) \in \mathbb{R}$ exists. Which means that: $$f'(a) = \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) \ \ \text{and} \ \  g'(b) = \lim_{ h \to 0 } \mathrm{Q}^{g}_{b}(h)$$ I define the function $k: D[a;A] \to \mathbb{R}; \ \ k(h) = f(a + h) - f(a)$ ; I know that $\lim_{ h \to 0 } k(h) = 0$ by the proof above. I then define the function $\mathrm{Q}^B: D[a;A] \to \mathbb{R}$ as: $$ \mathrm{Q}^B(h) = \begin{cases} \mathrm{Q}^{g}_{b}(k(h)) & k(h) \ne 0\\ g'(b) & k(h) = 0 \end{cases} $$ We can then deduce the following: $$ \begin{align} &\begin{aligned} \underline{\textbf{We know that:}} &&& \lim_{ h \to 0 } k(h) = 0 &&\iff \forall \varepsilon > 0, \exists \delta > 0, \forall h \in D[a;A]: 0<|h|<\delta \implies |k(h)| < \varepsilon \\ &&& \lim_{ h \to 0 } \mathrm{Q}^{g}_{b}(h) = g'(b) &&\iff \forall \varepsilon > 0, \exists \delta > 0, \forall h \in D[b;B]: 0<|h|<\delta \implies |\mathrm{Q}^{g}_{b}(h) - g'(b)| < \varepsilon \\ \end{aligned} \\ \\ &\begin{aligned} &\text{Take an arbitrary $\epsilon>0$, hence} && \text{$\exists \delta > 0, \forall h \in D[b;B]: 0<|h|<\delta \implies |\mathrm{Q}^{g}_{b}(h) - g'(b)| < \epsilon$} \\ &\text{Take the particular $d>0$ for which this is true, hence} && \text{$\forall h \in D[b;B]: 0<|h|<d \implies |\mathrm{Q}^{g}_{b}(h) - g'(b)| < \epsilon$} \\ &\text{Since $d>0$, we also have}&&\text{$\exists \delta > 0, \forall h \in D[a;A]: 0<|h|<\delta \implies |k(h)| < d$} \\ &\text{Take the particular $\varDelta>0$ for which this is true, hence}&&\text{$\forall h \in D[a;A]: 0<|h|<\varDelta \implies |k(h)| < d$} \\ &\text{Take an arbitrary $H \in D[a;A]$, hence}&&\text{$0<|H|<\varDelta \implies |k(H)| < d$} \end{aligned} \\ \\ &\begin{aligned} \underline{\text{Assume that $0<|H|<\varDelta$:}} &&&\text{Therefore we have $|k(H)|<d$} \\ &&&\text{By law of excluded middle, we have $k(H) = 0$ or $k(H) \ne 0$} \\ &&&\begin{aligned} \underline{\text{When $k(H) = 0$:}} &&&\text{$\mathrm{Q}^{B}(H) = g'(b)$, and therefore $0 = |\mathrm{Q}^{B}(H) - g'(b)| < \epsilon$} \end{aligned} \\ &&&\begin{aligned} \underline{\text{When $k(H) \ne 0$:}} &&&\text{$\mathrm{Q}^{B}(H) = \mathrm{Q}^{g}_{b}(k(H))$, and therefore $k(H) \in B[b;B]$} \\ &&&\text{Hence, we have $0<|k(H)|<d \implies |\mathrm{Q}^{g}_{b}(k(H)) - g'(b)| < \epsilon$} \\ &&&\text{Since $|k(H)| > 0$, we have $0<|k(H)|< d$} \\ &&&\text{By implication, we have $|\mathrm{Q}^{g}_{b}(k(H)) - g'(b)| = |\mathrm{Q}^{B}(H) - g'(b)| < \epsilon$} \end{aligned} \\  &&&\text{By cases, we conclude that $|\mathrm{Q}^{B}(H) - g'(b)| < \epsilon$} \end{aligned} \\ \\ &\begin{aligned} &\text{Therefore, we have} && 0<|H|<\varDelta \implies |\mathrm{Q}^{B}(H) - g'(b)| < \epsilon \\ &\text{$H$ is arbitrary, $\varDelta$ is particular, $\epsilon$ is arbitrary, so we have}&&\text{$\forall \varepsilon > 0, \exists \delta > 0, \forall h \in D[a;A]: 0<|h|<\delta \implies |\mathrm{Q}^{B}(h) - g'(b)| < \varepsilon$} \\ &&& \iff \lim_{ h \to 0 } \mathrm{Q}^{B}(h) = g'(b) \end{aligned} \\ \\ &\begin{aligned} \underline{\textbf{We conclude that:}} &&& \lim_{ h \to 0 } \mathrm{Q}^{B}(h) = g'(b) \end{aligned} \end{align} $$ From here it is rather trivial to show that $\mathrm{Q}^{u}_{a}(h) = \mathrm{Q}^{B}(h) \cdot \mathrm{Q}^{f}_{a}(h)$ , just consider the cases $k(h) \not= 0, k(h) = 0$ in turn. Finally, by limit composition, we have $$ \begin{align}  &\lim_{ h \to 0 } \mathrm{Q}^{B}(h) = g'(b) \ \ \text{and} \ \ \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) = f'(a) &\implies& \begin{aligned} \lim_{ h \to 0 } \left[ \mathrm{Q}^{B}(h) \cdot \mathrm{Q}^{f}_{a}(h) \right] &= \lim_{ h \to 0 } \mathrm{Q}^{u}_{a}(h) \\ &= u'(a) = (g \circ f)'(a) \\ &= g'(b)f'(a) = g'(f(a))f'(a) \end{aligned} \\ &&\implies& u'(a) = g'(f(a))f'(a) \\ \end{align} $$ Hence, the chain rule holds. To the best of my knowledge, I don't think I made use of any additional assumptions to deduce this, beyond the definitions I provided. EDIT 2: As pointed out by Hans Lundmark (thankyou), my assumption that $f[A] \subseteq B$ is a little strong. The most general case in which composition still makes sense, is if we define the following: $A^* = \{ \ x \in A \ | \ f(x) \in B \ \}$ the restricted domain of $f$ for which composition can occur $f_{r} = \{ \ \langle x, y \rangle \in f \ | \ x \in A^* \ \}$ the corresponding restricted $f$ I can now define the composite function $g \circ f: A^* \to \mathbb{R}; \ \ x \mapsto g(f_{r}(x))$ , and as before: $u = g \circ f$ . Fix the point $a \in A^*$ and assume that $f'(a) \in \mathbb{R}$ exists. Define $b \in B^*$ as $b = f(a)$ and assume that $g'(b) \in \mathbb{R}$ exists. That seems to be the most general case for which the concept of function composition can make sense. And so, if I pick $A = \left\{ \ \frac{1}{2x} \ | \ x \in \mathbb{Z} \setminus \{ 0 \}, \frac{x}{3} \not\in \mathbb{Z} \  \right\} \cup \{ 0 \}$ and $B = \left\{ \ \frac{1}{3x} \ | \ x \in \mathbb{Z} \setminus \{ 0 \}, \frac{x}{2} \not\in \mathbb{Z} \  \right\} \cup \{ 0 \}$ and $f(x)=x$ and $g(x) = x$ and $a=0$ , then we see that $f'(0)=1$ and $g'(f(0))=1$ , but $u'(0)$ does not exist. Because $A^*= \{ 0 \}$ , so $0$ is not an accumulation point of $A^*$ . Here is a plot I drew up real quick: The extra assumption needed to make this work, is to require that $a$ is an accumulation point of $A^*$ . So then $f_{r}'(a) = f'(a)$ , and since $f_{r}[A^*] = B^* \subseteq B$ , we can just use the above proof and deduce that $u'(a)=g'(f(a))f'(a)$ . And none of these extra assumptions is needed when you assume the open interval. Thank you Hans Lundmark that one :)","If I have a subset , a function , and a point , then (according to the definitions I came across) for to be differentiable at , then there must exist some open interval so that and . I tried understanding why this requirement exists, by first discarding it and seeing if it naturally arises from the other bits of the definition. NOTATION: For any , I define as: and for any such that , I define as: DEFINITION: First I will fix the point , and then define the derivative of at as: REFLECTION: If exists, then exists. For that to exist, has to be an accumulation point of (every punctured neighbourhood of contains at least one point from ). From that, it follows that must be an accumulation point of . But now I am stuck, I am missing some sort of step or requirement or something, that would allow me to say: Now, I could always restrict the definition of limits to require that the domain of a function includes within it a punctured neighbourhood of the limit point. But that kind of feels like cheating? What is it that I am missing to make this step? EDIT: I thought maybe some common properties of derivatives cannot be proven with this definition. For example: But sure enough, if I consider the function , I can deduce that , so I can use the limit composition to say that: In a similar vein, Hans Lundmark suggested that you need extra assumptions to prove the chain rule, but that doesn't seem to be true either (I might be wrong please check my proofðŸ™). For any subsets , and for any functions and such that , the composite function is well defined, which I will denote as . I will now: Fix the point and assume that exists. Define as and assume that exists. Which means that: I define the function ; I know that by the proof above. I then define the function as: We can then deduce the following: From here it is rather trivial to show that , just consider the cases in turn. Finally, by limit composition, we have Hence, the chain rule holds. To the best of my knowledge, I don't think I made use of any additional assumptions to deduce this, beyond the definitions I provided. EDIT 2: As pointed out by Hans Lundmark (thankyou), my assumption that is a little strong. The most general case in which composition still makes sense, is if we define the following: the restricted domain of for which composition can occur the corresponding restricted I can now define the composite function , and as before: . Fix the point and assume that exists. Define as and assume that exists. That seems to be the most general case for which the concept of function composition can make sense. And so, if I pick and and and and , then we see that and , but does not exist. Because , so is not an accumulation point of . Here is a plot I drew up real quick: The extra assumption needed to make this work, is to require that is an accumulation point of . So then , and since , we can just use the above proof and deduce that . And none of these extra assumptions is needed when you assume the open interval. Thank you Hans Lundmark that one :)","A \subseteq \mathbb{R} f: A \to \mathbb{R} a \in A f a I a \in I I \subseteq A x \in X\subseteq \mathbb{R} D[x;X] \subseteq \mathbb{R} D[x;X] = \{ \ h \in \mathbb{R} \setminus \{ 0 \} \ | \ x + h \in X \ \} \phi: X \to S S \subseteq \mathbb{R} \mathrm{Q}^{\phi}_{x} : D[x;X] \to \mathbb{R} \mathrm{Q}^{\phi}_{x}(h) = \frac{\phi(x+h) - \phi(x)}{h} a \in A f a f'(a) = \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) f'(a) \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) 0\in\mathbb{R} D[a;A] 0\in\mathbb{R} D[a;A] a\in A A a_{0} \ \text{is an accumulation point of} \ A \ \ \wedge \ \ \text{something else} \implies \text{there must exist some open interval I so that a_{0} \in I and I \subseteq A.} f'(a) = \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) \implies \lim_{ x \to a } f(x) = f(a) I: D[a;A] \to \mathbb{R}; \ \  I(h) = h \lim_{ h \to 0 } I(h) = 0 
\begin{align} 
&\lim_{ h \to 0 } I(h) = 0 \ \ \text{and} \ \ \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) = f'(a)
&\implies& \begin{aligned}
\lim_{ h \to 0 } \left[ I(h) \cdot \mathrm{Q}^{f}_{a}(h) \right]
&= \lim_{ h \to 0 } \left[ h \cdot \frac{f(a+h)-f(a)}{h} \right] \\
&= \lim_{ h \to 0 } \left[ f(a+h)-f(a) \right] \\
&= 0 \cdot f'(a) \\
&= 0 \\
\end{aligned} \\
&&\implies& \lim_{ x \to a } f(x) = a \\
\end{align}
 A,B \subseteq \mathbb{R} f: A \to \mathbb{R} g: B \to \mathbb{R} f[A] \subseteq B g \circ f: A \to \mathbb{R} u a \in A f'(a) \in \mathbb{R} b \in B b = f(a) g'(b) \in \mathbb{R} f'(a) = \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) \ \ \text{and} \ \  g'(b) = \lim_{ h \to 0 } \mathrm{Q}^{g}_{b}(h) k: D[a;A] \to \mathbb{R}; \ \ k(h) = f(a + h) - f(a) \lim_{ h \to 0 } k(h) = 0 \mathrm{Q}^B: D[a;A] \to \mathbb{R} 
\mathrm{Q}^B(h) = \begin{cases}
\mathrm{Q}^{g}_{b}(k(h)) & k(h) \ne 0\\
g'(b) & k(h) = 0
\end{cases}
 
\begin{align}
&\begin{aligned}
\underline{\textbf{We know that:}} &&& \lim_{ h \to 0 } k(h) = 0 &&\iff \forall \varepsilon > 0, \exists \delta > 0, \forall h \in D[a;A]: 0<|h|<\delta \implies |k(h)| < \varepsilon \\
&&& \lim_{ h \to 0 } \mathrm{Q}^{g}_{b}(h) = g'(b) &&\iff \forall \varepsilon > 0, \exists \delta > 0, \forall h \in D[b;B]: 0<|h|<\delta \implies |\mathrm{Q}^{g}_{b}(h) - g'(b)| < \varepsilon \\
\end{aligned} \\ \\
&\begin{aligned}
&\text{Take an arbitrary \epsilon>0, hence} && \text{\exists \delta > 0, \forall h \in D[b;B]: 0<|h|<\delta \implies |\mathrm{Q}^{g}_{b}(h) - g'(b)| < \epsilon} \\
&\text{Take the particular d>0 for which this is true, hence} && \text{\forall h \in D[b;B]: 0<|h|<d \implies |\mathrm{Q}^{g}_{b}(h) - g'(b)| < \epsilon} \\
&\text{Since d>0, we also have}&&\text{\exists \delta > 0, \forall h \in D[a;A]: 0<|h|<\delta \implies |k(h)| < d} \\
&\text{Take the particular \varDelta>0 for which this is true, hence}&&\text{\forall h \in D[a;A]: 0<|h|<\varDelta \implies |k(h)| < d} \\
&\text{Take an arbitrary H \in D[a;A], hence}&&\text{0<|H|<\varDelta \implies |k(H)| < d}
\end{aligned} \\ \\
&\begin{aligned}
\underline{\text{Assume that 0<|H|<\varDelta:}}
&&&\text{Therefore we have |k(H)|<d} \\
&&&\text{By law of excluded middle, we have k(H) = 0 or k(H) \ne 0} \\
&&&\begin{aligned}
\underline{\text{When k(H) = 0:}}
&&&\text{\mathrm{Q}^{B}(H) = g'(b), and therefore 0 = |\mathrm{Q}^{B}(H) - g'(b)| < \epsilon}
\end{aligned} \\
&&&\begin{aligned}
\underline{\text{When k(H) \ne 0:}}
&&&\text{\mathrm{Q}^{B}(H) = \mathrm{Q}^{g}_{b}(k(H)), and therefore k(H) \in B[b;B]} \\
&&&\text{Hence, we have 0<|k(H)|<d \implies |\mathrm{Q}^{g}_{b}(k(H)) - g'(b)| < \epsilon} \\
&&&\text{Since |k(H)| > 0, we have 0<|k(H)|< d} \\
&&&\text{By implication, we have |\mathrm{Q}^{g}_{b}(k(H)) - g'(b)| = |\mathrm{Q}^{B}(H) - g'(b)| < \epsilon}
\end{aligned} \\ 
&&&\text{By cases, we conclude that |\mathrm{Q}^{B}(H) - g'(b)| < \epsilon}
\end{aligned} \\ \\
&\begin{aligned}
&\text{Therefore, we have} && 0<|H|<\varDelta \implies |\mathrm{Q}^{B}(H) - g'(b)| < \epsilon \\
&\text{H is arbitrary, \varDelta is particular, \epsilon is arbitrary, so we have}&&\text{\forall \varepsilon > 0, \exists \delta > 0, \forall h \in D[a;A]: 0<|h|<\delta \implies |\mathrm{Q}^{B}(h) - g'(b)| < \varepsilon} \\
&&& \iff \lim_{ h \to 0 } \mathrm{Q}^{B}(h) = g'(b)
\end{aligned} \\ \\
&\begin{aligned}
\underline{\textbf{We conclude that:}} &&& \lim_{ h \to 0 } \mathrm{Q}^{B}(h) = g'(b)
\end{aligned}
\end{align}
 \mathrm{Q}^{u}_{a}(h) = \mathrm{Q}^{B}(h) \cdot \mathrm{Q}^{f}_{a}(h) k(h) \not= 0, k(h) = 0 
\begin{align} 
&\lim_{ h \to 0 } \mathrm{Q}^{B}(h) = g'(b) \ \ \text{and} \ \ \lim_{ h \to 0 } \mathrm{Q}^{f}_{a}(h) = f'(a)
&\implies& \begin{aligned}
\lim_{ h \to 0 } \left[ \mathrm{Q}^{B}(h) \cdot \mathrm{Q}^{f}_{a}(h) \right]
&= \lim_{ h \to 0 } \mathrm{Q}^{u}_{a}(h) \\
&= u'(a) = (g \circ f)'(a) \\
&= g'(b)f'(a) = g'(f(a))f'(a)
\end{aligned} \\
&&\implies& u'(a) = g'(f(a))f'(a) \\
\end{align}
 f[A] \subseteq B A^* = \{ \ x \in A \ | \ f(x) \in B \ \} f f_{r} = \{ \ \langle x, y \rangle \in f \ | \ x \in A^* \ \} f g \circ f: A^* \to \mathbb{R}; \ \ x \mapsto g(f_{r}(x)) u = g \circ f a \in A^* f'(a) \in \mathbb{R} b \in B^* b = f(a) g'(b) \in \mathbb{R} A = \left\{ \ \frac{1}{2x} \ | \ x \in \mathbb{Z} \setminus \{ 0 \}, \frac{x}{3} \not\in \mathbb{Z} \  \right\} \cup \{ 0 \} B = \left\{ \ \frac{1}{3x} \ | \ x \in \mathbb{Z} \setminus \{ 0 \}, \frac{x}{2} \not\in \mathbb{Z} \  \right\} \cup \{ 0 \} f(x)=x g(x) = x a=0 f'(0)=1 g'(f(0))=1 u'(0) A^*= \{ 0 \} 0 A^* a A^* f_{r}'(a) = f'(a) f_{r}[A^*] = B^* \subseteq B u'(a)=g'(f(a))f'(a)","['calculus', 'analysis']"
20,A question on Fekete's subadditive lemma,A question on Fekete's subadditive lemma,,"Fekete's subadditive lemma states that for any sequence $\{a_n\}_{n\geq 1}$ with $a_{m+n}\leq a_m+a_n$ for each $m,n\geq 1$ , the limit $\lim_{n\to\infty}\frac{a_n}{n}$ exists. My question is: if we only know $a_{m+n}\leq a_m+a_n$ for each $m,n\geq t$ (where $t$ is a constant), can we deduce that the limit $\lim_{n\to\infty}\frac{a_n}{n}$ exists?","Fekete's subadditive lemma states that for any sequence with for each , the limit exists. My question is: if we only know for each (where is a constant), can we deduce that the limit exists?","\{a_n\}_{n\geq 1} a_{m+n}\leq a_m+a_n m,n\geq 1 \lim_{n\to\infty}\frac{a_n}{n} a_{m+n}\leq a_m+a_n m,n\geq t t \lim_{n\to\infty}\frac{a_n}{n}","['real-analysis', 'analysis']"
21,Estimate for $\sin(x)/x$ and all its derivatives,Estimate for  and all its derivatives,\sin(x)/x,"Consider the function $f(x) := \frac{\sin x}{x}, f(0) := 1$ . It is easy to see that $$\lvert f(x) \rvert \leq \frac{2}{1+\lvert x \rvert}. $$ I am trying to prove that more generally, for each $k\in \mathbb N$ we have $$\lvert f^{(k)}(x) \rvert \leq \frac{C}{1+\lvert x \rvert}, $$ where $C$ may or may not depend on $k$ , but not on $x$ . From plotting the first few derivatives of $f$ , this seems obvious. For large values of $\lvert x \rvert$ I was able to prove it, but the case where $x$ is near $0$ seems to be more subtle. For example, $$f'(x) = \frac{x\cos x - \sin (x)}{x^2},$$ and the enumerator behaves like $x^3$ near $0$ , so cancels out the singularity. I appreciate any approach for proving the inequality for general $k$ !","Consider the function . It is easy to see that I am trying to prove that more generally, for each we have where may or may not depend on , but not on . From plotting the first few derivatives of , this seems obvious. For large values of I was able to prove it, but the case where is near seems to be more subtle. For example, and the enumerator behaves like near , so cancels out the singularity. I appreciate any approach for proving the inequality for general !","f(x) := \frac{\sin x}{x}, f(0) := 1 \lvert f(x) \rvert \leq \frac{2}{1+\lvert x \rvert}.  k\in \mathbb N \lvert f^{(k)}(x) \rvert \leq \frac{C}{1+\lvert x \rvert},  C k x f \lvert x \rvert x 0 f'(x) = \frac{x\cos x - \sin (x)}{x^2}, x^3 0 k","['real-analysis', 'analysis', 'inequality', 'asymptotics', 'estimation']"
22,"$\sum_{n=1}^\infty 2^{-n} f_n$ has unbounded variation on every $[a,b] \subset [0,1]$ with $a < b$",has unbounded variation on every  with,"\sum_{n=1}^\infty 2^{-n} f_n [a,b] \subset [0,1] a < b","Consider $n\in \Bbb N$ , and define a piecewise linear function $f_n: [0,1] \to \Bbb R$ so that the graph of $f_n$ contains the points $\{(k \cdot 2^{-n}, (-1)^k)\}_{0\le k\le 2^n}$ . Define $f: =\sum_{n=1}^\infty 2^{-n} f_n$ . Since $|f_n| \le 1$ and $\sum_{n\ge 1} 2^{-n} < \infty$ , $f$ is continuous. How would you show that $f$ has unbounded variation on every closed subinterval $[a,b]$ of $[0,1]$ with $a < b$ ? The points $\{k \cdot 2^{-n}\}_{0\le k\le 2^n}$ form a partition of $[0,1]$ for every $n\ge 1$ , so it seems a natural choice for a partition of $[a,b]$ to consider would consist of $\{a,b\}$ and points from $\{k \cdot 2^{-n}\}_{0\le k\le 2^n}$ that fall in the interval $[a,b]$ . P.S. This is what $f_2$ looks like: The plots of $\{f_n\}$ will just have more ""oscillations"" as $n$ increases. Edit: The current answer looks good. However, is there a way to avoid using differentiability a.e. of functions of bounded variation - perhaps directly computing the variation?","Consider , and define a piecewise linear function so that the graph of contains the points . Define . Since and , is continuous. How would you show that has unbounded variation on every closed subinterval of with ? The points form a partition of for every , so it seems a natural choice for a partition of to consider would consist of and points from that fall in the interval . P.S. This is what looks like: The plots of will just have more ""oscillations"" as increases. Edit: The current answer looks good. However, is there a way to avoid using differentiability a.e. of functions of bounded variation - perhaps directly computing the variation?","n\in \Bbb N f_n: [0,1] \to \Bbb R f_n \{(k \cdot 2^{-n}, (-1)^k)\}_{0\le k\le 2^n} f: =\sum_{n=1}^\infty 2^{-n} f_n |f_n| \le 1 \sum_{n\ge 1} 2^{-n} < \infty f f [a,b] [0,1] a < b \{k \cdot 2^{-n}\}_{0\le k\le 2^n} [0,1] n\ge 1 [a,b] \{a,b\} \{k \cdot 2^{-n}\}_{0\le k\le 2^n} [a,b] f_2 \{f_n\} n","['real-analysis', 'analysis', 'bounded-variation']"
23,Closure of product with an particular metric,Closure of product with an particular metric,,"Let $(\mathbb{R}^2,d)$ a metric space, with $d(x,y)=\|x\|+\|y\|$ when $x\not=y$ and $d(x,y)=0$ when $x=y$ . Find the closure of $(0,1)\times(0,1)$ . My intuition says that the closure is $[0,1]\times[0,1]$ but since we are working with that metric, I don't know if the closure changes, and if it doesn't, I don't know how to formally prove that $[0,1]\times[0,1]$ is the closure. I would appreciate any help.","Let a metric space, with when and when . Find the closure of . My intuition says that the closure is but since we are working with that metric, I don't know if the closure changes, and if it doesn't, I don't know how to formally prove that is the closure. I would appreciate any help.","(\mathbb{R}^2,d) d(x,y)=\|x\|+\|y\| x\not=y d(x,y)=0 x=y (0,1)\times(0,1) [0,1]\times[0,1] [0,1]\times[0,1]","['real-analysis', 'analysis', 'metric-spaces', 'normed-spaces', 'euclidean-geometry']"
24,A problem from Applied Asymptotic Analysis by Peter D. Miller (Exercise 1.8),A problem from Applied Asymptotic Analysis by Peter D. Miller (Exercise 1.8),,"transcribed exercise Exercise 1.8. Suppose that $\mu$ is a continuous parameter and that for each $\mu \in[0,1]$ , we have $f(z, \mu)=O(g(z, \mu))$ as $z \rightarrow z_0$ from $D$ . The above proof suggests that it might be true that if the integrals exist in the Riemann sense for all $z$ close enough to $z_0$ , then $$ \int_0^1 f(z, \mu) d \mu=O\left(\int_0^1|g(z, \mu)| d \mu\right) \quad \text { as } z \rightarrow z_0 \text { from } D, $$ since the integrals can be approximated by Riemann sums. Under what additional hypotheses on $f(z, \mu)$ and $g(z, \mu)$ can the proof be adapted to the continuous case? Can you find a counterexample? I am reading Applied Asymptotic Analysis in the series Graduate Studies in Mathematics Volume 75. I'm confused by ex 1.8. I don't think this proposition is true but I can't find a counterexample. However, I find that when $f(z,\mu),g(z,\mu)$ is continuous in $D\times[0,1]$ and $g(z,\mu)\neq0$ ,the statement is true. Any attempt to find a counterexample will be helpful! Here's what the notation means: Definition 1.2 (Big-oh near $\left.z_0\right)$ . Let $f(z)$ and $g(z)$ be two complex-valued functions defined in some set $D$ of the complex plane whose closure contains a point $z_0$ (that is, $z_0$ is a limit point of $D$ ). Then we write $$ f(z)=O(g(z)) \quad \text { as } z \rightarrow z_0 \text { from } D $$ if there is a number $\delta>0$ such that $$ f(z)=O(g(z)), \quad z \in D \text { with } 0<\left|z-z_0\right|<\delta, $$ Miller, Peter D. , Applied asymptotic analysis, Graduate Studies in Mathematics 75. Providence, RI: American Mathematical Society (AMS) (ISBN 0-8218-4078-9/hbk). xv, 467Â p. (2006). ZBL1101.41031 .","transcribed exercise Exercise 1.8. Suppose that is a continuous parameter and that for each , we have as from . The above proof suggests that it might be true that if the integrals exist in the Riemann sense for all close enough to , then since the integrals can be approximated by Riemann sums. Under what additional hypotheses on and can the proof be adapted to the continuous case? Can you find a counterexample? I am reading Applied Asymptotic Analysis in the series Graduate Studies in Mathematics Volume 75. I'm confused by ex 1.8. I don't think this proposition is true but I can't find a counterexample. However, I find that when is continuous in and ,the statement is true. Any attempt to find a counterexample will be helpful! Here's what the notation means: Definition 1.2 (Big-oh near . Let and be two complex-valued functions defined in some set of the complex plane whose closure contains a point (that is, is a limit point of ). Then we write if there is a number such that Miller, Peter D. , Applied asymptotic analysis, Graduate Studies in Mathematics 75. Providence, RI: American Mathematical Society (AMS) (ISBN 0-8218-4078-9/hbk). xv, 467Â p. (2006). ZBL1101.41031 .","\mu \mu \in[0,1] f(z, \mu)=O(g(z, \mu)) z \rightarrow z_0 D z z_0 
\int_0^1 f(z, \mu) d \mu=O\left(\int_0^1|g(z, \mu)| d \mu\right) \quad \text { as } z \rightarrow z_0 \text { from } D,
 f(z, \mu) g(z, \mu) f(z,\mu),g(z,\mu) D\times[0,1] g(z,\mu)\neq0 \left.z_0\right) f(z) g(z) D z_0 z_0 D 
f(z)=O(g(z)) \quad \text { as } z \rightarrow z_0 \text { from } D
 \delta>0 
f(z)=O(g(z)), \quad z \in D \text { with } 0<\left|z-z_0\right|<\delta,
","['real-analysis', 'analysis', 'asymptotics', 'examples-counterexamples', 'riemann-integration']"
25,Find limit of sequence $(x_n)$,Find limit of sequence,(x_n),"Find limit of sequence $(x_n)$ : $$x_1 = a >0$$ $$x_{n+1} = \frac{n}{2n-1}\frac{x_n^2+2}{x_n}, n \in Z^+$$ I think I can prove $(x_n)$ is low bounded (which is obvious that $x_n>0$ ) and decreasing sequence. Then I can calculate the limit of sequence is $\sqrt{2}$ All my attempts to prove it's a decreasing sequence have been unsuccessful. My attemps: Try to prove $x_{n+1}-x_{n} <0$ from a number $N_0$ large enough. It lead to I have to prove $x_n \ge \sqrt{\frac{2n}{n-1}}$ and I stuck. Does anyone have an idea?",Find limit of sequence : I think I can prove is low bounded (which is obvious that ) and decreasing sequence. Then I can calculate the limit of sequence is All my attempts to prove it's a decreasing sequence have been unsuccessful. My attemps: Try to prove from a number large enough. It lead to I have to prove and I stuck. Does anyone have an idea?,"(x_n) x_1 = a >0 x_{n+1} = \frac{n}{2n-1}\frac{x_n^2+2}{x_n}, n \in Z^+ (x_n) x_n>0 \sqrt{2} x_{n+1}-x_{n} <0 N_0 x_n \ge \sqrt{\frac{2n}{n-1}}","['real-analysis', 'sequences-and-series', 'analysis']"
26,"If $xyz = x+y+z$ for $x,y,z > 0$, then $\sqrt{1+x^2} + \sqrt{1+y^2} + \sqrt{1+z^2} \ge 6$","If  for , then","xyz = x+y+z x,y,z > 0 \sqrt{1+x^2} + \sqrt{1+y^2} + \sqrt{1+z^2} \ge 6","If $x,y,z \in \Bbb R_{>0}$ satisfy $xyz = x+y+z$ , prove that $$\sqrt{1+x^2} + \sqrt{1+y^2} + \sqrt{1+z^2} \ge 6$$ We can express $1+x^2$ as $$1+x^2 = (1-xy)(1-xz) = \frac{(y-xyz)(z-xyz)}{yz} = \frac{(x+z)(x+y)}{yz}$$ since $x + y + z = xyz$ implies $1+ x^2 + xy + xz = 1 + x^2yz$ . Thus, our desired inequality boils down to $$\sqrt{\frac{(x+z)(x+y)}{yz}} + \sqrt{\frac{(y+z)(y+x)}{xz}} + \sqrt{\frac{(z+x)(z+y)}{xy}} \ge 6$$ which looks more complicated. Perhaps I am not headed in the right direction? I'd appreciate any hints or solutions. Thank you!","If satisfy , prove that We can express as since implies . Thus, our desired inequality boils down to which looks more complicated. Perhaps I am not headed in the right direction? I'd appreciate any hints or solutions. Thank you!","x,y,z \in \Bbb R_{>0} xyz = x+y+z \sqrt{1+x^2} + \sqrt{1+y^2} + \sqrt{1+z^2} \ge 6 1+x^2 1+x^2 = (1-xy)(1-xz) = \frac{(y-xyz)(z-xyz)}{yz} = \frac{(x+z)(x+y)}{yz} x + y + z = xyz 1+ x^2 + xy + xz = 1 + x^2yz \sqrt{\frac{(x+z)(x+y)}{yz}} + \sqrt{\frac{(y+z)(y+x)}{xz}} + \sqrt{\frac{(z+x)(z+y)}{xy}} \ge 6","['analysis', 'inequality', 'contest-math', 'a.m.-g.m.-inequality']"
27,Reduction of elliptic integrals to Legendre's normal form,Reduction of elliptic integrals to Legendre's normal form,,"Let $P(x)$ be a real polynomial of degree four without multiple roots and suppose that all roots are real. According to my textbook (KÃ¶nigsberger, Analysis 1, first printing, p. 208), there is a rational transformation $x=T(t)=\frac{at+b}{ct+d}$ such that $$ \frac{dx}{\sqrt{P(x)}}=\frac{1}{\sqrt{P(T(t))}}\cdot\frac{ad-bc}{(ct+d)^2}dt=\text{const.}\cdot\frac{dt}{\sqrt{Q(t)}} $$ where $Q(t)=(1-t^2)(1-k^2t^2)$ and $k=\frac{x_1-x_2}{x_1-x_4}\colon\!\frac{x_3-x_2}{x_3-x_4}$ with $x_1<x_2<x_3<x_4$ - the real roots in order. By making the Ansatz $P(T(t))(ct+d)^4=Q(t)$ I was able to determined $a,b,c,d$ , namely $a=\frac{2x_1}{x_1-x_2}-1$ , $b=1$ , $c=\frac{2}{x_1-x_2}$ , $d=0$ , so that $P(T(t))(ct+d)^4$ becomes $$ (1-t^2)(1+(2(x_1-x_3)/(x_1-x_2)-1)t))(1+(2(x_1-x_4)/(x_1-x_2)-1)t) $$ but the last two factors in the last displayed equation do not summarize nicely as $(1-k^2t^2)$ and I suspect that I am doing something wrong although I get something close to the desired DoppelverhÃ¤ltnis expression for $k$ . What is the right fractional linear transformation in terms of $a,b,c,d\in\mathbb{R}$ and especially what is the constant $\text{const.}$ in the first displayed equation? Is it $ad-bc$ as I expected? Any help is appreciated! ---Edit--- You can write $P(x)=a_0(x-x_1)(x-x_2)(x-x_3)(x-x_4)$ , then, because $\sqrt{a_0}$ merges with $\text{const.}$ , we get $$ P(T(t))(ct+d)^4=(at+b-(ct+d)x_1)(at+b-(ct+d)x_2)(at+b-(ct+d)x_3)(at+b-(ct+d)x_4)\,. $$ By comparing coefficients and still changing the constant, we get a system of equations as follows: $$ a-cx_1=-q\,,\,a-cx_2=p\,,\,b-dx_1=q\,,\,b-x_2d=p\,,\,a-cx_3=-kr\,,a-cx_4=ks,b-x_3d=r\,,\,b-x_4d=s $$ with yet to be determined constants $p,q,r,s$ and where $k$ is the DoppelverhÃ¤ltnis of $x_1,x_2,x_3,x_4$ in a, yet to be determined, order. Solving these euqations should be possible and $k$ should be as desired. I tried but I did not come to an end. The values of $a,b,c,d$ above are obviously wrong and arise from a special choice of constants $p,q,r,s$ which is too restrictive.","Let be a real polynomial of degree four without multiple roots and suppose that all roots are real. According to my textbook (KÃ¶nigsberger, Analysis 1, first printing, p. 208), there is a rational transformation such that where and with - the real roots in order. By making the Ansatz I was able to determined , namely , , , , so that becomes but the last two factors in the last displayed equation do not summarize nicely as and I suspect that I am doing something wrong although I get something close to the desired DoppelverhÃ¤ltnis expression for . What is the right fractional linear transformation in terms of and especially what is the constant in the first displayed equation? Is it as I expected? Any help is appreciated! ---Edit--- You can write , then, because merges with , we get By comparing coefficients and still changing the constant, we get a system of equations as follows: with yet to be determined constants and where is the DoppelverhÃ¤ltnis of in a, yet to be determined, order. Solving these euqations should be possible and should be as desired. I tried but I did not come to an end. The values of above are obviously wrong and arise from a special choice of constants which is too restrictive.","P(x) x=T(t)=\frac{at+b}{ct+d} 
\frac{dx}{\sqrt{P(x)}}=\frac{1}{\sqrt{P(T(t))}}\cdot\frac{ad-bc}{(ct+d)^2}dt=\text{const.}\cdot\frac{dt}{\sqrt{Q(t)}}
 Q(t)=(1-t^2)(1-k^2t^2) k=\frac{x_1-x_2}{x_1-x_4}\colon\!\frac{x_3-x_2}{x_3-x_4} x_1<x_2<x_3<x_4 P(T(t))(ct+d)^4=Q(t) a,b,c,d a=\frac{2x_1}{x_1-x_2}-1 b=1 c=\frac{2}{x_1-x_2} d=0 P(T(t))(ct+d)^4 
(1-t^2)(1+(2(x_1-x_3)/(x_1-x_2)-1)t))(1+(2(x_1-x_4)/(x_1-x_2)-1)t)
 (1-k^2t^2) k a,b,c,d\in\mathbb{R} \text{const.} ad-bc P(x)=a_0(x-x_1)(x-x_2)(x-x_3)(x-x_4) \sqrt{a_0} \text{const.} 
P(T(t))(ct+d)^4=(at+b-(ct+d)x_1)(at+b-(ct+d)x_2)(at+b-(ct+d)x_3)(at+b-(ct+d)x_4)\,.
 
a-cx_1=-q\,,\,a-cx_2=p\,,\,b-dx_1=q\,,\,b-x_2d=p\,,\,a-cx_3=-kr\,,a-cx_4=ks,b-x_3d=r\,,\,b-x_4d=s
 p,q,r,s k x_1,x_2,x_3,x_4 k a,b,c,d p,q,r,s","['real-analysis', 'analysis', 'indefinite-integrals', 'mobius-transformation', 'elliptic-integrals']"
28,Closure operators coincide means that the topologies coincide,Closure operators coincide means that the topologies coincide,,"Assume there are two topologies on a single space such that  the closure of any given set in either topology is the same. Is it true that the topologies are also the same? I think they are: assume there is an open set in the first topology that is not open in the second topology. Then, its complement in the first topology is closed, so the complement of the set is equal to its own closure in the first topology, which is equal to the complementâ€™s closure in the second topology. Hence, the complement is closed in the second topology, which means the complement of the complement is open in the second topology. But the complement  of the complement of a set is the set itself. Hence the set is open in the second topology and we have a contradiction. If this is false: what if we impose that the topological be sequential spaces?","Assume there are two topologies on a single space such that  the closure of any given set in either topology is the same. Is it true that the topologies are also the same? I think they are: assume there is an open set in the first topology that is not open in the second topology. Then, its complement in the first topology is closed, so the complement of the set is equal to its own closure in the first topology, which is equal to the complementâ€™s closure in the second topology. Hence, the complement is closed in the second topology, which means the complement of the complement is open in the second topology. But the complement  of the complement of a set is the set itself. Hence the set is open in the second topology and we have a contradiction. If this is false: what if we impose that the topological be sequential spaces?",,"['sequences-and-series', 'general-topology', 'analysis', 'solution-verification', 'metric-spaces']"
29,Importance of Fixed-point theorems [duplicate],Importance of Fixed-point theorems [duplicate],,"This question already has an answer here : What is the role of fixed point theorems in modern mathematics? (1 answer) Closed 2 years ago . I have a more general question on the importance of fixed-point theorems. In mathematics youre being introduced to so many fixed-point theorems but i still could not figure out why they are so important. Why would be a simply looking statement as $f(x)=x$ be so important. I would appreciate any answer. Thanks in advance. On wikipedia it says nothing about the importance, contextualisation of theorems in mathematics is sometimes not given.","This question already has an answer here : What is the role of fixed point theorems in modern mathematics? (1 answer) Closed 2 years ago . I have a more general question on the importance of fixed-point theorems. In mathematics youre being introduced to so many fixed-point theorems but i still could not figure out why they are so important. Why would be a simply looking statement as be so important. I would appreciate any answer. Thanks in advance. On wikipedia it says nothing about the importance, contextualisation of theorems in mathematics is sometimes not given.",f(x)=x,"['calculus', 'general-topology', 'analysis', 'discrete-mathematics', 'fixed-point-theorems']"
30,Proving that $\limsup_{h \to 0^{+}} \frac{F(x + h) - F(x)}{h}$ is measurable if $F$ is continuous,Proving that  is measurable if  is continuous,\limsup_{h \to 0^{+}} \frac{F(x + h) - F(x)}{h} F,"I am trying to write up a detailed proof for the following statement: Let $F: \mathbb{R} \rightarrow \mathbb{R}$ be continuous. Then $$D^{+}(F)(x) := \limsup_{h \to 0^{+}} \frac{F(x+h) - F(x)}{h}$$ is measurable. (Note: This question has been asked a few years ago here , but I would like to solicit feedback on my particular proof. I also have specific questions/concerns about this proof that were not addressed in the linked post. For these reasons, I feel this merited a separate post...) My attempt : To ease notation, let $\Delta_h F(x) := \frac{F(x + h) - F(x)}{h}$ for $h \neq 0$ . By unravelling the definition of $\limsup\limits_{h \to 0^{+}}$ , we have \begin{align*}    D^{+}(F)(x) &= \limsup_{h \to 0^{+}} \Delta_h F(x) \\[5pt]                &= \lim_{\delta \to 0^{+}} \left( \sup\left\{\Delta_h F(x): h \in B_{\delta}(0) \cap (0,\infty) \setminus \{0\} \right\} \right) \\[5pt]                &= \lim_{\delta \to 0^{+}} \sup_{h \in (0,\delta)} \Delta_h F(x). \end{align*} I now claim (with some wariness--see my question below) that \begin{align*}     \hspace{2cm} \lim_{\delta \to 0^{+}} \sup_{h \in (0,\delta)} \Delta_h F(x) = \lim_{\delta \to 0^{+}} \sup_{h \in (0,\delta) \cap \mathbb{Q}} \Delta_h F(x) \hspace{2cm} (\star) \end{align*} Now for each $\delta > 0$ , let $\{h_n^{\delta}\}_{n=1}^{\infty}$ be an enumeration of $(0,\delta) \cap \mathbb{Q}$ . Then we have $$ \sup_{h \in (0,\delta) \cap \mathbb{Q}} \Delta_h F(x) = \sup_{n \in \mathbb{N}} \Delta_{h_n^{\delta}} F(x)$$ for all $x \in \mathbb{R}$ and all $\delta > 0$ . And since $F$ is continuous, the function $x \mapsto \Delta_{h_{n}^{\delta}} F(x) = \frac{F(x + h_n^{\delta}) - F(x)}{h_n^{\delta}}$ is clearly continuous for all $h_n^{\delta}$ , and hence measurable. It follows that $G_{\delta}(x) := \sup_{n \in \mathbb{N}} {\Delta_{h_n^{\delta}}} F(x)$ is measurable because the supremum of a sequence of measurable functions is measurable . Now let $\{\delta_n\}_{n=1}^{\infty}$ be a sequence in $(0,\infty)$ such that $\delta_n \to 0$ . We then have $$ D^{+}(F)(x) = \lim_{\delta \to 0^{+}} G_{\delta}(x) = \lim_{n \to \infty} G_{\delta_n}(x). $$ And so finally, $D^{+}(F)(x)$ is measurable because the pointwise limit of a sequence of measurable functions is measurable. $\quad \square$ My questions : 1.) Is my proof sound? Any issues, major or minor? 2.) Main question: Is $(\star)$ actually true? My concern with regards to $(\star)$ is: Do we know a priori if $\sup_{h \in (0,\delta)} \Delta_h F(x)$ is finite a.e.? If yes, how would we prove it? (More generally, does the definition of measurability even apply to functions which are $\pm \infty$ on a set of positive measure?) In the case where $\sup_{h \in (0,\delta)} \Delta_h F(x)$ is assumed to be bounded on $(0,\delta)$ (for some $\delta > 0$ ), my argument for $(\star)$ is based on the following (more generalized) claim: Claim : Let $(S,d)$ be a metric space and let $E$ be a dense subset of $S$ . If $f: S \rightarrow \mathbb{R}$ is continuous, then $$ \sup_{x \in S} f(x) = \sup_{x \in E} f(x).$$ Proof : Let $\alpha := \sup_{x \in S} f(x)$ and $\alpha' := \sup_{x \in E} f(x)$ . Clearly, $\alpha' \leq \alpha$ , so we just need to show the reverse inequality. Fix $\epsilon > 0$ . Then there is some $x_0 \in S$ such that $f(x_0) > \alpha - \frac{\epsilon}{2}$ . Then since $f$ is continuous at $x_0$ , there exists some $\delta > 0$ such that \begin{align*}    x \in N_{\delta}(x_0) &\implies |f(x_0) - f(x)| < \frac{\epsilon}{2}  \\[3pt]                          &\implies -\frac{\epsilon}{2} < f(x_0) - f(x) <  \frac{\epsilon}{2} \\[3pt] &\implies f(x) > f(x_0) - \frac{\epsilon}{2} > \left(\alpha - \frac{\epsilon}{2}\right) - \frac{\epsilon}{2} = \alpha - \epsilon. \end{align*} Then since $E$ is a dense subset of $S$ , there exists some $x_1 \in E \cap N_{\delta}(x_0)$ . Hence, $\alpha' \geq f(x_1) > \alpha - \epsilon$ , i.e. $\alpha' > \alpha - \epsilon$ .  And since $\epsilon > 0$ was arbitrary, it follows that $\alpha' \geq \alpha. \quad \square$ Finally: Does this claim & proof look sound?","I am trying to write up a detailed proof for the following statement: Let be continuous. Then is measurable. (Note: This question has been asked a few years ago here , but I would like to solicit feedback on my particular proof. I also have specific questions/concerns about this proof that were not addressed in the linked post. For these reasons, I feel this merited a separate post...) My attempt : To ease notation, let for . By unravelling the definition of , we have I now claim (with some wariness--see my question below) that Now for each , let be an enumeration of . Then we have for all and all . And since is continuous, the function is clearly continuous for all , and hence measurable. It follows that is measurable because the supremum of a sequence of measurable functions is measurable . Now let be a sequence in such that . We then have And so finally, is measurable because the pointwise limit of a sequence of measurable functions is measurable. My questions : 1.) Is my proof sound? Any issues, major or minor? 2.) Main question: Is actually true? My concern with regards to is: Do we know a priori if is finite a.e.? If yes, how would we prove it? (More generally, does the definition of measurability even apply to functions which are on a set of positive measure?) In the case where is assumed to be bounded on (for some ), my argument for is based on the following (more generalized) claim: Claim : Let be a metric space and let be a dense subset of . If is continuous, then Proof : Let and . Clearly, , so we just need to show the reverse inequality. Fix . Then there is some such that . Then since is continuous at , there exists some such that Then since is a dense subset of , there exists some . Hence, , i.e. .  And since was arbitrary, it follows that Finally: Does this claim & proof look sound?","F: \mathbb{R} \rightarrow \mathbb{R} D^{+}(F)(x) := \limsup_{h \to 0^{+}} \frac{F(x+h) - F(x)}{h} \Delta_h F(x) := \frac{F(x + h) - F(x)}{h} h \neq 0 \limsup\limits_{h \to 0^{+}} \begin{align*}
   D^{+}(F)(x) &= \limsup_{h \to 0^{+}} \Delta_h F(x) \\[5pt]
               &= \lim_{\delta \to 0^{+}} \left( \sup\left\{\Delta_h F(x): h \in B_{\delta}(0) \cap (0,\infty) \setminus \{0\} \right\} \right) \\[5pt]
               &= \lim_{\delta \to 0^{+}} \sup_{h \in (0,\delta)} \Delta_h F(x).
\end{align*} \begin{align*}
    \hspace{2cm} \lim_{\delta \to 0^{+}} \sup_{h \in (0,\delta)} \Delta_h F(x) = \lim_{\delta \to 0^{+}} \sup_{h \in (0,\delta) \cap \mathbb{Q}} \Delta_h F(x) \hspace{2cm} (\star)
\end{align*} \delta > 0 \{h_n^{\delta}\}_{n=1}^{\infty} (0,\delta) \cap \mathbb{Q}  \sup_{h \in (0,\delta) \cap \mathbb{Q}} \Delta_h F(x) = \sup_{n \in \mathbb{N}} \Delta_{h_n^{\delta}} F(x) x \in \mathbb{R} \delta > 0 F x \mapsto \Delta_{h_{n}^{\delta}} F(x) = \frac{F(x + h_n^{\delta}) - F(x)}{h_n^{\delta}} h_n^{\delta} G_{\delta}(x) := \sup_{n \in \mathbb{N}} {\Delta_{h_n^{\delta}}} F(x) \{\delta_n\}_{n=1}^{\infty} (0,\infty) \delta_n \to 0  D^{+}(F)(x) = \lim_{\delta \to 0^{+}} G_{\delta}(x) = \lim_{n \to \infty} G_{\delta_n}(x).  D^{+}(F)(x) \quad \square (\star) (\star) \sup_{h \in (0,\delta)} \Delta_h F(x) \pm \infty \sup_{h \in (0,\delta)} \Delta_h F(x) (0,\delta) \delta > 0 (\star) (S,d) E S f: S \rightarrow \mathbb{R}  \sup_{x \in S} f(x) = \sup_{x \in E} f(x). \alpha := \sup_{x \in S} f(x) \alpha' := \sup_{x \in E} f(x) \alpha' \leq \alpha \epsilon > 0 x_0 \in S f(x_0) > \alpha - \frac{\epsilon}{2} f x_0 \delta > 0 \begin{align*}
   x \in N_{\delta}(x_0) &\implies |f(x_0) - f(x)| < \frac{\epsilon}{2}  \\[3pt]
                         &\implies -\frac{\epsilon}{2} < f(x_0) - f(x) <  \frac{\epsilon}{2} \\[3pt]
&\implies f(x) > f(x_0) - \frac{\epsilon}{2} > \left(\alpha - \frac{\epsilon}{2}\right) - \frac{\epsilon}{2} = \alpha - \epsilon.
\end{align*} E S x_1 \in E \cap N_{\delta}(x_0) \alpha' \geq f(x_1) > \alpha - \epsilon \alpha' > \alpha - \epsilon \epsilon > 0 \alpha' \geq \alpha. \quad \square","['real-analysis', 'analysis', 'measure-theory', 'derivatives', 'limsup-and-liminf']"
31,Mistake in proof of Lemma 2.3 in Chapter 3 of Stein and Shakarchi's Fourier Analysis,Mistake in proof of Lemma 2.3 in Chapter 3 of Stein and Shakarchi's Fourier Analysis,,"I'm reading Fourier Analysis by Stein and Shakarchi and it seems to me that there's a mistake in the proof of Lemma 2.3 in Chapter 3. I wonder if there's something I'm missing or if someone could suggest an alternative proof. Any insight would be appreciated! Lemma 2.3 states: ""Suppose that the Abel means $A_r = \sum_{n = 1}^\infty r^n c_n$ of the series $\sum_{n=1}^\infty c_n$ are bounded as $r$ tends to 1 (with $r < 1$ ). If $c_n = O (1/n)$ , then the partial sums $S_N = \sum_{n = 1}^N c_n$ are bounded."" The proof is as follows. Let $r = 1 - 1/N$ and choose $M$ so that $n|c_n| \leq M$ . We estimate the difference $S_N - A_r = \sum_{n = 1}^N (c_n - r^n c_n) - \sum_{n = N+1}^\infty r^n c_n$ as follows: $|S_N - A_r| \leq \sum_{n = 1}^N |c_n| (1-r^n) + \sum_{n= N+1}^\infty r^n |c_n| \leq M \sum_{n=1}^N (1-r) + \frac{M}{N} \sum_{n = N+1}^\infty r^n \leq MN(1-r) + \frac{M}{N} \frac{1}{1-r} = 2M$ , where we have used the simple observation that $1-r^n = (1-r)(1+r+\cdots + r^{n-1}) \leq n(1-r)$ . So we see that if $M$ satisfies both $|A_r| \leq M$ and $n|c_n| \leq M$ , then $|S_N| \leq 3M$ . The mistake I notice here is in the line where the authors claim that $MN(1-r) + \frac{M}{N} \frac{1}{1-r} = 2M$ . By my calculations, using their assumption that $r = 1 - 1/N$ , we instead have the following: $MN(1-r) + \frac{M}{N} \frac{1}{1-r} = M(N-1) + \frac{M}{N-1} = M(\frac{(N-1)^2 + 1}{N-1}) = M \frac{N^2 - 2N + 2}{N-1} = M \frac{(N-1)(N-2)}{N-1} = M(N-2)$ . This clearly isn't bounded. Thank you again! Any insight would be appreciated!","I'm reading Fourier Analysis by Stein and Shakarchi and it seems to me that there's a mistake in the proof of Lemma 2.3 in Chapter 3. I wonder if there's something I'm missing or if someone could suggest an alternative proof. Any insight would be appreciated! Lemma 2.3 states: ""Suppose that the Abel means of the series are bounded as tends to 1 (with ). If , then the partial sums are bounded."" The proof is as follows. Let and choose so that . We estimate the difference as follows: , where we have used the simple observation that . So we see that if satisfies both and , then . The mistake I notice here is in the line where the authors claim that . By my calculations, using their assumption that , we instead have the following: . This clearly isn't bounded. Thank you again! Any insight would be appreciated!","A_r = \sum_{n = 1}^\infty r^n c_n \sum_{n=1}^\infty c_n r r < 1 c_n = O
(1/n) S_N = \sum_{n = 1}^N c_n r = 1 - 1/N M n|c_n| \leq M S_N - A_r = \sum_{n = 1}^N (c_n - r^n c_n) - \sum_{n = N+1}^\infty r^n c_n |S_N - A_r| \leq \sum_{n = 1}^N |c_n| (1-r^n) + \sum_{n= N+1}^\infty r^n |c_n| \leq M \sum_{n=1}^N (1-r) + \frac{M}{N} \sum_{n = N+1}^\infty r^n \leq MN(1-r) + \frac{M}{N} \frac{1}{1-r} = 2M 1-r^n = (1-r)(1+r+\cdots + r^{n-1}) \leq n(1-r) M |A_r| \leq M n|c_n| \leq M |S_N| \leq 3M MN(1-r) + \frac{M}{N} \frac{1}{1-r} = 2M r = 1 - 1/N MN(1-r) + \frac{M}{N} \frac{1}{1-r} = M(N-1) + \frac{M}{N-1} = M(\frac{(N-1)^2 + 1}{N-1}) = M \frac{N^2 - 2N + 2}{N-1} = M \frac{(N-1)(N-2)}{N-1} = M(N-2)","['real-analysis', 'analysis', 'fourier-analysis', 'fourier-series']"
32,Fractional Part integral $I=\int_{0}^{1}\int_{0}^{1} \left\{\frac{x}{y}\right\}\mathrm{d}x\mathrm{d}y$,Fractional Part integral,I=\int_{0}^{1}\int_{0}^{1} \left\{\frac{x}{y}\right\}\mathrm{d}x\mathrm{d}y,"Let $$I=\int_{0}^{1}\int_{0}^{1} \left\{\frac{x}{y}\right\}\mathrm{d}x\mathrm{d}y.$$ When I tried computing the integral I seem to be getting a different answer to Wolramaplha, and can't find a similar integral anywhere on MSE or the internet. Here's how I did it $$\int_{0}^{1}\left(\int_{0}^{1} \left\{\frac{x}{y}\right\}\mathrm{d}y\right)\mathrm{d}x$$ , Lets evaluate, (Note: $0<x<1$ ) $$I_1=\int_{0}^{1} \left\{\frac{x}{y}\right\}\mathrm{d}y =\int_{1}^{\infty}\frac{xy-\lfloor xy \rfloor  }{y^2}\mathrm{d}y=x(1-\gamma)$$ Therefore $I=\frac{(1-\gamma)}{2}$ . However Wolfram gives $I= 0.458868$ . Can someone help, or if my answer is wrong then provide a solution.","Let When I tried computing the integral I seem to be getting a different answer to Wolramaplha, and can't find a similar integral anywhere on MSE or the internet. Here's how I did it , Lets evaluate, (Note: ) Therefore . However Wolfram gives . Can someone help, or if my answer is wrong then provide a solution.",I=\int_{0}^{1}\int_{0}^{1} \left\{\frac{x}{y}\right\}\mathrm{d}x\mathrm{d}y. \int_{0}^{1}\left(\int_{0}^{1} \left\{\frac{x}{y}\right\}\mathrm{d}y\right)\mathrm{d}x 0<x<1 I_1=\int_{0}^{1} \left\{\frac{x}{y}\right\}\mathrm{d}y =\int_{1}^{\infty}\frac{xy-\lfloor xy \rfloor  }{y^2}\mathrm{d}y=x(1-\gamma) I=\frac{(1-\gamma)}{2} I= 0.458868,['analysis']
33,"$\int_0^1 f(x)e^{nx} \, dx=0$ for all $n \in \Bbb N\cup{\{0\}}$ implies $f(x) = 0$ [closed]",for all  implies  [closed],"\int_0^1 f(x)e^{nx} \, dx=0 n \in \Bbb N\cup{\{0\}} f(x) = 0","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question $f$ is a continuous real valued function $f: [0,1] \to\mathbb R$ and $\int_0^1 f(x)e^{nx} \, dx=0$ $\forall n \in N\cup{\{0\}} \implies f(x) = 0$ on the interval $[0,1]$ I am trying to prove this result. I have tried using Stone-Weirestrass with both polynomials and exponential functions; however, I cannot seem to prove it this way. Can someone offer a hint or suggestion.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question is a continuous real valued function and on the interval I am trying to prove this result. I have tried using Stone-Weirestrass with both polynomials and exponential functions; however, I cannot seem to prove it this way. Can someone offer a hint or suggestion.","f f: [0,1] \to\mathbb R \int_0^1 f(x)e^{nx} \, dx=0 \forall n \in N\cup{\{0\}} \implies f(x) = 0 [0,1]","['real-analysis', 'analysis', 'weierstrass-approximation']"
34,"If $a_n \neq 0$ for all natural numbers n , $\sum_{n=1}^{+\infty} a_n $ converges $\lim_{n\to+\infty} a_n/b_n =1\sum_{n=1}^{+\infty} b_n$ converges [closed]","If  for all natural numbers n ,  converges  converges [closed]",a_n \neq 0 \sum_{n=1}^{+\infty} a_n  \lim_{n\to+\infty} a_n/b_n =1\sum_{n=1}^{+\infty} b_n,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If $a_n \neq 0$ for all natural numbers $n$ , $\sum_{n=1}^{+\infty} a_n $ converges $\lim_{n\to+\infty} \frac{a_n}{b_n} =1 $ then $\sum_{n=1}^{+\infty} b_n$ converges I am trying to find counterexamples to this proposition but I'm unable to do so, is it likely that it is true? Thank you for the help.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question If for all natural numbers , converges then converges I am trying to find counterexamples to this proposition but I'm unable to do so, is it likely that it is true? Thank you for the help.",a_n \neq 0 n \sum_{n=1}^{+\infty} a_n  \lim_{n\to+\infty} \frac{a_n}{b_n} =1  \sum_{n=1}^{+\infty} b_n,"['real-analysis', 'calculus', 'sequences-and-series', 'analysis']"
35,Taylor's theorem and estimating $|f'(x)|$ using $f$ and $f''$,Taylor's theorem and estimating  using  and,|f'(x)| f f'',"I found a different solution to a problem about Taylor theorem, which is differ from the one introduced in my textbook. And I guess my solution is simpler than the one on the textbook, so I want to check whether there is a flaw on my work. The problem is: A function $f:[0, 2]\to \Bbb R$ is twice differentiable, $|f(x)|\leq 1$ and $|f''(x)|<1$ for all $x\in[0, 2]$ . Show that $|f'(x)|<2$ for all $x\in [0, 2]$ . And this is my work: For any $x\in [0, 2]$ and $y(\neq x)\in [0, 2]$ , by Taylor theorem, $$f(y)=f(x)+f'(x)(y-x)+\frac{f''(c)}{2}(y-x)^2$$ for some $c$ between $x$ and $y$ . By hypothesis, $|f(x)|\leq1, \, |f(y)|\leq 1$ and $|f''(c)|<1$ and hence $$\begin{align} |f'(x)|&=\left|\frac{f(y)-f(x)}{y-x}-\frac{f''(c)}{2}(y-x)\right|\\[.4em] &\leq |f(y)-f(x)|\cdot \frac{1}{|y-x|}+\frac{|f''(c)|}{2}|y-x| \\[.4em] &<\frac{2}{|y-x|}+\frac{|y-x|}{2} \end{align}$$ From AM-GM inequality the latter one has the minimum value $2$ when $|y-x|=1$ , and such $y$ exists for every $x\in [0, 2]$ thus $$|f'(x)|<\min\left\{\frac{2}{|y-x|}+\frac{|y-x|}{2}: x,\, y\in [0, 2], \; x\neq y\right\}=2,$$ which shows the requried result. â–  Thanks.","I found a different solution to a problem about Taylor theorem, which is differ from the one introduced in my textbook. And I guess my solution is simpler than the one on the textbook, so I want to check whether there is a flaw on my work. The problem is: A function is twice differentiable, and for all . Show that for all . And this is my work: For any and , by Taylor theorem, for some between and . By hypothesis, and and hence From AM-GM inequality the latter one has the minimum value when , and such exists for every thus which shows the requried result. â–  Thanks.","f:[0, 2]\to \Bbb R |f(x)|\leq 1 |f''(x)|<1 x\in[0, 2] |f'(x)|<2 x\in [0, 2] x\in [0, 2] y(\neq x)\in [0, 2] f(y)=f(x)+f'(x)(y-x)+\frac{f''(c)}{2}(y-x)^2 c x y |f(x)|\leq1, \, |f(y)|\leq 1 |f''(c)|<1 \begin{align} |f'(x)|&=\left|\frac{f(y)-f(x)}{y-x}-\frac{f''(c)}{2}(y-x)\right|\\[.4em] &\leq |f(y)-f(x)|\cdot \frac{1}{|y-x|}+\frac{|f''(c)|}{2}|y-x| \\[.4em] &<\frac{2}{|y-x|}+\frac{|y-x|}{2} \end{align} 2 |y-x|=1 y x\in [0, 2] |f'(x)|<\min\left\{\frac{2}{|y-x|}+\frac{|y-x|}{2}: x,\, y\in [0, 2], \; x\neq y\right\}=2,","['real-analysis', 'analysis', 'taylor-expansion']"
36,$L^1$ Approximation of BV functions by piecewise constant function,Approximation of BV functions by piecewise constant function,L^1,"Let $f \in BV(\mathbb{R}) \cap L^1(\mathbb{R}),$ consider the piecewise constant function  defined by \begin{eqnarray} f^{\delta}(x):= \frac{1}{\delta}\int\limits_{k\delta}^{(k+1)\delta}f(y)dy \quad \text{for } x\in [{k\delta},{(k+1)\delta}) \end{eqnarray} Then clearly $f^{\delta} \in L^{1}(\mathbb{R})$ and $f^{\delta} \rightarrow f$ pointwise a.e.in $\mathbb{R}.$ Now, my doubt is ""Do we have $\|f^{\delta}-f\|_{L^1(\mathbb{R})}\leq C \delta$ for some $C$ independent of $\delta?$ "" If so how to prove it?","Let consider the piecewise constant function  defined by Then clearly and pointwise a.e.in Now, my doubt is ""Do we have for some independent of "" If so how to prove it?","f \in BV(\mathbb{R}) \cap L^1(\mathbb{R}), \begin{eqnarray}
f^{\delta}(x):= \frac{1}{\delta}\int\limits_{k\delta}^{(k+1)\delta}f(y)dy \quad \text{for } x\in [{k\delta},{(k+1)\delta})
\end{eqnarray} f^{\delta} \in L^{1}(\mathbb{R}) f^{\delta} \rightarrow f \mathbb{R}. \|f^{\delta}-f\|_{L^1(\mathbb{R})}\leq C \delta C \delta?","['real-analysis', 'analysis', 'measure-theory', 'bounded-variation']"
37,Show that series $\sum_{n=2}^{\infty} \frac{1}{(\log(n))^{\log(\log(n))}}$ is divergent [duplicate],Show that series  is divergent [duplicate],\sum_{n=2}^{\infty} \frac{1}{(\log(n))^{\log(\log(n))}},This question already has an answer here : Does $\sum_{n=3}^\infty \frac {1}{(\log n)^{\log(\log(n)}}$ converge? (1 answer) Closed 3 years ago . Show that series $$\sum_{n=2}^{\infty} \frac{1}{(\log(n))^{\log(\log(n))}}$$ is divergent . What inequality can we use here. i tried various method but none of these give any result. Any hint please .,This question already has an answer here : Does $\sum_{n=3}^\infty \frac {1}{(\log n)^{\log(\log(n)}}$ converge? (1 answer) Closed 3 years ago . Show that series is divergent . What inequality can we use here. i tried various method but none of these give any result. Any hint please .,\sum_{n=2}^{\infty} \frac{1}{(\log(n))^{\log(\log(n))}},"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
38,Continuous functions and open sets/covers,Continuous functions and open sets/covers,,"I'm taking a course in elementary algebraic geometry, but I seem to be lacking a topological background. The following result is often used: Let $X,Y$ be topological Spaces, and $U_i$ for $i \in I$ an open cover for $X$ (the convention here is that $\bigcup_{i \in I} U_i = X$ ). Then, a function $f: X \to Y$ is continuous if and only if $f|_{U_i}: U_i \to Y$ is continuous for $\forall i$ where $U_i$ is endowed with the subspace topology. How does one prove this? I've not seen this lemma before. (Apparently it's called 'local property of continuity'?)","I'm taking a course in elementary algebraic geometry, but I seem to be lacking a topological background. The following result is often used: Let be topological Spaces, and for an open cover for (the convention here is that ). Then, a function is continuous if and only if is continuous for where is endowed with the subspace topology. How does one prove this? I've not seen this lemma before. (Apparently it's called 'local property of continuity'?)","X,Y U_i i \in I X \bigcup_{i \in I} U_i = X f: X \to Y f|_{U_i}: U_i \to Y \forall i U_i",['analysis']
39,"$f : \mathbb{R} \to \mathbb{R}$ is continuous iff $\{(x,y):y\lt f(x)\}$ and $\{(x,y):y\gt f(x)\}$ are both open sets in $\mathbb R^2$",is continuous iff  and  are both open sets in,"f : \mathbb{R} \to \mathbb{R} \{(x,y):y\lt f(x)\} \{(x,y):y\gt f(x)\} \mathbb R^2","Given $f: \mathbb{R} \to \mathbb{R}$ define two sets $A$ and $B$ as $ A = \{(x, y) : y < f(x)\}$ and $B = \{ (x, y) : y > f(x)\}$ in $\mathbb{R^2}$ . Show that $f$ is continuous iff  both $A$ and $B$ are open. I'm not sure if this follows from the definition of continuity. Can someone help?",Given define two sets and as and in . Show that is continuous iff  both and are open. I'm not sure if this follows from the definition of continuity. Can someone help?,"f: \mathbb{R} \to \mathbb{R} A B  A = \{(x, y) : y < f(x)\} B = \{ (x, y) : y > f(x)\} \mathbb{R^2} f A B","['real-analysis', 'analysis']"
40,"Proving or disproving: If $0<a<b<1$, then $(1-a)^b>(1-b)^a$","Proving or disproving: If , then",0<a<b<1 (1-a)^b>(1-b)^a,"Prove or disprove If $0<a<b<1$ , then $$(1-a)^b>(1-b)^a$$ I think this looks true when evaluating the differential equation $\frac{dy}{dx}=-y$ with initial condition $y(0)=1$ using euler method As an example with step size a=0.2 and b=0.3 with step size $a=0.2$ Evaluate the value of $y$ , 3 times by Euler method with step size a=0.2 will get $y_{a3}=1(1-0.2)^3=1(1-0.2)^{10(0.3)}$ While using step size of $0.3$ and repeat 2 times $y_{b2}=1(1-0.3)^2=1(1-0.3)^{10(0.2)}$ Plotting this in (x,y) axis, one can compare at $x=0.6 ,y_{a3}>y_{b2}$ . I tried with different value of a and b, the inequality looks true as saying Euler method with smaller step size will be the upper boundary for bigger step size for this differential equation. Which also mean the exact solution of this exponential function is the uppest boundary for all step size bigger than dx. I tried binomial expansion, but it only makes it much complicated. $(1-a)^b=1-ab+\frac{b(b-1)}{2!}(-a)^2+..$ $(1-b)^a=1-ab+\frac{a(a-1)}{2!}(-b)^2+..$ I can only show for the 3rd term $\frac{b(b-1)}{2!}(-a)^2>\frac{a(a-1)}{2!}(-b)^2$ since $b>a$ I do not found any way to proof nth term of $(1-a)^b$ will always bigger than $(1-b)^a$ , this is where i stuck.","Prove or disprove If , then I think this looks true when evaluating the differential equation with initial condition using euler method As an example with step size a=0.2 and b=0.3 with step size Evaluate the value of , 3 times by Euler method with step size a=0.2 will get While using step size of and repeat 2 times Plotting this in (x,y) axis, one can compare at . I tried with different value of a and b, the inequality looks true as saying Euler method with smaller step size will be the upper boundary for bigger step size for this differential equation. Which also mean the exact solution of this exponential function is the uppest boundary for all step size bigger than dx. I tried binomial expansion, but it only makes it much complicated. I can only show for the 3rd term since I do not found any way to proof nth term of will always bigger than , this is where i stuck.","0<a<b<1 (1-a)^b>(1-b)^a \frac{dy}{dx}=-y y(0)=1 a=0.2 y y_{a3}=1(1-0.2)^3=1(1-0.2)^{10(0.3)} 0.3 y_{b2}=1(1-0.3)^2=1(1-0.3)^{10(0.2)} x=0.6 ,y_{a3}>y_{b2} (1-a)^b=1-ab+\frac{b(b-1)}{2!}(-a)^2+.. (1-b)^a=1-ab+\frac{a(a-1)}{2!}(-b)^2+.. \frac{b(b-1)}{2!}(-a)^2>\frac{a(a-1)}{2!}(-b)^2 b>a (1-a)^b (1-b)^a","['analysis', 'functions', 'inequality', 'logarithms', 'exponential-function']"
41,Prove or disprove the recursively defined sequence is convergent.,Prove or disprove the recursively defined sequence is convergent.,,"The sequence $\{a_n\}$ is defined by $a_1=1, a_2=0$ and $a_{n+2}=a_{n+1}+\displaystyle\frac{a_n}{n^2}$ for $n\in \mathbb{N}$ . Since $\displaystyle\frac{1}{n^2}$ is summable, when $n$ is large, the sequence is something like $a_n=a_{n-1}+\displaystyle\sum_{i\leq n-2}\frac{a_i}{i^2}$ , so I think the sequence should be convergent. Then I want to use the Monotone convergent theorem, i.e. to show $\{a_n\}$ is monotonic and bounded. For monotonic, it is easy to see that $\{a_n\}$ is increasing. But for the upper bound, assuming $\{a_n\}$ converges and taking the limit $n\to \infty$ does not give any hints for me to find a suitable upper bound. I have also used computer programs to compute up to the 10000th term, but it seems that $\{a_n\}$ is still increasing, does not converges to a certain number. So I wonder if it is convergent or not.","The sequence is defined by and for . Since is summable, when is large, the sequence is something like , so I think the sequence should be convergent. Then I want to use the Monotone convergent theorem, i.e. to show is monotonic and bounded. For monotonic, it is easy to see that is increasing. But for the upper bound, assuming converges and taking the limit does not give any hints for me to find a suitable upper bound. I have also used computer programs to compute up to the 10000th term, but it seems that is still increasing, does not converges to a certain number. So I wonder if it is convergent or not.","\{a_n\} a_1=1, a_2=0 a_{n+2}=a_{n+1}+\displaystyle\frac{a_n}{n^2} n\in \mathbb{N} \displaystyle\frac{1}{n^2} n a_n=a_{n-1}+\displaystyle\sum_{i\leq n-2}\frac{a_i}{i^2} \{a_n\} \{a_n\} \{a_n\} n\to \infty \{a_n\}","['analysis', 'convergence-divergence', 'recurrence-relations']"
42,Are there sets where it cannot possibly have a metric on it?,Are there sets where it cannot possibly have a metric on it?,,"To avoid any ambiguity, a metric space, by definition, is a set $X$ with a distance function $d$ such that $d$ satisfies positivity, symmetry property and triangle inequality. I was wondering does there exist a set where there cannot possibly be equipped with a distance function? In other words this set cannot possibly be made into a metric space? I hope I explained my question sufficiently clear and apologies in advance if this question was not clear. Many thanks in advance!","To avoid any ambiguity, a metric space, by definition, is a set with a distance function such that satisfies positivity, symmetry property and triangle inequality. I was wondering does there exist a set where there cannot possibly be equipped with a distance function? In other words this set cannot possibly be made into a metric space? I hope I explained my question sufficiently clear and apologies in advance if this question was not clear. Many thanks in advance!",X d d,"['analysis', 'metric-spaces', 'examples-counterexamples']"
43,Topology induced by a norm,Topology induced by a norm,,"I came across the notion of a $\textit{topology induced by a norm}$ . If $(X,\Vert\ . \Vert)$ is a normed space w.r.t a norm $\Vert\ . \Vert: X \to \mathbb{R}$ . Most sources define the topology $\tau$ on $X$ induced by $\Vert\ . \Vert$ as the sets $U \subset X$ open w.r.t. the metric $d: X \times X \to \mathbb{R}$ given by $d(x,y) = \Vert x - y \Vert$ . But would I be correct in assuming that an equivalent definition would be $\tau = \{\Vert\ . \Vert^{-1}(U) \mid U \subset \mathbb{R}\ \textrm{open} \}$ ?",I came across the notion of a . If is a normed space w.r.t a norm . Most sources define the topology on induced by as the sets open w.r.t. the metric given by . But would I be correct in assuming that an equivalent definition would be ?,"\textit{topology induced by a norm} (X,\Vert\ . \Vert) \Vert\ . \Vert: X \to \mathbb{R} \tau X \Vert\ . \Vert U \subset X d: X \times X \to \mathbb{R} d(x,y) = \Vert x - y \Vert \tau = \{\Vert\ . \Vert^{-1}(U) \mid U \subset \mathbb{R}\ \textrm{open} \}","['real-analysis', 'general-topology', 'analysis', 'normed-spaces']"
44,What kind of field does define exp function?,What kind of field does define exp function?,,"In Haskell programming language, Fractional typeclass effectively represents a normed field, and has Floating as a sub-typeclass. Floating is to define $\exp$ and trigonometric functions (and related constants and functions). ""Types"" that satisfies Floating include $â„$ and $â„‚$ . I wonder there would be other field that defines $\exp$ . I would define $\exp$ as Taylor series: $$ \exp x = \sum_{n=0}^\infty \frac{x^n}{n!} $$ In a field, since $n \in â„¤$ , $x^n$ is well-defined, division is well-defined, and if integers are defined as repeated addition of unity, $n!$ is well-defined. The only ways to make $\exp x$ undefined would be divergence of the series, or a division by zero. Convergence and divergence is well-defined because the field is normed, making the $\epsilon$ -definition of limit of number sequence applicable. As a consequence, a notable non-example would be $\text{GF}(p^k)$ because $p â‰¡ 0$ , hence $p! â‰¡ 0$ , resulting in a division by zero. Is there an example rather than $â„$ and $â„‚$ ?","In Haskell programming language, Fractional typeclass effectively represents a normed field, and has Floating as a sub-typeclass. Floating is to define and trigonometric functions (and related constants and functions). ""Types"" that satisfies Floating include and . I wonder there would be other field that defines . I would define as Taylor series: In a field, since , is well-defined, division is well-defined, and if integers are defined as repeated addition of unity, is well-defined. The only ways to make undefined would be divergence of the series, or a division by zero. Convergence and divergence is well-defined because the field is normed, making the -definition of limit of number sequence applicable. As a consequence, a notable non-example would be because , hence , resulting in a division by zero. Is there an example rather than and ?","\exp â„ â„‚ \exp \exp 
\exp x = \sum_{n=0}^\infty \frac{x^n}{n!}
 n \in â„¤ x^n n! \exp x \epsilon \text{GF}(p^k) p â‰¡ 0 p! â‰¡ 0 â„ â„‚","['abstract-algebra', 'analysis', 'field-theory']"
45,Markov Chain: Calculating Expectation Reach a Certain Set of States,Markov Chain: Calculating Expectation Reach a Certain Set of States,,"Suppose I have a Markov chain $Z_k$ with $6$ states, as depicted below: The probability of moving from one node to a neighboring node is $1/2$ . For example, the probability of moving from node $1$ to node $2$ is $1/2$ and the probability of moving from node $1$ to node $6$ is $1/2$ etc. Suppose $P(Z_0=1)=1$ . That is we start state $1$ . We need to compute two things: Compute the expected time we first reach the bottom of our pyramid (states $3$ , $4$ , or $5$ ). That is compute $E[T_B]$ where $T_B=min\{j: Z_j \in \{3,4,5\}\}$ My attempt: I try listing out all the possibilities I can go from: I can go from State $1-2-3$ . Time is $2$ when base is reached and probability $\frac{1}{2} \cdot \frac{1}{2}$ $1-2-1-2-3$ . Time is $4$ when base is reached. Probability of occurring is $\frac{1}{16}$ . $1-2-1-2-1-2-3$ . Time is $6$ when base is reached. Probability of occurring is $\frac{1}{64}$ $1-2-1-2-1-2-1-2-3$ . Time is $8$ when base is reached. Probability of occurring is $\frac{1}{64}$ . etc... Thus my conclusion for these types of sequences expected value is: $2 \cdot \frac{1}{4}+ 4*\frac{1}{16}+6*\frac{1}{64}+8*\frac{1}{256}  ...$ $\sum_{k=1}^{\infty} 2k \cdot (\frac{1}{2})^{2k}=\frac{8}{9}$ Now, I can also go from State $1-6-5$ . Time is $2$ when base is reached and probability $\frac{1}{2} \cdot \frac{1}{2}$ $1-6-1-6-5$ . Time is $4$ when base is reached. Probability of occurring is $\frac{1}{16}$ . $1-6-1-6-1-6-5$ . Time is $6$ when base is reached. Probability of occurring is $\frac{1}{64}$ $1-6-1-6-1-6-1-6-5$ . Time is $8$ when base is reached. Probability of occurring is $\frac{1}{64}$ . Thus my conclusion for these types of sequences expected value is: $2 \cdot \frac{1}{4}+ 4*\frac{1}{16}+6*\frac{1}{64}+8*\frac{1}{256}  ...$ $\sum_{k=1}^{\infty} 2k \cdot (\frac{1}{2})^{2k}=\frac{8}{9}$ But there are more possibilities: $1-2-1-6-5.$ The time is $4$ probability $1/16$ $1-2-1-6-1-6-5$ . The time is $6$ probability $1/64$ 10. $1-2-1-6-1-6-1-6-5 etc..$ . The time is $8$ with probability $1/256$ . $\sum_{k=2}^{\infty} 2k \cdot (\frac{1}{2})^{2k}=\frac{7}{18}$ Still more possibilities....: Too many possibilities (unfortunately gave up) as its like the Markov Chain restarts when we go back to $1$ . Couldn't figure it out. Please let me know what I should do and thank you for the help","Suppose I have a Markov chain with states, as depicted below: The probability of moving from one node to a neighboring node is . For example, the probability of moving from node to node is and the probability of moving from node to node is etc. Suppose . That is we start state . We need to compute two things: Compute the expected time we first reach the bottom of our pyramid (states , , or ). That is compute where My attempt: I try listing out all the possibilities I can go from: I can go from State . Time is when base is reached and probability . Time is when base is reached. Probability of occurring is . . Time is when base is reached. Probability of occurring is . Time is when base is reached. Probability of occurring is . etc... Thus my conclusion for these types of sequences expected value is: Now, I can also go from State . Time is when base is reached and probability . Time is when base is reached. Probability of occurring is . . Time is when base is reached. Probability of occurring is . Time is when base is reached. Probability of occurring is . Thus my conclusion for these types of sequences expected value is: But there are more possibilities: The time is probability . The time is probability 10. . The time is with probability . Still more possibilities....: Too many possibilities (unfortunately gave up) as its like the Markov Chain restarts when we go back to . Couldn't figure it out. Please let me know what I should do and thank you for the help","Z_k 6 1/2 1 2 1/2 1 6 1/2 P(Z_0=1)=1 1 3 4 5 E[T_B] T_B=min\{j: Z_j \in \{3,4,5\}\} 1-2-3 2 \frac{1}{2} \cdot \frac{1}{2} 1-2-1-2-3 4 \frac{1}{16} 1-2-1-2-1-2-3 6 \frac{1}{64} 1-2-1-2-1-2-1-2-3 8 \frac{1}{64} 2 \cdot \frac{1}{4}+ 4*\frac{1}{16}+6*\frac{1}{64}+8*\frac{1}{256}  ... \sum_{k=1}^{\infty} 2k \cdot (\frac{1}{2})^{2k}=\frac{8}{9} 1-6-5 2 \frac{1}{2} \cdot \frac{1}{2} 1-6-1-6-5 4 \frac{1}{16} 1-6-1-6-1-6-5 6 \frac{1}{64} 1-6-1-6-1-6-1-6-5 8 \frac{1}{64} 2 \cdot \frac{1}{4}+ 4*\frac{1}{16}+6*\frac{1}{64}+8*\frac{1}{256}  ... \sum_{k=1}^{\infty} 2k \cdot (\frac{1}{2})^{2k}=\frac{8}{9} 1-2-1-6-5. 4 1/16 1-2-1-6-1-6-5 6 1/64 1-2-1-6-1-6-1-6-5 etc.. 8 1/256 \sum_{k=2}^{\infty} 2k \cdot (\frac{1}{2})^{2k}=\frac{7}{18} 1","['probability', 'sequences-and-series', 'analysis', 'stochastic-processes', 'markov-chains']"
46,Formula for $n^{th}$ derivative of $1/(x^2 +1)$,Formula for  derivative of,n^{th} 1/(x^2 +1),"I am completely done with this problem. I transformed $x^2+1$ to $(x+i)(x-i)$ , also used standard form of $n^{th}$ derivative of $\arctan(x)$ , again gone negative binomial expansion but nothing is working. If anyone can make it I will be very glad. The question is: Show that the $n^{th}$ derivative of $1/(x^2+1)$ is equal to $$ \frac{(-1)^n \cdot n!}{(x^2+1)^{n+1}} \cdot \left[(n+1)x^n - \,{}^{n+1}C_3 \,x^{n-2} +\,{}^{n+1}C_5\,x^{n-4} - \cdots \right] $$","I am completely done with this problem. I transformed to , also used standard form of derivative of , again gone negative binomial expansion but nothing is working. If anyone can make it I will be very glad. The question is: Show that the derivative of is equal to","x^2+1 (x+i)(x-i) n^{th} \arctan(x) n^{th} 1/(x^2+1) 
\frac{(-1)^n \cdot n!}{(x^2+1)^{n+1}} \cdot \left[(n+1)x^n - \,{}^{n+1}C_3 \,x^{n-2} +\,{}^{n+1}C_5\,x^{n-4} - \cdots \right]
","['analysis', 'derivatives', 'induction']"
47,"Why did Terence Tao write Proposition 2.1.11 about mathematical induction in ""Analysis I""?","Why did Terence Tao write Proposition 2.1.11 about mathematical induction in ""Analysis I""?",,"I am reading ""Analysis I"" by Terence Tao. Axiom 2.5 (Principle of mathematical induction). Let $P(n)$ be any property pertaining to a natural number $n$ . Suppose that $P(0)$ is true, and suppose that whenever $P(n)$ is true, $P(n++)$ is also true. Then $P(n)$ is true for every natural number $n$ . Next he wrote this proposition: Proposition 2.1.11. A certain property $P(n)$ is true for every natural number $n$ . Proof. We use induction. We first verify the base case $n=0$ , i.e., we prove $P(0)$ . (Insert proof of $P(0)$ here). Now suppose inductively that $n$ is a natural number, and $P(n)$ has already been proven. We now prove $P(n++)$ . (Insert proof of $P(n++)$ , assuming that $P(n)$ is true, here). This closes the induction, and thus $P(n)$ is true for all numbers $n$ . What is this proposition? I cannot understand why Tao wrote this proposition. Isn't it obvious from Axiom 2.5? I cannot understand.","I am reading ""Analysis I"" by Terence Tao. Axiom 2.5 (Principle of mathematical induction). Let be any property pertaining to a natural number . Suppose that is true, and suppose that whenever is true, is also true. Then is true for every natural number . Next he wrote this proposition: Proposition 2.1.11. A certain property is true for every natural number . Proof. We use induction. We first verify the base case , i.e., we prove . (Insert proof of here). Now suppose inductively that is a natural number, and has already been proven. We now prove . (Insert proof of , assuming that is true, here). This closes the induction, and thus is true for all numbers . What is this proposition? I cannot understand why Tao wrote this proposition. Isn't it obvious from Axiom 2.5? I cannot understand.",P(n) n P(0) P(n) P(n++) P(n) n P(n) n n=0 P(0) P(0) n P(n) P(n++) P(n++) P(n) P(n) n,"['calculus', 'analysis', 'induction']"
48,How to find asymptotic behavior of this function?,How to find asymptotic behavior of this function?,,"I've read in a book that this function: $$b(r) = \sqrt{\frac{r^3}{r - 2M}}$$ behaves approximately as $$ b(r) \approx r + M $$ for $r \gg M$ . I checked it numerically, by plotting a graph of this function, and found that it is true. How to show this mathematically?","I've read in a book that this function: behaves approximately as for . I checked it numerically, by plotting a graph of this function, and found that it is true. How to show this mathematically?",b(r) = \sqrt{\frac{r^3}{r - 2M}}  b(r) \approx r + M  r \gg M,"['analysis', 'functions']"
49,How to generate the shortest possible equation that has exactly one solution for specified integer?,How to generate the shortest possible equation that has exactly one solution for specified integer?,,"I'm basically trying to create a function that converts a positive integer n into an equation that has exactly one solution which can be computed back into the n . So if n = 1282388557824 the function should convert it to n = 264 ^ 5 since it's the shortest (or at least one of the shortest) equation and has only 1 solution. The function can use any mathematical operators that a computer can calculate. How would we even go about finding the shortest possible equation (or one of the shortest) without a slow brute-force? Any smart tricks we can use? Let's say we have n = 6415607 , then how can we quickly find that the shortest equation for it is (23 ^ 5) - (12 ^ 4) and not something short like 186 ^ 3 ? (it's not, it's just an example) It's fine if some of the integers cannot be compressed into an equation. There's 2 preferred conditions: The equation should be as short and as easy to compute as possible. For example, for n = 17 it should generate something like n = 2 ^ 4 + 1 Computation speed should not grow exponentially with integer length, the function should generate the equation relatively quickly, regardless of the integer's length. Let's say, something like under 0.1 ms for a 10 digit long integer and under 1 sec for a 100,000 digit long integer. It would be nice if you could write the answer in a form of a function written in any programming language. I understand the algorithms better this way, math language is often too hard for me.","I'm basically trying to create a function that converts a positive integer n into an equation that has exactly one solution which can be computed back into the n . So if n = 1282388557824 the function should convert it to n = 264 ^ 5 since it's the shortest (or at least one of the shortest) equation and has only 1 solution. The function can use any mathematical operators that a computer can calculate. How would we even go about finding the shortest possible equation (or one of the shortest) without a slow brute-force? Any smart tricks we can use? Let's say we have n = 6415607 , then how can we quickly find that the shortest equation for it is (23 ^ 5) - (12 ^ 4) and not something short like 186 ^ 3 ? (it's not, it's just an example) It's fine if some of the integers cannot be compressed into an equation. There's 2 preferred conditions: The equation should be as short and as easy to compute as possible. For example, for n = 17 it should generate something like n = 2 ^ 4 + 1 Computation speed should not grow exponentially with integer length, the function should generate the equation relatively quickly, regardless of the integer's length. Let's say, something like under 0.1 ms for a 10 digit long integer and under 1 sec for a 100,000 digit long integer. It would be nice if you could write the answer in a form of a function written in any programming language. I understand the algorithms better this way, math language is often too hard for me.",,"['abstract-algebra', 'number-theory', 'analysis', 'logic']"
50,Dimension of a preimage,Dimension of a preimage,,"Suppose we have a differentiable function $f:\mathbb{R}^{k}\to\mathbb{R}^{\ell}$ where $k>\ell$ . How can we formalize the fact that the ""inverse"" of a point $\mathbf{y}\in\mathbb{R}^{\ell}$ , $f^{-1}(\{\mathbf{y}\})$ will be a $(k-\ell)$ -dimensional set ?","Suppose we have a differentiable function where . How can we formalize the fact that the ""inverse"" of a point , will be a -dimensional set ?",f:\mathbb{R}^{k}\to\mathbb{R}^{\ell} k>\ell \mathbf{y}\in\mathbb{R}^{\ell} f^{-1}(\{\mathbf{y}\}) (k-\ell),"['general-topology', 'analysis', 'functions', 'differential-topology', 'inverse']"
51,Bijection from $\mathbb{N}$ to $\mathbb{N}$ such that $f(n) \geq n \text{ for large } n$ or $f(n) \leq n \text{ for large } n$ holds.,Bijection from  to  such that  or  holds.,\mathbb{N} \mathbb{N} f(n) \geq n \text{ for large } n f(n) \leq n \text{ for large } n,"Let us consider an arbitrary bijective map $f: \mathbb{N} \to \mathbb{N}$ . Then which one of the following is correct? \begin{align} & f(n) \geq n \text{  for large } n \\ & f(n) \leq n \text{  for large } n \end{align} I know that $f(n)=n$ is a bijection. But if $f$ is any other bijection other than identity, which of the above must hold? It may be happen that both can hold for different $f$ .  Please give me some example or if any proof, of the above.","Let us consider an arbitrary bijective map . Then which one of the following is correct? I know that is a bijection. But if is any other bijection other than identity, which of the above must hold? It may be happen that both can hold for different .  Please give me some example or if any proof, of the above.",f: \mathbb{N} \to \mathbb{N} \begin{align} & f(n) \geq n \text{  for large } n \\ & f(n) \leq n \text{  for large } n \end{align} f(n)=n f f,"['real-analysis', 'analysis']"
52,"If $f:I\to \mathbb{R}$ is convex and interval $I$ is bounded, prove that $f$ is bounded below.","If  is convex and interval  is bounded, prove that  is bounded below.",f:I\to \mathbb{R} I f,"Let $I$ be a bounded interval and $f:I\to \mathbb{R}$ be a convex function. Prove that $f$ is bounded below in $I.$ Attempt. Let $a,~b\in I$ , by convexity of $f$ on $[a,b]:$ $$f(x)\leq g(x):=f(a)+\frac{f(b)-f(a)}{b-a}(x-a)$$ for all $x\in [a,b]$ . So it is enough to prove that: $f(x)\geq g(x)$ for $x\in I,~x<a$ or $x>b$ , $f$ attains a minimum value $m$ on $[a,b]$ , Thanks for the help.","Let be a bounded interval and be a convex function. Prove that is bounded below in Attempt. Let , by convexity of on for all . So it is enough to prove that: for or , attains a minimum value on , Thanks for the help.","I f:I\to \mathbb{R} f I. a,~b\in I f [a,b]: f(x)\leq g(x):=f(a)+\frac{f(b)-f(a)}{b-a}(x-a) x\in [a,b] f(x)\geq g(x) x\in I,~x<a x>b f m [a,b]","['real-analysis', 'analysis', 'convex-analysis']"
53,$\sum_{n=1}^\infty \frac{n!e^n}{n^{n+ \frac{3}{2}}}$ - any ideas for a simple proof of divergence?,- any ideas for a simple proof of divergence?,\sum_{n=1}^\infty \frac{n!e^n}{n^{n+ \frac{3}{2}}},I am looking for a simple proof of divergence for the series: $\sum_{n=1}^\infty \frac{n!e^n}{n^{n+\frac{3}{2}}}$ That's a part of the more general problem: For what values of X is the series $\sum_{n=1}^\infty \frac{n!e^n}{n^{n+X}}$ convergent and for what values is it divergent? I am not allowed to use Stirling's approximation in the proof. I've already managed to prove convergence for $X>\frac{3}{2}$ and divergence for $X<\frac{3}{2}$ . And now I am stuck with $X=\frac{3}{2}$ - I know the series is divergent (from Stirling approximation and WolframAlpha) but I have no idea for an elementary proof. This question is related to my previous one about the elementary proof for the simpler case i.e.: $\sum_{n=1}^\infty \frac{n!e^n}{n^n}$ so you might be interested in checking it out: Divergent infinite series $n!e^n/n^n$ - simpler proof of divergence?,I am looking for a simple proof of divergence for the series: That's a part of the more general problem: For what values of X is the series convergent and for what values is it divergent? I am not allowed to use Stirling's approximation in the proof. I've already managed to prove convergence for and divergence for . And now I am stuck with - I know the series is divergent (from Stirling approximation and WolframAlpha) but I have no idea for an elementary proof. This question is related to my previous one about the elementary proof for the simpler case i.e.: so you might be interested in checking it out: Divergent infinite series - simpler proof of divergence?,\sum_{n=1}^\infty \frac{n!e^n}{n^{n+\frac{3}{2}}} \sum_{n=1}^\infty \frac{n!e^n}{n^{n+X}} X>\frac{3}{2} X<\frac{3}{2} X=\frac{3}{2} \sum_{n=1}^\infty \frac{n!e^n}{n^n} n!e^n/n^n,"['real-analysis', 'calculus', 'analysis', 'convergence-divergence', 'divergent-series']"
54,$\{x\} \times B \subset \mathbb{R}^{n+m}$ is compact if $B \subset \mathbb{R}^m$ is compact and $x \in \mathbb{R}^n$,is compact if  is compact and,\{x\} \times B \subset \mathbb{R}^{n+m} B \subset \mathbb{R}^m x \in \mathbb{R}^n,"This is from Spivak Calculus on manifolds page 14.  I cannot use the fact that the cartesian product of two compact sets which are subsets of Euclidean space is compact because he uses what is written in the title to prove that fact. It seems that I am only allowed to use the definition of compactness here, which is that a set is compact if every open cover of it has a finite subvoer. (The book only deals with Euclidean space here, and Spivak defines an open set $U$ to be such that for every element $x \in U$ , there is an open rectangle $A$ such that $x \in A \subset U$ .) As he says in the book, this must be very easy to see, but I just cannot figure out. Thanks in advance.","This is from Spivak Calculus on manifolds page 14.  I cannot use the fact that the cartesian product of two compact sets which are subsets of Euclidean space is compact because he uses what is written in the title to prove that fact. It seems that I am only allowed to use the definition of compactness here, which is that a set is compact if every open cover of it has a finite subvoer. (The book only deals with Euclidean space here, and Spivak defines an open set to be such that for every element , there is an open rectangle such that .) As he says in the book, this must be very easy to see, but I just cannot figure out. Thanks in advance.",U x \in U A x \in A \subset U,"['real-analysis', 'general-topology', 'analysis']"
55,"Second order taylor series expansion of $\cos(x+2y)e^{2x}$ around $(2, -1)$",Second order taylor series expansion of  around,"\cos(x+2y)e^{2x} (2, -1)","I'm asked to find the second order taylor series expansion of $\cos(x+2y)e^{2x}$ around $(2, -1)$ . It's from an old exam of a professor who doesn't give solutions to them, so I don't know if my results are correct. So I woud like that you tell me if my results are correct, and if not, where I made a mistake. Thank you. As first partial derivatives, I get $$f_x=-\sin(x+2y)e^{2x}+2\cos(x+2y)e^{2x}$$ $$f_y=-2\sin(x+2y)e^{2x}$$ As second partial derivatives, I get $$\begin{align} f_{xx}&=-\cos(x+2y)e^{2x}-2\sin(x+2y)e^{2x}-2\sin(x+2y)e^{2x}+4\cos(x+2y)e^{2x}\\ &=3\cos(x+2y)e^{2x}-4\sin(x+2y)e^{2x} \end{align}$$ $$f_{yy}=-4\cos(x+2y)e^{2x}$$ $$f_{xy}=f_{yx}=-2\cos(x+2y)e^{2x}-4\sin(x+2y)e^{2x}$$ So now, if I plug everything into the formula, I get $$e^{4}+2e^{4}(x-2)+\frac{1}{2!}(3e^{4}(x-2)^2-4e^{4}(x-2)(y+1)-4e^{4}(y+1)^2)=e^{4}(1+2(x-2)+\frac{1}{2!}(3(x-2)^2-4(x-2)(y+1)-4(y+1)^2))$$ Are my results correct ? I just proceeded as usual, using the common two-variables taylor series expansion. Thanks for your help !","I'm asked to find the second order taylor series expansion of around . It's from an old exam of a professor who doesn't give solutions to them, so I don't know if my results are correct. So I woud like that you tell me if my results are correct, and if not, where I made a mistake. Thank you. As first partial derivatives, I get As second partial derivatives, I get So now, if I plug everything into the formula, I get Are my results correct ? I just proceeded as usual, using the common two-variables taylor series expansion. Thanks for your help !","\cos(x+2y)e^{2x} (2, -1) f_x=-\sin(x+2y)e^{2x}+2\cos(x+2y)e^{2x} f_y=-2\sin(x+2y)e^{2x} \begin{align}
f_{xx}&=-\cos(x+2y)e^{2x}-2\sin(x+2y)e^{2x}-2\sin(x+2y)e^{2x}+4\cos(x+2y)e^{2x}\\
&=3\cos(x+2y)e^{2x}-4\sin(x+2y)e^{2x}
\end{align} f_{yy}=-4\cos(x+2y)e^{2x} f_{xy}=f_{yx}=-2\cos(x+2y)e^{2x}-4\sin(x+2y)e^{2x} e^{4}+2e^{4}(x-2)+\frac{1}{2!}(3e^{4}(x-2)^2-4e^{4}(x-2)(y+1)-4e^{4}(y+1)^2)=e^{4}(1+2(x-2)+\frac{1}{2!}(3(x-2)^2-4(x-2)(y+1)-4(y+1)^2))","['real-analysis', 'analysis']"
56,"Why is there an ""implication"" rather than and ""and"" in this definition of the derivative?","Why is there an ""implication"" rather than and ""and"" in this definition of the derivative?",,"I am readig Pugh's Analysis book: Definition Let $f:U \to \mathbb{R}^m$ be given where $U$ is an open subset of $\mathbb{R}^n$ . The function $f$ is differentiable a $p \in U$ with derivative $(Df)_p = T$ if $T:\mathbb{R}^n \to \mathbb{R}^m$ is a linear transformation and $f(p+v) = f(p)+T(v)+R(v) \implies \lim_{|v| \to 0} \dfrac {R(v)}{|v|}=0$ . Partly due to the missing quantifiers, I'm having trouble understanding why there is a "" $\implies$ "" there rather than a "" $\wedge$ "". Isn't it more natural to say ""T is the derivative if we can write $f(p+v) = f(p)+T(v)+R(v)$ AND $\lim_{|v| \to 0} \dfrac {R(v)}{|v|}=0$ ""? I'm having trouble seeing what the impact of changing these would be.","I am readig Pugh's Analysis book: Definition Let be given where is an open subset of . The function is differentiable a with derivative if is a linear transformation and . Partly due to the missing quantifiers, I'm having trouble understanding why there is a "" "" there rather than a "" "". Isn't it more natural to say ""T is the derivative if we can write AND ""? I'm having trouble seeing what the impact of changing these would be.",f:U \to \mathbb{R}^m U \mathbb{R}^n f p \in U (Df)_p = T T:\mathbb{R}^n \to \mathbb{R}^m f(p+v) = f(p)+T(v)+R(v) \implies \lim_{|v| \to 0} \dfrac {R(v)}{|v|}=0 \implies \wedge f(p+v) = f(p)+T(v)+R(v) \lim_{|v| \to 0} \dfrac {R(v)}{|v|}=0,"['real-analysis', 'analysis', 'logic', 'frechet-derivative']"
57,$\left.\left(\frac{\mathrm d}{\mathrm d x}\right)^n J_0(x)\right|_{x=0}={}$?,?,\left.\left(\frac{\mathrm d}{\mathrm d x}\right)^n J_0(x)\right|_{x=0}={},"I am interested in determining a closed expression for the n-th derivative of the Bessel function of the first kind $J_0(x)$ , centered in $x=0$ : \begin{equation} \left.\left(\frac{\mathrm  d}{\mathrm d x} \right)^n J_0(x)\right|_{x=0} \end{equation} Can I compute it? If yes, how? Thanks in advance!","I am interested in determining a closed expression for the n-th derivative of the Bessel function of the first kind , centered in : Can I compute it? If yes, how? Thanks in advance!","J_0(x) x=0 \begin{equation}
\left.\left(\frac{\mathrm  d}{\mathrm d x} \right)^n J_0(x)\right|_{x=0}
\end{equation}","['analysis', 'special-functions', 'bessel-functions']"
58,How do I find the value of this series? $\frac{n^{2k}}{k!}$,How do I find the value of this series?,\frac{n^{2k}}{k!},In my last exam there was a task where we had to find the value of this series: $$ \sum_{k=0}^\infty  \frac{n^{2k}}{k!} $$ I thought about using a ratio test where I did the following: $$ \dfrac{a_{k+1} }{a_{k}} = \frac{\dfrac{n^{2*(k+1)}}{(k+1)!}}{\dfrac{n^2*k}{k!}} = \dfrac{n^{2(k+1)}k!}{(k+1)!*n^{2k}} $$ The problem that I have now is that I don't know how to continue or if this the right approach.,In my last exam there was a task where we had to find the value of this series: $$ \sum_{k=0}^\infty  \frac{n^{2k}}{k!} $$ I thought about using a ratio test where I did the following: $$ \dfrac{a_{k+1} }{a_{k}} = \frac{\dfrac{n^{2*(k+1)}}{(k+1)!}}{\dfrac{n^2*k}{k!}} = \dfrac{n^{2(k+1)}k!}{(k+1)!*n^{2k}} $$ The problem that I have now is that I don't know how to continue or if this the right approach.,,"['sequences-and-series', 'analysis']"
59,Suppose $f\in L_2(\mathbb R)$ and $f$ is a continuous function on $\mathbb R$. Is $\displaystyle\sum_{k\in\mathbb{Z}}|f(k)|^2<\infty $?,Suppose  and  is a continuous function on . Is ?,f\in L_2(\mathbb R) f \mathbb R \displaystyle\sum_{k\in\mathbb{Z}}|f(k)|^2<\infty ,Suppose $f\in L_2(\mathbb R)$ and $f$ is a continuous function on $\mathbb R$. Is $\displaystyle\sum_{k\in\mathbb{Z}}|f(k)|^2<\infty $ ? I am trying to prove the relation $\displaystyle\sum_{k\in\mathbb{Z}}|f(k)|^2<\int_{\mathbb R}|f(x)|^2dx$. Is that relation true? I am struggling to prove this. Please help me!,Suppose $f\in L_2(\mathbb R)$ and $f$ is a continuous function on $\mathbb R$. Is $\displaystyle\sum_{k\in\mathbb{Z}}|f(k)|^2<\infty $ ? I am trying to prove the relation $\displaystyle\sum_{k\in\mathbb{Z}}|f(k)|^2<\int_{\mathbb R}|f(x)|^2dx$. Is that relation true? I am struggling to prove this. Please help me!,,"['analysis', 'fourier-analysis', 'harmonic-analysis']"
60,An alternative proof for $D_2 D_1 f(x) = D_1 D_2 f(x)$ when $f \in C^2$,An alternative proof for  when,D_2 D_1 f(x) = D_1 D_2 f(x) f \in C^2,"In the book of Analysis On Manifolds by Munkres, at page 103, question 4-b, it is asked to show that $$D_2 D_1 f(x) = D_1 D_2 f(x)$$ for all $x \in A$, where $A \subseteq  \mathbb{R}^2 $ is open and $f\in C^2(A)$. Proof: Let $Q$ be a rectangle in $A$, so we do know that  $$\int_Q D_2 D_1 f(x) = \int_Q D_1 D_2 f(x),$$ hence consider  $$\int_Q |D_2 D_1 f(x) - D_1 D_2 f(x)| = $$ Since the integral above exists, and the integrand is non-negative and the integral is zero, by a theorem, the integrand $|D_2 D_1 f(x) - D_1 D_2 f(x)|$ vanishes except on a a set of measure zero. Now we claim that, the integrand $|D_2 D_1 f(x) - D_1 D_2 f(x)|$ vanishes everywhere on $Q$. Assume that is it not the case; then $\exists a \in Q$ s.t $|D_2 D_1 f(a) - D_1 D_2 f(a)| \not = 0$, so by a lemma, there exists a neighbourhood $U$ of $a$ s.t that the integrand is still non-zero, but that contradicts with the fact that the integrand vanishes on $Q$ except on a set of measure zero, since a open ball does not have a measure zero. So, $|D_2 D_1 f(x) - D_1 D_2 f(x)| = 0$ $\forall x \in Q$ implies  $$D_2 D_1 f(x) = D_1 D_2 f(x) \quad \forall x \in Q.$$ Question: Is there any flaw in my proof ? or is there anything that is not clear ? or do you have any suggestion ?","In the book of Analysis On Manifolds by Munkres, at page 103, question 4-b, it is asked to show that $$D_2 D_1 f(x) = D_1 D_2 f(x)$$ for all $x \in A$, where $A \subseteq  \mathbb{R}^2 $ is open and $f\in C^2(A)$. Proof: Let $Q$ be a rectangle in $A$, so we do know that  $$\int_Q D_2 D_1 f(x) = \int_Q D_1 D_2 f(x),$$ hence consider  $$\int_Q |D_2 D_1 f(x) - D_1 D_2 f(x)| = $$ Since the integral above exists, and the integrand is non-negative and the integral is zero, by a theorem, the integrand $|D_2 D_1 f(x) - D_1 D_2 f(x)|$ vanishes except on a a set of measure zero. Now we claim that, the integrand $|D_2 D_1 f(x) - D_1 D_2 f(x)|$ vanishes everywhere on $Q$. Assume that is it not the case; then $\exists a \in Q$ s.t $|D_2 D_1 f(a) - D_1 D_2 f(a)| \not = 0$, so by a lemma, there exists a neighbourhood $U$ of $a$ s.t that the integrand is still non-zero, but that contradicts with the fact that the integrand vanishes on $Q$ except on a set of measure zero, since a open ball does not have a measure zero. So, $|D_2 D_1 f(x) - D_1 D_2 f(x)| = 0$ $\forall x \in Q$ implies  $$D_2 D_1 f(x) = D_1 D_2 f(x) \quad \forall x \in Q.$$ Question: Is there any flaw in my proof ? or is there anything that is not clear ? or do you have any suggestion ?",,"['real-analysis', 'integration', 'analysis', 'riemann-integration']"
61,Fourier transform of $t H(t)$,Fourier transform of,t H(t),"I want to calculate the Fourier transform of $f(t):=t\cdot H(t)$ ($H$ denotes the Heaviside function). For the integral I got $$\hat{f}(y)=\left[\left(\frac{1}{y^2}+\frac{ix}{y}\right)e^{-i\omega x}\right]_{x=0}^{\infty}$$ Now I'm having trouble getting handling the boundaries, since $|e^{-i\omega x}|=1$. Could I maybe compute $\hat{g}$ for $g(x):=xe^{-\varepsilon x}H(x)$ for $\varepsilon>0$ and then send $\varepsilon \rightarrow 0$ to get the transform I want?","I want to calculate the Fourier transform of $f(t):=t\cdot H(t)$ ($H$ denotes the Heaviside function). For the integral I got $$\hat{f}(y)=\left[\left(\frac{1}{y^2}+\frac{ix}{y}\right)e^{-i\omega x}\right]_{x=0}^{\infty}$$ Now I'm having trouble getting handling the boundaries, since $|e^{-i\omega x}|=1$. Could I maybe compute $\hat{g}$ for $g(x):=xe^{-\varepsilon x}H(x)$ for $\varepsilon>0$ and then send $\varepsilon \rightarrow 0$ to get the transform I want?",,"['analysis', 'fourier-analysis']"
62,$\lim_{n \rightarrow \infty}\int_0^1f_n(x)$ [duplicate],[duplicate],\lim_{n \rightarrow \infty}\int_0^1f_n(x),"This question already has answers here : Limit of $s_n = \int\limits_0^1 \frac{nx^{n-1}}{1+x} dx$ as $n \to \infty$ (5 answers) Closed 2 years ago . For $n=1, 2,...,$ let $f_n(x)=\frac{2nx^{n-1}}{x+1}, x\in [0, 1]$. Then $\lim_{n \rightarrow \infty}\int_0^1f_n(x)$ Function is unbounded at $1$, How do I solve?","This question already has answers here : Limit of $s_n = \int\limits_0^1 \frac{nx^{n-1}}{1+x} dx$ as $n \to \infty$ (5 answers) Closed 2 years ago . For $n=1, 2,...,$ let $f_n(x)=\frac{2nx^{n-1}}{x+1}, x\in [0, 1]$. Then $\lim_{n \rightarrow \infty}\int_0^1f_n(x)$ Function is unbounded at $1$, How do I solve?",,"['real-analysis', 'analysis']"
63,"$\frac{f'(x)}{f(x}\le \frac{g'(x)}{g(x)} \Rightarrow f(x)\le g(x) \,\forall\,x\in[a,b]$",,"\frac{f'(x)}{f(x}\le \frac{g'(x)}{g(x)} \Rightarrow f(x)\le g(x) \,\forall\,x\in[a,b]","Let $\,f,g:\,[a,b]\rightarrow \mathbb R$ be differentiable, postive functions with $f(a)=g(a)$ and $\frac{f'(x)}{f(x)}\le \frac{g'(x)}{g(x)}\,\forall\,x\in[a,b]$ $$Prove,\,that: \frac{f'(x)}{f(x)}\le \frac{g'(x)}{g(x)} \Rightarrow f(x)\le g(x) \,\forall\,x\in[a,b]$$ I suppose it has something to do with Rolles Theorem and/or the mean value theorem for differential equations, but I have no idea how I can constructively approach this problem. Thank you in advance for any help! coltrane","Let $\,f,g:\,[a,b]\rightarrow \mathbb R$ be differentiable, postive functions with $f(a)=g(a)$ and $\frac{f'(x)}{f(x)}\le \frac{g'(x)}{g(x)}\,\forall\,x\in[a,b]$ $$Prove,\,that: \frac{f'(x)}{f(x)}\le \frac{g'(x)}{g(x)} \Rightarrow f(x)\le g(x) \,\forall\,x\in[a,b]$$ I suppose it has something to do with Rolles Theorem and/or the mean value theorem for differential equations, but I have no idea how I can constructively approach this problem. Thank you in advance for any help! coltrane",,"['real-analysis', 'analysis']"
64,On changing the order of summation between a finite sum and an infinite sum.,On changing the order of summation between a finite sum and an infinite sum.,,"Suppose that for every $j=1,...,t$ we have a convergent series of complex numbers $\displaystyle\sum_{m=0}^\infty a_j^m$. My question is: Is it true in general that $$\sum_{j=1}^t\left(\sum_{m=0}^\infty a_j^m\right)=\sum_{m=0}^{\infty}\left(\sum_{j=1}^ta_j^m\right)$$   If not, under which extra conditions this can be true? Thank you very much for your help.","Suppose that for every $j=1,...,t$ we have a convergent series of complex numbers $\displaystyle\sum_{m=0}^\infty a_j^m$. My question is: Is it true in general that $$\sum_{j=1}^t\left(\sum_{m=0}^\infty a_j^m\right)=\sum_{m=0}^{\infty}\left(\sum_{j=1}^ta_j^m\right)$$   If not, under which extra conditions this can be true? Thank you very much for your help.",,"['analysis', 'complex-numbers', 'fourier-series']"
65,"How to calculate $\int_0^\pi x\,\cos^4x\, dx$",How to calculate,"\int_0^\pi x\,\cos^4x\, dx","Assume that $$\int_{0}^\pi x\,f(\sin(x))dx=\frac{\pi}2 \int_{0}^\pi f(\sin(x))dx$$ and use it to calculate $$\int_{0}^\pi x\,\cos^{4}(x)\, dx$$ Can anyone help me with that? I proved the identity but I am stuck with the rest.","Assume that $$\int_{0}^\pi x\,f(\sin(x))dx=\frac{\pi}2 \int_{0}^\pi f(\sin(x))dx$$ and use it to calculate $$\int_{0}^\pi x\,\cos^{4}(x)\, dx$$ Can anyone help me with that? I proved the identity but I am stuck with the rest.",,"['integration', 'analysis', 'definite-integrals', 'trigonometric-integrals']"
66,Asymptotics of $\sum_{0 \leq j \leq k \leq n-1} {\binom{2n}{j}}{\binom{2n-1}{k}}^{-1}$,Asymptotics of,\sum_{0 \leq j \leq k \leq n-1} {\binom{2n}{j}}{\binom{2n-1}{k}}^{-1},"For any integer $n\geq 1$, define $$f(n) = \sum_{0 \leq j \leq k \leq n-1} \frac{\binom{2n}{j}}{\binom{2n-1}{k}}$$ Our lecture says that $f(n) = n +n\log n +O(1)$. But I cannot prove it, the best result I've gotten is $f(n) \leq cn^{\frac{3}{2}}$, and now I'm kind of doubtful about the result. Is it possible to prove it or deny it?","For any integer $n\geq 1$, define $$f(n) = \sum_{0 \leq j \leq k \leq n-1} \frac{\binom{2n}{j}}{\binom{2n-1}{k}}$$ Our lecture says that $f(n) = n +n\log n +O(1)$. But I cannot prove it, the best result I've gotten is $f(n) \leq cn^{\frac{3}{2}}$, and now I'm kind of doubtful about the result. Is it possible to prove it or deny it?",,"['combinatorics', 'analysis', 'asymptotics', 'binomial-coefficients']"
67,Showing $f=0$ almost everywhere.,Showing  almost everywhere.,f=0,"let $f\in L^2(\mathbb{R},\mathcal{L},m) $, and suppose that    $$\int_\mathbb{R}f(y)e^{-(x-y)^2/2}dy=0$$ for all $x\in\mathbb{R}$. Prove that $f=0$ a.e. I can say  $$f*g(x)=\int_\mathbb{R}f(y)e^{-(x-y)^2/2}dy=0,$$ where $g=e^{-x^2/2}$. Hence, $\widehat{f*g}=0$. Now I'm not sure if it's good enough to say $\widehat{f*g}=\hat{f} \cdot \hat{g}$. As a result, if $\hat{g}\neq 0$, then $\hat{f}=0$ and then $f=0$. Is it a valid argument?","let $f\in L^2(\mathbb{R},\mathcal{L},m) $, and suppose that    $$\int_\mathbb{R}f(y)e^{-(x-y)^2/2}dy=0$$ for all $x\in\mathbb{R}$. Prove that $f=0$ a.e. I can say  $$f*g(x)=\int_\mathbb{R}f(y)e^{-(x-y)^2/2}dy=0,$$ where $g=e^{-x^2/2}$. Hence, $\widehat{f*g}=0$. Now I'm not sure if it's good enough to say $\widehat{f*g}=\hat{f} \cdot \hat{g}$. As a result, if $\hat{g}\neq 0$, then $\hat{f}=0$ and then $f=0$. Is it a valid argument?",,"['real-analysis', 'analysis', 'fourier-analysis', 'fourier-transform']"
68,$\mathbb{R} \backslash \left\{ \emptyset\right\}$ is open.,is open.,\mathbb{R} \backslash \left\{ \emptyset\right\},"Recall. An open set is a set of the form $\bigcup U$ where $U$ is a set of open intervals. $\mathbb{R} \backslash \left\{ \emptyset\right\}$ is open. Proof. $\mathbb{R} \backslash \left\{ \emptyset\right\}=\left( -\infty ,0\right) \cup\left( 0,\infty \right) $. This question is from my lecture notes. I think, there is a typo that this question should be $\mathbb{R} \backslash \left\{ 0\right\}$ is open, right?","Recall. An open set is a set of the form $\bigcup U$ where $U$ is a set of open intervals. $\mathbb{R} \backslash \left\{ \emptyset\right\}$ is open. Proof. $\mathbb{R} \backslash \left\{ \emptyset\right\}=\left( -\infty ,0\right) \cup\left( 0,\infty \right) $. This question is from my lecture notes. I think, there is a typo that this question should be $\mathbb{R} \backslash \left\{ 0\right\}$ is open, right?",,['general-topology']
69,"If $f,g$ each have property p then $f \circ g$ also has property $p$",If  each have property p then  also has property,"f,g f \circ g p","What type of characteristics should properties have for the following to hold true? If $f,g$ each have property $p$ then $f  \circ g$ also has property $p$ Examples: If $f,g$ continuous then $f  \circ g$ is continuous If $f,g$ entire then $f  \circ g$ is entire If $f,g$ differentiable then $f  \circ g$ is differentiable If $f,g$ one to one then $f  \circ g$ is one to one If $f,g$ Contractible then $f  \circ g$ is Contractible If $f,g$ polynomial then $f  \circ g$ is polynomial If $f,g$ lineaer then $f  \circ g$ is linear Counter Examples: If $f,g$ integrable then $f  \circ g$ is integrable (false) see Robert Israel's answer If $f,g$ measureable then $f  \circ g$ is measureable Update Edit: The motivation for the question was to use the statement as a filter to specify properties p, I had seen too often question being asked for a specific property, was wondering if there is a way to devise a test for p rather than test each p individually to see if it satisfies the statement. (This is as good as I can explain my intent for asking the question, making a list seems to be a beneficial side effect). If anyone can elucidate the motivation with better mathematical terminology please edit.","What type of characteristics should properties have for the following to hold true? If $f,g$ each have property $p$ then $f  \circ g$ also has property $p$ Examples: If $f,g$ continuous then $f  \circ g$ is continuous If $f,g$ entire then $f  \circ g$ is entire If $f,g$ differentiable then $f  \circ g$ is differentiable If $f,g$ one to one then $f  \circ g$ is one to one If $f,g$ Contractible then $f  \circ g$ is Contractible If $f,g$ polynomial then $f  \circ g$ is polynomial If $f,g$ lineaer then $f  \circ g$ is linear Counter Examples: If $f,g$ integrable then $f  \circ g$ is integrable (false) see Robert Israel's answer If $f,g$ measureable then $f  \circ g$ is measureable Update Edit: The motivation for the question was to use the statement as a filter to specify properties p, I had seen too often question being asked for a specific property, was wondering if there is a way to devise a test for p rather than test each p individually to see if it satisfies the statement. (This is as good as I can explain my intent for asking the question, making a list seems to be a beneficial side effect). If anyone can elucidate the motivation with better mathematical terminology please edit.",,['analysis']
70,Proving $|\cos z\ |^2 + |\sin z\ |^2 \geq 1$,Proving,|\cos z\ |^2 + |\sin z\ |^2 \geq 1,"My attempt: For $z :=x+iy$, $$\cos z =\cos x \cos iy - \sin x \sin iy \\ = \cos x \cosh y - i \sin x \sinh y\\ \sin z = \sin x \cosh y + i \cos x \sinh y$$ So $$|\cos z\ |^2 + |\sin z\ |^2 = \cos ^2 x \cosh ^2 y + \sin ^2 x \sinh ^2 y + \cosh^2 y \sin^2x + \cos ^2x \sinh ^2 y\\ = \cos ^2x ( \cosh ^2 y + \sinh ^2 y) + \sin ^2 x (\cosh^2 y + \sinh^2 y) \\ = \cos 2y \leq 1.$$ I'm not sure where I went wrong, and a numerical check on Wolfram Alpha shows that the inequality should be $\geq$ as suggested.","My attempt: For $z :=x+iy$, $$\cos z =\cos x \cos iy - \sin x \sin iy \\ = \cos x \cosh y - i \sin x \sinh y\\ \sin z = \sin x \cosh y + i \cos x \sinh y$$ So $$|\cos z\ |^2 + |\sin z\ |^2 = \cos ^2 x \cosh ^2 y + \sin ^2 x \sinh ^2 y + \cosh^2 y \sin^2x + \cos ^2x \sinh ^2 y\\ = \cos ^2x ( \cosh ^2 y + \sinh ^2 y) + \sin ^2 x (\cosh^2 y + \sinh^2 y) \\ = \cos 2y \leq 1.$$ I'm not sure where I went wrong, and a numerical check on Wolfram Alpha shows that the inequality should be $\geq$ as suggested.",,"['analysis', 'trigonometry']"
71,Metric space consisting two elements.,Metric space consisting two elements.,,"Suppose that $S$ be a set consisting exactly $2$ elements. Suppose we define a function $\displaystyle d:S \times S \to [0,\infty)$ by $\displaystyle d(x,y)=\begin{cases}1 &\text{ , if }x\not=y\\0 &\text{ , if} x=y\end{cases}$ How I can show that $d$ defines a metric on $S$ ? Problem is on triangular inequality..To prove triangular inequality we need at least three points. How I can show the triangular inequality ? Same problem for a set consisting only $1$-element or empty set. What's the idea behind these ?","Suppose that $S$ be a set consisting exactly $2$ elements. Suppose we define a function $\displaystyle d:S \times S \to [0,\infty)$ by $\displaystyle d(x,y)=\begin{cases}1 &\text{ , if }x\not=y\\0 &\text{ , if} x=y\end{cases}$ How I can show that $d$ defines a metric on $S$ ? Problem is on triangular inequality..To prove triangular inequality we need at least three points. How I can show the triangular inequality ? Same problem for a set consisting only $1$-element or empty set. What's the idea behind these ?",,"['real-analysis', 'general-topology', 'analysis', 'metric-spaces']"
72,"Neglect $1/2 \ln(2\pi n)$ in Stirlings approximation formula, but this term is not bounded or gets smaller, but larger","Neglect  in Stirlings approximation formula, but this term is not bounded or gets smaller, but larger",1/2 \ln(2\pi n),"One form of Stirlings approximation reads $$  \ln(n!) \approx n\ln(n) - n + \frac{1}{2} \ln(2\pi n) $$ another $$  \ln(n!) \approx n\ln n - n. $$ But thats makes me wonder, for the difference of both is $\frac{1}{2}\ln(2\pi n)$, which gets arbitrary large (surely very slow, but it is not bounded...), so the error between both approximations gets larger and larger for $n \to \infty$, but it is not the point of an approximation formula to give a lower error for $n \to \infty$? So, in what sense is the second approximation valid, when the difference between both terms nonetheless becomes larger and larger for $n\to \infty$? Could anybody please explain this to me?","One form of Stirlings approximation reads $$  \ln(n!) \approx n\ln(n) - n + \frac{1}{2} \ln(2\pi n) $$ another $$  \ln(n!) \approx n\ln n - n. $$ But thats makes me wonder, for the difference of both is $\frac{1}{2}\ln(2\pi n)$, which gets arbitrary large (surely very slow, but it is not bounded...), so the error between both approximations gets larger and larger for $n \to \infty$, but it is not the point of an approximation formula to give a lower error for $n \to \infty$? So, in what sense is the second approximation valid, when the difference between both terms nonetheless becomes larger and larger for $n\to \infty$? Could anybody please explain this to me?",,"['real-analysis', 'analysis', 'approximation', 'factorial', 'analytic-number-theory']"
73,"Is $f(x)=x^2\cos\left(\frac{1}{x}\right)$ of bounded variation on $[-1,1]$?",Is  of bounded variation on ?,"f(x)=x^2\cos\left(\frac{1}{x}\right) [-1,1]","Is $f(x)=x^2\cos\left(\frac{1}{x}\right)$ with $f(0)=0$ of bounded variation on $[-1,1]$? I know that $g(x)=x\cos\left(\frac{1}{x}\right)$ and $h(x)=x^2\cos\left(\frac{1}{x^2}\right)$ are both NOT of bounded variation, so I'm guessing that $f$ isn't either...? I just don't know which partition to take, since the proofs of the ones for $g$ and $h$ involved expressing the Variation into a harmonic series, which diverges when the number of points in the partition increases, but I don't know how to do the same for $f$. Thanks!","Is $f(x)=x^2\cos\left(\frac{1}{x}\right)$ with $f(0)=0$ of bounded variation on $[-1,1]$? I know that $g(x)=x\cos\left(\frac{1}{x}\right)$ and $h(x)=x^2\cos\left(\frac{1}{x^2}\right)$ are both NOT of bounded variation, so I'm guessing that $f$ isn't either...? I just don't know which partition to take, since the proofs of the ones for $g$ and $h$ involved expressing the Variation into a harmonic series, which diverges when the number of points in the partition increases, but I don't know how to do the same for $f$. Thanks!",,"['real-analysis', 'analysis']"
74,Integral representation of the Digamma function,Integral representation of the Digamma function,,"The digamma function is defined to be $\psi^{(0)}(x)=\frac{d}{dx}ln(\Gamma(x))$, from which we can derive:  $$\psi^{(0)}(x)=\frac{d}{dx}ln(\Gamma(x))=\frac{\Gamma'(x)}{\Gamma(x)}$$  also, $$\frac{d}{dx}\Gamma(x)=\int_{0}^{\infty}\frac{d}{dx}t^{x-1}e^{-t}dt=\int_{0}^{\infty}t^{x-1}ln(t)e^{-t}dt$$ therefore, $$\psi^{(0)}(x)=\frac{\int_{0}^{\infty}t^{x-1}ln(t)e^{-t}dt}{\int_{0}^{\infty}t^{x-1}e^{-t}dt}$$ that's as far as I got, so my question is, is there a way to simply this expression so that the digamma function can be expressed as a single integral, as opposed to a quotient of integrals?","The digamma function is defined to be $\psi^{(0)}(x)=\frac{d}{dx}ln(\Gamma(x))$, from which we can derive:  $$\psi^{(0)}(x)=\frac{d}{dx}ln(\Gamma(x))=\frac{\Gamma'(x)}{\Gamma(x)}$$  also, $$\frac{d}{dx}\Gamma(x)=\int_{0}^{\infty}\frac{d}{dx}t^{x-1}e^{-t}dt=\int_{0}^{\infty}t^{x-1}ln(t)e^{-t}dt$$ therefore, $$\psi^{(0)}(x)=\frac{\int_{0}^{\infty}t^{x-1}ln(t)e^{-t}dt}{\int_{0}^{\infty}t^{x-1}e^{-t}dt}$$ that's as far as I got, so my question is, is there a way to simply this expression so that the digamma function can be expressed as a single integral, as opposed to a quotient of integrals?",,"['calculus', 'analysis']"
75,Identity theorem for $\mathbb{R}^n$,Identity theorem for,\mathbb{R}^n,"This question is follow up to an interesting question I found here . The results of this question states the following: If $U$ is a domain, and $f,g$ are two real-analytic functions defined   on $U$, and if $V\subset U$ is a nonempty open set with $f\lvert_V  \equiv g\lvert_V$, then $f \equiv g$. If the domain is one-dimensional   (an interval in $\mathbb{R}$), then it suffices that $f\lvert_M \equiv  g\lvert_M$ for some $M\subset U$ that has an accumulation point in   $U$. I have a few question about this theorem: I was looking for a reference for this. However, in the previously pointed out reference A Primer of Real Analytic Functions by Krantz and Parks, I was not able to locate this theorem.  I would appriciate if someone could point me to proper reference of this theorem. My main question, is about the second part of the theorem.  Specifically, I am interested in why there is such a difference going from $\mathbb{R}$ to $\mathbb{R}^n$. That is in one-dimension we can assume that $M$ is just a set with an accumulation point, but in $\mathbb{R}^n$ we have to assume that $M$ is an open set.  I would really like see a counter example that demonstraes that assuming that $M$ is a set with accumulation points is not sufficient in $\mathbb{R}^n$. I would really like for you to speculate or suggest an extra assumptions on functions $f$ and $g$ such that it suffices to consider $M$ to be only a set with an accumulation point.","This question is follow up to an interesting question I found here . The results of this question states the following: If $U$ is a domain, and $f,g$ are two real-analytic functions defined   on $U$, and if $V\subset U$ is a nonempty open set with $f\lvert_V  \equiv g\lvert_V$, then $f \equiv g$. If the domain is one-dimensional   (an interval in $\mathbb{R}$), then it suffices that $f\lvert_M \equiv  g\lvert_M$ for some $M\subset U$ that has an accumulation point in   $U$. I have a few question about this theorem: I was looking for a reference for this. However, in the previously pointed out reference A Primer of Real Analytic Functions by Krantz and Parks, I was not able to locate this theorem.  I would appriciate if someone could point me to proper reference of this theorem. My main question, is about the second part of the theorem.  Specifically, I am interested in why there is such a difference going from $\mathbb{R}$ to $\mathbb{R}^n$. That is in one-dimension we can assume that $M$ is just a set with an accumulation point, but in $\mathbb{R}^n$ we have to assume that $M$ is an open set.  I would really like see a counter example that demonstraes that assuming that $M$ is a set with accumulation points is not sufficient in $\mathbb{R}^n$. I would really like for you to speculate or suggest an extra assumptions on functions $f$ and $g$ such that it suffices to consider $M$ to be only a set with an accumulation point.",,"['real-analysis', 'analysis', 'analytic-functions']"
76,Closed form for definite integral involving Erf and Gaussian?,Closed form for definite integral involving Erf and Gaussian?,,"Question Is there a closed form for integrals such as $\int_{-\infty }^{\infty } e^{-y^2} \text{erf}(1-y) \, dy$ The integrant seems simple enough?","Question Is there a closed form for integrals such as $\int_{-\infty }^{\infty } e^{-y^2} \text{erf}(1-y) \, dy$ The integrant seems simple enough?",,"['integration', 'analysis']"
77,How to evaluate a truncated binomial series in the infinity limit,How to evaluate a truncated binomial series in the infinity limit,,"For a standard binomial series, if it is truncated in the following way: $$\sum_{k=0}^{n'}{n \choose k}(1-x)^{n-k}x^k$$ with $n'<n$, say e.g. $n'=n/2$  what is the behavior of the truncated series in the limit of $n$ and $n'$ goes to infinity? Numerical calculations seem to suggest that it will give a step function. How to show this analytically?","For a standard binomial series, if it is truncated in the following way: $$\sum_{k=0}^{n'}{n \choose k}(1-x)^{n-k}x^k$$ with $n'<n$, say e.g. $n'=n/2$  what is the behavior of the truncated series in the limit of $n$ and $n'$ goes to infinity? Numerical calculations seem to suggest that it will give a step function. How to show this analytically?",,"['calculus', 'sequences-and-series', 'analysis']"
78,"$\frac{1}{\sin x}-\frac{1}{x}$ bounded on $[0,\pi/2]$.",bounded on .,"\frac{1}{\sin x}-\frac{1}{x} [0,\pi/2]","Why is $$\frac{1}{\sin x}-\frac{1}{x}$$ bounded when $x\in [0,\pi/2]$. I've come across this fact in Fourier series, but I can't figure out a why this is true. I would appreciate any help.","Why is $$\frac{1}{\sin x}-\frac{1}{x}$$ bounded when $x\in [0,\pi/2]$. I've come across this fact in Fourier series, but I can't figure out a why this is true. I would appreciate any help.",,"['calculus', 'analysis']"
79,"Show that if $f$ is lower continuous then $f^{-1}((\alpha,\infty))$ is open",Show that if  is lower continuous then  is open,"f f^{-1}((\alpha,\infty))","Let a function $f:X\to\Bbb R$, where $X$ is a metric space. Then $f$ is lower continuous if for all $a\in X$ we have that $f(a)\le \liminf f(x_n)$ for every $(x_n)\to a$. Alternatively we can say that $f$ is lower continuous if for all $\epsilon >0$ exists some $\delta>0$ such that $$x\in\Bbb B(a,\delta)\implies f(a)-f(x)<\epsilon$$ Now I must prove that for any $\alpha\in\Bbb R$ the preimage of $(\alpha,\infty)$ is open. What I tried is set $f(a)=\alpha$, but from this approach I cant conclude that the preimage of $(\alpha,\infty)$ is open. At most I can conclude that for any $\epsilon>0$ exists a ball $\Bbb B(a,\delta)$ where some of it images belong to some set of the kind $(\alpha,\beta)$. Some hint or solution will be appreciated, thank you.","Let a function $f:X\to\Bbb R$, where $X$ is a metric space. Then $f$ is lower continuous if for all $a\in X$ we have that $f(a)\le \liminf f(x_n)$ for every $(x_n)\to a$. Alternatively we can say that $f$ is lower continuous if for all $\epsilon >0$ exists some $\delta>0$ such that $$x\in\Bbb B(a,\delta)\implies f(a)-f(x)<\epsilon$$ Now I must prove that for any $\alpha\in\Bbb R$ the preimage of $(\alpha,\infty)$ is open. What I tried is set $f(a)=\alpha$, but from this approach I cant conclude that the preimage of $(\alpha,\infty)$ is open. At most I can conclude that for any $\epsilon>0$ exists a ball $\Bbb B(a,\delta)$ where some of it images belong to some set of the kind $(\alpha,\beta)$. Some hint or solution will be appreciated, thank you.",,"['general-topology', 'analysis', 'continuity']"
80,Sigma finite measure positive on uncountable subset of the reals,Sigma finite measure positive on uncountable subset of the reals,,"We can construct a sigma finite (finite even!) measure $\mu$ which assigns a positive measure to every member of a countable subset $A\subset \mathbb R$ by writing $$A=\{q_1, q_2, ...\}$$ and defining $$\mu(B)=\sum_{q_n\in A}\frac{1}{2^n}$$ Can we construct a sigma-finite measure on $\mathcal P (\mathbb R)$ which is positive on every member of some uncountable subset of $\mathbb R$. Intuition tells me that we shouldn't be able to since this will create a ""too big mass of measure"" which won't allow sigma finitness. If we can't construct such a measure, how can we prove it is impossible?","We can construct a sigma finite (finite even!) measure $\mu$ which assigns a positive measure to every member of a countable subset $A\subset \mathbb R$ by writing $$A=\{q_1, q_2, ...\}$$ and defining $$\mu(B)=\sum_{q_n\in A}\frac{1}{2^n}$$ Can we construct a sigma-finite measure on $\mathcal P (\mathbb R)$ which is positive on every member of some uncountable subset of $\mathbb R$. Intuition tells me that we shouldn't be able to since this will create a ""too big mass of measure"" which won't allow sigma finitness. If we can't construct such a measure, how can we prove it is impossible?",,"['analysis', 'measure-theory']"
81,Is it true that $\lim_{n\to \infty} \dfrac{\int_{\frac{1}{n}}^{1}\frac{f(x)}{x}dx}{n}=0$?,Is it true that ?,\lim_{n\to \infty} \dfrac{\int_{\frac{1}{n}}^{1}\frac{f(x)}{x}dx}{n}=0,"Suppose $f$ is $L^{1}[0,\infty)$ and non-negative, and $\int_{0}^{\infty}f(x)dx=1$. Is it true that $lim_{n\to \infty} \dfrac{\int_{\frac{1}{n}}^{1}\frac{f(x)}{x}dx}{n}=0$? I derive this problem from a probability problem where I am asked to compute a limit of an expectation and $f$ is the probability density function. However, I cannot make an estimate for this integral. Thanks!","Suppose $f$ is $L^{1}[0,\infty)$ and non-negative, and $\int_{0}^{\infty}f(x)dx=1$. Is it true that $lim_{n\to \infty} \dfrac{\int_{\frac{1}{n}}^{1}\frac{f(x)}{x}dx}{n}=0$? I derive this problem from a probability problem where I am asked to compute a limit of an expectation and $f$ is the probability density function. However, I cannot make an estimate for this integral. Thanks!",,"['real-analysis', 'probability', 'analysis', 'probability-distributions']"
82,Graph of a continuous function has measure zero in $\mathbb{R}^2$,Graph of a continuous function has measure zero in,\mathbb{R}^2,"I've been working on the following exercise and can't quite get it. If anyone has any suggestions, please let me know. Let $f \colon \mathbb{R} \to \mathbb{R}$ be continuous. Show that the graph of the function, $G_f = \{(x,f(x)) \colon x \in \mathbb{R}\}$ is a set of measure zero in $\mathbb{R}^2$. My intuition:  Consider writing $\mathbb{R} = \bigcup_{n \in \mathbb{Z} } [n,n+1]$, and note that since $f$ is continuous on $\mathbb{R}$, it's uniformly continuous on each compact interval comprising the union mentioned above. Thus, given any $\epsilon > 0$ there exists a $\delta > 0$ such that $|f(x) - f(y)| < \epsilon$ whenever $|x-y| < \delta$ for all $x,y \in [n,n+1]$. I would like to use this idea, however, I'm not able to cook up a clean way to handle this when I union over all $n.$. :/","I've been working on the following exercise and can't quite get it. If anyone has any suggestions, please let me know. Let $f \colon \mathbb{R} \to \mathbb{R}$ be continuous. Show that the graph of the function, $G_f = \{(x,f(x)) \colon x \in \mathbb{R}\}$ is a set of measure zero in $\mathbb{R}^2$. My intuition:  Consider writing $\mathbb{R} = \bigcup_{n \in \mathbb{Z} } [n,n+1]$, and note that since $f$ is continuous on $\mathbb{R}$, it's uniformly continuous on each compact interval comprising the union mentioned above. Thus, given any $\epsilon > 0$ there exists a $\delta > 0$ such that $|f(x) - f(y)| < \epsilon$ whenever $|x-y| < \delta$ for all $x,y \in [n,n+1]$. I would like to use this idea, however, I'm not able to cook up a clean way to handle this when I union over all $n.$. :/",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
83,Convergent Fourier series of continuous function,Convergent Fourier series of continuous function,,Let $f$ be a continuous function. It is known that its Fourier series is convergent almost everywhere to $f$ and it may fail to converge on some measure zero set. However I would like to know whether one can find a continuous function $f$ with the property that its Fourier series is convergent everywhere but not to $f$ (in other words for each $x$ the partial sums $S_N(x)$ converge to $S(x)$ and there are some points $x$ such that $S_N(x)$ converges to $S(x)\neq f(x)$).,Let $f$ be a continuous function. It is known that its Fourier series is convergent almost everywhere to $f$ and it may fail to converge on some measure zero set. However I would like to know whether one can find a continuous function $f$ with the property that its Fourier series is convergent everywhere but not to $f$ (in other words for each $x$ the partial sums $S_N(x)$ converge to $S(x)$ and there are some points $x$ such that $S_N(x)$ converges to $S(x)\neq f(x)$).,,"['analysis', 'fourier-analysis', 'fourier-series']"
84,Convergence of the integral $\int_0^\infty e^{-x^3} \mathop{\mathrm{d}x}$,Convergence of the integral,\int_0^\infty e^{-x^3} \mathop{\mathrm{d}x},"I want to test the convergence of the integral $\int_0^\infty e^{-x^3} \mathop{\mathrm{d}x}$. There are some parts of the solution which does not make sense to me, I'm hoping that someone can explain it to me. The solution says: The integrals $\int_0^\infty e^{-x^3} \mathop{\mathrm{d}x}$ and $\int_1^\infty e^{-x^3} \mathop{\mathrm{d}x}$ diverge and converge simultaneously. Therefore, $$\int_1^\infty e^{-x^3} \mathop{\mathrm{d}x} \leq \int_1^\infty e^{-x} \mathop{\mathrm{d}x} = 1.$$   We conclude that $\int_0^\infty e^{-x^3} \mathop{\mathrm{d}x}$ converges by the comparison principle of Question $11$. For reference, Question $11$ says: Let $f$, $F$ be continuous on $(a,b)$. If the improper integral $\int^b_a F(x) \mathop{\mathrm{d}x}$ converges and $|f(x)| \leq F(x)$ for all $x:a < x < b$, then the improper integral $\int^b_a f(x)$ converges as well. Here $-\infty \leq a < b \leq \infty$. I really feel that the solution has made an error somewhere. $1.$ What does it mean when it says that the integrals diverge and converge simultaneously? $2.$ How does the inequality say anything about the convergence of $\int_0^\infty e^{-x^3} \mathop{\mathrm{d}x}$?","I want to test the convergence of the integral $\int_0^\infty e^{-x^3} \mathop{\mathrm{d}x}$. There are some parts of the solution which does not make sense to me, I'm hoping that someone can explain it to me. The solution says: The integrals $\int_0^\infty e^{-x^3} \mathop{\mathrm{d}x}$ and $\int_1^\infty e^{-x^3} \mathop{\mathrm{d}x}$ diverge and converge simultaneously. Therefore, $$\int_1^\infty e^{-x^3} \mathop{\mathrm{d}x} \leq \int_1^\infty e^{-x} \mathop{\mathrm{d}x} = 1.$$   We conclude that $\int_0^\infty e^{-x^3} \mathop{\mathrm{d}x}$ converges by the comparison principle of Question $11$. For reference, Question $11$ says: Let $f$, $F$ be continuous on $(a,b)$. If the improper integral $\int^b_a F(x) \mathop{\mathrm{d}x}$ converges and $|f(x)| \leq F(x)$ for all $x:a < x < b$, then the improper integral $\int^b_a f(x)$ converges as well. Here $-\infty \leq a < b \leq \infty$. I really feel that the solution has made an error somewhere. $1.$ What does it mean when it says that the integrals diverge and converge simultaneously? $2.$ How does the inequality say anything about the convergence of $\int_0^\infty e^{-x^3} \mathop{\mathrm{d}x}$?",,"['real-analysis', 'integration', 'analysis', 'convergence-divergence', 'improper-integrals']"
85,How to calculate $\int_{0}^{\infty}e^{-ax}\sin^nx~\mathrm dx$,How to calculate,\int_{0}^{\infty}e^{-ax}\sin^nx~\mathrm dx,"I have no idea how to evaluate  $$\int_{0}^{\infty}e^{-ax}\sin^nx~\mathrm dx$$ In Table of Integrals,Series and Products  3.895 ï¼ŒI found a formula about,but I don't know how to prove it.","I have no idea how to evaluate  $$\int_{0}^{\infty}e^{-ax}\sin^nx~\mathrm dx$$ In Table of Integrals,Series and Products  3.895 ï¼ŒI found a formula about,but I don't know how to prove it.",,"['calculus', 'integration', 'analysis']"
86,What is the minimal correction to the harmonic series such that it converges?,What is the minimal correction to the harmonic series such that it converges?,,"as you all hopefully know, the series $$ \sum_{k\ge 1}\frac{1}{k} $$ diverges. Now I know that you can add some logarithmic corrections, such that it converges: $$ \sum_{k\ge 1}\frac{1}{k\log(k)^2} $$ (this might be wrong, i only remember this faintly). I once saw a wiki page which explained which sort of logarithmic corrections one can (and has to) make in order for the series to converge, does anyone know? Thx!","as you all hopefully know, the series $$ \sum_{k\ge 1}\frac{1}{k} $$ diverges. Now I know that you can add some logarithmic corrections, such that it converges: $$ \sum_{k\ge 1}\frac{1}{k\log(k)^2} $$ (this might be wrong, i only remember this faintly). I once saw a wiki page which explained which sort of logarithmic corrections one can (and has to) make in order for the series to converge, does anyone know? Thx!",,"['real-analysis', 'sequences-and-series', 'analysis', 'power-series']"
87,"Numerical analysis, differential equations, complex analysis for statistics","Numerical analysis, differential equations, complex analysis for statistics",,"I am a student of the Statistics Department. And now I can choose a subject from such list: numerical mathematics (analysis), differential equations, complex analysis, real analysis. Can you please tell me how can we use this courses in statistics and are they important for statistics at all? And which course should I choose?","I am a student of the Statistics Department. And now I can choose a subject from such list: numerical mathematics (analysis), differential equations, complex analysis, real analysis. Can you please tell me how can we use this courses in statistics and are they important for statistics at all? And which course should I choose?",,"['analysis', 'statistics', 'numerical-methods', 'advice', 'learning']"
88,Are compactly supported smooth functions dense in continuous functions?,Are compactly supported smooth functions dense in continuous functions?,,"Let $\Omega\subset \Bbb R^N$ be open(or a domain if needed). Is it true that $C^{\infty}_c(\Omega)$ is dense in $C^0(\Omega)$? Actually, I'm always confused about some sets(especially $C^{\infty}_c$) are dense in other sets. Can there be a easy explanation or insight?","Let $\Omega\subset \Bbb R^N$ be open(or a domain if needed). Is it true that $C^{\infty}_c(\Omega)$ is dense in $C^0(\Omega)$? Actually, I'm always confused about some sets(especially $C^{\infty}_c$) are dense in other sets. Can there be a easy explanation or insight?",,"['real-analysis', 'analysis']"
89,Real Analysis - Uniform Convergence of $f_n$,Real Analysis - Uniform Convergence of,f_n,"I am given that: For $n \in \mathbb{N}$, define $f_n: \mathbb{R} \to \mathbb{R}$ by $$f_n(x)=\frac{x^{4n}}{4+x^{4n}}.$$ I need to determine whether the sequence $(f_n)$ converges uniformly on $\mathbb{R}$. This is what I have done: \begin{align} \lim_{n \to \infty}f_n(x)&=\lim_{n \to \infty}\frac{x^{4n}}{4+x^{4n}}=     \begin{cases}        0, & \text{if}\ x \in (-1,1) \\       \frac15, & \text{if}\ x \in \{-1,1\} \\       1, & \text{if}\ x \in \mathbb{R}\setminus[-1,1]     \end{cases} = f(x) \end{align} Now I am struggling to show whether or not $f_n$ converges uniformly to $f$ over $\mathbb{R}$.","I am given that: For $n \in \mathbb{N}$, define $f_n: \mathbb{R} \to \mathbb{R}$ by $$f_n(x)=\frac{x^{4n}}{4+x^{4n}}.$$ I need to determine whether the sequence $(f_n)$ converges uniformly on $\mathbb{R}$. This is what I have done: \begin{align} \lim_{n \to \infty}f_n(x)&=\lim_{n \to \infty}\frac{x^{4n}}{4+x^{4n}}=     \begin{cases}        0, & \text{if}\ x \in (-1,1) \\       \frac15, & \text{if}\ x \in \{-1,1\} \\       1, & \text{if}\ x \in \mathbb{R}\setminus[-1,1]     \end{cases} = f(x) \end{align} Now I am struggling to show whether or not $f_n$ converges uniformly to $f$ over $\mathbb{R}$.",,"['real-analysis', 'analysis', 'convergence-divergence', 'uniform-convergence']"
90,how to estimate $\prod_{k=2}^n \log(k)$?,how to estimate ?,\prod_{k=2}^n \log(k),"I wonder if I can estimate $\prod_{k=2}^n \log(k)$ as $a^l$ for some a. I know that it is bounded by $e^{n^2}$, but I would like to get something finer.","I wonder if I can estimate $\prod_{k=2}^n \log(k)$ as $a^l$ for some a. I know that it is bounded by $e^{n^2}$, but I would like to get something finer.",,"['real-analysis', 'combinatorics', 'analysis', 'products', 'infinite-product']"
91,Prove $\{u_n\}$ converges to $\sqrt[3]{u_2^2 u_1}$ without using subsequences. [duplicate],Prove  converges to  without using subsequences. [duplicate],\{u_n\} \sqrt[3]{u_2^2 u_1},"This question already has answers here : Compute $\lim_\limits{n\to\infty}a_n$ where $a_{n+2}=\sqrt{a_n.a_{n+1}}$ (4 answers) Closed 8 years ago . Prove that the sequence $\{u_n\}$ is defined by $0\lt u_1\lt u_2 \;\text {and} \;u_{n+2}=\sqrt {u_{n+1}u_n}\; \text{for}\; n\ge 1$, converges to $\sqrt[3]{u_1{u_2}^2}$. I did this using subsequences, I posted below. This kind of problems are usually done by monotonicity and boundedness. So, I suspect if their is a solution to this problem using this method. I answered this in this method here .","This question already has answers here : Compute $\lim_\limits{n\to\infty}a_n$ where $a_{n+2}=\sqrt{a_n.a_{n+1}}$ (4 answers) Closed 8 years ago . Prove that the sequence $\{u_n\}$ is defined by $0\lt u_1\lt u_2 \;\text {and} \;u_{n+2}=\sqrt {u_{n+1}u_n}\; \text{for}\; n\ge 1$, converges to $\sqrt[3]{u_1{u_2}^2}$. I did this using subsequences, I posted below. This kind of problems are usually done by monotonicity and boundedness. So, I suspect if their is a solution to this problem using this method. I answered this in this method here .",,"['real-analysis', 'sequences-and-series']"
92,"Is $f(x) = x^3 \sin \frac{1}{x} $ uniformly continuous on $(0, \infty)$?",Is  uniformly continuous on ?,"f(x) = x^3 \sin \frac{1}{x}  (0, \infty)","Since the derivitive of $f$ is bounded on a neighborhood of $0$, $f$ is uniformly continuous on $(0, M)$ where $M$ is any positive number. I'd like to prove that $f$ is uniformly continuous on a neighborhood of $\infty$. The derivity of $f$ goes to $\infty$ as $x \rightarrow \infty$. So I guess $f$ is not uniforly continuous on $(0, \infty)$. I 'll appreciate it if you give me a proof.","Since the derivitive of $f$ is bounded on a neighborhood of $0$, $f$ is uniformly continuous on $(0, M)$ where $M$ is any positive number. I'd like to prove that $f$ is uniformly continuous on a neighborhood of $\infty$. The derivity of $f$ goes to $\infty$ as $x \rightarrow \infty$. So I guess $f$ is not uniforly continuous on $(0, \infty)$. I 'll appreciate it if you give me a proof.",,"['real-analysis', 'analysis', 'uniform-continuity']"
93,Combining Fubini and Tonelli's in one single Assumption,Combining Fubini and Tonelli's in one single Assumption,,"I am referring to the statements on Wikipedia , there it is said that Fubini's Theorem states that if $f : X\times Y \to \mathbb R$ is integrable, then $$  \int_X \left( \int_Y f(x,y) dy\right) dx =   \int_Y \left( \int_X f(x,y) dx\right) dy =  \int_{X\times Y} f(x,y) d(x,y) $$ and for Tonelli's Theorem instead of supposing integrability, we just assume $f$ to be non-negative $f \ge 0$, and then the above equalities hold. Then later it is written that Fubini and Tonelli could be combined, meaning that if one of the following conditions \begin{align*}  \int_X \left( \int_Y |f(x,y)| dy\right) dx & < \infty \\   \int_Y \left( \int_X |f(x,y)| dx\right) dy & < \infty \\  \int_{X\times Y} |f(x,y)| d(x,y) & < \infty \end{align*} hold, then the above formula for interated integrals is valid. I do not understand in what sense this entails Tonelli's condition of non-negativity? For example take $f(x,y) = \max\{0, x\}, X = Y = \mathbb R$, then Tonelli's Theorem applies, and all integrals have value $\infty$, but none of the above mentioned integrals is finite, so I think this condition does not incorporate Tonelli's Theorem??","I am referring to the statements on Wikipedia , there it is said that Fubini's Theorem states that if $f : X\times Y \to \mathbb R$ is integrable, then $$  \int_X \left( \int_Y f(x,y) dy\right) dx =   \int_Y \left( \int_X f(x,y) dx\right) dy =  \int_{X\times Y} f(x,y) d(x,y) $$ and for Tonelli's Theorem instead of supposing integrability, we just assume $f$ to be non-negative $f \ge 0$, and then the above equalities hold. Then later it is written that Fubini and Tonelli could be combined, meaning that if one of the following conditions \begin{align*}  \int_X \left( \int_Y |f(x,y)| dy\right) dx & < \infty \\   \int_Y \left( \int_X |f(x,y)| dx\right) dy & < \infty \\  \int_{X\times Y} |f(x,y)| d(x,y) & < \infty \end{align*} hold, then the above formula for interated integrals is valid. I do not understand in what sense this entails Tonelli's condition of non-negativity? For example take $f(x,y) = \max\{0, x\}, X = Y = \mathbb R$, then Tonelli's Theorem applies, and all integrals have value $\infty$, but none of the above mentioned integrals is finite, so I think this condition does not incorporate Tonelli's Theorem??",,"['integration', 'analysis', 'measure-theory']"
94,"Little-o, Big-O and differentiation","Little-o, Big-O and differentiation",,"The functions $f,g, h: \mathbb{R} \rightarrow \mathbb{R}$ have the origin 0 as an internal point of their domain. Prove that if $f = \mathcal{O}(x^{k})$, $f = \mathcal{o}(x^{k-1})$ Prove that if $g = \mathcal{o}(x)$, g is differentiable in 0. Next assume that for $m \in \mathbb{N}_{\leq 1}$, $g = \mathcal{o}(x^{m})$ and $g$ is $m$ times differentiable. Calculate $g^{(k)}(0)$ for $k \in \{0, 1, ..., m\}$ Assume $h$ is $n+1$ times differentiable with $h^{(n+1)}$ continuous and $h = \mathcal{o}(x^{n})$. Prove that $h = \mathcal{O}(x^{n+1})$. What I have tried is the following. As $f = \mathcal{O}(x^{k})$, there exists an open interval $I \subset Dom(f)$ with $0 \in I$ and $C>0$ so that if $x \in I$, $0 \leq |f(x)| \leq C |x|^{k}$. Using the squeeze theorem it follows that $\lim_{x \rightarrow 0} f(x) = 0$, and because $0$ is an internal point of the domain of $f$, $f(0)=0$ so that the first condition for $f$ to be an element of $\mathcal{o}(x^{k-1})$ has been met. Next observe that $I = ]a,b[$ for some $a, b \in \mathbb{R}$ with $a < b$. Because $0 \in I$, $a < 0 < b$. Let $\delta < min\{|a|,|b|,1\}$ and name $D = ]-\delta,\delta[$. It follows that $0 \in D \subset I$. For $x \in D$, $|x| < 1$ and $|x|^{k} < |x|^{k-1}$, so $|h(x)| \leq C|x|^{k} \leq C|x|^{k-1}$. Is this the right direction to go in? Because I cant find the answer from here. Because $g = \mathcal{o}(x)$, g(0) = 0 and $lim_{x \rightarrow 0} \frac{g(x)}{|x|} = 0$. Then $lim_{x \rightarrow 0} \frac{g(x)}{|x|} = lim_{x \rightarrow 0} \frac{g(x)-g(x)}{|x|-0} = lim_{x \rightarrow 0} \frac{g(x)-g(x)}{x-0} = g'(0) = 0$. Doesnt it follow from this that every higher derivative of $g$ also equals 0? Nothing much, cause I have no idea how to start. Any help is greatly appreciated. edit : The definitions used are: $f = \mathcal{o}(x^{k}) \leftrightarrow f(0) = 0, \lim_{x \rightarrow 0} \frac{f(x)}{|x|^{k}} = 0$ $f = \mathcal{O}(x^{k}) \leftrightarrow \forall x \in I, |f(x)| \leq C|x|^{k}$ For $I \subset Dom(f)$ open containing $0$, and $C > 0$.","The functions $f,g, h: \mathbb{R} \rightarrow \mathbb{R}$ have the origin 0 as an internal point of their domain. Prove that if $f = \mathcal{O}(x^{k})$, $f = \mathcal{o}(x^{k-1})$ Prove that if $g = \mathcal{o}(x)$, g is differentiable in 0. Next assume that for $m \in \mathbb{N}_{\leq 1}$, $g = \mathcal{o}(x^{m})$ and $g$ is $m$ times differentiable. Calculate $g^{(k)}(0)$ for $k \in \{0, 1, ..., m\}$ Assume $h$ is $n+1$ times differentiable with $h^{(n+1)}$ continuous and $h = \mathcal{o}(x^{n})$. Prove that $h = \mathcal{O}(x^{n+1})$. What I have tried is the following. As $f = \mathcal{O}(x^{k})$, there exists an open interval $I \subset Dom(f)$ with $0 \in I$ and $C>0$ so that if $x \in I$, $0 \leq |f(x)| \leq C |x|^{k}$. Using the squeeze theorem it follows that $\lim_{x \rightarrow 0} f(x) = 0$, and because $0$ is an internal point of the domain of $f$, $f(0)=0$ so that the first condition for $f$ to be an element of $\mathcal{o}(x^{k-1})$ has been met. Next observe that $I = ]a,b[$ for some $a, b \in \mathbb{R}$ with $a < b$. Because $0 \in I$, $a < 0 < b$. Let $\delta < min\{|a|,|b|,1\}$ and name $D = ]-\delta,\delta[$. It follows that $0 \in D \subset I$. For $x \in D$, $|x| < 1$ and $|x|^{k} < |x|^{k-1}$, so $|h(x)| \leq C|x|^{k} \leq C|x|^{k-1}$. Is this the right direction to go in? Because I cant find the answer from here. Because $g = \mathcal{o}(x)$, g(0) = 0 and $lim_{x \rightarrow 0} \frac{g(x)}{|x|} = 0$. Then $lim_{x \rightarrow 0} \frac{g(x)}{|x|} = lim_{x \rightarrow 0} \frac{g(x)-g(x)}{|x|-0} = lim_{x \rightarrow 0} \frac{g(x)-g(x)}{x-0} = g'(0) = 0$. Doesnt it follow from this that every higher derivative of $g$ also equals 0? Nothing much, cause I have no idea how to start. Any help is greatly appreciated. edit : The definitions used are: $f = \mathcal{o}(x^{k}) \leftrightarrow f(0) = 0, \lim_{x \rightarrow 0} \frac{f(x)}{|x|^{k}} = 0$ $f = \mathcal{O}(x^{k}) \leftrightarrow \forall x \in I, |f(x)| \leq C|x|^{k}$ For $I \subset Dom(f)$ open containing $0$, and $C > 0$.",,"['real-analysis', 'analysis']"
95,Measure of intersection of set and its translation,Measure of intersection of set and its translation,,"I came across an old qualifying exam question: Let $A\subset [0,1)$ be a Lebesgue measurable subset of unit intreval such that $0<\mu(A)<1$. For every $x\in [0,1)$ let $A+x=\{x+y$ mod 1$:y\in A\}$. Prove that there exists $x_0=x_0(A)\in[0,1)$ such that $\mu(A\cap(A+x_0))<\mu(A)$ Intuitively, it seems that if $A$ is a measurable interval then there exist an interval $I$ such that they overlap by more than half. Moreover the intersection $(A\cap(A+x_0))$ would need less intervals to ""cover up"" than $A$. Hence we have the inequality. But I am having a hard time making this rigorous.","I came across an old qualifying exam question: Let $A\subset [0,1)$ be a Lebesgue measurable subset of unit intreval such that $0<\mu(A)<1$. For every $x\in [0,1)$ let $A+x=\{x+y$ mod 1$:y\in A\}$. Prove that there exists $x_0=x_0(A)\in[0,1)$ such that $\mu(A\cap(A+x_0))<\mu(A)$ Intuitively, it seems that if $A$ is a measurable interval then there exist an interval $I$ such that they overlap by more than half. Moreover the intersection $(A\cap(A+x_0))$ would need less intervals to ""cover up"" than $A$. Hence we have the inequality. But I am having a hard time making this rigorous.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
96,A function whose absolute value is Lebesgue integrable,A function whose absolute value is Lebesgue integrable,,I'm looking for a function f which is not Lebesgue integrable but |f| is integrable or can we say that such function does not exist?,I'm looking for a function f which is not Lebesgue integrable but |f| is integrable or can we say that such function does not exist?,,"['analysis', 'lebesgue-integral', 'examples-counterexamples', 'absolute-value']"
97,Integral over a sequence of sets whose measures $\to 0.$,Integral over a sequence of sets whose measures,\to 0.,"If $ f \in L_p$ with $1 \leq p \leq \infty $ and ${A_n}$ is a sequence of measurable sets such that $ \mu (An) \rightarrow 0,$ then $ \int_{A_n} f \rightarrow 0$. Can someone give me a hint?","If $ f \in L_p$ with $1 \leq p \leq \infty $ and ${A_n}$ is a sequence of measurable sets such that $ \mu (An) \rightarrow 0,$ then $ \int_{A_n} f \rightarrow 0$. Can someone give me a hint?",,"['real-analysis', 'analysis']"
98,How to rewrite $\cos{2n\theta}$ as a summation of $\sin\theta$,How to rewrite  as a summation of,\cos{2n\theta} \sin\theta,I want to rewrite $\cos{2 n \theta}$ as  $$ \cos{2 n \theta}=\sum_{m=0}^{M} a_m \sin^m\theta $$ How to determine $M$ and coefficients $a_m$. Any comment is much appreciated. Many thanks in advance.,I want to rewrite $\cos{2 n \theta}$ as  $$ \cos{2 n \theta}=\sum_{m=0}^{M} a_m \sin^m\theta $$ How to determine $M$ and coefficients $a_m$. Any comment is much appreciated. Many thanks in advance.,,"['calculus', 'analysis']"
99,Divergent sequence with decreasing function,Divergent sequence with decreasing function,,"Let $f: \mathbb{R} \to (0,\infty)$ be a decreasing function. Define a sequence $(a_n)$ by $a_1=1   $ and $a_{n+1}=a_n+f(a_n)$ for every $n\ge 1$. Prove that $(a_n) \to \infty$. I have tried by contradiction to assume it is bounded and therefore converges. clearly the $a_n$ are strictly increasing and I tried to use Cauchy definition to arrive at a contradiction that $f$ is not decreasing but could not process when having to choose epsilons and which $N$. What can be fixed etc. Any help?","Let $f: \mathbb{R} \to (0,\infty)$ be a decreasing function. Define a sequence $(a_n)$ by $a_1=1   $ and $a_{n+1}=a_n+f(a_n)$ for every $n\ge 1$. Prove that $(a_n) \to \infty$. I have tried by contradiction to assume it is bounded and therefore converges. clearly the $a_n$ are strictly increasing and I tried to use Cauchy definition to arrive at a contradiction that $f$ is not decreasing but could not process when having to choose epsilons and which $N$. What can be fixed etc. Any help?",,"['real-analysis', 'sequences-and-series', 'analysis', 'cauchy-sequences']"
