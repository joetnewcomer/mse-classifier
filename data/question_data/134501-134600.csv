,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to solve this first-order nonlinear differential equation?,How to solve this first-order nonlinear differential equation?,,"1) I'm wondering how one might solve this differential equation: (here $c$ is a real constant) $$ c = y - \frac{x}{2} \left( y' - \frac{1}{y'}\right).$$ WolframAlpha says it is $y = \frac{1}{2}\left( 2c \pm e^{-k}x^2 \mp e^{k}\right)$, and I expected to find an $x^2$ in there based off of where the equation comes from, but I'm not super experienced with nonlinear DE's. ( For those that are curious: lets say that a ray comes from above $x_0$ parallel to the $y$-axis, hits the graph of $y = f(x)$, and the angle between the ray and the tangent line at $x_0$ was equal to the angle between the reflected line and the tangent line---as if the ray were a pool ball and the tangent line were the side of the table. Then the reflected line has a $y$-intercept of $c$. ) 2) I'm curious if anyone might have some advice for solving differential equations of this form? $$ g(x) = y - \frac{x}{2} \left( y' - \frac{1}{y'}\right).$$ Thank you!","1) I'm wondering how one might solve this differential equation: (here $c$ is a real constant) $$ c = y - \frac{x}{2} \left( y' - \frac{1}{y'}\right).$$ WolframAlpha says it is $y = \frac{1}{2}\left( 2c \pm e^{-k}x^2 \mp e^{k}\right)$, and I expected to find an $x^2$ in there based off of where the equation comes from, but I'm not super experienced with nonlinear DE's. ( For those that are curious: lets say that a ray comes from above $x_0$ parallel to the $y$-axis, hits the graph of $y = f(x)$, and the angle between the ray and the tangent line at $x_0$ was equal to the angle between the reflected line and the tangent line---as if the ray were a pool ball and the tangent line were the side of the table. Then the reflected line has a $y$-intercept of $c$. ) 2) I'm curious if anyone might have some advice for solving differential equations of this form? $$ g(x) = y - \frac{x}{2} \left( y' - \frac{1}{y'}\right).$$ Thank you!",,['ordinary-differential-equations']
1,Limit of function as $x \to\infty $ when $f'(x)$ is given,Limit of function as  when  is given,x \to\infty  f'(x),"Let $f:[1,\infty)\rightarrow \mathbb R $ is a differentiable function which satisfies $$f'(x)=\frac {1}{x^2+(f (x))^2}  \text{ and } f(1)=1$$ then find the limit of $f $ as $x \to\infty $ My attempt : So I first thought of making a differential equation and then calculate the limit. But the differential equation formed $y'(y^2+x^2)=1$ is a non standard equation and it cannot be solved. Even calculators on the internet show that ""no solution found"". However, Wolfram Alpha does provide a graph but not the solution. Also $f^{\prime\prime}(x)<0$. Can this fact be used in some way? Next I thought of using Rolle's theorem but I am unable to figure out some way to use it. Can anyone provide me some idea on how to approach this problem?","Let $f:[1,\infty)\rightarrow \mathbb R $ is a differentiable function which satisfies $$f'(x)=\frac {1}{x^2+(f (x))^2}  \text{ and } f(1)=1$$ then find the limit of $f $ as $x \to\infty $ My attempt : So I first thought of making a differential equation and then calculate the limit. But the differential equation formed $y'(y^2+x^2)=1$ is a non standard equation and it cannot be solved. Even calculators on the internet show that ""no solution found"". However, Wolfram Alpha does provide a graph but not the solution. Also $f^{\prime\prime}(x)<0$. Can this fact be used in some way? Next I thought of using Rolle's theorem but I am unable to figure out some way to use it. Can anyone provide me some idea on how to approach this problem?",,"['ordinary-differential-equations', 'derivatives']"
2,Cauchy Problem (PDE),Cauchy Problem (PDE),,"Find the solution to the Cauchy data problem $$\frac{\partial u}{\partial t} − u\frac{\partial u}{\partial x} = −2u$$ where $u(x, 0) = x$ . I know how to solve homogeneous Cauchy problems. However, I am struggling to understand non-homogeneous equations like this one. Can somebody please give a detailed solution?","Find the solution to the Cauchy data problem where . I know how to solve homogeneous Cauchy problems. However, I am struggling to understand non-homogeneous equations like this one. Can somebody please give a detailed solution?","\frac{\partial u}{\partial t} − u\frac{\partial u}{\partial x} = −2u u(x, 0) = x","['real-analysis', 'ordinary-differential-equations', 'derivatives', 'partial-differential-equations']"
3,Differential equation $y'=-\frac{x+y}{x+2y}$,Differential equation,y'=-\frac{x+y}{x+2y},"Differential equation. How do you solve this ? $$y'=-\frac{x+y}{x+2y}$$ set $u = \dfrac yx$, then $y' = u'x + u$, $$u'x + u = -\frac{1+u}{1+2u}$$ $$u'x = -\frac{2u^2+2u+1}{1+2u}$$ $$\frac{(1+2u)u'}{2u^2+2u+1} = -\frac1x$$ I solved it so far but I do not know anymore","Differential equation. How do you solve this ? $$y'=-\frac{x+y}{x+2y}$$ set $u = \dfrac yx$, then $y' = u'x + u$, $$u'x + u = -\frac{1+u}{1+2u}$$ $$u'x = -\frac{2u^2+2u+1}{1+2u}$$ $$\frac{(1+2u)u'}{2u^2+2u+1} = -\frac1x$$ I solved it so far but I do not know anymore",,['ordinary-differential-equations']
4,$\int \frac{du}{u-2}=-\int dx $ where's my stupid mistake?,where's my stupid mistake?,\int \frac{du}{u-2}=-\int dx ,"I'm struggling with this very simple ode since it gives me 2 different solutions depending on where I put the minus. from  $$\int \frac{du}{u-2}=-\int dx $$ follows $$ln|u-2|=-x +ln|C|$$ which yields $$u=2+Ce^{-x}$$ Now if I rewerite the 1st equation to $$\int \frac{du}{2-u}=\int dx$$ then $$ln|2-u|=x+ln|C|$$ which gives $$u=2-Ce^x$$ I know, there's probably a pretty stupid mistake, but I just can't see it. any hints?","I'm struggling with this very simple ode since it gives me 2 different solutions depending on where I put the minus. from  $$\int \frac{du}{u-2}=-\int dx $$ follows $$ln|u-2|=-x +ln|C|$$ which yields $$u=2+Ce^{-x}$$ Now if I rewerite the 1st equation to $$\int \frac{du}{2-u}=\int dx$$ then $$ln|2-u|=x+ln|C|$$ which gives $$u=2-Ce^x$$ I know, there's probably a pretty stupid mistake, but I just can't see it. any hints?",,['ordinary-differential-equations']
5,Does a 2D stable limit cycle always contain an unstable equilibrium point?,Does a 2D stable limit cycle always contain an unstable equilibrium point?,,"I believe that the answer is yes (provided there are no further limit cycles within the first one). Let there be a stable limit cycle with no other limit cycles within it. By reversing time, $t \rightarrow -t$, we get a new system with trajectories bounded to the interior of the limit cycle. Using, say, Brouwer’s fixed point theorem, one can show that there must be a fixed point within the region. However, when reading different formulations of the Poincaré–Bendixson theorem, I often encountered the statements like this: If the trajectories are bounded and there are no fixed points, then the trajectories must converge to a limit cycle. Are these are just unlucky/incorrect formulations or there is something in it?","I believe that the answer is yes (provided there are no further limit cycles within the first one). Let there be a stable limit cycle with no other limit cycles within it. By reversing time, $t \rightarrow -t$, we get a new system with trajectories bounded to the interior of the limit cycle. Using, say, Brouwer’s fixed point theorem, one can show that there must be a fixed point within the region. However, when reading different formulations of the Poincaré–Bendixson theorem, I often encountered the statements like this: If the trajectories are bounded and there are no fixed points, then the trajectories must converge to a limit cycle. Are these are just unlucky/incorrect formulations or there is something in it?",,"['ordinary-differential-equations', 'dynamical-systems']"
6,Vector Fields given a set of differential equations,Vector Fields given a set of differential equations,,"I'm having some trouble figuring out how to draw (by hand) the vector field given a set of differential equations. Consider the following: $\frac{dx}{dt} = x+y$ $\frac{dy}{dt} = -x + y$ Normally, when I am given just one differential equation, like $\frac{dy}{dt} = y$, I can easily compute the values by hand and can plot this out - think of this as picking coordinates of $(t,y)$. Would this approach be the same for this given system of differential equations? I'm more interested in the process, but a graph of how this should look like would be greatly appreciated as well.","I'm having some trouble figuring out how to draw (by hand) the vector field given a set of differential equations. Consider the following: $\frac{dx}{dt} = x+y$ $\frac{dy}{dt} = -x + y$ Normally, when I am given just one differential equation, like $\frac{dy}{dt} = y$, I can easily compute the values by hand and can plot this out - think of this as picking coordinates of $(t,y)$. Would this approach be the same for this given system of differential equations? I'm more interested in the process, but a graph of how this should look like would be greatly appreciated as well.",,['ordinary-differential-equations']
7,A formula for homogeneous differential form integrating factor,A formula for homogeneous differential form integrating factor,,"(From Boyce and Di Prima book) Suppose $M$ and $N$ are differentiable function such that $$M(x,y)\,dx+N(x,y)\,dy=0$$ is an homogeneous differential form . I can show that the 2 variables functions $\mu$ defined as: $$ \mu(x,y)=\frac{1}{x\,M(x,y)+y\,N(x,y)}$$ is an integrating factor that transform any homogeneous equation $M(x,y)\,dx+N(x,y)\,dy=0$ into an exact form , that is: $$ \frac{\partial}{\partial y}\big(\mu(x,y)\cdot M(x,y)\big)=\frac{\partial}{\partial x}\big(\mu(x,y)\cdot N(x,y)\big).$$ My questions: 1) Where this integrating factor comes from ?","(From Boyce and Di Prima book) Suppose $M$ and $N$ are differentiable function such that $$M(x,y)\,dx+N(x,y)\,dy=0$$ is an homogeneous differential form . I can show that the 2 variables functions $\mu$ defined as: $$ \mu(x,y)=\frac{1}{x\,M(x,y)+y\,N(x,y)}$$ is an integrating factor that transform any homogeneous equation $M(x,y)\,dx+N(x,y)\,dy=0$ into an exact form , that is: $$ \frac{\partial}{\partial y}\big(\mu(x,y)\cdot M(x,y)\big)=\frac{\partial}{\partial x}\big(\mu(x,y)\cdot N(x,y)\big).$$ My questions: 1) Where this integrating factor comes from ?",,"['ordinary-differential-equations', 'partial-derivative']"
8,Is there a close form solution for parallel transport on 2 sphere along the great circles.,Is there a close form solution for parallel transport on 2 sphere along the great circles.,,"I need to transport a tangent vector on 2-sphere from point $p$ to $q$ along the geodesic, which is defined using great circles in this case. I believe there are iterative solutions to achieve that like explained here . Is there a close form solution to achieve that?","I need to transport a tangent vector on 2-sphere from point $p$ to $q$ along the geodesic, which is defined using great circles in this case. I believe there are iterative solutions to achieve that like explained here . Is there a close form solution to achieve that?",,"['ordinary-differential-equations', 'differential-geometry', 'manifolds', 'lie-groups', 'riemannian-geometry']"
9,General Solution of Euler's Equation,General Solution of Euler's Equation,,Find the general solution to the Euler's Equation: $$ x^2\frac{d^2y}{dx^2}+2x\frac{dy}{dx}-6y=0 $$ using change of independent variable given by transformation: $$ x = e^z $$ Any help would be greatly appreciated.Thanks :),Find the general solution to the Euler's Equation: $$ x^2\frac{d^2y}{dx^2}+2x\frac{dy}{dx}-6y=0 $$ using change of independent variable given by transformation: $$ x = e^z $$ Any help would be greatly appreciated.Thanks :),,['ordinary-differential-equations']
10,Chain Rule for Multivariable Calculus,Chain Rule for Multivariable Calculus,,"Suppose I have $f:\mathbb{R^2}\rightarrow\mathbb{R}$ such that $f(t,s(t))$. If I want to observe how sensitive $f$ is with respect to $t$, my understanding is that $\frac{\partial f}{\partial t}$ does not accurately capture the impact of $t$ on $f$, because $t$ also works through $s(t)$ to impact $f$ as well. My confusion is that is this in a way pre-composed function? Then, if I want to ask the question how sensitive the function is with respect to a tiny change in $t$, the correct derivative I should go for it ""total derivative"" not ""partial"", which is, $\frac{df}{dt}=$$\frac{\partial f}{\partial t}+\frac{\partial f}{\partial s(t)}\frac{\partial s(t)}{\partial t}$? Can somebody explain clearly how mapping is actually done, and depending what ""question"" (i.e. sensitivity) I am interested in, looking at ""total"" versus ""partial"" matter? Thanks.","Suppose I have $f:\mathbb{R^2}\rightarrow\mathbb{R}$ such that $f(t,s(t))$. If I want to observe how sensitive $f$ is with respect to $t$, my understanding is that $\frac{\partial f}{\partial t}$ does not accurately capture the impact of $t$ on $f$, because $t$ also works through $s(t)$ to impact $f$ as well. My confusion is that is this in a way pre-composed function? Then, if I want to ask the question how sensitive the function is with respect to a tiny change in $t$, the correct derivative I should go for it ""total derivative"" not ""partial"", which is, $\frac{df}{dt}=$$\frac{\partial f}{\partial t}+\frac{\partial f}{\partial s(t)}\frac{\partial s(t)}{\partial t}$? Can somebody explain clearly how mapping is actually done, and depending what ""question"" (i.e. sensitivity) I am interested in, looking at ""total"" versus ""partial"" matter? Thanks.",,"['calculus', 'ordinary-differential-equations', 'derivatives', 'function-and-relation-composition']"
11,How can I solve $y''=\frac{a}{y^2}$ where a is a (positive) constant?,How can I solve  where a is a (positive) constant?,y''=\frac{a}{y^2},"Actually, I found out a way to solve that, but I can't get rid of complex numbers. And it does not make sense when it comes to complex numbers as the original question that involves this differential equation has nothing to do with complex numbers, as this is from a physics question.","Actually, I found out a way to solve that, but I can't get rid of complex numbers. And it does not make sense when it comes to complex numbers as the original question that involves this differential equation has nothing to do with complex numbers, as this is from a physics question.",,"['ordinary-differential-equations', 'complex-numbers', 'physics']"
12,Solution to the differential equation $xy''+y'+xy=0$,Solution to the differential equation,xy''+y'+xy=0,Show that the differential equation   $$xy''+y'+xy=0$$   admits a solution of the form    $$\varphi(x)=\int_0^1f(t)\cos(xt)dt$$   for some function $f(t)$. Since $$\varphi'(x)=\frac{d}{dx}\int_0^1f(t)\cos(xt)dt =\int_0^1\frac{\partial}{\partial x}f(t)\cos(xt)dt =-\int_0^1t f(t)\sin(xt)dt$$ and  $$\varphi''(x)=-\frac{d}{dx}\int_0^1t f(t)\sin(xt)dt =-\int_0^1\frac{\partial}{\partial x}t f(t)\sin(xt)dt =-\int_0^1t^2 f(t)\cos(xt)dt$$ it must be satisfied  $$\int_0^1f(t)\left[-xt^2\cos(xt)-t\sin(xt)+x\cos(xt)\right]dt=0.$$ I don't know how to proceed from this point. I've tried to use integration by parts to simplify the expressions for $\varphi'$ and $\varphi''$ but became even worse.,Show that the differential equation   $$xy''+y'+xy=0$$   admits a solution of the form    $$\varphi(x)=\int_0^1f(t)\cos(xt)dt$$   for some function $f(t)$. Since $$\varphi'(x)=\frac{d}{dx}\int_0^1f(t)\cos(xt)dt =\int_0^1\frac{\partial}{\partial x}f(t)\cos(xt)dt =-\int_0^1t f(t)\sin(xt)dt$$ and  $$\varphi''(x)=-\frac{d}{dx}\int_0^1t f(t)\sin(xt)dt =-\int_0^1\frac{\partial}{\partial x}t f(t)\sin(xt)dt =-\int_0^1t^2 f(t)\cos(xt)dt$$ it must be satisfied  $$\int_0^1f(t)\left[-xt^2\cos(xt)-t\sin(xt)+x\cos(xt)\right]dt=0.$$ I don't know how to proceed from this point. I've tried to use integration by parts to simplify the expressions for $\varphi'$ and $\varphi''$ but became even worse.,,['ordinary-differential-equations']
13,"How to find concentration of a solution, differential equation problem","How to find concentration of a solution, differential equation problem",,"A tank initially contains $100 \text{gal}$ of brine, a solution of salt and water, whose salt concentration is $0.5 \frac{\text{lb}}{\text{gal}}$. Brine whose salt concentration is $2 \frac{\text{lb}}{\text{gal}}$ flows into the tank at the rate of $3\frac{ \text{gal}}{ \text{min}}$. The mixture flows out at the rate of $2\frac{ \text{gal}}{ \text{min}}$. Assume the salt is uniformly distributed throughout the mixture. Find the salt concentration of the brine at the end of $30$ minutes. Letting $x $ be the amount of salt at any time $t$ I got $$ \frac{\text{dx}}{\text{dt}}=6-\frac{2x}{100+t} $$  $$ x = \frac{6[10,000t+100t^2+\frac{t^3}{3}]}{(100+t)^2} $$ What is the meaning of concentration of the brine? Is it $\frac{\text{dx}}{\text{dt}}$?","A tank initially contains $100 \text{gal}$ of brine, a solution of salt and water, whose salt concentration is $0.5 \frac{\text{lb}}{\text{gal}}$. Brine whose salt concentration is $2 \frac{\text{lb}}{\text{gal}}$ flows into the tank at the rate of $3\frac{ \text{gal}}{ \text{min}}$. The mixture flows out at the rate of $2\frac{ \text{gal}}{ \text{min}}$. Assume the salt is uniformly distributed throughout the mixture. Find the salt concentration of the brine at the end of $30$ minutes. Letting $x $ be the amount of salt at any time $t$ I got $$ \frac{\text{dx}}{\text{dt}}=6-\frac{2x}{100+t} $$  $$ x = \frac{6[10,000t+100t^2+\frac{t^3}{3}]}{(100+t)^2} $$ What is the meaning of concentration of the brine? Is it $\frac{\text{dx}}{\text{dt}}$?",,"['calculus', 'ordinary-differential-equations']"
14,Find the general solution to differential equation $x(x+1)^2(y'-\sqrt x)=(3x^2+4x+1)y$,Find the general solution to differential equation,x(x+1)^2(y'-\sqrt x)=(3x^2+4x+1)y,"Equation can be transformed to linear differential equation: $$LHS=x^3y'+2x^2y'+xy'-x^{7/2}-2x^{5/2}-x^{3/2}$$ $$RHS=3x^2y+4xy+y$$ $$\Rightarrow y'(x^3+2x^2+x)+y(-3x^2-4x-1)=x^{7/2}+2x^{5/2}+x^{3/2}$$ After dividing by $(x^3+2x^2+x),x\neq 0\land x\neq -1$ $$\Rightarrow y'+\frac{-3x^2-4x-1}{x^3+2x^2+x}y=\frac{x^{7/2}+2x^{5/2}+x^{3/2}}{x^3+2x^2+x}$$ The general solution is $$y=e^{-\int \frac{-3x^2-4x-1}{x^3+2x^2+x}\mathrm dx}\left(c+\int \frac{x^{7/2}+2x^{5/2}+x^{3/2}}{x^3+2x^2+x}{e^{\int \frac{-3x^2-4x-1}{x^3+2x^2+x}\mathrm dx}}\mathrm dx\right)$$ Integral $\int \frac{-3x^2-4x-1}{x^3+2x^2+x}\mathrm dx$ can be found using partial fractions, $$\int \frac{-3x^2-4x-1}{x^3+2x^2+x}\mathrm dx=\ln(|x|(x+1)^2)+c$$ How to evaluate integral  $$\int \frac{x^{7/2}+2x^{5/2}+x^{3/2}}{x^3+2x^2+x}{e^{\int \frac{-3x^2-4x-1}{x^3+2x^2+x}\mathrm dx}}\mathrm dx=\int \frac{x^{7/2}+2x^{5/2}+x^{3/2}}{x^3+2x^2+x}e^{\ln(|x|(x+1)^2)}\mathrm dx?$$ Is there an easier method than transforming to linear equation?","Equation can be transformed to linear differential equation: $$LHS=x^3y'+2x^2y'+xy'-x^{7/2}-2x^{5/2}-x^{3/2}$$ $$RHS=3x^2y+4xy+y$$ $$\Rightarrow y'(x^3+2x^2+x)+y(-3x^2-4x-1)=x^{7/2}+2x^{5/2}+x^{3/2}$$ After dividing by $(x^3+2x^2+x),x\neq 0\land x\neq -1$ $$\Rightarrow y'+\frac{-3x^2-4x-1}{x^3+2x^2+x}y=\frac{x^{7/2}+2x^{5/2}+x^{3/2}}{x^3+2x^2+x}$$ The general solution is $$y=e^{-\int \frac{-3x^2-4x-1}{x^3+2x^2+x}\mathrm dx}\left(c+\int \frac{x^{7/2}+2x^{5/2}+x^{3/2}}{x^3+2x^2+x}{e^{\int \frac{-3x^2-4x-1}{x^3+2x^2+x}\mathrm dx}}\mathrm dx\right)$$ Integral $\int \frac{-3x^2-4x-1}{x^3+2x^2+x}\mathrm dx$ can be found using partial fractions, $$\int \frac{-3x^2-4x-1}{x^3+2x^2+x}\mathrm dx=\ln(|x|(x+1)^2)+c$$ How to evaluate integral  $$\int \frac{x^{7/2}+2x^{5/2}+x^{3/2}}{x^3+2x^2+x}{e^{\int \frac{-3x^2-4x-1}{x^3+2x^2+x}\mathrm dx}}\mathrm dx=\int \frac{x^{7/2}+2x^{5/2}+x^{3/2}}{x^3+2x^2+x}e^{\ln(|x|(x+1)^2)}\mathrm dx?$$ Is there an easier method than transforming to linear equation?",,"['calculus', 'integration', 'ordinary-differential-equations']"
15,Solving differential equation $xdy + ydx = 0$,Solving differential equation,xdy + ydx = 0,Solve following equation $$xdy + ydx = 0$$ My process: group variables $$xdy = -ydx \ \ \ ; \ \ \ \frac{x}{dx} = -\frac{y}{dy}$$ Integrate $$\int{\frac{dx}{x}} = \int{\frac{-dy}{y}}$$ $$\ln |x| + K = - \ln |y| + K$$ Solution  $$\ln|xy| = K$$ However my textbook gives answer: $$xy = K$$ Where am I going wrong?,Solve following equation $$xdy + ydx = 0$$ My process: group variables $$xdy = -ydx \ \ \ ; \ \ \ \frac{x}{dx} = -\frac{y}{dy}$$ Integrate $$\int{\frac{dx}{x}} = \int{\frac{-dy}{y}}$$ $$\ln |x| + K = - \ln |y| + K$$ Solution  $$\ln|xy| = K$$ However my textbook gives answer: $$xy = K$$ Where am I going wrong?,,"['calculus', 'integration', 'ordinary-differential-equations']"
16,Evolution semigroups for differential equations,Evolution semigroups for differential equations,,"I would like to ask whether ""evolution semigroups"" are really useful (to discover something that can't be discovered in some other way?). There is a huge machinery to deal with them, but from my point of view the theory fails to provide new insight. Let me recall the notion with a simple example. Say that you consider a differential equation $x'=A(t)x$, where $A(t)$ is an $n\times n$ matrix varying continuously with $t\in\mathbb R$. It induces what is sometimes called an evolution family $X(t,s)$ such that $X(t,s)x(s)=x(t)$ for any solution $x$. The evolution semigroup (perhaps it would be better: evolution semiflow) associated to the equation is defined by $$(T(t)u)(s)=X(s,s-t)u(s-t)$$ for each continuous function $u\colon\mathbb R\to\mathbb R^n$. There are many other possibilities (of equations, spaces, etc), this is just an example. So my question is: What can we gain easier from looking at $T(t)$ instead of looking at $X(t,s)$? As far as I understand the two have the same information (thus why the word ""easier""), although the machinery developed is so huge, that I wonder whether I am missing something. All becomes much trickier when we consider unbounded linear operators with dense domains that can in general depend on $t$.","I would like to ask whether ""evolution semigroups"" are really useful (to discover something that can't be discovered in some other way?). There is a huge machinery to deal with them, but from my point of view the theory fails to provide new insight. Let me recall the notion with a simple example. Say that you consider a differential equation $x'=A(t)x$, where $A(t)$ is an $n\times n$ matrix varying continuously with $t\in\mathbb R$. It induces what is sometimes called an evolution family $X(t,s)$ such that $X(t,s)x(s)=x(t)$ for any solution $x$. The evolution semigroup (perhaps it would be better: evolution semiflow) associated to the equation is defined by $$(T(t)u)(s)=X(s,s-t)u(s-t)$$ for each continuous function $u\colon\mathbb R\to\mathbb R^n$. There are many other possibilities (of equations, spaces, etc), this is just an example. So my question is: What can we gain easier from looking at $T(t)$ instead of looking at $X(t,s)$? As far as I understand the two have the same information (thus why the word ""easier""), although the machinery developed is so huge, that I wonder whether I am missing something. All becomes much trickier when we consider unbounded linear operators with dense domains that can in general depend on $t$.",,"['ordinary-differential-equations', 'semigroup-of-operators']"
17,Resolvent Kernel of Volterra Integral Equation,Resolvent Kernel of Volterra Integral Equation,,"The resolvent kernal $R(x,t,\lambda)$ for the  Volterra integral equation $$\phi(x)=x+\lambda\int\limits_a^x\phi(s)ds$$ is   $\begin{array}1 1. e^{\lambda(x+t)} && 2. e^{\lambda(x-t)} && 3. \lambda e^{(x+t)} && 4. e^{\lambda xt} \end{array}$ From what I learned from here : Find iterative kernel and then try guess resolvent kernel, $R=\sum\limits_{i=1}^\infty K_i(x,t)$ Comparing with general form of Volterra equation of second kind, $$g(x)\phi(x)=f(x)+\lambda\int\limits_a^x K(x,t)\phi(t)dt$$ Kernel, $K(x,t)=1,K_1(x,t)=K(x,t)=1$ Now, $K_2(x,t)=\int\limits_t^x K(x,s)K_1(s,t)ds=\int\limits_t^x ds=x-t$ $K_3(x,t)=\int\limits_t^x K(x,s)K_2(s,t)ds \\=\int\limits_t^x (s-t)ds=\frac{s^2}2-ts\big|_{s=t}^x \\=\frac{x^2}2-tx-\frac{t^2}2+t^2=\frac{x^2}2-tx+\frac{t^2}2 \\=\frac1{\sqrt2}(x-t)^2$ And I am stuck, though I am tempted to choose option 2, since it has $x-t$. I haven't done any coursework in Integral Equations, and I know only bits and pieces about it. My doubts are: Was I right in choosing $K(x,t)=1$? How does $\lambda$ find its place in resolvent kernel? How to proceed further? Are there any trial and error methods, where I can plug in the options to see if it is the answer?","The resolvent kernal $R(x,t,\lambda)$ for the  Volterra integral equation $$\phi(x)=x+\lambda\int\limits_a^x\phi(s)ds$$ is   $\begin{array}1 1. e^{\lambda(x+t)} && 2. e^{\lambda(x-t)} && 3. \lambda e^{(x+t)} && 4. e^{\lambda xt} \end{array}$ From what I learned from here : Find iterative kernel and then try guess resolvent kernel, $R=\sum\limits_{i=1}^\infty K_i(x,t)$ Comparing with general form of Volterra equation of second kind, $$g(x)\phi(x)=f(x)+\lambda\int\limits_a^x K(x,t)\phi(t)dt$$ Kernel, $K(x,t)=1,K_1(x,t)=K(x,t)=1$ Now, $K_2(x,t)=\int\limits_t^x K(x,s)K_1(s,t)ds=\int\limits_t^x ds=x-t$ $K_3(x,t)=\int\limits_t^x K(x,s)K_2(s,t)ds \\=\int\limits_t^x (s-t)ds=\frac{s^2}2-ts\big|_{s=t}^x \\=\frac{x^2}2-tx-\frac{t^2}2+t^2=\frac{x^2}2-tx+\frac{t^2}2 \\=\frac1{\sqrt2}(x-t)^2$ And I am stuck, though I am tempted to choose option 2, since it has $x-t$. I haven't done any coursework in Integral Equations, and I know only bits and pieces about it. My doubts are: Was I right in choosing $K(x,t)=1$? How does $\lambda$ find its place in resolvent kernel? How to proceed further? Are there any trial and error methods, where I can plug in the options to see if it is the answer?",,"['ordinary-differential-equations', 'integral-equations']"
18,Can you solve $y'+x+e^y=0$ by series expansion?,Can you solve  by series expansion?,y'+x+e^y=0,Find an approximate solution by series expansion of $y(x)$ around $x = 0$ up to fourth order in $x$ given the inital conditions $y(0)=0$ Let  $$ y=\sum_0^{\infty}a_nx^n \implies y'=\sum_0^{\infty}a_n nx^{n-1} \\  \implies \sum_0^{\infty}a_n nx^{n-1}+x+e^{\sum_0^{\infty}a_nx^n}=0 $$ Usually I would try to combine my terms but I can't do it here. Is this even the right approach?,Find an approximate solution by series expansion of $y(x)$ around $x = 0$ up to fourth order in $x$ given the inital conditions $y(0)=0$ Let  $$ y=\sum_0^{\infty}a_nx^n \implies y'=\sum_0^{\infty}a_n nx^{n-1} \\  \implies \sum_0^{\infty}a_n nx^{n-1}+x+e^{\sum_0^{\infty}a_nx^n}=0 $$ Usually I would try to combine my terms but I can't do it here. Is this even the right approach?,,['ordinary-differential-equations']
19,Numerical solution to a system of second order differential equations,Numerical solution to a system of second order differential equations,,"I'm writing a sort of physical simulator.  I have $n$ bodies that move in a two dimensional space under the force of gravity (for instance it could be a simplified version of the solar system).  Let's call $m_1, \dots, m_n$ their masses, $(x_1, y_1), \dots (x_n,y_n)$ their positions, $(vx_1, vy_1), \dots (vx_n,vy_n)$ their velocities and $(ax_1, ay_1), \dots (ax_n,ay_n)$ their accelerations. Suppose we are given some initial conditions on the positions and velocities, and let $dt$ be a small amount of time. My goal is to compute the positions of all bodies at time $t_0+dt$, $t_0+2dt$, $\dots$ and so on, where $t_0$ is the initial time. Here is what I have done: First I compute for all bodies the total force that acts at the current time on them, due to all other bodies. And so I compute the instantaneous acceleration for each body; Then I compute the new velocity for each body, by applying the formulas $vx_i \leftarrow vx_i + dt\cdot ax_i$ and $vy_i \leftarrow vy_i + dt\cdot ay_i$; Finally I compute the new position for each body, by applying the analogous formulas  $x_i \leftarrow x_i + dt\cdot vx_i$ and $y_i \leftarrow y_i + dt\cdot vy_i$. My solution in some sense remember me Euler method for solving differential equations. It is quite intuitive and simple, but rather accurate. What I am wondering is if there is a clever better method for solving the problem. For better I mean that, given a fixed $dt$, it can get closer to the exact solution by requiring equal or less computations. For instance, if we interchange points 2. and 3., by experimentation, I noticed that we obtain a method which requires the same computations but is far less accurate (for it to be as accurate as the original method, we have to use a smaller step $dt$).","I'm writing a sort of physical simulator.  I have $n$ bodies that move in a two dimensional space under the force of gravity (for instance it could be a simplified version of the solar system).  Let's call $m_1, \dots, m_n$ their masses, $(x_1, y_1), \dots (x_n,y_n)$ their positions, $(vx_1, vy_1), \dots (vx_n,vy_n)$ their velocities and $(ax_1, ay_1), \dots (ax_n,ay_n)$ their accelerations. Suppose we are given some initial conditions on the positions and velocities, and let $dt$ be a small amount of time. My goal is to compute the positions of all bodies at time $t_0+dt$, $t_0+2dt$, $\dots$ and so on, where $t_0$ is the initial time. Here is what I have done: First I compute for all bodies the total force that acts at the current time on them, due to all other bodies. And so I compute the instantaneous acceleration for each body; Then I compute the new velocity for each body, by applying the formulas $vx_i \leftarrow vx_i + dt\cdot ax_i$ and $vy_i \leftarrow vy_i + dt\cdot ay_i$; Finally I compute the new position for each body, by applying the analogous formulas  $x_i \leftarrow x_i + dt\cdot vx_i$ and $y_i \leftarrow y_i + dt\cdot vy_i$. My solution in some sense remember me Euler method for solving differential equations. It is quite intuitive and simple, but rather accurate. What I am wondering is if there is a clever better method for solving the problem. For better I mean that, given a fixed $dt$, it can get closer to the exact solution by requiring equal or less computations. For instance, if we interchange points 2. and 3., by experimentation, I noticed that we obtain a method which requires the same computations but is far less accurate (for it to be as accurate as the original method, we have to use a smaller step $dt$).",,"['ordinary-differential-equations', 'numerical-methods', 'physics']"
20,differential inclusions vs differential equations,differential inclusions vs differential equations,,"Can someone please clarify what the difference (no pun intended) between the two is? I am reading this tutorial and at the very start they state that a differential inclusion is a solution to $ \frac{\mathrm{d}}{\mathrm{d}t}x(t) \in F(t, x(t)) $ So that means that the derivative of $x(t)$ is included in $F$ which is a function the argument $t$ and the $x(t)$ itself. Then a solution is a family of functions, right? But why is this different the just a differential equation? I suppose it's meant to be more general, but right now I'm failing to see where. P.S. no tag for differential inclusions, so differential equations it is.","Can someone please clarify what the difference (no pun intended) between the two is? I am reading this tutorial and at the very start they state that a differential inclusion is a solution to $ \frac{\mathrm{d}}{\mathrm{d}t}x(t) \in F(t, x(t)) $ So that means that the derivative of $x(t)$ is included in $F$ which is a function the argument $t$ and the $x(t)$ itself. Then a solution is a family of functions, right? But why is this different the just a differential equation? I suppose it's meant to be more general, but right now I'm failing to see where. P.S. no tag for differential inclusions, so differential equations it is.",,['ordinary-differential-equations']
21,Infinite many solutions of the differential equation.,Infinite many solutions of the differential equation.,,Consider the differential equation $\frac{dy}{dx} =y^{1/3} $ with $y(0)=0$. Then show that the given differential equation has infinite many solutions. I found two solutions of the given equation $y_{1}$ and $y_{2}$ where $y_{1}=0$ that is zero and $$y_{2}= \left\{ 	\begin{array}{ll} 		(\frac{2}{3}x)^{3/2}  & \mbox{if } x \geq 0 \\ 		0 & \mbox{if } x \leq 0 	\end{array} \right. $$ Please help me to find other solution's of the  given differential equation.,Consider the differential equation $\frac{dy}{dx} =y^{1/3} $ with $y(0)=0$. Then show that the given differential equation has infinite many solutions. I found two solutions of the given equation $y_{1}$ and $y_{2}$ where $y_{1}=0$ that is zero and $$y_{2}= \left\{ 	\begin{array}{ll} 		(\frac{2}{3}x)^{3/2}  & \mbox{if } x \geq 0 \\ 		0 & \mbox{if } x \leq 0 	\end{array} \right. $$ Please help me to find other solution's of the  given differential equation.,,['ordinary-differential-equations']
22,solutions to nonhomogeneous system of differential equations with general solution already known,solutions to nonhomogeneous system of differential equations with general solution already known,,"Let's say we have the general solution to $X' = A(t)X$, where $X=(x_1, x_2)^T$. How do you find the general solution to the system $X'= A(t)X + b(t)$ where $b(t)$ is a $2 \times 1$ matrix with two polynomials as entries. How do you find the particular solution?","Let's say we have the general solution to $X' = A(t)X$, where $X=(x_1, x_2)^T$. How do you find the general solution to the system $X'= A(t)X + b(t)$ where $b(t)$ is a $2 \times 1$ matrix with two polynomials as entries. How do you find the particular solution?",,"['ordinary-differential-equations', 'systems-of-equations']"
23,Why is $f(x) = x + \frac{1}{x}$ a mapping contraction?,Why is  a mapping contraction?,f(x) = x + \frac{1}{x},"Why is $f(x) = x + \frac{1}{x}$ a mapping contraction? The metric space in question is $[1,\infty)$. Also, if this were a contraction, wouldn't it have a fixed point by Banach's theorem? It looks to me like it's not a contraction because for example if I take $x=5$ and $x'=6$: $$d\left(5+\frac{1}{5},6+\frac{1}{6}\right) > 1 = d(5,6)$$ Would appreciate any help.","Why is $f(x) = x + \frac{1}{x}$ a mapping contraction? The metric space in question is $[1,\infty)$. Also, if this were a contraction, wouldn't it have a fixed point by Banach's theorem? It looks to me like it's not a contraction because for example if I take $x=5$ and $x'=6$: $$d\left(5+\frac{1}{5},6+\frac{1}{6}\right) > 1 = d(5,6)$$ Would appreciate any help.",,['ordinary-differential-equations']
24,Necessary Condition for uniform convergence of series of functions,Necessary Condition for uniform convergence of series of functions,,"I would like to make sure that the follwing is a necessary condition for uniform convergence of series of functions: :Let  $$ \sum _{n=1}^{\infty }f_{n}(x)  $$ be a series of functions, than a necessary condition for its uniform convergence is that  $$ \left \{ f_{n}(x) \right \}_{n=1}^{_{n=\infty }} $$ uniformly converges to zero. Is that true?","I would like to make sure that the follwing is a necessary condition for uniform convergence of series of functions: :Let  $$ \sum _{n=1}^{\infty }f_{n}(x)  $$ be a series of functions, than a necessary condition for its uniform convergence is that  $$ \left \{ f_{n}(x) \right \}_{n=1}^{_{n=\infty }} $$ uniformly converges to zero. Is that true?",,"['sequences-and-series', 'ordinary-differential-equations', 'convergence-divergence', 'uniform-convergence']"
25,Basis of a Kernel,Basis of a Kernel,,How would i find the basis of the kernel of the differential operator below $$8y'' + 3y' + 7y$$ We know the equation was homogenous and i believe the basis is two dimensional,How would i find the basis of the kernel of the differential operator below $$8y'' + 3y' + 7y$$ We know the equation was homogenous and i believe the basis is two dimensional,,"['ordinary-differential-equations', 'differential', 'homogeneous-equation']"
26,References for Linear Algebra needed for Differential Equations and Linear Programming,References for Linear Algebra needed for Differential Equations and Linear Programming,,"I am in need of learning the Linear Algebraic theory behind the following Applied disciplines. Could someone please recommend Linear Algebra books for: Differential Equations: Specifically learning about characteristic values needed for solving first order linear systems with constant coefficients. As in, a proper explanation of the spaces and invariant subspaces involved when the eigenvalues are real-distinct, complex and repeated. The theory behind algebraic and geometric multiplicity of an eigenvalue and so on. Linear Programming: Mainly focussing on Duality Theory. But would like to learn the Linear Algebra behind the Simplex Method and how a basic feasible solution is a ""basis"" and so on. A treatise on Convex Sets will also be useful. I have only taken an introductory course on Linear Algebra. So I've read the first few chapters of Axler and Hoffman - Kunze. But skimming through the chapters on eigen-values, both don't seem to meet the requirements. Any help is appreciated. Thanks in advance.","I am in need of learning the Linear Algebraic theory behind the following Applied disciplines. Could someone please recommend Linear Algebra books for: Differential Equations: Specifically learning about characteristic values needed for solving first order linear systems with constant coefficients. As in, a proper explanation of the spaces and invariant subspaces involved when the eigenvalues are real-distinct, complex and repeated. The theory behind algebraic and geometric multiplicity of an eigenvalue and so on. Linear Programming: Mainly focussing on Duality Theory. But would like to learn the Linear Algebra behind the Simplex Method and how a basic feasible solution is a ""basis"" and so on. A treatise on Convex Sets will also be useful. I have only taken an introductory course on Linear Algebra. So I've read the first few chapters of Axler and Hoffman - Kunze. But skimming through the chapters on eigen-values, both don't seem to meet the requirements. Any help is appreciated. Thanks in advance.",,"['linear-algebra', 'ordinary-differential-equations', 'reference-request', 'linear-programming']"
27,How to guess 2nd order ode solution,How to guess 2nd order ode solution,,"I am solving this equation $$ y''+y'\cos x-y\sin x=0 $$ My question is this: do i have to guess some y solution in order complete my answer with $y=y_1c_1+y_2c_2$, or is there some other way? If i have to guess, are there any guidelines how to do that? I just can not see any solutions, thanks for help","I am solving this equation $$ y''+y'\cos x-y\sin x=0 $$ My question is this: do i have to guess some y solution in order complete my answer with $y=y_1c_1+y_2c_2$, or is there some other way? If i have to guess, are there any guidelines how to do that? I just can not see any solutions, thanks for help",,['ordinary-differential-equations']
28,Differential equation with $\sqrt{xy}$,Differential equation with,\sqrt{xy},"$$7\sqrt{xy} \frac{dy}{dx}=4, \quad x,y>0$$ How do I solve this equation for $y$","$$7\sqrt{xy} \frac{dy}{dx}=4, \quad x,y>0$$ How do I solve this equation for $y$",,"['calculus', 'ordinary-differential-equations']"
29,Differential Equation $\frac{dP}{dt} = kP(1-P)$,Differential Equation,\frac{dP}{dt} = kP(1-P),"I have a question about solving this differential equation. So, the question is to solve it given that $P(0)=\frac23$ So this is what I've done so far $$\frac{dP}{dt} = kP(1-P)$$ $$ k\,dt = \frac{dP}{P(1-P)}$$ $$ \int{k\,dt} = \int\frac{dP}{P(1-P)} $$ $$ kt + C = \ln(P) - \ln(1-P) $$ $$ \frac23k + C = \ln(0) - \ln(1) $$ This is where I'm lost in finding $C$ because $\ln(0)$ is $-\infty$ Am I doing something wrong?","I have a question about solving this differential equation. So, the question is to solve it given that $P(0)=\frac23$ So this is what I've done so far $$\frac{dP}{dt} = kP(1-P)$$ $$ k\,dt = \frac{dP}{P(1-P)}$$ $$ \int{k\,dt} = \int\frac{dP}{P(1-P)} $$ $$ kt + C = \ln(P) - \ln(1-P) $$ $$ \frac23k + C = \ln(0) - \ln(1) $$ This is where I'm lost in finding $C$ because $\ln(0)$ is $-\infty$ Am I doing something wrong?",,"['calculus', 'ordinary-differential-equations']"
30,Solving the differential equation $y' \tan y = \frac1x$,Solving the differential equation,y' \tan y = \frac1x,"Express the differential equation   $$\tan y\,\frac{dy}{dx}=\frac{1}{x}$$   in a form not involving $\frac{dy}{dx}$. I understand the concept of a differential equation (though, as a student, I am fairly new to the topic), but I'm not sure how to put this in a form that can be integrated. Thanks so much, cyanfox","Express the differential equation   $$\tan y\,\frac{dy}{dx}=\frac{1}{x}$$   in a form not involving $\frac{dy}{dx}$. I understand the concept of a differential equation (though, as a student, I am fairly new to the topic), but I'm not sure how to put this in a form that can be integrated. Thanks so much, cyanfox",,['ordinary-differential-equations']
31,A 6 meter ladder...,A 6 meter ladder...,,"A $6$ meter long ladder leans with a vertical wall and top of the ladder is 3 meters above the ground.If it slips at a rate of $2$ m/s then how fast the level is decreasing from the wall? My attempt:First i draw the picture which is right triangle with hypotenuse $6$ and opposite $3$ then by pythagorean theorem i found base is $3\sqrt3$.If i suppose base is x and opposite is y,then what i have to calculate $\frac{dx}{dt}$ or $\frac{dy}{dt}$ and further how i can do this?","A $6$ meter long ladder leans with a vertical wall and top of the ladder is 3 meters above the ground.If it slips at a rate of $2$ m/s then how fast the level is decreasing from the wall? My attempt:First i draw the picture which is right triangle with hypotenuse $6$ and opposite $3$ then by pythagorean theorem i found base is $3\sqrt3$.If i suppose base is x and opposite is y,then what i have to calculate $\frac{dx}{dt}$ or $\frac{dy}{dt}$ and further how i can do this?",,"['ordinary-differential-equations', 'classical-mechanics']"
32,Computing the spherical mean and showing it satisfies PDE,Computing the spherical mean and showing it satisfies PDE,,"Compute the spherical mean of the function $h : \mathbb R^3 \to \mathbb R$ with $$  h(x,y,z) = x $$ and show that it satisfies the differential equation $$  u_{rr} + \frac{2}{r} u_r = u_{xx} + u_{yy} + u_{zz} $$ (note that the spherical mean of a function is a function of some fixed point and a radius).","Compute the spherical mean of the function $h : \mathbb R^3 \to \mathbb R$ with $$  h(x,y,z) = x $$ and show that it satisfies the differential equation $$  u_{rr} + \frac{2}{r} u_r = u_{xx} + u_{yy} + u_{zz} $$ (note that the spherical mean of a function is a function of some fixed point and a radius).",,"['analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
33,Analytical solution for non-linear differential equation,Analytical solution for non-linear differential equation,,"Consider the following differential equation $$\frac{\mathrm{d}x}{\mathrm{d}t} = \frac{-a x^2}{1 + bx}$$ where $a,b$ are constants. How is this solved analytically? I want to find some explicit function $x(t)$.","Consider the following differential equation $$\frac{\mathrm{d}x}{\mathrm{d}t} = \frac{-a x^2}{1 + bx}$$ where $a,b$ are constants. How is this solved analytically? I want to find some explicit function $x(t)$.",,['ordinary-differential-equations']
34,How do I solve $F = \nabla\times G$ for $G$?,How do I solve  for ?,F = \nabla\times G G,"Given the vector valued function $F(x,y,z) = (xz,-yz,y)$ find $G$ such that $F = \nabla\times G$ I let $G(x,y,z) = (G_1,G_2,G_3)$ and expanded $\nabla \times G$ then equated the components to $F$ but I don't know what to do from here. How would I go about solving these equations? or is there a better way of doing this? Thanks for any help.","Given the vector valued function $F(x,y,z) = (xz,-yz,y)$ find $G$ such that $F = \nabla\times G$ I let $G(x,y,z) = (G_1,G_2,G_3)$ and expanded $\nabla \times G$ then equated the components to $F$ but I don't know what to do from here. How would I go about solving these equations? or is there a better way of doing this? Thanks for any help.",,"['calculus', 'ordinary-differential-equations', 'multivariable-calculus', 'cross-product']"
35,Solving a differential equation?,Solving a differential equation?,,"I'm trying to analyze the transient state of a RC circuit. My book gives me the following differential equation: $$\frac{d(v(t))}{dt} + av(t) = c$$ for some constants $a$ and $c$. The book thens proceeds to solve it, and says that: $$v(t) = K_1 + K_2e^{-t/\tau}$$ for some constants $\tau, K_1, K_2$. We haven't learned differential equations yet, so I wasn't able to follow along the solution of the differential equation. However, doesn't: $v(t) = c/a$ also satisfy the differential equation? Why isn't this a valid solution to the equation? If it is a valid question, what could possibly motivate the book to not include it?","I'm trying to analyze the transient state of a RC circuit. My book gives me the following differential equation: $$\frac{d(v(t))}{dt} + av(t) = c$$ for some constants $a$ and $c$. The book thens proceeds to solve it, and says that: $$v(t) = K_1 + K_2e^{-t/\tau}$$ for some constants $\tau, K_1, K_2$. We haven't learned differential equations yet, so I wasn't able to follow along the solution of the differential equation. However, doesn't: $v(t) = c/a$ also satisfy the differential equation? Why isn't this a valid solution to the equation? If it is a valid question, what could possibly motivate the book to not include it?",,"['calculus', 'ordinary-differential-equations', 'physics']"
36,"Ordinary differential equation $y'(t)=\sin(f(t,y))$",Ordinary differential equation,"y'(t)=\sin(f(t,y))","One whose solution never makes me happy is the following: $$y'(t)=\sin(y+t)\text{.}$$ I would start by substituting $z(t)=y(t)+t$ to get an ODE in $z(t)$, but then I'm not sure about how to substitute back my solution to check if it's correct or not$\dots$","One whose solution never makes me happy is the following: $$y'(t)=\sin(y+t)\text{.}$$ I would start by substituting $z(t)=y(t)+t$ to get an ODE in $z(t)$, but then I'm not sure about how to substitute back my solution to check if it's correct or not$\dots$",,"['ordinary-differential-equations', 'trigonometry']"
37,Using the Jordan form Complex,Using the Jordan form Complex,,Let $C$ be a complex $n \times n$ matrix with $\det C \neq 0$. Prove that there is a complex matrix $B$ such that $C = e^B$ Hint: use the Jordan form matrices for comlexas,Let $C$ be a complex $n \times n$ matrix with $\det C \neq 0$. Prove that there is a complex matrix $B$ such that $C = e^B$ Hint: use the Jordan form matrices for comlexas,,"['linear-algebra', 'ordinary-differential-equations']"
38,Use of Routh-Hurwitz if you have the eigenvalues?,Use of Routh-Hurwitz if you have the eigenvalues?,,"This is for self-study of $N$-dimensional system of linear homogeneous ordinary differential equations of the form: $$ \mathbf{\dot{x}}=A\mathbf{x} $$ where $A$ is the coefficient matrix of the system. I have learned that you can check for stability by determining if the real parts of all the eigenvalues of $A$ are negative. You can check for oscillations if there are any purely imaginary eigenvalues of $A$. The author in the book I'm reading then introduces the Routh-Hurwitz criterion for detecting stability and oscillations of the system. This seems to be a more efficient computational short-cut than calculating eigenvalues. What are the advantages of using Routh-Hurwitz criteria for stability and oscillations, if you already have the eigenvalues? For instance, will it be useful when I start to study nonlinear dynamics? Is there some additional application that I am completely missing, that I would miss out on by focusing on eigenvalues? Wikipedia entry on RH stability analysis has stuff about control systems, and ends up with a lot of material in the s-domain (Laplace transforms), but for my applications I will be staying in the time-domain for the most part, and just focusing fairly narrowly on stability and oscillations in linear (or linearized) systems. I am asking b/c it seems easy to calculate eigenvalues on my computer, and the Routh-Hurwitz criterion comes off as the sort of thing that might save me a lot of time if I were doing this by hand, but not very helpful for doing analysis of small-fry systems via Matlab where I have the eig(A) function. Note I posted this question at Stack Overflow but it was suggested it was more a math than programming question so I've moved it here: https://stackoverflow.com/questions/22029482/routh-hurwitz-useful-when-i-can-just-calculate-eigenvalues","This is for self-study of $N$-dimensional system of linear homogeneous ordinary differential equations of the form: $$ \mathbf{\dot{x}}=A\mathbf{x} $$ where $A$ is the coefficient matrix of the system. I have learned that you can check for stability by determining if the real parts of all the eigenvalues of $A$ are negative. You can check for oscillations if there are any purely imaginary eigenvalues of $A$. The author in the book I'm reading then introduces the Routh-Hurwitz criterion for detecting stability and oscillations of the system. This seems to be a more efficient computational short-cut than calculating eigenvalues. What are the advantages of using Routh-Hurwitz criteria for stability and oscillations, if you already have the eigenvalues? For instance, will it be useful when I start to study nonlinear dynamics? Is there some additional application that I am completely missing, that I would miss out on by focusing on eigenvalues? Wikipedia entry on RH stability analysis has stuff about control systems, and ends up with a lot of material in the s-domain (Laplace transforms), but for my applications I will be staying in the time-domain for the most part, and just focusing fairly narrowly on stability and oscillations in linear (or linearized) systems. I am asking b/c it seems easy to calculate eigenvalues on my computer, and the Routh-Hurwitz criterion comes off as the sort of thing that might save me a lot of time if I were doing this by hand, but not very helpful for doing analysis of small-fry systems via Matlab where I have the eig(A) function. Note I posted this question at Stack Overflow but it was suggested it was more a math than programming question so I've moved it here: https://stackoverflow.com/questions/22029482/routh-hurwitz-useful-when-i-can-just-calculate-eigenvalues",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'stability-theory']"
39,"$y - y_0 = \frac{d^2}{dx^2}\left[ \ln\left( \frac{y}{y_0} \right) \right]$, solve for $y$.",", solve for .",y - y_0 = \frac{d^2}{dx^2}\left[ \ln\left( \frac{y}{y_0} \right) \right] y,"I'm looking at a simple differential equation of the form: $$y - y_0 = \frac{d^2}{dx^2}\left[ \ln\left( \frac{y}{y_0} \right) \right].\tag{$\star$}$$ Here $y=y(x)$ is a function of $x$ only, $y_0$ is a constant, and $x,y,y_0 \in \mathbb{R}$ are real. After an hour or so working on a problem, Eq. $\left(\star\right)$ popped out. I haven't made a significant effort to solve this DE just yet, because it is so simple looking and I thought it'd be a nice question for this site. This is ""work-work"", not ""homework"". Is there a name for this DE? Are there simple solutions, besides the trivial solution $y(x) = y_0$?","I'm looking at a simple differential equation of the form: $$y - y_0 = \frac{d^2}{dx^2}\left[ \ln\left( \frac{y}{y_0} \right) \right].\tag{$\star$}$$ Here $y=y(x)$ is a function of $x$ only, $y_0$ is a constant, and $x,y,y_0 \in \mathbb{R}$ are real. After an hour or so working on a problem, Eq. $\left(\star\right)$ popped out. I haven't made a significant effort to solve this DE just yet, because it is so simple looking and I thought it'd be a nice question for this site. This is ""work-work"", not ""homework"". Is there a name for this DE? Are there simple solutions, besides the trivial solution $y(x) = y_0$?",,['ordinary-differential-equations']
40,Examples of applications of Linear differential equations to physics.,Examples of applications of Linear differential equations to physics.,,"I wonder which other real life applications do exist for linear differential equations, besides harmonic oscillators and pendulums. I'm looking for examples to include in a document that talks about the topic. So basically I need things that are easy to model with a single differential equation.","I wonder which other real life applications do exist for linear differential equations, besides harmonic oscillators and pendulums. I'm looking for examples to include in a document that talks about the topic. So basically I need things that are easy to model with a single differential equation.",,"['ordinary-differential-equations', 'soft-question', 'physics', 'mathematical-physics']"
41,"Calculate the Wronskian of $f(t)=t|t|$ and $g(t)=t^2$ on the following intervals: $(0,+\infty)$, $(-\infty, 0)$ and $0$?","Calculate the Wronskian of  and  on the following intervals: ,  and ?","f(t)=t|t| g(t)=t^2 (0,+\infty) (-\infty, 0) 0","How to Calculate the Wronskian of $f(t)=t|t|$ and $g(t)=t^2$ on the following intervals: $(0,+\infty)$, $(-\infty, 0)$ and $0$. And then how would I show that the Wronskian of the two functions $f$ and $g$ is equal to zero, i.e. $W(f,g)=0$? Also how would I establish that functions f and g are linearly independent on the interval $(-\infty, +\infty)$. Can a Wronksian be zero on all points and yet still be linearly independent?","How to Calculate the Wronskian of $f(t)=t|t|$ and $g(t)=t^2$ on the following intervals: $(0,+\infty)$, $(-\infty, 0)$ and $0$. And then how would I show that the Wronskian of the two functions $f$ and $g$ is equal to zero, i.e. $W(f,g)=0$? Also how would I establish that functions f and g are linearly independent on the interval $(-\infty, +\infty)$. Can a Wronksian be zero on all points and yet still be linearly independent?",,"['real-analysis', 'ordinary-differential-equations']"
42,Runge-Kutta and Butcher table?,Runge-Kutta and Butcher table?,,"In the Wikipedia article on Runge-Kutta methods, there is a notation explained using a Butcher table with a $c_{i}$ vector (nodes), a $b_{i}$ vector (weights) and a runge-kutta matrix $a_{ij}$. My question is : does every runge-kutta-something method is entirely summed up by this Butcher table, or is there some subtleties ? For example, if we take the Feagin RK12 and RK14 methods explained here : http://sce.uhcl.edu/rungekutta/GlascowRK.ppt http://www.peterstone.name/Maplepgs/Maple/nmthds/RKcoeff/Runge_Kutta_schemes/RK12/RKcoeff12a_1.pdf and with the coefficients : http://sce.uhcl.edu/rungekutta/rk1210.txt http://sce.uhcl.edu/rungekutta/rk1412.txt do the coefficients completely constrain the numerical scheme or is there additional details to put in the integrator ? For example when we say RK14(12) which is a ""14th order method with an embedded 12th order method"", can I simply put the Butcher table in a generic RK integrator which takes $c_{i}$, $b_{i}$ and $a_{ij}$ as arguments, or is there some additional details to know ?","In the Wikipedia article on Runge-Kutta methods, there is a notation explained using a Butcher table with a $c_{i}$ vector (nodes), a $b_{i}$ vector (weights) and a runge-kutta matrix $a_{ij}$. My question is : does every runge-kutta-something method is entirely summed up by this Butcher table, or is there some subtleties ? For example, if we take the Feagin RK12 and RK14 methods explained here : http://sce.uhcl.edu/rungekutta/GlascowRK.ppt http://www.peterstone.name/Maplepgs/Maple/nmthds/RKcoeff/Runge_Kutta_schemes/RK12/RKcoeff12a_1.pdf and with the coefficients : http://sce.uhcl.edu/rungekutta/rk1210.txt http://sce.uhcl.edu/rungekutta/rk1412.txt do the coefficients completely constrain the numerical scheme or is there additional details to put in the integrator ? For example when we say RK14(12) which is a ""14th order method with an embedded 12th order method"", can I simply put the Butcher table in a generic RK integrator which takes $c_{i}$, $b_{i}$ and $a_{ij}$ as arguments, or is there some additional details to know ?",,"['ordinary-differential-equations', 'numerical-methods', 'runge-kutta-methods']"
43,Euler's method for second order differential equation,Euler's method for second order differential equation,,Not really homework but sample exam. The question is to use Euler's Method to approximate Y: $Y''(t) = Y'(t) - 2Y(t)$ $Y'(0) = Y(0) = 1$ with $t_0 = 0$ and $h=0.2$ So what I did: First iteration: $t_1 = 0.2$ $y(t_1) = y(t_0)+h \cdot y'(t_0) = 1 + 0.2 \times 1 = 1.2$ $y'(t_1) = y'(t_0) + h \cdot (y'(t_0) - 2y(t_0)) = 1 + 0.2 \times (1-2\times1) = 0.8$ Second iteration: $t_2 = 0.4$ $y(t_2) = y(t_1)+h \cdot y'(t_1) = 1.2 + 0.2 \times 0.8 = 1.36$ $y'(t_2) = y'(t_1) + h \cdot (y'(t_1) - 2y(t_1)) = 0.8 + 0.2 \times (0.8-2\times1.2) = -2.4$ Correct?,Not really homework but sample exam. The question is to use Euler's Method to approximate Y: $Y''(t) = Y'(t) - 2Y(t)$ $Y'(0) = Y(0) = 1$ with $t_0 = 0$ and $h=0.2$ So what I did: First iteration: $t_1 = 0.2$ $y(t_1) = y(t_0)+h \cdot y'(t_0) = 1 + 0.2 \times 1 = 1.2$ $y'(t_1) = y'(t_0) + h \cdot (y'(t_0) - 2y(t_0)) = 1 + 0.2 \times (1-2\times1) = 0.8$ Second iteration: $t_2 = 0.4$ $y(t_2) = y(t_1)+h \cdot y'(t_1) = 1.2 + 0.2 \times 0.8 = 1.36$ $y'(t_2) = y'(t_1) + h \cdot (y'(t_1) - 2y(t_1)) = 0.8 + 0.2 \times (0.8-2\times1.2) = -2.4$ Correct?,,"['ordinary-differential-equations', 'numerical-methods']"
44,resources to study PDE from,resources to study PDE from,,"I am an undergrad engineering student. I recently completed my second year, with that said, I have taken several calculus courses. Most recently I completed differential equations and multivariable calculus before that. Now I need to study something which uses partial differential equation extensively, it also uses finite element method and optimization techniques. But the issue is I haven't studied such courses yet, PDE course will be next year I think. So what I am looking for is some really good resources on these topics. I need to master these asap (really fast) else I am screwed.","I am an undergrad engineering student. I recently completed my second year, with that said, I have taken several calculus courses. Most recently I completed differential equations and multivariable calculus before that. Now I need to study something which uses partial differential equation extensively, it also uses finite element method and optimization techniques. But the issue is I haven't studied such courses yet, PDE course will be next year I think. So what I am looking for is some really good resources on these topics. I need to master these asap (really fast) else I am screwed.",,"['calculus', 'ordinary-differential-equations', 'reference-request', 'partial-differential-equations', 'numerical-methods']"
45,differential equations in SIR epidemic model and obtain Ro,differential equations in SIR epidemic model and obtain Ro,,"I need to know why the differential equation system that expresses epidemic's model SIR in some texts appears: $$\frac{dS}{dt}  =-\beta\frac{S}{N}I$$  $$\frac{dI}{dt}= \beta \frac{S}{N}I - \gamma I$$ $$\frac{dR}{dt}=\gamma I$$ and in other is expressed in this way: $$\frac{dS}{dt}=-\beta S I$$ $$dI = \beta S I - \gamma I$$ $$\frac{dR}{dt}= \gamma I$$ Another question that I have is the origin of $R_o$ number, how can I obtain this number from this system Please could you in addition recommend me some bibliography about this topic specially considering vaccination I really appreciate your help","I need to know why the differential equation system that expresses epidemic's model SIR in some texts appears: $$\frac{dS}{dt}  =-\beta\frac{S}{N}I$$  $$\frac{dI}{dt}= \beta \frac{S}{N}I - \gamma I$$ $$\frac{dR}{dt}=\gamma I$$ and in other is expressed in this way: $$\frac{dS}{dt}=-\beta S I$$ $$dI = \beta S I - \gamma I$$ $$\frac{dR}{dt}= \gamma I$$ Another question that I have is the origin of $R_o$ number, how can I obtain this number from this system Please could you in addition recommend me some bibliography about this topic specially considering vaccination I really appreciate your help",,['ordinary-differential-equations']
46,steady states and stability,steady states and stability,,"im just checking to see if im doing this right? $$\frac{du}{d\tau}=u(1-u)-h    $$ show this equations has 2 steady states and check their linear stability. this is what i have done: $u=0$ and $u=1$ $$\frac{d^2u}{d\tau^2} = -2u+1    $$ at $u=0$, $\frac{d^2u}{d\tau^2}$=1 which is unstable at $u=1$, $\frac{d^2u}{d\tau^2}$=-1 which is stable is this right? thanks in advance","im just checking to see if im doing this right? $$\frac{du}{d\tau}=u(1-u)-h    $$ show this equations has 2 steady states and check their linear stability. this is what i have done: $u=0$ and $u=1$ $$\frac{d^2u}{d\tau^2} = -2u+1    $$ at $u=0$, $\frac{d^2u}{d\tau^2}$=1 which is unstable at $u=1$, $\frac{d^2u}{d\tau^2}$=-1 which is stable is this right? thanks in advance",,"['ordinary-differential-equations', 'biology']"
47,Finding the solutions of $y''+y'-6y=0$,Finding the solutions of,y''+y'-6y=0,"Hi guys just a quick question I am not used to that I need help with. How do I find the solutions of the following differential equations satisfying the given conditions: $$ {\sf a})\quad y''+y'-6y = 0, \qquad y=3, y'=1\ \mathsf{when}\ x=0.$$ I'm just a bit confused because I'm used to finding the general solution not the solutions.","Hi guys just a quick question I am not used to that I need help with. How do I find the solutions of the following differential equations satisfying the given conditions: $$ {\sf a})\quad y''+y'-6y = 0, \qquad y=3, y'=1\ \mathsf{when}\ x=0.$$ I'm just a bit confused because I'm used to finding the general solution not the solutions.",,['ordinary-differential-equations']
48,Nonhomogeneous second order ODE,Nonhomogeneous second order ODE,,"Solve  $y''-2y'-3y=3te^{-t}$ Attempt : now the solution has two parts. Namely, $y=y_h+y_p$ where $y_h$ is the solution of the homogeneous equation and $y_p$ is the solution of the nonhomogeneous equation. $y_h$ is easy to find. $$y_h=c_1e^{3t}+c_2e^{-t}$$ I am struggling with finding $y_p$. The way I set it up is: $$y_p=(At+B)e^{-t}=Ate^{-t}+Be^{-t}$$ Since $Be^{-t}$ is also a solution for $y_h$ I multiply RHS of $y_p$ by $t$ and get: $$y_p=t(At+B)e^{-t}=At^2e^{-t}+Bte^{-t}$$ Then, I find derivatives of $y_p$ and plug them into differential equation, simplify, and get: $$2A-2B-2At^2-4At=-3t$$ Now, if this is correct how would I solve for A and B? It is not homework. Thank you for your help.","Solve  $y''-2y'-3y=3te^{-t}$ Attempt : now the solution has two parts. Namely, $y=y_h+y_p$ where $y_h$ is the solution of the homogeneous equation and $y_p$ is the solution of the nonhomogeneous equation. $y_h$ is easy to find. $$y_h=c_1e^{3t}+c_2e^{-t}$$ I am struggling with finding $y_p$. The way I set it up is: $$y_p=(At+B)e^{-t}=Ate^{-t}+Be^{-t}$$ Since $Be^{-t}$ is also a solution for $y_h$ I multiply RHS of $y_p$ by $t$ and get: $$y_p=t(At+B)e^{-t}=At^2e^{-t}+Bte^{-t}$$ Then, I find derivatives of $y_p$ and plug them into differential equation, simplify, and get: $$2A-2B-2At^2-4At=-3t$$ Now, if this is correct how would I solve for A and B? It is not homework. Thank you for your help.",,['ordinary-differential-equations']
49,Solve differential equation y'' + 4y' + 5 =0,Solve differential equation y'' + 4y' + 5 =0,,How to go about solving this: $$y'' + 4y' + 5 = 0$$ $$y = Ae^{px} + Be^{qx}$$ I know the following: p and q are solutions to the characteristic equation  $am^2 + bm + c = 0$ So in this case $m^2 + 4m + 5 = 0$ However I do not know what to do after this.,How to go about solving this: $$y'' + 4y' + 5 = 0$$ $$y = Ae^{px} + Be^{qx}$$ I know the following: p and q are solutions to the characteristic equation  $am^2 + bm + c = 0$ So in this case $m^2 + 4m + 5 = 0$ However I do not know what to do after this.,,['ordinary-differential-equations']
50,How to solve $\sin(x)\frac{d}{dx}\beta \left ( x \right )+\cos(x)\beta (x)=1$?,How to solve ?,\sin(x)\frac{d}{dx}\beta \left ( x \right )+\cos(x)\beta (x)=1,I'm trying to solve $$\sin(x)\frac{d}{dx}\beta \left ( x \right )+\cos(x)\beta (x)=1$$ What I get is :  $$\beta (x)=\beta \left ( \alpha  \right )e^{\sin(\alpha )-\sin(x)}+e^{-\sin(x)}\int_{\alpha }^{x}e^{\sin(t)}dt$$ But I think that this solution is incorrect .The textbook says that there's exactly one solution that has a finite limit as $x$ tends to $0$ . But all the solutions I get have a finite limit . So what's the correct solution?,I'm trying to solve $$\sin(x)\frac{d}{dx}\beta \left ( x \right )+\cos(x)\beta (x)=1$$ What I get is :  $$\beta (x)=\beta \left ( \alpha  \right )e^{\sin(\alpha )-\sin(x)}+e^{-\sin(x)}\int_{\alpha }^{x}e^{\sin(t)}dt$$ But I think that this solution is incorrect .The textbook says that there's exactly one solution that has a finite limit as $x$ tends to $0$ . But all the solutions I get have a finite limit . So what's the correct solution?,,['ordinary-differential-equations']
51,Is this differential equation separable??,Is this differential equation separable??,,"Just a quick question... I have the equation: $$\frac{dw}{dy} w^{-2}=0$$ Is this separable? i.e. can I go : $$\begin{align} \frac{dw}{dy} w^{-2}&=0 \\[8pt] (w^{-2}) \,dw&=(0)\,dy \tag{*} \\[8pt] -w^{-1}&=A \end{align}$$ and thus $$w = B$$ with $A,B$ being arbitrary constants with $B \not= 0$ Is this ok? The only thing I'm wondering about is the starred line... can I separate the equation like that with only a zero on the RHS?","Just a quick question... I have the equation: $$\frac{dw}{dy} w^{-2}=0$$ Is this separable? i.e. can I go : $$\begin{align} \frac{dw}{dy} w^{-2}&=0 \\[8pt] (w^{-2}) \,dw&=(0)\,dy \tag{*} \\[8pt] -w^{-1}&=A \end{align}$$ and thus $$w = B$$ with $A,B$ being arbitrary constants with $B \not= 0$ Is this ok? The only thing I'm wondering about is the starred line... can I separate the equation like that with only a zero on the RHS?",,['ordinary-differential-equations']
52,How to conclude this solution is periodic?,How to conclude this solution is periodic?,,"I've met the following problem in finishing my argument. The expression I ended with is $$\frac{\mathrm d}{\mathrm d\xi}U(\xi)=\pm\sqrt{C_1-\frac{U^4(\xi)}{2}},$$ with $U:\mathbb R\to\mathbb R$ and $C_1$ is non zero because otherwise I would have no square root unless $U\equiv 0$, which is not admissible in my situation. Therefore I would say the solution $U$ is periodic, and also I would like to find some bounds on the period. The first problem I ask is the following: how would you proceed in showing that $U$ is periodic? The second question is to find some bounds on the period. I mean: separating the variables and choosing the $+$ sign one gets $$\frac{\mathrm d U}{\sqrt{C_1-\frac{U^4}{2}}}=\mathrm d \xi,$$ but then I derived no useful informations about the period. Could you help me? Thank you in advance.","I've met the following problem in finishing my argument. The expression I ended with is $$\frac{\mathrm d}{\mathrm d\xi}U(\xi)=\pm\sqrt{C_1-\frac{U^4(\xi)}{2}},$$ with $U:\mathbb R\to\mathbb R$ and $C_1$ is non zero because otherwise I would have no square root unless $U\equiv 0$, which is not admissible in my situation. Therefore I would say the solution $U$ is periodic, and also I would like to find some bounds on the period. The first problem I ask is the following: how would you proceed in showing that $U$ is periodic? The second question is to find some bounds on the period. I mean: separating the variables and choosing the $+$ sign one gets $$\frac{\mathrm d U}{\sqrt{C_1-\frac{U^4}{2}}}=\mathrm d \xi,$$ but then I derived no useful informations about the period. Could you help me? Thank you in advance.",,"['ordinary-differential-equations', 'proof-writing', 'mathematical-physics']"
53,Solving $\frac{dP}{dt} = k(M - P)$,Solving,\frac{dP}{dt} = k(M - P),"I am suppose to solve for P(t), to find an epxression for P(t) and I am suppose to find the limit. I can't find anything. $$\frac{dP}{dt} = k(M  - P)$$ $$\frac{dP}{M - P} = k \, dt$$ $$\int \frac{dP}{M - P} = \int k \, dt$$ $$ \ln \frac{1}{M - P} = xk + c$$ $$ \frac{1}{M - P} = e^{xk} + e^c$$ $$ \frac{1}{e^{xk} + e^c} = M - P$$ $$ -\frac{1}{e^{xk} + e^c} +M=  P$$ This is wrong but I am not sure why.","I am suppose to solve for P(t), to find an epxression for P(t) and I am suppose to find the limit. I can't find anything. $$\frac{dP}{dt} = k(M  - P)$$ $$\frac{dP}{M - P} = k \, dt$$ $$\int \frac{dP}{M - P} = \int k \, dt$$ $$ \ln \frac{1}{M - P} = xk + c$$ $$ \frac{1}{M - P} = e^{xk} + e^c$$ $$ \frac{1}{e^{xk} + e^c} = M - P$$ $$ -\frac{1}{e^{xk} + e^c} +M=  P$$ This is wrong but I am not sure why.",,['calculus']
54,Solving polynomial differential equation,Solving polynomial differential equation,,"I have $a(v)$ where $a$ is acceleration and $v$ is velocity. $a$ can be described as a polynomial of degree 3: $$a(v) = \sum\limits_{i=0}^3 p_i v^i = \sum\limits_{i=0}^3 p_i \left(\frac{dd(t)}{dt}\right)^i,$$ where $d(t)$ is distance with respect to time. I want to solve (or approximate) this equation for $d(t)$, but it's been a few years since I graduated, and I seem to have forgotten most of my math skills :)","I have $a(v)$ where $a$ is acceleration and $v$ is velocity. $a$ can be described as a polynomial of degree 3: $$a(v) = \sum\limits_{i=0}^3 p_i v^i = \sum\limits_{i=0}^3 p_i \left(\frac{dd(t)}{dt}\right)^i,$$ where $d(t)$ is distance with respect to time. I want to solve (or approximate) this equation for $d(t)$, but it's been a few years since I graduated, and I seem to have forgotten most of my math skills :)",,"['ordinary-differential-equations', 'polynomials']"
55,A simple question about the solution of homogeneous equation to this differential equation,A simple question about the solution of homogeneous equation to this differential equation,,"Given that $t,1+t,t^2,-t$ are the solutions to $y'''+a(t)y''+b(t)y""+c(t)y=d(t)$,what is the solution of homogeneous equation to this differential equation? What i have done is tried the properties of linear differential equation that $L(t)=L(1+t)=L(t^2)=L(-t)=d(t)$ so the homogeneous solution should be independent and i claim that $1,t,t^2$ should be the solution. However, i am not sure hot can i actually conclude that these are the solutions? It seems that it can be quite a number of sets of solution by the linearity.","Given that $t,1+t,t^2,-t$ are the solutions to $y'''+a(t)y''+b(t)y""+c(t)y=d(t)$,what is the solution of homogeneous equation to this differential equation? What i have done is tried the properties of linear differential equation that $L(t)=L(1+t)=L(t^2)=L(-t)=d(t)$ so the homogeneous solution should be independent and i claim that $1,t,t^2$ should be the solution. However, i am not sure hot can i actually conclude that these are the solutions? It seems that it can be quite a number of sets of solution by the linearity.",,['ordinary-differential-equations']
56,How to derive a differential equation of an ellipse,How to derive a differential equation of an ellipse,,"I am quite new to differential equations and derivatives. I want to derive an differential form for equation of an ellipse. If i start with an ordinary ellipse equation \begin{equation} \frac{x^2}{a^2}+\frac{y^2}{b^2}=1 \end{equation} How do i derive it then to get this form $$ -\frac{dx}{dy} = \frac{a^2}{b^2} \frac{y}{x} $$ I would need an equation and some brief explanation on the procedure.","I am quite new to differential equations and derivatives. I want to derive an differential form for equation of an ellipse. If i start with an ordinary ellipse equation \begin{equation} \frac{x^2}{a^2}+\frac{y^2}{b^2}=1 \end{equation} How do i derive it then to get this form $$ -\frac{dx}{dy} = \frac{a^2}{b^2} \frac{y}{x} $$ I would need an equation and some brief explanation on the procedure.",,['ordinary-differential-equations']
57,"ordinary differential equation: $(f'(z))^2 = c\,f(z)^3 + f(z)^2$",ordinary differential equation:,"(f'(z))^2 = c\,f(z)^3 + f(z)^2","I swear I have seen this type of ODE before, but I can't remember how to attack it. In general, I would like to know how to solve $$\left(f'(z)\right)^m = c\,G(z)^n$$ where $m,\;n \in \mathbb{N}$ and $G(z)$ is just a polynomial in $f(z)$. This sounds hard, so I would be happy with, $$\left(f'(z)\right)^2 = c\,G(z)^n,$$ though this latter equation may be too difficult too. For my homework, though, I need to know, $$\left(f'(z)\right)^2 = \left( c\,f^3 + f^2 \right).$$ It was also suggested in the homework question to utilize $$g^2 = 3\,c-f.$$ If I do this without thinking I get an equation $$\left(f'(z)\right)^2 = c\,f(z)^2,$$ which seems much easier, though I am still a little rattled by the plus-minus. In case it matters, this is a related to a method for solving the Korteweg-deVries equation, $$u_t + u\,u_x + u_{xxx} = 0.$$ I have seen some solutions (but did not understand them) where the polynomial was ""factored"" into 3 roots... something like that. I just don't want to know the answer, but how to get it. Please keep in mind that this is my first class in PDEs. Thanks for any help!","I swear I have seen this type of ODE before, but I can't remember how to attack it. In general, I would like to know how to solve $$\left(f'(z)\right)^m = c\,G(z)^n$$ where $m,\;n \in \mathbb{N}$ and $G(z)$ is just a polynomial in $f(z)$. This sounds hard, so I would be happy with, $$\left(f'(z)\right)^2 = c\,G(z)^n,$$ though this latter equation may be too difficult too. For my homework, though, I need to know, $$\left(f'(z)\right)^2 = \left( c\,f^3 + f^2 \right).$$ It was also suggested in the homework question to utilize $$g^2 = 3\,c-f.$$ If I do this without thinking I get an equation $$\left(f'(z)\right)^2 = c\,f(z)^2,$$ which seems much easier, though I am still a little rattled by the plus-minus. In case it matters, this is a related to a method for solving the Korteweg-deVries equation, $$u_t + u\,u_x + u_{xxx} = 0.$$ I have seen some solutions (but did not understand them) where the polynomial was ""factored"" into 3 roots... something like that. I just don't want to know the answer, but how to get it. Please keep in mind that this is my first class in PDEs. Thanks for any help!",,['ordinary-differential-equations']
58,How can I show that this family of curves may be described by this differential equation?,How can I show that this family of curves may be described by this differential equation?,,"I have a homework problem in which I wish to show that the family of curves given by $$x^2 + y^2 = cx,$$ where $c$ is an abitrary constant may be described by the differential equation $$\frac{dy}{dx} = \frac{y^2-x^2}{2xy}.$$ I thought that I could use implicit differentiation to differentiate the original equation to get the second equation, but instead I get the equation $$\frac{c-2x}{2y}=\frac{dy}{dx}.$$ As you can see the derivative I get is not in the form of the equation that I am supposed to get. I do not see a way for my solution to even become similar to the proposed solution as one contains constants whereas the other does not. What is the correct procedure I should use to solve the problem?","I have a homework problem in which I wish to show that the family of curves given by where is an abitrary constant may be described by the differential equation I thought that I could use implicit differentiation to differentiate the original equation to get the second equation, but instead I get the equation As you can see the derivative I get is not in the form of the equation that I am supposed to get. I do not see a way for my solution to even become similar to the proposed solution as one contains constants whereas the other does not. What is the correct procedure I should use to solve the problem?","x^2 + y^2 = cx, c \frac{dy}{dx} = \frac{y^2-x^2}{2xy}. \frac{c-2x}{2y}=\frac{dy}{dx}.","['calculus', 'ordinary-differential-equations']"
59,Harmonic Oscillator and Quadrature,Harmonic Oscillator and Quadrature,,Consider the simple harmonic oscillator $\frac{d^2p}{dt^2}=-p$ as a Hamiltonian system with Hamiltonian given by $H=\frac{1}{2}p^2+\frac{1}{2}q^2$. The famous Liouville theorem for integrable systems then says this system can be integrated by ''quadrature.'' I have searched high and low on the internet for a decent explanation of what this means and can't really turn up much other than it is some process involving integrals of known functions and ''algebraic'' operations (Note: The quotations around quadrature and algebraic is directly from Arnold's book on classical mechanics.) My questions is the following: the harmonic oscillator is probably the first and easiest example of a Hamiltonian system then how do you arrive at the solution to the harmonic oscillator using ''quadrature.'' Or should I think of solvable by quadrature as an existence theorem to an ODE.,Consider the simple harmonic oscillator $\frac{d^2p}{dt^2}=-p$ as a Hamiltonian system with Hamiltonian given by $H=\frac{1}{2}p^2+\frac{1}{2}q^2$. The famous Liouville theorem for integrable systems then says this system can be integrated by ''quadrature.'' I have searched high and low on the internet for a decent explanation of what this means and can't really turn up much other than it is some process involving integrals of known functions and ''algebraic'' operations (Note: The quotations around quadrature and algebraic is directly from Arnold's book on classical mechanics.) My questions is the following: the harmonic oscillator is probably the first and easiest example of a Hamiltonian system then how do you arrive at the solution to the harmonic oscillator using ''quadrature.'' Or should I think of solvable by quadrature as an existence theorem to an ODE.,,"['ordinary-differential-equations', 'symplectic-geometry']"
60,Does the solution of any differential equation (without boundary conditions) is always smooth,Does the solution of any differential equation (without boundary conditions) is always smooth,,Pardon me if this question sounds silly. Consider any differential equation without any boundary conditions. Does the solution of it is always smooth ?,Pardon me if this question sounds silly. Consider any differential equation without any boundary conditions. Does the solution of it is always smooth ?,,"['real-analysis', 'ordinary-differential-equations']"
61,The solution I find for the differential equation doesn't work in the interval I need,The solution I find for the differential equation doesn't work in the interval I need,,"This is the problem: $$(1-x^2 ) y''-2xy'+2y=x-x^3,\qquad     y(2)=0,\qquad        y(4)=1.$$ So by inspection I know one of the solutions for the homogenous is $y_1 = x$. Now, to find the other solution I used the following equations: $$ U = \frac{1}{y_1^2}e^{-\int p dx}  $$ $$ y_2 = y_1 \int_{} U dx $$ Where $ y_1=x $ and $\displaystyle p= \frac{-2x}{1-x^2} $ And the result I found is: $$ y_2 = 1+ \frac{x}{2}  \ln(1-x)-\frac{x}{2}  \ln(1+x) $$ It works very well... except it will not work for $x>1$ and the initial values I have are $y(2)=0$ and $y(4)=1$. Now, I just checked that this solution works too: $$ y_2 = 1+ \frac{x}{2}  \ln(x-1)-\frac{x}{2}  \ln(1+x) $$ and it's actually the solution I need. The problem is I don't know how to get that solution. I mean, I don't want to say ""so here you just change $ \displaystyle \frac{x}{2}  \ln(1-x)$ for $\displaystyle \frac{x}{2} \ln(x-1) $ and you have it!"". Well, that's all. I know how to do the rest.","This is the problem: $$(1-x^2 ) y''-2xy'+2y=x-x^3,\qquad     y(2)=0,\qquad        y(4)=1.$$ So by inspection I know one of the solutions for the homogenous is $y_1 = x$. Now, to find the other solution I used the following equations: $$ U = \frac{1}{y_1^2}e^{-\int p dx}  $$ $$ y_2 = y_1 \int_{} U dx $$ Where $ y_1=x $ and $\displaystyle p= \frac{-2x}{1-x^2} $ And the result I found is: $$ y_2 = 1+ \frac{x}{2}  \ln(1-x)-\frac{x}{2}  \ln(1+x) $$ It works very well... except it will not work for $x>1$ and the initial values I have are $y(2)=0$ and $y(4)=1$. Now, I just checked that this solution works too: $$ y_2 = 1+ \frac{x}{2}  \ln(x-1)-\frac{x}{2}  \ln(1+x) $$ and it's actually the solution I need. The problem is I don't know how to get that solution. I mean, I don't want to say ""so here you just change $ \displaystyle \frac{x}{2}  \ln(1-x)$ for $\displaystyle \frac{x}{2} \ln(x-1) $ and you have it!"". Well, that's all. I know how to do the rest.",,['ordinary-differential-equations']
62,System of differential equations in Maple,System of differential equations in Maple,,"I have problems entering a system of differential equations to Maple 13. Equations are: $x' = -4x + 2y$ $y' = 5x - 4y$ Solve for $x = 0, y = 0$ Thank you in advance","I have problems entering a system of differential equations to Maple 13. Equations are: $x' = -4x + 2y$ $y' = 5x - 4y$ Solve for $x = 0, y = 0$ Thank you in advance",,"['ordinary-differential-equations', 'maple']"
63,Poincare-Bendixson Theorem,Poincare-Bendixson Theorem,,Can someone sketch some ideas of how to use the Poincaré-Bendixson Theorem to prove that there must be a fixed point contained inside a periodic orbit?,Can someone sketch some ideas of how to use the Poincaré-Bendixson Theorem to prove that there must be a fixed point contained inside a periodic orbit?,,['ordinary-differential-equations']
64,Differential Equation $f''(x)+\frac{(n-1)(f'(x))^2}{\sinh(x)}=0 $,Differential Equation,f''(x)+\frac{(n-1)(f'(x))^2}{\sinh(x)}=0 ,"How do I solve the following differential equation: $$ f''(x)+\frac{(n-1)(f'(x))^2}{\sinh(x)}=0 $$ under the boundary conditions $f(1)=1$ and $\lim_{x\to\infty}f(x)=0$. More generally, how to solve $$ f''(x)+g(x)(f'(x))^2=0 $$ for some known function $g(x)$ for the same boundary conditions.","How do I solve the following differential equation: $$ f''(x)+\frac{(n-1)(f'(x))^2}{\sinh(x)}=0 $$ under the boundary conditions $f(1)=1$ and $\lim_{x\to\infty}f(x)=0$. More generally, how to solve $$ f''(x)+g(x)(f'(x))^2=0 $$ for some known function $g(x)$ for the same boundary conditions.",,[]
65,Could you recommend some classic textbooks on ordinary/partial differential equation?,Could you recommend some classic textbooks on ordinary/partial differential equation?,,"I love R. Courant and F. John's Introduction to Calculus and Analysis because of its wide coverage, precise description and friendly written style. Are there any classic textbooks like it on ODE/PDE? thanks.","I love R. Courant and F. John's Introduction to Calculus and Analysis because of its wide coverage, precise description and friendly written style. Are there any classic textbooks like it on ODE/PDE? thanks.",,"['ordinary-differential-equations', 'reference-request', 'partial-differential-equations', 'book-recommendation']"
66,Orthogonal trajectories to family of curves $\left\Vert x \right\Vert_p=1$ where $x\in\mathbb{R}^2$,Orthogonal trajectories to family of curves  where,\left\Vert x \right\Vert_p=1 x\in\mathbb{R}^2,"I have been trying to find the orthogonal trajectories of the family of $p$ -norm curves $\left\Vert x \right\Vert_p=1$ , where $x\in\mathbb{R}^2$ and $p>0$ . I eventually reached a step where I must find $p(x,y)$ such that $(1-x^{p(x,y)})^\frac{1}{p(x,y)}=y$ for all $x,y\in(0,1)$ . Does a closed-form expression for $p(x,y)$ exist? Graphing the equation with $p$ on the $z$ axis in a 3D graphing calculator at least suggests that $p$ is injective. More importantly, is this not a good approach for finding the orthogonal trajectories? I am following the steps described in Find the orthogonal trajectories of the family of curves .","I have been trying to find the orthogonal trajectories of the family of -norm curves , where and . I eventually reached a step where I must find such that for all . Does a closed-form expression for exist? Graphing the equation with on the axis in a 3D graphing calculator at least suggests that is injective. More importantly, is this not a good approach for finding the orthogonal trajectories? I am following the steps described in Find the orthogonal trajectories of the family of curves .","p \left\Vert x \right\Vert_p=1 x\in\mathbb{R}^2 p>0 p(x,y) (1-x^{p(x,y)})^\frac{1}{p(x,y)}=y x,y\in(0,1) p(x,y) p z p","['ordinary-differential-equations', 'normed-spaces', 'closed-form', 'curves', 'plane-curves']"
67,Can a real function have convergence that oscillates depending on its derivative?,Can a real function have convergence that oscillates depending on its derivative?,,"I recently read a post on here in which a user asked if there existed a function, $$\lim_{x\rightarrow\infty}f(x)$$ is convergent, but; $$\lim_{x\rightarrow\infty}f'(x)$$ does not converge. I wanted to know if there was any way to generalize this question such that, $$\lim_{x\rightarrow\infty}f^{(2n)}(x)$$ converges but, $$\lim_{x\rightarrow\infty}f^{(2n+1)}(x)$$ does not converge. $\forall n\in\mathbb{R}$ Even if it is that there are special cases in which this holds, I am very curious to see if such a property can exist on a general level.","I recently read a post on here in which a user asked if there existed a function, is convergent, but; does not converge. I wanted to know if there was any way to generalize this question such that, converges but, does not converge. Even if it is that there are special cases in which this holds, I am very curious to see if such a property can exist on a general level.",\lim_{x\rightarrow\infty}f(x) \lim_{x\rightarrow\infty}f'(x) \lim_{x\rightarrow\infty}f^{(2n)}(x) \lim_{x\rightarrow\infty}f^{(2n+1)}(x) \forall n\in\mathbb{R},"['calculus', 'ordinary-differential-equations', 'limits', 'conditional-convergence']"
68,ODE distribution function in phase space,ODE distribution function in phase space,,"Suppose I have an ODE system governed by the matrix $A$ $$ \frac{d\mathbf{x}}{dt} = A \mathbf{x} $$ Let's say I choose a large number of random initial conditions and let them evolve for some time $t$ . These initial conditions will have some sort of distribution in phase space -- which I'm assuming can be described by a probability distribution function $f(\mathbf{x},t)$ --  that will evolve with time. Is there an equation that governs the time evolution of $f$ ? How would such an equation change if we add an inhomogeneous term?",Suppose I have an ODE system governed by the matrix Let's say I choose a large number of random initial conditions and let them evolve for some time . These initial conditions will have some sort of distribution in phase space -- which I'm assuming can be described by a probability distribution function --  that will evolve with time. Is there an equation that governs the time evolution of ? How would such an equation change if we add an inhomogeneous term?,"A  \frac{d\mathbf{x}}{dt} = A \mathbf{x}  t f(\mathbf{x},t) f","['ordinary-differential-equations', 'probability-distributions', 'partial-differential-equations']"
69,Unfamiliar form of coupled ODEs,Unfamiliar form of coupled ODEs,,"I've recently come across a system of coupled ODE's: $$x_2''-x_1''=A-Bx_2$$ $$x_1''-x_2''=C-Dx_1$$ where $x_1, x_2$ are functions of only $t$ . I am sort of stumped. Just by adding these equations together, one gets: $0 = A+C-Bx_2-Dx_1$ which appears to imply, at least to me, that any function will work given that, $x_1 = \frac{A+C-Bx_2}{D}$ However, this is not true when I enter a possible pair of functions, $x_1 = \frac{A+C-Bt^2}{D}$ $x_2 = t^2$ This clearly doesn't work as $x_1''$ and $x_2''$ both wouldn't have dependence on $t$ which implies my assumption that any equations would work as long as they constrained to the $0 = A+C-Bx_2-Dx_1$ equation is false. How do I go about solving this coupled ODE?","I've recently come across a system of coupled ODE's: where are functions of only . I am sort of stumped. Just by adding these equations together, one gets: which appears to imply, at least to me, that any function will work given that, However, this is not true when I enter a possible pair of functions, This clearly doesn't work as and both wouldn't have dependence on which implies my assumption that any equations would work as long as they constrained to the equation is false. How do I go about solving this coupled ODE?","x_2''-x_1''=A-Bx_2 x_1''-x_2''=C-Dx_1 x_1, x_2 t 0 = A+C-Bx_2-Dx_1 x_1 = \frac{A+C-Bx_2}{D} x_1 = \frac{A+C-Bt^2}{D} x_2 = t^2 x_1'' x_2'' t 0 = A+C-Bx_2-Dx_1",['ordinary-differential-equations']
70,How to solve this 2nd order ODE with Dirac delta?,How to solve this 2nd order ODE with Dirac delta?,,"I need to solve the following ODE $$ f''(x) - \zeta f(x) + \zeta\delta(x-b) = 0,$$ where $x\in(-\infty,\infty)$ and where $f(x)\rightarrow 0$ as $x \rightarrow \mp \infty$ . The solution ignoring the Dirac impulse is given by $$f(x) = c_1 e^{\sqrt{\zeta}x} + c_2 e^{-\sqrt{\zeta}x}.$$ Since I have a Dirac impulse at $x=b$ , I should be solving for two ODEs, one below $x=b$ and another above $x=b$ . Then I have to put together both solutions at $x=b$ . This is where I am confused, how can I do this part? A bit more on the intuition behind the problem. The ODE in question is a steady state Fokker-Planck (or Kolmogorov Forward) Equation. Mass is injected at $x=b$ and dissipates both to the left and right of $x=b$ . Then, mass anywhere in $x\in(-\infty,\infty)$ is taken out at a rate $\zeta$ and reinjected back to $x=b$ .","I need to solve the following ODE where and where as . The solution ignoring the Dirac impulse is given by Since I have a Dirac impulse at , I should be solving for two ODEs, one below and another above . Then I have to put together both solutions at . This is where I am confused, how can I do this part? A bit more on the intuition behind the problem. The ODE in question is a steady state Fokker-Planck (or Kolmogorov Forward) Equation. Mass is injected at and dissipates both to the left and right of . Then, mass anywhere in is taken out at a rate and reinjected back to ."," f''(x) - \zeta f(x) + \zeta\delta(x-b) = 0, x\in(-\infty,\infty) f(x)\rightarrow 0 x \rightarrow \mp \infty f(x) = c_1 e^{\sqrt{\zeta}x} + c_2 e^{-\sqrt{\zeta}x}. x=b x=b x=b x=b x=b x=b x\in(-\infty,\infty) \zeta x=b","['ordinary-differential-equations', 'partial-differential-equations', 'stochastic-processes', 'dirac-delta']"
71,Logarithm and absolute value,Logarithm and absolute value,,"$$y' - y \tan x = 2x \sec x,\quad y(0)=0\tag1$$ integrating factor $= e^{-\int \tan x\ dx} = e^{\ln|\cos x|} = \cos x$ Can we write $|\cos x|$ as $\cos x$ above? $$I.F. y = \int I.F.\ 2x\ \sec x\ dx\\(\cos x) y = \int \cos x \ 2x\ \sec x\ dx = \int 2x\ dx$$ If we had taken the integrating factor to be $ |\cos x|$ , then in the above line $|\cos x|$ and $\sec x$ wouldn't have cancelled.","integrating factor Can we write as above? If we had taken the integrating factor to be , then in the above line and wouldn't have cancelled.","y' - y \tan x = 2x \sec x,\quad y(0)=0\tag1 = e^{-\int \tan x\ dx} = e^{\ln|\cos x|} = \cos x |\cos x| \cos x I.F. y = \int I.F.\ 2x\ \sec x\ dx\\(\cos x) y = \int \cos x \ 2x\ \sec x\ dx = \int 2x\ dx  |\cos x| |\cos x| \sec x","['calculus', 'ordinary-differential-equations', 'absolute-value']"
72,"Solution of non-linear differential equation $y''+ay'+b\sin(y)\cos(y)=c$, where $a,b,c$ are constants.","Solution of non-linear differential equation , where  are constants.","y''+ay'+b\sin(y)\cos(y)=c a,b,c","I am working on a project, in that project model I arrived at a differential equation. For further analysis, I need the solution of this equation. I tried to solve the equation but couldn't get success. Please anyone give some idea to solve this. $y''+ay'+b\sin(y)\cos(y)=c$ , where $a,b,c$ are the constants means they are combinations of parameter values of real life materials. If $y'=0$ then it can be solved by multiplying the equation by $y'$ and integrating it. But, in the present case I'm not getting any clue. Thanks in advance. Note: Ideas in the direction of approximated solution are also invited.","I am working on a project, in that project model I arrived at a differential equation. For further analysis, I need the solution of this equation. I tried to solve the equation but couldn't get success. Please anyone give some idea to solve this. , where are the constants means they are combinations of parameter values of real life materials. If then it can be solved by multiplying the equation by and integrating it. But, in the present case I'm not getting any clue. Thanks in advance. Note: Ideas in the direction of approximated solution are also invited.","y''+ay'+b\sin(y)\cos(y)=c a,b,c y'=0 y'",['ordinary-differential-equations']
73,Can we be certain that the only nonzero differentiable function satisfying $g(x+y)=\frac{g(x)+g(y)}{1+g(x)g(y)}$ and $|g|\lt 1$ is $\tanh(kx)$?,Can we be certain that the only nonzero differentiable function satisfying  and  is ?,g(x+y)=\frac{g(x)+g(y)}{1+g(x)g(y)} |g|\lt 1 \tanh(kx),"The following question is a question aimed for the Further Maths UK syllabus (it is a Step $2$ question)- i.e. not very formal, so I am certain my solution to the question is correct as the question intended it, but I am uncertain whether it is formally the only, or the right, solution. I am especially uncertain about my application of the Picard existence & uniqueness theorem. The problem: The function $g$ has derivative $g'$ and satisfies: $$g(x+y)=\frac{g(x)+g(y)}{1+g(x)g(y)}$$ For all $x$ and $y$ , $|g(x)|\lt 1$ for all $x$ , and $g'(0)=k\neq0$ . Find $g'(x)$ in terms of $g(x)$ and $k$ , and hence find $g(x)$ in terms of $x$ and $k$ . My solution (but is it unique, and is it rigorously done?): Consider the difference quotient for $y\neq 0$ , some $x\in\Bbb R$ : $$\tag{1}\frac{g(x+y)-g(x)}{y}=(1-g^2(x))\cdot\frac{g(y)}{y(1+g(x)g(y))}$$ First note that $|g(x)|\lt1$ gives $(1-g^2(x))\neq0$ and $1+g(x)g(y)\neq0$ . Secondly, if $g'(0)\neq0$ we must have that the numerator of the difference quotient must be nonzero in some small deleted neighbourhood of $0$ (else we could take a convergent sequence $y_n\to0$ where $g(y_n)=0$ for all $n$ , and then $g'(0)=0$ if the limit exists) so if we set $x=0$ , we may divide by $g(y)$ in the above with $y$ close to $0$ but $g(y)\neq0$ : $$\frac{g(y)-g(0)}{y}=(1-g^2(0))\cdot\frac{1}{\frac{y}{g(y)}+y\cdot g(0)}$$ For the above limit as $y\to0$ to exist, the limit as $y\to0$ of the denominator must exist: as $\lim_{y\to0}y\cdot g(0)$ exists (and equals $0$ ) we conclude $\lim_{y\to0}\frac{y}{g(y)}$ must exist, giving: $$\begin{align}k=\lim_{y\to0}\frac{g(y)-g(0)}{y}&=(1-g^2(0))\cdot\frac{1}{\lim_{y\to0}\frac{y}{g(y)}}\\\lim_{y\to0}\frac{g(y)}{y}&=\frac{k}{(1-g^2(0))}\end{align}$$ This further implies that $g(y)\to0$ as $y\to0$ and by continuity of differentiable functions $g(0)=0$ , so that: $$k=\lim_{y\to0}\frac{g(y)}{y}=\lim_{y\to0}\frac{g(y)-g(0)}{y}$$ Using the same argument that $g(y)\neq 0$ in some small deleted neighbourhood of $0$ and dividing by $g(y)$ in $(1)$ , for now arbitrary $x$ , we get: $$g'(x)=\lim_{y\to0}\frac{g(x+y)-g(x)}{y}=(1-g^2(x))\cdot\lim_{y\to0}\frac{g(y)}{y}=(1-g^2(x))\cdot k$$ The differential equation $g'(x)=k(1-g^2(x))$ has a solution in $g(x)=\tanh(kx)$ . I think that by the Picard existence theorem, the solution is unique since $|g(x)|\lt 1$ gives a Lipschitz bound on $k(1-g^2(x))$ , although even then we only have uniqueness potentially in some small interval around $0$ . Can we really conclude $g(x)=\tanh(kx)$ with certainty? And just how necessary is the condition $|g(x)|\lt1$ ?","The following question is a question aimed for the Further Maths UK syllabus (it is a Step question)- i.e. not very formal, so I am certain my solution to the question is correct as the question intended it, but I am uncertain whether it is formally the only, or the right, solution. I am especially uncertain about my application of the Picard existence & uniqueness theorem. The problem: The function has derivative and satisfies: For all and , for all , and . Find in terms of and , and hence find in terms of and . My solution (but is it unique, and is it rigorously done?): Consider the difference quotient for , some : First note that gives and . Secondly, if we must have that the numerator of the difference quotient must be nonzero in some small deleted neighbourhood of (else we could take a convergent sequence where for all , and then if the limit exists) so if we set , we may divide by in the above with close to but : For the above limit as to exist, the limit as of the denominator must exist: as exists (and equals ) we conclude must exist, giving: This further implies that as and by continuity of differentiable functions , so that: Using the same argument that in some small deleted neighbourhood of and dividing by in , for now arbitrary , we get: The differential equation has a solution in . I think that by the Picard existence theorem, the solution is unique since gives a Lipschitz bound on , although even then we only have uniqueness potentially in some small interval around . Can we really conclude with certainty? And just how necessary is the condition ?",2 g g' g(x+y)=\frac{g(x)+g(y)}{1+g(x)g(y)} x y |g(x)|\lt 1 x g'(0)=k\neq0 g'(x) g(x) k g(x) x k y\neq 0 x\in\Bbb R \tag{1}\frac{g(x+y)-g(x)}{y}=(1-g^2(x))\cdot\frac{g(y)}{y(1+g(x)g(y))} |g(x)|\lt1 (1-g^2(x))\neq0 1+g(x)g(y)\neq0 g'(0)\neq0 0 y_n\to0 g(y_n)=0 n g'(0)=0 x=0 g(y) y 0 g(y)\neq0 \frac{g(y)-g(0)}{y}=(1-g^2(0))\cdot\frac{1}{\frac{y}{g(y)}+y\cdot g(0)} y\to0 y\to0 \lim_{y\to0}y\cdot g(0) 0 \lim_{y\to0}\frac{y}{g(y)} \begin{align}k=\lim_{y\to0}\frac{g(y)-g(0)}{y}&=(1-g^2(0))\cdot\frac{1}{\lim_{y\to0}\frac{y}{g(y)}}\\\lim_{y\to0}\frac{g(y)}{y}&=\frac{k}{(1-g^2(0))}\end{align} g(y)\to0 y\to0 g(0)=0 k=\lim_{y\to0}\frac{g(y)}{y}=\lim_{y\to0}\frac{g(y)-g(0)}{y} g(y)\neq 0 0 g(y) (1) x g'(x)=\lim_{y\to0}\frac{g(x+y)-g(x)}{y}=(1-g^2(x))\cdot\lim_{y\to0}\frac{g(y)}{y}=(1-g^2(x))\cdot k g'(x)=k(1-g^2(x)) g(x)=\tanh(kx) |g(x)|\lt 1 k(1-g^2(x)) 0 g(x)=\tanh(kx) |g(x)|\lt1,"['real-analysis', 'ordinary-differential-equations', 'solution-verification', 'functional-equations', 'hyperbolic-functions']"
74,Finding Lyapunov function for particular system,Finding Lyapunov function for particular system,,"I am trying to find Lyapunov function for $$\begin{cases}\dot{t} = y\\\dot{y} = t^2-t\end{cases}$$ I tried common examples but, maybe I was wrong in my computations, couldn't derive anything. Could you help? Is there any approaches that cover some easy situations like this one (when variables are ""separated"")?","I am trying to find Lyapunov function for I tried common examples but, maybe I was wrong in my computations, couldn't derive anything. Could you help? Is there any approaches that cover some easy situations like this one (when variables are ""separated"")?",\begin{cases}\dot{t} = y\\\dot{y} = t^2-t\end{cases},"['ordinary-differential-equations', 'lyapunov-functions']"
75,What is the general solution of $\frac{df(x)}{dx} = f(x-a)$? [duplicate],What is the general solution of ? [duplicate],\frac{df(x)}{dx} = f(x-a),"This question already has answers here : How to solve differential equations of the form $f'(x) = f(x + a)$ (2 answers) Closed 2 years ago . Let $a \in \mathbb{R}$ be a constant. What is the general solution of the following delay differential equation (DDE)? $$\frac{df(x)}{dx} = f(x-a)$$ For example, for $a = - \frac{\pi}{2}$ , $$\begin{split} \frac{d(\sin x)}{dx} &= \cos x \\ &= \sin\left(x + \frac{\pi}{2}\right) \end{split}$$","This question already has answers here : How to solve differential equations of the form $f'(x) = f(x + a)$ (2 answers) Closed 2 years ago . Let be a constant. What is the general solution of the following delay differential equation (DDE)? For example, for ,","a \in \mathbb{R} \frac{df(x)}{dx} = f(x-a) a = - \frac{\pi}{2} \begin{split}
\frac{d(\sin x)}{dx} &= \cos x \\
&= \sin\left(x + \frac{\pi}{2}\right)
\end{split}","['ordinary-differential-equations', 'delay-differential-equations']"
76,Solving $y^{(2)}-5y^{(1)}+4y=\frac{1}{e^x+1}.$,Solving,y^{(2)}-5y^{(1)}+4y=\frac{1}{e^x+1}.,"The equation is the following $$ y^{(2)} - 5y^{(1)} + 4y =  \frac{1}{e^x + 1} $$ I’ve just started studying these kind of equations and I know of only two method to solve it, they both involve the following first step: Find 2 independent solutions of the associated homogeneous equation. I have done it through the characteristic polynomial and obtained the following 2 solutions: $$ y_1(x) = e^x \quad y_2(x)= e^{4x} $$ Now, 2 different possibilities I’m aware of are: A) If the $f(x)$ on the right is of the kind $e^{kx}P_m(x)$ , with $P_m$ polynomial of degree m, or of the kind $e^{kx}(P_m(x)\sin(\omega x)+R_k(x)\cos(\omega x))$ , then we know that the equation admits certain solutions similar in form to the $f(x)$ , that we need to specify finding the constants of the polynomials. The $f(x)$ in this case doesn’t belong to either of the types, still I tried this method imposing $e^z = \frac{1}{e^x+1}$ but I couldn’t make it work. B)The variation of parameters method. I solved the system to find $\gamma_1^{(1)}(x)$ and $\gamma_2^{(1)}(x)$ obtaining $$ \gamma_1^{(1)}(x) = -\frac{1}{3(e^x+1)}$$ $$ \gamma_2^{(1)}(x) = \frac{1}{3e^{3x}(e^x+1)}$$ Still, I wasn’t able to integrate the second function. I searched online for a solution and it came out very complicated which tells me that this isn’t the way is was intended to be solved (cause it was part of an exam and it shouldn’t take more than half an hour). Can you show me how it’s done? Thanks","The equation is the following I’ve just started studying these kind of equations and I know of only two method to solve it, they both involve the following first step: Find 2 independent solutions of the associated homogeneous equation. I have done it through the characteristic polynomial and obtained the following 2 solutions: Now, 2 different possibilities I’m aware of are: A) If the on the right is of the kind , with polynomial of degree m, or of the kind , then we know that the equation admits certain solutions similar in form to the , that we need to specify finding the constants of the polynomials. The in this case doesn’t belong to either of the types, still I tried this method imposing but I couldn’t make it work. B)The variation of parameters method. I solved the system to find and obtaining Still, I wasn’t able to integrate the second function. I searched online for a solution and it came out very complicated which tells me that this isn’t the way is was intended to be solved (cause it was part of an exam and it shouldn’t take more than half an hour). Can you show me how it’s done? Thanks", y^{(2)} - 5y^{(1)} + 4y =  \frac{1}{e^x + 1}   y_1(x) = e^x \quad y_2(x)= e^{4x}  f(x) e^{kx}P_m(x) P_m e^{kx}(P_m(x)\sin(\omega x)+R_k(x)\cos(\omega x)) f(x) f(x) e^z = \frac{1}{e^x+1} \gamma_1^{(1)}(x) \gamma_2^{(1)}(x)  \gamma_1^{(1)}(x) = -\frac{1}{3(e^x+1)}  \gamma_2^{(1)}(x) = \frac{1}{3e^{3x}(e^x+1)},"['calculus', 'ordinary-differential-equations']"
77,how to solve $(y')^4 x -2 y (y')^3 + 12 x^3 = 0$,how to solve,(y')^4 x -2 y (y')^3 + 12 x^3 = 0,"Is there a smart way to solve this first order ode $$     (y')^4 x -2 y (y')^3 + 12 x^3 = 0 $$ This is problem from Ordinary differential equations and their solutions. By George Moseley Murphy. 1960. I solved it myself, but my method is a brute force. First solved for $y'$ , which gave 4 solutions. This generated 4 ode's to solve. Each one of these ode's turned out to be isobaric. After applying the isobaric transformation, the ode becomes separable. However, the integrals are so complicated and could not solve them even on the computer.  So the solutions are left with unevaluated integrals. But they were verified correct by Maple. But will not post them here, as the integrals are too large. Maple solves this and gives these simple 5(!) solutions The Maple trace says it used trying 1st order ODE linearizable_by_differentiation Which I do not know how. Here is the full trace ode:=x*diff(y(x),x)^4-2*y(x)*diff(y(x),x)^3+12*x^3 = 0; infolevel[dsolve]:=5;  dsolve(ode)   trying 1st order ODE linearizable_by_differentiation  -> Solving 1st order ODE of high degree, Lie methods, 1st trial  -> Computing symmetries using: way = 2  -> Solving 1st order ODE of high degree, 2nd attempt. Trying parametric methods   *** Sublevel 3 ***   Methods for first order ODEs:   --- Trying classification methods ---   trying homogeneous types:   trying exact   Looking for potential symmetries   trying an equivalence to an Abel ODE   trying 1st order ODE linearizable_by_differentiation   -> Calling odsolve with the ODE diff(y(x) x) = (3*(x^4+12*y(x)^2)*y(x)/x- 4*y(x)*x^3)/(-x^4+36*y(x)^2) y(x)   *** Sublevel 3 ***   Methods for first order ODEs:   --- Trying classification methods ---   trying a quadrature   trying 1st order linear   <- 1st order linear successful   <- 1st order, parametric methods successful My question is, how did Maple obtain these simple solutions? From trace it says it used Lie symmetries which I am still learning. Does anyone see a ""simple"" method to solve this ode using some smart transformation and be able to obtain the solutions found by Maple?","Is there a smart way to solve this first order ode This is problem from Ordinary differential equations and their solutions. By George Moseley Murphy. 1960. I solved it myself, but my method is a brute force. First solved for , which gave 4 solutions. This generated 4 ode's to solve. Each one of these ode's turned out to be isobaric. After applying the isobaric transformation, the ode becomes separable. However, the integrals are so complicated and could not solve them even on the computer.  So the solutions are left with unevaluated integrals. But they were verified correct by Maple. But will not post them here, as the integrals are too large. Maple solves this and gives these simple 5(!) solutions The Maple trace says it used trying 1st order ODE linearizable_by_differentiation Which I do not know how. Here is the full trace ode:=x*diff(y(x),x)^4-2*y(x)*diff(y(x),x)^3+12*x^3 = 0; infolevel[dsolve]:=5;  dsolve(ode)   trying 1st order ODE linearizable_by_differentiation  -> Solving 1st order ODE of high degree, Lie methods, 1st trial  -> Computing symmetries using: way = 2  -> Solving 1st order ODE of high degree, 2nd attempt. Trying parametric methods   *** Sublevel 3 ***   Methods for first order ODEs:   --- Trying classification methods ---   trying homogeneous types:   trying exact   Looking for potential symmetries   trying an equivalence to an Abel ODE   trying 1st order ODE linearizable_by_differentiation   -> Calling odsolve with the ODE diff(y(x) x) = (3*(x^4+12*y(x)^2)*y(x)/x- 4*y(x)*x^3)/(-x^4+36*y(x)^2) y(x)   *** Sublevel 3 ***   Methods for first order ODEs:   --- Trying classification methods ---   trying a quadrature   trying 1st order linear   <- 1st order linear successful   <- 1st order, parametric methods successful My question is, how did Maple obtain these simple solutions? From trace it says it used Lie symmetries which I am still learning. Does anyone see a ""simple"" method to solve this ode using some smart transformation and be able to obtain the solutions found by Maple?","
    (y')^4 x -2 y (y')^3 + 12 x^3 = 0
 y'",['ordinary-differential-equations']
78,Integrating after using integrating factor,Integrating after using integrating factor,,I have a question here $x\frac{dy}{dx}-2y=x^4\sin(x)$ . I rearranged it to $\frac{dy}{dx}-\frac{2y}{x}=x^3\sin(x)$ I need to find a general solution to this. I used the integrating factor method and know the integrating factor is $e^{\int\frac{2}{x}dx}$ which is $e^{2\ln|x|}$ . I multiplied everything by the integrating factor. The equation becomes $\frac{dy}{dx}2\ln|x|-\frac{2y}{x}\ln|x|y=x^3\sin(x)2\ln|x|$ Question. How do I integrate and find the general solution from here? General solution = having $y=....$ . So clearly I need to integrate. How do I arrange this in a way so I can integrate it? And what is the final answer?,I have a question here . I rearranged it to I need to find a general solution to this. I used the integrating factor method and know the integrating factor is which is . I multiplied everything by the integrating factor. The equation becomes Question. How do I integrate and find the general solution from here? General solution = having . So clearly I need to integrate. How do I arrange this in a way so I can integrate it? And what is the final answer?,x\frac{dy}{dx}-2y=x^4\sin(x) \frac{dy}{dx}-\frac{2y}{x}=x^3\sin(x) e^{\int\frac{2}{x}dx} e^{2\ln|x|} \frac{dy}{dx}2\ln|x|-\frac{2y}{x}\ln|x|y=x^3\sin(x)2\ln|x| y=....,"['calculus', 'integration', 'ordinary-differential-equations']"
79,Generalizing solutions to the differential equation $y^{(n)} = y$.,Generalizing solutions to the differential equation .,y^{(n)} = y,"Question: Can you generalize to find solutions of the equation $y^{(n)} = y$ ? What I have done: I know the derivative all the way to the fifth power, and what I have surmised is the further down the derivative ""rabbit hole"" so to speak of this differential equation, the first 2 terms are always $c_1e^x$ and $c_2e^{-x}$ with cosines and since splitting the remaining terms. But I feel that isn't the true solution, any help is much appreciated.","Question: Can you generalize to find solutions of the equation ? What I have done: I know the derivative all the way to the fifth power, and what I have surmised is the further down the derivative ""rabbit hole"" so to speak of this differential equation, the first 2 terms are always and with cosines and since splitting the remaining terms. But I feel that isn't the true solution, any help is much appreciated.",y^{(n)} = y c_1e^x c_2e^{-x},"['calculus', 'ordinary-differential-equations']"
80,What is the integrating factor for this non-exact differential equation?,What is the integrating factor for this non-exact differential equation?,,"I am trying to solve this non-exact differential equation: $$2y(x^2-y+x)dx\,+\,(x^2-2y)dy = 0$$ Assuming that the integrating factor is of the form $x^my^m$ : $$2(n+1)x^{m+2}y^n-2(n+2)x^my^{n+1}+2(n+1)x^{m+1}y^n=(m+2)x^{m+1}y^n-2mx^{m-1}y^{n+1}$$ Equating co-efficients of like terms: $$2n+2=0\\-2n-4=0\\2n+2=m+2\\-2m=0$$ Obviously this system has no solution. What could I be missing here? Is the integrating factor of a form different from $x^my^n$ ?",I am trying to solve this non-exact differential equation: Assuming that the integrating factor is of the form : Equating co-efficients of like terms: Obviously this system has no solution. What could I be missing here? Is the integrating factor of a form different from ?,"2y(x^2-y+x)dx\,+\,(x^2-2y)dy = 0 x^my^m 2(n+1)x^{m+2}y^n-2(n+2)x^my^{n+1}+2(n+1)x^{m+1}y^n=(m+2)x^{m+1}y^n-2mx^{m-1}y^{n+1} 2n+2=0\\-2n-4=0\\2n+2=m+2\\-2m=0 x^my^n","['ordinary-differential-equations', 'integrating-factor']"
81,Envelope of oscillator signal,Envelope of oscillator signal,,"How do we find envelope ODE of a modulated oscillator obeying $$y^{''}(t)+y (t)\left(\dfrac{2 \pi}{t-c}\right)^2 = 0 $$ after obtaining solution say for initial conditions $y(0) = 1, y'(0) = 0?$ . Does it oscillate indefinitely? Could not readily apply $\; c-, p-$ discriminant methods. Please help. Numerical solution with $ c=t _{max} =6$ Envelope sketched by hand.",How do we find envelope ODE of a modulated oscillator obeying after obtaining solution say for initial conditions . Does it oscillate indefinitely? Could not readily apply discriminant methods. Please help. Numerical solution with Envelope sketched by hand.,"y^{''}(t)+y (t)\left(\dfrac{2 \pi}{t-c}\right)^2 = 0  y(0) = 1, y'(0) = 0? \; c-, p-  c=t _{max} =6","['ordinary-differential-equations', 'signal-processing']"
82,How is $y'=x^2+1$ a differential equation?,How is  a differential equation?,y'=x^2+1,"From James Stewart Essential Calculus Early Transcendentals Textbook, A differential equation is an equation that contains an unknown function and one or more of its derivatives. But the equation $y'=x^2+1$ doesn't contain the unknown function $y$ , it only contains $y'$ and $x$ . And yet my textbook says its a DE of order 1. Even Wikipedia says a DE ""relates one or more functions and their derivatives"", but $y'=x^2+1$ doesn't contain the function $y$ so how is it a DE ?","From James Stewart Essential Calculus Early Transcendentals Textbook, A differential equation is an equation that contains an unknown function and one or more of its derivatives. But the equation doesn't contain the unknown function , it only contains and . And yet my textbook says its a DE of order 1. Even Wikipedia says a DE ""relates one or more functions and their derivatives"", but doesn't contain the function so how is it a DE ?",y'=x^2+1 y y' x y'=x^2+1 y,['ordinary-differential-equations']
83,Number of arbitrary constants in an ODE,Number of arbitrary constants in an ODE,,"Consider the general solution of a differential equation- $$y=(C_1+C_2)\cos(x+C_3)+C_4\exp(x)+C_5$$ Without differentiating this equation and finding the differential equation for it, how can we say what the order of that DE is. Ofcourse, we can always count the number of arbitrary constants, which in this case I think are $4$ (since $C_1+ C_2$ is just one ). But my textbook says that the DE has order $3$ . Am I missing something? I think my mistake is in not absorbing one more constant, which I tried doing in the following way- $$y=K (\cos C_3 \cos x-\sin C_3 \sin x)+C_4\exp(x)+C_5$$ and then we can also absorb the cosine and sine of $C_3$ . But even here, it appears to me that we still have $4$ constants. Any help is appreciated.","Consider the general solution of a differential equation- Without differentiating this equation and finding the differential equation for it, how can we say what the order of that DE is. Ofcourse, we can always count the number of arbitrary constants, which in this case I think are (since is just one ). But my textbook says that the DE has order . Am I missing something? I think my mistake is in not absorbing one more constant, which I tried doing in the following way- and then we can also absorb the cosine and sine of . But even here, it appears to me that we still have constants. Any help is appreciated.",y=(C_1+C_2)\cos(x+C_3)+C_4\exp(x)+C_5 4 C_1+ C_2 3 y=K (\cos C_3 \cos x-\sin C_3 \sin x)+C_4\exp(x)+C_5 C_3 4,"['ordinary-differential-equations', 'constants']"
84,Solve the diffrential equation $\left( {{x^2} + xy + 4x + 2y + 4} \right)\frac{{dy}}{{dx}} - {y^2} = 0$,Solve the diffrential equation,\left( {{x^2} + xy + 4x + 2y + 4} \right)\frac{{dy}}{{dx}} - {y^2} = 0,"A solution curve of the differential equation $\left( {{x^2} + xy + 4x + 2y + 4} \right)\frac{{dy}}{{dx}} - {y^2} = 0$ , $x>0$ passes through the point (1,3). Find the solution curve. I am not able to proceed as I am not able to convert the standard differentiable form","A solution curve of the differential equation , passes through the point (1,3). Find the solution curve. I am not able to proceed as I am not able to convert the standard differentiable form",\left( {{x^2} + xy + 4x + 2y + 4} \right)\frac{{dy}}{{dx}} - {y^2} = 0 x>0,['ordinary-differential-equations']
85,Error analysis by differentiation,Error analysis by differentiation,,"I've been studying physics and I found this weird differentiation. $\ln x = \ln a  + \ln b$ Now differentiating both sides, $\dfrac{dx}x = \dfrac{da}a + \dfrac{db}b$ First of all this weird differentiation doesn't make sense to me. I understand that $(\ln x)'$ will be $\frac1x$ and in no way $\frac{dx}x$ . So I asked a person about it and they replied with this: Basically, they told me that we have differentiated both sides wrt $x$ . Now according to me, differentiating R.H.S. i.e. $\ln a$ wrt x should yield $0$ . But according to them, it is $\dfrac{d(\ln a)}{dx} = \dfrac{da}{adx}$ I don't get it!","I've been studying physics and I found this weird differentiation. Now differentiating both sides, First of all this weird differentiation doesn't make sense to me. I understand that will be and in no way . So I asked a person about it and they replied with this: Basically, they told me that we have differentiated both sides wrt . Now according to me, differentiating R.H.S. i.e. wrt x should yield . But according to them, it is I don't get it!",\ln x = \ln a  + \ln b \dfrac{dx}x = \dfrac{da}a + \dfrac{db}b (\ln x)' \frac1x \frac{dx}x x \ln a 0 \dfrac{d(\ln a)}{dx} = \dfrac{da}{adx},"['ordinary-differential-equations', 'partial-differential-equations']"
86,Use Frobenius' method to find two independent solutions to the ODE $4tx''(t)+2x'(t)+x(t)=0$,Use Frobenius' method to find two independent solutions to the ODE,4tx''(t)+2x'(t)+x(t)=0,"It is known that $t=0$ is a regular singular point of $4tx''(t)+2x'(t)+x(t)=0$ . By Frobenius' method, show that two independent solutions of the ODE are given by $x_1(t)=\sum_{k=0}^\infty \frac{(-1)^k}{(2k)!}t^k$ and $x_2(t)=\sqrt{t}\sum_{k=0}^\infty$$\frac{(-1)^k}{(2k+1)!}t^k$ -- So far I have used the Frobenius ansatz $x(t)=\sum_{k=0}^\infty a_kt^{k+r}$ which implies $x'(t)=\sum_{k=0}^\infty (k+r)a_kt^{k+r-1}$ and $x''(t)=\sum_{k=0}^\infty (k+r-1)(k+r)a_kt^{k+r-2}$ Substituting into the ODE gives $4\sum_{k=0}^\infty (k+r-1)(k+r)a_kt^{k+r-1}+2\sum_{k=0}^\infty (k+r)a_kt^{k+r-1}+\sum_{k=0}^\infty a_kt^{k+r}=0$ We can then change the summation index to make all of the powers of $t$ in each summation equal to $k+r-1$ : $4\sum_{k=0}^\infty (k+r-1)(k+r)a_kt^{k+r-1}+2\sum_{k=0}^\infty (k+r)a_kt^{k+r-1}+\sum_{k=1}^\infty a_{k-1}t^{k+r-1}=0$ Which we can combine to make: $4(r-\frac{1}{2})ra_0t^{r-1}+\sum_{k=1}^\infty t^{k+r-1}(a_k(4(k+r-\frac{1}{2})(k+r))+a_{k-1})$ It is here that I am unsure of what to do. I believe I need to find the indicial equation? Any help would be much appreciated!","It is known that is a regular singular point of . By Frobenius' method, show that two independent solutions of the ODE are given by and -- So far I have used the Frobenius ansatz which implies and Substituting into the ODE gives We can then change the summation index to make all of the powers of in each summation equal to : Which we can combine to make: It is here that I am unsure of what to do. I believe I need to find the indicial equation? Any help would be much appreciated!",t=0 4tx''(t)+2x'(t)+x(t)=0 x_1(t)=\sum_{k=0}^\infty \frac{(-1)^k}{(2k)!}t^k x_2(t)=\sqrt{t}\sum_{k=0}^\infty\frac{(-1)^k}{(2k+1)!}t^k x(t)=\sum_{k=0}^\infty a_kt^{k+r} x'(t)=\sum_{k=0}^\infty (k+r)a_kt^{k+r-1} x''(t)=\sum_{k=0}^\infty (k+r-1)(k+r)a_kt^{k+r-2} 4\sum_{k=0}^\infty (k+r-1)(k+r)a_kt^{k+r-1}+2\sum_{k=0}^\infty (k+r)a_kt^{k+r-1}+\sum_{k=0}^\infty a_kt^{k+r}=0 t k+r-1 4\sum_{k=0}^\infty (k+r-1)(k+r)a_kt^{k+r-1}+2\sum_{k=0}^\infty (k+r)a_kt^{k+r-1}+\sum_{k=1}^\infty a_{k-1}t^{k+r-1}=0 4(r-\frac{1}{2})ra_0t^{r-1}+\sum_{k=1}^\infty t^{k+r-1}(a_k(4(k+r-\frac{1}{2})(k+r))+a_{k-1}),"['sequences-and-series', 'ordinary-differential-equations', 'frobenius-method']"
87,Solve $x(1-x)y''+2(1-2x)y'-2y=0$ by the Frobenius Method,Solve  by the Frobenius Method,x(1-x)y''+2(1-2x)y'-2y=0,"Find the second solution. First solution is $\dfrac 1 {1-x}$ .  Solve by Frobenius Method : $$x(1-x)y''+2(1-2x)y'-2y=0\,.$$ The first solution I am able to get is $\dfrac{1}{1-x}$ . Other solution is $\dfrac{1}{x}$ , but I am getting $-\dfrac{1}{x(1-x)}$ . Where am I going wrong?","Find the second solution. First solution is .  Solve by Frobenius Method : The first solution I am able to get is . Other solution is , but I am getting . Where am I going wrong?","\dfrac 1 {1-x} x(1-x)y''+2(1-2x)y'-2y=0\,. \dfrac{1}{1-x} \dfrac{1}{x} -\dfrac{1}{x(1-x)}","['calculus', 'ordinary-differential-equations', 'power-series', 'frobenius-method']"
88,( Homework Help ) on Differential Equation $y^4dx+2xy^3dy=\frac{ydx-xdy}{x^3y^3}$,( Homework Help ) on Differential Equation,y^4dx+2xy^3dy=\frac{ydx-xdy}{x^3y^3},I am stuck with a question of differential equation. $$~y^4dx+2xy^3dy=\dfrac{ydx-xdy}{x^3y^3}~$$ My book solves it in a peculiar way by multiplying it by $~\dfrac{y}{x}~$ and forming perfect derivatives of $~x^3y^6~$ and $~\ln\left(\dfrac{y}{x}\right)~$ . I could not understand the intuition behind this rearrangement of the terms. I tried other methods like forming homogeneous equation by substitution or trigonometric substitutions with no success. So what is the exact logic behind forming these perfect differentials in the question? Also is there any other method to solve the question?,I am stuck with a question of differential equation. My book solves it in a peculiar way by multiplying it by and forming perfect derivatives of and . I could not understand the intuition behind this rearrangement of the terms. I tried other methods like forming homogeneous equation by substitution or trigonometric substitutions with no success. So what is the exact logic behind forming these perfect differentials in the question? Also is there any other method to solve the question?,~y^4dx+2xy^3dy=\dfrac{ydx-xdy}{x^3y^3}~ ~\dfrac{y}{x}~ ~x^3y^6~ ~\ln\left(\dfrac{y}{x}\right)~,"['calculus', 'ordinary-differential-equations']"
89,"Stability of non-linear, non-autonomous ODE","Stability of non-linear, non-autonomous ODE",,"Consider the ODE $y'(t)=S(K(t)y(t)+b(t))$ where $K(t)$ is a matrix, $y(t), b(t)$ are vectors and $S$ applies a function $\mathbb{R}\to\mathbb{R}$ componentwise. Can the stability of this ODE be assured by chosing $K$ such that the Jacobian of the right-hand side  only has eigenvalues with negative real part at each instant ? I read a paper where this is claimed, but I cannot find a version of this criterion for non-autonomous, non-linear systems. Also why can we talk about the whole ODE being stable if generally speaking for non-linear ODEs we can only speak about stability for specific solutions ?","Consider the ODE where is a matrix, are vectors and applies a function componentwise. Can the stability of this ODE be assured by chosing such that the Jacobian of the right-hand side  only has eigenvalues with negative real part at each instant ? I read a paper where this is claimed, but I cannot find a version of this criterion for non-autonomous, non-linear systems. Also why can we talk about the whole ODE being stable if generally speaking for non-linear ODEs we can only speak about stability for specific solutions ?","y'(t)=S(K(t)y(t)+b(t)) K(t) y(t), b(t) S \mathbb{R}\to\mathbb{R} K","['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes', 'stability-theory']"
90,Show that a solution of the IVP is bounded,Show that a solution of the IVP is bounded,,"in my way to understand ODE's I've found some problems that I have no idea how to tackle and how to relate to what I've learn so far. For example, this one: Let $x(t)\in C^{1}([0,T])$ be a solution of the IVP $\dot x = A(t)x$ , $x(0)=x_{0}$ with $(t,x) \in [0,T]\times \mathbb{R}^{n}$ and $x_{0} \in \mathbb{R}^{n}$ . Suppose $A(t)v \cdot v \leq M|v|^{2}$ for all $v \in \mathbb{R}^{n}$ and $M>0$ some constant. Show that $|x(t)|\leq |x_{0}|e^{Mt}$ . Any help would be really appreciated. Thanks so much for all your help. :)","in my way to understand ODE's I've found some problems that I have no idea how to tackle and how to relate to what I've learn so far. For example, this one: Let be a solution of the IVP , with and . Suppose for all and some constant. Show that . Any help would be really appreciated. Thanks so much for all your help. :)","x(t)\in C^{1}([0,T]) \dot x = A(t)x x(0)=x_{0} (t,x) \in [0,T]\times \mathbb{R}^{n} x_{0} \in \mathbb{R}^{n} A(t)v \cdot v \leq M|v|^{2} v \in \mathbb{R}^{n} M>0 |x(t)|\leq |x_{0}|e^{Mt}","['real-analysis', 'calculus', 'ordinary-differential-equations']"
91,Product of two Hölder continuous function on a bounded interval is also Hölder continuous.,Product of two Hölder continuous function on a bounded interval is also Hölder continuous.,,"Prove that the product of two Hölder-1/2 continuous functions on a bounded interval is also Hölder-1/2 continuous. Given a function on a bounded interval, $f:I\rightarrow \mathbb{R}$ , $f$ is Hölder-1/2 continuous if there exists some $M$ such that $|f(x_1)-f(x_2)|\leq M|x_1-x_2|^{1/2}$ . But I'm having trouble how to link this to the product of two Hölder continuous functions, $fg:I\rightarrow\mathbb{R}$ .","Prove that the product of two Hölder-1/2 continuous functions on a bounded interval is also Hölder-1/2 continuous. Given a function on a bounded interval, , is Hölder-1/2 continuous if there exists some such that . But I'm having trouble how to link this to the product of two Hölder continuous functions, .",f:I\rightarrow \mathbb{R} f M |f(x_1)-f(x_2)|\leq M|x_1-x_2|^{1/2} fg:I\rightarrow\mathbb{R},"['ordinary-differential-equations', 'holder-spaces']"
92,Solve the differential equation $\sin x\frac {dy}{dx}+(\cos x)y=\sin(x^2)$ [closed],Solve the differential equation  [closed],\sin x\frac {dy}{dx}+(\cos x)y=\sin(x^2),"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question $$\sin x\frac {dy}{dx}+(\cos x)y=\sin(x^2)$$ $$\frac {d}{dx} y \sin x=\sin(x^2)$$ $$y\sin x=\int \sin(x^2)dx = -\frac{1}{2x}\cos(x^2)+C$$ $$y=-\frac{\cos(x^2)}{2x\sin x}+\frac {C}{\sin x}$$ where C is constant Is my answer correct?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question where C is constant Is my answer correct?",\sin x\frac {dy}{dx}+(\cos x)y=\sin(x^2) \frac {d}{dx} y \sin x=\sin(x^2) y\sin x=\int \sin(x^2)dx = -\frac{1}{2x}\cos(x^2)+C y=-\frac{\cos(x^2)}{2x\sin x}+\frac {C}{\sin x},['ordinary-differential-equations']
93,Bernoulli equation with $e^y$ term,Bernoulli equation with  term,e^y,"I'm having a hard time solving the following differential equation: $$(x^3 + e^y)y' = 3x^2$$ I'm familiar with the approach of introducing $z=y^{1-2}=y^{-1}$ , but that doesn't do the trick. Am I missing something?","I'm having a hard time solving the following differential equation: I'm familiar with the approach of introducing , but that doesn't do the trick. Am I missing something?",(x^3 + e^y)y' = 3x^2 z=y^{1-2}=y^{-1},['ordinary-differential-equations']
94,Understanding a non-autonomous ODE,Understanding a non-autonomous ODE,,"Consider $x'(t) = a(t)x$ a) Find a formula involving integrals for the solution of this system. b) Prove that your formula gives the general solution of this system. I am new to ODE's and we have gone over integrating the system... $x'/x = a(t)$ in order to attempt to isolate x in terms of $t$ .  I thus get $log(x) = 0.5at^2+C \; \text{(a constant)}$ , which yields $x(t) = e^(.5at^2+C)$ , not sure if I'm right here.  I'm also not sure why it says ""involving integrals,"" making it sound like the solution needs integral signs in it? For part b, I have no idea how to even start it, not sure what I need to show. Any help appreciated!","Consider a) Find a formula involving integrals for the solution of this system. b) Prove that your formula gives the general solution of this system. I am new to ODE's and we have gone over integrating the system... in order to attempt to isolate x in terms of .  I thus get , which yields , not sure if I'm right here.  I'm also not sure why it says ""involving integrals,"" making it sound like the solution needs integral signs in it? For part b, I have no idea how to even start it, not sure what I need to show. Any help appreciated!",x'(t) = a(t)x x'/x = a(t) t log(x) = 0.5at^2+C \; \text{(a constant)} x(t) = e^(.5at^2+C),['ordinary-differential-equations']
95,Solve Inverse Laplace Transform Using Input Integral Theorem,Solve Inverse Laplace Transform Using Input Integral Theorem,,"Problem Using the input integral principle below $$ \mathscr{L} \left[ \int_{0}^{t} f(u)du \right] (s) = \frac{1}{s} \mathscr{L} \left[ f(t) \right] (s), \ s > c $$ Find $ \mathscr{L}^{-1} \left[ \frac{1}{s(s^2+1)} \right](t) \ $ without using partial fractions. Attempt Letting $ \ f(t) = \mathscr{L}^{-1} \left[ \frac{1}{s(s^2+1)} \right](t) $ , $$ \mathscr{L} \left[ \int_{0}^{t} f(u)du \right] (s) = \frac{1}{s} \mathscr{L} \left[ \mathscr{L}^{-1} \left[ \frac{1}{s(s^2+1)} \right](t) \right] (s), \ s > c $$ $$ \mathscr{L} \left[ \int_{0}^{t} f(u)du \right] (s) = \frac{1}{s}  \left[ \frac{1}{s(s^2+1)} \right], \ s > c $$ $$ \mathscr{L} \left[ \int_{0}^{t} f(u)du \right] (s) = \frac{1}{s^2(s^2+1)} , \ s > c $$ Notes Perhaps I've approached this problem incorrectly, but I'm confused as how to proceed with it. All I'm looking for is a hint or correct first step in solving this problem, with a little bit of explanation as to what the correct method toward solving this problem entails. Also, I searched up what the input integral principle is and I'm not finding anything on it. Did my professor invent this name or is it an alias for something else? That being said, any help is appreciated. Thanks!","Problem Using the input integral principle below Find without using partial fractions. Attempt Letting , Notes Perhaps I've approached this problem incorrectly, but I'm confused as how to proceed with it. All I'm looking for is a hint or correct first step in solving this problem, with a little bit of explanation as to what the correct method toward solving this problem entails. Also, I searched up what the input integral principle is and I'm not finding anything on it. Did my professor invent this name or is it an alias for something else? That being said, any help is appreciated. Thanks!"," \mathscr{L} \left[ \int_{0}^{t} f(u)du \right] (s) = \frac{1}{s} \mathscr{L} \left[ f(t) \right] (s), \ s > c   \mathscr{L}^{-1} \left[ \frac{1}{s(s^2+1)} \right](t) \   \ f(t) = \mathscr{L}^{-1} \left[ \frac{1}{s(s^2+1)} \right](t)   \mathscr{L} \left[ \int_{0}^{t} f(u)du \right] (s) = \frac{1}{s} \mathscr{L} \left[ \mathscr{L}^{-1} \left[ \frac{1}{s(s^2+1)} \right](t) \right] (s), \ s > c   \mathscr{L} \left[ \int_{0}^{t} f(u)du \right] (s) = \frac{1}{s}  \left[ \frac{1}{s(s^2+1)} \right], \ s > c   \mathscr{L} \left[ \int_{0}^{t} f(u)du \right] (s) = \frac{1}{s^2(s^2+1)} , \ s > c ","['integration', 'ordinary-differential-equations', 'definite-integrals', 'laplace-transform']"
96,Solving $f'(t)=f(t+\pi/2)$,Solving,f'(t)=f(t+\pi/2),"As the title indicates, I want to solve the first order ODE $f'(t)=f(t+\pi/2)$. Here, $f$ is a real function with real variable $t$, and the equation is true for all real $t$. Immediately, I can tell that $f(t)=\sin t$ or $\cos t$ are solutions, since $f'(t)=\frac{d}{dt}\sin t=\cos t = \sin(t+\pi/2)=f(t+\pi/2)$ and similarly for the cosine. After some pondering, I realised that $f(t)=C_1\cos t + C_2\sin t$ is a solution as well for any constants $C_1$ and $C_2$. I'm tempted to think that these are all the solutions, but I cannot say so for sure; I know that these are all the solutions if the principle of superposition applies, but the way I learnt it, the principle only applies to differential equations of the form $p(D)f(t)=g(t)$ where $p$ is a polynomial. Thus, I have two questions. Firstly, is what I have given indeed all of the solutions, and how do you prove that? (Perhaps equivalently, how do you prove or disprove that the principle of superposition holds for any imaginable differential equation, no matter what form it is in?) Secondly, and more importantly, how do you properly solve this question? I got my solutions based on pure guessing and not proper reasoning, but there has to be a method to solve this step-by-step without such a huge leap of faith! I have googled for this exact question, but as most resources out there are dedicated to solving equations where the argument of the functions do not change (i.e. the arguments are always $t$ and not $t+\pi/2$), I couldn't find anything useful.","As the title indicates, I want to solve the first order ODE $f'(t)=f(t+\pi/2)$. Here, $f$ is a real function with real variable $t$, and the equation is true for all real $t$. Immediately, I can tell that $f(t)=\sin t$ or $\cos t$ are solutions, since $f'(t)=\frac{d}{dt}\sin t=\cos t = \sin(t+\pi/2)=f(t+\pi/2)$ and similarly for the cosine. After some pondering, I realised that $f(t)=C_1\cos t + C_2\sin t$ is a solution as well for any constants $C_1$ and $C_2$. I'm tempted to think that these are all the solutions, but I cannot say so for sure; I know that these are all the solutions if the principle of superposition applies, but the way I learnt it, the principle only applies to differential equations of the form $p(D)f(t)=g(t)$ where $p$ is a polynomial. Thus, I have two questions. Firstly, is what I have given indeed all of the solutions, and how do you prove that? (Perhaps equivalently, how do you prove or disprove that the principle of superposition holds for any imaginable differential equation, no matter what form it is in?) Secondly, and more importantly, how do you properly solve this question? I got my solutions based on pure guessing and not proper reasoning, but there has to be a method to solve this step-by-step without such a huge leap of faith! I have googled for this exact question, but as most resources out there are dedicated to solving equations where the argument of the functions do not change (i.e. the arguments are always $t$ and not $t+\pi/2$), I couldn't find anything useful.",,"['calculus', 'ordinary-differential-equations', 'periodic-functions']"
97,Construct Two Linearly Independent Power Series Solutions to $(1+z^2)u''+3zu'+u=0$,Construct Two Linearly Independent Power Series Solutions to,(1+z^2)u''+3zu'+u=0,"I am trying to construct two linearly independent, power series solutions to the ODE $$(1+z^2)u''+3zu'+u=0$$ My attempt: Let $$u=\sum_{k=0}^{\infty}A_kz^k\implies u'=\sum_{k=1}^{\infty}kA_kz^{k-1}\implies u''=\sum_{k=2}^{\infty}k(k-1)A_kz^{k-2}$$ Substituting this into the ODE, I find that  $$\sum_{k=0}^{\infty}\left((k+2)(k+1)A_{k+2}+(k+1)^2A_k\right)z^k=0$$ Hence $u$ is a solution iff $$A_{k+2}=-\frac{k+1}{k+2}A_k \ \ \ k\geq 0$$ I do not see how to proceed, especially considering there are no intial conditions to find $A_0$ and $A_1$. Any advice would be greatly appreciated.","I am trying to construct two linearly independent, power series solutions to the ODE $$(1+z^2)u''+3zu'+u=0$$ My attempt: Let $$u=\sum_{k=0}^{\infty}A_kz^k\implies u'=\sum_{k=1}^{\infty}kA_kz^{k-1}\implies u''=\sum_{k=2}^{\infty}k(k-1)A_kz^{k-2}$$ Substituting this into the ODE, I find that  $$\sum_{k=0}^{\infty}\left((k+2)(k+1)A_{k+2}+(k+1)^2A_k\right)z^k=0$$ Hence $u$ is a solution iff $$A_{k+2}=-\frac{k+1}{k+2}A_k \ \ \ k\geq 0$$ I do not see how to proceed, especially considering there are no intial conditions to find $A_0$ and $A_1$. Any advice would be greatly appreciated.",,['ordinary-differential-equations']
98,Using inverse Laplace transform to solve differential equation,Using inverse Laplace transform to solve differential equation,,The differential equation is as follows- $$\frac{d^2 x}{dt^2} + 5 \frac{dx}{dt} + 6x = e^t $$ I use laplace transform to make it to become - $$X(s) = \frac{1}{(s-1)(s+3)(s+2)}$$ where $X(s)$ is the Laplace transform of $X(t)$ So now I am trying to find $X(t)$ using inverse transform. From partial fractions- $X(s) = \frac{1}{(s-1)(s+3)(s+2)} = \frac{A}{s-1} + \frac{B}{s+3} + \frac{C}{s+2} $ Numerator - $ 1 = A(s+3)(s+2) + B(s-1)(s+2) + C(s-1)(s+3) $ I am stuck from here on how to carry on this partial fraction Can I sub all s values to be 0 ? For example $1 = A(0+3)(0+2)$ $1= B(0-1)(0+2) $ $1 = C (0-1)(0+3) $,The differential equation is as follows- $$\frac{d^2 x}{dt^2} + 5 \frac{dx}{dt} + 6x = e^t $$ I use laplace transform to make it to become - $$X(s) = \frac{1}{(s-1)(s+3)(s+2)}$$ where $X(s)$ is the Laplace transform of $X(t)$ So now I am trying to find $X(t)$ using inverse transform. From partial fractions- $X(s) = \frac{1}{(s-1)(s+3)(s+2)} = \frac{A}{s-1} + \frac{B}{s+3} + \frac{C}{s+2} $ Numerator - $ 1 = A(s+3)(s+2) + B(s-1)(s+2) + C(s-1)(s+3) $ I am stuck from here on how to carry on this partial fraction Can I sub all s values to be 0 ? For example $1 = A(0+3)(0+2)$ $1= B(0-1)(0+2) $ $1 = C (0-1)(0+3) $,,"['ordinary-differential-equations', 'laplace-transform', 'partial-fractions']"
99,Solving a differential equation in distribution/generalised functions.,Solving a differential equation in distribution/generalised functions.,,I am stuck trying to solve the following differential equation in terms of distribution (theory). $$x\frac{du}{dx} - \lambda u = 0.$$ This is just for $\mathbb{R}$ and $\lambda \in \mathbb{C}$. I know the regular distribution corresponding to the function $f(x) = Ax^{\lambda}$ is a solution but the solution says that $c\delta$ is also a solution for some $c \in \mathbb{C}$ and I am not sure I can see how to derive this.,I am stuck trying to solve the following differential equation in terms of distribution (theory). $$x\frac{du}{dx} - \lambda u = 0.$$ This is just for $\mathbb{R}$ and $\lambda \in \mathbb{C}$. I know the regular distribution corresponding to the function $f(x) = Ax^{\lambda}$ is a solution but the solution says that $c\delta$ is also a solution for some $c \in \mathbb{C}$ and I am not sure I can see how to derive this.,,"['ordinary-differential-equations', 'distribution-theory']"
