,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Find all possible Jordan Canonical forms and the eigenspaces dimensions of a nilpotent matrix,Find all possible Jordan Canonical forms and the eigenspaces dimensions of a nilpotent matrix,,"$A$ is a $10X10$ matrix. $$\operatorname{rank}\left(A^2\right)=2$$ $$\operatorname{rank}\left(A^3\right)=0$$ So from this I know that: All the eigenvalues are $0$ (so i'll just talk about the blocks's sizes) The minimal polynomial is $\lambda^3$, so the largest Jordan block is of size 3 There are 2 blocks of at least size 3 (because of the nullity difference between the ranks) $\implies$ There are 3 blocks of size 3. So I think the possible JCFs are: Two blocks of size 3, two blocks of size 2 Two blocks of size 3, one block of size 2, two blocks of size 1 Two blocks of size 3, four blocks of size 1 But I'm not sure how to find the dimension of the eigenspaces for $A^2,A^3$. Is it just the rank of the matrix itself?","$A$ is a $10X10$ matrix. $$\operatorname{rank}\left(A^2\right)=2$$ $$\operatorname{rank}\left(A^3\right)=0$$ So from this I know that: All the eigenvalues are $0$ (so i'll just talk about the blocks's sizes) The minimal polynomial is $\lambda^3$, so the largest Jordan block is of size 3 There are 2 blocks of at least size 3 (because of the nullity difference between the ranks) $\implies$ There are 3 blocks of size 3. So I think the possible JCFs are: Two blocks of size 3, two blocks of size 2 Two blocks of size 3, one block of size 2, two blocks of size 1 Two blocks of size 3, four blocks of size 1 But I'm not sure how to find the dimension of the eigenspaces for $A^2,A^3$. Is it just the rank of the matrix itself?",,"['linear-algebra', 'matrices', 'proof-verification', 'nilpotence']"
1,Properties of matrix stable (numerical) rank,Properties of matrix stable (numerical) rank,,"I happened to notice that there is concept ""stable rank"" that people used a lot in matrix computation theories, such as the work of Rudelson & Vershynin (2005) . It is defined to be the ratio between squared Frobenius norm and the squared spectral norm of a matrix. $$r(M) = ||M||_F^2/||M||_2^2$$ I was wondering if there is any literature or reference that discuss some basic properties of this stable rank. For example, do we know $$r(M_1+M_2) \le r(M_1) + r(M_2)$$ which is expected to be true since the stable rank is expected to be more stable than rank? Thanks.","I happened to notice that there is concept ""stable rank"" that people used a lot in matrix computation theories, such as the work of Rudelson & Vershynin (2005) . It is defined to be the ratio between squared Frobenius norm and the squared spectral norm of a matrix. $$r(M) = ||M||_F^2/||M||_2^2$$ I was wondering if there is any literature or reference that discuss some basic properties of this stable rank. For example, do we know $$r(M_1+M_2) \le r(M_1) + r(M_2)$$ which is expected to be true since the stable rank is expected to be more stable than rank? Thanks.",,"['matrices', 'numerical-methods', 'spectral-theory', 'matrix-decomposition', 'matrix-rank']"
2,Condition number of a $2\times 2$ square block matrix,Condition number of a  square block matrix,2\times 2,"Is there a general rule to relate the condition number of the $2\times2$ square block matrix $ \left(\begin{array}{cc} A & B\\ C & D\\ \end{array}\right), $ where the matrices have the following sizes: $A$: $n\times{n}$ $B$: $n\times{m}$ $C$: $m\times{n}$ $D$: $m\times{m}$ If not, what I'm actually after is the following, more specific case; $ \left(\begin{array}{cc} A & B\\ B^T & 0\\ \end{array}\right), $ where $A$ is symmetric, such that the block matrix is also symmetric. I couldn't find any properties of the condition number that would help, neither did writing out the definition...","Is there a general rule to relate the condition number of the $2\times2$ square block matrix $ \left(\begin{array}{cc} A & B\\ C & D\\ \end{array}\right), $ where the matrices have the following sizes: $A$: $n\times{n}$ $B$: $n\times{m}$ $C$: $m\times{n}$ $D$: $m\times{m}$ If not, what I'm actually after is the following, more specific case; $ \left(\begin{array}{cc} A & B\\ B^T & 0\\ \end{array}\right), $ where $A$ is symmetric, such that the block matrix is also symmetric. I couldn't find any properties of the condition number that would help, neither did writing out the definition...",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
3,"What is the opposite of ""sparsity"" in a matrix?","What is the opposite of ""sparsity"" in a matrix?",,"If a sparse matrix has only 1% non-zero entries, I find it weird to speak of ""1% sparsity"". In particular, ""increasing sparsity"" goes along with a smaller percentage of non-zero entries, so this is determined to cause confusion. However, I have never read about a matrix with ""99% sparsity"", expressing that the matrix has 1% non-zero entries. The number of non-zero entries seems to be what's used to characterize a sparse matrix. So which word to use? The dictionary proposes ""density"", but that may sound odd. A sparse matrix with 1% density? Sure, if there's nothing better. Or is there? Non-subjective variant of my question: What are terms used by other authors for the fraction of non-zero entries in a sparse matrix?","If a sparse matrix has only 1% non-zero entries, I find it weird to speak of ""1% sparsity"". In particular, ""increasing sparsity"" goes along with a smaller percentage of non-zero entries, so this is determined to cause confusion. However, I have never read about a matrix with ""99% sparsity"", expressing that the matrix has 1% non-zero entries. The number of non-zero entries seems to be what's used to characterize a sparse matrix. So which word to use? The dictionary proposes ""density"", but that may sound odd. A sparse matrix with 1% density? Sure, if there's nothing better. Or is there? Non-subjective variant of my question: What are terms used by other authors for the fraction of non-zero entries in a sparse matrix?",,"['linear-algebra', 'matrices', 'terminology', 'numerical-linear-algebra', 'sparse-matrices']"
4,Left ideals of $M_2(K)$ with $K$ a field,Left ideals of  with  a field,M_2(K) K,"Is it true that the only proper left ideals of $M_2(K)$, the ring of the matrices whose coefficients are in a field $K$, are $$ \left\{\begin{pmatrix}ah & ak \\ bh & bk\end{pmatrix}: a,b \in K\right\} $$ for all $h,k \in K$ not both $0$? This is my attempt. Proper ideals don't contain invertible elements, so the determinants of the elements in the ideals are all zeros. $2 \times 2$ matrices with null determinant are those with one of the two columns multiple of the other and one of the two rows multiple of the other, that is, the matrices of the form  $$\begin{pmatrix}ah & ak \\ bh & bk\end{pmatrix}, \quad a,b,h,k \in K$$ Let $I$ be a proper left ideal. It contains a matrix $\bigr(\begin{smallmatrix}ah & ak \\ bh & bk\end{smallmatrix}\bigl)$ for some $a\neq 0,b\neq 0,h,k \in K$ and $h,k$ not both $0$ and thus, for all $x,y \in K$, $$\begin{pmatrix}x & 0 \\ 0 & y \end{pmatrix} \begin{pmatrix}ah & ak \\ bh & bk\end{pmatrix} = \begin{pmatrix} ahx & akx \\ bhy & bky\end{pmatrix}$$ which implies that  $$\left\{\begin{pmatrix}ih & ik \\ jh & jk\end{pmatrix} : i,j \in K\right\} \subseteq I$$ The left hand side is itself a proper left ideal. Wlog $h\neq 0$. If $I$ contained $\bigr(\begin{smallmatrix}ah & al \\ bh & bl\end{smallmatrix}\bigl)$, with $l\neq k$, then it would also contain $$\begin{pmatrix}1& 0 \\ 0 & 0\end{pmatrix} \begin{pmatrix}ah & ak \\ bh & bk\end{pmatrix} +\begin{pmatrix}0 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix}ah & al \\ bh & bl\end{pmatrix} = \begin{pmatrix}ah & ak \\ bh & bl\end{pmatrix}$$ which is invertible, then $I=M_2(K)$, contradiction. Is this correct?","Is it true that the only proper left ideals of $M_2(K)$, the ring of the matrices whose coefficients are in a field $K$, are $$ \left\{\begin{pmatrix}ah & ak \\ bh & bk\end{pmatrix}: a,b \in K\right\} $$ for all $h,k \in K$ not both $0$? This is my attempt. Proper ideals don't contain invertible elements, so the determinants of the elements in the ideals are all zeros. $2 \times 2$ matrices with null determinant are those with one of the two columns multiple of the other and one of the two rows multiple of the other, that is, the matrices of the form  $$\begin{pmatrix}ah & ak \\ bh & bk\end{pmatrix}, \quad a,b,h,k \in K$$ Let $I$ be a proper left ideal. It contains a matrix $\bigr(\begin{smallmatrix}ah & ak \\ bh & bk\end{smallmatrix}\bigl)$ for some $a\neq 0,b\neq 0,h,k \in K$ and $h,k$ not both $0$ and thus, for all $x,y \in K$, $$\begin{pmatrix}x & 0 \\ 0 & y \end{pmatrix} \begin{pmatrix}ah & ak \\ bh & bk\end{pmatrix} = \begin{pmatrix} ahx & akx \\ bhy & bky\end{pmatrix}$$ which implies that  $$\left\{\begin{pmatrix}ih & ik \\ jh & jk\end{pmatrix} : i,j \in K\right\} \subseteq I$$ The left hand side is itself a proper left ideal. Wlog $h\neq 0$. If $I$ contained $\bigr(\begin{smallmatrix}ah & al \\ bh & bl\end{smallmatrix}\bigl)$, with $l\neq k$, then it would also contain $$\begin{pmatrix}1& 0 \\ 0 & 0\end{pmatrix} \begin{pmatrix}ah & ak \\ bh & bk\end{pmatrix} +\begin{pmatrix}0 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix}ah & al \\ bh & bl\end{pmatrix} = \begin{pmatrix}ah & ak \\ bh & bl\end{pmatrix}$$ which is invertible, then $I=M_2(K)$, contradiction. Is this correct?",,"['matrices', 'ring-theory', 'ideals']"
5,Solving for homography - SVD vs linear least squares (Matlab),Solving for homography - SVD vs linear least squares (Matlab),,"so I had an assignment in Matlab for solving for a homography (and stitching images) and I solved it by converting the coordinates into homogeneous form (since scale doesn't matter in our assignment) by sticking a 1 at the end and then using the built in linear least squares operator (x = A\B that one). My though process was if a homography is a transformation such that Hx=x' (where x is a set of points in image1 and x' is a corresponding set of points in image2), if you solve the linear least squares equation H = x'\x, that should give you the H that most closely transforms all points in image1 to the domain of image2. However, all of the literature I read used the SVD method, where you try to minimize AX=0, where A is this massive matrix storing all of the coefficients and X is the elements of the homography in vector form (like the one you can see in the answer here: How to compute homography matrix H from corresponding points (2d-2d planar Homography) ) And you basically do [U S V] = svd(A) and then X is the last column of V (corresponding to the smallest singular value). From what I understand, A\B is supposed to be for nonhomogenous least squares and a homography needs homogeneous least squares. But after I converted my x,y points into homogenous coordinates, ran the built in linear equation solver and then used my homography to warp and image and stitch them together, I got a pretty good mosaic. Maybe it was because we were doing a pretty simple panorama and it just happened to work out. But is there some major difference between the two methods? I converted nonhomogenous coordinates into homogenous coordinates before using mldivide. That seems like it should be equivalent to the other method, but I'm not sure. Thanks ahead of time!","so I had an assignment in Matlab for solving for a homography (and stitching images) and I solved it by converting the coordinates into homogeneous form (since scale doesn't matter in our assignment) by sticking a 1 at the end and then using the built in linear least squares operator (x = A\B that one). My though process was if a homography is a transformation such that Hx=x' (where x is a set of points in image1 and x' is a corresponding set of points in image2), if you solve the linear least squares equation H = x'\x, that should give you the H that most closely transforms all points in image1 to the domain of image2. However, all of the literature I read used the SVD method, where you try to minimize AX=0, where A is this massive matrix storing all of the coefficients and X is the elements of the homography in vector form (like the one you can see in the answer here: How to compute homography matrix H from corresponding points (2d-2d planar Homography) ) And you basically do [U S V] = svd(A) and then X is the last column of V (corresponding to the smallest singular value). From what I understand, A\B is supposed to be for nonhomogenous least squares and a homography needs homogeneous least squares. But after I converted my x,y points into homogenous coordinates, ran the built in linear equation solver and then used my homography to warp and image and stitch them together, I got a pretty good mosaic. Maybe it was because we were doing a pretty simple panorama and it just happened to work out. But is there some major difference between the two methods? I converted nonhomogenous coordinates into homogenous coordinates before using mldivide. That seems like it should be equivalent to the other method, but I'm not sure. Thanks ahead of time!",,"['matrices', 'linear-transformations']"
6,Why don't more celestial bodies exhibit higher-order rotations?,Why don't more celestial bodies exhibit higher-order rotations?,,"It is well known that the Earth spins on its axis. It is also well known that the Earth's axis also precesses, i.e. spins around a secondary axis, much more slowly. Less well known is that we have seen asteroids that seem to tumble through space, with precession rates on roughly the same order of magnitude as rotation rates. My question is, why haven't we seen many instances of higher orders of rotation (where the secondary axis spins around a tertiary axis, the tertiary axis spins around a quaternary axis, etc.)? Can rotations of a high enough order always be expressed in terms of lower-order rotations at higher rates of rotation, or do higher-order rotations eventually decay into lower-order ones (as, for example, rotating around the $x$, $y$, $z$, and $x$ axes, all at the same rate*, produces an about-face, a seemingly impossible behavior)? (The latter seems to be stated by a comment on this question , but it doesn't go into much explanation.) *This generates the matrix $$\begin{pmatrix}   \cos^2t & \sin t\cos t(\sin t-1) & \sin t(\sin t+\cos^2t) \\   \sin t(\sin t+\cos^2 t) & \cos t(\sin^3t-\sin^2t+\cos^2t) & \sin t\cos^2t(\sin t-2) \\   \sin t\cos t(\sin t-1) & \sin t(\sin^3t+2\cos^2t) & \cos t(\sin^3t-\sin^2t+\cos^2t) \\  \end{pmatrix}$$ with the about-face occurring at $t=\pi/2$.","It is well known that the Earth spins on its axis. It is also well known that the Earth's axis also precesses, i.e. spins around a secondary axis, much more slowly. Less well known is that we have seen asteroids that seem to tumble through space, with precession rates on roughly the same order of magnitude as rotation rates. My question is, why haven't we seen many instances of higher orders of rotation (where the secondary axis spins around a tertiary axis, the tertiary axis spins around a quaternary axis, etc.)? Can rotations of a high enough order always be expressed in terms of lower-order rotations at higher rates of rotation, or do higher-order rotations eventually decay into lower-order ones (as, for example, rotating around the $x$, $y$, $z$, and $x$ axes, all at the same rate*, produces an about-face, a seemingly impossible behavior)? (The latter seems to be stated by a comment on this question , but it doesn't go into much explanation.) *This generates the matrix $$\begin{pmatrix}   \cos^2t & \sin t\cos t(\sin t-1) & \sin t(\sin t+\cos^2t) \\   \sin t(\sin t+\cos^2 t) & \cos t(\sin^3t-\sin^2t+\cos^2t) & \sin t\cos^2t(\sin t-2) \\   \sin t\cos t(\sin t-1) & \sin t(\sin^3t+2\cos^2t) & \cos t(\sin^3t-\sin^2t+\cos^2t) \\  \end{pmatrix}$$ with the about-face occurring at $t=\pi/2$.",,"['matrices', 'geometry', 'physics', 'rotations']"
7,What are the basis vectors of the cone of positive semi definite matrices?,What are the basis vectors of the cone of positive semi definite matrices?,,"I was wondering if we could find a set of basis vectors that span the cone of positive semidefinite matrices? I know this question is hard, but I would really appreciate if even someone can share a related paper about this topic in the literature of Linear Algebra. I could not find any paper about this issue. I should mention that any answer to this question should be highly related to the topic of semidefinite programming in the area of mathematical optimization. Thanks","I was wondering if we could find a set of basis vectors that span the cone of positive semidefinite matrices? I know this question is hard, but I would really appreciate if even someone can share a related paper about this topic in the literature of Linear Algebra. I could not find any paper about this issue. I should mention that any answer to this question should be highly related to the topic of semidefinite programming in the area of mathematical optimization. Thanks",,"['linear-algebra', 'matrices', 'semidefinite-programming']"
8,Symmetric matrix and power of two [closed],Symmetric matrix and power of two [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question This is my first post. Given an square matrix $n\times n$, such that the elements on the main diagonal are 1, each row an each column has exact one time a number in $\{1,2,...,n\}$, the first column and first row are $(1,2,3,...,n)$, and if $a_{r,s}=a_{u,v}$, then $a_{u,s}=a_{r,v}$, how to prove that $n$ is a power of two? I have already shown that the matrix is symmetric, but i don't know how to conclude. Any hint is wellcome. For example, matrix $\left[\begin{array}{cccc}1&2&3&4\\2&1&4&3\\3&4&1&2\\4&3&2&1\end{array}\right]$ satisfies all the above conditions.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question This is my first post. Given an square matrix $n\times n$, such that the elements on the main diagonal are 1, each row an each column has exact one time a number in $\{1,2,...,n\}$, the first column and first row are $(1,2,3,...,n)$, and if $a_{r,s}=a_{u,v}$, then $a_{u,s}=a_{r,v}$, how to prove that $n$ is a power of two? I have already shown that the matrix is symmetric, but i don't know how to conclude. Any hint is wellcome. For example, matrix $\left[\begin{array}{cccc}1&2&3&4\\2&1&4&3\\3&4&1&2\\4&3&2&1\end{array}\right]$ satisfies all the above conditions.",,"['linear-algebra', 'matrices']"
9,Matrix Calculus and Linear Transformations,Matrix Calculus and Linear Transformations,,"I'm working on making the jump from differentiating real valued functions ($f: \mathbb{R}^n \rightarrow \mathbb{R}$) and vector valued functions ($g: \mathbb{R}^n \rightarrow \mathbb{R}^m$) to matrix valued functions ($h: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}^{n \times n}$) but am having trouble with matrix valued functions. I see that for first two cases, the total derivative $\mathscr{D}f$ is a linear transformation that can be represented as the Jacobian matrix, but when I was going through the Matrix Cookbook , I was confused on page ten.  The authors show $$\frac{\partial \mathbf{a}^{T} \mathbf{X} \mathbf{b}}{\partial \mathbf{X}} = \mathbf{a b}^T$$ and I interpreted this as computing the total derivative $\mathscr{D}f$ (the total derivative is the concept I'm trying to understand) where $f: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}$ takes $X$ to $\mathbf{a}^{T} \mathbf{X} \mathbf{b}$. Is this the right interpretation of the authors' notation, or are they (by using the partial notation here) computing something fundamentally different that I'm just missing? Since I interpreted this as the total derivative, I tried working with the Frechet definition to compute the term $\mathscr{D}f(X)H$, but since $f$ takes $\mathbb{R}^{n \times n}$ to $\mathbb{R}$, I'm having trouble lining up dimensions as $f(X + H)$ and $f(X)$ are $\mathbb{R}$ but $\mathscr{D}f(X)H$ is $\mathbb{R}^{1 \times n}$. This makes me think I'm mixing up concepts...  Any clarification would be greatly appreciated. Thanks.","I'm working on making the jump from differentiating real valued functions ($f: \mathbb{R}^n \rightarrow \mathbb{R}$) and vector valued functions ($g: \mathbb{R}^n \rightarrow \mathbb{R}^m$) to matrix valued functions ($h: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}^{n \times n}$) but am having trouble with matrix valued functions. I see that for first two cases, the total derivative $\mathscr{D}f$ is a linear transformation that can be represented as the Jacobian matrix, but when I was going through the Matrix Cookbook , I was confused on page ten.  The authors show $$\frac{\partial \mathbf{a}^{T} \mathbf{X} \mathbf{b}}{\partial \mathbf{X}} = \mathbf{a b}^T$$ and I interpreted this as computing the total derivative $\mathscr{D}f$ (the total derivative is the concept I'm trying to understand) where $f: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}$ takes $X$ to $\mathbf{a}^{T} \mathbf{X} \mathbf{b}$. Is this the right interpretation of the authors' notation, or are they (by using the partial notation here) computing something fundamentally different that I'm just missing? Since I interpreted this as the total derivative, I tried working with the Frechet definition to compute the term $\mathscr{D}f(X)H$, but since $f$ takes $\mathbb{R}^{n \times n}$ to $\mathbb{R}$, I'm having trouble lining up dimensions as $f(X + H)$ and $f(X)$ are $\mathbb{R}$ but $\mathscr{D}f(X)H$ is $\mathbb{R}^{1 \times n}$. This makes me think I'm mixing up concepts...  Any clarification would be greatly appreciated. Thanks.",,"['matrices', 'multivariable-calculus', 'derivatives', 'linear-transformations']"
10,Is it true that here will be strict inequality in interlacing of eigenvalues?,Is it true that here will be strict inequality in interlacing of eigenvalues?,,"Let $A,B,C$ be full rank ,real symmetric matrices. Consider $H=\left(\begin{array}{ccc} A&\begin{array}{ccc} 0&\cdots&0\\\vdots &\vdots &\vdots\\0&\cdots&1 \end{array}&O\\\begin{array}{ccc} 0&\cdots&0\\\vdots &\vdots &\vdots\\0&\cdots&1\end{array}&B&\begin{array}{ccc} 0&\cdots&0\\\vdots &\vdots &\vdots\\0&\cdots&1 \end{array}\\O&\begin{array}{ccc} 0&\cdots&0\\\vdots &\vdots &\vdots\\0&\cdots&1 \end{array}&C \end{array} \right)$ (all dots stand for zeros.) where $O$ is the zero matrix of respective order. let $P$ be submatrix of $H$ obtained by removing rows and columns corresponding to last row of $A$ and $B$. By ""Interlacing Theorem"", we know that eigenvalues of $P$ will be interlaced among eigenvalues of $H$. Is it true that here will be strict inequality in interlacing? In general, what will be the condition on $A,B$ to guarntee strict inequality in interlacing? I tried some examples in matlab and found inequalities to be strict in $H$ and $P$. But have no idea how to proceed. Any help or reference is highly welcome.","Let $A,B,C$ be full rank ,real symmetric matrices. Consider $H=\left(\begin{array}{ccc} A&\begin{array}{ccc} 0&\cdots&0\\\vdots &\vdots &\vdots\\0&\cdots&1 \end{array}&O\\\begin{array}{ccc} 0&\cdots&0\\\vdots &\vdots &\vdots\\0&\cdots&1\end{array}&B&\begin{array}{ccc} 0&\cdots&0\\\vdots &\vdots &\vdots\\0&\cdots&1 \end{array}\\O&\begin{array}{ccc} 0&\cdots&0\\\vdots &\vdots &\vdots\\0&\cdots&1 \end{array}&C \end{array} \right)$ (all dots stand for zeros.) where $O$ is the zero matrix of respective order. let $P$ be submatrix of $H$ obtained by removing rows and columns corresponding to last row of $A$ and $B$. By ""Interlacing Theorem"", we know that eigenvalues of $P$ will be interlaced among eigenvalues of $H$. Is it true that here will be strict inequality in interlacing? In general, what will be the condition on $A,B$ to guarntee strict inequality in interlacing? I tried some examples in matlab and found inequalities to be strict in $H$ and $P$. But have no idea how to proceed. Any help or reference is highly welcome.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
11,Maximal dimension of a vector space of square matrices in which every nonzero matrix is invertible,Maximal dimension of a vector space of square matrices in which every nonzero matrix is invertible,,"I'm interested in the maximal dimension of a subspace $V\leq\mathbb R^{n\times n}$ in which every nonzero matrix is invertible. Odd $n$: For odd $n$ the maximum is $1$: if $A$ and $B$ would be linearly independent, then $\det(xA+B)$ is an odd polynomial of degree $\leq n$ with leading coefficient $\det A\neq0$, so it has a real root $x$; hence $0\neq xA+B$ is not invertible. Even $n$: For $n$ even the maximum can be no larger than $n$: if $A_1,\ldots,A_{n+1}$ would be linearly independent, then the first row of every nontrivial linear combination must be nonzero, meaning that the first rows of $A_1,\ldots,A_{n+1}$ are linearly independent, contradicting $\dim\mathbb R^n=n$. For $n=2$ the maximum is $2$, as $\begin{vmatrix}a&-b\\b&a\end{vmatrix}=a^2+b^2\neq0$ for $a,b$ not both zero. For $n=4$ the maximum is $4$, as $$\begin{vmatrix}-a&b&c&d\\ b&a&-d&c\\ c&d&a&-b\\ d&-c&b&a\end{vmatrix}=-(a^2+b^2+c^2+d^2)^2.$$ Questions: For $n$ even, is the maximal dimension always $n$? Do similar such matrices exist with determinant $\pm\,(a_1^2+\cdots+a_n^2)^{n/2}$? This is an attempt to generalize problem $3$ of the PUMA 2016 .","I'm interested in the maximal dimension of a subspace $V\leq\mathbb R^{n\times n}$ in which every nonzero matrix is invertible. Odd $n$: For odd $n$ the maximum is $1$: if $A$ and $B$ would be linearly independent, then $\det(xA+B)$ is an odd polynomial of degree $\leq n$ with leading coefficient $\det A\neq0$, so it has a real root $x$; hence $0\neq xA+B$ is not invertible. Even $n$: For $n$ even the maximum can be no larger than $n$: if $A_1,\ldots,A_{n+1}$ would be linearly independent, then the first row of every nontrivial linear combination must be nonzero, meaning that the first rows of $A_1,\ldots,A_{n+1}$ are linearly independent, contradicting $\dim\mathbb R^n=n$. For $n=2$ the maximum is $2$, as $\begin{vmatrix}a&-b\\b&a\end{vmatrix}=a^2+b^2\neq0$ for $a,b$ not both zero. For $n=4$ the maximum is $4$, as $$\begin{vmatrix}-a&b&c&d\\ b&a&-d&c\\ c&d&a&-b\\ d&-c&b&a\end{vmatrix}=-(a^2+b^2+c^2+d^2)^2.$$ Questions: For $n$ even, is the maximal dimension always $n$? Do similar such matrices exist with determinant $\pm\,(a_1^2+\cdots+a_n^2)^{n/2}$? This is an attempt to generalize problem $3$ of the PUMA 2016 .",,"['linear-algebra', 'matrices', 'vector-spaces', 'contest-math', 'determinant']"
12,When is $P_\mathcal S ACA'Q_\mathcal S = 0$?,When is ?,P_\mathcal S ACA'Q_\mathcal S = 0,"All matrices in this question are real. Let C be a positive definite matrix and $A$ an arbitrary square matrix such that the product $ACA'$ makes sense. Let $\mathcal S$ be a space such that $R(A')\subseteq \mathcal S$, where $R$ here stands for range, or the column space of its matrix argument. The associated orthogonal projections are denoted $P_\mathcal S, Q_\mathcal S := I - P_\mathcal S$. What are necessary conditions on $\mathcal S$ for $P_\mathcal S ACA'Q_\mathcal S = 0$ to hold always? An obvious sufficient condition is that $R(A) \subseteq \mathcal S$ since then $(A'Q_\mathcal S)' = Q_\mathcal S A=0$. My intuition tells me that it is also necessary, but I cannot prove it. As pointed out in the comments, $A'$ means the transpose of $A$. Edit: We may assume $P_\mathcal S C Q_\mathcal S = 0$.","All matrices in this question are real. Let C be a positive definite matrix and $A$ an arbitrary square matrix such that the product $ACA'$ makes sense. Let $\mathcal S$ be a space such that $R(A')\subseteq \mathcal S$, where $R$ here stands for range, or the column space of its matrix argument. The associated orthogonal projections are denoted $P_\mathcal S, Q_\mathcal S := I - P_\mathcal S$. What are necessary conditions on $\mathcal S$ for $P_\mathcal S ACA'Q_\mathcal S = 0$ to hold always? An obvious sufficient condition is that $R(A) \subseteq \mathcal S$ since then $(A'Q_\mathcal S)' = Q_\mathcal S A=0$. My intuition tells me that it is also necessary, but I cannot prove it. As pointed out in the comments, $A'$ means the transpose of $A$. Edit: We may assume $P_\mathcal S C Q_\mathcal S = 0$.",,"['linear-algebra', 'matrices', 'vector-spaces']"
13,What lies beyond the Möbius transform?,What lies beyond the Möbius transform?,,"Consider the matrix $\pmatrix{a & b \\ c & d} ^n$ This is isomorphic to the $n$ th iteration of the Möbius transform $\frac{a z + b}{c z + d}$ when the determinant is nonzero. So I wonder what is the analogue isomorphism from a power of a $3 \times 3$ matrix to the iteration of an analytic function ? I prefer to stay on the complex plane, so I am not so intrested in iterating functions defined for noncomplex numbers or more than 2 dimensions.","Consider the matrix $\pmatrix{a & b \\ c & d} ^n$ This is isomorphic to the $n$ th iteration of the Möbius transform $\frac{a z + b}{c z + d}$ when the determinant is nonzero. So I wonder what is the analogue isomorphism from a power of a $3 \times 3$ matrix to the iteration of an analytic function ? I prefer to stay on the complex plane, so I am not so intrested in iterating functions defined for noncomplex numbers or more than 2 dimensions.",,"['matrices', 'mobius-transformation']"
14,What can we say about the eigenvalues and diagonalization of this $2N\times2N$ matrix $A$?,What can we say about the eigenvalues and diagonalization of this  matrix ?,2N\times2N A,"There is a $2N\times2N$ matrix $A$, where $N$ is a positive integer, which is of the form: $A=\left(\begin{array}{cc} B & C\\ -C^{*} & -B^{*} \end{array}\right),$ where $B$ is a hermitian matrix, and $C$ is a symmetric matrix. What can we say about the eigenvalues of $A$ and it's diagonalization? For example, if $N=1$: $A=\left(\begin{array}{cc} b & c\\ -c^{*} & -b^{*} \end{array}\right),$ where $b$ must be a real number, and $c$ has no restriction . The eigenvalues are the same if and only if $\mid c\mid=\mid b\mid$, and the eigenvalues are equal to $0$. But the matrix $A$ is not diagonalizable under this condition: $A=b\left(\begin{array}{cc} 1 & e^{i\theta}\\ -e^{-i\theta} & -1 \end{array}\right)=b\left(\begin{array}{cc} -e^{i\theta} & -e^{i\theta}\\ 1 & 0 \end{array}\right)\left(\begin{array}{cc} 0 & 1\\ 0 & 0 \end{array}\right)\left(\begin{array}{cc} 0 & 1\\ -e^{-i\theta} & -1 \end{array}\right).$ Is this an accident only for $N=1$ case? Is there some relation like this for general cases $N>1$?","There is a $2N\times2N$ matrix $A$, where $N$ is a positive integer, which is of the form: $A=\left(\begin{array}{cc} B & C\\ -C^{*} & -B^{*} \end{array}\right),$ where $B$ is a hermitian matrix, and $C$ is a symmetric matrix. What can we say about the eigenvalues of $A$ and it's diagonalization? For example, if $N=1$: $A=\left(\begin{array}{cc} b & c\\ -c^{*} & -b^{*} \end{array}\right),$ where $b$ must be a real number, and $c$ has no restriction . The eigenvalues are the same if and only if $\mid c\mid=\mid b\mid$, and the eigenvalues are equal to $0$. But the matrix $A$ is not diagonalizable under this condition: $A=b\left(\begin{array}{cc} 1 & e^{i\theta}\\ -e^{-i\theta} & -1 \end{array}\right)=b\left(\begin{array}{cc} -e^{i\theta} & -e^{i\theta}\\ 1 & 0 \end{array}\right)\left(\begin{array}{cc} 0 & 1\\ 0 & 0 \end{array}\right)\left(\begin{array}{cc} 0 & 1\\ -e^{-i\theta} & -1 \end{array}\right).$ Is this an accident only for $N=1$ case? Is there some relation like this for general cases $N>1$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
15,"Low-degree ""determinant"" for non-square matrices?","Low-degree ""determinant"" for non-square matrices?",,"Consider a matrix $A\in \mathbb R^{n\times n}$ of indeterminates. The determinant of $A$ is a degree $n$ polynomial in the $n^2$ entries satisfying $\det A\ne0\iff A$ is nonsingular. What about when $A\in\mathbb R^{n\times m}$ is not square? Is there is a similar low-degree polynomial in the $nm$ indeterminates that satisfies $f(x_{1,1},\ldots,x_{n,m})\ne0\iff A$ has full rank? It looks like $2\cdot \min\{n,m\}$ is achievable by letting $f=\det (A^TA)$ or $\det (AA^T)$, whichever makes sense. But what about degree $\min \{n,m\}$? Or at least $\max\{n,m\}$? Edit1: Changed $\mathbb F$ to $\mathbb R$. Edit2: Thought about it and I guess $\min \{n,m\}$ isn't achievable when one considers the $n\times 1$ case (not possible to test whether vector is $\mathbb 0$ with degree 1). Is $2\cdot \min\{n,m\}$ optimal in all cases of $n\ne m$?","Consider a matrix $A\in \mathbb R^{n\times n}$ of indeterminates. The determinant of $A$ is a degree $n$ polynomial in the $n^2$ entries satisfying $\det A\ne0\iff A$ is nonsingular. What about when $A\in\mathbb R^{n\times m}$ is not square? Is there is a similar low-degree polynomial in the $nm$ indeterminates that satisfies $f(x_{1,1},\ldots,x_{n,m})\ne0\iff A$ has full rank? It looks like $2\cdot \min\{n,m\}$ is achievable by letting $f=\det (A^TA)$ or $\det (AA^T)$, whichever makes sense. But what about degree $\min \{n,m\}$? Or at least $\max\{n,m\}$? Edit1: Changed $\mathbb F$ to $\mathbb R$. Edit2: Thought about it and I guess $\min \{n,m\}$ isn't achievable when one considers the $n\times 1$ case (not possible to test whether vector is $\mathbb 0$ with degree 1). Is $2\cdot \min\{n,m\}$ optimal in all cases of $n\ne m$?",,"['linear-algebra', 'matrices', 'determinant']"
16,Notation: Rows and Columns of Matrix,Notation: Rows and Columns of Matrix,,"A purely notational question: It is possible to denote the rows of a matrix $\mathbf{A}\in\mathbb{R}^{n\times m}$ by $\mathbf{w_1},\ldots,\mathbf{w_n}\in\mathbb{R}^{1\times m}$ and the columns by $\mathbf{v_1},\ldots,\mathbf{v_m}\in\mathbb{R}^{1\times n}$, such that \begin{equation} \mathbf{A}=\left(\begin{matrix}\mathbf{w_1}\\\vdots\\\mathbf{w_n}\end{matrix}\right)=\left(\begin{matrix}\mathbf{v_1}&\cdots&\mathbf{v_m}\end{matrix}\right). \end{equation} Now, I write a paper containing a lot of matrices and in which I have to talk a lot about their rows. Thus instead of $\mathbf{w_k}$, I conveniently used the notation $\mathbf{A_k}$ to denote the $k^{th}$ row of $\mathbf{A}$, which has the big advantage that it is immediately clear of which matrix $\mathbf{A_k}$ is the $k^{th}$ row. However, I also sometimes have to talk about the columns of $\mathbf{A}$, which were denoted by $\mathbf{v_l}$ above. Denoting them by $\mathbf{A}_l$ is not an option, since this already denotes the $l^{th}$ row. Giving them a completely different name neither, since this makes the paper ugly and hard to read. What I would like is a clean notation to denote either rows or columns which still associates the rows/columns with $\mathbf{A}$. I thought of $\mathbf{A}_{*,l}$ and $\mathbf{A_{k,*}}$, but some matrix names already contain sub- and super-scripts themselves, making this notation ugly, too. In short, I need a notation for the rows respectively columns which would look also nice for e.g. $\mathbf{A_\beta^i}(t)\in\mathbb{R}^{n\times m}$. It would be OK if the notation would be nicer for the rows than for the columns, since I talk about the latter less often.","A purely notational question: It is possible to denote the rows of a matrix $\mathbf{A}\in\mathbb{R}^{n\times m}$ by $\mathbf{w_1},\ldots,\mathbf{w_n}\in\mathbb{R}^{1\times m}$ and the columns by $\mathbf{v_1},\ldots,\mathbf{v_m}\in\mathbb{R}^{1\times n}$, such that \begin{equation} \mathbf{A}=\left(\begin{matrix}\mathbf{w_1}\\\vdots\\\mathbf{w_n}\end{matrix}\right)=\left(\begin{matrix}\mathbf{v_1}&\cdots&\mathbf{v_m}\end{matrix}\right). \end{equation} Now, I write a paper containing a lot of matrices and in which I have to talk a lot about their rows. Thus instead of $\mathbf{w_k}$, I conveniently used the notation $\mathbf{A_k}$ to denote the $k^{th}$ row of $\mathbf{A}$, which has the big advantage that it is immediately clear of which matrix $\mathbf{A_k}$ is the $k^{th}$ row. However, I also sometimes have to talk about the columns of $\mathbf{A}$, which were denoted by $\mathbf{v_l}$ above. Denoting them by $\mathbf{A}_l$ is not an option, since this already denotes the $l^{th}$ row. Giving them a completely different name neither, since this makes the paper ugly and hard to read. What I would like is a clean notation to denote either rows or columns which still associates the rows/columns with $\mathbf{A}$. I thought of $\mathbf{A}_{*,l}$ and $\mathbf{A_{k,*}}$, but some matrix names already contain sub- and super-scripts themselves, making this notation ugly, too. In short, I need a notation for the rows respectively columns which would look also nice for e.g. $\mathbf{A_\beta^i}(t)\in\mathbb{R}^{n\times m}$. It would be OK if the notation would be nicer for the rows than for the columns, since I talk about the latter less often.",,"['matrices', 'notation']"
17,Simultaneous diagonalization of two symmetric matrices vs. diagonalization of one nonsymmetric matrix,Simultaneous diagonalization of two symmetric matrices vs. diagonalization of one nonsymmetric matrix,,"In physics, when considering the motion of a system with $N$ degrees of freedom described by vector $x$, the linearized equations of motion take the form $$M \ddot{x} = - K x.$$ Here, $M$ is a symmetric (in most cases diagonal), positive definite matrix, and $K$ is a symmetric, (in general) indefinite matrix. Using the standard ansatz $x(t) \propto e^{i\omega t}$, we have $\ddot{x} = -\omega^2 x$, which, in turn, leads to the eigenvalue equation $$\omega^2 M x = K x.$$ Usually, this problem is solved by simultaneously diagonalizing both $M$ and $K$. However, given that $M$ is positive definite, wouldn't it make more sense to write the eigenvalue problem as $$M^{-1} K x = \omega^2 x$$ and solve it the usual way? Wouldn't the solutions (eigenvalues and eigenvectors) necessarily be the same?","In physics, when considering the motion of a system with $N$ degrees of freedom described by vector $x$, the linearized equations of motion take the form $$M \ddot{x} = - K x.$$ Here, $M$ is a symmetric (in most cases diagonal), positive definite matrix, and $K$ is a symmetric, (in general) indefinite matrix. Using the standard ansatz $x(t) \propto e^{i\omega t}$, we have $\ddot{x} = -\omega^2 x$, which, in turn, leads to the eigenvalue equation $$\omega^2 M x = K x.$$ Usually, this problem is solved by simultaneously diagonalizing both $M$ and $K$. However, given that $M$ is positive definite, wouldn't it make more sense to write the eigenvalue problem as $$M^{-1} K x = \omega^2 x$$ and solve it the usual way? Wouldn't the solutions (eigenvalues and eigenvectors) necessarily be the same?",,['matrices']
18,A property about matrices with precisely three eigenvalues,A property about matrices with precisely three eigenvalues,,"Suppose $A$ is a symmetric matrix all of whose entries are from $\{0,1\}$ and suppose further that the all ones vector $\vec{1}$ is an eigenvector of $A$ for the eigenvalue $k.$ In addition suppose that $A$ has precisely two additional eigenvalues $\lambda \ne \mu.$ In some notes that I am currently reading it say that if $v$ is a vector orthogonal to $\vec{1}$ then $$(A-\lambda I)(A-\mu I) \cdot v = 0.$$ I have a bit of a hard time understanding why is this true?  Can someone give a proof/short explanation?","Suppose $A$ is a symmetric matrix all of whose entries are from $\{0,1\}$ and suppose further that the all ones vector $\vec{1}$ is an eigenvector of $A$ for the eigenvalue $k.$ In addition suppose that $A$ has precisely two additional eigenvalues $\lambda \ne \mu.$ In some notes that I am currently reading it say that if $v$ is a vector orthogonal to $\vec{1}$ then $$(A-\lambda I)(A-\mu I) \cdot v = 0.$$ I have a bit of a hard time understanding why is this true?  Can someone give a proof/short explanation?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
19,Characteristic and minimal polynomials,Characteristic and minimal polynomials,,"Given $A\in\Bbb Z^{n\times n}$, is it possible to find characteristic and minimal polynomials of $A$ by chinese remainder theorem if we know characteristic and minimal polynomials respectively of $A\bmod p\in\Bbb Z_p^{n\times n}$ at sufficiently many coprime $p$s?","Given $A\in\Bbb Z^{n\times n}$, is it possible to find characteristic and minimal polynomials of $A$ by chinese remainder theorem if we know characteristic and minimal polynomials respectively of $A\bmod p\in\Bbb Z_p^{n\times n}$ at sufficiently many coprime $p$s?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'number-theory', 'chinese-remainder-theorem']"
20,Variational characterization of nuclear norm,Variational characterization of nuclear norm,,"The nuclear norm $||\cdot||_{*}$ of a matrix is defined as the sum of its singular values. Working from the result at the bottom of this blog post , we have, for a matrix $\mathbf{X}$ and its decomposition $\mathbf{L} \mathbf{R}^T$, $$ \|\mathbf{X}\|_* = \min_{\mathbf{X=LR}^T} \|\mathbf{L}\|_F \|\mathbf{R}\|_F = \min_{\mathbf{X=LR}^T} \frac{1}{2} \left(\|\mathbf{L}\|_F^2 + \|\mathbf{R}\|_F^2\right) $$ where, for example, $\mathbf{L} = \mathbf{U\Sigma}^{1/2}$, $\mathbf{R} = \mathbf{\Sigma}^{1/2} \mathbf{V}^T$, and $\mathbf{X} = \mathbf{U\Sigma V}^T$ is the SVD of $\mathbf{X}$. The basis for this is the paper by Recht et al. from 2007: ""Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization"" . In a paper on Robust Principal Component Analysis (RPCA) by Feng et al., this result is used to reformulate the RPCA problem so as to minimize the nuclear norm without needing to access all the samples to perform an SVD calculation (see Eq.2 in that paper). I'm interested in a possible extension to the case of two-dimensional SVD ( original paper and Wikipedia article ), where for a group of matrices $(\mathbf{X}_1,...,\mathbf{X}_n)$, we instead have the following type of decomposition: $$ \mathbf{X}_i = \mathbf{L} \mathbf{M}_i \mathbf{R}^T $$ Is the following correct? $$ \|\mathbf{X}_i\|_* = \min_{\mathbf{X}_i=\mathbf{L}\mathbf{M}_i\mathbf{R}^T} \frac{1}{2} \left(\|\mathbf{L}\|_F^2 + \|\mathbf{M}_i\|_F^2 + \|\mathbf{R}\|_F^2\right) $$ Or am I being far too hopeful for such a simple solution...","The nuclear norm $||\cdot||_{*}$ of a matrix is defined as the sum of its singular values. Working from the result at the bottom of this blog post , we have, for a matrix $\mathbf{X}$ and its decomposition $\mathbf{L} \mathbf{R}^T$, $$ \|\mathbf{X}\|_* = \min_{\mathbf{X=LR}^T} \|\mathbf{L}\|_F \|\mathbf{R}\|_F = \min_{\mathbf{X=LR}^T} \frac{1}{2} \left(\|\mathbf{L}\|_F^2 + \|\mathbf{R}\|_F^2\right) $$ where, for example, $\mathbf{L} = \mathbf{U\Sigma}^{1/2}$, $\mathbf{R} = \mathbf{\Sigma}^{1/2} \mathbf{V}^T$, and $\mathbf{X} = \mathbf{U\Sigma V}^T$ is the SVD of $\mathbf{X}$. The basis for this is the paper by Recht et al. from 2007: ""Guaranteed Minimum-Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization"" . In a paper on Robust Principal Component Analysis (RPCA) by Feng et al., this result is used to reformulate the RPCA problem so as to minimize the nuclear norm without needing to access all the samples to perform an SVD calculation (see Eq.2 in that paper). I'm interested in a possible extension to the case of two-dimensional SVD ( original paper and Wikipedia article ), where for a group of matrices $(\mathbf{X}_1,...,\mathbf{X}_n)$, we instead have the following type of decomposition: $$ \mathbf{X}_i = \mathbf{L} \mathbf{M}_i \mathbf{R}^T $$ Is the following correct? $$ \|\mathbf{X}_i\|_* = \min_{\mathbf{X}_i=\mathbf{L}\mathbf{M}_i\mathbf{R}^T} \frac{1}{2} \left(\|\mathbf{L}\|_F^2 + \|\mathbf{M}_i\|_F^2 + \|\mathbf{R}\|_F^2\right) $$ Or am I being far too hopeful for such a simple solution...",,"['linear-algebra', 'matrices', 'convex-optimization', 'normed-spaces', 'nuclear-norm']"
21,"Given a tridiagonal, 10x10 matrix, how can I show that its largest eigenvalue is greater than 1?","Given a tridiagonal, 10x10 matrix, how can I show that its largest eigenvalue is greater than 1?",,"EDIT: I have finished this problem -- if anyone has interest in solving this problem, too, please read up on Rayleigh Quotients, the Courant min-max principle, and the interlacing property that the two theorems imply -- in this order.  Have fun :-) Given a tri-diagonal, $10x10$ matrix A, I want to show that a) its largest eigenvalue $\lambda_{max}(A)>1$ b) the matrix has 9 negative eigenvalues the matrix is $$\left(\begin{matrix}1&-3&0&0&\cdots&0&0\\-3&-2&1&0&\cdots&0&0\\0&1&-2&1&\cdots&0&0\\0&0&1&-2&\cdots&0&0\\\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\0&0&0&0&\cdots&-2&1\\0&0&0&0&\cdots&1&-2\end{matrix}\right)_{10\times10}$$ How can I get started on this problem?  What concepts are being tested here?  This is an old linear algebra exam question, dating back to 1994 :-) Thanks,","EDIT: I have finished this problem -- if anyone has interest in solving this problem, too, please read up on Rayleigh Quotients, the Courant min-max principle, and the interlacing property that the two theorems imply -- in this order.  Have fun :-) Given a tri-diagonal, $10x10$ matrix A, I want to show that a) its largest eigenvalue $\lambda_{max}(A)>1$ b) the matrix has 9 negative eigenvalues the matrix is $$\left(\begin{matrix}1&-3&0&0&\cdots&0&0\\-3&-2&1&0&\cdots&0&0\\0&1&-2&1&\cdots&0&0\\0&0&1&-2&\cdots&0&0\\\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\0&0&0&0&\cdots&-2&1\\0&0&0&0&\cdots&1&-2\end{matrix}\right)_{10\times10}$$ How can I get started on this problem?  What concepts are being tested here?  This is an old linear algebra exam question, dating back to 1994 :-) Thanks,",,"['linear-algebra', 'matrices']"
22,Orthogonal similarity transformation,Orthogonal similarity transformation,,"Can someone please show me how to diagonalize a matrix such as the one below using an orthogonal similarity transformation? $$         \begin{bmatrix}         2 & 1 & 1 \\         1 & 2 & 1 \\         1 & 1 & 2 \\         \end{bmatrix} $$ I have been looking everywhere online to find an example of orthogonal similarity transformations but I can't find any. Am I searching for the wrong thing? Is there another name for it, because similarity transformations seem awfully close to Jordan canonical form? Please help. Thank you in advance.","Can someone please show me how to diagonalize a matrix such as the one below using an orthogonal similarity transformation? $$         \begin{bmatrix}         2 & 1 & 1 \\         1 & 2 & 1 \\         1 & 1 & 2 \\         \end{bmatrix} $$ I have been looking everywhere online to find an example of orthogonal similarity transformations but I can't find any. Am I searching for the wrong thing? Is there another name for it, because similarity transformations seem awfully close to Jordan canonical form? Please help. Thank you in advance.",,"['linear-algebra', 'matrices']"
23,Diagonalization of matrices of the form BAB,Diagonalization of matrices of the form BAB,,"If $A$ and $B$ are real symmetric matrices, the matrix $BAB$ is also a real symmetric matrix. My question is : knowing the orthogonal diagonalization of $A$ and $B$, can we obtain the orthonormal diagonalization of $BAB$ ? Starting from $A = UDU^{\top}$ and $B = VD'V^{\top}$ with $U,V$ real orthogonal matrices and $D,D'$ diagonal matrices, we get : $$ BAB = VD'V^{\top}UDU^{\top}VD'V^{\top} $$ which I do not find very helpful. My intuition is that the answer to the question is no. It is not even clear what the eigenvalues of $BAB$ would be in terms of $A$ and $B$. Can an additional hypothesis, such as ""$B$ is positive definite"" make the problem easier?","If $A$ and $B$ are real symmetric matrices, the matrix $BAB$ is also a real symmetric matrix. My question is : knowing the orthogonal diagonalization of $A$ and $B$, can we obtain the orthonormal diagonalization of $BAB$ ? Starting from $A = UDU^{\top}$ and $B = VD'V^{\top}$ with $U,V$ real orthogonal matrices and $D,D'$ diagonal matrices, we get : $$ BAB = VD'V^{\top}UDU^{\top}VD'V^{\top} $$ which I do not find very helpful. My intuition is that the answer to the question is no. It is not even clear what the eigenvalues of $BAB$ would be in terms of $A$ and $B$. Can an additional hypothesis, such as ""$B$ is positive definite"" make the problem easier?",,"['linear-algebra', 'matrices', 'diagonalization']"
24,Unitriangular group $UT_n(\Bbb Z)$ is nilpotent with class $n$,Unitriangular group  is nilpotent with class,UT_n(\Bbb Z) n,"The unitriangular group $UT_n(\Bbb Z)$ is the group of all $n \times n$ invertible triangular matrices with the identity on each entry of the main diagonal, and integer entries everywhere else in the triangle. Show that this group is nilpotent, and that its nilpotence class is $n$ . Definition ( upper central series ): For any group $G$ define the following subgroups inductively: $$Z_0(G) = 1, \qquad Z_1(G) = Z(G)$$ and $Z_{i+1}(G)$ is the subgroup of $G$ containing $Z_i(G)$ such that $$Z_{i+1}(G)/Z_i(G) = Z(G/Z_i(G)).$$ The chain of subgroups $$Z_0(G) \leq Z_1(G) \leq Z_2(G) \leq \cdots$$ is called the upper central series of $G$ . Definition ( nilpotent ): A group $G$ is called nilpotent if $Z_c(G) = G$ for some $c \in \Bbb Z$ . The smallest such $c$ is called the nilpotence class of $G$ . To show that it is nilpotent, I think it is suffient to show that it is a $p-$ group; i.e. $|UT_n(\Bbb Z)| = p^{\alpha}$ where $p$ is a prime number and $\alpha$ is a positive integer. I feel like there must be some sort of algorithm to calculate how many possible matrices we can get, similar to the formula for finding the order of $GL_n(\Bbb F)$ , the general linear group. I tried Googling, but I can't find a formula for the unitriangular group $UT_n(\Bbb Z)$ . To show the nilpotence class is $n$ , I have to prove that $Z_n(G) = G$ , and that $n$ is the smallest such integer. So by the given definition above, I know that $Z_n(G)/Z_{n-1}(G) = Z(G/Z_n(G))$ . How can I manipulate this to arrive at $Z_n(G) = G$ , and also show that $n$ is the smallest such integer?","The unitriangular group is the group of all invertible triangular matrices with the identity on each entry of the main diagonal, and integer entries everywhere else in the triangle. Show that this group is nilpotent, and that its nilpotence class is . Definition ( upper central series ): For any group define the following subgroups inductively: and is the subgroup of containing such that The chain of subgroups is called the upper central series of . Definition ( nilpotent ): A group is called nilpotent if for some . The smallest such is called the nilpotence class of . To show that it is nilpotent, I think it is suffient to show that it is a group; i.e. where is a prime number and is a positive integer. I feel like there must be some sort of algorithm to calculate how many possible matrices we can get, similar to the formula for finding the order of , the general linear group. I tried Googling, but I can't find a formula for the unitriangular group . To show the nilpotence class is , I have to prove that , and that is the smallest such integer. So by the given definition above, I know that . How can I manipulate this to arrive at , and also show that is the smallest such integer?","UT_n(\Bbb Z) n \times n n G Z_0(G) = 1, \qquad Z_1(G) = Z(G) Z_{i+1}(G) G Z_i(G) Z_{i+1}(G)/Z_i(G) = Z(G/Z_i(G)). Z_0(G) \leq Z_1(G) \leq Z_2(G) \leq \cdots G G Z_c(G) = G c \in \Bbb Z c G p- |UT_n(\Bbb Z)| = p^{\alpha} p \alpha GL_n(\Bbb F) UT_n(\Bbb Z) n Z_n(G) = G n Z_n(G)/Z_{n-1}(G) = Z(G/Z_n(G)) Z_n(G) = G n","['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'p-groups']"
25,A Problem regarding properties of a matrix and the dimention of the vector space [closed],A Problem regarding properties of a matrix and the dimention of the vector space [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $A,B$ are two $n\times n$ matrices such that $A^2+B^2=AB$ and $AB-BA$ is invertible . Show that $3$ divides $n.$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Let $A,B$ are two $n\times n$ matrices such that $A^2+B^2=AB$ and $AB-BA$ is invertible . Show that $3$ divides $n.$",,"['linear-algebra', 'matrices']"
26,Congruence and similarity of matrices/ geometry,Congruence and similarity of matrices/ geometry,,"Given two matrices $A$ and $B$ we can tell if those two matrices are congruent, similar, or neither.  Similarly given two figures in Euclidean geometry we can tell if they are congruent, similar, or neither. Is there a relationship between these two ideas (congruence/ similarity for matrices and congruence/ similarity for geometric figures) that justifies the use of the same names?","Given two matrices $A$ and $B$ we can tell if those two matrices are congruent, similar, or neither.  Similarly given two figures in Euclidean geometry we can tell if they are congruent, similar, or neither. Is there a relationship between these two ideas (congruence/ similarity for matrices and congruence/ similarity for geometric figures) that justifies the use of the same names?",,"['linear-algebra', 'matrices', 'geometry']"
27,A Set of Trace Inequalities,A Set of Trace Inequalities,,"Let $\rho$ be a $N \times N$ positive semi-definite Hermitian matrix.  Let $S$ be a $N \times N$ matrix that squares to the identity.  Consider an arbitrary word constructed by multiplying $2n$ $\rho$'s together and an even number of $S$'s.   As I'm about to take the trace of the word, words are considered distinct only if they cannot be related by a cyclic permutation.  For example, when $n=4$, the distinct words would be $\rho^4$, $(\rho^2 S)^2$, $\rho S \rho^3 S$, and $(\rho S)^4$. I have reason to suspect that for fixed $n$, $\rho^{2n}$ is the word with the largest trace and $(\rho S)^{2n}$ is the word with the smallest trace.  Numerical experiments on random matrices along with multiplying out $2 \times 2$ matrices suggest that the trace decreases as the number of $S$'s increase.  Also, as we distribute the $S$'s more evenly among the $\rho$'s, the trace also appears to decrease. For certain words, relevant inequalities can be derived starting with the Lieb-Thirring inequality or the von Neumann inequality ( see this wikipedia page ). For example, in the $2n=4$ case, let's define $\sigma = S \rho S$.  As $\sigma$ and $\rho$ are related by a similarity transformation, they have the same spectrum and in particular are both positive semi-definite.  Then by Lieb-Thirring, we have $$ \mbox{tr} ((\rho S)^4 )= \mbox{tr}( (\rho \sigma)^2) \leq \mbox{tr} (\rho^2 \sigma^2) = \mbox{tr}((\rho^2 S)^2) $$ But I haven't been able to find a proof for general $n$.  I was hoping someone might have a proof, or a suggestion of a strategy, or a counter-example.","Let $\rho$ be a $N \times N$ positive semi-definite Hermitian matrix.  Let $S$ be a $N \times N$ matrix that squares to the identity.  Consider an arbitrary word constructed by multiplying $2n$ $\rho$'s together and an even number of $S$'s.   As I'm about to take the trace of the word, words are considered distinct only if they cannot be related by a cyclic permutation.  For example, when $n=4$, the distinct words would be $\rho^4$, $(\rho^2 S)^2$, $\rho S \rho^3 S$, and $(\rho S)^4$. I have reason to suspect that for fixed $n$, $\rho^{2n}$ is the word with the largest trace and $(\rho S)^{2n}$ is the word with the smallest trace.  Numerical experiments on random matrices along with multiplying out $2 \times 2$ matrices suggest that the trace decreases as the number of $S$'s increase.  Also, as we distribute the $S$'s more evenly among the $\rho$'s, the trace also appears to decrease. For certain words, relevant inequalities can be derived starting with the Lieb-Thirring inequality or the von Neumann inequality ( see this wikipedia page ). For example, in the $2n=4$ case, let's define $\sigma = S \rho S$.  As $\sigma$ and $\rho$ are related by a similarity transformation, they have the same spectrum and in particular are both positive semi-definite.  Then by Lieb-Thirring, we have $$ \mbox{tr} ((\rho S)^4 )= \mbox{tr}( (\rho \sigma)^2) \leq \mbox{tr} (\rho^2 \sigma^2) = \mbox{tr}((\rho^2 S)^2) $$ But I haven't been able to find a proof for general $n$.  I was hoping someone might have a proof, or a suggestion of a strategy, or a counter-example.",,"['linear-algebra', 'matrices', 'inequality']"
28,Connected components of a given subspace of $M_{n \times n}(\mathbb{R})$.,Connected components of a given subspace of .,M_{n \times n}(\mathbb{R}),"This question is motivated by this question, which gave me quite a headache today. Context: I posted originally what I thought was a quick proof using the derivative of the given function. It was intended to be a straightforward answer using a well-known strategy of proving something is constant and equal to some other thing by proving its derivative is zero and you are in a connected set. But I came across an issue: I didn't realize that I was not in a connected set, as the domain of definition of my function had to take into account the inverses I was considering (see here ). I managed to go around this issue (although at the cost of simplicity), but one of my initial attempts to solve my blunder was to try to pinpoint the connected components of the set $\mathcal{D}$ I was considering. I wasn't able to solve this issue (trying to show it was path-connected got me troubled in a lot of possible cases for the matrices). Hence, this present question arose What are the connected components of the space $\mathcal{D}=\mathcal{A} \cap \mathcal{B}$, where $\mathcal{A}=\{A \mid \exists A^{-1}\}$ and $\mathcal{B}= \{B \mid \exists (I+B)^{-1}\}$?","This question is motivated by this question, which gave me quite a headache today. Context: I posted originally what I thought was a quick proof using the derivative of the given function. It was intended to be a straightforward answer using a well-known strategy of proving something is constant and equal to some other thing by proving its derivative is zero and you are in a connected set. But I came across an issue: I didn't realize that I was not in a connected set, as the domain of definition of my function had to take into account the inverses I was considering (see here ). I managed to go around this issue (although at the cost of simplicity), but one of my initial attempts to solve my blunder was to try to pinpoint the connected components of the set $\mathcal{D}$ I was considering. I wasn't able to solve this issue (trying to show it was path-connected got me troubled in a lot of possible cases for the matrices). Hence, this present question arose What are the connected components of the space $\mathcal{D}=\mathcal{A} \cap \mathcal{B}$, where $\mathcal{A}=\{A \mid \exists A^{-1}\}$ and $\mathcal{B}= \{B \mid \exists (I+B)^{-1}\}$?",,"['linear-algebra', 'general-topology', 'matrices']"
29,Eigenvalues of differentiable matrices,Eigenvalues of differentiable matrices,,"I have a real-valued matrix, $M(a)$, which is a differentiable function of $a$, but not continuously differentiable, with $M(0)=I$. I'll assume $M'(0)$ has distinct eigenvalues. I'm looking for results proving the differentiability of the eigenvalues of $M(0)$.  I suspect it may be a challenging proof because when I read the literature, I see people making very restrictive assumptions, usually that the eigenvalues are distinct (clearly not true for me), and if not, it seems that people assume the matrix is symmetric, also not true for me. If I assume the eigenvalues and eigenvectors are differentiable, say with $v_1(a)$ being one of the eigenvectors, then I believe I can conclude that $v_1(0)$ is an eigenvector of $M'(0)$ and I can work out $\lambda_1'(0)$.  If so I've got the result I'm trying to prove. Is it known whether the eigenvalues of a differentiable real matrix are in fact differentiable, even if repeated?  If true - where can I find a reference?  If false, where can I found a counterexample?  If unknown - what's the challenge in proving it?","I have a real-valued matrix, $M(a)$, which is a differentiable function of $a$, but not continuously differentiable, with $M(0)=I$. I'll assume $M'(0)$ has distinct eigenvalues. I'm looking for results proving the differentiability of the eigenvalues of $M(0)$.  I suspect it may be a challenging proof because when I read the literature, I see people making very restrictive assumptions, usually that the eigenvalues are distinct (clearly not true for me), and if not, it seems that people assume the matrix is symmetric, also not true for me. If I assume the eigenvalues and eigenvectors are differentiable, say with $v_1(a)$ being one of the eigenvectors, then I believe I can conclude that $v_1(0)$ is an eigenvector of $M'(0)$ and I can work out $\lambda_1'(0)$.  If so I've got the result I'm trying to prove. Is it known whether the eigenvalues of a differentiable real matrix are in fact differentiable, even if repeated?  If true - where can I find a reference?  If false, where can I found a counterexample?  If unknown - what's the challenge in proving it?",,"['linear-algebra', 'matrices', 'derivatives', 'eigenvalues-eigenvectors']"
30,Can a real symmetric matrix have 0 (Zero) as one of the eigen values?,Can a real symmetric matrix have 0 (Zero) as one of the eigen values?,,"From what I know (correct me if I am wrong): $0$ as an eigen value of a real symmetric matrix implies it is Singular (Non- invertible). I am not aware of any such property with reference to real symmetric matrices. Also, I wish to know if the following statements are correct or not. a)  If two matrices have the same eigenvalues, they have the same eigenvectors.  (I think it's false) b)  If two matrices have the same eigen vectors, they have the same eigen values.  (I think that's true) Correct me.","From what I know (correct me if I am wrong): as an eigen value of a real symmetric matrix implies it is Singular (Non- invertible). I am not aware of any such property with reference to real symmetric matrices. Also, I wish to know if the following statements are correct or not. a)  If two matrices have the same eigenvalues, they have the same eigenvectors.  (I think it's false) b)  If two matrices have the same eigen vectors, they have the same eigen values.  (I think that's true) Correct me.",0,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
31,"PSL(2, $\mathbb Q$) and elements of infinite order.","PSL(2, ) and elements of infinite order.",\mathbb Q,"I can't seem to find any literature on PSL(2, $\mathbb Q$). The following is the very second exercise of Serre's Trees . Given any injection $f:\mathbb{Z} \to \mathrm{PSL}(2, \mathbb{Q})$ and surjection $g:\mathbb{Z} \to \mathbb{Z}/2\mathbb{Z}$, then the amalgamation is trivial. Let $G = \mathrm{PSL}(2, \mathbb{Q})$ and $A \in G$ is a matrix with infinite order (so determines injection $f$). Using the first exercise of the section, I was able to show that the amalgamated product is isomorphic to $G/\langle\langle~A^2~\rangle \rangle$. So it remains to show that the normal subgroup of $G$ generated by $A^2$ is the whole group. How should I go about proving this?","I can't seem to find any literature on PSL(2, $\mathbb Q$). The following is the very second exercise of Serre's Trees . Given any injection $f:\mathbb{Z} \to \mathrm{PSL}(2, \mathbb{Q})$ and surjection $g:\mathbb{Z} \to \mathbb{Z}/2\mathbb{Z}$, then the amalgamation is trivial. Let $G = \mathrm{PSL}(2, \mathbb{Q})$ and $A \in G$ is a matrix with infinite order (so determines injection $f$). Using the first exercise of the section, I was able to show that the amalgamated product is isomorphic to $G/\langle\langle~A^2~\rangle \rangle$. So it remains to show that the normal subgroup of $G$ generated by $A^2$ is the whole group. How should I go about proving this?",,"['abstract-algebra', 'matrices', 'group-theory']"
32,"Why is it called a 'cofactor', and is there some intuition or geometric interpretation?","Why is it called a 'cofactor', and is there some intuition or geometric interpretation?",,"My hope is that understanding the reason why things are named the way they are in mathematics will help aid in developing mathematical maturity and intuition.  Often things are named, and then explain with only the ""how"" of, not the what or ""why"".  This seems to be the case with cofactors in matrix algebra.  Now I now what a factor is, but why is it called ""co-factor"".  What is the ""co"" part?  And as a mathematical object, does it have any meaning on it's own other than being part of a recipe for a solution?  Is there a geometric intuition of a cofactor that can explain the What and hence the Why of the name?","My hope is that understanding the reason why things are named the way they are in mathematics will help aid in developing mathematical maturity and intuition.  Often things are named, and then explain with only the ""how"" of, not the what or ""why"".  This seems to be the case with cofactors in matrix algebra.  Now I now what a factor is, but why is it called ""co-factor"".  What is the ""co"" part?  And as a mathematical object, does it have any meaning on it's own other than being part of a recipe for a solution?  Is there a geometric intuition of a cofactor that can explain the What and hence the Why of the name?",,"['matrices', 'terminology', 'math-history']"
33,Proof with Linear Transformation,Proof with Linear Transformation,,"Let $u$ = ($u$1, $u$2, $u$3) ∈ $\Bbb R^3$ and $v$ = ($v$1, $v$2) ∈ $\Bbb R^2$ be non-zero (row)vectors. Define F : $\Bbb R^3$ → $\Bbb R^2$ by F($x$) = ($u$ · $x$)$v$. (a) Show that ker F = ${({span\ \ {u}})}^⊥$. (b) Show that $R$(F) = span($v$). (c) Show that the standard matrix of F is $v^{t}u$. Here, F is a linear transformation, ker(F) and $R$(F) is the Kernel and Range  of the linear transformation respectively. The $(span\ \ {u})^{⊥}$ is the set of all vectors orthogonal to the span of $u$ and $v^t$ is the transpose of the row vector $v$ With part a) I figured that if ($u$ · $x$)$v$ belonged to the ker(F), then ($u$ · $x$)$v$=$0$ which meant that $u$ · $x$=$0$ because it previously stated that both $u$ and $v$ are non-zero vectors. From here I'm not actually sure how to show that the ker(F)=$(span\ \ \{u\})^{⊥}$ unless it has something to do with the dot product of the vectors $x$ and $u$ that I 'm missing. The other parts, I'm unsure if I must first do part a) in order to complete them. Does anyone have any advice or directions from here. Please feel free to edit my question for extra clarity.  Thank You!","Let $u$ = ($u$1, $u$2, $u$3) ∈ $\Bbb R^3$ and $v$ = ($v$1, $v$2) ∈ $\Bbb R^2$ be non-zero (row)vectors. Define F : $\Bbb R^3$ → $\Bbb R^2$ by F($x$) = ($u$ · $x$)$v$. (a) Show that ker F = ${({span\ \ {u}})}^⊥$. (b) Show that $R$(F) = span($v$). (c) Show that the standard matrix of F is $v^{t}u$. Here, F is a linear transformation, ker(F) and $R$(F) is the Kernel and Range  of the linear transformation respectively. The $(span\ \ {u})^{⊥}$ is the set of all vectors orthogonal to the span of $u$ and $v^t$ is the transpose of the row vector $v$ With part a) I figured that if ($u$ · $x$)$v$ belonged to the ker(F), then ($u$ · $x$)$v$=$0$ which meant that $u$ · $x$=$0$ because it previously stated that both $u$ and $v$ are non-zero vectors. From here I'm not actually sure how to show that the ker(F)=$(span\ \ \{u\})^{⊥}$ unless it has something to do with the dot product of the vectors $x$ and $u$ that I 'm missing. The other parts, I'm unsure if I must first do part a) in order to complete them. Does anyone have any advice or directions from here. Please feel free to edit my question for extra clarity.  Thank You!",,"['linear-algebra', 'matrices', 'linear-transformations']"
34,Proving that $\det(A) = 0$ when the columns are linearly dependent,Proving that  when the columns are linearly dependent,\det(A) = 0,"Proposition: Let $A$ be a $(n \times n)$-matrix. If the columns of $A$ are linearly dependent, then $\det(A) = 0$. Attempt at proof: Let $A = (A_1, A_2, \ldots, A_n)$, where each $A_i$ is a column vector. Since the columns are linearly dependent, we have \begin{align*} \lambda_1 A_1 + \ldots + \lambda_n A_n = 0 \end{align*} where not all $\lambda_i$ are zero. Suppose, without loss of generality, that $\lambda_1 \neq 0$. Then we get \begin{align*} A_1 = - \frac{\lambda_2}{\lambda_1} A_2 - \ldots - \frac{\lambda_n}{\lambda_1} A_n. \end{align*} It follows that \begin{align*} \det(A_1, \ldots, A_n) &= \det (- \frac{\lambda_2}{\lambda_1} A_2 - \ldots - \frac{\lambda_n}{\lambda_1} A_n; A_2, A_3, \ldots, A_n) \\ &= \sum_{i=2}^n - \frac{\lambda_i}{\lambda_1} \det(A_2 + \ldots + A_k + \ldots + A_n; A_2, A_3, \ldots, A_n) \end{align*} This follows since the determinant is linear in each column. Now, I want to write \begin{align*} = \sum_{i=2}^n - \frac{\lambda_i}{\lambda_1} \det(A_k, \ldots, A_k, \ldots) = 0 \end{align*} since two columns are equal. But I'm not sure if my notation is correct in the last step? How do I break down the $A_2 + \ldots + A_k + \ldots$ etc.? Suggestions are appreciated!","Proposition: Let $A$ be a $(n \times n)$-matrix. If the columns of $A$ are linearly dependent, then $\det(A) = 0$. Attempt at proof: Let $A = (A_1, A_2, \ldots, A_n)$, where each $A_i$ is a column vector. Since the columns are linearly dependent, we have \begin{align*} \lambda_1 A_1 + \ldots + \lambda_n A_n = 0 \end{align*} where not all $\lambda_i$ are zero. Suppose, without loss of generality, that $\lambda_1 \neq 0$. Then we get \begin{align*} A_1 = - \frac{\lambda_2}{\lambda_1} A_2 - \ldots - \frac{\lambda_n}{\lambda_1} A_n. \end{align*} It follows that \begin{align*} \det(A_1, \ldots, A_n) &= \det (- \frac{\lambda_2}{\lambda_1} A_2 - \ldots - \frac{\lambda_n}{\lambda_1} A_n; A_2, A_3, \ldots, A_n) \\ &= \sum_{i=2}^n - \frac{\lambda_i}{\lambda_1} \det(A_2 + \ldots + A_k + \ldots + A_n; A_2, A_3, \ldots, A_n) \end{align*} This follows since the determinant is linear in each column. Now, I want to write \begin{align*} = \sum_{i=2}^n - \frac{\lambda_i}{\lambda_1} \det(A_k, \ldots, A_k, \ldots) = 0 \end{align*} since two columns are equal. But I'm not sure if my notation is correct in the last step? How do I break down the $A_2 + \ldots + A_k + \ldots$ etc.? Suggestions are appreciated!",,"['linear-algebra', 'matrices', 'proof-verification', 'determinant']"
35,"Given matrices $A$ and $B$, how can I find a scalar $s$ that makes $A + s B$ rank-$1$?","Given matrices  and , how can I find a scalar  that makes  rank-?",A B s A + s B 1,"Given $3 \times 3$ matrices $A$ and $B$ , how can I find a scalar $s$ that makes the matrix $A + s B$ rank- $1$ ? Is there a method using singular value decomposition or eigenvalues? Thanks!","Given matrices and , how can I find a scalar that makes the matrix rank- ? Is there a method using singular value decomposition or eigenvalues? Thanks!",3 \times 3 A B s A + s B 1,"['linear-algebra', 'matrices', 'matrix-rank', 'svd', 'rank-1-matrices']"
36,Matrix which represents the product of ideal classes of 2 matrices.,Matrix which represents the product of ideal classes of 2 matrices.,,"Let $f(x)$ be an irreducible monic degree $n$ polynomial with $\mathbb{Z}$-coefficients and $\Theta$ be a root of $f(x)$. There is an old theorem of Latimer and MacDuffee that there is a 1-1 correspondence between the similarity classes of integral matrices with characteristic polynomial $f(x)$ and $C(\mathbb{Z}[\Theta])$, the ideal class group of $\mathbb{Z}[\Theta]$. If $f(x)=x_n+a_{n-1}x_{n-1}+\cdots +a_0$, then a matrix which corresponds to the identity of $\mathbb{Z}[\Theta]$ is the companion matrix of $f(x)$, \begin{equation*}\begin{bmatrix}0&1&0&\cdots&0&0\\0&0&1&\cdots&0&0\\ \vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\ 0&0&0&\cdots&0&1\\ -a_0&-a_1&-a_2&\cdots &-a_{n-2}&-a_{n-1} \end{bmatrix}. \end{equation*} Also,  if $[I_X]\in C(\mathbb{Z}[\Theta])$ corresponds to a matrix $X$, then $[I_X]^{-1}=[I_{X^T}]$. That is, the ideal class corresponds to the transpose of $X$ is the inverse of the ideal class corresponds to $X$. This is due to Taussky. Question: Suppose that $A$ and $B$ are two $n\times n$ matrices with characteristic polynomial $f(x)$. How can we find a matrix $X_{A,B}$ such that $[I_{X_{A,B}}]=[I_A][I_B]\in C(\mathbb{Z}[\Theta])$? Are there any known results about this?","Let $f(x)$ be an irreducible monic degree $n$ polynomial with $\mathbb{Z}$-coefficients and $\Theta$ be a root of $f(x)$. There is an old theorem of Latimer and MacDuffee that there is a 1-1 correspondence between the similarity classes of integral matrices with characteristic polynomial $f(x)$ and $C(\mathbb{Z}[\Theta])$, the ideal class group of $\mathbb{Z}[\Theta]$. If $f(x)=x_n+a_{n-1}x_{n-1}+\cdots +a_0$, then a matrix which corresponds to the identity of $\mathbb{Z}[\Theta]$ is the companion matrix of $f(x)$, \begin{equation*}\begin{bmatrix}0&1&0&\cdots&0&0\\0&0&1&\cdots&0&0\\ \vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\ 0&0&0&\cdots&0&1\\ -a_0&-a_1&-a_2&\cdots &-a_{n-2}&-a_{n-1} \end{bmatrix}. \end{equation*} Also,  if $[I_X]\in C(\mathbb{Z}[\Theta])$ corresponds to a matrix $X$, then $[I_X]^{-1}=[I_{X^T}]$. That is, the ideal class corresponds to the transpose of $X$ is the inverse of the ideal class corresponds to $X$. This is due to Taussky. Question: Suppose that $A$ and $B$ are two $n\times n$ matrices with characteristic polynomial $f(x)$. How can we find a matrix $X_{A,B}$ such that $[I_{X_{A,B}}]=[I_A][I_B]\in C(\mathbb{Z}[\Theta])$? Are there any known results about this?",,"['matrices', 'algebraic-number-theory']"
37,Invertibility of infinite order matrix,Invertibility of infinite order matrix,,how the matrix $[e^{-(x_j-x_k)^2}]$ is invertible where $\{x_j\}$ be any real sequence such that $(x_{j+1}-x_j) >0$ for all $j \in Z$ where $Z$ denotes the set of integers.,how the matrix $[e^{-(x_j-x_k)^2}]$ is invertible where $\{x_j\}$ be any real sequence such that $(x_{j+1}-x_j) >0$ for all $j \in Z$ where $Z$ denotes the set of integers.,,"['matrices', 'exponential-function', 'numerical-linear-algebra', 'matrix-equations', 'matrix-calculus']"
38,Matrix representation of $\mathbb{C}$ as $^*$Algebra.,Matrix representation of  as Algebra.,\mathbb{C} ^*,"We know that there are many matrix representations of the field $\mathbb{C}$. For $2 \times 2$ real entries matrices, e.g., all the subrings of $M(2,\mathbb{R})$ generated by $I$ and a matrix $J$ such that $J^2=-I$ are matrix representations of $\mathbb{C}$. This means that any matrix of the form $$ J=\left[ \begin {array}{cccc} a&b\\ c&-a \end{array} \right] \qquad bc \le-1 \qquad a=\sqrt{-1-bc} $$  is a possible representation of the imaginary unity $i$ and a complex number $z=x+iy$ is represented by the matrix $M(z)=Z=xI+yJ$ I think that this fact can be see as a consequence of the Artin–Wedderburn theorem about the matrix representation of rings ( see my answer to: What does it mean to represent a number in term of a $2\times2$ matrix? ),   that generalize such kind of representation to all semisimple rings. Now, consider $\mathbb{C}$ as a $^*$Algebra (with complex conjugation as the involution). If we search a $^*$Algebra isomorphism such that $M(\bar z)=M^T(z)$ (where the involution for the matrix ring is the transpose), we see that there are only two possible representations, with : $$ J=\left[ \begin {array}{cccc} 0&1\\ -1&0 \end{array} \right] \qquad \mbox{or}\qquad J=\left[ \begin {array}{cccc} 0&-1\\ 1&0 \end{array} \right] $$ So we have a great restriction of the possible representations. My question is if this restriction is a consequence of some general representation theorem for *Algebras in the same sense as the matrix representation of the field $\mathbb{C}$ is a consequence of the A-W theorem.","We know that there are many matrix representations of the field $\mathbb{C}$. For $2 \times 2$ real entries matrices, e.g., all the subrings of $M(2,\mathbb{R})$ generated by $I$ and a matrix $J$ such that $J^2=-I$ are matrix representations of $\mathbb{C}$. This means that any matrix of the form $$ J=\left[ \begin {array}{cccc} a&b\\ c&-a \end{array} \right] \qquad bc \le-1 \qquad a=\sqrt{-1-bc} $$  is a possible representation of the imaginary unity $i$ and a complex number $z=x+iy$ is represented by the matrix $M(z)=Z=xI+yJ$ I think that this fact can be see as a consequence of the Artin–Wedderburn theorem about the matrix representation of rings ( see my answer to: What does it mean to represent a number in term of a $2\times2$ matrix? ),   that generalize such kind of representation to all semisimple rings. Now, consider $\mathbb{C}$ as a $^*$Algebra (with complex conjugation as the involution). If we search a $^*$Algebra isomorphism such that $M(\bar z)=M^T(z)$ (where the involution for the matrix ring is the transpose), we see that there are only two possible representations, with : $$ J=\left[ \begin {array}{cccc} 0&1\\ -1&0 \end{array} \right] \qquad \mbox{or}\qquad J=\left[ \begin {array}{cccc} 0&-1\\ 1&0 \end{array} \right] $$ So we have a great restriction of the possible representations. My question is if this restriction is a consequence of some general representation theorem for *Algebras in the same sense as the matrix representation of the field $\mathbb{C}$ is a consequence of the A-W theorem.",,"['matrices', 'ring-theory', 'complex-numbers', 'c-star-algebras']"
39,Constrained zero diagonal low rank approximation of a matrix with zero diagonal,Constrained zero diagonal low rank approximation of a matrix with zero diagonal,,"Suppose that you have a $n\times n$ matrix $A$ that is symmetric and has zero diagonal, such as for example $$ A=\pmatrix{ 0 & 2 & 2\\ 2 & 0 & 1\\ 2 & 1 & 0}, $$ and you want to represent it by a low rank approximation but respecting the symmetry and the zero diagonal in the output of that approximation. Like doing the eigenvalue decomposition by the spectral theorem $$ A=\sum_{i=1}^n \lambda_iv_iv_i^T $$ and then truncate this sum to get an approximation for $k\leq n$ (assuming $|\lambda_1|\geq\ldots\geq|\lambda_n|$). But if you do this for $A$ of course you get something like (with two eigenvalues) $$ A=\pmatrix{ 0 & 2 & 2\\ 2 & 0.5 & 0.5\\ 2 & 0.5 & 0.5 },$$ which is not in the spirit of what I want. Question : Is there a standard procedure/method for dimension reduction respecting these constraints? EDIT 1 : I realized that since you need to have a zero diagonal output as an approximation matrix, this implies full rank except if some of the entries are zero. So I guess the answer is in choosing which entries are set to zero and how the others are reweighted. Thanks!","Suppose that you have a $n\times n$ matrix $A$ that is symmetric and has zero diagonal, such as for example $$ A=\pmatrix{ 0 & 2 & 2\\ 2 & 0 & 1\\ 2 & 1 & 0}, $$ and you want to represent it by a low rank approximation but respecting the symmetry and the zero diagonal in the output of that approximation. Like doing the eigenvalue decomposition by the spectral theorem $$ A=\sum_{i=1}^n \lambda_iv_iv_i^T $$ and then truncate this sum to get an approximation for $k\leq n$ (assuming $|\lambda_1|\geq\ldots\geq|\lambda_n|$). But if you do this for $A$ of course you get something like (with two eigenvalues) $$ A=\pmatrix{ 0 & 2 & 2\\ 2 & 0.5 & 0.5\\ 2 & 0.5 & 0.5 },$$ which is not in the spirit of what I want. Question : Is there a standard procedure/method for dimension reduction respecting these constraints? EDIT 1 : I realized that since you need to have a zero diagonal output as an approximation matrix, this implies full rank except if some of the entries are zero. So I guess the answer is in choosing which entries are set to zero and how the others are reweighted. Thanks!",,"['matrices', 'eigenvalues-eigenvectors', 'matrix-rank']"
40,Eigenvalues and positivity of Hermitian Toeplitz matrices,Eigenvalues and positivity of Hermitian Toeplitz matrices,,"I want to check the eigenvalues (and also the positivity) of the $n \times n$ complex Toeplitz matrix \begin{equation} T = \begin{bmatrix} r & z_1 & z_2 & z_3 &\cdots & z_{n-1}\\ \bar{z}_1 & r & z_1 & z_2 & \cdots & z_{n-2}\\ \bar{z}_2 & \bar{z}_1 & r & z_1 & \cdots & z_{n-3}\\ \vdots & \vdots &\vdots &\vdots &\ddots &\vdots \\ \bar{z}_{n-1} & \bar{z}_{n-2} & \bar{z}_{n-1} & \cdots & \cdots & r \end{bmatrix}_{n \times n}. \end{equation} $\bar{z}$ is the complex conjugate of $z$, and $r$ is a real number. Is there a canonical way to calculate the eigenvalues of the above matrix? I am sure,this question has been answered somewhere in the stackexchange, but I can not determine where. Please help.","I want to check the eigenvalues (and also the positivity) of the $n \times n$ complex Toeplitz matrix \begin{equation} T = \begin{bmatrix} r & z_1 & z_2 & z_3 &\cdots & z_{n-1}\\ \bar{z}_1 & r & z_1 & z_2 & \cdots & z_{n-2}\\ \bar{z}_2 & \bar{z}_1 & r & z_1 & \cdots & z_{n-3}\\ \vdots & \vdots &\vdots &\vdots &\ddots &\vdots \\ \bar{z}_{n-1} & \bar{z}_{n-2} & \bar{z}_{n-1} & \cdots & \cdots & r \end{bmatrix}_{n \times n}. \end{equation} $\bar{z}$ is the complex conjugate of $z$, and $r$ is a real number. Is there a canonical way to calculate the eigenvalues of the above matrix? I am sure,this question has been answered somewhere in the stackexchange, but I can not determine where. Please help.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
41,How to solve the eigenvalues of a complex matrix of very high condition number?,How to solve the eigenvalues of a complex matrix of very high condition number?,,"WHAT I FACE: I'm dealing with a complex matrix of very high condition number and I have to solve the eigenvalue and eigenfunction of it. But in Matlab, I got the problem that the results are not converging with increasing resolution number, so these results are not reliable. WHAT I NEED: I in fact only need to get one eigenvalue and its associated eigenfunction (largest real part), so I tried with eigs in Matlab, but it says that ""znaupd did not find any eigenvalues to sufficient accuracy"", even though I have relaxed the tolerance to a very high value. WHAT I HAVE TRIED: As I said, I have tried eig and eigs in Matlab, but these two commands can't give me accurate results. What should I do if I want to solve this kind of problem (to get one eigenvalue of a very-high-condition-number matrix)?  Should I move to other solvers other than Matlab? I think Matlab is already the best we can do, right? Thanks. Any discussion will be appreciated. By the way, I'm using the collocation spectral method for the grid discretization.","WHAT I FACE: I'm dealing with a complex matrix of very high condition number and I have to solve the eigenvalue and eigenfunction of it. But in Matlab, I got the problem that the results are not converging with increasing resolution number, so these results are not reliable. WHAT I NEED: I in fact only need to get one eigenvalue and its associated eigenfunction (largest real part), so I tried with eigs in Matlab, but it says that ""znaupd did not find any eigenvalues to sufficient accuracy"", even though I have relaxed the tolerance to a very high value. WHAT I HAVE TRIED: As I said, I have tried eig and eigs in Matlab, but these two commands can't give me accurate results. What should I do if I want to solve this kind of problem (to get one eigenvalue of a very-high-condition-number matrix)?  Should I move to other solvers other than Matlab? I think Matlab is already the best we can do, right? Thanks. Any discussion will be appreciated. By the way, I'm using the collocation spectral method for the grid discretization.",,"['matrices', 'eigenvalues-eigenvectors', 'matlab', 'condition-number']"
42,Two basic examples of trace diagrams?,Two basic examples of trace diagrams?,,"In the wikipedia entry on Trace Diagrams (see http://en.wikipedia.org/wiki/Trace_diagram ), the statement is made that ""The simplest trace diagrams represent the trace and determinant of a matrix"". Could anyone provide me with the graphic representation of those 2 cases for, say, a 3x3 quadratic matrix with any arbitrary number entries? Thanks in advance.","In the wikipedia entry on Trace Diagrams (see http://en.wikipedia.org/wiki/Trace_diagram ), the statement is made that ""The simplest trace diagrams represent the trace and determinant of a matrix"". Could anyone provide me with the graphic representation of those 2 cases for, say, a 3x3 quadratic matrix with any arbitrary number entries? Thanks in advance.",,['matrices']
43,Proximal operator for the nuclear matrix norm of Hankel matrix,Proximal operator for the nuclear matrix norm of Hankel matrix,,"I have a problem in hand for which I need to compute the proximal operator of the composite function $ {\left\| \mbox{Hankel} (x) \right\|}_{\ast} $ where $ x \in \mathbb R^N $ and $ \left\| \cdot \right\|_{\ast} $ denotes the matrix nuclear norm. For a general matrix $X$ , the proximal map of the $\| X \|_{\ast}$ becomes a soft-thresholding of the singular values. I'll be grateful if somebody help me to evaluate the proximal map of $\| \mbox{Hankel} (x)\|_{\ast}$ .","I have a problem in hand for which I need to compute the proximal operator of the composite function where and denotes the matrix nuclear norm. For a general matrix , the proximal map of the becomes a soft-thresholding of the singular values. I'll be grateful if somebody help me to evaluate the proximal map of .", {\left\| \mbox{Hankel} (x) \right\|}_{\ast}   x \in \mathbb R^N   \left\| \cdot \right\|_{\ast}  X \| X \|_{\ast} \| \mbox{Hankel} (x)\|_{\ast},"['matrices', 'convex-analysis', 'proximal-operators', 'nuclear-norm', 'hankel-matrices']"
44,How to prove the following about eigenvalues,How to prove the following about eigenvalues,,"Let $\mathbf{M} = [m_{ij}]$ be a symmetric matrix of size $m\times m$ of real elements. Let $\mathbf{A} = [a_{ij}^R + ia_{ij}^I]$ be a random Hermitian matrix whose elements have variance, $\sigma^2$, and mean $m_{ij}$, respectively i.e mean matrix is same as $\mathbf{M}$. Next, let $\mathbf{B}= [b_{ij}]$ be a random matrix of real elements with mean $m_{ij}$ and variance $\frac{\sigma^2}{2}$. Using monte carlo, I found that the eigenvalue distribution of $\mathbf{A}$ and $\mathbf{B}$ only differ by a multiplying constant. i.e. if $p_{\Lambda}(\mathbf{X}_A)$ and $p_{\mu}(\mathbf{X}_A)$ are the eigenvalue density function of $\mathbf{A}$ and $\mathbf{B}$ then, $p_{\Lambda}(\mathbf{X}_A) = K$ $p_{\mu}(\mathbf{X}_A)$ where $K$ is a constant. How to prove it analytically? Here $\mathbf{X}_A$ is a set of eigenvalues $x_1,x_2,\cdots,x_m$.","Let $\mathbf{M} = [m_{ij}]$ be a symmetric matrix of size $m\times m$ of real elements. Let $\mathbf{A} = [a_{ij}^R + ia_{ij}^I]$ be a random Hermitian matrix whose elements have variance, $\sigma^2$, and mean $m_{ij}$, respectively i.e mean matrix is same as $\mathbf{M}$. Next, let $\mathbf{B}= [b_{ij}]$ be a random matrix of real elements with mean $m_{ij}$ and variance $\frac{\sigma^2}{2}$. Using monte carlo, I found that the eigenvalue distribution of $\mathbf{A}$ and $\mathbf{B}$ only differ by a multiplying constant. i.e. if $p_{\Lambda}(\mathbf{X}_A)$ and $p_{\mu}(\mathbf{X}_A)$ are the eigenvalue density function of $\mathbf{A}$ and $\mathbf{B}$ then, $p_{\Lambda}(\mathbf{X}_A) = K$ $p_{\mu}(\mathbf{X}_A)$ where $K$ is a constant. How to prove it analytically? Here $\mathbf{X}_A$ is a set of eigenvalues $x_1,x_2,\cdots,x_m$.",,"['matrices', 'probability-distributions', 'eigenvalues-eigenvectors', 'random-matrices']"
45,Prove Matrix logarithm and exponential are inverse.,Prove Matrix logarithm and exponential are inverse.,,"How do we show that the exponential function is the inverse of the logarithm function in the case for matrices, since they are defined as series. I.e. Defining $\ln(I+A) = A-A^2/2+\cdots$ and $\exp(A) = I + A + A^2/2! +\cdots$. We want to show $\exp(\ln(I+A))=I+A$. For the $\ln(I+A)$ to be defined we must have that $A$ is of norm less than 1. I assume this restriction does not go away when taking exponential.","How do we show that the exponential function is the inverse of the logarithm function in the case for matrices, since they are defined as series. I.e. Defining $\ln(I+A) = A-A^2/2+\cdots$ and $\exp(A) = I + A + A^2/2! +\cdots$. We want to show $\exp(\ln(I+A))=I+A$. For the $\ln(I+A)$ to be defined we must have that $A$ is of norm less than 1. I assume this restriction does not go away when taking exponential.",,"['linear-algebra', 'matrices']"
46,Closed form for elements of inverse matrix of lower triangular matrix of any size,Closed form for elements of inverse matrix of lower triangular matrix of any size,,"If we have a lower triangular matrix $$A=\left(\begin{array}{rrrrr}a_{1,1}&0&0&\cdots&0\\a_{1,2}&a_{2,2}&0&\cdots&0\\a_{1,3} &1_{2,3}&a_{3,3}&\cdots&0\\ \vdots&\vdots&\vdots&\ddots&\vdots\\a_{1,n}&a_{2,n}&a_{3,n}&\cdots&a_{n,n}\end{array}\right)$$ does there exist a formula for a certain element of $A^{-1}$ in terms of its coordinates, the elements of $A$ , and $\det(A)$ ? Given such a matrix of any particular size, we can calculate each term of the inverse matrix by expanding the multiplication and solving. For instance, if $A$ is a $3\rm x 3$ matrix, then we have $$A^{-1}=\left(\begin{array}{lll} \frac 1{a_{1,1}}&0&0\\ -a_{1,2}\over a_{1,1}a_{2,2}&\frac 1{a_{2,2}}&0\\ a_{1,2}a_{2,3}-a_{1,3}a_{2,2}\over a_{1,1}a_{2,2}a_{3,3}&-a_{2,3}\over a_{2,2}a_{3,3}&\frac 1{a_{3,3}} \end{array}\right).$$ We can, by similar methods, calculate exact formulae for any specific value of $n$ . These formulae seem tantalizingly as if there is some general formula that holds for all $n$ for each entry, but I am unable to find one. Does such a formula exist? Note: I am aware of a few other questions on this subject, but none of them seem to answer this; however, I know very little linear algebra I may have just failed to understand them.","If we have a lower triangular matrix does there exist a formula for a certain element of in terms of its coordinates, the elements of , and ? Given such a matrix of any particular size, we can calculate each term of the inverse matrix by expanding the multiplication and solving. For instance, if is a matrix, then we have We can, by similar methods, calculate exact formulae for any specific value of . These formulae seem tantalizingly as if there is some general formula that holds for all for each entry, but I am unable to find one. Does such a formula exist? Note: I am aware of a few other questions on this subject, but none of them seem to answer this; however, I know very little linear algebra I may have just failed to understand them.","A=\left(\begin{array}{rrrrr}a_{1,1}&0&0&\cdots&0\\a_{1,2}&a_{2,2}&0&\cdots&0\\a_{1,3} &1_{2,3}&a_{3,3}&\cdots&0\\ \vdots&\vdots&\vdots&\ddots&\vdots\\a_{1,n}&a_{2,n}&a_{3,n}&\cdots&a_{n,n}\end{array}\right) A^{-1} A \det(A) A 3\rm x 3 A^{-1}=\left(\begin{array}{lll}
\frac 1{a_{1,1}}&0&0\\
-a_{1,2}\over a_{1,1}a_{2,2}&\frac 1{a_{2,2}}&0\\
a_{1,2}a_{2,3}-a_{1,3}a_{2,2}\over a_{1,1}a_{2,2}a_{3,3}&-a_{2,3}\over a_{2,2}a_{3,3}&\frac 1{a_{3,3}}
\end{array}\right). n n","['linear-algebra', 'matrices', 'inverse']"
47,Self pseudo-inverse matrix,Self pseudo-inverse matrix,,"Let $A\in L^\infty(\Omega;\Bbb R^{2\times 2})$ , be positive definite such that $A^\dagger = A$ , where $A^\dagger$ is the pseudo-inverse of $A$ , one of the attributes of being a pseudo-inverse is that $AA^\dagger A=A$ ,  assuming $A\ne 0,A\ne I_d$ , then $A = A^3$ and so $A=\lim_{n\to\infty}A^{3n}=0$ if the largest eigenvalue of $A$ is less than $1$ , or $\infty$ if the largest eigenvalue is greater than $1$ . Does this imply in fact that $A=A^\dagger=A^{-1}$ , since if this is true then nothing ""blows up""?","Let , be positive definite such that , where is the pseudo-inverse of , one of the attributes of being a pseudo-inverse is that ,  assuming , then and so if the largest eigenvalue of is less than , or if the largest eigenvalue is greater than . Does this imply in fact that , since if this is true then nothing ""blows up""?","A\in L^\infty(\Omega;\Bbb R^{2\times 2}) A^\dagger = A A^\dagger A AA^\dagger A=A A\ne 0,A\ne I_d A = A^3 A=\lim_{n\to\infty}A^{3n}=0 A 1 \infty 1 A=A^\dagger=A^{-1}","['matrices', 'pseudoinverse']"
48,A more structural proof using homomorphisms and similar tools that every ideal of $M_n(R)$ is of the form $M_n(I)$,A more structural proof using homomorphisms and similar tools that every ideal of  is of the form,M_n(R) M_n(I),"Let $R$ be a ring with unity , we know that if $J$ is an ideal of $M_n(R)$ then for some ideal $I$ of $R$ , $J=M_n(I)$ . The proof I know is very tedious and uses laborious manipulations using elementary matrices and all that . I was thinking , can we give a more structural proof , like using ring homomorphisms or so ? I kind of feel some hint of correspondence theorem in the result but cannot actually grasp it ... Please help","Let $R$ be a ring with unity , we know that if $J$ is an ideal of $M_n(R)$ then for some ideal $I$ of $R$ , $J=M_n(I)$ . The proof I know is very tedious and uses laborious manipulations using elementary matrices and all that . I was thinking , can we give a more structural proof , like using ring homomorphisms or so ? I kind of feel some hint of correspondence theorem in the result but cannot actually grasp it ... Please help",,['matrices']
49,How to compute QR decomposition of a product of matrices,How to compute QR decomposition of a product of matrices,,"Suppose I have $A=A_nA_{n-1}\cdots A_2A_1$ How can I compute the $QR$ factorization of $A$ without explicitly multiplying $A_1, A_2, \ldots, A_n$ together? The suggestion I got is that, suppose $n=3$ and $Q_3^T A =R$ The write  $$Q_3^T A =Q_3^T A_3Q_2Q_2^T A_2Q_1Q_1^T A_1Q_0,  Q_0=I$$ Then find orthogonal $Q_i$ such that $Q_i^T A_iQ_{i-1}$ is upper triangular.","Suppose I have $A=A_nA_{n-1}\cdots A_2A_1$ How can I compute the $QR$ factorization of $A$ without explicitly multiplying $A_1, A_2, \ldots, A_n$ together? The suggestion I got is that, suppose $n=3$ and $Q_3^T A =R$ The write  $$Q_3^T A =Q_3^T A_3Q_2Q_2^T A_2Q_1Q_1^T A_1Q_0,  Q_0=I$$ Then find orthogonal $Q_i$ such that $Q_i^T A_iQ_{i-1}$ is upper triangular.",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'triangulation']"
50,Calculation of an expression ($\max_{U}\min_i \sum_j |U_{ij}|^2 |e_i^j|^2$),Calculation of an expression (),\max_{U}\min_i \sum_j |U_{ij}|^2 |e_i^j|^2,"There is an orthonormal basis $\{e_i\}(i=1,\ldots,n)$ in $\mathbb{C}^n$, each of them is represented in form of column vectors $$\begin{pmatrix} e_i^1\\ \vdots\\e_i^n\end{pmatrix}.$$ My purpose is to calculate the following expression: $$ \max_{U}\min_i \sum_j |U_{ij}|^2 |e_i^j|^2, $$ where $U$ is any $n$-dimension unitary matrix.","There is an orthonormal basis $\{e_i\}(i=1,\ldots,n)$ in $\mathbb{C}^n$, each of them is represented in form of column vectors $$\begin{pmatrix} e_i^1\\ \vdots\\e_i^n\end{pmatrix}.$$ My purpose is to calculate the following expression: $$ \max_{U}\min_i \sum_j |U_{ij}|^2 |e_i^j|^2, $$ where $U$ is any $n$-dimension unitary matrix.",,"['linear-algebra', 'matrices', 'optimization', 'summation']"
51,"(Fast) eigen decomposition of $DXD$ where $D$ is diagonal, $X$ is symmetric with known eigen decomposition","(Fast) eigen decomposition of  where  is diagonal,  is symmetric with known eigen decomposition",DXD D X,"Assuming that I already know the eigen-decomposition of a real symmetric matrix $X$, is there any way to use it to retrieve efficiently the eigen-decomposition of $DXD$, where $D$ is a diagonal matrix? (I mean, more efficiently than computing $DXD$ and decomposing it). Modulo a base change, this is the same as asking for an efficient way to eigen-decompose $A\Sigma A$ with $\Sigma$ diagonal and $A$ symmetric with known eigen-decomposition. Maybe some iterative algorithm can use the known eigenvectors as a starting point and converge really fast? In fact, I strongly doubt there is anything, but I will feel better if someone can provide an definitive argument to close the question.","Assuming that I already know the eigen-decomposition of a real symmetric matrix $X$, is there any way to use it to retrieve efficiently the eigen-decomposition of $DXD$, where $D$ is a diagonal matrix? (I mean, more efficiently than computing $DXD$ and decomposing it). Modulo a base change, this is the same as asking for an efficient way to eigen-decompose $A\Sigma A$ with $\Sigma$ diagonal and $A$ symmetric with known eigen-decomposition. Maybe some iterative algorithm can use the known eigenvectors as a starting point and converge really fast? In fact, I strongly doubt there is anything, but I will feel better if someone can provide an definitive argument to close the question.",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'quadratic-forms']"
52,Block matrix determinant with symmetrically placed blocks,Block matrix determinant with symmetrically placed blocks,,"I am trying to solve the determinant of the following block matrix $$\begin{bmatrix}A-Ia&B\\B &A-Ib \end{bmatrix}$$ where $a$ and $b$ are integers and $I$ is an identity matrix. Matrices $A$ and $B$ are square. Unfortunately, I can't invoke the general expression since I don't want to attempt to invert any of these matrices (very complex). I see from Wikipedia that $$\det\begin{bmatrix}A&B\\B &A \end{bmatrix} = \det(A+B)\det(A-B)$$ Can anyone help me prove that this can be applied to my matrix (Such that I might get something like $\det(A-Ia+B)\det(A-Ib-B)$ )? Or alternatively, can anyone solve the determinant of my matrix without taking an inverse? Thanks!","I am trying to solve the determinant of the following block matrix where and are integers and is an identity matrix. Matrices and are square. Unfortunately, I can't invoke the general expression since I don't want to attempt to invert any of these matrices (very complex). I see from Wikipedia that Can anyone help me prove that this can be applied to my matrix (Such that I might get something like )? Or alternatively, can anyone solve the determinant of my matrix without taking an inverse? Thanks!",\begin{bmatrix}A-Ia&B\\B &A-Ib \end{bmatrix} a b I A B \det\begin{bmatrix}A&B\\B &A \end{bmatrix} = \det(A+B)\det(A-B) \det(A-Ia+B)\det(A-Ib-B),"['matrices', 'determinant', 'block-matrices']"
53,Why a tensor product of $2\times 2$ unitaries cannot implement a $3\times 3$ unitary?,Why a tensor product of  unitaries cannot implement a  unitary?,2\times 2 3\times 3,"Let $\{v_1, \dotsc, v_m\} \in \mathbb{C}^{2^n}$ be a set of orthonormal vectors. Define a map $R_m$ from $2^n \times 2^n$ to $m \times m$ matrices as follows: $$R_m(M) := \sum_{i,j=1}^m (v_i^*M v_j) E_{ij}$$ where $E_{ij} := e_i e_j^*$ is the all-zeroes matrix with entry 1 at position $(i,j)$. In other words, $R_m$ performs a unitary change of basis and then takes the top-left $3 \times 3$ block of the resulting matrix. For any $n \geq 2$, show that no matter how the vectors $\{v_i\}$ are chosen, $$\mathrm{U}(3) \not\subseteq \bigl\{ R_3(U_1 \otimes \dotsb \otimes U_n) : U_i \in \mathrm{U}(2) \bigr\}.$$ That is, as matrices $U_i$ range over all possible $2 \times 2$ unitaries, their tensor product will never contain the set of all $3 \times 3$ unitaries in some basis. More generally, this should hold even if $2 \times 2$ unitaries are replaced by $d \times d$ unitaries and $m = 3$ is replaced by any $m > d$. The motivation for this question comes from ""encoded universality"" in quantum computing.","Let $\{v_1, \dotsc, v_m\} \in \mathbb{C}^{2^n}$ be a set of orthonormal vectors. Define a map $R_m$ from $2^n \times 2^n$ to $m \times m$ matrices as follows: $$R_m(M) := \sum_{i,j=1}^m (v_i^*M v_j) E_{ij}$$ where $E_{ij} := e_i e_j^*$ is the all-zeroes matrix with entry 1 at position $(i,j)$. In other words, $R_m$ performs a unitary change of basis and then takes the top-left $3 \times 3$ block of the resulting matrix. For any $n \geq 2$, show that no matter how the vectors $\{v_i\}$ are chosen, $$\mathrm{U}(3) \not\subseteq \bigl\{ R_3(U_1 \otimes \dotsb \otimes U_n) : U_i \in \mathrm{U}(2) \bigr\}.$$ That is, as matrices $U_i$ range over all possible $2 \times 2$ unitaries, their tensor product will never contain the set of all $3 \times 3$ unitaries in some basis. More generally, this should hold even if $2 \times 2$ unitaries are replaced by $d \times d$ unitaries and $m = 3$ is replaced by any $m > d$. The motivation for this question comes from ""encoded universality"" in quantum computing.",,"['group-theory', 'matrices', 'representation-theory', 'lie-groups', 'quantum-computation']"
54,The relation between $\inf_{R\in \mathsf{U}_n} \left\Vert A - BR\right\Vert^2_F$ and $\left\Vert AA^*-BB^*\right\Vert$,The relation between  and,\inf_{R\in \mathsf{U}_n} \left\Vert A - BR\right\Vert^2_F \left\Vert AA^*-BB^*\right\Vert,"Suppose that $A$ and $B$ are two arbitrary $m\times n$ matrices with $m>n$. Let $\mathsf{U}_n$ denote the set of $n\times n$ unitary matrices. I'd like find positive constants $c_1$ and $c_2$ such that  $$ \begin{align*} c_2\left\Vert AA^*-BB^*\right\Vert &\geq\inf_{R\in \mathsf{U}_n} \left\Vert A - BR\right\Vert^2_F \geq c_1\left\Vert AA^*-BB^*\right\Vert, \end{align*} $$  holds (and the inequalities are sharp enough). I'm more interested in the upped bound. I have a partial answer that addresses some special cases. For example, if $n=1$ then I've shown that $c_2=\sqrt{2}$. Furthermore, if we assume that $A$ and $B$ have the same range, it's not difficult to find the appropriate $c_2$. My conjecture is that $c_2 = \sqrt{2n}$, but I haven't been able to prove it. The infimum above is the standard orthogonal Procrustes problem and it is relatively straightforward to show that $$ \begin{align*} \inf_{R\in \mathsf{U}_n} \left\Vert A - BR\right\Vert^2_F &=\left\Vert A\right\Vert_F ^2 +\left\Vert B\right\Vert_F ^2 -2\left\Vert A^*B\right\Vert_*,  \end{align*} $$ where $\left\Vert \cdot\right\Vert_*$ denotes the nuclear norm (or the Schatten 1-norm ).","Suppose that $A$ and $B$ are two arbitrary $m\times n$ matrices with $m>n$. Let $\mathsf{U}_n$ denote the set of $n\times n$ unitary matrices. I'd like find positive constants $c_1$ and $c_2$ such that  $$ \begin{align*} c_2\left\Vert AA^*-BB^*\right\Vert &\geq\inf_{R\in \mathsf{U}_n} \left\Vert A - BR\right\Vert^2_F \geq c_1\left\Vert AA^*-BB^*\right\Vert, \end{align*} $$  holds (and the inequalities are sharp enough). I'm more interested in the upped bound. I have a partial answer that addresses some special cases. For example, if $n=1$ then I've shown that $c_2=\sqrt{2}$. Furthermore, if we assume that $A$ and $B$ have the same range, it's not difficult to find the appropriate $c_2$. My conjecture is that $c_2 = \sqrt{2n}$, but I haven't been able to prove it. The infimum above is the standard orthogonal Procrustes problem and it is relatively straightforward to show that $$ \begin{align*} \inf_{R\in \mathsf{U}_n} \left\Vert A - BR\right\Vert^2_F &=\left\Vert A\right\Vert_F ^2 +\left\Vert B\right\Vert_F ^2 -2\left\Vert A^*B\right\Vert_*,  \end{align*} $$ where $\left\Vert \cdot\right\Vert_*$ denotes the nuclear norm (or the Schatten 1-norm ).",,"['linear-algebra', 'matrices', 'inequality', 'nuclear-norm']"
55,An inequality concerning non-negative integer matrices with constant row and column sums,An inequality concerning non-negative integer matrices with constant row and column sums,,"I'd appreciate any suggestions for how to prove (or disprove) the inequality described below. Some notation first: for positive integers $k$ and $M$, let ${\mathcal D}_{k,M}$ denote the set of all $k \times k$ non-negative integer matrices with row and column sums all equal to $M$. For $k \times k$ matrices $A = [a_{i,j}]$ and $B = [b_{i,j}]$, we write $A \le B$ if $a_{i,j} \le b_{i,j}$ for all $i,j$. Now, let $M, N$ be fixed positive integers with $M < N$. I'd like to prove that for any $B \in \mathcal{D}_{k,N}$, we have $\displaystyle \sum_{A \in \mathcal{D}_{k,M}: A \le B} \prod_{i,j} \frac{\binom{b_{i,j}}{a_{i,j}}}{\binom{N-b_{i,j}}{M-a_{i,j}}} \ge {\binom{N}{M}}^{2k-k^2}$. It is easy to check that the inequality holds with equality when $B$ is $N$ times a permutation matrix. It is also straightforward to show that the inequality holds when $B$ has at most two non-zero entries in each row. In particular, the inequality holds when $B$ is a convex combination of $B_1$ and $B_2$, where each of $B_1$ and $B_2$ is $N$ times a permutation matrix. How to proceed when $B$ is a convex combination of three or more such matrices is not clear.","I'd appreciate any suggestions for how to prove (or disprove) the inequality described below. Some notation first: for positive integers $k$ and $M$, let ${\mathcal D}_{k,M}$ denote the set of all $k \times k$ non-negative integer matrices with row and column sums all equal to $M$. For $k \times k$ matrices $A = [a_{i,j}]$ and $B = [b_{i,j}]$, we write $A \le B$ if $a_{i,j} \le b_{i,j}$ for all $i,j$. Now, let $M, N$ be fixed positive integers with $M < N$. I'd like to prove that for any $B \in \mathcal{D}_{k,N}$, we have $\displaystyle \sum_{A \in \mathcal{D}_{k,M}: A \le B} \prod_{i,j} \frac{\binom{b_{i,j}}{a_{i,j}}}{\binom{N-b_{i,j}}{M-a_{i,j}}} \ge {\binom{N}{M}}^{2k-k^2}$. It is easy to check that the inequality holds with equality when $B$ is $N$ times a permutation matrix. It is also straightforward to show that the inequality holds when $B$ has at most two non-zero entries in each row. In particular, the inequality holds when $B$ is a convex combination of $B_1$ and $B_2$, where each of $B_1$ and $B_2$ is $N$ times a permutation matrix. How to proceed when $B$ is a convex combination of three or more such matrices is not clear.",,"['matrices', 'inequality', 'binomial-coefficients']"
56,simple formula for det(AB-BA)?,simple formula for det(AB-BA)?,,"I was wondering whether there exists a simple formula for $\det(AB-BA)$ , given two square matrices $A$ and $B$ with coefficients in a commutative ring.","I was wondering whether there exists a simple formula for , given two square matrices and with coefficients in a commutative ring.",\det(AB-BA) A B,"['linear-algebra', 'matrices', 'determinant']"
57,Tools to bound the singular values of a finite sum of random matrices from below?,Tools to bound the singular values of a finite sum of random matrices from below?,,"Matrix Chernoff bounds (see also this arXiv paper ) are usually used to give upper bounds on the largest eigenvalue of a finite sum of random matrices. Sometimes it can also be used to give a lower bound on the smallest eigenvalue of a finite sum of random positive semidefinite (PSD) matrices. For PSD matrices, the eigenvalues are the same as the singular values. My question is: Is there any tool, or any other special cases (apart from the PSD case) for which there is a tool, to obtain lower bounds on the singular values of a finite sum of random matrices, or random symmetrical matrices?","Matrix Chernoff bounds (see also this arXiv paper ) are usually used to give upper bounds on the largest eigenvalue of a finite sum of random matrices. Sometimes it can also be used to give a lower bound on the smallest eigenvalue of a finite sum of random positive semidefinite (PSD) matrices. For PSD matrices, the eigenvalues are the same as the singular values. My question is: Is there any tool, or any other special cases (apart from the PSD case) for which there is a tool, to obtain lower bounds on the singular values of a finite sum of random matrices, or random symmetrical matrices?",,"['calculus', 'linear-algebra', 'matrices', 'inequality', 'concentration-of-measure']"
58,Second structural equations in lorentzian space $\Bbb L^3$.,Second structural equations in lorentzian space .,\Bbb L^3,"I'm rewriting O'Neill's ""Elementary Differential Geometry""'s section on connection forms in Lorentz-Minkowski space $\Bbb L^3$, and I'm having trouble proving the second structural equations $${\rm d}\omega_{ij} = \sum_k \omega_{ik} \wedge \omega_{\color{blue}{kj}}.$$ I'm using the signature $(+,+,-)$, that is, $$G = \begin{pmatrix} 1& 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0  & -1\end{pmatrix}.$$ I know for a fact that this result is true, I've seen it in a more general form in his other book, ""The Geometry of Kerr Black Holes"". The proof there is overkill for what I'm doing here. Since these sign conventions are such a pain, I'll put the definitions I'm working with here, along with two useful results that I've proved: Definition: Given a frame field $\{{\bf E}_i\}_{i = 1}^3$, the $1$-forms defined by $$\omega_{ij}({\bf V}) = \epsilon_j \langle  \nabla_{\bf V}{\bf E}_i, {\bf E}_j \rangle$$ are called the connection    forms of the given frame field. Also, $\omega_{ij} = -\epsilon_i  \epsilon_j \omega_{ji}$, where $\epsilon_i = \langle {\bf E}_i,{\bf  E}_i\rangle$. Definition: Given a matrix $A = (a_{ij})_{1 \leq i,j \leq 3}$, where the $a_{ij}$ are functions, we define ${\rm d}A = ({\rm  d}a_{ij})_{1 \leq i,j \leq 3}$ and $A_\epsilon = (\epsilon_i  a_{ij})_{1 \leq i,j \leq 3}$. Theorem: Let $\{{\bf E}_i\}_{i = 1}^3$ be a frame field, and $A = (a_{ij})_{1 \leq i,j\leq 3}$ be its attitude matrix, that is, ${\bf E}_i = \sum_j a_{ij} \partial_j$. If $\omega = (\omega_{ij})_{1 \leq i,j \leq 3}$, then $$\omega = {\rm d}A \cdot GA_\epsilon^{ \ t},$$ where the product in    the right side is to be seen as the wedge product. Note: $A_\epsilon^{  \ t} \neq A_{\ \epsilon}^t$. Lemma: With notation as above: $$A \cdot GA_{\epsilon}^{\ t} = {\rm Id_3}.$$ With this I can go on. I have: $$\begin{align}\omega &=  {\rm d}A \cdot GA_\epsilon^{ \ t} \\ &\implies {\rm d}\omega = {\rm d}( {\rm d}A \cdot GA_\epsilon^{ \ t}) \\ &\implies {\rm d}\omega = -  {\rm d}A\cdot {\rm d}(GA_\epsilon^{\ t})  \\ &\implies {\rm d}\omega = - {\rm d}A \cdot G ({\rm d}A)_\epsilon^{\ t} \\ &\implies{\rm d}\omega = -{\rm d}A \cdot GA_{\epsilon}^{\ t} \cdot A \cdot G^t({\rm d}A)_\epsilon^{\ t} \\ &\implies {\rm d}\omega = -\omega \cdot ({\rm d}A)_\epsilon G A^t\end{align}.$$ Since I don't know how to go from here, I'll open both sides. Brace yourself, it'll be ugly. $$\begin{align} {\rm d}\omega_{ij} &= -\sum_k \omega_{ik} \wedge (({\rm d}A)_\epsilon G A^t)_{kj}\\ &= -\sum_k \omega_{ik} \wedge \left(\sum_l \epsilon_k {\rm d}a_{kl}(GA^t)_{lj}\right) \\ &= -\sum_k \epsilon_k\omega_{ik} \wedge \left(\sum_l {\rm d}a_{kl} (g_{l1}a_{j1} + g_{l2}a_{j2}+g_{l3}a_{j3})\right) \\  &= -\sum_k \epsilon_k \omega_{ik } \wedge ({\rm d}a_{k1}a_{j1}+{\rm d}a_{k2}a_{j2} - {\rm d}a_{k3}a_{j3}) \\ &= \sum_{k} \omega_{ik} \wedge (-\epsilon_k \epsilon_j \omega_{kj}) \\ &= \sum_k \omega_{ik} \wedge \omega_{\color{red}{jk}}\end{align}.$$ The indices are swapped in this last $\omega$ and I can't find the mistake. Please help.","I'm rewriting O'Neill's ""Elementary Differential Geometry""'s section on connection forms in Lorentz-Minkowski space $\Bbb L^3$, and I'm having trouble proving the second structural equations $${\rm d}\omega_{ij} = \sum_k \omega_{ik} \wedge \omega_{\color{blue}{kj}}.$$ I'm using the signature $(+,+,-)$, that is, $$G = \begin{pmatrix} 1& 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0  & -1\end{pmatrix}.$$ I know for a fact that this result is true, I've seen it in a more general form in his other book, ""The Geometry of Kerr Black Holes"". The proof there is overkill for what I'm doing here. Since these sign conventions are such a pain, I'll put the definitions I'm working with here, along with two useful results that I've proved: Definition: Given a frame field $\{{\bf E}_i\}_{i = 1}^3$, the $1$-forms defined by $$\omega_{ij}({\bf V}) = \epsilon_j \langle  \nabla_{\bf V}{\bf E}_i, {\bf E}_j \rangle$$ are called the connection    forms of the given frame field. Also, $\omega_{ij} = -\epsilon_i  \epsilon_j \omega_{ji}$, where $\epsilon_i = \langle {\bf E}_i,{\bf  E}_i\rangle$. Definition: Given a matrix $A = (a_{ij})_{1 \leq i,j \leq 3}$, where the $a_{ij}$ are functions, we define ${\rm d}A = ({\rm  d}a_{ij})_{1 \leq i,j \leq 3}$ and $A_\epsilon = (\epsilon_i  a_{ij})_{1 \leq i,j \leq 3}$. Theorem: Let $\{{\bf E}_i\}_{i = 1}^3$ be a frame field, and $A = (a_{ij})_{1 \leq i,j\leq 3}$ be its attitude matrix, that is, ${\bf E}_i = \sum_j a_{ij} \partial_j$. If $\omega = (\omega_{ij})_{1 \leq i,j \leq 3}$, then $$\omega = {\rm d}A \cdot GA_\epsilon^{ \ t},$$ where the product in    the right side is to be seen as the wedge product. Note: $A_\epsilon^{  \ t} \neq A_{\ \epsilon}^t$. Lemma: With notation as above: $$A \cdot GA_{\epsilon}^{\ t} = {\rm Id_3}.$$ With this I can go on. I have: $$\begin{align}\omega &=  {\rm d}A \cdot GA_\epsilon^{ \ t} \\ &\implies {\rm d}\omega = {\rm d}( {\rm d}A \cdot GA_\epsilon^{ \ t}) \\ &\implies {\rm d}\omega = -  {\rm d}A\cdot {\rm d}(GA_\epsilon^{\ t})  \\ &\implies {\rm d}\omega = - {\rm d}A \cdot G ({\rm d}A)_\epsilon^{\ t} \\ &\implies{\rm d}\omega = -{\rm d}A \cdot GA_{\epsilon}^{\ t} \cdot A \cdot G^t({\rm d}A)_\epsilon^{\ t} \\ &\implies {\rm d}\omega = -\omega \cdot ({\rm d}A)_\epsilon G A^t\end{align}.$$ Since I don't know how to go from here, I'll open both sides. Brace yourself, it'll be ugly. $$\begin{align} {\rm d}\omega_{ij} &= -\sum_k \omega_{ik} \wedge (({\rm d}A)_\epsilon G A^t)_{kj}\\ &= -\sum_k \omega_{ik} \wedge \left(\sum_l \epsilon_k {\rm d}a_{kl}(GA^t)_{lj}\right) \\ &= -\sum_k \epsilon_k\omega_{ik} \wedge \left(\sum_l {\rm d}a_{kl} (g_{l1}a_{j1} + g_{l2}a_{j2}+g_{l3}a_{j3})\right) \\  &= -\sum_k \epsilon_k \omega_{ik } \wedge ({\rm d}a_{k1}a_{j1}+{\rm d}a_{k2}a_{j2} - {\rm d}a_{k3}a_{j3}) \\ &= \sum_{k} \omega_{ik} \wedge (-\epsilon_k \epsilon_j \omega_{kj}) \\ &= \sum_k \omega_{ik} \wedge \omega_{\color{red}{jk}}\end{align}.$$ The indices are swapped in this last $\omega$ and I can't find the mistake. Please help.",,"['matrices', 'differential-geometry', 'differential-forms', 'semi-riemannian-geometry']"
59,How does pointwise multiplication of two matrices affect their eigenvectors?,How does pointwise multiplication of two matrices affect their eigenvectors?,,"More specifically, suppose I have a known matrix $X\in\mathbb{R}^{d\times n}$ and an unkown vector $\alpha \in \mathbb{R}^n$. What can be said about the eigenvectors of $\alpha\alpha^T \odot X^T X$ in terms of the eigenvectors of $X^T X$? (The notation $\odot$ means $[A\odot B]_{ij}=A_{ij}B_{ij}$).","More specifically, suppose I have a known matrix $X\in\mathbb{R}^{d\times n}$ and an unkown vector $\alpha \in \mathbb{R}^n$. What can be said about the eigenvectors of $\alpha\alpha^T \odot X^T X$ in terms of the eigenvectors of $X^T X$? (The notation $\odot$ means $[A\odot B]_{ij}=A_{ij}B_{ij}$).",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
60,"If $n$ is even, every skew-symmetric $n\times n$ matrix $A$ can be factored as $A=SBS^T$","If  is even, every skew-symmetric  matrix  can be factored as",n n\times n A A=SBS^T,"If $n$ is even, every skew-symmetric $n\times n$ matrix $A$ can be factored as $A=SBS^T$ where $S$ is a invertible matrix and $B$ has the form $B = \left( \begin{array}{ccc} 0 & a_1 & 0 & 0 &0&0\\ -a_1 & 0 &0&0 &0&0\\ 0&0&0&a_{2}&0&0\\ 0 & 0&-a_{2}&0&0&0\\ 0&0&0&0&0&a_{n/2}\\ 0&0&0&0&-a_{n/2}&0 \end{array} \right)$ The $a_{n/2}$ is supposed to represent that $B$ can be any even $n\times n$ size (the size of $A$). So my book shows this result without proving it, and I was wondering why it's true. It doesn't seem clear to me why this specific factorization holds. How can you prove that it's true? What would $S$ or $S^T$ look like? Some useful results: If $E$ is an elementary matrix obtained from $I_n$ by carrying out one   elementary row operation on $I_n$, then $EA$ is a matrix obtained by   carrying out a single elementary row operation on $A$, and $AE$ is a   matrix obtained by carrying out a single elementary column operation   on $A$. $A$ is invertible iff it can be factored as a product of elementary   matrices: $A = E_1...E_m$.","If $n$ is even, every skew-symmetric $n\times n$ matrix $A$ can be factored as $A=SBS^T$ where $S$ is a invertible matrix and $B$ has the form $B = \left( \begin{array}{ccc} 0 & a_1 & 0 & 0 &0&0\\ -a_1 & 0 &0&0 &0&0\\ 0&0&0&a_{2}&0&0\\ 0 & 0&-a_{2}&0&0&0\\ 0&0&0&0&0&a_{n/2}\\ 0&0&0&0&-a_{n/2}&0 \end{array} \right)$ The $a_{n/2}$ is supposed to represent that $B$ can be any even $n\times n$ size (the size of $A$). So my book shows this result without proving it, and I was wondering why it's true. It doesn't seem clear to me why this specific factorization holds. How can you prove that it's true? What would $S$ or $S^T$ look like? Some useful results: If $E$ is an elementary matrix obtained from $I_n$ by carrying out one   elementary row operation on $I_n$, then $EA$ is a matrix obtained by   carrying out a single elementary row operation on $A$, and $AE$ is a   matrix obtained by carrying out a single elementary column operation   on $A$. $A$ is invertible iff it can be factored as a product of elementary   matrices: $A = E_1...E_m$.",,"['linear-algebra', 'matrices']"
61,Oscillation matrix whose $n-1$ power is totally positive,Oscillation matrix whose  power is totally positive,n-1,"Let $A$ be a $n\times n$ real matrix. If all the sub-determinant of $A$ is $\geq0$, then $A$ is called totally non-negative. If all the sub-determinant of $A$ is $>0$, then $A$ is called totally positive. If for a totally non-negative matrix $A$, there exists a positive integer $m$ such that $A^m$ is totally positive, then $A$ is called an oscillation matrix. Prove that if $A$ is an oscillation matrix, then $A^{n-1}$ is totally positive... I have no ideas...my god.","Let $A$ be a $n\times n$ real matrix. If all the sub-determinant of $A$ is $\geq0$, then $A$ is called totally non-negative. If all the sub-determinant of $A$ is $>0$, then $A$ is called totally positive. If for a totally non-negative matrix $A$, there exists a positive integer $m$ such that $A^m$ is totally positive, then $A$ is called an oscillation matrix. Prove that if $A$ is an oscillation matrix, then $A^{n-1}$ is totally positive... I have no ideas...my god.",,['matrices']
62,Seeking diagonal $D$ such that $AD^2B$ is positive definite (A and B are symmetric and p.d.),Seeking diagonal  such that  is positive definite (A and B are symmetric and p.d.),D AD^2B,"Let A,B be two real symmetric, non-commuting positive definite matrices. Then the product AB is not positive definite (in the sense that ${\bf x}^TAB{\bf x}$ may be anything). I'm trying to find conditions on a diagonal matrix D such that ${\bf x}^TAD^2B{\bf x}\geq 0$ for any nonzero real vector ${\bf x}$. I'm particularly interested in allowing $D$ to contain zeros. Then $AD^2B$ would be singular. I know that this is equivalent to proving that $AD^2B+BD^2A$ is symmetric and positive definite, but cannot get any further. Thanks for any help!","Let A,B be two real symmetric, non-commuting positive definite matrices. Then the product AB is not positive definite (in the sense that ${\bf x}^TAB{\bf x}$ may be anything). I'm trying to find conditions on a diagonal matrix D such that ${\bf x}^TAD^2B{\bf x}\geq 0$ for any nonzero real vector ${\bf x}$. I'm particularly interested in allowing $D$ to contain zeros. Then $AD^2B$ would be singular. I know that this is equivalent to proving that $AD^2B+BD^2A$ is symmetric and positive definite, but cannot get any further. Thanks for any help!",,"['linear-algebra', 'matrices']"
63,Eigenvectors of difference of inverse matrices,Eigenvectors of difference of inverse matrices,,"I have two matrices $A$ and $B$, symmetric and positive semi-definite (in fact, they are covariance matrices), and I am interested in computing the eigenvectors of the matrix $A^{-1}-B^{-1}$. From numerical simulations, it appears that the eigenvectors of $A^{-1}-B^{-1}$ are often well approximated simply by the eigenvectors of $A-B$ . Moreover, if I regularize the matrices $A$ and $B$ before inverting them, by computing the matrix $(A+k\cdot I)^{-1} - (B+k\cdot I)^{-1} $ (where k is a positive constant, and $I$ is the identity matrix), the eigenvectors of $A-B$ provide an almost perfect approximation, especially if k is large. Is there a way to prove that the eigenvectors of $A-B$ are a good approximation of the eigenvectors of $A^{-1}-B^{-1}$ ? Edit : in light of your (very helpful) comments, I expanded my numerical tests to larger matrices, where I can see that it is not generally true that the eigenvectors of $A-B$ are close to those of $A^{-1}-B^{-1}$. However, this still works if I consider the ""regularized"" versions of $A$ and $B$, i.e. the eigenvectors of $A-B$ correspond to the eigenvectors of $(A+k\cdot I)^{-1} - (B+k\cdot I)^{-1} $ (where k is a large positive constant, and $I$ is the identity matrix). Is there any way to prove this? Edit 2 : I think I might have found the proof for this. There is a well known result that $(I-A)^{-1}= I+A+A^2+A^3+...$ (this can be shown by multiplying both sides by $I-A$). My expressions $A+k \cdot I$ can be rewritten as $\epsilon \cdot A +I$ (for small $\epsilon$), and so I can approximate $(\epsilon \cdot A +I)^{-1}$ with $I-A$ because I drop the following terms. Finally I have: $(A+k\cdot I)^{-1} - (B+k\cdot I)^{-1} = (\epsilon \cdot A+ I)^{-1} - (\epsilon \cdot B+ I)^{-1} = (I-A)-(I-B) = B-A$ Please let me know if you have any other insights about this, and thanks again for your help!","I have two matrices $A$ and $B$, symmetric and positive semi-definite (in fact, they are covariance matrices), and I am interested in computing the eigenvectors of the matrix $A^{-1}-B^{-1}$. From numerical simulations, it appears that the eigenvectors of $A^{-1}-B^{-1}$ are often well approximated simply by the eigenvectors of $A-B$ . Moreover, if I regularize the matrices $A$ and $B$ before inverting them, by computing the matrix $(A+k\cdot I)^{-1} - (B+k\cdot I)^{-1} $ (where k is a positive constant, and $I$ is the identity matrix), the eigenvectors of $A-B$ provide an almost perfect approximation, especially if k is large. Is there a way to prove that the eigenvectors of $A-B$ are a good approximation of the eigenvectors of $A^{-1}-B^{-1}$ ? Edit : in light of your (very helpful) comments, I expanded my numerical tests to larger matrices, where I can see that it is not generally true that the eigenvectors of $A-B$ are close to those of $A^{-1}-B^{-1}$. However, this still works if I consider the ""regularized"" versions of $A$ and $B$, i.e. the eigenvectors of $A-B$ correspond to the eigenvectors of $(A+k\cdot I)^{-1} - (B+k\cdot I)^{-1} $ (where k is a large positive constant, and $I$ is the identity matrix). Is there any way to prove this? Edit 2 : I think I might have found the proof for this. There is a well known result that $(I-A)^{-1}= I+A+A^2+A^3+...$ (this can be shown by multiplying both sides by $I-A$). My expressions $A+k \cdot I$ can be rewritten as $\epsilon \cdot A +I$ (for small $\epsilon$), and so I can approximate $(\epsilon \cdot A +I)^{-1}$ with $I-A$ because I drop the following terms. Finally I have: $(A+k\cdot I)^{-1} - (B+k\cdot I)^{-1} = (\epsilon \cdot A+ I)^{-1} - (\epsilon \cdot B+ I)^{-1} = (I-A)-(I-B) = B-A$ Please let me know if you have any other insights about this, and thanks again for your help!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'inverse']"
64,"Looking for an algo to ""sorta"" diagonalize a similarity matrix.","Looking for an algo to ""sorta"" diagonalize a similarity matrix.",,"I've got a big fat similarity matrix. The rows and columns represent people, and the values represent some positive measure of their closeness (0 meaning no connection at all). The n-th row and n-th colum corresponds to the same person - thus the matrix is square. I'm looking for an algorithm to find some permutation of rows/columns such that the resulting matrix has as much ""mass"" aligned towards the diagonal as possible. The goal is find a matrix so that average closeness of neighbors (neighboring rows/columns) is maximized. The ultimate goal is to use this as a sort of clustering algorithm.","I've got a big fat similarity matrix. The rows and columns represent people, and the values represent some positive measure of their closeness (0 meaning no connection at all). The n-th row and n-th colum corresponds to the same person - thus the matrix is square. I'm looking for an algorithm to find some permutation of rows/columns such that the resulting matrix has as much ""mass"" aligned towards the diagonal as possible. The goal is find a matrix so that average closeness of neighbors (neighboring rows/columns) is maximized. The ultimate goal is to use this as a sort of clustering algorithm.",,"['matrices', 'algorithms', 'clustering']"
65,Does there exist a finite axiomatization of the quasi-algebraic theory of real matrix rings?,Does there exist a finite axiomatization of the quasi-algebraic theory of real matrix rings?,,"Some definitions. Let us take the signature of ring theory to consist of the function symbols $\{+,-,0,\cdot,1\}$ equipped with their usual airities, where the minus symbol represents a unary operation. Write $\mathrm{E}$ for the set of all equations in the language of rings. By the quasi-algebraic language of rings, let us mean the following set of formulae. $$L = E \cup \bigcup_{n \geq 1}\{\varphi_0,\ldots,\varphi_{n-1} \rightarrow \psi \mid  (\forall i < n)(\varphi_i \in E) \wedge (\psi  \in E)\}.$$ By the quasi-algebraic theory of real matrix rings, let us mean the set $T$ of all $q \in L$ such that for every natural number $n \geq 1$, every subring of the matrix ring $M_n(\mathbb{R})$ is a model for $q$. Actually, since the formulae of interest are downward-absolute, there is no need to consider subrings at all; in particular, $T$ can be described as the set of all $q \in L$ such that for every natural number $n \geq 1$, the matrix ring $M_n(\mathbb{R})$ is a model for $q$. For example, the formula $xy=1 \rightarrow yx=1$ is an element of $T$. Question. Does there exist a finite axiomatization of $T$? If so, an explicit list of axioms would be much appreciated.","Some definitions. Let us take the signature of ring theory to consist of the function symbols $\{+,-,0,\cdot,1\}$ equipped with their usual airities, where the minus symbol represents a unary operation. Write $\mathrm{E}$ for the set of all equations in the language of rings. By the quasi-algebraic language of rings, let us mean the following set of formulae. $$L = E \cup \bigcup_{n \geq 1}\{\varphi_0,\ldots,\varphi_{n-1} \rightarrow \psi \mid  (\forall i < n)(\varphi_i \in E) \wedge (\psi  \in E)\}.$$ By the quasi-algebraic theory of real matrix rings, let us mean the set $T$ of all $q \in L$ such that for every natural number $n \geq 1$, every subring of the matrix ring $M_n(\mathbb{R})$ is a model for $q$. Actually, since the formulae of interest are downward-absolute, there is no need to consider subrings at all; in particular, $T$ can be described as the set of all $q \in L$ such that for every natural number $n \geq 1$, the matrix ring $M_n(\mathbb{R})$ is a model for $q$. For example, the formula $xy=1 \rightarrow yx=1$ is an element of $T$. Question. Does there exist a finite axiomatization of $T$? If so, an explicit list of axioms would be much appreciated.",,"['matrices', 'logic', 'ring-theory']"
66,"Solving a linear equation for a symmetric,positive matrix","Solving a linear equation for a symmetric,positive matrix",,"Given the Problem $A x = b$ for some regular matrix $A \in \mathbb{R}^{n \times n}$ and $b\in\mathbb{R}^n$. One can compute $x$ with the Cholesky factorization in $O(n^3)$. If $A$ is known to be a symmetric, positive (i.e. $A_{ij} >0$) and positive definite matrix, is it possible to solve $Ax = b$ faster than $O(n^3)$?","Given the Problem $A x = b$ for some regular matrix $A \in \mathbb{R}^{n \times n}$ and $b\in\mathbb{R}^n$. One can compute $x$ with the Cholesky factorization in $O(n^3)$. If $A$ is known to be a symmetric, positive (i.e. $A_{ij} >0$) and positive definite matrix, is it possible to solve $Ax = b$ faster than $O(n^3)$?",,['matrices']
67,When does a strictly diagonally dominant matrix have dominant principal minors?,When does a strictly diagonally dominant matrix have dominant principal minors?,,"$A$ is an $N\times N$ matrix with diagonal elements $a_{ii}=1-s_{i}$, and off diagonal elements $a_{ij}=s_{i}w_{ij}$ for $i≠j$. Assume $0≤s_{i}<1/2$ and $\sum_{j≠i}w_{ij}=1$ for all $i$ and $0≤w_{ij}≤1$. As $1-s_{i}>∑_{j≠i}a_{ij}=s_{i}∑_{j≠i}w_{ij}$ for all $i$, matrix $A$ is strictly row diagonally dominant. Thus, $\det(A)>0$ and has positive principal minors, $M_{ii}$ where $ii$ is the submatrix from eliminating row $i$ and column $i$. (see Tsatsomeros 2002 ). There exists an inverse matrix $B=A^{-1}$. I want to show that the diagonal elements $b_{ii}=\frac{M_{ii}}{\det(A)} > 1$. I am able to show this for $N=3$ and $4$. For example, by expanding across the top gives $b_{11}>1$ as $M_{11}>-ω_{12}M_{12}+ω_{13}M_{13}-ω_{14}M_{14}$ , which follows from $M_{11}>‖M_{1j}‖_{j≠1}$. Here the results follows if the principal minor is dominant in the sense of being larger than the minors along the same row. I do not know how to extend this to $N>4$ and have not found the result in my imperfect search of the literature.","$A$ is an $N\times N$ matrix with diagonal elements $a_{ii}=1-s_{i}$, and off diagonal elements $a_{ij}=s_{i}w_{ij}$ for $i≠j$. Assume $0≤s_{i}<1/2$ and $\sum_{j≠i}w_{ij}=1$ for all $i$ and $0≤w_{ij}≤1$. As $1-s_{i}>∑_{j≠i}a_{ij}=s_{i}∑_{j≠i}w_{ij}$ for all $i$, matrix $A$ is strictly row diagonally dominant. Thus, $\det(A)>0$ and has positive principal minors, $M_{ii}$ where $ii$ is the submatrix from eliminating row $i$ and column $i$. (see Tsatsomeros 2002 ). There exists an inverse matrix $B=A^{-1}$. I want to show that the diagonal elements $b_{ii}=\frac{M_{ii}}{\det(A)} > 1$. I am able to show this for $N=3$ and $4$. For example, by expanding across the top gives $b_{11}>1$ as $M_{11}>-ω_{12}M_{12}+ω_{13}M_{13}-ω_{14}M_{14}$ , which follows from $M_{11}>‖M_{1j}‖_{j≠1}$. Here the results follows if the principal minor is dominant in the sense of being larger than the minors along the same row. I do not know how to extend this to $N>4$ and have not found the result in my imperfect search of the literature.",,['matrices']
68,What is the connection between $\rho$ and $\sigma$ if $\rho\rho^T=\sigma\sigma^T$?,What is the connection between  and  if ?,\rho \sigma \rho\rho^T=\sigma\sigma^T,"I want to prove that there exists a Borel function $R(\rho,\sigma)$ with values in $M^{d\times d}$ defined on $D=\lbrace(\rho,\sigma)\in M^{d\times d}\times M^{d\times d}\,: \rho\rho^T=\sigma\sigma^T\rbrace$ such that $\sigma=\rho R(\rho,\sigma)$ and $RR^T=I$. My idea is: Diagonalize $\rho\rho^T=\sigma\sigma^T=QDQ^T$ where Q is an orthogonal matrix. It's obvious that $\sigma=UQ\sqrt{D}$ with U orthogonal matrix satisfies the request but is this the only possibility? I would appreciate any possible help. Thank you in advance.","I want to prove that there exists a Borel function $R(\rho,\sigma)$ with values in $M^{d\times d}$ defined on $D=\lbrace(\rho,\sigma)\in M^{d\times d}\times M^{d\times d}\,: \rho\rho^T=\sigma\sigma^T\rbrace$ such that $\sigma=\rho R(\rho,\sigma)$ and $RR^T=I$. My idea is: Diagonalize $\rho\rho^T=\sigma\sigma^T=QDQ^T$ where Q is an orthogonal matrix. It's obvious that $\sigma=UQ\sqrt{D}$ with U orthogonal matrix satisfies the request but is this the only possibility? I would appreciate any possible help. Thank you in advance.",,"['linear-algebra', 'matrices', 'numerical-methods']"
69,How to find the rank of a Toeplitz matrix?,How to find the rank of a Toeplitz matrix?,,Is there any trick to compute or estimate the rank of a Toeplitz matrix? Or is this still unknown for a general Toeplitz matrix?,Is there any trick to compute or estimate the rank of a Toeplitz matrix? Or is this still unknown for a general Toeplitz matrix?,,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'matrix-rank', 'toeplitz-matrices']"
70,Expectation of the absolut value of the determinant of a random matrix,Expectation of the absolut value of the determinant of a random matrix,,Let $A$ be a random matrix of size $m\times m$ with integer entries $-n\ldots n$. Each value should have the same  probability. What is the expectation of the random variable $$X := |\det A|$$ Can the variance of $X$ also be calculated ?,Let $A$ be a random matrix of size $m\times m$ with integer entries $-n\ldots n$. Each value should have the same  probability. What is the expectation of the random variable $$X := |\det A|$$ Can the variance of $X$ also be calculated ?,,"['matrices', 'statistics', 'determinant']"
71,Probability that a random integer matrix is singular,Probability that a random integer matrix is singular,,"Let $A$ be a $n\times n$-matrix with integers in the range $u..v$ , where $u<v$ are arbitary   integers. Is there a formula, or at least, a good estimate, for the probability  that the matrix is singular? In the case $u=v$, the answer is obviously $1$. It is intuitively clear, that the probability very quickly tends to $0$, if the range  increases. As I tried, the probability that a $4\times4$-matrix with integers from $-100..100$  is singular is very small indeed. Of course, brute force is hopeless, a simulation  would give crude approximations. I prefer suitable bounds.","Let $A$ be a $n\times n$-matrix with integers in the range $u..v$ , where $u<v$ are arbitary   integers. Is there a formula, or at least, a good estimate, for the probability  that the matrix is singular? In the case $u=v$, the answer is obviously $1$. It is intuitively clear, that the probability very quickly tends to $0$, if the range  increases. As I tried, the probability that a $4\times4$-matrix with integers from $-100..100$  is singular is very small indeed. Of course, brute force is hopeless, a simulation  would give crude approximations. I prefer suitable bounds.",,"['matrices', 'determinant']"
72,Sum of Nonnegative Matrix and Diagonal Matrix,Sum of Nonnegative Matrix and Diagonal Matrix,,"Setup: Let $D = D^T > 0$ be a positive definite and diagonal $n\times n$ matrix, and let $A = A^T \in \mathbb{R}^{n\times n}$ be nonnegative with zero diagonals. That is, $a_{ij} \geq 0$ for $i\neq j$ and $a_{ii} = 0$. In my particular case, $A$ is an adjacency matrix of an undirected graph. Question : Give necessary and sufficient conditions under which $D - A$ positive definite. Failing that, what's a necessary condition so I can understand the possible gap between necessity and sufficiency. Partial Answer : A sufficient, but I believe not necessary, condition is that $\min_i D_{ii} > \rho(A) = \lambda_{\rm max}(A)$, where the last equality follows from the Perron theorem. From the Disc Theorem, another sufficient condition is that $D_{ii} \geq \sum_{j=1}^n a_{ij}$ with strict inequality in at least one row. Happy Thinking, -John","Setup: Let $D = D^T > 0$ be a positive definite and diagonal $n\times n$ matrix, and let $A = A^T \in \mathbb{R}^{n\times n}$ be nonnegative with zero diagonals. That is, $a_{ij} \geq 0$ for $i\neq j$ and $a_{ii} = 0$. In my particular case, $A$ is an adjacency matrix of an undirected graph. Question : Give necessary and sufficient conditions under which $D - A$ positive definite. Failing that, what's a necessary condition so I can understand the possible gap between necessity and sufficiency. Partial Answer : A sufficient, but I believe not necessary, condition is that $\min_i D_{ii} > \rho(A) = \lambda_{\rm max}(A)$, where the last equality follows from the Perron theorem. From the Disc Theorem, another sufficient condition is that $D_{ii} \geq \sum_{j=1}^n a_{ij}$ with strict inequality in at least one row. Happy Thinking, -John",,"['linear-algebra', 'matrices', 'algebraic-graph-theory', 'matrix-decomposition']"
73,"What is the maximal torus in the Lorentz group $O(m,n)$?",What is the maximal torus in the Lorentz group ?,"O(m,n)","I'm close to certain it's just the product of the maximal tori of $O(m)$ and $O(n)$, but I can't quite prove it. I've tried the following: $\newcommand{\mattwo}[4]{\left(\begin{array}{cc}#1&#2\\#3&#4\end{array}\right)}$ Any element of the Lorentz group can be decomposed into $$\mattwo{P}{0}{0}{Q}\exp\mattwo{0}{X}{X^T}{0}$$ Where $P, Q$ are orthogonal block matrices, $P\in O(m)$, $Q\in O(n)$. So we can definitely pick maximal tori in $P$ and $Q$ (call them $T_1, T_2$), and $T_1\times T_2$ will definitely be a torus in $O(m,n)$. To prove it's maximal, assume not. I try to find a contradiction using the fact that tori must be abelian. That's where things get a little ugly. Take an arbitrary element $\mattwo{P'}{0}{0}{Q'}\exp\mattwo{0}{X}{X^T}{0}\in O(m,n)$ and supposed for the sake of contradiction that it lives in a larger torus containing $T_1\times T_2$. Then it must commute with every element in $T_1\times T_2$. This means for every $P\in T_1, Q\in T_2$, we must have $$\mattwo{P}{0}{0}{Q}\mattwo{P'}{0}{0}{Q'}\exp\mattwo{0}{X}{X^T}{0}=\mattwo{P'}{0}{0}{Q'}\exp\mattwo{0}{X}{X^T}{0}\mattwo{P}{0}{0}{Q}$$ Knowing that $\exp\mattwo{0}{X}{X^T}{0}$ is symmetric, we can say $$\exp\mattwo{0}{X}{X^T}{0}=\mattwo{X_1}{X_2}{X_2^T}{X_3}$$ Which multiplies out to $$\mattwo{PP'X_1}{PP'X_2}{QQ'X_2^T}{QQ'X_3}=\mattwo{P'X_1P}{P'X_2Q}{Q'X_2^TP}{Q'X_3Q}$$ and this gives us \begin{align*} PP'X_1&=P'X_1P\\ PP'X_2&=P'X_2Q\\ QQ'X_2^T&=Q'X_2^TP\\ QQ'X_3&=Q'X_3Q\\ \end{align*} which must hold for every $P\in T_1, Q\in T_2$. This seems like it would be a strict enough set of four equations that would force $\exp\mattwo{0}{X}{X^T}{0}=\mattwo{1}{0}{0}{1}$, which would then force $P'\in T_1$, $Q'\in T_2$, but I can't for the life of me get the four equations to imply that.","I'm close to certain it's just the product of the maximal tori of $O(m)$ and $O(n)$, but I can't quite prove it. I've tried the following: $\newcommand{\mattwo}[4]{\left(\begin{array}{cc}#1&#2\\#3&#4\end{array}\right)}$ Any element of the Lorentz group can be decomposed into $$\mattwo{P}{0}{0}{Q}\exp\mattwo{0}{X}{X^T}{0}$$ Where $P, Q$ are orthogonal block matrices, $P\in O(m)$, $Q\in O(n)$. So we can definitely pick maximal tori in $P$ and $Q$ (call them $T_1, T_2$), and $T_1\times T_2$ will definitely be a torus in $O(m,n)$. To prove it's maximal, assume not. I try to find a contradiction using the fact that tori must be abelian. That's where things get a little ugly. Take an arbitrary element $\mattwo{P'}{0}{0}{Q'}\exp\mattwo{0}{X}{X^T}{0}\in O(m,n)$ and supposed for the sake of contradiction that it lives in a larger torus containing $T_1\times T_2$. Then it must commute with every element in $T_1\times T_2$. This means for every $P\in T_1, Q\in T_2$, we must have $$\mattwo{P}{0}{0}{Q}\mattwo{P'}{0}{0}{Q'}\exp\mattwo{0}{X}{X^T}{0}=\mattwo{P'}{0}{0}{Q'}\exp\mattwo{0}{X}{X^T}{0}\mattwo{P}{0}{0}{Q}$$ Knowing that $\exp\mattwo{0}{X}{X^T}{0}$ is symmetric, we can say $$\exp\mattwo{0}{X}{X^T}{0}=\mattwo{X_1}{X_2}{X_2^T}{X_3}$$ Which multiplies out to $$\mattwo{PP'X_1}{PP'X_2}{QQ'X_2^T}{QQ'X_3}=\mattwo{P'X_1P}{P'X_2Q}{Q'X_2^TP}{Q'X_3Q}$$ and this gives us \begin{align*} PP'X_1&=P'X_1P\\ PP'X_2&=P'X_2Q\\ QQ'X_2^T&=Q'X_2^TP\\ QQ'X_3&=Q'X_3Q\\ \end{align*} which must hold for every $P\in T_1, Q\in T_2$. This seems like it would be a strict enough set of four equations that would force $\exp\mattwo{0}{X}{X^T}{0}=\mattwo{1}{0}{0}{1}$, which would then force $P'\in T_1$, $Q'\in T_2$, but I can't for the life of me get the four equations to imply that.",,"['algebraic-topology', 'group-theory', 'matrices', 'lie-groups']"
74,Can these two matrices be represented as diagonal matrices with respect to an orthonormal basis?,Can these two matrices be represented as diagonal matrices with respect to an orthonormal basis?,,"I'm having difficulty understand some questions. I will highlight the terms I do not understand. Question 1: Let $A =\begin{pmatrix}  1& -2 \\   1& 3 \end{pmatrix}$ For the matrix $A$, decide whether there is an orthonormal basis $B$ for $\mathbb{C}^2$ such that the matrix of $A: \mathbb{C}^2 \rightarrow \mathbb{C}^2$ with respect to $B$ is diagonal. If so, find such a basis and write down a unitary matrix $P$ such that $\overline{P}^TAP$ is diagonal. What I think we need to do: In class, we have learnt the following theorem - ""Spectral Theorem for normal operators"" Suppose that $V$ is an $n$-dimensional inner product space over $\mathbb{C}$ and that $T: V \rightarrow V$ is a linear map and is normal. Then there is an orthonormal basis for $V$ consisting of eigenvalues of $T$. In particular, if $A \in M_{n}(\mathbb{C})$ is normal then there is an unitary matrix $P \in M_{n}(\mathbb{C})$ such that $P^{*}AP$ is diagonal. My guess is we need to apply this theorem in some way? Now some terms I am having difficulty understanding is this sentence. ""Orthonormal basis $B$ for $\mathbb{C}^2$ such that the matrix of $A: \mathbb{C}^2 \rightarrow \mathbb{C}^2$ with respect to $B$"". I understand what an orthonormal basis is but not sure about the rest of the sentence. Question 2 (looks similar). This question is in an exam past test BUT not a continuation from the last question above. Let $A =\begin{pmatrix}  0& 1 \\   1& 0 \end{pmatrix}$ For the matrix $A$, decide whether there is an orthonormal basis $B$ for $\mathbb{R}^2$ such that the matrix of $A: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ with respect to $B$ is diagonal. If so, find such a basis and write down an orthogonal matrix $P$ such that $P^TAP$ is diagonal. So question two looks similar to 1) but we are asked for an orthogonal matrix instead of a unitary matrix. I understand what these two terms mean. Other than that I have no clue on how to start these questions. If anyone can help me get started that would be great.","I'm having difficulty understand some questions. I will highlight the terms I do not understand. Question 1: Let $A =\begin{pmatrix}  1& -2 \\   1& 3 \end{pmatrix}$ For the matrix $A$, decide whether there is an orthonormal basis $B$ for $\mathbb{C}^2$ such that the matrix of $A: \mathbb{C}^2 \rightarrow \mathbb{C}^2$ with respect to $B$ is diagonal. If so, find such a basis and write down a unitary matrix $P$ such that $\overline{P}^TAP$ is diagonal. What I think we need to do: In class, we have learnt the following theorem - ""Spectral Theorem for normal operators"" Suppose that $V$ is an $n$-dimensional inner product space over $\mathbb{C}$ and that $T: V \rightarrow V$ is a linear map and is normal. Then there is an orthonormal basis for $V$ consisting of eigenvalues of $T$. In particular, if $A \in M_{n}(\mathbb{C})$ is normal then there is an unitary matrix $P \in M_{n}(\mathbb{C})$ such that $P^{*}AP$ is diagonal. My guess is we need to apply this theorem in some way? Now some terms I am having difficulty understanding is this sentence. ""Orthonormal basis $B$ for $\mathbb{C}^2$ such that the matrix of $A: \mathbb{C}^2 \rightarrow \mathbb{C}^2$ with respect to $B$"". I understand what an orthonormal basis is but not sure about the rest of the sentence. Question 2 (looks similar). This question is in an exam past test BUT not a continuation from the last question above. Let $A =\begin{pmatrix}  0& 1 \\   1& 0 \end{pmatrix}$ For the matrix $A$, decide whether there is an orthonormal basis $B$ for $\mathbb{R}^2$ such that the matrix of $A: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ with respect to $B$ is diagonal. If so, find such a basis and write down an orthogonal matrix $P$ such that $P^TAP$ is diagonal. So question two looks similar to 1) but we are asked for an orthogonal matrix instead of a unitary matrix. I understand what these two terms mean. Other than that I have no clue on how to start these questions. If anyone can help me get started that would be great.",,"['linear-algebra', 'matrices', 'diagonalization']"
75,Determinant of triangular matrix except for one column (atomic/Gauss/Frobenius),Determinant of triangular matrix except for one column (atomic/Gauss/Frobenius),,"Is there some ""smart"" way to calculate determinants that look like this? $\begin{vmatrix}-1&a_{1,2}&a_{1,3}&a_{1,4}&\cdots&a_{1,m-1}&a_{1,m} \\-1&a_{2,2}&a_{2,3}&a_{2,4}&\cdots&a_{2,m-1}&a_{2,m} \\-1&0&a_{3,3}&a_{3,3}&\cdots&a_{3,m-1}&a_{3,m} \\-1&0&0&a_{4,4}&\cdots&a_{4,m-1}&a_{4,m}  \\-1&0&0&0&\cdots&a_{5,m-1}&a_{5,m}  \\\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots \\-1&0&0&0&0&a_{m-1,m-1}&a_{m-1,m} \\-1&0&0&0&0&0&a_{m,m} \end{vmatrix} $ It's a special case of an atomic triangular matrix / Gauss matrix / Gauss transformation matrix / Frobenius matrix.","Is there some ""smart"" way to calculate determinants that look like this? $\begin{vmatrix}-1&a_{1,2}&a_{1,3}&a_{1,4}&\cdots&a_{1,m-1}&a_{1,m} \\-1&a_{2,2}&a_{2,3}&a_{2,4}&\cdots&a_{2,m-1}&a_{2,m} \\-1&0&a_{3,3}&a_{3,3}&\cdots&a_{3,m-1}&a_{3,m} \\-1&0&0&a_{4,4}&\cdots&a_{4,m-1}&a_{4,m}  \\-1&0&0&0&\cdots&a_{5,m-1}&a_{5,m}  \\\vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots \\-1&0&0&0&0&a_{m-1,m-1}&a_{m-1,m} \\-1&0&0&0&0&0&a_{m,m} \end{vmatrix} $ It's a special case of an atomic triangular matrix / Gauss matrix / Gauss transformation matrix / Frobenius matrix.",,"['linear-algebra', 'matrices', 'determinant']"
76,tangent/normal space to set of symmetric isospectral matrices,tangent/normal space to set of symmetric isospectral matrices,,"Let $\Lambda = \{\lambda_1, \ldots, \lambda_n\}$ be a set of $n$ distinct real numbers. $M_n(\mathbb{R})$ denotes the set of all $n \times n$ real matrices, and for $B\in M_n(\mathbb{R})$, $B^T$ denotes the transpose of B, and $\sigma(B)$ is the (multi)set of the eigenvalues of $B$. I'm trying to figure out the tangent space and the normal space to the set $$S = \{ B \in M_n(\mathbb{R}) \, : \, B^T = B, \sigma(B) = \Lambda\},$$ at the point $A= \text{diag}(\lambda_1,\ldots,\lambda_n)$, the diagonal matrix with diagonal entries $\lambda_i$'s. Note that $S$ is a manifold in a neighborhood of $A$. This is my thought process:  Define a path in $S$ as follows:  $$A(t)= Q(t) \, A \, Q(t)^T,$$ for a family of orthogonal matrices $Q(t)$, such that $Q(0) = I$, the identity matrix. Then $A(0) = A$. \begin{align*}\dot{A}(t) &= \dot{Q}(t) \, A \, Q(t)\\ &+\, {Q}(t) \, \dot{A} \, Q(t)\\ &+ \,  {Q}(t) \, A \, \dot{Q}(t) \end{align*} But $\dot{A} = O$, hence $$\dot A(0) = \dot Q(0) \, A + A \, \dot Q(0)^T.$$ So, if I know all the tangents/normals to orthogonal matrices at $0$, that is, at $I$, I can describe all the tangents/normals to $S$ at $A$. But it is not too hard to show that the set of all skew-symmetric matrices (i.e. $\{B : B^T = -B\}$) is the tangent space to the orthogonal matrices at $I$. Hence, the tangent space to $S$ at $A$ is $$ T_A(S) = \{ B \, A + A \, B^T : B^T = -B \}.$$ So here are my questions: Question 0: Is my argument above correct? Question 1: Why are these all of the tangent vectors? I think(?) $S$ is an $n(n-1)/2$ dimensional manifold, and $T_A(S)$ is $n(n-1)/2$ dimensional, is that really the reason for $T_A(S)$ begin the whole tangent space? Can you elaborate on it, please? Question 2.1: What is the normal space to the set of orthogonal matrices, at $I$? Question 2.2: How do you find the normal space to $S$ at $A$? Is it the same as the following?  $$\{B \in M_n(\mathbb{R}) : C\, B = O, \forall C \in T_A\}.$$ Edit: Direct calculations yield that $T_A(S)$ is the set of all symmetric matrices with zero diagonals, as long as $\lambda_i$'s are all distinct.","Let $\Lambda = \{\lambda_1, \ldots, \lambda_n\}$ be a set of $n$ distinct real numbers. $M_n(\mathbb{R})$ denotes the set of all $n \times n$ real matrices, and for $B\in M_n(\mathbb{R})$, $B^T$ denotes the transpose of B, and $\sigma(B)$ is the (multi)set of the eigenvalues of $B$. I'm trying to figure out the tangent space and the normal space to the set $$S = \{ B \in M_n(\mathbb{R}) \, : \, B^T = B, \sigma(B) = \Lambda\},$$ at the point $A= \text{diag}(\lambda_1,\ldots,\lambda_n)$, the diagonal matrix with diagonal entries $\lambda_i$'s. Note that $S$ is a manifold in a neighborhood of $A$. This is my thought process:  Define a path in $S$ as follows:  $$A(t)= Q(t) \, A \, Q(t)^T,$$ for a family of orthogonal matrices $Q(t)$, such that $Q(0) = I$, the identity matrix. Then $A(0) = A$. \begin{align*}\dot{A}(t) &= \dot{Q}(t) \, A \, Q(t)\\ &+\, {Q}(t) \, \dot{A} \, Q(t)\\ &+ \,  {Q}(t) \, A \, \dot{Q}(t) \end{align*} But $\dot{A} = O$, hence $$\dot A(0) = \dot Q(0) \, A + A \, \dot Q(0)^T.$$ So, if I know all the tangents/normals to orthogonal matrices at $0$, that is, at $I$, I can describe all the tangents/normals to $S$ at $A$. But it is not too hard to show that the set of all skew-symmetric matrices (i.e. $\{B : B^T = -B\}$) is the tangent space to the orthogonal matrices at $I$. Hence, the tangent space to $S$ at $A$ is $$ T_A(S) = \{ B \, A + A \, B^T : B^T = -B \}.$$ So here are my questions: Question 0: Is my argument above correct? Question 1: Why are these all of the tangent vectors? I think(?) $S$ is an $n(n-1)/2$ dimensional manifold, and $T_A(S)$ is $n(n-1)/2$ dimensional, is that really the reason for $T_A(S)$ begin the whole tangent space? Can you elaborate on it, please? Question 2.1: What is the normal space to the set of orthogonal matrices, at $I$? Question 2.2: How do you find the normal space to $S$ at $A$? Is it the same as the following?  $$\{B \in M_n(\mathbb{R}) : C\, B = O, \forall C \in T_A\}.$$ Edit: Direct calculations yield that $T_A(S)$ is the set of all symmetric matrices with zero diagonals, as long as $\lambda_i$'s are all distinct.",,"['linear-algebra', 'matrices', 'differential-geometry', 'lie-groups']"
77,A basis for the column space of a real matrix,A basis for the column space of a real matrix,,"Let $A$ be a real square matrix, and let its column space be $$\mathrm{col}(A)=\{y\in\mathbb{C}^n:y=Ax\text{ for some } x\in\mathbb{C}^n\}.$$ Under what conditions is $\mathrm{col}(A)$ spanned by eigenvectors of $A$ associated to nonzero eigenvalues? Do we get the same answer if we take $\mathbb{R}$ rather than $\mathbb{C}$ as the field (so $\mathrm{col}(A)=\{y\in\mathbb{R}^n:y=Ax\text{ for some } x\in\mathbb{R}^n\}$ and we look for conditions for the real and imaginary parts of the eigenvectors of $A$ associated to nonzero eigenvalues to be in $\mathrm{col}(A)$? My attempt to answer 1 so far: Any eigenvector $v\in\mathbb{C}^n$ is in $\mathrm{col}(A)$, so $\mathrm{col}(A)$ is spanned by eigenvectors of $A$ associated to nonzero eigenvalues if there are $\mathrm{rank}(A)$ linearly independent eigenvectors eigenvectors of $A$ associated to nonzero eigenvalues. Can we say more? As for 2, I'm confused.","Let $A$ be a real square matrix, and let its column space be $$\mathrm{col}(A)=\{y\in\mathbb{C}^n:y=Ax\text{ for some } x\in\mathbb{C}^n\}.$$ Under what conditions is $\mathrm{col}(A)$ spanned by eigenvectors of $A$ associated to nonzero eigenvalues? Do we get the same answer if we take $\mathbb{R}$ rather than $\mathbb{C}$ as the field (so $\mathrm{col}(A)=\{y\in\mathbb{R}^n:y=Ax\text{ for some } x\in\mathbb{R}^n\}$ and we look for conditions for the real and imaginary parts of the eigenvectors of $A$ associated to nonzero eigenvalues to be in $\mathrm{col}(A)$? My attempt to answer 1 so far: Any eigenvector $v\in\mathbb{C}^n$ is in $\mathrm{col}(A)$, so $\mathrm{col}(A)$ is spanned by eigenvectors of $A$ associated to nonzero eigenvalues if there are $\mathrm{rank}(A)$ linearly independent eigenvectors eigenvectors of $A$ associated to nonzero eigenvalues. Can we say more? As for 2, I'm confused.",,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
78,"Does the cross section of $[-1,1]^n$ on a $k$-dimensional subspace always contains a rotated image of $[-1,1]^k$?",Does the cross section of  on a -dimensional subspace always contains a rotated image of ?,"[-1,1]^n k [-1,1]^k","This question is inspired by a recent bounty question , but the two questions are different and solving this one, I believe, will not lead to an answer of that bounty question. Suppose $n>k\ge1$ and $V\in M_{k,n}(\mathbb R)$ is the submatrix taken from the first $k$ rows of a real orthogonal matrix of size $n$. Does there always exist a real orthogonal matrix $R$ of size $k$ such that $\|Rv_j\|_1\le1$ for every $j$ (or equivalently, $\|RV\|_1\le1$ where $\|\cdot\|_1$ is the maximum column sum norm)? Numerical experiments suggest that the answer is negative in general, but affirmative when $k=2$. So, here is my main question: does the aforementioned $R$ always exist when $k=2$?","This question is inspired by a recent bounty question , but the two questions are different and solving this one, I believe, will not lead to an answer of that bounty question. Suppose $n>k\ge1$ and $V\in M_{k,n}(\mathbb R)$ is the submatrix taken from the first $k$ rows of a real orthogonal matrix of size $n$. Does there always exist a real orthogonal matrix $R$ of size $k$ such that $\|Rv_j\|_1\le1$ for every $j$ (or equivalently, $\|RV\|_1\le1$ where $\|\cdot\|_1$ is the maximum column sum norm)? Numerical experiments suggest that the answer is negative in general, but affirmative when $k=2$. So, here is my main question: does the aforementioned $R$ always exist when $k=2$?",,"['linear-algebra', 'matrices']"
79,Unitary Farey Sequence Matrices,Unitary Farey Sequence Matrices,,"Take the Farey sequence $\mathcal{F}_n$ with values $a_m\in \mathcal{F}_n$ and put them into a vector  $$ \vec v_k=\frac1{\sqrt{|\mathcal{F}_n|}}\biggr(\exp(2\pi i k a_m)\biggr)_m $$ The dimension of this vector is $|\mathcal{F}_n| = 1 + \sum_{m=1}^n \varphi(m). $ Let's call $k$ a root when $\vec v_k \cdot \vec v_0=0$. I tried for quite a while to get a set $OV$ of $v_k$, where $k$ can be $0$, positive or negative, such that the collection of vectors $\vec v_k$ creates a unitary matrix.  The largest sets I found, have six elements, e.g. $n=40$: $OV=\{0, 1,-179,180,-29748,29749 \}$. I also found that the roots for even $n$ fall into certain categories. For $n=40$ we get: $2^23^25p$, e.g. $\color{red}{2^23\cdot 5=180}$ $2^23\cdot 37^np$, e.g. $\color{blue}{2^23\cdot 37\cdot 67=29748}$ $2^33\cdot 29^np$, e.g. $\color{green}{2^33\cdot 29\cdot 43=29928}$ $2^43\cdot 29^np$ $2^33\cdot 7^k23^np$, where $n,k>0$ and $p$ may be any prime larger than $40$. Roots of odd $n$ are products of primes larger than $40$. While analyzing this special set for $n=40$, I found that at least, either the sum or the difference of any pair would give rise to another root: $\vec v_0 \cdot \vec v_{(-\color{blue}{29748}-\color{red}{180})}=\vec v_0 \cdot \vec v_{-\color{green}{29928}}=0$, whereas $\vec v_0 \cdot \vec v_{(-29748+180)}\neq 0$. $\vec v_0 \cdot \vec v_{(29749\pm 180)}=0$, where $29749\pm 180$ is either prime or a product of primes larger than $40$. There seems to be kind of ""closed under addition/subtraction""-relation among the roots. Is this an equivalence class? My questions: Are these matrices already known? How/where are they used? How to create them? What about these closedness relations?","Take the Farey sequence $\mathcal{F}_n$ with values $a_m\in \mathcal{F}_n$ and put them into a vector  $$ \vec v_k=\frac1{\sqrt{|\mathcal{F}_n|}}\biggr(\exp(2\pi i k a_m)\biggr)_m $$ The dimension of this vector is $|\mathcal{F}_n| = 1 + \sum_{m=1}^n \varphi(m). $ Let's call $k$ a root when $\vec v_k \cdot \vec v_0=0$. I tried for quite a while to get a set $OV$ of $v_k$, where $k$ can be $0$, positive or negative, such that the collection of vectors $\vec v_k$ creates a unitary matrix.  The largest sets I found, have six elements, e.g. $n=40$: $OV=\{0, 1,-179,180,-29748,29749 \}$. I also found that the roots for even $n$ fall into certain categories. For $n=40$ we get: $2^23^25p$, e.g. $\color{red}{2^23\cdot 5=180}$ $2^23\cdot 37^np$, e.g. $\color{blue}{2^23\cdot 37\cdot 67=29748}$ $2^33\cdot 29^np$, e.g. $\color{green}{2^33\cdot 29\cdot 43=29928}$ $2^43\cdot 29^np$ $2^33\cdot 7^k23^np$, where $n,k>0$ and $p$ may be any prime larger than $40$. Roots of odd $n$ are products of primes larger than $40$. While analyzing this special set for $n=40$, I found that at least, either the sum or the difference of any pair would give rise to another root: $\vec v_0 \cdot \vec v_{(-\color{blue}{29748}-\color{red}{180})}=\vec v_0 \cdot \vec v_{-\color{green}{29928}}=0$, whereas $\vec v_0 \cdot \vec v_{(-29748+180)}\neq 0$. $\vec v_0 \cdot \vec v_{(29749\pm 180)}=0$, where $29749\pm 180$ is either prime or a product of primes larger than $40$. There seems to be kind of ""closed under addition/subtraction""-relation among the roots. Is this an equivalence class? My questions: Are these matrices already known? How/where are they used? How to create them? What about these closedness relations?",,"['matrices', 'reference-request', 'equivalence-relations', 'farey-sequences']"
80,"Matrix similarity problem (complex, real)","Matrix similarity problem (complex, real)",,"I'm trying to solve this problem: Given complex matrices A and B, prove there's a nonsingular real matrix Q such that $A=QBQ^{-1}$, if and only if there's a nonsingular complex matrix S such that $A=SBS^{-1}$ and $\overline{A}=S\overline{B}S^{-1}$ The forward direction is fine since a real matrix is also complex. I'm having trouble with the backwards direction. The only clue I have is a theorem that says given 2 real matrices that are similar over complex numbers... they must also be similar over the real numbers. $A\overline{A} = SB\overline{B}S^{-1}$ does this somehow imply S is a real matrix? Also: $A = SBS^{-1}$ $A = \overline{S}B\overline{S^{-1}}$ Does A being similar to B by two different matrices somehow imply they are equal... thereby implying $S = \overline{S}$ ? Appreciate any help. Thanks. EDIT: We get: $A + \overline{A} = S(B+\overline{B})S^{-1}$ Since $A+\overline{A}$ and $B+\overline{B}$ are real, we can use the theorem to show that there's a real matrix T such that: $A + \overline{A} = T(B+\overline{B})T^{-1}$ Similarly, $i(A-\overline{A}) = Si(B-\overline{B})S^{-1}$ Since $i(A-\overline{A})$ and $i(B-\overline{B})$ are real, we can use the theorem to show that there's a real matrix G such that: $i(A - \overline{A}) = Gi(B-\overline{B})G^{-1}$ so $(A - \overline{A}) = G(B-\overline{B})G^{-1}$ Can we use real matrices G and T to somehow show $A = QBQ^{-1}$ for a real matrix Q ? Ok. I think I've found the solution now. The theorem also says that if one pair of real matrices are similar via a complex matrix S... then they're similar via a real matrix T... and if another pair of real matrices are also similar via S, they are also similar via the same real matrix T. Hence in my exposition above G = T. So we get (by adding the two equations): $2A = 2TBT^{-1}$ $A = TBT^{-1}$","I'm trying to solve this problem: Given complex matrices A and B, prove there's a nonsingular real matrix Q such that $A=QBQ^{-1}$, if and only if there's a nonsingular complex matrix S such that $A=SBS^{-1}$ and $\overline{A}=S\overline{B}S^{-1}$ The forward direction is fine since a real matrix is also complex. I'm having trouble with the backwards direction. The only clue I have is a theorem that says given 2 real matrices that are similar over complex numbers... they must also be similar over the real numbers. $A\overline{A} = SB\overline{B}S^{-1}$ does this somehow imply S is a real matrix? Also: $A = SBS^{-1}$ $A = \overline{S}B\overline{S^{-1}}$ Does A being similar to B by two different matrices somehow imply they are equal... thereby implying $S = \overline{S}$ ? Appreciate any help. Thanks. EDIT: We get: $A + \overline{A} = S(B+\overline{B})S^{-1}$ Since $A+\overline{A}$ and $B+\overline{B}$ are real, we can use the theorem to show that there's a real matrix T such that: $A + \overline{A} = T(B+\overline{B})T^{-1}$ Similarly, $i(A-\overline{A}) = Si(B-\overline{B})S^{-1}$ Since $i(A-\overline{A})$ and $i(B-\overline{B})$ are real, we can use the theorem to show that there's a real matrix G such that: $i(A - \overline{A}) = Gi(B-\overline{B})G^{-1}$ so $(A - \overline{A}) = G(B-\overline{B})G^{-1}$ Can we use real matrices G and T to somehow show $A = QBQ^{-1}$ for a real matrix Q ? Ok. I think I've found the solution now. The theorem also says that if one pair of real matrices are similar via a complex matrix S... then they're similar via a real matrix T... and if another pair of real matrices are also similar via S, they are also similar via the same real matrix T. Hence in my exposition above G = T. So we get (by adding the two equations): $2A = 2TBT^{-1}$ $A = TBT^{-1}$",,['matrices']
81,Is it possible to triangularize a matrix only by adding scalar multiples of rows to each other?,Is it possible to triangularize a matrix only by adding scalar multiples of rows to each other?,,"I am working on showing if $B$ is a $s \times s$ matrix, $D$ is a $t \times t$  matrix, $C$ is a $s \times t$ matrix, and $0$ is a $t \times s$ zero matrix, then $\det(A)=\det(B)\det(D)$, where $$A = \begin{bmatrix}B &C\\ 0& D\end{bmatrix}$$ I understand two more complicated proofs, but I figured out if I could triangularize a matrix by just adding scalar multiples of row i to row j (in that sense my determinant will not change), then I can make to proof much more easier. However, is that generally true? After put in some thought, I realized I can use all elementary row operations in my proof. It will not affect the proof. A scratch of my proof will be I am trying to triangularize B and D and A as well, then because the determinant is just the product of diagonal element I can show $\det(A)=\det(B)\det(D)$. If I used row swaps and multiply a row by a scalar, I will have shown $K\det(A)=K\det(B)\det(D)$, where K is some constant due to the row operations. This then gives the result I want. So, the question no longer have anything to do with the proof now. But still, is it possible?","I am working on showing if $B$ is a $s \times s$ matrix, $D$ is a $t \times t$  matrix, $C$ is a $s \times t$ matrix, and $0$ is a $t \times s$ zero matrix, then $\det(A)=\det(B)\det(D)$, where $$A = \begin{bmatrix}B &C\\ 0& D\end{bmatrix}$$ I understand two more complicated proofs, but I figured out if I could triangularize a matrix by just adding scalar multiples of row i to row j (in that sense my determinant will not change), then I can make to proof much more easier. However, is that generally true? After put in some thought, I realized I can use all elementary row operations in my proof. It will not affect the proof. A scratch of my proof will be I am trying to triangularize B and D and A as well, then because the determinant is just the product of diagonal element I can show $\det(A)=\det(B)\det(D)$. If I used row swaps and multiply a row by a scalar, I will have shown $K\det(A)=K\det(B)\det(D)$, where K is some constant due to the row operations. This then gives the result I want. So, the question no longer have anything to do with the proof now. But still, is it possible?",,"['linear-algebra', 'matrices']"
82,Symmetric non-degenerate bilinear forms over $\mathbb{Z}$ and $\mathbb{Q}$,Symmetric non-degenerate bilinear forms over  and,\mathbb{Z} \mathbb{Q},"Consider the four non-degenerate symmetric bilinear forms over $\mathbb{Q}$ given be the matrices $\bigl(\begin{smallmatrix} 1&0\\ 0&1 \end{smallmatrix} \bigr)$,$\bigl(\begin{smallmatrix} 1&0\\ 0&-1 \end{smallmatrix} \bigr)$,$\bigl(\begin{smallmatrix} -1&0\\ 0&1 \end{smallmatrix} \bigr)$ and $\bigl(\begin{smallmatrix} -1&0\\ 0&-1 \end{smallmatrix} \bigr)$. Considered over $\mathbb{Z}$ these matrices become unimodular symmetric bilinear forms. I want to classify all unimodular symmetric bilinear forms over $\mathbb{Z}$, which are, considered over $\mathbb{Q}$ equivalent to one of the given forms above. Can somebody help me?","Consider the four non-degenerate symmetric bilinear forms over $\mathbb{Q}$ given be the matrices $\bigl(\begin{smallmatrix} 1&0\\ 0&1 \end{smallmatrix} \bigr)$,$\bigl(\begin{smallmatrix} 1&0\\ 0&-1 \end{smallmatrix} \bigr)$,$\bigl(\begin{smallmatrix} -1&0\\ 0&1 \end{smallmatrix} \bigr)$ and $\bigl(\begin{smallmatrix} -1&0\\ 0&-1 \end{smallmatrix} \bigr)$. Considered over $\mathbb{Z}$ these matrices become unimodular symmetric bilinear forms. I want to classify all unimodular symmetric bilinear forms over $\mathbb{Z}$, which are, considered over $\mathbb{Q}$ equivalent to one of the given forms above. Can somebody help me?",,"['linear-algebra', 'abstract-algebra', 'matrices', 'modules', 'bilinear-form']"
83,zariski closure of a subset of determinantal variety,zariski closure of a subset of determinantal variety,,"I am trying to compute the Zariski closure of the following set of $n \times n$ matrices (over $\mathbb{C}$). \begin{equation} \mathcal{A}=\{X\in\mathcal{M}_{n,n}: X^*=X,~ \text{rank}(X)=1, Tr(X)=1\}. \end{equation} In other words, these are all rank $1$ Hermitian operators. (The last condition is perhaps not required in actual computation.) This is a subset of determinantal varieties of rank $1$ matrices. My guess is, this will be  the Zariski closure of $\mathcal{A}$. However I may be wrong (my Algebraic geometry knowledge is very basis and not proper and this is not my area of work). Advanced thanks for any help or suggestion.","I am trying to compute the Zariski closure of the following set of $n \times n$ matrices (over $\mathbb{C}$). \begin{equation} \mathcal{A}=\{X\in\mathcal{M}_{n,n}: X^*=X,~ \text{rank}(X)=1, Tr(X)=1\}. \end{equation} In other words, these are all rank $1$ Hermitian operators. (The last condition is perhaps not required in actual computation.) This is a subset of determinantal varieties of rank $1$ matrices. My guess is, this will be  the Zariski closure of $\mathcal{A}$. However I may be wrong (my Algebraic geometry knowledge is very basis and not proper and this is not my area of work). Advanced thanks for any help or suggestion.",,"['matrices', 'algebraic-geometry']"
84,Texture mapping from a camera image (knowing the camera pose),Texture mapping from a camera image (knowing the camera pose),,"I'm not sure if I should ask this question here or on stackoverflow, so forgive me if I'm wrong. I want to apply a texture (taken from a camera) on a 3D surface, let me explain my problem: I have acquire a 3D surface through a Kinect camera. At some intervals I have saved the camera position (A rotation|translation matrix) as well as a 2D image of what the camera is looking at. Now, what I'm trying to do is to find which 3D points (X,Y,Z) correspond to the 2D point (u,v) in my image, if I know that, I can apply the color of my 2D point on the corresponding 3D point. I guess I have to start from this formula to solve this issue ? (taken from here ) The first matrix represents the intrinsic parameters of my camera while the second matrix represents my Rotation|Translation matrix. Both matrices are known. Here is a picture representing what I'm trying to achieve (sorry for my poor drawing skills) As you can see, the 3D surface is represented with triangles, each point of the triangle is stored in a .obj file, so I can also have access to them. From the formula, I just have to take the 3D point interesting me, apply the formula and find the corresponding 2D point. However, I don't have any ideas of how to find the 3D points that are in my camera range. If I take a point that is not in my camera range, the corresponding 2D point would not make any sense and so the texture mapping will be wrong. Any idea of how I can solve this issue ? Thanks !","I'm not sure if I should ask this question here or on stackoverflow, so forgive me if I'm wrong. I want to apply a texture (taken from a camera) on a 3D surface, let me explain my problem: I have acquire a 3D surface through a Kinect camera. At some intervals I have saved the camera position (A rotation|translation matrix) as well as a 2D image of what the camera is looking at. Now, what I'm trying to do is to find which 3D points (X,Y,Z) correspond to the 2D point (u,v) in my image, if I know that, I can apply the color of my 2D point on the corresponding 3D point. I guess I have to start from this formula to solve this issue ? (taken from here ) The first matrix represents the intrinsic parameters of my camera while the second matrix represents my Rotation|Translation matrix. Both matrices are known. Here is a picture representing what I'm trying to achieve (sorry for my poor drawing skills) As you can see, the 3D surface is represented with triangles, each point of the triangle is stored in a .obj file, so I can also have access to them. From the formula, I just have to take the 3D point interesting me, apply the formula and find the corresponding 2D point. However, I don't have any ideas of how to find the 3D points that are in my camera range. If I take a point that is not in my camera range, the corresponding 2D point would not make any sense and so the texture mapping will be wrong. Any idea of how I can solve this issue ? Thanks !",,"['matrices', 'computer-science', '3d', 'image-processing']"
85,Inverse of identity plus scalar multiple of matrix,Inverse of identity plus scalar multiple of matrix,,"Given the matrix $M = ( I + \alpha D P )$, where $I$ is the nxn identity, $D$ is nxn symmetric and invertible, $P$ is nxn symmetric but not always invertible, and $\alpha$ is a scalar, is there a way to updatae $M^{-1}$ when $\alpha$ changes without performing a new matrix inverse calculation? I need to calculate this inverse many times, so I'm trying to find a formula that doesn't require a new matrix inverse every time $\alpha$ changes. I've tried manipulations with the Woodbury identity, but they all include a matrix inverse that's a function of $\alpha$. Let me edit this to backup a step and add some more information: I'm solving for $s$ in this equation $s=s^{tr}-\alpha D P s$, where $s$ is a $n \times 1 $ vector and $s^{tr}$ is a constant $n \times 1 $ vector. Then the solution is then $s=(I+\alpha D P)^{-1}s^{tr}$. This is part of an iterative solution to solve for $s$ and $\alpha$, where $\alpha$ depends on $\| s\|$.","Given the matrix $M = ( I + \alpha D P )$, where $I$ is the nxn identity, $D$ is nxn symmetric and invertible, $P$ is nxn symmetric but not always invertible, and $\alpha$ is a scalar, is there a way to updatae $M^{-1}$ when $\alpha$ changes without performing a new matrix inverse calculation? I need to calculate this inverse many times, so I'm trying to find a formula that doesn't require a new matrix inverse every time $\alpha$ changes. I've tried manipulations with the Woodbury identity, but they all include a matrix inverse that's a function of $\alpha$. Let me edit this to backup a step and add some more information: I'm solving for $s$ in this equation $s=s^{tr}-\alpha D P s$, where $s$ is a $n \times 1 $ vector and $s^{tr}$ is a constant $n \times 1 $ vector. Then the solution is then $s=(I+\alpha D P)^{-1}s^{tr}$. This is part of an iterative solution to solve for $s$ and $\alpha$, where $\alpha$ depends on $\| s\|$.",,"['matrices', 'inverse']"
86,Proving that a matrix and its transpose are similar,Proving that a matrix and its transpose are similar,,"This is a well-known fact, several proofs have been outlined on Math.Se but they all rely on arguments involving Jordan normal form or modules,  which are beyond my reach. Does anybody know of a more elementary proof?","This is a well-known fact, several proofs have been outlined on Math.Se but they all rely on arguments involving Jordan normal form or modules,  which are beyond my reach. Does anybody know of a more elementary proof?",,['matrices']
87,Trick to diagonalize symmetric matrices?,Trick to diagonalize symmetric matrices?,,I will write an exam on Quantum Mechanics soon. I was wondering whether there is any smart and fast way to determine the eigenvalues/eigenvectors of a symmetric 3x3 matrix other than by calculating the characteristic polynomial? So I am only intersted in fast techniques that one can use by hand to get those things.,I will write an exam on Quantum Mechanics soon. I was wondering whether there is any smart and fast way to determine the eigenvalues/eigenvectors of a symmetric 3x3 matrix other than by calculating the characteristic polynomial? So I am only intersted in fast techniques that one can use by hand to get those things.,,['linear-algebra']
88,"Non Existence of matrices $A,B\in M_n(\mathbb{R})$ such that $(I-(AB-BA))^n=0$",Non Existence of matrices  such that,"A,B\in M_n(\mathbb{R}) (I-(AB-BA))^n=0","Question is to Prove: Non Existence of matrices $A,B\in M_n(\mathbb{R})$ such that $(I-(AB-BA))^n=0$. This question has already been asked already but then i am asking for clarification of another solution i have tried on my own. First of all $I-(AB-BA)=0$ does not have solution because of trace property. Now, I Would let $C=AB-BA$ and consider $(C-I)^n=0$ Now, Minimal polynomial of $C$ would be $(x-1)^m$ for some $m\leq n$ and $m\neq 1$ so I would consider jordan form for this... I would end up with the case that jordan form of $C$ (some of would be jordan blocks with entries in the diagonal $1$ and rest of the rows would just be )would be $$\begin{bmatrix}1&1&0&0&\cdots& 0\\ 0&1&1&0&\cdots& 0\\ 0&0&1&1&\cdots& 0\\ 0&0&0&1&\cdots& 0\\ -&-&-&-&-&-\\0&0&0&0&\cdots & 1\end{bmatrix}\\$$ So, Trace of Jordan form of $C$ would be $n$. thus for the same reason as why $AB-BA=I$ have no solution we now conlcude that : $(I-(AB-BA))^n=0$ is not satisfied by any matrices. I have used that Trace of $A$ would be same as Trace of $CAC^{-1}$ for any $C$. Not very sure if i can use this.. I would be thankful if some one can confirm if what i have done is sufficient/correct. Thank you.","Question is to Prove: Non Existence of matrices $A,B\in M_n(\mathbb{R})$ such that $(I-(AB-BA))^n=0$. This question has already been asked already but then i am asking for clarification of another solution i have tried on my own. First of all $I-(AB-BA)=0$ does not have solution because of trace property. Now, I Would let $C=AB-BA$ and consider $(C-I)^n=0$ Now, Minimal polynomial of $C$ would be $(x-1)^m$ for some $m\leq n$ and $m\neq 1$ so I would consider jordan form for this... I would end up with the case that jordan form of $C$ (some of would be jordan blocks with entries in the diagonal $1$ and rest of the rows would just be )would be $$\begin{bmatrix}1&1&0&0&\cdots& 0\\ 0&1&1&0&\cdots& 0\\ 0&0&1&1&\cdots& 0\\ 0&0&0&1&\cdots& 0\\ -&-&-&-&-&-\\0&0&0&0&\cdots & 1\end{bmatrix}\\$$ So, Trace of Jordan form of $C$ would be $n$. thus for the same reason as why $AB-BA=I$ have no solution we now conlcude that : $(I-(AB-BA))^n=0$ is not satisfied by any matrices. I have used that Trace of $A$ would be same as Trace of $CAC^{-1}$ for any $C$. Not very sure if i can use this.. I would be thankful if some one can confirm if what i have done is sufficient/correct. Thank you.",,['linear-algebra']
89,Understanding higher order SVD,Understanding higher order SVD,,"Can someone explain the singular value decomposition of a tensor (maybe a 3 dimensional matrix) with an example? It is intuitively difficult to the get the meaning from just the formulas. On a related note, consider the following scenario. There are n cars on the road and say the distance traveled by each of them is determined only by mood of the driver(let's think that higher the value the more he drives) and number of people in the car(which is independent to the mood of the driver). If I were to model this as a 3 dimensional(distance traveled by the car, mood, number of people in car being the 3 dimensions) matrix and with some sufficient data points, can I get the distance traveled by a specific car given a mood and the number of people in the car(of course the data point is missing for this)? The real use case I have probably has more dimensions(add hour of the day as a 4th dimension). My problem is to exactly understand the process of decomposing the tensor to a singular matrix and  also, given other (n-1) points, how do I get the distance traveled by the car? Probably there are better approaches to solve this example, but I am specifically looking for this approach to understand higher order SVD.","Can someone explain the singular value decomposition of a tensor (maybe a 3 dimensional matrix) with an example? It is intuitively difficult to the get the meaning from just the formulas. On a related note, consider the following scenario. There are n cars on the road and say the distance traveled by each of them is determined only by mood of the driver(let's think that higher the value the more he drives) and number of people in the car(which is independent to the mood of the driver). If I were to model this as a 3 dimensional(distance traveled by the car, mood, number of people in car being the 3 dimensions) matrix and with some sufficient data points, can I get the distance traveled by a specific car given a mood and the number of people in the car(of course the data point is missing for this)? The real use case I have probably has more dimensions(add hour of the day as a 4th dimension). My problem is to exactly understand the process of decomposing the tensor to a singular matrix and  also, given other (n-1) points, how do I get the distance traveled by the car? Probably there are better approaches to solve this example, but I am specifically looking for this approach to understand higher order SVD.",,"['linear-algebra', 'matrices', 'numerical-linear-algebra', 'multilinear-algebra', 'svd']"
90,Special matrices with determinant 0,Special matrices with determinant 0,,"Define a quadratic matrix A with n rows and n columns by filling it with consecutive primes, starting with some prime p. The object is, to find the least starting prime p, such that A has determinant 0 , if there is one. For n=3 , PARI yields [2213 2221 2237] [2239 2243 2251] [2267 2269 2273] Is there a solution for each n>2 ? Solutions I found so far : 3  2213 4  73 5  659 6  753937 I am currently searching for the case n=7.","Define a quadratic matrix A with n rows and n columns by filling it with consecutive primes, starting with some prime p. The object is, to find the least starting prime p, such that A has determinant 0 , if there is one. For n=3 , PARI yields [2213 2221 2237] [2239 2243 2251] [2267 2269 2273] Is there a solution for each n>2 ? Solutions I found so far : 3  2213 4  73 5  659 6  753937 I am currently searching for the case n=7.",,"['matrices', 'prime-numbers', 'determinant']"
91,Getting a positive semi-definite matrix from two positive definite matrices,Getting a positive semi-definite matrix from two positive definite matrices,,"Suppose I have two positive-definite Hermitian matrices $A$ and $B$. Their eigenvalues are strictly positive reals. Consider the matrices $A-tB$ for $0 \le t < \infty$. My goal is to conclude that there is some smallest $t$ such that $A-tB$ has zero as an eigenvalue, and all other eigenvalues non-negative. How do I show this (and is this even true)? I know some results about continuity (the eigenvalues of a convergent sequence of matrices also converge), but am not sure if such a $t$ exists: $$t^* := \inf_{0 \le t < \infty} \{t :\text{ $A-tB$ has $0$ as an eigenvalue}\} \implies A-t^* B \text{ is positive semi-definite}$$","Suppose I have two positive-definite Hermitian matrices $A$ and $B$. Their eigenvalues are strictly positive reals. Consider the matrices $A-tB$ for $0 \le t < \infty$. My goal is to conclude that there is some smallest $t$ such that $A-tB$ has zero as an eigenvalue, and all other eigenvalues non-negative. How do I show this (and is this even true)? I know some results about continuity (the eigenvalues of a convergent sequence of matrices also converge), but am not sure if such a $t$ exists: $$t^* := \inf_{0 \le t < \infty} \{t :\text{ $A-tB$ has $0$ as an eigenvalue}\} \implies A-t^* B \text{ is positive semi-definite}$$",,"['matrices', 'continuity', 'eigenvalues-eigenvectors', 'bilinear-form']"
92,Show $r(F)=r(F^2)$ implies $Im(F) \cap Ker(F)=\{0\}$,Show  implies,r(F)=r(F^2) Im(F) \cap Ker(F)=\{0\},"I wonder if I've made some mistakes in the proof of the following or if there is some simpler solution. Problem: Let $V$ be a finite dimensional vectorspace and $F:V \rightarrow V$ a linear operator. Show that $rank(F^2)=rank(F)$ implies $Im(F) \cap Ker(F)= \{0\}$. My solution: I work with the matrix instead of the mapping, denote the matrix as $F$ too. Assume $\dim(V)=n$, and then $F$ is a square matrix of size $n$. $$Im(F)=\{FX : X \in \mathbb{K^n} \}$$ $$Ker(F)=\{X \in \mathbb{K^n} : FX=0\} $$ Assume there exist $v \in Im(F) \cap Ker(F)$ and $v \not=0$. Then $Fv=0$ and there exists $X \in \mathbb{K^n}$ such that $v=FX$. Thus, $F^2 X=0 \implies X \in Ker(F^2)$ and $X \notin Ker(F)$. We also have that $$\dim(Ker(F))=\dim(Ker(F^2))$$ since $F$ and $F^2$ have same size and rank. Thus there exist $Y \in \mathbb{K^n}$ such that $Y \in Ker(F)$ and $Y \notin Ker(F^2)$, which is a contradiction since $FY=0 \implies F^2 Y=F 0 = 0 \implies Y \in Ker(F^2)$.","I wonder if I've made some mistakes in the proof of the following or if there is some simpler solution. Problem: Let $V$ be a finite dimensional vectorspace and $F:V \rightarrow V$ a linear operator. Show that $rank(F^2)=rank(F)$ implies $Im(F) \cap Ker(F)= \{0\}$. My solution: I work with the matrix instead of the mapping, denote the matrix as $F$ too. Assume $\dim(V)=n$, and then $F$ is a square matrix of size $n$. $$Im(F)=\{FX : X \in \mathbb{K^n} \}$$ $$Ker(F)=\{X \in \mathbb{K^n} : FX=0\} $$ Assume there exist $v \in Im(F) \cap Ker(F)$ and $v \not=0$. Then $Fv=0$ and there exists $X \in \mathbb{K^n}$ such that $v=FX$. Thus, $F^2 X=0 \implies X \in Ker(F^2)$ and $X \notin Ker(F)$. We also have that $$\dim(Ker(F))=\dim(Ker(F^2))$$ since $F$ and $F^2$ have same size and rank. Thus there exist $Y \in \mathbb{K^n}$ such that $Y \in Ker(F)$ and $Y \notin Ker(F^2)$, which is a contradiction since $FY=0 \implies F^2 Y=F 0 = 0 \implies Y \in Ker(F^2)$.",,"['linear-algebra', 'matrices', 'proof-verification']"
93,Algorithm for reversion of power series?,Algorithm for reversion of power series?,,"Given a function $f(x)$ of the form: $$f(x) = x/(a_0x^0+a_1x^1+a_2x^2+a_3x^3+a_4x^4+a_5x^5+...a_nx^n)$$ Let $A$ be an arbitrary (any) infinite lower triangular matrix with ones in the diagonal: $$A = \left(\begin{array}{cccc} 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 1 & 1 & 1 & 0 \\ 1 & 1 & 1 & 1 \end{array}\right)$$ Downshift the entries in matrix $A$ one row and call it $X$: $$X=\left(\begin{array}{cccc} 0 & 0 & 0 & 0 \\ 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 1 & 1 & 1 & 0 \end{array} \right)$$ Calculate matrix powers of $X$: $X^0,X^1,X^2,X^3,X^4,X^5,...,X^n$ Then replace the entries in matrix $A$ with: $$A=a_0X^0+a_1X^1+a_2X^2+a_3X^3+...+a_nX^n$$ Repeat process until the first column in $A$ has converged. Is it then true that the entries in the first column of $A$ will be the coefficients in the power series for the reverse function of $f(x)$? Calculations suggests it is. http://pastebin.com/fsCtBUe1 https://oeis.org/transforms.html Mathematica: (*program start*) (*coefficients (coeff) in power series can be changed*) Clear[t, n, k, i, nn, x]; coeff = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,    1}; mp[m_, e_] :=   If[e == 0, IdentityMatrix@Length@m, MatrixPower[m, e]]; nn =   Length[coeff]; cc = Range[nn]*0 + 1; Monitor[  Do[Clear[t]; t[n_, 1] := t[n, 1] = cc[[n]];   t[n_, k_] :=     t[n, k] =      If[n >= k,       Sum[t[n - i, k - 1], {i, 1, 2 - 1}] -        0*Sum[t[n - i, k], {i, 1, k - 1}], 0];   A4 = Table[Table[t[n, k], {k, 1, nn}], {n, 1, nn}];   A5 = A4[[1 ;; nn - 1]]; A5 = Prepend[A5, ConstantArray[0, nn]];   cc = Total[     Table[coeff[[n]]*mp[A5, n - 1][[All, 1]], {n, 1, nn}]];, {i, 1,     nn}], i]; cc (*Mats Granvik,Jul 11 2015*) (*program end*)","Given a function $f(x)$ of the form: $$f(x) = x/(a_0x^0+a_1x^1+a_2x^2+a_3x^3+a_4x^4+a_5x^5+...a_nx^n)$$ Let $A$ be an arbitrary (any) infinite lower triangular matrix with ones in the diagonal: $$A = \left(\begin{array}{cccc} 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 1 & 1 & 1 & 0 \\ 1 & 1 & 1 & 1 \end{array}\right)$$ Downshift the entries in matrix $A$ one row and call it $X$: $$X=\left(\begin{array}{cccc} 0 & 0 & 0 & 0 \\ 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 1 & 1 & 1 & 0 \end{array} \right)$$ Calculate matrix powers of $X$: $X^0,X^1,X^2,X^3,X^4,X^5,...,X^n$ Then replace the entries in matrix $A$ with: $$A=a_0X^0+a_1X^1+a_2X^2+a_3X^3+...+a_nX^n$$ Repeat process until the first column in $A$ has converged. Is it then true that the entries in the first column of $A$ will be the coefficients in the power series for the reverse function of $f(x)$? Calculations suggests it is. http://pastebin.com/fsCtBUe1 https://oeis.org/transforms.html Mathematica: (*program start*) (*coefficients (coeff) in power series can be changed*) Clear[t, n, k, i, nn, x]; coeff = {1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,    1}; mp[m_, e_] :=   If[e == 0, IdentityMatrix@Length@m, MatrixPower[m, e]]; nn =   Length[coeff]; cc = Range[nn]*0 + 1; Monitor[  Do[Clear[t]; t[n_, 1] := t[n, 1] = cc[[n]];   t[n_, k_] :=     t[n, k] =      If[n >= k,       Sum[t[n - i, k - 1], {i, 1, 2 - 1}] -        0*Sum[t[n - i, k], {i, 1, k - 1}], 0];   A4 = Table[Table[t[n, k], {k, 1, nn}], {n, 1, nn}];   A5 = A4[[1 ;; nn - 1]]; A5 = Prepend[A5, ConstantArray[0, nn]];   cc = Total[     Table[coeff[[n]]*mp[A5, n - 1][[All, 1]], {n, 1, nn}]];, {i, 1,     nn}], i]; cc (*Mats Granvik,Jul 11 2015*) (*program end*)",,"['matrices', 'power-series', 'recursive-algorithms']"
94,How to compute the eigenvalues of a block matrix with a special structure and when the submatrices are square.,How to compute the eigenvalues of a block matrix with a special structure and when the submatrices are square.,,"I have the next block matrix $$ M = \begin{bmatrix}A & B \\ C &D\end{bmatrix}, $$ where $A, D$ are Hurwitz (eigenvalues with negative real part) square matrices of different dimensions and $B, C$ have the right dimensions. Furthermore, we have the next relations $B D^{-1} C = 0$, $BC = 0$ and $CB=0$, where $0$ stands for the appropriated zero matrix. I know that $\det(M) = \det(D) \det(A-B D^{-1} C)=\det(D) \det(A)$. Can I say something about the eigenvalues of $M$ given this special structure? I guess this is the right characteristic polynomial right? $\lambda(M) = \det(D-\lambda I)\det((A-\lambda I)-B (D-\lambda I)^{-1} C)$ . I do not know if $B (D-\lambda I)^{-1} C$ could be nicely approximated, simplified or considered as a perturbation (small) matrix? And what about the next special case, if $A, B, C, D$ are square matrices with the same dimensions, can be simplified in some way the characteristic polynomial? i.e. is true for such case that $\lambda(M) = \det(A-\lambda I)\det(D -\lambda I) - \det(B)\det(C) = \det(A-\lambda I)\det(D -\lambda I) - \det(BC) \\ =\det(A-\lambda I)\det(D -\lambda I)$","I have the next block matrix $$ M = \begin{bmatrix}A & B \\ C &D\end{bmatrix}, $$ where $A, D$ are Hurwitz (eigenvalues with negative real part) square matrices of different dimensions and $B, C$ have the right dimensions. Furthermore, we have the next relations $B D^{-1} C = 0$, $BC = 0$ and $CB=0$, where $0$ stands for the appropriated zero matrix. I know that $\det(M) = \det(D) \det(A-B D^{-1} C)=\det(D) \det(A)$. Can I say something about the eigenvalues of $M$ given this special structure? I guess this is the right characteristic polynomial right? $\lambda(M) = \det(D-\lambda I)\det((A-\lambda I)-B (D-\lambda I)^{-1} C)$ . I do not know if $B (D-\lambda I)^{-1} C$ could be nicely approximated, simplified or considered as a perturbation (small) matrix? And what about the next special case, if $A, B, C, D$ are square matrices with the same dimensions, can be simplified in some way the characteristic polynomial? i.e. is true for such case that $\lambda(M) = \det(A-\lambda I)\det(D -\lambda I) - \det(B)\det(C) = \det(A-\lambda I)\det(D -\lambda I) - \det(BC) \\ =\det(A-\lambda I)\det(D -\lambda I)$",,['linear-algebra']
95,Ultrametric matrices and their inverse,Ultrametric matrices and their inverse,,"A non-negative square matrix $A$ is ultrametric iff: $A(i,i)>\{A(i,k),A(k,i)\}\forall k,i$ $A(i,j)\geq\min\{A(i,k),A(k,j)\}\forall i,j,k$ It is well-known that the inverse of non-negative ultrametric is always diagonally dominant. Now assume that, we slightly relax the second constraint and change it to: If $A(i,j)>0$ then $A(i,j)\geq\min\{A(i,k),A(k,j)\} \forall i,j,k$ In words, the second constraint only holds for non-zero entries. Also for simplicity we can assume that the diagonal of $A$ is all $1$, and other entries are less than $1$. My question is can we say any of the following about $A^{-1}$: $A^{-1}$ is diagonally dominant. $A^{-1}(i,i) > \sum_{k\neq i, A(i,k)>0} |A^{-1}(i,k)|$ Specially I think the second one should be true. Any comments will be appreciated.","A non-negative square matrix $A$ is ultrametric iff: $A(i,i)>\{A(i,k),A(k,i)\}\forall k,i$ $A(i,j)\geq\min\{A(i,k),A(k,j)\}\forall i,j,k$ It is well-known that the inverse of non-negative ultrametric is always diagonally dominant. Now assume that, we slightly relax the second constraint and change it to: If $A(i,j)>0$ then $A(i,j)\geq\min\{A(i,k),A(k,j)\} \forall i,j,k$ In words, the second constraint only holds for non-zero entries. Also for simplicity we can assume that the diagonal of $A$ is all $1$, and other entries are less than $1$. My question is can we say any of the following about $A^{-1}$: $A^{-1}$ is diagonally dominant. $A^{-1}(i,i) > \sum_{k\neq i, A(i,k)>0} |A^{-1}(i,k)|$ Specially I think the second one should be true. Any comments will be appreciated.",,"['linear-algebra', 'matrices', 'discrete-mathematics', 'inverse']"
96,Set of 4 anticommutative matrices,Set of 4 anticommutative matrices,,"How would you go about showing that there cannot be a set of four 2 by 2 matrices that satisfy the anticommutative relation $AB + BA = 0 $ or $2I$ if $A=B$? i.e minimum order has to be 4. I know that for such matrices A, B, C and D (of order 4), $A^2 = I$, $det(A) = +1$ or $-1$. I have also tried writing out these properties using $a_{ij}$ notation and see if some unsolvable equations would come out, unsuccessfully. Just need a hint to get started, thanks.","How would you go about showing that there cannot be a set of four 2 by 2 matrices that satisfy the anticommutative relation $AB + BA = 0 $ or $2I$ if $A=B$? i.e minimum order has to be 4. I know that for such matrices A, B, C and D (of order 4), $A^2 = I$, $det(A) = +1$ or $-1$. I have also tried writing out these properties using $a_{ij}$ notation and see if some unsolvable equations would come out, unsuccessfully. Just need a hint to get started, thanks.",,"['linear-algebra', 'matrices', 'clifford-algebras']"
97,Upper bound for smallest eigenvalue,Upper bound for smallest eigenvalue,,"I am looking for a (simple) upper bound for the smallest eigenvalue of an $n\times n$ matrix, involving determinant or trace or something else that can be easily computed. I've got an upper bound from the $nth$-root of the determinant, but need a little improved bound. In the original post, I forgot to say that the matrix is symmetric and positive definite. Any ideas would be welcome, Thanks","I am looking for a (simple) upper bound for the smallest eigenvalue of an $n\times n$ matrix, involving determinant or trace or something else that can be easily computed. I've got an upper bound from the $nth$-root of the determinant, but need a little improved bound. In the original post, I forgot to say that the matrix is symmetric and positive definite. Any ideas would be welcome, Thanks",,"['linear-algebra', 'matrices']"
98,trace class norms of random matrices,trace class norms of random matrices,,"We denote by $||.||_1$ the trace class norm. on $M_n$.Let $(r_{ij})_{1 \leq i,j\leq n}$ be a family of independent identically distributed random variables which take the values $-1$ and $1$ with equal probability. Let $e_{ij}$ be the standard unit matrix of $M_n$. We define the random matrix $$ A=\sum_{i,j=1}^n r_{ij}e_{ij}. $$ I am interested by lower bounds of the quantities $$ P\big(||A||_1<\alpha\mathbb{E}||A||_1\big)\  \text{ and } \  P\big(||A||_1>\beta\mathbb{E}||A||_1\big) $$ where $\alpha, \beta>0$. I would like obtain the existence of random matrices with $||.||_1$ far from  the expectation (as far away as possible). Any comment is welcome. Remark 1: it seems to me that Szarek proved that  $$ \mathbb{E}(||A||_1)=O(n^{3/2}). $$ Remark 2: If $B$ is a matrix of $M_n$ such each entry of $B$ is 1 or -1 then  $$ ||B||_1\leq K_n $$ where $K_n$ is a constant depending on the dimension $n$.","We denote by $||.||_1$ the trace class norm. on $M_n$.Let $(r_{ij})_{1 \leq i,j\leq n}$ be a family of independent identically distributed random variables which take the values $-1$ and $1$ with equal probability. Let $e_{ij}$ be the standard unit matrix of $M_n$. We define the random matrix $$ A=\sum_{i,j=1}^n r_{ij}e_{ij}. $$ I am interested by lower bounds of the quantities $$ P\big(||A||_1<\alpha\mathbb{E}||A||_1\big)\  \text{ and } \  P\big(||A||_1>\beta\mathbb{E}||A||_1\big) $$ where $\alpha, \beta>0$. I would like obtain the existence of random matrices with $||.||_1$ far from  the expectation (as far away as possible). Any comment is welcome. Remark 1: it seems to me that Szarek proved that  $$ \mathbb{E}(||A||_1)=O(n^{3/2}). $$ Remark 2: If $B$ is a matrix of $M_n$ such each entry of $B$ is 1 or -1 then  $$ ||B||_1\leq K_n $$ where $K_n$ is a constant depending on the dimension $n$.",,"['matrices', 'probability-theory', 'random-matrices']"
99,Ideals (one-sided ideals) of $n×n$ upper triangular matrices,Ideals (one-sided ideals) of  upper triangular matrices,n×n,"Is there any characterization of ideals (one-sided ideals) of $n\times n$ upper triangular matrices? I have just seen in monthly journal about $2 \times 2$ matrices in the below article Left and Right Ideals in the Ring of $2 \times 2$ Matrices Henryk Minc The American Mathematical Monthly Vol. 71, No. 1 (Jan., 1964), pp. 72-75","Is there any characterization of ideals (one-sided ideals) of $n\times n$ upper triangular matrices? I have just seen in monthly journal about $2 \times 2$ matrices in the below article Left and Right Ideals in the Ring of $2 \times 2$ Matrices Henryk Minc The American Mathematical Monthly Vol. 71, No. 1 (Jan., 1964), pp. 72-75",,"['linear-algebra', 'matrices', 'ideals']"
