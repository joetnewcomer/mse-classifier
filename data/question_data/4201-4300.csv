,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,More identities of the Ramanujan Double factorial type.,More identities of the Ramanujan Double factorial type.,,Ramanujan discovered the following identity $$x=\sum_{n=0}^\infty (-1)^n \left[\dfrac{(2n-1)!!}{(2n)!!}\right]^3=\left[\dfrac{\Gamma\left(\frac98\right)}{\Gamma\left(\frac54\right)\Gamma\left(\frac78\right)}\right]^{2}$$ which contained the double factorials . Are there any further identities of this type ? or probably its generalization .,Ramanujan discovered the following identity $$x=\sum_{n=0}^\infty (-1)^n \left[\dfrac{(2n-1)!!}{(2n)!!}\right]^3=\left[\dfrac{\Gamma\left(\frac98\right)}{\Gamma\left(\frac54\right)\Gamma\left(\frac78\right)}\right]^{2}$$ which contained the double factorials . Are there any further identities of this type ? or probably its generalization .,,"['real-analysis', 'number-theory', 'factorial', 'gamma-function']"
1,How to find all polynomials P(x) such that $P(x^2-2)=P(x)^2 -2$?,How to find all polynomials P(x) such that ?,P(x^2-2)=P(x)^2 -2,"I am trying the fallowing exercise : Solve $P(X^2 -2)=P(X)^2 -2$ with P a monic polynomial (non-constant) My attempt : Let P satisfying $P(X^2-2) = (P(X))^2-2$ Then $Q(X)=P(X^2-2) = (P(X))^2-2$ Therefore,  $$Q(X^2-2) = (P(X^2-2))^2-2 = (P(X)^2-2)^2-2 = Q^2-2$$ As X is a solution, by defining the sequence: $(P_n)_{n \geq 1}$ with $P_1 = X$ and for all $n \geq 1, P_{n+1} = P_n^2-2$ We obtain a sequence of polynomials which are solutions. But I don't know how to prove it's the only one. If someone have an idea to prove it or an another method to solve the problem ? Thank you in advance for your time.","I am trying the fallowing exercise : Solve $P(X^2 -2)=P(X)^2 -2$ with P a monic polynomial (non-constant) My attempt : Let P satisfying $P(X^2-2) = (P(X))^2-2$ Then $Q(X)=P(X^2-2) = (P(X))^2-2$ Therefore,  $$Q(X^2-2) = (P(X^2-2))^2-2 = (P(X)^2-2)^2-2 = Q^2-2$$ As X is a solution, by defining the sequence: $(P_n)_{n \geq 1}$ with $P_1 = X$ and for all $n \geq 1, P_{n+1} = P_n^2-2$ We obtain a sequence of polynomials which are solutions. But I don't know how to prove it's the only one. If someone have an idea to prove it or an another method to solve the problem ? Thank you in advance for your time.",,"['calculus', 'real-analysis']"
2,"If $\{x_{2m}\}$ and $\{x_{2m-1}\}$ converge to the same limit, does $\{x_m\}$ converge?","If  and  converge to the same limit, does  converge?",\{x_{2m}\} \{x_{2m-1}\} \{x_m\},"Given a sequence $x_1<x_2<x_3<x_4<\ldots,$ I found both subsequences $\{x_{2m}\}$ and $\{x_{2m-1}\}$ converge to $\lambda^*$. In this case, can I say $\{x_m\}$ also converge to $\lambda^*$? (i.e. a sequence $\{x_m\}$ is divided into two subsequences $\{x_{2m}\}$ and $\{x_{2m-1}\}$) If so, if the rate of convergence is superlinear for both subsequences $\{x_{2m}\}$ and $\{x_{2m-1}\}$, can I also say that $\{ x_m\}$ converge superlinearly? What is more curious to me is the case where two subsequnces converge at different rates of convergence. For example, if $\{x_{2m}\}$ converges linearly to $\lambda^*$ and $\{x_{2m-1}\}$ converges superlinearly to $\lambda^*$, what is the rate of convergence of $\{x_m\}$?","Given a sequence $x_1<x_2<x_3<x_4<\ldots,$ I found both subsequences $\{x_{2m}\}$ and $\{x_{2m-1}\}$ converge to $\lambda^*$. In this case, can I say $\{x_m\}$ also converge to $\lambda^*$? (i.e. a sequence $\{x_m\}$ is divided into two subsequences $\{x_{2m}\}$ and $\{x_{2m-1}\}$) If so, if the rate of convergence is superlinear for both subsequences $\{x_{2m}\}$ and $\{x_{2m-1}\}$, can I also say that $\{ x_m\}$ converge superlinearly? What is more curious to me is the case where two subsequnces converge at different rates of convergence. For example, if $\{x_{2m}\}$ converges linearly to $\lambda^*$ and $\{x_{2m-1}\}$ converges superlinearly to $\lambda^*$, what is the rate of convergence of $\{x_m\}$?",,"['real-analysis', 'sequences-and-series']"
3,"Suppose $f:[0,1] \Rightarrow \mathbb{R}$ is continuous and $\int_0^x f(x)dx = \int_x^1 f(x)dx$. Prove that $f(x) = 0$ for all $x$",Suppose  is continuous and . Prove that  for all,"f:[0,1] \Rightarrow \mathbb{R} \int_0^x f(x)dx = \int_x^1 f(x)dx f(x) = 0 x","Suppose $f:[0,1] \Rightarrow \mathbb{R}$ is continuous and $\int_0^x f(x)dx = \int_x^1 f(x)dx$. Prove that $f(x) = 0$ for all $x$. So, I can intuitively see that this is true. My proof mostly makes sense, I think, but I'm not sure if it covers the case where there are negative and positive values in each segment, resulting in a mean value of 0, but still having nonzero values. Can someone tell me how to cover that, or how this does? Suppose $f(x) \neq 0$ for all x $\in$ [0,1]. By the mean value theorem, there exist some c and d for which $f(c)(x) = f(d)(1-x) = \int_0^x f(x)dx$. Since x $\neq$ (1-x) for all x $\in$ [0,1], it follows that $f(c) = f(d) = 0$. Therefore, $f(x) = 0$ for all $x \in [0,1]$.","Suppose $f:[0,1] \Rightarrow \mathbb{R}$ is continuous and $\int_0^x f(x)dx = \int_x^1 f(x)dx$. Prove that $f(x) = 0$ for all $x$. So, I can intuitively see that this is true. My proof mostly makes sense, I think, but I'm not sure if it covers the case where there are negative and positive values in each segment, resulting in a mean value of 0, but still having nonzero values. Can someone tell me how to cover that, or how this does? Suppose $f(x) \neq 0$ for all x $\in$ [0,1]. By the mean value theorem, there exist some c and d for which $f(c)(x) = f(d)(1-x) = \int_0^x f(x)dx$. Since x $\neq$ (1-x) for all x $\in$ [0,1], it follows that $f(c) = f(d) = 0$. Therefore, $f(x) = 0$ for all $x \in [0,1]$.",,"['real-analysis', 'proof-writing']"
4,Quotient of two smooth functions is smooth,Quotient of two smooth functions is smooth,,"Let $f:\mathbb R\to \mathbb R$ be a $C^\infty$-smooth function. Suppose that   $f^{(k)}(0)=0$ for $k=0,\dots,n-1$. Prove that the function $g(x)=f(x)/x^n$ extends to a $C^\infty$-smooth function on $\mathbb R$. Comment: by l'Hôpital's rule, $g$ has a finite limit at $0$, namely $f^{(n)}(0)/n!$. So, it extends to a continuous function on $\mathbb R$. However, I do not see any elementary way to show that $g$ is $C^\infty$-smooth. (One could chop up the  Fourier transform of $g$ and thus reduce the problem to analytic functions, as one does in the proof of the Malgrange preparation theorem. But this looks like an overkill.) Related posts: The quotient of two functions How to show a function is a test function? Zeros of $C^\infty$ functions (which is the special case of the above, with accepted  answer that does not contain a   proof).","Let $f:\mathbb R\to \mathbb R$ be a $C^\infty$-smooth function. Suppose that   $f^{(k)}(0)=0$ for $k=0,\dots,n-1$. Prove that the function $g(x)=f(x)/x^n$ extends to a $C^\infty$-smooth function on $\mathbb R$. Comment: by l'Hôpital's rule, $g$ has a finite limit at $0$, namely $f^{(n)}(0)/n!$. So, it extends to a continuous function on $\mathbb R$. However, I do not see any elementary way to show that $g$ is $C^\infty$-smooth. (One could chop up the  Fourier transform of $g$ and thus reduce the problem to analytic functions, as one does in the proof of the Malgrange preparation theorem. But this looks like an overkill.) Related posts: The quotient of two functions How to show a function is a test function? Zeros of $C^\infty$ functions (which is the special case of the above, with accepted  answer that does not contain a   proof).",,"['real-analysis', 'derivatives']"
5,How to prove $\inf(S)=-\sup(-S)$?,How to prove ?,\inf(S)=-\sup(-S),Let $S\subset \mathbb{R}$ which is bounded below. Let $x\in S$ such   that $-x\in -S$. Prove $\inf(S)=-\sup(-S).$ By definition a lower bound on $S\subset \mathbb{R}$ is a number $a\in \mathbb{R}$ such that $s\ge a$ for each $s\in S$. Since $S$ is non-empty and is bounded below then there exists an infimum. How can I continue this proof?,Let $S\subset \mathbb{R}$ which is bounded below. Let $x\in S$ such   that $-x\in -S$. Prove $\inf(S)=-\sup(-S).$ By definition a lower bound on $S\subset \mathbb{R}$ is a number $a\in \mathbb{R}$ such that $s\ge a$ for each $s\in S$. Since $S$ is non-empty and is bounded below then there exists an infimum. How can I continue this proof?,,"['real-analysis', 'supremum-and-infimum']"
6,Are all the norms of $L^p$ space equivalent?,Are all the norms of  space equivalent?,L^p,"Are all the norms of $L^p$ space equivalent? That is, for any $p,~q \in R^+$, there exist two positive number $C_1,~C_2$ such that $$ C_1\|u\|_{L^q} \le \|u\|_{L^p} \le C_2\|u\|_{L^q}. $$","Are all the norms of $L^p$ space equivalent? That is, for any $p,~q \in R^+$, there exist two positive number $C_1,~C_2$ such that $$ C_1\|u\|_{L^q} \le \|u\|_{L^p} \le C_2\|u\|_{L^q}. $$",,"['real-analysis', 'functional-analysis', 'lp-spaces']"
7,Do proper dense subgroups of the real numbers have uncountable index,Do proper dense subgroups of the real numbers have uncountable index,,"Just what it says on the tin.  Let $G$ be a dense subgroup of $\mathbb{R}$; assume that $G \neq \mathbb{R}$.  I know that the index of $G$ in $\mathbb{R}$ has to be infinite (since any subgroup of $\mathbb{C}$ of finite index in $\mathbb{C}$ has to have index 1 or 2); does it have to be uncountable, though?  All the examples I can readily come up with (e.g. $\mathbb{Q}$, $\mathbb{Z}[\sqrt{2}]$, ...) have uncountable index in $\mathbb{R}$. Thanks!","Just what it says on the tin.  Let $G$ be a dense subgroup of $\mathbb{R}$; assume that $G \neq \mathbb{R}$.  I know that the index of $G$ in $\mathbb{R}$ has to be infinite (since any subgroup of $\mathbb{C}$ of finite index in $\mathbb{C}$ has to have index 1 or 2); does it have to be uncountable, though?  All the examples I can readily come up with (e.g. $\mathbb{Q}$, $\mathbb{Z}[\sqrt{2}]$, ...) have uncountable index in $\mathbb{R}$. Thanks!",,"['real-analysis', 'abstract-algebra', 'group-theory', 'axiom-of-choice']"
8,"How would I go about finding a closed form solution for $g(x,n) = f(f(f(...(x))))$, $n$ times?","How would I go about finding a closed form solution for ,  times?","g(x,n) = f(f(f(...(x)))) n","A friend of mine mentioned that given the functions $g(x, n)$ and $f(x)$ such that: $$g(x,n)=\overbrace{f(f(f(...f(x))}^\text{n}$$ For example: $$g(x,1)= f(x)\\g(x,2) = f(f(x))$$ and that there is a closed form solution. I don't know exactly where I would begin, but any help or pointers would be very much appreciated. I am also not sure if I denoted the function $g(x,n)$ using the correct notation. Thank you in advance.","A friend of mine mentioned that given the functions $g(x, n)$ and $f(x)$ such that: $$g(x,n)=\overbrace{f(f(f(...f(x))}^\text{n}$$ For example: $$g(x,1)= f(x)\\g(x,2) = f(f(x))$$ and that there is a closed form solution. I don't know exactly where I would begin, but any help or pointers would be very much appreciated. I am also not sure if I denoted the function $g(x,n)$ using the correct notation. Thank you in advance.",,"['real-analysis', 'complex-analysis', 'function-and-relation-composition']"
9,Half the rationals?,Half the rationals?,,"Let $\mathbb{Q}[n]$ be the set of rational numbers with denominator $\le n$ and for any $X\subseteq \mathbb{Q}$, let $X[n]=X\cap \mathbb{Q}[n]$. Is there a set of rational numbers, X , such that for any interval Y of rationals:     $$\underset{n\to \infty }{\mathop{\lim }}\,\frac{card(X[n]\cap Y)}{card(\mathbb{Q}[n]\cap Y)} = 1/2 ?$$","Let $\mathbb{Q}[n]$ be the set of rational numbers with denominator $\le n$ and for any $X\subseteq \mathbb{Q}$, let $X[n]=X\cap \mathbb{Q}[n]$. Is there a set of rational numbers, X , such that for any interval Y of rationals:     $$\underset{n\to \infty }{\mathop{\lim }}\,\frac{card(X[n]\cap Y)}{card(\mathbb{Q}[n]\cap Y)} = 1/2 ?$$",,"['real-analysis', 'elementary-number-theory', 'rational-numbers']"
10,How may I prove this integral inequality?,How may I prove this integral inequality?,,Prove the following inequality: $$\frac{\sqrt{\pi}}{2}\le\int_{0}^{1}  \left({\log(\csc(x))}\right)^{1/3} dx$$ What should i start with? (it's not a homework but a hobby related activity),Prove the following inequality: $$\frac{\sqrt{\pi}}{2}\le\int_{0}^{1}  \left({\log(\csc(x))}\right)^{1/3} dx$$ What should i start with? (it's not a homework but a hobby related activity),,"['calculus', 'real-analysis', 'inequality']"
11,Are undefined terms allowed in a sequence?,Are undefined terms allowed in a sequence?,,"Is this a valid sequence: $$\left\{\frac1{(n-3)}\right\}?$$ That is, can a sequence have individual terms that are undefined? If so, does this mean that the above sequence is unbounded (since the third term is not smaller than any real number)? [What I do know is that if the above sequence were valid, it would be convergent (by definition), which would in turn mean that it is bounded (by the Boundedness Theorem).] Subsequent to the main discussion here, I have just come across an old exam question that asks us to show that $\langle\tan(\frac{\sqrt n\pi}4):n∈N\rangle$ is divergent. But this sequence is undefined at say, $n=4$ , so like the one above, it isn't even a valid sequence, is it? (I've used a different notation for sequences on Brian M Scott's advice.)","Is this a valid sequence: That is, can a sequence have individual terms that are undefined? If so, does this mean that the above sequence is unbounded (since the third term is not smaller than any real number)? [What I do know is that if the above sequence were valid, it would be convergent (by definition), which would in turn mean that it is bounded (by the Boundedness Theorem).] Subsequent to the main discussion here, I have just come across an old exam question that asks us to show that is divergent. But this sequence is undefined at say, , so like the one above, it isn't even a valid sequence, is it? (I've used a different notation for sequences on Brian M Scott's advice.)",\left\{\frac1{(n-3)}\right\}? \langle\tan(\frac{\sqrt n\pi}4):n∈N\rangle n=4,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
12,Weak convergence on a dense subset of test functions,Weak convergence on a dense subset of test functions,,"Let $S$ be a (norm-)dense subset of a Banach space $B$. Suppose $\varphi_n$ is a sequence in the dual space $B^*$ and that for some $\varphi \in B^*$ we have $\varphi_n(g) \to \varphi(g)$ for all $g \in S$. Is it true that $\varphi_n(f) \to \varphi(f)$ for all $f \in B$? Here are some of my thoughts: Fix $f \in B$. For any $\epsilon > 0$ choose $g \in S$ with $\| f - g \|_B < \epsilon$. By the triangle inequality we have \begin{align} | \varphi_n(f) - \varphi(f) | & \le | \varphi_n(f) - \varphi_n(g) | + | \varphi_n(g) - \varphi(g) | + | \varphi(g) - \varphi(f) | \\ & \le \| \varphi_n \|_{B^*} \| f - g \|_B + | \varphi_n(g) - \varphi(g) | + \| \varphi \|_{B^*} \| f - g \|_B \end{align} By hypothesis, the middle term can be made smaller than $\epsilon$ for sufficiently large $n$, hence $$| \varphi_n(f) - \varphi(f) | \le \| \varphi_n \|_{B^*} \epsilon + (1 + \| \varphi \|_{B^*}) \epsilon $$ The second term can be made arbitrarily small, but the first term may be unbounded if $\| \varphi_n \|_{B^*}$ get arbitrarily large. Is there any reason to believe that these norms are uniformly bounded?","Let $S$ be a (norm-)dense subset of a Banach space $B$. Suppose $\varphi_n$ is a sequence in the dual space $B^*$ and that for some $\varphi \in B^*$ we have $\varphi_n(g) \to \varphi(g)$ for all $g \in S$. Is it true that $\varphi_n(f) \to \varphi(f)$ for all $f \in B$? Here are some of my thoughts: Fix $f \in B$. For any $\epsilon > 0$ choose $g \in S$ with $\| f - g \|_B < \epsilon$. By the triangle inequality we have \begin{align} | \varphi_n(f) - \varphi(f) | & \le | \varphi_n(f) - \varphi_n(g) | + | \varphi_n(g) - \varphi(g) | + | \varphi(g) - \varphi(f) | \\ & \le \| \varphi_n \|_{B^*} \| f - g \|_B + | \varphi_n(g) - \varphi(g) | + \| \varphi \|_{B^*} \| f - g \|_B \end{align} By hypothesis, the middle term can be made smaller than $\epsilon$ for sufficiently large $n$, hence $$| \varphi_n(f) - \varphi(f) | \le \| \varphi_n \|_{B^*} \epsilon + (1 + \| \varphi \|_{B^*}) \epsilon $$ The second term can be made arbitrarily small, but the first term may be unbounded if $\| \varphi_n \|_{B^*}$ get arbitrarily large. Is there any reason to believe that these norms are uniformly bounded?",,"['real-analysis', 'functional-analysis']"
13,Proof of subadditivity of the Lebesgue measure,Proof of subadditivity of the Lebesgue measure,,"In the proof of the following proposition:  Let $\{E_n\}$ be a countable collection of sets of real numbers. Then $$ m^\ast\left(\bigcup E_n\right)\leq \sum m^\ast\left(E_n\right)~,$$ we  suppose that $m^\ast(E_n)$ is finite for all $n$. Then for each $E_n$, there is a countable collection $\{I_{k}^{n}:k\geq 1\}$ such that $E_n\subset \bigcup_{k}I_{k}^{n}$ $~~$ and $$\sum l\left(I^{n}_{k}\right) \leq m^\ast(E_n)+\frac{\epsilon}{2^n},~~~\epsilon >0.$$ Here is my question. I don't understand why the last inequality true. Explanations will be very much appreciated. Thanks.","In the proof of the following proposition:  Let $\{E_n\}$ be a countable collection of sets of real numbers. Then $$ m^\ast\left(\bigcup E_n\right)\leq \sum m^\ast\left(E_n\right)~,$$ we  suppose that $m^\ast(E_n)$ is finite for all $n$. Then for each $E_n$, there is a countable collection $\{I_{k}^{n}:k\geq 1\}$ such that $E_n\subset \bigcup_{k}I_{k}^{n}$ $~~$ and $$\sum l\left(I^{n}_{k}\right) \leq m^\ast(E_n)+\frac{\epsilon}{2^n},~~~\epsilon >0.$$ Here is my question. I don't understand why the last inequality true. Explanations will be very much appreciated. Thanks.",,"['real-analysis', 'measure-theory']"
14,Showing the derivative of a differentiable function has a point of continuity,Showing the derivative of a differentiable function has a point of continuity,,"The question goes like this -  Let $f:[0,1]\rightarrow \mathbb{R}$ be a differentiable function. Show that $f'(x)$ has a continuity point. Thanks for the help!","The question goes like this -  Let $f:[0,1]\rightarrow \mathbb{R}$ be a differentiable function. Show that $f'(x)$ has a continuity point. Thanks for the help!",,"['calculus', 'real-analysis']"
15,"Is there a sequence $(a_n)$ so that for every $r \in \mathbb{R}$, there is a subsequence of $(a_n)$ convergent to r?","Is there a sequence  so that for every , there is a subsequence of  convergent to r?",(a_n) r \in \mathbb{R} (a_n),"My guess is that there is no such sequence and prove it by contradiction. My attempt is as follows: Let $A_r$ denote the set containing all terms of such subsequence for each $r \in \mathbb{R}$ . Since each subsequence that converges to $r$ contains at least one distinct term, then the union $\bigcup_{r\in \mathbb{R}}A_r$ has uncountably many elements. However, the sequences cannot have uncountably many terms. Is there anything wrong with my attempt? Or a better solution to go?","My guess is that there is no such sequence and prove it by contradiction. My attempt is as follows: Let denote the set containing all terms of such subsequence for each . Since each subsequence that converges to contains at least one distinct term, then the union has uncountably many elements. However, the sequences cannot have uncountably many terms. Is there anything wrong with my attempt? Or a better solution to go?",A_r r \in \mathbb{R} r \bigcup_{r\in \mathbb{R}}A_r,['real-analysis']
16,Asymptotics of $J_n=\int^\pi_0 \max_{1\leq k\leq n} |\sin(kt)|\frac{dt}{t}$,Asymptotics of,J_n=\int^\pi_0 \max_{1\leq k\leq n} |\sin(kt)|\frac{dt}{t},"This is a problem in an old qualifier test in analysis that I am trying to solve. Show that $I_n=\int^\pi_0\frac{|\sin nt|}{t}\,dt \sim \frac{\pi}{2} \log n$ as $n\rightarrow\infty$ $J_n=\int^\pi_0 \max_{1\leq k\leq n} |\sin(kt)|\frac{dt}{t}\sim \log(n)$ as $n\rightarrow\infty$ I worked out the first part by splitting the integral in several pieces: \begin{align} \int^\pi_0 \frac{|\sin nt|}{t}\,dt &= \int^{n\pi}_0 \frac{|\sin t|}{t}\,dt\\ &=\sum^{n-1}_{k=0}\int^{(k+1)\pi}_{k\pi}\frac{|\sin t|}{t}\,dt\\ &=\int^{\pi}_0\frac{\sin t}{t}\,dt +\sum^{n-1}_{k=1} \int^{(k+1)\pi}_{k\pi}\frac{|\sin t|}{t}\,dt \end{align} For $k\geq 2$ , $$\frac{2}{\pi}\frac{1}{k+1}=\frac{1}{(k+1)\pi}\int^\pi_0|\sin t|\,dt \leq \int^{(k+1)\pi}_{k\pi}\frac{|\sin t|}{t}\,dt\leq \frac{1}{\pi k}\int^\pi_0|\sin t|\,dt=\frac{2}{\pi}\frac{1}{k}$$ Therefore $$ \int^\pi_0\frac{\sin t}{t}\,dt+\frac{\pi}{2}\sum^n_{k=2}\frac{1}{k}\leq  I_n \leq \int^\pi_0\frac{\sin t}{t}\,dt+\frac{\pi}{2}\sum^{n-1}_{k=1}\frac1k $$ From this, part 1 follows. The part I am not making much progress is the second one. Any hints/ideas will be appreciated. Thank you!","This is a problem in an old qualifier test in analysis that I am trying to solve. Show that as as I worked out the first part by splitting the integral in several pieces: For , Therefore From this, part 1 follows. The part I am not making much progress is the second one. Any hints/ideas will be appreciated. Thank you!","I_n=\int^\pi_0\frac{|\sin nt|}{t}\,dt \sim \frac{\pi}{2} \log n n\rightarrow\infty J_n=\int^\pi_0 \max_{1\leq k\leq n} |\sin(kt)|\frac{dt}{t}\sim \log(n) n\rightarrow\infty \begin{align}
\int^\pi_0 \frac{|\sin nt|}{t}\,dt &= \int^{n\pi}_0 \frac{|\sin t|}{t}\,dt\\
&=\sum^{n-1}_{k=0}\int^{(k+1)\pi}_{k\pi}\frac{|\sin t|}{t}\,dt\\
&=\int^{\pi}_0\frac{\sin t}{t}\,dt +\sum^{n-1}_{k=1} \int^{(k+1)\pi}_{k\pi}\frac{|\sin t|}{t}\,dt
\end{align} k\geq 2 \frac{2}{\pi}\frac{1}{k+1}=\frac{1}{(k+1)\pi}\int^\pi_0|\sin t|\,dt \leq \int^{(k+1)\pi}_{k\pi}\frac{|\sin t|}{t}\,dt\leq \frac{1}{\pi k}\int^\pi_0|\sin t|\,dt=\frac{2}{\pi}\frac{1}{k} 
\int^\pi_0\frac{\sin t}{t}\,dt+\frac{\pi}{2}\sum^n_{k=2}\frac{1}{k}\leq 
I_n \leq \int^\pi_0\frac{\sin t}{t}\,dt+\frac{\pi}{2}\sum^{n-1}_{k=1}\frac1k
","['real-analysis', 'integration', 'asymptotics']"
17,Does the sequence of positive roots of $x^n+x^{n-1}+\cdots+x-1=0$ converge?,Does the sequence of positive roots of  converge?,x^n+x^{n-1}+\cdots+x-1=0,"For a positive integer $n$ , let $a_n$ denote the unique positive real root of the equation $$x^n+x^{n-1}+\cdots+x-1=0.$$ Then what can we say about the sequence $\{a_n\}$ ? We can observe that all $a_n<1$ so if $\lim a_n$ exists, let's say at $L$ , then $L \le 1$ . Also \begin{align}  &&x^n+x^{n-1}+\cdots+x-1 &= 0 \\  \implies&&  x^{n+1}-1 &= 2(x-1) \\  \implies&& x^{n+1} &= 2x-1 \\  \implies&& a_n^{n+1} &= 2a_n-1.  \end{align} Taking limit both side we get $0=2L-1$ , i.e. $L=\frac12$ but how to show that the limit always exists?","For a positive integer , let denote the unique positive real root of the equation Then what can we say about the sequence ? We can observe that all so if exists, let's say at , then . Also Taking limit both side we get , i.e. but how to show that the limit always exists?","n a_n x^n+x^{n-1}+\cdots+x-1=0. \{a_n\} a_n<1 \lim a_n L L \le 1 \begin{align} 
&&x^n+x^{n-1}+\cdots+x-1 &= 0 \\ 
\implies&& 
x^{n+1}-1 &= 2(x-1) \\ 
\implies&& x^{n+1} &= 2x-1 \\ 
\implies&& a_n^{n+1} &= 2a_n-1. 
\end{align} 0=2L-1 L=\frac12","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'roots']"
18,Parallelogram law functional equation: $ f ( x + y ) + f ( x - y ) = 2 \big( f ( x ) + f ( y ) \big) $,Parallelogram law functional equation:, f ( x + y ) + f ( x - y ) = 2 \big( f ( x ) + f ( y ) \big) ,"I'm trying to solve the following functional equation, called the parallelogram law : $$ f ( x + y ) + f ( x - y ) = 2 \big( f ( x ) + f ( y ) \big) $$ It can be easily proven that we have the family of solutions $ f ( x ) = c x ^ 2 $ . Is there other type of solution apart from this? I've proven that this are all the solutions in the case $ f : \mathbb Q \to \mathbb Q $ this is the only solution. To prove this, first let $ x = y = 0 $ . We then get $$ 2 f ( 0 ) = 4 f ( 0 ) \implies f ( 0 ) = 0 $$ Now will prove by induction that $ f ( n x ) = n ^ 2 f ( x ) $ , $ \forall n \in \mathbb N $ . Statement hold for $ n = 1 $ . Now, supposing the induction hypothesis holds, let $ x = n x $ and $ y = x $ . Then $$ f \big( ( n + 1 ) x \big) + f \big( ( n - 1 ) x \big) = 2 \big( f ( n x ) + f ( x ) \big) $$ $$ f \big( ( n + 1 ) x \big) + ( n - 1 ) ^ 2 f ( x ) = 2 \left( n ^ 2 + 1 \right) f ( x ) $$ $$ f \big( ( n + 1 ) x \big) = ( n + 1 ) ^ 2 f ( x ) $$ Now letting $ x = 0 $ , we get $$ f ( y ) + f ( - y ) = 2 f ( y ) \implies f ( - y ) = f ( y ) $$ $$ f ( n x ) = n ^ 2 f ( x ) \text , \ \forall n \in \mathbb Z $$ Also notice that $$ f ( x ) = f \left( n \frac x n \right) = n ^ 2 f \left( \frac x n \right) $$ $$ f \left( \frac x n \right) = \frac { f ( x ) } { n ^ 2 } \text , \ \forall n \in \mathbb Z \setminus \{ 0 \} $$ We can conclude that $ f ( r x ) = r ^ 2 f ( x ) \forall r \in \mathbb Q $ . The above reasoning is valid also when $ f : \mathbb R \to \mathbb R $ . If we assume continuity  in some interval or monotonicity in some interval $ [ a , b ] $ , the solution is of form of a quadratic polynomial. If the solution is not unique, what are the minimal assumptions one has to make to ensure the statement hold (injectivity, positivity, Riemann integrability, measurability, not being dense in a region of $\mathbb R ^ 2 $ )?","I'm trying to solve the following functional equation, called the parallelogram law : It can be easily proven that we have the family of solutions . Is there other type of solution apart from this? I've proven that this are all the solutions in the case this is the only solution. To prove this, first let . We then get Now will prove by induction that , . Statement hold for . Now, supposing the induction hypothesis holds, let and . Then Now letting , we get Also notice that We can conclude that . The above reasoning is valid also when . If we assume continuity  in some interval or monotonicity in some interval , the solution is of form of a quadratic polynomial. If the solution is not unique, what are the minimal assumptions one has to make to ensure the statement hold (injectivity, positivity, Riemann integrability, measurability, not being dense in a region of )?"," f ( x + y ) + f ( x - y ) = 2 \big( f ( x ) + f ( y ) \big)   f ( x ) = c x ^ 2   f : \mathbb Q \to \mathbb Q   x = y = 0   2 f ( 0 ) = 4 f ( 0 ) \implies f ( 0 ) = 0   f ( n x ) = n ^ 2 f ( x )   \forall n \in \mathbb N   n = 1   x = n x   y = x   f \big( ( n + 1 ) x \big) + f \big( ( n - 1 ) x \big) = 2 \big( f ( n x ) + f ( x ) \big)   f \big( ( n + 1 ) x \big) + ( n - 1 ) ^ 2 f ( x ) = 2 \left( n ^ 2 + 1 \right) f ( x )   f \big( ( n + 1 ) x \big) = ( n + 1 ) ^ 2 f ( x )   x = 0   f ( y ) + f ( - y ) = 2 f ( y ) \implies f ( - y ) = f ( y )   f ( n x ) = n ^ 2 f ( x ) \text , \ \forall n \in \mathbb Z   f ( x ) = f \left( n \frac x n \right) = n ^ 2 f \left( \frac x n \right)   f \left( \frac x n \right) = \frac { f ( x ) } { n ^ 2 } \text , \ \forall n \in \mathbb Z \setminus \{ 0 \}   f ( r x ) = r ^ 2 f ( x ) \forall r \in \mathbb Q   f : \mathbb R \to \mathbb R   [ a , b ]  \mathbb R ^ 2 ","['real-analysis', 'recreational-mathematics', 'functional-equations']"
19,Can any subset of $\mathbb{R}$ be generated from open intervals?,Can any subset of  be generated from open intervals?,\mathbb{R},"Can any subset of $\mathbb{R}$ be generated by taking countable unions, countable intersections and complements of open intervals? Clearly, singletons can be generated from the complement of the union of half-rays, e.g.: $$a=((-\infty, a) \cup (a, \infty))^C .$$ Closed intervals can also be generated by a countable intersection of open sets of the form $(a- \frac1n, b+\frac1n)$ and similarly for half-open intervals. From this, it seems obvious that any countable union/intersection of intervals can be generated. How about for example uncountable unions/intersections of intervals?  It is unclear to me whether this is enough to generate all subsets of $\mathbb{R}$ . Can this be done?","Can any subset of be generated by taking countable unions, countable intersections and complements of open intervals? Clearly, singletons can be generated from the complement of the union of half-rays, e.g.: Closed intervals can also be generated by a countable intersection of open sets of the form and similarly for half-open intervals. From this, it seems obvious that any countable union/intersection of intervals can be generated. How about for example uncountable unions/intersections of intervals?  It is unclear to me whether this is enough to generate all subsets of . Can this be done?","\mathbb{R} a=((-\infty, a) \cup (a, \infty))^C . (a- \frac1n, b+\frac1n) \mathbb{R}","['real-analysis', 'measure-theory']"
20,Doubts regarding $\epsilon$-$\delta$ definition of limits,Doubts regarding - definition of limits,\epsilon \delta,"I am learning $\epsilon$ - $\delta$ definition of limits. I was confused on a few points and read some of the related answers on this and other sites. But I couldn't find discussion on any of these questions anywhere, so I am asking them here. The questions are- Why should $|x-c|<\delta$ imply $|f(x)-L|<\epsilon$ ? Can't it be the other way round? That is, would this be wrong definition: "" Let $f$ be a function defined on an open interval around $c$ (except possibly at $c$ ). Then, if for any $\epsilon>0$ there exists a $\delta > 0$ such that $0< |x-c|<\delta$ whenever $|f(x)-L|<\epsilon$ ; then $$\lim_{x \to c} f(x) = L$$ ""? 2  Why do we choose open intervals around $x$ and $f(x)$ instead of closed ones (i.e. $x \in (x-\delta,x+\delta),f(x) \in (f(x)-\epsilon,f(x)+\epsilon)$ instead of $x \in [x-\delta,x+\delta],f(x) \in [L-\epsilon,L+\epsilon]$ ) ? Why are $\delta$ and $\epsilon$ chosen to be greater than zero? Can't we choose them to be less than zero and specify the bounds of $x$ and $f(x)$ thus: $\delta < x-c< -\delta$ and $\epsilon < f(x) - L < -\epsilon$ ? My guess is that the answer to the points 2 & 3 should be: It is just by convention . But I have no source to back it up. I couldn't find any discussion on this anywhere. Not in my textbook, nor on Wikipedia or other sites. So, what are the answers to these questions? EDIT: If someone says that we can make the choices I suggested in points 2 & 3 too (instead of the ones we do currently), and so the latter are just historical conventions, please cite the source for your statement.","I am learning - definition of limits. I was confused on a few points and read some of the related answers on this and other sites. But I couldn't find discussion on any of these questions anywhere, so I am asking them here. The questions are- Why should imply ? Can't it be the other way round? That is, would this be wrong definition: "" Let be a function defined on an open interval around (except possibly at ). Then, if for any there exists a such that whenever ; then ""? 2  Why do we choose open intervals around and instead of closed ones (i.e. instead of ) ? Why are and chosen to be greater than zero? Can't we choose them to be less than zero and specify the bounds of and thus: and ? My guess is that the answer to the points 2 & 3 should be: It is just by convention . But I have no source to back it up. I couldn't find any discussion on this anywhere. Not in my textbook, nor on Wikipedia or other sites. So, what are the answers to these questions? EDIT: If someone says that we can make the choices I suggested in points 2 & 3 too (instead of the ones we do currently), and so the latter are just historical conventions, please cite the source for your statement.","\epsilon \delta |x-c|<\delta |f(x)-L|<\epsilon f c c \epsilon>0 \delta > 0 0< |x-c|<\delta |f(x)-L|<\epsilon \lim_{x \to c} f(x) = L x f(x) x \in (x-\delta,x+\delta),f(x) \in (f(x)-\epsilon,f(x)+\epsilon) x \in [x-\delta,x+\delta],f(x) \in [L-\epsilon,L+\epsilon] \delta \epsilon x f(x) \delta < x-c< -\delta \epsilon < f(x) - L < -\epsilon","['calculus', 'real-analysis', 'limits', 'definition', 'epsilon-delta']"
21,"dist$(x,A)=0$ if and only if $x\in \overline{A}$ (closure of $A$)",dist if and only if  (closure of ),"(x,A)=0 x\in \overline{A} A","Let $(X,d)$ be  a metric space and non-empty $A\subset X$. The distance from $x$ to $A$ is defined as $$\mbox{dist}(x,A)=\inf\{d(x,a):a\in A\}$$ I want to show that dist$(x,A)=0$ if and only if $x$ is in the closure of $A$. (=>) : dist$(x,A)=0$ implies $\inf \{d(x,a):a\in A\}=0$. Suppose $x\not\in \overline{A}$, then $x\in X\setminus \overline{A}$, which is an open set. Thus $\exists\epsilon >0$ such that punctured open ball $B(x;\epsilon)\setminus\{x\}= \{y\in X:0<d(x,y)<\epsilon\} \subset X\setminus\overline{A}$. But then dist$(x,A)=\inf\{d(x,a):a\in A\}\ge \inf\{d(x,y):y\in B(x,\epsilon)\setminus\{x\}\}>0$. This is a contradiction! Hence, $x\in \overline{A}$. (<=) : Suppose $x\in \overline{A}$. If $x\in A^\circ$ (the interior of $A$) then $\inf\{d(x,a):a\in A\}=d(x,x)=0$.   If $x\not\in A^\circ$ then $\exists $ a sequence $(a_k)_k\subset A$ such that $a_k$ converges to $x$. Then, $\forall \epsilon > 0, \exists N\in\mathbb{N}$ such that $k\ge N\implies d(a_k,x)<\epsilon$. So that dist$(x,A)=\inf\{d(x,a_k)\}=\lim\limits_{k\to\infty} d(x,a_n) = 0$. Please let me know if my proof is rigorous / correct. I'm not completely sure, think it's better to check.","Let $(X,d)$ be  a metric space and non-empty $A\subset X$. The distance from $x$ to $A$ is defined as $$\mbox{dist}(x,A)=\inf\{d(x,a):a\in A\}$$ I want to show that dist$(x,A)=0$ if and only if $x$ is in the closure of $A$. (=>) : dist$(x,A)=0$ implies $\inf \{d(x,a):a\in A\}=0$. Suppose $x\not\in \overline{A}$, then $x\in X\setminus \overline{A}$, which is an open set. Thus $\exists\epsilon >0$ such that punctured open ball $B(x;\epsilon)\setminus\{x\}= \{y\in X:0<d(x,y)<\epsilon\} \subset X\setminus\overline{A}$. But then dist$(x,A)=\inf\{d(x,a):a\in A\}\ge \inf\{d(x,y):y\in B(x,\epsilon)\setminus\{x\}\}>0$. This is a contradiction! Hence, $x\in \overline{A}$. (<=) : Suppose $x\in \overline{A}$. If $x\in A^\circ$ (the interior of $A$) then $\inf\{d(x,a):a\in A\}=d(x,x)=0$.   If $x\not\in A^\circ$ then $\exists $ a sequence $(a_k)_k\subset A$ such that $a_k$ converges to $x$. Then, $\forall \epsilon > 0, \exists N\in\mathbb{N}$ such that $k\ge N\implies d(a_k,x)<\epsilon$. So that dist$(x,A)=\inf\{d(x,a_k)\}=\lim\limits_{k\to\infty} d(x,a_n) = 0$. Please let me know if my proof is rigorous / correct. I'm not completely sure, think it's better to check.",,"['real-analysis', 'general-topology', 'proof-verification', 'metric-spaces']"
22,Uniformly continuous with unbounded derivative,Uniformly continuous with unbounded derivative,,"A bounded derivative is a sufficient condition for uniform continuity, but not necessary. I know the counterexample $f(x) = \sqrt{x}$ on the interval $[0,\infty)$ where the derivative is unbounded at $0$, but the function is uniformly continuous. Is there an example where $f$ is uniformly continuous and $f'(x)$ is unbounded as $x \to \infty$ but bounded on any compact interval?","A bounded derivative is a sufficient condition for uniform continuity, but not necessary. I know the counterexample $f(x) = \sqrt{x}$ on the interval $[0,\infty)$ where the derivative is unbounded at $0$, but the function is uniformly continuous. Is there an example where $f$ is uniformly continuous and $f'(x)$ is unbounded as $x \to \infty$ but bounded on any compact interval?",,"['real-analysis', 'uniform-continuity']"
23,Do isometries send lines to lines?,Do isometries send lines to lines?,,"My question is if any isometry $f:V\to W$ between real normed spaces sends lines to lines. I've seen several questions/answers about this but only in euclidean spaces. So I thought it was false on general (real) normed spaces. However I found this theorem of Mazur-Ulam: any surjective isometry $f:V\to W$ is an affine map, hence it maps lines to lines. But if my isometry is not surjective, would this still apply? I think that considering the image space $f(V)$ it would be the same, because $f:V\to f(V)$ is affine and any line $L$ would be sent to a line $f(L)$ in $f(V)$ which is also a line in $W$. Is this correct? Thank you.","My question is if any isometry $f:V\to W$ between real normed spaces sends lines to lines. I've seen several questions/answers about this but only in euclidean spaces. So I thought it was false on general (real) normed spaces. However I found this theorem of Mazur-Ulam: any surjective isometry $f:V\to W$ is an affine map, hence it maps lines to lines. But if my isometry is not surjective, would this still apply? I think that considering the image space $f(V)$ it would be the same, because $f:V\to f(V)$ is affine and any line $L$ would be sent to a line $f(L)$ in $f(V)$ which is also a line in $W$. Is this correct? Thank you.",,"['real-analysis', 'linear-algebra', 'functional-analysis', 'normed-spaces', 'isometry']"
24,How to prove $\sum\limits_{n=1}^\infty\frac{\sin(n)}n=\frac{\pi-1}2$ using only real numbers.,How to prove  using only real numbers.,\sum\limits_{n=1}^\infty\frac{\sin(n)}n=\frac{\pi-1}2,"I noticed that a lot of the time, people ask whether the following sum converges: $$\sum_{n=1}^\infty\frac{\sin(n)}n$$ Though I've never stopped to ask what it equaled.  According to this other post , the sum is given as $$\sum_{n=1}^\infty\frac{\sin(n)}n=\frac{\pi-1}2$$ The solution involves realizing $\sin(n)=\Im e^{in}$ and the Taylor expansion for the natural logarithm. While thats great and all, how can I prove this using only real numbers?","I noticed that a lot of the time, people ask whether the following sum converges: $$\sum_{n=1}^\infty\frac{\sin(n)}n$$ Though I've never stopped to ask what it equaled.  According to this other post , the sum is given as $$\sum_{n=1}^\infty\frac{\sin(n)}n=\frac{\pi-1}2$$ The solution involves realizing $\sin(n)=\Im e^{in}$ and the Taylor expansion for the natural logarithm. While thats great and all, how can I prove this using only real numbers?",,"['real-analysis', 'sequences-and-series', 'alternative-proof', 'pi']"
25,"If $g$ is Riemann-integrable in a closed interval and $f$ is a increasing function in a closed interval, is $g\circ f$ Riemann-integrable?","If  is Riemann-integrable in a closed interval and  is a increasing function in a closed interval, is  Riemann-integrable?",g f g\circ f,"If $g$ is Riemann-integrable in a closed interval and $f$ is a increasing function in a closed interval, is $g\circ f$ Riemann-integrable? To clarify: the problem stated that the composition is well defined. I think that the statement of the question is true but Im having trouble to write/concrete a proof. I know that a monotone function defined in a closed interval is Riemann-integrable by a previous result. And I know that the reverse statement ($g$ being monotone and $f$ integrable) is not true in general. After thinking some moment my idea for the proof is to show that $g\circ f$ have, at most, countable discontinuities. To do this I was thinking how to show some kind of order-correspondence between the points of the domain of a monotone function and it image. Can you help me? I get stuck and it is very possible that Im wrong in my assumption about that the statement is true. Some hint will be appreciated. EDIT: I get a new idea, design a partition of $f$ that take any possible discontinuity into a closed interval of known length, and after see what happen in the composition with this closed intervals with discontinuities, and by the other hand see what happen with the parts of continuous mapping. After reading this answer I get the idea to provide a proof for the validity of the statement. First I will characterize the different kind of images of closed intervals that a monotonic function can create. For an increasing function we have that $x<y$ implies that $f(x)\le f(y)$, and that at most a monotone function can have a countable number of discontinuities in any closed interval. No discontinuities on the image: if the image of $f([x_1,x_2])$ is continuous then it can be at most of three types: A constant function $f([x_1,x_2])=\{a\}$. Then $(g\circ f)([x_1,x_2])=g(\{a\})=\{c\}$, then the function $(g\circ f)$ is constant in $[x_1,x_2]$ so is Riemann-integrable A strictly increasing function i.e. $f([x_1,x_2])=[a,b]$. Then $(g\circ f)([x_1,x_2])=g([a,b])=[c,d]$. If $g$ is Riemann integrable in $[a,b]$ then exists a sequence of partitions $(P_n)$ such that $$\lim_{n\to\infty}U(g,P_n)-L(g,P_n)=0$$ Then because $f$ is bijective in $[a,b]$ then for every $P_n$ exists a partition $P'_n$ in $[x_1,x_2]$ such that $$\lim_{n\to\infty}U(g\circ f,P'_n)-L(g\circ f,P'_n)=0$$ so $g\circ f$ is Riemann-integrable in $[x_1,x_2]$ A mix of both previous cases. Then the interval $[x_1,x_2]$ can be partitioned on types of subintervals discussed previously (constant and strictly monotonic images), so $g\circ f$ is Riemann-integrable here too. Discontinuity in the image : a jump discontinuity in $[x_1,x_2]$ is mapped into two types of the previously discussed intervals with the difference that can be the case that in the image exists an interval with an open boundary of the kind $[a,b)$ or $(a,b]$. But cause $f$ and $g$ are Riemann integrable in closed intervals this mean that they are bounded, and if $g$ is integrable in any subinterval $[a-\varepsilon,b]$ then is integrable in $(a,b]$ (this is known by a previous proof). My question: it is this proof correct? It lacks something essential? Can you help me to write it better? Thank you in advance. EDIT 2: as the user @ParamanandSingh pointed in the comments it is possible that the discontinuities cannot be isolated, one by one, inside some interval. So I will search further for a correct proof of the statement.","If $g$ is Riemann-integrable in a closed interval and $f$ is a increasing function in a closed interval, is $g\circ f$ Riemann-integrable? To clarify: the problem stated that the composition is well defined. I think that the statement of the question is true but Im having trouble to write/concrete a proof. I know that a monotone function defined in a closed interval is Riemann-integrable by a previous result. And I know that the reverse statement ($g$ being monotone and $f$ integrable) is not true in general. After thinking some moment my idea for the proof is to show that $g\circ f$ have, at most, countable discontinuities. To do this I was thinking how to show some kind of order-correspondence between the points of the domain of a monotone function and it image. Can you help me? I get stuck and it is very possible that Im wrong in my assumption about that the statement is true. Some hint will be appreciated. EDIT: I get a new idea, design a partition of $f$ that take any possible discontinuity into a closed interval of known length, and after see what happen in the composition with this closed intervals with discontinuities, and by the other hand see what happen with the parts of continuous mapping. After reading this answer I get the idea to provide a proof for the validity of the statement. First I will characterize the different kind of images of closed intervals that a monotonic function can create. For an increasing function we have that $x<y$ implies that $f(x)\le f(y)$, and that at most a monotone function can have a countable number of discontinuities in any closed interval. No discontinuities on the image: if the image of $f([x_1,x_2])$ is continuous then it can be at most of three types: A constant function $f([x_1,x_2])=\{a\}$. Then $(g\circ f)([x_1,x_2])=g(\{a\})=\{c\}$, then the function $(g\circ f)$ is constant in $[x_1,x_2]$ so is Riemann-integrable A strictly increasing function i.e. $f([x_1,x_2])=[a,b]$. Then $(g\circ f)([x_1,x_2])=g([a,b])=[c,d]$. If $g$ is Riemann integrable in $[a,b]$ then exists a sequence of partitions $(P_n)$ such that $$\lim_{n\to\infty}U(g,P_n)-L(g,P_n)=0$$ Then because $f$ is bijective in $[a,b]$ then for every $P_n$ exists a partition $P'_n$ in $[x_1,x_2]$ such that $$\lim_{n\to\infty}U(g\circ f,P'_n)-L(g\circ f,P'_n)=0$$ so $g\circ f$ is Riemann-integrable in $[x_1,x_2]$ A mix of both previous cases. Then the interval $[x_1,x_2]$ can be partitioned on types of subintervals discussed previously (constant and strictly monotonic images), so $g\circ f$ is Riemann-integrable here too. Discontinuity in the image : a jump discontinuity in $[x_1,x_2]$ is mapped into two types of the previously discussed intervals with the difference that can be the case that in the image exists an interval with an open boundary of the kind $[a,b)$ or $(a,b]$. But cause $f$ and $g$ are Riemann integrable in closed intervals this mean that they are bounded, and if $g$ is integrable in any subinterval $[a-\varepsilon,b]$ then is integrable in $(a,b]$ (this is known by a previous proof). My question: it is this proof correct? It lacks something essential? Can you help me to write it better? Thank you in advance. EDIT 2: as the user @ParamanandSingh pointed in the comments it is possible that the discontinuities cannot be isolated, one by one, inside some interval. So I will search further for a correct proof of the statement.",,"['real-analysis', 'integration', 'proof-verification', 'proof-writing']"
26,integral inequality for $f(x)$ and $f(\sqrt{x})$,integral inequality for  and,f(x) f(\sqrt{x}),"Show that if $f(x)\in [0;1]$, $f\in C$ and $\int\limits_{1}^{+\infty}f(t)dt=A$ then $\int\limits_{1}^{+\infty}tf(t)dt>\frac{A^2}{2}$ I only have noticed two small things: If $A=1$ inequality is obvious, as expectation of random variable, that takes values greater than $1$ $\int\limits_{1}^{+\infty}tf(t)dt = \dfrac{1}{2}\int\limits_{1}^{+\infty}f(\sqrt{t})dt$ Please, provide hint for next steps Also, some time left I obtain: $$ A^2 = \left[\int\limits_{1}^{+\infty}f(x)x\dfrac{1}{x}dx\right]^2 \leq \int\limits_{1}^{+\infty}\left(f(x)x\right)^2dx\int\limits_{1}^{+\infty}\left(\frac{1}{x}\right)^2dx\leq \int\limits_{1}^{+\infty}f(x)x^2dx $$","Show that if $f(x)\in [0;1]$, $f\in C$ and $\int\limits_{1}^{+\infty}f(t)dt=A$ then $\int\limits_{1}^{+\infty}tf(t)dt>\frac{A^2}{2}$ I only have noticed two small things: If $A=1$ inequality is obvious, as expectation of random variable, that takes values greater than $1$ $\int\limits_{1}^{+\infty}tf(t)dt = \dfrac{1}{2}\int\limits_{1}^{+\infty}f(\sqrt{t})dt$ Please, provide hint for next steps Also, some time left I obtain: $$ A^2 = \left[\int\limits_{1}^{+\infty}f(x)x\dfrac{1}{x}dx\right]^2 \leq \int\limits_{1}^{+\infty}\left(f(x)x\right)^2dx\int\limits_{1}^{+\infty}\left(\frac{1}{x}\right)^2dx\leq \int\limits_{1}^{+\infty}f(x)x^2dx $$",,"['real-analysis', 'improper-integrals', 'integral-inequality']"
27,Do there exist bump functions with uniformly bounded derivatives? [duplicate],Do there exist bump functions with uniformly bounded derivatives? [duplicate],,"This question already has answers here : Does a smooth ""transition function"" with bounded derivatives exist? (2 answers) Closed 8 years ago . Let us consider a bump function $\phi: \mathbb{R} \longrightarrow \mathbb{R}$, smooth, with compact support. The most common examples are built from the function $$ \psi(x) = \begin{cases} \exp ( \frac {1}{x^2 - 1}) & \lvert x \rvert < 1 \\ 0 & \text{otherwise} \end{cases}.$$ Although this function behaves very well, its derivatives become arbitrarily large inside the unit ball. I wonder --- do there exist bump functions with ""small"" derivatives? Stated more precisely, does there exist a bump function $\phi \in C^\infty_c(\mathbb{R})$ and some $M < \infty$ such that $\phi^{(k)} \leq M$ for all $k$th derivatives? Intuitively, I feel the answer is no , as a sort of cost of vanishing entirely.","This question already has answers here : Does a smooth ""transition function"" with bounded derivatives exist? (2 answers) Closed 8 years ago . Let us consider a bump function $\phi: \mathbb{R} \longrightarrow \mathbb{R}$, smooth, with compact support. The most common examples are built from the function $$ \psi(x) = \begin{cases} \exp ( \frac {1}{x^2 - 1}) & \lvert x \rvert < 1 \\ 0 & \text{otherwise} \end{cases}.$$ Although this function behaves very well, its derivatives become arbitrarily large inside the unit ball. I wonder --- do there exist bump functions with ""small"" derivatives? Stated more precisely, does there exist a bump function $\phi \in C^\infty_c(\mathbb{R})$ and some $M < \infty$ such that $\phi^{(k)} \leq M$ for all $k$th derivatives? Intuitively, I feel the answer is no , as a sort of cost of vanishing entirely.",,"['real-analysis', 'functional-analysis']"
28,Prove that $|f|\leq 1$ whenever $|x|\leq 1$.,Prove that  whenever .,|f|\leq 1 |x|\leq 1,"Let $f :\mathbb{R}^2\rightarrow \mathbb{R}^2 $ be everywhere differentiable such that the Jacobian is not singular at any point in $\mathbb{R}^2$. Assume $|f|\leq 1$ whenever $|x|=1$. Prove that $|f|\leq 1$ whenever $|x|\leq 1$. I think this is straight forward if we apply maximum modulus principle. But how may I prove it without using it? I tried to use Implicit function theorem by defining $g:\mathbb{R}^2\rightarrow \mathbb{R}$ by $g(x)=|f(x)|^2$, but no success. At least a hint is appreciated.","Let $f :\mathbb{R}^2\rightarrow \mathbb{R}^2 $ be everywhere differentiable such that the Jacobian is not singular at any point in $\mathbb{R}^2$. Assume $|f|\leq 1$ whenever $|x|=1$. Prove that $|f|\leq 1$ whenever $|x|\leq 1$. I think this is straight forward if we apply maximum modulus principle. But how may I prove it without using it? I tried to use Implicit function theorem by defining $g:\mathbb{R}^2\rightarrow \mathbb{R}$ by $g(x)=|f(x)|^2$, but no success. At least a hint is appreciated.",,"['real-analysis', 'multivariable-calculus', 'implicit-function-theorem', 'maximum-principle']"
29,Computing a double gamma-digamma-trigamma series,Computing a double gamma-digamma-trigamma series,,"What are your thoughts on this series? $$\sum _{k=1}^{\infty } \sum _{n=1}^{\infty } \frac{\Gamma (k)^2 \Gamma (n) }{\Gamma (2 k+n)}((\psi ^{(0)}(n)-\psi ^{(0)}(2 k+n)) (\psi ^{(0)}(k)-\psi ^{(0)}(2 k+n))-\psi ^{(1)}(2 k+n)).$$ EDIT: Noting the interest for this series, I wanna add that the series gets reduced to the calculation of $$\int_0^1 \left(\frac{\text{Li}_3(x)}{x^2-2 x+2}+\frac{\text{Li}_3\left(x-x^2\right)}{x^2-2 x+2}-\frac{\text{Li}_3\left(\frac{x}{x-1}\right)}{x^2-2 x+2}-\frac{\text{Li}_3\left(\frac{(x-1) x}{x^2-x+1}\right)}{x^2-2 x+2}-\frac{\text{Li}_2\left(\frac{x}{x-1}\right) \log (1-x)}{x^2-2 x+2}-\frac{\text{Li}_2(x) \log (1-x)}{x^2-2 x+2}+\frac{\text{Li}_2\left(\frac{x}{x-1}\right) \log (x)}{x^2-2 x+2}+\frac{\text{Li}_2\left(\frac{(x-1) x}{x^2-x+1}\right) \log (x)}{x^2-2 x+2}-\frac{\text{Li}_2(x) \log (x)}{x^2-2 x+2}-\frac{\text{Li}_2\left(x-x^2\right) \log (x)}{x^2-2 x+2}-\frac{\text{Li}_2\left(x-x^2\right) \log \left(x^2-x+1\right)}{x^2-2 x+2}-\frac{\text{Li}_2\left(\frac{(x-1) x}{x^2-x+1}\right) \log \left(x^2-x+1\right)}{x^2-2 x+2}-\frac{\log ^3(1-x)}{3 \left(x^2-2 x+2\right)}-\frac{\log ^3\left(x^2-x+1\right)}{3 \left(x^2-2 x+2\right)}-\frac{\log ^2(x) \log (1-x)}{x^2-2 x+2}-\frac{\log ^2(x) \log \left(x^2-x+1\right)}{x^2-2 x+2}+\frac{\pi ^2 \log (1-x)}{6 \left(x^2-2 x+2\right)}+\frac{\pi ^2 \log \left(x^2-x+1\right)}{6 \left(x^2-2 x+2\right)}\right) \, dx$$ where we are pretty familiar with all the stuff in here. One can find the closed form by calculating the integral, a bit long, but it's a nice journey to go. A 300 points bounty moment: After 2 years and 10 months since the problem has been posed no full solution has been provided. Is it possible to find a slick solution?","What are your thoughts on this series? $$\sum _{k=1}^{\infty } \sum _{n=1}^{\infty } \frac{\Gamma (k)^2 \Gamma (n) }{\Gamma (2 k+n)}((\psi ^{(0)}(n)-\psi ^{(0)}(2 k+n)) (\psi ^{(0)}(k)-\psi ^{(0)}(2 k+n))-\psi ^{(1)}(2 k+n)).$$ EDIT: Noting the interest for this series, I wanna add that the series gets reduced to the calculation of $$\int_0^1 \left(\frac{\text{Li}_3(x)}{x^2-2 x+2}+\frac{\text{Li}_3\left(x-x^2\right)}{x^2-2 x+2}-\frac{\text{Li}_3\left(\frac{x}{x-1}\right)}{x^2-2 x+2}-\frac{\text{Li}_3\left(\frac{(x-1) x}{x^2-x+1}\right)}{x^2-2 x+2}-\frac{\text{Li}_2\left(\frac{x}{x-1}\right) \log (1-x)}{x^2-2 x+2}-\frac{\text{Li}_2(x) \log (1-x)}{x^2-2 x+2}+\frac{\text{Li}_2\left(\frac{x}{x-1}\right) \log (x)}{x^2-2 x+2}+\frac{\text{Li}_2\left(\frac{(x-1) x}{x^2-x+1}\right) \log (x)}{x^2-2 x+2}-\frac{\text{Li}_2(x) \log (x)}{x^2-2 x+2}-\frac{\text{Li}_2\left(x-x^2\right) \log (x)}{x^2-2 x+2}-\frac{\text{Li}_2\left(x-x^2\right) \log \left(x^2-x+1\right)}{x^2-2 x+2}-\frac{\text{Li}_2\left(\frac{(x-1) x}{x^2-x+1}\right) \log \left(x^2-x+1\right)}{x^2-2 x+2}-\frac{\log ^3(1-x)}{3 \left(x^2-2 x+2\right)}-\frac{\log ^3\left(x^2-x+1\right)}{3 \left(x^2-2 x+2\right)}-\frac{\log ^2(x) \log (1-x)}{x^2-2 x+2}-\frac{\log ^2(x) \log \left(x^2-x+1\right)}{x^2-2 x+2}+\frac{\pi ^2 \log (1-x)}{6 \left(x^2-2 x+2\right)}+\frac{\pi ^2 \log \left(x^2-x+1\right)}{6 \left(x^2-2 x+2\right)}\right) \, dx$$ where we are pretty familiar with all the stuff in here. One can find the closed form by calculating the integral, a bit long, but it's a nice journey to go. A 300 points bounty moment: After 2 years and 10 months since the problem has been posed no full solution has been provided. Is it possible to find a slick solution?",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'special-functions']"
30,"Why do we say ""radius"" of convergence?","Why do we say ""radius"" of convergence?",,"In an intuitive sense, I have never understood why a power series centered on $c$ cannot converge for some interval like $(c-3,c+2]$. Also, I have had a few professors casually mention that a series converges for a disk in the complex plane, centered on $c$ and with the radius of convergence as its radius. Is this just a deep result that I havent seen yet because I havent taken enough real/complex analysis? Or is there an obvious reason for this. Edit: perhaps a better way to ask my question: why is it that, if the series converges when $x$ is between $c$ and $c+a$, then it also converges for $x$ between $c$ and $c-a$","In an intuitive sense, I have never understood why a power series centered on $c$ cannot converge for some interval like $(c-3,c+2]$. Also, I have had a few professors casually mention that a series converges for a disk in the complex plane, centered on $c$ and with the radius of convergence as its radius. Is this just a deep result that I havent seen yet because I havent taken enough real/complex analysis? Or is there an obvious reason for this. Edit: perhaps a better way to ask my question: why is it that, if the series converges when $x$ is between $c$ and $c+a$, then it also converges for $x$ between $c$ and $c-a$",,"['real-analysis', 'complex-analysis', 'soft-question', 'power-series']"
31,Prove that $\lim_{t\to \infty} t\mu(\{x:f(x)\geq t\})=0$,Prove that,\lim_{t\to \infty} t\mu(\{x:f(x)\geq t\})=0,"Problem Suppose $f$ is a non-negative integrable function on a measure space $(X,\mathcal{A},\mu).$ Prove that $$\lim_{t\to \infty} t\mu(\{x:f(x)\geq t\})=0$$ Attempt Let $E_t=\{x:f(x)\geq t\}$ Note that $f\chi_{E_t}\leq f$. Since $f$ is integrable, the Lebesgue dominated convergence theorem tells us that $$\lim_{t\to\infty} \int f\chi_{E_t}\rightarrow \int \lim_{t\to\infty} f\chi_{E_t}=0,$$ since $\lim_{t\to\infty} f\chi_{E_t}=0$ a.e. $\ ^{(1)}$ The result follows from the fact that $$t\mu(E_t)\leq\int f\chi_{E_t}.$$ Question I am having trouble proving $(1)$. That is, how can I show that $f\chi_{E_t}\rightarrow 0$ a.e?","Problem Suppose $f$ is a non-negative integrable function on a measure space $(X,\mathcal{A},\mu).$ Prove that $$\lim_{t\to \infty} t\mu(\{x:f(x)\geq t\})=0$$ Attempt Let $E_t=\{x:f(x)\geq t\}$ Note that $f\chi_{E_t}\leq f$. Since $f$ is integrable, the Lebesgue dominated convergence theorem tells us that $$\lim_{t\to\infty} \int f\chi_{E_t}\rightarrow \int \lim_{t\to\infty} f\chi_{E_t}=0,$$ since $\lim_{t\to\infty} f\chi_{E_t}=0$ a.e. $\ ^{(1)}$ The result follows from the fact that $$t\mu(E_t)\leq\int f\chi_{E_t}.$$ Question I am having trouble proving $(1)$. That is, how can I show that $f\chi_{E_t}\rightarrow 0$ a.e?",,"['real-analysis', 'integration', 'measure-theory']"
32,$\lim_{n\rightarrow \infty} n \sum^\infty_{k=n} \frac1{2k(2k+1)}=\frac14$?,?,\lim_{n\rightarrow \infty} n \sum^\infty_{k=n} \frac1{2k(2k+1)}=\frac14,"This isn't a homework problem, just something that came up while I was studying measure theory. It is well known that the limit of the tails of any convergent series goes to 0. However, the problem that I have asks for the order in which such a limit vanishes. In particular, I wish to find $$\lim_{n\rightarrow \infty} n \sum^\infty_{k=n} \frac1{2k(2k+1)}.$$ Some playing around on Mathematica suggests that the limit should be $\frac14$. However, I'm having the worst time trying to show this with a straight-forward proof. As far as I can tell, there is no way to rewrite the tail series in terms of elementary functions of $n$ through the method of telescoping series or other similar series tricks. Am I missing something obvious, or is this limit really a bear to work through? I'd appreciate any help offered.","This isn't a homework problem, just something that came up while I was studying measure theory. It is well known that the limit of the tails of any convergent series goes to 0. However, the problem that I have asks for the order in which such a limit vanishes. In particular, I wish to find $$\lim_{n\rightarrow \infty} n \sum^\infty_{k=n} \frac1{2k(2k+1)}.$$ Some playing around on Mathematica suggests that the limit should be $\frac14$. However, I'm having the worst time trying to show this with a straight-forward proof. As far as I can tell, there is no way to rewrite the tail series in terms of elementary functions of $n$ through the method of telescoping series or other similar series tricks. Am I missing something obvious, or is this limit really a bear to work through? I'd appreciate any help offered.",,"['real-analysis', 'sequences-and-series', 'analysis']"
33,The smooth Nullstellensatz,The smooth Nullstellensatz,,"Let $n$ be a positive integer, let $f_1, \ldots, f_r : \mathbb{R}^n \to \mathbb{R}$ be smooth functions, let $Z_i = f_i^{-1} \{ 0 \} \subseteq \mathbb{R}^n$, and suppose $Z_1 \cap \cdots \cap Z_r = \emptyset$. Question. Must there exist smooth functions $g_1, \ldots, g_r : \mathbb{R}^n \to \mathbb{R}$ such that $f_1 g_1 + \cdots + f_r g_r = 1$? I believe the answer is no in general. On the other hand: It suffices to show that there exist smooth functions $g_1, \ldots, g_r : \mathbb{R}^n \to \mathbb{R}$ such that $f_1 g_1 + \cdots + f_r g_r$ vanishes nowhere, because we can rescale. Therefore a counterexample will have the property that, for any smooth functions $g_1, \ldots, g_r : \mathbb{R}^n \to \mathbb{R}$, $f_1 g_1 + \cdots + f_r g_r$ vanishes somewhere . If $0$ is a regular value for each $f_i$, then the answer is yes: take a partition of unity $h_1 + \cdots + h_r = 1$ where each $h_i$ has support contained in $\mathbb{R}^n \setminus Z_i$ and then define $g_i = h_i / f_i$. If we replace $\mathbb{R}$ with $\mathbb{C}$ and ""smooth"" with ""polynomial"", then this is a special case of Hilbert's Nullstellensatz.","Let $n$ be a positive integer, let $f_1, \ldots, f_r : \mathbb{R}^n \to \mathbb{R}$ be smooth functions, let $Z_i = f_i^{-1} \{ 0 \} \subseteq \mathbb{R}^n$, and suppose $Z_1 \cap \cdots \cap Z_r = \emptyset$. Question. Must there exist smooth functions $g_1, \ldots, g_r : \mathbb{R}^n \to \mathbb{R}$ such that $f_1 g_1 + \cdots + f_r g_r = 1$? I believe the answer is no in general. On the other hand: It suffices to show that there exist smooth functions $g_1, \ldots, g_r : \mathbb{R}^n \to \mathbb{R}$ such that $f_1 g_1 + \cdots + f_r g_r$ vanishes nowhere, because we can rescale. Therefore a counterexample will have the property that, for any smooth functions $g_1, \ldots, g_r : \mathbb{R}^n \to \mathbb{R}$, $f_1 g_1 + \cdots + f_r g_r$ vanishes somewhere . If $0$ is a regular value for each $f_i$, then the answer is yes: take a partition of unity $h_1 + \cdots + h_r = 1$ where each $h_i$ has support contained in $\mathbb{R}^n \setminus Z_i$ and then define $g_i = h_i / f_i$. If we replace $\mathbb{R}$ with $\mathbb{C}$ and ""smooth"" with ""polynomial"", then this is a special case of Hilbert's Nullstellensatz.",,"['real-analysis', 'differential-geometry', 'smooth-manifolds']"
34,How do I prove using the definition that the logarithmic function is continuous,How do I prove using the definition that the logarithmic function is continuous,,"Do not define the logarithmic function as an integral. Do not use that $e^x>x$ because this assumes the continuity of the exponential. I have no clue on how to solve this one. I guess I got it, but i'll leave this question here in case someone else has the same problem. $|\ln x - \ln a|<\epsilon \rightarrow -\epsilon < \ln \frac x a < \epsilon$ $ae^{-\epsilon} - a < x-a < ae^{\epsilon} - a$ (of course $a>0$) taking $\delta = \min \{ a(1 - e^{-\epsilon}), a(e^{\epsilon} -1) \} = a(1 - e^{-\epsilon})$ we're done. I guess. If anybody has another way of proofing please show me. The definitions of the logarithm function can be these: $\log : (0, +\infty) \rightarrow \mathbb{R} $ $ \log(x) + \log(y) = \log(xy) , \forall (x,y)$ both real greater then zero. Of course some basic properties come from this definition and you can use them. But you can also define it as the inverse of the exponential as long as you don't use the continuity of the inverse to prove it. A better definition may be: $\ln(x) = \lim_{n \rightarrow \infty} n(x^{\frac 1 n} -1)$ I need to prove the continuity of $f(x)=\log x$ using a $\epsilon-\delta$ proof","Do not define the logarithmic function as an integral. Do not use that $e^x>x$ because this assumes the continuity of the exponential. I have no clue on how to solve this one. I guess I got it, but i'll leave this question here in case someone else has the same problem. $|\ln x - \ln a|<\epsilon \rightarrow -\epsilon < \ln \frac x a < \epsilon$ $ae^{-\epsilon} - a < x-a < ae^{\epsilon} - a$ (of course $a>0$) taking $\delta = \min \{ a(1 - e^{-\epsilon}), a(e^{\epsilon} -1) \} = a(1 - e^{-\epsilon})$ we're done. I guess. If anybody has another way of proofing please show me. The definitions of the logarithm function can be these: $\log : (0, +\infty) \rightarrow \mathbb{R} $ $ \log(x) + \log(y) = \log(xy) , \forall (x,y)$ both real greater then zero. Of course some basic properties come from this definition and you can use them. But you can also define it as the inverse of the exponential as long as you don't use the continuity of the inverse to prove it. A better definition may be: $\ln(x) = \lim_{n \rightarrow \infty} n(x^{\frac 1 n} -1)$ I need to prove the continuity of $f(x)=\log x$ using a $\epsilon-\delta$ proof",,"['calculus', 'real-analysis', 'continuity']"
35,Definition of cluster point,Definition of cluster point,,"I'm studying if the book Multidimensional Real Analysis by Duistermaat and the definition of cluster point is: A point $a \in \mathbb{R}^n$ is said to be a cluster point of a subset $A$ if for every $\delta >0$ we have $B(a; \delta) \cap A \neq \emptyset$, where $B(a; \delta) = \{x \in \mathbb{R}^n \;|\; ||x-a||<\delta\}$ But in many other books and internet says that: A point $a \in \mathbb{R}^n$ is said to be a cluster point of a subset $A$ if for every $\delta >0$ we have $(B(a; \delta)-{a}) \cap A \neq \emptyset$, where $B(a; \delta) = \{x \in \mathbb{R}^n \;|\; ||x-a||<\delta\}$ It's easy to see that it isn't equivalent definitions. For example, by the first definition, the point $0$ is a cluster point of the set $S = \{0\}\cup[1,2]$, but it is not by the second one. Which definition is the usual?","I'm studying if the book Multidimensional Real Analysis by Duistermaat and the definition of cluster point is: A point $a \in \mathbb{R}^n$ is said to be a cluster point of a subset $A$ if for every $\delta >0$ we have $B(a; \delta) \cap A \neq \emptyset$, where $B(a; \delta) = \{x \in \mathbb{R}^n \;|\; ||x-a||<\delta\}$ But in many other books and internet says that: A point $a \in \mathbb{R}^n$ is said to be a cluster point of a subset $A$ if for every $\delta >0$ we have $(B(a; \delta)-{a}) \cap A \neq \emptyset$, where $B(a; \delta) = \{x \in \mathbb{R}^n \;|\; ||x-a||<\delta\}$ It's easy to see that it isn't equivalent definitions. For example, by the first definition, the point $0$ is a cluster point of the set $S = \{0\}\cup[1,2]$, but it is not by the second one. Which definition is the usual?",,"['real-analysis', 'multivariable-calculus']"
36,"Differentiable function with bounded derivative, yet not uniformly continuous","Differentiable function with bounded derivative, yet not uniformly continuous",,"It is well-known that if a differentiable function $f:I \to \mathbb{R}$ ($I$ an interval) has bounded derivative, then it is uniformly continuous.  On the other hand, there are differentiable functions, which are uniformly continuous, but whose derivative is unbounded.  My related question is as follows.  Does there exist a differentiable function $f:\mathbb{R} \to \mathbb{R}$, and a subset $X \subseteq \mathbb{R}$, such that $f'$ is bounded on $X$, and yet $f$ is not uniformly continuous on $X$?  Note that $X$ cannot be an interval, a finite disjoint union of intervals, nor can it be a discrete set.","It is well-known that if a differentiable function $f:I \to \mathbb{R}$ ($I$ an interval) has bounded derivative, then it is uniformly continuous.  On the other hand, there are differentiable functions, which are uniformly continuous, but whose derivative is unbounded.  My related question is as follows.  Does there exist a differentiable function $f:\mathbb{R} \to \mathbb{R}$, and a subset $X \subseteq \mathbb{R}$, such that $f'$ is bounded on $X$, and yet $f$ is not uniformly continuous on $X$?  Note that $X$ cannot be an interval, a finite disjoint union of intervals, nor can it be a discrete set.",,"['calculus', 'real-analysis', 'analysis', 'uniform-continuity']"
37,Follow-up regarding right-continuous $f:\mathbb{R} \to\mathbb{R}$ is Borel measurable,Follow-up regarding right-continuous  is Borel measurable,f:\mathbb{R} \to\mathbb{R},"I have a follow-up to another question here on math.stackexchange, Are right continuous functions measurable? .  The thread was a couple of years old, so I hope it's okay if I start a new question.  The person who answered the question claimed that the preimage of an open set under a right-continuous function $f: \mathbb{R} \to \mathbb{R}$ is a Borel measurable set because it is the countable union of half-open intervals.  I was trying to think through the ""countable union"" part.  If we have an open set $U \subseteq \mathbb{R}$, and let $A = f^{-1}(U)$, then certainly by definition if we take $A \cap \mathbb{Q}$, then for every $a \in A \cap \mathbb{Q}$, there is some $\delta > 0$ such that $[a,a+\delta) \subseteq A$, and thus $\bigcup_{a \in A \cap \mathbb{Q}} [a,a+\delta) \subseteq A$.  But then, why does the reverse containment hold (or does it hold at all)?  I tried to prove that $A \subseteq \bigcup_{a \in A \cap \mathbb{Q}} [a,a+\delta)$ using contradiction, but didn't get anywhere.  The best I could come up with was that if $b \in A$ and $b \notin \bigcup_{a \in A \cap \mathbb{Q}} [a,a+\delta)$, then because of the density of the rationals in the reals, we could write $\{ b \} \cup \bigcup_{n \in \mathbb{N}} [a_n,a_n+\delta_n) \subseteq A$, but this clearly didn't get me anywhere...","I have a follow-up to another question here on math.stackexchange, Are right continuous functions measurable? .  The thread was a couple of years old, so I hope it's okay if I start a new question.  The person who answered the question claimed that the preimage of an open set under a right-continuous function $f: \mathbb{R} \to \mathbb{R}$ is a Borel measurable set because it is the countable union of half-open intervals.  I was trying to think through the ""countable union"" part.  If we have an open set $U \subseteq \mathbb{R}$, and let $A = f^{-1}(U)$, then certainly by definition if we take $A \cap \mathbb{Q}$, then for every $a \in A \cap \mathbb{Q}$, there is some $\delta > 0$ such that $[a,a+\delta) \subseteq A$, and thus $\bigcup_{a \in A \cap \mathbb{Q}} [a,a+\delta) \subseteq A$.  But then, why does the reverse containment hold (or does it hold at all)?  I tried to prove that $A \subseteq \bigcup_{a \in A \cap \mathbb{Q}} [a,a+\delta)$ using contradiction, but didn't get anywhere.  The best I could come up with was that if $b \in A$ and $b \notin \bigcup_{a \in A \cap \mathbb{Q}} [a,a+\delta)$, then because of the density of the rationals in the reals, we could write $\{ b \} \cup \bigcup_{n \in \mathbb{N}} [a_n,a_n+\delta_n) \subseteq A$, but this clearly didn't get me anywhere...",,"['real-analysis', 'measure-theory', 'continuity']"
38,Proof that absolute continuity implies differentiability a.e.,Proof that absolute continuity implies differentiability a.e.,,Can somebody recommend a book/resource that provides a proof that absolute continuity of a function implies its almost-everywhere differentiability?,Can somebody recommend a book/resource that provides a proof that absolute continuity of a function implies its almost-everywhere differentiability?,,"['real-analysis', 'complex-analysis', 'analysis']"
39,How prove this $\lim_{n\to\infty}\sqrt{n}\cdot\sqrt[n]{l-a_{n}}=\frac{\sqrt{e}}{2}$,How prove this,\lim_{n\to\infty}\sqrt{n}\cdot\sqrt[n]{l-a_{n}}=\frac{\sqrt{e}}{2},"Define the sequence $\{a_{n}\}_{n\ge 2}$ by   $$a_{n}=\sqrt{1+\sqrt{2+\cdots+\sqrt{n}}}.$$ It is well known   $a_{n}\longrightarrow l$ for a certain real number $l$. Show that   $$\lim_{n\to\infty}\sqrt{n}\cdot\sqrt[n]{l-a_{n}}=\dfrac{\sqrt{e}}{2}.$$ This is a nice result, but I can't find a solution. Thank you everyone.","Define the sequence $\{a_{n}\}_{n\ge 2}$ by   $$a_{n}=\sqrt{1+\sqrt{2+\cdots+\sqrt{n}}}.$$ It is well known   $a_{n}\longrightarrow l$ for a certain real number $l$. Show that   $$\lim_{n\to\infty}\sqrt{n}\cdot\sqrt[n]{l-a_{n}}=\dfrac{\sqrt{e}}{2}.$$ This is a nice result, but I can't find a solution. Thank you everyone.",,"['real-analysis', 'sequences-and-series', 'limits']"
40,"Is $C([0,1])$ a ""subset"" of $L^\infty([0,1])$?","Is  a ""subset"" of ?","C([0,1]) L^\infty([0,1])","This is motivated from an exercise in real analysis: Prove that $C([0,1])$ is not dense in $L^\infty([0,1])$. My first question is how $C([0,1])$ is identified as a subset of $L^\infty([0,1])$? (I think one would never say something like ""$A$ is (not) dense in $B$"" if $A$ is not even a subset of $B$. ) First of all, $L^\infty([0,1])$ is defined as a quotient space, but $C([0,1])$ is a set of functions: $$ C([0,1]):=\{f:[0,1]\to{\Bbb R}\mid f \  \text{is continuous}\}. \tag{1} $$ I think one should also take $C([0,1])$ as  $$ C([0,1]):=\{f:[0,1]\to{\Bbb R}|f\sim g \  \text{for some g where g is continuous on}\ [0,1]\} \tag{2} $$ where $f\sim g$ if only if $f=g$ almost everywhere. But I've never read any textbook (PDE, measure theory, or functional analysis, etc) that defines $C([0,1])$ (or more generally $C(X)$ where $X\subset{\Bbb R}$ is compact) in this way before. Second question : Could anyone come up with a reference with such definition? [EDITED:]The original title doesn't reflect my point. I've changed it accordingly. [EDITED:] Some thoughts after reading the comments and answers: When one regards $C([0,1])$ as  a subset of $L^\infty([0,1])$, (1) is  not correct, and (2) would be not correct either. The final version I can come up with is  $$ C([0,1]):=\{f:[0,1]\to{\Bbb R}|f\sim g \  \text{for some g where g is continuous on}\ [0,1]\}\big/\sim. \tag{3} $$","This is motivated from an exercise in real analysis: Prove that $C([0,1])$ is not dense in $L^\infty([0,1])$. My first question is how $C([0,1])$ is identified as a subset of $L^\infty([0,1])$? (I think one would never say something like ""$A$ is (not) dense in $B$"" if $A$ is not even a subset of $B$. ) First of all, $L^\infty([0,1])$ is defined as a quotient space, but $C([0,1])$ is a set of functions: $$ C([0,1]):=\{f:[0,1]\to{\Bbb R}\mid f \  \text{is continuous}\}. \tag{1} $$ I think one should also take $C([0,1])$ as  $$ C([0,1]):=\{f:[0,1]\to{\Bbb R}|f\sim g \  \text{for some g where g is continuous on}\ [0,1]\} \tag{2} $$ where $f\sim g$ if only if $f=g$ almost everywhere. But I've never read any textbook (PDE, measure theory, or functional analysis, etc) that defines $C([0,1])$ (or more generally $C(X)$ where $X\subset{\Bbb R}$ is compact) in this way before. Second question : Could anyone come up with a reference with such definition? [EDITED:]The original title doesn't reflect my point. I've changed it accordingly. [EDITED:] Some thoughts after reading the comments and answers: When one regards $C([0,1])$ as  a subset of $L^\infty([0,1])$, (1) is  not correct, and (2) would be not correct either. The final version I can come up with is  $$ C([0,1]):=\{f:[0,1]\to{\Bbb R}|f\sim g \  \text{for some g where g is continuous on}\ [0,1]\}\big/\sim. \tag{3} $$",,['real-analysis']
41,Heine Borel Theorem $\iff$ Bolzano Weierstrass Theorem?,Heine Borel Theorem  Bolzano Weierstrass Theorem?,\iff,Are the statements of the Heine-Borel Thm and Bolzano-Weierstrass Thm equivalent?,Are the statements of the Heine-Borel Thm and Bolzano-Weierstrass Thm equivalent?,,"['real-analysis', 'general-topology']"
42,Why can't a neighborhood be a finite set?,Why can't a neighborhood be a finite set?,,"Rudin defines a neighborhood as follows: Let $X$ be a metric space endowed with a distance function $d$. A neighborhood of a point $p \in X$ is a set $N_r(p)$ consisting of all $q \in X$ such that $d(p,q) < r$ for some $r > 0$. He later proves two facts: every neighborhood is an open set, and every finite set is closed. But it would seem to me that a neighborhood as defined above could be finite. For example, let $X = \{1,2,3\}$ with $d(x,y) = |x-y|$ be a metric space . Consider the neighborhood around $p = 2$ of radius $0.5$, i.e.: the set of all points $q$ in $X$ such that $d(q,p)<0.5$. But the neighborhood is simply $\{2\}$. Therefore, the neighborhood is a finite set, and therefore closed. But every neighborhood is open. This is a contradiction. So my understanding of a neighborhood is broken somehow. It has further implications: if we define $E = \{2\}$, then the $0.5$-radius neighborhood is $\{2\}$, which is a subset of $E$, which means that $2$ is an interior point of $E$. Since $2$ is the only point in $E$, all points in $E$ are interior points, and $E$ is open. But $E$ is finite, and therefore closed. I'm probably missing something obvious. Can anyone spot my mistake?","Rudin defines a neighborhood as follows: Let $X$ be a metric space endowed with a distance function $d$. A neighborhood of a point $p \in X$ is a set $N_r(p)$ consisting of all $q \in X$ such that $d(p,q) < r$ for some $r > 0$. He later proves two facts: every neighborhood is an open set, and every finite set is closed. But it would seem to me that a neighborhood as defined above could be finite. For example, let $X = \{1,2,3\}$ with $d(x,y) = |x-y|$ be a metric space . Consider the neighborhood around $p = 2$ of radius $0.5$, i.e.: the set of all points $q$ in $X$ such that $d(q,p)<0.5$. But the neighborhood is simply $\{2\}$. Therefore, the neighborhood is a finite set, and therefore closed. But every neighborhood is open. This is a contradiction. So my understanding of a neighborhood is broken somehow. It has further implications: if we define $E = \{2\}$, then the $0.5$-radius neighborhood is $\{2\}$, which is a subset of $E$, which means that $2$ is an interior point of $E$. Since $2$ is the only point in $E$, all points in $E$ are interior points, and $E$ is open. But $E$ is finite, and therefore closed. I'm probably missing something obvious. Can anyone spot my mistake?",,"['real-analysis', 'general-topology']"
43,Slight generalization of an exercise in (blue) Rudin,Slight generalization of an exercise in (blue) Rudin,,"Exercise 7.20 of blue Rudin ( Principles of Mathematical Analysis ), 3rd edition, says: If $f$ is continuous on $[0,1]$ and if  $$\int_0^1f(x)x^n\,dx = 0, (n=0,1,2,\ldots),$$ prove that $f(x)=0$ on $[0,1].$ One proof: Let $\{p_n\}$ be a sequence of polynomials uniformly approximating $f.$ Then $p_nf\rightarrow f^2$ uniformly too. Then $$\lim_{n\to\infty}\int_0^1p_n(x)f(x)\,dx = 0 = \int_0^1f^2(x)\,dx,$$ since we can interchange the integral and the limit. Since $f^2$ is continuous, we have $f^2=0$ and thus $f=0$ too. Now, my question is if we drop the assumption that $\int_0^1f(x)\,dx = 0,$ does the result still hold? Slightly rephrased, If $f$ is continuous on $[0,1]$ and satisfies  $$\int_0^1f(x)x^n\,dx = 0, (n=1,2,3,\ldots),$$ is $f=0$ on $[0,1]?$ If $f(0) = 0$ then this is true. In this case, extend $f$ to an odd function on $[-1,1].$ Take a sequence of polynomials $\{q_n\}$ uniformly approximating $f.$ Then the sequence  $${\frac{q_n(x)-q_n(-x)}{2}}$$ is an approximation of $f$ without a constant term, and we can apply the same trick as above. So,we only need to look at the case when $f(0)\neq 0.$ Does anyone know a proof, or have a counterexample to this generalized statement?","Exercise 7.20 of blue Rudin ( Principles of Mathematical Analysis ), 3rd edition, says: If $f$ is continuous on $[0,1]$ and if  $$\int_0^1f(x)x^n\,dx = 0, (n=0,1,2,\ldots),$$ prove that $f(x)=0$ on $[0,1].$ One proof: Let $\{p_n\}$ be a sequence of polynomials uniformly approximating $f.$ Then $p_nf\rightarrow f^2$ uniformly too. Then $$\lim_{n\to\infty}\int_0^1p_n(x)f(x)\,dx = 0 = \int_0^1f^2(x)\,dx,$$ since we can interchange the integral and the limit. Since $f^2$ is continuous, we have $f^2=0$ and thus $f=0$ too. Now, my question is if we drop the assumption that $\int_0^1f(x)\,dx = 0,$ does the result still hold? Slightly rephrased, If $f$ is continuous on $[0,1]$ and satisfies  $$\int_0^1f(x)x^n\,dx = 0, (n=1,2,3,\ldots),$$ is $f=0$ on $[0,1]?$ If $f(0) = 0$ then this is true. In this case, extend $f$ to an odd function on $[-1,1].$ Take a sequence of polynomials $\{q_n\}$ uniformly approximating $f.$ Then the sequence  $${\frac{q_n(x)-q_n(-x)}{2}}$$ is an approximation of $f$ without a constant term, and we can apply the same trick as above. So,we only need to look at the case when $f(0)\neq 0.$ Does anyone know a proof, or have a counterexample to this generalized statement?",,['real-analysis']
44,On Some Properties of Hölder Continuous Functions,On Some Properties of Hölder Continuous Functions,,"The function space $H^{\alpha} (\Omega)$ for $0 < \alpha \le 1$,  is the set of functions: $$\{ f \in C^0(\Omega) : \sup_{x \neq y} \dfrac{|f(x) - f(y)|}{|x-y|^{\alpha}} < \infty \}$$ with the metric $d_{H^{\alpha}} = || f - g ||_{H^{\alpha}}$, where  $$||f||_{H^{\alpha}} = ||f||_{sup} + [f]_{H^{\alpha}} \text{ ,        } [f]_{H^{\alpha}} = \sup_{x \neq y}  \dfrac{|f(x) - f(y)|}{|x-y|^{\alpha}} $$ Now, if $0 < \alpha < \beta \le 1$, then $$[f]_{H^{\alpha}} \le 2 ||f||_{sup}^{1-\frac{\alpha}{\beta}}  [f]_{H^{\beta}}^{\frac{\alpha}{\beta}} \space \forall f \in H^{\beta}$$ And also, there is some constant $M$ so that: $$||f||_{H^{\alpha}} \le M ||f||_{sup}^{1-\frac{\alpha}{\beta}}  ||f||_{H^{\beta}}^{\frac{\alpha}{\beta}} \space \forall f \in H^{\beta}$$ These were some questions on a problem set: I have checked that $d_{H^{\alpha}}$ is a metric, and proved the two properties (in the second I found that $M = 2$ is sufficient). However, rather blindly. It's easy to show from the first that if $0 < \alpha < \beta \le 1$, then $H^{\beta} \subset H^{\alpha}$. What else do these formulas mean? Are they just some useful inequalities, or do they establish some connection between $H^{\beta}$ and $H^{\alpha}$? Thanks.","The function space $H^{\alpha} (\Omega)$ for $0 < \alpha \le 1$,  is the set of functions: $$\{ f \in C^0(\Omega) : \sup_{x \neq y} \dfrac{|f(x) - f(y)|}{|x-y|^{\alpha}} < \infty \}$$ with the metric $d_{H^{\alpha}} = || f - g ||_{H^{\alpha}}$, where  $$||f||_{H^{\alpha}} = ||f||_{sup} + [f]_{H^{\alpha}} \text{ ,        } [f]_{H^{\alpha}} = \sup_{x \neq y}  \dfrac{|f(x) - f(y)|}{|x-y|^{\alpha}} $$ Now, if $0 < \alpha < \beta \le 1$, then $$[f]_{H^{\alpha}} \le 2 ||f||_{sup}^{1-\frac{\alpha}{\beta}}  [f]_{H^{\beta}}^{\frac{\alpha}{\beta}} \space \forall f \in H^{\beta}$$ And also, there is some constant $M$ so that: $$||f||_{H^{\alpha}} \le M ||f||_{sup}^{1-\frac{\alpha}{\beta}}  ||f||_{H^{\beta}}^{\frac{\alpha}{\beta}} \space \forall f \in H^{\beta}$$ These were some questions on a problem set: I have checked that $d_{H^{\alpha}}$ is a metric, and proved the two properties (in the second I found that $M = 2$ is sufficient). However, rather blindly. It's easy to show from the first that if $0 < \alpha < \beta \le 1$, then $H^{\beta} \subset H^{\alpha}$. What else do these formulas mean? Are they just some useful inequalities, or do they establish some connection between $H^{\beta}$ and $H^{\alpha}$? Thanks.",,"['real-analysis', 'functional-analysis', 'holder-spaces']"
45,Minimize $\frac{1}{2}\sum_{k=1}^m (x_{k+1}-x_{k})^2$,Minimize,\frac{1}{2}\sum_{k=1}^m (x_{k+1}-x_{k})^2,"Given sequence: $$ \begin{cases} x_{n+1}(2\cos(\frac{\pi}{m})-x_n)=1,\forall n\geq 1\\ x_1=x\in\mathbb R,m\in\mathbb N,m\geq 2 \end{cases} $$ Minimize $$A=\frac{1}{2}\sum_{k=1}^m (x_{k+1}-x_{k})^2$$ Edit I've proved that $\frac{dA}{dx}|_{x=-1}=0$ (see my answer). The problem now is to show that $A_{\min}=A|_{x=-1}$ is the global minimum. The part below belongs to the original post $A$ is actually the area bounded by the zig-zag loop between $y=\frac{1}{2\cos(\frac{\pi}{m})-x}$ and $y=x$ . I tried it on a computer and observed that $A$ minimizes at $x=-1,\forall m$ , and other $m-1$ values of $x$ , resulting in the same minimal value of $A$ (the only minimum of $A$ ). I did try to prove the necessary condition $\frac{dA}{dx} |_{x=-1}=0$ , after some algebra, the equivalent equation is: $$\sum_{k=1}^m \left(x_k-2\cos\left(\frac{\pi}{m}\right)x_{k+1}^2+x_{k+1}^3\right)\prod_{i=1}^k x_i^2|_{x=-1}=0$$ I recognized a sort of symmetry inside the sum (sum of two terms (of the sum above) with indices adding up to $m$ is $0$ and the last term is $0$ as well) but I couldn't figure out how to deal with the product in the sum. How do I proceed from here? Also, with the necessary condition proved, how can I show that there is exactly one minimal value of $A$ for every $m$ ? [Feel free to check out this graph ]","Given sequence: Minimize Edit I've proved that (see my answer). The problem now is to show that is the global minimum. The part below belongs to the original post is actually the area bounded by the zig-zag loop between and . I tried it on a computer and observed that minimizes at , and other values of , resulting in the same minimal value of (the only minimum of ). I did try to prove the necessary condition , after some algebra, the equivalent equation is: I recognized a sort of symmetry inside the sum (sum of two terms (of the sum above) with indices adding up to is and the last term is as well) but I couldn't figure out how to deal with the product in the sum. How do I proceed from here? Also, with the necessary condition proved, how can I show that there is exactly one minimal value of for every ? [Feel free to check out this graph ]","
\begin{cases}
x_{n+1}(2\cos(\frac{\pi}{m})-x_n)=1,\forall n\geq 1\\
x_1=x\in\mathbb R,m\in\mathbb N,m\geq 2
\end{cases}
 A=\frac{1}{2}\sum_{k=1}^m (x_{k+1}-x_{k})^2 \frac{dA}{dx}|_{x=-1}=0 A_{\min}=A|_{x=-1} A y=\frac{1}{2\cos(\frac{\pi}{m})-x} y=x A x=-1,\forall m m-1 x A A \frac{dA}{dx} |_{x=-1}=0 \sum_{k=1}^m \left(x_k-2\cos\left(\frac{\pi}{m}\right)x_{k+1}^2+x_{k+1}^3\right)\prod_{i=1}^k x_i^2|_{x=-1}=0 m 0 0 A m","['real-analysis', 'sequences-and-series', 'optimization', 'recurrence-relations', 'convex-optimization']"
46,"If a continuous monotone function $f:[0,1]^2\to[0,1]$ aggregates nicely, is it ""multiplication in disguise""?","If a continuous monotone function  aggregates nicely, is it ""multiplication in disguise""?","f:[0,1]^2\to[0,1]","Suppose we have a continuous function $f:[0,1]^2\to [0,1]$ such that: $f(a,b)=f(b,a)$ $b\ge c \implies f(a,b) \ge f(a,c)$ $f(a,f(b,c))= f(f(a,b),c)$ $f(0,0)=0, f(1,1)=1$ In other words, $f$ is monotone increasing and surjective onto $[0,1]$ , and the result of turning a multiset of real numbers into a single real number by repeatedly replacing $a,b$ with $f(a,b)$ is independent of the order in which this aggregation is performed. Obviously, $f(a,b)=ab$ works. In general, if $g:[0,1]\to[0,1]$ is a strictly montone function (either increasing or decreasing) whose image is the whole interval, then $f(a,b)=g^{-1}(g(a)\cdot g(b))$ also works. Equivalently, if $h:[0,1]\to\mathbb{R}_{\ge0}\cup\{\infty\}$ is a strictly monotone function whose image is the full set of nonnegative real numbers along with $\infty$ , then taking $f(a,b) = h^{-1}(h(a)+h(b))$ works (where we take $x+\infty=\infty$ for all $x$ ). This is just a transformation of the above example with $h(x)=-\log(g(x))$ . Are there any other examples? In other words, do all such nicely-aggregating functions necessarily behave like a relabeling of addition/multiplication under some bijective map? One corollary of this that might either be provable without showing the full result or (if false) suggestive of a potential counterexample would be that $f(0,1)$ must equal either $0$ or $1$ .","Suppose we have a continuous function such that: In other words, is monotone increasing and surjective onto , and the result of turning a multiset of real numbers into a single real number by repeatedly replacing with is independent of the order in which this aggregation is performed. Obviously, works. In general, if is a strictly montone function (either increasing or decreasing) whose image is the whole interval, then also works. Equivalently, if is a strictly monotone function whose image is the full set of nonnegative real numbers along with , then taking works (where we take for all ). This is just a transformation of the above example with . Are there any other examples? In other words, do all such nicely-aggregating functions necessarily behave like a relabeling of addition/multiplication under some bijective map? One corollary of this that might either be provable without showing the full result or (if false) suggestive of a potential counterexample would be that must equal either or .","f:[0,1]^2\to [0,1] f(a,b)=f(b,a) b\ge c \implies f(a,b) \ge f(a,c) f(a,f(b,c))= f(f(a,b),c) f(0,0)=0, f(1,1)=1 f [0,1] a,b f(a,b) f(a,b)=ab g:[0,1]\to[0,1] f(a,b)=g^{-1}(g(a)\cdot g(b)) h:[0,1]\to\mathbb{R}_{\ge0}\cup\{\infty\} \infty f(a,b) = h^{-1}(h(a)+h(b)) x+\infty=\infty x h(x)=-\log(g(x)) f(0,1) 0 1","['real-analysis', 'functional-equations']"
47,Generalizing Scheffe's Lemma using only Convergence in Probability,Generalizing Scheffe's Lemma using only Convergence in Probability,,"I thought about this question recently, because we accidentally stated this lemma with convergence in probability instead of the usual almost sure convergence as an exercise. The usual proof with Fatou's lemma does not work in this case. It turns out that you can in fact generalize Scheffe's lemma to the following Generalized Scheffe's Lemma Assume that $(X_n)_{n\in\mathbb{N}}\subset L^1 $ converges in probability to $X_\infty\in L^1$ . Then the following statements are equivalent: $\mathbb{E}[|X_{n}|]\to \mathbb{E}[|X_{\infty}|]<\infty$ , as $n\to \infty$ . For all $\epsilon>0$ we have $\limsup_{n\to\infty} \mathbb{E}[|X_n|\mathbb{1}_{|X_\infty-X_n|> \epsilon}] \le \epsilon$ $\{X_\infty, X_1, X_2,\dots\}$ are uniformly integrable $X_n\to X_{\infty}$ in $L^1$ , as $n\to \infty$ This result is not new, see references in the comments, but I wanted it to become easier to find. So here is a proof.","I thought about this question recently, because we accidentally stated this lemma with convergence in probability instead of the usual almost sure convergence as an exercise. The usual proof with Fatou's lemma does not work in this case. It turns out that you can in fact generalize Scheffe's lemma to the following Generalized Scheffe's Lemma Assume that converges in probability to . Then the following statements are equivalent: , as . For all we have are uniformly integrable in , as This result is not new, see references in the comments, but I wanted it to become easier to find. So here is a proof.","(X_n)_{n\in\mathbb{N}}\subset L^1  X_\infty\in L^1 \mathbb{E}[|X_{n}|]\to \mathbb{E}[|X_{\infty}|]<\infty n\to \infty \epsilon>0 \limsup_{n\to\infty} \mathbb{E}[|X_n|\mathbb{1}_{|X_\infty-X_n|> \epsilon}] \le \epsilon \{X_\infty, X_1, X_2,\dots\} X_n\to X_{\infty} L^1 n\to \infty","['real-analysis', 'integration', 'probability-theory', 'measure-theory']"
48,Evaluate a sum by Ramanujan $\sum_{n=1}^{\infty} \frac{(1\cdot 3\dots (2n-1))^3}{(2\cdot 4\dots (2n))^3}\sum_{k=1}^{n}\frac{1}{2k-1}$,Evaluate a sum by Ramanujan,\sum_{n=1}^{\infty} \frac{(1\cdot 3\dots (2n-1))^3}{(2\cdot 4\dots (2n))^3}\sum_{k=1}^{n}\frac{1}{2k-1},"The Ramanujan sum is given below $$\left ( \frac{1}{2} \right )^3+\left ( \frac{1\cdot 3}{2\cdot 4} \right )^3\left ( 1+\frac{1}{3} \right )+\left ( \frac{1\cdot 3\cdot 5}{2\cdot 4\cdot 6} \right )^3\left ( 1+\frac{1}{3}+\frac{1}{5} \right )+\ldots =\frac{\pi^2}{6\Gamma ^4\left ( \frac{3}{4} \right )}$$ I tried to calculate it, but I couldn't finish it. I have a mistake somewhere or the integral is too complex at the end. I will be glad if you help me bring it \begin{aligned} \operatorname{LHS}&=\sum\limits_{n=1}^{\infty }\frac{\Gamma ^3\left ( n+\frac{1}{2} \right )}{\Gamma ^3\left ( \frac{1}{2} \right )\left ( n! \right )^3}\sum\limits_{k=1}^{n }\frac{1}{2k-1}=\sum\limits_{n=1}^{\infty }\frac{\Gamma ^3\left ( n+\frac{1}{2} \right )}{\Gamma ^3\left ( \frac{1}{2} \right )\left ( n! \right )^3}\int\limits_{0}^{1}\frac{1-x^{2n}}{1-x^2}dx=&&\\         &=\int\limits_{0}^{1}\frac{1}{1-x^2}\left [ \; _{3}F_2\left ( \frac{1}{2},\frac{1}{2},\frac{1}{2};1,1;1 \right )- \; _{3}F_{2}\left ( \frac{1}{2},\frac{1}{2},\frac{1}{2};1,1;x^2 \right ) \right ]dx=&&\\         &=\int\limits_{0}^{1}\frac{1}{1-x^2}\left [ \; _{2}F^2_1\left ( \frac{1}{4},\frac{1}{4};1;1 \right )- \; _{2}F^2_1\left ( \frac{1}{4},\frac{1}{4};1;x^2 \right ) \right ]dx=&&\\         &=\int\limits_{0}^{1}\frac{1}{1-x^2}\left [ \; _{2}F^2_1\left ( \frac{1}{2},\frac{1}{2};1;\frac{1}{2} \right )- \; _{2}F^2_1 \left ( \frac{1}{2},\frac{1}{2};1;\frac{1-\sqrt{1-x^2}}{2} \right ) \right ]dx=&&\\         &=\frac{4}{\pi^2}\int\limits_{0}^{1}\frac{1}{1-x^2}\left [ \mathcal{K}^2\left ( \frac{1}{\sqrt{2}} \right )-\mathcal{K}^2\left ( \sqrt{\frac{1-\sqrt{1-x^2}}{2}} \right ) \right ]dx=&&\\         &=\frac{4}{\pi^2}\int\limits_{0}^{\pi /2}\frac{1}{\cos t}\left [ \mathcal{K}^2\left ( \frac{1}{\sqrt{2}} \right )-\mathcal{K}^2\left ( \sin \frac{t}{2} \right ) \right ]dt \end{aligned}","The Ramanujan sum is given below I tried to calculate it, but I couldn't finish it. I have a mistake somewhere or the integral is too complex at the end. I will be glad if you help me bring it","\left ( \frac{1}{2} \right )^3+\left ( \frac{1\cdot 3}{2\cdot 4} \right )^3\left ( 1+\frac{1}{3} \right )+\left ( \frac{1\cdot 3\cdot 5}{2\cdot 4\cdot 6} \right )^3\left ( 1+\frac{1}{3}+\frac{1}{5} \right )+\ldots =\frac{\pi^2}{6\Gamma ^4\left ( \frac{3}{4} \right )} \begin{aligned}
\operatorname{LHS}&=\sum\limits_{n=1}^{\infty }\frac{\Gamma ^3\left ( n+\frac{1}{2} \right )}{\Gamma ^3\left ( \frac{1}{2} \right )\left ( n! \right )^3}\sum\limits_{k=1}^{n }\frac{1}{2k-1}=\sum\limits_{n=1}^{\infty }\frac{\Gamma ^3\left ( n+\frac{1}{2} \right )}{\Gamma ^3\left ( \frac{1}{2} \right )\left ( n! \right )^3}\int\limits_{0}^{1}\frac{1-x^{2n}}{1-x^2}dx=&&\\
        &=\int\limits_{0}^{1}\frac{1}{1-x^2}\left [ \; _{3}F_2\left ( \frac{1}{2},\frac{1}{2},\frac{1}{2};1,1;1 \right )- \; _{3}F_{2}\left ( \frac{1}{2},\frac{1}{2},\frac{1}{2};1,1;x^2 \right ) \right ]dx=&&\\
        &=\int\limits_{0}^{1}\frac{1}{1-x^2}\left [ \; _{2}F^2_1\left ( \frac{1}{4},\frac{1}{4};1;1 \right )- \; _{2}F^2_1\left ( \frac{1}{4},\frac{1}{4};1;x^2 \right ) \right ]dx=&&\\
        &=\int\limits_{0}^{1}\frac{1}{1-x^2}\left [ \; _{2}F^2_1\left ( \frac{1}{2},\frac{1}{2};1;\frac{1}{2} \right )- \; _{2}F^2_1 \left ( \frac{1}{2},\frac{1}{2};1;\frac{1-\sqrt{1-x^2}}{2} \right ) \right ]dx=&&\\
        &=\frac{4}{\pi^2}\int\limits_{0}^{1}\frac{1}{1-x^2}\left [ \mathcal{K}^2\left ( \frac{1}{\sqrt{2}} \right )-\mathcal{K}^2\left ( \sqrt{\frac{1-\sqrt{1-x^2}}{2}} \right ) \right ]dx=&&\\
        &=\frac{4}{\pi^2}\int\limits_{0}^{\pi /2}\frac{1}{\cos t}\left [ \mathcal{K}^2\left ( \frac{1}{\sqrt{2}} \right )-\mathcal{K}^2\left ( \sin \frac{t}{2} \right ) \right ]dt
\end{aligned}","['real-analysis', 'calculus', 'sequences-and-series']"
49,Solve $f(x)=\frac{1}{2}\int_{x-1}^{x+1}{f(t)}dt$,Solve,f(x)=\frac{1}{2}\int_{x-1}^{x+1}{f(t)}dt,"Let $f(x)\in\mathbb{C}(\mathbb{R})$ , $\inf\lim\limits_{|x|\to\infty}{f(x)}=0$ (that is $\inf\lim\limits_{x\to+\infty}{f(x)}=0$ and $\inf\lim\limits_{x\to-\infty}{f(x)}=0$ ), such that $$f(x)=\frac{1}{2}\int_{x-1}^{x+1}{f(t)}dt$$ Prove that: $$f(x)\equiv 0,\  x\in\mathbb{R}$$ My attempt I try to solve this equation, but I have trouble in it. Obviously, $f(x)= ax + b$ satisfies the equation. However, I don't know how to deal with the general case. I want to get some help. Thanks.","Let , (that is and ), such that Prove that: My attempt I try to solve this equation, but I have trouble in it. Obviously, satisfies the equation. However, I don't know how to deal with the general case. I want to get some help. Thanks.","f(x)\in\mathbb{C}(\mathbb{R}) \inf\lim\limits_{|x|\to\infty}{f(x)}=0 \inf\lim\limits_{x\to+\infty}{f(x)}=0 \inf\lim\limits_{x\to-\infty}{f(x)}=0 f(x)=\frac{1}{2}\int_{x-1}^{x+1}{f(t)}dt f(x)\equiv 0,\  x\in\mathbb{R} f(x)= ax + b","['real-analysis', 'calculus', 'integration']"
50,Verifying an increasing function on a closed interval is Riemann Integrable,Verifying an increasing function on a closed interval is Riemann Integrable,,"So I was having some difficulty coming up with a conceptual reason for why an increasing function on a closed interval would be Riemann Integrable, when without effort a proof seemingly fell out of some computations. Could anyone verify if it is correct? Thank you very much. Here's what I have. $\textbf{Proof:}$ Suppose that $f:[a,b]\to\mathbb{R}$ is increasing. WLOG we can assume $f(b)>f(a)$ , for else $f$ is constant, and trivially integrable. Let $\epsilon>0$ . Choose a partition $P=\{x_i\}_0^n$ of $[a,b]$ such that for all $1\leq i,j\leq n$ it follows that $x_i-x_{i-1}=x_j-x_{j-1}<\epsilon/(f(b)-f(a))$ . We compute $$U(f,P)-L(f,P)=\sum_{i=1}^n\bigg(\sup_{[x_{i-1},x_i]}f(x)-\inf_{[x_{i-1},x_i]}f(x)\bigg)(x_i-x_{i-1})$$ $$=\sum_{i=1}^n\bigg(\sup_{[x_{i-1},x_i]}f(x)-\inf_{[x_{i-1},x_i]}f(x)\bigg)(x_1-x_{0})$$ $$=\sum_{i=1}^n(f(x_i)-f(x_{i-1}))(x_1-x_{0})=(f(b)-f(a))(x_1-x_0)$$ $$<(f(b)-f(a))(\epsilon/(f(b)-f(a)))=\epsilon.$$ This completes the proof. $\square$","So I was having some difficulty coming up with a conceptual reason for why an increasing function on a closed interval would be Riemann Integrable, when without effort a proof seemingly fell out of some computations. Could anyone verify if it is correct? Thank you very much. Here's what I have. Suppose that is increasing. WLOG we can assume , for else is constant, and trivially integrable. Let . Choose a partition of such that for all it follows that . We compute This completes the proof.","\textbf{Proof:} f:[a,b]\to\mathbb{R} f(b)>f(a) f \epsilon>0 P=\{x_i\}_0^n [a,b] 1\leq i,j\leq n x_i-x_{i-1}=x_j-x_{j-1}<\epsilon/(f(b)-f(a)) U(f,P)-L(f,P)=\sum_{i=1}^n\bigg(\sup_{[x_{i-1},x_i]}f(x)-\inf_{[x_{i-1},x_i]}f(x)\bigg)(x_i-x_{i-1}) =\sum_{i=1}^n\bigg(\sup_{[x_{i-1},x_i]}f(x)-\inf_{[x_{i-1},x_i]}f(x)\bigg)(x_1-x_{0}) =\sum_{i=1}^n(f(x_i)-f(x_{i-1}))(x_1-x_{0})=(f(b)-f(a))(x_1-x_0) <(f(b)-f(a))(\epsilon/(f(b)-f(a)))=\epsilon. \square","['real-analysis', 'integration', 'proof-verification', 'riemann-integration', 'monotone-functions']"
51,Prove that we can't find effective bounds on the point guaranteed by the Mean Value Theorem.,Prove that we can't find effective bounds on the point guaranteed by the Mean Value Theorem.,,"I wish to show that we cannot find effective bounds on the point that the Mean Value Theorem proves to exist. To prove this loose statement, I aimed at the slightly more specific claim: For each real number $M$ and each real number $\xi$ that lies strictly between $0$ and $1$ , construct a function $f$ such that $$f(0)=0,\; f(1)=M,\;f\text{ is continuous on }[0,1],\; f\text{ is differentiable on }(0,1),\;\text{ and }\xi\text{ is the unique point strictly between 0 and 1 such that}\;f'(\xi)=M\,.$$ For the $M\neq 0$ and $\xi\neq 1/e$ case, we can show that $$g(x)=\begin{cases} 0&\text{ if }x=0,\\ 1/e&\text{ if }x=1\\ 1&\text{ if }x=\infty,\\ \sqrt[1-x]{x}&\text{ otherwise} \end{cases}$$ is strictly increasing and continuous on $[0,\infty]$ . Thus there is a unique positive $\alpha$ such that $g(\alpha)=\xi$ . In turn, we can define $f(x)=Mx^\alpha$ which will satisfy the claim. For the $M\neq 0$ and $\xi=1/e$ case, take the obvious continuous extension of $f(x)=M(x+x\ln(x))$ . For $M=0$ , we first choose $\alpha\geq 1$ and $\beta\geq 1$ such that $\frac{\alpha}{\alpha+\beta}=\xi$ . We then define $f(x)=x^\alpha(1-x)^\beta$ which will satisfy the claim. My question however is this: Can we construct such an $f$ to be a polynomial? An existential proof isn't desirable here, as I hope to use this family of polynomials as examples. It'd be useful to prove the uniqueness of $\xi$ through calculation (but possibly an appeal to monotonicity and the Intermediate Value Theorem).","I wish to show that we cannot find effective bounds on the point that the Mean Value Theorem proves to exist. To prove this loose statement, I aimed at the slightly more specific claim: For each real number and each real number that lies strictly between and , construct a function such that For the and case, we can show that is strictly increasing and continuous on . Thus there is a unique positive such that . In turn, we can define which will satisfy the claim. For the and case, take the obvious continuous extension of . For , we first choose and such that . We then define which will satisfy the claim. My question however is this: Can we construct such an to be a polynomial? An existential proof isn't desirable here, as I hope to use this family of polynomials as examples. It'd be useful to prove the uniqueness of through calculation (but possibly an appeal to monotonicity and the Intermediate Value Theorem).","M \xi 0 1 f f(0)=0,\; f(1)=M,\;f\text{ is continuous on }[0,1],\; f\text{ is differentiable on }(0,1),\;\text{ and }\xi\text{ is the unique point strictly between 0 and 1 such that}\;f'(\xi)=M\,. M\neq 0 \xi\neq 1/e g(x)=\begin{cases}
0&\text{ if }x=0,\\
1/e&\text{ if }x=1\\
1&\text{ if }x=\infty,\\
\sqrt[1-x]{x}&\text{ otherwise}
\end{cases} [0,\infty] \alpha g(\alpha)=\xi f(x)=Mx^\alpha M\neq 0 \xi=1/e f(x)=M(x+x\ln(x)) M=0 \alpha\geq 1 \beta\geq 1 \frac{\alpha}{\alpha+\beta}=\xi f(x)=x^\alpha(1-x)^\beta f \xi",['calculus']
52,A problem about the differential mean value theorem $2ηf(1)+(c^2-1)f'(η)=f(ξ)$,A problem about the differential mean value theorem,2ηf(1)+(c^2-1)f'(η)=f(ξ),"Assume that the function $f : \left[0, 1\right] \to \mathbb{R}$ is continuous on $\left[0,1\right]$ and is differentiable on $\left(0,1\right)$ . Let $c \in \left(0,1\right)$ . Prove that there exist $\xi, \eta \in \left(0, 1\right)$ such that \begin{align} 2 \eta f\left(1\right) + \left(c^2 - 1\right) f^\prime\left(\eta\right) = f\left(\xi\right) . \end{align} I tried to use the Lagrange mean value theorem and the Rolle mean value theorem on $[0,1]$ , but failed.","Assume that the function is continuous on and is differentiable on . Let . Prove that there exist such that I tried to use the Lagrange mean value theorem and the Rolle mean value theorem on , but failed.","f : \left[0, 1\right] \to \mathbb{R} \left[0,1\right] \left(0,1\right) c \in \left(0,1\right) \xi, \eta \in \left(0, 1\right) \begin{align}
2 \eta f\left(1\right) + \left(c^2 - 1\right) f^\prime\left(\eta\right) = f\left(\xi\right) .
\end{align} [0,1]","['real-analysis', 'analysis', 'proof-writing', 'proof-explanation']"
53,How can I calculate $\lim_{n \rightarrow \infty} \frac {e^n(2n)!}{(4n)^nn!}$,How can I calculate,\lim_{n \rightarrow \infty} \frac {e^n(2n)!}{(4n)^nn!},How should I get the result $$\lim_{n \rightarrow \infty} \frac {e^n(2n)!}{(4n)^nn!}=\sqrt2$$ without relying on Stirling's formula or the Central Limit Theorem? I am totally clueless. Can somebody help me?,How should I get the result $$\lim_{n \rightarrow \infty} \frac {e^n(2n)!}{(4n)^nn!}=\sqrt2$$ without relying on Stirling's formula or the Central Limit Theorem? I am totally clueless. Can somebody help me?,,"['calculus', 'real-analysis', 'probability', 'analysis', 'limits']"
54,Higher-order Lipschitz conditions?,Higher-order Lipschitz conditions?,,"There is a sequence of conditions on functions generalizing Lipschitz continuity; these conditions bear the same relationship to higher-order derivatives that Lipschitz continuity bears to first-order derivatives.  Are these known and under what name? To be specific, let $I$ be an interval in the real line, $f$ a real-valued function on $I$ , and $C$ a positive real number. $f$ is $0$ -Lipschitz (aka bounded ) on $I$ with constant $C$ if, for all $a \in I$ , $\lvert{f(a)}\rvert \leq C$ ; $f$ is $1$ -Lipschitz (aka Lipschitz ) on $I$ with constant $C$ if, for all $a < b \in I$ , $\left| \frac {f(b) - f(a)} {b - a} \right| \leq C$ ; $f$ is $2$ -Lipschitz on $I$ with constant $C$ if, for all $a < b < c \in I$ , $\left| \frac {\frac {f(c) - f(b)} {c - b} - \frac {f(b) - f(a)} {b - a}} {c - a} \right| \leq \frac 1 2 C$ ; $f$ is $3$ -Lipschitz on $I$ with constant $C$ if, for all $a < b < c < d \in I$ , $\left| \frac {\frac {\frac {f(d) - f(c)} {d - c} - \frac {f(c) - f(b)} {c - b}} {d - b} - \frac {\frac {f(c) - f(b)} {c - b} - \frac {f(b) - f(a)} {b - a}} {c - a}} {d - a} \right| \leq \frac 1 6 C$ ; etc. In general, $f$ is $n$ -Lipschitz on $I$ with constant $C$ if, for all $a_0 < \cdots < a_n \in I$ , $$ \left| \frac {\det \begin{bmatrix} 1 & 1 & \cdots & 1 & 1 \\ a_0 & a_1 & \cdots & a_{n-1} & a_n \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ a_0^{n-1} & a_1^{n-1} & \cdots & a_{n-1}^{n-1} & a_n^{n-1} \\ f(a_0) & f(a_1) & \cdots & f(a_{n-1}) & f(a_n) \end{bmatrix}} {\det \begin{bmatrix} 1 & 1 & \cdots & 1 & 1 \\ a_0 & a_1 & \cdots & a_{n-1} & a_n \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ a_0^{n-1} & a_1^{n-1} & \cdots & a_{n-1}^{n-1} & a_n^{n-1} \\ a_0^n & a_1^n & \cdots & a_{n-1}^n & a_n^n \end{bmatrix}} \right| \leq \frac 1 {n!} C .$$ (The denominator is a Vandermonde determinant, and the numerator is the same with the highest powers replaced by the values of the function.) Of course, in this case, then the same inequality holds if $a_0, \ldots, a_n$ are out of order (since this amounts to swapping columns in the determinants), as long as they are distinct, and they don't even have to be distinct if you clear fractions: $$ n! \left| \det \begin{bmatrix} 1 & 1 & \cdots & 1 & 1 \\ a_0 & a_1 & \cdots & a_{n-1} & a_n \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ a_0^{n-1} & a_1^{n-1} & \cdots & a_{n-1}^{n-1} & a_n^{n-1} \\ f(a_0) & f(a_1) & \cdots & f(a_{n-1}) & f(a_n) \end{bmatrix} \right| \leq C \left| \det \begin{bmatrix} 1 & 1 & \cdots & 1 & 1 \\ a_0 & a_1 & \cdots & a_{n-1} & a_n \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ a_0^{n-1} & a_1^{n-1} & \cdots & a_{n-1}^{n-1} & a_n^{n-1} \\ a_0^n & a_1^n & \cdots & a_{n-1}^n & a_n^n \end{bmatrix} \right| ;$$ if the list $(a_0, \ldots, a_n)$ has any repetition, then this just says that $0 = 0$ .  But I like to think of using an increasing list, since that shows where the formulas come from.  For example, the expression inside the absolute value in the $2$ -Lipschitz condition is a sort of second-order difference quotient, saying how much the difference quotient (which appears in the ordinary $1$ -Lipschitz condition) changes as we move from $(a,b)$ to $(b,c)$ , relative to the overall range.  (I don't have any slick reason for why the result can be neatly packed up as a ratio of determinants, but it's easy enough to prove this using induction and basic facts about Vandermonde determinants.) Some basic results: Assuming that $f$ is differentiable $n$ times on $I$ , then $f$ is $n$ -Lipschitz on $I$ with constant $C$ if and only if the $n$ th derivative $f^{(n)}$ is bounded (on $I$ with constant $C$ ). More generally, assuming that $f$ is differentiable $k$ times on $I$ , then $f$ is $n$ -Lipschitz if and only if $f^{(k)}$ is $(n - k)$ -Lipschitz.  (The factorials guarantee the same constant.) I'd like to say that, if $f$ is $n$ -Lipschitz, then $f^{(n)}$ exists almost everywhere, still bounded by the same constant, but I haven't proved this. I don't know how to make sense of this in an arbitrary metric space, or even in $\mathbb{R}^2$ .  Neither the form with the higher-order difference quotient nor the form with the determinants makes sense a priori for even a $2$ -Lipschitz function.  The metric-space derivative is defined by essentially putting absolute values around every subtraction in a difference quotient, changing $b - a$ into $\lvert{b - a}\rvert$ , which is reinterpreted as $d(a,b)$ .  But for the $2$ -Lipschitz condition, this applies the absolute value too soon, before a necessary additional subtraction, so it's not the same. Anyway, if anybody has seen anything like this or has any thoughts on it, then I'm interested.","There is a sequence of conditions on functions generalizing Lipschitz continuity; these conditions bear the same relationship to higher-order derivatives that Lipschitz continuity bears to first-order derivatives.  Are these known and under what name? To be specific, let be an interval in the real line, a real-valued function on , and a positive real number. is -Lipschitz (aka bounded ) on with constant if, for all , ; is -Lipschitz (aka Lipschitz ) on with constant if, for all , ; is -Lipschitz on with constant if, for all , ; is -Lipschitz on with constant if, for all , ; etc. In general, is -Lipschitz on with constant if, for all , (The denominator is a Vandermonde determinant, and the numerator is the same with the highest powers replaced by the values of the function.) Of course, in this case, then the same inequality holds if are out of order (since this amounts to swapping columns in the determinants), as long as they are distinct, and they don't even have to be distinct if you clear fractions: if the list has any repetition, then this just says that .  But I like to think of using an increasing list, since that shows where the formulas come from.  For example, the expression inside the absolute value in the -Lipschitz condition is a sort of second-order difference quotient, saying how much the difference quotient (which appears in the ordinary -Lipschitz condition) changes as we move from to , relative to the overall range.  (I don't have any slick reason for why the result can be neatly packed up as a ratio of determinants, but it's easy enough to prove this using induction and basic facts about Vandermonde determinants.) Some basic results: Assuming that is differentiable times on , then is -Lipschitz on with constant if and only if the th derivative is bounded (on with constant ). More generally, assuming that is differentiable times on , then is -Lipschitz if and only if is -Lipschitz.  (The factorials guarantee the same constant.) I'd like to say that, if is -Lipschitz, then exists almost everywhere, still bounded by the same constant, but I haven't proved this. I don't know how to make sense of this in an arbitrary metric space, or even in .  Neither the form with the higher-order difference quotient nor the form with the determinants makes sense a priori for even a -Lipschitz function.  The metric-space derivative is defined by essentially putting absolute values around every subtraction in a difference quotient, changing into , which is reinterpreted as .  But for the -Lipschitz condition, this applies the absolute value too soon, before a necessary additional subtraction, so it's not the same. Anyway, if anybody has seen anything like this or has any thoughts on it, then I'm interested.","I f I C f 0 I C a \in I \lvert{f(a)}\rvert \leq C f 1 I C a < b \in I \left| \frac {f(b) - f(a)} {b - a} \right| \leq C f 2 I C a < b < c \in I \left| \frac {\frac {f(c) - f(b)} {c - b} - \frac {f(b) - f(a)} {b - a}} {c - a} \right| \leq \frac 1 2 C f 3 I C a < b < c < d \in I \left| \frac {\frac {\frac {f(d) - f(c)} {d - c} - \frac {f(c) - f(b)} {c - b}} {d - b} - \frac {\frac {f(c) - f(b)} {c - b} - \frac {f(b) - f(a)} {b - a}} {c - a}} {d - a} \right| \leq \frac 1 6 C f n I C a_0 < \cdots < a_n \in I  \left| \frac {\det \begin{bmatrix} 1 & 1 & \cdots & 1 & 1 \\ a_0 & a_1 & \cdots & a_{n-1} & a_n \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ a_0^{n-1} & a_1^{n-1} & \cdots & a_{n-1}^{n-1} & a_n^{n-1} \\ f(a_0) & f(a_1) & \cdots & f(a_{n-1}) & f(a_n) \end{bmatrix}} {\det \begin{bmatrix} 1 & 1 & \cdots & 1 & 1 \\ a_0 & a_1 & \cdots & a_{n-1} & a_n \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ a_0^{n-1} & a_1^{n-1} & \cdots & a_{n-1}^{n-1} & a_n^{n-1} \\ a_0^n & a_1^n & \cdots & a_{n-1}^n & a_n^n \end{bmatrix}} \right| \leq \frac 1 {n!} C . a_0, \ldots, a_n  n! \left| \det \begin{bmatrix} 1 & 1 & \cdots & 1 & 1 \\ a_0 & a_1 & \cdots & a_{n-1} & a_n \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ a_0^{n-1} & a_1^{n-1} & \cdots & a_{n-1}^{n-1} & a_n^{n-1} \\ f(a_0) & f(a_1) & \cdots & f(a_{n-1}) & f(a_n) \end{bmatrix} \right| \leq C \left| \det \begin{bmatrix} 1 & 1 & \cdots & 1 & 1 \\ a_0 & a_1 & \cdots & a_{n-1} & a_n \\ \vdots & \vdots & \ddots & \vdots & \vdots \\ a_0^{n-1} & a_1^{n-1} & \cdots & a_{n-1}^{n-1} & a_n^{n-1} \\ a_0^n & a_1^n & \cdots & a_{n-1}^n & a_n^n \end{bmatrix} \right| ; (a_0, \ldots, a_n) 0 = 0 2 1 (a,b) (b,c) f n I f n I C n f^{(n)} I C f k I f n f^{(k)} (n - k) f n f^{(n)} \mathbb{R}^2 2 b - a \lvert{b - a}\rvert d(a,b) 2","['real-analysis', 'lipschitz-functions']"
55,Convolution of L^p and L^q function is uniformly continuous or not?,Convolution of L^p and L^q function is uniformly continuous or not?,,"This is a homework question (the due date has passed) and I have been thinking of it for a while. We are asked to prove or disprove the following statement: $f \in L^p(\mathbb{R}), g \in L^p(\mathbb{R}), 1 < p, q < \infty, \frac{1}{p} + \frac{1}{q} = 1$. $f*g(x) = \int f(x-y)g(y)dy$ is uniformly continuous or not? I know the convolution of a $L^1$ and a $L^{\infty}$ function is uniformly continuous. Below is my attempt, yet I feel I probably did not use Holder's inequality correctly: Consider a fixed $x \in \mathbb{R}$ and any $a \in \mathbb{R}$, then  $$|f*g(x-a) - f*g(x)| = | \int f(x-a-y)g(y)dy - \int f(x-y)g(y)dy| \\  = | \int f(x-y)g(y-a)dy - \int f(x-y)g(y)dy| \\  = | \int f(x-y)[g(y-a) - g(y)]dy| \\  \leq \int | f(x-y)[g(y-a) - g(y)] | dy \\  \leq ||f||_p ||h||_q $$ where $h(y) = g(y-a) - g(y), 1 < p, q < \infty, \frac{1}{p} + \frac{1}{q} = 1$. Since the difference between $f*g(x-a)$ and $f*g(x)$ for any fixed $x$ and any $a$ is bounded, I can choose $a$ such that $||h||_q < \frac{\varepsilon}{|| f ||_p}$, so $|f*g(u) - f*g(x)| < \varepsilon$ for any arbitrary $\varepsilon > 0$ and any $u \in (x-a, x+a)$. I thought I used Holder's inequality correctly, since I was considering some fixed $x$, meaning that I can consider $f(x-y)$ as a function in $y$. Could anyone check my solution? I am really unsure about my use of Holder's inequality.","This is a homework question (the due date has passed) and I have been thinking of it for a while. We are asked to prove or disprove the following statement: $f \in L^p(\mathbb{R}), g \in L^p(\mathbb{R}), 1 < p, q < \infty, \frac{1}{p} + \frac{1}{q} = 1$. $f*g(x) = \int f(x-y)g(y)dy$ is uniformly continuous or not? I know the convolution of a $L^1$ and a $L^{\infty}$ function is uniformly continuous. Below is my attempt, yet I feel I probably did not use Holder's inequality correctly: Consider a fixed $x \in \mathbb{R}$ and any $a \in \mathbb{R}$, then  $$|f*g(x-a) - f*g(x)| = | \int f(x-a-y)g(y)dy - \int f(x-y)g(y)dy| \\  = | \int f(x-y)g(y-a)dy - \int f(x-y)g(y)dy| \\  = | \int f(x-y)[g(y-a) - g(y)]dy| \\  \leq \int | f(x-y)[g(y-a) - g(y)] | dy \\  \leq ||f||_p ||h||_q $$ where $h(y) = g(y-a) - g(y), 1 < p, q < \infty, \frac{1}{p} + \frac{1}{q} = 1$. Since the difference between $f*g(x-a)$ and $f*g(x)$ for any fixed $x$ and any $a$ is bounded, I can choose $a$ such that $||h||_q < \frac{\varepsilon}{|| f ||_p}$, so $|f*g(u) - f*g(x)| < \varepsilon$ for any arbitrary $\varepsilon > 0$ and any $u \in (x-a, x+a)$. I thought I used Holder's inequality correctly, since I was considering some fixed $x$, meaning that I can consider $f(x-y)$ as a function in $y$. Could anyone check my solution? I am really unsure about my use of Holder's inequality.",,"['real-analysis', 'lp-spaces', 'uniform-continuity', 'holder-inequality']"
56,Are the analytic functions dense in the space of continuous functions?,Are the analytic functions dense in the space of continuous functions?,,"We can consider functions defined on the real or complex numbers in any finite number of dimensions. The functions are defined everywhere. They need not have limited support. They may be defined over open or closed subsets of R^n or C^n (typing on my phone so avoiding latex) or everywhere. In defining continuous functions, continuity is considered in the context of the topology induced by the Euclidean metric. Distance between functions can be taken to be the distance in the L2 norm (the integral over the entire domain of the squared difference between two functions). Which of these assumptions, if changed, might change the answer to the question? For example, we might want to be able to approximate continuous functions with analytic functions. But we might also wish to do this confidently for as wide a class of functions as possible: for instance, what about piecewise continuous functions with finite or countable points of discontinuity? We can consider widening the class of functions to include either of the latter two. Moreover, we might want to impose the strictest convergence criteria as possible for the approximations. For example, we might want to strengthen the requirement from closeness in L^2 to closeness that is uniform or close to uniform (e.g. uniform but for a finite set of points, i.e. ""almost everywhere""), where closeness is viewed in the sense of convergence of a sequence of successively better approximations. So consider the stated (numbered) assumptions, but feel free to comment on whether the answer would still hold for more favorable assumptions given that the goal is to approximate functions as stated.","We can consider functions defined on the real or complex numbers in any finite number of dimensions. The functions are defined everywhere. They need not have limited support. They may be defined over open or closed subsets of R^n or C^n (typing on my phone so avoiding latex) or everywhere. In defining continuous functions, continuity is considered in the context of the topology induced by the Euclidean metric. Distance between functions can be taken to be the distance in the L2 norm (the integral over the entire domain of the squared difference between two functions). Which of these assumptions, if changed, might change the answer to the question? For example, we might want to be able to approximate continuous functions with analytic functions. But we might also wish to do this confidently for as wide a class of functions as possible: for instance, what about piecewise continuous functions with finite or countable points of discontinuity? We can consider widening the class of functions to include either of the latter two. Moreover, we might want to impose the strictest convergence criteria as possible for the approximations. For example, we might want to strengthen the requirement from closeness in L^2 to closeness that is uniform or close to uniform (e.g. uniform but for a finite set of points, i.e. ""almost everywhere""), where closeness is viewed in the sense of convergence of a sequence of successively better approximations. So consider the stated (numbered) assumptions, but feel free to comment on whether the answer would still hold for more favorable assumptions given that the goal is to approximate functions as stated.",,"['real-analysis', 'complex-analysis', 'analysis']"
57,"Show that the extension of $f : S → \mathbb{R}$ to the closure of its domain, $\overline{S}$, is unique","Show that the extension of  to the closure of its domain, , is unique",f : S → \mathbb{R} \overline{S},"I trying to solve the following question Assume that $f : S → \mathbb{R}$ is a uniformly continuous function   deﬁned on a subset $S$ of a metric space $M$. 1. Prove that $f$ extends to a uniformly continuous function $f : \overline{S} → \mathbb{R}$. 2. Prove that $\overline{f}$ is the unique continuous extension of $f$ to a function deﬁned on $\overline{S}$. 3. Prove the same things when $\mathbb{R}$ is replaced with a       complete metric space $N$. For the first question, I show that for some sequence $x_n \in S$ that converges to  $\hat{x} \in \overline{S},$ $\overline{f}(x_n)$ converges to $\overline{f}(\hat{x})$ since uniformly continuous functions take Cauchy sequences to Cauchy sequences and since $\mathbb{R}$ is complete. However, I feel like while this shows that $\overline{f}(x_n)$ converges, it does not fully establish that $\overline{f}(x_n)$ converges to $\overline{f}(\hat{x})$. Is this proof sufficient to show that $f$ extends to a uniformly continuous function $f : \overline{S} → \mathbb{R}$? For number 3, the same proof applies since I used the completeness of $\mathbb{R}$ as the basis for my proof. However, I don't understand how I would show that $\overline{f}$ is the unique extension of $f$. Is it sufficient to say that since all sequences in $S$ must converge to their unique limits in $\overline{S}$, $\overline{f}$ must also be unique?","I trying to solve the following question Assume that $f : S → \mathbb{R}$ is a uniformly continuous function   deﬁned on a subset $S$ of a metric space $M$. 1. Prove that $f$ extends to a uniformly continuous function $f : \overline{S} → \mathbb{R}$. 2. Prove that $\overline{f}$ is the unique continuous extension of $f$ to a function deﬁned on $\overline{S}$. 3. Prove the same things when $\mathbb{R}$ is replaced with a       complete metric space $N$. For the first question, I show that for some sequence $x_n \in S$ that converges to  $\hat{x} \in \overline{S},$ $\overline{f}(x_n)$ converges to $\overline{f}(\hat{x})$ since uniformly continuous functions take Cauchy sequences to Cauchy sequences and since $\mathbb{R}$ is complete. However, I feel like while this shows that $\overline{f}(x_n)$ converges, it does not fully establish that $\overline{f}(x_n)$ converges to $\overline{f}(\hat{x})$. Is this proof sufficient to show that $f$ extends to a uniformly continuous function $f : \overline{S} → \mathbb{R}$? For number 3, the same proof applies since I used the completeness of $\mathbb{R}$ as the basis for my proof. However, I don't understand how I would show that $\overline{f}$ is the unique extension of $f$. Is it sufficient to say that since all sequences in $S$ must converge to their unique limits in $\overline{S}$, $\overline{f}$ must also be unique?",,"['real-analysis', 'sequences-and-series', 'metric-spaces', 'uniform-continuity', 'complete-spaces']"
58,Calculating the sum of $\sum\frac{n^2-2}{n!}$ [duplicate],Calculating the sum of  [duplicate],\sum\frac{n^2-2}{n!},This question already has answers here : What's the value of $\sum\limits_{k=1}^{\infty}\frac{k^2}{k!}$? (8 answers) Closed 7 years ago . Calculating the sum of $\sum\frac{n^2-2}{n!}$ I want to calculate the sum of $\sum_{n=0}^{+\infty}\frac{n^2-2}{n!}$. This is what I have done so far: $$ \sum_{n=0}^{+\infty}\frac{n^2-2}{n!}=\sum_{n=0}^{+\infty}\frac{n^2}{n!}-2\sum_{n=0}^{+\infty}\frac{1}{n!}=\sum_{n=0}^{+\infty}\frac{n}{(n-1)!}-2e$$ And here I don't know how to deal with the $\frac{n}{(n-1)!} $. Any tips? EDIT: One of the answers recommends to write down the sum as follows: $$\sum_{n=0}^{+\infty}\frac{n^2-2}{n!}=\sum_{n=0}^{+\infty}\frac{n(n-1)}{n!} + \sum_{n=0}^{+\infty}\frac{n}{n!}-2\sum_{n=0}^{+\infty}\frac{1}{n!}$$ Which equals to: $$\sum_{n=0}^{+\infty}\frac{n(n-1)}{n!} + \sum_{n=0}^{+\infty}\frac{n}{n!}-2\sum_{n=0}^{+\infty}\frac{1}{n!}=\sum_{n=0}^{+\infty}\frac{(n-1)}{(n-1)!}+\sum_{n=0}^{+\infty}\frac{1}{(n-1)!} -2e$$ But here I have negative factorials. What should I do next? Or can I just say that $\sum_{n=0}^{+\infty}\frac{1}{(n-1)!}=e$?,This question already has answers here : What's the value of $\sum\limits_{k=1}^{\infty}\frac{k^2}{k!}$? (8 answers) Closed 7 years ago . Calculating the sum of $\sum\frac{n^2-2}{n!}$ I want to calculate the sum of $\sum_{n=0}^{+\infty}\frac{n^2-2}{n!}$. This is what I have done so far: $$ \sum_{n=0}^{+\infty}\frac{n^2-2}{n!}=\sum_{n=0}^{+\infty}\frac{n^2}{n!}-2\sum_{n=0}^{+\infty}\frac{1}{n!}=\sum_{n=0}^{+\infty}\frac{n}{(n-1)!}-2e$$ And here I don't know how to deal with the $\frac{n}{(n-1)!} $. Any tips? EDIT: One of the answers recommends to write down the sum as follows: $$\sum_{n=0}^{+\infty}\frac{n^2-2}{n!}=\sum_{n=0}^{+\infty}\frac{n(n-1)}{n!} + \sum_{n=0}^{+\infty}\frac{n}{n!}-2\sum_{n=0}^{+\infty}\frac{1}{n!}$$ Which equals to: $$\sum_{n=0}^{+\infty}\frac{n(n-1)}{n!} + \sum_{n=0}^{+\infty}\frac{n}{n!}-2\sum_{n=0}^{+\infty}\frac{1}{n!}=\sum_{n=0}^{+\infty}\frac{(n-1)}{(n-1)!}+\sum_{n=0}^{+\infty}\frac{1}{(n-1)!} -2e$$ But here I have negative factorials. What should I do next? Or can I just say that $\sum_{n=0}^{+\infty}\frac{1}{(n-1)!}=e$?,,"['real-analysis', 'sequences-and-series', 'summation']"
59,Intuition for Conditional Expectation,Intuition for Conditional Expectation,,"It seems like NNT aka Nero in The Black Swan (2007) is giving the law of iterated expectations that involve filtrations in a heuristic way by matching the everyday usage of the word 'expect' with the mathematical definition of expectation (a Riemann integral or sum in elementary probability theory; a Lebesgue or Riemann-Stieltjes integral in advanced probability theory). I'm guessing the correspondence between the precise and the heuristic is as follows: Heuristic: $\text{If I expect to expect} \ \color{green}{\text{something}} \ \text{at} \ \color{red}{\text{some date in the future}},$ $\text{then I already expect that} \ \color{green}{\text{something}} \ \text{at} \ \color{purple}{\text{present}}.$ Precise in the case of one non-trivial $\sigma-$ algebra, $$E[E[\color{green}{X}|\color{red}{\mathscr F_t}]] = E[\color{green}{X}|\color{purple}{\mathscr F_0}] (= E[\color{green}{X}])$$ Or Precise in the case of two non-trivial $\sigma-$ algebras, $$E[E[\color{green}{X}|\color{red}{\mathscr F_{t+1}}]|\color{purple}{\mathscr F_t}] = E[\color{green}{X}|\color{purple}{\mathscr F_t}]$$ where $\color{green}{X}$ is a random variable in $(\Omega, \mathscr F, \mathbb P)$ with filtration $\{\mathscr F_t\}_{t\in I}$ where $I \subseteq \mathbb R$ An example I thought of for second case I currently expect to expect tomorrow at 1pm that someone will try to prank me tomorrow at 3pm if and only if I currently expect someone to prank me tomorrow at 3pm Where 3pm refers to the larger $\mathscr F_{.}$ and 1pm refers to the smaller $\mathscr F_{.}$ . 1. Anything wrong? If so, please explain why, and suggest how it may be improved. 2. How to similarly heuristically explain law of iterated expectation when we don't have filtrations? For example $$E[E[\color{green}{X}|\color{blue}{Y}]] = E[\color{green}{X}]$$ $\text{If I expect to expect} \ \color{green}{\text{something}}$ _____ $\color{blue}{(?)}$ _____, $\text{then I (?)expect that} \ \color{green}{\text{something}} $ _____ $(?)$ _____ What I tried: I guess we can consider X as payoff of playing one game out of Y possible games. So the amount we expect to win is equal to the (probabilistically) weighted average of the amounts we expect to win in each of the Y games. But I wanted to use similar language to the one with filtrations so I'm looking for something like If I expect to expect to win 5 dollars (something something) then I expect to win 5 dollars Of course without the something something we have simply $E[E[X]] = E[X]$","It seems like NNT aka Nero in The Black Swan (2007) is giving the law of iterated expectations that involve filtrations in a heuristic way by matching the everyday usage of the word 'expect' with the mathematical definition of expectation (a Riemann integral or sum in elementary probability theory; a Lebesgue or Riemann-Stieltjes integral in advanced probability theory). I'm guessing the correspondence between the precise and the heuristic is as follows: Heuristic: Precise in the case of one non-trivial algebra, Or Precise in the case of two non-trivial algebras, where is a random variable in with filtration where An example I thought of for second case I currently expect to expect tomorrow at 1pm that someone will try to prank me tomorrow at 3pm if and only if I currently expect someone to prank me tomorrow at 3pm Where 3pm refers to the larger and 1pm refers to the smaller . 1. Anything wrong? If so, please explain why, and suggest how it may be improved. 2. How to similarly heuristically explain law of iterated expectation when we don't have filtrations? For example _____ _____, _____ _____ What I tried: I guess we can consider X as payoff of playing one game out of Y possible games. So the amount we expect to win is equal to the (probabilistically) weighted average of the amounts we expect to win in each of the Y games. But I wanted to use similar language to the one with filtrations so I'm looking for something like If I expect to expect to win 5 dollars (something something) then I expect to win 5 dollars Of course without the something something we have simply","\text{If I expect to expect} \ \color{green}{\text{something}} \ \text{at} \ \color{red}{\text{some date in the future}}, \text{then I already expect that} \ \color{green}{\text{something}} \ \text{at} \ \color{purple}{\text{present}}. \sigma- E[E[\color{green}{X}|\color{red}{\mathscr F_t}]] = E[\color{green}{X}|\color{purple}{\mathscr F_0}] (= E[\color{green}{X}]) \sigma- E[E[\color{green}{X}|\color{red}{\mathscr F_{t+1}}]|\color{purple}{\mathscr F_t}] = E[\color{green}{X}|\color{purple}{\mathscr F_t}] \color{green}{X} (\Omega, \mathscr F, \mathbb P) \{\mathscr F_t\}_{t\in I} I \subseteq \mathbb R \mathscr F_{.} \mathscr F_{.} E[E[\color{green}{X}|\color{blue}{Y}]] = E[\color{green}{X}] \text{If I expect to expect} \ \color{green}{\text{something}} \color{blue}{(?)} \text{then I (?)expect that} \ \color{green}{\text{something}}  (?) E[E[X]] = E[X]","['real-analysis', 'probability', 'probability-theory', 'measure-theory', 'conditional-expectation']"
60,Continuous function that is uniformly continuous on a dense subset,Continuous function that is uniformly continuous on a dense subset,,"Let $(X,d_X)$ and $(Y,d_Y)$ be two metric spaces, $E \subset X$ a dense set and $f:X \to Y$ be a continuous function that is uniformly continuous on E.   Is $f$ then uniformly continuous on $X$? If yes, is the following proof right? If not, where is the error in my proof? Proof: Let $\epsilon > 0$ be given. Since $f$ is uniformly continuous on $E$, there is some $\delta > 0$ such that $$\forall_{x,y \in E}\;d_X(x,y)<\delta \Rightarrow d_Y(f(x),f(y))< \frac{\epsilon}{3}$$ Now let $x,y \in X$ be arbitrary with $d_X(x,y) < \frac{\delta}{3}$. Because $f$ is continuous at $x$ and $y$ there are $\lambda_x$ and $\lambda_y$ such that $$\forall_{z \in X}\;d_X(z,x)<\lambda_x \Rightarrow d_Y(f(z),f(y)) < \frac{\epsilon}{3}$$ and $$\forall_{z \in X}\;d_X(z,y)<\lambda_x \Rightarrow d_Y(f(z),f(y)) < \frac{\epsilon}{3}$$ We set $k:= \min\{\lambda_x, \lambda_y, \delta/3\}$ and find (since $E$ is dense in $X$) $x', y' \in E$ such that $d_X(x,x') < k$ and $d_X(y,y') <k$. But then we have $$d_X(x',y') \leq d_X(x',x)+d_X(x,y)+d_X(y,y') < \frac{\delta}{3}+\frac{\delta}{3}+\frac{\delta}{3} = \delta$$ and thus $$d_Y(f(x),f(y))\leq d_Y(f(x),f(x'))+d_Y(f(x'),f(y'))+d_Y(f(y'),f(y)) < \frac{\epsilon}{3}+\frac{\epsilon}{3}+\frac{\epsilon}{3} = \epsilon$$","Let $(X,d_X)$ and $(Y,d_Y)$ be two metric spaces, $E \subset X$ a dense set and $f:X \to Y$ be a continuous function that is uniformly continuous on E.   Is $f$ then uniformly continuous on $X$? If yes, is the following proof right? If not, where is the error in my proof? Proof: Let $\epsilon > 0$ be given. Since $f$ is uniformly continuous on $E$, there is some $\delta > 0$ such that $$\forall_{x,y \in E}\;d_X(x,y)<\delta \Rightarrow d_Y(f(x),f(y))< \frac{\epsilon}{3}$$ Now let $x,y \in X$ be arbitrary with $d_X(x,y) < \frac{\delta}{3}$. Because $f$ is continuous at $x$ and $y$ there are $\lambda_x$ and $\lambda_y$ such that $$\forall_{z \in X}\;d_X(z,x)<\lambda_x \Rightarrow d_Y(f(z),f(y)) < \frac{\epsilon}{3}$$ and $$\forall_{z \in X}\;d_X(z,y)<\lambda_x \Rightarrow d_Y(f(z),f(y)) < \frac{\epsilon}{3}$$ We set $k:= \min\{\lambda_x, \lambda_y, \delta/3\}$ and find (since $E$ is dense in $X$) $x', y' \in E$ such that $d_X(x,x') < k$ and $d_X(y,y') <k$. But then we have $$d_X(x',y') \leq d_X(x',x)+d_X(x,y)+d_X(y,y') < \frac{\delta}{3}+\frac{\delta}{3}+\frac{\delta}{3} = \delta$$ and thus $$d_Y(f(x),f(y))\leq d_Y(f(x),f(x'))+d_Y(f(x'),f(y'))+d_Y(f(y'),f(y)) < \frac{\epsilon}{3}+\frac{\epsilon}{3}+\frac{\epsilon}{3} = \epsilon$$",,"['real-analysis', 'proof-verification', 'continuity', 'uniform-continuity']"
61,"$f'\in \mathcal R([0,1])$ , then $\lim_{n \to \infty} \sum_{k=1}^n f(\frac kn) - n \int_{0 }^1 f(x)dx=\frac{f(1)-f(0)}2$?",", then ?","f'\in \mathcal R([0,1]) \lim_{n \to \infty} \sum_{k=1}^n f(\frac kn) - n \int_{0 }^1 f(x)dx=\frac{f(1)-f(0)}2","If $f:[0,1]\to \mathbb R$ is a differentiable function with continuous derivative then I can show that $$ \lim_{n \to \infty} \left[ \sum_{k=1}^n f\!\left(\dfrac kn \right) - n \int_0^1 f(x)\,dx \right] = \frac{f(1)-f(0)}2. $$ My question is: does this fact remain true if we only assume that $f$ is differentiable (on $[0,1]$)? Or just that the derivative of $f$ is bounded on $[0,1]$ and Riemann integrable? (I know that just the continuity of $f$ does not lead to the identity.)","If $f:[0,1]\to \mathbb R$ is a differentiable function with continuous derivative then I can show that $$ \lim_{n \to \infty} \left[ \sum_{k=1}^n f\!\left(\dfrac kn \right) - n \int_0^1 f(x)\,dx \right] = \frac{f(1)-f(0)}2. $$ My question is: does this fact remain true if we only assume that $f$ is differentiable (on $[0,1]$)? Or just that the derivative of $f$ is bounded on $[0,1]$ and Riemann integrable? (I know that just the continuity of $f$ does not lead to the identity.)",,"['real-analysis', 'integration']"
62,How to show this cover of $\mathbb{Q}$ doesn't cover $\mathbb{R}$?,How to show this cover of  doesn't cover ?,\mathbb{Q} \mathbb{R},"Let $\{q_n : n \in \mathbb{N}\}$ be an enumeration of $\mathbb{Q}$ and define $\mathcal{O} = \{I_n : n \in \mathbb{N}\}$ being $$I_n = \left(q_n - \frac{1}{2^n}, q_n + \frac{1}{2^n}\right).$$ It is obvious that $\mathcal{O}$ is an open cover of $\mathbb{Q}$, but I want to show this is not a cover of $\mathbb{R}$. One possible way that I've already seem is to see that the total length of the intervals is $2\sum_{n=1}^\infty 2^{-n}=2$ while the length of $\mathbb{R}$ is $+\infty$. Although this works, I'm trying to find another way to prove this result. One thing that is intuitively clear is that if $n\to \infty$ then the intervals shrink as small as desired around the rational midpoint, since $1/2^n \to 0$ as $n\to \infty$. I thought on using this to show that there is some irrational in the ""middle"" of two such $I_n$ for large enough $n$, but I don't even know how to start this. Anyway, how can I prove this result without using the measure argument? Is my idea correct? If so, how can it be made rigorous?","Let $\{q_n : n \in \mathbb{N}\}$ be an enumeration of $\mathbb{Q}$ and define $\mathcal{O} = \{I_n : n \in \mathbb{N}\}$ being $$I_n = \left(q_n - \frac{1}{2^n}, q_n + \frac{1}{2^n}\right).$$ It is obvious that $\mathcal{O}$ is an open cover of $\mathbb{Q}$, but I want to show this is not a cover of $\mathbb{R}$. One possible way that I've already seem is to see that the total length of the intervals is $2\sum_{n=1}^\infty 2^{-n}=2$ while the length of $\mathbb{R}$ is $+\infty$. Although this works, I'm trying to find another way to prove this result. One thing that is intuitively clear is that if $n\to \infty$ then the intervals shrink as small as desired around the rational midpoint, since $1/2^n \to 0$ as $n\to \infty$. I thought on using this to show that there is some irrational in the ""middle"" of two such $I_n$ for large enough $n$, but I don't even know how to start this. Anyway, how can I prove this result without using the measure argument? Is my idea correct? If so, how can it be made rigorous?",,"['real-analysis', 'alternative-proof', 'rational-numbers']"
63,"If $f(0)=f(1)=f(2)=0$, $\forall x, \exists c, f(x)=\frac{1}{6}x(x-1)(x-2)f'''(c)$","If ,","f(0)=f(1)=f(2)=0 \forall x, \exists c, f(x)=\frac{1}{6}x(x-1)(x-2)f'''(c)","Let $f:[0,2]\to \mathbb R$ be a $C^3$ function such that $f(0)=f(1)=f(2)=0$ Prove that $\forall x\in[0,2], \exists c\in[0,2], f(x)=\frac{1}{6}x(x-1)(x-2)f'''(c)$ This problem got me stuck. I guess one has to use the mean value theorem at some point. Using Rolle, there exists $0<\xi_1<\eta<\xi_2$ such that $f'(\xi_1)=f'(\xi_2)=0$ and $f''(\eta)=0$ .","Let be a function such that Prove that This problem got me stuck. I guess one has to use the mean value theorem at some point. Using Rolle, there exists such that and .","f:[0,2]\to \mathbb R C^3 f(0)=f(1)=f(2)=0 \forall x\in[0,2], \exists c\in[0,2], f(x)=\frac{1}{6}x(x-1)(x-2)f'''(c) 0<\xi_1<\eta<\xi_2 f'(\xi_1)=f'(\xi_2)=0 f''(\eta)=0","['real-analysis', 'derivatives']"
64,Find a rigorous reference that prove the following integration by parts formula in higher dimension?,Find a rigorous reference that prove the following integration by parts formula in higher dimension?,,"My professor in the real analysis class had state the following in class but forgot to put the reference of this formula in the power point slide. The formula for integration by parts can be extended to functions of several variables. Instead of an interval one needs to integrate over an n-dimensional set. Also, one replaces the derivative with a partial derivative. More specifically, suppose $\Omega$ is an open bounded subset of $\mathbb R^n$ with a piece-wise smooth boundary $\Gamma$. If $u$ and $v$ are two continuously differentiable functions on the closure of $\Omega$, then the formula for integration by parts is $\int_{\Omega} \frac{\partial u}{\partial x_i} v \,d\Omega = \int_{\Gamma} u v \, \hat\nu_i \,d\Gamma - \int_{\Omega} u \frac{\partial v}{\partial x_i} \, d\Omega$ where $\hat{\mathbf{\nu}}$ is the outward unit surface normal to $\Gamma$, $\hat\nu_i$ is its $i$-th component, and $i$ ranges from $1$ to $n$. Find a rigorous reference that prove the following integration by parts formula in higher dimension?","My professor in the real analysis class had state the following in class but forgot to put the reference of this formula in the power point slide. The formula for integration by parts can be extended to functions of several variables. Instead of an interval one needs to integrate over an n-dimensional set. Also, one replaces the derivative with a partial derivative. More specifically, suppose $\Omega$ is an open bounded subset of $\mathbb R^n$ with a piece-wise smooth boundary $\Gamma$. If $u$ and $v$ are two continuously differentiable functions on the closure of $\Omega$, then the formula for integration by parts is $\int_{\Omega} \frac{\partial u}{\partial x_i} v \,d\Omega = \int_{\Gamma} u v \, \hat\nu_i \,d\Gamma - \int_{\Omega} u \frac{\partial v}{\partial x_i} \, d\Omega$ where $\hat{\mathbf{\nu}}$ is the outward unit surface normal to $\Gamma$, $\hat\nu_i$ is its $i$-th component, and $i$ ranges from $1$ to $n$. Find a rigorous reference that prove the following integration by parts formula in higher dimension?",,"['real-analysis', 'integration', 'multivariable-calculus', 'reference-request']"
65,Evaluate the general infinite square root [duplicate],Evaluate the general infinite square root [duplicate],,"This question already has answers here : $\sqrt{7\sqrt{7\sqrt{7\sqrt{7\sqrt{7\cdots}}}}}$ approximation [closed] (7 answers) Closed 8 years ago . $$x = \sqrt{n\sqrt{n\sqrt{n}} \cdots}$$ I see that: $$x = \sqrt{nx}$$ $$x^2 -nx = 0$$ Them: $$x(x - n) = 0 \implies x \in \{0, n\}$$ How should I reject the $x = 0$ solution?  (any level proof is fine, analysis, calculus etc...)","This question already has answers here : $\sqrt{7\sqrt{7\sqrt{7\sqrt{7\sqrt{7\cdots}}}}}$ approximation [closed] (7 answers) Closed 8 years ago . $$x = \sqrt{n\sqrt{n\sqrt{n}} \cdots}$$ I see that: $$x = \sqrt{nx}$$ $$x^2 -nx = 0$$ Them: $$x(x - n) = 0 \implies x \in \{0, n\}$$ How should I reject the $x = 0$ solution?  (any level proof is fine, analysis, calculus etc...)",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'proof-writing']"
66,Derivative of bilinear forms,Derivative of bilinear forms,,"I want to solve the following problems: Let $f:\mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ be a bilinear form.   Prove that it's differential is $$ Df_{(x,y)}(a,b) = f(x,b) + f(a,y).$$ Let $f:\mathbb{R}^3 \times \mathbb{R}^3 \to \mathbb{R}^3$ be the cross product funtion, that is, $f(x,y) = x \times y$. Calulate it's derivate in the point $(x,y)$. I know the definition of differentiability for funtions $f:\mathbb{R}^n \to \mathbb{R}^m$. But here I am working with other funtions, so I don't know how to start. Thanks.","I want to solve the following problems: Let $f:\mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ be a bilinear form.   Prove that it's differential is $$ Df_{(x,y)}(a,b) = f(x,b) + f(a,y).$$ Let $f:\mathbb{R}^3 \times \mathbb{R}^3 \to \mathbb{R}^3$ be the cross product funtion, that is, $f(x,y) = x \times y$. Calulate it's derivate in the point $(x,y)$. I know the definition of differentiability for funtions $f:\mathbb{R}^n \to \mathbb{R}^m$. But here I am working with other funtions, so I don't know how to start. Thanks.",,"['calculus', 'real-analysis', 'multivariable-calculus']"
67,"If f is differentiable on (a,b) continuous at a and f has bounded derivative must f be right differentiable at a?","If f is differentiable on (a,b) continuous at a and f has bounded derivative must f be right differentiable at a?",,"If $f$ is differentiable on $(a,b)$ continuous at a, and $f$ has bounded derivative, must $f$ be right differentiable at $a$? In case answer to previous question is true, is the statement still true if we replace bounded derivative with finite vertical distance travelled by $f$? To clarify what I mean by finite vertical distance travelled, is there a constant $K$ such that for all $e>0$ if we let $g$ be the derivative of $f$ on $(a+e,b-e)$ then the integral of $|g|$ on $[a+e,b-e]$ is less than $K$?","If $f$ is differentiable on $(a,b)$ continuous at a, and $f$ has bounded derivative, must $f$ be right differentiable at $a$? In case answer to previous question is true, is the statement still true if we replace bounded derivative with finite vertical distance travelled by $f$? To clarify what I mean by finite vertical distance travelled, is there a constant $K$ such that for all $e>0$ if we let $g$ be the derivative of $f$ on $(a+e,b-e)$ then the integral of $|g|$ on $[a+e,b-e]$ is less than $K$?",,"['calculus', 'real-analysis', 'derivatives', 'definite-integrals']"
68,Prove that the distance between 2 Cauchy sequences is convergent.,Prove that the distance between 2 Cauchy sequences is convergent.,,"Here is the exact question: Let $(S,d)$ be a metric space. Let $(p_n)$ and $(q_n)$ be two Cauchy sequences in $(S,d)$(note that these two sequences are not necessarily convergent since $(S,d)$ is not necessarily complete). Define a sequence $a_n = d(p_n, q_n)\in\mathbb{R}$. Prove that $(a_n)$ is a convergent sequence. My attempt: Since $p_n$ is Cauchy, there exists $N_1$ such that $m,n> N_1 \implies d(p_m, p_n)< \epsilon$. In particular, $d(p_m, p_n)< (m-n)\epsilon$ for $m,n> N_1$ Since $q_n$ is Cauchy, $d(q_m, q_n) < (m-n)\epsilon$ for $m,n> N_2$ Thus for $N= \max(N_1, N_2)$, and $m,n> N, d(p_n, q_n) = d[(d(p_N, q_N) +...+ d(p_m, q_m)),0]$, which is less than $(m-n)\epsilon- (m-n)\epsilon= 0$, and thus $<\epsilon$ I don't think the final line of my argument is correct.","Here is the exact question: Let $(S,d)$ be a metric space. Let $(p_n)$ and $(q_n)$ be two Cauchy sequences in $(S,d)$(note that these two sequences are not necessarily convergent since $(S,d)$ is not necessarily complete). Define a sequence $a_n = d(p_n, q_n)\in\mathbb{R}$. Prove that $(a_n)$ is a convergent sequence. My attempt: Since $p_n$ is Cauchy, there exists $N_1$ such that $m,n> N_1 \implies d(p_m, p_n)< \epsilon$. In particular, $d(p_m, p_n)< (m-n)\epsilon$ for $m,n> N_1$ Since $q_n$ is Cauchy, $d(q_m, q_n) < (m-n)\epsilon$ for $m,n> N_2$ Thus for $N= \max(N_1, N_2)$, and $m,n> N, d(p_n, q_n) = d[(d(p_N, q_N) +...+ d(p_m, q_m)),0]$, which is less than $(m-n)\epsilon- (m-n)\epsilon= 0$, and thus $<\epsilon$ I don't think the final line of my argument is correct.",,"['real-analysis', 'convergence-divergence', 'metric-spaces', 'cauchy-sequences']"
69,Proof that arithmetic and geometric mean converge,Proof that arithmetic and geometric mean converge,,"I need some help with understanding a part of this proof and also writing it up correctly. Given $a_n\geq a_{n+1}\geq b_{n+1} \geq b_n$ with $a_1=a$ and $b_1=b$. I am also given that  $$a_{n+1}=\frac{a_n+b_n}{2}$$ and $$b_{n+1}=\sqrt{a_nb_n}$$ I need to show that sequences ${a_n}$ and ${b_n}$ converges and that ${a_n}$ and ${b_n}$ have the same limit. I am told to use the monotonic convergence theorem to prove that both sequences converges and I have the following proof: Notice that {$a_n$} is monotonically decreasing while {$b_n$}  is monotonically increasing.  Since {$a_n$}  is bounded above by supremum $a_1$ below by its infimum $b_1$, {$a_n$} according to the monotonic convergence theorem has to converge. Similarly, notice that {$b_n$} is bounded below by infimum $b$ and supremum $a$.  By monotonic convergence theorem {$b_n$} must also converge as well. Next, I am told to show that {$a_n$} and {$b_n$} have the same limit.  In other words, if [$a_n-b_n$] as n tends to infinity must be 0.  For this part, it seems to be the case that one can prove it by just showing that  $a_{n+1} - b_{n+1} \leq (1/2) (a_n - b_n) $.  And I know you can just show this by using the definition of the arithmetic mean, which is  $a_{n+1} - b_{n+1} \leq a_{n+1} - b_n = (1/2) (a_n - b_n)$.  Why is that?  It seems incompletely and not so obvious to me.  An explanation here would help. Please help me edit my proof (what I have already) and clarify my understanding","I need some help with understanding a part of this proof and also writing it up correctly. Given $a_n\geq a_{n+1}\geq b_{n+1} \geq b_n$ with $a_1=a$ and $b_1=b$. I am also given that  $$a_{n+1}=\frac{a_n+b_n}{2}$$ and $$b_{n+1}=\sqrt{a_nb_n}$$ I need to show that sequences ${a_n}$ and ${b_n}$ converges and that ${a_n}$ and ${b_n}$ have the same limit. I am told to use the monotonic convergence theorem to prove that both sequences converges and I have the following proof: Notice that {$a_n$} is monotonically decreasing while {$b_n$}  is monotonically increasing.  Since {$a_n$}  is bounded above by supremum $a_1$ below by its infimum $b_1$, {$a_n$} according to the monotonic convergence theorem has to converge. Similarly, notice that {$b_n$} is bounded below by infimum $b$ and supremum $a$.  By monotonic convergence theorem {$b_n$} must also converge as well. Next, I am told to show that {$a_n$} and {$b_n$} have the same limit.  In other words, if [$a_n-b_n$] as n tends to infinity must be 0.  For this part, it seems to be the case that one can prove it by just showing that  $a_{n+1} - b_{n+1} \leq (1/2) (a_n - b_n) $.  And I know you can just show this by using the definition of the arithmetic mean, which is  $a_{n+1} - b_{n+1} \leq a_{n+1} - b_n = (1/2) (a_n - b_n)$.  Why is that?  It seems incompletely and not so obvious to me.  An explanation here would help. Please help me edit my proof (what I have already) and clarify my understanding",,"['real-analysis', 'analysis', 'proof-verification', 'proof-writing']"
70,Solution to $y(x) + y'(x) + y''(x) + y'''(x) + \cdots = 0$,Solution to,y(x) + y'(x) + y''(x) + y'''(x) + \cdots = 0,"Is there a non-trivial solution to the following differential equation? $$y(x) + y'(x) + y''(x) + y'''(x) + \cdots= 0$$ That is, is there a smooth function $y : \mathbb{R} \to \mathbb{R}$ such that for each $x$, the series $$\sum_{n = 0}^{\infty}\frac{d^n y}{dx^n}(x)$$ converges to zero.","Is there a non-trivial solution to the following differential equation? $$y(x) + y'(x) + y''(x) + y'''(x) + \cdots= 0$$ That is, is there a smooth function $y : \mathbb{R} \to \mathbb{R}$ such that for each $x$, the series $$\sum_{n = 0}^{\infty}\frac{d^n y}{dx^n}(x)$$ converges to zero.",,"['real-analysis', 'analysis', 'ordinary-differential-equations']"
71,Using normal families to bound a complex integral,Using normal families to bound a complex integral,,"I am trying to prove that $$\int_{\partial T(Q)} |F'(z)| \,ds(z) \lesssim \int\int_{T(Q)} |F'(z)| |\varphi'(z)|^2 \log \frac{1}{|z|} \,dx\, dy$$ This is an estimate on page $6$ of this paper by Peter Jones, and the author states that this follows ""by normal families and"" and knowing that $\varphi'$ has Bloch norm at most $6$. The symbol $\lesssim$ means $\le$ with constants that should not depend on $F$, $\varphi$ or $Q$. For notation, $F$ is a Riemann map (i.e. conformal equivalence) between the disk and some other simply connected region whose boundary has finite length (so that $F'$ is in the Hardy class $H^1$). Then $\varphi$ is a logarithm of $F'$, sot hat $e^{\varphi} = F'$; earlier in the paper, it's noted that $\varphi$ has Bloch norm at most $6$, i.e. $$|\varphi'(z)| \le 6 (1 - |z|^2)^{-1}$$ We can decompose the circle $T$ into dyadic arcs $I$ of length $\pi 2^{-n}$, and define ""squares"" $$Q = Q_I = \left\{z : \frac{z}{|z|} \in I, 1 - \pi 2^{-n} \le |z| \le 1\right\}$$ (so that $Q$ is part of a sector of the circle, and its ""side-lengths"" are all comparable). Let $z_{Q}$ be the center of this region, and $T_Q$ it upper half - i.e. $\{z \in Q : 1 - \pi 2^{-n} \le |z| \le 1 - \pi 2^{-n - 1}\}$. Finally, assume that there exists a $z \in T_Q$ with $|\varphi(z) - \varphi(z_Q)| \ge \epsilon$. I believe this is necessary - in the extreme case, when $\varphi$ is constant, the right-hand integral is $0$ while the left is not. My question : I don't see what is meant by ""by normal families"" here, or what normal family is being considered here. If anyone could clarify this statement, or point out some general theorem this is referencing (or refer me to a similar, but wordier proof), I would greatly appreciate it. Remark This is somewhat similar to an inequality proved earlier in the paper, namely $$\int\int_{\mathbb D} |F'(z)| |\varphi'(z)|^2 \log \frac{1}{|z|} dx dy \le 4 \int_{T} |F'(e^{i\theta})| \,d\theta$$ which follows from Green's formula via $$\int_T |G(e^{i\theta}) - G(0)|^2 d\theta = \int\int_{\mathbb{D}} |G'(z)|^2 \log \frac{1}{|z|} \,dx\, dy$$ where $G$ is a square root of $F'$. However, the inequality here goes in the other direction.","I am trying to prove that $$\int_{\partial T(Q)} |F'(z)| \,ds(z) \lesssim \int\int_{T(Q)} |F'(z)| |\varphi'(z)|^2 \log \frac{1}{|z|} \,dx\, dy$$ This is an estimate on page $6$ of this paper by Peter Jones, and the author states that this follows ""by normal families and"" and knowing that $\varphi'$ has Bloch norm at most $6$. The symbol $\lesssim$ means $\le$ with constants that should not depend on $F$, $\varphi$ or $Q$. For notation, $F$ is a Riemann map (i.e. conformal equivalence) between the disk and some other simply connected region whose boundary has finite length (so that $F'$ is in the Hardy class $H^1$). Then $\varphi$ is a logarithm of $F'$, sot hat $e^{\varphi} = F'$; earlier in the paper, it's noted that $\varphi$ has Bloch norm at most $6$, i.e. $$|\varphi'(z)| \le 6 (1 - |z|^2)^{-1}$$ We can decompose the circle $T$ into dyadic arcs $I$ of length $\pi 2^{-n}$, and define ""squares"" $$Q = Q_I = \left\{z : \frac{z}{|z|} \in I, 1 - \pi 2^{-n} \le |z| \le 1\right\}$$ (so that $Q$ is part of a sector of the circle, and its ""side-lengths"" are all comparable). Let $z_{Q}$ be the center of this region, and $T_Q$ it upper half - i.e. $\{z \in Q : 1 - \pi 2^{-n} \le |z| \le 1 - \pi 2^{-n - 1}\}$. Finally, assume that there exists a $z \in T_Q$ with $|\varphi(z) - \varphi(z_Q)| \ge \epsilon$. I believe this is necessary - in the extreme case, when $\varphi$ is constant, the right-hand integral is $0$ while the left is not. My question : I don't see what is meant by ""by normal families"" here, or what normal family is being considered here. If anyone could clarify this statement, or point out some general theorem this is referencing (or refer me to a similar, but wordier proof), I would greatly appreciate it. Remark This is somewhat similar to an inequality proved earlier in the paper, namely $$\int\int_{\mathbb D} |F'(z)| |\varphi'(z)|^2 \log \frac{1}{|z|} dx dy \le 4 \int_{T} |F'(e^{i\theta})| \,d\theta$$ which follows from Green's formula via $$\int_T |G(e^{i\theta}) - G(0)|^2 d\theta = \int\int_{\mathbb{D}} |G'(z)|^2 \log \frac{1}{|z|} \,dx\, dy$$ where $G$ is a square root of $F'$. However, the inequality here goes in the other direction.",,"['real-analysis', 'complex-analysis']"
72,"a set of functions that are pointwise equicontinuous but not uniformly equicontinuous, supposing the domain of f is noncompact","a set of functions that are pointwise equicontinuous but not uniformly equicontinuous, supposing the domain of f is noncompact",,Can anyone think of an example of such set of functions?(If domain is compact then pointwsie equicontinuity implies uniformly equicontinuous),Can anyone think of an example of such set of functions?(If domain is compact then pointwsie equicontinuity implies uniformly equicontinuous),,"['real-analysis', 'equicontinuity']"
73,Is a function determined by its integrals over open sets?,Is a function determined by its integrals over open sets?,,"If $f \in L^1(\mathbb R)$ satisfies $$   \int_U f = 0 $$ for every open set $U \subset \mathbb R$, then is it true that $f = 0$ a.e.?","If $f \in L^1(\mathbb R)$ satisfies $$   \int_U f = 0 $$ for every open set $U \subset \mathbb R$, then is it true that $f = 0$ a.e.?",,['real-analysis']
74,On the root of $\cos (a_1x) + \cdots + \cos (a_nx) = 0$,On the root of,\cos (a_1x) + \cdots + \cos (a_nx) = 0,"This is a problem I was trying to solve for a while with no succeed. Show that the equation $\cos (a_1x) + \cdots + \cos (a_nx) = 0$ has at least one solution in $[0,\frac {\pi}{a_1}]$, where $0 < a_1 < \cdots < a_n$.","This is a problem I was trying to solve for a while with no succeed. Show that the equation $\cos (a_1x) + \cdots + \cos (a_nx) = 0$ has at least one solution in $[0,\frac {\pi}{a_1}]$, where $0 < a_1 < \cdots < a_n$.",,"['calculus', 'real-analysis', 'fourier-analysis']"
75,Uniformly convergence in compact sets,Uniformly convergence in compact sets,,"(Theorem 7.13 in Baby Rudin) Suppose $K$ is compact, and (a) $\{f_n\}$ is a sequence of continuous functions on $K$, (b) $\{f_n\}$ converges pointwise to a continuous function $f$ on $K$, (c) $f_n(x) \ge f_{n+1}(x)$ for all $x \in K, n = 1, 2, 3, \cdots$. Then $f_n \to f$ uniformly on $K$. I was reading this theorem in Baby Rudin, but the proof uses a trick that is not easy for me to come up with. What I had in mind was to work in the compact spaces (since $K$ compact and $f_n$ continuous) $f_1(K), f_2(K), \cdots$ directly, instead of in the domain $K$. I understand that (a) can give me a set of compact spaces $f_1(K), f_2(K), \cdots$; and (b) can help me construct a convergence sequence (but not sure in what space, since union of countably many compact sets is not necessarily compact); and (c) can give me a monotone function (again, not sure in what space). I got stuck at the point where a warning sign says: ""be careful about the union/intersection of countably many compact sets."" Any hint to proceed?","(Theorem 7.13 in Baby Rudin) Suppose $K$ is compact, and (a) $\{f_n\}$ is a sequence of continuous functions on $K$, (b) $\{f_n\}$ converges pointwise to a continuous function $f$ on $K$, (c) $f_n(x) \ge f_{n+1}(x)$ for all $x \in K, n = 1, 2, 3, \cdots$. Then $f_n \to f$ uniformly on $K$. I was reading this theorem in Baby Rudin, but the proof uses a trick that is not easy for me to come up with. What I had in mind was to work in the compact spaces (since $K$ compact and $f_n$ continuous) $f_1(K), f_2(K), \cdots$ directly, instead of in the domain $K$. I understand that (a) can give me a set of compact spaces $f_1(K), f_2(K), \cdots$; and (b) can help me construct a convergence sequence (but not sure in what space, since union of countably many compact sets is not necessarily compact); and (c) can give me a monotone function (again, not sure in what space). I got stuck at the point where a warning sign says: ""be careful about the union/intersection of countably many compact sets."" Any hint to proceed?",,['real-analysis']
76,Graph of continuous function from compact space is compact.,Graph of continuous function from compact space is compact.,,"I know this question seems to have been asked hundreds of times, but I don't really see how any of the existing answers address my concern, so I'm hoping that maybe someone here might be able to clarify. Exercise 6, chapter 4 Rudin's ""Principles of Mathematical Analysis"" Suppose E is compact, and prove that f is continuous on E if and only if its graph is compact. So, if I suppose that $f|X\rightarrow Y$, then the graph of $f$ is $G_f=\{x,f(x)|x\in X\}$. This in turn is a subset of $X\times Y$- no problem. However, in order to reason on whether $G_f$ is compact, don't I need to assume some topology on $X\times Y$? Don't I necessarily sacrifice generality by making such an assumption? So first, am I correct in thinking that assuming a topology on $X\times Y$ constitutes a loss of generality, and second, is there a way to approach this problem without making such an assumption (i.e., using the case of a general topology, rather than a specific one such as the product topology).","I know this question seems to have been asked hundreds of times, but I don't really see how any of the existing answers address my concern, so I'm hoping that maybe someone here might be able to clarify. Exercise 6, chapter 4 Rudin's ""Principles of Mathematical Analysis"" Suppose E is compact, and prove that f is continuous on E if and only if its graph is compact. So, if I suppose that $f|X\rightarrow Y$, then the graph of $f$ is $G_f=\{x,f(x)|x\in X\}$. This in turn is a subset of $X\times Y$- no problem. However, in order to reason on whether $G_f$ is compact, don't I need to assume some topology on $X\times Y$? Don't I necessarily sacrifice generality by making such an assumption? So first, am I correct in thinking that assuming a topology on $X\times Y$ constitutes a loss of generality, and second, is there a way to approach this problem without making such an assumption (i.e., using the case of a general topology, rather than a specific one such as the product topology).",,"['real-analysis', 'general-topology', 'continuity', 'compactness']"
77,"If $m^*(E)=\infty$, then $E=\bigcup_{k=1}^{\infty}E_k$, $E_k$ measurable and $m^*(E_k)<+\infty$","If , then ,  measurable and",m^*(E)=\infty E=\bigcup_{k=1}^{\infty}E_k E_k m^*(E_k)<+\infty,"Reading Royden's fourth edition of Real Analysis . I'm working with outer measure defined as $$m^*(E)=\inf\left\{\sum_{n=1}^\infty l(I_n):\,E\subset \bigcup_{n=1}^\infty I_n\right\},$$ where each $I_n$ is a bounded, open interval. Also, $E$ is measurable if and only if $$m^*(A)=m^*(A\cap E)+m^*(A\cap E^C),$$ for every set $A$. In reading the proof of Theorem 11 on page 40, I start with $E$ a measurable set. Then I suddenly read the statement: ""Consider the case where $m^*(E)=\infty$. Then $E$ may be expressed as the disjoint union of a countable collection $\{E_k\}_{k=1}^\infty$ of measurable sets, each of which has finite outer measure. I am stuck on this last sentence. How come this is true?","Reading Royden's fourth edition of Real Analysis . I'm working with outer measure defined as $$m^*(E)=\inf\left\{\sum_{n=1}^\infty l(I_n):\,E\subset \bigcup_{n=1}^\infty I_n\right\},$$ where each $I_n$ is a bounded, open interval. Also, $E$ is measurable if and only if $$m^*(A)=m^*(A\cap E)+m^*(A\cap E^C),$$ for every set $A$. In reading the proof of Theorem 11 on page 40, I start with $E$ a measurable set. Then I suddenly read the statement: ""Consider the case where $m^*(E)=\infty$. Then $E$ may be expressed as the disjoint union of a countable collection $\{E_k\}_{k=1}^\infty$ of measurable sets, each of which has finite outer measure. I am stuck on this last sentence. How come this is true?",,['real-analysis']
78,$k$-cells: Why $a_i < b_i$ instead of $a_i \le b_i$,-cells: Why  instead of,k a_i < b_i a_i \le b_i,"In Rudin, The Principles of Mathematical Analysis , there is the following definition: Definition: If $a_i < b_i$ for $i=1,2,...,k$, the set of all points, $ \boldsymbol{x} = ( x_1, x_2, ..., x_k )$ in $ \mathbb{R} ^ k $ whose coordinates satisfy the inequalities $ a_i \le x_i \le b_i$ $( 1 \le i \le k )$ is called a $k$-cell. The examples that are provided give a $1$-cell as an interval and a $2$-cell as a rectangle. What I am trying to figure out is why the definition is given this particular way. In particular, why the definition is not phrased the same way, with the exception that $a_i \le b_i$. Upon consideration, the only difference between the two situations is what may arise in a particular $k$-cell. For example, consider a $3$-cell. This would provide the set of all $\boldsymbol{x} = (x_1, x_2, x_3)$ where each $a_i \le x_i \le b_i$. Such a characterization gives a cuboid. However, by allowing for the possibility that $a_i = b_i$, we allow for the possibility that at least one $x_i$ is fixed in the set. Thus in the $3$-cell case we would be allowed to have cuboids, planes, lines, and points. Thus the restriction $a_i \le b_i$ seems to be more flexible. Thus my question is, why is the more restrictive $a_i < b_i$ used rather than $a_i \le b_i$.","In Rudin, The Principles of Mathematical Analysis , there is the following definition: Definition: If $a_i < b_i$ for $i=1,2,...,k$, the set of all points, $ \boldsymbol{x} = ( x_1, x_2, ..., x_k )$ in $ \mathbb{R} ^ k $ whose coordinates satisfy the inequalities $ a_i \le x_i \le b_i$ $( 1 \le i \le k )$ is called a $k$-cell. The examples that are provided give a $1$-cell as an interval and a $2$-cell as a rectangle. What I am trying to figure out is why the definition is given this particular way. In particular, why the definition is not phrased the same way, with the exception that $a_i \le b_i$. Upon consideration, the only difference between the two situations is what may arise in a particular $k$-cell. For example, consider a $3$-cell. This would provide the set of all $\boldsymbol{x} = (x_1, x_2, x_3)$ where each $a_i \le x_i \le b_i$. Such a characterization gives a cuboid. However, by allowing for the possibility that $a_i = b_i$, we allow for the possibility that at least one $x_i$ is fixed in the set. Thus in the $3$-cell case we would be allowed to have cuboids, planes, lines, and points. Thus the restriction $a_i \le b_i$ seems to be more flexible. Thus my question is, why is the more restrictive $a_i < b_i$ used rather than $a_i \le b_i$.",,"['real-analysis', 'general-topology', 'metric-spaces']"
79,Does a convergent power series on a closed disk always converge uniformly?,Does a convergent power series on a closed disk always converge uniformly?,,"If I have a power series $\displaystyle\sum_{i=0}^{+\infty} {a_iz^i} \in\mathbb{C}[[z]]$ with radius of convergence $r>0$ and I know that the series $\displaystyle\sum_{i=0}^{+\infty} {a_iz^i}$ converges for all $|z|=r$, can I conclude via Abel's theorem (which gives me uniform convergence on every segment joining a point on the circle $|z|=r$ and the origin) that $\displaystyle\sum_{i=0}^{+\infty} {a_iz^i}$ converges uniformly on the subset of $\mathbb{C}$ $|z|\le r$?","If I have a power series $\displaystyle\sum_{i=0}^{+\infty} {a_iz^i} \in\mathbb{C}[[z]]$ with radius of convergence $r>0$ and I know that the series $\displaystyle\sum_{i=0}^{+\infty} {a_iz^i}$ converges for all $|z|=r$, can I conclude via Abel's theorem (which gives me uniform convergence on every segment joining a point on the circle $|z|=r$ and the origin) that $\displaystyle\sum_{i=0}^{+\infty} {a_iz^i}$ converges uniformly on the subset of $\mathbb{C}$ $|z|\le r$?",,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'convergence-divergence', 'power-series']"
80,Prove that $\int_a^cf(x)\mathrm{d}x+(c-a)g(c)=\int_c^bg(x)\mathrm{d}x+(b-c)f(c)$,Prove that,\int_a^cf(x)\mathrm{d}x+(c-a)g(c)=\int_c^bg(x)\mathrm{d}x+(b-c)f(c),"Let $f$ , $g$ be real continuous functions in $[a,b]$. Prove that there is $c\in(a,b)$ such that $$\int_a^cf(x)\mathrm{d}x+(c-a)g(c)=\int_c^bg(x)\mathrm{d}x+(b-c)f(c)$$ What would you suggest me to do here? Thanks.","Let $f$ , $g$ be real continuous functions in $[a,b]$. Prove that there is $c\in(a,b)$ such that $$\int_a^cf(x)\mathrm{d}x+(c-a)g(c)=\int_c^bg(x)\mathrm{d}x+(b-c)f(c)$$ What would you suggest me to do here? Thanks.",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
81,Are there any continuous functions from the real line onto the complex plane?,Are there any continuous functions from the real line onto the complex plane?,,Is there any measurable continuous differentiable analytic surjective function $f:\mathbb{R}\to\mathbb{C}$?,Is there any measurable continuous differentiable analytic surjective function $f:\mathbb{R}\to\mathbb{C}$?,,"['real-analysis', 'general-topology']"
82,I want to prove that the following set is closed,I want to prove that the following set is closed,,"Let $A\subseteq R$ be a compact set and $B\subseteq R$ closed. Then $S=\{b\sin a;b\in B,a\in A\}$ is closed. What I have done is to consider the continuous function $$f:\mathbb{R}\times \mathbb{R}\to \mathbb{R}$$ defined by $f(x,y)=x\sin y.\;$ Then $\;S=f(B\times A).\;$ If $\,f\,$ is closed, then $\,S\,$ is closed. Is $\,f\,$ closed? ($\mathbb{R}$ is the real numbers) Thanks!","Let $A\subseteq R$ be a compact set and $B\subseteq R$ closed. Then $S=\{b\sin a;b\in B,a\in A\}$ is closed. What I have done is to consider the continuous function $$f:\mathbb{R}\times \mathbb{R}\to \mathbb{R}$$ defined by $f(x,y)=x\sin y.\;$ Then $\;S=f(B\times A).\;$ If $\,f\,$ is closed, then $\,S\,$ is closed. Is $\,f\,$ closed? ($\mathbb{R}$ is the real numbers) Thanks!",,['real-analysis']
83,"$C$ be a closed subset of the Cantor set $\Delta$. Show the existence of a continuous function $f:\Delta\to C$ s.t. $f(x)=x$, $x\in C$","be a closed subset of the Cantor set . Show the existence of a continuous function  s.t. ,",C \Delta f:\Delta\to C f(x)=x x\in C,"Question: Let $C$ be a closed subset of the Cantor set $\Delta$. Prove there is a continuous function $f$ from $\Delta$ onto $C$ s.t. for every $x \in C$ we have $f(x)=x$. Context: Advanced Undergraduate Analysis. I am familiar with Rudin and Carothers. This was a fact posed by a professor that has been on my mind for awhile now. I was considering the Cantor function $\Delta \rightarrow [0,1]$ but I don't know how I could show that it is continuous on $\Delta$. Any insight or help would be appreciated.","Question: Let $C$ be a closed subset of the Cantor set $\Delta$. Prove there is a continuous function $f$ from $\Delta$ onto $C$ s.t. for every $x \in C$ we have $f(x)=x$. Context: Advanced Undergraduate Analysis. I am familiar with Rudin and Carothers. This was a fact posed by a professor that has been on my mind for awhile now. I was considering the Cantor function $\Delta \rightarrow [0,1]$ but I don't know how I could show that it is continuous on $\Delta$. Any insight or help would be appreciated.",,"['real-analysis', 'analysis', 'functions', 'continuity']"
84,Mean of a Convergent Sequence [duplicate],Mean of a Convergent Sequence [duplicate],,"This question already has answers here : Closed 11 years ago . Possible Duplicate: Two Limits Equal - Proof Prove convergence of the sequence $(z_1+z_2+\cdots + z_n)/n$ of Cesaro means Suppose $a_n \rightarrow c$ as $n \rightarrow \infty$. We want to show that $\dfrac{1}{n} \sum_{i=1}^n a_n \rightarrow c$ as $n \rightarrow \infty$. I know I can use the definition to prove this by showing that for any given $\epsilon$, we can find an $N$ such that whenever $n \ge N$, we can have  $|\dfrac{1}{n} \sum_{i=1}^n a_n-c| \le \epsilon$. I am wondering whether there is another easier way to prove this. Thank you very much. Hanna","This question already has answers here : Closed 11 years ago . Possible Duplicate: Two Limits Equal - Proof Prove convergence of the sequence $(z_1+z_2+\cdots + z_n)/n$ of Cesaro means Suppose $a_n \rightarrow c$ as $n \rightarrow \infty$. We want to show that $\dfrac{1}{n} \sum_{i=1}^n a_n \rightarrow c$ as $n \rightarrow \infty$. I know I can use the definition to prove this by showing that for any given $\epsilon$, we can find an $N$ such that whenever $n \ge N$, we can have  $|\dfrac{1}{n} \sum_{i=1}^n a_n-c| \le \epsilon$. I am wondering whether there is another easier way to prove this. Thank you very much. Hanna",,"['calculus', 'real-analysis', 'convergence-divergence']"
85,How to prove that the image of a continuous curve in $\mathbb{R}^2$ has measure $0$?,How to prove that the image of a continuous curve in  has measure ?,\mathbb{R}^2 0,"How to prove that the image of a continuous curve in $R^2$ has measure $0$? This is an exercise given in Real Analysis-Stein & Shakarchi.  A hint is given as follow: Cover the curve by rectangles, using the uniformly continuity of $f$.","How to prove that the image of a continuous curve in $R^2$ has measure $0$? This is an exercise given in Real Analysis-Stein & Shakarchi.  A hint is given as follow: Cover the curve by rectangles, using the uniformly continuity of $f$.",,['real-analysis']
86,Set and its Complement are Measure Dense,Set and its Complement are Measure Dense,,I'm going over old comprehensive exams and part of one question is giving me a bit of trouble. It asked for an example of a subset of the real numbers such that the set and its complement were measure dense (with respect to the Lebesgue measure). A set is measure dense if its intersection with any open interval has positive measure. Any help would be greatly appreciated.,I'm going over old comprehensive exams and part of one question is giving me a bit of trouble. It asked for an example of a subset of the real numbers such that the set and its complement were measure dense (with respect to the Lebesgue measure). A set is measure dense if its intersection with any open interval has positive measure. Any help would be greatly appreciated.,,"['real-analysis', 'measure-theory']"
87,$f_n(x)$ convergence in measure implies $\frac{|f_n(x)-f(x)|}{1+|f_n(x)-f(x)|}$ convergence almost everywhere,convergence in measure implies  convergence almost everywhere,f_n(x) \frac{|f_n(x)-f(x)|}{1+|f_n(x)-f(x)|},"Let $E\subset\mathbb{R},m(E)<+\infty$, $\{f_n(x)\}$ are measurable functions defined on $E$. Then $\{f_n(x)\}$ converges to $f(x)$ in measure $\Leftrightarrow$ $$\lim_{n\rightarrow\infty}\frac{|f_n(x)-f(x)|}{1+|f_n(x)-f(x)|}=0, a.e. x\in E.$$ I think it's easy to see how to go from right to left since $\frac{|f_n(x)-f(x)|}{1+|f_n(x)-f(x)|}<\epsilon\Rightarrow|f_n(x)-f(x)|<2\epsilon$ and almost everywhere convergence implies convergence in measure. What baffles me is the other direction.","Let $E\subset\mathbb{R},m(E)<+\infty$, $\{f_n(x)\}$ are measurable functions defined on $E$. Then $\{f_n(x)\}$ converges to $f(x)$ in measure $\Leftrightarrow$ $$\lim_{n\rightarrow\infty}\frac{|f_n(x)-f(x)|}{1+|f_n(x)-f(x)|}=0, a.e. x\in E.$$ I think it's easy to see how to go from right to left since $\frac{|f_n(x)-f(x)|}{1+|f_n(x)-f(x)|}<\epsilon\Rightarrow|f_n(x)-f(x)|<2\epsilon$ and almost everywhere convergence implies convergence in measure. What baffles me is the other direction.",,"['real-analysis', 'measure-theory']"
88,Existence of a sequence of continuous functions convergent pointwise to the indicator function of irrational numbers,Existence of a sequence of continuous functions convergent pointwise to the indicator function of irrational numbers,,"Prove that there does not exist a sequence of continuous functions $ f_n :\left[ {0,1} \right] \to R $ such that converges pointwise, to the function  $$f(x)=   \begin{cases}     0 & \text{if $x$ is rational},\\\\     1 & \text{otherwise}.   \end{cases}. $$ I have no idea How can I prove this. Prove that there no exist such sequence if the convergence is uniform, it's easy, because the limit would be continuous, but here I don't know How can I do. I suppose that some ""nice"" properties are ""preserved"" in the limit, in this kind of convergence, but I don't know any of them.","Prove that there does not exist a sequence of continuous functions $ f_n :\left[ {0,1} \right] \to R $ such that converges pointwise, to the function  $$f(x)=   \begin{cases}     0 & \text{if $x$ is rational},\\\\     1 & \text{otherwise}.   \end{cases}. $$ I have no idea How can I prove this. Prove that there no exist such sequence if the convergence is uniform, it's easy, because the limit would be continuous, but here I don't know How can I do. I suppose that some ""nice"" properties are ""preserved"" in the limit, in this kind of convergence, but I don't know any of them.",,"['real-analysis', 'sequences-and-series', 'analysis']"
89,Representation of Real Number as infinte Product $\prod(1\pm2^{-k})$,Representation of Real Number as infinte Product,\prod(1\pm2^{-k}),"In some paper describing a variant of CORDIC, I found basically the claim that every real number $x$ in some interval $[u,v]$ with $u\approx0.2887$ and $v\approx2.384$ , can be represented as: $$ x = \prod_{k=1}^\infty (1\pm2^{-k}) \tag1 $$ As I found, the algorithm isn't working properly, and the cause could be that the paper is wrong, which I am trying to understand. (And maybe also to fix. For my purpose it's sufficient when $v/u\geqslant2$ .) To show that a representation like $(1)$ does not exist for a full interval containing $1$ , I startet like this: For some sequence $a=\{a_k\}_{k=1}^\infty, a_k\in\{-1,+1\}$ , let $$ f(a,n) := \prod_{k=n}^\infty (1+a_k 2^{-k}) \tag2 $$ We can introduce a complete order on all possible sequences by means of: Let $a > b$ if there is some index $m$ such that $a_m>b_m$ , and $a_k=b_k$ for all $k < m$ . Similar for $a < b$ . Let $a=b$ if $a_k=b_k$ for all $k$ . Then what I want to show is that $$ a > b \quad\implies\quad f(a,n) > f(b,n) \qquad\text{for all }n \leqslant m\tag3 $$ where $m$ is the smallest index such that $a_m\neq b_m$ . For example, take the sequence $p=\{+1,-1,-1,-1,-1,\dots\}$ , then $$ f(p,1) > f(-p,1) $$ but there are no sequences between $-p$ and $p$ , so the image of $f(\cdot,1)$ lacks the interval $(f(-p,1),f(p,1)) \tag{3a}$ . To show $(3)$ , let $a>b$ be two sequences, and let $m$ be the smallest index such that $a_m\neq b_m$ .  As the sequences coincide for all indexes smaller than $m$ , it's enought to show that $$ f(a,m) = (1+2^{-m})\prod_{m+1}^\infty (1+a_k2^{-k})  \ > \ (1-2^{-m})\prod_{m+1}^\infty (1+b_k2^{-k}) = f(b,m) \tag 4 $$ which follows from $$ (1+2^{-m})\prod_{m+1}^\infty (1-2^{-k}) \ > \ (1-2^{-m})\prod_{m+1}^\infty (1+2^{-k})  \tag 5 $$ due to $a_k\geqslant-1$ and $b_k\leqslant1$ . Rearranging $(5)$ : $$ \frac{1+2^{-m}}{1-2^{-m}}  \ >\  \prod_{m+1}^\infty \frac{1+2^{-k}}{1-2^{-k}} \tag 6 $$ Numerical evidence proposes that $(6)$ actually holds, bit I am stuck here. Questions: How to show inequality $(3)\,$ ? Provided $(3)$ holds, how to show that there are always values $x$ near $1$ which cannot be represended by means of products of form $(1)\,$ ? Maybe it's even simpler, and $(3)$ is not needed at all. Interactive Desmos plot of $(x(a), f(a))$ Edit Below is a graphic that shows a plot of $f(a,1)$ for all $2^{15}$ sequences $a$ , where the expansion was stopped after $a_{15}$ . The $x$ -coordinates for the plot are $x(a)$ , where $$\begin{align} x: \{\pm1\}^{\Bbb N} &\to [0,1] \\ a &\mapsto \sum_{k=1}^\infty (1+a_k) \cdot 2^{-k-1} \end{align}$$ i.e. interpret $a$ as a real numer in binary representation with bits $$ 0.d_1 d_2 d_3\cdots $$ where the bits are $d_k = (1+a_k)/2\in\{0,1\}$ . Notice that $a>b ~\Rightarrow~ x(a)\geqslant x(b).$ Notice that $x(p)=x(-p)=0.5$ with $p$ from $(3\mathrm{a})$ , but the graph below has a jump at $0.5$ .  There are less pronounced jumps at 0.75 and 0.25 as well; presumable the graph is a self-similar and discontinuous. The jumps mean that the image of $f$ has holes. original graphic","In some paper describing a variant of CORDIC, I found basically the claim that every real number in some interval with and , can be represented as: As I found, the algorithm isn't working properly, and the cause could be that the paper is wrong, which I am trying to understand. (And maybe also to fix. For my purpose it's sufficient when .) To show that a representation like does not exist for a full interval containing , I startet like this: For some sequence , let We can introduce a complete order on all possible sequences by means of: Let if there is some index such that , and for all . Similar for . Let if for all . Then what I want to show is that where is the smallest index such that . For example, take the sequence , then but there are no sequences between and , so the image of lacks the interval . To show , let be two sequences, and let be the smallest index such that .  As the sequences coincide for all indexes smaller than , it's enought to show that which follows from due to and . Rearranging : Numerical evidence proposes that actually holds, bit I am stuck here. Questions: How to show inequality ? Provided holds, how to show that there are always values near which cannot be represended by means of products of form ? Maybe it's even simpler, and is not needed at all. Interactive Desmos plot of Edit Below is a graphic that shows a plot of for all sequences , where the expansion was stopped after . The -coordinates for the plot are , where i.e. interpret as a real numer in binary representation with bits where the bits are . Notice that Notice that with from , but the graph below has a jump at .  There are less pronounced jumps at 0.75 and 0.25 as well; presumable the graph is a self-similar and discontinuous. The jumps mean that the image of has holes. original graphic","x [u,v] u\approx0.2887 v\approx2.384 
x = \prod_{k=1}^\infty (1\pm2^{-k}) \tag1
 v/u\geqslant2 (1) 1 a=\{a_k\}_{k=1}^\infty, a_k\in\{-1,+1\} 
f(a,n) := \prod_{k=n}^\infty (1+a_k 2^{-k}) \tag2
 a > b m a_m>b_m a_k=b_k k < m a < b a=b a_k=b_k k 
a > b \quad\implies\quad f(a,n) > f(b,n) \qquad\text{for all }n \leqslant m\tag3
 m a_m\neq b_m p=\{+1,-1,-1,-1,-1,\dots\} 
f(p,1) > f(-p,1)
 -p p f(\cdot,1) (f(-p,1),f(p,1)) \tag{3a} (3) a>b m a_m\neq b_m m 
f(a,m) = (1+2^{-m})\prod_{m+1}^\infty (1+a_k2^{-k})
 \ > \ (1-2^{-m})\prod_{m+1}^\infty (1+b_k2^{-k}) = f(b,m) \tag 4
 
(1+2^{-m})\prod_{m+1}^\infty (1-2^{-k})
\ > \ (1-2^{-m})\prod_{m+1}^\infty (1+2^{-k})  \tag 5
 a_k\geqslant-1 b_k\leqslant1 (5) 
\frac{1+2^{-m}}{1-2^{-m}}
 \ >\  \prod_{m+1}^\infty \frac{1+2^{-k}}{1-2^{-k}} \tag 6
 (6) (3)\, (3) x 1 (1)\, (3) (x(a), f(a)) f(a,1) 2^{15} a a_{15} x x(a) \begin{align}
x: \{\pm1\}^{\Bbb N} &\to [0,1] \\
a &\mapsto \sum_{k=1}^\infty (1+a_k) \cdot 2^{-k-1}
\end{align} a 
0.d_1 d_2 d_3\cdots
 d_k = (1+a_k)/2\in\{0,1\} a>b ~\Rightarrow~ x(a)\geqslant x(b). x(p)=x(-p)=0.5 p (3\mathrm{a}) 0.5 f","['real-analysis', 'numerical-methods', 'real-numbers']"
90,Suppose $\sum \frac{1}{a_n} $ diverges. Then does $\sum \frac{1}{n\Delta a_n}\ $ diverge?,Suppose  diverges. Then does  diverge?,\sum \frac{1}{a_n}  \sum \frac{1}{n\Delta a_n}\ ,"Let $(a_n)$ be a strictly increasing sequence of positive real numbers, and denote $\Delta a_n:= a_{n+1} - a_n.$ Suppose $\displaystyle\sum \frac{1}{a_n} $ diverges. Then does $\displaystyle\sum \frac{1}{n\Delta a_n}\ $ diverge? My first thought for counter-examples was to try $a_n = n\log n.$ But I'm pretty sure $\displaystyle\sum \frac{1}{n\left( (n+1)\log(n+1) - n\log n \right)}\ $ diverges. I can't think of a good proof strategy for the affirmative.","Let be a strictly increasing sequence of positive real numbers, and denote Suppose diverges. Then does diverge? My first thought for counter-examples was to try But I'm pretty sure diverges. I can't think of a good proof strategy for the affirmative.",(a_n) \Delta a_n:= a_{n+1} - a_n. \displaystyle\sum \frac{1}{a_n}  \displaystyle\sum \frac{1}{n\Delta a_n}\  a_n = n\log n. \displaystyle\sum \frac{1}{n\left( (n+1)\log(n+1) - n\log n \right)}\ ,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'examples-counterexamples', 'divergent-series']"
91,Can I replace modulus inequalities with rooted square arguements?,Can I replace modulus inequalities with rooted square arguements?,,"Suppose I want to show $|x-5|<|x+1|$ . One way (and the way my lecturer shows) to do it is look at the negative and positive regions and solve the inequality. But with the definition $\sqrt{x}\geq0$ , $\forall x\in \Bbb R$ , I can say that $|x|=\sqrt{x^{2}}$ . So could I, in general over $\Bbb R$ , use this definition to substitute the modulus argument? Obviously in some cases this won't do anything because I will end up with the same +/- situation, but at least here the task is (at least for my brain) more algebraically simple, because: $$ |x-5|<|x+1| \iff\sqrt{(x-5)^{2}} < \sqrt{(x+1)^{2}}\iff(x-5)^{2}<(x+1)^{2}$$ With dif. squares: $$(x-5)^{2}-(x+1)^{2}<0$$ $$(2x-4)(-6)<0$$ $$x>2$$ To me this seems extremely obvious because I can use the ""intuition"" $$a<b, c<d\implies ac<bd \quad\forall a,b,c,d \in \Bbb R_{\geq 0 },$$ but I'm new to real analysis and everything seems to have weird caveats that make nothing intuitive true in general haha. Is there a proof for this or is the intuition something that just logically extends from Peano's axioms and natural number theorems?","Suppose I want to show . One way (and the way my lecturer shows) to do it is look at the negative and positive regions and solve the inequality. But with the definition , , I can say that . So could I, in general over , use this definition to substitute the modulus argument? Obviously in some cases this won't do anything because I will end up with the same +/- situation, but at least here the task is (at least for my brain) more algebraically simple, because: With dif. squares: To me this seems extremely obvious because I can use the ""intuition"" but I'm new to real analysis and everything seems to have weird caveats that make nothing intuitive true in general haha. Is there a proof for this or is the intuition something that just logically extends from Peano's axioms and natural number theorems?","|x-5|<|x+1| \sqrt{x}\geq0 \forall x\in \Bbb R |x|=\sqrt{x^{2}} \Bbb R 
|x-5|<|x+1| \iff\sqrt{(x-5)^{2}} < \sqrt{(x+1)^{2}}\iff(x-5)^{2}<(x+1)^{2} (x-5)^{2}-(x+1)^{2}<0 (2x-4)(-6)<0 x>2 a<b, c<d\implies ac<bd \quad\forall a,b,c,d \in \Bbb R_{\geq 0 },","['real-analysis', 'inequality', 'absolute-value']"
92,A little unknown test for convergence of series with positive terms,A little unknown test for convergence of series with positive terms,,"While helping a highschooler studying for her Calculus class, she showed me a convergence test for series which is kind of a hybrid between the ratio and root test, and of which  I was not aware: Theorem: Suppose $a_n>0$ for all $n$ . If $\limsup_n\Big(\frac{a_{n+1}}{a_n}\Big)^n<\frac1e$ , then the  series $\sum_na_n$ converges. If there is $N$ such that for all $n\geq N$ , $\Big(\frac{a_{n+1}}{a_n}\Big)^n\geq\frac1e$ , then the series $\sum_na_n$ diverges. I put this test to the test with simple examples: $\sum_n\frac1n$ : $\Big(\frac{n+1}{n}\Big)^n=\big(1+\tfrac1n\big)^n\nearrow e$ and so, with $a_n=\frac1n$ , $\Big(\frac{a_{n+1}}{a_n}\Big)^n\geq\frac1e$ which means that $\sum_n\frac1n$ diverges, as it should. $\sum_n\frac1{n^p}$ : $\Big(\frac{n}{n+1}\Big)^{pn}=\frac{1}{\big(1+\frac1n\big)^{np}}\xrightarrow{n\rightarrow\infty}\frac{1}{e^p}$ . For $p>1$ we get convergence and for $p<1$ divergence, as it should $\sum_n\frac{(a)_n(b_n)}{(c)_nn!}$ , where $(z)_0:=1$ and $(z)_n=z\cdot\ldots\cdot(z+n-1)$ for $n\geq1$ , $z\in \mathbb{C}$ . For simplicity, assume that $a,b,c\in\mathbb{R}\setminus\mathbb{Z}_-$ . Let $u_n=\Big|\frac{(a)_n(b_n)}{(c)_nn!}\Big|$ . Then, for all $n$ large enough $$\Big(\frac{u_{n+1}}{u_n}\big)^n=\Big(\frac{(a+n)(b+n)}{(n+1)(n+c)}\Big)^n= \Big(1+\frac{a-1}{n+1}\Big)^n\Big(1+\frac{b-c}{n+1}\Big)^n\xrightarrow{n\rightarrow\infty}e^{a+b-c-1}$$ The series $\sum_nu_n$ converges if $a+b-c<0$ and diverges if $a+b-c>0$ . This can also be obtained by Raabe's test for example. In fact, the  Theorem above seems to be at the same level as Raabe's test  in the de Morgan hierarchy . Question: Does anybody know of the provenance of the Theorem above (a reference)? Thanks! Edit: Just to present a  short proof of the theorem above: (1) Let $p>1$ be such that $\limsup_n\Big(\frac{a_{n+1}}{a_n}\Big)^n<\frac{1}{e^p}<\frac{1}{e}$ Then, there is $N$ such that for all $n\geq N$ $$a_{n+1}<e^{-p/n}a_n$$ Let $H_n:=\sum^n_{k=1}\frac1k$ the $n$ -th sum of the harmonic series. It follows that $$a_{n+1}<\exp(-pH_n)e^{H_N}A_N<c_Nn^{-p},\qquad n\geq N$$ for some constant $C_N>0$ . (2) The assumption here  implies that for all $n\geq N$ $$a_{n+1}\geq e^{-1/n}a_n$$ and so, $$a_{n+1}\geq e^{-\big(\tfrac{1}{n}+\ldots+\tfrac{1}{N}\big)}a_N=e^{-H_n}e^{H_N}a_N\geq \frac{C_N}{n}$$ for some constant $c_N>0$ . Hence $\sum_na_n$ diverges.","While helping a highschooler studying for her Calculus class, she showed me a convergence test for series which is kind of a hybrid between the ratio and root test, and of which  I was not aware: Theorem: Suppose for all . If , then the  series converges. If there is such that for all , , then the series diverges. I put this test to the test with simple examples: : and so, with , which means that diverges, as it should. : . For we get convergence and for divergence, as it should , where and for , . For simplicity, assume that . Let . Then, for all large enough The series converges if and diverges if . This can also be obtained by Raabe's test for example. In fact, the  Theorem above seems to be at the same level as Raabe's test  in the de Morgan hierarchy . Question: Does anybody know of the provenance of the Theorem above (a reference)? Thanks! Edit: Just to present a  short proof of the theorem above: (1) Let be such that Then, there is such that for all Let the -th sum of the harmonic series. It follows that for some constant . (2) The assumption here  implies that for all and so, for some constant . Hence diverges.","a_n>0 n \limsup_n\Big(\frac{a_{n+1}}{a_n}\Big)^n<\frac1e \sum_na_n N n\geq N \Big(\frac{a_{n+1}}{a_n}\Big)^n\geq\frac1e \sum_na_n \sum_n\frac1n \Big(\frac{n+1}{n}\Big)^n=\big(1+\tfrac1n\big)^n\nearrow e a_n=\frac1n \Big(\frac{a_{n+1}}{a_n}\Big)^n\geq\frac1e \sum_n\frac1n \sum_n\frac1{n^p} \Big(\frac{n}{n+1}\Big)^{pn}=\frac{1}{\big(1+\frac1n\big)^{np}}\xrightarrow{n\rightarrow\infty}\frac{1}{e^p} p>1 p<1 \sum_n\frac{(a)_n(b_n)}{(c)_nn!} (z)_0:=1 (z)_n=z\cdot\ldots\cdot(z+n-1) n\geq1 z\in \mathbb{C} a,b,c\in\mathbb{R}\setminus\mathbb{Z}_- u_n=\Big|\frac{(a)_n(b_n)}{(c)_nn!}\Big| n \Big(\frac{u_{n+1}}{u_n}\big)^n=\Big(\frac{(a+n)(b+n)}{(n+1)(n+c)}\Big)^n=
\Big(1+\frac{a-1}{n+1}\Big)^n\Big(1+\frac{b-c}{n+1}\Big)^n\xrightarrow{n\rightarrow\infty}e^{a+b-c-1} \sum_nu_n a+b-c<0 a+b-c>0 p>1 \limsup_n\Big(\frac{a_{n+1}}{a_n}\Big)^n<\frac{1}{e^p}<\frac{1}{e} N n\geq N a_{n+1}<e^{-p/n}a_n H_n:=\sum^n_{k=1}\frac1k n a_{n+1}<\exp(-pH_n)e^{H_N}A_N<c_Nn^{-p},\qquad n\geq N C_N>0 n\geq N a_{n+1}\geq e^{-1/n}a_n a_{n+1}\geq e^{-\big(\tfrac{1}{n}+\ldots+\tfrac{1}{N}\big)}a_N=e^{-H_n}e^{H_N}a_N\geq \frac{C_N}{n} c_N>0 \sum_na_n","['real-analysis', 'calculus', 'sequences-and-series', 'reference-request']"
93,Why does a circle appear when we square a polynomial whose inflection points are all on the $x$-axis?,Why does a circle appear when we square a polynomial whose inflection points are all on the -axis?,x,"I challenged myself to find a general formula for an $n$ -degree polynomial with $n-2$ inflection points, all on the $x$ -axis. Here is what I came up with (explanation is at the end). $$\text{Even }n:f_n(x)=1+\sum_{k=1}^{n/2}\left(x^{2k}\prod_{i=1}^k \frac{(2i-2)(2i-3)-n(n-1)}{2i(2i-1)}\right)$$ $$\text{Odd }n: g_n(x)=x+\sum_{k=1}^{(n-1)/2}\left(x^{2k+1}\prod_{i=1}^k \frac{(2i-1)(2i-2)-n(n-1)}{2i(2i+1)}\right)$$ For example, here is the graph of $y=f_8(x)=\frac{1}{5}(429x^8-924x^6+630x^4-140x^2+5)$ . Then I discovered that these polynomials have another interesting property. On a whim, for even $n$ , I drew the graph of $y=f_n(x)|f_n(x)|$ , which is like squaring the function but preserves positive and negative. As $n\to\infty$ , the turning points approach a circle . For odd $n$ , the turning points on the graph of $y=n^2 g_n(x)|g_n(x)|$ (note the $n^2$ ) approach a circle. (Another nice feature is that the turning points appear to be uniformly spaced around the circle.) My question is: Prove that, as $n\to\infty$ , the turning points on $y=f_n(x)|f_n(x)|$ and $y=n^2 g_n(x)|g_n(x)|$ approach a circle. (I think we will see the same phenomenon with any $n$ -degree polynomial whose $n-2$ inflection points are all on the $x$ -axis, where $n$ is large. That is, if we take any such polynomial, and multiply it by its modulus, and apply a certain vertical stretch, then the turning points will be approximately on a circle, uniformly spaced.) Here is how I derived $f_n(x)$ . Assume $f_n(x)=\sum\limits_{k=0}^n a_k x^k$ with leftmost and rightmost roots at $x=\pm1$ , and $a_0=1, a_1=0$ . To ensure that the inflection points are all on the $x$ -axis, let $f_n(x)=(x-1)(x+1)\frac{{f_n}''(x)}{n(n-1)}$ . Equate coefficients, then the above expression for $f_n(x)$ follows. The same method can be used to derive $g_n(x)$ , except we assume $a_0=0$ and $a_1=1$ . Possibly related: question about an $(n+1)$ -degree polynomial that is tangent to a circle at $n$ points. EDIT My derivation of $f_n(x)$ and $g_n(x)$ is incomplete, because I have not shown that they each have $n-2$ inflection points. I do not know how to show this.","I challenged myself to find a general formula for an -degree polynomial with inflection points, all on the -axis. Here is what I came up with (explanation is at the end). For example, here is the graph of . Then I discovered that these polynomials have another interesting property. On a whim, for even , I drew the graph of , which is like squaring the function but preserves positive and negative. As , the turning points approach a circle . For odd , the turning points on the graph of (note the ) approach a circle. (Another nice feature is that the turning points appear to be uniformly spaced around the circle.) My question is: Prove that, as , the turning points on and approach a circle. (I think we will see the same phenomenon with any -degree polynomial whose inflection points are all on the -axis, where is large. That is, if we take any such polynomial, and multiply it by its modulus, and apply a certain vertical stretch, then the turning points will be approximately on a circle, uniformly spaced.) Here is how I derived . Assume with leftmost and rightmost roots at , and . To ensure that the inflection points are all on the -axis, let . Equate coefficients, then the above expression for follows. The same method can be used to derive , except we assume and . Possibly related: question about an -degree polynomial that is tangent to a circle at points. EDIT My derivation of and is incomplete, because I have not shown that they each have inflection points. I do not know how to show this.","n n-2 x \text{Even }n:f_n(x)=1+\sum_{k=1}^{n/2}\left(x^{2k}\prod_{i=1}^k \frac{(2i-2)(2i-3)-n(n-1)}{2i(2i-1)}\right) \text{Odd }n: g_n(x)=x+\sum_{k=1}^{(n-1)/2}\left(x^{2k+1}\prod_{i=1}^k \frac{(2i-1)(2i-2)-n(n-1)}{2i(2i+1)}\right) y=f_8(x)=\frac{1}{5}(429x^8-924x^6+630x^4-140x^2+5) n y=f_n(x)|f_n(x)| n\to\infty n y=n^2 g_n(x)|g_n(x)| n^2 n\to\infty y=f_n(x)|f_n(x)| y=n^2 g_n(x)|g_n(x)| n n-2 x n f_n(x) f_n(x)=\sum\limits_{k=0}^n a_k x^k x=\pm1 a_0=1, a_1=0 x f_n(x)=(x-1)(x+1)\frac{{f_n}''(x)}{n(n-1)} f_n(x) g_n(x) a_0=0 a_1=1 (n+1) n f_n(x) g_n(x) n-2","['real-analysis', 'calculus', 'derivatives', 'circles']"
94,Computing a limit of a sum mixed with product.,Computing a limit of a sum mixed with product.,,Given $d\ge 2$ integer and $m_0>0$ define $$m_k= m_0\left(\frac{d}{d-1}\right)^k\quad \text{and}\quad\sigma_n= \frac{1}{m_n+d}$$ I would like to compute $$\lim_{n\to \infty}\prod_{j=1}^{n}(1-\sigma_j)$$ and $$\lim_{n\to \infty}\sum_{k=1}^{n-2} \sigma_k\prod_{j=k+1}^{n-1}(1-\sigma_j)$$ I expect the following results $\frac{m_0}{m_0+d}$ for the first and $\frac{1}{m_0+d}$ for the second.,Given integer and define I would like to compute and I expect the following results for the first and for the second.,d\ge 2 m_0>0 m_k= m_0\left(\frac{d}{d-1}\right)^k\quad \text{and}\quad\sigma_n= \frac{1}{m_n+d} \lim_{n\to \infty}\prod_{j=1}^{n}(1-\sigma_j) \lim_{n\to \infty}\sum_{k=1}^{n-2} \sigma_k\prod_{j=k+1}^{n-1}(1-\sigma_j) \frac{m_0}{m_0+d} \frac{1}{m_0+d},"['real-analysis', 'sequences-and-series', 'algebra-precalculus', 'limits', 'infinite-product']"
95,Approximation of Lebesgue measurable set,Approximation of Lebesgue measurable set,,"Let $E \subseteq \mathbb{R}$ be a set with finite Lebesgue measure. I want to show that for each $\varepsilon > 0$ , there exists a disjoint collection of closed intervals $\{I_k\}$ such that $m(E \setminus \bigcup I_k) = 0 $ and $\sum_{k}m(I_k) \leqslant m(E) + \varepsilon$ . I know that $E$ can be approximated by a $F_\sigma$ set, but it seems like we are proving a slightly stronger statement. I considered \begin{equation*} m^{**}(E) = \inf \{ \sum_{k}m(I_k) \mid E \subseteq \bigcup_{k}I_k\} \end{equation*} where the $I_k$ 's are disjoint closed intervals. I thought $m^{**}(E) = m^{*}(E)$ (one direction is easy; for the other direction, I use the fact that each closed interval can be realized as the closure of an open interval, and that $m^{*}(E) = \inf \{m(O): E \subset O, O \text{ open}\}$ ). Is this correct? If so, how can we proceed from there and argue that the infimum can be achieved?","Let be a set with finite Lebesgue measure. I want to show that for each , there exists a disjoint collection of closed intervals such that and . I know that can be approximated by a set, but it seems like we are proving a slightly stronger statement. I considered where the 's are disjoint closed intervals. I thought (one direction is easy; for the other direction, I use the fact that each closed interval can be realized as the closure of an open interval, and that ). Is this correct? If so, how can we proceed from there and argue that the infimum can be achieved?","E \subseteq \mathbb{R} \varepsilon > 0 \{I_k\} m(E \setminus \bigcup I_k) = 0  \sum_{k}m(I_k) \leqslant m(E) + \varepsilon E F_\sigma \begin{equation*}
m^{**}(E) = \inf \{ \sum_{k}m(I_k) \mid E \subseteq \bigcup_{k}I_k\}
\end{equation*} I_k m^{**}(E) = m^{*}(E) m^{*}(E) = \inf \{m(O): E \subset O, O \text{ open}\}","['real-analysis', 'measure-theory']"
96,Showing an infinite sequence is constant under some condition,Showing an infinite sequence is constant under some condition,,"Let $a_1,a_2,...$ be an infinite sequence of positive real numbers such that for each positive integer $n$ we have $$ \frac{a_1+a_2+..+a_n}n\ge\sqrt{\frac{a^2_1+a^2_2+...+a^2_{n+1}}{n+1}}. $$ Prove that the sequence $a_1,a_2,...\ $ is constant. MY ATTEMPT/THOUGHTS: My initial plan is to show that the sequence is bounded and then proving it is constant. For that I considered the following. Let $m_n=\min\{a_1,a_2,...,a_n\}$ , $M_n=\max\{a_1,a_2,...,a_n\}$ , and $$S_n=\frac{a^2_1+a^2_2+...+a^2_n}{n}.$$ Then we have $$m^2_n\le S_{n+1} \le M^2_n.$$ Also from the given inequality, we have, on squaring, $$\frac{1}{n}S_n+2\frac{a_1a_2+a_1a_3+....+a_{n-1}a_n}{n^2}\ge S^2_{n+1}.$$ I have no idea how to proceed after this or even if I am moving in the right direction! Do you have any suggestions? Thanks for your time.","Let be an infinite sequence of positive real numbers such that for each positive integer we have Prove that the sequence is constant. MY ATTEMPT/THOUGHTS: My initial plan is to show that the sequence is bounded and then proving it is constant. For that I considered the following. Let , , and Then we have Also from the given inequality, we have, on squaring, I have no idea how to proceed after this or even if I am moving in the right direction! Do you have any suggestions? Thanks for your time.","a_1,a_2,... n 
\frac{a_1+a_2+..+a_n}n\ge\sqrt{\frac{a^2_1+a^2_2+...+a^2_{n+1}}{n+1}}.
 a_1,a_2,...\  m_n=\min\{a_1,a_2,...,a_n\} M_n=\max\{a_1,a_2,...,a_n\} S_n=\frac{a^2_1+a^2_2+...+a^2_n}{n}. m^2_n\le S_{n+1} \le M^2_n. \frac{1}{n}S_n+2\frac{a_1a_2+a_1a_3+....+a_{n-1}a_n}{n^2}\ge S^2_{n+1}.","['real-analysis', 'sequences-and-series', 'convergence-divergence', 'contest-math']"
97,"If $|f(x)|\leq1$ and $|f''(x)|\leq1$, then $|f'(x)|\leq2$.","If  and , then .",|f(x)|\leq1 |f''(x)|\leq1 |f'(x)|\leq2,"Let $f:\mathbb{R}\to\mathbb{R}$ be twice differentiable, and supose that for all $x\in\mathbb{R},\,|f(x)|\leq1$ and $|f''(x)|\leq1$ . Prove that $|f'(x)|\leq2$ for all $x\in\mathbb{R}.$ I tried working with Taylor polynomials but had no successful idea.  This question is from the book Berkley Problems in Mathematics .","Let be twice differentiable, and supose that for all and . Prove that for all I tried working with Taylor polynomials but had no successful idea.  This question is from the book Berkley Problems in Mathematics .","f:\mathbb{R}\to\mathbb{R} x\in\mathbb{R},\,|f(x)|\leq1 |f''(x)|\leq1 |f'(x)|\leq2 x\in\mathbb{R}.","['real-analysis', 'derivatives']"
98,How to calculate $\int_0^1\frac{\ln^2x\ln^2(1-x^2)}{1-x^2}\ dx$?,How to calculate ?,\int_0^1\frac{\ln^2x\ln^2(1-x^2)}{1-x^2}\ dx,"Using the derivative of beta function, find $$I=\int_0^1\frac{\ln^2x\ln^2(1-x^2)}{1-x^2}\ dx$$ setting $x^2=y$ gives $$I=\frac18\int_0^1\frac{\ln^2y\ln^2(1-y)}{\sqrt{y}(1-y)}\ dy=\frac18\left.\frac{\partial^4}{\partial a^2\partial b^2}\text{B}(a,b)\right|_{a\mapsto 1/2\\b\mapsto0^{+}}$$ Any good software that can find the 4th derivative and also gives the final result? Wolfram fails to calculate it (or maybe I do not know how to use it well) and when I tried to do it manually, some terms involve $\psi(b)$ and if we take the limit, then $\psi(0)$ is undefined and even if I take the limit of $\psi(b)$ together with other terms, still undefined. I do not know how to avoid this problem as I am not experienced with the beta function. Thank you. Note: Solution should be done without using harmonic series.","Using the derivative of beta function, find setting gives Any good software that can find the 4th derivative and also gives the final result? Wolfram fails to calculate it (or maybe I do not know how to use it well) and when I tried to do it manually, some terms involve and if we take the limit, then is undefined and even if I take the limit of together with other terms, still undefined. I do not know how to avoid this problem as I am not experienced with the beta function. Thank you. Note: Solution should be done without using harmonic series.","I=\int_0^1\frac{\ln^2x\ln^2(1-x^2)}{1-x^2}\ dx x^2=y I=\frac18\int_0^1\frac{\ln^2y\ln^2(1-y)}{\sqrt{y}(1-y)}\ dy=\frac18\left.\frac{\partial^4}{\partial a^2\partial b^2}\text{B}(a,b)\right|_{a\mapsto 1/2\\b\mapsto0^{+}} \psi(b) \psi(0) \psi(b)","['real-analysis', 'calculus', 'integration', 'harmonic-numbers', 'beta-function']"
99,Why this function is not integrable,Why this function is not integrable,,"I have a question as I look at the example 8.9(a) in Rudin's Real and Complex Analysis : Let $X$ and $Y$ be the closed unit interval $[0,1]$ , let $\{\delta_n\}$ be an increasing sequence of distinct points in $[0,1]$ that converges to $1$ , and to each positive integer $n$ , let $g_n$ be a real continuous function on $[0,1]$ with support in $(\delta_n,\delta_{n+1})$ , and such that $\int_{0}^{1} g_n(t)~dt=1$ . Define $f$ over $X\times Y$ as follows: $$ f(x,y):=\sum_{n=1}^\infty[g_n(x)-g_{n+1}(x)]g_n(y). $$ It is easy to check that the Fubini Theorem does not apply for $f(x,y)$ . And the book says that this is because the function of $f(x,y)$ is not integrable, i.e. $$\int_0^1\,dx\int_0^1|f(x,y)|\,dy=\infty$$ But I could not easily see why it is not integrable.","I have a question as I look at the example 8.9(a) in Rudin's Real and Complex Analysis : Let and be the closed unit interval , let be an increasing sequence of distinct points in that converges to , and to each positive integer , let be a real continuous function on with support in , and such that . Define over as follows: It is easy to check that the Fubini Theorem does not apply for . And the book says that this is because the function of is not integrable, i.e. But I could not easily see why it is not integrable.","X Y [0,1] \{\delta_n\} [0,1] 1 n g_n [0,1] (\delta_n,\delta_{n+1}) \int_{0}^{1} g_n(t)~dt=1 f X\times Y 
f(x,y):=\sum_{n=1}^\infty[g_n(x)-g_{n+1}(x)]g_n(y).
 f(x,y) f(x,y) \int_0^1\,dx\int_0^1|f(x,y)|\,dy=\infty","['real-analysis', 'calculus', 'integration', 'fubini-tonelli-theorems']"
