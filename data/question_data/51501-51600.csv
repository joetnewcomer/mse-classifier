,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Extensions of Riemann-Stieltjes without continuity problems,Extensions of Riemann-Stieltjes without continuity problems,,"Are there any extensions of Riemann-Stieltjes integration that are able to handle the following integral? $\int_0^1 \alpha \space  d\alpha$ where $ \alpha(x) = \left\{  \begin{array}{lr}    0 & x < \frac{1}{2}  \\ \frac{1}{2} & x = \frac{1}{2} \\ 1 & x > \frac{1}{2} \end{array} \right. $ Everything I've found so far have strong requirements about the funcions' continuity, and are undefined when there are shared discontinuities, even for such a simple function. My interest: Where integration by parts hold. What consequence/uses would there be to it?","Are there any extensions of Riemann-Stieltjes integration that are able to handle the following integral? where Everything I've found so far have strong requirements about the funcions' continuity, and are undefined when there are shared discontinuities, even for such a simple function. My interest: Where integration by parts hold. What consequence/uses would there be to it?","\int_0^1 \alpha \space  d\alpha 
\alpha(x) = \left\{  \begin{array}{lr}
   0 & x < \frac{1}{2} 
\\ \frac{1}{2} & x = \frac{1}{2}
\\ 1 & x > \frac{1}{2}
\end{array} \right.
","['real-analysis', 'integration', 'probability-theory', 'lebesgue-integral', 'stieltjes-integral']"
1,Solution of advanced functional differential equation,Solution of advanced functional differential equation,,"Statement Consider an advanced functional differential equation $$ Lf(x) = f(2x+\pi)+f(2x-\pi),\quad  L\equiv\frac{d^2}{dx^2}+1. \tag{1} $$ Let's construct a solution of Eq. $(1)$ with finite support $I\equiv\operatorname{supp}f=\bigl[-\pi,\,\pi\bigr].$ Notation: Denote solution as $\lambda(x),$ i.e. $\lambda(x)\equiv f(x).$ Solution Applying Fourier transform to $(1)$ after some algebra an expression of a spectrum can be obtained $$ \hat{\lambda}(t)=\prod\limits_{k=0}^{\infty}\frac{\cos\Bigl(\frac{\pi}{2}\cdot t\cdot 2^{-k}\bigr)}{1-(t\cdot 2^{-k})^2}. \tag{2} $$ Expression $(2)$ transforms into a simpler one as well $$ \hat{\lambda}(t) = \operatorname{sinc}(\pi\cdot t)\cdot\prod\limits_{k=0}^{\infty}\frac{1}{1-(t\cdot 2^{-k})^2}. \tag{3} $$ Solution of $(1)$ is defined by inverse Fourier transform $$ \lambda(x)=\frac{1}{2\pi}\int\limits_{\mathbb{R}}e^{itx}\cdot\hat{\lambda}(t)\,dt. \tag{4} $$ Computation Consider an approximation of $\lambda(x)$ by lacunary Fourier series $$ \lambda(x) = \frac{a_0}{2} + \sum\limits_{k=0}^{\infty}a_{2^k}\cos(2^kx). \tag{5} $$ Note, that $$ a_n = \dfrac{1}{\pi}\int\limits_{I}\lambda(x)\cos(nx)\,dx = \dfrac{1}{\pi}\hat{\lambda}(n). \tag{6} $$ Function $\hat{\lambda}(t)$ does not vanish only at points $n=2^k,\,k=0,1,\ldots,$ and $$ a_{2^k} = \dfrac{1}{\pi}\lim_{t\rightarrow 2^k}\hat{\lambda}(t). \tag{7} $$ Values of first several coefficients are as follows $$ \mathbf{a}=(a_0,\,a_1,\,a_2,\,a_4,\,a_8)=\biggl(\frac1\pi,\,2.3\cdot10^{-1},\,7.7\cdot10^{-2},\,-5.1\cdot10^{-3},\, 8.1\cdot10^{-5},\,-3.2\cdot10^{-7}\biggr) $$ Plot of $\lambda(x)$ graph by $(5)$ with $5$ terms approximation $\mathbf{a}$ is a blue line, first derivative $\lambda'(x)$ (orange), and second derivative $\lambda''(x)$ (green) are shown in the figure Questions Does a rectangular function function $\chi_{I}(x)$ , which is also a characteristic function of the interval $I$ satisfy the Eq. $(1)?$ How to construct a fast convergence algorithm to compute values of $f(x)$ , like a proposed one ? Derivation an exact expression of $a_{2^k}$ in (7)? Discussion The problem above is related to the problem of Recursive Integration over Piecewise Polynomials: Closed form? and the form of Eq. (1) close to the Fabius equation . Reference Kolodyazhny, V.M., Rvachov, V.A. Cybern Syst Anal (2007) 43: 893 (page 898).  DOI: https://doi.org/10.1007/s10559-007-0114-y","Statement Consider an advanced functional differential equation Let's construct a solution of Eq. with finite support Notation: Denote solution as i.e. Solution Applying Fourier transform to after some algebra an expression of a spectrum can be obtained Expression transforms into a simpler one as well Solution of is defined by inverse Fourier transform Computation Consider an approximation of by lacunary Fourier series Note, that Function does not vanish only at points and Values of first several coefficients are as follows Plot of graph by with terms approximation is a blue line, first derivative (orange), and second derivative (green) are shown in the figure Questions Does a rectangular function function , which is also a characteristic function of the interval satisfy the Eq. How to construct a fast convergence algorithm to compute values of , like a proposed one ? Derivation an exact expression of in (7)? Discussion The problem above is related to the problem of Recursive Integration over Piecewise Polynomials: Closed form? and the form of Eq. (1) close to the Fabius equation . Reference Kolodyazhny, V.M., Rvachov, V.A. Cybern Syst Anal (2007) 43: 893 (page 898).  DOI: https://doi.org/10.1007/s10559-007-0114-y","
Lf(x) = f(2x+\pi)+f(2x-\pi),\quad 
L\equiv\frac{d^2}{dx^2}+1. \tag{1}
 (1) I\equiv\operatorname{supp}f=\bigl[-\pi,\,\pi\bigr]. \lambda(x), \lambda(x)\equiv f(x). (1) 
\hat{\lambda}(t)=\prod\limits_{k=0}^{\infty}\frac{\cos\Bigl(\frac{\pi}{2}\cdot t\cdot 2^{-k}\bigr)}{1-(t\cdot 2^{-k})^2}. \tag{2}
 (2) 
\hat{\lambda}(t) = \operatorname{sinc}(\pi\cdot t)\cdot\prod\limits_{k=0}^{\infty}\frac{1}{1-(t\cdot 2^{-k})^2}. \tag{3}
 (1) 
\lambda(x)=\frac{1}{2\pi}\int\limits_{\mathbb{R}}e^{itx}\cdot\hat{\lambda}(t)\,dt. \tag{4}
 \lambda(x) 
\lambda(x) = \frac{a_0}{2} + \sum\limits_{k=0}^{\infty}a_{2^k}\cos(2^kx).
\tag{5}
 
a_n = \dfrac{1}{\pi}\int\limits_{I}\lambda(x)\cos(nx)\,dx = \dfrac{1}{\pi}\hat{\lambda}(n).
\tag{6}
 \hat{\lambda}(t) n=2^k,\,k=0,1,\ldots, 
a_{2^k} = \dfrac{1}{\pi}\lim_{t\rightarrow 2^k}\hat{\lambda}(t).
\tag{7}
 
\mathbf{a}=(a_0,\,a_1,\,a_2,\,a_4,\,a_8)=\biggl(\frac1\pi,\,2.3\cdot10^{-1},\,7.7\cdot10^{-2},\,-5.1\cdot10^{-3},\, 8.1\cdot10^{-5},\,-3.2\cdot10^{-7}\biggr)
 \lambda(x) (5) 5 \mathbf{a} \lambda'(x) \lambda''(x) \chi_{I}(x) I (1)? f(x) a_{2^k}","['integration', 'fourier-analysis', 'recurrence-relations', 'functional-equations', 'fractals']"
2,Find an implicit solution to $y ^ { \prime } ( x ) = \frac { x ^ { 3 } } { 2 y \sqrt { 1 + y ^ { 2 } } }$,Find an implicit solution to,y ^ { \prime } ( x ) = \frac { x ^ { 3 } } { 2 y \sqrt { 1 + y ^ { 2 } } },"$y ^ { \prime } ( x ) = \frac { x ^ { 3 } } { 2 y \sqrt { 1 + y ^ { 2 } } }$ $\frac { d y } { d x } = \frac { x ^ { 3 } } { 2 y \sqrt { 1 + y ^ { 2 } } }$ $\frac { d x } { d y } = \frac { 2 y \sqrt { 1 + y ^ { 2 } } } { x ^ { 3 } }$ $\int x ^ { 3 } d x = 2 \int y \left( 1 + y ^ { 2 } \right) ^ { 1 / 2 } d y$ R.H.S. $\int y \sqrt { 1 + y ^ { 2 } } d y$ let $u = 1 + y ^ { 2 }$ $ d u = 2 y d y $ $ \frac { d u } { 2 } = y d y $ $\int y \sqrt { 1 + y ^ { 2 } } d y$ $\int \sqrt { 1 + y ^ { 2 } } y d y$ $\int \frac { \sqrt { u } } { 2 } d u$ $\frac { 1 } { 2 } \int \sqrt { u } d u$ $= \frac { 1 } { 2 } \left[ \frac { 2 u ^ { 3 / 2 } } { 3 } \right]$ $= \frac { u ^ { 3 / 2 } } { 3 } = \frac { 1 } { 3 } \left( 1 + y ^ { 2 } \right) ^ { 3 / 2 } \ldots 1$ $\int x ^ { 3 } d y = 2 \int y \left( 1 + y ^ { 2 } \right) ^ { 1 / 2 } d y$ $\frac { x ^ { 4 } } { 4 } + c = 2 \left[ 1 / 3 \left( 1 + y ^ { 2 } \right) ^ { 3 / 2 } \right]$ $\frac { x ^ { 4 } } { 4 } + c = \frac { 2 } { 3 } \left( 1 + y ^ { 2 } \right) ^ { 3 / 2 }$ cant finish it , or maybe my working is wrong ?","R.H.S. let cant finish it , or maybe my working is wrong ?",y ^ { \prime } ( x ) = \frac { x ^ { 3 } } { 2 y \sqrt { 1 + y ^ { 2 } } } \frac { d y } { d x } = \frac { x ^ { 3 } } { 2 y \sqrt { 1 + y ^ { 2 } } } \frac { d x } { d y } = \frac { 2 y \sqrt { 1 + y ^ { 2 } } } { x ^ { 3 } } \int x ^ { 3 } d x = 2 \int y \left( 1 + y ^ { 2 } \right) ^ { 1 / 2 } d y \int y \sqrt { 1 + y ^ { 2 } } d y u = 1 + y ^ { 2 }  d u = 2 y d y   \frac { d u } { 2 } = y d y  \int y \sqrt { 1 + y ^ { 2 } } d y \int \sqrt { 1 + y ^ { 2 } } y d y \int \frac { \sqrt { u } } { 2 } d u \frac { 1 } { 2 } \int \sqrt { u } d u = \frac { 1 } { 2 } \left[ \frac { 2 u ^ { 3 / 2 } } { 3 } \right] = \frac { u ^ { 3 / 2 } } { 3 } = \frac { 1 } { 3 } \left( 1 + y ^ { 2 } \right) ^ { 3 / 2 } \ldots 1 \int x ^ { 3 } d y = 2 \int y \left( 1 + y ^ { 2 } \right) ^ { 1 / 2 } d y \frac { x ^ { 4 } } { 4 } + c = 2 \left[ 1 / 3 \left( 1 + y ^ { 2 } \right) ^ { 3 / 2 } \right] \frac { x ^ { 4 } } { 4 } + c = \frac { 2 } { 3 } \left( 1 + y ^ { 2 } \right) ^ { 3 / 2 },['integration']
3,Solve $\int_0^s \left[ 1-\int_{-\infty}^{\infty} \frac{e^{-\frac{y^2}{2}}}{\sqrt{2 \pi}} \tanh\left( x-\sqrt{x}y \right)dy \right] dx$,Solve,\int_0^s \left[ 1-\int_{-\infty}^{\infty} \frac{e^{-\frac{y^2}{2}}}{\sqrt{2 \pi}} \tanh\left( x-\sqrt{x}y \right)dy \right] dx,Let $$ f(x) = 1-\int_{-\infty}^{\infty} \frac{e^{-\frac{y^2}{2}}}{\sqrt{2 \pi}} \tanh\left( x-\sqrt{x}y \right)dy. $$ I would like to understand how to get to the solution of the following integral: $$ g(s)=\frac{1}{2}\int_0^s f(x) dx = s-\int_{-\infty}^{\infty} \frac{e^{-\frac{y^2}{2}}}{\sqrt{2 \pi}} \ln\cosh\left( s-\sqrt{s}y \right)dy $$ I know that $$ \frac{d}{dx} \ln\cosh\left( x-\sqrt{x}y \right) = (1-\frac{y}{2\sqrt{x}})\tanh(x-\sqrt{x}y)$$ So far I tried the following: $$ \begin{align} g(s) &= \frac{s}{2} - \frac{1}{2}\int_{-\infty}^{\infty} \frac{e^{-\frac{y^2}{2}}}{\sqrt{2 \pi}} \int_{0}^s (1-\frac{y}{2\sqrt{x}}+\frac{y}{2\sqrt{x}}) \tanh\left( x-\sqrt{x}y \right)dx dy \\ &= \frac{s}{2} -\frac{1}{2}\int_{-\infty}^{\infty} \frac{e^{-\frac{y^2}{2}}}{\sqrt{2 \pi}} \ln\cosh\left( s-\sqrt{s}y \right)dy + \frac{1}{2}\int_{-\infty}^{\infty} \frac{ye^{-\frac{y^2}{2}}}{\sqrt{2 \pi}} \int_{0}^s \frac{\tanh\left( x-\sqrt{x}y \right)}{2\sqrt{x}} dx dy\end{align} $$ But I'm stuck there. The first part equals $1/2$ times the solution. Maybe I should use some substitution in the remaining integral?,Let I would like to understand how to get to the solution of the following integral: I know that So far I tried the following: But I'm stuck there. The first part equals times the solution. Maybe I should use some substitution in the remaining integral?, f(x) = 1-\int_{-\infty}^{\infty} \frac{e^{-\frac{y^2}{2}}}{\sqrt{2 \pi}} \tanh\left( x-\sqrt{x}y \right)dy.   g(s)=\frac{1}{2}\int_0^s f(x) dx = s-\int_{-\infty}^{\infty} \frac{e^{-\frac{y^2}{2}}}{\sqrt{2 \pi}} \ln\cosh\left( s-\sqrt{s}y \right)dy   \frac{d}{dx} \ln\cosh\left( x-\sqrt{x}y \right) = (1-\frac{y}{2\sqrt{x}})\tanh(x-\sqrt{x}y)  \begin{align} g(s) &= \frac{s}{2} - \frac{1}{2}\int_{-\infty}^{\infty} \frac{e^{-\frac{y^2}{2}}}{\sqrt{2 \pi}} \int_{0}^s (1-\frac{y}{2\sqrt{x}}+\frac{y}{2\sqrt{x}}) \tanh\left( x-\sqrt{x}y \right)dx dy \\ &= \frac{s}{2} -\frac{1}{2}\int_{-\infty}^{\infty} \frac{e^{-\frac{y^2}{2}}}{\sqrt{2 \pi}} \ln\cosh\left( s-\sqrt{s}y \right)dy + \frac{1}{2}\int_{-\infty}^{\infty} \frac{ye^{-\frac{y^2}{2}}}{\sqrt{2 \pi}} \int_{0}^s \frac{\tanh\left( x-\sqrt{x}y \right)}{2\sqrt{x}} dx dy\end{align}  1/2,['integration']
4,What is more historically correct?,What is more historically correct?,,"I was wondering what was formally correct, when resolving an integral by substitution, use as a new variable $t$ or $u$ ? I noticed that here in Italy we use $t$ , while I saw that abroad (e.g USA) use $u$ . When was this variable method of integration designed? I am aware that I can use any variable and that the result would be the same but I'm curious to know originally what was used by Newton, Leibniz, etc. I hope someone knows how to answer, it is an unresolved doubt that I have for some time.","I was wondering what was formally correct, when resolving an integral by substitution, use as a new variable or ? I noticed that here in Italy we use , while I saw that abroad (e.g USA) use . When was this variable method of integration designed? I am aware that I can use any variable and that the result would be the same but I'm curious to know originally what was used by Newton, Leibniz, etc. I hope someone knows how to answer, it is an unresolved doubt that I have for some time.",t u t u,"['calculus', 'integration', 'notation', 'math-history', 'substitution']"
5,$\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3+x)}}dx$ (Now solved) [closed],(Now solved) [closed],\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3+x)}}dx,"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 5 years ago . Improve this question I have this integral: $\require{cancel}$ $$\xcancel{I=\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3-x)}}dx}$$ and the clue given for solving it is: $$\int_a^bf(x)dx=\int_a^bf(a+b-x)dx$$ so, $$\xcancel{\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3-x)}}dx=\int_2^4\frac{\sqrt{\ln(9+x)}}{\sqrt{\ln(3+x)}+\sqrt{\ln(x-3)}}dx=\int_2^4\frac{\sqrt{ln(x+9)}\left(\sqrt{\ln(3+x)}-\sqrt{\ln(x-3)}\right)}{\ln(3+x)-\ln(x-3)}dx}$$ where shall I go from here (if this is the right approach)? EDIT if we take the equation to actually be: $$I=\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3+x)}}dx$$ this removes any problems involving $\ln(0)$ as pointed out by users below If we now use the same method as I outlined above we obtain: $$\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3+x)}}dx=\int_2^4\frac{\sqrt{\ln(9-x)}}{\sqrt{ln(3+x)}+\sqrt{\ln(9-x)}}dx$$ and we can see that the bottom of the fraction is the same on the left and the right, so my first idea would be to add the two sides, giving: $$2I=\int_2^4\frac{\sqrt{\ln(3+x)}+\sqrt{\ln(9-x)}}{\sqrt{\ln(3+x)}+\sqrt{\ln(9-x)}}dx=\int_2^41dx=2$$ $$\therefore I=1$$","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 5 years ago . Improve this question I have this integral: $\require{cancel}$ $$\xcancel{I=\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3-x)}}dx}$$ and the clue given for solving it is: $$\int_a^bf(x)dx=\int_a^bf(a+b-x)dx$$ so, $$\xcancel{\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3-x)}}dx=\int_2^4\frac{\sqrt{\ln(9+x)}}{\sqrt{\ln(3+x)}+\sqrt{\ln(x-3)}}dx=\int_2^4\frac{\sqrt{ln(x+9)}\left(\sqrt{\ln(3+x)}-\sqrt{\ln(x-3)}\right)}{\ln(3+x)-\ln(x-3)}dx}$$ where shall I go from here (if this is the right approach)? EDIT if we take the equation to actually be: $$I=\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3+x)}}dx$$ this removes any problems involving $\ln(0)$ as pointed out by users below If we now use the same method as I outlined above we obtain: $$\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3+x)}}dx=\int_2^4\frac{\sqrt{\ln(9-x)}}{\sqrt{ln(3+x)}+\sqrt{\ln(9-x)}}dx$$ and we can see that the bottom of the fraction is the same on the left and the right, so my first idea would be to add the two sides, giving: $$2I=\int_2^4\frac{\sqrt{\ln(3+x)}+\sqrt{\ln(9-x)}}{\sqrt{\ln(3+x)}+\sqrt{\ln(9-x)}}dx=\int_2^41dx=2$$ $$\therefore I=1$$",,"['calculus', 'integration']"
6,$\nabla\int_{S_1(0)} f(|x|w)d\sigma(w) = \int_{S_1(0)} \nabla f(|x|w)d\sigma(w)$,,\nabla\int_{S_1(0)} f(|x|w)d\sigma(w) = \int_{S_1(0)} \nabla f(|x|w)d\sigma(w),"Let $U=\{x:x\in \mathbb{R}^n, x\neq 0\}$ and $f\in C^2(U)$. Define,   for $x\in U$, $$f_{\#}(x) = \int_{S_1(0)} f(|x|w)d\sigma(w)$$ Show   that $\nabla(f_{\#}) = (\nabla f)_{\#}$ Hint: use the divergence theorem and the formula for polar integration I know that I must use the divergence theorem: $$\int_{\Omega}div(\vec{X})(x) dx = \int_{\partial \Omega}\vec{X}(x)\cdot n(x) dx$$ But I don't know what it means by polar integration formula, since the polar integration technique is only for $\mathbb{R}^2$. I don't see what to do in $n$ dimensions. I tried seeing the integral as $$f_{\#}(x) = \int_{S_1(0)}f(|x|w)d\sigma(w) = \frac{1}{|x|^{n-1}}\int_{S_{|x|}(0)}f(w)d\sigma(w) $$ but then I'd have to take the laplacian of that, which is unpratical. I tried to integrate both sides to end with a ball and try to see some divergence in order to apply the divergence theorem: $$\int_0^1\int_{S_1(0)}|x|^ {n-1}f(|x|w)d\sigma(w) = \int_{B_1(0)}f(|x|w)d\sigma(w)$$ But I don't see how it helps. I think the best way is to begin with $(\nabla)_{\#}$: $$(\nabla f)_{\#}(x) = \int_{S_1(0)}\nabla f(|x|w)d\sigma w $$ I think that if I see the function as a vector field times the normal I can use the divergence theorem to transform this integral to an integral of a ball, but I think I cannot do that.","Let $U=\{x:x\in \mathbb{R}^n, x\neq 0\}$ and $f\in C^2(U)$. Define,   for $x\in U$, $$f_{\#}(x) = \int_{S_1(0)} f(|x|w)d\sigma(w)$$ Show   that $\nabla(f_{\#}) = (\nabla f)_{\#}$ Hint: use the divergence theorem and the formula for polar integration I know that I must use the divergence theorem: $$\int_{\Omega}div(\vec{X})(x) dx = \int_{\partial \Omega}\vec{X}(x)\cdot n(x) dx$$ But I don't know what it means by polar integration formula, since the polar integration technique is only for $\mathbb{R}^2$. I don't see what to do in $n$ dimensions. I tried seeing the integral as $$f_{\#}(x) = \int_{S_1(0)}f(|x|w)d\sigma(w) = \frac{1}{|x|^{n-1}}\int_{S_{|x|}(0)}f(w)d\sigma(w) $$ but then I'd have to take the laplacian of that, which is unpratical. I tried to integrate both sides to end with a ball and try to see some divergence in order to apply the divergence theorem: $$\int_0^1\int_{S_1(0)}|x|^ {n-1}f(|x|w)d\sigma(w) = \int_{B_1(0)}f(|x|w)d\sigma(w)$$ But I don't see how it helps. I think the best way is to begin with $(\nabla)_{\#}$: $$(\nabla f)_{\#}(x) = \int_{S_1(0)}\nabla f(|x|w)d\sigma w $$ I think that if I see the function as a vector field times the normal I can use the divergence theorem to transform this integral to an integral of a ball, but I think I cannot do that.",,"['integration', 'multivariable-calculus', 'partial-differential-equations', 'divergence-operator']"
7,"Triangle inequality for integrals, but for an arbitrary norm","Triangle inequality for integrals, but for an arbitrary norm",,"Given an arbitrary norm on $\mathbb{R}^q$ . For a continous function $f: [a,b]\times\mathbb{R}^q \rightarrow \mathbb{R}^q$ , I want to find out whether $$ \left\|\int_{a}^{b} f(x,y_1,...,y_q)\,\mathrm{d}x\right\| \le \int_{a}^{b} \| f(x,y_1,...,y_q)\|\,\mathrm{d}x$$ holds. I know that this is easily provable for the euclidean norm $\|\cdot\|_{2}$ , but the proof I know involves the Cauchy-Schwarz inequality which is only available for induced norms. That is why I tried making use of the fact that all norms on $\mathbb{R}^q$ are equivalent, which means $$\exists\, \alpha,\beta>0\,\, \forall v\in\mathbb{R}^q:\alpha\|v\|\le\|v\|_{2}\le\beta\|v\|. $$ Using the triangle equality for $\|\cdot\|_{2}$ , this yields $$ \alpha\left\|\int_{a}^{b} f\,\mathrm{d}x\right\| \le \left\|\int_{a}^{b} f\,\mathrm{d}x\right\|_{2}\le\int_{a}^{b} \| f\|_{2}\,\mathrm{d}x\le \beta\int_{a}^{b} \| f\|\,\mathrm{d}x,$$ but as there is no more information about $\alpha$ and $\beta$ , I cannot conclude the above inequality. I appreciate any help.","Given an arbitrary norm on . For a continous function , I want to find out whether holds. I know that this is easily provable for the euclidean norm , but the proof I know involves the Cauchy-Schwarz inequality which is only available for induced norms. That is why I tried making use of the fact that all norms on are equivalent, which means Using the triangle equality for , this yields but as there is no more information about and , I cannot conclude the above inequality. I appreciate any help.","\mathbb{R}^q f: [a,b]\times\mathbb{R}^q \rightarrow \mathbb{R}^q  \left\|\int_{a}^{b} f(x,y_1,...,y_q)\,\mathrm{d}x\right\| \le \int_{a}^{b} \| f(x,y_1,...,y_q)\|\,\mathrm{d}x \|\cdot\|_{2} \mathbb{R}^q \exists\, \alpha,\beta>0\,\, \forall v\in\mathbb{R}^q:\alpha\|v\|\le\|v\|_{2}\le\beta\|v\|.  \|\cdot\|_{2}  \alpha\left\|\int_{a}^{b} f\,\mathrm{d}x\right\| \le \left\|\int_{a}^{b} f\,\mathrm{d}x\right\|_{2}\le\int_{a}^{b} \| f\|_{2}\,\mathrm{d}x\le \beta\int_{a}^{b} \| f\|\,\mathrm{d}x, \alpha \beta","['integration', 'multivariable-calculus']"
8,N-dependent (even) function integral,N-dependent (even) function integral,,"We want to compute, for any $n \in \mathbb{N}$ the following integral: $$\int_{-1}^{1} \frac{x^n}{\sqrt[n]{1+x}+\sqrt[n]{1-x}}dx$$ My attempt:  if $n$ is odd, the integral is trivially equal to $0$ since $x^n/(\sqrt[n]{1+x}+\sqrt[n]{1-x})$ is itself an odd function. If $n$ is even, so does $x^n/(\sqrt[n]{1+x}+\sqrt[n]{1-x})$ and therefore $$\int_{-1}^{1} \frac{x^n}{\sqrt[n]{1+x}+\sqrt[n]{1-x}}dx=2\int_{0}^{1} \frac{x^n}{\sqrt[n]{1+x}+\sqrt[n]{1-x}}dx$$ but this is not really helping. Any ideas? Thanks","We want to compute, for any the following integral: My attempt:  if is odd, the integral is trivially equal to since is itself an odd function. If is even, so does and therefore but this is not really helping. Any ideas? Thanks",n \in \mathbb{N} \int_{-1}^{1} \frac{x^n}{\sqrt[n]{1+x}+\sqrt[n]{1-x}}dx n 0 x^n/(\sqrt[n]{1+x}+\sqrt[n]{1-x}) n x^n/(\sqrt[n]{1+x}+\sqrt[n]{1-x}) \int_{-1}^{1} \frac{x^n}{\sqrt[n]{1+x}+\sqrt[n]{1-x}}dx=2\int_{0}^{1} \frac{x^n}{\sqrt[n]{1+x}+\sqrt[n]{1-x}}dx,"['calculus', 'integration', 'analysis', 'definite-integrals', 'even-and-odd-functions']"
9,Finding zeros of function by integration: a novel relationship or not?,Finding zeros of function by integration: a novel relationship or not?,,"It seems that in certain cases one can find the zero of a function by solving an integration problem instead. This surprises me, and I am wondering to what extent this (1) has been studied, and/or (2) is interesting. For example, consider $f(x) = x^3 +ax -1$. This has a (unique) zero on the positive real axis , $x_0 \in \mathbb R^+$ (for any value of $a \in \mathbb R$). I am surprised to find that one can express this zero as follows: $$ x_0 =  \frac{3}{4a} \left( 2 - \frac{1}{\mathcal I} \right) \qquad \textrm{ where }\qquad \mathcal I := \frac{1}{\pi} \int_0^\infty \frac{1}{1+(y^{2/3}-a)^2 \; y^{2/3}} \mathrm d y. $$ I was indeed able to prove this using some unconventional methods (which can be found in a physicist's toolbox). Is the above correspondence surprising to mathematicians? (By the way, the fact that the $f(x)$ I chose above is a third-order polynomial is a red herring, as I found similar relationships where $f(x)$ contains, for example, logarithms.)","It seems that in certain cases one can find the zero of a function by solving an integration problem instead. This surprises me, and I am wondering to what extent this (1) has been studied, and/or (2) is interesting. For example, consider $f(x) = x^3 +ax -1$. This has a (unique) zero on the positive real axis , $x_0 \in \mathbb R^+$ (for any value of $a \in \mathbb R$). I am surprised to find that one can express this zero as follows: $$ x_0 =  \frac{3}{4a} \left( 2 - \frac{1}{\mathcal I} \right) \qquad \textrm{ where }\qquad \mathcal I := \frac{1}{\pi} \int_0^\infty \frac{1}{1+(y^{2/3}-a)^2 \; y^{2/3}} \mathrm d y. $$ I was indeed able to prove this using some unconventional methods (which can be found in a physicist's toolbox). Is the above correspondence surprising to mathematicians? (By the way, the fact that the $f(x)$ I chose above is a third-order polynomial is a red herring, as I found similar relationships where $f(x)$ contains, for example, logarithms.)",,"['calculus', 'integration', 'roots']"
10,Hermite Polynomial Integral Limit,Hermite Polynomial Integral Limit,,"I'm trying to find the following limit: $$ \underset{n \to \infty}{\textrm{lim}} \frac{\sqrt[3]{n}}{2^n n! \sqrt{\pi}}\ \int_{\sqrt{2n+1}}^{\infty}\textrm{H}^2_n(x) e^{-x^2}dx $$ Where H is a hermite polynomial (the physicist kind). I'm pretty sure the limit is $$ \frac{1}{3^{2/3}\Gamma^2(1/3)} $$ This is based on an asymptotic formula using Airy functions. Any thoughts on how to find the limit? If it is any help, the limit can also be represented as $$ \underset{n\rightarrow \infty}{\textrm{lim}}\frac{\sqrt[3]{n}}{2 \cdot n!}\frac{d^n }{dx^n} \left .\frac{\textrm{erfc}\left ( \sqrt{\tfrac{1-x}{1+x}} \sqrt{2n+1}\right )}{1-x}  \right |_{x=0} $$","I'm trying to find the following limit: $$ \underset{n \to \infty}{\textrm{lim}} \frac{\sqrt[3]{n}}{2^n n! \sqrt{\pi}}\ \int_{\sqrt{2n+1}}^{\infty}\textrm{H}^2_n(x) e^{-x^2}dx $$ Where H is a hermite polynomial (the physicist kind). I'm pretty sure the limit is $$ \frac{1}{3^{2/3}\Gamma^2(1/3)} $$ This is based on an asymptotic formula using Airy functions. Any thoughts on how to find the limit? If it is any help, the limit can also be represented as $$ \underset{n\rightarrow \infty}{\textrm{lim}}\frac{\sqrt[3]{n}}{2 \cdot n!}\frac{d^n }{dx^n} \left .\frac{\textrm{erfc}\left ( \sqrt{\tfrac{1-x}{1+x}} \sqrt{2n+1}\right )}{1-x}  \right |_{x=0} $$",,"['integration', 'analysis', 'limits']"
11,Theoretical integral curve - fields lemma proof,Theoretical integral curve - fields lemma proof,,"I need help or a thorough proof on understanding the following theoretical lemma regarding integral curves and fields. This is a part of the ""Introduction to Partial Differential Equations with Applications - Zachmanoglou"" . First of all, where to begin from for proving part (a) of the lemma-protasis ? Secondly, I can't comprehend part (b). Let $C$ be an integral curve of the vector field $V = (P,Q,R)$ and suppose that $C$ is given parametrically by the equations    $$x=x(t), \; \; y = y(t), \; \; z = z(t) \; ; \; t \in I$$   where the functions $x(t), y(t)$ and $z(t)$  are in $C^1(I)$ and the tangent vector    $$T(t) = \bigg(\frac{\mathrm{d}x(t)}{dt},\frac{\mathrm{d}y(t)}{dt}, \frac{\mathrm{d}z(t)}{dt}\bigg)$$   never vanishes for $t \in I$. (a) Show that there exists a function $μ(t)$ in $C^1(I)$ such that for every $t \in I$ it is :   $$μ(t) \neq 0 \; \; \text{and} \; \; V(x(t),y(t),z(t)) = μ(t)T(t)$$ (b) Let $t = τ(t)$ be a solution of the differential equation   $$\frac{\mathrm{d}t}{\mathrm{d}τ}=μ(t)$$   where $τ$ varies over some interval $I'$ as $t$ varies over $I$. Set   $$\bar{x}(τ)=x(t(τ)), \; \bar{y}(τ)=y(t(τ)), \; \bar{z}(τ)=\bar{z}(t(τ))$$   Show that in terms of the new parametric representation    $$x = \bar{x}(τ), \; \; y = \bar{y}(τ), \; \; z = \bar{z}(τ) \; ; \; t \in I'$$   the curve $C$ is a solution of the system of equations associated with $V$   $$\frac{\mathrm{d}x}{\mathrm{d}τ}=P, \; \frac{\mathrm{d}y}{\mathrm{d}τ}=Q, \; \frac{\mathrm{d}z}{\mathrm{d}τ}=R$$","I need help or a thorough proof on understanding the following theoretical lemma regarding integral curves and fields. This is a part of the ""Introduction to Partial Differential Equations with Applications - Zachmanoglou"" . First of all, where to begin from for proving part (a) of the lemma-protasis ? Secondly, I can't comprehend part (b). Let $C$ be an integral curve of the vector field $V = (P,Q,R)$ and suppose that $C$ is given parametrically by the equations    $$x=x(t), \; \; y = y(t), \; \; z = z(t) \; ; \; t \in I$$   where the functions $x(t), y(t)$ and $z(t)$  are in $C^1(I)$ and the tangent vector    $$T(t) = \bigg(\frac{\mathrm{d}x(t)}{dt},\frac{\mathrm{d}y(t)}{dt}, \frac{\mathrm{d}z(t)}{dt}\bigg)$$   never vanishes for $t \in I$. (a) Show that there exists a function $μ(t)$ in $C^1(I)$ such that for every $t \in I$ it is :   $$μ(t) \neq 0 \; \; \text{and} \; \; V(x(t),y(t),z(t)) = μ(t)T(t)$$ (b) Let $t = τ(t)$ be a solution of the differential equation   $$\frac{\mathrm{d}t}{\mathrm{d}τ}=μ(t)$$   where $τ$ varies over some interval $I'$ as $t$ varies over $I$. Set   $$\bar{x}(τ)=x(t(τ)), \; \bar{y}(τ)=y(t(τ)), \; \bar{z}(τ)=\bar{z}(t(τ))$$   Show that in terms of the new parametric representation    $$x = \bar{x}(τ), \; \; y = \bar{y}(τ), \; \; z = \bar{z}(τ) \; ; \; t \in I'$$   the curve $C$ is a solution of the system of equations associated with $V$   $$\frac{\mathrm{d}x}{\mathrm{d}τ}=P, \; \frac{\mathrm{d}y}{\mathrm{d}τ}=Q, \; \frac{\mathrm{d}z}{\mathrm{d}τ}=R$$",,"['integration', 'ordinary-differential-equations', 'partial-differential-equations']"
12,What makes this integral so hard to solve?,What makes this integral so hard to solve?,,"The integral $$ F(x) = \int \frac{x}{\sqrt{x^4+10x^2-96x-71}} \, dx ,$$ has a closed-form expression, namely, $$F(x) = -\frac{1}{8} \ln\left((x^6 + 15x^4-80x^3 + 27x^2-528x + 781)\sqrt{x^4 + 10x^2-96x - 71} - (x^8 + 20x^6 -128 x^5 + 54x^4-1408x^3 + 3124x^2 + 10001)\right) \\ +C. $$ It seems that most, if not all, computer integrators such as Mathematica fail to find its antiderivative. What makes this integral so hard to solve, and how do you begin solving it?","The integral $$ F(x) = \int \frac{x}{\sqrt{x^4+10x^2-96x-71}} \, dx ,$$ has a closed-form expression, namely, $$F(x) = -\frac{1}{8} \ln\left((x^6 + 15x^4-80x^3 + 27x^2-528x + 781)\sqrt{x^4 + 10x^2-96x - 71} - (x^8 + 20x^6 -128 x^5 + 54x^4-1408x^3 + 3124x^2 + 10001)\right) \\ +C. $$ It seems that most, if not all, computer integrators such as Mathematica fail to find its antiderivative. What makes this integral so hard to solve, and how do you begin solving it?",,"['integration', 'indefinite-integrals']"
13,"Integrate over sinusoidal region $\int\int(1 - \cos 2\theta ) (1 + \cos(\frac{\theta + 2 \pi u}2 )) du \,d\theta$",Integrate over sinusoidal region,"\int\int(1 - \cos 2\theta ) (1 + \cos(\frac{\theta + 2 \pi u}2 )) du \,d\theta","I'm trying to obtain a closed form expression for the following definite integral in $\mathbb{R}^2$: $$ \int_{0}^{\pi} \int_{u = 0}^{u= A \sin\theta} f(\theta, u)\,\mathrm{d}u \,\mathrm{d}\theta ~,\qquad 0 < A<1,\qquad f(\theta,u)\equiv \bigl(1 - \cos 2\theta \bigr) \Bigl(1 + \cos\bigl(\frac{\theta + 2 \pi u}2 \bigr)\Bigr) $$ The region of integration is the ""hump"" under the sine curve with amplitude $A$. Note that $f(\theta,u)$ is asymmetric (either mirror or rotational) within $0<\theta<\pi$. Nonetheless, it is rotationally symmetric $f(2\pi - \theta, 1- u) = f(\theta,u)$ if one considers twice the integral by extending the domain of $\theta$. The figures below display the integrand as the yellow surface (front and back view). The sine hump region of integration is enclosed by the blue wall, and outside the region is dimmed. The top row is $f(\theta, u)$ itself, the middle row is for region $A = 9/10$, and the bottom row is $A = 1/4$. My Questions Can this deceivingly simple-looking integral be evaluated to a closed form as a function of $A$? Regardless of (1), is this integral related to some ""named"" integral or special functions? My Failed Attempts So Far This is a 2-dim region so the basic two ways are ""vertical first"" and ""horizontal first"". Doing vertical first $\int \mathrm{d}u$ for $u = 0$ to $u = A \sin\theta$ yields a nested sine $\sin\bigl( \frac{\theta}2 + A \pi \sin\theta\bigr)$. It doesn't take long for my Mathematica to declare that this cannot be evaluated (returning the input as is after 50 sec). Doing horizontal first $\int \mathrm{d}\theta$ for $\theta = \theta_u$ to $u = \pi - \theta_u$, where $\theta_u \equiv \arcsin(\frac{u}A)$, produces many terms like $\sin\bigl( \pi u + \frac{\theta_u}2 \bigr)$ which I don't know how to handle. My Mathematica churned out something (after about 3 min) with many complex (imaginary) terms and a lot of Erf function (antiderivative of Gaussian). On the coding side, I tried ComplexExpand with FullSimplify etc but it just got worse (over 30 thousand terms) and less comprehensible. So far I don't have any good ideas for change of variables. Any input would be appreciated.","I'm trying to obtain a closed form expression for the following definite integral in $\mathbb{R}^2$: $$ \int_{0}^{\pi} \int_{u = 0}^{u= A \sin\theta} f(\theta, u)\,\mathrm{d}u \,\mathrm{d}\theta ~,\qquad 0 < A<1,\qquad f(\theta,u)\equiv \bigl(1 - \cos 2\theta \bigr) \Bigl(1 + \cos\bigl(\frac{\theta + 2 \pi u}2 \bigr)\Bigr) $$ The region of integration is the ""hump"" under the sine curve with amplitude $A$. Note that $f(\theta,u)$ is asymmetric (either mirror or rotational) within $0<\theta<\pi$. Nonetheless, it is rotationally symmetric $f(2\pi - \theta, 1- u) = f(\theta,u)$ if one considers twice the integral by extending the domain of $\theta$. The figures below display the integrand as the yellow surface (front and back view). The sine hump region of integration is enclosed by the blue wall, and outside the region is dimmed. The top row is $f(\theta, u)$ itself, the middle row is for region $A = 9/10$, and the bottom row is $A = 1/4$. My Questions Can this deceivingly simple-looking integral be evaluated to a closed form as a function of $A$? Regardless of (1), is this integral related to some ""named"" integral or special functions? My Failed Attempts So Far This is a 2-dim region so the basic two ways are ""vertical first"" and ""horizontal first"". Doing vertical first $\int \mathrm{d}u$ for $u = 0$ to $u = A \sin\theta$ yields a nested sine $\sin\bigl( \frac{\theta}2 + A \pi \sin\theta\bigr)$. It doesn't take long for my Mathematica to declare that this cannot be evaluated (returning the input as is after 50 sec). Doing horizontal first $\int \mathrm{d}\theta$ for $\theta = \theta_u$ to $u = \pi - \theta_u$, where $\theta_u \equiv \arcsin(\frac{u}A)$, produces many terms like $\sin\bigl( \pi u + \frac{\theta_u}2 \bigr)$ which I don't know how to handle. My Mathematica churned out something (after about 3 min) with many complex (imaginary) terms and a lot of Erf function (antiderivative of Gaussian). On the coding side, I tried ComplexExpand with FullSimplify etc but it just got worse (over 30 thousand terms) and less comprehensible. So far I don't have any good ideas for change of variables. Any input would be appreciated.",,"['integration', 'multivariable-calculus', 'special-functions', 'closed-form', 'complex-integration']"
14,The integral of the standard mollifier function,The integral of the standard mollifier function,,"Firstly, I would like to say that I know that it was asked before here , but the question wasn't answered. I'm self studying PDE by Evans's book and I'm trying understand why the integral of a mollifier is $1$. I will put the definition of a mollifier below according Evans's book. $\textbf{Definition:}$ Let $\eta: \mathbb{R}^n \longrightarrow \mathbb{R}$ defined by $\eta(x) := \begin{cases} C \exp \left( \frac{1}{|x|^2 - 1} \right) \hspace{1.0cm} \text{if} \ |x| < 1\\ 0 \hspace{3.6cm} \text{if} \ |x| \geq 1, \end{cases}$ the constant $C > 0$ selected so that $\int_{\mathbb{R}^n} \eta \ dx = 1$. The function $\eta$ is called $\textit{the standard mollifier}$. I'll be gratefull if someone can tells me how can I compute $\int_{\mathbb{R}^n} \eta \ dx$.","Firstly, I would like to say that I know that it was asked before here , but the question wasn't answered. I'm self studying PDE by Evans's book and I'm trying understand why the integral of a mollifier is $1$. I will put the definition of a mollifier below according Evans's book. $\textbf{Definition:}$ Let $\eta: \mathbb{R}^n \longrightarrow \mathbb{R}$ defined by $\eta(x) := \begin{cases} C \exp \left( \frac{1}{|x|^2 - 1} \right) \hspace{1.0cm} \text{if} \ |x| < 1\\ 0 \hspace{3.6cm} \text{if} \ |x| \geq 1, \end{cases}$ the constant $C > 0$ selected so that $\int_{\mathbb{R}^n} \eta \ dx = 1$. The function $\eta$ is called $\textit{the standard mollifier}$. I'll be gratefull if someone can tells me how can I compute $\int_{\mathbb{R}^n} \eta \ dx$.",,"['integration', 'multivariable-calculus', 'partial-differential-equations']"
15,Volume content of a region $A \in \mathbb{R}^3$ is different for different order of integration$?$,Volume content of a region  is different for different order of integration,A \in \mathbb{R}^3 ?,"I'm interested in computing the volume content of the region $A \in \mathbb{R}^3$ which is enclosed by the following three surfaces : $$0<x+y+z<1$$ $$0<y+z<1$$ $$0<z<1$$ Order of integration $x \rightarrow y \rightarrow z :$ $$\iiint_A\,dx\,dy\,dz=\int_{0}^{1}\int_{-z}^{1-z}\int_{-y-z}^{1-y-z}\,dx\,dy\,dz=1$$ Order of integration $x \rightarrow z \rightarrow y :$ $$\iiint_A\,dx\,dy\,dz=\int_{-1}^{1}\int_{-y}^{1-y}\int_{-y-z}^{1-y-z}\,dx\,dz\,dy=2$$ Order of integration $z \rightarrow x \rightarrow y :$ $$\iiint_A\,dx\,dy\,dz=\int_{-1}^{1}\int_{-1}^{1}\int_{-x-y}^{1-x-y}\,dz\,dx\,dy=4$$ What kind of dark magic is this$?$ Am I doing anything wrong in taking the limits of integration$?$","I'm interested in computing the volume content of the region $A \in \mathbb{R}^3$ which is enclosed by the following three surfaces : $$0<x+y+z<1$$ $$0<y+z<1$$ $$0<z<1$$ Order of integration $x \rightarrow y \rightarrow z :$ $$\iiint_A\,dx\,dy\,dz=\int_{0}^{1}\int_{-z}^{1-z}\int_{-y-z}^{1-y-z}\,dx\,dy\,dz=1$$ Order of integration $x \rightarrow z \rightarrow y :$ $$\iiint_A\,dx\,dy\,dz=\int_{-1}^{1}\int_{-y}^{1-y}\int_{-y-z}^{1-y-z}\,dx\,dz\,dy=2$$ Order of integration $z \rightarrow x \rightarrow y :$ $$\iiint_A\,dx\,dy\,dz=\int_{-1}^{1}\int_{-1}^{1}\int_{-x-y}^{1-x-y}\,dz\,dx\,dy=4$$ What kind of dark magic is this$?$ Am I doing anything wrong in taking the limits of integration$?$",,"['integration', 'definite-integrals']"
16,Finding a solution to a PDE arising from using Feynman's trick of differentiating under the integral sign,Finding a solution to a PDE arising from using Feynman's trick of differentiating under the integral sign,,"When considering possible generalisations of the improper integral $$\int^\infty_0 e^{-x^2} \cos (x^2) \, dx,$$ (for its evaluation see here ) one that immediately comes to mind is $$u(a,b) = \int^\infty_0 e^{-ax^2} \cos (bx^2) \, dx, \quad a > 0, b \in \mathbb{R}.$$ This integral can be readily evaluated using complex methods. But if we confine our attention to using only real methods, employing a method reminiscent of the most common way used to evaluate the Gaussian integral, by squaring the integral before converting it to polar coordinates similar to what was done here , one finds $$u(a,b) = \frac{\sqrt{\pi}}{2 \sqrt{2} \sqrt{a^2 + b^2}} \sqrt{a + \sqrt{a^2 + b^2}}. \tag1$$ As an alternative (real) method to squaring the integral one can try Feynman's trick by differentiating under the integral sign. Differentiating with respect to $a$ one has $$u_a(a,b) = - \int^\infty_0 x^2 e^{-ax^2} \cos (bx^2) \, dx.$$ Differentiating with respect to $b$ $$u_b(a,b) = - \int^\infty_0 x e^{-ax^2} \cdot x \sin (bx^2) \, dx,$$ followed by a single integration by parts gives $$u_b (a,b) = -\frac{1}{2b} u(a,b) + \frac{a}{b} \int^\infty_0 x^2 e^{-ax^2} \cos (b x^2) \, dx = -\frac{1}{2b} u(a,b) - \frac{a}{b} u_a(a,b),$$ which after rearranging becomes $$2a u_a + 2b u_b + u = 0. \tag2$$ Two boundary conditions for the above first-order semi-linear partial differential equation can be found. They are $$u(a,0) = \int^\infty_0 e^{-ax^2} \, dx = \sqrt{\frac{\pi}{4a}},$$ and $$u(0,b) = \int^\infty_0 \cos (b x^2) \, dx = \sqrt{\frac{\pi}{8b}}.$$ I understand the method of characteristics may be used to find a solution to (2). If this is correct can someone show me how this can be done so we arrive at (1). If not, what other method(s) could be used to find a solution to (2)? I should add if the limits of integration are extended to all $\mathbb{R}$, a further generalisation of the improper integral can be found here where it was evaluated using a very slick and simple complex method.","When considering possible generalisations of the improper integral $$\int^\infty_0 e^{-x^2} \cos (x^2) \, dx,$$ (for its evaluation see here ) one that immediately comes to mind is $$u(a,b) = \int^\infty_0 e^{-ax^2} \cos (bx^2) \, dx, \quad a > 0, b \in \mathbb{R}.$$ This integral can be readily evaluated using complex methods. But if we confine our attention to using only real methods, employing a method reminiscent of the most common way used to evaluate the Gaussian integral, by squaring the integral before converting it to polar coordinates similar to what was done here , one finds $$u(a,b) = \frac{\sqrt{\pi}}{2 \sqrt{2} \sqrt{a^2 + b^2}} \sqrt{a + \sqrt{a^2 + b^2}}. \tag1$$ As an alternative (real) method to squaring the integral one can try Feynman's trick by differentiating under the integral sign. Differentiating with respect to $a$ one has $$u_a(a,b) = - \int^\infty_0 x^2 e^{-ax^2} \cos (bx^2) \, dx.$$ Differentiating with respect to $b$ $$u_b(a,b) = - \int^\infty_0 x e^{-ax^2} \cdot x \sin (bx^2) \, dx,$$ followed by a single integration by parts gives $$u_b (a,b) = -\frac{1}{2b} u(a,b) + \frac{a}{b} \int^\infty_0 x^2 e^{-ax^2} \cos (b x^2) \, dx = -\frac{1}{2b} u(a,b) - \frac{a}{b} u_a(a,b),$$ which after rearranging becomes $$2a u_a + 2b u_b + u = 0. \tag2$$ Two boundary conditions for the above first-order semi-linear partial differential equation can be found. They are $$u(a,0) = \int^\infty_0 e^{-ax^2} \, dx = \sqrt{\frac{\pi}{4a}},$$ and $$u(0,b) = \int^\infty_0 \cos (b x^2) \, dx = \sqrt{\frac{\pi}{8b}}.$$ I understand the method of characteristics may be used to find a solution to (2). If this is correct can someone show me how this can be done so we arrive at (1). If not, what other method(s) could be used to find a solution to (2)? I should add if the limits of integration are extended to all $\mathbb{R}$, a further generalisation of the improper integral can be found here where it was evaluated using a very slick and simple complex method.",,"['integration', 'partial-differential-equations', 'improper-integrals']"
17,Integration by parts and non-absolutely continuous distributions,Integration by parts and non-absolutely continuous distributions,,"Let $x\in [a,b]$ be a real random variable with distribution $H$ that is not absolutely continuous (w.r.t Lebesgue measure).  I saw this in a paper: $$ \int_a^b xH(dx) = b-\int_a^bH(x)dx. $$ I get it is integration by parts but I am not sure why it should work. Is there some implicit assumption on $H$ at work here or does this always hold?  Edit: I think there may be an implicit assumption that $H$ is a Borel measure.","Let $x\in [a,b]$ be a real random variable with distribution $H$ that is not absolutely continuous (w.r.t Lebesgue measure).  I saw this in a paper: $$ \int_a^b xH(dx) = b-\int_a^bH(x)dx. $$ I get it is integration by parts but I am not sure why it should work. Is there some implicit assumption on $H$ at work here or does this always hold?  Edit: I think there may be an implicit assumption that $H$ is a Borel measure.",,"['integration', 'measure-theory', 'continuity', 'lebesgue-integral', 'lebesgue-measure']"
18,A quicker way to approach the indefinite integral $\int{\frac{x^3dx}{1+x^5}}$,A quicker way to approach the indefinite integral,\int{\frac{x^3dx}{1+x^5}},"Problem:   Evaluate the integral $$\int{\frac{x^3dx}{1+x^5}}$$ Source: Given to me as a challenge. I can't seem to find a valid substitution for the integral. Also the way should be quick enough to apply in a timed test. I don't know the answer and I have almost given up. I called the challenger and he said that I can ask for help but not look up on Wolfram or any such website as it won't be a proper solution. I don't think it is solvable though :p My try: Well, I have tried to get around by using many substitutions which make the thing more complicated than it already is. Also I'm looking for the QUICKEST way to approach this problem in case  it happens to meet me on a test.","Problem:   Evaluate the integral $$\int{\frac{x^3dx}{1+x^5}}$$ Source: Given to me as a challenge. I can't seem to find a valid substitution for the integral. Also the way should be quick enough to apply in a timed test. I don't know the answer and I have almost given up. I called the challenger and he said that I can ask for help but not look up on Wolfram or any such website as it won't be a proper solution. I don't think it is solvable though :p My try: Well, I have tried to get around by using many substitutions which make the thing more complicated than it already is. Also I'm looking for the QUICKEST way to approach this problem in case  it happens to meet me on a test.",,"['integration', 'indefinite-integrals']"
19,An integral giving back a function which is a factor in the integrand,An integral giving back a function which is a factor in the integrand,,"I suspect that, under opportune assumptions on $\varphi:\mathbb{R}^3\times\mathbb{R}\to\mathbb{R}$ , $(\boldsymbol{\xi},\tau)\mapsto \varphi(\boldsymbol{\xi},\tau)$ , such as $\varphi\in C_c^2(\mathbb{R}^4)$ , the following identity holds, for any $\alpha\in\mathbb{R}$ : $$ \int_{\mathbb{R}^3}\frac{\nabla_{\boldsymbol{\xi}}^2\varphi(\boldsymbol{y},t-\alpha\|\boldsymbol{x}-\boldsymbol{y}\|)}{\|\boldsymbol{x}-\boldsymbol{y}\|} -\frac{\alpha^2\ddot\varphi(\boldsymbol{y},t-\alpha\|\boldsymbol{x}-\boldsymbol{y}\|)}{\|\boldsymbol{x}-\boldsymbol{y}\|} d\mu_{\boldsymbol{y}}=-4\pi\varphi(\boldsymbol{x},t)\label{1}\tag{1}$$ where $\nabla_{\boldsymbol{\xi}}^2\varphi$ is the Laplacian calculated with respect to the first tridimensional variable of $\varphi$ (called $\boldsymbol{\xi}$ at the beginning of the post) and $\ddot\varphi$ is the second order derivative with respect to the second variable of $\varphi$ . I am convinced that this equality holds because, if it did, it could be used to rigourously prove that the Lorenz gauge retarded potential $\boldsymbol{A}$ satisfies the equality $$ \nabla^2\boldsymbol{A}-\varepsilon_0\mu_0\frac{\partial^2 }{\partial t^2}\boldsymbol{A}=-\mu_0\boldsymbol{J} $$ in the same way the same equality with $\alpha=0$ , which holds as proved here , can be used to rigourously prove that the the magnetostatic potential is such that $$ \nabla^2\boldsymbol{A}=-\mu_0\boldsymbol{J}. $$ Can anybody help me to prove the equality \eqref{1}? As pointed out in the comments, whose author Daniel Fischer I thank again, the integral might be calculated by integrating by parts and taking the limit of the (Riemann) integral $$ \int_{\mathbb{R}^3\setminus{B(\boldsymbol{x},\delta)}}\frac{\nabla_{\boldsymbol{\xi}}^2\varphi(\boldsymbol{y},t-\alpha\|\boldsymbol{x}-\boldsymbol{y}\|)}{\|\boldsymbol{x}-\boldsymbol{y}\|} -\frac{\alpha^2\ddot\varphi(\boldsymbol{y},t-\alpha\|\boldsymbol{x}-\boldsymbol{y}\|)}{\|\boldsymbol{x}-\boldsymbol{y}\|} dy_1dy_2dy_3 $$ as $\delta\to 0$ , but, in the formula of integration by parts $$ \int_\Omega \frac{\partial f}{\partial x_j} g\ d^3x = \int_{\partial \Omega} fgn_j\, d\sigma -\int_{\Omega} f\frac{\partial g}{\partial x_j}\, d^3x $$ (where $n_j$ is the $j$ -th component of the external normal vector to $\partial\Omega$ ), I do not know what to chose as $f$ and $g$ in our integrand. I heartily thank any answerer.","I suspect that, under opportune assumptions on , , such as , the following identity holds, for any : where is the Laplacian calculated with respect to the first tridimensional variable of (called at the beginning of the post) and is the second order derivative with respect to the second variable of . I am convinced that this equality holds because, if it did, it could be used to rigourously prove that the Lorenz gauge retarded potential satisfies the equality in the same way the same equality with , which holds as proved here , can be used to rigourously prove that the the magnetostatic potential is such that Can anybody help me to prove the equality \eqref{1}? As pointed out in the comments, whose author Daniel Fischer I thank again, the integral might be calculated by integrating by parts and taking the limit of the (Riemann) integral as , but, in the formula of integration by parts (where is the -th component of the external normal vector to ), I do not know what to chose as and in our integrand. I heartily thank any answerer.","\varphi:\mathbb{R}^3\times\mathbb{R}\to\mathbb{R} (\boldsymbol{\xi},\tau)\mapsto \varphi(\boldsymbol{\xi},\tau) \varphi\in C_c^2(\mathbb{R}^4) \alpha\in\mathbb{R} 
\int_{\mathbb{R}^3}\frac{\nabla_{\boldsymbol{\xi}}^2\varphi(\boldsymbol{y},t-\alpha\|\boldsymbol{x}-\boldsymbol{y}\|)}{\|\boldsymbol{x}-\boldsymbol{y}\|} -\frac{\alpha^2\ddot\varphi(\boldsymbol{y},t-\alpha\|\boldsymbol{x}-\boldsymbol{y}\|)}{\|\boldsymbol{x}-\boldsymbol{y}\|} d\mu_{\boldsymbol{y}}=-4\pi\varphi(\boldsymbol{x},t)\label{1}\tag{1} \nabla_{\boldsymbol{\xi}}^2\varphi \varphi \boldsymbol{\xi} \ddot\varphi \varphi \boldsymbol{A} 
\nabla^2\boldsymbol{A}-\varepsilon_0\mu_0\frac{\partial^2 }{\partial t^2}\boldsymbol{A}=-\mu_0\boldsymbol{J}
 \alpha=0 
\nabla^2\boldsymbol{A}=-\mu_0\boldsymbol{J}.
 
\int_{\mathbb{R}^3\setminus{B(\boldsymbol{x},\delta)}}\frac{\nabla_{\boldsymbol{\xi}}^2\varphi(\boldsymbol{y},t-\alpha\|\boldsymbol{x}-\boldsymbol{y}\|)}{\|\boldsymbol{x}-\boldsymbol{y}\|} -\frac{\alpha^2\ddot\varphi(\boldsymbol{y},t-\alpha\|\boldsymbol{x}-\boldsymbol{y}\|)}{\|\boldsymbol{x}-\boldsymbol{y}\|} dy_1dy_2dy_3
 \delta\to 0 
\int_\Omega \frac{\partial f}{\partial x_j} g\ d^3x = \int_{\partial \Omega} fgn_j\, d\sigma -\int_{\Omega} f\frac{\partial g}{\partial x_j}\, d^3x
 n_j j \partial\Omega f g","['real-analysis', 'integration', 'multivariable-calculus', 'lebesgue-integral']"
20,"Probs. 10 (a), (b), and (c), Chap. 6, in Baby Rudin: Holder's Inequality for Integrals","Probs. 10 (a), (b), and (c), Chap. 6, in Baby Rudin: Holder's Inequality for Integrals",,"Here is Prob. 10, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $p$ and $q$ be positive real numbers such that $$ \frac{1}{p} +  \frac{1}{q} =1. $$ Prove the following statements. (a) If $u \geq 0$ and $v \geq 0$ , then $$ u v \leq \frac{u^p}{p} + \frac{v^q}{q}. $$ Equality holds if and only if $u^p = v^q$ . (b) If $f \in \mathscr{R}$ , $g \in \mathscr{R}$ , $f \geq 0$ , $g \geq 0$ , and $$ \int_a^b f^p \ \mathrm{d} \alpha = 1 = \int_a^b g^q \ \mathrm{d} \alpha, $$ then $$ \int_a^b f g \ \mathrm{d} \alpha \leq 1. $$ (c) If $f$ and $g$ are complex functions in $\mathscr{R} (\alpha) $ , then $$ \left\lvert \int_a^b f g \ \mathrm{d} \alpha \right\rvert \leq \left\{ \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right\}^{1/p} \left\{ \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right\}^{1/q}. $$ This is Holder's inequality. When $p = q = 2$ it is usually called the Schwarz inequality. (Note that Theorem 1.35 is a very special case of this. ) (d) Show that Holder's inequality is also true for the ""improper"" integrals described in Exercises 7 and 8. Here is Theorem 1.35 in Rudin, 3rd edition: If $a_1, \ldots, a_n$ and $b_1, \ldots, b_n$ are complex numbers, then $$ \left\lvert \sum_{j=1}^n a_j \overline{b_j} \right\rvert^2 \leq \sum_{j=1}^n \left\lvert a_j \right\rvert^2 \sum_{j=1}^n \left\lvert b_j \right\rvert^2. $$ Here are the links to my Math SE posts on Probs. 7 and 8, Chap. 6, in Baby Rudin, 3rd edition: Prob. 7 (a), Chap. 6, in Baby Rudin: If $f$ is integrable on $[c, 1]$ for every $c>0$, then $\int_0^1 f(x) \ \mathrm{d}x = $ . . . Prob. 7 (b), Chap. 6, in Baby Rudin: Example of a function such that $\lim_{c \to 0+} \int_c^1 f(x) \ \mathrm{d}x$ exists but . . . Prob. 8, Chap. 6, in Baby Rudin: The Integral Test for Convergence of Series My Attempt: As $$ \frac{1}{p}  + \frac{1}{q} = 1,$$ so $$ q+p = pq,  $$ which implies that $$ pq- p - q = 0, $$ and hence $$ pq - p - q + 1 = 1, $$ that is, $$ (p-1) (q-1) = 1. \tag{0} $$ So $$ (q-1)p = (q-1)(p-1 + 1) = (q-1)(p-1) + q - 1 = 1 + q-1 = q. \tag{1} $$ in what follows, we will be using (0) and (1). Prob. 10 (a) If $u = 0$ or $v=0$ , then $$ 0 = uv \leq \frac{u^p}{p} + \frac{v^q}{q}. $$ So let us suppose that $u$ and $v$ both are positive real numbers. For any fixed $v > 0$ , let us define a function $f_v \colon (0, +\infty) \longrightarrow \mathbb{R}$ by the formula $$ f_v(u) \colon= \frac{u^p}{p} + \frac{v^q}{q} - uv \ \mbox{ for all } u \in (0, +\infty). $$ Then $$ f_v^\prime(u) = u^{p-1} - v \ \mbox{ for all } u \in (0, +\infty). $$ So $f_v^\prime$ vanishes only at $$ u = v^{\frac{1}{p-1}} = v^{q-1}. $$ [Refer to (0) above.]  In fact, $$ f_v^\prime (u) \ \begin{cases} < 0 \ & \mbox{ if } 0 < u < v^{q-1}, \\ = 0 \ & \mbox{ if } u = v^{q-1}, \\ > 0 \ & \mbox{ if } u > v^{q-1}. \end{cases} $$ Thus $f_v$ is strictly decreasing on $\left( 0, v^{q-1} \right]$ , and, $f_v$ is strictly increasing on $\left[ v^{q-1}, +\infty \right)$ . This implies that $f_v$ has a relative minimum value at $u = v^{q-1}$ , and this relative minimum, being the unique extreme value of $f_v$ within the entire domain of this function, is in fact also the absolute minimum value ; moreover, from (1) above we also have $$ \begin{align}  f_v \left( v^{q-1} \right) &= \frac{ \left( v^{q-1} \right)^p }{p} + \frac{v^q}{q} - v^{q-1} v \\  &= \frac{ v^{p(q-1)} }{p} + \frac{v^q}{q} - v^q \\  &= \frac{v^q}{p} + \frac{v^q}{q} - v^q \qquad [ \mbox{ using (1) above } ] \\  &= v^q \left( \frac{1}{p} + \frac{1}{q} - 1 \right) \\ &= 0  \end{align} $$ because $$ \frac{1}{p} + \frac{1}{q} =1. $$ Hence $$ f_v(u) \geq 0 \ \mbox{ for all } u \in (0, +\infty); $$ that is, $$ \frac{u^p}{p} + \frac{v^q}{q} - uv \geq 0 \mbox{ for all } u \in (0, +\infty), $$ and so $$ uv \leq \frac{u^p}{p} + \frac{v^q}{q} $$ for all positive real numbers $u$ and $v$ . Is this proof correct? Prob. 10 (b) In this proof we will be using Theorem 6.12 (a) and (b) in Baby Rudin. Here are the links to my Math SE posts on these results. Theorem 6.12 (a) in Baby Rudin: $\int_a^b \left( f_1 + f_2 \right) d \alpha=\int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha$ https://math.stackexchange.com/questions/2327134/theorem-6-12-a-in-baby-rudin-if-f-in-mathscrr-alpha-on-a-b-then-c Theorem 6.12 (b) in Baby Rudin: If $f_1 \leq f_2$ on $[a, b]$, then $\int_a^b f_1 d\alpha \leq \int_a^b f_2 d\alpha$ And, we will also be using Theorem 6.13 (a) in Baby Rudin, which is as follows: If $f \in \mathscr{R}(\alpha)$ and $g \in \mathscr{R}(\alpha)$ on $[a, b]$ , then $fg \in \mathscr{R}(\alpha)$ . As $f \geq 0$ and $g \geq 0$ on $[a, b]$ , so we also have $$ f g \leq \frac{f^p}{p} + \frac{g^q}{q} \tag{2} $$ on $[a, b]$ . As $f \in \mathscr{R}(\alpha)$ and $g \in \mathscr{R}(\alpha)$ on $[a, b]$ , so $fg \in \mathscr{R}(\alpha)$ on $[a, b]$ , by Theorem 6.13 (a) in Baby Rudin. Moreover, $$ \begin{align} \int_a^b f g \ \mathrm{d} \alpha &\leq \int_a^b \left( \frac{f^p}{p} + \frac{g^q}{q} \right) \ \mathrm{d} \alpha \qquad \mbox{ [ using (2) and Theorem 6.12 (b) in Rudin ] } \\ &= \frac{1}{p} \int_a^b f^p \ \mathrm{d} \alpha  + \frac{1}{q} \int_a^b g^q  \ \mathrm{d} \alpha \qquad \mbox{ [ using Theorem 6.12 (a) in Rudin ] } \\ &= \frac{1}{p} \cdot 1 + \frac{1}{q} \cdot 1 \qquad \mbox{ [ using our hypothesis ] } \\ &= 1, \qquad \mbox{ [ using the condition on $p$ and $q$ in our hypothesis ] } \end{align} $$ Prob. 10 (c) In what follows, we will be using Theorems 6.11, 6.12 (a) , and 6.13 in Rudin. Here are the links to some Math SE posts on these theorems. Theorems 6.11 and 6.13 from PMA Rudin Theorem 6.11 of Rudin's Principles of Mathematical Analysis And, here is the link to my Math SE post on Prob. 2, Chap. 6, in Baby Rudin: Prob. 2, Chap. 6, in Baby Rudin: If $f\geq 0$ and continuous on $[a,b]$ with $\int_a^bf(x)\ \mathrm{d}x=0$, then $f=0$ As $f$ and $g$ are complex functions in $\mathscr{R}(\alpha)$ on $[a, b]$ , so $\lvert f \rvert$ and $\lvert g \rvert$ are real functions in $\mathscr{R}$ on $[a, b]$ , by virtue of Theorem 6.13 (b) in Baby Rudin. As $\lvert f \rvert \in \mathscr{R}(\alpha)$ and $\lvert g \rvert \in \mathscr{R}(\alpha)$ on $[a, b]$ , so $\lvert f \rvert$ and $\lvert g \rvert$ are bounded functions on $[a, b]$ . Let us put $$ M \colon= 1 + \max \left\{ \ \sup \{ \ \lvert f(x) \rvert \ \colon \ a \leq x \leq b \ \},   \sup \{ \ \lvert g(x) \rvert \ \colon \ a \leq x \leq b \ \} \ \right\}. $$ Then $M > 0$ . Let $r$ be a real number. If the mapping $y \mapsto y^r$ is continuous on $[0, +\infty)$ , then we can conclude from Theorem 6.11 in Rudin that $\lvert f \rvert^r $ and $\lvert g \rvert^r $ are also in $\mathscr{R}$ on $[a, b]$ . Thus in particular, $\lvert f \rvert^p $ and $\lvert g \rvert^q $ are in $\mathscr{R}$ on $[a, b]$ . But how to show that the mapping $y \mapsto y^r$ is continuous on $[0, +\infty)$ , especially when $r$ is irrational? I would be really grateful for a rigorous and detailed proof of this (preferably using only the tools we have at our disposal after studying Rudin up to Prob. 9, Chap. 6). We assume that $\lvert f \rvert^p $ and $\lvert g \rvert^q $ are in $\mathscr{R}$ on $[a, b]$ . If $\int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha = 0$ or $\int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha = 0$ , then we have $\lvert f \rvert^p = 0$ or $\lvert g \rvert^q = 0$ on $[a, b]$ , by virtue of Prob. 2, Chap. 6, in Baby Rudin, provided the functions $\lvert f \rvert$ and $\lvert g \rvert$ are continuous on $[a, b]$ (But what if that is not the case? How to proceed then?); this would imply that $f =0$ or $g = 0$ on $[a, b]$ , and hence $fg = 0$ on $[a, b]$ , and so the Holder's inequality would be trivially satisfied, as both sides of the inequality would be zero. So let us assume that $\int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \not= 0$ and $\int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \not= 0$ . Now let us put $$ \tilde{f} \colon= \frac{ \lvert f \rvert }{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} }, \qquad \tilde{g} \colon= \frac{ \lvert g \rvert }{ \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q} } . \tag{3} $$ Then $\tilde{f}$ and $\tilde{g}$ both are in $\mathscr{R}(\alpha)$ on $[a, b]$ , because of what we have obtained (or assumed) so far together with  Theorem 6.12 (a) in Rudin. Moreover, $$  \begin{align} \int_a^b \tilde{f}^p \ \mathrm{d} \alpha &= \int_a^b \left( \frac{ \lvert f \rvert }{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} } \right)^p \ \mathrm{d} \alpha \qquad \mbox{ [ using (3) above ] } \\ &= \int_a^b \frac{ \lvert f \rvert^p }{ \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha  }  \ \mathrm{d} \alpha \\ &= \frac{ 1 }{ \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha  }   \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha  \\ & \qquad \qquad \mbox{ [ using Theorem 6.12 (a) in Rudin since $ \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha$ is constant ] } \\ &= 1.  \end{align} $$ Similarly, we can show that $$ \int_a^b \tilde{g}^q \ \mathrm{d} \alpha = 1. $$ Thus we have $\tilde{f} \in \mathscr{R}(\alpha)$ , $\tilde{g} \in \mathscr{R}(\alpha)$ , $\tilde{f} \geq 0$ , $\tilde{g} \geq 0$ , and $$ \int_a^b \tilde{f}^p \ \mathrm{d} \alpha = 1 = \int_a^b \tilde{g}^q \ \mathrm{d} \alpha . $$ So by Prob. 10 (b) above we can conclude that $$ \int_a^b \tilde{f} \tilde{g} \ \mathrm{d} \alpha \leq 1. \tag{4}$$ Now using (3) in (4) we obtain $$ \int_a^b \left( \frac{ \lvert f \rvert }{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} } \frac{ \lvert g \rvert }{ \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q} } \right) \ \mathrm{d} \alpha \leq 1, $$ which is the same as $$ \int_a^b \left( \frac{ \lvert f \rvert \ \lvert g \rvert }{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q}  }  \right) \ \mathrm{d} \alpha \leq 1, $$ which  simplifies to $$ \frac{1}{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q}   } \int_a^b \lvert f \rvert \ \lvert g \rvert  \ \mathrm{d} \alpha \leq 1, $$ by virtue of Theorem 6.12 (a) in Rudin because both the quantities $\left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} $ and $\left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q} $ are constant. The last inequality implies that $$  \int_a^b \lvert f \rvert \ \lvert g \rvert  \ \mathrm{d} \alpha \leq \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q}. \tag{5} $$ Finally, as $f \in \mathscr{R}(\alpha)$ and $g \in \mathscr{R}(\alpha)$ on $[a, b]$ , so $f g \in \mathscr{R}(\alpha)$ by Theorem 6.13 (a) in Rudin;  moreover, $$  \begin{align}  \left\lvert \int_a^b f g \ \mathrm{d} \alpha \right\rvert &\leq \int_a^b \lvert f g \rvert  \ \mathrm{d} \alpha  \qquad \mbox{ [ using Theorem 6.13 (b) in Rudin ] }  \\ &=  \int_a^b \lvert f \rvert \ \lvert g \rvert  \ \mathrm{d} \alpha \\ &\leq \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q}. \qquad \mbox{ [ using (5) above ] } \end{align} $$ Is what I've done so far correct? If so, then is my presentation rigorous and lucid enough? If the above proofs are all correct, then the following questions remain. Given a real number $r$ , especially an irrational one, how to rigorously (and only using the tools Rudin has provided us up to this point in the book) prove that the map $y \mapsto y^r$ is continuous on $[0, +\infty)$ ? If both of $\lvert f \rvert$ and $\lvert g \rvert$ are discontinuous at some point(s) of $[a, b]$ , then how to establish the Holder's inequality in case one of $ \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha$ and $ \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha$ is zero?","Here is Prob. 10, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let and be positive real numbers such that Prove the following statements. (a) If and , then Equality holds if and only if . (b) If , , , , and then (c) If and are complex functions in , then This is Holder's inequality. When it is usually called the Schwarz inequality. (Note that Theorem 1.35 is a very special case of this. ) (d) Show that Holder's inequality is also true for the ""improper"" integrals described in Exercises 7 and 8. Here is Theorem 1.35 in Rudin, 3rd edition: If and are complex numbers, then Here are the links to my Math SE posts on Probs. 7 and 8, Chap. 6, in Baby Rudin, 3rd edition: Prob. 7 (a), Chap. 6, in Baby Rudin: If $f$ is integrable on $[c, 1]$ for every $c>0$, then $\int_0^1 f(x) \ \mathrm{d}x = $ . . . Prob. 7 (b), Chap. 6, in Baby Rudin: Example of a function such that $\lim_{c \to 0+} \int_c^1 f(x) \ \mathrm{d}x$ exists but . . . Prob. 8, Chap. 6, in Baby Rudin: The Integral Test for Convergence of Series My Attempt: As so which implies that and hence that is, So in what follows, we will be using (0) and (1). Prob. 10 (a) If or , then So let us suppose that and both are positive real numbers. For any fixed , let us define a function by the formula Then So vanishes only at [Refer to (0) above.]  In fact, Thus is strictly decreasing on , and, is strictly increasing on . This implies that has a relative minimum value at , and this relative minimum, being the unique extreme value of within the entire domain of this function, is in fact also the absolute minimum value ; moreover, from (1) above we also have because Hence that is, and so for all positive real numbers and . Is this proof correct? Prob. 10 (b) In this proof we will be using Theorem 6.12 (a) and (b) in Baby Rudin. Here are the links to my Math SE posts on these results. Theorem 6.12 (a) in Baby Rudin: $\int_a^b \left( f_1 + f_2 \right) d \alpha=\int_a^b f_1 d \alpha + \int_a^b f_2 d \alpha$ https://math.stackexchange.com/questions/2327134/theorem-6-12-a-in-baby-rudin-if-f-in-mathscrr-alpha-on-a-b-then-c Theorem 6.12 (b) in Baby Rudin: If $f_1 \leq f_2$ on $[a, b]$, then $\int_a^b f_1 d\alpha \leq \int_a^b f_2 d\alpha$ And, we will also be using Theorem 6.13 (a) in Baby Rudin, which is as follows: If and on , then . As and on , so we also have on . As and on , so on , by Theorem 6.13 (a) in Baby Rudin. Moreover, Prob. 10 (c) In what follows, we will be using Theorems 6.11, 6.12 (a) , and 6.13 in Rudin. Here are the links to some Math SE posts on these theorems. Theorems 6.11 and 6.13 from PMA Rudin Theorem 6.11 of Rudin's Principles of Mathematical Analysis And, here is the link to my Math SE post on Prob. 2, Chap. 6, in Baby Rudin: Prob. 2, Chap. 6, in Baby Rudin: If $f\geq 0$ and continuous on $[a,b]$ with $\int_a^bf(x)\ \mathrm{d}x=0$, then $f=0$ As and are complex functions in on , so and are real functions in on , by virtue of Theorem 6.13 (b) in Baby Rudin. As and on , so and are bounded functions on . Let us put Then . Let be a real number. If the mapping is continuous on , then we can conclude from Theorem 6.11 in Rudin that and are also in on . Thus in particular, and are in on . But how to show that the mapping is continuous on , especially when is irrational? I would be really grateful for a rigorous and detailed proof of this (preferably using only the tools we have at our disposal after studying Rudin up to Prob. 9, Chap. 6). We assume that and are in on . If or , then we have or on , by virtue of Prob. 2, Chap. 6, in Baby Rudin, provided the functions and are continuous on (But what if that is not the case? How to proceed then?); this would imply that or on , and hence on , and so the Holder's inequality would be trivially satisfied, as both sides of the inequality would be zero. So let us assume that and . Now let us put Then and both are in on , because of what we have obtained (or assumed) so far together with  Theorem 6.12 (a) in Rudin. Moreover, Similarly, we can show that Thus we have , , , , and So by Prob. 10 (b) above we can conclude that Now using (3) in (4) we obtain which is the same as which  simplifies to by virtue of Theorem 6.12 (a) in Rudin because both the quantities and are constant. The last inequality implies that Finally, as and on , so by Theorem 6.13 (a) in Rudin;  moreover, Is what I've done so far correct? If so, then is my presentation rigorous and lucid enough? If the above proofs are all correct, then the following questions remain. Given a real number , especially an irrational one, how to rigorously (and only using the tools Rudin has provided us up to this point in the book) prove that the map is continuous on ? If both of and are discontinuous at some point(s) of , then how to establish the Holder's inequality in case one of and is zero?","p q  \frac{1}{p} +  \frac{1}{q} =1.  u \geq 0 v \geq 0  u v \leq \frac{u^p}{p} + \frac{v^q}{q}.  u^p = v^q f \in \mathscr{R} g \in \mathscr{R} f \geq 0 g \geq 0  \int_a^b f^p \ \mathrm{d} \alpha = 1 = \int_a^b g^q \ \mathrm{d} \alpha,   \int_a^b f g \ \mathrm{d} \alpha \leq 1.  f g \mathscr{R} (\alpha)   \left\lvert \int_a^b f g \ \mathrm{d} \alpha \right\rvert \leq \left\{ \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right\}^{1/p} \left\{ \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right\}^{1/q}.  p = q = 2 a_1, \ldots, a_n b_1, \ldots, b_n  \left\lvert \sum_{j=1}^n a_j \overline{b_j} \right\rvert^2 \leq \sum_{j=1}^n \left\lvert a_j \right\rvert^2 \sum_{j=1}^n \left\lvert b_j \right\rvert^2.   \frac{1}{p}  + \frac{1}{q} = 1,  q+p = pq,    pq- p - q = 0,   pq - p - q + 1 = 1,   (p-1) (q-1) = 1. \tag{0}   (q-1)p = (q-1)(p-1 + 1) = (q-1)(p-1) + q - 1 = 1 + q-1 = q. \tag{1}  u = 0 v=0  0 = uv \leq \frac{u^p}{p} + \frac{v^q}{q}.  u v v > 0 f_v \colon (0, +\infty) \longrightarrow \mathbb{R}  f_v(u) \colon= \frac{u^p}{p} + \frac{v^q}{q} - uv \ \mbox{ for all } u \in (0, +\infty).   f_v^\prime(u) = u^{p-1} - v \ \mbox{ for all } u \in (0, +\infty).  f_v^\prime  u = v^{\frac{1}{p-1}} = v^{q-1}.  
f_v^\prime (u) \ \begin{cases} < 0 \ & \mbox{ if } 0 < u < v^{q-1}, \\ = 0 \ & \mbox{ if } u = v^{q-1}, \\ > 0 \ & \mbox{ if } u > v^{q-1}. \end{cases}  f_v \left( 0, v^{q-1} \right] f_v \left[ v^{q-1}, +\infty \right) f_v u = v^{q-1} f_v 
\begin{align}
 f_v \left( v^{q-1} \right) &= \frac{ \left( v^{q-1} \right)^p }{p} + \frac{v^q}{q} - v^{q-1} v \\ 
&= \frac{ v^{p(q-1)} }{p} + \frac{v^q}{q} - v^q \\ 
&= \frac{v^q}{p} + \frac{v^q}{q} - v^q \qquad [ \mbox{ using (1) above } ] \\ 
&= v^q \left( \frac{1}{p} + \frac{1}{q} - 1 \right) \\
&= 0 
\end{align}
  \frac{1}{p} + \frac{1}{q} =1.   f_v(u) \geq 0 \ \mbox{ for all } u \in (0, +\infty);   \frac{u^p}{p} + \frac{v^q}{q} - uv \geq 0 \mbox{ for all } u \in (0, +\infty),   uv \leq \frac{u^p}{p} + \frac{v^q}{q}  u v f \in \mathscr{R}(\alpha) g \in \mathscr{R}(\alpha) [a, b] fg \in \mathscr{R}(\alpha) f \geq 0 g \geq 0 [a, b]  f g \leq \frac{f^p}{p} + \frac{g^q}{q} \tag{2}  [a, b] f \in \mathscr{R}(\alpha) g \in \mathscr{R}(\alpha) [a, b] fg \in \mathscr{R}(\alpha) [a, b] 
\begin{align}
\int_a^b f g \ \mathrm{d} \alpha &\leq \int_a^b \left( \frac{f^p}{p} + \frac{g^q}{q} \right) \ \mathrm{d} \alpha \qquad \mbox{ [ using (2) and Theorem 6.12 (b) in Rudin ] } \\
&= \frac{1}{p} \int_a^b f^p \ \mathrm{d} \alpha  + \frac{1}{q} \int_a^b g^q  \ \mathrm{d} \alpha \qquad \mbox{ [ using Theorem 6.12 (a) in Rudin ] } \\
&= \frac{1}{p} \cdot 1 + \frac{1}{q} \cdot 1 \qquad \mbox{ [ using our hypothesis ] } \\
&= 1, \qquad \mbox{ [ using the condition on p and q in our hypothesis ] }
\end{align}
 f g \mathscr{R}(\alpha) [a, b] \lvert f \rvert \lvert g \rvert \mathscr{R} [a, b] \lvert f \rvert \in \mathscr{R}(\alpha) \lvert g \rvert \in \mathscr{R}(\alpha) [a, b] \lvert f \rvert \lvert g \rvert [a, b]  M \colon= 1 + \max \left\{ \ \sup \{ \ \lvert f(x) \rvert \ \colon \ a \leq x \leq b \ \},  
\sup \{ \ \lvert g(x) \rvert \ \colon \ a \leq x \leq b \ \} \ \right\}.  M > 0 r y \mapsto y^r [0, +\infty) \lvert f \rvert^r  \lvert g \rvert^r  \mathscr{R} [a, b] \lvert f \rvert^p  \lvert g \rvert^q  \mathscr{R} [a, b] y \mapsto y^r [0, +\infty) r \lvert f \rvert^p  \lvert g \rvert^q  \mathscr{R} [a, b] \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha = 0 \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha = 0 \lvert f \rvert^p = 0 \lvert g \rvert^q = 0 [a, b] \lvert f \rvert \lvert g \rvert [a, b] f =0 g = 0 [a, b] fg = 0 [a, b] \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \not= 0 \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \not= 0  \tilde{f} \colon= \frac{ \lvert f \rvert }{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} }, \qquad \tilde{g} \colon= \frac{ \lvert g \rvert }{ \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q} } . \tag{3}  \tilde{f} \tilde{g} \mathscr{R}(\alpha) [a, b]  
\begin{align}
\int_a^b \tilde{f}^p \ \mathrm{d} \alpha &= \int_a^b \left( \frac{ \lvert f \rvert }{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} } \right)^p \ \mathrm{d} \alpha \qquad \mbox{ [ using (3) above ] } \\
&= \int_a^b \frac{ \lvert f \rvert^p }{ \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha  }  \ \mathrm{d} \alpha \\
&= \frac{ 1 }{ \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha  }   \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha  \\
& \qquad \qquad \mbox{ [ using Theorem 6.12 (a) in Rudin since  \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha is constant ] } \\
&= 1. 
\end{align}
  \int_a^b \tilde{g}^q \ \mathrm{d} \alpha = 1.  \tilde{f} \in \mathscr{R}(\alpha) \tilde{g} \in \mathscr{R}(\alpha) \tilde{f} \geq 0 \tilde{g} \geq 0  \int_a^b \tilde{f}^p \ \mathrm{d} \alpha = 1 = \int_a^b \tilde{g}^q \ \mathrm{d} \alpha .   \int_a^b \tilde{f} \tilde{g} \ \mathrm{d} \alpha \leq 1. \tag{4}  \int_a^b \left( \frac{ \lvert f \rvert }{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} } \frac{ \lvert g \rvert }{ \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q} } \right) \ \mathrm{d} \alpha \leq 1,   \int_a^b \left( \frac{ \lvert f \rvert \ \lvert g \rvert }{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q}  }  \right) \ \mathrm{d} \alpha \leq 1,   \frac{1}{ \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q}   } \int_a^b \lvert f \rvert \ \lvert g \rvert  \ \mathrm{d} \alpha \leq 1,  \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p}  \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q}    \int_a^b \lvert f \rvert \ \lvert g \rvert  \ \mathrm{d} \alpha \leq \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q}. \tag{5}  f \in \mathscr{R}(\alpha) g \in \mathscr{R}(\alpha) [a, b] f g \in \mathscr{R}(\alpha)  
\begin{align}
 \left\lvert \int_a^b f g \ \mathrm{d} \alpha \right\rvert &\leq \int_a^b \lvert f g \rvert  \ \mathrm{d} \alpha  \qquad \mbox{ [ using Theorem 6.13 (b) in Rudin ] }  \\
&=  \int_a^b \lvert f \rvert \ \lvert g \rvert  \ \mathrm{d} \alpha \\
&\leq \left( \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha \right)^{1/p} \left( \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha \right)^{1/q}. \qquad \mbox{ [ using (5) above ] }
\end{align}
 r y \mapsto y^r [0, +\infty) \lvert f \rvert \lvert g \rvert [a, b]  \int_a^b \lvert f \rvert^p \ \mathrm{d} \alpha  \int_a^b \lvert g \rvert^q \ \mathrm{d} \alpha","['real-analysis', 'integration', 'analysis', 'definite-integrals', 'integral-inequality']"
21,Combinatorial interpretation of this integral?,Combinatorial interpretation of this integral?,,"The result is that $$\int_{0}^1 x^m (1-x)^n dx = \frac{m!n!}{(m+n+1)!}.$$ I think we could just do this by repeated integration by parts... but I could have also done this: $$\int_{0}^1 x^m(1-x)^n dx = \int_{0}^1 x^m \left[\sum_{k=0}^{n}\begin{pmatrix}n \\k \end{pmatrix}(-1)^kx^k\right]dx\\ = \int_{0}^1 \sum_{k=0}^n \begin{pmatrix}n\\k\end{pmatrix}(-1)^k x^{k+m} dx\\ = \sum_{k=0}^n \int_0^1 \begin{pmatrix}n\\k\end{pmatrix}(-1)^kx^{k+m}dx \qquad \text{assuming I can interchange the integral and sum... why??} \\ = \sum_{k=0}^n \begin{pmatrix}n\\k \end{pmatrix} (-1)^k \frac{1}{k+m+1}.$$ Putting it into a calculator, it gives the original result. Is there some interpretation for this, and how would I have shown the result? It looks like some sort of taylor series...","The result is that $$\int_{0}^1 x^m (1-x)^n dx = \frac{m!n!}{(m+n+1)!}.$$ I think we could just do this by repeated integration by parts... but I could have also done this: $$\int_{0}^1 x^m(1-x)^n dx = \int_{0}^1 x^m \left[\sum_{k=0}^{n}\begin{pmatrix}n \\k \end{pmatrix}(-1)^kx^k\right]dx\\ = \int_{0}^1 \sum_{k=0}^n \begin{pmatrix}n\\k\end{pmatrix}(-1)^k x^{k+m} dx\\ = \sum_{k=0}^n \int_0^1 \begin{pmatrix}n\\k\end{pmatrix}(-1)^kx^{k+m}dx \qquad \text{assuming I can interchange the integral and sum... why??} \\ = \sum_{k=0}^n \begin{pmatrix}n\\k \end{pmatrix} (-1)^k \frac{1}{k+m+1}.$$ Putting it into a calculator, it gives the original result. Is there some interpretation for this, and how would I have shown the result? It looks like some sort of taylor series...",,"['integration', 'combinatorics']"
22,"Integration of unknown function, where $f \in C(\mathbb{R})$","Integration of unknown function, where",f \in C(\mathbb{R}),"Given that $f\in C(\mathbb{R})$, $f(5) = 2$ and $f(2)  = -2$, evaluate the following integral  $$ \int_{2}^5 \frac{f'(x) \sin(f(x))}{1 + f^2(x)}\mathrm{d}x. $$ I would guess that I should use integration by parts, where  $$v'(x)=\frac{f'(x) }{1 + f^2(x)}$$ and  $$u(x) = \sin(f(x))\,,$$ but it gets me nowhere as I have an integral of $\arctan$ times $\cos$ that I don't know how to solve.","Given that $f\in C(\mathbb{R})$, $f(5) = 2$ and $f(2)  = -2$, evaluate the following integral  $$ \int_{2}^5 \frac{f'(x) \sin(f(x))}{1 + f^2(x)}\mathrm{d}x. $$ I would guess that I should use integration by parts, where  $$v'(x)=\frac{f'(x) }{1 + f^2(x)}$$ and  $$u(x) = \sin(f(x))\,,$$ but it gets me nowhere as I have an integral of $\arctan$ times $\cos$ that I don't know how to solve.",,"['calculus', 'integration']"
23,"Prob. 4, Chap. 6, in Baby Rudin: If $f(x)=0$ for all irrational $x$ and $f(x)=1$ for all rational $x$, then $f$ is not integrable on $[a, b]$","Prob. 4, Chap. 6, in Baby Rudin: If  for all irrational  and  for all rational , then  is not integrable on","f(x)=0 x f(x)=1 x f [a, b]","Here is Prob. 4, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f(x) = 0$ for all irrational $x$, $f(x) = 1$ for all rational $x$, prove that $f \not\in \mathscr{R}$ on $[a, b]$ for any $a < b$. My Attempt: By Theorem 1.20 (b) in Baby Rudin, 3rd edition, between any two real numbers, there is at least one (in fact, infinitely many) rational numbers. From this we can also prove that between any two real numbers, there is at least one (in fact infinitely many) irrational numbers. Thus between any two real numbers, there are infinitely many rational numbers and infinitely many irrational numbers. Now let $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$ be an arbitrary  partition of the interval $[a, b]$, where    $$ a = x_0 < x_1 < \cdots < x_n = b. $$ Now for each $i = 1, \ldots, n$, there are infinitely many rational numbers and infinitely many irrational numbers in the sub-interval  $\left[ x_{i-1}, x_i \right]$; so we must have    $$ \sup \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} = 1, $$   and    $$ \inf \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} = 0.  $$   Therefore,    $$  \begin{align} U (P, f) &= \sum_{i=1}^n \sup \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} \cdot \left( x_i - x_{i-1} \right) \\  &= \sum_{i=1}^n 1 \cdot \left( x_i - x_{i-1} \right) \\  &= x_n - x_0 \\ &= b-a,  \end{align} $$   and    $$  \begin{align} L (P, f) &= \sum_{i=1}^n \inf \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} \cdot \left( x_i - x_{i-1} \right) \\  &= \sum_{i=1}^n 0 \cdot \left( x_i - x_{i-1} \right) \\  &= 0. \end{align} $$   So if $\varepsilon$ be any real number such that $0 < \varepsilon < b-a$, then we see that    $$ U(P, f) - L(P, f) = b-a > \varepsilon $$   for any partition $P$ of the interval $[a, b]$. Therefore (by Theorem 6.6 in Baby Rudin, 3rd edition) we can conclude that    $f \not\in \mathscr{R}$ on $[a, b]$. Is my proof good enough logic, rigor, and presentation-wise?","Here is Prob. 4, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f(x) = 0$ for all irrational $x$, $f(x) = 1$ for all rational $x$, prove that $f \not\in \mathscr{R}$ on $[a, b]$ for any $a < b$. My Attempt: By Theorem 1.20 (b) in Baby Rudin, 3rd edition, between any two real numbers, there is at least one (in fact, infinitely many) rational numbers. From this we can also prove that between any two real numbers, there is at least one (in fact infinitely many) irrational numbers. Thus between any two real numbers, there are infinitely many rational numbers and infinitely many irrational numbers. Now let $P = \left\{ \ x_0, x_1, \ldots, x_n \ \right\}$ be an arbitrary  partition of the interval $[a, b]$, where    $$ a = x_0 < x_1 < \cdots < x_n = b. $$ Now for each $i = 1, \ldots, n$, there are infinitely many rational numbers and infinitely many irrational numbers in the sub-interval  $\left[ x_{i-1}, x_i \right]$; so we must have    $$ \sup \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} = 1, $$   and    $$ \inf \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} = 0.  $$   Therefore,    $$  \begin{align} U (P, f) &= \sum_{i=1}^n \sup \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} \cdot \left( x_i - x_{i-1} \right) \\  &= \sum_{i=1}^n 1 \cdot \left( x_i - x_{i-1} \right) \\  &= x_n - x_0 \\ &= b-a,  \end{align} $$   and    $$  \begin{align} L (P, f) &= \sum_{i=1}^n \inf \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\} \cdot \left( x_i - x_{i-1} \right) \\  &= \sum_{i=1}^n 0 \cdot \left( x_i - x_{i-1} \right) \\  &= 0. \end{align} $$   So if $\varepsilon$ be any real number such that $0 < \varepsilon < b-a$, then we see that    $$ U(P, f) - L(P, f) = b-a > \varepsilon $$   for any partition $P$ of the interval $[a, b]$. Therefore (by Theorem 6.6 in Baby Rudin, 3rd edition) we can conclude that    $f \not\in \mathscr{R}$ on $[a, b]$. Is my proof good enough logic, rigor, and presentation-wise?",,"['real-analysis', 'integration', 'analysis', 'proof-verification', 'definite-integrals']"
24,"Solving the integral $\int_{-\infty}^{\infty}e^x e^{-e^x} e^{-x^2}\,dx$",Solving the integral,"\int_{-\infty}^{\infty}e^x e^{-e^x} e^{-x^2}\,dx","I am struggling to solve the integral $$\int_{-\infty}^{\infty}e^x e^{-e^x} e^{-x^2}\,dx.$$ Is there a way to find an exact solution in terms of standard mathematical functions (including non-elementary functions such as the error function and the gamma function )? What I have tried: Through substitution ($u = e^x$) I have found that the value is equal to $$\int_{0}^{\infty}e^{-u} e^{-(ln\ u)^2}\,du$$ I have tried further substitutions but none have helped me arrive at a value. I know from WolframAlpha that the value is around $0.529$.","I am struggling to solve the integral $$\int_{-\infty}^{\infty}e^x e^{-e^x} e^{-x^2}\,dx.$$ Is there a way to find an exact solution in terms of standard mathematical functions (including non-elementary functions such as the error function and the gamma function )? What I have tried: Through substitution ($u = e^x$) I have found that the value is equal to $$\int_{0}^{\infty}e^{-u} e^{-(ln\ u)^2}\,du$$ I have tried further substitutions but none have helped me arrive at a value. I know from WolframAlpha that the value is around $0.529$.",,"['calculus', 'integration']"
25,Approximation of a Riemann sum (not really) by a Laplacian integral,Approximation of a Riemann sum (not really) by a Laplacian integral,,"I have a sum of the form: $$S_n = \frac{1}{n} \sum_{i=0}^n \mathrm{e}^{n f(i/n)} g(i/n)$$ where $f(x)$ and $g(x)$ are smooth functions defined for $0\le x \le 1$. I am interested in the Asymptotic behaviour of $S_n$ as $n\rightarrow +\infty$. What I have tried is to replace the sum by an integral: $$S_n \approx\int_0^1 \mathrm{e}^{n f(x)} g(x) \mathrm{d}x$$ and then I was thinking of doing a saddle-point approximation ( Laplace's method ), but I am not sure of the validity of replacing the sum by the integral. It looks like a Riemann sum, except for the $n$ multiplying $f$ in the exponent. Any suggestions appreciated. If it makes your life easier assume that $f(x)$ has a single maximum inside the interval $x\in(0,1)$. I'd be happy if someone could at least point out some reference to the literature where sums like this have popped out before. I have the feeling that this has been treated before. Edit, TLTR. The summarized version of the question is: Prove or disprove that $S_n / I_n \rightarrow c$ for some constant $c$ as $n\rightarrow \infty$, where $$S_n = \frac{1}{n} \sum_{i=0}^n \mathrm{e}^{n f(i/n)} g(i/n), \quad I_n = \int_0^1 \mathrm{e}^{n f(x)} g(x) \mathrm{d}x$$ Moreover, is the difference between $I_n$ and $S_n$ exponentially decreasing in $n$? That is, $|I_nc - S_n| / S_n = \mathcal{O}(n^p e^{-qn})$ for some numbers $p,q$?","I have a sum of the form: $$S_n = \frac{1}{n} \sum_{i=0}^n \mathrm{e}^{n f(i/n)} g(i/n)$$ where $f(x)$ and $g(x)$ are smooth functions defined for $0\le x \le 1$. I am interested in the Asymptotic behaviour of $S_n$ as $n\rightarrow +\infty$. What I have tried is to replace the sum by an integral: $$S_n \approx\int_0^1 \mathrm{e}^{n f(x)} g(x) \mathrm{d}x$$ and then I was thinking of doing a saddle-point approximation ( Laplace's method ), but I am not sure of the validity of replacing the sum by the integral. It looks like a Riemann sum, except for the $n$ multiplying $f$ in the exponent. Any suggestions appreciated. If it makes your life easier assume that $f(x)$ has a single maximum inside the interval $x\in(0,1)$. I'd be happy if someone could at least point out some reference to the literature where sums like this have popped out before. I have the feeling that this has been treated before. Edit, TLTR. The summarized version of the question is: Prove or disprove that $S_n / I_n \rightarrow c$ for some constant $c$ as $n\rightarrow \infty$, where $$S_n = \frac{1}{n} \sum_{i=0}^n \mathrm{e}^{n f(i/n)} g(i/n), \quad I_n = \int_0^1 \mathrm{e}^{n f(x)} g(x) \mathrm{d}x$$ Moreover, is the difference between $I_n$ and $S_n$ exponentially decreasing in $n$? That is, $|I_nc - S_n| / S_n = \mathcal{O}(n^p e^{-qn})$ for some numbers $p,q$?",,"['integration', 'sequences-and-series', 'reference-request', 'riemann-sum', 'laplace-method']"
26,1985 Putnam B4 Integration in Rectangular Coordinates,1985 Putnam B4 Integration in Rectangular Coordinates,,"Let C be the circle radius 1, center the origin. A point P is chosen at random on the circumference of C, and another point Q is chosen at random in the interior of C. What is the probability that the rectangle with diagonal PQ, and sides parallel to the x-axis and y-axis, lies entirely inside (or on) C? The answer is $4/\pi^2$ and is gotten by integrating with polar coordinates like so: Given P, let R be the rectangle inscribed in C with sides parallel to the axes. Then the rectangle with diagonal PQ lies entirely inside C iff Q lies inside R. Let O be the center of C. Let the diameter of C parallel to the y-axis make an angle θ with OP. Then the area of R is 4 sin θ cos θ. So the required probability is $2/\pi\int_0^{\pi/2} (4\sin\theta\cos\theta)/\pi\,d\theta = 4/\pi^2\int_0^{\pi/2}\sin 2\theta\,d\theta = 4/\pi^2$. When I was trying to solve this I got the first part but tried to integrate in rectangular coordinates: Let $P=(x,y)$ then the area of the rectangle is $4x\sqrt{1-x^2}$ and integrating  $4\int_0^14x\sqrt{1-x^2}/\pi$ doesn't give $4/\pi^2$. I then have 3 questions: 1) Why does the integral in the solution have a $2/\pi$ coefficient? 2) Why does the integral in the solution go only to $\pi/2$, I thought its limits would be $2\pi$ or $4$ times the given boudaries. 3) How would you properly do the integral in rectangular coordinates?","Let C be the circle radius 1, center the origin. A point P is chosen at random on the circumference of C, and another point Q is chosen at random in the interior of C. What is the probability that the rectangle with diagonal PQ, and sides parallel to the x-axis and y-axis, lies entirely inside (or on) C? The answer is $4/\pi^2$ and is gotten by integrating with polar coordinates like so: Given P, let R be the rectangle inscribed in C with sides parallel to the axes. Then the rectangle with diagonal PQ lies entirely inside C iff Q lies inside R. Let O be the center of C. Let the diameter of C parallel to the y-axis make an angle θ with OP. Then the area of R is 4 sin θ cos θ. So the required probability is $2/\pi\int_0^{\pi/2} (4\sin\theta\cos\theta)/\pi\,d\theta = 4/\pi^2\int_0^{\pi/2}\sin 2\theta\,d\theta = 4/\pi^2$. When I was trying to solve this I got the first part but tried to integrate in rectangular coordinates: Let $P=(x,y)$ then the area of the rectangle is $4x\sqrt{1-x^2}$ and integrating  $4\int_0^14x\sqrt{1-x^2}/\pi$ doesn't give $4/\pi^2$. I then have 3 questions: 1) Why does the integral in the solution have a $2/\pi$ coefficient? 2) Why does the integral in the solution go only to $\pi/2$, I thought its limits would be $2\pi$ or $4$ times the given boudaries. 3) How would you properly do the integral in rectangular coordinates?",,"['integration', 'definite-integrals', 'contest-math']"
27,How do I integrate $\int\cos^2(x)\sin^\frac{2-\alpha}{\alpha-1}(x)\ dx$,How do I integrate,\int\cos^2(x)\sin^\frac{2-\alpha}{\alpha-1}(x)\ dx,"I am having trouble solving this integral. $$\int\cos^2(x)\sin^\frac{2-\alpha}{\alpha-1}(x)\ dx$$ This is what I have done so far using integration by parts: Let $u=\sin^\frac{2-\alpha}{\alpha-1}(x)$ and $v'=\cos^2(x)$. Then we have: $u'=\frac{2-\alpha}{\alpha-1}\sin^\frac{3-2\alpha}{\alpha-1}(x)$ and $v=\frac{1}{2}x\ +\ \frac{1}{4}\sin(2x)$ from using the double angle formula: $\cos^2(x)=\frac{1\ +\ \cos(2x)}{2}$. So our original integral becomes: $$\sin^\frac{2-\alpha}{\alpha-1}(x)\ \times\ (\frac{1}{2}x\ +\ \frac{1}{4}\sin(2x))\ -\ \int\frac{2-\alpha}{\alpha-1}\sin^\frac{3-2\alpha}{\alpha-1}(x)\ \times\ (\frac{1}{2}x\ +\ \frac{1}{4}\sin(2x))\ dx$$ The left part of this is fine but the annoying part is the integral bit. Taking the constants $\frac{2-\alpha}{\alpha-1}$ and $\frac{1}{2}$ out of the integral leaves: $$\frac{2-\alpha}{2\alpha-2}\int\sin^\frac{3-2\alpha}{\alpha-1}(x)\ \times\ (x\ +\ \frac{1}{2}\sin(2x))\ dx$$ $$=\frac{2-\alpha}{2\alpha-2}\int\sin^\frac{3-2\alpha}{\alpha-1}(x)\ \times\ (x\ +\ \frac{1}{2}2\sin(x)\cos(x))\ dx$$ $$=\frac{2-\alpha}{2\alpha-2}\int\sin^\frac{3-2\alpha}{\alpha-1}(x)\ \times\ (x\ +\ \sin(x)\cos(x))\ dx$$ $$=\frac{2-\alpha}{2\alpha-2}\left[\int\sin^\frac{3-2\alpha}{\alpha-1}(x)\times x\ dx\ +\int\sin^\frac{3-2\alpha}{\alpha-1}(x)\sin(x)\cos(x)\ dx\right]$$ $$=\frac{2-\alpha}{2\alpha-2}\left[\int\sin^\frac{3-2\alpha}{\alpha-1}(x)\times x\ dx\ +\int\sin^\frac{2-\alpha}{\alpha-1}(x)\cos(x)\ dx\right]$$ The right integral is easy via integration by parts but the problem here is that I don't know how to integrate the left integral. Please help!!! By the way, I typed the original integral in Wolfram Alpha and the answer it gave included some crazy function called the hypergeometric function. I hope there is a way to solve the integral without getting into such complex functions because I really want to graph it and see how it looks :)","I am having trouble solving this integral. $$\int\cos^2(x)\sin^\frac{2-\alpha}{\alpha-1}(x)\ dx$$ This is what I have done so far using integration by parts: Let $u=\sin^\frac{2-\alpha}{\alpha-1}(x)$ and $v'=\cos^2(x)$. Then we have: $u'=\frac{2-\alpha}{\alpha-1}\sin^\frac{3-2\alpha}{\alpha-1}(x)$ and $v=\frac{1}{2}x\ +\ \frac{1}{4}\sin(2x)$ from using the double angle formula: $\cos^2(x)=\frac{1\ +\ \cos(2x)}{2}$. So our original integral becomes: $$\sin^\frac{2-\alpha}{\alpha-1}(x)\ \times\ (\frac{1}{2}x\ +\ \frac{1}{4}\sin(2x))\ -\ \int\frac{2-\alpha}{\alpha-1}\sin^\frac{3-2\alpha}{\alpha-1}(x)\ \times\ (\frac{1}{2}x\ +\ \frac{1}{4}\sin(2x))\ dx$$ The left part of this is fine but the annoying part is the integral bit. Taking the constants $\frac{2-\alpha}{\alpha-1}$ and $\frac{1}{2}$ out of the integral leaves: $$\frac{2-\alpha}{2\alpha-2}\int\sin^\frac{3-2\alpha}{\alpha-1}(x)\ \times\ (x\ +\ \frac{1}{2}\sin(2x))\ dx$$ $$=\frac{2-\alpha}{2\alpha-2}\int\sin^\frac{3-2\alpha}{\alpha-1}(x)\ \times\ (x\ +\ \frac{1}{2}2\sin(x)\cos(x))\ dx$$ $$=\frac{2-\alpha}{2\alpha-2}\int\sin^\frac{3-2\alpha}{\alpha-1}(x)\ \times\ (x\ +\ \sin(x)\cos(x))\ dx$$ $$=\frac{2-\alpha}{2\alpha-2}\left[\int\sin^\frac{3-2\alpha}{\alpha-1}(x)\times x\ dx\ +\int\sin^\frac{3-2\alpha}{\alpha-1}(x)\sin(x)\cos(x)\ dx\right]$$ $$=\frac{2-\alpha}{2\alpha-2}\left[\int\sin^\frac{3-2\alpha}{\alpha-1}(x)\times x\ dx\ +\int\sin^\frac{2-\alpha}{\alpha-1}(x)\cos(x)\ dx\right]$$ The right integral is easy via integration by parts but the problem here is that I don't know how to integrate the left integral. Please help!!! By the way, I typed the original integral in Wolfram Alpha and the answer it gave included some crazy function called the hypergeometric function. I hope there is a way to solve the integral without getting into such complex functions because I really want to graph it and see how it looks :)",,"['integration', 'indefinite-integrals', 'hypergeometric-function', 'integration-by-parts']"
28,Show $\lim\limits_{h\to 0^+}h \sum_{n=1}^{\infty}f(nh)=\int_0^{\infty}f(x)dx$,Show,\lim\limits_{h\to 0^+}h \sum_{n=1}^{\infty}f(nh)=\int_0^{\infty}f(x)dx,"Let $f$ be a monotonous function over $[0, \infty)$ for which the improper integral $\int_0^{\infty}f(x)dx$ converges. I want to show $$\lim\limits_{h\to 0^+}h \sum_{n=1}^{\infty}f(nh)=\int_0^{\infty}f(x)dx$$ First, we notice that for the integral to converge $f$ has to be decreasing. Now we have $$\int_{(n-1)h}^{nh}f(x)dx\geq f(nh)h\geq\int_{nh}^{(n+1)h}f(x)dx$$ Summing from $n=1$, we get $$\sum_{n=1}^{\infty}\int_{(n-1)h}^{nh}f(x)dx\geq \sum_{n=1}^{\infty}f(nh)h\geq \sum_{n=1}^{\infty}\int_{nh}^{(n+1)h}f(x)dx$$ $$\int_{0}^{\infty}f(x)dx\geq h\sum_{n=1}^{\infty}f(nh)\geq \int_{h}^{\infty}f(x)dx$$ We apply the limit $$\lim\limits_{h\to 0^+}\int_{0}^{\infty}f(x)dx\geq \lim\limits_{h\to 0^+}h\sum_{n=1}^{\infty}f(nh)\geq \lim\limits_{h\to 0^+}\int_{h}^{\infty}f(x)dx$$ $$\int_{0}^{\infty}f(x)dx\geq \lim\limits_{h\to 0^+} h\sum_{n=1}^{\infty}f(nh)\geq \int_{0}^{\infty}f(x)dx$$ Finally $$\lim\limits_{h\to 0^+} h\sum_{n=1}^{\infty}f(nh)= \int_{0}^{\infty}f(x)dx$$ Is this line of reasoning correct?","Let $f$ be a monotonous function over $[0, \infty)$ for which the improper integral $\int_0^{\infty}f(x)dx$ converges. I want to show $$\lim\limits_{h\to 0^+}h \sum_{n=1}^{\infty}f(nh)=\int_0^{\infty}f(x)dx$$ First, we notice that for the integral to converge $f$ has to be decreasing. Now we have $$\int_{(n-1)h}^{nh}f(x)dx\geq f(nh)h\geq\int_{nh}^{(n+1)h}f(x)dx$$ Summing from $n=1$, we get $$\sum_{n=1}^{\infty}\int_{(n-1)h}^{nh}f(x)dx\geq \sum_{n=1}^{\infty}f(nh)h\geq \sum_{n=1}^{\infty}\int_{nh}^{(n+1)h}f(x)dx$$ $$\int_{0}^{\infty}f(x)dx\geq h\sum_{n=1}^{\infty}f(nh)\geq \int_{h}^{\infty}f(x)dx$$ We apply the limit $$\lim\limits_{h\to 0^+}\int_{0}^{\infty}f(x)dx\geq \lim\limits_{h\to 0^+}h\sum_{n=1}^{\infty}f(nh)\geq \lim\limits_{h\to 0^+}\int_{h}^{\infty}f(x)dx$$ $$\int_{0}^{\infty}f(x)dx\geq \lim\limits_{h\to 0^+} h\sum_{n=1}^{\infty}f(nh)\geq \int_{0}^{\infty}f(x)dx$$ Finally $$\lim\limits_{h\to 0^+} h\sum_{n=1}^{\infty}f(nh)= \int_{0}^{\infty}f(x)dx$$ Is this line of reasoning correct?",,"['real-analysis', 'integration', 'proof-verification', 'improper-integrals']"
29,Evaluate the integral $ \int _0^{+\infty} \frac{x^m}{(a+bx^n)^p}$,Evaluate the integral, \int _0^{+\infty} \frac{x^m}{(a+bx^n)^p},"I'm trying to evaluate the following integral: $$ \int _0^{+\infty} \frac{x^m}{(a+bx^n)^p}$$ $a>0, b>0, n>0$ Could you please say if my reasoning is correct? I thought that it was a good idea to use Chebyshev theorem on the integration of binomial differentials to evaluate the integral. Then I change the integral to the form: $$ \int _0^{+\infty} x^m(a+bx^n)^{-p}$$ However, the task gives no information regarding $p$ and whether $m, n \in \Bbb{Z} $. So I need to look at 4 cases: $p \in \Bbb{Z}$ Then use the substitution $x=t^r$ ,  $dx=rt^{r-1}$ where $r$ is common denominator of rational numbers $m$ and $n$. $\frac{m+1}{n} \in \Bbb{Z}$ Then use the substitution $t=\sqrt[r]{a+bx^n}$ where $r$ is denominator of $p$. $\frac{m+1}{n}+p \in \Bbb{Z}$ Then use the substitution $t=\sqrt[r]{\frac{a+bx^n}{x^n}}$ where $r$ is denominator of $p$. If all 3 previous cases are not applicable, then it's impossible to evaluate the integral. Is my reasoning correct? I don't really know how to proceed further with these substitutions to evaluate the integral since there are no specific numbers here. Any help is very much appreciated!","I'm trying to evaluate the following integral: $$ \int _0^{+\infty} \frac{x^m}{(a+bx^n)^p}$$ $a>0, b>0, n>0$ Could you please say if my reasoning is correct? I thought that it was a good idea to use Chebyshev theorem on the integration of binomial differentials to evaluate the integral. Then I change the integral to the form: $$ \int _0^{+\infty} x^m(a+bx^n)^{-p}$$ However, the task gives no information regarding $p$ and whether $m, n \in \Bbb{Z} $. So I need to look at 4 cases: $p \in \Bbb{Z}$ Then use the substitution $x=t^r$ ,  $dx=rt^{r-1}$ where $r$ is common denominator of rational numbers $m$ and $n$. $\frac{m+1}{n} \in \Bbb{Z}$ Then use the substitution $t=\sqrt[r]{a+bx^n}$ where $r$ is denominator of $p$. $\frac{m+1}{n}+p \in \Bbb{Z}$ Then use the substitution $t=\sqrt[r]{\frac{a+bx^n}{x^n}}$ where $r$ is denominator of $p$. If all 3 previous cases are not applicable, then it's impossible to evaluate the integral. Is my reasoning correct? I don't really know how to proceed further with these substitutions to evaluate the integral since there are no specific numbers here. Any help is very much appreciated!",,"['calculus', 'integration']"
30,An integral involving powers of logarithms and a dilogarithm.,An integral involving powers of logarithms and a dilogarithm.,,"Consider a following integral: \begin{equation} {\mathcal I}^{(q,p)} := \int\limits_0^1 \frac{Li_2(-\xi)}{\xi}\cdot [\log(1-\xi)]^q \cdot [\log(1+\xi)]^p d\xi \end{equation} where $q$ and $p$ are non-negative integers. By expanding parts of the integrand in a series and then integrating term by term we have arrived at the following result: \begin{eqnarray} {\mathcal I}^{(3,0)} = \frac{\pi^6}{180} + \log(2) \cdot \left( \pi^2 \zeta(3) - 12 \zeta(5)\right) + r \end{eqnarray} where the remainder $r$ reads \begin{eqnarray} r&=& 3! \zeta\left( \begin{array}{rrr} 4, & 1, & 1\\ &\frac{1}{2}& \end{array}\right) \\ &=& \frac{3}{2} \sum\limits_{k=1}^\infty \left( \frac{[H_{k-1}]^2 - H_{k-1}^{(2)}}{k^3}\right)\cdot \left( H_{\frac{k}{2}} - H_{\frac{k-1}{2}}\right) \\ &=& -6 \int_0^{-1} \frac{S_{2,3}(-\xi)}{1-\xi} d\xi \end{eqnarray} Here $S_{2,3}()$ is the Nielsen generalized poly-logarithm. Now the question is the following, firstly is it possible to reduce the multivariate zeta function in the remainder to single zeta functions and secondly what is the result for generic values of $q$ and $p$?","Consider a following integral: \begin{equation} {\mathcal I}^{(q,p)} := \int\limits_0^1 \frac{Li_2(-\xi)}{\xi}\cdot [\log(1-\xi)]^q \cdot [\log(1+\xi)]^p d\xi \end{equation} where $q$ and $p$ are non-negative integers. By expanding parts of the integrand in a series and then integrating term by term we have arrived at the following result: \begin{eqnarray} {\mathcal I}^{(3,0)} = \frac{\pi^6}{180} + \log(2) \cdot \left( \pi^2 \zeta(3) - 12 \zeta(5)\right) + r \end{eqnarray} where the remainder $r$ reads \begin{eqnarray} r&=& 3! \zeta\left( \begin{array}{rrr} 4, & 1, & 1\\ &\frac{1}{2}& \end{array}\right) \\ &=& \frac{3}{2} \sum\limits_{k=1}^\infty \left( \frac{[H_{k-1}]^2 - H_{k-1}^{(2)}}{k^3}\right)\cdot \left( H_{\frac{k}{2}} - H_{\frac{k-1}{2}}\right) \\ &=& -6 \int_0^{-1} \frac{S_{2,3}(-\xi)}{1-\xi} d\xi \end{eqnarray} Here $S_{2,3}()$ is the Nielsen generalized poly-logarithm. Now the question is the following, firstly is it possible to reduce the multivariate zeta function in the remainder to single zeta functions and secondly what is the result for generic values of $q$ and $p$?",,"['integration', 'zeta-functions']"
31,Approximating an integral as a parameter grows large,Approximating an integral as a parameter grows large,,"I am trying to calculate the following integral : $$I(\lambda,\alpha)=\int_{\lambda}^1 \mathrm{d}\tau \frac{1-\tau^\alpha}{1-\tau}\exp(-k \tau)$$ where $\lambda<1$, $k$ is a positive constant and $\alpha$ is a large integer. I was thinking of doing the substitution $y=\alpha (1-\tau)$, in which case the integral becomes : $$e^{-k}\int_0^{\alpha(1-\lambda)}\frac{\mathrm{d}y}{y}\left(1-\left(1-\frac{y}{\alpha}\right)^\alpha\right)\exp{\left(\frac{ky}{\alpha}\right)}  $$ and I expect at some point to say that for large $\alpha$, then $(1-\frac{y}{\alpha})^\alpha\simeq e^{-y}$, but this is false when $y$ is close to $\alpha(1-\lambda)$. Is there any way to properly control the error in this assumption and still get an asymptotic equivalent for $I(\lambda,\alpha)$ as $\alpha\to \infty$ ? Thanks in advance.","I am trying to calculate the following integral : $$I(\lambda,\alpha)=\int_{\lambda}^1 \mathrm{d}\tau \frac{1-\tau^\alpha}{1-\tau}\exp(-k \tau)$$ where $\lambda<1$, $k$ is a positive constant and $\alpha$ is a large integer. I was thinking of doing the substitution $y=\alpha (1-\tau)$, in which case the integral becomes : $$e^{-k}\int_0^{\alpha(1-\lambda)}\frac{\mathrm{d}y}{y}\left(1-\left(1-\frac{y}{\alpha}\right)^\alpha\right)\exp{\left(\frac{ky}{\alpha}\right)}  $$ and I expect at some point to say that for large $\alpha$, then $(1-\frac{y}{\alpha})^\alpha\simeq e^{-y}$, but this is false when $y$ is close to $\alpha(1-\lambda)$. Is there any way to properly control the error in this assumption and still get an asymptotic equivalent for $I(\lambda,\alpha)$ as $\alpha\to \infty$ ? Thanks in advance.",,"['integration', 'asymptotics', 'approximate-integration']"
32,"Calculate $\int_{0}^{1} (x-f(x))^{2016} dx$, given $f(f(x))=x$.","Calculate , given .",\int_{0}^{1} (x-f(x))^{2016} dx f(f(x))=x,"This question was asked in an entrance test for an undergraduate mathematics program today, held all over India. Question: $f$ is a differentiable function in $[0,1]$ such that $f(f(x))=x$ and $f(0)=1$. Find the value of $\int_{0}^{1} (x-f(x))^{2016} dx$. I tried to solve it by substituting $f(f(x))$ in place of $x$, but could not proceed much further. Any hints or solutions will be appreciated.","This question was asked in an entrance test for an undergraduate mathematics program today, held all over India. Question: $f$ is a differentiable function in $[0,1]$ such that $f(f(x))=x$ and $f(0)=1$. Find the value of $\int_{0}^{1} (x-f(x))^{2016} dx$. I tried to solve it by substituting $f(f(x))$ in place of $x$, but could not proceed much further. Any hints or solutions will be appreciated.",,['definite-integrals']
33,Is there a better convolution method for deriving $\sum_{p\le x}\frac{1}{p}$ when $p$ is an almost prime?,Is there a better convolution method for deriving  when  is an almost prime?,\sum_{p\le x}\frac{1}{p} p,"It's easy enough to derive an infinite sum for the logarithmic integral using the integral derived by Gauss through stepwise integration.  For example, in my review of calculus I found: $$ li(x) - li(b) = \int^x_b \frac{dt}{\log t} = \sum^{n}_{k=1}(k-1)!\bigg(\frac{x}{(\log x)^k} - \frac{b}{(\log b)^k}\bigg)$$ Further, G.J.O. Jameson makes the symbol $u_P$ for the characteristic function of primes, whereas Halmos might use the symbol $\chi_P$ for the function that is $1$ for primes and $0$ for composites (using Iverson brackets $[ p \in \mathbb{P} ]$ where Kowalski and Iwaniec use the double struck capital $\mathbb{P} \subset \mathbb{Z}^+$ to refer to the primes in positive integers.). Hardy and Wright only give an expression indicating that the series of inverse primes numbers is roughly asymptotic to the second nested logarithm.  Note that I'm not including error bounds. $$ \sum_{p\in {\mathbb{P}\cap [2,x]}} \frac{1}{p} \sim \log\log(x) - \log\log(2) + 1 $$ Because the Dirichlet convolution $u_P * u_P$ is the characteristic of the numbers with two prime divisors is it possible that there is a convolution operator for $li' \star li'$ since $u_P$ behaves like $\frac{1}{\log(x)}$ on average? I'm asking how to define the convolution operator ($\star$, see below).  I'm not really well versed, but it seems like there should be a smooth analogous operator to  the second Dirichlet Convolution of the characteristic function of primes constructed using logarithmic integral, such that $$ li'(x) = \frac{1}{\log x} \ , \ \ \ \ (li' \star li')(x) \sim \frac{x \log\log x }{\log x}$$ because Hardy/Wright makes it very clear that they mean the following $$ \frac{x \log \log x}{\log x} \sim li(x) \sum _{p\le x}{\frac{1}{p}}  $$ How do I find the operator$\ \star$ , where $$(li' \star li')(x) \sim li(x) \sum _{p\le x}{\frac{1}{p}}  $$ UPDATE There is this post on Math Overflow with answer by Lucia , but my study of the Zeta Function is only in an early stage -- I haven't much exposure to calculus in the complex space and I get easily lost with standard references on Sieve Theory.  H.E. Rose treats the topic in Chapter 3 of A Course on Number Theory and assigns the proof that $\sum_{p \le x}\frac{1}{p} \sim (\log \log x)^2 + O(\log\log x) $ for when $p$ is a semiprime as a problem. Answer this one for the bounty.","It's easy enough to derive an infinite sum for the logarithmic integral using the integral derived by Gauss through stepwise integration.  For example, in my review of calculus I found: $$ li(x) - li(b) = \int^x_b \frac{dt}{\log t} = \sum^{n}_{k=1}(k-1)!\bigg(\frac{x}{(\log x)^k} - \frac{b}{(\log b)^k}\bigg)$$ Further, G.J.O. Jameson makes the symbol $u_P$ for the characteristic function of primes, whereas Halmos might use the symbol $\chi_P$ for the function that is $1$ for primes and $0$ for composites (using Iverson brackets $[ p \in \mathbb{P} ]$ where Kowalski and Iwaniec use the double struck capital $\mathbb{P} \subset \mathbb{Z}^+$ to refer to the primes in positive integers.). Hardy and Wright only give an expression indicating that the series of inverse primes numbers is roughly asymptotic to the second nested logarithm.  Note that I'm not including error bounds. $$ \sum_{p\in {\mathbb{P}\cap [2,x]}} \frac{1}{p} \sim \log\log(x) - \log\log(2) + 1 $$ Because the Dirichlet convolution $u_P * u_P$ is the characteristic of the numbers with two prime divisors is it possible that there is a convolution operator for $li' \star li'$ since $u_P$ behaves like $\frac{1}{\log(x)}$ on average? I'm asking how to define the convolution operator ($\star$, see below).  I'm not really well versed, but it seems like there should be a smooth analogous operator to  the second Dirichlet Convolution of the characteristic function of primes constructed using logarithmic integral, such that $$ li'(x) = \frac{1}{\log x} \ , \ \ \ \ (li' \star li')(x) \sim \frac{x \log\log x }{\log x}$$ because Hardy/Wright makes it very clear that they mean the following $$ \frac{x \log \log x}{\log x} \sim li(x) \sum _{p\le x}{\frac{1}{p}}  $$ How do I find the operator$\ \star$ , where $$(li' \star li')(x) \sim li(x) \sum _{p\le x}{\frac{1}{p}}  $$ UPDATE There is this post on Math Overflow with answer by Lucia , but my study of the Zeta Function is only in an early stage -- I haven't much exposure to calculus in the complex space and I get easily lost with standard references on Sieve Theory.  H.E. Rose treats the topic in Chapter 3 of A Course on Number Theory and assigns the proof that $\sum_{p \le x}\frac{1}{p} \sim (\log \log x)^2 + O(\log\log x) $ for when $p$ is a semiprime as a problem. Answer this one for the bounty.",,"['integration', 'number-theory', 'convolution', 'dirichlet-convolution']"
34,Equality of integrals versus equality of integrands,Equality of integrals versus equality of integrands,,"This is a puzzling point for me, and I see it often used in some textbooks when authors switch between differential and integral forms of equations (e.g. in electrodynamics). I wish to know if there is any proven mathematical theorem or lemma that relates to this. Given the equality between two integrals: $$\int_{V} f(x,y,z)\, dV =\int_{V} g(x,y,z)\, dV.$$ It is often said that, if this equality holds for an arbitrary domain $V$ (e.g. a volume), then this  equality implies equality of the involved integrands too, that is $f=g$. Now, although I understand that this works because the arbitrariness of $V$ implies that one could choose an infinitesimally small $V$ where the integrands could be considered constants and taken outside the integrals, allowing for the above conclusion to be obtained, I am not sure whether it is a general rule or one that doesn't have other necessary conditions. For example, would it work if the functions $f$ and $g$ were dependent on $V$ to start with? We could have cases or phenomena where arbitrary $V$ gives different $f$ and $g$ and the equality of integrals still holds, but would it then still imply equality of integrands? Is there any known theorem or lemma that talk about this more rigorously?","This is a puzzling point for me, and I see it often used in some textbooks when authors switch between differential and integral forms of equations (e.g. in electrodynamics). I wish to know if there is any proven mathematical theorem or lemma that relates to this. Given the equality between two integrals: $$\int_{V} f(x,y,z)\, dV =\int_{V} g(x,y,z)\, dV.$$ It is often said that, if this equality holds for an arbitrary domain $V$ (e.g. a volume), then this  equality implies equality of the involved integrands too, that is $f=g$. Now, although I understand that this works because the arbitrariness of $V$ implies that one could choose an infinitesimally small $V$ where the integrands could be considered constants and taken outside the integrals, allowing for the above conclusion to be obtained, I am not sure whether it is a general rule or one that doesn't have other necessary conditions. For example, would it work if the functions $f$ and $g$ were dependent on $V$ to start with? We could have cases or phenomena where arbitrary $V$ gives different $f$ and $g$ and the equality of integrals still holds, but would it then still imply equality of integrands? Is there any known theorem or lemma that talk about this more rigorously?",,"['calculus', 'real-analysis', 'integration', 'functional-analysis', 'mathematical-physics']"
35,How to think about inequalities in analysis,How to think about inequalities in analysis,,"My question is kind of vague, but I'm wondering how to think about many inequalities that appear in real analysis. For example, I don't really see why Holder's Inequality should be true, and its proof is quite tricky and not revealing. And Holder's inequality is much simpler compared to inequalities regarding the Marcinkiewicz integral or the Hardy Littlewood maximal function. How does one wrap one's head around these? Reading the proofs is easy but understanding why they are true seems much much harder, and something I have to get better at.","My question is kind of vague, but I'm wondering how to think about many inequalities that appear in real analysis. For example, I don't really see why Holder's Inequality should be true, and its proof is quite tricky and not revealing. And Holder's inequality is much simpler compared to inequalities regarding the Marcinkiewicz integral or the Hardy Littlewood maximal function. How does one wrap one's head around these? Reading the proofs is easy but understanding why they are true seems much much harder, and something I have to get better at.",,"['real-analysis', 'integration', 'analysis']"
36,How to prove this Lambert W identity: $-\pi W(-1) = \int_{-\infty}^{\frac{-1}{e}} \Im ( W ' (x) ) \ln(1 + \frac{1}{x} ) dx $? [closed],How to prove this Lambert W identity: ? [closed],-\pi W(-1) = \int_{-\infty}^{\frac{-1}{e}} \Im ( W ' (x) ) \ln(1 + \frac{1}{x} ) dx ,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 days ago . Improve this question I was wondering how to prove this Lambert W identity: $-\pi W(-1) = \int_{-\infty}^{\frac{-1}{e}} \Im ( W ' (x) ) \ln(1 + \frac{1}{x})dx $ . Maybe with contour integration?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 days ago . Improve this question I was wondering how to prove this Lambert W identity: . Maybe with contour integration?",-\pi W(-1) = \int_{-\infty}^{\frac{-1}{e}} \Im ( W ' (x) ) \ln(1 + \frac{1}{x})dx ,"['calculus', 'integration', 'special-functions', 'harmonic-functions', 'lambert-w']"
37,Solve this integral equation that results in a linear function,Solve this integral equation that results in a linear function,,"I need to find the family of real-valued single variable functions $F:\, (0,1) \to [0,1]$ that satisfy the following integral equation: $$\int_{\theta = 0}^{\pi} F\big(~ x\, \sin\theta ~\big)\text{d} \theta + \int_{\theta = \pi}^{2\pi} \bigg[ 1 - F\big(~ 1 + x\, \sin\theta ~\big) \bigg] \text{d} \theta = Ax $$ or equivalently $$\int_{\theta = 0}^{\pi} \bigg[ F\big(~ x\, \sin\theta ~\big) + 1 - F\big(~ 1 - x\, \sin\theta ~\big) \bigg]\text{d} \theta  = Ax $$ where $0 < x < 1$ and $0 < A < 1$ , with $F$ required to be non-decreasing and differentiable (just once is needed). The open- or closed-ness of the domain and range of $F$ is not important to me. The idea is that the integrations yield a RHS that is directly proportional to the coefficient of the argument in the integrand. My Questions: How can this be solved? I'm open to additional assumptions (necessary or not) or restrictions to the solution $F$ . If this kind of problem is hard to tackle in general, what material (like a particular topic, or some textbooks) should I familiarize myself with to even begin considering this task? So far I know that linear $F(u) = u$ and quadratic $F(u) = u^2$ both satisfy the equation. The cubic $F(u) = u^3$ is not a solution. Due to the some possible skew-symmetry with respect to $\theta = \pi$ and $\frac12$ for the argument of $F$ , it's tempting to guess that rotational symmetric function (odd functions) like $F(u) = \int_{0}^{u} [1+8(s-\frac12)^3 ]\;\text{d}s$ , which is a quartic polynomial, might do the job, but it doesn't. Nor do $F(u) = \frac14 \int_{0}^{u} [1 - \cos( \pi s) ]\;\text{d}s = \frac14\left[u- \frac1{\pi} \sin(\pi u)  \right]$ or the several sinusoidal functions I looked at. Taking the derivative $\partial_x$ to the whole equation isn't very helpful. If I'm not mistaken the equation is sensitive to the range of $\theta$ angle integration as well as the "" $1-$ "" in the 2nd integral and the "" $+1$ "" in $F$ , as seen in the first format. Thank you. P.S. One might notice that $F$ can be a probability CDF. This is indeed the context within which this integral equation emerged. As of now I don't see much reason to provide the context, which might be a distraction.","I need to find the family of real-valued single variable functions that satisfy the following integral equation: or equivalently where and , with required to be non-decreasing and differentiable (just once is needed). The open- or closed-ness of the domain and range of is not important to me. The idea is that the integrations yield a RHS that is directly proportional to the coefficient of the argument in the integrand. My Questions: How can this be solved? I'm open to additional assumptions (necessary or not) or restrictions to the solution . If this kind of problem is hard to tackle in general, what material (like a particular topic, or some textbooks) should I familiarize myself with to even begin considering this task? So far I know that linear and quadratic both satisfy the equation. The cubic is not a solution. Due to the some possible skew-symmetry with respect to and for the argument of , it's tempting to guess that rotational symmetric function (odd functions) like , which is a quartic polynomial, might do the job, but it doesn't. Nor do or the several sinusoidal functions I looked at. Taking the derivative to the whole equation isn't very helpful. If I'm not mistaken the equation is sensitive to the range of angle integration as well as the "" "" in the 2nd integral and the "" "" in , as seen in the first format. Thank you. P.S. One might notice that can be a probability CDF. This is indeed the context within which this integral equation emerged. As of now I don't see much reason to provide the context, which might be a distraction.","F:\, (0,1) \to [0,1] \int_{\theta = 0}^{\pi} F\big(~ x\, \sin\theta ~\big)\text{d} \theta + \int_{\theta = \pi}^{2\pi} \bigg[ 1 - F\big(~ 1 + x\, \sin\theta ~\big) \bigg] \text{d} \theta = Ax  \int_{\theta = 0}^{\pi} \bigg[ F\big(~ x\, \sin\theta ~\big) + 1 - F\big(~ 1 - x\, \sin\theta ~\big) \bigg]\text{d} \theta  = Ax  0 < x < 1 0 < A < 1 F F F F(u) = u F(u) = u^2 F(u) = u^3 \theta = \pi \frac12 F F(u) = \int_{0}^{u} [1+8(s-\frac12)^3 ]\;\text{d}s F(u) = \frac14 \int_{0}^{u} [1 - \cos( \pi s) ]\;\text{d}s = \frac14\left[u- \frac1{\pi} \sin(\pi u)  \right] \partial_x \theta 1- +1 F F","['integration', 'functional-equations', 'integral-equations']"
38,"Sequence $1, \frac{1}{2},\frac{1}{6},\frac{1}{8},\frac{7}{90},\frac{19}{288},\frac{41}{840},\frac{751}{17280}$",Sequence,"1, \frac{1}{2},\frac{1}{6},\frac{1}{8},\frac{7}{90},\frac{19}{288},\frac{41}{840},\frac{751}{17280}","Let $d\geq 0$ be a degree. Using the fact that the $L_1$ norm and the $L_{\infty}$ norm are equivalent on the finite-dimensional space of polynomials of degree $\leq d$, we see that there is a constant $c_d>0$ such that $$\int_{[0,1]}f \geq c_df(0)$$ whenever $f\in{\mathbb R}[x]$ with degree $d$ and $f\geq 0$ on $[0,1]$. I have computed the optimal value for $c_d$  for $d\leq 7$ : $$ \begin{array}{|c|c|c|c|c|c|c|c|} \hline d & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\ \hline c_d & 1 & \frac{1}{2} & \frac{1}{6} & \frac{1}{8} & \frac{7}{90}  & \frac{19}{288} & \frac{41}{840} & \frac{751}{17280} \\ \hline \end{array} $$ Is this sequence already known ? I found those values of $c_d$ by showing that $\int_{[0,1]}f$ can be written as a nonnegative linear combination of the $f(\frac{k}{d})$ for $0\leq k\leq d$, with the coefficient before $f(0)$ equal to $c_d$. This method breaks down for $d=8$ (and in fact, seems to break down for practically all $d\geq 8$) because negative coefficients appear in the decomposition.","Let $d\geq 0$ be a degree. Using the fact that the $L_1$ norm and the $L_{\infty}$ norm are equivalent on the finite-dimensional space of polynomials of degree $\leq d$, we see that there is a constant $c_d>0$ such that $$\int_{[0,1]}f \geq c_df(0)$$ whenever $f\in{\mathbb R}[x]$ with degree $d$ and $f\geq 0$ on $[0,1]$. I have computed the optimal value for $c_d$  for $d\leq 7$ : $$ \begin{array}{|c|c|c|c|c|c|c|c|} \hline d & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\ \hline c_d & 1 & \frac{1}{2} & \frac{1}{6} & \frac{1}{8} & \frac{7}{90}  & \frac{19}{288} & \frac{41}{840} & \frac{751}{17280} \\ \hline \end{array} $$ Is this sequence already known ? I found those values of $c_d$ by showing that $\int_{[0,1]}f$ can be written as a nonnegative linear combination of the $f(\frac{k}{d})$ for $0\leq k\leq d$, with the coefficient before $f(0)$ equal to $c_d$. This method breaks down for $d=8$ (and in fact, seems to break down for practically all $d\geq 8$) because negative coefficients appear in the decomposition.",,"['integration', 'sequences-and-series', 'polynomials']"
39,Tricky integral,Tricky integral,,"I am trying to do the following integral for a few days now, but getting nowhere. \begin{align} \int_0^x \mathrm dy\,y^2 \cos(y^2) C^2 \!\!\left(\!\dfrac{\sqrt{2}\,y}{\sqrt\pi}\!\right)\! \end{align} In reality only the first one is causing me troubles, however I have pasted the entire expression as it might lead to some cancellations. Any help would be welcome. $C()$ is the Fresnel integral.","I am trying to do the following integral for a few days now, but getting nowhere. \begin{align} \int_0^x \mathrm dy\,y^2 \cos(y^2) C^2 \!\!\left(\!\dfrac{\sqrt{2}\,y}{\sqrt\pi}\!\right)\! \end{align} In reality only the first one is causing me troubles, however I have pasted the entire expression as it might lead to some cancellations. Any help would be welcome. $C()$ is the Fresnel integral.",,"['integration', 'definite-integrals', 'indefinite-integrals', 'fresnel-integrals']"
40,Deriving a Series Representation of the Bessel Function of the First Kind,Deriving a Series Representation of the Bessel Function of the First Kind,,"I've tried to use an integral representation of the Bessel Function of the First Kind $J_n(x)$ to derive a series representation of the function. My end result is pretty close to the answer that it should be, but it still doesn't quite match up and I'd be grateful for anybody that could point out where my work is faulty. \begin{align} J_n(x) &= \frac{1}{2\pi}\int_0^{2\pi}\cos(n\phi-x\sin\phi)d\phi\\ &= \frac{1}{2\pi}\int_0^{2\pi}e^{in\phi}e^{-ix\sin\phi}d\phi \tag1\\  &=\frac{1}{2\pi}\int_0^{2\pi}\left(e^{i\phi}\right)^ne^{-\frac{x}{2}\left(e^{i\phi}-e^{-i\phi}\right)}d\phi \tag2 \end{align} In $(1)$, I used the fact that the integral of the imaginary part of the integrand over a complete period is equal to $0$. A parametrisation of the unit circle in the complex plane is given by $z=e^{i\phi}, \phi\in(0,2\pi)$, thus $e^{-i\phi}=\left(e^{i\phi}\right)^{-1}=\frac{1}{z}$. Letting $\Gamma$ be the unit circle in the complex plane traversed in the counterclockwise direction, $(2)$ becomes \begin{align} J_n(x) &= \frac{1}{2\pi}\oint_\Gamma z^n e^{-\frac{x}{2}z}e^{\frac{x}{2z}}dz\\ &= \frac{1}{2\pi}\oint_\Gamma z^n e^{-\frac{x}{2}z}\sum_{k=0}^{\infty}\frac{x^k}{2^kk!z^k}dz \tag3\\ &=\frac{1}{2\pi}\sum_{k=0}^{\infty}\frac{x^k}{2^kk!}\oint_\Gamma z^{n-k}e^{-\frac{x}{2}z}dz\\ &=i\sum_{k=0}^{\infty}\frac{x^k}{2^kk!}\text{Res}\left(z^{n-k}e^{-\frac{x}{2}z},0\right) \tag4 \end{align} $(3)$ uses the Laurent series expansion of $e^{\frac{1}{z}}$ and $(4)$ uses the Residue Theorem. Let $m:=k-n\rightarrow k=m+n$. Then the Residue of $z^{-m}e^{-\frac{x}{2}z}$ is only non-zero for strictly positive values of $m$ and $(4)$ becomes \begin{align} J_n(x) &= i\sum_{m=1}^{\infty}\frac{x^{m+n}}{2^{m+n}(m+n)!}\text{Res}\left(z^{-m}e^{-\frac{x}{2}z},0\right)\\ &= i\sum_{m=1}^{\infty}\frac{x^{m+n}}{2^{m+n}(m+n)!}\frac{1}{(m-1)!}\lim\limits_{z\rightarrow 0}\frac{d^{m-1}}{dz^{m-1}}e^{-\frac{x}{2}z}\\ &=i\sum_{m=1}^{\infty}\frac{x^{m+n}}{2^{m+n}(m+n)!}\frac{(-1)^{m-1}}{(m-1)!}\frac{x^{m-1}}{2^{m-1}} \tag5 \end{align} Letting $r=m-1\rightarrow m=r+1$, $(5)$ becomes \begin{align} J_n(x)=i\sum_{r=0}^{\infty}(-1)^r\frac{x^{2r+n+1}}{2^{2r+n+1}r!(r+n+1)!}\tag6 \end{align} However, comparing this with the known series representation, the $i$ shouldn't be there (obviously, since $J_n(x)$ is a real function) and the sum (disregarding the $i$ for a moment) is actually a series representation of $J_{n+1}(x)$, not $J_n(x)$. I've been over this calculation a few times, and I simply cannot find where I made a mistake that either allowed the $i$ to creep in there or that raised the order of the Bessel Function that I am trying to find a series representation for. Any ideas?","I've tried to use an integral representation of the Bessel Function of the First Kind $J_n(x)$ to derive a series representation of the function. My end result is pretty close to the answer that it should be, but it still doesn't quite match up and I'd be grateful for anybody that could point out where my work is faulty. \begin{align} J_n(x) &= \frac{1}{2\pi}\int_0^{2\pi}\cos(n\phi-x\sin\phi)d\phi\\ &= \frac{1}{2\pi}\int_0^{2\pi}e^{in\phi}e^{-ix\sin\phi}d\phi \tag1\\  &=\frac{1}{2\pi}\int_0^{2\pi}\left(e^{i\phi}\right)^ne^{-\frac{x}{2}\left(e^{i\phi}-e^{-i\phi}\right)}d\phi \tag2 \end{align} In $(1)$, I used the fact that the integral of the imaginary part of the integrand over a complete period is equal to $0$. A parametrisation of the unit circle in the complex plane is given by $z=e^{i\phi}, \phi\in(0,2\pi)$, thus $e^{-i\phi}=\left(e^{i\phi}\right)^{-1}=\frac{1}{z}$. Letting $\Gamma$ be the unit circle in the complex plane traversed in the counterclockwise direction, $(2)$ becomes \begin{align} J_n(x) &= \frac{1}{2\pi}\oint_\Gamma z^n e^{-\frac{x}{2}z}e^{\frac{x}{2z}}dz\\ &= \frac{1}{2\pi}\oint_\Gamma z^n e^{-\frac{x}{2}z}\sum_{k=0}^{\infty}\frac{x^k}{2^kk!z^k}dz \tag3\\ &=\frac{1}{2\pi}\sum_{k=0}^{\infty}\frac{x^k}{2^kk!}\oint_\Gamma z^{n-k}e^{-\frac{x}{2}z}dz\\ &=i\sum_{k=0}^{\infty}\frac{x^k}{2^kk!}\text{Res}\left(z^{n-k}e^{-\frac{x}{2}z},0\right) \tag4 \end{align} $(3)$ uses the Laurent series expansion of $e^{\frac{1}{z}}$ and $(4)$ uses the Residue Theorem. Let $m:=k-n\rightarrow k=m+n$. Then the Residue of $z^{-m}e^{-\frac{x}{2}z}$ is only non-zero for strictly positive values of $m$ and $(4)$ becomes \begin{align} J_n(x) &= i\sum_{m=1}^{\infty}\frac{x^{m+n}}{2^{m+n}(m+n)!}\text{Res}\left(z^{-m}e^{-\frac{x}{2}z},0\right)\\ &= i\sum_{m=1}^{\infty}\frac{x^{m+n}}{2^{m+n}(m+n)!}\frac{1}{(m-1)!}\lim\limits_{z\rightarrow 0}\frac{d^{m-1}}{dz^{m-1}}e^{-\frac{x}{2}z}\\ &=i\sum_{m=1}^{\infty}\frac{x^{m+n}}{2^{m+n}(m+n)!}\frac{(-1)^{m-1}}{(m-1)!}\frac{x^{m-1}}{2^{m-1}} \tag5 \end{align} Letting $r=m-1\rightarrow m=r+1$, $(5)$ becomes \begin{align} J_n(x)=i\sum_{r=0}^{\infty}(-1)^r\frac{x^{2r+n+1}}{2^{2r+n+1}r!(r+n+1)!}\tag6 \end{align} However, comparing this with the known series representation, the $i$ shouldn't be there (obviously, since $J_n(x)$ is a real function) and the sum (disregarding the $i$ for a moment) is actually a series representation of $J_{n+1}(x)$, not $J_n(x)$. I've been over this calculation a few times, and I simply cannot find where I made a mistake that either allowed the $i$ to creep in there or that raised the order of the Bessel Function that I am trying to find a series representation for. Any ideas?",,"['integration', 'complex-analysis', 'power-series', 'contour-integration', 'bessel-functions']"
41,A holomorphic function inequality. Is it right?,A holomorphic function inequality. Is it right?,,"I have proved using Green's theorem the following equality:$$\frac{1}{2\pi i}\left(\frac{1}{\zeta}-\bar{\zeta}\right)\int_{\partial \mathbb{D}}f(z+\zeta)\log\left(\frac{z}{z-\zeta}\right)dz=f(\zeta)+\frac{\zeta}{\pi}\iint_{\mathbb{D}}\frac{\bar{z}-\bar{\zeta}}{1-\zeta \bar{z}}f(z+\zeta)dxdy $$ For some $\zeta$ inside the disc and for $f:\Omega\to \mathbb{C}$ , $\mathbb{D} \subset \Omega$. Now I would like to estimate the integral in the left side. I consider a $\zeta$ such that $0<|\zeta|<1$  Now i would like to estimate $f(\zeta)$. After calculating the contour integral with residues, what i get is: $$f(\zeta)=\zeta f(2\zeta)\left(\frac{1-|\zeta|^2}{\zeta}\right)+\frac{\zeta}{\pi}\iint_{\mathbb{D}}\frac{\bar{z}-\bar{\zeta}}{1-\zeta \bar{z}}f(z+\zeta)dxdy$$ and after using the triangle inequality I get: $$|f(\zeta)|\le (1-|\zeta|^2)|f(2\zeta)|+\frac{|\zeta|}{\pi}\iint_{\mathbb{D}}|f(z+\zeta)|dxdy.$$  If $f$ is univalent then we substitute $f(z+\zeta) $ with $(f^{'}(z+\zeta))^2 $ one has a lower bound for the area of this function inside the unit disc. $$\frac{|f^{'}(z)|^2-(1-|z|^2)|f^{'}(2z)|^2}{|z|}\le \mathrm{Αrea} f(\mathbb{D})$$ Are my estimations meaningful ?","I have proved using Green's theorem the following equality:$$\frac{1}{2\pi i}\left(\frac{1}{\zeta}-\bar{\zeta}\right)\int_{\partial \mathbb{D}}f(z+\zeta)\log\left(\frac{z}{z-\zeta}\right)dz=f(\zeta)+\frac{\zeta}{\pi}\iint_{\mathbb{D}}\frac{\bar{z}-\bar{\zeta}}{1-\zeta \bar{z}}f(z+\zeta)dxdy $$ For some $\zeta$ inside the disc and for $f:\Omega\to \mathbb{C}$ , $\mathbb{D} \subset \Omega$. Now I would like to estimate the integral in the left side. I consider a $\zeta$ such that $0<|\zeta|<1$  Now i would like to estimate $f(\zeta)$. After calculating the contour integral with residues, what i get is: $$f(\zeta)=\zeta f(2\zeta)\left(\frac{1-|\zeta|^2}{\zeta}\right)+\frac{\zeta}{\pi}\iint_{\mathbb{D}}\frac{\bar{z}-\bar{\zeta}}{1-\zeta \bar{z}}f(z+\zeta)dxdy$$ and after using the triangle inequality I get: $$|f(\zeta)|\le (1-|\zeta|^2)|f(2\zeta)|+\frac{|\zeta|}{\pi}\iint_{\mathbb{D}}|f(z+\zeta)|dxdy.$$  If $f$ is univalent then we substitute $f(z+\zeta) $ with $(f^{'}(z+\zeta))^2 $ one has a lower bound for the area of this function inside the unit disc. $$\frac{|f^{'}(z)|^2-(1-|z|^2)|f^{'}(2z)|^2}{|z|}\le \mathrm{Αrea} f(\mathbb{D})$$ Are my estimations meaningful ?",,"['integration', 'complex-analysis', 'analysis', 'estimation']"
42,Deriving Yang-Mills equations,Deriving Yang-Mills equations,,"On the same spirit of this unanswered question I am proposing this question which I have been trying for some time now. Here I'm working with dimension $n = 4$ (identifying $\mathbb H = \mathbb R^4$) considering the principal $SU(2)$-bundle with $\rho : SU(2) \to GL(2, \mathbb C)$ being the adjoint representation $ad_g (A) = g^{-1} A g$, for all $g \in SU(2)$ and $A \in \mathfrak {su}(2)$.  Moreover, the gauge potential is written (in local coordinates) as $\mathcal A = \mathcal A_{\alpha} dx^\alpha$, $\alpha = 1,2,3,4$ and its gauge field strength (curvature) is given by $$\mathcal F = d\mathcal A + \frac{1}{2}[\mathcal A, \mathcal A] = \frac{1}{2} \mathcal F_{\alpha\beta} dx^\alpha \wedge dx^\beta$$ where (after some lengthy calculation) $$\mathcal F_{\alpha\beta} = \partial_\alpha \mathcal A_\beta - \partial_\beta\mathcal A_\alpha + [\mathcal A_\alpha, \mathcal A_\beta]\ \  , \ \ \beta= 1,2,3,4$$ where $\partial_\alpha = \frac{\partial}{\partial x^\alpha}$. Question: Derive the Euler-Lagrange equations of the Yang-Mills functional given by $$\mathcal {YM} (\mathcal A) = \frac{1}{4}\int_{\mathbb R^4} \|\mathcal F\|^2 d(\bf vol_{\mathbb R^4})$$   such equations are called in the physics literature Yang-Mills equations $$\ast d^{\mathcal A}(\ast \mathcal F) = \sum_{\alpha = 1}^4 (\partial_\alpha \mathcal F_{\alpha\beta} + [\mathcal A_\alpha, \mathcal F_{\alpha\beta}]) = 0$$ where $\ast$ is the Hodge dual operator, $d^\mathcal A$ is the covariant exterior derivative. Attempt: Since the space of gauge potentials is an affine space we may consider the variations $\mathcal A + t\mathcal B$ (family of 1-parameter of gauge potentials) thus $$\begin{aligned}\frac{d}{dt} \Big(\mathcal {YM}(\mathcal A + t \mathcal B)\Big)\Big|_{t= 0} &= \frac{1}{4}\int_{\mathbb R^4} \frac{d}{dt} \Big(\|\mathcal F\|^2\Big)\Big|_{t = 0} d (\bf vol_{\mathbb R^4})\\ &=\frac{1}{2} \int_{\mathbb R^4} \|\mathcal F\| \frac{\left\langle \mathcal F, \frac{d}{dt} (\mathcal F)\Big|_{t =0}\right\rangle}{\|\mathcal F\|} d(\bf vol_{\mathbb R^4})\\&=\frac{1}{2} \int_{\mathbb R^4} \left\langle \mathcal F, \frac{d}{dt} (\mathcal F)\Big|_{t=0}\right\rangle d(\bf vol_{\mathbb R^4})\end{aligned}$$ where $$\begin{aligned} \mathcal F &= \frac{1}{2} \mathcal F_{\alpha \beta} dx^\alpha \wedge dx^\beta \\&= \frac{1}{2}\Bigg( \partial_\alpha (\mathcal A_\beta + t \mathcal B_\beta) - \partial_\beta (\mathcal A_\alpha + t \mathcal B_\alpha) + [\mathcal A_\alpha, \mathcal A_\beta] \\&+ t [\mathcal A_\alpha, \mathcal B_\beta] + t [\mathcal B_\alpha, \mathcal A_\beta] + t^2 [\mathcal B_\alpha, \mathcal B_\beta]\Bigg) dx^\alpha \wedge dx^\beta\end{aligned}$$ by linearity we then obtain $$\frac{d}{dt} (\mathcal F)\Big|_{t = 0} = \Bigg(\partial_\alpha \mathcal B_\beta - \partial_\beta \mathcal B_\alpha + [\mathcal A _\alpha, \mathcal B_\beta] + [\mathcal B_\alpha, \mathcal A_\beta]\Bigg) dx^\alpha \wedge dx^\beta$$ Now $$\begin{aligned}\frac{d}{dt}\Big(\mathcal {YM}(\mathcal A + t \mathcal B)\Big)\Big|_{t = 0} &=\frac{1}{2} \int_{\mathbb R^4} \left\langle \mathcal F, \frac{d}{dt} (\mathcal F)\Big|_{t=0}\right\rangle d(\bf vol_{\mathbb R^4}) \\&= \frac{1}{2}\int_{\mathbb R^4} \mathcal F_{\alpha\beta} (\partial_\alpha \mathcal B_\beta - \partial_\beta \mathcal B_\alpha + [\mathcal A _\alpha, \mathcal B_\beta] + [\mathcal B_\alpha, \mathcal A_\beta])d(\bf vol_{\mathbb R^4}) \end{aligned}$$  Next step should be integration by parts. Then using the fact (Divergence Theorem) $$\int_{\mathbb R^4} \partial_i (f) g dV_g = - \int_{\mathbb R^4} f \partial_i (g) dV_g $$ and that $\mathcal F_{\beta\alpha} = - \mathcal F_{\alpha\beta}$ we have $$\begin{aligned}\delta\mathcal {YM}(A)&=\frac{1}{2} \int_{\mathbb R^4} \left\langle \mathcal F, \frac{d}{dt} (\mathcal F)\Big|_{t=0}\right\rangle d(\bf {vol}_{\mathbb R^4}) \\&= \frac{1}{2}\int_{\mathbb R^4} \mathcal F_{\alpha\beta} (\partial_\alpha \mathcal B_\beta - \partial_\beta \mathcal B_\alpha + [\mathcal A _\alpha, \mathcal B_\beta] + [\mathcal B_\alpha, \mathcal A_\beta])d(\bf {vol}_{\mathbb R^4})\\&= \frac{1}{2}\left(\int_{\mathbb R^4} \mathcal F_{\alpha\beta} (\partial_\alpha \mathcal B_\beta) d(\bf {vol}_{\mathbb R^4}) - \int_{\mathbb R^4}\mathcal F_{\alpha\beta}(\partial_\beta \mathcal B_\alpha) d(\bf {vol}_{\mathbb R^4})\right) \\&+ \frac{1}{2}\left(\int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] + \mathcal F_{\alpha\beta}[\mathcal B_\alpha, \mathcal A_\beta])d(\bf {vol}_{\mathbb R^4}) \right)\\&= -\frac{1}{2}\left(\int_{\mathbb R^4} 2\mathcal B_\beta (\partial_\alpha \mathcal F_{\alpha\beta} d(\bf {vol}_{\mathbb R^4})\right) + \frac{1}{2}\left(\int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] + \mathcal F_{\alpha\beta}[\mathcal B_\alpha, \mathcal A_\beta]d(\bf {vol}_{\mathbb R^4}) \right)\\&=-\int_{\mathbb R^4} \mathcal B_\beta (\partial_\alpha \mathcal F_{\alpha\beta} d(\bf {vol}_{\mathbb R^4}) \\&+ \frac{1}{2}\left(\int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] + \mathcal F_{\alpha\beta}[\mathcal B_\alpha, \mathcal A_\beta])d(\bf {vol}_{\mathbb R^4}) \right)\\&=-\int_{\mathbb R^4} \mathcal B_\beta (\partial_\alpha \mathcal F_{\alpha\beta} d(\bf {vol}_{\mathbb R^4}) + \int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] d(\bf {vol}_{\mathbb R^4})\end{aligned}$$ I need only show that $$\int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] d(\bf {vol}_{\mathbb R^4}) = - \int_{\mathbb R^4} \mathcal B_\beta [\mathcal A_\alpha,\mathcal F_{\alpha\beta}]d (\bf {vol}_{\mathbb R^4}) $$ I could only think of using Jacobi's identity here to yield the change of $\mathcal F_{\alpha \beta}$ and $\mathcal B_\beta$ $$\begin{aligned} 0 &= [\mathcal F_{\alpha\beta}, [\mathcal A_\alpha, \mathcal B_\beta]] + [\mathcal A_\alpha, [\mathcal B_\beta, \mathcal F_{\alpha\beta}]] + [\mathcal B_\beta , [\mathcal F_{\alpha\beta}, \mathcal A_\alpha]] \\&= [\mathcal F_{\alpha\beta}, [\mathcal A_\alpha, \mathcal B_\beta]] + [\mathcal A_\alpha, [\mathcal B_\beta, \mathcal F_{\alpha\beta}]] - [\mathcal B_\beta , [\mathcal A_\alpha, \mathcal F_{\alpha\beta}]]\end{aligned}$$ But couldn't reach any further. Where do I want to get? $$\begin{aligned}\delta (\mathcal {YM}) (\mathcal A)  = - \int_{\mathbb R^4} \mathcal B_\alpha (\partial_\alpha \mathcal F_{\alpha\beta} + [\mathcal A_\alpha, \mathcal F_{\alpha \beta} ]) d(\bf vol_{\mathbb R^4})\end{aligned}$$ then the stationary points satisfy $$\sum_{\alpha = 1}^4 (\partial_\alpha \mathcal F_{\alpha\beta} + [\mathcal A_\alpha, \mathcal F_{\alpha\beta}]) = 0$$","On the same spirit of this unanswered question I am proposing this question which I have been trying for some time now. Here I'm working with dimension $n = 4$ (identifying $\mathbb H = \mathbb R^4$) considering the principal $SU(2)$-bundle with $\rho : SU(2) \to GL(2, \mathbb C)$ being the adjoint representation $ad_g (A) = g^{-1} A g$, for all $g \in SU(2)$ and $A \in \mathfrak {su}(2)$.  Moreover, the gauge potential is written (in local coordinates) as $\mathcal A = \mathcal A_{\alpha} dx^\alpha$, $\alpha = 1,2,3,4$ and its gauge field strength (curvature) is given by $$\mathcal F = d\mathcal A + \frac{1}{2}[\mathcal A, \mathcal A] = \frac{1}{2} \mathcal F_{\alpha\beta} dx^\alpha \wedge dx^\beta$$ where (after some lengthy calculation) $$\mathcal F_{\alpha\beta} = \partial_\alpha \mathcal A_\beta - \partial_\beta\mathcal A_\alpha + [\mathcal A_\alpha, \mathcal A_\beta]\ \  , \ \ \beta= 1,2,3,4$$ where $\partial_\alpha = \frac{\partial}{\partial x^\alpha}$. Question: Derive the Euler-Lagrange equations of the Yang-Mills functional given by $$\mathcal {YM} (\mathcal A) = \frac{1}{4}\int_{\mathbb R^4} \|\mathcal F\|^2 d(\bf vol_{\mathbb R^4})$$   such equations are called in the physics literature Yang-Mills equations $$\ast d^{\mathcal A}(\ast \mathcal F) = \sum_{\alpha = 1}^4 (\partial_\alpha \mathcal F_{\alpha\beta} + [\mathcal A_\alpha, \mathcal F_{\alpha\beta}]) = 0$$ where $\ast$ is the Hodge dual operator, $d^\mathcal A$ is the covariant exterior derivative. Attempt: Since the space of gauge potentials is an affine space we may consider the variations $\mathcal A + t\mathcal B$ (family of 1-parameter of gauge potentials) thus $$\begin{aligned}\frac{d}{dt} \Big(\mathcal {YM}(\mathcal A + t \mathcal B)\Big)\Big|_{t= 0} &= \frac{1}{4}\int_{\mathbb R^4} \frac{d}{dt} \Big(\|\mathcal F\|^2\Big)\Big|_{t = 0} d (\bf vol_{\mathbb R^4})\\ &=\frac{1}{2} \int_{\mathbb R^4} \|\mathcal F\| \frac{\left\langle \mathcal F, \frac{d}{dt} (\mathcal F)\Big|_{t =0}\right\rangle}{\|\mathcal F\|} d(\bf vol_{\mathbb R^4})\\&=\frac{1}{2} \int_{\mathbb R^4} \left\langle \mathcal F, \frac{d}{dt} (\mathcal F)\Big|_{t=0}\right\rangle d(\bf vol_{\mathbb R^4})\end{aligned}$$ where $$\begin{aligned} \mathcal F &= \frac{1}{2} \mathcal F_{\alpha \beta} dx^\alpha \wedge dx^\beta \\&= \frac{1}{2}\Bigg( \partial_\alpha (\mathcal A_\beta + t \mathcal B_\beta) - \partial_\beta (\mathcal A_\alpha + t \mathcal B_\alpha) + [\mathcal A_\alpha, \mathcal A_\beta] \\&+ t [\mathcal A_\alpha, \mathcal B_\beta] + t [\mathcal B_\alpha, \mathcal A_\beta] + t^2 [\mathcal B_\alpha, \mathcal B_\beta]\Bigg) dx^\alpha \wedge dx^\beta\end{aligned}$$ by linearity we then obtain $$\frac{d}{dt} (\mathcal F)\Big|_{t = 0} = \Bigg(\partial_\alpha \mathcal B_\beta - \partial_\beta \mathcal B_\alpha + [\mathcal A _\alpha, \mathcal B_\beta] + [\mathcal B_\alpha, \mathcal A_\beta]\Bigg) dx^\alpha \wedge dx^\beta$$ Now $$\begin{aligned}\frac{d}{dt}\Big(\mathcal {YM}(\mathcal A + t \mathcal B)\Big)\Big|_{t = 0} &=\frac{1}{2} \int_{\mathbb R^4} \left\langle \mathcal F, \frac{d}{dt} (\mathcal F)\Big|_{t=0}\right\rangle d(\bf vol_{\mathbb R^4}) \\&= \frac{1}{2}\int_{\mathbb R^4} \mathcal F_{\alpha\beta} (\partial_\alpha \mathcal B_\beta - \partial_\beta \mathcal B_\alpha + [\mathcal A _\alpha, \mathcal B_\beta] + [\mathcal B_\alpha, \mathcal A_\beta])d(\bf vol_{\mathbb R^4}) \end{aligned}$$  Next step should be integration by parts. Then using the fact (Divergence Theorem) $$\int_{\mathbb R^4} \partial_i (f) g dV_g = - \int_{\mathbb R^4} f \partial_i (g) dV_g $$ and that $\mathcal F_{\beta\alpha} = - \mathcal F_{\alpha\beta}$ we have $$\begin{aligned}\delta\mathcal {YM}(A)&=\frac{1}{2} \int_{\mathbb R^4} \left\langle \mathcal F, \frac{d}{dt} (\mathcal F)\Big|_{t=0}\right\rangle d(\bf {vol}_{\mathbb R^4}) \\&= \frac{1}{2}\int_{\mathbb R^4} \mathcal F_{\alpha\beta} (\partial_\alpha \mathcal B_\beta - \partial_\beta \mathcal B_\alpha + [\mathcal A _\alpha, \mathcal B_\beta] + [\mathcal B_\alpha, \mathcal A_\beta])d(\bf {vol}_{\mathbb R^4})\\&= \frac{1}{2}\left(\int_{\mathbb R^4} \mathcal F_{\alpha\beta} (\partial_\alpha \mathcal B_\beta) d(\bf {vol}_{\mathbb R^4}) - \int_{\mathbb R^4}\mathcal F_{\alpha\beta}(\partial_\beta \mathcal B_\alpha) d(\bf {vol}_{\mathbb R^4})\right) \\&+ \frac{1}{2}\left(\int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] + \mathcal F_{\alpha\beta}[\mathcal B_\alpha, \mathcal A_\beta])d(\bf {vol}_{\mathbb R^4}) \right)\\&= -\frac{1}{2}\left(\int_{\mathbb R^4} 2\mathcal B_\beta (\partial_\alpha \mathcal F_{\alpha\beta} d(\bf {vol}_{\mathbb R^4})\right) + \frac{1}{2}\left(\int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] + \mathcal F_{\alpha\beta}[\mathcal B_\alpha, \mathcal A_\beta]d(\bf {vol}_{\mathbb R^4}) \right)\\&=-\int_{\mathbb R^4} \mathcal B_\beta (\partial_\alpha \mathcal F_{\alpha\beta} d(\bf {vol}_{\mathbb R^4}) \\&+ \frac{1}{2}\left(\int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] + \mathcal F_{\alpha\beta}[\mathcal B_\alpha, \mathcal A_\beta])d(\bf {vol}_{\mathbb R^4}) \right)\\&=-\int_{\mathbb R^4} \mathcal B_\beta (\partial_\alpha \mathcal F_{\alpha\beta} d(\bf {vol}_{\mathbb R^4}) + \int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] d(\bf {vol}_{\mathbb R^4})\end{aligned}$$ I need only show that $$\int_{\mathbb R^4} \mathcal F_{\alpha\beta}[\mathcal A_\alpha, \mathcal B_\beta] d(\bf {vol}_{\mathbb R^4}) = - \int_{\mathbb R^4} \mathcal B_\beta [\mathcal A_\alpha,\mathcal F_{\alpha\beta}]d (\bf {vol}_{\mathbb R^4}) $$ I could only think of using Jacobi's identity here to yield the change of $\mathcal F_{\alpha \beta}$ and $\mathcal B_\beta$ $$\begin{aligned} 0 &= [\mathcal F_{\alpha\beta}, [\mathcal A_\alpha, \mathcal B_\beta]] + [\mathcal A_\alpha, [\mathcal B_\beta, \mathcal F_{\alpha\beta}]] + [\mathcal B_\beta , [\mathcal F_{\alpha\beta}, \mathcal A_\alpha]] \\&= [\mathcal F_{\alpha\beta}, [\mathcal A_\alpha, \mathcal B_\beta]] + [\mathcal A_\alpha, [\mathcal B_\beta, \mathcal F_{\alpha\beta}]] - [\mathcal B_\beta , [\mathcal A_\alpha, \mathcal F_{\alpha\beta}]]\end{aligned}$$ But couldn't reach any further. Where do I want to get? $$\begin{aligned}\delta (\mathcal {YM}) (\mathcal A)  = - \int_{\mathbb R^4} \mathcal B_\alpha (\partial_\alpha \mathcal F_{\alpha\beta} + [\mathcal A_\alpha, \mathcal F_{\alpha \beta} ]) d(\bf vol_{\mathbb R^4})\end{aligned}$$ then the stationary points satisfy $$\sum_{\alpha = 1}^4 (\partial_\alpha \mathcal F_{\alpha\beta} + [\mathcal A_\alpha, \mathcal F_{\alpha\beta}]) = 0$$",,"['differential-geometry', 'calculus-of-variations', 'integration-by-parts']"
43,Proportion of cube closer to centre than outside,Proportion of cube closer to centre than outside,,"The question ‘what proportion $p_2$ of a square is closer to the centre than the outside?’ has been asked here before and the answer shown to be $\frac{4\sqrt{2}-5}{3}$. In another question asking the same for an equilateral triangle an answer is given generalizing this to a regular $n$-gon . I once extended the solution method of this answer to computing the proportion $p_3$ of a cube closer to the centre than the outside, by considering a $2\times2\times2$ cube centred at the origin and considering by symmetry only the square pyramid whose base is the side $y=1$ and apex is the origin $(0,0,0)$ and then further restricting this by requiring by symmetry $z>0$ and $x>z$ (the volume under consideration will then be $\frac{1}{48}$th of the cube). The integrand of our problem is then $\frac{1-x^2-z^2}{2}-x$ but the limits of the integration become more complex than the two-dimensional case, and the solution to the problem is now given (unless I’ve made an error) by: $$p_3=6\int_{0}^{\frac{\sqrt{3}-1}{2}}\int_{z}^{\sqrt{2-z^2}-1}\frac{1-x^2-z^2}{2}-x\;dx\;dz$$ where the upper limits of the integrations are found by solving $\frac{1-x^2-z^2}{2}=x$ and then $\sqrt{2-z^2}-1=z$ and the lower limits follow by the definition of the region we are integrating on; the factor of $6$ comes from dividing $48$ by $2^3$ (the volume of the whole cube). I once managed to integrate this by hand to: $$p_3=\frac{5}{4}-\frac{9\sqrt{3}}{8}+\frac{\pi}{4}$$ and Wolfram Alpha agrees giving $p_3\approx0.086841$. However, if we step this up to a 4-cube then the integral becomes correspondingly more complicated and I have no idea how to explicitly compute the resulting integral. I do not know how this method could possibly be extended easily either. My question is this: the problem of what proportion $p_n$ of an $n$-cube is closer to the centre than the periphery seems to be one that might have a simpler or deeper way of solving it (perhaps just because it can be stated so easily); is there an alternative solution method to just doing a brute force integration over the bounding region? If not, is there any way the integration can be transformed to be more tractable so that for instance it can be calculated explicitly for a 4-cube?","The question ‘what proportion $p_2$ of a square is closer to the centre than the outside?’ has been asked here before and the answer shown to be $\frac{4\sqrt{2}-5}{3}$. In another question asking the same for an equilateral triangle an answer is given generalizing this to a regular $n$-gon . I once extended the solution method of this answer to computing the proportion $p_3$ of a cube closer to the centre than the outside, by considering a $2\times2\times2$ cube centred at the origin and considering by symmetry only the square pyramid whose base is the side $y=1$ and apex is the origin $(0,0,0)$ and then further restricting this by requiring by symmetry $z>0$ and $x>z$ (the volume under consideration will then be $\frac{1}{48}$th of the cube). The integrand of our problem is then $\frac{1-x^2-z^2}{2}-x$ but the limits of the integration become more complex than the two-dimensional case, and the solution to the problem is now given (unless I’ve made an error) by: $$p_3=6\int_{0}^{\frac{\sqrt{3}-1}{2}}\int_{z}^{\sqrt{2-z^2}-1}\frac{1-x^2-z^2}{2}-x\;dx\;dz$$ where the upper limits of the integrations are found by solving $\frac{1-x^2-z^2}{2}=x$ and then $\sqrt{2-z^2}-1=z$ and the lower limits follow by the definition of the region we are integrating on; the factor of $6$ comes from dividing $48$ by $2^3$ (the volume of the whole cube). I once managed to integrate this by hand to: $$p_3=\frac{5}{4}-\frac{9\sqrt{3}}{8}+\frac{\pi}{4}$$ and Wolfram Alpha agrees giving $p_3\approx0.086841$. However, if we step this up to a 4-cube then the integral becomes correspondingly more complicated and I have no idea how to explicitly compute the resulting integral. I do not know how this method could possibly be extended easily either. My question is this: the problem of what proportion $p_n$ of an $n$-cube is closer to the centre than the periphery seems to be one that might have a simpler or deeper way of solving it (perhaps just because it can be stated so easily); is there an alternative solution method to just doing a brute force integration over the bounding region? If not, is there any way the integration can be transformed to be more tractable so that for instance it can be calculated explicitly for a 4-cube?",,"['integration', 'geometry', 'definite-integrals', 'volume']"
44,Evaluate $\int_0^1 \frac{\log(1-z)\log(1-z^3)}{z^2}dz$,Evaluate,\int_0^1 \frac{\log(1-z)\log(1-z^3)}{z^2}dz,"Integrals of the kind $$\int_0^1 \frac{\log(1-z)\log(1-z^n)}{z^2}dz$$ where $n\geq 1$ is an integer, arises from a natural way when one apply Möbius inversion to get identities realated to $\zeta(2)$ (see my appendix if there are no mistakes), but how Question. Can you state a closed form for $$\int_0^1 \frac{\log(1-z)\log(1-z^3)}{z^2}dz?$$ I am asking for a detailed explanationabout how get the definite integral. Many thanks. I don't know if the general integral was in the literature but my trials with Wolfram Alpha, seems that is very difficult to get particular cases. See this Example. Other case of the indefinite integral, for example $n=5$ also is provided by Wolfram Alpha online calculator when one type integrate (log(1-x)log(1-x^5))/x^2 dx My idea was multiply both formulas of the first paragraph in page 77 from: Benito, Navas, Varona, Möbius inversion from the point of view of flows , Proceedings of the Segundas Jornadas de Teoría de Números, Biblioteca de la Revista Matemática Iberoamericana (2008), then integrating one gets $$\zeta(2)=\int_0^1\sum_{n=1}^\infty\frac{z^{n-1}}{n}dz=\int_0^1 \left(\frac{\log(1-z)}{z^2}\sum_{n=1}^\infty\frac{\mu(n)}{n}\log(1-z^n)\right)dz.$$ Here as you see $\zeta(s)$ is the Riemann's Zeta function and $\mu(n)$ is the Möbius function. And now from there one more time by absolute convergence, I've asked myself what's about these coefficients $$a_n=\int_0^1 \frac{\log(1-z)\log(1-z^n)}{z^2}dz,$$ that for integers $n\geq 1$ satisfy $$\zeta(2)=\sum_{n=1}^\infty\frac{\mu(n)}{n}a_n.$$","Integrals of the kind where is an integer, arises from a natural way when one apply Möbius inversion to get identities realated to (see my appendix if there are no mistakes), but how Question. Can you state a closed form for I am asking for a detailed explanationabout how get the definite integral. Many thanks. I don't know if the general integral was in the literature but my trials with Wolfram Alpha, seems that is very difficult to get particular cases. See this Example. Other case of the indefinite integral, for example also is provided by Wolfram Alpha online calculator when one type integrate (log(1-x)log(1-x^5))/x^2 dx My idea was multiply both formulas of the first paragraph in page 77 from: Benito, Navas, Varona, Möbius inversion from the point of view of flows , Proceedings of the Segundas Jornadas de Teoría de Números, Biblioteca de la Revista Matemática Iberoamericana (2008), then integrating one gets Here as you see is the Riemann's Zeta function and is the Möbius function. And now from there one more time by absolute convergence, I've asked myself what's about these coefficients that for integers satisfy","\int_0^1 \frac{\log(1-z)\log(1-z^n)}{z^2}dz n\geq 1 \zeta(2) \int_0^1 \frac{\log(1-z)\log(1-z^3)}{z^2}dz? n=5 \zeta(2)=\int_0^1\sum_{n=1}^\infty\frac{z^{n-1}}{n}dz=\int_0^1 \left(\frac{\log(1-z)}{z^2}\sum_{n=1}^\infty\frac{\mu(n)}{n}\log(1-z^n)\right)dz. \zeta(s) \mu(n) a_n=\int_0^1 \frac{\log(1-z)\log(1-z^n)}{z^2}dz, n\geq 1 \zeta(2)=\sum_{n=1}^\infty\frac{\mu(n)}{n}a_n.","['integration', 'definite-integrals']"
45,Fourier transform of a cosine,Fourier transform of a cosine,,"I'm trying to wrap my head around the following. I understand the steps that lead from $$x(y)=A \cos(2 \pi f_0 t)$$ to  $$X(y)=A \frac{\delta(f-f_0)+\delta(f+f_0)}{2}$$ $f_0$ is the carrier frequency. The problem I have is that I would expect the integration of cosine from -$\infty$ to $\infty$ to ""blow"".","I'm trying to wrap my head around the following. I understand the steps that lead from $$x(y)=A \cos(2 \pi f_0 t)$$ to  $$X(y)=A \frac{\delta(f-f_0)+\delta(f+f_0)}{2}$$ $f_0$ is the carrier frequency. The problem I have is that I would expect the integration of cosine from -$\infty$ to $\infty$ to ""blow"".",,"['integration', 'fourier-transform']"
46,"At what value on the interval $(0, 3\pi/2)$ does $f(x) = \int_{x}^{2x} \frac{\sin(t)}{t} \, dt $ attain a local maximum?",At what value on the interval  does  attain a local maximum?,"(0, 3\pi/2) f(x) = \int_{x}^{2x} \frac{\sin(t)}{t} \, dt ","I had a question in regards to the following question I encountered on a GRE practice exam. The question is as follows: The following function is defined for all positive $x$: $$f(x) = \int_{x}^{2x} \frac{\sin(t)}{t} \, dt $$ At what value on the interval $(0, 3\pi/2)$ does the function attain a local maximum?    A.) $\pi/6$ B.) $\pi/3$ C.) $\pi/2$ D.) $\pi$ E.) $2\pi/3$ I arrived at the correct answer B, but did it slightly differently than the solution manual did and wanted some clarification. So I calculated $$f'(x) = \frac{\sin(2x) - \sin(x)}{x},$$ set the numerator equal to 0 (since the denominator being equal to zero is outside the domain) and arrived at $\sin(2x) = \sin(x)$. At this point I ruled out A, C, and E since $\sin(2x) \neq \sin(x)$ for those $x$-values and then computed the second derivative and determined that $f''(\pi/3) < 0$ and $f''(\pi)>0$; so the answer is B. The solution manual used a trig identity I did not know off the top of my head and said that $\sin(2x) - \sin(x) = 0$ implies that $\sin(x)(2 \cos(x) - 1) = 0$ and so $\sin(x) = 0$ or $\cos(x) = 1/2$, which implies that $x = \pi/3$ because $0 < x < 3\pi/2$. Why were they able to rule out the solution $x = \pi$ since after all $\sin(\pi) = 0$ and $0 < \pi < 3\pi/2$?","I had a question in regards to the following question I encountered on a GRE practice exam. The question is as follows: The following function is defined for all positive $x$: $$f(x) = \int_{x}^{2x} \frac{\sin(t)}{t} \, dt $$ At what value on the interval $(0, 3\pi/2)$ does the function attain a local maximum?    A.) $\pi/6$ B.) $\pi/3$ C.) $\pi/2$ D.) $\pi$ E.) $2\pi/3$ I arrived at the correct answer B, but did it slightly differently than the solution manual did and wanted some clarification. So I calculated $$f'(x) = \frac{\sin(2x) - \sin(x)}{x},$$ set the numerator equal to 0 (since the denominator being equal to zero is outside the domain) and arrived at $\sin(2x) = \sin(x)$. At this point I ruled out A, C, and E since $\sin(2x) \neq \sin(x)$ for those $x$-values and then computed the second derivative and determined that $f''(\pi/3) < 0$ and $f''(\pi)>0$; so the answer is B. The solution manual used a trig identity I did not know off the top of my head and said that $\sin(2x) - \sin(x) = 0$ implies that $\sin(x)(2 \cos(x) - 1) = 0$ and so $\sin(x) = 0$ or $\cos(x) = 1/2$, which implies that $x = \pi/3$ because $0 < x < 3\pi/2$. Why were they able to rule out the solution $x = \pi$ since after all $\sin(\pi) = 0$ and $0 < \pi < 3\pi/2$?",,"['calculus', 'integration', 'definite-integrals', 'maxima-minima']"
47,Integral of product of two inverse regularized incomplete beta functions,Integral of product of two inverse regularized incomplete beta functions,,"I want to compute the integral of the product of two inverse regularized incomplete beta functions over $[0,1]$ in closed form, that is, to evaluate   $$ J = \int_0^1 I_t^{-1}(a_1,b_1) \: I_t^{-1}(a_2,b_2) \: \mathrm{d}t $$   in terms of parameters $a_1, b_1, a_2, b_2 > 0$. Let us first collect few useful facts: (Indefinite integral) $\displaystyle\int I_{t}^{-1}(a,b)\:\mathrm{d}t = \frac{\left(I_t^{-1}(a,b)\right)^{a+1}}{(a+1)\:B(a,b)}\:_2 F_1\left(a+1, 1-b; a+2; I_t^{-1}(a,b)\right) + \text{constant}$. (Derivative) $\displaystyle\frac{\mathrm{d}}{\mathrm{d}t}I_t^{-1}(a,b) = B(a,b)\:\left(I_t^{-1}(a,b)\right)^{1-a}\:\left(1-I_t^{-1}(a,b)\right)^{1-b}$. (Boundary evaluation) $I_0^{-1}(a,b) = 0, \quad I_1^{-1}(a,b)=1$. ( Gauss's Hypergeometric theorem ) $_2 F_{1}(p,q;r;1) = \dfrac{\Gamma(r) \:\Gamma(r-p-q)}{\Gamma(r-p)\:\Gamma(r-q)}$. (Complete Beta and Gamma functions) $B(a,b) = \dfrac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$. Integrating-by-parts the original integral $J$ by taking $I_t^{-1}(a_1,b_1)$ as the first function, and $I_t^{-1}(a_2,b_2)$ as the second, we are led to $J = J_1 - J_2$, with a simple expression for \begin{align} J_1 & = \left. \left[I_t^{-1}(a_1,b_1) \int I_t^{-1}(a_2,b_2) \: \mathrm{d}t \right] \right|_{t=0}^{t=1} \\[10pt] & = \frac{a_2}{a_2 + b_2}, \quad \text{(using facts 1,3,4,5, and } \Gamma(z+1) = z\Gamma(z)), \end{align} and a complicated-looking expression for \begin{align} J_2 & = \int_0^1 \left(\frac{\mathrm{d}}{\mathrm{d}t}I_t^{-1}(a_1,b_1) \int I_t^{-1}(a_2,b_2)\:\mathrm{d}t\right)\:\mathrm{d}t \\[10pt] & = \frac{B(a_1,b_1)}{(a_2+1) B(a_2,b_2)} \int_0^1 \left(I_t^{-1}(a_1, b_1)\right)^{1-a_1} \left(1 - I_t^{-1}(a_1,b_1)\right)^{1-b_1} \left(I_t^{-1}(a_2,b_2)\right)^{a_2+1}\:_2 F_1\left(a_2+1,1-b_2;a_2+2;I_t^{-1}(a_2,b_2)\right)\mathrm{d}t, \quad\text{(using facts 1,2)} \end{align} Can $J_2$ be further simplified? Or perhaps there is a more elegant way to compute $J$ than my naive attempt via integration-by-parts? Any help, suggestions or ideas are welcome.","I want to compute the integral of the product of two inverse regularized incomplete beta functions over $[0,1]$ in closed form, that is, to evaluate   $$ J = \int_0^1 I_t^{-1}(a_1,b_1) \: I_t^{-1}(a_2,b_2) \: \mathrm{d}t $$   in terms of parameters $a_1, b_1, a_2, b_2 > 0$. Let us first collect few useful facts: (Indefinite integral) $\displaystyle\int I_{t}^{-1}(a,b)\:\mathrm{d}t = \frac{\left(I_t^{-1}(a,b)\right)^{a+1}}{(a+1)\:B(a,b)}\:_2 F_1\left(a+1, 1-b; a+2; I_t^{-1}(a,b)\right) + \text{constant}$. (Derivative) $\displaystyle\frac{\mathrm{d}}{\mathrm{d}t}I_t^{-1}(a,b) = B(a,b)\:\left(I_t^{-1}(a,b)\right)^{1-a}\:\left(1-I_t^{-1}(a,b)\right)^{1-b}$. (Boundary evaluation) $I_0^{-1}(a,b) = 0, \quad I_1^{-1}(a,b)=1$. ( Gauss's Hypergeometric theorem ) $_2 F_{1}(p,q;r;1) = \dfrac{\Gamma(r) \:\Gamma(r-p-q)}{\Gamma(r-p)\:\Gamma(r-q)}$. (Complete Beta and Gamma functions) $B(a,b) = \dfrac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$. Integrating-by-parts the original integral $J$ by taking $I_t^{-1}(a_1,b_1)$ as the first function, and $I_t^{-1}(a_2,b_2)$ as the second, we are led to $J = J_1 - J_2$, with a simple expression for \begin{align} J_1 & = \left. \left[I_t^{-1}(a_1,b_1) \int I_t^{-1}(a_2,b_2) \: \mathrm{d}t \right] \right|_{t=0}^{t=1} \\[10pt] & = \frac{a_2}{a_2 + b_2}, \quad \text{(using facts 1,3,4,5, and } \Gamma(z+1) = z\Gamma(z)), \end{align} and a complicated-looking expression for \begin{align} J_2 & = \int_0^1 \left(\frac{\mathrm{d}}{\mathrm{d}t}I_t^{-1}(a_1,b_1) \int I_t^{-1}(a_2,b_2)\:\mathrm{d}t\right)\:\mathrm{d}t \\[10pt] & = \frac{B(a_1,b_1)}{(a_2+1) B(a_2,b_2)} \int_0^1 \left(I_t^{-1}(a_1, b_1)\right)^{1-a_1} \left(1 - I_t^{-1}(a_1,b_1)\right)^{1-b_1} \left(I_t^{-1}(a_2,b_2)\right)^{a_2+1}\:_2 F_1\left(a_2+1,1-b_2;a_2+2;I_t^{-1}(a_2,b_2)\right)\mathrm{d}t, \quad\text{(using facts 1,2)} \end{align} Can $J_2$ be further simplified? Or perhaps there is a more elegant way to compute $J$ than my naive attempt via integration-by-parts? Any help, suggestions or ideas are welcome.",,"['integration', 'definite-integrals', 'special-functions', 'hypergeometric-function', 'beta-function']"
48,Verifying closed form evaluation of an Ising-class multiple integral,Verifying closed form evaluation of an Ising-class multiple integral,,"For $n\in\mathbb{N}\land n\ge2$, define the so-called Ising-class integral of the third kind, $E_{n}$, via the sequence of $\left(n-1\right)$-dimensional integrals   $$E_{n}:=2\int_{\left[0,1\right]^{n-1}}\prod_{\ell=2}^{n}\mathrm{d}t_{\ell}\,\left(\prod_{1\le j<k\le n}\frac{u_{k}-u_{j}}{u_{k}+u_{j}}\right)^{2},\tag{1}$$   where $u_{k}:=\prod_{i=1}^{k}t_{i}$. In a paper by David Bailey (see article 96 on this list ), it is stated that while closed form expressions are known only for the $2\le n\le4$ cases, a conjectured value for the $n=5$ case has been obtained experimentally: $$\begin{align} E_{5} &\stackrel{?}{=}42-40\ln{\left(2\right)}+464\ln^{2}{\left(2\right)}-124\,\zeta{\left(2\right)}\\ &~~~~~+80\ln{\left(2\right)}\,\zeta{\left(2\right)}-74\,\zeta{\left(3\right)}\\ &~~~~~+88\ln^{4}{\left(2\right)}+240\ln^{2}{\left(2\right)}\,\zeta{\left(2\right)}-1272\ln{\left(2\right)}\,\zeta{\left(3\right)}\\ &~~~~~+1701\,\zeta{\left(4\right)}-1984\operatorname{Li}_{4}{\left(\frac12\right)}\\ &\approx0.003493653711729521740688067279184251569632944955.\tag{2}\\ \end{align}$$ The conjectured value for $E_{5}$ given above has been found to agree numerically with the value of the integral $(1)$ out to at least $240$ digits. Question: Can a closed form expression for $E_{5}$ be found? It would suffice to prove that $(2)$ is correct. Note: To avoid cumbersome subscripts, rename the variables as $t_{2}=:t\land t_{3}=:u\land t_{4}=:v\land t_{5}=:w$. Then, we obtain the alternative expression of $E_{5}$ as the iterated integral $$\begin{align} E_{5} &=2\int_{0}^{1}\mathrm{d}t\int_{0}^{1}\mathrm{d}u\int_{0}^{1}\mathrm{d}v\int_{0}^{1}\mathrm{d}w\,\left(\frac{1-t}{1+t}\right)^{2}\left(\frac{1-tu}{1+tu}\right)^{2}\\ &~~~~~\times\left(\frac{1-u}{1+u}\right)^{2}\left(\frac{1-tuv}{1+tuv}\right)^{2}\left(\frac{1-uv}{1+uv}\right)^{2}\left(\frac{1-v}{1+v}\right)^{2}\\ &~~~~~\times\left(\frac{1-tuvw}{1+tuvw}\right)^{2}\left(\frac{1-uvw}{1+uvw}\right)^{2}\left(\frac{1-vw}{1+vw}\right)^{2}\left(\frac{1-w}{1+w}\right)^{2}.\tag{3}\\ \end{align}$$ Proof : $$\begin{align} E_{5} &=2\int_{\left[0,1\right]^{4}}\prod_{\ell=2}^{5}\mathrm{d}t_{\ell}\,\left(\prod_{1\le j<k\le5}\frac{u_{k}-u_{j}}{u_{k}+u_{j}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\prod_{1\le j<k\le5}\frac{u_{k}-u_{j}}{u_{k}+u_{j}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\prod_{1\le j<k\le4}\frac{u_{k}-u_{j}}{u_{k}+u_{j}}\right)^{2}\left(\prod_{j=1}^{4}\frac{u_{5}-u_{j}}{u_{5}+u_{j}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\prod_{1\le j<k\le3}\frac{u_{k}-u_{j}}{u_{k}+u_{j}}\right)^{2}\left(\prod_{j=1}^{3}\frac{u_{4}-u_{j}}{u_{4}+u_{j}}\right)^{2}\\ &~~~~~\times\left(\prod_{j=1}^{4}\frac{u_{5}-u_{j}}{u_{5}+u_{j}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\prod_{1\le j<k\le2}\frac{u_{k}-u_{j}}{u_{k}+u_{j}}\right)^{2}\left(\prod_{j=1}^{2}\frac{u_{3}-u_{j}}{u_{3}+u_{j}}\right)^{2}\\ &~~~~~\times\left(\prod_{j=1}^{3}\frac{u_{4}-u_{j}}{u_{4}+u_{j}}\right)^{2}\left(\prod_{j=1}^{4}\frac{u_{5}-u_{j}}{u_{5}+u_{j}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\frac{u_{2}-u_{1}}{u_{2}+u_{1}}\right)^{2}\left(\frac{u_{3}-u_{1}}{u_{3}+u_{1}}\right)^{2}\left(\frac{u_{3}-u_{2}}{u_{3}+u_{2}}\right)^{2}\\ &~~~~~\times\left(\frac{u_{4}-u_{1}}{u_{4}+u_{1}}\right)^{2}\left(\frac{u_{4}-u_{2}}{u_{4}+u_{2}}\right)^{2}\left(\frac{u_{4}-u_{3}}{u_{4}+u_{3}}\right)^{2}\\ &~~~~~\times\left(\frac{u_{5}-u_{1}}{u_{5}+u_{1}}\right)^{2}\left(\frac{u_{5}-u_{2}}{u_{5}+u_{2}}\right)^{2}\left(\frac{u_{5}-u_{3}}{u_{5}+u_{3}}\right)^{2}\left(\frac{u_{5}-u_{4}}{u_{5}+u_{4}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\frac{t_{1}t_{2}-t_{1}}{t_{1}t_{2}+t_{1}}\right)^{2}\left(\frac{t_{1}t_{2}t_{3}-t_{1}}{t_{1}t_{2}t_{3}+t_{1}}\right)^{2}\left(\frac{t_{1}t_{2}t_{3}-t_{1}t_{2}}{t_{1}t_{2}t_{3}+t_{1}t_{2}}\right)^{2}\\ &~~~~~\times\left(\frac{t_{1}t_{2}t_{3}t_{4}-t_{1}}{t_{1}t_{2}t_{3}t_{4}+t_{1}}\right)^{2}\left(\frac{t_{1}t_{2}t_{3}t_{4}-t_{1}t_{2}}{t_{1}t_{2}t_{3}t_{4}+t_{1}t_{2}}\right)^{2}\left(\frac{t_{1}t_{2}t_{3}t_{4}-t_{1}t_{2}t_{3}}{t_{1}t_{2}t_{3}t_{4}+t_{1}t_{2}t_{3}}\right)^{2}\\ &~~~~~\times\left(\frac{t_{1}t_{2}t_{3}t_{4}t_{5}-t_{1}}{t_{1}t_{2}t_{3}t_{4}t_{5}+t_{1}}\right)^{2}\left(\frac{t_{1}t_{2}t_{3}t_{4}t_{5}-t_{1}t_{2}}{t_{1}t_{2}t_{3}t_{4}t_{5}+t_{1}t_{2}}\right)^{2}\left(\frac{t_{1}t_{2}t_{3}t_{4}t_{5}-t_{1}t_{2}t_{3}}{t_{1}t_{2}t_{3}t_{4}t_{5}+t_{1}t_{2}t_{3}}\right)^{2}\\ &~~~~~\times\left(\frac{t_{1}t_{2}t_{3}t_{4}t_{5}-t_{1}t_{2}t_{3}t_{4}}{t_{1}t_{2}t_{3}t_{4}t_{5}+t_{1}t_{2}t_{3}t_{4}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\frac{t_{2}-1}{t_{2}+1}\right)^{2}\left(\frac{t_{2}t_{3}-1}{t_{2}t_{3}+1}\right)^{2}\left(\frac{t_{3}-1}{t_{3}+1}\right)^{2}\\ &~~~~~\times\left(\frac{t_{2}t_{3}t_{4}-1}{t_{2}t_{3}t_{4}+1}\right)^{2}\left(\frac{t_{3}t_{4}-1}{t_{3}t_{4}+1}\right)^{2}\left(\frac{t_{4}-1}{t_{4}+1}\right)^{2}\\ &~~~~~\times\left(\frac{t_{2}t_{3}t_{4}t_{5}-1}{t_{2}t_{3}t_{4}t_{5}+1}\right)^{2}\left(\frac{t_{3}t_{4}t_{5}-1}{t_{3}t_{4}t_{5}+1}\right)^{2}\left(\frac{t_{4}t_{5}-1}{t_{4}t_{5}+1}\right)^{2}\left(\frac{t_{5}-1}{t_{5}+1}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t\,\mathrm{d}u\,\mathrm{d}v\,\mathrm{d}w\,\left(\frac{t-1}{t+1}\right)^{2}\left(\frac{tu-1}{tu+1}\right)^{2}\left(\frac{u-1}{u+1}\right)^{2}\\ &~~~~~\times\left(\frac{tuv-1}{tuv+1}\right)^{2}\left(\frac{uv-1}{uv+1}\right)^{2}\left(\frac{v-1}{v+1}\right)^{2}\\ &~~~~~\times\left(\frac{tuvw-1}{tuvw+1}\right)^{2}\left(\frac{uvw-1}{uvw+1}\right)^{2}\left(\frac{vw-1}{vw+1}\right)^{2}\left(\frac{w-1}{w+1}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t\,\mathrm{d}u\,\mathrm{d}v\,\mathrm{d}w\,\left(\frac{1-t}{1+t}\right)^{2}\left(\frac{1-tu}{1+tu}\right)^{2}\left(\frac{1-u}{1+u}\right)^{2}\\ &~~~~~\times\left(\frac{1-tuv}{1+tuv}\right)^{2}\left(\frac{1-uv}{1+uv}\right)^{2}\left(\frac{1-v}{1+v}\right)^{2}\\ &~~~~~\times\left(\frac{1-tuvw}{1+tuvw}\right)^{2}\left(\frac{1-uvw}{1+uvw}\right)^{2}\left(\frac{1-vw}{1+vw}\right)^{2}\left(\frac{1-w}{1+w}\right)^{2}\\ &=2\int_{0}^{1}\mathrm{d}t\int_{0}^{1}\mathrm{d}u\int_{0}^{1}\mathrm{d}v\int_{0}^{1}\mathrm{d}w\,\left(\frac{1-t}{1+t}\right)^{2}\left(\frac{1-tu}{1+tu}\right)^{2}\\ &~~~~~\times\left(\frac{1-u}{1+u}\right)^{2}\left(\frac{1-tuv}{1+tuv}\right)^{2}\left(\frac{1-uv}{1+uv}\right)^{2}\left(\frac{1-v}{1+v}\right)^{2}\\ &~~~~~\times\left(\frac{1-tuvw}{1+tuvw}\right)^{2}\left(\frac{1-uvw}{1+uvw}\right)^{2}\left(\frac{1-vw}{1+vw}\right)^{2}\left(\frac{1-w}{1+w}\right)^{2}.\blacksquare\\ \end{align}$$","For $n\in\mathbb{N}\land n\ge2$, define the so-called Ising-class integral of the third kind, $E_{n}$, via the sequence of $\left(n-1\right)$-dimensional integrals   $$E_{n}:=2\int_{\left[0,1\right]^{n-1}}\prod_{\ell=2}^{n}\mathrm{d}t_{\ell}\,\left(\prod_{1\le j<k\le n}\frac{u_{k}-u_{j}}{u_{k}+u_{j}}\right)^{2},\tag{1}$$   where $u_{k}:=\prod_{i=1}^{k}t_{i}$. In a paper by David Bailey (see article 96 on this list ), it is stated that while closed form expressions are known only for the $2\le n\le4$ cases, a conjectured value for the $n=5$ case has been obtained experimentally: $$\begin{align} E_{5} &\stackrel{?}{=}42-40\ln{\left(2\right)}+464\ln^{2}{\left(2\right)}-124\,\zeta{\left(2\right)}\\ &~~~~~+80\ln{\left(2\right)}\,\zeta{\left(2\right)}-74\,\zeta{\left(3\right)}\\ &~~~~~+88\ln^{4}{\left(2\right)}+240\ln^{2}{\left(2\right)}\,\zeta{\left(2\right)}-1272\ln{\left(2\right)}\,\zeta{\left(3\right)}\\ &~~~~~+1701\,\zeta{\left(4\right)}-1984\operatorname{Li}_{4}{\left(\frac12\right)}\\ &\approx0.003493653711729521740688067279184251569632944955.\tag{2}\\ \end{align}$$ The conjectured value for $E_{5}$ given above has been found to agree numerically with the value of the integral $(1)$ out to at least $240$ digits. Question: Can a closed form expression for $E_{5}$ be found? It would suffice to prove that $(2)$ is correct. Note: To avoid cumbersome subscripts, rename the variables as $t_{2}=:t\land t_{3}=:u\land t_{4}=:v\land t_{5}=:w$. Then, we obtain the alternative expression of $E_{5}$ as the iterated integral $$\begin{align} E_{5} &=2\int_{0}^{1}\mathrm{d}t\int_{0}^{1}\mathrm{d}u\int_{0}^{1}\mathrm{d}v\int_{0}^{1}\mathrm{d}w\,\left(\frac{1-t}{1+t}\right)^{2}\left(\frac{1-tu}{1+tu}\right)^{2}\\ &~~~~~\times\left(\frac{1-u}{1+u}\right)^{2}\left(\frac{1-tuv}{1+tuv}\right)^{2}\left(\frac{1-uv}{1+uv}\right)^{2}\left(\frac{1-v}{1+v}\right)^{2}\\ &~~~~~\times\left(\frac{1-tuvw}{1+tuvw}\right)^{2}\left(\frac{1-uvw}{1+uvw}\right)^{2}\left(\frac{1-vw}{1+vw}\right)^{2}\left(\frac{1-w}{1+w}\right)^{2}.\tag{3}\\ \end{align}$$ Proof : $$\begin{align} E_{5} &=2\int_{\left[0,1\right]^{4}}\prod_{\ell=2}^{5}\mathrm{d}t_{\ell}\,\left(\prod_{1\le j<k\le5}\frac{u_{k}-u_{j}}{u_{k}+u_{j}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\prod_{1\le j<k\le5}\frac{u_{k}-u_{j}}{u_{k}+u_{j}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\prod_{1\le j<k\le4}\frac{u_{k}-u_{j}}{u_{k}+u_{j}}\right)^{2}\left(\prod_{j=1}^{4}\frac{u_{5}-u_{j}}{u_{5}+u_{j}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\prod_{1\le j<k\le3}\frac{u_{k}-u_{j}}{u_{k}+u_{j}}\right)^{2}\left(\prod_{j=1}^{3}\frac{u_{4}-u_{j}}{u_{4}+u_{j}}\right)^{2}\\ &~~~~~\times\left(\prod_{j=1}^{4}\frac{u_{5}-u_{j}}{u_{5}+u_{j}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\prod_{1\le j<k\le2}\frac{u_{k}-u_{j}}{u_{k}+u_{j}}\right)^{2}\left(\prod_{j=1}^{2}\frac{u_{3}-u_{j}}{u_{3}+u_{j}}\right)^{2}\\ &~~~~~\times\left(\prod_{j=1}^{3}\frac{u_{4}-u_{j}}{u_{4}+u_{j}}\right)^{2}\left(\prod_{j=1}^{4}\frac{u_{5}-u_{j}}{u_{5}+u_{j}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\frac{u_{2}-u_{1}}{u_{2}+u_{1}}\right)^{2}\left(\frac{u_{3}-u_{1}}{u_{3}+u_{1}}\right)^{2}\left(\frac{u_{3}-u_{2}}{u_{3}+u_{2}}\right)^{2}\\ &~~~~~\times\left(\frac{u_{4}-u_{1}}{u_{4}+u_{1}}\right)^{2}\left(\frac{u_{4}-u_{2}}{u_{4}+u_{2}}\right)^{2}\left(\frac{u_{4}-u_{3}}{u_{4}+u_{3}}\right)^{2}\\ &~~~~~\times\left(\frac{u_{5}-u_{1}}{u_{5}+u_{1}}\right)^{2}\left(\frac{u_{5}-u_{2}}{u_{5}+u_{2}}\right)^{2}\left(\frac{u_{5}-u_{3}}{u_{5}+u_{3}}\right)^{2}\left(\frac{u_{5}-u_{4}}{u_{5}+u_{4}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\frac{t_{1}t_{2}-t_{1}}{t_{1}t_{2}+t_{1}}\right)^{2}\left(\frac{t_{1}t_{2}t_{3}-t_{1}}{t_{1}t_{2}t_{3}+t_{1}}\right)^{2}\left(\frac{t_{1}t_{2}t_{3}-t_{1}t_{2}}{t_{1}t_{2}t_{3}+t_{1}t_{2}}\right)^{2}\\ &~~~~~\times\left(\frac{t_{1}t_{2}t_{3}t_{4}-t_{1}}{t_{1}t_{2}t_{3}t_{4}+t_{1}}\right)^{2}\left(\frac{t_{1}t_{2}t_{3}t_{4}-t_{1}t_{2}}{t_{1}t_{2}t_{3}t_{4}+t_{1}t_{2}}\right)^{2}\left(\frac{t_{1}t_{2}t_{3}t_{4}-t_{1}t_{2}t_{3}}{t_{1}t_{2}t_{3}t_{4}+t_{1}t_{2}t_{3}}\right)^{2}\\ &~~~~~\times\left(\frac{t_{1}t_{2}t_{3}t_{4}t_{5}-t_{1}}{t_{1}t_{2}t_{3}t_{4}t_{5}+t_{1}}\right)^{2}\left(\frac{t_{1}t_{2}t_{3}t_{4}t_{5}-t_{1}t_{2}}{t_{1}t_{2}t_{3}t_{4}t_{5}+t_{1}t_{2}}\right)^{2}\left(\frac{t_{1}t_{2}t_{3}t_{4}t_{5}-t_{1}t_{2}t_{3}}{t_{1}t_{2}t_{3}t_{4}t_{5}+t_{1}t_{2}t_{3}}\right)^{2}\\ &~~~~~\times\left(\frac{t_{1}t_{2}t_{3}t_{4}t_{5}-t_{1}t_{2}t_{3}t_{4}}{t_{1}t_{2}t_{3}t_{4}t_{5}+t_{1}t_{2}t_{3}t_{4}}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t_{2}\,\mathrm{d}t_{3}\,\mathrm{d}t_{4}\,\mathrm{d}t_{5}\,\left(\frac{t_{2}-1}{t_{2}+1}\right)^{2}\left(\frac{t_{2}t_{3}-1}{t_{2}t_{3}+1}\right)^{2}\left(\frac{t_{3}-1}{t_{3}+1}\right)^{2}\\ &~~~~~\times\left(\frac{t_{2}t_{3}t_{4}-1}{t_{2}t_{3}t_{4}+1}\right)^{2}\left(\frac{t_{3}t_{4}-1}{t_{3}t_{4}+1}\right)^{2}\left(\frac{t_{4}-1}{t_{4}+1}\right)^{2}\\ &~~~~~\times\left(\frac{t_{2}t_{3}t_{4}t_{5}-1}{t_{2}t_{3}t_{4}t_{5}+1}\right)^{2}\left(\frac{t_{3}t_{4}t_{5}-1}{t_{3}t_{4}t_{5}+1}\right)^{2}\left(\frac{t_{4}t_{5}-1}{t_{4}t_{5}+1}\right)^{2}\left(\frac{t_{5}-1}{t_{5}+1}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t\,\mathrm{d}u\,\mathrm{d}v\,\mathrm{d}w\,\left(\frac{t-1}{t+1}\right)^{2}\left(\frac{tu-1}{tu+1}\right)^{2}\left(\frac{u-1}{u+1}\right)^{2}\\ &~~~~~\times\left(\frac{tuv-1}{tuv+1}\right)^{2}\left(\frac{uv-1}{uv+1}\right)^{2}\left(\frac{v-1}{v+1}\right)^{2}\\ &~~~~~\times\left(\frac{tuvw-1}{tuvw+1}\right)^{2}\left(\frac{uvw-1}{uvw+1}\right)^{2}\left(\frac{vw-1}{vw+1}\right)^{2}\left(\frac{w-1}{w+1}\right)^{2}\\ &=2\int_{\left[0,1\right]^{4}}\mathrm{d}t\,\mathrm{d}u\,\mathrm{d}v\,\mathrm{d}w\,\left(\frac{1-t}{1+t}\right)^{2}\left(\frac{1-tu}{1+tu}\right)^{2}\left(\frac{1-u}{1+u}\right)^{2}\\ &~~~~~\times\left(\frac{1-tuv}{1+tuv}\right)^{2}\left(\frac{1-uv}{1+uv}\right)^{2}\left(\frac{1-v}{1+v}\right)^{2}\\ &~~~~~\times\left(\frac{1-tuvw}{1+tuvw}\right)^{2}\left(\frac{1-uvw}{1+uvw}\right)^{2}\left(\frac{1-vw}{1+vw}\right)^{2}\left(\frac{1-w}{1+w}\right)^{2}\\ &=2\int_{0}^{1}\mathrm{d}t\int_{0}^{1}\mathrm{d}u\int_{0}^{1}\mathrm{d}v\int_{0}^{1}\mathrm{d}w\,\left(\frac{1-t}{1+t}\right)^{2}\left(\frac{1-tu}{1+tu}\right)^{2}\\ &~~~~~\times\left(\frac{1-u}{1+u}\right)^{2}\left(\frac{1-tuv}{1+tuv}\right)^{2}\left(\frac{1-uv}{1+uv}\right)^{2}\left(\frac{1-v}{1+v}\right)^{2}\\ &~~~~~\times\left(\frac{1-tuvw}{1+tuvw}\right)^{2}\left(\frac{1-uvw}{1+uvw}\right)^{2}\left(\frac{1-vw}{1+vw}\right)^{2}\left(\frac{1-w}{1+w}\right)^{2}.\blacksquare\\ \end{align}$$",,"['integration', 'definite-integrals', 'special-functions', 'closed-form', 'polylogarithm']"
49,Prove that $f(z)=\frac{1}{2\pi}\int_0^{2\pi} f(Re^{i\phi})Re(\frac{Re^{i\phi}+z}{Re^{i\phi}-z}) d\phi$,Prove that,f(z)=\frac{1}{2\pi}\int_0^{2\pi} f(Re^{i\phi})Re(\frac{Re^{i\phi}+z}{Re^{i\phi}-z}) d\phi,"Suppose $f$ is holomorphic on disc $D_{R_0}$ centred at the origin and    of radius $R_0$. $0<R<R_0$, $z\in D_R$. Prove that $f(z)=\dfrac{1}{2\pi}\int_0^{2\pi}  f(Re^{i\phi})Re(\dfrac{Re^{i\phi}+z}{Re^{i\phi}-z}) d\phi$. The hint given in the book is : if $w=\dfrac{R^2}{\bar z}$, then the integral $\dfrac{f(\zeta)}{\zeta-w}$   around the circle of radius $R$ centered ar the origin is zero. I used the hint to got $f(z)=\dfrac{1}{2\pi} \displaystyle \int_0^{2\pi}f(Re^{i\phi})\Big(\dfrac{Re^{i\phi}}{Re^{i\phi}-z}+\dfrac{Re^{i\phi}}{Re^{i\phi}-\frac{R^2}{\bar z}} \Big)d\phi$, but I do not know how to make that ugly thing in the bracket to the desired form. Could you please give me some hints?","Suppose $f$ is holomorphic on disc $D_{R_0}$ centred at the origin and    of radius $R_0$. $0<R<R_0$, $z\in D_R$. Prove that $f(z)=\dfrac{1}{2\pi}\int_0^{2\pi}  f(Re^{i\phi})Re(\dfrac{Re^{i\phi}+z}{Re^{i\phi}-z}) d\phi$. The hint given in the book is : if $w=\dfrac{R^2}{\bar z}$, then the integral $\dfrac{f(\zeta)}{\zeta-w}$   around the circle of radius $R$ centered ar the origin is zero. I used the hint to got $f(z)=\dfrac{1}{2\pi} \displaystyle \int_0^{2\pi}f(Re^{i\phi})\Big(\dfrac{Re^{i\phi}}{Re^{i\phi}-z}+\dfrac{Re^{i\phi}}{Re^{i\phi}-\frac{R^2}{\bar z}} \Big)d\phi$, but I do not know how to make that ugly thing in the bracket to the desired form. Could you please give me some hints?",,"['integration', 'complex-analysis']"
50,Integration and little o notation,Integration and little o notation,,"Let $f,g : [0, \infty) \rightarrow (0, \infty)$ such that $$f (x) = o(g(x))$$  that is, $ \lim_{x \rightarrow \infty} \frac{f(x)}{g(x)} = 0.$ 1) Is it true that $$\int_0^x f(t) dt = o(\int_0^x g(t) dt) \ ?$$ Prove or give counterexample ? 2)If it is not true, what condition should be added to $g(x)$ such that little o preserve under integration ? 1) Since it asks some conditions on no.2, I think that no.1 should be false. But I cannot think of examples. Simple functions : $e^x,$ polynomial, $\log x, \sin x, \cos x, \frac{1}{x^m}$ do not work (I try somes combination of them). I guess the example might be something more complicated and connect to analysis. Any hints on how to build functions ? 2) Suppose I want to show that $$\lim \frac{F(x)}{G(x)} = 0$$ where $F, G$ denote the corresponding antiderivative of $f ,g$ without invoke any more condition on $g$. One thing is $G(x)$ should not be $0$. Since $g$ is positive, I think this clearly confirm that $G(x) \neq 0$ for any $x$. If the condition is true, for any $\epsilon > 0$, it must exist $M > 0$ such that $$F(x) < \epsilon G(x)$$ for any $x > M$. Let $\epsilon > 0$. Since $f = o(g)$, there exist $M$ such that $f(x) < \epsilon g(x)$ for $x > M.$ So $$\int_0^x f(t) dt < \int_0^x \epsilon g(t) dt$$ for any $x > M.$ Seem like I prove it and $g$ does not require anything. However, I feel bad and not sure about this. Seem like it is wrong somewhere, but I am not sure where and why. Help please.","Let $f,g : [0, \infty) \rightarrow (0, \infty)$ such that $$f (x) = o(g(x))$$  that is, $ \lim_{x \rightarrow \infty} \frac{f(x)}{g(x)} = 0.$ 1) Is it true that $$\int_0^x f(t) dt = o(\int_0^x g(t) dt) \ ?$$ Prove or give counterexample ? 2)If it is not true, what condition should be added to $g(x)$ such that little o preserve under integration ? 1) Since it asks some conditions on no.2, I think that no.1 should be false. But I cannot think of examples. Simple functions : $e^x,$ polynomial, $\log x, \sin x, \cos x, \frac{1}{x^m}$ do not work (I try somes combination of them). I guess the example might be something more complicated and connect to analysis. Any hints on how to build functions ? 2) Suppose I want to show that $$\lim \frac{F(x)}{G(x)} = 0$$ where $F, G$ denote the corresponding antiderivative of $f ,g$ without invoke any more condition on $g$. One thing is $G(x)$ should not be $0$. Since $g$ is positive, I think this clearly confirm that $G(x) \neq 0$ for any $x$. If the condition is true, for any $\epsilon > 0$, it must exist $M > 0$ such that $$F(x) < \epsilon G(x)$$ for any $x > M$. Let $\epsilon > 0$. Since $f = o(g)$, there exist $M$ such that $f(x) < \epsilon g(x)$ for $x > M.$ So $$\int_0^x f(t) dt < \int_0^x \epsilon g(t) dt$$ for any $x > M.$ Seem like I prove it and $g$ does not require anything. However, I feel bad and not sure about this. Seem like it is wrong somewhere, but I am not sure where and why. Help please.",,"['real-analysis', 'integration', 'analysis']"
51,How to understand integral of wedge product?,How to understand integral of wedge product?,,"Taken from Bott and Tu (page 70): Let $\sigma$ be the positive generator of $H^{n-1}(S^{n-1})$ and $\psi = \pi^* \sigma$ the corresponding generator of $H^{n-1}(\mathbf R^n - 0)$. [...] Let $d\rho = \rho'(r)\,dr$ be a bump form on $\mathbf R$ with total integral $1$. Then $(d\rho) \wedge \psi$ is a compactly supported form on $\mathbf R^n$ with total integral $1$. If it's not already obvious, $\pi$ is the projection $\mathbf R^n - 0 \to S^{n-1}$ and $\rho$ is a function of the radius $r$ in $\mathbf R^n$. Question: How am I to understand the last sentence intuitively? So far, the picture in my head is for $n = 3$. I imagine $\sigma \in H^2(S^2)$ to be a function that eats a $2$-dimensional subset of $S^2$ and spits out its area. Then $\psi \in H^2(\mathbf R^3)$ is a function that eats a $2$-manifold in $\mathbf R^3$, projects it onto the unit sphere, and spits out its area. I'm not sure how these help me understand what $d\rho \wedge \psi$ is doing, or why it should integrate to $1$.","Taken from Bott and Tu (page 70): Let $\sigma$ be the positive generator of $H^{n-1}(S^{n-1})$ and $\psi = \pi^* \sigma$ the corresponding generator of $H^{n-1}(\mathbf R^n - 0)$. [...] Let $d\rho = \rho'(r)\,dr$ be a bump form on $\mathbf R$ with total integral $1$. Then $(d\rho) \wedge \psi$ is a compactly supported form on $\mathbf R^n$ with total integral $1$. If it's not already obvious, $\pi$ is the projection $\mathbf R^n - 0 \to S^{n-1}$ and $\rho$ is a function of the radius $r$ in $\mathbf R^n$. Question: How am I to understand the last sentence intuitively? So far, the picture in my head is for $n = 3$. I imagine $\sigma \in H^2(S^2)$ to be a function that eats a $2$-dimensional subset of $S^2$ and spits out its area. Then $\psi \in H^2(\mathbf R^3)$ is a function that eats a $2$-manifold in $\mathbf R^3$, projects it onto the unit sphere, and spits out its area. I'm not sure how these help me understand what $d\rho \wedge \psi$ is doing, or why it should integrate to $1$.",,"['integration', 'differential-forms']"
52,A tricky trigonometric integral,A tricky trigonometric integral,,"How can I evaluate $$I = \int_0^{\pi / 2} 5x \sin(6x) \cos (9x^5) \,\textrm{d}x$$ without Computer Algebra Systems? Mathematica fails to find the value of $I$ and RIES isn't very helpful too. I've tried expanding the function under integral sign into power series with no results. If it's too hard, I will be satisfied with $$\int_0^{\pi/2} x \sin x \cos (x^2) \,\textrm{d}x$$ or anything like that.","How can I evaluate $$I = \int_0^{\pi / 2} 5x \sin(6x) \cos (9x^5) \,\textrm{d}x$$ without Computer Algebra Systems? Mathematica fails to find the value of $I$ and RIES isn't very helpful too. I've tried expanding the function under integral sign into power series with no results. If it's too hard, I will be satisfied with $$\int_0^{\pi/2} x \sin x \cos (x^2) \,\textrm{d}x$$ or anything like that.",,"['calculus', 'integration', 'definite-integrals']"
53,How does uncertainty of dataset propagates through numerical integration?,How does uncertainty of dataset propagates through numerical integration?,,"In the following text, uncertainty refers to standard deviation. I have 500 time series which I use in a few equations and get averages and uncertainties. Through calculations for each time series, I end up with a data set with 500 values of the position z, the resistance $R(z)$, the absolute and the relative uncertainty: $$  z \,,  R( z )  \,, \pm \Delta R(z) \,, \pm \frac{\Delta R(z)}{ R(z) }$$ What I want is to calculate the permeation $P$ through the following formula: $$ P=\frac{1}{\int_{z_{min}}^{z_{max}}R(z)dz} $$ therefore, I need to numerically integrate these 500 values. This is easy with a trapezoidal but the question is what happens to the uncertainty? How does it propagate through the integral? Essentially I want to report the average $ \left( P \pm \Delta P \right) \, [units]$","In the following text, uncertainty refers to standard deviation. I have 500 time series which I use in a few equations and get averages and uncertainties. Through calculations for each time series, I end up with a data set with 500 values of the position z, the resistance $R(z)$, the absolute and the relative uncertainty: $$  z \,,  R( z )  \,, \pm \Delta R(z) \,, \pm \frac{\Delta R(z)}{ R(z) }$$ What I want is to calculate the permeation $P$ through the following formula: $$ P=\frac{1}{\int_{z_{min}}^{z_{max}}R(z)dz} $$ therefore, I need to numerically integrate these 500 values. This is easy with a trapezoidal but the question is what happens to the uncertainty? How does it propagate through the integral? Essentially I want to report the average $ \left( P \pm \Delta P \right) \, [units]$",,"['integration', 'numerical-methods', 'error-propagation']"
54,Showing that $\int_0^\infty x^{-x} \mathrm{d}x \leq 2$.,Showing that .,\int_0^\infty x^{-x} \mathrm{d}x \leq 2,"This integral is very closely related to the sophmores dream that states $$ \int_0^1 x^{-x}\mathrm{d}x = \sum_{n=1}^\infty n^{-n} = 1.27\ldots $$ For example here http://en.wikipedia.org/wiki/Sophomore%27s_dream Now I want to bound the integral, and showing that is less that 2.  For the interval $[0,1]$ a good bound is rewriting it to $\exp(x\log x)$ and using the expansion $$ 1 - x \log(x) + \frac12 (-x \log(x))^2$$ but how does one handle $[1,\infty)$ ? In this answer here How to evaluate $ \int_0^\infty {1 \over x^x}dx$ in terms of summation of series? gives bounds to the integral, but they are not tight enough.. So to taste my question again, how does one prove that $$ \int_0^\infty \frac{\mathrm{d}x}{x^x} \leq 2 $$","This integral is very closely related to the sophmores dream that states $$ \int_0^1 x^{-x}\mathrm{d}x = \sum_{n=1}^\infty n^{-n} = 1.27\ldots $$ For example here http://en.wikipedia.org/wiki/Sophomore%27s_dream Now I want to bound the integral, and showing that is less that 2.  For the interval $[0,1]$ a good bound is rewriting it to $\exp(x\log x)$ and using the expansion $$ 1 - x \log(x) + \frac12 (-x \log(x))^2$$ but how does one handle $[1,\infty)$ ? In this answer here How to evaluate $ \int_0^\infty {1 \over x^x}dx$ in terms of summation of series? gives bounds to the integral, but they are not tight enough.. So to taste my question again, how does one prove that $$ \int_0^\infty \frac{\mathrm{d}x}{x^x} \leq 2 $$",,"['real-analysis', 'approximation']"
55,"Rigorous justification for ""complex"" change of variable in integration","Rigorous justification for ""complex"" change of variable in integration",,"Suppose that I have $X_1,\ldots,X_n$ i.i.d. $\sim $ $X$ and $Y_1,\ldots,Y_n$ i.i.d. $\sim$ $Y$ for some continuous $X$ and $Y$. Consider the r.v.'s $\bar{X}=\frac{1}{n}\sum_jX_j$ and $\bar{Y}=\frac{1}{n}\sum_jY_j$. First, the joint MGF for $(X,Y)$ can be written as $$ M(t,u)=\exp{K(t,u)}=E[\exp(tX+uY)]. $$ Here $K$ denotes the joint CGF (the joint cumulant generating function). Then, I know enough to write the density $f_n$ of $(\bar{X},\bar{Y})$ as $$ f_n(\bar{x},\bar{y})=\frac{n^2}{(2\pi)^2}\int_{-\infty}^\infty\int_{-\infty}^\infty\exp[n(K(it,iu)-it\bar{x}-iu\bar{y})] \, dt \, du.\tag{$*$} $$ My reading (source available if requested) says that I can write ($*$) as $$ f_n(\bar{x},\bar{y})=\frac{n^2}{(2\pi i)^2}\int\int\exp[n(K(t,u)-t\bar{x}-u\bar{y})] \, dt \, du.\tag{$**$} $$ with ""integration being along admissible paths in the $t$ and $u$ planes."" Heuristically, I can see how to go from ($*$) to ($**$) by two changes of variables: $it\to t$ and $iu\to u$, but how do I justify ($**$) rigorously? Can you please recommend (self-learning-friendly) references? My background: undergrad real and complex analysis. For complex analysis, I'm familiar with the Cauchy Integral Formula, Cauchy Theorem, Residue Theorem, McLauren series, contour integrals.","Suppose that I have $X_1,\ldots,X_n$ i.i.d. $\sim $ $X$ and $Y_1,\ldots,Y_n$ i.i.d. $\sim$ $Y$ for some continuous $X$ and $Y$. Consider the r.v.'s $\bar{X}=\frac{1}{n}\sum_jX_j$ and $\bar{Y}=\frac{1}{n}\sum_jY_j$. First, the joint MGF for $(X,Y)$ can be written as $$ M(t,u)=\exp{K(t,u)}=E[\exp(tX+uY)]. $$ Here $K$ denotes the joint CGF (the joint cumulant generating function). Then, I know enough to write the density $f_n$ of $(\bar{X},\bar{Y})$ as $$ f_n(\bar{x},\bar{y})=\frac{n^2}{(2\pi)^2}\int_{-\infty}^\infty\int_{-\infty}^\infty\exp[n(K(it,iu)-it\bar{x}-iu\bar{y})] \, dt \, du.\tag{$*$} $$ My reading (source available if requested) says that I can write ($*$) as $$ f_n(\bar{x},\bar{y})=\frac{n^2}{(2\pi i)^2}\int\int\exp[n(K(t,u)-t\bar{x}-u\bar{y})] \, dt \, du.\tag{$**$} $$ with ""integration being along admissible paths in the $t$ and $u$ planes."" Heuristically, I can see how to go from ($*$) to ($**$) by two changes of variables: $it\to t$ and $iu\to u$, but how do I justify ($**$) rigorously? Can you please recommend (self-learning-friendly) references? My background: undergrad real and complex analysis. For complex analysis, I'm familiar with the Cauchy Integral Formula, Cauchy Theorem, Residue Theorem, McLauren series, contour integrals.",,"['integration', 'complex-analysis', 'probability-theory', 'reference-request', 'fourier-analysis']"
56,Help understanding the proof of Trace Theorem given in Evans,Help understanding the proof of Trace Theorem given in Evans,,"I need help to understand the proof of the Trace Theorem given in Evans L.C. Partial differential equations (AMS, 1997) : Asume $U$ is a bounded open set and that $\partial U$ is $C^1$. Then there exists a bounded linear operator $T:W^{1,p}(U) \to L^p(\partial U)$ such that: $\quad$(i) $Tu=u|_{\partial U}$ if $u \in W^{1,p}(U) \cap C(\bar{U})$ and $\quad$(ii) $\|Tu\|_{L^p(\partial U)} \le C \| U \|_{W^{1,p} (U)},$ for each $u \in W^{1,p}(U)$, with the constant $C$ depending only on $p$ and $U$. The proof provided starts like this: Okay, so we first asume that there is a neighborhood $U_{x^0}$ of $x^0$ such that $U_{x^0} \subset \{x \in \mathbb{R}^n \ | \ x_n=0\}$ and prove (1) . Then the proof continues by saying ""If $\partial U$ is not flat near $x^0$"", we straighten out the boundary near $x^0$ to obtain the setting above. Applying estimate (1) and changing variables... (The proof continues) Could someone clarify me a little more precisely what straighten out the boundary near $x^0$ means? Updated: This is the definition of open set of class $C^1 $ which gives us a diffeomorphism to the set $Q_0$ which is ""flat"". We define the following sets: $R_+ = \{x=(x_1,...,x_n) \in \mathbb{R}^n \ | \ x_n \geq 0\} $ $ Q = \{x=(x_1,...,x_n) \in \mathbb{R}^n \ | \ (\sum_{i=1}^{n-1} x_i^2)^{1/2} < 1 \ y \  |x_n|<1 \}$ $ Q_+=R_+ \cap Q $ $ Q_0=\{(x_1,...,x_{n-1},0) \in \mathbb{R}^n \ | \ (\sum_{i=1}^{n-1} x_i^2)^{1/2} < 1 \}$ An open set $\Omega$ is of class $C^1$ if for every $x \in \partial \Omega$ there exists a neighborhood $U_x$ of $x$ in $\mathbb{R}^n$ and   a bijective map $H: Q \to U_x$ such that: $H \in C^1(\overline{Q})$ $H^{-1} \in C^1(\overline{U_x})$ $H(Q_+)=U_x \cap Q$ $H(Q_0)= U_x \cap \partial \Omega$","I need help to understand the proof of the Trace Theorem given in Evans L.C. Partial differential equations (AMS, 1997) : Asume $U$ is a bounded open set and that $\partial U$ is $C^1$. Then there exists a bounded linear operator $T:W^{1,p}(U) \to L^p(\partial U)$ such that: $\quad$(i) $Tu=u|_{\partial U}$ if $u \in W^{1,p}(U) \cap C(\bar{U})$ and $\quad$(ii) $\|Tu\|_{L^p(\partial U)} \le C \| U \|_{W^{1,p} (U)},$ for each $u \in W^{1,p}(U)$, with the constant $C$ depending only on $p$ and $U$. The proof provided starts like this: Okay, so we first asume that there is a neighborhood $U_{x^0}$ of $x^0$ such that $U_{x^0} \subset \{x \in \mathbb{R}^n \ | \ x_n=0\}$ and prove (1) . Then the proof continues by saying ""If $\partial U$ is not flat near $x^0$"", we straighten out the boundary near $x^0$ to obtain the setting above. Applying estimate (1) and changing variables... (The proof continues) Could someone clarify me a little more precisely what straighten out the boundary near $x^0$ means? Updated: This is the definition of open set of class $C^1 $ which gives us a diffeomorphism to the set $Q_0$ which is ""flat"". We define the following sets: $R_+ = \{x=(x_1,...,x_n) \in \mathbb{R}^n \ | \ x_n \geq 0\} $ $ Q = \{x=(x_1,...,x_n) \in \mathbb{R}^n \ | \ (\sum_{i=1}^{n-1} x_i^2)^{1/2} < 1 \ y \  |x_n|<1 \}$ $ Q_+=R_+ \cap Q $ $ Q_0=\{(x_1,...,x_{n-1},0) \in \mathbb{R}^n \ | \ (\sum_{i=1}^{n-1} x_i^2)^{1/2} < 1 \}$ An open set $\Omega$ is of class $C^1$ if for every $x \in \partial \Omega$ there exists a neighborhood $U_x$ of $x$ in $\mathbb{R}^n$ and   a bijective map $H: Q \to U_x$ such that: $H \in C^1(\overline{Q})$ $H^{-1} \in C^1(\overline{U_x})$ $H(Q_+)=U_x \cap Q$ $H(Q_0)= U_x \cap \partial \Omega$",,"['integration', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces', 'proof-explanation']"
57,"Let $f:[-1,1]\to \mathbb{R}$ diferentiable, with $f'$ integrable, such that $\frac{\int_{-1}^{1}e^xf(x)dx}{f(1)-f(-1)}=2(e+e^{-1})$.","Let  diferentiable, with  integrable, such that .","f:[-1,1]\to \mathbb{R} f' \frac{\int_{-1}^{1}e^xf(x)dx}{f(1)-f(-1)}=2(e+e^{-1})","Let $f:[-1,1]\to \mathbb{R}$ diferentiable, with $f'$ integrable, such that $$\frac{\int_{-1}^{1}e^xf(x)dx}{f(1)-f(-1)}=2(e+e^{-1})$$   Prove that there exists $c\in (-1,1)$ such that $$\int_{-1}^{1}\frac{f(x)}{e^x}dx=4ef'(c)$$ My progress so far: I integrated $\int_{-1}^{1}e^x f(x) dx$ by parts. I got $(e-e^{-1})-\int_{-1}^{1} f'(x)e^x dx =2(e+e^{-1})$. By Mean Value Theorem for integrals, $$\frac{e-e^{-1}}{e+e^{-1}}=f'(c)e^c$$ for $c \in (-1,1)$. But how to proceed now?","Let $f:[-1,1]\to \mathbb{R}$ diferentiable, with $f'$ integrable, such that $$\frac{\int_{-1}^{1}e^xf(x)dx}{f(1)-f(-1)}=2(e+e^{-1})$$   Prove that there exists $c\in (-1,1)$ such that $$\int_{-1}^{1}\frac{f(x)}{e^x}dx=4ef'(c)$$ My progress so far: I integrated $\int_{-1}^{1}e^x f(x) dx$ by parts. I got $(e-e^{-1})-\int_{-1}^{1} f'(x)e^x dx =2(e+e^{-1})$. By Mean Value Theorem for integrals, $$\frac{e-e^{-1}}{e+e^{-1}}=f'(c)e^c$$ for $c \in (-1,1)$. But how to proceed now?",,['calculus']
58,The Heuristic Gauss-Kronrod Based Error Estimator in Quadpack,The Heuristic Gauss-Kronrod Based Error Estimator in Quadpack,,"I'm trying to understand the local error estimate that Quadpack (and subsequently other libraries such as GSL, quadpack++, cubature, etc.) uses for it's general adaptive quadrature subroutine QAG. The integration procedure is a Gauss-Kronrod rule and Quadpack uses a heuristic error estimate that I struggle to make sense of. The error estimate can be found on page 67 in the Quadpack book or on page 11 in this excellent (and free) review of Error Estimation in Adaptive Quadrature . Or in the Quadpack source code for that matter. It states: Let $G_n[a, b]$ is the n-point Gauss quadrature rule of degree $2n−1$ and $K_{2n+1}[a, b]$ is the $2n+1$ point Gauss-Kronrod extension of degree $3n+1$ which is in turn used as the approximation to the integral. The local error estimate is: $$\varepsilon_k = \tilde{I}_k \min \left\{ 1,  \left(200\frac{\left|G_n[a_k, b_k]-K_{2n+1}[a_k, b_k]\right|}{ \tilde{I}_k} \right)^{3/2} \right\} $$ where $$ \tilde{I}_k = \int_{a_k}^{b_k}\left| f(x) - \frac{K_{2n+1}[a_k, b_k]}{b_k-a_k} \right| dx$$ which is also evaluated numerically using the $K_{2n+1}[a,b]$ rule. The power $3/2$ has been chosen experimentally in such a way that the error scales exponentially with a break-even point at approximately relative machine precision for single floating point arithmetic. This means that the error estimate is less pessimistic for small values of $\left|G_n[a_k, b_k]-K_{2n+1}[a_k, b_k]\right|$ and more reliable for large values. Indeed this is verified by plotting (letting $\tilde{I}_k = 1$): Furthermore Quadpack also evaluates  $$ \hat{I}_k = \int_{a_k}^{b_k}| f(x) | dx$$ using the $K_{2n+1}[a,b]$ rule. Denote by $f_{min}$ and $\varepsilon_{mac}$ the minimum floating point number and the floating point machine precision respectively (these number depends on single or double precision). Finally Quadpack checks if  $$\hat{I}_k > \frac{f_{min}}{50 \varepsilon_{mac}}$$ holds true and in this case they set $$\varepsilon_k = \max \left\{50 \varepsilon_{mac} \hat{I}_k, \varepsilon_k \right\}$$ which will be used as the final error estimation of the integral on $[a_k,b_k]$. Question 1: What is the reason for doing the last step with the absolute value integrand. I do not under stand the point of it at all. Question 2: Is there a particular reason why they choose the approximate single precision machine epsilon as the break even point, or is it just an arbitrary small number? Why not use the double precision machine epsilon? Thanks in advance!","I'm trying to understand the local error estimate that Quadpack (and subsequently other libraries such as GSL, quadpack++, cubature, etc.) uses for it's general adaptive quadrature subroutine QAG. The integration procedure is a Gauss-Kronrod rule and Quadpack uses a heuristic error estimate that I struggle to make sense of. The error estimate can be found on page 67 in the Quadpack book or on page 11 in this excellent (and free) review of Error Estimation in Adaptive Quadrature . Or in the Quadpack source code for that matter. It states: Let $G_n[a, b]$ is the n-point Gauss quadrature rule of degree $2n−1$ and $K_{2n+1}[a, b]$ is the $2n+1$ point Gauss-Kronrod extension of degree $3n+1$ which is in turn used as the approximation to the integral. The local error estimate is: $$\varepsilon_k = \tilde{I}_k \min \left\{ 1,  \left(200\frac{\left|G_n[a_k, b_k]-K_{2n+1}[a_k, b_k]\right|}{ \tilde{I}_k} \right)^{3/2} \right\} $$ where $$ \tilde{I}_k = \int_{a_k}^{b_k}\left| f(x) - \frac{K_{2n+1}[a_k, b_k]}{b_k-a_k} \right| dx$$ which is also evaluated numerically using the $K_{2n+1}[a,b]$ rule. The power $3/2$ has been chosen experimentally in such a way that the error scales exponentially with a break-even point at approximately relative machine precision for single floating point arithmetic. This means that the error estimate is less pessimistic for small values of $\left|G_n[a_k, b_k]-K_{2n+1}[a_k, b_k]\right|$ and more reliable for large values. Indeed this is verified by plotting (letting $\tilde{I}_k = 1$): Furthermore Quadpack also evaluates  $$ \hat{I}_k = \int_{a_k}^{b_k}| f(x) | dx$$ using the $K_{2n+1}[a,b]$ rule. Denote by $f_{min}$ and $\varepsilon_{mac}$ the minimum floating point number and the floating point machine precision respectively (these number depends on single or double precision). Finally Quadpack checks if  $$\hat{I}_k > \frac{f_{min}}{50 \varepsilon_{mac}}$$ holds true and in this case they set $$\varepsilon_k = \max \left\{50 \varepsilon_{mac} \hat{I}_k, \varepsilon_k \right\}$$ which will be used as the final error estimation of the integral on $[a_k,b_k]$. Question 1: What is the reason for doing the last step with the absolute value integrand. I do not under stand the point of it at all. Question 2: Is there a particular reason why they choose the approximate single precision machine epsilon as the break even point, or is it just an arbitrary small number? Why not use the double precision machine epsilon? Thanks in advance!",,"['integration', 'numerical-methods']"
59,How to show the following identity,How to show the following identity,,"Suppose we have two functions: $$ \tag 0 g_{1}(t, m, l) = \int \limits_{-\pi}^{\pi}\int\limits_{-\pi}^{\pi}\frac{\cos(lx)\cos(my)}{t - \cos(2x) - 2\cos(x)\cos(y)}dxdy, $$ $$ \tag {0'} g_{2}(t, m, l) = \int \limits_{-\pi}^{\pi}\int\limits_{-\pi}^{\pi}\frac{\cos(lx)\cos(my)}{t - \cos(2x) - 2\cos(x)\cos(3y)}dxdy $$ Here $m, l$ are positive integer numbers. One scientific paper claims that  $$ \tag 1 g_{2}(t,m,l) = g_{1}\left(t, \frac{m}{3}, l \right), $$ but I don't see how this can be true; especially I don't understand which transformation I have to perform to see this equality. For example, for $m =0$ $(1)$ provides the statement that $$ g_{2}(t, 0, l) = g_{1}(t, 0,l) $$ which seems to be incorrect. Could you help me? Precisely, an article is this one, and the statement $(1)$ is the direct consequence of Eq. $(3.13)$, which relates expression given by Eq. $(3.5)$ with the one which is given by Eq. $(2.2a)$. They coincide with my expressions $(0), (0')$ up to simple coordinate transformation, which isn't relevant, up to multiplier in Eq. $(3.5)$, and up to the redefinition of constant $t \to \frac{4t^{2} - 3}{2}$ in $(3.5)$, which aren't relevant.","Suppose we have two functions: $$ \tag 0 g_{1}(t, m, l) = \int \limits_{-\pi}^{\pi}\int\limits_{-\pi}^{\pi}\frac{\cos(lx)\cos(my)}{t - \cos(2x) - 2\cos(x)\cos(y)}dxdy, $$ $$ \tag {0'} g_{2}(t, m, l) = \int \limits_{-\pi}^{\pi}\int\limits_{-\pi}^{\pi}\frac{\cos(lx)\cos(my)}{t - \cos(2x) - 2\cos(x)\cos(3y)}dxdy $$ Here $m, l$ are positive integer numbers. One scientific paper claims that  $$ \tag 1 g_{2}(t,m,l) = g_{1}\left(t, \frac{m}{3}, l \right), $$ but I don't see how this can be true; especially I don't understand which transformation I have to perform to see this equality. For example, for $m =0$ $(1)$ provides the statement that $$ g_{2}(t, 0, l) = g_{1}(t, 0,l) $$ which seems to be incorrect. Could you help me? Precisely, an article is this one, and the statement $(1)$ is the direct consequence of Eq. $(3.13)$, which relates expression given by Eq. $(3.5)$ with the one which is given by Eq. $(2.2a)$. They coincide with my expressions $(0), (0')$ up to simple coordinate transformation, which isn't relevant, up to multiplier in Eq. $(3.5)$, and up to the redefinition of constant $t \to \frac{4t^{2} - 3}{2}$ in $(3.5)$, which aren't relevant.",,"['integration', 'definite-integrals']"
60,Integral of divergence over a closed surface,Integral of divergence over a closed surface,,"I am reading a paper, where an integral of a divergence over a closed surface is used without proof. $\oint_S [\nabla \cdot \vec{v}(\vec{r})] d\vec{s} = 0$ , where $\vec{v}$ is tangential to the surface ( $\vec{v}(r)\cdot \vec{n}(\vec{r}) = 0$ ) I have looked at vector calculus identities and Green theorems and can't seem to find the expression I need. Any suggestions?","I am reading a paper, where an integral of a divergence over a closed surface is used without proof. , where is tangential to the surface ( ) I have looked at vector calculus identities and Green theorems and can't seem to find the expression I need. Any suggestions?",\oint_S [\nabla \cdot \vec{v}(\vec{r})] d\vec{s} = 0 \vec{v} \vec{v}(r)\cdot \vec{n}(\vec{r}) = 0,"['integration', 'vector-analysis', 'surface-integrals']"
61,Evaluate the Integral.,Evaluate the Integral.,,"Can someone help evaluate the following integral? (I have tried doing it with Mathematica but I just get the integral back - I am assuming that it can't figure out how to solve it?) \begin{equation}  I=\int \frac{\csc ^2(\pi  f) (\sin (2 \pi  f))^{2a}}{-2 \cos (2 \pi  a f)+\cos (2 \pi  f-2 \pi  a f)-2 \cos (2 \pi  f)+3} \, df \end{equation} $a(>>1)$ is just a constant.  Some context: The above integral is actually the result of a particular electrical circuit which implements the following $Z$ domain transfer function: $\begin{equation} H(z)= \frac{\left(1-z^{-a}\right)^2}{\left(1-z^{-1}\right) \left(2-z^{-a}-z^{-1}\right)}  \end{equation}$ where $z=e^{2\pi i f}$. In order to find the total power at the output, I need to evaluate the following integral: $\begin{equation} \int_{f=0}^{f=f_s} |H(z)|^2 df  \end{equation}$ The integral that you see in the first equation is actually the above function ($|H(z)|^2$) after simplification by Mathematica.","Can someone help evaluate the following integral? (I have tried doing it with Mathematica but I just get the integral back - I am assuming that it can't figure out how to solve it?) \begin{equation}  I=\int \frac{\csc ^2(\pi  f) (\sin (2 \pi  f))^{2a}}{-2 \cos (2 \pi  a f)+\cos (2 \pi  f-2 \pi  a f)-2 \cos (2 \pi  f)+3} \, df \end{equation} $a(>>1)$ is just a constant.  Some context: The above integral is actually the result of a particular electrical circuit which implements the following $Z$ domain transfer function: $\begin{equation} H(z)= \frac{\left(1-z^{-a}\right)^2}{\left(1-z^{-1}\right) \left(2-z^{-a}-z^{-1}\right)}  \end{equation}$ where $z=e^{2\pi i f}$. In order to find the total power at the output, I need to evaluate the following integral: $\begin{equation} \int_{f=0}^{f=f_s} |H(z)|^2 df  \end{equation}$ The integral that you see in the first equation is actually the above function ($|H(z)|^2$) after simplification by Mathematica.",,['integration']
62,How to calculate the area of the visible parts of a 3D PieChart?,How to calculate the area of the visible parts of a 3D PieChart?,,"I have created a 3D Pie Chart whose major feat (among the others) is to be rotated: I did it to demonstrate how the visual perception of data in a Pie Chart can be distorted depending on the position, so that with a 3D pie chart, the closer the slices are, the more important they 'seem' perceptively, since they occupy a bigger area (area of ellipse + visible area of cylinder's side), while the more distant they are the less important they look, since they occupy a smaller area (only the ellipse's area, without the cylinder's), despite all numerical weighs being equal. A method of evaluation of how much a slice's weigh is perceived compared to its basic weigh (the percentage), is the area of the slices of the 3D Pie Chart (inner pie area + outer pie area). In order to demonstrate my point, I need to calculate the total visual area of the slices. What approach is best to be used ? EDIT 1 I was able to calculate the total visible area of the PieChart by using the formulas to calculate the area of a cylinder, as shown below. Area of Pie Chart = Area of Ellipse + Area of Cylinder's side AE -> (Area of Ellipse): PE -> (Perimeter of Ellipse) (It is an approximation): ACS -> (Area of Cylinder's side): ACS = PE * H (height of cylinder) VACS -> Visible part of Cylinder's Area's side: VACS = ACS / 2 Total Visible Area of Pie Chart -> AE + VACS EDIT 2 I was also able to calculate the slice closest to the viewer (the one having 29.5%): For each slice:  - AE * percentage e.g. if AE is 74.28, then the light blue slice within the Ellipse (29.5%) must be: then the light blue slice's outer side, with VACS = 25.36: the result for the light blue slice would be -> 21.9126 + 7.4812 = 29,3938 but what about the other 2 (26% and 10.1%)?","I have created a 3D Pie Chart whose major feat (among the others) is to be rotated: I did it to demonstrate how the visual perception of data in a Pie Chart can be distorted depending on the position, so that with a 3D pie chart, the closer the slices are, the more important they 'seem' perceptively, since they occupy a bigger area (area of ellipse + visible area of cylinder's side), while the more distant they are the less important they look, since they occupy a smaller area (only the ellipse's area, without the cylinder's), despite all numerical weighs being equal. A method of evaluation of how much a slice's weigh is perceived compared to its basic weigh (the percentage), is the area of the slices of the 3D Pie Chart (inner pie area + outer pie area). In order to demonstrate my point, I need to calculate the total visual area of the slices. What approach is best to be used ? EDIT 1 I was able to calculate the total visible area of the PieChart by using the formulas to calculate the area of a cylinder, as shown below. Area of Pie Chart = Area of Ellipse + Area of Cylinder's side AE -> (Area of Ellipse): PE -> (Perimeter of Ellipse) (It is an approximation): ACS -> (Area of Cylinder's side): ACS = PE * H (height of cylinder) VACS -> Visible part of Cylinder's Area's side: VACS = ACS / 2 Total Visible Area of Pie Chart -> AE + VACS EDIT 2 I was also able to calculate the slice closest to the viewer (the one having 29.5%): For each slice:  - AE * percentage e.g. if AE is 74.28, then the light blue slice within the Ellipse (29.5%) must be: then the light blue slice's outer side, with VACS = 25.36: the result for the light blue slice would be -> 21.9126 + 7.4812 = 29,3938 but what about the other 2 (26% and 10.1%)?",,"['calculus', 'integration', 'geometry', 'surfaces', 'area']"
63,fourier transform for pde equation,fourier transform for pde equation,,"I was solving the pde using fourier transform: $u_{tt}-u_{xx}+m^2u=0$ with initial values $u(0,x)=f(x)$ and $u_t(0,x)=g(x)$. I have received the answer $$U(t,k)=Ae^{-it \sqrt {k^2+m^2}}+Be^{it \sqrt {k^2+m^2}}$$ where $A, B, m$ are constants, and $U(t,k)$ is the Fourier transform of $u(t,x)$. Now I have problems with turning $U$ to $u$. I tried to do it directly, calculating the integral for inverse fourier, but I got stuck since the integral contains $e^{it \sqrt{k^2+m^2}+ikx}dk$ and $f(x)$, $g(x)$ a bit different from that. Can someone help me, please? Any hints are welcome! Thank you!","I was solving the pde using fourier transform: $u_{tt}-u_{xx}+m^2u=0$ with initial values $u(0,x)=f(x)$ and $u_t(0,x)=g(x)$. I have received the answer $$U(t,k)=Ae^{-it \sqrt {k^2+m^2}}+Be^{it \sqrt {k^2+m^2}}$$ where $A, B, m$ are constants, and $U(t,k)$ is the Fourier transform of $u(t,x)$. Now I have problems with turning $U$ to $u$. I tried to do it directly, calculating the integral for inverse fourier, but I got stuck since the integral contains $e^{it \sqrt{k^2+m^2}+ikx}dk$ and $f(x)$, $g(x)$ a bit different from that. Can someone help me, please? Any hints are welcome! Thank you!",,"['integration', 'partial-differential-equations', 'fourier-analysis']"
64,Simplify $\int_0^\infty \frac{\text{d}{x}}{e^x+x^n}$,Simplify,\int_0^\infty \frac{\text{d}{x}}{e^x+x^n},I seem to have seen quite a lot of integrals in the form: $$\int_0^\infty \frac{\text{d}x}{e^x+(1+x^n)}$$ But none of those hold a closed forms (at least to my knowledge) $$\Large\color\red{\int_0^\infty \frac{\text{d}x}{e^x+x^n}}$$ Does a closed form exist of this integral? (in terms of $n$),I seem to have seen quite a lot of integrals in the form: $$\int_0^\infty \frac{\text{d}x}{e^x+(1+x^n)}$$ But none of those hold a closed forms (at least to my knowledge) $$\Large\color\red{\int_0^\infty \frac{\text{d}x}{e^x+x^n}}$$ Does a closed form exist of this integral? (in terms of $n$),,['integration']
65,Exterior Differential (and its Equivalent Differential Operator) of an Integral 0-Form,Exterior Differential (and its Equivalent Differential Operator) of an Integral 0-Form,,"I am reading Witten's 1982 paper ""Supersymmetry and Morse Theory,"" and while I am slowly learning the material as I read through the paper, I have come across an equivalence that, while it should be rather basic, I can't seem to show rigorously. In the paper, Witten defines a (modified) exterior derivative $\text{d}_t$ as $$\text{d}_t = \text{d} + t\, \text{d} h$$ where $$h(\phi) = \int_S W(\phi(x)) \, \text{d}x,$$ the region $S$ is a circle with circumference $L$ and $\phi$ is a real-valued function from $S$ to the reals, i.e. $\phi: S \rightarrow \mathbb{R}$. Now, I am having trouble proving to myself that this exterior derivative $\text{d}_t$ is equivalent to the standard differential operator $$\frac{d}{d \phi} + t L \frac{dW}{d\phi}$$ acting on real-valued functions of the variable $\phi$. Now, as far as I can tell, Witten is making the identification $d \leftrightarrow \frac{d}{d \phi}$ here for this equivalency. However, given that, I can't seem to get the remaining term $t L \frac{dW}{d\phi}$. My attempt: We make the identification $\text{d} \leftrightarrow \frac{d}{d\phi}$, so the last term in the expression for $\text{d}_t$ is simply $\frac{dh}{d \phi}$. So, it should be as simple as evaluating $$\frac{d}{d\phi} \int_S W(\phi(x)) \, \text{d}x.$$ However, in evaluating this, the trouble I am running in to is that $\phi$ is an arbitrary function that is not necessarily invertible, so I cannot just rewrite the integral as an integral over $\phi$. I have also tried using the mechanics of exterior derivatives first, but then I run into the issue of evaluating $$\text{d} \int_S W(\phi(x)) \, \text{d}x,$$ which I'm not so sure where to begin (the exterior derivative is defined on the real line, not on $S$). Possibly, we can commute the derivative, which leaves us with $$\int_S \text{d} (W(\phi(x)) \, \text{d}x).$$ Then, using one of the identities of the exterior derivative yields $$\int_S \text{d} (W(\phi(x))) \land \text{d}x,$$ at which point I think the correct next step would be $$\int_S \frac{dW}{d\phi} \,\text{d} \phi \land \text{d}x.$$ From here, though, it's not quite clear what to do. Do we integrate over the region $S$, taking the integrand (a 2-form) to a 1-form? If so, this leads me to conclude that the result be (something like) $$L \frac{dW}{d\phi} \, \text{d} \phi,$$ which, with the identification, would be $$L \frac{dW}{d\phi} \frac{d}{d\phi},$$ not what we expected. Where is my reasoning flawed? How do I obtain the result desired?","I am reading Witten's 1982 paper ""Supersymmetry and Morse Theory,"" and while I am slowly learning the material as I read through the paper, I have come across an equivalence that, while it should be rather basic, I can't seem to show rigorously. In the paper, Witten defines a (modified) exterior derivative $\text{d}_t$ as $$\text{d}_t = \text{d} + t\, \text{d} h$$ where $$h(\phi) = \int_S W(\phi(x)) \, \text{d}x,$$ the region $S$ is a circle with circumference $L$ and $\phi$ is a real-valued function from $S$ to the reals, i.e. $\phi: S \rightarrow \mathbb{R}$. Now, I am having trouble proving to myself that this exterior derivative $\text{d}_t$ is equivalent to the standard differential operator $$\frac{d}{d \phi} + t L \frac{dW}{d\phi}$$ acting on real-valued functions of the variable $\phi$. Now, as far as I can tell, Witten is making the identification $d \leftrightarrow \frac{d}{d \phi}$ here for this equivalency. However, given that, I can't seem to get the remaining term $t L \frac{dW}{d\phi}$. My attempt: We make the identification $\text{d} \leftrightarrow \frac{d}{d\phi}$, so the last term in the expression for $\text{d}_t$ is simply $\frac{dh}{d \phi}$. So, it should be as simple as evaluating $$\frac{d}{d\phi} \int_S W(\phi(x)) \, \text{d}x.$$ However, in evaluating this, the trouble I am running in to is that $\phi$ is an arbitrary function that is not necessarily invertible, so I cannot just rewrite the integral as an integral over $\phi$. I have also tried using the mechanics of exterior derivatives first, but then I run into the issue of evaluating $$\text{d} \int_S W(\phi(x)) \, \text{d}x,$$ which I'm not so sure where to begin (the exterior derivative is defined on the real line, not on $S$). Possibly, we can commute the derivative, which leaves us with $$\int_S \text{d} (W(\phi(x)) \, \text{d}x).$$ Then, using one of the identities of the exterior derivative yields $$\int_S \text{d} (W(\phi(x))) \land \text{d}x,$$ at which point I think the correct next step would be $$\int_S \frac{dW}{d\phi} \,\text{d} \phi \land \text{d}x.$$ From here, though, it's not quite clear what to do. Do we integrate over the region $S$, taking the integrand (a 2-form) to a 1-form? If so, this leads me to conclude that the result be (something like) $$L \frac{dW}{d\phi} \, \text{d} \phi,$$ which, with the identification, would be $$L \frac{dW}{d\phi} \frac{d}{d\phi},$$ not what we expected. Where is my reasoning flawed? How do I obtain the result desired?",,"['integration', 'differential-geometry', 'derivatives', 'differential-forms']"
66,Closed form for integral of integer powers of Sinc function,Closed form for integral of integer powers of Sinc function,,"( Edit: Thank you Vladimir for providing the references for the closed form value of the integrals. My revised question is then to how to derive this closed form.) For all $n\in\mathbb{N}^+$, define $\mathcal{I}_n$ by the definite integral,   $$\mathcal{I}_n:=\int_{0}^{\infty}\frac{\sin^n{(x)}}{x^n}\mathrm{d}x.$$   Prove that $\mathcal{I}_n$ has the following closed form:   $$\mathcal{I}_n\stackrel{?}=\pi\,2^{-n}\left(n\sum_{k=0}^{\lfloor\frac{n-1}{2}\rfloor}\frac{(-2)^k(n-2k)^{n-1}}{k!(n-k)!}\right),~~\forall n\in\mathbb{N}^+.$$ Integrals of small positive integer powers of the $\operatorname{sinc}$ function come up on a regular basis here, but it occurred to me that while I probably know the derivations for the $1\le n\le 4$ cases like the back of my hand, I can't recall ever working the integrals wfor any value of $n$ higher than that. The values of the first four integrals are, $$\mathcal{I}_1=\frac{\pi}{2},\\ \mathcal{I}_2=\frac{\pi}{2},\\ \mathcal{I}_3=\frac{3\pi}{8},\\ \mathcal{I}_4=\frac{\pi}{3}.$$ So I set out to first calculate $\mathcal{I}_5$ to see if any obvious pattern jumped out (and see if the trend of being equal to rational multiples of $\pi$ continued). I wound up getting frustrated and asking WolframAlpha instead. It turns that while the first four cases hinted very much at the possibility of a simple pattern relating the values of $\mathcal{I}_n$ for different positive integers $n$ (or possibly two separate patterns for even and odd $n$), the next few values most definitely did not: $$\mathcal{I}_5=\frac{115\pi}{384},\\ \mathcal{I}_6=\frac{11\pi}{40},\\ \mathcal{I}_7=\frac{5887\pi}{23040}\\ \mathcal{I}_8=\frac{151\pi}{630}.$$ So my questions are, 1) is there a systematic way to compute these integrals for all $n$?; and 2) is there an elegant way to represent these values in closed form for general $n$?","( Edit: Thank you Vladimir for providing the references for the closed form value of the integrals. My revised question is then to how to derive this closed form.) For all $n\in\mathbb{N}^+$, define $\mathcal{I}_n$ by the definite integral,   $$\mathcal{I}_n:=\int_{0}^{\infty}\frac{\sin^n{(x)}}{x^n}\mathrm{d}x.$$   Prove that $\mathcal{I}_n$ has the following closed form:   $$\mathcal{I}_n\stackrel{?}=\pi\,2^{-n}\left(n\sum_{k=0}^{\lfloor\frac{n-1}{2}\rfloor}\frac{(-2)^k(n-2k)^{n-1}}{k!(n-k)!}\right),~~\forall n\in\mathbb{N}^+.$$ Integrals of small positive integer powers of the $\operatorname{sinc}$ function come up on a regular basis here, but it occurred to me that while I probably know the derivations for the $1\le n\le 4$ cases like the back of my hand, I can't recall ever working the integrals wfor any value of $n$ higher than that. The values of the first four integrals are, $$\mathcal{I}_1=\frac{\pi}{2},\\ \mathcal{I}_2=\frac{\pi}{2},\\ \mathcal{I}_3=\frac{3\pi}{8},\\ \mathcal{I}_4=\frac{\pi}{3}.$$ So I set out to first calculate $\mathcal{I}_5$ to see if any obvious pattern jumped out (and see if the trend of being equal to rational multiples of $\pi$ continued). I wound up getting frustrated and asking WolframAlpha instead. It turns that while the first four cases hinted very much at the possibility of a simple pattern relating the values of $\mathcal{I}_n$ for different positive integers $n$ (or possibly two separate patterns for even and odd $n$), the next few values most definitely did not: $$\mathcal{I}_5=\frac{115\pi}{384},\\ \mathcal{I}_6=\frac{11\pi}{40},\\ \mathcal{I}_7=\frac{5887\pi}{23040}\\ \mathcal{I}_8=\frac{151\pi}{630}.$$ So my questions are, 1) is there a systematic way to compute these integrals for all $n$?; and 2) is there an elegant way to represent these values in closed form for general $n$?",,"['integration', 'definite-integrals', 'special-functions', 'improper-integrals', 'closed-form']"
67,"Integrate $\int x|J_1(x)|^2 \, dx$",Integrate,"\int x|J_1(x)|^2 \, dx","Hi I am trying to integrate: $$ \int_0^R x|J_1(bx)|^2 \, dx,\quad R>0, b\in \mathbb{C} $$ where $|J_1|^2=\bar{J}_1 J_1$ where $\bar{J_1}$ means complex conjugation of the bessel function. My attempt was $$ \int_0^R x J_1(bx) \bar{J_1}(bx) \, dx=\int_0^R x J_1(bx) J_1(\bar b x) \, dx. $$ if we now define $c\equiv \bar b$ we can write $$ \int_0^R x J_1(bx) {J_1}(\bar bx) \, dx=\int_0^R x J_1(bx) J_1(c x) \, dx, $$ but this integral can be done and is given by: $$ \int_0^R x J_1(bx) J_1(c x)dx=R\frac{cJ_1(bR)J_0(cR)-bJ_0(bR)J_1(cR)}{b^2-c^2}. $$ Is this correct, if so, is there anyway to simplify this? If not, how can we do this integral?  Is it possible? I feel like if it is not correct, maybe it is not okay to use $c\equiv \bar b$ and proceed with the integration.  Thanks!!","Hi I am trying to integrate: $$ \int_0^R x|J_1(bx)|^2 \, dx,\quad R>0, b\in \mathbb{C} $$ where $|J_1|^2=\bar{J}_1 J_1$ where $\bar{J_1}$ means complex conjugation of the bessel function. My attempt was $$ \int_0^R x J_1(bx) \bar{J_1}(bx) \, dx=\int_0^R x J_1(bx) J_1(\bar b x) \, dx. $$ if we now define $c\equiv \bar b$ we can write $$ \int_0^R x J_1(bx) {J_1}(\bar bx) \, dx=\int_0^R x J_1(bx) J_1(c x) \, dx, $$ but this integral can be done and is given by: $$ \int_0^R x J_1(bx) J_1(c x)dx=R\frac{cJ_1(bR)J_0(cR)-bJ_0(bR)J_1(cR)}{b^2-c^2}. $$ Is this correct, if so, is there anyway to simplify this? If not, how can we do this integral?  Is it possible? I feel like if it is not correct, maybe it is not okay to use $c\equiv \bar b$ and proceed with the integration.  Thanks!!",,"['calculus', 'integration', 'complex-analysis', 'special-functions', 'bessel-functions']"
68,On finding an explicit form of a particular recurrence relation,On finding an explicit form of a particular recurrence relation,,"Let $f$ be integrable over the interval $[0, 1]$, and $$I_n = \int_0^{1} x^n f(x) \, \mathrm{d}x.$$ Suppose $f(x) = f(1-x)$; we can then show that $$I_n = \sum_{k=0}^{n} \binom{n}{k} (-1)^k \, I_{k}, $$ by letting $u = 1-x$ and using the binomial theorem. My question is if we can extract an explicit form for $I_n$ from this recurrence relation. I don't mind if some of the constants involved (in particular, say $I_0$, $I_1$, $I_2$, etc.) are part of the explicit form, and I actually do expect those constants to pop up somewhere (not entirely sure how exactly, but that's my gut instinct). If not, then is there any existing literature that you would recommend me to look up that involve problems like this? Thank you.","Let $f$ be integrable over the interval $[0, 1]$, and $$I_n = \int_0^{1} x^n f(x) \, \mathrm{d}x.$$ Suppose $f(x) = f(1-x)$; we can then show that $$I_n = \sum_{k=0}^{n} \binom{n}{k} (-1)^k \, I_{k}, $$ by letting $u = 1-x$ and using the binomial theorem. My question is if we can extract an explicit form for $I_n$ from this recurrence relation. I don't mind if some of the constants involved (in particular, say $I_0$, $I_1$, $I_2$, etc.) are part of the explicit form, and I actually do expect those constants to pop up somewhere (not entirely sure how exactly, but that's my gut instinct). If not, then is there any existing literature that you would recommend me to look up that involve problems like this? Thank you.",,"['integration', 'definite-integrals', 'summation']"
69,Upper bounding a definite integral,Upper bounding a definite integral,,"So I have the following problem. Let $F$ be the set of functions for which $|f(x)| \le 2$  for all $x$ and $\int\limits_{0}^{5} [f(x)]^2dx \le 16$. Over all the functions in $F$, compute the maximum value of $\int\limits_{0}^{5} xf(x)dx$. My first intention to solve problem was to use the Cauchy-Schwarz Inequality. Therefore, if we let: $$I = \int\limits_{0}^{5} xf(x)dx$$ We can then say that: $$I^2 = \left[\int\limits_{0}^{5} xf(x)dx\right]^2$$ By the Cauchy-Schwarz Inequality, we have that: $$I^2 = \left[\int\limits_{0}^{5} xf(x)dx\right]^2 \le \int\limits_{0}^{5} x^2dx * \int\limits_{0}^{5} [f(x)]^2dx$$ We want the maximum value, so I let $\int\limits_{0}^{5} [f(x)]^2dx = 16$: $$I^2 = \left[\int\limits_{0}^{5} xf(x)dx\right]^2 \le \left[\frac{x^3}{3}\right]_{0}^{5} * 16$$ $$I^2 = \left[\int\limits_{0}^{5} xf(x)dx\right]^2 \le \frac{125}{3} * 16 = \frac{125*16}{3}$$ Therefore, I got $I$ to be: $$I = \frac{20\sqrt{15}}{3}$$ However, the answer provided by the solution was $24$. Was there an error in my calculations? I have an inkling that the first statement where $|f(x) \le 2|$ might be the reason that I cannot use the inequality, but I am not sure. Any help would be appreciated.","So I have the following problem. Let $F$ be the set of functions for which $|f(x)| \le 2$  for all $x$ and $\int\limits_{0}^{5} [f(x)]^2dx \le 16$. Over all the functions in $F$, compute the maximum value of $\int\limits_{0}^{5} xf(x)dx$. My first intention to solve problem was to use the Cauchy-Schwarz Inequality. Therefore, if we let: $$I = \int\limits_{0}^{5} xf(x)dx$$ We can then say that: $$I^2 = \left[\int\limits_{0}^{5} xf(x)dx\right]^2$$ By the Cauchy-Schwarz Inequality, we have that: $$I^2 = \left[\int\limits_{0}^{5} xf(x)dx\right]^2 \le \int\limits_{0}^{5} x^2dx * \int\limits_{0}^{5} [f(x)]^2dx$$ We want the maximum value, so I let $\int\limits_{0}^{5} [f(x)]^2dx = 16$: $$I^2 = \left[\int\limits_{0}^{5} xf(x)dx\right]^2 \le \left[\frac{x^3}{3}\right]_{0}^{5} * 16$$ $$I^2 = \left[\int\limits_{0}^{5} xf(x)dx\right]^2 \le \frac{125}{3} * 16 = \frac{125*16}{3}$$ Therefore, I got $I$ to be: $$I = \frac{20\sqrt{15}}{3}$$ However, the answer provided by the solution was $24$. Was there an error in my calculations? I have an inkling that the first statement where $|f(x) \le 2|$ might be the reason that I cannot use the inequality, but I am not sure. Any help would be appreciated.",,"['calculus', 'integration', 'inequality']"
70,What are the restrictions on using substitution in integration?,What are the restrictions on using substitution in integration?,,"What are the restrictions on using substitution in integration? For example, integration in this case may be done by the substitution $u=\tan x$ , as seen in the following $\textit{Mathematica}$ notebook Integration done by substitution $u=\tan {x\over 2}$ . The source function is a continuous positive function over $\mathbb {R}$ which implied by necessity that its antiderivative is continuous and increasing over $\mathbb {R}$ (which contradicts the graph of its analytical antiderivative that has jumb discontinuities). When I sent this question to Dr.Math, he replied - thanks to him - that the antiderivative of the function $ f(x)={1\over {1+{\sin x}^2}} $ is: $$ {\tan^{-1} (\sqrt 2 \tan x) \over \sqrt 2}  + \lfloor \frac x \pi-\frac 12 \rfloor * \frac{\pi}{\sqrt 2} + C $$ Which matches the ""True"" antiderivative of the function after redefining the integral at $( {\pi \over 2} + n.\pi) $ .  The second term has a derivative of $0$ over $ \Bbb R $ . This kind of ""weird"" functions makes me wonder about the ""Restrictions"" on using substitution when evaluating integrals, how to know that the function I work on needs refinements, and how to construct this ""True"" antiderivative ?","What are the restrictions on using substitution in integration? For example, integration in this case may be done by the substitution , as seen in the following notebook Integration done by substitution . The source function is a continuous positive function over which implied by necessity that its antiderivative is continuous and increasing over (which contradicts the graph of its analytical antiderivative that has jumb discontinuities). When I sent this question to Dr.Math, he replied - thanks to him - that the antiderivative of the function is: Which matches the ""True"" antiderivative of the function after redefining the integral at .  The second term has a derivative of over . This kind of ""weird"" functions makes me wonder about the ""Restrictions"" on using substitution when evaluating integrals, how to know that the function I work on needs refinements, and how to construct this ""True"" antiderivative ?",u=\tan x \textit{Mathematica} u=\tan {x\over 2} \mathbb {R} \mathbb {R}  f(x)={1\over {1+{\sin x}^2}}   {\tan^{-1} (\sqrt 2 \tan x) \over \sqrt 2}  + \lfloor \frac x \pi-\frac 12 \rfloor * \frac{\pi}{\sqrt 2} + C  ( {\pi \over 2} + n.\pi)  0  \Bbb R ,"['integration', 'continuity', 'substitution']"
71,When can we use Differentiation under the Integral sign?,When can we use Differentiation under the Integral sign?,,"Let me elaborate, 'Feynman's' trick ranks up in the top ten on most people's list, right behind contour integration, for best ways to evaluate definite integrals. However, unlike contour integration , this method doesn't really seem to have a users manual on how and when to use the method. In addition I rarely see the trick used on this site, and even when I do, it's usually in conjunction with contour integration or some other trick, so it makes it hard to see exactly what property is being exploited. Are there certain classes of integrals where using this method makes sense? In general, when is it best to use Feynman's trick? Here are my own thoughts. $e^{p \cdot x}$ is an important kernel used in the Laplace transform. In addition, it's derivative with respect to $p$ adds an $x$ term to the integral increasing the likelihood that a trick like integration by parts would work. Thus, functions with Laplace transforms might be solvable using Feynman Integration.","Let me elaborate, 'Feynman's' trick ranks up in the top ten on most people's list, right behind contour integration, for best ways to evaluate definite integrals. However, unlike contour integration , this method doesn't really seem to have a users manual on how and when to use the method. In addition I rarely see the trick used on this site, and even when I do, it's usually in conjunction with contour integration or some other trick, so it makes it hard to see exactly what property is being exploited. Are there certain classes of integrals where using this method makes sense? In general, when is it best to use Feynman's trick? Here are my own thoughts. $e^{p \cdot x}$ is an important kernel used in the Laplace transform. In addition, it's derivative with respect to $p$ adds an $x$ term to the integral increasing the likelihood that a trick like integration by parts would work. Thus, functions with Laplace transforms might be solvable using Feynman Integration.",,"['integration', 'soft-question']"
72,Geometric proof for Sophomore's dream,Geometric proof for Sophomore's dream,,"Is there a ""visual proof"" for sophomore's dream ? $$\int_0^1 x^{-x}\,dx = \sum_{n=1}^\infty n^{-n}.$$ In the wikipedia article there are two algebraic proofs, but the integral and the summation has a geometric interpretation. Because of that, I was thinking about it, that maybe there is a nice geometric proof for this statement.","Is there a ""visual proof"" for sophomore's dream ? $$\int_0^1 x^{-x}\,dx = \sum_{n=1}^\infty n^{-n}.$$ In the wikipedia article there are two algebraic proofs, but the integral and the summation has a geometric interpretation. Because of that, I was thinking about it, that maybe there is a nice geometric proof for this statement.",,"['calculus', 'integration', 'geometry', 'proof-writing']"
73,$\int_0^1 \frac{dx}{(1+x)e^x} $,,\int_0^1 \frac{dx}{(1+x)e^x} ,"How to integrate the folllowing: $$\int_0^1 \frac{1}{(1+x)e^x} \, dx $$ The major problem that I am facing is eliminating the exponential, I am unable to convert it into something else by substitution means.","How to integrate the folllowing: $$\int_0^1 \frac{1}{(1+x)e^x} \, dx $$ The major problem that I am facing is eliminating the exponential, I am unable to convert it into something else by substitution means.",,"['integration', 'definite-integrals']"
74,Area of two polar regions,Area of two polar regions,,"I'm trying to find the region inside r=sinθ and outside r=1+cosθ. My issue is my limits of integration. I get an intersection at $\frac π2$ and one at the pole. What are my limits for the integral? sinθ will intersect the pole at 0 or π but 1+cosθ only intersects the pole at π. From looking at the graph this is  integral I came up with: $$\frac12\int_{\frac π2}^π(sinθ)^2-(1+cosθ)^2dθ$$ Is this correct? This is the picture: How do I deal with intersections occurring at different angles? Edit: New picture, the red region is what I'm looking for.","I'm trying to find the region inside r=sinθ and outside r=1+cosθ. My issue is my limits of integration. I get an intersection at $\frac π2$ and one at the pole. What are my limits for the integral? sinθ will intersect the pole at 0 or π but 1+cosθ only intersects the pole at π. From looking at the graph this is  integral I came up with: $$\frac12\int_{\frac π2}^π(sinθ)^2-(1+cosθ)^2dθ$$ Is this correct? This is the picture: How do I deal with intersections occurring at different angles? Edit: New picture, the red region is what I'm looking for.",,"['calculus', 'integration', 'definite-integrals', 'polar-coordinates']"
75,"Find the closed form of $\int_0^{\large \frac{\pi}{2}}\frac{x^{2n}\cdot\log{{\sin{x}}}}{\sin^{2n}{x}}dx, \space n\ge 1$",Find the closed form of,"\int_0^{\large \frac{\pi}{2}}\frac{x^{2n}\cdot\log{{\sin{x}}}}{\sin^{2n}{x}}dx, \space n\ge 1","I was thinking of the generalization of the problem here , that is $$\int_0^{\large \frac{\pi}{2}}\frac{x^{2n}\cdot\log{{\sin{x}}}}{\sin^{2n}{x}}dx, \space n\ge 1$$ Maybe you recommend me some tools?","I was thinking of the generalization of the problem here , that is $$\int_0^{\large \frac{\pi}{2}}\frac{x^{2n}\cdot\log{{\sin{x}}}}{\sin^{2n}{x}}dx, \space n\ge 1$$ Maybe you recommend me some tools?",,"['calculus', 'real-analysis', 'integration', 'definite-integrals']"
76,Graphs of interesting integrals of the form: $\int \sin^a(x^a)\cos^a(x^a)$,Graphs of interesting integrals of the form:,\int \sin^a(x^a)\cos^a(x^a),"Here are a few graphs of the form:- $$\int \sin^a(x^a)\cos^a(x^a)dx$$ Where $a$ is an even, positive integer. $a = 2$ $a = 4$ $a = 6$ Now, a few graphs of the form:- $$\int \sin^a(x^a)\cos^a(x^a)dx$$ Where $a$ is an odd, positive integer. $a = 1$ (Common) $a = 3$ $a = 5$ $a = 7$ The integrals themselves are hideous.. But the graphs are fun to observe (At least for me), especially for the cases where a is odd. I know that the difference in these graphs (Between odd and even $a$ ) is caused mainly by the powers the sin and cos functions are raised to, rather than the powers of the arguments of these functions. I have 4 questions :- Why does $\int \sin^a(x^a)\cos^a(x^a)dx$ , where $a$ is even, have fewer and fewer oscillations (i.e. the line becomes less wavy) as the value of $a$ increases? (Note: I know that the lines come closer and closer to the x-axis as evidenced by the outputs of the integral on the graph) What is going on when $a$ is odd? Can you explain why that beaker-like structure is formed; specifically, why does it seems to dip around a certain value ,then raise again to oscillate so much and the die out? Some questions regarding an observation of the case $a = 5$: Why does there seem to be a tiny bump just after the '4' on the x-axis? What value could be causing this, and why? And when $a = 7$, why do there seem to be tiny successive bumps beyond 2 and -2? I realize that answers may not exist for all of these questions, because these maybe the intrinsic qualities of the graph of the integral in discussion, but I'm hoping that some people may have insights and explanations for the features of the graphs I've questioned about.... Especially questions 3 and 4. I hope you don't consider the question silly; I'm genuinely interested in knowing the reasons (If they exist) for the above graphs.","Here are a few graphs of the form:- $$\int \sin^a(x^a)\cos^a(x^a)dx$$ Where $a$ is an even, positive integer. $a = 2$ $a = 4$ $a = 6$ Now, a few graphs of the form:- $$\int \sin^a(x^a)\cos^a(x^a)dx$$ Where $a$ is an odd, positive integer. $a = 1$ (Common) $a = 3$ $a = 5$ $a = 7$ The integrals themselves are hideous.. But the graphs are fun to observe (At least for me), especially for the cases where a is odd. I know that the difference in these graphs (Between odd and even $a$ ) is caused mainly by the powers the sin and cos functions are raised to, rather than the powers of the arguments of these functions. I have 4 questions :- Why does $\int \sin^a(x^a)\cos^a(x^a)dx$ , where $a$ is even, have fewer and fewer oscillations (i.e. the line becomes less wavy) as the value of $a$ increases? (Note: I know that the lines come closer and closer to the x-axis as evidenced by the outputs of the integral on the graph) What is going on when $a$ is odd? Can you explain why that beaker-like structure is formed; specifically, why does it seems to dip around a certain value ,then raise again to oscillate so much and the die out? Some questions regarding an observation of the case $a = 5$: Why does there seem to be a tiny bump just after the '4' on the x-axis? What value could be causing this, and why? And when $a = 7$, why do there seem to be tiny successive bumps beyond 2 and -2? I realize that answers may not exist for all of these questions, because these maybe the intrinsic qualities of the graph of the integral in discussion, but I'm hoping that some people may have insights and explanations for the features of the graphs I've questioned about.... Especially questions 3 and 4. I hope you don't consider the question silly; I'm genuinely interested in knowing the reasons (If they exist) for the above graphs.",,"['calculus', 'integration', 'trigonometry', 'graphing-functions']"
77,"Am not getting the right answer for $I = \int\limits_{S_\epsilon} \frac{x \,dy\,dz + y \,dx\,dz + z \,dx\,dy}{(x^2+y^2+z^2)^{\frac32}}$",Am not getting the right answer for,"I = \int\limits_{S_\epsilon} \frac{x \,dy\,dz + y \,dx\,dz + z \,dx\,dy}{(x^2+y^2+z^2)^{\frac32}}","I need to perform the following integration $$I = \int\limits_{S_\epsilon} \frac{x \,dy\,dz + y \,dx\,dz + z \,dx\,dy}{(x^2+y^2+z^2)^{\frac32}},$$ where $S_{\epsilon}$ is a sphere of radius $\epsilon$ around the origin. So $(x^2+y^2+z^2)^{\frac12} = \epsilon$. By symmetry the integral reduces to $$I=3\epsilon ^{-3}  \int_{S_\epsilon} z\,dx\,dy =3\epsilon ^{-3} \int_{B_\epsilon} \pm\sqrt{\epsilon ^2-x^2-y^2}\,dx\,dy .   $$ Here $B_{\epsilon}$ is a disc in the $xy$ plane of radius $\epsilon$. Now considering the plus or minus sign this would mean that $I = 0$ right? I think I made a mistake somewhere because the answer should be $4\pi$. I found $$ \int_{B_\epsilon} \sqrt{\epsilon ^2 -x^2 -y^2}\,dx\,dy = \frac{2\pi \epsilon^3}{3}. $$ I think I am missing something somwhere. Thanks in advance!","I need to perform the following integration $$I = \int\limits_{S_\epsilon} \frac{x \,dy\,dz + y \,dx\,dz + z \,dx\,dy}{(x^2+y^2+z^2)^{\frac32}},$$ where $S_{\epsilon}$ is a sphere of radius $\epsilon$ around the origin. So $(x^2+y^2+z^2)^{\frac12} = \epsilon$. By symmetry the integral reduces to $$I=3\epsilon ^{-3}  \int_{S_\epsilon} z\,dx\,dy =3\epsilon ^{-3} \int_{B_\epsilon} \pm\sqrt{\epsilon ^2-x^2-y^2}\,dx\,dy .   $$ Here $B_{\epsilon}$ is a disc in the $xy$ plane of radius $\epsilon$. Now considering the plus or minus sign this would mean that $I = 0$ right? I think I made a mistake somewhere because the answer should be $4\pi$. I found $$ \int_{B_\epsilon} \sqrt{\epsilon ^2 -x^2 -y^2}\,dx\,dy = \frac{2\pi \epsilon^3}{3}. $$ I think I am missing something somwhere. Thanks in advance!",,"['integration', 'definite-integrals']"
78,"How can I write fun, cool, and challenging integration problems?","How can I write fun, cool, and challenging integration problems?",,"I just asked this question on another site but with only a handful of somewhat helpful responses. I just made the following integral problem which I think is very cool and nice, but I don't know how to solve it. (I found it by taking the derivative which is what you see here.) $\int \left(\frac{x^2-2}{x^3+6x} \right)^2 dx$. (I won't tell you the answer.) Once I had obtained this nice looking derivative and turned it into a nice looking integration problem I wanted to make more such problems, but the functions I was deriving didn't really seem to have challenging nor interesting derivatives. How can I come up with some fun integration problems (definite or indefinite)? I should also mention that my knowledge of calculus does not include complex analysis such as the Residues things or the contour theorems, only basic Calc 2 level stuff (eg. taylor series, integration by parts, integration tables from my physics textbook as I don't know where my calculus one is.)","I just asked this question on another site but with only a handful of somewhat helpful responses. I just made the following integral problem which I think is very cool and nice, but I don't know how to solve it. (I found it by taking the derivative which is what you see here.) $\int \left(\frac{x^2-2}{x^3+6x} \right)^2 dx$. (I won't tell you the answer.) Once I had obtained this nice looking derivative and turned it into a nice looking integration problem I wanted to make more such problems, but the functions I was deriving didn't really seem to have challenging nor interesting derivatives. How can I come up with some fun integration problems (definite or indefinite)? I should also mention that my knowledge of calculus does not include complex analysis such as the Residues things or the contour theorems, only basic Calc 2 level stuff (eg. taylor series, integration by parts, integration tables from my physics textbook as I don't know where my calculus one is.)",,['integration']
79,Is every Volterra's function unbounded?,Is every Volterra's function unbounded?,,"Volterra's function is a function $f\colon\mathbb{R}\to\mathbb{R}$ such that: $V$ is differentiable, $V'$ is bounded, $V'$ is not Riemann-integrable. http://en.wikipedia.org/wiki/Volterra%27s_function Is every Volterra's function unbounded? I've searched the site and found some results, like What is an example that a function is differentiable but derivative is not Riemann integrable Bounded Function Which is Not Riemann Integrable but is doesn't deal with boundedness of a function.","Volterra's function is a function such that: is differentiable, is bounded, is not Riemann-integrable. http://en.wikipedia.org/wiki/Volterra%27s_function Is every Volterra's function unbounded? I've searched the site and found some results, like What is an example that a function is differentiable but derivative is not Riemann integrable Bounded Function Which is Not Riemann Integrable but is doesn't deal with boundedness of a function.",f\colon\mathbb{R}\to\mathbb{R} V V' V',"['real-analysis', 'integration', 'derivatives']"
80,Bochner integral vs regulated integral,Bochner integral vs regulated integral,,"I'm reading Serge Lang's Real And Functional Analysis and at some point he introduces the regulated integral in order to prove the Fundamental Theorem Of Calculus (in the context of Banach Spaces), or at least that seems like the only application where he uses regulated integrals. He also introduced the Bochner integral in the same book some chapters earlier, so I wondered, is there some reason why to switch to the regulated integral from the Bochner integral or does it only make the proof of the Fundamental Theorem easier?","I'm reading Serge Lang's Real And Functional Analysis and at some point he introduces the regulated integral in order to prove the Fundamental Theorem Of Calculus (in the context of Banach Spaces), or at least that seems like the only application where he uses regulated integrals. He also introduced the Bochner integral in the same book some chapters earlier, so I wondered, is there some reason why to switch to the regulated integral from the Bochner integral or does it only make the proof of the Fundamental Theorem easier?",,"['integration', 'derivatives', 'banach-spaces', 'riemann-sum']"
81,"If $f$ is integrable, then $\sum\limits_{n\ge 1}\frac{1}{\sqrt n}\vert f(x-\sqrt n)\vert$ is almost everywhere finite","If  is integrable, then  is almost everywhere finite",f \sum\limits_{n\ge 1}\frac{1}{\sqrt n}\vert f(x-\sqrt n)\vert,"I would like to show that $$\sum_{n\ge0}\left\vert \frac{1}{\sqrt n} f \left(x-\sqrt n \right)\right\vert \tag{$*$}$$ converges for almost every (a.e.) $x$. The only technique I have is based on the answer to If $f\in L^1(\mathbb{R})$, then $\sum_{n\ge 1}f(x+n)$ Converges for a.e. $x$. , but I think some additional technique is needed. In that question I was hoping for a technique general enough to answer both questions, but the $\sqrt \cdot$ is throwing me off. Here's what I have: Given an integer $k$, $\int_k^{k+1}\sum_{n\ge0}\vert \frac{1}{n}f(x-\sqrt n)\vert=\sum_{n\ge 0}\int_k^{k+1}\frac{1}{\sqrt n}\vert f(x-\sqrt n)\vert$. The idea is to show that this integral is finite (implying the sum $(*)$ converges on $(k,k+1)$) and use the fact that $k$ is arbitrary to conclude that the sum converges a.e. on $\mathbb{R}$. A naive attempt would be to bound the RHS by $\sum_{n\ge 0}\int_k^{k+1}\vert f(x-\sqrt n)=\sum_{n\ge 0}\int_{-\sqrt n - k}^{-\sqrt n - k +1}\vert f(x) \vert$, but the latest expression is larger than $\int_{-\infty }^\infty\vert f\vert$. I don't have a nice way to (a) deal with the $1/\sqrt n$ and (b) deal with the limits of integration, but they can probably be dealt with simultaneously I just don't see how.","I would like to show that $$\sum_{n\ge0}\left\vert \frac{1}{\sqrt n} f \left(x-\sqrt n \right)\right\vert \tag{$*$}$$ converges for almost every (a.e.) $x$. The only technique I have is based on the answer to If $f\in L^1(\mathbb{R})$, then $\sum_{n\ge 1}f(x+n)$ Converges for a.e. $x$. , but I think some additional technique is needed. In that question I was hoping for a technique general enough to answer both questions, but the $\sqrt \cdot$ is throwing me off. Here's what I have: Given an integer $k$, $\int_k^{k+1}\sum_{n\ge0}\vert \frac{1}{n}f(x-\sqrt n)\vert=\sum_{n\ge 0}\int_k^{k+1}\frac{1}{\sqrt n}\vert f(x-\sqrt n)\vert$. The idea is to show that this integral is finite (implying the sum $(*)$ converges on $(k,k+1)$) and use the fact that $k$ is arbitrary to conclude that the sum converges a.e. on $\mathbb{R}$. A naive attempt would be to bound the RHS by $\sum_{n\ge 0}\int_k^{k+1}\vert f(x-\sqrt n)=\sum_{n\ge 0}\int_{-\sqrt n - k}^{-\sqrt n - k +1}\vert f(x) \vert$, but the latest expression is larger than $\int_{-\infty }^\infty\vert f\vert$. I don't have a nice way to (a) deal with the $1/\sqrt n$ and (b) deal with the limits of integration, but they can probably be dealt with simultaneously I just don't see how.",,"['real-analysis', 'integration', 'summation']"
82,$f$ is Riemann integrable iif the set of discontinuous points of $f$ has Lebesgue measure zero,is Riemann integrable iif the set of discontinuous points of  has Lebesgue measure zero,f f,"This is a well known result in mathematics, but it's my first time attempting to prove it. I'm following the second book of Analysis from Folland. Below are the notations used and the theorem, from the text. Now we have the following exercise, which is supposed to led us to prove the part b of this theorem. I managed to prove the part a of this exercise, but part b is hard to do. The best idea I have so far is to write for each $x\in[a,b]$, $$\lim_{\delta\to 0}\sup_{|x-y|\leq\delta}f(y) = \lim_{n\to\infty}\sup_{|x-y|\leq 1/n}f(y)$$ and note that $\sup_{|x-y|\leq 1/n}f(y) = M_i$ for some convenient partition chosen. In fact, for each $n\in\mathbb{N}$ we may choose the partition $P_n$ so it there is always a $M_i$ equal to $\sup_{|x-y|\leq 1/n}f(y)$. The problem is that there is uncountable sup's for $H$ while there is countable for $G_{P_n}$. So I don't know how this approach will (if it will) work. I need some help here, thanks.","This is a well known result in mathematics, but it's my first time attempting to prove it. I'm following the second book of Analysis from Folland. Below are the notations used and the theorem, from the text. Now we have the following exercise, which is supposed to led us to prove the part b of this theorem. I managed to prove the part a of this exercise, but part b is hard to do. The best idea I have so far is to write for each $x\in[a,b]$, $$\lim_{\delta\to 0}\sup_{|x-y|\leq\delta}f(y) = \lim_{n\to\infty}\sup_{|x-y|\leq 1/n}f(y)$$ and note that $\sup_{|x-y|\leq 1/n}f(y) = M_i$ for some convenient partition chosen. In fact, for each $n\in\mathbb{N}$ we may choose the partition $P_n$ so it there is always a $M_i$ equal to $\sup_{|x-y|\leq 1/n}f(y)$. The problem is that there is uncountable sup's for $H$ while there is countable for $G_{P_n}$. So I don't know how this approach will (if it will) work. I need some help here, thanks.",,"['integration', 'measure-theory', 'lebesgue-measure']"
83,Find values so that integral is a bounded operator,Find values so that integral is a bounded operator,,"Find all positive values $\alpha$ for which the formula \begin{align*} A_\alpha u(x) = \int\limits_0^1 \frac{u(y)}{(x+y)^\alpha} \,dy  \end{align*} defines a bounded operator in $L^1([0,1])$.  Compute its norm. I know that this defines a bounded operator for $\alpha <1$.  What I did will not work for $\alpha \geq 1$.  I tired to use specific $L^1$ functions but its not working.  Do you have any advice on how to approach these types of problems? \begin{align*} ||A_\alpha u(x)||_1 & = \int\limits_0^1\left| \int\limits_0^1 \frac{u(y)}{(x+y)^{\alpha}}\, dy \right|\,dx \\ & \leq \int\limits_0^1 \int\limits_0^1 \frac{|u(y)|}{(x+y)^{\alpha}}\,dy\, dx \\ & \leq \int\limits_0^1 \int\limits_0^1 \frac{|u(y)|}{x^{\alpha}}\,dy\, dx\\ & =\int\limits_0^1 ||u||_1 \frac{1}{x^{\alpha}}\, dx\\ \end{align*} The integral above converges if $\alpha <1$.","Find all positive values $\alpha$ for which the formula \begin{align*} A_\alpha u(x) = \int\limits_0^1 \frac{u(y)}{(x+y)^\alpha} \,dy  \end{align*} defines a bounded operator in $L^1([0,1])$.  Compute its norm. I know that this defines a bounded operator for $\alpha <1$.  What I did will not work for $\alpha \geq 1$.  I tired to use specific $L^1$ functions but its not working.  Do you have any advice on how to approach these types of problems? \begin{align*} ||A_\alpha u(x)||_1 & = \int\limits_0^1\left| \int\limits_0^1 \frac{u(y)}{(x+y)^{\alpha}}\, dy \right|\,dx \\ & \leq \int\limits_0^1 \int\limits_0^1 \frac{|u(y)|}{(x+y)^{\alpha}}\,dy\, dx \\ & \leq \int\limits_0^1 \int\limits_0^1 \frac{|u(y)|}{x^{\alpha}}\,dy\, dx\\ & =\int\limits_0^1 ||u||_1 \frac{1}{x^{\alpha}}\, dx\\ \end{align*} The integral above converges if $\alpha <1$.",,"['integration', 'analysis']"
84,"Find a function that maximizes $\int_{0}^{1}f(x)\,\rm dx$ with given constraints",Find a function that maximizes  with given constraints,"\int_{0}^{1}f(x)\,\rm dx","Find a function $f(x)$ that maximizes the following integral $$\max\int_{0}^{1}f(x)\,\rm dx\quad \text{s.t.}\quad \frac{d}{dx}ln(f(x))<0$$ $f(x)$ also continues, $f:[0,1]\rightarrow R$ and we know that $f(0)=a$, $f(1)=b$ and $a>b>0$. I think that the function f(x) that maximizes this integral in the interval $[0,1]$ with the given constraints is a linear function: $f(x)=(b-a)x+a$ and this integral would be: $$\frac{a}{2}+\frac{b}{2}$$ Am I right?","Find a function $f(x)$ that maximizes the following integral $$\max\int_{0}^{1}f(x)\,\rm dx\quad \text{s.t.}\quad \frac{d}{dx}ln(f(x))<0$$ $f(x)$ also continues, $f:[0,1]\rightarrow R$ and we know that $f(0)=a$, $f(1)=b$ and $a>b>0$. I think that the function f(x) that maximizes this integral in the interval $[0,1]$ with the given constraints is a linear function: $f(x)=(b-a)x+a$ and this integral would be: $$\frac{a}{2}+\frac{b}{2}$$ Am I right?",,"['integration', 'optimization', 'definite-integrals']"
85,Evaluate $\int_1^\infty\frac{e^{-\large i\sigma x}}{(-x-\sqrt{x^2-1})^n}dx$,Evaluate,\int_1^\infty\frac{e^{-\large i\sigma x}}{(-x-\sqrt{x^2-1})^n}dx,"thank you very much for helping me with the following integral. $$\int_1^\infty\frac{e^{-\large i\sigma x}}{(-x-\sqrt{x^2-1})^n}dx$$ I have tried to find out if maybe the integral can be expressed in terms of the modified Bessel functions of the second kind, however I haven't been able to.  I also tried to do the integral using differentiation under the integral sign, but still I can't find a solution.","thank you very much for helping me with the following integral. $$\int_1^\infty\frac{e^{-\large i\sigma x}}{(-x-\sqrt{x^2-1})^n}dx$$ I have tried to find out if maybe the integral can be expressed in terms of the modified Bessel functions of the second kind, however I haven't been able to.  I also tried to do the integral using differentiation under the integral sign, but still I can't find a solution.",,"['calculus', 'integration']"
86,using complex or real analysis solve $\int_{0}^{\pi/2}\frac{x^m}{\sin x}dx$ [duplicate],using complex or real analysis solve  [duplicate],\int_{0}^{\pi/2}\frac{x^m}{\sin x}dx,"This question already has answers here : Evaluating $\int_{0}^{\frac{\pi}{2}} x^n \csc(x) dx$ (4 answers) Closed last year . closed form for $$\int_{0}^{\frac{\pi}{2}}\frac{x^m}{\sin x}\ dx$$ I slove it for some m but in general i failed. I tried by part , by substitution,by using $\sin x =\frac{e^{ix}-e^{-ix}}{2i}$ . I guess that $\sin x =\frac{e^{ix}-e^{-ix}}{2i}$ then using geometric series lead to the answer,I used it and got something it seems related to Zeta function. can any one solve it using residue theorem or using real analysis ?","This question already has answers here : Evaluating $\int_{0}^{\frac{\pi}{2}} x^n \csc(x) dx$ (4 answers) Closed last year . closed form for $$\int_{0}^{\frac{\pi}{2}}\frac{x^m}{\sin x}\ dx$$ I slove it for some m but in general i failed. I tried by part , by substitution,by using $\sin x =\frac{e^{ix}-e^{-ix}}{2i}$ . I guess that $\sin x =\frac{e^{ix}-e^{-ix}}{2i}$ then using geometric series lead to the answer,I used it and got something it seems related to Zeta function. can any one solve it using residue theorem or using real analysis ?",,"['integration', 'contour-integration', 'closed-form']"
87,Yet another multivariable integral over a simplex,Yet another multivariable integral over a simplex,,"Let $p$ be a positive integer, let $(q_0,q_1,\cdots,q_p)$ be a sequence of positive integers and let $\beta \neq 1/2$ be a positive number. Then let $B>A>0$.  The question is to prove the following identity for a integration over a $p$ dimensional  simplex. \begin{eqnarray} &&J^{(A,B)}_{\vec{q},p} := \int\limits_{A\le \xi_0 \le \xi_ \le \cdots \le \xi_{p-1} \le B}  \prod\limits_{j=0}^p \left(\xi_{j-1} - \xi_j\right)^{q_j} \cdot  \prod\limits_{j=0}^{p-1} \frac{d \xi_j}{\xi_j^{q_{j+1} 2 \beta}}=\\ && (-1)^{Q_P} A^{q_0} \cdot \sum\limits_{m=0}^p \sum\limits_{l=0}^{q_m}  \frac{(-1)^{l+q_m} \binom{q_m}{l} }{ \prod\limits_{\stackrel{\eta=0}{\eta\neq m}}^p\left[(q_\eta+1) \binom{l+\eta-m-(1-2\beta)(Q_m-Q_\eta)}{q_\eta+1}\right]} \cdot A^{(1-2\beta)(Q_m-Q_0)+m-l} B^{(1-2\beta)(Q_p-Q_m)+p-m+l} \end{eqnarray} Here $\xi_{-1}=A$ and $\xi_{p}=B$ and $Q_j := q_0+q_1+\cdots+q_j$.  The number of terms on the right hand side equals $q_0+q_1+\cdots+q_p+(p+1)$. Note 1: In the case $q_0=q_1=\cdots=q_p=q$  and $\beta :> \beta/q$ we retrieve the result given in Another mutivariable integral over a simplex . Note 2: Setting $\beta = 0$ we get  \begin{equation} rhs = -\left(A-B\right)^{p + \sum\limits_{\eta=0}^p q_\eta} \cdot \frac{\prod\limits_{\eta=0}^p q_\eta!}{\left(p + \sum\limits_{\eta=0}^p q_\eta\right)!} \end{equation} as it should be. We also provide a similar integral below where the values of exponents in the denominator are all arbitrary. Let $(q_1,\cdots,q_{p+1})$ be positive integers and let $(\beta_1,\cdots,\beta_p)$ be positive reals numbers. Then we have: \begin{eqnarray} &&I^{(A,B)}_{\vec{q},p} := \int\limits_{A\le \xi_0 \le \xi_ \le \cdots \le \xi_{p-1} \le B}  \prod\limits_{j=0}^p \left(\xi_{j-1} - \xi_j\right)^{q_{j+1}} \cdot  \prod\limits_{j=0}^{p-1} \frac{d \xi_j}{\xi_j^{\beta_{j+1}}}=  (-1)^{Q_{p+1}} \sum\limits_{m=1}^{p+1} \sum\limits_{l=0}^{q_m}\\ && \frac{(-1)^{l+q_m} \binom{q_m}{l}  }{\prod\limits_{\stackrel{j=1}{j\neq m}}^{p+1} (q_j+1) \binom{l+j-m+Q_j-Q_m+B_{m-1}-B_{j-1}}{q_j+1} }  A^{Q_m-B_{m-1}+m-1-l} B^{Q_{p+1}-Q_m-B_p+ B_{m-1}+p-m+1+l}  \end{eqnarray} Here $\xi_{-1}=A$ and $\xi_{p}=B$ and $Q_j := q_1+q_2+\cdots+q_j$ and $B_j := \beta_1+\beta_2+\cdots+\beta_j$. Now we check particular cases of the formula above. Note 1: Let $q_1=q_2=\cdots=q_{p+1}=0$ and $\beta_j = \beta$ for $j=1,\cdots,p$. Then: \begin{equation} rhs = \frac{\left(A^{1-\beta} - B^{1-\beta}\right)^p}{p! (-1+\beta)^p} \end{equation} as it should be. Note 2: Now, let $q_1=q_2=\cdots=q_{p+1}=0$ and all the beta values be arbitrary. Then: \begin{equation} rhs = \sum\limits_{m=0}^p \frac{(-1)^m  A^{-B_m+m} B^{-(B_p-B_m)+p-m} }{\prod\limits_{j=1}^m (j+ B_{m-j}-B_m) \prod\limits_{j=1}^{p-m}(j+B_m - B_{m+j})} \end{equation} Note 3: Finally, let  $q_1=q_2=\cdots=q_{p+1}=1$ and all the beta values be arbitrary. Then we have: \begin{eqnarray} rhs = (-1)^{p+1}\sum\limits_{l=0}^1 (-1)^{l+1} \sum\limits_{m=0}^p  \frac{ A^{-B_m+2 m + 1-l} B^{-(B_p-B_m)+2(p-m)+l} }{\prod\limits_{j=1}^m (2j - l + B_{m-j}-B_m)_{(2)} \prod\limits_{j=1}^{p-m}(2j + l +B_m - B_{m+j})_{(2)}} \end{eqnarray}","Let $p$ be a positive integer, let $(q_0,q_1,\cdots,q_p)$ be a sequence of positive integers and let $\beta \neq 1/2$ be a positive number. Then let $B>A>0$.  The question is to prove the following identity for a integration over a $p$ dimensional  simplex. \begin{eqnarray} &&J^{(A,B)}_{\vec{q},p} := \int\limits_{A\le \xi_0 \le \xi_ \le \cdots \le \xi_{p-1} \le B}  \prod\limits_{j=0}^p \left(\xi_{j-1} - \xi_j\right)^{q_j} \cdot  \prod\limits_{j=0}^{p-1} \frac{d \xi_j}{\xi_j^{q_{j+1} 2 \beta}}=\\ && (-1)^{Q_P} A^{q_0} \cdot \sum\limits_{m=0}^p \sum\limits_{l=0}^{q_m}  \frac{(-1)^{l+q_m} \binom{q_m}{l} }{ \prod\limits_{\stackrel{\eta=0}{\eta\neq m}}^p\left[(q_\eta+1) \binom{l+\eta-m-(1-2\beta)(Q_m-Q_\eta)}{q_\eta+1}\right]} \cdot A^{(1-2\beta)(Q_m-Q_0)+m-l} B^{(1-2\beta)(Q_p-Q_m)+p-m+l} \end{eqnarray} Here $\xi_{-1}=A$ and $\xi_{p}=B$ and $Q_j := q_0+q_1+\cdots+q_j$.  The number of terms on the right hand side equals $q_0+q_1+\cdots+q_p+(p+1)$. Note 1: In the case $q_0=q_1=\cdots=q_p=q$  and $\beta :> \beta/q$ we retrieve the result given in Another mutivariable integral over a simplex . Note 2: Setting $\beta = 0$ we get  \begin{equation} rhs = -\left(A-B\right)^{p + \sum\limits_{\eta=0}^p q_\eta} \cdot \frac{\prod\limits_{\eta=0}^p q_\eta!}{\left(p + \sum\limits_{\eta=0}^p q_\eta\right)!} \end{equation} as it should be. We also provide a similar integral below where the values of exponents in the denominator are all arbitrary. Let $(q_1,\cdots,q_{p+1})$ be positive integers and let $(\beta_1,\cdots,\beta_p)$ be positive reals numbers. Then we have: \begin{eqnarray} &&I^{(A,B)}_{\vec{q},p} := \int\limits_{A\le \xi_0 \le \xi_ \le \cdots \le \xi_{p-1} \le B}  \prod\limits_{j=0}^p \left(\xi_{j-1} - \xi_j\right)^{q_{j+1}} \cdot  \prod\limits_{j=0}^{p-1} \frac{d \xi_j}{\xi_j^{\beta_{j+1}}}=  (-1)^{Q_{p+1}} \sum\limits_{m=1}^{p+1} \sum\limits_{l=0}^{q_m}\\ && \frac{(-1)^{l+q_m} \binom{q_m}{l}  }{\prod\limits_{\stackrel{j=1}{j\neq m}}^{p+1} (q_j+1) \binom{l+j-m+Q_j-Q_m+B_{m-1}-B_{j-1}}{q_j+1} }  A^{Q_m-B_{m-1}+m-1-l} B^{Q_{p+1}-Q_m-B_p+ B_{m-1}+p-m+1+l}  \end{eqnarray} Here $\xi_{-1}=A$ and $\xi_{p}=B$ and $Q_j := q_1+q_2+\cdots+q_j$ and $B_j := \beta_1+\beta_2+\cdots+\beta_j$. Now we check particular cases of the formula above. Note 1: Let $q_1=q_2=\cdots=q_{p+1}=0$ and $\beta_j = \beta$ for $j=1,\cdots,p$. Then: \begin{equation} rhs = \frac{\left(A^{1-\beta} - B^{1-\beta}\right)^p}{p! (-1+\beta)^p} \end{equation} as it should be. Note 2: Now, let $q_1=q_2=\cdots=q_{p+1}=0$ and all the beta values be arbitrary. Then: \begin{equation} rhs = \sum\limits_{m=0}^p \frac{(-1)^m  A^{-B_m+m} B^{-(B_p-B_m)+p-m} }{\prod\limits_{j=1}^m (j+ B_{m-j}-B_m) \prod\limits_{j=1}^{p-m}(j+B_m - B_{m+j})} \end{equation} Note 3: Finally, let  $q_1=q_2=\cdots=q_{p+1}=1$ and all the beta values be arbitrary. Then we have: \begin{eqnarray} rhs = (-1)^{p+1}\sum\limits_{l=0}^1 (-1)^{l+1} \sum\limits_{m=0}^p  \frac{ A^{-B_m+2 m + 1-l} B^{-(B_p-B_m)+2(p-m)+l} }{\prod\limits_{j=1}^m (2j - l + B_{m-j}-B_m)_{(2)} \prod\limits_{j=1}^{p-m}(2j + l +B_m - B_{m+j})_{(2)}} \end{eqnarray}",,['integration']
88,Find center of mass of triangle with varying density,Find center of mass of triangle with varying density,,"My objective: Find the center of mass of a thin triangular plate bounded by the y-axis and the lines  $y= 7x+3$ and $y= 36-4x$. Assume that the density is given by $\delta(x,y) =  7x+2y+2$. In class we were given the formula $\bar x = \frac{\iiint_V x\delta dV}{\iiint_V \delta dV}$ to find the x center, so I set it up like $\bar x = \frac{\int_{0}^{3}\int_{0}^{36} x(7x+2y+2)\,dydx}{\int_{0}^{3}\int_{0}^{36} (7x+2y+2)\,dydx} = \frac{156}{97}$, but that isn't correct. Is this approach wrong or did I make a mistake somewhere?","My objective: Find the center of mass of a thin triangular plate bounded by the y-axis and the lines  $y= 7x+3$ and $y= 36-4x$. Assume that the density is given by $\delta(x,y) =  7x+2y+2$. In class we were given the formula $\bar x = \frac{\iiint_V x\delta dV}{\iiint_V \delta dV}$ to find the x center, so I set it up like $\bar x = \frac{\int_{0}^{3}\int_{0}^{36} x(7x+2y+2)\,dydx}{\int_{0}^{3}\int_{0}^{36} (7x+2y+2)\,dydx} = \frac{156}{97}$, but that isn't correct. Is this approach wrong or did I make a mistake somewhere?",,"['calculus', 'integration', 'multivariable-calculus', 'average']"
89,Which integrals can be solved using Feynman's Technique?,Which integrals can be solved using Feynman's Technique?,,How to check whether an integral can be easily solved using Feynman's approach. What are the main criteria needed to be taken into account?,How to check whether an integral can be easily solved using Feynman's approach. What are the main criteria needed to be taken into account?,,['integration']
90,Multiple Integration order doesn't agree.,Multiple Integration order doesn't agree.,,"Let $0<x,y,t,z<1$ with the additional condition: $$\begin{align*} x &< t\\ \wedge & \ \\ y &<z \end{align*}$$ Call the set of all $x,y,t,z$ satisfying the above conditions $S$. I want to evaluate $\int_S dxdydtdz$. One way of doing it is first integrating out $z$ and then integrating along columns down up. This ordering gives: $$\int_0^1\int_0^t\int_x^1\int_y^1 dzdydxdt=\int_0^1\int_0^t\int_x^1(1-y) dydxdt=\frac{1}{8}.$$ Another way is to just integrate up along columns without first integrating $z$: $$\int_0^1\int_0^1\int_0^t\int_x^zdydxdzdt=\frac{1}{12}.$$ Why do these not agree? It seems like the second way is wrong. I think maybe in the second way one needs to integrate $x$ from $0$ to $\min(x,z)$, but then again I thought the $y$ variable ensures this.","Let $0<x,y,t,z<1$ with the additional condition: $$\begin{align*} x &< t\\ \wedge & \ \\ y &<z \end{align*}$$ Call the set of all $x,y,t,z$ satisfying the above conditions $S$. I want to evaluate $\int_S dxdydtdz$. One way of doing it is first integrating out $z$ and then integrating along columns down up. This ordering gives: $$\int_0^1\int_0^t\int_x^1\int_y^1 dzdydxdt=\int_0^1\int_0^t\int_x^1(1-y) dydxdt=\frac{1}{8}.$$ Another way is to just integrate up along columns without first integrating $z$: $$\int_0^1\int_0^1\int_0^t\int_x^zdydxdzdt=\frac{1}{12}.$$ Why do these not agree? It seems like the second way is wrong. I think maybe in the second way one needs to integrate $x$ from $0$ to $\min(x,z)$, but then again I thought the $y$ variable ensures this.",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus']"
91,Integration using exponent,Integration using exponent,,What could be the techniques we need to use to solve this integration $\displaystyle \int\tan^2\theta\frac{\sin^2(\sec\theta\tan\theta)}{\sec^2\theta}d\theta \tag1$? How do I convert this in to a solvable standard form ?,What could be the techniques we need to use to solve this integration $\displaystyle \int\tan^2\theta\frac{\sin^2(\sec\theta\tan\theta)}{\sec^2\theta}d\theta \tag1$? How do I convert this in to a solvable standard form ?,,"['calculus', 'integration', 'indefinite-integrals']"
92,Solving an integral (using Cauchy contour integral?),Solving an integral (using Cauchy contour integral?),,"I need to solve this integral:  \begin{equation} f(t)=\int_0^\infty x^2 \sqrt x \left( e^{a x} -1\right)^{-1/2} \frac{e^{i(b-x)t}-1}{b-x} dx \end{equation} where $a$ and $b$ are real, positive constants. I was thinking Cauchy contour integral might help.. Any help/advice is welcome!","I need to solve this integral:  \begin{equation} f(t)=\int_0^\infty x^2 \sqrt x \left( e^{a x} -1\right)^{-1/2} \frac{e^{i(b-x)t}-1}{b-x} dx \end{equation} where $a$ and $b$ are real, positive constants. I was thinking Cauchy contour integral might help.. Any help/advice is welcome!",,"['calculus', 'integration', 'contour-integration']"
93,Prove the exsistence of 3 zero points of a function,Prove the exsistence of 3 zero points of a function,,"Assuming $a<b$, function f(x) is continous at [a,b], and we have $\int_a^bf(x)dx=\int_a^bxf(x)dx=\int_a^bx^2f(x)dx=0$ Prove that $\exists \ x_1,x_2,x_3\text {(different from each other)}  \in(a,b)$ satisfying $f(x_1)=f(x_2)=f(x_3)=0$. I have proved the existence of one zero point by differential mean value theorem, but have no idea about going on. Thanks for any solution or hint.","Assuming $a<b$, function f(x) is continous at [a,b], and we have $\int_a^bf(x)dx=\int_a^bxf(x)dx=\int_a^bx^2f(x)dx=0$ Prove that $\exists \ x_1,x_2,x_3\text {(different from each other)}  \in(a,b)$ satisfying $f(x_1)=f(x_2)=f(x_3)=0$. I have proved the existence of one zero point by differential mean value theorem, but have no idea about going on. Thanks for any solution or hint.",,['calculus']
94,Volume of revolving $ y = \sin(x) $ about a line $ y = c $,Volume of revolving  about a line, y = \sin(x)   y = c ,Consider the surface formed by revolving $y=\sin(x)$ about the line $y=c$ from some $0\le{c}\le{1}$ along the interval $0\le{x}\le{\pi}$. Set up and evaluate an integral to calculate the volume $V(c)$ as a function of $c$. (My attempt)  $$ \begin{align} V &= \pi\int_0^\pi[(\sin(x))^2-c^2]dx \\   &= \pi\int_0^\pi[(\sin^2(x))-c^2]dx \\   &= \pi\int_0^\pi\left[\frac12(1-\cos(2x))-c^2\right]dx \\   &= \pi\left[\frac12(x-\sin(x)(\cos(x))-\frac{c^2x}{2}\right]_0^\pi \\   &= \pi\left[\left(\frac12(\pi-\sin(\pi)\cos(\pi)-\frac{\pi c^2}{2}\right)-\left(\frac12(0-\sin(0)\cos(0)-0\right)\right] \end{align} $$ So far... is this correct? The second part of the question: What value of c maximises the volume $V(c)$? ^ no idea on that one. help appreciated.,Consider the surface formed by revolving $y=\sin(x)$ about the line $y=c$ from some $0\le{c}\le{1}$ along the interval $0\le{x}\le{\pi}$. Set up and evaluate an integral to calculate the volume $V(c)$ as a function of $c$. (My attempt)  $$ \begin{align} V &= \pi\int_0^\pi[(\sin(x))^2-c^2]dx \\   &= \pi\int_0^\pi[(\sin^2(x))-c^2]dx \\   &= \pi\int_0^\pi\left[\frac12(1-\cos(2x))-c^2\right]dx \\   &= \pi\left[\frac12(x-\sin(x)(\cos(x))-\frac{c^2x}{2}\right]_0^\pi \\   &= \pi\left[\left(\frac12(\pi-\sin(\pi)\cos(\pi)-\frac{\pi c^2}{2}\right)-\left(\frac12(0-\sin(0)\cos(0)-0\right)\right] \end{align} $$ So far... is this correct? The second part of the question: What value of c maximises the volume $V(c)$? ^ no idea on that one. help appreciated.,,"['calculus', 'integration', 'multivariable-calculus', 'optimization', 'volume']"
95,An inequality between integrals of series of characteristic functions of cubes,An inequality between integrals of series of characteristic functions of cubes,,"Let $1\leq p<\infty$. Prove that there exists $C>0$ such that  $$ \left(\int\left|\sum_{i=1}^\infty a_i\chi_{2Q_i}\right|^p \, dx\right)^{1/p} \leq  C\left(\int\left|\sum_{i=1}^\infty |a_i|\chi_{Q_i}\right|^p \, dx\right)^{1/p}. $$ Here $\{a_i\}$ is a sequence of real numbers and $\{Q_i\}$ is a sequence of cubes in $\mathbb{R}^n$,  $2Q_i$ is the cube with the same center as $Q_i$ but twice its length. How to prove this? Thanks.","Let $1\leq p<\infty$. Prove that there exists $C>0$ such that  $$ \left(\int\left|\sum_{i=1}^\infty a_i\chi_{2Q_i}\right|^p \, dx\right)^{1/p} \leq  C\left(\int\left|\sum_{i=1}^\infty |a_i|\chi_{Q_i}\right|^p \, dx\right)^{1/p}. $$ Here $\{a_i\}$ is a sequence of real numbers and $\{Q_i\}$ is a sequence of cubes in $\mathbb{R}^n$,  $2Q_i$ is the cube with the same center as $Q_i$ but twice its length. How to prove this? Thanks.",,"['real-analysis', 'integration', 'sequences-and-series', 'measure-theory', 'harmonic-analysis']"
96,Are there high performance computing applications for symbolic integration?,Are there high performance computing applications for symbolic integration?,,"Currently there are a number of applications for numerical integration in applied mathematics and physics. Many of these are integral transforms (often Fourier or Laplace), or solving definite integrals with no closed form solution such as running the numbers on the Dirac equation for molecular modelling or doing lattice quantum chromodynamics to predict things like the mass of the proton. Years ago I thought it might be interesting to try to implement the Risch algorithm as a library function for similar HPC applications, but it's my understanding that symbolic integration of indefinite integrals isn't of much use in these areas because most integrals don't have closed form solutions. Apparently there are heuristics for finding definite integrals for expressions that don't have closed form antiderivatives that often exploit special functions to make the numerical integration easier. I'm curious if there's any high performance computing need to do symbolic integration or if it could help improve the performance of the applications where we currently do fairly dumb brute force numerical integration. Do we only need symbolic integration for doing the set up for packages that run numerical methods? Are there applications for symbolic integration where you want to run the Risch algorithm (and other symbolic integration methods) over millions of expressions, or is the utility of symbolic integration in computer algebra systems only as a research tool that doesn't require large amounts of computational power the way numerical integration does? Are there any applications for high performance computing symbolic integration packages? If so, what are some examples?","Currently there are a number of applications for numerical integration in applied mathematics and physics. Many of these are integral transforms (often Fourier or Laplace), or solving definite integrals with no closed form solution such as running the numbers on the Dirac equation for molecular modelling or doing lattice quantum chromodynamics to predict things like the mass of the proton. Years ago I thought it might be interesting to try to implement the Risch algorithm as a library function for similar HPC applications, but it's my understanding that symbolic integration of indefinite integrals isn't of much use in these areas because most integrals don't have closed form solutions. Apparently there are heuristics for finding definite integrals for expressions that don't have closed form antiderivatives that often exploit special functions to make the numerical integration easier. I'm curious if there's any high performance computing need to do symbolic integration or if it could help improve the performance of the applications where we currently do fairly dumb brute force numerical integration. Do we only need symbolic integration for doing the set up for packages that run numerical methods? Are there applications for symbolic integration where you want to run the Risch algorithm (and other symbolic integration methods) over millions of expressions, or is the utility of symbolic integration in computer algebra systems only as a research tool that doesn't require large amounts of computational power the way numerical integration does? Are there any applications for high performance computing symbolic integration packages? If so, what are some examples?",,"['integration', 'soft-question', 'numerical-methods', 'computer-algebra-systems', 'symbolic-computation']"
97,Properties and representations of the the rescaled complementary error function $\mathrm{erfcx}{z}$,Properties and representations of the the rescaled complementary error function,\mathrm{erfcx}{z},"Consider the rescaled complementary error function: $$ \mathrm{erfcx}(z) = {e^{z^2}} \left( {1-\mathrm{erf}(z)} \right) $$ $z \in \Bbb{C}$ which also has the following integral representation: $$ \mathrm{erfcx}(z) = -{\frac{i}{\sqrt{\pi}}}\int {\frac{e^{-t^2}}{t-iz} dt} $$ What is the correct decomposition into real and imaginary parts? Are they related to other known functions? What are the symmetries of this function? Estimate $\mathrm{erfcx}(z) - \mathrm{erfcx}(z^*)$ ($z^*$ denoting complex conjugation) Does $\mathrm{erfcx}(z) - \mathrm{erfcx}(z^*)$ have any notable properties? What are its stationary points (e.g. along lines parallel to the real axis)? Can anyone suggest possible routes for graphing the real and imaginary parts of this function? Motivation The complementary error function appears in the solution of a transport problem that I am trying to solve - I have exposed it to some detail here . I hope that the study of this function will enable the suitability of my solution for the required application. Here are my attempts to solve these questions - I believe I have made some progress, but they are incomplete. 1. It is worth noting that $\mathrm{erfcx}(z)=w(iz)$ where $w(z)$ is the Faddeeva function. Therefore, the properties of $\mathrm{erfcx}(z)$ should follow trivially from that relation (as they are given, for example in Abramowitz & Stegun ). However, I do find in the literature the restriction of many of the properties to the upper half of the imaginary plane, but as I would like to see if such restrictions can be avoided I shall state the more salient ones explicitly. Let $z = \alpha + i \beta$; $\alpha , \beta \in \Bbb{R}$. Then $$ \mathrm{erfcx}(z) = u(\alpha,\beta)+ i\ v(\alpha,\beta)= $$ $$ e^{\alpha^2 -\beta^2}  \left[  \cos{(2\alpha\beta)}(1 - \Re[\mathrm{erf}z]) + \sin{(2\alpha\beta)}\Im[\mathrm{erf}z] \right] -i\ e^{\alpha^2 -\beta^2} \left[ \cos{(2\alpha\beta)}\Im[\mathrm{erf}z] + \sin{(2\alpha\beta)}(1 - \Re[\mathrm{erf}z]) \right] $$ which (taking $\Re[\mathrm{erf}z]$ to be odd wrt $\alpha$ and even wrt $\beta$ and conversely $\Im[\mathrm{erf}z]$ even wrt $\alpha$ and odd wrt $\beta$ - if I am not terribly mistaken) implies $u(\alpha,\beta)=u(\alpha,-\beta)$ and $v(\alpha,\beta)=-v(\alpha,-\beta)$. As the Faddeeva function $w(z)$ is decomposed to (real and imaginary) Voigt functions $$ w(p + iq) = U(p,q)+i\ V(p,q) $$ one is tempted to write $u(\alpha,\beta)=U(-\beta, \alpha)$ and $v(\alpha,\beta)=V(-\beta, \alpha)$ But does this relation hold $\forall \alpha , \beta \in \Bbb{R}$? Moreover, are there calculation methods $v$ and $u$ and relations to other commonly used special functions? With appropriate scaling, for $\alpha>0$ $U$ is related to the Voigt profile; for $\alpha=0$, $V$ is related to Dawson's integral $\sqrt{\pi/4}{e^{-x^2}\mathrm{erfi}(x)}$. But can a more generalised representation be found, valid for all $z$? 2. $u$ has even parity wrt $\beta$ and $v$ is odd wrt $\beta$. The symmetries of $w(z)$ would be expected to hold for $\mathrm{erfcx}(z)$ as well. 3. $$ \mathrm{erfcx}(z) - \mathrm{erfcx}(z^*) = 2 i \ v = -2i\ e^{\alpha^2 -\beta^2} \left( \cos{(2\alpha\beta}\Im[\mathrm{erf}z]  + \sin{2\alpha\beta}(1- \Re[\mathrm{erf}z])  \right) $$ showing that this difference is purely imaginary. But a way to calculate this wouldbe useful. 4. I have not yet looked into this problem in any particular detail; the ODE representation of $w$ and the associated recurrence relations will probably be of use here. 5. I have found a number of C libraries for complex error functions (e.g. here , which includes a short bibliography for the calculation). I am in the process of implementing them; but if there are other quidirty hacks for estimating the imaginary and real parts of $\mathrm{erfcx}(z)$ I would be very glad to hear about them. Addendum As the expressions I obtain for $\Re[\mathrm{erf}(\alpha + i \beta)]$ and $\Im[\mathrm{erf}(\alpha + i \beta)]$ are long-winded (and often encountered in the relevant literature) I shall append them here. ( a ) Migrating along the real axis to $z=\alpha$ and then up to $z=\alpha + i\beta$ $$ \mathrm{erf}(\alpha + i \beta) = \sqrt{\frac{4}{\pi}}\int _{0}^{\alpha}{e^{-t^2} dt} + i \sqrt{\frac{4}{\pi}}\int _{0}^{\beta}{e^{-(\alpha+ i \ t)^2} dt} = $$ $$ \mathrm{erf}(\alpha) + \sqrt{\frac{4}{\pi}} e^{-\alpha^2} \int _{0}^{\beta}{e^{t^2} \sin{(2\alpha t)} dt} + i \sqrt{\frac{4}{\pi}} e^{-\alpha^2} \int _{0}^{\beta}{e^{t^2} \cos{(2\alpha t)} dt} $$ ( b ) Migrating along the imaginary axis to $z=i\beta$ and then parallel to the real axis to $z=\alpha + i\beta$ $$ \mathrm{erf}(\alpha + i \beta) = i\sqrt{\frac{4}{\pi}}\int _{0}^{\beta}{e^{t^2} dt} +  \sqrt{\frac{4}{\pi}}\int _{0}^{\alpha}{e^{-(t + i \beta)^2} dt} = $$ $$ \sqrt{\frac{4}{\pi}} e^{\beta^2} \int _{0}^{\alpha}{e^{-t^2} \cos{2 \beta t} dt} +  i\sqrt{\frac{4}{\pi}} e^{\beta^2} \int _{0}^{\alpha}{e^{-t^2} \sin{2 \beta t} dt} +  i\ \mathrm{erfi}(\beta) $$ I find the occurrence of definite integrals of the form$ \int _{0}^{\kappa}{e^{\pm t^2} \cos{2 \lambda t} dt}$ and $ \int _{0}^{\kappa}{e^{\pm t^2} \sin{2 \lambda t} dt}$ quite suggestive, esp. when seen e.g. in conjunction with an integral representation of Dawson’s function: $$ D_{+} (\chi)= \int_{0}^{\infty}{e^{-t^2} \sin{2 \chi t} dt} $$ Is the presence of such formulae, which resemble the sine and cosine transforms of the Gaussian coincindental? If not, can it be somehow exploited? (Apologies for the long question – but at least I will not be accused of not being thorough.)","Consider the rescaled complementary error function: $$ \mathrm{erfcx}(z) = {e^{z^2}} \left( {1-\mathrm{erf}(z)} \right) $$ $z \in \Bbb{C}$ which also has the following integral representation: $$ \mathrm{erfcx}(z) = -{\frac{i}{\sqrt{\pi}}}\int {\frac{e^{-t^2}}{t-iz} dt} $$ What is the correct decomposition into real and imaginary parts? Are they related to other known functions? What are the symmetries of this function? Estimate $\mathrm{erfcx}(z) - \mathrm{erfcx}(z^*)$ ($z^*$ denoting complex conjugation) Does $\mathrm{erfcx}(z) - \mathrm{erfcx}(z^*)$ have any notable properties? What are its stationary points (e.g. along lines parallel to the real axis)? Can anyone suggest possible routes for graphing the real and imaginary parts of this function? Motivation The complementary error function appears in the solution of a transport problem that I am trying to solve - I have exposed it to some detail here . I hope that the study of this function will enable the suitability of my solution for the required application. Here are my attempts to solve these questions - I believe I have made some progress, but they are incomplete. 1. It is worth noting that $\mathrm{erfcx}(z)=w(iz)$ where $w(z)$ is the Faddeeva function. Therefore, the properties of $\mathrm{erfcx}(z)$ should follow trivially from that relation (as they are given, for example in Abramowitz & Stegun ). However, I do find in the literature the restriction of many of the properties to the upper half of the imaginary plane, but as I would like to see if such restrictions can be avoided I shall state the more salient ones explicitly. Let $z = \alpha + i \beta$; $\alpha , \beta \in \Bbb{R}$. Then $$ \mathrm{erfcx}(z) = u(\alpha,\beta)+ i\ v(\alpha,\beta)= $$ $$ e^{\alpha^2 -\beta^2}  \left[  \cos{(2\alpha\beta)}(1 - \Re[\mathrm{erf}z]) + \sin{(2\alpha\beta)}\Im[\mathrm{erf}z] \right] -i\ e^{\alpha^2 -\beta^2} \left[ \cos{(2\alpha\beta)}\Im[\mathrm{erf}z] + \sin{(2\alpha\beta)}(1 - \Re[\mathrm{erf}z]) \right] $$ which (taking $\Re[\mathrm{erf}z]$ to be odd wrt $\alpha$ and even wrt $\beta$ and conversely $\Im[\mathrm{erf}z]$ even wrt $\alpha$ and odd wrt $\beta$ - if I am not terribly mistaken) implies $u(\alpha,\beta)=u(\alpha,-\beta)$ and $v(\alpha,\beta)=-v(\alpha,-\beta)$. As the Faddeeva function $w(z)$ is decomposed to (real and imaginary) Voigt functions $$ w(p + iq) = U(p,q)+i\ V(p,q) $$ one is tempted to write $u(\alpha,\beta)=U(-\beta, \alpha)$ and $v(\alpha,\beta)=V(-\beta, \alpha)$ But does this relation hold $\forall \alpha , \beta \in \Bbb{R}$? Moreover, are there calculation methods $v$ and $u$ and relations to other commonly used special functions? With appropriate scaling, for $\alpha>0$ $U$ is related to the Voigt profile; for $\alpha=0$, $V$ is related to Dawson's integral $\sqrt{\pi/4}{e^{-x^2}\mathrm{erfi}(x)}$. But can a more generalised representation be found, valid for all $z$? 2. $u$ has even parity wrt $\beta$ and $v$ is odd wrt $\beta$. The symmetries of $w(z)$ would be expected to hold for $\mathrm{erfcx}(z)$ as well. 3. $$ \mathrm{erfcx}(z) - \mathrm{erfcx}(z^*) = 2 i \ v = -2i\ e^{\alpha^2 -\beta^2} \left( \cos{(2\alpha\beta}\Im[\mathrm{erf}z]  + \sin{2\alpha\beta}(1- \Re[\mathrm{erf}z])  \right) $$ showing that this difference is purely imaginary. But a way to calculate this wouldbe useful. 4. I have not yet looked into this problem in any particular detail; the ODE representation of $w$ and the associated recurrence relations will probably be of use here. 5. I have found a number of C libraries for complex error functions (e.g. here , which includes a short bibliography for the calculation). I am in the process of implementing them; but if there are other quidirty hacks for estimating the imaginary and real parts of $\mathrm{erfcx}(z)$ I would be very glad to hear about them. Addendum As the expressions I obtain for $\Re[\mathrm{erf}(\alpha + i \beta)]$ and $\Im[\mathrm{erf}(\alpha + i \beta)]$ are long-winded (and often encountered in the relevant literature) I shall append them here. ( a ) Migrating along the real axis to $z=\alpha$ and then up to $z=\alpha + i\beta$ $$ \mathrm{erf}(\alpha + i \beta) = \sqrt{\frac{4}{\pi}}\int _{0}^{\alpha}{e^{-t^2} dt} + i \sqrt{\frac{4}{\pi}}\int _{0}^{\beta}{e^{-(\alpha+ i \ t)^2} dt} = $$ $$ \mathrm{erf}(\alpha) + \sqrt{\frac{4}{\pi}} e^{-\alpha^2} \int _{0}^{\beta}{e^{t^2} \sin{(2\alpha t)} dt} + i \sqrt{\frac{4}{\pi}} e^{-\alpha^2} \int _{0}^{\beta}{e^{t^2} \cos{(2\alpha t)} dt} $$ ( b ) Migrating along the imaginary axis to $z=i\beta$ and then parallel to the real axis to $z=\alpha + i\beta$ $$ \mathrm{erf}(\alpha + i \beta) = i\sqrt{\frac{4}{\pi}}\int _{0}^{\beta}{e^{t^2} dt} +  \sqrt{\frac{4}{\pi}}\int _{0}^{\alpha}{e^{-(t + i \beta)^2} dt} = $$ $$ \sqrt{\frac{4}{\pi}} e^{\beta^2} \int _{0}^{\alpha}{e^{-t^2} \cos{2 \beta t} dt} +  i\sqrt{\frac{4}{\pi}} e^{\beta^2} \int _{0}^{\alpha}{e^{-t^2} \sin{2 \beta t} dt} +  i\ \mathrm{erfi}(\beta) $$ I find the occurrence of definite integrals of the form$ \int _{0}^{\kappa}{e^{\pm t^2} \cos{2 \lambda t} dt}$ and $ \int _{0}^{\kappa}{e^{\pm t^2} \sin{2 \lambda t} dt}$ quite suggestive, esp. when seen e.g. in conjunction with an integral representation of Dawson’s function: $$ D_{+} (\chi)= \int_{0}^{\infty}{e^{-t^2} \sin{2 \chi t} dt} $$ Is the presence of such formulae, which resemble the sine and cosine transforms of the Gaussian coincindental? If not, can it be somehow exploited? (Apologies for the long question – but at least I will not be accused of not being thorough.)",,"['integration', 'complex-analysis', 'special-functions', 'graphing-functions', 'error-function']"
98,A trigonometric integral with sin(cos(x)) in exponent,A trigonometric integral with sin(cos(x)) in exponent,,"Evaluate: $$\int_0^{\pi} x\csc^{\sin(\cos x)}(x)\,dx$$ I honestly don't know how to deal with this case. If I apply the property $\int_a^b f(x)\,dx=\int_a^b f(a+b-x)\,dx$, I get: $$\int_0^{\pi} (\pi-x) \frac{1}{(\sin x)^{-\sin(\cos x)}}\,dx$$ but I don't think this is going to help. Any help is appreciated. Thanks!","Evaluate: $$\int_0^{\pi} x\csc^{\sin(\cos x)}(x)\,dx$$ I honestly don't know how to deal with this case. If I apply the property $\int_a^b f(x)\,dx=\int_a^b f(a+b-x)\,dx$, I get: $$\int_0^{\pi} (\pi-x) \frac{1}{(\sin x)^{-\sin(\cos x)}}\,dx$$ but I don't think this is going to help. Any help is appreciated. Thanks!",,"['calculus', 'integration', 'trigonometry', 'definite-integrals']"
99,Does the fundamental theorem of calculus hold for BV functions?,Does the fundamental theorem of calculus hold for BV functions?,,"I am a bit confused and I hope you can help me in understanding a bit better these things. Let us start by considering one dimensional case. Let $f\colon \mathbb (a,b) \to \mathbb R$ be a function. As far as I know, the equality  $$\tag{FTC} f(x) = f(a) + \int_a^x f^{\prime}(t)dt $$ holds for every $x \in (a,b)$ if and only if the function $f$ ia $AC(a,b)$. This makes sense, because an absolutely continuous function on the real line is differentiable Lebesgue a.e. (for instance, because it can be written as sum of monotone functions: then a deep theorem by Lebesgue assures a.e. differentiability). Moreover, we know that AC is a (proper) subset of BV; the formula (FTC) does not hold in BV (counterexample: the devil staircase ). Am I right so far? Hope so. Now let turn to the n-dimensional case, $f \colon \mathbb R^n \to \mathbb R$. We define a BV function as a function whose first (distributional) derivatives are signed measures with finite total variation. Now the point is: here I read (Theorem 9 ) that for every BV function there is a signed measure such that $f=\mu((-\infty,x])=\int_{-\infty}^x d\mu$. But this is exactly FTC, up to writing in the RHS $\int_{\infty}^x f^{\prime}$ instead of $\int f^{\prime}dt$! I mean, the only difference is that $\mu$ need not be absolutely continuous (as a measure) with respect to Lebesgue measure. So my final conjecture is: FTC holds formulation for every BV function but the RHS must be considered as the distributional derivative. And AC functions are precisely the BV functions whose distributional derivative is absolutely continuous wrt Lebesgue. Is this correct? Is there anything to add? Thanks.","I am a bit confused and I hope you can help me in understanding a bit better these things. Let us start by considering one dimensional case. Let $f\colon \mathbb (a,b) \to \mathbb R$ be a function. As far as I know, the equality  $$\tag{FTC} f(x) = f(a) + \int_a^x f^{\prime}(t)dt $$ holds for every $x \in (a,b)$ if and only if the function $f$ ia $AC(a,b)$. This makes sense, because an absolutely continuous function on the real line is differentiable Lebesgue a.e. (for instance, because it can be written as sum of monotone functions: then a deep theorem by Lebesgue assures a.e. differentiability). Moreover, we know that AC is a (proper) subset of BV; the formula (FTC) does not hold in BV (counterexample: the devil staircase ). Am I right so far? Hope so. Now let turn to the n-dimensional case, $f \colon \mathbb R^n \to \mathbb R$. We define a BV function as a function whose first (distributional) derivatives are signed measures with finite total variation. Now the point is: here I read (Theorem 9 ) that for every BV function there is a signed measure such that $f=\mu((-\infty,x])=\int_{-\infty}^x d\mu$. But this is exactly FTC, up to writing in the RHS $\int_{\infty}^x f^{\prime}$ instead of $\int f^{\prime}dt$! I mean, the only difference is that $\mu$ need not be absolutely continuous (as a measure) with respect to Lebesgue measure. So my final conjecture is: FTC holds formulation for every BV function but the RHS must be considered as the distributional derivative. And AC functions are precisely the BV functions whose distributional derivative is absolutely continuous wrt Lebesgue. Is this correct? Is there anything to add? Thanks.",,"['real-analysis', 'integration', 'distribution-theory', 'bounded-variation']"
