,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Asymptotic expansion of $\int_0^{2\pi}ae^{x\cos a}da$,Asymptotic expansion of,\int_0^{2\pi}ae^{x\cos a}da,"I want to find the first two leading terms of the expansion of $\int_0^{2\pi}ae^{x\cos a}da$ Well in $[0,2\pi]$ $\cos a$ has has maxima $0,2\pi$ so I rewrite the integral to $\int_0^{\epsilon}ae^{x\cos a} da+\int_{2\pi-\epsilon}^{2\pi}ae^{x\cos a}da$ $\cos a=1-\frac{a^2}{2}+O(a^4)$ How can I continue using Laplace Method?","I want to find the first two leading terms of the expansion of $\int_0^{2\pi}ae^{x\cos a}da$ Well in $[0,2\pi]$ $\cos a$ has has maxima $0,2\pi$ so I rewrite the integral to $\int_0^{\epsilon}ae^{x\cos a} da+\int_{2\pi-\epsilon}^{2\pi}ae^{x\cos a}da$ $\cos a=1-\frac{a^2}{2}+O(a^4)$ How can I continue using Laplace Method?",,"['analysis', 'asymptotics']"
1,Showing a Space-Filling Curve is Continuous,Showing a Space-Filling Curve is Continuous,,"Setting: Let $F':[0,1] \rightarrow [0,1]$ be the Cantor Function. Goal: Show that there exists a well-defined, surjective, continuous function from $[0,1]$ to $[0,1]^2$ (i.e., a space-filling curve). Attempt: EDIT: It turns out my function is non-sensical in that the domain isn't even $[0,1]$!  I'm leaving it here to show an attempt was   made at answering the question, but I've yet to come up with a   suitable candidate function from $[0,1]$ to $[0,1]^2$. Let $G: [0,1] \rightarrow [0,1] \times [0,1]$ s.t. $$ G(x,y) = (F'(x),F'(y)) $$ I claim that $G$ is a well-defined, surjective, continuous map. First $G$ is clearly well-defined (there are no issues with representatives or equivalence classes). To see that $G$ is surjective, let $(a,b) \in [0,1]^2$. Consider $F'$ is surjective (I've already shown this on my own). Hence we have that $\exists x,y \in [0,1]$ s.t. $F'(x) = a$ and $F'(y) = b$. Then consider that $G(x,y) = (F'(x), F'(y)) = (a,b)$ so that $G$ is surjective as desired. Question: Why is $G$ continuous?  I've shown elsewhere that $C \cong [0,1]$, and I've been given the hint that $C \cong C \times C$ would be a useful fact to prove.  Why would this fact be useful?","Setting: Let $F':[0,1] \rightarrow [0,1]$ be the Cantor Function. Goal: Show that there exists a well-defined, surjective, continuous function from $[0,1]$ to $[0,1]^2$ (i.e., a space-filling curve). Attempt: EDIT: It turns out my function is non-sensical in that the domain isn't even $[0,1]$!  I'm leaving it here to show an attempt was   made at answering the question, but I've yet to come up with a   suitable candidate function from $[0,1]$ to $[0,1]^2$. Let $G: [0,1] \rightarrow [0,1] \times [0,1]$ s.t. $$ G(x,y) = (F'(x),F'(y)) $$ I claim that $G$ is a well-defined, surjective, continuous map. First $G$ is clearly well-defined (there are no issues with representatives or equivalence classes). To see that $G$ is surjective, let $(a,b) \in [0,1]^2$. Consider $F'$ is surjective (I've already shown this on my own). Hence we have that $\exists x,y \in [0,1]$ s.t. $F'(x) = a$ and $F'(y) = b$. Then consider that $G(x,y) = (F'(x), F'(y)) = (a,b)$ so that $G$ is surjective as desired. Question: Why is $G$ continuous?  I've shown elsewhere that $C \cong [0,1]$, and I've been given the hint that $C \cong C \times C$ would be a useful fact to prove.  Why would this fact be useful?",,"['real-analysis', 'general-topology', 'analysis', 'algebraic-topology']"
2,the best constant in an inequality?,the best constant in an inequality?,,"I learnt how to show the below inequality by C-S inequality: k is from $0$ to $\infty$ If $\sum a_{k}^{2}9^{k}\le 5$ then $\sum |a_{k}|2^{k}\le 3$. next,I tried to show that 3 is the best possible constant in the last inequality. Could you please help me to show how 3 is the best possible constant.","I learnt how to show the below inequality by C-S inequality: k is from $0$ to $\infty$ If $\sum a_{k}^{2}9^{k}\le 5$ then $\sum |a_{k}|2^{k}\le 3$. next,I tried to show that 3 is the best possible constant in the last inequality. Could you please help me to show how 3 is the best possible constant.",,"['calculus', 'sequences-and-series', 'analysis', 'inequality']"
3,Find critical points of sin(x*y),Find critical points of sin(x*y),,"so I got this homework problem that I was having trouble with. The problem is: Let $f(x, y) = \sin(xy)$ defined on all of $\mathbb{R}^2$. Find the critical points of $f$ and classify them as local maxima, local minima, or saddle points. The problem is that I calculated the following: $f_{xx} = -y^2 \sin(xy)$, $f_{yy} = -x^2 \sin(xy)$ and $f_{xy} = f_{yx} = -yx \sin(xy)$. Then, the determinant of the hessian ends up being $0$ no matter what $(x, y)$ we choose, so the 2nd derivative test is inconclusive. Am I doing something wrong here? Thanks in advance.","so I got this homework problem that I was having trouble with. The problem is: Let $f(x, y) = \sin(xy)$ defined on all of $\mathbb{R}^2$. Find the critical points of $f$ and classify them as local maxima, local minima, or saddle points. The problem is that I calculated the following: $f_{xx} = -y^2 \sin(xy)$, $f_{yy} = -x^2 \sin(xy)$ and $f_{xy} = f_{yx} = -yx \sin(xy)$. Then, the determinant of the hessian ends up being $0$ no matter what $(x, y)$ we choose, so the 2nd derivative test is inconclusive. Am I doing something wrong here? Thanks in advance.",,"['calculus', 'analysis']"
4,"Prove that if $C$ is a convex set containing $B(r)$, then $\sup\{d(y,0)\mid y\in C\}=\infty$","Prove that if  is a convex set containing , then","C B(r) \sup\{d(y,0)\mid y\in C\}=\infty","Let $0<p<1$. Define a metric on $l^p$ by $d((a_k)_{k=1}^\infty,(b_k)_{k=1}^{\infty})=\sum_{k=1}^\infty |a_k-b_k|^p$. For any $r>0$, let $B(r)=\{x\in l^p\mid d(x,0)<r\}$. Prove that if $C$ is a convex set containing $B(r)$, then $\sup\{d(y,0)\mid y\in C\}=\infty$. Deduce that $l^p$ is not a locally convex topological vector space. How to prove this question? Thanks.","Let $0<p<1$. Define a metric on $l^p$ by $d((a_k)_{k=1}^\infty,(b_k)_{k=1}^{\infty})=\sum_{k=1}^\infty |a_k-b_k|^p$. For any $r>0$, let $B(r)=\{x\in l^p\mid d(x,0)<r\}$. Prove that if $C$ is a convex set containing $B(r)$, then $\sup\{d(y,0)\mid y\in C\}=\infty$. Deduce that $l^p$ is not a locally convex topological vector space. How to prove this question? Thanks.",,"['real-analysis', 'analysis', 'functional-analysis']"
5,Applications of the theory of distributions outside of PDEs?,Applications of the theory of distributions outside of PDEs?,,"Are there any interesting, important or powerful mathematical applications to the Theory of Distributions besides those dealing with partial differential equations?","Are there any interesting, important or powerful mathematical applications to the Theory of Distributions besides those dealing with partial differential equations?",,"['analysis', 'partial-differential-equations', 'distribution-theory', 'applications']"
6,$f(x) = |\cos x|$ prove that f is differentiable at these points and not differentiable at all other points.,prove that f is differentiable at these points and not differentiable at all other points.,f(x) = |\cos x|,"Define $f : \mathbb R → \mathbb R$ by $f(x) = |\cos x|$. Determine the set of points where f is differentiable and calculate the derivative of f on this set. Also prove that $f$ is differentiable at these points and not differentiable at all other points. My working:  from the graph of $|\cos x|$, we can see that the $|\cos x|$ is differentiable everywhere except at  points where $x=(k+\frac{1}{2})\pi$, where $k$ is an integer, but this is intuitive, i'm having difficulty giving a proper proof of $|\cos x|$ being differentiable at these points.","Define $f : \mathbb R → \mathbb R$ by $f(x) = |\cos x|$. Determine the set of points where f is differentiable and calculate the derivative of f on this set. Also prove that $f$ is differentiable at these points and not differentiable at all other points. My working:  from the graph of $|\cos x|$, we can see that the $|\cos x|$ is differentiable everywhere except at  points where $x=(k+\frac{1}{2})\pi$, where $k$ is an integer, but this is intuitive, i'm having difficulty giving a proper proof of $|\cos x|$ being differentiable at these points.",,"['analysis', 'derivatives']"
7,Two definitions of Taylor polynomials,Two definitions of Taylor polynomials,,"I'm studying a book which states Given a function $f:I\to \mathbb R$, $n$ times derivable in the point   $a\in I$, the Taylor polynomial of order $n$ of $f$ in the point $a$   is the polynomial: $$p(h)=f(a)+f'(a)\cdot h+\frac{f''(a)}{2!}\cdot  h^2+\ldots+\frac{f^{(n)}(a)}{n!}\cdot h^n$$ This is the only polynomial of degree $\le n$ whose derivatives (from   the order $0$ to the order n) in the point $0$ coincide with the   corresponding derivatives of $f$ in the point $a$ However, I saw in another sources ( this and this ) that the Taylor polynomial is given by: $$p(x)=f(a)+f'(a)\cdot (x-a)+\frac{f''(a)}{2!}\cdot (x-a)^2+\ldots+\frac{f^{(n)}(a)}{n!}\cdot (x-a)^n$$ I would like to know why these polynomials are equivalent, i.e., why we can call both of Taylor polynomial, I'm really confused. Thanks.","I'm studying a book which states Given a function $f:I\to \mathbb R$, $n$ times derivable in the point   $a\in I$, the Taylor polynomial of order $n$ of $f$ in the point $a$   is the polynomial: $$p(h)=f(a)+f'(a)\cdot h+\frac{f''(a)}{2!}\cdot  h^2+\ldots+\frac{f^{(n)}(a)}{n!}\cdot h^n$$ This is the only polynomial of degree $\le n$ whose derivatives (from   the order $0$ to the order n) in the point $0$ coincide with the   corresponding derivatives of $f$ in the point $a$ However, I saw in another sources ( this and this ) that the Taylor polynomial is given by: $$p(x)=f(a)+f'(a)\cdot (x-a)+\frac{f''(a)}{2!}\cdot (x-a)^2+\ldots+\frac{f^{(n)}(a)}{n!}\cdot (x-a)^n$$ I would like to know why these polynomials are equivalent, i.e., why we can call both of Taylor polynomial, I'm really confused. Thanks.",,"['calculus', 'real-analysis', 'analysis']"
8,"Why study $\mathcal{C}[0,1]$",Why study,"\mathcal{C}[0,1]","I know this is probably a naive question here on this site, but I don't do a lot of higher math.  The group $\mathcal{C}[0,1]$ seems to be an important set within the framework of many mathematical texts.  I have not studied this outright, save for my Calc classes and very basic analysis.  I know there are segues into linear algebra with it as well, but why study this set of continuous functions?  What can groups of functions between the unit interval tell us about anything?  What behavior can be identified by studying them?  Is the most important framework for theory of measures and probability, considering the importance of the unit interval in the context of probability?","I know this is probably a naive question here on this site, but I don't do a lot of higher math.  The group $\mathcal{C}[0,1]$ seems to be an important set within the framework of many mathematical texts.  I have not studied this outright, save for my Calc classes and very basic analysis.  I know there are segues into linear algebra with it as well, but why study this set of continuous functions?  What can groups of functions between the unit interval tell us about anything?  What behavior can be identified by studying them?  Is the most important framework for theory of measures and probability, considering the importance of the unit interval in the context of probability?",,"['group-theory', 'analysis', 'functions']"
9,Interior ball condition,Interior ball condition,,"Let $\Omega\subset\mathbb{R}^n$ be a open set. We say that $y\in\partial \Omega$ satisfies the interior ball condition, if there is $x\in \Omega$ and $r>0$ such that $$B(x,r)\subset\Omega,\ y\in \partial B(x,r),$$ where $B(x,r)=\{z\in\mathbb{R}^n:\ \|z-x\|<r\}$. I am trying to prove that (I don't know if it is true) the set of points in $\partial \Omega$ which satisfies the interior ball condition are dense in $\partial\Omega$. If we go by contradiction then, there is $y\in\partial\Omega$ (which we can assume to not be isolated) and a neighborhood $F$ of $y$ ($F\subset\partial\Omega$) such that $$d(x,\partial\Omega)<d(x,F),\ \forall x\tag{1}$$ Now I am trying to get a contradiction with $(1)$. Any idea is appreciated. Update: I think I have a answer, please verify if it is correct. Assume ad absurdum that there is $y\in \partial\Omega$, $y$ is not isolated in $\partial\Omega$, such that in the set $V_r=\overline{B(y,r)\cap \partial\Omega}$ (for some $r>0$) there is no point which satisfies IBC. If there is $x\in \Omega$ with $d(x,\partial\Omega)= d(x,V_r)$ we are done, hence, assume that $(1)$ is satisfied for $F=V_r$. Take $z\in V_r$ with $\|z-y\|<\delta$ and $0<\delta<r$. As $d(z,\partial\Omega)<d(z,V_r)$, the infimum of $d(z,\partial\Omega)$ is achieved in $\partial\Omega\setminus V_r$ in some point $w_{\delta}$. By choosing $\delta$ sufficiently small, we must have that $\|z-y\|=d(z,y)<d(z,w_{\delta})$, because $w_{\delta}$ is outside the ball $B(x,r)$. Therefore, we have a contradiction.","Let $\Omega\subset\mathbb{R}^n$ be a open set. We say that $y\in\partial \Omega$ satisfies the interior ball condition, if there is $x\in \Omega$ and $r>0$ such that $$B(x,r)\subset\Omega,\ y\in \partial B(x,r),$$ where $B(x,r)=\{z\in\mathbb{R}^n:\ \|z-x\|<r\}$. I am trying to prove that (I don't know if it is true) the set of points in $\partial \Omega$ which satisfies the interior ball condition are dense in $\partial\Omega$. If we go by contradiction then, there is $y\in\partial\Omega$ (which we can assume to not be isolated) and a neighborhood $F$ of $y$ ($F\subset\partial\Omega$) such that $$d(x,\partial\Omega)<d(x,F),\ \forall x\tag{1}$$ Now I am trying to get a contradiction with $(1)$. Any idea is appreciated. Update: I think I have a answer, please verify if it is correct. Assume ad absurdum that there is $y\in \partial\Omega$, $y$ is not isolated in $\partial\Omega$, such that in the set $V_r=\overline{B(y,r)\cap \partial\Omega}$ (for some $r>0$) there is no point which satisfies IBC. If there is $x\in \Omega$ with $d(x,\partial\Omega)= d(x,V_r)$ we are done, hence, assume that $(1)$ is satisfied for $F=V_r$. Take $z\in V_r$ with $\|z-y\|<\delta$ and $0<\delta<r$. As $d(z,\partial\Omega)<d(z,V_r)$, the infimum of $d(z,\partial\Omega)$ is achieved in $\partial\Omega\setminus V_r$ in some point $w_{\delta}$. By choosing $\delta$ sufficiently small, we must have that $\|z-y\|=d(z,y)<d(z,w_{\delta})$, because $w_{\delta}$ is outside the ball $B(x,r)$. Therefore, we have a contradiction.",,"['analysis', 'proof-verification']"
10,Finding a radius of convergence of power series,Finding a radius of convergence of power series,,"I have  to find the radius of convergence of some power series but I find myself in trouble for three of them : the series are $\sum2^kx^{k!}$ $\sum\sinh(k)x^k$ $\sum\sin(k)x^k$. For the first one I have tried the Ratio Test for series first but I don't kow how to deal with the factorials as exponents... For the second one, I'm tempted to say it never converges but I cannot prove it. And for the last one, I'd say it converges for $|x|< 1$ since $-1<\sin(h)<1$ anyway, but I'm not quite sure... Some hints would help a lot! Thank you.","I have  to find the radius of convergence of some power series but I find myself in trouble for three of them : the series are $\sum2^kx^{k!}$ $\sum\sinh(k)x^k$ $\sum\sin(k)x^k$. For the first one I have tried the Ratio Test for series first but I don't kow how to deal with the factorials as exponents... For the second one, I'm tempted to say it never converges but I cannot prove it. And for the last one, I'd say it converges for $|x|< 1$ since $-1<\sin(h)<1$ anyway, but I'm not quite sure... Some hints would help a lot! Thank you.",,"['analysis', 'convergence-divergence', 'power-series']"
11,Example of a false proof when a Fourier series is not unique?,Example of a false proof when a Fourier series is not unique?,,"I am attempting to come up with an example to illustrate why one should care that a function has a unique Fourier series expansion.  Inspired by the fact that one can rearrange terms in a conditionally convergent series to obtain any number they want, I'm hoping that if one has a function with a non-unique Fourier series expansion, then one can do something with that series to obtain some kind of false result.  Is anyone aware of such an example? Thank you.","I am attempting to come up with an example to illustrate why one should care that a function has a unique Fourier series expansion.  Inspired by the fact that one can rearrange terms in a conditionally convergent series to obtain any number they want, I'm hoping that if one has a function with a non-unique Fourier series expansion, then one can do something with that series to obtain some kind of false result.  Is anyone aware of such an example? Thank you.",,"['analysis', 'fourier-series']"
12,Hyperplane which can separate two closed convex set,Hyperplane which can separate two closed convex set,,"Define: $$E=:\ell^1(R)=\{x=(x_n)_n: \|x\|_1=\sum_{n=1}^{\infty}|x_n|<\infty\}$$ We have two closed convex sets $X,Y$ as subsets of normed vector space $\ell^1(R)$ with $X\cap Y=\emptyset$ and $Y$ is a subspace of $E$ and $$r=\inf_{x\in X , y\in Y}\|x-y\|_1$$ is positive(nonzero). I wanted to define a hyperplane which can separate these two subsets and I found $a\in X, b\in Y$ such that $\frac{b+a}{2}\notin Y\cup X$ and for some positive $\varepsilon$ $$r\leq\|b-a\|_1<r+\varepsilon$$ I defined $f:E\rightarrow R$ by  $$f(e)=\sum_{n=1}^{\infty}e_n(b_n-a_n)+\frac{1}{2}\sum_{n=1}^{\infty}(a_n^2-b_n^2)$$ Now I need to prove that $\Big[\text{for every  $x\in X$, $f(x)<0$ and every $y\in Y$, $f(y)>0$} \Big]$.","Define: $$E=:\ell^1(R)=\{x=(x_n)_n: \|x\|_1=\sum_{n=1}^{\infty}|x_n|<\infty\}$$ We have two closed convex sets $X,Y$ as subsets of normed vector space $\ell^1(R)$ with $X\cap Y=\emptyset$ and $Y$ is a subspace of $E$ and $$r=\inf_{x\in X , y\in Y}\|x-y\|_1$$ is positive(nonzero). I wanted to define a hyperplane which can separate these two subsets and I found $a\in X, b\in Y$ such that $\frac{b+a}{2}\notin Y\cup X$ and for some positive $\varepsilon$ $$r\leq\|b-a\|_1<r+\varepsilon$$ I defined $f:E\rightarrow R$ by  $$f(e)=\sum_{n=1}^{\infty}e_n(b_n-a_n)+\frac{1}{2}\sum_{n=1}^{\infty}(a_n^2-b_n^2)$$ Now I need to prove that $\Big[\text{for every  $x\in X$, $f(x)<0$ and every $y\in Y$, $f(y)>0$} \Big]$.",,"['real-analysis', 'analysis', 'convex-analysis', 'banach-spaces']"
13,Does this type of functions exist [duplicate],Does this type of functions exist [duplicate],,This question already has answers here : No continuous function switches $\mathbb{Q}$ and the irrationals (4 answers) Closed 10 years ago . Does there exist a cont function $f$ : $\mathbb{R}$ $\rightarrow$$\mathbb{R}$  which takes irrational values at rational points and rational values at irrational points?,This question already has answers here : No continuous function switches $\mathbb{Q}$ and the irrationals (4 answers) Closed 10 years ago . Does there exist a cont function $f$ : $\mathbb{R}$ $\rightarrow$$\mathbb{R}$  which takes irrational values at rational points and rational values at irrational points?,,"['analysis', 'connectedness']"
14,Does $\phi\circ g$ convex imply $\phi\circ f\circ g$ convex?,Does  convex imply  convex?,\phi\circ g \phi\circ f\circ g,Assume that $\phi$ is an increasing function and $g$ is a decreasing function with $\phi\circ g$ convex. If $f$ is an increasing convex function does this imply $\phi\circ f\circ g$ is a convex function? I came to this from an applied problem where we can use the fact that $f\circ \phi\circ g$ is convex and I'd also like to say something about what happens if we did $f$ and $\phi$ in the opposite order.,Assume that $\phi$ is an increasing function and $g$ is a decreasing function with $\phi\circ g$ convex. If $f$ is an increasing convex function does this imply $\phi\circ f\circ g$ is a convex function? I came to this from an applied problem where we can use the fact that $f\circ \phi\circ g$ is convex and I'd also like to say something about what happens if we did $f$ and $\phi$ in the opposite order.,,"['analysis', 'convex-analysis', 'examples-counterexamples']"
15,"How prove this $g(x)=\sup{\{f(x,y)|0\le y\le 1\}}$ is continuous on $[0,1]$",How prove this  is continuous on,"g(x)=\sup{\{f(x,y)|0\le y\le 1\}} [0,1]","let $f(x,y):[0,1]\times[0,1]\to R$ is continuous real function. show that $$g(x)=\sup{\{f(x,y)|0\le y\le 1\}}$$ is continuous on $[0,1]$ My try: since $f(x,y)$ is continuous on $D=[0,1]\times [0,1]$, so $f(x,y)$ is Uniformly continuous on $D$,so $\forall\varepsilon>0$,then exist $\delta>0$,such $|x_{1}-x_{2}|<\delta,|y_{1}-y_{2}<\delta$,then we have $$|f(x_{1},y_{1})-f(x_{2},y_{2})|<\varepsilon$$ so $$g(x_{1})-g(x_{2})=|\sup f(x_{1},y)-\sup f(x_{2},y)|<\sup|f(x_{1},y)-f(x_{2},y)|$$ Now maybe follow is not true? $$|\sup f(x_{1},y)-\sup f(x_{2},y)|<\sup|f(x_{1},y)-f(x_{2},y)|$$","let $f(x,y):[0,1]\times[0,1]\to R$ is continuous real function. show that $$g(x)=\sup{\{f(x,y)|0\le y\le 1\}}$$ is continuous on $[0,1]$ My try: since $f(x,y)$ is continuous on $D=[0,1]\times [0,1]$, so $f(x,y)$ is Uniformly continuous on $D$,so $\forall\varepsilon>0$,then exist $\delta>0$,such $|x_{1}-x_{2}|<\delta,|y_{1}-y_{2}<\delta$,then we have $$|f(x_{1},y_{1})-f(x_{2},y_{2})|<\varepsilon$$ so $$g(x_{1})-g(x_{2})=|\sup f(x_{1},y)-\sup f(x_{2},y)|<\sup|f(x_{1},y)-f(x_{2},y)|$$ Now maybe follow is not true? $$|\sup f(x_{1},y)-\sup f(x_{2},y)|<\sup|f(x_{1},y)-f(x_{2},y)|$$",,['analysis']
16,The statements $f(n) = O(n^{\epsilon})$ for all $\epsilon > 0$ and $f(n) = n^{o(1)}$.,The statements  for all  and .,f(n) = O(n^{\epsilon}) \epsilon > 0 f(n) = n^{o(1)},"Consider the statements \begin{align} \tag{A} f(n) &= O(n^{\epsilon}) \text{ for all } \epsilon > 0 \\ \tag{B} f(n) &= n^{o(1)} \end{align} Questions: It's clear that (B) implies (A). Does (A) imply (B)? If the answer to the first question is no, is there a more brief way to write (A)?","Consider the statements \begin{align} \tag{A} f(n) &= O(n^{\epsilon}) \text{ for all } \epsilon > 0 \\ \tag{B} f(n) &= n^{o(1)} \end{align} Questions: It's clear that (B) implies (A). Does (A) imply (B)? If the answer to the first question is no, is there a more brief way to write (A)?",,"['real-analysis', 'sequences-and-series', 'analysis', 'asymptotics']"
17,proving a function is differentiable,proving a function is differentiable,,"$g: \mathbb{R} \to \mathbb{R} $ $$g(x) = \begin{cases} x^2\sin(1/x)& \text{if $x\ne 0$}, \\  0 &\text{if $x = 0$}.\end{cases}$$ Prove that g is differentiable everywhere, and that its derivative $g':\mathbb{R} \to \mathbb{R}$ is continuous on $\{x\in\mathbb{R}| x\not=0\}$ but discontinuous at 0. My attempt, since $x^2$ is differentiable everywhere, and $\sin(1/x)$ is differentiable every $x\not=0$ then the product is differentiable everywhere but $x\not=0$. if $x = 0$ could I simply state that since $g(x) = 0$ then it is differentiable at $ x = 0$ or would I have to use $x^2\sin(1/x)$? I wrote down on paper that $-x^2 \leq x^2\sin(1/x) \leq x^2$ and used the sandwich theorem to say it's differentiable at x = 0. Is this true or would I have to use $g(x) = 0$? I've done all other parts of the question, just stuck on this part.","$g: \mathbb{R} \to \mathbb{R} $ $$g(x) = \begin{cases} x^2\sin(1/x)& \text{if $x\ne 0$}, \\  0 &\text{if $x = 0$}.\end{cases}$$ Prove that g is differentiable everywhere, and that its derivative $g':\mathbb{R} \to \mathbb{R}$ is continuous on $\{x\in\mathbb{R}| x\not=0\}$ but discontinuous at 0. My attempt, since $x^2$ is differentiable everywhere, and $\sin(1/x)$ is differentiable every $x\not=0$ then the product is differentiable everywhere but $x\not=0$. if $x = 0$ could I simply state that since $g(x) = 0$ then it is differentiable at $ x = 0$ or would I have to use $x^2\sin(1/x)$? I wrote down on paper that $-x^2 \leq x^2\sin(1/x) \leq x^2$ and used the sandwich theorem to say it's differentiable at x = 0. Is this true or would I have to use $g(x) = 0$? I've done all other parts of the question, just stuck on this part.",,['analysis']
18,counterexample for Dominated Convergence Theorem,counterexample for Dominated Convergence Theorem,,The Dominated Convergence Theorem is as follows: What if the sequence $\left\{f_n \right\} \notin L^1$? Could someone provide a counterexample as to why the theorem wouldn't hold? Thanks!,The Dominated Convergence Theorem is as follows: What if the sequence $\left\{f_n \right\} \notin L^1$? Could someone provide a counterexample as to why the theorem wouldn't hold? Thanks!,,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
19,Prove statement about a sequence of homeomorphisms $f_n:\mathbb R \to \mathbb R$,Prove statement about a sequence of homeomorphisms,f_n:\mathbb R \to \mathbb R,"The problem statement: Let $\{f_n\}_{n \in \mathbb N}$ be a sequence of homeomorphisms from $\mathbb R$ to $\mathbb R$ and let $F$ be a closed subset of $\mathbb R$ that doesn't contain any rational number. Prove that there is a real number $r$ such that $r \not \in f_n(F)$ for all $n \in \mathbb N$ My attempt at a solution: I am not so sure how to attack this problem, I suppose that it may be easier to prove it by the absurd or by the contrapositive. So, suppose that for all $r \in \mathbb R$, $r \in \bigcup_{n \in \mathbb N} f_n(F)$.This means $\mathbb R=\bigcup_{n \in \mathbb N} f_n(F)$ Now, I would like to construct a sequence of real numbers $\{y_n\}_{n \in \mathbb N}$ such that $y_n \to z$ where $f^{-1}_n(y_n) \in F$ and $f^{-1}_{n_0}(z)=q$, for some natural number $n_0$ with $q \in \mathbb Q$. I don't know how to continue, I would like to conclude that $q$ is a limit point of $F$ constructing an appropiate sequence in the image of each $f_n$ and arrive to the desired absurd $q \in F$. Could anyone suggest me how to continue from here?","The problem statement: Let $\{f_n\}_{n \in \mathbb N}$ be a sequence of homeomorphisms from $\mathbb R$ to $\mathbb R$ and let $F$ be a closed subset of $\mathbb R$ that doesn't contain any rational number. Prove that there is a real number $r$ such that $r \not \in f_n(F)$ for all $n \in \mathbb N$ My attempt at a solution: I am not so sure how to attack this problem, I suppose that it may be easier to prove it by the absurd or by the contrapositive. So, suppose that for all $r \in \mathbb R$, $r \in \bigcup_{n \in \mathbb N} f_n(F)$.This means $\mathbb R=\bigcup_{n \in \mathbb N} f_n(F)$ Now, I would like to construct a sequence of real numbers $\{y_n\}_{n \in \mathbb N}$ such that $y_n \to z$ where $f^{-1}_n(y_n) \in F$ and $f^{-1}_{n_0}(z)=q$, for some natural number $n_0$ with $q \in \mathbb Q$. I don't know how to continue, I would like to conclude that $q$ is a limit point of $F$ constructing an appropiate sequence in the image of each $f_n$ and arrive to the desired absurd $q \in F$. Could anyone suggest me how to continue from here?",,"['general-topology', 'analysis', 'metric-spaces']"
20,Proving completeness and compactness of a sequence of metric spaces.,Proving completeness and compactness of a sequence of metric spaces.,,"The problem statement Let $(X_n,d_n)_{n \in \mathbb N}$ be a sequence of metric spaces. Consider the product space $X=\prod_{n \in \mathbb N} X_n$ with the distance $d((x_n),(y_n))=\sum_{n \in \mathbb N} \dfrac{d_n(x_n,y_n)}{n^2[1+d_n(x_n,y_n)]}$ $a)$ Prove that $(X,d)$ is complete if and only if each $(X_n,d_n)$ is complete. $b)$ Prove that $(X,d)$ is compact if and only if each $(X_n,d_n)$ is compact. My attempt at a solution: For $a)$, after Willie's answer I could do the following: $\implies$ Let $n$ fixed, call it $n=n_0$ and let $\{y^{j}\}_{j \in \mathbb N}$ be a Cauchy sequence in $(X_{n_0},d_{n_0})$. I fix an arbitrary $x_i \in (X_i,d_i)$ for $i \neq n_0$ and now I consider the sequence $\{\vec{x}^{j}\}_{j \in \mathbb N}$ defined as $\vec{x}^{(j)} = (x_1, x_2, \ldots, x_{n_0-1}, y^{(j)}, x_{n_0+1} , \ldots) \in X$. Lets prove $\{\vec{x}^{j}\}_{j \in \mathbb N}$ is a Cauchy sequence: let $\epsilon>0$, by hypothesis, there is $N \in \mathbb n$ : $\space \forall \space m,n \geq N$, $d_{n_0}(y^n,y^m)< \epsilon$. For $k \neq n_0$, $d_k(x_{k}^m,x_{k}^n)=d_k(x_k,x_k)=0$. Then, $d(\vec{x}^{m},\vec{x}^{n})=\sum_{k \in \mathbb N} \dfrac{d_k(x_{k}^m,x_{k}^n)}{k^2[1+d_k(x_{k}^m,x_{k}^n)]}=\dfrac {d_{n_0}(x_{n_o}^m,x_{n_0}^n)}{{n_0}^2[1+d_{n_0}(x_{n_o}^m,x_{n_0}^n)]}\leq d_{n_0}(y^m,y^n)<\epsilon \space \forall n,m \geq N$. This proves $\{\vec{x}^{j}\}_{j \in \mathbb N}$ is a Cauchy sequence in $X$. which means $\vec{x}^{j} \to  \vec{a}^{\infty}$, $\vec{a}^{\infty}=(a_1,a_2,...,,a_{n_0-1},a_{n_0},a_{n_0+1},...)$. Lets prove that $y^j \to a_{n_0}$ in $(X_{n_0},d_{n_0})$. Given $0<\epsilon<1$, there is $N \in \mathbb N$ : $\space \forall \space n\geq N$, $d(\vec{x}^n,\vec{a}^{\infty})<\dfrac{\epsilon}{n_{0}^2}$. So, $\dfrac{d_{n_0}(y^j,a_{n_0})}{n_{0}^2[1+d_{n_0}(y^j,a_{n_0})]}\leq \sum_{k \in \mathbb N} \dfrac{d_k(x_{k}^n,a_k)}{k^2[1+d_k(x_{k}^n,a_k)]}=d(\vec{x}^n,\vec{a})< \dfrac{\epsilon}{n_{0}^2}$ for $n \geq N$. From here, it follows that $d_{n_0}(y^j,a_{n_0})<\dfrac{\epsilon}{1-\epsilon}<\epsilon$ for all $n\geq N$. We've proved that $\{y^{j}\}_{j \in \mathbb N}$ is a convergent sequence in $(X_{n_0},d_{n_0})$, since the Cauchy sequence  and $n=n_0$ were arbitrary, one can conclude that for every $n \in \mathbb N$, $(X_n,d_n)$ is complete. Now it remains to prove the other implication (I had problems with this one): Let $\{\vec{x}^n\}_{n \in \mathbb N}$ be a Cauchy sequence in $(X,d)$. For a fixed $n=n_0$, lets prove that $\{x_{n_0}^{n}\}_{n \in \mathbb N}$ is a Cauchy sequence in $(X_{n_0},d_{n_0})$. Let $0<\epsilon<1$, by hypothesis, there is $N \in \mathbb N: n,m\geq N \implies d(\vec{x}^n,\vec{x}^m)<\dfrac{\epsilon}{{n_0}^2}$. But then, $\dfrac{d_{n_0}(x_{n_0}^n,x_{n_0}^m)}{n_{0}^2[1+d_{n_0}((x_{n_0}^n,x_{n_0}^m)]}\leq \sum_{k \in \mathbb N} \dfrac{d_k(x_{k}^n,x_{k}^m)}{k^2[1+d_k((x_{k}^n,x_{k}^m)]}=d(\vec{x}^n,\vec{x}^m)<\dfrac{\epsilon}{{n_0}^2}$. We have that $d_{n_0}(x_{n_0}^n,x_{n_0}^m)<\dfrac{\epsilon}{1-\epsilon}<\epsilon$. So the sequence $\{x_{n_0}^n\}_{n \in \mathbb N}$ in $(X_{n_0},d_{n_0})$, which means there is $y^{n_0}=\lim_{n \to \infty} x_{n_0}^n$. If we call $\vec{y}^{\infty}=(y_1,y_2,...,y_n,...)$, lets show that $\vec{y}^{\infty}=\lim_{n \to \infty}\vec{x}^n$. Here I got stuck, given $\epsilon>0$, I don't know which $N$ to choose such that for $n\geq N$, $d(\vec{x}^n,\vec{y}^{\infty})<\epsilon$, for every term of the sequence $\{\vec{x}^n\}_{n \in \mathbb N}$, I will have a different $N_k$ that will depende on the sequence of the space $(X_k,d_k)$. A secondt attempt for this last part: Note that $0\leq d(\vec{x}^n,\vec{y}^{\infty})=\sum_{k \in \mathbb N} \dfrac{d_k(x_{k}^n,y_k)}{k^2[1+d_k(x_{k}^n,y_k)]}\leq \sum_{k \in \mathbb N} \dfrac{1}{k^2}$. Then, $\sum_{k \in \mathbb N} \dfrac{d_k(x_{k}^n,y_k)}{k^2[1+d_k(x_{k}^n,y_k)]}$ is convergent, which means that given $\epsilon>0$, there is $N \in \mathbb N$: $\sum_{N+1}^{\infty}  \dfrac{d_k(x_{k}^n,y_k)}{k^2[1+d_k(x_{k}^n,y_k)]}<\dfrac{\epsilon}{2}$. For a fixed $k$, $\{x_{k}^n\}_{n \in \mathbb N}$ converges to $y_k$ in $(X_k,d_k)$, so there is $n_k : \space \forall \space n_k\geq n$, $d_k(x_{k}^n,y_k)<\dfrac{\epsilon}{2N}$. Consider $M=\max\{n_1,n_2,...,n_N\}$,then $\sum_{k=1}^N \dfrac{d_k(x_{k}^n,y_k)}{k^2[1+d_k(x_{k}^n,y_k)]}\leq \sum_{k=1}^N d_k(x_{k}^n,y_k)<N\dfrac{\epsilon}{2N}=\dfrac{\epsilon}{2}$. So we have that for $n\geq M$, $d(\vec{x}^n,\vec{y}^{\infty})=\sum_{k=1}^N \dfrac{d_k(x_{k}^n,y_k)}{k^2[1+d_k(x_{k}^n,y_k)]}+\sum_{k=N+1}^{\infty} \dfrac{d_k(x_{k}^n,y_k)}{k^2[1+d_k(x_{k}^n,y_k)]}<\dfrac{\epsilon}{2}+\dfrac{\epsilon}{2}=\epsilon$ This proves that $\vec{x}^n \to \vec{y}^{\infty}$; since the sequence $\{\vec{x}^n\}_{n \in \mathbb N}$ was an arbitrary Cauchy sequence in $(X,d)$, then  $(X,d)$ is complete. Sorry if my notation is confusing, I did the best I could but with so many sequences and indexes I easily get lost.","The problem statement Let $(X_n,d_n)_{n \in \mathbb N}$ be a sequence of metric spaces. Consider the product space $X=\prod_{n \in \mathbb N} X_n$ with the distance $d((x_n),(y_n))=\sum_{n \in \mathbb N} \dfrac{d_n(x_n,y_n)}{n^2[1+d_n(x_n,y_n)]}$ $a)$ Prove that $(X,d)$ is complete if and only if each $(X_n,d_n)$ is complete. $b)$ Prove that $(X,d)$ is compact if and only if each $(X_n,d_n)$ is compact. My attempt at a solution: For $a)$, after Willie's answer I could do the following: $\implies$ Let $n$ fixed, call it $n=n_0$ and let $\{y^{j}\}_{j \in \mathbb N}$ be a Cauchy sequence in $(X_{n_0},d_{n_0})$. I fix an arbitrary $x_i \in (X_i,d_i)$ for $i \neq n_0$ and now I consider the sequence $\{\vec{x}^{j}\}_{j \in \mathbb N}$ defined as $\vec{x}^{(j)} = (x_1, x_2, \ldots, x_{n_0-1}, y^{(j)}, x_{n_0+1} , \ldots) \in X$. Lets prove $\{\vec{x}^{j}\}_{j \in \mathbb N}$ is a Cauchy sequence: let $\epsilon>0$, by hypothesis, there is $N \in \mathbb n$ : $\space \forall \space m,n \geq N$, $d_{n_0}(y^n,y^m)< \epsilon$. For $k \neq n_0$, $d_k(x_{k}^m,x_{k}^n)=d_k(x_k,x_k)=0$. Then, $d(\vec{x}^{m},\vec{x}^{n})=\sum_{k \in \mathbb N} \dfrac{d_k(x_{k}^m,x_{k}^n)}{k^2[1+d_k(x_{k}^m,x_{k}^n)]}=\dfrac {d_{n_0}(x_{n_o}^m,x_{n_0}^n)}{{n_0}^2[1+d_{n_0}(x_{n_o}^m,x_{n_0}^n)]}\leq d_{n_0}(y^m,y^n)<\epsilon \space \forall n,m \geq N$. This proves $\{\vec{x}^{j}\}_{j \in \mathbb N}$ is a Cauchy sequence in $X$. which means $\vec{x}^{j} \to  \vec{a}^{\infty}$, $\vec{a}^{\infty}=(a_1,a_2,...,,a_{n_0-1},a_{n_0},a_{n_0+1},...)$. Lets prove that $y^j \to a_{n_0}$ in $(X_{n_0},d_{n_0})$. Given $0<\epsilon<1$, there is $N \in \mathbb N$ : $\space \forall \space n\geq N$, $d(\vec{x}^n,\vec{a}^{\infty})<\dfrac{\epsilon}{n_{0}^2}$. So, $\dfrac{d_{n_0}(y^j,a_{n_0})}{n_{0}^2[1+d_{n_0}(y^j,a_{n_0})]}\leq \sum_{k \in \mathbb N} \dfrac{d_k(x_{k}^n,a_k)}{k^2[1+d_k(x_{k}^n,a_k)]}=d(\vec{x}^n,\vec{a})< \dfrac{\epsilon}{n_{0}^2}$ for $n \geq N$. From here, it follows that $d_{n_0}(y^j,a_{n_0})<\dfrac{\epsilon}{1-\epsilon}<\epsilon$ for all $n\geq N$. We've proved that $\{y^{j}\}_{j \in \mathbb N}$ is a convergent sequence in $(X_{n_0},d_{n_0})$, since the Cauchy sequence  and $n=n_0$ were arbitrary, one can conclude that for every $n \in \mathbb N$, $(X_n,d_n)$ is complete. Now it remains to prove the other implication (I had problems with this one): Let $\{\vec{x}^n\}_{n \in \mathbb N}$ be a Cauchy sequence in $(X,d)$. For a fixed $n=n_0$, lets prove that $\{x_{n_0}^{n}\}_{n \in \mathbb N}$ is a Cauchy sequence in $(X_{n_0},d_{n_0})$. Let $0<\epsilon<1$, by hypothesis, there is $N \in \mathbb N: n,m\geq N \implies d(\vec{x}^n,\vec{x}^m)<\dfrac{\epsilon}{{n_0}^2}$. But then, $\dfrac{d_{n_0}(x_{n_0}^n,x_{n_0}^m)}{n_{0}^2[1+d_{n_0}((x_{n_0}^n,x_{n_0}^m)]}\leq \sum_{k \in \mathbb N} \dfrac{d_k(x_{k}^n,x_{k}^m)}{k^2[1+d_k((x_{k}^n,x_{k}^m)]}=d(\vec{x}^n,\vec{x}^m)<\dfrac{\epsilon}{{n_0}^2}$. We have that $d_{n_0}(x_{n_0}^n,x_{n_0}^m)<\dfrac{\epsilon}{1-\epsilon}<\epsilon$. So the sequence $\{x_{n_0}^n\}_{n \in \mathbb N}$ in $(X_{n_0},d_{n_0})$, which means there is $y^{n_0}=\lim_{n \to \infty} x_{n_0}^n$. If we call $\vec{y}^{\infty}=(y_1,y_2,...,y_n,...)$, lets show that $\vec{y}^{\infty}=\lim_{n \to \infty}\vec{x}^n$. Here I got stuck, given $\epsilon>0$, I don't know which $N$ to choose such that for $n\geq N$, $d(\vec{x}^n,\vec{y}^{\infty})<\epsilon$, for every term of the sequence $\{\vec{x}^n\}_{n \in \mathbb N}$, I will have a different $N_k$ that will depende on the sequence of the space $(X_k,d_k)$. A secondt attempt for this last part: Note that $0\leq d(\vec{x}^n,\vec{y}^{\infty})=\sum_{k \in \mathbb N} \dfrac{d_k(x_{k}^n,y_k)}{k^2[1+d_k(x_{k}^n,y_k)]}\leq \sum_{k \in \mathbb N} \dfrac{1}{k^2}$. Then, $\sum_{k \in \mathbb N} \dfrac{d_k(x_{k}^n,y_k)}{k^2[1+d_k(x_{k}^n,y_k)]}$ is convergent, which means that given $\epsilon>0$, there is $N \in \mathbb N$: $\sum_{N+1}^{\infty}  \dfrac{d_k(x_{k}^n,y_k)}{k^2[1+d_k(x_{k}^n,y_k)]}<\dfrac{\epsilon}{2}$. For a fixed $k$, $\{x_{k}^n\}_{n \in \mathbb N}$ converges to $y_k$ in $(X_k,d_k)$, so there is $n_k : \space \forall \space n_k\geq n$, $d_k(x_{k}^n,y_k)<\dfrac{\epsilon}{2N}$. Consider $M=\max\{n_1,n_2,...,n_N\}$,then $\sum_{k=1}^N \dfrac{d_k(x_{k}^n,y_k)}{k^2[1+d_k(x_{k}^n,y_k)]}\leq \sum_{k=1}^N d_k(x_{k}^n,y_k)<N\dfrac{\epsilon}{2N}=\dfrac{\epsilon}{2}$. So we have that for $n\geq M$, $d(\vec{x}^n,\vec{y}^{\infty})=\sum_{k=1}^N \dfrac{d_k(x_{k}^n,y_k)}{k^2[1+d_k(x_{k}^n,y_k)]}+\sum_{k=N+1}^{\infty} \dfrac{d_k(x_{k}^n,y_k)}{k^2[1+d_k(x_{k}^n,y_k)]}<\dfrac{\epsilon}{2}+\dfrac{\epsilon}{2}=\epsilon$ This proves that $\vec{x}^n \to \vec{y}^{\infty}$; since the sequence $\{\vec{x}^n\}_{n \in \mathbb N}$ was an arbitrary Cauchy sequence in $(X,d)$, then  $(X,d)$ is complete. Sorry if my notation is confusing, I did the best I could but with so many sequences and indexes I easily get lost.",,"['analysis', 'metric-spaces', 'compactness']"
21,Does this Manifold exist?,Does this Manifold exist?,,"The excercise is the following: Give an example or disprove: There is at least one m-dimensional manifold that is compact in some $\mathbb{R}^n$ such that one chart is sufficient to get the whole manifold. My idea was that this cannot be right. Reasoning: If yes, then there is a chart $\phi:U \rightarrow M$, where $U$ is open, but in that case $\phi^{-1}(M)$ is compact. This contradicts $U$ open since a chart is a bijection, so the inverse map should have got us $U$. My problem is, that this argument is very trivial and I am somewhat scared that it is wrong, so is there anyone who could check my argument?","The excercise is the following: Give an example or disprove: There is at least one m-dimensional manifold that is compact in some $\mathbb{R}^n$ such that one chart is sufficient to get the whole manifold. My idea was that this cannot be right. Reasoning: If yes, then there is a chart $\phi:U \rightarrow M$, where $U$ is open, but in that case $\phi^{-1}(M)$ is compact. This contradicts $U$ open since a chart is a bijection, so the inverse map should have got us $U$. My problem is, that this argument is very trivial and I am somewhat scared that it is wrong, so is there anyone who could check my argument?",,"['calculus', 'real-analysis']"
22,An example of a non-closable operator,An example of a non-closable operator,,"I've encountered the following: Consider the usual Hilbert space $L^2([0,1],dx)$ and the dense subspace $\mathcal{D}=\mathcal{C}[0,1]$.  Define $T$ on $\mathcal{D}$ by $T(f)=f(0)$.  This is a densely defined operator, but it its adjoint is not densely defined. I'm not so familiar with computing adjoints. Could someone give me a hint how one can find and see that the adjoint is not densely-defined? I'm also interested if there are other 'simple' examples of non-closable operators Thanks","I've encountered the following: Consider the usual Hilbert space $L^2([0,1],dx)$ and the dense subspace $\mathcal{D}=\mathcal{C}[0,1]$.  Define $T$ on $\mathcal{D}$ by $T(f)=f(0)$.  This is a densely defined operator, but it its adjoint is not densely defined. I'm not so familiar with computing adjoints. Could someone give me a hint how one can find and see that the adjoint is not densely-defined? I'm also interested if there are other 'simple' examples of non-closable operators Thanks",,"['analysis', 'functional-analysis', 'operator-theory']"
23,Lower bound on a number theoretic function,Lower bound on a number theoretic function,,"Let $n$ be a positive odd integer, let  $$n_j = \Bigl\{\frac{n}{2^{j+1}}\Bigr\}\,,$$ where $\{x\}$ denotes the fractional part of $x$, and finally let $k = \lceil \log_2 n\rceil$. Consider the function $$ f(n) = \prod_{j=1}^{k} \bigl[1- 4n_j(1-n_j)\bigr]\,. $$ Each term in this product is bounded in the interval $(0,1)$, so $f(n)$ will tend to $0$ for large $n$. My question is how one can Prove that $f(n) \ge \frac{c}{n^a}$ for some absolute positive constants $a$, $c$. It is easy to show (using bounds on the fractional part) that there exist $a,c>0$ such that $$ f(n) \ge \frac{c}{n^{a \log n}} \,, $$ so that the product decays only subexponentially slowly, but a heuristic argument as well as numerics indicates that the true scaling is actually a much slower inverse polynomial. The heuristic argument is, roughly, that the terms in the product are highly correlated, and if one term is small, the following term must be large. I have been unable to leverage this into a proof, however. I indicated in the title that this is a ""number theoretic"" function because the behavior depends heavily on the bit-wise structure of $n$, and it has some interesting fractal features as a result.","Let $n$ be a positive odd integer, let  $$n_j = \Bigl\{\frac{n}{2^{j+1}}\Bigr\}\,,$$ where $\{x\}$ denotes the fractional part of $x$, and finally let $k = \lceil \log_2 n\rceil$. Consider the function $$ f(n) = \prod_{j=1}^{k} \bigl[1- 4n_j(1-n_j)\bigr]\,. $$ Each term in this product is bounded in the interval $(0,1)$, so $f(n)$ will tend to $0$ for large $n$. My question is how one can Prove that $f(n) \ge \frac{c}{n^a}$ for some absolute positive constants $a$, $c$. It is easy to show (using bounds on the fractional part) that there exist $a,c>0$ such that $$ f(n) \ge \frac{c}{n^{a \log n}} \,, $$ so that the product decays only subexponentially slowly, but a heuristic argument as well as numerics indicates that the true scaling is actually a much slower inverse polynomial. The heuristic argument is, roughly, that the terms in the product are highly correlated, and if one term is small, the following term must be large. I have been unable to leverage this into a proof, however. I indicated in the title that this is a ""number theoretic"" function because the behavior depends heavily on the bit-wise structure of $n$, and it has some interesting fractal features as a result.",,"['analysis', 'number-theory', 'inequality', 'binary']"
24,Example of two series with certain properties?,Example of two series with certain properties?,,"Find 2 series $\sum a_k$ and $\sum b_k$ such that $\sum b_k$ converges conditionally, $\dfrac{a_k}{b_k} \rightarrow 1$ as $k \rightarrow \infty$, and $\sum a_k$ diverges. Can someone give me a hint with this? Thanks.","Find 2 series $\sum a_k$ and $\sum b_k$ such that $\sum b_k$ converges conditionally, $\dfrac{a_k}{b_k} \rightarrow 1$ as $k \rightarrow \infty$, and $\sum a_k$ diverges. Can someone give me a hint with this? Thanks.",,"['real-analysis', 'sequences-and-series', 'analysis']"
25,Verify the spectral radius $r(A) = \lim_{n\rightarrow\infty}||A^n||^{1/n}$.,Verify the spectral radius .,r(A) = \lim_{n\rightarrow\infty}||A^n||^{1/n},I want to Verify the spectral radius $r(A) = \lim_{n\rightarrow\infty}\|A^n\|^{1/n}$. Where $A$ is a matrix. I have a proof that involves Jordan Blocks. The proof is long and involved but it not to hard to understand. I am interested in knowing if there is an easier (shorter) proof that does not involve using Jordan Blocks. Any help and comments are appreciated. Thank you in advance.,I want to Verify the spectral radius $r(A) = \lim_{n\rightarrow\infty}\|A^n\|^{1/n}$. Where $A$ is a matrix. I have a proof that involves Jordan Blocks. The proof is long and involved but it not to hard to understand. I am interested in knowing if there is an easier (shorter) proof that does not involve using Jordan Blocks. Any help and comments are appreciated. Thank you in advance.,,"['analysis', 'functional-analysis', 'numerical-methods', 'self-learning', 'numerical-linear-algebra']"
26,"Proving the existence of a non-monotone continuous function defined on $[0,1]$",Proving the existence of a non-monotone continuous function defined on,"[0,1]","Let $(I_n)_{n \in \mathbb N}$ be the sequence of intervals of $[0,1]$ with rational endpoints, and for every $n \in \mathbb N~$ let $E_n=\{f \in C[0,1] : f \:\text{is monotone in}\: I_n\}$. Prove that for every $n \in \mathbb N$, $E_n$ is closed and nowhere dense in $(C[0,1],d_\infty)$. Deduce that there are continuous functions in the interval $[0,1]$ which aren't monotone in any subinterval. For a given $n$, $E_n$ can be expressed as $E_n=E_{n\nearrow} \cup E_{n\swarrow}$ where $E_{n\nearrow}$ and $E_{n\swarrow}$ are the sets of monotonically increasing functions and monotonically decreasing functions in $E_n$ respectively. I am having problems trying to prove that these two sets are closed. I mean, take $f \in (C[0,1],d_\infty)$ such that there is $\{f_k\}_{k \in \mathbb N} \subset E_{n\nearrow}$ with $f_k \to f$. How can I prove $f \in E_{n\nearrow}$?. Suppose I could prove this, then I have to show that $\overline {E_n}^\circ=E_n^\circ=\emptyset$. This means that for every $f \in E_n$ and every $r>0$, there is $g \in B(f,r)$ such that g is not monotone. Again, I am stuck. If I could solve this two points, it's not difficult to check the hypothesis and apply the Baire category theorem to prove the last statement.","Let $(I_n)_{n \in \mathbb N}$ be the sequence of intervals of $[0,1]$ with rational endpoints, and for every $n \in \mathbb N~$ let $E_n=\{f \in C[0,1] : f \:\text{is monotone in}\: I_n\}$. Prove that for every $n \in \mathbb N$, $E_n$ is closed and nowhere dense in $(C[0,1],d_\infty)$. Deduce that there are continuous functions in the interval $[0,1]$ which aren't monotone in any subinterval. For a given $n$, $E_n$ can be expressed as $E_n=E_{n\nearrow} \cup E_{n\swarrow}$ where $E_{n\nearrow}$ and $E_{n\swarrow}$ are the sets of monotonically increasing functions and monotonically decreasing functions in $E_n$ respectively. I am having problems trying to prove that these two sets are closed. I mean, take $f \in (C[0,1],d_\infty)$ such that there is $\{f_k\}_{k \in \mathbb N} \subset E_{n\nearrow}$ with $f_k \to f$. How can I prove $f \in E_{n\nearrow}$?. Suppose I could prove this, then I have to show that $\overline {E_n}^\circ=E_n^\circ=\emptyset$. This means that for every $f \in E_n$ and every $r>0$, there is $g \in B(f,r)$ such that g is not monotone. Again, I am stuck. If I could solve this two points, it's not difficult to check the hypothesis and apply the Baire category theorem to prove the last statement.",,"['analysis', 'metric-spaces', 'baire-category']"
27,Painting $\mathbb R^+$ with two colors which sum of two same color numbers be the same.,Painting  with two colors which sum of two same color numbers be the same.,\mathbb R^+,Can any one paint  $\mathbb R^+$ with two colors which sum of two numbers with the same color has the same color. Additional condition: Both colors should be used. I tried use Cauchy functions like ($f(x+y)=f(x)+f(y)$). But there was no result.,Can any one paint  $\mathbb R^+$ with two colors which sum of two numbers with the same color has the same color. Additional condition: Both colors should be used. I tried use Cauchy functions like ($f(x+y)=f(x)+f(y)$). But there was no result.,,"['abstract-algebra', 'analysis', 'functions', 'ideals']"
28,"How to show that $W^{2,\infty}(B_1)=C^{1,1}(\bar B_1)$?",How to show that ?,"W^{2,\infty}(B_1)=C^{1,1}(\bar B_1)","Suppose that $B_1$ is the open unit ball in $\mathbb R^n$, denote $W^{2,\infty}(B_1)$ be the sobolev spaces and $C^{1,1}(\bar B_1)$ is the Holder spaces. It seems the equality $W^{2,\infty}(B_1)=C^{1,1}(\bar B_1)$ holds at  first glance, but how to move from intuition to a strictly argument? What's more, I don't know why closed $\bar B_1$ rather than $B_1$? Another thing: is this some kind of problem suitable for research or just a well known results?","Suppose that $B_1$ is the open unit ball in $\mathbb R^n$, denote $W^{2,\infty}(B_1)$ be the sobolev spaces and $C^{1,1}(\bar B_1)$ is the Holder spaces. It seems the equality $W^{2,\infty}(B_1)=C^{1,1}(\bar B_1)$ holds at  first glance, but how to move from intuition to a strictly argument? What's more, I don't know why closed $\bar B_1$ rather than $B_1$? Another thing: is this some kind of problem suitable for research or just a well known results?",,"['analysis', 'partial-differential-equations', 'sobolev-spaces']"
29,"$C^{2, \alpha}$ regularity for elliptic equations with Neumann boundary conditons",regularity for elliptic equations with Neumann boundary conditons,"C^{2, \alpha}","Say $\Omega\subseteq \mathbb{R}^n$ is a bounded open set and $0<\alpha<1$. I need some $C^{2, \alpha}(\overline\Omega)$ regularity result for elliptic equations with Neumann boundary conditions but that is what I find in Jurgen Jost' book: If $u$ is a weak solution of $\Delta u=f$ in $\Omega$ and $f\in C^\alpha(\Omega)$, then $u\in C^{2,\alpha}(\Omega)$..........(1) But it says that we can identify $C^{2, \alpha}(\Omega)$ with $C^{2,\alpha}(\overline\Omega)$ here: http://www.math.ucsd.edu/~bdriver/231-02-03/Lecture_Notes/Holder-spaces.pdf . However, Jost used both $C^{2, \alpha}(\Omega)$ and  $C^{2,\alpha}(\overline\Omega)$ (when he is doing regularity for Dirichlet boundary) in his book, which indicated these two spaces are not the same? Or do we have $u\in C^{2,\alpha}(\overline\Omega)$ in (1)? Otherwise where can I find some $C^{2,\alpha}(\overline\Omega)$ regularity result with Neumann boundary conditon?","Say $\Omega\subseteq \mathbb{R}^n$ is a bounded open set and $0<\alpha<1$. I need some $C^{2, \alpha}(\overline\Omega)$ regularity result for elliptic equations with Neumann boundary conditions but that is what I find in Jurgen Jost' book: If $u$ is a weak solution of $\Delta u=f$ in $\Omega$ and $f\in C^\alpha(\Omega)$, then $u\in C^{2,\alpha}(\Omega)$..........(1) But it says that we can identify $C^{2, \alpha}(\Omega)$ with $C^{2,\alpha}(\overline\Omega)$ here: http://www.math.ucsd.edu/~bdriver/231-02-03/Lecture_Notes/Holder-spaces.pdf . However, Jost used both $C^{2, \alpha}(\Omega)$ and  $C^{2,\alpha}(\overline\Omega)$ (when he is doing regularity for Dirichlet boundary) in his book, which indicated these two spaces are not the same? Or do we have $u\in C^{2,\alpha}(\overline\Omega)$ in (1)? Otherwise where can I find some $C^{2,\alpha}(\overline\Omega)$ regularity result with Neumann boundary conditon?",,"['analysis', 'partial-differential-equations', 'definition']"
30,How prove this analysis function $a\le\frac{1}{2}$,How prove this analysis function,a\le\frac{1}{2},"let $$f(x)=\begin{cases} x\sin{\dfrac{1}{x}}&x\neq 0\\ 0&x=0 \end{cases}$$   show that:there exsit $M>0,(x^2+y^2\neq 0)$ ,   $$F(x,y)=\dfrac{f(x)-f(y)}{|x-y|^{a}}|\le M  \Longleftrightarrow a\le\dfrac{1}{2}$$ My try: (1)if $a\le\dfrac{1}{2}$, then   $$\dfrac{f(x)-f(y)}{|x-y|^a}=\dfrac{x\sin{\frac{1}{x}}-y\sin{\frac{1}{y}}}{|x-y|^a}$$   then How can prove   $$|x\sin{\frac{1}{x}}-y\sin{\frac{1}{y}}|<M|x-y|^a,a\le\dfrac{1}{2}$$ By other hand : and if for any $x,y\in R$,and such $$ |x\sin{\frac{1}{x}}-y\sin{\frac{1}{y}}|<M|x-y|^a$$   then How prove must $a\le\dfrac{1}{2}$? I think this is nice problem,Thank you By the way:when I deal this problem, I find this nice equality $$|x\sin{\frac{1}{x}}-y\sin{\frac{1}{y}}|<2\sqrt{|x-y|}$$ But I can't prove  ,Thank you","let $$f(x)=\begin{cases} x\sin{\dfrac{1}{x}}&x\neq 0\\ 0&x=0 \end{cases}$$   show that:there exsit $M>0,(x^2+y^2\neq 0)$ ,   $$F(x,y)=\dfrac{f(x)-f(y)}{|x-y|^{a}}|\le M  \Longleftrightarrow a\le\dfrac{1}{2}$$ My try: (1)if $a\le\dfrac{1}{2}$, then   $$\dfrac{f(x)-f(y)}{|x-y|^a}=\dfrac{x\sin{\frac{1}{x}}-y\sin{\frac{1}{y}}}{|x-y|^a}$$   then How can prove   $$|x\sin{\frac{1}{x}}-y\sin{\frac{1}{y}}|<M|x-y|^a,a\le\dfrac{1}{2}$$ By other hand : and if for any $x,y\in R$,and such $$ |x\sin{\frac{1}{x}}-y\sin{\frac{1}{y}}|<M|x-y|^a$$   then How prove must $a\le\dfrac{1}{2}$? I think this is nice problem,Thank you By the way:when I deal this problem, I find this nice equality $$|x\sin{\frac{1}{x}}-y\sin{\frac{1}{y}}|<2\sqrt{|x-y|}$$ But I can't prove  ,Thank you",,['analysis']
31,"Prove $\int_0^1|f(t)-g(t)|dt \le (\int_0^1|f(t)-g(t)|^2dt)^{1/2} \le \sup_{t\in[0,1]}|f(t)-g(t)|$",Prove,"\int_0^1|f(t)-g(t)|dt \le (\int_0^1|f(t)-g(t)|^2dt)^{1/2} \le \sup_{t\in[0,1]}|f(t)-g(t)|","Let $C[0,1]$ be the set of all continuous real-valued functions on $[0,1]$. Let these be 3 metrics on $C$. $p(f,g)=\sup_{t\in[0,1]}|f(t)-g(t)|$ $d(f,g)=(\int_0^1|f(t)-g(t)|^2dt)^{1/2}$ $t(f,g)=\int_0^1|f(t)-g(t)|dt$ Prove that for every $f,g\in C$, the following holds $t(f,g)\le d(f,g)\le p(f,g)$ I understand that $t(f,g)\le p(f,g)$ since $t(f,g)=\int_0^1|f(t)-g(t)|dt \le \int_0^1\sup_{t\in[0,1]}|f(t)-g(t)|dt =\sup_{t\in[0,1]}|f(t)-g(t)|=p(f,g)$. But I can't get the others. I think using Schwarz's inequality might be useful $(\int_0^1 w(t)v(t)dt)^2 \le (\int_0^1w^2(t)dt)(\int_0^1v^2(t)dt)$","Let $C[0,1]$ be the set of all continuous real-valued functions on $[0,1]$. Let these be 3 metrics on $C$. $p(f,g)=\sup_{t\in[0,1]}|f(t)-g(t)|$ $d(f,g)=(\int_0^1|f(t)-g(t)|^2dt)^{1/2}$ $t(f,g)=\int_0^1|f(t)-g(t)|dt$ Prove that for every $f,g\in C$, the following holds $t(f,g)\le d(f,g)\le p(f,g)$ I understand that $t(f,g)\le p(f,g)$ since $t(f,g)=\int_0^1|f(t)-g(t)|dt \le \int_0^1\sup_{t\in[0,1]}|f(t)-g(t)|dt =\sup_{t\in[0,1]}|f(t)-g(t)|=p(f,g)$. But I can't get the others. I think using Schwarz's inequality might be useful $(\int_0^1 w(t)v(t)dt)^2 \le (\int_0^1w^2(t)dt)(\int_0^1v^2(t)dt)$",,"['real-analysis', 'analysis', 'functional-analysis', 'metric-spaces']"
32,How to show this identity involving multi-index?,How to show this identity involving multi-index?,,"I need some help for showing, $$\displaystyle \sum_{\beta \leq \alpha} \binom{\alpha}{\beta}(-1)^{|\alpha-\beta|}=0,$$ where $\alpha, \beta\in\mathbb N_0^n$ and $N_0=\mathbb N\cup \{0\}$. Any help will be welcome, thanks..","I need some help for showing, $$\displaystyle \sum_{\beta \leq \alpha} \binom{\alpha}{\beta}(-1)^{|\alpha-\beta|}=0,$$ where $\alpha, \beta\in\mathbb N_0^n$ and $N_0=\mathbb N\cup \{0\}$. Any help will be welcome, thanks..",,['analysis']
33,Is this inequality true? (Inequality involving probability distribution and products),Is this inequality true? (Inequality involving probability distribution and products),,"Suppose $f(z)$ is a discrete probability distribution with space $S$. Suppose $g(z),h(z)>0$ for all $z \in S$. Is it true that $$\prod_{z \in S}{g(z)^{f(z)}}+\prod_{z \in S}{h(z)^{f(z)}} \leq \prod_{z \in S}{[g(z)+h(z)]^{f(z)}}?$$ My first impulse was to use Jensen's Inequality, but to no avail. This inequality is part of a much larger theorem I am trying to prove. Thanks for your help!","Suppose $f(z)$ is a discrete probability distribution with space $S$. Suppose $g(z),h(z)>0$ for all $z \in S$. Is it true that $$\prod_{z \in S}{g(z)^{f(z)}}+\prod_{z \in S}{h(z)^{f(z)}} \leq \prod_{z \in S}{[g(z)+h(z)]^{f(z)}}?$$ My first impulse was to use Jensen's Inequality, but to no avail. This inequality is part of a much larger theorem I am trying to prove. Thanks for your help!",,"['probability', 'analysis']"
34,"$\lim \limits_{t \to a} \int_{c}^{d} f(t, s) ds= \int_{c}^{d} \lim \limits_{t \to a} f(t,s) ds$?",?,"\lim \limits_{t \to a} \int_{c}^{d} f(t, s) ds= \int_{c}^{d} \lim \limits_{t \to a} f(t,s) ds","When is $\lim \limits_{t \to a} \int_{c}^{d} f(t, s) ds= \int_{c}^{d} \lim \limits_{t \to a}  f(t,s) ds$ ? My guess is that $f$ uniformly continuous in a neighborhood of $(a,s)$ for $c\leq s \leq d$ is a sufficient condition for the above equality to hold. This is my try for a proof of this: for $(t,s)$ in this neighborhood we may choose $\delta >0$ such that $\left |f(t,s)-f(a,s)  \right |< \frac{\epsilon}{d-c}$ whenever $\left |(t,s)-(a,s)  \right |< \delta$ . And hence in such an neighborhood \begin{align}\left |\lim \limits_{t \to a} \int_{c}^{d} f(t, s) ds- \int_{c}^{d} \lim \limits_{t \to a}  f(t,s) ds\right |&= \left |\lim \limits_{t \to a} \int_{c}^{d} f(t, s) -  f(a,s) ds\right |\\ \ \\ &\leq \lim \limits_{t \to a} \int_{c}^{d}\left | f(t, s) -  f(a,s) \right |ds\\ \ \\ &< \epsilon.\end{align} I wonder if the proof given is correct and furthermore I wonder if the assumption of $f$ uniformly continuous is needed? Can any one give an example of a continous function for which the above doesn't hold. Btw I am aware that every continous function from a compact metric space into any metric space is uniformly continous, so I understand that $f$ being continuous on a compact set is sufficient.","When is ? My guess is that uniformly continuous in a neighborhood of for is a sufficient condition for the above equality to hold. This is my try for a proof of this: for in this neighborhood we may choose such that whenever . And hence in such an neighborhood I wonder if the proof given is correct and furthermore I wonder if the assumption of uniformly continuous is needed? Can any one give an example of a continous function for which the above doesn't hold. Btw I am aware that every continous function from a compact metric space into any metric space is uniformly continous, so I understand that being continuous on a compact set is sufficient.","\lim \limits_{t \to a} \int_{c}^{d} f(t, s) ds= \int_{c}^{d} \lim \limits_{t \to a}  f(t,s) ds f (a,s) c\leq s \leq d (t,s) \delta >0 \left |f(t,s)-f(a,s)  \right |< \frac{\epsilon}{d-c} \left |(t,s)-(a,s)  \right |< \delta \begin{align}\left |\lim \limits_{t \to a} \int_{c}^{d} f(t, s) ds- \int_{c}^{d} \lim \limits_{t \to a}  f(t,s) ds\right |&= \left |\lim \limits_{t \to a} \int_{c}^{d} f(t, s) -  f(a,s) ds\right |\\ \ \\ &\leq \lim \limits_{t \to a} \int_{c}^{d}\left | f(t, s) -  f(a,s) \right |ds\\ \ \\ &< \epsilon.\end{align} f f","['calculus', 'analysis']"
35,Why does $\sum\limits_{k=n+1}^\infty\frac{r^{2k+1}}{(2k+1)!}$ converge to $0$?,Why does  converge to ?,\sum\limits_{k=n+1}^\infty\frac{r^{2k+1}}{(2k+1)!} 0,"I'm being told that because the following series is the tail end of a convergent series, it converges to zero as $n$ gets large: $$\sum\limits_{k=n+1}^\infty\frac{r^{2k+1}}{(2k+1)!}$$ The tail end of which convergent series? $e^r$? If so, then the above series is actually every other term of the tail send of the power series for $e^r$, right? Or how else to see that the above series converges to $0$? Or does the series sum to zero simply because as $n$ gets large, the number of terms get arbitrarily small?","I'm being told that because the following series is the tail end of a convergent series, it converges to zero as $n$ gets large: $$\sum\limits_{k=n+1}^\infty\frac{r^{2k+1}}{(2k+1)!}$$ The tail end of which convergent series? $e^r$? If so, then the above series is actually every other term of the tail send of the power series for $e^r$, right? Or how else to see that the above series converges to $0$? Or does the series sum to zero simply because as $n$ gets large, the number of terms get arbitrarily small?",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
36,Real Analysis: Prove a set is closed,Real Analysis: Prove a set is closed,,"Let R be equipped with the usual Euclidean metric. Show that the set $S=\{(x,y) \mid 0 \leq y\leq x.\}$ is closed. The approach i am trying is to prove the complement is open, which would imply that the set is closed. I believe the complement is the union of the set where $y < 0$ and the set where $y > x$. I understand that if I can prove that if both sets are open, then the union is open. However, i get tripped up on how to prove the sets are open, Also as a side note: i'm new here, and any help on how to use mathematical notation such as the less than or equal sign would be appreciated","Let R be equipped with the usual Euclidean metric. Show that the set $S=\{(x,y) \mid 0 \leq y\leq x.\}$ is closed. The approach i am trying is to prove the complement is open, which would imply that the set is closed. I believe the complement is the union of the set where $y < 0$ and the set where $y > x$. I understand that if I can prove that if both sets are open, then the union is open. However, i get tripped up on how to prove the sets are open, Also as a side note: i'm new here, and any help on how to use mathematical notation such as the less than or equal sign would be appreciated",,"['general-topology', 'analysis', 'metric-spaces']"
37,How can I construct envelop of unity?,How can I construct envelop of unity?,,"Given a topological space $B$ and an open covering $\{ U_i \}_{i\in I}$ of $B$ with a partition of unity $\{ \varphi_i \}_{i \in I}$ such that $\operatorname{supp}(\varphi_i) \subset U_i$ $\forall i$, how can I construct a family of maps $\{ \eta_j \}_{j \in  J}$ with the properties that $\{\operatorname{supp}(\eta_j)\}$ is locally finite, $\forall j \exists i$ such that $\operatorname{supp}(\eta_j)\subset U_i$ and $\forall b \in B$ $\max_{j\in J}(\eta_j(b))=1$? I found this problem studying the homotopical classification of fibre bundle in Husemoller's ""Fibre Bundle"".","Given a topological space $B$ and an open covering $\{ U_i \}_{i\in I}$ of $B$ with a partition of unity $\{ \varphi_i \}_{i \in I}$ such that $\operatorname{supp}(\varphi_i) \subset U_i$ $\forall i$, how can I construct a family of maps $\{ \eta_j \}_{j \in  J}$ with the properties that $\{\operatorname{supp}(\eta_j)\}$ is locally finite, $\forall j \exists i$ such that $\operatorname{supp}(\eta_j)\subset U_i$ and $\forall b \in B$ $\max_{j\in J}(\eta_j(b))=1$? I found this problem studying the homotopical classification of fibre bundle in Husemoller's ""Fibre Bundle"".",,"['general-topology', 'analysis']"
38,Does projection onto a finite dimensional subspace commute with intersection of a decreasing sequence of subspaces: $\cap_i P_W(V_i)=P_W(\cap_i V_i)$?,Does projection onto a finite dimensional subspace commute with intersection of a decreasing sequence of subspaces: ?,\cap_i P_W(V_i)=P_W(\cap_i V_i),"Let $V$ to be an infinite dimensional linear space over some field $k$. (you can take $k=\mathbb{C}$, or further assume $V$ is a complex Hilbert space). And assume $W$ is a finite dimensional subspace of $V$, and denote the projection from $V$ to $W$ by $P_W$. Let $V_1\supseteq V_2\supseteq V_3\supseteq\cdots$ be a decreasing sequence of subspaces of $V$, denote $V_{\infty}=\cap_{i=1}^{\infty}V_i$. Since $\{P_W(V_i)\}_{i=1}^{\infty}$ is a decreasing sequence of subspaces of the finite dimensional space $W$, then it would be stable after some sufficient large $j$, i.e, $P_W(V_{j})=P_W(V_{j+1})=\cdots :=\lim_iP_W(V_i)$. My question is: $$\lim_iP_W(V_i)=P_W(V_{\infty})?$$ Any counterexamples? Remarks: 1, Note that $P_W(V_i)\neq V_i\cap W$, $W\cap (W_1+W_2)\neq W\cap W_1+W\cap W_2$ in general for linear subspaces $W, W_1, W_2$. 2, The nontrivial case is all the $V_i$ have infinite dimension. 3, If $V, V_i's$ are all Hilbert spaces, we think $P_W$ as the orthogonal projection.","Let $V$ to be an infinite dimensional linear space over some field $k$. (you can take $k=\mathbb{C}$, or further assume $V$ is a complex Hilbert space). And assume $W$ is a finite dimensional subspace of $V$, and denote the projection from $V$ to $W$ by $P_W$. Let $V_1\supseteq V_2\supseteq V_3\supseteq\cdots$ be a decreasing sequence of subspaces of $V$, denote $V_{\infty}=\cap_{i=1}^{\infty}V_i$. Since $\{P_W(V_i)\}_{i=1}^{\infty}$ is a decreasing sequence of subspaces of the finite dimensional space $W$, then it would be stable after some sufficient large $j$, i.e, $P_W(V_{j})=P_W(V_{j+1})=\cdots :=\lim_iP_W(V_i)$. My question is: $$\lim_iP_W(V_i)=P_W(V_{\infty})?$$ Any counterexamples? Remarks: 1, Note that $P_W(V_i)\neq V_i\cap W$, $W\cap (W_1+W_2)\neq W\cap W_1+W\cap W_2$ in general for linear subspaces $W, W_1, W_2$. 2, The nontrivial case is all the $V_i$ have infinite dimension. 3, If $V, V_i's$ are all Hilbert spaces, we think $P_W$ as the orthogonal projection.",,"['linear-algebra', 'analysis', 'operator-theory', 'hilbert-spaces', 'approximation']"
39,Numerical integration over a surface of a sphere,Numerical integration over a surface of a sphere,,"I am integrating a double integral in spherical coordinates over the surface of a sphere in MATLAB numerically. Although I have changed the relative and absolute tolerance I get the feeling that this algorithm never terminates. And when I checked the values of my function that MATLAB had evaluated everything looked fine, no huge oscillations, no singularities. Do you know whether there are other algorithms that I could use which work faster and still give me reliable results? So the sphere sits at $(0,0,0)$ and has a radius of $10^{-6}$. Don't be confused that the $y$-component is not shown, I took this one as being fixed in order to plot this. Okay, maybe I define my problem a little bit better: I want to perform this integration as fast as possible with an accuracy that should be about $10^{-1}$ or $10^{-2}$, this is sufficient. Of course, I thought about adding this lower precision to the integral2 function, but still, this function is so slowly that it is useless for what I am currently doing here.","I am integrating a double integral in spherical coordinates over the surface of a sphere in MATLAB numerically. Although I have changed the relative and absolute tolerance I get the feeling that this algorithm never terminates. And when I checked the values of my function that MATLAB had evaluated everything looked fine, no huge oscillations, no singularities. Do you know whether there are other algorithms that I could use which work faster and still give me reliable results? So the sphere sits at $(0,0,0)$ and has a radius of $10^{-6}$. Don't be confused that the $y$-component is not shown, I took this one as being fixed in order to plot this. Okay, maybe I define my problem a little bit better: I want to perform this integration as fast as possible with an accuracy that should be about $10^{-1}$ or $10^{-2}$, this is sufficient. Of course, I thought about adding this lower precision to the integral2 function, but still, this function is so slowly that it is useless for what I am currently doing here.",,"['real-analysis', 'analysis']"
40,Show if $\|\cdot\|$ is a norm then $\|f(\cdot)\|$ is a norm where $f$ is linear and invertible,Show if  is a norm then  is a norm where  is linear and invertible,\|\cdot\| \|f(\cdot)\| f,"I want to show that if $\|\cdot\|$ is a norm then $\|f(\cdot)\|$ is a norm where $f$ is a linear and invertible function. First I need to show if $x\neq0$ then $\|f(x)\|>0$. Since $f$ is invertible $f(x)=0 \iff x=0$. Then since $\|\cdot\|$ is a norm it is true. Next I need to show $\|f(\lambda x)\| = |\lambda|\|f(x)\|$: By the linearity of $f$, $\|f(\lambda x)\| = \|\lambda f(x)\|$. Since $\|\cdot\|$ is a norm it is true. Finally we need to show the triangle inequality. Starting with the linearity of $f$, $\|f(x+y)\|=\|f(x)+f(y)\|$. Since $\|\cdot\|$ is a norm we can write: \begin{equation}                                                                \|f(x)+f(y)\| \leq \|f(x)\| + \|f(y)\|                                                                        \end{equation} and thus we have shown the triangle inequality. 1.Is it sufficient to conclude in every case ""since $\|\cdot\|$ is a norm it is true""? 1.What would make this proof clearer?","I want to show that if $\|\cdot\|$ is a norm then $\|f(\cdot)\|$ is a norm where $f$ is a linear and invertible function. First I need to show if $x\neq0$ then $\|f(x)\|>0$. Since $f$ is invertible $f(x)=0 \iff x=0$. Then since $\|\cdot\|$ is a norm it is true. Next I need to show $\|f(\lambda x)\| = |\lambda|\|f(x)\|$: By the linearity of $f$, $\|f(\lambda x)\| = \|\lambda f(x)\|$. Since $\|\cdot\|$ is a norm it is true. Finally we need to show the triangle inequality. Starting with the linearity of $f$, $\|f(x+y)\|=\|f(x)+f(y)\|$. Since $\|\cdot\|$ is a norm we can write: \begin{equation}                                                                \|f(x)+f(y)\| \leq \|f(x)\| + \|f(y)\|                                                                        \end{equation} and thus we have shown the triangle inequality. 1.Is it sufficient to conclude in every case ""since $\|\cdot\|$ is a norm it is true""? 1.What would make this proof clearer?",,"['linear-algebra', 'analysis', 'normed-spaces']"
41,What is this ambiguous computational existence in the positive integers?,What is this ambiguous computational existence in the positive integers?,,"I'm reading From Sets and Types to Topology and Analysis: Towards Practicable Foundations for Constructive Mathematics. The authors mention Bishop’s Foundations of Constructive Analysis : The successful formalization of mathematics helped keep mathematics on a wrong course. The   fact that space has been arithmetized loses much of its signiﬁcance if space, number, and everything else are ﬁtted into a matrix of idealism where even the positive integers have an ambiguous computational existence. What is this ambiguous computational existence in the positive integers?","I'm reading From Sets and Types to Topology and Analysis: Towards Practicable Foundations for Constructive Mathematics. The authors mention Bishop’s Foundations of Constructive Analysis : The successful formalization of mathematics helped keep mathematics on a wrong course. The   fact that space has been arithmetized loses much of its signiﬁcance if space, number, and everything else are ﬁtted into a matrix of idealism where even the positive integers have an ambiguous computational existence. What is this ambiguous computational existence in the positive integers?",,"['analysis', 'logic', 'foundations']"
42,IFT application.,IFT application.,,"Suppose that $f:=(u,v):\Bbb R\to \Bbb R^2$ is $C^2$ and $(x_0,y_0)=f(t_0)$ A) prove that if $f'(t_0)\not=0$ then $u'(t_0)$ and $v'(t_0)$cannot both be zero. B) if $f'(t_0)\not=0$ show that either there is $C^1$ function $g$ such that $g(x_0)=t_0$ and $u(g(x))=x$ for $x$ near $x_0$ or there is $C^1$ function $h$ such that $h(y_0)=t_0$ and $u(g(y))=y$ for $y$ near $y_0$ I guess, I need to use Implicit Function thm. But I dont know how to use this therem to solve these two parts. I just have been starting to learn this theorem by myself. Please explicitly show me. Thank you :)","Suppose that $f:=(u,v):\Bbb R\to \Bbb R^2$ is $C^2$ and $(x_0,y_0)=f(t_0)$ A) prove that if $f'(t_0)\not=0$ then $u'(t_0)$ and $v'(t_0)$cannot both be zero. B) if $f'(t_0)\not=0$ show that either there is $C^1$ function $g$ such that $g(x_0)=t_0$ and $u(g(x))=x$ for $x$ near $x_0$ or there is $C^1$ function $h$ such that $h(y_0)=t_0$ and $u(g(y))=y$ for $y$ near $y_0$ I guess, I need to use Implicit Function thm. But I dont know how to use this therem to solve these two parts. I just have been starting to learn this theorem by myself. Please explicitly show me. Thank you :)",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'self-learning']"
43,"$(a,b)$ is saddle point if $f_{xy}(a,b)\not= 0$",is saddle point if,"(a,b) f_{xy}(a,b)\not= 0","Suppose that $V$ is open in $\Bbb R^2$ that $(a,b)\in V$$\ \ \ \ \ f:V\to\Bbb R$ has second order partial total differential on $V$ with $f_x(a,b)=f_y(a,b)=0$ If the second order partial derivatives of f are continuous and exactly two of three numbers $f_{xx}(a,b)$ $f_{xy}(a,b)$ and $f_{yy}(a,b)$ are zero, Then prove that (a,b) is saddle point if $f_{xy}(a,b)\not= 0$ I think that Since $f_{xy}(a,b)\not=0$ $$f_{xx}(a,b)=f_{yy}(a,b)=0$$ I want to solve this question by using the theorem. Thm: let $A,B,C\in \Bbb R$ and $D=AC-B^2$ and $\varphi $(h,k)=$Ah^{2}+2Bhk+Ck^2$ if (i) $D>0$ and A, $\varphi(h,k)$ have same sign $\forall (h,k)\not= 0$ (ii) ıf $D<0$ then $\varphi (h,k)$ takes on both positive and negative valuesas (h,k) varies over $\Bbb R^2$ But, after here, I dont have any idea. Show me the solution? Thank you.","Suppose that $V$ is open in $\Bbb R^2$ that $(a,b)\in V$$\ \ \ \ \ f:V\to\Bbb R$ has second order partial total differential on $V$ with $f_x(a,b)=f_y(a,b)=0$ If the second order partial derivatives of f are continuous and exactly two of three numbers $f_{xx}(a,b)$ $f_{xy}(a,b)$ and $f_{yy}(a,b)$ are zero, Then prove that (a,b) is saddle point if $f_{xy}(a,b)\not= 0$ I think that Since $f_{xy}(a,b)\not=0$ $$f_{xx}(a,b)=f_{yy}(a,b)=0$$ I want to solve this question by using the theorem. Thm: let $A,B,C\in \Bbb R$ and $D=AC-B^2$ and $\varphi $(h,k)=$Ah^{2}+2Bhk+Ck^2$ if (i) $D>0$ and A, $\varphi(h,k)$ have same sign $\forall (h,k)\not= 0$ (ii) ıf $D<0$ then $\varphi (h,k)$ takes on both positive and negative valuesas (h,k) varies over $\Bbb R^2$ But, after here, I dont have any idea. Show me the solution? Thank you.",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'self-learning']"
44,Properties of the Fourier transform of a certain function,Properties of the Fourier transform of a certain function,,"In my research I met the Fourier transform of the function $f(x)=(1+x^2)^{-1/2}$. I was not able to find its explicit formula. Is this a function known as a 'special function'? I would like to know if it is nonnegative, summable, etc.","In my research I met the Fourier transform of the function $f(x)=(1+x^2)^{-1/2}$. I was not able to find its explicit formula. Is this a function known as a 'special function'? I would like to know if it is nonnegative, summable, etc.",,"['analysis', 'special-functions']"
45,closure of a set is closed,closure of a set is closed,,"I know that the closure of a set is closed and the proof can be found in Rudin. However this is done by using the fact that the complement of a closed set is open. I am using a different book now and it is asked in the exercises to show that the closure of a set is closed. The only thing mentioned about closed sets so far in this book is that it contains all of its accumulation points. The accumulation point $x$ of a set $E$ is such that $\forall c>0, (x-c,x+c)\cap {E}$ contains infinitely many points. Now I am trying to establish the above fact using only this definition of a closed set presented so far. However, I am stuck... If $\bar{E}$ denotes closure, $\bar{E} = E \cup E'$ where $E'$ is the set of all accumulation points of $E$. Then let $x$ be an accumulation point of $\bar{E}$. I want to show that $x \in \bar{E}$. Since $x$ is an accumulation point, $\forall c>0, (x-c,x+c)\cap {\bar{E}}$ contains infinitely many points. But $(x-c,x+c)\cap {\bar{E}} = \{(x-c,x+c)\cap E\} \cup \{(x-c,x+c)\cap E'\}$. So either  $\{(x-c,x+c)\cap E\}$, or $\{(x-c,x+c)\cap E'\}$, or both contains infinitely many points. If  $\{(x-c,x+c)\cap E\}$ contains infinitely many points, $x\in E' \subset \bar{E}$. But I am now stuck with the other case left. Any suggestions/ideas? Help appreciated!","I know that the closure of a set is closed and the proof can be found in Rudin. However this is done by using the fact that the complement of a closed set is open. I am using a different book now and it is asked in the exercises to show that the closure of a set is closed. The only thing mentioned about closed sets so far in this book is that it contains all of its accumulation points. The accumulation point $x$ of a set $E$ is such that $\forall c>0, (x-c,x+c)\cap {E}$ contains infinitely many points. Now I am trying to establish the above fact using only this definition of a closed set presented so far. However, I am stuck... If $\bar{E}$ denotes closure, $\bar{E} = E \cup E'$ where $E'$ is the set of all accumulation points of $E$. Then let $x$ be an accumulation point of $\bar{E}$. I want to show that $x \in \bar{E}$. Since $x$ is an accumulation point, $\forall c>0, (x-c,x+c)\cap {\bar{E}}$ contains infinitely many points. But $(x-c,x+c)\cap {\bar{E}} = \{(x-c,x+c)\cap E\} \cup \{(x-c,x+c)\cap E'\}$. So either  $\{(x-c,x+c)\cap E\}$, or $\{(x-c,x+c)\cap E'\}$, or both contains infinitely many points. If  $\{(x-c,x+c)\cap E\}$ contains infinitely many points, $x\in E' \subset \bar{E}$. But I am now stuck with the other case left. Any suggestions/ideas? Help appreciated!",,"['real-analysis', 'analysis']"
46,The value of $ A \exp\left(\frac{-1}{2\pi} \int_{-\pi}^{\pi} \ln(1+A+2BC \cos x) dx \right)$,The value of, A \exp\left(\frac{-1}{2\pi} \int_{-\pi}^{\pi} \ln(1+A+2BC \cos x) dx \right),I'm looking for the value of: $$ A \exp\left(\frac{-1}{2\pi} \int_{-\pi}^{\pi} \ln(1+A+2BC \cos x) dx \right)$$  I know we could take $y=1+A+2BC \cos x$ but changing variable in this way makes the integral from $1+ A - 2BC$ to $1+ A - 2BC$ which makes the integral zero! How to solve it ? thanks,I'm looking for the value of: $$ A \exp\left(\frac{-1}{2\pi} \int_{-\pi}^{\pi} \ln(1+A+2BC \cos x) dx \right)$$  I know we could take $y=1+A+2BC \cos x$ but changing variable in this way makes the integral from $1+ A - 2BC$ to $1+ A - 2BC$ which makes the integral zero! How to solve it ? thanks,,"['calculus', 'analysis', 'integration']"
47,Problem with simple laplacian equation,Problem with simple laplacian equation,,"I would like to solve the following PDE: $$  \partial_x^2 u + \partial_y^2 u = -\frac{2 x^2 (x^2-y^2)}{\left(x^2+y^2\right)^2} $$ The right side comes from $ x^2 \partial_x^2 \log(x^2 +y^2) $. Switching the polar coordinates, the right side is deceptively simple: $$ -2 \cos(\theta)^2 \cos(2 \theta)  $$ In polar coordinates, the laplacian is: $$ \partial_r^2 + \frac{1}{r}\partial_r + \frac{1}{r^2} \partial_\theta^2 $$ so it seems as though it should be fairly simple to find a solution. If I assume $ u(r, \theta) $ is like $ r^2 F(\theta) $, and try to solve for theta, I get something that is not periodic in $ \theta $. I am not sure if there is nonetheless a way to extract a meaningful solution. I'm at a bit of a loss for any other approaches. Any suggestions? Edit: For $ F(\theta) $, I used mathematica to get: $$ F(\theta) = \frac{1}{24} (-3 - 3 \cos(2 \theta) + \cos(4 \theta) - 6 \theta \sin(2 \theta)) $$ Plus of course any homogenous solution. It is the $ \theta \sin (2 \theta) $ term that makes me so sad.","I would like to solve the following PDE: $$  \partial_x^2 u + \partial_y^2 u = -\frac{2 x^2 (x^2-y^2)}{\left(x^2+y^2\right)^2} $$ The right side comes from $ x^2 \partial_x^2 \log(x^2 +y^2) $. Switching the polar coordinates, the right side is deceptively simple: $$ -2 \cos(\theta)^2 \cos(2 \theta)  $$ In polar coordinates, the laplacian is: $$ \partial_r^2 + \frac{1}{r}\partial_r + \frac{1}{r^2} \partial_\theta^2 $$ so it seems as though it should be fairly simple to find a solution. If I assume $ u(r, \theta) $ is like $ r^2 F(\theta) $, and try to solve for theta, I get something that is not periodic in $ \theta $. I am not sure if there is nonetheless a way to extract a meaningful solution. I'm at a bit of a loss for any other approaches. Any suggestions? Edit: For $ F(\theta) $, I used mathematica to get: $$ F(\theta) = \frac{1}{24} (-3 - 3 \cos(2 \theta) + \cos(4 \theta) - 6 \theta \sin(2 \theta)) $$ Plus of course any homogenous solution. It is the $ \theta \sin (2 \theta) $ term that makes me so sad.",,"['analysis', 'differential-geometry', 'partial-differential-equations']"
48,Prove that a compact cone is not diffeomorphic to the 2-sphere,Prove that a compact cone is not diffeomorphic to the 2-sphere,,"In Tapp's ""Matrix Groups for Undergraduates"" he briefly states (p.103) that a compact cone (he just shows a picture of a manifold with a ''cone point'') is not diffeomorphic to a 2-sphere.  I would love for someone to give me a simple proof, using only elementary analysis/topology methods, why this is true.  To be on the same page: Let $C = \left\{x \in \mathbb{R}^3 \mid 0 \leq z = \sqrt{x^2 + y^2} \leq 1\right\}$ be the compact cone.  Let $f : C \to S^2 \subset \mathbb{R}^3$ be a homeomorphism.  Prove that $f$ is not a diffeomorphism by proving that $f$ is not smooth at the origin; that is, there does not exist a smooth local extension of $f$ about the origin.","In Tapp's ""Matrix Groups for Undergraduates"" he briefly states (p.103) that a compact cone (he just shows a picture of a manifold with a ''cone point'') is not diffeomorphic to a 2-sphere.  I would love for someone to give me a simple proof, using only elementary analysis/topology methods, why this is true.  To be on the same page: Let $C = \left\{x \in \mathbb{R}^3 \mid 0 \leq z = \sqrt{x^2 + y^2} \leq 1\right\}$ be the compact cone.  Let $f : C \to S^2 \subset \mathbb{R}^3$ be a homeomorphism.  Prove that $f$ is not a diffeomorphism by proving that $f$ is not smooth at the origin; that is, there does not exist a smooth local extension of $f$ about the origin.",,"['analysis', 'differential-geometry', 'differential-topology']"
49,"Is [0,1] closed?","Is [0,1] closed?",,"I thought it was closed, under the usual topology $\mathbb{R}$, since its compliment $(-\infty, 0) \cup (1,\infty)$ is open. However, then then intersection number would not agree mod 2, since it can arbitrarily intersect a compact manifold even or odd times. P.S. The corollary. $X$ and $Z$ are closed submanifolds inside $Y$ with complementary dimension, and at least one of them is compact. If $g_0, g_1: X \to Y$ are arbitrary homotopic maps, then we have $I_2(g_0, Z) = I_2(g_1, Z).$ The contradiction (my question): Let [0,1] be the closed manifold $Z$, and then it can intersect an arbitrary compact manifold any times, contradicting with the corollary. Aneesh Karthik C's comment answered my question, so just to clarify: I was thinking $g_0$ is one wiggle of [0,1] such that it intersects a compact manifold once, and $g_1$ is some other sort that [0,1] intersect twice. Then it contradicts with the corollary. But apparently it doesn't, because [0,1] does not satisfy the corollary as a closed manifold. By definition, a closed manifold is a type of topological space, namely a compact manifold without boundary. Since [0,1] is not a closed manifold, it can intersect a compact manifold as much as it want, without contradicting with the theorem. I didn't realize that [0,1] is not a closed manifold. So I thought it contradicts and that's why I ask the question.","I thought it was closed, under the usual topology $\mathbb{R}$, since its compliment $(-\infty, 0) \cup (1,\infty)$ is open. However, then then intersection number would not agree mod 2, since it can arbitrarily intersect a compact manifold even or odd times. P.S. The corollary. $X$ and $Z$ are closed submanifolds inside $Y$ with complementary dimension, and at least one of them is compact. If $g_0, g_1: X \to Y$ are arbitrary homotopic maps, then we have $I_2(g_0, Z) = I_2(g_1, Z).$ The contradiction (my question): Let [0,1] be the closed manifold $Z$, and then it can intersect an arbitrary compact manifold any times, contradicting with the corollary. Aneesh Karthik C's comment answered my question, so just to clarify: I was thinking $g_0$ is one wiggle of [0,1] such that it intersects a compact manifold once, and $g_1$ is some other sort that [0,1] intersect twice. Then it contradicts with the corollary. But apparently it doesn't, because [0,1] does not satisfy the corollary as a closed manifold. By definition, a closed manifold is a type of topological space, namely a compact manifold without boundary. Since [0,1] is not a closed manifold, it can intersect a compact manifold as much as it want, without contradicting with the theorem. I didn't realize that [0,1] is not a closed manifold. So I thought it contradicts and that's why I ask the question.",,"['general-topology', 'analysis', 'differential-topology']"
50,Deformation of integration paths,Deformation of integration paths,,"First of all: I'm more of an algebraic person. So happily differentials/integrals are not what I deal with a lot. However, I got this exercise to solve: Let $\Omega \subset \mathbb{R}^n$ open, $K: \Omega \rightarrow \mathbb{R}^n$ a $C^1$ vector field with $\partial_iK_j  = \partial_j K_i$ for $i, j= 1, ..., n$. Let $\gamma, \gamma ' \in C^1([0,1]; \Omega)$ be curves with $\gamma(0) = \gamma'(0)$ and $\gamma(1) = \gamma' (1)$. Let $\Phi \in C^2 ( [0,1] \times [0,1]; \mathbb{R}^n)$ be a homotopy between $\gamma$ and $\gamma'$. Then it is to show that $\int_{\gamma} K d x= \int_{\gamma '} K d x$. I know this holds for continous homotopies already, but I have to present the solution to a group of first years and the proof for general case seems a bit lengthy. But, here I have a $C^2$ homotopy, and I'd like to use it by showing $\frac{\partial}{\partial s} \int_{\Phi(s, -)} K dx = 0$ How can I do this?","First of all: I'm more of an algebraic person. So happily differentials/integrals are not what I deal with a lot. However, I got this exercise to solve: Let $\Omega \subset \mathbb{R}^n$ open, $K: \Omega \rightarrow \mathbb{R}^n$ a $C^1$ vector field with $\partial_iK_j  = \partial_j K_i$ for $i, j= 1, ..., n$. Let $\gamma, \gamma ' \in C^1([0,1]; \Omega)$ be curves with $\gamma(0) = \gamma'(0)$ and $\gamma(1) = \gamma' (1)$. Let $\Phi \in C^2 ( [0,1] \times [0,1]; \mathbb{R}^n)$ be a homotopy between $\gamma$ and $\gamma'$. Then it is to show that $\int_{\gamma} K d x= \int_{\gamma '} K d x$. I know this holds for continous homotopies already, but I have to present the solution to a group of first years and the proof for general case seems a bit lengthy. But, here I have a $C^2$ homotopy, and I'd like to use it by showing $\frac{\partial}{\partial s} \int_{\Phi(s, -)} K dx = 0$ How can I do this?",,['analysis']
51,What is the meaning of $f(x) \rightarrow a$ as $g(x) \rightarrow b$?,What is the meaning of  as ?,f(x) \rightarrow a g(x) \rightarrow b,"The motivating example was the case: $$f(x, y)\rightarrow0\mathrm{\ \ as\ \ }\sqrt{x^2+y^2}\rightarrow\infty$$ What exactly does this mean? I might define it as: Any sequence $x_n$ with $g(x_n)\rightarrow b$ verifies $f(x_n)\rightarrow a$. Is this right? Are there other definitions?","The motivating example was the case: $$f(x, y)\rightarrow0\mathrm{\ \ as\ \ }\sqrt{x^2+y^2}\rightarrow\infty$$ What exactly does this mean? I might define it as: Any sequence $x_n$ with $g(x_n)\rightarrow b$ verifies $f(x_n)\rightarrow a$. Is this right? Are there other definitions?",,"['analysis', 'convergence-divergence', 'definition']"
52,convergence of $\prod_{m=2}^\infty \frac {1}{1-m^{-s}}$,convergence of,\prod_{m=2}^\infty \frac {1}{1-m^{-s}},Is  the product $$\prod_{m=2}^\infty \frac {1}{1-m^{-s}}$$  convergent for all real $s>1$$\space$ ?,Is  the product $$\prod_{m=2}^\infty \frac {1}{1-m^{-s}}$$  convergent for all real $s>1$$\space$ ?,,"['analysis', 'convergence-divergence', 'infinite-product']"
53,"total differential of $f+g$, $fg$ and $\frac fg$","total differential of ,  and",f+g fg \frac fg,"Let $f,g:\mathbb R^n\rightarrow\mathbb R$ be differentiable. We had in lectures that $f+g,fg,\frac fg$ are differentiable too. As an exercise I want to prove this. $f$ is differentiable in $x\in\mathbb R^n$ $\Leftrightarrow$ there is  a lineare function $A$ such that $\lim_{h\rightarrow0}\frac{f(x+h)-f(x)-A(h)}{\|h\|}=0$ So for $fg$ I get\begin{align*} &\lim_{h\rightarrow0}\frac{(fg)(x+h)-(fg)(x)-A(h)}{\|h\|}\\& =\lim_{h\rightarrow0}\frac{f(x+h)g(x+h)-f(x)g(x)-A(h)}{\|h\|} \end{align*}Now I am really stuck. How can you show with above that $fg$ is totally differentiable in $x$? Solution: \begin{align} &\lim_{h\rightarrow0}\frac{f(x+h)g(x+h)-f(x)g(x)-(gDf+fDg)(x)h}{\|h\|}\\ &=\lim_{h\rightarrow0}\frac{f(x+h)g(x+h)-f(x+h)g(x)-f(x+h)Dg(x)h}{\|h\|}\\&+\frac{f(x+h)g(x)-f(x)g(x)-g(x)Df(x)h+f(x+h)Dg(x)h}{\|h\|}\\&-\frac{f(x)Dg(x)h}{\|h\|}\\ &=\lim_{h\rightarrow0}\frac{f(x+h)(g(x+h)-g(x)-Dg(x)h)}{\|h\|}+g(x)\frac{f(x+h)-f(x)-Df(x)h}{\|h\|}+\frac{(f(x+h)-f(x))Dg(x)h}{\|h\|} \end{align} So the first term is $fDg$ since $f$ is continuous and differentiable. The second term is $gDf$ and the last one is 0 since $f$ is continuous and $Dg$ linear and so bounded. So the limit equals 0 and $fg$ is differentiable with derivative $gDf+fDg$.","Let $f,g:\mathbb R^n\rightarrow\mathbb R$ be differentiable. We had in lectures that $f+g,fg,\frac fg$ are differentiable too. As an exercise I want to prove this. $f$ is differentiable in $x\in\mathbb R^n$ $\Leftrightarrow$ there is  a lineare function $A$ such that $\lim_{h\rightarrow0}\frac{f(x+h)-f(x)-A(h)}{\|h\|}=0$ So for $fg$ I get\begin{align*} &\lim_{h\rightarrow0}\frac{(fg)(x+h)-(fg)(x)-A(h)}{\|h\|}\\& =\lim_{h\rightarrow0}\frac{f(x+h)g(x+h)-f(x)g(x)-A(h)}{\|h\|} \end{align*}Now I am really stuck. How can you show with above that $fg$ is totally differentiable in $x$? Solution: \begin{align} &\lim_{h\rightarrow0}\frac{f(x+h)g(x+h)-f(x)g(x)-(gDf+fDg)(x)h}{\|h\|}\\ &=\lim_{h\rightarrow0}\frac{f(x+h)g(x+h)-f(x+h)g(x)-f(x+h)Dg(x)h}{\|h\|}\\&+\frac{f(x+h)g(x)-f(x)g(x)-g(x)Df(x)h+f(x+h)Dg(x)h}{\|h\|}\\&-\frac{f(x)Dg(x)h}{\|h\|}\\ &=\lim_{h\rightarrow0}\frac{f(x+h)(g(x+h)-g(x)-Dg(x)h)}{\|h\|}+g(x)\frac{f(x+h)-f(x)-Df(x)h}{\|h\|}+\frac{(f(x+h)-f(x))Dg(x)h}{\|h\|} \end{align} So the first term is $fDg$ since $f$ is continuous and differentiable. The second term is $gDf$ and the last one is 0 since $f$ is continuous and $Dg$ linear and so bounded. So the limit equals 0 and $fg$ is differentiable with derivative $gDf+fDg$.",,['calculus']
54,Limit calculation using Riemann integral,Limit calculation using Riemann integral,,My task is to calculate limit: $$\lim_{n \rightarrow \infty} \sqrt[n^2]{ \frac{(n+1)^{n+1}(n+2)^{n+2}\cdots(n+n)^{n+n}}{n^{n+1}n^{n+2}\cdots n^{n+n}} }$$I denoted that limit as $a_n$. So: $$\log a_n=\frac{1}{n^2} \left (  (n+1)\log \left (1+\frac{1}{n} \right )+\cdots+(n+n)\log \left (1+\frac{n}{n} \right )\right )=$$$$=\cdots=\frac{1}{n} \left ( \sum_{k=1}^n\log(1+\frac{k}{n})+\sum_{k=1}^n\frac{k}{n}\log(1+\frac{k}{n}) \right )$$ The only (quite crucial however) problem I've got is the term $\frac{1}{n}$ before the above. I know how to calculate parenthesis:$$ \lim_{n \rightarrow \infty}\left ( \sum_{k=1}^n\log(1+\frac{k}{n})+\sum_{k=1}^n\frac{k}{n}\log(1+\frac{k}{n}) \right )=$$$$\int_0^1\log(1+x)dx+\int_0^1x\log(1+x)dx=\cdots$$But I have no idea what $\frac{1}{n} $does. Any hints? Thanks in advance.,My task is to calculate limit: $$\lim_{n \rightarrow \infty} \sqrt[n^2]{ \frac{(n+1)^{n+1}(n+2)^{n+2}\cdots(n+n)^{n+n}}{n^{n+1}n^{n+2}\cdots n^{n+n}} }$$I denoted that limit as $a_n$. So: $$\log a_n=\frac{1}{n^2} \left (  (n+1)\log \left (1+\frac{1}{n} \right )+\cdots+(n+n)\log \left (1+\frac{n}{n} \right )\right )=$$$$=\cdots=\frac{1}{n} \left ( \sum_{k=1}^n\log(1+\frac{k}{n})+\sum_{k=1}^n\frac{k}{n}\log(1+\frac{k}{n}) \right )$$ The only (quite crucial however) problem I've got is the term $\frac{1}{n}$ before the above. I know how to calculate parenthesis:$$ \lim_{n \rightarrow \infty}\left ( \sum_{k=1}^n\log(1+\frac{k}{n})+\sum_{k=1}^n\frac{k}{n}\log(1+\frac{k}{n}) \right )=$$$$\int_0^1\log(1+x)dx+\int_0^1x\log(1+x)dx=\cdots$$But I have no idea what $\frac{1}{n} $does. Any hints? Thanks in advance.,,"['analysis', 'integration']"
55,Probability Calculation using combinations,Probability Calculation using combinations,,"In a population of $250$ items, $20$ are defective. Suppose $4$ items are sampled at random, without replacement.  a. What is the probability that the sample will consist of $4$ defective items? Solution (don't know if it is correct or not): $P$($4$ defective) = $\dfrac{1}{\binom{20}{4}} = \dfrac{1}{4845}$ b. What is the probability that the sample will consist of $3$ or fewer defective items? Solution (again, don't know if correct or not): $\dfrac{1}{\binom{20}{3}} + \dfrac{1}{\binom{20}{2}} + \dfrac{1}{\binom{20}{1}}$ c. What is the probability that the sample will consist of neither zero nor four defectives? (NO clue how to do this one) Help would be greatly appreciated. Thank you.","In a population of $250$ items, $20$ are defective. Suppose $4$ items are sampled at random, without replacement.  a. What is the probability that the sample will consist of $4$ defective items? Solution (don't know if it is correct or not): $P$($4$ defective) = $\dfrac{1}{\binom{20}{4}} = \dfrac{1}{4845}$ b. What is the probability that the sample will consist of $3$ or fewer defective items? Solution (again, don't know if correct or not): $\dfrac{1}{\binom{20}{3}} + \dfrac{1}{\binom{20}{2}} + \dfrac{1}{\binom{20}{1}}$ c. What is the probability that the sample will consist of neither zero nor four defectives? (NO clue how to do this one) Help would be greatly appreciated. Thank you.",,"['probability', 'analysis']"
56,A tricky analysis problem,A tricky analysis problem,,"Let $n,m \in \mathbb{N}, n>4,m\geq1$ We define \begin{equation}   f_i(x)=\begin{cases}     m_ix, & \text{if $x \in\left[\frac{1}{m_i}\left(j+\frac{1}{n}\right),\frac{1}{m_i}\left(j+1-\frac{1}{n}\right)\right]$}   \end{cases} \end{equation} where $x\in \mathbb{R}$, $i$ runs from $1$ to $n-1$ and $j$ runs from $0$ to $m_i-1$. Find $a$ where $f_i(a)$ exists for all $i$. For instance, let $n=5$. Then there are $n-1$ ($4$ in this case) number of functions $f_i(x)$ and $m_i$. Let $m_1=1$, $m_2=2$, $m_3=4$ and $m_4=5$. Then $f_i(x)$ all exist when $x$ is in the orange strips. The diagram is not drawn immaculately and the width of the first orange should be equal to the width of the second orange strip due to symmetry. This question asks for the existence of such a strip in general.","Let $n,m \in \mathbb{N}, n>4,m\geq1$ We define \begin{equation}   f_i(x)=\begin{cases}     m_ix, & \text{if $x \in\left[\frac{1}{m_i}\left(j+\frac{1}{n}\right),\frac{1}{m_i}\left(j+1-\frac{1}{n}\right)\right]$}   \end{cases} \end{equation} where $x\in \mathbb{R}$, $i$ runs from $1$ to $n-1$ and $j$ runs from $0$ to $m_i-1$. Find $a$ where $f_i(a)$ exists for all $i$. For instance, let $n=5$. Then there are $n-1$ ($4$ in this case) number of functions $f_i(x)$ and $m_i$. Let $m_1=1$, $m_2=2$, $m_3=4$ and $m_4=5$. Then $f_i(x)$ all exist when $x$ is in the orange strips. The diagram is not drawn immaculately and the width of the first orange should be equal to the width of the second orange strip due to symmetry. This question asks for the existence of such a strip in general.",,"['real-analysis', 'analysis']"
57,Does $f(x)=x^{2}\sin\left(\frac{1}{x^2}\right)$ satisfy the relation $f(x)+f(y)−2f\left(\frac{x+y}{2}\right)=O\left(\left|x−y\right|^2\right)$?,Does  satisfy the relation ?,f(x)=x^{2}\sin\left(\frac{1}{x^2}\right) f(x)+f(y)−2f\left(\frac{x+y}{2}\right)=O\left(\left|x−y\right|^2\right),"Does $f(x)=x^{2}\sin\left(\frac{1}{x^2}\right)$,  $x\in(0,1)$ satisfy the relation $f(x)+f(y)−2f\left(\frac{x+y}{2}\right)=O\left(\left|x−y\right|^2\right)$?","Does $f(x)=x^{2}\sin\left(\frac{1}{x^2}\right)$,  $x\in(0,1)$ satisfy the relation $f(x)+f(y)−2f\left(\frac{x+y}{2}\right)=O\left(\left|x−y\right|^2\right)$?",,"['real-analysis', 'analysis']"
58,"I'm having trouble with a definition of the upper and lower limits, and a theorem that follows it.","I'm having trouble with a definition of the upper and lower limits, and a theorem that follows it.",,"The following is the definition. Let $\{s_n\}$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ such that $s_{n_{k}}\rightarrow{x}$. This set $E$ contains all subsequential limits, plus possibly the numbers $+\infty$, $-\infty$. Now, putting $s^* = \sup (E)$ and $s_{*} = \inf (E)$. The numbers $s^{*}$ and $s_{*}$ are called upper and lower limits of ${s_n}$. $${\lim_{n \to \infty}} \sup s_n = s^{*}$$ $${\lim_{n \to \infty}}\inf s_n = s_{*}$$ I have no idea what this means. From my understanding, if a sequence has a limit the limit is unique. Why is this definition implying that a sequence has multiple limits? Or is it implying that a subsequence of a sequence can have a different limit ? My book lacks in examples and I cannot figure out what's going on at all ... Also, I learned that $\infty$ is not a number. Why is this definition treating it as if it is one ? Theorem . Let ${s_n}$ be a sequence of real numbers. Then $s^*$ has the following properties. a) $s^* \in E$ b) If $x>s^*$, $\exists N \in \Bbb Z$ such that $\forall n \ge N$, $s_n < x$. Moreover, $s^*$ is unique. I was able to understand the proof of a), and b), but I couldn't really understand the proof of the uniqueness. The book says, Suppose $p<q$ where both $p$ and $q$ are upper limits. Choose $x$ such that $p < x < q$. Since $p$ satisfies b), we have $s_n<x$ for $n \ge N$. But then $q$ cannot satisfy a). 1) Why is it guaranteed that such $x$ between $p$ and $q$ exist? 2) Why will $q$ not satisfy a) ?","The following is the definition. Let $\{s_n\}$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ such that $s_{n_{k}}\rightarrow{x}$. This set $E$ contains all subsequential limits, plus possibly the numbers $+\infty$, $-\infty$. Now, putting $s^* = \sup (E)$ and $s_{*} = \inf (E)$. The numbers $s^{*}$ and $s_{*}$ are called upper and lower limits of ${s_n}$. $${\lim_{n \to \infty}} \sup s_n = s^{*}$$ $${\lim_{n \to \infty}}\inf s_n = s_{*}$$ I have no idea what this means. From my understanding, if a sequence has a limit the limit is unique. Why is this definition implying that a sequence has multiple limits? Or is it implying that a subsequence of a sequence can have a different limit ? My book lacks in examples and I cannot figure out what's going on at all ... Also, I learned that $\infty$ is not a number. Why is this definition treating it as if it is one ? Theorem . Let ${s_n}$ be a sequence of real numbers. Then $s^*$ has the following properties. a) $s^* \in E$ b) If $x>s^*$, $\exists N \in \Bbb Z$ such that $\forall n \ge N$, $s_n < x$. Moreover, $s^*$ is unique. I was able to understand the proof of a), and b), but I couldn't really understand the proof of the uniqueness. The book says, Suppose $p<q$ where both $p$ and $q$ are upper limits. Choose $x$ such that $p < x < q$. Since $p$ satisfies b), we have $s_n<x$ for $n \ge N$. But then $q$ cannot satisfy a). 1) Why is it guaranteed that such $x$ between $p$ and $q$ exist? 2) Why will $q$ not satisfy a) ?",,['analysis']
59,A combinatorial identity with Pochhammer's symbol,A combinatorial identity with Pochhammer's symbol,,"Let $m,k$ be an positive integers with $k\le m$. I am trying to prove $$\sum_{j=0}^k{\frac{1}{2}\choose k-j}\frac{2^{2j}(m+j)!}{(m-j)!(2j)!}=\frac{P(n,k)}{(2k)!}$$ where $n=2m+1$ and $P(n,k)=\prod_{i=0}^{k-1} (n^2-(2i)^2)$. We can use Pochhammer's symbol and write the left side as $$(-1)^k\sum_{j=0}^k\frac{(-1/2)_{k-j}(m+1)_j(-m)_j}{(1/2)_j(k-j)!j!}$$ We can also write the right hand side as $$(-1)^k\frac{(-m-\frac{1}{2})_k(m+\frac{1}{2})_k}{(\frac{1}{2})_kk!}.$$ So I think the identity must follow from some identities in hypergeometric functions.  Does anyone see a way to do it?","Let $m,k$ be an positive integers with $k\le m$. I am trying to prove $$\sum_{j=0}^k{\frac{1}{2}\choose k-j}\frac{2^{2j}(m+j)!}{(m-j)!(2j)!}=\frac{P(n,k)}{(2k)!}$$ where $n=2m+1$ and $P(n,k)=\prod_{i=0}^{k-1} (n^2-(2i)^2)$. We can use Pochhammer's symbol and write the left side as $$(-1)^k\sum_{j=0}^k\frac{(-1/2)_{k-j}(m+1)_j(-m)_j}{(1/2)_j(k-j)!j!}$$ We can also write the right hand side as $$(-1)^k\frac{(-m-\frac{1}{2})_k(m+\frac{1}{2})_k}{(\frac{1}{2})_kk!}.$$ So I think the identity must follow from some identities in hypergeometric functions.  Does anyone see a way to do it?",,"['combinatorics', 'analysis', 'discrete-mathematics', 'hypergeometric-function']"
60,The number $ \frac{(m)^{(k)}(m)_k}{(1/2)^{(k)} k!}$,The number, \frac{(m)^{(k)}(m)_k}{(1/2)^{(k)} k!},"For a real number $a$ and a positive integer $k$, denote by $(a)^{(k)}$ the number $a(a+1)\cdots (a+k-1)$ and $(a)_k$ the number $a(a-1)\cdots (a-k+1)$. Let $m$ be a positive integer $\ge k$. Can anyone show me, or point me to a reference,  why the number $$ \frac{(m)^{(k)}(m)_k}{(1/2)^{(k)} k!}= \frac{2^{2k}(m)^{(k)}(m)_k}{(2k)!}$$ is always an integer?","For a real number $a$ and a positive integer $k$, denote by $(a)^{(k)}$ the number $a(a+1)\cdots (a+k-1)$ and $(a)_k$ the number $a(a-1)\cdots (a-k+1)$. Let $m$ be a positive integer $\ge k$. Can anyone show me, or point me to a reference,  why the number $$ \frac{(m)^{(k)}(m)_k}{(1/2)^{(k)} k!}= \frac{2^{2k}(m)^{(k)}(m)_k}{(2k)!}$$ is always an integer?",,"['combinatorics', 'analysis', 'number-theory', 'special-functions']"
61,Calculate volume of inequality,Calculate volume of inequality,,"$\{(x,y,z) \in \mathbb{R}^3 \mid 2\cdot \max(\lvert x\rvert,\lvert y\rvert)^2+z^2\leq 4\}$ Any tips for me anyone? I made a sketch but what now?","$\{(x,y,z) \in \mathbb{R}^3 \mid 2\cdot \max(\lvert x\rvert,\lvert y\rvert)^2+z^2\leq 4\}$ Any tips for me anyone? I made a sketch but what now?",,"['analysis', 'integration', 'volume']"
62,Concept of integration to differential form,Concept of integration to differential form,,"How to integrate differential form actually. As far as I know, a differential form is a multilinear function mapping from a vector space to a real number. Let's take $\int_c fdx+gdy$ as an example. It is integrating a differential $1$-form while I don't quite get the meaning of the integral process. Shouldn't the function act on something, like the tangent vector in the tangent space? For instance $dx^i(e_p^i)=1$ however when we are doing integration, we won't add a tangent vector next to $dx$, why?","How to integrate differential form actually. As far as I know, a differential form is a multilinear function mapping from a vector space to a real number. Let's take $\int_c fdx+gdy$ as an example. It is integrating a differential $1$-form while I don't quite get the meaning of the integral process. Shouldn't the function act on something, like the tangent vector in the tangent space? For instance $dx^i(e_p^i)=1$ however when we are doing integration, we won't add a tangent vector next to $dx$, why?",,"['real-analysis', 'analysis', 'integration', 'differential-forms']"
63,An identity related to Legendre polynomials,An identity related to Legendre polynomials,,"Let $m$ be a positive integer. I believe the the following identity  $$1+\sum_{k=1}^m (-1)^k\frac{P(k,m)}{(2k)!}=(-1)^m\frac{2^{2m}(m!)^2}{(2m)!}$$  where $P(k,m)=\prod_{i=0}^{k-1} (2m-2i)(2m+2i+1)$, is true, but I don't see a quick proof. Anyone?","Let $m$ be a positive integer. I believe the the following identity  $$1+\sum_{k=1}^m (-1)^k\frac{P(k,m)}{(2k)!}=(-1)^m\frac{2^{2m}(m!)^2}{(2m)!}$$  where $P(k,m)=\prod_{i=0}^{k-1} (2m-2i)(2m+2i+1)$, is true, but I don't see a quick proof. Anyone?",,"['combinatorics', 'sequences-and-series', 'analysis', 'hypergeometric-function']"
64,Calculation of a derivative,Calculation of a derivative,,I have to calculate the following derivative $$\frac{\partial}{\partial{\Vert x\Vert}}e^{ix\cdot y}$$ Then I write $$e^{ix\cdot y}=e^{i\Vert x\Vert\Vert y\Vert\cos\alpha}$$ andI derive; is this reasoning correct?,I have to calculate the following derivative $$\frac{\partial}{\partial{\Vert x\Vert}}e^{ix\cdot y}$$ Then I write $$e^{ix\cdot y}=e^{i\Vert x\Vert\Vert y\Vert\cos\alpha}$$ andI derive; is this reasoning correct?,,"['calculus', 'analysis', 'derivatives']"
65,Bound for $|\log(1 - x) + x|$.,Bound for .,|\log(1 - x) + x|,I am having trouble proving the following: Deduce that for $0 < x < 1$ we have $$|\log(1 - x) + x| < cx^2$$ for some constant $c$. Any help will be appreciated.,I am having trouble proving the following: Deduce that for $0 < x < 1$ we have $$|\log(1 - x) + x| < cx^2$$ for some constant $c$. Any help will be appreciated.,,"['calculus', 'analysis']"
66,Evaluating difficult spectrum,Evaluating difficult spectrum,,"Can anyone see how to show the spectrum of the bounded linear operator $T$ on $l^1$ defined by $$T((\alpha_j)) = (\alpha_j - 2\alpha_{j+1} + \alpha_{j+2})$$ is the cardioid $$\{(r, θ) : 0 ≤ θ < 2π, \;0 ≤ r ≤ 2 + 2 \cosθ\}?$$","Can anyone see how to show the spectrum of the bounded linear operator $T$ on $l^1$ defined by $$T((\alpha_j)) = (\alpha_j - 2\alpha_{j+1} + \alpha_{j+2})$$ is the cardioid $$\{(r, θ) : 0 ≤ θ < 2π, \;0 ≤ r ≤ 2 + 2 \cosθ\}?$$",,"['analysis', 'functional-analysis', 'banach-spaces', 'spectral-theory']"
67,Convergent sequence on a step function,Convergent sequence on a step function,,"I'm not quite sure how to go about proving this: Let $\phi:[a,b] \to \mathbb{R}$ be a step function and $s \in (a,b)$. Let $(x_n)$ be a sequence in $[a,b]$ with each $x_n > s$ and $\lim_{n\to\infty}x_n=s$. Prove that the sequence $(\phi(x_n))$ converges. I was thinking that eventually $\phi(x_n)$ must be constant as it would settle on some value that step function takes, but what about if there were a discontinuity at $s$ and the step function jumps? (This is me supposing $(\phi(x_n))$ converges to $\phi(s))$ which I don't know!) Any insight would be appreciated!","I'm not quite sure how to go about proving this: Let $\phi:[a,b] \to \mathbb{R}$ be a step function and $s \in (a,b)$. Let $(x_n)$ be a sequence in $[a,b]$ with each $x_n > s$ and $\lim_{n\to\infty}x_n=s$. Prove that the sequence $(\phi(x_n))$ converges. I was thinking that eventually $\phi(x_n)$ must be constant as it would settle on some value that step function takes, but what about if there were a discontinuity at $s$ and the step function jumps? (This is me supposing $(\phi(x_n))$ converges to $\phi(s))$ which I don't know!) Any insight would be appreciated!",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
68,Prove that $1+rx = (1+x)^r$ holds only if $x=0$.,Prove that  holds only if .,1+rx = (1+x)^r x=0,"I believe my work is correct here, just looking for closure. Is it true   that since r>1, the last line could be (1+x)^1? Suppose $1+rx\le(1+x)^r$  for any real number $r>1$ and $x>-1$. Prove that the equality holds only if $x=0$. My work : For $x=0$, letting $f(x) = (1+x)^r$: [mean value theorem] $f'(c) = \frac{f(x)-f(0)}{x-0}$ $r(1+c)^{r-1} = \frac{(1+x)^r-1^r}{x-0}$ $\frac{(1+x)^r-1^r}{(1+c)^{r-1}}=xr$ $(1+x)-1=xr$ $1+x = 1+xr$","I believe my work is correct here, just looking for closure. Is it true   that since r>1, the last line could be (1+x)^1? Suppose $1+rx\le(1+x)^r$  for any real number $r>1$ and $x>-1$. Prove that the equality holds only if $x=0$. My work : For $x=0$, letting $f(x) = (1+x)^r$: [mean value theorem] $f'(c) = \frac{f(x)-f(0)}{x-0}$ $r(1+c)^{r-1} = \frac{(1+x)^r-1^r}{x-0}$ $\frac{(1+x)^r-1^r}{(1+c)^{r-1}}=xr$ $(1+x)-1=xr$ $1+x = 1+xr$",,['analysis']
69,Strictly monotone increasing,Strictly monotone increasing,,"Let $f$ be a real-valued function defined on an interval $[a, \infty)$. One says that $f(x)$ tends to infinity with $x$ if for every $B \gt 0$ there is an $A \gt 0$ such that for every $x \gt A$ we have $f(x) \gt B$. Let $f$ be a strictly monotone increasing, continuous function on an interval $[a, \infty)$. Set $c = f(a)$, and assume that $f(x)$ tends to infinity with $x$. Using the intermediate value theorem, prove that for every $y$ in $[c, \infty)$ there exists one and only one $x$ in $[a, b)$ such that $f(x) = y$. Can some help me with this, explain for the capital $A$ and $B$ Thanks","Let $f$ be a real-valued function defined on an interval $[a, \infty)$. One says that $f(x)$ tends to infinity with $x$ if for every $B \gt 0$ there is an $A \gt 0$ such that for every $x \gt A$ we have $f(x) \gt B$. Let $f$ be a strictly monotone increasing, continuous function on an interval $[a, \infty)$. Set $c = f(a)$, and assume that $f(x)$ tends to infinity with $x$. Using the intermediate value theorem, prove that for every $y$ in $[c, \infty)$ there exists one and only one $x$ in $[a, b)$ such that $f(x) = y$. Can some help me with this, explain for the capital $A$ and $B$ Thanks",,"['calculus', 'real-analysis', 'analysis']"
70,Non Uniformly Elliptic Equations page 117 [G-T],Non Uniformly Elliptic Equations page 117 [G-T],,"Let $\Omega\subset\mathbb{R}^n$ be open and bounded. Suppose also that $\Omega$ satisfies the exterior sphere condition at $x_0$ and let $B=B_R(y)$ be a ball such that $B\cap\overline{\Omega}=x_0$. Let $L$ be the following linear differential operator: \begin{equation} Lu(x)=a_{ij}(x)D_{ij}u(x)+b_{i}(x)D_{i}u(x)+c(x)u(x)=f(x)\quad a_{ij}(x)=a_{ji}(x). \end{equation} So $A(x)$ is a real $n\times n$ symmetric matrix at each $x\in\Omega$. I would like to show that if $\vert A(x)\cdot(x-y)\vert\geq\delta>0$ for all $x\in N\cap\Omega$ of some neighbourhood, $N$ of $x_0$, then:  \begin{equation} (x-y)^T A\cdot(x-y)\geq \lambda \vert (x-y)\vert^2\quad\forall\ x\in N\cap\Omega \end{equation} where $\lambda>0$. This is the claim in Gillbarg-Trudinger's Second Order Elliptic PDE book on page 117 (3rd edition). I thought I'd try to argue by contradiction. So if there is some $x_{\ast}\in N\cap\Omega$ such that: \begin{equation} \vert A(x_{\ast})\cdot(x_{\ast}-y)\vert\geq\delta>0\quad\text{but}\quad (x_{\ast}-y)\cdot A(x_{\ast})\cdot(x-y)=0, \end{equation} then it must be because the $(x_{\ast}-y)$ is perpendicular to $A(x_{\ast})\cdot(x_{\ast}-y)=0$. I'm not sure where to go from here.","Let $\Omega\subset\mathbb{R}^n$ be open and bounded. Suppose also that $\Omega$ satisfies the exterior sphere condition at $x_0$ and let $B=B_R(y)$ be a ball such that $B\cap\overline{\Omega}=x_0$. Let $L$ be the following linear differential operator: \begin{equation} Lu(x)=a_{ij}(x)D_{ij}u(x)+b_{i}(x)D_{i}u(x)+c(x)u(x)=f(x)\quad a_{ij}(x)=a_{ji}(x). \end{equation} So $A(x)$ is a real $n\times n$ symmetric matrix at each $x\in\Omega$. I would like to show that if $\vert A(x)\cdot(x-y)\vert\geq\delta>0$ for all $x\in N\cap\Omega$ of some neighbourhood, $N$ of $x_0$, then:  \begin{equation} (x-y)^T A\cdot(x-y)\geq \lambda \vert (x-y)\vert^2\quad\forall\ x\in N\cap\Omega \end{equation} where $\lambda>0$. This is the claim in Gillbarg-Trudinger's Second Order Elliptic PDE book on page 117 (3rd edition). I thought I'd try to argue by contradiction. So if there is some $x_{\ast}\in N\cap\Omega$ such that: \begin{equation} \vert A(x_{\ast})\cdot(x_{\ast}-y)\vert\geq\delta>0\quad\text{but}\quad (x_{\ast}-y)\cdot A(x_{\ast})\cdot(x-y)=0, \end{equation} then it must be because the $(x_{\ast}-y)$ is perpendicular to $A(x_{\ast})\cdot(x_{\ast}-y)=0$. I'm not sure where to go from here.",,"['real-analysis', 'linear-algebra', 'analysis', 'partial-differential-equations']"
71,Integration over inequality,Integration over inequality,,What are the conditions for taking integral over an inequality? For example given the following inequality: $f(x)<g(x)$ when can we say $$\int_{a}^{b}f(x)dx<\int_{a}^{b}g(x)dx $$ holds? Please give me rigorous explanation.,What are the conditions for taking integral over an inequality? For example given the following inequality: $f(x)<g(x)$ when can we say $$\int_{a}^{b}f(x)dx<\int_{a}^{b}g(x)dx $$ holds? Please give me rigorous explanation.,,"['calculus', 'analysis']"
72,Application of FTC and change of variable,Application of FTC and change of variable,,"Let $f:[0,1]\to \mathbb{R}$ be continuous such that $$\int_{0}^{1} f(xt)dt=0$$ for all $x \in [0,1]$. Show that $f(x)=0$ for all $x \in [0,1]$. Using the FTC and substitution: $$F(t)=\frac{1}{x}\int_{0}^{t} f(u)du$$ $$F'(t)=\frac{1}{x}[f(t)]$$ I'm not sure if I am going in the right direction.  As of right now, I can't see how to go from my last step to showing that $f(x)=0$ for all $x$ in $[0,1]$.","Let $f:[0,1]\to \mathbb{R}$ be continuous such that $$\int_{0}^{1} f(xt)dt=0$$ for all $x \in [0,1]$. Show that $f(x)=0$ for all $x \in [0,1]$. Using the FTC and substitution: $$F(t)=\frac{1}{x}\int_{0}^{t} f(u)du$$ $$F'(t)=\frac{1}{x}[f(t)]$$ I'm not sure if I am going in the right direction.  As of right now, I can't see how to go from my last step to showing that $f(x)=0$ for all $x$ in $[0,1]$.",,['analysis']
73,Show this sequence is equicontinuous,Show this sequence is equicontinuous,,"I'm stuck on an analysis problem to which I've reduced to the following, so some assumptions may be superfluous. Let $\{ f_n(x) \} \subset C(X,\mathbb{R}^{\geq0})$ (i.e. $f_n$ is continuous and nonnegative)  where $X$ is a compact subset of $\mathbb{R}^n$. Suppose further that for all $n,x$, $f_{n+1}(x) \leq f_n(x)$ and that $f_n$ converges pointwise to the continuous function $f$. Show that the sequence $f_n$ is equicontinuous. Any help on this problem is appreciated. Thanks.","I'm stuck on an analysis problem to which I've reduced to the following, so some assumptions may be superfluous. Let $\{ f_n(x) \} \subset C(X,\mathbb{R}^{\geq0})$ (i.e. $f_n$ is continuous and nonnegative)  where $X$ is a compact subset of $\mathbb{R}^n$. Suppose further that for all $n,x$, $f_{n+1}(x) \leq f_n(x)$ and that $f_n$ converges pointwise to the continuous function $f$. Show that the sequence $f_n$ is equicontinuous. Any help on this problem is appreciated. Thanks.",,['analysis']
74,Composition of Riemann integrable functions,Composition of Riemann integrable functions,,"I know that if $f:[a,b]\to[m,M]$ is Riemann integrable and $g:[m,M]\to\mathbb{R}$ is continuous, then $g\circ f$ is also integrable on $[a,b]$. I'm trying to think about the following 3 cases: 1) $f$ the same, $g$ is Riemann integrable but not continuous on $[m,M]$, such that the statement is false. 2) $f:[a,b]\to\mathbb{R}$ is continuous and $g:[m,M]\to\mathbb{R}$ is bounded and Riemann integrable, then what about $g\circ f$? 3) $f$ is Riemann integrable on $[a,b]$ but not continuous, $g$ same as in 2), what about $g\circ f$? Thank you. [EDIT]: Let $f$ be continuous on $[0,1]$, vanish on cantor set C and positive otherwise. What kind of riemann integrable bounded real $g$ will make $g\circ f$ not riemann integrable?","I know that if $f:[a,b]\to[m,M]$ is Riemann integrable and $g:[m,M]\to\mathbb{R}$ is continuous, then $g\circ f$ is also integrable on $[a,b]$. I'm trying to think about the following 3 cases: 1) $f$ the same, $g$ is Riemann integrable but not continuous on $[m,M]$, such that the statement is false. 2) $f:[a,b]\to\mathbb{R}$ is continuous and $g:[m,M]\to\mathbb{R}$ is bounded and Riemann integrable, then what about $g\circ f$? 3) $f$ is Riemann integrable on $[a,b]$ but not continuous, $g$ same as in 2), what about $g\circ f$? Thank you. [EDIT]: Let $f$ be continuous on $[0,1]$, vanish on cantor set C and positive otherwise. What kind of riemann integrable bounded real $g$ will make $g\circ f$ not riemann integrable?",,['analysis']
75,A question on immersions,A question on immersions,,"I am facing the following problem: Let $\alpha:\mathbb{R}\rightarrow\mathbb{R}^2$ and $\beta:\mathbb{R}\rightarrow\mathbb{R}^2$ be $C^1$ curves with $\alpha(0)=(0,0)=\beta (0)$, such that $\alpha '(0)$ and $\beta '(0)$ are linearly independent. Show that there are open sets $U$ and $V$ in $\mathbb{R}^2$ and a $C^1$ diffeomorphism $\phi :U\rightarrow V$ such that $\phi(0,0)=(0,0)$, $\phi (\alpha(x))=(x,0)$ and $\phi (\beta (y))=(0,y)$ whenever $\alpha (x)$ and $\beta (y)$ are in $U$. Using the Theorem for Local Form of Immersions (I don't know the english name for this theorem, I'm using a portuguese Analysis book and haven't found it elsewhere in the internet) I can find $C^1$ homeomorphisms $h_\alpha:\mathbb{R}^2\rightarrow\mathbb{R}^2$ and $h_\beta:\mathbb{R}^2\rightarrow\mathbb{R}^2$ such that $h_\alpha\circ\alpha (t) = (t,0)$ and $h_\beta \circ \beta (t)=(0,t)$. Now, I'm trying to define a function $h:\mathbb{R}^2\rightarrow\mathbb{R}^2$ such that it solves the problem; it should involve $h_\alpha$ and $h_\beta$ though I don't know how to assemble these parts together. Any help would be appreciated.","I am facing the following problem: Let $\alpha:\mathbb{R}\rightarrow\mathbb{R}^2$ and $\beta:\mathbb{R}\rightarrow\mathbb{R}^2$ be $C^1$ curves with $\alpha(0)=(0,0)=\beta (0)$, such that $\alpha '(0)$ and $\beta '(0)$ are linearly independent. Show that there are open sets $U$ and $V$ in $\mathbb{R}^2$ and a $C^1$ diffeomorphism $\phi :U\rightarrow V$ such that $\phi(0,0)=(0,0)$, $\phi (\alpha(x))=(x,0)$ and $\phi (\beta (y))=(0,y)$ whenever $\alpha (x)$ and $\beta (y)$ are in $U$. Using the Theorem for Local Form of Immersions (I don't know the english name for this theorem, I'm using a portuguese Analysis book and haven't found it elsewhere in the internet) I can find $C^1$ homeomorphisms $h_\alpha:\mathbb{R}^2\rightarrow\mathbb{R}^2$ and $h_\beta:\mathbb{R}^2\rightarrow\mathbb{R}^2$ such that $h_\alpha\circ\alpha (t) = (t,0)$ and $h_\beta \circ \beta (t)=(0,t)$. Now, I'm trying to define a function $h:\mathbb{R}^2\rightarrow\mathbb{R}^2$ such that it solves the problem; it should involve $h_\alpha$ and $h_\beta$ though I don't know how to assemble these parts together. Any help would be appreciated.",,"['real-analysis', 'analysis']"
76,A problem about $C^1$-convergence! (Elliptic theory),A problem about -convergence! (Elliptic theory),C^1,"Let a function $u:\overline\Omega\subset\mathbb{R}^n\rightarrow\mathbb{R}$ that satisfies $$\Delta u+f(u)=0 \ \ \ \mbox{in} \ \ \Omega,$$ and consider $$w(x_1,...,x_{n-1})=\lim_{x_n\rightarrow+\infty}u(x_1,...,x_n).$$ Why the function $u(x_1,...,x_n)$ converges uniformly in the $C^1$ sense in compact sets of $\mathbb{R}^{n-1}$ to the function $w$, and $w$ satisfies $$\Delta w+f(w)=0.$$ This argument is used in the paper: Further qualitative properties for elliptic equations in unbounded domains, by Berestycki, Caffarelli and Nirenberg. I didn't undesrtand this point. Someone can help me? Thanks.","Let a function $u:\overline\Omega\subset\mathbb{R}^n\rightarrow\mathbb{R}$ that satisfies $$\Delta u+f(u)=0 \ \ \ \mbox{in} \ \ \Omega,$$ and consider $$w(x_1,...,x_{n-1})=\lim_{x_n\rightarrow+\infty}u(x_1,...,x_n).$$ Why the function $u(x_1,...,x_n)$ converges uniformly in the $C^1$ sense in compact sets of $\mathbb{R}^{n-1}$ to the function $w$, and $w$ satisfies $$\Delta w+f(w)=0.$$ This argument is used in the paper: Further qualitative properties for elliptic equations in unbounded domains, by Berestycki, Caffarelli and Nirenberg. I didn't undesrtand this point. Someone can help me? Thanks.",,"['analysis', 'partial-differential-equations']"
77,A Banach space induced by a maximum sum norm? $||u||_J = \sum \max |u_i(x)|$,A Banach space induced by a maximum sum norm?,||u||_J = \sum \max |u_i(x)|,"Let $J=[a,b] \subset \mathbb{R}$, how does one then see that if  $C(J)=C(J,\mathbb{C}^n)$ with the norm $$||u||_J = \sum_{i=1}^{n} ||u_i||_\infty = \sum _{i=1} ^{n} \max _{x\in J} |u_i (x) |$$ is a Banach space? What have I tried: One has to show completeness by showing every cauchy sequence $(u_n)_\mathbb{N}$ converges in $C(J,\mathbb{C}^n)$. Let $(u_n)_n$ be a Cauchy sequence, with n,k>N : $$|u_n - u_k|< \epsilon $$ Can one use that J is compact so there is at least one Cauchy subsequence which converges, (and then use that to make an epsilon/2 proof)?","Let $J=[a,b] \subset \mathbb{R}$, how does one then see that if  $C(J)=C(J,\mathbb{C}^n)$ with the norm $$||u||_J = \sum_{i=1}^{n} ||u_i||_\infty = \sum _{i=1} ^{n} \max _{x\in J} |u_i (x) |$$ is a Banach space? What have I tried: One has to show completeness by showing every cauchy sequence $(u_n)_\mathbb{N}$ converges in $C(J,\mathbb{C}^n)$. Let $(u_n)_n$ be a Cauchy sequence, with n,k>N : $$|u_n - u_k|< \epsilon $$ Can one use that J is compact so there is at least one Cauchy subsequence which converges, (and then use that to make an epsilon/2 proof)?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
78,How does one show that the definite Riemann integral of a function is $ > 0 $?,How does one show that the definite Riemann integral of a function is ?, > 0 ,"Let $ f \in R[a,b] $, where $ f \ge 0 $, and suppose that $ f(x) > 0 $ for a point of continuity $ x $ of $ f $. Then one can show that $$ \int_{a}^{b} f(x) ~d{x} > 0. $$ One can construct an area function $ I(a,b;f) $ with the property $$ \inf_{x \in [a,b]} f(x) \cdot (b - a) \le I(a,b;f) \le \sup_{x \in [a,b]} f(x) \cdot (b - a). $$ Since both the infimum and the supremum are greater than $ 0 $, so must be $ I(a,b;f) $. One can also show that $$ \int_{a}^{b} |f(x) - g(x)| ~d{x} = 0 $$ if $ f =_{\text{a.e.}} g $. How does one show this?","Let $ f \in R[a,b] $, where $ f \ge 0 $, and suppose that $ f(x) > 0 $ for a point of continuity $ x $ of $ f $. Then one can show that $$ \int_{a}^{b} f(x) ~d{x} > 0. $$ One can construct an area function $ I(a,b;f) $ with the property $$ \inf_{x \in [a,b]} f(x) \cdot (b - a) \le I(a,b;f) \le \sup_{x \in [a,b]} f(x) \cdot (b - a). $$ Since both the infimum and the supremum are greater than $ 0 $, so must be $ I(a,b;f) $. One can also show that $$ \int_{a}^{b} |f(x) - g(x)| ~d{x} = 0 $$ if $ f =_{\text{a.e.}} g $. How does one show this?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
79,Inverse Function Theorem/ Polynomial,Inverse Function Theorem/ Polynomial,,"I was thinking about this after I read about Jacobian conjecture. But I can't see what I did wrong? Maybe you can help me. Let $F: \mathbb{C}^n \to \mathbb{C}^n$ be of the form  $F(x_1, \dots, x_n)= (F_1(x_1, \dots, x_n), \dots,   F_n( \dots , x_1, \dots, x_n) )$ for some $F_1,\dots , F_n \in \mathbb{C}[X_1, \dots, X_n ]$, which I will call polynomial, with constant Jacobian determinant $\det DF$. Then by Implicit Function Theorem $F$ has a local inverse $F'$ around $0$. Since $F$ is analytical $F'$ is also analytical. We also know that $ DF' = (DF)^{-1}= \frac{1}{\det DF} \text{adj}(DF) $ but the adjugate is computed in terms of the minors of $DF$, $\det DF$ is constant and thus $DF'$ is given by polynomial functions in its components, in particular $D^k F =0$ for some $k\in \mathbb{N}$. Thus $F'$ is polynomial  because of its Taylor expansion. Since $F'$ is polynomial it is defined on the whole space $\mathbb{C}^n$. Now we have $F' \circ F\mid_U= \text{id}$ for some open neighborhood $U$ of $0$, but this implies $F' \circ F= \text{id}$ since $F' \circ F$ is a polynomial.","I was thinking about this after I read about Jacobian conjecture. But I can't see what I did wrong? Maybe you can help me. Let $F: \mathbb{C}^n \to \mathbb{C}^n$ be of the form  $F(x_1, \dots, x_n)= (F_1(x_1, \dots, x_n), \dots,   F_n( \dots , x_1, \dots, x_n) )$ for some $F_1,\dots , F_n \in \mathbb{C}[X_1, \dots, X_n ]$, which I will call polynomial, with constant Jacobian determinant $\det DF$. Then by Implicit Function Theorem $F$ has a local inverse $F'$ around $0$. Since $F$ is analytical $F'$ is also analytical. We also know that $ DF' = (DF)^{-1}= \frac{1}{\det DF} \text{adj}(DF) $ but the adjugate is computed in terms of the minors of $DF$, $\det DF$ is constant and thus $DF'$ is given by polynomial functions in its components, in particular $D^k F =0$ for some $k\in \mathbb{N}$. Thus $F'$ is polynomial  because of its Taylor expansion. Since $F'$ is polynomial it is defined on the whole space $\mathbb{C}^n$. Now we have $F' \circ F\mid_U= \text{id}$ for some open neighborhood $U$ of $0$, but this implies $F' \circ F= \text{id}$ since $F' \circ F$ is a polynomial.",,"['linear-algebra', 'analysis', 'multivariable-calculus']"
80,Inequality of real numbers,Inequality of real numbers,,"Is it true that for $a,b \in \mathbb{R}$ and $p\geq 1$ (or $p\geq 2)$ there exists a constant $C>0$ independent of $a,b$ of course, such that: $(a-b)^{p-2} ab \leq C (a-b)^p$ Thanks a lot! :)","Is it true that for $a,b \in \mathbb{R}$ and $p\geq 1$ (or $p\geq 2)$ there exists a constant $C>0$ independent of $a,b$ of course, such that: $(a-b)^{p-2} ab \leq C (a-b)^p$ Thanks a lot! :)",,"['calculus', 'real-analysis', 'analysis']"
81,"Is the Sobolev Space $H^k(0,1)$ a banach algebra?",Is the Sobolev Space  a banach algebra?,"H^k(0,1)","In Adams'book:Sobolev Spaces, I know that if $kp>n,\Omega\subset R^n$ is boundary domain and has cone property, then $W^{k,p}(\Omega)$ could see as a banach algebra. My question is that does it hold for the one-dimensional and $p=2$ case, namely, for $H^k(0,1)$ ? If it is not true, provide a counterexample please.","In Adams'book:Sobolev Spaces, I know that if $kp>n,\Omega\subset R^n$ is boundary domain and has cone property, then $W^{k,p}(\Omega)$ could see as a banach algebra. My question is that does it hold for the one-dimensional and $p=2$ case, namely, for $H^k(0,1)$ ? If it is not true, provide a counterexample please.",,"['real-analysis', 'analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
82,Partial derivatives of an interesting function.,Partial derivatives of an interesting function.,,"Calculate the partial derivatives of the function $ \displaystyle F(x,y)=\int_{x}^{ \displaystyle\int_{0}^{y} g(s) ds} f(t)dt $ where $f,g$ are continuous from $\mathbb{R}$ to $\mathbb{R}$.","Calculate the partial derivatives of the function $ \displaystyle F(x,y)=\int_{x}^{ \displaystyle\int_{0}^{y} g(s) ds} f(t)dt $ where $f,g$ are continuous from $\mathbb{R}$ to $\mathbb{R}$.",,"['calculus', 'real-analysis', 'analysis']"
83,Index of a Fredholm Operator on Paths,Index of a Fredholm Operator on Paths,,"I'm a novice to analysis but I need to understand the following example. Any help would be greatly appreciated. This might be of interest to some because it gives a way of quantifying changes in eigenspaces as the index of a Fredholm operator. Let $\alpha: (-\infty,\infty) \to Sym^2(\Bbb{R}^n)^\vee$ be a path to the space of symmetric $n\times n$ real matrices such that $\alpha$ is constant near $-\infty$, and constant near $\infty$. I'll denote these two constant values by $\alpha(\pm \infty)$. Consider the differential operator $$ d_\alpha(X):= {\partial_s}X + \alpha(s)X$$ which takes a path $X: \Bbb R \to \Bbb R^n$ and outputs another path $d_\alpha X$. I declare the domain to be the space of $W^{1,2}$ maps $X: \Bbb R \to \Bbb R^n$, and naturally the target is the space of $L^2$ maps. (Here $W^{1,2}$ is the Sobolev space of $L^2$ maps whose first derivatives are $L^2$ too.) To calculate the index of $d_\alpha$, I further expand the domain by taking the direct sum with the finite-dimensional vector space spanned by $X$ of the form $$ X(s) = \exp(-s \lambda ) v $$ where $\lambda$ is a negative eigenvalue of $\alpha(\infty)$ and $v$ is an eigenvector for $\lambda$. One easily sees that $d_\alpha X$ indeed lands in $L^2$ so we have an operator $$ \tilde  d_\alpha:  W^{1,2}(\Bbb R , \Bbb R^n) \oplus \{(e^{-s \lambda} v)\} \to L^2(\Bbb R , \Bbb R^n). $$ Now, I want to prove three things: (1) that the kernel of this operator is finite-dimensional, and   identified with the negative eigenvalue eigenspace of   $\alpha(-\infty)$, (2) that this map is surjective, and (3) that as a result, $\text{index}(d_\alpha) = \dim \sigma_-(\alpha(-\infty)) -\dim \sigma_-(\alpha(\infty))$, where $\sigma_-$ is the space of vectors with negative eigenvalues. Here's how I've gone about proving these statements, but I'm stuck in all claims. (1) To see this, let $\{v_i\}$ be an eigenbasis for $\alpha(-\infty)$ and $\lambda_i$ the corresponding eigenvalues, writing $X(s) = \sum x_i(s) v_i$ near $-\infty$. Then the equation $\tilde d_\alpha = 0$ breaks into first-order ODEs in one variable: $$ \partial_s x_i + \lambda_i x_i = 0. $$ This has the obvious solutions $x_i(s) = \exp(-s \lambda_i)v_i$, but -- to be in $W^{1,2}$ -- $\lambda_i$ must be a negative eigenvalue. (Otherwise the integral $\int_{-\infty}^s |X(s)|^2$ will blow up.) I now want to say that, since we have identified what a solution $X$ must look like near $-\infty$, by the existence of first-order ODEs, every solution near $-\infty$ extends to a full solution, and by uniqueness of first-order ODEs, this determines all solutions as in one-to-one correspondence with the space of all eigenvectors of $\alpha(-\infty)$ with negative eigenvalue. Here is my question: Does the usual proof of uniqueness and existence of solutions guarantee that my solutions will indeed be in $W^{1,2}$? If you have a reference (or a quick proof) that the $L^2$ norms of both $X$ and its derivative are bounded, I'd be very grateful. Note that $\alpha$ is constant near $\pm \infty$, so its values and derivatives are bounded, but I'm not sure if this is enough. (2) Let $Y(s)$ be some $L^2$ path. We want to exhibit $X(s)$ such that $\tilde d_\alpha X = Y$. This time, take an eigenbasis for $\alpha(\infty)$, calling it $w_i$, with corresponding eigenvalues $\nu_i$. Again the equation splits into the components of $w_i$ and we are left to solve the first-order ODE $$ \partial_s x_i + \nu_i x_i = y_i. $$ Using the usual tricks from calculus, we see that a solution to this is given by $$ x_i(s) = {\int_{s_0}^s y_i(t) e^{\nu_i t} dt \over e^{\nu_i s} }. $$ One can extend this to a full solution . Again, my question is: How can one guarantee that such a solution is in $W^{1,2}$? And does it require a clever addition of a function of the form $\exp(-\nu_is)w_i$ to exhibit an $L^2$ solution? (3) To see this, I can just add the indices of each Fredholm operator in the composition $$ W^{1,2}(\Bbb R , \Bbb R^n)  \to W^{1,2}(\Bbb R , \Bbb R^n) \oplus \text{span}(e^{-s \lambda} v) \to L^2(\Bbb R , \Bbb R^n). $$ Is there a more clever way of seeing this, or a more natural way?","I'm a novice to analysis but I need to understand the following example. Any help would be greatly appreciated. This might be of interest to some because it gives a way of quantifying changes in eigenspaces as the index of a Fredholm operator. Let $\alpha: (-\infty,\infty) \to Sym^2(\Bbb{R}^n)^\vee$ be a path to the space of symmetric $n\times n$ real matrices such that $\alpha$ is constant near $-\infty$, and constant near $\infty$. I'll denote these two constant values by $\alpha(\pm \infty)$. Consider the differential operator $$ d_\alpha(X):= {\partial_s}X + \alpha(s)X$$ which takes a path $X: \Bbb R \to \Bbb R^n$ and outputs another path $d_\alpha X$. I declare the domain to be the space of $W^{1,2}$ maps $X: \Bbb R \to \Bbb R^n$, and naturally the target is the space of $L^2$ maps. (Here $W^{1,2}$ is the Sobolev space of $L^2$ maps whose first derivatives are $L^2$ too.) To calculate the index of $d_\alpha$, I further expand the domain by taking the direct sum with the finite-dimensional vector space spanned by $X$ of the form $$ X(s) = \exp(-s \lambda ) v $$ where $\lambda$ is a negative eigenvalue of $\alpha(\infty)$ and $v$ is an eigenvector for $\lambda$. One easily sees that $d_\alpha X$ indeed lands in $L^2$ so we have an operator $$ \tilde  d_\alpha:  W^{1,2}(\Bbb R , \Bbb R^n) \oplus \{(e^{-s \lambda} v)\} \to L^2(\Bbb R , \Bbb R^n). $$ Now, I want to prove three things: (1) that the kernel of this operator is finite-dimensional, and   identified with the negative eigenvalue eigenspace of   $\alpha(-\infty)$, (2) that this map is surjective, and (3) that as a result, $\text{index}(d_\alpha) = \dim \sigma_-(\alpha(-\infty)) -\dim \sigma_-(\alpha(\infty))$, where $\sigma_-$ is the space of vectors with negative eigenvalues. Here's how I've gone about proving these statements, but I'm stuck in all claims. (1) To see this, let $\{v_i\}$ be an eigenbasis for $\alpha(-\infty)$ and $\lambda_i$ the corresponding eigenvalues, writing $X(s) = \sum x_i(s) v_i$ near $-\infty$. Then the equation $\tilde d_\alpha = 0$ breaks into first-order ODEs in one variable: $$ \partial_s x_i + \lambda_i x_i = 0. $$ This has the obvious solutions $x_i(s) = \exp(-s \lambda_i)v_i$, but -- to be in $W^{1,2}$ -- $\lambda_i$ must be a negative eigenvalue. (Otherwise the integral $\int_{-\infty}^s |X(s)|^2$ will blow up.) I now want to say that, since we have identified what a solution $X$ must look like near $-\infty$, by the existence of first-order ODEs, every solution near $-\infty$ extends to a full solution, and by uniqueness of first-order ODEs, this determines all solutions as in one-to-one correspondence with the space of all eigenvectors of $\alpha(-\infty)$ with negative eigenvalue. Here is my question: Does the usual proof of uniqueness and existence of solutions guarantee that my solutions will indeed be in $W^{1,2}$? If you have a reference (or a quick proof) that the $L^2$ norms of both $X$ and its derivative are bounded, I'd be very grateful. Note that $\alpha$ is constant near $\pm \infty$, so its values and derivatives are bounded, but I'm not sure if this is enough. (2) Let $Y(s)$ be some $L^2$ path. We want to exhibit $X(s)$ such that $\tilde d_\alpha X = Y$. This time, take an eigenbasis for $\alpha(\infty)$, calling it $w_i$, with corresponding eigenvalues $\nu_i$. Again the equation splits into the components of $w_i$ and we are left to solve the first-order ODE $$ \partial_s x_i + \nu_i x_i = y_i. $$ Using the usual tricks from calculus, we see that a solution to this is given by $$ x_i(s) = {\int_{s_0}^s y_i(t) e^{\nu_i t} dt \over e^{\nu_i s} }. $$ One can extend this to a full solution . Again, my question is: How can one guarantee that such a solution is in $W^{1,2}$? And does it require a clever addition of a function of the form $\exp(-\nu_is)w_i$ to exhibit an $L^2$ solution? (3) To see this, I can just add the indices of each Fredholm operator in the composition $$ W^{1,2}(\Bbb R , \Bbb R^n)  \to W^{1,2}(\Bbb R , \Bbb R^n) \oplus \text{span}(e^{-s \lambda} v) \to L^2(\Bbb R , \Bbb R^n). $$ Is there a more clever way of seeing this, or a more natural way?",,"['analysis', 'partial-differential-equations']"
84,For any curve of unit length there exists a closed rectangle with area at most $A$ that covers the curve.,For any curve of unit length there exists a closed rectangle with area at most  that covers the curve.,A,Find the minimum possible value of $A$ such that for any curve of unit length there exists a closed rectangle with area at most $A$ that covers the curve.,Find the minimum possible value of $A$ such that for any curve of unit length there exists a closed rectangle with area at most $A$ that covers the curve.,,"['real-analysis', 'analysis', 'optimization', 'contest-math', 'plane-curves']"
85,Convergence to an exponential,Convergence to an exponential,,"Suppose that $(c_{j, n})$ is a real infinite dimensional triangular array where $1 \leq j \leq n$ with the properties that $\max\limits_{1 \leq j \leq n } |c_{j, n}| \rightarrow 0$ and $\sum\limits_{j=1}^n c_{j, n} \rightarrow \lambda$ when $n\rightarrow\infty$, and $\sup\limits_{n\in\mathbb{N}} \sum_{j=1}^n |c_{j, n}|<\infty.$ Please help me to prove that therefore $\prod\limits_{j=1}^n(1+c_{j, n})\rightarrow e^\lambda$.","Suppose that $(c_{j, n})$ is a real infinite dimensional triangular array where $1 \leq j \leq n$ with the properties that $\max\limits_{1 \leq j \leq n } |c_{j, n}| \rightarrow 0$ and $\sum\limits_{j=1}^n c_{j, n} \rightarrow \lambda$ when $n\rightarrow\infty$, and $\sup\limits_{n\in\mathbb{N}} \sum_{j=1}^n |c_{j, n}|<\infty.$ Please help me to prove that therefore $\prod\limits_{j=1}^n(1+c_{j, n})\rightarrow e^\lambda$.",,"['real-analysis', 'analysis', 'convergence-divergence']"
86,A particular product of distributions,A particular product of distributions,,"Suppose you have two continuous functions $f,g: \mathbb{R}\to\mathbb{R}$; is the product $f'g$ as a distribution, at least locally? I am interested in a local result, actually, so you can as well assume that $f$ and $g$ are compactly supported. Obivously, $f'$ is, as $f\in L^1_{\mathrm{loc}}$. Moreover, I am aware of the following particular cases: if $f\in W^{1,1}(I)$, then $f'g$ is well defined on $I$ if $f'\in L^p(I)$ and $g\in L^q(I)$ for $p^{-1}+q^{-1}=1$, then $f'g$ is defined on $I$; if $g\in W^{1,1}(I)$, then again $f'g$ is well defined on $I$ I am interested in a general result or in a counterexample. Thank you!","Suppose you have two continuous functions $f,g: \mathbb{R}\to\mathbb{R}$; is the product $f'g$ as a distribution, at least locally? I am interested in a local result, actually, so you can as well assume that $f$ and $g$ are compactly supported. Obivously, $f'$ is, as $f\in L^1_{\mathrm{loc}}$. Moreover, I am aware of the following particular cases: if $f\in W^{1,1}(I)$, then $f'g$ is well defined on $I$ if $f'\in L^p(I)$ and $g\in L^q(I)$ for $p^{-1}+q^{-1}=1$, then $f'g$ is defined on $I$; if $g\in W^{1,1}(I)$, then again $f'g$ is well defined on $I$ I am interested in a general result or in a counterexample. Thank you!",,"['analysis', 'functional-analysis', 'distribution-theory']"
87,Show this function is convex.,Show this function is convex.,,"Could someone point me in the right direction for proving the following? Given that $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$ is an affine map given by $f(x)=A\mathbf{x}+\mathbf{b}$, $g:\mathbb{R}^m\rightarrow \mathbb{R}$ is convex, and that $g(A\mathbf{x}+\mathbf{b})$ is also convex, prove that $h(x)=\max \{\mathbf{a}_1^T\mathbf{x}+b_1,\mathbf{a}_2^T\mathbf{x}+b_2,\dots,\mathbf{a}_m^T\mathbf{x}+b_m\}$ is a convex function. I know that $\max\{x_1,x_2,...,x_m\}$ is a convex function, but I am not sure how to use it to prove this. I previously proved that the composition $g\circ f$ is convex.","Could someone point me in the right direction for proving the following? Given that $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$ is an affine map given by $f(x)=A\mathbf{x}+\mathbf{b}$, $g:\mathbb{R}^m\rightarrow \mathbb{R}$ is convex, and that $g(A\mathbf{x}+\mathbf{b})$ is also convex, prove that $h(x)=\max \{\mathbf{a}_1^T\mathbf{x}+b_1,\mathbf{a}_2^T\mathbf{x}+b_2,\dots,\mathbf{a}_m^T\mathbf{x}+b_m\}$ is a convex function. I know that $\max\{x_1,x_2,...,x_m\}$ is a convex function, but I am not sure how to use it to prove this. I previously proved that the composition $g\circ f$ is convex.",,"['real-analysis', 'analysis', 'optimization', 'convex-analysis']"
88,Inequality in a Hilbert space,Inequality in a Hilbert space,,"This was a homework question a couple weeks ago that I couldn't solve.  I'd appreciate a solution. Let $\mathcal{H}$ be a Hilbert space and $\{\eta_n \}_{n \in \mathbb{Z}}$ a set of not necessarily orthogonal elements of $\mathcal{H}$.  Let $A$ be the matrix, defined on $\ell_2(\mathbb{Z})$, such that $a_{nm} = |(\eta_n,\eta_m)|$.  Show that for all $f \in \mathcal{H}$,   $$\sum_{n \in \mathbb{Z}} |(f,\eta_n)|^2 \le ||A|| \, ||f||^2$$","This was a homework question a couple weeks ago that I couldn't solve.  I'd appreciate a solution. Let $\mathcal{H}$ be a Hilbert space and $\{\eta_n \}_{n \in \mathbb{Z}}$ a set of not necessarily orthogonal elements of $\mathcal{H}$.  Let $A$ be the matrix, defined on $\ell_2(\mathbb{Z})$, such that $a_{nm} = |(\eta_n,\eta_m)|$.  Show that for all $f \in \mathcal{H}$,   $$\sum_{n \in \mathbb{Z}} |(f,\eta_n)|^2 \le ||A|| \, ||f||^2$$",,"['analysis', 'functional-analysis']"
89,Continuous Connection?,Continuous Connection?,,"Consider two compact convex sets $C_1, C_2 \subset \mathbb{R}^n$ such that $C_2 \subset C_1$. Let us denote by $\partial C_1$ and $\partial C_2$ their boundaries, that satisfy and $\partial C_1 \cap \partial C_2 = \varnothing$. Consider two continuous, bounded, functions $f_1: C_1 \rightarrow \mathbb{R}^n$ and $f_2: C_1 \rightarrow \mathbb{R}^n$. Consider a continuous, bounded, function $f: C_1 \rightarrow \mathbb{R}^n$ such that: $$f(y) = f_1(y) \ \ \forall y \in \partial C_1$$ $$f(y) = f_2(y) \ \ \forall y \in \partial C_2$$ 1) Prove that there exists a continuous function $g: C_1 \rightarrow \mathbb{R}_{\geq 0}$ such that: $$ g(y) = 0 \ \ \forall y \in \partial C_1 $$ $$ g(y) = 1 \ \ \forall y \in \partial C_2 $$ $$ f(x) = ( 1-g(x) ) f_1(x) + g(x) f_2(x) \ \ \forall x \in \text{closure}(C_1 \setminus C_2) $$","Consider two compact convex sets $C_1, C_2 \subset \mathbb{R}^n$ such that $C_2 \subset C_1$. Let us denote by $\partial C_1$ and $\partial C_2$ their boundaries, that satisfy and $\partial C_1 \cap \partial C_2 = \varnothing$. Consider two continuous, bounded, functions $f_1: C_1 \rightarrow \mathbb{R}^n$ and $f_2: C_1 \rightarrow \mathbb{R}^n$. Consider a continuous, bounded, function $f: C_1 \rightarrow \mathbb{R}^n$ such that: $$f(y) = f_1(y) \ \ \forall y \in \partial C_1$$ $$f(y) = f_2(y) \ \ \forall y \in \partial C_2$$ 1) Prove that there exists a continuous function $g: C_1 \rightarrow \mathbb{R}_{\geq 0}$ such that: $$ g(y) = 0 \ \ \forall y \in \partial C_1 $$ $$ g(y) = 1 \ \ \forall y \in \partial C_2 $$ $$ f(x) = ( 1-g(x) ) f_1(x) + g(x) f_2(x) \ \ \forall x \in \text{closure}(C_1 \setminus C_2) $$",,"['real-analysis', 'analysis', 'functional-analysis']"
90,Limit involving Khinchin's constant for continued fractions,Limit involving Khinchin's constant for continued fractions,,"Hoi, i wish to show a few things... Suppose we know $\lim_{n\to\infty}(a_1a_2\cdots a_{n})^{1/n} = \prod_{k=1}^{\infty}\left(1+\frac{1}{k(k+2)}\right)^{\frac{\log k}{\log 2}}$ I hope to show this implies $\frac{a_1+\cdots + a_n}{n}\to\infty $ This is like showing that $a_1+\cdots + a_n$ grows harder then $n^{1+\epsilon}$. Can we conclude for example for large n something like $a_n \approx (1+\frac{1}{n(n+2)})^{n\log(n)/\log(2)}$. Can someone maybe suggest some ideas? thank you","Hoi, i wish to show a few things... Suppose we know $\lim_{n\to\infty}(a_1a_2\cdots a_{n})^{1/n} = \prod_{k=1}^{\infty}\left(1+\frac{1}{k(k+2)}\right)^{\frac{\log k}{\log 2}}$ I hope to show this implies $\frac{a_1+\cdots + a_n}{n}\to\infty $ This is like showing that $a_1+\cdots + a_n$ grows harder then $n^{1+\epsilon}$. Can we conclude for example for large n something like $a_n \approx (1+\frac{1}{n(n+2)})^{n\log(n)/\log(2)}$. Can someone maybe suggest some ideas? thank you",,"['analysis', 'elementary-number-theory']"
91,Proof $a + \infty = \infty$,Proof,a + \infty = \infty,"Given a convergent sequence $(a_n)$ with limit $a \in \mathbb{R}$ and a divergent sequence $(b_n)$ tending to infinity. I want to prove now using the boundedness of $(a_n)$: $\exists C \in \mathbb{R} \forall n \in \mathbb{N}: |a_n| \leq C$ $\forall K > 0 ~\exists N ~\forall n \geq N : b_n > K$ that the sequence $(a_n + b_n)$ is also tending to infinity. This is obviously true, however I can't seem to come up with a consistent solution. If anyone liked to share a promising approach, I'd be grateful.","Given a convergent sequence $(a_n)$ with limit $a \in \mathbb{R}$ and a divergent sequence $(b_n)$ tending to infinity. I want to prove now using the boundedness of $(a_n)$: $\exists C \in \mathbb{R} \forall n \in \mathbb{N}: |a_n| \leq C$ $\forall K > 0 ~\exists N ~\forall n \geq N : b_n > K$ that the sequence $(a_n + b_n)$ is also tending to infinity. This is obviously true, however I can't seem to come up with a consistent solution. If anyone liked to share a promising approach, I'd be grateful.",,"['sequences-and-series', 'analysis']"
92,Extending of domain of smooth function of two variables,Extending of domain of smooth function of two variables,,"Let $f: [a,b]\times [c,d] \rightarrow \mathbb R$ be a smooth function of two variable (assuming that in boundary points $f$ has continuous one side partial derivatives). Is a simple way to extend $f$  to a smooth function $F: \mathbb R \times \mathbb R \rightarrow \mathbb R$?","Let $f: [a,b]\times [c,d] \rightarrow \mathbb R$ be a smooth function of two variable (assuming that in boundary points $f$ has continuous one side partial derivatives). Is a simple way to extend $f$  to a smooth function $F: \mathbb R \times \mathbb R \rightarrow \mathbb R$?",,['analysis']
93,Reflexivity of a Banach space,Reflexivity of a Banach space,,"I've run into a few problems in which reflexivity of a Banach space is given as a hypothesis.  These problems are sometimes of the type where the banach space is specific/concrete, and sometimes it is just any Banach space.  To me, reflexivity seems to be a hard condition to use.  Is there an easy list of tricks to use this hypothesis?  For example, does reflexivity imply any more tangible results via a standard theorem from functional analysis?  An example of the type of answer I'm looking for is ""Reflexivity often allows one to use the uniform boundedness principle"" or ""Reflexivity implies that the unit ball is weakly compact.""  But one of the reasons I'm asking these questions is because I feel I'm missing some other tricks that get used in functional analysis because I'm asked the question ""Show that every C* algebra that is reflexive as a Banach space is finite dimensional"" and I feel that I simply don't know enough tricks/theorems to do this. (I could also use hints on this specific question, which might go some ways in revealing to me more tricks for reflexivity in general.)","I've run into a few problems in which reflexivity of a Banach space is given as a hypothesis.  These problems are sometimes of the type where the banach space is specific/concrete, and sometimes it is just any Banach space.  To me, reflexivity seems to be a hard condition to use.  Is there an easy list of tricks to use this hypothesis?  For example, does reflexivity imply any more tangible results via a standard theorem from functional analysis?  An example of the type of answer I'm looking for is ""Reflexivity often allows one to use the uniform boundedness principle"" or ""Reflexivity implies that the unit ball is weakly compact.""  But one of the reasons I'm asking these questions is because I feel I'm missing some other tricks that get used in functional analysis because I'm asked the question ""Show that every C* algebra that is reflexive as a Banach space is finite dimensional"" and I feel that I simply don't know enough tricks/theorems to do this. (I could also use hints on this specific question, which might go some ways in revealing to me more tricks for reflexivity in general.)",,"['analysis', 'functional-analysis', 'operator-theory']"
94,A generalization of Lagrange identity,A generalization of Lagrange identity,,"Let $k,n$ be positive integers, $k\le n$. Let $v_1,\cdots,v_k$ be vectors in $\mathbb{R}^n$. Let $M$ be the $k\times n$ matrix with rows $v_1,\cdots,v_k$ in this order. The  Gram determinant of $M$ is defined as the determinant of the $k\times k$ matrix $MM^*$. For each subset $\sigma$ of $S=\{1,\cdots,n\}$ of $n-k$  elements, let $x_{\sigma}$ be the determinant of the submatrix of $M$ by deleting  the $i$-th columns for $i\in \sigma$. Can anyone show me where I can find the proof that the Gram determinant of $M$ is equal to the sum of $x_{\sigma}^2$ over all subsets $\sigma$ of $S$ of $n-k$ elements? Note that for $k=2$ the assertion is the Lagrange identity.","Let $k,n$ be positive integers, $k\le n$. Let $v_1,\cdots,v_k$ be vectors in $\mathbb{R}^n$. Let $M$ be the $k\times n$ matrix with rows $v_1,\cdots,v_k$ in this order. The  Gram determinant of $M$ is defined as the determinant of the $k\times k$ matrix $MM^*$. For each subset $\sigma$ of $S=\{1,\cdots,n\}$ of $n-k$  elements, let $x_{\sigma}$ be the determinant of the submatrix of $M$ by deleting  the $i$-th columns for $i\in \sigma$. Can anyone show me where I can find the proof that the Gram determinant of $M$ is equal to the sum of $x_{\sigma}^2$ over all subsets $\sigma$ of $S$ of $n-k$ elements? Note that for $k=2$ the assertion is the Lagrange identity.",,"['linear-algebra', 'analysis', 'multilinear-algebra']"
95,Uncountably many?,Uncountably many?,,Is there any way to show that the set of disjoint translations of the cantor ternary set is countable? That is show that there are countably many disjoint sets of the form $\{x+C: x\in \mathbb{R}\}$??? Thanks,Is there any way to show that the set of disjoint translations of the cantor ternary set is countable? That is show that there are countably many disjoint sets of the form $\{x+C: x\in \mathbb{R}\}$??? Thanks,,['analysis']
96,"If $\pi(x)$ is a probability distribution on $\mathbb{R}^d$, when is $\iint_{u,v} \min\big(\pi(u), \pi(v) \big) \ du \ dv$ finite?","If  is a probability distribution on , when is  finite?","\pi(x) \mathbb{R}^d \iint_{u,v} \min\big(\pi(u), \pi(v) \big) \ du \ dv","Consider a smooth probability density $\pi(x)$ on $\mathbb{R}^d$. I am looking for natural conditions that ensure that the integral $\iint_{u,v} \ \min\big(\pi(u), \pi(v) \big) \ du \ dv$ is finite. If $\pi$ is radially decreasing, this is equivalent to the condition $\mathbb{E}\big[ \|X\|^{d} \big] < \infty$. Are there smooth densities verifying this moment condition such that $\iint_{u,v} \min\big(\pi(u), \pi(v) \big) \ du \ dv = \infty$?","Consider a smooth probability density $\pi(x)$ on $\mathbb{R}^d$. I am looking for natural conditions that ensure that the integral $\iint_{u,v} \ \min\big(\pi(u), \pi(v) \big) \ du \ dv$ is finite. If $\pi$ is radially decreasing, this is equivalent to the condition $\mathbb{E}\big[ \|X\|^{d} \big] < \infty$. Are there smooth densities verifying this moment condition such that $\iint_{u,v} \min\big(\pi(u), \pi(v) \big) \ du \ dv = \infty$?",,"['probability', 'analysis']"
97,Extreme points in the set of positive linear functionals of norm $\leq 1$,Extreme points in the set of positive linear functionals of norm,\leq 1,"Let $A$ be a C* algebra, and $S$ the set of positive linear functionals on $A$ in the unit ball of $A^*$ (Which has the weak-* topology.)  I am having difficulty seeing that all nonzero extreme points of S must be pure states.  For me, a pure state $ \psi$ has norm 1 and if $\phi$ is a positive linear functional for which $ \phi \leq \psi$ then there exists $c \in [0, 1]$ for which $\phi=c*\psi$.  So far, my attempts have been as follows.  If $\psi$ is nonzero, then for it to be an extreme point, it's clear $\psi$ must have norm 1.  So I just need to see it's pure now.  Assume that $0 \leq \phi \leq \psi$, from which it follows that $||\phi|| \leq ||\psi||$ and $||\psi-\phi|| \leq ||\psi||$.  Then $1/2\psi=1/2\phi+1/2(\psi-\phi)$.  That's not good enough though, since $1/2\psi$ need not be extreme.  I don't see how I can use a condition like ""extreme"" which relies on convex combinations when multiplying by scalars like $1/2$ is not permissible.","Let $A$ be a C* algebra, and $S$ the set of positive linear functionals on $A$ in the unit ball of $A^*$ (Which has the weak-* topology.)  I am having difficulty seeing that all nonzero extreme points of S must be pure states.  For me, a pure state $ \psi$ has norm 1 and if $\phi$ is a positive linear functional for which $ \phi \leq \psi$ then there exists $c \in [0, 1]$ for which $\phi=c*\psi$.  So far, my attempts have been as follows.  If $\psi$ is nonzero, then for it to be an extreme point, it's clear $\psi$ must have norm 1.  So I just need to see it's pure now.  Assume that $0 \leq \phi \leq \psi$, from which it follows that $||\phi|| \leq ||\psi||$ and $||\psi-\phi|| \leq ||\psi||$.  Then $1/2\psi=1/2\phi+1/2(\psi-\phi)$.  That's not good enough though, since $1/2\psi$ need not be extreme.  I don't see how I can use a condition like ""extreme"" which relies on convex combinations when multiplying by scalars like $1/2$ is not permissible.",,"['analysis', 'functional-analysis', 'operator-algebras']"
98,on the sum $ \sum _{n=1}^{\infty}J_{0} (2\pi nx) $,on the sum, \sum _{n=1}^{\infty}J_{0} (2\pi nx) ,given the zeroeth order Bessel function.. is then possible to compute the sum $ \sum _{n=1}^{\infty}J_{0} (2\pi nx) $ for every 'x' positive real number ?,given the zeroeth order Bessel function.. is then possible to compute the sum $ \sum _{n=1}^{\infty}J_{0} (2\pi nx) $ for every 'x' positive real number ?,,"['analysis', 'special-functions']"
99,Fourier Coefficients in arbitrary Hilbert Spaces,Fourier Coefficients in arbitrary Hilbert Spaces,,"Say we have an orthonormal basis $\{e_n\}$ for a infinite Hilbert Space $H$.  I want to prove that any vector $x=\sum_{n=1}^\infty\langle x, e_n\rangle e_n$.  I don't know where to start.  Could I have  any help?","Say we have an orthonormal basis $\{e_n\}$ for a infinite Hilbert Space $H$.  I want to prove that any vector $x=\sum_{n=1}^\infty\langle x, e_n\rangle e_n$.  I don't know where to start.  Could I have  any help?",,"['analysis', 'functional-analysis', 'hilbert-spaces']"
