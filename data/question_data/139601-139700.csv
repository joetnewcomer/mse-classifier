,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"If a matrix contains a differential operator, can it have eigenvalues? If yes, how can I determine them?","If a matrix contains a differential operator, can it have eigenvalues? If yes, how can I determine them?",,"If a matrix contains a differential operator, can it have eigenvalues? If yes, how can I determine them? I will assume that the answer is yes and try to solve the following problem. Consider the matrix $A$ defined as: $$ A = \begin{bmatrix}0 & {d \over dx} \\1 & 0  \end{bmatrix} \tag 1$$ The eigenvectors can be found by solving a system of equations given by: $$ A\vec v - \lambda \vec v =0 \tag 2$$ where $\vec v$ is the eigenvector and $\lambda$ is the eigenvalue. Let $f(x)$ and $g(x)$ be components of $\vec v$ : $$ \vec v = \begin{bmatrix}f(x)  \\g(x)   \end{bmatrix} \tag 3 $$ Now equation $(2)$ becomes: $$ \begin{bmatrix}0 & {d \over dx} \\1 & 0  \end{bmatrix} \begin{bmatrix}f(x)  \\g(x)   \end{bmatrix} = \lambda \begin{bmatrix}f(x)  \\g(x)   \end{bmatrix}\tag 4$$ which is essentially the following system of equations: $$ {d g(x) \over dx} - \lambda f(x)=0 \tag 5$$ $$ f(x) - \lambda g(x)=0 \tag 6$$ The solutions of this system of equations are: $$ f(x) = C \lambda e^{\lambda ^2 x} \tag 7$$ $$ g(x) = C e^{\lambda ^2 x} \tag 8$$ The constant $C$ should be determined from the boundary condition. However, I don't know how to actually determine the value of $\lambda$ (assuming that it actually exists). If $A$ was a linear operator, the eigenvalues would be calculated from its determinant. But now $A$ contains a differential operator. So my question is, how can the eigenvalues of $A$ be determined?","If a matrix contains a differential operator, can it have eigenvalues? If yes, how can I determine them? I will assume that the answer is yes and try to solve the following problem. Consider the matrix defined as: The eigenvectors can be found by solving a system of equations given by: where is the eigenvector and is the eigenvalue. Let and be components of : Now equation becomes: which is essentially the following system of equations: The solutions of this system of equations are: The constant should be determined from the boundary condition. However, I don't know how to actually determine the value of (assuming that it actually exists). If was a linear operator, the eigenvalues would be calculated from its determinant. But now contains a differential operator. So my question is, how can the eigenvalues of be determined?","A  A = \begin{bmatrix}0 & {d \over dx} \\1 & 0
 \end{bmatrix} \tag 1  A\vec v - \lambda \vec v =0 \tag 2 \vec v \lambda f(x) g(x) \vec v  \vec v = \begin{bmatrix}f(x)  \\g(x) 
 \end{bmatrix} \tag 3  (2)  \begin{bmatrix}0 & {d \over dx} \\1 & 0
 \end{bmatrix} \begin{bmatrix}f(x)  \\g(x) 
 \end{bmatrix} = \lambda \begin{bmatrix}f(x)  \\g(x) 
 \end{bmatrix}\tag 4  {d g(x) \over dx} - \lambda f(x)=0 \tag 5  f(x) - \lambda g(x)=0 \tag 6  f(x) = C \lambda e^{\lambda ^2 x} \tag 7  g(x) = C e^{\lambda ^2 x} \tag 8 C \lambda A A A","['matrices', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'systems-of-equations', 'eigenfunctions']"
1,Misunderstanding of Fourier series and differential equation,Misunderstanding of Fourier series and differential equation,,"Let's assume simple differential equation: $$\frac{d^2}{dx^2}y=2 \quad y(\pm1)=0$$ the solution for the equation is: $$y=x^2-1$$ and the Fourier transform for the $y(x)$ in the solution interval $[-1,1]$ : $$y(x)=\frac{-2}{3}+\sum_{n=1}^{\infty}{\frac{4(-1)^n\cos(n\pi x)}{n^2\pi ^2}}$$ by substituting the Fourier series inside the original equation: $$\frac{d^2}{dx^2}y(x)=\sum_{n=1}^{\infty}{4(-1)^{n+1}\cos(n\pi x)\neq2}$$ But we can solve the equation by assuming Fourier series that fulfill Boundary conditions $$y(x)=\sum_{n=0}^{\infty}A_n \cos(\frac{2n+1}{2}\pi x)$$ The constant term can be transformed to $$2=\sum_{n=0}^{\infty}\frac{8(-1)^n}{(2n+1)\pi}\cos(\frac{2n+1}{2}\pi x)$$ $$\frac{d^2}{dx^2}y(x)=\sum_{n=0}^{\infty}A_n\frac{-(2n+1)^2\pi^2}{4} \cos(\frac{2n+1}{2}\pi x)$$ $$A_n\frac{-(2n+1)^2\pi^2}{4}=\frac{8(-1)^n}{(2n+1)\pi}$$ $$A_n=\frac{32(-1)^{n+1}}{(2n+1)^3\pi^3}$$ which can be proven to satisfy the required solution: $$y(x)=\sum_{n=0}^{\infty}A_n \cos(\frac{2n+1}{2}\pi x)=x^2-1$$ What I misunderstand? Why the first attempt is wrong? But it actually uses orthogonal basis in the solution interval.",Let's assume simple differential equation: the solution for the equation is: and the Fourier transform for the in the solution interval : by substituting the Fourier series inside the original equation: But we can solve the equation by assuming Fourier series that fulfill Boundary conditions The constant term can be transformed to which can be proven to satisfy the required solution: What I misunderstand? Why the first attempt is wrong? But it actually uses orthogonal basis in the solution interval.,"\frac{d^2}{dx^2}y=2 \quad y(\pm1)=0 y=x^2-1 y(x) [-1,1] y(x)=\frac{-2}{3}+\sum_{n=1}^{\infty}{\frac{4(-1)^n\cos(n\pi x)}{n^2\pi ^2}} \frac{d^2}{dx^2}y(x)=\sum_{n=1}^{\infty}{4(-1)^{n+1}\cos(n\pi x)\neq2} y(x)=\sum_{n=0}^{\infty}A_n \cos(\frac{2n+1}{2}\pi x) 2=\sum_{n=0}^{\infty}\frac{8(-1)^n}{(2n+1)\pi}\cos(\frac{2n+1}{2}\pi x) \frac{d^2}{dx^2}y(x)=\sum_{n=0}^{\infty}A_n\frac{-(2n+1)^2\pi^2}{4} \cos(\frac{2n+1}{2}\pi x) A_n\frac{-(2n+1)^2\pi^2}{4}=\frac{8(-1)^n}{(2n+1)\pi} A_n=\frac{32(-1)^{n+1}}{(2n+1)^3\pi^3} y(x)=\sum_{n=0}^{\infty}A_n \cos(\frac{2n+1}{2}\pi x)=x^2-1","['ordinary-differential-equations', 'fourier-series']"
2,Solving a non-linear Bernoulli ODE with a power substitution,Solving a non-linear Bernoulli ODE with a power substitution,,"So I have this problem where I have a differential equation on the form $$\frac{dy}{dt}=ay-\frac{a}{b^2}y^3$$ In this case I am only interested in the positive solution to the equation. Now since this is a non-linear ODE, it appeared somewhat tedious to solve, but then I recognised that it was infact an equation on the form $$y'+p(x)y=q(x)y^\beta$$ With $p(x)=-a$ and $q(x)=-a/b^2$ , so using the substitution $u=y^{1-\beta}$ we can write the equation as $$u'+(1-\beta)py=(1-\beta)q$$ Now for convenience we define $\gamma=(1-\beta)p$ and $\delta=(1-\beta)q$ such that the relationship between $\gamma$ and $\delta$ is $$\delta=\frac{q}{p}\gamma$$ Since we have reduced the non-linear ODE to a linear ODE, I thought it be wise to throw the laplace transform at it, so we end up with $$U(s)=\frac{q}{p}\frac{\gamma}{s(s+\gamma)}+\frac{U(0)}{s+\gamma}$$ Taking the inverse laplace transform and using $q/p=1/b^2$ and $\gamma=(1-\beta)p=(1-3)(-a)=2a$ , will then give $$u(t)=\frac{q}{p}(1-e^{-\gamma t})+U(0)e^{-\gamma t}=\frac{1}{b^2}(1-e^{-2at})+c_1e^{-2at}=\frac{e^{2at}-1+b^2c_1}{b^2e^{2at}}$$ Where U(0) is just some constant $c_1$ . Resubbing in $u(t)=y^{1-\beta}=y(t)^{-2}$ , inverting and then taking the square root yields $$y(t)=\frac{|b|e^{at}}{\sqrt{e^{2at}+b^2c_1-1}}$$ . However, when running this equation through Wolfram (y'know just in case...) I get the solution $$y(t)=\frac{be^{\frac{1}{2}(2at+2b^2c_1)}}{\sqrt{e^{2at+2b^2c_1}-1}}$$ And I am for the lack of better words too daft to see where I done goofed. If anyone immediately sees where it is I would love to have it pointed out.","So I have this problem where I have a differential equation on the form In this case I am only interested in the positive solution to the equation. Now since this is a non-linear ODE, it appeared somewhat tedious to solve, but then I recognised that it was infact an equation on the form With and , so using the substitution we can write the equation as Now for convenience we define and such that the relationship between and is Since we have reduced the non-linear ODE to a linear ODE, I thought it be wise to throw the laplace transform at it, so we end up with Taking the inverse laplace transform and using and , will then give Where U(0) is just some constant . Resubbing in , inverting and then taking the square root yields . However, when running this equation through Wolfram (y'know just in case...) I get the solution And I am for the lack of better words too daft to see where I done goofed. If anyone immediately sees where it is I would love to have it pointed out.",\frac{dy}{dt}=ay-\frac{a}{b^2}y^3 y'+p(x)y=q(x)y^\beta p(x)=-a q(x)=-a/b^2 u=y^{1-\beta} u'+(1-\beta)py=(1-\beta)q \gamma=(1-\beta)p \delta=(1-\beta)q \gamma \delta \delta=\frac{q}{p}\gamma U(s)=\frac{q}{p}\frac{\gamma}{s(s+\gamma)}+\frac{U(0)}{s+\gamma} q/p=1/b^2 \gamma=(1-\beta)p=(1-3)(-a)=2a u(t)=\frac{q}{p}(1-e^{-\gamma t})+U(0)e^{-\gamma t}=\frac{1}{b^2}(1-e^{-2at})+c_1e^{-2at}=\frac{e^{2at}-1+b^2c_1}{b^2e^{2at}} c_1 u(t)=y^{1-\beta}=y(t)^{-2} y(t)=\frac{|b|e^{at}}{\sqrt{e^{2at}+b^2c_1-1}} y(t)=\frac{be^{\frac{1}{2}(2at+2b^2c_1)}}{\sqrt{e^{2at+2b^2c_1}-1}},['ordinary-differential-equations']
3,Special way to solve systems of ODEs,Special way to solve systems of ODEs,,"If we have a system of two  differential equations and it can be written in the form: $$\frac{dx}{f_1(x,y,t)}=\frac{dy}{f_2(x,y,t)}=\frac{dt}{f_3(x,y,t)}=\gamma$$ Then we can conclude that: $$\frac{\alpha_1dx+\alpha_2dy+\alpha_3dt}{\alpha_1f_1(x,y,t)+\alpha_2f_2(x,y,t)+\alpha_3f_3(x,y,t)}=\gamma$$ Now if we choose the correct $\alpha_1,\alpha_2,\alpha_3$ , Then we can get a  solution. I don't understand if there is an easy way to get the correct coefficients of it is purely a guesswork. Here's an example: $$\frac{dx}{1+\sqrt{z-x-y}}=\frac{dy}{1}=\frac{dz}{2}=\gamma \Rightarrow \frac{\alpha_1dx+\alpha_2dy+\alpha_3dz}{\alpha_1(1+\sqrt{z-x-y})+\alpha_2+2\alpha_3}=\gamma$$ Then one easy tripple of coefficients is: $$\alpha_1=0 ;\\ \alpha_2=2 ;\\ \alpha_3=-1$$ Then after substitution we get: $$2dy-dz=d(2y-z)=0 \Rightarrow 2y-z=C_1$$ To get the full solution of the system, we need one more tripple of coefficients but I don't know what logic to use to find the most appropriate one. Can you help not only with this particular example but with this method as a whole? Also if you know how is it called because I can't even find it online to educate myself.","If we have a system of two  differential equations and it can be written in the form: Then we can conclude that: Now if we choose the correct , Then we can get a  solution. I don't understand if there is an easy way to get the correct coefficients of it is purely a guesswork. Here's an example: Then one easy tripple of coefficients is: Then after substitution we get: To get the full solution of the system, we need one more tripple of coefficients but I don't know what logic to use to find the most appropriate one. Can you help not only with this particular example but with this method as a whole? Also if you know how is it called because I can't even find it online to educate myself.","\frac{dx}{f_1(x,y,t)}=\frac{dy}{f_2(x,y,t)}=\frac{dt}{f_3(x,y,t)}=\gamma \frac{\alpha_1dx+\alpha_2dy+\alpha_3dt}{\alpha_1f_1(x,y,t)+\alpha_2f_2(x,y,t)+\alpha_3f_3(x,y,t)}=\gamma \alpha_1,\alpha_2,\alpha_3 \frac{dx}{1+\sqrt{z-x-y}}=\frac{dy}{1}=\frac{dz}{2}=\gamma \Rightarrow \frac{\alpha_1dx+\alpha_2dy+\alpha_3dz}{\alpha_1(1+\sqrt{z-x-y})+\alpha_2+2\alpha_3}=\gamma \alpha_1=0 ;\\ \alpha_2=2 ;\\ \alpha_3=-1 2dy-dz=d(2y-z)=0 \Rightarrow 2y-z=C_1","['ordinary-differential-equations', 'systems-of-equations']"
4,Bounding an unknown IVP solution,Bounding an unknown IVP solution,,"Given the Following IVP: $$y'=f(x,y)=\frac 1 {2+{\cos(xy)}}$$ $$y(0)=\frac 1 2$$ I need to show that the IVP has a single solution $u(x)$ that is defined in $(-\infty,\infty)$ , increasing, and that $u(x)<x$ for all $x>1$ I was able to show, using the fact that $f_y(x,y)=\frac {x \sin(xy)} {{2+\cos(xy)}^2}$ is bounded in $[a,b]\times(-\infty,\infty)$ for all $a$ , $b$ , that there exist a single solution, and to show that it's derivative is positive, so that it is increasing, but I couldn't get the inequality right. I Tried using the Mean Value Theorem but the bound that I got wasn't tight enough ( $u(x)<x+\frac 1 2$ ). I also tried to bound integral up to a general point $x>1$ but this didn't work either. Any assistance would be appreciated.","Given the Following IVP: I need to show that the IVP has a single solution that is defined in , increasing, and that for all I was able to show, using the fact that is bounded in for all , , that there exist a single solution, and to show that it's derivative is positive, so that it is increasing, but I couldn't get the inequality right. I Tried using the Mean Value Theorem but the bound that I got wasn't tight enough ( ). I also tried to bound integral up to a general point but this didn't work either. Any assistance would be appreciated.","y'=f(x,y)=\frac 1 {2+{\cos(xy)}} y(0)=\frac 1 2 u(x) (-\infty,\infty) u(x)<x x>1 f_y(x,y)=\frac {x \sin(xy)} {{2+\cos(xy)}^2} [a,b]\times(-\infty,\infty) a b u(x)<x+\frac 1 2 x>1","['ordinary-differential-equations', 'initial-value-problems']"
5,"Meaning of $\frac{dx}{f(x,y)}=\frac{dy}{g(x,y)}$",Meaning of,"\frac{dx}{f(x,y)}=\frac{dy}{g(x,y)}",Consider the rotation group $SO(2)$ which has infinitesimal generator $v = -y\partial_x +x\partial _y$ . The corresponding characteristic system is $$\frac{dx}{-y}=\frac{dy}{x}.$$ This first order ordinary differential equation is easily solved; the solutions are $x^2+y^2=c$ for $c$ an arbitrary constant. I don't really understand the form of the ODE. Does it really mean $\frac{dy}{dx}=-\frac{y}{x}$ ? and that the solutions are $y = \pm\sqrt{c-x^2}$ for any $c$ ? Why write it into this weird form?,Consider the rotation group which has infinitesimal generator . The corresponding characteristic system is This first order ordinary differential equation is easily solved; the solutions are for an arbitrary constant. I don't really understand the form of the ODE. Does it really mean ? and that the solutions are for any ? Why write it into this weird form?,SO(2) v = -y\partial_x +x\partial _y \frac{dx}{-y}=\frac{dy}{x}. x^2+y^2=c c \frac{dy}{dx}=-\frac{y}{x} y = \pm\sqrt{c-x^2} c,"['ordinary-differential-equations', 'differential-geometry']"
6,Two different solutions to implicit ODE,Two different solutions to implicit ODE,,Where is the gap in my Logic? Im trying to solve the following implicit first order differential equation: $$y=y'^2x+y'^2$$ so I make the substitution $$p=y' \Rightarrow p=\frac{\partial y}{\partial x}+\frac{\partial y}{\partial p}p'=p^2+(2px+2p)p’$$ . Then after solving the linear differential equation we get that: $$p=1+\frac{c}{\sqrt{x+1}}=y’$$ So if we integrate $p$ then we get: $$y=x+2c\sqrt{x+1}+c'$$ where we have two parameters. But if we instead choose to plug $p$ in the original equation we get the answer: $$y=(\sqrt{x+1}+c)^2$$ Why the first metod yields infinitely many more solutions?,Where is the gap in my Logic? Im trying to solve the following implicit first order differential equation: so I make the substitution . Then after solving the linear differential equation we get that: So if we integrate then we get: where we have two parameters. But if we instead choose to plug in the original equation we get the answer: Why the first metod yields infinitely many more solutions?,y=y'^2x+y'^2 p=y' \Rightarrow p=\frac{\partial y}{\partial x}+\frac{\partial y}{\partial p}p'=p^2+(2px+2p)p’ p=1+\frac{c}{\sqrt{x+1}}=y’ p y=x+2c\sqrt{x+1}+c' p y=(\sqrt{x+1}+c)^2,['ordinary-differential-equations']
7,How to eliminate $\dot{x}$ term in differential equation $\ddot{x} + p(t) \dot{x} + q(t) x = 0$?,How to eliminate  term in differential equation ?,\dot{x} \ddot{x} + p(t) \dot{x} + q(t) x = 0,"Vladimir Arnold's Ordinary Differential Equations (page 252) states that ""the more general equation $\ddot{x} + p(t) \dot{x} + q(t) x = 0$ is easily reduced to the form $\ddot{x} + q(t) x = 0$ "". The book doesn't explain how to do this; I guess it's assumed to be obvious, but it's not obvious to me. Can someone please explain?","Vladimir Arnold's Ordinary Differential Equations (page 252) states that ""the more general equation is easily reduced to the form "". The book doesn't explain how to do this; I guess it's assumed to be obvious, but it's not obvious to me. Can someone please explain?",\ddot{x} + p(t) \dot{x} + q(t) x = 0 \ddot{x} + q(t) x = 0,['ordinary-differential-equations']
8,Brezis' theorem 7.7: how to approach to prove $\left |\frac{d u_\lambda}{dt} (T) \right| \le\frac{|u_0|}{T}$?,Brezis' theorem 7.7: how to approach to prove ?,\left |\frac{d u_\lambda}{dt} (T) \right| \le\frac{|u_0|}{T},"Let $(H, \langle \cdot, \cdot \rangle)$ be a real Hilbert space and $|\cdot|$ its induced norm. Let $A: D(A) \subset H \to H$ be a maximal monotone (unbounded linear) operator. Let $I:H \to H$ be the identity map. For every $\lambda>0$ , we define the resolvent $J_\lambda$ and the Yosida approximation $A_\lambda$ of $A$ by $$ J_\lambda=(I+\lambda A)^{-1} \quad \text { and } \quad A_\lambda=\frac{1}{\lambda}\left(I-J_\lambda\right). $$ We define by induction the subspaces $$ D(A^{k+1}) := \{ v \in D(A^k) :Av \in D(A^k)\} \quad \forall k \in \mathbb N^*. $$ Then $D(A^k)$ is a Hilbert space for the inner product $$ \langle u, v \rangle_{D(A^k)} := \sum_{j=0}^k \langle A^j u, A^j v \rangle. $$ I'm reading Theorem 7.7 at page 194 of Brezis' Functional Analysis , i.e., Let $A$ be a self-adjoint maximal monotone (unbounded linear) operator. Then, given any $u_0 \in H$ there exists a unique function $$ u \in C([0,+\infty) ; H) \cap C^1((0,+\infty) ; H) \cap C((0, +\infty); D(A)) $$ such that $$ \begin{cases} \frac{d u}{d t}+A u&=0 \quad \text{on} \quad (0,+\infty), \\ u(0)&=u_0. \end{cases} $$ Moreover, we have $$ |u(t)| \leq\left|u_0\right| \quad \text { and } \quad\left|\frac{d u}{d t}(t)\right|=|Au(t)| \leq \frac{1}{t} \left|u_0\right| \quad \forall t > 0, $$ and $$ u \in u \in C^k ((0,+\infty) ; D(A^\ell)) \quad \forall k, \ell \in \mathbb N. $$ Assume that $u_0 \in D(A^2)$ . For $\lambda>0$ , let $u_\lambda$ be the solution of the problem $$ \begin{cases} \frac{d u_\lambda}{d t}+A_\lambda u_\lambda&=0 \quad \text{on} \quad [0,+\infty), \\ u_\lambda(0)&=u_0. \end{cases} $$ The author then goes on to prove that $$ \left |\frac{d u_\lambda}{dt} (T) \right| \le\frac{|u_0|}{T} \quad \forall T>0, \quad (*) $$ by using the following properties $A_\lambda$ is self-adjoint. $u_\lambda \in C^2([0, +\infty) ; H) \cap C^1 ([0, +\infty) ; D(A)) \cap C([0, +\infty); D(A^2))$ . $\frac{d}{dt} (A_\lambda u_\lambda) = A_\lambda (\frac{d u_\lambda}{dt})$ . the map $t \mapsto |\frac{d u_\lambda}{dt} (t)|$ is non-increasing. His proof of $(*)$ is non-trivial by integrating and then combining inequalities. It's like magic coming out of nowhere. I could not get a feeling of how such ideas arise. Could you shed some light on how you approach to prove $(*)$ ?","Let be a real Hilbert space and its induced norm. Let be a maximal monotone (unbounded linear) operator. Let be the identity map. For every , we define the resolvent and the Yosida approximation of by We define by induction the subspaces Then is a Hilbert space for the inner product I'm reading Theorem 7.7 at page 194 of Brezis' Functional Analysis , i.e., Let be a self-adjoint maximal monotone (unbounded linear) operator. Then, given any there exists a unique function such that Moreover, we have and Assume that . For , let be the solution of the problem The author then goes on to prove that by using the following properties is self-adjoint. . . the map is non-increasing. His proof of is non-trivial by integrating and then combining inequalities. It's like magic coming out of nowhere. I could not get a feeling of how such ideas arise. Could you shed some light on how you approach to prove ?","(H, \langle \cdot, \cdot \rangle) |\cdot| A: D(A) \subset H \to H I:H \to H \lambda>0 J_\lambda A_\lambda A 
J_\lambda=(I+\lambda A)^{-1} \quad \text { and } \quad A_\lambda=\frac{1}{\lambda}\left(I-J_\lambda\right).
 
D(A^{k+1}) := \{ v \in D(A^k) :Av \in D(A^k)\}
\quad \forall k \in \mathbb N^*.
 D(A^k) 
\langle u, v \rangle_{D(A^k)} := \sum_{j=0}^k \langle A^j u, A^j v \rangle.
 A u_0 \in H 
u \in C([0,+\infty) ; H) \cap C^1((0,+\infty) ; H) \cap C((0, +\infty); D(A))
 
\begin{cases}
\frac{d u}{d t}+A u&=0 \quad \text{on} \quad (0,+\infty), \\
u(0)&=u_0.
\end{cases}
 
|u(t)| \leq\left|u_0\right| \quad \text { and } \quad\left|\frac{d u}{d t}(t)\right|=|Au(t)| \leq \frac{1}{t} \left|u_0\right| \quad \forall t > 0,
 
u \in u \in C^k ((0,+\infty) ; D(A^\ell))
\quad \forall k, \ell \in \mathbb N.
 u_0 \in D(A^2) \lambda>0 u_\lambda 
\begin{cases}
\frac{d u_\lambda}{d t}+A_\lambda u_\lambda&=0 \quad \text{on} \quad [0,+\infty), \\
u_\lambda(0)&=u_0.
\end{cases}
 
\left |\frac{d u_\lambda}{dt} (T) \right| \le\frac{|u_0|}{T}
\quad \forall T>0, \quad (*)
 A_\lambda u_\lambda \in C^2([0, +\infty) ; H) \cap C^1 ([0, +\infty) ; D(A)) \cap C([0, +\infty); D(A^2)) \frac{d}{dt} (A_\lambda u_\lambda) = A_\lambda (\frac{d u_\lambda}{dt}) t \mapsto |\frac{d u_\lambda}{dt} (t)| (*) (*)","['real-analysis', 'functional-analysis', 'ordinary-differential-equations', 'proof-explanation', 'hilbert-spaces']"
9,"How to solve ODE's given evaluations and constraints of an equation or its derivatives, but not the equations themselves?","How to solve ODE's given evaluations and constraints of an equation or its derivatives, but not the equations themselves?",,"How to begin to solve ODE problems where no equations are known, only the evaluations of equations at certain points? I tried googling this and searching the exchange here, but I'm not even sure what the name for this type of problem is, so I am having difficulty finding relevant results. Seems like all solutions for ODE require having some equation already and starting from there. Here is an example on the simpler side that I am currently working on where I am trying to find f that satisfies all of the following: $$ f'(0) = \infty $$ $$ f'(1) = \infty $$ $$ f'(x) > 0 $$ $$ f(0) = 0 $$ $$ f(1) = 1 $$ $$ f(x) = 1 - f(1 - x) $$ and smooth and continuous for all $x \in [0, 1]$ Simply, a continuously increasing function from 0 to 1 with vertical slope at the edges and symmetry about the middle point. I know the requirements, but I have no equations to start from. And this just an example that I happen to have handy, so I am not necessarily looking for just an answer to this set; I would like an understanding of how to approach this type of problem. (not sure what all tags this should have, feel free to let me know in comments)","How to begin to solve ODE problems where no equations are known, only the evaluations of equations at certain points? I tried googling this and searching the exchange here, but I'm not even sure what the name for this type of problem is, so I am having difficulty finding relevant results. Seems like all solutions for ODE require having some equation already and starting from there. Here is an example on the simpler side that I am currently working on where I am trying to find f that satisfies all of the following: and smooth and continuous for all Simply, a continuously increasing function from 0 to 1 with vertical slope at the edges and symmetry about the middle point. I know the requirements, but I have no equations to start from. And this just an example that I happen to have handy, so I am not necessarily looking for just an answer to this set; I would like an understanding of how to approach this type of problem. (not sure what all tags this should have, feel free to let me know in comments)"," f'(0) = \infty   f'(1) = \infty   f'(x) > 0   f(0) = 0   f(1) = 1   f(x) = 1 - f(1 - x)  x \in [0, 1]","['functional-analysis', 'ordinary-differential-equations', 'functions', 'functional-equations']"
10,differential equation with distribution,differential equation with distribution,,"I'm trying to solve the differential equation: \begin{equation} \frac{d^{2}y}{dx^{2}}=\delta(x)y \end{equation} My 'guess' was a solution of the following form: \begin{equation} y=A\exp\left[\int_{x_{0}}^{x}dx^{\prime}\int_{x_{0}}^{x^{\prime}}dx^{\prime\prime}\delta(x^{\prime\prime})\right]+B\exp\left[\int_{x_{0}}^{x}dx^{\prime}\delta(x^{\prime})\right] \end{equation} Is there any way to solve such an equation? EDIT: more interestingly, \begin{equation} \frac{d^{2}y}{dx^{2}}=\delta^{\prime}(x)y \end{equation}","I'm trying to solve the differential equation: My 'guess' was a solution of the following form: Is there any way to solve such an equation? EDIT: more interestingly,","\begin{equation}
\frac{d^{2}y}{dx^{2}}=\delta(x)y
\end{equation} \begin{equation}
y=A\exp\left[\int_{x_{0}}^{x}dx^{\prime}\int_{x_{0}}^{x^{\prime}}dx^{\prime\prime}\delta(x^{\prime\prime})\right]+B\exp\left[\int_{x_{0}}^{x}dx^{\prime}\delta(x^{\prime})\right]
\end{equation} \begin{equation}
\frac{d^{2}y}{dx^{2}}=\delta^{\prime}(x)y
\end{equation}","['ordinary-differential-equations', 'distribution-theory']"
11,Is there a solution to this ODE: $ax'' - bx^γ+c = 0$?,Is there a solution to this ODE: ?,ax'' - bx^γ+c = 0,"I have arrived here when analysing the adiabatic oscillations of a piston in a cylinder with (pressurised) air behind it and and a vacuum in front, with the pressure force from the air on the piston balanced by a weight on a cable over a pulley. I have found an approximate solution for values of $a$ , $b$ and ω in the form of $x = a + b \cos(ωt)$ using numerical methods and a spreadsheet, but am curious to know if there is a calculus solution to the ODE or if the numerical approximation is the only answer. I have reviewed the literature and am unable to find a clue to the solution. Please note, this is not a homework question, it is a problem I have arrived at in my private study of gas properties.","I have arrived here when analysing the adiabatic oscillations of a piston in a cylinder with (pressurised) air behind it and and a vacuum in front, with the pressure force from the air on the piston balanced by a weight on a cable over a pulley. I have found an approximate solution for values of , and ω in the form of using numerical methods and a spreadsheet, but am curious to know if there is a calculus solution to the ODE or if the numerical approximation is the only answer. I have reviewed the literature and am unable to find a clue to the solution. Please note, this is not a homework question, it is a problem I have arrived at in my private study of gas properties.",a b x = a + b \cos(ωt),['ordinary-differential-equations']
12,Is state space representation useful for nonlinear control systems?,Is state space representation useful for nonlinear control systems?,,"I understand that the state space representation is mathematically equivalent to the transfer function representation for linear systems, and that it allows us to solve the corresponding DE by finding the eigenvalues of a matrix. However, for nonlinear systems, the transfer function can only represent a linear approximation, while the state space form can represent the full system. But what's the advantage of using state space form for nonlinear systems, if we can't generally solve them by matrix methods? How does state space representation help us analyze or design nonlinear control systems any better than we could by sticking with the original DE representation? Some background: My impression was that the state space form of linear systems is essentially just syntactic sugar for the final result of transforming a nth order DE into a system of n first order DE's, and writing that system as a single matrix equation. It ""hides"" the derivatives under the extra parameterization variables. But for nonlinear systems, we can't just get a system of linear equations and write it as a single matrix equation that doesn't explicitly involve derivatives. So I don't see how the state space form simplifies anything for nonlinear systems.","I understand that the state space representation is mathematically equivalent to the transfer function representation for linear systems, and that it allows us to solve the corresponding DE by finding the eigenvalues of a matrix. However, for nonlinear systems, the transfer function can only represent a linear approximation, while the state space form can represent the full system. But what's the advantage of using state space form for nonlinear systems, if we can't generally solve them by matrix methods? How does state space representation help us analyze or design nonlinear control systems any better than we could by sticking with the original DE representation? Some background: My impression was that the state space form of linear systems is essentially just syntactic sugar for the final result of transforming a nth order DE into a system of n first order DE's, and writing that system as a single matrix equation. It ""hides"" the derivatives under the extra parameterization variables. But for nonlinear systems, we can't just get a system of linear equations and write it as a single matrix equation that doesn't explicitly involve derivatives. So I don't see how the state space form simplifies anything for nonlinear systems.",,"['ordinary-differential-equations', 'dynamical-systems', 'control-theory', 'nonlinear-system', 'applications']"
13,Finding the solutions of $xp^2-2py+4x=0$,Finding the solutions of,xp^2-2py+4x=0,"I am trying to find the general solution, $p$ and $c$ discriminant and hence the singular solutions (if any) of the following D.E. where $p=\mathrm dy/\mathrm dx$ . $$xp^2-2py+4x=0$$ The obvious thing to do is isolate $p$ using the quadratic formula. The $p$ -discriminant is $4y^2-16x^2$ so I suspect one or both of $y=\pm 2x$ could be the envelope of the general solution (i.e., singular soln. of our D.E.). $$p=\frac{y}{x}\pm \sqrt{\left(\frac yx\right)^2-4}$$ Substitute $y=tx$ to get: $$x\cdot\frac{\mathrm dt}{\mathrm dx}=\pm\sqrt{t^2-4}$$ Integrating: $$\ln|cx|=\pm \ln\left|t+\sqrt{t^2-4}\right|$$ where $c$ is the parameter of the general soln. Now we have two cases: $$cx=t+\sqrt{t^2-4}\\ \implies cx^2=y+\sqrt{y^2-4x^2}$$ $$cx=\frac{1}{t+\sqrt{t^2-4}}=\frac{t-\sqrt{t^2-4}}4\\ \implies cx^2=y-\sqrt{y^2-4x^2}\\ \text{where } 4c\mapsto c$$ I am unable to decide which one to go with for finding the $c$ -discriminant. Is there any other way to solve this D.E. instead of isolating $p$ by quadratic formula? Update : I realized I had made a lot of silly mistakes. I've fixed them. Now it's clear that collectively, the two cases yield (take $c/2\mapsto c$ ): $$c=\frac{y\pm\sqrt{y^2-4x^2}}{2x^2}\\ =\frac{-(-y)\pm\sqrt{(-y)^2-4x^2\cdot 1}}{2x^2}$$ Using the quadratic formula (in reverse), the $c$ -discriminant turns out to be $y^2-4x^2$ . This confirms that $y=\pm 2x$ are indeed envelops and hence, singular solutions. $$\phi(x,y,c)=x^2c^2-yc+1=0$$ See this Desmos slideshow (graph of the envelope and the family $\phi$ ). Alternate approach: Isolate $y$ in the original D.E. $$y=\frac{px}{2}+\frac{2x}{p}$$ This seems a little off from the Clairut's form i.e., $y=px+f(p)$ . Substitute $u=x^2$ . Denote $\mathrm dy/\mathrm du$ by $m$ . So differentiating w.r.t. $y$ , we have $\frac{1}{m}=\frac{2x}p$ i.e., $p=2mx$ . $$y=um+\frac{1}{m}$$ This is in Clairut's form i.e., $y=um+f(m)$ so the general soln. is given by $y=uc+f(c)$ i.e., $$y=uc+\frac{1}{c}=cx^2+\frac{1}{c}$$ And the singular soln. can be determined by eliminating $m$ from the D.E. using $u+f'(m)=0$ i.e., $m=\pm1/\sqrt u$ . $$y=\pm 2\sqrt u=\pm 2x$$","I am trying to find the general solution, and discriminant and hence the singular solutions (if any) of the following D.E. where . The obvious thing to do is isolate using the quadratic formula. The -discriminant is so I suspect one or both of could be the envelope of the general solution (i.e., singular soln. of our D.E.). Substitute to get: Integrating: where is the parameter of the general soln. Now we have two cases: I am unable to decide which one to go with for finding the -discriminant. Is there any other way to solve this D.E. instead of isolating by quadratic formula? Update : I realized I had made a lot of silly mistakes. I've fixed them. Now it's clear that collectively, the two cases yield (take ): Using the quadratic formula (in reverse), the -discriminant turns out to be . This confirms that are indeed envelops and hence, singular solutions. See this Desmos slideshow (graph of the envelope and the family ). Alternate approach: Isolate in the original D.E. This seems a little off from the Clairut's form i.e., . Substitute . Denote by . So differentiating w.r.t. , we have i.e., . This is in Clairut's form i.e., so the general soln. is given by i.e., And the singular soln. can be determined by eliminating from the D.E. using i.e., .","p c p=\mathrm dy/\mathrm dx xp^2-2py+4x=0 p p 4y^2-16x^2 y=\pm 2x p=\frac{y}{x}\pm \sqrt{\left(\frac yx\right)^2-4} y=tx x\cdot\frac{\mathrm dt}{\mathrm dx}=\pm\sqrt{t^2-4} \ln|cx|=\pm \ln\left|t+\sqrt{t^2-4}\right| c cx=t+\sqrt{t^2-4}\\ \implies cx^2=y+\sqrt{y^2-4x^2} cx=\frac{1}{t+\sqrt{t^2-4}}=\frac{t-\sqrt{t^2-4}}4\\ \implies cx^2=y-\sqrt{y^2-4x^2}\\ \text{where } 4c\mapsto c c p c/2\mapsto c c=\frac{y\pm\sqrt{y^2-4x^2}}{2x^2}\\ =\frac{-(-y)\pm\sqrt{(-y)^2-4x^2\cdot 1}}{2x^2} c y^2-4x^2 y=\pm 2x \phi(x,y,c)=x^2c^2-yc+1=0 \phi y y=\frac{px}{2}+\frac{2x}{p} y=px+f(p) u=x^2 \mathrm dy/\mathrm du m y \frac{1}{m}=\frac{2x}p p=2mx y=um+\frac{1}{m} y=um+f(m) y=uc+f(c) y=uc+\frac{1}{c}=cx^2+\frac{1}{c} m u+f'(m)=0 m=\pm1/\sqrt u y=\pm 2\sqrt u=\pm 2x","['ordinary-differential-equations', 'discriminant']"
14,"Are there formal definitions of ""state"" and ""state variable"" in the context of state space models in control theory?","Are there formal definitions of ""state"" and ""state variable"" in the context of state space models in control theory?",,"I'm taking a class on control theory and I thought I understood the state space representation of linear systems -- it seemed like essentially just extra syntax (or, ""syntactic sugar"" as programmers would call it) used in the process of a turning an nth order linear differential equation into a system of n first order linear DE's.  It seemed the ""state variables"" were additional syntax to ""hide"" the additional derivatives. e.g. If I have the DE $$v''(t) = v'(t) + 2u(t) + 3v(t)$$ I can define the state variables $$x_1 = v(t), x_2 = x_1'(t) = v'(t)$$ and rewrite the single 2nd order DE as the set of equations $$ x_2' = x_2 + 2u + 3x_1 $$ $$y = x_1 $$ where the first equation is considered the ""state"" equation and the second is the output equation.  And since this is a set of linear equations we could of course represent it in matrix form.  The process is simple enough, at least for the sorts of problems we're dealing with in my class. But the thing I realized today that I don't get is, what makes the first equation the ""state"" equation?  What even is a state, mathematically, in the context of state spaces? I've seen informal definitions stuff like, A state variable is one of the set of variables that are used to describe the mathematical ""state"" of a dynamical system. Intuitively, the state of a system describes enough about the system to determine its future behaviour in the absence of any external forces affecting the system. ( Wikipedia ) and The state variables represent values from inside the system that can change over time. ( Wikibooks ) And I've seen specific examples where it intuitively makes sense to call one set of variables the state variables: e.g. if we're modeling an RLC circuit and we just care about the voltage on the resistor and are controlling the voltage of the power source, it makes sense intuitively to call the resistor voltage the output, the power source voltage the input, and the voltages through the inductor and capacitor the state variables. But surely there has to be a more formal and less hand-wavy definition than that?  What I'm really looking for is something in terms of linear and/or abstract algebra that doesn't rely on vague and nonmathematical terms like ""inside the system"".","I'm taking a class on control theory and I thought I understood the state space representation of linear systems -- it seemed like essentially just extra syntax (or, ""syntactic sugar"" as programmers would call it) used in the process of a turning an nth order linear differential equation into a system of n first order linear DE's.  It seemed the ""state variables"" were additional syntax to ""hide"" the additional derivatives. e.g. If I have the DE I can define the state variables and rewrite the single 2nd order DE as the set of equations where the first equation is considered the ""state"" equation and the second is the output equation.  And since this is a set of linear equations we could of course represent it in matrix form.  The process is simple enough, at least for the sorts of problems we're dealing with in my class. But the thing I realized today that I don't get is, what makes the first equation the ""state"" equation?  What even is a state, mathematically, in the context of state spaces? I've seen informal definitions stuff like, A state variable is one of the set of variables that are used to describe the mathematical ""state"" of a dynamical system. Intuitively, the state of a system describes enough about the system to determine its future behaviour in the absence of any external forces affecting the system. ( Wikipedia ) and The state variables represent values from inside the system that can change over time. ( Wikibooks ) And I've seen specific examples where it intuitively makes sense to call one set of variables the state variables: e.g. if we're modeling an RLC circuit and we just care about the voltage on the resistor and are controlling the voltage of the power source, it makes sense intuitively to call the resistor voltage the output, the power source voltage the input, and the voltages through the inductor and capacitor the state variables. But surely there has to be a more formal and less hand-wavy definition than that?  What I'm really looking for is something in terms of linear and/or abstract algebra that doesn't rely on vague and nonmathematical terms like ""inside the system"".","v''(t) = v'(t) + 2u(t) + 3v(t) x_1 = v(t), x_2 = x_1'(t) = v'(t)  x_2' = x_2 + 2u + 3x_1
 y = x_1
","['linear-algebra', 'ordinary-differential-equations', 'definition', 'control-theory', 'linear-control']"
15,Differential Equations: What is the difference between a linear and nonlinear centre?,Differential Equations: What is the difference between a linear and nonlinear centre?,,"In 2nd year differential equations, we define a centre to be the graphical phenomena we witness around a critical point with complex eigenvalues in form $\pm bi$ . But, we also throw around this term ""non-linear centre,"" saying that for a conservative system, a linear centre is often a non-linear centre. I'm having a tough time differentiating between a ""non-linear centre"" and a ""linear centre,"" being only familiar with what a ""centre"" is based on my earlier definition. What's the difference? Could you give an example?","In 2nd year differential equations, we define a centre to be the graphical phenomena we witness around a critical point with complex eigenvalues in form . But, we also throw around this term ""non-linear centre,"" saying that for a conservative system, a linear centre is often a non-linear centre. I'm having a tough time differentiating between a ""non-linear centre"" and a ""linear centre,"" being only familiar with what a ""centre"" is based on my earlier definition. What's the difference? Could you give an example?",\pm bi,"['ordinary-differential-equations', 'differential']"
16,An Ambiguous Definition of Envelopes with reference to Differential Equations and Singular Solutions.,An Ambiguous Definition of Envelopes with reference to Differential Equations and Singular Solutions.,,"Recently, I was, going through a definition of envelope. I know that the definitions can be written and the exposition of it, might vary, but the following definition, which I am hereby mentioning, has the definition of an envelope given with the reference to a differential equations of 1st order(, and probably 2nd degree,) and I am unable to make up /decipher the meaning. Here it is: Note:Throughout this post, the variable $p$ designates, $\frac{dy}{dx}.$ The function, $f(x,y,p)=0$ respresents a differential equation and $\phi(x,y,c)$ represents the corresponding general solution of $f$ with $c$ as the arbitrary constant. The envelope: If in $\phi(x,y,c)= 0,$ c be given all possible values, there is obtained a set of curves, infinite in number, of the same kind. Suppose that the $c'$ s are arranged in order of magnitude, the successive $c$ 's thus differing by infinitesimal amounts, and that all these curves are drawn. Curves corresponding, to two consecutive values of $c$ are called consecutive curves, and their intersection is called an "" ultimate point of intersection "". The limiting position of these points of intersection includes the envelope of the system of curves. It is shown in works on the differential calculus, that the envelope is part of the locus of the equation obtained by eliminating $c$ between $$\phi(x,y,c)=0,$$ and $$\frac{\partial \phi}{\partial c}=0,$$ that is, the envelope is part of the locus of the $c$ discriminant relation. This might have been anticipated, because in the limit the $c$ 's for two consecutive curves become equal, and the $c$ discriminant relation represents the locus of points for which $\phi(x,y,c)= 0,$ will have equal values of $c.$ It is also shown in the differential calculus, that at any point on the envelope, the latter is touched by some curve of the system; that is, that the envelope and some one of the curves have the same value of $p$ at the point. The definitions used in this article for reference are : p-discriminant -When the equation is quadratic, the discriminant can be written immediately; but when it is such that the condition for equal roots is not easily perceived, the discriminant is found in the following way. The given equation being $F=0,$ form another equation by (partially)  differentiating $F$ with respect to the variable, and eliminate the variable between the two equations. For example, $\phi(x,y,c)= 0,$ may be looked on as an equation in $c$ , its coefficients then being funetions of $x$ and $y.$ The simplest rational function of $x$ and $y,$ whose vanishing expresses that the equation $\phi(x,y,c)=0$ has equal roots for $c,$ is called the $c$ discriminant of $\phi,$ and is obtained by eliminating $c$ between the equations, $$\phi(x,y,c)= 0,$$ and $$\frac{\partial \phi}{\partial c}=0.$$ Thus the $c$ discriminant relation represents the locus, for each point of which $\phi(x,y,c)= 0,$ has equal values of $c$ . Similarly, the $p$ discriminant of $f(x, y, p) = 0,$ the differential equation corresponding to $\phi (x, y, c)= 0, $ is obtained by eliminating $p$ between the equations, $$f(x,y,p)= 0,$$ and $$\frac{\partial f}{\partial p}=0.$$ Thus the $p$ discriminant relation represents the locus, for each point of which $f(x, y, p)= 0$ has equal values of $p.$ In order that there may be a $c$ and a $p$ discriminant, the above equations must be of the second degree at least in $c$ and $p.$ Here, we assumed, that these equations are of the same degree in $c$ and $p,$ and hence, if there is a $p$ discriminant, there must be a $c$ discriminant. First of all, I dont understand the fact where it says, ""Curves corresponding, to two consecutive values of $c$ are called consecutive curves, and their intersection is called an "" ultimate point of intersection ."" I feel this is erroneous , for $c$ is an arbitrary real constant and there is nothing as, ""consecutive real numbers,"" since there are infinite real numbers between any two real numbers. Finally, where they are supposedly asserting the definition of the envelope as  : The limiting position of these points of intersection includes the envelope of the system of curves. I don't have an idea, what are they trying to imply by the phrase, "" limiting position of these points of intersection "". I don't understand the meaning at all. This whole article is turning to be much confusing. Any explanations addressing these issues will be greatly appreciated. An intuitive geometrical  picture concerning this particular notion in the article will be much helpful. The above definitions/contents  appear in the book ,""An Introductory course in Differential Equations "" by D.A Murray, on the chapter titled ""Singular Solutions.""","Recently, I was, going through a definition of envelope. I know that the definitions can be written and the exposition of it, might vary, but the following definition, which I am hereby mentioning, has the definition of an envelope given with the reference to a differential equations of 1st order(, and probably 2nd degree,) and I am unable to make up /decipher the meaning. Here it is: Note:Throughout this post, the variable designates, The function, respresents a differential equation and represents the corresponding general solution of with as the arbitrary constant. The envelope: If in c be given all possible values, there is obtained a set of curves, infinite in number, of the same kind. Suppose that the s are arranged in order of magnitude, the successive 's thus differing by infinitesimal amounts, and that all these curves are drawn. Curves corresponding, to two consecutive values of are called consecutive curves, and their intersection is called an "" ultimate point of intersection "". The limiting position of these points of intersection includes the envelope of the system of curves. It is shown in works on the differential calculus, that the envelope is part of the locus of the equation obtained by eliminating between and that is, the envelope is part of the locus of the discriminant relation. This might have been anticipated, because in the limit the 's for two consecutive curves become equal, and the discriminant relation represents the locus of points for which will have equal values of It is also shown in the differential calculus, that at any point on the envelope, the latter is touched by some curve of the system; that is, that the envelope and some one of the curves have the same value of at the point. The definitions used in this article for reference are : p-discriminant -When the equation is quadratic, the discriminant can be written immediately; but when it is such that the condition for equal roots is not easily perceived, the discriminant is found in the following way. The given equation being form another equation by (partially)  differentiating with respect to the variable, and eliminate the variable between the two equations. For example, may be looked on as an equation in , its coefficients then being funetions of and The simplest rational function of and whose vanishing expresses that the equation has equal roots for is called the discriminant of and is obtained by eliminating between the equations, and Thus the discriminant relation represents the locus, for each point of which has equal values of . Similarly, the discriminant of the differential equation corresponding to is obtained by eliminating between the equations, and Thus the discriminant relation represents the locus, for each point of which has equal values of In order that there may be a and a discriminant, the above equations must be of the second degree at least in and Here, we assumed, that these equations are of the same degree in and and hence, if there is a discriminant, there must be a discriminant. First of all, I dont understand the fact where it says, ""Curves corresponding, to two consecutive values of are called consecutive curves, and their intersection is called an "" ultimate point of intersection ."" I feel this is erroneous , for is an arbitrary real constant and there is nothing as, ""consecutive real numbers,"" since there are infinite real numbers between any two real numbers. Finally, where they are supposedly asserting the definition of the envelope as  : The limiting position of these points of intersection includes the envelope of the system of curves. I don't have an idea, what are they trying to imply by the phrase, "" limiting position of these points of intersection "". I don't understand the meaning at all. This whole article is turning to be much confusing. Any explanations addressing these issues will be greatly appreciated. An intuitive geometrical  picture concerning this particular notion in the article will be much helpful. The above definitions/contents  appear in the book ,""An Introductory course in Differential Equations "" by D.A Murray, on the chapter titled ""Singular Solutions.""","p \frac{dy}{dx}. f(x,y,p)=0 \phi(x,y,c) f c \phi(x,y,c)= 0, c' c c c \phi(x,y,c)=0, \frac{\partial \phi}{\partial c}=0, c c c \phi(x,y,c)= 0, c. p F=0, F \phi(x,y,c)= 0, c x y. x y, \phi(x,y,c)=0 c, c \phi, c \phi(x,y,c)= 0, \frac{\partial \phi}{\partial c}=0. c \phi(x,y,c)= 0, c p f(x, y, p) = 0, \phi (x, y, c)= 0,  p f(x,y,p)= 0, \frac{\partial f}{\partial p}=0. p f(x, y, p)= 0 p. c p c p. c p, p c c c","['ordinary-differential-equations', 'differential-geometry', 'definition', 'envelope']"
17,Differential equation $(1-x)(y')^2+(y-x)y'+y=0.$,Differential equation,(1-x)(y')^2+(y-x)y'+y=0.,"I have the following problem. Find all differentiable on $[0,1)$ non-negative functions $f$ , for which for any $u\in(0,1)$ the tangent line to graph of $f$ in the point $(u,f(u))$ intersects the lines $y=0$ and $x=1$ in the points $A$ and $B$ respectively and $x_{A}=y_{B}$ . Also, $f'(0)=0$ . Easy to show that our function it's a root of the equation $$y=xy'-\frac{y'^2}{1+y'}$$ or $$(1-x)(y')^2+(y-x)y'+y=0.$$ My question: is there some way to solve this equation directly? Also, I assumed that the graph of $f$ is parabola with focus $F\left(\frac{1}{2},\frac{1}{2}\right)$ and directrix $y=x-1,$ which gives an equation of the parabola: $$(x+y)^2=4y,$$ which gives a solution of our equation. My second question: is there some better way to get this parabola? Thank you!","I have the following problem. Find all differentiable on non-negative functions , for which for any the tangent line to graph of in the point intersects the lines and in the points and respectively and . Also, . Easy to show that our function it's a root of the equation or My question: is there some way to solve this equation directly? Also, I assumed that the graph of is parabola with focus and directrix which gives an equation of the parabola: which gives a solution of our equation. My second question: is there some better way to get this parabola? Thank you!","[0,1) f u\in(0,1) f (u,f(u)) y=0 x=1 A B x_{A}=y_{B} f'(0)=0 y=xy'-\frac{y'^2}{1+y'} (1-x)(y')^2+(y-x)y'+y=0. f F\left(\frac{1}{2},\frac{1}{2}\right) y=x-1, (x+y)^2=4y,","['ordinary-differential-equations', 'conic-sections']"
18,$\partial^2_{\phi}f(\phi) +(A+B\phi+C\phi^2)f(\phi)=0$,,\partial^2_{\phi}f(\phi) +(A+B\phi+C\phi^2)f(\phi)=0,"I have the differential equation $$ \partial^2_{\phi}f(\phi) +(A+B\phi+C\phi^2)f(\phi)=0 $$ where $A, B, C\in\mathbb{C}$ and $\phi\in[0,2\pi]$ . I need the function to be antiperiodic, $f(\phi)=-f(\phi+2\pi)$ . Under what conditions for $A,B,C$ does a solution exist? Is there a name for this family of differential equations. Are there analitical solutions?","I have the differential equation where and . I need the function to be antiperiodic, . Under what conditions for does a solution exist? Is there a name for this family of differential equations. Are there analitical solutions?","
\partial^2_{\phi}f(\phi) +(A+B\phi+C\phi^2)f(\phi)=0
 A, B, C\in\mathbb{C} \phi\in[0,2\pi] f(\phi)=-f(\phi+2\pi) A,B,C",['ordinary-differential-equations']
19,Gronwall inequality for a $2$-dimensional system of linear differential inequalities,Gronwall inequality for a -dimensional system of linear differential inequalities,2,"Let $$z(t)=\begin{pmatrix}x(t) \\ y(t)\end{pmatrix}~,~ A=\begin{pmatrix}0 & -\beta \\ \alpha & -(\alpha + \beta)\end{pmatrix}$$ satisfies the following system of linear differential inequalities $$z'(t) \leq A\,z(t)$$ where $\alpha, \beta > 0$ are fixed constants, and $x(t), y(t) \in  [0,\infty)$ , I am wondering if there is a vectorized version of Gronwall lemma which allows me to deduce that $$z(t)\leq \mathrm{e}^{At}\,z(0).$$ It seems hard to find a literature which addresses precisely my question. Any help (including referring to precise literatures) will be greatly appreciated! Side note: by writing $u \leq v$ for $u,v \in \mathbb{R}^2_+$ I mean that each component of $u$ is less than or equal to the corresponding component of $v$ . Also (and unfortunately), the off-diagonal entries of $A$ are not all non-negative, which means that the discussion mentioned in this post may not be applicable...","Let satisfies the following system of linear differential inequalities where are fixed constants, and , I am wondering if there is a vectorized version of Gronwall lemma which allows me to deduce that It seems hard to find a literature which addresses precisely my question. Any help (including referring to precise literatures) will be greatly appreciated! Side note: by writing for I mean that each component of is less than or equal to the corresponding component of . Also (and unfortunately), the off-diagonal entries of are not all non-negative, which means that the discussion mentioned in this post may not be applicable...","z(t)=\begin{pmatrix}x(t) \\ y(t)\end{pmatrix}~,~ A=\begin{pmatrix}0 & -\beta \\ \alpha & -(\alpha + \beta)\end{pmatrix} z'(t) \leq A\,z(t) \alpha, \beta > 0 x(t), y(t) \in  [0,\infty) z(t)\leq \mathrm{e}^{At}\,z(0). u \leq v u,v \in \mathbb{R}^2_+ u v A","['ordinary-differential-equations', 'inequality']"
20,"My friend starts at $(1,0)$ and moves up. I start at $(0,0)$ and continuously approach them at the same speed. What's the equation of my movement?",My friend starts at  and moves up. I start at  and continuously approach them at the same speed. What's the equation of my movement?,"(1,0) (0,0)","My friend starts at $(1,0)$ and moves up. I start at $(0,0)$ and continuously approach them at the same speed. What's the equation of my movement? Furthermore, let's say my equation of my movement is $f(x)$ . Then, what is $\int_0^1 f(x)dx$ ? I've determined that the equations can be determined by the following system of differential equations, where $x$ and $y$ represent coordinates and $t$ represents time: $$\frac{dx}{dt} = \cos\left(\arctan\left(\frac{t-y}{1-x}\right)\right)$$ $$\frac{dy}{dt} = \sin\left(\arctan\left(\frac{t-y}{1-x}\right)\right)$$ $$t_0 = 0, x_0 = 0, y_0 = 0$$ However, I do not know how to solve these equations. I am aware that $\frac{dy}{dx} = \frac{t-y}{1-x}$ , but am unable to find $t$ in terms of $x$ and $y$ . Here is approximately the graph of my movement in a simulation:","My friend starts at and moves up. I start at and continuously approach them at the same speed. What's the equation of my movement? Furthermore, let's say my equation of my movement is . Then, what is ? I've determined that the equations can be determined by the following system of differential equations, where and represent coordinates and represents time: However, I do not know how to solve these equations. I am aware that , but am unable to find in terms of and . Here is approximately the graph of my movement in a simulation:","(1,0) (0,0) f(x) \int_0^1 f(x)dx x y t \frac{dx}{dt} = \cos\left(\arctan\left(\frac{t-y}{1-x}\right)\right) \frac{dy}{dt} = \sin\left(\arctan\left(\frac{t-y}{1-x}\right)\right) t_0 = 0, x_0 = 0, y_0 = 0 \frac{dy}{dx} = \frac{t-y}{1-x} t x y","['integration', 'ordinary-differential-equations']"
21,"Show that the differential equation $\frac{dy}{dx}=\sqrt y,y=0$ has non-unique solution.",Show that the differential equation  has non-unique solution.,"\frac{dy}{dx}=\sqrt y,y=0","Show that the differential equation $\frac{dy}{dx}=\sqrt y,y(0)=0$ has non-unique solution. To solve this problem I used Lipschitz theorem. My solution goes like this: Let $f$ be continuous in an unbounded domain $$D:a\leq x\leq b,-\infty\leq y\leq \infty.$$ Let $f$ satisfy a Lipschitz condition in $D$ .  That is, assume $\exists k\in\Bbb R$ such that $|f(x,y_1)-f(x,y_2)|\leq k|y_1-y_2|$ for all $(x,y_1),(x,y_2)\in D.$ Then the differential equation $\frac{dy}{dx}=f(x,y)$ and $y(x_0)=y_0$ ( where $(x_0,y_0)\in D$ ) has a unique solution in $[a,b].$ ( Lipschitz Theorem) Now, here, we have $f(x,y)=\sqrt y$ and $\frac{\partial f}{\partial y}=\frac {1}{2\sqrt y}.$ We see that $f$ is continuous everywhere, but $\frac{\partial f}{\partial y}=\frac {1}{2\sqrt y},$ is not continuous at $y=0$ and also not bounded at $y=0$ and so, Lipschitz condition is not satisfied( since $f$ is said to satisfy a Lipschitz condition with respect to $y$ in a domain $D$ iff $\frac{\partial f}{\partial y}$ is bounded). Thus, we can conclude that  the given differential equation $\frac{dy}{dx}=\sqrt y$ and $y(0)=0$ has no unique solution. But I am confused as I studied Picard's Theorem as well which goes like this: The differential equation $\frac{dy}{dx}=f(x,y)$ and $y(x_0)=y_0$ has a unique solution on $|x-x_0|\leq h$ , if both $f$ and $\frac{\partial f}{\partial y}$ are continuous in the domain $D=\{(x,y):|x-x_0|\leq h,|y-y_0|\leq k,h,k>0\}$ If we use this theorem to conclude the above assertion in the question is true, then the reasoning would go like this: Here, $\frac{\partial f}{\partial y}$ is not continuous in the domain $D$ if $(0,y)\in D.$ Thus, Picard's theorem won't be satisfied if $(0,y) \in D.$ But according to Picard's theorem, we have: the differential equation $\frac{dy}{dx}=\sqrt y$ and $y(0)=0$ has a unique solution on $|x|\leq h(,h>0)$ , if both $f$ and $\frac{\partial f}{\partial y}$ are continuous in the domain $D=\{(x,y):|x|\leq h,|y|\leq k,h,k>0\}.$ But, $\frac{\partial f}{\partial y}$ is not continuous in $D$ as $(0,y)\in D.$ So, the differential equation $\frac{dy}{dx}=\sqrt y,y(0)=0$ has non-unique solution. But I am confused whether both of my solutions (one using Lipschitz theorem and the other one using Picard's theorem) is valid or not? I think, Lipschitz theorem is more appropriate to solve these sort of problems as Picard's theorem, gurantees a unique solution in a particular or (better say,) a bounded domain but the Lipschitz theorem gurantees the existence of a unique solution, in an unbounded domain( Is it ?) But, in this case what I did was that: I showed both the Lipschitz theorem and the Picard's theorem are not satisfied for the set of equations : $\frac{dy}{dx}=\sqrt y,y(0)=0.$ So, in general, if conditions of  Lipschitz's theorem  are not satisfied, then can we directly conclude that a set of equations, say: $\frac{dy}{dx}=f(x,y)$ and $y(x_0)=y_0$ (where $(x_0,y_0)$ is in the domain of $f$ ) do not have a unique solution in the domain of $f,$ just as the thing was with this case? Then, the usage of Picard's theorem further is unnecessary(here), isn't it?","Show that the differential equation has non-unique solution. To solve this problem I used Lipschitz theorem. My solution goes like this: Let be continuous in an unbounded domain Let satisfy a Lipschitz condition in .  That is, assume such that for all Then the differential equation and ( where ) has a unique solution in ( Lipschitz Theorem) Now, here, we have and We see that is continuous everywhere, but is not continuous at and also not bounded at and so, Lipschitz condition is not satisfied( since is said to satisfy a Lipschitz condition with respect to in a domain iff is bounded). Thus, we can conclude that  the given differential equation and has no unique solution. But I am confused as I studied Picard's Theorem as well which goes like this: The differential equation and has a unique solution on , if both and are continuous in the domain If we use this theorem to conclude the above assertion in the question is true, then the reasoning would go like this: Here, is not continuous in the domain if Thus, Picard's theorem won't be satisfied if But according to Picard's theorem, we have: the differential equation and has a unique solution on , if both and are continuous in the domain But, is not continuous in as So, the differential equation has non-unique solution. But I am confused whether both of my solutions (one using Lipschitz theorem and the other one using Picard's theorem) is valid or not? I think, Lipschitz theorem is more appropriate to solve these sort of problems as Picard's theorem, gurantees a unique solution in a particular or (better say,) a bounded domain but the Lipschitz theorem gurantees the existence of a unique solution, in an unbounded domain( Is it ?) But, in this case what I did was that: I showed both the Lipschitz theorem and the Picard's theorem are not satisfied for the set of equations : So, in general, if conditions of  Lipschitz's theorem  are not satisfied, then can we directly conclude that a set of equations, say: and (where is in the domain of ) do not have a unique solution in the domain of just as the thing was with this case? Then, the usage of Picard's theorem further is unnecessary(here), isn't it?","\frac{dy}{dx}=\sqrt y,y(0)=0 f D:a\leq x\leq b,-\infty\leq y\leq \infty. f D \exists k\in\Bbb R |f(x,y_1)-f(x,y_2)|\leq k|y_1-y_2| (x,y_1),(x,y_2)\in D. \frac{dy}{dx}=f(x,y) y(x_0)=y_0 (x_0,y_0)\in D [a,b]. f(x,y)=\sqrt y \frac{\partial f}{\partial y}=\frac {1}{2\sqrt y}. f \frac{\partial f}{\partial y}=\frac {1}{2\sqrt y}, y=0 y=0 f y D \frac{\partial f}{\partial y} \frac{dy}{dx}=\sqrt y y(0)=0 \frac{dy}{dx}=f(x,y) y(x_0)=y_0 |x-x_0|\leq h f \frac{\partial f}{\partial y} D=\{(x,y):|x-x_0|\leq h,|y-y_0|\leq k,h,k>0\} \frac{\partial f}{\partial y} D (0,y)\in D. (0,y) \in D. \frac{dy}{dx}=\sqrt y y(0)=0 |x|\leq h(,h>0) f \frac{\partial f}{\partial y} D=\{(x,y):|x|\leq h,|y|\leq k,h,k>0\}. \frac{\partial f}{\partial y} D (0,y)\in D. \frac{dy}{dx}=\sqrt y,y(0)=0 \frac{dy}{dx}=\sqrt y,y(0)=0. \frac{dy}{dx}=f(x,y) y(x_0)=y_0 (x_0,y_0) f f,","['ordinary-differential-equations', 'solution-verification']"
22,Proving the solution of the IVP is positive for all $t$,Proving the solution of the IVP is positive for all,t,"I have the following problem: \begin{equation} \frac{d x}{d t}=-\frac{x^3}{1-t} \quad x(0)=x_0>0 \end{equation} How can I show that $x(t)>0$ for $t \in\left[0, \alpha\right)$ , where $\alpha \leq 1$ ? I have some ideas, but have not been able to use them to write/reach a proper conclusion. We can study the sign of $\frac{d x}{d t}$ to conclude that it is negative in the interval $\left[0, \alpha\right)$ , thus implying that $x(t)$ should be decreasing in the same interval. Moreover, from the initial condition, we know that $x(t)$ must be positive in a neighborhood of $x_{0}$ . How can I combine these facts together with the theorem of existence and uniqueness to show that $x(t)$ can never reach zero nor attain negative values? Thanks in advance, Lucas","I have the following problem: How can I show that for , where ? I have some ideas, but have not been able to use them to write/reach a proper conclusion. We can study the sign of to conclude that it is negative in the interval , thus implying that should be decreasing in the same interval. Moreover, from the initial condition, we know that must be positive in a neighborhood of . How can I combine these facts together with the theorem of existence and uniqueness to show that can never reach zero nor attain negative values? Thanks in advance, Lucas","\begin{equation}
\frac{d x}{d t}=-\frac{x^3}{1-t} \quad x(0)=x_0>0
\end{equation} x(t)>0 t \in\left[0, \alpha\right) \alpha \leq 1 \frac{d x}{d t} \left[0, \alpha\right) x(t) x(t) x_{0} x(t)","['calculus', 'ordinary-differential-equations', 'derivatives']"
23,Is my method of approximation correct to solve this question?,Is my method of approximation correct to solve this question?,,"(I hope this one to be an appropriate question over StackExchange platform.) Given $y = \dfrac{\sqrt{1 + 2x} \sqrt[4]{1 + 4x}}{\sqrt[3]{1 + 3x} \sqrt[5]{1 + 5x} \sqrt[7]{1 + 7x}}$ , to find $y'(0)$ . $$y = (1 + 2x)^{\frac{1}{2}}(1 + 4x)^{\frac{1}{4}}(1 + 3x)^{-\frac{1}{3}}(1 + 5x)^{-\frac{1}{5}}(1 + 7x)^{-\frac{1}{7}}$$ Before differentiating $y$ , $lim_{x \to 0} y$ is obtained using the method of binomial expansion and the powers of $x$ greater than $1$ have been neglected. Reason to neglect: as $x$ tends to $0$ , the denominator of $x$ is on the increasing side, hence raising $x$ to higher power (say $a$ ) lowers its value significantly so that the expression ( $x^a$ ) tends more towards $\texttt{zero}$ . Having said that, $$\lim_{x \to 0} y = (1 + x + ...x^2 + ...)(1 + x + 0)(1 - x + ...x^2 + ...)(1 - x + 0)(1 - x + 0) = (1 + x)^2(1 - x)^3$$ Writing more casually: $$\lim_{x \to 0} y = (1 + 2x + ...)(1 - 3x + 0) \approxeq 1 + (2 - 3)x + 0 = 1 - x$$ So now on differentiating $y$ at $x = 0$ , it is expected to get: $y'(0) = 0 - 1$ , i.e., $y'(0) = \mathbf{- 1}$ . I duly acknowledge that this method is not an ideal method but I want to enquire whether this type of inference will yield correct answers in most cases or not and whether (all) the inferences used/mentioned here are appropriate. Also tell me how do you feel about this method of solving. Thank you!","(I hope this one to be an appropriate question over StackExchange platform.) Given , to find . Before differentiating , is obtained using the method of binomial expansion and the powers of greater than have been neglected. Reason to neglect: as tends to , the denominator of is on the increasing side, hence raising to higher power (say ) lowers its value significantly so that the expression ( ) tends more towards . Having said that, Writing more casually: So now on differentiating at , it is expected to get: , i.e., . I duly acknowledge that this method is not an ideal method but I want to enquire whether this type of inference will yield correct answers in most cases or not and whether (all) the inferences used/mentioned here are appropriate. Also tell me how do you feel about this method of solving. Thank you!",y = \dfrac{\sqrt{1 + 2x} \sqrt[4]{1 + 4x}}{\sqrt[3]{1 + 3x} \sqrt[5]{1 + 5x} \sqrt[7]{1 + 7x}} y'(0) y = (1 + 2x)^{\frac{1}{2}}(1 + 4x)^{\frac{1}{4}}(1 + 3x)^{-\frac{1}{3}}(1 + 5x)^{-\frac{1}{5}}(1 + 7x)^{-\frac{1}{7}} y lim_{x \to 0} y x 1 x 0 x x a x^a \texttt{zero} \lim_{x \to 0} y = (1 + x + ...x^2 + ...)(1 + x + 0)(1 - x + ...x^2 + ...)(1 - x + 0)(1 - x + 0) = (1 + x)^2(1 - x)^3 \lim_{x \to 0} y = (1 + 2x + ...)(1 - 3x + 0) \approxeq 1 + (2 - 3)x + 0 = 1 - x y x = 0 y'(0) = 0 - 1 y'(0) = \mathbf{- 1},"['calculus', 'ordinary-differential-equations', 'limits', 'functions', 'derivatives']"
24,Differentiation with respect to a ratio,Differentiation with respect to a ratio,,"I have a function: $y= 35(α/β)^4-84(α/β)^5+70(α/β)^6-20(α/β)^7$ Is it legitimate to say $x = α/β$ , and therefore the derivative with respect to $x$ is: $140(α/β)^3-420(α/β)^4+420(α/β)^5-140(α/β)^6$ Noting that alpha is a variable and beta is a constant value, and their ratio (and therefore the value of x) in this specific problem is a value between 0 and 1, where 0 is the start of a rotation, and 1 is the end of the rotation. Sorry that this is super basic, I did this work well over a decade ago, its a long time since I last did any calculus and I feel like I am losing my mind looking back at this. Slapping it into Wolfram Alpha suggests I was correct, but I'm not sure I trust it or myself! :D","I have a function: Is it legitimate to say , and therefore the derivative with respect to is: Noting that alpha is a variable and beta is a constant value, and their ratio (and therefore the value of x) in this specific problem is a value between 0 and 1, where 0 is the start of a rotation, and 1 is the end of the rotation. Sorry that this is super basic, I did this work well over a decade ago, its a long time since I last did any calculus and I feel like I am losing my mind looking back at this. Slapping it into Wolfram Alpha suggests I was correct, but I'm not sure I trust it or myself! :D",y= 35(α/β)^4-84(α/β)^5+70(α/β)^6-20(α/β)^7 x = α/β x 140(α/β)^3-420(α/β)^4+420(α/β)^5-140(α/β)^6,"['ordinary-differential-equations', 'derivatives']"
25,How to solve the given differential equation $xg(f(x))f'(g(x))g'(x)= f(g(x))g'(f(x))f'(x)$?,How to solve the given differential equation ?,xg(f(x))f'(g(x))g'(x)= f(g(x))g'(f(x))f'(x),"Suppose $f$ and $g$ are differentiable functions such that $\forall x\in\Bbb R$ and $f(x),g(x)>0$ , $$xg(f(x))f'(g(x))g'(x)= f(g(x))g'(f(x))f'(x)$$ Also, $$\int_0^x f(g(t)) \,dt= \frac{1-e^{-2x}}{2}$$ If $g(f(0))=1$ what is $f(g(0))+g(f(0))$ ? This is absolutely a new kind of problem to me, so my tries just include trying to put $x=0$ and trying to find any pattern of sorts but it isn't quite getting me anywhere, and tried differentiating the given integral and I get $f(g(x))g'(x)=e^{-2x}$ . What is the proper way to solve this?","Suppose and are differentiable functions such that and , Also, If what is ? This is absolutely a new kind of problem to me, so my tries just include trying to put and trying to find any pattern of sorts but it isn't quite getting me anywhere, and tried differentiating the given integral and I get . What is the proper way to solve this?","f g \forall x\in\Bbb R f(x),g(x)>0 xg(f(x))f'(g(x))g'(x)= f(g(x))g'(f(x))f'(x) \int_0^x f(g(t)) \,dt= \frac{1-e^{-2x}}{2} g(f(0))=1 f(g(0))+g(f(0)) x=0 f(g(x))g'(x)=e^{-2x}","['calculus', 'ordinary-differential-equations', 'definite-integrals']"
26,Boundary conditions for weight function in weak form of an ODE or PDE,Boundary conditions for weight function in weak form of an ODE or PDE,,"I am really quite confused by how to pick a weight function's boundary conditions when evaluating the weak form of a differential equation. For example, given this ODE in the domain $0<x<1$ : $$u'''' + \frac{2}{x} u''' = 0$$ where $u(0) = u(1) = 0$ (Dirichlet BCs) and $u'(0) = u'(1) = 0$ (Neumann BCs), we pick a weight function $w(x)$ with the exact same Dirichlet and Neumann BCs , i.e. $w(0) = w(1) = w'(0) = w'(1) = 0$ , such that the boundary terms from subsequent IBP all vanish . However, when given this ODE in the domain $-1<x<1$ : $$xu''+2u'+2u = 0$$ with $u(1) = 0$ and $u(-1) = -2$ , we instead pick an arbitrary weight function such that $w(1) = w(-1) = 0$ ( not the same as the Dirichlet BCs of $u$ ) such that the boundary term from the subsequent IBP vanishes . Lastly, given Poisson's equation (a PDE): $$- \nabla^2u = f$$ with $$ \begin{aligned} u=0 & \text { on } S_g, \\ \nabla u \cdot \boldsymbol{n}=h & \text { on } S_h, \end{aligned} $$ where $S_g$ and $S_h$ cover the entire boundary but do not overlap, the weight function we pick is 0 on $S_g$ (i.e. same Dirichlet BC but not the same Neumann BC ). This leaves us with the following weak form: $$ \int_V \nabla w \cdot \nabla u d V=\int_V w f d V+\int_{S_h} w h d S  $$ where now the boundary term from IBP hasn't fully vanished (since we only picked $w=0$ on $S_g$ and not $S$ entirely). So I can't seem to figure out how to pick the boundary conditions of the weight function $w$ . E.g. in the case of the two ODEs I described, we pick the BCs of $w$ such that the boundary terms from IBP vanish (i.e. are equal to 0). Whereas with the PDE, the weight function we pick has boundary conditions such that the boundary term (surface integral) doesn't vanish. Is there a different approach to ODEs and PDEs? I would very much appreciate someone describing the general rules to follow with regards to the boundary conditions of $w$ .","I am really quite confused by how to pick a weight function's boundary conditions when evaluating the weak form of a differential equation. For example, given this ODE in the domain : where (Dirichlet BCs) and (Neumann BCs), we pick a weight function with the exact same Dirichlet and Neumann BCs , i.e. , such that the boundary terms from subsequent IBP all vanish . However, when given this ODE in the domain : with and , we instead pick an arbitrary weight function such that ( not the same as the Dirichlet BCs of ) such that the boundary term from the subsequent IBP vanishes . Lastly, given Poisson's equation (a PDE): with where and cover the entire boundary but do not overlap, the weight function we pick is 0 on (i.e. same Dirichlet BC but not the same Neumann BC ). This leaves us with the following weak form: where now the boundary term from IBP hasn't fully vanished (since we only picked on and not entirely). So I can't seem to figure out how to pick the boundary conditions of the weight function . E.g. in the case of the two ODEs I described, we pick the BCs of such that the boundary terms from IBP vanish (i.e. are equal to 0). Whereas with the PDE, the weight function we pick has boundary conditions such that the boundary term (surface integral) doesn't vanish. Is there a different approach to ODEs and PDEs? I would very much appreciate someone describing the general rules to follow with regards to the boundary conditions of .","0<x<1 u'''' + \frac{2}{x} u''' = 0 u(0) = u(1) = 0 u'(0) = u'(1) = 0 w(x) w(0) = w(1) = w'(0) = w'(1) = 0 -1<x<1 xu''+2u'+2u = 0 u(1) = 0 u(-1) = -2 w(1) = w(-1) = 0 u - \nabla^2u = f 
\begin{aligned}
u=0 & \text { on } S_g, \\
\nabla u \cdot \boldsymbol{n}=h & \text { on } S_h,
\end{aligned}
 S_g S_h S_g 
\int_V \nabla w \cdot \nabla u d V=\int_V w f d V+\int_{S_h} w h d S 
 w=0 S_g S w w w","['ordinary-differential-equations', 'partial-differential-equations', 'boundary-value-problem']"
27,How to know if a Numerical method gives an exact solution?,How to know if a Numerical method gives an exact solution?,,"Consider the following 2 step numerical method $y_{n+1}-y_n=\frac{h}{2}(3f_n -f_{n-1})$ I proved that this method is convergent but the problem asked Show that for any step size h, the method gives exact solutions to the initial value problems $y'=ax+b$ , with $y(x_0)=y_0$ ( $a,b \in \mathbb{R}$ ) provided that we choose $y(x_1)=y_1$ Well the exact solution is $y=\frac{a}{2}x²+bx+c$ . How to show that the numerical method gives an exact result? I tried finding $y_2=y(x_0+2h)$ in the numerical and the exact method and they both gave me the same answer $y_2= \frac{a}{2}x_0²+ bx_0+ 2ahx_0+ 2bh +2ah²+c$ Is this enough? Or how do I solve such a question?","Consider the following 2 step numerical method I proved that this method is convergent but the problem asked Show that for any step size h, the method gives exact solutions to the initial value problems , with ( ) provided that we choose Well the exact solution is . How to show that the numerical method gives an exact result? I tried finding in the numerical and the exact method and they both gave me the same answer Is this enough? Or how do I solve such a question?","y_{n+1}-y_n=\frac{h}{2}(3f_n -f_{n-1}) y'=ax+b y(x_0)=y_0 a,b \in \mathbb{R} y(x_1)=y_1 y=\frac{a}{2}x²+bx+c y_2=y(x_0+2h) y_2= \frac{a}{2}x_0²+ bx_0+ 2ahx_0+ 2bh +2ah²+c","['ordinary-differential-equations', 'numerical-methods']"
28,How to extend this Taylor series formulation to second order ODE?,How to extend this Taylor series formulation to second order ODE?,,"Let $y^{\prime}=f\left(  x,y\right) $ be first order ode with expansion around $x_{0}$ with initial conditions $y\left(  x_{0}\right)  =y_{0}$ , and where $f\left( x,y\right)  $ is analytic at $x_{0}$ then the solution to the ode in series expansion around $x_0$ by Taylor series is given by $$ y=y_{0}+\sum_{n=1}^{\infty}\frac{x^{n}}{n!}\left.  F_{n}\left(  x,y\right) \right\vert _{x=x_{0},y=y_{0}} $$ Where \begin{align*} F_{1}\left(  x,y\right)    & =f\left(  x,y\right)  \\ F_{n+1}\left(  x,y\right)    & =\frac{\partial F_{n}}{\partial x}+\left( \frac{\partial F_{n}}{\partial y}\right)  F_{1} \end{align*} What would be the equivalent formulation (if one exists) for a second order ode $y^{\prime\prime}=f\left(  x,y,y^{\prime}\right)  $ with initial conditions $y\left(  x_{0}\right)  =y_{0},y^{\prime}\left(  x_{0}\right) =y_{0}^{\prime}$ where it is assumed also that $f\left(  x,y,y^{\prime}\right)  $ is analytic at $x_{0}$ ? The above was taken from sympy ode solver here with reference for the above formula given as Travis W. Walker, Analytic power series technique for solving first-order differential equations, p.p 17, 18 But I could not find such book searching.  I could only find this page which references paper Walker, T.W.  “Analytic Power Series Technique for Solving First-Order Ordinary Differential Equations.”  MAA Rocky Mountain Section 2008 Meeting, Spearfish, South Dakota.  25-26 Apr. 2008. Where the above formulation is given. But all links from the above page are dead now. One advantage of this formulation over standard power series, is that is it easier to automate and to program. (no need to find recurrence relation for the $a_n$ for example). The $F_n$ expressions above do this job already. Is it possible to extend the above for second order ode by use of Taylor series expansion? We would need additional term for the partial derivative of $f(x,y,y')$ w.r.t. $y'$ ofcourse. Does any one knows of a reference where it is given in the above form? (compared to the standard power series form, for ordinary point).","Let be first order ode with expansion around with initial conditions , and where is analytic at then the solution to the ode in series expansion around by Taylor series is given by Where What would be the equivalent formulation (if one exists) for a second order ode with initial conditions where it is assumed also that is analytic at ? The above was taken from sympy ode solver here with reference for the above formula given as Travis W. Walker, Analytic power series technique for solving first-order differential equations, p.p 17, 18 But I could not find such book searching.  I could only find this page which references paper Walker, T.W.  “Analytic Power Series Technique for Solving First-Order Ordinary Differential Equations.”  MAA Rocky Mountain Section 2008 Meeting, Spearfish, South Dakota.  25-26 Apr. 2008. Where the above formulation is given. But all links from the above page are dead now. One advantage of this formulation over standard power series, is that is it easier to automate and to program. (no need to find recurrence relation for the for example). The expressions above do this job already. Is it possible to extend the above for second order ode by use of Taylor series expansion? We would need additional term for the partial derivative of w.r.t. ofcourse. Does any one knows of a reference where it is given in the above form? (compared to the standard power series form, for ordinary point).","y^{\prime}=f\left(  x,y\right)  x_{0} y\left(  x_{0}\right)  =y_{0} f\left(
x,y\right)   x_{0} x_0 
y=y_{0}+\sum_{n=1}^{\infty}\frac{x^{n}}{n!}\left.  F_{n}\left(  x,y\right)
\right\vert _{x=x_{0},y=y_{0}}
 \begin{align*}
F_{1}\left(  x,y\right)    & =f\left(  x,y\right)  \\
F_{n+1}\left(  x,y\right)    & =\frac{\partial F_{n}}{\partial x}+\left(
\frac{\partial F_{n}}{\partial y}\right)  F_{1}
\end{align*} y^{\prime\prime}=f\left(  x,y,y^{\prime}\right)   y\left(  x_{0}\right)  =y_{0},y^{\prime}\left(  x_{0}\right)
=y_{0}^{\prime} f\left(  x,y,y^{\prime}\right)   x_{0} a_n F_n f(x,y,y') y'","['ordinary-differential-equations', 'taylor-expansion', 'power-series']"
29,How I can solve this nonlinear second order ODE?,How I can solve this nonlinear second order ODE?,,"I am trying to solve a non-linear ODE. Let $g(x)$ is an arbitrary smooth real function, I want to solve the following equation $2f(x)f''(x) +4(f'(x))^2=g(x)$ for $f(x)$ . Multiplying $\dfrac32 f(x)$ , I obtain $(f^3(x))''=\dfrac32f(x)g(x)$ . In the following case I have solved the Eq. $\bullet$ If $g(x)$ identically be zero, then $f(x)=\sqrt[3]{ax+b}$ , for $a,b\in \mathbb{R}$ . $\bullet$ If $g(x)=\alpha f^2(x)$ for some constant $\alpha$ , then by taking $f^3(x)=F(x)$ , the equation can be solved. In fact, we have $F''(x)=\dfrac32 \alpha F(x)$ . But, in general case of $g(x)$ , I do not know how solve the equation. Any hint is highly appreciated.","I am trying to solve a non-linear ODE. Let is an arbitrary smooth real function, I want to solve the following equation for . Multiplying , I obtain . In the following case I have solved the Eq. If identically be zero, then , for . If for some constant , then by taking , the equation can be solved. In fact, we have . But, in general case of , I do not know how solve the equation. Any hint is highly appreciated.","g(x) 2f(x)f''(x) +4(f'(x))^2=g(x) f(x) \dfrac32 f(x) (f^3(x))''=\dfrac32f(x)g(x) \bullet g(x) f(x)=\sqrt[3]{ax+b} a,b\in \mathbb{R} \bullet g(x)=\alpha f^2(x) \alpha f^3(x)=F(x) F''(x)=\dfrac32 \alpha F(x) g(x)",['ordinary-differential-equations']
30,How to manipulate differential equation into Weierstrass P form?,How to manipulate differential equation into Weierstrass P form?,,"I have the diffeq $y'^2=\frac23 y^3+\alpha$ for some $\alpha\in\mathbb R$ . This has two solutions in the form of a Weierstrass P elliptic function which are $$6^{1/3}\wp\left(\frac{x\pm C}{6^{1/3}};0, \alpha\right)$$ The latter two arguments make sense, since they are just the $g$ constants in the weierstrass diffeq $y'^2=4y^3-g_2y-g_3$ , but I am having trouble getting the $6^{1/3}$ factors and the main argument. What I attempted to do was use the substitution $u(x)=6^{1/3}y(x)$ . This generates the differential $dy=\frac{du}{6^{1/3}}$ which I can substitute into my diffeq to get $$\left(\frac{dy}{dx}\right)^2=\frac23 y^3+\alpha\Longleftrightarrow \left(\frac{du}{6^{1/3}dx}\right)^2=\frac23\left(\frac{u}{6^{1/3}}\right)^3+\alpha\Longleftrightarrow\frac{1}{6^{2/3}}u'^2=4u^3+\alpha$$ This is close, but not quite correct as there is an extra factor on the LHS. Any hints as to how to proceed? Thanks Alternatively, (since the diffeq is seperable) if anyone is able to reduce the integral (which is the square root of a cubic) down to an elliptic integral of the first kind and invert it, that works too, but I think this would take more work so I'm not too sure. Edit : Additionally, I have also tried solving the diffeq given here using the inverse Weierstrass P function $$\wp^{-1}(z; g_2, g_3)=\int_{\infty}^z\frac{dt}{\sqrt{4t^3-g_2t-g_3}}$$ which after seperation gives me something like $$\int\frac{du}{6^{1/3}\sqrt{4u^3+\alpha}}=\int\pm1dx$$ $$\wp^{-1}(u; 0, \alpha) = 6^{1/3}(C\pm x)\tag{???}$$ This is quite close but still not quite right since the factor of $6^{1/3}$ is not in the correct part of the equation. Furthermore, I'm not sure how to relate the bounds of the definite integral definition to the indefinite one (for example, there could be a negative I am missing or smth). Help would be appreciated!","I have the diffeq for some . This has two solutions in the form of a Weierstrass P elliptic function which are The latter two arguments make sense, since they are just the constants in the weierstrass diffeq , but I am having trouble getting the factors and the main argument. What I attempted to do was use the substitution . This generates the differential which I can substitute into my diffeq to get This is close, but not quite correct as there is an extra factor on the LHS. Any hints as to how to proceed? Thanks Alternatively, (since the diffeq is seperable) if anyone is able to reduce the integral (which is the square root of a cubic) down to an elliptic integral of the first kind and invert it, that works too, but I think this would take more work so I'm not too sure. Edit : Additionally, I have also tried solving the diffeq given here using the inverse Weierstrass P function which after seperation gives me something like This is quite close but still not quite right since the factor of is not in the correct part of the equation. Furthermore, I'm not sure how to relate the bounds of the definite integral definition to the indefinite one (for example, there could be a negative I am missing or smth). Help would be appreciated!","y'^2=\frac23 y^3+\alpha \alpha\in\mathbb R 6^{1/3}\wp\left(\frac{x\pm C}{6^{1/3}};0, \alpha\right) g y'^2=4y^3-g_2y-g_3 6^{1/3} u(x)=6^{1/3}y(x) dy=\frac{du}{6^{1/3}} \left(\frac{dy}{dx}\right)^2=\frac23 y^3+\alpha\Longleftrightarrow \left(\frac{du}{6^{1/3}dx}\right)^2=\frac23\left(\frac{u}{6^{1/3}}\right)^3+\alpha\Longleftrightarrow\frac{1}{6^{2/3}}u'^2=4u^3+\alpha \wp^{-1}(z; g_2, g_3)=\int_{\infty}^z\frac{dt}{\sqrt{4t^3-g_2t-g_3}} \int\frac{du}{6^{1/3}\sqrt{4u^3+\alpha}}=\int\pm1dx \wp^{-1}(u; 0, \alpha) = 6^{1/3}(C\pm x)\tag{???} 6^{1/3}","['ordinary-differential-equations', 'substitution', 'elliptic-functions']"
31,How does the integral $\int y''\cdot y'\text{ d}x$ work in the context of differential equations?,How does the integral  work in the context of differential equations?,\int y''\cdot y'\text{ d}x,"In this question, the accepted answer starts off solving the diffeq $(y'/y)'=-ay$ by substituting $u = \log y$ to get $u'' = -a e^{u}$ and then multiplying both sides with $u'$ and integrating both sides to get $$\frac{(u')^{2}}{2} = -a e^{u} + \frac{c_{1}}{2}$$ I am baffled at how to get $$\int u'' \cdot u' \text{ d}x=\frac{(u')^{2}}{2}$$ through the context of differentials. Typically, I can understand integrating say $u'(x)$ through differentials, where we have $$\int \frac{du}{dx}\text{ d}x = \int \text{ d}u = u+C$$ How do I get the same thing for integrating $u'' u'$ ? Obviously, I can see this through the chain rule where $$\frac{d}{dx}\left[\frac12 y'^2\right] = \frac12\cdot 2y'\cdot y'' = y'' y'$$ but I cannot reason how how the integral would work $$\int \frac{d^2u}{dx^2}\cdot \frac{du}{dx}\text{ d}x = \int \frac{d^2u}{dx^2} \text{ d}u?????$$ Help is appreciated! :)","In this question, the accepted answer starts off solving the diffeq by substituting to get and then multiplying both sides with and integrating both sides to get I am baffled at how to get through the context of differentials. Typically, I can understand integrating say through differentials, where we have How do I get the same thing for integrating ? Obviously, I can see this through the chain rule where but I cannot reason how how the integral would work Help is appreciated! :)",(y'/y)'=-ay u = \log y u'' = -a e^{u} u' \frac{(u')^{2}}{2} = -a e^{u} + \frac{c_{1}}{2} \int u'' \cdot u' \text{ d}x=\frac{(u')^{2}}{2} u'(x) \int \frac{du}{dx}\text{ d}x = \int \text{ d}u = u+C u'' u' \frac{d}{dx}\left[\frac12 y'^2\right] = \frac12\cdot 2y'\cdot y'' = y'' y' \int \frac{d^2u}{dx^2}\cdot \frac{du}{dx}\text{ d}x = \int \frac{d^2u}{dx^2} \text{ d}u?????,"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives']"
32,An explicit form for this differential equation.,An explicit form for this differential equation.,,So I came across this differential equation: $$xyy'-y^2=(x+y)^2 e^{-y/x}$$ And I managed to simplify it this way: First I multiplied by $\frac1{xy}$ : $$y'-\frac{y}{x}=\left(\frac{x}{y}+2+\frac{y}{x}\right) e^{-y/x}$$ Then substituted $v = \frac{y}{x}$ sub to get: $$\begin{split} v+xv'-v&=\left(\frac1v+2+v\right) e^{-v}\\ xvv'&=(1+2v+v^2) e^{-v}\\ xvv'&=(v+1)^2 e^{-v} \end{split}$$ Which is separable. I then proceeded to separate and integrate both sides to get $$\frac{-e^v}{v+1}\ + e^v=\ln⁡(x)+c$$ or $$\frac{e^v}{v+1}\ =\ln⁡(x)+c$$ Is there a way to get a more explicit equation? Is there something wrong that I did?,So I came across this differential equation: And I managed to simplify it this way: First I multiplied by : Then substituted sub to get: Which is separable. I then proceeded to separate and integrate both sides to get or Is there a way to get a more explicit equation? Is there something wrong that I did?,"xyy'-y^2=(x+y)^2 e^{-y/x} \frac1{xy} y'-\frac{y}{x}=\left(\frac{x}{y}+2+\frac{y}{x}\right) e^{-y/x} v = \frac{y}{x} \begin{split}
v+xv'-v&=\left(\frac1v+2+v\right) e^{-v}\\
xvv'&=(1+2v+v^2) e^{-v}\\
xvv'&=(v+1)^2 e^{-v}
\end{split} \frac{-e^v}{v+1}\ + e^v=\ln⁡(x)+c \frac{e^v}{v+1}\
=\ln⁡(x)+c",['ordinary-differential-equations']
33,Difficulty solving $4x^2y^3y''=x^2-y^4$,Difficulty solving,4x^2y^3y''=x^2-y^4,"Solve $4x^2y^3y''=x^2-y^4.$ I was trying to reduce the order of this nonlinear ODE or to transform it into a linear ODE, but I couldn't come up with a suitable substitution. I considered \begin{aligned}(y^4)''&=(4y^3y')'\\&=12y^2(y')^2+4y^3y''\\\implies 4y^3y''&=(y^4)''-12y^2(y')^2\\\implies 4x^2y^3y''&=x^2(y^4)''-12x^2y^2(y')^2\\&= x^2(y^4)''-3(x^2\cdot 4y^2\cdot (y')^2)\\&=x^2(y^4)''-3(x\cdot 2yy')^2\\&=x^2(y^4)''-3(x(y^2)')^2\end{aligned} on the LHS. If I took $u:=y^2,$ I would get $$x^2(u^2)''-3(xu')^2=x^2-u^2,$$ but that situation doesn't seem any better than the initial one. Is it possible to transform the given ODE into a linear one (not necessarily with constant coefficients)? I assume it should be possible as I found it in old materials which don't cover anything beyond linear equations and Riccati's equation, which we transformed into linear. However, I would appreciate any working method suggested.","Solve I was trying to reduce the order of this nonlinear ODE or to transform it into a linear ODE, but I couldn't come up with a suitable substitution. I considered on the LHS. If I took I would get but that situation doesn't seem any better than the initial one. Is it possible to transform the given ODE into a linear one (not necessarily with constant coefficients)? I assume it should be possible as I found it in old materials which don't cover anything beyond linear equations and Riccati's equation, which we transformed into linear. However, I would appreciate any working method suggested.","4x^2y^3y''=x^2-y^4. \begin{aligned}(y^4)''&=(4y^3y')'\\&=12y^2(y')^2+4y^3y''\\\implies 4y^3y''&=(y^4)''-12y^2(y')^2\\\implies 4x^2y^3y''&=x^2(y^4)''-12x^2y^2(y')^2\\&= x^2(y^4)''-3(x^2\cdot 4y^2\cdot (y')^2)\\&=x^2(y^4)''-3(x\cdot 2yy')^2\\&=x^2(y^4)''-3(x(y^2)')^2\end{aligned} u:=y^2, x^2(u^2)''-3(xu')^2=x^2-u^2,",['ordinary-differential-equations']
34,How to find appropriate discretization method for a continuous time domain state space model?,How to find appropriate discretization method for a continuous time domain state space model?,,"I have a dsp algorithm which is based on the below given state space model in the continuous-time domain $$ \begin{bmatrix}  \frac{\mathrm{d}\hat{\psi}_{r_{\alpha}}}{\mathrm{d}t} \\ \frac{\mathrm{d}\hat{\psi}_{r_{\beta}}}{\mathrm{d}t} \end{bmatrix} = \begin{bmatrix} -\frac{R_R}{L_L + L_M} & -p_p\cdot\omega_m \\ p_p\cdot\omega_m & -\frac{R_R}{L_L + L_M} \end{bmatrix} \cdot \begin{bmatrix} \hat{\psi}_{r_{\alpha}} \\ \hat{\psi}_{r_{\beta}} \end{bmatrix} + \begin{bmatrix} \frac{L_M\cdot R_R}{L_L + L_M} & 0 \\ 0 & \frac{L_M\cdot R_R}{L_L + L_M} \end{bmatrix} \cdot \begin{bmatrix} i_{s_{\alpha}} \\ i_{s_{\beta}} \end{bmatrix} $$ It is basically a model of a dynamic system, where the variables $i_{s_{\alpha}}$ , $i_{s_{\beta}}$ and $\omega_m$ are the inputs of the dynamic system and the $\hat{\psi}_{r_{\alpha}}$ , $\hat{\psi}_{r_{\beta}}$ are its outputs (the unmeasurable state variables of the system). This model is intended to be used as a state observer. I know that there are much more robust approaches for estimation of the unmeasurable state variables of a dynamic system but I would like to do a comparison between several methods. As far as the parameters of the state space model: $R_S = 7.400\cdot 10^{-3}\,\Omega$ , $R_R = 7.548\cdot 10^{-3}\,\Omega$ , $L_M = 4.265\cdot 10^{-3}\,\mathrm{H}$ , $L_L = 0.231\cdot 10^{-3}\,\mathrm{H}$ , $p_p = 3.0$ . For the software implementation purposes it is necessary to discretize the continuous-time domain model (sampling period is $T = 100\cdot 10^{-6}\,\mathrm{s}$ ). I have been thinking about the simple Euler method (i.e. the state transition matrix $\Psi = \mathbf{I} + \frac{(\mathbf{A}\cdot T)}{2!} + \frac{(\mathbf{A}\cdot T)^2}{3!} + \ldots$ approximated only via the first two terms of the series expansion) usage. But I have noticed that the above given system is probably much more complex for computation than it seems at first glance. I have calculated the eigenvalues of the system matrix (i.e. poles of the system) in respect to the input $\omega_m$ and I have found that the poles are pretty near to the stability boundary in the s-plane. If I transformed the system into the discrete form via the ZOH method I found that the system poles (in respect to the input $\omega_m$ ) are even nearly on the stability boundary. My question is how I can implement this system in a software (in the single precision floating point) to be sure that the solution (the system state variables) will be stable? The idea behind my question is that I have noticed that the poles of the discretized system (zero-order hold discretization) are very close to the unit circle boundary (the ""reserve"" is only a couple of ten thousandths). Source of my doubt is that in case I implement the discrete system in a control software it is from my point of view very good chance that due to the limited precision the poles can move outside the unit circle and the system can become very easily unstable.","I have a dsp algorithm which is based on the below given state space model in the continuous-time domain It is basically a model of a dynamic system, where the variables , and are the inputs of the dynamic system and the , are its outputs (the unmeasurable state variables of the system). This model is intended to be used as a state observer. I know that there are much more robust approaches for estimation of the unmeasurable state variables of a dynamic system but I would like to do a comparison between several methods. As far as the parameters of the state space model: , , , , . For the software implementation purposes it is necessary to discretize the continuous-time domain model (sampling period is ). I have been thinking about the simple Euler method (i.e. the state transition matrix approximated only via the first two terms of the series expansion) usage. But I have noticed that the above given system is probably much more complex for computation than it seems at first glance. I have calculated the eigenvalues of the system matrix (i.e. poles of the system) in respect to the input and I have found that the poles are pretty near to the stability boundary in the s-plane. If I transformed the system into the discrete form via the ZOH method I found that the system poles (in respect to the input ) are even nearly on the stability boundary. My question is how I can implement this system in a software (in the single precision floating point) to be sure that the solution (the system state variables) will be stable? The idea behind my question is that I have noticed that the poles of the discretized system (zero-order hold discretization) are very close to the unit circle boundary (the ""reserve"" is only a couple of ten thousandths). Source of my doubt is that in case I implement the discrete system in a control software it is from my point of view very good chance that due to the limited precision the poles can move outside the unit circle and the system can become very easily unstable.","
\begin{bmatrix} 
\frac{\mathrm{d}\hat{\psi}_{r_{\alpha}}}{\mathrm{d}t} \\
\frac{\mathrm{d}\hat{\psi}_{r_{\beta}}}{\mathrm{d}t}
\end{bmatrix}
=
\begin{bmatrix}
-\frac{R_R}{L_L + L_M} & -p_p\cdot\omega_m \\
p_p\cdot\omega_m & -\frac{R_R}{L_L + L_M}
\end{bmatrix}
\cdot
\begin{bmatrix}
\hat{\psi}_{r_{\alpha}} \\
\hat{\psi}_{r_{\beta}}
\end{bmatrix}
+
\begin{bmatrix}
\frac{L_M\cdot R_R}{L_L + L_M} & 0 \\
0 & \frac{L_M\cdot R_R}{L_L + L_M}
\end{bmatrix}
\cdot
\begin{bmatrix}
i_{s_{\alpha}} \\
i_{s_{\beta}}
\end{bmatrix}
 i_{s_{\alpha}} i_{s_{\beta}} \omega_m \hat{\psi}_{r_{\alpha}} \hat{\psi}_{r_{\beta}} R_S = 7.400\cdot 10^{-3}\,\Omega R_R = 7.548\cdot 10^{-3}\,\Omega L_M = 4.265\cdot 10^{-3}\,\mathrm{H} L_L = 0.231\cdot 10^{-3}\,\mathrm{H} p_p = 3.0 T = 100\cdot 10^{-6}\,\mathrm{s} \Psi = \mathbf{I} + \frac{(\mathbf{A}\cdot T)}{2!} + \frac{(\mathbf{A}\cdot T)^2}{3!} + \ldots \omega_m \omega_m","['ordinary-differential-equations', 'dynamical-systems', 'control-theory', 'discrete-time']"
35,"Nature of critical point $(0,0)$ for the given 2nd order ODE",Nature of critical point  for the given 2nd order ODE,"(0,0)","Consider the 2nd order ODE: $$\frac{d^2y}{d^2t}+2\alpha \frac{dy}{dt}+\beta^2 y=0$$ where $\alpha >\beta>0$ . Then find the nature of the critical point of the above ODE. First I put $\frac{dy}{dt}=x$ and $\frac{dx}{dt}=-2\alpha x-\beta^2y$ . Then $(0,0)$ is a critical point and the Jacobian matrix is $$J=\left(\begin{matrix} 1 & 0\\-2\alpha & -\beta^2\end{matrix}\right) $$ Roots of the characteristic equation of $J$ are $1$ and $-\beta^2$ , which are real and opposite sign. So, the critical point $(0,0)$ is a saddle point. Is it correct ?","Consider the 2nd order ODE: where . Then find the nature of the critical point of the above ODE. First I put and . Then is a critical point and the Jacobian matrix is Roots of the characteristic equation of are and , which are real and opposite sign. So, the critical point is a saddle point. Is it correct ?","\frac{d^2y}{d^2t}+2\alpha \frac{dy}{dt}+\beta^2 y=0 \alpha >\beta>0 \frac{dy}{dt}=x \frac{dx}{dt}=-2\alpha x-\beta^2y (0,0) J=\left(\begin{matrix} 1 & 0\\-2\alpha & -\beta^2\end{matrix}\right)  J 1 -\beta^2 (0,0)","['ordinary-differential-equations', 'systems-of-equations']"
36,When is $e^f$ convex for non-convex $f$?,When is  convex for non-convex ?,e^f f,"Assuming that $f(x)\in C^2(\mathbb{R})$ is non-convex, when is $e^{f(t)}$ convex? Clearly the function $\exp:\mathbb{R}\to(0,\infty)$ preserves convexity under composition: $$\frac{d^2}{dt^2} e^{f(t)}=e^f\left(f'^2+f''\right)\ge 0$$ whenever $f''\ge 0$ . But when does it create convexity when it previously did not exist? The question is equivalent to $f$ satisfying the differential inequality $$0>f''\ge-(f')^2.$$ Let $f'(t)=v(t)$ . Then the inequality reads $$0>v'\ge-v^2.$$","Assuming that is non-convex, when is convex? Clearly the function preserves convexity under composition: whenever . But when does it create convexity when it previously did not exist? The question is equivalent to satisfying the differential inequality Let . Then the inequality reads","f(x)\in C^2(\mathbb{R}) e^{f(t)} \exp:\mathbb{R}\to(0,\infty) \frac{d^2}{dt^2} e^{f(t)}=e^f\left(f'^2+f''\right)\ge 0 f''\ge 0 f 0>f''\ge-(f')^2. f'(t)=v(t) 0>v'\ge-v^2.","['calculus', 'ordinary-differential-equations', 'analysis', 'convex-analysis']"
37,Convergence of discrete eigenvalues of the heat equation,Convergence of discrete eigenvalues of the heat equation,,"Consider the eigenvalue problem associated with the heat equation \begin{equation} \phi''(x) = \lambda \phi(x), \qquad \phi(0)=\phi(1)=1. \end{equation} Whilst the eigenvalues can be calculated analytically, I want to calculate them numerically. To do this, consider the finite difference approximation \begin{equation} \frac{\phi^{i+1}-2\phi^i+\phi^{i-1}}{h^2}=\lambda \phi^i \end{equation} where $i=1,...,N+1$ and $h=1/N$ . This can be assembled into the matrix-vector form \begin{equation} \boldsymbol{A}\boldsymbol{\phi} = \lambda \boldsymbol{\phi} \end{equation} such that $\boldsymbol{\phi} = \big(\phi^2,...,\phi^j,...,\phi^N)^T$ and $\boldsymbol{A} $ is a tri-diagonal matrix of size $(N-1)\times (N-1)$ with $-2/h^2$ on the diagonals and $1/h^2$ on both the lower and upper diagonals. To find the eigenvalues, I use the $\texttt{eig}$ function in matlab, i.e. $\texttt{eig}(A).$ My issue is the following. As I increase $N$ (consequently decreasing the step size, $h$ ) my eigenvalues do not converge. I have attached a figure illustrating the eigenvalues for $N=100, N=200$ and $N=300$ . Why do the eigenvalues not converge? Is there something fundamentally wrong with my formulation, or is there a rational step that I'm missing that maps the eigenvalues calculated numerically to their continuous spectum? I have also found that my eigenvalues diverge like $1/h^2$ as I decrease $h$ .","Consider the eigenvalue problem associated with the heat equation Whilst the eigenvalues can be calculated analytically, I want to calculate them numerically. To do this, consider the finite difference approximation where and . This can be assembled into the matrix-vector form such that and is a tri-diagonal matrix of size with on the diagonals and on both the lower and upper diagonals. To find the eigenvalues, I use the function in matlab, i.e. My issue is the following. As I increase (consequently decreasing the step size, ) my eigenvalues do not converge. I have attached a figure illustrating the eigenvalues for and . Why do the eigenvalues not converge? Is there something fundamentally wrong with my formulation, or is there a rational step that I'm missing that maps the eigenvalues calculated numerically to their continuous spectum? I have also found that my eigenvalues diverge like as I decrease .","\begin{equation}
\phi''(x) = \lambda \phi(x), \qquad \phi(0)=\phi(1)=1.
\end{equation} \begin{equation}
\frac{\phi^{i+1}-2\phi^i+\phi^{i-1}}{h^2}=\lambda \phi^i
\end{equation} i=1,...,N+1 h=1/N \begin{equation}
\boldsymbol{A}\boldsymbol{\phi} = \lambda \boldsymbol{\phi}
\end{equation} \boldsymbol{\phi} = \big(\phi^2,...,\phi^j,...,\phi^N)^T \boldsymbol{A}  (N-1)\times (N-1) -2/h^2 1/h^2 \texttt{eig} \texttt{eig}(A). N h N=100, N=200 N=300 1/h^2 h","['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
38,Find particular solution of system of linear ODE,Find particular solution of system of linear ODE,,"I would like to solve the ODE system $$ \begin{pmatrix}x'(t)\\y'(t)\end{pmatrix}=\begin{pmatrix}0 & 12\\3 & 0\end{pmatrix}\begin{pmatrix}x(t)\\y(t)\end{pmatrix}-\begin{pmatrix}36\\9\end{pmatrix} $$ with initial conditions $x(0)=3$ and $2y(1)-x(1)=7$ . I already determined the fundamental matrix for the homogeneous problem to be $$ \begin{pmatrix}2e^{6t} & -2e^{-6t}\\e^{6t} & e^{-6t}\end{pmatrix}. $$ My problem is how to find a particular solution $x_p,y_p$ . Is it suitable to make the ansatz $x_p(t)=c_1$ and $y_p(t)=c_2$ for constants $c_1,c_2$ ? Then I get $x_p=y_p=3$ and the general solution $$ \begin{align} x(t)&=2C_1e^{6t}-2C_2e^{-6t}+3,\\ y(t)&=C_1e^{6t}+C_2e^{-6t}+3. \end{align} $$ Using the initial value conditions, my solution is $$ \begin{align} x(t)&=2e^6\left(e^{6t}-e^{-6t}\right)+3,\\ y(t)&=e^6\left(e^{6t}+e^{-6t}\right)+3. \end{align} $$","I would like to solve the ODE system with initial conditions and . I already determined the fundamental matrix for the homogeneous problem to be My problem is how to find a particular solution . Is it suitable to make the ansatz and for constants ? Then I get and the general solution Using the initial value conditions, my solution is","
\begin{pmatrix}x'(t)\\y'(t)\end{pmatrix}=\begin{pmatrix}0 & 12\\3 & 0\end{pmatrix}\begin{pmatrix}x(t)\\y(t)\end{pmatrix}-\begin{pmatrix}36\\9\end{pmatrix}
 x(0)=3 2y(1)-x(1)=7 
\begin{pmatrix}2e^{6t} & -2e^{-6t}\\e^{6t} & e^{-6t}\end{pmatrix}.
 x_p,y_p x_p(t)=c_1 y_p(t)=c_2 c_1,c_2 x_p=y_p=3 
\begin{align}
x(t)&=2C_1e^{6t}-2C_2e^{-6t}+3,\\
y(t)&=C_1e^{6t}+C_2e^{-6t}+3.
\end{align}
 
\begin{align}
x(t)&=2e^6\left(e^{6t}-e^{-6t}\right)+3,\\
y(t)&=e^6\left(e^{6t}+e^{-6t}\right)+3.
\end{align}
",['ordinary-differential-equations']
39,Help for generic logisitic population growth with variable harvesting problem,Help for generic logisitic population growth with variable harvesting problem,,"I am attempting to work through a common problem entailing variable harvesting of a population growing logistically. I am struggling to determine all the properties for stable harvesting. I will detail my work to show where I need help The given ODE is, $\frac{dN}{dt}=RN(1-\frac{N}{K})-HN$ Where, $N$ is the population $R$ is the instinsic growth rate of the population $K$ is the carrying capacity $H$ is relative portion of the population to be harvested The goal is to determine properties for $H$ that allow for equilibrium of the population. The given ODE has no method for finding a general solution as it's nonlinear. Instead, I choose a more pratical method by setting $K=1$ to represent the maximum relative carry capacity. This would in turn mean that any choices for $R$ and the resulting $N$ would be in decimal form to be fractional pieces of $K=1$ . This provides, $\frac{dN}{dt}=RN(1-\frac{N}{1})-HN=\frac{dN}{dt}=RN(1-N)-HN$ $\space$ Next I determine equilibrium $N_e$ through finding solutions to $\frac{dN_e}{dt}=0$ $\frac{dN_e}{dt}=RN_e(1-N_e)-HN_e=RN_e-RN_e^2-HN_e=N_e(R-RN_e-H)=0$ . Two solutions exist, $N_{e_1}=0$ and $N_{e_2}=\frac{R-H}{R}=1-\frac{H}{R}$ . An equilibrium point for a population must be greater than zero, otherwise the population is tending towards extinction. Thus, $N_{e_1}=0$ is an invalid solution and we must have $N_{e_2}=1-\frac{H}{R}>0$ . By solving inequality of $N_{e_2}$ for $H$ , we discover $H<R$ . Which is a logical property. If the relative portion of the population harvested $H$ was greater than the growth rate $R$ , the population would reach extinction. $\space$ The next step is to determine the maximum harvest which sustains $N_{e_2}$ . To do so, we employ the second derivative test. $\frac{d^2N}{dt^2}=\frac{d}{dt}[RN_{e_2}(1-N_{e_2})-HN_{e_2}]=R(1-2N_{e_2})-H=R[1-2(1-\frac{H}{R})]-H$ By the properties of the second derivative test, for a local maximum to exist, we must have $\frac{d^2N}{dt^2}<0$ . Thus, $R[1-2(1-\frac{H}{R})]-H<0$ $\space$ $\space$ This is where I fail to continue the problem. When solving the above inequality for $H$ I always find $H<R$ , which is the exact same property discovered earlier in the solution. There should be a unique maximum for $H$ . I'm unsure of where I made error in my logic or what other methods to employ. Help would be greatly appreciated. Thank you.","I am attempting to work through a common problem entailing variable harvesting of a population growing logistically. I am struggling to determine all the properties for stable harvesting. I will detail my work to show where I need help The given ODE is, Where, is the population is the instinsic growth rate of the population is the carrying capacity is relative portion of the population to be harvested The goal is to determine properties for that allow for equilibrium of the population. The given ODE has no method for finding a general solution as it's nonlinear. Instead, I choose a more pratical method by setting to represent the maximum relative carry capacity. This would in turn mean that any choices for and the resulting would be in decimal form to be fractional pieces of . This provides, Next I determine equilibrium through finding solutions to . Two solutions exist, and . An equilibrium point for a population must be greater than zero, otherwise the population is tending towards extinction. Thus, is an invalid solution and we must have . By solving inequality of for , we discover . Which is a logical property. If the relative portion of the population harvested was greater than the growth rate , the population would reach extinction. The next step is to determine the maximum harvest which sustains . To do so, we employ the second derivative test. By the properties of the second derivative test, for a local maximum to exist, we must have . Thus, This is where I fail to continue the problem. When solving the above inequality for I always find , which is the exact same property discovered earlier in the solution. There should be a unique maximum for . I'm unsure of where I made error in my logic or what other methods to employ. Help would be greatly appreciated. Thank you.",\frac{dN}{dt}=RN(1-\frac{N}{K})-HN N R K H H K=1 R N K=1 \frac{dN}{dt}=RN(1-\frac{N}{1})-HN=\frac{dN}{dt}=RN(1-N)-HN \space N_e \frac{dN_e}{dt}=0 \frac{dN_e}{dt}=RN_e(1-N_e)-HN_e=RN_e-RN_e^2-HN_e=N_e(R-RN_e-H)=0 N_{e_1}=0 N_{e_2}=\frac{R-H}{R}=1-\frac{H}{R} N_{e_1}=0 N_{e_2}=1-\frac{H}{R}>0 N_{e_2} H H<R H R \space N_{e_2} \frac{d^2N}{dt^2}=\frac{d}{dt}[RN_{e_2}(1-N_{e_2})-HN_{e_2}]=R(1-2N_{e_2})-H=R[1-2(1-\frac{H}{R})]-H \frac{d^2N}{dt^2}<0 R[1-2(1-\frac{H}{R})]-H<0 \space \space H H<R H,"['calculus', 'ordinary-differential-equations', 'mathematical-modeling']"
40,Compatibility of flow and inverse flow of a non-autonomous ODE,Compatibility of flow and inverse flow of a non-autonomous ODE,,"Suppose we are given a function $f:\mathbb{R}^d\times [0,T]\to\mathbb{R}^d$ , s.t. the ODE \begin{align} \dot{y}(t)&= f(y(t),t), \quad t\in[0,T]\newline       y(0)&= x \end{align} is well-posed, for any $x\in\mathbb{R}^d$ (e.g. $f$ is $\mathcal{C}^1$ with bounded derivative). Then we can define the flow of this ODE to be the map $[0,T]\times\mathbb{R}^d\ni(t,x)\mapsto \phi_t(x)\in\mathbb{R}^d$ . Moreover we can define the ODE reversed in time: \begin{align} \dot{z}(s)&= -f(z(s),T-s), \quad s\in[0,T]\newline       z(0)&= \xi \end{align} for any $y\in\mathbb{R}^d$ and a corresponding reverse flow $[0,T]\times\mathbb{R}^d\ni(s,x)\mapsto \psi_s(\xi)\in\mathbb{R}^d$ . I can manage to prove the identity \begin{equation} \psi_s(\phi_t(x)) = \phi_{t-s}(x) \end{equation} for any $0\leq s\leq t \leq T$ . Under what circumstances does $\phi_t(\psi_s(x)) = \phi_{t-s}(x)$ also hold? I am having trouble proving this (I tried showing, that both sides satisfy the same ODE, so I can conclude with uniqueness, but there I run into well-definedness issues). What I can prove however is \begin{equation} \phi_t(\psi_s(x)) = \psi_{s-t}(x) \end{equation} for any $0\leq t\leq s \leq T$ . Heuristically speaking it shouldn't matter, if I move backwards first and then forwards or vice versa. I read other posts on this website about it, and it seems to have something to do with whether the initial ODE is autonomous or not, but I just can't seem to wrap my head around it. Can someone help me?","Suppose we are given a function , s.t. the ODE is well-posed, for any (e.g. is with bounded derivative). Then we can define the flow of this ODE to be the map . Moreover we can define the ODE reversed in time: for any and a corresponding reverse flow . I can manage to prove the identity for any . Under what circumstances does also hold? I am having trouble proving this (I tried showing, that both sides satisfy the same ODE, so I can conclude with uniqueness, but there I run into well-definedness issues). What I can prove however is for any . Heuristically speaking it shouldn't matter, if I move backwards first and then forwards or vice versa. I read other posts on this website about it, and it seems to have something to do with whether the initial ODE is autonomous or not, but I just can't seem to wrap my head around it. Can someone help me?","f:\mathbb{R}^d\times [0,T]\to\mathbb{R}^d \begin{align}
\dot{y}(t)&= f(y(t),t), \quad t\in[0,T]\newline
      y(0)&= x
\end{align} x\in\mathbb{R}^d f \mathcal{C}^1 [0,T]\times\mathbb{R}^d\ni(t,x)\mapsto \phi_t(x)\in\mathbb{R}^d \begin{align}
\dot{z}(s)&= -f(z(s),T-s), \quad s\in[0,T]\newline
      z(0)&= \xi
\end{align} y\in\mathbb{R}^d [0,T]\times\mathbb{R}^d\ni(s,x)\mapsto \psi_s(\xi)\in\mathbb{R}^d \begin{equation}
\psi_s(\phi_t(x)) = \phi_{t-s}(x)
\end{equation} 0\leq s\leq t \leq T \phi_t(\psi_s(x)) = \phi_{t-s}(x) \begin{equation}
\phi_t(\psi_s(x)) = \psi_{s-t}(x)
\end{equation} 0\leq t\leq s \leq T","['real-analysis', 'ordinary-differential-equations']"
41,How is this nonlinear differential equation solved with hyperbolic functions? [closed],How is this nonlinear differential equation solved with hyperbolic functions? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I am actually writing a research paper on how to determine the approximate region of where a rolling ball in a roulette wheel will fall through a differential equation. The differential equation is: $$\dfrac {dy}{dt}= -ay^2+b$$ where $(a)$ and $(b)$ are constants. I found the answer to this differential equation in another research paper and the answer the research paper shows, but without any steps leading to it is: $$y(t)= -b\coth(c-abt)$$ where $c$ is the constant arising from the integration. I don't understand how this research paper managed to solve the nonlinear differential equation through the use of hyperbolic functions. Does anybody know how it has been done and is able to explain it please?","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I am actually writing a research paper on how to determine the approximate region of where a rolling ball in a roulette wheel will fall through a differential equation. The differential equation is: where and are constants. I found the answer to this differential equation in another research paper and the answer the research paper shows, but without any steps leading to it is: where is the constant arising from the integration. I don't understand how this research paper managed to solve the nonlinear differential equation through the use of hyperbolic functions. Does anybody know how it has been done and is able to explain it please?",\dfrac {dy}{dt}= -ay^2+b (a) (b) y(t)= -b\coth(c-abt) c,"['calculus', 'integration', 'ordinary-differential-equations', 'hyperbolic-functions']"
42,How find y from $x^2 y^3 d x+x\left(1+y^2\right) d y=0$,How find y from,x^2 y^3 d x+x\left(1+y^2\right) d y=0,"Suppose $\alpha (x,y)=\frac{1}{xy^3}$ is integral factor of equation $$x^2 y^3 d x+x\left(1+y^2\right) d y=0$$ Check $\alpha (x,y)$ : $x^2 y^3 d x+x\left(1+y^2\right) d y=0 \mid \cdot \frac{1}{x y^3} \quad x \neq 0, y \neq 0$ $x d x+y^3\left(1+y^2\right) d y=0 \quad (\square)$ Let $M(x)=x, \quad \text{and} \quad N(x)=y^3\left(1+y^2\right)$ where $ D=\left\{ \left( x;y \right) \in \mathbb{R} ^2\,\,| \begin{matrix} 	x>0&		\land&		y>0\\ \end{matrix} \right\}  $ I notice: $D$ is connected set $M(x),\: N(x)$ are continuous function over $D$ ( $C^1$ ) Let us check that the partial derivatives are equal: $\frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}$ therefore $$\frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}$$ Back to $ (\square)$ : $\int M(x, y) d x+\int N(x, y) d y=\mathbf{C}$ $\int x d x+\int \frac{1+y^2}{y^3} d y=\mathbf{C}$ $\frac{x^2}{2}+\int\left(\frac{1}{y^3}+\frac{1}{y}\right) d y=\mathbf{C}$ $\frac{x^2}{2}+\frac{y^{-3+1}}{-3+1}+\ln (y)=\mathbf{C}$ $\frac{x^2}{2}-\frac{1}{2 y^2}+\ln (y)=\mathbf{C}$ Question: How find solutions for the variable $y$ like WolframAlpha. I know WolframAlpha use Lambert function, but i don't know find y. $$ y=-\frac{1}{\sqrt{W\left(e^{x^2-2 C}\right)}} $$ $$ y=\frac{1}{\sqrt{W\left(e^{x^2-2 C}\right)}} $$ What happens if $(0,0) \in R^2$ . Is solution differential equation? What happens if $ (x, y) \in R^2 - D$ without (0,0)? Why D must be connected set? Thank you in advance.","Suppose is integral factor of equation Check : Let where I notice: is connected set are continuous function over ( ) Let us check that the partial derivatives are equal: therefore Back to : Question: How find solutions for the variable like WolframAlpha. I know WolframAlpha use Lambert function, but i don't know find y. What happens if . Is solution differential equation? What happens if without (0,0)? Why D must be connected set? Thank you in advance.","\alpha (x,y)=\frac{1}{xy^3} x^2 y^3 d x+x\left(1+y^2\right) d y=0 \alpha (x,y) x^2 y^3 d x+x\left(1+y^2\right) d y=0 \mid \cdot \frac{1}{x y^3} \quad x \neq 0, y \neq 0 x d x+y^3\left(1+y^2\right) d y=0 \quad (\square) M(x)=x, \quad \text{and} \quad N(x)=y^3\left(1+y^2\right) 
D=\left\{ \left( x;y \right) \in \mathbb{R} ^2\,\,| \begin{matrix}
	x>0&		\land&		y>0\\
\end{matrix} \right\} 
 D M(x),\: N(x) D C^1 \frac{\partial M}{\partial y}=\frac{\partial N}{\partial x} \frac{\partial M}{\partial y}=\frac{\partial N}{\partial x}  (\square) \int M(x, y) d x+\int N(x, y) d y=\mathbf{C} \int x d x+\int \frac{1+y^2}{y^3} d y=\mathbf{C} \frac{x^2}{2}+\int\left(\frac{1}{y^3}+\frac{1}{y}\right) d y=\mathbf{C} \frac{x^2}{2}+\frac{y^{-3+1}}{-3+1}+\ln (y)=\mathbf{C} \frac{x^2}{2}-\frac{1}{2 y^2}+\ln (y)=\mathbf{C} y 
y=-\frac{1}{\sqrt{W\left(e^{x^2-2 C}\right)}}
 
y=\frac{1}{\sqrt{W\left(e^{x^2-2 C}\right)}}
 (0,0) \in R^2  (x, y) \in R^2 - D","['ordinary-differential-equations', 'lambert-w']"
43,How to solve a differential equation of this form?,How to solve a differential equation of this form?,,"Please consider a differential equation of the form: $$ (ax + by + c) dx + (ex + fy + g) dy = 0 $$ For the special case of $c = g = 0$ , then this equation is homogenous and I know how to solve it. Normally, I would solve this equation by setting up the following system of equations: \begin{align*} ax + by + c &= 0 \\ ex + fy + g &= 0  \end{align*} Assuming that this set of equations of a unique solution, I know how to solve differential equation of this form. However, I how do I solve a differential equation of this form when the above system of differential equations does not have a unique solution? I am thinking the correct substitution is: $$ z = ax + by $$ which gives me: \begin{align*} dz &= a\, dx \\ dz &= b\, dy  \end{align*} Do I have this right?","Please consider a differential equation of the form: For the special case of , then this equation is homogenous and I know how to solve it. Normally, I would solve this equation by setting up the following system of equations: Assuming that this set of equations of a unique solution, I know how to solve differential equation of this form. However, I how do I solve a differential equation of this form when the above system of differential equations does not have a unique solution? I am thinking the correct substitution is: which gives me: Do I have this right?"," (ax + by + c) dx + (ex + fy + g) dy = 0  c = g = 0 \begin{align*}
ax + by + c &= 0 \\
ex + fy + g &= 0 
\end{align*}  z = ax + by  \begin{align*}
dz &= a\, dx \\
dz &= b\, dy 
\end{align*}",['ordinary-differential-equations']
44,Eigen values of the ODE $(1+x^2)y’’+2xy’+\lambda x^2y=0$.,Eigen values of the ODE .,(1+x^2)y’’+2xy’+\lambda x^2y=0,"How to find eigen values of the Sturm Liouville Boundary value problem $$(1+x^2)y’’+2xy’+\lambda x^2y=0, y’(1)=0, y’(10)=0?$$ I know how to solve Sturm Liouville problem of the form $y’’+\lambda =0$ by making three cases as $\lambda=0,\lambda>0$ and $\lambda <0$ . In this Problem I only know that $\lambda=0$ is an eigen value because for $\lambda=0$ there are non constant solutions. I don’t know to discuss all other eigenvalues . Please help . Thank you.",How to find eigen values of the Sturm Liouville Boundary value problem I know how to solve Sturm Liouville problem of the form by making three cases as and . In this Problem I only know that is an eigen value because for there are non constant solutions. I don’t know to discuss all other eigenvalues . Please help . Thank you.,"(1+x^2)y’’+2xy’+\lambda x^2y=0, y’(1)=0, y’(10)=0? y’’+\lambda =0 \lambda=0,\lambda>0 \lambda <0 \lambda=0 \lambda=0","['ordinary-differential-equations', 'eigenvalues-eigenvectors', 'boundary-value-problem', 'sturm-liouville']"
45,Need help in linearizing a non-linear system of ODE,Need help in linearizing a non-linear system of ODE,,"We have the system \begin{align} \begin{cases} \dot{x} = (1-x)^2-y^2\\ \dot{y} = \epsilon^{xy}-1 \end{cases} \end{align} and I must plot the phase graph. To do this I already know that I must linearize in the first order, since for Taylor degree $\geq2 \ $ , $\Delta x \to 0$ . However I am stuck and cannot even start. I have in my mind that I must be able to set the system in such a way such that \begin{align} \begin{cases} y=\text{something}\\ \dot{y}= \text{other something} \end{cases} \end{align} so that then I can run \begin{align} F(x+h)=F(x_0)+h\cdot \dfrac{dF}{dx}|_{x=x_0}. \end{align} The critical points of the starting system are $(0,1), (0,-1), (1,0)$ .","We have the system and I must plot the phase graph. To do this I already know that I must linearize in the first order, since for Taylor degree , . However I am stuck and cannot even start. I have in my mind that I must be able to set the system in such a way such that so that then I can run The critical points of the starting system are .","\begin{align}
\begin{cases}
\dot{x} = (1-x)^2-y^2\\
\dot{y} = \epsilon^{xy}-1
\end{cases}
\end{align} \geq2 \  \Delta x \to 0 \begin{align}
\begin{cases}
y=\text{something}\\
\dot{y}= \text{other something}
\end{cases}
\end{align} \begin{align}
F(x+h)=F(x_0)+h\cdot \dfrac{dF}{dx}|_{x=x_0}.
\end{align} (0,1), (0,-1), (1,0)","['ordinary-differential-equations', 'derivatives', 'dynamical-systems', 'linearization']"
46,Trajectories with predator-prey population in $xy$-plane,Trajectories with predator-prey population in -plane,xy,"I've been solving a predator-prey population DE successfully with programming. But I don't understand exactly what a trajectory in $xy$ -plane should look like or if I have done it correctly. The predator-prey function looks like: $$\begin{cases} \dfrac{dx}{dt} = x(1-  y) \\ \dfrac{dy}{dt} = y(0.9 x  - 1) \end{cases}$$ I have reformulated the DE to the dynamic system: $$C = \ln(y) + \ln(x) - y - 0.9x$$ I ""replaced"" $C$ to the function and plotted it in the $xy$ -plane. But I'm not sure I have come up with a satifying answer to the question.","I've been solving a predator-prey population DE successfully with programming. But I don't understand exactly what a trajectory in -plane should look like or if I have done it correctly. The predator-prey function looks like: I have reformulated the DE to the dynamic system: I ""replaced"" to the function and plotted it in the -plane. But I'm not sure I have come up with a satifying answer to the question.","xy \begin{cases}
\dfrac{dx}{dt} = x(1-  y) \\
\dfrac{dy}{dt} = y(0.9 x  - 1)
\end{cases} C = \ln(y) + \ln(x) - y - 0.9x C xy","['ordinary-differential-equations', 'dynamical-systems']"
47,Params in change of variables in ODEs,Params in change of variables in ODEs,,"Let $\frac{dx}{dt} = x(a-bx)$ be a ODE with $a, b > 0$ . Considering the change of variables $s = \alpha t$ and $y = \beta x$ . I need to find $\alpha, \beta$ that transforms the equation into $\frac{dy}{ds} = y(1-y)$ . Appling the chain rule, I obtain $$ \frac{dy}{ds} = \frac{\frac{dy}{dt}}{\frac{ds}{dt}} = \frac{\beta}{\alpha} \frac{dx}{dt} = \frac{\beta}{\alpha} x (a-bx) $$ And replacing $y = \beta x$ and simplifing I get $$ 1 - \beta x = \frac{\beta}{\alpha}x(a-bx) $$ How can i continue?","Let be a ODE with . Considering the change of variables and . I need to find that transforms the equation into . Appling the chain rule, I obtain And replacing and simplifing I get How can i continue?","\frac{dx}{dt} = x(a-bx) a, b > 0 s = \alpha t y = \beta x \alpha, \beta \frac{dy}{ds} = y(1-y) 
\frac{dy}{ds} = \frac{\frac{dy}{dt}}{\frac{ds}{dt}} = \frac{\beta}{\alpha} \frac{dx}{dt} = \frac{\beta}{\alpha} x (a-bx)
 y = \beta x 
1 - \beta x = \frac{\beta}{\alpha}x(a-bx)
",['ordinary-differential-equations']
48,Finding a simpler differential equation for this geometric problem,Finding a simpler differential equation for this geometric problem,,"This is the problem I am trying to solve: Find the curves which tangent segment between the coordinate axis is constant Let $k$ be the length of the segment, then we have to find the points of intersection of the tangent line to the curve with the $x$ and $y$ axis, $Q$ and $P$ respectively. Using the equation $Y-y=y’(Q-x)$ , we obtain: $$ Q = x - \frac{y}{y’} $$ and $$ P = y’(\frac{y}{y’}-x) $$ So the equation for the distance would be $$ k^2=(y’)^2(\frac{y}{y’}-x)^2+(x - \frac{y}{y’})^2  $$ I changed it up a little bit and ended up with this $$ k^2=(y’)^2(\frac{y}{y’}-x)^2+\frac{1}{(y’)^2}(xy’ - y)^2  $$ I tried expanding the squares, but the equation is too hard for me to solve. I think the reasoning that got me there is right, so I thought there must be a simpler way to express the differential equation. The final equation kinda looks like the one of a circumference, so maybe there’s a way to parametrize it with that, but I couldn’t manage to do anything. I would really appreciate any hint to keep going, since I’ve been stuck with this problem for a couple of days now.","This is the problem I am trying to solve: Find the curves which tangent segment between the coordinate axis is constant Let be the length of the segment, then we have to find the points of intersection of the tangent line to the curve with the and axis, and respectively. Using the equation , we obtain: and So the equation for the distance would be I changed it up a little bit and ended up with this I tried expanding the squares, but the equation is too hard for me to solve. I think the reasoning that got me there is right, so I thought there must be a simpler way to express the differential equation. The final equation kinda looks like the one of a circumference, so maybe there’s a way to parametrize it with that, but I couldn’t manage to do anything. I would really appreciate any hint to keep going, since I’ve been stuck with this problem for a couple of days now.","k x y Q P Y-y=y’(Q-x) 
Q = x - \frac{y}{y’}
 
P = y’(\frac{y}{y’}-x)
 
k^2=(y’)^2(\frac{y}{y’}-x)^2+(x - \frac{y}{y’})^2 
 
k^2=(y’)^2(\frac{y}{y’}-x)^2+\frac{1}{(y’)^2}(xy’ - y)^2 
","['geometry', 'ordinary-differential-equations']"
49,On the Bernoulli differential equation $ y'(x) = \dfrac{y}{x + \sqrt{xy}} $,On the Bernoulli differential equation, y'(x) = \dfrac{y}{x + \sqrt{xy}} ,"$$ y'(x) = \frac{y}{x + \sqrt{xy}} $$ I have attempted to solve it. However, I am not able to arrive at the exact expression of the final answer. Maybe I am making a mistake in a step? Can you please help? The answer should be $$ \sqrt{x} = \sqrt{y}(\log\sqrt{y} + \mbox{Constant}) $$ PS: This is my first question on this site, I will be more than willing to provide my own attempts at this, if it is required. Thanks. Here are my workings, $$ \frac{1}{\sqrt{x}}x'(y)-\frac{\sqrt{x}}{y}=\frac{1}{\sqrt{y}} $$ $$ Taking, \sqrt{x}=z $$ Differentiating  wrt  to x and dividing by dy $$ ,\frac{1}{2\sqrt{x}}x'(y)=z'(y) $$ Substituting, $$ z'(y)-\frac{z}{2y}=\frac{1}{2\sqrt{y}} $$ Now taking integrating factor $$ e^{\int_{}{}-\frac{1}{2y} dy}\ $$ $$ =e^{-\frac{1}{2}\log y } $$ Multiplying both sides by the integrating factor, $$ z'(y)e^{-\frac{1}{2}\log y }-\frac{z}{2y}e^{-\frac{1}{2}\log y }=\frac{1}{2\sqrt{y}}e^{-\frac{1}{2}\log y } $$ $$ (ze^{-\frac{1}{2}\log y })'(y)=\frac{1}{2\sqrt{y}}e^{-\frac{1}{2}\log y } $$ $$ ze^{-\frac{1}{2}\log y }=\int_{}{}\frac{1}{2y} dy\ $$ $$ze^{-\frac{1}{2}\log y }=\frac{1}{2}log(y)+Constant $$ $$ \frac{\sqrt{x}}{\sqrt{y}}=\frac{1}{2}log(y)+Constant $$ There you go, did I mess up some logarithmic property? or is it something with the integration I am missing? The answer is close, but not it.","I have attempted to solve it. However, I am not able to arrive at the exact expression of the final answer. Maybe I am making a mistake in a step? Can you please help? The answer should be PS: This is my first question on this site, I will be more than willing to provide my own attempts at this, if it is required. Thanks. Here are my workings, Differentiating  wrt  to x and dividing by dy Substituting, Now taking integrating factor Multiplying both sides by the integrating factor, There you go, did I mess up some logarithmic property? or is it something with the integration I am missing? The answer is close, but not it."," y'(x) = \frac{y}{x + \sqrt{xy}}   \sqrt{x} = \sqrt{y}(\log\sqrt{y} + \mbox{Constant})   \frac{1}{\sqrt{x}}x'(y)-\frac{\sqrt{x}}{y}=\frac{1}{\sqrt{y}}   Taking, \sqrt{x}=z   ,\frac{1}{2\sqrt{x}}x'(y)=z'(y)   z'(y)-\frac{z}{2y}=\frac{1}{2\sqrt{y}}   e^{\int_{}{}-\frac{1}{2y} dy}\   =e^{-\frac{1}{2}\log y }   z'(y)e^{-\frac{1}{2}\log y }-\frac{z}{2y}e^{-\frac{1}{2}\log y }=\frac{1}{2\sqrt{y}}e^{-\frac{1}{2}\log y }   (ze^{-\frac{1}{2}\log y })'(y)=\frac{1}{2\sqrt{y}}e^{-\frac{1}{2}\log y }   ze^{-\frac{1}{2}\log y }=\int_{}{}\frac{1}{2y} dy\  ze^{-\frac{1}{2}\log y }=\frac{1}{2}log(y)+Constant   \frac{\sqrt{x}}{\sqrt{y}}=\frac{1}{2}log(y)+Constant ",['ordinary-differential-equations']
50,Analitic solution to ${df(x)\over{dx}} + {1\over2} f^2 (x) = ax^2 +bx + c$,Analitic solution to,{df(x)\over{dx}} + {1\over2} f^2 (x) = ax^2 +bx + c,"What is the analytic solution to the following differential equation? $${df(x)\over{dx}} + {1\over2} f^2 (x) = ax^2 +bx + c$$ I know that this is a nonlinear nonhomogeneous first-order ODE. However, I do not know how to solve it.","What is the analytic solution to the following differential equation? I know that this is a nonlinear nonhomogeneous first-order ODE. However, I do not know how to solve it.",{df(x)\over{dx}} + {1\over2} f^2 (x) = ax^2 +bx + c,"['ordinary-differential-equations', 'nonlinear-system']"
51,System of first order differential equation.,System of first order differential equation.,,"Consider the linear system $y’=Ay+h$ where $$ A=\begin{bmatrix} 1 & 1\\ 4 & -2 \end{bmatrix} $$ and $$h=\begin{bmatrix} 3t+1\\ 2t+5 \end{bmatrix}$$ Suppose $y(t)$ is a solution such that $$\lim_{t\to\infty}\frac{y(t)}{t}=k\in\mathbb R^2$$ What is the value of $k?$ $1$ . $\begin{bmatrix} \frac{-4}{3}\\ \frac{-5}{3} \end{bmatrix}$ . $2$ . $\begin{bmatrix} \frac{4}{3}\\ \frac{-5}{3} \end{bmatrix}$ . $3$ . $\begin{bmatrix} \frac{2}{3}\\ \frac{-5}{3} \end{bmatrix}$ . $4$ . $\begin{bmatrix} \frac{-2}{3}\\ \frac{-5}{3} \end{bmatrix}$ . I find eigen value of corresponding homogeneous system as $2$ and $-3$ and corresponding eigen vectors as $\begin{bmatrix} 1\\ 1 \end{bmatrix}$ and $\begin{bmatrix} 1\\ -4 \end{bmatrix}$ . Therefore general solution of corresponding homogeneous differential equation is given by $y_c=\Phi(x)c=\begin{bmatrix} e^{2t} & e^{-3t}\\ e^{2t} & -4e^{-3t} \end{bmatrix}c$ , for arbitrary constant $c$ . Now as I know that by using variation of parameter general solution is given by $$y=\Phi(t)c+\Phi(t)\int {\Phi(x)}^{-1}h(x)dx= \begin{bmatrix} e^{2t} & e^{-3t}\\ e^{2t} & -4e^{-3t} \end{bmatrix}c+ \begin{bmatrix} e^{2t} & e^{-3t}\\ e^{2t} & -4e^{-3t} \end{bmatrix}\int {\begin{bmatrix} e^{2x} & e^{-3x}\\ e^{2x} & -4e^{-3x} \end{bmatrix}}^{-1} \begin{bmatrix} 3x+1\\ 2x+5 \end{bmatrix}dx$$ Now I am unable to reach at final answer . Please help. Thank you.","Consider the linear system where and Suppose is a solution such that What is the value of . . . . . . . . I find eigen value of corresponding homogeneous system as and and corresponding eigen vectors as and . Therefore general solution of corresponding homogeneous differential equation is given by , for arbitrary constant . Now as I know that by using variation of parameter general solution is given by Now I am unable to reach at final answer . Please help. Thank you.","y’=Ay+h 
A=\begin{bmatrix}
1 & 1\\
4 & -2
\end{bmatrix}
 h=\begin{bmatrix}
3t+1\\
2t+5
\end{bmatrix} y(t) \lim_{t\to\infty}\frac{y(t)}{t}=k\in\mathbb R^2 k? 1 \begin{bmatrix}
\frac{-4}{3}\\
\frac{-5}{3}
\end{bmatrix} 2 \begin{bmatrix}
\frac{4}{3}\\
\frac{-5}{3}
\end{bmatrix} 3 \begin{bmatrix}
\frac{2}{3}\\
\frac{-5}{3}
\end{bmatrix} 4 \begin{bmatrix}
\frac{-2}{3}\\
\frac{-5}{3}
\end{bmatrix} 2 -3 \begin{bmatrix}
1\\
1
\end{bmatrix} \begin{bmatrix}
1\\
-4
\end{bmatrix} y_c=\Phi(x)c=\begin{bmatrix}
e^{2t} & e^{-3t}\\
e^{2t} & -4e^{-3t}
\end{bmatrix}c c y=\Phi(t)c+\Phi(t)\int {\Phi(x)}^{-1}h(x)dx= \begin{bmatrix}
e^{2t} & e^{-3t}\\
e^{2t} & -4e^{-3t}
\end{bmatrix}c+ \begin{bmatrix}
e^{2t} & e^{-3t}\\
e^{2t} & -4e^{-3t}
\end{bmatrix}\int {\begin{bmatrix}
e^{2x} & e^{-3x}\\
e^{2x} & -4e^{-3x}
\end{bmatrix}}^{-1} \begin{bmatrix}
3x+1\\
2x+5
\end{bmatrix}dx",['ordinary-differential-equations']
52,Is this method of solving this differential inequality correct?,Is this method of solving this differential inequality correct?,,"If $ P(1)=0 , and \frac{d(P(x)}{dx} >P(x),\forall x \geq 1$ , I have been asked to prove that $P(x)>0 ,\forall x\geq 1$ my attempt:- $dx = \frac{dP(x)}{P(x)}$ $\int(dx) \geq \int{\frac{dP(x)}{P(x)}}$ $x+C \geq ln(P(x))$ so $e^{x+c} \geq P(x)$ and as $e^x$ is greater than zero, for all x, this inequality, must be true my book solved this by multiplying both sides by $e^{-x}$ , and then solving the ODE. Is my method right too, and if not, where is it wrong?","If , I have been asked to prove that my attempt:- so and as is greater than zero, for all x, this inequality, must be true my book solved this by multiplying both sides by , and then solving the ODE. Is my method right too, and if not, where is it wrong?"," P(1)=0 , and \frac{d(P(x)}{dx} >P(x),\forall x \geq 1 P(x)>0 ,\forall x\geq 1 dx = \frac{dP(x)}{P(x)} \int(dx) \geq \int{\frac{dP(x)}{P(x)}} x+C \geq ln(P(x)) e^{x+c} \geq P(x) e^x e^{-x}","['integration', 'ordinary-differential-equations', 'derivatives', 'solution-verification']"
53,Picard-Lindelöf and the choice of the metric,Picard-Lindelöf and the choice of the metric,,"So the theorem of Picard-Lindelöf ensures that an ordinary differential equation $\frac{dy}{dt}=f(y)$ has a unique solution if $f$ is Lipschitz-continuous. The general form of Lipschitz continuity is given if $\forall x_1,x_2 \in X : d_Y(f(x_1),f(x_2)) \le L \cdot d_X(x_1,x_2)$ with any two metric spaces $(X,d_X)$ and $(Y,d_Y)$ . But what if we have a function $f$ that is Lipschitz-continuous for some metrics $d_X,d_Y$ but not for others? I currently can neither prove nor disprove that such a function might exist, so I am assuming that PL should be applicable even in that case. I could imagine the following situations: This is no problem at all because PL only provides a sufficient condition for existence and convergence. So essentially providing any pair of metrics for which $f$ is Lipschitz is enough. The solution of the ODE actually somehow depends on the metric and I currently fail to see how. Picard-Lindelöff actually depends on some specific subset of metrics for which $f$ must be Lipschitz, but this is sometimes omitted when talking about PL. This situation is impossible, that is there is some proof that a function that is Lipschitz under one pair of metrics is Lipschitz under all pairs of metrics. Which of these four situations is the correct one? I am currently tending towards 1, because this would imply very weak conditions (just find any pair of metrics that make it work) for PL.","So the theorem of Picard-Lindelöf ensures that an ordinary differential equation has a unique solution if is Lipschitz-continuous. The general form of Lipschitz continuity is given if with any two metric spaces and . But what if we have a function that is Lipschitz-continuous for some metrics but not for others? I currently can neither prove nor disprove that such a function might exist, so I am assuming that PL should be applicable even in that case. I could imagine the following situations: This is no problem at all because PL only provides a sufficient condition for existence and convergence. So essentially providing any pair of metrics for which is Lipschitz is enough. The solution of the ODE actually somehow depends on the metric and I currently fail to see how. Picard-Lindelöff actually depends on some specific subset of metrics for which must be Lipschitz, but this is sometimes omitted when talking about PL. This situation is impossible, that is there is some proof that a function that is Lipschitz under one pair of metrics is Lipschitz under all pairs of metrics. Which of these four situations is the correct one? I am currently tending towards 1, because this would imply very weak conditions (just find any pair of metrics that make it work) for PL.","\frac{dy}{dt}=f(y) f \forall x_1,x_2 \in X : d_Y(f(x_1),f(x_2)) \le L \cdot d_X(x_1,x_2) (X,d_X) (Y,d_Y) f d_X,d_Y f f","['ordinary-differential-equations', 'metric-spaces', 'lipschitz-functions']"
54,2nd order homogenous differential equation solution,2nd order homogenous differential equation solution,,Suppose we have this differential equation : $-\frac{2}{x^2}y(x)+\frac{d^{2}y(x)}{dx^2}=0$ One obvious function which satisfies the equation is $y(x)=x^2$ We take $x^{2}$ to be a partial solution and from it we try to find the general solution. Let $f_{o}(x) = u(x)\cdot x^{2} $ be the full solution of the differential equation: they by differentiating $f_{o}(x)$ we get $f_{o}'(x) = u'(x)x^{2}+u(x)2x$ and $f_{o}''(x) = u''(x)x^{2}+2xu'(x)+2xu'(x)+2u(x)$ we substitute into the previous equation and we get $-\frac{2}{x^{2}}u(x)x^{2}+u''(x)x^{2}+2xu'(x)+2xu'(x)+2u(x)=0\rightarrow u''(x)x^{2}+4xu'(x)=0$ Now I cannot find $u''(x)$ I tried doing the same procedure to find u(x) but the equation I get is the same with the one I have already found so I cannot continue.Help appreciated.,Suppose we have this differential equation : One obvious function which satisfies the equation is We take to be a partial solution and from it we try to find the general solution. Let be the full solution of the differential equation: they by differentiating we get and we substitute into the previous equation and we get Now I cannot find I tried doing the same procedure to find u(x) but the equation I get is the same with the one I have already found so I cannot continue.Help appreciated.,-\frac{2}{x^2}y(x)+\frac{d^{2}y(x)}{dx^2}=0 y(x)=x^2 x^{2} f_{o}(x) = u(x)\cdot x^{2}  f_{o}(x) f_{o}'(x) = u'(x)x^{2}+u(x)2x f_{o}''(x) = u''(x)x^{2}+2xu'(x)+2xu'(x)+2u(x) -\frac{2}{x^{2}}u(x)x^{2}+u''(x)x^{2}+2xu'(x)+2xu'(x)+2u(x)=0\rightarrow u''(x)x^{2}+4xu'(x)=0 u''(x),"['calculus', 'ordinary-differential-equations', 'derivatives']"
55,Question about multiple scales approximation for a system of ODEs,Question about multiple scales approximation for a system of ODEs,,"I have been trying to apply the method of multiple scales to a certain set of partial differential equations, but I keep having problems.  I would appreciate if someone could point out my mistake, whether in my application of the method or in my interpretation of the results.  I apologize about the length of the post, but I wanted to include my full derivation. The relevant equations may be written as \begin{gather}   \frac{\partial f}{\partial x} - i\epsilon a(x)g = 0, \\   \frac{\partial g}{\partial x} - i\epsilon b(x)f = 0, \end{gather} where $\epsilon\ll1$ and $a(x)$ and $b(x)$ are statistically homogeneous, positive and integrable functions (e.g., they could take the form $a(x)=1+\sin(kx)/4$ ).  If I assume that $f(x)=f_0(x_0,x_1,x_2)+\epsilon f_1(x_0,x_1,x_2) + \epsilon^2 f_2(x_0,x_1,x_2)$ and similarly for $g(x)$ , where $x_n=\epsilon^n x$ , and if I further assume that $f_n$ and $g_n$ are $O(1)$ , I get an series of sets of differential equations.  The $O(1)$ equations may be written as \begin{gather}   \frac{\partial f_0}{\partial x_0} = \frac{\partial g_0}{\partial x_0} = 0, \end{gather} and so we find that the largest-scale approximation of $f$ and $g$ does not depend on the smallest length scale, $x_0$ .  The $O(\epsilon)$ equations are \begin{gather}   \frac{\partial f_1}{\partial x_0} + \frac{\partial f_0}{\partial x_1} - i a(x_0)g_0 = 0, \\   \frac{\partial g_1}{\partial x_0} + \frac{\partial g_0}{\partial x_1} - i b(x_0)f_0 = 0. \end{gather} Since $f_0$ and $g_0$ do not depend on $x_0$ , we may integrate, yielding \begin{gather}   f_1 + x_0\frac{\partial f_0}{\partial x_1} - i g_0\int a(x')dx' + c_1 = 0, \\   g_1 + x_0\frac{\partial g_0}{\partial x_1} - i f_0\int b(x')dx' + c_2 = 0, \end{gather} where $c_1$ and $c_2$ are constants of integration.  These constants may be set equal to zero without loss of generality, as they may be incorporated into $f_0$ and $g_0$ .  We need to remove secular terms.  Clearly, the $x_0$ terms are secular, but the integrals over $a$ and $b$ are also secular.  If we decompose $a(x_0)$ and $b(x_0)$ such that $a(x_0)=\langle a\rangle + \Delta a(x_0)$ , where $\langle a\rangle$ is the average of $a(x_0)$ , and $b(x_0)=\langle b\rangle + \Delta b(x_0)$ , we may then write \begin{gather}   f_1 + x_0\left[\frac{\partial f_0}{\partial x_1} - ig_0\langle a\rangle\right] - i g_0\int_0^{x_0} \Delta a(x_0)dx_0 = 0, \\   g_1 + x_0\left[\frac{\partial g_0}{\partial x_1} - if_0\langle b\rangle\right] - i f_0\int_0^{x_0} \Delta b(x_0')dx_0' = 0. \end{gather} Now the integrals are not secular, and we may set the terms in the square brackets equal to zero.  Thus, we may write \begin{gather}   f_1 = i g_0\int_0^{x_0} \Delta a(x_0)dx_0, \\   g_1 = i f_0\int_0^{x_0} \Delta b(x_0)dx_0, \\   \frac{\partial f_0}{\partial x_1} - i\langle a\rangle g_0 = 0, \\   \frac{\partial g_0}{\partial x_1} - i\langle b\rangle f_0 = 0. \end{gather} Thus, I have equations for $f_0$ and $g_0$ that may be explicitly solved as a function of $x_1$ .  I can then write the solutions as \begin{gather}   f_0(x_1,x_2) = A(x_2)e^{i\sqrt{\langle a\rangle\langle b\rangle}x_1} + B(x_2)e^{-i\sqrt{\langle a\rangle\langle b\rangle}x_1}, \\   g_0(x_1,x_2) = \sqrt{\frac{\langle b\rangle}{\langle a\rangle}}A(x_2)e^{i\sqrt{\langle a\rangle\langle b\rangle}x_1} - \sqrt{\frac{\langle b\rangle}{\langle a\rangle}}B(x_2)e^{-i\sqrt{\langle a\rangle\langle b\rangle}x_1}. \end{gather} To this point I do not think there is an issue, but I would like to extend the results to higher orders.  The $O(\epsilon^2)$ equations are given by \begin{gather}   \frac{\partial f_2}{\partial x_0} + \frac{\partial f_1}{\partial x_1} + \frac{\partial f_0}{\partial x_2} - i a(x_0)g_1 = 0, \\   \frac{\partial g_2}{\partial x_0} + \frac{\partial g_1}{\partial x_1} + \frac{\partial g_0}{\partial x_2} - i b(x_0)f_1 = 0. \end{gather} Substituting in the solutions to $f_1$ and $g_1$ and their $x_1$ gradients then yields \begin{gather}   \frac{\partial f_2}{\partial x_0} + \frac{\partial f_0}{\partial x_2} + \left[ a(x_0)\int_0^{x_0} \Delta b(x_0')dx_0' - \langle b\rangle \int_0^{x_0} \Delta a(x_0')dx_0' \right] f_0 = 0, \\   \frac{\partial g_2}{\partial x_0} + \frac{\partial g_0}{\partial x_2} + \left[ b(x_0)\int_0^{x_0} \Delta a(x_0')dx_0' - \langle a\rangle \int_0^{x_0} \Delta b(x_0')dx_0' \right] g_0 = 0. \end{gather} We may integrate with respect to $x_0$ again, and eliminating the secular terms leads to the conclusion \begin{gather}   \frac{\partial f_0}{\partial x_2} + \left[ \left\langle a(x_0)\int_0^{x_0} \Delta b(x_0')dx_0'\right\rangle - \langle b\rangle \left\langle \int_0^{x_0} \Delta a(x_0')dx_0' \right\rangle \right] f_0 = 0, \\   \frac{\partial g_0}{\partial x_2} + \left[ \left\langle b(x_0)\int_0^{x_0} \Delta a(x_0')dx_0'\right\rangle - \langle a\rangle \left\langle \int_0^{x_0} \Delta b(x_0')dx_0' \right\rangle \right] g_0 = 0. \end{gather} These equations may also be solved explicitly.  If we let \begin{gather}   \alpha \equiv \left\langle a(x_0)\int_0^{x_0} \Delta b(x_0')dx_0'\right\rangle - \langle b\rangle \left\langle \int_0^{x_0} \Delta a(x_0')dx_0' \right\rangle, \\   \beta \equiv \left\langle b(x_0)\int_0^{x_0} \Delta a(x_0')dx_0'\right\rangle - \langle a\rangle \left\langle \int_0^{x_0} \Delta b(x_0')dx_0' \right\rangle, \end{gather} then the solutions are given by \begin{gather}   f_0(x_1,x_2) = f_{01}(x_1)e^{-\alpha x_2}, \\   g_0(x_1,x_2) = g_{01}(x_1)e^{-\beta x_2}. \end{gather} Combining this result with the $O(\epsilon)$ result leads to the conclusions \begin{gather}   A(x_2) = B(x_2) = C_0e^{-\alpha x_2}, \\   A(x_2) = B(x_2) = D_0e^{-\beta x_2}. \end{gather} However, both of these statements can only be true if $\alpha=\beta$ , which is not always the case, or if $C_0=D_0=0$ , which is the trivial solution.  I know that a non-trivial solution can exist for the original equations, even when $\alpha\ne\beta$ . Did I make a mistake somewhere?  Is there a restriction on the method of multiple scales that makes may analysis invalid?  Any assistance would be greatly appreciated.","I have been trying to apply the method of multiple scales to a certain set of partial differential equations, but I keep having problems.  I would appreciate if someone could point out my mistake, whether in my application of the method or in my interpretation of the results.  I apologize about the length of the post, but I wanted to include my full derivation. The relevant equations may be written as where and and are statistically homogeneous, positive and integrable functions (e.g., they could take the form ).  If I assume that and similarly for , where , and if I further assume that and are , I get an series of sets of differential equations.  The equations may be written as and so we find that the largest-scale approximation of and does not depend on the smallest length scale, .  The equations are Since and do not depend on , we may integrate, yielding where and are constants of integration.  These constants may be set equal to zero without loss of generality, as they may be incorporated into and .  We need to remove secular terms.  Clearly, the terms are secular, but the integrals over and are also secular.  If we decompose and such that , where is the average of , and , we may then write Now the integrals are not secular, and we may set the terms in the square brackets equal to zero.  Thus, we may write Thus, I have equations for and that may be explicitly solved as a function of .  I can then write the solutions as To this point I do not think there is an issue, but I would like to extend the results to higher orders.  The equations are given by Substituting in the solutions to and and their gradients then yields We may integrate with respect to again, and eliminating the secular terms leads to the conclusion These equations may also be solved explicitly.  If we let then the solutions are given by Combining this result with the result leads to the conclusions However, both of these statements can only be true if , which is not always the case, or if , which is the trivial solution.  I know that a non-trivial solution can exist for the original equations, even when . Did I make a mistake somewhere?  Is there a restriction on the method of multiple scales that makes may analysis invalid?  Any assistance would be greatly appreciated.","\begin{gather}
  \frac{\partial f}{\partial x} - i\epsilon a(x)g = 0, \\
  \frac{\partial g}{\partial x} - i\epsilon b(x)f = 0,
\end{gather} \epsilon\ll1 a(x) b(x) a(x)=1+\sin(kx)/4 f(x)=f_0(x_0,x_1,x_2)+\epsilon f_1(x_0,x_1,x_2) + \epsilon^2 f_2(x_0,x_1,x_2) g(x) x_n=\epsilon^n x f_n g_n O(1) O(1) \begin{gather}
  \frac{\partial f_0}{\partial x_0} = \frac{\partial g_0}{\partial x_0} = 0,
\end{gather} f g x_0 O(\epsilon) \begin{gather}
  \frac{\partial f_1}{\partial x_0} + \frac{\partial f_0}{\partial x_1} - i a(x_0)g_0 = 0, \\
  \frac{\partial g_1}{\partial x_0} + \frac{\partial g_0}{\partial x_1} - i b(x_0)f_0 = 0.
\end{gather} f_0 g_0 x_0 \begin{gather}
  f_1 + x_0\frac{\partial f_0}{\partial x_1} - i g_0\int a(x')dx' + c_1 = 0, \\
  g_1 + x_0\frac{\partial g_0}{\partial x_1} - i f_0\int b(x')dx' + c_2 = 0,
\end{gather} c_1 c_2 f_0 g_0 x_0 a b a(x_0) b(x_0) a(x_0)=\langle a\rangle + \Delta a(x_0) \langle a\rangle a(x_0) b(x_0)=\langle b\rangle + \Delta b(x_0) \begin{gather}
  f_1 + x_0\left[\frac{\partial f_0}{\partial x_1} - ig_0\langle a\rangle\right] - i g_0\int_0^{x_0} \Delta a(x_0)dx_0 = 0, \\
  g_1 + x_0\left[\frac{\partial g_0}{\partial x_1} - if_0\langle b\rangle\right] - i f_0\int_0^{x_0} \Delta b(x_0')dx_0' = 0.
\end{gather} \begin{gather}
  f_1 = i g_0\int_0^{x_0} \Delta a(x_0)dx_0, \\
  g_1 = i f_0\int_0^{x_0} \Delta b(x_0)dx_0, \\
  \frac{\partial f_0}{\partial x_1} - i\langle a\rangle g_0 = 0, \\
  \frac{\partial g_0}{\partial x_1} - i\langle b\rangle f_0 = 0.
\end{gather} f_0 g_0 x_1 \begin{gather}
  f_0(x_1,x_2) = A(x_2)e^{i\sqrt{\langle a\rangle\langle b\rangle}x_1} + B(x_2)e^{-i\sqrt{\langle a\rangle\langle b\rangle}x_1}, \\
  g_0(x_1,x_2) = \sqrt{\frac{\langle b\rangle}{\langle a\rangle}}A(x_2)e^{i\sqrt{\langle a\rangle\langle b\rangle}x_1} - \sqrt{\frac{\langle b\rangle}{\langle a\rangle}}B(x_2)e^{-i\sqrt{\langle a\rangle\langle b\rangle}x_1}.
\end{gather} O(\epsilon^2) \begin{gather}
  \frac{\partial f_2}{\partial x_0} + \frac{\partial f_1}{\partial x_1} + \frac{\partial f_0}{\partial x_2} - i a(x_0)g_1 = 0, \\
  \frac{\partial g_2}{\partial x_0} + \frac{\partial g_1}{\partial x_1} + \frac{\partial g_0}{\partial x_2} - i b(x_0)f_1 = 0.
\end{gather} f_1 g_1 x_1 \begin{gather}
  \frac{\partial f_2}{\partial x_0} + \frac{\partial f_0}{\partial x_2} + \left[ a(x_0)\int_0^{x_0} \Delta b(x_0')dx_0' - \langle b\rangle \int_0^{x_0} \Delta a(x_0')dx_0' \right] f_0 = 0, \\
  \frac{\partial g_2}{\partial x_0} + \frac{\partial g_0}{\partial x_2} + \left[ b(x_0)\int_0^{x_0} \Delta a(x_0')dx_0' - \langle a\rangle \int_0^{x_0} \Delta b(x_0')dx_0' \right] g_0 = 0.
\end{gather} x_0 \begin{gather}
  \frac{\partial f_0}{\partial x_2} + \left[ \left\langle a(x_0)\int_0^{x_0} \Delta b(x_0')dx_0'\right\rangle - \langle b\rangle \left\langle \int_0^{x_0} \Delta a(x_0')dx_0' \right\rangle \right] f_0 = 0, \\
  \frac{\partial g_0}{\partial x_2} + \left[ \left\langle b(x_0)\int_0^{x_0} \Delta a(x_0')dx_0'\right\rangle - \langle a\rangle \left\langle \int_0^{x_0} \Delta b(x_0')dx_0' \right\rangle \right] g_0 = 0.
\end{gather} \begin{gather}
  \alpha \equiv \left\langle a(x_0)\int_0^{x_0} \Delta b(x_0')dx_0'\right\rangle - \langle b\rangle \left\langle \int_0^{x_0} \Delta a(x_0')dx_0' \right\rangle, \\
  \beta \equiv \left\langle b(x_0)\int_0^{x_0} \Delta a(x_0')dx_0'\right\rangle - \langle a\rangle \left\langle \int_0^{x_0} \Delta b(x_0')dx_0' \right\rangle,
\end{gather} \begin{gather}
  f_0(x_1,x_2) = f_{01}(x_1)e^{-\alpha x_2}, \\
  g_0(x_1,x_2) = g_{01}(x_1)e^{-\beta x_2}.
\end{gather} O(\epsilon) \begin{gather}
  A(x_2) = B(x_2) = C_0e^{-\alpha x_2}, \\
  A(x_2) = B(x_2) = D_0e^{-\beta x_2}.
\end{gather} \alpha=\beta C_0=D_0=0 \alpha\ne\beta","['ordinary-differential-equations', 'systems-of-equations', 'perturbation-theory']"
56,Correct use of separation of variables for second order DE?,Correct use of separation of variables for second order DE?,,Is this the correct use of separation of variables? I multiped both sides by dx^2 then integrated. Please let me know if you find any mistakes. Thanks for any help.,Is this the correct use of separation of variables? I multiped both sides by dx^2 then integrated. Please let me know if you find any mistakes. Thanks for any help.,,"['calculus', 'integration', 'ordinary-differential-equations']"
57,"Solving $y'=f(x, y)$ where $\forall k>0. f(kx, ky) = f(x, y) \ne f(-x, -y) = f(-kx, -ky)$ (Tom Apostol Calculus vol. $1$, ex. $8.26.6$)","Solving  where  (Tom Apostol Calculus vol. , ex. )","y'=f(x, y) \forall k>0. f(kx, ky) = f(x, y) \ne f(-x, -y) = f(-kx, -ky) 1 8.26.6","Tom Apostol wrote that a homogeneous first-order equation is of a form $y' = f(x, y)$ , where $\forall x,y \in R. \forall t \ne 0. f(x, y) = f(tx, ty)$ . Now, his excercise $8.26.6$ is: $xy' = y - \sqrt{x^2 + y^2}$ Can be transformer into something which looks like a first-order homogeneous equation: $y' = \frac{y - \sqrt{x^2 + y^2}}{x}$ However, I'm not sure why it can be seen as such, because for negative $t$ , it does not hold that $f(x, y) = f(-x, -y)$ . Namely: $f(x, y) = \frac{y - \sqrt{x^2 + y^2}}{x}$ $f(-x, -y) = \frac{-y - \sqrt{x^2 + y^2}}{-x} = \frac{y + \sqrt{x^2 + y^2}}{x} \ne f(x, y)$ However, if we proceed and solve it as a homogeneous solution (e.g. by introducing $v = \frac{y}{x}$ ), we can solve it. Why is that justified, even though $f(x, y) = f(tx, ty)$ only for $t>0$ ? Thanks!","Tom Apostol wrote that a homogeneous first-order equation is of a form , where . Now, his excercise is: Can be transformer into something which looks like a first-order homogeneous equation: However, I'm not sure why it can be seen as such, because for negative , it does not hold that . Namely: However, if we proceed and solve it as a homogeneous solution (e.g. by introducing ), we can solve it. Why is that justified, even though only for ? Thanks!","y' = f(x, y) \forall x,y \in R. \forall t \ne 0. f(x, y) = f(tx, ty) 8.26.6 xy' = y - \sqrt{x^2 + y^2} y' = \frac{y - \sqrt{x^2 + y^2}}{x} t f(x, y) = f(-x, -y) f(x, y) = \frac{y - \sqrt{x^2 + y^2}}{x} f(-x, -y) = \frac{-y - \sqrt{x^2 + y^2}}{-x} = \frac{y + \sqrt{x^2 + y^2}}{x} \ne f(x, y) v = \frac{y}{x} f(x, y) = f(tx, ty) t>0",['ordinary-differential-equations']
58,Help with a limit of a certain differential equation.,Help with a limit of a certain differential equation.,,"I have an ODE $y’+a(t)y=b(t)$ with initial condition $y(0)=0$ . The functions $a(t),b(t)$ are continuous and each of them satisfy $$\lim_{t\rightarrow \infty}a(t)=A>0.$$ And $$\lim_{t\rightarrow \infty}b(t)=B>0.$$ When I graph $y$ , I see that $$\lim_{t\rightarrow \infty}y(t)=B/A.$$ .m, but I don’t know how to prove it analytically. Maybe it’s simple and I am overcomplicating. If I take the limit as $t\rightarrow \infty$ , then the ODE becomes $y’+Ay=B$ and intuitively, the conclusion is clear but I’m not sure if I can do such thing. Any help is appreciated!","I have an ODE with initial condition . The functions are continuous and each of them satisfy And When I graph , I see that .m, but I don’t know how to prove it analytically. Maybe it’s simple and I am overcomplicating. If I take the limit as , then the ODE becomes and intuitively, the conclusion is clear but I’m not sure if I can do such thing. Any help is appreciated!","y’+a(t)y=b(t) y(0)=0 a(t),b(t) \lim_{t\rightarrow \infty}a(t)=A>0. \lim_{t\rightarrow \infty}b(t)=B>0. y \lim_{t\rightarrow \infty}y(t)=B/A. t\rightarrow \infty y’+Ay=B","['real-analysis', 'ordinary-differential-equations', 'limits']"
59,Solving $2(y+e^x)dy + (y^2+4y e^x)dx = 0 $ and understanding integrating-factors,Solving  and understanding integrating-factors,2(y+e^x)dy + (y^2+4y e^x)dx = 0 ,"We want to solve $2(y+e^x)dy + (y^2+4y e^x)dx = 0 $ which, across the spectrum is the standard format of the integrating factor technique for ODE. My book, however, covers only the integrating factors of the forms \begin{align} \dfrac{1}{N(x,y)} \cdot \left( \dfrac{\partial M}{\partial y} - \dfrac{\partial N}{\partial x} \right)\\ \dfrac{1}{M(x,y)} \cdot \left( \dfrac{\partial N}{\partial x} - \dfrac{\partial M}{\partial y} \right) \end{align} where of course we want the first expression to be a function of $x$ only or the second to be a function of $y$ only. In the above example however, doing these calculations yields nothing like that, and cannot be covered by my book. How can one proceed further?","We want to solve which, across the spectrum is the standard format of the integrating factor technique for ODE. My book, however, covers only the integrating factors of the forms where of course we want the first expression to be a function of only or the second to be a function of only. In the above example however, doing these calculations yields nothing like that, and cannot be covered by my book. How can one proceed further?","2(y+e^x)dy + (y^2+4y e^x)dx = 0  \begin{align}
\dfrac{1}{N(x,y)} \cdot \left( \dfrac{\partial M}{\partial y} - \dfrac{\partial N}{\partial x} \right)\\
\dfrac{1}{M(x,y)} \cdot \left( \dfrac{\partial N}{\partial x} - \dfrac{\partial M}{\partial y} \right)
\end{align} x y","['ordinary-differential-equations', 'analysis', 'partial-differential-equations', 'integrating-factor']"
60,Find P if Integrating factor of $x(1-x^2)dy+(2x^2y-y-ax^3)dx=0$ is $e^{\int Pdx}$.,Find P if Integrating factor of  is .,x(1-x^2)dy+(2x^2y-y-ax^3)dx=0 e^{\int Pdx},"I have seen one method which correctly evaluates $P=\frac{2x^2-1}{x(1-x^2)}$ . But I have seen a method that says if $\frac{\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}}{N}$ is a function of just x ,i.e, it is equal to f(x) then its Integrating Factor (IF)= $e^{\int f(x)dx}$ Here M= $2x^2y-y-ax^3$ and N= $x(1-x^2)$ So, $\frac{\partial M}{\partial y}=2x^2-1$ and $\frac{\partial N}{\partial x}=1-3x^2$ Hence, $$\frac{\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}}{N}=\;\;\frac{5x^2-2}{x(1-x^2)}$$ Here's the link to the first method: https://haygot.s3.amazonaws.com/questions/1374381_1134731_ans_55dd44fc75824f228cd5c8286c3144f1.jpg Please tell why the answer is not matching.","I have seen one method which correctly evaluates . But I have seen a method that says if is a function of just x ,i.e, it is equal to f(x) then its Integrating Factor (IF)= Here M= and N= So, and Hence, Here's the link to the first method: https://haygot.s3.amazonaws.com/questions/1374381_1134731_ans_55dd44fc75824f228cd5c8286c3144f1.jpg Please tell why the answer is not matching.",P=\frac{2x^2-1}{x(1-x^2)} \frac{\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}}{N} e^{\int f(x)dx} 2x^2y-y-ax^3 x(1-x^2) \frac{\partial M}{\partial y}=2x^2-1 \frac{\partial N}{\partial x}=1-3x^2 \frac{\frac{\partial M}{\partial y}-\frac{\partial N}{\partial x}}{N}=\;\;\frac{5x^2-2}{x(1-x^2)},"['ordinary-differential-equations', 'solution-verification', 'partial-functions']"
61,Monotonicity of the solution of a differential equation,Monotonicity of the solution of a differential equation,,"I am thinking over this exercise: ""Given the differential equation $y'=g(y)$ , where $g$ is a continuous function in an interval $I$ , prove that every solution of this equation is monotonic in any interval of extrems $x_0$ and $x_1$ , where $x_0$ and $x_1$ are two consecutive zeros in $I$ of the function $g$ "". I have thought that, as in $(x_0,x_1)$ there are no zeros of $g$ , then there are no zeros of $y'$ . Consequently, there can be no maxima or minima in this interval, so there is no change of monotonicity. However,it seems to me as if this reasoning is not enough because, in that case, the problem would be so trivial. Thanks for your help.","I am thinking over this exercise: ""Given the differential equation , where is a continuous function in an interval , prove that every solution of this equation is monotonic in any interval of extrems and , where and are two consecutive zeros in of the function "". I have thought that, as in there are no zeros of , then there are no zeros of . Consequently, there can be no maxima or minima in this interval, so there is no change of monotonicity. However,it seems to me as if this reasoning is not enough because, in that case, the problem would be so trivial. Thanks for your help.","y'=g(y) g I x_0 x_1 x_0 x_1 I g (x_0,x_1) g y'",['ordinary-differential-equations']
62,Computation of adjoint operator,Computation of adjoint operator,,"I am struggeling with the computation of the adjoint operator $T^*$ to the following given operator $T$ : Let $T: \mathbb{R}^n \rightarrow L^2(0,1), u_0 \mapsto u(t)$ , where $u(t)$ is given via \begin{align*} u'(t) &= Au, t>0\\ u(0) &= u_0 \end{align*} I know that the adjoint operator is defined through the equation \begin{align*}  \langle T^*v, u_0 \rangle_2 = \langle v, Tu_0\rangle_{L^2(0,1)} \end{align*} Since $T$ is defined in an implicit way through the given differential equation, I assumed that $T^*$ has to be definite in a similar way through another ODE. I tried the following: \begin{align*} \langle T^*v, u_0 \rangle_2 = \langle v, Tu_0\rangle_{L^2(0,1)} = \int_0^1 v(t) \cdot Tu_0(t) dt = \int_0^1 v(t) \cdot u(t) dt  \end{align*} Then I tried to use integration by parts but that didn't seem to help since there are no derivatives in the integral. Can someone give me a hint? Edit: I am looking for a representation of $T^*$ that derives from the solution of the corresponding adjoint problem. So I tried to derive equations that have to be satisfied by $v$ . Unfortunately, I wasn't very successful so far. Thanks in advance:)","I am struggeling with the computation of the adjoint operator to the following given operator : Let , where is given via I know that the adjoint operator is defined through the equation Since is defined in an implicit way through the given differential equation, I assumed that has to be definite in a similar way through another ODE. I tried the following: Then I tried to use integration by parts but that didn't seem to help since there are no derivatives in the integral. Can someone give me a hint? Edit: I am looking for a representation of that derives from the solution of the corresponding adjoint problem. So I tried to derive equations that have to be satisfied by . Unfortunately, I wasn't very successful so far. Thanks in advance:)","T^* T T: \mathbb{R}^n \rightarrow L^2(0,1), u_0 \mapsto u(t) u(t) \begin{align*}
u'(t) &= Au, t>0\\
u(0) &= u_0
\end{align*} \begin{align*}
 \langle T^*v, u_0 \rangle_2 = \langle v, Tu_0\rangle_{L^2(0,1)}
\end{align*} T T^* \begin{align*}
\langle T^*v, u_0 \rangle_2 = \langle v, Tu_0\rangle_{L^2(0,1)} = \int_0^1 v(t) \cdot Tu_0(t) dt = \int_0^1 v(t) \cdot u(t) dt 
\end{align*} T^* v","['ordinary-differential-equations', 'adjoint-operators']"
63,How to solve a second order ODE with no constant coefficients?,How to solve a second order ODE with no constant coefficients?,,"Consider $x''(t)=a(t)x(t)$ on some time interval $[0,T]$ . For a first order equation one can formally divide by $x(t)$ and hence get the solution by the exponential function. For constants coeffcients one could take the ansatz $x(t)=e^{\lambda t}$ . Now both approaches do not seem to work...I would be grateful for any hints how to solve this kind of equations!",Consider on some time interval . For a first order equation one can formally divide by and hence get the solution by the exponential function. For constants coeffcients one could take the ansatz . Now both approaches do not seem to work...I would be grateful for any hints how to solve this kind of equations!,"x''(t)=a(t)x(t) [0,T] x(t) x(t)=e^{\lambda t}","['real-analysis', 'calculus', 'ordinary-differential-equations']"
64,About divergence of a vector field,About divergence of a vector field,,"Let $f:\mathbb{R}^n\to \mathbb{R}^n$ . Divergence of $f$ is defined as $$\operatorname{div}(f)=\sum_{i=1}^n \frac{\partial f_i}{\partial x_i}$$ I am reading a paper where it says that the divergence of a vector field defined by the odes $$\frac{dq_i}{dt}=\frac{\partial H}{\partial p_i},\frac{dp_i}{dt}=-\frac{\partial H}{\partial q_i}$$ for $i=1,\ldots, n$ where $H:\mathbb{R}^n\times \mathbb{R}^n\to \mathbb{R}$ is given by $$\sum_{i=1}^n \frac{\partial}{\partial q_i}\frac{dq_i}{dt}+\frac{\partial}{\partial p_i}\frac{dp_i}{dt}$$ and I don't understand how that follows from definition of divergence of a function and the odes. Any suggestions?",Let . Divergence of is defined as I am reading a paper where it says that the divergence of a vector field defined by the odes for where is given by and I don't understand how that follows from definition of divergence of a function and the odes. Any suggestions?,"f:\mathbb{R}^n\to \mathbb{R}^n f \operatorname{div}(f)=\sum_{i=1}^n \frac{\partial f_i}{\partial x_i} \frac{dq_i}{dt}=\frac{\partial H}{\partial p_i},\frac{dp_i}{dt}=-\frac{\partial H}{\partial q_i} i=1,\ldots, n H:\mathbb{R}^n\times \mathbb{R}^n\to \mathbb{R} \sum_{i=1}^n \frac{\partial}{\partial q_i}\frac{dq_i}{dt}+\frac{\partial}{\partial p_i}\frac{dp_i}{dt}",['ordinary-differential-equations']
65,Maxima/minima and monotonicity of $y(x)$ if $y(x)$ is solution of differential equation $\frac{dy}{dx}=y^2-1+\cos x$,Maxima/minima and monotonicity of  if  is solution of differential equation,y(x) y(x) \frac{dy}{dx}=y^2-1+\cos x,"On the open interval $(-c,c)$ , where $c$ is a positive real number, $y(x)$ is an infinitely differentiable solution of the differential equation $$\frac{dy}{dx}=y^2-1+\cos x$$ with the initial condition $y(0)=0$ . Then which one of the following is correct? (A) $y(x)$ has a local maximum at the origin (B) $y(x)$ has a local minimum at the origin (C) $y(x)$ is strictly increasing on the open interval $(-\delta,\delta)$ for some positive real number $\delta$ (D) $y(x)$ is strictly decreasing on the open interval $(-\delta,\delta)$ for some positive real number $\delta$ My Attempt Since $y(0)=0$ and function is differentiable and hence continuous so it should be very close to $0$ in the neighborhood of $x=0$ . So $y^2+\cos x$ could be greater than $1$ in the neighborhood of $x=0$ .On the other hand the opposite may as well be true.So, $(C)$ or $(D)$ may be correct. But what will be actual/logical explanation.","On the open interval , where is a positive real number, is an infinitely differentiable solution of the differential equation with the initial condition . Then which one of the following is correct? (A) has a local maximum at the origin (B) has a local minimum at the origin (C) is strictly increasing on the open interval for some positive real number (D) is strictly decreasing on the open interval for some positive real number My Attempt Since and function is differentiable and hence continuous so it should be very close to in the neighborhood of . So could be greater than in the neighborhood of .On the other hand the opposite may as well be true.So, or may be correct. But what will be actual/logical explanation.","(-c,c) c y(x) \frac{dy}{dx}=y^2-1+\cos x y(0)=0 y(x) y(x) y(x) (-\delta,\delta) \delta y(x) (-\delta,\delta) \delta y(0)=0 0 x=0 y^2+\cos x 1 x=0 (C) (D)","['real-analysis', 'calculus', 'ordinary-differential-equations', 'maxima-minima', 'monotone-functions']"
66,"$y''+y=x^2+1, y(\pi)=\pi^2, y'(\pi)=2\pi$ - By Laplace Transform",- By Laplace Transform,"y''+y=x^2+1, y(\pi)=\pi^2, y'(\pi)=2\pi","I am solving the following IVP by Laplace Transform: $$y''+y=x^2+1,\qquad y(\pi)=\pi^2, \qquad y'(\pi)=2\pi$$ Let $f(x)=u_{\pi}(x)y(x-\pi).$ Then, $$f''(x)+f'(x)=u_{\pi}(x)(x-\pi)^2+u_{\pi}(x), \qquad f(0)=\pi ^2, \qquad f'(0)=2\pi.$$ Using the Laplace Transform and writing $F:=\mathcal{L}\{f\}(s)$ , we have $$s^2F-sf(0)-f'(0)+F=e^{-\pi s}\dfrac{s+1}{s^2},$$ $$(s^2+1)F-(s+1)\pi^2-2\pi=e^{-\pi s}\dfrac{s+1}{s^2},$$ $$F=e^{-\pi s}\dfrac{s+1}{s^2(s^2+1)}+\dfrac{(s+1)\pi^2}{(s^2 +1)}+\dfrac{2\pi}{(s^2+1)}.$$ Be $$\dfrac{s+1}{s^2(s^2+1)}=\dfrac{As+B}{s^2}+\dfrac{Cs+D}{s^2+1}. $$ We have $$As^3+As+Bs^2+B+Cs^3+Ds^2=s+1,$$ $$A+C=0, B+D=0, A=1, B=1,$$ $$C=-1, D=-1, A=1, B=1.$$ So, $$\dfrac{s+1}{s^2(s^2+1)}=\dfrac{s+1}{s^2}-\dfrac{s+1}{s^2+1}= \dfrac{1}{s}+\dfrac{1}{s^2}-\dfrac{s}{s^2+1}-\dfrac{1}{s^2+1}.$$ Then, $$F=e^{-\pi s}\left(\dfrac{1}{s}+\dfrac{1}{s^2}-\dfrac{s}{s^2+1}-\dfrac {1}{s^2+1}\right)+\pi^2\left(\dfrac{s}{s^2+1}+\dfrac{1}{s^2+1}\right)+ 2\pi\dfrac{1}{s^2+1}.$$ By the inverse transform, $$f(x)=u_\pi(x)\left(1+(x-\pi)-\cos(x-\pi)-\sin(x-\pi)\right)+\pi^2 \left(\cos(x)+\sin(x)\right)+2\pi\sin(x).$$ Returning to the variable $y$ and remembering that $\cos(x+\pi)=-\cos(x)$ and $\sin(x+\pi)=-\sin(x)$ , we have $$y(x)=1+x-\cos(x)-\sin(x)+\pi^2(-\cos(x)-\sin(x))-2\pi \sin (x) .$$ However, the Wolfram's solution is $y(x)=-1 + x^2 - \cos(x)$ . Thank you in advance!","I am solving the following IVP by Laplace Transform: Let Then, Using the Laplace Transform and writing , we have Be We have So, Then, By the inverse transform, Returning to the variable and remembering that and , we have However, the Wolfram's solution is . Thank you in advance!","y''+y=x^2+1,\qquad y(\pi)=\pi^2, \qquad y'(\pi)=2\pi f(x)=u_{\pi}(x)y(x-\pi). f''(x)+f'(x)=u_{\pi}(x)(x-\pi)^2+u_{\pi}(x), \qquad f(0)=\pi ^2, \qquad f'(0)=2\pi. F:=\mathcal{L}\{f\}(s) s^2F-sf(0)-f'(0)+F=e^{-\pi s}\dfrac{s+1}{s^2}, (s^2+1)F-(s+1)\pi^2-2\pi=e^{-\pi s}\dfrac{s+1}{s^2}, F=e^{-\pi s}\dfrac{s+1}{s^2(s^2+1)}+\dfrac{(s+1)\pi^2}{(s^2 +1)}+\dfrac{2\pi}{(s^2+1)}. \dfrac{s+1}{s^2(s^2+1)}=\dfrac{As+B}{s^2}+\dfrac{Cs+D}{s^2+1}.  As^3+As+Bs^2+B+Cs^3+Ds^2=s+1, A+C=0, B+D=0, A=1, B=1, C=-1, D=-1, A=1, B=1. \dfrac{s+1}{s^2(s^2+1)}=\dfrac{s+1}{s^2}-\dfrac{s+1}{s^2+1}= \dfrac{1}{s}+\dfrac{1}{s^2}-\dfrac{s}{s^2+1}-\dfrac{1}{s^2+1}. F=e^{-\pi s}\left(\dfrac{1}{s}+\dfrac{1}{s^2}-\dfrac{s}{s^2+1}-\dfrac {1}{s^2+1}\right)+\pi^2\left(\dfrac{s}{s^2+1}+\dfrac{1}{s^2+1}\right)+ 2\pi\dfrac{1}{s^2+1}. f(x)=u_\pi(x)\left(1+(x-\pi)-\cos(x-\pi)-\sin(x-\pi)\right)+\pi^2 \left(\cos(x)+\sin(x)\right)+2\pi\sin(x). y \cos(x+\pi)=-\cos(x) \sin(x+\pi)=-\sin(x) y(x)=1+x-\cos(x)-\sin(x)+\pi^2(-\cos(x)-\sin(x))-2\pi \sin (x) . y(x)=-1 + x^2 - \cos(x)","['ordinary-differential-equations', 'laplace-transform', 'initial-value-problems']"
67,Nonlinear differential equation $u''(r) = (r^2 -3) u(r)$,Nonlinear differential equation,u''(r) = (r^2 -3) u(r),"I am trying to solve the following equation, this being a form of the Schrodinger equation for the harmonic oscillator, E=3 being the 1st energy level: $u''(r) = (r^2 - 3) u(r)$ I am going trough a book on numerical analysis and this is solved with the Numerov method, the exact solution is only cited: $A r e^{-r^2/2}$ How would I go about obtaining this solution? I tried Laplace transforms but this being a nonlinear equation, that method won't work, even Mathematica failed me Thanks in advance","I am trying to solve the following equation, this being a form of the Schrodinger equation for the harmonic oscillator, E=3 being the 1st energy level: I am going trough a book on numerical analysis and this is solved with the Numerov method, the exact solution is only cited: How would I go about obtaining this solution? I tried Laplace transforms but this being a nonlinear equation, that method won't work, even Mathematica failed me Thanks in advance",u''(r) = (r^2 - 3) u(r) A r e^{-r^2/2},"['ordinary-differential-equations', 'quantum-mechanics']"
68,Solving $-u''+u=\delta'(x-1)$ using the Fourier transform,Solving  using the Fourier transform,-u''+u=\delta'(x-1),"Using Fourier transforms, solve the following boundary value problem $$-u''+u=\delta'(x-1)$$ where $\delta$ stands for the Dirac delta function, with $u(x) \to 0$ as $\lvert x \rvert \to \infty$ . I applied Fourier transforms to both sides of the equation and I arrived to the conclusion that the Fourier transform of the solution must be $$u_F(k)=\frac{ike^{-ik}}{\sqrt{2\pi}(k^2+1)}$$ Then, using a Foruier transforms table, I get the following solution $u(x)=e^{x-1}/2$ if ( $x<1$ ) and $u(x)=-e^{1-x}/2$ if ( $x\geq 1$ ), which is the derivative of $e^{-\lvert x-1\rvert}/2$ . As this function has Fourier transform $\frac{e^{-ik}}{\sqrt{2\pi}(k^2+1)}$ , its derivative has Fourier transform $ik\frac{e^{-ik}}{\sqrt{2\pi}(k^2+1)}$ , and hence should be the solution to the boundary problem. The problem is this function is not even continuous at $x=1$ . What exactly is it that I'm doing wrong? Or, if I've done everything right so far, how can I fix this discontinuity problem?","Using Fourier transforms, solve the following boundary value problem where stands for the Dirac delta function, with as . I applied Fourier transforms to both sides of the equation and I arrived to the conclusion that the Fourier transform of the solution must be Then, using a Foruier transforms table, I get the following solution if ( ) and if ( ), which is the derivative of . As this function has Fourier transform , its derivative has Fourier transform , and hence should be the solution to the boundary problem. The problem is this function is not even continuous at . What exactly is it that I'm doing wrong? Or, if I've done everything right so far, how can I fix this discontinuity problem?",-u''+u=\delta'(x-1) \delta u(x) \to 0 \lvert x \rvert \to \infty u_F(k)=\frac{ike^{-ik}}{\sqrt{2\pi}(k^2+1)} u(x)=e^{x-1}/2 x<1 u(x)=-e^{1-x}/2 x\geq 1 e^{-\lvert x-1\rvert}/2 \frac{e^{-ik}}{\sqrt{2\pi}(k^2+1)} ik\frac{e^{-ik}}{\sqrt{2\pi}(k^2+1)} x=1,"['ordinary-differential-equations', 'fourier-analysis', 'fourier-transform', 'distribution-theory', 'dirac-delta']"
69,How do I solve this nonlinear ODE given the asymptotic series solutions as follows?,How do I solve this nonlinear ODE given the asymptotic series solutions as follows?,,"Differential Equation: $$-{\frac { \left( {\frac {\rm d}{{\rm d}R}}f \left( R \right)  \right)  ^{2}}{2\,f \left( R \right) }}+{\frac {{\rm d}^{2}}{{\rm d}{R}^{2}}}f  \left( R \right) +{\frac {{\frac {\rm d}{{\rm d}R}}f \left( R  \right) }{R}}+2\,f \left( R \right) -2\, \left( f \left( R \right)   \right) ^{2}-2\,{\frac {{B}^{2}}{f \left( R \right) {R}^{2}}}=0. $$ Series Solution at $R \to 0$ : $${R}^{-2}+1+ \left( {\frac {{B}^{2}}{3}}+{\frac{1}{3}} \right) {R}^{2} + \left( {\frac{2}{33}}+{\frac {2\,{B}^{2}}{33}} \right) {R}^{4}+  \left( {\frac{31}{2277}}+{\frac {53\,{B}^{2}}{2277}}+{\frac {2\,{B}^{ 4}}{207}} \right) {R}^{6}+ \left( {\frac{70}{29601}}+{\frac {136\,{B}^ {2}}{29601}}+{\frac {2\,{B}^{4}}{897}} \right) {R}^{8}+O \left( {R}^{ 10} \right).$$ Series Solution as $R \to \infty$ : $$1-{\frac {{B}^{2}}{{R}^{2}}}+{\frac {-2\,{B}^{4}-2\,{B}^{2}}{{R}^{4}}} +{\frac {-7\,{B}^{6}-23\,{B}^{4}-16\,{B}^{2}}{{R}^{6}}}+{\frac {-30\,{ B}^{8}-216\,{B}^{6}-474\,{B}^{4}-288\,{B}^{2}}{{R}^{8}}}+O \left( {R}^ {-10} \right) .$$ B is a free parameter. I tried solving it like a boundary value problem using series solution at very small R and large R as boundary conditions. I tried Newton iteration and imaginary time propagation for that. But that worked only for B=1. If I try solving it as an initial value problem starting from some Rmax, even RKF45 doesn't give good result. Can someone please suggest either analytical or numerical way of solving this equation? Or perhaps a way of analyzing the properties of this differential equation other than frobenius series solution? (If the questions lacks details, please let me know before downvoting) Edit: 2 downvotes without any explanation. I mean if you don’t wanna respond, then don’t respond. Why do you have to ruin my chances of getting any help? This is the worst forum. Most of the times people just keep downvoting without any explanation.","Differential Equation: Series Solution at : Series Solution as : B is a free parameter. I tried solving it like a boundary value problem using series solution at very small R and large R as boundary conditions. I tried Newton iteration and imaginary time propagation for that. But that worked only for B=1. If I try solving it as an initial value problem starting from some Rmax, even RKF45 doesn't give good result. Can someone please suggest either analytical or numerical way of solving this equation? Or perhaps a way of analyzing the properties of this differential equation other than frobenius series solution? (If the questions lacks details, please let me know before downvoting) Edit: 2 downvotes without any explanation. I mean if you don’t wanna respond, then don’t respond. Why do you have to ruin my chances of getting any help? This is the worst forum. Most of the times people just keep downvoting without any explanation.","-{\frac { \left( {\frac {\rm d}{{\rm d}R}}f \left( R \right)  \right) 
^{2}}{2\,f \left( R \right) }}+{\frac {{\rm d}^{2}}{{\rm d}{R}^{2}}}f
 \left( R \right) +{\frac {{\frac {\rm d}{{\rm d}R}}f \left( R
 \right) }{R}}+2\,f \left( R \right) -2\, \left( f \left( R \right) 
 \right) ^{2}-2\,{\frac {{B}^{2}}{f \left( R \right) {R}^{2}}}=0.  R \to 0 {R}^{-2}+1+ \left( {\frac {{B}^{2}}{3}}+{\frac{1}{3}} \right) {R}^{2}
+ \left( {\frac{2}{33}}+{\frac {2\,{B}^{2}}{33}} \right) {R}^{4}+
 \left( {\frac{31}{2277}}+{\frac {53\,{B}^{2}}{2277}}+{\frac {2\,{B}^{
4}}{207}} \right) {R}^{6}+ \left( {\frac{70}{29601}}+{\frac {136\,{B}^
{2}}{29601}}+{\frac {2\,{B}^{4}}{897}} \right) {R}^{8}+O \left( {R}^{
10} \right). R \to \infty 1-{\frac {{B}^{2}}{{R}^{2}}}+{\frac {-2\,{B}^{4}-2\,{B}^{2}}{{R}^{4}}}
+{\frac {-7\,{B}^{6}-23\,{B}^{4}-16\,{B}^{2}}{{R}^{6}}}+{\frac {-30\,{
B}^{8}-216\,{B}^{6}-474\,{B}^{4}-288\,{B}^{2}}{{R}^{8}}}+O \left( {R}^
{-10} \right) .","['ordinary-differential-equations', 'numerical-methods', 'asymptotics', 'nonlinear-system', 'frobenius-method']"
70,Show $-\sin(x)$ is a solution to the same ODE,Show  is a solution to the same ODE,-\sin(x),"Let $\psi:\mathbb{R}\rightarrow\mathbb{R}$ be any function. Suppose $\cos(x)$ is a solution to the ODE $y'=\psi(y)$ over $\mathbb{R}$ . Show $-\sin(x)$ is a solution to the same ODE. Attempt: It is true that $-\sin(x)=\cos(x+\pi/2)$ , for all $x\in\mathbb{R}$ . Therefore, $$\frac{d}{dx}(-\sin(x))=\cos'(x+\pi/2)=\psi(\cos(x+\pi/2))=\psi(-\sin(x)).$$ Since $\frac{d}{dx}(-\sin(x))=\psi(-\sin(x))$ , $-\sin(x)$ must be a solution to the same ODE. There is an answer for this question at Show that satisfy the ODE . I'm wondering if my solution is 1. correct and 2. equivalent to the previous answer. The previous answer was: You could observe that $$-\sin(x) = \cos\left(\frac{\pi}{2}+x\right)$$ and that if $$y(x) = \cos(x)$$ then $y'(x) =-\sin(x) = f(\cos(x))$ and so we can say that $$y(x+\pi/2) = \cos(x+\pi/2)$$ which implies that $$y'(x+\pi/2) =-\sin(x+\pi/2) = f(\cos(x+\pi/2)).$$","Let be any function. Suppose is a solution to the ODE over . Show is a solution to the same ODE. Attempt: It is true that , for all . Therefore, Since , must be a solution to the same ODE. There is an answer for this question at Show that satisfy the ODE . I'm wondering if my solution is 1. correct and 2. equivalent to the previous answer. The previous answer was: You could observe that and that if then and so we can say that which implies that",\psi:\mathbb{R}\rightarrow\mathbb{R} \cos(x) y'=\psi(y) \mathbb{R} -\sin(x) -\sin(x)=\cos(x+\pi/2) x\in\mathbb{R} \frac{d}{dx}(-\sin(x))=\cos'(x+\pi/2)=\psi(\cos(x+\pi/2))=\psi(-\sin(x)). \frac{d}{dx}(-\sin(x))=\psi(-\sin(x)) -\sin(x) -\sin(x) = \cos\left(\frac{\pi}{2}+x\right) y(x) = \cos(x) y'(x) =-\sin(x) = f(\cos(x)) y(x+\pi/2) = \cos(x+\pi/2) y'(x+\pi/2) =-\sin(x+\pi/2) = f(\cos(x+\pi/2)).,"['ordinary-differential-equations', 'solution-verification']"
71,Solve the following differential equation: $xy''-\cos(x)y'+\sin(x)y=2$,Solve the following differential equation:,xy''-\cos(x)y'+\sin(x)y=2,"Solve the following differential equation: $$xy''-\cos(x)y'+\sin(x)y=2$$ We have that $f_1(x)=x,f_2(x)=-\cos(x),f_3(x)=\sin(x)$ And since $f''_1(x)-f'_2(x)+f_3(x)=0$ so the second order differential equation is exact, hence we should solve: $$\frac{d}{dx}\left[f_{1}\left(x\right)y'+\left(f_{2}\left(x\right)-f'_{1}\left(x\right)\right)y'\right]=r\left(x\right)$$ Or equivalently $$xy'+\left(-\cos\left(x\right)-1\right)y=2\int_{ }^{ }dx$$ From which we conclude $$y'+\frac{\left(-\cos\left(x\right)-1\right)}{x}y=2+\frac{c_{1}}{x}$$ So $$y=e^{-\int_{ }^{ }\frac{\left(-\cos\left(x\right)-1\right)}{x}dx}\left(\int_{ }^{ }\left(2+\frac{c_{1}}{x}\right)e^{\int_{ }^{ }\frac{\left(-\cos\left(x\right)-1\right)}{x}dx}dx+c_{2}\right)$$ But $$e^{\int_{ }^{ }\frac{\left(-\cos\left(x\right)-1\right)}{x}dx}$$ doesn't have a closed form, So what should I do?","Solve the following differential equation: We have that And since so the second order differential equation is exact, hence we should solve: Or equivalently From which we conclude So But doesn't have a closed form, So what should I do?","xy''-\cos(x)y'+\sin(x)y=2 f_1(x)=x,f_2(x)=-\cos(x),f_3(x)=\sin(x) f''_1(x)-f'_2(x)+f_3(x)=0 \frac{d}{dx}\left[f_{1}\left(x\right)y'+\left(f_{2}\left(x\right)-f'_{1}\left(x\right)\right)y'\right]=r\left(x\right) xy'+\left(-\cos\left(x\right)-1\right)y=2\int_{ }^{ }dx y'+\frac{\left(-\cos\left(x\right)-1\right)}{x}y=2+\frac{c_{1}}{x} y=e^{-\int_{ }^{ }\frac{\left(-\cos\left(x\right)-1\right)}{x}dx}\left(\int_{ }^{ }\left(2+\frac{c_{1}}{x}\right)e^{\int_{ }^{ }\frac{\left(-\cos\left(x\right)-1\right)}{x}dx}dx+c_{2}\right) e^{\int_{ }^{ }\frac{\left(-\cos\left(x\right)-1\right)}{x}dx}",['ordinary-differential-equations']
72,On the domain of a solution to a differential equation,On the domain of a solution to a differential equation,,"When solving elementary, separable ordinary differential equations (ODEs), and obtaining the original family of solutions: How do I know the domain over which the family of solutions satisfies the differential equation? Does it satisfy it for the entire $\Bbb R$ ? If so, can I always find a particular solution going through any point on the $xy$ plane?","When solving elementary, separable ordinary differential equations (ODEs), and obtaining the original family of solutions: How do I know the domain over which the family of solutions satisfies the differential equation? Does it satisfy it for the entire ? If so, can I always find a particular solution going through any point on the plane?",\Bbb R xy,['ordinary-differential-equations']
73,How to simplify this differential equation of orbital motion,How to simplify this differential equation of orbital motion,,"While solving for expression of orbit of a body under central force , this method used some substitution to simplify given problem But I cannot understand the simplification of $\left(\frac{\mathrm{d} r}{\mathrm{~d} \theta}\right)^{2}=\left(\frac{L^{2}}{k m}\right)^{2} \frac{1}{u^{4}}\left(\frac{\mathrm{d} u}{\mathrm{~d} \theta}\right)^{2}$ into $u^{\prime}(\theta)^{2}=-u^{2}+2 u+\frac{2 E L^{2}}{m k^{2}}$ so, anyone can please describe it in bit more detailed steps","While solving for expression of orbit of a body under central force , this method used some substitution to simplify given problem But I cannot understand the simplification of into so, anyone can please describe it in bit more detailed steps",\left(\frac{\mathrm{d} r}{\mathrm{~d} \theta}\right)^{2}=\left(\frac{L^{2}}{k m}\right)^{2} \frac{1}{u^{4}}\left(\frac{\mathrm{d} u}{\mathrm{~d} \theta}\right)^{2} u^{\prime}(\theta)^{2}=-u^{2}+2 u+\frac{2 E L^{2}}{m k^{2}},"['calculus', 'ordinary-differential-equations', 'physics']"
74,"Solve : $y''+a^2y=\sin(bx) ,a,b\in \mathbb{R}$",Solve :,"y''+a^2y=\sin(bx) ,a,b\in \mathbb{R}","Solve : $y''+a^2y=\sin(bx) ,a,b\in  \mathbb{R}$ My solution : Suppose the solution form is: $$y(x)=x(A\cos(bx)+B\sin(bx))$$ $$\implies y''=-2Ab\sin(bx)+2Bb\cos(bx)+x(-Ab^2\cos(bx)-Bb^2\sin(bx))$$ Subtitue $y,\,y''$ in the original ode: $$-2Ab\sin(bx)+2Bb\cos(bx)+x(-Ab^2\cos(bx)-Bb^2\sin(bx))+a^2(x(A\cos(bx)+B\sin(bx))=\sin(bx)$$ Then, \begin{cases} \begin{align} -Ab^2+a^2A=0 \\ -Bb^2+a^2B=0 \\ \end{align} \end{cases} \begin{cases} \begin{align} -2Bb=0 \\ -2Ab=1=0 \\ \end{align} \end{cases} I get $A=B=0$ . Where am I getting wrong? I've tried to solve it by guessing an Ansatz .","Solve : My solution : Suppose the solution form is: Subtitue in the original ode: Then, I get . Where am I getting wrong? I've tried to solve it by guessing an Ansatz .","y''+a^2y=\sin(bx) ,a,b\in 
\mathbb{R} y(x)=x(A\cos(bx)+B\sin(bx)) \implies y''=-2Ab\sin(bx)+2Bb\cos(bx)+x(-Ab^2\cos(bx)-Bb^2\sin(bx)) y,\,y'' -2Ab\sin(bx)+2Bb\cos(bx)+x(-Ab^2\cos(bx)-Bb^2\sin(bx))+a^2(x(A\cos(bx)+B\sin(bx))=\sin(bx) \begin{cases}
\begin{align}
-Ab^2+a^2A=0 \\
-Bb^2+a^2B=0 \\
\end{align}
\end{cases} \begin{cases}
\begin{align}
-2Bb=0 \\
-2Ab=1=0 \\
\end{align}
\end{cases} A=B=0",['ordinary-differential-equations']
75,Transfinite limit of an orbit of a dynamical system and rationalizability of symmetric games.,Transfinite limit of an orbit of a dynamical system and rationalizability of symmetric games.,,"Consider a dynamical system and let $v(t)=(v^1(t),\dots,v^n(t))$ be an orbit. Suppose we are able to prove, for some $k=1,\dots,n$ : $$\lim_{t\to\infty} v^k(t)=0$$ Of course, we cannot be sure that $\exists T\ v^k(T)=0$ . In any arbitrary large, but finite time, the limit point can only be approximated. Intuitively, it takes $\omega$ steps to reach the limit point. Then, we can imagine of ""placing the system in its limit point"" and study its behavior iterating from there. In a sense, study the $\omega+\omega$ -limit of the system. My first question is: Is there something as the ""transfinite limit"" of a dynamical system? Can we formalize the previous idea, maybe using nets associated to the orbits of the system? I am asking this because in (evolutionary) game theory an interesting result hold, in words: considering a symmetric normal form game, we can associate to it a dynamical system describing how it would be played by a population of players interacting by imitation. This is a replicator dynamics, and the orbits of the system describe the evolution of of the frequency with which an action is played by the population. Then, one can prove that in the limit, the frequency with which a dominated action is played tend to $0$ .  An interesting set of actions is the set of rationalizable or equivalently iteratively undominated actions. Is there a ""limit"" in which the population plays only rationalizable actions? If the previous idea can be formalized, I would say that the answer is positive: in the first $\omega$ steps dominated actions are played with frequency $0$ , hence eliminated. Then, $\omega+\omega$ steps the  dominated actions of the resulting subgames are played with frequency $0$ and hence deleted, and so on until in some $\omega+\dots+\omega$ steps only rationalizable actions are played with positive probability. My second curiosity is the: If the ""transfinite limit"" of  a dynamical system is studied, are there general results linking the behavior at various ordinal levels?","Consider a dynamical system and let be an orbit. Suppose we are able to prove, for some : Of course, we cannot be sure that . In any arbitrary large, but finite time, the limit point can only be approximated. Intuitively, it takes steps to reach the limit point. Then, we can imagine of ""placing the system in its limit point"" and study its behavior iterating from there. In a sense, study the -limit of the system. My first question is: Is there something as the ""transfinite limit"" of a dynamical system? Can we formalize the previous idea, maybe using nets associated to the orbits of the system? I am asking this because in (evolutionary) game theory an interesting result hold, in words: considering a symmetric normal form game, we can associate to it a dynamical system describing how it would be played by a population of players interacting by imitation. This is a replicator dynamics, and the orbits of the system describe the evolution of of the frequency with which an action is played by the population. Then, one can prove that in the limit, the frequency with which a dominated action is played tend to .  An interesting set of actions is the set of rationalizable or equivalently iteratively undominated actions. Is there a ""limit"" in which the population plays only rationalizable actions? If the previous idea can be formalized, I would say that the answer is positive: in the first steps dominated actions are played with frequency , hence eliminated. Then, steps the  dominated actions of the resulting subgames are played with frequency and hence deleted, and so on until in some steps only rationalizable actions are played with positive probability. My second curiosity is the: If the ""transfinite limit"" of  a dynamical system is studied, are there general results linking the behavior at various ordinal levels?","v(t)=(v^1(t),\dots,v^n(t)) k=1,\dots,n \lim_{t\to\infty} v^k(t)=0 \exists T\ v^k(T)=0 \omega \omega+\omega 0 \omega 0 \omega+\omega 0 \omega+\dots+\omega","['real-analysis', 'ordinary-differential-equations', 'dynamical-systems', 'game-theory', 'ordinals']"
76,"Periodic solution of ODE, why does my counterexample not work?","Periodic solution of ODE, why does my counterexample not work?",,"I am considering the ordinary differential equation of form $\frac{dN}{dt}= f(N)$ , for example $\frac{dN}{dt} = N$ . I know that this kind of ode has no non-constant periodic solution. However, I am thinking about $N(t)=\sin(t)$ . Since I can write $$ \frac{dN}{dt} = \cos(t) = \sqrt{1-N^2} :=f(N).$$ So the above differential equation does have periodic solution. I don't understand why my example is not a good counterexample. My feeling is that my example has restriction on N ( $-1\leq N\leq1$ ). Am I correct?","I am considering the ordinary differential equation of form , for example . I know that this kind of ode has no non-constant periodic solution. However, I am thinking about . Since I can write So the above differential equation does have periodic solution. I don't understand why my example is not a good counterexample. My feeling is that my example has restriction on N ( ). Am I correct?",\frac{dN}{dt}= f(N) \frac{dN}{dt} = N N(t)=\sin(t)  \frac{dN}{dt} = \cos(t) = \sqrt{1-N^2} :=f(N). -1\leq N\leq1,['ordinary-differential-equations']
77,Why is it true that the solution to this IVP is always $<3$ when $x\geq 0$ (without solving the equation)?,Why is it true that the solution to this IVP is always  when  (without solving the equation)?,<3 x\geq 0,"I'm doing some more practice problems for my upcoming DEs test, and I tried this true/false question: The solution to the initial value problem $dy/dx=(x-2)(y-3)^2,y(0)=0$ , will always be less than $3$ ; that is, $y(x)<3$ for $x\geq 0$ . Bear in mind that the question assumes the person doing the problem doesn't know how to solve DEs yet, since that is covered in the subsequent chapter. Using the uniqueness and exactness theorem, given that $dy/dx=F(x,y)$ , I checked if $F$ and $\partial F/\partial y$ were continuous at the point $(0,0)$ by inputting the point $(0,0)$ into both. If they are continuous, there exists a rectangle $R$ that includes the point $(0,0)$ in which $F$ and $\partial F/\partial y$ are continuous. $F(0,0)=-2(-3)^2=-18$ $\displaystyle\frac{\partial F}{\partial y}=2(x-2)(y-3)$ $\displaystyle\frac{\partial F}{\partial y}\bigg|_{(0,0)}=2(-2)(-3)=12$ Since the limits of $F$ and $\partial F/\partial y$ can be solved via substitution, the limits are the same as the evaluations of the points, and they are both continuous at any point. Therefore, there exists a rectangle $R$ that includes $(0,0)$ where both are continuous, meaning there is a unique solution to the initial value problem. However, I at first concluded that since $\partial F/\partial y$ is positive, $y$ is increasing at $(0,0)$ , and the statement is false. However, the answer is that the statement is true. I've thought about it a bit more and realized that since $dy/dx$ is negative at that point, $y$ is actually decreasing at $(0,0)$ , not increasing! Also, we can't conclude anything about the statement from the values at $(0,0)$ besides the fact that only one unique solution exists. So I tried setting $dy/dx$ to $0$ to see if I could find any critical points, in an attempt to see if there's a maximum at $y=3$ . $dy/dx=(x-2)(y-3)^2=0 \implies x=2\;\text{or}\;y=3\implies$ Critical value at $y=3$ . But the second derivative test was inconclusive: $\begin{align}\displaystyle\frac{d^2 y}{dx^2}&=(y-3)^2+2(x-2)(y-3)\displaystyle\frac{dy}{dx}\\ &=(y-3)^2+2(x-2)(y-3)(x-2)(y-3)^2\\ &=(y-3)^2+2(x-2)^2(y-3)^3\\ \displaystyle\frac{d^2y}{dx^2}\bigg|_{y=3}&=(3-3)^2+(x-2)^2(3-3)^3\\ &=0 \end{align}$ So that didn't work! However, I just tried solving the DE to see what would happen since the test I'm taking covers solving separable DEs anyway: $\begin{align} \displaystyle\frac{dy}{dx}&=(x-2)(y-3)^2 \\ \displaystyle\int\frac{dy}{(y-3)^2}&=\int(x-2)dx\\ -\displaystyle\frac{1}{y-3}&=x^2-2x+C \\ \end{align}$ Applying the initial condition: $-\displaystyle\frac{1}{0-3}=(0)^2-2(0)+C\implies C=\displaystyle\frac{1}{3}$ . Therefore the unique solution is $y=-\displaystyle\frac{1}{x^2-2x+\frac{1}{3}}+3$ It can now be easily seen that the maximum of this expression is $y=3$ since $\lim_{x\to \infty} y(x)=3$ . But is there any way to determine this without solving the equation?","I'm doing some more practice problems for my upcoming DEs test, and I tried this true/false question: The solution to the initial value problem , will always be less than ; that is, for . Bear in mind that the question assumes the person doing the problem doesn't know how to solve DEs yet, since that is covered in the subsequent chapter. Using the uniqueness and exactness theorem, given that , I checked if and were continuous at the point by inputting the point into both. If they are continuous, there exists a rectangle that includes the point in which and are continuous. Since the limits of and can be solved via substitution, the limits are the same as the evaluations of the points, and they are both continuous at any point. Therefore, there exists a rectangle that includes where both are continuous, meaning there is a unique solution to the initial value problem. However, I at first concluded that since is positive, is increasing at , and the statement is false. However, the answer is that the statement is true. I've thought about it a bit more and realized that since is negative at that point, is actually decreasing at , not increasing! Also, we can't conclude anything about the statement from the values at besides the fact that only one unique solution exists. So I tried setting to to see if I could find any critical points, in an attempt to see if there's a maximum at . Critical value at . But the second derivative test was inconclusive: So that didn't work! However, I just tried solving the DE to see what would happen since the test I'm taking covers solving separable DEs anyway: Applying the initial condition: . Therefore the unique solution is It can now be easily seen that the maximum of this expression is since . But is there any way to determine this without solving the equation?","dy/dx=(x-2)(y-3)^2,y(0)=0 3 y(x)<3 x\geq 0 dy/dx=F(x,y) F \partial F/\partial y (0,0) (0,0) R (0,0) F \partial F/\partial y F(0,0)=-2(-3)^2=-18 \displaystyle\frac{\partial F}{\partial y}=2(x-2)(y-3) \displaystyle\frac{\partial F}{\partial y}\bigg|_{(0,0)}=2(-2)(-3)=12 F \partial F/\partial y R (0,0) \partial F/\partial y y (0,0) dy/dx y (0,0) (0,0) dy/dx 0 y=3 dy/dx=(x-2)(y-3)^2=0 \implies x=2\;\text{or}\;y=3\implies y=3 \begin{align}\displaystyle\frac{d^2 y}{dx^2}&=(y-3)^2+2(x-2)(y-3)\displaystyle\frac{dy}{dx}\\
&=(y-3)^2+2(x-2)(y-3)(x-2)(y-3)^2\\
&=(y-3)^2+2(x-2)^2(y-3)^3\\
\displaystyle\frac{d^2y}{dx^2}\bigg|_{y=3}&=(3-3)^2+(x-2)^2(3-3)^3\\
&=0
\end{align} \begin{align}
\displaystyle\frac{dy}{dx}&=(x-2)(y-3)^2 \\
\displaystyle\int\frac{dy}{(y-3)^2}&=\int(x-2)dx\\
-\displaystyle\frac{1}{y-3}&=x^2-2x+C \\
\end{align} -\displaystyle\frac{1}{0-3}=(0)^2-2(0)+C\implies C=\displaystyle\frac{1}{3} y=-\displaystyle\frac{1}{x^2-2x+\frac{1}{3}}+3 y=3 \lim_{x\to \infty} y(x)=3","['ordinary-differential-equations', 'initial-value-problems']"
78,Second-order nonlinear ODE $d^{2}y/dx^{2} =y^{-1/2}f(x)$,Second-order nonlinear ODE,d^{2}y/dx^{2} =y^{-1/2}f(x),"Has this second-order nonlinear ODE been studied before or been given a known name that I can look up for further study or investigation? $$ y’’ \sqrt{y}+f(x)=0, $$ in single real variable $x$ , with $y=y(x)$ , an arbitrary function $f(x)$ , and with the double primes indicating second derivative with respect to $x$ ?","Has this second-order nonlinear ODE been studied before or been given a known name that I can look up for further study or investigation? in single real variable , with , an arbitrary function , and with the double primes indicating second derivative with respect to ?"," y’’ \sqrt{y}+f(x)=0,  x y=y(x) f(x) x","['ordinary-differential-equations', 'mathematical-physics', 'potential-theory']"
79,Verify that an implicit equation is the solution to the differential equation.,Verify that an implicit equation is the solution to the differential equation.,,"Verify that \begin{equation} a.\;x^3+y^3-3xy=0,\; \mathbb{R}_{x\neq2^{2/3}} \end{equation} is the solution to \begin{equation} b.\;(y^2-x)y' - y+x^2=0 \end{equation} As we know the function g(x) of a. is not easily attainable, therefore we resort to stating one graphically. Such that at $2^{2/3}$ does not exist meaning it makes a jump. The book I'm reading says that if we implicitly differentiate a. then it agrees with b. and hence the definition of the solution is met. Definition of solution: Implicit Solution of ODE f(x,y) is the implicit solution of a differential equation: \begin{equation} F(x, y, y',\dots,y^{(n)})=0,\: D \end{equation} on the domain D if it defines a function $g(x)$ in D such that f[x,g(x)]=0 which \begin{equation} F[x, g(x) g(x)',\dots,g(x)^{(n)}]=0 \end{equation} I don't understand the logic behind the author's assertion and the definition.","Verify that is the solution to As we know the function g(x) of a. is not easily attainable, therefore we resort to stating one graphically. Such that at does not exist meaning it makes a jump. The book I'm reading says that if we implicitly differentiate a. then it agrees with b. and hence the definition of the solution is met. Definition of solution: Implicit Solution of ODE f(x,y) is the implicit solution of a differential equation: on the domain D if it defines a function in D such that f[x,g(x)]=0 which I don't understand the logic behind the author's assertion and the definition.","\begin{equation}
a.\;x^3+y^3-3xy=0,\; \mathbb{R}_{x\neq2^{2/3}}
\end{equation} \begin{equation}
b.\;(y^2-x)y' - y+x^2=0
\end{equation} 2^{2/3} \begin{equation}
F(x, y, y',\dots,y^{(n)})=0,\: D
\end{equation} g(x) \begin{equation}
F[x, g(x) g(x)',\dots,g(x)^{(n)}]=0
\end{equation}","['calculus', 'ordinary-differential-equations', 'functions', 'problem-solving', 'differential']"
80,Finding the extrema of a functional (calculus of variations),Finding the extrema of a functional (calculus of variations),,"I am trying to understand the basics of calculus of variations. To that effect I am trying to solve a problem stated as follows: Find an extremal for $$I(y) = \int^1_0y'^2 - y^2 + 2xy dx$$ So I calculated: \begin{align*} \frac{\partial F}{\partial y'} &= 2y' \implies  \frac{d}{dx}\bigg(\frac{\partial F}{\partial y'}\bigg) = 2y'' \\ 	\frac{\partial F}{\partial y} &= -2y + 2x \end{align*} Which gives the Euler equation $2y'' +2y - 2x = 0$ (I think, unless I messed up my math). Ok now I need to try to find solutions to this differential equation, however, I only know how to solve linear second order DE's. The $x$ term is throwing me off and I am not sure how to solve this. I know $y(x) = x$ is a solution by inspection, but inspection is a poor man's approach to solving DE's.","I am trying to understand the basics of calculus of variations. To that effect I am trying to solve a problem stated as follows: Find an extremal for So I calculated: Which gives the Euler equation (I think, unless I messed up my math). Ok now I need to try to find solutions to this differential equation, however, I only know how to solve linear second order DE's. The term is throwing me off and I am not sure how to solve this. I know is a solution by inspection, but inspection is a poor man's approach to solving DE's.","I(y) = \int^1_0y'^2 - y^2 + 2xy dx \begin{align*}
\frac{\partial F}{\partial y'} &= 2y' \implies  \frac{d}{dx}\bigg(\frac{\partial F}{\partial y'}\bigg) = 2y'' \\
	\frac{\partial F}{\partial y} &= -2y + 2x
\end{align*} 2y'' +2y - 2x = 0 x y(x) = x","['calculus', 'ordinary-differential-equations', 'optimization', 'calculus-of-variations']"
81,Infer boundedness from differential inequality $\frac{dx}{dt} \leq x(t)^2 + y(t)$?,Infer boundedness from differential inequality ?,\frac{dx}{dt} \leq x(t)^2 + y(t),"The original problem comes from a paper saying that $$\frac{d}{dt}\|u\|_{H^s} \leq C\|u\|_{H^s}^2 + \|f(t)\|_{H^s},$$ where $\|\cdot\|_{H^s}$ denotes the Sobolev norm of order $s$ , and $f(t) \in L^1(0,T;H^s)$ . Then the author concludes that $x \in L^\infty(0,T; H^s)$ . But I think it is fine to simplify it into a real-valued differential inequality for positive function $x(t)$ : $$\frac{dx}{dt} \leq x(t)^2 + y(t),$$ where $y(t)$ is some integrable function. How do we see $x(t)$ is also bounded in some interval, say $[0,T]$ ? If the power of $x(t)$ in the RHS is $1$ , then we could use Gronwall inequality to conclude the result. But it is not the case here. Could anyone help with it? I appreciate any hint and suggestion! Edit: On another paper I read that: $x(t) \leq g(t)$ , where $g(t)$ solves $$ \frac{dg}{dt} = g(t)^2 + y(t), g(0) = x(0). $$ And $T$ is chosen as the any number such that $\sup_{t\in[0,T]} g(t) < \infty$ . Questions : Does the ODE $$ \frac{dg}{dt} = g(t)^2 + y(t), g(0) = x(0) $$ has a solution? Why $x(t) \leq g(t)$ on $[0,T]$ ?","The original problem comes from a paper saying that where denotes the Sobolev norm of order , and . Then the author concludes that . But I think it is fine to simplify it into a real-valued differential inequality for positive function : where is some integrable function. How do we see is also bounded in some interval, say ? If the power of in the RHS is , then we could use Gronwall inequality to conclude the result. But it is not the case here. Could anyone help with it? I appreciate any hint and suggestion! Edit: On another paper I read that: , where solves And is chosen as the any number such that . Questions : Does the ODE has a solution? Why on ?","\frac{d}{dt}\|u\|_{H^s} \leq C\|u\|_{H^s}^2 + \|f(t)\|_{H^s}, \|\cdot\|_{H^s} s f(t) \in L^1(0,T;H^s) x \in L^\infty(0,T; H^s) x(t) \frac{dx}{dt} \leq x(t)^2 + y(t), y(t) x(t) [0,T] x(t) 1 x(t) \leq g(t) g(t) 
\frac{dg}{dt} = g(t)^2 + y(t), g(0) = x(0).
 T \sup_{t\in[0,T]} g(t) < \infty 
\frac{dg}{dt} = g(t)^2 + y(t), g(0) = x(0)
 x(t) \leq g(t) [0,T]","['ordinary-differential-equations', 'analysis', 'partial-differential-equations']"
82,"$2xy''+y'+xy=0,\ x>0$",,"2xy''+y'+xy=0,\ x>0","$2xy''+y'+xy=0, x>0$ I have to find a power series solution and the  radius of convergence. My solution: I know that ${y=\sum_{n=0}^{\infty}a_{n}X^{\lambda+n}=a_{0}X^{\lambda}+a_{1}X^{\lambda+1}+a_{2}X^{\lambda+2}+\cdots}.$ ${y'=\sum_{n=0}^{\infty}(\lambda+n)a_{n}x^{\lambda+n-1}=\lambda a_{0}X^{\lambda-1}+(\lambda+1)a_{1}X^{\lambda}+(\lambda+2)a_{2}X^{\lambda+1}+\cdots}.$ ${y''=\sum_{n=0}^{\infty}(\lambda+n)(\lambda+n-1)a_{n}X^{\lambda+n-2}=\lambda(\lambda-1)a_{0}X^{\lambda-2}+\lambda(\lambda+1)a_{1}X^{\lambda-1}+(\lambda+2)(\lambda+1)a_{2}X^{\lambda}+\cdots}$ Then, $2x(\lambda(\lambda-1)a_{0}X^{\lambda-2}+\lambda(\lambda+1)a_{1}X^{\lambda-1}+\cdots+(\lambda+n+1)(\lambda+n)a_{n+1}X^{\lambda+n-1})+1(\lambda a_{0}X^{\lambda-1}+\cdots+(\lambda+n+1)a_{n+1}x^{\lambda+n})+x(a_{0}X^{\lambda}+\cdots+a_{n}X^{\lambda+n})=0.$ Find the indicial equation: $2\lambda(\lambda+1)+(\lambda+1)+0=0\impliesλ=-\frac{1}{2},\:λ=-1.$ Then the recurrence equation: $a_{n+1}=\frac{-a_{n-1}}{2λ^{2}+2n^{2}+3λ+4nλ+3n+1}\implies a_{n+2}=\frac{-a_{n}}{2n^{2}+2λ^{2}+7λ+7n+4nλ+6},$ with $λ=-\frac{1}{2} \implies a_{n+2}=\frac{-a_{n}}{2n^{2}+5n+3}.$ Then, $y_{1}(x)=x^{-\frac{1}{2}}(\sum_{n=0}^{\infty}\frac{-a_{n}}{2n^{2}+5n+3}),$ and $λ=-1 \implies a_{n+2}=\frac{-a_{n}}{2n^{2}+3n+1}$ Then, $$y_{2}(x)=x^{-1}(\sum_{n=0}^{\infty}\frac{-a_{n}}{2n^{2}+3n+1}).$$ I have to find the  radius of convergence; how can I do that?","I have to find a power series solution and the  radius of convergence. My solution: I know that Then, Find the indicial equation: Then the recurrence equation: with Then, and Then, I have to find the  radius of convergence; how can I do that?","2xy''+y'+xy=0, x>0 {y=\sum_{n=0}^{\infty}a_{n}X^{\lambda+n}=a_{0}X^{\lambda}+a_{1}X^{\lambda+1}+a_{2}X^{\lambda+2}+\cdots}. {y'=\sum_{n=0}^{\infty}(\lambda+n)a_{n}x^{\lambda+n-1}=\lambda a_{0}X^{\lambda-1}+(\lambda+1)a_{1}X^{\lambda}+(\lambda+2)a_{2}X^{\lambda+1}+\cdots}. {y''=\sum_{n=0}^{\infty}(\lambda+n)(\lambda+n-1)a_{n}X^{\lambda+n-2}=\lambda(\lambda-1)a_{0}X^{\lambda-2}+\lambda(\lambda+1)a_{1}X^{\lambda-1}+(\lambda+2)(\lambda+1)a_{2}X^{\lambda}+\cdots} 2x(\lambda(\lambda-1)a_{0}X^{\lambda-2}+\lambda(\lambda+1)a_{1}X^{\lambda-1}+\cdots+(\lambda+n+1)(\lambda+n)a_{n+1}X^{\lambda+n-1})+1(\lambda a_{0}X^{\lambda-1}+\cdots+(\lambda+n+1)a_{n+1}x^{\lambda+n})+x(a_{0}X^{\lambda}+\cdots+a_{n}X^{\lambda+n})=0. 2\lambda(\lambda+1)+(\lambda+1)+0=0\impliesλ=-\frac{1}{2},\:λ=-1. a_{n+1}=\frac{-a_{n-1}}{2λ^{2}+2n^{2}+3λ+4nλ+3n+1}\implies a_{n+2}=\frac{-a_{n}}{2n^{2}+2λ^{2}+7λ+7n+4nλ+6}, λ=-\frac{1}{2} \implies a_{n+2}=\frac{-a_{n}}{2n^{2}+5n+3}. y_{1}(x)=x^{-\frac{1}{2}}(\sum_{n=0}^{\infty}\frac{-a_{n}}{2n^{2}+5n+3}), λ=-1 \implies a_{n+2}=\frac{-a_{n}}{2n^{2}+3n+1} y_{2}(x)=x^{-1}(\sum_{n=0}^{\infty}\frac{-a_{n}}{2n^{2}+3n+1}).",['ordinary-differential-equations']
83,Understanding the solution of a second order differential equation by combination of solutions,Understanding the solution of a second order differential equation by combination of solutions,,"I am currently reading the paper The dielectric lamellar diffraction grating , in which the following ODE is solved for $u$ : $$u^{''}+\zeta^{2}S(x-c)u=-\beta^{2}u,$$ where $\zeta^{2}=k_2^2-k_1^2$ , $\beta^2=k_1^2-\mu^2$ , $S(x)=0$ for $x < 0$ and $S(x)=1$ for $x\geq0$ . They do this the following way, which I don't understand at all: Let $\theta$ and $\psi$ be two linearly independent solutions which are continuous and continuously differentiable at $x = c$ , such that $\theta(0) = 1$ , $\psi(0) = 0$ , $\theta^{'}(0) = 0$ , $\psi^{'}(0) = 1$ . Then for $0\leq x\leq c$ , we have that $$\theta = \text{cos}(\beta x),\;\;\;\psi=\frac{1}{\beta}\text{sin}(\beta x)$$ and for $c<x$ we have that $$\theta=\text{cos}(\beta c)\text{cos}(\gamma(x-c))-\frac{\beta}{\gamma}\text{sin}(\beta c)\text{sin}(\gamma(x-c)),$$ $$\psi=\frac{1}{\beta}\text{sin}(\beta c)\text{cos}(\gamma(x-c))+\frac{1}{\gamma}\text{cos}(\beta c)\text{sin}(\gamma(x-c)),$$ where $\gamma^{2}=\beta^{2}+\zeta^{2}$ . Can somebody please explain to me what is going on here? Why these two solutions? Where do all the sin/cos come from? EDIT (based on answers) The ODE is a piecewise defined function, so let's look at it piece by piece. $x < c$ in this case: $x-c< 0$ , hence $S(x-c) = 0$ , and the ODE becomes: $$u^{''}=-\beta^{2}u,$$ for which the answer is: $$u(x) = c_1 \cos(\beta x) + c_2 \sin(\beta x) $$ $x \geq c$ in this case: $x-c \geq 0$ , hence $S(x-c) = 1$ , and the ODE becomes: $$u^{''}+\zeta^{2}u=-\beta^{2}u$$ which can be rewritten as: $$u^{''}=-(\beta^{2}+\zeta^{2})u$$ substituting $m^2 = \beta^{2}+\zeta^{2}$ we obtain again the same kind of equation as for the first piece ( $x < c$ ) of the function: $$u^{''}=-m^2u$$ for which the answer is: $$u(x) = c_1 \cos(m x) + c_2 \sin(m x) $$ resubstituting: $$u(x) = c_1 \cos(\sqrt{\beta^{2}+\zeta^{2}}x) + c_2 \sin(\sqrt{\beta^{2}+\zeta^{2}} x) $$ So, I still don't understand: The use of $\theta$ and $\psi$ ? Their solution compared to the one posted here? Thank you.","I am currently reading the paper The dielectric lamellar diffraction grating , in which the following ODE is solved for : where , , for and for . They do this the following way, which I don't understand at all: Let and be two linearly independent solutions which are continuous and continuously differentiable at , such that , , , . Then for , we have that and for we have that where . Can somebody please explain to me what is going on here? Why these two solutions? Where do all the sin/cos come from? EDIT (based on answers) The ODE is a piecewise defined function, so let's look at it piece by piece. in this case: , hence , and the ODE becomes: for which the answer is: in this case: , hence , and the ODE becomes: which can be rewritten as: substituting we obtain again the same kind of equation as for the first piece ( ) of the function: for which the answer is: resubstituting: So, I still don't understand: The use of and ? Their solution compared to the one posted here? Thank you.","u u^{''}+\zeta^{2}S(x-c)u=-\beta^{2}u, \zeta^{2}=k_2^2-k_1^2 \beta^2=k_1^2-\mu^2 S(x)=0 x < 0 S(x)=1 x\geq0 \theta \psi x = c \theta(0) = 1 \psi(0) = 0 \theta^{'}(0) = 0 \psi^{'}(0) = 1 0\leq x\leq c \theta = \text{cos}(\beta x),\;\;\;\psi=\frac{1}{\beta}\text{sin}(\beta x) c<x \theta=\text{cos}(\beta c)\text{cos}(\gamma(x-c))-\frac{\beta}{\gamma}\text{sin}(\beta c)\text{sin}(\gamma(x-c)), \psi=\frac{1}{\beta}\text{sin}(\beta c)\text{cos}(\gamma(x-c))+\frac{1}{\gamma}\text{cos}(\beta c)\text{sin}(\gamma(x-c)), \gamma^{2}=\beta^{2}+\zeta^{2} x < c x-c< 0 S(x-c) = 0 u^{''}=-\beta^{2}u, u(x) = c_1 \cos(\beta x) + c_2 \sin(\beta x)  x \geq c x-c \geq 0 S(x-c) = 1 u^{''}+\zeta^{2}u=-\beta^{2}u u^{''}=-(\beta^{2}+\zeta^{2})u m^2 = \beta^{2}+\zeta^{2} x < c u^{''}=-m^2u u(x) = c_1 \cos(m x) + c_2 \sin(m x)  u(x) = c_1 \cos(\sqrt{\beta^{2}+\zeta^{2}}x) + c_2 \sin(\sqrt{\beta^{2}+\zeta^{2}} x)  \theta \psi",['ordinary-differential-equations']
84,Name of invariance for ODE,Name of invariance for ODE,,"If I have a first order ODE $\dot{x} = f(x)$ with $x = (x_1, x_2, \cdots,x_n) \in \mathbb{R}^n$ . What is the property called that if $x$ is a solution then a solution vector given by a permutation of the components of $x$ is also a solution?",If I have a first order ODE with . What is the property called that if is a solution then a solution vector given by a permutation of the components of is also a solution?,"\dot{x} = f(x) x = (x_1, x_2, \cdots,x_n) \in \mathbb{R}^n x x","['ordinary-differential-equations', 'dynamical-systems']"
85,Local Existence for Heat equation with Lipschitz nonlinearity,Local Existence for Heat equation with Lipschitz nonlinearity,,"Let's say we have $x\in [0,1]$ and we have the heat equation $$\frac{d}{dt}u(x,t) = \Delta u(x,t) + f(u)$$ with $u(0,t) = u(1,t) = 0$ and $u(x,0)\in L^2$ . $f$ is lipschitz continuous. I want to show local existence of the weak solution in $L^2$ . Here's what I did: $$u(x,t) = S(t)u_0 + \int_0^t S(t-s)f(u)ds$$ is the weak solution where $S(t)$ is the heat semigroup $(u_0 = u(x,0))$ . Define $$\mathcal{K}(v):= S(t)v_0 + \int_0^t S(t-s)f(v)ds$$ Then for $v, w\in L^2$ with $v_0 = w_0$ we consider \begin{align} \sup_{t\in [0,T]}\left\| \mathcal{K}v - \mathcal{K}w \right\|_2 &= \sup_{t\in [0,T]}\left\|\int_0^T S(t-s)[f(v)-f(w)]\,ds\right\|_2\\ &\leq \sup_{t\in [0,T]}\int_0^T\left\|S(t-s)[f(v)-f(w)]\right\|_2\,ds\\ &\leq \sup_{t\in [0,T]}\int_0^T e^{-\omega_1(t-s)}C\left\|v-w\right\|_2 \,ds\\ &\leq T*1*C*\sup_{t\in [0,T]}\left\|v-w\right\|_2. \end{align} In the last two steps, $C$ is the Lipschitz constant for $f$ and $\omega_1$ is the principal eigenvalue of the Laplace operator. Pick $T$ small enough and we have a contraction mapping, so that implies the existence of a local solution.  Does this argument work? Can I tweak it slightly to also work for $f$ locally Lipschitz?  I was thinking for the locally Lipschitz case of maybe considering the ball in $L^2$ of radius $M$ , and only choosing $v$ and $w$ in that ball.","Let's say we have and we have the heat equation with and . is lipschitz continuous. I want to show local existence of the weak solution in . Here's what I did: is the weak solution where is the heat semigroup . Define Then for with we consider In the last two steps, is the Lipschitz constant for and is the principal eigenvalue of the Laplace operator. Pick small enough and we have a contraction mapping, so that implies the existence of a local solution.  Does this argument work? Can I tweak it slightly to also work for locally Lipschitz?  I was thinking for the locally Lipschitz case of maybe considering the ball in of radius , and only choosing and in that ball.","x\in [0,1] \frac{d}{dt}u(x,t) = \Delta u(x,t) + f(u) u(0,t) = u(1,t) = 0 u(x,0)\in L^2 f L^2 u(x,t) = S(t)u_0 + \int_0^t S(t-s)f(u)ds S(t) (u_0 = u(x,0)) \mathcal{K}(v):= S(t)v_0 + \int_0^t S(t-s)f(v)ds v, w\in L^2 v_0 = w_0 \begin{align}
\sup_{t\in [0,T]}\left\| \mathcal{K}v - \mathcal{K}w \right\|_2 &= \sup_{t\in [0,T]}\left\|\int_0^T S(t-s)[f(v)-f(w)]\,ds\right\|_2\\
&\leq \sup_{t\in [0,T]}\int_0^T\left\|S(t-s)[f(v)-f(w)]\right\|_2\,ds\\
&\leq \sup_{t\in [0,T]}\int_0^T e^{-\omega_1(t-s)}C\left\|v-w\right\|_2 \,ds\\
&\leq T*1*C*\sup_{t\in [0,T]}\left\|v-w\right\|_2.
\end{align} C f \omega_1 T f L^2 M v w","['ordinary-differential-equations', 'partial-differential-equations', 'heat-equation', 'contraction-operator']"
86,Exponentiating expression containing ln(abs(x)),Exponentiating expression containing ln(abs(x)),,"I am trying to figure out when we write +/- after exponentiating expressions containing natural log. So, say that we have integrated (1/x) with respect to x. Then we have ln(abs(x)) + C. That is, ln(x) with absolute value signs around the x. Now, if this is the left-hand side of the equation and we want to exponentiate both sides in order to solve, do we need to have +/- exp expression due to the absolute value inside the natural log? I am confused about when you need the +/- because in a WolframAlpha solution to a Diff EQs problem that I’m working on, they just took the positive expression instead of using +/-. But some problems in the book have answers with +/- for a similar process, so I am confused. Thanks very much.","I am trying to figure out when we write +/- after exponentiating expressions containing natural log. So, say that we have integrated (1/x) with respect to x. Then we have ln(abs(x)) + C. That is, ln(x) with absolute value signs around the x. Now, if this is the left-hand side of the equation and we want to exponentiate both sides in order to solve, do we need to have +/- exp expression due to the absolute value inside the natural log? I am confused about when you need the +/- because in a WolframAlpha solution to a Diff EQs problem that I’m working on, they just took the positive expression instead of using +/-. But some problems in the book have answers with +/- for a similar process, so I am confused. Thanks very much.",,"['ordinary-differential-equations', 'logarithms', 'exponentiation', 'absolute-value']"
87,Solving a nonlinear autonomous system using polar coordinates,Solving a nonlinear autonomous system using polar coordinates,,"I have the nonlinear system: \begin{equation} \begin{array} fx'=-y-x\sqrt{x^2+y^2}\\ y'=x-y\sqrt{x^2+y^2} \end{array} \end{equation} which I solve by polar coordinates transformation. $x=r\cos\phi \ y=r\sin\phi$ and $x'=dr\cos\phi-r\sin\phi$ and $y'=dr\sin\phi+r\cos\phi$ . \begin{equation} \begin{array} fdr\cos\phi-r\sin\phi=-r\sin\phi-r\cos\phi\sqrt{r^2\cos^2\phi+r^2\sin^2\phi}\\ dr\sin\phi+r\cos\phi=r\cos\phi-r\sin\phi\sqrt{r^2\cos^2\phi+r^2\sin^2\phi}\ \end{array} \end{equation} Clearly, many terms cancel out and we obtain: \begin{equation} \begin{array} fdr=-r^2\\ dr=-r^2\ \end{array} \end{equation} Then, integrating both sides with respect to $r$ we obtain: \begin{equation} \begin{array} fr=-\frac{1}{3}r^3+c\\ r=-\frac{1}{3}r^3+c \end{array} \end{equation} Since $r=\frac{x}{\cos\phi} and r=\frac{y}{\sin\phi}$ we get \begin{equation} \begin{array} fx=-\frac{x^3}{\cos^2\phi}+c\cos\phi\\ y=-\frac{y^3}{\sin^3\phi}+c\sin\phi \end{array} \end{equation} The summing up the two equations, and isolating for y, I get a very scary result: y≈0.26457 sin^4(ϕ) cos(ϕ) (sqrt((27 c csc^9(ϕ) sec^2(ϕ) + 27 c csc^8(ϕ) sec^3(ϕ) - 27 x^3 csc^9(ϕ) sec^5(ϕ) - 27 x csc^9(ϕ) sec^3(ϕ))^2 + 108 csc^15(ϕ) sec^6(ϕ)) + 27 c csc^9(ϕ) sec^2(ϕ) + 27 c csc^8(ϕ) sec^3(ϕ) - 27 x^3 csc^9(ϕ) sec^5(ϕ) - 27 x csc^9(ϕ) sec^3(ϕ))^(1/3) - (1.2599 csc(ϕ) sec(ϕ))/(sqrt((27 c csc^9(ϕ) sec^2(ϕ) + 27 c csc^8(ϕ) sec^3(ϕ) - 27 x^3 csc^9(ϕ) sec^5(ϕ) - 27 x csc^9(ϕ) sec^3(ϕ))^2 + 108 csc^15(ϕ) sec^6(ϕ)) + 27 c csc^9(ϕ) sec^2(ϕ) + 27 c csc^8(ϕ) sec^3(ϕ) - 27 x^3 csc^9(ϕ) sec^5(ϕ) - 27 x csc^9(ϕ) sec^3(ϕ))^(1/3) and csc(ϕ) sec(ϕ)!=0 What went wrong here? Thanks UPDATE , with Jean Maries correction we get: \begin{equation} \begin{array} fx=\frac{\cos\phi}{\phi+c}\\ y=\frac{\sin\phi}{\phi+c} \end{array} \end{equation} Summing up and solving for y: \begin{equation} y=\frac{\cos\phi}{\phi+c}+\frac{\sin\phi}{\phi+c}-x \end{equation} which plotted , with an arbitrary value of $c$ is:","I have the nonlinear system: which I solve by polar coordinates transformation. and and . Clearly, many terms cancel out and we obtain: Then, integrating both sides with respect to we obtain: Since we get The summing up the two equations, and isolating for y, I get a very scary result: y≈0.26457 sin^4(ϕ) cos(ϕ) (sqrt((27 c csc^9(ϕ) sec^2(ϕ) + 27 c csc^8(ϕ) sec^3(ϕ) - 27 x^3 csc^9(ϕ) sec^5(ϕ) - 27 x csc^9(ϕ) sec^3(ϕ))^2 + 108 csc^15(ϕ) sec^6(ϕ)) + 27 c csc^9(ϕ) sec^2(ϕ) + 27 c csc^8(ϕ) sec^3(ϕ) - 27 x^3 csc^9(ϕ) sec^5(ϕ) - 27 x csc^9(ϕ) sec^3(ϕ))^(1/3) - (1.2599 csc(ϕ) sec(ϕ))/(sqrt((27 c csc^9(ϕ) sec^2(ϕ) + 27 c csc^8(ϕ) sec^3(ϕ) - 27 x^3 csc^9(ϕ) sec^5(ϕ) - 27 x csc^9(ϕ) sec^3(ϕ))^2 + 108 csc^15(ϕ) sec^6(ϕ)) + 27 c csc^9(ϕ) sec^2(ϕ) + 27 c csc^8(ϕ) sec^3(ϕ) - 27 x^3 csc^9(ϕ) sec^5(ϕ) - 27 x csc^9(ϕ) sec^3(ϕ))^(1/3) and csc(ϕ) sec(ϕ)!=0 What went wrong here? Thanks UPDATE , with Jean Maries correction we get: Summing up and solving for y: which plotted , with an arbitrary value of is:","\begin{equation}
\begin{array}
fx'=-y-x\sqrt{x^2+y^2}\\
y'=x-y\sqrt{x^2+y^2}
\end{array}
\end{equation} x=r\cos\phi \ y=r\sin\phi x'=dr\cos\phi-r\sin\phi y'=dr\sin\phi+r\cos\phi \begin{equation}
\begin{array}
fdr\cos\phi-r\sin\phi=-r\sin\phi-r\cos\phi\sqrt{r^2\cos^2\phi+r^2\sin^2\phi}\\
dr\sin\phi+r\cos\phi=r\cos\phi-r\sin\phi\sqrt{r^2\cos^2\phi+r^2\sin^2\phi}\
\end{array}
\end{equation} \begin{equation}
\begin{array}
fdr=-r^2\\
dr=-r^2\
\end{array}
\end{equation} r \begin{equation}
\begin{array}
fr=-\frac{1}{3}r^3+c\\
r=-\frac{1}{3}r^3+c
\end{array}
\end{equation} r=\frac{x}{\cos\phi} and r=\frac{y}{\sin\phi} \begin{equation}
\begin{array}
fx=-\frac{x^3}{\cos^2\phi}+c\cos\phi\\
y=-\frac{y^3}{\sin^3\phi}+c\sin\phi
\end{array}
\end{equation} \begin{equation}
\begin{array}
fx=\frac{\cos\phi}{\phi+c}\\
y=\frac{\sin\phi}{\phi+c}
\end{array}
\end{equation} \begin{equation}
y=\frac{\cos\phi}{\phi+c}+\frac{\sin\phi}{\phi+c}-x
\end{equation} c","['ordinary-differential-equations', 'polar-coordinates', 'nonlinear-system']"
88,Confused about reducing order of non-linear homogenous ODE,Confused about reducing order of non-linear homogenous ODE,,"I have two problems for which I know the answers (and working) but am still confused about the method used to solve them. The equations are $yy^{(2)}=(y^{(1)})^2$ and $x^{(2)}+(x^{(1)})^2=0$ In my notes it instructs that in the case of the independent variable missing from the equation (x and t, respectively), the substitution to reduce the order is made as follows: $$ p = y^{(1)} $$ $$ y^{(2)}=\frac{dp}{dx}=\frac{dp}{dy}\frac{dy}{dx}=p\frac{dp}{dy} $$ Then, for the first equation: $yy^{(2)}=(y^{(1)})^2 \implies yp\frac{dp}{dy}=p^2$ But for the second equation, the substitution is made as: $x^{(2)}+(x^{(1)})^2=0 \implies \frac{dp}{dt}=-p^2$ and not $p\frac{dp}{dx}=-p^2$ I've tested this with online calculators and even they compute these two differential equations in the two different ways. Any explanation of where I'm going wrong would be greatly appreciated.","I have two problems for which I know the answers (and working) but am still confused about the method used to solve them. The equations are and In my notes it instructs that in the case of the independent variable missing from the equation (x and t, respectively), the substitution to reduce the order is made as follows: Then, for the first equation: But for the second equation, the substitution is made as: and not I've tested this with online calculators and even they compute these two differential equations in the two different ways. Any explanation of where I'm going wrong would be greatly appreciated.",yy^{(2)}=(y^{(1)})^2 x^{(2)}+(x^{(1)})^2=0  p = y^{(1)}   y^{(2)}=\frac{dp}{dx}=\frac{dp}{dy}\frac{dy}{dx}=p\frac{dp}{dy}  yy^{(2)}=(y^{(1)})^2 \implies yp\frac{dp}{dy}=p^2 x^{(2)}+(x^{(1)})^2=0 \implies \frac{dp}{dt}=-p^2 p\frac{dp}{dx}=-p^2,['ordinary-differential-equations']
89,How to solve this DE for $v^3$?,How to solve this DE for ?,v^3,"I've been looking over this for a while now and can't find where I went wrong in my attempt. A car's velocity $v$ is related to its displacement $x$ by the differential equation $$v\frac{dv}{dx}=\frac{p}{v}-kv^2$$ where $p$ and $k$ are constants. Show that $$v^3=\frac{1}{k}(p-pe^{-3x})$$ My attempt: $$\frac{dv}{dx}=\frac{p-kv^3}{v^2}$$ $$\frac{v^2}{p-kv^3}\frac{dv}{dx}=1$$ $$\int\frac{v^2}{p-kv^3}dv=x+c$$ Let $u=p-kv^3$ , $du=-3kv^2dv$ $$-\frac{1}{3k}\int\frac{1}{u}du=x+c$$ $$\ln(p-kv^3)=-3k(x+c)$$ $$v^3=\frac{1}{k}(p-e^{-3k(x+c)})$$ After substituting $v=0$ , $x=0$ I found that, $$ -3kc=\ln(p)$$ $$ v^3=\frac{1}{k}(p-e^{-3kx}\cdot e^{-3kc})$$ $$v^3=\frac{1}{k}(p-pe^{-3kx})$$ But this does not match up with the required form so where is the mistake I have made? Thanks.","I've been looking over this for a while now and can't find where I went wrong in my attempt. A car's velocity is related to its displacement by the differential equation where and are constants. Show that My attempt: Let , After substituting , I found that, But this does not match up with the required form so where is the mistake I have made? Thanks.",v x v\frac{dv}{dx}=\frac{p}{v}-kv^2 p k v^3=\frac{1}{k}(p-pe^{-3x}) \frac{dv}{dx}=\frac{p-kv^3}{v^2} \frac{v^2}{p-kv^3}\frac{dv}{dx}=1 \int\frac{v^2}{p-kv^3}dv=x+c u=p-kv^3 du=-3kv^2dv -\frac{1}{3k}\int\frac{1}{u}du=x+c \ln(p-kv^3)=-3k(x+c) v^3=\frac{1}{k}(p-e^{-3k(x+c)}) v=0 x=0  -3kc=\ln(p)  v^3=\frac{1}{k}(p-e^{-3kx}\cdot e^{-3kc}) v^3=\frac{1}{k}(p-pe^{-3kx}),"['calculus', 'ordinary-differential-equations']"
90,Existence and Uniqueness of the Cauchy Problem for General Competition Systems?,Existence and Uniqueness of the Cauchy Problem for General Competition Systems?,,"I have the original system \begin{equation}     \begin{array}{lcl}       \dot u_{1} & = & A_{1}u_{1}(1 - u_{1} - a_{12}u_{2} - \dots - a_{1n}u_{n})  \\         \dot u_{2} & = & A_{2}u_{2}(1 - a_{21}u_{1} - u_{2} - \dots - a_{2n}u_{n}) \\          & \vdots & \\         \dot u_{n} & = & A_{n}u_{n}(1 - a_{n1}u_{1} - \dots - a_{n(n-1)}u_{n-1} - u_{n}),     \end{array} \end{equation} which is a general n-dimensional competition system. We use the fact that $A_{i} = a_{ii}$ to write this system as \begin{equation}     \dot u_{i} = \Phi_{i}(t; u_{1}, u_{2}, \dots, u_{n}), \end{equation} where \begin{equation*} \begin{array}{lcl}     \Phi_{1} & = & u_{1}(a_{11} - a_{11}u_{1} - a_{11}a_{12}u_{2} - \dots - a_{11}a_{1n}u_{n}) \\     \Phi_{2} & = & u_{2}(a_{22} - a_{22}a_{21}u_{1} - a_{22}u_{2} - \dots - a_{22}a_{2n}u_{n}) \\     & \vdots & \\     \Phi_{n} & = & u_{n}(a_{nn} - a_{nn}a_{n1}u_{1} - \dots - a_{nn}a_{n(n-1)}u_{n-1} - a_{nn}u_{n}). \end{array} \end{equation*} Here, the $a_{ij}$ 's are smooth, positive 1-periodic functions of t defined over $\mathbb{R}$ . Before I properly consider the periodic case, I need to consider properties of the time-averaged system (which, in this case, will be autonomous). We define this as \begin{equation}     \dot w_{i} = \phi_{i}(w_{1}, w_{2}, \dots, w_{n}), \end{equation} where \begin{equation*}     \phi_{i}(w_{1}, w_{2}, \dots, w_{n}) = \int_{0}^{1} \Phi_{i}(w_{1}, w_{2}, \dots, w_{n}) \ dt. \end{equation*} It follows that the system (7) can be written as \begin{equation}     \dot w_{i} = w_{i}\bigg(\bar{a_{ii}}(1 - w_{i}) - \sum_{k \neq i} \bar{a_{ik}}w_{k}\bigg), \ \ \ i = 1, 2, \dots, n, \end{equation} where \begin{equation*}     \bar{a_{ij}} = \int_{0}^{1} a_{ij}(t) \ dt. \end{equation*} Now, my goal is to prove that the Cauchy problem for this system $\dot w_{i}$ has a unique global solution whenever the initial data $w_{0} = (w_{0_{1}}, w_{0_{2}}, \dots, w_{0_{n}})$ satisfy $w_{0_{i}} \in \mathbb{R}_{+0} = \mathbb{R}_{+} \cup \{0\} \forall i \in [1,n].$ Unfortunately, I have not been able to find much information on conditions for the existence and uniqueness of nonlinear systems of ODEs, or at least for general n-dimensional competition systems like this one. How should I approach a proof for this? I'm a bit stuck.","I have the original system which is a general n-dimensional competition system. We use the fact that to write this system as where Here, the 's are smooth, positive 1-periodic functions of t defined over . Before I properly consider the periodic case, I need to consider properties of the time-averaged system (which, in this case, will be autonomous). We define this as where It follows that the system (7) can be written as where Now, my goal is to prove that the Cauchy problem for this system has a unique global solution whenever the initial data satisfy Unfortunately, I have not been able to find much information on conditions for the existence and uniqueness of nonlinear systems of ODEs, or at least for general n-dimensional competition systems like this one. How should I approach a proof for this? I'm a bit stuck.","\begin{equation}
    \begin{array}{lcl}
      \dot u_{1} & = & A_{1}u_{1}(1 - u_{1} - a_{12}u_{2} - \dots - a_{1n}u_{n})  \\
        \dot u_{2} & = & A_{2}u_{2}(1 - a_{21}u_{1} - u_{2} - \dots - a_{2n}u_{n}) \\ 
        & \vdots & \\
        \dot u_{n} & = & A_{n}u_{n}(1 - a_{n1}u_{1} - \dots - a_{n(n-1)}u_{n-1} - u_{n}),
    \end{array}
\end{equation} A_{i} = a_{ii} \begin{equation}
    \dot u_{i} = \Phi_{i}(t; u_{1}, u_{2}, \dots, u_{n}),
\end{equation} \begin{equation*}
\begin{array}{lcl}
    \Phi_{1} & = & u_{1}(a_{11} - a_{11}u_{1} - a_{11}a_{12}u_{2} - \dots - a_{11}a_{1n}u_{n}) \\
    \Phi_{2} & = & u_{2}(a_{22} - a_{22}a_{21}u_{1} - a_{22}u_{2} - \dots - a_{22}a_{2n}u_{n}) \\
    & \vdots & \\
    \Phi_{n} & = & u_{n}(a_{nn} - a_{nn}a_{n1}u_{1} - \dots - a_{nn}a_{n(n-1)}u_{n-1} - a_{nn}u_{n}).
\end{array}
\end{equation*} a_{ij} \mathbb{R} \begin{equation}
    \dot w_{i} = \phi_{i}(w_{1}, w_{2}, \dots, w_{n}),
\end{equation} \begin{equation*}
    \phi_{i}(w_{1}, w_{2}, \dots, w_{n}) = \int_{0}^{1} \Phi_{i}(w_{1}, w_{2}, \dots, w_{n}) \ dt.
\end{equation*} \begin{equation}
    \dot w_{i} = w_{i}\bigg(\bar{a_{ii}}(1 - w_{i}) - \sum_{k \neq i} \bar{a_{ik}}w_{k}\bigg), \ \ \ i = 1, 2, \dots, n,
\end{equation} \begin{equation*}
    \bar{a_{ij}} = \int_{0}^{1} a_{ij}(t) \ dt.
\end{equation*} \dot w_{i} w_{0} = (w_{0_{1}}, w_{0_{2}}, \dots, w_{0_{n}}) w_{0_{i}} \in \mathbb{R}_{+0} = \mathbb{R}_{+} \cup \{0\} \forall i \in [1,n].","['ordinary-differential-equations', 'dynamical-systems']"
91,Differential Equation to Model Temperature of Water,Differential Equation to Model Temperature of Water,,"Question The water in a hot-water tank cools at a rate which is proportional to $T − T_0$ , where $T$ is the temperature of the water in degrees celcius at time $t$ minutes and $T_0$ is the temperature of the surrounding air in degrees celcius. When $T = 60$ , the water is cooling at $1$ celcius per minute. When switched on, the heater supplies sufficient heat to raise the water temperature by $2$ degrees celcius each minute (neglecting heat loss by cooling). If $T = 20$ when the heater is switched on and $T_0$ = 20. Find the differential equation $\frac{dT}{dt}$ (Where both heating and cooling are taking place). So the temperature leaving the water is leaving at a rate of $\frac{dT_{out}}{dt}=k(T-T_0)$ for some constant $k$ . We can find $k$ by letting $T=60$ , $T_0=20$ and $\frac{dT_{out}}{dt}=-1$ . I assumed here that the air temperature is staying constant at $20$ as I'm thinking that is what the last scentence of the question implied but it is hard to tell. From this I found that $\frac{dT_{out}}{dt}=\frac{20-T}{40}$ It was given that $\frac{dT_{in}}{dt}=2$ so the overall temperature must be: $$\frac{dT}{dt}=\frac{dT_{in}}{dt}-\frac{dT_{out}}{dt}$$ $$\frac{dT}{dt}=\frac{60+T}{40}$$ However, the answer should be $\frac{dT}{dt}=\frac{100-T}{40}$ . Please let me know where I went wrong. Thanks.","Question The water in a hot-water tank cools at a rate which is proportional to , where is the temperature of the water in degrees celcius at time minutes and is the temperature of the surrounding air in degrees celcius. When , the water is cooling at celcius per minute. When switched on, the heater supplies sufficient heat to raise the water temperature by degrees celcius each minute (neglecting heat loss by cooling). If when the heater is switched on and = 20. Find the differential equation (Where both heating and cooling are taking place). So the temperature leaving the water is leaving at a rate of for some constant . We can find by letting , and . I assumed here that the air temperature is staying constant at as I'm thinking that is what the last scentence of the question implied but it is hard to tell. From this I found that It was given that so the overall temperature must be: However, the answer should be . Please let me know where I went wrong. Thanks.",T − T_0 T t T_0 T = 60 1 2 T = 20 T_0 \frac{dT}{dt} \frac{dT_{out}}{dt}=k(T-T_0) k k T=60 T_0=20 \frac{dT_{out}}{dt}=-1 20 \frac{dT_{out}}{dt}=\frac{20-T}{40} \frac{dT_{in}}{dt}=2 \frac{dT}{dt}=\frac{dT_{in}}{dt}-\frac{dT_{out}}{dt} \frac{dT}{dt}=\frac{60+T}{40} \frac{dT}{dt}=\frac{100-T}{40},['ordinary-differential-equations']
92,How to find first integral in a given quasilinear partial differential equation?,How to find first integral in a given quasilinear partial differential equation?,,"I have to solve PDE $$y(x+y)\dfrac{\partial z}{\partial x}+x(x+y)\dfrac{\partial z}{\partial y}=2(x^2-y^2)+xz-yz.$$ From this equation I get the system of differential equations: $$\dfrac{dx}{y(x+y)}=\dfrac{dy}{x(x+y)}=\dfrac{dz}{2(x^2-y^2)+xz-yz}.$$ I have $$\dfrac{dx}{y(x+y)}=\dfrac{dy}{x(x+y)},$$ from which I get $$xdx=ydy$$ and by integrating I get the first integral $$x^2-y^2=C_1.$$ I don't know how to get another first integral.",I have to solve PDE From this equation I get the system of differential equations: I have from which I get and by integrating I get the first integral I don't know how to get another first integral.,"y(x+y)\dfrac{\partial z}{\partial x}+x(x+y)\dfrac{\partial z}{\partial y}=2(x^2-y^2)+xz-yz. \dfrac{dx}{y(x+y)}=\dfrac{dy}{x(x+y)}=\dfrac{dz}{2(x^2-y^2)+xz-yz}. \dfrac{dx}{y(x+y)}=\dfrac{dy}{x(x+y)}, xdx=ydy x^2-y^2=C_1.","['ordinary-differential-equations', 'partial-differential-equations', 'systems-of-equations']"
93,"Cahill, Physical Mathematics, Exercise 7.14","Cahill, Physical Mathematics, Exercise 7.14",,"Problem: Integrate the ODE $(xy+1)dx+2x^{2}(2xy-1)dy=0$ . Hint given in the book (Kevin Cahill, Physical Mathematics Exercise 7.14, page 332, Second edition): Use the variable $v(x)=xy(x)$ instead of $y(x)$ . Now I tried solving this equation using the hint and separating the variables since: \begin{equation} \frac{dv}{dx}=f(x)g(v) \end{equation} where $f(x)=\frac{1}{x}$ and $g(v)=\frac{-(v+1)+2v(2v-1)}{2(2v-1)}$ . Separating and integrating gives me: $x=(v-1)^{2/5}+(4v+1)^{6}+c$ . But this is not helpful since I need to find $v=v(x)$ and from that $y=y(x)$ and inverting the above relation seems difficult. Have I done anything wrong here? Is there any other method to solve this problem? Thanks in advance.","Problem: Integrate the ODE . Hint given in the book (Kevin Cahill, Physical Mathematics Exercise 7.14, page 332, Second edition): Use the variable instead of . Now I tried solving this equation using the hint and separating the variables since: where and . Separating and integrating gives me: . But this is not helpful since I need to find and from that and inverting the above relation seems difficult. Have I done anything wrong here? Is there any other method to solve this problem? Thanks in advance.","(xy+1)dx+2x^{2}(2xy-1)dy=0 v(x)=xy(x) y(x) \begin{equation}
\frac{dv}{dx}=f(x)g(v)
\end{equation} f(x)=\frac{1}{x} g(v)=\frac{-(v+1)+2v(2v-1)}{2(2v-1)} x=(v-1)^{2/5}+(4v+1)^{6}+c v=v(x) y=y(x)",['ordinary-differential-equations']
94,Trivial solution of differential equations,Trivial solution of differential equations,,"For the following differential equation: $$y'+2xy^2=0$$ the trivial solution $y=0$ is a valid solution (as far as I know). However, the general solution for this DE is: $$y=\frac{1}{x^2+c}$$ For this general solution, there is no way to get exactly $y=0$ . How is it possible for the trivial solution to be valid but not achievable by the general solution, or did I make a mistake somewhere?","For the following differential equation: the trivial solution is a valid solution (as far as I know). However, the general solution for this DE is: For this general solution, there is no way to get exactly . How is it possible for the trivial solution to be valid but not achievable by the general solution, or did I make a mistake somewhere?",y'+2xy^2=0 y=0 y=\frac{1}{x^2+c} y=0,['ordinary-differential-equations']
95,Questions regarding Homogeneous Differential Equation,Questions regarding Homogeneous Differential Equation,,"I have problem with two differential Equation solutions done in my text book. I am begginer in this field. I know only Uniqueness Existence Theorem and few methods. $\frac{dy}{dx}= \frac{y^2- x^2}{2xy}$ and $\frac{dy}{dx}= \frac{2xy}{x^2-y^2}$ and $y(1)=1$ For the first one , they put $y = vx$ and got the solution $y^2+x^2= 2x$ for all $(x,y) \in \mathbb R^2$ . But my doubt is following. It is clear from the question that the differential equation is defined on the set $\mathbb R - A$ where $A = \{ (x,y) : xy = 0\}$ . so clearly solution of this differential equation should have been defined on that set only.  I can not understand how they are getting solution for the whole $\mathbb R^2$ . For the second one , they  have done in the following way. $\frac{dy}{dx}= \frac{2xy}{x^2-y^2}$ implies $\frac{dx}{dy}= \frac{x^2-y^2}{2xy}$ After that they have put $x$ in the place of y and vice-versa in the previous solution to get the solution of this differential equation. I can not understand how $\frac{dy}{dx}= \frac{2xy}{x^2-y^2}$ implies $\frac{dx}{dy}= \frac{x^2-y^2}{2xy}$ . Why are they not caring about what will happen when $\frac{dy}{dx}= \frac{2xy}{x^2-y^2}=0$ ? Can anyone please help me ? I got stuck on this and I can not go ahead without understanding this. PLease help me.","I have problem with two differential Equation solutions done in my text book. I am begginer in this field. I know only Uniqueness Existence Theorem and few methods. and and For the first one , they put and got the solution for all . But my doubt is following. It is clear from the question that the differential equation is defined on the set where . so clearly solution of this differential equation should have been defined on that set only.  I can not understand how they are getting solution for the whole . For the second one , they  have done in the following way. implies After that they have put in the place of y and vice-versa in the previous solution to get the solution of this differential equation. I can not understand how implies . Why are they not caring about what will happen when ? Can anyone please help me ? I got stuck on this and I can not go ahead without understanding this. PLease help me.","\frac{dy}{dx}= \frac{y^2- x^2}{2xy} \frac{dy}{dx}= \frac{2xy}{x^2-y^2} y(1)=1 y = vx y^2+x^2= 2x (x,y) \in \mathbb R^2 \mathbb R - A A = \{ (x,y) : xy = 0\} \mathbb R^2 \frac{dy}{dx}= \frac{2xy}{x^2-y^2} \frac{dx}{dy}= \frac{x^2-y^2}{2xy} x \frac{dy}{dx}= \frac{2xy}{x^2-y^2} \frac{dx}{dy}= \frac{x^2-y^2}{2xy} \frac{dy}{dx}= \frac{2xy}{x^2-y^2}=0","['real-analysis', 'calculus', 'ordinary-differential-equations', 'derivatives']"
96,Solving second order ode with differentiated input,Solving second order ode with differentiated input,,"I would like to solve the ode: $$\ddot{x}+5\dot{x}+6x=\dot{f}+f, \\$$ with initial conditions: $$x(0)=2, \dot{x}(0)=0 \\$$ and input: $$f(t)=u(t)=\begin{cases} 0 & \text{ if } t< 0 \\ 1 & \text{ if } t\geq  0  \end{cases}$$ without the use of Laplace. Given the derivative in the right-hand-side, I thought about using the linearity property in order to make the problem more typical: $$\ddot{y}+5\dot{y}+6y=u,$$ $$x = \dot{y}+y $$ So, if that was correct, the initial problem consists now in solving the second ode: finding $y$ , its derivative and then summing them up to find the $x$ . I calculated the homogeneous solution: $$y_{h}(t)=(c_{1}e^{-2t}+c_{2}e^{-3t})u(t)$$ and the particular one: $$y_{p}(t)=\frac{1}{6}u(t)$$ The total $y$ solution is $$y(t)=(c_{1}e^{-2t}+c_{2}e^{-3t})u(t)+\frac{1}{6}u(t)$$ , while its derivative is $$\dot{y}=-2c_{1}e^{-2t}-3c_{2}e^{-3t}$$ The solution $x$ is $$x(t)=-c_{1}e^{-2t}-2c_{2}e^{-3t}+\frac{1}{6}$$ , but we also need to calculate its derivative in order to get the values of $c_{1}$ and $c_{2}$ . So, we have: $$\dot{x}=2c_{1}e^{-2t}+6c_{2}e^{-3t}$$ , and the constants are $c_{1}=-\frac{33}{6}$ and $c_{2}=\frac{11}{6}$ The correct result (found through Laplace) is $$x(t)=\frac{1}{6}+6.5e^{-2t}-4.67e^{-3t}$$ . Needless to say that as many time as i have solved it, I never got this result, therefore I believe that the problem is not in the actual calculations but in the strategy of solving the problem. Would someone help shed some light on where I have made mistakes? Second question: An alternative way of computing the particular solution is based on the use of the Wronskian, provided that we already know the two solutions of the homogeneous equation. The form I have followed is this: $$y_{p}(t)=-y_{1}\int \frac{y_{2}}{W}u(t)dt+y_{2}\int \frac{y_{1}}{W}u(t)dt$$ , where $W = y_{1}\dot{y_{2}}-y_{2}\dot{y_{1}}$ , comes from its definition. After replacing the functions and knowing that $u$ is the unit step function, we have to solve the indefinite integrals. We could probably get rid of the step function in the integral if we made the integral from $0^{+}$ to $t$ , but that would give an extra term from the integration. If we want this result to match the result that was found above, we only have to ""keep"" the validation at $t$ and not at $0^{+}$ . Why are we not keeping in general the validation at the ""down limit"" of the integral?","I would like to solve the ode: with initial conditions: and input: without the use of Laplace. Given the derivative in the right-hand-side, I thought about using the linearity property in order to make the problem more typical: So, if that was correct, the initial problem consists now in solving the second ode: finding , its derivative and then summing them up to find the . I calculated the homogeneous solution: and the particular one: The total solution is , while its derivative is The solution is , but we also need to calculate its derivative in order to get the values of and . So, we have: , and the constants are and The correct result (found through Laplace) is . Needless to say that as many time as i have solved it, I never got this result, therefore I believe that the problem is not in the actual calculations but in the strategy of solving the problem. Would someone help shed some light on where I have made mistakes? Second question: An alternative way of computing the particular solution is based on the use of the Wronskian, provided that we already know the two solutions of the homogeneous equation. The form I have followed is this: , where , comes from its definition. After replacing the functions and knowing that is the unit step function, we have to solve the indefinite integrals. We could probably get rid of the step function in the integral if we made the integral from to , but that would give an extra term from the integration. If we want this result to match the result that was found above, we only have to ""keep"" the validation at and not at . Why are we not keeping in general the validation at the ""down limit"" of the integral?","\ddot{x}+5\dot{x}+6x=\dot{f}+f, \\ x(0)=2, \dot{x}(0)=0 \\ f(t)=u(t)=\begin{cases}
0 & \text{ if } t< 0 \\
1 & \text{ if } t\geq  0 
\end{cases} \ddot{y}+5\dot{y}+6y=u, x = \dot{y}+y  y x y_{h}(t)=(c_{1}e^{-2t}+c_{2}e^{-3t})u(t) y_{p}(t)=\frac{1}{6}u(t) y y(t)=(c_{1}e^{-2t}+c_{2}e^{-3t})u(t)+\frac{1}{6}u(t) \dot{y}=-2c_{1}e^{-2t}-3c_{2}e^{-3t} x x(t)=-c_{1}e^{-2t}-2c_{2}e^{-3t}+\frac{1}{6} c_{1} c_{2} \dot{x}=2c_{1}e^{-2t}+6c_{2}e^{-3t} c_{1}=-\frac{33}{6} c_{2}=\frac{11}{6} x(t)=\frac{1}{6}+6.5e^{-2t}-4.67e^{-3t} y_{p}(t)=-y_{1}\int \frac{y_{2}}{W}u(t)dt+y_{2}\int \frac{y_{1}}{W}u(t)dt W = y_{1}\dot{y_{2}}-y_{2}\dot{y_{1}} u 0^{+} t t 0^{+}","['ordinary-differential-equations', 'wronskian']"
97,Neural odes: What is the relationship between backpropagation and the adjoint sensitivity method,Neural odes: What is the relationship between backpropagation and the adjoint sensitivity method,,"I was reading the Neural ODE paper by Chen, Duvenaud, et. al. and trying to understand the relationship between backpropagation and the adjoint sensitivity method. I also looked at Gil Strang's latest book Linear Algebra and Learning from Data for some more background on both backpropagation and the adjoint method. It seems like both backpropagation and the adjoint method will compute the gradient of a scalar function. So if I have my loss function $\mathcal{L} = \sum_i^{N}(y_i - f(x, \theta)^2$ , and I want to optimize the parameters $\theta$ to minimize the loss, then I could use reverse mode automatic differentiation, a.k.a backpropagation, to get the $\frac{d\mathcal{L}}{d\theta}$ . But it also seems like the adjoint method is a different way to obtain the same $\frac{d\mathcal{L}}{d\theta}$ ? I was not clear no whether the adjoint method was a necessary step in reverse mode automatic differentiation? Or is it that applying the adjoint method will just reorganize some steps in the reverse mode AD calculation to make it faster? I understand that the paper's big claim is that using adjoint sensitivities allows us to solve neural odes faster compared to backprop, but I was not clear on how backprop and adjoint methods are different? Thanks.","I was reading the Neural ODE paper by Chen, Duvenaud, et. al. and trying to understand the relationship between backpropagation and the adjoint sensitivity method. I also looked at Gil Strang's latest book Linear Algebra and Learning from Data for some more background on both backpropagation and the adjoint method. It seems like both backpropagation and the adjoint method will compute the gradient of a scalar function. So if I have my loss function , and I want to optimize the parameters to minimize the loss, then I could use reverse mode automatic differentiation, a.k.a backpropagation, to get the . But it also seems like the adjoint method is a different way to obtain the same ? I was not clear no whether the adjoint method was a necessary step in reverse mode automatic differentiation? Or is it that applying the adjoint method will just reorganize some steps in the reverse mode AD calculation to make it faster? I understand that the paper's big claim is that using adjoint sensitivities allows us to solve neural odes faster compared to backprop, but I was not clear on how backprop and adjoint methods are different? Thanks.","\mathcal{L} = \sum_i^{N}(y_i - f(x, \theta)^2 \theta \frac{d\mathcal{L}}{d\theta} \frac{d\mathcal{L}}{d\theta}","['ordinary-differential-equations', 'optimization', 'numerical-methods', 'neural-networks']"
98,Differential equation for Brownian motion,Differential equation for Brownian motion,,"I was trying to solve the following differential equation: $$ \frac{dv}{dt} + \frac{\gamma}{m}v + \omega_0^2x = \frac{1}{m} \psi(t)$$ where: $v=\frac{dx}{dt}$ and $x=x(t)$ . But I cant really advance. The solution for it looks like this: $$ v(t)=v_0 e^{- \Gamma t}C(t) - \frac{\omega_0^2}{\Delta}x_0e^{- \Gamma t} \sinh(\Delta t)+ \frac{1}{m} \int_0^tdt' \psi(t')e^{- \Gamma (t-t')}C(t-t') \hspace{1cm}(1)$$ where: $C(t)= \cosh(\Delta t )- \frac{\Gamma}{\Delta}\sinh(\Delta t)$ , $\Gamma = \frac{\gamma}{m}$ and $\Delta = \sqrt{\Gamma^2 - \omega_0^2}$ . I tried the annihilation polynomial (by, first, assuming the homogeneous equation): $$ P(\lambda) = \lambda^2+ \frac{\gamma}{m} \lambda + \omega_0^2 =0$$ which lead me to: $$ \lambda_{\pm} = - \frac{\Gamma}{2} \pm \sqrt{ \frac{\Gamma^2}{4}- \omega_0^2  }$$ So, the homogeneous solution would be: $$ x(t) = c_1e^{ - \frac{ \Gamma}{2}t} \cos \left(\sqrt{ \frac{\Gamma^2}{4}- \omega_0^2  } t \right) + c_2e^{ - \frac{ \Gamma}{2}t} \sin \left(\sqrt{ \frac{\Gamma^2}{4}- \omega_0^2  } \right)$$ I dont know where the integration comes from, in the last term of $(1)$ on the RHS, since I would have to apply a derivative to this solution I obtained, in order to get the expression for the velocity $v(t)$ . I still need to find the particular solution, and I dont know how to find it either. How can I obtain $(1)$ ?","I was trying to solve the following differential equation: where: and . But I cant really advance. The solution for it looks like this: where: , and . I tried the annihilation polynomial (by, first, assuming the homogeneous equation): which lead me to: So, the homogeneous solution would be: I dont know where the integration comes from, in the last term of on the RHS, since I would have to apply a derivative to this solution I obtained, in order to get the expression for the velocity . I still need to find the particular solution, and I dont know how to find it either. How can I obtain ?", \frac{dv}{dt} + \frac{\gamma}{m}v + \omega_0^2x = \frac{1}{m} \psi(t) v=\frac{dx}{dt} x=x(t)  v(t)=v_0 e^{- \Gamma t}C(t) - \frac{\omega_0^2}{\Delta}x_0e^{- \Gamma t} \sinh(\Delta t)+ \frac{1}{m} \int_0^tdt' \psi(t')e^{- \Gamma (t-t')}C(t-t') \hspace{1cm}(1) C(t)= \cosh(\Delta t )- \frac{\Gamma}{\Delta}\sinh(\Delta t) \Gamma = \frac{\gamma}{m} \Delta = \sqrt{\Gamma^2 - \omega_0^2}  P(\lambda) = \lambda^2+ \frac{\gamma}{m} \lambda + \omega_0^2 =0  \lambda_{\pm} = - \frac{\Gamma}{2} \pm \sqrt{ \frac{\Gamma^2}{4}- \omega_0^2  }  x(t) = c_1e^{ - \frac{ \Gamma}{2}t} \cos \left(\sqrt{ \frac{\Gamma^2}{4}- \omega_0^2  } t \right) + c_2e^{ - \frac{ \Gamma}{2}t} \sin \left(\sqrt{ \frac{\Gamma^2}{4}- \omega_0^2  } \right) (1) v(t) (1),"['ordinary-differential-equations', 'physics']"
99,Derivative of the Wronskian,Derivative of the Wronskian,,"Consider a non-autonomous linear system of ode's: $X'=A(t)X$ , $X:\mathbb{R}\rightarrow\mathbb{R}^n$ . Let $B(t)$ be a fundamental matrix solution $\dot{B}=A(t)B$ of the system and $W(t):=\det B(t)$ the Wronskian. Show that $$\dot{W}=tr(A(t))W.$$ My idea: I started by induction. For $n=2$ we have $B=\left(\begin{array}{cc} x_{1} & y_{1}\\ x_{2} & y_{2} \end{array}\right)$ and $$W	=\det B	=x_{1}y_{2}-y_{1}x_{2}.$$ Therefore, $$ \begin{eqnarray} \dot{W}	&=&\dot{x_{1}}y_{2}+x_{1}\dot{y_{2}}-\dot{y_{1}}x_{2}-y_{1}\dot{x_{2}}\\ 	&=&\dot{x_{1}}y_{2}-\dot{y_{1}}x_{2}+x_{1}\dot{y_{2}}-y_{1}\dot{x_{2}}\\ 	&=&\det\left(\begin{array}{cc} \dot{x_{1}} & x_{2}\\ \dot{y_{1}} & y_{2} \end{array}\right)+\det\left(\begin{array}{cc} x_{1} & \dot{x_{2}}\\ y_{1} & \dot{y_{2}} \end{array}\right)\\ 	&=&\det\left(\begin{array}{cc} a_{11}x_{1}+a_{12}x_{2} & x_{2}\\ a_{11}y_{1}+a_{12}y_{2} & y_{2} \end{array}\right)+\det\left(\begin{array}{cc} x_{1} & a_{21}x_{1}+a_{22}x_{2}\\ y_{1} & a_{21}y_{1}+a_{22}y_{2} \end{array}\right)\\ 	&=&\det\left(\begin{array}{cc} a_{11}x_{1} & x_{2}\\ a_{11}y_{1} & y_{2} \end{array}\right)+\det\left(\begin{array}{cc} a_{12}x_{2} & x_{2}\\ a_{12}y_{2} & y_{2} \end{array}\right)+\det\left(\begin{array}{cc} x_{1} & a_{21}x_{1}\\ y_{1} & a_{21}y_{1} \end{array}\right)+\det\left(\begin{array}{cc} x_{1} & a_{22}x_{2}\\ y_{1} & a_{22}y_{2} \end{array}\right)\\ 	&=&\det\left(\begin{array}{cc} a_{11}x_{1} & x_{2}\\ a_{11}y_{1} & y_{2} \end{array}\right)+\det\left(\begin{array}{cc} x_{1} & a_{22}x_{2}\\ y_{1} & a_{22}y_{2} \end{array}\right)\\ 	&=&a_{11}\det\left(\begin{array}{cc} x_{1} & x_{2}\\ y_{1} & y_{2} \end{array}\right)+a_{22}\det\left(\begin{array}{cc} x_{1} & x_{2}\\ y_{1} & y_{2} \end{array}\right)\\ 	&=&(a_{11}+a_{22})\det\left(\begin{array}{cc} x_{1} & x_{2}\\ y_{1} & y_{2} \end{array}\right)\\ 	&=&tr(A)W. \end{eqnarray}$$ Now I suppose that it is valid for $n>2$ and consider taking $n+1$ and the formula for the determinant $$W=\sum_{k=1}^{n+1}(-1)^{i+k}b_{ik}\det B_{ik}$$ where $B_{ik}$ is the $ik$ -minor of $B$ . Up to here I haven't been able to advance. Any suggestions?","Consider a non-autonomous linear system of ode's: , . Let be a fundamental matrix solution of the system and the Wronskian. Show that My idea: I started by induction. For we have and Therefore, Now I suppose that it is valid for and consider taking and the formula for the determinant where is the -minor of . Up to here I haven't been able to advance. Any suggestions?","X'=A(t)X X:\mathbb{R}\rightarrow\mathbb{R}^n B(t) \dot{B}=A(t)B W(t):=\det B(t) \dot{W}=tr(A(t))W. n=2 B=\left(\begin{array}{cc}
x_{1} & y_{1}\\
x_{2} & y_{2}
\end{array}\right) W	=\det B	=x_{1}y_{2}-y_{1}x_{2}. 
\begin{eqnarray}
\dot{W}	&=&\dot{x_{1}}y_{2}+x_{1}\dot{y_{2}}-\dot{y_{1}}x_{2}-y_{1}\dot{x_{2}}\\
	&=&\dot{x_{1}}y_{2}-\dot{y_{1}}x_{2}+x_{1}\dot{y_{2}}-y_{1}\dot{x_{2}}\\
	&=&\det\left(\begin{array}{cc}
\dot{x_{1}} & x_{2}\\
\dot{y_{1}} & y_{2}
\end{array}\right)+\det\left(\begin{array}{cc}
x_{1} & \dot{x_{2}}\\
y_{1} & \dot{y_{2}}
\end{array}\right)\\
	&=&\det\left(\begin{array}{cc}
a_{11}x_{1}+a_{12}x_{2} & x_{2}\\
a_{11}y_{1}+a_{12}y_{2} & y_{2}
\end{array}\right)+\det\left(\begin{array}{cc}
x_{1} & a_{21}x_{1}+a_{22}x_{2}\\
y_{1} & a_{21}y_{1}+a_{22}y_{2}
\end{array}\right)\\
	&=&\det\left(\begin{array}{cc}
a_{11}x_{1} & x_{2}\\
a_{11}y_{1} & y_{2}
\end{array}\right)+\det\left(\begin{array}{cc}
a_{12}x_{2} & x_{2}\\
a_{12}y_{2} & y_{2}
\end{array}\right)+\det\left(\begin{array}{cc}
x_{1} & a_{21}x_{1}\\
y_{1} & a_{21}y_{1}
\end{array}\right)+\det\left(\begin{array}{cc}
x_{1} & a_{22}x_{2}\\
y_{1} & a_{22}y_{2}
\end{array}\right)\\
	&=&\det\left(\begin{array}{cc}
a_{11}x_{1} & x_{2}\\
a_{11}y_{1} & y_{2}
\end{array}\right)+\det\left(\begin{array}{cc}
x_{1} & a_{22}x_{2}\\
y_{1} & a_{22}y_{2}
\end{array}\right)\\
	&=&a_{11}\det\left(\begin{array}{cc}
x_{1} & x_{2}\\
y_{1} & y_{2}
\end{array}\right)+a_{22}\det\left(\begin{array}{cc}
x_{1} & x_{2}\\
y_{1} & y_{2}
\end{array}\right)\\
	&=&(a_{11}+a_{22})\det\left(\begin{array}{cc}
x_{1} & x_{2}\\
y_{1} & y_{2}
\end{array}\right)\\
	&=&tr(A)W.
\end{eqnarray} n>2 n+1 W=\sum_{k=1}^{n+1}(-1)^{i+k}b_{ik}\det B_{ik} B_{ik} ik B","['ordinary-differential-equations', 'wronskian']"
