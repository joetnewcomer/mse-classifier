,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Show that $\int_0^{\pi/3}\arccos(2\sin^2 x-\cos x)\mathrm dx=\frac{\pi^2}{5}$,Show that,\int_0^{\pi/3}\arccos(2\sin^2 x-\cos x)\mathrm dx=\frac{\pi^2}{5},"There is numerical evidence that $I=\int_0^{\pi/3}\arccos(2\sin^2 x-\cos x)\mathrm dx=\frac{\pi^2}{5}$ . How can this be proved? I was trying to answer another question , and I got it down to this integral. I tried integration by parts, substitution and Maclaurin series, but it seems that this integral requires more advanced techniques. Wolfram does not evaluate the indefinite integral. Here is the graph of $y=\arccos(2\sin^2 x-\cos x)$ for $0\le x\le \frac{\pi}{3}$ . It intersects the axes at $(\frac{\pi}{3},0)$ and $(0,\pi)$ . Not sure if this helps, but apparently we also have $\int_0^{\pi/3}\arccos^{\color{red}{2}}(2\sin^2 x-\cos x)\mathrm dx=\frac{19\pi^3}{135}$ .","There is numerical evidence that . How can this be proved? I was trying to answer another question , and I got it down to this integral. I tried integration by parts, substitution and Maclaurin series, but it seems that this integral requires more advanced techniques. Wolfram does not evaluate the indefinite integral. Here is the graph of for . It intersects the axes at and . Not sure if this helps, but apparently we also have .","I=\int_0^{\pi/3}\arccos(2\sin^2 x-\cos x)\mathrm dx=\frac{\pi^2}{5} y=\arccos(2\sin^2 x-\cos x) 0\le x\le \frac{\pi}{3} (\frac{\pi}{3},0) (0,\pi) \int_0^{\pi/3}\arccos^{\color{red}{2}}(2\sin^2 x-\cos x)\mathrm dx=\frac{19\pi^3}{135}","['calculus', 'integration', 'trigonometry', 'definite-integrals', 'closed-form']"
1,What Mathematics questions can be better solved with concepts from Physics?,What Mathematics questions can be better solved with concepts from Physics?,,"Over the years, I've seen several questions in mathematics that can be solved using concepts borrowed from Physics. Having seen these question, I'm interested to find out what other mathematics questions you've found that can be better solved with a concept from physics - or at least where the application of physics is interesting and perhaps illuminating. Examples One of these questions is on minimizing the time taken for a lifeguard to go out to a stationary distressed swimmer. In the scenario, the lifeguard runs faster than he swims in the water, and as such the straight line is not the fastest way for the lifeguard to reach the swimmer. Students will normally use calculus to solve this problem, and the answer can be obtained after some work - however, a much more convenient (and intuitive) way is to borrow from the idea of refractive indices in geometric optics. We recast the situation by replacing the beach and the sea with two materials with different refractive indices, choosing the appropriate refractive indices proportionate to the ratio between the lifeguard's velocities while running and swimming. The problem is then reduced to finding a beam of light that passes through both the swimmer and the lifeguard's position. (For a more complete explanation, you can visit this site: http://findingmoonshine.blogspot.sg/2012_05_01_archive.html ) Another of these questions requires one to prove that, in an acute-angled triangle, the angle subtended by any side of the triangle at the Toricelli point is 120°. Again, instead of using trigonometry, one can use the concept of hanging equal weights from a (frictionless) string at each of the vertices of the triangle, and then tying each of the three strings together at one knot placed on the surface of the triangle. The equilibrium position of the knot is the Toricelli point, and one can then complete the proof by considering forces acting on the knot. Looking forward to hearing from you!","Over the years, I've seen several questions in mathematics that can be solved using concepts borrowed from Physics. Having seen these question, I'm interested to find out what other mathematics questions you've found that can be better solved with a concept from physics - or at least where the application of physics is interesting and perhaps illuminating. Examples One of these questions is on minimizing the time taken for a lifeguard to go out to a stationary distressed swimmer. In the scenario, the lifeguard runs faster than he swims in the water, and as such the straight line is not the fastest way for the lifeguard to reach the swimmer. Students will normally use calculus to solve this problem, and the answer can be obtained after some work - however, a much more convenient (and intuitive) way is to borrow from the idea of refractive indices in geometric optics. We recast the situation by replacing the beach and the sea with two materials with different refractive indices, choosing the appropriate refractive indices proportionate to the ratio between the lifeguard's velocities while running and swimming. The problem is then reduced to finding a beam of light that passes through both the swimmer and the lifeguard's position. (For a more complete explanation, you can visit this site: http://findingmoonshine.blogspot.sg/2012_05_01_archive.html ) Another of these questions requires one to prove that, in an acute-angled triangle, the angle subtended by any side of the triangle at the Toricelli point is 120°. Again, instead of using trigonometry, one can use the concept of hanging equal weights from a (frictionless) string at each of the vertices of the triangle, and then tying each of the three strings together at one knot placed on the surface of the triangle. The equilibrium position of the knot is the Toricelli point, and one can then complete the proof by considering forces acting on the knot. Looking forward to hearing from you!",,"['calculus', 'geometry', 'soft-question', 'physics', 'big-list']"
2,Simple geometric proof for Snell's law of refraction,Simple geometric proof for Snell's law of refraction,,Snell's law of refraction can be derived from Fermat's principle that light travels paths that minimize the time using simple calculus. Since Snell's law only involves sines I wonder whether this minimum problem has a simple geometric solution.,Snell's law of refraction can be derived from Fermat's principle that light travels paths that minimize the time using simple calculus. Since Snell's law only involves sines I wonder whether this minimum problem has a simple geometric solution.,,['calculus']
3,"When is a vector ""glued"" to the origin?","When is a vector ""glued"" to the origin?",,"Let $V$ be a real finite-dimensional vector space (I guess this forces $V$ to be $\mathbb{R}^n$). My intuition is that a vector $v\in V$ must be ""glued"" to the origin, since the origin is the only canonical thing that $V$ has (not even the basis is canonical, and I suspect the origin (aka. the zero vector) is the only vector that has the same representation under every basis). But, in many contexts, vectors are not thought of as being ""glued"" to the origin, in particular when we think of them as ""displacements"": a ""displacement vector"" from $a$ to $b$ can be the same as a displacement vector from $c$ to $d$ under suitable conditions. (Intuitively, a ""displacement vector"" can be moved around without making it a different vector.) In stark contrast, a ""position"" in Euclidean space cannot be moved around without making it a different position, since a position is only equal to itself and no other position. So ""displacements"" and ""positions"" can be written as vectors, but clearly they don't behave the same. In calculus texts, authors usually switch back and forth between vectors that are ""glued"" to the origin, and vectors that are ""not glued"" to the origin. But I find that this obscures the nature of what a vector is, and I'd like a rigorous distinction. Spivak's A Comprehensive Introduction to Differential Geometry, Volume I (Chapter 3: The tangent bundle) suggests that the appropriate language is that of tangent bundles . Namely, at each point $x\in\mathbb{R}^n$ we have a copy of $\mathbb{R}^n$: its tangent space. So, a point together with its tangent space looks like $(x,\mathbb{R}^n)$. If we let $x$ vary over $\mathbb{R}^n$, I suppose the set of all tangent spaces would be the set $\{(x,\mathbb{R}^n) \ | \ x\in\mathbb{R}^n\}$, which looks suspiciously like $\mathbb{R}^n\times\mathbb{R}^n$, ie. $\mathbb{R}^{2n}$. Now a "" position vector "" seems to be a vector in the original ${\mathbb R}^n$, and a "" displacement vector "" starting at $x \in {\mathbb R}^n$ seems to be a vector in the tangent space $(x, {\mathbb R}^n)$. Then a vector space always has an origin, and a vector is always ""glued"" to that origin. What allows us to ""move vectors around"" with impunity is that ${\mathbb R}^n$ is isomorphic to ${\mathbb R}^n$. Moreover, a motivation for tangent spaces seems to be precisely to formalize the idea of ""displacements"" on a manifold. How this is related to the idea of affine spaces (which also seem to deal with ""displacements""), I don't know. Is any part of the above discussion correct? When is a vector ""glued"" to the origin? What is a rigorous formulation of ""position vectors"" and ""displacements vectors""? Does it use tangent bundles? Does it use affine spaces?","Let $V$ be a real finite-dimensional vector space (I guess this forces $V$ to be $\mathbb{R}^n$). My intuition is that a vector $v\in V$ must be ""glued"" to the origin, since the origin is the only canonical thing that $V$ has (not even the basis is canonical, and I suspect the origin (aka. the zero vector) is the only vector that has the same representation under every basis). But, in many contexts, vectors are not thought of as being ""glued"" to the origin, in particular when we think of them as ""displacements"": a ""displacement vector"" from $a$ to $b$ can be the same as a displacement vector from $c$ to $d$ under suitable conditions. (Intuitively, a ""displacement vector"" can be moved around without making it a different vector.) In stark contrast, a ""position"" in Euclidean space cannot be moved around without making it a different position, since a position is only equal to itself and no other position. So ""displacements"" and ""positions"" can be written as vectors, but clearly they don't behave the same. In calculus texts, authors usually switch back and forth between vectors that are ""glued"" to the origin, and vectors that are ""not glued"" to the origin. But I find that this obscures the nature of what a vector is, and I'd like a rigorous distinction. Spivak's A Comprehensive Introduction to Differential Geometry, Volume I (Chapter 3: The tangent bundle) suggests that the appropriate language is that of tangent bundles . Namely, at each point $x\in\mathbb{R}^n$ we have a copy of $\mathbb{R}^n$: its tangent space. So, a point together with its tangent space looks like $(x,\mathbb{R}^n)$. If we let $x$ vary over $\mathbb{R}^n$, I suppose the set of all tangent spaces would be the set $\{(x,\mathbb{R}^n) \ | \ x\in\mathbb{R}^n\}$, which looks suspiciously like $\mathbb{R}^n\times\mathbb{R}^n$, ie. $\mathbb{R}^{2n}$. Now a "" position vector "" seems to be a vector in the original ${\mathbb R}^n$, and a "" displacement vector "" starting at $x \in {\mathbb R}^n$ seems to be a vector in the tangent space $(x, {\mathbb R}^n)$. Then a vector space always has an origin, and a vector is always ""glued"" to that origin. What allows us to ""move vectors around"" with impunity is that ${\mathbb R}^n$ is isomorphic to ${\mathbb R}^n$. Moreover, a motivation for tangent spaces seems to be precisely to formalize the idea of ""displacements"" on a manifold. How this is related to the idea of affine spaces (which also seem to deal with ""displacements""), I don't know. Is any part of the above discussion correct? When is a vector ""glued"" to the origin? What is a rigorous formulation of ""position vectors"" and ""displacements vectors""? Does it use tangent bundles? Does it use affine spaces?",,"['calculus', 'differential-geometry', 'vector-spaces', 'vectors', 'affine-geometry']"
4,"A closed form for $\int_0^\infty e^{-a\,x} \operatorname{erfi}(\sqrt{x})^3\ dx$",A closed form for,"\int_0^\infty e^{-a\,x} \operatorname{erfi}(\sqrt{x})^3\ dx","Let $\operatorname{erfi}(x)$ be the imaginary error function $$\operatorname{erfi}(x)=\frac{2}{\sqrt{\pi}}\int_0^xe^{z^2}dz.$$ Consider the following parameterized integral $$I(a)=\int_0^\infty e^{-a\,x} \operatorname{erfi}(\sqrt{x})^3\ dx.$$ I found some conjectured special values of $I(a)$ that are correct up to at least several hundreds of digits of precision: $$I(3)\stackrel{?}{=}\frac{1}{\sqrt{2}},\ \ I(4)\stackrel{?}{=}\frac{1}{4\sqrt{3}}.$$ Are these conjectured values correct? Is there a general closed-form formula for $I(a)$? Or, at least, are there any other closed-form special values of $I(a)$ for simple (e.g. integer or rational) values of $a$?","Let $\operatorname{erfi}(x)$ be the imaginary error function $$\operatorname{erfi}(x)=\frac{2}{\sqrt{\pi}}\int_0^xe^{z^2}dz.$$ Consider the following parameterized integral $$I(a)=\int_0^\infty e^{-a\,x} \operatorname{erfi}(\sqrt{x})^3\ dx.$$ I found some conjectured special values of $I(a)$ that are correct up to at least several hundreds of digits of precision: $$I(3)\stackrel{?}{=}\frac{1}{\sqrt{2}},\ \ I(4)\stackrel{?}{=}\frac{1}{4\sqrt{3}}.$$ Are these conjectured values correct? Is there a general closed-form formula for $I(a)$? Or, at least, are there any other closed-form special values of $I(a)$ for simple (e.g. integer or rational) values of $a$?",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'error-function']"
5,What exactly is a differential? [duplicate],What exactly is a differential? [duplicate],,"This question already has answers here : What is the practical difference between a differential and a derivative? (7 answers) Closed 8 years ago . I've seen the formula for differentials alot, namely $$dy=f'(x)dx$$ but what I think when I see this is that someone is manipulating the ""formula"" $$f'(x)=\frac{dy}{dx}$$ When I think of ""differential equation"", I think of $$f\left(x,y,\frac{dy}{dx},\frac{d^2y}{dx^2},\cdots,\frac{d^ny}{dx^n}\right)=0$$ not $$f(x,y,dy,dx,\cdots)=0$$ I've heard that $\Delta y$ and $\Delta x$ can be approximated by $dy$ and $dx$ (or maybe its the other way around?), but that doesn't make much sense to me. If you replace $dy$ and $dx$ by $\Delta y$ and $\Delta x$, you sort of have Euler's method, but this still doesn't clear much up for me. So, What exactly is a differential, and why is it useful ?","This question already has answers here : What is the practical difference between a differential and a derivative? (7 answers) Closed 8 years ago . I've seen the formula for differentials alot, namely $$dy=f'(x)dx$$ but what I think when I see this is that someone is manipulating the ""formula"" $$f'(x)=\frac{dy}{dx}$$ When I think of ""differential equation"", I think of $$f\left(x,y,\frac{dy}{dx},\frac{d^2y}{dx^2},\cdots,\frac{d^ny}{dx^n}\right)=0$$ not $$f(x,y,dy,dx,\cdots)=0$$ I've heard that $\Delta y$ and $\Delta x$ can be approximated by $dy$ and $dx$ (or maybe its the other way around?), but that doesn't make much sense to me. If you replace $dy$ and $dx$ by $\Delta y$ and $\Delta x$, you sort of have Euler's method, but this still doesn't clear much up for me. So, What exactly is a differential, and why is it useful ?",,"['calculus', 'intuition']"
6,Are there always singularities at the edge of a disk of convergence?,Are there always singularities at the edge of a disk of convergence?,,"Take a function that is analytic at 0 and consider its Maclaurin Series.  Here are some examples I'll refer to: $$\frac{1}{1-x} =\sum_{n=0}^\infty x^n$$ $$\frac{1}{1+x^2} =\sum_{n=0}^\infty(-1)^nx^{2n}$$ $$\ln(1-x) =-\sum_{n=1}^\infty\frac{x^n}{n}$$ $$\sqrt{1-x} =1-\sum_{n=1}^\infty\frac{(2n-2)!}{2^{2n-1}n!(n-1)!}x^n$$ Each of these series has a radius of convergence of 1.  And each function either $\bullet$ has a singularity along the edge of the disk of convergence (at 1, $\pm i$, and 1 in the first three examples respectively) or $\bullet$ has a derivative with a singularity along the edge of the disk of convergence (the last example is this way at 1). My question is: Suppose a function $f$ is analytic at 0 and its Maclaurin Series has a radius of convergence $r<\infty$.  Does it have to be the case that some derivative (0th, 1st, 2nd, ...) of $f$ blows up somewhere along the edge of the disk of convergence?","Take a function that is analytic at 0 and consider its Maclaurin Series.  Here are some examples I'll refer to: $$\frac{1}{1-x} =\sum_{n=0}^\infty x^n$$ $$\frac{1}{1+x^2} =\sum_{n=0}^\infty(-1)^nx^{2n}$$ $$\ln(1-x) =-\sum_{n=1}^\infty\frac{x^n}{n}$$ $$\sqrt{1-x} =1-\sum_{n=1}^\infty\frac{(2n-2)!}{2^{2n-1}n!(n-1)!}x^n$$ Each of these series has a radius of convergence of 1.  And each function either $\bullet$ has a singularity along the edge of the disk of convergence (at 1, $\pm i$, and 1 in the first three examples respectively) or $\bullet$ has a derivative with a singularity along the edge of the disk of convergence (the last example is this way at 1). My question is: Suppose a function $f$ is analytic at 0 and its Maclaurin Series has a radius of convergence $r<\infty$.  Does it have to be the case that some derivative (0th, 1st, 2nd, ...) of $f$ blows up somewhere along the edge of the disk of convergence?",,"['calculus', 'sequences-and-series', 'complex-analysis', 'convergence-divergence', 'taylor-expansion']"
7,"A closed form for $\int_0^\pi \lvert \sin(m t) \cos(n t) \rvert \, \mathrm{d} t$",A closed form for,"\int_0^\pi \lvert \sin(m t) \cos(n t) \rvert \, \mathrm{d} t","Motivated by this nice question I have been trying to compute the function $f: \mathbb{R}^+ \to \left[0,\frac{1}{2}\right]$ defined by $$f(\alpha) = \lim_{x \to \infty} \frac{1}{x} \int \limits_0^x \lvert\sin(\alpha s) \cos(s)\rvert \, \mathrm{d} s \, .$$ Note that $f(\alpha) \leq \frac{1}{2}$ follows from the Cauchy-Schwarz inequality. In the answers to the original question the equidistribution theorem is used to show that $f(3 - 2 \sqrt{2}) = \frac{4}{\pi^2}$ holds. This argument can be extended to every irrational number, so we have $f(\alpha) = \frac{4}{\pi^2} \approx 0.405285$ for any $\alpha \in \mathbb{R}^+ \setminus \mathbb{Q}^+$ . For rational arguments we can take $m , n \in \mathbb{N}$ with $\gcd(m,n) =1$ . The change of variables $s = n t$ yields $$ f \left(\frac{m}{n}\right) = \lim_{x \to \infty} \frac{1}{x} \int \limits_0^x \lvert\sin(m t) \cos(n t)\rvert \, \mathrm{d} t \, .$$ The integrand is periodic with period $\pi$ , so $$ f \left(\frac{m}{n}\right) = \lim_{x \to \infty} \frac{1}{x} \left[\bigg\lfloor \frac{x}{\pi} \bigg\rfloor \int \limits_0^\pi \lvert\sin(m t) \cos(n t)\rvert \, \mathrm{d} t + \mathcal{O} (1)\right] = \frac{1}{\pi} \int \limits_0^\pi \lvert\sin(m t) \cos(n t)\rvert \, \mathrm{d} t\, .$$ Now the idea is to split the interval of integration into subintervals on which the sign of the product is constant and then use $$ \sin(m t) \cos(n t) = \frac{\sin[(m+n)t] + \sin[(m-n)t]}{2}$$ to find the integrals. The result is basically given by a finite sum of cosines evaluated at the zeroes of the integrand. The first few results are \begin{align} f(1) &= \frac{1}{\pi} \approx 0.318301 \, , \\  f(2) &= \frac{4}{3 \pi} \approx 0.424413 \, , \\ f\left(\frac{1}{2}\right) &= \frac{2(2\sqrt{2}-1)}{3\pi} \approx 0.388004 \, . \end{align} Interestingly, most of the other values (especially those with large $m$ and $n$ ) seem to be very close to $\frac{4}{\pi^2}$ . This method can be used (at least in principle) to compute $f$ at every rational argument, but it becomes increasingly complicated for larger values of $m$ and $n$ . Maybe I am overlooking a simple trick, maybe there is a better method. My question is: How can we find a general expression for $f\left(\frac{m}{n}\right)$ with arbitrary coprime $m,n \in \mathbb{N}$ ? Edit 1 July 2020 Thanks to Zacky's bounty and River Li's and asgeige's nice answers we have some closed-form results for special cases and a promising (but still somewhat complicated) conjecture for the general case. Using similar methods, I have also found $$ f(m) = \frac{2}{\pi (m^2-1)} \left[m \csc \left(\frac{\pi}{2m}\right) \cos \left(\frac{\pi}{2m} 1_{2\mathbb{N}}(m)\right) - 1_{2 \mathbb{N}-1}(m)\right]$$ for $m \in \mathbb{N} \setminus \{1\}$ ( $1_A$ is the indicator function of the set $A$ ), which implies $\lim_{m \to \infty} f(m) = \frac{4}{\pi^2}$ . While a simple expression for general values of $m,n$ seems unlikely, it may be possible to show that $\lim_{n \to \infty} f \left(\frac{m}{n}\right) = \frac{4}{\pi^2}$ and $\lim_{m \to \infty} f \left(\frac{m}{n}\right) = \frac{4}{\pi^2}$ do indeed hold for fixed $m \in \mathbb{N}$ and $n \in \mathbb{N}$ , respectively. Edit 11 January 2021 Thanks to River Li's second answer we now have a proof of the conjectured limits. We can even use partial fractions and the pole expansions of $\csc$ and $\cot$ to simplify the remaining series and obtain the following general result (valid for $m, n \in \mathbb{N}$ coprime and not both equal to $1$ ): $$ f\left(\frac{m}{n}\right) = \frac{g_m\left(\frac{\pi}{2m}\right) - g_m \left(\frac{\pi}{2n}\right)}{m^2-n^2} \, , \, g_m = \begin{cases} x \mapsto \frac{\csc(x)}{x} &, \, m \in 2 \mathbb{N} - 1 \\ x \mapsto \frac{\cot(x)}{x} &, \, m \in 2 \mathbb{N}\end{cases} \, . $$","Motivated by this nice question I have been trying to compute the function defined by Note that follows from the Cauchy-Schwarz inequality. In the answers to the original question the equidistribution theorem is used to show that holds. This argument can be extended to every irrational number, so we have for any . For rational arguments we can take with . The change of variables yields The integrand is periodic with period , so Now the idea is to split the interval of integration into subintervals on which the sign of the product is constant and then use to find the integrals. The result is basically given by a finite sum of cosines evaluated at the zeroes of the integrand. The first few results are Interestingly, most of the other values (especially those with large and ) seem to be very close to . This method can be used (at least in principle) to compute at every rational argument, but it becomes increasingly complicated for larger values of and . Maybe I am overlooking a simple trick, maybe there is a better method. My question is: How can we find a general expression for with arbitrary coprime ? Edit 1 July 2020 Thanks to Zacky's bounty and River Li's and asgeige's nice answers we have some closed-form results for special cases and a promising (but still somewhat complicated) conjecture for the general case. Using similar methods, I have also found for ( is the indicator function of the set ), which implies . While a simple expression for general values of seems unlikely, it may be possible to show that and do indeed hold for fixed and , respectively. Edit 11 January 2021 Thanks to River Li's second answer we now have a proof of the conjectured limits. We can even use partial fractions and the pole expansions of and to simplify the remaining series and obtain the following general result (valid for coprime and not both equal to ):","f: \mathbb{R}^+ \to \left[0,\frac{1}{2}\right] f(\alpha) = \lim_{x \to \infty} \frac{1}{x} \int \limits_0^x \lvert\sin(\alpha s) \cos(s)\rvert \, \mathrm{d} s \, . f(\alpha) \leq \frac{1}{2} f(3 - 2 \sqrt{2}) = \frac{4}{\pi^2} f(\alpha) = \frac{4}{\pi^2} \approx 0.405285 \alpha \in \mathbb{R}^+ \setminus \mathbb{Q}^+ m , n \in \mathbb{N} \gcd(m,n) =1 s = n t  f \left(\frac{m}{n}\right) = \lim_{x \to \infty} \frac{1}{x} \int \limits_0^x \lvert\sin(m t) \cos(n t)\rvert \, \mathrm{d} t \, . \pi  f \left(\frac{m}{n}\right) = \lim_{x \to \infty} \frac{1}{x} \left[\bigg\lfloor \frac{x}{\pi} \bigg\rfloor \int \limits_0^\pi \lvert\sin(m t) \cos(n t)\rvert \, \mathrm{d} t + \mathcal{O} (1)\right] = \frac{1}{\pi} \int \limits_0^\pi \lvert\sin(m t) \cos(n t)\rvert \, \mathrm{d} t\, .  \sin(m t) \cos(n t) = \frac{\sin[(m+n)t] + \sin[(m-n)t]}{2} \begin{align}
f(1) &= \frac{1}{\pi} \approx 0.318301 \, , \\ 
f(2) &= \frac{4}{3 \pi} \approx 0.424413 \, , \\
f\left(\frac{1}{2}\right) &= \frac{2(2\sqrt{2}-1)}{3\pi} \approx 0.388004 \, .
\end{align} m n \frac{4}{\pi^2} f m n f\left(\frac{m}{n}\right) m,n \in \mathbb{N}  f(m) = \frac{2}{\pi (m^2-1)} \left[m \csc \left(\frac{\pi}{2m}\right) \cos \left(\frac{\pi}{2m} 1_{2\mathbb{N}}(m)\right) - 1_{2 \mathbb{N}-1}(m)\right] m \in \mathbb{N} \setminus \{1\} 1_A A \lim_{m \to \infty} f(m) = \frac{4}{\pi^2} m,n \lim_{n \to \infty} f \left(\frac{m}{n}\right) = \frac{4}{\pi^2} \lim_{m \to \infty} f \left(\frac{m}{n}\right) = \frac{4}{\pi^2} m \in \mathbb{N} n \in \mathbb{N} \csc \cot m, n \in \mathbb{N} 1  f\left(\frac{m}{n}\right) = \frac{g_m\left(\frac{\pi}{2m}\right) - g_m \left(\frac{\pi}{2n}\right)}{m^2-n^2} \, , \, g_m = \begin{cases} x \mapsto \frac{\csc(x)}{x} &, \, m \in 2 \mathbb{N} - 1 \\ x \mapsto \frac{\cot(x)}{x} &, \, m \in 2 \mathbb{N}\end{cases} \, . ","['calculus', 'integration', 'limits', 'definite-integrals', 'trigonometric-integrals']"
8,Prove that the min and max of 2 continuous function are continuous,Prove that the min and max of 2 continuous function are continuous,,"Prove that if $f$ and $g$ are continuous functions the so are $\min⁡\{f(x),g(x)\}$ and $\max⁡\{f(x),g(x)\}$ I know this is true when $f$ and $g$ are not intersect each other, then I can compare them. However, I don't know how to prove it's true when they are intersect.","Prove that if and are continuous functions the so are and I know this is true when and are not intersect each other, then I can compare them. However, I don't know how to prove it's true when they are intersect.","f g \min⁡\{f(x),g(x)\} \max⁡\{f(x),g(x)\} f g","['calculus', 'proof-writing']"
9,"What is $\int_0^1 \ln (1-x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx$?",What is ?,"\int_0^1 \ln (1-x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx","There are well-known closed-form evaluations for integrals of the form $\int_0^1  a(x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx  $ for certain algebraic functions $a(x)$. For example, an evaluation of this form is given in the following link: Evaluating $\int_0^1 \log \log \left(\frac{1}{x}\right) \frac{dx}{1+x^2}$ . Closed-form evaluations for integrals of the form $\int_0^1  a(x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx$ may be used to evaluate Glaisher-type infinite products. For example, the evaluation of $ \int_{0}^{1}   \frac{\ln \left(\ln \left(\frac{1}{x}\right)\right)}{x^{2}+1} \, dx  $ may be used to prove that: \begin{equation*} \prod _{i=0}^{\infty }     \frac{  (4 i+3)^{\frac{1}{4 i+3}} }{ (4 i+1)^{\frac{1}{4 i+1}}} =      \frac{   2^{   \frac{ \pi}{2} } e^{\frac{\gamma  \pi }{4}} \pi ^{   \frac{3 \pi}{4}}}{  \Gamma^{\pi } \left(\frac{1}{4}\right)}.  \end{equation*} Mathematica does not seem to be able to evaluate integrals of the form $\int_0^1  T(x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx  $ for certain elementary transcendental functions $T\left(x\right)$. For example, Mathematica is unable to evaluate the following integrals, and the Inverse Symbolic Calculator is not currently able to identify the following numbers: \begin{align*}  \int_0^1 \ln (1-x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx   &   =  1.834962542065861...   \\  \int_0^1 \ln (x+1) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx   &   =   -0.4553656688368576...      \\ \int_0^1 \tan^{-1}(x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx   &   =  -0.521812852476476...   \end{align*} I am interested in evaluating $\int_0^1 \ln (1-x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx$, but it does not seem to be feasible to use integration by parts to evaluate this integral, and it is not obvious to me how to evaluate this integral using substitution. I have also considered expanding the integrand of this integral using power series to try to evaluate this integral. A new closed-form evaluation of this integral would be interesting, partly because such an evaluation may be used to construct new Glaisher-type infinite products. A similar problem is given in the following link: Integral ${\large\int}_0^1\frac{\ln^2\ln\left(\frac1x\right)}{1+x+x^2}dx$ . I've tried to mimic the strategy used in the answer given in this link, but using the substitution $t=\ln\left(\frac{1}{x}\right)$ in this case yields the following integral, which Mathematica is unable to evaluate: \begin{equation*} \int_0^{\infty } e^{-t} \ln \left(1-e^{-t}\right) \ln (t) \, dt. \end{equation*} It is also natural to ask: What are some non-trivial examples of elementary transcendental functions $T\left(x\right)$ such that there is a closed-form evaluation of $\int_0^1  T\left(x\right) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx$? What is $\int_0^1  \sin^{-1} x \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx$? What is $\int_0^1  \ln(x^{2} +1) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx$?","There are well-known closed-form evaluations for integrals of the form $\int_0^1  a(x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx  $ for certain algebraic functions $a(x)$. For example, an evaluation of this form is given in the following link: Evaluating $\int_0^1 \log \log \left(\frac{1}{x}\right) \frac{dx}{1+x^2}$ . Closed-form evaluations for integrals of the form $\int_0^1  a(x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx$ may be used to evaluate Glaisher-type infinite products. For example, the evaluation of $ \int_{0}^{1}   \frac{\ln \left(\ln \left(\frac{1}{x}\right)\right)}{x^{2}+1} \, dx  $ may be used to prove that: \begin{equation*} \prod _{i=0}^{\infty }     \frac{  (4 i+3)^{\frac{1}{4 i+3}} }{ (4 i+1)^{\frac{1}{4 i+1}}} =      \frac{   2^{   \frac{ \pi}{2} } e^{\frac{\gamma  \pi }{4}} \pi ^{   \frac{3 \pi}{4}}}{  \Gamma^{\pi } \left(\frac{1}{4}\right)}.  \end{equation*} Mathematica does not seem to be able to evaluate integrals of the form $\int_0^1  T(x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx  $ for certain elementary transcendental functions $T\left(x\right)$. For example, Mathematica is unable to evaluate the following integrals, and the Inverse Symbolic Calculator is not currently able to identify the following numbers: \begin{align*}  \int_0^1 \ln (1-x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx   &   =  1.834962542065861...   \\  \int_0^1 \ln (x+1) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx   &   =   -0.4553656688368576...      \\ \int_0^1 \tan^{-1}(x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx   &   =  -0.521812852476476...   \end{align*} I am interested in evaluating $\int_0^1 \ln (1-x) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx$, but it does not seem to be feasible to use integration by parts to evaluate this integral, and it is not obvious to me how to evaluate this integral using substitution. I have also considered expanding the integrand of this integral using power series to try to evaluate this integral. A new closed-form evaluation of this integral would be interesting, partly because such an evaluation may be used to construct new Glaisher-type infinite products. A similar problem is given in the following link: Integral ${\large\int}_0^1\frac{\ln^2\ln\left(\frac1x\right)}{1+x+x^2}dx$ . I've tried to mimic the strategy used in the answer given in this link, but using the substitution $t=\ln\left(\frac{1}{x}\right)$ in this case yields the following integral, which Mathematica is unable to evaluate: \begin{equation*} \int_0^{\infty } e^{-t} \ln \left(1-e^{-t}\right) \ln (t) \, dt. \end{equation*} It is also natural to ask: What are some non-trivial examples of elementary transcendental functions $T\left(x\right)$ such that there is a closed-form evaluation of $\int_0^1  T\left(x\right) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx$? What is $\int_0^1  \sin^{-1} x \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx$? What is $\int_0^1  \ln(x^{2} +1) \ln \left(\ln \left(\frac{1}{x}\right)\right) \, dx$?",,"['calculus', 'integration', 'definite-integrals', 'logarithms', 'closed-form']"
10,Integral $\int_0^\infty \frac{\sin x}{\cosh ax+\cos x}\frac{x}{x^2-\pi^2}dx=\tan^{-1}\left(\frac{1}{a}\right)-\frac{1}{a}$,Integral,\int_0^\infty \frac{\sin x}{\cosh ax+\cos x}\frac{x}{x^2-\pi^2}dx=\tan^{-1}\left(\frac{1}{a}\right)-\frac{1}{a},Please help me prove the following identity: $$\int_0^\infty \frac{\sin x}{\cosh ax+\cos x}\frac{x}{x^2-\pi^2}dx=\tan^{-1}\left(\frac{1}{a}\right)-\frac{1}{a}\quad a>0$$ This integral is from Gradshteyn and Ryzhik's tables.,Please help me prove the following identity: $$\int_0^\infty \frac{\sin x}{\cosh ax+\cos x}\frac{x}{x^2-\pi^2}dx=\tan^{-1}\left(\frac{1}{a}\right)-\frac{1}{a}\quad a>0$$ This integral is from Gradshteyn and Ryzhik's tables.,,"['calculus', 'integration', 'analysis', 'definite-integrals', 'improper-integrals']"
11,Evaluating $\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}}{n}$,Evaluating,\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}}{n},I would appreciate to understand the main steps giving the evaluation of this series: $$ S=\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}}{n}$$ where $H_n$ is the harmonic number. I've tried with no success to obtain this sum with the help of Wolfram Alpha.,I would appreciate to understand the main steps giving the evaluation of this series: $$ S=\sum_{n=1}^{\infty} (-1)^{n-1}\frac{H_{2n}}{n}$$ where $H_n$ is the harmonic number. I've tried with no success to obtain this sum with the help of Wolfram Alpha.,,"['calculus', 'sequences-and-series', 'closed-form']"
12,Prove the integral evaluates to $\frac{K}{\pi}$,Prove the integral evaluates to,\frac{K}{\pi},"Yesterday I received the following integral that might require some tedious steps to do $$\int_0^{\infty}{\small\left[ \frac{x}{\log^2\left(e^{\large x^2}-1\right)}- \frac{x}{\sqrt{e^{\large x^2}-1}\log^2\left(e^{\large x^2}-1\right)}-\frac{x}{\sqrt{e^{\large x^2}-1}\log\left(\left(e^{\large x^2}-1\right)^2\right)}\right] \ dx}=\frac{K}{\pi}$$ where $K$ is the Catalan's constant What to do? Well, one can let $x^2\mapsto x$, $e^{x}-1\mapsto x$ and then we get an integral where we can add a parameter of the type $x^s$, and then I have an integral $I(s)$ that I differentiate twice with respect to $s$, and then integrate over $[0,\infty)$. Of course, the last step is to integrate twice with respect to $s$, but this is not the work I love. My feeling is that there is a pretty easy way to prove the integral result that I don't see yet,  but maybe you see it and share it with me.","Yesterday I received the following integral that might require some tedious steps to do $$\int_0^{\infty}{\small\left[ \frac{x}{\log^2\left(e^{\large x^2}-1\right)}- \frac{x}{\sqrt{e^{\large x^2}-1}\log^2\left(e^{\large x^2}-1\right)}-\frac{x}{\sqrt{e^{\large x^2}-1}\log\left(\left(e^{\large x^2}-1\right)^2\right)}\right] \ dx}=\frac{K}{\pi}$$ where $K$ is the Catalan's constant What to do? Well, one can let $x^2\mapsto x$, $e^{x}-1\mapsto x$ and then we get an integral where we can add a parameter of the type $x^s$, and then I have an integral $I(s)$ that I differentiate twice with respect to $s$, and then integrate over $[0,\infty)$. Of course, the last step is to integrate twice with respect to $s$, but this is not the work I love. My feeling is that there is a pretty easy way to prove the integral result that I don't see yet,  but maybe you see it and share it with me.",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals', 'catalans-constant']"
13,A functional equation with no solution,A functional equation with no solution,,"Let $f:\mathbb{R}\to (0,\infty)$ be a differentiable function satisfying  $$f(f(x))=f^\prime(x)$$for each $x$. Show no such function exists. I got this problem in an exam. I haven't done anything significant with it. I have found that $f^\prime=f\circ f>0$ so $f(f(x))>f(0)$ hence we have $f^\prime(x)>f(0)$. But I have no idea how to use it. I tried to apply the mean value theorem on $$\frac{f(f(x))-f(0)}{f(x)}=f^\prime(c)=\frac{f^\prime(x)-f(0)}{f(x)}$$ but that doesn't lead anywhere. Can someone help me? Thanks a lot.","Let $f:\mathbb{R}\to (0,\infty)$ be a differentiable function satisfying  $$f(f(x))=f^\prime(x)$$for each $x$. Show no such function exists. I got this problem in an exam. I haven't done anything significant with it. I have found that $f^\prime=f\circ f>0$ so $f(f(x))>f(0)$ hence we have $f^\prime(x)>f(0)$. But I have no idea how to use it. I tried to apply the mean value theorem on $$\frac{f(f(x))-f(0)}{f(x)}=f^\prime(c)=\frac{f^\prime(x)-f(0)}{f(x)}$$ but that doesn't lead anywhere. Can someone help me? Thanks a lot.",,"['calculus', 'derivatives', 'functional-equations']"
14,Derivative of the elliptic integral of the first kind,Derivative of the elliptic integral of the first kind,,The complete elliptic integral of the first kind is defined as $$K(k)=\int_0^{\pi/2} \frac{dx}{\sqrt{1-k^2\sin^2{x}}}$$ and the complete elliptic integral of the second kind is defined as $$E(k)=\int_0^{\pi/2}\sqrt{1-k^2\sin^2{x}}~dx$$ for $0\leq k<1$. I'm supposed to prove the following relation $$K'(k)=\frac{E(k)}{k(1-k^2)}-\frac{K(k)}{k}.$$ What I tried so far Without much thought about the exchange of integration and differentiation I tried to compute \begin{align}K'(k)&=\int_0^{\pi/2} \frac{k\sin^2{x}}{(1-k^2\sin^2{x})^{3/2}}dx=-\frac{1}{k}\int_0^{\pi/2}\left(\frac{1-k^2\sin^2{x}}{(1-k^2\sin^2{x})^{3/2}}-\frac{1}{(1-k^2\sin^2{x})^{3/2}}\right)dx\\ &=-\frac{K(k)}{k}+\frac{1}{k}\int_0^{\pi/2}\frac{1}{(1-k^2\sin^2{x})^{3/2}} dx.\end{align} Comparing this with the result I'm supposed to obtain it would remain to show $$\int_0^{\pi/2}\frac{1}{(1-k^2\sin^2{x})^{3/2}} dx=\int_0^{\pi/2}\frac{\sqrt{1-k^2\sin^2{x}}}{1-k^2}dx.$$ Some numerical computations suggest that this identity is correct but I have know idea how to show it. Any hints or solutions would be appreciated!,The complete elliptic integral of the first kind is defined as $$K(k)=\int_0^{\pi/2} \frac{dx}{\sqrt{1-k^2\sin^2{x}}}$$ and the complete elliptic integral of the second kind is defined as $$E(k)=\int_0^{\pi/2}\sqrt{1-k^2\sin^2{x}}~dx$$ for $0\leq k<1$. I'm supposed to prove the following relation $$K'(k)=\frac{E(k)}{k(1-k^2)}-\frac{K(k)}{k}.$$ What I tried so far Without much thought about the exchange of integration and differentiation I tried to compute \begin{align}K'(k)&=\int_0^{\pi/2} \frac{k\sin^2{x}}{(1-k^2\sin^2{x})^{3/2}}dx=-\frac{1}{k}\int_0^{\pi/2}\left(\frac{1-k^2\sin^2{x}}{(1-k^2\sin^2{x})^{3/2}}-\frac{1}{(1-k^2\sin^2{x})^{3/2}}\right)dx\\ &=-\frac{K(k)}{k}+\frac{1}{k}\int_0^{\pi/2}\frac{1}{(1-k^2\sin^2{x})^{3/2}} dx.\end{align} Comparing this with the result I'm supposed to obtain it would remain to show $$\int_0^{\pi/2}\frac{1}{(1-k^2\sin^2{x})^{3/2}} dx=\int_0^{\pi/2}\frac{\sqrt{1-k^2\sin^2{x}}}{1-k^2}dx.$$ Some numerical computations suggest that this identity is correct but I have know idea how to show it. Any hints or solutions would be appreciated!,,"['calculus', 'special-functions', 'elliptic-integrals']"
15,Evaluate $\int_{0}^{\pi/4}x\ln^{2}(\sin(x))dx$,Evaluate,\int_{0}^{\pi/4}x\ln^{2}(\sin(x))dx,"Here is a some what challenging log sine integral. $$I=\int_{0}^{\pi/4}x\ln^{2}(\sin(x))dx$$ The upper limit of integration is $\frac{\pi}{4}$ instead of the usual $\frac{\pi}{2}$ . I tried the famous identity $\displaystyle\ln(\sin(x))=-\sum_{k=1}^{\infty}\frac{\cos(2kx)}{k}-\ln(2)$ But, the log is squared and I do not think Parseval is applicable here. But, I may be wrong.  By Parseval, I mean $\displaystyle\sum_{n=1}^{\infty}\sum_{k=1}^{\infty}\frac{\cos(kx)\cos(nx)}{kn}$ . I have tried other means.  I made the sub $t=\sin(x)$ . This led to $\displaystyle\int_{0}^{\frac{1}{\sqrt{2}}}\frac{\sin^{-1}(t)\ln^{2}(t)}{\sqrt{1-t^{2}}}dt$ Now, I thought perhaps the famous sum $\displaystyle\frac{\sin^{-1}(t)}{\sqrt{1-t^{2}}}=\sum_{k=1}^{\infty}\frac{(2t)^{2k-1}}{k\binom{2k}{k}}$ could be used. So, I integrated and got the form $$I=\frac{\ln^{2}(2)}{16}\sum_{k=1}^{\infty}\frac{(k!)^{2}2^{k}}{k^{2}(2k)!}+\frac{\ln(2)}{8}\sum_{k=1}^{\infty}\frac{(k!)^{2}2^{k}}{k^{3}(2k)!}+\frac{1}{8}\sum_{k=1}^{\infty}\frac{(k!)^{2}2^{k}}{k^{4}(2k)!}$$ These sums actually evaluate to the correct integral result, so it would appear they are equivalent to said integral. But evaluating them may be even more daunting. Though the first one from the left evaluates to $\frac{{\pi}^{2}}{8}$ and the center one evaluates to $$\frac{\ln(2)}{8}\left(\pi G-\frac{35}{16}\zeta(3)+\frac{{\pi}^{2}}{8}\ln(2)\right)\;.$$ This center one looks to be related to the integral: $\displaystyle8\ln(2)\int_{0}^{\frac{\pi}{4}}\ln(\sin(x))dx=\frac{35}{16}\ln(2)\zeta(3)-\pi\ln(2)G-\frac{{\pi}^{2}\ln^{2}(2)}{4}$ , which I got by expanding the square of the log sine and integrating. This just leaves the sum to the right.  It is tougher and I can find no closed form. Alas, If I could evaluate it then I think the integral would be solved. I also thought...how about a closed form of some sort for $$\int_{0}^{\frac{\pi}{4}}x\sin^{a}(x)dx$$ that could be differentiated w.r.t 'a' and applied?. I do not know of a closed form for  this, though. I got an encouraging looking solution by expanding, making the sub $t=x/2$ , then solving for the integral on the far right (which is the integral in question): $\displaystyle  \int_{0}^{\frac{\pi}{2}}x\ln^{2}(2\sin(x/2))dx$ $=\displaystyle \ln^{2}(2)\int_{0}^{\frac{\pi}{2}}xdx+8\ln(2)\int_{0}^{\frac{\pi}{4}}x\ln(\sin(x))dx+\color{red}4\int_{0}^{\frac{\pi}{4}}x\ln^{2}(\sin(x))dx$ But, it did not check numerically. ( Error has now been fixed. - Editor ) Numerically, the integral in question should evaluate to $I = 0.263605....$ Then again, I may just have a computation error in all of that. So, does anyone know of a method for evaluating this log-sine integral?. EDIT:  Here is what Mathematica gave for the solution.  It may be too involved to be worth the effort. $\displaystyle I=\frac{37{\pi}^{4}}{92160}-\frac{\pi}{4}\sum_{k=1}^{\infty}\frac{\sin(\pi k/4)}{2^{k/2}k^{3}}$ $\displaystyle +\frac{\pi}{8}\ln(2)G+\frac{25}{768}{\pi}^{2}\ln^{2}(2)+\frac{\ln^{4}(2)}{384}+\sum_{k=1}^{\infty}\frac{\cos(\pi k/4)}{2^{k/2}k^{4}}-\frac{35}{128}\ln(2)\zeta(3)$ . Sometimes with these math engines the solution can be simplified down. But, maybe not in this case. Those complex polylogs may pose a problem.","Here is a some what challenging log sine integral. The upper limit of integration is instead of the usual . I tried the famous identity But, the log is squared and I do not think Parseval is applicable here. But, I may be wrong.  By Parseval, I mean . I have tried other means.  I made the sub . This led to Now, I thought perhaps the famous sum could be used. So, I integrated and got the form These sums actually evaluate to the correct integral result, so it would appear they are equivalent to said integral. But evaluating them may be even more daunting. Though the first one from the left evaluates to and the center one evaluates to This center one looks to be related to the integral: , which I got by expanding the square of the log sine and integrating. This just leaves the sum to the right.  It is tougher and I can find no closed form. Alas, If I could evaluate it then I think the integral would be solved. I also thought...how about a closed form of some sort for that could be differentiated w.r.t 'a' and applied?. I do not know of a closed form for  this, though. I got an encouraging looking solution by expanding, making the sub , then solving for the integral on the far right (which is the integral in question): But, it did not check numerically. ( Error has now been fixed. - Editor ) Numerically, the integral in question should evaluate to Then again, I may just have a computation error in all of that. So, does anyone know of a method for evaluating this log-sine integral?. EDIT:  Here is what Mathematica gave for the solution.  It may be too involved to be worth the effort. . Sometimes with these math engines the solution can be simplified down. But, maybe not in this case. Those complex polylogs may pose a problem.",I=\int_{0}^{\pi/4}x\ln^{2}(\sin(x))dx \frac{\pi}{4} \frac{\pi}{2} \displaystyle\ln(\sin(x))=-\sum_{k=1}^{\infty}\frac{\cos(2kx)}{k}-\ln(2) \displaystyle\sum_{n=1}^{\infty}\sum_{k=1}^{\infty}\frac{\cos(kx)\cos(nx)}{kn} t=\sin(x) \displaystyle\int_{0}^{\frac{1}{\sqrt{2}}}\frac{\sin^{-1}(t)\ln^{2}(t)}{\sqrt{1-t^{2}}}dt \displaystyle\frac{\sin^{-1}(t)}{\sqrt{1-t^{2}}}=\sum_{k=1}^{\infty}\frac{(2t)^{2k-1}}{k\binom{2k}{k}} I=\frac{\ln^{2}(2)}{16}\sum_{k=1}^{\infty}\frac{(k!)^{2}2^{k}}{k^{2}(2k)!}+\frac{\ln(2)}{8}\sum_{k=1}^{\infty}\frac{(k!)^{2}2^{k}}{k^{3}(2k)!}+\frac{1}{8}\sum_{k=1}^{\infty}\frac{(k!)^{2}2^{k}}{k^{4}(2k)!} \frac{{\pi}^{2}}{8} \frac{\ln(2)}{8}\left(\pi G-\frac{35}{16}\zeta(3)+\frac{{\pi}^{2}}{8}\ln(2)\right)\;. \displaystyle8\ln(2)\int_{0}^{\frac{\pi}{4}}\ln(\sin(x))dx=\frac{35}{16}\ln(2)\zeta(3)-\pi\ln(2)G-\frac{{\pi}^{2}\ln^{2}(2)}{4} \int_{0}^{\frac{\pi}{4}}x\sin^{a}(x)dx t=x/2 \displaystyle  \int_{0}^{\frac{\pi}{2}}x\ln^{2}(2\sin(x/2))dx =\displaystyle \ln^{2}(2)\int_{0}^{\frac{\pi}{2}}xdx+8\ln(2)\int_{0}^{\frac{\pi}{4}}x\ln(\sin(x))dx+\color{red}4\int_{0}^{\frac{\pi}{4}}x\ln^{2}(\sin(x))dx I = 0.263605.... \displaystyle I=\frac{37{\pi}^{4}}{92160}-\frac{\pi}{4}\sum_{k=1}^{\infty}\frac{\sin(\pi k/4)}{2^{k/2}k^{3}} \displaystyle +\frac{\pi}{8}\ln(2)G+\frac{25}{768}{\pi}^{2}\ln^{2}(2)+\frac{\ln^{4}(2)}{384}+\sum_{k=1}^{\infty}\frac{\cos(\pi k/4)}{2^{k/2}k^{4}}-\frac{35}{128}\ln(2)\zeta(3),"['calculus', 'integration', 'sequences-and-series', 'definite-integrals', 'closed-form']"
16,"Is $[0,1]$ an *oriented* manifold with boundary? (and Stokes theorem)",Is  an *oriented* manifold with boundary? (and Stokes theorem),"[0,1]","The definitions I am using are a manifold with boundary is something locally homeomorphic to $(0,1] \times \mathbb{R}^n$ or $\mathbb{R}^n$. an oriented manifold is one where the transition functions between any two charts have positive Jacobian. Its clear that $[0,1]$ is a manifold with boundary. Is it oriented? I seem to be getting the conclusion that it is not, because around $0$, the local chart is $x \to 1-x$, and around $1$ the local chart is $x \to x$, and these have opposite orientations. But Stokes theorem surely should apply to this setting and reduce to the fundamental theorem of calculus. Recall that Stokes theorem says that if $M$ is a compact oriented $n$-manifold with boundary $\partial M$ with the induced orientation, and $\omega$ is an $n-1$ form, then $$\int_{\partial M} \omega = \int_M d\omega.$$ Taking $\omega=f$ a zero form, i.e., function and $M=[0,1]$, I expect to recover $$f(1)-f(0) = \int_0^1 f'(x) dx ,$$ so $[0,1]$ I suspect is an oriented manifold with boundary, but I'm not seeing exactly why. In a related vein, I think I can see intuitively that the closed unit disk is an oriented manifold with boundary, and then Stokes gives Green's theorem. @Bill, in a comment below you wrote ""the chart containing 1 orients the interval 0→1.""  Well, let me be more precise: the chart $U_1=(0,1] \to (0,1]$ given by $x \to x$ (for $x \in (0,1]$) orients $(0,1]$. I cannot extend this particular chart to include $0$ because then $[0,1]$ would not be a homeomorphic to $(0,1]$ which is in my definition of manifold with closed boundary. To get a chart including the zero, I need to include another chart, e.g $U_2=[0,1)$ and then the map $x \to 1-x$ would be a homeomorphism $U_2 \to (0,1]$ as required in the definition I am using for manifold with boundary. But now, these charts have opposite orientations. I don't know how to come up with two (or more) charts that do not give opposite orientations.  The definition of manifold with boundary I am using is the one on p.25 of Voisin's Hodge Theory and Complex Analytic Geometry 1, and its easily seen to be equivalent to the more standard one homeomorphic to an open subset of the closed upper half plane. I agree with your second commment.","The definitions I am using are a manifold with boundary is something locally homeomorphic to $(0,1] \times \mathbb{R}^n$ or $\mathbb{R}^n$. an oriented manifold is one where the transition functions between any two charts have positive Jacobian. Its clear that $[0,1]$ is a manifold with boundary. Is it oriented? I seem to be getting the conclusion that it is not, because around $0$, the local chart is $x \to 1-x$, and around $1$ the local chart is $x \to x$, and these have opposite orientations. But Stokes theorem surely should apply to this setting and reduce to the fundamental theorem of calculus. Recall that Stokes theorem says that if $M$ is a compact oriented $n$-manifold with boundary $\partial M$ with the induced orientation, and $\omega$ is an $n-1$ form, then $$\int_{\partial M} \omega = \int_M d\omega.$$ Taking $\omega=f$ a zero form, i.e., function and $M=[0,1]$, I expect to recover $$f(1)-f(0) = \int_0^1 f'(x) dx ,$$ so $[0,1]$ I suspect is an oriented manifold with boundary, but I'm not seeing exactly why. In a related vein, I think I can see intuitively that the closed unit disk is an oriented manifold with boundary, and then Stokes gives Green's theorem. @Bill, in a comment below you wrote ""the chart containing 1 orients the interval 0→1.""  Well, let me be more precise: the chart $U_1=(0,1] \to (0,1]$ given by $x \to x$ (for $x \in (0,1]$) orients $(0,1]$. I cannot extend this particular chart to include $0$ because then $[0,1]$ would not be a homeomorphic to $(0,1]$ which is in my definition of manifold with closed boundary. To get a chart including the zero, I need to include another chart, e.g $U_2=[0,1)$ and then the map $x \to 1-x$ would be a homeomorphism $U_2 \to (0,1]$ as required in the definition I am using for manifold with boundary. But now, these charts have opposite orientations. I don't know how to come up with two (or more) charts that do not give opposite orientations.  The definition of manifold with boundary I am using is the one on p.25 of Voisin's Hodge Theory and Complex Analytic Geometry 1, and its easily seen to be equivalent to the more standard one homeomorphic to an open subset of the closed upper half plane. I agree with your second commment.",,"['calculus', 'manifolds']"
17,distance from the centre of a $n$-cube as $n \rightarrow \infty$,distance from the centre of a -cube as,n n \rightarrow \infty,"I've figured out the pattern for calculating the average distance from the centre of an n-cube; but I don't have a formula for the answer. Is there an easy way to figure this out? Average distance of points from the centre of a unit 0-cube (point) $$A_0 = 0$$ Average distance of points from the centre of a unit 1-cube (line) $$A_1 = \int_{x=-\frac{1}{2}}^{x=\frac{1}{2}}{x}\; dx = 0.250000$$ Average distance of points from the centre of a unit 2-cube (square) $$A_2 = \int_{x=-\frac{1}{2}}^{x=\frac{1}{2}}{\int_{y=-\frac{1}{2}}^{y=\frac{1}{2}}\sqrt{x^2+y^2}}\;dy \; dx \approx 0.382598$$ Average distance of points from the centre of a unit 3-cube (cube) $$A_3 = \int_{x=-\frac{1}{2}}^{x=\frac{1}{2}}{\int_{y=-\frac{1}{2}}^{y=\frac{1}{2}}\int_{z=-\frac{1}{2}}^{z=\frac{1}{2}}{\sqrt{x^2+y^2+z^2}}}\;dz\;dy \; dx \approx 0.480296$$ Average distance of points from the centre of a unit 4-cube (tesseract) $$A_4 \approx 0.560950$$ My gut instinct is that $A_n \rightarrow \infty$ as $n \rightarrow \infty$ as in my head higher dimensional cubes become more spiky and I expect the mass to become concentrated in the corners. I feel justified in saying this because the number of ""corners"" is $2^n$ with a potential distance of $\frac{\sqrt{n}}{2}$ If somehow it were to approach some limit, that would be cool (to me at least) Thanks in advance for any help, advice or answers","I've figured out the pattern for calculating the average distance from the centre of an n-cube; but I don't have a formula for the answer. Is there an easy way to figure this out? Average distance of points from the centre of a unit 0-cube (point) $$A_0 = 0$$ Average distance of points from the centre of a unit 1-cube (line) $$A_1 = \int_{x=-\frac{1}{2}}^{x=\frac{1}{2}}{x}\; dx = 0.250000$$ Average distance of points from the centre of a unit 2-cube (square) $$A_2 = \int_{x=-\frac{1}{2}}^{x=\frac{1}{2}}{\int_{y=-\frac{1}{2}}^{y=\frac{1}{2}}\sqrt{x^2+y^2}}\;dy \; dx \approx 0.382598$$ Average distance of points from the centre of a unit 3-cube (cube) $$A_3 = \int_{x=-\frac{1}{2}}^{x=\frac{1}{2}}{\int_{y=-\frac{1}{2}}^{y=\frac{1}{2}}\int_{z=-\frac{1}{2}}^{z=\frac{1}{2}}{\sqrt{x^2+y^2+z^2}}}\;dz\;dy \; dx \approx 0.480296$$ Average distance of points from the centre of a unit 4-cube (tesseract) $$A_4 \approx 0.560950$$ My gut instinct is that $A_n \rightarrow \infty$ as $n \rightarrow \infty$ as in my head higher dimensional cubes become more spiky and I expect the mass to become concentrated in the corners. I feel justified in saying this because the number of ""corners"" is $2^n$ with a potential distance of $\frac{\sqrt{n}}{2}$ If somehow it were to approach some limit, that would be cool (to me at least) Thanks in advance for any help, advice or answers",,"['calculus', 'integration', 'limits', 'multivariable-calculus']"
18,"If $n^c\in\mathbb N$ for every $n\in\mathbb N$, then $c$ is a non-negative integer?","If  for every , then  is a non-negative integer?",n^c\in\mathbb N n\in\mathbb N c,"Supposing that a real number $c$ is given, is the following true? ""If $n^c$ is a natural number for every natural number $n$, then $c$ is a non-negative integer."" Though this seems true, I can't prove that. Can anyone help?","Supposing that a real number $c$ is given, is the following true? ""If $n^c$ is a natural number for every natural number $n$, then $c$ is a non-negative integer."" Though this seems true, I can't prove that. Can anyone help?",,"['calculus', 'algebra-precalculus', 'exponentiation']"
19,Proof of this fairly obscure differentiation trick?,Proof of this fairly obscure differentiation trick?,,"Suppose we're tying to differentiate the function $f(x)=x^x$. Now the textbook method would be to notice that $f(x)=e^{x \log{x}}$ and use the chain rule to find $$f'(x)=(1+\log{x})\ e^{x \log{x}}=(1+\log{x})\ x^x.$$ But suppose that I didn't make this observation and instead tried to apply the following differentiation rules: $$\frac{d}{dx}x^c=cx^{c-1} \qquad (1)\\ \frac{d}{dx}c^x = \log{c}\ \cdot c^x \quad (2)$$ which are valid for any constant $c$. Obviously neither rules are applicable to the form $x^x$ because in this case neither the base nor the exponent are constant. But if I pretend that the exponent is constant and apply rule $(1)$, I would get $f'(x)\stackrel{?}{=}x\cdot x^{x-1}=x^x.$ Likewise, if I pretend that the base is constant and apply rule $(2)$ I obtain $f'(x)\stackrel{?}{=}\log{x}\cdot x^x$. It isn't hard to see that neither of the derivatives are correct. But here's where the magic happens: if we sum the two “derivatives” we end up with $$x^x+ \log{x}\cdot x^x=(1+\log{x})\ x^x$$ which is the correct expression for $f'(x)$. This same trick yields correct results in other contexts as well. In fact, in some cases it turns out to be a more efficient way of taking derivatives. For example, consider $$g(x)=x^2 = \color{blue} x\cdot \color{red} x.$$ If we pretend the blue $\color{blue} x$ is a constant we would get $g'(x)\stackrel{?}{=}\color{blue}x\cdot 1=x$. Now if we pretend the red $\color{red}x$ is constant we get $g'(x)\stackrel{?}{=}1\cdot \color{red} x=x$. Summing both expressions we end up with $2x$ which is of course a correct expression for the derivative. These observations have led me to the following conjecture: Let $f(x,y)$ be a differentiable function mapping $\mathbb{R}^2$ to $\mathbb{R}.$ Let $f'_1 (x,y)=\frac{\partial}{\partial x} f(x,y)$ and $f'_2 (x,y)=\frac{\partial}{\partial y} f(x,y)$. Then for any $t$ we have: $$\frac{d}{dt}f(t,t)=f'_1 (t,t) + f'_2 (t,t).$$ (I apologise for the somewhat awkward notation which I could not seem to get around without causing undue ambiguity.) This formulation also seems to lend itself to following generalisation: Let $f:\mathbb{R}^N \to \mathbb{R}$ be a function differentiable in each of its variables $x_1,x_2,\ldots,x_N$. For $n=1,2,\ldots,N$ define $f'_n(x_1,x_2,\ldots,x_N)=\frac{\partial}{\partial x_n}f(x_1,x_2,\ldots,x_N)$. Let $t$ be any real number and define the $N$-tuple $T=(t,t,\ldots,t)$. Then one has: $$\frac{d}{dt} f(T)=\sum_n f'_n(T).$$ Thus my question is: Is this true? How can it be proven? (Specifically in the case $N=2$ but also in the general case.)","Suppose we're tying to differentiate the function $f(x)=x^x$. Now the textbook method would be to notice that $f(x)=e^{x \log{x}}$ and use the chain rule to find $$f'(x)=(1+\log{x})\ e^{x \log{x}}=(1+\log{x})\ x^x.$$ But suppose that I didn't make this observation and instead tried to apply the following differentiation rules: $$\frac{d}{dx}x^c=cx^{c-1} \qquad (1)\\ \frac{d}{dx}c^x = \log{c}\ \cdot c^x \quad (2)$$ which are valid for any constant $c$. Obviously neither rules are applicable to the form $x^x$ because in this case neither the base nor the exponent are constant. But if I pretend that the exponent is constant and apply rule $(1)$, I would get $f'(x)\stackrel{?}{=}x\cdot x^{x-1}=x^x.$ Likewise, if I pretend that the base is constant and apply rule $(2)$ I obtain $f'(x)\stackrel{?}{=}\log{x}\cdot x^x$. It isn't hard to see that neither of the derivatives are correct. But here's where the magic happens: if we sum the two “derivatives” we end up with $$x^x+ \log{x}\cdot x^x=(1+\log{x})\ x^x$$ which is the correct expression for $f'(x)$. This same trick yields correct results in other contexts as well. In fact, in some cases it turns out to be a more efficient way of taking derivatives. For example, consider $$g(x)=x^2 = \color{blue} x\cdot \color{red} x.$$ If we pretend the blue $\color{blue} x$ is a constant we would get $g'(x)\stackrel{?}{=}\color{blue}x\cdot 1=x$. Now if we pretend the red $\color{red}x$ is constant we get $g'(x)\stackrel{?}{=}1\cdot \color{red} x=x$. Summing both expressions we end up with $2x$ which is of course a correct expression for the derivative. These observations have led me to the following conjecture: Let $f(x,y)$ be a differentiable function mapping $\mathbb{R}^2$ to $\mathbb{R}.$ Let $f'_1 (x,y)=\frac{\partial}{\partial x} f(x,y)$ and $f'_2 (x,y)=\frac{\partial}{\partial y} f(x,y)$. Then for any $t$ we have: $$\frac{d}{dt}f(t,t)=f'_1 (t,t) + f'_2 (t,t).$$ (I apologise for the somewhat awkward notation which I could not seem to get around without causing undue ambiguity.) This formulation also seems to lend itself to following generalisation: Let $f:\mathbb{R}^N \to \mathbb{R}$ be a function differentiable in each of its variables $x_1,x_2,\ldots,x_N$. For $n=1,2,\ldots,N$ define $f'_n(x_1,x_2,\ldots,x_N)=\frac{\partial}{\partial x_n}f(x_1,x_2,\ldots,x_N)$. Let $t$ be any real number and define the $N$-tuple $T=(t,t,\ldots,t)$. Then one has: $$\frac{d}{dt} f(T)=\sum_n f'_n(T).$$ Thus my question is: Is this true? How can it be proven? (Specifically in the case $N=2$ but also in the general case.)",,"['calculus', 'vector-analysis']"
20,When do equations represent the same curve?,When do equations represent the same curve?,,"Suppose we have two sets of parametric equations $\mathbf c_1(u) = (x_1(u), y_1(u))$ and $\mathbf c_2(v) = (x_2(v), y_2(v))$ representing two 2D planar curves. When I say ""2D planar curves"" I mean that $\mathbf c_1(u)$ and $\mathbf c_2(u)$ are mappings from compact intervals in $\mathbb R$ to $\mathbb R^2$. We can assume (without loss of generality, I think) that $\mathbf c_1:I \to \mathbb R^2$ and $\mathbf c_2:I \to \mathbb R^2$, where $I=[0,1]$. You can assume some continuity or differentiability of  $\mathbf c_1(u)$ and $\mathbf c_2(u)$, if that helps. I'm interested to know how we can determine that these two sets of equations represent the same curve. In other words, how can I determine that $\mathbf c_1(I)$ and $\mathbf c_2(I)$ are the same point set. An interesting special case: what if the parametric equations are all rational functions? In this case, it's often possible to implicitize -- i.e. convert to equations of the form $f_1(x,y)=0$ and $f_2(x,y)=0$. Then, if the two curves are the same point set, I would guess that something can be said about $f_1$ and $f_2$? Maybe one is a multiple of the other, or something like that?? Even simpler (but still interesting): what if all the functions involved are polynomials. The implicitization doesn't necessarily solve the original problem, though. It's clear that $\mathbf c_1(I)$ is a subset of the zero set $Z_1 = \{(x,y) \in \mathbb R^2 : f_1(x,y) = 0\}$, but it might be a proper subset. So, even if we know how to relate $Z_1$ and $Z_2$, this might not tell us much about how $\mathbf c_1(I)$ is related to $\mathbf c_2(I)$. Can we say anything about when the implicitization approach will work and when it won't? My question was inspired by this one . There might be some connection with this question , but both the question and the answer are written in jargon that's not familiar to me. This has practical applications -- curves in engineering and manufacturing are often described by using rational or polynomial parameterizations, and it would be nice if we had some way to identify when two curves are the same. In engineering & manufacturing, we only care about the shapes of curves (i.e. sets like $\mathbf c_1(I)$ and $\mathbf c_2(I)$), not their parameterization. For example, a circular wheel is still circular, regardless of how the circle curve is parameterized. The parameterization is artificial, in some sense, and I want to be able to ignore its effects when comparing two curves. In case it matters to anyone, this isn't homework  :-). Example (for the rational case) $$\mathbf c_1(t) = \left( \frac{1 - (2 - \sqrt2)t - (\sqrt2 - 1)t^2}                                {1 - (2 - \sqrt2)t + (2 - \sqrt2)t^2},                           \frac{\sqrt2 t - (\sqrt2 - 1)t^2}                                {1 - (2 - \sqrt2)t + (2 - \sqrt2)t^2} \right)$$ $$\mathbf c_2(t) = \left( \frac{1 -t^2}{1 + t^2},                           \frac{2t}    {1 + t^2} \right)$$ Here $\mathbf c_1(I) = \mathbf c_2(I)$. They are both the first quadrant of the unit circle, actually. Progress (December 2017) Apparently, if two two implicit equations $f_1(x,y)=0$ and $f_2(x,y)=0$ represent the same curve, and $f_1$ and $f_2$ are both irreducible polynomials, then one must be a constant multiple of the other. This result is mentioned (without proof) in this paper by Sendra , so I suppose it must be well-known.","Suppose we have two sets of parametric equations $\mathbf c_1(u) = (x_1(u), y_1(u))$ and $\mathbf c_2(v) = (x_2(v), y_2(v))$ representing two 2D planar curves. When I say ""2D planar curves"" I mean that $\mathbf c_1(u)$ and $\mathbf c_2(u)$ are mappings from compact intervals in $\mathbb R$ to $\mathbb R^2$. We can assume (without loss of generality, I think) that $\mathbf c_1:I \to \mathbb R^2$ and $\mathbf c_2:I \to \mathbb R^2$, where $I=[0,1]$. You can assume some continuity or differentiability of  $\mathbf c_1(u)$ and $\mathbf c_2(u)$, if that helps. I'm interested to know how we can determine that these two sets of equations represent the same curve. In other words, how can I determine that $\mathbf c_1(I)$ and $\mathbf c_2(I)$ are the same point set. An interesting special case: what if the parametric equations are all rational functions? In this case, it's often possible to implicitize -- i.e. convert to equations of the form $f_1(x,y)=0$ and $f_2(x,y)=0$. Then, if the two curves are the same point set, I would guess that something can be said about $f_1$ and $f_2$? Maybe one is a multiple of the other, or something like that?? Even simpler (but still interesting): what if all the functions involved are polynomials. The implicitization doesn't necessarily solve the original problem, though. It's clear that $\mathbf c_1(I)$ is a subset of the zero set $Z_1 = \{(x,y) \in \mathbb R^2 : f_1(x,y) = 0\}$, but it might be a proper subset. So, even if we know how to relate $Z_1$ and $Z_2$, this might not tell us much about how $\mathbf c_1(I)$ is related to $\mathbf c_2(I)$. Can we say anything about when the implicitization approach will work and when it won't? My question was inspired by this one . There might be some connection with this question , but both the question and the answer are written in jargon that's not familiar to me. This has practical applications -- curves in engineering and manufacturing are often described by using rational or polynomial parameterizations, and it would be nice if we had some way to identify when two curves are the same. In engineering & manufacturing, we only care about the shapes of curves (i.e. sets like $\mathbf c_1(I)$ and $\mathbf c_2(I)$), not their parameterization. For example, a circular wheel is still circular, regardless of how the circle curve is parameterized. The parameterization is artificial, in some sense, and I want to be able to ignore its effects when comparing two curves. In case it matters to anyone, this isn't homework  :-). Example (for the rational case) $$\mathbf c_1(t) = \left( \frac{1 - (2 - \sqrt2)t - (\sqrt2 - 1)t^2}                                {1 - (2 - \sqrt2)t + (2 - \sqrt2)t^2},                           \frac{\sqrt2 t - (\sqrt2 - 1)t^2}                                {1 - (2 - \sqrt2)t + (2 - \sqrt2)t^2} \right)$$ $$\mathbf c_2(t) = \left( \frac{1 -t^2}{1 + t^2},                           \frac{2t}    {1 + t^2} \right)$$ Here $\mathbf c_1(I) = \mathbf c_2(I)$. They are both the first quadrant of the unit circle, actually. Progress (December 2017) Apparently, if two two implicit equations $f_1(x,y)=0$ and $f_2(x,y)=0$ represent the same curve, and $f_1$ and $f_2$ are both irreducible polynomials, then one must be a constant multiple of the other. This result is mentioned (without proof) in this paper by Sendra , so I suppose it must be well-known.",,"['calculus', 'abstract-algebra', 'algebraic-geometry', 'analytic-geometry', 'algebraic-curves']"
21,How to show the sequence $\left\{6(-\frac{5}{6})^n\right\} _{n=1}^\infty$ converges to $0$?,How to show the sequence  converges to ?,\left\{6(-\frac{5}{6})^n\right\} _{n=1}^\infty 0,"Some quick notes about what I have studied so far about sequences (Note: this is an edit after I accepted the answer and realized my lack of understanding for this simple sequence): Here I am working with sequences of Real numbers, therefore, for that sequence, I defined a function $a$ , such that $$a: \mathbb{N}\to \mathbb{R}$$ $$a:n  \mapsto a(n) = a_n$$ The sequence is denoted as $\{a_n\}_{n\in\mathbb{N}}$ , such that $\mathbb{N} = \{1, 2, 3, ...\}$ . Given sequence $\left\{6(-\frac{5}{6})^n\right\} _{n=1}^\infty$ Once a sequence $\{a_n\}_{n\in\mathbb{N}}$ is convergent for $L\in\mathbb{R}$ when $n\to\infty$ , therefore $$\lim _{n\to\infty} a_n = L$$ this is for only if for a given $\epsilon > 0$ , so we can find a $N_0 \in \mathbb{N}$ such that $\forall n > N_0$ we have $|a_n - L| < \epsilon $ . Basically, a sequence is convergent if it tends to a Real number. On the other hand, it is divergent if it does not converge to a Real number. This is the reason why I tried to find the limit but I ended up not being able to solve it. I know that for the sequence $a_n$ , such that $a_n = 6\left(-\dfrac{5}{6}\right)^n$ we have. $$a_n = 6\left(-\dfrac{5}{6}\right)^n = 6 \cdot\dfrac{(-5)^n}{6^n} = 6^{1-n}\cdot(-5)^n$$ According to Wolfram Alpha, $\lim _{n\to \infty }(6^{1-n}(-5)^n) = 0$ and thus the sequence converges to $0$ . But I couldn't solve the limit. Wolfram solves it using exponential but it wasn't a straightforward solution for me. How can I solve $\lim _{n\to \infty }(6^{1-n}(-5)^n)$ in a simpler and good way? This might be a  consequence of the first question, but how can I show that the sequence is convergent? What would be another approach?","Some quick notes about what I have studied so far about sequences (Note: this is an edit after I accepted the answer and realized my lack of understanding for this simple sequence): Here I am working with sequences of Real numbers, therefore, for that sequence, I defined a function , such that The sequence is denoted as , such that . Given sequence Once a sequence is convergent for when , therefore this is for only if for a given , so we can find a such that we have . Basically, a sequence is convergent if it tends to a Real number. On the other hand, it is divergent if it does not converge to a Real number. This is the reason why I tried to find the limit but I ended up not being able to solve it. I know that for the sequence , such that we have. According to Wolfram Alpha, and thus the sequence converges to . But I couldn't solve the limit. Wolfram solves it using exponential but it wasn't a straightforward solution for me. How can I solve in a simpler and good way? This might be a  consequence of the first question, but how can I show that the sequence is convergent? What would be another approach?","a a: \mathbb{N}\to \mathbb{R} a:n  \mapsto a(n) = a_n \{a_n\}_{n\in\mathbb{N}} \mathbb{N} = \{1, 2, 3, ...\} \left\{6(-\frac{5}{6})^n\right\} _{n=1}^\infty \{a_n\}_{n\in\mathbb{N}} L\in\mathbb{R} n\to\infty \lim _{n\to\infty} a_n = L \epsilon > 0 N_0 \in \mathbb{N} \forall n > N_0 |a_n - L| < \epsilon  a_n a_n = 6\left(-\dfrac{5}{6}\right)^n a_n = 6\left(-\dfrac{5}{6}\right)^n = 6 \cdot\dfrac{(-5)^n}{6^n} = 6^{1-n}\cdot(-5)^n \lim _{n\to \infty }(6^{1-n}(-5)^n) = 0 0 \lim _{n\to \infty }(6^{1-n}(-5)^n)","['calculus', 'sequences-and-series']"
22,Find the limiting value of $S=a^{\sqrt{1}}+a^{\sqrt{2}}+a^{\sqrt{3}}+a^{\sqrt{4}}+...$ for $0 \leq a < 1$,Find the limiting value of  for,S=a^{\sqrt{1}}+a^{\sqrt{2}}+a^{\sqrt{3}}+a^{\sqrt{4}}+... 0 \leq a < 1,"Question How can I find the sum of the series $$S=a^{\sqrt{1}}+a^{\sqrt{2} }+a^{\sqrt{3}}+a^{\sqrt{4}}+...$$ under the condition $0 \leq a < 1$ Short version I think the sum of the series should be about $$S=\frac{2} {(\ln a)^2}+c$$ where c is a correction term. So the question is how should I find the value of $c$? Long version My approach so far: The sum can be restated in terms of $y=f(x)$ where $$y_1=\sum_{n=1}^{\left \lfloor{x}\right \rfloor}a^{\sqrt{n}}$$ so the problem now is to find the limiting value of $y_1$ as $x \rightarrow \infty$. Instead of finding the limit of $y_1(x)$ as defined now, I tried to find another function with a similar growth rate: $$\frac{\mathrm{d} y_2}{\mathrm{d} x}=a^{\sqrt{x}}$$ $${\textrm{i.e.  }} y_2=\int a^{\sqrt{x}}{\mathrm{d} x} $$ which on solving yields $$y_2=\frac{2 \sqrt{x} a^{\sqrt{x}}} {\ln a} - \frac{2a^{\sqrt{x}}} {(\ln a)^2}+\frac{2} {(\ln a)^2}$$ where the constant of integration is so chosen to make the curve pass through the origin. Now plotting both these functions on Desmos shows that $y_1$ and $y_2$ resemble each other closely. However $y_2$ has faster initial growth, and therefore it's limiting value is slightly larger than the limiting value of $y_1$ which is what I'm after. The limiting value of $y_2$ is $$S_2=\lim_{x \rightarrow \infty}{y_2(x)} =\frac{2} {(\ln a)^2}$$ Therefore, I think the actual sum of the original series should be: $$S=\frac{2} {(\ln a)^2}+c$$ where $c$ is a correction term, possibly depending on $a$. I'm stuck on how to find the correction term $c$ and what it's form should be. Is my approach so far correct? Any help would be appreciated.","Question How can I find the sum of the series $$S=a^{\sqrt{1}}+a^{\sqrt{2} }+a^{\sqrt{3}}+a^{\sqrt{4}}+...$$ under the condition $0 \leq a < 1$ Short version I think the sum of the series should be about $$S=\frac{2} {(\ln a)^2}+c$$ where c is a correction term. So the question is how should I find the value of $c$? Long version My approach so far: The sum can be restated in terms of $y=f(x)$ where $$y_1=\sum_{n=1}^{\left \lfloor{x}\right \rfloor}a^{\sqrt{n}}$$ so the problem now is to find the limiting value of $y_1$ as $x \rightarrow \infty$. Instead of finding the limit of $y_1(x)$ as defined now, I tried to find another function with a similar growth rate: $$\frac{\mathrm{d} y_2}{\mathrm{d} x}=a^{\sqrt{x}}$$ $${\textrm{i.e.  }} y_2=\int a^{\sqrt{x}}{\mathrm{d} x} $$ which on solving yields $$y_2=\frac{2 \sqrt{x} a^{\sqrt{x}}} {\ln a} - \frac{2a^{\sqrt{x}}} {(\ln a)^2}+\frac{2} {(\ln a)^2}$$ where the constant of integration is so chosen to make the curve pass through the origin. Now plotting both these functions on Desmos shows that $y_1$ and $y_2$ resemble each other closely. However $y_2$ has faster initial growth, and therefore it's limiting value is slightly larger than the limiting value of $y_1$ which is what I'm after. The limiting value of $y_2$ is $$S_2=\lim_{x \rightarrow \infty}{y_2(x)} =\frac{2} {(\ln a)^2}$$ Therefore, I think the actual sum of the original series should be: $$S=\frac{2} {(\ln a)^2}+c$$ where $c$ is a correction term, possibly depending on $a$. I'm stuck on how to find the correction term $c$ and what it's form should be. Is my approach so far correct? Any help would be appreciated.",,"['calculus', 'integration', 'sequences-and-series', 'limits', 'summation']"
23,Summation using residues,Summation using residues,,"In reference to this question about showing that the following interesting series takes on the value $$\sum_{n=0}^\infty \frac{1}{(2n+1)\operatorname{sinh}((2n+1)\pi)}=\frac{\log(2)}{8}$$ I tried the approach of using the residue theorem, from which one can show that $$\sum_{n=-\infty}^\infty f(n) = -\sum_k \mathrm{Res}_{z=z_k} [\pi \cot{\pi z} \: f(z)] $$ where $z_k$ are non-integral poles of $\pi \cot{\pi z} f(z)$.  In this case, $$f(z) = \frac{\mathrm{csch}{[(2 z+1) \pi]}}{2 z+1}$$ There is a pole of order $1$ at $z=-1/2$.  Note that is is not of order $2$ because $\cot{z} \sim (\pi/2) - z$ as $z \rightarrow \pi/2$.  One may then compute the single residue of $\pi \cot{\pi z} f(z)$ as $$\mathrm{Res}_{z=z_k} \left [\pi \cot{\pi z} \: \frac{\mathrm{csch}{[(2 z+1) \pi]}}{2 z+1} \right ] = -\frac{\pi}{4} $$ So I get as a value for the posted series as $\pi/8$, which is not correct.  Nevertheless, I cannot locate my error.  Perhaps someone can.  For example, the series obviously converges, and quickly.  But is there an error in my assumption that $$\lim_{N \rightarrow \infty} \oint_{C_N} dz \: \pi \cot{\pi z} \: \frac{\mathrm{csch}{[(2 z+1) \pi]}}{2 z+1} = 0$$ where $C_N$ is a rectaingle symmetric about the real and imaginary axes, extending from $[-N-1/2,N+1/2]$?  Or is there something else I do not see?","In reference to this question about showing that the following interesting series takes on the value $$\sum_{n=0}^\infty \frac{1}{(2n+1)\operatorname{sinh}((2n+1)\pi)}=\frac{\log(2)}{8}$$ I tried the approach of using the residue theorem, from which one can show that $$\sum_{n=-\infty}^\infty f(n) = -\sum_k \mathrm{Res}_{z=z_k} [\pi \cot{\pi z} \: f(z)] $$ where $z_k$ are non-integral poles of $\pi \cot{\pi z} f(z)$.  In this case, $$f(z) = \frac{\mathrm{csch}{[(2 z+1) \pi]}}{2 z+1}$$ There is a pole of order $1$ at $z=-1/2$.  Note that is is not of order $2$ because $\cot{z} \sim (\pi/2) - z$ as $z \rightarrow \pi/2$.  One may then compute the single residue of $\pi \cot{\pi z} f(z)$ as $$\mathrm{Res}_{z=z_k} \left [\pi \cot{\pi z} \: \frac{\mathrm{csch}{[(2 z+1) \pi]}}{2 z+1} \right ] = -\frac{\pi}{4} $$ So I get as a value for the posted series as $\pi/8$, which is not correct.  Nevertheless, I cannot locate my error.  Perhaps someone can.  For example, the series obviously converges, and quickly.  But is there an error in my assumption that $$\lim_{N \rightarrow \infty} \oint_{C_N} dz \: \pi \cot{\pi z} \: \frac{\mathrm{csch}{[(2 z+1) \pi]}}{2 z+1} = 0$$ where $C_N$ is a rectaingle symmetric about the real and imaginary axes, extending from $[-N-1/2,N+1/2]$?  Or is there something else I do not see?",,"['calculus', 'complex-analysis', 'complex-integration']"
24,Ramanujan style nested differential Equation,Ramanujan style nested differential Equation,,"So I was exploring some math the other day... and I came across the following neat identity: Given $y$ is a function of $x$ ($y(x)$) and $$ y = 1 + \frac{\mathrm{d}}{\mathrm{d}x} \left(1 + \frac{\mathrm{d}}{\mathrm{d}x} \left(1 + \frac{\mathrm{d}}{\mathrm{d}x} \left(1 + \frac{\mathrm{d}}{\mathrm{d}x} \left(1 + \frac{\mathrm{d}}{\mathrm{d}x} \left( \cdots \right) \right) \right) \right) \right) \text{ (repeated differential)} $$ then we can solve this equation as follows: $$ y - 1 = \frac{\mathrm{d}}{\mathrm{d}x} \left(1 + \frac{\mathrm{d}}{\mathrm{d}x} \left( \cdots \right) \right) \iff \int y - 1 \, \mathrm{d} x = 1 + \frac{\mathrm{d}}{\mathrm{d}x} \left( 1 + \frac{\mathrm{d}}{\mathrm{d}x} \left( \cdots \right) \right) $$ $$ \implies \int y - 1 \, \mathrm{d} x = y \iff y - 1 = \frac{\mathrm{d} y }{ \mathrm{d} x} $$ So $$ \ln \left( y - 1 \right) = x + C \iff y = Ce^x + 1 $$ This problem reminded me a lot of nested radical expressions such as: $$ x = 1 + \sqrt{1 + \sqrt{ 1 + \sqrt{ \cdots }}} \iff x - 1 =  \sqrt{1 + \sqrt{ 1 + \sqrt{ \cdots }}} $$ $$ \implies (x - 1)^2 = x \iff x^2 - 3x + 1 = 0 $$ and so $$ x = \frac{3}{2} + \frac{\sqrt{5}}{2} $$ This reminded of the Ramanujan nested radical which is: $$ x = 0 + \sqrt{ 1 + 2 \sqrt{ 1 + 3 \sqrt{1 + 4 \sqrt{ \cdots }}}} $$ whose solution cannot be done by simple series manipulations but requires knowledge of general formula found by algebraically manipulating the binomial theorem... This made me curious... say $y$ is a function of $x$ ($y(x)$) and $$ y = 0 + \frac{\mathrm{d}}{\mathrm{d}x} \left(1 + 2\frac{\mathrm{d}}{\mathrm{d}x} \left(1 + 3\frac{\mathrm{d}}{\mathrm{d}x} \left(1 + 4\frac{\mathrm{d}}{\mathrm{d}x} \left(1 + 5\frac{\mathrm{d}}{\mathrm{d}x} \left( \cdots \right) \right) \right) \right) \right) $$ What would the solution come out to be?","So I was exploring some math the other day... and I came across the following neat identity: Given $y$ is a function of $x$ ($y(x)$) and $$ y = 1 + \frac{\mathrm{d}}{\mathrm{d}x} \left(1 + \frac{\mathrm{d}}{\mathrm{d}x} \left(1 + \frac{\mathrm{d}}{\mathrm{d}x} \left(1 + \frac{\mathrm{d}}{\mathrm{d}x} \left(1 + \frac{\mathrm{d}}{\mathrm{d}x} \left( \cdots \right) \right) \right) \right) \right) \text{ (repeated differential)} $$ then we can solve this equation as follows: $$ y - 1 = \frac{\mathrm{d}}{\mathrm{d}x} \left(1 + \frac{\mathrm{d}}{\mathrm{d}x} \left( \cdots \right) \right) \iff \int y - 1 \, \mathrm{d} x = 1 + \frac{\mathrm{d}}{\mathrm{d}x} \left( 1 + \frac{\mathrm{d}}{\mathrm{d}x} \left( \cdots \right) \right) $$ $$ \implies \int y - 1 \, \mathrm{d} x = y \iff y - 1 = \frac{\mathrm{d} y }{ \mathrm{d} x} $$ So $$ \ln \left( y - 1 \right) = x + C \iff y = Ce^x + 1 $$ This problem reminded me a lot of nested radical expressions such as: $$ x = 1 + \sqrt{1 + \sqrt{ 1 + \sqrt{ \cdots }}} \iff x - 1 =  \sqrt{1 + \sqrt{ 1 + \sqrt{ \cdots }}} $$ $$ \implies (x - 1)^2 = x \iff x^2 - 3x + 1 = 0 $$ and so $$ x = \frac{3}{2} + \frac{\sqrt{5}}{2} $$ This reminded of the Ramanujan nested radical which is: $$ x = 0 + \sqrt{ 1 + 2 \sqrt{ 1 + 3 \sqrt{1 + 4 \sqrt{ \cdots }}}} $$ whose solution cannot be done by simple series manipulations but requires knowledge of general formula found by algebraically manipulating the binomial theorem... This made me curious... say $y$ is a function of $x$ ($y(x)$) and $$ y = 0 + \frac{\mathrm{d}}{\mathrm{d}x} \left(1 + 2\frac{\mathrm{d}}{\mathrm{d}x} \left(1 + 3\frac{\mathrm{d}}{\mathrm{d}x} \left(1 + 4\frac{\mathrm{d}}{\mathrm{d}x} \left(1 + 5\frac{\mathrm{d}}{\mathrm{d}x} \left( \cdots \right) \right) \right) \right) \right) $$ What would the solution come out to be?",,"['calculus', 'sequences-and-series', 'integration', 'ordinary-differential-equations', 'derivatives']"
25,Functions that are always less than their derivatives,Functions that are always less than their derivatives,,"I was wondering if there are functions for which $$f'(x) > f(x)$$ for all $x$. Only examples I could think of were $e^x - c$ and simply $- c$ in which $c > 0$. Also, is there any significance in a function that is always less than its derivative? Edit: Thank you very much for all the replies. It seems almost all functions that apply are exponential by nature...  Are there more examples like - 1/x? Again are there any applications/physical manifestations of these functions? [for example an object with a velocity that is always greater than its position/acceleration  is always greater than its velocity]","I was wondering if there are functions for which $$f'(x) > f(x)$$ for all $x$. Only examples I could think of were $e^x - c$ and simply $- c$ in which $c > 0$. Also, is there any significance in a function that is always less than its derivative? Edit: Thank you very much for all the replies. It seems almost all functions that apply are exponential by nature...  Are there more examples like - 1/x? Again are there any applications/physical manifestations of these functions? [for example an object with a velocity that is always greater than its position/acceleration  is always greater than its velocity]",,"['calculus', 'integration', 'ordinary-differential-equations', 'derivatives', 'inequality']"
26,Paradox about the volume of a cylinder,Paradox about the volume of a cylinder,,"Trying to apply Cavalieri's method of indivisibles to calculate the volume of a cylinder with radius $R$ and height $h$ , I get the following paradoxical argument. A cylinder with radius $R$ and height $h$ can be seen as a solid obtained by rotating a rectangle with height $h$ and base $R$ about its height. Therefore, the volume of the cylinder can be thought as made out of an infinity of areas of such rectangles of infinitesimal thickness rotated for $360^\circ$ ; hence, the volume $V$ of the cylinder should the area of the rectangle $A_\text{rect} = R \cdot h$ multiplied by the circumference of the rotation circle $C_\text{circ} = 2\pi R$ : \begin{align} V = A_\text{rect} \cdot C_\text{circ} = 2 \pi R^2 \cdot h \end{align} Of course, the right volume of a cylinder with radius $R$ and height $h$ is \begin{align} V = A_\text{circ} \cdot h = \pi R^2 \cdot h \end{align} where $A_\text{circ} = \pi R^2$ is the area of the base circle of the cylinder. Question: Where is the error in my previous argument based on infinitesimals?","Trying to apply Cavalieri's method of indivisibles to calculate the volume of a cylinder with radius and height , I get the following paradoxical argument. A cylinder with radius and height can be seen as a solid obtained by rotating a rectangle with height and base about its height. Therefore, the volume of the cylinder can be thought as made out of an infinity of areas of such rectangles of infinitesimal thickness rotated for ; hence, the volume of the cylinder should the area of the rectangle multiplied by the circumference of the rotation circle : Of course, the right volume of a cylinder with radius and height is where is the area of the base circle of the cylinder. Question: Where is the error in my previous argument based on infinitesimals?","R h R h h R 360^\circ V A_\text{rect} = R \cdot h C_\text{circ} = 2\pi R \begin{align}
V = A_\text{rect} \cdot C_\text{circ} = 2 \pi R^2 \cdot h
\end{align} R h \begin{align}
V = A_\text{circ} \cdot h = \pi R^2 \cdot h
\end{align} A_\text{circ} = \pi R^2","['calculus', 'integration', 'proof-verification', 'volume', 'infinitesimals']"
27,Can $x^3+3x^2+1=0$ be solved using high school methods?,Can  be solved using high school methods?,x^3+3x^2+1=0,"I encountered the following problem in a high-school math text, which I wasn't able to solve using factorization/factor theorem: Solve $x^3+3x^2+1=0$ Am I missing something here, or is indeed a more advanced method necessary to solve this particular cubic? The answer provided was $x\doteq-3.1$, which I was only able to confirm using CAS.","I encountered the following problem in a high-school math text, which I wasn't able to solve using factorization/factor theorem: Solve $x^3+3x^2+1=0$ Am I missing something here, or is indeed a more advanced method necessary to solve this particular cubic? The answer provided was $x\doteq-3.1$, which I was only able to confirm using CAS.",,"['calculus', 'algebra-precalculus', 'roots', 'cubics']"
28,How to calculate limit of a function having factorial in denominator,How to calculate limit of a function having factorial in denominator,,For $n$ tending to infinity find the following limit $$\frac{2^n}{n!}.$$ I have a feeling that it is multiplication of many numbers with the last one turning to $0$ but the first one is finite so limit should be $0$. But I am not sure and neither am I able to put it in mathematical form. Thank you for your help.,For $n$ tending to infinity find the following limit $$\frac{2^n}{n!}.$$ I have a feeling that it is multiplication of many numbers with the last one turning to $0$ but the first one is finite so limit should be $0$. But I am not sure and neither am I able to put it in mathematical form. Thank you for your help.,,"['calculus', 'limits']"
29,Indefinite integral of secant cubed $\int \sec^3 x\>dx$,Indefinite integral of secant cubed,\int \sec^3 x\>dx,"I need to calculate the following indefinite integral: $$I=\int \frac{1}{\cos^3(x)}dx$$ I know what the result is (from Mathematica): $$I=\tanh^{-1}(\tan(x/2))+(1/2)\sec(x)\tan(x)$$ but I don't know how to integrate it myself. I have been trying some substitutions to no avail. Equivalently, I need to know how to compute: $$I=\int \sqrt{1+z^2}dz$$ which follows after making the change of variables $z=\tan x$.","I need to calculate the following indefinite integral: $$I=\int \frac{1}{\cos^3(x)}dx$$ I know what the result is (from Mathematica): $$I=\tanh^{-1}(\tan(x/2))+(1/2)\sec(x)\tan(x)$$ but I don't know how to integrate it myself. I have been trying some substitutions to no avail. Equivalently, I need to know how to compute: $$I=\int \sqrt{1+z^2}dz$$ which follows after making the change of variables $z=\tan x$.",,"['calculus', 'integration', 'indefinite-integrals']"
30,Show that this sum is an integer.,Show that this sum is an integer.,,"I have to show that $$g\left(\frac{1}{2015}\right) + g\left(\frac{2}{2015}\right) +\cdots + g\left(\frac{2014}{2015}\right) $$ is an integer. Here $g(t)=\dfrac{3^t}{3^t+3^{1/2}}$. I tried to solve it using power series, but I can not finish with any convincing argument to said that the sum is an integer. (Also the use of a computer isn't allowed.)","I have to show that $$g\left(\frac{1}{2015}\right) + g\left(\frac{2}{2015}\right) +\cdots + g\left(\frac{2014}{2015}\right) $$ is an integer. Here $g(t)=\dfrac{3^t}{3^t+3^{1/2}}$. I tried to solve it using power series, but I can not finish with any convincing argument to said that the sum is an integer. (Also the use of a computer isn't allowed.)",,"['calculus', 'elementary-number-theory', 'summation']"
31,"Improper Integral $\int\limits_0^1\frac{\ln(x)}{x^2-1}\,dx$",Improper Integral,"\int\limits_0^1\frac{\ln(x)}{x^2-1}\,dx","How can I prove that? $$\int_0^1\frac{\ln(x)}{x^2-1}\,dx=\frac{\pi^2}{8}$$ I know that  $$\int_0^1\frac{\ln(x)}{x^2-1}\,dx=\sum_{n=0}^{\infty}\int_0^1-x^{2n}\ln(x)\,dx=\sum_{n=0}^{\infty}\frac{1}{(2n+1)^2}=\frac{\pi^2}{8}$$ but I want another method.","How can I prove that? $$\int_0^1\frac{\ln(x)}{x^2-1}\,dx=\frac{\pi^2}{8}$$ I know that  $$\int_0^1\frac{\ln(x)}{x^2-1}\,dx=\sum_{n=0}^{\infty}\int_0^1-x^{2n}\ln(x)\,dx=\sum_{n=0}^{\infty}\frac{1}{(2n+1)^2}=\frac{\pi^2}{8}$$ but I want another method.",,"['calculus', 'integration']"
32,Prove $\int^\infty_0\frac x{e^x-1}dx=\frac{\pi^2}{6}$,Prove,\int^\infty_0\frac x{e^x-1}dx=\frac{\pi^2}{6},"I know that $$\int^\infty_0\frac x{e^x-1}dx=\frac{\pi^2}{6}$$ For substituting $u=2$ into $$\zeta(u)\Gamma(u)=\int^\infty_0\frac{x^{u-1}}{e^x-1}dx$$ However, I suspect that there is an easier proof, maybe by the use of complex analysis. I haven't learnt the zeta function yet. All I know is the above formula and $\zeta(2)=\frac{\pi^2}{6}$. Can we can use the above integral to find out some of the $\zeta$'s value?","I know that $$\int^\infty_0\frac x{e^x-1}dx=\frac{\pi^2}{6}$$ For substituting $u=2$ into $$\zeta(u)\Gamma(u)=\int^\infty_0\frac{x^{u-1}}{e^x-1}dx$$ However, I suspect that there is an easier proof, maybe by the use of complex analysis. I haven't learnt the zeta function yet. All I know is the above formula and $\zeta(2)=\frac{\pi^2}{6}$. Can we can use the above integral to find out some of the $\zeta$'s value?",,"['calculus', 'complex-analysis', 'integration']"
33,Zero function implies zero polynomial.,Zero function implies zero polynomial.,,"I'm trying to help someone with a problem in Apostol's book (Chapter 1 BTW, so before basically any calculus concepts are covered) at the moment and I'm stumped on a question. I'm trying to prove that if $p$ is a polynomial of degree $n$, that is where $$p(x) = a_0 + a_1x + \cdots + a_nx^n$$ for some real numbers $a_0, \dots, a_n$, and if $p(x) = 0$ for all $x\in \Bbb R$, then $a_k = 0$ for all $k$. Looking through the site, I find this question , but the solution given uses the derivative.  But this before the definition of the derivative in Apostol's book, so I can't use that to prove this.  I also know that we can use linear algebra to solve this, but pretend I don't understand the concept of linear independence either as Apostol's book doesn't presuppose that.  Then what can we do to prove this?  It feels like there should be a proof by induction possible, but I'm not seeing how to do the induction step. My Attempt: Proving that $a_0 = 0$ is trivial by evaluating $p(0)$.  But then I'm left with $$p(x) = x(a_1 + \cdots +a_nx^{n-1})$$ Here I see that for all $x\ne 0$, $a_1 + \cdots a_nx^{n-1}=0$.  But because of that $x\ne 0$ part, that means I can't use the same trick to show that $a_1 = 0$. Any ideas?","I'm trying to help someone with a problem in Apostol's book (Chapter 1 BTW, so before basically any calculus concepts are covered) at the moment and I'm stumped on a question. I'm trying to prove that if $p$ is a polynomial of degree $n$, that is where $$p(x) = a_0 + a_1x + \cdots + a_nx^n$$ for some real numbers $a_0, \dots, a_n$, and if $p(x) = 0$ for all $x\in \Bbb R$, then $a_k = 0$ for all $k$. Looking through the site, I find this question , but the solution given uses the derivative.  But this before the definition of the derivative in Apostol's book, so I can't use that to prove this.  I also know that we can use linear algebra to solve this, but pretend I don't understand the concept of linear independence either as Apostol's book doesn't presuppose that.  Then what can we do to prove this?  It feels like there should be a proof by induction possible, but I'm not seeing how to do the induction step. My Attempt: Proving that $a_0 = 0$ is trivial by evaluating $p(0)$.  But then I'm left with $$p(x) = x(a_1 + \cdots +a_nx^{n-1})$$ Here I see that for all $x\ne 0$, $a_1 + \cdots a_nx^{n-1}=0$.  But because of that $x\ne 0$ part, that means I can't use the same trick to show that $a_1 = 0$. Any ideas?",,"['calculus', 'algebra-precalculus', 'polynomials']"
34,Proving that an additive function $f$ is continuous if it is continuous at a single point,Proving that an additive function  is continuous if it is continuous at a single point,f,Suppose that $f$ is continuous at $x_0$ and $f$ satisfies $f(x)+f(y)=f(x+y)$. Then how can we prove that $f$ is continuous at $x$ for all $x$? I seems to have problem doing anything with it. Thanks in advance.,Suppose that $f$ is continuous at $x_0$ and $f$ satisfies $f(x)+f(y)=f(x+y)$. Then how can we prove that $f$ is continuous at $x$ for all $x$? I seems to have problem doing anything with it. Thanks in advance.,,"['calculus', 'functional-equations']"
35,Evaluating $\int\limits_0^\infty \! \frac{x^{1/n}}{1+x^2} \ \mathrm{d}x$,Evaluating,\int\limits_0^\infty \! \frac{x^{1/n}}{1+x^2} \ \mathrm{d}x,"I've been trying to evaluate the following integral from the 2011 Harvard PhD Qualifying Exam. For all $n\in\mathbb{N}^+$ in general: $$\int\limits_0^\infty \! \frac{x^{1/n}}{1+x^2} \ \mathrm{d}x$$ However, I'm not quite sure where to begin, even. There is a possibility that it is related to complex analysis, so I tried going at it with Cauchy's and also with residues, but I still haven't managed to get any further in solving it.","I've been trying to evaluate the following integral from the 2011 Harvard PhD Qualifying Exam. For all $n\in\mathbb{N}^+$ in general: $$\int\limits_0^\infty \! \frac{x^{1/n}}{1+x^2} \ \mathrm{d}x$$ However, I'm not quite sure where to begin, even. There is a possibility that it is related to complex analysis, so I tried going at it with Cauchy's and also with residues, but I still haven't managed to get any further in solving it.",,"['calculus', 'integration', 'complex-analysis', 'improper-integrals', 'contour-integration']"
36,"Why/How does the determinant of the Hessian matrix, combined with the 2nd derivatives, tell us max., min., saddle points? Reasoning behind it?","Why/How does the determinant of the Hessian matrix, combined with the 2nd derivatives, tell us max., min., saddle points? Reasoning behind it?",,"In my classes, we are taught the following. If the determinant of the Hessian matrix at the critical point $\det(D^2f(c)) > 0$ and $f_{xx}(c) > 0$, the function $f$ at c is concave up. If the determinant of the Hessian matrix at the critical point $\det(D^2f(c)) > 0$ and $f_{xx}(c) < 0$, the function $f$ at c is concave down. If the determinant of the Hessian matrix at the critical point $\det(D^2f(c)) < 0$, the function $f$ at c is a saddle point. However, the reasoning behind this is never explained. We are never taught WHY or HOW. I would like to know why the determinant of the Hessian matrix, combined with the second derivative at the critical point, contains this information about max., min., and saddle points. I would also like to know how this is derived, as I think this would likely go hand-in-hand with why. Please give clear reasoning behind each step - not just 'this is what it is' without any reasoning. When researching this topic, I recall reading mentions regarding eigenvectors or eigenvalues, but I honestly cannot remember. Thank you.","In my classes, we are taught the following. If the determinant of the Hessian matrix at the critical point $\det(D^2f(c)) > 0$ and $f_{xx}(c) > 0$, the function $f$ at c is concave up. If the determinant of the Hessian matrix at the critical point $\det(D^2f(c)) > 0$ and $f_{xx}(c) < 0$, the function $f$ at c is concave down. If the determinant of the Hessian matrix at the critical point $\det(D^2f(c)) < 0$, the function $f$ at c is a saddle point. However, the reasoning behind this is never explained. We are never taught WHY or HOW. I would like to know why the determinant of the Hessian matrix, combined with the second derivative at the critical point, contains this information about max., min., and saddle points. I would also like to know how this is derived, as I think this would likely go hand-in-hand with why. Please give clear reasoning behind each step - not just 'this is what it is' without any reasoning. When researching this topic, I recall reading mentions regarding eigenvectors or eigenvalues, but I honestly cannot remember. Thank you.",,"['calculus', 'linear-algebra', 'multivariable-calculus', 'optimization']"
37,About $\int_0^{\pi/2}\arctan(1-\sin^2 (x) \cos^2 (x))dx = \pi \left( \frac{\pi}{4}-\arctan \sqrt{\frac{\sqrt{2}-1}{2}}\right)$,About,\int_0^{\pi/2}\arctan(1-\sin^2 (x) \cos^2 (x))dx = \pi \left( \frac{\pi}{4}-\arctan \sqrt{\frac{\sqrt{2}-1}{2}}\right),In this question sos440 has mentioned about an integral that he computed: $$\int_0^{\pi/2}\arctan(1-\sin^2 (x) \cos^2 (x))dx = \pi \left( \frac{\pi}{4}- \arctan \sqrt{\frac{\sqrt{2}-1}{2}}\right)$$ I would really like to know how this can be proved. I tried using differentiation under the integral sign on parmeter $a$ : $$\int_0^{\pi/2}\arctan(1-a\sin^2 (x) \cos^2 (x))dx $$ but it didn't really work. I would be really grateful if sos440 or somebody else can tell me the secret behind discovering this formula. Thanks!,In this question sos440 has mentioned about an integral that he computed: I would really like to know how this can be proved. I tried using differentiation under the integral sign on parmeter : but it didn't really work. I would be really grateful if sos440 or somebody else can tell me the secret behind discovering this formula. Thanks!,\int_0^{\pi/2}\arctan(1-\sin^2 (x) \cos^2 (x))dx = \pi \left( \frac{\pi}{4}- \arctan \sqrt{\frac{\sqrt{2}-1}{2}}\right) a \int_0^{\pi/2}\arctan(1-a\sin^2 (x) \cos^2 (x))dx ,"['calculus', 'integration', 'definite-integrals']"
38,A closed form for: $\int_{0}^{\infty} \frac{1}{(x-\log x)^2}dx$,A closed form for:,\int_{0}^{\infty} \frac{1}{(x-\log x)^2}dx,"Is it possible to find a closed-form expression for this integral? $$\int_{0}^{\infty} \frac{1}{(x-\log x)^2}dx$$ Generalization of the Integral: $$\int_{0}^{\infty} \frac{1}{(x-\log x)^{p}}dx$$ where, $\log x$ is a natural logarithm, $p\in\mathbb{Z^{+}}_{≥2}$ The indefinite integral can not be expressed by elementary mathematical functions according to Wolfram Alpha. I can add a visual plot. So, I dont know, is it possible to find a closed-form or not. But, I have a numerical solution: $$\int_{0}^{\infty} \frac{1}{(x-\log x)^2}dx≈2.51792$$","Is it possible to find a closed-form expression for this integral? Generalization of the Integral: where, is a natural logarithm, The indefinite integral can not be expressed by elementary mathematical functions according to Wolfram Alpha. I can add a visual plot. So, I dont know, is it possible to find a closed-form or not. But, I have a numerical solution:",\int_{0}^{\infty} \frac{1}{(x-\log x)^2}dx \int_{0}^{\infty} \frac{1}{(x-\log x)^{p}}dx \log x p\in\mathbb{Z^{+}}_{≥2} \int_{0}^{\infty} \frac{1}{(x-\log x)^2}dx≈2.51792,"['calculus', 'integration', 'definite-integrals', 'problem-solving', 'closed-form']"
39,"If $f:[0,\infty)\to [0,\infty)$ and $f(x+y)=f(x)+f(y)$ then prove that $f(x)=ax$",If  and  then prove that,"f:[0,\infty)\to [0,\infty) f(x+y)=f(x)+f(y) f(x)=ax","Let $\,f:[0,\infty)\to [0,\infty)$ be a function such that $\,f(x+y)=f(x)+f(y),\,$ for all $\,x,y\ge 0$. Prove that $\,f(x)=ax,\,$ for some constant $a$. My proof : We have , $\,f(0)=0$. Then ,  $$\displaystyle f'(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}=\lim_{h\to 0}\frac{f(h)}{h}=\lim_{h\to 0}\frac{f(h)-f(0)}{h}=f'(0)=a\text{(constant)}.$$ Then, $\,f(x)=ax+b$. As, $\,f(0)=0$ so $b=0$ and $f(x)=ax.$ Is my proof correct?","Let $\,f:[0,\infty)\to [0,\infty)$ be a function such that $\,f(x+y)=f(x)+f(y),\,$ for all $\,x,y\ge 0$. Prove that $\,f(x)=ax,\,$ for some constant $a$. My proof : We have , $\,f(0)=0$. Then ,  $$\displaystyle f'(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}=\lim_{h\to 0}\frac{f(h)}{h}=\lim_{h\to 0}\frac{f(h)-f(0)}{h}=f'(0)=a\text{(constant)}.$$ Then, $\,f(x)=ax+b$. As, $\,f(0)=0$ so $b=0$ and $f(x)=ax.$ Is my proof correct?",,"['calculus', 'analysis', 'functional-equations']"
40,Why am I getting the wrong answer when I factor an $i$ out of the integrand?,Why am I getting the wrong answer when I factor an  out of the integrand?,i,"Consider the following definite integral: $$I=\int^{0}_{-1}x\sqrt{-x}dx \tag{1}$$ With the substitution $x=-u$ , I got $I=-\frac{2}{5}$ (which seems correct). But I then tried a different method by first taking out $\sqrt{-1}=i$ from the integrand: $$I=i\int^{0}_{-1}x\sqrt{x}dx=\frac{2i}{5}[x^{\frac{5}{2}}]^{0}_{-1}=\frac{2i}{5}{(0-(\sqrt{-1})^5})=-\frac{2i^6}{5}=+\frac{2}{5} \tag{2}$$ which is clearly wrong. I understand that $x\sqrt{x}$ is not even defined within $(-1,0)$ , but why can't we use the same 'imaginary approach' ( $\sqrt{-1}=i$ ) to treat this undefined part of the function (i.e. the third equality in $(2)$ ). I can't find a better way of phrasing my question so it may seem gibberish, but why is $(2)$ just invalid?","Consider the following definite integral: With the substitution , I got (which seems correct). But I then tried a different method by first taking out from the integrand: which is clearly wrong. I understand that is not even defined within , but why can't we use the same 'imaginary approach' ( ) to treat this undefined part of the function (i.e. the third equality in ). I can't find a better way of phrasing my question so it may seem gibberish, but why is just invalid?","I=\int^{0}_{-1}x\sqrt{-x}dx \tag{1} x=-u I=-\frac{2}{5} \sqrt{-1}=i I=i\int^{0}_{-1}x\sqrt{x}dx=\frac{2i}{5}[x^{\frac{5}{2}}]^{0}_{-1}=\frac{2i}{5}{(0-(\sqrt{-1})^5})=-\frac{2i^6}{5}=+\frac{2}{5} \tag{2} x\sqrt{x} (-1,0) \sqrt{-1}=i (2) (2)","['calculus', 'integration', 'definite-integrals', 'complex-numbers']"
41,Is there a formula for $\sum_{n=1}^{k} \frac1{n^3}$?,Is there a formula for ?,\sum_{n=1}^{k} \frac1{n^3},I am searching for the value of $$\sum_{n=k+1}^{\infty} \frac1{n^3} \stackrel{?}{=} \sum_{n = 1}^{\infty} \frac1{n^3} - \sum_{n=1}^{k} \frac1{n^3} = \zeta(3) - \sum_{n=1}^{k} \frac1{n^3}$$ For which I think I need to know the value of $$\sum_{n=1}^{k} \frac1{n^3}$$ Does anyone know of a formula (and a reference if it is complicated)?,I am searching for the value of $$\sum_{n=k+1}^{\infty} \frac1{n^3} \stackrel{?}{=} \sum_{n = 1}^{\infty} \frac1{n^3} - \sum_{n=1}^{k} \frac1{n^3} = \zeta(3) - \sum_{n=1}^{k} \frac1{n^3}$$ For which I think I need to know the value of $$\sum_{n=1}^{k} \frac1{n^3}$$ Does anyone know of a formula (and a reference if it is complicated)?,,"['calculus', 'sequences-and-series', 'asymptotics', 'summation']"
42,Usage of dx in Integrals,Usage of dx in Integrals,,"All the integrals I'm familiar with have the form: $\int f(x)\mathrm{d}x$. And I understand these as the sum of infinite tiny rectangles with an area of: $f(x_i)\cdot\mathrm{d}x$. Is it valid to have integrals that do not have a differential, such as $\mathrm{d}x$, or that have the differential elsewhere than as a factor ? Let me give couple of examples on what I'm thinking of: $\int 1$ If this is valid notation, I'd expect it to sum infinite ones together, thus to go inifinity. $\int e^{\mathrm{d}x}$ Again, I'd expect this to go to infinity as $e^0 = 1$, assuming the notation is valid. $\int (e^{\mathrm{d}x} - 1)$ This I could potentially imagine to have a finite value. Are any such integrals valid? If so, are there any interesting / enlightening examples of such integrals?","All the integrals I'm familiar with have the form: $\int f(x)\mathrm{d}x$. And I understand these as the sum of infinite tiny rectangles with an area of: $f(x_i)\cdot\mathrm{d}x$. Is it valid to have integrals that do not have a differential, such as $\mathrm{d}x$, or that have the differential elsewhere than as a factor ? Let me give couple of examples on what I'm thinking of: $\int 1$ If this is valid notation, I'd expect it to sum infinite ones together, thus to go inifinity. $\int e^{\mathrm{d}x}$ Again, I'd expect this to go to infinity as $e^0 = 1$, assuming the notation is valid. $\int (e^{\mathrm{d}x} - 1)$ This I could potentially imagine to have a finite value. Are any such integrals valid? If so, are there any interesting / enlightening examples of such integrals?",,"['calculus', 'notation']"
43,Challenge: Demonstrate a Contradiction in Leibniz' differential notation,Challenge: Demonstrate a Contradiction in Leibniz' differential notation,,"I want to know if the Leibniz differential notation actually leads to contradictions - I am starting to think it does not. And just to eliminate the most commonly showcased 'difficulty': For the level curve $f(x,y)=0$ in the plane we have $$\frac{dy}{dx}=-\frac{\dfrac{\partial f}{\partial x}}{\dfrac{\partial f}{\partial y}}$$ If we were to ""cancel"" the differentials we would incorrectly derive $\frac{dy}{dx}=-\frac{dy}{dx}$. Why does this not work? Simple: The ""$\partial f$"" in the numerator is a response to the change in $x$, whereas the ""$\partial f$"" in the denominator is a response to the change in $y$. They are different numbers, and so cannot be cancelled. Related: consult the answer to this previous question. The other part has been moved to a new post here .","I want to know if the Leibniz differential notation actually leads to contradictions - I am starting to think it does not. And just to eliminate the most commonly showcased 'difficulty': For the level curve $f(x,y)=0$ in the plane we have $$\frac{dy}{dx}=-\frac{\dfrac{\partial f}{\partial x}}{\dfrac{\partial f}{\partial y}}$$ If we were to ""cancel"" the differentials we would incorrectly derive $\frac{dy}{dx}=-\frac{dy}{dx}$. Why does this not work? Simple: The ""$\partial f$"" in the numerator is a response to the change in $x$, whereas the ""$\partial f$"" in the denominator is a response to the change in $y$. They are different numbers, and so cannot be cancelled. Related: consult the answer to this previous question. The other part has been moved to a new post here .",,['calculus']
44,Integrating $\int_0^\pi \frac{x\cos x}{1+\sin^2 x}dx$ [duplicate],Integrating  [duplicate],\int_0^\pi \frac{x\cos x}{1+\sin^2 x}dx,"This question already has answers here : Integrate $\int_{0}^{\pi}{\frac{x\cos{x}}{1+\sin^{2}{x}}dx}$ (3 answers) I need assistance in integrating $ \frac{x \sin x}{1+(\cos x)^2}$ (2 answers) Closed 9 years ago . I am working on $\displaystyle\int_0^\pi \frac{x\cos x}{1+\sin^2 x}\,dx$ First: I use integrating by part then get $$ x\arctan(\sin x)\Big|_0^\pi-\int_0^\pi \arctan(\sin x)\,dx $$ then I have $\displaystyle -\int_0^\pi \arctan(\sin x)\,dx$ because $x\arctan(\sin x)\Big|_0^\pi$ is equal to $0$ However, I don't know how to integrate $\displaystyle -\int_0^\pi \arctan(\sin x)\,dx$ Can someone give me a hint? Thanks","This question already has answers here : Integrate $\int_{0}^{\pi}{\frac{x\cos{x}}{1+\sin^{2}{x}}dx}$ (3 answers) I need assistance in integrating $ \frac{x \sin x}{1+(\cos x)^2}$ (2 answers) Closed 9 years ago . I am working on $\displaystyle\int_0^\pi \frac{x\cos x}{1+\sin^2 x}\,dx$ First: I use integrating by part then get $$ x\arctan(\sin x)\Big|_0^\pi-\int_0^\pi \arctan(\sin x)\,dx $$ then I have $\displaystyle -\int_0^\pi \arctan(\sin x)\,dx$ because $x\arctan(\sin x)\Big|_0^\pi$ is equal to $0$ However, I don't know how to integrate $\displaystyle -\int_0^\pi \arctan(\sin x)\,dx$ Can someone give me a hint? Thanks",,"['calculus', 'integration', 'trigonometry', 'definite-integrals', 'closed-form']"
45,On what interval does a Taylor series approximate (or equal?) its function?,On what interval does a Taylor series approximate (or equal?) its function?,,"Suppose I have a function $f$ that is infinitely differentiable on some interval $I$. When I construct a Taylor series $P$ for it, using some point $a$ in $I$, does $f(x) = P(x)$ for all $x$ in $I$? I'm confused as to whether Taylor series approximate (or equal - I'm not about that either) functions on one point, or on an interval.","Suppose I have a function $f$ that is infinitely differentiable on some interval $I$. When I construct a Taylor series $P$ for it, using some point $a$ in $I$, does $f(x) = P(x)$ for all $x$ in $I$? I'm confused as to whether Taylor series approximate (or equal - I'm not about that either) functions on one point, or on an interval.",,"['calculus', 'taylor-expansion']"
46,Trig substitution; why can we ignore the absolute value?,Trig substitution; why can we ignore the absolute value?,,"If we have to integrate $$f(x)=\frac{x}{\sqrt{1-x^2}}$$ and we substitute $x=\sin \theta$ then we eventually have to take the square root of $\cos^2x$ which is equal to $|\cos x|$.  But in my textbook and class lectures, we simply remove the absolute value sign and replace it with $\cos x$. Why do we do this? I don't understand how we can do this.","If we have to integrate $$f(x)=\frac{x}{\sqrt{1-x^2}}$$ and we substitute $x=\sin \theta$ then we eventually have to take the square root of $\cos^2x$ which is equal to $|\cos x|$.  But in my textbook and class lectures, we simply remove the absolute value sign and replace it with $\cos x$. Why do we do this? I don't understand how we can do this.",,['calculus']
47,Fourier transform of $\left|\frac{\sin x}{x}\right|$,Fourier transform of,\left|\frac{\sin x}{x}\right|,"Is there a closed form (possibly, using known special functions) for the Fourier transform of the function $f(x)=\left|\frac{\sin x}{x}\right|$? $\hspace{.7in}$ I tried to find one using Mathematica , but it ran for several hours without producing any result.","Is there a closed form (possibly, using known special functions) for the Fourier transform of the function $f(x)=\left|\frac{\sin x}{x}\right|$? $\hspace{.7in}$ I tried to find one using Mathematica , but it ran for several hours without producing any result.",,"['calculus', 'trigonometry', 'fourier-analysis', 'signal-processing', 'closed-form']"
48,Definite Integral $\int_0^{\pi/2} \frac{\log(\cos x)}{x^2+\log^2(\cos x)}dx = \frac{\pi}{2}\left(1-\frac{1}{\log 2}\right)$,Definite Integral,\int_0^{\pi/2} \frac{\log(\cos x)}{x^2+\log^2(\cos x)}dx = \frac{\pi}{2}\left(1-\frac{1}{\log 2}\right),I want to prove that $$\int_0^{\pi/2} \frac{\log(\cos x)}{x^2+\log^2(\cos x)}dx = \frac{\pi}{2}\left(1-\frac{1}{\log 2}\right)$$,I want to prove that $$\int_0^{\pi/2} \frac{\log(\cos x)}{x^2+\log^2(\cos x)}dx = \frac{\pi}{2}\left(1-\frac{1}{\log 2}\right)$$,,"['calculus', 'complex-analysis', 'integration', 'definite-integrals']"
49,Extract real and imaginary parts of $\operatorname{Li}_2\left(i\left(2\pm\sqrt3\right)\right)$,Extract real and imaginary parts of,\operatorname{Li}_2\left(i\left(2\pm\sqrt3\right)\right),"We know that polylogarithms of complex argument sometimes have simple real and imaginary parts, e.g. $$\operatorname{Re}\big[\operatorname{Li}_2\left(i\right)\big]=-\frac{\pi^2}{48},\hspace{1em}\operatorname{Im}\big[\operatorname{Li}_2\left(i\right)\big]={\bf G},$$ where ${\bf G}$ is the Catalan constant. Are there closed forms (free of polylogs and imaginary numbers) for any of the following expressions? $$\operatorname{Re}\big[\operatorname{Li}_2\left(i\left(2\pm\sqrt3\right)\right)\big],\hspace{1em}\operatorname{Im}\big[\operatorname{Li}_2\left(i\left(2\pm\sqrt3\right)\right)\big]$$","We know that polylogarithms of complex argument sometimes have simple real and imaginary parts, e.g. $$\operatorname{Re}\big[\operatorname{Li}_2\left(i\right)\big]=-\frac{\pi^2}{48},\hspace{1em}\operatorname{Im}\big[\operatorname{Li}_2\left(i\right)\big]={\bf G},$$ where ${\bf G}$ is the Catalan constant. Are there closed forms (free of polylogs and imaginary numbers) for any of the following expressions? $$\operatorname{Re}\big[\operatorname{Li}_2\left(i\left(2\pm\sqrt3\right)\right)\big],\hspace{1em}\operatorname{Im}\big[\operatorname{Li}_2\left(i\left(2\pm\sqrt3\right)\right)\big]$$",,"['calculus', 'complex-analysis', 'special-functions', 'closed-form', 'polylogarithm']"
50,"Evaluation of $\int\frac{5x^3+3x-1}{(x^3+3x+1)^3}\,dx$",Evaluation of,"\int\frac{5x^3+3x-1}{(x^3+3x+1)^3}\,dx","Evaluate the integral   $$\int\frac{5x^3+3x-1}{(x^3+3x+1)^3}\,dx$$ My Attempt: Let $f(x) = \frac{ax+b}{(x^3+3x+1)^2}.$ Now differentiate both side with respect to $x$, and we get $$ \begin{align} f'(x) &= \frac{(x^3+3x+1)^2\cdot a-2\cdot (x^3+3x+1)\cdot (3x^2+3)\cdot (ax+b)}{(x^3+3x+1)^4}\\ &= \frac{5x^3+3x-1}{(x^3+3x+1)^3}\cdot\frac{(x^3+3x+1)\cdot a-6\cdot (x^2+1)\cdot (ax+b)}{(x^3+3x+1)^3}\\ &= \frac{5x^3+3x-1}{(x^3+3x+1)^3} \frac{-5ax^3+6bx^2-3ax+(a-6b)}{(x^3+3x+1)^3}\\ &= \frac{5x^3+3x-1}{(x^3+3x+1)^3} \end{align} $$ for $a = -1$ and $b = 0$. Thus, by the Fundamental Theorem of Calculus, $$ \begin{align} \int\frac{5x^3+3x-1}{(x^3+3x+1)^3}\,dx &= \int f'(x)\,dx\\ &= f(x)\\ &= -\frac{x}{(x^3+3x+1)^2}+\mathcal{C} \end{align} $$ How we can solve the above integral directly (maybe by using the substitution method)?","Evaluate the integral   $$\int\frac{5x^3+3x-1}{(x^3+3x+1)^3}\,dx$$ My Attempt: Let $f(x) = \frac{ax+b}{(x^3+3x+1)^2}.$ Now differentiate both side with respect to $x$, and we get $$ \begin{align} f'(x) &= \frac{(x^3+3x+1)^2\cdot a-2\cdot (x^3+3x+1)\cdot (3x^2+3)\cdot (ax+b)}{(x^3+3x+1)^4}\\ &= \frac{5x^3+3x-1}{(x^3+3x+1)^3}\cdot\frac{(x^3+3x+1)\cdot a-6\cdot (x^2+1)\cdot (ax+b)}{(x^3+3x+1)^3}\\ &= \frac{5x^3+3x-1}{(x^3+3x+1)^3} \frac{-5ax^3+6bx^2-3ax+(a-6b)}{(x^3+3x+1)^3}\\ &= \frac{5x^3+3x-1}{(x^3+3x+1)^3} \end{align} $$ for $a = -1$ and $b = 0$. Thus, by the Fundamental Theorem of Calculus, $$ \begin{align} \int\frac{5x^3+3x-1}{(x^3+3x+1)^3}\,dx &= \int f'(x)\,dx\\ &= f(x)\\ &= -\frac{x}{(x^3+3x+1)^2}+\mathcal{C} \end{align} $$ How we can solve the above integral directly (maybe by using the substitution method)?",,"['calculus', 'integration', 'indefinite-integrals']"
51,Difference between Slope and Gradient,Difference between Slope and Gradient,,"It has been a few years since studying contour maps. Often I hear slope and gradient interchangeably in describing steepness. Does anyone know any good definitions and analogies of slope and gradient. Thanks, Amanda","It has been a few years since studying contour maps. Often I hear slope and gradient interchangeably in describing steepness. Does anyone know any good definitions and analogies of slope and gradient. Thanks, Amanda",,"['calculus', 'terminology']"
52,Partial derivatives inverse question [closed],Partial derivatives inverse question [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question If $ \partial u/\partial v=a $, then $ \partial v/\partial u=1/a$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question If $ \partial u/\partial v=a $, then $ \partial v/\partial u=1/a$?",,['calculus']
53,"How to prove $4\times{_2F_1}(-1/4,3/4;7/4;(2-\sqrt3)/4)-{_2F_1}(3/4,3/4;7/4;(2-\sqrt3)/4)\stackrel?=\frac{3\sqrt[4]{2+\sqrt3}}{\sqrt2}$",How to prove,"4\times{_2F_1}(-1/4,3/4;7/4;(2-\sqrt3)/4)-{_2F_1}(3/4,3/4;7/4;(2-\sqrt3)/4)\stackrel?=\frac{3\sqrt[4]{2+\sqrt3}}{\sqrt2}","I have the following conjecture, which is supported by numerical calculations up to at least $10^5$ decimal digits: $$4\times{_2F_1}\left(-\frac{1}{4},\frac{3}{4};\frac{7}{4};\frac{2-\sqrt{3}}{4}\right)-{_2F_1}\left(\frac{3}{4},\frac{3}{4};\frac{7}{4};\frac{2-\sqrt{3}}{4}\right)\,\stackrel?=\,\frac{3\sqrt[4]{2+\sqrt{3}}}{\sqrt{2}},$$ where $_2F_1$ denotes the hypergeometric function . Can you suggest any ideas how to prove it? The conjectural closed form was obtained using WolframAlpha query ToRadicals[RootApproximant[2.94844576626425580599908814238570067699233]]","I have the following conjecture, which is supported by numerical calculations up to at least $10^5$ decimal digits: $$4\times{_2F_1}\left(-\frac{1}{4},\frac{3}{4};\frac{7}{4};\frac{2-\sqrt{3}}{4}\right)-{_2F_1}\left(\frac{3}{4},\frac{3}{4};\frac{7}{4};\frac{2-\sqrt{3}}{4}\right)\,\stackrel?=\,\frac{3\sqrt[4]{2+\sqrt{3}}}{\sqrt{2}},$$ where $_2F_1$ denotes the hypergeometric function . Can you suggest any ideas how to prove it? The conjectural closed form was obtained using WolframAlpha query ToRadicals[RootApproximant[2.94844576626425580599908814238570067699233]]",,"['calculus', 'special-functions', 'closed-form', 'conjectures', 'hypergeometric-function']"
54,"Integral $\int_0^\infty F(x)\,F\left(x\,\sqrt2\right)\frac{e^{-x^2}}{x^2} \, dx$ involving Dawson's function",Integral  involving Dawson's function,"\int_0^\infty F(x)\,F\left(x\,\sqrt2\right)\frac{e^{-x^2}}{x^2} \, dx","I need your help evaluating this integral: $$I=\int_0^\infty F(x)\,F\left(x\,\sqrt2\right)\frac{e^{-x^2}}{x^2} \, dx,\tag1$$ where $F(x)$ represents Dawson's function/integral : $$F(x)=e^{-x^2}\int_0^x e^{y^2} \, dy = \frac{\sqrt{\pi}}{2} e^{-x^{2}} \operatorname{erfi}(x).\tag2$$ Dawson's function can also be represented by the infinite integral $$F(x) = \frac{1}{2} \int_{0}^{\infty} e^{-t^{2}/4} \sin(xt) \,  dt.$$ Since $F(x)$ behaves like $x$ near $x=0$ and like $\frac{1}{2x}$ for large values of $x$ , we know that integral $(1)$ converges.","I need your help evaluating this integral: where represents Dawson's function/integral : Dawson's function can also be represented by the infinite integral Since behaves like near and like for large values of , we know that integral converges.","I=\int_0^\infty F(x)\,F\left(x\,\sqrt2\right)\frac{e^{-x^2}}{x^2} \, dx,\tag1 F(x) F(x)=e^{-x^2}\int_0^x e^{y^2} \, dy = \frac{\sqrt{\pi}}{2} e^{-x^{2}} \operatorname{erfi}(x).\tag2 F(x) = \frac{1}{2} \int_{0}^{\infty} e^{-t^{2}/4} \sin(xt) \,  dt. F(x) x x=0 \frac{1}{2x} x (1)","['calculus', 'integration', 'definite-integrals', 'special-functions', 'error-function']"
55,Proof of Wirtinger inequality,Proof of Wirtinger inequality,,"Quoting from Ana Cannas da Silva's book on Symplectic Geometry: ""As an exercise in Fourier series, show the Wirtinger inequality: for $f\in C^1([a,b])$, with $f(a)=f(b)=0$ we have $$ \int_a^b\Big|\frac{\mathrm{d}f}{\mathrm{d}t}\Big|^2\mathrm{d}t \ge\frac{\pi^2}{(b-a)^2} \int_a^b\left|\ f\right|^2\mathrm{d}t."" $$ I already found a few questions about this topic in the site, but I couldn't actually grasp what's happening here. Also, I would very much like you to show me where I go wrong with my try, which is sketched below. I know that, for $f\in \mathcal{L}^2([0,2\pi])\supset C^1([0,2\pi])$, I can expand: $$ f(t)=\sum_{n=-\infty}^{+\infty}c_n e^{int},\ \ \ c_n=\frac{1}{2\pi}\int_0^{2\pi}\mathrm{d}t\ e^{-int}f(t). $$ Rescaling  $t \to \omega (t - a)$, where $\omega = 2\pi/(b-a)$, we can get a more general form for $f\in C([a,b])$: $$ f(t)=\sum_{n=-\infty}^{+\infty}c_n e^{in\omega t},\ \ \ c_n=\frac{1}{b-a}\int_a^{b}\mathrm{d}t\ e^{-in\omega t}f(t). $$ Now, having: $$ \frac{\mathrm{d}}{\mathrm{d}t}f(t) = \sum_{n=-\infty}^{+\infty}\tilde{c}_ne^{i\omega nt},\ \ \tilde{c}_n=\frac{1}{b-a}\int_a^b\mathrm{d}t\ e^{-i\omega nt}\frac{\mathrm{d}}{\mathrm{d}t}f(t). $$ Using the fact that $\,f(a)=f(b)=0$ we get: $$ \tilde{c}_0 = \frac{1}{b-a}\int_a^b\mathrm{d}t \frac{\mathrm{d}}{\mathrm{d}t} f(t) = \frac{f(b)-f(a)}{b-a} =0 \longrightarrow  \frac{\mathrm{d}}{\mathrm{d}t}f(t) =  \sum_{n\in\mathbb Z\setminus\{0\}}\tilde{c}_n e^{i\omega nt} $$ Now deriving the series expansion of $f$ yields: $$ \frac{\mathrm{d}}{\mathrm{d}t}f(t) = \frac{\mathrm{d}}{\mathrm{d}t} \sum_{n=-\infty}^{+\infty}c_n e^{in\omega t} = \sum_{n=-\infty}^{+\infty}(in\omega)c_n e^{in\omega t}=\sum_{n\in\mathbb Z\setminus\{0\}}(in\omega)c_n e^{in\omega t} $$ Comparing the two expressions we establish: $\tilde{c}_n = i\omega n c_n$, for $n\not= 0$. Parseval's Equality reads here for $\mathrm{d}f/\mathrm{d}t$: $$ \int_a^b\left|\frac{\mathrm{d}f}{\mathrm{d}t}\right|^2\mathrm{d}t= \sum_{n\in\mathbb Z\setminus\{0\}}  |\tilde{c_n}|^2 =  \omega^2\sum_{n\in\mathbb Z\setminus\{0\}} n^2 |c_n|^2 \ge \omega^2\sum_{n\in\mathbb Z\setminus\{0\}}  |c_n|^2 =  \omega^2 \left(\int_a^b\mathrm{d}t\left|f(t)\right|^2-|c_0|^2\right) $$ where in the last passage we used Parseval's Equality for $f$: $\int_a^b|f|^2\mathrm{d}t=\sum_{n=-\infty}^{+\infty}|c_n|^2$. We are now left with finding a suitable way of estimating $|c_0|^2$: $$ |c_0|^2 = \left|\frac{1}{b-a}\int_a^b\mathrm{d}t\ f(t)\right|^2, $$ thus $$ \int_a^b\left|\frac{\mathrm{d}f}{\mathrm{d}t}\right|^2\mathrm{d}t \ge \frac{4\pi^2}{(b-a)^2}\left(\int_a^b\left|f\right|^2\mathrm{d}t - \frac{1}{(b-a)^2}\left|\int_a^bf\mathrm{d}t\right|^2\right). $$ Which is not exactly what I wanted. Please lend me a hand!","Quoting from Ana Cannas da Silva's book on Symplectic Geometry: ""As an exercise in Fourier series, show the Wirtinger inequality: for $f\in C^1([a,b])$, with $f(a)=f(b)=0$ we have $$ \int_a^b\Big|\frac{\mathrm{d}f}{\mathrm{d}t}\Big|^2\mathrm{d}t \ge\frac{\pi^2}{(b-a)^2} \int_a^b\left|\ f\right|^2\mathrm{d}t."" $$ I already found a few questions about this topic in the site, but I couldn't actually grasp what's happening here. Also, I would very much like you to show me where I go wrong with my try, which is sketched below. I know that, for $f\in \mathcal{L}^2([0,2\pi])\supset C^1([0,2\pi])$, I can expand: $$ f(t)=\sum_{n=-\infty}^{+\infty}c_n e^{int},\ \ \ c_n=\frac{1}{2\pi}\int_0^{2\pi}\mathrm{d}t\ e^{-int}f(t). $$ Rescaling  $t \to \omega (t - a)$, where $\omega = 2\pi/(b-a)$, we can get a more general form for $f\in C([a,b])$: $$ f(t)=\sum_{n=-\infty}^{+\infty}c_n e^{in\omega t},\ \ \ c_n=\frac{1}{b-a}\int_a^{b}\mathrm{d}t\ e^{-in\omega t}f(t). $$ Now, having: $$ \frac{\mathrm{d}}{\mathrm{d}t}f(t) = \sum_{n=-\infty}^{+\infty}\tilde{c}_ne^{i\omega nt},\ \ \tilde{c}_n=\frac{1}{b-a}\int_a^b\mathrm{d}t\ e^{-i\omega nt}\frac{\mathrm{d}}{\mathrm{d}t}f(t). $$ Using the fact that $\,f(a)=f(b)=0$ we get: $$ \tilde{c}_0 = \frac{1}{b-a}\int_a^b\mathrm{d}t \frac{\mathrm{d}}{\mathrm{d}t} f(t) = \frac{f(b)-f(a)}{b-a} =0 \longrightarrow  \frac{\mathrm{d}}{\mathrm{d}t}f(t) =  \sum_{n\in\mathbb Z\setminus\{0\}}\tilde{c}_n e^{i\omega nt} $$ Now deriving the series expansion of $f$ yields: $$ \frac{\mathrm{d}}{\mathrm{d}t}f(t) = \frac{\mathrm{d}}{\mathrm{d}t} \sum_{n=-\infty}^{+\infty}c_n e^{in\omega t} = \sum_{n=-\infty}^{+\infty}(in\omega)c_n e^{in\omega t}=\sum_{n\in\mathbb Z\setminus\{0\}}(in\omega)c_n e^{in\omega t} $$ Comparing the two expressions we establish: $\tilde{c}_n = i\omega n c_n$, for $n\not= 0$. Parseval's Equality reads here for $\mathrm{d}f/\mathrm{d}t$: $$ \int_a^b\left|\frac{\mathrm{d}f}{\mathrm{d}t}\right|^2\mathrm{d}t= \sum_{n\in\mathbb Z\setminus\{0\}}  |\tilde{c_n}|^2 =  \omega^2\sum_{n\in\mathbb Z\setminus\{0\}} n^2 |c_n|^2 \ge \omega^2\sum_{n\in\mathbb Z\setminus\{0\}}  |c_n|^2 =  \omega^2 \left(\int_a^b\mathrm{d}t\left|f(t)\right|^2-|c_0|^2\right) $$ where in the last passage we used Parseval's Equality for $f$: $\int_a^b|f|^2\mathrm{d}t=\sum_{n=-\infty}^{+\infty}|c_n|^2$. We are now left with finding a suitable way of estimating $|c_0|^2$: $$ |c_0|^2 = \left|\frac{1}{b-a}\int_a^b\mathrm{d}t\ f(t)\right|^2, $$ thus $$ \int_a^b\left|\frac{\mathrm{d}f}{\mathrm{d}t}\right|^2\mathrm{d}t \ge \frac{4\pi^2}{(b-a)^2}\left(\int_a^b\left|f\right|^2\mathrm{d}t - \frac{1}{(b-a)^2}\left|\int_a^bf\mathrm{d}t\right|^2\right). $$ Which is not exactly what I wanted. Please lend me a hand!",,"['calculus', 'integration', 'inequality', 'fourier-series', 'integral-inequality']"
56,Evaluate $\sum^\infty_{n=1} \frac{1}{n^4} $using Parseval's theorem (Fourier series),Evaluate using Parseval's theorem (Fourier series),\sum^\infty_{n=1} \frac{1}{n^4} ,"Evaluate $\sum^\infty_{n=1} \frac{1}{n^4} $using Parseval's theorem (Fourier series). I have , somehow, to find the sum of $\sum_{n=1}^\infty \frac{1}{n^4}$ using Parseval's theorem. I tried some things that didn't work so I won't post them. Can you please explain me how do I find the sum of this series using Parseval's identity? Thanks","Evaluate $\sum^\infty_{n=1} \frac{1}{n^4} $using Parseval's theorem (Fourier series). I have , somehow, to find the sum of $\sum_{n=1}^\infty \frac{1}{n^4}$ using Parseval's theorem. I tried some things that didn't work so I won't post them. Can you please explain me how do I find the sum of this series using Parseval's identity? Thanks",,"['calculus', 'sequences-and-series', 'fourier-series']"
57,Bounds on roots of polynomials,Bounds on roots of polynomials,,"What bound is there on the roots of a given polynomial, in terms of its degree and coefficients? Consider the polynomial $p(x) = 3x^7 – 5x^3 + 42$. Would you not agree, without doing any calculation, that one million ($10^6$) cannot be a root? It just wouldn’t be in accord with the smallness of the coefficients and the well-behavedness of polynomials. And yet I don’t recall ever having encountered anything in the literature that gave a bound on the absolute value of the roots of a polynomial in terms of the degree and coefficients of the polynomial, but I’m pretty sure such must exist, and that I simply missed it, and so I’m tagging this a a reference-request. By the way, during my post of a question, every time, it seems, there are stray graphics on the screen, as if from someone else's question or answer. Is this happening to anyone else?","What bound is there on the roots of a given polynomial, in terms of its degree and coefficients? Consider the polynomial $p(x) = 3x^7 – 5x^3 + 42$. Would you not agree, without doing any calculation, that one million ($10^6$) cannot be a root? It just wouldn’t be in accord with the smallness of the coefficients and the well-behavedness of polynomials. And yet I don’t recall ever having encountered anything in the literature that gave a bound on the absolute value of the roots of a polynomial in terms of the degree and coefficients of the polynomial, but I’m pretty sure such must exist, and that I simply missed it, and so I’m tagging this a a reference-request. By the way, during my post of a question, every time, it seems, there are stray graphics on the screen, as if from someone else's question or answer. Is this happening to anyone else?",,"['calculus', 'reference-request']"
58,How to find $\int x^{1/x}\mathrm dx$,How to find,\int x^{1/x}\mathrm dx,EDIT : The full answer has been posted by myself. Feel free to check the logic within. How does one indefinitely integrate a function in the form of $$f(x)=x^{1/x}$$ Looking at all the things that I know there is nothing about exponents with variables. So how does one find: $$\int x^{1/x}\;\mathrm dx?$$ I am more interested in the technique for doing so rather than the solution as it is of no real significance but merely a curiosity.,EDIT : The full answer has been posted by myself. Feel free to check the logic within. How does one indefinitely integrate a function in the form of $$f(x)=x^{1/x}$$ Looking at all the things that I know there is nothing about exponents with variables. So how does one find: $$\int x^{1/x}\;\mathrm dx?$$ I am more interested in the technique for doing so rather than the solution as it is of no real significance but merely a curiosity.,,"['calculus', 'integration', 'indefinite-integrals']"
59,"Is there an analogue to the ""Delta"" symbol for ratios?","Is there an analogue to the ""Delta"" symbol for ratios?",,"A capital delta ($\Delta$) is commonly used to indicate a difference (especially an incremental difference). For example, $\Delta x = x_1 - x_0$ My question is: is there an analogue of this notation for ratios? In other words, what's the best symbol to use for $[?]$ in $[?]x = \dfrac{x_1}{x_0}$?","A capital delta ($\Delta$) is commonly used to indicate a difference (especially an incremental difference). For example, $\Delta x = x_1 - x_0$ My question is: is there an analogue of this notation for ratios? In other words, what's the best symbol to use for $[?]$ in $[?]x = \dfrac{x_1}{x_0}$?",,"['calculus', 'notation']"
60,Proof of associativity of convolution,Proof of associativity of convolution,,"I intend to prove the associativity of convolution but failed after several trials, i.e. $(f \ast g) \ast h = f \ast (g \ast h)$ where $(f \ast g)(t) = \int^{t}_{0}f(s)g(t-s)ds $ There are a number of proves considering $(f \ast g)(t) = \int^{\infty}_{-\infty}f(s)g(t-s)ds $. Those did not help since I have a different definition. Would anyone be able to show the proof here? Thanks.","I intend to prove the associativity of convolution but failed after several trials, i.e. $(f \ast g) \ast h = f \ast (g \ast h)$ where $(f \ast g)(t) = \int^{t}_{0}f(s)g(t-s)ds $ There are a number of proves considering $(f \ast g)(t) = \int^{\infty}_{-\infty}f(s)g(t-s)ds $. Those did not help since I have a different definition. Would anyone be able to show the proof here? Thanks.",,"['calculus', 'convolution']"
61,"Closed form for $\int_{-1}^1\frac{\ln\left(2+x\,\sqrt3\right)}{\sqrt{1-x^2}\,\left(2+x\,\sqrt3\right)^n}dx$",Closed form for,"\int_{-1}^1\frac{\ln\left(2+x\,\sqrt3\right)}{\sqrt{1-x^2}\,\left(2+x\,\sqrt3\right)^n}dx","I'm trying to find a closed form for the following integral: $$\mathcal{J}(n)=\int_{-1}^1\frac{\ln\left(2+x\,\sqrt3\right)}{\sqrt{1-x^2}\,\left(2+x\,\sqrt3\right)^n}dx\tag1$$ I have conjectured values of $\mathcal{J}(n)$ (supported by numerical inegration) for some integer values of $n$: $$\begin{align}&\mathcal{J}(1)\stackrel?=-\pi\ln\left(\frac32\right),\\&\mathcal{J}(2)\stackrel?=-\pi\left(1+2\ln\left(\frac32\right)\right),\\&\mathcal{J}(3)\stackrel?=-\pi\left(\frac{15}4+\frac{11}2\ln\left(\frac32\right)\right),\\&\mathcal{J}(4)\stackrel?=-\pi\left(\frac{77}6+17\ln\left(\frac32\right)\right).\end{align}\tag2$$ These values suggest that a general form for $n\in\mathbb{N}$ is $$\mathcal{J}(n)\stackrel?=-\pi\left(a_n+b_n\ln\left(\frac32\right)\right),\tag3$$ where $a_n, b_n$ are some rational coefficients. Moreover, I conjecture that $$b_n\stackrel?={_2F_1}\left(1-n,n;\,1;\,-\frac12\right).\tag4$$ Are my conjectures true? Can we find a formula or recurrence relation for $a_n$? Can we find a general formula for $\mathcal{J}(z)$ for non-integer values of $z$?","I'm trying to find a closed form for the following integral: $$\mathcal{J}(n)=\int_{-1}^1\frac{\ln\left(2+x\,\sqrt3\right)}{\sqrt{1-x^2}\,\left(2+x\,\sqrt3\right)^n}dx\tag1$$ I have conjectured values of $\mathcal{J}(n)$ (supported by numerical inegration) for some integer values of $n$: $$\begin{align}&\mathcal{J}(1)\stackrel?=-\pi\ln\left(\frac32\right),\\&\mathcal{J}(2)\stackrel?=-\pi\left(1+2\ln\left(\frac32\right)\right),\\&\mathcal{J}(3)\stackrel?=-\pi\left(\frac{15}4+\frac{11}2\ln\left(\frac32\right)\right),\\&\mathcal{J}(4)\stackrel?=-\pi\left(\frac{77}6+17\ln\left(\frac32\right)\right).\end{align}\tag2$$ These values suggest that a general form for $n\in\mathbb{N}$ is $$\mathcal{J}(n)\stackrel?=-\pi\left(a_n+b_n\ln\left(\frac32\right)\right),\tag3$$ where $a_n, b_n$ are some rational coefficients. Moreover, I conjecture that $$b_n\stackrel?={_2F_1}\left(1-n,n;\,1;\,-\frac12\right).\tag4$$ Are my conjectures true? Can we find a formula or recurrence relation for $a_n$? Can we find a general formula for $\mathcal{J}(z)$ for non-integer values of $z$?",,"['calculus', 'definite-integrals', 'closed-form', 'conjectures', 'hypergeometric-function']"
62,Generalizing $\sum \limits_{n=1}^{\infty }n^{2}/x^{n}$ to $\sum \limits_{n=1}^{\infty }n^{p}/x^{n}$,Generalizing  to,\sum \limits_{n=1}^{\infty }n^{2}/x^{n} \sum \limits_{n=1}^{\infty }n^{p}/x^{n},"For computing the present worth of an infinite sequence of equally spaced payments $(n^{2})$ I had the need to evaluate $$\displaystyle\sum_{n=1}^{\infty}\frac{n^{2}}{x^{n}}=\dfrac{x(x+1)}{(x-1)^{3}}\qquad x>1.$$ The method I used was based on the geometric series $\displaystyle\sum_{n=1}^{\infty}x^{n}=\dfrac{x}{1-x}$ differentiating each side followed by a multiplication by $x$, differentiating a second time and multiplying again by $x$. There is at least a second (more difficult) method that is to compute the series partial sums and letting $n$ go to infinity. Question: Is there a closed form for $$\displaystyle\sum_{n=1}^{\infty }\dfrac{n^{p}}{x^{n}}\qquad x>1,p\in\mathbb{Z}^{+}\quad ?$$ What is the sketch of its proof in case it exists?","For computing the present worth of an infinite sequence of equally spaced payments $(n^{2})$ I had the need to evaluate $$\displaystyle\sum_{n=1}^{\infty}\frac{n^{2}}{x^{n}}=\dfrac{x(x+1)}{(x-1)^{3}}\qquad x>1.$$ The method I used was based on the geometric series $\displaystyle\sum_{n=1}^{\infty}x^{n}=\dfrac{x}{1-x}$ differentiating each side followed by a multiplication by $x$, differentiating a second time and multiplying again by $x$. There is at least a second (more difficult) method that is to compute the series partial sums and letting $n$ go to infinity. Question: Is there a closed form for $$\displaystyle\sum_{n=1}^{\infty }\dfrac{n^{p}}{x^{n}}\qquad x>1,p\in\mathbb{Z}^{+}\quad ?$$ What is the sketch of its proof in case it exists?",,"['calculus', 'sequences-and-series']"
63,How to prove $\sum_{n=1}^\infty\operatorname{arccot}\frac{\sqrt[2^n]2+\cos\frac\pi{2^n}}{\sin\frac\pi{2^n}}=\operatorname{arccot}\frac{\ln2}\pi$? [closed],How to prove ? [closed],\sum_{n=1}^\infty\operatorname{arccot}\frac{\sqrt[2^n]2+\cos\frac\pi{2^n}}{\sin\frac\pi{2^n}}=\operatorname{arccot}\frac{\ln2}\pi,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How can I prove the following identity? $$\sum_{n=1}^\infty\operatorname{arccot}\frac{\sqrt[2^n]2+\cos\frac\pi{2^n}}{\sin\frac\pi{2^n}}=\operatorname{arccot}\frac{\ln2}\pi$$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How can I prove the following identity? $$\sum_{n=1}^\infty\operatorname{arccot}\frac{\sqrt[2^n]2+\cos\frac\pi{2^n}}{\sin\frac\pi{2^n}}=\operatorname{arccot}\frac{\ln2}\pi$$",,"['calculus', 'sequences-and-series', 'trigonometry', 'contest-math', 'closed-form']"
64,Two seemingly contradictory series in a calc 2 exam,Two seemingly contradictory series in a calc 2 exam,,"On a calculus exam I took recently, there were two problems. Find the sum of the series defined by $\sum_{n=1}^{\infty}\frac{1}{(n(n+1))}$ A series $\sum_{n=1}^{\infty}a_n$ has partial sums $s_n = \frac{n-1}{n+1}$ . Find $a_n$ and $\sum_{n=1}^{\infty}a_n$ I got both problems correct, but my answers seem to conflict with each other. For 1, I got 1. For 2, I also got 1 for the sum, and $\frac{2}{n(n+1)}$ for the value of $a_n$ My question is, how can this be possible? The value of $a_n$ in question 2 should be twice that of question 1, so shouldn't the value of the series in question 2 be 2? But when you look at the definition of the series in question 2, the sum as n approaches infinity is obviously 1. Did I make a mistake in finding $a_n$ ? I got it from $s_n - s_{n-1}$ , which should just give me $a_n$ . I brought it up with my TA and professor, and they didn't know, either. What am I missing here?","On a calculus exam I took recently, there were two problems. Find the sum of the series defined by A series has partial sums . Find and I got both problems correct, but my answers seem to conflict with each other. For 1, I got 1. For 2, I also got 1 for the sum, and for the value of My question is, how can this be possible? The value of in question 2 should be twice that of question 1, so shouldn't the value of the series in question 2 be 2? But when you look at the definition of the series in question 2, the sum as n approaches infinity is obviously 1. Did I make a mistake in finding ? I got it from , which should just give me . I brought it up with my TA and professor, and they didn't know, either. What am I missing here?",\sum_{n=1}^{\infty}\frac{1}{(n(n+1))} \sum_{n=1}^{\infty}a_n s_n = \frac{n-1}{n+1} a_n \sum_{n=1}^{\infty}a_n \frac{2}{n(n+1)} a_n a_n a_n s_n - s_{n-1} a_n,"['calculus', 'sequences-and-series']"
65,Calculating in closed form $\int _0^1\int _0^1\frac{1}{1+x y (x+y)} \ dx \ dy$,Calculating in closed form,\int _0^1\int _0^1\frac{1}{1+x y (x+y)} \ dx \ dy,"Integrating with respect to a variable and then to the other one, things look pretty complicated, but I'm sure you have ideas that might simplify the job to do here. This time we're talking about $$\int _0^1\int _0^1\frac{1}{1+x y (x+y)} \ dx \ dy$$ The bounty moment : after 2 years and 8 months from the releasing moment of the question, it's time for a 300 points bounty for finding the simplest closed-form of the integral! Supplementary question : Here is an extension of the question for those with a need for more challenging questions. Calculate $$\int _0^1\int _0^1\cdots\int _0^1\frac{1}{1+x_1 x_2\cdots x_n (x_1+x_2+\cdots +x_n)} \ \textrm{d}x_1 \ \textrm{d}x_2\cdots \textrm{d}x_n, \ n\ge 2.$$ Last but not least, special greetings to Cleo!","Integrating with respect to a variable and then to the other one, things look pretty complicated, but I'm sure you have ideas that might simplify the job to do here. This time we're talking about $$\int _0^1\int _0^1\frac{1}{1+x y (x+y)} \ dx \ dy$$ The bounty moment : after 2 years and 8 months from the releasing moment of the question, it's time for a 300 points bounty for finding the simplest closed-form of the integral! Supplementary question : Here is an extension of the question for those with a need for more challenging questions. Calculate $$\int _0^1\int _0^1\cdots\int _0^1\frac{1}{1+x_1 x_2\cdots x_n (x_1+x_2+\cdots +x_n)} \ \textrm{d}x_1 \ \textrm{d}x_2\cdots \textrm{d}x_n, \ n\ge 2.$$ Last but not least, special greetings to Cleo!",,"['calculus', 'integration', 'sequences-and-series', 'definite-integrals', 'special-functions']"
66,"Evaluate $\lim\limits_{x\to\infty}\frac1x\int_0^x\max\{\sin t,\sin(t\sqrt2)\}dt$",Evaluate,"\lim\limits_{x\to\infty}\frac1x\int_0^x\max\{\sin t,\sin(t\sqrt2)\}dt","I want to evaluate $$L=\lim_{x\to\infty}\frac1x\int_0^x\max\{\sin t,\sin(t\sqrt2)\}dt$$ My attempt $$L=\lim_{x\to\infty}\frac1{2x}\int_0^x\Big(\sin t+\sin(t\sqrt2)+\big|\sin t-\sin(t\sqrt2)\big|\Big)dt\\ =\lim_{x\to\infty}\frac1{2x}\int_0^x\big|\sin t-\sin(t\sqrt2)\big|dt\\ =\lim_{x\to\infty}\frac1x\int_0^x\bigg|\cos\frac{\sqrt2+1}2t\cdot\sin\frac{\sqrt2-1}2t\bigg|dt$$ Denote $s_n$ the $n$th zero point of $\cos\frac{\sqrt2+1}2t\cdot\sin\frac{\sqrt2-1}2t\ (t\ge0)$. Since $1$, $\sqrt2$ and $\pi$ are linear independent in $\mathbb Q$, the order of the zero points should be $1$. According to the squeeze theorem, we have $$L=\lim_{n\to\infty}\frac1{s_{n+1}}\sum_{k=0}^n(-1)^k\int_{s_k}^{s_{k+1}}\big(\sin t-\sin(t\sqrt2)\big)dt\\ =\lim_{n\to\infty}\frac1{s_{n+1}}\sum_{k=0}^n(-1)^k\bigg(\cos s_k-\cos s_{k+1}+\frac{\cos\sqrt2s_k-\cos\sqrt2s_{k+1}}{\sqrt2}\bigg)dt$$ I can't go further. I think the zero points of that function is the key point.","I want to evaluate $$L=\lim_{x\to\infty}\frac1x\int_0^x\max\{\sin t,\sin(t\sqrt2)\}dt$$ My attempt $$L=\lim_{x\to\infty}\frac1{2x}\int_0^x\Big(\sin t+\sin(t\sqrt2)+\big|\sin t-\sin(t\sqrt2)\big|\Big)dt\\ =\lim_{x\to\infty}\frac1{2x}\int_0^x\big|\sin t-\sin(t\sqrt2)\big|dt\\ =\lim_{x\to\infty}\frac1x\int_0^x\bigg|\cos\frac{\sqrt2+1}2t\cdot\sin\frac{\sqrt2-1}2t\bigg|dt$$ Denote $s_n$ the $n$th zero point of $\cos\frac{\sqrt2+1}2t\cdot\sin\frac{\sqrt2-1}2t\ (t\ge0)$. Since $1$, $\sqrt2$ and $\pi$ are linear independent in $\mathbb Q$, the order of the zero points should be $1$. According to the squeeze theorem, we have $$L=\lim_{n\to\infty}\frac1{s_{n+1}}\sum_{k=0}^n(-1)^k\int_{s_k}^{s_{k+1}}\big(\sin t-\sin(t\sqrt2)\big)dt\\ =\lim_{n\to\infty}\frac1{s_{n+1}}\sum_{k=0}^n(-1)^k\bigg(\cos s_k-\cos s_{k+1}+\frac{\cos\sqrt2s_k-\cos\sqrt2s_{k+1}}{\sqrt2}\bigg)dt$$ I can't go further. I think the zero points of that function is the key point.",,"['calculus', 'integration', 'definite-integrals']"
67,"Prove ${\large\int}_0^1\frac{\ln(1+8x)}{x^{2/3}\,(1-x)^{2/3}\,(1+8x)^{1/3}}dx=\frac{\ln3}{\pi\sqrt3}\Gamma^3\!\left(\tfrac13\right)$",Prove,"{\large\int}_0^1\frac{\ln(1+8x)}{x^{2/3}\,(1-x)^{2/3}\,(1+8x)^{1/3}}dx=\frac{\ln3}{\pi\sqrt3}\Gamma^3\!\left(\tfrac13\right)","Here is one more numerically discovered conjecture that I was not able to prove, and asking you for help: $${\large\int}_0^1\frac{\ln(1+8x)}{x^{\small2/3}\,(1-x)^{\small2/3}\,(1+8x)^{\small1/3}}dx\stackrel{\color{#A0A0A0}{\small?}}=\frac{\ln3}{\pi\sqrt3}\Gamma^3\!\left(\tfrac13\right),\tag1$$ or, equivalently, $$\frac{d}{da}{_2F_1}\!\left(a,\frac13;\,\frac23;\,-8\right)\Bigg|_{a=\frac13}\stackrel{\color{#A0A0A0}{\small?}}=-\frac{2\ln3}3.\tag2$$ Update: I can suggest a generalization of this conjecture that might be easier to prove: $$_2F_1\!\left(-a,\frac16-\frac{a}2;\,\frac23;\,-8\right)=2\times3^{\frac{3a-1}2}\cos\left(\frac\pi6(3a+1)\right).\tag3$$ This is actually the identity 07.23.03.0658.01 from the Wolfram Functions Site generalized to non-integer values of $a$.","Here is one more numerically discovered conjecture that I was not able to prove, and asking you for help: $${\large\int}_0^1\frac{\ln(1+8x)}{x^{\small2/3}\,(1-x)^{\small2/3}\,(1+8x)^{\small1/3}}dx\stackrel{\color{#A0A0A0}{\small?}}=\frac{\ln3}{\pi\sqrt3}\Gamma^3\!\left(\tfrac13\right),\tag1$$ or, equivalently, $$\frac{d}{da}{_2F_1}\!\left(a,\frac13;\,\frac23;\,-8\right)\Bigg|_{a=\frac13}\stackrel{\color{#A0A0A0}{\small?}}=-\frac{2\ln3}3.\tag2$$ Update: I can suggest a generalization of this conjecture that might be easier to prove: $$_2F_1\!\left(-a,\frac16-\frac{a}2;\,\frac23;\,-8\right)=2\times3^{\frac{3a-1}2}\cos\left(\frac\pi6(3a+1)\right).\tag3$$ This is actually the identity 07.23.03.0658.01 from the Wolfram Functions Site generalized to non-integer values of $a$.",,"['calculus', 'derivatives', 'definite-integrals', 'special-functions', 'hypergeometric-function']"
68,A closed form for a triple integral involving Heron's formula,A closed form for a triple integral involving Heron's formula,,"Let $$S(x,y,z)=\frac14\sqrt{(x+y+z) (-x+y+z) (x-y+z) (x+y-z) }\tag1$$ (note that it's Heron's formula for the area of a triangle with sides of lengths $x,y,z$ ). I'm trying to evaluate the following integral in a closed form: $$\mathcal U=\int_0^1\int_0^x\int_{x - y}^{x + y}\sqrt{S(x,y,z) } dz dy dx.\tag2$$ I wasn't able to evaluate it symbolically (either manually or using Mathematica ), but using numerical integration and heuristic methods, I found a plausible form: $$\mathcal U\stackrel{\color{gray}?}=\frac\pi{12\sqrt{2 }}+\frac{\Gamma\left(\frac14\right)^4\sqrt{10 }}{1440 \pi^2}\left(\left(\small\frac12-\frac1{\sqrt{5 \phi }}\right)\textbf K(\alpha)^2-\textbf K(\alpha) \textbf E(\alpha)\right)\\ \quad\quad\;\approx0.117599420842157114228246644831065494814051852697…,\tag3$$ where $\textbf K(\cdot),\textbf E(\cdot)$ are the complete elliptic integrals of the first and the second kind, and $\alpha=\frac1{\phi \sqrt2}+\frac1{\sqrt{2 \phi}}$ . Can we prove that $(3)$ is indeed the true value of the integral? Update: You might ask, what heuristic methods could possibly give us the conjectured value? Here is a brief explanation. First, calculate a sequence of values of $\int\!\int\!\int S^{2n}dz dy dx,n\in\mathbb N;$ their evaluation is straightforward and they are all rational numbers. Then, use FindSequenceFunction to find a possible recurrence relation for that sequence. We need at least $30$ elements of the sequence to get a result: $$\small{(5+2 n)^2 (7+4 n) (9+4 n) (11+4 n) (27+20 n)\cdot a(n+2)=\\4 (2+n) (7+4 n) \left(6750+16947 n+15764 n^2+6416 n^3+960    n^4\right)\cdot a(n+1)+\\128 (1+n)^2 (2+n) (3+2 n) (3+4 n) (47+20 n)\cdot a(n)}\tag4$$ Of course, there is no guarantee that this relation holds for all larger $n$ , but we cross our fingers 🤞 and conjecture that it does. Next, use FunctionExpand to find a possible explicit expression for the general term determined by this recurrence relation; it's complicated and involves generalized hypergeometric functions; we also need to manually adjust a periodic factor to ensure the result remains real even for non-integer $n$ . After some simplifications we arrive at this: $$\small\tfrac{4^n (20 n+27) \Gamma (2 n+2)\sqrt\pi}{(n+1) (16 n+12) \Gamma \left(2 n+\frac{7}{2}\right)}\,{_5F_4}\left(1,n+\tfrac{3}{2},n+\tfrac{3}{2},n+\tfrac{3}{2},n+\tfrac{47}{20};n+\tfrac{27}{20},n+\tfrac{7}{4},n+2,n+\tfrac{9}{4};-4\right)\tag5$$ Then we cross our fingers again 🤞 and conjecture that the same formula holds for all non-integer $n$ as well. Next, substitute $n=1/4$ and use FunctionExpand again to expand the generalized hypergeometric functions in terms of elliptic integrals. The result is quite large and unwieldy ( $19$ terms), but it appears to match the integral numerically, which is a sign we might be on the right track. We cross our fingers one more time 🤞 and conjecture that only a few terms of the expression are actually linearly independent over $\mathbb Z,$ and the result can be significantly simplified. Using FindIntegerNullVector and high enough numeric precision, we find a possible basis, and a linear combination of its elements that numerically matches the integral. After some radical denestings (using ResourceFunction[""RadicalDenest""] ) and other simplifications, this gives us the conjectured value $(3).$","Let (note that it's Heron's formula for the area of a triangle with sides of lengths ). I'm trying to evaluate the following integral in a closed form: I wasn't able to evaluate it symbolically (either manually or using Mathematica ), but using numerical integration and heuristic methods, I found a plausible form: where are the complete elliptic integrals of the first and the second kind, and . Can we prove that is indeed the true value of the integral? Update: You might ask, what heuristic methods could possibly give us the conjectured value? Here is a brief explanation. First, calculate a sequence of values of their evaluation is straightforward and they are all rational numbers. Then, use FindSequenceFunction to find a possible recurrence relation for that sequence. We need at least elements of the sequence to get a result: Of course, there is no guarantee that this relation holds for all larger , but we cross our fingers 🤞 and conjecture that it does. Next, use FunctionExpand to find a possible explicit expression for the general term determined by this recurrence relation; it's complicated and involves generalized hypergeometric functions; we also need to manually adjust a periodic factor to ensure the result remains real even for non-integer . After some simplifications we arrive at this: Then we cross our fingers again 🤞 and conjecture that the same formula holds for all non-integer as well. Next, substitute and use FunctionExpand again to expand the generalized hypergeometric functions in terms of elliptic integrals. The result is quite large and unwieldy ( terms), but it appears to match the integral numerically, which is a sign we might be on the right track. We cross our fingers one more time 🤞 and conjecture that only a few terms of the expression are actually linearly independent over and the result can be significantly simplified. Using FindIntegerNullVector and high enough numeric precision, we find a possible basis, and a linear combination of its elements that numerically matches the integral. After some radical denestings (using ResourceFunction[""RadicalDenest""] ) and other simplifications, this gives us the conjectured value","S(x,y,z)=\frac14\sqrt{(x+y+z) (-x+y+z) (x-y+z) (x+y-z) }\tag1 x,y,z \mathcal U=\int_0^1\int_0^x\int_{x - y}^{x + y}\sqrt{S(x,y,z) } dz dy dx.\tag2 \mathcal U\stackrel{\color{gray}?}=\frac\pi{12\sqrt{2 }}+\frac{\Gamma\left(\frac14\right)^4\sqrt{10 }}{1440 \pi^2}\left(\left(\small\frac12-\frac1{\sqrt{5 \phi }}\right)\textbf K(\alpha)^2-\textbf K(\alpha) \textbf E(\alpha)\right)\\ \quad\quad\;\approx0.117599420842157114228246644831065494814051852697…,\tag3 \textbf K(\cdot),\textbf E(\cdot) \alpha=\frac1{\phi \sqrt2}+\frac1{\sqrt{2 \phi}} (3) \int\!\int\!\int S^{2n}dz dy dx,n\in\mathbb N; 30 \small{(5+2 n)^2 (7+4 n) (9+4 n) (11+4 n) (27+20 n)\cdot a(n+2)=\\4 (2+n) (7+4 n) \left(6750+16947 n+15764 n^2+6416 n^3+960
   n^4\right)\cdot a(n+1)+\\128 (1+n)^2 (2+n) (3+2 n) (3+4 n) (47+20 n)\cdot a(n)}\tag4 n n \small\tfrac{4^n (20 n+27) \Gamma (2 n+2)\sqrt\pi}{(n+1) (16 n+12) \Gamma \left(2 n+\frac{7}{2}\right)}\,{_5F_4}\left(1,n+\tfrac{3}{2},n+\tfrac{3}{2},n+\tfrac{3}{2},n+\tfrac{47}{20};n+\tfrac{27}{20},n+\tfrac{7}{4},n+2,n+\tfrac{9}{4};-4\right)\tag5 n n=1/4 19 \mathbb Z, (3).","['calculus', 'integration', 'definite-integrals', 'closed-form', 'elliptic-integrals']"
69,Are there any special rules when making a substitution in an integral?,Are there any special rules when making a substitution in an integral?,,"Please consider the integral: $$\int_{-a}^{a}x^2dx=\frac{2a^3}{3}$$ I would like to know why I can't make the substitution: $$u=x^2$$ When I make the substitution, the limits of the integral will be the same, and the integral itself will be zero, which is the wrong answer. So why does this simple change of variables not work as I have expected? Please note that I do not want help solving the integral, I know how to solve it several ways. My question is why does this specific attempt at a solution not work?","Please consider the integral: $$\int_{-a}^{a}x^2dx=\frac{2a^3}{3}$$ I would like to know why I can't make the substitution: $$u=x^2$$ When I make the substitution, the limits of the integral will be the same, and the integral itself will be zero, which is the wrong answer. So why does this simple change of variables not work as I have expected? Please note that I do not want help solving the integral, I know how to solve it several ways. My question is why does this specific attempt at a solution not work?",,"['calculus', 'integration', 'indefinite-integrals']"
70,"An integral $\int_0^\infty P_s(x-1)\,e^{-x}\,dx$ involving Legendre functions",An integral  involving Legendre functions,"\int_0^\infty P_s(x-1)\,e^{-x}\,dx","Let $P_s(x)$ denote the Legendre functions of the $1^{st}$ kind , i.e. the Legendre polynomial generalized to an arbitrary (not necessarily integer) order $s$ . It can be expressed using the hypergeometric function : $$P_s(x)={_2F_1}\left(-s,s+1;\ 1;\ \frac{1-x}2\right).\tag1$$ Let's consider the definite integral $$\mathcal{J}(s)=\int_0^\infty P_s(x-1)\,e^{-x}\,dx.\tag2$$ It evaluates to integer values when $s\in\mathbb{Z}^+$ . Using a computer-assisted search for the general term formula, I discovered the following conjecture, that I haven't yet been able to prove: $$\mathcal{J}(s)\stackrel?=\left(K_{3/2}(1)\cdot I_{s+1/2}(-1)-I_{3/2}(-1)\cdot K_{s+1/2}(1)\right)\sqrt{-1},\tag3$$ where $I_\nu(z)$ and $K_\nu(z)$ are the modified Bessel functions of the $1^{st}$ and $2^{nd}$ kind. Unfortunately, it only seems to hold for $s\in\mathbb{Z}^+$ , and does not generalize to non-integer values of $s$ . Can we prove the conjecture $(3)$ ? Can we find a more general formula that holds not only for integer values of $s$ ? Can we find (or at least conjecture) a closed form for $\mathcal{J}(1/2)$ ?","Let denote the Legendre functions of the kind , i.e. the Legendre polynomial generalized to an arbitrary (not necessarily integer) order . It can be expressed using the hypergeometric function : Let's consider the definite integral It evaluates to integer values when . Using a computer-assisted search for the general term formula, I discovered the following conjecture, that I haven't yet been able to prove: where and are the modified Bessel functions of the and kind. Unfortunately, it only seems to hold for , and does not generalize to non-integer values of . Can we prove the conjecture ? Can we find a more general formula that holds not only for integer values of ? Can we find (or at least conjecture) a closed form for ?","P_s(x) 1^{st} s P_s(x)={_2F_1}\left(-s,s+1;\ 1;\ \frac{1-x}2\right).\tag1 \mathcal{J}(s)=\int_0^\infty P_s(x-1)\,e^{-x}\,dx.\tag2 s\in\mathbb{Z}^+ \mathcal{J}(s)\stackrel?=\left(K_{3/2}(1)\cdot I_{s+1/2}(-1)-I_{3/2}(-1)\cdot K_{s+1/2}(1)\right)\sqrt{-1},\tag3 I_\nu(z) K_\nu(z) 1^{st} 2^{nd} s\in\mathbb{Z}^+ s (3) s \mathcal{J}(1/2)","['calculus', 'integration', 'special-functions', 'conjectures', 'hypergeometric-function']"
71,Getting different answers when integrating using different techniques,Getting different answers when integrating using different techniques,,"Question: Is it possible to get multiple correct results when evaluating an indefinite integral?  If I use two different techniques to evaluate an integral, and I get two different answers, have I necessarily done something wrong? Often, an indefinite integral can be evaluated using different techniques.  For example, an integrand might be simplified via partial fractions or other algebraic techniques before integration, or it might be amenable to a clever substitution.  These techniques give different results.  For example, looking over a few other questions on MSE: From this question : evaluate $$ \int x(x^2+2)^4\,\mathrm{d}x. $$ Via the substitution $u = x^2+2$ , this becomes $$ \int x(x^2+2)^4\,\mathrm{d}x       = \frac{1}{10}x^{10} + x^8 + 4x^6 + 8x^4 + 8x^2 + \frac{32}{5} + C.     $$ However, multiplying out the polynomial and integrating using the power rule gives $$ \int x(x^2+2)^4\,\mathrm{d}x       = \frac{1}{10}x^{10} + x^8 + 4x^6 + 8x^4 + 8x^2 + C    $$ From this question :  evaluate $$ \int \frac{1-x}{(x+1)^2} \,\mathrm{d}x. $$ Simplifying the integrand using partial fractions then integrating gives $$ \int \frac{1-x}{(x+1)^2} \,\mathrm{d}x       = \frac{-2}{(x+1)} - \ln|x+1| + C.    $$ Via integration by parts, we get $$ \int \frac{1-x}{(x+1)^2} \,\mathrm{d}x       = \frac{x-1}{(x+1)} - \ln|x+1| + C.    $$ From this question : evaluate $$ \int \frac {\tan(\pi x)\sec^2(\pi x)}2\,\mathrm{d}x. $$ Using the substitution $u = \sec(\pi x)$ , this becomes $$ \int \frac {\tan(\pi x)\sec^2(\pi x)}2\,\mathrm{d}x       = \frac {\sec^2(\pi x)}{4\pi} + C.    $$ Using the substitution $u = \tan(\pi x)$ , this becomes $$ \int \frac {\tan(\pi x)\sec^2(\pi x)}2\,\mathrm{d}x       = \frac {\tan^2(\pi x)}{4\pi} + C.    $$","Question: Is it possible to get multiple correct results when evaluating an indefinite integral?  If I use two different techniques to evaluate an integral, and I get two different answers, have I necessarily done something wrong? Often, an indefinite integral can be evaluated using different techniques.  For example, an integrand might be simplified via partial fractions or other algebraic techniques before integration, or it might be amenable to a clever substitution.  These techniques give different results.  For example, looking over a few other questions on MSE: From this question : evaluate Via the substitution , this becomes However, multiplying out the polynomial and integrating using the power rule gives From this question :  evaluate Simplifying the integrand using partial fractions then integrating gives Via integration by parts, we get From this question : evaluate Using the substitution , this becomes Using the substitution , this becomes"," \int x(x^2+2)^4\,\mathrm{d}x.  u = x^2+2  \int x(x^2+2)^4\,\mathrm{d}x
      = \frac{1}{10}x^{10} + x^8 + 4x^6 + 8x^4 + 8x^2 + \frac{32}{5} + C. 
     \int x(x^2+2)^4\,\mathrm{d}x
      = \frac{1}{10}x^{10} + x^8 + 4x^6 + 8x^4 + 8x^2 + C
     \int \frac{1-x}{(x+1)^2} \,\mathrm{d}x.   \int \frac{1-x}{(x+1)^2} \,\mathrm{d}x
      = \frac{-2}{(x+1)} - \ln|x+1| + C.
     \int \frac{1-x}{(x+1)^2} \,\mathrm{d}x
      = \frac{x-1}{(x+1)} - \ln|x+1| + C.
     \int \frac {\tan(\pi x)\sec^2(\pi x)}2\,\mathrm{d}x.  u = \sec(\pi x)  \int \frac {\tan(\pi x)\sec^2(\pi x)}2\,\mathrm{d}x
      = \frac {\sec^2(\pi x)}{4\pi} + C.
    u = \tan(\pi x)  \int \frac {\tan(\pi x)\sec^2(\pi x)}2\,\mathrm{d}x
      = \frac {\tan^2(\pi x)}{4\pi} + C.
   ","['calculus', 'integration', 'indefinite-integrals', 'faq']"
72,Intriguing Indefinite Integral: $\int ( \frac{x^2-3x+1/3 }{x^3-x+1})^2 \mathrm{d}x$,Intriguing Indefinite Integral:,\int ( \frac{x^2-3x+1/3 }{x^3-x+1})^2 \mathrm{d}x,"Evaluate $$\int \left( \frac{x^2-3x+\frac{1}{3}}{x^3-x+1}\right)^2 \mathrm{d}x$$ I tried using partial fractions but the denominator doesn't factor out nicely. I also substituted $x=\dfrac{1}{t}$ to get $$\frac{-1}{9} \int \left(\frac{t^2-9t+3}{t^3-t^2+1}\right)^2 \, \mathrm{d}t $$ But I don't know how to solve this either. Please Help. Thanks in advance.",Evaluate I tried using partial fractions but the denominator doesn't factor out nicely. I also substituted to get But I don't know how to solve this either. Please Help. Thanks in advance.,"\int \left( \frac{x^2-3x+\frac{1}{3}}{x^3-x+1}\right)^2 \mathrm{d}x x=\dfrac{1}{t} \frac{-1}{9} \int \left(\frac{t^2-9t+3}{t^3-t^2+1}\right)^2 \, \mathrm{d}t ","['calculus', 'integration']"
73,Is there a continuous Taylor/MacLaurin transform (like the Fourier transform)?,Is there a continuous Taylor/MacLaurin transform (like the Fourier transform)?,,"My consideration might be total nonsense (as a high school student, I lack the mathematical knowledge to really check my idea), but I was just wondering whether one could find a continuous generalization of the Taylor series comparable to the continuous Fourier transform. Just like the Fourier transform decomposes a function $f(t)$ into a sum of trigonometric functions with a result $\hat{f}(\omega)=\mathcal{F}(f)(\omega)$ with the domain of frequencies , could one define a Taylor transform $\mathcal{T}(f)$ that would operate on $n$th derivatives? Instead of the trigonometric functions in Fourier transform, $f$ would be decomposed into a sum of polynomials of form $\frac{1}{n!}x^n$. So is it a valid operation to generalize the Taylor series for continuous values of $n$ (i.e. using integrals instead of sums)? My naive approach would be $$f(t) = \intop_0^{\infty} \mathcal{T}(f)(n)\cdot \frac{t^n}{\Gamma(n+1)} \mathrm{d}n$$ So in fact, the transform would define (or at least require) non-integer derivatives of a function. Do these exist (like a non-integer iterates of functions can be defined too)? Example: Assuming that $\frac{\mathrm{d}^ne^x}{\mathrm{d}x^n}(0) = 1$ for any $x\in \mathbb{R}$, I'd come up with $$e^t = \intop_0^{\infty} \frac{t^n}{\Gamma(n+1)} \mathrm{d}n$$ as a continuous version of $$e^t = \sum_{n=0}^{\infty} \frac{t^n}{n!}$$ Thus $$\mathcal{T}(e^t)(n) = 1,\,n\in\mathbb{R}$$ Is a transform like I explained possible or - in case it is - even somehow useful?","My consideration might be total nonsense (as a high school student, I lack the mathematical knowledge to really check my idea), but I was just wondering whether one could find a continuous generalization of the Taylor series comparable to the continuous Fourier transform. Just like the Fourier transform decomposes a function $f(t)$ into a sum of trigonometric functions with a result $\hat{f}(\omega)=\mathcal{F}(f)(\omega)$ with the domain of frequencies , could one define a Taylor transform $\mathcal{T}(f)$ that would operate on $n$th derivatives? Instead of the trigonometric functions in Fourier transform, $f$ would be decomposed into a sum of polynomials of form $\frac{1}{n!}x^n$. So is it a valid operation to generalize the Taylor series for continuous values of $n$ (i.e. using integrals instead of sums)? My naive approach would be $$f(t) = \intop_0^{\infty} \mathcal{T}(f)(n)\cdot \frac{t^n}{\Gamma(n+1)} \mathrm{d}n$$ So in fact, the transform would define (or at least require) non-integer derivatives of a function. Do these exist (like a non-integer iterates of functions can be defined too)? Example: Assuming that $\frac{\mathrm{d}^ne^x}{\mathrm{d}x^n}(0) = 1$ for any $x\in \mathbb{R}$, I'd come up with $$e^t = \intop_0^{\infty} \frac{t^n}{\Gamma(n+1)} \mathrm{d}n$$ as a continuous version of $$e^t = \sum_{n=0}^{\infty} \frac{t^n}{n!}$$ Thus $$\mathcal{T}(e^t)(n) = 1,\,n\in\mathbb{R}$$ Is a transform like I explained possible or - in case it is - even somehow useful?",,"['calculus', 'analysis']"
74,Is there an epsilon-delta definition of the second derivative?,Is there an epsilon-delta definition of the second derivative?,,"Is there an epsilon-delta definition for the second derivative? I know that there is such a definition for the first derivate $f'(x)$ which can be derived from the limit $f'(x) = \lim_{y\rightarrow x} \frac{f(y)-f(x)}{y-x}$ for a function $f:D\rightarrow \mathbb{R}$: $$\forall \epsilon > 0\, \exists \delta > 0\, \forall y \in D\setminus \{x\}:|y-x|<\delta \Rightarrow \left|\frac{f(y)-f(x)}{y-x}-f'(x)\right|<\epsilon$$ So $f'(x)$ can be described as the number which fulfills the above statement. Is there a similar statement for the second derivative? Update: This MSE thread shows that there are different definitions for the derivative (and thus for the second derivative). So I want to make my question more concrete: My definition of derivation: Let be $f:D\rightarrow\mathbb{R}$ with $D\subseteq\mathbb{R}$ arbitrary . Let $D^*$ be the set off all points $x\in D$ for which there is at least one sequence $(x_n)$ in $D\setminus\{x\}$ with $\lim_{n\rightarrow\infty} x_n=x$. I define the limit $\lim_{y\rightarrow x\ ,y\in D\setminus\{x\}} {f(y)-f(x) \over y-x}$ as the first derivation for a given $x\in D^*$ (if the limit exists). My definition of the second derivative: Let be $f:D\rightarrow\mathbb{R}$ with $D\subseteq\mathbb{R}$ arbitrary. We call $f''(x)$ the second derivative if there exists an open interval $x\in O\subseteq \mathbb{R}$ so that $f$ is differentiable on $O\cap D$ and $f''(x)$ is the first derivative of the function $f': (O\cap D)\rightarrow\mathbb{R}:x\mapsto f'(x)$ at the point $x$ (which also means that $x\in(O\cap D)^*$). My question: Is there a statement $\forall \epsilon > 0: \exists \delta > 0: A(\epsilon, \delta, f, x, c)$ for $f:D\rightarrow \mathbb{R}$ ($D\subseteq \mathbb{R}$) and $c,x\in\mathbb{R}$ which is equivalent to the statement that $f$ is differentiable on a set $x\in O\cap D$ where $O$ is an open interval and that $c$ is the second derivative of $f$ at $x$? I also will accept answers where you need more restrictions to the question. For example you might want to use the value of the first derivative $f'(x)$ (at the same point where you want to define the second derivative) in your statement or you want to restrict $f$ on functions with open domains or domains which are intervals. In this case I will accept your answer and open a new thread asking for a more general solution. Please notice that there is a community wiki post where I want to collect all the progress we made so far.","Is there an epsilon-delta definition for the second derivative? I know that there is such a definition for the first derivate $f'(x)$ which can be derived from the limit $f'(x) = \lim_{y\rightarrow x} \frac{f(y)-f(x)}{y-x}$ for a function $f:D\rightarrow \mathbb{R}$: $$\forall \epsilon > 0\, \exists \delta > 0\, \forall y \in D\setminus \{x\}:|y-x|<\delta \Rightarrow \left|\frac{f(y)-f(x)}{y-x}-f'(x)\right|<\epsilon$$ So $f'(x)$ can be described as the number which fulfills the above statement. Is there a similar statement for the second derivative? Update: This MSE thread shows that there are different definitions for the derivative (and thus for the second derivative). So I want to make my question more concrete: My definition of derivation: Let be $f:D\rightarrow\mathbb{R}$ with $D\subseteq\mathbb{R}$ arbitrary . Let $D^*$ be the set off all points $x\in D$ for which there is at least one sequence $(x_n)$ in $D\setminus\{x\}$ with $\lim_{n\rightarrow\infty} x_n=x$. I define the limit $\lim_{y\rightarrow x\ ,y\in D\setminus\{x\}} {f(y)-f(x) \over y-x}$ as the first derivation for a given $x\in D^*$ (if the limit exists). My definition of the second derivative: Let be $f:D\rightarrow\mathbb{R}$ with $D\subseteq\mathbb{R}$ arbitrary. We call $f''(x)$ the second derivative if there exists an open interval $x\in O\subseteq \mathbb{R}$ so that $f$ is differentiable on $O\cap D$ and $f''(x)$ is the first derivative of the function $f': (O\cap D)\rightarrow\mathbb{R}:x\mapsto f'(x)$ at the point $x$ (which also means that $x\in(O\cap D)^*$). My question: Is there a statement $\forall \epsilon > 0: \exists \delta > 0: A(\epsilon, \delta, f, x, c)$ for $f:D\rightarrow \mathbb{R}$ ($D\subseteq \mathbb{R}$) and $c,x\in\mathbb{R}$ which is equivalent to the statement that $f$ is differentiable on a set $x\in O\cap D$ where $O$ is an open interval and that $c$ is the second derivative of $f$ at $x$? I also will accept answers where you need more restrictions to the question. For example you might want to use the value of the first derivative $f'(x)$ (at the same point where you want to define the second derivative) in your statement or you want to restrict $f$ on functions with open domains or domains which are intervals. In this case I will accept your answer and open a new thread asking for a more general solution. Please notice that there is a community wiki post where I want to collect all the progress we made so far.",,['calculus']
75,Conjecture: $\sum\limits_{k=1}^nk^m=S_3(n)\times\frac{P_{m-3}(n)}{N_m}$ for odd $m>1 \ ;\ =S_2(n)\times\frac{P_{m-2}'(n)}{N_m}$ for even $m$.,Conjecture:  for odd  for even .,\sum\limits_{k=1}^nk^m=S_3(n)\times\frac{P_{m-3}(n)}{N_m} m>1 \ ;\ =S_2(n)\times\frac{P_{m-2}'(n)}{N_m} m,"When I was in high school, I was fascinated by $\displaystyle\sum\limits_{k=1}^n k= \frac{n(n+1)}{2}$ so I tried to find the general solution for $\displaystyle\sum\limits_{k=1}^n k^m$ s.t $m \in \mathbb{N}$ . I used a similar approach to my answer here My approach was turning $n^m$ to $n(n-1)(n-2)\dots (n-m+1)+ f(n)$ and to use the fact that $$\displaystyle\dbinom{n}{k}+\dbinom{n}{k+1}= \dbinom{n+1}{k+1}$$ $f(n) $ will be combination of sums $\displaystyle\sum_{k=1}^n k^i$ st $i<m$ which I evaluated before I was able to to find the sum up to $m=6$ .  Here, I tried to search for a pattern to find the general solution of $\sum\limits_{k=1}^n k^m $ s.t $m \in \mathbb{N}$ which I failed to do, but I noticed this pattern: $\\[3pt]$ $$S_1(n):={\sum\limits_{k=1}^n k= \frac{n(n+1)}{2}}$$ $$S_2(n):={\sum\limits_{k=1}^n k^2= \color{blue}{\frac{n(2n+1)(n+1)}{6}}}$$ $$S_3(n):={\sum\limits_{k=1}^n k^3= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}}}}$$ $$S_4(n):={\sum\limits_{k=1}^n k^4=\color{blue}{\frac{n(2n+1)(n+1)}{6}} \times \frac{3n^2+3n-1}{  5}}$$ $$S_5(n):={\sum\limits_{k=1}^n k^5= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}}} \times\frac{2n^2+2n-1}{ 3}}$$ $$S_6(n):={\sum\limits_{k=1}^n k^6= \color{blue}{\frac{n(2n+1)(n+1)}{6}} \times \frac{3n^4+6n^3-3n+1}{ 7 }}$$ $$S_7(n):={\sum\limits_{k=1}^n k^7= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}} }\times\frac{3n^4+6n^3-n^2-4n+2}{ 6}}$$ $$S_8(n):={\sum\limits_{k=1}^n k^8= \color{blue}{\frac{n(2n+1)(n+1)}{6}} \times \frac{5n^6+15n^5+5n^4-15n^3-n^2+9n+3}{ 15}}$$ $$S_9(n):={\sum\limits_{k=1}^n k^9= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}}} \times\frac{(n^2+n-1)(2n^4 +4n^3-n^3-3n^2+3)}{ 5}}$$ $$S_{10}(n):={\sum\limits_{k=1}^n k^{10}= \color{blue}{\frac{n(2n+1)(n+1)}{6}} \times \frac{ 3 n^8+ 12 n^7+ 8 n^6 - 18 n^5- 10 n^4+ 24 n^3   + 2 n^2 - 15 n +5}{ 11}}$$ $$S_{11}(n):={\sum\limits_{k=1}^n k^{11}= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}}} \times\frac{2n^8 +8n^7+4n^6-16n^5-5n^4+26n^3-3n^2-20n+10}{ 6}}$$ $\\[6pt]$ I noticed that: For odd $m>1$ , $\displaystyle\sum\limits_{k=1}^n k^m= \color{red}{\frac{{n^2(n+1)^2}}{{4}}} \times \frac{P_{m-3}(n)}{N_m}$ s.t $P_{m-3}(n)$ is an $m-3$ polynomial with integer coefficients $ \{a_{m-3},\dots a_1,a_0 \}$ such that $\gcd \{ a_{m-3},\dots a_1,a_0 \}=1$ , $N_m\in \mathbb {N}$ . For even $m$ , $\displaystyle\sum\limits_{k=1}^n k^m= \color{blue}{\frac{n(2n+1)(n+1)}{6}}\times \frac{P_{m-2}'(n)}{N_m}$ s.t $P_{m-2}'(n)$ is an $m-2$ polynomial with integer coefficients $\{ a_{m-2},\dots a_1,a_0 \}$ such that $\gcd \{ a_{m-2},\dots a_1,a_0 \}=1$ , $N_m\in \mathbb {N}$ . When I was in high school I couldn't prove this pattern, and this question reminded me of this observation that I had totally forgotten about. Now, after two years from my first attempt, I tried to prove this pattern again, but I couldn't. EDIT This question has been partly answered in this MathOverflow answer (The answer shows that $\displaystyle\sum\limits_{k=1}^n k^m$ is divisible by ${n^2(n+1)^2}$ for odd $m>1$ and $\displaystyle\sum\limits_{k=1}^n k^m$ is divisible by ${n(2n+1)(n+1)}$ for even $m$ ) the only missing part is to show that the denominator is a multiple of $4$ if $m\in 2\mathbb{N}+1$ , and the denominator is a multiple of $6$ if $m \in 2\mathbb{N}$ . Although this question has been almost solved before, I still would like to see if there is any other proofs or methods. Update: I asked this question on MO here","When I was in high school, I was fascinated by so I tried to find the general solution for s.t . I used a similar approach to my answer here My approach was turning to and to use the fact that will be combination of sums st which I evaluated before I was able to to find the sum up to .  Here, I tried to search for a pattern to find the general solution of s.t which I failed to do, but I noticed this pattern: I noticed that: For odd , s.t is an polynomial with integer coefficients such that , . For even , s.t is an polynomial with integer coefficients such that , . When I was in high school I couldn't prove this pattern, and this question reminded me of this observation that I had totally forgotten about. Now, after two years from my first attempt, I tried to prove this pattern again, but I couldn't. EDIT This question has been partly answered in this MathOverflow answer (The answer shows that is divisible by for odd and is divisible by for even ) the only missing part is to show that the denominator is a multiple of if , and the denominator is a multiple of if . Although this question has been almost solved before, I still would like to see if there is any other proofs or methods. Update: I asked this question on MO here","\displaystyle\sum\limits_{k=1}^n k= \frac{n(n+1)}{2} \displaystyle\sum\limits_{k=1}^n k^m m \in \mathbb{N} n^m n(n-1)(n-2)\dots (n-m+1)+ f(n) \displaystyle\dbinom{n}{k}+\dbinom{n}{k+1}= \dbinom{n+1}{k+1} f(n)  \displaystyle\sum_{k=1}^n k^i i<m m=6 \sum\limits_{k=1}^n k^m  m \in \mathbb{N} \\[3pt] S_1(n):={\sum\limits_{k=1}^n k= \frac{n(n+1)}{2}} S_2(n):={\sum\limits_{k=1}^n k^2= \color{blue}{\frac{n(2n+1)(n+1)}{6}}} S_3(n):={\sum\limits_{k=1}^n k^3= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}}}} S_4(n):={\sum\limits_{k=1}^n k^4=\color{blue}{\frac{n(2n+1)(n+1)}{6}} \times \frac{3n^2+3n-1}{  5}} S_5(n):={\sum\limits_{k=1}^n k^5= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}}} \times\frac{2n^2+2n-1}{ 3}} S_6(n):={\sum\limits_{k=1}^n k^6= \color{blue}{\frac{n(2n+1)(n+1)}{6}} \times \frac{3n^4+6n^3-3n+1}{ 7 }} S_7(n):={\sum\limits_{k=1}^n k^7= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}} }\times\frac{3n^4+6n^3-n^2-4n+2}{ 6}} S_8(n):={\sum\limits_{k=1}^n k^8= \color{blue}{\frac{n(2n+1)(n+1)}{6}} \times \frac{5n^6+15n^5+5n^4-15n^3-n^2+9n+3}{ 15}} S_9(n):={\sum\limits_{k=1}^n k^9= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}}} \times\frac{(n^2+n-1)(2n^4 +4n^3-n^3-3n^2+3)}{ 5}} S_{10}(n):={\sum\limits_{k=1}^n k^{10}= \color{blue}{\frac{n(2n+1)(n+1)}{6}} \times \frac{ 3 n^8+ 12 n^7+ 8 n^6 - 18 n^5- 10 n^4+ 24 n^3   + 2 n^2 - 15 n +5}{ 11}} S_{11}(n):={\sum\limits_{k=1}^n k^{11}= \color{red}{\frac{\color{red}{n^2(n+1)^2}}{\color{red}{4}}} \times\frac{2n^8 +8n^7+4n^6-16n^5-5n^4+26n^3-3n^2-20n+10}{ 6}} \\[6pt] m>1 \displaystyle\sum\limits_{k=1}^n k^m= \color{red}{\frac{{n^2(n+1)^2}}{{4}}} \times \frac{P_{m-3}(n)}{N_m} P_{m-3}(n) m-3  \{a_{m-3},\dots a_1,a_0 \} \gcd \{ a_{m-3},\dots a_1,a_0 \}=1 N_m\in \mathbb {N} m \displaystyle\sum\limits_{k=1}^n k^m= \color{blue}{\frac{n(2n+1)(n+1)}{6}}\times \frac{P_{m-2}'(n)}{N_m} P_{m-2}'(n) m-2 \{ a_{m-2},\dots a_1,a_0 \} \gcd \{ a_{m-2},\dots a_1,a_0 \}=1 N_m\in \mathbb {N} \displaystyle\sum\limits_{k=1}^n k^m {n^2(n+1)^2} m>1 \displaystyle\sum\limits_{k=1}^n k^m {n(2n+1)(n+1)} m 4 m\in 2\mathbb{N}+1 6 m \in 2\mathbb{N}","['calculus', 'sequences-and-series', 'number-theory', 'summation', 'conjectures']"
76,How to prove $ \lim_{n \to \infty} e^n \cdot \left( \sum_{k=0}^{n-1} ({k-n \over e})^k/k! \right)- 2 \cdot n = \frac 23$?,How to prove ?, \lim_{n \to \infty} e^n \cdot \left( \sum_{k=0}^{n-1} ({k-n \over e})^k/k! \right)- 2 \cdot n = \frac 23,"I observed for the function  $$ f(n)= e^n \sum_{k=0}^{n-1}\left(\dfrac{k - n}{e}\right)^k \cdot \dfrac{1}{k!} \tag 1$$ with small $n$ that n  sum  -------------   1  2.7182818   2  4.6707743   3  6.6665656   4  8.6666045   5  10.666662   6  12.666667   7  14.666667   8  16.666667 So an obvious hypothesis is $$ \lim_{n \to \infty} \bigl(f(n)-2n\bigr) = \frac 23 \tag 2$$ However, I have no idea, how to prove this but would like to understand how I can approach such a proof (I'll have then some similar ones with likely the same or related logic) So I would like to understand ... Q: how I could prove that assumed limit (2).","I observed for the function  $$ f(n)= e^n \sum_{k=0}^{n-1}\left(\dfrac{k - n}{e}\right)^k \cdot \dfrac{1}{k!} \tag 1$$ with small $n$ that n  sum  -------------   1  2.7182818   2  4.6707743   3  6.6665656   4  8.6666045   5  10.666662   6  12.666667   7  14.666667   8  16.666667 So an obvious hypothesis is $$ \lim_{n \to \infty} \bigl(f(n)-2n\bigr) = \frac 23 \tag 2$$ However, I have no idea, how to prove this but would like to understand how I can approach such a proof (I'll have then some similar ones with likely the same or related logic) So I would like to understand ... Q: how I could prove that assumed limit (2).",,"['calculus', 'sequences-and-series', 'exponential-function']"
77,Shortcut/trick for integrating a factored polynomial?,Shortcut/trick for integrating a factored polynomial?,,"If I'm integrating a factored polynomial, say $$\int{x(x+1)(x-2)(x+3)dx},$$ does some shortcut exist that keeps me from having to expand the polynomial? Currently, I'd just do all the multiplication which can get tedious with a higher order polynomial, and then integrate it once I had it in $ax^n$ form. I seem to remember learning some trick that helped here, and I saw this done very quickly in person a few weeks ago. I've looked on the web and text books but I haven't found anything. Anyone know if such a thing exists?","If I'm integrating a factored polynomial, say $$\int{x(x+1)(x-2)(x+3)dx},$$ does some shortcut exist that keeps me from having to expand the polynomial? Currently, I'd just do all the multiplication which can get tedious with a higher order polynomial, and then integrate it once I had it in $ax^n$ form. I seem to remember learning some trick that helped here, and I saw this done very quickly in person a few weeks ago. I've looked on the web and text books but I haven't found anything. Anyone know if such a thing exists?",,"['calculus', 'polynomials', 'integration']"
78,Cleverly showing that $\lim_{x\to 0}\frac{x^{(\sin x)^x}-(\sin x)^{x^{\sin x}}}{x^3}=\frac{1}{6}$,Cleverly showing that,\lim_{x\to 0}\frac{x^{(\sin x)^x}-(\sin x)^{x^{\sin x}}}{x^3}=\frac{1}{6},"$$\lim_{x\to 0}\frac{\textstyle x^{\textstyle(\sin  x)^{\textstyle x}}-(\textstyle \sin  x)^{\textstyle x^{\textstyle \sin  x}}}{\textstyle x^3}=\frac{1}{6}$$ The limit is easy to get results, but how to rigorously prove it without using Taylor formula? At first, I guess the numerator is equivalent to $x-\sin x$ And I also find the following several limits have the same results: $$\lim_{x\to 0}\frac{\textstyle x^{\textstyle x^{\textstyle x}}-(\textstyle \sin  x)^{\textstyle (\sin x)^{\textstyle \sin  x}}}{\textstyle x^3}=\frac{1}{6}$$ $$\lim_{x\to 0}\frac{\textstyle x^{\textstyle x^{\textstyle \tan x}}-(\textstyle \sin  x)^{\textstyle x^{\textstyle \tan  x}}}{\textstyle x^3}=\frac{1}{6}$$ I guess if $f(x)\sim g(x)\sim h(x)\sim O(x)^k$ when $x\to 0$ , then $$\lim_{x\to 0}\textstyle f(x)^{\textstyle g(x)^{\textstyle h(x)}}=f(x)$$","The limit is easy to get results, but how to rigorously prove it without using Taylor formula? At first, I guess the numerator is equivalent to And I also find the following several limits have the same results: I guess if when , then",\lim_{x\to 0}\frac{\textstyle x^{\textstyle(\sin  x)^{\textstyle x}}-(\textstyle \sin  x)^{\textstyle x^{\textstyle \sin  x}}}{\textstyle x^3}=\frac{1}{6} x-\sin x \lim_{x\to 0}\frac{\textstyle x^{\textstyle x^{\textstyle x}}-(\textstyle \sin  x)^{\textstyle (\sin x)^{\textstyle \sin  x}}}{\textstyle x^3}=\frac{1}{6} \lim_{x\to 0}\frac{\textstyle x^{\textstyle x^{\textstyle \tan x}}-(\textstyle \sin  x)^{\textstyle x^{\textstyle \tan  x}}}{\textstyle x^3}=\frac{1}{6} f(x)\sim g(x)\sim h(x)\sim O(x)^k x\to 0 \lim_{x\to 0}\textstyle f(x)^{\textstyle g(x)^{\textstyle h(x)}}=f(x),"['calculus', 'limits', 'exponentiation']"
79,Asymptotic expression of an oscillatory integral,Asymptotic expression of an oscillatory integral,,"Consider the integral $$ f(\alpha,\beta)= \int_0^{2\pi}\,dx \sqrt{1- \cos(\alpha x ) \cos(\beta x)}$$  as a function of the two parameters $\alpha,\beta$. I am interested in the asymptotic behavior for $\alpha, \beta \gg 1$. For $\alpha = \beta$ the integral can be evaluated explicitly with the result $$ f(\alpha , \alpha) = \frac{2}{\alpha} \left[ \lfloor 2 \alpha\rfloor  + \sin^2 \left(\frac\pi2 \{ 2 \alpha \}  \right) \right]$$ with $\{ x \} = x - \lfloor x \rfloor$. For large $\alpha$ the function $f(\alpha, \alpha)$ thus approaches $4$. If we see what happens if we keep $\beta$ large but fixed and vary $\alpha$, we see that $\beta \approx \alpha$ with $f(\alpha,\beta) \approx 4$ looks like a minimum of the function and it quickly approaches $2\pi$ (at least for $\alpha$ large) for $\beta$ sufficiently different from $\alpha$. However, there are oscillations on top of the mean value $2\pi$. In the figure you see a numerical evaluation of the integral for $\beta=20$ and $\alpha$ between 0 and 40. Is the value of $f(\alpha,\beta)$ for $\alpha,\beta \gg 1$ and $\alpha \neq \beta$ indeed $2\pi$? Why the value $f=4$ for $\alpha \approx \beta$ is lower than the generic value $2\pi$ (for $\alpha$, $\beta$ large)? What is the period of the (fast) oscillations as a function of $\alpha$ with $\beta$ fixed which are visible in the plot? What is the shape of the envelope? (it is a peaked function -> Lorentzian, or Gaussian, or ...?) Does anybody know how to obtain a good asymptotic expression for $f(\alpha, \beta)$?","Consider the integral $$ f(\alpha,\beta)= \int_0^{2\pi}\,dx \sqrt{1- \cos(\alpha x ) \cos(\beta x)}$$  as a function of the two parameters $\alpha,\beta$. I am interested in the asymptotic behavior for $\alpha, \beta \gg 1$. For $\alpha = \beta$ the integral can be evaluated explicitly with the result $$ f(\alpha , \alpha) = \frac{2}{\alpha} \left[ \lfloor 2 \alpha\rfloor  + \sin^2 \left(\frac\pi2 \{ 2 \alpha \}  \right) \right]$$ with $\{ x \} = x - \lfloor x \rfloor$. For large $\alpha$ the function $f(\alpha, \alpha)$ thus approaches $4$. If we see what happens if we keep $\beta$ large but fixed and vary $\alpha$, we see that $\beta \approx \alpha$ with $f(\alpha,\beta) \approx 4$ looks like a minimum of the function and it quickly approaches $2\pi$ (at least for $\alpha$ large) for $\beta$ sufficiently different from $\alpha$. However, there are oscillations on top of the mean value $2\pi$. In the figure you see a numerical evaluation of the integral for $\beta=20$ and $\alpha$ between 0 and 40. Is the value of $f(\alpha,\beta)$ for $\alpha,\beta \gg 1$ and $\alpha \neq \beta$ indeed $2\pi$? Why the value $f=4$ for $\alpha \approx \beta$ is lower than the generic value $2\pi$ (for $\alpha$, $\beta$ large)? What is the period of the (fast) oscillations as a function of $\alpha$ with $\beta$ fixed which are visible in the plot? What is the shape of the envelope? (it is a peaked function -> Lorentzian, or Gaussian, or ...?) Does anybody know how to obtain a good asymptotic expression for $f(\alpha, \beta)$?",,"['calculus', 'asymptotics', 'definite-integrals']"
80,"Find $\lim\limits_{n \to \infty} \int_0^1 f_n(x) \, dx$ with $f_0(x) = x$ and $f_{n+1}(x) = \sin (\pi f_n(x))$",Find  with  and,"\lim\limits_{n \to \infty} \int_0^1 f_n(x) \, dx f_0(x) = x f_{n+1}(x) = \sin (\pi f_n(x))","Let $I = [0, 1]$ and consider the functions $f_n \colon I \to I$ defined by $$f_0(x) = x \qquad f_{n+1}(x) = \sin(\pi f_n(x))$$ The functions exhibit an oscillating behavior. For example, these are the graphs of $\color{red}{f_1}, \color{green}{f_2}, \color{blue}{f_3}$ : If we compute the definite integral of $f_n$ for $n \ge 1$ we find the following: $$\begin{array}{| c | c |} \hline n & \int_0^1 f_n(x)\, dx \\ \hline 1 & 0.63662 \\ 2 & 0.517825 \\ 3 & 0.483655 \\ 4 & 0.472943 \\ 5 & 0.469547 \\ \hline \end{array}$$ So my question is: What is $\displaystyle\lim_{n \to \infty} \int_0^1 f_n(x) \, dx$ ? Actually, I haven't been able to prove that the sequence is decreasing, so I'm not even sure the limit exists. For any $n$ , I can prove that the interval $I$ can be divided into $2^n$ intervals $$I_1 = [a_0, a_1],\quad I_2 = [a_1, a_2],\quad \dotsc,\quad I_{2^n} = [a_{2^n-1}, a_{2^n}]$$ such that: $f_n(a_k) = 1$ if $k$ is odd and $f_n(a_k) = 0$ if $k$ is even; $f_n$ is monotonic in each $I_k$ , increasing if $k$ is odd and decreasing if $k$ is even. Moreover, the equation $\sin (\pi x) = x$ has exactly one positive solution $\alpha \approx 0.736$ . Hence, for any $n$ , if $x > 0$ and $f_{n+1}(x) = f_n(x)$ then $f_n(x) = \alpha$ and more generally $f_m(x) = \alpha$ for any $m \ge n$ . But I don't know if this helps.","Let and consider the functions defined by The functions exhibit an oscillating behavior. For example, these are the graphs of : If we compute the definite integral of for we find the following: So my question is: What is ? Actually, I haven't been able to prove that the sequence is decreasing, so I'm not even sure the limit exists. For any , I can prove that the interval can be divided into intervals such that: if is odd and if is even; is monotonic in each , increasing if is odd and decreasing if is even. Moreover, the equation has exactly one positive solution . Hence, for any , if and then and more generally for any . But I don't know if this helps.","I = [0, 1] f_n \colon I \to I f_0(x) = x \qquad f_{n+1}(x) = \sin(\pi f_n(x)) \color{red}{f_1}, \color{green}{f_2}, \color{blue}{f_3} f_n n \ge 1 \begin{array}{| c | c |}
\hline
n & \int_0^1 f_n(x)\, dx \\
\hline
1 & 0.63662 \\
2 & 0.517825 \\
3 & 0.483655 \\
4 & 0.472943 \\
5 & 0.469547 \\
\hline
\end{array} \displaystyle\lim_{n \to \infty} \int_0^1 f_n(x) \, dx n I 2^n I_1 = [a_0, a_1],\quad I_2 = [a_1, a_2],\quad \dotsc,\quad I_{2^n} = [a_{2^n-1}, a_{2^n}] f_n(a_k) = 1 k f_n(a_k) = 0 k f_n I_k k k \sin (\pi x) = x \alpha \approx 0.736 n x > 0 f_{n+1}(x) = f_n(x) f_n(x) = \alpha f_m(x) = \alpha m \ge n","['calculus', 'sequences-and-series', 'limits', 'definite-integrals']"
81,Proving that $\int_0^\infty\Big(\sqrt[n]{1+x^n}-x\Big)dx~=~\frac12\cdot{-1/n\choose+1/n}^{-1}$,Proving that,\int_0^\infty\Big(\sqrt[n]{1+x^n}-x\Big)dx~=~\frac12\cdot{-1/n\choose+1/n}^{-1},"How can we prove, without employing the aid of residues or various transforms, that, for $n>2$ $$\int_0^\infty\Big(\sqrt[n]{1+x^n}-x\Big)dx~=~\frac12\cdot{-1/n\choose+1/n}^{-1}$$ Motivation: In my previous question , thanks to Will Jagy's simple but brilliant answer, I was able to express the area of the superellipse $x^n+y^n=r^n$, for odd values of $n=2k+1$, with $k\in\mathbb N^*$, as $A_n=r^2\displaystyle\cdot{2/n\choose1/n}^{-1}+r^2\cdot{-1/n\choose+1/n}^{-1}$, where the first term, representing the surface inside the first sector or quadrant, comes from a simple evaluation of the beta function . My fruitless efforts and endeavors: $\big(1\big).$ Breaking up the integration interval into $(0,1)$ and $(1,\infty)$, expanding the integrand into an appropriate binomial series for each of the two cases, and then switching the order of summation and integration. In the end, this made me rewrite the initial question in terms of an infinite series, but led me no closer to finding an answer: $$2a\sum_{k=1}^\infty{a\choose k}\Bigg(\frac1{k+a}+\frac1{k-2a}\Bigg)={-a\choose+a}^{-1}-1.$$ $\big(2\big).$ Letting $x=\sqrt[n]{\sinh^2t~}$. Needless to say, that did not get me very far either. $\big(3\big).$ Letting $x^n=u$, we are able to obtain an expression in terms of incomplete beta functions .","How can we prove, without employing the aid of residues or various transforms, that, for $n>2$ $$\int_0^\infty\Big(\sqrt[n]{1+x^n}-x\Big)dx~=~\frac12\cdot{-1/n\choose+1/n}^{-1}$$ Motivation: In my previous question , thanks to Will Jagy's simple but brilliant answer, I was able to express the area of the superellipse $x^n+y^n=r^n$, for odd values of $n=2k+1$, with $k\in\mathbb N^*$, as $A_n=r^2\displaystyle\cdot{2/n\choose1/n}^{-1}+r^2\cdot{-1/n\choose+1/n}^{-1}$, where the first term, representing the surface inside the first sector or quadrant, comes from a simple evaluation of the beta function . My fruitless efforts and endeavors: $\big(1\big).$ Breaking up the integration interval into $(0,1)$ and $(1,\infty)$, expanding the integrand into an appropriate binomial series for each of the two cases, and then switching the order of summation and integration. In the end, this made me rewrite the initial question in terms of an infinite series, but led me no closer to finding an answer: $$2a\sum_{k=1}^\infty{a\choose k}\Bigg(\frac1{k+a}+\frac1{k-2a}\Bigg)={-a\choose+a}^{-1}-1.$$ $\big(2\big).$ Letting $x=\sqrt[n]{\sinh^2t~}$. Needless to say, that did not get me very far either. $\big(3\big).$ Letting $x^n=u$, we are able to obtain an expression in terms of incomplete beta functions .",,"['calculus', 'integration', 'definite-integrals', 'closed-form', 'beta-function']"
82,Delta function integrated from zero,Delta function integrated from zero,,"I am trying to understand the motivation behind the following identity stated in Bracewell's book on Fourier transforms: $$\delta^{(2)}(x,y)=\frac{\delta(r)}{\pi r},$$ where $\delta^{(2)}$ is a 2-dimensional delta function.  Starting with something we know to be true, we can do $$1 = \iint \delta^{(2)}(x,y) dx\,dy = \int_0^\infty \int_0^{2\pi} \frac{\delta(r)}{\pi r} r\,dr\,d\theta = 2 \int_0^\infty \delta(r).$$ This suggests that the integral of delta function from 0 to infinity is 1/2.  In fact, this seems to make sense if we treat the delta function as a limiting case of an even function peaked at zero (Gaussian, sinc, etc.)  However, Wikipedia, citing Bracewell, claims the following to be true: $$\int_0^\infty \delta(r-a) e^{-s r} dr = e^{-s a},$$ and plugging in 0 for a and s we get $$\int_0^\infty \delta(r) dr = 1.$$ What is going on here?..  Where's the screw-up?..  If the integral from 0 to infinity is not 1/2, then how do we justify the above polar-coordinate expression for a 2D delta function?..","I am trying to understand the motivation behind the following identity stated in Bracewell's book on Fourier transforms: $$\delta^{(2)}(x,y)=\frac{\delta(r)}{\pi r},$$ where $\delta^{(2)}$ is a 2-dimensional delta function.  Starting with something we know to be true, we can do $$1 = \iint \delta^{(2)}(x,y) dx\,dy = \int_0^\infty \int_0^{2\pi} \frac{\delta(r)}{\pi r} r\,dr\,d\theta = 2 \int_0^\infty \delta(r).$$ This suggests that the integral of delta function from 0 to infinity is 1/2.  In fact, this seems to make sense if we treat the delta function as a limiting case of an even function peaked at zero (Gaussian, sinc, etc.)  However, Wikipedia, citing Bracewell, claims the following to be true: $$\int_0^\infty \delta(r-a) e^{-s r} dr = e^{-s a},$$ and plugging in 0 for a and s we get $$\int_0^\infty \delta(r) dr = 1.$$ What is going on here?..  Where's the screw-up?..  If the integral from 0 to infinity is not 1/2, then how do we justify the above polar-coordinate expression for a 2D delta function?..",,"['calculus', 'fourier-analysis', 'distribution-theory']"
83,When is ${\large\int}\frac{dx}{\left(1+x^a\right)^a}$ an elementary function?,When is  an elementary function?,{\large\int}\frac{dx}{\left(1+x^a\right)^a},"Consider the following indefinite integral: $$F_a(x)=\int\frac{dx}{\left(1+x^a\right)^a}.$$ Here $a\in\mathbb R$ is a parameter, and $x>0$ is a variable. For what values of the parameter $a$ the function $F_a(x)$ is an elementary function of the variable $x$? Here are some examples (in each case there is implicitly an additive constant of integration). $$F_1(x)=\int\frac{dx}{\left(1+x\right)}=\ln(1+x)$$ $$F_{-1}(x)=\int\frac{dx}{\left(1+x^{-1}\right)^{-1}}=x+\ln x$$ $$F_2(x)=\int\frac{dx}{\left(1+x^2\right)^2}=\frac12\left(\frac{x}{x^2+1}+\arctan(x)\right)$$ $$F_{1/2}(x)=\int\frac{dx}{\left(1+x^{1/2}\right)^{1/2}}=\frac43\left(x^{1/2}-2\right) \left(1+x^{1/2}\right)^{1/2}$$ $$F_{1/3}(x)=\int\frac{dx}{\left(1+x^{1/3}\right)^{1/3}}=\frac9{40}\left(9-6x^{1/3}+5x^{2/3}\right)\left(1+x^{1/3}\right)^{2/3}$$ $$F_{1+\sqrt2}(x)={\large\int}\frac{dx}{\left(1+x^{1+\sqrt2}\right)^{1+\sqrt2}}=\left(x+\frac{x^{2+\sqrt2}}{\sqrt2}\right)\left(1+x^{1+\sqrt2}\right)^{-\sqrt2}$$ $$F_{1-\sqrt2}(x)={\large\int}\frac{dx}{\left(1+x^{1-\sqrt2}\right)^{1-\sqrt2}}=\left(x-\frac{x^{2-\sqrt{2}}}{\sqrt{2}}\right) \left(1+x^{1-\sqrt{2}}\right)^{\sqrt{2}}$$ $$F_{(1+\sqrt5)/2}(x)={\large\int}\frac{dx}{\left(1+x^{(1+\sqrt5)/2}\right)^{(1+\sqrt5)/2}}=x \left(1+x^{(1+\sqrt5)/2}\right)^{(1-\sqrt5)/2}$$ $$F_{(1-\sqrt5)/2}(x)={\large\int}\frac{dx}{\left(1+x^{(1-\sqrt5)/2}\right)^{(1-\sqrt5)/2}}=x\left(1+x^{(1-\sqrt{5})/2}\right)^{(1+\sqrt{5})/2}$$ All these results can be easily verified by differentiation. For the general case, Mathematica gives the result in terms of the hypergeometric function: $$F_a(x)=\int\frac{dx}{\left(1+x^a\right)^a}=x\cdot{_2F_1}\left(\frac1a,a;1+\frac1a;-x^a\right).$$ For what values of the parameter $a$ does this hypergeometric function reduce to an elementary function?","Consider the following indefinite integral: $$F_a(x)=\int\frac{dx}{\left(1+x^a\right)^a}.$$ Here $a\in\mathbb R$ is a parameter, and $x>0$ is a variable. For what values of the parameter $a$ the function $F_a(x)$ is an elementary function of the variable $x$? Here are some examples (in each case there is implicitly an additive constant of integration). $$F_1(x)=\int\frac{dx}{\left(1+x\right)}=\ln(1+x)$$ $$F_{-1}(x)=\int\frac{dx}{\left(1+x^{-1}\right)^{-1}}=x+\ln x$$ $$F_2(x)=\int\frac{dx}{\left(1+x^2\right)^2}=\frac12\left(\frac{x}{x^2+1}+\arctan(x)\right)$$ $$F_{1/2}(x)=\int\frac{dx}{\left(1+x^{1/2}\right)^{1/2}}=\frac43\left(x^{1/2}-2\right) \left(1+x^{1/2}\right)^{1/2}$$ $$F_{1/3}(x)=\int\frac{dx}{\left(1+x^{1/3}\right)^{1/3}}=\frac9{40}\left(9-6x^{1/3}+5x^{2/3}\right)\left(1+x^{1/3}\right)^{2/3}$$ $$F_{1+\sqrt2}(x)={\large\int}\frac{dx}{\left(1+x^{1+\sqrt2}\right)^{1+\sqrt2}}=\left(x+\frac{x^{2+\sqrt2}}{\sqrt2}\right)\left(1+x^{1+\sqrt2}\right)^{-\sqrt2}$$ $$F_{1-\sqrt2}(x)={\large\int}\frac{dx}{\left(1+x^{1-\sqrt2}\right)^{1-\sqrt2}}=\left(x-\frac{x^{2-\sqrt{2}}}{\sqrt{2}}\right) \left(1+x^{1-\sqrt{2}}\right)^{\sqrt{2}}$$ $$F_{(1+\sqrt5)/2}(x)={\large\int}\frac{dx}{\left(1+x^{(1+\sqrt5)/2}\right)^{(1+\sqrt5)/2}}=x \left(1+x^{(1+\sqrt5)/2}\right)^{(1-\sqrt5)/2}$$ $$F_{(1-\sqrt5)/2}(x)={\large\int}\frac{dx}{\left(1+x^{(1-\sqrt5)/2}\right)^{(1-\sqrt5)/2}}=x\left(1+x^{(1-\sqrt{5})/2}\right)^{(1+\sqrt{5})/2}$$ All these results can be easily verified by differentiation. For the general case, Mathematica gives the result in terms of the hypergeometric function: $$F_a(x)=\int\frac{dx}{\left(1+x^a\right)^a}=x\cdot{_2F_1}\left(\frac1a,a;1+\frac1a;-x^a\right).$$ For what values of the parameter $a$ does this hypergeometric function reduce to an elementary function?",,"['calculus', 'integration', 'indefinite-integrals', 'hypergeometric-function', 'elementary-functions']"
84,Is $\sum_{k=1}^{n} \sin(k^2)$ bounded by a constant $M$?,Is  bounded by a constant ?,\sum_{k=1}^{n} \sin(k^2) M,I know $\sum_{k=1}^{n} \sin(k)$ is bounded by a constant . How about $\sum_{k=1}^{n} \sin(k^2)$?,I know $\sum_{k=1}^{n} \sin(k)$ is bounded by a constant . How about $\sum_{k=1}^{n} \sin(k^2)$?,,"['calculus', 'sequences-and-series']"
85,Using Fourier Series to compute sums,Using Fourier Series to compute sums,,"I have just started learning the basics of Fourier series and have some doubts about it. I am aware that Fourier series can be used to compute infinite sums. For example, $\zeta(2)$ and $\eta(2)$ can be evaluated by using the Fourier series expansion of $x^2$, where $x\in[-\pi, \pi]$. $$x^2=\frac{\pi^2}{3}+\sum_{n \ge 1}\frac{4(-1)^n}{n^2}\cos{nx}$$ Letting $x=\pi$ and $x=0$ will yield the required results. This then brings me to my question. Given a sum to compute, how does one determine the appropriate $f(x)$ and $L$? For example, given a sum like $$\beta(3)=\sum_{n \ge 0}\frac{(-1)^n}{(2n+1)^3}$$ , may I ask how we are supposed to know which function we have to consider? Also, I am interested in knowing how to apply this technique to evaluate sums of the more general form $$\sum_{n \ge 0}\frac{z^n}{(n+a)^s}$$ i.e. the lerch transcendent, and how to determine if it is not possible to utilise this method. (For example, it does not work on $\zeta(2n+1)$) Thank you for putting up with my ignorance. Help will be greatly appreciated.","I have just started learning the basics of Fourier series and have some doubts about it. I am aware that Fourier series can be used to compute infinite sums. For example, $\zeta(2)$ and $\eta(2)$ can be evaluated by using the Fourier series expansion of $x^2$, where $x\in[-\pi, \pi]$. $$x^2=\frac{\pi^2}{3}+\sum_{n \ge 1}\frac{4(-1)^n}{n^2}\cos{nx}$$ Letting $x=\pi$ and $x=0$ will yield the required results. This then brings me to my question. Given a sum to compute, how does one determine the appropriate $f(x)$ and $L$? For example, given a sum like $$\beta(3)=\sum_{n \ge 0}\frac{(-1)^n}{(2n+1)^3}$$ , may I ask how we are supposed to know which function we have to consider? Also, I am interested in knowing how to apply this technique to evaluate sums of the more general form $$\sum_{n \ge 0}\frac{z^n}{(n+a)^s}$$ i.e. the lerch transcendent, and how to determine if it is not possible to utilise this method. (For example, it does not work on $\zeta(2n+1)$) Thank you for putting up with my ignorance. Help will be greatly appreciated.",,"['calculus', 'sequences-and-series', 'fourier-analysis', 'summation', 'fourier-series']"
86,Prove that $\int_0^x f^3 \le \left(\int_0^x f\right)^2$,Prove that,\int_0^x f^3 \le \left(\int_0^x f\right)^2,"This problem comes from Calculus by Spivak, namely in Chapter 14- ""The Fundamental Theorem of Calculus"". Suppose that $f$ is a differentiable function with $f(0)=0$ and $0<f'\le1$. Prove that for all $x\ge0$ we have $$ \int_0^x f^3 \le \left(\int_0^x f\right)^2. $$ Now, I have a (proposed) solution, so my question is whether the following is correct. We know that both the l.h.s. and r.h.s. of the inequality begin at 0, so we can prove the inequality by showing that the same inequality holds for the derivatives of each side. (If both begin at the same value and one increases more quickly or at the same rate than the other, then that one will also take on a value greater than or equal to the other for all $x\ge0$.) So, we have $$f^3 \le 2f\int_0^x f. $$ If $f=0$ the inequality is clearly satisfied, otherwise we have that $$f^2 \le 2\int_0^x f. $$ We then apply the same logic as before, showing that this inequality holds after differentiating (since, again, both expressions evaluate to 0 when $x=0$). So, we have that $$2ff' \le 2f.$$ We have already taken care of the case that $f=0$ (and the inequality holds anyway for $f=0$) so we end up with $$f' \le 1.$$ This is given, so the first inequality is proven. I sort of feel (for no particular reason) like part of this may be incorrect, which is why I'm asking here. So if any part (or the whole thing) is incorrect, could someone please point to the mistake? If not, great. Thanks.","This problem comes from Calculus by Spivak, namely in Chapter 14- ""The Fundamental Theorem of Calculus"". Suppose that $f$ is a differentiable function with $f(0)=0$ and $0<f'\le1$. Prove that for all $x\ge0$ we have $$ \int_0^x f^3 \le \left(\int_0^x f\right)^2. $$ Now, I have a (proposed) solution, so my question is whether the following is correct. We know that both the l.h.s. and r.h.s. of the inequality begin at 0, so we can prove the inequality by showing that the same inequality holds for the derivatives of each side. (If both begin at the same value and one increases more quickly or at the same rate than the other, then that one will also take on a value greater than or equal to the other for all $x\ge0$.) So, we have $$f^3 \le 2f\int_0^x f. $$ If $f=0$ the inequality is clearly satisfied, otherwise we have that $$f^2 \le 2\int_0^x f. $$ We then apply the same logic as before, showing that this inequality holds after differentiating (since, again, both expressions evaluate to 0 when $x=0$). So, we have that $$2ff' \le 2f.$$ We have already taken care of the case that $f=0$ (and the inequality holds anyway for $f=0$) so we end up with $$f' \le 1.$$ This is given, so the first inequality is proven. I sort of feel (for no particular reason) like part of this may be incorrect, which is why I'm asking here. So if any part (or the whole thing) is incorrect, could someone please point to the mistake? If not, great. Thanks.",,['calculus']
87,Prove that $f(x)=0$ has no repeated roots,Prove that  has no repeated roots,f(x)=0,"$$\text{If } f(x)=\frac{x^n}{n!}+\frac{x^{n-1}}{(n-1)!}+\cdots+x+ 1\text{, then show that } f(x) = 0\\ \text{ has no repeated roots.}$$ I tried solving this question and I think I have come up with a proper answer. I need some verification. My solution/attempt First, we need to prove a theorem. Theorem 1 : If a polynomial function $f(x)$ has a repeated root (say $a$, i.e. $f(a)=0$), then $f'(a)=0$. Proof: We assume that $f(x)$ has a degree of $n \geq 2$. Since $a$ is a factor of $f(x)$, we can write: $$f(x)=(x-a)^m\cdot h(x) \tag{1}$$ where $2  \leq m \leq n$ and $h(x)$ is a polynomial of degree $n - m$ On differentiating $(1)$, we can write $$f'(x) = m(x-a)^{m-1}\cdot h(x) + (x-a)^m\cdot h'(x)\tag{2}$$ Plugging in $x=a$, we obtain $$f'(a)=0+0=0$$ Thus, theorem 1 is true. Now, from the question, we have $$f(x)=\frac{x^n}{n!}+\frac{x^{n-1}}{(n-1)!}+\cdots+x+ 1\tag{3}$$ If we plug in $x=0$, we get $f(0)=1 \neq 0$, for any value of $n$. Hence $0$ is not a root of $f(x)$. On differentiating, we get $$f'(x)=\frac{x^{n-1}}{(n-1)!}+\frac{x^{n-2}}{(n-2)!}+\cdots+x+ 1\tag{4}$$ From $(3)$ and $(4)$, we obtain $$f(x)-f'(x)=\frac{x^n}{n!} \tag{5}$$ Suppose $c \neq 0$ is a root of $f(x)$. Then from $(4)$, we have $$f'(c)=-\frac{c^n}{n!} \neq 0\tag{6}$$ Hence, from theorem 1 , we can conclude that $f(x)=0$ has no repeated roots. Q.E.D. My question I am a beginner in this field. Did I do all the steps correctly? Are there any points I need to take care of?","$$\text{If } f(x)=\frac{x^n}{n!}+\frac{x^{n-1}}{(n-1)!}+\cdots+x+ 1\text{, then show that } f(x) = 0\\ \text{ has no repeated roots.}$$ I tried solving this question and I think I have come up with a proper answer. I need some verification. My solution/attempt First, we need to prove a theorem. Theorem 1 : If a polynomial function $f(x)$ has a repeated root (say $a$, i.e. $f(a)=0$), then $f'(a)=0$. Proof: We assume that $f(x)$ has a degree of $n \geq 2$. Since $a$ is a factor of $f(x)$, we can write: $$f(x)=(x-a)^m\cdot h(x) \tag{1}$$ where $2  \leq m \leq n$ and $h(x)$ is a polynomial of degree $n - m$ On differentiating $(1)$, we can write $$f'(x) = m(x-a)^{m-1}\cdot h(x) + (x-a)^m\cdot h'(x)\tag{2}$$ Plugging in $x=a$, we obtain $$f'(a)=0+0=0$$ Thus, theorem 1 is true. Now, from the question, we have $$f(x)=\frac{x^n}{n!}+\frac{x^{n-1}}{(n-1)!}+\cdots+x+ 1\tag{3}$$ If we plug in $x=0$, we get $f(0)=1 \neq 0$, for any value of $n$. Hence $0$ is not a root of $f(x)$. On differentiating, we get $$f'(x)=\frac{x^{n-1}}{(n-1)!}+\frac{x^{n-2}}{(n-2)!}+\cdots+x+ 1\tag{4}$$ From $(3)$ and $(4)$, we obtain $$f(x)-f'(x)=\frac{x^n}{n!} \tag{5}$$ Suppose $c \neq 0$ is a root of $f(x)$. Then from $(4)$, we have $$f'(c)=-\frac{c^n}{n!} \neq 0\tag{6}$$ Hence, from theorem 1 , we can conclude that $f(x)=0$ has no repeated roots. Q.E.D. My question I am a beginner in this field. Did I do all the steps correctly? Are there any points I need to take care of?",,"['calculus', 'derivatives', 'proof-verification']"
88,Why is Euler's number $2.71828$ and not anything else? [closed],Why is Euler's number  and not anything else? [closed],2.71828,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question Why is Euler's number $\mathtt 2.71828$ and not for example $\mathtt 3.7589$ ? I know that $e$ is the base of natural logarithms. I know about areas on hyperbola xy=1 and I know its formula: $$e =\sum_{n=0}^\infty \frac{1}{n!} \approx 2.71828$$ And I also know it has many other characterizations. But, why is $e$ equal to that formula (which sum is approximately $\mathtt 2.71828$ )? I googled that many times and every time it ends in having "" $e$ is the base of natural logarithms"". I don't want to work out any equations using $e$ without understanding it perfectly. Summary: I'm looking for the origin of $e$ , if $\pi$ came from the radius of a circle with a unit diameter, then what is $e$ ???","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question Why is Euler's number and not for example ? I know that is the base of natural logarithms. I know about areas on hyperbola xy=1 and I know its formula: And I also know it has many other characterizations. But, why is equal to that formula (which sum is approximately )? I googled that many times and every time it ends in having "" is the base of natural logarithms"". I don't want to work out any equations using without understanding it perfectly. Summary: I'm looking for the origin of , if came from the radius of a circle with a unit diameter, then what is ???",\mathtt 2.71828 \mathtt 3.7589 e e =\sum_{n=0}^\infty \frac{1}{n!} \approx 2.71828 e \mathtt 2.71828 e e e \pi e,"['calculus', 'logarithms', 'irrational-numbers', 'eulers-number-e']"
89,What does it mean when dx is put on the start in an integral? [duplicate],What does it mean when dx is put on the start in an integral? [duplicate],,"This question already has answers here : Notation: Why write the differential first? (4 answers) Closed 7 years ago . I have seen something like this before: $\int \frac{dx}{(e+1)^2}$. This is apparently another way to write $\int \frac{1}{(e+1)^2}dx$. However, considering this statement: $\int\frac{du}{(u-1)u^2} = \int du(\frac{1}{u-1}-\frac{1}{u}-\frac{1}{u^2})$. On the left side, $du$ is moved, If I had to evaluate an integral that is written in this way, how would I expand it into the usual $\int f(x)dx$ form? (From the comments) Is this truly a product and if not why is it commutative?","This question already has answers here : Notation: Why write the differential first? (4 answers) Closed 7 years ago . I have seen something like this before: $\int \frac{dx}{(e+1)^2}$. This is apparently another way to write $\int \frac{1}{(e+1)^2}dx$. However, considering this statement: $\int\frac{du}{(u-1)u^2} = \int du(\frac{1}{u-1}-\frac{1}{u}-\frac{1}{u^2})$. On the left side, $du$ is moved, If I had to evaluate an integral that is written in this way, how would I expand it into the usual $\int f(x)dx$ form? (From the comments) Is this truly a product and if not why is it commutative?",,"['calculus', 'integration', 'notation', 'differential']"
90,Finding the limit of $\left(\frac{n}{n+1}\right)^n$,Finding the limit of,\left(\frac{n}{n+1}\right)^n,"Find the limit of: $$\lim_{n\to\infty}\left(\frac{n}{n+1}\right)^n$$ I'm pretty sure it goes to zero since $(n+1)^n > n^n$ but when I input large numbers it goes to $0.36$. Also, when factoring: $$n^{1/n}\left(\frac{1}{1+\frac1n}\right)^n$$ it looks like it goes to $1$. So how can I find this limit?","Find the limit of: $$\lim_{n\to\infty}\left(\frac{n}{n+1}\right)^n$$ I'm pretty sure it goes to zero since $(n+1)^n > n^n$ but when I input large numbers it goes to $0.36$. Also, when factoring: $$n^{1/n}\left(\frac{1}{1+\frac1n}\right)^n$$ it looks like it goes to $1$. So how can I find this limit?",,"['calculus', 'limits', 'exponential-function', 'eulers-number-e']"
91,Can a non-zero vector field have zero divergence and zero curl?,Can a non-zero vector field have zero divergence and zero curl?,,"I don't see how. Curl and divergence are essentially ""opposites"" - essentially two ""orthogonal"" concepts. The entire field should be able to be broken into a curl component and a divergence component and if both are zero, the field must be zero. I'm visualizing it like a vector in $\mathbb{R}^2$. A vector cannot have a zero $x$ component and a zero $y$ component and still be non-zero. EDIT: Here's a slightly more formal formulation of my thoughts: The way I see it, the curl and divergence form a ""basis"" - they are essentially orthogonal vectors. So how can a non-zero vector not be in their span? Please don't just give me a counterexample. Please explain why my logic is incorrect .","I don't see how. Curl and divergence are essentially ""opposites"" - essentially two ""orthogonal"" concepts. The entire field should be able to be broken into a curl component and a divergence component and if both are zero, the field must be zero. I'm visualizing it like a vector in $\mathbb{R}^2$. A vector cannot have a zero $x$ component and a zero $y$ component and still be non-zero. EDIT: Here's a slightly more formal formulation of my thoughts: The way I see it, the curl and divergence form a ""basis"" - they are essentially orthogonal vectors. So how can a non-zero vector not be in their span? Please don't just give me a counterexample. Please explain why my logic is incorrect .",,"['calculus', 'multivariable-calculus', 'differential-geometry', 'vector-spaces', 'vector-analysis']"
92,Determine the maximum of $f(x) = x + \sqrt{4-x^2}$ without calculus,Determine the maximum of  without calculus,f(x) = x + \sqrt{4-x^2},"I was given this problem in a Calc BC course while we were still doing review, so using derivatives or any sort of calculus was generally forbidden. We were only doing review because a lot of people hadn't taken Calc AB due to scheduling issues so the teacher felt he should at least cover some of what would be covered in AB for a while. Of course using calculus this problem is quite trivial. $f'(x) = x/\sqrt{4-x^2}$ , set this equal to zero and the maximum occurs at ( $\sqrt2$ , $2\sqrt2$ ). But again, this method was not allowed. I immediately recognized the $\sqrt{4-x^2}$ as a semicircle with maximum value at 2, so I knew that because x was being added, the value must be greater than 2 at the maximum. Also, the x value must be less than two. I then attempted to rearrange the equation such that it could be written in polar coordinates but, letting $y=f(x)$ caused $x^2$ to cancel out and I was left with $y^2 = 2x\sqrt{4-x^2}$ which only seems to complicate the equation further. I've been stuck at this point for a while as I haven't thought too much about the problem, but my teacher dismissed the problem as he didn't realize he had assigned it. Regardless, this was in the pre-calculus portion of our textbook, so I assume that there has to be a way to solve it (our textbook only gives solutions for odd problems).","I was given this problem in a Calc BC course while we were still doing review, so using derivatives or any sort of calculus was generally forbidden. We were only doing review because a lot of people hadn't taken Calc AB due to scheduling issues so the teacher felt he should at least cover some of what would be covered in AB for a while. Of course using calculus this problem is quite trivial. , set this equal to zero and the maximum occurs at ( , ). But again, this method was not allowed. I immediately recognized the as a semicircle with maximum value at 2, so I knew that because x was being added, the value must be greater than 2 at the maximum. Also, the x value must be less than two. I then attempted to rearrange the equation such that it could be written in polar coordinates but, letting caused to cancel out and I was left with which only seems to complicate the equation further. I've been stuck at this point for a while as I haven't thought too much about the problem, but my teacher dismissed the problem as he didn't realize he had assigned it. Regardless, this was in the pre-calculus portion of our textbook, so I assume that there has to be a way to solve it (our textbook only gives solutions for odd problems).",f'(x) = x/\sqrt{4-x^2} \sqrt2 2\sqrt2 \sqrt{4-x^2} y=f(x) x^2 y^2 = 2x\sqrt{4-x^2},"['calculus', 'algebra-precalculus', 'optimization', 'maxima-minima']"
93,"A logarithmic integral $\int^1_0 \frac{\log\left(\frac{1+x}{1-x}\right)}{x\sqrt{1-x^2}}\,dx$",A logarithmic integral,"\int^1_0 \frac{\log\left(\frac{1+x}{1-x}\right)}{x\sqrt{1-x^2}}\,dx","How to prove the following $$\int^1_0 \frac{\log\left(\frac{1+x}{1-x}\right)}{x\sqrt{1-x^2}}\,dx=\frac{\pi^2}{2}$$ I thought of separating the two integrals and use the beta or hypergeometric functions but I thought these are not best ideas to approach the problem. Any other ideas ?","How to prove the following $$\int^1_0 \frac{\log\left(\frac{1+x}{1-x}\right)}{x\sqrt{1-x^2}}\,dx=\frac{\pi^2}{2}$$ I thought of separating the two integrals and use the beta or hypergeometric functions but I thought these are not best ideas to approach the problem. Any other ideas ?",,"['calculus', 'integration', 'definite-integrals', 'improper-integrals']"
94,Help me evaluate $\int_0^1 \frac{\log(x+1)}{1+x^2} dx$,Help me evaluate,\int_0^1 \frac{\log(x+1)}{1+x^2} dx,"I need to evaluate this integral: $\int_0^1 \frac{\log(x+1)}{1+x^2} dx$. I've tried $t=\log(x+1)$, $t=x+1$, but to no avail. I've noticed that: $\int_0^1 \frac{\log(x+1)}{1+x^2} dx = \int_0^1\log(x+1) \arctan'(x)dx =\left. \log(x+1)\arctan(x) \right|_{x=0}^{x=1} - \int_0^1\frac{\arctan(x)}{x+1}dx$ But can't get further than this. Any help is appreciated, thank you.","I need to evaluate this integral: $\int_0^1 \frac{\log(x+1)}{1+x^2} dx$. I've tried $t=\log(x+1)$, $t=x+1$, but to no avail. I've noticed that: $\int_0^1 \frac{\log(x+1)}{1+x^2} dx = \int_0^1\log(x+1) \arctan'(x)dx =\left. \log(x+1)\arctan(x) \right|_{x=0}^{x=1} - \int_0^1\frac{\arctan(x)}{x+1}dx$ But can't get further than this. Any help is appreciated, thank you.",,"['calculus', 'integration']"
95,"Can $\sum_{x \in [0,1]} e^x$ be represented as an integral?",Can  be represented as an integral?,"\sum_{x \in [0,1]} e^x","In $$\sum_{x \in [0,1]} e^x,$$ $e^x$ is summed over all values in the interval $[0,1]$. Am I right to say that $$\sum_{x \in [0,1]} e^x = \int^{x=1}_{x=0} e^x \, \mathrm dx?$$","In $$\sum_{x \in [0,1]} e^x,$$ $e^x$ is summed over all values in the interval $[0,1]$. Am I right to say that $$\sum_{x \in [0,1]} e^x = \int^{x=1}_{x=0} e^x \, \mathrm dx?$$",,"['calculus', 'integration', 'nonstandard-analysis']"
96,Why are derivatives specified as d/dx?,Why are derivatives specified as d/dx?,,"Is the purpose of the derivative notation d/dx strictly for symbolic manipulation purposes? I remember being confused when I first saw the notation for derivatives - it looks vaguely like there's some division going on and there are some fancy 'd' characters that are added in... I recall thinking that it was a lot of characters to represent an action with respect to one variable. Of course, once you start moving the dx around it makes a little more sense as to why they exist - but is this the only reason? Any history lesson or examples  where this notation is helpful or unhelpful is appreciated.","Is the purpose of the derivative notation d/dx strictly for symbolic manipulation purposes? I remember being confused when I first saw the notation for derivatives - it looks vaguely like there's some division going on and there are some fancy 'd' characters that are added in... I recall thinking that it was a lot of characters to represent an action with respect to one variable. Of course, once you start moving the dx around it makes a little more sense as to why they exist - but is this the only reason? Any history lesson or examples  where this notation is helpful or unhelpful is appreciated.",,"['calculus', 'reference-request', 'notation', 'math-history']"
97,Newton vs Leibniz notation,Newton vs Leibniz notation,,"I have often come across the cursory remarks made here and there in calculus lectures , math documentaries or in calculus textbooks that Leibniz's notation for calculus is better off than that of Newton's and is thus more widely used. Though I have always followed Leibniz's notation( matter of familiarity, as that's what I have been taught) , but of late I had the idea of following Newton's notation just to see where I could get stuck just because of ""notational"" issues. Is there any limitation of Newton's notation that I might encounter while doing calculus ; and which may make it seem a bad idea to do calculus in Newton's notation? Here ""Leibniz notation"" is $\frac{dy}{dx}$ for the derivative of $y$ , and ""Newton's notation"" is $\dot{y}$ for the derivative of $y$ .","I have often come across the cursory remarks made here and there in calculus lectures , math documentaries or in calculus textbooks that Leibniz's notation for calculus is better off than that of Newton's and is thus more widely used. Though I have always followed Leibniz's notation( matter of familiarity, as that's what I have been taught) , but of late I had the idea of following Newton's notation just to see where I could get stuck just because of ""notational"" issues. Is there any limitation of Newton's notation that I might encounter while doing calculus ; and which may make it seem a bad idea to do calculus in Newton's notation? Here ""Leibniz notation"" is for the derivative of , and ""Newton's notation"" is for the derivative of .",\frac{dy}{dx} y \dot{y} y,['calculus']
98,Understanding the Taylor expansion of a function,Understanding the Taylor expansion of a function,,"Suppose $$f(x) = \frac{1}{1+x^2}$$ We know this function is defined everywhere and is continuous everywhere and so on... Using the geometric series, we can write $$ \frac{1}{1+x^2} = \sum (-x^2)^n = \sum (-1)^n x^{2n} $$ But, this only converges iff $|-x^2|<1$ iff $|x|<1$. Why does this converge only in $(-1,1)$ when we know it is defined everywhere, though?","Suppose $$f(x) = \frac{1}{1+x^2}$$ We know this function is defined everywhere and is continuous everywhere and so on... Using the geometric series, we can write $$ \frac{1}{1+x^2} = \sum (-x^2)^n = \sum (-1)^n x^{2n} $$ But, this only converges iff $|-x^2|<1$ iff $|x|<1$. Why does this converge only in $(-1,1)$ when we know it is defined everywhere, though?",,"['calculus', 'taylor-expansion']"
99,Is there a rule of integration that corresponds to the quotient rule?,Is there a rule of integration that corresponds to the quotient rule?,,"When teaching the integration method of u-substitution, I like to emphasize its connection with the chain rule of integration. Likewise, the intimate connection between the product rule of derivatives and the method of integration by parts comes up in discussion. Is there an analogous rule of integration for the quotient rule? Of course, if you spot an integral of the form $\int \left (\frac{f(x)}{g(x)} \right )' = \int \frac{g(x) \cdot f(x)' - f(x) \cdot g(x)'}{\left [ g(x)\right ]^2 }$, then the antiderivative is obvious. But is there another form/manipulation/""trick""?","When teaching the integration method of u-substitution, I like to emphasize its connection with the chain rule of integration. Likewise, the intimate connection between the product rule of derivatives and the method of integration by parts comes up in discussion. Is there an analogous rule of integration for the quotient rule? Of course, if you spot an integral of the form $\int \left (\frac{f(x)}{g(x)} \right )' = \int \frac{g(x) \cdot f(x)' - f(x) \cdot g(x)'}{\left [ g(x)\right ]^2 }$, then the antiderivative is obvious. But is there another form/manipulation/""trick""?",,"['calculus', 'integration', 'indefinite-integrals']"
