,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"max and min of $f(x,y)=y^8-y^4x^6+x^4$",max and min of,"f(x,y)=y^8-y^4x^6+x^4","The function is continuous in $\mathbb{R}^2$ and $f(x,y)=f(x,-y)=f(-x,y)=f(-x,-y)$. If I consider $f(x,0)=x^4$ for $x\rightarrow +\infty$ $f$ is not limited up so $\sup f(x,y)=+\infty$. But the origin is absolute min? $f(x,x)=x^8-x^{10}+x^4\rightarrow -\infty$ if $x\rightarrow \infty$? so f is not limit down","The function is continuous in $\mathbb{R}^2$ and $f(x,y)=f(x,-y)=f(-x,y)=f(-x,-y)$. If I consider $f(x,0)=x^4$ for $x\rightarrow +\infty$ $f$ is not limited up so $\sup f(x,y)=+\infty$. But the origin is absolute min? $f(x,x)=x^8-x^{10}+x^4\rightarrow -\infty$ if $x\rightarrow \infty$? so f is not limit down",,"['calculus', 'real-analysis', 'multivariable-calculus', 'optimization', 'maxima-minima']"
1,Partial differentiation notation in thermodynamics,Partial differentiation notation in thermodynamics,,"In my thermodynamics notes, the definition of isothermal heat capacity at constant volume is $$C_V := \frac T N \left(\frac{\partial S}{\partial T} \right)_V $$ and at constant pressure is $$C_P := \frac T N \left(\frac{\partial S}{\partial T} \right)_P $$ where $T$ is temperature, $N$ is the number of molecules in the system and $S$ is entropy. I don't fully understand the derivation symbols. To me, the partial derivative of entropy with respect to temperature would correspond to the ""slope in the temperature direction"", meaning that all other variables are considered to be ""frozen"". This would mean that the two partial derivatives above, and consequently $C_V$ and $C_P$, are one and the same object. But it is clearly not so. Why is there a need to specify that we are keeping volume / pressure constant, when this is already implied in the definition of partial differentiation? How would one make this notation mathematically rigorous, i.e., what is the actual math behind this abbreviated notation? (Possibly taking differential forms into account?)","In my thermodynamics notes, the definition of isothermal heat capacity at constant volume is $$C_V := \frac T N \left(\frac{\partial S}{\partial T} \right)_V $$ and at constant pressure is $$C_P := \frac T N \left(\frac{\partial S}{\partial T} \right)_P $$ where $T$ is temperature, $N$ is the number of molecules in the system and $S$ is entropy. I don't fully understand the derivation symbols. To me, the partial derivative of entropy with respect to temperature would correspond to the ""slope in the temperature direction"", meaning that all other variables are considered to be ""frozen"". This would mean that the two partial derivatives above, and consequently $C_V$ and $C_P$, are one and the same object. But it is clearly not so. Why is there a need to specify that we are keeping volume / pressure constant, when this is already implied in the definition of partial differentiation? How would one make this notation mathematically rigorous, i.e., what is the actual math behind this abbreviated notation? (Possibly taking differential forms into account?)",,"['multivariable-calculus', 'partial-derivative', 'physics']"
2,Whys does 2 dimensional curl always measure twice the angular velocity of rotational component of velocity field?,Whys does 2 dimensional curl always measure twice the angular velocity of rotational component of velocity field?,,"I thought I understood curl in 2 D until I saw the above statement in the manual.  Why is this necessarily true?.  I used the vector field F= <-y,x> and yes I can see when curl is calculated by using the second derivatives and subtracting them in the correct order it does in fact  = 2. But this is one example, how is it that the twist will always ""double up"". Intuitively the force field is twisting in both x and y but they may not be twisting in the same direction and when the stronger direction wins out why would it be double ??? Should it not be a fraction of the stronger twist since the directions are different unless it is the case they always twist in the same direction. I am convinced there is an intuition failure on my part here.","I thought I understood curl in 2 D until I saw the above statement in the manual.  Why is this necessarily true?.  I used the vector field F= <-y,x> and yes I can see when curl is calculated by using the second derivatives and subtracting them in the correct order it does in fact  = 2. But this is one example, how is it that the twist will always ""double up"". Intuitively the force field is twisting in both x and y but they may not be twisting in the same direction and when the stronger direction wins out why would it be double ??? Should it not be a fraction of the stronger twist since the directions are different unless it is the case they always twist in the same direction. I am convinced there is an intuition failure on my part here.",,['multivariable-calculus']
3,How to calculate $\iint\ln(x^2+y^2)$ over a part of a circle?,How to calculate  over a part of a circle?,\iint\ln(x^2+y^2),"Given the range $\frac{1}{\sqrt2}\le x\le 1$ and $\sqrt{1-x^2}\le y\le x$   calculate $\iint\ln(x^2+y^2)$. This is how the domain looks like: We need to calculate the integral on the area in red. It seems quite hard next to impossible to pull off the calculation with the given ranges so I thought to calculate the integral on the triangle $ABC$ (let this domain be $B$) and from that to subtract the integral in on circle with the angle between $0$ and $\pi/4$ (let this domain be $C$). So $\iint_C$ I actually was to able to pull off. But $\iint_B$ doesn't seem feasible at least to me. $$ \iint_C\ln(x^2+y^2)=\int_0^{\pi/4}\int_0^1 \ln(r^2)r \,dr\,d\theta=\frac{1}{2}\int\bigg[ r^2\ln r^2-r^2\bigg]_0^1=\frac{1}{2}\int-1\,d\theta=-\frac{\pi}{8} $$ Now: $$ \iint_B \ln(x^2+y^2)\,dx \,dy=\int_0^1\int_0^x \ln(x^2+y^2) $$ I don't see how this can be integrated, substitution method doesn't help here. Can I convert to polar coordinates again? But then: $$ \int_0^1\int_0^{r\cos\theta} r\ln r^2 $$ which is not feasible for me. How to tackle this?","Given the range $\frac{1}{\sqrt2}\le x\le 1$ and $\sqrt{1-x^2}\le y\le x$   calculate $\iint\ln(x^2+y^2)$. This is how the domain looks like: We need to calculate the integral on the area in red. It seems quite hard next to impossible to pull off the calculation with the given ranges so I thought to calculate the integral on the triangle $ABC$ (let this domain be $B$) and from that to subtract the integral in on circle with the angle between $0$ and $\pi/4$ (let this domain be $C$). So $\iint_C$ I actually was to able to pull off. But $\iint_B$ doesn't seem feasible at least to me. $$ \iint_C\ln(x^2+y^2)=\int_0^{\pi/4}\int_0^1 \ln(r^2)r \,dr\,d\theta=\frac{1}{2}\int\bigg[ r^2\ln r^2-r^2\bigg]_0^1=\frac{1}{2}\int-1\,d\theta=-\frac{\pi}{8} $$ Now: $$ \iint_B \ln(x^2+y^2)\,dx \,dy=\int_0^1\int_0^x \ln(x^2+y^2) $$ I don't see how this can be integrated, substitution method doesn't help here. Can I convert to polar coordinates again? But then: $$ \int_0^1\int_0^{r\cos\theta} r\ln r^2 $$ which is not feasible for me. How to tackle this?",,"['integration', 'multivariable-calculus']"
4,When calculating work using line integrals how is it known that paramaterization will not change the path?,When calculating work using line integrals how is it known that paramaterization will not change the path?,,"I am having a bit of an intuition problem here. As I am doing work problems on  the work done by using line integrals it is said the work done for some but not all line integrals depends on the path.  Let us focus on those.  I  used different parameterizations of the path. In one case the path was nothing more that a straight line.   I let x = t and y = t, being my parameterization. In another case the path was a parabola and I let x = t and y = t^2 , that being the parameterization. Now the professor said how I parameterized made no difference. So I used x = sin t and y = sin t in the line case and x = sin t and y = sin^2 t in the second case. The results being the same. My question is when I use the sin function to parameterize how can the results possibly be guaranteed to be the same? The work function does not change , I can see that and the bound are kept the same I can see that, and the path is still the same in that but in one case I used a variable and the other case I used a sin function...how do I know the sin function does not affect the path in some way...struggling here with intuition.  Thank you","I am having a bit of an intuition problem here. As I am doing work problems on  the work done by using line integrals it is said the work done for some but not all line integrals depends on the path.  Let us focus on those.  I  used different parameterizations of the path. In one case the path was nothing more that a straight line.   I let x = t and y = t, being my parameterization. In another case the path was a parabola and I let x = t and y = t^2 , that being the parameterization. Now the professor said how I parameterized made no difference. So I used x = sin t and y = sin t in the line case and x = sin t and y = sin^2 t in the second case. The results being the same. My question is when I use the sin function to parameterize how can the results possibly be guaranteed to be the same? The work function does not change , I can see that and the bound are kept the same I can see that, and the path is still the same in that but in one case I used a variable and the other case I used a sin function...how do I know the sin function does not affect the path in some way...struggling here with intuition.  Thank you",,"['multivariable-calculus', 'line-integrals']"
5,Is this hypothesis necessary for Fubini's theorem (simple version for Riemann integrals),Is this hypothesis necessary for Fubini's theorem (simple version for Riemann integrals),,"I've seen the following statement of Fubini's theorem for double integrals in a multivariable course notes: Let $I=[a,b]×[c,d]$ and $f:I\to \mathbb{R}$ an integrable function. If foreach $x \in [a,b]$ the function $g_x(y)=(x,y)$ is integrable, then $$\int \int_I f=\int_a^b\int_c^df(x,y)dydx$$ But I've also found this more general version: Let $A \subset \mathbb{R}^n$ and $B \subset \mathbb{R}^m$ be closed boxes (i.e product of closed intervals) and $f:A×B \to \mathbb{R}$ an integrable function. If for each $x \in A$ the function $g_x(y)=f(x,y)$ is integrable, then $$\int_{A×B}f=\int_A\int_B f$$ provided $\int_A\int_B f$ exists. So my question is why is that extra hypothesis needed in the second version. Vould someone provide an example illustrating this necessity? Note: both cases refer to the Riemann integral","I've seen the following statement of Fubini's theorem for double integrals in a multivariable course notes: Let and an integrable function. If foreach the function is integrable, then But I've also found this more general version: Let and be closed boxes (i.e product of closed intervals) and an integrable function. If for each the function is integrable, then provided exists. So my question is why is that extra hypothesis needed in the second version. Vould someone provide an example illustrating this necessity? Note: both cases refer to the Riemann integral","I=[a,b]×[c,d] f:I\to \mathbb{R} x \in [a,b] g_x(y)=(x,y) \int \int_I f=\int_a^b\int_c^df(x,y)dydx A \subset \mathbb{R}^n B \subset \mathbb{R}^m f:A×B \to \mathbb{R} x \in A g_x(y)=f(x,y) \int_{A×B}f=\int_A\int_B f \int_A\int_B f","['real-analysis', 'integration', 'multivariable-calculus', 'multiple-integral']"
6,Partial derivatives vs. Total Derivatives for chain rule.,Partial derivatives vs. Total Derivatives for chain rule.,,"If I had a function $f(x,y)$ where $x=x(s,t)$ and $y=y(s,t)$ then $$ \frac{df}{dt} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}. $$ I have been reading this and have spotted in case 2 that $$ \frac{\partial f}{\partial t} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}. $$ $f$ has no explicit $t$ dependence so how can this be correct? How can I have a chain rule for partial derivatives? A chain rule implies that there is no explicit dependence on the variable that we are differentiating with respect to, and I thought partial derivatives only deal with explicit dependencies and not implicit? Thanks","If I had a function $f(x,y)$ where $x=x(s,t)$ and $y=y(s,t)$ then $$ \frac{df}{dt} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}. $$ I have been reading this and have spotted in case 2 that $$ \frac{\partial f}{\partial t} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}. $$ $f$ has no explicit $t$ dependence so how can this be correct? How can I have a chain rule for partial derivatives? A chain rule implies that there is no explicit dependence on the variable that we are differentiating with respect to, and I thought partial derivatives only deal with explicit dependencies and not implicit? Thanks",,"['multivariable-calculus', 'partial-derivative']"
7,Differentiability of multivariable functions represented through integral,Differentiability of multivariable functions represented through integral,,"I have question about differentiability of functions from $\Bbb R^n$ to $\Bbb R$ represented through Lebesgue (Riemann) Integral. Which is somehow generalization of single variable case. Question 1. Let $\alpha : \Bbb R^n \to \Bbb (0, +\infty)$ be nice function (for example assume $C^1$), and Let $f : \Bbb R^n \to \Bbb R$ be continuous function   define $$ g(x) = \int_{B(x; \alpha(x))} f(y) ~ dy $$ is $g$ differentiable ? if so what is $\nabla g ?$ If not, what if I fix the center of the Ball  i.e., $$ g(x) = \int_{B(a; \alpha(x))} f(y) ~ dy  $$ Obviously  the answer is positive in dimension  one. Question2.  Let $f : \Bbb R^n \times \Bbb R^m \to \Bbb R $ be continuous and differentiable w.r.t to first argument,  (you even may assume $f$ is $C^1$ if I didn't make enough assumptions) Let   $$ g(x) = \int_{ \Bbb R^m } f(x,y) ~ dy $$ Is $g$ differentiable ? if so what is $\nabla g ?$","I have question about differentiability of functions from $\Bbb R^n$ to $\Bbb R$ represented through Lebesgue (Riemann) Integral. Which is somehow generalization of single variable case. Question 1. Let $\alpha : \Bbb R^n \to \Bbb (0, +\infty)$ be nice function (for example assume $C^1$), and Let $f : \Bbb R^n \to \Bbb R$ be continuous function   define $$ g(x) = \int_{B(x; \alpha(x))} f(y) ~ dy $$ is $g$ differentiable ? if so what is $\nabla g ?$ If not, what if I fix the center of the Ball  i.e., $$ g(x) = \int_{B(a; \alpha(x))} f(y) ~ dy  $$ Obviously  the answer is positive in dimension  one. Question2.  Let $f : \Bbb R^n \times \Bbb R^m \to \Bbb R $ be continuous and differentiable w.r.t to first argument,  (you even may assume $f$ is $C^1$ if I didn't make enough assumptions) Let   $$ g(x) = \int_{ \Bbb R^m } f(x,y) ~ dy $$ Is $g$ differentiable ? if so what is $\nabla g ?$",,"['real-analysis', 'multivariable-calculus', 'partial-differential-equations']"
8,The Hessian matrix A may be Indefinite or what is known Positive Semidefinite or Negative Semidefinite,The Hessian matrix A may be Indefinite or what is known Positive Semidefinite or Negative Semidefinite,,"We are about to look at an important type of matrix in multivariable calculus known as Hessian Matrices. We will then formulate a generalized second derivatives test for a real-valued function $z=f(x_1,x_2,...,x_n)$ of n variables with continuous partial derivatives at a critical point $a=(a1,a2,...,an)∈D(f)$ to determine whether $f(a)$ is a local maximum value, local minimum value, or saddle point of $f$ Definition: Let $x=(x1,x2,...,xn)$ and let $z=f(x1,x2,...,xn)=f(x)$ be an n variable real-valued function whose second partial derivatives exist. Then the Hessian Matrix of f is the n×n matrix of second partial derivatives of f denoted $$\mathcal H (\mathbf{x}) = \begin{bmatrix} f_{11} (\mathbf{x}) & f_{12} (\mathbf{x}) & \cdots & f_{1n} (\mathbf{x})\\ f_{21} (\mathbf{x}) & f_{22} (\mathbf{x}) & \cdots & f_{2n} (\mathbf{x})\\ \vdots & \vdots & \ddots & \vdots \\ f_{n1} (\mathbf{x}) & f_{n2} (\mathbf{x}) & \cdots & f_{nn} (\mathbf{x}) \end{bmatrix}$$. a)$\mathcal H (\mathbf{x})$ is said to be $\textbf{Positive Definite}$ if $D_i>0$ for i=1,2,...,n. b) $\mathcal H (\mathbf{x})$ is said to be $\textbf{Negative Definite}$ if $D_i<0$ for odd i∈{1,2,...,n} and Di>0 for even i∈{1,2,...,n}. c) $\mathcal H (\mathbf{x})$ is said to be $\textbf{Indefinite}$ if $det(\mathcal H (\mathbf{x}))=D_n≠0$ and neither a) nor b) hold. d) If $det(\mathcal H (\mathbf{x}))=D_n=0$, then $\mathcal H (\mathbf{x})$ may be Indefinite or what is known Positive Semidefinite or Negative Semidefinite. I'm studying a function that has a $det(\mathcal H (\mathbf{x}))=D_n=0$ What is the appropriate way to classify critical points if $det(\mathcal H (\mathbf{x}))=D_n=0$ For example  Assume we have this function $$f(x_1,x_2,x_3)=x_1 x_2+x_1 x_3-x_1x_2 x_3 $$ therefore, we found the critical point by solve this system  $$\begin{equation} \begin{cases} \frac{df}{dx_1}=x_2+x_3-x_3 x_2=0\\ \frac{df}{dx_2}=x_1-x_1 x_3 =0\\ \frac{df}{dx_3}=x_1-x_1 x_2 =0  \end{cases} \end{equation}$$ we get $c=(0,0,0)$ ,use the Hessian matrix of f $$\mathcal H (\mathbf{f(x_i)})=\left( \begin{array}{ccc}  0 & 1-x_3 & 1-x_2 \\  1-x_3 & 0 & -x_1 \\  1-x_2 & -x_1 & 0 \\ \end{array} \right)$$ $det(\mathcal H (\mathbf{f(x_i)}))=D_n=0$ This test fails to determine the type of critical points Is there another way? Or modification of the solution and correction. Thanks for the help.","We are about to look at an important type of matrix in multivariable calculus known as Hessian Matrices. We will then formulate a generalized second derivatives test for a real-valued function $z=f(x_1,x_2,...,x_n)$ of n variables with continuous partial derivatives at a critical point $a=(a1,a2,...,an)∈D(f)$ to determine whether $f(a)$ is a local maximum value, local minimum value, or saddle point of $f$ Definition: Let $x=(x1,x2,...,xn)$ and let $z=f(x1,x2,...,xn)=f(x)$ be an n variable real-valued function whose second partial derivatives exist. Then the Hessian Matrix of f is the n×n matrix of second partial derivatives of f denoted $$\mathcal H (\mathbf{x}) = \begin{bmatrix} f_{11} (\mathbf{x}) & f_{12} (\mathbf{x}) & \cdots & f_{1n} (\mathbf{x})\\ f_{21} (\mathbf{x}) & f_{22} (\mathbf{x}) & \cdots & f_{2n} (\mathbf{x})\\ \vdots & \vdots & \ddots & \vdots \\ f_{n1} (\mathbf{x}) & f_{n2} (\mathbf{x}) & \cdots & f_{nn} (\mathbf{x}) \end{bmatrix}$$. a)$\mathcal H (\mathbf{x})$ is said to be $\textbf{Positive Definite}$ if $D_i>0$ for i=1,2,...,n. b) $\mathcal H (\mathbf{x})$ is said to be $\textbf{Negative Definite}$ if $D_i<0$ for odd i∈{1,2,...,n} and Di>0 for even i∈{1,2,...,n}. c) $\mathcal H (\mathbf{x})$ is said to be $\textbf{Indefinite}$ if $det(\mathcal H (\mathbf{x}))=D_n≠0$ and neither a) nor b) hold. d) If $det(\mathcal H (\mathbf{x}))=D_n=0$, then $\mathcal H (\mathbf{x})$ may be Indefinite or what is known Positive Semidefinite or Negative Semidefinite. I'm studying a function that has a $det(\mathcal H (\mathbf{x}))=D_n=0$ What is the appropriate way to classify critical points if $det(\mathcal H (\mathbf{x}))=D_n=0$ For example  Assume we have this function $$f(x_1,x_2,x_3)=x_1 x_2+x_1 x_3-x_1x_2 x_3 $$ therefore, we found the critical point by solve this system  $$\begin{equation} \begin{cases} \frac{df}{dx_1}=x_2+x_3-x_3 x_2=0\\ \frac{df}{dx_2}=x_1-x_1 x_3 =0\\ \frac{df}{dx_3}=x_1-x_1 x_2 =0  \end{cases} \end{equation}$$ we get $c=(0,0,0)$ ,use the Hessian matrix of f $$\mathcal H (\mathbf{f(x_i)})=\left( \begin{array}{ccc}  0 & 1-x_3 & 1-x_2 \\  1-x_3 & 0 & -x_1 \\  1-x_2 & -x_1 & 0 \\ \end{array} \right)$$ $det(\mathcal H (\mathbf{f(x_i)}))=D_n=0$ This test fails to determine the type of critical points Is there another way? Or modification of the solution and correction. Thanks for the help.",,"['multivariable-calculus', 'hessian-matrix', 'positive-semidefinite', 'multivalued-functions']"
9,Integrating a function over a square using polar coordinates,Integrating a function over a square using polar coordinates,,"Say we have a function $f(x,y)$ over the unit circle. To integrate with polar coordinates we replace the x and y in $f(x,y)$ with $r\cos\theta$ and $y\sin\theta$ to get $f(r,\theta)$ and we integrate $f(r,\theta)rdrd\theta$ for $r$ between $0$ and $1$ and $\theta$ between 0 and $2\pi$. What if we want to integrate over a square using polar coordinates. What must we do?","Say we have a function $f(x,y)$ over the unit circle. To integrate with polar coordinates we replace the x and y in $f(x,y)$ with $r\cos\theta$ and $y\sin\theta$ to get $f(r,\theta)$ and we integrate $f(r,\theta)rdrd\theta$ for $r$ between $0$ and $1$ and $\theta$ between 0 and $2\pi$. What if we want to integrate over a square using polar coordinates. What must we do?",,"['calculus', 'integration', 'multivariable-calculus', 'polar-coordinates']"
10,Find flux through a sphere,Find flux through a sphere,,"Find flux of $ \vec{F} =<0,0,z>$ through the sphere of radius $a$ centered at the origin. Using Gauss Divergence Theorem $$ \nabla \cdot \vec{F} = 1 \\ \therefore \int_S \vec{F}\cdot \hat{n} ~dS= \iiint_R 1  ~dV =\frac{4}{3}\pi a^3 $$ Calculating the surface integral by using sperical co-ordinates $ (\rho, \phi, \theta )$ $$ \hat{n} = \frac{1}{a} <x,y,z> \\ \vec{F} \cdot \hat{n}=\frac{z^2}{a}$$ w.k.t $$ dS= a^2 \sin(\theta) ~ d\phi d\theta \\ z= a cos(\phi)$$ substituting: $$\int_s \vec{F}\cdot \hat{n} ~dS = \int_0^{2\pi} \int_0^\pi \frac{a^2 cos^2(\phi)}{a} a^2 \sin(\theta) ~ d\phi d\theta \\ $$ This integral will evaluate to zero , because $sin(\theta)$ will be integrated from $0$ to $2\pi$. I'm getting two different answers. Where am I going wrong?","Find flux of $ \vec{F} =<0,0,z>$ through the sphere of radius $a$ centered at the origin. Using Gauss Divergence Theorem $$ \nabla \cdot \vec{F} = 1 \\ \therefore \int_S \vec{F}\cdot \hat{n} ~dS= \iiint_R 1  ~dV =\frac{4}{3}\pi a^3 $$ Calculating the surface integral by using sperical co-ordinates $ (\rho, \phi, \theta )$ $$ \hat{n} = \frac{1}{a} <x,y,z> \\ \vec{F} \cdot \hat{n}=\frac{z^2}{a}$$ w.k.t $$ dS= a^2 \sin(\theta) ~ d\phi d\theta \\ z= a cos(\phi)$$ substituting: $$\int_s \vec{F}\cdot \hat{n} ~dS = \int_0^{2\pi} \int_0^\pi \frac{a^2 cos^2(\phi)}{a} a^2 \sin(\theta) ~ d\phi d\theta \\ $$ This integral will evaluate to zero , because $sin(\theta)$ will be integrated from $0$ to $2\pi$. I'm getting two different answers. Where am I going wrong?",,['multivariable-calculus']
11,Double integral; limits and variable exchange,Double integral; limits and variable exchange,,"I am struggling to figure out the correct way of calculating certain integrals of this nature, i.e. more complicated limits and functions: $$\int_{x=0}^1 \left( \int_{y=\sqrt[3]{x}}^1 \frac{dy}{\sqrt{1+y^8}}\right) dx $$ Typically I'd try to re-write the limits and/or a variable change so it is easier to perform the integration. However, I am struggling and I can't seem to find any help in my textbook and class material. The idea I had would be to rewrite the limits to $ \; 0 \le x \le y^3 \; , \; 0 \le y \le 1 \;$ and first integrate $\; \int_{x=0}^{y^3}dx \;$ and then $ \int dy \;$, but I seem to get stuck on the next step. I.e when. $$ \int_{y=0}^1 \frac{([x]_0^{y^3})}{\sqrt{1+y^8}}dy = \int_{y=0}^1 \frac{y^3}{\sqrt{1+y^8}}dy $$ What would my next steps be and how? I would like to say that a variable exchange would be in order, e.g. $ u=y^4 \Rightarrow \frac{du}{dy}= 4y^3 \; $ but I can't figure it out. The answer in my book does not give any advice or guidance. This is my first post, so I do ask for your forgiveness if there is something wrong! Thank you,","I am struggling to figure out the correct way of calculating certain integrals of this nature, i.e. more complicated limits and functions: $$\int_{x=0}^1 \left( \int_{y=\sqrt[3]{x}}^1 \frac{dy}{\sqrt{1+y^8}}\right) dx $$ Typically I'd try to re-write the limits and/or a variable change so it is easier to perform the integration. However, I am struggling and I can't seem to find any help in my textbook and class material. The idea I had would be to rewrite the limits to $ \; 0 \le x \le y^3 \; , \; 0 \le y \le 1 \;$ and first integrate $\; \int_{x=0}^{y^3}dx \;$ and then $ \int dy \;$, but I seem to get stuck on the next step. I.e when. $$ \int_{y=0}^1 \frac{([x]_0^{y^3})}{\sqrt{1+y^8}}dy = \int_{y=0}^1 \frac{y^3}{\sqrt{1+y^8}}dy $$ What would my next steps be and how? I would like to say that a variable exchange would be in order, e.g. $ u=y^4 \Rightarrow \frac{du}{dy}= 4y^3 \; $ but I can't figure it out. The answer in my book does not give any advice or guidance. This is my first post, so I do ask for your forgiveness if there is something wrong! Thank you,",,"['integration', 'multivariable-calculus']"
12,How to remember laplacian in polar and (hyper)spherical coordinates,How to remember laplacian in polar and (hyper)spherical coordinates,,"I am trying to remember the equation for the laplacian in polar coordinates: $$ \Delta u=u_{rr}+\frac{u_r}{r}+\frac{u_{\theta \theta}}{r^2} $$ without having to derive it everytime and hopefully gain some intuition in the process. I am also hoping to get some understanding for the formula in $\mathbb{R}^n$ in hyperspherical coordinates: $$ \Delta u=u_{rr}+\frac{(n-1)u_r}{r}+\frac{\Delta_s u}{r^2} $$ where the $\Delta_su$ represents the laplace beltrami operator, which only depends on angular coordinates and which I definitely do not want to derive, ever. When this was introduced in my PDE class, the professor drew a circle on the board with a tangential vector and talked about a correction term on the order of $r^2$ from traveling along the circle vs this tangential vector. No one really got what he meant, and maybe it is unclear. Anyway, I a mostly looking for some intuition of this kind for why this expression makes sense. I would be happy to learn about the polar version but definitely would be happy if someone can shed light on the n dimensional one as well. Thanks!","I am trying to remember the equation for the laplacian in polar coordinates: $$ \Delta u=u_{rr}+\frac{u_r}{r}+\frac{u_{\theta \theta}}{r^2} $$ without having to derive it everytime and hopefully gain some intuition in the process. I am also hoping to get some understanding for the formula in $\mathbb{R}^n$ in hyperspherical coordinates: $$ \Delta u=u_{rr}+\frac{(n-1)u_r}{r}+\frac{\Delta_s u}{r^2} $$ where the $\Delta_su$ represents the laplace beltrami operator, which only depends on angular coordinates and which I definitely do not want to derive, ever. When this was introduced in my PDE class, the professor drew a circle on the board with a tangential vector and talked about a correction term on the order of $r^2$ from traveling along the circle vs this tangential vector. No one really got what he meant, and maybe it is unclear. Anyway, I a mostly looking for some intuition of this kind for why this expression makes sense. I would be happy to learn about the polar version but definitely would be happy if someone can shed light on the n dimensional one as well. Thanks!",,"['calculus', 'multivariable-calculus', 'partial-differential-equations', 'vector-analysis', 'laplacian']"
13,Interpreting divergence of velocity field,Interpreting divergence of velocity field,,"The wikipedia article on divergence describes one interpretation of divergence: ""The velocity of the air at each point defines a vector field. While air is heated in a region, it expands in all directions, and thus the velocity field points outward from that region."" If we have a vector field which represents a force, I interpret the divergence as representing the strength of the field at whatever point it's taken at. However I'm confused on how to interpret the divergence of a velocity field. Clearly if the divergence is positive gas is expanding outward and if it's negative it's contracting, however what quantity is actually represented? If I have a velocity field with $m/s$ units, then the div presumably has $m/s^2$ units. Is the quantity we get the actual acceleration of gas away from that point?","The wikipedia article on divergence describes one interpretation of divergence: ""The velocity of the air at each point defines a vector field. While air is heated in a region, it expands in all directions, and thus the velocity field points outward from that region."" If we have a vector field which represents a force, I interpret the divergence as representing the strength of the field at whatever point it's taken at. However I'm confused on how to interpret the divergence of a velocity field. Clearly if the divergence is positive gas is expanding outward and if it's negative it's contracting, however what quantity is actually represented? If I have a velocity field with $m/s$ units, then the div presumably has $m/s^2$ units. Is the quantity we get the actual acceleration of gas away from that point?",,"['multivariable-calculus', 'divergence-operator']"
14,"If all directional derivatives exist at $0\in \mathbb R^2$, why the gradient doesn't exist?","If all directional derivatives exist at , why the gradient doesn't exist?",0\in \mathbb R^2,"Let $f:\mathbb R^2\longrightarrow \mathbb R$ a function s.t. all directional derivative exist at $0$. Why the gradient doesn't exist ? Don't we have that $\nabla f(0)=(\frac{\partial f}{\partial x}(0),\frac{\partial f}{\partial y}(0))$ ? Indeed, $$\frac{\partial f}{\partial x}(0)=\nabla f(0)\cdot (1,0)\quad \text{and}\quad \frac{\partial f}{\partial y}(0)=\nabla f(0)\cdot (0,1)$$ so the gradient should exist, no ?","Let $f:\mathbb R^2\longrightarrow \mathbb R$ a function s.t. all directional derivative exist at $0$. Why the gradient doesn't exist ? Don't we have that $\nabla f(0)=(\frac{\partial f}{\partial x}(0),\frac{\partial f}{\partial y}(0))$ ? Indeed, $$\frac{\partial f}{\partial x}(0)=\nabla f(0)\cdot (1,0)\quad \text{and}\quad \frac{\partial f}{\partial y}(0)=\nabla f(0)\cdot (0,1)$$ so the gradient should exist, no ?",,"['real-analysis', 'multivariable-calculus']"
15,Parameterising the intersection of a plane and paraboloid,Parameterising the intersection of a plane and paraboloid,,"Suppose we have the paraboloid $z=x^2+y^2$ and the plane $z=y$. Their intersection produces a curve $C$, and certain surfaces bounded by it, for example the disc $S$ which directly fills the area of $C$ and the paraboloid $S'$ given by $z=x^2+y^2$ which extends from $C$ downwards and is bounded by $C$. My question is on how to parameterise these objects. My initial instinct is to substitute one equation into the other giving the circle $(y-\frac{1}{2})^2+x^2=\frac{1}{4}$. Now I could go ahead and parameterise this circle using polars or even do integrals directly using $x$ and $y$. But this gives the projection of the intersection of the plane and paraboloid in the xy-plane. But I have not directly parameterised $C$. Since the $z=y$ plane makes a $45$ degree angle with the xy-plane, I think that we can introduce a factor of $\sqrt2$ into our parameterisation, but I would like to know if there is a way to directly parameterise $C$ (and also the surfaces $S$ and $S'$)? Note my question is similar to this one except instead of being level, my disc is slanted (and I would also like to find out how to parameterise the 'bowl shaped' surface). Many thanks for any help.","Suppose we have the paraboloid $z=x^2+y^2$ and the plane $z=y$. Their intersection produces a curve $C$, and certain surfaces bounded by it, for example the disc $S$ which directly fills the area of $C$ and the paraboloid $S'$ given by $z=x^2+y^2$ which extends from $C$ downwards and is bounded by $C$. My question is on how to parameterise these objects. My initial instinct is to substitute one equation into the other giving the circle $(y-\frac{1}{2})^2+x^2=\frac{1}{4}$. Now I could go ahead and parameterise this circle using polars or even do integrals directly using $x$ and $y$. But this gives the projection of the intersection of the plane and paraboloid in the xy-plane. But I have not directly parameterised $C$. Since the $z=y$ plane makes a $45$ degree angle with the xy-plane, I think that we can introduce a factor of $\sqrt2$ into our parameterisation, but I would like to know if there is a way to directly parameterise $C$ (and also the surfaces $S$ and $S'$)? Note my question is similar to this one except instead of being level, my disc is slanted (and I would also like to find out how to parameterise the 'bowl shaped' surface). Many thanks for any help.",,"['multivariable-calculus', 'surfaces', 'parametric', 'surface-integrals', 'parametrization']"
16,multi variable calculus problem,multi variable calculus problem,,In this problem putiing it in limit  1 / n f(k/n) form it turns out to be π\4  which is not in the option . its answer is given as the first three options . how can it have three possible answers?,In this problem putiing it in limit  1 / n f(k/n) form it turns out to be π\4  which is not in the option . its answer is given as the first three options . how can it have three possible answers?,,['multivariable-calculus']
17,Example of conservative vector field that is not irrotational,Example of conservative vector field that is not irrotational,,"Can anyone suggest an example of a vector field $F: A \subset \mathbb{R}^3 \to \mathbb{R}^3$ that satisfies all the following conditions? $F$ does not belong to $C^1(A)$ $F$ is conservative in $A$ It is not true that $\mathrm{curl} F(x)=\bar{0}$ $\,\,\,\,\forall x \in A$ (Also an example in dimensions other than $\mathbb{R}^3$ would be good)","Can anyone suggest an example of a vector field $F: A \subset \mathbb{R}^3 \to \mathbb{R}^3$ that satisfies all the following conditions? $F$ does not belong to $C^1(A)$ $F$ is conservative in $A$ It is not true that $\mathrm{curl} F(x)=\bar{0}$ $\,\,\,\,\forall x \in A$ (Also an example in dimensions other than $\mathbb{R}^3$ would be good)",,"['calculus', 'integration', 'multivariable-calculus', 'vector-analysis']"
18,How do I convert this double integral to polar?,How do I convert this double integral to polar?,,"I'm having trouble converting this double integral from Cartesian to polar: $$\int_0^1\int_0^{\sqrt{(1-x^2)}}e^{x^2+y^2}dydx$$ What I know: $$e^{x^2+y^2}dydx = re^{r^2}drd\theta$$ $$\sqrt{1-x^2} = \sqrt{r^2\sin^2{\theta}} = r\sin{\theta}$$ My issue: I can't figure out how to convert the limits. I graphed $y=\sqrt{1-x^2}$ using wolfram alpha, and found that it forms half of an ellipse on the positive side of the x-axis with the origin being the center. Since it forms an ellipse, I don't know the limits of r because the radius is not uniform around the shape. I assume that the limits of $\theta$ are $0\le\theta\le{\pi/2}$ since the limits of X in Cartesian form are $0\le{x}\le1$ and the shape is an ellipse centered around the origin. Could I get some guidance in the conversion of the limits?","I'm having trouble converting this double integral from Cartesian to polar: $$\int_0^1\int_0^{\sqrt{(1-x^2)}}e^{x^2+y^2}dydx$$ What I know: $$e^{x^2+y^2}dydx = re^{r^2}drd\theta$$ $$\sqrt{1-x^2} = \sqrt{r^2\sin^2{\theta}} = r\sin{\theta}$$ My issue: I can't figure out how to convert the limits. I graphed $y=\sqrt{1-x^2}$ using wolfram alpha, and found that it forms half of an ellipse on the positive side of the x-axis with the origin being the center. Since it forms an ellipse, I don't know the limits of r because the radius is not uniform around the shape. I assume that the limits of $\theta$ are $0\le\theta\le{\pi/2}$ since the limits of X in Cartesian form are $0\le{x}\le1$ and the shape is an ellipse centered around the origin. Could I get some guidance in the conversion of the limits?",,"['multivariable-calculus', 'polar-coordinates']"
19,"What does ""Principal"" mean in ""Principal Unit Normal Vector""?","What does ""Principal"" mean in ""Principal Unit Normal Vector""?",,"When working with space curves, why is it called the ""Principal Unit Normal Vector""?  I know there are two.  So how do we know which one is the principal one?  Also, is there a principal unit binormal vector and principal unit tangent vector?  Why or why not?  What makes the naming scheme need the word principal for some and not others?  And what does ""principal"" mean?","When working with space curves, why is it called the ""Principal Unit Normal Vector""?  I know there are two.  So how do we know which one is the principal one?  Also, is there a principal unit binormal vector and principal unit tangent vector?  Why or why not?  What makes the naming scheme need the word principal for some and not others?  And what does ""principal"" mean?",,"['multivariable-calculus', 'differential-geometry', 'terminology']"
20,"Show differentiability of $f(x) = \sqrt{x^4 + y^4}$ in $(0,0)$",Show differentiability of  in,"f(x) = \sqrt{x^4 + y^4} (0,0)","I'm trying to show the differentiability of $$f:\mathbb R^2 \to \mathbb R\text;\quad f(x) = \sqrt{x^4 + y^4}$$ in (0,0). Here's my attempt: Since $\partial_xf(x,y) = \frac{4x^3}{2\sqrt{x^4+y^4}}$ we would divide be zero plugging in (0,0), so we need to compute the limit manually (same holds for $\partial_y f(x,y)$). Doing that, we get $$\lim \limits_{h\to 0} \frac{f(h,0) - f(0)}{h} = \lim \limits_{h\to 0} \frac{h^2}{h} = \lim \limits_{h\to 0} h = 0$$ (analog for $\partial_y f(x,y)$), hence $\partial_xf(0,0) = 0$. Now it remains to show that $\partial _xf(x,y)$ (resp. $\partial_yf(x,y)$ is continuous. This is true since $$\lim \limits_{(x,y)\to(0,0)} \partial_xf(x,y) = \lim \limits_{(x,y)\to(0,0)}  \frac{4x^3}{2\sqrt{x^4+y^4}} = 0 = \partial_xf(0,0)$$ (analog for the partial to y). Since both partials exist and are continuous, we're done. Now, is that attempt correct? If yes, did I do something I didn't necessarily have to do? Are there better/faster/easier methods to proof differentiability? ADDITIONAL QUESTION: I did not prove that $\lim \limits_{(x,y)\to(0,0)}  \frac{4x^3}{2\sqrt{x^4+y^4}} = 0$. Do I need to do that and how would I do that?","I'm trying to show the differentiability of $$f:\mathbb R^2 \to \mathbb R\text;\quad f(x) = \sqrt{x^4 + y^4}$$ in (0,0). Here's my attempt: Since $\partial_xf(x,y) = \frac{4x^3}{2\sqrt{x^4+y^4}}$ we would divide be zero plugging in (0,0), so we need to compute the limit manually (same holds for $\partial_y f(x,y)$). Doing that, we get $$\lim \limits_{h\to 0} \frac{f(h,0) - f(0)}{h} = \lim \limits_{h\to 0} \frac{h^2}{h} = \lim \limits_{h\to 0} h = 0$$ (analog for $\partial_y f(x,y)$), hence $\partial_xf(0,0) = 0$. Now it remains to show that $\partial _xf(x,y)$ (resp. $\partial_yf(x,y)$ is continuous. This is true since $$\lim \limits_{(x,y)\to(0,0)} \partial_xf(x,y) = \lim \limits_{(x,y)\to(0,0)}  \frac{4x^3}{2\sqrt{x^4+y^4}} = 0 = \partial_xf(0,0)$$ (analog for the partial to y). Since both partials exist and are continuous, we're done. Now, is that attempt correct? If yes, did I do something I didn't necessarily have to do? Are there better/faster/easier methods to proof differentiability? ADDITIONAL QUESTION: I did not prove that $\lim \limits_{(x,y)\to(0,0)}  \frac{4x^3}{2\sqrt{x^4+y^4}} = 0$. Do I need to do that and how would I do that?",,"['calculus', 'real-analysis', 'multivariable-calculus', 'derivatives']"
21,"Constructing a dense subset $ C \subseteq [0,1] \times [0,1] $ with a special property. [duplicate]",Constructing a dense subset  with a special property. [duplicate]," C \subseteq [0,1] \times [0,1] ","This question already has answers here : Boundary Question in $\mathbb{R}^{2}$ (Manifolds) (2 answers) Closed 4 years ago . Construct a subset $ C \subseteq [0,1] \times [0,1] $, dense in $[0,1] \times [0,1] $ that has the property of every horizontal or vertical line has at most one point of $C$. I think if I guarantee that the set I'm constructing has a point in every quarter of the square, then in every 16-portion of the squarte, then in every 4^n portion of the square it'll work. But my problem is writing this. Ant the second part of this problem is proving the following: Let $C$ be the created subset, $ \int_{0}^{1} ( \int_{0}^1 \chi (C) (x,y) dx ) dy =  \int_{0}^{1} ( \int_{0}^{1} \chi (C) (x,y) dy ) dx = 0$, but $\chi(C)$ is not integrable. Can I use Fubini theorem here? The set is 2-measurable, so the first equality should hold, right? But why is it equal to 0? Thanks!","This question already has answers here : Boundary Question in $\mathbb{R}^{2}$ (Manifolds) (2 answers) Closed 4 years ago . Construct a subset $ C \subseteq [0,1] \times [0,1] $, dense in $[0,1] \times [0,1] $ that has the property of every horizontal or vertical line has at most one point of $C$. I think if I guarantee that the set I'm constructing has a point in every quarter of the square, then in every 16-portion of the squarte, then in every 4^n portion of the square it'll work. But my problem is writing this. Ant the second part of this problem is proving the following: Let $C$ be the created subset, $ \int_{0}^{1} ( \int_{0}^1 \chi (C) (x,y) dx ) dy =  \int_{0}^{1} ( \int_{0}^{1} \chi (C) (x,y) dy ) dx = 0$, but $\chi(C)$ is not integrable. Can I use Fubini theorem here? The set is 2-measurable, so the first equality should hold, right? But why is it equal to 0? Thanks!",,['real-analysis']
22,Integral of a Gradient function (and another function?),Integral of a Gradient function (and another function?),,"I'm aware of line integrals around planes and curves, but I cannot make up how to approach this question: $$\int_C f∇f \cdot \,d\mathbf{r} $$ where $f(x,y,z)=xz\cos(x^2+y^2)$ and C is the intersection of the cylinder $x^2+y^2=1$ and $x+y+2z=2$ Suppose I find the intersection, does the integral constitute of a product of a gradient function and its function? If so, how do I approach this?","I'm aware of line integrals around planes and curves, but I cannot make up how to approach this question: $$\int_C f∇f \cdot \,d\mathbf{r} $$ where $f(x,y,z)=xz\cos(x^2+y^2)$ and C is the intersection of the cylinder $x^2+y^2=1$ and $x+y+2z=2$ Suppose I find the intersection, does the integral constitute of a product of a gradient function and its function? If so, how do I approach this?",,"['integration', 'multivariable-calculus']"
23,Partial derivative of x - is quotient rule necessary?,Partial derivative of x - is quotient rule necessary?,,"Let  $$u(x,y)=\frac{x}{x^2+y^2}$$ I'm trying to determine if the given function is harmonic. I know that the 2nd partial derivative with respect to $x$ should, when added to the 2nd partial derivative of $y$, equal $0$. However, I'm kind of stuck. I'm using the quotient rule to solve for the partial derivative of $x$, but is this the right way to take a partial derivative of a quotient?","Let  $$u(x,y)=\frac{x}{x^2+y^2}$$ I'm trying to determine if the given function is harmonic. I know that the 2nd partial derivative with respect to $x$ should, when added to the 2nd partial derivative of $y$, equal $0$. However, I'm kind of stuck. I'm using the quotient rule to solve for the partial derivative of $x$, but is this the right way to take a partial derivative of a quotient?",,"['multivariable-calculus', 'derivatives', 'partial-derivative']"
24,function has a solution : Derivative at a point has rank $n$,function has a solution : Derivative at a point has rank,n,"Let $f:\mathbb{R}^{k+n}\rightarrow \mathbb{R}^n$ be class $C^1$; suppose that $f(a)=0$ and that $Df(a)$ has rank $n$. Show that if $c$ is a point of $\mathbb{R}^n$ sufficiently close to $0$, then the equation $f(x)=c$ has a solution. Write $f=f(x,y)$ with $x\in \mathbb{R}^k$ and $y\in \mathbb{R}^n$ As rank of $Df(a)=n$, we see that $\det\frac{\partial f}{\partial y}(a)\neq 0$.. write $a=(a_1,a_2)$ where $a_2\in \mathbb{R}^n$ Then by implicit function thoerem, there exists $g:\mathbb{R}^k\rightarrow \mathbb{R}^n$ such that $f(a_1,g(a_1))=0$ We have $f(m,g(m))=0$ in a nbd around  $a_1$... I do not know how to use this to solve $f(x)=c$.. May be i should start with the function $h(x)=f(x)-c$ then there is a nbd around $a_1$ such that $h(m,g(g(m)))=f(m,g(m))-c=0$ Something like this works but i am not sure... I dont know why $c$ has to be sufficienlty close to $0$","Let $f:\mathbb{R}^{k+n}\rightarrow \mathbb{R}^n$ be class $C^1$; suppose that $f(a)=0$ and that $Df(a)$ has rank $n$. Show that if $c$ is a point of $\mathbb{R}^n$ sufficiently close to $0$, then the equation $f(x)=c$ has a solution. Write $f=f(x,y)$ with $x\in \mathbb{R}^k$ and $y\in \mathbb{R}^n$ As rank of $Df(a)=n$, we see that $\det\frac{\partial f}{\partial y}(a)\neq 0$.. write $a=(a_1,a_2)$ where $a_2\in \mathbb{R}^n$ Then by implicit function thoerem, there exists $g:\mathbb{R}^k\rightarrow \mathbb{R}^n$ such that $f(a_1,g(a_1))=0$ We have $f(m,g(m))=0$ in a nbd around  $a_1$... I do not know how to use this to solve $f(x)=c$.. May be i should start with the function $h(x)=f(x)-c$ then there is a nbd around $a_1$ such that $h(m,g(g(m)))=f(m,g(m))-c=0$ Something like this works but i am not sure... I dont know why $c$ has to be sufficienlty close to $0$",,"['multivariable-calculus', 'derivatives', 'implicit-function-theorem']"
25,Prove $xe^y + ye^x =0$ has no explicit solution $y(x)$,Prove  has no explicit solution,xe^y + ye^x =0 y(x),"By the implicit function theorem there exists a solution $ y(x)$ of $$xe^y + ye^x=0$$ in a neighborhood of $ (0,0)$. Nevertheless, my textbook asks me to ""observe that there is no way to write down an explicit solution $ y=y(x)$ in a neighborhood of the point $ (x_0, y_0) =(0,0)$."" How can I show this?","By the implicit function theorem there exists a solution $ y(x)$ of $$xe^y + ye^x=0$$ in a neighborhood of $ (0,0)$. Nevertheless, my textbook asks me to ""observe that there is no way to write down an explicit solution $ y=y(x)$ in a neighborhood of the point $ (x_0, y_0) =(0,0)$."" How can I show this?",,['multivariable-calculus']
26,Calculation of double integral.,Calculation of double integral.,,Calculate the double integral: $$\int_{0}^{\frac{\pi^3}{8}}\; dx \int _{\sqrt[3] x}^{\frac{\pi}{2}}\; \cos\frac{2x}{\pi y}\;dy\;.$$ Can someone hint how to approach this as we have to integrate with respect to y but y is in denominator. I think the right approach might be changing it into polar co-ordinates but I am not able to set the limits.,Calculate the double integral: $$\int_{0}^{\frac{\pi^3}{8}}\; dx \int _{\sqrt[3] x}^{\frac{\pi}{2}}\; \cos\frac{2x}{\pi y}\;dy\;.$$ Can someone hint how to approach this as we have to integrate with respect to y but y is in denominator. I think the right approach might be changing it into polar co-ordinates but I am not able to set the limits.,,"['calculus', 'multivariable-calculus']"
27,Deriving the Formula of Total Derivative for Multivariate Functions,Deriving the Formula of Total Derivative for Multivariate Functions,,"How is the formula for the total derivative with respect to x derived? The formula is: $$\large \frac{df}{dx}=\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial z}\frac{dz}{dx}...$$ Alternatively dividing both sides by $dx$, gives the formula for total differential: $$\large df=\frac{\partial f}{\partial x}dx+\frac{\partial f}{\partial y}dy+\frac{\partial f}{\partial z}dz...$$ Wikipedia references the use of chain rule, but never actually goes into deriving the formula. Additionally, the notation of $\large \frac{dy}{dx}$ boggles me a bit, what exactly does this refer to? Is it the case that it can be solved for only by holding the function itself constant?","How is the formula for the total derivative with respect to x derived? The formula is: $$\large \frac{df}{dx}=\frac{\partial f}{\partial x}+\frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial z}\frac{dz}{dx}...$$ Alternatively dividing both sides by $dx$, gives the formula for total differential: $$\large df=\frac{\partial f}{\partial x}dx+\frac{\partial f}{\partial y}dy+\frac{\partial f}{\partial z}dz...$$ Wikipedia references the use of chain rule, but never actually goes into deriving the formula. Additionally, the notation of $\large \frac{dy}{dx}$ boggles me a bit, what exactly does this refer to? Is it the case that it can be solved for only by holding the function itself constant?",,"['multivariable-calculus', 'implicit-differentiation']"
28,Using Lagrange multipliers to find the max and min,Using Lagrange multipliers to find the max and min,,"I have the equation $$f(x,y)=x^2+y^2$$ and the constraint $$(x-1)^2+4y^2=4$$ So I must find the min and max. My try: So I get the equations: $$2x=λ(2x-2)$$ $$2y=λ8y$$ and I get λ=1/4 and x=1/3 Any help is appreciated","I have the equation $$f(x,y)=x^2+y^2$$ and the constraint $$(x-1)^2+4y^2=4$$ So I must find the min and max. My try: So I get the equations: $$2x=λ(2x-2)$$ $$2y=λ8y$$ and I get λ=1/4 and x=1/3 Any help is appreciated",,"['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
29,"Lies, Damn Lies and,... Gradients?","Lies, Damn Lies and,... Gradients?",,"Help! I think I'm stuck in a local minimum and I can't get out! Ok that's not news, many people all over the world are stuck in local minima everyday. What is news is that in my case I know there is a path out, and I'm wondering if a gradient calculation will help me find it. Here is my situation,... Imagine a bowl-shaped surface centered at x = 0, y = 0. Now imagine that there is a path out of this bowl but that it lies perfectly on the diagonal (so where x = y). In this case I think the partial derivative in the x-direction will tell me that I'm at the bottom of a 'U' and that there is no where to go, whilst the partial derivative in the y-direction will also tell me the same. So in the end the gradient should also tell me that I'm stuck at the bottom of a bowl. Is this correct? In other words,... do gradients have blind spots ? Is it true that the 'gradient' calculation is in essence just an approximation based on 'sample' partial derivatives taken in the respective directions of the  coordinate system ? So if I change the coordinate system I might very well get a different value for the gradient even for the same point on the surface? Or is there another type of 'gradient' that I can calculate which will tell me how to get out of the bowl regardless of coordinate system? Thanks for your patience,... Terry","Help! I think I'm stuck in a local minimum and I can't get out! Ok that's not news, many people all over the world are stuck in local minima everyday. What is news is that in my case I know there is a path out, and I'm wondering if a gradient calculation will help me find it. Here is my situation,... Imagine a bowl-shaped surface centered at x = 0, y = 0. Now imagine that there is a path out of this bowl but that it lies perfectly on the diagonal (so where x = y). In this case I think the partial derivative in the x-direction will tell me that I'm at the bottom of a 'U' and that there is no where to go, whilst the partial derivative in the y-direction will also tell me the same. So in the end the gradient should also tell me that I'm stuck at the bottom of a bowl. Is this correct? In other words,... do gradients have blind spots ? Is it true that the 'gradient' calculation is in essence just an approximation based on 'sample' partial derivatives taken in the respective directions of the  coordinate system ? So if I change the coordinate system I might very well get a different value for the gradient even for the same point on the surface? Or is there another type of 'gradient' that I can calculate which will tell me how to get out of the bowl regardless of coordinate system? Thanks for your patience,... Terry",,['multivariable-calculus']
30,Maximize $xy^2$ on the ellipse $x^2+4y^2=4$,Maximize  on the ellipse,xy^2 x^2+4y^2=4,"I was using Lagrange multiplier, any steps gone wrong? $$f(x,y)=xy^2$$ $$c(x,y)=x^2+4y^2$$ Partial Derivatives $$\frac {\partial f}{\partial x} = y^2 $$ $$\frac {\partial f}{\partial y} = 2xy $$ $$\frac {\partial c}{\partial x} = 2x $$ $$\frac {\partial c}{\partial y} = 8y $$ Find Lambda $$y^2=2x\lambda$$ $$\lambda=\frac {y^2}{2x} $$ $$ 2xy=8y\lambda $$ $$\lambda= \frac{x}{4} $$ Let $$ \frac {y^2}{2x}=\frac {x}{4} $$ yields $$y=\frac {x}{\sqrt 2}$$ and $$y=-\frac {x}{\sqrt 2}$$ Bringing $$y=\frac {x}{\sqrt 2}$$ into C $$ x^2 + 2x^2 = 4 $$ $$ 3x^2=4 $$ yields $$ x=\frac {2}{\sqrt 3} and -\frac {2}{\sqrt 3} $$ Since $$ y=\frac {x}{\pm\sqrt 2} $$ plugging $x=\frac {2}{\pm\sqrt 3}$ into y I got 4 points: $(\frac {2}{\sqrt 3},\frac {2}{\sqrt 6}),(-\frac {2}{\sqrt 3},\frac {2}{\sqrt 6}),(-\frac {2}{\sqrt 3},-\frac {2}{\sqrt 6}),(\frac {2}{\sqrt 3},-\frac {2}{\sqrt 6})$ is every step ok up to this point?","I was using Lagrange multiplier, any steps gone wrong? $$f(x,y)=xy^2$$ $$c(x,y)=x^2+4y^2$$ Partial Derivatives $$\frac {\partial f}{\partial x} = y^2 $$ $$\frac {\partial f}{\partial y} = 2xy $$ $$\frac {\partial c}{\partial x} = 2x $$ $$\frac {\partial c}{\partial y} = 8y $$ Find Lambda $$y^2=2x\lambda$$ $$\lambda=\frac {y^2}{2x} $$ $$ 2xy=8y\lambda $$ $$\lambda= \frac{x}{4} $$ Let $$ \frac {y^2}{2x}=\frac {x}{4} $$ yields $$y=\frac {x}{\sqrt 2}$$ and $$y=-\frac {x}{\sqrt 2}$$ Bringing $$y=\frac {x}{\sqrt 2}$$ into C $$ x^2 + 2x^2 = 4 $$ $$ 3x^2=4 $$ yields $$ x=\frac {2}{\sqrt 3} and -\frac {2}{\sqrt 3} $$ Since $$ y=\frac {x}{\pm\sqrt 2} $$ plugging $x=\frac {2}{\pm\sqrt 3}$ into y I got 4 points: $(\frac {2}{\sqrt 3},\frac {2}{\sqrt 6}),(-\frac {2}{\sqrt 3},\frac {2}{\sqrt 6}),(-\frac {2}{\sqrt 3},-\frac {2}{\sqrt 6}),(\frac {2}{\sqrt 3},-\frac {2}{\sqrt 6})$ is every step ok up to this point?",,"['calculus', 'multivariable-calculus', 'lagrange-multiplier']"
31,Stokes Theorem. Where is my mistake?,Stokes Theorem. Where is my mistake?,,"Use Stoke's Theorem to prove that the following line integral has the indicated value. $$ \int_\mathscr{C} y \,dx +z\,dy+x\,dz = \pi a^2 \sqrt{3}$$ where $\mathscr{C}$ is the intersection curve $\mathscr{C}: \begin{cases} x^2+y^2+z^2=a^2 \\ x+y+z=0 \end{cases}$ This is what I did. $$ \mathbf{I} = \int_\mathscr{C} y \,dx +z\,dy+x\,dz = \int_\mathscr{C} \mathbf{F}(x,y,z) \cdot d\mathbf{\overrightarrow{r}} \qquad \text{where} \,\mathbf{F}(x,y,z)=(y,z,x)$$ Then, by the Stoke's theorem $$ \mathbf{I} =\iint_S(\nabla \times \mathbf{F}) \cdot\mathrm{d}\mathbf{S}= \iint_S (-1,-1,-1) \cdot \mathrm{d}\mathbf{S} $$ I choose $S$ as the plane that contains $\mathscr{C}$. $$ \mathbb{\alpha}(x,y)=(x,y,-x-y) \qquad(x,y)\in T \\ $$ In cylindrical coordinates $$ \mathbb{\alpha}(r,\theta)=\left( rcos\theta,rsin\theta,-r(cos\theta+ sin\theta)\right) \qquad (r,\theta)\in [0,a]\times[0,2\pi]=T^* \\  N= \frac{\partial\alpha}{\partial r} \times \frac{\partial\alpha}{\partial\theta}=(r,r,r)$$ Therefore $$ \mathbf{I}= \iint_{T^*} (-1,-1,-1) \cdot (r,r,r) \mathrm{d}A = \int_0^{2\pi} \int_0^a -3r \, \mathrm{d}r\mathrm{d}\theta = -3a^2\pi$$ Unfortunately, I can't find where is my mistake. Can you help me, please?","Use Stoke's Theorem to prove that the following line integral has the indicated value. $$ \int_\mathscr{C} y \,dx +z\,dy+x\,dz = \pi a^2 \sqrt{3}$$ where $\mathscr{C}$ is the intersection curve $\mathscr{C}: \begin{cases} x^2+y^2+z^2=a^2 \\ x+y+z=0 \end{cases}$ This is what I did. $$ \mathbf{I} = \int_\mathscr{C} y \,dx +z\,dy+x\,dz = \int_\mathscr{C} \mathbf{F}(x,y,z) \cdot d\mathbf{\overrightarrow{r}} \qquad \text{where} \,\mathbf{F}(x,y,z)=(y,z,x)$$ Then, by the Stoke's theorem $$ \mathbf{I} =\iint_S(\nabla \times \mathbf{F}) \cdot\mathrm{d}\mathbf{S}= \iint_S (-1,-1,-1) \cdot \mathrm{d}\mathbf{S} $$ I choose $S$ as the plane that contains $\mathscr{C}$. $$ \mathbb{\alpha}(x,y)=(x,y,-x-y) \qquad(x,y)\in T \\ $$ In cylindrical coordinates $$ \mathbb{\alpha}(r,\theta)=\left( rcos\theta,rsin\theta,-r(cos\theta+ sin\theta)\right) \qquad (r,\theta)\in [0,a]\times[0,2\pi]=T^* \\  N= \frac{\partial\alpha}{\partial r} \times \frac{\partial\alpha}{\partial\theta}=(r,r,r)$$ Therefore $$ \mathbf{I}= \iint_{T^*} (-1,-1,-1) \cdot (r,r,r) \mathrm{d}A = \int_0^{2\pi} \int_0^a -3r \, \mathrm{d}r\mathrm{d}\theta = -3a^2\pi$$ Unfortunately, I can't find where is my mistake. Can you help me, please?",,"['multivariable-calculus', 'vector-analysis']"
32,Using Lagrange's Method in Finding Extreme Values of $x^2 + y^2 + z^2$ for $\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$ (New to This Method),Using Lagrange's Method in Finding Extreme Values of  for  (New to This Method),x^2 + y^2 + z^2 \frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1,"Did I do this hw question correctly (at least in theory, I do not expect anyone to check my algebra work)? In particular, did I solve for lambda and plug lambda back into my equations for x,y, and z correctly, or is there a better way? Something does not feel right about it. Thanks! Apply Lagrange's method in finding the extreme values $x^2 + y^2 + z^2$ subject to the constraint $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$, where $a > b > c > 0$. Here is my work thus far: Let $f(x,y,z) = x^2 + y^2 + z^2$, and let  $g(x,y,z) - k = \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} - 1$. Then let $u=f(x,y,z) + \lambda(g(x,y,z) - k)$. This gives us: $$u=(x^2 + y^2 + z^2) + \lambda\frac{x^2}{a^2} + \lambda\frac{y^2}{b^2} + \lambda\frac{z^2}{c^2} - \lambda$$ Taking the partial of u with respect to each variable (including lambda) and setting it equal to zero, we get: $u_x = 2x + \frac{2\lambda x}{a^2} = 0$ $u_y = 2y + \frac{2\lambda y}{b^2} = 0$ $u_z = 2z + \frac{2\lambda z}{c^2} = 0$ $u_\lambda = \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$ Solving for x, y, and z, we get $x = -\frac{\lambda}{a^2},\ y = -\frac{\lambda}{b^2},\ z = -\frac{\lambda}{c^2}$, which we can plug into our partial, $u_\lambda$, for x, y, and z. In our partial, we get $$\frac{\lambda^2}{a^6} + \frac{\lambda^2}{b^6} + \frac{\lambda^2}{c^6} = 1$$ Solving for $\lambda$ gives us the  difficult solution of $$\lambda = \pm \frac{a^3b^3c^3}{\sqrt{a^6b^6 + a^6c^6 + b^6c^6}}$$ Since $a,\ b,\ c>0$, the plugging in the positive solution for $\lambda$ will give us our max and vice versa for the min (if the function were odd, see below). Plugging $\lambda$ into our x, y, and z equations yield: $x = \pm \frac{ab^3c^3}{\sqrt{a^6b^6 + a^6c^6 + b^6c^6}}$ $y = \pm \frac{a^3bc^3}{\sqrt{a^6b^6 + a^6c^6 + b^6c^6}}$ $z = \pm \frac{a^3b^3c}{\sqrt{a^6b^6 + a^6c^6 + b^6c^6}}$ Since f is even (each variable is raised to the second power), then either the positive or negative solution will give a positive result.","Did I do this hw question correctly (at least in theory, I do not expect anyone to check my algebra work)? In particular, did I solve for lambda and plug lambda back into my equations for x,y, and z correctly, or is there a better way? Something does not feel right about it. Thanks! Apply Lagrange's method in finding the extreme values $x^2 + y^2 + z^2$ subject to the constraint $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$, where $a > b > c > 0$. Here is my work thus far: Let $f(x,y,z) = x^2 + y^2 + z^2$, and let  $g(x,y,z) - k = \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} - 1$. Then let $u=f(x,y,z) + \lambda(g(x,y,z) - k)$. This gives us: $$u=(x^2 + y^2 + z^2) + \lambda\frac{x^2}{a^2} + \lambda\frac{y^2}{b^2} + \lambda\frac{z^2}{c^2} - \lambda$$ Taking the partial of u with respect to each variable (including lambda) and setting it equal to zero, we get: $u_x = 2x + \frac{2\lambda x}{a^2} = 0$ $u_y = 2y + \frac{2\lambda y}{b^2} = 0$ $u_z = 2z + \frac{2\lambda z}{c^2} = 0$ $u_\lambda = \frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2} = 1$ Solving for x, y, and z, we get $x = -\frac{\lambda}{a^2},\ y = -\frac{\lambda}{b^2},\ z = -\frac{\lambda}{c^2}$, which we can plug into our partial, $u_\lambda$, for x, y, and z. In our partial, we get $$\frac{\lambda^2}{a^6} + \frac{\lambda^2}{b^6} + \frac{\lambda^2}{c^6} = 1$$ Solving for $\lambda$ gives us the  difficult solution of $$\lambda = \pm \frac{a^3b^3c^3}{\sqrt{a^6b^6 + a^6c^6 + b^6c^6}}$$ Since $a,\ b,\ c>0$, the plugging in the positive solution for $\lambda$ will give us our max and vice versa for the min (if the function were odd, see below). Plugging $\lambda$ into our x, y, and z equations yield: $x = \pm \frac{ab^3c^3}{\sqrt{a^6b^6 + a^6c^6 + b^6c^6}}$ $y = \pm \frac{a^3bc^3}{\sqrt{a^6b^6 + a^6c^6 + b^6c^6}}$ $z = \pm \frac{a^3b^3c}{\sqrt{a^6b^6 + a^6c^6 + b^6c^6}}$ Since f is even (each variable is raised to the second power), then either the positive or negative solution will give a positive result.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'optimization', 'lagrange-multiplier']"
33,"Is this inequality true? If yes, for what functions?","Is this inequality true? If yes, for what functions?",,"Let $B=B(0,1)\subset \mathbb R^2$. Let $u$ be a radially symmetric differentiable function on $B$ and $v=Ax+b$ be a linear function where $A$ is a $2\times 2$ matrix satisfies $A=-A^T$, and $b=(b_1,b_2)$ is any constant. Define for $a=(a_1,a_2)\in\mathbb R^2$, $|a|=\sqrt{a_1^2+a_2^2}$. I want to prove $$ \int_B |\nabla u|\,dx\leq \int_B|\nabla u-Ax-b|\,dx. \tag 1 $$ I think this inequality probability is not true but I can not find a counterexample... Please help me to find a counterexample and if possible, what kind of assumptions should I add to $u$ so that equation $(1)$ is true? Thank you!","Let $B=B(0,1)\subset \mathbb R^2$. Let $u$ be a radially symmetric differentiable function on $B$ and $v=Ax+b$ be a linear function where $A$ is a $2\times 2$ matrix satisfies $A=-A^T$, and $b=(b_1,b_2)$ is any constant. Define for $a=(a_1,a_2)\in\mathbb R^2$, $|a|=\sqrt{a_1^2+a_2^2}$. I want to prove $$ \int_B |\nabla u|\,dx\leq \int_B|\nabla u-Ax-b|\,dx. \tag 1 $$ I think this inequality probability is not true but I can not find a counterexample... Please help me to find a counterexample and if possible, what kind of assumptions should I add to $u$ so that equation $(1)$ is true? Thank you!",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus']"
34,Is this function in $L^1_{loc}(\mathbb R^3)$,Is this function in,L^1_{loc}(\mathbb R^3),"It seems such a trivial question, but for whatever reason I don't understand. Let $u: \mathbb R^3 \to \mathbb R $ be $$u(x) = \frac 1{4\pi |x|}$$ The book says that $u \in L^1_{loc}(\mathbb R^3)$ which I don't understand,because in a compact subset containing $0$ that function is not inegrable! $u(x) = \frac 1{4\pi\sqrt{x_1^2 + x_2^2 + x_3^2}}$ and if for example $x_2 = x_3 = 0$ and our function is $\sim \frac 1{|x_1|}$ which is not integrable..","It seems such a trivial question, but for whatever reason I don't understand. Let $u: \mathbb R^3 \to \mathbb R $ be $$u(x) = \frac 1{4\pi |x|}$$ The book says that $u \in L^1_{loc}(\mathbb R^3)$ which I don't understand,because in a compact subset containing $0$ that function is not inegrable! $u(x) = \frac 1{4\pi\sqrt{x_1^2 + x_2^2 + x_3^2}}$ and if for example $x_2 = x_3 = 0$ and our function is $\sim \frac 1{|x_1|}$ which is not integrable..",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus', 'definite-integrals']"
35,"Critical points of $f(x,y) = \sin x \sin y, -\pi<x<\pi, -\pi<y<\pi$",Critical points of,"f(x,y) = \sin x \sin y, -\pi<x<\pi, -\pi<y<\pi","$\frac{df}{dx} = \cos x\sin y = 0$ $\frac{df}{dy} = \cos y\sin x = 0$ $\cos x\sin y = \cos y\sin x$ $\frac{\cos x}{\sin x} = \frac{\cos y}{\sin y}$ $\cot x = \cot y$ $P_1 = (\pi/2,\pi/2)$ $P_2 = (-\pi/2,\pi/2)$ $P_3 = (\pi/2,-\pi/2)$ $P_4 = (-\pi/2,-\pi/2)$ $P5 = (0,0)$ This gives me that $P_1$ and $P_4$ are maximum, $P_2$ and $P_3$ are minimum and $P_5$ is saddle. But when testing with Wolfram Alpha I get something different: http://www.wolframalpha.com/input/?i=local+maxima+senxseny+%2C+-pi%3Cx%3Cpi+%2C+-pi%3Cy%3Cpi http://www.wolframalpha.com/input/?i=local+minima+senxseny+%2C+-pi%3Cx%3Cpi+%2C+-pi%3Cy%3Cpi Did I do anything wrong? EDIT: Can people please stop removing the second wolfram link? They aren't the same, one is for maximum point and the other is for minimum.","$\frac{df}{dx} = \cos x\sin y = 0$ $\frac{df}{dy} = \cos y\sin x = 0$ $\cos x\sin y = \cos y\sin x$ $\frac{\cos x}{\sin x} = \frac{\cos y}{\sin y}$ $\cot x = \cot y$ $P_1 = (\pi/2,\pi/2)$ $P_2 = (-\pi/2,\pi/2)$ $P_3 = (\pi/2,-\pi/2)$ $P_4 = (-\pi/2,-\pi/2)$ $P5 = (0,0)$ This gives me that $P_1$ and $P_4$ are maximum, $P_2$ and $P_3$ are minimum and $P_5$ is saddle. But when testing with Wolfram Alpha I get something different: http://www.wolframalpha.com/input/?i=local+maxima+senxseny+%2C+-pi%3Cx%3Cpi+%2C+-pi%3Cy%3Cpi http://www.wolframalpha.com/input/?i=local+minima+senxseny+%2C+-pi%3Cx%3Cpi+%2C+-pi%3Cy%3Cpi Did I do anything wrong? EDIT: Can people please stop removing the second wolfram link? They aren't the same, one is for maximum point and the other is for minimum.",,['multivariable-calculus']
36,Confidence ellipse for a 2D gaussian,Confidence ellipse for a 2D gaussian,,"For a 1D gaussian, the interval +/- 1SD about the mean will comprise ~68% of the area under the curve.  Consider a 2D gaussian with a mean of zero and a diagonal covariance matrix (i.e., it is not rotated), with standard deviations $\sigma_x$ and $\sigma_y$.  Consider the ellipse drawn to pass through the points $(\pm\sigma_x, 0)$ and $(0, \pm\sigma_y)$.  Will it also contain 68% of the volume underneath the 2D curve?  I found this question which asks a more general version of this question, but I couldn't immediately tell from the reference given what the answer was.","For a 1D gaussian, the interval +/- 1SD about the mean will comprise ~68% of the area under the curve.  Consider a 2D gaussian with a mean of zero and a diagonal covariance matrix (i.e., it is not rotated), with standard deviations $\sigma_x$ and $\sigma_y$.  Consider the ellipse drawn to pass through the points $(\pm\sigma_x, 0)$ and $(0, \pm\sigma_y)$.  Will it also contain 68% of the volume underneath the 2D curve?  I found this question which asks a more general version of this question, but I couldn't immediately tell from the reference given what the answer was.",,"['multivariable-calculus', 'normal-distribution']"
37,Curl: invariant under change of basis or not?,Curl: invariant under change of basis or not?,,"I wondered how the curl$$\text{rot}\mathbf{F}=\left( \begin{array}{ccc}\partial_y F_3-\partial_z F_2 \\ \partial_z F_1-\partial_x F_3 \\ \partial_x F_2-\partial_y F_1 \end{array} \right)$$of a vector field $\mathbf{F}=(F_1,F_2,F_3)$ changes when the basis $\mathbb{R}^3$ is changed. I would have thought that it is invariant, because of the intuitive and physical interpretation of the curl and because, if I correctly understand other quantities typical of vector fields, like divergence, are (if I am not wrong $\text{div}\mathbf{F}$ is the trace of the Jacobian matrix $J_{\mathbf{F}}$ of $\mathbf{F}$, and the trace of $E J_{\mathbf{F}} E^{\text{T}}$ - see below - is the same of $J_{\mathbf{F}}$), but I have got serious problem to prove it. Trial : If I am not wrong, if $E\in\text{O}(3)$ is the basis change matrix, and if we define the function $\mathbf{G}$ as $\mathbf{y}\mapsto E\mathbf{F}(^t E \mathbf{y})$, invariancy is equivalent to $$E\text{rot}\mathbf{F}=\text{rot}\mathbf{G}$$please correct me if I am wrong. The Jacobian matrix $J_{\mathbf{G}}(\mathbf{y})$ of $\mathbf{G}$ in $\mathbf{y}$ should be:$$J_{\mathbf{G}}(\mathbf{y})=E J_{\mathbf{F}}(\mathbf{x}) E^{\text{T}}$$where $E^{\text{T}}$ is the transpose, and inverse, matrix of $E$. I have used such an identity to find the expression of $\text{rot}\mathbf{G}(\mathbf{y})$ in terms of the components of $J_{\mathbf{F}}(\mathbf{x})$ and $E$, but my calculations do not give me the expected result: for example, in the first component of the expression of $\text{rot}\mathbf{G}$ calculated in such a way , the coefficient of $\partial_x F_2$ is $(e_{21}e_{32}-e_{22}e_{31})$, while, in the first component of $E\text{rot}\mathbf{F}$, the coefficient of $\partial_x F_2$ is $e_{13}$... Is the curl invariant under a change of orthogonal basis and, if it is, how can it be correctly proved? Thank you so much for any answer!","I wondered how the curl$$\text{rot}\mathbf{F}=\left( \begin{array}{ccc}\partial_y F_3-\partial_z F_2 \\ \partial_z F_1-\partial_x F_3 \\ \partial_x F_2-\partial_y F_1 \end{array} \right)$$of a vector field $\mathbf{F}=(F_1,F_2,F_3)$ changes when the basis $\mathbb{R}^3$ is changed. I would have thought that it is invariant, because of the intuitive and physical interpretation of the curl and because, if I correctly understand other quantities typical of vector fields, like divergence, are (if I am not wrong $\text{div}\mathbf{F}$ is the trace of the Jacobian matrix $J_{\mathbf{F}}$ of $\mathbf{F}$, and the trace of $E J_{\mathbf{F}} E^{\text{T}}$ - see below - is the same of $J_{\mathbf{F}}$), but I have got serious problem to prove it. Trial : If I am not wrong, if $E\in\text{O}(3)$ is the basis change matrix, and if we define the function $\mathbf{G}$ as $\mathbf{y}\mapsto E\mathbf{F}(^t E \mathbf{y})$, invariancy is equivalent to $$E\text{rot}\mathbf{F}=\text{rot}\mathbf{G}$$please correct me if I am wrong. The Jacobian matrix $J_{\mathbf{G}}(\mathbf{y})$ of $\mathbf{G}$ in $\mathbf{y}$ should be:$$J_{\mathbf{G}}(\mathbf{y})=E J_{\mathbf{F}}(\mathbf{x}) E^{\text{T}}$$where $E^{\text{T}}$ is the transpose, and inverse, matrix of $E$. I have used such an identity to find the expression of $\text{rot}\mathbf{G}(\mathbf{y})$ in terms of the components of $J_{\mathbf{F}}(\mathbf{x})$ and $E$, but my calculations do not give me the expected result: for example, in the first component of the expression of $\text{rot}\mathbf{G}$ calculated in such a way , the coefficient of $\partial_x F_2$ is $(e_{21}e_{32}-e_{22}e_{31})$, while, in the first component of $E\text{rot}\mathbf{F}$, the coefficient of $\partial_x F_2$ is $e_{13}$... Is the curl invariant under a change of orthogonal basis and, if it is, how can it be correctly proved? Thank you so much for any answer!",,"['linear-algebra', 'multivariable-calculus', 'multivalued-functions']"
38,Why does curl($F$)=$0$ $\iff$ $F$ is conservative?,Why does curl()=   is conservative?,F 0 \iff F,Why is it true that$$\displaystyle curl (\vec{F})=0 \iff \vec{F} \text{ is conservative}$$ i.e. $$\displaystyle \exists f \text{ s.t. }\nabla f=\vec{F}$$,Why is it true that$$\displaystyle curl (\vec{F})=0 \iff \vec{F} \text{ is conservative}$$ i.e. $$\displaystyle \exists f \text{ s.t. }\nabla f=\vec{F}$$,,['multivariable-calculus']
39,intuition behind saddle point,intuition behind saddle point,,"Recently,I'm studying multivariable calculus. One of my friend said to me that the graph of $z=y^2-x^2$ has a saddle point .I  don't understand the concept of saddle point with intuition.Can anyone explain the concept of saddle point with intuition and give reasons why the graph of $z=y^2-x^2$ has a saddle point?","Recently,I'm studying multivariable calculus. One of my friend said to me that the graph of $z=y^2-x^2$ has a saddle point .I  don't understand the concept of saddle point with intuition.Can anyone explain the concept of saddle point with intuition and give reasons why the graph of $z=y^2-x^2$ has a saddle point?",,['multivariable-calculus']
40,Why isn't $d\mathbf{A}$ normalized in Stokes' theorem?,Why isn't  normalized in Stokes' theorem?,d\mathbf{A},"For a nice curve $C$ which is a boundary of a smooth surface $D$, Stokes' theorem says that $$\begin{align*} \oint_C \mathbf{F}\cdot d \mathbf{s} = \iint_D (\nabla \times \mathbf{F} )\cdot d\mathbf{A} \end{align*}$$ However, I don't think I understand exactly what is going on with the $d\mathbf{A}$ term. I would think that it is the vector of magnitude differential area $dA$ and direction $\hat{n}$, where $\hat{n}$ is the unit vector in the direction perpendicular to the orientation of $dA$. However, by reading practice problems online, I see that this is not the case. What exactly is the definition of this $d\mathbf{A}$? In addition, many sources online show that after evaluating the dot product $(\nabla \times \mathbf{F} )\cdot \mathbf{A}$ (no $dA$ term), the surface integral is magically done only in the $xy$ plane, rather than any other randomly defined plane. Is this legal?","For a nice curve $C$ which is a boundary of a smooth surface $D$, Stokes' theorem says that $$\begin{align*} \oint_C \mathbf{F}\cdot d \mathbf{s} = \iint_D (\nabla \times \mathbf{F} )\cdot d\mathbf{A} \end{align*}$$ However, I don't think I understand exactly what is going on with the $d\mathbf{A}$ term. I would think that it is the vector of magnitude differential area $dA$ and direction $\hat{n}$, where $\hat{n}$ is the unit vector in the direction perpendicular to the orientation of $dA$. However, by reading practice problems online, I see that this is not the case. What exactly is the definition of this $d\mathbf{A}$? In addition, many sources online show that after evaluating the dot product $(\nabla \times \mathbf{F} )\cdot \mathbf{A}$ (no $dA$ term), the surface integral is magically done only in the $xy$ plane, rather than any other randomly defined plane. Is this legal?",,['multivariable-calculus']
41,Prove $f$ is uniformly continuous,Prove  is uniformly continuous,f,"I have a question about uniform continuity and want to verify what I did is correct. The question is Suppose that $f : \mathbb{R}^2 \rightarrow \mathbb{R}$ and all first partial derivatives exist and bounded. Prove $f$ is uniformly continuous or give counter example. I believe this is true statement. I tried this as follows. Proof. Since the first partial derivatives of $f$ exist and bounded, then $\|\nabla f\| \leq M$ for some real number $M$. The Mean Value Theorem says that there exists $c \in \text{dom}(f)$ such that $f(x)-f(y) = \nabla f(c)(x-y)$. Caucy inequality shows that $\|f(x) - f(y)\| \leq \|\nabla f(c) \| \|x-y\| \leq M \|x-y\|$. Let $\delta = \frac{\epsilon}{M} > 0$ and $\epsilon > 0$. Then, for every $\epsilon > 0$, there exists $\delta > 0$ such that $\forall x,y \in \text{dom}(f)$, $\|x-y\| < \delta$ implies $\|f(x)-f(y)\| \leq M \|x-y\| < M \delta = \epsilon$. Can anyone verify that this is correct?? It seems that it is not formally stated but I tried my best. Also one thing I am not sure about is that if I can use the mean value theorem for the function defined on $\mathbb{R}^2$. One variable mean value theorem can be applied to a function defined on a closed interval such as $[a,b]$. The textbook I am using defines the mean value theorem on a line segment with two end points $a,b$. So I was confused if I can use the mean value theorem to a function defined on $\mathbb{R}^2$.","I have a question about uniform continuity and want to verify what I did is correct. The question is Suppose that $f : \mathbb{R}^2 \rightarrow \mathbb{R}$ and all first partial derivatives exist and bounded. Prove $f$ is uniformly continuous or give counter example. I believe this is true statement. I tried this as follows. Proof. Since the first partial derivatives of $f$ exist and bounded, then $\|\nabla f\| \leq M$ for some real number $M$. The Mean Value Theorem says that there exists $c \in \text{dom}(f)$ such that $f(x)-f(y) = \nabla f(c)(x-y)$. Caucy inequality shows that $\|f(x) - f(y)\| \leq \|\nabla f(c) \| \|x-y\| \leq M \|x-y\|$. Let $\delta = \frac{\epsilon}{M} > 0$ and $\epsilon > 0$. Then, for every $\epsilon > 0$, there exists $\delta > 0$ such that $\forall x,y \in \text{dom}(f)$, $\|x-y\| < \delta$ implies $\|f(x)-f(y)\| \leq M \|x-y\| < M \delta = \epsilon$. Can anyone verify that this is correct?? It seems that it is not formally stated but I tried my best. Also one thing I am not sure about is that if I can use the mean value theorem for the function defined on $\mathbb{R}^2$. One variable mean value theorem can be applied to a function defined on a closed interval such as $[a,b]$. The textbook I am using defines the mean value theorem on a line segment with two end points $a,b$. So I was confused if I can use the mean value theorem to a function defined on $\mathbb{R}^2$.",,"['real-analysis', 'multivariable-calculus', 'uniform-continuity']"
42,Having problems deriving coordinate expression for Lie derivative,Having problems deriving coordinate expression for Lie derivative,,"I am having serious problems in deriving the coordinate component expression for Lie derivative of vector fields. I already know how to do that using an outdated coordinate-based approach mostly used in old physics literature, and I also know I can do this in a more simple way by using a coordinate system adapted to my vector field, I want to get the correct form using the definition of the Lie derivative. Some of this might be because of my low understanding of flows. Let $M$ be a real, $n$-dimensional, $C^\infty$ manifold, let $X$ and $V$ be smooth vector fields defined in the neighborhood of a point $p\in M$, and let $ (U,x) $ be a chart so that $p\in U$ and $x(p)=(x^1(p),...,x^n(p))$. Let be $V=V^\mu \partial/\partial x^\mu$ and $X=X^\mu\partial/\partial x^\mu$ in $U$. Let $\Phi^X_t$ be the flow of $X$. If I understand this well, then $$ \frac{d}{dt}\Phi^X_t(p)=X(p). $$ The Lie-derivative should be $$ \left.\mathcal{L}_XV\right|_p=\frac{d}{dt}(\Phi^X_{-t})_*(\left.V\right|_{\Phi^X_t(p)}). $$ My attempt was as follows: $$ \mathcal{L}_XV|_p[f]=\frac{d}{dt}(V|_{\Phi^X_t(p)}[f\circ\Phi^X_{-t}])= \\ =\frac{d}{dt}(V|_{\Phi^X_t(p)}[f\circ x^{-1}\circ x\circ\Phi^X_{-t}]), $$ then, I try to evaluate the vector field on that massive composition by $$ \frac{d}{dt}(V^\mu(\Phi^X_t)\left.\frac{\partial}{\partial x^\mu}\right|_{x(\Phi^X_t)}(f\circ x^{-1}\circ x\circ\Phi^X_{-t}))= \\ =\frac{d}{dt}(V^\mu(\Phi^X_t)\left.\frac{\partial}{\partial x^\mu}\right|_{x(\Phi^X_{-t})}(f\circ x^{-1})\frac{\partial}{\partial x^\nu}(x\circ\Phi^X_{-t})), $$ and then what? I am not even sure that where I wrote $\partial/\partial x^\nu$, should I have written $d/dt$ instead? In either case, I have no idea how to go any further, and any help is greatly appreciated.","I am having serious problems in deriving the coordinate component expression for Lie derivative of vector fields. I already know how to do that using an outdated coordinate-based approach mostly used in old physics literature, and I also know I can do this in a more simple way by using a coordinate system adapted to my vector field, I want to get the correct form using the definition of the Lie derivative. Some of this might be because of my low understanding of flows. Let $M$ be a real, $n$-dimensional, $C^\infty$ manifold, let $X$ and $V$ be smooth vector fields defined in the neighborhood of a point $p\in M$, and let $ (U,x) $ be a chart so that $p\in U$ and $x(p)=(x^1(p),...,x^n(p))$. Let be $V=V^\mu \partial/\partial x^\mu$ and $X=X^\mu\partial/\partial x^\mu$ in $U$. Let $\Phi^X_t$ be the flow of $X$. If I understand this well, then $$ \frac{d}{dt}\Phi^X_t(p)=X(p). $$ The Lie-derivative should be $$ \left.\mathcal{L}_XV\right|_p=\frac{d}{dt}(\Phi^X_{-t})_*(\left.V\right|_{\Phi^X_t(p)}). $$ My attempt was as follows: $$ \mathcal{L}_XV|_p[f]=\frac{d}{dt}(V|_{\Phi^X_t(p)}[f\circ\Phi^X_{-t}])= \\ =\frac{d}{dt}(V|_{\Phi^X_t(p)}[f\circ x^{-1}\circ x\circ\Phi^X_{-t}]), $$ then, I try to evaluate the vector field on that massive composition by $$ \frac{d}{dt}(V^\mu(\Phi^X_t)\left.\frac{\partial}{\partial x^\mu}\right|_{x(\Phi^X_t)}(f\circ x^{-1}\circ x\circ\Phi^X_{-t}))= \\ =\frac{d}{dt}(V^\mu(\Phi^X_t)\left.\frac{\partial}{\partial x^\mu}\right|_{x(\Phi^X_{-t})}(f\circ x^{-1})\frac{\partial}{\partial x^\nu}(x\circ\Phi^X_{-t})), $$ and then what? I am not even sure that where I wrote $\partial/\partial x^\nu$, should I have written $d/dt$ instead? In either case, I have no idea how to go any further, and any help is greatly appreciated.",,"['multivariable-calculus', 'differential-geometry', 'lie-derivative']"
43,"What vectors in the plane z=x are orthogonal to $(1,-1,0)$?",What vectors in the plane z=x are orthogonal to ?,"(1,-1,0)","I believe all such vectors should be of the form $(a,b,a)$. Hence,  $$ (a,b,a) \cdot (1,-1,0) = a - b + 0 = 0 \implies a=b$$ So all the vectors we seek are of the form $$ (a,a,a)$$ where $a \in \mathbb{R}$. Is this the right approach?","I believe all such vectors should be of the form $(a,b,a)$. Hence,  $$ (a,b,a) \cdot (1,-1,0) = a - b + 0 = 0 \implies a=b$$ So all the vectors we seek are of the form $$ (a,a,a)$$ where $a \in \mathbb{R}$. Is this the right approach?",,"['multivariable-calculus', 'proof-verification']"
44,Approaching $\infty$ in $\mathbb R^n ; n=2$ or higher.,Approaching  in  or higher.,\infty \mathbb R^n ; n=2,"say I have a double limit in the sense of having a function from $\mathbb R^2 \rightarrow \mathbb R$ in which there are two variables approaching infinity:. $$\lim_{n,m \to \infty} f(m,n) $$ I am thinking specifically to the expression : $$\lim_{n,m \to \infty}  \left(1-\frac{c}{m}\right)^n $$ for $c$ a Real constant. First of all, how does one approach infinity in $\mathbb R^n ; n \geq2 $? Do we work with 1-point compactifications and then we demand that the values of each of $m,n$ be eventually in a neighborhood of $\infty$  (in the 1-pt compactification, the neighborhoods of $\infty$ are the complements of compact sets)? Secondly, for the limit, say $L$ to exist, do we require that the above expression $f(m,n)$ approach $L$ in a $\delta -\epsilon$ sense, no matter how $m,n$ approach infinity? Thanks.","say I have a double limit in the sense of having a function from $\mathbb R^2 \rightarrow \mathbb R$ in which there are two variables approaching infinity:. $$\lim_{n,m \to \infty} f(m,n) $$ I am thinking specifically to the expression : $$\lim_{n,m \to \infty}  \left(1-\frac{c}{m}\right)^n $$ for $c$ a Real constant. First of all, how does one approach infinity in $\mathbb R^n ; n \geq2 $? Do we work with 1-point compactifications and then we demand that the values of each of $m,n$ be eventually in a neighborhood of $\infty$  (in the 1-pt compactification, the neighborhoods of $\infty$ are the complements of compact sets)? Secondly, for the limit, say $L$ to exist, do we require that the above expression $f(m,n)$ approach $L$ in a $\delta -\epsilon$ sense, no matter how $m,n$ approach infinity? Thanks.",,"['real-analysis', 'multivariable-calculus']"
45,"How is Loomis and Sternberg's ""Advanced Calculus"" as an introductory analysis text?","How is Loomis and Sternberg's ""Advanced Calculus"" as an introductory analysis text?",,"Will it be a good substitute for a standard mathematical analysis text like Baby Rudin or should I buy both of them to avoid any gaps in my knowledge before progressing any further? (P.S., what exactly does ""advanced calculus"" mean? Every single book I've seen on ""advanced calculus"" seems to have vastly differing table of contents.)","Will it be a good substitute for a standard mathematical analysis text like Baby Rudin or should I buy both of them to avoid any gaps in my knowledge before progressing any further? (P.S., what exactly does ""advanced calculus"" mean? Every single book I've seen on ""advanced calculus"" seems to have vastly differing table of contents.)",,"['calculus', 'real-analysis', 'multivariable-calculus', 'vector-analysis']"
46,Comparison of the change of variable theorem,Comparison of the change of variable theorem,,"I would like to compare the change of variable theorem for 1 variable and more. What are the differences, in which case we need stronger assumptions? How do they differ? What is the best way to write the theorems for comparison? Multivariable: Let $\varphi : \Omega \rightarrow \mathbb{R}^n$ (where $\Omega \subset \mathbb{R}^k$ and $k\leq n)$ be injective and differentiable with continuous partial derivatives. Then for every $f: \varphi(\Omega) \rightarrow \mathbb{R}^n:$ $$\int_{\varphi(\Omega)} f(y) dy=\int_\Omega f(\varphi(x))|J\varphi(x)| dx,  $$ One variable: If $\varphi:[a,b] \rightarrow I \subset \mathbb{R}$ has continuos derivative. Let $f:I \rightarrow \mathbb{R}.$ Then:  $$\int_{\varphi(a)}^{\varphi(b)} f(y) dy=\int_{a}^{b} f(\varphi(x))\varphi'(x) dx$$ I think that the $1-1$ function in multivariable is an important difference, but I don't know why I need this assumption?","I would like to compare the change of variable theorem for 1 variable and more. What are the differences, in which case we need stronger assumptions? How do they differ? What is the best way to write the theorems for comparison? Multivariable: Let $\varphi : \Omega \rightarrow \mathbb{R}^n$ (where $\Omega \subset \mathbb{R}^k$ and $k\leq n)$ be injective and differentiable with continuous partial derivatives. Then for every $f: \varphi(\Omega) \rightarrow \mathbb{R}^n:$ $$\int_{\varphi(\Omega)} f(y) dy=\int_\Omega f(\varphi(x))|J\varphi(x)| dx,  $$ One variable: If $\varphi:[a,b] \rightarrow I \subset \mathbb{R}$ has continuos derivative. Let $f:I \rightarrow \mathbb{R}.$ Then:  $$\int_{\varphi(a)}^{\varphi(b)} f(y) dy=\int_{a}^{b} f(\varphi(x))\varphi'(x) dx$$ I think that the $1-1$ function in multivariable is an important difference, but I don't know why I need this assumption?",,"['calculus', 'real-analysis', 'integration', 'multivariable-calculus']"
47,quadratic form corresponding to function at critical point is positive definite implies local minimum,quadratic form corresponding to function at critical point is positive definite implies local minimum,,"Let $f: \mathbb{R}^n \to \mathbb{R}$ be a $C^3$ function. Have $x_0$ be a critical point of $f$. How would I go about proving that if the quadratic form $q(h)$ corresponding to $f$ at $x_0$ is positive-definite, then $f(x_0)$ is a local minimum? I think I will probably have to push some epsilons and utilize some form of Taylor's theorem. Any help would be appreciated. Thanks!","Let $f: \mathbb{R}^n \to \mathbb{R}$ be a $C^3$ function. Have $x_0$ be a critical point of $f$. How would I go about proving that if the quadratic form $q(h)$ corresponding to $f$ at $x_0$ is positive-definite, then $f(x_0)$ is a local minimum? I think I will probably have to push some epsilons and utilize some form of Taylor's theorem. Any help would be appreciated. Thanks!",,"['calculus', 'real-analysis']"
48,"Finding $ \frac{\partial z}{\partial x}$ and $\frac{\partial z}{\partial y} $ if $ F(cx - az, cy-bz) = 0 $",Finding  and  if," \frac{\partial z}{\partial x} \frac{\partial z}{\partial y}   F(cx - az, cy-bz) = 0 ","If it is given that $ F(cx - az, cy-bz) = 0 $ , then find $ \dfrac{\partial z}{\partial x}$ and $\dfrac{\partial z}{\partial y}$ . How do I go about doing this? I don't really understand which variables are independent and which aren't. I'm new to partial derivatives. I know chain rule though. Some hints? As per Blah's answer, here's what I've done: $$ F_x = F_u (c - az_x) + F_v(-bz_x) = 0 $$ $$ F_y = F_u (- az_y) + F_v(c -bz_y) = 0 $$ (The "" $=0$ "" is there because $F$ is constant (zero) for all values of $x$ and $y$ ) So, by doing some algebra, I get $z_x = \dfrac{cF_u} { bF_v + aF_u }$ and something similar for $z_y$ . But the above contains $F_u$ and $F_v$ . How do I get rid of those?","If it is given that , then find and . How do I go about doing this? I don't really understand which variables are independent and which aren't. I'm new to partial derivatives. I know chain rule though. Some hints? As per Blah's answer, here's what I've done: (The "" "" is there because is constant (zero) for all values of and ) So, by doing some algebra, I get and something similar for . But the above contains and . How do I get rid of those?"," F(cx - az, cy-bz) = 0   \dfrac{\partial z}{\partial x} \dfrac{\partial z}{\partial y}  F_x = F_u (c - az_x) + F_v(-bz_x) = 0   F_y = F_u (- az_y) + F_v(c -bz_y) = 0  =0 F x y z_x = \dfrac{cF_u} { bF_v + aF_u } z_y F_u F_v","['multivariable-calculus', 'partial-derivative']"
49,Question related to Lagrange multipliers,Question related to Lagrange multipliers,,"I am stuck with the following problem: A is symmetric $n\times n$ matrix and $f(x)=(Ax)x$ for $x\in {\bf R}^n$. I need to show that the maximum and the minimum values of $f$ on the unit sphere ${x: |x|=1}$ are the largest and the smallest eigenvalues of $A$. Here is how far I could go : Let $e_1,e_2...e_n$ be a basis for A with eigenvalues $\lambda_1,\lambda_2,...\lambda_n$ Set $x=\sum a_je_j$ and $A=\sum \lambda_je_j$. Then, $x^2=\sum a_j^2e_j^2$ and $(Ax)x=\sum \lambda_ja_j^2$. If this is right, then I have reduced the problem to minimizing and maximizing $(Ax)x=\sum \lambda_ja_j^2$ with respect to $x^2=\sum a_j^2e_j^2=1$. I need to use to proceed, I guess, Lagrange multipliers, but I do not know how to do it. Could someone please check my ideas and help me to solve this problem finally? Thanks in advance!","I am stuck with the following problem: A is symmetric $n\times n$ matrix and $f(x)=(Ax)x$ for $x\in {\bf R}^n$. I need to show that the maximum and the minimum values of $f$ on the unit sphere ${x: |x|=1}$ are the largest and the smallest eigenvalues of $A$. Here is how far I could go : Let $e_1,e_2...e_n$ be a basis for A with eigenvalues $\lambda_1,\lambda_2,...\lambda_n$ Set $x=\sum a_je_j$ and $A=\sum \lambda_je_j$. Then, $x^2=\sum a_j^2e_j^2$ and $(Ax)x=\sum \lambda_ja_j^2$. If this is right, then I have reduced the problem to minimizing and maximizing $(Ax)x=\sum \lambda_ja_j^2$ with respect to $x^2=\sum a_j^2e_j^2=1$. I need to use to proceed, I guess, Lagrange multipliers, but I do not know how to do it. Could someone please check my ideas and help me to solve this problem finally? Thanks in advance!",,"['real-analysis', 'multivariable-calculus', 'lagrange-multiplier']"
50,Sketch parametric curve,Sketch parametric curve,,"An exercise in my textbook asks to sketch the parametrical curve of the following equation: $$x=e^t\cdot\cos(t)\\y=e^t\cdot\sin(t)\\t\ge0$$ I would usually try to solve one of the equations for t and replace on the other one, or even solve for $\cos(t)$ and $\sin(t)$ and do something like $\cos^2(t)+\sin^2(t)=1$, but both of them wouldn't work... Can anybody help me?","An exercise in my textbook asks to sketch the parametrical curve of the following equation: $$x=e^t\cdot\cos(t)\\y=e^t\cdot\sin(t)\\t\ge0$$ I would usually try to solve one of the equations for t and replace on the other one, or even solve for $\cos(t)$ and $\sin(t)$ and do something like $\cos^2(t)+\sin^2(t)=1$, but both of them wouldn't work... Can anybody help me?",,"['calculus', 'multivariable-calculus']"
51,What is the domain of this multivariable function?,What is the domain of this multivariable function?,,"Let $h(x,y,z) = (z^2 -xz + zy -xy)^{1/4}$. What is the domain on this function? I know that \begin{align*} z^2 -xz + zy -xy \geq 0 \\ \implies z(x+y) -x(z+y) \geq 0 \\ \implies (z-x)(z+y) \geq 0 \end{align*} So is $D(h) = \{ (x,y,z) \in \mathbb{R}^3 | (z-x)(z+y) \geq 0 \}$ sufficient? Also how can this be described in words? Is it just a pair of intersecting planes?","Let $h(x,y,z) = (z^2 -xz + zy -xy)^{1/4}$. What is the domain on this function? I know that \begin{align*} z^2 -xz + zy -xy \geq 0 \\ \implies z(x+y) -x(z+y) \geq 0 \\ \implies (z-x)(z+y) \geq 0 \end{align*} So is $D(h) = \{ (x,y,z) \in \mathbb{R}^3 | (z-x)(z+y) \geq 0 \}$ sufficient? Also how can this be described in words? Is it just a pair of intersecting planes?",,['multivariable-calculus']
52,If a partial derivative exists at a point is then do all directional derivatives exist as well?,If a partial derivative exists at a point is then do all directional derivatives exist as well?,,"I thought that if partial derivatives exist then gradient also exist ,then all direction derivatives should also exist . is this true and  if it is not then why am i wrong ? $D_u=▽.u$ where  ▽ is the gradient and u is the direction vector along which you need the directional derivative.","I thought that if partial derivatives exist then gradient also exist ,then all direction derivatives should also exist . is this true and  if it is not then why am i wrong ? where  ▽ is the gradient and u is the direction vector along which you need the directional derivative.",D_u=▽.u,['multivariable-calculus']
53,System of equations in Lagrange multiplier problem,System of equations in Lagrange multiplier problem,,"Continuing from Confounding Lagrange multiplier problem : I'm having trouble solving the system of equations below arisen from a Lagrange multiplier problem where we are to optimize $f(x,y,z) = 4x^2 + 3y^2 + 5z^2$ over $g(x,y,z) = xy + 2yz + 3xz = 6$. $$ \begin{cases} 8x = \lambda (y + 3z) \\ 6y = \lambda (x + 2z) \\ 10 z = \lambda (2y + 3x) \\ xy + 2yz + 3xz = 6 \end{cases} $$ One suggestion I have got is to eliminate the terms $xy$, $yz$ and $xz$, however I have been unable to figure out how to do so. Help much appreciated!","Continuing from Confounding Lagrange multiplier problem : I'm having trouble solving the system of equations below arisen from a Lagrange multiplier problem where we are to optimize $f(x,y,z) = 4x^2 + 3y^2 + 5z^2$ over $g(x,y,z) = xy + 2yz + 3xz = 6$. $$ \begin{cases} 8x = \lambda (y + 3z) \\ 6y = \lambda (x + 2z) \\ 10 z = \lambda (2y + 3x) \\ xy + 2yz + 3xz = 6 \end{cases} $$ One suggestion I have got is to eliminate the terms $xy$, $yz$ and $xz$, however I have been unable to figure out how to do so. Help much appreciated!",,"['calculus', 'multivariable-calculus', 'optimization', 'lagrange-multiplier']"
54,Verification of the Stokes theorem for the surface that is a part of a cone,Verification of the Stokes theorem for the surface that is a part of a cone,,"Let $S$ consist of the part of the cone $z=(x^2+y^2)^{1/2}$ for $x^2+y^2\leq9$ and suppose $${\bf A}=(-y,x,-xyz).$$ Verify that Stokes theorem is satisfied for this choice of $\bf A$ and $S$. In the solution to this question in order to work out the surface integral you can project on to $z=3$ and evaluate over the region $x^2+y^2\leq9$. I was just wondering whether or not you could do the same but this time project onto the plane $z=0$?","Let $S$ consist of the part of the cone $z=(x^2+y^2)^{1/2}$ for $x^2+y^2\leq9$ and suppose $${\bf A}=(-y,x,-xyz).$$ Verify that Stokes theorem is satisfied for this choice of $\bf A$ and $S$. In the solution to this question in order to work out the surface integral you can project on to $z=3$ and evaluate over the region $x^2+y^2\leq9$. I was just wondering whether or not you could do the same but this time project onto the plane $z=0$?",,['multivariable-calculus']
55,"Integrating $ \int_0^2 \int_0^ \sqrt{1-(x-1)^2} \frac{x+y}{x^2+y^2} dy\,dx$ in polar coordinates",Integrating  in polar coordinates," \int_0^2 \int_0^ \sqrt{1-(x-1)^2} \frac{x+y}{x^2+y^2} dy\,dx","I'm having a problem integrating $ \displaystyle\int_0^2  \int_0^ \sqrt{1-(x-1)^2} \frac{x+y}{x^2+y^2} \,dy\,dx$. I drew the graph, and it looks like half a circle on top of the $x$ axis. I tried dividing it into two parts, with Area 1: $0\leq \theta\leq\frac{ \pi }{4} $ with $\csc\theta\leq r\leq 2\cos\theta$ and Area 2: $\frac{ \pi }{4}\leq \theta\leq\frac{ \pi }{2} $, but I dont know how to set r boundaries with this. Is it just $0\leq r\leq \csc\theta$? no.... this can't be right.... Could someone please help me out?","I'm having a problem integrating $ \displaystyle\int_0^2  \int_0^ \sqrt{1-(x-1)^2} \frac{x+y}{x^2+y^2} \,dy\,dx$. I drew the graph, and it looks like half a circle on top of the $x$ axis. I tried dividing it into two parts, with Area 1: $0\leq \theta\leq\frac{ \pi }{4} $ with $\csc\theta\leq r\leq 2\cos\theta$ and Area 2: $\frac{ \pi }{4}\leq \theta\leq\frac{ \pi }{2} $, but I dont know how to set r boundaries with this. Is it just $0\leq r\leq \csc\theta$? no.... this can't be right.... Could someone please help me out?",,"['calculus', 'integration', 'multivariable-calculus', 'polar-coordinates']"
56,Divergence Theorem Question,Divergence Theorem Question,,"$$\iint\limits_\sum f \ d \sigma = \iiint\limits_S \operatorname{div} \textbf{f} \ dV$$ $$\operatorname{div} \textbf{f}=1+2+3=6$$ After this, we could multiply $6$ by the volume of the sphere $\frac{4}{3} \pi (3)^3$ to get $216 \pi$. Shouldn't computing the integral give the same answer? $$6\int_0^{2\pi} \int_0^{2\pi}  \int_0^3 \ dr \ d \theta \ d \phi$$ $$=6(3)(2\pi)(2\pi)$$ $$=72 \pi^2$$ Is something wrong in the integral? Thanks.","$$\iint\limits_\sum f \ d \sigma = \iiint\limits_S \operatorname{div} \textbf{f} \ dV$$ $$\operatorname{div} \textbf{f}=1+2+3=6$$ After this, we could multiply $6$ by the volume of the sphere $\frac{4}{3} \pi (3)^3$ to get $216 \pi$. Shouldn't computing the integral give the same answer? $$6\int_0^{2\pi} \int_0^{2\pi}  \int_0^3 \ dr \ d \theta \ d \phi$$ $$=6(3)(2\pi)(2\pi)$$ $$=72 \pi^2$$ Is something wrong in the integral? Thanks.",,['multivariable-calculus']
57,Maximizing Area of Triangle in Circle,Maximizing Area of Triangle in Circle,,"I was playing around with another example that I made up where I am trying to maximize the area of a triangle inscribed in a circle of radius. I want to do the problem using the method of Lagrange Multipliers. Attempt: Consider the circle of radius 2 given by $x^2+y^2=4$ . The function we are trying to maximize is $A(x,y) = \frac{1}{2}xy$ . Let $h(x,y)=4-x^2-y^2$ . Then, $\nabla h(x,y) = [-2x \ \ -2y ]$ and $\nabla A(x,y) = [\frac{y}{2} \ \ \frac{x}{2}]$ . We want, $ [\frac{y}{2} \ \ \frac{x}{2}]= \lambda[-2x \ \ -2y ] $ . So, $[x \ \ y]=[-4\lambda y \ \ -4\lambda x]$ . Hence, $(-4\lambda y)^2+(-4\lambda x)^2 = 4 \Rightarrow 4\lambda^2(y^2+x^2)=1 \Rightarrow x^2+y^2 = \frac{1}{4\lambda^2} \Rightarrow \lambda = \frac{1}{4}$ . It then follows that $[x \ \ y] = [-0.5y \ \ -0.5x]$ . And, $h(-0.5y,y) = 4-\frac{5}{4}y^2 = 0 \Rightarrow [x \ \ y] = [\frac{-4}{\sqrt{5}} \ \ \frac{4}{\sqrt{5}}]$ . But then $A(x,y)<0$ . Question: What am I doing wrong?","I was playing around with another example that I made up where I am trying to maximize the area of a triangle inscribed in a circle of radius. I want to do the problem using the method of Lagrange Multipliers. Attempt: Consider the circle of radius 2 given by . The function we are trying to maximize is . Let . Then, and . We want, . So, . Hence, . It then follows that . And, . But then . Question: What am I doing wrong?","x^2+y^2=4 A(x,y) = \frac{1}{2}xy h(x,y)=4-x^2-y^2 \nabla h(x,y) = [-2x \ \ -2y ] \nabla A(x,y) = [\frac{y}{2} \ \ \frac{x}{2}]  [\frac{y}{2} \ \ \frac{x}{2}]= \lambda[-2x \ \ -2y ]  [x \ \ y]=[-4\lambda y \ \ -4\lambda x] (-4\lambda y)^2+(-4\lambda x)^2 = 4 \Rightarrow 4\lambda^2(y^2+x^2)=1 \Rightarrow x^2+y^2 = \frac{1}{4\lambda^2} \Rightarrow \lambda = \frac{1}{4} [x \ \ y] = [-0.5y \ \ -0.5x] h(-0.5y,y) = 4-\frac{5}{4}y^2 = 0 \Rightarrow [x \ \ y] = [\frac{-4}{\sqrt{5}} \ \ \frac{4}{\sqrt{5}}] A(x,y)<0","['multivariable-calculus', 'lagrange-multiplier']"
58,Sum of concave and strictly concave functions,Sum of concave and strictly concave functions,,"How can I prove, for the general case, that the sum of a concave and a strictly concave function, yields a strictly concave function?","How can I prove, for the general case, that the sum of a concave and a strictly concave function, yields a strictly concave function?",,"['calculus', 'multivariable-calculus']"
59,Find the surface area of portion of the plane that is inside the cylinder.,Find the surface area of portion of the plane that is inside the cylinder.,,"I am given the plane $x + y + z = 1$ and the cylinder $x^2 + y^2 = 4,$ and have to find the surface area of  portion of the plane that is inside the cylinder. I am very confused with this. I tried writting the intersection of the two surfaces as a parametric curve and got: $$ \mathbf{r} (t) = (2 \cos{t}, 2 \sin{t}, 1 - 2 \cos{t} - 2 \sin{t}).$$ The plan was then to calculate the surface area enclosed by this curve, but I don't know how to do this. I know there are formulas for doing that when the curve has parametric equation of the form $\mathbf{s}(t) = (x(t), y(t)),$ but not in my case. How can I do it?","I am given the plane $x + y + z = 1$ and the cylinder $x^2 + y^2 = 4,$ and have to find the surface area of  portion of the plane that is inside the cylinder. I am very confused with this. I tried writting the intersection of the two surfaces as a parametric curve and got: $$ \mathbf{r} (t) = (2 \cos{t}, 2 \sin{t}, 1 - 2 \cos{t} - 2 \sin{t}).$$ The plan was then to calculate the surface area enclosed by this curve, but I don't know how to do this. I know there are formulas for doing that when the curve has parametric equation of the form $\mathbf{s}(t) = (x(t), y(t)),$ but not in my case. How can I do it?",,['multivariable-calculus']
60,"Why is $g(x,y)=\frac{xy}{(x^2+y^2)^{1/2}}$ not differentiable at $(0,0)$?",Why is  not differentiable at ?,"g(x,y)=\frac{xy}{(x^2+y^2)^{1/2}} (0,0)","I need to know why the following is not true Let $g(x,y)=\frac{xy}{(x^2+y^2)^{1/2}}$ when $(x,y)$ not equal to $(0,0)$ and $g(x,y)=0$ when $(x,y)=0$ then  $g(x,y)$ on the $x$ axes$(y=0)$ and the $y$ axes $(x=0)$ is $0$ so the partial derivatives at $(0,0)$ exist and they are zero and since zero is constant function it is continuous hence $g(x,y)$ has continuous partial derivatives and therefore is differentiable at $(0,0)$","I need to know why the following is not true Let $g(x,y)=\frac{xy}{(x^2+y^2)^{1/2}}$ when $(x,y)$ not equal to $(0,0)$ and $g(x,y)=0$ when $(x,y)=0$ then  $g(x,y)$ on the $x$ axes$(y=0)$ and the $y$ axes $(x=0)$ is $0$ so the partial derivatives at $(0,0)$ exist and they are zero and since zero is constant function it is continuous hence $g(x,y)$ has continuous partial derivatives and therefore is differentiable at $(0,0)$",,"['real-analysis', 'multivariable-calculus', 'derivatives']"
61,How do I evaluate this integral by hand?,How do I evaluate this integral by hand?,,"TL;DR how do I evaluate $\int_0^{2 \pi } \frac{1}{\cos ^2(\theta )+1} \, d\theta$ by hand? I'm trying to solve this problem: Find the volume of the region defined by $x^2+xy+y^2+yz+z^2\le1$. (The answer is $\frac{4 \sqrt{2} \pi }{3}$) I have tried several tactics. First of all, I rotated it $45^\circ$ in the $xz$ plane (converting all $x$ to $\frac{(x-z)}{\sqrt{2}}$ and all $z$ to $\frac{(x+z)}{\sqrt{2}}$ to obtain this: $$x^2+y^2+z^2+xy\sqrt{2}\le1$$ Now if I solve for y at the boundary I get $$y=\frac{1}{2} \left(-\sqrt{2} x\pm \sqrt{2} \sqrt{-x^2-2 z^2+2}\right)$$ And then I can integrate out the $y$ to get $$\int _{-1}^1\int _{-\sqrt{2} \sqrt{1-z^2}}^{\sqrt{2} \sqrt{1-z^2}}\sqrt{2}    \sqrt{-x^2-2 z^2+2}\ dxdz$$ Which I can evaluate with Mathematica but not by hand [it does get the right answer in Mathematica, so I know I'm on the right track.] I can then switch to polar coordinates in $x$ and $z$, yielding $$\sqrt{2}  \int _0^{2 \pi }\int _0^{\sqrt{\frac{2}{2 \sin ^2(\theta )+\cos ^2(\theta    )}}}r \sqrt{-2 r^2 \sin ^2(\theta )-r^2 \cos ^2(\theta )+2}drd\theta$$ This is relatively easy to integrate (the inner integral), so I get $$\frac{4}{3} \int_0^{2 \pi } \frac{1}{\cos ^2(\theta )+1} \, d\theta$$ which I can't figure out how to evaluate by hand. Where do I go from here?","TL;DR how do I evaluate $\int_0^{2 \pi } \frac{1}{\cos ^2(\theta )+1} \, d\theta$ by hand? I'm trying to solve this problem: Find the volume of the region defined by $x^2+xy+y^2+yz+z^2\le1$. (The answer is $\frac{4 \sqrt{2} \pi }{3}$) I have tried several tactics. First of all, I rotated it $45^\circ$ in the $xz$ plane (converting all $x$ to $\frac{(x-z)}{\sqrt{2}}$ and all $z$ to $\frac{(x+z)}{\sqrt{2}}$ to obtain this: $$x^2+y^2+z^2+xy\sqrt{2}\le1$$ Now if I solve for y at the boundary I get $$y=\frac{1}{2} \left(-\sqrt{2} x\pm \sqrt{2} \sqrt{-x^2-2 z^2+2}\right)$$ And then I can integrate out the $y$ to get $$\int _{-1}^1\int _{-\sqrt{2} \sqrt{1-z^2}}^{\sqrt{2} \sqrt{1-z^2}}\sqrt{2}    \sqrt{-x^2-2 z^2+2}\ dxdz$$ Which I can evaluate with Mathematica but not by hand [it does get the right answer in Mathematica, so I know I'm on the right track.] I can then switch to polar coordinates in $x$ and $z$, yielding $$\sqrt{2}  \int _0^{2 \pi }\int _0^{\sqrt{\frac{2}{2 \sin ^2(\theta )+\cos ^2(\theta    )}}}r \sqrt{-2 r^2 \sin ^2(\theta )-r^2 \cos ^2(\theta )+2}drd\theta$$ This is relatively easy to integrate (the inner integral), so I get $$\frac{4}{3} \int_0^{2 \pi } \frac{1}{\cos ^2(\theta )+1} \, d\theta$$ which I can't figure out how to evaluate by hand. Where do I go from here?",,"['calculus', 'multivariable-calculus', 'problem-solving']"
62,"Show that $f(x,y)= \|x-y\|_2^2$ is differentiable",Show that  is differentiable,"f(x,y)= \|x-y\|_2^2","Problem : Show that $f: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ with $f(x,y)=\|x-y\|_2^2$ is differentiable and compute its differential at every point in the domain of $f$ Note : $\| \cdot \|_2$ denotes the euclidian norm $d(x,y)_2 =\sqrt{\sum\limits_{i=1}^n (x_i-y_i)^2}$ One exercise I found in Zorich Analysis II . I am however in general clueless on how to show that a function is differentiable. Of course there is always the brute force way such as computing the Jacobian-Matrix and discuss its entries, however that might be a little bit difficult with the above mapping since $f: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$, that would be one hell of a matrix to compute. I found that most people enjoy working with this definition of differentiability in $\mathbb{R}^n$ $f(x)=f(x_0)+A(x-x_0)+\|x-x_0\|\alpha (x)$ where $\alpha: U \to \mathbb{R}^n$ such that $\lim\limits_{x \to x_0} \alpha (x)=0$ and $A$ is a linear function. I understand that the above is only a slight variation of: Def : $f$ is at $x_0$ differentiable $\iff \exists A: \mathbb{R}^m \to \mathbb{R}^n: \displaystyle \lim_{x \to x_0} \frac{f(x)-f(x_0)-A(x-x_0)}{\|x-x_0\|}=0 $ In C.T. Michaels Analysis II the author mentions that by the definition nothing is known about the linear mapping $A$, however in most cases $A$ can easily be found by computing the Jacobian-Matrix and then verifying the definition with that Matrix. I have read a couple of entries on this website where people attempt to show that a function is differentiable or not at some generic point. But to be honest it all seemed a bit like Voodoo-Magic to me. Question : I am not primarily interested in the solution to this exercise but in the thought process one would have to show that $f$ defined as above is differentiable: Where do you start? Is there something in specific you have to notice about the function beforehand? Like symmetry, roots or anything to start discussing its differential. Is it a lot of lucky guessing and in the end succeeding after a lot of failed attempts? Or is there something like a 'most natural attempt' to generally solve such exercises.","Problem : Show that $f: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$ with $f(x,y)=\|x-y\|_2^2$ is differentiable and compute its differential at every point in the domain of $f$ Note : $\| \cdot \|_2$ denotes the euclidian norm $d(x,y)_2 =\sqrt{\sum\limits_{i=1}^n (x_i-y_i)^2}$ One exercise I found in Zorich Analysis II . I am however in general clueless on how to show that a function is differentiable. Of course there is always the brute force way such as computing the Jacobian-Matrix and discuss its entries, however that might be a little bit difficult with the above mapping since $f: \mathbb{R}^n \times \mathbb{R}^n \to \mathbb{R}$, that would be one hell of a matrix to compute. I found that most people enjoy working with this definition of differentiability in $\mathbb{R}^n$ $f(x)=f(x_0)+A(x-x_0)+\|x-x_0\|\alpha (x)$ where $\alpha: U \to \mathbb{R}^n$ such that $\lim\limits_{x \to x_0} \alpha (x)=0$ and $A$ is a linear function. I understand that the above is only a slight variation of: Def : $f$ is at $x_0$ differentiable $\iff \exists A: \mathbb{R}^m \to \mathbb{R}^n: \displaystyle \lim_{x \to x_0} \frac{f(x)-f(x_0)-A(x-x_0)}{\|x-x_0\|}=0 $ In C.T. Michaels Analysis II the author mentions that by the definition nothing is known about the linear mapping $A$, however in most cases $A$ can easily be found by computing the Jacobian-Matrix and then verifying the definition with that Matrix. I have read a couple of entries on this website where people attempt to show that a function is differentiable or not at some generic point. But to be honest it all seemed a bit like Voodoo-Magic to me. Question : I am not primarily interested in the solution to this exercise but in the thought process one would have to show that $f$ defined as above is differentiable: Where do you start? Is there something in specific you have to notice about the function beforehand? Like symmetry, roots or anything to start discussing its differential. Is it a lot of lucky guessing and in the end succeeding after a lot of failed attempts? Or is there something like a 'most natural attempt' to generally solve such exercises.",,"['calculus', 'real-analysis', 'multivariable-calculus', 'self-learning', 'intuition']"
63,Finding a partial derivative of a double summation. [closed],Finding a partial derivative of a double summation. [closed],,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 10 years ago . Improve this question How to find $$\frac{\partial T }{ \partial {\dot q_i}},$$ given that $$T= \sum_i\sum_j{\alpha_{ij}\dot q_i\dot q_j}?$$","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 10 years ago . Improve this question How to find $$\frac{\partial T }{ \partial {\dot q_i}},$$ given that $$T= \sum_i\sum_j{\alpha_{ij}\dot q_i\dot q_j}?$$",,['multivariable-calculus']
64,"Prove that $\exists x_0 \in D$ such that $f(x_0) = (0,0)$",Prove that  such that,"\exists x_0 \in D f(x_0) = (0,0)","Here's the question (from last quarter's final): Define $D$ to be the closed unit disk, that is $D = \{(x_1,x_2):x_1^2 + x_2^2 \leq 1\}$. Let $f:D \to \mathbb{R}^2$ be a $C^1$ mapping.  If $f'(x)$ is invertible for all $x \in D$ and   $$ |f(x) - x| \leq \frac 13, \quad x \in D $$   prove that there exists a point $x_0 \in D$ such that $f(x_0) = (0,0)$. Thoughts so far: clearly, the inverse function theorem applies, so that $f^{-1}$ can be defined in the neighborhood of any point.  Furthermore, the inequality applies so that $$ |x - f^{-1}(x)| \leq \frac 13, \quad x \in f(D)  $$ I'm not sure how any of this would allow me to show that $0 \in f(D)$, so I'm pretty much stuck for now. Any input, be it a gentle nudge or full solution, would be very much appreciated. EDIT: As copper.hat points out, the Brouwer fixed-point theorem makes quick work of this problem.  However, I am still looking for a solution ""in the spirit of the question"", hopefully one that uses the existence of an inverse in  the neighborhood of each point.","Here's the question (from last quarter's final): Define $D$ to be the closed unit disk, that is $D = \{(x_1,x_2):x_1^2 + x_2^2 \leq 1\}$. Let $f:D \to \mathbb{R}^2$ be a $C^1$ mapping.  If $f'(x)$ is invertible for all $x \in D$ and   $$ |f(x) - x| \leq \frac 13, \quad x \in D $$   prove that there exists a point $x_0 \in D$ such that $f(x_0) = (0,0)$. Thoughts so far: clearly, the inverse function theorem applies, so that $f^{-1}$ can be defined in the neighborhood of any point.  Furthermore, the inequality applies so that $$ |x - f^{-1}(x)| \leq \frac 13, \quad x \in f(D)  $$ I'm not sure how any of this would allow me to show that $0 \in f(D)$, so I'm pretty much stuck for now. Any input, be it a gentle nudge or full solution, would be very much appreciated. EDIT: As copper.hat points out, the Brouwer fixed-point theorem makes quick work of this problem.  However, I am still looking for a solution ""in the spirit of the question"", hopefully one that uses the existence of an inverse in  the neighborhood of each point.",,"['real-analysis', 'multivariable-calculus']"
65,Show that $f$ is harmonic,Show that  is harmonic,f,"Let us consider the function: $$ f(α,β) \equiv \sum_{n = 1}^{\infty}\left(-1\right)^{n - 1}\left[% {n^{2\alpha - 1} - 1 \over n^{\alpha}}\,\cos\left(\beta\ln\left(n\right)\right) \right] $$ My question is: Show that $f$ is harmonic $\quad\forall\ s = \alpha + \beta\,{\rm i}\quad$ with $\quad 0 < \alpha < 1$.","Let us consider the function: $$ f(α,β) \equiv \sum_{n = 1}^{\infty}\left(-1\right)^{n - 1}\left[% {n^{2\alpha - 1} - 1 \over n^{\alpha}}\,\cos\left(\beta\ln\left(n\right)\right) \right] $$ My question is: Show that $f$ is harmonic $\quad\forall\ s = \alpha + \beta\,{\rm i}\quad$ with $\quad 0 < \alpha < 1$.",,"['multivariable-calculus', 'proof-verification', 'riemann-zeta', 'harmonic-functions', 'zeta-functions']"
66,How to Calculate the Volume of Revolution without the Shell method or the Disk method,How to Calculate the Volume of Revolution without the Shell method or the Disk method,,"Given the function $f=e^x$, calculate the volume as it revolves around the $x$-axis from $x=0$ to $x=2$. Now, I know that it is easy to calculate the volume using the Shell method, but is there another way to do it? Can we 'extrend' it into three dimentions, more functions and use triple and double integrals instead? I want to do it with more integrals (and perhaps more functions) as I find it to be much more satisfying to do so.","Given the function $f=e^x$, calculate the volume as it revolves around the $x$-axis from $x=0$ to $x=2$. Now, I know that it is easy to calculate the volume using the Shell method, but is there another way to do it? Can we 'extrend' it into three dimentions, more functions and use triple and double integrals instead? I want to do it with more integrals (and perhaps more functions) as I find it to be much more satisfying to do so.",,"['calculus', 'integration', 'multivariable-calculus', 'volume']"
67,proving that $\vec{a} \times \vec{b} + \vec{b} \times \vec{c} + \vec{c} \times \vec{a}$ is the normal to the plane.,proving that  is the normal to the plane.,\vec{a} \times \vec{b} + \vec{b} \times \vec{c} + \vec{c} \times \vec{a},"$P$, $Q$, and $R$ are points in $ \mathbb{R}^3 $ which are not on the same line. if $\vec{a} = \vec{OP}$, $\vec{b} = \vec{OQ}$, and $\vec{c} = \vec{OR}$, show that  $\vec{a} \times \vec{b} + \vec{b} \times \vec{c} + \vec{c} \times \vec{a}$ is perpendicular to the plane containing $P$, $Q$, and $R$. So far, I have defined: $\vec{a} = <a_1, a_2, a_3> , \vec{b} = <b_1, b_2, b_3> , \vec{c} = <c_1, c_2, c_3> $ The lines spanning between the points a, b, and c. $ \vec{ab} = <b_1 - a_1, b_2 - a_2, b_3 - a_3> $ $ \vec{bc} = <c_1 - b_1, c_2 - b_2, c_3 - b_3> $ $ \vec{ca} = <a_1 - c_1, a_2 - c_2, a_3 - c_3> $ The perpendicular lines to the planes ab, bc, and ca. $ \vec{a \times b} = <a_2b_3 - a_3b_2, a_3b_1 - a_1b_3, a_1b_2 - a_2b_1> $ $ \vec{b \times c} = <b_2c_3 - b_3c_2, b_3c_1 - b_1c_3, b_1c_2 - b_2c_1> $ $ \vec{c \times a} = <c_2a_3 - c_3a_2, c_3a_1 - c_1a_3, c_1a_2 - c_2a_1> $ Adding them I get: $ <a_2b_3 + b_2c_3 + c_2a_3 - a_3b_2 - b_3c_2 - c_3a_2, a_3b_1 + b_3c_1 + c_3a_1 - a_1b_3 - b_1c_3 - c_1a_3, a_1b_2 + b_1c_2 + c_1a_2 - a_2b_1 - b_2c_1 - c_2a_1> $ Dot-producting the vectors $\vec{ab}$, $\vec{bc}$, and$\vec{ca}$ with the vectors $\vec{a \times b}$, $\vec{b \times c}$, and $\vec{c \times a}$ expands like crazy. What I have been going for so far is that since vectors $\vec{ab}$, $\vec{bc}$, and$\vec{ca}$ make up the plane, if I dot product each one with the cross-product, $\vec{a} \times \vec{b} + \vec{b} \times \vec{c} + \vec{c} \times \vec{a}$, it should result in zero, or at least everything cancelling out since this is supposed to be the perpendicular to the plane. Maybe I'm just tired, but it doesn't seem to be resulting in that. I feel like I'm getting close, but not quite getting the result that I am looking for. What exactly am I missing? Am I going in the right direction even or is there something completely obvious that I am missing?","$P$, $Q$, and $R$ are points in $ \mathbb{R}^3 $ which are not on the same line. if $\vec{a} = \vec{OP}$, $\vec{b} = \vec{OQ}$, and $\vec{c} = \vec{OR}$, show that  $\vec{a} \times \vec{b} + \vec{b} \times \vec{c} + \vec{c} \times \vec{a}$ is perpendicular to the plane containing $P$, $Q$, and $R$. So far, I have defined: $\vec{a} = <a_1, a_2, a_3> , \vec{b} = <b_1, b_2, b_3> , \vec{c} = <c_1, c_2, c_3> $ The lines spanning between the points a, b, and c. $ \vec{ab} = <b_1 - a_1, b_2 - a_2, b_3 - a_3> $ $ \vec{bc} = <c_1 - b_1, c_2 - b_2, c_3 - b_3> $ $ \vec{ca} = <a_1 - c_1, a_2 - c_2, a_3 - c_3> $ The perpendicular lines to the planes ab, bc, and ca. $ \vec{a \times b} = <a_2b_3 - a_3b_2, a_3b_1 - a_1b_3, a_1b_2 - a_2b_1> $ $ \vec{b \times c} = <b_2c_3 - b_3c_2, b_3c_1 - b_1c_3, b_1c_2 - b_2c_1> $ $ \vec{c \times a} = <c_2a_3 - c_3a_2, c_3a_1 - c_1a_3, c_1a_2 - c_2a_1> $ Adding them I get: $ <a_2b_3 + b_2c_3 + c_2a_3 - a_3b_2 - b_3c_2 - c_3a_2, a_3b_1 + b_3c_1 + c_3a_1 - a_1b_3 - b_1c_3 - c_1a_3, a_1b_2 + b_1c_2 + c_1a_2 - a_2b_1 - b_2c_1 - c_2a_1> $ Dot-producting the vectors $\vec{ab}$, $\vec{bc}$, and$\vec{ca}$ with the vectors $\vec{a \times b}$, $\vec{b \times c}$, and $\vec{c \times a}$ expands like crazy. What I have been going for so far is that since vectors $\vec{ab}$, $\vec{bc}$, and$\vec{ca}$ make up the plane, if I dot product each one with the cross-product, $\vec{a} \times \vec{b} + \vec{b} \times \vec{c} + \vec{c} \times \vec{a}$, it should result in zero, or at least everything cancelling out since this is supposed to be the perpendicular to the plane. Maybe I'm just tired, but it doesn't seem to be resulting in that. I feel like I'm getting close, but not quite getting the result that I am looking for. What exactly am I missing? Am I going in the right direction even or is there something completely obvious that I am missing?",,['multivariable-calculus']
68,Lagrange Multipliers where no solutions(s) satisfy the constraints,Lagrange Multipliers where no solutions(s) satisfy the constraints,,"$f(x) = x-y$ subject to constraint $x^2-y^2=2$ Using the method of Lagrange Multipliers, we get: $(1, -1) = \lambda(2x, -2y)$ which gives $x=y$ but this does not satisfy $x^2-y^2=2$ Is this because the Lagrange method is not applicable here? Or is it valid for the Lagrange method to not produce any solutions?","$f(x) = x-y$ subject to constraint $x^2-y^2=2$ Using the method of Lagrange Multipliers, we get: $(1, -1) = \lambda(2x, -2y)$ which gives $x=y$ but this does not satisfy $x^2-y^2=2$ Is this because the Lagrange method is not applicable here? Or is it valid for the Lagrange method to not produce any solutions?",,['multivariable-calculus']
69,How to find the second derivative of an implicit function?,How to find the second derivative of an implicit function?,,"We know from multivariable calculus that if $y(x)$ is a function given implicitly by the equation $F(x,y) = 0$, then $$ \frac{dy}{dx} = -\frac{F_x}{F_y} \tag{1} $$ This is quickly proved by applying the multivariable chain rule to $\frac{d}{dx}F(x,y(x))=0$. There is also a formula for the second derivative of $y$, but it is more complicated: $$\frac{d^2 y}{dx^2} = -\frac{F_{xx}F^2_y - 2F_{xy}F_xF_y+F_{yy}F^2_x}{F^3_y} \tag{2}$$ How is the formula (2) derived?","We know from multivariable calculus that if $y(x)$ is a function given implicitly by the equation $F(x,y) = 0$, then $$ \frac{dy}{dx} = -\frac{F_x}{F_y} \tag{1} $$ This is quickly proved by applying the multivariable chain rule to $\frac{d}{dx}F(x,y(x))=0$. There is also a formula for the second derivative of $y$, but it is more complicated: $$\frac{d^2 y}{dx^2} = -\frac{F_{xx}F^2_y - 2F_{xy}F_xF_y+F_{yy}F^2_x}{F^3_y} \tag{2}$$ How is the formula (2) derived?",,"['multivariable-calculus', 'partial-derivative', 'implicit-differentiation']"
70,Maximize the volume of a rectangular box in the first octant with one vertex in the plane $x+2y+3z=3$,Maximize the volume of a rectangular box in the first octant with one vertex in the plane,x+2y+3z=3,"Find the volume of the largest rectangular box in the first octant with the three faces in the coordinate planes and one vertex in the plane $x+2y+3z=3$. Now I know that $V=xyz$ and I have set $$V=xy\left(1-\frac{x}{3}-\frac{2y}{3}\right).$$ After I take the partial derivatives with respect to $x$ and $y$, I am not sure what to do. Any help would be great! Thanks.","Find the volume of the largest rectangular box in the first octant with the three faces in the coordinate planes and one vertex in the plane $x+2y+3z=3$. Now I know that $V=xyz$ and I have set $$V=xy\left(1-\frac{x}{3}-\frac{2y}{3}\right).$$ After I take the partial derivatives with respect to $x$ and $y$, I am not sure what to do. Any help would be great! Thanks.",,"['multivariable-calculus', 'optimization']"
71,"An equation of the plane that passes through the line of intersection of $x − z = 1$ and $y + 4z = 1$, and is perpendicular to $x + y − 2z = 2$","An equation of the plane that passes through the line of intersection of  and , and is perpendicular to",x − z = 1 y + 4z = 1 x + y − 2z = 2,"Find an equation of the plane. The plane that passes through the line of intersection of the planes  $x − z = 1$ and $y + 4z = 1$  and is perpendicular to the plane  $x + y − 2z = 2$. I keep getting the answer of $7x-y+5z=6$ and I am told that it is wrong. I do not understand what I am doing wrong. I have the 1st normal vector to be $\langle 1,0,-1\rangle$ and the second normal vector to be $\langle0,1,4\rangle$ and when I did a cross product on them, I got $\langle1,-4,1\rangle$ to be the direction of the line. I then got a 2nd vector parallel to the desired plane as $\langle1,1,-2\rangle$ since its perpendicular to $x+y-2z=2$ I got a normal plane by the cross product of the 2 normal vectors and the result was $\langle 7,-1,5\rangle$ Then I plugged $\langle7,-1,5\rangle$ into the scalar equation of the plane for $\langle a,b,c\rangle$ and used the point $(1,1,0) $ and for my final answer I got $7x-y+5z=6$ I need help figuring out where I went wrong.","Find an equation of the plane. The plane that passes through the line of intersection of the planes  $x − z = 1$ and $y + 4z = 1$  and is perpendicular to the plane  $x + y − 2z = 2$. I keep getting the answer of $7x-y+5z=6$ and I am told that it is wrong. I do not understand what I am doing wrong. I have the 1st normal vector to be $\langle 1,0,-1\rangle$ and the second normal vector to be $\langle0,1,4\rangle$ and when I did a cross product on them, I got $\langle1,-4,1\rangle$ to be the direction of the line. I then got a 2nd vector parallel to the desired plane as $\langle1,1,-2\rangle$ since its perpendicular to $x+y-2z=2$ I got a normal plane by the cross product of the 2 normal vectors and the result was $\langle 7,-1,5\rangle$ Then I plugged $\langle7,-1,5\rangle$ into the scalar equation of the plane for $\langle a,b,c\rangle$ and used the point $(1,1,0) $ and for my final answer I got $7x-y+5z=6$ I need help figuring out where I went wrong.",,['multivariable-calculus']
72,How do you determine whether a curve uses arc length as a parameter?,How do you determine whether a curve uses arc length as a parameter?,,"I know how to find arc length because it's simply a matter of plugging in values into a formula: s(t) = $\int_a^t |v(u)|\,du$ But given an equation r(t), how do I show whether or not the curves use arc length as a parameter? e.g) $r(t) = <2 \cos{t}, 2 \sin{t}>$ for $0 \leq t \leq 2\pi$ I did some calculating and figured this much out: $$v(t) = \left< -2\sin{t}, 2\cos{t}\right >$$ $$|v(t)| = 2$$ $$s(t) = 2t$$ Any examples or tips?","I know how to find arc length because it's simply a matter of plugging in values into a formula: s(t) = $\int_a^t |v(u)|\,du$ But given an equation r(t), how do I show whether or not the curves use arc length as a parameter? e.g) $r(t) = <2 \cos{t}, 2 \sin{t}>$ for $0 \leq t \leq 2\pi$ I did some calculating and figured this much out: $$v(t) = \left< -2\sin{t}, 2\cos{t}\right >$$ $$|v(t)| = 2$$ $$s(t) = 2t$$ Any examples or tips?",,"['calculus', 'multivariable-calculus']"
73,"Callahan's Advanced Calculus: A Geometric View vs Hubbard's Vector C, L A, and Di Forms vs Ad- Calculus: A Differential Forms Approach by Edwards","Callahan's Advanced Calculus: A Geometric View vs Hubbard's Vector C, L A, and Di Forms vs Ad- Calculus: A Differential Forms Approach by Edwards",,"My friend has given me the chance to get one of the books mentioned in the title of this question for free. I've learned single variable calculus. Which one of these books do you think would best serve my purposes, which are learning deeply multivariable calculus from scratch with physical intuition?","My friend has given me the chance to get one of the books mentioned in the title of this question for free. I've learned single variable calculus. Which one of these books do you think would best serve my purposes, which are learning deeply multivariable calculus from scratch with physical intuition?",,"['multivariable-calculus', 'reference-request', 'soft-question', 'advice']"
74,How to compute the gradient of a quadratic form? [duplicate],How to compute the gradient of a quadratic form? [duplicate],,"This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 3 years ago . Let scalar field $f : \mathbb{R}^n \to \mathbb{R}$ be given by $$f(x) = x^TA^TAx - \lambda( x^T x - 1)$$ where $A$ is an $n \times n$ matrix and $\lambda$ is a scalar. How to compute the gradient $\nabla f$ ? I know that it would be the Jacobian matrix (or gradient), but is there a faster way to compute it rather than writing out everything in terms of components and taking partial derivatives? The reason I ask is my classmate, when we considered this function in class, was able to compute the derivative as $A^T A x - \lambda x$ rather quickly.","This question already has answers here : How to take the gradient of the quadratic form? (6 answers) Closed 3 years ago . Let scalar field be given by where is an matrix and is a scalar. How to compute the gradient ? I know that it would be the Jacobian matrix (or gradient), but is there a faster way to compute it rather than writing out everything in terms of components and taking partial derivatives? The reason I ask is my classmate, when we considered this function in class, was able to compute the derivative as rather quickly.",f : \mathbb{R}^n \to \mathbb{R} f(x) = x^TA^TAx - \lambda( x^T x - 1) A n \times n \lambda \nabla f A^T A x - \lambda x,"['multivariable-calculus', 'derivatives', 'quadratic-forms', 'scalar-fields']"
75,Difference between Scalar field and a multivariable Function?,Difference between Scalar field and a multivariable Function?,,If a scalar field gives out a normal number for every orders pairs what's the difference between it and a function.,If a scalar field gives out a normal number for every orders pairs what's the difference between it and a function.,,"['multivariable-calculus', 'functions', 'vector-spaces']"
76,Derivative of positive part of a function,Derivative of positive part of a function,,"Let $f,g: A \to \mathbb{R}$ be two continuous functions defined on a compact subset $A \subset R^{2}$. Define $H:\mathbb{R}^{+} \to \mathbb{R}$ by $$H(\epsilon):=\int\int_{A}(f+\epsilon g)^{+}\mathrm{d}y\mathrm{d}x$$ where the plus superscript represents the positive part of a function, i.e. $$(f+\epsilon g)^{+} = \mathrm{max}_{(x,y) \in A} \{f+\epsilon g,0\}$$ Strictly prove (by using definition of derivative) that $H$ is differentiable at $\epsilon=0$ and calculate $H'(0)$. My approach: We have $$\lim_{\delta \to 0}\frac{H(\delta)-H(0)}{\delta} =\lim_{\delta \to 0}\int\int_{A} \frac{(f+\delta g)^{+}-f^{+}}{\delta}\mathrm{d}y\mathrm{d}x =\lim_{\delta \to 0}\int\int_{A} \frac{\delta g+|f+\delta g|-|f|}{2\delta}\mathrm{d}y\mathrm{d}x = \lim_{\delta \to 0}\int\int_{A} \frac{g}{2}+\frac{|f+\delta g|-|f|}{2\delta}\mathrm{d}y\mathrm{d}x $$ Note I have used $f^{+}=\frac{|f|+f}{2}$ for converting the positive parts to absolute values. Then I tried to use triangle inequality to squeeze the limit, e.g. $$\frac{g}{2}+ \frac{|f+\delta g|-|f|}{2\delta} \leq \frac{g}{2}+ \frac{|f|+|\delta g|-|f|}{2\delta} =g^{+}$$ and similarly one can show $-g^{-} =\mathrm{min}_{(x,y)\in A}\{g,0\}$ to be a lower bound. But these bounds are not sharp enough for computing the limit.","Let $f,g: A \to \mathbb{R}$ be two continuous functions defined on a compact subset $A \subset R^{2}$. Define $H:\mathbb{R}^{+} \to \mathbb{R}$ by $$H(\epsilon):=\int\int_{A}(f+\epsilon g)^{+}\mathrm{d}y\mathrm{d}x$$ where the plus superscript represents the positive part of a function, i.e. $$(f+\epsilon g)^{+} = \mathrm{max}_{(x,y) \in A} \{f+\epsilon g,0\}$$ Strictly prove (by using definition of derivative) that $H$ is differentiable at $\epsilon=0$ and calculate $H'(0)$. My approach: We have $$\lim_{\delta \to 0}\frac{H(\delta)-H(0)}{\delta} =\lim_{\delta \to 0}\int\int_{A} \frac{(f+\delta g)^{+}-f^{+}}{\delta}\mathrm{d}y\mathrm{d}x =\lim_{\delta \to 0}\int\int_{A} \frac{\delta g+|f+\delta g|-|f|}{2\delta}\mathrm{d}y\mathrm{d}x = \lim_{\delta \to 0}\int\int_{A} \frac{g}{2}+\frac{|f+\delta g|-|f|}{2\delta}\mathrm{d}y\mathrm{d}x $$ Note I have used $f^{+}=\frac{|f|+f}{2}$ for converting the positive parts to absolute values. Then I tried to use triangle inequality to squeeze the limit, e.g. $$\frac{g}{2}+ \frac{|f+\delta g|-|f|}{2\delta} \leq \frac{g}{2}+ \frac{|f|+|\delta g|-|f|}{2\delta} =g^{+}$$ and similarly one can show $-g^{-} =\mathrm{min}_{(x,y)\in A}\{g,0\}$ to be a lower bound. But these bounds are not sharp enough for computing the limit.",,"['real-analysis', 'multivariable-calculus']"
77,"What is meant by $C^{k,a}$?",What is meant by ?,"C^{k,a}","Will somebody please explain to me what is meant by the notation $C^{k,a}$ ? It is similar to $C^k$ in the sense that it classifies some property of a function (ex. $C^2$ ==> twice continuously differentiable), in my case a curve. Help much appreciated.","Will somebody please explain to me what is meant by the notation $C^{k,a}$ ? It is similar to $C^k$ in the sense that it classifies some property of a function (ex. $C^2$ ==> twice continuously differentiable), in my case a curve. Help much appreciated.",,"['real-analysis', 'differential-geometry', 'multivariable-calculus']"
78,"Is the point $(3, 2, −1, 4, 1)$ in the open ball $B^{5}$ $((1, 2, −4, 2, 3), 3)?$",Is the point  in the open ball,"(3, 2, −1, 4, 1) B^{5} ((1, 2, −4, 2, 3), 3)?","This is question 11 from the text here . Is the point $(3, 2, −1, 4, 1)$ in the open ball $B^5 ((1, 2, −4, 2, 3), 3)$? In my attempt to solve this question, I took the point, subtracted each corresponding component from the ball's center, and then summed the squares of these values: $$(3-1)^2+(2-2)^2+(-1-(-4))^2+(4-2)^2+(1-3)^2=21$$ Because $21>r^2$, that is $21>9$, I concluded that the point is not in the open ball.  However, the correct answer is yes, the point is inside the open ball.  (Answers are posted here ) For reference (because it might help explain my confusion) this is the definition of an open ball from the book: The set of all points $(x_1, x_2,...,x_n )$ in $\mathbb{R}^n$ which satisfy the inequality   $(x_1 − p_1 )^2 + (x_2 − p_2)^2 +...+ (x_n − p_n )^2 < r^2$   (1.1.15)   is called an open n-dimensional ball with radius r and center p, which we denote $B^n(\textbf{p},r)$. Can someone help me figure out where I am going wrong?  Thanks! Update: I contacted the author of this textbook, Dan Sloughter, and he promptly corrected the answer guide.  Also, I was using an outdated version of the book/answers.  In case anyone needs them in the future, the updated links are: http://www.synechism.org/wp/the-calculus-of-functions-of-several-variables/ and http://dananne.org/dw/doku.php?id=cfsv:cfsv","This is question 11 from the text here . Is the point $(3, 2, −1, 4, 1)$ in the open ball $B^5 ((1, 2, −4, 2, 3), 3)$? In my attempt to solve this question, I took the point, subtracted each corresponding component from the ball's center, and then summed the squares of these values: $$(3-1)^2+(2-2)^2+(-1-(-4))^2+(4-2)^2+(1-3)^2=21$$ Because $21>r^2$, that is $21>9$, I concluded that the point is not in the open ball.  However, the correct answer is yes, the point is inside the open ball.  (Answers are posted here ) For reference (because it might help explain my confusion) this is the definition of an open ball from the book: The set of all points $(x_1, x_2,...,x_n )$ in $\mathbb{R}^n$ which satisfy the inequality   $(x_1 − p_1 )^2 + (x_2 − p_2)^2 +...+ (x_n − p_n )^2 < r^2$   (1.1.15)   is called an open n-dimensional ball with radius r and center p, which we denote $B^n(\textbf{p},r)$. Can someone help me figure out where I am going wrong?  Thanks! Update: I contacted the author of this textbook, Dan Sloughter, and he promptly corrected the answer guide.  Also, I was using an outdated version of the book/answers.  In case anyone needs them in the future, the updated links are: http://www.synechism.org/wp/the-calculus-of-functions-of-several-variables/ and http://dananne.org/dw/doku.php?id=cfsv:cfsv",,['multivariable-calculus']
79,"Find maximum and minimum of $f(x,y) = x^2 y^2 - 2x - 2y$ in $0 \leq x \leq y \leq 5$.",Find maximum and minimum of  in .,"f(x,y) = x^2 y^2 - 2x - 2y 0 \leq x \leq y \leq 5","Find maximum and minimum of $f(x,y) = x^2 y^2 - 2x - 2y$ in $0 \leq x \leq y \leq 5$. So first we need to check inside the domain, I got only one point $A(1,1)$ where $f(1,1) = -3$. and after further checking it is a saddle point. Now we want to check on the edges, we have 3 edges: $(1) x=y, 0\leq x\leq y \leq 5$ and $(2) y=5, 0 \leq x \leq 5$ and $(3) x=0, 0\leq y \leq 5$. So I started with each of them, using Lagrange. I started with the first and got: $l(x,y,\lambda) = x^2y^2 - 2x - 2y + \lambda (x-y)$. $l_x(x,y) = 2xy^2 - 2 + \lambda = 0$ $l_y(x,y) = 2x^2y - 2 - \lambda = 0 $ $x-y = 0 \rightarrow x=y$. But then what to do ? I always get $x,y = \sqrt[3]{1+ 0.5\lambda}$, which gets me nowhere. Any help would be appreciated","Find maximum and minimum of $f(x,y) = x^2 y^2 - 2x - 2y$ in $0 \leq x \leq y \leq 5$. So first we need to check inside the domain, I got only one point $A(1,1)$ where $f(1,1) = -3$. and after further checking it is a saddle point. Now we want to check on the edges, we have 3 edges: $(1) x=y, 0\leq x\leq y \leq 5$ and $(2) y=5, 0 \leq x \leq 5$ and $(3) x=0, 0\leq y \leq 5$. So I started with each of them, using Lagrange. I started with the first and got: $l(x,y,\lambda) = x^2y^2 - 2x - 2y + \lambda (x-y)$. $l_x(x,y) = 2xy^2 - 2 + \lambda = 0$ $l_y(x,y) = 2x^2y - 2 - \lambda = 0 $ $x-y = 0 \rightarrow x=y$. But then what to do ? I always get $x,y = \sqrt[3]{1+ 0.5\lambda}$, which gets me nowhere. Any help would be appreciated",,"['calculus', 'multivariable-calculus', 'optimization']"
80,Find the volume inside both $x^2+y^2+z^2=4$ and $x^2+y^2=1$.,Find the volume inside both  and .,x^2+y^2+z^2=4 x^2+y^2=1,"What is the volume inside both $x^2+y^2+z^2=4$ and $x^2+y^2=1$? The chapter I am working on is called Change of Variables in Multiple Integrals, for my Vector Calculus class. I understand that we will be taking the double integral of these two shapes to find the volume between both of them, but I am very lost as to how to begin. Should I convert to polar coordinates or should I replace one equation into the other, IE $x^2+y^2=u$ and I could place that into the first equation, then replace it with 1 since $x^2+y^2=u=1$. Or should I set them equal to each other and try to find the bounds like that? I'd appreciate any input, I am just very confused as to how to approach the problem. Thank you!","What is the volume inside both $x^2+y^2+z^2=4$ and $x^2+y^2=1$? The chapter I am working on is called Change of Variables in Multiple Integrals, for my Vector Calculus class. I understand that we will be taking the double integral of these two shapes to find the volume between both of them, but I am very lost as to how to begin. Should I convert to polar coordinates or should I replace one equation into the other, IE $x^2+y^2=u$ and I could place that into the first equation, then replace it with 1 since $x^2+y^2=u=1$. Or should I set them equal to each other and try to find the bounds like that? I'd appreciate any input, I am just very confused as to how to approach the problem. Thank you!",,"['integration', 'multivariable-calculus']"
81,"What is the exact, rigorous, full statement of Divergence (Gauss') Theorem in $\mathbb{R}^3$ (without being too complicated)?","What is the exact, rigorous, full statement of Divergence (Gauss') Theorem in  (without being too complicated)?",\mathbb{R}^3,"The wolfram page http://mathworld.wolfram.com/DivergenceTheorem.html states the formula $$ \int_{V} \nabla \cdot \mathbf{F} dS = \int_{\partial V} \mathbf{F} \cdot d\mathbf{S} $$ but it does not speak much of what kind of conditions should be imposed on $\mathbf{F}, V$ and so on. I think it is enough for $\mathbf{F}$ to be continuously differentiable over $V$ (is it?). But what should be on $V$? Q1) Is it enough for $V$ to have $\partial V$ as a parametrized (smooth) surface (even piecewise)? Q2) (It may be a topological one.) But my textbook says a parametrized surface is the image of a continuously differentiable mapping $\mathbf{r} : \mathcal{R} \to \mathbb{R}^3$ where $\mathcal{R}$ is a region (i.e., open, bounded, its boundary having Jordan content 0) in $\mathbb{R}^2$. Then can a sphere have a parametrization? Q3) What should be the exact imposition on $\mathbf{F}$ including how to specify its domain? (I hope you'd not talk about manifolds and forms and other complex definitions...)","The wolfram page http://mathworld.wolfram.com/DivergenceTheorem.html states the formula $$ \int_{V} \nabla \cdot \mathbf{F} dS = \int_{\partial V} \mathbf{F} \cdot d\mathbf{S} $$ but it does not speak much of what kind of conditions should be imposed on $\mathbf{F}, V$ and so on. I think it is enough for $\mathbf{F}$ to be continuously differentiable over $V$ (is it?). But what should be on $V$? Q1) Is it enough for $V$ to have $\partial V$ as a parametrized (smooth) surface (even piecewise)? Q2) (It may be a topological one.) But my textbook says a parametrized surface is the image of a continuously differentiable mapping $\mathbf{r} : \mathcal{R} \to \mathbb{R}^3$ where $\mathcal{R}$ is a region (i.e., open, bounded, its boundary having Jordan content 0) in $\mathbb{R}^2$. Then can a sphere have a parametrization? Q3) What should be the exact imposition on $\mathbf{F}$ including how to specify its domain? (I hope you'd not talk about manifolds and forms and other complex definitions...)",,['multivariable-calculus']
82,Chain rule for functions of two variables,Chain rule for functions of two variables,,"Suppose that $f(x,y)$ is a function of two variables with $f_x(0,2) = 2$ and $f_y(0,2) = -1$. Using the chain rule compute the numerical value of $f_\theta(r\cos\theta,r\sin\theta) = 2$ at $r=2$, $\theta=\frac{\pi}{2}$. Any hints on how to do this question would be appreciated. Thanks in advance.","Suppose that $f(x,y)$ is a function of two variables with $f_x(0,2) = 2$ and $f_y(0,2) = -1$. Using the chain rule compute the numerical value of $f_\theta(r\cos\theta,r\sin\theta) = 2$ at $r=2$, $\theta=\frac{\pi}{2}$. Any hints on how to do this question would be appreciated. Thanks in advance.",,['multivariable-calculus']
83,Green's theorem and flux,Green's theorem and flux,,"Given the vector field $\vec{F}(x,y) = (x^2+y^2)^{-1}\begin{bmatrix} x \\ y \end{bmatrix}$, calculate the flux of $\vec{F}$ across the circle $C$ of radius $a$ centered at the origin (with positive orientation). It is my understanding that Green's theorem for flux and divergence says $$ \int\limits_C \Phi_{\vec{F}} = \int\limits_C P\,dy - Q\,dx = \iint\limits_R\nabla\cdot\vec{F}\,dA $$ if $\vec{F} = \begin{bmatrix} P & Q \end{bmatrix}$ (omitting other hypotheses of course). Note that $R$ is the region bounded by the curve $C$. For our $\vec{F}$, we have $\nabla\cdot\vec{F} = 0$. So shouldn't the flux of $\vec{F}$ through $C$ be zero? Everytime I try to compute it, I obtain zero. However, it appears that I am supposed to be getting $2\pi$. Can someone help me to understand where I am going wrong?","Given the vector field $\vec{F}(x,y) = (x^2+y^2)^{-1}\begin{bmatrix} x \\ y \end{bmatrix}$, calculate the flux of $\vec{F}$ across the circle $C$ of radius $a$ centered at the origin (with positive orientation). It is my understanding that Green's theorem for flux and divergence says $$ \int\limits_C \Phi_{\vec{F}} = \int\limits_C P\,dy - Q\,dx = \iint\limits_R\nabla\cdot\vec{F}\,dA $$ if $\vec{F} = \begin{bmatrix} P & Q \end{bmatrix}$ (omitting other hypotheses of course). Note that $R$ is the region bounded by the curve $C$. For our $\vec{F}$, we have $\nabla\cdot\vec{F} = 0$. So shouldn't the flux of $\vec{F}$ through $C$ be zero? Everytime I try to compute it, I obtain zero. However, it appears that I am supposed to be getting $2\pi$. Can someone help me to understand where I am going wrong?",,[]
84,tangent plane to $\sqrt{x} + \sqrt{y} + \sqrt{z} = \sqrt{C}$,tangent plane to,\sqrt{x} + \sqrt{y} + \sqrt{z} = \sqrt{C},"Let $ S = \left\{ (z,y,z) \in R^3 : \sqrt{x} + \sqrt{y} + \sqrt{z} = \sqrt{C} \right\} $ be a surface. a) Find the tangent plane to $S$ at $(x_{0}, y_{0}, z_{0})$. b) Let $P_{0}, Q_{0}$ and $R_{0}$ be the points where the tangent plane to $S$ at  $(x_{0}, y_{0}, z_{0})$ crosses the axes. If $P_{1}, Q_{1}$ and $R_{1}$ are the points where the tangent plane to $S$ at other point $(x_{1}, y_{1}, z_{1})$ crosses the axes , prove that $P_{0} + Q_{0} +  R_{0} =P_{1} + Q_{1} +  R_{1}$ I'm not sure how to even start. If I want to compute the tangent plane by using partial derivatives I need a function first, so I was thinking of: $ \sqrt{x} + \sqrt{y} + \sqrt{z} = \sqrt{C} \iff (\sqrt{x} + \sqrt{y} + \sqrt{z})^2 = C \iff f(x,y,z) = C \iff f(x,y,z) = (\sqrt{x} + \sqrt{y} + \sqrt{z})^2 $ And now I would compute the tangent plane with $\nabla{f}(x_o,y_o,z_o) \cdot (x-x_o,y-y_o,z-z_o)$ Does this make any sense? Or can I just take partial derivatives of $\sqrt{x} + \sqrt{y} + \sqrt{z} = \sqrt{C}$? For the second part, I'm even more lost. I guess I need to figure out how $P_{x}, Q_{x}$ and $R_{x}$ for any given tangent plane and prove what we asked.","Let $ S = \left\{ (z,y,z) \in R^3 : \sqrt{x} + \sqrt{y} + \sqrt{z} = \sqrt{C} \right\} $ be a surface. a) Find the tangent plane to $S$ at $(x_{0}, y_{0}, z_{0})$. b) Let $P_{0}, Q_{0}$ and $R_{0}$ be the points where the tangent plane to $S$ at  $(x_{0}, y_{0}, z_{0})$ crosses the axes. If $P_{1}, Q_{1}$ and $R_{1}$ are the points where the tangent plane to $S$ at other point $(x_{1}, y_{1}, z_{1})$ crosses the axes , prove that $P_{0} + Q_{0} +  R_{0} =P_{1} + Q_{1} +  R_{1}$ I'm not sure how to even start. If I want to compute the tangent plane by using partial derivatives I need a function first, so I was thinking of: $ \sqrt{x} + \sqrt{y} + \sqrt{z} = \sqrt{C} \iff (\sqrt{x} + \sqrt{y} + \sqrt{z})^2 = C \iff f(x,y,z) = C \iff f(x,y,z) = (\sqrt{x} + \sqrt{y} + \sqrt{z})^2 $ And now I would compute the tangent plane with $\nabla{f}(x_o,y_o,z_o) \cdot (x-x_o,y-y_o,z-z_o)$ Does this make any sense? Or can I just take partial derivatives of $\sqrt{x} + \sqrt{y} + \sqrt{z} = \sqrt{C}$? For the second part, I'm even more lost. I guess I need to figure out how $P_{x}, Q_{x}$ and $R_{x}$ for any given tangent plane and prove what we asked.",,['multivariable-calculus']
85,Prove a partial derivative change-of-variables identity,Prove a partial derivative change-of-variables identity,,"If $u=f(x,y)$ where $x=e^{3s}\cos(2t)$ and $y=e^{3s}\sin(2t)$, then show that $$ \left(\dfrac{\partial u}{\partial x}\right)^2+\left(\dfrac{\partial u}{\partial y}\right)^2= g(s,t)\left(\dfrac{\partial u}{\partial s}\right)^2 + h(s,t) \left(\dfrac{\partial u}{\partial t}\right)^2$$ My attempt: I found $du/ds$ and $du/dt$, squared them, and added them. In the examples from class, something cancels out nicely but nothing did in this question. After some searching around, I found a midterm solution which is very similar to my question. So far, I have $U_s^2 + U_t^2 = U_x^2(9x^2+4y^2)+10 U_s U_t xy+ U_y^2(9y^2+4x^2)$ Now I'm stuck because of the $9$ and $4$ in front of $x$ and $y$. Any help is greatly appreciated!","If $u=f(x,y)$ where $x=e^{3s}\cos(2t)$ and $y=e^{3s}\sin(2t)$, then show that $$ \left(\dfrac{\partial u}{\partial x}\right)^2+\left(\dfrac{\partial u}{\partial y}\right)^2= g(s,t)\left(\dfrac{\partial u}{\partial s}\right)^2 + h(s,t) \left(\dfrac{\partial u}{\partial t}\right)^2$$ My attempt: I found $du/ds$ and $du/dt$, squared them, and added them. In the examples from class, something cancels out nicely but nothing did in this question. After some searching around, I found a midterm solution which is very similar to my question. So far, I have $U_s^2 + U_t^2 = U_x^2(9x^2+4y^2)+10 U_s U_t xy+ U_y^2(9y^2+4x^2)$ Now I'm stuck because of the $9$ and $4$ in front of $x$ and $y$. Any help is greatly appreciated!",,"['calculus', 'multivariable-calculus', 'partial-derivative']"
86,Derivative of vectors,Derivative of vectors,,"I know very little about vector calculus. What is the derivative of $\langle\alpha,\alpha\rangle$ (dot product) and $\alpha^TK\alpha$ and $\langle\alpha,y\rangle$. All these derivatives are by the variable $\alpha$. Where $K$ is a matrix and $y$ is a vector.","I know very little about vector calculus. What is the derivative of $\langle\alpha,\alpha\rangle$ (dot product) and $\alpha^TK\alpha$ and $\langle\alpha,y\rangle$. All these derivatives are by the variable $\alpha$. Where $K$ is a matrix and $y$ is a vector.",,"['calculus', 'linear-algebra', 'multivariable-calculus']"
87,How do you find the volume of these two functions?,How do you find the volume of these two functions?,,"Find the volume of the region inside the surface $x^2 + y^2 + z^2 = 16$ and outside the surface $x^2 + y^2 = 4$. How would you set this up and solve it using double integration and polar? I came up with a graph that shows a cylinder in the middle of a sphere. I kind have the idea of subtracting the cylinder volume to the volume of the sphere but I don't know how to set it up, like what would the boundaries be?","Find the volume of the region inside the surface $x^2 + y^2 + z^2 = 16$ and outside the surface $x^2 + y^2 = 4$. How would you set this up and solve it using double integration and polar? I came up with a graph that shows a cylinder in the middle of a sphere. I kind have the idea of subtracting the cylinder volume to the volume of the sphere but I don't know how to set it up, like what would the boundaries be?",,['multivariable-calculus']
88,Find minimum in a constrained two-variable inequation [closed],Find minimum in a constrained two-variable inequation [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I would appreciate if somebody could help me with the following problem: Q: find minimum  $$9a^2+9b^2+c^2$$ where $a^2+b^2\leq 9, c=\sqrt{9-a^2}\sqrt{9-b^2}-2ab$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I would appreciate if somebody could help me with the following problem: Q: find minimum  $$9a^2+9b^2+c^2$$ where $a^2+b^2\leq 9, c=\sqrt{9-a^2}\sqrt{9-b^2}-2ab$",,"['multivariable-calculus', 'inequality', 'optimization']"
89,"Max and min value of $f(x,y)$",Max and min value of,"f(x,y)","$$f(x,y) = 2\cdot x\cdot y$$ $$x^2+y^2 \leq 4$$ I have no idea about this question.There is a region.How can I solve?","$$f(x,y) = 2\cdot x\cdot y$$ $$x^2+y^2 \leq 4$$ I have no idea about this question.There is a region.How can I solve?",,['multivariable-calculus']
90,"Triple integral of $r^{2}e^{ir\cos\theta}\sin\theta \,dr\,d\theta \,d\phi$",Triple integral of,"r^{2}e^{ir\cos\theta}\sin\theta \,dr\,d\theta \,d\phi","I'm trying to calculate this integral but I'm a bit stuck. Has anyone got any tips/tricks to deal with the $e^{ir\cosθ}$ part? $$\iiint r^{2}e^{ir\cos\theta}\sin\theta \,dr\,d\theta \,d\phi$$ Limits: $0\leq r \leq a$, $0\leq\theta\leq \pi$, $0\leq\phi\leq2\pi$. I'm a first year chemistry student so keep the maths as simple as possible!","I'm trying to calculate this integral but I'm a bit stuck. Has anyone got any tips/tricks to deal with the $e^{ir\cosθ}$ part? $$\iiint r^{2}e^{ir\cos\theta}\sin\theta \,dr\,d\theta \,d\phi$$ Limits: $0\leq r \leq a$, $0\leq\theta\leq \pi$, $0\leq\phi\leq2\pi$. I'm a first year chemistry student so keep the maths as simple as possible!",,"['integration', 'multivariable-calculus']"
91,Finding the winding number of a curve,Finding the winding number of a curve,,"Let $\gamma(t)=(r \cos t,r \sin t)$, for some $r>0$, and let $\Gamma$ be a $C^2$-curve in $\mathbb R^2-\{\bf 0\} $, with parameter interval $[0,2 \pi]$, with $\Gamma(0)=\Gamma(2 \pi)$, such that the segments $[\gamma(t),\Gamma(t)]$ do not contain $\bf 0$ for any $t \in [0,2 \pi]$. Prove that $$ \int_\Gamma \eta =2\pi $$ where $\eta=\dfrac{y dx-x dy}{x^2+y^2}$. I need to prove that using the fact that $\eta=d \left( \arctan \frac{y}{x} \right)$ for $x \neq 0$, and $\eta=\left( -\arctan \frac{x}{y} \right)$ for $y \neq 0$. (In less rigorous terms $\eta=d \theta$).","Let $\gamma(t)=(r \cos t,r \sin t)$, for some $r>0$, and let $\Gamma$ be a $C^2$-curve in $\mathbb R^2-\{\bf 0\} $, with parameter interval $[0,2 \pi]$, with $\Gamma(0)=\Gamma(2 \pi)$, such that the segments $[\gamma(t),\Gamma(t)]$ do not contain $\bf 0$ for any $t \in [0,2 \pi]$. Prove that $$ \int_\Gamma \eta =2\pi $$ where $\eta=\dfrac{y dx-x dy}{x^2+y^2}$. I need to prove that using the fact that $\eta=d \left( \arctan \frac{y}{x} \right)$ for $x \neq 0$, and $\eta=\left( -\arctan \frac{x}{y} \right)$ for $y \neq 0$. (In less rigorous terms $\eta=d \theta$).",,"['calculus', 'real-analysis', 'multivariable-calculus', 'integration', 'differential-forms']"
92,"In polar/cylindrical coordinates, does $r dr d\theta=r d\theta dr$?","In polar/cylindrical coordinates, does ?",r dr d\theta=r d\theta dr,"In polar/cylindrical coordinates, does $r dr d\theta=r d\theta dr$? For example, I believe the integral for the area of a half-circle is given by $$\int_0^1\int_0^{\pi}{rd\theta dr}.$$ What would this integral be if the order were reversed? I find it hard to visualize taking $dr$ first.","In polar/cylindrical coordinates, does $r dr d\theta=r d\theta dr$? For example, I believe the integral for the area of a half-circle is given by $$\int_0^1\int_0^{\pi}{rd\theta dr}.$$ What would this integral be if the order were reversed? I find it hard to visualize taking $dr$ first.",,"['calculus', 'multivariable-calculus']"
93,Eliminating parameters to obtain surface equation,Eliminating parameters to obtain surface equation,,"Given the following vector equation, how do I eliminate the parameters $u,v$ to get an equation of a surface in rectangular coordinates? $$\vec{r}(u,v)=3u\cos(v)\hat{\imath} + 4u\sin(v)\hat{\jmath} + u\hat{k}$$ I can express this as a set of parametric equations: $$x(u,v)=3u\cos(v)$$ $$y(u,v)=4u\sin(v)$$ $$z(u,v)=u$$ I'm not sure where to go from here -- how do you combine these three into one equation? Is there a general procedure to follow when eliminating parameters?","Given the following vector equation, how do I eliminate the parameters $u,v$ to get an equation of a surface in rectangular coordinates? $$\vec{r}(u,v)=3u\cos(v)\hat{\imath} + 4u\sin(v)\hat{\jmath} + u\hat{k}$$ I can express this as a set of parametric equations: $$x(u,v)=3u\cos(v)$$ $$y(u,v)=4u\sin(v)$$ $$z(u,v)=u$$ I'm not sure where to go from here -- how do you combine these three into one equation? Is there a general procedure to follow when eliminating parameters?",,"['multivariable-calculus', 'parametric']"
94,A Question in Rudin PMA chapter 9,A Question in Rudin PMA chapter 9,,If $f$ is a differentiable mapping of a connected open set $E\subset \Bbb{R}^n$ into $\Bbb{R}^m$ and if $f'(x)=0$ for every $x\in E$ prove that $f$ is constant in E. How can this be done without using convexity? I'm trying to use that $f'(x)=0 \implies f$ is constant in open balls which are convex to show $A=E$ where $A=\{x\in E \mid f(x)=f(x_0\}$ where $x_0$ is fixed. Is there another way to do this with minimal use of connectedness or convexity?,If $f$ is a differentiable mapping of a connected open set $E\subset \Bbb{R}^n$ into $\Bbb{R}^m$ and if $f'(x)=0$ for every $x\in E$ prove that $f$ is constant in E. How can this be done without using convexity? I'm trying to use that $f'(x)=0 \implies f$ is constant in open balls which are convex to show $A=E$ where $A=\{x\in E \mid f(x)=f(x_0\}$ where $x_0$ is fixed. Is there another way to do this with minimal use of connectedness or convexity?,,"['real-analysis', 'multivariable-calculus']"
95,Surface integral of Poynting vector for time-invariant E & B fields,Surface integral of Poynting vector for time-invariant E & B fields,,"I've got this question from electrodynamics, but I have a feeling that this is probably purely a maths question. I'm using standard terminology, but for folks not into electrodynamics here is the background: Consider fields $\rho \left( \vec{r} \right)$, $\vec{J} \left( \vec{r} \right)$, $\vec{E} \left( \vec{r} \right)$ and $\vec{B} \left( \vec{r} \right)$ in $\mathbb{R}^3$. Take any finite volume $V_s$ outside of which $\vec{J}\left(\vec{r}\right)$ and $\rho\left(\vec{r}\right)$ are $0$. Then define $\vec{E}\left(\vec{r}\right) $ and $\vec{B}\left(\vec{r}\right) $ for any $\vec{r}$ as follows: $$ \vec{E} \left( \vec{r} \right) =     \frac{1}{4\pi \epsilon_0}       \iiint_{V_s}            \frac{\rho\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|^3}            \left(\vec{r}-\vec{r}_s\right)       \space dV\left(\vec{r}_s\right) $$ $$ \vec{B} \left( \vec{r} \right) =     \frac{\mu_0}{4\pi}       \iiint_{V_s}            \frac{\vec{J}\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|^3}            \times \left(\vec{r}-\vec{r}_s\right)        \space dV\left(\vec{r}_s\right) $$ Now my question is, given just this information, is it possible to mathematically prove that following surface integral will always evaluate to zero? If so, what is the proof? (To clarify, $\partial V_s$ is the bounding surface of the volume $V_s$ mentioned earlier) $$ \frac{1}{\mu_0} \oint_{\partial V_s} \left(\vec{E}\left(\vec{r}\right) \times \vec{B}\left(\vec{r}\right)\right)\cdot d\vec{S}\left(\vec{r}\right) $$ The meaning in physics is, I'm looking for proof that time invariant sources (static charges and constant currents confined to a volume) cannot radiate any energy, and I'm trying to do that without invoking the Hertzian dipole and Fourier analysis. Update I've summarized the consolidated solution in my answer below. Thanks to all in stackexchange Mathematics & Physics who helped.","I've got this question from electrodynamics, but I have a feeling that this is probably purely a maths question. I'm using standard terminology, but for folks not into electrodynamics here is the background: Consider fields $\rho \left( \vec{r} \right)$, $\vec{J} \left( \vec{r} \right)$, $\vec{E} \left( \vec{r} \right)$ and $\vec{B} \left( \vec{r} \right)$ in $\mathbb{R}^3$. Take any finite volume $V_s$ outside of which $\vec{J}\left(\vec{r}\right)$ and $\rho\left(\vec{r}\right)$ are $0$. Then define $\vec{E}\left(\vec{r}\right) $ and $\vec{B}\left(\vec{r}\right) $ for any $\vec{r}$ as follows: $$ \vec{E} \left( \vec{r} \right) =     \frac{1}{4\pi \epsilon_0}       \iiint_{V_s}            \frac{\rho\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|^3}            \left(\vec{r}-\vec{r}_s\right)       \space dV\left(\vec{r}_s\right) $$ $$ \vec{B} \left( \vec{r} \right) =     \frac{\mu_0}{4\pi}       \iiint_{V_s}            \frac{\vec{J}\left(\vec{r}_s\right)}{\left|\vec{r}-\vec{r}_s\right|^3}            \times \left(\vec{r}-\vec{r}_s\right)        \space dV\left(\vec{r}_s\right) $$ Now my question is, given just this information, is it possible to mathematically prove that following surface integral will always evaluate to zero? If so, what is the proof? (To clarify, $\partial V_s$ is the bounding surface of the volume $V_s$ mentioned earlier) $$ \frac{1}{\mu_0} \oint_{\partial V_s} \left(\vec{E}\left(\vec{r}\right) \times \vec{B}\left(\vec{r}\right)\right)\cdot d\vec{S}\left(\vec{r}\right) $$ The meaning in physics is, I'm looking for proof that time invariant sources (static charges and constant currents confined to a volume) cannot radiate any energy, and I'm trying to do that without invoking the Hertzian dipole and Fourier analysis. Update I've summarized the consolidated solution in my answer below. Thanks to all in stackexchange Mathematics & Physics who helped.",,"['multivariable-calculus', 'vector-analysis']"
96,Proving a certain inequality in $\mathbb{R}^n$,Proving a certain inequality in,\mathbb{R}^n,Show that: $$\|{x-y}\|\|x+y\| \leq \|x\|^2+\|y\|^2$$ My guess is that we need to show that the left-hand side reduces to $\|x+y\|^2$ (the Triangle Inequality holds) but I haven't been able to make much progress in doing so.,Show that: $$\|{x-y}\|\|x+y\| \leq \|x\|^2+\|y\|^2$$ My guess is that we need to show that the left-hand side reduces to $\|x+y\|^2$ (the Triangle Inequality holds) but I haven't been able to make much progress in doing so.,,"['linear-algebra', 'multivariable-calculus']"
97,"Continuity of multivariable functions when ""component functions"" are continuous","Continuity of multivariable functions when ""component functions"" are continuous",,"Given topological spaces $X_1, X_2, \dotsc, X_n, Y$, consider a multivariable function $f : \prod_{i = 1}^nX_i \to Y$ such that for any $(x_1, x_2, \dotsc, x_n) \in \prod_{i = 1}^nX_i$, the functions in the family $\{x \mapsto f(x_1, \dotsc, x_{i - 1}, x, x_{i + 1}, \dotsc, x_n)\}_{i = 1}^n$ are all continuous. Must $f$ itself be continuous? It seems to be true, so is there a Theorem that proves this?","Given topological spaces $X_1, X_2, \dotsc, X_n, Y$, consider a multivariable function $f : \prod_{i = 1}^nX_i \to Y$ such that for any $(x_1, x_2, \dotsc, x_n) \in \prod_{i = 1}^nX_i$, the functions in the family $\{x \mapsto f(x_1, \dotsc, x_{i - 1}, x, x_{i + 1}, \dotsc, x_n)\}_{i = 1}^n$ are all continuous. Must $f$ itself be continuous? It seems to be true, so is there a Theorem that proves this?",,"['general-topology', 'reference-request', 'multivariable-calculus', 'continuity']"
98,Second derivative of function of two variables,Second derivative of function of two variables,,"I'm having problemes using the chain rule in the 2-variables case. I know that the first derivative of a function $f=f\left(t,u(t)\right)$ is $$\frac{df}{dt}=\frac{df}{dt}+\frac{df}{du}\frac{du}{dt}$$ Then, if I apply the chain rule in this expression I get: $$\frac{d^2f}{d^2t}=\left[\frac{df}{dtdu}\frac{du}{dt}+\frac{d^2f}{d^2t}\right]+\left[\frac{d^2u}{d^2t}\frac{df}{du}+\frac{du}{dt}\left(\frac{d^2f}{d^2u}\frac{du}{dt}+\frac{df}{dudt}\right)\right]$$ where the first group comes from derivating the first term of $\frac{df}{dt}$ and the second group from derivating the second one. However, shouldn't this derivative look like the following? $$\frac{d^2f}{d^2t}=\frac{d^2f}{d^2t}+2\frac{du}{dt}\frac{df}{dudt}+\frac{d^2f}{d^2u}\left(\frac{du}{dt}\right)^2$$ I'd appreciate if someone could explain me a little bit what I am doing wrong.","I'm having problemes using the chain rule in the 2-variables case. I know that the first derivative of a function $f=f\left(t,u(t)\right)$ is $$\frac{df}{dt}=\frac{df}{dt}+\frac{df}{du}\frac{du}{dt}$$ Then, if I apply the chain rule in this expression I get: $$\frac{d^2f}{d^2t}=\left[\frac{df}{dtdu}\frac{du}{dt}+\frac{d^2f}{d^2t}\right]+\left[\frac{d^2u}{d^2t}\frac{df}{du}+\frac{du}{dt}\left(\frac{d^2f}{d^2u}\frac{du}{dt}+\frac{df}{dudt}\right)\right]$$ where the first group comes from derivating the first term of $\frac{df}{dt}$ and the second group from derivating the second one. However, shouldn't this derivative look like the following? $$\frac{d^2f}{d^2t}=\frac{d^2f}{d^2t}+2\frac{du}{dt}\frac{df}{dudt}+\frac{d^2f}{d^2u}\left(\frac{du}{dt}\right)^2$$ I'd appreciate if someone could explain me a little bit what I am doing wrong.",,['multivariable-calculus']
99,"Show that $f(x,y)=2x-y$ is uniformly continuous",Show that  is uniformly continuous,"f(x,y)=2x-y","Show that $f(x,y)=2x-y$ is uniformly continuous in $\mathbb{R^2}$. Use the definition. How can I do this using just the definition of uniform continuity?","Show that $f(x,y)=2x-y$ is uniformly continuous in $\mathbb{R^2}$. Use the definition. How can I do this using just the definition of uniform continuity?",,"['calculus', 'real-analysis', 'multivariable-calculus']"
