,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"If $\sum\limits_{n=0}^\infty c_n$ is bounded uniformly, then $\lim\limits_{x\to1^-} \sum\limits_{n=0}^\infty c_nx^n$ exists?","If  is bounded uniformly, then  exists?",\sum\limits_{n=0}^\infty c_n \lim\limits_{x\to1^-} \sum\limits_{n=0}^\infty c_nx^n,"Determine whether the following holds: If $\sum\limits_{n=0}^\infty c_n$ is bounded uniformly (i.e. there exists $M>0$ such that, for every $N$, $|\sum\limits_{n=0}^N c_n|\leq M$), then $\lim\limits_{x\to1^-} \sum\limits_{n=0}^\infty c_nx^n$ exists.","Determine whether the following holds: If $\sum\limits_{n=0}^\infty c_n$ is bounded uniformly (i.e. there exists $M>0$ such that, for every $N$, $|\sum\limits_{n=0}^N c_n|\leq M$), then $\lim\limits_{x\to1^-} \sum\limits_{n=0}^\infty c_nx^n$ exists.",,"['real-analysis', 'analysis']"
1,Suppose $A$ is a countable subset of $\mathbb{R}$. Show that $\mathbb{R} \sim \mathbb{R} \setminus A $.,Suppose  is a countable subset of . Show that .,A \mathbb{R} \mathbb{R} \sim \mathbb{R} \setminus A ,"Here is the question I am trying to answer: Let $\mathbb{R}$ denote the reals and suppose A is a countable subset of $\mathbb{R}$. Show that $\mathbb{R} \sim \mathbb{R} \setminus A $. My attempt: Because $\mathbb{R}$ is uncountable and the subset $A$ of $\mathbb{R}$ is countable,  $\mathbb{R} \setminus A $ is uncountable. Therefore $ \mathbb{R} \sim \mathbb{R} \setminus A $ . My question: Is this specific enough or do I need to find a better way to show that $\mathbb{R} \sim \mathbb{R} \setminus A $? If I need to be more specific, do I have to come up with a function $f$ such that $f$ maps $\mathbb{R}$ onto $ \mathbb{R} \setminus A$ such that the function is 1-1 and onto? If I do need to create a function, any helpful hints to help me create this function? Thanks a lot.","Here is the question I am trying to answer: Let $\mathbb{R}$ denote the reals and suppose A is a countable subset of $\mathbb{R}$. Show that $\mathbb{R} \sim \mathbb{R} \setminus A $. My attempt: Because $\mathbb{R}$ is uncountable and the subset $A$ of $\mathbb{R}$ is countable,  $\mathbb{R} \setminus A $ is uncountable. Therefore $ \mathbb{R} \sim \mathbb{R} \setminus A $ . My question: Is this specific enough or do I need to find a better way to show that $\mathbb{R} \sim \mathbb{R} \setminus A $? If I need to be more specific, do I have to come up with a function $f$ such that $f$ maps $\mathbb{R}$ onto $ \mathbb{R} \setminus A$ such that the function is 1-1 and onto? If I do need to create a function, any helpful hints to help me create this function? Thanks a lot.",,"['real-analysis', 'analysis', 'functions', 'cardinals', 'real-numbers']"
2,Absolutely convergent series of continuous functions is continuous,Absolutely convergent series of continuous functions is continuous,,"For $m, d\ge 1$, let $f_n: K\to \mathbb{R}^m$ be a continuous function for every $n\ge 0$, where $K\subset \mathbb{R}^d$ is compact. Show that if $\sum_{n=0}^\infty \|f_n\|_K <\infty$ then $f(x):=\sum_{n=0}^\infty f_n(x) <\infty$ is also a continuous function $f:K\to\mathbb{R}^m$. Proof: $\infty>\sum_{n=0}^\infty \|f_n\|_K =\sum_{n=0}^\infty \max\{\|f_n(x)\|:x\in K\} \ge \>\sum_{n=0}^\infty \|f_n(x)\|$, thus $\sum_{n=0}^\infty f_n(x)$ converges for any $x\in K$, which implies that $f(x)$ exists. This also implies that $\forall \varepsilon > 0, \exists N>0$, such that $k>N\implies \|\sum_{n=0}^k f_n(x) - f(x)\|<\varepsilon/2$. Since $f_n(x)$ is continuous for every $x\in K$, then $\forall \varepsilon >0, \exists \delta > 0$ such that, for $x_0\in K$, $\|x-x_0\|<\delta\implies \|f_n(x)-f_n(x_0)\| < \varepsilon$. Now, let $k>N$, then $\left\| \sum_{n=0}^k f_n(x)-\sum_{n=0}^k f_n(x_0)\right\| =\left\| \sum_{n=0}^k f_n(x)+f(x_0)-f(x_0)-\sum_{n=0}^k f_n(x_0)\right\|\le$ $\left\| \sum_{n=0}^k f_n(x)-f(x_0)\right\|+\left\|\sum_{n=0}^k f_n(x_0)-f(x_0)\right\|$, and this is where I get stuck since I do not know what to do with the $\left\| \sum_{n=0}^k f_n(x)-f(x_0)\right\|$ term, so some help would be appreciated. Is there something that I do not know about compact sets which can be applied here? Update: I've just found out that sequences of continuous functions are uniformly convergent on compact sets, so I would change my last inequality as follows: $\left\| \sum_{n=0}^k f_n(x)-\sum_{n=0}^k f_n(x_0)\right\| =\left\| \sum_{n=0}^k f_n(x)+f(x)-f(x)-\sum_{n=0}^k f_n(x_0)\right\|\le$ $\left\| \sum_{n=0}^k f_n(x)-f(x)\right\|+\left\|\sum_{n=0}^k f_n(x_0)-f(x)\right\| \le \varepsilon/2+\sup\limits_{x\in K}\left\|\sum_{n=0}^k f_n(x)-f(x)\right\|<\varepsilon/2+\varepsilon/2=\varepsilon$ Do you think this is correct now?","For $m, d\ge 1$, let $f_n: K\to \mathbb{R}^m$ be a continuous function for every $n\ge 0$, where $K\subset \mathbb{R}^d$ is compact. Show that if $\sum_{n=0}^\infty \|f_n\|_K <\infty$ then $f(x):=\sum_{n=0}^\infty f_n(x) <\infty$ is also a continuous function $f:K\to\mathbb{R}^m$. Proof: $\infty>\sum_{n=0}^\infty \|f_n\|_K =\sum_{n=0}^\infty \max\{\|f_n(x)\|:x\in K\} \ge \>\sum_{n=0}^\infty \|f_n(x)\|$, thus $\sum_{n=0}^\infty f_n(x)$ converges for any $x\in K$, which implies that $f(x)$ exists. This also implies that $\forall \varepsilon > 0, \exists N>0$, such that $k>N\implies \|\sum_{n=0}^k f_n(x) - f(x)\|<\varepsilon/2$. Since $f_n(x)$ is continuous for every $x\in K$, then $\forall \varepsilon >0, \exists \delta > 0$ such that, for $x_0\in K$, $\|x-x_0\|<\delta\implies \|f_n(x)-f_n(x_0)\| < \varepsilon$. Now, let $k>N$, then $\left\| \sum_{n=0}^k f_n(x)-\sum_{n=0}^k f_n(x_0)\right\| =\left\| \sum_{n=0}^k f_n(x)+f(x_0)-f(x_0)-\sum_{n=0}^k f_n(x_0)\right\|\le$ $\left\| \sum_{n=0}^k f_n(x)-f(x_0)\right\|+\left\|\sum_{n=0}^k f_n(x_0)-f(x_0)\right\|$, and this is where I get stuck since I do not know what to do with the $\left\| \sum_{n=0}^k f_n(x)-f(x_0)\right\|$ term, so some help would be appreciated. Is there something that I do not know about compact sets which can be applied here? Update: I've just found out that sequences of continuous functions are uniformly convergent on compact sets, so I would change my last inequality as follows: $\left\| \sum_{n=0}^k f_n(x)-\sum_{n=0}^k f_n(x_0)\right\| =\left\| \sum_{n=0}^k f_n(x)+f(x)-f(x)-\sum_{n=0}^k f_n(x_0)\right\|\le$ $\left\| \sum_{n=0}^k f_n(x)-f(x)\right\|+\left\|\sum_{n=0}^k f_n(x_0)-f(x)\right\| \le \varepsilon/2+\sup\limits_{x\in K}\left\|\sum_{n=0}^k f_n(x)-f(x)\right\|<\varepsilon/2+\varepsilon/2=\varepsilon$ Do you think this is correct now?",,"['sequences-and-series', 'analysis', 'functions', 'convergence-divergence', 'continuity']"
3,The Cauchy Principal value of a rational function with only real poles,The Cauchy Principal value of a rational function with only real poles,,"$\newcommand{\PV}{\operatorname{P.V.}}$I have a doubt about the Cauchy Principal Value of real rational functions. $f: \mathbb{R}\rightarrow \mathbb{R}$ is a rational function with $\deg(\text{denominator})>\deg(\text{numerator})$. $\{x_1,x_2,\ldots,x_n \} \subset \mathbb{R} $ is the  set of all $f$ poles $\PV$ exists and it is $$\PV \int_{-\infty}^{+\infty} f(x) \ dx=\pi i \left( \sum_{k=1}^n \operatorname{Res}(f,x_k) \right) $$ $$\pi i \left( \sum_{k=1}^n \operatorname{Res}(f,x_k) \right) \in \mathbb{I}$$ So: $$\PV \int_{-\infty}^{+\infty} f(x) \ dx=0$$ because: $$\operatorname{Re} \left( \pi i \left( \sum_{k=1}^n \operatorname{Res}(f,x_k) \right) \right)=0$$ Is it true? $\PV \int_{-\infty}^{+\infty} f(x) \ dx$ doesn't exist if $\deg(\text{numerator}) \ge \deg(\text{denominator})$, does it? In general, which are the conditions of existence of P V? Is it correct? Thanks","$\newcommand{\PV}{\operatorname{P.V.}}$I have a doubt about the Cauchy Principal Value of real rational functions. $f: \mathbb{R}\rightarrow \mathbb{R}$ is a rational function with $\deg(\text{denominator})>\deg(\text{numerator})$. $\{x_1,x_2,\ldots,x_n \} \subset \mathbb{R} $ is the  set of all $f$ poles $\PV$ exists and it is $$\PV \int_{-\infty}^{+\infty} f(x) \ dx=\pi i \left( \sum_{k=1}^n \operatorname{Res}(f,x_k) \right) $$ $$\pi i \left( \sum_{k=1}^n \operatorname{Res}(f,x_k) \right) \in \mathbb{I}$$ So: $$\PV \int_{-\infty}^{+\infty} f(x) \ dx=0$$ because: $$\operatorname{Re} \left( \pi i \left( \sum_{k=1}^n \operatorname{Res}(f,x_k) \right) \right)=0$$ Is it true? $\PV \int_{-\infty}^{+\infty} f(x) \ dx$ doesn't exist if $\deg(\text{numerator}) \ge \deg(\text{denominator})$, does it? In general, which are the conditions of existence of P V? Is it correct? Thanks",,"['real-analysis', 'integration', 'analysis', 'improper-integrals']"
4,"show $ x \to (x,f(x))$ is an embedding",show  is an embedding," x \to (x,f(x))","Let $f:\mathbb{R}^n \to \mathbb{R}^m$ be ca continuous function and $(\mathbb{R}^n,\mathcal{O}_{\mathbb{R}^n}) $ the real numbers equipped with the euclidian topology.  Why the map $F:\mathbb{R}^n \to \mathbb{R}^{m+n}$ with $ x \to (x,f(x))$ is an embedding? Of course $F$ is naturally injective and F is also continuous. But it's not clear to me why the map $F: (\mathbb{R}^n,\mathcal{O}_{\mathbb{R}^n}) \to (F(\mathbb{R}^n),\mathcal{O}_{\mathbb{R}^{m+n} | \mathbb{R}^n})$ is open? Where $\mathcal{O}_{\mathbb{R}^{m+n} | \mathbb{R}^n}$ is the subspace topology from $\mathbb{R}^n$ in $\mathbb{R}^{m+n}$.  Can someone give me a little hint?","Let $f:\mathbb{R}^n \to \mathbb{R}^m$ be ca continuous function and $(\mathbb{R}^n,\mathcal{O}_{\mathbb{R}^n}) $ the real numbers equipped with the euclidian topology.  Why the map $F:\mathbb{R}^n \to \mathbb{R}^{m+n}$ with $ x \to (x,f(x))$ is an embedding? Of course $F$ is naturally injective and F is also continuous. But it's not clear to me why the map $F: (\mathbb{R}^n,\mathcal{O}_{\mathbb{R}^n}) \to (F(\mathbb{R}^n),\mathcal{O}_{\mathbb{R}^{m+n} | \mathbb{R}^n})$ is open? Where $\mathcal{O}_{\mathbb{R}^{m+n} | \mathbb{R}^n}$ is the subspace topology from $\mathbb{R}^n$ in $\mathbb{R}^{m+n}$.  Can someone give me a little hint?",,"['general-topology', 'analysis']"
5,Rudin Chapter 5 Exercise 14,Rudin Chapter 5 Exercise 14,,"The question is as follows. Let $f$ be differentiable real function defined in $(a,b)$. Prove that $f$ is convex iff $f'$ is monotonically increasing. Assume next that $f″(x)$ exists for every $x\in (a,b)$, and prove that $f$ is convex iff $f″(x)\geq 0$ for all $x\in (a,b)$. For the second part of the question, can we directly say that $f'(x)$ is monotonically increasing if and only if $f''(x)\geq 0$? I have this question because the theorem derived from the Mean Value Theorem only directly implies the backwards direction (i.e. if the derivative is larger than 0, the function is monotonically increasing).","The question is as follows. Let $f$ be differentiable real function defined in $(a,b)$. Prove that $f$ is convex iff $f'$ is monotonically increasing. Assume next that $f″(x)$ exists for every $x\in (a,b)$, and prove that $f$ is convex iff $f″(x)\geq 0$ for all $x\in (a,b)$. For the second part of the question, can we directly say that $f'(x)$ is monotonically increasing if and only if $f''(x)\geq 0$? I have this question because the theorem derived from the Mean Value Theorem only directly implies the backwards direction (i.e. if the derivative is larger than 0, the function is monotonically increasing).",,"['real-analysis', 'analysis']"
6,A well defined function.,A well defined function.,,I don't understand how the following statement implies a function is well defined: For $$ f: X \rightarrow Y$$ 'A function $f$ is well defined rule that assigns a unique element $f(x)  \in Y$ to each $x \in X$.,I don't understand how the following statement implies a function is well defined: For $$ f: X \rightarrow Y$$ 'A function $f$ is well defined rule that assigns a unique element $f(x)  \in Y$ to each $x \in X$.,,"['real-analysis', 'analysis', 'functions', 'notation', 'definition']"
7,Integral of pointwise max function,Integral of pointwise max function,,"Consider given some functions $f_1,f_2,g\in C^\infty([a,b],\mathbb{R})$ which have both positive and negative values. Call $F:[a,b]\rightarrow \mathbb{R}$ the pointwise maximum of $f_1$ and $f_2$: $F(x):=max\{f_1(x),f_2(x)\}$. Suppose it is true $\int_a^bf_1gdx\ge 0$ and $\int_a^bf_2gdx\ge 0$. Then is it always true $\int_a^bFgdx\ge 0$? I suspect the answer is yes, but I can't work out a formal proof.","Consider given some functions $f_1,f_2,g\in C^\infty([a,b],\mathbb{R})$ which have both positive and negative values. Call $F:[a,b]\rightarrow \mathbb{R}$ the pointwise maximum of $f_1$ and $f_2$: $F(x):=max\{f_1(x),f_2(x)\}$. Suppose it is true $\int_a^bf_1gdx\ge 0$ and $\int_a^bf_2gdx\ge 0$. Then is it always true $\int_a^bFgdx\ge 0$? I suspect the answer is yes, but I can't work out a formal proof.",,"['real-analysis', 'integration', 'analysis']"
8,Prove using an inequality that $e$ is irrational,Prove using an inequality that  is irrational,e,"I have to prove that $e$ is irrational using this inequality $$0<e-\sum_{k=0}^n\frac1{k!}<\frac1{n\cdot n!}$$ The exercise leave the hint ""prove by contradiction"". I know too that $2<e\le 3$. What I tried is set $e=p/q$ for $p,q\in\Bbb N$, and $\sum 1/k!=A/n!$ where $A\in\Bbb N$. Then I written $$0<\frac{p}{q}-\frac{A}{n!}<\frac1{nn!}$$ but I dont get any idea from here. Indeed I dont know exactly what to do here, I never used a inequality to prove the irrationality of a number. Can you give me some hint (or solution)? Thank you. P.S.: I dont know exactly what kind of tags I have to use for this question.","I have to prove that $e$ is irrational using this inequality $$0<e-\sum_{k=0}^n\frac1{k!}<\frac1{n\cdot n!}$$ The exercise leave the hint ""prove by contradiction"". I know too that $2<e\le 3$. What I tried is set $e=p/q$ for $p,q\in\Bbb N$, and $\sum 1/k!=A/n!$ where $A\in\Bbb N$. Then I written $$0<\frac{p}{q}-\frac{A}{n!}<\frac1{nn!}$$ but I dont get any idea from here. Indeed I dont know exactly what to do here, I never used a inequality to prove the irrationality of a number. Can you give me some hint (or solution)? Thank you. P.S.: I dont know exactly what kind of tags I have to use for this question.",,"['analysis', 'proof-writing', 'irrational-numbers']"
9,Everybody present,Everybody present,,"Once upon a time there was a big party with n participants. People arrived to the party,spent some time there and went home. the bartender noticed that any two people had a drink together. a)Show that there was a moment when everybody was present at the same time b)What should we assume about endpoints of the time intervals for the statement to hold? c)What if there were infinitely many people? My approach: Since everyone had a drink together then for every 2 participantsthere was a time when they were together, then we supose that for any n=k participants there was a time that they were together. Now assume that there wasn't a time when k+1 participants were together Now let's call them with $p_1,p_2,...,p_k,p_{k+1}$ and w.l.o.g. say $T(p_1,p_2,...,p_k) < T(p_2,...,p_k,p_{k+1})<T(p_1,p_3,...,p_{k+1}) $ , where T is the time when they have been together so: in order to happen second one $p_1$ should go home before and to happen third one after second one $p_1$ should be there but he went home so contradiction. I don't have any clue about b and c","Once upon a time there was a big party with n participants. People arrived to the party,spent some time there and went home. the bartender noticed that any two people had a drink together. a)Show that there was a moment when everybody was present at the same time b)What should we assume about endpoints of the time intervals for the statement to hold? c)What if there were infinitely many people? My approach: Since everyone had a drink together then for every 2 participantsthere was a time when they were together, then we supose that for any n=k participants there was a time that they were together. Now assume that there wasn't a time when k+1 participants were together Now let's call them with $p_1,p_2,...,p_k,p_{k+1}$ and w.l.o.g. say $T(p_1,p_2,...,p_k) < T(p_2,...,p_k,p_{k+1})<T(p_1,p_3,...,p_{k+1}) $ , where T is the time when they have been together so: in order to happen second one $p_1$ should go home before and to happen third one after second one $p_1$ should be there but he went home so contradiction. I don't have any clue about b and c",,"['real-analysis', 'analysis', 'induction']"
10,How to prove the limit of a sequence: $\lim{\frac{n}{(n+1)^{3/2}}}=0$,How to prove the limit of a sequence:,\lim{\frac{n}{(n+1)^{3/2}}}=0,"I am trying to prove that $\lim{\frac{n}{(n+1)^{3/2}}}=0$. I know that I must prove this using ""$\epsilon, N$"" definition of the limit. Attempt: By definition of the limit, we need: $\forall \epsilon > 0, \exists N \in \mathbb{N}$, s.t. if $n \ge N$, then $|\frac{n}{(n+1)^{3/2}}|<\epsilon$. So I need to find n as an explicit function of $\epsilon$ by simplifying inequality $|\frac{n}{(n+1)^{3/2}}|<\epsilon$. First problem - I think I can open absolute value, but I am not sure how to justify it. Then $\frac{n}{(n+1)^{3/2}}<\epsilon$. I do not know how to proceed further. I tried using binomial theorem (as far as I understand, special case of Taylor expansion) and got $n + \frac{2}{3} + \frac{8}{3n} - \frac{48}{3n^2} + \frac{384}{6n^3} - ... < \epsilon$ , but I do not see how it can be useful.","I am trying to prove that $\lim{\frac{n}{(n+1)^{3/2}}}=0$. I know that I must prove this using ""$\epsilon, N$"" definition of the limit. Attempt: By definition of the limit, we need: $\forall \epsilon > 0, \exists N \in \mathbb{N}$, s.t. if $n \ge N$, then $|\frac{n}{(n+1)^{3/2}}|<\epsilon$. So I need to find n as an explicit function of $\epsilon$ by simplifying inequality $|\frac{n}{(n+1)^{3/2}}|<\epsilon$. First problem - I think I can open absolute value, but I am not sure how to justify it. Then $\frac{n}{(n+1)^{3/2}}<\epsilon$. I do not know how to proceed further. I tried using binomial theorem (as far as I understand, special case of Taylor expansion) and got $n + \frac{2}{3} + \frac{8}{3n} - \frac{48}{3n^2} + \frac{384}{6n^3} - ... < \epsilon$ , but I do not see how it can be useful.",,"['real-analysis', 'sequences-and-series', 'analysis']"
11,"How to show that if $f$ is rapidly decreasing, we have $|f(x-y)|\le C_N/(1+|x|)^N$ when $|y|\le |x|/2$.","How to show that if  is rapidly decreasing, we have  when .",f |f(x-y)|\le C_N/(1+|x|)^N |y|\le |x|/2,"Let $u(x,t)=(f*\mathcal{H}_t)(x)$ for $t>0$ where $f$ is a function in the Schwartz space and $\mathcal{H}_t$ is the heat kernel. Then we have the following estimate (from Stein and Shakarchi's Fourier Analysis): $$|u(x,t)|\le \int_{|y|\le|x|/2}|f(x-y)|\mathcal{H}_t(y)dy+\int_{|y|\ge|x|/2}|f(x-y)|\mathcal{H}_t(y)dy \\ \le \frac{C_N}{(1+|x|)^N}+\frac{C}{\sqrt{t}}e^{-cx^2/t}.$$ To get the first inequality, the text says ""Indeed,since $f$ is rapidly decreasing, we have $|f(x-y)|\le C_N/(1+|x|)^N$ when $|y|\le |x|/2$."" However, this is what I don't understand. I don't know how to get this specific form of inequality given $|y|\le |x|/2$. A related inequality given in the text is that if $g$ is rapidly decreasing then by considering the two cases $|x|\le 2|y|$ and $|x|\ge 2|y|$, we have $\sup_x |x|^l |g(x-y)|\le A_l (1+|y|)^l.$ I think this one can be shown by similar reasoning as the above one, but I really have no idea how to show this by considering the two cases. A function $f$ is rapidly decreasing if it is indefinitely differentiable and  $$\sup_{x\in R} |x|^k |f^{(l)}(x)|<\infty \; \text{for every} \; k,l\ge 0.$$ I would greatly appreciate it if anyone could show this inequality.","Let $u(x,t)=(f*\mathcal{H}_t)(x)$ for $t>0$ where $f$ is a function in the Schwartz space and $\mathcal{H}_t$ is the heat kernel. Then we have the following estimate (from Stein and Shakarchi's Fourier Analysis): $$|u(x,t)|\le \int_{|y|\le|x|/2}|f(x-y)|\mathcal{H}_t(y)dy+\int_{|y|\ge|x|/2}|f(x-y)|\mathcal{H}_t(y)dy \\ \le \frac{C_N}{(1+|x|)^N}+\frac{C}{\sqrt{t}}e^{-cx^2/t}.$$ To get the first inequality, the text says ""Indeed,since $f$ is rapidly decreasing, we have $|f(x-y)|\le C_N/(1+|x|)^N$ when $|y|\le |x|/2$."" However, this is what I don't understand. I don't know how to get this specific form of inequality given $|y|\le |x|/2$. A related inequality given in the text is that if $g$ is rapidly decreasing then by considering the two cases $|x|\le 2|y|$ and $|x|\ge 2|y|$, we have $\sup_x |x|^l |g(x-y)|\le A_l (1+|y|)^l.$ I think this one can be shown by similar reasoning as the above one, but I really have no idea how to show this by considering the two cases. A function $f$ is rapidly decreasing if it is indefinitely differentiable and  $$\sup_{x\in R} |x|^k |f^{(l)}(x)|<\infty \; \text{for every} \; k,l\ge 0.$$ I would greatly appreciate it if anyone could show this inequality.",,"['real-analysis', 'analysis', 'convergence-divergence', 'fourier-analysis', 'fourier-transform']"
12,Is the order inevitable in constructing the real numbers?,Is the order inevitable in constructing the real numbers?,,"There are several ways to construct real numbers, such as the Dedekind cut , monotone bounded sequences and Cauchy sequences . It is obvious that the former two involves the order of the rational numbers, but whether the order is needed in constructing the real numbers via the Cauchy sequences is not clear at the first glance. However, there are many people claimed that it is because the Cauchy sequence doesn't require the order of the set, we can generalize this concept, using Cauchy sequence to complete the incomplete metric space. But I'm not quite convinced, so I ask here. Although the way to defining real numbers by Cauchy sequences requires only the rational numbers to be a metric space, in order to discuss $\forall \varepsilon\in\mathbb{Q}^{+},~ \exists N\in\mathbb{N},~ \forall m,n\in\mathbb{N},~m,n>N\Rightarrow d(x_m,x_n)<\varepsilon$, how did we define the metric here? It's $d(x,y)=|x-y|$ for any $x,y\in\mathbb{Q}$, and the absolute function is defined as the following way: $|a|:= \begin{cases} a~~~~~~~,a\geq 0\\ -a~~~~,a<0 \end{cases}$ So there is the situation to compare a number with $0$, hence the order relation involved. Therefore, it seems the Cauchy sequences way also requires the order relation.","There are several ways to construct real numbers, such as the Dedekind cut , monotone bounded sequences and Cauchy sequences . It is obvious that the former two involves the order of the rational numbers, but whether the order is needed in constructing the real numbers via the Cauchy sequences is not clear at the first glance. However, there are many people claimed that it is because the Cauchy sequence doesn't require the order of the set, we can generalize this concept, using Cauchy sequence to complete the incomplete metric space. But I'm not quite convinced, so I ask here. Although the way to defining real numbers by Cauchy sequences requires only the rational numbers to be a metric space, in order to discuss $\forall \varepsilon\in\mathbb{Q}^{+},~ \exists N\in\mathbb{N},~ \forall m,n\in\mathbb{N},~m,n>N\Rightarrow d(x_m,x_n)<\varepsilon$, how did we define the metric here? It's $d(x,y)=|x-y|$ for any $x,y\in\mathbb{Q}$, and the absolute function is defined as the following way: $|a|:= \begin{cases} a~~~~~~~,a\geq 0\\ -a~~~~,a<0 \end{cases}$ So there is the situation to compare a number with $0$, hence the order relation involved. Therefore, it seems the Cauchy sequences way also requires the order relation.",,"['analysis', 'order-theory', 'real-numbers', 'cauchy-sequences']"
13,$\text{ Proving }\; A \subseteq \Bbb R \text{ A is bounded above} \Rightarrow A^c \text{ is not?} $,,\text{ Proving }\; A \subseteq \Bbb R \text{ A is bounded above} \Rightarrow A^c \text{ is not?} ,"Prove: Let $A \subseteq \Bbb R$. Prove that if $A$ is bounded above, then $A^c$, the complement of $A$ is not bounded above. $ A^c = $ those element of the universe that are not in A. $ \Bbb R =$ real numbers. Proof: (direct proof) Suppose that $A^c$ is bounded above and $ s= \text{supremum}(A), t = \text{supremum}(A^c)  $ $\Rightarrow$ $a\le s, ∀ a \in A; b \le t,  ∀ b \in B.$ . How does one finish this proof? Any hints would be appreciated.","Prove: Let $A \subseteq \Bbb R$. Prove that if $A$ is bounded above, then $A^c$, the complement of $A$ is not bounded above. $ A^c = $ those element of the universe that are not in A. $ \Bbb R =$ real numbers. Proof: (direct proof) Suppose that $A^c$ is bounded above and $ s= \text{supremum}(A), t = \text{supremum}(A^c)  $ $\Rightarrow$ $a\le s, ∀ a \in A; b \le t,  ∀ b \in B.$ . How does one finish this proof? Any hints would be appreciated.",,"['analysis', 'proof-verification', 'real-numbers']"
14,Hints on showing Cauchy sequence converges,Hints on showing Cauchy sequence converges,,"Let $T>0$ and $L\geq0$. Let $C[0,T]$ be the space of all continuous real valued functions on $[0,T]$ with the metric $\rho$ defined by $$\rho(x,y)=\sup_{0\leq t\leq T}e^{-Lt}\left|x(t)-y(t)\right|$$ How can we verify that $\left(C[0,T],\rho\right)$ is a complete metric space? My working: Let $\{x_n(t)\}$ be an arbitrary Cauchy sequence in $C[0,T]$. We need to show that $\{x_n(t)\}$ converges to say $x(t)\in C[0,T]$. The definition of Cauchy sequence states that $\{x_n(t)\}$ is Cauchy if $\forall\epsilon>0,\exists N$ such that $m,n\geq N\implies\rho(x_m(t),x_n(t))<\epsilon$. But $\rho(x_m(t),x_n(t))=\sup_{0\leq t\leq T}e^{-Lt}\left|x_m(t)-x_n(t)\right|$. Since every Cauchy sequence is bounded, then $\forall t\in[0,T]$, $\rho\left(x_m(t),x_n(t)\right)<K$ for a constant $K$. Then I am stuck, I am not sure how to show $\{x_n(t)\}$ converges. Could anybody please give some hints? Thanks.","Let $T>0$ and $L\geq0$. Let $C[0,T]$ be the space of all continuous real valued functions on $[0,T]$ with the metric $\rho$ defined by $$\rho(x,y)=\sup_{0\leq t\leq T}e^{-Lt}\left|x(t)-y(t)\right|$$ How can we verify that $\left(C[0,T],\rho\right)$ is a complete metric space? My working: Let $\{x_n(t)\}$ be an arbitrary Cauchy sequence in $C[0,T]$. We need to show that $\{x_n(t)\}$ converges to say $x(t)\in C[0,T]$. The definition of Cauchy sequence states that $\{x_n(t)\}$ is Cauchy if $\forall\epsilon>0,\exists N$ such that $m,n\geq N\implies\rho(x_m(t),x_n(t))<\epsilon$. But $\rho(x_m(t),x_n(t))=\sup_{0\leq t\leq T}e^{-Lt}\left|x_m(t)-x_n(t)\right|$. Since every Cauchy sequence is bounded, then $\forall t\in[0,T]$, $\rho\left(x_m(t),x_n(t)\right)<K$ for a constant $K$. Then I am stuck, I am not sure how to show $\{x_n(t)\}$ converges. Could anybody please give some hints? Thanks.",,"['sequences-and-series', 'analysis', 'convergence-divergence', 'metric-spaces', 'cauchy-sequences']"
15,Proof that the function is uniformly continuous,Proof that the function is uniformly continuous,,"Proof that the function $f: [0, \infty) \ni  x \mapsto \frac{x^{2}}{x + 1} \in \mathbb{R}$ is uniformly continuous. On the internet I found out that a function is uniformly continuous when $\forall \varepsilon > 0 \ \exists \delta > 0: \left | f(x)-f(x_{0}) \right | < \varepsilon$ whenever $\left | x - x_{0} \right | < \delta .$ Because I don't know how to prove it calculative, I have drawn the function and showed its uniform continuity like that. But I'd like to know how to do it the other, more professional and efficient way. I've watched some videos but anyway couldn't find a solution. Also tried to for almost 2 hours myself but nothing came out, too. For the drawing, I think there is actually a mistake, at the beginning the epsilon (first one I haven't drawed) seems smaller than the others, while the others have same size. (In addition I skipped the other part of the function because it's trivial). Here is the picture:","Proof that the function $f: [0, \infty) \ni  x \mapsto \frac{x^{2}}{x + 1} \in \mathbb{R}$ is uniformly continuous. On the internet I found out that a function is uniformly continuous when $\forall \varepsilon > 0 \ \exists \delta > 0: \left | f(x)-f(x_{0}) \right | < \varepsilon$ whenever $\left | x - x_{0} \right | < \delta .$ Because I don't know how to prove it calculative, I have drawn the function and showed its uniform continuity like that. But I'd like to know how to do it the other, more professional and efficient way. I've watched some videos but anyway couldn't find a solution. Also tried to for almost 2 hours myself but nothing came out, too. For the drawing, I think there is actually a mistake, at the beginning the epsilon (first one I haven't drawed) seems smaller than the others, while the others have same size. (In addition I skipped the other part of the function because it's trivial). Here is the picture:",,"['calculus', 'analysis']"
16,Theorem 3.44 in Baby Rudin: Can we replace the coefficients with their absolute values?,Theorem 3.44 in Baby Rudin: Can we replace the coefficients with their absolute values?,,"Here's Theorem 3.44 in the book Principles of Mathematical Analysis by Walter Rudin, third edition: Suppose the radius of convergence of $\sum c_n z^n $ is $1$, and suppose $c_0 \geq c_1 \geq c_2 \geq \cdots$, $\lim_{n\to\infty} c_n = 0$. Then $\sum c_n z^n$ converges at every point on the circle $\vert z \vert = 1$, except possibly at $z = 1$. Now I have the following couple of questions. (1) Can we replace the second condition in the above theorem by the following? Suppose $\vert c_0 \vert \geq \vert c_1 \vert \geq \vert c_2 \vert \geq \cdots$. Does the conclusion of the above theorem still hold? Let $\{c_n\}_{n\in\mathbb{N}}$ be a sequence of complex numbers; suppose the power series $\sum c_n z^n$ has radius of convergence equal to $R$, where $R$ can either be a non-negative real number or $+\infty$. Then we can we say $R$ is also the radius of convergence of the power series $\sum \vert c_n \vert z^n$. Am I right?","Here's Theorem 3.44 in the book Principles of Mathematical Analysis by Walter Rudin, third edition: Suppose the radius of convergence of $\sum c_n z^n $ is $1$, and suppose $c_0 \geq c_1 \geq c_2 \geq \cdots$, $\lim_{n\to\infty} c_n = 0$. Then $\sum c_n z^n$ converges at every point on the circle $\vert z \vert = 1$, except possibly at $z = 1$. Now I have the following couple of questions. (1) Can we replace the second condition in the above theorem by the following? Suppose $\vert c_0 \vert \geq \vert c_1 \vert \geq \vert c_2 \vert \geq \cdots$. Does the conclusion of the above theorem still hold? Let $\{c_n\}_{n\in\mathbb{N}}$ be a sequence of complex numbers; suppose the power series $\sum c_n z^n$ has radius of convergence equal to $R$, where $R$ can either be a non-negative real number or $+\infty$. Then we can we say $R$ is also the radius of convergence of the power series $\sum \vert c_n \vert z^n$. Am I right?",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'power-series']"
17,What possible values of $a$ lead to the inequality $|f(x_1) - f(x_2)| \gt |g(x_1) - g(x_2)|$.,What possible values of  lead to the inequality .,a |f(x_1) - f(x_2)| \gt |g(x_1) - g(x_2)|,"Let $a \in \mathbb{R}$ and define $$f(x) = e^x, \quad g(x) = x^2 + ax + 1.$$ Find the possible values of $a$ such that for all $x_1, x_2 \in [0, 2], x_1 \not = x_2$, $$|f(x_1) - f(x_2)| \gt |g(x_1) - g(x_2)|.$$ By factorizing we have $$|e^{x_1} - e^{x_2}| \gt |(x_1 - x_2)(x_1 + x_2 + a)|.$$ But I am not sure how to proceed. In particular, I don't know how to deal with the absolute values and the two variables. Assume that $x_1 \gt x_2$ (without loss of generality) and we have $$\frac {e^{x_1} - e^{x_2}}{x_1 - x_2} \gt |x_1 + x_2 + a|,$$ but that doesn't seem to yield any solution. This question is about derivatives, if that helps.","Let $a \in \mathbb{R}$ and define $$f(x) = e^x, \quad g(x) = x^2 + ax + 1.$$ Find the possible values of $a$ such that for all $x_1, x_2 \in [0, 2], x_1 \not = x_2$, $$|f(x_1) - f(x_2)| \gt |g(x_1) - g(x_2)|.$$ By factorizing we have $$|e^{x_1} - e^{x_2}| \gt |(x_1 - x_2)(x_1 + x_2 + a)|.$$ But I am not sure how to proceed. In particular, I don't know how to deal with the absolute values and the two variables. Assume that $x_1 \gt x_2$ (without loss of generality) and we have $$\frac {e^{x_1} - e^{x_2}}{x_1 - x_2} \gt |x_1 + x_2 + a|,$$ but that doesn't seem to yield any solution. This question is about derivatives, if that helps.",,"['calculus', 'real-analysis', 'analysis']"
18,"Possible wrong gradient in my book, need confirmation","Possible wrong gradient in my book, need confirmation",,"While reading this chapter about iterative algorithms, I came up with the following function and it's gradient. I just can't find the same gradient showed. I tried two different approaches and both disagree with the book. Now I'm starting to think they made some mistake and I just need to confirm whether this is the case or not. By the way, one of my computations is the following. $$\frac{\partial f}{\partial x_i}(x) = \lim_{t\to 0} \frac{f(x+te_i)-f(x)}{t} = \lim_{t\to 0} \frac{\frac{1}{2}(x+te_i)^TA(x+te_i) -b^T(x+te_i)- \frac{1}{2}x^TAx + b^Tx}{t} =$$ $$= \lim_{t\to0} \frac{\frac{1}{2}(x^TAx + te^T_iAx + tx^Te_i + t^2e_i^Te_i) -b^Tx - tb^Te_i -\frac{1}{2}x^TAx + b^Tx}{t} = $$ $$= \lim_{t\to0} \frac{e^T_iAx + x^Te_i + te^T_ie_i - 2b^Te_i}{2} = \frac{e^T_iAx + x^Te_i - 2b^Te_i}{2} = \frac{(Ax)_i + x_i-2b_i}{2},$$ where $(Ax)_i$ is the ith line of $Ax$. Therefore, $\nabla f(x) = \frac{Ax + x - 2b}{2}$ or $\frac{Ax+x}{2}-b$. Anyway, the resulty doesn't match. Also, if the book indeed made a mistake, what should be the function $f$ to get that gradient? Thanks.","While reading this chapter about iterative algorithms, I came up with the following function and it's gradient. I just can't find the same gradient showed. I tried two different approaches and both disagree with the book. Now I'm starting to think they made some mistake and I just need to confirm whether this is the case or not. By the way, one of my computations is the following. $$\frac{\partial f}{\partial x_i}(x) = \lim_{t\to 0} \frac{f(x+te_i)-f(x)}{t} = \lim_{t\to 0} \frac{\frac{1}{2}(x+te_i)^TA(x+te_i) -b^T(x+te_i)- \frac{1}{2}x^TAx + b^Tx}{t} =$$ $$= \lim_{t\to0} \frac{\frac{1}{2}(x^TAx + te^T_iAx + tx^Te_i + t^2e_i^Te_i) -b^Tx - tb^Te_i -\frac{1}{2}x^TAx + b^Tx}{t} = $$ $$= \lim_{t\to0} \frac{e^T_iAx + x^Te_i + te^T_ie_i - 2b^Te_i}{2} = \frac{e^T_iAx + x^Te_i - 2b^Te_i}{2} = \frac{(Ax)_i + x_i-2b_i}{2},$$ where $(Ax)_i$ is the ith line of $Ax$. Therefore, $\nabla f(x) = \frac{Ax + x - 2b}{2}$ or $\frac{Ax+x}{2}-b$. Anyway, the resulty doesn't match. Also, if the book indeed made a mistake, what should be the function $f$ to get that gradient? Thanks.",,"['linear-algebra', 'analysis']"
19,"Mixed partials of the Beta function B$(a,b)$ at $(1,0^+)$",Mixed partials of the Beta function B at,"(a,b) (1,0^+)","In this post M.N.C.E gave the equality below $$\frac{\partial ^{5}}{\partial a^{3}\partial b^{2}}\mathrm{B}\left ( 1,0^{+} \right )=\left [ \frac{1}{b}+O\left ( 1 \right ) \right ]\left [ \left ( 12\zeta ^{2}\left ( 3 \right )-\frac{23\pi ^{6}}{1260} \right )b+O\left ( b^{2} \right ) \right ]_{b=0}$$ where $\mathrm B(a,b)$ is the Beta function. But how to get this equality? Can the same method be used to evaluate  $$\frac{\partial ^{6}}{\partial a^{3}\partial b^{3}}\mathrm{B}\left ( 1,0^{+} \right )~~,~~\frac{\partial ^{6}}{\partial a^{4}\partial b^{2}}\mathrm{B}\left ( 1,0^{+} \right )?$$","In this post M.N.C.E gave the equality below $$\frac{\partial ^{5}}{\partial a^{3}\partial b^{2}}\mathrm{B}\left ( 1,0^{+} \right )=\left [ \frac{1}{b}+O\left ( 1 \right ) \right ]\left [ \left ( 12\zeta ^{2}\left ( 3 \right )-\frac{23\pi ^{6}}{1260} \right )b+O\left ( b^{2} \right ) \right ]_{b=0}$$ where $\mathrm B(a,b)$ is the Beta function. But how to get this equality? Can the same method be used to evaluate  $$\frac{\partial ^{6}}{\partial a^{3}\partial b^{3}}\mathrm{B}\left ( 1,0^{+} \right )~~,~~\frac{\partial ^{6}}{\partial a^{4}\partial b^{2}}\mathrm{B}\left ( 1,0^{+} \right )?$$",,"['calculus', 'integration', 'analysis', 'special-functions', 'beta-function']"
20,Closed-Form solution for system of simple nonlinear equations,Closed-Form solution for system of simple nonlinear equations,,"I am interested in analytical solutions for a system of nonlinear equations. Motivation : The source of the question is a very convinient method to create random matrices with special properties . Mathematica can give me solutions up to certain sizes of the linear system, but I would like to have it for arbitrary size N. I can also use numerical algorithms (which I am doing at the moment), but for N in the order of $N\approx10.000$, they are quite slow. System of nonlinear equations : $$ (w_i \cdot \sum_{j=1}^N w_j) - w_i^2 = d_i $$ for $i=1...N$, and $w$ and $d$ are vectors with $N$ dimensions, and $w_i$ and $d_i$ is the $i$-th component of the vector. Both $d_i$ and $w_i \in \mathbb{R_+}$. I am providing the vector $d$ (i.e. N real non-negative numbers), and want to solve for $w_i$. Is there a way to solve this system analytically for arbitrary N? Edit : For clarification, if N=3 we have the following system of equations: $$ w_1 \cdot (w_2 + w_3) = d_1 \\ w_2 \cdot (w_1 + w_3) = d_2 \\ w_3 \cdot (w_1 + w_2) = d_3 $$ with $w_i, d_i \in \mathbb{R}$. For a given vector $d=(d_1,d_2,d_3)$, I want to get $w=(w_1,w_2,w_3)$. Edit2: I think I see a way how it could be solved, but I'm not certain: Let's set $c=\sum_{j=1}^N w_j$, which is the sum of all weights. What we have now: $$c \cdot w_i - w_i^2 = d_i \\ w_i^2 - c \cdot w_i + d_i = 0 $$ which has two solutions: $$w_{i_{1,2}} = \frac{c}{2} \pm \sqrt{ \left(\frac{c}{2}\right)^2 - d_i} $$ and the normalisation constant $c$ can be calculated by the sum of all weights: $$\sum_{j=1}^N w_j = \sum_{j=1}^N \left(\frac{c}{2} \pm \sqrt{ \left(\frac{c}{2}\right)^2 - d_j} \right) = c $$ Is this correct? Do you know an analytical solution for c?","I am interested in analytical solutions for a system of nonlinear equations. Motivation : The source of the question is a very convinient method to create random matrices with special properties . Mathematica can give me solutions up to certain sizes of the linear system, but I would like to have it for arbitrary size N. I can also use numerical algorithms (which I am doing at the moment), but for N in the order of $N\approx10.000$, they are quite slow. System of nonlinear equations : $$ (w_i \cdot \sum_{j=1}^N w_j) - w_i^2 = d_i $$ for $i=1...N$, and $w$ and $d$ are vectors with $N$ dimensions, and $w_i$ and $d_i$ is the $i$-th component of the vector. Both $d_i$ and $w_i \in \mathbb{R_+}$. I am providing the vector $d$ (i.e. N real non-negative numbers), and want to solve for $w_i$. Is there a way to solve this system analytically for arbitrary N? Edit : For clarification, if N=3 we have the following system of equations: $$ w_1 \cdot (w_2 + w_3) = d_1 \\ w_2 \cdot (w_1 + w_3) = d_2 \\ w_3 \cdot (w_1 + w_2) = d_3 $$ with $w_i, d_i \in \mathbb{R}$. For a given vector $d=(d_1,d_2,d_3)$, I want to get $w=(w_1,w_2,w_3)$. Edit2: I think I see a way how it could be solved, but I'm not certain: Let's set $c=\sum_{j=1}^N w_j$, which is the sum of all weights. What we have now: $$c \cdot w_i - w_i^2 = d_i \\ w_i^2 - c \cdot w_i + d_i = 0 $$ which has two solutions: $$w_{i_{1,2}} = \frac{c}{2} \pm \sqrt{ \left(\frac{c}{2}\right)^2 - d_i} $$ and the normalisation constant $c$ can be calculated by the sum of all weights: $$\sum_{j=1}^N w_j = \sum_{j=1}^N \left(\frac{c}{2} \pm \sqrt{ \left(\frac{c}{2}\right)^2 - d_j} \right) = c $$ Is this correct? Do you know an analytical solution for c?",,"['analysis', 'nonlinear-system']"
21,Theorem 3.17 in Baby Rudin: Infinite Limits and Upper and Lower Limits of Real Sequences,Theorem 3.17 in Baby Rudin: Infinite Limits and Upper and Lower Limits of Real Sequences,,"Here're Definitions 3.15 and 3.16 and Theorem 3.17 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition. Definition 3.15: Let $\{s_n \}$ be a sequence of real numbers with the following property: For every real $M$ there is an integer $N$ such that $n \geq N$ implies $s_n \geq M$. We then write $$s_n \rightarrow +\infty.$$ Similarly, if for every real $M$ there is an integer $N$ such that $n \geq N$ implies $s_n \leq M$, we write $$s_n \rightarrow -\infty.$$ Definition 3.16: Let $\{ s_n \}$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ (in the extended real number system) such that $s_{n_k} \rightarrow x$ for some subsequence $\{s_{n_k}\}$. This set $E$ contains all subsequential limits as defined in Definition 3.5, plus possibly the numbers $+\infty$, $-\infty$. We now recall Definitions 1.8 and 1.23 and put $$s^* = \sup E,$$ $$s_* = \inf E.$$  The numbers $s^*$, $s_*$ are called the upper and lower limits of $\{ s_n \}$; we use the notation $$\lim_{n\to\infty} \sup s_n = s^*, \ \ \ \lim_{n\to \infty} \inf s_n = s_*.$$ Now, Definition 3.5: Given a sequence $\{ p_n \}$, consider a sequence $\{n_k\}$ of positive integers, such that $n_1 < n_2 < n_3 < \cdots$. Then the sequence $\{p_{n_k}  \}$ is called a subsequence of $\{p_n \}$. If $\{p_{n_k}\}$ converges, its limit is called a subsequential limit of $\{p_n \}$. Definition 1.8: Suppose $S$ is an ordered set, $E \subset S$, and $E$ is bounded above. Suppose there exists an $\alpha \in S$ with the following properties: (i) $\alpha$ is an upper bound of $E$. (ii) If $\gamma < \alpha$ then $\gamma$ is not an upper bound of $E$. Then $\alpha$ is called the least upper bound of $E$ or the supremum of $E$, and we write $$\alpha = \sup E.$$ The greatest lower bound, or infimum, of a set $E$ which is bounded below is defined in the same manner: The statement $$\alpha = \inf E$$ means that $\alpha$ is a lower bound of $E$ and that no $\beta$ with $\beta > \alpha$ is a lower bound of $E$. Definition 1.23: The extended real number system consists of the real field $R$ and two symbols, $+\infty$ and $-\infty$. We preserve the original order in $R$, and define $$-\infty < x < +\infty$$ for every $x \in R$. It is then clear that $+\infty$ is an upper bound of every subset of the extended real number system, and that every nonempty subset has a least upper bound. If, for example, $E$ is a nonempty set of real numbers which is not bounded above in $R$, then $\sup E = +\infty$ in the extended real number system. Exactly the same remarks apply to lower bounds. Finally, here's Theorem 3.17. Let $\{s_n \}$ be a sequence of real numbers. Let $E$ and $S^*$ have the same meaning as in Definition 3.16. Then $s^*$ has the following two properties: (a) $s^* \in E$. (b) If $x> s^*$, there is an integer $N$ such that $n \geq N$ implies $s_n < x$. Moreover, $s^*$ is the only number with the properties (a) and (b). Of course, an analogous result is true for $s_*$. And, here's Rudin's proof: (a) If $s^* = +\infty$, then $E$ is not bounded above; hence $\{s_n\}$ is not bounded above, and there is a subsequence $\{s_{n_k}\}$ such that $s_{n_k} \rightarrow +\infty$. If $s^*$ is real, then $E$ is bounded above, and at least one subsequential limit exists, so that (a) follows from Theorems 3.7 and 2.28. [I'll state these theorems after the proof. ] If $s^* = -\infty$, then $E$ contains only one element, namely $-\infty$, and there is no subsequential limit. Hence, for any real $M$, $s_n > M$ for at most a finite number of values of $n$, so that $s_n \rightarrow -\infty$. This establishes (a) in all cases. (b) Suppose there is a number $x > s^*$ such that $s_n \geq x$ for infinitely many values of $n$. In that case, there is a number $y \in E$ such that $y \geq x > s^*$, contradicting the definition of $s^*$. Thus $s^*$ satisfies both (a) and (b). To show the uniqueness, suppose there are two numbers, $p$ and $q$, which satisfy (a) and (b), and suppose $p < q$. Choose $x$ such that $p < x < q$. Since $p$ satisfies (b), we have $s_n < x$ for $n \geq N$. But then $q$ cannot satisfy (a). Now for Theorem 2.28: Let $E$ be a nonempty set of real numbers which is bounded above. Let $y = \sup E$. Then $y \in \overline{E}$. Hence $y \in E$ if $E$ is closed. And, Theorem 3.7: The subsequential limits of a sequence $\{p_n\}$ in a metric space $X$ form a closed subset of $X$. My reason for copying so many definitions and theorem from Rudin's book into my question is to keep my post as self-contained as possible. Now here's my attempt at filling in the details in the proof of Theorem 3.17. (a)  If $s^* = +\infty$, then the set $E$ is not bounded above in $\mathbb{R}$. We now find a subsequence of $\{s_n \}$ which diverges to $+\infty$. As $E$ is not bounded above in $\mathbb{R}$, so there is an element $x_1 \in E$ such that $x_1 > 1$. If $x_1 = +\infty$, then we are done. Otherwise, $x_1$ is a subsequential limit of $\{s_n\}$. Thus there is a strictly increasing function $\varphi_1 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_1 = \lim_{n\to\infty} s_{\varphi_1(n)}.$$ So, for any real number $\varepsilon > 0$, we can find a natural number $N_1$ such that $n > N_1$ implies that $$ \left\vert s_{\varphi_1(n)} - x_1 \right\vert < \varepsilon.$$ Let's take $\varepsilon$ such that $$ 0 < \varepsilon < \frac{x_1 - 1}{2}.$$ Then we can conclude that $$s_{\varphi_1(N_1 + 1)} > 1.$$ Let $$n_1 \colon= \varphi_1(N_1+1).$$ Then $$s_{n_1} > 1.$$    Now as $E$ is not bounded above in $\mathbb{R}$, there is an element $x_2 \in E$ such that $x_2 > 2$. If $x_2 = +\infty$, then we are done. Otherwise, $x_2$ is a subsequential limit of $\{s_n\}$. That is, there is a strictly increasing function $\varphi_2 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_2 = \lim_{n\to\infty} s_{\varphi_2(n)}.$$ So, for any real number $\varepsilon > 0$, we can find a natural number $N_2$ such that $n > N_2$ implies that $$ \left\vert s_{\varphi_2(n)} - x_2 \right\vert < \varepsilon. $$ Thus, if we take $\varepsilon$ such that $$0 < \varepsilon < \frac{x_2 - 2}{2},$$ then we can conclude that $$s_{\varphi_2(N_2 + 1)} > 2.$$ Let's put $$n_2 \colon= \max \left\{ \varphi_2( n_1 +1 ), \varphi_2(N_2+1) \right\} .$$ We note that since $\varphi_2 \colon \mathbb{N} \to \mathbb{N}$ is a strictly increasing function, we can show that $$n \leq \varphi_2(n)$$ for all $n \in \mathbb{N}$. So we have $$n_2 \geq \varphi_2(n_1 + 1) \geq n_1 + 1 > n_1.$$ Now we can find an element $x_3 \in E$ such that $x_3 > 3$. If $x_3 = +\infty$, then we are done. Otherwise, $x_3$ is a subsequential limit of $\{s_n \}$. That is, there is a strictly increasing function $\varphi_3 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_3 = \lim_{n \to \infty} s_{\varphi_3(n)}.$$ So, for any real number $\varepsilon > 0$, we can find a natural number $N_3$ such that $n > N_3$ implies that $$\left\vert s_{\varphi_3(n)} - x_3 \right\vert < \varepsilon.$$ So, if we take $\varepsilon$ such that $$0 < \varepsilon < \frac{x_3-3}{3},$$ then we can conclude that $$s_{\varphi_3(N_3+1)} > 3.$$   Let's take $$n_3 \colon= \max \left\{ \varphi_3 ( n_2+1), \varphi_3 (N_3 + 1) \right\}.$$ Since $\varphi_3 \colon \mathbb{N} \to \mathbb{N}$ is a strictly increasing function, we can show that $$ n \leq \varphi_3(n) $$ for all $n \in \mathbb{N}$. So    $$n_3 \geq \varphi_3(n_2 + 1) \geq n_2 + 1 > n_2.$$ In this way, we obtain a sequence $\{n_k\}$ of natural numbers such that  $n_1 < n_2 < n_3 < \cdots$ and $$s_{n_k} > k$$ for all $k \in \mathbb{N}$. Now for any real number $M$, there is a natural number $K$ such that $K > M$. So, for all $k \in \mathbb{N}$ such that $k > K$, we have $$s_{n_k} > k > K > M;$$ that is, $s_{n_k} > M$ for all $k \in \mathbb{N}$ such that  $k > K$. Therefore, we can conclude that $$s_{n_k} \rightarrow +\infty,$$ which shows that $E$ contains $+\infty$. If $s^* \in \mathbb{R}$, then $E$ is bounded above in $\mathbb{R}$ and $s^* = \sup E$. We show that some subsequence of $\{s_n \}$ converges to $s^*$. There exists an element $x_1 \in E$ such that $s^*-1 < x_1 \leq s^*$. If $x_1 = s^*$, then we are done. Otherwise we have $s^*-1 < x_1 < s^*$. Now $x_1$ is a subsequential limit of $\{s_n\}$. So there is a strictly increasing function $\varphi_1 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_1 = \lim_{n\to\infty} s_{\varphi_1(n)}.$$ Thus, for any real number $\varepsilon > 0$ we can find a natural number $N_1$ such that $n > N_1$ implies $$\left\vert s_{\varphi_1(n)} - x_1 \right\vert < \varepsilon. $$ By taking $\varepsilon$ such that $$ 0 < \varepsilon < \frac 1 3 \min \{ s^* - x_1, \ x_1 - s^* + 1 \},$$  we can conclude that $$s^* -1 < s_{\varphi_1(N_1 + 1) } < s^*.$$ Let's take $$n_1 \colon= \varphi_1(N_1 + 1).$$ As $s^* = \sup E$ in $\mathbb{R}$, there is an element $x_2 \in E$ such that $s^* - \frac 1 2 < x_2 \leq s^*$. If $x_2 = s^*$, then we are done. Otherwise we have $$s^*- \frac 1 2 < x_2 < s^*.$$ Now $x_2$ is a subsequential limit of $\{s_n\}$. So there is a strictly increasing function $\varphi_2 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_2 = \lim_{n\to\infty} s_{\varphi_2(n)}.$$ So, given a real number $\varepsilon > 0$, we can find a natural number $N_2$ such that $n>N_2$ implies that $$\left\vert s_{\varphi_2(n)} - x_2 \right\vert < \varepsilon.$$ Thus, by taking $\varepsilon$ such that    $$0< \varepsilon < \frac 1 3 \min \left\{ s^* - x_2 , \ x_2 - s^* + \frac 1 2 \right\},$$ we can conclude that $$s^* - \frac 1 2 < s_{\varphi_2(N_2 + 1)} < s^*.$$ Let's take $$n_2 \colon= \max \left\{ \varphi_2(n_1 + 1), \varphi_2(N_2+ 1) \right\}.$$ Then $n_2 > n_1$ and $$s^* - \frac 1 2 < s_{n_2} < s^*.$$ Now as $s^* = \sup E$ in $\mathbb{R}$, we can find an element $x_3 \in E$ such that $$s^* -\frac 1 3 < x_3 \leq  s^*.$$ If this $x_3 = s^*$, then we are done. Otherwise we have $$s^* - \frac 1 3 < x_3 < s^*.$$ Moreover, $x_3$ is a subsequential limit of $\{s_n\}$. So there is a strictly increasing function $\varphi_3 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_2 = \lim_{n\to\infty} s_{\varphi_3(n)}.$$  Thus, given a real number $\varepsilon > 0$, we can find a natural number $N_3$ such that $n > N_3$ implies $$\left\vert s_{\varphi_3(n)} - x_3 \right\vert < \varepsilon.$$ Thus, by taking $\varepsilon$ such that   $$0< \varepsilon < \frac 1 3 \min \left\{ s^* - x_3 , \ x_3 - s^* + \frac 1 3 \right\},$$ we can conclude that $$s^* - \frac 1 3 < s_{\varphi_3(N_3 + 1)} < s^*.$$ Let's take $$n_3 \colon= \max \left\{ \varphi_3(n_2 + 1), \varphi_3(N_3+1) \right\}.$$ Then $n_3 > n_2$ and $$s^* - \frac 1 3 < s_{n_3} < s^*.$$ Continuing in this way, we obtain a sequence $\{n_k \}$ of natural numbers such that $n_1 < n_2 < n_3 < \cdots$ and $$s^* - \frac 1 k < s_{n_k} < s^*$$ for all $k \in \mathbb{N}$. From here, we can show that the subsequence $\{s_{n_k}\}$ of $\{s_n\}$ converges to $s^*$. If $s^* = -\infty$, then there is no subsequential limit of $\{s_n\}$. Moreover, $\{s_n \}$ is bounded above in $\mathbb{R}$, for otherwise there would be a subsequence of $\{s_n \}$ which would diverge to  $+\infty$. Let $M$ be any real number. If we were to have  $s_n \geq M$ for infinitely many values of $n \in \mathbb{N}$, then we could find a subsequence of $\{s_n\}$ which would be bounded and hence have a convergent subsequence (by Theorem 3.6 (b) in Rudin), which would in turn be a convergent subsequence of $\{s_n\}$ whose limit $x$ would satisfy $x \in E$ and $x > s^*$, a contradiction. So we must have $s_n \geq M$ for at most a finite number of values of $n$. Thus, there is a natural number $N$ such that $n > N$ implies that $s_n < M$. Since $M$ was an arbitrary real number, we can conclude that $$s_n \rightarrow -\infty, $$ which shows that $-\infty \in E$. Thus we have shown that $s^* \in E$ in all possible cases. Now part (b): We assume the contrary. That is, we assume that $x > s^*$ and that there are infinitely many $n$ such that $s_n \geq x$. Since $s^* < x$, $s^*$ cannot be $+\infty$. Moreover, there is a subsequence  $\{s_{\varphi(n)}\}$ of $\{s_n\}$ such that $$s_{\varphi(n)} \geq x$$   for all $n \in \mathbb{N}$, where $\varphi \colon \mathbb{N} \to \mathbb{N}$ is a strictly increasing function. If the subsequence $\{s_{\varphi(n)}\}$ is not bounded above, then some subsequence of  $\{s_{\varphi(n)}\}$ --- which would in turn be a subsequence of  $\{s_n\}$ ---  would diverge to $+\infty$, showing that $+\infty \geq x > s^*$ and $+\infty \in E$, a contradiction. On the other hand, if the subsequence  $\{s_{\varphi(n)}\}$ is bounded above, then it would be a bounded sequence in $\mathbb{R}$ and so would have a convergent subsequence (by Theorem 3.6(b) in Rudin), which would in turn be a convergent subsequence of $\{s_n\}$, whose limit $s$ would satisfy $s \in E$ and $s \geq x > s^*$, which is a contradiction to the fact that $s^*$ is the supremum of $E$. To show that $s^*$ is the only number with the properties (a) and (b), suppose that there are two numbers $p$ and $q$ which satisfiy properties (a) and (b) of Theorem 3.17 in Rudin, and suppose that $p < q$. Let's choose a real number $x$ such that $p < x < q$. Then by property (b), there is an integer $N$ such that $s_n < x$ for all $n \in \mathbb{N}$ such that $n \geq N$. Let us chooes a real number $\varepsilon$ such that $$0 < \varepsilon < \frac{ q-x}{3}.$$ Then there are at most a finite number of terms of $\{s_n\}$ in the neighborhood $(q-\varepsilon, q+ \varepsilon)$. So no subsequence of $\{s_n \}$ can converge to $q$ (or diverge to $+\infty$ if $q = +\infty$), showing that $q \not\in E$, which is a contradiction to the property  (a) for $q$. Based on how I have elaborated Rudin's proof in the last few paragraphs, have I been able to understand this proof correctly? If not, where am I going wrong (or falling short)? What is lacking in my reasoning, apart from brevity of course?","Here're Definitions 3.15 and 3.16 and Theorem 3.17 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition. Definition 3.15: Let $\{s_n \}$ be a sequence of real numbers with the following property: For every real $M$ there is an integer $N$ such that $n \geq N$ implies $s_n \geq M$. We then write $$s_n \rightarrow +\infty.$$ Similarly, if for every real $M$ there is an integer $N$ such that $n \geq N$ implies $s_n \leq M$, we write $$s_n \rightarrow -\infty.$$ Definition 3.16: Let $\{ s_n \}$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ (in the extended real number system) such that $s_{n_k} \rightarrow x$ for some subsequence $\{s_{n_k}\}$. This set $E$ contains all subsequential limits as defined in Definition 3.5, plus possibly the numbers $+\infty$, $-\infty$. We now recall Definitions 1.8 and 1.23 and put $$s^* = \sup E,$$ $$s_* = \inf E.$$  The numbers $s^*$, $s_*$ are called the upper and lower limits of $\{ s_n \}$; we use the notation $$\lim_{n\to\infty} \sup s_n = s^*, \ \ \ \lim_{n\to \infty} \inf s_n = s_*.$$ Now, Definition 3.5: Given a sequence $\{ p_n \}$, consider a sequence $\{n_k\}$ of positive integers, such that $n_1 < n_2 < n_3 < \cdots$. Then the sequence $\{p_{n_k}  \}$ is called a subsequence of $\{p_n \}$. If $\{p_{n_k}\}$ converges, its limit is called a subsequential limit of $\{p_n \}$. Definition 1.8: Suppose $S$ is an ordered set, $E \subset S$, and $E$ is bounded above. Suppose there exists an $\alpha \in S$ with the following properties: (i) $\alpha$ is an upper bound of $E$. (ii) If $\gamma < \alpha$ then $\gamma$ is not an upper bound of $E$. Then $\alpha$ is called the least upper bound of $E$ or the supremum of $E$, and we write $$\alpha = \sup E.$$ The greatest lower bound, or infimum, of a set $E$ which is bounded below is defined in the same manner: The statement $$\alpha = \inf E$$ means that $\alpha$ is a lower bound of $E$ and that no $\beta$ with $\beta > \alpha$ is a lower bound of $E$. Definition 1.23: The extended real number system consists of the real field $R$ and two symbols, $+\infty$ and $-\infty$. We preserve the original order in $R$, and define $$-\infty < x < +\infty$$ for every $x \in R$. It is then clear that $+\infty$ is an upper bound of every subset of the extended real number system, and that every nonempty subset has a least upper bound. If, for example, $E$ is a nonempty set of real numbers which is not bounded above in $R$, then $\sup E = +\infty$ in the extended real number system. Exactly the same remarks apply to lower bounds. Finally, here's Theorem 3.17. Let $\{s_n \}$ be a sequence of real numbers. Let $E$ and $S^*$ have the same meaning as in Definition 3.16. Then $s^*$ has the following two properties: (a) $s^* \in E$. (b) If $x> s^*$, there is an integer $N$ such that $n \geq N$ implies $s_n < x$. Moreover, $s^*$ is the only number with the properties (a) and (b). Of course, an analogous result is true for $s_*$. And, here's Rudin's proof: (a) If $s^* = +\infty$, then $E$ is not bounded above; hence $\{s_n\}$ is not bounded above, and there is a subsequence $\{s_{n_k}\}$ such that $s_{n_k} \rightarrow +\infty$. If $s^*$ is real, then $E$ is bounded above, and at least one subsequential limit exists, so that (a) follows from Theorems 3.7 and 2.28. [I'll state these theorems after the proof. ] If $s^* = -\infty$, then $E$ contains only one element, namely $-\infty$, and there is no subsequential limit. Hence, for any real $M$, $s_n > M$ for at most a finite number of values of $n$, so that $s_n \rightarrow -\infty$. This establishes (a) in all cases. (b) Suppose there is a number $x > s^*$ such that $s_n \geq x$ for infinitely many values of $n$. In that case, there is a number $y \in E$ such that $y \geq x > s^*$, contradicting the definition of $s^*$. Thus $s^*$ satisfies both (a) and (b). To show the uniqueness, suppose there are two numbers, $p$ and $q$, which satisfy (a) and (b), and suppose $p < q$. Choose $x$ such that $p < x < q$. Since $p$ satisfies (b), we have $s_n < x$ for $n \geq N$. But then $q$ cannot satisfy (a). Now for Theorem 2.28: Let $E$ be a nonempty set of real numbers which is bounded above. Let $y = \sup E$. Then $y \in \overline{E}$. Hence $y \in E$ if $E$ is closed. And, Theorem 3.7: The subsequential limits of a sequence $\{p_n\}$ in a metric space $X$ form a closed subset of $X$. My reason for copying so many definitions and theorem from Rudin's book into my question is to keep my post as self-contained as possible. Now here's my attempt at filling in the details in the proof of Theorem 3.17. (a)  If $s^* = +\infty$, then the set $E$ is not bounded above in $\mathbb{R}$. We now find a subsequence of $\{s_n \}$ which diverges to $+\infty$. As $E$ is not bounded above in $\mathbb{R}$, so there is an element $x_1 \in E$ such that $x_1 > 1$. If $x_1 = +\infty$, then we are done. Otherwise, $x_1$ is a subsequential limit of $\{s_n\}$. Thus there is a strictly increasing function $\varphi_1 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_1 = \lim_{n\to\infty} s_{\varphi_1(n)}.$$ So, for any real number $\varepsilon > 0$, we can find a natural number $N_1$ such that $n > N_1$ implies that $$ \left\vert s_{\varphi_1(n)} - x_1 \right\vert < \varepsilon.$$ Let's take $\varepsilon$ such that $$ 0 < \varepsilon < \frac{x_1 - 1}{2}.$$ Then we can conclude that $$s_{\varphi_1(N_1 + 1)} > 1.$$ Let $$n_1 \colon= \varphi_1(N_1+1).$$ Then $$s_{n_1} > 1.$$    Now as $E$ is not bounded above in $\mathbb{R}$, there is an element $x_2 \in E$ such that $x_2 > 2$. If $x_2 = +\infty$, then we are done. Otherwise, $x_2$ is a subsequential limit of $\{s_n\}$. That is, there is a strictly increasing function $\varphi_2 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_2 = \lim_{n\to\infty} s_{\varphi_2(n)}.$$ So, for any real number $\varepsilon > 0$, we can find a natural number $N_2$ such that $n > N_2$ implies that $$ \left\vert s_{\varphi_2(n)} - x_2 \right\vert < \varepsilon. $$ Thus, if we take $\varepsilon$ such that $$0 < \varepsilon < \frac{x_2 - 2}{2},$$ then we can conclude that $$s_{\varphi_2(N_2 + 1)} > 2.$$ Let's put $$n_2 \colon= \max \left\{ \varphi_2( n_1 +1 ), \varphi_2(N_2+1) \right\} .$$ We note that since $\varphi_2 \colon \mathbb{N} \to \mathbb{N}$ is a strictly increasing function, we can show that $$n \leq \varphi_2(n)$$ for all $n \in \mathbb{N}$. So we have $$n_2 \geq \varphi_2(n_1 + 1) \geq n_1 + 1 > n_1.$$ Now we can find an element $x_3 \in E$ such that $x_3 > 3$. If $x_3 = +\infty$, then we are done. Otherwise, $x_3$ is a subsequential limit of $\{s_n \}$. That is, there is a strictly increasing function $\varphi_3 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_3 = \lim_{n \to \infty} s_{\varphi_3(n)}.$$ So, for any real number $\varepsilon > 0$, we can find a natural number $N_3$ such that $n > N_3$ implies that $$\left\vert s_{\varphi_3(n)} - x_3 \right\vert < \varepsilon.$$ So, if we take $\varepsilon$ such that $$0 < \varepsilon < \frac{x_3-3}{3},$$ then we can conclude that $$s_{\varphi_3(N_3+1)} > 3.$$   Let's take $$n_3 \colon= \max \left\{ \varphi_3 ( n_2+1), \varphi_3 (N_3 + 1) \right\}.$$ Since $\varphi_3 \colon \mathbb{N} \to \mathbb{N}$ is a strictly increasing function, we can show that $$ n \leq \varphi_3(n) $$ for all $n \in \mathbb{N}$. So    $$n_3 \geq \varphi_3(n_2 + 1) \geq n_2 + 1 > n_2.$$ In this way, we obtain a sequence $\{n_k\}$ of natural numbers such that  $n_1 < n_2 < n_3 < \cdots$ and $$s_{n_k} > k$$ for all $k \in \mathbb{N}$. Now for any real number $M$, there is a natural number $K$ such that $K > M$. So, for all $k \in \mathbb{N}$ such that $k > K$, we have $$s_{n_k} > k > K > M;$$ that is, $s_{n_k} > M$ for all $k \in \mathbb{N}$ such that  $k > K$. Therefore, we can conclude that $$s_{n_k} \rightarrow +\infty,$$ which shows that $E$ contains $+\infty$. If $s^* \in \mathbb{R}$, then $E$ is bounded above in $\mathbb{R}$ and $s^* = \sup E$. We show that some subsequence of $\{s_n \}$ converges to $s^*$. There exists an element $x_1 \in E$ such that $s^*-1 < x_1 \leq s^*$. If $x_1 = s^*$, then we are done. Otherwise we have $s^*-1 < x_1 < s^*$. Now $x_1$ is a subsequential limit of $\{s_n\}$. So there is a strictly increasing function $\varphi_1 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_1 = \lim_{n\to\infty} s_{\varphi_1(n)}.$$ Thus, for any real number $\varepsilon > 0$ we can find a natural number $N_1$ such that $n > N_1$ implies $$\left\vert s_{\varphi_1(n)} - x_1 \right\vert < \varepsilon. $$ By taking $\varepsilon$ such that $$ 0 < \varepsilon < \frac 1 3 \min \{ s^* - x_1, \ x_1 - s^* + 1 \},$$  we can conclude that $$s^* -1 < s_{\varphi_1(N_1 + 1) } < s^*.$$ Let's take $$n_1 \colon= \varphi_1(N_1 + 1).$$ As $s^* = \sup E$ in $\mathbb{R}$, there is an element $x_2 \in E$ such that $s^* - \frac 1 2 < x_2 \leq s^*$. If $x_2 = s^*$, then we are done. Otherwise we have $$s^*- \frac 1 2 < x_2 < s^*.$$ Now $x_2$ is a subsequential limit of $\{s_n\}$. So there is a strictly increasing function $\varphi_2 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_2 = \lim_{n\to\infty} s_{\varphi_2(n)}.$$ So, given a real number $\varepsilon > 0$, we can find a natural number $N_2$ such that $n>N_2$ implies that $$\left\vert s_{\varphi_2(n)} - x_2 \right\vert < \varepsilon.$$ Thus, by taking $\varepsilon$ such that    $$0< \varepsilon < \frac 1 3 \min \left\{ s^* - x_2 , \ x_2 - s^* + \frac 1 2 \right\},$$ we can conclude that $$s^* - \frac 1 2 < s_{\varphi_2(N_2 + 1)} < s^*.$$ Let's take $$n_2 \colon= \max \left\{ \varphi_2(n_1 + 1), \varphi_2(N_2+ 1) \right\}.$$ Then $n_2 > n_1$ and $$s^* - \frac 1 2 < s_{n_2} < s^*.$$ Now as $s^* = \sup E$ in $\mathbb{R}$, we can find an element $x_3 \in E$ such that $$s^* -\frac 1 3 < x_3 \leq  s^*.$$ If this $x_3 = s^*$, then we are done. Otherwise we have $$s^* - \frac 1 3 < x_3 < s^*.$$ Moreover, $x_3$ is a subsequential limit of $\{s_n\}$. So there is a strictly increasing function $\varphi_3 \colon \mathbb{N} \to \mathbb{N}$ such that $$x_2 = \lim_{n\to\infty} s_{\varphi_3(n)}.$$  Thus, given a real number $\varepsilon > 0$, we can find a natural number $N_3$ such that $n > N_3$ implies $$\left\vert s_{\varphi_3(n)} - x_3 \right\vert < \varepsilon.$$ Thus, by taking $\varepsilon$ such that   $$0< \varepsilon < \frac 1 3 \min \left\{ s^* - x_3 , \ x_3 - s^* + \frac 1 3 \right\},$$ we can conclude that $$s^* - \frac 1 3 < s_{\varphi_3(N_3 + 1)} < s^*.$$ Let's take $$n_3 \colon= \max \left\{ \varphi_3(n_2 + 1), \varphi_3(N_3+1) \right\}.$$ Then $n_3 > n_2$ and $$s^* - \frac 1 3 < s_{n_3} < s^*.$$ Continuing in this way, we obtain a sequence $\{n_k \}$ of natural numbers such that $n_1 < n_2 < n_3 < \cdots$ and $$s^* - \frac 1 k < s_{n_k} < s^*$$ for all $k \in \mathbb{N}$. From here, we can show that the subsequence $\{s_{n_k}\}$ of $\{s_n\}$ converges to $s^*$. If $s^* = -\infty$, then there is no subsequential limit of $\{s_n\}$. Moreover, $\{s_n \}$ is bounded above in $\mathbb{R}$, for otherwise there would be a subsequence of $\{s_n \}$ which would diverge to  $+\infty$. Let $M$ be any real number. If we were to have  $s_n \geq M$ for infinitely many values of $n \in \mathbb{N}$, then we could find a subsequence of $\{s_n\}$ which would be bounded and hence have a convergent subsequence (by Theorem 3.6 (b) in Rudin), which would in turn be a convergent subsequence of $\{s_n\}$ whose limit $x$ would satisfy $x \in E$ and $x > s^*$, a contradiction. So we must have $s_n \geq M$ for at most a finite number of values of $n$. Thus, there is a natural number $N$ such that $n > N$ implies that $s_n < M$. Since $M$ was an arbitrary real number, we can conclude that $$s_n \rightarrow -\infty, $$ which shows that $-\infty \in E$. Thus we have shown that $s^* \in E$ in all possible cases. Now part (b): We assume the contrary. That is, we assume that $x > s^*$ and that there are infinitely many $n$ such that $s_n \geq x$. Since $s^* < x$, $s^*$ cannot be $+\infty$. Moreover, there is a subsequence  $\{s_{\varphi(n)}\}$ of $\{s_n\}$ such that $$s_{\varphi(n)} \geq x$$   for all $n \in \mathbb{N}$, where $\varphi \colon \mathbb{N} \to \mathbb{N}$ is a strictly increasing function. If the subsequence $\{s_{\varphi(n)}\}$ is not bounded above, then some subsequence of  $\{s_{\varphi(n)}\}$ --- which would in turn be a subsequence of  $\{s_n\}$ ---  would diverge to $+\infty$, showing that $+\infty \geq x > s^*$ and $+\infty \in E$, a contradiction. On the other hand, if the subsequence  $\{s_{\varphi(n)}\}$ is bounded above, then it would be a bounded sequence in $\mathbb{R}$ and so would have a convergent subsequence (by Theorem 3.6(b) in Rudin), which would in turn be a convergent subsequence of $\{s_n\}$, whose limit $s$ would satisfy $s \in E$ and $s \geq x > s^*$, which is a contradiction to the fact that $s^*$ is the supremum of $E$. To show that $s^*$ is the only number with the properties (a) and (b), suppose that there are two numbers $p$ and $q$ which satisfiy properties (a) and (b) of Theorem 3.17 in Rudin, and suppose that $p < q$. Let's choose a real number $x$ such that $p < x < q$. Then by property (b), there is an integer $N$ such that $s_n < x$ for all $n \in \mathbb{N}$ such that $n \geq N$. Let us chooes a real number $\varepsilon$ such that $$0 < \varepsilon < \frac{ q-x}{3}.$$ Then there are at most a finite number of terms of $\{s_n\}$ in the neighborhood $(q-\varepsilon, q+ \varepsilon)$. So no subsequence of $\{s_n \}$ can converge to $q$ (or diverge to $+\infty$ if $q = +\infty$), showing that $q \not\in E$, which is a contradiction to the property  (a) for $q$. Based on how I have elaborated Rudin's proof in the last few paragraphs, have I been able to understand this proof correctly? If not, where am I going wrong (or falling short)? What is lacking in my reasoning, apart from brevity of course?",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'limsup-and-liminf']"
22,"Real Analysis, Folland Problem 6.2.17 Dual of $L^p$","Real Analysis, Folland Problem 6.2.17 Dual of",L^p,"Theorem 6.14 - Let $p$ and $q$ be conjugate exponents. Suppose that $g$ is a measurable function on $X$ such that $fg\in L^1$ for all $f$ in the space $\sum$ of simple functions that vanish outside a set of finite measure, and the quantity $$M_q(g) = \sup\{|\int fg|:f \in \sum \ \text{and} \ \|f\|_{p} =1\}$$ is finite. Also, suppose either that $S_g = \{x: g(x)\neq 0\}$ is $\sigma$-finite or that $\mu$ is semifinite. Then $g\in L^q$ and $M_q(g) = \|g\|_{q}$ Problem 6.2.17 - With notation as in Theorem 6.14, if $\mu$ is semifinite, $q < \infty$, and $M_q(g) < \infty$, then $\{x: |g(x)| > \epsilon \}$ has finite measure for all $\epsilon > 0$ has finite measure for all $\epsilon > 0$ and hence $S_g$ is $\sigma$-finite. As you can tell by the plethora of posts I have on $L^p$ spaces I am really struggling to grasp these concepts and solve these problems. Any advice or suggestions is greatly appreciated.","Theorem 6.14 - Let $p$ and $q$ be conjugate exponents. Suppose that $g$ is a measurable function on $X$ such that $fg\in L^1$ for all $f$ in the space $\sum$ of simple functions that vanish outside a set of finite measure, and the quantity $$M_q(g) = \sup\{|\int fg|:f \in \sum \ \text{and} \ \|f\|_{p} =1\}$$ is finite. Also, suppose either that $S_g = \{x: g(x)\neq 0\}$ is $\sigma$-finite or that $\mu$ is semifinite. Then $g\in L^q$ and $M_q(g) = \|g\|_{q}$ Problem 6.2.17 - With notation as in Theorem 6.14, if $\mu$ is semifinite, $q < \infty$, and $M_q(g) < \infty$, then $\{x: |g(x)| > \epsilon \}$ has finite measure for all $\epsilon > 0$ has finite measure for all $\epsilon > 0$ and hence $S_g$ is $\sigma$-finite. As you can tell by the plethora of posts I have on $L^p$ spaces I am really struggling to grasp these concepts and solve these problems. Any advice or suggestions is greatly appreciated.",,"['analysis', 'lp-spaces']"
23,"Showing that $\langle T(u),T(v)\rangle = \langle u, v \rangle$ implies $T$ is a linear isometry",Showing that  implies  is a linear isometry,"\langle T(u),T(v)\rangle = \langle u, v \rangle T","Let $T$ belong to $\mathcal{L}(H)$ (i.e., the set of linear operators from $H \mapsto H$ where $H$ is a Hilbert space). I need to show that $T$ is an isometry iff $\langle T(u),T(v) \rangle = \langle u, v \rangle$ for all $u,v \in H$ . I have been successfully able to show that if $T$ is an isometry, then $\langle T(u), T(v) \rangle=\langle u, v \rangle $ : Suppose that $T$ is an isometry; i.e., that $\forall w \in H$ , $||T(w)||=||w||$ . Then, of course we have that $||T(w)||^{2} = ||w||^{2}$ . Now, consider $u+v \in H$ . Then, since $T$ is an isometry, $||T(u+v)||^{2} = ||u+v||^{2}$ implies that $\langle T(u+v),T(u+v) \rangle = \langle u + v, u+v \rangle \implies \langle T(u), T(u+v)\rangle + \langle T(v), T(u+v)\rangle = \langle u,u+v \rangle + \langle v, u+v \rangle \implies \langle T(u), T(u) \rangle + \langle T(u), T(v) \rangle + \langle T(v), T(u) \rangle + \langle T(v), T(v) \rangle = \langle u, u \rangle +  \langle u, v \rangle + \langle v, u \rangle + \langle v,v \rangle \implies \langle T(u), T(u) \rangle + \langle T(u), T(v) \rangle + \langle T(u), T(v) \rangle + \langle T(v), T(v) \rangle = \langle u, u \rangle +  \langle u, v \rangle + \langle u, v \rangle + \langle v,v \rangle \implies \langle T(u), T(u) \rangle + 2\langle T(u), T(v) \rangle + \langle T(v), T(v) \rangle = \langle u, u \rangle +  2\langle u, v \rangle + \langle v,v \rangle \implies ||T(u)||^{2} + 2\langle T(u),T(v) \rangle + ||T(v)||^{2} = ||u||^{2} + 2\langle u,v \rangle + ||v||^{2}$ . Then, because $T$ is an isometry, $||T(u)||=||u|| \implies ||T(u)||^{2} = ||u||^{2}$ and $||T(v)||=||v|| \implies ||T(v)||^{2} = ||v||^{2}$ , we have that $2\langle T(u),T(v) \rangle = 2\langle u,v \rangle \implies \langle T(u),T(v) \rangle = \langle u,v \rangle$ . I am having difficulty showing the other direction without using the thing I am trying to prove! Everywhere I've looked has said this direction is ""obvious"", which, although it might be true to those more knowledgeable than I, is meaningless to me. Short of telling me this is ""obvious"" or ""directly follows from"" something without explaining in detail why, could someone please help me show the $\langle T(u), T(v) \rangle = \langle u, v \rangle \implies \, T\,\text{is an isometry}$ direction? Thank you. Also, I understand that a lot of books give this as the definition of what it means to be an isometry; however, it is not. It's something that needs to be proven. The definition of isometry I am assuming as a basic principle here is $\forall w \in H$ , $||T(w)||=||w||$ . And please refrain from snarkiness. Sometimes I have trouble with very simple things, even though very complicated things come easy to me. Please try to understand that not everybody's brain works the same way.","Let belong to (i.e., the set of linear operators from where is a Hilbert space). I need to show that is an isometry iff for all . I have been successfully able to show that if is an isometry, then : Suppose that is an isometry; i.e., that , . Then, of course we have that . Now, consider . Then, since is an isometry, implies that . Then, because is an isometry, and , we have that . I am having difficulty showing the other direction without using the thing I am trying to prove! Everywhere I've looked has said this direction is ""obvious"", which, although it might be true to those more knowledgeable than I, is meaningless to me. Short of telling me this is ""obvious"" or ""directly follows from"" something without explaining in detail why, could someone please help me show the direction? Thank you. Also, I understand that a lot of books give this as the definition of what it means to be an isometry; however, it is not. It's something that needs to be proven. The definition of isometry I am assuming as a basic principle here is , . And please refrain from snarkiness. Sometimes I have trouble with very simple things, even though very complicated things come easy to me. Please try to understand that not everybody's brain works the same way.","T \mathcal{L}(H) H \mapsto H H T \langle T(u),T(v) \rangle = \langle u, v \rangle u,v \in H T \langle T(u), T(v) \rangle=\langle u, v \rangle  T \forall w \in H ||T(w)||=||w|| ||T(w)||^{2} = ||w||^{2} u+v \in H T ||T(u+v)||^{2} = ||u+v||^{2} \langle T(u+v),T(u+v) \rangle = \langle u + v, u+v \rangle \implies \langle T(u), T(u+v)\rangle + \langle T(v), T(u+v)\rangle = \langle u,u+v \rangle + \langle v, u+v \rangle \implies \langle T(u), T(u) \rangle + \langle T(u), T(v) \rangle + \langle T(v), T(u) \rangle + \langle T(v), T(v) \rangle = \langle u, u \rangle +  \langle u, v \rangle + \langle v, u \rangle + \langle v,v \rangle \implies \langle T(u), T(u) \rangle + \langle T(u), T(v) \rangle + \langle T(u), T(v) \rangle + \langle T(v), T(v) \rangle = \langle u, u \rangle +  \langle u, v \rangle + \langle u, v \rangle + \langle v,v \rangle \implies \langle T(u), T(u) \rangle + 2\langle T(u), T(v) \rangle + \langle T(v), T(v) \rangle = \langle u, u \rangle +  2\langle u, v \rangle + \langle v,v \rangle \implies ||T(u)||^{2} + 2\langle T(u),T(v) \rangle + ||T(v)||^{2} = ||u||^{2} + 2\langle u,v \rangle + ||v||^{2} T ||T(u)||=||u|| \implies ||T(u)||^{2} = ||u||^{2} ||T(v)||=||v|| \implies ||T(v)||^{2} = ||v||^{2} 2\langle T(u),T(v) \rangle = 2\langle u,v \rangle \implies \langle T(u),T(v) \rangle = \langle u,v \rangle \langle T(u), T(v) \rangle = \langle u, v \rangle \implies \, T\,\text{is an isometry} \forall w \in H ||T(w)||=||w||","['analysis', 'hilbert-spaces']"
24,Divergence of the sequence $n^2-100n$ as $n$ goes to $\infty$.,Divergence of the sequence  as  goes to .,n^2-100n n \infty,"State the definition of divergence to $+\infty$. Argue straight from   this definition that $n^2-100n$ diverges to $+\infty$. The definition is the negation of the definition of convergent sequence which says $\forall \epsilon>0 ~ \exists N \in \mathbb{R}$ s.t. $ |a_n-\ell| < \epsilon$ $\forall n > N$. The negation would then be $\exists \epsilon>0$ s.t.$ ~ \forall N \in \mathbb{R}$, $ |a_n-\ell| \ge \epsilon$ whenever $\exists n > N$. But I don't know how to prove divergence; so far I've only met with far more simple things.","State the definition of divergence to $+\infty$. Argue straight from   this definition that $n^2-100n$ diverges to $+\infty$. The definition is the negation of the definition of convergent sequence which says $\forall \epsilon>0 ~ \exists N \in \mathbb{R}$ s.t. $ |a_n-\ell| < \epsilon$ $\forall n > N$. The negation would then be $\exists \epsilon>0$ s.t.$ ~ \forall N \in \mathbb{R}$, $ |a_n-\ell| \ge \epsilon$ whenever $\exists n > N$. But I don't know how to prove divergence; so far I've only met with far more simple things.",,"['real-analysis', 'analysis', 'epsilon-delta']"
25,"Question on the definition of outward normal vector from Spivak, Calculus on Manifolds","Question on the definition of outward normal vector from Spivak, Calculus on Manifolds",,"The following definition of the outward unit normal at the boundary of a manifold $M \subseteq \mathbb R^n$ is taken from Spivak, Calculus on manifolds (page 119). If $M$ is a $k$-dimensional manifold-with-boundary and $x \in \partial M$, then $(\partial M)_x$ is a $(k-1)$-dimensional subspace of the $k$-dimensional vector space $M_x$. Thus there are exactly two unit vectors in $M_x$ which are perpendicular to $(\partial M)_x$; they can be distinguished as follows: If $f : W \to \mathbb R^n$ is a coordinate system with $W \subseteq H^k$ and $f(0) = x$, then only one of these vectors is $f_{\ast}(v_0)$ for some $v_0$ with $v^k < 0$. This unit vector is called the outward unit vector . What wonders me is how a coordinate system is defined according to the book. If $M \subseteq \mathbb R^n$ is $k$-dimensional manifold, then for each $x \in M$ there exists an open set $U$ containing $x$, an open set $W \subseteq \mathbb R^k$, and a 1-1 differentiable function $f : W \to \mathbb R^n$ such that (1) $f(W) = M \cap U$ (2) $f'(y)$ has rank $k$ for each $y \in W$, (3) $f^{-1} : f(W) \to W$ is continuous. This $f : W \to \mathbb R^n$ is called a coordinate system . As $H^k := \{ x \in \mathbb R^k : x^k \ge 0 \}$ is not open, if $W \subseteq H^k$ and $x \in \partial M$, then as $W$ does not touches the boundardy of $H^k$ (i.e those points $x$ with $x^k = 0$) the condition $f(0) = x$ implies $0 \notin W$ and so $x \notin M \cap U$. But by the requirement $x \in U$. So this gives a contradiction? Have I understood something wrong here? I guess this might be an error in the book, and I think to fix it the definition of a coordinate system $f : W \to \mathbb R^n$ must be altered to account for boundary points, i.e. I guess what would work is if $x$ is not a boundary point, then the conditions are as above, and if $x \in \partial M$ to alter (1) to (1') $f(W \cap H^k) = M \cap U$ (guess the others could also be restricted to $W \cap H^k$, but I guess leaving them would also be fine). But after this alteration the assertion ''$W \subseteq H^k$'' has to be dropped or we would still get the same contradictions for $W$ open in $\mathbb R^k$. So am I right in my objections? And how would you fix it?","The following definition of the outward unit normal at the boundary of a manifold $M \subseteq \mathbb R^n$ is taken from Spivak, Calculus on manifolds (page 119). If $M$ is a $k$-dimensional manifold-with-boundary and $x \in \partial M$, then $(\partial M)_x$ is a $(k-1)$-dimensional subspace of the $k$-dimensional vector space $M_x$. Thus there are exactly two unit vectors in $M_x$ which are perpendicular to $(\partial M)_x$; they can be distinguished as follows: If $f : W \to \mathbb R^n$ is a coordinate system with $W \subseteq H^k$ and $f(0) = x$, then only one of these vectors is $f_{\ast}(v_0)$ for some $v_0$ with $v^k < 0$. This unit vector is called the outward unit vector . What wonders me is how a coordinate system is defined according to the book. If $M \subseteq \mathbb R^n$ is $k$-dimensional manifold, then for each $x \in M$ there exists an open set $U$ containing $x$, an open set $W \subseteq \mathbb R^k$, and a 1-1 differentiable function $f : W \to \mathbb R^n$ such that (1) $f(W) = M \cap U$ (2) $f'(y)$ has rank $k$ for each $y \in W$, (3) $f^{-1} : f(W) \to W$ is continuous. This $f : W \to \mathbb R^n$ is called a coordinate system . As $H^k := \{ x \in \mathbb R^k : x^k \ge 0 \}$ is not open, if $W \subseteq H^k$ and $x \in \partial M$, then as $W$ does not touches the boundardy of $H^k$ (i.e those points $x$ with $x^k = 0$) the condition $f(0) = x$ implies $0 \notin W$ and so $x \notin M \cap U$. But by the requirement $x \in U$. So this gives a contradiction? Have I understood something wrong here? I guess this might be an error in the book, and I think to fix it the definition of a coordinate system $f : W \to \mathbb R^n$ must be altered to account for boundary points, i.e. I guess what would work is if $x$ is not a boundary point, then the conditions are as above, and if $x \in \partial M$ to alter (1) to (1') $f(W \cap H^k) = M \cap U$ (guess the others could also be restricted to $W \cap H^k$, but I guess leaving them would also be fine). But after this alteration the assertion ''$W \subseteq H^k$'' has to be dropped or we would still get the same contradictions for $W$ open in $\mathbb R^k$. So am I right in my objections? And how would you fix it?",,"['general-topology', 'analysis', 'differential-geometry', 'manifolds', 'differential-topology']"
26,$f'$ exists for a function of bounded variation,exists for a function of bounded variation,f',"If $f \in BV[a, b]$, show that $f'$ exists and is integrable. My Attempt : I know that for any $f \in BV[a, b]$, we can write it as difference of two monotonic increasing functions and monotonic increasing functions are differentiable. But not sure if this is rigorous and correct.","If $f \in BV[a, b]$, show that $f'$ exists and is integrable. My Attempt : I know that for any $f \in BV[a, b]$, we can write it as difference of two monotonic increasing functions and monotonic increasing functions are differentiable. But not sure if this is rigorous and correct.",,"['analysis', 'measure-theory', 'bounded-variation']"
27,Proof convergence implies $\liminf = \limsup$.,Proof convergence implies .,\liminf = \limsup,"I have yet to see very straightforward proofs that convergence of a sequence implies equality of $\liminf$ and $\limsup$, so I'd like to attempt to present one here: Statement: If $\{a_k\}$ converges, $$\liminf_{ k \to \infty} \{a_k\} = \limsup_{k \to \infty} \{a_k\}.$$ Proof: Let $\epsilon >0$. Recall that $$\liminf_{ k \to \infty} \{a_k\} = \lim_{n \to \infty} \inf_{k \geq n} \{a_k\},$$ since $\inf_{k \geq n} \{a_k\}$ is monotonically decreasing in $n$. Similarly for $\limsup$. Thus, there is $N_1$ so that if $n \geq N_1$, $$\left|\liminf_{ k \to \infty} \{a_k\} - \inf_{k \geq n} \{a_k\}\right| < \frac{\epsilon}{4} \qquad(1)$$ and  $$\left|\limsup_{ k \to \infty} \{a_k\} - \sup_{k \geq n} \{a_k\}\right| < \frac{\epsilon}{4} \qquad (2).$$ Now, since the sequence converges, it is cauchy. Let $N_2$ be such that for $n,m \geq N_2$, $$\left|a_n-a_m\right| < \frac{\epsilon}{8}.$$ Let $N = \max\{N_1,N_2\}$. By the definition of inf, there exists $a_j$ with $j \geq N$ so that $$\left|a_j - \inf_{k \geq N} \{a_k\}\right| < \frac{\epsilon}{8}$$ and so, for $n \geq N$, $$\left|a_n - \inf_{k \geq N} \{a_k\}\right| < \frac{\epsilon}{4} \qquad (3)$$ by the triangle inequality, and the fact that the sequence is cauchy. Similarly, for $n \geq N$, $$\left|a_n - \sup_{k \geq N} \{a_k\}\right| < \frac{\epsilon}{4} \qquad (4).$$ Combining (1)-(4), we obtain $$\left|\liminf_{ k \to \infty} \{a_k\} - \limsup_{ k \to \infty} \{a_k\} \right| < \epsilon$$ and since $\epsilon$ was arbitrary, we have our desired equality. Indeed, although this proof may be longer than standard proofs, I believe it more accurately portrays to the students that we are trying to control oscillation after some index, similar to the oscillation of a continuous function i.e. show that this proof is essentially the discrete analogue of the proof that a function is continuous iff its oscillation tends to 0.","I have yet to see very straightforward proofs that convergence of a sequence implies equality of $\liminf$ and $\limsup$, so I'd like to attempt to present one here: Statement: If $\{a_k\}$ converges, $$\liminf_{ k \to \infty} \{a_k\} = \limsup_{k \to \infty} \{a_k\}.$$ Proof: Let $\epsilon >0$. Recall that $$\liminf_{ k \to \infty} \{a_k\} = \lim_{n \to \infty} \inf_{k \geq n} \{a_k\},$$ since $\inf_{k \geq n} \{a_k\}$ is monotonically decreasing in $n$. Similarly for $\limsup$. Thus, there is $N_1$ so that if $n \geq N_1$, $$\left|\liminf_{ k \to \infty} \{a_k\} - \inf_{k \geq n} \{a_k\}\right| < \frac{\epsilon}{4} \qquad(1)$$ and  $$\left|\limsup_{ k \to \infty} \{a_k\} - \sup_{k \geq n} \{a_k\}\right| < \frac{\epsilon}{4} \qquad (2).$$ Now, since the sequence converges, it is cauchy. Let $N_2$ be such that for $n,m \geq N_2$, $$\left|a_n-a_m\right| < \frac{\epsilon}{8}.$$ Let $N = \max\{N_1,N_2\}$. By the definition of inf, there exists $a_j$ with $j \geq N$ so that $$\left|a_j - \inf_{k \geq N} \{a_k\}\right| < \frac{\epsilon}{8}$$ and so, for $n \geq N$, $$\left|a_n - \inf_{k \geq N} \{a_k\}\right| < \frac{\epsilon}{4} \qquad (3)$$ by the triangle inequality, and the fact that the sequence is cauchy. Similarly, for $n \geq N$, $$\left|a_n - \sup_{k \geq N} \{a_k\}\right| < \frac{\epsilon}{4} \qquad (4).$$ Combining (1)-(4), we obtain $$\left|\liminf_{ k \to \infty} \{a_k\} - \limsup_{ k \to \infty} \{a_k\} \right| < \epsilon$$ and since $\epsilon$ was arbitrary, we have our desired equality. Indeed, although this proof may be longer than standard proofs, I believe it more accurately portrays to the students that we are trying to control oscillation after some index, similar to the oscillation of a continuous function i.e. show that this proof is essentially the discrete analogue of the proof that a function is continuous iff its oscillation tends to 0.",,"['real-analysis', 'sequences-and-series', 'analysis', 'proof-verification', 'proof-writing']"
28,Continuity in Slices + Mapping Compact Sets to Compact Sets = Continuous,Continuity in Slices + Mapping Compact Sets to Compact Sets = Continuous,,"The following is a problem from Berkeley's Grad Prelims: Let  $f:\mathbb R^2 \to \mathbb R$ satisfy (i) Given any $x_0,y_0 \in \mathbb R$, $y \mapsto f(x_0,y)$ and $x \mapsto f(x,y_0)$ are continuous. (ii) For each compact $K$, $f(K)$ is compact. Prove $f$ is continuous. Attempt: Let $(x_n,y_n) \to (x,y)$. We wish to show $f((x_n,y_n)) \to f((x,y))$. Sequential compactness gives us a subsequence $$f((x_{n_k},y_{n_k})) \to f((\tilde x, \tilde y)) \in f(K).$$ I'm un sure how to proceed to show (i) $f((\tilde x, \tilde y)) = f((x,y))$, and (ii) how the existence of such a subsequence and the slice continuity give convergence of the entire sequence. Originally I had hoped that the slice continuity would make $\{f((x_n,y_n))\}$ cauchy, but that doesn't seem to be the case. I'm looking for a small hint as to how to proceed. Please, no answers.","The following is a problem from Berkeley's Grad Prelims: Let  $f:\mathbb R^2 \to \mathbb R$ satisfy (i) Given any $x_0,y_0 \in \mathbb R$, $y \mapsto f(x_0,y)$ and $x \mapsto f(x,y_0)$ are continuous. (ii) For each compact $K$, $f(K)$ is compact. Prove $f$ is continuous. Attempt: Let $(x_n,y_n) \to (x,y)$. We wish to show $f((x_n,y_n)) \to f((x,y))$. Sequential compactness gives us a subsequence $$f((x_{n_k},y_{n_k})) \to f((\tilde x, \tilde y)) \in f(K).$$ I'm un sure how to proceed to show (i) $f((\tilde x, \tilde y)) = f((x,y))$, and (ii) how the existence of such a subsequence and the slice continuity give convergence of the entire sequence. Originally I had hoped that the slice continuity would make $\{f((x_n,y_n))\}$ cauchy, but that doesn't seem to be the case. I'm looking for a small hint as to how to proceed. Please, no answers.",,"['real-analysis', 'sequences-and-series', 'analysis', 'proof-verification', 'continuity']"
29,How to prove that doesnt exist a natural number such that is equal to it successor from Peano axioms?,How to prove that doesnt exist a natural number such that is equal to it successor from Peano axioms?,,"Im getting a hard time trying to prove the general for any natural number $n$ such that $$\nexists n\in\Bbb N: S(n)= n$$ From the second Peano axiom we know that $$\nexists n\in\Bbb N: S(1)= n$$ and from the third axiom that $$S(a)=S(b)\to a=b$$ I really dont have a clear clue about how to prove this. Someone tell me that I must use the ""axiom"" number 5, the induction. But the induction need the existence of a second natural number more than $1$ (or more than $0$ using the axioms that use it) that hold the implicit difference on the axiom 2. Example: $$S(1)\ne 1 \to S(1)=2$$ But now, how to prove that $S(2)\ne 2$? Thank you in advance.","Im getting a hard time trying to prove the general for any natural number $n$ such that $$\nexists n\in\Bbb N: S(n)= n$$ From the second Peano axiom we know that $$\nexists n\in\Bbb N: S(1)= n$$ and from the third axiom that $$S(a)=S(b)\to a=b$$ I really dont have a clear clue about how to prove this. Someone tell me that I must use the ""axiom"" number 5, the induction. But the induction need the existence of a second natural number more than $1$ (or more than $0$ using the axioms that use it) that hold the implicit difference on the axiom 2. Example: $$S(1)\ne 1 \to S(1)=2$$ But now, how to prove that $S(2)\ne 2$? Thank you in advance.",,"['analysis', 'proof-explanation', 'peano-axioms']"
30,Definite Integral of Polynomial of Sine and Cosines,Definite Integral of Polynomial of Sine and Cosines,,"I was wondering if there is a way to compute definite integral  \begin{align} I(m,n) :=\int_{0}^{\pi} \sin^m (\theta) \cdot \cos^{n}(\theta) \ \mathrm{d} \theta \end{align} in general for integer-valued $m$ and $n$. This problem arises when I try to compute integrals of trigonometric functions over a high dimensional sphere. In fact I am more interested in the asymptotic order of this integral. I was wondering how $I(m,n)$ behaves when $n$ goes to infinity. For example, what is the limit of $\lim _{n\rightarrow \infty} I(2, n)$?","I was wondering if there is a way to compute definite integral  \begin{align} I(m,n) :=\int_{0}^{\pi} \sin^m (\theta) \cdot \cos^{n}(\theta) \ \mathrm{d} \theta \end{align} in general for integer-valued $m$ and $n$. This problem arises when I try to compute integrals of trigonometric functions over a high dimensional sphere. In fact I am more interested in the asymptotic order of this integral. I was wondering how $I(m,n)$ behaves when $n$ goes to infinity. For example, what is the limit of $\lim _{n\rightarrow \infty} I(2, n)$?",,"['real-analysis', 'integration', 'analysis', 'trigonometry', 'definite-integrals']"
31,Theorem 1.4. of Serge Lang´s Undergraduate Analysis,Theorem 1.4. of Serge Lang´s Undergraduate Analysis,,Could somebody explain to me the boxed inequality. I have the feeling that the second member should be $L_1/2^{K-1}$. Thanks.,Could somebody explain to me the boxed inequality. I have the feeling that the second member should be $L_1/2^{K-1}$. Thanks.,,['analysis']
32,How to approximate a non-increasing function from above/below with decreasing functions,How to approximate a non-increasing function from above/below with decreasing functions,,"Say I have a non-increasing (not necessarily continuous) function $$\psi: [1,\infty)\rightarrow (0,\infty)$$ which approaches $0$. Let $0<c<1<C$.  It's my feeling that there should be $C^1$, strictly decreasing functions $\overline\psi, \underline\psi$ with $$c\psi\leq \underline\psi \leq \psi \leq \overline\psi \leq C\psi$$ It seems like you could maybe prove this by piecing together some $C^1$ functions defined on closed intervals. But I wonder if there's a cleaner, possibly fancier way to do it.","Say I have a non-increasing (not necessarily continuous) function $$\psi: [1,\infty)\rightarrow (0,\infty)$$ which approaches $0$. Let $0<c<1<C$.  It's my feeling that there should be $C^1$, strictly decreasing functions $\overline\psi, \underline\psi$ with $$c\psi\leq \underline\psi \leq \psi \leq \overline\psi \leq C\psi$$ It seems like you could maybe prove this by piecing together some $C^1$ functions defined on closed intervals. But I wonder if there's a cleaner, possibly fancier way to do it.",,"['real-analysis', 'analysis']"
33,Lebesgue measure on a continuous function to prove equality,Lebesgue measure on a continuous function to prove equality,,"Let $f(x)$ be a continuous function on [−1, 1]. Show that there exists a constant $c$ such that the Lebesgue measures $\mu (\{x ∈ [−1, 1] : f(x) ≥ c\}) ≥ 1$,$\quad$ $\mu (\{x ∈ [−1, 1] : f(x) ≤ c\}) ≥ 1$. This part has been done here. Lebesgue measure on a continuous function Now I want to extend the result to prove the following. For this constant $c$, prove the equality $\int_{-1}^{1} |f(x) − c| dx$ = $min_{k\in R}$ $\int_{-1}^{1} |f(x) − k| dx$","Let $f(x)$ be a continuous function on [−1, 1]. Show that there exists a constant $c$ such that the Lebesgue measures $\mu (\{x ∈ [−1, 1] : f(x) ≥ c\}) ≥ 1$,$\quad$ $\mu (\{x ∈ [−1, 1] : f(x) ≤ c\}) ≥ 1$. This part has been done here. Lebesgue measure on a continuous function Now I want to extend the result to prove the following. For this constant $c$, prove the equality $\int_{-1}^{1} |f(x) − c| dx$ = $min_{k\in R}$ $\int_{-1}^{1} |f(x) − k| dx$",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
34,Why is the intersection of an open set and a single point open?,Why is the intersection of an open set and a single point open?,,"I'm reading baby Rudin as independent study and he has the following theorem on page 36 (second edition): Let $X$ be a metric space. Suppose $Y\subset X$. A subset $E$ of $Y$ is open relative to $Y$ if and only if $E = Y \cap G$ for some open subset $G$ of $X$. My question is the following. Let's say I define $Y = \{p\} \cup S$ where $S \subset X$, $p \in X$, $p \notin S$ and define $G$ such that $G \cap S = \emptyset$ and $p \in G$ and $G$ is open. So $Y \cap G = E = \{p\}$ In what sense is $\{p\}$ open relative to $Y$? There is certainly no positive radius neighborhood of $p$ that is contained in $E$.","I'm reading baby Rudin as independent study and he has the following theorem on page 36 (second edition): Let $X$ be a metric space. Suppose $Y\subset X$. A subset $E$ of $Y$ is open relative to $Y$ if and only if $E = Y \cap G$ for some open subset $G$ of $X$. My question is the following. Let's say I define $Y = \{p\} \cup S$ where $S \subset X$, $p \in X$, $p \notin S$ and define $G$ such that $G \cap S = \emptyset$ and $p \in G$ and $G$ is open. So $Y \cap G = E = \{p\}$ In what sense is $\{p\}$ open relative to $Y$? There is certainly no positive radius neighborhood of $p$ that is contained in $E$.",,"['general-topology', 'analysis', 'metric-spaces']"
35,$\lg_{2} \left( \prod\limits_{a=1}^{2015} \prod\limits_{b=1}^{2015} (1 + e^{\frac{2\pi iab}{2015}}) \right)$? [closed],? [closed],\lg_{2} \left( \prod\limits_{a=1}^{2015} \prod\limits_{b=1}^{2015} (1 + e^{\frac{2\pi iab}{2015}}) \right),"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How can this problem be solved? $$ \lg_{2} \left( \prod\limits_{a=1}^{2015} \prod\limits_{b=1}^{2015}  (1 + e^{\frac{2\pi iab}{2015}})  \right) $$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question How can this problem be solved? $$ \lg_{2} \left( \prod\limits_{a=1}^{2015} \prod\limits_{b=1}^{2015}  (1 + e^{\frac{2\pi iab}{2015}})  \right) $$",,['calculus']
36,What does the symbol $\Subset$ mean? [duplicate],What does the symbol  mean? [duplicate],\Subset,"This question already has an answer here : What does the symbol $\subset\subset$ mean? [duplicate] (1 answer) Closed 8 years ago . It always possible to shrink a domain $U_\alpha$ to $V_\alpha$ in the sense that $$V_\alpha\Subset U_\alpha\quad\text{and} \quad\bigcup_{\alpha}U_\alpha=\bigcup_{\alpha}V_\alpha\,?$$ What does $\Subset$ mean ? Is it an error ?","This question already has an answer here : What does the symbol $\subset\subset$ mean? [duplicate] (1 answer) Closed 8 years ago . It always possible to shrink a domain $U_\alpha$ to $V_\alpha$ in the sense that $$V_\alpha\Subset U_\alpha\quad\text{and} \quad\bigcup_{\alpha}U_\alpha=\bigcup_{\alpha}V_\alpha\,?$$ What does $\Subset$ mean ? Is it an error ?",,"['general-topology', 'analysis', 'notation']"
37,Proof by Induction with Two Basis,Proof by Induction with Two Basis,,"I did an assignment where I used the proof by induction three times in the same manner, and now I have doubts creeping up whether this method was valid or not. I would be quite relieved if you could tell me if that's correct or not: The sequence $(a_n)_{n\in\mathbb{N}}$ is given through $$a_1 = 1,\quad a_2 = \frac{1}{2},\quad a_{n+2}=a_na_{n+1} \text{ for } n\geq 1.$$ I want to show $A(n): a_n \geq a_{n+1} \forall\, n\geq 1$. So I did the base case as follows $$n=1:\quad a_1 = 1,\quad a_2=\frac{1}{2},\quad \therefore a_1=1 \geq \frac{1}{2} = a_2 \quad\checkmark$$ $$n=2:\quad a_2 = \frac{1}{2},\quad a_3=\frac{1}{2},\quad \therefore a_2=\frac{1}{2} \geq \frac{1}{2} = a_3 \quad\checkmark$$ Now comes the part where I'm insecure. For the induction step I assume that $A(n): a_{n} \geq a_{n+1}$ and $A(n+1): a_{n+1} \geq a_{n+2}$ are true and I want to deduce $A(n+2): a_{n+2} \geq a_{n+3}$. Because of the ordering axiom of $(\mathbb{R},+,\cdot\,)$ I think I can multiply $A(n)$ and $A(n+1)$ to get $$a_n \cdot a_{n+1} \geq a_{n+1} \cdot a_{n+2}$$ and with the definition of the recursive defined sequence $a_n$ I would get $$a_{n+2} = a_n \cdot a_{n+1} \quad\geq\quad a_{n+1} \cdot a_{n+2} = a_{(n+1)+2} =  a_{n+3} \quad\checkmark$$ the induction step. Do I get away with this?","I did an assignment where I used the proof by induction three times in the same manner, and now I have doubts creeping up whether this method was valid or not. I would be quite relieved if you could tell me if that's correct or not: The sequence $(a_n)_{n\in\mathbb{N}}$ is given through $$a_1 = 1,\quad a_2 = \frac{1}{2},\quad a_{n+2}=a_na_{n+1} \text{ for } n\geq 1.$$ I want to show $A(n): a_n \geq a_{n+1} \forall\, n\geq 1$. So I did the base case as follows $$n=1:\quad a_1 = 1,\quad a_2=\frac{1}{2},\quad \therefore a_1=1 \geq \frac{1}{2} = a_2 \quad\checkmark$$ $$n=2:\quad a_2 = \frac{1}{2},\quad a_3=\frac{1}{2},\quad \therefore a_2=\frac{1}{2} \geq \frac{1}{2} = a_3 \quad\checkmark$$ Now comes the part where I'm insecure. For the induction step I assume that $A(n): a_{n} \geq a_{n+1}$ and $A(n+1): a_{n+1} \geq a_{n+2}$ are true and I want to deduce $A(n+2): a_{n+2} \geq a_{n+3}$. Because of the ordering axiom of $(\mathbb{R},+,\cdot\,)$ I think I can multiply $A(n)$ and $A(n+1)$ to get $$a_n \cdot a_{n+1} \geq a_{n+1} \cdot a_{n+2}$$ and with the definition of the recursive defined sequence $a_n$ I would get $$a_{n+2} = a_n \cdot a_{n+1} \quad\geq\quad a_{n+1} \cdot a_{n+2} = a_{(n+1)+2} =  a_{n+3} \quad\checkmark$$ the induction step. Do I get away with this?",,"['sequences-and-series', 'analysis', 'induction']"
38,Prove that the product of $n$ bounded and uniformly continuous functions are also uniformly continous [duplicate],Prove that the product of  bounded and uniformly continuous functions are also uniformly continous [duplicate],n,"This question already has answers here : two functions are uniformly continuous on some interval I and each is bounded on I then their product is also uniformly continuous on I . [closed] (3 answers) If $f,g$ are uniformly continuous prove $f+g$ is uniformly continuous but $fg$ and $\dfrac{f}{g}$ are not (4 answers) Closed 6 years ago . Let $f_{1}(x),f_{2}(x),\dots,f_{n}(x)$ be $n$ bounded and uniformly continuous functions on $\mathbb{R}$. Prove that their product $f_1(x)f_2(x)\cdots f_n(x)$ is also a uniformly continuous function on $\mathbb{R}$ My approach is to use mathematical induction. At the last step, I need to prove that the product of two uniformly continuos functions (which are the product of the first $n-1$ uniformly continuous functions by inductive hypothesis and the $n^{\text{th}}$ function), which doesn't require me to use the fact the functions are bounded. So, I sense that this approach is not correct (but I'm not sure why) as I didn't use the fact that the functions are bounded. If it's really wrong, how do I make use of the fact that the functions are bounded? Thank you :)","This question already has answers here : two functions are uniformly continuous on some interval I and each is bounded on I then their product is also uniformly continuous on I . [closed] (3 answers) If $f,g$ are uniformly continuous prove $f+g$ is uniformly continuous but $fg$ and $\dfrac{f}{g}$ are not (4 answers) Closed 6 years ago . Let $f_{1}(x),f_{2}(x),\dots,f_{n}(x)$ be $n$ bounded and uniformly continuous functions on $\mathbb{R}$. Prove that their product $f_1(x)f_2(x)\cdots f_n(x)$ is also a uniformly continuous function on $\mathbb{R}$ My approach is to use mathematical induction. At the last step, I need to prove that the product of two uniformly continuos functions (which are the product of the first $n-1$ uniformly continuous functions by inductive hypothesis and the $n^{\text{th}}$ function), which doesn't require me to use the fact the functions are bounded. So, I sense that this approach is not correct (but I'm not sure why) as I didn't use the fact that the functions are bounded. If it's really wrong, how do I make use of the fact that the functions are bounded? Thank you :)",,"['analysis', 'uniform-continuity']"
39,prove that $ \int_E f(x) dm \ge \delta$ whenever $m(E) \ge \epsilon$,prove that  whenever, \int_E f(x) dm \ge \delta m(E) \ge \epsilon,"Assume that $f: [0,1] \to [0, \infty) $ is a Lebesgue measurable function such that $ f(x)\gt 0  $ a.e x. Show that for every $ \epsilon \gt 0 $ there is $\delta \gt 0$ such that for every lebesgue measurable E  with lebesgue measure $m(E) \ge \epsilon $ we have $ \int_E f(x) dm \ge \delta$. I have seen similar problem related to $ \epsilon $ and $\delta $ when f is lebesgue integrable. but the inclusion was opposite. I think this should not be too tough but I could not get after spending long time.","Assume that $f: [0,1] \to [0, \infty) $ is a Lebesgue measurable function such that $ f(x)\gt 0  $ a.e x. Show that for every $ \epsilon \gt 0 $ there is $\delta \gt 0$ such that for every lebesgue measurable E  with lebesgue measure $m(E) \ge \epsilon $ we have $ \int_E f(x) dm \ge \delta$. I have seen similar problem related to $ \epsilon $ and $\delta $ when f is lebesgue integrable. but the inclusion was opposite. I think this should not be too tough but I could not get after spending long time.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
40,Summation by parts interpretation,Summation by parts interpretation,,"So in Rudin, I'm given the theorem for summation by parts: $$\text{Given two sequences} \{a_n\}, \{b_n\}, \text{put}\\ A_n = \sum_{k=0}^n a_k\\$$ $\text{if}$ $$n \ge 0; \text{put}\ A_{-1} = 0$$  Then, if $$0 \le p \le q$$ we have $$\sum_{n=p}^q a_n b_n = \sum_{n=p}^{q-1}A_n(b_n - b_{n+1}) + A_qb_q - A_{p-1}b_p$$ I'm having trouble understanding when/how I would use this.  $\\$All it says in the book is ""useful in the investigation of series of the form $\sum a_nb_n$ when $b_n$ is monotone"".  I understand that it is meant to be used to see if $\sum a_nb_n$ converges or not, but I don't know how to use it. $\\$Also, I'm confused by the subscripts; what is the $A_{-1}$ and where does this come into play?","So in Rudin, I'm given the theorem for summation by parts: $$\text{Given two sequences} \{a_n\}, \{b_n\}, \text{put}\\ A_n = \sum_{k=0}^n a_k\\$$ $\text{if}$ $$n \ge 0; \text{put}\ A_{-1} = 0$$  Then, if $$0 \le p \le q$$ we have $$\sum_{n=p}^q a_n b_n = \sum_{n=p}^{q-1}A_n(b_n - b_{n+1}) + A_qb_q - A_{p-1}b_p$$ I'm having trouble understanding when/how I would use this.  $\\$All it says in the book is ""useful in the investigation of series of the form $\sum a_nb_n$ when $b_n$ is monotone"".  I understand that it is meant to be used to see if $\sum a_nb_n$ converges or not, but I don't know how to use it. $\\$Also, I'm confused by the subscripts; what is the $A_{-1}$ and where does this come into play?",,"['sequences-and-series', 'analysis', 'summation-by-parts']"
41,Characterization Theorem of the Lebesgue Stieltjes Measure on the Real Line (Folland Theorem 1.18),Characterization Theorem of the Lebesgue Stieltjes Measure on the Real Line (Folland Theorem 1.18),,"This is a theorem from Folland's Real Analysis, and I have difficulty understanding the proof. My problem is the last two sentences. $H_n$ as defined is clearly compact, and I follow the inequality obtained, which comes from additivity of the measure. Also, taking the limit as $n\to \infty$, we clearly get the last inequality too, but the limit should also affect $H_n$ in the above inequality. However,$\lim_{n\to \infty} H_n=\bigcup_{-\infty}^{\infty}H_n$ need not be a compact set. So how is the result proven here? I would greatly appreciate some help to complete my understanding of this proof.","This is a theorem from Folland's Real Analysis, and I have difficulty understanding the proof. My problem is the last two sentences. $H_n$ as defined is clearly compact, and I follow the inequality obtained, which comes from additivity of the measure. Also, taking the limit as $n\to \infty$, we clearly get the last inequality too, but the limit should also affect $H_n$ in the above inequality. However,$\lim_{n\to \infty} H_n=\bigcup_{-\infty}^{\infty}H_n$ need not be a compact set. So how is the result proven here? I would greatly appreciate some help to complete my understanding of this proof.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
42,"Given two metrics $\rho,\sigma$ on $\chi$ show that the following is a metric on $\chi$",Given two metrics  on  show that the following is a metric on,"\rho,\sigma \chi \chi",The problem asks us to show that $$ \rho_{2}=(\rho^2+\sigma^2)^{1/2} $$ is also a metric on $\chi$. I'm having trouble showing the triangle inequality; I've tried numerous algebraic manipulations (filling white boards) but haven't gotten it to work. I thought I could apply the Cauchy-Schwarz inequality but these metrics aren't necessarily on $\mathbb{R}^{n}$. I feel I am missing something obvious.  Any hints or proofs would be appreciated. Thank you.,The problem asks us to show that $$ \rho_{2}=(\rho^2+\sigma^2)^{1/2} $$ is also a metric on $\chi$. I'm having trouble showing the triangle inequality; I've tried numerous algebraic manipulations (filling white boards) but haven't gotten it to work. I thought I could apply the Cauchy-Schwarz inequality but these metrics aren't necessarily on $\mathbb{R}^{n}$. I feel I am missing something obvious.  Any hints or proofs would be appreciated. Thank you.,,"['real-analysis', 'analysis', 'metric-spaces']"
43,"How to prove function $f(x,y)=\frac{1}{xy}$ is not uniformly continuous?",How to prove function  is not uniformly continuous?,"f(x,y)=\frac{1}{xy}","Here I consider uniform continuity of functions in $\mathbb{R}^n$. Take a function of two variables for example. We said that $f(x,y)$ is uniformly continuous if for any $\epsilon>0$, we can find a $\delta>0$ [depends on $\epsilon$ only] such that $|f(x_1,y_1)-f(x_2,y_2)|<\epsilon$ whenever $|x_1-x_2|+|y_1-y_2|<\delta$.  Hence to find a counter example. We need is to show \begin{align*} 	\exists\epsilon_0>0\,\,\text{such that}\,\,\forall\delta>0,\,\,\exists(x_1,x_2)\in \mathbb{R}^2\,\,\text{with}\,\,|x_1-x_2|<\delta\,\,\text{and}\,\,|f(x_1)-f(x_2)|\ge\epsilon_0 \end{align*} Now consider $f(x,y)=\frac{1}{xy}$, I was told that it is not uniformly continuous and I have trouble show this. Could anyone help for a solution using the definition I wrote?","Here I consider uniform continuity of functions in $\mathbb{R}^n$. Take a function of two variables for example. We said that $f(x,y)$ is uniformly continuous if for any $\epsilon>0$, we can find a $\delta>0$ [depends on $\epsilon$ only] such that $|f(x_1,y_1)-f(x_2,y_2)|<\epsilon$ whenever $|x_1-x_2|+|y_1-y_2|<\delta$.  Hence to find a counter example. We need is to show \begin{align*} 	\exists\epsilon_0>0\,\,\text{such that}\,\,\forall\delta>0,\,\,\exists(x_1,x_2)\in \mathbb{R}^2\,\,\text{with}\,\,|x_1-x_2|<\delta\,\,\text{and}\,\,|f(x_1)-f(x_2)|\ge\epsilon_0 \end{align*} Now consider $f(x,y)=\frac{1}{xy}$, I was told that it is not uniformly continuous and I have trouble show this. Could anyone help for a solution using the definition I wrote?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
44,"$S\equiv\{x\mid x\in\mathbb{R},x\geq 0,x^2<c\}$ , show that $\sup S = \sqrt{c}$",", show that","S\equiv\{x\mid x\in\mathbb{R},x\geq 0,x^2<c\} \sup S = \sqrt{c}","Here is my attempt: Since $x\geq 0$ and $x\in\mathbb{R}$, so there are two cases for $x$: $x > 1 $ or $ 0 \leq x \leq 1$ if $x > 1 $, then $x^2 < c < x\cdot c + x=x(c+1)\rightarrow x < c + 1$; if $ 0 \leq x \leq 1$, then $x < c+1$. Thus set $S$ has a least upper bound, $\sup S$. I am stuck at this step, can anyone give me a hit or suggestion to keep going. (I am still learning the completeness axiom, haven't start Archimedian Property yet) Thanks.","Here is my attempt: Since $x\geq 0$ and $x\in\mathbb{R}$, so there are two cases for $x$: $x > 1 $ or $ 0 \leq x \leq 1$ if $x > 1 $, then $x^2 < c < x\cdot c + x=x(c+1)\rightarrow x < c + 1$; if $ 0 \leq x \leq 1$, then $x < c+1$. Thus set $S$ has a least upper bound, $\sup S$. I am stuck at this step, can anyone give me a hit or suggestion to keep going. (I am still learning the completeness axiom, haven't start Archimedian Property yet) Thanks.",,['analysis']
45,"Show that if $|g'(x)|\le M|x-a|^n$ for $|x-a|\lt \delta$, then $|g(x)-g(a)|\le M|x-a|^{n+1}/(n+1)$ for $|x-a|\lt \delta$.","Show that if  for , then  for .",|g'(x)|\le M|x-a|^n |x-a|\lt \delta |g(x)-g(a)|\le M|x-a|^{n+1}/(n+1) |x-a|\lt \delta,"Problem: Show that if $|g'(x)|\le M|x-a|^n$ for $|x-a|\lt \delta$, then $|g(x)-g(a)|\le M|x-a|^{n+1}/(n+1)$ for $|x-a|\lt \delta$. Solution: I have difficulty understanding this solution. Applying the Mean Value Theorem, I get, $|g(x)-g(a)|=|(x-a)||g'(t)|\le M|x-a|^{n+1}$. I don't see where the $n+1$ in the denominator comes from. I tried solving this by integrating both sides $\int_a^x$, but then this only works if $g'$ is integrable on $[a,x]$, which is not a given condition on this problem. What am I getting wrong here? I would greatly appreciate some help.","Problem: Show that if $|g'(x)|\le M|x-a|^n$ for $|x-a|\lt \delta$, then $|g(x)-g(a)|\le M|x-a|^{n+1}/(n+1)$ for $|x-a|\lt \delta$. Solution: I have difficulty understanding this solution. Applying the Mean Value Theorem, I get, $|g(x)-g(a)|=|(x-a)||g'(t)|\le M|x-a|^{n+1}$. I don't see where the $n+1$ in the denominator comes from. I tried solving this by integrating both sides $\int_a^x$, but then this only works if $g'$ is integrable on $[a,x]$, which is not a given condition on this problem. What am I getting wrong here? I would greatly appreciate some help.",,"['calculus', 'real-analysis', 'analysis']"
46,Prove $\|x\|\le\sum_{i=1}^n|x_i|\le\sqrt{n}\|x\|$ for all $x\in\mathbb{R}^n$. [duplicate],Prove  for all . [duplicate],\|x\|\le\sum_{i=1}^n|x_i|\le\sqrt{n}\|x\| x\in\mathbb{R}^n,"This question already has an answer here : Prove $\|x\|_1\le \sqrt n \|x\|_2$ [closed] (1 answer) Closed 8 years ago . Prove $\|x\|\le\sum_{i=1}^n|x_i|\le\sqrt{n}\|x\|$ for all $x\in\mathbb{R}^n$. I'm stumped, can't find any intuition here or where to begin. Any suggestions?","This question already has an answer here : Prove $\|x\|_1\le \sqrt n \|x\|_2$ [closed] (1 answer) Closed 8 years ago . Prove $\|x\|\le\sum_{i=1}^n|x_i|\le\sqrt{n}\|x\|$ for all $x\in\mathbb{R}^n$. I'm stumped, can't find any intuition here or where to begin. Any suggestions?",,"['linear-algebra', 'general-topology', 'analysis']"
47,All closed rational rays measurable implies $f$ measurable,All closed rational rays measurable implies  measurable,f,"Is the following proof correct? Let $f: X \to \mathbb{R}$ where $X$ is a measurable space. Suppose $\{x: f(x) \geq r\}$ is measurable for each $r \in \mathbb{Q}$. Then, $f$ is measurable. Proof: Let $U \subset \mathbb{R}$ so that $U = (a,b)$. Then, there exists sequences $\{r_n(a) : r_n \in \mathbb{Q}\}_{n \in \mathbb{N}} \to a$ and  $\{r_n(b) : r_n \in \mathbb{Q}\}_{n \in \mathbb{N}} \to b$. Thus, we may write $$(a,b) = \bigcup_{n \in \mathbb{N}} [r_n(a), \infty) \cap \bigcup_{n \in \mathbb{N}} [r_n(b), \infty).$$ Therefore, $$f^{-1}(U) = \bigcup_{n \in \mathbb{N}} f^{-1}([r_n(a), \infty)) \cap \bigcup_{n \in \mathbb{N}} f^{-1}([r_n(b), \infty))$$ with the lhs being measurable by the properties of a measurable space. Any issues with this proof?","Is the following proof correct? Let $f: X \to \mathbb{R}$ where $X$ is a measurable space. Suppose $\{x: f(x) \geq r\}$ is measurable for each $r \in \mathbb{Q}$. Then, $f$ is measurable. Proof: Let $U \subset \mathbb{R}$ so that $U = (a,b)$. Then, there exists sequences $\{r_n(a) : r_n \in \mathbb{Q}\}_{n \in \mathbb{N}} \to a$ and  $\{r_n(b) : r_n \in \mathbb{Q}\}_{n \in \mathbb{N}} \to b$. Thus, we may write $$(a,b) = \bigcup_{n \in \mathbb{N}} [r_n(a), \infty) \cap \bigcup_{n \in \mathbb{N}} [r_n(b), \infty).$$ Therefore, $$f^{-1}(U) = \bigcup_{n \in \mathbb{N}} f^{-1}([r_n(a), \infty)) \cap \bigcup_{n \in \mathbb{N}} f^{-1}([r_n(b), \infty))$$ with the lhs being measurable by the properties of a measurable space. Any issues with this proof?",,"['real-analysis', 'analysis', 'measure-theory', 'proof-verification']"
48,Definition of differentiability at the point in multivariable calculus.,Definition of differentiability at the point in multivariable calculus.,,"I'm self-studying the analysis from Zorich and the next definition of differentiability is given: $f:E\to \mathbb{R}^n$ is differentiable at the point $x$, which is a limit point of $E\subset \mathbb{R}^m$, if  $f(x+h)-f(x)=L(x)\cdot h+\alpha(x;h)$, so that  $L(x):\mathbb{R}^m\to \mathbb{R}^n$ is linear map wrt $h$ and $\dfrac{\alpha(x;h)}h\to 0$ while $h\to 0$ for $x+h \in E$. Why do we need the linearity of $L$ wrt $h$? What do we lose if $L$ is not linear? Why $x$ is the argument of $L$, isn't it fixed? Shouldn't it be $L(h):\mathbb{R}^m\to \mathbb{R}^n$?","I'm self-studying the analysis from Zorich and the next definition of differentiability is given: $f:E\to \mathbb{R}^n$ is differentiable at the point $x$, which is a limit point of $E\subset \mathbb{R}^m$, if  $f(x+h)-f(x)=L(x)\cdot h+\alpha(x;h)$, so that  $L(x):\mathbb{R}^m\to \mathbb{R}^n$ is linear map wrt $h$ and $\dfrac{\alpha(x;h)}h\to 0$ while $h\to 0$ for $x+h \in E$. Why do we need the linearity of $L$ wrt $h$? What do we lose if $L$ is not linear? Why $x$ is the argument of $L$, isn't it fixed? Shouldn't it be $L(h):\mathbb{R}^m\to \mathbb{R}^n$?",,"['analysis', 'multivariable-calculus', 'derivatives', 'definition']"
49,Intuition for visualising dense monotonic discontinuous function,Intuition for visualising dense monotonic discontinuous function,,"My question is about the function defined in Rudin 4.31, mentioned by this question: Remark 4.31 in Baby Rudin: How to verify these points? The function is defined as $$f(x) \colon= \sum_{x_n < x} c_n \ \ \ \ \text{ for all } x \in (a,b).$$ I'm having trouble trying to visualise what such a function would look like in terms of its ""smoothness"". In particular, I'm interested in the case where  $$a = 0$$ $$b = 1$$ $$\{x_n\} = \{\mathrm{rationals\:in\:(0,1)}\}$$ $$c_n = \frac{1}{2^n}$$ My question is What would the graph of $f$ look like in terms of its ""smoothness""? I thought it would look something like the Weierstrass function but monotonic. It seems like there would have to be some sort of recursive/fractal structure, and yet there also has to be a definite ""jump"" at each rational point in the domain. I would appreciate a diagram with an intuitive explanation. Edit: This image is how I'm trying to imagine it. The blue line is $f(x)$. The total length of the green gaps converge to $2$. The red gaps can't really exist, because the rationals are dense, so I'm wondering if the actual graph when you get rid of the red gaps would look ""smooth"" or ""jagged"" (like the Weierstrass function).","My question is about the function defined in Rudin 4.31, mentioned by this question: Remark 4.31 in Baby Rudin: How to verify these points? The function is defined as $$f(x) \colon= \sum_{x_n < x} c_n \ \ \ \ \text{ for all } x \in (a,b).$$ I'm having trouble trying to visualise what such a function would look like in terms of its ""smoothness"". In particular, I'm interested in the case where  $$a = 0$$ $$b = 1$$ $$\{x_n\} = \{\mathrm{rationals\:in\:(0,1)}\}$$ $$c_n = \frac{1}{2^n}$$ My question is What would the graph of $f$ look like in terms of its ""smoothness""? I thought it would look something like the Weierstrass function but monotonic. It seems like there would have to be some sort of recursive/fractal structure, and yet there also has to be a definite ""jump"" at each rational point in the domain. I would appreciate a diagram with an intuitive explanation. Edit: This image is how I'm trying to imagine it. The blue line is $f(x)$. The total length of the green gaps converge to $2$. The red gaps can't really exist, because the rationals are dense, so I'm wondering if the actual graph when you get rid of the red gaps would look ""smooth"" or ""jagged"" (like the Weierstrass function).",,"['calculus', 'real-analysis', 'analysis', 'continuity', 'intuition']"
50,"$f:[a,b]\to \mathbb R$ is continuous , has a finite number of local maxima and minima ; then how to prove that $f$ is bounded variation on $[a,b]$ ?","is continuous , has a finite number of local maxima and minima ; then how to prove that  is bounded variation on  ?","f:[a,b]\to \mathbb R f [a,b]","If $f:[a,b]\to \mathbb R$ is a continuous function having  finite number of local maxima and minima ; then how to prove that $f$ is bounded variation on $[a,b]$ ?","If $f:[a,b]\to \mathbb R$ is a continuous function having  finite number of local maxima and minima ; then how to prove that $f$ is bounded variation on $[a,b]$ ?",,"['real-analysis', 'analysis']"
51,"inequality involving $x$, $x^3$,$\sin(x),\cos(x)$","inequality involving , ,","x x^3 \sin(x),\cos(x)","Let $x \in \left[0,\dfrac {\pi} 2 \right]$. Prove the inequality $$6x \ge 6\sin x +x^3 \cdot \cos x$$ there is nice solution using Taylor expansion. Is there other one?","Let $x \in \left[0,\dfrac {\pi} 2 \right]$. Prove the inequality $$6x \ge 6\sin x +x^3 \cdot \cos x$$ there is nice solution using Taylor expansion. Is there other one?",,['analysis']
52,Prove that $Df(p)=f(p)T$ where $T(q)=\int_{0}^1q$,Prove that  where,Df(p)=f(p)T T(q)=\int_{0}^1q,"Let $E=\mathcal{C}[0,1]$ provide with $\|\cdot\|_\infty$ norm. Let $f:E\to \mathbb{R}$, given by $f(p)=e^{\int_0^1 p}$.  I need to prove that $f$ is differentiable. My approach: Let $p,q\in\mathcal{C}[0,1]$ then, $$f(p+tq)-f(p)=e^{\int_0^1 p+tq}-e^{\int_0^1 p}=e^{\int_0^1 p}\left( e^{t\int_{0}^1 q}-1\right)=f(p)\left(1+t\int_{0}^1q+t\ o(1)-1 \right)$$ The last equality is true, just applying Taylor's Formula. Then we have $$\lim_{t\to 0}\frac{f(p+tq)-f(p)}{t}= f'_{q}(p)=f(p)\int_{0}^1 q$$ So I deduce that if $f$ is differentiable, then $Df(p)=f(p)T$, where $T(q)=\int_{0}^1q$. The operator $f(p)T$ is linear because $T$ is linear, and is continuous because: $\left|f(p)\int_{0}^1 q \right|\leq f(p)\|q\|_{\infty}$, from this I know that $\left(\int_{0}^1q\right)^2\leq \|q\|_{\infty}^2$. Maybe my question is silly, but how can I prove that: $$f(p+q)-f(p)=f(p)T(q)+\|q\|_{\infty}\ o(1)$$ I tried for longer but finally I'm stuck! Thanks in advance.","Let $E=\mathcal{C}[0,1]$ provide with $\|\cdot\|_\infty$ norm. Let $f:E\to \mathbb{R}$, given by $f(p)=e^{\int_0^1 p}$.  I need to prove that $f$ is differentiable. My approach: Let $p,q\in\mathcal{C}[0,1]$ then, $$f(p+tq)-f(p)=e^{\int_0^1 p+tq}-e^{\int_0^1 p}=e^{\int_0^1 p}\left( e^{t\int_{0}^1 q}-1\right)=f(p)\left(1+t\int_{0}^1q+t\ o(1)-1 \right)$$ The last equality is true, just applying Taylor's Formula. Then we have $$\lim_{t\to 0}\frac{f(p+tq)-f(p)}{t}= f'_{q}(p)=f(p)\int_{0}^1 q$$ So I deduce that if $f$ is differentiable, then $Df(p)=f(p)T$, where $T(q)=\int_{0}^1q$. The operator $f(p)T$ is linear because $T$ is linear, and is continuous because: $\left|f(p)\int_{0}^1 q \right|\leq f(p)\|q\|_{\infty}$, from this I know that $\left(\int_{0}^1q\right)^2\leq \|q\|_{\infty}^2$. Maybe my question is silly, but how can I prove that: $$f(p+q)-f(p)=f(p)T(q)+\|q\|_{\infty}\ o(1)$$ I tried for longer but finally I'm stuck! Thanks in advance.",,['analysis']
53,Can anyone prove D'Alembert Criterion (Dalambert) criterion for converging positive sequences?,Can anyone prove D'Alembert Criterion (Dalambert) criterion for converging positive sequences?,,"This will most likely be on the exam,  but it is not given in the text book. In my notebook I have this proof which I will type out, but it makes no sense. Here it goes: $$\text{D'Alembert  Criterion}$$ Let $\sum_{n=1}^{\infty}a_n$ with $a_n > 0$. 1.) If $\exists N$ such that $\forall n >N$, $\frac{a_{n+1}}{a_n}\leq q < 1$ then $\sum_{n=1}^{\infty}a_n$ converges. 2.) If $\exists N$ such that $\forall n >N$, $\frac{a_{n+1}}{a_n}> 1$ then $\sum_{n=1}^{\infty}a_n$ diverges. Proof: $$  \forall n >N \ \ \  \frac{a_{n+1}}{a_n}\leq q < 1 \\  For \ \ \  n=1: \frac{a_{2}}{a_1} \leq q \implies a_2 \leq qa_1  \\  For \ \ \  n=2: \frac{a_{3}}{a_2} \leq q \implies a_3 \leq q^2a_1 \\  For \ \ \  n=2: \frac{a_{3}}{a_2} \leq q \implies a_4 \leq q^3a_1$$ $$\\ .... \\ \\  For \ \ \  \frac{a_{n+1}}{a_{n}} \leq q \implies a_3 \leq q^{n-1}a_1=\frac{a_1}{q}q^n $$ So if $\sum_{n=1}^{\infty}q^n = \frac{q}{1-q}$ for $|q|<1 \implies a_n conv.???$","This will most likely be on the exam,  but it is not given in the text book. In my notebook I have this proof which I will type out, but it makes no sense. Here it goes: $$\text{D'Alembert  Criterion}$$ Let $\sum_{n=1}^{\infty}a_n$ with $a_n > 0$. 1.) If $\exists N$ such that $\forall n >N$, $\frac{a_{n+1}}{a_n}\leq q < 1$ then $\sum_{n=1}^{\infty}a_n$ converges. 2.) If $\exists N$ such that $\forall n >N$, $\frac{a_{n+1}}{a_n}> 1$ then $\sum_{n=1}^{\infty}a_n$ diverges. Proof: $$  \forall n >N \ \ \  \frac{a_{n+1}}{a_n}\leq q < 1 \\  For \ \ \  n=1: \frac{a_{2}}{a_1} \leq q \implies a_2 \leq qa_1  \\  For \ \ \  n=2: \frac{a_{3}}{a_2} \leq q \implies a_3 \leq q^2a_1 \\  For \ \ \  n=2: \frac{a_{3}}{a_2} \leq q \implies a_4 \leq q^3a_1$$ $$\\ .... \\ \\  For \ \ \  \frac{a_{n+1}}{a_{n}} \leq q \implies a_3 \leq q^{n-1}a_1=\frac{a_1}{q}q^n $$ So if $\sum_{n=1}^{\infty}q^n = \frac{q}{1-q}$ for $|q|<1 \implies a_n conv.???$",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
54,Eigenvalue of Laplacian with Robin boundary condition,Eigenvalue of Laplacian with Robin boundary condition,,"Let $\Omega$ be a bounded domain in $\mathbb{R}^{n}$ with smooth boundary $\partial \Omega$ and let $\nu$ denote the outer unit normal. Let $u$ be an eigenfunction of $-\Delta$ in $\Omega$ satisfying $\frac{\partial u}{\partial \nu} - u = 0$ on $\partial \Omega$. Must the associated eigenvalue for $u$ be negative? Since $u$ is an eigenfunction of $-\Delta$ and satisfies the above boundary condition on $\partial \Omega$, integration by parts gives that $$\lambda \int_{\Omega}u^{2}\, dx = \int_{\Omega}|\nabla u|^{2}\, dx - \int_{\partial \Omega}u^{2}\, d\sigma.$$ However, it is not immediate that the right hand side is always $< 0$.","Let $\Omega$ be a bounded domain in $\mathbb{R}^{n}$ with smooth boundary $\partial \Omega$ and let $\nu$ denote the outer unit normal. Let $u$ be an eigenfunction of $-\Delta$ in $\Omega$ satisfying $\frac{\partial u}{\partial \nu} - u = 0$ on $\partial \Omega$. Must the associated eigenvalue for $u$ be negative? Since $u$ is an eigenfunction of $-\Delta$ and satisfies the above boundary condition on $\partial \Omega$, integration by parts gives that $$\lambda \int_{\Omega}u^{2}\, dx = \int_{\Omega}|\nabla u|^{2}\, dx - \int_{\partial \Omega}u^{2}\, d\sigma.$$ However, it is not immediate that the right hand side is always $< 0$.",,"['real-analysis', 'analysis', 'partial-differential-equations']"
55,How to continue $f \in C ^\infty[a;b]$ to $f \in C ^\infty (\mathbb {R })$?,How to continue  to ?,f \in C ^\infty[a;b] f \in C ^\infty (\mathbb {R }),"Is there a general way to extend a smooth function on a closed interval $[a,b]$ to one that is defined on the entire $\mathbb R$? It is not OK to reflect this function in points $a$ and $b$, and then in points $a -(b-a) $ and $b +(b-a) $ etc symmetrically because the result wouldn't be differentable in this points","Is there a general way to extend a smooth function on a closed interval $[a,b]$ to one that is defined on the entire $\mathbb R$? It is not OK to reflect this function in points $a$ and $b$, and then in points $a -(b-a) $ and $b +(b-a) $ etc symmetrically because the result wouldn't be differentable in this points",,['analysis']
56,"Examine the convergence of a sequence $\{a_{n}\}$ which is given by $a_{1}=a>0,a_{2}=b>0, a_{n+2}=\sqrt{a_{n+1}a_{n}},n\ge 1$",Examine the convergence of a sequence  which is given by,"\{a_{n}\} a_{1}=a>0,a_{2}=b>0, a_{n+2}=\sqrt{a_{n+1}a_{n}},n\ge 1","I used inequality between arithmetic and geometric means to show that a sequence $\{a_{n}\}$ is bounded: $$a_{n+2}=\sqrt{a_{n+1}a_{n}}\le \frac{a_{n+1}+a_{n}}{2}$$ Solving this, I get quadratic inequality $$a^2_{n+1}-2a_{n+1}a_{n}+a^2_{n}\ge 0$$ which gets me to $$0\le a_{n}\le 1$$ thus, sequence is bounded. I get that sequence is not monotonic, because $$a_{n}\le a_{n+2} \le {a_{n+1}}$$ or $$a_{n+1}\le a_{n+2} \le {a_{n}}$$ Is this right?","I used inequality between arithmetic and geometric means to show that a sequence $\{a_{n}\}$ is bounded: $$a_{n+2}=\sqrt{a_{n+1}a_{n}}\le \frac{a_{n+1}+a_{n}}{2}$$ Solving this, I get quadratic inequality $$a^2_{n+1}-2a_{n+1}a_{n}+a^2_{n}\ge 0$$ which gets me to $$0\le a_{n}\le 1$$ thus, sequence is bounded. I get that sequence is not monotonic, because $$a_{n}\le a_{n+2} \le {a_{n+1}}$$ or $$a_{n+1}\le a_{n+2} \le {a_{n}}$$ Is this right?",,"['calculus', 'sequences-and-series', 'analysis', 'convergence-divergence', 'recurrence-relations']"
57,"Proof that $f(x)=0 \forall x \in [a,b]$",Proof that,"f(x)=0 \forall x \in [a,b]","Lemma : If $f \in C([a,b])$ and $\int_a^b f(x) h(x) dx=0 \  \forall h \in C^2([a,b])$ with $h(a)=h(b)=0$ then $f(x)=0 \ \forall x \in [a,b]$. Proof of lemma : Suppose that there is a $x_0 \in (a,b)$ such that $f(x_0) \neq 0$, for example without loss of generality we suppose that $f(x_0)>0$. Because of continuity there is an interval $[x_1, x_2] \subset (a,b)$ such that $x_0 \in (x_1, x_2)$ and $f(x)>0 \ \forall x \in (x_1, x_2)$. We define the function $g(x)=\left\{\begin{matrix} (x_2-x)^3 (x-x_1)^3 & , x \in (x_1, x_2)\\ \\ 0 & ,  x \in [a,b] \setminus{(x_1,x_2)}  \end{matrix}\right.$. Then $g \in C^2([a,b])$ and $g(a)=g(b)=0$. From the hypothesis we have: $$\int_a^b f(x)g(x) dx=0$$ But $\int_a^b f(x)g(x) dx= \int_{x_1}^{x_2} f(x)g(x) dx>0$, contradiction. First of all, why do we say that there is an interval $[x_1, x_2] \subset (a,b)$ such that $x_0 \in (x_1, x_2)$ and $f(x)>0 \forall x \in (x_1, x_2)$? Why don't we pick the closed interval $[x_1, x_2]$ ? Also why does it hold that $\int_a^b f(x) g(x) dx= \int_{x_1}^{x_2} f(x) g(x) dx$? Furthermore, the prof told us that we couldn't take the function $g(x)=\left\{\begin{matrix} (x_2-x)^2 (x-x_1)^2 & , x \in (x_1, x_2)\\ \\ 0 & ,  x \in [a,b] \setminus{(x_1,x_2)}  \end{matrix}\right.$ but the powers both of $(x_2-x), (x-x_1)$ have to be greater or equal to $3$. Why is it like that?","Lemma : If $f \in C([a,b])$ and $\int_a^b f(x) h(x) dx=0 \  \forall h \in C^2([a,b])$ with $h(a)=h(b)=0$ then $f(x)=0 \ \forall x \in [a,b]$. Proof of lemma : Suppose that there is a $x_0 \in (a,b)$ such that $f(x_0) \neq 0$, for example without loss of generality we suppose that $f(x_0)>0$. Because of continuity there is an interval $[x_1, x_2] \subset (a,b)$ such that $x_0 \in (x_1, x_2)$ and $f(x)>0 \ \forall x \in (x_1, x_2)$. We define the function $g(x)=\left\{\begin{matrix} (x_2-x)^3 (x-x_1)^3 & , x \in (x_1, x_2)\\ \\ 0 & ,  x \in [a,b] \setminus{(x_1,x_2)}  \end{matrix}\right.$. Then $g \in C^2([a,b])$ and $g(a)=g(b)=0$. From the hypothesis we have: $$\int_a^b f(x)g(x) dx=0$$ But $\int_a^b f(x)g(x) dx= \int_{x_1}^{x_2} f(x)g(x) dx>0$, contradiction. First of all, why do we say that there is an interval $[x_1, x_2] \subset (a,b)$ such that $x_0 \in (x_1, x_2)$ and $f(x)>0 \forall x \in (x_1, x_2)$? Why don't we pick the closed interval $[x_1, x_2]$ ? Also why does it hold that $\int_a^b f(x) g(x) dx= \int_{x_1}^{x_2} f(x) g(x) dx$? Furthermore, the prof told us that we couldn't take the function $g(x)=\left\{\begin{matrix} (x_2-x)^2 (x-x_1)^2 & , x \in (x_1, x_2)\\ \\ 0 & ,  x \in [a,b] \setminus{(x_1,x_2)}  \end{matrix}\right.$ but the powers both of $(x_2-x), (x-x_1)$ have to be greater or equal to $3$. Why is it like that?",,"['analysis', 'functions']"
58,IVT and fixed point theorem,IVT and fixed point theorem,,"Suppose that $f:[0,1]→[0,2]$ is continuous. Use the Intermediate Value Theorem to prove that there exists $c∈[0,1]$ such that $f(c)=2c^2$ The answer to this goes from the Fixed point theorem. But in that theorem the domain and codomain were the same but in this it is not. Here is what I did: Define $g:[0,1] \rightarrow [0,2]$ by $g(x)=2x^2$. If $g(0)=0$ or $g(1)=2$ then we are done. Otherwise, since $f(0),f(1) \in [0,2]$, $f(0)>0$ and $f(1)<2$ Define $h:[0,1] \rightarrow [0,2]$ by $h(x)= 2x^2 - f(x)$. This is a continuous function. So $h(0)<0<h(1)$. By IVT, there exists $c \in (0,1)$ such that $h(c)=0$ giving the desired result. Does this still work since the dom and codom of each function weren't the same. I know that for IVT, they dont have to be the same but for the ixed point theorem, it says they have to. Is my answer correct?","Suppose that $f:[0,1]→[0,2]$ is continuous. Use the Intermediate Value Theorem to prove that there exists $c∈[0,1]$ such that $f(c)=2c^2$ The answer to this goes from the Fixed point theorem. But in that theorem the domain and codomain were the same but in this it is not. Here is what I did: Define $g:[0,1] \rightarrow [0,2]$ by $g(x)=2x^2$. If $g(0)=0$ or $g(1)=2$ then we are done. Otherwise, since $f(0),f(1) \in [0,2]$, $f(0)>0$ and $f(1)<2$ Define $h:[0,1] \rightarrow [0,2]$ by $h(x)= 2x^2 - f(x)$. This is a continuous function. So $h(0)<0<h(1)$. By IVT, there exists $c \in (0,1)$ such that $h(c)=0$ giving the desired result. Does this still work since the dom and codom of each function weren't the same. I know that for IVT, they dont have to be the same but for the ixed point theorem, it says they have to. Is my answer correct?",,"['real-analysis', 'analysis']"
59,Proving that a trigonometric sum is in $L^2$,Proving that a trigonometric sum is in,L^2,"How can I use Parseval's identity to prove that $$f(x)=\sum_{k=1}^\infty \frac{\sin(kx)}{1+k}$$ is in $L^2(0,\pi)$? Thank you!","How can I use Parseval's identity to prove that $$f(x)=\sum_{k=1}^\infty \frac{\sin(kx)}{1+k}$$ is in $L^2(0,\pi)$? Thank you!",,"['analysis', 'summation', 'fourier-analysis', 'fourier-series', 'lebesgue-integral']"
60,The closure of an open set in $\mathbb{R}^n$ is a manifold,The closure of an open set in  is a manifold,\mathbb{R}^n,"I want to solve the following exercise from M. Spivak's Calculus on Manifolds (p. 114): (a) Let $A \subseteq \mathbb{R}^n$ be an open set such that boundary $A$ is an $(n-1)$-dimensional manifold. Show that $N=A \cup \text{ boundary }A$ is an $n$-dimensional manifold-with-boundary. (It is well to bear in mind the following example: if $A=\{x \in \mathbb{R}^n: |x|<1 \text{ or } 1<|x|<2 \}$ then $N=A \cup \text{ boundary }A$ is a manifold-with-boundary, but $\partial N \neq \text{boundary }A$.) (b) Prove a similar assertion for an open subset of an $n$-dimensional manifold. My attempt: If $x \in \text{int } N$ then $x$ satisfies the manifold condition ""(M)"" from the textbook: Setting $U=V:=\text{int }N$ and $h=\text{Id}_U$ we have that $h$ is a diffeomorphism $U \to V$ with $h(U \cap N)=V \cap \mathbb{R}^n$. If $x \in N \setminus \text{ int }N$ I want to prove that $x$ satisfies the condition $(M')$: there exist open sets $U' \ni x,V'$ in $\mathbb{R}^n$ and a diffeomorphism $g:U' \to V'$ such that $g(U' \cap N)=V' \cap \mathbb{H}^n$, where $\mathbb{H}^n$ is the half space where the last coordinate is non-negative. The condition ""(M')"" also requires that $g(x)$ has its last coordinate$=0$. Now, since boundary $A$ is an $(n-1)$ dimensional manifold we have the existence of open sets $U'' \ni x,V'' \in \mathbb{R}^n$ and a diffeomorphim $f:U'' \to V''$ such that $f(U'' \cap \text{ boundary }A)=V'' \cap (\mathbb{R}^{n-1} \times \{0\})$. I now want to say that we can pretty much use the same $f$ as $g$. As $f(U'' \cap A)$ must not intersect the hyper-plane $\{x_n=0\}$. If $U'' \cap A$ is connected I can compose with a negation if necessary to make the image be in the half-space $\mathbb{H}^n$. However, if there are several connected components I couldn't do anything. Can anyone please help me finish this proof? Thank you!","I want to solve the following exercise from M. Spivak's Calculus on Manifolds (p. 114): (a) Let $A \subseteq \mathbb{R}^n$ be an open set such that boundary $A$ is an $(n-1)$-dimensional manifold. Show that $N=A \cup \text{ boundary }A$ is an $n$-dimensional manifold-with-boundary. (It is well to bear in mind the following example: if $A=\{x \in \mathbb{R}^n: |x|<1 \text{ or } 1<|x|<2 \}$ then $N=A \cup \text{ boundary }A$ is a manifold-with-boundary, but $\partial N \neq \text{boundary }A$.) (b) Prove a similar assertion for an open subset of an $n$-dimensional manifold. My attempt: If $x \in \text{int } N$ then $x$ satisfies the manifold condition ""(M)"" from the textbook: Setting $U=V:=\text{int }N$ and $h=\text{Id}_U$ we have that $h$ is a diffeomorphism $U \to V$ with $h(U \cap N)=V \cap \mathbb{R}^n$. If $x \in N \setminus \text{ int }N$ I want to prove that $x$ satisfies the condition $(M')$: there exist open sets $U' \ni x,V'$ in $\mathbb{R}^n$ and a diffeomorphism $g:U' \to V'$ such that $g(U' \cap N)=V' \cap \mathbb{H}^n$, where $\mathbb{H}^n$ is the half space where the last coordinate is non-negative. The condition ""(M')"" also requires that $g(x)$ has its last coordinate$=0$. Now, since boundary $A$ is an $(n-1)$ dimensional manifold we have the existence of open sets $U'' \ni x,V'' \in \mathbb{R}^n$ and a diffeomorphim $f:U'' \to V''$ such that $f(U'' \cap \text{ boundary }A)=V'' \cap (\mathbb{R}^{n-1} \times \{0\})$. I now want to say that we can pretty much use the same $f$ as $g$. As $f(U'' \cap A)$ must not intersect the hyper-plane $\{x_n=0\}$. If $U'' \cap A$ is connected I can compose with a negation if necessary to make the image be in the half-space $\mathbb{H}^n$. However, if there are several connected components I couldn't do anything. Can anyone please help me finish this proof? Thank you!",,"['analysis', 'manifolds']"
61,Equivalency of real numbers and points in the plane?,Equivalency of real numbers and points in the plane?,,"I understand that the set of real numbers is equivalent to the set of real numbers in the interval $(-1,1)$ by simply using $arctan$ function. However, I do not know how to find a one-to-one mapping from real numbers (or any equivalent set to real numbers) to the set of all points in a plane in $\mathbb{R}^2$. Can anyone help?","I understand that the set of real numbers is equivalent to the set of real numbers in the interval $(-1,1)$ by simply using $arctan$ function. However, I do not know how to find a one-to-one mapping from real numbers (or any equivalent set to real numbers) to the set of all points in a plane in $\mathbb{R}^2$. Can anyone help?",,"['real-analysis', 'analysis']"
62,Is $\limsup_{z\to z_0}f(z)=\limsup_{k\to\infty}f(z_k)$?,Is ?,\limsup_{z\to z_0}f(z)=\limsup_{k\to\infty}f(z_k),"Let $\Omega\in\Bbb C$ open, $f:\Omega\to\Bbb R$ a generic function. Let $(z_k)_k\subset\Omega$ s.t. $\lim_{k}z_k=:z_0\in\Omega$. The question is the following: is true that $$ \limsup_{z\to z_0}f(z)=\limsup_{k\to\infty}f(z_k)\;\;\;\;\;? $$ I recall that $\limsup_{z\to z_0}f(z):=\lim_{r\to0^+}\left(\sup_{B(z_0,r[}f(z)\right)$ and $\limsup_{k\to\infty}f(z_k):=\lim_{k\to\infty}\left(\sup_{n\ge k}f(z_n)\right)$. My feeling is the answer is yes, but I wasn't able to prove this. Can someone help me? Many thanks!","Let $\Omega\in\Bbb C$ open, $f:\Omega\to\Bbb R$ a generic function. Let $(z_k)_k\subset\Omega$ s.t. $\lim_{k}z_k=:z_0\in\Omega$. The question is the following: is true that $$ \limsup_{z\to z_0}f(z)=\limsup_{k\to\infty}f(z_k)\;\;\;\;\;? $$ I recall that $\limsup_{z\to z_0}f(z):=\lim_{r\to0^+}\left(\sup_{B(z_0,r[}f(z)\right)$ and $\limsup_{k\to\infty}f(z_k):=\lim_{k\to\infty}\left(\sup_{n\ge k}f(z_n)\right)$. My feeling is the answer is yes, but I wasn't able to prove this. Can someone help me? Many thanks!",,"['analysis', 'limsup-and-liminf']"
63,Find the minimum value of $ \int_0^1 (1+x^2)f^2(x)dx$,Find the minimum value of, \int_0^1 (1+x^2)f^2(x)dx,"I was trying to solve a question of an entrance exam. I have taken help for a similar problem from MSE but again I am stuck in the problem. Please help me. Let $ D= \{f \in [0,1] : f \text{continuous and} \displaystyle \int_0^1f(x)dx = 1\}$. Find the value of $$\text{min}_{f\in D} \displaystyle \int_0^1 (1+x^2)f^2(x)dx$$ I have tried the same eay as described in this problem but I failed to apply Euler-Lagrange equation.I can not find any other way to proceed.  Please help me. Thnx in advance.","I was trying to solve a question of an entrance exam. I have taken help for a similar problem from MSE but again I am stuck in the problem. Please help me. Let $ D= \{f \in [0,1] : f \text{continuous and} \displaystyle \int_0^1f(x)dx = 1\}$. Find the value of $$\text{min}_{f\in D} \displaystyle \int_0^1 (1+x^2)f^2(x)dx$$ I have tried the same eay as described in this problem but I failed to apply Euler-Lagrange equation.I can not find any other way to proceed.  Please help me. Thnx in advance.",,"['real-analysis', 'integration', 'analysis']"
64,Convergence of a sequence by convergence of sub-subsequence,Convergence of a sequence by convergence of sub-subsequence,,"Suppose that $\{p_n\}_{n \in \mathbb{N}}$ is a sequence in a metric space $X$. Assuming that every subsequence of $\{p_n\}_{n \in \mathbb{N}}$ has itself a subsequence that converges, say, to $p$, show that $\{p_n\}_{n \in \mathbb{N}} \to p$. Solution Attempt : Take $ \mathbb{N} \supset K := \{n,n+1,n+2, \ldots\}$ for $n \in \mathbb{N}$ to be our subsequence. Now, there exists $J_1 \subset K \subset \mathbb{N}$ so that $\{p_j\}_{j \in J_1} \to p$. If $J_1 = K$, we're done, otherwise assume $J_1 \subset K$. Thus, $K \setminus J_1$ is non-empty, take it to be our subsequence. Thus, there exists $J_2 \subset (K \setminus J_1)$ so that $\{p_j\}_{j \in J_2} \to p$. If $J_2 = (K \setminus J_1)$, we're done, otherwise assume $J_2 \subset (K \setminus J_1)$ so that the next iteration is non-empty. Inductively let, $$K \setminus \bigcup_{i=1}^{k} J_i$$ be our next subsequence so that there exists $$J_{k+1} \subset \Big(K \setminus \bigcup_{i=1}^{k} J_i \Big)$$ so that $\{p_j\}_{j \in J_{k+1}} \to p$. It is possible that we are left with a finite set $B \subset K$ so that $B$ is never in $J_k$. In this case, set $m = \max\{B\}$, so that the original sequence converges above $m$. Thus, $\{p_n\}_{n \in \mathbb{N}} \to p$ as desired. Is this a correct proof? I proved the statement by contradiction rather easily, but I wanted to try my hand at a direct proof. Any comments are appreciated.","Suppose that $\{p_n\}_{n \in \mathbb{N}}$ is a sequence in a metric space $X$. Assuming that every subsequence of $\{p_n\}_{n \in \mathbb{N}}$ has itself a subsequence that converges, say, to $p$, show that $\{p_n\}_{n \in \mathbb{N}} \to p$. Solution Attempt : Take $ \mathbb{N} \supset K := \{n,n+1,n+2, \ldots\}$ for $n \in \mathbb{N}$ to be our subsequence. Now, there exists $J_1 \subset K \subset \mathbb{N}$ so that $\{p_j\}_{j \in J_1} \to p$. If $J_1 = K$, we're done, otherwise assume $J_1 \subset K$. Thus, $K \setminus J_1$ is non-empty, take it to be our subsequence. Thus, there exists $J_2 \subset (K \setminus J_1)$ so that $\{p_j\}_{j \in J_2} \to p$. If $J_2 = (K \setminus J_1)$, we're done, otherwise assume $J_2 \subset (K \setminus J_1)$ so that the next iteration is non-empty. Inductively let, $$K \setminus \bigcup_{i=1}^{k} J_i$$ be our next subsequence so that there exists $$J_{k+1} \subset \Big(K \setminus \bigcup_{i=1}^{k} J_i \Big)$$ so that $\{p_j\}_{j \in J_{k+1}} \to p$. It is possible that we are left with a finite set $B \subset K$ so that $B$ is never in $J_k$. In this case, set $m = \max\{B\}$, so that the original sequence converges above $m$. Thus, $\{p_n\}_{n \in \mathbb{N}} \to p$ as desired. Is this a correct proof? I proved the statement by contradiction rather easily, but I wanted to try my hand at a direct proof. Any comments are appreciated.",,"['real-analysis', 'analysis', 'proof-verification', 'proof-writing']"
65,Sup and inf of $n \sin(1/n)$,Sup and inf of,n \sin(1/n),"If $n$ is a natural number then, what is the supremum and infimum of $n\sin(1/n)$? is the question I want to solve. I drew $sin(x)/x$ graph and I think that the supremum is $1$ and infimum is $sin(1)$. Is that right? And how can I solve?","If $n$ is a natural number then, what is the supremum and infimum of $n\sin(1/n)$? is the question I want to solve. I drew $sin(x)/x$ graph and I think that the supremum is $1$ and infimum is $sin(1)$. Is that right? And how can I solve?",,"['analysis', 'supremum-and-infimum']"
66,sum approximation of a Lipschitz-continuous function,sum approximation of a Lipschitz-continuous function,,"Let $f: [0, 1] \to \mathbb{R}$ be a Lipschitz continuous function with a Lipschitz constant $L > 0$, meaning: $$|f(x) - f(y)| ≤ L|x - y|  \space\space\space \forall x, y \in [0, 1]$$ For the approximation of $f$ using Riemann-sums with equidistant supporting points, proof the following statement: $$\left|\int_0^1 f(x)dx - \frac{1}{n} \sum_{k=1}^n f\left(\frac{k}{n}\right) \right|\space ≤ \frac{L}{n} $$ Now, as I haven't worked much with Riemann sums yet, I thought that solving this using the mean value theorem for integration might be a proper way, but I haven't come so far yet. Thanks in advance!","Let $f: [0, 1] \to \mathbb{R}$ be a Lipschitz continuous function with a Lipschitz constant $L > 0$, meaning: $$|f(x) - f(y)| ≤ L|x - y|  \space\space\space \forall x, y \in [0, 1]$$ For the approximation of $f$ using Riemann-sums with equidistant supporting points, proof the following statement: $$\left|\int_0^1 f(x)dx - \frac{1}{n} \sum_{k=1}^n f\left(\frac{k}{n}\right) \right|\space ≤ \frac{L}{n} $$ Now, as I haven't worked much with Riemann sums yet, I thought that solving this using the mean value theorem for integration might be a proper way, but I haven't come so far yet. Thanks in advance!",,"['real-analysis', 'integration', 'analysis', 'definite-integrals']"
67,"Proof that the set $\{(x_1, x_2) \in E^2: x_1>x_2\}$ is open",Proof that the set  is open,"\{(x_1, x_2) \in E^2: x_1>x_2\}","(Note: $E^2$ denotes $2$-dimensional Euclidean space) My question concerns the below ""proof."" Once the radius of the open ball is determined, how can it be shown that the ball contains only points in S? Let $S \subset E^2 = \{(x_1, x_2) \in E^2: x_1>x_2\}$ . Let the point $\ p = (x_i, x_j) \in S$. Since $x_i>x_j$, the difference $x_i-x_j$ is a positive real number. Call this $\epsilon$. Then $p$ is the center of an open ball with radius $\frac{\epsilon}{\sqrt{2}}$ which contains only points in S.","(Note: $E^2$ denotes $2$-dimensional Euclidean space) My question concerns the below ""proof."" Once the radius of the open ball is determined, how can it be shown that the ball contains only points in S? Let $S \subset E^2 = \{(x_1, x_2) \in E^2: x_1>x_2\}$ . Let the point $\ p = (x_i, x_j) \in S$. Since $x_i>x_j$, the difference $x_i-x_j$ is a positive real number. Call this $\epsilon$. Then $p$ is the center of an open ball with radius $\frac{\epsilon}{\sqrt{2}}$ which contains only points in S.",,"['real-analysis', 'general-topology', 'analysis']"
68,If $\int_1^x f(t)^2dt \le \frac{x^3-1}{3}$ then $\int_1^2 f(t)dt \le \frac{3}{2}$,If  then,\int_1^x f(t)^2dt \le \frac{x^3-1}{3} \int_1^2 f(t)dt \le \frac{3}{2},"If $f:[1,2]\to [0, \infty )$ is an Riemann integrable function such that $\int_1^x f(t)^2dt \le \frac{x^3-1}{3} , \forall x \in [1,2]$. Prove that $\int_1^2 f(t)dt \le \frac{3}{2}$ . First, I used Cauchy's inequality: $(x-1) \int_1^x f^2(t)dt \ge \left( \int_1^x f(t)dt \right)^2 $ so $\int_1^x f(t)dt \le \sqrt{\frac{(x^3-1)(x-1)}{3}}$ , so $\int_1^x f(t)dt \le \sqrt{ \frac{7}{3}}$, but $\frac{3}{2} < \sqrt{\frac{7}{3}}$. Another attempt is: From $(f(x)-x)^2 \ge 0, \forall x\in [1,2]$, so $f^2(x)+x^2 \ge 2xf(x), \forall x\in [1,2]$. Integrating this inequality on [1,x] and using the hyphotesis we get that $\frac{x^3-1}{3} \ge \int_1^x tf(t)dt , \forall x\in[1,2]$. Can you help me, please ?! Thank you!","If $f:[1,2]\to [0, \infty )$ is an Riemann integrable function such that $\int_1^x f(t)^2dt \le \frac{x^3-1}{3} , \forall x \in [1,2]$. Prove that $\int_1^2 f(t)dt \le \frac{3}{2}$ . First, I used Cauchy's inequality: $(x-1) \int_1^x f^2(t)dt \ge \left( \int_1^x f(t)dt \right)^2 $ so $\int_1^x f(t)dt \le \sqrt{\frac{(x^3-1)(x-1)}{3}}$ , so $\int_1^x f(t)dt \le \sqrt{ \frac{7}{3}}$, but $\frac{3}{2} < \sqrt{\frac{7}{3}}$. Another attempt is: From $(f(x)-x)^2 \ge 0, \forall x\in [1,2]$, so $f^2(x)+x^2 \ge 2xf(x), \forall x\in [1,2]$. Integrating this inequality on [1,x] and using the hyphotesis we get that $\frac{x^3-1}{3} \ge \int_1^x tf(t)dt , \forall x\in[1,2]$. Can you help me, please ?! Thank you!",,['analysis']
69,Pointwise convergence of a sequence of piecewise functions $\{f_n\}$,Pointwise convergence of a sequence of piecewise functions,\{f_n\},"For $n \ge 1$, define functions $f_n$ on $[0,\infty)$ by $$f_n (x) =  \begin{cases}   e^{-x} &\quad\text{for}\quad 0 \le x \le n\\   e^{-2n} (e^n + n - x) &\quad\text{for}\quad n \le x \le n + e^n \\   0 &\quad\text{for}\quad x\ge n + e^n.  \end{cases} $$ Find the pointwise limit $f$ of $f_n$.  Show that the convergence is uniform on $[0,\infty)$. Would the $\lim_{n\to\infty}f_n=e^{-x}$ and it is uniform since $f=e^{-x} \forall x\in[0,\infty).$","For $n \ge 1$, define functions $f_n$ on $[0,\infty)$ by $$f_n (x) =  \begin{cases}   e^{-x} &\quad\text{for}\quad 0 \le x \le n\\   e^{-2n} (e^n + n - x) &\quad\text{for}\quad n \le x \le n + e^n \\   0 &\quad\text{for}\quad x\ge n + e^n.  \end{cases} $$ Find the pointwise limit $f$ of $f_n$.  Show that the convergence is uniform on $[0,\infty)$. Would the $\lim_{n\to\infty}f_n=e^{-x}$ and it is uniform since $f=e^{-x} \forall x\in[0,\infty).$",,"['calculus', 'sequences-and-series', 'analysis', 'convergence-divergence']"
70,"For normed vectorspaces $V$, $A,B \subset V$ if $A$ is compact and $B$ is closed then $A+B$ is closed","For normed vectorspaces ,  if  is compact and  is closed then  is closed","V A,B \subset V A B A+B","I am looking for a 'direct' way to show the following statement: Problem : Let $V$ be a normed vectorspace, show that if $A$ is compact and $B$ is closed then $A+B:= \lbrace a+b \mid a \in A, b \in B \rbrace$ is closed My approach : I am trying to show this statement directly i.e. premise $\implies$ conclusion. This was ridiculously easy for $A,B$ compact $\implies A+B$ compact. I don't seem to have much luck with this one though. Let $(x_n)$ be a convergent sequence in $A+B$ such that $x_n \to x$. If I manage to show that $x \in A+B$ then I am done. It is true that $x_n = a_n + b_n$ for all $n \in \mathbb{N}$ and $(a_n,b_n) \in A \times B$. Luckily I have that $A$ is compact, that means that there exists a subsequence $(a_{n_k})$ of $(a_n)$ such that $a_{n_k} \to a \in A$ thanks to  compact $\equiv $ bound & closed and Bolzano-Weierstrass. Now I fail to make any statement about the sequence $(b_n) \in B$. I know that if $(b_n)$ converges, then thanks to $B$ being closed it follows that the limiting point would also be a member of $B$. However it feels rather vague to me to say that $(b_n)$ converges because $(x_n) \in A + B$ converges. Could someone give me some nudges in the right direction? Or do I need to forfeit the direct approach and try to come up with a contradiction if I assume $x \notin A+B$? This seems to be much harder.","I am looking for a 'direct' way to show the following statement: Problem : Let $V$ be a normed vectorspace, show that if $A$ is compact and $B$ is closed then $A+B:= \lbrace a+b \mid a \in A, b \in B \rbrace$ is closed My approach : I am trying to show this statement directly i.e. premise $\implies$ conclusion. This was ridiculously easy for $A,B$ compact $\implies A+B$ compact. I don't seem to have much luck with this one though. Let $(x_n)$ be a convergent sequence in $A+B$ such that $x_n \to x$. If I manage to show that $x \in A+B$ then I am done. It is true that $x_n = a_n + b_n$ for all $n \in \mathbb{N}$ and $(a_n,b_n) \in A \times B$. Luckily I have that $A$ is compact, that means that there exists a subsequence $(a_{n_k})$ of $(a_n)$ such that $a_{n_k} \to a \in A$ thanks to  compact $\equiv $ bound & closed and Bolzano-Weierstrass. Now I fail to make any statement about the sequence $(b_n) \in B$. I know that if $(b_n)$ converges, then thanks to $B$ being closed it follows that the limiting point would also be a member of $B$. However it feels rather vague to me to say that $(b_n)$ converges because $(x_n) \in A + B$ converges. Could someone give me some nudges in the right direction? Or do I need to forfeit the direct approach and try to come up with a contradiction if I assume $x \notin A+B$? This seems to be much harder.",,"['real-analysis', 'analysis', 'vector-spaces', 'compactness']"
71,Proving that rational numbers are dense,Proving that rational numbers are dense,,"I am trying to show that for any real number a , there exist infinitely many rational numbers m/n with $ |a - m/n| < 1 /n^{2} $ . I've tried to attempt the question by assuming there are finite rational numbers and finding a contradiction.","I am trying to show that for any real number a , there exist infinitely many rational numbers m/n with $ |a - m/n| < 1 /n^{2} $ . I've tried to attempt the question by assuming there are finite rational numbers and finding a contradiction.",,"['analysis', 'rational-numbers']"
72,"$f \in \mathcal{L}^1(\mathbb{R}) \cap \mathcal{C}^1(\mathbb{R}),f' \in \mathcal{L}^1(\mathbb{R}) \Longrightarrow f \in \mathcal{C}_{0}(\mathbb{R})$",,"f \in \mathcal{L}^1(\mathbb{R}) \cap \mathcal{C}^1(\mathbb{R}),f' \in \mathcal{L}^1(\mathbb{R}) \Longrightarrow f \in \mathcal{C}_{0}(\mathbb{R})",I want to show the following theorem: Suppose $f \in \mathcal{L}^1(\mathbb{R}) \cap \mathcal{C}^1(\mathbb{R})$ and $f' \in \mathcal{L}^1(\mathbb{R})$. Then it holds that $f \in \mathcal{C}_{0}(\mathbb{R})$. How can I show that $\lim_{|x| \to \infty} f(x)=0?$,I want to show the following theorem: Suppose $f \in \mathcal{L}^1(\mathbb{R}) \cap \mathcal{C}^1(\mathbb{R})$ and $f' \in \mathcal{L}^1(\mathbb{R})$. Then it holds that $f \in \mathcal{C}_{0}(\mathbb{R})$. How can I show that $\lim_{|x| \to \infty} f(x)=0?$,,"['real-analysis', 'analysis']"
73,Limit of $nx_n$,Limit of,nx_n,"Okay, so I have this problem: Given the sequence $(x_n)_{n\in\mathbb{N}}$ defined by $x_{n+1}=\dfrac{3x_n^2}{(1+x_n)^3-1}$, with $x_1>0$, find $\displaystyle\lim_{n\to\infty}x_n$ and $\displaystyle\lim_{n\to\infty}nx_n$. I've proved that the first limit equals $0$, by proving the sequence is bounded and monotonic, but I have no idea what to do about the second one. Could you give me a hint ?","Okay, so I have this problem: Given the sequence $(x_n)_{n\in\mathbb{N}}$ defined by $x_{n+1}=\dfrac{3x_n^2}{(1+x_n)^3-1}$, with $x_1>0$, find $\displaystyle\lim_{n\to\infty}x_n$ and $\displaystyle\lim_{n\to\infty}nx_n$. I've proved that the first limit equals $0$, by proving the sequence is bounded and monotonic, but I have no idea what to do about the second one. Could you give me a hint ?",,"['sequences-and-series', 'analysis', 'convergence-divergence']"
74,Is this integral finite? (convergent),Is this integral finite? (convergent),,"I came to this problem and I got very curious to know... intuitively I would say this integral is not finite but maybe it is. Let us consider $\mathbb{R}^2$ and only the region $R=\{(x,y)\in \mathbb{R}^2: \, |x|>1, |y|>1\}$. Is the following integral finite? $$\int_{R} \frac{1}{(\max \{x,y\})^2} dxdy.$$ Any ideas? :) Thanks a lot :)","I came to this problem and I got very curious to know... intuitively I would say this integral is not finite but maybe it is. Let us consider $\mathbb{R}^2$ and only the region $R=\{(x,y)\in \mathbb{R}^2: \, |x|>1, |y|>1\}$. Is the following integral finite? $$\int_{R} \frac{1}{(\max \{x,y\})^2} dxdy.$$ Any ideas? :) Thanks a lot :)",,"['calculus', 'real-analysis', 'integration', 'analysis', 'improper-integrals']"
75,Infinite Riemann sum of $x^3$,Infinite Riemann sum of,x^3,Write $$\int_{1}^{3} x^3 dx$$ as a riemann sum. Here is what I thought: $$\Delta(x) = \frac{2}{n}$$ $$f(x) = (\Delta(x)k)^3 = \left(\frac{2k}{n}\right)^3$$ $$I = \int_{1}^{3} x^3dx = \lim_{n \to \infty} \sum_{k=1}^{n} \frac{2}{n}\cdot \left(\frac{2k}{n}\right)^3$$ But the sum evaluates to $4$ instead of $20$ which is the actual value...?,Write $$\int_{1}^{3} x^3 dx$$ as a riemann sum. Here is what I thought: $$\Delta(x) = \frac{2}{n}$$ $$f(x) = (\Delta(x)k)^3 = \left(\frac{2k}{n}\right)^3$$ $$I = \int_{1}^{3} x^3dx = \lim_{n \to \infty} \sum_{k=1}^{n} \frac{2}{n}\cdot \left(\frac{2k}{n}\right)^3$$ But the sum evaluates to $4$ instead of $20$ which is the actual value...?,,"['calculus', 'real-analysis', 'integration', 'analysis', 'riemann-sum']"
76,Covariant derivative and geodesic,Covariant derivative and geodesic,,"Let $f: U \subset \mathbb{R}^2 \rightarrow \mathbb{R}^3$ be a surface patch. Then if we have two vector fields $$X = \sum_i \xi^i \frac{\partial f}{\partial u^i}$$ and $$Y = \sum_i \eta^i \frac{\partial f}{\partial u^i}$$ the covariant derivative is given by $$\nabla_X Y:= \sum_{i,k} \xi^{i} \left( \frac{\partial \eta^k}{\partial u^i} + \sum_j \Gamma_{i,j}^k \eta^j\right)\frac{\partial f}{\partial u^k}$$ Now my question is: If I have a curve $\gamma: I \rightarrow U$ and $c:=f \circ \gamma,$ then my textbook claims that $$\nabla_{c'}c' = \sum_{i,k} \dot{u}^{i}(t) \left( \frac{\partial \dot{u}^{k}(t)}{\partial u^i} + \sum_j \Gamma_{i,j}^k \dot{u}^{j}(t)\right)\frac{\partial f}{\partial u^k}$$ Although I don't know what this function $u$ should actually denote, this equation looks somehow wrong, as $\dot{u}$ is a function of $t$ but is differentiated with respect to $u^{i}$, which does not make sense to me. Could anybody explain this to me? So I have essentially two questions: 1.)  How is $u$ defined? 2.) How does this equation follow from the definition of the covariant derivative and why is the derivative of $\dot{u}$ with respect to $u^{i}$ not zero?","Let $f: U \subset \mathbb{R}^2 \rightarrow \mathbb{R}^3$ be a surface patch. Then if we have two vector fields $$X = \sum_i \xi^i \frac{\partial f}{\partial u^i}$$ and $$Y = \sum_i \eta^i \frac{\partial f}{\partial u^i}$$ the covariant derivative is given by $$\nabla_X Y:= \sum_{i,k} \xi^{i} \left( \frac{\partial \eta^k}{\partial u^i} + \sum_j \Gamma_{i,j}^k \eta^j\right)\frac{\partial f}{\partial u^k}$$ Now my question is: If I have a curve $\gamma: I \rightarrow U$ and $c:=f \circ \gamma,$ then my textbook claims that $$\nabla_{c'}c' = \sum_{i,k} \dot{u}^{i}(t) \left( \frac{\partial \dot{u}^{k}(t)}{\partial u^i} + \sum_j \Gamma_{i,j}^k \dot{u}^{j}(t)\right)\frac{\partial f}{\partial u^k}$$ Although I don't know what this function $u$ should actually denote, this equation looks somehow wrong, as $\dot{u}$ is a function of $t$ but is differentiated with respect to $u^{i}$, which does not make sense to me. Could anybody explain this to me? So I have essentially two questions: 1.)  How is $u$ defined? 2.) How does this equation follow from the definition of the covariant derivative and why is the derivative of $\dot{u}$ with respect to $u^{i}$ not zero?",,"['calculus', 'real-analysis']"
77,The existence of $y$ such that $f(x+y)=f(x)+f(y)$,The existence of  such that,y f(x+y)=f(x)+f(y),"For a real-valued $C^1(\mathbb{R})$ function with $f(0)\neq0$. $f'$ is strictly increasing from $-\infty$ to $\infty$ as $x$ goes from $-\infty$ to $\infty$. Prove that for any $x \neq 0$, there exists $y$ such that $f(x+y)=f(x)+f(y)$. Intuitively I think it must be a $y$ for this convex function such that $$\frac{f(x+y)-f(y)}{x} = \frac{f(x)}{x}$$ by considering its slope. But I don't know how to prove it in a rigorous way. Any idea will be appreciated.","For a real-valued $C^1(\mathbb{R})$ function with $f(0)\neq0$. $f'$ is strictly increasing from $-\infty$ to $\infty$ as $x$ goes from $-\infty$ to $\infty$. Prove that for any $x \neq 0$, there exists $y$ such that $f(x+y)=f(x)+f(y)$. Intuitively I think it must be a $y$ for this convex function such that $$\frac{f(x+y)-f(y)}{x} = \frac{f(x)}{x}$$ by considering its slope. But I don't know how to prove it in a rigorous way. Any idea will be appreciated.",,"['calculus', 'real-analysis', 'analysis']"
78,Is the polynomial a zero polynomial?,Is the polynomial a zero polynomial?,,"Let $p(x)$ be a polynomial over $\mathbb{R}$ with $deg[p(x)]\leqslant n$. If $p(1)=p(2)=\cdots = p(n+1)=0$, then will the polynomial be necessarily a zero polynomial? i.e., if a polynomial of degree $n$ takes the value zero at $n+1$ distinct points, will the polynomial be necessarily a zero polynomial?","Let $p(x)$ be a polynomial over $\mathbb{R}$ with $deg[p(x)]\leqslant n$. If $p(1)=p(2)=\cdots = p(n+1)=0$, then will the polynomial be necessarily a zero polynomial? i.e., if a polynomial of degree $n$ takes the value zero at $n+1$ distinct points, will the polynomial be necessarily a zero polynomial?",,"['analysis', 'polynomials']"
79,A problem with equality in a inequality for convex function,A problem with equality in a inequality for convex function,,"Let $f:\rightarrow \mathbb R$ be a convex function on a convex subset $D$ of linear space $X$. Assume that for some pairwise disjoit $x_1,x_2,x_3\in D$  and some $t_1,t_2,t_3\in (0,1)$ such that $t_1+t_2+t_3=1$ the following equality holds: $$ f(t_1x_1+t_2x_2+t_3x_3)=t_1f(x_1)+t_2f(x_2)+t_3f(x_3). $$ Is it true that  $$ f(s_1x_1+s_2x_2+s_3x_3)=s_1f(x_1)+s_2f(x_2)+s_3f(x_3) $$ for all $s_1,s_2,s_3\in (0,1)$ such that $s_1+s_2+s_3=1$? What does the last condition geometrically mean?","Let $f:\rightarrow \mathbb R$ be a convex function on a convex subset $D$ of linear space $X$. Assume that for some pairwise disjoit $x_1,x_2,x_3\in D$  and some $t_1,t_2,t_3\in (0,1)$ such that $t_1+t_2+t_3=1$ the following equality holds: $$ f(t_1x_1+t_2x_2+t_3x_3)=t_1f(x_1)+t_2f(x_2)+t_3f(x_3). $$ Is it true that  $$ f(s_1x_1+s_2x_2+s_3x_3)=s_1f(x_1)+s_2f(x_2)+s_3f(x_3) $$ for all $s_1,s_2,s_3\in (0,1)$ such that $s_1+s_2+s_3=1$? What does the last condition geometrically mean?",,"['analysis', 'convex-analysis']"
80,"Is $[0,1]^2 \setminus \{(a,b)\}$ connected?",Is  connected?,"[0,1]^2 \setminus \{(a,b)\}","I am pretty sure that this set is in fact connected but I am struggling to see how to prove it, it is simple to see that $[0,1] \setminus \{x\}$ is disconnected but I can't see how to relate techniques used in that example to this one.","I am pretty sure that this set is in fact connected but I am struggling to see how to prove it, it is simple to see that $[0,1] \setminus \{x\}$ is disconnected but I can't see how to relate techniques used in that example to this one.",,"['analysis', 'connectedness']"
81,How to construct a smooth function with compact support satisfying $f(x)+f(x^{-1})=1$,How to construct a smooth function with compact support satisfying,f(x)+f(x^{-1})=1,"How to construct a smooth function with compact support satisfying $$ f(x)+f(x^{-1})=1 $$ For example, let $$ g(x)=\left\{\begin{array}{ll} 0,&\mbox{if $x\leq 0$},\\ \frac{1}{1+e^{\frac{1}{x}-\frac{1}{1-x}}},&\mbox{if $0<x<1$},\\ 1,&\mbox{if $x\geq 1$}, \end{array} \right. $$ Then $$ G(x)=g\left(\frac{x-a}{b-a}\right)g\left(\frac{d-x}{d-c}\right) $$ is a smooth function which equals 1 on [b,c] and vanishes outside (a,d). However, I don't know how to modify it to satisfy $$ G(x)+G(x^{-1})=1 $$","How to construct a smooth function with compact support satisfying $$ f(x)+f(x^{-1})=1 $$ For example, let $$ g(x)=\left\{\begin{array}{ll} 0,&\mbox{if $x\leq 0$},\\ \frac{1}{1+e^{\frac{1}{x}-\frac{1}{1-x}}},&\mbox{if $0<x<1$},\\ 1,&\mbox{if $x\geq 1$}, \end{array} \right. $$ Then $$ G(x)=g\left(\frac{x-a}{b-a}\right)g\left(\frac{d-x}{d-c}\right) $$ is a smooth function which equals 1 on [b,c] and vanishes outside (a,d). However, I don't know how to modify it to satisfy $$ G(x)+G(x^{-1})=1 $$",,['analysis']
82,"Irrational numbers in $[0,1]$ [closed]",Irrational numbers in  [closed],"[0,1]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Why irrational numbers in interval $[0,1]$ can't be written as countable union of closed sets of $\mathbb R$? My idea is the following: Let: $$S=\mathbb Q^c\cap[0,1]=\cup_{i=1}^{\infty}[a_i,b_i]$$  We can suppose every $[a_i,b_i]$ is disjoint from others and $[a_i,b_i]\subset[0,1]$. Now define: $$X=\{a_i,b_i\,;i\in \mathbb N\}$$ Then $X$ is close in $S$. because it's complement in $S$ is union of open set. So, $X$ is complete. $X$ is perfect. Let $\epsilon \gt0$ be given. Then: $(a_i-\epsilon,a_i)$ contain one of $b_j$. Similarly, every $b_i$ is a limit point of $X$. But we know every complete and perfect metric space is uncountable. So $X$ is uncountable. contradict.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Why irrational numbers in interval $[0,1]$ can't be written as countable union of closed sets of $\mathbb R$? My idea is the following: Let: $$S=\mathbb Q^c\cap[0,1]=\cup_{i=1}^{\infty}[a_i,b_i]$$  We can suppose every $[a_i,b_i]$ is disjoint from others and $[a_i,b_i]\subset[0,1]$. Now define: $$X=\{a_i,b_i\,;i\in \mathbb N\}$$ Then $X$ is close in $S$. because it's complement in $S$ is union of open set. So, $X$ is complete. $X$ is perfect. Let $\epsilon \gt0$ be given. Then: $(a_i-\epsilon,a_i)$ contain one of $b_j$. Similarly, every $b_i$ is a limit point of $X$. But we know every complete and perfect metric space is uncountable. So $X$ is uncountable. contradict.",,"['analysis', 'metric-spaces']"
83,Need formal mathematical definition of this concept,Need formal mathematical definition of this concept,,"Assume we have a function $y=f(x)$ that is $\textit{C}^\infty$ and the function has a number of local maximums. Assume there are $k$ such maximums $\{m_1, m_2, m_3, \ldots , m_k\}$ where $f'(m_i)=0$. If I performed a gradient ascent algorithm for any point $x$ where $f(x)$ is defined, it would get to one of these local maximums $m_i$. In this way, every point $x$ where $f(x)$ is defined, it belongs to a certain local maximum (via gradient ascent). Is there a formal mathematical treatment of this concept that I just described? I have reduced it to two-dimensions, but I am looking to apply it to three dimensional surfaces.","Assume we have a function $y=f(x)$ that is $\textit{C}^\infty$ and the function has a number of local maximums. Assume there are $k$ such maximums $\{m_1, m_2, m_3, \ldots , m_k\}$ where $f'(m_i)=0$. If I performed a gradient ascent algorithm for any point $x$ where $f(x)$ is defined, it would get to one of these local maximums $m_i$. In this way, every point $x$ where $f(x)$ is defined, it belongs to a certain local maximum (via gradient ascent). Is there a formal mathematical treatment of this concept that I just described? I have reduced it to two-dimensions, but I am looking to apply it to three dimensional surfaces.",,"['calculus', 'general-topology', 'analysis', 'differential-geometry', 'algorithms']"
84,there doesn't exist any sequence of polynomials which converge to $\sin x$ uniformly on $\mathbb{R}$,there doesn't exist any sequence of polynomials which converge to  uniformly on,\sin x \mathbb{R},"Show that there doesn't exist any sequence of polynomials which converge to $\sin x$ uniformly on $\mathbb{R}$. Suppose there is a sequence of polynomials $\{p_n\}$ which converges to $\sin x$ uniformly on $\mathbb{R}$. Then given $\epsilon \gt 0$, there exists a $n_0 \in \mathbb{N}$  such that $\forall n \ge n_0, |p_n(x)-\sin x|\lt \epsilon.$ In particular for $\epsilon=1$, $|p_n(x)|\lt 2$ for all $n \ge n_0$.This makes $p_{n_0}(x)$ a constant polynomial say $c=p_{n_0}(x)$. Then I need to find a contradiction somehow. How do I do that?? Thanks for the help!!","Show that there doesn't exist any sequence of polynomials which converge to $\sin x$ uniformly on $\mathbb{R}$. Suppose there is a sequence of polynomials $\{p_n\}$ which converges to $\sin x$ uniformly on $\mathbb{R}$. Then given $\epsilon \gt 0$, there exists a $n_0 \in \mathbb{N}$  such that $\forall n \ge n_0, |p_n(x)-\sin x|\lt \epsilon.$ In particular for $\epsilon=1$, $|p_n(x)|\lt 2$ for all $n \ge n_0$.This makes $p_{n_0}(x)$ a constant polynomial say $c=p_{n_0}(x)$. Then I need to find a contradiction somehow. How do I do that?? Thanks for the help!!",,"['real-analysis', 'analysis']"
85,Interesting Gradient problem? Don't know how to write it?,Interesting Gradient problem? Don't know how to write it?,,"if $p=q+\varepsilon v$, where $v=\dfrac{\nabla f(q)}{\|\nabla f(q)\|}$. $f(q)=0$.  $\nabla f(x) $ is not $0$ on the domain. How to prove there exists $\varepsilon_1>0$ such that if $\varepsilon_1>\varepsilon>0$, then $f(p)>0$? I know gradient direction is the fastest increasing direction, so by the condition given, it seems obvious. How to write it rigorously?","if $p=q+\varepsilon v$, where $v=\dfrac{\nabla f(q)}{\|\nabla f(q)\|}$. $f(q)=0$.  $\nabla f(x) $ is not $0$ on the domain. How to prove there exists $\varepsilon_1>0$ such that if $\varepsilon_1>\varepsilon>0$, then $f(p)>0$? I know gradient direction is the fastest increasing direction, so by the condition given, it seems obvious. How to write it rigorously?",,"['calculus', 'analysis', 'partial-differential-equations']"
86,Weighted sum of cosines,Weighted sum of cosines,,"Consider $$f(x) = \sum_{k=1}^\infty \cos(kx) k^\alpha.$$ The first question is: does this have a name (Mathematica gives it as a sum of polylogs of complex arguments, but this seems unnatural). Also, does the logarithmic derivative have a name and/or nice properties? (The exponent $\alpha$ in my applications is a negative real number, but I will take what I can get...)","Consider $$f(x) = \sum_{k=1}^\infty \cos(kx) k^\alpha.$$ The first question is: does this have a name (Mathematica gives it as a sum of polylogs of complex arguments, but this seems unnatural). Also, does the logarithmic derivative have a name and/or nice properties? (The exponent $\alpha$ in my applications is a negative real number, but I will take what I can get...)",,"['analysis', 'special-functions', 'fourier-series']"
87,Finding Factorial using Integral Definition,Finding Factorial using Integral Definition,,"$n! = \int_{0}^{\infty} {e}^{-x}{x}^{n} \,dx$ How can we find $400!$? $400! = \int_{0}^{\infty} {e}^{-x}{x}^{400} \,dx$ Integration by parts is way too complicated, what are the other options?","$n! = \int_{0}^{\infty} {e}^{-x}{x}^{n} \,dx$ How can we find $400!$? $400! = \int_{0}^{\infty} {e}^{-x}{x}^{400} \,dx$ Integration by parts is way too complicated, what are the other options?",,"['calculus', 'integration', 'analysis', 'definite-integrals', 'factorial']"
88,Curves in $\mathbb R^n$ and boundary points,Curves in  and boundary points,\mathbb R^n,"Thinking of a curve as a 1-dimensional continuum, intuitively one would think that a curve can never have this property. Perhaps intuition is correct and this question is somewhat naive, but here it is anyway: Can we find a curve or trajectory $A \subset \mathbb R^n$ such that there is at least one point on the curve that is not a boundary point (under the Euclidean topology)? Infinitely many such points? The entirety of the curve? I feel that such a curve would probably have a fractal construction in some sense, but I can't fathom what it would look like.","Thinking of a curve as a 1-dimensional continuum, intuitively one would think that a curve can never have this property. Perhaps intuition is correct and this question is somewhat naive, but here it is anyway: Can we find a curve or trajectory $A \subset \mathbb R^n$ such that there is at least one point on the curve that is not a boundary point (under the Euclidean topology)? Infinitely many such points? The entirety of the curve? I feel that such a curve would probably have a fractal construction in some sense, but I can't fathom what it would look like.",,"['general-topology', 'analysis', 'curves']"
89,Constructing a countable dense subset of a totally bounded set,Constructing a countable dense subset of a totally bounded set,,"Given a metric space $(X,d)$, and (non-empty) totally bounded set $E$ in $X$, is it possible to construct $D \subseteq E$ which is countable and dense? I feel that this should definitely be possible. Since $E$ is totally bounded, I suspect we need to pick the $\epsilon$-balls that cover $E$ in such a way that the centres of them are dense (and assumed to be in $E$). I've thought about picking $\epsilon>0$ so that every $\epsilon$-ball contains the centre of another $\epsilon$-ball, but this does not guarantee the density we require. I'm not sure which other $\epsilon$ to try.","Given a metric space $(X,d)$, and (non-empty) totally bounded set $E$ in $X$, is it possible to construct $D \subseteq E$ which is countable and dense? I feel that this should definitely be possible. Since $E$ is totally bounded, I suspect we need to pick the $\epsilon$-balls that cover $E$ in such a way that the centres of them are dense (and assumed to be in $E$). I've thought about picking $\epsilon>0$ so that every $\epsilon$-ball contains the centre of another $\epsilon$-ball, but this does not guarantee the density we require. I'm not sure which other $\epsilon$ to try.",,"['general-topology', 'analysis', 'metric-spaces']"
90,Approximating by smooth functions with compact support.,Approximating by smooth functions with compact support.,,"Consider a bounded domain $D \subset \mathbb{R}^n$ and the Sobolev space $H^1_{0}(D):=\overline{C_c^{\infty}(D)}^{W^{1, 2}(D)}$. Further, consider a Sobolev function which happens to be smooth: $u\in H^{1}_{0}(D)\cap C^{\infty}(D)$ . Now remove some point $a\in D$ and restrict our function $u$ by removing the point $a$ of the domain of definition, i.e. consider the function $u_{\vert D-\lbrace{a\rbrace}}$. I'm wondering if the restricted function is still a sobolev space, i.e. $u_{\vert D-\lbrace{a\rbrace}}\in H^{1}_0(D-\lbrace{ a\rbrace})$? Or to put it in an equivalent way: Is it possible to approximate $u_{\vert D-\lbrace{a\rbrace}}$ with respect to $\Vert\cdot \Vert_{W^{1,2}(D)}$ by smooth functions with compact support? I appreciate any help! Best regards","Consider a bounded domain $D \subset \mathbb{R}^n$ and the Sobolev space $H^1_{0}(D):=\overline{C_c^{\infty}(D)}^{W^{1, 2}(D)}$. Further, consider a Sobolev function which happens to be smooth: $u\in H^{1}_{0}(D)\cap C^{\infty}(D)$ . Now remove some point $a\in D$ and restrict our function $u$ by removing the point $a$ of the domain of definition, i.e. consider the function $u_{\vert D-\lbrace{a\rbrace}}$. I'm wondering if the restricted function is still a sobolev space, i.e. $u_{\vert D-\lbrace{a\rbrace}}\in H^{1}_0(D-\lbrace{ a\rbrace})$? Or to put it in an equivalent way: Is it possible to approximate $u_{\vert D-\lbrace{a\rbrace}}$ with respect to $\Vert\cdot \Vert_{W^{1,2}(D)}$ by smooth functions with compact support? I appreciate any help! Best regards",,"['analysis', 'functional-analysis', 'sobolev-spaces']"
91,Rudin Principles of mathematical analysis p307,Rudin Principles of mathematical analysis p307,,"""For if $ A=\bigcup A^{'}_{n}$ with $A^{'}_{n} \in M_F(\mu)$, write $A_1=A^{'}_{1} $, and $$ A_n=(A^{'}_1\cup ...\cup A^{'}_n)-(A^{'}_n \cup ... \cup A^{'}_{n-1})$$ $(n=2,3,4,...)$. Then  $$ A=\bigcup_{n=1}^{\infty}A_n$$ I can't understand why $A_n$ is expressed like the above? Should the correct one be  $$ A_n=A^{'}_n-(A^{'}_1 \cup ... \cup A^{'}_{n-1})$$","""For if $ A=\bigcup A^{'}_{n}$ with $A^{'}_{n} \in M_F(\mu)$, write $A_1=A^{'}_{1} $, and $$ A_n=(A^{'}_1\cup ...\cup A^{'}_n)-(A^{'}_n \cup ... \cup A^{'}_{n-1})$$ $(n=2,3,4,...)$. Then  $$ A=\bigcup_{n=1}^{\infty}A_n$$ I can't understand why $A_n$ is expressed like the above? Should the correct one be  $$ A_n=A^{'}_n-(A^{'}_1 \cup ... \cup A^{'}_{n-1})$$",,['analysis']
92,Showing that if the $n$th derivative of a function is bounded then it is real analytic,Showing that if the th derivative of a function is bounded then it is real analytic,n,"I reproduce from my lecture notes: Suppose $f$ is $C^\infty$ on $[a,b]$ with $$\left|f^{(n)}(x)\right|\leqslant M~~\text{for all}~~x\in(a,b).$$ Then $f$ is real analytic in $[a,b]$ . Proof . Fix $x_0\in[a,b]$ .  Then for each $x$ in $[a,b]$ the error after truncating the series at the $n$ th term is given, for some $c\in[a,b]$ , by $$\left|\frac{f^{(n+1)}(c)}{(n+1)!}(x-x_0)^{n+1}\right|\leqslant \frac{M}{(n+1)!}\left|b-a\right|^{n+1}.\tag{$*$}$$ This tends to zero as $n\to\infty$ and so $f$ is real analytic in $[a,b]$ . Question: how did my lecturer deduce the inequality $(*)$ from $\left|f^{(n)}(x)\right|\leqslant M$ ? I am searching for different versions of this proof but they all somehow fail to explain this step, assuming that is is obvious. Any (fairly elementary) explanation is welcomed. Thanks.","I reproduce from my lecture notes: Suppose is on with Then is real analytic in . Proof . Fix .  Then for each in the error after truncating the series at the th term is given, for some , by This tends to zero as and so is real analytic in . Question: how did my lecturer deduce the inequality from ? I am searching for different versions of this proof but they all somehow fail to explain this step, assuming that is is obvious. Any (fairly elementary) explanation is welcomed. Thanks.","f C^\infty [a,b] \left|f^{(n)}(x)\right|\leqslant M~~\text{for all}~~x\in(a,b). f [a,b] x_0\in[a,b] x [a,b] n c\in[a,b] \left|\frac{f^{(n+1)}(c)}{(n+1)!}(x-x_0)^{n+1}\right|\leqslant \frac{M}{(n+1)!}\left|b-a\right|^{n+1}.\tag{*} n\to\infty f [a,b] (*) \left|f^{(n)}(x)\right|\leqslant M","['real-analysis', 'analysis', 'power-series', 'taylor-expansion']"
93,Differentiable Strictly Convex Function on Interval,Differentiable Strictly Convex Function on Interval,,"Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be a differentiable, strictly convex function. Let $I\subset \mathbb{R}$ be a closed, bounded interval such that $f'(x) \neq 0$ on $I$. Is $f$ strongly convex on $I$? Note: If $f$ is twice differentiable the answer is yes.","Let $f:\mathbb{R}\rightarrow \mathbb{R}$ be a differentiable, strictly convex function. Let $I\subset \mathbb{R}$ be a closed, bounded interval such that $f'(x) \neq 0$ on $I$. Is $f$ strongly convex on $I$? Note: If $f$ is twice differentiable the answer is yes.",,"['calculus', 'analysis', 'derivatives', 'convex-analysis']"
94,What is the value of $a$ so that this condition holds?,What is the value of  so that this condition holds?,a,"Let $f(x) \colon= x-x^2$, $g(x) \colon= ax$. Determine the value of $a$ so that the region above the graph of $g$ and below the graph of $f$ has area equal to $9/2$. Here $f(x) - g(x) = (1-a)x - x^2 = x((1-a) - x) = 0$ if and only if $x = 0$ or $x = 1-a$. So the required area is $$ \int_{0}^{1-a} ( (1-a)x - x^2 ) \ dx = \frac{(1-a)^2}{2} - \frac{(1-a)^3}{3} = \frac{(1-a)^2}{6} ( 3 - 2(1-a) ) =  \frac{(1-a)^2}{6} (2a + 1) =  \frac{(1-a)^2 (2a+1) }{6} =  \frac{(a^2 -2a +1 ) (2a+1)}{6} = \frac{2a^3 - 3a^2 + 1}{6} = \frac{9}{2},  $$  and this last equation simplifies to $$ 2a^3 - 3a^2 - 26 = 0.$$  Is the process so far correct? And if so, then how to solve this last equation for $a$?","Let $f(x) \colon= x-x^2$, $g(x) \colon= ax$. Determine the value of $a$ so that the region above the graph of $g$ and below the graph of $f$ has area equal to $9/2$. Here $f(x) - g(x) = (1-a)x - x^2 = x((1-a) - x) = 0$ if and only if $x = 0$ or $x = 1-a$. So the required area is $$ \int_{0}^{1-a} ( (1-a)x - x^2 ) \ dx = \frac{(1-a)^2}{2} - \frac{(1-a)^3}{3} = \frac{(1-a)^2}{6} ( 3 - 2(1-a) ) =  \frac{(1-a)^2}{6} (2a + 1) =  \frac{(1-a)^2 (2a+1) }{6} =  \frac{(a^2 -2a +1 ) (2a+1)}{6} = \frac{2a^3 - 3a^2 + 1}{6} = \frac{9}{2},  $$  and this last equation simplifies to $$ 2a^3 - 3a^2 - 26 = 0.$$  Is the process so far correct? And if so, then how to solve this last equation for $a$?",,"['calculus', 'integration', 'analysis', 'definite-integrals', 'area']"
95,"Let $(X,μ)$ be a measure space. Find a necessary and sufficient condition on $(X,μ)$ that $L_q(E) ⊂ L_p(E)$ for all $1 ≤ p < q ≤ ∞.$",Let  be a measure space. Find a necessary and sufficient condition on  that  for all,"(X,μ) (X,μ) L_q(E) ⊂ L_p(E) 1 ≤ p < q ≤ ∞.","Let $(X,μ)$ be a measure space. Find a necessary and sufficient condition on $(X,μ)$ that $L_q(E) ⊂ L_p(E)$ for all $1 ≤ p < q ≤ ∞.$ I want to say that the condition is that $E$ is finite.  This is clearly sufficient.  Also it is clearly necessary for normal lebesgue measure.  The counter example for arbitrary measure I want to use is $f(x) = \sum_{n = 1}^\infty \frac{1}{n^x}1_{E_n}(x)\mu(E_i)^{-1}$ where $xp < 1$ but $xq > 1$.  Then it is p-test from calculus and done. But this depends on writing $E = \sum_{n = 1}^\infty E_i$ with the $E_i$ finite and disjoint.  Can I assume this?  Thanks.  If not, is there another approach. Actually I don't think what I wrote made much sense since then $f$ is not measurable.","Let $(X,μ)$ be a measure space. Find a necessary and sufficient condition on $(X,μ)$ that $L_q(E) ⊂ L_p(E)$ for all $1 ≤ p < q ≤ ∞.$ I want to say that the condition is that $E$ is finite.  This is clearly sufficient.  Also it is clearly necessary for normal lebesgue measure.  The counter example for arbitrary measure I want to use is $f(x) = \sum_{n = 1}^\infty \frac{1}{n^x}1_{E_n}(x)\mu(E_i)^{-1}$ where $xp < 1$ but $xq > 1$.  Then it is p-test from calculus and done. But this depends on writing $E = \sum_{n = 1}^\infty E_i$ with the $E_i$ finite and disjoint.  Can I assume this?  Thanks.  If not, is there another approach. Actually I don't think what I wrote made much sense since then $f$ is not measurable.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
96,Question on $x$-section of measurable rectangle in product measure space $X \times Y$,Question on -section of measurable rectangle in product measure space,x X \times Y,"I'm reviewing my analysis notes.  We have that $(X, \Sigma, \mu)$ and $(Y, \tau, \nu)$ are complete measure spaces.  We are considering the product measure space $(X \times Y, \Sigma(\lambda^{*}), \lambda)$. Basically, if $R = A \times B$, where $A \in \Sigma$ and $B \in \tau$, we define the $x$-section of $R$ as $$R_{x} = \{ y \in B \mid (x, y) \in A \times B \}.$$ If we consider $F$ to be a union of these $R$'s (i.e., $F = \bigcup \limits_{j = 1}^{\infty} R_{j}$), then my professor defines $F_{x} = \bigcup \limits_{j = 1}^{\infty} ((A_{j} \times B_{j})_{x})$. Now, I think $\bigcup \limits_{j = 1}^{\infty} ((A_{j} \times B_{j})_{x})$ should be equal to $\bigcup \limits_{j = 1}^{\infty} B_{j}$, but in the notes, we have it equal to $\bigcup \limits_{x \in A_{j}} B_{j}$.  I'm not even sure what this union over $x \in A_{j}$ means in this context, let alone why it is equal to our $x$-section.  Any help would be appreciated.","I'm reviewing my analysis notes.  We have that $(X, \Sigma, \mu)$ and $(Y, \tau, \nu)$ are complete measure spaces.  We are considering the product measure space $(X \times Y, \Sigma(\lambda^{*}), \lambda)$. Basically, if $R = A \times B$, where $A \in \Sigma$ and $B \in \tau$, we define the $x$-section of $R$ as $$R_{x} = \{ y \in B \mid (x, y) \in A \times B \}.$$ If we consider $F$ to be a union of these $R$'s (i.e., $F = \bigcup \limits_{j = 1}^{\infty} R_{j}$), then my professor defines $F_{x} = \bigcup \limits_{j = 1}^{\infty} ((A_{j} \times B_{j})_{x})$. Now, I think $\bigcup \limits_{j = 1}^{\infty} ((A_{j} \times B_{j})_{x})$ should be equal to $\bigcup \limits_{j = 1}^{\infty} B_{j}$, but in the notes, we have it equal to $\bigcup \limits_{x \in A_{j}} B_{j}$.  I'm not even sure what this union over $x \in A_{j}$ means in this context, let alone why it is equal to our $x$-section.  Any help would be appreciated.",,"['analysis', 'measure-theory']"
97,Distributions with support of the form $\left\lbrace x \right\rbrace$,Distributions with support of the form,\left\lbrace x \right\rbrace,Doing some calculations with Distributions I came up with the following theorem: THEOREM: Let $O \subseteq \mathbb{R^d}$ be an open subset and $x \in O$. Suppose $T \in \mathcal{D}'(O)$ with $\operatorname{supp}T \subseteq \left\lbrace x \right\rbrace$. Then there exists some $n \in \mathbb{N}_0$ and constants $c_{\alpha} \in \mathbb{C}$ with $|\alpha| \leq n$ such that   \begin{align*} T=\sum_{|\alpha| \leq n} c_{\alpha} \partial^{\alpha} \delta_{x}. \end{align*} Does anybody know where I can find a proof of this?,Doing some calculations with Distributions I came up with the following theorem: THEOREM: Let $O \subseteq \mathbb{R^d}$ be an open subset and $x \in O$. Suppose $T \in \mathcal{D}'(O)$ with $\operatorname{supp}T \subseteq \left\lbrace x \right\rbrace$. Then there exists some $n \in \mathbb{N}_0$ and constants $c_{\alpha} \in \mathbb{C}$ with $|\alpha| \leq n$ such that   \begin{align*} T=\sum_{|\alpha| \leq n} c_{\alpha} \partial^{\alpha} \delta_{x}. \end{align*} Does anybody know where I can find a proof of this?,,"['analysis', 'distribution-theory']"
98,$\lim_{x\to 0+}\ln(x)\cdot x = 0$ by boundedness of $\ln(x)\cdot x$,by boundedness of,\lim_{x\to 0+}\ln(x)\cdot x = 0 \ln(x)\cdot x,"I saw a proof that  $$  \lim_{x\to 0} \ln|x|\cdot x = 0 $$ where is is argued that for $x \in (0,1)$ we have $$  | \ln(x) x | = \left| \int_1^x x/t ~\mathrm d t \right|  = \left| \int_x^1 x/t ~\mathrm d t \right| \le \left|\int_x^1 1 dt\right| = |1 - x| \le 1 $$ and therefore the result follows, but why should the fact that $|\ln(x)x|$ is bounded imply it converges to zero?","I saw a proof that  $$  \lim_{x\to 0} \ln|x|\cdot x = 0 $$ where is is argued that for $x \in (0,1)$ we have $$  | \ln(x) x | = \left| \int_1^x x/t ~\mathrm d t \right|  = \left| \int_x^1 x/t ~\mathrm d t \right| \le \left|\int_x^1 1 dt\right| = |1 - x| \le 1 $$ and therefore the result follows, but why should the fact that $|\ln(x)x|$ is bounded imply it converges to zero?",,"['calculus', 'real-analysis', 'analysis']"
99,Test for convergence $\sum_{n = 2}^\infty \frac{1}{(n+1)\ln^2(n+1)}$ [duplicate],Test for convergence  [duplicate],\sum_{n = 2}^\infty \frac{1}{(n+1)\ln^2(n+1)},This question already has an answer here : Convergence of series involving in iterated logarithms $\sum \frac{1}{n(\log n)^{\alpha_1}\cdots (\log^k(n))^{\alpha_k} }$ (1 answer) Closed 9 years ago . Test for convergence $$\sum_{n = 2}^\infty \frac{1}{(n+1)\ln^2(n+1)}$$ Here's my attempt! I decided to use the integral test for this. $$\frac{1}{(n+1)\ln^2(n+1)} > \frac{1}{(n+1)^2\ln^2(n+1)}$$ Set $f(x) = \dfrac{1}{x\ln(x)}$ and let $x = (n + 1)^2$ $$\int_9^\infty\frac{1}{x\ln(x)}dx = \int_{\ln(9)}^\infty\frac{du}{u} = \ln(u)_{\ln(9)}^\infty = \infty$$,This question already has an answer here : Convergence of series involving in iterated logarithms $\sum \frac{1}{n(\log n)^{\alpha_1}\cdots (\log^k(n))^{\alpha_k} }$ (1 answer) Closed 9 years ago . Test for convergence $$\sum_{n = 2}^\infty \frac{1}{(n+1)\ln^2(n+1)}$$ Here's my attempt! I decided to use the integral test for this. $$\frac{1}{(n+1)\ln^2(n+1)} > \frac{1}{(n+1)^2\ln^2(n+1)}$$ Set $f(x) = \dfrac{1}{x\ln(x)}$ and let $x = (n + 1)^2$ $$\int_9^\infty\frac{1}{x\ln(x)}dx = \int_{\ln(9)}^\infty\frac{du}{u} = \ln(u)_{\ln(9)}^\infty = \infty$$,,"['real-analysis', 'analysis']"
