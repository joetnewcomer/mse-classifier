,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Divergence rate of harmonic series on $\Bbb Z^d$,Divergence rate of harmonic series on,\Bbb Z^d,"We know that on $\Bbb Z$ we have that $\sum_{k = -N}^{N} \frac{1}{|k|}$ diverges at a rate of $\log(N)$ . I'm wondering if there is a similar identity for the rate of divergence of the series $$\sum_{k \in [-N,N]^d \cap \Bbb Z^d} \frac{1}{|k|^d}.$$ I'm not really sure how to even start with such a problem. We know that by the Euler-Maclaurin formulas, we can link the divergence of the above sum to the integral $$ \int_{(0,\infty)} \! \rho^{-d} \rho^{d-1} \ \mathrm{d} \rho = \int_{(0,\infty)} \! \rho^{-1} \ \mathrm{d} \rho.$$ Where the last integral is ""related"" in some sense to the Harmonic series. My hope now is that the sum of $|k|^{-d}$ on $\Bbb Z^d$ is also divergent at a rate $\log(N)$ , but I don't know how to make any of this rigorous. Can anyone help me with this?","We know that on we have that diverges at a rate of . I'm wondering if there is a similar identity for the rate of divergence of the series I'm not really sure how to even start with such a problem. We know that by the Euler-Maclaurin formulas, we can link the divergence of the above sum to the integral Where the last integral is ""related"" in some sense to the Harmonic series. My hope now is that the sum of on is also divergent at a rate , but I don't know how to make any of this rigorous. Can anyone help me with this?","\Bbb Z \sum_{k = -N}^{N} \frac{1}{|k|} \log(N) \sum_{k \in [-N,N]^d \cap \Bbb Z^d} \frac{1}{|k|^d}.  \int_{(0,\infty)} \! \rho^{-d} \rho^{d-1} \ \mathrm{d} \rho = \int_{(0,\infty)} \! \rho^{-1} \ \mathrm{d} \rho. |k|^{-d} \Bbb Z^d \log(N)","['real-analysis', 'analysis']"
1,connected set of sum of upper semi continuous function,connected set of sum of upper semi continuous function,,"Let $C(X)$ :space of continuous functions on a compact space. Consider $f$ and $g :C(X)\rightarrow \mathbb{R}$ are upper semi continuous. suppose  for every $T\in C(X)$ set of $f(T)$ and  set $(f+g)(T)$ are closed interval(connected set).Can we say set of $g(T)$ is closed interval(connected set),as well? If Not,under which condition we have it.","Let :space of continuous functions on a compact space. Consider and are upper semi continuous. suppose  for every set of and  set are closed interval(connected set).Can we say set of is closed interval(connected set),as well? If Not,under which condition we have it.",C(X) f g :C(X)\rightarrow \mathbb{R} T\in C(X) f(T) (f+g)(T) g(T),"['real-analysis', 'functional-analysis', 'analysis', 'continuity', 'harmonic-analysis']"
2,Condition of being a measure,Condition of being a measure,,"I'm working on a measure theory problem and I'm completely stumped. I'm trying to find out for which integers $j$ , $\mu$ will be a measure on $(\mathbb Z_+, \mathcal P(\mathbb Z_+))$ where $\mu$ is defined to be: $$ \mu(E)= \begin{cases}  \sum_{n\in E}n^j&\text{if}\, c(E)< \infty\\  \infty&\text{if}\, c(E) = \infty\\ \end{cases} $$ * $c$ is the counting measure here. I think I can simply consider finite sets or countable union of disjoint sets { ${A_n}$ } $_{n=1}^\infty$ where for some $N$ , $A_n = \varnothing$ where $n\geq N$ . Satisfying $\mu(\varnothing)$ = $0$ is trivial, and since we are on $\mathbb Z_+$ , $\mu(E) \geq 0$ for every $E \in \mathcal P(\mathbb Z_+)$ . I think countable additivity would impose some condition on $j$ , but not really sure of the point of attack here. Any help is appreciated!","I'm working on a measure theory problem and I'm completely stumped. I'm trying to find out for which integers , will be a measure on where is defined to be: * is the counting measure here. I think I can simply consider finite sets or countable union of disjoint sets { } where for some , where . Satisfying = is trivial, and since we are on , for every . I think countable additivity would impose some condition on , but not really sure of the point of attack here. Any help is appreciated!","j \mu (\mathbb Z_+, \mathcal P(\mathbb Z_+)) \mu 
\mu(E)=
\begin{cases}
 \sum_{n\in E}n^j&\text{if}\, c(E)< \infty\\
 \infty&\text{if}\, c(E) = \infty\\
\end{cases}
 c {A_n} _{n=1}^\infty N A_n = \varnothing n\geq N \mu(\varnothing) 0 \mathbb Z_+ \mu(E) \geq 0 E \in \mathcal P(\mathbb Z_+) j","['real-analysis', 'analysis', 'measure-theory']"
3,Calculate powers of sums,Calculate powers of sums,,"Assuming we have an unital associative complex algebra with generatorn $a_i,b_i$, $i_1,...,n$ such that they anticommute, that is $$a_ia_j=-a_ja_i,\quad b_ib_j=-b_jb_i,\quad a_ib_j=-b_ja_i$$. Consider the element $$F_n:=\sum_{i=1}^n2a_ib_i.$$ Is it possible to give, for fixed $n$, expanded expressions for $F_n^k$ for $k=2,...,n$?  Considering the case $n=2$ and using that powers of the generator vanishes as they anticommute we get $$F_2^2=4a_1b_1a_2b_2+4a_2b_2a_1b_1=8a_1b_1a_2b_2.$$ But what happens in general?","Assuming we have an unital associative complex algebra with generatorn $a_i,b_i$, $i_1,...,n$ such that they anticommute, that is $$a_ia_j=-a_ja_i,\quad b_ib_j=-b_jb_i,\quad a_ib_j=-b_ja_i$$. Consider the element $$F_n:=\sum_{i=1}^n2a_ib_i.$$ Is it possible to give, for fixed $n$, expanded expressions for $F_n^k$ for $k=2,...,n$?  Considering the case $n=2$ and using that powers of the generator vanishes as they anticommute we get $$F_2^2=4a_1b_1a_2b_2+4a_2b_2a_1b_1=8a_1b_1a_2b_2.$$ But what happens in general?",,['abstract-algebra']
4,"Integral including functions $\operatorname{erfc}(.), \exp(.) $ and $ \cos(.)$",Integral including functions  and,"\operatorname{erfc}(.), \exp(.)   \cos(.)","I have following integral. MATHEMATICA evaluates it as follows for $a>0$: $$I=\int_{0}^{\pi/2}\cos (\theta ) e^{a^2 \cos ^2(\theta )} \text{erfc}(a \cos (\theta ))d\theta=\frac{\sqrt{\pi } \left(1-e^{a^2} \text{erfc}\left(a\right)\right)}{2 a}$$ However, I have no clue how this result comes. I have checked few integral table books such as Table of Integrals, Series, and Products, and also functions.wolfram.com . But I could not find matching expressions. Does anyone have an idea?","I have following integral. MATHEMATICA evaluates it as follows for $a>0$: $$I=\int_{0}^{\pi/2}\cos (\theta ) e^{a^2 \cos ^2(\theta )} \text{erfc}(a \cos (\theta ))d\theta=\frac{\sqrt{\pi } \left(1-e^{a^2} \text{erfc}\left(a\right)\right)}{2 a}$$ However, I have no clue how this result comes. I have checked few integral table books such as Table of Integrals, Series, and Products, and also functions.wolfram.com . But I could not find matching expressions. Does anyone have an idea?",,"['integration', 'analysis']"
5,"Tao Analysis Vol I questions, ""logically equivalence"" problems","Tao Analysis Vol I questions, ""logically equivalence"" problems",,"Questions from section A.1. (reading this book on my own / self-studying, not a student, this is not homework): Edit: Apparently I had misinterpreted some or/and stuff so I am redoing my questions now: Suppose that you have shown that whenever $X$ is true, then $Y$ is   true, and whenever $X$ is false, $Y$ is false. Have you now   demonstrated that $X$ and $Y$ are logically equivalent? Explain. I believe this is saying, $(X \rightarrow Y) \land (\lnot X \rightarrow \lnot Y)$ which reduces to $(X \land Y) \lor (\lnot X \land \lnot Y)$, and this is the definition of ""if and only if"", so yes, they are logically equivalent. Suppose that you have shown that whenever $X$ is true, then $Y$ is   true, and whenever $Y$ is false, then $X$ is false. Have you now   demonstrated that $X$ is true if and only if $Y$ is true? Explain. $(X \rightarrow Y) \land (\lnot Y \rightarrow \lnot X)$, which reduces to $\lnot X \lor Y$, or just $X \rightarrow Y$ which is not the same as if-and-only-if. Suppose that you know that $X$ is true if and only if $Y$ is true, and   you know that $Y$ is true if and only if $Z$ is true. Is this enough   to show that $X, Y, Z$ are all logically equivalent? Explain. I think so? We know $X$ is logically equivalent to $Y$, and $Y$ is logically equivalent to $Z$, and by transitive property, $X$ is logically equivalent to $Z$, so they're all logically equivalent. I'm not sure if I am ""allowed"" to just invoke transitive property like that or if I have to show that it even applies here. Suppose you know that whenever $X$ is true, then $Y$ is true; that   whenever $Y$ is true, then $Z$ is true; and whenever $Z$ is true, then   $X$ is true. Is this enough to show that $X, Y, Z$ are all logically   equivalent? Explain. $(X \rightarrow Y) \land (Y \rightarrow Z) \land (Z \rightarrow X)$, right? So by transitive property again, we have $(X \rightarrow Z) \land (Z \rightarrow X)$ so $X$ and $Z$ are logically equivalent, and similar for the other two combinations.","Questions from section A.1. (reading this book on my own / self-studying, not a student, this is not homework): Edit: Apparently I had misinterpreted some or/and stuff so I am redoing my questions now: Suppose that you have shown that whenever $X$ is true, then $Y$ is   true, and whenever $X$ is false, $Y$ is false. Have you now   demonstrated that $X$ and $Y$ are logically equivalent? Explain. I believe this is saying, $(X \rightarrow Y) \land (\lnot X \rightarrow \lnot Y)$ which reduces to $(X \land Y) \lor (\lnot X \land \lnot Y)$, and this is the definition of ""if and only if"", so yes, they are logically equivalent. Suppose that you have shown that whenever $X$ is true, then $Y$ is   true, and whenever $Y$ is false, then $X$ is false. Have you now   demonstrated that $X$ is true if and only if $Y$ is true? Explain. $(X \rightarrow Y) \land (\lnot Y \rightarrow \lnot X)$, which reduces to $\lnot X \lor Y$, or just $X \rightarrow Y$ which is not the same as if-and-only-if. Suppose that you know that $X$ is true if and only if $Y$ is true, and   you know that $Y$ is true if and only if $Z$ is true. Is this enough   to show that $X, Y, Z$ are all logically equivalent? Explain. I think so? We know $X$ is logically equivalent to $Y$, and $Y$ is logically equivalent to $Z$, and by transitive property, $X$ is logically equivalent to $Z$, so they're all logically equivalent. I'm not sure if I am ""allowed"" to just invoke transitive property like that or if I have to show that it even applies here. Suppose you know that whenever $X$ is true, then $Y$ is true; that   whenever $Y$ is true, then $Z$ is true; and whenever $Z$ is true, then   $X$ is true. Is this enough to show that $X, Y, Z$ are all logically   equivalent? Explain. $(X \rightarrow Y) \land (Y \rightarrow Z) \land (Z \rightarrow X)$, right? So by transitive property again, we have $(X \rightarrow Z) \land (Z \rightarrow X)$ so $X$ and $Z$ are logically equivalent, and similar for the other two combinations.",,"['analysis', 'proof-verification', 'logic', 'proof-explanation']"
6,How to check for local extrema or saddle point given an semidefinite matrix,How to check for local extrema or saddle point given an semidefinite matrix,,"I've computed the Hessian of a given function $f(a,b,c) = y-a\sin(bx-c)$ and got the following result: $\begin{pmatrix} 0 & -x\cdot\cos(bx - c) & \cos(bx - c)  \\ -x\cdot\cos(bx - c) & ax^2\cdot\sin(bx - c) & -ax\sin(bx - c) \\ \cos(bx - c) & -ax\cdot\sin(bx - c) & a\cdot\sin(bx - c) \end{pmatrix}$ This matrix is positive semi-definite and thus one can not state for a given point $P=(a_i,b_i,c_i)$ if it is a local min, max or a saddle point. Is there any other way to explicitly determine if we have a loc. min, max or saddle point?","I've computed the Hessian of a given function $f(a,b,c) = y-a\sin(bx-c)$ and got the following result: $\begin{pmatrix} 0 & -x\cdot\cos(bx - c) & \cos(bx - c)  \\ -x\cdot\cos(bx - c) & ax^2\cdot\sin(bx - c) & -ax\sin(bx - c) \\ \cos(bx - c) & -ax\cdot\sin(bx - c) & a\cdot\sin(bx - c) \end{pmatrix}$ This matrix is positive semi-definite and thus one can not state for a given point $P=(a_i,b_i,c_i)$ if it is a local min, max or a saddle point. Is there any other way to explicitly determine if we have a loc. min, max or saddle point?",,"['analysis', 'positive-semidefinite', 'hessian-matrix']"
7,How can we convert double sums to single sums?,How can we convert double sums to single sums?,,"Let's say that $n\to A(n),B(n)$ is a bijection from $\mathbb{N} \to \mathbb{N^2}$. This would be the inverse of a pairing function . The canonical example would be $B(n)= n-\frac{1}{2}\lfloor \frac{\sqrt{8n+1}-1}{2}\rfloor \lfloor \frac{\sqrt{8n+1}+1}{2} \rfloor $ $A(n) = \lfloor \frac{\sqrt{8n+1}-1}{2}\rfloor-B\left(n\right)$ Then is it the case that $$\sum_{b=0}^\infty \sum_{a=0}^\infty{f(a,b)}=\sum_{n=0}^\infty f(A(n),B(n))?$$ I assume that if I am given that LHS is absolutely convergent the equality holds for any pairing function. I would guess that this is overkill however. If $\sum_{b=0}^\infty \sum_{a=0}^\infty{f(a,b)}$ is conditionally convergent we still may be able to find a suitable $A(n),B(n)$. Questions 1) What conditions are required for this equality to hold? 2) I don't know a whole lot about double sums. Is this a standard technique? Where can I learn more? 3) Where can I find more pairing functions? I can't imagine that the one above is the most convenient to work with... Example Let's say $f(a,b)=\frac{1}{(a+1)^2(b+1)^2}$ Then it's easy enough (Given we know the solution of the Basel problem ) to find the left hand side is $\pi^4/36$. But to find what the righthand side looks like is going to be tricky. We have a new series of rationals that approaches $\pi^4/36$. It's $\frac{1}{4}+\frac{1}{4}+\frac{1}{9}+\frac{1}{25}+\dots$ it doesn't have such a nice pattern to it because of our selection of the functions $A,B$.","Let's say that $n\to A(n),B(n)$ is a bijection from $\mathbb{N} \to \mathbb{N^2}$. This would be the inverse of a pairing function . The canonical example would be $B(n)= n-\frac{1}{2}\lfloor \frac{\sqrt{8n+1}-1}{2}\rfloor \lfloor \frac{\sqrt{8n+1}+1}{2} \rfloor $ $A(n) = \lfloor \frac{\sqrt{8n+1}-1}{2}\rfloor-B\left(n\right)$ Then is it the case that $$\sum_{b=0}^\infty \sum_{a=0}^\infty{f(a,b)}=\sum_{n=0}^\infty f(A(n),B(n))?$$ I assume that if I am given that LHS is absolutely convergent the equality holds for any pairing function. I would guess that this is overkill however. If $\sum_{b=0}^\infty \sum_{a=0}^\infty{f(a,b)}$ is conditionally convergent we still may be able to find a suitable $A(n),B(n)$. Questions 1) What conditions are required for this equality to hold? 2) I don't know a whole lot about double sums. Is this a standard technique? Where can I learn more? 3) Where can I find more pairing functions? I can't imagine that the one above is the most convenient to work with... Example Let's say $f(a,b)=\frac{1}{(a+1)^2(b+1)^2}$ Then it's easy enough (Given we know the solution of the Basel problem ) to find the left hand side is $\pi^4/36$. But to find what the righthand side looks like is going to be tricky. We have a new series of rationals that approaches $\pi^4/36$. It's $\frac{1}{4}+\frac{1}{4}+\frac{1}{9}+\frac{1}{25}+\dots$ it doesn't have such a nice pattern to it because of our selection of the functions $A,B$.",,"['sequences-and-series', 'functional-analysis', 'analysis', 'special-functions']"
8,Extension of a function which is uniformly continuous on compact subsets,Extension of a function which is uniformly continuous on compact subsets,,"Let $d\in\mathbb N$ $\Lambda\subseteq\mathbb R^d$ be open $-\infty<a<b<c<\infty$ If $f:\Lambda\times\left((a,c)\setminus\left\{b\right\}\right)\to\mathbb R$ is uniformly continuous on any compact subset of $\Lambda\times\left((a,c)\setminus\left\{b\right\}\right)$, are we able to deduce that $f$ has a unique extension $\tilde f$ to $\Lambda\times(a,c)$ and that $\tilde f$ is uniformly continuous? I know that a uniformly continuous function into a Banach space has a unique extension to the closure of its domain.","Let $d\in\mathbb N$ $\Lambda\subseteq\mathbb R^d$ be open $-\infty<a<b<c<\infty$ If $f:\Lambda\times\left((a,c)\setminus\left\{b\right\}\right)\to\mathbb R$ is uniformly continuous on any compact subset of $\Lambda\times\left((a,c)\setminus\left\{b\right\}\right)$, are we able to deduce that $f$ has a unique extension $\tilde f$ to $\Lambda\times(a,c)$ and that $\tilde f$ is uniformly continuous? I know that a uniformly continuous function into a Banach space has a unique extension to the closure of its domain.",,"['analysis', 'uniform-continuity']"
9,"$f: [0,1] \to \mathbb{R}$ absolutely continuous, $f' \in \{0,1\}$ (a.e.), $f(0)=0$. Prove that for some measurable subset $A$, $f(x)=m(A \cap (0,x))$","absolutely continuous,  (a.e.), . Prove that for some measurable subset ,","f: [0,1] \to \mathbb{R} f' \in \{0,1\} f(0)=0 A f(x)=m(A \cap (0,x))","Problem: Suppose that $f: [0,1] \to \mathbb{R}$ is absolutely continuous, $f' \in \{0,1\}$ (a.e.) and $f(0)=0$ . Prove that for some measurable subset $A \subset [0,1]$ and every $x \in [0,1]$ we have $f(x)=m(A \cap (0,x))$ I have proved it on my own. Please help me verify and tell me how much of the problem you think I have solved. (This was an exam problem and I'm trying to verify my given answer). I didn't have time to study derivatives for the real analysis exam and my argument might lack a bit of rigor, I think. Edit: I'm going to write down my proof as an answer to mark this question solved.","Problem: Suppose that is absolutely continuous, (a.e.) and . Prove that for some measurable subset and every we have I have proved it on my own. Please help me verify and tell me how much of the problem you think I have solved. (This was an exam problem and I'm trying to verify my given answer). I didn't have time to study derivatives for the real analysis exam and my argument might lack a bit of rigor, I think. Edit: I'm going to write down my proof as an answer to mark this question solved.","f: [0,1] \to \mathbb{R} f' \in \{0,1\} f(0)=0 A \subset [0,1] x \in [0,1] f(x)=m(A \cap (0,x))","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'absolute-continuity']"
10,Is matrix exponentiation $\exp:\mathcal{L}(E)\to\mathcal{L}(E)$ continuously Fréchet differentiable?,Is matrix exponentiation  continuously Fréchet differentiable?,\exp:\mathcal{L}(E)\to\mathcal{L}(E),"This is exercise VII.3.8 in Amann & Escher, Analysis II . Let $E$ be a Banach space. Denote by $\mathcal{L}(E)$ the space of all bounded linear maps from $E$ to itself. Then the exponential map $\exp:\mathcal{L}(E)\to\mathcal{L}(E)$ is continuously Fréchet differentiable. For $A,B\in\mathcal{L}(E)$, if $AB\neq BA$ then the expression $e^{A+B}-e^A$ is difficult to manipulate. I read the wikipedia page, where it says for any matrices $X,Y$ $$\|e^{X+Y}-e^X\|\leq\|Y\|e^{\|X\|}e^{\|Y\|},$$ so that $\exp$ is continuous when $E=\mathbb{C}^n$. But this does not solve the problem. I don't know how to proceed. Any help will be apprecated! By the way, I'm not sure about whether the claim is correct, because I've already seen a few wrong exercises in this book...","This is exercise VII.3.8 in Amann & Escher, Analysis II . Let $E$ be a Banach space. Denote by $\mathcal{L}(E)$ the space of all bounded linear maps from $E$ to itself. Then the exponential map $\exp:\mathcal{L}(E)\to\mathcal{L}(E)$ is continuously Fréchet differentiable. For $A,B\in\mathcal{L}(E)$, if $AB\neq BA$ then the expression $e^{A+B}-e^A$ is difficult to manipulate. I read the wikipedia page, where it says for any matrices $X,Y$ $$\|e^{X+Y}-e^X\|\leq\|Y\|e^{\|X\|}e^{\|Y\|},$$ so that $\exp$ is continuous when $E=\mathbb{C}^n$. But this does not solve the problem. I don't know how to proceed. Any help will be apprecated! By the way, I'm not sure about whether the claim is correct, because I've already seen a few wrong exercises in this book...",,"['matrices', 'analysis', 'matrix-exponential', 'frechet-derivative']"
11,Why $(y-\overline y)^t[f(x)-f(\overline x)]\ge-\Vert y-\overline y\Vert\Vert f(x)-f(\overline x)\Vert$?,Why ?,(y-\overline y)^t[f(x)-f(\overline x)]\ge-\Vert y-\overline y\Vert\Vert f(x)-f(\overline x)\Vert,Let $f:\mathbb R^n\to\mathbb R^{m+l}$ be continuous and $\overline y\in\mathbb R^{m+l}$. Why $0\ge (y-\overline y)^t[f(x)-f(\overline x)]\ge-\Vert y-\overline y\Vert\Vert f(x)-f(\overline x)\Vert$ ? What relation exists between the transpose of a vector and the norm with minus sign? Thank you. Edit: sorry I forgot the 0 at the beginning of the inequality.,Let $f:\mathbb R^n\to\mathbb R^{m+l}$ be continuous and $\overline y\in\mathbb R^{m+l}$. Why $0\ge (y-\overline y)^t[f(x)-f(\overline x)]\ge-\Vert y-\overline y\Vert\Vert f(x)-f(\overline x)\Vert$ ? What relation exists between the transpose of a vector and the norm with minus sign? Thank you. Edit: sorry I forgot the 0 at the beginning of the inequality.,,"['linear-algebra', 'analysis', 'inequality', 'continuity', 'proof-explanation']"
12,Applying Inverse Function Theorem to open set $A$ to prove $f^{-1}:f(A) \to A$ is a differentiable map,Applying Inverse Function Theorem to open set  to prove  is a differentiable map,A f^{-1}:f(A) \to A,"This is the first question in Spivak's Calculus on Manifolds from the chapter on the Inverse Function Theorem. Problem 2-36. * Let $A \subset \mathbb{R}^n$ be an open set and $f : A \to \mathbb{R}^n$ a continuously differentiable $1$-$1$ function such that $\det f'(x) \neq 0$ for all $x$. Show that $f(A)$ is an open set and $f^{-1} : f(A) \to A$ is differentiable. Show also that $f(B)$ is open for any open set $B \subset A$. To prove that $f^{-1}:f(A) \to A$ is differentiable, I began by applying the Inverse Function Theorem to some $x\in A$, which gives an inverse function from some subset of $f(A)$, lets call it $W_{f(x)}$ to a subset of $A$, called $V_x$, thus giving $f^{-1}:W_{f(x)} \to V_x$ as a differentiable map.  How then is $f^{-1}:f(A) \to A$ differentiable?  Do I generate inverse functions for each $y\in f(A)$ and define $f^{-1}:f(A) \to A$ as a piecewise function such that $f^{-1}:W_{f(x_1)} \to V_{x_1}$ is the function that applies when we are mapping an element that's contained in $W_{f(x_1)}$?  Or, do I just need one of the differentiable maps $f^{-1}:W_{f(x)} \to V_x$ to prove this? Thanks in advance.","This is the first question in Spivak's Calculus on Manifolds from the chapter on the Inverse Function Theorem. Problem 2-36. * Let $A \subset \mathbb{R}^n$ be an open set and $f : A \to \mathbb{R}^n$ a continuously differentiable $1$-$1$ function such that $\det f'(x) \neq 0$ for all $x$. Show that $f(A)$ is an open set and $f^{-1} : f(A) \to A$ is differentiable. Show also that $f(B)$ is open for any open set $B \subset A$. To prove that $f^{-1}:f(A) \to A$ is differentiable, I began by applying the Inverse Function Theorem to some $x\in A$, which gives an inverse function from some subset of $f(A)$, lets call it $W_{f(x)}$ to a subset of $A$, called $V_x$, thus giving $f^{-1}:W_{f(x)} \to V_x$ as a differentiable map.  How then is $f^{-1}:f(A) \to A$ differentiable?  Do I generate inverse functions for each $y\in f(A)$ and define $f^{-1}:f(A) \to A$ as a piecewise function such that $f^{-1}:W_{f(x_1)} \to V_{x_1}$ is the function that applies when we are mapping an element that's contained in $W_{f(x_1)}$?  Or, do I just need one of the differentiable maps $f^{-1}:W_{f(x)} \to V_x$ to prove this? Thanks in advance.",,"['calculus', 'analysis', 'multivariable-calculus', 'functions', 'inverse']"
13,"If the composition of a (strictly positive with compact domain) function with the logarithm is Lipschitz, then the function is Lipschitz.","If the composition of a (strictly positive with compact domain) function with the logarithm is Lipschitz, then the function is Lipschitz.",,"I'm reading a proof in which the authors want to show that a certain function $f:[0,T]\to\Bbb{R}_+\cup \{0\}$ is Lipschitz continuous. They prove that $$e^{−C|t−s|}f(t)≤f(s)≤e^{C|t−s|}f(t)$$ for all $t,s\in [0,T]$ and certain $C>0$ and then say that ""the result follows"". Why? I don't even know that $f$ is continuous , a priori . I've tried the following: Suppose $f$ is not Lipschitz. Then there are sequences $t_n,s_n\in [0,T]$ (which I can suppose $t_n\to t\in [0,T]$ and $s_n\to s\in [0,T]$, by compactness) such that the definition of Lipschitz fails for constants, say, $e^n$, $n\in \Bbb N$, i.e.  $$|f(t_n)-f(s_n)|>e^n|t_n-s_n|.$$ If $t\neq s$, then the inequality above says that $|f(t_n)-f(s_n)|\to +\infty$ (the images of $t_n,s_n$ by $f$ get arbitrarily far). But then... what? And what if $t=s$? Then I can't say if $e^n|t_n-s_n|\to 0$ or $+\infty$. Furthermore, I have the difficult of working with $f$ without knowing anything about its continuity... Remark.: If I knew that $f>0$ ( strictly positive), then applying the logarithm the proven inequality would be equivalent to $$|\ln (f(t))-\ln (f(s))|\leq C|t-s|.$$ This is not exactly the case (since $f$ can be zero) but I've used this as motivation to the title of this post. Edit: I've just realized that if $f(t)=0$ for some $t\in [0,T]$, then the proven inequality says that $0\leq f(s)\leq 0$, for any $s\in [0,T]$ and therefore $f\equiv 0$. So we can indeed suppose that $f>0$ as stated in the title.","I'm reading a proof in which the authors want to show that a certain function $f:[0,T]\to\Bbb{R}_+\cup \{0\}$ is Lipschitz continuous. They prove that $$e^{−C|t−s|}f(t)≤f(s)≤e^{C|t−s|}f(t)$$ for all $t,s\in [0,T]$ and certain $C>0$ and then say that ""the result follows"". Why? I don't even know that $f$ is continuous , a priori . I've tried the following: Suppose $f$ is not Lipschitz. Then there are sequences $t_n,s_n\in [0,T]$ (which I can suppose $t_n\to t\in [0,T]$ and $s_n\to s\in [0,T]$, by compactness) such that the definition of Lipschitz fails for constants, say, $e^n$, $n\in \Bbb N$, i.e.  $$|f(t_n)-f(s_n)|>e^n|t_n-s_n|.$$ If $t\neq s$, then the inequality above says that $|f(t_n)-f(s_n)|\to +\infty$ (the images of $t_n,s_n$ by $f$ get arbitrarily far). But then... what? And what if $t=s$? Then I can't say if $e^n|t_n-s_n|\to 0$ or $+\infty$. Furthermore, I have the difficult of working with $f$ without knowing anything about its continuity... Remark.: If I knew that $f>0$ ( strictly positive), then applying the logarithm the proven inequality would be equivalent to $$|\ln (f(t))-\ln (f(s))|\leq C|t-s|.$$ This is not exactly the case (since $f$ can be zero) but I've used this as motivation to the title of this post. Edit: I've just realized that if $f(t)=0$ for some $t\in [0,T]$, then the proven inequality says that $0\leq f(s)\leq 0$, for any $s\in [0,T]$ and therefore $f\equiv 0$. So we can indeed suppose that $f>0$ as stated in the title.",,"['calculus', 'analysis', 'functions', 'lipschitz-functions']"
14,Lemma which is used on Open Mapping Theorem,Lemma which is used on Open Mapping Theorem,,"Let $X$ and $Y$ Banach spaces wrt $\|.\|_X$ and $\|.\|_Y$ norms respectively and $S \subseteq X$ . $T:X \to Y$ is a linear and bounded (continuous) map Show that $\theta_X \in int(S) \Rightarrow \theta_Y \in int(T(S)) $ (where “int” is interior of a set and $\theta$ s are zeros of spaces) I have written if $\theta_X \in int(S) \Rightarrow \exists r \gt 0 , B(\theta_X,r) \subseteq S$ ( $B(\theta_X ,r)$ is open ball which has $\theta_X$ as center and $r$ as radius) I have been trying to show that : $ \exists \varepsilon \gt 0 , B(\theta_Y,\varepsilon) \subseteq T(S)$ i.e. $\theta_Y $ is an interior point of $T(S)$ but I stuck. I know it is very basic but I cannot guess anything about it. Is there any mistake in my writtens? How can I use being Banach space in here? I need a proof or guidance as simple as possible. Thanks in advance for helps",Let and Banach spaces wrt and norms respectively and . is a linear and bounded (continuous) map Show that (where “int” is interior of a set and s are zeros of spaces) I have written if ( is open ball which has as center and as radius) I have been trying to show that : i.e. is an interior point of but I stuck. I know it is very basic but I cannot guess anything about it. Is there any mistake in my writtens? How can I use being Banach space in here? I need a proof or guidance as simple as possible. Thanks in advance for helps,"X Y \|.\|_X \|.\|_Y S \subseteq X T:X \to Y \theta_X \in int(S) \Rightarrow \theta_Y \in int(T(S))  \theta \theta_X \in int(S) \Rightarrow \exists r \gt 0 , B(\theta_X,r) \subseteq S B(\theta_X ,r) \theta_X r  \exists \varepsilon \gt 0 , B(\theta_Y,\varepsilon) \subseteq T(S) \theta_Y  T(S)","['calculus', 'real-analysis', 'functional-analysis', 'analysis', 'banach-spaces']"
15,Closed-form expression of a tricky integral,Closed-form expression of a tricky integral,,"Let $a$, $b$ and $c$ be positive real scalars such that $a>b,c$ and consider the following integral $$ \int_{t_0}^{t_1} \log\left(\frac{a-c}{a-b\sin(\tau)}\right)\,\mathrm{d}\tau. $$ My question. Does there exist some sort of nice closed-form expression of the above integral? If so, does there also exist a ""nice"" closed-form expression of the following multiple integral   $$ \int_{t_0}^{t_1} \int_{\tau_{1,0}}^{\tau_1} \cdots \int_{\tau_{k-1,0}}^{\tau_{k-1}} \log\left(\frac{a-c}{a-b\sin(\tau_k)}\right)\,\mathrm{d}\tau_k \cdots\,\mathrm{d}\tau_2\,\mathrm{d}\tau_1 \ \ \ ? $$","Let $a$, $b$ and $c$ be positive real scalars such that $a>b,c$ and consider the following integral $$ \int_{t_0}^{t_1} \log\left(\frac{a-c}{a-b\sin(\tau)}\right)\,\mathrm{d}\tau. $$ My question. Does there exist some sort of nice closed-form expression of the above integral? If so, does there also exist a ""nice"" closed-form expression of the following multiple integral   $$ \int_{t_0}^{t_1} \int_{\tau_{1,0}}^{\tau_1} \cdots \int_{\tau_{k-1,0}}^{\tau_{k-1}} \log\left(\frac{a-c}{a-b\sin(\tau_k)}\right)\,\mathrm{d}\tau_k \cdots\,\mathrm{d}\tau_2\,\mathrm{d}\tau_1 \ \ \ ? $$",,"['integration', 'analysis', 'closed-form']"
16,Is this bounded uniformly on $H^{-2}(\mathbb{T})$?,Is this bounded uniformly on ?,H^{-2}(\mathbb{T}),"Let be $u(x,t) = e^{it\partial_x^2}u_0$ be the linear solution of \begin{align*} i\partial_t u +  \partial_x^2 u &= 0\\  u(x,0) &= u_0 \end{align*} so i would like to know if we have $u_0 \in L^2(\mathbb{T})$, $\varepsilon >0$ and if we define $u_\varepsilon = \dfrac{e^{i\varepsilon \partial_x^2}u_0 - u_0}{\varepsilon}$. It is $u_\varepsilon $ bounded uniformly on $H^{-2}(\mathbb{T})$ in $\varepsilon \to  0$ . $\mathbb{T}$ is the one dimension torus. Hope u can help me. Thank you in advance","Let be $u(x,t) = e^{it\partial_x^2}u_0$ be the linear solution of \begin{align*} i\partial_t u +  \partial_x^2 u &= 0\\  u(x,0) &= u_0 \end{align*} so i would like to know if we have $u_0 \in L^2(\mathbb{T})$, $\varepsilon >0$ and if we define $u_\varepsilon = \dfrac{e^{i\varepsilon \partial_x^2}u_0 - u_0}{\varepsilon}$. It is $u_\varepsilon $ bounded uniformly on $H^{-2}(\mathbb{T})$ in $\varepsilon \to  0$ . $\mathbb{T}$ is the one dimension torus. Hope u can help me. Thank you in advance",,"['functional-analysis', 'analysis', 'sobolev-spaces']"
17,"Prob. 1, Sec. 3.7, in Bartle & Sherbert's INTRO. TO REAL ANALYSIS: Removal of the zero terms of a convergent series does not affect the sum?","Prob. 1, Sec. 3.7, in Bartle & Sherbert's INTRO. TO REAL ANALYSIS: Removal of the zero terms of a convergent series does not affect the sum?",,"Here is Prob. 1, Sec. 3.7, in the book Introduction to Real Analysis by Robert G. Bartle and Donald R. Sherbert, 4th edition: Let $\sum a_n$ be a given series and let $\sum b_n$ be the series in which the terms are the same and in the same order as in $\sum a_n$ except that the terms for which $a_n = 0$ have been omitted. Show that $\sum a_n$ converges to $A$ if and only if $\sum b_n$ converges to $A$. My Attempt: In order to have a series at all for $\sum b_n$, we must of course assume that there are non-zero terms in $\sum a_n$; rather, we must also assume that there are infinitely many non-zero terms of the series $\sum a_n$, for otherwise both $\sum_{n=1}^\infty a_n$ and $\sum_{n=1}^\infty b_n$ be finite sums having the same non-zero terms and will thus be equal. Am I right? For the sake of definiteness, let us assume that $a_n = 0$ whenever $n$ is a prime number and that $a_n$ is non-zero otherwise. Let $\left( s_n \right)_{n \in \mathbb{N}}$ and $\left( t_n \right)_{n \in \mathbb{N}}$ be the sequences of the partial sums of $\sum a_n$ and $\sum b_n$, respectively.  Then we note that    $$ \begin{align}  t_1 &= s_1 = s_2 = s_3, \\  t_2 &= s_4 = s_5, \\ t_3 &= s_6 = s_7, \\  t_4 &= s_8, \\  t_5 &= s_9, \\  t_6 &= s_{10} = s_{11}, \\ t_7 &= s_{12} = s_{13}, \\ t_8 &= s_{14}, \\ t_9 &= s_{15}, \\ t_{10} &= s_{16} = s_{17}, \end{align} $$   and so on and so forth. In this way, we observe that $\left( t_n \right)_{n \in \mathbb{N}}$ is a subsequence of $\left( s_n \right)_{n \in \mathbb{N}}$ and therefore the former sequence has the same limit as the latter. Is this kind of ""proof"" satisfactory enough, I wonder? I would like to make it more general and rigorous though. Reading off the above array from the right to left, we observe that the terms of the sequence $\left( s_n \right)_{n \in \mathbb{N}}$ also appear in the sequence $\left( t_n \right)_{n \in \mathbb{N}}$ and the predecessor-successor relationship between terms is preserved in passing from the latter sequence to the former. Thus (intuitively at least) the former sequence cannot have any limit different from the limit of the latter sequence. How to make this part of my argument more precise and rigorous, employing only what has preceded this particular exercise problem in the book? In general, how to give a proof of each one of the two parts in the full level of generality, rigor, and detail?","Here is Prob. 1, Sec. 3.7, in the book Introduction to Real Analysis by Robert G. Bartle and Donald R. Sherbert, 4th edition: Let $\sum a_n$ be a given series and let $\sum b_n$ be the series in which the terms are the same and in the same order as in $\sum a_n$ except that the terms for which $a_n = 0$ have been omitted. Show that $\sum a_n$ converges to $A$ if and only if $\sum b_n$ converges to $A$. My Attempt: In order to have a series at all for $\sum b_n$, we must of course assume that there are non-zero terms in $\sum a_n$; rather, we must also assume that there are infinitely many non-zero terms of the series $\sum a_n$, for otherwise both $\sum_{n=1}^\infty a_n$ and $\sum_{n=1}^\infty b_n$ be finite sums having the same non-zero terms and will thus be equal. Am I right? For the sake of definiteness, let us assume that $a_n = 0$ whenever $n$ is a prime number and that $a_n$ is non-zero otherwise. Let $\left( s_n \right)_{n \in \mathbb{N}}$ and $\left( t_n \right)_{n \in \mathbb{N}}$ be the sequences of the partial sums of $\sum a_n$ and $\sum b_n$, respectively.  Then we note that    $$ \begin{align}  t_1 &= s_1 = s_2 = s_3, \\  t_2 &= s_4 = s_5, \\ t_3 &= s_6 = s_7, \\  t_4 &= s_8, \\  t_5 &= s_9, \\  t_6 &= s_{10} = s_{11}, \\ t_7 &= s_{12} = s_{13}, \\ t_8 &= s_{14}, \\ t_9 &= s_{15}, \\ t_{10} &= s_{16} = s_{17}, \end{align} $$   and so on and so forth. In this way, we observe that $\left( t_n \right)_{n \in \mathbb{N}}$ is a subsequence of $\left( s_n \right)_{n \in \mathbb{N}}$ and therefore the former sequence has the same limit as the latter. Is this kind of ""proof"" satisfactory enough, I wonder? I would like to make it more general and rigorous though. Reading off the above array from the right to left, we observe that the terms of the sequence $\left( s_n \right)_{n \in \mathbb{N}}$ also appear in the sequence $\left( t_n \right)_{n \in \mathbb{N}}$ and the predecessor-successor relationship between terms is preserved in passing from the latter sequence to the former. Thus (intuitively at least) the former sequence cannot have any limit different from the limit of the latter sequence. How to make this part of my argument more precise and rigorous, employing only what has preceded this particular exercise problem in the book? In general, how to give a proof of each one of the two parts in the full level of generality, rigor, and detail?",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence']"
18,$L^p$ convergence of composition,convergence of composition,L^p,"Let $f \in L^{p}(U)$ and $(t_{n})$ be a sequence of functions from $U$ to $U$ such that $$t_{n} \rightarrow \operatorname{id},$$ in the space $L^{\infty}(U)$ . Does the following hold: $$f \circ t_{n} \rightarrow f,$$ in the space $L^{p}(U)$ ? I tried using the dominated convergence theorem for $p_{n}(x)=\left|f(x) - f(t_{n}(x))\right|^{p}$ to show that the statement is true, but I couldn't find the ""dominating"" function.","Let and be a sequence of functions from to such that in the space . Does the following hold: in the space ? I tried using the dominated convergence theorem for to show that the statement is true, but I couldn't find the ""dominating"" function.","f \in L^{p}(U) (t_{n}) U U t_{n} \rightarrow \operatorname{id}, L^{\infty}(U) f \circ t_{n} \rightarrow f, L^{p}(U) p_{n}(x)=\left|f(x) - f(t_{n}(x))\right|^{p}","['sequences-and-series', 'analysis', 'measure-theory', 'convergence-divergence', 'lp-spaces']"
19,Does this claim holds even if the set isn't closed?,Does this claim holds even if the set isn't closed?,,"Let $M$ be a smooth manifold, and $C^\infty_0(M)$ the space of smooth compactly supported functions on $M$ . Let $\mathcal{D}'(M)$ be the space of distributions, meaning continuous linear functionals $\mathfrak{J} : C^\infty_0(M)\to \mathbb{R}$ . Now, we have the following definitions: Definition 1. Let $U\subset M$ , we say that $\mathfrak{J}$ vanishes in $U$ if for every $\phi \in C^\infty_0(M)$ with $\operatorname{supp}\phi\subset U$ we have $\mathfrak{J}[\phi]=0$ . Definition 2. Define $$\mathfrak{U}=\{U\subset M : \text{$U$ is open and $\mathfrak{J}$ vanishes on $U$}\},$$ we define the support of $\mathfrak{J}$ to be $$\operatorname{supp}\mathfrak{J}=M\setminus \bigcup \mathfrak{U}.$$ i.e., as the complement of the union of all open sets on which $\mathfrak{J}$ vanishes. Now consider the following situation: $\mathfrak{J}$ depends on the values of the test functions just on $A\subset M$ . This means that if $\phi_1,\phi_2\in C^\infty_0(M)$ with $\phi_1|_A = \phi_2|_A$ then $\mathfrak{J}[\phi_1]=\mathfrak{J}[\phi_2]$ . We can thus prove the following claim: Proposition: If $A$ is closed, $\operatorname{supp} \mathfrak{J}\subset A.$ Proof: To show that $\operatorname{supp}\mathfrak{J}\subset A$ it is enough to prove that $M\setminus A\subset \bigcup \mathfrak{U}$ . Since $A$ is closed, $M\setminus A$ is open. On the other hand $\mathfrak{J}$ vanishes on $M\setminus A$ . Indeed, let $\phi\in C^\infty_0(M)$ have $\operatorname{supp}\phi\subset A$ , meaning that $\phi|_A =0$ . Thus $\mathfrak{J}[\phi]=\mathfrak{J}[0]$ and the later is zero by linearity, showing that $\mathfrak{J}$ vanishes in $A$ . This shows that $M\setminus A\in \mathfrak{U}$ and hence $M\setminus A\subset \bigcup \mathfrak{U}$ which in turn shows that $\operatorname{supp}\mathfrak{J}\subset A$ . Now, I've used that $A$ is closed and hence $M\setminus A$ is open in order to conclude $M\setminus A\in \mathfrak{U}$ (since all sets on this collection are open), which seems one important part of the proof. But what if $A$ is not closed? It seems reasonable to me that whatever $A$ is if $\mathfrak{J}[\phi]$ depends on $\phi$ just inside $A$ we should have $\operatorname{supp}\mathfrak{J}\subset A$ . Is that indeed the case? How the proof would change?","Let be a smooth manifold, and the space of smooth compactly supported functions on . Let be the space of distributions, meaning continuous linear functionals . Now, we have the following definitions: Definition 1. Let , we say that vanishes in if for every with we have . Definition 2. Define we define the support of to be i.e., as the complement of the union of all open sets on which vanishes. Now consider the following situation: depends on the values of the test functions just on . This means that if with then . We can thus prove the following claim: Proposition: If is closed, Proof: To show that it is enough to prove that . Since is closed, is open. On the other hand vanishes on . Indeed, let have , meaning that . Thus and the later is zero by linearity, showing that vanishes in . This shows that and hence which in turn shows that . Now, I've used that is closed and hence is open in order to conclude (since all sets on this collection are open), which seems one important part of the proof. But what if is not closed? It seems reasonable to me that whatever is if depends on just inside we should have . Is that indeed the case? How the proof would change?","M C^\infty_0(M) M \mathcal{D}'(M) \mathfrak{J} : C^\infty_0(M)\to \mathbb{R} U\subset M \mathfrak{J} U \phi \in C^\infty_0(M) \operatorname{supp}\phi\subset U \mathfrak{J}[\phi]=0 \mathfrak{U}=\{U\subset M : \text{U is open and \mathfrak{J} vanishes on U}\}, \mathfrak{J} \operatorname{supp}\mathfrak{J}=M\setminus \bigcup \mathfrak{U}. \mathfrak{J} \mathfrak{J} A\subset M \phi_1,\phi_2\in C^\infty_0(M) \phi_1|_A = \phi_2|_A \mathfrak{J}[\phi_1]=\mathfrak{J}[\phi_2] A \operatorname{supp} \mathfrak{J}\subset A. \operatorname{supp}\mathfrak{J}\subset A M\setminus A\subset \bigcup \mathfrak{U} A M\setminus A \mathfrak{J} M\setminus A \phi\in C^\infty_0(M) \operatorname{supp}\phi\subset A \phi|_A =0 \mathfrak{J}[\phi]=\mathfrak{J}[0] \mathfrak{J} A M\setminus A\in \mathfrak{U} M\setminus A\subset \bigcup \mathfrak{U} \operatorname{supp}\mathfrak{J}\subset A A M\setminus A M\setminus A\in \mathfrak{U} A A \mathfrak{J}[\phi] \phi A \operatorname{supp}\mathfrak{J}\subset A","['functional-analysis', 'analysis', 'differential-geometry', 'distribution-theory']"
20,Showing $(a^{k_n})\to a^k$ whenever $(k_n)\to k$.,Showing  whenever .,(a^{k_n})\to a^k (k_n)\to k,"If the sequence $(k_n)\to k$, is it possible to show $(a^{k_n})\to a^k$ without having to involve the theory of limits of functions. i.e., to show that $\forall\varepsilon > 0, \exists K\in\mathbb{N}$ s.t $\forall n\ge K$ we have $\mid a^{k_n}-a^k\mid < \varepsilon.$ Rudin uses this result in one of the examples in a chapter which comes before  concept of limits of functions.","If the sequence $(k_n)\to k$, is it possible to show $(a^{k_n})\to a^k$ without having to involve the theory of limits of functions. i.e., to show that $\forall\varepsilon > 0, \exists K\in\mathbb{N}$ s.t $\forall n\ge K$ we have $\mid a^{k_n}-a^k\mid < \varepsilon.$ Rudin uses this result in one of the examples in a chapter which comes before  concept of limits of functions.",,"['real-analysis', 'sequences-and-series', 'analysis']"
21,"Let $I = [0 , 1]$; let $Q = I \times I$. Define $f: Q \to \mathbb{R}$ by letting $f(x , y) = 1 /q$ if y is rational and $x = p/q$",Let ; let . Define  by letting  if y is rational and,"I = [0 , 1] Q = I \times I f: Q \to \mathbb{R} f(x , y) = 1 /q x = p/q","Let $I = [0 , 1]$; let $Q = I \times I$. Define $f: Q \to \mathbb{R}$ by letting $f(x , y) = 1 /q$ if y is rational and $X = p/q$, where p and q are positive integers with no common factor; let $f(x, y) = 0$ otherwise. I know that this question is already posted here P and here P, but I still do not understand what the answers say or convince me, I would like a clearer answer and for all the questions. (a) How do I show that $\int_{Q}f$ exists? I have thought that this function is almost zero except in a set of zero measure but I do not know how to use this. One could show that this function is integrable considering cases on $x$ and $y$ as for example that both are rational or irrational? Could that be reduced to the case where $x$ and $y$ are rational? This function is not very similar to the Thomae function? (b) I think that $\underline{\int_{y\in I}}f(x,y)=\sup\{L(f(x,y),P_B)\}$, where $L(f(x,y), P_B)=\sum_{R_B}m_{R_B}(f)v(R_B)$ and $R_B$ is a rectangle determined by the partition $P_B$ where $P=(P_A,P_B)$ and $P$ is a partition of $Q$. But we know that $\sum_{R_B}m_{R_B}(f)v(R_B)=0$ because in a very small interval we will always find an irrational and so $f(x,y)=0$, with which $\underline{\int_{y\in I}}f(x,y)=0$. I do not know how to calculate $\overline{\int_{y\in I}}f(x,y)$, could someone help me please? I think that $\overline{\int_{y\in I}}f(x,y)=\inf\{U(f(x,y),P_B)\}$, but in this case I do not know which one is $M_{R_B}(f)$. (c) Fubini's theorem says that if $\int_{Q}f$ exists then $\int_{Q}f=\int_{x\in A}\underline{\int_{y\in I}}f(x,y)=\int_{x\in A}\overline{\int_{y\in I}}f(x,y)$, with which I have to calculate $\int_{x\in A}\underline{\int_{y\in I}}f(x,y)$ and $\int_{x\in A}\overline{\int_{y\in I}}f(x,y)$ and for this I need $\overline{\int_{y\in I}}f(x,y)$.","Let $I = [0 , 1]$; let $Q = I \times I$. Define $f: Q \to \mathbb{R}$ by letting $f(x , y) = 1 /q$ if y is rational and $X = p/q$, where p and q are positive integers with no common factor; let $f(x, y) = 0$ otherwise. I know that this question is already posted here P and here P, but I still do not understand what the answers say or convince me, I would like a clearer answer and for all the questions. (a) How do I show that $\int_{Q}f$ exists? I have thought that this function is almost zero except in a set of zero measure but I do not know how to use this. One could show that this function is integrable considering cases on $x$ and $y$ as for example that both are rational or irrational? Could that be reduced to the case where $x$ and $y$ are rational? This function is not very similar to the Thomae function? (b) I think that $\underline{\int_{y\in I}}f(x,y)=\sup\{L(f(x,y),P_B)\}$, where $L(f(x,y), P_B)=\sum_{R_B}m_{R_B}(f)v(R_B)$ and $R_B$ is a rectangle determined by the partition $P_B$ where $P=(P_A,P_B)$ and $P$ is a partition of $Q$. But we know that $\sum_{R_B}m_{R_B}(f)v(R_B)=0$ because in a very small interval we will always find an irrational and so $f(x,y)=0$, with which $\underline{\int_{y\in I}}f(x,y)=0$. I do not know how to calculate $\overline{\int_{y\in I}}f(x,y)$, could someone help me please? I think that $\overline{\int_{y\in I}}f(x,y)=\inf\{U(f(x,y),P_B)\}$, but in this case I do not know which one is $M_{R_B}(f)$. (c) Fubini's theorem says that if $\int_{Q}f$ exists then $\int_{Q}f=\int_{x\in A}\underline{\int_{y\in I}}f(x,y)=\int_{x\in A}\overline{\int_{y\in I}}f(x,y)$, with which I have to calculate $\int_{x\in A}\underline{\int_{y\in I}}f(x,y)$ and $\int_{x\in A}\overline{\int_{y\in I}}f(x,y)$ and for this I need $\overline{\int_{y\in I}}f(x,y)$.",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus', 'vector-analysis']"
22,Prove that $T$ is bounded and onto.,Prove that  is bounded and onto.,T,"I need just a hint, not a whole solution. Problem: Let $$T:C[0,1]\to c$$ where $c$ is the space of the convergent sequences and $$T(f)=\left(f\left(\frac{1}{n}\right)\right)$$ prove that $T$ is bounded surjection. What I have done: The boundedness is easy to check. For the sujectivity lets assume that $a_{n}=f(n)\in c$ so I take $(f\circ g)(n)$ where $g(n)=\frac{1}{n}$ so    $$ T((f\circ g)(n))=\left(f\left(\frac{1}{n}\right)\right)$$   But the problem is that I cant show that $f\circ g$ belongs to $c$.    How can I handle this problem? Is my approach true or I should change?","I need just a hint, not a whole solution. Problem: Let $$T:C[0,1]\to c$$ where $c$ is the space of the convergent sequences and $$T(f)=\left(f\left(\frac{1}{n}\right)\right)$$ prove that $T$ is bounded surjection. What I have done: The boundedness is easy to check. For the sujectivity lets assume that $a_{n}=f(n)\in c$ so I take $(f\circ g)(n)$ where $g(n)=\frac{1}{n}$ so    $$ T((f\circ g)(n))=\left(f\left(\frac{1}{n}\right)\right)$$   But the problem is that I cant show that $f\circ g$ belongs to $c$.    How can I handle this problem? Is my approach true or I should change?",,"['real-analysis', 'functional-analysis', 'analysis', 'operator-theory']"
23,"Derivative of $d(x,A)$",Derivative of,"d(x,A)","For $A \subseteq \mathbb{R}$, define $$d(x, A) = \inf_{y \in A}|x-y|.$$ Let $A \subseteq \mathbb{R},$ define $f : \mathbb{R} \rightarrow [0,\infty)$ by $$f(x) = d(x, A).$$ Then I can show that, for any $x, y$, $$|f(x) - f(y)| < |x - y|.$$ So $f$ is Lipschitz which yields that $f$ is of bounded varition. So $f'$ exists almost everywhere. I need to show that for $x$ such that $f'(x)$ exists, then $f'(x) \in \{-1, 0, 1\}.$ $\textbf{Attemp}$ I know that $f(x) = 0 \leftrightarrow d(x, A) = 0 \leftrightarrow x \in \ \mbox{cl} \ A$ where $\mbox{cl} \ A$ is the closure of the set $A$. Since $$ \frac{|f(x) - f(y)|}{|x - y|} \leq 1$$ for any $x,y \in \mathbb{R} ,$ then $|f'(x)| \leq 1.$ Specifically, $$f'(x) = \lim_{y \rightarrow x} \frac{f(x) - f(y)}{x-y} .$$ If $x \in \ \mbox{cl} \ A,$ then $f(x) = 0.$ Then there exists a sequence $\{x_n\}$ such that $x_n \in A$ and $x_n \rightarrow x.$ So $f(x_n) = 0$ for all $n$. This yields that $f(x) - f(x_n) = 0$ for all $n$. Since $f'(x)$ exists, then $$\lim_{y \rightarrow x} \frac{f(x) - f(y)}{x-y} = \lim_{n \rightarrow \infty} \frac{f(x) - f(x_n)}{x - x_n} = 0.$$ Now $x \not\in \ \mbox{cl} \ A.$ I am not sure how to proceed in this case. Any help or suggestion ?","For $A \subseteq \mathbb{R}$, define $$d(x, A) = \inf_{y \in A}|x-y|.$$ Let $A \subseteq \mathbb{R},$ define $f : \mathbb{R} \rightarrow [0,\infty)$ by $$f(x) = d(x, A).$$ Then I can show that, for any $x, y$, $$|f(x) - f(y)| < |x - y|.$$ So $f$ is Lipschitz which yields that $f$ is of bounded varition. So $f'$ exists almost everywhere. I need to show that for $x$ such that $f'(x)$ exists, then $f'(x) \in \{-1, 0, 1\}.$ $\textbf{Attemp}$ I know that $f(x) = 0 \leftrightarrow d(x, A) = 0 \leftrightarrow x \in \ \mbox{cl} \ A$ where $\mbox{cl} \ A$ is the closure of the set $A$. Since $$ \frac{|f(x) - f(y)|}{|x - y|} \leq 1$$ for any $x,y \in \mathbb{R} ,$ then $|f'(x)| \leq 1.$ Specifically, $$f'(x) = \lim_{y \rightarrow x} \frac{f(x) - f(y)}{x-y} .$$ If $x \in \ \mbox{cl} \ A,$ then $f(x) = 0.$ Then there exists a sequence $\{x_n\}$ such that $x_n \in A$ and $x_n \rightarrow x.$ So $f(x_n) = 0$ for all $n$. This yields that $f(x) - f(x_n) = 0$ for all $n$. Since $f'(x)$ exists, then $$\lim_{y \rightarrow x} \frac{f(x) - f(y)}{x-y} = \lim_{n \rightarrow \infty} \frac{f(x) - f(x_n)}{x - x_n} = 0.$$ Now $x \not\in \ \mbox{cl} \ A.$ I am not sure how to proceed in this case. Any help or suggestion ?",,"['real-analysis', 'analysis', 'derivatives']"
24,About proof of fundamental lemma of calculus of variation,About proof of fundamental lemma of calculus of variation,,"I have found several proofs of fundamental lemma of calculus of variation for $C^1$ functions, but could someone help me to show this lemma with the following hypothesis?: Let be $g:[x,y]\rightarrow{\mathbb{R}}$ a continuous function such that $\int_x^yg(t)h(t)=0$  $\forall{h\in{C^\infty_c(x,y)}}$, show that $g=0$. Thanks.","I have found several proofs of fundamental lemma of calculus of variation for $C^1$ functions, but could someone help me to show this lemma with the following hypothesis?: Let be $g:[x,y]\rightarrow{\mathbb{R}}$ a continuous function such that $\int_x^yg(t)h(t)=0$  $\forall{h\in{C^\infty_c(x,y)}}$, show that $g=0$. Thanks.",,"['real-analysis', 'functional-analysis', 'analysis']"
25,Thomae function -- points lying on the same line,Thomae function -- points lying on the same line,,"Plotted is $y=f(x)$ for $x\in[0,1]$, where $$f(x) =\begin{cases}\frac{1}{q}\quad\text{ if }x=\frac{p}{q}\text{, $p$, $q$ coprime ($q$ positive)} \\ 0\quad\text{ if $x$ is irrational}\end{cases}$$ I can see why horizontal lines appear, but what is the reason for all the other straight lines appearing?","Plotted is $y=f(x)$ for $x\in[0,1]$, where $$f(x) =\begin{cases}\frac{1}{q}\quad\text{ if }x=\frac{p}{q}\text{, $p$, $q$ coprime ($q$ positive)} \\ 0\quad\text{ if $x$ is irrational}\end{cases}$$ I can see why horizontal lines appear, but what is the reason for all the other straight lines appearing?",,"['real-analysis', 'analysis', 'functions', 'continuity', 'special-functions']"
26,"Uniform convergence of $n\sin\sqrt{4\pi^2n^2+x^2}$ on $[0,a]$ and $\mathbb{R}.$",Uniform convergence of  on  and,"n\sin\sqrt{4\pi^2n^2+x^2} [0,a] \mathbb{R}.","Let $f_n(x)=n\sin\sqrt{4\pi^2n^2+x^2}$. Prove that $(f_n)$ is uniformly convergent on $[0,a]$ for every $a>0.$ Is the convergence uniform on $\mathbb{R}$? Attempt. For $x\in \mathbb{R}$ constant we have: $$f_n(x)=n\sin(\sqrt{4\pi^2n^2+x^2}-2\pi n)=n\sin\bigg(\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2\pi n}\bigg)= \frac{\sin\bigg(\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2\pi n}\bigg)}{\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2\pi n}}\,\frac{n\,x^2}{\sqrt{4\pi^2n^2+x^2}+2\pi n}\rightarrow 1\cdot \frac{x^2}{4\pi}=\frac{x^2}{4\pi},$$ so we are done with the pointwise convergence. So far I am stuck on the uniform convergence, in other words how to prove $$\max_{0\leq  x\leq a}|f_n(x)-x^2/(4\pi)|\rightarrow 0$$ and $$\sup_{x\in \mathbb{R}}|f_n(x)-x^2/(4\pi)|=\sup_{x\geq 0}|f_n(x)-x^2/(4\pi)|\nrightarrow 0.$$ Thanks for the help.","Let $f_n(x)=n\sin\sqrt{4\pi^2n^2+x^2}$. Prove that $(f_n)$ is uniformly convergent on $[0,a]$ for every $a>0.$ Is the convergence uniform on $\mathbb{R}$? Attempt. For $x\in \mathbb{R}$ constant we have: $$f_n(x)=n\sin(\sqrt{4\pi^2n^2+x^2}-2\pi n)=n\sin\bigg(\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2\pi n}\bigg)= \frac{\sin\bigg(\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2\pi n}\bigg)}{\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2\pi n}}\,\frac{n\,x^2}{\sqrt{4\pi^2n^2+x^2}+2\pi n}\rightarrow 1\cdot \frac{x^2}{4\pi}=\frac{x^2}{4\pi},$$ so we are done with the pointwise convergence. So far I am stuck on the uniform convergence, in other words how to prove $$\max_{0\leq  x\leq a}|f_n(x)-x^2/(4\pi)|\rightarrow 0$$ and $$\sup_{x\in \mathbb{R}}|f_n(x)-x^2/(4\pi)|=\sup_{x\geq 0}|f_n(x)-x^2/(4\pi)|\nrightarrow 0.$$ Thanks for the help.",,"['real-analysis', 'analysis', 'uniform-convergence', 'pointwise-convergence']"
27,Convergence (rate) of a three term recurrence,Convergence (rate) of a three term recurrence,,"Suppose we have following recurrence relation for a positive real sequence $\{a_n\}$ \begin{align*} a_{n+1} \le \left(q+\frac {c_1} {n+1}\right) a_n + \frac{c_2}{n} a_{n-1}, \end{align*} where $c_1$ and $c_2$ are some positive constants and $0 < q < 1$. Will the sequence $\{a_n\}_{n=1}^{\infty}$ stay bounded (or better, converging to $0$)? My intuition is: since there are only finitely many $n$ with $q+c_1/(n+1) > 1$, eventually the sequence will decrease and so converge to $0$. But I haven't come up a rigorous proof. I did some simulation by making the inequality to be equality, it seems like the sequence converges to $0$ even with $q=0.999$. UPDATE: Thanks to @Michael. I got one solution which is based on the observation pointed out by @Michael. He also gave another way to proceed based on this observation (see his answer.) We observe for every $1 > r >0$, for sufficiently large $n$, the recurrence is equivalent to \begin{align*} a_{n+1} \le \frac{1+q}{2} a_n + r a_{n-1}. \end{align*} Without loss of generality, we may assume for all $n$, above relation holds. Now if we can find some parameters $0<q'<1$ and $c >0$ such that \begin{align*} a_{n+1} + c a_n \le q'(a_n + c a_{n-1}), \end{align*} it is clear $a_n \to 0$ since the sequence is positive. One choice is putting $q' = \frac{3+q} 4$ and $c=\frac{1-q} 4$. In this case, we can take $r = \frac{(1-q)(3+q)} {16}$. The remaining question is whether it is possible to estimate the rate of convergence. Any comments are welcome. Thanks. UPDATE on convergence rate: @Michael pointed out some convergence rate when $c_1, c_2$ are sufficiently small. I think if using my method, we could give a convergence rate related to $\varepsilon > 0$. For any $1-q>\varepsilon >0$, there exists $N$ such that $n \ge N$,  \begin{align*} a_{n+1} + c a_n \le (q+ \varepsilon) (a_n + c a_{n-1}). \end{align*} Let $N'$ be the smallest integer such that $c_1/N' \le \varepsilon$ and $M = \max_{j \le N'} (a_j + c a_{j-1})$. Then $a_{n+1} \le a_{n+1} + ca_n \le (q+\varepsilon)^{n-N'} M$. Looks like some pseudo linear rate. Could we do better? Any comments will be helpful. Thanks.","Suppose we have following recurrence relation for a positive real sequence $\{a_n\}$ \begin{align*} a_{n+1} \le \left(q+\frac {c_1} {n+1}\right) a_n + \frac{c_2}{n} a_{n-1}, \end{align*} where $c_1$ and $c_2$ are some positive constants and $0 < q < 1$. Will the sequence $\{a_n\}_{n=1}^{\infty}$ stay bounded (or better, converging to $0$)? My intuition is: since there are only finitely many $n$ with $q+c_1/(n+1) > 1$, eventually the sequence will decrease and so converge to $0$. But I haven't come up a rigorous proof. I did some simulation by making the inequality to be equality, it seems like the sequence converges to $0$ even with $q=0.999$. UPDATE: Thanks to @Michael. I got one solution which is based on the observation pointed out by @Michael. He also gave another way to proceed based on this observation (see his answer.) We observe for every $1 > r >0$, for sufficiently large $n$, the recurrence is equivalent to \begin{align*} a_{n+1} \le \frac{1+q}{2} a_n + r a_{n-1}. \end{align*} Without loss of generality, we may assume for all $n$, above relation holds. Now if we can find some parameters $0<q'<1$ and $c >0$ such that \begin{align*} a_{n+1} + c a_n \le q'(a_n + c a_{n-1}), \end{align*} it is clear $a_n \to 0$ since the sequence is positive. One choice is putting $q' = \frac{3+q} 4$ and $c=\frac{1-q} 4$. In this case, we can take $r = \frac{(1-q)(3+q)} {16}$. The remaining question is whether it is possible to estimate the rate of convergence. Any comments are welcome. Thanks. UPDATE on convergence rate: @Michael pointed out some convergence rate when $c_1, c_2$ are sufficiently small. I think if using my method, we could give a convergence rate related to $\varepsilon > 0$. For any $1-q>\varepsilon >0$, there exists $N$ such that $n \ge N$,  \begin{align*} a_{n+1} + c a_n \le (q+ \varepsilon) (a_n + c a_{n-1}). \end{align*} Let $N'$ be the smallest integer such that $c_1/N' \le \varepsilon$ and $M = \max_{j \le N'} (a_j + c a_{j-1})$. Then $a_{n+1} \le a_{n+1} + ca_n \le (q+\varepsilon)^{n-N'} M$. Looks like some pseudo linear rate. Could we do better? Any comments will be helpful. Thanks.",,"['analysis', 'optimization', 'asymptotics', 'upper-lower-bounds']"
28,Showing L1 norm goes to 0,Showing L1 norm goes to 0,,"I'm stuck on the following problem and would appreciate any hints or solutions.   (Hints are preferable at first though) Let $\mu$ be a finite measure on $\left(X,B\right)$ and let $\alpha$   be a positive function on $\mathbb{R}$ such that $\alpha(x)/x\to \infty$   as $x \to \infty$. Suppose $f_n\to f$ a.e on $X$ and    $||\alpha\circ f_n||_1\leq M<\infty$ $\forall n\in \mathbb{N}$ Show that $\sup_{n}||f_n\chi_{[fn\geq K]}||_1\to 0$ as $K\to \infty$ Source: Old qualifying problem from fall of 1997. http://www-bcf.usc.edu/~mathgp/quals/real/all_medium.pdf Here are my thoughts: I noticed that $f_n\chi_{[fn\geq K]}$ goes to 0 as $K$ gets large for each fixed $x$. So I was thinking of somehow applying dominated convergence.  The problem, is that you have to do this with $K$, not $n$, which means you have to probably look at some sequence $K_i$?  The other thing is that we need to somehow exploit $||\alpha \circ f_n||_1\leq M$.  I think we can do that by using a simpler function instead of $\alpha$ that is dominated by $\alpha$ for all values greater than a suitable $K$.","I'm stuck on the following problem and would appreciate any hints or solutions.   (Hints are preferable at first though) Let $\mu$ be a finite measure on $\left(X,B\right)$ and let $\alpha$   be a positive function on $\mathbb{R}$ such that $\alpha(x)/x\to \infty$   as $x \to \infty$. Suppose $f_n\to f$ a.e on $X$ and    $||\alpha\circ f_n||_1\leq M<\infty$ $\forall n\in \mathbb{N}$ Show that $\sup_{n}||f_n\chi_{[fn\geq K]}||_1\to 0$ as $K\to \infty$ Source: Old qualifying problem from fall of 1997. http://www-bcf.usc.edu/~mathgp/quals/real/all_medium.pdf Here are my thoughts: I noticed that $f_n\chi_{[fn\geq K]}$ goes to 0 as $K$ gets large for each fixed $x$. So I was thinking of somehow applying dominated convergence.  The problem, is that you have to do this with $K$, not $n$, which means you have to probably look at some sequence $K_i$?  The other thing is that we need to somehow exploit $||\alpha \circ f_n||_1\leq M$.  I think we can do that by using a simpler function instead of $\alpha$ that is dominated by $\alpha$ for all values greater than a suitable $K$.",,"['real-analysis', 'analysis', 'measure-theory', 'normed-spaces']"
29,A question about the Peano form of the remainder in a function's Maclaurin formula?,A question about the Peano form of the remainder in a function's Maclaurin formula?,,"Let the function $f:\mathbb{R} \rightarrow \mathbb{R}$ be $n$ times differentiable at $x=0$, and we have $$f(x)=P_n(x)+o(x^n)$$ where $P_n(x)$ is the $n$-th order Taylor polynomial and $o(x^n)$ is the Peano form of the remainder. Then we surely have $$f(x^2)=P_n(x^2)+o(x^{2n})$$. But in the textbook I'm using, it goes like$$f(x^2)=P_n(x^2)+o(x^{2n+1})$$ How to prove it? Here's my effort: Let $g(x)=o(x^n)$, then we have $\displaystyle \lim_{x\rightarrow0}\frac{g(x)}{x^n} =0$, but it doesn't imply that  $\displaystyle \lim_{x\rightarrow0}\frac{g(x^2)}{x^{2n+1}} =0$. I guess the Peano form of the remainder may not be an ordinary ""Little-O of $x$"", but I can't find out the extra information about it. Could anyone help me out here?","Let the function $f:\mathbb{R} \rightarrow \mathbb{R}$ be $n$ times differentiable at $x=0$, and we have $$f(x)=P_n(x)+o(x^n)$$ where $P_n(x)$ is the $n$-th order Taylor polynomial and $o(x^n)$ is the Peano form of the remainder. Then we surely have $$f(x^2)=P_n(x^2)+o(x^{2n})$$. But in the textbook I'm using, it goes like$$f(x^2)=P_n(x^2)+o(x^{2n+1})$$ How to prove it? Here's my effort: Let $g(x)=o(x^n)$, then we have $\displaystyle \lim_{x\rightarrow0}\frac{g(x)}{x^n} =0$, but it doesn't imply that  $\displaystyle \lim_{x\rightarrow0}\frac{g(x^2)}{x^{2n+1}} =0$. I guess the Peano form of the remainder may not be an ordinary ""Little-O of $x$"", but I can't find out the extra information about it. Could anyone help me out here?",,"['analysis', 'taylor-expansion']"
30,Is there a criterion for showing that a two variable continuous function $f:\Bbb S^1\times\Bbb S^1\to \Bbb R^2$ pass from the origin?,Is there a criterion for showing that a two variable continuous function  pass from the origin?,f:\Bbb S^1\times\Bbb S^1\to \Bbb R^2,"Is there a criterion for showing that  a symmetric ($f(x,y)=f(y,x)$) two variable continuous function $f:\Bbb S^1\times\Bbb S^1\to \Bbb R^2$ pass from the origin? i.e.  $$\exists\,(x,y)\in \Bbb S^1\times\Bbb S^1,\,x\neq y\quad s.t. \quad f(x,y)=(0,0).$$   where we know that $f$ pass from all four regions and $f(x,x)=(0,0)$ for all $x\in\Bbb S^1$. I tried to define a real value function from $f$ and use from intermediate value theorem but I couldn't find a helpful map. Is it sufficient to show ${\rm Im} f$ is simply connected?","Is there a criterion for showing that  a symmetric ($f(x,y)=f(y,x)$) two variable continuous function $f:\Bbb S^1\times\Bbb S^1\to \Bbb R^2$ pass from the origin? i.e.  $$\exists\,(x,y)\in \Bbb S^1\times\Bbb S^1,\,x\neq y\quad s.t. \quad f(x,y)=(0,0).$$   where we know that $f$ pass from all four regions and $f(x,x)=(0,0)$ for all $x\in\Bbb S^1$. I tried to define a real value function from $f$ and use from intermediate value theorem but I couldn't find a helpful map. Is it sufficient to show ${\rm Im} f$ is simply connected?",,"['calculus', 'general-topology', 'analysis', 'algebraic-topology']"
31,Relation between function and Laplacian in Riemannain manifold,Relation between function and Laplacian in Riemannain manifold,,"Let $f$ and $k$ be two real valued function on the Riemannian manifold $(M,g)$ such that for each $p\in M$,  $$k(p)\geq \frac{\partial f}{\partial x_i}|_p.$$ Then wthat is the relation between Laplacian $\Delta f$ and $k$? I have tried as follows Since $grad(f)=g^{ij}\frac{\partial f}{\partial x_j}\partial _i$, hence $grad(f)|p\leq k(p)g^{ij}\partial _i|p$. Then putting the value of $grad(f)$ in Laplacian we get $$\Delta(f)(p)=g_p(\nabla_j k(p)g^{ij}\partial _i,\partial_j).$$ After that I can not go further. Please anyone help me. Thank you","Let $f$ and $k$ be two real valued function on the Riemannian manifold $(M,g)$ such that for each $p\in M$,  $$k(p)\geq \frac{\partial f}{\partial x_i}|_p.$$ Then wthat is the relation between Laplacian $\Delta f$ and $k$? I have tried as follows Since $grad(f)=g^{ij}\frac{\partial f}{\partial x_j}\partial _i$, hence $grad(f)|p\leq k(p)g^{ij}\partial _i|p$. Then putting the value of $grad(f)$ in Laplacian we get $$\Delta(f)(p)=g_p(\nabla_j k(p)g^{ij}\partial _i,\partial_j).$$ After that I can not go further. Please anyone help me. Thank you",,"['analysis', 'differential-geometry', 'riemannian-geometry']"
32,It is always possible to construct continuous function?,It is always possible to construct continuous function?,,"Given $D \subset X \subset \mathbb R$, I want to create a function $f: X \rightarrow \mathbb R$ such that (1) it's continuous (2) it has fixed value for every element of D (different values) When it is possible? For example, if D is finite, is certainly possible (with a polynomial, for example). It is possible even if $D = \mathbb N$ and, for example, I want that f(n) = n for every $n \in \mathbb N$. Or it is possible for every D if I fixed a single value for every element. It's (obviously) not possible if I choose $D = \mathbb R$ and I set the values such that the function is discontinuous. It's also not possible if I choose $D = \{x_n\}$ with $x_0 = 0, x_1=0.9, x_2=0.99 ... x_n \rightarrow 1$ and I set $f(x_n) = n$, in fact $f(1) = lim_{n \to \infty}f(x_n) = \infty$ (it would be continuous in [0, 1)?) I think it is a well-known question, but I don't know how to search it. Thanks in advance. (edit: and what if I work in a generic topological space, not $\mathbb R$?)","Given $D \subset X \subset \mathbb R$, I want to create a function $f: X \rightarrow \mathbb R$ such that (1) it's continuous (2) it has fixed value for every element of D (different values) When it is possible? For example, if D is finite, is certainly possible (with a polynomial, for example). It is possible even if $D = \mathbb N$ and, for example, I want that f(n) = n for every $n \in \mathbb N$. Or it is possible for every D if I fixed a single value for every element. It's (obviously) not possible if I choose $D = \mathbb R$ and I set the values such that the function is discontinuous. It's also not possible if I choose $D = \{x_n\}$ with $x_0 = 0, x_1=0.9, x_2=0.99 ... x_n \rightarrow 1$ and I set $f(x_n) = n$, in fact $f(1) = lim_{n \to \infty}f(x_n) = \infty$ (it would be continuous in [0, 1)?) I think it is a well-known question, but I don't know how to search it. Thanks in advance. (edit: and what if I work in a generic topological space, not $\mathbb R$?)",,"['general-topology', 'analysis', 'continuity']"
33,Proof problem explanation,Proof problem explanation,,"Let $f$ be a function defined at $x$. Suppose that every sequence $p_1, p_2, p_3,\dots$ in the domain of $f$ converging to $x$ has the property that $f(p_1), f(p_2), f(p_3),\dots$ converges to $f(x)$. Prove that $f$ is continuous at $x$ by contradiction. Proof: Suppose $f$ is not continuous at $a$. If $f$ is not continuous at $a$ then $a$ cannot be an isolated point, since every function is continuous at an isolated point of its domain. If $f$ is not continuous there is some epsilon, for which no matter how what $\delta$ we choose there is a point $x_n \in S$ with $ \lVert f(x_n) − f(a)\rVert \geq \epsilon$. So let’s take $\delta = 1/n$ and $x_n \in  S, \lVert x_n − a\rVert < 1/n$, $\lVert f(x_n) − f(a)\rVert \geq \epsilon$. Then $x_n \to a$ but $f(x_n) \to f(a)$. Hence some sequence of points $x_n$ converges to $a$ but $f(x_n)$ does not converge to $f(a)$ So a few questions for this. Can I use any arbitrary letter instead of epsilon? like $M$? Also can anyone explain in better detail the part after If $f$ is not continuous there is some $\epsilon$. I am confused from that point on.","Let $f$ be a function defined at $x$. Suppose that every sequence $p_1, p_2, p_3,\dots$ in the domain of $f$ converging to $x$ has the property that $f(p_1), f(p_2), f(p_3),\dots$ converges to $f(x)$. Prove that $f$ is continuous at $x$ by contradiction. Proof: Suppose $f$ is not continuous at $a$. If $f$ is not continuous at $a$ then $a$ cannot be an isolated point, since every function is continuous at an isolated point of its domain. If $f$ is not continuous there is some epsilon, for which no matter how what $\delta$ we choose there is a point $x_n \in S$ with $ \lVert f(x_n) − f(a)\rVert \geq \epsilon$. So let’s take $\delta = 1/n$ and $x_n \in  S, \lVert x_n − a\rVert < 1/n$, $\lVert f(x_n) − f(a)\rVert \geq \epsilon$. Then $x_n \to a$ but $f(x_n) \to f(a)$. Hence some sequence of points $x_n$ converges to $a$ but $f(x_n)$ does not converge to $f(a)$ So a few questions for this. Can I use any arbitrary letter instead of epsilon? like $M$? Also can anyone explain in better detail the part after If $f$ is not continuous there is some $\epsilon$. I am confused from that point on.",,['analysis']
34,Help proving/verifying a proof of a theorem,Help proving/verifying a proof of a theorem,,"I have been trying to prove this theorem which I think is wrong. Theorem : Assume that $$f(h)=p(h)+O(h^n),g(h)=q(h)+O(h^m)$$and $$r=min\{m,n| m,n \in \mathbb{Z}_+\}$$ then $$f(h)+g(h)=p(h)+q(h)+O(h^r)$$ My views are,if we suppose that WLOG $$m\leq n$$ then $$h^m=O(h^n)$$ therefore $$O(h^m)+O(h^n)=O(h^n)$$ contrary to the proposition. So my question; am I right?if not where am I going wrong and how can I then prove the theorem above?","I have been trying to prove this theorem which I think is wrong. Theorem : Assume that $$f(h)=p(h)+O(h^n),g(h)=q(h)+O(h^m)$$and $$r=min\{m,n| m,n \in \mathbb{Z}_+\}$$ then $$f(h)+g(h)=p(h)+q(h)+O(h^r)$$ My views are,if we suppose that WLOG $$m\leq n$$ then $$h^m=O(h^n)$$ therefore $$O(h^m)+O(h^n)=O(h^n)$$ contrary to the proposition. So my question; am I right?if not where am I going wrong and how can I then prove the theorem above?",,"['analysis', 'proof-verification', 'numerical-methods']"
35,Operator is continuous by boundedness?,Operator is continuous by boundedness?,,"I have a fairly simple question, I just want to make sure I don't oversee something: Let $l^\infty$ denote the vector space of all bounded sequences equipped with the $\sup$-norm and $c_0$ the subspace of all sequences converging to $0$. Define $$A:l^\infty \to c_0; \quad Ax:= (k^{-1}x_k)_{k \in \mathbb N}$$   Clearly, $A$ is a well-defined linear operator. Show that $A$ is continuous. Now my solution is simply the observation that $$\lVert Ax \rVert = \sup_k \lvert k^{-1}x_k \rvert \leq \lVert x \rVert_\infty$$ which means $A$ is bounded and, therefore, continuous. In the solution they make use of sequential continuity but essentially use the same inequality. Did I miss something?","I have a fairly simple question, I just want to make sure I don't oversee something: Let $l^\infty$ denote the vector space of all bounded sequences equipped with the $\sup$-norm and $c_0$ the subspace of all sequences converging to $0$. Define $$A:l^\infty \to c_0; \quad Ax:= (k^{-1}x_k)_{k \in \mathbb N}$$   Clearly, $A$ is a well-defined linear operator. Show that $A$ is continuous. Now my solution is simply the observation that $$\lVert Ax \rVert = \sup_k \lvert k^{-1}x_k \rvert \leq \lVert x \rVert_\infty$$ which means $A$ is bounded and, therefore, continuous. In the solution they make use of sequential continuity but essentially use the same inequality. Did I miss something?",,"['functional-analysis', 'analysis', 'continuity', 'operator-theory']"
36,Example 1.5-9 in Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS: How to construct such examples?,Example 1.5-9 in Kreyszig's INTRODUCTORY FUNCTIONAL ANALYSIS WITH APPLICATIONS: How to construct such examples?,,"Let $X$ denote the set of all the real (or complex) valued continuous functions on the closed interval $[0, 1]$, and let $$ d(x, y) \colon= \int_0^1 \lvert x(t) - y(t) \rvert \ \mathrm{d} t $$ for all $x, y \in X$. This $d$ is a metric on $X$, and $(X, d)$ is not complete, as has been shown by Kreyszig, for the sequence $\left( x_n \right)$, where  $$ x_n (t) \colon=  \begin{cases}  0 \ & \mbox{ if } \ 0 \leq t \leq \frac{1}{2}, \\  (m+2)\left( t - \frac{1}{2} \right) \ & \mbox{ if } \ \frac{1}{2} \leq t \leq \frac{1}{2} + \frac{1}{m+2}, \\ 1 \ & \mbox{ if } \ \frac{1}{2} + \frac{1}{m+2} \leq t \leq 1,  \end{cases} $$ for $n \in \mathbb{N}$, is a Cauchy sequence that fails to converge to any point $x$ in $(X, d)$. Now my question is, what is the gist of the process involved in the  construction of such examples? I mean what do we need to look for in the functions $x_n$ that would constitute a Cauchy but not a convergent sequence? Another example given by Kreyszig is in Prob. 13, Sec. 1.5. Here $x_n$ is defined as  $$ x_n(t) \colon= \begin{cases}  n \ & \mbox{ if } \ 0 \leq t \leq n^{-2}, \\  t^{-1/2} \ & \mbox{ if } \ n^{-2} \leq t \leq 1.    \end{cases}  $$ What is the gist of examples like this one? How do we generalise each one of these two examples to an arbitrary closed interval $[a, b]$, where $a$ and $b$ are some real numbers such that $a < b$?","Let $X$ denote the set of all the real (or complex) valued continuous functions on the closed interval $[0, 1]$, and let $$ d(x, y) \colon= \int_0^1 \lvert x(t) - y(t) \rvert \ \mathrm{d} t $$ for all $x, y \in X$. This $d$ is a metric on $X$, and $(X, d)$ is not complete, as has been shown by Kreyszig, for the sequence $\left( x_n \right)$, where  $$ x_n (t) \colon=  \begin{cases}  0 \ & \mbox{ if } \ 0 \leq t \leq \frac{1}{2}, \\  (m+2)\left( t - \frac{1}{2} \right) \ & \mbox{ if } \ \frac{1}{2} \leq t \leq \frac{1}{2} + \frac{1}{m+2}, \\ 1 \ & \mbox{ if } \ \frac{1}{2} + \frac{1}{m+2} \leq t \leq 1,  \end{cases} $$ for $n \in \mathbb{N}$, is a Cauchy sequence that fails to converge to any point $x$ in $(X, d)$. Now my question is, what is the gist of the process involved in the  construction of such examples? I mean what do we need to look for in the functions $x_n$ that would constitute a Cauchy but not a convergent sequence? Another example given by Kreyszig is in Prob. 13, Sec. 1.5. Here $x_n$ is defined as  $$ x_n(t) \colon= \begin{cases}  n \ & \mbox{ if } \ 0 \leq t \leq n^{-2}, \\  t^{-1/2} \ & \mbox{ if } \ n^{-2} \leq t \leq 1.    \end{cases}  $$ What is the gist of examples like this one? How do we generalise each one of these two examples to an arbitrary closed interval $[a, b]$, where $a$ and $b$ are some real numbers such that $a < b$?",,"['real-analysis', 'functional-analysis', 'analysis', 'metric-spaces', 'complete-spaces']"
37,"Equivalent definition of the variation of the function on $[a,b]$",Equivalent definition of the variation of the function on,"[a,b]","Let $f : \mathbb{R} \rightarrow \mathbb{R}$. The variation of $f$ over $[a,b] \subset \mathbb{R}$ is defined as $$ V_{a}^{b}(f) := \sup_{ \substack{ a = t_0 < t_1 < \ldots < t_n = b \\ n \in \mathbb{N}} } \sum_{i=1}^{n} |f(t_i) - f(t_{i-1})| $$ (i.e. the supremum is taken taken over all possible partitions $a = t_0 < t_1 < \ldots < t_n = b$, $n \in \mathbb{N}$ )? Is it true that $$V_{a}^{b}(f) = \lim_{\delta_n \rightarrow 0} \sum_{i=1}^{n}|f(t_i) -f(t_{i-1})|$$ where $\delta_n = \max_{1 \leq i \leq n}(t_i - t_{i-1}).$ Of course, it can easily be shown by the triangle that the sum increases, as new points are added to the partition. But still, how can one actually show that the supremum is also the limit as the distance of the points goes to zero?","Let $f : \mathbb{R} \rightarrow \mathbb{R}$. The variation of $f$ over $[a,b] \subset \mathbb{R}$ is defined as $$ V_{a}^{b}(f) := \sup_{ \substack{ a = t_0 < t_1 < \ldots < t_n = b \\ n \in \mathbb{N}} } \sum_{i=1}^{n} |f(t_i) - f(t_{i-1})| $$ (i.e. the supremum is taken taken over all possible partitions $a = t_0 < t_1 < \ldots < t_n = b$, $n \in \mathbb{N}$ )? Is it true that $$V_{a}^{b}(f) = \lim_{\delta_n \rightarrow 0} \sum_{i=1}^{n}|f(t_i) -f(t_{i-1})|$$ where $\delta_n = \max_{1 \leq i \leq n}(t_i - t_{i-1}).$ Of course, it can easily be shown by the triangle that the sum increases, as new points are added to the partition. But still, how can one actually show that the supremum is also the limit as the distance of the points goes to zero?",,"['calculus', 'analysis', 'stochastic-processes', 'stochastic-calculus', 'bounded-variation']"
38,On the solution of the functional equation $(x(t))^{p}u(t)=(x\circ f)(t)$,On the solution of the functional equation,(x(t))^{p}u(t)=(x\circ f)(t),"I am given a real number $p$ and two real valued functions $f(t),u(t)$ on $\mathbb{R}$ You can assume $u(t)$ and $f(t)$ are as nice as they need to be, including that they're invertible. I am trying to find a function $x(t)$ that solves the equation $$(x(t))^{p}u(t)=(x\circ f)(t).$$ Ideally, the solution for $x(t)$ is some expression involving $u(t)$ and $f(t).$ Also, observe if $t^{*}$ is a fixed point of $f$ and $p\neq1,$ then $$x(t^{*})=(u(t^{*}))^{\frac{1}{1-p}}.$$","I am given a real number $p$ and two real valued functions $f(t),u(t)$ on $\mathbb{R}$ You can assume $u(t)$ and $f(t)$ are as nice as they need to be, including that they're invertible. I am trying to find a function $x(t)$ that solves the equation $$(x(t))^{p}u(t)=(x\circ f)(t).$$ Ideally, the solution for $x(t)$ is some expression involving $u(t)$ and $f(t).$ Also, observe if $t^{*}$ is a fixed point of $f$ and $p\neq1,$ then $$x(t^{*})=(u(t^{*}))^{\frac{1}{1-p}}.$$",,['functional-analysis']
39,"Prob. 1, Chap. 6, in Baby Rudin: If $f(x_0)=1$ and $f(x)=0$ for all $x \neq x_0$, then $\int f\ \mathrm{d}\alpha=0$","Prob. 1, Chap. 6, in Baby Rudin: If  and  for all , then",f(x_0)=1 f(x)=0 x \neq x_0 \int f\ \mathrm{d}\alpha=0,"Here is Prob. 1, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $\alpha$ increases on $[a, b]$, $a \leq x_0 \leq b$, $\alpha$ is continuous at $x_0$,$f\left( x_0 \right) = 1$, and $f(x) = 0$ if $x \neq x_0$. Prove that $f \in \mathscr{R}(\alpha)$ and that $\int f \ \mathrm{d} \alpha = 0$. My Attempt: As $\alpha$ is continuous at $x_0$, so, given $\varepsilon > 0$, we can find a $\delta > 0$ such that $$ \left\lvert \alpha(t) - \alpha \left( x_0 \right) \right\rvert < \frac{\varepsilon}{4} \tag{*} $$   for all $t \in [a, b]$ for which $\left\lvert t-x_0 \right\rvert < \delta$. Let $n$ be a natural number such that $n > 2$, and let $P = \left\{ t_0, t_1, \ldots, t_{n-1}, t_n \right\}$ be a partition of $[a, b]$ such that $x_0$ is one of the points of $P$, and such that $\Delta t_j < \frac{\delta}{2}$ for each $j = 1, \ldots, n$. [We have included $x_0$ in $P$ in order to account for the case when $x_0$ is either of the endpoints of $[a, b]$.] Then, for any point $u_j \in \left[ t_{j-1}, t_j \right]$ ($1 \leq j \leq n$), we have    $$ \begin{align} & \left\lvert \sum_{j=1}^n f \left( u_j \right) \left[ \alpha \left( t_j \right) - \alpha \left( t_{j-1} \right) \right] \right\rvert \\  &\leq  \begin{cases}  \alpha\left( t_1 \right) - \alpha \left( t_0 \right)  \qquad \mbox{ if $x_0 = a$} \\ \left[ \alpha \left( t_{i+1} \right) - \alpha \left( t_i \right) \right] + \left[ \alpha \left( t_i \right) - \alpha \left( t_{i-1} \right) \right] = \alpha \left( t_{i+1} \right) - \alpha \left( t_{i-1} \right) \qquad \mbox{ if $x_0 \in (a, b)$ and $x_0 = t_i$ for some $i \in \{ 1, \ldots, n-1 \}$} \\ \alpha \left( t_n \right) - \alpha \left( t_{n-1} \right) \qquad \mbox{ if $x_0 = b$} \end{cases} \\ &< \frac{\varepsilon}{4}. \qquad \mbox{ [ by (*) above ] } \end{align} $$   Thus we can conclude that, for any choice of points $u_j \in \left[ t_{j-1}, t_j \right]$ ($1 \leq j \leq n$), we have    $$ - \frac{\varepsilon}{4} \leq \sum_{j=1}^n f \left( u_j \right) \Delta \alpha_j \leq  \frac{\varepsilon}{4}, \tag{0} $$ But    $$ L(P, f, \alpha) = \inf \left\{ \ \sum_{j=1}^n f\left( u_j \right)  \left[ \alpha \left( t_j \right) - \alpha \left( t_{j-1} \right) \right] \ \colon \ u_j \in \left[ t_{j-1}, t_j \right]  \ \mbox{ for } \ j = 1, \ldots, n \ \right\}, \tag{A}$$   and    $$ U(P, f, \alpha) = \sup  \left\{ \ \sum_{j=1}^n f\left( u_j \right)  \left[ \alpha \left( t_j \right) - \alpha \left( t_{j-1} \right) \right] \ \colon \ u_j \in \left[ t_{j-1}, t_j \right]  \ \mbox{ for } \ j = 1, \ldots, n \ \right\}. \tag{B}$$ Here is the link to my post here on Math SE on how we can obtain (A) and (B) Riemann-Stieltjes Upper and Lower Sums as Suprema and Infima So from (0) we can conclude that    $$ -\frac{\varepsilon}{4} \leq L(P, f, \alpha) \leq U(P, f, \alpha) \leq \frac{\varepsilon}{4}. \tag{1}$$ Now from (1) we obtain    $$ U(P, f, \alpha) - L(P, f, \alpha) \leq \frac{\varepsilon}{2} < \varepsilon; $$   but as $\varepsilon$ is an arbitrary positive real number, so the last set of inequalities, by virtue of Theorem 6.6 in Baby Rudin, implies that $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Moreover, as    $$ \int_a^b f \ \mathrm{d} \alpha = \sup \left\{ \ L(Q, f, \alpha) \ \colon \ \mbox{ Q is a partition of $[a, b]$ } \ \right\} =  \inf \left\{ \ U(Q, f, \alpha) \ \colon \ \mbox{ Q is a partition of $[a, b]$ } \ \right\}, $$    so we must also have    $$ L(P, f, \alpha ) \leq  \int_a^b f \ \mathrm{d} \alpha \leq U(P, f, \alpha). \tag{2} $$ So from (1) and (2), we obtain    $$ - \frac{\varepsilon}{4} \leq  \int_a^b f \ \mathrm{d} \alpha \leq  \frac{\varepsilon}{4}, $$   which implies that    $$ \left\lvert  \int_a^b f \ \mathrm{d} \alpha  \right\rvert < \varepsilon. \tag{3} $$ But $\varepsilon$ was an arbitrary positive real number. So from (3) we can conclude that    $$  \int_a^b f \ \mathrm{d} \alpha = 0, $$   as required. Is my proof good enough? Or, are there any issues with its logic, rigor, or presentation?","Here is Prob. 1, Chap. 6, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $\alpha$ increases on $[a, b]$, $a \leq x_0 \leq b$, $\alpha$ is continuous at $x_0$,$f\left( x_0 \right) = 1$, and $f(x) = 0$ if $x \neq x_0$. Prove that $f \in \mathscr{R}(\alpha)$ and that $\int f \ \mathrm{d} \alpha = 0$. My Attempt: As $\alpha$ is continuous at $x_0$, so, given $\varepsilon > 0$, we can find a $\delta > 0$ such that $$ \left\lvert \alpha(t) - \alpha \left( x_0 \right) \right\rvert < \frac{\varepsilon}{4} \tag{*} $$   for all $t \in [a, b]$ for which $\left\lvert t-x_0 \right\rvert < \delta$. Let $n$ be a natural number such that $n > 2$, and let $P = \left\{ t_0, t_1, \ldots, t_{n-1}, t_n \right\}$ be a partition of $[a, b]$ such that $x_0$ is one of the points of $P$, and such that $\Delta t_j < \frac{\delta}{2}$ for each $j = 1, \ldots, n$. [We have included $x_0$ in $P$ in order to account for the case when $x_0$ is either of the endpoints of $[a, b]$.] Then, for any point $u_j \in \left[ t_{j-1}, t_j \right]$ ($1 \leq j \leq n$), we have    $$ \begin{align} & \left\lvert \sum_{j=1}^n f \left( u_j \right) \left[ \alpha \left( t_j \right) - \alpha \left( t_{j-1} \right) \right] \right\rvert \\  &\leq  \begin{cases}  \alpha\left( t_1 \right) - \alpha \left( t_0 \right)  \qquad \mbox{ if $x_0 = a$} \\ \left[ \alpha \left( t_{i+1} \right) - \alpha \left( t_i \right) \right] + \left[ \alpha \left( t_i \right) - \alpha \left( t_{i-1} \right) \right] = \alpha \left( t_{i+1} \right) - \alpha \left( t_{i-1} \right) \qquad \mbox{ if $x_0 \in (a, b)$ and $x_0 = t_i$ for some $i \in \{ 1, \ldots, n-1 \}$} \\ \alpha \left( t_n \right) - \alpha \left( t_{n-1} \right) \qquad \mbox{ if $x_0 = b$} \end{cases} \\ &< \frac{\varepsilon}{4}. \qquad \mbox{ [ by (*) above ] } \end{align} $$   Thus we can conclude that, for any choice of points $u_j \in \left[ t_{j-1}, t_j \right]$ ($1 \leq j \leq n$), we have    $$ - \frac{\varepsilon}{4} \leq \sum_{j=1}^n f \left( u_j \right) \Delta \alpha_j \leq  \frac{\varepsilon}{4}, \tag{0} $$ But    $$ L(P, f, \alpha) = \inf \left\{ \ \sum_{j=1}^n f\left( u_j \right)  \left[ \alpha \left( t_j \right) - \alpha \left( t_{j-1} \right) \right] \ \colon \ u_j \in \left[ t_{j-1}, t_j \right]  \ \mbox{ for } \ j = 1, \ldots, n \ \right\}, \tag{A}$$   and    $$ U(P, f, \alpha) = \sup  \left\{ \ \sum_{j=1}^n f\left( u_j \right)  \left[ \alpha \left( t_j \right) - \alpha \left( t_{j-1} \right) \right] \ \colon \ u_j \in \left[ t_{j-1}, t_j \right]  \ \mbox{ for } \ j = 1, \ldots, n \ \right\}. \tag{B}$$ Here is the link to my post here on Math SE on how we can obtain (A) and (B) Riemann-Stieltjes Upper and Lower Sums as Suprema and Infima So from (0) we can conclude that    $$ -\frac{\varepsilon}{4} \leq L(P, f, \alpha) \leq U(P, f, \alpha) \leq \frac{\varepsilon}{4}. \tag{1}$$ Now from (1) we obtain    $$ U(P, f, \alpha) - L(P, f, \alpha) \leq \frac{\varepsilon}{2} < \varepsilon; $$   but as $\varepsilon$ is an arbitrary positive real number, so the last set of inequalities, by virtue of Theorem 6.6 in Baby Rudin, implies that $f \in \mathscr{R}(\alpha)$ on $[a, b]$. Moreover, as    $$ \int_a^b f \ \mathrm{d} \alpha = \sup \left\{ \ L(Q, f, \alpha) \ \colon \ \mbox{ Q is a partition of $[a, b]$ } \ \right\} =  \inf \left\{ \ U(Q, f, \alpha) \ \colon \ \mbox{ Q is a partition of $[a, b]$ } \ \right\}, $$    so we must also have    $$ L(P, f, \alpha ) \leq  \int_a^b f \ \mathrm{d} \alpha \leq U(P, f, \alpha). \tag{2} $$ So from (1) and (2), we obtain    $$ - \frac{\varepsilon}{4} \leq  \int_a^b f \ \mathrm{d} \alpha \leq  \frac{\varepsilon}{4}, $$   which implies that    $$ \left\lvert  \int_a^b f \ \mathrm{d} \alpha  \right\rvert < \varepsilon. \tag{3} $$ But $\varepsilon$ was an arbitrary positive real number. So from (3) we can conclude that    $$  \int_a^b f \ \mathrm{d} \alpha = 0, $$   as required. Is my proof good enough? Or, are there any issues with its logic, rigor, or presentation?",,"['real-analysis', 'integration', 'analysis', 'proof-verification', 'definite-integrals']"
40,Functional and Borel measure,Functional and Borel measure,,"Let $g:\mathbb R\to\mathbb R$, $g\in V$ Define functional $F:V \times\mathbb R\to\mathbb R$ $$(g,x)\mapsto F(g,x)$$ Since $\forall \text{operator }  T\  \exists\mu$ s.t. $  T(f)=\int_{\mathbb R} f \, d\mu$, where $\mu$ is a complex Borel measure, Is it possible to prove that $\forall  F\  \exists\mu$ s.t. $  F(g,x)=\int_{-\infty}^x g \, d\mu$? or any similar forms. On what kind of text I could learn more no this topic? After some reading I find a possible way of doing this: For any operator $F$ there exists another operator $T_x:V\to V$ s.t. $F(g,x)=T_x(g)$ then, if $T_x$ is continuous and linear, $$T_x(g)=\int_\mathbb R gd\mu$$ where $\mu:\mathbb R\to\mathbb R$ is a Bounded variation (BV) function. I am not sure that, in this case, whether $\mu$ has to be a complex Borel measure or just BV function is enough?","Let $g:\mathbb R\to\mathbb R$, $g\in V$ Define functional $F:V \times\mathbb R\to\mathbb R$ $$(g,x)\mapsto F(g,x)$$ Since $\forall \text{operator }  T\  \exists\mu$ s.t. $  T(f)=\int_{\mathbb R} f \, d\mu$, where $\mu$ is a complex Borel measure, Is it possible to prove that $\forall  F\  \exists\mu$ s.t. $  F(g,x)=\int_{-\infty}^x g \, d\mu$? or any similar forms. On what kind of text I could learn more no this topic? After some reading I find a possible way of doing this: For any operator $F$ there exists another operator $T_x:V\to V$ s.t. $F(g,x)=T_x(g)$ then, if $T_x$ is continuous and linear, $$T_x(g)=\int_\mathbb R gd\mu$$ where $\mu:\mathbb R\to\mathbb R$ is a Bounded variation (BV) function. I am not sure that, in this case, whether $\mu$ has to be a complex Borel measure or just BV function is enough?",,"['functional-analysis', 'analysis', 'measure-theory', 'operator-theory', 'borel-sets']"
41,Question about baby rudin theorem 5.12 corollary [duplicate],Question about baby rudin theorem 5.12 corollary [duplicate],,"This question already has answers here : Discontinuities of the derivative of a differentiable function on closed interval (2 answers) Closed 6 years ago . The corollary says if $ f $ is differentiable on $ [a,b] $ then $ f' $ cannot have any simple discontinuities on $[a,b] $. I just don't how to prove it. I think it should be proved on both two cases of simple discontinuities(first type and second type of simple discontinuities). Thanks in advance.","This question already has answers here : Discontinuities of the derivative of a differentiable function on closed interval (2 answers) Closed 6 years ago . The corollary says if $ f $ is differentiable on $ [a,b] $ then $ f' $ cannot have any simple discontinuities on $[a,b] $. I just don't how to prove it. I think it should be proved on both two cases of simple discontinuities(first type and second type of simple discontinuities). Thanks in advance.",,"['real-analysis', 'analysis', 'derivatives', 'continuity']"
42,"Convergence rate for $\int_{\{f\ge n\}}|f(x)|\,dx$ for $f\in L^1(\Omega)$",Convergence rate for  for,"\int_{\{f\ge n\}}|f(x)|\,dx f\in L^1(\Omega)","Let $\Omega\subset\mathbb R^d$ be open and bounded and $f\in L^1(\Omega)$. Consider the following sequence indedxed by $n\in\mathbb N$: $$ \int_{\{f\ge n\}}|f(x)|\,dx .$$ This sequence converges to zero by the Beppo Levi-theorem.  My question: Can one find an explicit convergence rate which is independent of $f$? More specifically, I'm looking for something along the lines of $$\int_{\{f\ge n\}}|f(x)|\,dx\leq C_f n^{-\alpha},$$  with some $\alpha>0$ and a constant $C_f$ which is allowed to depend on $f$.","Let $\Omega\subset\mathbb R^d$ be open and bounded and $f\in L^1(\Omega)$. Consider the following sequence indedxed by $n\in\mathbb N$: $$ \int_{\{f\ge n\}}|f(x)|\,dx .$$ This sequence converges to zero by the Beppo Levi-theorem.  My question: Can one find an explicit convergence rate which is independent of $f$? More specifically, I'm looking for something along the lines of $$\int_{\{f\ge n\}}|f(x)|\,dx\leq C_f n^{-\alpha},$$  with some $\alpha>0$ and a constant $C_f$ which is allowed to depend on $f$.",,"['integration', 'analysis', 'measure-theory', 'convergence-divergence', 'lp-spaces']"
43,How to obtain uniform bounds on a polynomial by looking at its coefficients.,How to obtain uniform bounds on a polynomial by looking at its coefficients.,,"To be more precise, let $p:[0,1]\to \mathbb R$ be a polynomial of degree $n$. Picking a basis of polynomials (e.g. monomials based at $0$), we can represent $p$ $$p(x)=\sum_{j=0}^na_jx^j,$$ for some unique coefficients $a_j\in \mathbb R$. Now, as $p$ is a continuous function on a compact set, it is bounded. I would like to be able to obtain relatively sharp information about the bound on $p$ by looking at the above coefficients, $a_j$. For example, one has $$\|p\|_{\infty}\leq\sum_{j=1}^n |a_j|\leq n\max_j|a_j|.\quad(\star)$$ But in general, this will clearly be a bad bound as one can have polynomials with many large (in absolute value) coefficients which manages to be small due to cancellations of positive and negative coefficients. So my (unfortunately rather vague) question is how can one get bounds sharper than the brute-force $(\star)$? Ideally, something that doesn't rely upon the one-dimensional nature of the domain. Perhaps [see Olivier below] my life would be easier with a different choice of basis polynomials?","To be more precise, let $p:[0,1]\to \mathbb R$ be a polynomial of degree $n$. Picking a basis of polynomials (e.g. monomials based at $0$), we can represent $p$ $$p(x)=\sum_{j=0}^na_jx^j,$$ for some unique coefficients $a_j\in \mathbb R$. Now, as $p$ is a continuous function on a compact set, it is bounded. I would like to be able to obtain relatively sharp information about the bound on $p$ by looking at the above coefficients, $a_j$. For example, one has $$\|p\|_{\infty}\leq\sum_{j=1}^n |a_j|\leq n\max_j|a_j|.\quad(\star)$$ But in general, this will clearly be a bad bound as one can have polynomials with many large (in absolute value) coefficients which manages to be small due to cancellations of positive and negative coefficients. So my (unfortunately rather vague) question is how can one get bounds sharper than the brute-force $(\star)$? Ideally, something that doesn't rely upon the one-dimensional nature of the domain. Perhaps [see Olivier below] my life would be easier with a different choice of basis polynomials?",,"['real-analysis', 'analysis', 'polynomials']"
44,Finding $f$ such that $f'(x)=A(x)$,Finding  such that,f f'(x)=A(x),"The problem is the following: Let $U\subset \Bbb R^m$ open and simply connected and $A: U \to \mathcal{L}(\Bbb R^m; \Bbb R^n)$ a differentiable application. Show that there exists $f:U\to\Bbb R^n$ twice differentiable satisfying $f'(x)=A(x)$ for all $x\in U$ if and only if $A$ satisfies $\big( A'(x) v \big) w = \big( A'(x) w \big) v$ for all $v, w\in\Bbb R^m$. The first part is easy and follows from the Schwarz Theorem for applications, since $A'(x)=f''(x)$. I aim to show the converse, but couldn't find a candidate for $f$. Any hints?  Further, why do we need the simply connected hypothesis?","The problem is the following: Let $U\subset \Bbb R^m$ open and simply connected and $A: U \to \mathcal{L}(\Bbb R^m; \Bbb R^n)$ a differentiable application. Show that there exists $f:U\to\Bbb R^n$ twice differentiable satisfying $f'(x)=A(x)$ for all $x\in U$ if and only if $A$ satisfies $\big( A'(x) v \big) w = \big( A'(x) w \big) v$ for all $v, w\in\Bbb R^m$. The first part is easy and follows from the Schwarz Theorem for applications, since $A'(x)=f''(x)$. I aim to show the converse, but couldn't find a candidate for $f$. Any hints?  Further, why do we need the simply connected hypothesis?",,"['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives', 'linear-transformations']"
45,Proof verification : $f$ is increasing at $x_0$.,Proof verification :  is increasing at .,f x_0,"Q. Let $f:(a,b) \rightarrow \mathbb R$ and $x_0 \in (a,b)$. $f$ is differentiable function. Determine the relationship of following statements. (a) $f^\prime (x_0) \gt 0$ (b) $f$ is increasing at $x_0$ (c) $f$ is increasing function on some intervals that contain $x_0$. My proof (a) $\rightarrow$ (b) $\lim_{h \to 0} {{f(x_0 +h) - f(x_0)} \over h} = \lim_{h \to 0+} {{f(x_0 +h) - f(x_0)} \over h} = \lim_{h \to 0-} {{f(x_0 +h) - f(x_0)} \over h} = \alpha \gt 0$ Therefore, there exists $\delta \gt 0$ s.t. $0 \lt h \lt \delta \Rightarrow f(x_0+h) - f(x_0) \gt 0$ and $-\delta \lt h \lt 0 \Rightarrow f(x_0+h) - f(x_0) \lt 0$. So $f$ is increasing at $x_0$. (b) $\rightarrow$ (c) Using the above notation of $\delta$, it follows that $f$ is increasing function on $N(x_0, \delta)$. (c) $\rightarrow$ (a) If $f$ is increasing function on $N(x_0, \delta)$, then $\lim_{h \to 0+} {{f(x_0 +h) - f(x_0)} \over h} = \lim_{h \to 0-} {{f(x_0 +h) - f(x_0)} \over h} \gt 0$. Therefore $f^\prime (x_0) \gt 0$. (a), (b), (c) is equivalent. I heard that (b) $\rightarrow$ (c) can be false in some functions, but as $f$ is differentiable, I think it doesn't matter. Did I proved it right? Do I have to add some more?","Q. Let $f:(a,b) \rightarrow \mathbb R$ and $x_0 \in (a,b)$. $f$ is differentiable function. Determine the relationship of following statements. (a) $f^\prime (x_0) \gt 0$ (b) $f$ is increasing at $x_0$ (c) $f$ is increasing function on some intervals that contain $x_0$. My proof (a) $\rightarrow$ (b) $\lim_{h \to 0} {{f(x_0 +h) - f(x_0)} \over h} = \lim_{h \to 0+} {{f(x_0 +h) - f(x_0)} \over h} = \lim_{h \to 0-} {{f(x_0 +h) - f(x_0)} \over h} = \alpha \gt 0$ Therefore, there exists $\delta \gt 0$ s.t. $0 \lt h \lt \delta \Rightarrow f(x_0+h) - f(x_0) \gt 0$ and $-\delta \lt h \lt 0 \Rightarrow f(x_0+h) - f(x_0) \lt 0$. So $f$ is increasing at $x_0$. (b) $\rightarrow$ (c) Using the above notation of $\delta$, it follows that $f$ is increasing function on $N(x_0, \delta)$. (c) $\rightarrow$ (a) If $f$ is increasing function on $N(x_0, \delta)$, then $\lim_{h \to 0+} {{f(x_0 +h) - f(x_0)} \over h} = \lim_{h \to 0-} {{f(x_0 +h) - f(x_0)} \over h} \gt 0$. Therefore $f^\prime (x_0) \gt 0$. (a), (b), (c) is equivalent. I heard that (b) $\rightarrow$ (c) can be false in some functions, but as $f$ is differentiable, I think it doesn't matter. Did I proved it right? Do I have to add some more?",,"['calculus', 'analysis']"
46,Radon-Nikodym application,Radon-Nikodym application,,"Let $ \mathcal{M}$ be the set of $\sigma$-algebra of Lebesgue measurable subsets of $ \mathbb{R} $. Let $f:\mathbb{R} \rightarrow (0,+\infty)$ be measurable and $g \in L^1(\mathbb{R}) $. For $E \in \mathcal{M}$, define   \begin{equation*} \mu(E)= \int_E f(x)dx \text{ and } \lambda(E)= \int_E g(x)dx \end{equation*}   whenever $ E \in \mathcal{M} $. Show $ \lambda << \mu $. Find $ d\lambda/d\mu $. if we write f= $\frac{d\mu}{dx} $ and g= $\frac{d\lambda}{dx} $ then $\frac{g}{f} = \dfrac{\frac{d\lambda}{dx}}{\frac{d\mu}{dx}} = \frac{d\lambda}{d\mu}$. So the Radon-Nikodym derivative of $ \lambda $ with respect to $ \mu $ is $\frac{d\lambda}{d\mu}=\dfrac{g(x)}{f(x)}$. I need some help to show that $ \lambda << \mu $. Given $E \in \mathcal{M}$ such that $\mu(E)=0$, we want to show that $\lambda(E)=0$","Let $ \mathcal{M}$ be the set of $\sigma$-algebra of Lebesgue measurable subsets of $ \mathbb{R} $. Let $f:\mathbb{R} \rightarrow (0,+\infty)$ be measurable and $g \in L^1(\mathbb{R}) $. For $E \in \mathcal{M}$, define   \begin{equation*} \mu(E)= \int_E f(x)dx \text{ and } \lambda(E)= \int_E g(x)dx \end{equation*}   whenever $ E \in \mathcal{M} $. Show $ \lambda << \mu $. Find $ d\lambda/d\mu $. if we write f= $\frac{d\mu}{dx} $ and g= $\frac{d\lambda}{dx} $ then $\frac{g}{f} = \dfrac{\frac{d\lambda}{dx}}{\frac{d\mu}{dx}} = \frac{d\lambda}{d\mu}$. So the Radon-Nikodym derivative of $ \lambda $ with respect to $ \mu $ is $\frac{d\lambda}{d\mu}=\dfrac{g(x)}{f(x)}$. I need some help to show that $ \lambda << \mu $. Given $E \in \mathcal{M}$ such that $\mu(E)=0$, we want to show that $\lambda(E)=0$",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
47,Find solution of maximum of a function with conditions,Find solution of maximum of a function with conditions,,"\begin{equation} \label{eqgen} \begin{aligned} & \underset{x}{\text{maximize}} & & F(x)\\ & \text{subject to} & & x \geq 0, \\ & & &x + f(x)\leq d, \end{aligned} \end{equation}   where $F(x), f(x)$ are continuous functions from $\mathbb{R}^+ \to \mathbb{R}^+$. Here is my approach. Intuitively, $x+f(x)\leq d, x\geq 0$ is equivalent to $x \in [a_1, b_1] \cup [a_2, b_2]\cup \cdots \cup [a_{n}, b_{n}]$, where $a_i\leq b_i$, $a_1$ is either 0 or solution of $x+f(x) = d$, $b_n$ is either d or solution of $x+f(x) = d$, and $a_i, b_i$ are solutions of $x+f(x) = d$ for other cases. Hence, the optimization problem becomes \begin{equation}  \begin{aligned} & \underset{x}{\text{maximize}} & & F(x)\\ & \text{subject to} & & x \in  [a_1, b_1] \cup [a_2, b_2]\cup \cdots \cup [a_{n}, b_{n}]. \end{aligned} \end{equation} max $F(x)$ on $[a_i, b_i]$ is attained at either $a_i, b_i$ or maximum of $F(x)$ on $[a_i, b_i]$. We can conclude that the optimal solution is either solutions of $x+f(x) = d, or x=0, x= d$  or local maximum of function $F(x)$. This is also what I want to prove. How can I formally write down the solution of the above approach? I wrote this and my professor does not accept my solution. Thank you in advance!","\begin{equation} \label{eqgen} \begin{aligned} & \underset{x}{\text{maximize}} & & F(x)\\ & \text{subject to} & & x \geq 0, \\ & & &x + f(x)\leq d, \end{aligned} \end{equation}   where $F(x), f(x)$ are continuous functions from $\mathbb{R}^+ \to \mathbb{R}^+$. Here is my approach. Intuitively, $x+f(x)\leq d, x\geq 0$ is equivalent to $x \in [a_1, b_1] \cup [a_2, b_2]\cup \cdots \cup [a_{n}, b_{n}]$, where $a_i\leq b_i$, $a_1$ is either 0 or solution of $x+f(x) = d$, $b_n$ is either d or solution of $x+f(x) = d$, and $a_i, b_i$ are solutions of $x+f(x) = d$ for other cases. Hence, the optimization problem becomes \begin{equation}  \begin{aligned} & \underset{x}{\text{maximize}} & & F(x)\\ & \text{subject to} & & x \in  [a_1, b_1] \cup [a_2, b_2]\cup \cdots \cup [a_{n}, b_{n}]. \end{aligned} \end{equation} max $F(x)$ on $[a_i, b_i]$ is attained at either $a_i, b_i$ or maximum of $F(x)$ on $[a_i, b_i]$. We can conclude that the optimal solution is either solutions of $x+f(x) = d, or x=0, x= d$  or local maximum of function $F(x)$. This is also what I want to prove. How can I formally write down the solution of the above approach? I wrote this and my professor does not accept my solution. Thank you in advance!",,"['calculus', 'real-analysis', 'analysis', 'optimization', 'nonlinear-optimization']"
48,Tensor product of $L^2$ spaces,Tensor product of  spaces,L^2,"For Tensor-products on $L^2$ spaces I am Aware of the following property: I know that $L^2(X,\mathbb{R})\otimes L^2(X,\mathbb{R}) \simeq L^2(X^2,\mathbb{R}).$ I was wondering if this is also true for Hilbert-space valued function functions, i.e. $L^2(X,H) \otimes L^2(X,H) \simeq L^2(X^2,H)$? If anything is unclear, please let me know. The problem seems to be that $L^2(X,H) \simeq L^2(X,\mathbb{C}) \otimes H.$ Thus, $L^2(X^2,H) \simeq L^2(X^2,\mathbb{C}) \otimes H.$ The left-hand side however is $L^2(X,\mathbb{C}) \otimes H\otimes L^2(X,\mathbb{C}) \otimes H.$","For Tensor-products on $L^2$ spaces I am Aware of the following property: I know that $L^2(X,\mathbb{R})\otimes L^2(X,\mathbb{R}) \simeq L^2(X^2,\mathbb{R}).$ I was wondering if this is also true for Hilbert-space valued function functions, i.e. $L^2(X,H) \otimes L^2(X,H) \simeq L^2(X^2,H)$? If anything is unclear, please let me know. The problem seems to be that $L^2(X,H) \simeq L^2(X,\mathbb{C}) \otimes H.$ Thus, $L^2(X^2,H) \simeq L^2(X^2,\mathbb{C}) \otimes H.$ The left-hand side however is $L^2(X,\mathbb{C}) \otimes H\otimes L^2(X,\mathbb{C}) \otimes H.$",,"['real-analysis', 'functional-analysis']"
49,"Prob. 7, Chap. 5 in Baby Rudin: If $f^\prime(x)$, $g^\prime(x)$ exist, $g^\prime(x)\neq 0$ and $f(x)=g(x)=0$, ...","Prob. 7, Chap. 5 in Baby Rudin: If ,  exist,  and , ...",f^\prime(x) g^\prime(x) g^\prime(x)\neq 0 f(x)=g(x)=0,"Here is Prob. 7, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f^\prime(x)$, $g^\prime(x)$ exist, $g^\prime(x) \neq 0$, and $f(x) = g(x) = 0$. Prove that $$ \lim_{t \to x} \frac{ f(t) }{g(t)} = \frac{f^\prime(x)}{g^\prime(x)}.$$ (This holds also for complex functions.) My Attempt: As $f(x) = g(x) = 0$ ans as $f^\prime(x)$ and $g^\prime(x)$ exist, with $g^\prime(x) \neq 0$, so we have    $$ \begin{align} \lim_{t\to x} \frac{f(t)}{g(t)} &= \lim_{t\to x} \frac{f(t) - 0 }{g(t) - 0 } \\ &= \lim_{t \to x} \frac{ f(t) - f(x) }{ g(t) - g(x) } \\ &= \lim_{t \to x} \frac{ \frac{f(t) - f(x)}{t-x}}{ \frac{g(t)-g(x)}{t-x}} \\ &= \frac{\lim_{t \to x} \left(  \frac{f(t) - f(x)}{t-x} \right) }{ \lim_{t \to x} \left( \frac{g(t)-g(x)}{t-x} \right) } \\ &= \frac{f^\prime(x)}{g^\prime(x)}. \end{align} $$ Is this reasoning correct? If so, I reckon the above calculation is valid for complex as well as real functions $f$ and $g$. Am I right? I'm afraid we cannot resort to the Generalized Mean Value Theorem here even for real functions $f$ and $g$ because we have no information about the continuity of $f$ and $g$ in any interval containing $x$ or about the differentiability of these functions in a segment around $x$. Please refer to a questions of mine at Math SE at the following link. Any example of a function which is discontinuous at each point in a deleted neighborhood of a point at which that function is differentiable? Am I right?","Here is Prob. 7, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f^\prime(x)$, $g^\prime(x)$ exist, $g^\prime(x) \neq 0$, and $f(x) = g(x) = 0$. Prove that $$ \lim_{t \to x} \frac{ f(t) }{g(t)} = \frac{f^\prime(x)}{g^\prime(x)}.$$ (This holds also for complex functions.) My Attempt: As $f(x) = g(x) = 0$ ans as $f^\prime(x)$ and $g^\prime(x)$ exist, with $g^\prime(x) \neq 0$, so we have    $$ \begin{align} \lim_{t\to x} \frac{f(t)}{g(t)} &= \lim_{t\to x} \frac{f(t) - 0 }{g(t) - 0 } \\ &= \lim_{t \to x} \frac{ f(t) - f(x) }{ g(t) - g(x) } \\ &= \lim_{t \to x} \frac{ \frac{f(t) - f(x)}{t-x}}{ \frac{g(t)-g(x)}{t-x}} \\ &= \frac{\lim_{t \to x} \left(  \frac{f(t) - f(x)}{t-x} \right) }{ \lim_{t \to x} \left( \frac{g(t)-g(x)}{t-x} \right) } \\ &= \frac{f^\prime(x)}{g^\prime(x)}. \end{align} $$ Is this reasoning correct? If so, I reckon the above calculation is valid for complex as well as real functions $f$ and $g$. Am I right? I'm afraid we cannot resort to the Generalized Mean Value Theorem here even for real functions $f$ and $g$ because we have no information about the continuity of $f$ and $g$ in any interval containing $x$ or about the differentiability of these functions in a segment around $x$. Please refer to a questions of mine at Math SE at the following link. Any example of a function which is discontinuous at each point in a deleted neighborhood of a point at which that function is differentiable? Am I right?",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
50,Folland Chapter 6 Problem 23b,Folland Chapter 6 Problem 23b,,"Let $(X,\mathcal{M},\mu)$ be a measure space. A set $E \in \mathcal{M}$ is called ""locally null"" if $\mu(E\cap F) = 0$ for every $F \in \mathcal{M}$ such that $\mu(F) < \infty$ . For $f: X \to \mathbb{C}$ measurable, define $\|f\|_* = \inf\{a : \{x : |f(x)| > a\} \text{ is locally null}\}$ . Problem 23b asks to show $\|\cdot\|_*$ is a norm on the set of $f$ such that $\|f\|_* < \infty$ (modded out by $f=g$ iff $f=g$ a.e.). In particular, we must have $\|f\|_* \ge 0$ . I claim this does not always hold. For example, consider $X = \mathbb{R}, \mathcal{M} = \mathcal{P}(X)$ , and $\mu(A) = 0$ if $A$ is at most countable, and $\mu(A) = \infty$ otherwise. Then $X$ itself is locally null so any $a < 0$ satisfies $\{x : |f(x)| > a\}$ is locally null. The author gives no restriction on the measure space. Is the problem flawed?","Let be a measure space. A set is called ""locally null"" if for every such that . For measurable, define . Problem 23b asks to show is a norm on the set of such that (modded out by iff a.e.). In particular, we must have . I claim this does not always hold. For example, consider , and if is at most countable, and otherwise. Then itself is locally null so any satisfies is locally null. The author gives no restriction on the measure space. Is the problem flawed?","(X,\mathcal{M},\mu) E \in \mathcal{M} \mu(E\cap F) = 0 F \in \mathcal{M} \mu(F) < \infty f: X \to \mathbb{C} \|f\|_* = \inf\{a : \{x : |f(x)| > a\} \text{ is locally null}\} \|\cdot\|_* f \|f\|_* < \infty f=g f=g \|f\|_* \ge 0 X = \mathbb{R}, \mathcal{M} = \mathcal{P}(X) \mu(A) = 0 A \mu(A) = \infty X a < 0 \{x : |f(x)| > a\}","['real-analysis', 'analysis', 'measure-theory']"
51,"Show that if A is a open set or a closed set, then $int(\partial A) = \emptyset$. Also find if the converse is true.","Show that if A is a open set or a closed set, then . Also find if the converse is true.",int(\partial A) = \emptyset,"Let $\partial A$ be a set of boundary points of $A\subset\mathbb{R}^n$ . Show that if A is a open set or a closed set, then $\text{int}(\partial A)=\emptyset$ . Also find if the converse is true. My answer) Suppose A is a open set. Then $A \cap \partial A = \emptyset$ . (This was mentioned in previous problem, and I already proved it.) If $x \in \text{int}(\partial A)$ , then there exists $\epsilon>0$ s.t. $N(x, \epsilon) \subset \partial A$ . However, this means $N(x, \epsilon) \cap A = \emptyset$ , and $x$ cannot be a boundary point. Therefore it is contradiction. Now, suppose A is a closed set. Then, $\partial A \subset A$ . (this is also mentioned in previous problem). If $x \in \text{int}(\partial A)$ , then there exists $\epsilon>0$ s.t. $N(x, \epsilon) \subset \partial A \subset A$ . This means $N(x, \epsilon) \cap A^\complement = \emptyset$ , and $x$ cannot be a boundary point. This is contradiction, too. Thus, $\text{int}(\partial A) = \emptyset$ . This is what I've got so far. The problem is I can't prove the converse. My hunch is that it is related with $A \subset \text{int}(A) \cup \partial A$ , but I can't progress the idea.","Let be a set of boundary points of . Show that if A is a open set or a closed set, then . Also find if the converse is true. My answer) Suppose A is a open set. Then . (This was mentioned in previous problem, and I already proved it.) If , then there exists s.t. . However, this means , and cannot be a boundary point. Therefore it is contradiction. Now, suppose A is a closed set. Then, . (this is also mentioned in previous problem). If , then there exists s.t. . This means , and cannot be a boundary point. This is contradiction, too. Thus, . This is what I've got so far. The problem is I can't prove the converse. My hunch is that it is related with , but I can't progress the idea.","\partial A A\subset\mathbb{R}^n \text{int}(\partial A)=\emptyset A \cap \partial A = \emptyset x \in \text{int}(\partial A) \epsilon>0 N(x, \epsilon) \subset \partial A N(x, \epsilon) \cap A = \emptyset x \partial A \subset A x \in \text{int}(\partial A) \epsilon>0 N(x, \epsilon) \subset \partial A \subset A N(x, \epsilon) \cap A^\complement = \emptyset x \text{int}(\partial A) = \emptyset A \subset \text{int}(A) \cup \partial A","['general-topology', 'analysis']"
52,Everywhere singular Jacobian implies non injectivity?,Everywhere singular Jacobian implies non injectivity?,,"When I was studying analysis some time ago, the next problem arose: Let $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$ be a differentiable function, not necessarily $C^1$, such that $\det(f'(a))=0$, for all $a$ in $\mathbb{R}^n$, where $f'(a)$ stands for the Jacobian matrix at $a$.   Is $f$ not injective? I do not have a proof, but I do have the conviction that this must be true. If $f$ is $C^1$, we can find, for each point, a direction in wich the directional derivative is 0, and the directions change smoothly. Then we could find an integral curve where all the points have the same image. I don't have a formal proof of this idea either. I think there must be a simpler argument that solves the general case. We do not need a whole curve of points with the same image, just two of them, so maybe the argument can be simplified. Any sugestiond through a solution or even a complete proof would be highly apreciated.","When I was studying analysis some time ago, the next problem arose: Let $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$ be a differentiable function, not necessarily $C^1$, such that $\det(f'(a))=0$, for all $a$ in $\mathbb{R}^n$, where $f'(a)$ stands for the Jacobian matrix at $a$.   Is $f$ not injective? I do not have a proof, but I do have the conviction that this must be true. If $f$ is $C^1$, we can find, for each point, a direction in wich the directional derivative is 0, and the directions change smoothly. Then we could find an integral curve where all the points have the same image. I don't have a formal proof of this idea either. I think there must be a simpler argument that solves the general case. We do not need a whole curve of points with the same image, just two of them, so maybe the argument can be simplified. Any sugestiond through a solution or even a complete proof would be highly apreciated.",,"['analysis', 'multivariable-calculus', 'determinant', 'problem-solving', 'jacobian']"
53,Distributional limit.,Distributional limit.,,I`m preparing for an exam and I have this question: how can I find the distributional limit for something like for $f\in L^1 (R)$ : $$\lim_{t \rightarrow 0} \int^{\infty}_0 f(x) \cos \left(t \sqrt x\right) dx $$ $$\lim_{t\rightarrow \infty} \int^{\infty}_0 f(x) \cos\left(t \sqrt x\right) dx $$ Thanks.,I`m preparing for an exam and I have this question: how can I find the distributional limit for something like for $f\in L^1 (R)$ : $$\lim_{t \rightarrow 0} \int^{\infty}_0 f(x) \cos \left(t \sqrt x\right) dx $$ $$\lim_{t\rightarrow \infty} \int^{\infty}_0 f(x) \cos\left(t \sqrt x\right) dx $$ Thanks.,,"['functional-analysis', 'analysis', 'distribution-theory', 'harmonic-analysis']"
54,"Prob. 19, Chap. 4 in Baby Rudin: Any real function on $\mathbb{R}$ whit the intermediate-value property for which ... is continuous","Prob. 19, Chap. 4 in Baby Rudin: Any real function on  whit the intermediate-value property for which ... is continuous",\mathbb{R},"Here is Prob. 19, Chap. 4 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a real function with domain $\mathbb{R}^1$ which has the intermediate value property: If $f(a) < c < f(b)$, then $f(x) = c$ for some $x$ between $a$ and $b$. Suppose also, for every rational $r$, that the set of all $x$ with $f(x) = r$ is closed. Prove that $f$ is continuous. Hint: If $x_n \to x_0$ but $f\left( x_n \right) > r > f(x_0)$ for some $r$ and all $n$, then $f \left( t_n \right) = r$ for some $t_n$ between $x_0$ and $x_n$; thus $t_n \to x_0$. Find a contradiction. (N. J. Fine, Amer. Math. Monthly, vol. 73, 1966, p. 782.) My effort: Suppose $f$ satisfies the hypotheses in Prob. 19, Chap. 4 in Baby Rudin, but $f$ fails to be continuous at a point $p \in \mathbb{R}$. Then there is a sequence $x_n$ of real numbers such that $$x_n \to p, \ \mbox{ but } \  f\left( x_n \right) \not\to f(p) \ \mbox{ as } \  n \to \infty.$$ Thus, there is a positive real number $\varepsilon_0$ such that, for every natural number $N$, there is a natural number $n_N > N$ such that $$f\left( x_{n_N} \right) \not\in \left( \ f(p)-\varepsilon_0, \ f(p) + \varepsilon_0 \ \right).$$   Therefore there is a subsequence $\left\{ y_n \right\}$ of $\left\{ x_n \right\}$ the images $f\left( y_n \right)$ of each of whose terms are outside the segment $\left( \ f(p)-\varepsilon_0, \ f(p) + \varepsilon_0 \ \right)$. So there is a subsequence $\left\{ z_n \right\}$ of $\left\{ y_n \right\}$ such that   $$f\left( z_n \right) \leq  \ f(p)-\varepsilon_0 \ \mbox{ for all } \ n \in \mathbb{N}$$    or $$ f \left( z_n \right) \geq  \ f(p)+\varepsilon_0 \ \mbox{ for all } \ n \in \mathbb{N}.$$    Let's assume, without any loss of generality, that    $$ f \left( z_n \right) \geq  \ f(p)+\varepsilon_0 \ \mbox{ for all } \ n \in \mathbb{N}.$$    Let $r$ be a rational number such that $$ f(p) < r < f(p) + \varepsilon_0. \ \tag{1} $$    Then we see that    $$f\left( z_n \right) > r > f(p) \ \mbox{ for all } n \in \mathbb{N}.$$   Now as $\left\{ x_n \right\}$ converges to $p$, so $\left\{ z_n \right\}$ also converges to $p$. Now as $f$ satisfies the intermediate value property, so, for each $n \in \mathbb{N}$, there is a point $t_n$ between $z_n$ and $p$ such that $$ f\left( t_n \right) = r. $$    Then the sequence $\left\{ t_n \right\}$ must also converge to $p$. But $\left\{ t_n \right\}$ is a sequence in the closed set $$ f^{-1} \left( \{ r \} \right) = \left\{ \ x \in \mathbb{R} \ \colon \ f(x) = r \ \right\}.$$   So the point $p$ must also belong to this set, which implies that $f(p) = r$, which contradicts (1) above. Hence any function $f$ which satisfies the all of the hypotheses of Prob. 19, Chap. 4 in Baby Rudin, 3rd edition, must also be continuous on all of $\mathbb{R}^1$. Is my proof correct? If so, have I correctly used the hint given by Rudin? If not, then where have I gone wrong?","Here is Prob. 19, Chap. 4 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f$ is a real function with domain $\mathbb{R}^1$ which has the intermediate value property: If $f(a) < c < f(b)$, then $f(x) = c$ for some $x$ between $a$ and $b$. Suppose also, for every rational $r$, that the set of all $x$ with $f(x) = r$ is closed. Prove that $f$ is continuous. Hint: If $x_n \to x_0$ but $f\left( x_n \right) > r > f(x_0)$ for some $r$ and all $n$, then $f \left( t_n \right) = r$ for some $t_n$ between $x_0$ and $x_n$; thus $t_n \to x_0$. Find a contradiction. (N. J. Fine, Amer. Math. Monthly, vol. 73, 1966, p. 782.) My effort: Suppose $f$ satisfies the hypotheses in Prob. 19, Chap. 4 in Baby Rudin, but $f$ fails to be continuous at a point $p \in \mathbb{R}$. Then there is a sequence $x_n$ of real numbers such that $$x_n \to p, \ \mbox{ but } \  f\left( x_n \right) \not\to f(p) \ \mbox{ as } \  n \to \infty.$$ Thus, there is a positive real number $\varepsilon_0$ such that, for every natural number $N$, there is a natural number $n_N > N$ such that $$f\left( x_{n_N} \right) \not\in \left( \ f(p)-\varepsilon_0, \ f(p) + \varepsilon_0 \ \right).$$   Therefore there is a subsequence $\left\{ y_n \right\}$ of $\left\{ x_n \right\}$ the images $f\left( y_n \right)$ of each of whose terms are outside the segment $\left( \ f(p)-\varepsilon_0, \ f(p) + \varepsilon_0 \ \right)$. So there is a subsequence $\left\{ z_n \right\}$ of $\left\{ y_n \right\}$ such that   $$f\left( z_n \right) \leq  \ f(p)-\varepsilon_0 \ \mbox{ for all } \ n \in \mathbb{N}$$    or $$ f \left( z_n \right) \geq  \ f(p)+\varepsilon_0 \ \mbox{ for all } \ n \in \mathbb{N}.$$    Let's assume, without any loss of generality, that    $$ f \left( z_n \right) \geq  \ f(p)+\varepsilon_0 \ \mbox{ for all } \ n \in \mathbb{N}.$$    Let $r$ be a rational number such that $$ f(p) < r < f(p) + \varepsilon_0. \ \tag{1} $$    Then we see that    $$f\left( z_n \right) > r > f(p) \ \mbox{ for all } n \in \mathbb{N}.$$   Now as $\left\{ x_n \right\}$ converges to $p$, so $\left\{ z_n \right\}$ also converges to $p$. Now as $f$ satisfies the intermediate value property, so, for each $n \in \mathbb{N}$, there is a point $t_n$ between $z_n$ and $p$ such that $$ f\left( t_n \right) = r. $$    Then the sequence $\left\{ t_n \right\}$ must also converge to $p$. But $\left\{ t_n \right\}$ is a sequence in the closed set $$ f^{-1} \left( \{ r \} \right) = \left\{ \ x \in \mathbb{R} \ \colon \ f(x) = r \ \right\}.$$   So the point $p$ must also belong to this set, which implies that $f(p) = r$, which contradicts (1) above. Hence any function $f$ which satisfies the all of the hypotheses of Prob. 19, Chap. 4 in Baby Rudin, 3rd edition, must also be continuous on all of $\mathbb{R}^1$. Is my proof correct? If so, have I correctly used the hint given by Rudin? If not, then where have I gone wrong?",,"['real-analysis', 'analysis', 'continuity']"
55,"Proving convergence of the series $\sum_{n=1}^{\infty}\frac {1}{1000n^2 + 25n - 5}$ with comparison test, what can you jump to","Proving convergence of the series  with comparison test, what can you jump to",\sum_{n=1}^{\infty}\frac {1}{1000n^2 + 25n - 5},"I'm just going to make up a series as an example. $$\sum_{n=1}^{\infty}\frac {1}{1000n^2 + 25n - 5}$$ Not sure if it is a valid series or not, but that's not my question. So if I had to go and prove a series converges by using the comparison test. Say I wanted to compare it to $$\sum_{n=1}^{\infty}\frac {1}{n^2}$$ which I know converges. Would it be too far fetched to compare these two series? If this was a valid series, then my proof would look like this: $$\sum_{n=1}^{\infty}\frac {1}{1000n^2 + 25n - 5}$$ $\le$ $$\sum_{n=1}^{\infty}\frac {1}{n^2}$$ Since p > 1, this series in particular converges by p-test, and by comparison test, the original series converges as well. So yeah, main question, can I jump straight to $$\sum_{n=1}^{\infty}\frac {1}{n^2}$$ from the original series? If not how would I go about doing so?","I'm just going to make up a series as an example. $$\sum_{n=1}^{\infty}\frac {1}{1000n^2 + 25n - 5}$$ Not sure if it is a valid series or not, but that's not my question. So if I had to go and prove a series converges by using the comparison test. Say I wanted to compare it to $$\sum_{n=1}^{\infty}\frac {1}{n^2}$$ which I know converges. Would it be too far fetched to compare these two series? If this was a valid series, then my proof would look like this: $$\sum_{n=1}^{\infty}\frac {1}{1000n^2 + 25n - 5}$$ $\le$ $$\sum_{n=1}^{\infty}\frac {1}{n^2}$$ Since p > 1, this series in particular converges by p-test, and by comparison test, the original series converges as well. So yeah, main question, can I jump straight to $$\sum_{n=1}^{\infty}\frac {1}{n^2}$$ from the original series? If not how would I go about doing so?",,"['sequences-and-series', 'analysis', 'convergence-divergence']"
56,show that $\|f_n-f\|_p \to 0 \iff \|f_n\|_p \to \|f\|_p$,show that,\|f_n-f\|_p \to 0 \iff \|f_n\|_p \to \|f\|_p,"Suppose that $1 \le p \lt \infty$. If $f_n,f \in L^p$ and $f_n \to f$ a.e, then show that $\|f_n-f\|_p \to 0 \iff \|f_n\|_p \to \|f\|_p$ Hint: If $f_n,g_n,f,g \in L^1, f_n \to f \text{ and } g_n \to g \text{ a.e }. |f_n| \le g_n \text{ and } \int g_n \to \int g$, then $\int f_n \to \int f$. Proof (hint): Apply Fatou's Lemma to $g_n \pm\text{Re}({f_n})$ and $g_n\pm\text{Im}({f_n})$ respectively to get the result. Now ($\implies $) follows by Triangle inequality. For ($\impliedby )$ we have $$|f_n-f|^p \le (|f_n|+|f|)^p \to 2^p |f|^p$$ Moreover $$\left(\int\left(|f_n|+|f| \right)^p \right)^{\frac{1}{p}} \le \|f_n\|_p+\|f\|_p \to 2\|f\|_p$$ $$\implies \limsup_n\left(\int\left(|f_n|+|f| \right)^p \right)^{\frac{1}{p}} \le 2\|f\|_p$$ By Fatou's Lemma we have $$\int \liminf_n \left(|f_n|+|f|\right)^p \le \liminf_n \int \left(|f_n|+|f|\right)^p $$ $$\implies 2^p \int |f|^p \le \liminf_n \int \left(|f_n|+|f|\right)^p \le \limsup_n \int \left(|f_n|+|f|\right)^p \le 2^p \|f\|_p^p$$ This gives us that $$\lim_n \int \left(|f_n|+|f|\right)^p=\int 2^p |f|^p=2^p||f||_p^p$$ Then $h_n=|f_n-f|^p \to 0$ and $|h_n| \le g_n=(|f_n|+|f|)^p \to 2^p|f|^p=g$. Also $\int g_n \to \int g$. Thus by the hint we have $\int |f_n-f|^p\to 0$. This seems alright to me. I just wanted to confirm. Thanks for the help!","Suppose that $1 \le p \lt \infty$. If $f_n,f \in L^p$ and $f_n \to f$ a.e, then show that $\|f_n-f\|_p \to 0 \iff \|f_n\|_p \to \|f\|_p$ Hint: If $f_n,g_n,f,g \in L^1, f_n \to f \text{ and } g_n \to g \text{ a.e }. |f_n| \le g_n \text{ and } \int g_n \to \int g$, then $\int f_n \to \int f$. Proof (hint): Apply Fatou's Lemma to $g_n \pm\text{Re}({f_n})$ and $g_n\pm\text{Im}({f_n})$ respectively to get the result. Now ($\implies $) follows by Triangle inequality. For ($\impliedby )$ we have $$|f_n-f|^p \le (|f_n|+|f|)^p \to 2^p |f|^p$$ Moreover $$\left(\int\left(|f_n|+|f| \right)^p \right)^{\frac{1}{p}} \le \|f_n\|_p+\|f\|_p \to 2\|f\|_p$$ $$\implies \limsup_n\left(\int\left(|f_n|+|f| \right)^p \right)^{\frac{1}{p}} \le 2\|f\|_p$$ By Fatou's Lemma we have $$\int \liminf_n \left(|f_n|+|f|\right)^p \le \liminf_n \int \left(|f_n|+|f|\right)^p $$ $$\implies 2^p \int |f|^p \le \liminf_n \int \left(|f_n|+|f|\right)^p \le \limsup_n \int \left(|f_n|+|f|\right)^p \le 2^p \|f\|_p^p$$ This gives us that $$\lim_n \int \left(|f_n|+|f|\right)^p=\int 2^p |f|^p=2^p||f||_p^p$$ Then $h_n=|f_n-f|^p \to 0$ and $|h_n| \le g_n=(|f_n|+|f|)^p \to 2^p|f|^p=g$. Also $\int g_n \to \int g$. Thus by the hint we have $\int |f_n-f|^p\to 0$. This seems alright to me. I just wanted to confirm. Thanks for the help!",,"['real-analysis', 'functional-analysis', 'analysis', 'measure-theory', 'lp-spaces']"
57,"Is $f$ differentiable at $(0, 0)$?",Is  differentiable at ?,"f (0, 0)","I asked a question earlier relating to the equation: Let $f : \mathbb{R^2} \rightarrow \mathbb{R}$ be given by $$f(x, y) =  \cases{     \frac{xy}{x+y}& if $x+y\neq0$  \\     0 & if $x+y=0$}$$ I have another question relating to this equation which goes as follows: Is $f$ differentiable at $(0, 0)$? Im really not sure how to find if its differentiable or not so any help will be appreciated","I asked a question earlier relating to the equation: Let $f : \mathbb{R^2} \rightarrow \mathbb{R}$ be given by $$f(x, y) =  \cases{     \frac{xy}{x+y}& if $x+y\neq0$  \\     0 & if $x+y=0$}$$ I have another question relating to this equation which goes as follows: Is $f$ differentiable at $(0, 0)$? Im really not sure how to find if its differentiable or not so any help will be appreciated",,"['real-analysis', 'analysis', 'derivatives']"
58,How is this Taylor's Theorem?,How is this Taylor's Theorem?,,"When doing A-levels, we showed that any $n$th degree polynomial function $p(x-a)$ can be expressed in terms of its $n$ derivatives $$p(x)=\sum_{i=0}^n \frac{p^{(i)}(a)}{i!}(x-a)^n$$ and this was the Taylor's theorem that we learned. Now at university, in our Analysis II course, we did the following theorem, also dubbed Taylor's theorem: If $f:[a,b]\to\mathbb R$ is a continuous function that is differentiable on  $(a,b)$, and $x,x+h\in(a,b)$, then $f(x+h)-f(x)=f'(x+\theta h)h$ for some $\theta\in(0,1)$. Which we proceeded to prove using the mean value theorem. This seems to me to be quite an unrelated theorem - does it have anything to do with the polynomial expansion in terms of derivatives?","When doing A-levels, we showed that any $n$th degree polynomial function $p(x-a)$ can be expressed in terms of its $n$ derivatives $$p(x)=\sum_{i=0}^n \frac{p^{(i)}(a)}{i!}(x-a)^n$$ and this was the Taylor's theorem that we learned. Now at university, in our Analysis II course, we did the following theorem, also dubbed Taylor's theorem: If $f:[a,b]\to\mathbb R$ is a continuous function that is differentiable on  $(a,b)$, and $x,x+h\in(a,b)$, then $f(x+h)-f(x)=f'(x+\theta h)h$ for some $\theta\in(0,1)$. Which we proceeded to prove using the mean value theorem. This seems to me to be quite an unrelated theorem - does it have anything to do with the polynomial expansion in terms of derivatives?",,"['calculus', 'analysis', 'taylor-expansion']"
59,Prove if $\lim_{x\to\ c} \ f'(x) = K$ then $f'(c) = K$ [duplicate],Prove if  then  [duplicate],\lim_{x\to\ c} \ f'(x) = K f'(c) = K,"This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 6 years ago . Suppose $f: (0, 1) \rightarrow {\rm I\!R}$ is continuous, with $c \in (0, 1)$, and suppose that $f'$ exists $\forall x \in (0, 1)$\{$c$}. Prove that if $\lim_{x\to\ c} \ f'(x) = K$ then $f'(c) = K$. $$\lim_{x\to\ c} \ f'(x) = K \Rightarrow \forall \epsilon > 0, \exists \delta > 0 \text{ such that } |f'(x) - K| < \epsilon \text{ for } x \in (c-\delta, c+\delta)$$ Since $f$ is continuous and differentiable on the open interval we can use the Mean Value Theorem on $[x, c]$ where $x \in (c-\delta, c)$. So $\exists x_1 \in (x, c)$ such that $f'(x_1) = \frac{f(x) - f(c)}{x-c}$. By our limit definition $|f'(x_1) - K| = |\frac{f(x) - f(c)}{x-c} - K| < \epsilon$. $$\Rightarrow \left|\frac{f(x)-f(c)}{x-c} - K\right| < \epsilon \text{ for } x \in (c-\delta, c)$$ We can use the MVT again on $[c, x]$ where $x \in (c, c+\delta)$, find an $x_2 \in (c, c+\delta)$ and get similar result. $$\Rightarrow \left|\frac{f(x)-f(c)}{x-c} - K\right| < \epsilon \text{ for } x \in (c-\delta, c+\delta)$$ $$\Rightarrow \lim_{x\to\ c} \frac{f(x)-f(c)}{x-c} = K$$ $$\Rightarrow f'(c) = K$$ I'm not sure that I did this right - I'm not too comfortable with proofs yet (especially unfamiliar with using the MVT in proofs). I didn't use the fact that the domain of f is (0,1) which is concerning.","This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 6 years ago . Suppose $f: (0, 1) \rightarrow {\rm I\!R}$ is continuous, with $c \in (0, 1)$, and suppose that $f'$ exists $\forall x \in (0, 1)$\{$c$}. Prove that if $\lim_{x\to\ c} \ f'(x) = K$ then $f'(c) = K$. $$\lim_{x\to\ c} \ f'(x) = K \Rightarrow \forall \epsilon > 0, \exists \delta > 0 \text{ such that } |f'(x) - K| < \epsilon \text{ for } x \in (c-\delta, c+\delta)$$ Since $f$ is continuous and differentiable on the open interval we can use the Mean Value Theorem on $[x, c]$ where $x \in (c-\delta, c)$. So $\exists x_1 \in (x, c)$ such that $f'(x_1) = \frac{f(x) - f(c)}{x-c}$. By our limit definition $|f'(x_1) - K| = |\frac{f(x) - f(c)}{x-c} - K| < \epsilon$. $$\Rightarrow \left|\frac{f(x)-f(c)}{x-c} - K\right| < \epsilon \text{ for } x \in (c-\delta, c)$$ We can use the MVT again on $[c, x]$ where $x \in (c, c+\delta)$, find an $x_2 \in (c, c+\delta)$ and get similar result. $$\Rightarrow \left|\frac{f(x)-f(c)}{x-c} - K\right| < \epsilon \text{ for } x \in (c-\delta, c+\delta)$$ $$\Rightarrow \lim_{x\to\ c} \frac{f(x)-f(c)}{x-c} = K$$ $$\Rightarrow f'(c) = K$$ I'm not sure that I did this right - I'm not too comfortable with proofs yet (especially unfamiliar with using the MVT in proofs). I didn't use the fact that the domain of f is (0,1) which is concerning.",,"['real-analysis', 'analysis', 'proof-verification', 'proof-writing']"
60,Does this function sequence have a convergent subsequence?,Does this function sequence have a convergent subsequence?,,"Let $\langle f_n \rangle$ be sequence of equi-continuous real-valued functions on $\mathbb R$ such that $f_n(0)=0$ for every $n$. Does $\langle f_n\rangle$ have a converging subsequence? I found that even equi-continuity condition and the value at $x=0$ does not guarantee the uniform convergence, as there is a counter-example of $f_n (x) = x/n$, which does not converge uniformly, but do the conditions of the question guarantee pointwise convergence of some subsequence?","Let $\langle f_n \rangle$ be sequence of equi-continuous real-valued functions on $\mathbb R$ such that $f_n(0)=0$ for every $n$. Does $\langle f_n\rangle$ have a converging subsequence? I found that even equi-continuity condition and the value at $x=0$ does not guarantee the uniform convergence, as there is a counter-example of $f_n (x) = x/n$, which does not converge uniformly, but do the conditions of the question guarantee pointwise convergence of some subsequence?",,"['sequences-and-series', 'analysis', 'convergence-divergence', 'uniform-convergence', 'arzela-ascoli']"
61,De Rham Theorem,De Rham Theorem,,"I was just reading about De Rham theorem : Let $\Omega$ be an open set of $\mathbb R^n$. Let pose $\mathcal V := \{u=(u_1,...,u_N)\in  \mathcal D^N (\Omega)\}$, And $f=(f_1,...,f_N) \in (\mathcal D'  (\Omega))^N $; So The two properties are equivalent: 1) $\exists P \in \mathcal D' (\Omega)$ such that: $f = \nabla  P . \\ $ 2) $\left \langle f,v \right \rangle = 0$, $\forall v \in \mathcal V .$ I want to prove that $2\Rightarrow 1 $, ($1\Rightarrow 2$  is OKEY). I need a reference for learning the proof. I will be very happy if someone could help me to prove it. Thanks!","I was just reading about De Rham theorem : Let $\Omega$ be an open set of $\mathbb R^n$. Let pose $\mathcal V := \{u=(u_1,...,u_N)\in  \mathcal D^N (\Omega)\}$, And $f=(f_1,...,f_N) \in (\mathcal D'  (\Omega))^N $; So The two properties are equivalent: 1) $\exists P \in \mathcal D' (\Omega)$ such that: $f = \nabla  P . \\ $ 2) $\left \langle f,v \right \rangle = 0$, $\forall v \in \mathcal V .$ I want to prove that $2\Rightarrow 1 $, ($1\Rightarrow 2$  is OKEY). I need a reference for learning the proof. I will be very happy if someone could help me to prove it. Thanks!",,"['real-analysis', 'functional-analysis', 'analysis', 'partial-differential-equations']"
62,How to prove that $\gamma'(n+1)\gamma(n+1)$ is incremental,How to prove that  is incremental,\gamma'(n+1)\gamma(n+1),"Not so long before I asked this question, I encountered a question that asked me to prove a proposition, but I'm not sure how to go about it. Prove that $\gamma'(n+1)\gamma(n+1)$ is incremental","Not so long before I asked this question, I encountered a question that asked me to prove a proposition, but I'm not sure how to go about it. Prove that $\gamma'(n+1)\gamma(n+1)$ is incremental",,['analysis']
63,"Probs. 24 (c) and (e), Chap. 3, in Baby Rudin: Completion of a metric space","Probs. 24 (c) and (e), Chap. 3, in Baby Rudin: Completion of a metric space",,"Here is Prob. 24, Chap. 3, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $X$ be a metric space. (a) Call two Cauchy sequences $\left\{ p_n \right\}$ , $\left\{ q_n \right\}$ in $X$ equivalent if $$ \lim_{n \to \infty} d \left( p_n, q_n \right) = 0.$$ Prove that this is an equivalence relation. [ This is easy.] (b) Let $X^*$ be the set of all equivalence classes so obtained. If $P \in X^*$ , $Q \in X^*$ , $\left\{ p_n \right\} \in P$ , $\left\{ q_n \right\} \in Q$ , define $$ \Delta (P, Q) = \lim_{n \to \infty} d \left( p_n, q_n \right); $$ by Exercise 23, this limit exists. Show that the number $\Delta (P, Q)$ is unchanged if $\left\{ p_n \right\}$ and $\left\{ q_n \right\}$ are replaced by equivalent sequences, and hence that $\Delta$ is a distance function in $X^*$ . [ I don't have any problem with that either.] (c) Prove that the resulting metric space $X^*$ is complete. [ How to do this? ] (d) For each $p \in X$ , there is a Cauchy sequence all of whose terms are $p$ ; let $P_p$ be the element of $X^*$ which contains this sequence. Prove that $$ \Delta \left( P_p, P_q \right) = d(p, q) $$ for all $p, q \in X$ . In other words, the mapping $\varphi$ defined by $\varphi (p) = P_p$ is an isometry (i.e. a distance-preserving mapping) of $X$ into $X^*$ . [ This isn't a problem either. ] (e) Prove that $\varphi(X)$ is dense in $X^*$ , and that $\varphi(X) = X^*$ if $X$ is complete. [ How to? ] By (d), we may identify $X$ and $\varphi(X)$ and thus regard $X$ as embedded in the complete metric space $X^*$ . We call $X^*$ the completion of $X$ . My effort: Prob. 24 (c): Let $\left\{ P_k \right\}$ be a Cauchy sequence in $X^*$ . Then, given any $\varepsilon > 0$ , we can find a natural number $K$ such that $$\Delta \left( P_k, P_r \right) < \varepsilon$$ for all $k, r \in \mathbb{N}$ such that $k > K$ and $r > K$ . For each $k \in \mathbb{N}$ , let $\left\{ p_{kn} \right\}$ be an element of $P_k$ . Then, for each $k \in \mathbb{N}$ , the sequence $\left\{ p_{kn} \right\}$ is a Cauchy sequence in $(X, d)$ and also $$ \lim_{n \to \infty } d \left( p_{kn}, p_{rn} \right) < \varepsilon$$ for all $k, r \in \mathbb{N}$ such that $k > K$ and $r > K$ . What next? Prob. 24 (e): Let $P \in X^*$ and let $\varepsilon > 0$ be given. We need to find an element $x \in X$ such that $$ \Delta (P, \varphi(x) ) < \varepsilon.$$ Let $\left\{ p_n \right\}$ be a Cauchy sequence in $P$ . Then there exists a natural number $N$ such that $$ d \left( p_m , p_n \right) < \frac{\varepsilon}{2} $$ for all $m, n \in \mathbb{N}$ such that $m > N$ and $n > N$ . Let $P_{p_{N+1}}$ be the element of $X^*$ that contains the Cauchy sequence $$ \left\{ p_{N+1}, p_{N+1}, p_{N+1}, \ldots \right\}.$$ Then we see that $$ \Delta \left( P, P_{p_{N+1}} \right) = \lim_{n \to \infty} d \left( p_n, p_{N+1} \right) \leq \frac{\varepsilon}{2} < \varepsilon. $$ And, $P_{p_{N+1}} = \varphi(p_{N+1})$ . Is this argument correct? If not, then where lies the flaw? What it the correct proof? An afterthought: Here's a proof of Prob. 24 (c) that I propose. Let $\left\{ P_k \right\}_{k \in \mathbb{N}}$ be a Cauchy sequence in $X^*$ . Then, given a real number $\varepsilon > 0$ , we can find a natural number $K$ such that $$ \Delta \left( P_k, P_r \right) < \frac{\varepsilon}{2}$$ for all $r, k \in \mathbb{N}$ such that $r >  K$ and $k > K$ . For each $k \in \mathbb{N}$ , since $P_k \in X^*$ , each $P_k$ is an equivalence class of Cauchy sequences in $X$ . Let $\left\{ p_{kn} \right\}_{n \in \mathbb{N}}$ be a Cauchy sequence in the equivalence class $P_k$ , for each $k \in \mathbb{N}$ . Then we can conclude that $$ \Delta \left( P_k, P_r \right) = \lim_{n \to \infty} d \left( p_{kn},  p_{rn} \right) < \frac{\varepsilon}{2}$$ for all $r, k \in \mathbb{N}$ such that $r > K$ and $k > K$ . Therefore we can find a natural number $N$ such that $$ d \left( p_{kn}, p_{rn} \right) < \frac{\varepsilon}{2}$$ for all $r, k, n \in \mathbb{N}$ such that $r > K$ , $k > K$ , and $n > N$ . So $$ d \left( p_{kn}, p_{K+1, \ N+1} \right) < \frac{\varepsilon}{2}$$ for all $k, n \in \mathbb{N}$ such that $k > K$ and $n > N$ . Now $$\left\{ p_{K+1, \ N+1}, \  p_{K+1, \ N+1}, \ p_{K+1, \ N+1}, \  \ldots \right\}$$ is a Cauchy sequence in $X$ . Let $P \in X^*$ be the equivalence class of this Cauchy sequence. Now, for each $k \in \mathbb{N}$ , since $\left\{ p_{kn} \right\}_{n \in \mathbb{N}}$ is a Cauchy sequence in the equivalence class $P_k$ , we can conclude that, for all $k \in \mathbb{N}$ such that $k > K$ , the following is true: $$ \Delta \left( P_k, P \right) = \lim_{n \to \infty} d \left( p_{kn}, p_{K+1, \ N+1} \right) \leq \frac{\varepsilon}{2} < \varepsilon.$$ Since $\varepsilon > 0$ was arbitrary, we can conclude that our original sequence $\left\{ P_k \right\}_{k \in \mathbb{N}}$ converges in $\left( X^*, \Delta \right)$ to the point $P \in X^*$ . Hence $\left( X^*, \Delta \right)$ is a complete metric space. Is this proof correct? If not, then where lies the problem?  I wonder if the $P\in X^*$ in my argument is dependent on our choice of $\varepsilon$ and is not uniform.","Here is Prob. 24, Chap. 3, in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let be a metric space. (a) Call two Cauchy sequences , in equivalent if Prove that this is an equivalence relation. [ This is easy.] (b) Let be the set of all equivalence classes so obtained. If , , , , define by Exercise 23, this limit exists. Show that the number is unchanged if and are replaced by equivalent sequences, and hence that is a distance function in . [ I don't have any problem with that either.] (c) Prove that the resulting metric space is complete. [ How to do this? ] (d) For each , there is a Cauchy sequence all of whose terms are ; let be the element of which contains this sequence. Prove that for all . In other words, the mapping defined by is an isometry (i.e. a distance-preserving mapping) of into . [ This isn't a problem either. ] (e) Prove that is dense in , and that if is complete. [ How to? ] By (d), we may identify and and thus regard as embedded in the complete metric space . We call the completion of . My effort: Prob. 24 (c): Let be a Cauchy sequence in . Then, given any , we can find a natural number such that for all such that and . For each , let be an element of . Then, for each , the sequence is a Cauchy sequence in and also for all such that and . What next? Prob. 24 (e): Let and let be given. We need to find an element such that Let be a Cauchy sequence in . Then there exists a natural number such that for all such that and . Let be the element of that contains the Cauchy sequence Then we see that And, . Is this argument correct? If not, then where lies the flaw? What it the correct proof? An afterthought: Here's a proof of Prob. 24 (c) that I propose. Let be a Cauchy sequence in . Then, given a real number , we can find a natural number such that for all such that and . For each , since , each is an equivalence class of Cauchy sequences in . Let be a Cauchy sequence in the equivalence class , for each . Then we can conclude that for all such that and . Therefore we can find a natural number such that for all such that , , and . So for all such that and . Now is a Cauchy sequence in . Let be the equivalence class of this Cauchy sequence. Now, for each , since is a Cauchy sequence in the equivalence class , we can conclude that, for all such that , the following is true: Since was arbitrary, we can conclude that our original sequence converges in to the point . Hence is a complete metric space. Is this proof correct? If not, then where lies the problem?  I wonder if the in my argument is dependent on our choice of and is not uniform.","X \left\{ p_n \right\} \left\{ q_n \right\} X  \lim_{n \to \infty} d \left( p_n, q_n \right) = 0. X^* P \in X^* Q \in X^* \left\{ p_n \right\} \in P \left\{ q_n \right\} \in Q  \Delta (P, Q) = \lim_{n \to \infty} d \left( p_n, q_n \right);  \Delta (P, Q) \left\{ p_n \right\} \left\{ q_n \right\} \Delta X^* X^* p \in X p P_p X^*  \Delta \left( P_p, P_q \right) = d(p, q)  p, q \in X \varphi \varphi (p) = P_p X X^* \varphi(X) X^* \varphi(X) = X^* X X \varphi(X) X X^* X^* X \left\{ P_k \right\} X^* \varepsilon > 0 K \Delta \left( P_k, P_r \right) < \varepsilon k, r \in \mathbb{N} k > K r > K k \in \mathbb{N} \left\{ p_{kn} \right\} P_k k \in \mathbb{N} \left\{ p_{kn} \right\} (X, d)  \lim_{n \to \infty } d \left( p_{kn}, p_{rn} \right) < \varepsilon k, r \in \mathbb{N} k > K r > K P \in X^* \varepsilon > 0 x \in X  \Delta (P, \varphi(x) ) < \varepsilon. \left\{ p_n \right\} P N  d \left( p_m , p_n \right) < \frac{\varepsilon}{2}  m, n \in \mathbb{N} m > N n > N P_{p_{N+1}} X^*  \left\{ p_{N+1}, p_{N+1}, p_{N+1}, \ldots \right\}.  \Delta \left( P, P_{p_{N+1}} \right) = \lim_{n \to \infty} d \left( p_n, p_{N+1} \right) \leq \frac{\varepsilon}{2} < \varepsilon.  P_{p_{N+1}} = \varphi(p_{N+1}) \left\{ P_k \right\}_{k \in \mathbb{N}} X^* \varepsilon > 0 K  \Delta \left( P_k, P_r \right) < \frac{\varepsilon}{2} r, k \in \mathbb{N} r > 
K k > K k \in \mathbb{N} P_k \in X^* P_k X \left\{ p_{kn} \right\}_{n \in \mathbb{N}} P_k k \in \mathbb{N}  \Delta \left( P_k, P_r \right) = \lim_{n \to \infty} d \left( p_{kn}, 
p_{rn} \right) < \frac{\varepsilon}{2} r, k \in \mathbb{N} r > K k > K N  d \left( p_{kn}, p_{rn} \right) < \frac{\varepsilon}{2} r, k, n \in \mathbb{N} r > K k > K n > N  d \left( p_{kn}, p_{K+1, \ N+1} \right) < \frac{\varepsilon}{2} k, n \in \mathbb{N} k > K n > N \left\{ p_{K+1, \ N+1}, \  p_{K+1, \ N+1}, \ p_{K+1, \ N+1}, \  \ldots \right\} X P \in X^* k \in \mathbb{N} \left\{ p_{kn} \right\}_{n \in \mathbb{N}} P_k k \in \mathbb{N} k > K  \Delta \left( P_k, P \right) = \lim_{n \to \infty} d \left( p_{kn}, p_{K+1, \ N+1} \right) \leq \frac{\varepsilon}{2} < \varepsilon. \varepsilon > 0 \left\{ P_k \right\}_{k \in \mathbb{N}} \left( X^*, \Delta \right) P \in X^* \left( X^*, \Delta \right) P\in X^* \varepsilon","['real-analysis', 'analysis', 'metric-spaces', 'complete-spaces']"
64,$u$ satisfies Schrodinger equation implies $\mathcal{F}^{-1} \left(\chi_{2}(\xi) \hat{u} \right) $ also?,satisfies Schrodinger equation implies  also?,u \mathcal{F}^{-1} \left(\chi_{2}(\xi) \hat{u} \right) ,"Consider  Schr\""odinger equation (SE): $i \frac{\partial }{\partial t}u (x,t )+ \Delta u(x,t) =0, (x, t)\in \mathbb R^{N}\times \mathbb R.$ $u(0,x)=\phi(x).$ Then, formally,  the solution of (SE) can be written as $u(x,t)= \mathcal{F}^{-1}\left( e^{-4\pi^2 |\xi|^2  it} \hat{\phi} \right) (x).$ See Section 1.1  and equation (1.2) for details. Define $\chi_{2}(D)u:= \mathcal{F}^{-1} \left(\chi_{2}(\xi) \hat{u} \right) $ where $\chi_{2}(\xi) = \chi_{[2, 4]}(\xi)$ (Characteristic function) (and $\mathcal{F}^{-1}$ denotes inverse Fourier transform and $\hat{\cdot}$ denotes the Fourier transform.) My Question:  If $u$ satisfies (SE), then can we say $\chi_2(D)u$ also satisfies (SE)?","Consider  Schr\""odinger equation (SE): $i \frac{\partial }{\partial t}u (x,t )+ \Delta u(x,t) =0, (x, t)\in \mathbb R^{N}\times \mathbb R.$ $u(0,x)=\phi(x).$ Then, formally,  the solution of (SE) can be written as $u(x,t)= \mathcal{F}^{-1}\left( e^{-4\pi^2 |\xi|^2  it} \hat{\phi} \right) (x).$ See Section 1.1  and equation (1.2) for details. Define $\chi_{2}(D)u:= \mathcal{F}^{-1} \left(\chi_{2}(\xi) \hat{u} \right) $ where $\chi_{2}(\xi) = \chi_{[2, 4]}(\xi)$ (Characteristic function) (and $\mathcal{F}^{-1}$ denotes inverse Fourier transform and $\hat{\cdot}$ denotes the Fourier transform.) My Question:  If $u$ satisfies (SE), then can we say $\chi_2(D)u$ also satisfies (SE)?",,"['analysis', 'partial-differential-equations', 'dispersive-pde']"
65,Continuity of Lebesgue Integral of a Function in 2 Variables,Continuity of Lebesgue Integral of a Function in 2 Variables,,"If $(X,M,\mu)$ is a measure space and $f: X \times [a,b] \to \mathbb{C}$ is s.t $\forall t\in[a,b]$ $f(\cdot,t) \in L^1(d\mu)$ ($\int_X|f(x,t)|d\mu(x) < \infty$) Let $F: [a,b] \to \mathbb{R}$ be defined as $F(t) = \int_X f(x,t)d\mu(x)$. Suppose that for some $g \in L^1(d\mu)$ $\forall x,t |f(x,t)| < g(x)$ and that $\forall x\in X$ $lim_{t \to t_0}f(x,t) = f(x,t_0)$ then show $lim_{t \to t_0}F(t) = F(t_0)$. My attempt: My initial idea was to try to prove this for sequences of rationals in $[a,b]$ and try to generalize: Let $\{t_n\} \in [a,b] \cap \mathbb{Q} $ converge to $t_0$ If we define $f_n(x,t_n) = f(x,t_n)$ then $f_n : X \to \mathbb{C}$ and $f_n$ converges point wise in $x$ to $f(x,t)$ by the assumption. Then by the dominated convergence theorem: $lim_{t\to t_0} \int_Xf(x,t) = lim_{n\to \infty} \int_X f_n(x,t_n)d\mu(x) = \int_Xf(x,t)$ is this argument ok? and is it fair to say that proving this for rational sequences is enough as every irrational is the limit of a sequence of rationals?","If $(X,M,\mu)$ is a measure space and $f: X \times [a,b] \to \mathbb{C}$ is s.t $\forall t\in[a,b]$ $f(\cdot,t) \in L^1(d\mu)$ ($\int_X|f(x,t)|d\mu(x) < \infty$) Let $F: [a,b] \to \mathbb{R}$ be defined as $F(t) = \int_X f(x,t)d\mu(x)$. Suppose that for some $g \in L^1(d\mu)$ $\forall x,t |f(x,t)| < g(x)$ and that $\forall x\in X$ $lim_{t \to t_0}f(x,t) = f(x,t_0)$ then show $lim_{t \to t_0}F(t) = F(t_0)$. My attempt: My initial idea was to try to prove this for sequences of rationals in $[a,b]$ and try to generalize: Let $\{t_n\} \in [a,b] \cap \mathbb{Q} $ converge to $t_0$ If we define $f_n(x,t_n) = f(x,t_n)$ then $f_n : X \to \mathbb{C}$ and $f_n$ converges point wise in $x$ to $f(x,t)$ by the assumption. Then by the dominated convergence theorem: $lim_{t\to t_0} \int_Xf(x,t) = lim_{n\to \infty} \int_X f_n(x,t_n)d\mu(x) = \int_Xf(x,t)$ is this argument ok? and is it fair to say that proving this for rational sequences is enough as every irrational is the limit of a sequence of rationals?",,"['analysis', 'measure-theory', 'lebesgue-integral']"
66,Solving system of equations with sums of odd power,Solving system of equations with sums of odd power,,"suppose we have given positive real numbers $a_1,...,a_n>0$. Consider the following system of equations: $$\sum_{i=1}^{n} (x_{i})^{2k-1} = a_k,\quad  k= 1,.....,n$$ with $x_1,...,x_n>0$. This system of equations does not have always solutions (see e.g. the answer of Leo163 below). But suppose $a_1,...,a_n$ are choosen in such a way that there exist a solution. The question is : How many solutions can this system have? By solutions I mean any multi set $\{ x_1,…,x_n \}$ such that the above equations are satisfied. Are there conditions such that the solution becomes unique? I would really appreciate any help. Best wishes","suppose we have given positive real numbers $a_1,...,a_n>0$. Consider the following system of equations: $$\sum_{i=1}^{n} (x_{i})^{2k-1} = a_k,\quad  k= 1,.....,n$$ with $x_1,...,x_n>0$. This system of equations does not have always solutions (see e.g. the answer of Leo163 below). But suppose $a_1,...,a_n$ are choosen in such a way that there exist a solution. The question is : How many solutions can this system have? By solutions I mean any multi set $\{ x_1,…,x_n \}$ such that the above equations are satisfied. Are there conditions such that the solution becomes unique? I would really appreciate any help. Best wishes",,"['abstract-algebra', 'analysis', 'algebraic-geometry', 'polynomials']"
67,Proof of Fubini's theorem for infinite sums.,Proof of Fubini's theorem for infinite sums.,,"In his book Analysis 1, the author Tao states Fubini's theorem as follows Let $f:N \times N \rightarrow \mathbb{R}$ be a function such that $\sum_{(n,m)\in N\times N}f(n,m)  $ is absolutely convergent. Then we have $$\begin{align*} \sum_{n=0}^{\infty}\Bigg(\sum^{\infty}_{m=0}f(n,m)\Bigg) &=\sum_{(n,m)\in N \times N}f(n,m) \\ &=\sum_{(m,n)\in N \times N}f(n,m)\\ &=\sum_{m=0}^{\infty}\Bigg(\sum^{\infty}_{n=0}f(n,m)\Bigg) \end{align*}$$ He says that the second inequality follows from the rearrangement of absolutely convergent series. But that theorem is for bijective functions $f:N\rightarrow N$. How can we use it to obtain the second equality in the theorem stated above ?","In his book Analysis 1, the author Tao states Fubini's theorem as follows Let $f:N \times N \rightarrow \mathbb{R}$ be a function such that $\sum_{(n,m)\in N\times N}f(n,m)  $ is absolutely convergent. Then we have $$\begin{align*} \sum_{n=0}^{\infty}\Bigg(\sum^{\infty}_{m=0}f(n,m)\Bigg) &=\sum_{(n,m)\in N \times N}f(n,m) \\ &=\sum_{(m,n)\in N \times N}f(n,m)\\ &=\sum_{m=0}^{\infty}\Bigg(\sum^{\infty}_{n=0}f(n,m)\Bigg) \end{align*}$$ He says that the second inequality follows from the rearrangement of absolutely convergent series. But that theorem is for bijective functions $f:N\rightarrow N$. How can we use it to obtain the second equality in the theorem stated above ?",,"['real-analysis', 'sequences-and-series', 'analysis']"
68,"Question about Measure Theory ""integral on $\mathbb{R}^n$""","Question about Measure Theory ""integral on """,\mathbb{R}^n,"I have this :  $u_n\rightarrow u~\text{in}~L^{\Phi}(\mathbb{R}^N)$ i.e., $\int_{\mathbb{R}^N}\Phi(|u_n-u|)dx\rightarrow0$ by measure theory, given $\varepsilon>0$ there exists $R>0$ such that $$\int_{B^c_R(0)}\Phi(|u_n|) dx\leq \varepsilon, ~\text{and}~ \int_{B^c_R(0)}\Phi(|u|) dx\leq \varepsilon$$ how to find this? Edit: Where $\Phi: \mathbb{R}\rightarrow[0,+\infty)$ continuous, convexe, even, increasing  and satisfy $\Phi(2t)\leq K \Phi(t), \forall t\geq 0$ Thank you","I have this :  $u_n\rightarrow u~\text{in}~L^{\Phi}(\mathbb{R}^N)$ i.e., $\int_{\mathbb{R}^N}\Phi(|u_n-u|)dx\rightarrow0$ by measure theory, given $\varepsilon>0$ there exists $R>0$ such that $$\int_{B^c_R(0)}\Phi(|u_n|) dx\leq \varepsilon, ~\text{and}~ \int_{B^c_R(0)}\Phi(|u|) dx\leq \varepsilon$$ how to find this? Edit: Where $\Phi: \mathbb{R}\rightarrow[0,+\infty)$ continuous, convexe, even, increasing  and satisfy $\Phi(2t)\leq K \Phi(t), \forall t\geq 0$ Thank you",,"['functional-analysis', 'analysis', 'measure-theory']"
69,How to show that $\left|\sum_{k=0}^\infty\frac{(ix)^k}{(k+1)!}\right|\le \left|\sum_{k=0}^\infty\frac{(ix)^k}{k!}\right|=|e^{ix}|=1$ with restrictions,How to show that  with restrictions,\left|\sum_{k=0}^\infty\frac{(ix)^k}{(k+1)!}\right|\le \left|\sum_{k=0}^\infty\frac{(ix)^k}{k!}\right|=|e^{ix}|=1,"How to show that $\left|\sum_{k=0}^\infty\frac{(ix)^k}{(k+1)!}\right|\le \left|\sum_{k=0}^\infty\frac{(ix)^k}{k!}\right|=|e^{ix}|=1$ with restrictions, for $x\in\Bbb R$. To prove this inequality we cant use any related to derivatives, integrals, geometric statements about sine or cosine, or uniform convergence. We can use limits and basic facts about the convergent properties of these power series. We already knows that $|e^{ix}|=1$ for $x\in\Bbb R$. The inequality is a slight rewrite of $$\frac{|e^{ix}-1|}{|x|}=\left|\sum_{k=0}^\infty\frac{(ix)^k}{(k+1)!}\right|\le 1,\quad\forall x\in\Bbb R$$ what need to be proved. I dont know exactly what to do here, Im completely lost. The best I can think is to prove something like $$\forall\epsilon>0,\exists N\in\Bbb N:\left|\sum_{k=0}^n\frac{(ix)^k}{(k+1)!}-L\right|<\epsilon,\quad\forall n\ge N$$ for some $0\le L<1$. The exercise leave the hint $\lim_{z\to 0}\frac{\exp(z)-1}{z}=1$ for $z\in\Bbb C\setminus\{0\}$, but I dont see how to relate this to our problem, because we need the result for any $x$, not just for $x=0$. Some hint or solution will be appreciated, thank you.","How to show that $\left|\sum_{k=0}^\infty\frac{(ix)^k}{(k+1)!}\right|\le \left|\sum_{k=0}^\infty\frac{(ix)^k}{k!}\right|=|e^{ix}|=1$ with restrictions, for $x\in\Bbb R$. To prove this inequality we cant use any related to derivatives, integrals, geometric statements about sine or cosine, or uniform convergence. We can use limits and basic facts about the convergent properties of these power series. We already knows that $|e^{ix}|=1$ for $x\in\Bbb R$. The inequality is a slight rewrite of $$\frac{|e^{ix}-1|}{|x|}=\left|\sum_{k=0}^\infty\frac{(ix)^k}{(k+1)!}\right|\le 1,\quad\forall x\in\Bbb R$$ what need to be proved. I dont know exactly what to do here, Im completely lost. The best I can think is to prove something like $$\forall\epsilon>0,\exists N\in\Bbb N:\left|\sum_{k=0}^n\frac{(ix)^k}{(k+1)!}-L\right|<\epsilon,\quad\forall n\ge N$$ for some $0\le L<1$. The exercise leave the hint $\lim_{z\to 0}\frac{\exp(z)-1}{z}=1$ for $z\in\Bbb C\setminus\{0\}$, but I dont see how to relate this to our problem, because we need the result for any $x$, not just for $x=0$. Some hint or solution will be appreciated, thank you.",,"['analysis', 'convergence-divergence', 'power-series']"
70,Proving that an isometry between compact spaces is a homeomorphism,Proving that an isometry between compact spaces is a homeomorphism,,"I've recently attempted to prove that if $f: X_1 \to X_2$ is isometric, $X_1, X_2$ compact, then $f$ is a homeomorphism. But before this, I have a few questions: 1) is this even necessarily true? 2) does an isometry have to be a homeomorphism? I've seen it defined as such, but it's not clear to me why it must be. Specifically surjectivity is a pain to show, as I've come to find. I think compactness is important here somehow but I can't quite figure it out. Here is what I've got: Suppose there is an isometry $f: X_1 \to X_2$. Clearly $f$ is continuous, for given any $\varepsilon>0, x,y \in X_1$ we can choose $\delta = \varepsilon$ so that $d(x,y) < \delta$ implies $ = d(f(x), f(y)) < \delta = \varepsilon  $, in their respective metrics. Injectivity is also clear, for if $f(x) = f(y)$, then $d(f(x), f(y)) = d(x,y) = 0$ implies $x=y$ by the definition of a metric. The existence and continuity of $f^{-1}$ is also now clear, for $f$ is injective and $X_1$ and $X_2$ are compact. Now we show that $f$ is surjective, and hence a homeomorphism. Suppose for the sake of contradiction that there exists a $p \in X_2$ such that $p \neq f(x)$ for any $x \in X_1$. Then there is some minimum distance $r = d(p,f(X_1))$ between $p$ and the image of $X_1$ under $f$. By compactness, we obtain a countable dense subset $\{q_1, q_2, ... q_n, ...\}$ of $X_2$. Constructing balls of radius $r$ around each $q_i, i \in \mathbb{N}$, we obtain an open cover of $X_2$. Then since $X_2$ is compact, $X_2 \subset \bigcup_{i=1}^N B_r(q_i)$. It follows that $p \in B_r(q_n)$ for some $n  \le n \le N$. Then since $f(X)$ is compact by the continuity of $f$, it is closed so that its complement $f(X)^c$ is open in $X_2$, and hence  $B_r(q_n) \subset f(X)^c$. So at the end there I was just finding every fact I could, but I'm not sure if they'll be useful. I don't think uniform continuity will come into it, but then again I don't know. Some guidance would be much appreciated!","I've recently attempted to prove that if $f: X_1 \to X_2$ is isometric, $X_1, X_2$ compact, then $f$ is a homeomorphism. But before this, I have a few questions: 1) is this even necessarily true? 2) does an isometry have to be a homeomorphism? I've seen it defined as such, but it's not clear to me why it must be. Specifically surjectivity is a pain to show, as I've come to find. I think compactness is important here somehow but I can't quite figure it out. Here is what I've got: Suppose there is an isometry $f: X_1 \to X_2$. Clearly $f$ is continuous, for given any $\varepsilon>0, x,y \in X_1$ we can choose $\delta = \varepsilon$ so that $d(x,y) < \delta$ implies $ = d(f(x), f(y)) < \delta = \varepsilon  $, in their respective metrics. Injectivity is also clear, for if $f(x) = f(y)$, then $d(f(x), f(y)) = d(x,y) = 0$ implies $x=y$ by the definition of a metric. The existence and continuity of $f^{-1}$ is also now clear, for $f$ is injective and $X_1$ and $X_2$ are compact. Now we show that $f$ is surjective, and hence a homeomorphism. Suppose for the sake of contradiction that there exists a $p \in X_2$ such that $p \neq f(x)$ for any $x \in X_1$. Then there is some minimum distance $r = d(p,f(X_1))$ between $p$ and the image of $X_1$ under $f$. By compactness, we obtain a countable dense subset $\{q_1, q_2, ... q_n, ...\}$ of $X_2$. Constructing balls of radius $r$ around each $q_i, i \in \mathbb{N}$, we obtain an open cover of $X_2$. Then since $X_2$ is compact, $X_2 \subset \bigcup_{i=1}^N B_r(q_i)$. It follows that $p \in B_r(q_n)$ for some $n  \le n \le N$. Then since $f(X)$ is compact by the continuity of $f$, it is closed so that its complement $f(X)^c$ is open in $X_2$, and hence  $B_r(q_n) \subset f(X)^c$. So at the end there I was just finding every fact I could, but I'm not sure if they'll be useful. I don't think uniform continuity will come into it, but then again I don't know. Some guidance would be much appreciated!",,"['real-analysis', 'general-topology', 'analysis']"
71,Tangent of an injective regular curve independent of its parametrization,Tangent of an injective regular curve independent of its parametrization,,"I am trying to show that the tangent of a regular curve is independent of its parametrization. Let $c, \tilde c: \mathbb R \to \mathbb R^2$ be injective $C^1$-curves and $c'(t) \not=0, \tilde c'(s) \not= 0$. Now assume that $c$ and $c'$ define the same curve as point set, i.e. $c(\mathbb R)=\tilde c (\mathbb R)$. I want to show that $c'(t_0)$ and $\tilde c'(s_0)$ differ just by a $\mathbb R$-multiple, whenever $c(t_0)=c(s_0)$. We may assume $t_0=s_0=0,c(s_0)=t(s_0)=0$. I tried to parametrize by arc length, but how can I know that then the parametrizations must be the same up to orientation? I tried to look at the parameter change between $c$ and $\tilde c$, but how can I know that $\tilde c ^{-1} \circ c$ is differentiable?","I am trying to show that the tangent of a regular curve is independent of its parametrization. Let $c, \tilde c: \mathbb R \to \mathbb R^2$ be injective $C^1$-curves and $c'(t) \not=0, \tilde c'(s) \not= 0$. Now assume that $c$ and $c'$ define the same curve as point set, i.e. $c(\mathbb R)=\tilde c (\mathbb R)$. I want to show that $c'(t_0)$ and $\tilde c'(s_0)$ differ just by a $\mathbb R$-multiple, whenever $c(t_0)=c(s_0)$. We may assume $t_0=s_0=0,c(s_0)=t(s_0)=0$. I tried to parametrize by arc length, but how can I know that then the parametrizations must be the same up to orientation? I tried to look at the parameter change between $c$ and $\tilde c$, but how can I know that $\tilde c ^{-1} \circ c$ is differentiable?",,"['analysis', 'differential-geometry', 'curves']"
72,Are these valid Dedekind cuts for $e$ and $\pi$?,Are these valid Dedekind cuts for  and ?,e \pi,"I took the liberty to attempt to construct Dedekind cuts for $e$ and $\pi.$ That is, come up with a set $\alpha$ of rational numbers (that would correspond to the reals $e$ and $\pi$ ) such that, If $x \in \alpha$ and $y \in \mathbb Q : y < x$ , then $y \in \alpha$ $\alpha \neq \varnothing$ $\alpha \neq \mathbb Q$ $\alpha$ has no greatest element I came up with the following (hopefully valid) rational Dedekind cuts, $e = \left\{a\in\mathbb Q \, | \, a <0 \lor \left( \exists n \in \mathbb N : a <  \left(1 + \frac{1}{n}\right)^n \right) \right\}$ $\pi = \left\{a\in\mathbb Q \, | \, a <0 \lor \left( \exists n \in \mathbb N : a^2 < \displaystyle\sum_{i=1}^n \frac{6}{i^2} \right) \right\}$ The justification for these seemly arbitrary cuts is the simple fact that $$e := \displaystyle\lim_{n \to \infty}\left(1 + \frac{1}{n}\right)^n$$ and $$\pi^2 = \displaystyle\lim_{n \to \infty} \displaystyle\sum_{i=1}^n \frac{6}{i^2}$$ Are these cuts valid, and how would one attempt to show that they indeed satisfy the requirements for $\alpha$ ?","I took the liberty to attempt to construct Dedekind cuts for and That is, come up with a set of rational numbers (that would correspond to the reals and ) such that, If and , then has no greatest element I came up with the following (hopefully valid) rational Dedekind cuts, The justification for these seemly arbitrary cuts is the simple fact that and Are these cuts valid, and how would one attempt to show that they indeed satisfy the requirements for ?","e \pi. \alpha e \pi x \in \alpha y \in \mathbb Q : y < x y \in \alpha \alpha \neq \varnothing \alpha \neq \mathbb Q \alpha e = \left\{a\in\mathbb Q \, | \, a <0 \lor \left( \exists n \in \mathbb N : a <  \left(1 + \frac{1}{n}\right)^n \right) \right\} \pi = \left\{a\in\mathbb Q \, | \, a <0 \lor \left( \exists n \in \mathbb N : a^2 < \displaystyle\sum_{i=1}^n \frac{6}{i^2} \right) \right\} e := \displaystyle\lim_{n \to \infty}\left(1 + \frac{1}{n}\right)^n \pi^2 = \displaystyle\lim_{n \to \infty} \displaystyle\sum_{i=1}^n \frac{6}{i^2} \alpha","['real-analysis', 'analysis', 'real-numbers']"
73,counter example of minimum principle in incomplete inner product space,counter example of minimum principle in incomplete inner product space,,minimum principle: Minimum principle in Hilbert space I want to construct a counter example when $H$ is a inner product space but not a Hilbert space. Can we find a closed convex subset of $H$ such that there is no minimum point( or more than one) in it?,minimum principle: Minimum principle in Hilbert space I want to construct a counter example when $H$ is a inner product space but not a Hilbert space. Can we find a closed convex subset of $H$ such that there is no minimum point( or more than one) in it?,,"['functional-analysis', 'analysis']"
74,Uniform convergence to the max function?,Uniform convergence to the max function?,,"Let $f_1,f_2:U\rightarrow \mathbb{R}$ be two differentiable functions on a open set $U$ of $\mathbb{R}^m$ and $f$ their pointwise maximum, i.e. $f(x):=max\{f_1(x),f_2(x)\}$ for every $x\in U$. The function $f$ can be non differentiable, but it's possible to approximate it by differentiable functions for example by defining, for every $n\in \mathbb{N}$, $f_n:U\rightarrow \mathbb{R}$ and $f_n(x):=\frac{1}{n}log(e^{nf_1(x)}+e^{nf_2(x)})$, then $f_n$ will pointwise converge to $f$ for $n\rightarrow \infty$. Will $f_n$ also converge to $f$ uniformly for $n\rightarrow \infty$ in general? If not, under which additional conditions is it possible?","Let $f_1,f_2:U\rightarrow \mathbb{R}$ be two differentiable functions on a open set $U$ of $\mathbb{R}^m$ and $f$ their pointwise maximum, i.e. $f(x):=max\{f_1(x),f_2(x)\}$ for every $x\in U$. The function $f$ can be non differentiable, but it's possible to approximate it by differentiable functions for example by defining, for every $n\in \mathbb{N}$, $f_n:U\rightarrow \mathbb{R}$ and $f_n(x):=\frac{1}{n}log(e^{nf_1(x)}+e^{nf_2(x)})$, then $f_n$ will pointwise converge to $f$ for $n\rightarrow \infty$. Will $f_n$ also converge to $f$ uniformly for $n\rightarrow \infty$ in general? If not, under which additional conditions is it possible?",,"['real-analysis', 'analysis', 'convergence-divergence']"
75,How often is $\sin(a^n) $ positive or negative?,How often is  positive or negative?,\sin(a^n) ,"Let $a > 1$. Define $$ f(x,a) = \#\{n:\sin(a^n) \ge 0, n \in N, 1 \le n \le x\}, $$ $$ g(x,a) = \#\{n:\sin(a^n) < 0, n \in N, 1 \le n \le x\}. $$ Questions What is known about the growth rate of $f(x,a)$ and $g(x,a)$? Can the quantity $\Delta(x,a) = f(x,a) - g(x,a)$ it be bounded in terms of $x$ and $a$? My thoughts : Since $\sin(\theta)$ is a circular function, I would look at the distribution of the remainder when $a^n$ is divided by $2\pi$. If the remainder is uniformly distributed in $(0,2\pi)$ then $\sin(a^n)$ is equally likely to be positive or negative hence I would expect $f(x,a) \sim g(x,a) \sim x/2$.","Let $a > 1$. Define $$ f(x,a) = \#\{n:\sin(a^n) \ge 0, n \in N, 1 \le n \le x\}, $$ $$ g(x,a) = \#\{n:\sin(a^n) < 0, n \in N, 1 \le n \le x\}. $$ Questions What is known about the growth rate of $f(x,a)$ and $g(x,a)$? Can the quantity $\Delta(x,a) = f(x,a) - g(x,a)$ it be bounded in terms of $x$ and $a$? My thoughts : Since $\sin(\theta)$ is a circular function, I would look at the distribution of the remainder when $a^n$ is divided by $2\pi$. If the remainder is uniformly distributed in $(0,2\pi)$ then $\sin(a^n)$ is equally likely to be positive or negative hence I would expect $f(x,a) \sim g(x,a) \sim x/2$.",,"['real-analysis', 'analysis', 'trigonometry']"
76,"""Strong"" translations are continuous for $L^p$?","""Strong"" translations are continuous for ?",L^p,"Say $\Omega\subset\mathbb R^n$ is a bounded set. Let $p\in[1,\infty)$, and $f\in L^p(\Omega)$. It is well-known that $$ \sup_{|h|\leq\rho}\Vert f(\cdot+h)-f\Vert_{L^p(\Omega)}\longrightarrow0 $$ as $\rho\searrow0$. My question is: what if one brings the supremum inside the integral? More precisely, is it the case that if $f\in L^p(\Omega)$, then $$ \left(\int\limits_{\Omega}\text{ess}\sup_{|h|\leq\rho}|f(x+h)-f(x)|^p\,dx\right)^{\frac1p}\longrightarrow0\quad\text{as}\quad \rho\searrow0\quad?\tag1 $$ It is trivial to show (1) for $\phi\in C_c^\infty(\Omega)$ (or even just uniformly continuous functions on $\Omega$). Through some work (approximate through mollifiers), (1) can be shown for essentially bounded functions on $\Omega$. I have not thought yet of a counter-example in a more general case, and I'm not sure whether it is even true. My intuition tells me that it should be false, since putting the supremum inside is effectively an $L^\infty$-constraint. If a counter-example exists, it seems like it must be a function which blows up at every scale. In any case, any help is appreciated!","Say $\Omega\subset\mathbb R^n$ is a bounded set. Let $p\in[1,\infty)$, and $f\in L^p(\Omega)$. It is well-known that $$ \sup_{|h|\leq\rho}\Vert f(\cdot+h)-f\Vert_{L^p(\Omega)}\longrightarrow0 $$ as $\rho\searrow0$. My question is: what if one brings the supremum inside the integral? More precisely, is it the case that if $f\in L^p(\Omega)$, then $$ \left(\int\limits_{\Omega}\text{ess}\sup_{|h|\leq\rho}|f(x+h)-f(x)|^p\,dx\right)^{\frac1p}\longrightarrow0\quad\text{as}\quad \rho\searrow0\quad?\tag1 $$ It is trivial to show (1) for $\phi\in C_c^\infty(\Omega)$ (or even just uniformly continuous functions on $\Omega$). Through some work (approximate through mollifiers), (1) can be shown for essentially bounded functions on $\Omega$. I have not thought yet of a counter-example in a more general case, and I'm not sure whether it is even true. My intuition tells me that it should be false, since putting the supremum inside is effectively an $L^\infty$-constraint. If a counter-example exists, it seems like it must be a function which blows up at every scale. In any case, any help is appreciated!",,"['real-analysis', 'analysis', 'lp-spaces']"
77,Is there a continuous characteristic function on $\mathbb{R}$ ?(help with proof),Is there a continuous characteristic function on  ?(help with proof),\mathbb{R},"I have a question that says: Is there a continuous characteristic function on $\mathbb{R}$ ? If $A\subset \mathbb{R}$ show that $X_A$ is continuous at each point of $int(A)$ . Are there any other points of continuity? In this problem I have interpreted the characteristic function to be defined as: $X_A: A \to \mathbb{R}$ where $X_A(x)=0$ if $x \notin A$ and $X_A(x)=1$ if $x \in A$ Here is my attempt: $X_A(x)$ is continuous if given a closed set $E$ in the range of $X_A$ then $X_A^{-1}(E)$ is closed in $A$ . E is closed in $R$ and in the range of the characteristic function  only when $E= \emptyset$ $E=\{1\}$ and $E=\{0\}$ because singletons are closed in $\mathbb{R}$ since their compliment is the union of two disjoint open sets. $X_A(\{1\})= x\in A$ and $X_A(\{0\})=x\in \mathbb{R}-A$ . But singletons in $ \mathbb{R} $ are closed so the inverse of a closed set is closed and $X_A$ is continuous. Now for the second part of the question: Suppose $x\in A^0$ , the interior of $A$ . Then $X_A(\{x\})$ is {1} which is closed in $\mathbb{R}$ hence $X_A$ is continuous. Finally: There are other points of continuity, those in $\mathbb{R}-A$ since the image of these points are closed subsets of $\mathbb{R}$ and their inverse image is closed. I feel like I have flawlessly applied the definitions of continuity and given the exact answer the book is looking for!!! But somehow I suspect my feelings betray reality. Can you help me figure out how to fix this? In particular, I don't know if my using a singleton was the correct way to prove this, are singletons closed in R? I am pretty sure they are for the reason I gave.","I have a question that says: Is there a continuous characteristic function on ? If show that is continuous at each point of . Are there any other points of continuity? In this problem I have interpreted the characteristic function to be defined as: where if and if Here is my attempt: is continuous if given a closed set in the range of then is closed in . E is closed in and in the range of the characteristic function  only when and because singletons are closed in since their compliment is the union of two disjoint open sets. and . But singletons in are closed so the inverse of a closed set is closed and is continuous. Now for the second part of the question: Suppose , the interior of . Then is {1} which is closed in hence is continuous. Finally: There are other points of continuity, those in since the image of these points are closed subsets of and their inverse image is closed. I feel like I have flawlessly applied the definitions of continuity and given the exact answer the book is looking for!!! But somehow I suspect my feelings betray reality. Can you help me figure out how to fix this? In particular, I don't know if my using a singleton was the correct way to prove this, are singletons closed in R? I am pretty sure they are for the reason I gave.",\mathbb{R} A\subset \mathbb{R} X_A int(A) X_A: A \to \mathbb{R} X_A(x)=0 x \notin A X_A(x)=1 x \in A X_A(x) E X_A X_A^{-1}(E) A R E= \emptyset E=\{1\} E=\{0\} \mathbb{R} X_A(\{1\})= x\in A X_A(\{0\})=x\in \mathbb{R}-A  \mathbb{R}  X_A x\in A^0 A X_A(\{x\}) \mathbb{R} X_A \mathbb{R}-A \mathbb{R},[]
78,"Prove that if $B(x,r)\subset B(x',r')\Longleftrightarrow d(x,x')\leq r'-r$",Prove that if,"B(x,r)\subset B(x',r')\Longleftrightarrow d(x,x')\leq r'-r","For this proof, we're in $\mathbb{R}^n$ and $d$ is the Euclidean metric. So the claim can be written $(|x-y|<r\Rightarrow |x'-y|<r')\Longleftrightarrow |x-x'|\leq r'-r$. I think that I have ($\Leftarrow$): Assume $|x-x'|\leq r'-r$. Consider $y\in B(x,r)$. Notice that   $|x-x'|+|x-y|<|x-x'|+r\leq r'$. Since $|(x'-x)+(x-y)|\leq |x'-x|+|x-y|< r'$, we have $|x'-y|<r'$. Therefore, $y\in B(x',r')$ and $B(x,r)\subset B(x',r')$. I've gone through 8 sheets of paper and as many hours on the other direction, but have yet to come up with anything substantial. What I'm most confused about is how we can obtain the $-r$ term on the right side of the inequality.","For this proof, we're in $\mathbb{R}^n$ and $d$ is the Euclidean metric. So the claim can be written $(|x-y|<r\Rightarrow |x'-y|<r')\Longleftrightarrow |x-x'|\leq r'-r$. I think that I have ($\Leftarrow$): Assume $|x-x'|\leq r'-r$. Consider $y\in B(x,r)$. Notice that   $|x-x'|+|x-y|<|x-x'|+r\leq r'$. Since $|(x'-x)+(x-y)|\leq |x'-x|+|x-y|< r'$, we have $|x'-y|<r'$. Therefore, $y\in B(x',r')$ and $B(x,r)\subset B(x',r')$. I've gone through 8 sheets of paper and as many hours on the other direction, but have yet to come up with anything substantial. What I'm most confused about is how we can obtain the $-r$ term on the right side of the inequality.",,['analysis']
79,A natural interesting example of a Borel but non-piecewise continuous function,A natural interesting example of a Borel but non-piecewise continuous function,,"I know that the characteristic function $1_{\mathbb{Q}}$ is a Borel function (from $\mathbb{R}$ to $\mathbb{R}$), but I wonder if this function actually appears ""naturally"" in applications (and is not just an exotic example to show that borel functions are more general than piecewise continuous function). So my question is this : are there explicit Borel functions (and even $L^1(\mathbb{R}))$ that are non-piecewise continuous, but still appear as fundamental or very important in applications (in analysis, or any other domain of mathematics or physics) and are not simply exotic objects? (I am aware it's not a very well-defined question, but I hope it's understandable)","I know that the characteristic function $1_{\mathbb{Q}}$ is a Borel function (from $\mathbb{R}$ to $\mathbb{R}$), but I wonder if this function actually appears ""naturally"" in applications (and is not just an exotic example to show that borel functions are more general than piecewise continuous function). So my question is this : are there explicit Borel functions (and even $L^1(\mathbb{R}))$ that are non-piecewise continuous, but still appear as fundamental or very important in applications (in analysis, or any other domain of mathematics or physics) and are not simply exotic objects? (I am aware it's not a very well-defined question, but I hope it's understandable)",,"['real-analysis', 'integration', 'analysis', 'measure-theory', 'lebesgue-integral']"
80,If $\sin(h(x))=f(x)$ and $\cos(h(x))=g(x)$ can we determine $h(x)$?,If  and  can we determine ?,\sin(h(x))=f(x) \cos(h(x))=g(x) h(x),"If $\sin(h(x))=f(x)$ and $\cos(h(x))=g(x)$ for some fixed functions$f,g:\mathbb{R}\rightarrow [-1,1]$ can we determine $h(x)$? To be honest, I was asked to solve a problem with functions of the form $f,g:\mathbb{R}\rightarrow [-1,1]$ involved. So I thought if we could substitute $f,g$ with sines and cosines without loss of generality,(the same way we substitute real numbers with trigonometric functions when solving classical inequalities problems) since we were given that $f^2(x)+g^2(x)=1 \quad \forall x \in \mathbb{R}$. I know that functions are not the same with real numbers, but this is actually the whole reason of my question. Therefore my thought is the following: Since it is $f^2(x)+g^2(x)=1 \quad \forall x \in \mathbb{R}$ we can say that  $f(x)=\sin(h(x))$ and $g(x)=\cos(h(x))$ for some function $h(x)$ that its formula depends on $f(x)$ and $g(x)$. If the sine/cosine function was invertible in all its domain then we could determine $h(x)$ by applying the inverse function of sine in both sides of the equation $f(x)=\sin(h(x))$. But I'm stuck here...","If $\sin(h(x))=f(x)$ and $\cos(h(x))=g(x)$ for some fixed functions$f,g:\mathbb{R}\rightarrow [-1,1]$ can we determine $h(x)$? To be honest, I was asked to solve a problem with functions of the form $f,g:\mathbb{R}\rightarrow [-1,1]$ involved. So I thought if we could substitute $f,g$ with sines and cosines without loss of generality,(the same way we substitute real numbers with trigonometric functions when solving classical inequalities problems) since we were given that $f^2(x)+g^2(x)=1 \quad \forall x \in \mathbb{R}$. I know that functions are not the same with real numbers, but this is actually the whole reason of my question. Therefore my thought is the following: Since it is $f^2(x)+g^2(x)=1 \quad \forall x \in \mathbb{R}$ we can say that  $f(x)=\sin(h(x))$ and $g(x)=\cos(h(x))$ for some function $h(x)$ that its formula depends on $f(x)$ and $g(x)$. If the sine/cosine function was invertible in all its domain then we could determine $h(x)$ by applying the inverse function of sine in both sides of the equation $f(x)=\sin(h(x))$. But I'm stuck here...",,"['calculus', 'real-analysis', 'analysis', 'functions', 'trigonometry']"
81,"A set of infinite sequences of only 0s and 1s with a metric, their convergence and the topology they induce?","A set of infinite sequences of only 0s and 1s with a metric, their convergence and the topology they induce?",,"The entire question is this: Let $S$ be the set of sequences of 0s and 1s. For $x = \{x_1, x_2, x_3,...\}$ and $y = \{y_1, y_2, y_3, ...\}$ define $$ d(x,y) = \sum_{j=1}^{\infty} \frac{|x_j -yj|}{2^j} $$ Explain why the infinite sum in the definition converges for all $x$ and $y$. Prove that $d(x,y)$ is a metric. Let $E$ be the subset of $S$ consisting of all sequences that are eventually 0. Thus, $x = (x_1, x_2, x_3...)$ is  in $E$ if there exists $N \geq 0$ such that $x_n = 0$ for all $n \geq N$. Prove that $E$ is dense in $S$ under the topology induced by $d$. My first instinct with the metric infinite sum convergence is that of course it converges because either x=y and then it's a sum of infinite 0s, converging to 0, or if x $\neq$ y then we can define a subsequence that is a subsequence of the geometric sequence that converges to 1, so it converges to 1. I also want this to be true because then it induces the discrete topology which is really easy to work in. But I'm not sure if the convergence is true? Because I was also thinking that we could have an x where each $x_i$ = 0 and $y = \{1, 0, 0, 0...\}$. Then this would converge to 1/2. So how does this thing converge? Am I overthinking it or underthinking it? I'm stronger with topology than analysis but if it's not the discrete topology I'm not sure where to go.","The entire question is this: Let $S$ be the set of sequences of 0s and 1s. For $x = \{x_1, x_2, x_3,...\}$ and $y = \{y_1, y_2, y_3, ...\}$ define $$ d(x,y) = \sum_{j=1}^{\infty} \frac{|x_j -yj|}{2^j} $$ Explain why the infinite sum in the definition converges for all $x$ and $y$. Prove that $d(x,y)$ is a metric. Let $E$ be the subset of $S$ consisting of all sequences that are eventually 0. Thus, $x = (x_1, x_2, x_3...)$ is  in $E$ if there exists $N \geq 0$ such that $x_n = 0$ for all $n \geq N$. Prove that $E$ is dense in $S$ under the topology induced by $d$. My first instinct with the metric infinite sum convergence is that of course it converges because either x=y and then it's a sum of infinite 0s, converging to 0, or if x $\neq$ y then we can define a subsequence that is a subsequence of the geometric sequence that converges to 1, so it converges to 1. I also want this to be true because then it induces the discrete topology which is really easy to work in. But I'm not sure if the convergence is true? Because I was also thinking that we could have an x where each $x_i$ = 0 and $y = \{1, 0, 0, 0...\}$. Then this would converge to 1/2. So how does this thing converge? Am I overthinking it or underthinking it? I'm stronger with topology than analysis but if it's not the discrete topology I'm not sure where to go.",,"['general-topology', 'analysis', 'convergence-divergence', 'metric-spaces']"
82,Simple calcul of a limit superior,Simple calcul of a limit superior,,"What is the limit superior of : $$ \limsup_{x\rightarrow 10} 1*\mathbb{I}_{x<10}(x)+2*\mathbb{I}_{x=10}(x)+\frac{1}{2}*\mathbb{I}_{x>10}(x) ? $$ If I take the definition of a limit sup I arrive to 2, but I have a little doubt on my procedure.","What is the limit superior of : $$ \limsup_{x\rightarrow 10} 1*\mathbb{I}_{x<10}(x)+2*\mathbb{I}_{x=10}(x)+\frac{1}{2}*\mathbb{I}_{x>10}(x) ? $$ If I take the definition of a limit sup I arrive to 2, but I have a little doubt on my procedure.",,"['analysis', 'limsup-and-liminf']"
83,Prove $(rs)^{t}=r^{t}s^{t}$ for a real number $t\gt 0$,Prove  for a real number,(rs)^{t}=r^{t}s^{t} t\gt 0,"This lecture note proved $(rs)^{t}=r^{t}s^{t}$ for a real number $t\gt 0$ by using the following statement: $$\text{sup}(AB)=\text{sup}A\,\text{sup}B.$$ I found this specific step problematic For the property, if $r,s\gt 1$, then we can just use equation (1), noting that $\{(rs)^{a}:a\in\mathbb{Q}\,\text{and}\,a\le t\}=AB$, where $A=\{r^{a}:a\in\mathbb{Q}\,\text{and}\,a\le t\}$ and $B=\{s^{a}:a\in\mathbb{Q}\,\text{and}\,a\le t\}$. because $AB$ is defined as $\{ab:a\in A,b\in B\}.$ In other words, there are some elements in $AB$ where their exponents do not match, thereby not satisfying the above beautiful representation $(rs)^{a}.$ In conclusion, my question is Is my hypothesis right, which means the above proof does not hold?","This lecture note proved $(rs)^{t}=r^{t}s^{t}$ for a real number $t\gt 0$ by using the following statement: $$\text{sup}(AB)=\text{sup}A\,\text{sup}B.$$ I found this specific step problematic For the property, if $r,s\gt 1$, then we can just use equation (1), noting that $\{(rs)^{a}:a\in\mathbb{Q}\,\text{and}\,a\le t\}=AB$, where $A=\{r^{a}:a\in\mathbb{Q}\,\text{and}\,a\le t\}$ and $B=\{s^{a}:a\in\mathbb{Q}\,\text{and}\,a\le t\}$. because $AB$ is defined as $\{ab:a\in A,b\in B\}.$ In other words, there are some elements in $AB$ where their exponents do not match, thereby not satisfying the above beautiful representation $(rs)^{a}.$ In conclusion, my question is Is my hypothesis right, which means the above proof does not hold?",,"['real-analysis', 'analysis', 'exponentiation']"
84,Closure of the set of weak solutions of conservation laws,Closure of the set of weak solutions of conservation laws,,"Consider the conservation law $$u_t + q(u)_x = 0 \quad \tag{CL}$$ A function $u$ is a weak solution of $(CL)$ if $u \in L^\infty_\text{loc}((0, \infty)\times \mathbb{R})$ and $$\int_0^\infty \int_{-\infty}^{+\infty} uv_t + q(u)v_x \ \  dt \, dx = 0,$$ for every test function $v \in C_c^\infty((0,\infty)\times \mathbb{R}).$ Let $\{u_\epsilon\}$ be a sequence of weak solutions of $(\text{CL})$ and $u \in L^\infty((0,\infty) \times \mathbb{R})$. How does one prove that if, for every $\epsilon$, $\Vert u_\epsilon  \Vert_{L^\infty((0,\infty)\times \mathbb{R})} < B$ for some $B > 0$    and $u_\epsilon \to u$ in $L^1_\text{loc}((0,\infty) \times \mathbb{R})$,    then $u$ is a weak solution of $(\text{CL})$? Also, is it possible to prove a stronger similar theorem?","Consider the conservation law $$u_t + q(u)_x = 0 \quad \tag{CL}$$ A function $u$ is a weak solution of $(CL)$ if $u \in L^\infty_\text{loc}((0, \infty)\times \mathbb{R})$ and $$\int_0^\infty \int_{-\infty}^{+\infty} uv_t + q(u)v_x \ \  dt \, dx = 0,$$ for every test function $v \in C_c^\infty((0,\infty)\times \mathbb{R}).$ Let $\{u_\epsilon\}$ be a sequence of weak solutions of $(\text{CL})$ and $u \in L^\infty((0,\infty) \times \mathbb{R})$. How does one prove that if, for every $\epsilon$, $\Vert u_\epsilon  \Vert_{L^\infty((0,\infty)\times \mathbb{R})} < B$ for some $B > 0$    and $u_\epsilon \to u$ in $L^1_\text{loc}((0,\infty) \times \mathbb{R})$,    then $u$ is a weak solution of $(\text{CL})$? Also, is it possible to prove a stronger similar theorem?",,['functional-analysis']
85,"In a metric space $(S,d)$, assume that $x_n \to x$ and $y_n \to y$. Prove that $d(x_n, y_n) \to d (x, y)$.","In a metric space , assume that  and . Prove that .","(S,d) x_n \to x y_n \to y d(x_n, y_n) \to d (x, y)","Let $(X,d)$ be a metric space and $(x_n)$,$(y_n)$ be sequences in $X$. (a) If $x_n \to x$ and $y_n \to y$, prove that $d(x_n,y_n) \to d(x,y)$. (b) If $x_n$ and $y_n$ are Cauchy sequences in $X$, prove that the real sequence $d(x_n,y_n)$ is convergent. Proof. (a) Is this correct?: Let $x,y,w,z∈X$. The triangle inequality implies that $|d(x,z)-d(w,y)| \leq d(x,y)+d(z,w)$, so $|d(x_n,y_n)-d(x,y)| \leq d(x_n,x)+d(y_n,y)$ which implies that $d(x_n,y_n) \to d(x, y)$ as $n \to ∞$ if $d(x_n,x) \to 0$ and $d (y_n, y) \to 0$. (b) I have no ideas, you help me please?","Let $(X,d)$ be a metric space and $(x_n)$,$(y_n)$ be sequences in $X$. (a) If $x_n \to x$ and $y_n \to y$, prove that $d(x_n,y_n) \to d(x,y)$. (b) If $x_n$ and $y_n$ are Cauchy sequences in $X$, prove that the real sequence $d(x_n,y_n)$ is convergent. Proof. (a) Is this correct?: Let $x,y,w,z∈X$. The triangle inequality implies that $|d(x,z)-d(w,y)| \leq d(x,y)+d(z,w)$, so $|d(x_n,y_n)-d(x,y)| \leq d(x_n,x)+d(y_n,y)$ which implies that $d(x_n,y_n) \to d(x, y)$ as $n \to ∞$ if $d(x_n,x) \to 0$ and $d (y_n, y) \to 0$. (b) I have no ideas, you help me please?",,"['analysis', 'proof-verification', 'metric-spaces', 'cauchy-sequences']"
86,Every Suslin set is the continuous image of some closed set in $\mathbb N^{\omega}$.,Every Suslin set is the continuous image of some closed set in .,\mathbb N^{\omega},"Let $A$ be some countable set, and let $A^{\omega}$ be the set of all sequences $\mathbb N \to A$ equipped with the product topology. A subset $X \subseteq A^{\omega}$ is called Suslin set if there is a countable set $B$, a Borel subset $Y \subseteq B^{\omega}$ and a continuous map $f : B^{\omega} \to A^{\omega}$ such that $X = f(Y)$, i.e. it is the continuous image of some Borel set. Does anybody has a proof of the following result: If $X \subseteq A^{\omega}$ is a Suslin set, then there exists a closed set $Y \subseteq \mathbb N^{\omega}$ and a continuous map $f : \mathbb N^{\omega} \to A^{\omega}$ such that $X = f(Y)$. It is mentioned in a book, but its proof is faulty and I cannot find it anywhere else.","Let $A$ be some countable set, and let $A^{\omega}$ be the set of all sequences $\mathbb N \to A$ equipped with the product topology. A subset $X \subseteq A^{\omega}$ is called Suslin set if there is a countable set $B$, a Borel subset $Y \subseteq B^{\omega}$ and a continuous map $f : B^{\omega} \to A^{\omega}$ such that $X = f(Y)$, i.e. it is the continuous image of some Borel set. Does anybody has a proof of the following result: If $X \subseteq A^{\omega}$ is a Suslin set, then there exists a closed set $Y \subseteq \mathbb N^{\omega}$ and a continuous map $f : \mathbb N^{\omega} \to A^{\omega}$ such that $X = f(Y)$. It is mentioned in a book, but its proof is faulty and I cannot find it anywhere else.",,"['analysis', 'set-theory', 'descriptive-set-theory']"
87,"For any subset $S$ of $\mathbb{R}$, the distance function $d_s(x) = d(x,S)$ is continuous","For any subset  of , the distance function  is continuous","S \mathbb{R} d_s(x) = d(x,S)","Let $S \subset \mathbb R$ be any set, and define for any $x \in \mathbb R$ the distance between $x$ and the set $S$ by $d(x,S) = \inf\{|x-s| : s \in S\}$. Prove that the function $d_s: \mathbb R \to [0,+\infty)$ given by $d_s(x) = d(x,S)$, is Lipschitz continuous. Prove that if $S$ is compact then for every $x$ in $\mathbb R$, there is $s$ in $S$ such that $|x-s| = d(x,S).$","Let $S \subset \mathbb R$ be any set, and define for any $x \in \mathbb R$ the distance between $x$ and the set $S$ by $d(x,S) = \inf\{|x-s| : s \in S\}$. Prove that the function $d_s: \mathbb R \to [0,+\infty)$ given by $d_s(x) = d(x,S)$, is Lipschitz continuous. Prove that if $S$ is compact then for every $x$ in $\mathbb R$, there is $s$ in $S$ such that $|x-s| = d(x,S).$",,"['real-analysis', 'analysis']"
88,Definition second differential of a vector field,Definition second differential of a vector field,,"Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ be a smooth function. Then we know that its differential $df: \mathbb{R}^2 \rightarrow Hom(\mathbb{R}^2,\mathbb{R}^2)$ maps vectors to matrices/linear maps. But how do we define $D^2 f$? Is it defined like $D^2f: \mathbb{R}^2 \rightarrow Hom(\mathbb{R}^2, Hom(\mathbb{R}^2, \mathbb{R}^2))$ (and in this case: how do I write it explicitly?) or is it defined like $D^2 f(y)(z,w) := D(Df(y)(w))(z)$ or like something else? In the solutions of the problem 5.2 of this problem sheet there's this ( screenshot ). I don't understand why does $D^2f$ have two arguments, $w$ and $z$. Is it possible to generalize the whole thing for $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$? Thank you!","Let $f: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ be a smooth function. Then we know that its differential $df: \mathbb{R}^2 \rightarrow Hom(\mathbb{R}^2,\mathbb{R}^2)$ maps vectors to matrices/linear maps. But how do we define $D^2 f$? Is it defined like $D^2f: \mathbb{R}^2 \rightarrow Hom(\mathbb{R}^2, Hom(\mathbb{R}^2, \mathbb{R}^2))$ (and in this case: how do I write it explicitly?) or is it defined like $D^2 f(y)(z,w) := D(Df(y)(w))(z)$ or like something else? In the solutions of the problem 5.2 of this problem sheet there's this ( screenshot ). I don't understand why does $D^2f$ have two arguments, $w$ and $z$. Is it possible to generalize the whole thing for $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$? Thank you!",,"['calculus', 'analysis', 'differential']"
89,Show the equality,Show the equality,,"Let $(X, \mathcal{A}, \mu)$ space with measure, $\mu(X) = 1$ , $\epsilon > 0$ and $f: X \rightarrow [\epsilon,\infty)$ a $\mathcal{A}$ -measurable and bounded function, I've tried show $$\lim_{p \rightarrow 0^{+}} \left(\int_{X} f^p(x)d\mu(x) \right)^{\frac{1}{p}} = \exp\left(\int_X \ln(f(x))d\mu(x)\right)$$ My attempt: I know is true, in this case, $\displaystyle \int_X \ln(f(x))d\mu(x) \leq \ln\left(\int_Xf(x)d\mu(x)\right)$ , because $\ln(t) \leq t - 1$ , $\forall t > 0$ , take $t = \frac{f(x)}{\int_Xf(x)d\mu(x)}$ and integrate over $X$ . So $\left(\int_{X} f^p(x)d\mu(x) \right)^{\frac{1}{p}} = \exp\left(\frac{1}{p}\ln\left(\int_X f^p(x)d\mu(x)\right)\right) \geq \exp\left(\frac{1}{p}\int_X \ln(f^p(x))d\mu(x) \right) = \exp\left(\int_X \ln(f(x))d\mu(x) \right)$ , then $$\lim_{p \rightarrow 0^{+}} \left(\int_{X} f^p(x)d\mu(x) \right)^{\frac{1}{p}} \geq \exp\left(\int_X \ln(f(x))d\mu(x)\right)$$ If $\left(\int_{X} f^p(x)d\mu(x) \right)^{\frac{1}{p}}$ is decreasing, the equality it's ok, but I don't know how show this. There is a hint in exercise, $\displaystyle \lim_{t \rightarrow 0^+} \frac{a^t - 1}{t} = \ln(a)$ . I've tried use this limit for show decreasing, but it wasn't successful. Thank you.","Let space with measure, , and a -measurable and bounded function, I've tried show My attempt: I know is true, in this case, , because , , take and integrate over . So , then If is decreasing, the equality it's ok, but I don't know how show this. There is a hint in exercise, . I've tried use this limit for show decreasing, but it wasn't successful. Thank you.","(X, \mathcal{A}, \mu) \mu(X) = 1 \epsilon > 0 f: X \rightarrow [\epsilon,\infty) \mathcal{A} \lim_{p \rightarrow 0^{+}} \left(\int_{X} f^p(x)d\mu(x) \right)^{\frac{1}{p}} = \exp\left(\int_X \ln(f(x))d\mu(x)\right) \displaystyle \int_X \ln(f(x))d\mu(x) \leq \ln\left(\int_Xf(x)d\mu(x)\right) \ln(t) \leq t - 1 \forall t > 0 t = \frac{f(x)}{\int_Xf(x)d\mu(x)} X \left(\int_{X} f^p(x)d\mu(x) \right)^{\frac{1}{p}} = \exp\left(\frac{1}{p}\ln\left(\int_X f^p(x)d\mu(x)\right)\right) \geq \exp\left(\frac{1}{p}\int_X \ln(f^p(x))d\mu(x) \right) = \exp\left(\int_X \ln(f(x))d\mu(x) \right) \lim_{p \rightarrow 0^{+}} \left(\int_{X} f^p(x)d\mu(x) \right)^{\frac{1}{p}} \geq \exp\left(\int_X \ln(f(x))d\mu(x)\right) \left(\int_{X} f^p(x)d\mu(x) \right)^{\frac{1}{p}} \displaystyle \lim_{t \rightarrow 0^+} \frac{a^t - 1}{t} = \ln(a)","['integration', 'analysis', 'measure-theory']"
90,Dilation convergence in $L^1$,Dilation convergence in,L^1,"Below is a question, which I asked before, from Stein's Real Analysis.  I've provided a partial solution, which I think it's pretty along the lines of what needs to be done, however, I have no finished the solution.  If anyone has a hint for the part I'm stuck at, I would very much appreciate it. :) Question:  Prove that if $f$ is integrable on $\mathbb{R}^d$ and $\delta > 0$, then $f(\delta x)$ converges to $f(x)$ in $L^1$-norm as $\delta \to 1$. My Attempt: Let $\epsilon > 0$ be given and suppose $\delta > 1$ (if not, just take $1/\delta$ in place of $\delta$). Since $f$ is integrable on $\mathbb{R}^d$, there exists a function $g$ continuous on $\mathbb{R}^d$ supported on a compact set $k$, with $\|f(x)-g(x)\| < \epsilon/3$.  Applying the triangle inequality, one finds that  $$ \|f(\delta x) - f(x)\| \leq \|f(\delta x) - g(\delta x)\| + \|f(x) - g(x)\| + \|g(\delta x) - g(x)\|. $$ With the assumptions on $f$ and $g$, together with the observation that  $$ \|(f-g)(\delta x)\| = \int_{\mathbb{R}^d} |(f-g)(\delta x)| dx = \frac{1}{\delta^d} \int_{\mathbb{R}^d} |(f-g)(x)| dx = \frac{1}{\delta^d} \|(f-g)(x)\| $$ one can estimate $\|(f(\delta x) - f(x)\|$ by  $$ \|f(\delta x) - f(x)\| < \frac{2 \epsilon}{3} + \|g(\delta x) - g(x)\|. $$ To complete the proof, it remains only to show that  $$ \|g(\delta x) - g(x)\| \to 0 \quad \text{ as } \quad \delta \to 1. $$ Observe that the function $g(\delta x)$ is supported on the set $\delta K$, a compact set; hence, the difference function $g(\delta x) - g(x)$ is supported on the compact set $\delta K \cup K$. Since $g(\delta x)$ is uniformly continuous on $\delta K$ and since $g(x)$ is uniformly continuous on $K$, there exist positive constants $L$ and $M$, with $|g(\delta x)| \leq L$ And $|g(x)| \leq M$; hence writing,  $$ \delta K \cup K = (\delta K \Delta K) \cup (\delta K \cap K), $$ a disjoint union, where $\delta K \Delta K = (\delta K \setminus K) \cup (K \setminus \delta K)$, and applying the triangle inequality, it follows that  $$ \|g(\delta x) - g(x)\| \leq \int_{\delta K \Delta K} |g(\delta x)| dx + \int_{\delta K \Delta K} |g(x)| dx + \int_{\delta K \cap K} |g(\delta x) - g(x)| dx. $$ Using the definition of the Lebesgue integral, one then finds that  $$ \|g(\delta x) - g(x)\| \leq 2(L+M) m(\delta K \Delta K) +  \int_{\delta K \cap K} |g(\delta x) - g(x)| dx. $$ Now, if ${c_n}$ is any sequences of positive numbers such that $\delta = c_1 \geq c_2 \geq \dots$ and $c_n \geq 1$ for all $n$, decreasing to $1$, then the corresponding sequences of compact set $K_n = c_n K \Delta K$ decreases to the empty set, that is,  $$ \bigcap_{n=1}^{\infty} K_n = \emptyset. $$ Moreover, since $m(K_1)$ has finite measure, by the continuity of Lebesgue measure, it follows that $m(K_n) \to 0$ as $c_n \to 1$. Hence, there exists $N \geq 1$ with $m(\delta K \Delta K) < \epsilon/12(L+M)$ whenever $n \geq n$ and $\delta \in (c_n, 1]$.  Thus,  $$ \|f(\delta x)- f(x)\| < \frac{2\epsilon}{3} + \frac{\epsilon}{6} + \int_{\delta K \cap K} |g(\delta x) -g(x)|dx $$ whenever $\delta$ is sufficiently close to $1$. Note:  The issue I'm having is dealing with the final integral $\int_{\delta K \cap K} |g(\delta x)-g(x)| dx$.  Any hints would be appreciated. : )","Below is a question, which I asked before, from Stein's Real Analysis.  I've provided a partial solution, which I think it's pretty along the lines of what needs to be done, however, I have no finished the solution.  If anyone has a hint for the part I'm stuck at, I would very much appreciate it. :) Question:  Prove that if $f$ is integrable on $\mathbb{R}^d$ and $\delta > 0$, then $f(\delta x)$ converges to $f(x)$ in $L^1$-norm as $\delta \to 1$. My Attempt: Let $\epsilon > 0$ be given and suppose $\delta > 1$ (if not, just take $1/\delta$ in place of $\delta$). Since $f$ is integrable on $\mathbb{R}^d$, there exists a function $g$ continuous on $\mathbb{R}^d$ supported on a compact set $k$, with $\|f(x)-g(x)\| < \epsilon/3$.  Applying the triangle inequality, one finds that  $$ \|f(\delta x) - f(x)\| \leq \|f(\delta x) - g(\delta x)\| + \|f(x) - g(x)\| + \|g(\delta x) - g(x)\|. $$ With the assumptions on $f$ and $g$, together with the observation that  $$ \|(f-g)(\delta x)\| = \int_{\mathbb{R}^d} |(f-g)(\delta x)| dx = \frac{1}{\delta^d} \int_{\mathbb{R}^d} |(f-g)(x)| dx = \frac{1}{\delta^d} \|(f-g)(x)\| $$ one can estimate $\|(f(\delta x) - f(x)\|$ by  $$ \|f(\delta x) - f(x)\| < \frac{2 \epsilon}{3} + \|g(\delta x) - g(x)\|. $$ To complete the proof, it remains only to show that  $$ \|g(\delta x) - g(x)\| \to 0 \quad \text{ as } \quad \delta \to 1. $$ Observe that the function $g(\delta x)$ is supported on the set $\delta K$, a compact set; hence, the difference function $g(\delta x) - g(x)$ is supported on the compact set $\delta K \cup K$. Since $g(\delta x)$ is uniformly continuous on $\delta K$ and since $g(x)$ is uniformly continuous on $K$, there exist positive constants $L$ and $M$, with $|g(\delta x)| \leq L$ And $|g(x)| \leq M$; hence writing,  $$ \delta K \cup K = (\delta K \Delta K) \cup (\delta K \cap K), $$ a disjoint union, where $\delta K \Delta K = (\delta K \setminus K) \cup (K \setminus \delta K)$, and applying the triangle inequality, it follows that  $$ \|g(\delta x) - g(x)\| \leq \int_{\delta K \Delta K} |g(\delta x)| dx + \int_{\delta K \Delta K} |g(x)| dx + \int_{\delta K \cap K} |g(\delta x) - g(x)| dx. $$ Using the definition of the Lebesgue integral, one then finds that  $$ \|g(\delta x) - g(x)\| \leq 2(L+M) m(\delta K \Delta K) +  \int_{\delta K \cap K} |g(\delta x) - g(x)| dx. $$ Now, if ${c_n}$ is any sequences of positive numbers such that $\delta = c_1 \geq c_2 \geq \dots$ and $c_n \geq 1$ for all $n$, decreasing to $1$, then the corresponding sequences of compact set $K_n = c_n K \Delta K$ decreases to the empty set, that is,  $$ \bigcap_{n=1}^{\infty} K_n = \emptyset. $$ Moreover, since $m(K_1)$ has finite measure, by the continuity of Lebesgue measure, it follows that $m(K_n) \to 0$ as $c_n \to 1$. Hence, there exists $N \geq 1$ with $m(\delta K \Delta K) < \epsilon/12(L+M)$ whenever $n \geq n$ and $\delta \in (c_n, 1]$.  Thus,  $$ \|f(\delta x)- f(x)\| < \frac{2\epsilon}{3} + \frac{\epsilon}{6} + \int_{\delta K \cap K} |g(\delta x) -g(x)|dx $$ whenever $\delta$ is sufficiently close to $1$. Note:  The issue I'm having is dealing with the final integral $\int_{\delta K \cap K} |g(\delta x)-g(x)| dx$.  Any hints would be appreciated. : )",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
91,Analysis Problem [closed],Analysis Problem [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Given that: For any $[a, b]\subset (-\infty,+\infty)$, $f$ is integrable in $[a,b]$, $p>0$, and ${\mid f\mid}^{p}$ is integrable in $(-\infty,+\infty)$. Prove that $$\lim_{h\to0}\int_{-\infty}^{+\infty }\mid {f(x+h)-f(x)}\mid ^{p} dx=0 $$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Given that: For any $[a, b]\subset (-\infty,+\infty)$, $f$ is integrable in $[a,b]$, $p>0$, and ${\mid f\mid}^{p}$ is integrable in $(-\infty,+\infty)$. Prove that $$\lim_{h\to0}\int_{-\infty}^{+\infty }\mid {f(x+h)-f(x)}\mid ^{p} dx=0 $$",,['analysis']
92,"$p(x) \in \mathbb R[x]$ be a polynomial of odd degree , $n>1$ be an integer , then is the function $A \to p(A)$ surjective on $M(n,\mathbb R)$?","be a polynomial of odd degree ,  be an integer , then is the function  surjective on ?","p(x) \in \mathbb R[x] n>1 A \to p(A) M(n,\mathbb R)","Let $p(x) \in \mathbb R[x]$ be a polynomial of odd degree , $n>1$ be an integer , then is the function $f: M(n,\mathbb R) \to M(n, \mathbb R)$ defined as $f(A)=p(A) , \forall A \in M(n,\mathbb R)$ surjective ? This is related $p(x) \in \mathbb R[x]$ be non-constant polynomial , $n>1$ , the function $A \to p(A)$ is surjective on $M(n, \mathbb C)$?","Let $p(x) \in \mathbb R[x]$ be a polynomial of odd degree , $n>1$ be an integer , then is the function $f: M(n,\mathbb R) \to M(n, \mathbb R)$ defined as $f(A)=p(A) , \forall A \in M(n,\mathbb R)$ surjective ? This is related $p(x) \in \mathbb R[x]$ be non-constant polynomial , $n>1$ , the function $A \to p(A)$ is surjective on $M(n, \mathbb C)$?",,"['linear-algebra', 'matrices']"
93,Is this true for functions with certain conditions?,Is this true for functions with certain conditions?,,"Let $f$ and $g$ be real-valued functions defined for all real values of $x$ and $y$, and satisfying the equation $f(x + y) + f(x − y) = 2f(x)g(y)$ for all $x$, $y$. Is it true that if $f(x)$ is not identically zero, and if $|f(x)| ≤ 1$ for all $x$, then $|g(y)| ≤ 1$ for all $y$?","Let $f$ and $g$ be real-valued functions defined for all real values of $x$ and $y$, and satisfying the equation $f(x + y) + f(x − y) = 2f(x)g(y)$ for all $x$, $y$. Is it true that if $f(x)$ is not identically zero, and if $|f(x)| ≤ 1$ for all $x$, then $|g(y)| ≤ 1$ for all $y$?",,['analysis']
94,Intermediate Value Like Property for Lebesgue Measure,Intermediate Value Like Property for Lebesgue Measure,,"Below is a question from N.L. Carother's book Real Analysis .  I've provided my attempt at a solutions, however, any feed back would be very appreciated. Suppose $E$ is a measurable subset of $\mathbb{R}$ such that $m(E) = 1$ .  Show that: (a) There is a measurable set $F$ with $F \subset E$ such that $m(F) = 1/2$ . (b) There is a closed set $F$ , consisting entirely of irrationals, such that $F \subset E$ and $m(F) = 1/2$ . (c) There is a compact set $F$ with empty interior such that $F \subset E$ and $m(F) = 1/2$ . My Attempt: (a) Define $f: \mathbb{R} \to \mathbb{R}$ be defined by $f(x) = m(V_x)$ , where $V_x = E \cap (-\infty, x]$ for each $x $ . It suffices to show that $f$ is an increasing continuous function and apply the Intermediate Value Theorem. To show that $f$ is increasing, suppose that $x<y$ and note that $V_x \subset V_y$ so that, by the monotonicity of the Lebesgue measure, $f(x) = m(V_x) \leq m(V_y) = f(y)$ . Fix any $x \in \mathbb{R}$ and $\epsilon > 0$ ; choose $\delta = \epsilon$ . Then, whenever $|x-y| < \delta$ with $x < y$ , we have that \begin{align} |f(x) - f(y)| \leq m(V_y \setminus V_x) \leq |x-y| < \delta = \epsilon, \end{align} which shows that $f$ is continuous. Now, as $x$ increases, the sets $V_x$ increase to $E$ . Hence $0 \leq f(x) \leq m(E) = 1$ for all $x$ . Thus, by the Intermediate value theorem, there exists some $u \in \mathbb{R}$ for which $f(u) = 1/2$ ; that is, $m(V_u) = 1/2.$ Setting $F = V_u = E \cap (-\infty, u]\subset E$ is our desired measurable set. (b) Let $(r_n)$ be an enumeration of the rationals. Define the set $R$ to be the union $$ R = \bigcup_{n=1}^{\infty} \left( r_n - \frac{1}{2^n}, r_n + \frac{1}{2^n} \right). $$ Once can see, without too much effort, that $m(R) = 2$ ; hence its complete, a closed set of infinite measure, $R^c$ as constructed consists of only irrationals. *From here I've been pretty stuck; I'm still sitting on the ideas, however, I'm not sure where to go.  Any HINTS would be of great asset.  : )","Below is a question from N.L. Carother's book Real Analysis .  I've provided my attempt at a solutions, however, any feed back would be very appreciated. Suppose is a measurable subset of such that .  Show that: (a) There is a measurable set with such that . (b) There is a closed set , consisting entirely of irrationals, such that and . (c) There is a compact set with empty interior such that and . My Attempt: (a) Define be defined by , where for each . It suffices to show that is an increasing continuous function and apply the Intermediate Value Theorem. To show that is increasing, suppose that and note that so that, by the monotonicity of the Lebesgue measure, . Fix any and ; choose . Then, whenever with , we have that which shows that is continuous. Now, as increases, the sets increase to . Hence for all . Thus, by the Intermediate value theorem, there exists some for which ; that is, Setting is our desired measurable set. (b) Let be an enumeration of the rationals. Define the set to be the union Once can see, without too much effort, that ; hence its complete, a closed set of infinite measure, as constructed consists of only irrationals. *From here I've been pretty stuck; I'm still sitting on the ideas, however, I'm not sure where to go.  Any HINTS would be of great asset.  : )","E \mathbb{R} m(E) = 1 F F \subset E m(F) = 1/2 F F \subset E m(F) = 1/2 F F \subset E m(F) = 1/2 f: \mathbb{R} \to \mathbb{R} f(x) = m(V_x) V_x = E \cap (-\infty, x] x  f f x<y V_x \subset V_y f(x) = m(V_x) \leq m(V_y) = f(y) x \in \mathbb{R} \epsilon > 0 \delta = \epsilon |x-y| < \delta x < y \begin{align}
|f(x) - f(y)| \leq m(V_y \setminus V_x) \leq |x-y| < \delta = \epsilon,
\end{align} f x V_x E 0 \leq f(x) \leq m(E) = 1 x u \in \mathbb{R} f(u) = 1/2 m(V_u) = 1/2. F = V_u = E \cap (-\infty, u]\subset E (r_n) R 
R = \bigcup_{n=1}^{\infty} \left( r_n - \frac{1}{2^n}, r_n + \frac{1}{2^n} \right).
 m(R) = 2 R^c","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
95,What properties do hyperreal extensions of real functions have?,What properties do hyperreal extensions of real functions have?,,"If I have a function $f : \mathbb R \to \mathbb R$ and extend it to the hyperreal function $f^* : \mathbb R^* \to \mathbb R^*$, what are some of the properties that I know $f^*$ must have? Specifically, is there any ""defining"" property? That is, is there a property $P$ such that we can say $f^*$ is the only hyperreal function that agrees with $f$ on the reals and satisfies $P$ ? (Analogously to how for any uniformly continuous function defined on a dense set, there is exactly one continuous extension defined on all reals) Edit : Apparently, I didn't make it clear enough what, exactly, I am interested in, and I'm sorry for that. I am not asking which properties of $f$ carry over to $f^*$; I am asking, if you don't know $f$ , which properties can still conclude $f^*$ must have. To rephrase my question in a way that, I hope, eliminates this ambiguity: If you hav some function $g : \mathbb R^* \to \mathbb R^*$, what are some necessary conditions of the proposition There is some function $f : \mathbb R \to \mathbb R$ such that $g = f^*$ ?","If I have a function $f : \mathbb R \to \mathbb R$ and extend it to the hyperreal function $f^* : \mathbb R^* \to \mathbb R^*$, what are some of the properties that I know $f^*$ must have? Specifically, is there any ""defining"" property? That is, is there a property $P$ such that we can say $f^*$ is the only hyperreal function that agrees with $f$ on the reals and satisfies $P$ ? (Analogously to how for any uniformly continuous function defined on a dense set, there is exactly one continuous extension defined on all reals) Edit : Apparently, I didn't make it clear enough what, exactly, I am interested in, and I'm sorry for that. I am not asking which properties of $f$ carry over to $f^*$; I am asking, if you don't know $f$ , which properties can still conclude $f^*$ must have. To rephrase my question in a way that, I hope, eliminates this ambiguity: If you hav some function $g : \mathbb R^* \to \mathbb R^*$, what are some necessary conditions of the proposition There is some function $f : \mathbb R \to \mathbb R$ such that $g = f^*$ ?",,"['analysis', 'nonstandard-analysis']"
96,How to find the set of values $S$ where $f$ is not differentiable?,How to find the set of values  where  is not differentiable?,S f,"Let's assume we are given an arbitrary function $f : \mathbb{R} \to \mathbb{R}$, and for the purposes of this question, let's assume we know nothing about the differentiability of $f$, i.e. we have no pre-requesite knowledge about the differentiability of $f$ Now for a function to be differentiable at some point $a$, the following limit must exist and be finite $$f'(a) = \lim_{h \to 0} \frac{f(a+h)-f(a)}{h}$$ Furthermore a function is not differentiable at some point $a$, if : $f$ is discontinuous at $a$, i.e $\lim_{x \to a}f(x) \neq f(a)$ $f'(a)$ does not exist $f$ has a vertical tangent at $a$, i.e. $\lim_{h\to 0} \frac{f(a+h)-f(a)}{h} = \infty$ If we do not know anything about the differentiability of $f$ beforehand, how do we go about finding the set of values (which could be zero, finite or infinite) for which $f$ is not differentiable? $S = \{a \ | a \in \mathbb{R} \text{ and} \ a \text{ satisfies conditions 1 or 2 or 3 above}\}$ What I'm trying to ask in a nutshell: In Real Analysis, what methods/techniques are commonly used to prove an arbitrary function $f$ is differentiable  $\forall \ a \in \mathbb{R}$, or to find the set of values $S$, where $f$ is not differentiable.? I say specifically an arbitrary function $f$, as there might be different techniques used for algebraic functions as opposed to say transcendental functions. From what I've encountered in introductory Real Analysis, this is not something that is gone into great depth or detail. What I want to know is how did Mathematicians prove that functions such as $f(x) = e^x$ are differentiable $\forall x \in \mathbb{R}$, and how did they prove that functions such as $f(x) = \sqrt[3]{x}$, are not differentiable at certain values of $x$ on the Real Field. (I've given a simple example for the second case, however I'm sure that there are functions, which are not differentiable at many points in $\mathbb{R}$). Elementary techniques such as Mathematical Induction fall apart when working with $\mathbb{R}$. Furthermore for finding the set of values $S$ where a function is not differentiable, I'm sure they didn't just find them heuristically by testing values of possible interest for a given function. So what techniques are commonly used to formulate these proofs?","Let's assume we are given an arbitrary function $f : \mathbb{R} \to \mathbb{R}$, and for the purposes of this question, let's assume we know nothing about the differentiability of $f$, i.e. we have no pre-requesite knowledge about the differentiability of $f$ Now for a function to be differentiable at some point $a$, the following limit must exist and be finite $$f'(a) = \lim_{h \to 0} \frac{f(a+h)-f(a)}{h}$$ Furthermore a function is not differentiable at some point $a$, if : $f$ is discontinuous at $a$, i.e $\lim_{x \to a}f(x) \neq f(a)$ $f'(a)$ does not exist $f$ has a vertical tangent at $a$, i.e. $\lim_{h\to 0} \frac{f(a+h)-f(a)}{h} = \infty$ If we do not know anything about the differentiability of $f$ beforehand, how do we go about finding the set of values (which could be zero, finite or infinite) for which $f$ is not differentiable? $S = \{a \ | a \in \mathbb{R} \text{ and} \ a \text{ satisfies conditions 1 or 2 or 3 above}\}$ What I'm trying to ask in a nutshell: In Real Analysis, what methods/techniques are commonly used to prove an arbitrary function $f$ is differentiable  $\forall \ a \in \mathbb{R}$, or to find the set of values $S$, where $f$ is not differentiable.? I say specifically an arbitrary function $f$, as there might be different techniques used for algebraic functions as opposed to say transcendental functions. From what I've encountered in introductory Real Analysis, this is not something that is gone into great depth or detail. What I want to know is how did Mathematicians prove that functions such as $f(x) = e^x$ are differentiable $\forall x \in \mathbb{R}$, and how did they prove that functions such as $f(x) = \sqrt[3]{x}$, are not differentiable at certain values of $x$ on the Real Field. (I've given a simple example for the second case, however I'm sure that there are functions, which are not differentiable at many points in $\mathbb{R}$). Elementary techniques such as Mathematical Induction fall apart when working with $\mathbb{R}$. Furthermore for finding the set of values $S$ where a function is not differentiable, I'm sure they didn't just find them heuristically by testing values of possible interest for a given function. So what techniques are commonly used to formulate these proofs?",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'continuity']"
97,Show completeness of metric subspace,Show completeness of metric subspace,,"I have problems solving the following 2 problems: Given is the metric $d:\Bbb R\times\Bbb R\to[0,\infty[$ with $$d(x,y):=|\arctan(x)-\arctan(y)|\;.$$ a) Show that the metric subspace $\big([-1,1], d\mid\big([-1,1]\times[-1,1]\big)\big)$ is complete. (I had to show before that the space as a whole is not complete which I already did.) b) Is $[-1,1]$ in $(\Bbb R, d)$ a compact set? Thanks for your help in advance, and maybe someone could help with the formatting.","I have problems solving the following 2 problems: Given is the metric with a) Show that the metric subspace is complete. (I had to show before that the space as a whole is not complete which I already did.) b) Is in a compact set? Thanks for your help in advance, and maybe someone could help with the formatting.","d:\Bbb R\times\Bbb R\to[0,\infty[ d(x,y):=|\arctan(x)-\arctan(y)|\;. \big([-1,1], d\mid\big([-1,1]\times[-1,1]\big)\big) [-1,1] (\Bbb R, d)","['general-topology', 'analysis', 'metric-spaces']"
98,Prove that a subset C of $\mathbb R^n$ is closed if and only if it contains all its limit points.,Prove that a subset C of  is closed if and only if it contains all its limit points.,\mathbb R^n,"Prove that a subset C of $\mathbb R^n$ is closed if and only if it contains all its limit points. A closed set is defined by a set of all boundary points My professor said  ""We may prove that C is not closed if and only if $C^c$ has a limit point of C. To prove this, it suffices to show that a point x in $C^c$ is a boundary point of C if and only it is a limit point of C."" and he left it as an exercise. I am a beginner of analysis and I want you to check if my proof is okay. This is how I prove: If part) Let $x\in C^c$ and x is a limit point of C. Then there is a sequence $\{x_k\}$ of distinct points in C such that $0<\Vert x_k -x \Vert<1/k$. Thus $N'(x;r)\bigcap C$ is infinite. It follows that $N(x;r)\bigcap C\neq\emptyset$. Since $x\in C^c$, we get $N(x;r)\bigcap C^c\neq\emptyset$. Hence x is a boundary point of C. Only if part) Let $x\in C^c$. Since x is a boundary point of C, there exists $x_k\in C$ such that $x=\lim_{k\to \infty} x_k$. Choose $\{x_{k_j}\}$ where $x_{k_j}\neq x_{k_l}.$ Then $\Vert x- x_{k_j} \Vert > \Vert x- x_{k_{j+1}} \Vert$, so $\{x_{k_j}\}$ converges to x. Thus x is a limit point. Please tell me if there is wrong or insufficient. Is it okay just to say that ""Choose $\{x_{k_j}\}$ where $x_{k_j}\neq x_{k_l}$""?","Prove that a subset C of $\mathbb R^n$ is closed if and only if it contains all its limit points. A closed set is defined by a set of all boundary points My professor said  ""We may prove that C is not closed if and only if $C^c$ has a limit point of C. To prove this, it suffices to show that a point x in $C^c$ is a boundary point of C if and only it is a limit point of C."" and he left it as an exercise. I am a beginner of analysis and I want you to check if my proof is okay. This is how I prove: If part) Let $x\in C^c$ and x is a limit point of C. Then there is a sequence $\{x_k\}$ of distinct points in C such that $0<\Vert x_k -x \Vert<1/k$. Thus $N'(x;r)\bigcap C$ is infinite. It follows that $N(x;r)\bigcap C\neq\emptyset$. Since $x\in C^c$, we get $N(x;r)\bigcap C^c\neq\emptyset$. Hence x is a boundary point of C. Only if part) Let $x\in C^c$. Since x is a boundary point of C, there exists $x_k\in C$ such that $x=\lim_{k\to \infty} x_k$. Choose $\{x_{k_j}\}$ where $x_{k_j}\neq x_{k_l}.$ Then $\Vert x- x_{k_j} \Vert > \Vert x- x_{k_{j+1}} \Vert$, so $\{x_{k_j}\}$ converges to x. Thus x is a limit point. Please tell me if there is wrong or insufficient. Is it okay just to say that ""Choose $\{x_{k_j}\}$ where $x_{k_j}\neq x_{k_l}$""?",,['analysis']
99,How to prove $ \int_{a}^{b}f\left ( x \right )\cot\left ( x \right )dx=2\sum_{n=1}^{\infty }f\left ( x \right )\sin\left ( 2nx \right )dx$,How to prove, \int_{a}^{b}f\left ( x \right )\cot\left ( x \right )dx=2\sum_{n=1}^{\infty }f\left ( x \right )\sin\left ( 2nx \right )dx,How to prove this equality below $$\int_{a}^{b}f\left ( x \right )\cot\left ( x \right )\mathrm{d}x=2\sum_{n=1}^{\infty }\int_{a}^{b}f\left ( x \right )\sin\left ( 2nx \right )\mathrm{d}x$$ The series RHS seems to be divergence...I don't know how to do first.,How to prove this equality below $$\int_{a}^{b}f\left ( x \right )\cot\left ( x \right )\mathrm{d}x=2\sum_{n=1}^{\infty }\int_{a}^{b}f\left ( x \right )\sin\left ( 2nx \right )\mathrm{d}x$$ The series RHS seems to be divergence...I don't know how to do first.,,"['calculus', 'integration', 'sequences-and-series', 'analysis']"
