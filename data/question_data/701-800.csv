,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,How to compute Riemann-Stieltjes / Lebesgue(-Stieltjes) integral?,How to compute Riemann-Stieltjes / Lebesgue(-Stieltjes) integral?,,"The definitions do not seem easy to me for computation. For example,  Lebesgue(-Stieltjes) integral is a measure theory concept, involving construction for from step function, simple functions, nonnegative function till general function. I was wondering, in practice, what common ways for computing Lebesgue(-Stieltjes) integral are? Is it most desirable, when possible, to convert Lebesgue(-Stieltjes) integral to Riemann(-Stieltjes) integral, and Riemann-Stieltjes) integral to Riemann integral, and then apply the methods learned from calculus to compute the equivalent Riemann integral? What about the cases when the equivalence/conversion is not possible? Is definition the only way to compute Riemann-Stieltjes or Lebesgue(-Stieltjes) integrals? My questions come from a previous reply by Gortaur Usually only Lebesgue   (Lebesgue-Stieltjes) integrals are   used in the probability theory. On the   other hand to calculate them you can   use an equivalence of   Lebesgue-Stieltjes and   Riemann-Stieltjes integrals (provided   necessary conditions). Thanks and regards!","The definitions do not seem easy to me for computation. For example,  Lebesgue(-Stieltjes) integral is a measure theory concept, involving construction for from step function, simple functions, nonnegative function till general function. I was wondering, in practice, what common ways for computing Lebesgue(-Stieltjes) integral are? Is it most desirable, when possible, to convert Lebesgue(-Stieltjes) integral to Riemann(-Stieltjes) integral, and Riemann-Stieltjes) integral to Riemann integral, and then apply the methods learned from calculus to compute the equivalent Riemann integral? What about the cases when the equivalence/conversion is not possible? Is definition the only way to compute Riemann-Stieltjes or Lebesgue(-Stieltjes) integrals? My questions come from a previous reply by Gortaur Usually only Lebesgue   (Lebesgue-Stieltjes) integrals are   used in the probability theory. On the   other hand to calculate them you can   use an equivalence of   Lebesgue-Stieltjes and   Riemann-Stieltjes integrals (provided   necessary conditions). Thanks and regards!",,"['real-analysis', 'integration', 'measure-theory']"
1,Constructing Continuous functions at given points,Constructing Continuous functions at given points,,"Ok. This question may sound very easy, but actually I am in great need of an answer. I have been facing trouble in constructing functions, which are only continuous at some particular sets. For example, the standard example of a function which is only continuous at one point, is the function, $f(x) = x, \ x \in \mathbb{Q}$ and $f(x) = -x, x  \in \mathbb{R} \setminus \mathbb{Q}$ . Similarly, I would like to know how to construct a function which is: Continuous at exactly $2,3,4$ points. Continuous exactly at integers Continuous exactly at Natural numbers Continuous exactly at Rationals. I would like to see many examples (with proof!), so that I won't struggle when somebody asks me to construct such functions.","Ok. This question may sound very easy, but actually I am in great need of an answer. I have been facing trouble in constructing functions, which are only continuous at some particular sets. For example, the standard example of a function which is only continuous at one point, is the function, and . Similarly, I would like to know how to construct a function which is: Continuous at exactly points. Continuous exactly at integers Continuous exactly at Natural numbers Continuous exactly at Rationals. I would like to see many examples (with proof!), so that I won't struggle when somebody asks me to construct such functions.","f(x) = x, \ x \in \mathbb{Q} f(x) = -x, x  \in \mathbb{R} \setminus \mathbb{Q} 2,3,4",['real-analysis']
2,How to prove that derivatives have the Intermediate Value Property,How to prove that derivatives have the Intermediate Value Property,,"I'm reading a book which gives this theorem without proof: If a and b are any two points in an interval on which ƒ is differentiable, then ƒ'   takes on every value between ƒ'(a) and ƒ'(b). As far as I can say, the theorem means that the fact ƒ' is the derivative of another function ƒ on [a, b] implies that ƒ' is continuous on [a, b]. Is my understanding correct? Is there a name of this theorem that I can use to find a proof of it?","I'm reading a book which gives this theorem without proof: If a and b are any two points in an interval on which ƒ is differentiable, then ƒ'   takes on every value between ƒ'(a) and ƒ'(b). As far as I can say, the theorem means that the fact ƒ' is the derivative of another function ƒ on [a, b] implies that ƒ' is continuous on [a, b]. Is my understanding correct? Is there a name of this theorem that I can use to find a proof of it?",,['real-analysis']
3,Prove that a function whose derivative is bounded is uniformly continuous.,Prove that a function whose derivative is bounded is uniformly continuous.,,Suppose that $f$ is a real-valued function on $\Bbb R$ whose derivative exists at each point and is bounded. Prove that $f$ is uniformly continuous.,Suppose that $f$ is a real-valued function on $\Bbb R$ whose derivative exists at each point and is bounded. Prove that $f$ is uniformly continuous.,,"['real-analysis', 'uniform-continuity']"
4,Jensen's inequality for integrals,Jensen's inequality for integrals,,What nice ways do you know in order to prove Jensen's inequality for integrals? I'm looking for some various approaching ways. Supposing that $\varphi$ is a convex function on the real line and $g$ is an integrable real-valued function we have that: $$\varphi\left(\int_a^b f\right) \leqslant \int_a^b \varphi(f).$$,What nice ways do you know in order to prove Jensen's inequality for integrals? I'm looking for some various approaching ways. Supposing that $\varphi$ is a convex function on the real line and $g$ is an integrable real-valued function we have that: $$\varphi\left(\int_a^b f\right) \leqslant \int_a^b \varphi(f).$$,,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'integral-inequality']"
5,A challenging logarithmic integral $\int_0^1 \frac{\log(1+x)\log(1-x)}{1+x}dx$,A challenging logarithmic integral,\int_0^1 \frac{\log(1+x)\log(1-x)}{1+x}dx,"While playing around with Mathematica , I found that $$\int_0^1 \frac{\log(1+x)\log(1-x)}{1+x}dx = \frac{1}{3}\log^3(2)-\frac{\pi^2}{12}\log(2)+\frac{\zeta(3)}{8}$$ Please help me prove this result.","While playing around with Mathematica , I found that $$\int_0^1 \frac{\log(1+x)\log(1-x)}{1+x}dx = \frac{1}{3}\log^3(2)-\frac{\pi^2}{12}\log(2)+\frac{\zeta(3)}{8}$$ Please help me prove this result.",,"['real-analysis', 'calculus', 'integration', 'definite-integrals', 'harmonic-numbers']"
6,"Showing that $\gamma = -\int_0^{\infty} e^{-t} \log t \,dt$, where $\gamma$ is the Euler-Mascheroni constant.","Showing that , where  is the Euler-Mascheroni constant.","\gamma = -\int_0^{\infty} e^{-t} \log t \,dt \gamma","I'm trying to show that $$\lim_{n \to \infty} \left[\sum_{k=1}^{n} \frac{1}{k} - \log n\right] = -\int_0^{\infty} e^{-t} \log t \,dt.$$ In other words, I'm trying to show that the above definitions of the Euler-Mascheroni constant $\gamma$ are equivalent. In another post here (which I can't seem to find now) someone noted that $$\int_0^{\infty} e^{-t} \log t \,dt = \left.\frac{d}{dx} \int_0^{\infty} t^x e^{-t} \,dt \right|_{x=0} = \Gamma'(1) = \psi(1),$$ where $\psi$ is the digamma function.  This may be a good place to start on the right-hand side. For the left-hand side I was tempted to represent the terms with integrals.  It is not hard to show that $$\sum_{k=1}^{n} \frac{1}{k} = \int_0^1 \frac{1-x^n}{1-x} \,dx,$$ but I'm not sure this gets us anywhere. Any help would be greatly appreciated.","I'm trying to show that $$\lim_{n \to \infty} \left[\sum_{k=1}^{n} \frac{1}{k} - \log n\right] = -\int_0^{\infty} e^{-t} \log t \,dt.$$ In other words, I'm trying to show that the above definitions of the Euler-Mascheroni constant $\gamma$ are equivalent. In another post here (which I can't seem to find now) someone noted that $$\int_0^{\infty} e^{-t} \log t \,dt = \left.\frac{d}{dx} \int_0^{\infty} t^x e^{-t} \,dt \right|_{x=0} = \Gamma'(1) = \psi(1),$$ where $\psi$ is the digamma function.  This may be a good place to start on the right-hand side. For the left-hand side I was tempted to represent the terms with integrals.  It is not hard to show that $$\sum_{k=1}^{n} \frac{1}{k} = \int_0^1 \frac{1-x^n}{1-x} \,dx,$$ but I'm not sure this gets us anywhere. Any help would be greatly appreciated.",,"['calculus', 'real-analysis', 'analysis']"
7,Does the sum of reciprocals of primes converge?,Does the sum of reciprocals of primes converge?,,"Is this series known to converge, and if so, what does it converge to (if known)? Where $p_n$ is prime number $n$, and $p_1 = 2$, $$\sum\limits_{n=1}^\infty \frac{1}{p_n}$$","Is this series known to converge, and if so, what does it converge to (if known)? Where $p_n$ is prime number $n$, and $p_1 = 2$, $$\sum\limits_{n=1}^\infty \frac{1}{p_n}$$",,"['real-analysis', 'sequences-and-series', 'number-theory', 'convergence-divergence', 'prime-numbers']"
8,How to parametrize a curve by its arc length,How to parametrize a curve by its arc length,,"I am reading on Wikipedia that ''...Any regular curve may be parametrized by the arc length (the natural parametrization) and...'' I know that if $a(t) = (x(t),y(t),z(t))$ is a curve (say, smooth) then it is regular iff for all $t$: $a' (t) \neq 0$. I also know the definition of arc length: The arc length of a curve $a$ between $t_0$ and $t$ is defined as $$ l = \int_{t_0}^t |a'(t)|dt$$ But what is the parametrization of $a$ using its arc lenght?","I am reading on Wikipedia that ''...Any regular curve may be parametrized by the arc length (the natural parametrization) and...'' I know that if $a(t) = (x(t),y(t),z(t))$ is a curve (say, smooth) then it is regular iff for all $t$: $a' (t) \neq 0$. I also know the definition of arc length: The arc length of a curve $a$ between $t_0$ and $t$ is defined as $$ l = \int_{t_0}^t |a'(t)|dt$$ But what is the parametrization of $a$ using its arc lenght?",,"['real-analysis', 'differential-geometry']"
9,A continuously differentiable map is locally Lipschitz,A continuously differentiable map is locally Lipschitz,,"Let $f:\mathbb R^d \to \mathbb R^m$ be a map of class $C^1$ . That is, $f$ is continuous and its derivative exists and is also continuous. Why is $f$ locally Lipschitz? Remark Such $f$ will not be globally Lipschitz in general, as the one-dimensional example $f(x)=x^2$ shows: for this example, $|f(x+1)-f(x)| = |2x+1|$ is unbounded.","Let be a map of class . That is, is continuous and its derivative exists and is also continuous. Why is locally Lipschitz? Remark Such will not be globally Lipschitz in general, as the one-dimensional example shows: for this example, is unbounded.",f:\mathbb R^d \to \mathbb R^m C^1 f f f f(x)=x^2 |f(x+1)-f(x)| = |2x+1|,"['real-analysis', 'multivariable-calculus', 'derivatives', 'lipschitz-functions', 'holder-spaces']"
10,Intuition for uniform continuity of a function on $\mathbb{R}$,Intuition for uniform continuity of a function on,\mathbb{R},"I understand the formal definition of uniform continuity of a function, and how it is different from standard continuity. My question is: Is there an intuitive way to classify a function on $\mathbb{R}$ as being uniformly continuous, just like there is for regular continuity? I mean, that for a ""nice"" function $f:\mathbb{R} \to \mathbb{R}$, it is usually easy to tell if it is continuous on an interval by looking at or thinking of the graph of the function on the interval, or on all of $\mathbb{R}$. Can I do the same for uniform continuity? Can I tell that a function is uniformly continuous just by what it looks like? Ideally, this intuition would fit with the Heine-Cantor theorem for compact sets on $\mathbb{R}$.","I understand the formal definition of uniform continuity of a function, and how it is different from standard continuity. My question is: Is there an intuitive way to classify a function on $\mathbb{R}$ as being uniformly continuous, just like there is for regular continuity? I mean, that for a ""nice"" function $f:\mathbb{R} \to \mathbb{R}$, it is usually easy to tell if it is continuous on an interval by looking at or thinking of the graph of the function on the interval, or on all of $\mathbb{R}$. Can I do the same for uniform continuity? Can I tell that a function is uniformly continuous just by what it looks like? Ideally, this intuition would fit with the Heine-Cantor theorem for compact sets on $\mathbb{R}$.",,"['real-analysis', 'intuition']"
11,How to understand intuitively the Stolz-Cesaro Theorem for sequences?,How to understand intuitively the Stolz-Cesaro Theorem for sequences?,,"I have to give a presentation on the theorem in Real Analysis with a fellow student. While I've looked over the proof and verified that, yes, step B does indeed follow logically from step A, etc. and have internalized the proof to the extent that I can replicate it myself on paper, I still feel that I have made little progress as to why this theorem works the way it does, i.e. what is the intuition behind the theorem, such that it should make sense that it follows the way it does. Therefore I ask: what kind of intuition is there about this theorem? It would also be helpful to understand how the theorem can be used effectively in analysis. I have seen it referred to as a sort of L'Hopital's rule for sequences, which certainly seems to make sense, but I similarly have little intuitive understanding of how that rule works, either. I am asking this question not only for my personal understanding, but also for the sake of being able to present it in an illuminating manner, such that the rest of the class can also come away with the same sort of intuition of how the theorem works and how it is useful. Any help would be greatly appreciated. EDIT: I should point out that the formulation of the theorem we are being tasked to prove is the following: Let $\lbrace a_n \rbrace$ , $\lbrace b_n \rbrace$ be sequences, ${b_n}$ strictly increasing and unbounded. Then if $\lim\limits_{n \to \infty} \frac {a_{n+1} - a_n}{b_{n+1} - b_n} = l$ for some $l \in \mathbb R$ , then $\lim\limits_{n \to \infty} \frac{a_n}{b_n} = l$ also.","I have to give a presentation on the theorem in Real Analysis with a fellow student. While I've looked over the proof and verified that, yes, step B does indeed follow logically from step A, etc. and have internalized the proof to the extent that I can replicate it myself on paper, I still feel that I have made little progress as to why this theorem works the way it does, i.e. what is the intuition behind the theorem, such that it should make sense that it follows the way it does. Therefore I ask: what kind of intuition is there about this theorem? It would also be helpful to understand how the theorem can be used effectively in analysis. I have seen it referred to as a sort of L'Hopital's rule for sequences, which certainly seems to make sense, but I similarly have little intuitive understanding of how that rule works, either. I am asking this question not only for my personal understanding, but also for the sake of being able to present it in an illuminating manner, such that the rest of the class can also come away with the same sort of intuition of how the theorem works and how it is useful. Any help would be greatly appreciated. EDIT: I should point out that the formulation of the theorem we are being tasked to prove is the following: Let , be sequences, strictly increasing and unbounded. Then if for some , then also.",\lbrace a_n \rbrace \lbrace b_n \rbrace {b_n} \lim\limits_{n \to \infty} \frac {a_{n+1} - a_n}{b_{n+1} - b_n} = l l \in \mathbb R \lim\limits_{n \to \infty} \frac{a_n}{b_n} = l,"['real-analysis', 'sequences-and-series', 'intuition']"
12,$\lim_{n\to\infty} e^{-n} \sum_{k=0}^{n} \frac{n^k}{k!} = \frac{1}{2}$ - basic methods,- basic methods,\lim_{n\to\infty} e^{-n} \sum_{k=0}^{n} \frac{n^k}{k!} = \frac{1}{2},"Prove that $$\lim\limits_{n\to\infty} e^{-n} \sum_{k=0}^{n} \frac{n^k}{k!} = \frac{1}{2}$$ This problem appeared on MSE many times, but each time it was solved using Poisson distribution or lots of integrals. I am wondering, is there any way to prove it using some basic properties of limits (their arithmetics, squeeze theorem etc.), definition of $e^x$ as $\lim\limits_{n\to\infty}(1+\frac{x}{n})^n$ , basic limits with $e$ , binomial expansion and logarithms, but without using integrals, series, Stirling formula, asymptotics, Taylor series? This problem was given to me by my mathematical analysis teacher, but it's not a homework, just additional problem to think on. My teacher claims it can be solved with knowledge introduced on lectures so far, which is not much, mainly things mentioned above. Of course, I can use theorems not mentioned on the lectures, but then I have to prove them, and again, with the baisc knowledge. I've been thinking about it for a few days and couldn't do any major progress in my attempts.","Prove that This problem appeared on MSE many times, but each time it was solved using Poisson distribution or lots of integrals. I am wondering, is there any way to prove it using some basic properties of limits (their arithmetics, squeeze theorem etc.), definition of as , basic limits with , binomial expansion and logarithms, but without using integrals, series, Stirling formula, asymptotics, Taylor series? This problem was given to me by my mathematical analysis teacher, but it's not a homework, just additional problem to think on. My teacher claims it can be solved with knowledge introduced on lectures so far, which is not much, mainly things mentioned above. Of course, I can use theorems not mentioned on the lectures, but then I have to prove them, and again, with the baisc knowledge. I've been thinking about it for a few days and couldn't do any major progress in my attempts.",\lim\limits_{n\to\infty} e^{-n} \sum_{k=0}^{n} \frac{n^k}{k!} = \frac{1}{2} e^x \lim\limits_{n\to\infty}(1+\frac{x}{n})^n e,"['real-analysis', 'calculus', 'limits']"
13,Compute $\sum_{k=0}^{\infty}\frac{1}{2^{k!}}$,Compute,\sum_{k=0}^{\infty}\frac{1}{2^{k!}},"How could the series below be computed ? $$\sum_{k=0}^{\infty}\frac{1}{2^{k!}}$$ It's not a series from a book, but a series I thought of many times, and I didn't manage to figure out what I should do here. I'm just curious to know if there are some known ways for approaching such a series. Thanks!","How could the series below be computed ? $$\sum_{k=0}^{\infty}\frac{1}{2^{k!}}$$ It's not a series from a book, but a series I thought of many times, and I didn't manage to figure out what I should do here. I'm just curious to know if there are some known ways for approaching such a series. Thanks!",,"['calculus', 'real-analysis', 'sequences-and-series']"
14,What does it mean for a polynomial to be the 'best' approximation of a function around a point?,What does it mean for a polynomial to be the 'best' approximation of a function around a point?,,"I think I understand how to use Taylor polynomials to approximate sine. For instance, if $$ \sin x \approx ax^2+bx+c $$ and we want the approximation to be particularly accurate when $x$ is close to $0$ , then we could adopt the following approach. When $x=0$ , $\sin x = 0$ , and so $ax^2+bx+c=0$ , meaning that $c=0$ . Therefore we get $$ \sin x \approx ax^2+bx $$ If we want the first derivatives to match, then $\frac{d}{dx}(ax^2+bx)$ should equal $1$ . Therefore, $b=1$ : $$ \sin x \approx ax^2+x $$ Finally, if we want the second derivatives to match, then $\frac{d^2}{dx^2}(ax^2+x)$ should equal $0$ , and so $a=0$ . The small angle approximation for sine is $$ \sin x \approx x $$ All of this makes sense to me. What I don't understand is when people try to put this on rigorous footing. I have often heard people say 'this shows that $x$ is the best quadratic approximation of $\sin x$ when $x$ is near to $0$ '. But what is meant by 'best', and 'near'? If the approximation suddenly became terrible when $x=0.5$ , then would this be considered close enough to $0$ for there to be a problem? It seems that there are formal definitions for these terms, but I don't know what they are.","I think I understand how to use Taylor polynomials to approximate sine. For instance, if and we want the approximation to be particularly accurate when is close to , then we could adopt the following approach. When , , and so , meaning that . Therefore we get If we want the first derivatives to match, then should equal . Therefore, : Finally, if we want the second derivatives to match, then should equal , and so . The small angle approximation for sine is All of this makes sense to me. What I don't understand is when people try to put this on rigorous footing. I have often heard people say 'this shows that is the best quadratic approximation of when is near to '. But what is meant by 'best', and 'near'? If the approximation suddenly became terrible when , then would this be considered close enough to for there to be a problem? It seems that there are formal definitions for these terms, but I don't know what they are.","
\sin x \approx ax^2+bx+c
 x 0 x=0 \sin x = 0 ax^2+bx+c=0 c=0 
\sin x \approx ax^2+bx
 \frac{d}{dx}(ax^2+bx) 1 b=1 
\sin x \approx ax^2+x
 \frac{d^2}{dx^2}(ax^2+x) 0 a=0 
\sin x \approx x
 x \sin x x 0 x=0.5 0","['real-analysis', 'calculus', 'taylor-expansion', 'approximation']"
15,Convergence/divergence of $\sum\frac{a_n}{1+na_n}$ when $a_n\geq0$ and $\sum a_n$ diverges,Convergence/divergence of  when  and  diverges,\sum\frac{a_n}{1+na_n} a_n\geq0 \sum a_n,"A question from Rudin (Principles) Chapter 3: Let $a_n\geq0$ and $\sum a_n$ diverges. What can be said about convergence/divergence of $\sum\frac{a_n}{1+na_n}$? This one is being recalcitrant. Given that $x>y$ implies $\frac{x}{1+nx}>\frac{y}{1+ny}$ and when $a_n=1/n\log n$ the sum in question diverges, it seems plausible that in general the sum will always diverge, but I can't get a proof out. If it does diverge, it does so pretty slowly as $$\frac{a_n}{1+na_n}=\frac{1}{n}-\frac{1}{n+n^2a_n}\leq\frac{1}{n}.$$","A question from Rudin (Principles) Chapter 3: Let $a_n\geq0$ and $\sum a_n$ diverges. What can be said about convergence/divergence of $\sum\frac{a_n}{1+na_n}$? This one is being recalcitrant. Given that $x>y$ implies $\frac{x}{1+nx}>\frac{y}{1+ny}$ and when $a_n=1/n\log n$ the sum in question diverges, it seems plausible that in general the sum will always diverge, but I can't get a proof out. If it does diverge, it does so pretty slowly as $$\frac{a_n}{1+na_n}=\frac{1}{n}-\frac{1}{n+n^2a_n}\leq\frac{1}{n}.$$",,"['real-analysis', 'sequences-and-series']"
16,When can we exchange order of two limits?,When can we exchange order of two limits?,,"My questions are about a sequence or function with several variables. I vaguely remember some while ago one of my teachers said taking limits of a sequence or function with respect to different variables is not exchangeable everywhere, i.e. $$ \lim_n \lim_m a_{n,m} \neq \lim_m \lim_n a_{n,m}, \quad \lim_x \lim_y f(x,y) \neq \lim_y \lim_x f(x,y).$$ So my question is what are the cases or examples when one can exchange the order of taking limits and when one cannot, to your knowledge? I would like to collect the cases together, and be aware of their difference and avoid making mistakes. If you could provide some general guidelines, that will be even nicer! To give you an example of what I am asking about, this is a question that confuses me: Assume $f: [0, \infty)    \rightarrow (0, \infty)$ is a function, satisfying $$    \int_0^{\infty} x f(x) \, dx < \infty.    $$ Determine the convergence of this series $\sum_{n=1}^{\infty}    \int_n^{\infty} f(x) dx$. The answer I saw is to exchange the order of $\sum_{n=1}^{\infty}$ and $\int_n^{\infty}$ as follows: $$    \sum_{n=1}^{\infty} \int_n^{\infty}    f(x) dx    = \int_1^{\infty} \sum_{n=1}^{\lfloor x \rfloor}  f(n) dx \leq    \int_1^{\infty} \lfloor x \rfloor     f(x) dx $$ where $\lfloor x \rfloor$ is the greatest integer less than $x$. In this way, the answer proves the series converges. I was wondering why the two steps are valid? Is there some special meaning of the first equality? Because it looks similar to the tail sum formula for expectation of a random variable $X$ with possible values $\{ 0,1,2,...,n\}$: $$\sum_{i=0}^n i P(X=i) = \sum_{i=0}^n P(X\geq i).$$ The formula is from Page 171 of Probability by Jim Pitman, 1993 . Are they really related? Really appreciate your help!","My questions are about a sequence or function with several variables. I vaguely remember some while ago one of my teachers said taking limits of a sequence or function with respect to different variables is not exchangeable everywhere, i.e. $$ \lim_n \lim_m a_{n,m} \neq \lim_m \lim_n a_{n,m}, \quad \lim_x \lim_y f(x,y) \neq \lim_y \lim_x f(x,y).$$ So my question is what are the cases or examples when one can exchange the order of taking limits and when one cannot, to your knowledge? I would like to collect the cases together, and be aware of their difference and avoid making mistakes. If you could provide some general guidelines, that will be even nicer! To give you an example of what I am asking about, this is a question that confuses me: Assume $f: [0, \infty)    \rightarrow (0, \infty)$ is a function, satisfying $$    \int_0^{\infty} x f(x) \, dx < \infty.    $$ Determine the convergence of this series $\sum_{n=1}^{\infty}    \int_n^{\infty} f(x) dx$. The answer I saw is to exchange the order of $\sum_{n=1}^{\infty}$ and $\int_n^{\infty}$ as follows: $$    \sum_{n=1}^{\infty} \int_n^{\infty}    f(x) dx    = \int_1^{\infty} \sum_{n=1}^{\lfloor x \rfloor}  f(n) dx \leq    \int_1^{\infty} \lfloor x \rfloor     f(x) dx $$ where $\lfloor x \rfloor$ is the greatest integer less than $x$. In this way, the answer proves the series converges. I was wondering why the two steps are valid? Is there some special meaning of the first equality? Because it looks similar to the tail sum formula for expectation of a random variable $X$ with possible values $\{ 0,1,2,...,n\}$: $$\sum_{i=0}^n i P(X=i) = \sum_{i=0}^n P(X\geq i).$$ The formula is from Page 171 of Probability by Jim Pitman, 1993 . Are they really related? Really appreciate your help!",,"['real-analysis', 'integration', 'limits']"
17,Uniqueness of product measure (non $\sigma$-finite case),Uniqueness of product measure (non -finite case),\sigma,"Let $(X,\mathscr{A},\mu), (Y,\mathscr{B},\nu)$ be two measure spaces, then we have the product measurable space $(X\times Y, \mathscr{A}\times\mathscr{B})$ where $\mathscr{A}\times\mathscr{B}$ is the $\sigma$-algebra generated by $\{A\times B, A\in \mathscr{A}, B\in \mathscr{B}\}$. Applying Caratheodory extension we can always construct a measure $m$ on $\mathscr{A}\times\mathscr{B}$ so that $m(A\times B)=\mu(A)\nu(B)$. Further, if $\mu$ and $\nu$ are $\sigma$-finite, by monotone class theorem such a measure $m$ is uniquely determined. Now my question is: When $\mu$ and $\nu$ are not $\sigma$-finite, is product measure still unique?","Let $(X,\mathscr{A},\mu), (Y,\mathscr{B},\nu)$ be two measure spaces, then we have the product measurable space $(X\times Y, \mathscr{A}\times\mathscr{B})$ where $\mathscr{A}\times\mathscr{B}$ is the $\sigma$-algebra generated by $\{A\times B, A\in \mathscr{A}, B\in \mathscr{B}\}$. Applying Caratheodory extension we can always construct a measure $m$ on $\mathscr{A}\times\mathscr{B}$ so that $m(A\times B)=\mu(A)\nu(B)$. Further, if $\mu$ and $\nu$ are $\sigma$-finite, by monotone class theorem such a measure $m$ is uniquely determined. Now my question is: When $\mu$ and $\nu$ are not $\sigma$-finite, is product measure still unique?",,"['real-analysis', 'measure-theory', 'probability-theory', 'examples-counterexamples']"
18,Compute $\lim\limits_{n\to\infty} \prod\limits_2^n \left(1-\frac1{k^3}\right)$,Compute,\lim\limits_{n\to\infty} \prod\limits_2^n \left(1-\frac1{k^3}\right),"I've just worked out the limit $\lim\limits_{n\to\infty} \prod\limits_{2}^{n} \left(1-\frac{1}{k^2}\right)$ that is simply solved, and the result is $\frac{1}{2}$. After that, I thought of calculating $\lim\limits_{n\to\infty} \prod\limits_{2}^{n} \left(1-\frac{1}{k^3}\right)$, but I don't know how to do it. According to W|A, the result is pretty nice, but I don't see how W|A gets that. (See here .) Is there any easy way to get the answer?","I've just worked out the limit $\lim\limits_{n\to\infty} \prod\limits_{2}^{n} \left(1-\frac{1}{k^2}\right)$ that is simply solved, and the result is $\frac{1}{2}$. After that, I thought of calculating $\lim\limits_{n\to\infty} \prod\limits_{2}^{n} \left(1-\frac{1}{k^3}\right)$, but I don't know how to do it. According to W|A, the result is pretty nice, but I don't see how W|A gets that. (See here .) Is there any easy way to get the answer?",,"['calculus', 'real-analysis', 'limits', 'products']"
19,How to find $\int_0^\infty \prod_{k=1}^n \frac{\sin \frac{x}{2k-1}}{\frac{x}{2k-1}}\mathrm dx$,How to find,\int_0^\infty \prod_{k=1}^n \frac{\sin \frac{x}{2k-1}}{\frac{x}{2k-1}}\mathrm dx,"I am trying to calculate the integral $$ I_n=\int \limits_0^\infty \prod_{k=1}^n \frac{\sin \frac{x}{2k-1}}{\frac{x}{2k-1}}\mathrm dx. $$ (I have literature on this, if people want). Note, we can write the amazing sequence $\{I_1,I_2,I_3,I_4,I_5,I_6,I_7\}$ as  $$ \bigg\{\frac{\pi}{2},\frac{\pi}{2},\frac{\pi}{2},\frac{\pi}{2},\frac{\pi}{2},\frac{\pi}{2},\frac{\pi}{2}\bigg\}. $$ BUT $I_8\neq \pi/2$, how can we derive this same result for $n=1,2,\ldots,7$?  And why does it deviate at $I_8$?  Thanks, in integral form this sequence is represented by $$ \frac{\pi}{2}=I_1=\int\limits_0^\infty \frac{\sin x}{x}\mathrm dx=I_2=\int \limits_0^\infty \frac{\sin x}{x}\frac{\sin \frac{x}{3}}{\frac{x}{3}}\mathrm dx=I_3=\int\limits_0^\infty \frac{\sin x}{x}\frac{\sin \frac{x}{3}}{\frac{x}{3}}\frac{\sin \frac{x}{5}}{\frac{x}{5}}\mathrm dx=\cdots $$ HOWEVER, this fails for $I_8$.  The strange result for $I_8$ is given by  $$ I_8= \frac{467807924713440738696537864469}{ 935615849440640907310521750000}\pi\approx \frac{\pi}{2}-2.31\cdot 10^{-11} $$ Note we can calculate $I_1$ by using integration wrt parameter and first considering the damped Sine- integral \begin{equation} \eta(\lambda)=\int_{0}^\infty e^{-\lambda x}\frac{\sin x}{x}\mathrm dx. \end{equation} We now wish to calculate the Dirichlet integral $I_1$ using calculus and  $\eta(\lambda)$, \begin{equation} I_1=\int_{0}^\infty \frac{\sin x}{x}\mathrm dx. \end{equation} by differentiating $\eta(\lambda)$. We start by differentiating this to obtain  $$ \eta'(\lambda)=\frac{d}{d\lambda} \int_{0}^{\infty} e^{-\lambda x}\frac{\sin x}{x}\mathrm dx =\int_{0}^\infty \frac{\partial}{\partial \lambda} e^{-\lambda x}\frac{\sin x}{x}\mathrm dx=-\int_{0}^\infty e^{-\lambda x}{\sin x}\ \mathrm dx. $$ Note, that passing the differentiation outside of the integral inside the integral is allowed since the integral is a continuous function of x and $\lambda$ for x$\in(-\infty,\infty)$ and $\lambda \in (0,\infty)$. We can easily integrate this by writing the sine function as the imaginary part of an exponential, that is $$ -\int_{0}^\infty e^{-\lambda x}{\sin x}\ \mathrm dx=-\Im\bigg[-\int_{0}^\infty e^{-\lambda x} e^{ix}\mathrm dx\bigg]=-\Im \bigg[-\int_{0}^\infty e^{-x(\lambda-i)}\mathrm dx\bigg]=-\Im{\frac{1}{\lambda-i}}=-\frac{1}{\lambda^2+1}, $$where I integrated the exponential using analysis rules and next used $$ -\Im\bigg [\frac{1}{\lambda-i}\bigg]=-\Im \bigg[\frac{1}{\lambda-i}\cdot \frac{\lambda+i}{\lambda+i}\bigg]=-\frac{1}{\lambda^2+1}. $$ Thus we can see that \begin{equation} \eta'(\lambda)= -\frac{1}{\lambda^2+1}. \end{equation} Now we need to use integrate this relation carefully.  We do this by writing $$ \int_{\lambda}^{\infty}\frac{\mathrm d\eta}{\mathrm d\xi}\mathrm d\xi=\eta(\infty)-\eta(\lambda)=-\eta(\lambda) $$ since $\eta(\infty)=0$.  We can now use this and the result above to give $$ -\eta(\lambda)=\int_{\lambda}^{\infty} \eta'(\xi)\mathrm d\xi=\int_{\lambda}^{\infty} -\frac{1}{\xi ^2 +1}\mathrm d\xi=-(\arctan{\infty}-\arctan{\lambda})=-\frac{\pi}{2}+\arctan{\lambda}, $$ thus we can easily see $$ \eta(\lambda)= \frac{\pi}{2}-\arctan{\lambda}. $$ We set $\lambda =0$ and obtain the desired result \begin{equation} \eta(\lambda=0)=I_1= \frac{\pi}{2}=\int_{0}^{\infty} \frac{\sin x}{x}\mathrm dx. \end{equation} But how to generalize this for $I_n$?  Thanks a lot..","I am trying to calculate the integral $$ I_n=\int \limits_0^\infty \prod_{k=1}^n \frac{\sin \frac{x}{2k-1}}{\frac{x}{2k-1}}\mathrm dx. $$ (I have literature on this, if people want). Note, we can write the amazing sequence $\{I_1,I_2,I_3,I_4,I_5,I_6,I_7\}$ as  $$ \bigg\{\frac{\pi}{2},\frac{\pi}{2},\frac{\pi}{2},\frac{\pi}{2},\frac{\pi}{2},\frac{\pi}{2},\frac{\pi}{2}\bigg\}. $$ BUT $I_8\neq \pi/2$, how can we derive this same result for $n=1,2,\ldots,7$?  And why does it deviate at $I_8$?  Thanks, in integral form this sequence is represented by $$ \frac{\pi}{2}=I_1=\int\limits_0^\infty \frac{\sin x}{x}\mathrm dx=I_2=\int \limits_0^\infty \frac{\sin x}{x}\frac{\sin \frac{x}{3}}{\frac{x}{3}}\mathrm dx=I_3=\int\limits_0^\infty \frac{\sin x}{x}\frac{\sin \frac{x}{3}}{\frac{x}{3}}\frac{\sin \frac{x}{5}}{\frac{x}{5}}\mathrm dx=\cdots $$ HOWEVER, this fails for $I_8$.  The strange result for $I_8$ is given by  $$ I_8= \frac{467807924713440738696537864469}{ 935615849440640907310521750000}\pi\approx \frac{\pi}{2}-2.31\cdot 10^{-11} $$ Note we can calculate $I_1$ by using integration wrt parameter and first considering the damped Sine- integral \begin{equation} \eta(\lambda)=\int_{0}^\infty e^{-\lambda x}\frac{\sin x}{x}\mathrm dx. \end{equation} We now wish to calculate the Dirichlet integral $I_1$ using calculus and  $\eta(\lambda)$, \begin{equation} I_1=\int_{0}^\infty \frac{\sin x}{x}\mathrm dx. \end{equation} by differentiating $\eta(\lambda)$. We start by differentiating this to obtain  $$ \eta'(\lambda)=\frac{d}{d\lambda} \int_{0}^{\infty} e^{-\lambda x}\frac{\sin x}{x}\mathrm dx =\int_{0}^\infty \frac{\partial}{\partial \lambda} e^{-\lambda x}\frac{\sin x}{x}\mathrm dx=-\int_{0}^\infty e^{-\lambda x}{\sin x}\ \mathrm dx. $$ Note, that passing the differentiation outside of the integral inside the integral is allowed since the integral is a continuous function of x and $\lambda$ for x$\in(-\infty,\infty)$ and $\lambda \in (0,\infty)$. We can easily integrate this by writing the sine function as the imaginary part of an exponential, that is $$ -\int_{0}^\infty e^{-\lambda x}{\sin x}\ \mathrm dx=-\Im\bigg[-\int_{0}^\infty e^{-\lambda x} e^{ix}\mathrm dx\bigg]=-\Im \bigg[-\int_{0}^\infty e^{-x(\lambda-i)}\mathrm dx\bigg]=-\Im{\frac{1}{\lambda-i}}=-\frac{1}{\lambda^2+1}, $$where I integrated the exponential using analysis rules and next used $$ -\Im\bigg [\frac{1}{\lambda-i}\bigg]=-\Im \bigg[\frac{1}{\lambda-i}\cdot \frac{\lambda+i}{\lambda+i}\bigg]=-\frac{1}{\lambda^2+1}. $$ Thus we can see that \begin{equation} \eta'(\lambda)= -\frac{1}{\lambda^2+1}. \end{equation} Now we need to use integrate this relation carefully.  We do this by writing $$ \int_{\lambda}^{\infty}\frac{\mathrm d\eta}{\mathrm d\xi}\mathrm d\xi=\eta(\infty)-\eta(\lambda)=-\eta(\lambda) $$ since $\eta(\infty)=0$.  We can now use this and the result above to give $$ -\eta(\lambda)=\int_{\lambda}^{\infty} \eta'(\xi)\mathrm d\xi=\int_{\lambda}^{\infty} -\frac{1}{\xi ^2 +1}\mathrm d\xi=-(\arctan{\infty}-\arctan{\lambda})=-\frac{\pi}{2}+\arctan{\lambda}, $$ thus we can easily see $$ \eta(\lambda)= \frac{\pi}{2}-\arctan{\lambda}. $$ We set $\lambda =0$ and obtain the desired result \begin{equation} \eta(\lambda=0)=I_1= \frac{\pi}{2}=\int_{0}^{\infty} \frac{\sin x}{x}\mathrm dx. \end{equation} But how to generalize this for $I_n$?  Thanks a lot..",,"['real-analysis', 'integration', 'complex-analysis', 'number-theory', 'definite-integrals']"
20,"Were ""real numbers"" used before things like Dedekind cuts, Cauchy sequences, etc. appeared?","Were ""real numbers"" used before things like Dedekind cuts, Cauchy sequences, etc. appeared?",,"Just the question in the title, I'm trying to understand how something like analysis could be developed without formal constructions of the real numbers. I'm also very interested, if the answer is ""yes"", to know what was that people thought was a ""real number"" at that time.","Just the question in the title, I'm trying to understand how something like analysis could be developed without formal constructions of the real numbers. I'm also very interested, if the answer is ""yes"", to know what was that people thought was a ""real number"" at that time.",,"['real-analysis', 'analysis', 'math-history', 'real-numbers']"
21,"Why is analysis called ""analysis""?","Why is analysis called ""analysis""?",,"Just as the topic says, how did the name ""analysis"" come to denote the specific mathematical branch dealing with limits and stuff? The term ""analysis"" seems very generic compared to the words for the other two main branches, ""algebra"" and ""geometry"", which do not seem to have other unrelated meanings.","Just as the topic says, how did the name ""analysis"" come to denote the specific mathematical branch dealing with limits and stuff? The term ""analysis"" seems very generic compared to the words for the other two main branches, ""algebra"" and ""geometry"", which do not seem to have other unrelated meanings.",,"['real-analysis', 'soft-question', 'terminology', 'math-history']"
22,Proof that a function with a countable set of discontinuities is Riemann integrable without the notion of measure,Proof that a function with a countable set of discontinuities is Riemann integrable without the notion of measure,,"Let $f:[a,b]\to \mathbb{R}$ be a bounded function and $A$ be the set of its discontinuities. I am asking for a (direct) proof that if $A$ is countable then $f$ is Riemann integrable in $[a,b]$ that doesn't explicitely, or implicitly, require the notion of sets of measure $0$ ( and of course without the use of the Lebesgue Criterion). One could take a typical proof of the Lebesgue Criterion, make the neccessary adjustments and give me the proof of what I am asking. I don't want that however, but rather a simpler and more direct proof that heavily relies on the fact that $A$ is countable. A proof that can't be trivially altered so that it holds even if $\lambda(A)=0$ EDIT: Here is the proof of WimC with all the details: Let $\epsilon>0$ and $$D=\left\{d_1,d_2,...\right\}\subseteq A$$ be the countable set of discontinuities of $f$. Define: $$I=\left\{x\in [a,b]:\exists \delta>0: \omega f((x-\delta,x+\delta)\cap [a,b])<\epsilon\right\}$$ Now  $$x\in I\iff \exists \delta>0: \omega f((x-\delta,x+\delta)\cap [a,b])<\epsilon\iff [\left|y-x\right|<\delta\implies \omega f(y)<\epsilon]$$ and because $\epsilon$ is in fact arbitrary, $$x\in I\iff \text{ $f$ is continuous at $x$}$$ In addition, if $x\in I$, $\exists \delta>0$ so that $$\omega f((x-\delta,x+\delta)\cap [a,b])<\epsilon$$ If $y\in B(x,\delta)\cap [a,b]$. Then $y\in I$ and so $I$ is open relative to $[a,b]$. Because $I=[a,b]\setminus D$, $[a,b]=I\cup D$. For $k\in \mathbb{N}$ define $$D_k=\left(d_k-\frac{\epsilon}{M2^{k+1}},d_k+\frac{\epsilon}{M2^{k+1}}\right)\cap [a,b]$$ Obviously $D\subset \bigcup_{k=1}^{\infty}D_k$ and $[a,b]=I\cup  \bigcup_{k=1}^{\infty}D_k$ (since $D_k\subseteq [a,b]$). The compactness of $[a,b]$ implies $[a,b]=I\cup  \bigcup_{k=1}^{N}D_k$.  Now $[a,b]\setminus  \bigcup_{k=1}^{N}D_k$ is compact (closed and bounded) and included in $I$.  As such it can be covered by  $$F_x=(x-\delta_x,x+\delta_x)$$ where $ \delta_x>0:$ is chosen so that $\omega f((x-\delta_x,x+\delta_x)\cap [a,b])<\epsilon$. Compactness implies the existence of a finite subcover, $$[a,b]\setminus  \bigcup_{k=1}^{N}D_k\subseteq  \bigcup_{i=1}^{M}(x_i-\delta_i,x+\delta_i)$$ As we can replace the intervals that intersect we can suppose  $$ \bigcap_{i=1}^{M}[x_i-\delta_i,x+\delta_i]=\emptyset$$ Therefore, $$[a,b]=  \bigcup_{k=1}^{N}\overline{D}_k\cup  \bigcup_{i=1}^{M}[x_i-\delta_i,x+\delta_i]=\bigcup_{i=0}^{n}[t_{i-1},t_i]$$ where for $i\le n$, $[t_{i-1},t_i]=  [x_k-\delta_k,x+\delta_k]$ or $[t_{i-1},t_i]=  \overline{D}_k$ because $$\bigcup_{k=1}^{N}\overline{D}_k\cap  \bigcup_{i=1}^{M}[x_i-\delta_i,x+\delta_i]=\emptyset$$ Considering all the endpoints of the above (that are pairwise different) we can create a partition $\mathcal{P}=\left\{a=t_0<...<t_n=b\right\}$ of $[a,b]$. We separate the indices: $A=\left\{i:[t_{i-1},t_i]=  [x_k-\delta_k,x+\delta_k]\right\}$ and $B=\left\{i:[t_{i-1},t_i]=  \overline{D}_k \right\}$. Therefore, \begin{gather}U_{f,\mathcal{P}}-L_{f,\mathcal{P}}=\sum_{i=1}^n\omega f([t_{i-1},t_i])(t_i-t_{i-1})=\sum_{i\in A}\omega f([t_{i-1},t_i])\ell([t_{i-1},t_i])+\sum_{i\in B}\omega f([t_{i-1},t_i])\ell([t_{i-1},t_i])\\ \le \sum_{i\in A}2\left\|f\right\|\ell([t_{i-1},t_i])+\sum_{i\in B}\epsilon\ell(\overline{D}_k)\le 2\left\|f\right\|\frac{\epsilon}{M}+\epsilon(b-a)=2\epsilon+\epsilon(b-a)\end{gather} My questions are: Is this proof correct? ( I doubt the point: ""where for $i\le n$, $[t_{i-1},t_i]=  [x_k-\delta_k,x+\delta_k]$ or $[t_{i-1},t_i]=  \overline{D}_k$ because $$\bigcup_{k=1}^{N}\overline{D}_k\cap  \bigcup_{i=1}^{M}[x_i-\delta_i,x+\delta_i]=\emptyset$$"") Second, can it be simplified?","Let $f:[a,b]\to \mathbb{R}$ be a bounded function and $A$ be the set of its discontinuities. I am asking for a (direct) proof that if $A$ is countable then $f$ is Riemann integrable in $[a,b]$ that doesn't explicitely, or implicitly, require the notion of sets of measure $0$ ( and of course without the use of the Lebesgue Criterion). One could take a typical proof of the Lebesgue Criterion, make the neccessary adjustments and give me the proof of what I am asking. I don't want that however, but rather a simpler and more direct proof that heavily relies on the fact that $A$ is countable. A proof that can't be trivially altered so that it holds even if $\lambda(A)=0$ EDIT: Here is the proof of WimC with all the details: Let $\epsilon>0$ and $$D=\left\{d_1,d_2,...\right\}\subseteq A$$ be the countable set of discontinuities of $f$. Define: $$I=\left\{x\in [a,b]:\exists \delta>0: \omega f((x-\delta,x+\delta)\cap [a,b])<\epsilon\right\}$$ Now  $$x\in I\iff \exists \delta>0: \omega f((x-\delta,x+\delta)\cap [a,b])<\epsilon\iff [\left|y-x\right|<\delta\implies \omega f(y)<\epsilon]$$ and because $\epsilon$ is in fact arbitrary, $$x\in I\iff \text{ $f$ is continuous at $x$}$$ In addition, if $x\in I$, $\exists \delta>0$ so that $$\omega f((x-\delta,x+\delta)\cap [a,b])<\epsilon$$ If $y\in B(x,\delta)\cap [a,b]$. Then $y\in I$ and so $I$ is open relative to $[a,b]$. Because $I=[a,b]\setminus D$, $[a,b]=I\cup D$. For $k\in \mathbb{N}$ define $$D_k=\left(d_k-\frac{\epsilon}{M2^{k+1}},d_k+\frac{\epsilon}{M2^{k+1}}\right)\cap [a,b]$$ Obviously $D\subset \bigcup_{k=1}^{\infty}D_k$ and $[a,b]=I\cup  \bigcup_{k=1}^{\infty}D_k$ (since $D_k\subseteq [a,b]$). The compactness of $[a,b]$ implies $[a,b]=I\cup  \bigcup_{k=1}^{N}D_k$.  Now $[a,b]\setminus  \bigcup_{k=1}^{N}D_k$ is compact (closed and bounded) and included in $I$.  As such it can be covered by  $$F_x=(x-\delta_x,x+\delta_x)$$ where $ \delta_x>0:$ is chosen so that $\omega f((x-\delta_x,x+\delta_x)\cap [a,b])<\epsilon$. Compactness implies the existence of a finite subcover, $$[a,b]\setminus  \bigcup_{k=1}^{N}D_k\subseteq  \bigcup_{i=1}^{M}(x_i-\delta_i,x+\delta_i)$$ As we can replace the intervals that intersect we can suppose  $$ \bigcap_{i=1}^{M}[x_i-\delta_i,x+\delta_i]=\emptyset$$ Therefore, $$[a,b]=  \bigcup_{k=1}^{N}\overline{D}_k\cup  \bigcup_{i=1}^{M}[x_i-\delta_i,x+\delta_i]=\bigcup_{i=0}^{n}[t_{i-1},t_i]$$ where for $i\le n$, $[t_{i-1},t_i]=  [x_k-\delta_k,x+\delta_k]$ or $[t_{i-1},t_i]=  \overline{D}_k$ because $$\bigcup_{k=1}^{N}\overline{D}_k\cap  \bigcup_{i=1}^{M}[x_i-\delta_i,x+\delta_i]=\emptyset$$ Considering all the endpoints of the above (that are pairwise different) we can create a partition $\mathcal{P}=\left\{a=t_0<...<t_n=b\right\}$ of $[a,b]$. We separate the indices: $A=\left\{i:[t_{i-1},t_i]=  [x_k-\delta_k,x+\delta_k]\right\}$ and $B=\left\{i:[t_{i-1},t_i]=  \overline{D}_k \right\}$. Therefore, \begin{gather}U_{f,\mathcal{P}}-L_{f,\mathcal{P}}=\sum_{i=1}^n\omega f([t_{i-1},t_i])(t_i-t_{i-1})=\sum_{i\in A}\omega f([t_{i-1},t_i])\ell([t_{i-1},t_i])+\sum_{i\in B}\omega f([t_{i-1},t_i])\ell([t_{i-1},t_i])\\ \le \sum_{i\in A}2\left\|f\right\|\ell([t_{i-1},t_i])+\sum_{i\in B}\epsilon\ell(\overline{D}_k)\le 2\left\|f\right\|\frac{\epsilon}{M}+\epsilon(b-a)=2\epsilon+\epsilon(b-a)\end{gather} My questions are: Is this proof correct? ( I doubt the point: ""where for $i\le n$, $[t_{i-1},t_i]=  [x_k-\delta_k,x+\delta_k]$ or $[t_{i-1},t_i]=  \overline{D}_k$ because $$\bigcup_{k=1}^{N}\overline{D}_k\cap  \bigcup_{i=1}^{M}[x_i-\delta_i,x+\delta_i]=\emptyset$$"") Second, can it be simplified?",,"['calculus', 'real-analysis', 'integration']"
23,What's the name of this theorem?,What's the name of this theorem?,,"It happens very often in physics that we find relations like: $$\int_V f(x) dx = \int_V g(x) dx$$ for an arbitrary volume $V$. From this we usually say ""Since the volume is arbitrary, the integrands have to be equal"" and we conclude $f(x) = g(x) $. Funnily enough, this is often stated without even mentioning the name of the theorem that allows us to do so. Intuitively, it's clear why is it so; but what's the theorem that formally allows us to do that?","It happens very often in physics that we find relations like: $$\int_V f(x) dx = \int_V g(x) dx$$ for an arbitrary volume $V$. From this we usually say ""Since the volume is arbitrary, the integrands have to be equal"" and we conclude $f(x) = g(x) $. Funnily enough, this is often stated without even mentioning the name of the theorem that allows us to do so. Intuitively, it's clear why is it so; but what's the theorem that formally allows us to do that?",,"['real-analysis', 'calculus', 'functions']"
24,A multivariate function with bounded partial derivatives is Lipschitz,A multivariate function with bounded partial derivatives is Lipschitz,,"I'm curious if I've done this correctly -- please offer suggestions/corrections if not! I'm new to working in $\Bbb R^n$ so clear insights would be appreciated. The problem: Let $f:\Bbb R^2 \to \Bbb R$ be such that each $D_1f$ and $D_2f$ are defined everywhere and are bounded functions. Prove that $f$ is Lipschitz. My attempt: The definition of Lipschitz that I'm working with is that there exists some $L > 0$  such that $|f(x) -f(y)| \leq L||x-y||$. Since $D_1f$ and $D_2f$ are bounded, there must exist: $S_1 = \sup \{||D_1f(x)|| : x \in \Bbb R^2\}$ $S_2 = \sup \{||D_2f(x)|| : x \in \Bbb R^2\}$ Now let $a = (a_1,a_2)$ and $b = (b_1,b_2) \in \Bbb R^2$. Then we have: \begin{align*} f(a) - f(b) &= f(a_1,a_2) - f(b_1,b_2) \\ &= f(a_1,a_2) - f(a_1,b_2) + f(a_1,b_2) - f(b_1,b_2) \end{align*} Then, by the triangle inequality: $$|f(a)-f(b)| \leq |f(a_1,a_2) - (a_1,b_2)| + |f(a_1,b_2) - f(b_1,b_2)|$$ And since the partial derivatives exist everywhere in $\Bbb R^2$, we can use the one-dimensional Mean Value Theorem to show that there exists some $c$ such that: $$\frac{f(a_1,a_2)-f(a_1,b_2)}{a_2-b_2} = D_2f(a_1,c)$$ And noting how we defined $S_2$, it follows that $$|f(a_1,a_2) - f(a_1,b_2) \leq S_2 |a_2 - b_2|$$ And similarly $$|f(a_1,b_2) - f(b_1,b_2)| \leq S_1 |a_1-b_1|$$ And using the statement we got from the triangle inequality, we have that $$|f(a)-f(b)| \leq S_1|a_1-b_1| + S_2|a_2 - b_2|$$ And by the Cauchy-Schwarz inequality, we have that $$S_1|a_1-b_1| + S_2|a_2 - b_2| \leq  \sqrt{S_1^2 + S_2^2}\cdot \sqrt{(a_1-b_1)^2+(a_2-b_2)^2} = \sqrt{S_1^2 + S_2^2} \cdot ||a-b||$$ Whereby $$|f(a)-f(b)| \leq \sqrt{S_1^2 + S_2^2} \cdot ||a-b||$$ So $f$ is Lipschitz with $L = \sqrt{S_1^2 + S_2^2}$.","I'm curious if I've done this correctly -- please offer suggestions/corrections if not! I'm new to working in $\Bbb R^n$ so clear insights would be appreciated. The problem: Let $f:\Bbb R^2 \to \Bbb R$ be such that each $D_1f$ and $D_2f$ are defined everywhere and are bounded functions. Prove that $f$ is Lipschitz. My attempt: The definition of Lipschitz that I'm working with is that there exists some $L > 0$  such that $|f(x) -f(y)| \leq L||x-y||$. Since $D_1f$ and $D_2f$ are bounded, there must exist: $S_1 = \sup \{||D_1f(x)|| : x \in \Bbb R^2\}$ $S_2 = \sup \{||D_2f(x)|| : x \in \Bbb R^2\}$ Now let $a = (a_1,a_2)$ and $b = (b_1,b_2) \in \Bbb R^2$. Then we have: \begin{align*} f(a) - f(b) &= f(a_1,a_2) - f(b_1,b_2) \\ &= f(a_1,a_2) - f(a_1,b_2) + f(a_1,b_2) - f(b_1,b_2) \end{align*} Then, by the triangle inequality: $$|f(a)-f(b)| \leq |f(a_1,a_2) - (a_1,b_2)| + |f(a_1,b_2) - f(b_1,b_2)|$$ And since the partial derivatives exist everywhere in $\Bbb R^2$, we can use the one-dimensional Mean Value Theorem to show that there exists some $c$ such that: $$\frac{f(a_1,a_2)-f(a_1,b_2)}{a_2-b_2} = D_2f(a_1,c)$$ And noting how we defined $S_2$, it follows that $$|f(a_1,a_2) - f(a_1,b_2) \leq S_2 |a_2 - b_2|$$ And similarly $$|f(a_1,b_2) - f(b_1,b_2)| \leq S_1 |a_1-b_1|$$ And using the statement we got from the triangle inequality, we have that $$|f(a)-f(b)| \leq S_1|a_1-b_1| + S_2|a_2 - b_2|$$ And by the Cauchy-Schwarz inequality, we have that $$S_1|a_1-b_1| + S_2|a_2 - b_2| \leq  \sqrt{S_1^2 + S_2^2}\cdot \sqrt{(a_1-b_1)^2+(a_2-b_2)^2} = \sqrt{S_1^2 + S_2^2} \cdot ||a-b||$$ Whereby $$|f(a)-f(b)| \leq \sqrt{S_1^2 + S_2^2} \cdot ||a-b||$$ So $f$ is Lipschitz with $L = \sqrt{S_1^2 + S_2^2}$.",,"['real-analysis', 'analysis', 'multivariable-calculus', 'proof-verification', 'lipschitz-functions']"
25,Does there exist a sequence $\{a_n\}_{n\ge1}$ with $a_n < a_{n+1}+a_{n^2}$ such that $\sum_{n=1}^{\infty}a_n$ converges?,Does there exist a sequence  with  such that  converges?,\{a_n\}_{n\ge1} a_n < a_{n+1}+a_{n^2} \sum_{n=1}^{\infty}a_n,Does there exist a sequence $\{a_n\}_{n\ge1}$ with $a_n < a_{n+1}+a_{n^2}$ such that $\sum_{n=1}^{\infty}a_n$ converges? Does there exist a sequence with the same property but with each term positive?,Does there exist a sequence $\{a_n\}_{n\ge1}$ with $a_n < a_{n+1}+a_{n^2}$ such that $\sum_{n=1}^{\infty}a_n$ converges? Does there exist a sequence with the same property but with each term positive?,,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'contest-math']"
26,Does an unbounded open subset of $\mathbb{R}$ contain infinitely many multiples of some number?,Does an unbounded open subset of  contain infinitely many multiples of some number?,\mathbb{R},"If $U$ is an unbounded open subset of $\mathbb{R}$ , does there exist some $a$ such that infinitely many integer multiples of $a$ belong to U?","If is an unbounded open subset of , does there exist some such that infinitely many integer multiples of belong to U?",U \mathbb{R} a a,"['real-analysis', 'general-topology']"
27,Is there an orthonormal set of polynomials whose derivatives are orthogonal?,Is there an orthonormal set of polynomials whose derivatives are orthogonal?,,"Let $T:\mathbb{R}[x]\rightarrow \mathbb{R}[x]$ be the linear map $p(x)\mapsto \frac{d}{dx} p(x)$ . Is there an orthonormal basis $b_1,b_2,...$ for the polynomial ring $\mathbb{R}[x]$ with respect to the $L^2([0,1])$ norm such that the elements $T(b_1), T(b_2),...$ are orthogonal? Edit: Perhaps I should add some motivation. It often happens in linear algebra that methods in the finite dimensional case carry over to the infinite dimensional case. For instance, Fourier series are a consequence of finding an orthonormal basis of $L^2([-\pi,\pi])$ . The SVD is an example that as far as I know doesn't have that many applications in pure math, despite it being extremely useful in applied math. If you know how the SVD works, you'll recognize that this is exactly what I'm asking for here, in the simplest example of an interesting linear operator on $\mathbb{R}[x]$ that I know, $\frac{d}{dx}$ .","Let be the linear map . Is there an orthonormal basis for the polynomial ring with respect to the norm such that the elements are orthogonal? Edit: Perhaps I should add some motivation. It often happens in linear algebra that methods in the finite dimensional case carry over to the infinite dimensional case. For instance, Fourier series are a consequence of finding an orthonormal basis of . The SVD is an example that as far as I know doesn't have that many applications in pure math, despite it being extremely useful in applied math. If you know how the SVD works, you'll recognize that this is exactly what I'm asking for here, in the simplest example of an interesting linear operator on that I know, .","T:\mathbb{R}[x]\rightarrow \mathbb{R}[x] p(x)\mapsto \frac{d}{dx} p(x) b_1,b_2,... \mathbb{R}[x] L^2([0,1]) T(b_1), T(b_2),... L^2([-\pi,\pi]) \mathbb{R}[x] \frac{d}{dx}","['real-analysis', 'linear-algebra', 'functional-analysis', 'polynomials', 'linear-transformations']"
28,"Finding $n$ points $x_i$ in the plane, with $\sum_{i=1}^n \vert x_i \vert^2=1$, minimizing $\sum_{i\neq j}^n \frac{1}{\sqrt{\vert x_i-x_j \vert}}$","Finding  points  in the plane, with , minimizing",n x_i \sum_{i=1}^n \vert x_i \vert^2=1 \sum_{i\neq j}^n \frac{1}{\sqrt{\vert x_i-x_j \vert}},"Let $x_1,..,x_n$ be points in $\mathbb R^2$ under the constraint $$\sum_{i=1}^n \vert x_i \vert^2=1.$$ So not all the points are on the circle, but their sum of the norms is constrained. I am looking for the minimizing configuration of the function $$f(x_1,..,x_n):=\sum_{i\neq j}^n \frac{1}{\sqrt{\vert x_i-x_j \vert}}$$ According to some of the first answers, it seems that we get orbits around a centre. Is there any explanation for that? Please let me know if you have any questions.","Let be points in under the constraint So not all the points are on the circle, but their sum of the norms is constrained. I am looking for the minimizing configuration of the function According to some of the first answers, it seems that we get orbits around a centre. Is there any explanation for that? Please let me know if you have any questions.","x_1,..,x_n \mathbb R^2 \sum_{i=1}^n \vert x_i \vert^2=1. f(x_1,..,x_n):=\sum_{i\neq j}^n \frac{1}{\sqrt{\vert x_i-x_j \vert}}","['real-analysis', 'geometry']"
29,What makes an integral an integral?,What makes an integral an integral?,,"I recently found an entire category of Wikipedia pages discussing different definitions of the integral.  It includes the familiar definitions of Riemann and Lebesgue, the less familiar but still well known Stieltjes integrals, and quite unfamiliar (but cool) definitions like the Henstock–Kurzweil integral and the Khinchin integral . Now my background is in physics, where we have another kind of integration called path integral (or functional integration ), which remains generally ill-defined despite many decades of fruitful use in theoretical physics.  Naturally, one of my first thoughts upon discovering the exotic integrals in the aforementioned Wikipedia pages (like Henstock–Kurzweil) was whether they could help give rigorous definition to physicists' path integrals, but unfortunately it doesn't look like that's the case. Thinking about all these different sorts of integrals got me to wondering: What are the common features of all these different sorts of integrals that make them ""integrals""? In other words, what are the minimal requirements for some mathematical definition to be an integral?  If I had to guess, I would say the following is a plausible, though imprecise, start: Given a vector space of functions $V$ , a definite integral $\int$ on this space is a function from a subspace $I\subseteq V$ of ""integrable functions"" to a number field $F$ , such that: $\int$ is linear. $\int$ agrees with our intuition for certain simple functions.  In finite dimensions, this could be e.g. that $\int$ applied to the indicator function of a cube is the volume of the cube.  In infinite dimensions, you might prefer to work with Gaussians instead of indicator functions of cubes. I would be tempted to add some condition about continuity, but I'm not sure that would be appropriate in infinite dimensions (i.e. $\int$ might be unbounded?).   I wonder if anyone has tried to define integrals in the abstract along these lines, or if these conditions are roughly the minimal ""common features"" of all integrals?","I recently found an entire category of Wikipedia pages discussing different definitions of the integral.  It includes the familiar definitions of Riemann and Lebesgue, the less familiar but still well known Stieltjes integrals, and quite unfamiliar (but cool) definitions like the Henstock–Kurzweil integral and the Khinchin integral . Now my background is in physics, where we have another kind of integration called path integral (or functional integration ), which remains generally ill-defined despite many decades of fruitful use in theoretical physics.  Naturally, one of my first thoughts upon discovering the exotic integrals in the aforementioned Wikipedia pages (like Henstock–Kurzweil) was whether they could help give rigorous definition to physicists' path integrals, but unfortunately it doesn't look like that's the case. Thinking about all these different sorts of integrals got me to wondering: What are the common features of all these different sorts of integrals that make them ""integrals""? In other words, what are the minimal requirements for some mathematical definition to be an integral?  If I had to guess, I would say the following is a plausible, though imprecise, start: Given a vector space of functions , a definite integral on this space is a function from a subspace of ""integrable functions"" to a number field , such that: is linear. agrees with our intuition for certain simple functions.  In finite dimensions, this could be e.g. that applied to the indicator function of a cube is the volume of the cube.  In infinite dimensions, you might prefer to work with Gaussians instead of indicator functions of cubes. I would be tempted to add some condition about continuity, but I'm not sure that would be appropriate in infinite dimensions (i.e. might be unbounded?).   I wonder if anyone has tried to define integrals in the abstract along these lines, or if these conditions are roughly the minimal ""common features"" of all integrals?",V \int I\subseteq V F \int \int \int \int,"['real-analysis', 'integration', 'functional-analysis', 'definition']"
30,Functional analysis proof of Ramanujan's Master Theorem,Functional analysis proof of Ramanujan's Master Theorem,,"According to mathworld , Ramanujan's master theorem is the statement that if $$f(z) = \sum_{k=0}^{\infty} \frac{\phi(k) (-z)^k}{k!}$$ for some function (analytic or integrable) $\phi$, then $$\int_0^{\infty} x^{n-1} f(x) \, \mathrm{d}x = \Gamma(n) \phi(-n).$$ As written it is obviously false as the values of an (analytic or integrable) function $\phi$ at natural numbers do not determine its values anywhere else. However it turns out that $$\int_0^{\infty} x^{s-1} f(x) \, \mathrm{d}x = \Gamma(s) \phi(-s)$$ for arbitrary $s$ under growth conditions on $\phi$. Recently I came across an elementary ""proof"": if $T$ denotes the shift operator $T\phi(s) := \phi(s+1),$ then we can write $$f(z) = \sum_{k=0}^{\infty} \frac{(-z)^kT^k \phi(0)}{k!} = e^{-zT}\phi (0)$$ such that $$\int_0^{\infty} x^{n-1} f(x) \, \mathrm{d}x = \int_0^{\infty} x^{n-1} e^{-xT} \phi(0) \, \mathrm{d}x = \Gamma(n) T^{-n}\phi(0) = \Gamma(n) \phi(-n),$$ by plugging $T$ into the Gamma integral $$\int_0^{\infty} x^{n-1} e^{-xs} \, \mathrm{d}x = \Gamma(n) s^{-n}.$$ I am curious whether this argument can be made rigorous with functional analysis on an appropriate function space (which necessarily would have to have some growth conditions).","According to mathworld , Ramanujan's master theorem is the statement that if $$f(z) = \sum_{k=0}^{\infty} \frac{\phi(k) (-z)^k}{k!}$$ for some function (analytic or integrable) $\phi$, then $$\int_0^{\infty} x^{n-1} f(x) \, \mathrm{d}x = \Gamma(n) \phi(-n).$$ As written it is obviously false as the values of an (analytic or integrable) function $\phi$ at natural numbers do not determine its values anywhere else. However it turns out that $$\int_0^{\infty} x^{s-1} f(x) \, \mathrm{d}x = \Gamma(s) \phi(-s)$$ for arbitrary $s$ under growth conditions on $\phi$. Recently I came across an elementary ""proof"": if $T$ denotes the shift operator $T\phi(s) := \phi(s+1),$ then we can write $$f(z) = \sum_{k=0}^{\infty} \frac{(-z)^kT^k \phi(0)}{k!} = e^{-zT}\phi (0)$$ such that $$\int_0^{\infty} x^{n-1} f(x) \, \mathrm{d}x = \int_0^{\infty} x^{n-1} e^{-xT} \phi(0) \, \mathrm{d}x = \Gamma(n) T^{-n}\phi(0) = \Gamma(n) \phi(-n),$$ by plugging $T$ into the Gamma integral $$\int_0^{\infty} x^{n-1} e^{-xs} \, \mathrm{d}x = \Gamma(n) s^{-n}.$$ I am curious whether this argument can be made rigorous with functional analysis on an appropriate function space (which necessarily would have to have some growth conditions).",,"['real-analysis', 'functional-analysis', 'operator-theory', 'gamma-function']"
31,Is $\sin(e)$ rational or irrational?,Is  rational or irrational?,\sin(e),"We know that $\pi$ and $e$ are transcendental numbers. Here $\sin(x)$ is a real trigonometric function. We know that $\sin(\pi)=0$ which is rational. Now I am wondering to know that whether $\sin(e)$ is rational or irrational. In addition, if it is irrational then whether $\sin(e)$ is transcendental.","We know that and are transcendental numbers. Here is a real trigonometric function. We know that which is rational. Now I am wondering to know that whether is rational or irrational. In addition, if it is irrational then whether is transcendental.",\pi e \sin(x) \sin(\pi)=0 \sin(e) \sin(e),"['real-analysis', 'trigonometry', 'real-numbers', 'transcendental-numbers']"
32,How badly-behaved are the derivatives of non-analytic smooth functions?,How badly-behaved are the derivatives of non-analytic smooth functions?,,"Suppose $f:\mathbb{R} \to \mathbb{R}$ is a smooth function such that $f^{(n)}(0) = 0$ for all $n \in \mathbb{N}_{\geq 0}$ and that $f$ is not analytic. In particular, we assume that $f$ is not identically $0$ in any neighbourhood of $x=0$. Does it follow that for all $\epsilon>0$ $$\lim_{n \to +\infty} \sup\{f^{(n)}(x) : x \in (-\epsilon, \epsilon)\} = +\infty$$ and $$\lim_{n \to +\infty} \inf\{f^{(n)}(x) : x \in (-\epsilon, \epsilon)\} = -\infty?$$ Motivation: the standard examples we all know and love [e.g., $\exp (-\frac{1}{|x|}), \exp\left(-\frac{1}{x^2}\right)$] have derivatives which exhibit extreme oscillatory behaviour near the origin when $n$ gets large. For these functions, this makes sense intuitively. To ensure the function ""smoothly"" and ""flatly"" reaches $x=0$, the first derivative needs to rapidly become small in magnitude, which is only possible if the second derivative temporarily becomes large in magnitude, but then it's necessary for the second derivative to rapidly become small again (since $f^{(2)}(0) = 0$), which means that the third derivative has to do some work, and you can see a pattern developing.","Suppose $f:\mathbb{R} \to \mathbb{R}$ is a smooth function such that $f^{(n)}(0) = 0$ for all $n \in \mathbb{N}_{\geq 0}$ and that $f$ is not analytic. In particular, we assume that $f$ is not identically $0$ in any neighbourhood of $x=0$. Does it follow that for all $\epsilon>0$ $$\lim_{n \to +\infty} \sup\{f^{(n)}(x) : x \in (-\epsilon, \epsilon)\} = +\infty$$ and $$\lim_{n \to +\infty} \inf\{f^{(n)}(x) : x \in (-\epsilon, \epsilon)\} = -\infty?$$ Motivation: the standard examples we all know and love [e.g., $\exp (-\frac{1}{|x|}), \exp\left(-\frac{1}{x^2}\right)$] have derivatives which exhibit extreme oscillatory behaviour near the origin when $n$ gets large. For these functions, this makes sense intuitively. To ensure the function ""smoothly"" and ""flatly"" reaches $x=0$, the first derivative needs to rapidly become small in magnitude, which is only possible if the second derivative temporarily becomes large in magnitude, but then it's necessary for the second derivative to rapidly become small again (since $f^{(2)}(0) = 0$), which means that the third derivative has to do some work, and you can see a pattern developing.",,"['real-analysis', 'derivatives', 'analyticity']"
33,"If $V\subset L^\infty[0,1]$ with $\|f\|_\infty \leq c\|f\|_2$, then $V$ is finite dimensional","If  with , then  is finite dimensional","V\subset L^\infty[0,1] \|f\|_\infty \leq c\|f\|_2 V","If $V$ is a linear subspace of $L^\infty[0,1]$ with $\|f\|_\infty \leq c\|f\|_2$ for all $f\in V$, then $V$ is finite dimensional. The proof is an explicit calculation: Since $L^\infty[0,1] \subset L^2[0,1]$, take $e_1,\cdots , e_n$ to be $L^2$-orthonormal vectors in $V$. Fix some $x$ in $[0,1]$. We have, for all $y\in[0,1]$, $$\left|\sum e_i(x)e_i(y)\right| \leq \left\|\sum e_i(x) e_i(\cdot) \right\|_\infty \leq c\left\|\sum e_i(x)e_i(\cdot)\right\|_2 = c \sqrt{ \sum e_i^2(x)},$$ take $y = x$ this implies  $$\sum e_i^2(x) \leq c^2.$$ Integrate both side and we get $$  n=\int_0^1 \sum e_i^2(x) \leq c^2.$$ This proof is simple but it is not really intuitive for me. Could you guys help me with a more functional analysis argument of this? Since for functions in $L^\infty[0,1]$ we always have $\|f\|_2 \leq  \|f\|_\infty$ , paired with $\|f\|_\infty \leq c \|f\|_2$, this means that the identity map is a continuous bijection between $(V, \|\cdot \|_2)$ and $(V, \|\cdot \|_\infty)$. Also $\|\cdot\|_2$ and $\|\cdot\|_\infty$ are equivalent on $V$. I know any two norms are equivalent in a finite dimensional space, but I dont know if there is anything special about the $L^2$ and $L^\infty$ norm to make the converse of that true as well.","If $V$ is a linear subspace of $L^\infty[0,1]$ with $\|f\|_\infty \leq c\|f\|_2$ for all $f\in V$, then $V$ is finite dimensional. The proof is an explicit calculation: Since $L^\infty[0,1] \subset L^2[0,1]$, take $e_1,\cdots , e_n$ to be $L^2$-orthonormal vectors in $V$. Fix some $x$ in $[0,1]$. We have, for all $y\in[0,1]$, $$\left|\sum e_i(x)e_i(y)\right| \leq \left\|\sum e_i(x) e_i(\cdot) \right\|_\infty \leq c\left\|\sum e_i(x)e_i(\cdot)\right\|_2 = c \sqrt{ \sum e_i^2(x)},$$ take $y = x$ this implies  $$\sum e_i^2(x) \leq c^2.$$ Integrate both side and we get $$  n=\int_0^1 \sum e_i^2(x) \leq c^2.$$ This proof is simple but it is not really intuitive for me. Could you guys help me with a more functional analysis argument of this? Since for functions in $L^\infty[0,1]$ we always have $\|f\|_2 \leq  \|f\|_\infty$ , paired with $\|f\|_\infty \leq c \|f\|_2$, this means that the identity map is a continuous bijection between $(V, \|\cdot \|_2)$ and $(V, \|\cdot \|_\infty)$. Also $\|\cdot\|_2$ and $\|\cdot\|_\infty$ are equivalent on $V$. I know any two norms are equivalent in a finite dimensional space, but I dont know if there is anything special about the $L^2$ and $L^\infty$ norm to make the converse of that true as well.",,"['real-analysis', 'functional-analysis', 'normed-spaces', 'lp-spaces']"
34,"Evaluating $\int_{0}^{\pi/4} \log(\sin(x)) \log(\cos(x)) \log(\cos(2x)) \,dx$",Evaluating,"\int_{0}^{\pi/4} \log(\sin(x)) \log(\cos(x)) \log(\cos(2x)) \,dx","What tools would you recommend me for evaluating this integral? $$\int_{0}^{\pi/4} \log(\sin(x)) \log(\cos(x)) \log(\cos(2x)) \,dx$$ My first thought was to use the beta function, but it's hard to get such a form because of $\cos(2x)$. What other options do I have?","What tools would you recommend me for evaluating this integral? $$\int_{0}^{\pi/4} \log(\sin(x)) \log(\cos(x)) \log(\cos(2x)) \,dx$$ My first thought was to use the beta function, but it's hard to get such a form because of $\cos(2x)$. What other options do I have?",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
35,Convergence/Divergence of infinite series $\sum\limits_{n=1}^{\infty} \frac{1}{n^{1+\left|{\cos n}\right|}}$,Convergence/Divergence of infinite series,\sum\limits_{n=1}^{\infty} \frac{1}{n^{1+\left|{\cos n}\right|}},"It is well known that $ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n}$ is divergent while $ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n^{1+\epsilon}}$ is convergent for a fixed positive value of $\epsilon$. It is not difficult to show that $ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n^{1+\frac{1}{n}}}$ is divergent using Limit comparison test with  $ \displaystyle\frac{1}{n}$. There is a post on this question here . Now comes my questions: (i) Is $ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n^{1+\left|{\cos n}\right|}}$ convergent or divergent? (I have tried several tests, like: comparison/limit comparison tests, but fail to get conclusion. My intuition is that it is divergent...) (ii) It was stated here that $ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n^{2-\cos n}}=\sum_{n=1}^{\infty} \frac{1}{n^{1+(1-\cos n)}}$ is divergent. So is there is general way to determine if $ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n^{1+f(n)}}$ with $f(n)>0$ for all natural number $n$, a convergent or divergent series? Any comment or answer?","It is well known that $ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n}$ is divergent while $ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n^{1+\epsilon}}$ is convergent for a fixed positive value of $\epsilon$. It is not difficult to show that $ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n^{1+\frac{1}{n}}}$ is divergent using Limit comparison test with  $ \displaystyle\frac{1}{n}$. There is a post on this question here . Now comes my questions: (i) Is $ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n^{1+\left|{\cos n}\right|}}$ convergent or divergent? (I have tried several tests, like: comparison/limit comparison tests, but fail to get conclusion. My intuition is that it is divergent...) (ii) It was stated here that $ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n^{2-\cos n}}=\sum_{n=1}^{\infty} \frac{1}{n^{1+(1-\cos n)}}$ is divergent. So is there is general way to determine if $ \displaystyle\sum_{n=1}^{\infty} \frac{1}{n^{1+f(n)}}$ with $f(n)>0$ for all natural number $n$, a convergent or divergent series? Any comment or answer?",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
36,Integral $\int_0^\infty \frac{\log^2 x \cos ax}{x^n-1}dx$,Integral,\int_0^\infty \frac{\log^2 x \cos ax}{x^n-1}dx,"Hi I am trying to calculate $$ I:=\int\limits_0^\infty \frac{\log^2 x \cos (ax)}{x^n-1}\mathrm dx,\quad \Re(n)>1, \, a\in \mathbb{R}. $$ Note if we set $a=0$ we get a similar integral given by $$ \int\limits_0^\infty \frac{\log^2 x }{x^n-1}\mathrm dx=-\frac{2\pi^3\cot(\pi/n)\csc^2(\pi/n)}{n^3},\quad \Re(n)>1. $$ I was trying to write I as  $$ I=\Re \bigg[\int\limits_0^\infty  \frac{ e^{i ax}\log^2 x}{x^n-1}\mathrm dx\bigg]=\Re\bigg[\int\limits_\infty^0\frac{e^{iax}\log^2 x}{1-x^n}\mathrm dx\bigg]=\Re\bigg[\int\limits_\infty^0e^{iax}\log^2 x\sum_{m=0}^\infty x^{nm} \mathrm dx\bigg]. $$ But was unsure of where to go from here. How can we calculate $I$? It is clear that this method is not going to work.","Hi I am trying to calculate $$ I:=\int\limits_0^\infty \frac{\log^2 x \cos (ax)}{x^n-1}\mathrm dx,\quad \Re(n)>1, \, a\in \mathbb{R}. $$ Note if we set $a=0$ we get a similar integral given by $$ \int\limits_0^\infty \frac{\log^2 x }{x^n-1}\mathrm dx=-\frac{2\pi^3\cot(\pi/n)\csc^2(\pi/n)}{n^3},\quad \Re(n)>1. $$ I was trying to write I as  $$ I=\Re \bigg[\int\limits_0^\infty  \frac{ e^{i ax}\log^2 x}{x^n-1}\mathrm dx\bigg]=\Re\bigg[\int\limits_\infty^0\frac{e^{iax}\log^2 x}{1-x^n}\mathrm dx\bigg]=\Re\bigg[\int\limits_\infty^0e^{iax}\log^2 x\sum_{m=0}^\infty x^{nm} \mathrm dx\bigg]. $$ But was unsure of where to go from here. How can we calculate $I$? It is clear that this method is not going to work.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
37,"Theorem 6.17 in Baby Rudin, 3rd ed: $\int_a^b f \,d\alpha = \int_a^b f(x) \alpha^\prime(x) \,dx$","Theorem 6.17 in Baby Rudin, 3rd ed:","\int_a^b f \,d\alpha = \int_a^b f(x) \alpha^\prime(x) \,dx","Here is Theorem 6.17 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Assume $\alpha$ increases monotonically and $\alpha^\prime \in \mathscr{R}$ on $[a, b]$ . Let $f$ be a bounded real function on $[a, b]$ . Then $f \in \mathscr{R}(\alpha)$ if and only if $f\alpha^\prime \in \mathscr{R}$ . In that case $$ \int_a^b f \,d\alpha = \int_a^b f(x) \alpha^\prime(x)\, dx. $$ For terminology, here is the link to my post here on Math SE on Theorem 6.15 in Baby Rudin, 3rd edition. Here is Rudin's proof of Theorem 6.17: Let $\varepsilon > 0$ be given and apply Theorem 6.6 to $\alpha^\prime$ : There is a partition $P = \left\{ \ x_0, \ldots, x_n \  \right\}$ of $[a, b]$ such that $$ \tag{28} U(P, \alpha^\prime) - L(P, \alpha^\prime) < \varepsilon. $$ The mean value theorem furnishes points $t_i \in \left[ x_{i-1}, x_i \right]$ such that $$ \Delta \alpha_i = \alpha^\prime \left( t_i \right) \Delta x_i $$ for $i = 1, \ldots, n$ . If $s_i \in \left[ x_{i-1}, x_i \right]$ , then $$ \tag{29}  \sum_{i=1}^n \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i < \varepsilon, $$ by (28) and Theorem 6.7 (b). Put $M = \sup \lvert f(x) \rvert$ . Since $$ \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i = \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( t_i \right) \Delta x_i $$ it follows from (29) that $$ \tag{30}  \left\lvert  \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i  -  \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i \right\rvert \leq M \varepsilon. $$ In particular, $$  \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i  \leq U( P, f \alpha^\prime ) + M \varepsilon, $$ for all choices of $s_i \in \left[ x_{i-1}, x_i \right]$ , so that $$ U(P, f, \alpha) \leq U(P, f \alpha^\prime) + M \varepsilon. $$ The same argument leads from (30) to $$ U(P, f \alpha^\prime) \leq U(P, f, \alpha) + M \varepsilon. $$ Thus $$ \tag{31} \left\lvert U(P, f, \alpha) - U(P, f \alpha^\prime ) \right\rvert \leq M \varepsilon. $$ Now note that (28) remains true if $P$ is replaced by any refinement. Hence (31) also remains true. We conclude that $$ \left\lvert \overline{\int}_a^b f \, d \alpha - \overline{\int}_a^b f (x) \alpha^\prime(x) \,dx  \right\rvert \leq M \varepsilon. $$ But $\varepsilon$ is arbitrary. Hence $$ \tag{32} \overline{\int}_a^b f \,d \alpha = \overline{\int}_a^b f (x) \alpha^\prime(x) \,dx  $$ for any bounded $f$ . The equality of the lower integrals follows from (30) in exactly the same way. The theorem follows. Here is Theorem 6.6 in Baby Rudin, 3rd edition: $f \in \mathscr{R}(\alpha)$ on $[a, b]$ if and only if for every $\varepsilon > 0$ there exists a partition $P$ such that $$ \tag{13} U(P, f, \alpha ) - L(P, f, \alpha ) < \varepsilon. $$ Here is Theorem 6.7: Theorem 6. 7(a): If (13) holda for some $P$ and some $\varepsilon$ , then (13) holds (with the same $\varepsilon$ ) for every refinement of $P$ . Theorem 6. 7(b): If (13) holds for $P = \left\{ \ x_0, \ldots, x_n \ \right\}$ and if $s_i$ , $t_i$ are arbitrary points in $\left[ x_{i-1}, x_i \right]$ , then $$ \sum_{i=1}^n \left\lvert f \left( s_i \right) - f \left( t_i \right) \right\rvert \Delta \alpha_i < \varepsilon. $$ Theorem 6.7(c): If $f \in \mathscr{R}(\alpha)$ and the hypotheses of (b) hold, then $$ \left\lvert \sum_{i=1}^n f \left( t_i \right) \Delta \alpha_i - \int_a^b f\, d \alpha \right\rvert < \varepsilon. $$ Now here is my understanding of Rudin's proof of Theorem 6.17: As $f$ is a bounded real function on $[a, b]$ , so there is a positive real number $\varepsilon > 0$ such that $$ \lvert f(x) \rvert < M \tag{0} $$ for all $x \in [a, b]$ . Let $\varepsilon > 0$ be a given real number. As $\alpha^\prime$ is Riemann-integrable on $[a, b]$ , so there exists a partition $P = \left\{ \ x_0, \ldots, x_n \ \right\}$ of $[a, b]$ such that $$ U(P, \alpha^\prime) - L(P, \alpha^\prime) < \frac{\varepsilon}{2M}. \tag{1} $$ Now as $\alpha^\prime$ exists on $[a, b]$ , so,  For each $i \in \{ 1, \ldots, n \}$ , the function $\alpha$ is continuous on $\left[ x_{i-1}, x_i \right]$ and differentiable on $\left( x_{i-1}, x_i \right)$ , and hence  there exists a point $t_i \in \left( x_{i-1}, x_i \right)$ such that $$  \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right) = \alpha^\prime \left( t_i \right) \left( x_i - x_{i-1} \right); $$ that is, $$ \Delta \alpha_i = \alpha^\prime \left( t_i \right) \Delta x_i \tag{2} $$ for each $i = 1, \ldots, n$ . And, for each $i \in \{1, \ldots, n \}$ , if $s_i \in \left[ x_{i-1}, x_i \right]$ , then, as $t_i \in \left[ x_{i-1}, x_i \right]$ also, so we conclude that $$\tag{3a} m_i ( \alpha^\prime )  \leq \alpha^\prime \left( s_i \right) \leq M_i (\alpha^\prime), $$ and $$\tag{3b}  m_i (\alpha^\prime) \leq \alpha^\prime \left( t_i \right) \leq M_i ( \alpha^\prime) ,  $$ where $$ m_i ( \alpha^\prime ) \colon=  \inf \left\{ \ \alpha^\prime (x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\}, $$ and $$ M_i ( \alpha^\prime) \colon= \sup  \left\{ \ \alpha^\prime (x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\};$$ now (3b) implies that $$ \tag{3c} - M_i ( \alpha^\prime ) \leq - \alpha^\prime \left( t_i \right) \leq - m_i (\alpha^\prime),  $$ and upon adding (3c) to (3a), we obtain $$ m_i ( \alpha^\prime ) - M_i( \alpha^\prime) \leq \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \leq M_i ( \alpha^\prime) - m_i( \alpha^\prime),   $$ which implies that $$ \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \leq M_i ( \alpha^\prime) - m_i( \alpha^\prime),  \tag{3d} $$ and, as $\Delta x_i = x_i - x_{i-1} \geq 0$ , so (3d) yields $$ \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i \leq \left[ M_i ( \alpha^\prime) - m_i( \alpha^\prime) \right] \Delta x_i,  \tag{3e} $$ for each $i = 1, \ldots, n$ . Now upon adding together all the inequalities in (3e), we get $$ \sum_{i=1}^n  \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i \leq \sum_{i=1}^n \left[ M_i ( \alpha^\prime) - m_i( \alpha^\prime) \right] \Delta x_i,  $$ that is, $$ \sum_{i=1}^n  \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i \leq U(P, \alpha^\prime) - L(P, \alpha^\prime). \tag{3} $$ From (1) and (3) we get $$ \sum_{i=1}^n  \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i < \frac{\varepsilon}{2M}, \tag{4} $$ for any points $s_i \in \left[ x_{i-1}, x_i \right]$ , for each $i \in \{1, \ldots, n\}$ ; the points $t_i \in \left[ x_{i-1}, x_i \right]$ are as given in (2) above. Now from (2) we see that $$ \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i = \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( t_i \right) \Delta x_i, \tag{5a} $$ where the points $s_i$ and $t_i$ , for each $i \in \{ 1, \ldots, n \}$ , are as in (4) above. So \begin{align}  & \ \ \ \left\lvert \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i - \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i  \right\rvert \\  &= \left\lvert \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( t_i \right) \Delta x_i - \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i  \right\rvert \\ & \qquad \qquad \mbox{ [ by (5a) ] } \\ &= \left\lvert \sum_{i=1}^n f \left( s_i \right) \left[ \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right) \right] \Delta x_i  \right\rvert \\ &\leq \sum_{i=1}^n \left\lvert  f \left( s_i \right) \left[ \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right) \right] \Delta x_i  \right\rvert \\ &= \sum_{i=1}^n \left\lvert  f \left( s_i \right)  \right\rvert  \left\lvert  \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right)  \right\rvert \Delta x_i   \\ &\leq \sum_{i=1}^n M \left\lvert  \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right)  \right\rvert \Delta x_i   \\ & \qquad \qquad \mbox{ [ using (0) above ] } \\ &= M \sum_{i=1}^n  \left\lvert  \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right)  \right\rvert \Delta x_i   \\ &< M \frac{\varepsilon}{2M} \qquad \mbox{ [ using (4) above ] } \\ &= \frac{\varepsilon}{2}.    \end{align} Thus we have shown that $$ \left\lvert \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i - \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i  \right\rvert < \frac{\varepsilon}{2}, \tag{5} $$ for any points $s_i \in \left[ x_{i-1}, x_i \right]$ , for each $i \in \{1, \ldots, n\}$ . From (5) we can conclude that, for any points $s_i \in \left[ x_{i-1}, x_i \right]$ , for each $i \in \{  1, \ldots, n\}$ , we have $$ \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i - \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i < \frac{\varepsilon}{2}, $$ and so \begin{align} \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i &<  \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i +  \frac{\varepsilon}{2} \\ &\leq U( P, f \alpha^\prime) + \frac{\varepsilon}{2}, \tag{6a}  \end{align} for any point $s_i \in \left[ x_{i-1}, x_i \right]$ , for each $i \in \{ 1, \ldots, n \}$ . Now we show that $$ U(P, f, \alpha) = \sup \left\{ \ \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i \colon \ s_i \in \left[ x_{i-1}, x_i \right] \mbox{ for each } i = 1, \ldots, n \ \right\}. \tag{6b} $$ For each $i \in \{ 1, \ldots, n \}$ , let $$M_i(f) \colon= \sup \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\}; \tag{6c} $$ now as $ f \left( s_i \right) \leq M_i (f)$ and as $\alpha$ is a monotonically increasing function on $[a, b]$ and hence also on $\left[ x_{i-1}, x_i \right]$ , so $$ \Delta \alpha_i = \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right) \geq 0,$$ and therefore $ f \left( s_i \right) \Delta \alpha_i \leq M_i(f) \Delta \alpha_i$ for each $i \in \{1, \ldots, n \}$ , which implies that $$ \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i \leq \sum_{i=1}^n M_i(f) \Delta \alpha_i = U(P, f, \alpha). $$ Thus $U(P, f, \alpha)$ is an upper bound of the set in (6b). If $\alpha$ is constant on the interval $[a, b]$ , then, for each $i = 1, \ldots, n$ , we have $\Delta \alpha_i = 0$ and so all the sums in the set in (6b) are zero, and also $U(P, f, \alpha) = 0$ , and so  (6b) holds. So let's assume that $\alpha$ is not constant on the interval $[a, b]$ . Then there exists a sub-interval $\left[ x_{k-1}, x_k \right]$ (for some $k = 1, \ldots, n$ ) such that $$\Delta \alpha_k = \alpha\left( x_k \right) - \alpha\left( x_{k-1} \right) > 0. \tag{*}$$ As $\varepsilon > 0$ and as $\alpha$ is monotonically increasing on $[a, b]$ , so, for each $i \in \{ 1, \ldots, n \}$ , we have $$ M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1  } < M_i(f),$$ and thus the real number $ M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1 } $ is not an upper bound of the set in (6c), which implies that there exists a point $p_i \in \left[ x_{i-1}, x_i \right]$ such that $$ M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1  } < f\left(p_i\right),$$ and since $\Delta \alpha_i \geq 0$ , we have $$ \left[ M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1 } \right] \Delta \alpha_i \leq  f\left(p_i\right) \Delta \alpha_i$$ for each $i = 1, \ldots, n$ ; but for $i= k$ we have $$ \left[ M_k(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1 } \right] \Delta \alpha_k <  f\left(p_k\right) \Delta \alpha_k;$$ therefore, $$ \sum_{i=1}^n \left[ M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1 } \right] \Delta \alpha_i < \sum_{i=1}^n  f\left(p_i\right) \Delta \alpha_i,$$ that is, $$ \sum_{i=1}^n M_i(f) \Delta \alpha_i - \frac{\varepsilon }{\alpha(b) - \alpha(a) + 1 } \left[ \alpha(b) - \alpha(a) \right]   < \sum_{i=1}^n  f\left(p_i\right) \Delta \alpha_i,  $$ which is the same as $$ U(P, f, \alpha) - \frac{\varepsilon }{\alpha(b) - \alpha(a) + 1 } \left[ \alpha(b) - \alpha(a) \right] < \sum_{i=1}^n  f\left(p_i\right) \Delta \alpha_i. \tag{6d} $$ But, as $a \leq b$ and as $\alpha$ is monotonically increasing, so $$ \alpha(a) \leq \alpha(b), $$ which implies that $$ 0 \leq \alpha(b) - \alpha(a) < \alpha(b) - \alpha(a) + 1, $$ and so $$ 0 \leq \frac{ \alpha(b) - \alpha(a)  }{ \alpha(b) - \alpha(a) + 1 } <  1, $$ which then  implies that $$ 0 \leq \frac{ \varepsilon  }{ \alpha(b) - \alpha(a) +1 } \left[ \alpha(b) - \alpha(a) \right] < \varepsilon, $$ and hence $$ - \varepsilon < - \frac{ \varepsilon  }{ \alpha(b) - \alpha(a) +1 } \left[ \alpha(b) - \alpha(a) \right]  \leq 0, $$ which implies that $$ U(P, f, \alpha) - \varepsilon < U(P, f, \alpha)  - \frac{ \varepsilon  }{ \alpha(b) - \alpha(a) +1 } \left[ \alpha(b) - \alpha(a) \right] , \tag{6e} $$ Now from (6d) and (6e) we have $$ U(P, f, \alpha) -  \varepsilon < \sum_{i=1}^n  f\left(p_i\right) \Delta \alpha_i,  $$ which shows that no real number less than $U(P, f, \alpha)$ can be an upper bound of the set in (6b). Thus the real number $U(P, f, \alpha)$ is an upper bound of the set in (6b), but no real number less than $U(P, f, \alpha)$ is an upper bound of that set. Therefore (6b) holds. Now from (6a) we see that the real number $U(P, f \alpha^\prime) + \frac{\varepsilon}{2}$ is an upper bound for the set in (6b), and from (6b) we know that $U(P, f, \alpha)$ is the least upper bound of this very set, so from (6a) and  (6b) we can conclude that $$ U(P, f, \alpha) \leq U(P, f \alpha^\prime) + \frac{\varepsilon}{2}. \tag{6} $$ Again from (5) we obtain $$ \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i -  \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i <  \frac{\varepsilon}{2},$$ which implies that \begin{align}  \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i  &< \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i +  \frac{\varepsilon}{2} \\ &\leq U(P, f, \alpha) + \frac{\varepsilon}{2}, \tag{7a}  \end{align} for any point $s_i \in \left[ x_{i-1}, x_i \right]$ , for each $i = 1, \ldots, n$ . We can show that $$ U(P, f\alpha^\prime) = \sup \left\{ \ \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i \ \colon \ s_i \in \left[ x_{i-1}, x_i \right] \mbox{ for each } i = 1, \ldots, n \ \right\}. \tag{7b} $$ From (7a) and (7b) we can conclude that $$ U(P, f\alpha^\prime) \leq  U(P, f, \alpha) + \frac{\varepsilon}{2}. \tag{7} $$ Now if $Q$ be any partition of $[a, b]$ such that $Q \supset P$ , then (by Theorem 6.4 in Baby Rudin, 3rd edition) we have $$ L(P, \alpha^\prime) \leq L(Q, \alpha^\prime) \leq U(Q, \alpha^\prime) \leq U(P, \alpha^\prime), $$ which implies that $$ U(Q, \alpha^\prime) - L(Q, \alpha^\prime) \leq U(P, \alpha^\prime) - L(P, \alpha^\prime), $$ and this together with (1) implies that (1) also holds for $Q$ , and  therefore both (6) and (7) also hold for $Q$ . That is, if $Q$ is any refinement of $P$ , then $$ U(Q, f, \alpha) \leq U(Q, f \alpha^\prime) + \frac{\varepsilon}{2}, \tag{6*} $$ and $$ U( Q, f\alpha^\prime) \leq  U(Q, f, \alpha) + \frac{\varepsilon}{2}. \tag{7*} $$ Now let's suppose that $$ \overline{\int}_a^b f \,d \alpha -   \overline{\int}_a^b f(x) \alpha^\prime(x) \,d x > \varepsilon. \tag{8a} $$ As $$ \overline{\int}_a^b f(x) \alpha^\prime(x) \,dx + \frac{\varepsilon}{2} >  \overline{\int}_a^b f(x) \alpha^\prime(x) \,dx, $$ so there exists a partition $P_1$ of $[a, b]$ such that $$ U\left( P_1, f \alpha^\prime \right) <  \overline{\int}_a^b f(x) \alpha^\prime(x) dx + \frac{\varepsilon}{2}, \tag{8b} $$ which implies $$- U\left( P_1, f \alpha^\prime \right) > - \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \frac{\varepsilon}{2},$$ and so by (8a) we obtain \begin{align}  \overline{\int}_a^b f d \alpha - U \left( P_1, f \alpha^\prime \right) &> \overline{\int}_a^b f d\alpha -  \int_a^b f(x) \alpha^\prime(x) dx - \frac{\varepsilon}{2} \\ &> \varepsilon - \frac{\varepsilon}{2} \\ &= \frac{\varepsilon}{2},  \end{align} which implies that $$ \overline{\int}_a^b f d \alpha - U \left( P_1, f \alpha^\prime \right) > \frac{\varepsilon}{2}. \tag{8c} $$ Now  if $Q$ is any partition of $[a, b]$ such that $Q \supset P_1$ , then (by Theorem 6.4 in Baby Rudin) $$ U(Q, f\alpha^\prime) \leq U \left( P_1, f\alpha^\prime \right),$$ and this together with  (8b) implies that $$ U(Q, f\alpha^\prime) <  \overline{\int}_a^b f(x) \alpha^\prime(x) dx + \frac{\varepsilon}{2}, \tag{8b*} $$ from which we obtain $$ \overline{\int}_a^b f d\alpha - U(Q, f \alpha^\prime ) > \frac{\varepsilon}{2}, \tag{8c*} $$ in just the same way as we have obtained (8c) from (8b). Now let $Q$ be any partition of $[a, b]$ such that $Q \supset P$ and $Q \supset P_1$ . Then (8c*) holds for this $Q$ as well. But as $$ U(Q, f, \alpha) \geq \overline{\int}_a^b f d \alpha, $$ so (8c*) gives $$  U(Q, f, \alpha) - U \left( Q, f \alpha^\prime \right)  \geq \overline{\int}_a^b f d \alpha - U \left( Q, f \alpha^\prime \right)   > \frac{\varepsilon}{2}, $$ which implies $$  U(Q, f, \alpha) - U \left( Q, f \alpha^\prime \right)  > \frac{\varepsilon}{2}, $$ and so $$  U(Q, f, \alpha) >  U \left( Q, f \alpha^\prime \right)  + \frac{\varepsilon}{2}, $$ which contradicts (6*). Therefore (8a) cannot hold, and so $$ \overline{\int}_a^b f d\alpha - \overline{\int}_a^b f(x) \alpha^\prime(x) dx \leq \varepsilon. \tag{8} $$ Now let's suppose that $$ \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \overline{\int}_a^b f d\alpha > \varepsilon. \tag{9a}$$ As $$ \overline{\int}_a^b f d\alpha + \frac{\varepsilon}{2} > \overline{\int}_a^b f d\alpha, $$ so there exists a partition $P_2$ of $[a, b]$ such that $$ U \left( P_2, f, \alpha \right) <  \overline{\int}_a^b f d\alpha + \frac{\varepsilon}{2}, \tag{9b} $$ which implies that $$ - U \left( P_2, f, \alpha \right) > - \overline{\int}_a^b f d\alpha - \frac{\varepsilon}{2}, $$ which together with (9a) gives \begin{align} \overline{\int}_a^b f(x) \alpha^\prime(x) dx - U \left( P_2, f, \alpha \right) &> \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \int_a^b f d\alpha - \frac{\varepsilon}{2} \\ &> \varepsilon - \frac{\varepsilon}{2} \\ &= \frac{\varepsilon}{2},  \end{align} which implies that $$ \overline{\int}_a^b f(x) \alpha^\prime(x) dx - U \left( P_2, f, \alpha \right) > \frac{\varepsilon}{2}. \tag{9c} $$ Now if $Q$ is any partition of $[a, b]$ such that $Q \supset P_2$ , then (by Theorem 6.4 in Baby Rudin) $$ U(Q, f, \alpha) \leq U \left( P_2, f, \alpha \right), $$ which together with (9b) yields $$  U(Q, f, \alpha) <  \overline{\int}_a^b f d\alpha + \frac{\varepsilon}{2}, \tag{9b*} $$ and then we obtain $$ \overline{\int}_a^b f(x) \alpha^\prime(x) dx - U ( Q, f, \alpha ) > \frac{\varepsilon}{2}, \tag{9c*} $$ in just the same way as we have obtained (9c) from (9b). Now let $Q$ be any partition of $[a, b]$ such that $Q \supset P$ and $Q \supset P_2$ . Then (9c*) holds for this $Q$ . But as $$ U ( Q, f \alpha^\prime) \geq \overline{\int}_a^b f(x) \alpha^\prime(x) dx, $$ so (9c*) gives $$ U(Q, f \alpha^\prime ) - U(Q, f, \alpha) \geq \overline{\int}_a^b f(x) \alpha^\prime (x) dx - U(Q, f, \alpha) > \frac{\varepsilon}{2},$$ which implies $$  U(Q, f \alpha^\prime ) - U(Q, f, \alpha) > \frac{\varepsilon}{2},$$ and so $$  U(Q, f \alpha^\prime ) >  U(Q, f, \alpha) + \frac{\varepsilon}{2},$$ which contradicts (7*). Thus (9a) cannot hold, and so we can conclude that $$ \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \overline{\int}_a^b f d\alpha \leq \varepsilon. \tag{9}$$ Now from (8) and (9) we can conclude that $$\left\lvert  \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \overline{\int}_a^b f d\alpha \right\rvert  \leq \varepsilon $$ for any real number $\varepsilon > 0$ . Hence $$\left\lvert  \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \overline{\int}_a^b f d\alpha \right\rvert  = 0, $$ which implies that $$ \overline{\int}_a^b f d\alpha = \overline{\int}_a^b f(x) \alpha^\prime(x) dx. \tag{A}  $$ An analogous argument gives $$ \underline{\int}_a^b f d\alpha = \underline{\int}_a^b f(x)  \alpha^\prime (x) dx. \tag{B} $$ Now suppose $f \in \mathscr{R}(\alpha)$ on $[a, b]$ . Then $$ \underline{\int}_a^b f d\alpha = \overline{\int}_a^b f d \alpha, $$ and then (A) and (B) together imply $$ \underline{\int}_a^b f(x) \alpha^\prime(x) dx = \overline{\int}_a^b f(x) \alpha^\prime(x) dx, $$ showing that $f\alpha^\prime \in \mathscr{R}$ on $[a, b]$ . Conversely, suppose $f \alpha^\prime \in \mathscr{R}$ on $[a, b]$ . Then $$ \underline{\int}_a^b f(x) \alpha^\prime(x) dx  = \overline{\int}_a^b f(x) \alpha^\prime(x) dx, $$ and then (A) and (B) together imply $$ \underline{\int}_a^b f d \alpha = \overline{\int}_a^b f d \alpha , $$ showing that $f \in \mathscr{R}(\alpha)$ on $[a, b]$ . Finally, we assume that $f \in \mathscr{R}(\alpha)$ on $[a, b]$ --- which (as we have just shown) is equivalent to assuming that $f \alpha^\prime \in \mathscr{R}$ on $[a, b]$ --- and then find from (A) and (B) that $$ \int_a^b f d\alpha = \int_a^b f(x) \alpha^\prime(x) dx, $$ as required. Is my rendering of Rudin's proof correct? If not, then where have I erred? I admit that my presentation is very very, very lengthy, but it is to demonstrate my reasoning clearly and fully enough.","Here is Theorem 6.17 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Assume increases monotonically and on . Let be a bounded real function on . Then if and only if . In that case For terminology, here is the link to my post here on Math SE on Theorem 6.15 in Baby Rudin, 3rd edition. Here is Rudin's proof of Theorem 6.17: Let be given and apply Theorem 6.6 to : There is a partition of such that The mean value theorem furnishes points such that for . If , then by (28) and Theorem 6.7 (b). Put . Since it follows from (29) that In particular, for all choices of , so that The same argument leads from (30) to Thus Now note that (28) remains true if is replaced by any refinement. Hence (31) also remains true. We conclude that But is arbitrary. Hence for any bounded . The equality of the lower integrals follows from (30) in exactly the same way. The theorem follows. Here is Theorem 6.6 in Baby Rudin, 3rd edition: on if and only if for every there exists a partition such that Here is Theorem 6.7: Theorem 6. 7(a): If (13) holda for some and some , then (13) holds (with the same ) for every refinement of . Theorem 6. 7(b): If (13) holds for and if , are arbitrary points in , then Theorem 6.7(c): If and the hypotheses of (b) hold, then Now here is my understanding of Rudin's proof of Theorem 6.17: As is a bounded real function on , so there is a positive real number such that for all . Let be a given real number. As is Riemann-integrable on , so there exists a partition of such that Now as exists on , so,  For each , the function is continuous on and differentiable on , and hence  there exists a point such that that is, for each . And, for each , if , then, as also, so we conclude that and where and now (3b) implies that and upon adding (3c) to (3a), we obtain which implies that and, as , so (3d) yields for each . Now upon adding together all the inequalities in (3e), we get that is, From (1) and (3) we get for any points , for each ; the points are as given in (2) above. Now from (2) we see that where the points and , for each , are as in (4) above. So Thus we have shown that for any points , for each . From (5) we can conclude that, for any points , for each , we have and so for any point , for each . Now we show that For each , let now as and as is a monotonically increasing function on and hence also on , so and therefore for each , which implies that Thus is an upper bound of the set in (6b). If is constant on the interval , then, for each , we have and so all the sums in the set in (6b) are zero, and also , and so  (6b) holds. So let's assume that is not constant on the interval . Then there exists a sub-interval (for some ) such that As and as is monotonically increasing on , so, for each , we have and thus the real number is not an upper bound of the set in (6c), which implies that there exists a point such that and since , we have for each ; but for we have therefore, that is, which is the same as But, as and as is monotonically increasing, so which implies that and so which then  implies that and hence which implies that Now from (6d) and (6e) we have which shows that no real number less than can be an upper bound of the set in (6b). Thus the real number is an upper bound of the set in (6b), but no real number less than is an upper bound of that set. Therefore (6b) holds. Now from (6a) we see that the real number is an upper bound for the set in (6b), and from (6b) we know that is the least upper bound of this very set, so from (6a) and  (6b) we can conclude that Again from (5) we obtain which implies that for any point , for each . We can show that From (7a) and (7b) we can conclude that Now if be any partition of such that , then (by Theorem 6.4 in Baby Rudin, 3rd edition) we have which implies that and this together with (1) implies that (1) also holds for , and  therefore both (6) and (7) also hold for . That is, if is any refinement of , then and Now let's suppose that As so there exists a partition of such that which implies and so by (8a) we obtain which implies that Now  if is any partition of such that , then (by Theorem 6.4 in Baby Rudin) and this together with  (8b) implies that from which we obtain in just the same way as we have obtained (8c) from (8b). Now let be any partition of such that and . Then (8c*) holds for this as well. But as so (8c*) gives which implies and so which contradicts (6*). Therefore (8a) cannot hold, and so Now let's suppose that As so there exists a partition of such that which implies that which together with (9a) gives which implies that Now if is any partition of such that , then (by Theorem 6.4 in Baby Rudin) which together with (9b) yields and then we obtain in just the same way as we have obtained (9c) from (9b). Now let be any partition of such that and . Then (9c*) holds for this . But as so (9c*) gives which implies and so which contradicts (7*). Thus (9a) cannot hold, and so we can conclude that Now from (8) and (9) we can conclude that for any real number . Hence which implies that An analogous argument gives Now suppose on . Then and then (A) and (B) together imply showing that on . Conversely, suppose on . Then and then (A) and (B) together imply showing that on . Finally, we assume that on --- which (as we have just shown) is equivalent to assuming that on --- and then find from (A) and (B) that as required. Is my rendering of Rudin's proof correct? If not, then where have I erred? I admit that my presentation is very very, very lengthy, but it is to demonstrate my reasoning clearly and fully enough.","\alpha \alpha^\prime \in \mathscr{R} [a, b] f [a, b] f \in \mathscr{R}(\alpha) f\alpha^\prime \in \mathscr{R}  \int_a^b f \,d\alpha = \int_a^b f(x) \alpha^\prime(x)\, dx.  \varepsilon > 0 \alpha^\prime P = \left\{ \ x_0, \ldots, x_n \  \right\} [a, b]  \tag{28} U(P, \alpha^\prime) - L(P, \alpha^\prime) < \varepsilon.  t_i \in \left[ x_{i-1}, x_i \right]  \Delta \alpha_i = \alpha^\prime \left( t_i \right) \Delta x_i  i = 1, \ldots, n s_i \in \left[ x_{i-1}, x_i \right]  \tag{29}  \sum_{i=1}^n \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i < \varepsilon,  M = \sup \lvert f(x) \rvert  \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i = \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( t_i \right) \Delta x_i   \tag{30}  \left\lvert  \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i  -  \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i \right\rvert \leq M \varepsilon.    \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i  \leq U( P, f \alpha^\prime ) + M \varepsilon,  s_i \in \left[ x_{i-1}, x_i \right]  U(P, f, \alpha) \leq U(P, f \alpha^\prime) + M \varepsilon.   U(P, f \alpha^\prime) \leq U(P, f, \alpha) + M \varepsilon.   \tag{31} \left\lvert U(P, f, \alpha) - U(P, f \alpha^\prime ) \right\rvert \leq M \varepsilon.  P  \left\lvert \overline{\int}_a^b f \, d \alpha - \overline{\int}_a^b f (x) \alpha^\prime(x) \,dx  \right\rvert \leq M \varepsilon.  \varepsilon  \tag{32} \overline{\int}_a^b f \,d \alpha = \overline{\int}_a^b f (x) \alpha^\prime(x) \,dx   f f \in \mathscr{R}(\alpha) [a, b] \varepsilon > 0 P  \tag{13} U(P, f, \alpha ) - L(P, f, \alpha ) < \varepsilon.  P \varepsilon \varepsilon P P = \left\{ \ x_0, \ldots, x_n \ \right\} s_i t_i \left[ x_{i-1}, x_i \right]  \sum_{i=1}^n \left\lvert f \left( s_i \right) - f \left( t_i \right) \right\rvert \Delta \alpha_i < \varepsilon.  f \in \mathscr{R}(\alpha)  \left\lvert \sum_{i=1}^n f \left( t_i \right) \Delta \alpha_i - \int_a^b f\, d \alpha \right\rvert < \varepsilon.  f [a, b] \varepsilon > 0  \lvert f(x) \rvert < M \tag{0}  x \in [a, b] \varepsilon > 0 \alpha^\prime [a, b] P = \left\{ \ x_0, \ldots, x_n \ \right\} [a, b]  U(P, \alpha^\prime) - L(P, \alpha^\prime) < \frac{\varepsilon}{2M}. \tag{1}  \alpha^\prime [a, b] i \in \{ 1, \ldots, n \} \alpha \left[ x_{i-1}, x_i \right] \left( x_{i-1}, x_i \right) t_i \in \left( x_{i-1}, x_i \right)   \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right) = \alpha^\prime \left( t_i \right) \left( x_i - x_{i-1} \right);   \Delta \alpha_i = \alpha^\prime \left( t_i \right) \Delta x_i \tag{2}  i = 1, \ldots, n i \in \{1, \ldots, n \} s_i \in \left[ x_{i-1}, x_i \right] t_i \in \left[ x_{i-1}, x_i \right] \tag{3a} m_i ( \alpha^\prime )  \leq \alpha^\prime \left( s_i \right) \leq M_i (\alpha^\prime),  \tag{3b}  m_i (\alpha^\prime) \leq \alpha^\prime \left( t_i \right) \leq M_i ( \alpha^\prime) ,    m_i ( \alpha^\prime ) \colon=  \inf \left\{ \ \alpha^\prime (x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\},   M_i ( \alpha^\prime) \colon= \sup  \left\{ \ \alpha^\prime (x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\};  \tag{3c} - M_i ( \alpha^\prime ) \leq - \alpha^\prime \left( t_i \right) \leq - m_i (\alpha^\prime),    m_i ( \alpha^\prime ) - M_i( \alpha^\prime) \leq \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \leq M_i ( \alpha^\prime) - m_i( \alpha^\prime),     \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \leq M_i ( \alpha^\prime) - m_i( \alpha^\prime),  \tag{3d}  \Delta x_i = x_i - x_{i-1} \geq 0  \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i \leq \left[ M_i ( \alpha^\prime) - m_i( \alpha^\prime) \right] \Delta x_i,  \tag{3e}  i = 1, \ldots, n  \sum_{i=1}^n  \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i \leq \sum_{i=1}^n \left[ M_i ( \alpha^\prime) - m_i( \alpha^\prime) \right] \Delta x_i,    \sum_{i=1}^n  \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i \leq U(P, \alpha^\prime) - L(P, \alpha^\prime). \tag{3}   \sum_{i=1}^n  \left\lvert \alpha^\prime \left( s_i \right) - \alpha^\prime \left( t_i \right) \right\rvert \Delta x_i < \frac{\varepsilon}{2M}, \tag{4}  s_i \in \left[ x_{i-1}, x_i \right] i \in \{1, \ldots, n\} t_i \in \left[ x_{i-1}, x_i \right]  \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i = \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( t_i \right) \Delta x_i, \tag{5a}  s_i t_i i \in \{ 1, \ldots, n \} \begin{align} 
& \ \ \ \left\lvert \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i - \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i  \right\rvert \\ 
&= \left\lvert \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( t_i \right) \Delta x_i - \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i  \right\rvert \\
& \qquad \qquad \mbox{ [ by (5a) ] } \\
&= \left\lvert \sum_{i=1}^n f \left( s_i \right) \left[ \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right) \right] \Delta x_i  \right\rvert \\
&\leq \sum_{i=1}^n \left\lvert  f \left( s_i \right) \left[ \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right) \right] \Delta x_i  \right\rvert \\
&= \sum_{i=1}^n \left\lvert  f \left( s_i \right)  \right\rvert  \left\lvert  \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right)  \right\rvert \Delta x_i   \\
&\leq \sum_{i=1}^n M \left\lvert  \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right)  \right\rvert \Delta x_i   \\
& \qquad \qquad \mbox{ [ using (0) above ] } \\
&= M \sum_{i=1}^n  \left\lvert  \alpha^\prime \left( t_i \right)  - \alpha^\prime \left( s_i \right)  \right\rvert \Delta x_i   \\
&< M \frac{\varepsilon}{2M} \qquad \mbox{ [ using (4) above ] } \\
&= \frac{\varepsilon}{2}.   
\end{align}  \left\lvert \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i - \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i  \right\rvert < \frac{\varepsilon}{2}, \tag{5}  s_i \in \left[ x_{i-1}, x_i \right] i \in \{1, \ldots, n\} s_i \in \left[ x_{i-1}, x_i \right] i \in \{  1, \ldots, n\}  \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i - \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i < \frac{\varepsilon}{2},  \begin{align}
\sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i &<  \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i +  \frac{\varepsilon}{2} \\
&\leq U( P, f \alpha^\prime) + \frac{\varepsilon}{2}, \tag{6a} 
\end{align} s_i \in \left[ x_{i-1}, x_i \right] i \in \{ 1, \ldots, n \}  U(P, f, \alpha) = \sup \left\{ \ \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i \colon \ s_i \in \left[ x_{i-1}, x_i \right] \mbox{ for each } i = 1, \ldots, n \ \right\}. \tag{6b}  i \in \{ 1, \ldots, n \} M_i(f) \colon= \sup \left\{ \ f(x) \ \colon \ x_{i-1} \leq x \leq x_i \ \right\}; \tag{6c}   f \left( s_i \right) \leq M_i (f) \alpha [a, b] \left[ x_{i-1}, x_i \right]  \Delta \alpha_i = \alpha \left( x_i \right) - \alpha \left( x_{i-1} \right) \geq 0,  f \left( s_i \right) \Delta \alpha_i \leq M_i(f) \Delta \alpha_i i \in \{1, \ldots, n \}  \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i \leq \sum_{i=1}^n M_i(f) \Delta \alpha_i = U(P, f, \alpha).  U(P, f, \alpha) \alpha [a, b] i = 1, \ldots, n \Delta \alpha_i = 0 U(P, f, \alpha) = 0 \alpha [a, b] \left[ x_{k-1}, x_k \right] k = 1, \ldots, n \Delta \alpha_k = \alpha\left( x_k \right) - \alpha\left( x_{k-1} \right) > 0. \tag{*} \varepsilon > 0 \alpha [a, b] i \in \{ 1, \ldots, n \}  M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1  } < M_i(f),  M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1 }  p_i \in \left[ x_{i-1}, x_i \right]  M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1  } < f\left(p_i\right), \Delta \alpha_i \geq 0  \left[ M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1 } \right] \Delta \alpha_i \leq  f\left(p_i\right) \Delta \alpha_i i = 1, \ldots, n i= k  \left[ M_k(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1 } \right] \Delta \alpha_k <  f\left(p_k\right) \Delta \alpha_k;  \sum_{i=1}^n \left[ M_i(f) - \frac{\varepsilon}{ \alpha(b) - \alpha(a) + 1 } \right] \Delta \alpha_i < \sum_{i=1}^n  f\left(p_i\right) \Delta \alpha_i,  \sum_{i=1}^n M_i(f) \Delta \alpha_i - \frac{\varepsilon }{\alpha(b) - \alpha(a) + 1 } \left[ \alpha(b) - \alpha(a) \right]   < \sum_{i=1}^n  f\left(p_i\right) \Delta \alpha_i,    U(P, f, \alpha) - \frac{\varepsilon }{\alpha(b) - \alpha(a) + 1 } \left[ \alpha(b) - \alpha(a) \right] < \sum_{i=1}^n  f\left(p_i\right) \Delta \alpha_i.
\tag{6d}  a \leq b \alpha  \alpha(a) \leq \alpha(b),   0 \leq \alpha(b) - \alpha(a) < \alpha(b) - \alpha(a) + 1,   0 \leq \frac{ \alpha(b) - \alpha(a)  }{ \alpha(b) - \alpha(a) + 1 } <  1,   0 \leq \frac{ \varepsilon  }{ \alpha(b) - \alpha(a) +1 } \left[ \alpha(b) - \alpha(a) \right] < \varepsilon,   - \varepsilon < - \frac{ \varepsilon  }{ \alpha(b) - \alpha(a) +1 } \left[ \alpha(b) - \alpha(a) \right]  \leq 0,   U(P, f, \alpha) - \varepsilon < U(P, f, \alpha)  - \frac{ \varepsilon  }{ \alpha(b) - \alpha(a) +1 } \left[ \alpha(b) - \alpha(a) \right] , \tag{6e}   U(P, f, \alpha) -  \varepsilon < \sum_{i=1}^n  f\left(p_i\right) \Delta \alpha_i,   U(P, f, \alpha) U(P, f, \alpha) U(P, f, \alpha) U(P, f \alpha^\prime) + \frac{\varepsilon}{2} U(P, f, \alpha)  U(P, f, \alpha) \leq U(P, f \alpha^\prime) + \frac{\varepsilon}{2}. \tag{6}   \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i -  \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i <  \frac{\varepsilon}{2}, \begin{align} 
\sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i 
&< \sum_{i=1}^n f \left( s_i \right) \Delta \alpha_i +  \frac{\varepsilon}{2} \\
&\leq U(P, f, \alpha) + \frac{\varepsilon}{2}, \tag{7a} 
\end{align} s_i \in \left[ x_{i-1}, x_i \right] i = 1, \ldots, n  U(P, f\alpha^\prime) = \sup \left\{ \ \sum_{i=1}^n f \left( s_i \right) \alpha^\prime \left( s_i \right) \Delta x_i \ \colon \ s_i \in \left[ x_{i-1}, x_i \right] \mbox{ for each } i = 1, \ldots, n \ \right\}. \tag{7b}   U(P, f\alpha^\prime) \leq  U(P, f, \alpha) + \frac{\varepsilon}{2}. \tag{7}  Q [a, b] Q \supset P  L(P, \alpha^\prime) \leq L(Q, \alpha^\prime) \leq U(Q, \alpha^\prime) \leq U(P, \alpha^\prime),   U(Q, \alpha^\prime) - L(Q, \alpha^\prime) \leq U(P, \alpha^\prime) - L(P, \alpha^\prime),  Q Q Q P  U(Q, f, \alpha) \leq U(Q, f \alpha^\prime) + \frac{\varepsilon}{2}, \tag{6*}   U( Q, f\alpha^\prime) \leq  U(Q, f, \alpha) + \frac{\varepsilon}{2}. \tag{7*}   \overline{\int}_a^b f \,d \alpha -   \overline{\int}_a^b f(x) \alpha^\prime(x) \,d x > \varepsilon. \tag{8a}   \overline{\int}_a^b f(x) \alpha^\prime(x) \,dx + \frac{\varepsilon}{2} > 
\overline{\int}_a^b f(x) \alpha^\prime(x) \,dx,  P_1 [a, b]  U\left( P_1, f \alpha^\prime \right) <  \overline{\int}_a^b f(x) \alpha^\prime(x) dx + \frac{\varepsilon}{2}, \tag{8b}  - U\left( P_1, f \alpha^\prime \right) > - \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \frac{\varepsilon}{2}, \begin{align}
 \overline{\int}_a^b f d \alpha - U \left( P_1, f \alpha^\prime \right) &> \overline{\int}_a^b f d\alpha -
 \int_a^b f(x) \alpha^\prime(x) dx - \frac{\varepsilon}{2} \\
&> \varepsilon - \frac{\varepsilon}{2} \\
&= \frac{\varepsilon}{2}, 
\end{align}  \overline{\int}_a^b f d \alpha - U \left( P_1, f \alpha^\prime \right) > \frac{\varepsilon}{2}. \tag{8c}  Q [a, b] Q \supset P_1  U(Q, f\alpha^\prime) \leq U \left( P_1, f\alpha^\prime \right),  U(Q, f\alpha^\prime) <  \overline{\int}_a^b f(x) \alpha^\prime(x) dx + \frac{\varepsilon}{2}, \tag{8b*}   \overline{\int}_a^b f d\alpha - U(Q, f \alpha^\prime ) > \frac{\varepsilon}{2}, \tag{8c*}  Q [a, b] Q \supset P Q \supset P_1 Q  U(Q, f, \alpha) \geq \overline{\int}_a^b f d \alpha,    U(Q, f, \alpha) - U \left( Q, f \alpha^\prime \right)  \geq \overline{\int}_a^b f d \alpha - U \left( Q, f \alpha^\prime \right)  
> \frac{\varepsilon}{2},    U(Q, f, \alpha) - U \left( Q, f \alpha^\prime \right)  > \frac{\varepsilon}{2},    U(Q, f, \alpha) >  U \left( Q, f \alpha^\prime \right)  + \frac{\varepsilon}{2},   \overline{\int}_a^b f d\alpha - \overline{\int}_a^b f(x) \alpha^\prime(x) dx \leq \varepsilon. \tag{8}   \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \overline{\int}_a^b f d\alpha > \varepsilon. \tag{9a}  \overline{\int}_a^b f d\alpha + \frac{\varepsilon}{2} > \overline{\int}_a^b f d\alpha,  P_2 [a, b]  U \left( P_2, f, \alpha \right) <  \overline{\int}_a^b f d\alpha + \frac{\varepsilon}{2}, \tag{9b}   - U \left( P_2, f, \alpha \right) > - \overline{\int}_a^b f d\alpha - \frac{\varepsilon}{2},  \begin{align}
\overline{\int}_a^b f(x) \alpha^\prime(x) dx - U \left( P_2, f, \alpha \right) &> \overline{\int}_a^b f(x) \alpha^\prime(x) dx -
\int_a^b f d\alpha - \frac{\varepsilon}{2} \\
&> \varepsilon - \frac{\varepsilon}{2} \\
&= \frac{\varepsilon}{2}, 
\end{align}  \overline{\int}_a^b f(x) \alpha^\prime(x) dx - U \left( P_2, f, \alpha \right) > \frac{\varepsilon}{2}. \tag{9c}  Q [a, b] Q \supset P_2  U(Q, f, \alpha) \leq U \left( P_2, f, \alpha \right),    U(Q, f, \alpha) <  \overline{\int}_a^b f d\alpha + \frac{\varepsilon}{2}, \tag{9b*}   \overline{\int}_a^b f(x) \alpha^\prime(x) dx - U ( Q, f, \alpha ) > \frac{\varepsilon}{2}, \tag{9c*}  Q [a, b] Q \supset P Q \supset P_2 Q  U ( Q, f \alpha^\prime) \geq \overline{\int}_a^b f(x) \alpha^\prime(x) dx,   U(Q, f \alpha^\prime ) - U(Q, f, \alpha) \geq \overline{\int}_a^b f(x) \alpha^\prime (x) dx - U(Q, f, \alpha) > \frac{\varepsilon}{2},   U(Q, f \alpha^\prime ) - U(Q, f, \alpha) > \frac{\varepsilon}{2},   U(Q, f \alpha^\prime ) >  U(Q, f, \alpha) + \frac{\varepsilon}{2},  \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \overline{\int}_a^b f d\alpha \leq \varepsilon. \tag{9} \left\lvert  \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \overline{\int}_a^b f d\alpha \right\rvert 
\leq \varepsilon  \varepsilon > 0 \left\lvert  \overline{\int}_a^b f(x) \alpha^\prime(x) dx - \overline{\int}_a^b f d\alpha \right\rvert 
= 0,   \overline{\int}_a^b f d\alpha = \overline{\int}_a^b f(x) \alpha^\prime(x) dx. \tag{A}    \underline{\int}_a^b f d\alpha = \underline{\int}_a^b f(x)  \alpha^\prime (x) dx. \tag{B}  f \in \mathscr{R}(\alpha) [a, b]  \underline{\int}_a^b f d\alpha = \overline{\int}_a^b f d \alpha,   \underline{\int}_a^b f(x) \alpha^\prime(x) dx = \overline{\int}_a^b f(x) \alpha^\prime(x) dx,  f\alpha^\prime \in \mathscr{R} [a, b] f \alpha^\prime \in \mathscr{R} [a, b]  \underline{\int}_a^b f(x) \alpha^\prime(x) dx  = \overline{\int}_a^b f(x) \alpha^\prime(x) dx,   \underline{\int}_a^b f d \alpha = \overline{\int}_a^b f d \alpha ,  f \in \mathscr{R}(\alpha) [a, b] f \in \mathscr{R}(\alpha) [a, b] f \alpha^\prime \in \mathscr{R} [a, b]  \int_a^b f d\alpha = \int_a^b f(x) \alpha^\prime(x) dx, ","['real-analysis', 'integration', 'analysis', 'definite-integrals', 'solution-verification']"
38,Choice of $q$ in Baby Rudin's Example 1.1,Choice of  in Baby Rudin's Example 1.1,q,"First, my apologies if this has already been asked/answered.  I wasn't able to find this question via search. My question comes from Rudin's ""Principles of Mathematical Analysis,"" or ""Baby Rudin,"" Ch 1, Example 1.1 on p. 2.  In the second version of the proof, showing that sets A and B do not have greatest or lowest elements respectively, he presents a seemingly arbitrary assignment of a number $q$ that satisfies equations (3) and (4), plus other conditions needed to show that $q$ is the right number for the proof.  As an exercise, I tried to derive his choice of $q$ so that I may learn more about the problem. If we write equations (3) as $q = p - (p^2 - 2)x$ , we can write (4) as $$ q^2 - 2 = (p^2 - 2)[1 - 2px + (p^2 - 2)x^2]. $$ Here, we need a rational $x > 0$ , chosen such that the expression in $[...]$ is positive.  Using the quadratic formula and the sign of $(p^2 - 2)$ , it can be shown that we need $$ x \in \left(0, \frac{1}{p + \sqrt{2}}\right) \mbox{ for } p \in A, $$ or, for $p \in B$ , $x < 1/\left(p + \sqrt{2}\right)$ or $x > 1/\left(p - \sqrt{2}\right)$ . Notice that there are MANY solutions to these equations! The easiest to see, perhaps, is letting $x = 1/(p + n)$ for $n \geq 2$ .  Notice that Rudin chooses $n = 2$ for his answer, but it checks out easily for other $n$ . The Question: Why does Rudin choose $x = 1/(p + 2)$ specifically?  Is it just to make the expressions work out clearly algebraically?  Why doesn't he comment on his particular choice or the nature of the set of solutions that will work for the proof?  Is there a simpler derivation for the number $q$ that I am missing?","First, my apologies if this has already been asked/answered.  I wasn't able to find this question via search. My question comes from Rudin's ""Principles of Mathematical Analysis,"" or ""Baby Rudin,"" Ch 1, Example 1.1 on p. 2.  In the second version of the proof, showing that sets A and B do not have greatest or lowest elements respectively, he presents a seemingly arbitrary assignment of a number that satisfies equations (3) and (4), plus other conditions needed to show that is the right number for the proof.  As an exercise, I tried to derive his choice of so that I may learn more about the problem. If we write equations (3) as , we can write (4) as Here, we need a rational , chosen such that the expression in is positive.  Using the quadratic formula and the sign of , it can be shown that we need or, for , or . Notice that there are MANY solutions to these equations! The easiest to see, perhaps, is letting for .  Notice that Rudin chooses for his answer, but it checks out easily for other . The Question: Why does Rudin choose specifically?  Is it just to make the expressions work out clearly algebraically?  Why doesn't he comment on his particular choice or the nature of the set of solutions that will work for the proof?  Is there a simpler derivation for the number that I am missing?","q q q q = p - (p^2 - 2)x 
q^2 - 2 = (p^2 - 2)[1 - 2px + (p^2 - 2)x^2].
 x > 0 [...] (p^2 - 2) 
x \in \left(0, \frac{1}{p + \sqrt{2}}\right) \mbox{ for } p \in A,
 p \in B x < 1/\left(p + \sqrt{2}\right) x > 1/\left(p - \sqrt{2}\right) x = 1/(p + n) n \geq 2 n = 2 n x = 1/(p + 2) q","['real-analysis', 'analysis', 'self-learning']"
39,How many times do you have to use L'Hôpital's rule?,How many times do you have to use L'Hôpital's rule?,,"I'm looking for cases like $$\lim_{x \to 0} \frac {1-\cos(x)}{x^2}$$ that will not give you the answer the first time you use L'Hôpital's rule on them. For example in this case it will result in a number $\frac{1}{2}$ the second time you use L'Hôpital's rule. I want examples of limits like  $\lim_{x \to c} \frac {f(x)}{g(x)}$ so that you have to use L'Hôpital's rule $5$ times, $18$  times, or say $n$ times on them to get an answer. Another question is about the case in which you use L'Hôpital's rule as many times as you want but you always end with $\lim_{x \to 0} \frac {0}{0}$. Does this case exist?","I'm looking for cases like $$\lim_{x \to 0} \frac {1-\cos(x)}{x^2}$$ that will not give you the answer the first time you use L'Hôpital's rule on them. For example in this case it will result in a number $\frac{1}{2}$ the second time you use L'Hôpital's rule. I want examples of limits like  $\lim_{x \to c} \frac {f(x)}{g(x)}$ so that you have to use L'Hôpital's rule $5$ times, $18$  times, or say $n$ times on them to get an answer. Another question is about the case in which you use L'Hôpital's rule as many times as you want but you always end with $\lim_{x \to 0} \frac {0}{0}$. Does this case exist?",,"['real-analysis', 'calculus', 'limits', 'derivatives', 'examples-counterexamples']"
40,Showing $ \sum_{n=0}^{\infty} \frac{1}{(3n+1)(3n+2)}=\frac{\pi}{3\sqrt{3}}$,Showing, \sum_{n=0}^{\infty} \frac{1}{(3n+1)(3n+2)}=\frac{\pi}{3\sqrt{3}},I would like to show that: $$ \sum_{n=0}^{\infty} \frac{1}{(3n+1)(3n+2)}=\frac{\pi}{3\sqrt{3}} $$ We have: $$ \sum_{n=0}^{\infty} \frac{1}{(3n+1)(3n+2)}=\sum_{n=0}^{\infty} \frac{1}{3n+1}-\frac{1}{3n+2} $$ I wanted to use the fact that $$\arctan(\sqrt{3})=\frac{\pi}{3} $$ but $\arctan(x)$ can only be written as a power series when $ -1\leq x \leq1$...,I would like to show that: $$ \sum_{n=0}^{\infty} \frac{1}{(3n+1)(3n+2)}=\frac{\pi}{3\sqrt{3}} $$ We have: $$ \sum_{n=0}^{\infty} \frac{1}{(3n+1)(3n+2)}=\sum_{n=0}^{\infty} \frac{1}{3n+1}-\frac{1}{3n+2} $$ I wanted to use the fact that $$\arctan(\sqrt{3})=\frac{\pi}{3} $$ but $\arctan(x)$ can only be written as a power series when $ -1\leq x \leq1$...,,"['real-analysis', 'sequences-and-series']"
41,Show $f$ is constant if $|f(x)-f(y)|\leq (x-y)^2$.,Show  is constant if .,f |f(x)-f(y)|\leq (x-y)^2,"Problem : Let $f$ be defined for all real $x$, and suppose that $$|f(x)-f(y)|\le (x-y)^2$$ for all real $x$ and $y$. Prove $f$ is constant. Source : W. Rudin, Principles of Mathematical Analysis , Chapter 5, exercise 1.","Problem : Let $f$ be defined for all real $x$, and suppose that $$|f(x)-f(y)|\le (x-y)^2$$ for all real $x$ and $y$. Prove $f$ is constant. Source : W. Rudin, Principles of Mathematical Analysis , Chapter 5, exercise 1.",,"['real-analysis', 'continuity']"
42,Find a function $f: \mathbb{R} \to \mathbb{R}$ that is continuous at precisely one point?,Find a function  that is continuous at precisely one point?,f: \mathbb{R} \to \mathbb{R},Is the function $f(x) = \lim_{k \to +\infty} \tan{kx}$ a right example? But I do not know whether this is a function at first. Is this a function? Could you give a correct real-valued function that is continuous at precisely one point? Thanks.,Is the function $f(x) = \lim_{k \to +\infty} \tan{kx}$ a right example? But I do not know whether this is a function at first. Is this a function? Could you give a correct real-valued function that is continuous at precisely one point? Thanks.,,"['real-analysis', 'general-topology']"
43,"How to find the integral $\int_{0}^{\infty}\exp(- (ax+b/x))\,dx$?",How to find the integral ?,"\int_{0}^{\infty}\exp(- (ax+b/x))\,dx","How do I find  $$\large\int_{0}^{\infty}e^{-\left(ax+\frac{b}{x}\right)}dx$$ where $a$ and $b$ are positive numbers? This is not a homework question. I will be quite happy if somebody can come up with a sort of bound, like an upper bound or a lower bound of integrand.","How do I find  $$\large\int_{0}^{\infty}e^{-\left(ax+\frac{b}{x}\right)}dx$$ where $a$ and $b$ are positive numbers? This is not a homework question. I will be quite happy if somebody can come up with a sort of bound, like an upper bound or a lower bound of integrand.",,"['real-analysis', 'calculus', 'definite-integrals', 'improper-integrals', 'closed-form']"
44,$f: \mathbf{R} \rightarrow \mathbf{R}$ monotone increasing $\Rightarrow$ $f$ is measurable,monotone increasing   is measurable,f: \mathbf{R} \rightarrow \mathbf{R} \Rightarrow f,"Problem. Let $f: \mathbf{R} \rightarrow \mathbf{R}$ be a monotone increasing function. Show that $f$ is measurable. Solution. We know that the set of discontinuites of any monotone increasing function $f$ is measure zero (since it is at most countable). We define a continuous function $g$ such that $g(x)=f(x)$ except the discontinuous points of $g$. Then $g(x)=f(x)$ almost everywhere. Note that any continuous function $g: \mathbf{R}\rightarrow \mathbf{R}$ is measurable. Also note that if $g$ is measurable and $f=g$ almost everywhere, then $f$ is measurable. Hence we conclude that $f$ is measurable. Is my solution correct? Thanks.","Problem. Let $f: \mathbf{R} \rightarrow \mathbf{R}$ be a monotone increasing function. Show that $f$ is measurable. Solution. We know that the set of discontinuites of any monotone increasing function $f$ is measure zero (since it is at most countable). We define a continuous function $g$ such that $g(x)=f(x)$ except the discontinuous points of $g$. Then $g(x)=f(x)$ almost everywhere. Note that any continuous function $g: \mathbf{R}\rightarrow \mathbf{R}$ is measurable. Also note that if $g$ is measurable and $f=g$ almost everywhere, then $f$ is measurable. Hence we conclude that $f$ is measurable. Is my solution correct? Thanks.",,"['real-analysis', 'measure-theory', 'proof-writing']"
45,Showing that $ \int_{0}^{1} \frac{x-1}{\ln(x)} \mathrm dx=\ln2 $,Showing that, \int_{0}^{1} \frac{x-1}{\ln(x)} \mathrm dx=\ln2 ,I would like to show that $$ \int_{0}^{1} \frac{x-1}{\ln(x)} \mathrm dx=\ln2 $$ What annoys me is that $ x-1 $ is the numerator so the geometric power series is useless. Any idea?,I would like to show that $$ \int_{0}^{1} \frac{x-1}{\ln(x)} \mathrm dx=\ln2 $$ What annoys me is that $ x-1 $ is the numerator so the geometric power series is useless. Any idea?,,"['real-analysis', 'integration']"
46,"In $\mathbb{R}^n$, locally lipschitz on compact set implies lipschitz","In , locally lipschitz on compact set implies lipschitz",\mathbb{R}^n,"I need to prove: Let $A$ be open in $\mathbb{R}^m$ , $g:A \longrightarrow \mathbb{R}^n$ a  locally lipschitz function and $C$ a compact subset of $A$ . Show that $g$ is lipschitz on $C$ . Can anyone help me?","I need to prove: Let be open in , a  locally lipschitz function and a compact subset of . Show that is lipschitz on . Can anyone help me?",A \mathbb{R}^m g:A \longrightarrow \mathbb{R}^n C A g C,['real-analysis']
47,Is a Lipschitz function differentiable?,Is a Lipschitz function differentiable?,,"Is a Lipschitz function differentiable? I have been wondering whether or not this property applies to all functions. I do not need a formal proof, just the concept behind it. Let $f: [a,b] \to [c,d]$ be a continuous function (What is more - it is uniformly continuous!) And let's assusme that it's also Lipschitz continuous on this interval. Does this set of assumptions imply that $f$ is differentiable  on $(a,b)$?","Is a Lipschitz function differentiable? I have been wondering whether or not this property applies to all functions. I do not need a formal proof, just the concept behind it. Let $f: [a,b] \to [c,d]$ be a continuous function (What is more - it is uniformly continuous!) And let's assusme that it's also Lipschitz continuous on this interval. Does this set of assumptions imply that $f$ is differentiable  on $(a,b)$?",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'lipschitz-functions']"
48,Show rigorously that the sum of integrals of $f$ and of its inverse is $bf(b)-af(a)$,Show rigorously that the sum of integrals of  and of its inverse is,f bf(b)-af(a),"Suppose $f$ is a continuous, strictly increasing function defined on a closed interval $[a,b]$ such that $f^{-1}$ is the inverse function of $f$ . Prove that, $$\int_{a}^bf(x)dx+\int_{f(a)}^{f(b)}f^{-1}(x)dx=bf(b)-af(a)$$ A high school student or a Calculus first year student will simply, possibly, apply change of variable technique, then integration by parts and he/she will arrive at the answer without giving much thought into the process. A smarter student would probably compare the integrals with areas and conclude that the equality is immediate. However, I am an undergrad student of Analysis and I would want to solve the problem ""carefully"". That is, I wouldn't want to forget my definitions, and the conditions of each technique. For example, while applying change of variables technique, I cannot apply it blindly; I must be prudent enough to realize that the criterion to apply it includes continuous differentiability of a function. Simply with $f$ continuous, I cannot apply change of variables technique. Is there any method to solve this problem rigorously? One may apply the techniques of integration (by parts, change of variables, etc.) only after proper justification. The reason I am not adding any work of mine is simply that I could not proceed even one line since I am not given $f$ is differentiable. However, this seems to hold for non-differentiable functions also. I would really want some help. Pictorial proofs and/or area arguments are invalid.","Suppose is a continuous, strictly increasing function defined on a closed interval such that is the inverse function of . Prove that, A high school student or a Calculus first year student will simply, possibly, apply change of variable technique, then integration by parts and he/she will arrive at the answer without giving much thought into the process. A smarter student would probably compare the integrals with areas and conclude that the equality is immediate. However, I am an undergrad student of Analysis and I would want to solve the problem ""carefully"". That is, I wouldn't want to forget my definitions, and the conditions of each technique. For example, while applying change of variables technique, I cannot apply it blindly; I must be prudent enough to realize that the criterion to apply it includes continuous differentiability of a function. Simply with continuous, I cannot apply change of variables technique. Is there any method to solve this problem rigorously? One may apply the techniques of integration (by parts, change of variables, etc.) only after proper justification. The reason I am not adding any work of mine is simply that I could not proceed even one line since I am not given is differentiable. However, this seems to hold for non-differentiable functions also. I would really want some help. Pictorial proofs and/or area arguments are invalid.","f [a,b] f^{-1} f \int_{a}^bf(x)dx+\int_{f(a)}^{f(b)}f^{-1}(x)dx=bf(b)-af(a) f f","['calculus', 'real-analysis', 'integration', 'definite-integrals']"
49,"Relation between metric spaces, normed vector spaces, and inner product space.","Relation between metric spaces, normed vector spaces, and inner product space.",,"I am wondering what exactly is the relationship between the three aforementioned spaced. All of them seem to show up many times in: Linear Algebra, Topology, and Analysis. However, I feel like I'm missing the bigger picture of how these spaces relate to each other. For example, in my course in multi-dimensional analysis, we started out talking about metric spaces, but later suddenly switched to normed vector spaces, without any explicit mention of this transition. In linear algebra we usually talked about inner product spaces, and in topology we talked about metric spaces and topological spaces. The bigger picture of the relation between these three is still unclear to me. Which is used where, for what reason, and how do they relate? I do know the definitions of all three of them: A metric space is a pair $(S,d)$ with $S$ a set and $d: S \times S \to \mathbb{R}_{\geq 0}$ a metric : $d(x,x) = 0$ for all $x \in S$ and $d(x,y) >0$ for $x \neq y$, $d(x,y) = d(y,x)$, $d(x,z) \leq d(x,y) + d(y,z)$. A (real) inner product space is a pair $(V,\langle \cdot \rangle)$ where $V$ is a (real) vector space and $\langle \cdot \rangle: V \times V \to \mathbb{R}$ is an inner product : $\langle v,w \rangle = \langle w,v \rangle$, $\langle a_1 v_1 + a_2v_2,w \rangle = a_1\langle v_1,w \rangle + a_2\langle v_2,w \rangle$ for all $a_1,a_2 \in \mathbb{R}$, $v \neq 0 \Longrightarrow \langle v,v \rangle > 0$. A (real) normed vector space is a pair $(V,\|\cdot\|)$ where $V$ is a (real) vector space and $\|\cdot\|: V \to \mathbb{R}_{\geq 0}: v \mapsto \|v\|$ is a norm on $V$: $\|v\| \geq 0$ and $\|v\|  = 0 \ \Longleftrightarrow \ v = 0$. For $t \in \mathbb{R}$ and $v \in V$ we have $\|tv\| = |t|\|v\|$ $\|v+w\| \leq \|v\| + \|w\|$. I also know that an inner product gives rise to a norm by taking $\|v\| = \sqrt{\langle v,v \rangle}$, for example the Euclidean norm derives from the standard inner product on $\mathbb{R}^n$ in this way. And Cauchy-Schwarz: $|\langle x,y \rangle| \leq \|x\|\|y\|$. I'm not interested in details about the definitions but in the intuition and bigger picture of these three spaces, and how they show up in Analysis.","I am wondering what exactly is the relationship between the three aforementioned spaced. All of them seem to show up many times in: Linear Algebra, Topology, and Analysis. However, I feel like I'm missing the bigger picture of how these spaces relate to each other. For example, in my course in multi-dimensional analysis, we started out talking about metric spaces, but later suddenly switched to normed vector spaces, without any explicit mention of this transition. In linear algebra we usually talked about inner product spaces, and in topology we talked about metric spaces and topological spaces. The bigger picture of the relation between these three is still unclear to me. Which is used where, for what reason, and how do they relate? I do know the definitions of all three of them: A metric space is a pair $(S,d)$ with $S$ a set and $d: S \times S \to \mathbb{R}_{\geq 0}$ a metric : $d(x,x) = 0$ for all $x \in S$ and $d(x,y) >0$ for $x \neq y$, $d(x,y) = d(y,x)$, $d(x,z) \leq d(x,y) + d(y,z)$. A (real) inner product space is a pair $(V,\langle \cdot \rangle)$ where $V$ is a (real) vector space and $\langle \cdot \rangle: V \times V \to \mathbb{R}$ is an inner product : $\langle v,w \rangle = \langle w,v \rangle$, $\langle a_1 v_1 + a_2v_2,w \rangle = a_1\langle v_1,w \rangle + a_2\langle v_2,w \rangle$ for all $a_1,a_2 \in \mathbb{R}$, $v \neq 0 \Longrightarrow \langle v,v \rangle > 0$. A (real) normed vector space is a pair $(V,\|\cdot\|)$ where $V$ is a (real) vector space and $\|\cdot\|: V \to \mathbb{R}_{\geq 0}: v \mapsto \|v\|$ is a norm on $V$: $\|v\| \geq 0$ and $\|v\|  = 0 \ \Longleftrightarrow \ v = 0$. For $t \in \mathbb{R}$ and $v \in V$ we have $\|tv\| = |t|\|v\|$ $\|v+w\| \leq \|v\| + \|w\|$. I also know that an inner product gives rise to a norm by taking $\|v\| = \sqrt{\langle v,v \rangle}$, for example the Euclidean norm derives from the standard inner product on $\mathbb{R}^n$ in this way. And Cauchy-Schwarz: $|\langle x,y \rangle| \leq \|x\|\|y\|$. I'm not interested in details about the definitions but in the intuition and bigger picture of these three spaces, and how they show up in Analysis.",,"['real-analysis', 'vector-spaces', 'metric-spaces', 'normed-spaces', 'inner-products']"
50,A nontrivial everywhere continuous function with uncountably many roots?,A nontrivial everywhere continuous function with uncountably many roots?,,"This is my first post on SE, forgive any blunders. I am looking for an example of a function $f:\mathbb{R} \to \mathbb{R}$ which is continuous everywhere but has uncountably many roots ($x$ such that $f(x) = 0$). I am not looking for trivial examples such as $f = 0$ for all $x$. This is not a homework problem. I'd prefer a nudge in the right direction rather than an explicit example. Thanks! Edit: Thanks all! I've constructed my example with your help.","This is my first post on SE, forgive any blunders. I am looking for an example of a function $f:\mathbb{R} \to \mathbb{R}$ which is continuous everywhere but has uncountably many roots ($x$ such that $f(x) = 0$). I am not looking for trivial examples such as $f = 0$ for all $x$. This is not a homework problem. I'd prefer a nudge in the right direction rather than an explicit example. Thanks! Edit: Thanks all! I've constructed my example with your help.",,"['real-analysis', 'analysis', 'functions']"
51,Does $\lim_{h\rightarrow 0}\ [f(x+h)-f(x-h)]=0$ imply that $f$ is continuous?,Does  imply that  is continuous?,\lim_{h\rightarrow 0}\ [f(x+h)-f(x-h)]=0 f,"Suppose $f$ is a real function defined on $\mathbb{R}$ which satisfies    $$\lim_{h\rightarrow 0}\ [f(x+h)-f(x-h)]=0.$$   Does this imply that $f$ is continuous? Source : W. Rudin, Principles of Mathematical Analysis , Chapter 4, exercise 1.","Suppose $f$ is a real function defined on $\mathbb{R}$ which satisfies    $$\lim_{h\rightarrow 0}\ [f(x+h)-f(x-h)]=0.$$   Does this imply that $f$ is continuous? Source : W. Rudin, Principles of Mathematical Analysis , Chapter 4, exercise 1.",,"['real-analysis', 'continuity', 'examples-counterexamples']"
52,Why limits work,Why limits work,,"I'm currently a first year student in electrical engineering and computer science. I know how to compute limits, derivatives, integrals with respect to one variable that is things from one variable calculus (mathematics 1). In mathematics 2 we're currently working on series (convergent, divergent, integral criteria, D'Alemberts criteria, Cauchy criteria, absolute convergence ...). English is not my mother tongue, so forgive me I spell something wrong or have grammar mistakes. I'll try to explain my questions as best as I can. I have multiple questions, but they are all intertwined. Since all these things ""need"" limits, they are my main confusion. I understand the intuition behind the limit and the epsilon-delta definition, but why it works in practice. That is why can I say when computing the derivative of for example $x^2$ is $2x$? In $\lim_{\Delta x \rightarrow 0}  \frac{f(x + \Delta x) - f(x)}{\Delta x}$ I can't just put $0$ since I would get $\frac{0}{0}$, which would be the ""true"" derivative, because I don't know what that is. After some manipulation I would get $\lim_{\Delta x \rightarrow 0} 2x + \Delta x$ and since $\Delta x$ goes to $0$ that would be equal to $2x$. But this $\Delta x$ will never be $0$, at least as I look at this and from the definition of the limit it would say that I can make $\Delta x$ as close to $0$, but not equal to, if I'm willing to make $x_1$ and $x_2$ as close to each other. Why can I now take this $2x$ and say for instance that the derivative of someone's position it time is $2x$ that is its velocity is $2x$ and not $2x +$ some small $\Delta x$? When trying to see if an infinite series (which never ends) converges or diverges why can I look at a sequence of partial sums (infinite) of that series and based on their convergence or divergence say if the whole series diverges or converges? When I come to professors and ask these and such questions they tell me why am I bothering my self with such question and that I should take it for granted. Then I just want to kill my self. I mean haven't I came here to study how and why things work? I would like it more if they would just tell me that if it is some ""higher"" or more complex part of mathematics and that I will learn about it later or that it just isn't know why it works the way it works. So should I even continue to study these things, since I will always come across something that I wouldn't be able to understand (since these ""basic"" limits are confusing me) and all these professors and academia will tell me that I shouldn't worry why it works the way it works and that I should just take it for granted. All the theorems used to proof derivative, integral, convergence, divergence etc. use in one way or another limits. But in the definition of the limit it says that I can make some $f(x)$ as close to some value L, but not equal to it, as long as I'm willing to make $x$ as close to some value $c$. This definition is supposed to be mathematical rigorous, but using these as close don't ""look"" rigorous to me. Please help me since I don't know should I even continue with my studies since there is always some mathematical proof which I cannot understand and is preventing me to go forward and that way I'm always lacking behind and everybody expects to understand everything the first time I hear it. I will be grateful for all comments and suggestions.","I'm currently a first year student in electrical engineering and computer science. I know how to compute limits, derivatives, integrals with respect to one variable that is things from one variable calculus (mathematics 1). In mathematics 2 we're currently working on series (convergent, divergent, integral criteria, D'Alemberts criteria, Cauchy criteria, absolute convergence ...). English is not my mother tongue, so forgive me I spell something wrong or have grammar mistakes. I'll try to explain my questions as best as I can. I have multiple questions, but they are all intertwined. Since all these things ""need"" limits, they are my main confusion. I understand the intuition behind the limit and the epsilon-delta definition, but why it works in practice. That is why can I say when computing the derivative of for example $x^2$ is $2x$? In $\lim_{\Delta x \rightarrow 0}  \frac{f(x + \Delta x) - f(x)}{\Delta x}$ I can't just put $0$ since I would get $\frac{0}{0}$, which would be the ""true"" derivative, because I don't know what that is. After some manipulation I would get $\lim_{\Delta x \rightarrow 0} 2x + \Delta x$ and since $\Delta x$ goes to $0$ that would be equal to $2x$. But this $\Delta x$ will never be $0$, at least as I look at this and from the definition of the limit it would say that I can make $\Delta x$ as close to $0$, but not equal to, if I'm willing to make $x_1$ and $x_2$ as close to each other. Why can I now take this $2x$ and say for instance that the derivative of someone's position it time is $2x$ that is its velocity is $2x$ and not $2x +$ some small $\Delta x$? When trying to see if an infinite series (which never ends) converges or diverges why can I look at a sequence of partial sums (infinite) of that series and based on their convergence or divergence say if the whole series diverges or converges? When I come to professors and ask these and such questions they tell me why am I bothering my self with such question and that I should take it for granted. Then I just want to kill my self. I mean haven't I came here to study how and why things work? I would like it more if they would just tell me that if it is some ""higher"" or more complex part of mathematics and that I will learn about it later or that it just isn't know why it works the way it works. So should I even continue to study these things, since I will always come across something that I wouldn't be able to understand (since these ""basic"" limits are confusing me) and all these professors and academia will tell me that I shouldn't worry why it works the way it works and that I should just take it for granted. All the theorems used to proof derivative, integral, convergence, divergence etc. use in one way or another limits. But in the definition of the limit it says that I can make some $f(x)$ as close to some value L, but not equal to it, as long as I'm willing to make $x$ as close to some value $c$. This definition is supposed to be mathematical rigorous, but using these as close don't ""look"" rigorous to me. Please help me since I don't know should I even continue with my studies since there is always some mathematical proof which I cannot understand and is preventing me to go forward and that way I'm always lacking behind and everybody expects to understand everything the first time I hear it. I will be grateful for all comments and suggestions.",,"['calculus', 'real-analysis', 'limits', 'soft-question', 'education']"
53,Exponential Function as an Infinite Product,Exponential Function as an Infinite Product,,"Is there any representation of the exponential function as an infinite product (where there is no maximal factor in the series of terms which essentially contributes)? I.e. $$\mathrm e^x=\prod_{n=0}^\infty a_n,$$ and by the sentence in brackets I mean that the $a_n$'s are not just mostly equal to $1$ or pairwise canceling away. The product is infinite but its factors don't contain a subseqeunce of $1$, if that makes sense. There is of course the limit definition as powers of $(1+x/n)$., but these are no definite $a_n$'s, which one could e.g. divide out.","Is there any representation of the exponential function as an infinite product (where there is no maximal factor in the series of terms which essentially contributes)? I.e. $$\mathrm e^x=\prod_{n=0}^\infty a_n,$$ and by the sentence in brackets I mean that the $a_n$'s are not just mostly equal to $1$ or pairwise canceling away. The product is infinite but its factors don't contain a subseqeunce of $1$, if that makes sense. There is of course the limit definition as powers of $(1+x/n)$., but these are no definite $a_n$'s, which one could e.g. divide out.",,"['calculus', 'real-analysis', 'limits', 'functions', 'exponential-function']"
54,"About the integral $\int_{-1}^1 \frac{1}{\pi^2+(2 \operatorname{arctanh}(x))^2} \, dx=\frac{1}{6} $",About the integral,"\int_{-1}^1 \frac{1}{\pi^2+(2 \operatorname{arctanh}(x))^2} \, dx=\frac{1}{6} ","Here is a question that naturally arose in the study of some specific integrals. I'm curious if for such integrals are known nice real analysis tools for calculating them ( including here all possible sources in literature that are publicaly available ).  At some point I'll add my real analysis solution. It's a question for the informative purpose rather than finding solutions, the solution is optional. Prove that $$\int_{-1}^1 \frac{1}{\pi^2+(2 \operatorname{arctanh}(x))^2} \, dx=\frac{1}{6}. $$ Here is a supplementary question $$\int_{-1}^1 \frac{\log(1-x)}{\pi^2+(2 \operatorname{arctanh}(x))^2} \, dx=\frac{1}{4}+\frac{\gamma }{6}+\frac{\log (2)}{6}-2 \log (A) $$ where $A$ is Glaisher–Kinkelin constant . for the passionates of integrals, series and limits.","Here is a question that naturally arose in the study of some specific integrals. I'm curious if for such integrals are known nice real analysis tools for calculating them ( including here all possible sources in literature that are publicaly available ).  At some point I'll add my real analysis solution. It's a question for the informative purpose rather than finding solutions, the solution is optional. Prove that $$\int_{-1}^1 \frac{1}{\pi^2+(2 \operatorname{arctanh}(x))^2} \, dx=\frac{1}{6}. $$ Here is a supplementary question $$\int_{-1}^1 \frac{\log(1-x)}{\pi^2+(2 \operatorname{arctanh}(x))^2} \, dx=\frac{1}{4}+\frac{\gamma }{6}+\frac{\log (2)}{6}-2 \log (A) $$ where $A$ is Glaisher–Kinkelin constant . for the passionates of integrals, series and limits.",,"['calculus', 'real-analysis', 'integration', 'reference-request', 'definite-integrals']"
55,Assume I'm unhappy with the Riemann Rearrangement Theorem. How else can I define a series?,Assume I'm unhappy with the Riemann Rearrangement Theorem. How else can I define a series?,,"I have been explaining the Riemann Rearrangement Theorem to a friend of mine, and they feel as though the definition of series using partial sums ""doesn't work"" for conditionally convergent sequences. I understand how they feel: the RRT feels counter-intuitive, and so the result should be denied as a contradiction and the partial-sum definition rejected. However, the definition of a series as $$\sum_{k=1}^{\infty} a_k = \lim_{n\to\infty} \left(\sum_{k=1}^n a_k\right)$$ feels like the most natural approach to take. If I wanted to add infinitely many numbers together by hand, this is how I would have to do it. Is there an alternative (but not equivalent) definition of an infinite sum that agrees with the partial-sum definition on absolutely convergent sequences, but where the RRT doesn't hold? My guess is that if you define some value for an infinite series agreeing on absolutely convergent sequences then this method of definition must imply the RRT, but I don't see how that proof could go.","I have been explaining the Riemann Rearrangement Theorem to a friend of mine, and they feel as though the definition of series using partial sums ""doesn't work"" for conditionally convergent sequences. I understand how they feel: the RRT feels counter-intuitive, and so the result should be denied as a contradiction and the partial-sum definition rejected. However, the definition of a series as feels like the most natural approach to take. If I wanted to add infinitely many numbers together by hand, this is how I would have to do it. Is there an alternative (but not equivalent) definition of an infinite sum that agrees with the partial-sum definition on absolutely convergent sequences, but where the RRT doesn't hold? My guess is that if you define some value for an infinite series agreeing on absolutely convergent sequences then this method of definition must imply the RRT, but I don't see how that proof could go.",\sum_{k=1}^{\infty} a_k = \lim_{n\to\infty} \left(\sum_{k=1}^n a_k\right),"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'definition']"
56,Interpreting the significance of Darboux's Theorem,Interpreting the significance of Darboux's Theorem,,"Suppose that $f$ is differentiable on $[a, b]$. Prove that if $f'(a) < c < f'(b)$, then $f'(x) = c$ for some $x$ in $(a,b)$. (This result is known as Darboux's theorem .) Source: Spivak's Calculus Ch.11 - Significance of the Derivative. I have a hard time picturing this or interpreting it... What are the implications of this? Similarly, for another theorem introduced in the same chapter, I am having the problem of seeing the significance of this... Suppose that $f$ is continuous at $a$, and that $f'(x)$ exists for all $x$ in some interval containing $a$, except perhaps for $x = a$. Suppose, moreover, that $\lim \limits_{x \to a} f'(x)$ exists. Then $f'(a)$ also exists, and $f'(a) = \lim \limits_{x \to a} f'(x)$. Doesn't this mean that $f'$ is just continuous at point $a$?","Suppose that $f$ is differentiable on $[a, b]$. Prove that if $f'(a) < c < f'(b)$, then $f'(x) = c$ for some $x$ in $(a,b)$. (This result is known as Darboux's theorem .) Source: Spivak's Calculus Ch.11 - Significance of the Derivative. I have a hard time picturing this or interpreting it... What are the implications of this? Similarly, for another theorem introduced in the same chapter, I am having the problem of seeing the significance of this... Suppose that $f$ is continuous at $a$, and that $f'(x)$ exists for all $x$ in some interval containing $a$, except perhaps for $x = a$. Suppose, moreover, that $\lim \limits_{x \to a} f'(x)$ exists. Then $f'(a)$ also exists, and $f'(a) = \lim \limits_{x \to a} f'(x)$. Doesn't this mean that $f'$ is just continuous at point $a$?",,"['calculus', 'real-analysis']"
57,"If $(a_n)$ is a decreasing sequence of strictly positive numbers and if $\sum{a_n}$ is convergent, show that $\lim{na_n}=0$ [duplicate]","If  is a decreasing sequence of strictly positive numbers and if  is convergent, show that  [duplicate]",(a_n) \sum{a_n} \lim{na_n}=0,"This question already has answers here : If $(a_n)\subset[0,\infty)$ is non-increasing and $\sum_{n=1}^\infty a_n<\infty$, then $\lim\limits_{n\to\infty}{n a_n} = 0$ (16 answers) Closed 8 years ago . If $(a_n)$ is a decreasing sequence of strictly positive numbers and if $\sum{a_n}$ is convergent, show that $\lim{na_n}=0$ Since $(a_n)$ is decreasing and bounded below, by Monotonic Convergence Theorem, $(a_n)$ converges. So, there exists $N$ such that $|a_n-L|< \epsilon$ Since $\sum{a_n}$ is convergent, by the Divergence test, we have $\lim_n{a_n}=0$, which means there exists $N$ such that $|a_n-0|< \epsilon$ Then I get stuck at here. I try to figure out the statement's meaning by inserting some examples, like $a_n=\dfrac{1}{n^2}$ Can anyone guide me on this question?","This question already has answers here : If $(a_n)\subset[0,\infty)$ is non-increasing and $\sum_{n=1}^\infty a_n<\infty$, then $\lim\limits_{n\to\infty}{n a_n} = 0$ (16 answers) Closed 8 years ago . If $(a_n)$ is a decreasing sequence of strictly positive numbers and if $\sum{a_n}$ is convergent, show that $\lim{na_n}=0$ Since $(a_n)$ is decreasing and bounded below, by Monotonic Convergence Theorem, $(a_n)$ converges. So, there exists $N$ such that $|a_n-L|< \epsilon$ Since $\sum{a_n}$ is convergent, by the Divergence test, we have $\lim_n{a_n}=0$, which means there exists $N$ such that $|a_n-0|< \epsilon$ Then I get stuck at here. I try to figure out the statement's meaning by inserting some examples, like $a_n=\dfrac{1}{n^2}$ Can anyone guide me on this question?",,['real-analysis']
58,"Does a smooth ""transition function"" with bounded derivatives exist?","Does a smooth ""transition function"" with bounded derivatives exist?",,"Does there exist a function $f: \mathbb{R} \to \mathbb{R}$ having the following properties? $f(x) = 0$ for all $x \le 0$. $f(x) = 1$ for all $x \ge 1$. For $0 < x < 1$, $f$ is strictly increasing. $f$ is everywhere $C^\infty$. The sequence of $L^\infty$ norms $\langle \left\lVert f \right\rVert_\infty, \left\lVert f' \right\rVert_\infty, \left\lVert f'' \right\rVert_\infty, \dots \rangle$ is bounded. If we impose only the first four conditions, there is a well-known answer : for $0 < x < 1$, define $f(x)$ by $$ f(x) = \frac{e^{-1/x}}{e^{-1/x} + e^{-1/(1-x)}} = \frac{1}{1 + e^{1/x - 1/(1-x)}} $$ However, the derivatives of this function appear to grow quite rapidly. (I'm not sure how to verify this, but it seems at least exponential to me.) If such a function does not exist, what is the smallest order of asymptotic growth that the sequence $\langle \left\lVert f \right\rVert_\infty, \left\lVert f' \right\rVert_\infty, \left\lVert f'' \right\rVert_\infty, \dots \rangle$ can have?","Does there exist a function $f: \mathbb{R} \to \mathbb{R}$ having the following properties? $f(x) = 0$ for all $x \le 0$. $f(x) = 1$ for all $x \ge 1$. For $0 < x < 1$, $f$ is strictly increasing. $f$ is everywhere $C^\infty$. The sequence of $L^\infty$ norms $\langle \left\lVert f \right\rVert_\infty, \left\lVert f' \right\rVert_\infty, \left\lVert f'' \right\rVert_\infty, \dots \rangle$ is bounded. If we impose only the first four conditions, there is a well-known answer : for $0 < x < 1$, define $f(x)$ by $$ f(x) = \frac{e^{-1/x}}{e^{-1/x} + e^{-1/(1-x)}} = \frac{1}{1 + e^{1/x - 1/(1-x)}} $$ However, the derivatives of this function appear to grow quite rapidly. (I'm not sure how to verify this, but it seems at least exponential to me.) If such a function does not exist, what is the smallest order of asymptotic growth that the sequence $\langle \left\lVert f \right\rVert_\infty, \left\lVert f' \right\rVert_\infty, \left\lVert f'' \right\rVert_\infty, \dots \rangle$ can have?",,"['calculus', 'real-analysis', 'functions', 'derivatives']"
59,A closed ball in a metric space is a closed set,A closed ball in a metric space is a closed set,,"Prove that a closed ball in a metric space is a closed set My attempt: Suppose $D(x_0, r)$ is a closed ball. We show that $X \setminus D $ is open. In other words, we need to find an open ball contained in $X \setminus D$. Pick $$t \in X-D \implies d(t,x_0) > r \implies d(t,x_0) - r > 0 $$ Let $B(y, r_1)$ be an open ball, and pick $z \in B(y,r_1)$. Then, we must have $d(y,z) < r_1 $. We need to choose $r_1$ so that $d(z,x_0) > r$. Notice by the triangle inequality $$ d(x_0,t) \leq d(x_0,z) + d(z,t) \implies d(z,x_0) \geq d(x_0,t) - d(z,t) > d(x_0,t) - r_1.$$ Notice, if we pick $r_1 = d(t,x_0)-r$ then we are done. Is this correct?","Prove that a closed ball in a metric space is a closed set My attempt: Suppose $D(x_0, r)$ is a closed ball. We show that $X \setminus D $ is open. In other words, we need to find an open ball contained in $X \setminus D$. Pick $$t \in X-D \implies d(t,x_0) > r \implies d(t,x_0) - r > 0 $$ Let $B(y, r_1)$ be an open ball, and pick $z \in B(y,r_1)$. Then, we must have $d(y,z) < r_1 $. We need to choose $r_1$ so that $d(z,x_0) > r$. Notice by the triangle inequality $$ d(x_0,t) \leq d(x_0,z) + d(z,t) \implies d(z,x_0) \geq d(x_0,t) - d(z,t) > d(x_0,t) - r_1.$$ Notice, if we pick $r_1 = d(t,x_0)-r$ then we are done. Is this correct?",,['real-analysis']
60,What on earth is the difference between Calculus and Analysis [duplicate],What on earth is the difference between Calculus and Analysis [duplicate],,"This question already has answers here : Are calculus and real analysis the same thing? (8 answers) Closed 11 years ago . I noticed that there isn't a word for Calculus in my native language, Dutch. So I just went to the English wikipedia entry on Calculus, and tried searching for the Dutch article, and as I suspected, it is non-existent. So I tried the opposite, and went to the Dutch wikipedia entry on calculus. There was a disambiguation page , I clicked on the page concerning mathematics (which exists, to my surprise), and I was redirected to a page named 'Analyse'. When I clicked on the English version, I didn't get Calculus again, but I got mathematical analysis . I already noticed before that these 2 are closely related, when I was shopping for a good (english) calculus textbook. The only thing I found in my research is that Calculus evolved from Analysis, i.e. that Calculus is more basic. But this to me is hard to believe. Some fields of calculus, such are PDEs, are still being developed, so I guess this doesn't perfectly outline the difference between the two. What is the difference?","This question already has answers here : Are calculus and real analysis the same thing? (8 answers) Closed 11 years ago . I noticed that there isn't a word for Calculus in my native language, Dutch. So I just went to the English wikipedia entry on Calculus, and tried searching for the Dutch article, and as I suspected, it is non-existent. So I tried the opposite, and went to the Dutch wikipedia entry on calculus. There was a disambiguation page , I clicked on the page concerning mathematics (which exists, to my surprise), and I was redirected to a page named 'Analyse'. When I clicked on the English version, I didn't get Calculus again, but I got mathematical analysis . I already noticed before that these 2 are closely related, when I was shopping for a good (english) calculus textbook. The only thing I found in my research is that Calculus evolved from Analysis, i.e. that Calculus is more basic. But this to me is hard to believe. Some fields of calculus, such are PDEs, are still being developed, so I guess this doesn't perfectly outline the difference between the two. What is the difference?",,"['calculus', 'real-analysis', 'soft-question']"
61,"Find examples of two series $\sum a_n$ and $\sum b_n$ both of which diverge but for which $\sum \min(a_n, b_n)$ converges",Find examples of two series  and  both of which diverge but for which  converges,"\sum a_n \sum b_n \sum \min(a_n, b_n)","Find examples of two series $\sum a_n$ and $\sum b_n$ both of which diverge but for which $\sum \min(a_n, b_n)$ converges. To make it more challenging, produce examples where $a_n$ and $b_n$ are positive and decreasing. Edit : This problem is taken verbatim from Exercise 2.7.11 on page 68 of Abbott's Understanding Analysis .","Find examples of two series and both of which diverge but for which converges. To make it more challenging, produce examples where and are positive and decreasing. Edit : This problem is taken verbatim from Exercise 2.7.11 on page 68 of Abbott's Understanding Analysis .","\sum a_n \sum b_n \sum \min(a_n, b_n) a_n b_n","['calculus', 'real-analysis', 'sequences-and-series', 'examples-counterexamples']"
62,Why does changing variables work?,Why does changing variables work?,,"I am slightly ashamed to be asking this, but I have been recently reflecting on changing variables in very simple problems. If I missed a question that already discusses this please point it out to me and I will delete this one. Anyhow writing this will probably be a learning experience. Directly from the Wikipedia page on the argument I take as an example the equation: $$x^6 - 9 x^3 + 8 = 0. \, $$ I quickly recognize this as a high school problem and use the methods that were taught to me, namely I set $x^3 = u$ so $x = u^{1/3}$. Then I proceed to solve quadratic equation that results from this substitution, and only at the end I apply the reverse transformation $x^3 = u$ to get an answer for my starting variable. With not much imagination I always thought that the function used when changing variables (in the above case $f(x) = x^3)$ should be bijective in the domain of interest of the starting equation. This is because I need the inverse to return to my ""starting variable"". But I notice on Wikipedia that a bit more is required; the change of variable function should be a diffeomorphism, we need differentiability (and even smooth manifolds for the domain and the image). This is where I realized that I was never taught a proof of why the change of variables method work or how it works but I was just applying these substitutions blindly. So could someone kindly point me to a source where I can improve my understanding on this very powerful method by adding rigour to what I am doing and possibly even a geometric interpretation.","I am slightly ashamed to be asking this, but I have been recently reflecting on changing variables in very simple problems. If I missed a question that already discusses this please point it out to me and I will delete this one. Anyhow writing this will probably be a learning experience. Directly from the Wikipedia page on the argument I take as an example the equation: $$x^6 - 9 x^3 + 8 = 0. \, $$ I quickly recognize this as a high school problem and use the methods that were taught to me, namely I set $x^3 = u$ so $x = u^{1/3}$. Then I proceed to solve quadratic equation that results from this substitution, and only at the end I apply the reverse transformation $x^3 = u$ to get an answer for my starting variable. With not much imagination I always thought that the function used when changing variables (in the above case $f(x) = x^3)$ should be bijective in the domain of interest of the starting equation. This is because I need the inverse to return to my ""starting variable"". But I notice on Wikipedia that a bit more is required; the change of variable function should be a diffeomorphism, we need differentiability (and even smooth manifolds for the domain and the image). This is where I realized that I was never taught a proof of why the change of variables method work or how it works but I was just applying these substitutions blindly. So could someone kindly point me to a source where I can improve my understanding on this very powerful method by adding rigour to what I am doing and possibly even a geometric interpretation.",,['real-analysis']
63,Creating a question that use the $\epsilon$-$\delta$ definition to prove that $f$ is a continuous function,Creating a question that use the - definition to prove that  is a continuous function,\epsilon \delta f,Let $f:\Bbb R\backslash \{1 \} \to \Bbb R$ be defined by $f(x)= \frac{1}{(1-x)}$. Use the $\epsilon$-$\delta$ definition to prove that $f$ is a continuous function. I do not need answers for it. I want your help to twist the questions a little bit with the goal to get another set of questions with more difficulty or similar difficulty and requires different tricks to solve the questions. Please provide the answers and explanation too.,Let $f:\Bbb R\backslash \{1 \} \to \Bbb R$ be defined by $f(x)= \frac{1}{(1-x)}$. Use the $\epsilon$-$\delta$ definition to prove that $f$ is a continuous function. I do not need answers for it. I want your help to twist the questions a little bit with the goal to get another set of questions with more difficulty or similar difficulty and requires different tricks to solve the questions. Please provide the answers and explanation too.,,['real-analysis']
64,"Evaluating $\sum\limits_{n=1}^{\infty} \frac{1}{n\operatorname{ GPF}(n)}$, where $\operatorname{ GPF}(n)$ is the greatest prime factor","Evaluating , where  is the greatest prime factor",\sum\limits_{n=1}^{\infty} \frac{1}{n\operatorname{ GPF}(n)} \operatorname{ GPF}(n),"$\operatorname{ GPF}(n)=$Greatest prime factor of $n$, eg. $\operatorname{ GPF}(17)=17$, $\operatorname{ GPF}(18)=3$. How to evaluate convergence/divergence/value of the sum $$\sum_{n=1}^{\infty} \frac{1}{n\operatorname{ GPF}(n)}\,?$$","$\operatorname{ GPF}(n)=$Greatest prime factor of $n$, eg. $\operatorname{ GPF}(17)=17$, $\operatorname{ GPF}(18)=3$. How to evaluate convergence/divergence/value of the sum $$\sum_{n=1}^{\infty} \frac{1}{n\operatorname{ GPF}(n)}\,?$$",,"['real-analysis', 'sequences-and-series', 'analytic-number-theory']"
65,"If $f$ is strictly convex in a convex set, show it has no more than 1 minimum","If  is strictly convex in a convex set, show it has no more than 1 minimum",f,"Suppouse $A \in \mathbb R^n$ is convex. If $f:A\to\mathbb R$ is strictly convex, show that the set of minimizers if either a singleton or empty. Ok, Suppose there exist more than one minimizer, then $f(x_i)\le f(x)\: \forall x\in B_r(x_i),\: r\gt 0$ where $x_i$ is a minimizer. Therefore there exist $x_j\:s.t\:f''(x_j)\lt0$ , which is a contradiction. I understand intuitively why this is so but i think my proof is wrong Any help would be appreciated Thanks","Suppouse is convex. If is strictly convex, show that the set of minimizers if either a singleton or empty. Ok, Suppose there exist more than one minimizer, then where is a minimizer. Therefore there exist , which is a contradiction. I understand intuitively why this is so but i think my proof is wrong Any help would be appreciated Thanks","A \in \mathbb R^n f:A\to\mathbb R f(x_i)\le f(x)\: \forall x\in B_r(x_i),\: r\gt 0 x_i x_j\:s.t\:f''(x_j)\lt0","['real-analysis', 'optimization']"
66,Sum of tangent functions where arguments are in specific arithmetic series,Sum of tangent functions where arguments are in specific arithmetic series,,"By looking through an book, I found this interesting series To prove that: $$\tan(\theta)+\tan \left(\theta+ \frac{\pi}{n} \right) + \tan\left(\theta + \frac{2\pi}{n}\right) + \dots + \tan \left (\theta + \frac{(n-1)\pi}{n} \right) = -n\cot \left(\frac{n\pi}{2} + n\theta \right)$$ I have tried the Gaussian pairing trick and using: $$\tan A + \tan B = \tan(A+B) (1-\tan A \tan B)$$ And $$\tan A \tan B = \frac{\cos(A-B) - \cos(A+B)}{\cos(A-B)+\cos(A+B)}$$ However I could not do anything of great use. I also considered perhaps roots of unity of: $$z^{2n} = \cos \theta + i\sin \theta$$ But that grew to no real use, but it may be an idea for others. I know I can sum an arithmetic series of the arguments for the cosine and sine functions but I cannot find a source on the internet about the tangent function. Thank You.","By looking through an book, I found this interesting series To prove that: $$\tan(\theta)+\tan \left(\theta+ \frac{\pi}{n} \right) + \tan\left(\theta + \frac{2\pi}{n}\right) + \dots + \tan \left (\theta + \frac{(n-1)\pi}{n} \right) = -n\cot \left(\frac{n\pi}{2} + n\theta \right)$$ I have tried the Gaussian pairing trick and using: $$\tan A + \tan B = \tan(A+B) (1-\tan A \tan B)$$ And $$\tan A \tan B = \frac{\cos(A-B) - \cos(A+B)}{\cos(A-B)+\cos(A+B)}$$ However I could not do anything of great use. I also considered perhaps roots of unity of: $$z^{2n} = \cos \theta + i\sin \theta$$ But that grew to no real use, but it may be an idea for others. I know I can sum an arithmetic series of the arguments for the cosine and sine functions but I cannot find a source on the internet about the tangent function. Thank You.",,"['real-analysis', 'sequences-and-series', 'trigonometric-series']"
67,arithmetic mean of a sequence converges,arithmetic mean of a sequence converges,,"We had a theorem that the means of a sequence also converges: Let $(a_n)_{n\in\mathbb N}$ be a convergent sequence. Then $\displaystyle \overline a_n=\sum_{k=1}^n \frac{a_k}n$ also converges. I've tried to prove it: $|\overline a_n-a|=\frac1n|\sum_{k=1}^n(a_k-a)|\leq\sum_{k=1}^{M-1}|a_k-a|+\sum_{k=M}^n|a_k-a|$ The second sum is $<\frac{\varepsilon}2$ , because there is an $M\in\mathbb N$ so that $(a_n)_{n\in\mathbb N}$ converges. Now you can consider all $n\geq\max\{M,\frac2{\varepsilon}\sum_{k=1}^{M-1}|a_k-a|\}$ and so $|\overline a_n-a|<\varepsilon$ . But can you also say that there is a $K\in\mathbb N$ such that $\frac1n\sum_{k=1}^{M-1}|a_k-a|<\frac{\varepsilon}2$ for all $n\geq K$ ? And do I have to take the first sum from $k=1$ to $M$ or can you do it as above? Thanks for helping.","We had a theorem that the means of a sequence also converges: Let be a convergent sequence. Then also converges. I've tried to prove it: The second sum is , because there is an so that converges. Now you can consider all and so . But can you also say that there is a such that for all ? And do I have to take the first sum from to or can you do it as above? Thanks for helping.","(a_n)_{n\in\mathbb N} \displaystyle \overline a_n=\sum_{k=1}^n \frac{a_k}n |\overline a_n-a|=\frac1n|\sum_{k=1}^n(a_k-a)|\leq\sum_{k=1}^{M-1}|a_k-a|+\sum_{k=M}^n|a_k-a| <\frac{\varepsilon}2 M\in\mathbb N (a_n)_{n\in\mathbb N} n\geq\max\{M,\frac2{\varepsilon}\sum_{k=1}^{M-1}|a_k-a|\} |\overline a_n-a|<\varepsilon K\in\mathbb N \frac1n\sum_{k=1}^{M-1}|a_k-a|<\frac{\varepsilon}2 n\geq K k=1 M",['real-analysis']
68,When does $\lim_{n\to\infty}f(x+\frac{1}{n})=f(x)$ a.e. fail,When does  a.e. fail,\lim_{n\to\infty}f(x+\frac{1}{n})=f(x),"We know that if $f\in L^1(\mathbb{R})$, then $\|f(\cdot+1/n)-f(\cdot)\|_{L^1}\to 0$ as $n\to \infty$, which implies that there exists a subsequence $f_{n_k}=f(x+\frac{1}{n_k})$ such that $f_{n_k}\to f$ a.e. My problem is that Is there any $f\in L^1(\mathbb{R})$  function such that the almost convergence result  $$ f(x)=\lim_{n\to\infty}f(x+1/n)\quad \text{a.e.} $$  is NOT valid? I intend to believe that there exits such function. Otherwise, there would be a beautiful statement such that $f(x)=\lim_{n\to\infty}f(x+1/n)$ a.e. for any $f\in L^1(\mathbb{R})$. Thank you for the long commenting below @David C. Ullrich. This question has been reduce to find a compact set $K$ such that for any $x\in K$, there are infinite $n$ such that $x+\frac{1}{n}\not\in K$.","We know that if $f\in L^1(\mathbb{R})$, then $\|f(\cdot+1/n)-f(\cdot)\|_{L^1}\to 0$ as $n\to \infty$, which implies that there exists a subsequence $f_{n_k}=f(x+\frac{1}{n_k})$ such that $f_{n_k}\to f$ a.e. My problem is that Is there any $f\in L^1(\mathbb{R})$  function such that the almost convergence result  $$ f(x)=\lim_{n\to\infty}f(x+1/n)\quad \text{a.e.} $$  is NOT valid? I intend to believe that there exits such function. Otherwise, there would be a beautiful statement such that $f(x)=\lim_{n\to\infty}f(x+1/n)$ a.e. for any $f\in L^1(\mathbb{R})$. Thank you for the long commenting below @David C. Ullrich. This question has been reduce to find a compact set $K$ such that for any $x\in K$, there are infinite $n$ such that $x+\frac{1}{n}\not\in K$.",,"['real-analysis', 'measure-theory', 'lebesgue-measure', 'harmonic-analysis']"
69,Why does Cantor's diagonal argument yield uncomputable numbers?,Why does Cantor's diagonal argument yield uncomputable numbers?,,"As everyone knows, the set of real numbers is uncountable. The most ubiquitous proof of this fact uses Cantor's diagonal argument . However, I was surprised to learn about a gap in my perception of the real numbers: A computable number is a real number that can be computed to within any desired precision by a finite, terminating algorithm. Turns out that the set of computable numbers is countable . My mind is effectively blown at this point. So I'm trying to reconcile this with Cantor's diagonal argument. Wikipedia has this to say: ""...Cantor's diagonal argument cannot be used to produce uncountably many computable reals; at best, the reals formed from this method will be uncomputable."" So much for background information. Say I have a list of real numbers $.a_{1n}a_{2n}a_{3n}\ldots$ for $n\geq 1$. Why do we get more than just computable numbers if we select digits different from the diagonal digits $a_{ii}$? I.e. if I make a number $.b_1b_2b_3\ldots$ where $b_i\neq a_{ii}$, why is this number not always computable? The main issue I'm having is that it seems like I'm ""computing"" the digits $b_i$ in some sense. Is the problem that I have a choice for each digit? Or is there some other subtlety that I'm missing?","As everyone knows, the set of real numbers is uncountable. The most ubiquitous proof of this fact uses Cantor's diagonal argument . However, I was surprised to learn about a gap in my perception of the real numbers: A computable number is a real number that can be computed to within any desired precision by a finite, terminating algorithm. Turns out that the set of computable numbers is countable . My mind is effectively blown at this point. So I'm trying to reconcile this with Cantor's diagonal argument. Wikipedia has this to say: ""...Cantor's diagonal argument cannot be used to produce uncountably many computable reals; at best, the reals formed from this method will be uncomputable."" So much for background information. Say I have a list of real numbers $.a_{1n}a_{2n}a_{3n}\ldots$ for $n\geq 1$. Why do we get more than just computable numbers if we select digits different from the diagonal digits $a_{ii}$? I.e. if I make a number $.b_1b_2b_3\ldots$ where $b_i\neq a_{ii}$, why is this number not always computable? The main issue I'm having is that it seems like I'm ""computing"" the digits $b_i$ in some sense. Is the problem that I have a choice for each digit? Or is there some other subtlety that I'm missing?",,"['real-analysis', 'set-theory']"
70,What is the 0th root-of-mean-of-powers?,What is the 0th root-of-mean-of-powers?,,"On $\mathbb{R}^n$ and $p\ge 1$ the $p$-norm is defined as $$\|x\|_p=\left ( \sum _{j=1} ^n |x_j| ^p \right ) ^{1/p}$$ and there is the $\infty$-norm which is $\|x\|_\infty=\max _j |x_j|$. It's called the $\infty$ norm because it is the limit of $\|\cdot\|_p$ for $p\to \infty$. Now we can use the definition above for $p<1$ as well and define a $p$-""norm"" for these $p$. The triangle inequality is not satisfied, but I will use the term ""norm"" nonetheless. For $p\to 0$ the limit of $\|x\|_p$ is obviously $\infty$ if there are at least two nonzero entries in $x$, but if we use the following modified definition $$\|x\|_p=\left ( \frac{1}{n} \sum _{j=1} ^n |x_j| ^p \right ) ^{1/p}$$ then this should have a limit for $p\to 0$, which should be called 0-norm. What is this limit?","On $\mathbb{R}^n$ and $p\ge 1$ the $p$-norm is defined as $$\|x\|_p=\left ( \sum _{j=1} ^n |x_j| ^p \right ) ^{1/p}$$ and there is the $\infty$-norm which is $\|x\|_\infty=\max _j |x_j|$. It's called the $\infty$ norm because it is the limit of $\|\cdot\|_p$ for $p\to \infty$. Now we can use the definition above for $p<1$ as well and define a $p$-""norm"" for these $p$. The triangle inequality is not satisfied, but I will use the term ""norm"" nonetheless. For $p\to 0$ the limit of $\|x\|_p$ is obviously $\infty$ if there are at least two nonzero entries in $x$, but if we use the following modified definition $$\|x\|_p=\left ( \frac{1}{n} \sum _{j=1} ^n |x_j| ^p \right ) ^{1/p}$$ then this should have a limit for $p\to 0$, which should be called 0-norm. What is this limit?",,['real-analysis']
71,The definition of locally Lipschitz,The definition of locally Lipschitz,,"Marsden's Elementary Classical Analysis seems to indicate this definition: A function $f:A{\subset}\mathbb R^n\to\mathbb R^m$ is locally Lipschitz if for each $x_0{\in}A,$ there exist constants $M{>}0$ and $\delta_0{>}0$ such that $$||x-x_0||<\delta_0\implies||f(x)-f(x_0)||\leq M||x-x_0||.$$ Here's a scan of the first edition of the text, where the only change from the second (latest) edition referred to above is that the last sentence now reads ""This is called the local Lipschitz property"" (emphasis mine). Two questions: Is the correct inequality $M\geq 0$ or $M>0$ ? Does $M$ depend on $x_0$ , like $\delta_0$ seems to? EDIT :  After pondering further, I've revised the definition to this: A function $f:A{\subset}\mathbb R^n\to\mathbb R^m$ is locally Lipschitz at $x_0{\in}A$ if there exist constants $\delta{>}0$ and $M{\in}\mathbb R$ such that for each $x{\in}A,$ $$||x-x_0||<\delta\implies||f(x)-f(x_0)||\leq M||x-x_0||.$$ Unlike regular/global Lipschitz, local Lipschitz can be defined at a point, and implies pointwise continuity.","Marsden's Elementary Classical Analysis seems to indicate this definition: A function is locally Lipschitz if for each there exist constants and such that Here's a scan of the first edition of the text, where the only change from the second (latest) edition referred to above is that the last sentence now reads ""This is called the local Lipschitz property"" (emphasis mine). Two questions: Is the correct inequality or ? Does depend on , like seems to? EDIT :  After pondering further, I've revised the definition to this: A function is locally Lipschitz at if there exist constants and such that for each Unlike regular/global Lipschitz, local Lipschitz can be defined at a point, and implies pointwise continuity.","f:A{\subset}\mathbb R^n\to\mathbb R^m x_0{\in}A, M{>}0 \delta_0{>}0 ||x-x_0||<\delta_0\implies||f(x)-f(x_0)||\leq M||x-x_0||. M\geq 0 M>0 M x_0 \delta_0 f:A{\subset}\mathbb R^n\to\mathbb R^m x_0{\in}A \delta{>}0 M{\in}\mathbb R x{\in}A, ||x-x_0||<\delta\implies||f(x)-f(x_0)||\leq M||x-x_0||.","['real-analysis', 'continuity', 'normed-spaces', 'lipschitz-functions']"
72,$\sigma$-algebra in Riesz representation theorem,-algebra in Riesz representation theorem,\sigma,"Let $X$ be a locally compact Hausdorff space and $I$ - a positive linear functional on $C_c(X)$. Then according to the Riesz representation theorem there exist a $\sigma$-algebra $\mathfrak{M}$ in $X$ and measure $\mu$ on $\mathfrak{M}$ such that $I(f)=\int_X f d \mu$ for $f \in C_c(X)$. $\mathfrak{M}$ and $\mu$ have the following properties: (a) $\mathfrak{M}$ contains all Borel sets, (b) $\mu(V)=\sup \{I(f): f \in C_c(X), 0\leq f \leq 1, \operatorname{supp} f \subset V\}$ for each open $V$, (c) $\mu(K) < \infty$  for compact $K$, (d) $\mu(E)=\inf \{\mu(V): E \subset V, \ V \mbox{ open}\}$ for each $E \in \mathfrak{M}$, (e) $\mu(E)=\sup \{\mu(K): K \subset E, \ K \mbox{ compact} \}$ for each open $E$ and for each $E\in \mathfrak{M}$ such that $\mu(E)< \infty$, (f) $\mu$ is a complete measure on $\mathfrak{M}$. Let a $\sigma$-algebra $\mathfrak{N}$ in $X$ and and a measure  $\nu$ on $\mathfrak{N}$ satisfy (a)-(f). Is it true that $\mathfrak{M}=\mathfrak{N}$?. I think it is easy when $X$ is $\sigma$-compact. In general I don't know. Thanks.","Let $X$ be a locally compact Hausdorff space and $I$ - a positive linear functional on $C_c(X)$. Then according to the Riesz representation theorem there exist a $\sigma$-algebra $\mathfrak{M}$ in $X$ and measure $\mu$ on $\mathfrak{M}$ such that $I(f)=\int_X f d \mu$ for $f \in C_c(X)$. $\mathfrak{M}$ and $\mu$ have the following properties: (a) $\mathfrak{M}$ contains all Borel sets, (b) $\mu(V)=\sup \{I(f): f \in C_c(X), 0\leq f \leq 1, \operatorname{supp} f \subset V\}$ for each open $V$, (c) $\mu(K) < \infty$  for compact $K$, (d) $\mu(E)=\inf \{\mu(V): E \subset V, \ V \mbox{ open}\}$ for each $E \in \mathfrak{M}$, (e) $\mu(E)=\sup \{\mu(K): K \subset E, \ K \mbox{ compact} \}$ for each open $E$ and for each $E\in \mathfrak{M}$ such that $\mu(E)< \infty$, (f) $\mu$ is a complete measure on $\mathfrak{M}$. Let a $\sigma$-algebra $\mathfrak{N}$ in $X$ and and a measure  $\nu$ on $\mathfrak{N}$ satisfy (a)-(f). Is it true that $\mathfrak{M}=\mathfrak{N}$?. I think it is easy when $X$ is $\sigma$-compact. In general I don't know. Thanks.",,"['real-analysis', 'functional-analysis', 'measure-theory', 'riesz-representation-theorem']"
73,"Example of a complete, non-archimedean ordered field","Example of a complete, non-archimedean ordered field",,"I'm looking for a concrete example of a complete (in the sense that all Cauchy sequences converge) but non-archimedean ordered field, to see that these two properties are independent (an example of archimedean non-complete ordered field is obviously the rationals). Thank you in advance.","I'm looking for a concrete example of a complete (in the sense that all Cauchy sequences converge) but non-archimedean ordered field, to see that these two properties are independent (an example of archimedean non-complete ordered field is obviously the rationals). Thank you in advance.",,"['real-analysis', 'field-theory']"
74,The Identity Theorem for real analytic functions,The Identity Theorem for real analytic functions,,What is the condition for two real analytic functions to be identically equal? We know that there is a nice condition ( Identity Theorem ) for holomorphic function to check if they are the same. What is its version for real analytic functions?,What is the condition for two real analytic functions to be identically equal? We know that there is a nice condition ( Identity Theorem ) for holomorphic function to check if they are the same. What is its version for real analytic functions?,,"['real-analysis', 'analyticity']"
75,What does it mean to extend a function?,What does it mean to extend a function?,,What does it mean to extend a function? Can someone please give an example? Thanks in advance!,What does it mean to extend a function? Can someone please give an example? Thanks in advance!,,"['real-analysis', 'functions']"
76,Are fractions with prime numerator and denominator dense?,Are fractions with prime numerator and denominator dense?,,"So the question is in the title. And I mean dense in positive real numbers ofcourse. Somehow I cannot grasp if this is very trivial or not. The prime numbers aren't that dense, but are there enough of primes to make the fractions constructed from them dense?","So the question is in the title. And I mean dense in positive real numbers ofcourse. Somehow I cannot grasp if this is very trivial or not. The prime numbers aren't that dense, but are there enough of primes to make the fractions constructed from them dense?",,"['real-analysis', 'number-theory']"
77,"If $\lim_n f_n(x_n)=f(x)$ for every $x_n \to x$ then $f_n \to f$ uniformly on $[0,1]$?",If  for every  then  uniformly on ?,"\lim_n f_n(x_n)=f(x) x_n \to x f_n \to f [0,1]","This is a self-posed question, so I do not know the answer and I would like to know what you think about. Let $f,f_n:[0,1]\to \mathbb R$ be continuous functions. Assume that for every sequence $(x_n)_{n\in \mathbb N}$ converging to a point $x\in [0,1]$ , we have $$\lim_n f_n(x_n)=f(x).$$ Can we say that $f_n$ converges uniformly to $f$ in $[0,1]$ ? First of all, do you think the question is well-posed? I'm stuck in thinking about it and I don't manage to prove it neither to find a counterexample. Thanks for your help.","This is a self-posed question, so I do not know the answer and I would like to know what you think about. Let be continuous functions. Assume that for every sequence converging to a point , we have Can we say that converges uniformly to in ? First of all, do you think the question is well-posed? I'm stuck in thinking about it and I don't manage to prove it neither to find a counterexample. Thanks for your help.","f,f_n:[0,1]\to \mathbb R (x_n)_{n\in \mathbb N} x\in [0,1] \lim_n f_n(x_n)=f(x). f_n f [0,1]","['real-analysis', 'convergence-divergence', 'continuity']"
78,"Let $f:\mathbb{R} \to \mathbb{R}$ continuous with $f(f(x))=e^x$, show that $\lim_{x\to \infty } \frac{f(x)}{x^n}=\infty$ (Brazilian Olympiad)","Let  continuous with , show that  (Brazilian Olympiad)",f:\mathbb{R} \to \mathbb{R} f(f(x))=e^x \lim_{x\to \infty } \frac{f(x)}{x^n}=\infty,"Given $f:\mathbb{R} \to \mathbb{R}$ continuous with $f(f(x))=e^x$ for all $x\in\mathbb{R}$ . Show that $\lim_{x\to \infty } \frac{f(x)}{x^n}=\infty$ for all $n\in\mathbb{N}$ . I appreciate any help! Edit 1: It is easy to see that $f$ is injective. So, if $f$ increases anywhere, $f$ will be an increasing function. Edit 2: As said in the comments, $f$ must indeed be increasing, since $f(1)=f(e^0)=f(f(f(0)))=e^{f(0)}>f(0)$ .","Given continuous with for all . Show that for all . I appreciate any help! Edit 1: It is easy to see that is injective. So, if increases anywhere, will be an increasing function. Edit 2: As said in the comments, must indeed be increasing, since .",f:\mathbb{R} \to \mathbb{R} f(f(x))=e^x x\in\mathbb{R} \lim_{x\to \infty } \frac{f(x)}{x^n}=\infty n\in\mathbb{N} f f f f f(1)=f(e^0)=f(f(f(0)))=e^{f(0)}>f(0),"['real-analysis', 'limits', 'contest-math', 'functional-equations']"
79,How to check the real analyticity of a function?,How to check the real analyticity of a function?,,"I recently learnt Taylor series in my class. I would like to know how is to possible to distinguish whether a function is real-analytic or not. First thing to check is if it is smooth. But how can I know whether the taylor series converges to the function? For example: $f(x)=\frac{1}{1-x}, x\in(0,1)$ has $n^{th}$ degree taylor polynomial $\sum_{k=0}^n x^k$. In this case, I understand that $f$ is analytic in its domain since the geometric series $\sum_{k=0}^\infty x^k$ for $x\in(0,1)$ converges to $\frac{1}{1-x}$. In general, what is the trick? For example, how to know if $\sin(x),\cos(x)$ are analytic?","I recently learnt Taylor series in my class. I would like to know how is to possible to distinguish whether a function is real-analytic or not. First thing to check is if it is smooth. But how can I know whether the taylor series converges to the function? For example: $f(x)=\frac{1}{1-x}, x\in(0,1)$ has $n^{th}$ degree taylor polynomial $\sum_{k=0}^n x^k$. In this case, I understand that $f$ is analytic in its domain since the geometric series $\sum_{k=0}^\infty x^k$ for $x\in(0,1)$ converges to $\frac{1}{1-x}$. In general, what is the trick? For example, how to know if $\sin(x),\cos(x)$ are analytic?",,"['real-analysis', 'analysis', 'convergence-divergence', 'taylor-expansion']"
80,"Integral $\int_0^\infty\frac{\log(1+\cos x)}{1+e^x}\,dx=0$",Integral,"\int_0^\infty\frac{\log(1+\cos x)}{1+e^x}\,dx=0","In this question , the result $$\int_0^\infty\frac{\log\cos^2x}{1+e^{2x}}\,dx=-\frac{\log^22}2$$ was shown by writing $\log\cos^2x=2\log\cos x$ . The OP of the linked question attempted the integral as follows $$\int_0^\infty\frac{\log\cos^2x}{1+e^{2x}}\,dx=\frac12\int_0^\infty\frac{\log(1+\cos x)}{1+e^x}\,dx-\frac{\log^22}2$$ by writing $\log\cos^2x=\log(1+\cos2x)-\log2$ . Equating these two expressions yields $$\int_0^\infty\frac{\log(1+\cos x)}{1+e^x}\,dx=0.$$ Is there a direct way to prove this result (without writing $\log\cos^2x$ in two different ways to get there)?","In this question , the result was shown by writing . The OP of the linked question attempted the integral as follows by writing . Equating these two expressions yields Is there a direct way to prove this result (without writing in two different ways to get there)?","\int_0^\infty\frac{\log\cos^2x}{1+e^{2x}}\,dx=-\frac{\log^22}2 \log\cos^2x=2\log\cos x \int_0^\infty\frac{\log\cos^2x}{1+e^{2x}}\,dx=\frac12\int_0^\infty\frac{\log(1+\cos x)}{1+e^x}\,dx-\frac{\log^22}2 \log\cos^2x=\log(1+\cos2x)-\log2 \int_0^\infty\frac{\log(1+\cos x)}{1+e^x}\,dx=0. \log\cos^2x","['real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
81,Continuous function with infinitely many zeros,Continuous function with infinitely many zeros,,"Let $f:[0;1]\rightarrow\mathbb{R}$ a continuous function. Let $Z$ denote the set of zeros of $f$ . If $Z$ is finite, it's easy to prove that $\lim\limits_{n\rightarrow+\infty}\left|\displaystyle\int_{0}^1\text{e}^{nx}f(x)\,\text{d}x\right|=+\infty$ . But is it also true if $f\ne0$ and $Z$ is infinite ?","Let a continuous function. Let denote the set of zeros of . If is finite, it's easy to prove that . But is it also true if and is infinite ?","f:[0;1]\rightarrow\mathbb{R} Z f Z \lim\limits_{n\rightarrow+\infty}\left|\displaystyle\int_{0}^1\text{e}^{nx}f(x)\,\text{d}x\right|=+\infty f\ne0 Z","['real-analysis', 'integration']"
82,Relations that ensure continuity,Relations that ensure continuity,,"We say that a function $f: \mathbb{R} \rightarrow \mathbb{R}$ preserves the binary relation $\sim \subseteq \mathbb{R}^2$ if $x \sim y$ implies $f(x) \sim f(y)$ for all $x,y\in\mathbb{R}$ . We say that $\sim$ ensures continuity if every function $f: \mathbb{R} \rightarrow \mathbb{R}$ that preserves $\sim$ is continuous. Part 1. Is there any binary relation that ensures continuity? A well-ordering would work for this, I think (edit: Silly me, I realized this is not actually true. But a totally rigid relation exists on any set given Choice anyway.) But I'd like to see something better (in Part 2), and if possible something that doesn't require Choice for Part 1. Part 2. Is there any binary relation that ensures continuity, and is preserved by at least two different functions?","We say that a function preserves the binary relation if implies for all . We say that ensures continuity if every function that preserves is continuous. Part 1. Is there any binary relation that ensures continuity? A well-ordering would work for this, I think (edit: Silly me, I realized this is not actually true. But a totally rigid relation exists on any set given Choice anyway.) But I'd like to see something better (in Part 2), and if possible something that doesn't require Choice for Part 1. Part 2. Is there any binary relation that ensures continuity, and is preserved by at least two different functions?","f: \mathbb{R} \rightarrow \mathbb{R} \sim \subseteq \mathbb{R}^2 x \sim y f(x) \sim f(y) x,y\in\mathbb{R} \sim f: \mathbb{R} \rightarrow \mathbb{R} \sim","['real-analysis', 'logic', 'continuity', 'relations', 'model-theory']"
83,A curious theorem by Peano,A curious theorem by Peano,,"Let $f$ be defined on $[a,b]$ and there differentiable. Show that for every $ \epsilon>0 $ there exists a partition $\, a=a_0<a_1<...<a_n=b \,$ of $ \,[a,b] \,$ so that $$\left|\frac {f(a_{i+1})-f(a_i)}{a_{i+1}-a_i}-f'(a_i) \right|<\epsilon\qquad(i=0,...,n-1)$$ This proof was left as an exercise to the Belgian mathematician P.Gilbert by G.Peano in a quarrel (1884) about a mistake made by C.Jordan in his Cours d'analyse vol.1 (1882). According to Peano, Jordan's proof of the mean value inequality theorem presented a fallacious argument: Gilbert did not agree. See nice pp. 12-14. Can someone give her/his own proof ? References T.M.Flett  $\,$ Some Historical Notes and Speculations concerning the Mean Value Theorems of the Differential Calculus (1974) Bull.Inst.Math.Appl. vol.10 pp.66-72 (there is a proof by Flett) J.Mawhin $\,$ Some Contributions of Peano to Analysis in the Light of the Work of Belgian Mathematicians in F.Skof (Ed.) Giuseppe Peano between Mathematics and Logic (2011) Springer (there is Gilbert's proof)","Let $f$ be defined on $[a,b]$ and there differentiable. Show that for every $ \epsilon>0 $ there exists a partition $\, a=a_0<a_1<...<a_n=b \,$ of $ \,[a,b] \,$ so that $$\left|\frac {f(a_{i+1})-f(a_i)}{a_{i+1}-a_i}-f'(a_i) \right|<\epsilon\qquad(i=0,...,n-1)$$ This proof was left as an exercise to the Belgian mathematician P.Gilbert by G.Peano in a quarrel (1884) about a mistake made by C.Jordan in his Cours d'analyse vol.1 (1882). According to Peano, Jordan's proof of the mean value inequality theorem presented a fallacious argument: Gilbert did not agree. See nice pp. 12-14. Can someone give her/his own proof ? References T.M.Flett  $\,$ Some Historical Notes and Speculations concerning the Mean Value Theorems of the Differential Calculus (1974) Bull.Inst.Math.Appl. vol.10 pp.66-72 (there is a proof by Flett) J.Mawhin $\,$ Some Contributions of Peano to Analysis in the Light of the Work of Belgian Mathematicians in F.Skof (Ed.) Giuseppe Peano between Mathematics and Logic (2011) Springer (there is Gilbert's proof)",,"['calculus', 'real-analysis', 'analysis', 'math-history']"
84,"If $f\colon \mathbb{R} \to \mathbb{R}$ is such that $f (x + y) = f (x) f (y)$ and continuous at $0$, then continuous everywhere","If  is such that  and continuous at , then continuous everywhere",f\colon \mathbb{R} \to \mathbb{R} f (x + y) = f (x) f (y) 0,"Prove that if $f\colon\mathbb{R}\to\mathbb{R}$ is such that $f(x+y)=f(x)f(y)$ for all $x,y$, and $f$ is continuous at $0$, then it is continuous everywhere. If there exists $c \in \mathbb{R}$ such that $f(c) = 0$, then $$f(x + c) = f(x)f(c) = 0.$$ As every real number $y$ can be written as $y = x + c$ for some real $x$, this function is either everywhere zero or nowhere zero. The latter case is the interesting one. So let's consider the case that $f$ is not the constant function $f = 0$. To prove continuity in this case, note that for any $x \in \mathbb{R}$ $$f(x) = f(x + 0) = f(x)f(0) \implies f(0) = 1.$$ Continuity at $0$ tells us that given any $\varepsilon_0 > 0$, we can find $\delta_0 > 0$ such that $|x| < \delta_0$ implies $$|f(x) - 1| < \varepsilon_0.$$ Okay, so let $c \in \mathbb{R}$ be fixed arbitrarily (recall that $f(c)$ is nonzero). Let $\varepsilon > 0$. By continuity of $f$ at $0$, we can choose $\delta > 0$ such that $$|x - c| < \delta\implies |f(x - c) - 1| < \frac{\varepsilon}{|f(c)|}.$$ Now notice that for all $x$ such that $|x - c| < \delta$, we have $$\begin{align*} |f(x) - f(c)| &= |f(x - c + c) - f(c)|\\  &= |f(x - c)f(c) - f(c)|\\  &= |f(c)| |f(x - c) - 1|\\  &\lt |f(c)| \frac{\varepsilon}{|f(c)|}\\   &= \varepsilon. \end{align*}$$ Hence $f$ is continuous at $c$. Since $c$ was arbitrary, $f$ is continuous on all of $\mathbb{R}$. Is my procedure correct?","Prove that if $f\colon\mathbb{R}\to\mathbb{R}$ is such that $f(x+y)=f(x)f(y)$ for all $x,y$, and $f$ is continuous at $0$, then it is continuous everywhere. If there exists $c \in \mathbb{R}$ such that $f(c) = 0$, then $$f(x + c) = f(x)f(c) = 0.$$ As every real number $y$ can be written as $y = x + c$ for some real $x$, this function is either everywhere zero or nowhere zero. The latter case is the interesting one. So let's consider the case that $f$ is not the constant function $f = 0$. To prove continuity in this case, note that for any $x \in \mathbb{R}$ $$f(x) = f(x + 0) = f(x)f(0) \implies f(0) = 1.$$ Continuity at $0$ tells us that given any $\varepsilon_0 > 0$, we can find $\delta_0 > 0$ such that $|x| < \delta_0$ implies $$|f(x) - 1| < \varepsilon_0.$$ Okay, so let $c \in \mathbb{R}$ be fixed arbitrarily (recall that $f(c)$ is nonzero). Let $\varepsilon > 0$. By continuity of $f$ at $0$, we can choose $\delta > 0$ such that $$|x - c| < \delta\implies |f(x - c) - 1| < \frac{\varepsilon}{|f(c)|}.$$ Now notice that for all $x$ such that $|x - c| < \delta$, we have $$\begin{align*} |f(x) - f(c)| &= |f(x - c + c) - f(c)|\\  &= |f(x - c)f(c) - f(c)|\\  &= |f(c)| |f(x - c) - 1|\\  &\lt |f(c)| \frac{\varepsilon}{|f(c)|}\\   &= \varepsilon. \end{align*}$$ Hence $f$ is continuous at $c$. Since $c$ was arbitrary, $f$ is continuous on all of $\mathbb{R}$. Is my procedure correct?",,"['real-analysis', 'analysis', 'functional-equations']"
85,Connected unbounded sets $S\subset \Bbb{R}^n$ such that $x\mapsto ||x||^t$ is uniformly continuous on $S$?,Connected unbounded sets  such that  is uniformly continuous on ?,S\subset \Bbb{R}^n x\mapsto ||x||^t S,"Spending the night perusing my old answers, and this question left me wondering about the following. Let's equip $\Bbb{R}^n$ with the usual Euclidean metric, and let us consider the map $N_t:\Bbb{R}^n\to \Bbb{R}$ , $N_t(\vec{x})=||\vec{x}||^t$ . The parameter $t$ is a positive constant, and the question I want to ask is: For which pairs $(n,t)$ does there exist an unbounded path-connected set $S\subset \Bbb{R}^n$ such that the restriction $N_t\vert_S$ is uniformly continuous? ""Clearly"" with $n=1$ we need $t\le1$ . Path-connected + unbounded leaves no wiggle room for choice of $S$ . The answers to the linked question give sets such that $N_2\vert_S$ is uniformly continuous, but those sets are not connected - hence this question. With $n\ge2$ the game is more interesting. I'm thinking of a $S$ as a path slowly spiralling outwards. With $n=2$ something like $S$ = Archimedean spiral, with the distance from the origin growing at the constant rate of one unit per each full revolution, looks good for showing that all $t\le2$ are ok. Here the length of the path in the $n$ th loop is about $2\pi n$ meaning that if $\vec{x}$ moves by $<\delta$ the distance from the origin will change by approximately a constant times $\delta/n$ . In that case $N_2(\vec{x})$ will grow from $n^2$ to $(n+K\delta/n)^2\approx n^2+2K\delta$ , which is ok, just barely, for the purposes of uniform continuity. It doesn't look nearly as promising for $t>2$ . If we are spiralling out any slower, then further out the distance between consecutive rounds of the spiral will tend to zero. Looks like that makes uniform continuity an unrealistic goal, and The argument by user147263 from the comments under the question shows that the exponent $t$ cannot exceed the dimension of the ambient space. I haven't really thought about $n\ge3$ . We can use the extra wiggle room by spending more time at about the same distance, like ""nearly cover"" the sphere at radius $n$ while gradually moving on to the sphere of radius $n+1$ and repeating ever after. But I don't know any good 3D-spirals, not forgetting $n$ D. Any thoughts? Ideas? Suitable higher dimensional spirals? Known work?","Spending the night perusing my old answers, and this question left me wondering about the following. Let's equip with the usual Euclidean metric, and let us consider the map , . The parameter is a positive constant, and the question I want to ask is: For which pairs does there exist an unbounded path-connected set such that the restriction is uniformly continuous? ""Clearly"" with we need . Path-connected + unbounded leaves no wiggle room for choice of . The answers to the linked question give sets such that is uniformly continuous, but those sets are not connected - hence this question. With the game is more interesting. I'm thinking of a as a path slowly spiralling outwards. With something like = Archimedean spiral, with the distance from the origin growing at the constant rate of one unit per each full revolution, looks good for showing that all are ok. Here the length of the path in the th loop is about meaning that if moves by the distance from the origin will change by approximately a constant times . In that case will grow from to , which is ok, just barely, for the purposes of uniform continuity. It doesn't look nearly as promising for . If we are spiralling out any slower, then further out the distance between consecutive rounds of the spiral will tend to zero. Looks like that makes uniform continuity an unrealistic goal, and The argument by user147263 from the comments under the question shows that the exponent cannot exceed the dimension of the ambient space. I haven't really thought about . We can use the extra wiggle room by spending more time at about the same distance, like ""nearly cover"" the sphere at radius while gradually moving on to the sphere of radius and repeating ever after. But I don't know any good 3D-spirals, not forgetting D. Any thoughts? Ideas? Suitable higher dimensional spirals? Known work?","\Bbb{R}^n N_t:\Bbb{R}^n\to \Bbb{R} N_t(\vec{x})=||\vec{x}||^t t (n,t) S\subset \Bbb{R}^n N_t\vert_S n=1 t\le1 S N_2\vert_S n\ge2 S n=2 S t\le2 n 2\pi n \vec{x} <\delta \delta/n N_2(\vec{x}) n^2 (n+K\delta/n)^2\approx n^2+2K\delta t>2 t n\ge3 n n+1 n","['real-analysis', 'analytic-geometry', 'parametric', 'uniform-continuity']"
86,Proving Minkowski's inequality with homogenization,Proving Minkowski's inequality with homogenization,,"The standard proof of Minkowski's inequality in $L^p$ space using Hölder's inequality seems to be pretty unmotivated (see here: http://en.wikipedia.org/wiki/Minkowski%27s_inequality ). Breaking up that factor into two parts, and applying Hölder's to each of them separately makes no geometrical intuitive sense to me. Rather, it would appear that the following proof (which I came up with, though is highly probable isn't in fact mine), appears much more motivated: We need to show that $(\int_{X} |f+g|^p d\mu)^{\frac{1}{p}} \le (\int_{X} |f|^p d\mu)^{\frac{1}{p}} +(\int_{X} |g|^p d\mu)^{\frac{1}{p}}$. Let $A=(\int_{X} |f|^p d\mu)^{\frac{1}{p}}$, and $B=(\int_{X} |g|^p)^{\frac{1}{p}} d\mu$. Then setting $f_1=\frac{f}{A}$, and $g_1=\frac{g}{B}$, we get the equivalent inequalities $\int_{X} (Af_1+Bg_1)^p d\mu \le (A+B)^p$ $\int_{X} (\frac{Af_1+Bg_1}{A+B})^p d\mu \le 1$ $\int_{X} (\frac{Af_1+Bg_1}{A+B})^p d\mu \le \int_{X} \frac{A|f_1|^p+B|g_1|^p}{A+B} d\mu$ Which follows immediately from the convexity of $x^p$. Now, there are a few reasons that I find this more appealing. First, each of the steps is motivated. The first step is an attempt at homogenization, then after rewriting it in the form with the R.H.S.=1, it becomes very apparent how to finish it off using the convexity of $x^p$. Second, a similar line of reasoning is used to prove Hölder's inequality, and is used in the following quite elegant proof of Cauchy-Schwarz for finite sequences: To prove $(\sum a_i^2)(\sum b_i^2) \ge (\sum a_ib_i)^2$, we first homogenize by setting $\sum a_i^2=\sum b_i^2=1$. Then after square rooting, we see it is equivalent to the inequality $\frac{\sum a_i^2 + \sum b_i^2}{2} \ge \sum a_ib_i$, which is a direct result of AM-GM. Note that the use of homogenization is specifically used so that we can replace the L.H.S. with something much stronger while keeping the R.H.S. the same, almost as if by magic (which was essentially what my proof amounted to). Third, the equality case almost becomes trivial to see, since only one inequality was applied in the entire process, and that was at the very end of the proof. Further, it's very easy to see geometrically the idea of convexity being used here, so the equality case also seems natural. My first question is, whether there are merits to the standard proof involving Holder's inequality that I seem to be missing which either make it more general, or make it of more interest than the proof I presented. My second question is to whether the briefly summarized proof involving dual spaces presented on the same Wikipedia page is in actuality more general than my proof. That is, whether there are spaces to which we simply cannot appeal to a convexity argument, and instead have to resort to the supremum proof. Or whether my proof can somehow be modified to prove the result in a more general class of spaces which encompasses all of those which the supremum argument works for. Cheers, Rofler","The standard proof of Minkowski's inequality in $L^p$ space using Hölder's inequality seems to be pretty unmotivated (see here: http://en.wikipedia.org/wiki/Minkowski%27s_inequality ). Breaking up that factor into two parts, and applying Hölder's to each of them separately makes no geometrical intuitive sense to me. Rather, it would appear that the following proof (which I came up with, though is highly probable isn't in fact mine), appears much more motivated: We need to show that $(\int_{X} |f+g|^p d\mu)^{\frac{1}{p}} \le (\int_{X} |f|^p d\mu)^{\frac{1}{p}} +(\int_{X} |g|^p d\mu)^{\frac{1}{p}}$. Let $A=(\int_{X} |f|^p d\mu)^{\frac{1}{p}}$, and $B=(\int_{X} |g|^p)^{\frac{1}{p}} d\mu$. Then setting $f_1=\frac{f}{A}$, and $g_1=\frac{g}{B}$, we get the equivalent inequalities $\int_{X} (Af_1+Bg_1)^p d\mu \le (A+B)^p$ $\int_{X} (\frac{Af_1+Bg_1}{A+B})^p d\mu \le 1$ $\int_{X} (\frac{Af_1+Bg_1}{A+B})^p d\mu \le \int_{X} \frac{A|f_1|^p+B|g_1|^p}{A+B} d\mu$ Which follows immediately from the convexity of $x^p$. Now, there are a few reasons that I find this more appealing. First, each of the steps is motivated. The first step is an attempt at homogenization, then after rewriting it in the form with the R.H.S.=1, it becomes very apparent how to finish it off using the convexity of $x^p$. Second, a similar line of reasoning is used to prove Hölder's inequality, and is used in the following quite elegant proof of Cauchy-Schwarz for finite sequences: To prove $(\sum a_i^2)(\sum b_i^2) \ge (\sum a_ib_i)^2$, we first homogenize by setting $\sum a_i^2=\sum b_i^2=1$. Then after square rooting, we see it is equivalent to the inequality $\frac{\sum a_i^2 + \sum b_i^2}{2} \ge \sum a_ib_i$, which is a direct result of AM-GM. Note that the use of homogenization is specifically used so that we can replace the L.H.S. with something much stronger while keeping the R.H.S. the same, almost as if by magic (which was essentially what my proof amounted to). Third, the equality case almost becomes trivial to see, since only one inequality was applied in the entire process, and that was at the very end of the proof. Further, it's very easy to see geometrically the idea of convexity being used here, so the equality case also seems natural. My first question is, whether there are merits to the standard proof involving Holder's inequality that I seem to be missing which either make it more general, or make it of more interest than the proof I presented. My second question is to whether the briefly summarized proof involving dual spaces presented on the same Wikipedia page is in actuality more general than my proof. That is, whether there are spaces to which we simply cannot appeal to a convexity argument, and instead have to resort to the supremum proof. Or whether my proof can somehow be modified to prove the result in a more general class of spaces which encompasses all of those which the supremum argument works for. Cheers, Rofler",,"['real-analysis', 'inequality']"
87,Learning Aid for Basic Theorems of Topological Vector Spaces in Functional Analysis,Learning Aid for Basic Theorems of Topological Vector Spaces in Functional Analysis,,"I am self-teaching myself the basics of functional analysis (e.g. topological vector spaces), and frankly I am starting to get a migraine sorting out/organizing in my head all of the details/equivalances/implications of the basic theorems involving the various topologies one places on $X$, $X^{*}$, and $X^{**}$.  In particular, details like the conclusion of the Banach-Alauglu theorem, weakly convex (dense) sets are equivalently strongly convex (dense) sets, closed unit ball in a normed space X is weakly compact if and only if $X=X^{**}$ (= meaning isometric isomorphism onto $X^{**}$), when sequential compactness is equivalent to compactness and in which topology/space, linear transformations from $X\to Y$ are continuous on the weak topologies if and only if they are continuous on the strong topologies, etc. etc. etc. Are there a succinct set of notes (or better yet a (large) diagram/table/some-sort-of-graphical-aid) online that clearly organizes the basic theorems regarding the spaces $X$, $X^{*}$, $X^{**}$ and $B(X,Y)$ (distinguishing further the type of space $X$ initially is: general TVS, locally convex, normed, Banach, Hilbert, etc.) and the various topologies one places on them (strong, weak, weak-*, strong operator, weak operator, etc.) I realize that this question is very broad, and I'm sure this is just one of those things that takes a lot of time and dedication in order to get a strong feeling for, but I wanted to ask nonetheless since I am sure there are other people in the same situation.","I am self-teaching myself the basics of functional analysis (e.g. topological vector spaces), and frankly I am starting to get a migraine sorting out/organizing in my head all of the details/equivalances/implications of the basic theorems involving the various topologies one places on $X$, $X^{*}$, and $X^{**}$.  In particular, details like the conclusion of the Banach-Alauglu theorem, weakly convex (dense) sets are equivalently strongly convex (dense) sets, closed unit ball in a normed space X is weakly compact if and only if $X=X^{**}$ (= meaning isometric isomorphism onto $X^{**}$), when sequential compactness is equivalent to compactness and in which topology/space, linear transformations from $X\to Y$ are continuous on the weak topologies if and only if they are continuous on the strong topologies, etc. etc. etc. Are there a succinct set of notes (or better yet a (large) diagram/table/some-sort-of-graphical-aid) online that clearly organizes the basic theorems regarding the spaces $X$, $X^{*}$, $X^{**}$ and $B(X,Y)$ (distinguishing further the type of space $X$ initially is: general TVS, locally convex, normed, Banach, Hilbert, etc.) and the various topologies one places on them (strong, weak, weak-*, strong operator, weak operator, etc.) I realize that this question is very broad, and I'm sure this is just one of those things that takes a lot of time and dedication in order to get a strong feeling for, but I wanted to ask nonetheless since I am sure there are other people in the same situation.",,"['real-analysis', 'functional-analysis', 'soft-question', 'topological-vector-spaces', 'learning']"
88,Is there a book only about epsilon delta proofs?,Is there a book only about epsilon delta proofs?,,"I want to know if there is such book, with beautiful epsilon delta proofs of all kind.","I want to know if there is such book, with beautiful epsilon delta proofs of all kind.",,"['calculus', 'real-analysis', 'complex-analysis', 'reference-request', 'epsilon-delta']"
89,What is the length of a point on the real number line?,What is the length of a point on the real number line?,,"Since an interval is made up of an infinite number of points, I am considering the relation of the length of an interval and the length of a point, this lead me to ask what is the length of a point on the real number line ? The nested interval theorem made me feel the length of a point should be $0$ because of $\lim_{{n\to\infty}}(b_{n}\!-\!a_{n})=0$ While if the length of a point  on the real number line is $0$, then I get a contradiction : Supposing we remove the point $0$ on the real number line, then there is a gap there, and the width of the gap is $0$ since the length of a point  on the real number line is $0$, however I think the width of the gap being $0$ is equivalent to there being no such gap on the real number line, so this leads to a contradiction. What's wrong here? Does a point on the real number line have a width? If so, what is the length of a point on the real number line? Infinitesimal ?","Since an interval is made up of an infinite number of points, I am considering the relation of the length of an interval and the length of a point, this lead me to ask what is the length of a point on the real number line ? The nested interval theorem made me feel the length of a point should be $0$ because of $\lim_{{n\to\infty}}(b_{n}\!-\!a_{n})=0$ While if the length of a point  on the real number line is $0$, then I get a contradiction : Supposing we remove the point $0$ on the real number line, then there is a gap there, and the width of the gap is $0$ since the length of a point  on the real number line is $0$, however I think the width of the gap being $0$ is equivalent to there being no such gap on the real number line, so this leads to a contradiction. What's wrong here? Does a point on the real number line have a width? If so, what is the length of a point on the real number line? Infinitesimal ?",,['real-analysis']
90,"Intuitively, how should I think of Measurable Functions?","Intuitively, how should I think of Measurable Functions?",,"Mesurable Functions by definition(from Stein and Shakarchi): A function $f$ defined on a measurable subset $E$ of $\mathbb{R}^d$ is measurable, if for all $a\in \mathbb{R}$, the set  $$f^{-1}([-\infty,a))=\{x\in E: f(x)<a\}$$ is measurable. Now a set $E$ is called measurable if $m_*(E)=0.$ Intuitively, the definition doesn't make much sense to me and would appreciate it if someone can explain it to me. A bonus would be if you can give me some simple examples of measurable and non measurable functions? Thanks.","Mesurable Functions by definition(from Stein and Shakarchi): A function $f$ defined on a measurable subset $E$ of $\mathbb{R}^d$ is measurable, if for all $a\in \mathbb{R}$, the set  $$f^{-1}([-\infty,a))=\{x\in E: f(x)<a\}$$ is measurable. Now a set $E$ is called measurable if $m_*(E)=0.$ Intuitively, the definition doesn't make much sense to me and would appreciate it if someone can explain it to me. A bonus would be if you can give me some simple examples of measurable and non measurable functions? Thanks.",,"['real-analysis', 'measure-theory']"
91,Is it acceptable to say that a divergent series that tends to infinity is 'equal to' infinity?,Is it acceptable to say that a divergent series that tends to infinity is 'equal to' infinity?,,"Consider a divergent series that tends to infinity such as $1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots$ . The limit of this series is unbounded, and I have often seen people say that the sum 'equals infinity' as a shorthand for this. However, is it acceptable to write $1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots = \infty$ in formal mathematics, or is it better to denote that the limit is equal to infinity? If so, how does one do this?","Consider a divergent series that tends to infinity such as . The limit of this series is unbounded, and I have often seen people say that the sum 'equals infinity' as a shorthand for this. However, is it acceptable to write in formal mathematics, or is it better to denote that the limit is equal to infinity? If so, how does one do this?",1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots 1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\cdots = \infty,"['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'notation']"
92,Nonexistence of an injective $C^1$ map between $\mathbb R^2$ and $\mathbb R$,Nonexistence of an injective  map between  and,C^1 \mathbb R^2 \mathbb R,I am getting bored waiting for the train so I'm thinking whether there can exist a $C^1$ injective map between $\mathbb{R}^2$ and $\mathbb{R}$. It seems to me that the answer is no but I can't find a proof or a counterexample... Can you help me?,I am getting bored waiting for the train so I'm thinking whether there can exist a $C^1$ injective map between $\mathbb{R}^2$ and $\mathbb{R}$. It seems to me that the answer is no but I can't find a proof or a counterexample... Can you help me?,,"['real-analysis', 'general-topology']"
93,Computing $\lim_{n\to\infty}n\sum_{k=1}^n\left( \frac{1}{(2k-1)^2} - \frac{3}{4k^2}\right)$,Computing,\lim_{n\to\infty}n\sum_{k=1}^n\left( \frac{1}{(2k-1)^2} - \frac{3}{4k^2}\right),"What ways would you propose for the limit below? $$\lim_{n\to\infty}n\sum_{k=1}^n\left( \frac{1}{(2k-1)^2} - \frac{3}{4k^2}\right)$$ Thanks in advance for your suggestions, hints! Sis.","What ways would you propose for the limit below? $$\lim_{n\to\infty}n\sum_{k=1}^n\left( \frac{1}{(2k-1)^2} - \frac{3}{4k^2}\right)$$ Thanks in advance for your suggestions, hints! Sis.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
94,Why is log-of-sum-of-exponentials $f(x)=\log\left(\sum_{i=1}^n e^ {x_i}\right)$ a convex function for $x \in\mathbb R^n$?,Why is log-of-sum-of-exponentials  a convex function for ?,f(x)=\log\left(\sum_{i=1}^n e^ {x_i}\right) x \in\mathbb R^n,"How to prove $f(x)=\log\left(\displaystyle\sum_{i=1}^n e^{x_i}\right)$ is a convex function? EDIT1: for above function $f(x)$ , following inequalities hold: $$\max\{x_1,x_2,\ldots,x_n\}\leqslant f(x)\leqslant\max\{x_1,x_2,\ldots,x_n\}+\log n$$ and I have tried proving its convexity via definition of a convex function with above inequalities, but that didn't work. EDIT2 : I have posted my answers below.","How to prove is a convex function? EDIT1: for above function , following inequalities hold: and I have tried proving its convexity via definition of a convex function with above inequalities, but that didn't work. EDIT2 : I have posted my answers below.","f(x)=\log\left(\displaystyle\sum_{i=1}^n e^{x_i}\right) f(x) \max\{x_1,x_2,\ldots,x_n\}\leqslant f(x)\leqslant\max\{x_1,x_2,\ldots,x_n\}+\log n","['real-analysis', 'convex-analysis']"
95,Why doesn't the nested interval theorem hold for open intervals?,Why doesn't the nested interval theorem hold for open intervals?,,"Why is the condition that the intervals be closed necessary? Could someone give me an example of a sequence of nonempty, bounded, nested intervals whose intersection is empty? I can't think of one, so why does the theorem require it? Here is the theorem: If $I_1 \supset I_2 \supset I_3 \supset \dots$ is a sequence of nested, closed, bounded, nonempty intervals, then $\bigcap_{n=1}^{\infty}I_n$ is nonempty. If, in addition, the length of $I_n$ approaches zero, then $\bigcap_{n=1}^{\infty}I_n$ consists of a single point. My apologies if this has been asked before but I couldn't find it.","Why is the condition that the intervals be closed necessary? Could someone give me an example of a sequence of nonempty, bounded, nested intervals whose intersection is empty? I can't think of one, so why does the theorem require it? Here is the theorem: If $I_1 \supset I_2 \supset I_3 \supset \dots$ is a sequence of nested, closed, bounded, nonempty intervals, then $\bigcap_{n=1}^{\infty}I_n$ is nonempty. If, in addition, the length of $I_n$ approaches zero, then $\bigcap_{n=1}^{\infty}I_n$ consists of a single point. My apologies if this has been asked before but I couldn't find it.",,"['real-analysis', 'analysis']"
96,Integral $\int_0^\infty \log^2 x\frac{1+x^2}{1+x^4}dx=\frac{3 \pi^3}{16\sqrt 2}$,Integral,\int_0^\infty \log^2 x\frac{1+x^2}{1+x^4}dx=\frac{3 \pi^3}{16\sqrt 2},"This integral below $$ I:=\int_0^\infty \log^2 x\frac{1+x^2}{1+x^4}dx=\frac{3 \pi^3}{16 \sqrt 2} $$ is what I am trying to prove.  Thanks. We can not expand the denominator as a series since the domain of integration is for $x\in [0,\infty)$.  Next I wrote $$ I=\int_0^\infty \log^2 x \frac{1+x^4-x^4+x^2}{1+x^4}dx=\int_0^\infty \log^2x \left(\frac{1+x^4}{1+x^4}+\frac{x^2-x^4}{1+x^4}\right)dx=\\ \int_0^\infty \log^2 x \, dx+\int_0^\infty \log^2 x \frac{x^2}{1+x^4}dx-\int_0^\infty \log^2 x \frac{x^4}{1+x^4}dx, $$ however only the middle integral is convergent.  I am not sure how to go about solving this problem.  Thank you","This integral below $$ I:=\int_0^\infty \log^2 x\frac{1+x^2}{1+x^4}dx=\frac{3 \pi^3}{16 \sqrt 2} $$ is what I am trying to prove.  Thanks. We can not expand the denominator as a series since the domain of integration is for $x\in [0,\infty)$.  Next I wrote $$ I=\int_0^\infty \log^2 x \frac{1+x^4-x^4+x^2}{1+x^4}dx=\int_0^\infty \log^2x \left(\frac{1+x^4}{1+x^4}+\frac{x^2-x^4}{1+x^4}\right)dx=\\ \int_0^\infty \log^2 x \, dx+\int_0^\infty \log^2 x \frac{x^2}{1+x^4}dx-\int_0^\infty \log^2 x \frac{x^4}{1+x^4}dx, $$ however only the middle integral is convergent.  I am not sure how to go about solving this problem.  Thank you",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
97,"Double Integral $\int_0^\infty \int_0^\infty \frac{\log x \log y}{\sqrt {xy}}\cos(x+y)\,dx\,dy=(\gamma+2\log 2)\pi^2$",Double Integral,"\int_0^\infty \int_0^\infty \frac{\log x \log y}{\sqrt {xy}}\cos(x+y)\,dx\,dy=(\gamma+2\log 2)\pi^2","Hi I am trying to solve this double integral $$ I:=\int_0^\infty \int_0^\infty \frac{\log x \log y}{\sqrt {xy}}\cos(x+y)\,dx\,dy=(\gamma+2\log 2)\pi^2. $$ Thank you. The constant in the result is given by $\gamma\approx .577$, and is known as the Euler-Mascheroni constant.  I was thinking to write $$ I=\Re \bigg[\int_0^\infty \int_0^\infty \frac{\log x \log y}{\sqrt{xy}}\, e^{i(x+y)}\, dx\, dy\bigg] $$ and using Leibniz's rule for differentiation under the integral sign to write $$ I(\eta, \xi)=\Re\bigg[ \int_0^\infty \int_0^\infty \ \frac{\log (\eta x)\log(\xi y)}{\sqrt{xy}} e^{i(x+y)}dx\,dy.   \bigg]\\ $$ After taking the derivatives it became obvious that I need to try another method since the x,y constants cancel out.  How can we solve this integral I?  Thanks.","Hi I am trying to solve this double integral $$ I:=\int_0^\infty \int_0^\infty \frac{\log x \log y}{\sqrt {xy}}\cos(x+y)\,dx\,dy=(\gamma+2\log 2)\pi^2. $$ Thank you. The constant in the result is given by $\gamma\approx .577$, and is known as the Euler-Mascheroni constant.  I was thinking to write $$ I=\Re \bigg[\int_0^\infty \int_0^\infty \frac{\log x \log y}{\sqrt{xy}}\, e^{i(x+y)}\, dx\, dy\bigg] $$ and using Leibniz's rule for differentiation under the integral sign to write $$ I(\eta, \xi)=\Re\bigg[ \int_0^\infty \int_0^\infty \ \frac{\log (\eta x)\log(\xi y)}{\sqrt{xy}} e^{i(x+y)}dx\,dy.   \bigg]\\ $$ After taking the derivatives it became obvious that I need to try another method since the x,y constants cancel out.  How can we solve this integral I?  Thanks.",,"['calculus', 'real-analysis', 'integration', 'complex-analysis', 'definite-integrals']"
98,Difference between Norm and Distance,Difference between Norm and Distance,,"I'm now studying metric space. Here, I don't understand why definitions of distance and norm in euclidean space are repectively given in my book. I understand the difference between two concepts when i'm working on non-euclidean space, but is there any even slight difference between these two concepts when it is $\mathbb{R}^k$?","I'm now studying metric space. Here, I don't understand why definitions of distance and norm in euclidean space are repectively given in my book. I understand the difference between two concepts when i'm working on non-euclidean space, but is there any even slight difference between these two concepts when it is $\mathbb{R}^k$?",,"['real-analysis', 'metric-spaces']"
99,"Why did mathematician construct extended real number systems, $\mathbb R \cup\{+\infty,-\infty\}$?","Why did mathematician construct extended real number systems, ?","\mathbb R \cup\{+\infty,-\infty\}","I know some properties cannot be defined with the real number system such as supremum of an unbounded set. but I want to know the philosophy behind this construction (extended real number system ($\mathbb R \cup\{+\infty,-\infty\} $) and projectively extended real number system ($\mathbb R \cup\{\infty\}$)) and why did mathematician want to do so? what are the beautiful properties they achieved? I want an answer with a philosophical point of view. P.S. is there any books or notes or something which I can refer?","I know some properties cannot be defined with the real number system such as supremum of an unbounded set. but I want to know the philosophy behind this construction (extended real number system ($\mathbb R \cup\{+\infty,-\infty\} $) and projectively extended real number system ($\mathbb R \cup\{\infty\}$)) and why did mathematician want to do so? what are the beautiful properties they achieved? I want an answer with a philosophical point of view. P.S. is there any books or notes or something which I can refer?",,"['real-analysis', 'infinity', 'philosophy']"
