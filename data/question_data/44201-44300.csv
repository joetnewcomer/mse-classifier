,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Discriminant of $x^n-1$,Discriminant of,x^n-1,"Question is to find discriminant of polynomial $x^n-1$ I consider $f(x)=x^n-1=(x-a_1)(x-a_2)(x-a_3)\cdots(x-a_n)$ Now, $$f'(x)=[(x-a_2)(x-a_3)\cdots(x-a_n)]+\cdots+[(x-a_1)(x-a_2)\cdots(x-a_{n-1})]$$ $f'(a_1)=(a_1-a_2)(a_1-a_3)\cdots(a_1-a_n)$ $f'(a_2)=(a_2-a_1)(a_2-a_3)\cdots(a_2-a_n)$ $f'(a_3)=(a_3-a_1)(a_3-a_2)\cdots (a_3-a_n)$ and so on.. Now i need to know how many sign changes do i need to get something which looks like discriminant I would write this in a matrix form to get some idea... $$\begin{bmatrix}12&13&14&15&\cdots&1n\\21&23&24&25&\cdots&2n\\31&32&34&35&\cdots&3n\\\\n1&n2&n3&n4&\cdots&n(n-1) \end{bmatrix}$$ See that i first row every element is in correct position i mean of the form $ij$ for $i<j$ In second row only one element is odd of the form $ij$ with $i>j$ but i want $i<j$ in discriminant so i would change this.. So my count starts... change sign 1 In third row there are two elements which are not behaving properly... So, I should change them also.. So, Now another  two changes... On the whole $1+2$ changes... In fourth row there would be $3$ misbehaving children so my count is $1+2+3$ In last row every body is behaving badly so i have to make $n-1$ changes in that.. On the whole i have to make $1+2+3+\cdots +n-1=\dfrac{n(n-1)}{2}$ changes.. So, $f'(a_1)f'(a_2)\cdots f'(a_n)=(-1)^{\dfrac{n(n-1)}{2}}(a_1-a_2)^2(a_1-a_3)^2\cdots (a_{n-1}-a_n)^2=(-1)^{\dfrac{n(n-1)}{2}} Disc(f)$ But then $f'(x)=nx^{n-1}$ This tells me that $f'(a_i)=n(a_i)^{n-1}$ So, $$Disc (f)=(-1)^{\dfrac{n(n-1)}{2}}  n^n(a_1a_2\cdots a_n)^{n-1}=(-1)^{\dfrac{n(n-1)}{2}}  n^n(-1)^{(n-1)^2}=(-1)^{\dfrac{n(n-1)}{2}+(n-1)^2}$$ As $(-1)^{n-1}=(-1)^{(n-1)^2}$ i would replace my $(-1)^{(n-1)^2}$ with $(-1)^{n-1}$ $$Disc (f)=(-1)^{\dfrac{n(n-1)}{2}}  n^n(a_1a_2\cdots a_n)^{n-1}=(-1)^{\dfrac{n(n-1)}{2}}  n^n(-1)^{n-1}=(-1)^{\dfrac{n(n-1)}{2}+n-1}n^n$$ i.e., $$Disc(x^n-1)=(-1)^{\dfrac{(n-1)(n+2)}{2}}n^n$$ As nobody was giving answer to my question i was trying my best and started editing this when ever i thought i find some thing and this is its final stage... This is fully solved now... Thank you.. Any other ways of approaches would be appreciated.. Thank you :)","Question is to find discriminant of polynomial $x^n-1$ I consider $f(x)=x^n-1=(x-a_1)(x-a_2)(x-a_3)\cdots(x-a_n)$ Now, $$f'(x)=[(x-a_2)(x-a_3)\cdots(x-a_n)]+\cdots+[(x-a_1)(x-a_2)\cdots(x-a_{n-1})]$$ $f'(a_1)=(a_1-a_2)(a_1-a_3)\cdots(a_1-a_n)$ $f'(a_2)=(a_2-a_1)(a_2-a_3)\cdots(a_2-a_n)$ $f'(a_3)=(a_3-a_1)(a_3-a_2)\cdots (a_3-a_n)$ and so on.. Now i need to know how many sign changes do i need to get something which looks like discriminant I would write this in a matrix form to get some idea... $$\begin{bmatrix}12&13&14&15&\cdots&1n\\21&23&24&25&\cdots&2n\\31&32&34&35&\cdots&3n\\\\n1&n2&n3&n4&\cdots&n(n-1) \end{bmatrix}$$ See that i first row every element is in correct position i mean of the form $ij$ for $i<j$ In second row only one element is odd of the form $ij$ with $i>j$ but i want $i<j$ in discriminant so i would change this.. So my count starts... change sign 1 In third row there are two elements which are not behaving properly... So, I should change them also.. So, Now another  two changes... On the whole $1+2$ changes... In fourth row there would be $3$ misbehaving children so my count is $1+2+3$ In last row every body is behaving badly so i have to make $n-1$ changes in that.. On the whole i have to make $1+2+3+\cdots +n-1=\dfrac{n(n-1)}{2}$ changes.. So, $f'(a_1)f'(a_2)\cdots f'(a_n)=(-1)^{\dfrac{n(n-1)}{2}}(a_1-a_2)^2(a_1-a_3)^2\cdots (a_{n-1}-a_n)^2=(-1)^{\dfrac{n(n-1)}{2}} Disc(f)$ But then $f'(x)=nx^{n-1}$ This tells me that $f'(a_i)=n(a_i)^{n-1}$ So, $$Disc (f)=(-1)^{\dfrac{n(n-1)}{2}}  n^n(a_1a_2\cdots a_n)^{n-1}=(-1)^{\dfrac{n(n-1)}{2}}  n^n(-1)^{(n-1)^2}=(-1)^{\dfrac{n(n-1)}{2}+(n-1)^2}$$ As $(-1)^{n-1}=(-1)^{(n-1)^2}$ i would replace my $(-1)^{(n-1)^2}$ with $(-1)^{n-1}$ $$Disc (f)=(-1)^{\dfrac{n(n-1)}{2}}  n^n(a_1a_2\cdots a_n)^{n-1}=(-1)^{\dfrac{n(n-1)}{2}}  n^n(-1)^{n-1}=(-1)^{\dfrac{n(n-1)}{2}+n-1}n^n$$ i.e., $$Disc(x^n-1)=(-1)^{\dfrac{(n-1)(n+2)}{2}}n^n$$ As nobody was giving answer to my question i was trying my best and started editing this when ever i thought i find some thing and this is its final stage... This is fully solved now... Thank you.. Any other ways of approaches would be appreciated.. Thank you :)",,['abstract-algebra']
1,p-adic cubic root,p-adic cubic root,,"Let $p$ be prime such that $p\equiv  2\bmod 3$. Show that for every $a\in \mathbb Z,p\nmid a$ there is a $x\in \mathbb Z_p$, where $\mathbb Z_p$ is the field of the p-adic integers, such that $x^3=a$.","Let $p$ be prime such that $p\equiv  2\bmod 3$. Show that for every $a\in \mathbb Z,p\nmid a$ there is a $x\in \mathbb Z_p$, where $\mathbb Z_p$ is the field of the p-adic integers, such that $x^3=a$.",,"['abstract-algebra', 'p-adic-number-theory']"
2,Why is an extension $\bar \phi : F(x) \rightarrow F(a)$ an isomorphism if $\phi : F[x] \to F(a)$ is injective?,Why is an extension  an isomorphism if  is injective?,\bar \phi : F(x) \rightarrow F(a) \phi : F[x] \to F(a),"Why is $\bar \phi : F(x) \rightarrow F(a)$ an isomorphism given  $\phi : F[x] \rightarrow F(a)$ satisfy $\ker \phi = \{0\}$ ? I've been trying to figure out why $\bar \phi$ is an isomorphism, and why $\phi$ is not an isomorphism. I know that since $\ker \phi = \{0\}$ it follows that $\phi$ is one-to-one (please correct me if I'm wrong). Now we only need to show $\overline{\phi}$ is surjective in order to show $\overline{\phi}$ is an isomorphism $F(x) \rightarrow F(a)$. How can I see that $\bar \phi$ is an isomorphism $F(x) \rightarrow F(a)$? And why can I extend a one-to-one homomorphism on the polynomial ring to achieve this result? OBS: $F(a)$ denote the smallest subfield of $E$ that contains $F$ and $\{a\}$.","Why is $\bar \phi : F(x) \rightarrow F(a)$ an isomorphism given  $\phi : F[x] \rightarrow F(a)$ satisfy $\ker \phi = \{0\}$ ? I've been trying to figure out why $\bar \phi$ is an isomorphism, and why $\phi$ is not an isomorphism. I know that since $\ker \phi = \{0\}$ it follows that $\phi$ is one-to-one (please correct me if I'm wrong). Now we only need to show $\overline{\phi}$ is surjective in order to show $\overline{\phi}$ is an isomorphism $F(x) \rightarrow F(a)$. How can I see that $\bar \phi$ is an isomorphism $F(x) \rightarrow F(a)$? And why can I extend a one-to-one homomorphism on the polynomial ring to achieve this result? OBS: $F(a)$ denote the smallest subfield of $E$ that contains $F$ and $\{a\}$.",,"['abstract-algebra', 'ring-theory', 'field-theory']"
3,Abel-Ruffini Theorem,Abel-Ruffini Theorem,,Is the Abel-Ruffini Theorem essentially equivalent to saying that the set of all complex numbers constructable by a concatenation of field operations and n-roots of rational numbers is a proper subset of the algebraic numbers?,Is the Abel-Ruffini Theorem essentially equivalent to saying that the set of all complex numbers constructable by a concatenation of field operations and n-roots of rational numbers is a proper subset of the algebraic numbers?,,"['abstract-algebra', 'galois-theory']"
4,"Finding a subgroup of the Galois group of $E/(\mathbb{Z}/(p))$ where $E = (\mathbb{Z}/(p))(t)$, $t$ transcendental.","Finding a subgroup of the Galois group of  where ,  transcendental.",E/(\mathbb{Z}/(p)) E = (\mathbb{Z}/(p))(t) t,"As the title states, the setup is Let $E = (\mathbb{Z}/(p))(t)$, and we are looking at it over $\mathbb{Z}/(p)$ and $t$ is transcendental. Let $G$ be a group of automorphisms of $E$ generated by $\sigma: t \to t+1$. Determine $F = Inv \, G$, and $[E:F]$. Adjoining a transcendental element to a finite field is one of my weakest aspects of Galois theory, so I don't really know where to start. So $\sigma^p = id$, so $|G| = p$?","As the title states, the setup is Let $E = (\mathbb{Z}/(p))(t)$, and we are looking at it over $\mathbb{Z}/(p)$ and $t$ is transcendental. Let $G$ be a group of automorphisms of $E$ generated by $\sigma: t \to t+1$. Determine $F = Inv \, G$, and $[E:F]$. Adjoining a transcendental element to a finite field is one of my weakest aspects of Galois theory, so I don't really know where to start. So $\sigma^p = id$, so $|G| = p$?",,"['abstract-algebra', 'galois-theory']"
5,$Q(\mathbb{Z}[t]) / \mathbb{Z}[t]$ is not injective,is not injective,Q(\mathbb{Z}[t]) / \mathbb{Z}[t],"I am doing this exercise: Let $R = \mathbb{Z}[t]$ and let $K$ be its fraction field. Show that the $R$ module $K/R$ is divisible but not injective. I have done the divisible part, but I am stuck on the injective part, any hint ?","I am doing this exercise: Let $R = \mathbb{Z}[t]$ and let $K$ be its fraction field. Show that the $R$ module $K/R$ is divisible but not injective. I have done the divisible part, but I am stuck on the injective part, any hint ?",,"['abstract-algebra', 'modules', 'injective-module']"
6,Prove that $B_{q}$ is flat over $B_{p}$,Prove that  is flat over,B_{q} B_{p},"I'm doing this exercise (exercise 18, p. 46) in ""Introduction to Commutative Algebra"" of Atiyah and get confused by the hint in this book. Here is the exercise: Let $f: A \rightarrow B$ be a flat homomorphism of rings, let $q$ be a prime ideal of $B$ and let $p = q^{c}$ . Then $f^*: \operatorname{Spec}(B_q) \rightarrow \operatorname{Spec}(A_p)$ is surjective [Hint: For $B_p$ is flat over $A_p$ by (3.10), and $B_q$ is a local ring of $B_p$ , hence is flat over $B_p$ ...] The highlighted part is the one I want to ask. Elements of the module $B_q$ have form $b/s$ , here $s \in B$ but $s \notin q$ . In the other hand, elements of the module $B_p$ have form $b/s$ , here $s \in A$ but $s \notin p$ . So why $B_q$ is local ring of $B_p$ . And one more thing, why is local ring flat? Please help me clarify this. I really appreciate. Thanks.","I'm doing this exercise (exercise 18, p. 46) in ""Introduction to Commutative Algebra"" of Atiyah and get confused by the hint in this book. Here is the exercise: Let be a flat homomorphism of rings, let be a prime ideal of and let . Then is surjective [Hint: For is flat over by (3.10), and is a local ring of , hence is flat over ...] The highlighted part is the one I want to ask. Elements of the module have form , here but . In the other hand, elements of the module have form , here but . So why is local ring of . And one more thing, why is local ring flat? Please help me clarify this. I really appreciate. Thanks.",f: A \rightarrow B q B p = q^{c} f^*: \operatorname{Spec}(B_q) \rightarrow \operatorname{Spec}(A_p) B_p A_p B_q B_p B_p B_q b/s s \in B s \notin q B_p b/s s \in A s \notin p B_q B_p,"['abstract-algebra', 'ring-theory', 'commutative-algebra']"
7,Is $\mathbb{C}$ algebraically closed (in a strong sense)?,Is  algebraically closed (in a strong sense)?,\mathbb{C},"Let  $p,q$ be polynomials in $\mathbb{C}[x,y]$ such that the ideal $(p,q)$ is a proper ideal of $\mathbb{C}[x,y]$. Does there exist complex numbers $z,w$ such that $$p(z,w)=0,\,\,\,\,\,q(z,w)=0\ ?$$ Motivation: Let $K$ be a field and $f$ be a non-constant polynomial in $K[x]$ that does not have a root in $K$. One can choose a non-unit irreducible factor $p$ of $f$ and construct the field extension $K[x]/(p)$. The field resulting will contain an isomorphic copy of $K$ and have a root to $f$. Now if we try to do this for two variables. Again let $K$ be a field. Let $p,q$ be two polynomials in $K[x,y]$. Now if it happens that $(p,q)=K[x,y]$ then there is no hope of finding a field extension of $K$ that will contain solution to the similtaneous equations $p=0,q=0$ for obvious reasons (by considering the evaluation homomorphism). If $(p,q)$ is a proper ideal of $K[x,y]$ then one can choose a maximal ideal containing $(p,q)$. If we set $E=K[x,y]/I$, one can see easily that the field $E$ contains an isomorphic copy of $K$ and has solutions to the similtaneous equations $p=0,q=0$","Let  $p,q$ be polynomials in $\mathbb{C}[x,y]$ such that the ideal $(p,q)$ is a proper ideal of $\mathbb{C}[x,y]$. Does there exist complex numbers $z,w$ such that $$p(z,w)=0,\,\,\,\,\,q(z,w)=0\ ?$$ Motivation: Let $K$ be a field and $f$ be a non-constant polynomial in $K[x]$ that does not have a root in $K$. One can choose a non-unit irreducible factor $p$ of $f$ and construct the field extension $K[x]/(p)$. The field resulting will contain an isomorphic copy of $K$ and have a root to $f$. Now if we try to do this for two variables. Again let $K$ be a field. Let $p,q$ be two polynomials in $K[x,y]$. Now if it happens that $(p,q)=K[x,y]$ then there is no hope of finding a field extension of $K$ that will contain solution to the similtaneous equations $p=0,q=0$ for obvious reasons (by considering the evaluation homomorphism). If $(p,q)$ is a proper ideal of $K[x,y]$ then one can choose a maximal ideal containing $(p,q)$. If we set $E=K[x,y]/I$, one can see easily that the field $E$ contains an isomorphic copy of $K$ and has solutions to the similtaneous equations $p=0,q=0$",,"['abstract-algebra', 'algebraic-geometry', 'polynomials']"
8,Galois group of irreducible Quartic polynomial over $\mathbb{Q}$,Galois group of irreducible Quartic polynomial over,\mathbb{Q},"Actual Question is  : What are all possible galois groups of an irreducible Quartic polynomial over $\mathbb{Q}$ As polynomial is irreducible, Galois group is transitive subgroup of $S_4$. I have no idea what (actually why) are all transitive subgroups of $S_4$. Firstly, I would distinguish what are all possible subgroups (and then see for transitive subgroups) of $S_4$. As $|S_4|=24$ only possible orders of subgroups of $S_4$ are $\{1,2,3,4,6,8,12,24\}$ Subgroup of order $24$ $S_4$ is subgroup of order $24$ in $S_4$ (Trivial) Subgroup of order $12$ I have proved sometime back that $A_4$ is the only subgroup of $S_4$ having order $12$ but i could not recall the proof now (I would be thankful if some one can give some hint). Subgroups of order $8$ As $|S_4|=2^3.3$, there does exist a sylow $2$ subgroup i.e., subgroup of order $8$ in $S_4$. Only groups (upto isomorphism) of order $8$ are : $\mathbb{Z}_8,\mathbb{Z}_4\times \mathbb{Z}_2, \mathbb{Z}_2\times \mathbb{Z}_2\times \mathbb{Z}_2, D_8,Q_8$. $\mathbb{Z}_8$ can not be a subgroup of  $S_4$ as $S_4$ does not have an element of order $8$. With lot of labor work i could see that no element of order $4$ commutes with an element of order $2$ thus $\mathbb{Z}_4\times \mathbb{Z}_2$ can not be subgroup of $S_4$.( Help needed ) As of now i can not conclude why  (I know it is but not why) $ \mathbb{Z}_2\times \mathbb{Z}_2\times \mathbb{Z}_2$ is a subgroup of $S_4$. ( Help needed ) I know $D_8$ can be seen as subgroup of $S_4$. I know $Q_8$ can not be a subgroup of $S_4$. Subgroups of order $6$ Only groups of order $6$ are $\mathbb{Z}_6$ and $S_3$. $\mathbb{Z}_6$ can not be subgroup of $S_4$ as $S_4$ do not have an element of order $6$. $S_3$ is subgroup of $S_4$. This is not transitive because  : Suppose $S_3$ fixes $4$ then there is no way i can get an element which maps say $3$ to $4$. Thus $S_3$ is not transitive subgroup of $S_4$.. So, this $S_3$ is out of this game. Subgroups of order $4$ Only subgroups of order $4$ are $\mathbb{Z}_4$ and $\mathbb{Z}_2\times \mathbb{Z}_2$. $\mathbb{Z}_4$ is a subgroup of $S_4$ as $\{Id, (1234),(13)(24),(1432)\}$ which can be easily seen to be transitive.. In $\mathbb{Z}_4$,  i can send  $1$ to $2$ with $(1234)$ ; $1$ to $3$ with $(13)(24)$ and $1$ to $4$ with $(1432)$ similarly i can other elements to every othe element.. So, $\mathbb{Z}_4$ is seen to be transitive subgroup of $S_4$. $\mathbb{Z}_2\times \mathbb{Z}_2$ is a subgroup of $S_4$ seen as $\{Id, (12)(34),(13)(24),(14)(23)\}$ . In this also i can send each element to all other elements and thus $\mathbb{Z}_2\times \mathbb{Z}_2$ is a transitive subgroup of $S_4$. Subgroups of order $3$ No subgroup of order $3$ can be transitive for the same reason (actually for a more simple reason) as why $S_3$ is not transitive. Subgroups of order $2$ No subgroup of order $2$ can be transitive for the same reason (actually for a more simple reason)as why $S_3$ is not transitive. Subgroups of order $1$ Trivial subgroup is not transitive. So, Only subgroups of importance (transitive subgroups) i am worried about are.... $S_4$ which is trivially transitive $A_4$ is transitive. $D_8$ is  transitive. $\mathbb{Z}_4$ is  transitive. $\mathbb{Z}_2\times \mathbb{Z}_2$ is  transitive. I would be thankful if some one can help me to make this a bit more clear and simple..","Actual Question is  : What are all possible galois groups of an irreducible Quartic polynomial over $\mathbb{Q}$ As polynomial is irreducible, Galois group is transitive subgroup of $S_4$. I have no idea what (actually why) are all transitive subgroups of $S_4$. Firstly, I would distinguish what are all possible subgroups (and then see for transitive subgroups) of $S_4$. As $|S_4|=24$ only possible orders of subgroups of $S_4$ are $\{1,2,3,4,6,8,12,24\}$ Subgroup of order $24$ $S_4$ is subgroup of order $24$ in $S_4$ (Trivial) Subgroup of order $12$ I have proved sometime back that $A_4$ is the only subgroup of $S_4$ having order $12$ but i could not recall the proof now (I would be thankful if some one can give some hint). Subgroups of order $8$ As $|S_4|=2^3.3$, there does exist a sylow $2$ subgroup i.e., subgroup of order $8$ in $S_4$. Only groups (upto isomorphism) of order $8$ are : $\mathbb{Z}_8,\mathbb{Z}_4\times \mathbb{Z}_2, \mathbb{Z}_2\times \mathbb{Z}_2\times \mathbb{Z}_2, D_8,Q_8$. $\mathbb{Z}_8$ can not be a subgroup of  $S_4$ as $S_4$ does not have an element of order $8$. With lot of labor work i could see that no element of order $4$ commutes with an element of order $2$ thus $\mathbb{Z}_4\times \mathbb{Z}_2$ can not be subgroup of $S_4$.( Help needed ) As of now i can not conclude why  (I know it is but not why) $ \mathbb{Z}_2\times \mathbb{Z}_2\times \mathbb{Z}_2$ is a subgroup of $S_4$. ( Help needed ) I know $D_8$ can be seen as subgroup of $S_4$. I know $Q_8$ can not be a subgroup of $S_4$. Subgroups of order $6$ Only groups of order $6$ are $\mathbb{Z}_6$ and $S_3$. $\mathbb{Z}_6$ can not be subgroup of $S_4$ as $S_4$ do not have an element of order $6$. $S_3$ is subgroup of $S_4$. This is not transitive because  : Suppose $S_3$ fixes $4$ then there is no way i can get an element which maps say $3$ to $4$. Thus $S_3$ is not transitive subgroup of $S_4$.. So, this $S_3$ is out of this game. Subgroups of order $4$ Only subgroups of order $4$ are $\mathbb{Z}_4$ and $\mathbb{Z}_2\times \mathbb{Z}_2$. $\mathbb{Z}_4$ is a subgroup of $S_4$ as $\{Id, (1234),(13)(24),(1432)\}$ which can be easily seen to be transitive.. In $\mathbb{Z}_4$,  i can send  $1$ to $2$ with $(1234)$ ; $1$ to $3$ with $(13)(24)$ and $1$ to $4$ with $(1432)$ similarly i can other elements to every othe element.. So, $\mathbb{Z}_4$ is seen to be transitive subgroup of $S_4$. $\mathbb{Z}_2\times \mathbb{Z}_2$ is a subgroup of $S_4$ seen as $\{Id, (12)(34),(13)(24),(14)(23)\}$ . In this also i can send each element to all other elements and thus $\mathbb{Z}_2\times \mathbb{Z}_2$ is a transitive subgroup of $S_4$. Subgroups of order $3$ No subgroup of order $3$ can be transitive for the same reason (actually for a more simple reason) as why $S_3$ is not transitive. Subgroups of order $2$ No subgroup of order $2$ can be transitive for the same reason (actually for a more simple reason)as why $S_3$ is not transitive. Subgroups of order $1$ Trivial subgroup is not transitive. So, Only subgroups of importance (transitive subgroups) i am worried about are.... $S_4$ which is trivially transitive $A_4$ is transitive. $D_8$ is  transitive. $\mathbb{Z}_4$ is  transitive. $\mathbb{Z}_2\times \mathbb{Z}_2$ is  transitive. I would be thankful if some one can help me to make this a bit more clear and simple..",,"['abstract-algebra', 'proof-verification']"
9,"""Vector spaces"" over a skew-field are free?","""Vector spaces"" over a skew-field are free?",,"Are modules over a skew field free? That is, if $F$ is a skewfield then can any module $M$ be written as $\underset{i \in I}{\bigoplus} F$ for some indexing set $I$?","Are modules over a skew field free? That is, if $F$ is a skewfield then can any module $M$ be written as $\underset{i \in I}{\bigoplus} F$ for some indexing set $I$?",,"['abstract-algebra', 'vector-spaces', 'modules', 'noncommutative-algebra', 'division-algebras']"
10,Are there real algebras that don't have rational structure constants?,Are there real algebras that don't have rational structure constants?,,"Take a finite dimensional associative algebra $A$ over the reals. Fix a basis $\{x_1, x_2, \ldots x_n\}$. The multiplication is completely specified by specifying structure constants $c^{ij}_k$ defined by the following equation: $$x_i \cdot x_j = \sum_k c^{ij}_k x_k \quad\forall i, j$$ Of course, the structure constants depend on the choice of basis. My question is: Are there algebras such that no choice of basis leads to structure constants that are all rational? My conjectured example would be the algebra spanned by two variables $a$ and $b$, and relations $a^2 = a, ab = ba = b, b^2 = \sqrt{2}a$, but I couldn't prove it yet. Note: One could also ask ""integer"" instead of ""rational"", this is equivalent.","Take a finite dimensional associative algebra $A$ over the reals. Fix a basis $\{x_1, x_2, \ldots x_n\}$. The multiplication is completely specified by specifying structure constants $c^{ij}_k$ defined by the following equation: $$x_i \cdot x_j = \sum_k c^{ij}_k x_k \quad\forall i, j$$ Of course, the structure constants depend on the choice of basis. My question is: Are there algebras such that no choice of basis leads to structure constants that are all rational? My conjectured example would be the algebra spanned by two variables $a$ and $b$, and relations $a^2 = a, ab = ba = b, b^2 = \sqrt{2}a$, but I couldn't prove it yet. Note: One could also ask ""integer"" instead of ""rational"", this is equivalent.",,['abstract-algebra']
11,Visualising a group structure.,Visualising a group structure.,,"I've found something really interesting, but it seems to ""big"" to investigate. If you take the direct product of some cyclic groups, sometimes interesting things happen. Let $C_n$ be a cyclic group of order $n$, if you consider $C_n\times C_m$ when $n$ and $m$ are co-prime something interesting happens. I proved that the result is (isomorphic to a) cyclic group if $m$ and $n$ are co-prime already, this is about something more interesting. I use $\langle g\rangle$ to denote the group $\{g^0,g^1,g^2,...,g^k\}$ A pattern emerges when you consider things like $\langle(g_1,g_2)\rangle$. I used the multiplicative modulo groups (as I know (if $n$ is prime at least) that these can be generated/are cyclic -can someone confirm this for all?) The group $C_2 \times C_2$ is interesting, it gives: $(1,1)$ $(1,2)\rightarrow(1,1)\rightarrow(1,2)$ $(2,1)\rightarrow(1,1)\rightarrow(2,1)$ $(2,2)\rightarrow(1,1)\rightarrow(2,2)$ Which can be drawn nicely as a graph ($(1,1)$ at the centre, surrounded by the nodes $(1,2)$ $(2,1)$ and $(2,2)$ with a line coming from $(1,1)$ to it, and another arc going back) I'm curious as to the larger pattern here, but if I try ... say $C_4 \times C_4$ (from multiplication under modulo $5$ for instance) what do I get? There are $16$ different ""chains"" to generate, this would take a while. Is there a better way to explore this? I am really curious but right now Graphviz and a python script seem like a good way, but this just shows the structure, it doesn't really explore it! It is also interesting that the graph is planar, well I think so anyway. What have I discovered?","I've found something really interesting, but it seems to ""big"" to investigate. If you take the direct product of some cyclic groups, sometimes interesting things happen. Let $C_n$ be a cyclic group of order $n$, if you consider $C_n\times C_m$ when $n$ and $m$ are co-prime something interesting happens. I proved that the result is (isomorphic to a) cyclic group if $m$ and $n$ are co-prime already, this is about something more interesting. I use $\langle g\rangle$ to denote the group $\{g^0,g^1,g^2,...,g^k\}$ A pattern emerges when you consider things like $\langle(g_1,g_2)\rangle$. I used the multiplicative modulo groups (as I know (if $n$ is prime at least) that these can be generated/are cyclic -can someone confirm this for all?) The group $C_2 \times C_2$ is interesting, it gives: $(1,1)$ $(1,2)\rightarrow(1,1)\rightarrow(1,2)$ $(2,1)\rightarrow(1,1)\rightarrow(2,1)$ $(2,2)\rightarrow(1,1)\rightarrow(2,2)$ Which can be drawn nicely as a graph ($(1,1)$ at the centre, surrounded by the nodes $(1,2)$ $(2,1)$ and $(2,2)$ with a line coming from $(1,1)$ to it, and another arc going back) I'm curious as to the larger pattern here, but if I try ... say $C_4 \times C_4$ (from multiplication under modulo $5$ for instance) what do I get? There are $16$ different ""chains"" to generate, this would take a while. Is there a better way to explore this? I am really curious but right now Graphviz and a python script seem like a good way, but this just shows the structure, it doesn't really explore it! It is also interesting that the graph is planar, well I think so anyway. What have I discovered?",,"['abstract-algebra', 'group-theory']"
12,Show $S_4$ is not isomorphic to $D_{24}$ by looking at their centers,Show  is not isomorphic to  by looking at their centers,S_4 D_{24},Every proof seems to use an argument that looks at the orders and finds an element with a certain order in $S_4$ and no element in $D_{24}$ has that order... Wouldn't it be much much easier to look at the centers? The center of $S_4$ is just $\{e\}$ but the center of $D_{24}$ has two elements in it... thus an isomorphism cannot exist.,Every proof seems to use an argument that looks at the orders and finds an element with a certain order in $S_4$ and no element in $D_{24}$ has that order... Wouldn't it be much much easier to look at the centers? The center of $S_4$ is just $\{e\}$ but the center of $D_{24}$ has two elements in it... thus an isomorphism cannot exist.,,"['abstract-algebra', 'group-theory', 'finite-groups']"
13,"How to prove $\mathbb{Z}[\sqrt{2}i]=\{a+b\sqrt{2}i\mid a,b\in\mathbb{Z}\}$ is a principal ideal domain?",How to prove  is a principal ideal domain?,"\mathbb{Z}[\sqrt{2}i]=\{a+b\sqrt{2}i\mid a,b\in\mathbb{Z}\}","How to prove $\mathbb{Z}[\sqrt{2}i]=\{a+b\sqrt{2}i\mid a,b\in\mathbb{Z}\}$ is a principal ideal domain? I can prove it is unique factorization domain. Moreover, how to prove $\mathbb{Z}[\sqrt{n}i]=\{a+b\sqrt{n}i\mid a,b\in\mathbb{Z}\}$ is not unique factorization domain for all $n\geq 3$ hence they are not principal ideal domain?","How to prove $\mathbb{Z}[\sqrt{2}i]=\{a+b\sqrt{2}i\mid a,b\in\mathbb{Z}\}$ is a principal ideal domain? I can prove it is unique factorization domain. Moreover, how to prove $\mathbb{Z}[\sqrt{n}i]=\{a+b\sqrt{n}i\mid a,b\in\mathbb{Z}\}$ is not unique factorization domain for all $n\geq 3$ hence they are not principal ideal domain?",,"['abstract-algebra', 'ring-theory']"
14,"Let $R = \mathbb Z[i]$. Show $I \cap \mathbb Z$ is an ideal in $\mathbb Z$, for all $a \in I \cap \mathbb Z$, $10 \mid a^2 = N(a)$.","Let . Show  is an ideal in , for all , .",R = \mathbb Z[i] I \cap \mathbb Z \mathbb Z a \in I \cap \mathbb Z 10 \mid a^2 = N(a),"Let $R = \mathbb Z[i]$, $z = 3+i$ and $I = \langle z \rangle$. I need to show $I \cap \mathbb Z$ is an ideal in $\mathbb Z$, for all $a \in I \cap \mathbb Z$, $10 \mid a^2 = N(a)$ and $10 \mid a$, and $10\mathbb Z = I \cap \mathbb Z$. I have already proved that $10\mathbb Z \subset I$ since $N(z) = 10$.","Let $R = \mathbb Z[i]$, $z = 3+i$ and $I = \langle z \rangle$. I need to show $I \cap \mathbb Z$ is an ideal in $\mathbb Z$, for all $a \in I \cap \mathbb Z$, $10 \mid a^2 = N(a)$ and $10 \mid a$, and $10\mathbb Z = I \cap \mathbb Z$. I have already proved that $10\mathbb Z \subset I$ since $N(z) = 10$.",,"['abstract-algebra', 'elementary-number-theory', 'ring-theory', 'ideals']"
15,"Any group of order $n$ satisfying $\gcd (n, \varphi(n)) =1$ is cyclic",Any group of order  satisfying  is cyclic,"n \gcd (n, \varphi(n)) =1","Sorry for the last mistaken problem I just posted. Now I know that only having the order being odd square free is not enough for a group to be cyclic. Here's the complete problem which the main goal is to show that any group of order $n$ satisfies $\gcd (n,\varphi(n))=1$ is cyclic. It asks me to do it in the following way: (a) If $n > 2$ , $n$ is an odd squarefree integer. (b) Show that there is a $H = G/N$ of prime order. (c) Show that $G \cong N \times H$ and $G$ is abelian. (d) $G$ is cyclic. Any ideas is appreciated.","Sorry for the last mistaken problem I just posted. Now I know that only having the order being odd square free is not enough for a group to be cyclic. Here's the complete problem which the main goal is to show that any group of order satisfies is cyclic. It asks me to do it in the following way: (a) If , is an odd squarefree integer. (b) Show that there is a of prime order. (c) Show that and is abelian. (d) is cyclic. Any ideas is appreciated.","n \gcd (n,\varphi(n))=1 n > 2 n H = G/N G \cong N \times H G G","['abstract-algebra', 'group-theory', 'finite-groups', 'cyclic-groups', 'totient-function']"
16,Why do the interesting antihomomorphisms tend to be involutions?,Why do the interesting antihomomorphisms tend to be involutions?,,"Given a semigroup $S$, define that an antihomomorphism on $S$ is a function $$* :S \rightarrow S$$ satisfying $(xy)^* = y^*x^*.$ Examples abound. Consider: Transposition, where $S$ equals the set of $2 \times 2$ real matrices. Conjugate-transposition, where $S$ equals the set of $2 \times 2$ complex matrices. The map that takes a binary relation to its converse, where $S$ equals the monoid of binary relations on a set $X$. Inversion, in any group. The weird thing is that in all of the above examples, the star operation is actually involutive . In fact, off the top of my head I can't think of any non-trivial antihomomorphisms that aren't also involutions. Why do the antihomomorphisms of interest tend to be involutions? I mean, is there some sort of ""killer theorem"" or something, that just makes involutive antihomomorphisms totally awesome? Conversely, I am also interested in examples of antihomomorphisms that fail to be involutions, but which are still deemed important.","Given a semigroup $S$, define that an antihomomorphism on $S$ is a function $$* :S \rightarrow S$$ satisfying $(xy)^* = y^*x^*.$ Examples abound. Consider: Transposition, where $S$ equals the set of $2 \times 2$ real matrices. Conjugate-transposition, where $S$ equals the set of $2 \times 2$ complex matrices. The map that takes a binary relation to its converse, where $S$ equals the monoid of binary relations on a set $X$. Inversion, in any group. The weird thing is that in all of the above examples, the star operation is actually involutive . In fact, off the top of my head I can't think of any non-trivial antihomomorphisms that aren't also involutions. Why do the antihomomorphisms of interest tend to be involutions? I mean, is there some sort of ""killer theorem"" or something, that just makes involutive antihomomorphisms totally awesome? Conversely, I am also interested in examples of antihomomorphisms that fail to be involutions, but which are still deemed important.",,"['abstract-algebra', 'intuition', 'semigroups']"
17,"Projective Modules, Annihilators, and Idempotents","Projective Modules, Annihilators, and Idempotents",,"Let $Ra$ be the left ideal of a ring $R$ generated by an element $a \in R$. Show that $Ra$ is a projective left $R$-module if and only if the left annihilator of $a$, $\{r \in R \mid ra = 0\}$ is of the form $Re$ for some idempotent element $e \in R$. Note: I know that for an idempotent e, $Re$ is a projective left R-module, since $R \cong Re \bigoplus R(1-e)$ shows that $Re$ is a direct summand of a free module.  It seems that this fact may be useful here, but I am not sure how to proceed.","Let $Ra$ be the left ideal of a ring $R$ generated by an element $a \in R$. Show that $Ra$ is a projective left $R$-module if and only if the left annihilator of $a$, $\{r \in R \mid ra = 0\}$ is of the form $Re$ for some idempotent element $e \in R$. Note: I know that for an idempotent e, $Re$ is a projective left R-module, since $R \cong Re \bigoplus R(1-e)$ shows that $Re$ is a direct summand of a free module.  It seems that this fact may be useful here, but I am not sure how to proceed.",,"['abstract-algebra', 'ring-theory', 'modules', 'projective-module']"
18,An efficient method for discovering finite groups,An efficient method for discovering finite groups,,"I wrote a program to ""discover"", by means of constructing Cayley tables, finite groups. Due to the method used (the one describe on the wikipedia article for Cayley tables , starting with a identity skeleton) the runtime is on the order of $O(n!)$ This allows me to find small groups with order at most 12 before the time taken to generate the tables becomes impractical. Is there a faster method for finding finite simple groups? This clearly rules out composing them as a product of smaller groups. Is there a procedure that could discover the Monster group, for example?","I wrote a program to ""discover"", by means of constructing Cayley tables, finite groups. Due to the method used (the one describe on the wikipedia article for Cayley tables , starting with a identity skeleton) the runtime is on the order of $O(n!)$ This allows me to find small groups with order at most 12 before the time taken to generate the tables becomes impractical. Is there a faster method for finding finite simple groups? This clearly rules out composing them as a product of smaller groups. Is there a procedure that could discover the Monster group, for example?",,"['abstract-algebra', 'group-theory']"
19,Does my proof make sense?,Does my proof make sense?,,"Theorem: For groups $(\Bbb R,+)$ and $(\Bbb R,*)$ (both only dealing with positive integers) there is a function $\phi$ that turns $(\Bbb R,+)\to(\Bbb R,*)$ and vice versa. Proof: Assume $(\Bbb R,+)\to(\Bbb R,*)$. So there is a function where elements $x_1,x_2$ going from additive operation to multiplicative operation where $x_1+x_2 \mapsto x_1 *x_2$. So there is some $\phi$ where $\phi(X_1*X_2)= \phi(x_1)*\phi(x_2)$ (here * is an operation) Take $\phi=e$, so $e^{x_1+x_2}=e^{x_1}*e^{x_2}$. Now the inverse of $e$ is $\ln$, so $\ln(x_1 *x_2) = \ln(x_1+x_2)$ I know there is a lot missing from the proof or at least it's not concrete by looking at it. I just need a little help in cleaning up the theorm and proof.","Theorem: For groups $(\Bbb R,+)$ and $(\Bbb R,*)$ (both only dealing with positive integers) there is a function $\phi$ that turns $(\Bbb R,+)\to(\Bbb R,*)$ and vice versa. Proof: Assume $(\Bbb R,+)\to(\Bbb R,*)$. So there is a function where elements $x_1,x_2$ going from additive operation to multiplicative operation where $x_1+x_2 \mapsto x_1 *x_2$. So there is some $\phi$ where $\phi(X_1*X_2)= \phi(x_1)*\phi(x_2)$ (here * is an operation) Take $\phi=e$, so $e^{x_1+x_2}=e^{x_1}*e^{x_2}$. Now the inverse of $e$ is $\ln$, so $\ln(x_1 *x_2) = \ln(x_1+x_2)$ I know there is a lot missing from the proof or at least it's not concrete by looking at it. I just need a little help in cleaning up the theorm and proof.",,['abstract-algebra']
20,A question based on Normal Subgroups,A question based on Normal Subgroups,,"Question: Let $G$ be a group and $H$ a subgroup of $G$. A conjugacy class of an element $α∈G$ is the set $\def\CC{\mathop{\rm CC}}\CC(a)=\{ g^{-1} ag \mid  g∈G\}$. Prove that $H$ is the union of conjugacy classes if and only if $H$ is normal in $G$. My Answer: Prove that if $H$ is normal in $G$ then $H$ is the union of conjugacy classes. Assume that $H$ is not the union of the conjugacy classes. This means that $H$ does not have an element that is of the form $g^{-1} ag$ where $a∈G$. Now, $H$ is normal in $G$, then $gH=Hg$ for all $g∈G$. Hence, for some $g∈G$ and $h∈H$ there exist $h'∈H$ such that $gh=h' g$. This shows that $h=g^{-1} h' g∈H$ which contradicts that $H$ does not have any element of the form $g^{-1} ag$, $a∈G$. Thus, it is proven that if $H$ is normal in $G$, then $H$ is the union of conjugacy classes. Conversely, prove that if $H$ is the union of conjugacy classes then $H$ is normal in $G$. A property of subgroup $N$ being normal to group $M$ is that for all $m∈M$, $mNm^{-1}⊆N$. Must then show that for all $g∈G$, $gHg^{-1}⊆H$. Let $a∈ gHg^{-1}$, then $a=ghg^{-1}$  for some $h∈H$. Now, the element $$a=ghg^{-1}=(g^{-1} )^{-1} hg^{-1}∈H.$$ Thus, by the stated property, $H$ is a normal subgroup in $G$. Therefore it is proven that $H$ is the union of conjugacy classes if and only if $H$ is normal in $G$. Is my answer correct or do i need to modify it? Kindly state if needed. Thanks","Question: Let $G$ be a group and $H$ a subgroup of $G$. A conjugacy class of an element $α∈G$ is the set $\def\CC{\mathop{\rm CC}}\CC(a)=\{ g^{-1} ag \mid  g∈G\}$. Prove that $H$ is the union of conjugacy classes if and only if $H$ is normal in $G$. My Answer: Prove that if $H$ is normal in $G$ then $H$ is the union of conjugacy classes. Assume that $H$ is not the union of the conjugacy classes. This means that $H$ does not have an element that is of the form $g^{-1} ag$ where $a∈G$. Now, $H$ is normal in $G$, then $gH=Hg$ for all $g∈G$. Hence, for some $g∈G$ and $h∈H$ there exist $h'∈H$ such that $gh=h' g$. This shows that $h=g^{-1} h' g∈H$ which contradicts that $H$ does not have any element of the form $g^{-1} ag$, $a∈G$. Thus, it is proven that if $H$ is normal in $G$, then $H$ is the union of conjugacy classes. Conversely, prove that if $H$ is the union of conjugacy classes then $H$ is normal in $G$. A property of subgroup $N$ being normal to group $M$ is that for all $m∈M$, $mNm^{-1}⊆N$. Must then show that for all $g∈G$, $gHg^{-1}⊆H$. Let $a∈ gHg^{-1}$, then $a=ghg^{-1}$  for some $h∈H$. Now, the element $$a=ghg^{-1}=(g^{-1} )^{-1} hg^{-1}∈H.$$ Thus, by the stated property, $H$ is a normal subgroup in $G$. Therefore it is proven that $H$ is the union of conjugacy classes if and only if $H$ is normal in $G$. Is my answer correct or do i need to modify it? Kindly state if needed. Thanks",,"['abstract-algebra', 'group-theory', 'normal-subgroups']"
21,Is there a different copy of $\Bbb{R}$ in $\Bbb{C}$ such that the extension is algebraic?,Is there a different copy of  in  such that the extension is algebraic?,\Bbb{R} \Bbb{C},"Since $\Bbb{C}$ contains a non-trivial copy of itself, I know that there are multiple subfields of $\Bbb{C}$ isomorphic to $\Bbb{R}$. But these inclusions make the extension non-algebraic. So are there other ways of including $\Bbb{R}$ in $\Bbb{C}$ (as a field) keeping the extension algebraic? Or is it that, given $\Bbb{C}$, there is an algebraic way to define the usual copy of $\Bbb{R}$? I can see that this is equivalent to asking for an order 2 automorphism of $\Bbb{C}$ which is not the usual conjugation. But I could neither come up with one, nor prove it does not exist. More generally, if $\overline F$ is finite over $F$, is the inclusion unique?","Since $\Bbb{C}$ contains a non-trivial copy of itself, I know that there are multiple subfields of $\Bbb{C}$ isomorphic to $\Bbb{R}$. But these inclusions make the extension non-algebraic. So are there other ways of including $\Bbb{R}$ in $\Bbb{C}$ (as a field) keeping the extension algebraic? Or is it that, given $\Bbb{C}$, there is an algebraic way to define the usual copy of $\Bbb{R}$? I can see that this is equivalent to asking for an order 2 automorphism of $\Bbb{C}$ which is not the usual conjugation. But I could neither come up with one, nor prove it does not exist. More generally, if $\overline F$ is finite over $F$, is the inclusion unique?",,"['abstract-algebra', 'field-theory']"
22,"If $H\leq G$ is of finite index, then, we have a normal subgroup $N\leq G$ of finite index.","If  is of finite index, then, we have a normal subgroup  of finite index.",H\leq G N\leq G,"Question is to prove that : Let $G$ be a group with a proper subgroup $H$ of finite index. Show that $G$ has a proper normal subgroup of finite index. what i have tried is : Assuming $|G/H|=n< \infty$ let $G$ act on set of left cosets of $H$ in $G$ and this would give me $\eta :G\rightarrow S_n$  a homomorphism.. As $Ker(\eta)\unlhd G$ we do have a normal subgroup of $G$. But, this question is asked before introducing group actions. So, another solution without group actions is what i am trying to get. I would like somebody to check my solution and let me know if there is any possible solution with out using group actions... we assume $G$ to be infinite group. This is because in any group $Z(G)\unlhd G$ and the index of  $Z(G)$ would be finite in ""finite groups"".. Thank you.","Question is to prove that : Let $G$ be a group with a proper subgroup $H$ of finite index. Show that $G$ has a proper normal subgroup of finite index. what i have tried is : Assuming $|G/H|=n< \infty$ let $G$ act on set of left cosets of $H$ in $G$ and this would give me $\eta :G\rightarrow S_n$  a homomorphism.. As $Ker(\eta)\unlhd G$ we do have a normal subgroup of $G$. But, this question is asked before introducing group actions. So, another solution without group actions is what i am trying to get. I would like somebody to check my solution and let me know if there is any possible solution with out using group actions... we assume $G$ to be infinite group. This is because in any group $Z(G)\unlhd G$ and the index of  $Z(G)$ would be finite in ""finite groups"".. Thank you.",,['abstract-algebra']
23,Abelian subgroup in a 2 group.,Abelian subgroup in a 2 group.,,"Let $G$ be a non-abelian 2-group of order greater than or equal to 32 and $|Z(G)|=4$. Does the group $G$ has an abelian subgroup $H$, such that $16 \leq |H| \leq |G|/2$?","Let $G$ be a non-abelian 2-group of order greater than or equal to 32 and $|Z(G)|=4$. Does the group $G$ has an abelian subgroup $H$, such that $16 \leq |H| \leq |G|/2$?",,"['abstract-algebra', 'group-theory', 'finite-groups', '2-groups']"
24,"Does ""maximal submodule <=> simple quotient module"" generalize to abelian categories?","Does ""maximal submodule <=> simple quotient module"" generalize to abelian categories?",,"Does the statement ""If $A$, $B$ are modules over a commutative ring $R$, then $B$ is a maximal submodule of $A$ if and only if $A/B$ is a simple module"" generalize to the setting of abelian categories?  That is, is it true to say ""If C is an abelian category and $A, B \in$ C , then $B$ is a maximal subobject of $A$ if and only if $A/B$ is a simple object?"" My hunch is that this statement does generalize, since I've seen it stated that the Jordan-Holder theorem for abelian categories is a ""straightforward generalization"" of the version for modules (wherein this fact is used), but I've had a lot of trouble finding anything on maximal subobjects at all and I haven't figured out the details yet.","Does the statement ""If $A$, $B$ are modules over a commutative ring $R$, then $B$ is a maximal submodule of $A$ if and only if $A/B$ is a simple module"" generalize to the setting of abelian categories?  That is, is it true to say ""If C is an abelian category and $A, B \in$ C , then $B$ is a maximal subobject of $A$ if and only if $A/B$ is a simple object?"" My hunch is that this statement does generalize, since I've seen it stated that the Jordan-Holder theorem for abelian categories is a ""straightforward generalization"" of the version for modules (wherein this fact is used), but I've had a lot of trouble finding anything on maximal subobjects at all and I haven't figured out the details yet.",,"['abstract-algebra', 'category-theory', 'modules', 'abelian-categories']"
25,"Suppose that characteristic $F$ is $p$. If $K/F$ is separable then $K = F(K^{p})$ where $K^{p} = \{ x^{p} \, |\, x\in K\}$.",Suppose that characteristic  is . If  is separable then  where .,"F p K/F K = F(K^{p}) K^{p} = \{ x^{p} \, |\, x\in K\}","I am having difficulty finishing this problem.  So far I have this: Want to show $K \subset F(K^{p})$.  Since $K/F$ is separable then $K/F$ is algebraic.  In particular, $\alpha\in K$ is separable over $F$ hence separable over $F(\alpha^{p})$.  So $\alpha\in F(\alpha^{p})$ by an exercise already done.  But then I do not know how to conclude $\alpha\in F(K^{p})$. Any suggestions?  Or is my approach completely wrong?","I am having difficulty finishing this problem.  So far I have this: Want to show $K \subset F(K^{p})$.  Since $K/F$ is separable then $K/F$ is algebraic.  In particular, $\alpha\in K$ is separable over $F$ hence separable over $F(\alpha^{p})$.  So $\alpha\in F(\alpha^{p})$ by an exercise already done.  But then I do not know how to conclude $\alpha\in F(K^{p})$. Any suggestions?  Or is my approach completely wrong?",,"['abstract-algebra', 'field-theory']"
26,"""Almost"" ring homomorphism","""Almost"" ring homomorphism",,"This is an exercise out of Herstein which seems pretty straightforward but is eluding me. Let $R,R'$ be rings and let $\phi:R\to R'$ be a mapping such that, for every $x,y\in R$: $${\rm 1.}\qquad\phi(x+y)=\phi(x)+\phi(y)$$ $${\rm 2.}\qquad\phi(xy)=\phi(x)\phi(y)\qquad{\rm or}\qquad\phi(xy)=\phi(y)\phi(x)$$ I'm to show that one of the conditions holds uniformly. That is, for all $x,y\in R$, $\phi(xy)=\phi(x)\phi(y)$ or, for all $x,y\in R$, $\phi(xy)=\phi(y)\phi(x)$. Of course, this doesn't exclude the possibility that both conditions hold uniformly (which happens when e.g. $R'$ is commutative). Herstein hints to fix $a\in R$ and to consider the sets $$W_a=\{x\in R\mid\phi(ax)=\phi(a)\phi(x)\},$$ $$V_a=\{x\in R\mid\phi(ax)=\phi(x)\phi(a)\}.$$ I have shown that one of $W_a$ or $V_a$ must be the entire ring $R$. I think I'm missing something simple to complete the argument. Any hints? Edit I forgot to mention that in my copy of the text, the condition to be shown is stated as ""$\phi(xy)=\phi(x)\phi(y)$ or $\phi(x)=\phi(y)\phi(x)$"". I am pretty confident that it's a typo, but maybe not?","This is an exercise out of Herstein which seems pretty straightforward but is eluding me. Let $R,R'$ be rings and let $\phi:R\to R'$ be a mapping such that, for every $x,y\in R$: $${\rm 1.}\qquad\phi(x+y)=\phi(x)+\phi(y)$$ $${\rm 2.}\qquad\phi(xy)=\phi(x)\phi(y)\qquad{\rm or}\qquad\phi(xy)=\phi(y)\phi(x)$$ I'm to show that one of the conditions holds uniformly. That is, for all $x,y\in R$, $\phi(xy)=\phi(x)\phi(y)$ or, for all $x,y\in R$, $\phi(xy)=\phi(y)\phi(x)$. Of course, this doesn't exclude the possibility that both conditions hold uniformly (which happens when e.g. $R'$ is commutative). Herstein hints to fix $a\in R$ and to consider the sets $$W_a=\{x\in R\mid\phi(ax)=\phi(a)\phi(x)\},$$ $$V_a=\{x\in R\mid\phi(ax)=\phi(x)\phi(a)\}.$$ I have shown that one of $W_a$ or $V_a$ must be the entire ring $R$. I think I'm missing something simple to complete the argument. Any hints? Edit I forgot to mention that in my copy of the text, the condition to be shown is stated as ""$\phi(xy)=\phi(x)\phi(y)$ or $\phi(x)=\phi(y)\phi(x)$"". I am pretty confident that it's a typo, but maybe not?",,"['abstract-algebra', 'ring-theory']"
27,Express $4+\sqrt{-2}$ as a product of irreducibles,Express  as a product of irreducibles,4+\sqrt{-2},"This is part of an old Oxford Part A exam paper. (1992 A1) Suppose we equip $R=\mathbb{Z}[\sqrt{-2}]$ with the Euclidean function $d$ defined by $$d(m+n\sqrt{-2})=|m+n\sqrt{-2}|^2$$ I want to determine the units of $R$ and express $4+\sqrt{-2}$ as a product of irreducibles and to use this to determine how many ideals of $R$ contain $4+\sqrt{-2}$. Progress I think I have shown that the only units are $1,-1$, but cannot see how one can show this element is a product of irreducibles. In general, we have that in a Euclidean Domain every element is a finite product of irreducibles, but I cannot see how to calculate them. Thanks","This is part of an old Oxford Part A exam paper. (1992 A1) Suppose we equip $R=\mathbb{Z}[\sqrt{-2}]$ with the Euclidean function $d$ defined by $$d(m+n\sqrt{-2})=|m+n\sqrt{-2}|^2$$ I want to determine the units of $R$ and express $4+\sqrt{-2}$ as a product of irreducibles and to use this to determine how many ideals of $R$ contain $4+\sqrt{-2}$. Progress I think I have shown that the only units are $1,-1$, but cannot see how one can show this element is a product of irreducibles. In general, we have that in a Euclidean Domain every element is a finite product of irreducibles, but I cannot see how to calculate them. Thanks",,"['abstract-algebra', 'ring-theory']"
28,Order of elements in a group.,Order of elements in a group.,,"Corollary 4.6.8. There is a group $G$ of order $n^3$ given by $G = \{b^ic^ja^k | 0 ≤ i, j, k < n\}$ , where $a$ , $b$ , and $c$ all have order $n$ , and $b$ commutes with $c$ , $a$ commutes with $c$ , and $aba^{−1} = bc$ . Thus, $$(b^ic^ja^k) (b^{i'}c^{j'}a^{k'}) = b^{i+i'}c^{j+j'+ki'}a^{k+k'}.$$ a) Consider the groups constructed in Corollary 4.6.8 for $n$ a prime number $p$ . When $p>2$ , show that every element in this group has exponent $p$ . (Why is this different from the case $n=2$ ?) Recall that in Problem 3 of Exercises 2.1.16 it was shown that any group of exponent $2$ must be abelian. As we see here this property is not shared by primes $p>2$ . b) Now consider the groups of Corollary 4.6.8 for general values of $n$ . For which values of $n$ does the group in question have exponent $n$ ? If the exponent is not equal to $n$ , what is it? This is what I did: I first tried to see if there was any pattern that I could detect by multiplying the elements $b^ic^ja^k = b^ia^kc^j$ over an over again. This is what I got: $(bac)(bac) = b^2a^ac^{2+1}$ $(b^2a^ac^{2+1})(bac) = b^3a^3c^{4+2}$ $(b^3a^3c^{4+2})(bac) = b^4a^4c^{7+3}$ $(b^4a^4c^{7+3})(bac) = b^5a^5c^{11+4}$ But I wasn't sure if this would help me in finding the order of bca...however, the corollary tells us that $aba^{-1}=bc$ , so $c=b^{-1}aba^{-1}$ , right? So I tried using that, and we have: $(bca) = bb^{-1}aba^{-1}a = ab$ . However, this doesn't really help much either. So I was wondering if anybody could help me with this... Thank you in advance.","Corollary 4.6.8. There is a group of order given by , where , , and all have order , and commutes with , commutes with , and . Thus, a) Consider the groups constructed in Corollary 4.6.8 for a prime number . When , show that every element in this group has exponent . (Why is this different from the case ?) Recall that in Problem 3 of Exercises 2.1.16 it was shown that any group of exponent must be abelian. As we see here this property is not shared by primes . b) Now consider the groups of Corollary 4.6.8 for general values of . For which values of does the group in question have exponent ? If the exponent is not equal to , what is it? This is what I did: I first tried to see if there was any pattern that I could detect by multiplying the elements over an over again. This is what I got: But I wasn't sure if this would help me in finding the order of bca...however, the corollary tells us that , so , right? So I tried using that, and we have: . However, this doesn't really help much either. So I was wondering if anybody could help me with this... Thank you in advance.","G n^3 G = \{b^ic^ja^k | 0 ≤ i, j, k < n\} a b c n b c a c aba^{−1} = bc (b^ic^ja^k) (b^{i'}c^{j'}a^{k'}) = b^{i+i'}c^{j+j'+ki'}a^{k+k'}. n p p>2 p n=2 2 p>2 n n n n b^ic^ja^k = b^ia^kc^j (bac)(bac) = b^2a^ac^{2+1} (b^2a^ac^{2+1})(bac) = b^3a^3c^{4+2} (b^3a^3c^{4+2})(bac) = b^4a^4c^{7+3} (b^4a^4c^{7+3})(bac) = b^5a^5c^{11+4} aba^{-1}=bc c=b^{-1}aba^{-1} (bca) = bb^{-1}aba^{-1}a = ab",['abstract-algebra']
29,Suppose all the Associated Primes are Minimal.,Suppose all the Associated Primes are Minimal.,,"Let $R$ be a commutative Noetherian ring with unit and let $I$ be a fixed ideal. I am sorry if the following turns out to be a very silly question. 1) Suppose $\operatorname{Ass}(R/I)$ are all minimal, then what can we say about $R/I$? One conclusion is that $I$ has a unique primary decomposition. If $R/I$ is CM, then $\operatorname{Ass}(R/I)$ are all minimal, but I don't think the converse is true. Can someone provide a counterexample? 2) Suppose $\operatorname{Ass}(R/I)$ are all minimal and have the same height. I still dont think we can conclude that $R/I$ is CM. Can someone provide a counterexample? Theorem 2.1.6 of Bruns and Herzog, CM Rings , gives a result in this direction but it needs additional hypothesis. It says that if every ideal $I$ is generated by height $I$ elements and $\operatorname{Ass}(R/I)$ are all minimal, then $R$ is CM. It would be nice if someone can say something nice about the ring in the special situation described in 1) and 2). Thanks.","Let $R$ be a commutative Noetherian ring with unit and let $I$ be a fixed ideal. I am sorry if the following turns out to be a very silly question. 1) Suppose $\operatorname{Ass}(R/I)$ are all minimal, then what can we say about $R/I$? One conclusion is that $I$ has a unique primary decomposition. If $R/I$ is CM, then $\operatorname{Ass}(R/I)$ are all minimal, but I don't think the converse is true. Can someone provide a counterexample? 2) Suppose $\operatorname{Ass}(R/I)$ are all minimal and have the same height. I still dont think we can conclude that $R/I$ is CM. Can someone provide a counterexample? Theorem 2.1.6 of Bruns and Herzog, CM Rings , gives a result in this direction but it needs additional hypothesis. It says that if every ideal $I$ is generated by height $I$ elements and $\operatorname{Ass}(R/I)$ are all minimal, then $R$ is CM. It would be nice if someone can say something nice about the ring in the special situation described in 1) and 2). Thanks.",,"['abstract-algebra', 'algebraic-geometry', 'commutative-algebra']"
30,Let G be a group and let $a\in G$. Define $F_a:G\mapsto G$ via $F_a(x)=axa^{-1}$ for all $a\in G$. Prove that $F_a$ is an isomorphism from G onto G,Let G be a group and let . Define  via  for all . Prove that  is an isomorphism from G onto G,a\in G F_a:G\mapsto G F_a(x)=axa^{-1} a\in G F_a,"This is what I have for my proof: $F_{a^{-1}}(F_{a}(x))=F_{a^{-1}}(axa^{-1})=a^{-1}axa^{-1}a=(a^{-1}a)x(a^{-1})(a)=1x1=x$ $F_{a}(F_{a^{-1}}(x))=F_{a}(a^{-1}xa)=aa^{-1}xaa^{-1}=(aa^{-1})x(aa^{-1})=1x1=x$ $\Rightarrow F_{a}$ has a two sided inverse $\Rightarrow F_a$ is invertible $\Rightarrow F_a$ is one-to-one and onto G So I need to establish that $F_a$ preserves that group operations to show its an isomorphism and since $F_a$ is a map between two groups we only need to show that it preserves that group product Let $x,y\in G$ $F_a(xy)=axya^{-1}=ax1ya^{-1}=ax(a^{-1}a)ya^{-1}=(axa^{-1})(aya^{-1})=F_a(x)F_a(y)$ $\Rightarrow F_a$ preserves the group product $\therefore F_a$ is an isomorphism from G onto G Do I need to add anything else or is that exactly what I need?","This is what I have for my proof: $F_{a^{-1}}(F_{a}(x))=F_{a^{-1}}(axa^{-1})=a^{-1}axa^{-1}a=(a^{-1}a)x(a^{-1})(a)=1x1=x$ $F_{a}(F_{a^{-1}}(x))=F_{a}(a^{-1}xa)=aa^{-1}xaa^{-1}=(aa^{-1})x(aa^{-1})=1x1=x$ $\Rightarrow F_{a}$ has a two sided inverse $\Rightarrow F_a$ is invertible $\Rightarrow F_a$ is one-to-one and onto G So I need to establish that $F_a$ preserves that group operations to show its an isomorphism and since $F_a$ is a map between two groups we only need to show that it preserves that group product Let $x,y\in G$ $F_a(xy)=axya^{-1}=ax1ya^{-1}=ax(a^{-1}a)ya^{-1}=(axa^{-1})(aya^{-1})=F_a(x)F_a(y)$ $\Rightarrow F_a$ preserves the group product $\therefore F_a$ is an isomorphism from G onto G Do I need to add anything else or is that exactly what I need?",,"['abstract-algebra', 'group-theory', 'solution-verification']"
31,"Nicolas Boubarki, Algebra I, Chapter 1, § 2, Ex. 12","Nicolas Boubarki, Algebra I, Chapter 1, § 2, Ex. 12",,"Nicolas Boubarki, Algebra I, Chapter 1, § 2, Ex. 12: ($E$ is a Semigroup with associative law (represented multiplicatively), $\gamma_a(x)=ax$.) Under a multiplicative law on $E$, let $ a \in E $  be such that $\gamma_a $ is surjective. (a) Show that, if there exists $u$ such that $ua=a$, then $ux=x$ for all $x\in E$. (b) For an element $b\in E$ to be such that $ba$ is left cancellable, it is necessary and sufficient that $\gamma_a$ be surjective and that $b$ be left cancellable. For those interested in part (a), simple proof is that for every $x\in E$ there exists $x^\prime \in E$ such that $ax^\prime=x$, consequently $ua=a \Rightarrow uax^\prime=ax^\prime \Rightarrow ux=x$. In (b), surjectivity of $\gamma_a$ and left cancellability of $b$ is required. However, I am concerned with ""sufficiency"" portion of part (b). When $E$ is infinite set there can always be a surjective function $\gamma_a$ which need not be injective, and left translation by $b$ is cancellable, however $ba$ need not be left cancellable.","Nicolas Boubarki, Algebra I, Chapter 1, § 2, Ex. 12: ($E$ is a Semigroup with associative law (represented multiplicatively), $\gamma_a(x)=ax$.) Under a multiplicative law on $E$, let $ a \in E $  be such that $\gamma_a $ is surjective. (a) Show that, if there exists $u$ such that $ua=a$, then $ux=x$ for all $x\in E$. (b) For an element $b\in E$ to be such that $ba$ is left cancellable, it is necessary and sufficient that $\gamma_a$ be surjective and that $b$ be left cancellable. For those interested in part (a), simple proof is that for every $x\in E$ there exists $x^\prime \in E$ such that $ax^\prime=x$, consequently $ua=a \Rightarrow uax^\prime=ax^\prime \Rightarrow ux=x$. In (b), surjectivity of $\gamma_a$ and left cancellability of $b$ is required. However, I am concerned with ""sufficiency"" portion of part (b). When $E$ is infinite set there can always be a surjective function $\gamma_a$ which need not be injective, and left translation by $b$ is cancellable, however $ba$ need not be left cancellable.",,['abstract-algebra']
32,Formulation of Künneth theorems (definition of $\mathrm{Hom}$ and $\otimes$ of complexes),Formulation of Künneth theorems (definition of  and  of complexes),\mathrm{Hom} \otimes,"In Rotman's An Introduction to Homological Algebra , there is written: Questions: Let $\mathbf{A}$ and $\mathbf{A'}$ be chain complexes with differentials $\partial$ and $\partial'$ respectively. (1) How is $\mathrm{Hom}_R(\mathbf{A},\mathbf{A}')$ defined? Is this a chain or cochain complex? From this post , I'm guessing the solution should be similar to: $$\mathrm{Hom}_R(\mathbf{A},\mathbf{A}'):=(\prod_{i-j=n} \mathrm{Hom}_R(A_i,A'_j))_{n\in\mathbb{Z}},$$ and the differential $d\!: \prod_{i-j=n} \mathrm{Hom}_R(A_i,A'_j) \longrightarrow \prod_{i-j=n-1} \mathrm{Hom}_R(A_i,A'_j)$ is, for $\alpha_{i,j}\in\mathrm{Hom}_R(A_i,C_j)$ with $i\!-\!j\!=\!n$ and $a\!\in\!A_i$, given by $$(d\alpha_{i,j})(a):= \partial'(\alpha_{i,j}(a))-(-1)^n\alpha_{i,j}(\partial a).$$  But this does not make sense: $\partial a\in A_{i-1}$, where $\alpha_{i,j}$ is not defined, and furthermore, $\partial'(\alpha_{i,j}(a))\in A'_{j-1}$, but $i\!-\!(j\!-\!1)\neq n\!-\!1$. Furthermore, why do we in the theorem take $H^n(\mathrm{Hom}_R(\mathbf{A},\mathbf{A}'))$ instead of $H_n(\mathrm{Hom}_R(\mathbf{A},\mathbf{A}'))$? (2) Why is the tensor product of complexes defined as $$\mathbf{A}\!\otimes\!\mathbf{A}':= (\bigoplus_{i+j=n}\!A_i\!\otimes\!A'_j)_{n\in\mathbb{Z}},$$ $$\partial\!\otimes\!\partial'(a\!\otimes\!a')\!:=\! (\partial a)\!\otimes\!a'+(-1)^{\deg a}a\!\otimes\!(\partial'\!a')?$$ I know that this is appropriate for cellular chain complexes, since an $n$-cell in a product of CW-complexes is a product of an $i$-cell and a $j$-cell with $i\!+\!j\!=\!n$. But cellular homology is just one example of many homology theories. Is there any other reason why this definition is preferred instead of other possible ones? (3) Why is $\mathrm{Hom}_R(\mathbf{A},\mathbf{A}')$ defined the way it is, i.e. what are examples where this occurs? (4) Must the complexes $\mathbf{A}$ and $\mathbf{A'}$ be positive for this theorem to hold? Do we get short exact sequences for all $n\!\in\!\mathbb{Z}$? (5) Can the sequence from the last theorem be rewritten as $$0\longleftarrow \!\!\!\prod_{i+j=n}\!\!\!\mathrm{Hom}_R(H_i(\mathbf{A}),H_j(\mathbf{A}')) \longleftarrow H^n(\mathrm{Hom}_R(\mathbf{A},\mathbf{A}')) \longleftarrow \!\!\!\!\!\prod_{i+j=n-1}\!\!\! \mathrm{Ext}^1_R(H_i(\mathbf{A}),H_j(\mathbf{A}')) \longleftarrow 0?$$","In Rotman's An Introduction to Homological Algebra , there is written: Questions: Let $\mathbf{A}$ and $\mathbf{A'}$ be chain complexes with differentials $\partial$ and $\partial'$ respectively. (1) How is $\mathrm{Hom}_R(\mathbf{A},\mathbf{A}')$ defined? Is this a chain or cochain complex? From this post , I'm guessing the solution should be similar to: $$\mathrm{Hom}_R(\mathbf{A},\mathbf{A}'):=(\prod_{i-j=n} \mathrm{Hom}_R(A_i,A'_j))_{n\in\mathbb{Z}},$$ and the differential $d\!: \prod_{i-j=n} \mathrm{Hom}_R(A_i,A'_j) \longrightarrow \prod_{i-j=n-1} \mathrm{Hom}_R(A_i,A'_j)$ is, for $\alpha_{i,j}\in\mathrm{Hom}_R(A_i,C_j)$ with $i\!-\!j\!=\!n$ and $a\!\in\!A_i$, given by $$(d\alpha_{i,j})(a):= \partial'(\alpha_{i,j}(a))-(-1)^n\alpha_{i,j}(\partial a).$$  But this does not make sense: $\partial a\in A_{i-1}$, where $\alpha_{i,j}$ is not defined, and furthermore, $\partial'(\alpha_{i,j}(a))\in A'_{j-1}$, but $i\!-\!(j\!-\!1)\neq n\!-\!1$. Furthermore, why do we in the theorem take $H^n(\mathrm{Hom}_R(\mathbf{A},\mathbf{A}'))$ instead of $H_n(\mathrm{Hom}_R(\mathbf{A},\mathbf{A}'))$? (2) Why is the tensor product of complexes defined as $$\mathbf{A}\!\otimes\!\mathbf{A}':= (\bigoplus_{i+j=n}\!A_i\!\otimes\!A'_j)_{n\in\mathbb{Z}},$$ $$\partial\!\otimes\!\partial'(a\!\otimes\!a')\!:=\! (\partial a)\!\otimes\!a'+(-1)^{\deg a}a\!\otimes\!(\partial'\!a')?$$ I know that this is appropriate for cellular chain complexes, since an $n$-cell in a product of CW-complexes is a product of an $i$-cell and a $j$-cell with $i\!+\!j\!=\!n$. But cellular homology is just one example of many homology theories. Is there any other reason why this definition is preferred instead of other possible ones? (3) Why is $\mathrm{Hom}_R(\mathbf{A},\mathbf{A}')$ defined the way it is, i.e. what are examples where this occurs? (4) Must the complexes $\mathbf{A}$ and $\mathbf{A'}$ be positive for this theorem to hold? Do we get short exact sequences for all $n\!\in\!\mathbb{Z}$? (5) Can the sequence from the last theorem be rewritten as $$0\longleftarrow \!\!\!\prod_{i+j=n}\!\!\!\mathrm{Hom}_R(H_i(\mathbf{A}),H_j(\mathbf{A}')) \longleftarrow H^n(\mathrm{Hom}_R(\mathbf{A},\mathbf{A}')) \longleftarrow \!\!\!\!\!\prod_{i+j=n-1}\!\!\! \mathrm{Ext}^1_R(H_i(\mathbf{A}),H_j(\mathbf{A}')) \longleftarrow 0?$$",,"['abstract-algebra', 'commutative-algebra', 'homological-algebra']"
33,Subgroups of the group of all roots of unity.,Subgroups of the group of all roots of unity.,,"Let $G=\mathbb{C}^*$ and let $\mu$ be the subgroup of roots of unity in $\mathbb{C}^*$ . Show that any finitely generated subgroup of $\mu$ is cyclic. Show that $\mu$ is not finitely generated and find a non-trivial subgroup of $\mu$ which is not finitely generated. I can see that a subgroup of $\mu$ is cyclic due to the nature of complex numbers and De-Moivre's theorem. The second part of this question confuses me though since, by the same logic, should follow a similar procedure. Perhaps it has something to do with the ""finite"" nature of the subgroup in comparison to $\mu$ . Any assistance would be helpful ! Thank you in advance.","Let and let be the subgroup of roots of unity in . Show that any finitely generated subgroup of is cyclic. Show that is not finitely generated and find a non-trivial subgroup of which is not finitely generated. I can see that a subgroup of is cyclic due to the nature of complex numbers and De-Moivre's theorem. The second part of this question confuses me though since, by the same logic, should follow a similar procedure. Perhaps it has something to do with the ""finite"" nature of the subgroup in comparison to . Any assistance would be helpful ! Thank you in advance.",G=\mathbb{C}^* \mu \mathbb{C}^* \mu \mu \mu \mu \mu,"['abstract-algebra', 'group-theory', 'complex-numbers', 'cyclic-groups']"
34,"If $M$ is an $R$-module and $I\subseteq\mathrm{Ann}(M)$ an ideal, then $M$ has a structure of $R/I$-module","If  is an -module and  an ideal, then  has a structure of -module",M R I\subseteq\mathrm{Ann}(M) M R/I,"Let $M$ be an $R$-module and let $I$ be an ideal of $R$ such that $I$ is a subset of $\mathrm{Ann}(M)$. Define a product of an element of $R/I$ by an element of $M$ as follows: $$(r+I)\cdot m=rm.$$ (a) Show that this product is well defined. (b) Show that this product, together with the sum of elements of $M$, turns $M$ into an $R/I$-module. My answer for (a): Let $r+I,s+I \in R/I$ such that $r+I=s+I$. Then $(r+I)m=(s+I)m$, for all $m \in M$. Then $rm=sm \implies r=s$  (since the inverse of $m$ is in $M$), so the product is well defined. Is my answer to (a) correct? How can I use Ann(M) to answer (b)?","Let $M$ be an $R$-module and let $I$ be an ideal of $R$ such that $I$ is a subset of $\mathrm{Ann}(M)$. Define a product of an element of $R/I$ by an element of $M$ as follows: $$(r+I)\cdot m=rm.$$ (a) Show that this product is well defined. (b) Show that this product, together with the sum of elements of $M$, turns $M$ into an $R/I$-module. My answer for (a): Let $r+I,s+I \in R/I$ such that $r+I=s+I$. Then $(r+I)m=(s+I)m$, for all $m \in M$. Then $rm=sm \implies r=s$  (since the inverse of $m$ is in $M$), so the product is well defined. Is my answer to (a) correct? How can I use Ann(M) to answer (b)?",,"['abstract-algebra', 'ring-theory', 'modules']"
35,Field Extensions Problem - From Paolo Aluffi's Book,Field Extensions Problem - From Paolo Aluffi's Book,,"This is the exercise 4.7, chapter VII, from Paolo Aluffi's algebra book. I'm sorry for just copying the question without writing any development myself, I don't have a single ideia about how to use the definitions or the theorems given to solve this. I need the answer but more than that I need to understand the answer, slow explanations are very welcome ( I'm slow at this subject ). Let $k\subseteq F=k(\alpha)$ be a simple algebraic extension. Prove that $F$ is normal over $k$ if and only if for every algebraic extension $F\subseteq K$ and every $\sigma\in \rm{Aut \ }_k(K)$, $\sigma(F)=F$. $\rm{Aut \ }_k(K)$ is the group of automorphisms of $K$ fixing the field $k$. Thanks in advance.","This is the exercise 4.7, chapter VII, from Paolo Aluffi's algebra book. I'm sorry for just copying the question without writing any development myself, I don't have a single ideia about how to use the definitions or the theorems given to solve this. I need the answer but more than that I need to understand the answer, slow explanations are very welcome ( I'm slow at this subject ). Let $k\subseteq F=k(\alpha)$ be a simple algebraic extension. Prove that $F$ is normal over $k$ if and only if for every algebraic extension $F\subseteq K$ and every $\sigma\in \rm{Aut \ }_k(K)$, $\sigma(F)=F$. $\rm{Aut \ }_k(K)$ is the group of automorphisms of $K$ fixing the field $k$. Thanks in advance.",,"['abstract-algebra', 'field-theory', 'extension-field']"
36,Not a Zero Divisor,Not a Zero Divisor,,"Let $R$ be a commutative ring. Then we say $a \in R$ is a zero divisor if there exists $b \neq 0$ such that $ab = 0$. I want to know what it means to not be a zero divisor. So I tried  to negate the statement: $a$ is not a zero divisor if for every $b \neq 0$ we have $ab \neq 0$. Also taking the contrapositive of the initial statement I got the following: If for every $b \neq 0$, $ab \neq 0$, then $a$ is not a zero divisor. Have I negated the definition of a zero divisor and taken the contrapositive correctly? My book has the following theorem: Suppose $a$ is not a zero-divisor. Then if $ab = ac$, we can conclude that $b = c$. Proof: $ab - ac = a(b-c) = 0$. Since $a$ is not a zero-divisor, $b-c = 0$ so $b=c$. I don't see why $b-c = 0$ because $a$ is not a zero-divisor. Could someone explain?","Let $R$ be a commutative ring. Then we say $a \in R$ is a zero divisor if there exists $b \neq 0$ such that $ab = 0$. I want to know what it means to not be a zero divisor. So I tried  to negate the statement: $a$ is not a zero divisor if for every $b \neq 0$ we have $ab \neq 0$. Also taking the contrapositive of the initial statement I got the following: If for every $b \neq 0$, $ab \neq 0$, then $a$ is not a zero divisor. Have I negated the definition of a zero divisor and taken the contrapositive correctly? My book has the following theorem: Suppose $a$ is not a zero-divisor. Then if $ab = ac$, we can conclude that $b = c$. Proof: $ab - ac = a(b-c) = 0$. Since $a$ is not a zero-divisor, $b-c = 0$ so $b=c$. I don't see why $b-c = 0$ because $a$ is not a zero-divisor. Could someone explain?",,['abstract-algebra']
37,Set of locations where the Hilbert symbol is not equal to $1$,Set of locations where the Hilbert symbol is not equal to,1,"Let $V$ be the set of prime together with the symbol $\infty$. For a prime $v=p$, denote the $p$-adic numbers by $\mathbb{Q}_p$ and the real numbers by $\mathbb{Q}_\infty$. For $v\in V$ the Hilbert symbol is defined for $a,b\in\mathbb{Q}^*_v$ as \begin{align*} (a,b)_v=\begin{cases}+1,&\text{ if }ax^2+by^2=z^2\text{ has a non-zero solution }(x,y,z)\in \mathbb{Q}_v^3;\\-1,&\text{ else.}\end{cases} \end{align*} Furthermore for $v\in V, a,b\in\mathbb{Q}^*$ we denote by $(a,b)_v$ the Hilbert symbol of $(\bar a,\bar b)_v$ where $\bar a,\bar b$ are the images of $a,b$ in $\mathbb{Q}_v$. Now a theorem by Hilbert says that $(a,b)_v=1$ for almost all $v\in V$ (and that furthermore $\prod_{v\in V}(a,b)_v=1$, but I'm not interested in this at the moment). The theorem can be found in ""A course in Arithmetic"" by Jean-Pierre Serre for example. It basically says that there is a finite set $E\subseteq V$ such that  \begin{align*} (a,b)_v=\begin{cases}+1,&\text{ if }v\notin E\\-1,&\text{ if }v\in E\end{cases} \end{align*} My question is if this set $E$ has a common name in the literature. Something like $E_{a,b}$ would make sense to me (since it depends on $a$ and $b$). If there is no widely used name, what are your suggestions?","Let $V$ be the set of prime together with the symbol $\infty$. For a prime $v=p$, denote the $p$-adic numbers by $\mathbb{Q}_p$ and the real numbers by $\mathbb{Q}_\infty$. For $v\in V$ the Hilbert symbol is defined for $a,b\in\mathbb{Q}^*_v$ as \begin{align*} (a,b)_v=\begin{cases}+1,&\text{ if }ax^2+by^2=z^2\text{ has a non-zero solution }(x,y,z)\in \mathbb{Q}_v^3;\\-1,&\text{ else.}\end{cases} \end{align*} Furthermore for $v\in V, a,b\in\mathbb{Q}^*$ we denote by $(a,b)_v$ the Hilbert symbol of $(\bar a,\bar b)_v$ where $\bar a,\bar b$ are the images of $a,b$ in $\mathbb{Q}_v$. Now a theorem by Hilbert says that $(a,b)_v=1$ for almost all $v\in V$ (and that furthermore $\prod_{v\in V}(a,b)_v=1$, but I'm not interested in this at the moment). The theorem can be found in ""A course in Arithmetic"" by Jean-Pierre Serre for example. It basically says that there is a finite set $E\subseteq V$ such that  \begin{align*} (a,b)_v=\begin{cases}+1,&\text{ if }v\notin E\\-1,&\text{ if }v\in E\end{cases} \end{align*} My question is if this set $E$ has a common name in the literature. Something like $E_{a,b}$ would make sense to me (since it depends on $a$ and $b$). If there is no widely used name, what are your suggestions?",,"['abstract-algebra', 'number-theory', 'algebraic-number-theory', 'p-adic-number-theory']"
38,Bounded index of nilpotency,Bounded index of nilpotency,,"A ring $R$ is said to have a bounded index (of nilpotency) if there is a positive integer $m$ such that $x^m = 0$ for every nilpotent $x\in R$. I wonder whether this property transfers to the ring of matrices, that is, If $R$ has bounded index, then $M_n(R)$ has bounded index? If $R$ is a field, then the answer is positive and extends obviously to integral domains. Noetherian rings have also the bounded index property, and it would be nice to have an answer in this case. (When $R$ is noetherian and reduced the answer is also positive.) Remark. This question was suggested by Bound on nilpotency index of endomorphisms .","A ring $R$ is said to have a bounded index (of nilpotency) if there is a positive integer $m$ such that $x^m = 0$ for every nilpotent $x\in R$. I wonder whether this property transfers to the ring of matrices, that is, If $R$ has bounded index, then $M_n(R)$ has bounded index? If $R$ is a field, then the answer is positive and extends obviously to integral domains. Noetherian rings have also the bounded index property, and it would be nice to have an answer in this case. (When $R$ is noetherian and reduced the answer is also positive.) Remark. This question was suggested by Bound on nilpotency index of endomorphisms .",,['abstract-algebra']
39,"Maximal ideals in $\mathbb{R}[x,y]/(xy-2)$?",Maximal ideals in ?,"\mathbb{R}[x,y]/(xy-2)","I'm working on a practice exam and one of the questions asks if there are any maximal ideals in $\mathbb{R}[x,y]/(xy-2)$ and, if so, to find one of them. Initially, I thought the quotient ring was a field because xy-2 is clearly irreducible; however, I forgot that this only applies to PIDs. My suspicion is that $(\bar{x})$ is a maximal ideal in the quotient ring, but I don't know how to demonstrate this. I was also thinking about the possibility of the quotient ring being local, although I'm not sure if this if the right direction. Thanks a lot.","I'm working on a practice exam and one of the questions asks if there are any maximal ideals in $\mathbb{R}[x,y]/(xy-2)$ and, if so, to find one of them. Initially, I thought the quotient ring was a field because xy-2 is clearly irreducible; however, I forgot that this only applies to PIDs. My suspicion is that $(\bar{x})$ is a maximal ideal in the quotient ring, but I don't know how to demonstrate this. I was also thinking about the possibility of the quotient ring being local, although I'm not sure if this if the right direction. Thanks a lot.",,"['abstract-algebra', 'ring-theory']"
40,"If $p$ is prime, what is the difference between $F_p$, $\mathbb{Z}_p$ and $\mathbb{Z}/(p \mathbb{Z})$?","If  is prime, what is the difference between ,  and ?",p F_p \mathbb{Z}_p \mathbb{Z}/(p \mathbb{Z}),"It's all in the title: $p$ is prime, what is the difference between $F_p$, $\mathbb{Z}_p$ and $\mathbb{Z}/(p \mathbb{Z})$?  Also, if $p$ is not a prime, what is the difference between $\mathbb{Z}_p$ and $\mathbb{Z}/(p \mathbb{Z})$?","It's all in the title: $p$ is prime, what is the difference between $F_p$, $\mathbb{Z}_p$ and $\mathbb{Z}/(p \mathbb{Z})$?  Also, if $p$ is not a prime, what is the difference between $\mathbb{Z}_p$ and $\mathbb{Z}/(p \mathbb{Z})$?",,['abstract-algebra']
41,Irreducible polynomial over field of order p,Irreducible polynomial over field of order p,,Let $p$ be a prime and $F=\mathbb{Z}/p\mathbb{Z}$ and $f(t)\in F[t]$ be an irreducible polynomial of degree $d$. I need to show that $f(t)$ divides $t^{(p^{n})}-t$ if and only if $d$ divides $n$.,Let $p$ be a prime and $F=\mathbb{Z}/p\mathbb{Z}$ and $f(t)\in F[t]$ be an irreducible polynomial of degree $d$. I need to show that $f(t)$ divides $t^{(p^{n})}-t$ if and only if $d$ divides $n$.,,"['abstract-algebra', 'polynomials', 'ring-theory', 'finite-fields']"
42,Positivity of the norm of an element of a cyclotomic field,Positivity of the norm of an element of a cyclotomic field,,"Let $l$ be an odd prime number and $\zeta$ be an $l$-th primitive root of unity in $\mathbb{C}$. Let $\mathbb{Q}(\zeta)$ be the cyclotomic field and $\alpha$ be a non-zero element of $\mathbb{Q}(\zeta)$. There exists a polynomial $f(X) \in \mathbb{Q}[X]$ such that $\alpha = f(\zeta)$. Let $N(\alpha) = f(\zeta)f(\zeta^2)...f(\zeta^{l-1})$. From $\bar\zeta = \zeta^{-1}$ it follows that $\bar f(\zeta) = f(\zeta^{-1})$. Likewise, $\bar f(\zeta^i) = f(\zeta^{-i})$ for $i = 1,2,\cdots,l - 1$.  Since $f(\zeta^i)\bar f(\zeta^i) = |f(\zeta^i)|^2 > 0$, it follows that $N(\alpha) > 0$. We used the fact that the field of complex numbers $\mathbb{C}$ has an $l$-th primitive root of unity. It seems to me that this fact can only be proved by using some (elementary) analysis. My question is: Can we prove $N(\alpha) > 0$ purely algebraically? In other words, can we prove $N(\alpha) > 0$ without using the field of real numbers? Please note that $\mathbb{Q}(\zeta) \cong \mathbb{Q}[X]/(1 + X + ... + X^{l-1})$ can be constructed purely algebraically from $\mathbb{Q}$.","Let $l$ be an odd prime number and $\zeta$ be an $l$-th primitive root of unity in $\mathbb{C}$. Let $\mathbb{Q}(\zeta)$ be the cyclotomic field and $\alpha$ be a non-zero element of $\mathbb{Q}(\zeta)$. There exists a polynomial $f(X) \in \mathbb{Q}[X]$ such that $\alpha = f(\zeta)$. Let $N(\alpha) = f(\zeta)f(\zeta^2)...f(\zeta^{l-1})$. From $\bar\zeta = \zeta^{-1}$ it follows that $\bar f(\zeta) = f(\zeta^{-1})$. Likewise, $\bar f(\zeta^i) = f(\zeta^{-i})$ for $i = 1,2,\cdots,l - 1$.  Since $f(\zeta^i)\bar f(\zeta^i) = |f(\zeta^i)|^2 > 0$, it follows that $N(\alpha) > 0$. We used the fact that the field of complex numbers $\mathbb{C}$ has an $l$-th primitive root of unity. It seems to me that this fact can only be proved by using some (elementary) analysis. My question is: Can we prove $N(\alpha) > 0$ purely algebraically? In other words, can we prove $N(\alpha) > 0$ without using the field of real numbers? Please note that $\mathbb{Q}(\zeta) \cong \mathbb{Q}[X]/(1 + X + ... + X^{l-1})$ can be constructed purely algebraically from $\mathbb{Q}$.",,"['abstract-algebra', 'field-theory', 'algebraic-number-theory']"
43,The completion of a noetherian local ring is a complete local ring,The completion of a noetherian local ring is a complete local ring,,"We have defined the completion of a noetherian local ring $A$ to be $$\hat{A}=\left\{(a_1,a_2,\ldots)\in\prod_{i=1}^\infty A/\mathfrak{m}^i:a_j\equiv a_i\bmod{\mathfrak{m}^i} \,\,\forall j>i\right\}.$$ I have a slight problem trying to understand the proof that then $\hat{A}$ is a complete local ring with maximal ideal $\hat{\mathfrak{m}}=\{(a_1,a_2,\ldots)\in\hat{A}:a_1=0\}$. Proof. If $(a_1,a_2,\ldots)\in\hat{\mathfrak{m}}$, then $a_i\equiv 0\bmod{\mathfrak{m}}$ for all $i$, i.e., $a_i\in\mathfrak{m}$. Hence $$\hat{\mathfrak{m}}^i=\left\{(a_1,a_2,\ldots)\in\hat{A}:a_j=0\,\,\forall j\leq i\right\}.$$ So the canonical map $\hat{A}\to A/\mathfrak{m}^i$, $(a_1,a_2,\ldots)\mapsto a_i$, is surjective with kernel $\hat{\mathfrak{m}}^i$. Thus $\hat{A}/\hat{\mathfrak{m}}^i\cong A/\mathfrak{m}^i$. In particular, $\hat{\mathfrak{m}}$ is a maximal ideal. But why is it the only one in $\hat{A}$? If $(a_1,a_2,\ldots)\not\in\hat{\mathfrak{m}}$, we have $a_1\neq 0$, hence $a_1\not\in\mathfrak{m}$, hence it is a unit. By the defining property of the completion, $a_j$ is a unit for all $j$. So I could choose a candidate for the inverse of $(a_1,a_2,\ldots)$ by choosing inverse elements of the $a_j$. Why would this candidate be in $\hat{A}$ then, i.e. how can I choose it properly such that the congruences on the right hand side of the definition of the completion would be fulfilled? Regards!","We have defined the completion of a noetherian local ring $A$ to be $$\hat{A}=\left\{(a_1,a_2,\ldots)\in\prod_{i=1}^\infty A/\mathfrak{m}^i:a_j\equiv a_i\bmod{\mathfrak{m}^i} \,\,\forall j>i\right\}.$$ I have a slight problem trying to understand the proof that then $\hat{A}$ is a complete local ring with maximal ideal $\hat{\mathfrak{m}}=\{(a_1,a_2,\ldots)\in\hat{A}:a_1=0\}$. Proof. If $(a_1,a_2,\ldots)\in\hat{\mathfrak{m}}$, then $a_i\equiv 0\bmod{\mathfrak{m}}$ for all $i$, i.e., $a_i\in\mathfrak{m}$. Hence $$\hat{\mathfrak{m}}^i=\left\{(a_1,a_2,\ldots)\in\hat{A}:a_j=0\,\,\forall j\leq i\right\}.$$ So the canonical map $\hat{A}\to A/\mathfrak{m}^i$, $(a_1,a_2,\ldots)\mapsto a_i$, is surjective with kernel $\hat{\mathfrak{m}}^i$. Thus $\hat{A}/\hat{\mathfrak{m}}^i\cong A/\mathfrak{m}^i$. In particular, $\hat{\mathfrak{m}}$ is a maximal ideal. But why is it the only one in $\hat{A}$? If $(a_1,a_2,\ldots)\not\in\hat{\mathfrak{m}}$, we have $a_1\neq 0$, hence $a_1\not\in\mathfrak{m}$, hence it is a unit. By the defining property of the completion, $a_j$ is a unit for all $j$. So I could choose a candidate for the inverse of $(a_1,a_2,\ldots)$ by choosing inverse elements of the $a_j$. Why would this candidate be in $\hat{A}$ then, i.e. how can I choose it properly such that the congruences on the right hand side of the definition of the completion would be fulfilled? Regards!",,"['abstract-algebra', 'commutative-algebra']"
44,Algebra Texts that Emphasize Universal Properties/Contructions,Algebra Texts that Emphasize Universal Properties/Contructions,,"I am interested in elementary algebra texts and/or notes that place early and continuous emphasis on universal constructions, functors and other aspects of category theory. One text that takes this approach is Aluffi's Algebra: Chapter 0 . Another text that emphasizes universal constructions early-on is Hu's Elements of Modern Algebra Are there other elementary algebra texts and/or notes that integrate universal constructions and principles of category theory into the development of the subject?","I am interested in elementary algebra texts and/or notes that place early and continuous emphasis on universal constructions, functors and other aspects of category theory. One text that takes this approach is Aluffi's Algebra: Chapter 0 . Another text that emphasizes universal constructions early-on is Hu's Elements of Modern Algebra Are there other elementary algebra texts and/or notes that integrate universal constructions and principles of category theory into the development of the subject?",,"['abstract-algebra', 'reference-request']"
45,Algorithms for symbolic definite integration?,Algorithms for symbolic definite integration?,,"What are the algorithms for symbolic definite integration? Apart from computing the antiderivative first. What are the basic ideas behind such algorithms? As far as I got it, the main idea behind symbolic indefinite integration is that we actually know what kind of terms should be in the answer. And it is easy to believe in this, since it is enough to think what kind of terms are produced during differentiation. But for definite integrals it seems that the answer could be virtually any combination of known functions and constants. I'm aware of experimental methods involving high-precision numerical integration and then using things like inverse symbolic calculator . But that's not what I'm looking for.","What are the algorithms for symbolic definite integration? Apart from computing the antiderivative first. What are the basic ideas behind such algorithms? As far as I got it, the main idea behind symbolic indefinite integration is that we actually know what kind of terms should be in the answer. And it is easy to believe in this, since it is enough to think what kind of terms are produced during differentiation. But for definite integrals it seems that the answer could be virtually any combination of known functions and constants. I'm aware of experimental methods involving high-precision numerical integration and then using things like inverse symbolic calculator . But that's not what I'm looking for.",,"['abstract-algebra', 'definite-integrals', 'computer-algebra-systems', 'symbolic-computation']"
46,For which $k$ do the $k$th powers of the roots of a polynomial give a basis for a number field?,For which  do the th powers of the roots of a polynomial give a basis for a number field?,k k,"Let $f \in \mathbb{Q}[x]$ of degreee $d$ be irreducible, with roots $\alpha_1,\ldots, \alpha_d$.  One particular basis for the field extension of $\mathbb{Q}$ obtained by adjoining the roots of $f$ is given by $\{\alpha_j\}_{j=1}^d$.  For which $k$ does the set $\{\alpha_j^k\}_{j=1}^d$ also provide a basis? If this is in general a difficult question, could I at least show that I obtain a basis for infinitely many $k$? Example: Consider the polynomial $x^2-2$.  The $k$th powers of the roots $\{\pm \sqrt{2}\}$ provide a basis for my field extension precisely when $k$ is odd. Edit: As Gerry Myerson points out, and as my example indicates, I am actually more interested in when the $k$th powers of the roots generate the extension over $\mathbb{Q}$.  Calling this a $\mathbb{Q}$-basis was erroneous.","Let $f \in \mathbb{Q}[x]$ of degreee $d$ be irreducible, with roots $\alpha_1,\ldots, \alpha_d$.  One particular basis for the field extension of $\mathbb{Q}$ obtained by adjoining the roots of $f$ is given by $\{\alpha_j\}_{j=1}^d$.  For which $k$ does the set $\{\alpha_j^k\}_{j=1}^d$ also provide a basis? If this is in general a difficult question, could I at least show that I obtain a basis for infinitely many $k$? Example: Consider the polynomial $x^2-2$.  The $k$th powers of the roots $\{\pm \sqrt{2}\}$ provide a basis for my field extension precisely when $k$ is odd. Edit: As Gerry Myerson points out, and as my example indicates, I am actually more interested in when the $k$th powers of the roots generate the extension over $\mathbb{Q}$.  Calling this a $\mathbb{Q}$-basis was erroneous.",,"['abstract-algebra', 'field-theory', 'galois-theory']"
47,Kernel of a substitution map,Kernel of a substitution map,,"Suppose $R=k[x,y,z]$ and $S=k[t]$. Consider the map $f:R\to S$ s.t. $f(x)=t$, $f(y)=t^2$ and $f(z)=t^3$. I suspect the kernel of this map is the ideal $(y-x^2,z-x^3)R$. It's clearly contained in the kernel, but I am not sure how to prove the reverse inclusion.","Suppose $R=k[x,y,z]$ and $S=k[t]$. Consider the map $f:R\to S$ s.t. $f(x)=t$, $f(y)=t^2$ and $f(z)=t^3$. I suspect the kernel of this map is the ideal $(y-x^2,z-x^3)R$. It's clearly contained in the kernel, but I am not sure how to prove the reverse inclusion.",,['abstract-algebra']
48,Simple functions and axiom of choice,Simple functions and axiom of choice,,"The question I have is more of a curiosity, and that is why I decided to post here instead of Mathoverflow. Before posing the question, let me set up some background. Background: Let $\Omega$ be a Boolean algebra. For the purposes of this post a measure $\nu$ on $\Omega$ is a finitely additive map $\Omega\to A$ with values in a linear space $A$. If $A$ is a commutative, unital algebra, $\nu$ is multiplicative or spectral if $\nu(E\cap F)= \nu(E)\nu(F)$ and $\nu(1)= 1$. I will denote by $\mathbf{A}(\Omega, V)$ the linear space of $V$-measures on $\Omega$ and by $\mathbf{AS}(\Omega, A)$ the set of spectral measures. note: the field of scalars is irrelevant; take the real field for the sake of determinateness. Theorem 1: The functor $V\mapsto \mathbf{A}(\Omega, V)$ is representable, that is, there is a linear space $S(\Omega)$ and a natural isomorphism $\mathbf{A}(\Omega, V)\cong \mathbf{Vect}(S(\Omega), V)$. $S(\Omega)$ is the space of simple elements . Denote by $\chi$ the universal measure $\Omega\to S(\Omega)$ and call elements of the form $\chi(E)$ characteristic . note: By Stone duality, $S(\Omega)$ is isomorphic to the linear space of linear combinations of characteristic functions of clopen sets of the Stone space of $\Omega$. Thus the monikers ""simple"" and ""characteristic elements"". Stone duality needs the Boolean prime ideal theorem. We can introduce a commutative unital algebra structure on $S(\Omega)$ by $\chi(E)\chi(F)= \chi(E\cap F)$ and then extending linearly (or juggling around the universal property). $\chi$ is now spectral and in fact the universal spectral measure. Theorem 2: The functor $A\mapsto \mathbf{AS}(\Omega, A)$ is naturally isomorphic to $\mathbf{CAlg}(S(\Omega), A)$. In fact, we have a functor $\mathbf{CAlg}\to \mathbf{Bool}$ that sends a commutative, unital algebra to the Boolean algebra of its idempotents. It is now easy to see that this functor is right adjoint to $S$ and, in particular, $S$ is cocontinuous. Problem: An easy application of the Boolean prime ideal theorem (BPI for short) implies that if $E\in \Omega$ is non-zero then $\chi(E)$ is likewise non-zero. In particular, $S(\Omega)$ is non-trivial ($0\neq 1$). Conversely, it is not difficult to see that if $1\neq 0$ then if $E\in \Omega$ is non-zero then $\chi(E)$ is non-zero. My question is can the non-triviality of $S(\Omega)$ be proved without recourse to the BPI? Say, in ZF alone? ZF + P with some P weaker than BPI? note: BPI is weaker than AC by a well-known result of Halpern and Levy. Here is what I managed to do until now. Theorem 2 guarantees that as long as we have a non-trivial spectral measure on $\Omega$, non-triviality of $S(\Omega)$ drops out. $S$ of the initial Boolean algebra (the non-trivial one constituted solely by the bottom and top elements) is the scalar field, thus, if we have an ultrafilter on $\Omega$ we can prove that $S(\Omega)$ is non-trivial. In particular, if $\Omega$ has even just one atom we are done. But I am at a loss as how to proceed in the case of arbitrary atomless algebras. If we can factor some morphism into a Boolean algebra with non-trivial $S$ then the problem would also be solved, but this is really not an advancement over constructing ultrafilters directly. At this point, and after not managing to get non-triviality of $S(\Omega)$ without BPI I started wondering if non-triviality of $S(\Omega)$ is actually equivalent to BPI, but the only thing I accomplished is to narrow down to the problem of constructing one algebra morphism from $S(\Omega)$ to the real field -- not much, that is.","The question I have is more of a curiosity, and that is why I decided to post here instead of Mathoverflow. Before posing the question, let me set up some background. Background: Let $\Omega$ be a Boolean algebra. For the purposes of this post a measure $\nu$ on $\Omega$ is a finitely additive map $\Omega\to A$ with values in a linear space $A$. If $A$ is a commutative, unital algebra, $\nu$ is multiplicative or spectral if $\nu(E\cap F)= \nu(E)\nu(F)$ and $\nu(1)= 1$. I will denote by $\mathbf{A}(\Omega, V)$ the linear space of $V$-measures on $\Omega$ and by $\mathbf{AS}(\Omega, A)$ the set of spectral measures. note: the field of scalars is irrelevant; take the real field for the sake of determinateness. Theorem 1: The functor $V\mapsto \mathbf{A}(\Omega, V)$ is representable, that is, there is a linear space $S(\Omega)$ and a natural isomorphism $\mathbf{A}(\Omega, V)\cong \mathbf{Vect}(S(\Omega), V)$. $S(\Omega)$ is the space of simple elements . Denote by $\chi$ the universal measure $\Omega\to S(\Omega)$ and call elements of the form $\chi(E)$ characteristic . note: By Stone duality, $S(\Omega)$ is isomorphic to the linear space of linear combinations of characteristic functions of clopen sets of the Stone space of $\Omega$. Thus the monikers ""simple"" and ""characteristic elements"". Stone duality needs the Boolean prime ideal theorem. We can introduce a commutative unital algebra structure on $S(\Omega)$ by $\chi(E)\chi(F)= \chi(E\cap F)$ and then extending linearly (or juggling around the universal property). $\chi$ is now spectral and in fact the universal spectral measure. Theorem 2: The functor $A\mapsto \mathbf{AS}(\Omega, A)$ is naturally isomorphic to $\mathbf{CAlg}(S(\Omega), A)$. In fact, we have a functor $\mathbf{CAlg}\to \mathbf{Bool}$ that sends a commutative, unital algebra to the Boolean algebra of its idempotents. It is now easy to see that this functor is right adjoint to $S$ and, in particular, $S$ is cocontinuous. Problem: An easy application of the Boolean prime ideal theorem (BPI for short) implies that if $E\in \Omega$ is non-zero then $\chi(E)$ is likewise non-zero. In particular, $S(\Omega)$ is non-trivial ($0\neq 1$). Conversely, it is not difficult to see that if $1\neq 0$ then if $E\in \Omega$ is non-zero then $\chi(E)$ is non-zero. My question is can the non-triviality of $S(\Omega)$ be proved without recourse to the BPI? Say, in ZF alone? ZF + P with some P weaker than BPI? note: BPI is weaker than AC by a well-known result of Halpern and Levy. Here is what I managed to do until now. Theorem 2 guarantees that as long as we have a non-trivial spectral measure on $\Omega$, non-triviality of $S(\Omega)$ drops out. $S$ of the initial Boolean algebra (the non-trivial one constituted solely by the bottom and top elements) is the scalar field, thus, if we have an ultrafilter on $\Omega$ we can prove that $S(\Omega)$ is non-trivial. In particular, if $\Omega$ has even just one atom we are done. But I am at a loss as how to proceed in the case of arbitrary atomless algebras. If we can factor some morphism into a Boolean algebra with non-trivial $S$ then the problem would also be solved, but this is really not an advancement over constructing ultrafilters directly. At this point, and after not managing to get non-triviality of $S(\Omega)$ without BPI I started wondering if non-triviality of $S(\Omega)$ is actually equivalent to BPI, but the only thing I accomplished is to narrow down to the problem of constructing one algebra morphism from $S(\Omega)$ to the real field -- not much, that is.",,"['abstract-algebra', 'logic', 'boolean-algebra', 'axiom-of-choice']"
49,Is the algebraic norm of an euclidean integer ring also an euclidean domain norm?,Is the algebraic norm of an euclidean integer ring also an euclidean domain norm?,,"Let $K$ be a finite extension of $\mathbb{Q}$ (a number field) and $\mathcal{O}_K$ its ring of integers. One defines the norm of an element $\alpha\in K$ to be the determinant of the transformation $m_\alpha: K\to K$ of multiplication by $\alpha$ (where $K$ is considered as a vector space over $\mathbb{Q}$). Now sometimes the integer ring is also a euclidean domain, i.e. it has a ""euclidean norm"" satisfying the defining property of division algorithm. My question is: in an integer ring which is also euclidean, will the norm defined above also serve as a euclidean norm? Put otherwise: is there an example for a euclidean integer ring whose norm is not an euclidean norm?","Let $K$ be a finite extension of $\mathbb{Q}$ (a number field) and $\mathcal{O}_K$ its ring of integers. One defines the norm of an element $\alpha\in K$ to be the determinant of the transformation $m_\alpha: K\to K$ of multiplication by $\alpha$ (where $K$ is considered as a vector space over $\mathbb{Q}$). Now sometimes the integer ring is also a euclidean domain, i.e. it has a ""euclidean norm"" satisfying the defining property of division algorithm. My question is: in an integer ring which is also euclidean, will the norm defined above also serve as a euclidean norm? Put otherwise: is there an example for a euclidean integer ring whose norm is not an euclidean norm?",,"['abstract-algebra', 'ring-theory', 'algebraic-number-theory']"
50,Number of real roots of a separable real polynomial doesn't change under small perturbations,Number of real roots of a separable real polynomial doesn't change under small perturbations,,"Say we have a polynomial with real coefficients and no repeated roots.  Knowing that the roots of a polynomial vary continuously in the coefficients (so long as we don't change the degree), it seems intuitive that all sufficiently close polynomials will have the same number of real roots, because in order to make two real roots non-real, or vice versa, you'd have to first bring them together in order to satisfy the constraint that the non-real ones must be conjugates, and it's not too hard to see that all sufficiently close polynomials will also have no repeated roots (nonzero discriminant is an open set). However I'm at a loss as to how to formalize the ""you'd have to bring them together"" argument above.  That repeated roots can be avoided is easy because we have the discriminant as noted above, and looking at the sign of the discriminant shows the number of real roots won't change mod 4, but I'm not sure how to do better than that algebraically.  It would be nice if Sturm's Theorem could be used but I don't expect that the polynomial quotients involved would be continuous at all the relevant points. Is there any sort of nice algebraic way to do this, or is the best way to formalize the intuitive argument using continuity of roots?  In the latter case, how would you actually prove that you have to bring two of them together? (A possible workaround I thought of for the algebraic approach would be to use a different, more general Sturm chain -- in particular, for polynomials of degree n, consider $\mathbb{R}(a_0,\ldots,a_n)$ ($a_0,\ldots,a_n$ indeterminates) and the generic polynomial $a_n x^n+\ldots+a_0$ and its derivative, take quotients and remainders there, and only afterward plug in the real numbers, thus getting rid of discontinuous-degree-change issues.  However this, or at least this particular variant, doesn't seem to actually work, as far as I can tell -- checking by hand the degree 3 case seems to indicate that this won't work since anything that's an intermediate initial coefficient will end up getting divided by, and the first one of those is $\frac{2}{9}\frac{a_2^2}{9a_3}-\frac{2}{3}a_1$, which can still be 0 without either $a_3$ or the discriminant being 0, which are the two things that can obviously be safely divided by.)","Say we have a polynomial with real coefficients and no repeated roots.  Knowing that the roots of a polynomial vary continuously in the coefficients (so long as we don't change the degree), it seems intuitive that all sufficiently close polynomials will have the same number of real roots, because in order to make two real roots non-real, or vice versa, you'd have to first bring them together in order to satisfy the constraint that the non-real ones must be conjugates, and it's not too hard to see that all sufficiently close polynomials will also have no repeated roots (nonzero discriminant is an open set). However I'm at a loss as to how to formalize the ""you'd have to bring them together"" argument above.  That repeated roots can be avoided is easy because we have the discriminant as noted above, and looking at the sign of the discriminant shows the number of real roots won't change mod 4, but I'm not sure how to do better than that algebraically.  It would be nice if Sturm's Theorem could be used but I don't expect that the polynomial quotients involved would be continuous at all the relevant points. Is there any sort of nice algebraic way to do this, or is the best way to formalize the intuitive argument using continuity of roots?  In the latter case, how would you actually prove that you have to bring two of them together? (A possible workaround I thought of for the algebraic approach would be to use a different, more general Sturm chain -- in particular, for polynomials of degree n, consider $\mathbb{R}(a_0,\ldots,a_n)$ ($a_0,\ldots,a_n$ indeterminates) and the generic polynomial $a_n x^n+\ldots+a_0$ and its derivative, take quotients and remainders there, and only afterward plug in the real numbers, thus getting rid of discontinuous-degree-change issues.  However this, or at least this particular variant, doesn't seem to actually work, as far as I can tell -- checking by hand the degree 3 case seems to indicate that this won't work since anything that's an intermediate initial coefficient will end up getting divided by, and the first one of those is $\frac{2}{9}\frac{a_2^2}{9a_3}-\frac{2}{3}a_1$, which can still be 0 without either $a_3$ or the discriminant being 0, which are the two things that can obviously be safely divided by.)",,"['abstract-algebra', 'analysis', 'polynomials', 'roots']"
51,$\mathrm{SO}(3)$-invariant polynomials,-invariant polynomials,\mathrm{SO}(3),"The Lie group $\mathrm{SO}(3,\mathbb{R})$ has a representation $V_l$ of dimension $2l+1$ for each $l=0,1,\ldots$ I am looking for a single polynomial in $\mathbb{R}[V_l]$ which is invariant under the $\mathrm{SO}(3)$ action. The obvious example for $l=1$ is $$x^2+y^2+z^2$$ This is a duplicate of this 2017 MSE question and the answer there is helpful but only partial. Unless I'm mistaken, $\mathrm{SO}(3,\mathbb{R})$ -invariance should boil down to $\mathrm{SU}(2)$ -invariance, and thus to $\mathrm{SL}(2,\mathbb{C})$ -invariance. This (under the guise of binary forms) has been studied for over 100 years and people apparently have found (minimal) generators of invariants of $k[\mathrm{Sym}^n(k^2)]$ for up to $n=8$ , maybe further. But I'm not asking for a full set of generators, I just want one (nonconstant) polynomial for each $n$ . Edit : I'm clarifying the action of $\mathrm{SO}(3,\mathbb{R})$ on $\mathbb{R}^{l+1}$ for the benefit of a commenter below. I also provide a nontrivial example for $l=2$ . The action of $\mathrm{SO}(3,\mathbb{R})$ on $\mathbb{R}^{2l+1}$ has been known for a very long time. The idea is that there's a map $\mathrm{SU}(2)\rightarrow \mathrm{SO}(3,\mathbb{R})$ with kernel $I, -I$ , so any irreducible representation of $\mathrm{SU}(2)$ where $-I$ acts trivially (namely, the irreps of odd dimension) will descend to a representation of $\mathrm{SO}(3,\mathbb{R})$ . The simplest way of realizing this representation is in terms of harmonic polynomials. Note that $\mathrm{SO}(3)\subset \mathrm{GL}(3)$ acts naturally on $\mathbb{R}[x,y,z]_l$ , the space of homogeneous polynomials of degree $l$ . Let $\mathfrak{h}_3^l$ be the $2l+1$ -dimensional vector subspace of $\mathbb{R}[x,y,z]_l$ consisting of the degree $l$ harmonic polynomials. This is defined as the kernel of the Laplacian $\Delta=\frac{\partial}{\partial x^2}+ \frac{\partial}{\partial y^2}+\frac{\partial}{\partial z^2}$ . The subspace $\mathfrak{h}_3^l$ is preserved by the action of $\mathrm{SO}(3,\mathbb{R})$ because the elements of $\mathrm{SO}(3,\mathbb{R})$ commute with the Laplacian. Finally, my question: $\mathrm{SO}(3,\mathbb{R})$ acts on $\mathbb{R}[\mathfrak{h}_3^l]$ . I seek a polynomial in this space which is fixed by the action (it is known to exist!). Example for $l=1$ : the vector space $\mathfrak{h}_3^1$ is spanned by the polynomials $p_1=x,p_2=y,p_3=z$ (they are all killed by the Laplacian). Now $\mathbb{R}[\mathfrak{h}_3^1]=\mathbb{R}[p_1,p_2,p_3]$ and there is an obvious $\mathrm{SO}(3,\mathbb{R})$ -invariant polynomial of degree $2$ : $p_1^2+p_2^2+p_3^2$ . Example for $l=2$ : the vector space $\mathfrak{h}_3^2$ is spanned by the polynomials $p_1=x^2-y^2$ , $p_2=y^2-z^2$ , $q_1=yz$ , $q_2=xz$ , $q_3=xy$ . Now consider the algebra $\mathbb{R}[\mathfrak{h}_3^2]=\mathbb{R}[p_1,p_2,q_1,q_2,q_3]$ . A quick calculation shows that the polynomial $f:=p_1^2+p_1p_2+p_2^2+3(q_1^2+q_2^2+q_3^2)$ is $\mathrm{SO}(3,\mathbb{R})$ -invariant. Can anyone give such a polynomial for arbitrary $l$ ? Please note that the realization of $V_l$ as $\mathfrak{h}_3^l$ is not strictly necessary, and an intrinsic description would probably be better.","The Lie group has a representation of dimension for each I am looking for a single polynomial in which is invariant under the action. The obvious example for is This is a duplicate of this 2017 MSE question and the answer there is helpful but only partial. Unless I'm mistaken, -invariance should boil down to -invariance, and thus to -invariance. This (under the guise of binary forms) has been studied for over 100 years and people apparently have found (minimal) generators of invariants of for up to , maybe further. But I'm not asking for a full set of generators, I just want one (nonconstant) polynomial for each . Edit : I'm clarifying the action of on for the benefit of a commenter below. I also provide a nontrivial example for . The action of on has been known for a very long time. The idea is that there's a map with kernel , so any irreducible representation of where acts trivially (namely, the irreps of odd dimension) will descend to a representation of . The simplest way of realizing this representation is in terms of harmonic polynomials. Note that acts naturally on , the space of homogeneous polynomials of degree . Let be the -dimensional vector subspace of consisting of the degree harmonic polynomials. This is defined as the kernel of the Laplacian . The subspace is preserved by the action of because the elements of commute with the Laplacian. Finally, my question: acts on . I seek a polynomial in this space which is fixed by the action (it is known to exist!). Example for : the vector space is spanned by the polynomials (they are all killed by the Laplacian). Now and there is an obvious -invariant polynomial of degree : . Example for : the vector space is spanned by the polynomials , , , , . Now consider the algebra . A quick calculation shows that the polynomial is -invariant. Can anyone give such a polynomial for arbitrary ? Please note that the realization of as is not strictly necessary, and an intrinsic description would probably be better.","\mathrm{SO}(3,\mathbb{R}) V_l 2l+1 l=0,1,\ldots \mathbb{R}[V_l] \mathrm{SO}(3) l=1 x^2+y^2+z^2 \mathrm{SO}(3,\mathbb{R}) \mathrm{SU}(2) \mathrm{SL}(2,\mathbb{C}) k[\mathrm{Sym}^n(k^2)] n=8 n \mathrm{SO}(3,\mathbb{R}) \mathbb{R}^{l+1} l=2 \mathrm{SO}(3,\mathbb{R}) \mathbb{R}^{2l+1} \mathrm{SU}(2)\rightarrow \mathrm{SO}(3,\mathbb{R}) I, -I \mathrm{SU}(2) -I \mathrm{SO}(3,\mathbb{R}) \mathrm{SO}(3)\subset \mathrm{GL}(3) \mathbb{R}[x,y,z]_l l \mathfrak{h}_3^l 2l+1 \mathbb{R}[x,y,z]_l l \Delta=\frac{\partial}{\partial x^2}+ \frac{\partial}{\partial y^2}+\frac{\partial}{\partial z^2} \mathfrak{h}_3^l \mathrm{SO}(3,\mathbb{R}) \mathrm{SO}(3,\mathbb{R}) \mathrm{SO}(3,\mathbb{R}) \mathbb{R}[\mathfrak{h}_3^l] l=1 \mathfrak{h}_3^1 p_1=x,p_2=y,p_3=z \mathbb{R}[\mathfrak{h}_3^1]=\mathbb{R}[p_1,p_2,p_3] \mathrm{SO}(3,\mathbb{R}) 2 p_1^2+p_2^2+p_3^2 l=2 \mathfrak{h}_3^2 p_1=x^2-y^2 p_2=y^2-z^2 q_1=yz q_2=xz q_3=xy \mathbb{R}[\mathfrak{h}_3^2]=\mathbb{R}[p_1,p_2,q_1,q_2,q_3] f:=p_1^2+p_1p_2+p_2^2+3(q_1^2+q_2^2+q_3^2) \mathrm{SO}(3,\mathbb{R}) l V_l \mathfrak{h}_3^l","['abstract-algebra', 'representation-theory', 'lie-groups', 'lie-algebras', 'invariant-theory']"
52,"Multiply an integer polynomial with another integer polynomial to get a ""big"" coefficient","Multiply an integer polynomial with another integer polynomial to get a ""big"" coefficient",,"I am new to number theory, I was wondering if the following questions have been studied before. Given $f(x) =  a_0 + a_1 x + a_2 x^2 \cdots + a_n x^n \in \mathbb Z[x]$ , we say that $f(x)$ has a big coefficient $a_i$ if $|a_i| >  \frac{1}{2} \sum_{k=0}^n\left|a_k\right|$ (so that $|a_i|$ is larger than the sum of the remaining coefficients' absolute values). It's easy to see that if a polynomial $P(x)$ has a (real or complex) root with modulus $1$ , then for any $Q(x) \in \mathbb Z[x]$ , the product $P(x)Q(x)$ does not have a big coefficient. I was wondering if the converse of this is true: Question: Suppose $P(x)$ does not have a root with modulus $1$ . Does this implies that there exists $Q(x) \in \mathbb Z[x]$ such that $P(x)Q(x)$ has a big coefficient? Thank you for reading. Any relevant idea/reference would be really appreciated.","I am new to number theory, I was wondering if the following questions have been studied before. Given , we say that has a big coefficient if (so that is larger than the sum of the remaining coefficients' absolute values). It's easy to see that if a polynomial has a (real or complex) root with modulus , then for any , the product does not have a big coefficient. I was wondering if the converse of this is true: Question: Suppose does not have a root with modulus . Does this implies that there exists such that has a big coefficient? Thank you for reading. Any relevant idea/reference would be really appreciated.",f(x) =  a_0 + a_1 x + a_2 x^2 \cdots + a_n x^n \in \mathbb Z[x] f(x) a_i |a_i| >  \frac{1}{2} \sum_{k=0}^n\left|a_k\right| |a_i| P(x) 1 Q(x) \in \mathbb Z[x] P(x)Q(x) P(x) 1 Q(x) \in \mathbb Z[x] P(x)Q(x),"['abstract-algebra', 'combinatorics', 'number-theory', 'elementary-number-theory', 'polynomials']"
53,Number of invertible symmetric $3\times 3$ matrices over a finite field $F =\mathbb{F}_q$,Number of invertible symmetric  matrices over a finite field,3\times 3 F =\mathbb{F}_q,"I was trying to find the number of symmetric invertible matrices of size $n\times n$ over a finite field $F= \mathbb{F}_q$ ( $q$ odd for simplicity) For $n=2$ it has to do with counting the number of zeroes of a quadratic form over a finite field, something that was seen before on this site. For $n = 3$ it is a completely new problem. I am trying it in this way: The first subset of (symmetric) invertible matrices are the one having a Gauss decomposition $$A = L D U$$ where $L$ is lower triangular unipotent, $U$ is upper triangular unipotent, and $D$ is diagonal with non-zero diagonal values. If $A$ is moreover symmetric then $L = U^t$ .  Now the number of such matrices is simple to determine. The  matrices with  a Gauss decomposition are those for which $D_1$ , $\ldots$ , $D_n \ne 0$ . where $D_k$ are the leading minors, $1\le k \le n$ . So we got a subset of the set of symmetric invertible matrices. Let's try to determine the number of $n\times n$ symmetric invertible matrices. Now, let $d_{n-1}$ the number of symmetric $(n-1)\times (n-1)$ matrices that are invertible.  From this we can complete the off diagonal elements in $q^{n-1}$ ways, and the last diagonal element in $(q-1)$ ways ( use the Cauchy formula for expanding the determinant ). The problem is what happens if the $D_{n-1}$ leading minor is in fact zero. Now its rank starts to matter.  Perhaps I could cover the case $n=3$ , but it looks a bit confusing. Notes: I am aware there exists a paper that solves this problem ( symmetric matrices with $0$ determinant over the ring $\mathbb{Z}_m$ ), but it seems hard to read.   Even inequalities would be helpful. Any feedback would be welcome!","I was trying to find the number of symmetric invertible matrices of size over a finite field ( odd for simplicity) For it has to do with counting the number of zeroes of a quadratic form over a finite field, something that was seen before on this site. For it is a completely new problem. I am trying it in this way: The first subset of (symmetric) invertible matrices are the one having a Gauss decomposition where is lower triangular unipotent, is upper triangular unipotent, and is diagonal with non-zero diagonal values. If is moreover symmetric then .  Now the number of such matrices is simple to determine. The  matrices with  a Gauss decomposition are those for which , , . where are the leading minors, . So we got a subset of the set of symmetric invertible matrices. Let's try to determine the number of symmetric invertible matrices. Now, let the number of symmetric matrices that are invertible.  From this we can complete the off diagonal elements in ways, and the last diagonal element in ways ( use the Cauchy formula for expanding the determinant ). The problem is what happens if the leading minor is in fact zero. Now its rank starts to matter.  Perhaps I could cover the case , but it looks a bit confusing. Notes: I am aware there exists a paper that solves this problem ( symmetric matrices with determinant over the ring ), but it seems hard to read.   Even inequalities would be helpful. Any feedback would be welcome!",n\times n F= \mathbb{F}_q q n=2 n = 3 A = L D U L U D A L = U^t D_1 \ldots D_n \ne 0 D_k 1\le k \le n n\times n d_{n-1} (n-1)\times (n-1) q^{n-1} (q-1) D_{n-1} n=3 0 \mathbb{Z}_m,"['abstract-algebra', 'matrices', 'finite-groups', 'finite-fields']"
54,Does $\sqrt a + \sqrt b$ have a four way conjugate?,Does  have a four way conjugate?,\sqrt a + \sqrt b,"Let $a, b$ be rational numbers that are not perfect squares.  Consider the set $S = \{\sqrt a + \sqrt b, \sqrt a - \sqrt b, - \sqrt a + \sqrt b, -\sqrt a - \sqrt b\}$ . If $p$ is a polynomial with rational coefficients, is it correct that if any element of $S$ is a root of $p$ , then every element of $S$ is? That is implied by the excellent post defining conjugates , and I want to confirm my understanding.","Let be rational numbers that are not perfect squares.  Consider the set . If is a polynomial with rational coefficients, is it correct that if any element of is a root of , then every element of is? That is implied by the excellent post defining conjugates , and I want to confirm my understanding.","a, b S = \{\sqrt a + \sqrt b, \sqrt a - \sqrt b, - \sqrt a + \sqrt b, -\sqrt a - \sqrt b\} p S p S","['abstract-algebra', 'polynomials', 'field-theory', 'irreducible-polynomials']"
55,Permutation analog of a character table,Permutation analog of a character table,,"I’m trying to learn a little bit about representation theory because it sometimes comes up in discussions in some math hobby groups that I’m in. I have no background in representation theory and my question is really, really naive. I don’t know how to compute a character table for a small, easy-to-work-with group by hand. Is there an analog to a character table for permutation representations and is it simpler to compute by hand? Richard Borcherds’ first lecture on representation theory introduces some of the basic concepts and constructs a character table for $S_3$ at 20m25s . $$   \begin{array}{lccc}     & \begin{matrix} \\ \\ \mathrm{id} \end{matrix}     & \begin{matrix} (12) \\[-0.2em] (23) \\[-0.2em] (31) \end{matrix}     & \begin{matrix} \\ (123) \\[-0.2em] (132) \end{matrix} \\[0.5em]     \hline     \chi_1 & 1 & \phantom{-}1 & \phantom{-}1 \\     \chi_2 & 1 & -1 & \phantom{-}1 \\     \chi_3 & 2 & \phantom{-}0 & -1   \end{array} $$ And then he says some facts about this table that I find absolutely amazing: It’s square. The columns are orthogonal. The rows are orthogonal (when weighted by conjugacy class size). I didn’t get this at first, but $\chi_3$ corresponds to the standard representation , which only costs you two dimensions in $\mathbb{C}$ to represent (as opposed to 3 if you do it naively by translating permutations into matrices). What follows is my attempt to build a permutation character table for $S_3$ . Wikipedia’s entry on character tables mentions that the table itself encodes information about irreducible representations over $\mathbb{C}$ . However, you can take traces of matrices in any vector space , this makes me wonder whether it is possible to construct something analogous to this table for permutation representations of $S_3$ . This would be a binary matrix, since traces would give us $1$ or $0$ and would count the parity of the number of fixed points. I’ll call my new pseudo-characters $\psi_1, \psi_2, \dotsc$ We have the identity pseudocharacter $\psi_1$ , which sends everything to $1$ , associated with the trivial permutation representation. We have the representation that sends elements of the transposition conjugacy class to $\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$ and everything else to the identity matrix. This representation is associated with a pseudocharacter $\psi_2$ which sends everything to $0$ because it’s two-dimensional and two is even. I guess this thing is irreducible as a permutation representation, but I don’t know how to prove it one way or the other. It certainly is not irreducible as a complex representation, though, since it isn’t listed in the above character table. And finally we have the faithful permutation representation, which we use in the above table to name the elements of the conjugacy classes of $S_3$ above. The identity permutation has three fixed points, a transposition has one, and derangements have no fixed points, so $\psi_3$ sends the identity and the transpositions to $1$ and the others to $2$ . We end up with the following table, which is much less nice. $$   \begin{array}{lccc}     & \begin{matrix} \\ \\ \mathrm{id} \end{matrix}     & \begin{matrix} (12) \\[-0.2em] (23) \\[-0.2em] (31) \end{matrix}     & \begin{matrix} \\ (123) \\[-0.2em] (132) \end{matrix} \\[0.5em]     \hline     \psi_1 & 1 & 1 & 1 \\     \psi_2 & 0 & 0 & 0 \\     \psi_3 & 1 & 1 & 0   \end{array} $$ We can also count the number of fixed points instead, which is also invariant under conjugation. $$   \begin{array}{lccc}     & \begin{matrix} \\ \\ \mathrm{id} \end{matrix}     & \begin{matrix} (12) \\[-0.2em] (23) \\[-0.2em] (31) \end{matrix}     & \begin{matrix} \\ (123) \\[-0.2em] (132) \end{matrix} \\[0.5em]     \hline     \psi_1 & 1 & 1 & 1 \\     \psi_2 & 2 & 0 & 2 \\     \psi_3 & 3 & 1 & 0   \end{array} $$ Anyway, since our group $S_3$ has three normal subgroups and we are effectively counting the number of fixed points in permutation representations of quotient groups of $S_3$ , I think we can use the above table to tell us what kinds of numbers of fixed points for each conjugacy class are possible in any permutation representation of $S_3$ , for example $\begin{bmatrix} 4 & 2 & 1 \end{bmatrix}$ is possible if we act on three elements standardly and leave a fourth untouched.","I’m trying to learn a little bit about representation theory because it sometimes comes up in discussions in some math hobby groups that I’m in. I have no background in representation theory and my question is really, really naive. I don’t know how to compute a character table for a small, easy-to-work-with group by hand. Is there an analog to a character table for permutation representations and is it simpler to compute by hand? Richard Borcherds’ first lecture on representation theory introduces some of the basic concepts and constructs a character table for at 20m25s . And then he says some facts about this table that I find absolutely amazing: It’s square. The columns are orthogonal. The rows are orthogonal (when weighted by conjugacy class size). I didn’t get this at first, but corresponds to the standard representation , which only costs you two dimensions in to represent (as opposed to 3 if you do it naively by translating permutations into matrices). What follows is my attempt to build a permutation character table for . Wikipedia’s entry on character tables mentions that the table itself encodes information about irreducible representations over . However, you can take traces of matrices in any vector space , this makes me wonder whether it is possible to construct something analogous to this table for permutation representations of . This would be a binary matrix, since traces would give us or and would count the parity of the number of fixed points. I’ll call my new pseudo-characters We have the identity pseudocharacter , which sends everything to , associated with the trivial permutation representation. We have the representation that sends elements of the transposition conjugacy class to and everything else to the identity matrix. This representation is associated with a pseudocharacter which sends everything to because it’s two-dimensional and two is even. I guess this thing is irreducible as a permutation representation, but I don’t know how to prove it one way or the other. It certainly is not irreducible as a complex representation, though, since it isn’t listed in the above character table. And finally we have the faithful permutation representation, which we use in the above table to name the elements of the conjugacy classes of above. The identity permutation has three fixed points, a transposition has one, and derangements have no fixed points, so sends the identity and the transpositions to and the others to . We end up with the following table, which is much less nice. We can also count the number of fixed points instead, which is also invariant under conjugation. Anyway, since our group has three normal subgroups and we are effectively counting the number of fixed points in permutation representations of quotient groups of , I think we can use the above table to tell us what kinds of numbers of fixed points for each conjugacy class are possible in any permutation representation of , for example is possible if we act on three elements standardly and leave a fourth untouched.","S_3 
  \begin{array}{lccc}
    & \begin{matrix} \\ \\ \mathrm{id} \end{matrix}
    & \begin{matrix} (12) \\[-0.2em] (23) \\[-0.2em] (31) \end{matrix}
    & \begin{matrix} \\ (123) \\[-0.2em] (132) \end{matrix} \\[0.5em]
    \hline
    \chi_1 & 1 & \phantom{-}1 & \phantom{-}1 \\
    \chi_2 & 1 & -1 & \phantom{-}1 \\
    \chi_3 & 2 & \phantom{-}0 & -1
  \end{array}
 \chi_3 \mathbb{C} S_3 \mathbb{C} S_3 1 0 \psi_1, \psi_2, \dotsc \psi_1 1 \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \psi_2 0 S_3 \psi_3 1 2 
  \begin{array}{lccc}
    & \begin{matrix} \\ \\ \mathrm{id} \end{matrix}
    & \begin{matrix} (12) \\[-0.2em] (23) \\[-0.2em] (31) \end{matrix}
    & \begin{matrix} \\ (123) \\[-0.2em] (132) \end{matrix} \\[0.5em]
    \hline
    \psi_1 & 1 & 1 & 1 \\
    \psi_2 & 0 & 0 & 0 \\
    \psi_3 & 1 & 1 & 0
  \end{array}
 
  \begin{array}{lccc}
    & \begin{matrix} \\ \\ \mathrm{id} \end{matrix}
    & \begin{matrix} (12) \\[-0.2em] (23) \\[-0.2em] (31) \end{matrix}
    & \begin{matrix} \\ (123) \\[-0.2em] (132) \end{matrix} \\[0.5em]
    \hline
    \psi_1 & 1 & 1 & 1 \\
    \psi_2 & 2 & 0 & 2 \\
    \psi_3 & 3 & 1 & 0
  \end{array}
 S_3 S_3 S_3 \begin{bmatrix} 4 & 2 & 1 \end{bmatrix}","['abstract-algebra', 'group-theory', 'representation-theory', 'characters']"
56,Computing the Galois group of the splitting field of $X^{q+1} + X + T$ over the function field $\mathbb{F}_q(T)$,Computing the Galois group of the splitting field of  over the function field,X^{q+1} + X + T \mathbb{F}_q(T),"Let $k = \mathbb{F}_q$ , $A=k[T]$ and $K=k(T)$ . Let $f(X) := X^{q+1} + X + T \in K[X]$ . Let $L$ be the splitting field of $f$ over $K$ . (In other words, fixing an algebraic closure $K^{\mathrm{alg}}$ of $K$ , let $L$ be the field generated by all the roots of $f$ in $K^{\mathrm{alg}}$ over $K$ ). My question : How to compute the Galois group of $L/K$ ? We explain the motivation for the above question in the remaining part of this post. But it seems that the question itself can be well-presented without the motivation below. Motivation : The problem arises when I'm learning Drinfeld modules. Let $\phi: A \rightarrow \operatorname{End}(\mathbb{G}_a)$ be a Drinfeld module over $K$ of rank 2, defined by $$ \phi_T := T + \tau + \tau^2. $$ Then the absolute Galois group $\operatorname{Gal}_K$ acts on the $T$ -torsion points $\phi[T]$ of $\phi$ , which induces a Galois representation $$ \pi_T: \operatorname{Gal}_K \rightarrow \operatorname{GL}_2(A/(T)) = \operatorname{GL}_2(\mathbb{F}_q). $$ In paper here , the authors found that in general $\pi_T$ may not be surjective. My ultimate goal is to check that in this case, $\pi_T$ is indeed surjective. Then by definition of $T$ -torsion point, we have an injection of groups $$ \overline{\pi_T}: \operatorname{Gal}(K(\phi[T])/K) \hookrightarrow \operatorname{GL}_2(\mathbb{F}_q), $$ and $K(\phi[T])$ is merely the splitting field of $\Phi_T(X)$ over $K$ , where $\Phi_T$ is the ""untwisted version"" of $\phi_T$ : $$ \Phi_T (X) = TX + X^{q} + X^{q^2} = X(T+ X^{q-1} + (X^{q-1})^{q+1}). $$ So we break the extension $K(\phi[T])/K$ into the splitting field $L$ of $f(Y) := T + Y + Y^{q+1} \in K[Y]$ and an extension adjointing some "" $(q-1)$ -th"" root. So to get my desired surjectivity of $\pi_T$ , by counting $\#(\operatorname{GL}_2(\mathbb{F}_q))$ , I'm hoping that at least we know the order of $L/K$ is $\geq q(q-1)(q+1)$ . But I got stuck here on how to calculating the Galois group, or at least the order of $L/K$ . Something more : It is more illustrating if one comes with a rank two Drinfeld module with nonsurjective $\pi_T$ . I was told to try $\psi_T := T + \tau + T^q \tau^2$ , for which $\pi_T$ has image $\operatorname{SL}_2(\mathbb{F}_q)$ . This is even more chanllenging for me, as merely by counting may not be suffice to settle down the subgroup structure of $\operatorname{GL}_2(\mathbb{F}_q)$ . Sorry for such a long post and thank you all for help! EDIT on July 13 after Jyrki Lahtonen's hint : It seems that Gauss' lemma really works!  To show $f(X) \in K[X]=k(T)[X]$ is irreducible, suppose otherwise it is reducible in $k(T)[X]$ , then by Gauss' lemma, it is reducible in $k[T][X]=k[T,X]$ . Since $\deg_{T}(f)=1$ , it factors as $$ f(X,T)=f_1(X,T) f_2(X),  $$ where $f_1(X,T) = T + g_1(X) \in k[X][T]$ and $g_1(X),f_2(X) \in k[X]$ . But now compare the coefficient of $T$ in $f \in k[X,T]$ , we see that $f_2(X)=1$ , showing that $f$ is irreducible in $k[X][T]$ and hence irreducible in $k(T)[X]$ .","Let , and . Let . Let be the splitting field of over . (In other words, fixing an algebraic closure of , let be the field generated by all the roots of in over ). My question : How to compute the Galois group of ? We explain the motivation for the above question in the remaining part of this post. But it seems that the question itself can be well-presented without the motivation below. Motivation : The problem arises when I'm learning Drinfeld modules. Let be a Drinfeld module over of rank 2, defined by Then the absolute Galois group acts on the -torsion points of , which induces a Galois representation In paper here , the authors found that in general may not be surjective. My ultimate goal is to check that in this case, is indeed surjective. Then by definition of -torsion point, we have an injection of groups and is merely the splitting field of over , where is the ""untwisted version"" of : So we break the extension into the splitting field of and an extension adjointing some "" -th"" root. So to get my desired surjectivity of , by counting , I'm hoping that at least we know the order of is . But I got stuck here on how to calculating the Galois group, or at least the order of . Something more : It is more illustrating if one comes with a rank two Drinfeld module with nonsurjective . I was told to try , for which has image . This is even more chanllenging for me, as merely by counting may not be suffice to settle down the subgroup structure of . Sorry for such a long post and thank you all for help! EDIT on July 13 after Jyrki Lahtonen's hint : It seems that Gauss' lemma really works!  To show is irreducible, suppose otherwise it is reducible in , then by Gauss' lemma, it is reducible in . Since , it factors as where and . But now compare the coefficient of in , we see that , showing that is irreducible in and hence irreducible in .","k = \mathbb{F}_q A=k[T] K=k(T) f(X) := X^{q+1} + X + T \in K[X] L f K K^{\mathrm{alg}} K L f K^{\mathrm{alg}} K L/K \phi: A \rightarrow \operatorname{End}(\mathbb{G}_a) K 
\phi_T := T + \tau + \tau^2.
 \operatorname{Gal}_K T \phi[T] \phi 
\pi_T: \operatorname{Gal}_K \rightarrow \operatorname{GL}_2(A/(T)) = \operatorname{GL}_2(\mathbb{F}_q).
 \pi_T \pi_T T 
\overline{\pi_T}: \operatorname{Gal}(K(\phi[T])/K) \hookrightarrow \operatorname{GL}_2(\mathbb{F}_q),
 K(\phi[T]) \Phi_T(X) K \Phi_T \phi_T 
\Phi_T (X) = TX + X^{q} + X^{q^2} = X(T+ X^{q-1} + (X^{q-1})^{q+1}).
 K(\phi[T])/K L f(Y) := T + Y + Y^{q+1} \in K[Y] (q-1) \pi_T \#(\operatorname{GL}_2(\mathbb{F}_q)) L/K \geq q(q-1)(q+1) L/K \pi_T \psi_T := T + \tau + T^q \tau^2 \pi_T \operatorname{SL}_2(\mathbb{F}_q) \operatorname{GL}_2(\mathbb{F}_q) f(X) \in K[X]=k(T)[X] k(T)[X] k[T][X]=k[T,X] \deg_{T}(f)=1 
f(X,T)=f_1(X,T) f_2(X), 
 f_1(X,T) = T + g_1(X) \in k[X][T] g_1(X),f_2(X) \in k[X] T f \in k[X,T] f_2(X)=1 f k[X][T] k(T)[X]","['abstract-algebra', 'field-theory', 'galois-theory', 'algebraic-number-theory']"
57,Does $x_1^{a_1}+\dots+x_k^{a_k}\equiv0$ always have no solution modulo some $m$.,Does  always have no solution modulo some .,x_1^{a_1}+\dots+x_k^{a_k}\equiv0 m,"Let $x_1,\dots,x_k\geq1$ be fixed integers. Does there always exist an integer $m \ge 1$ so that $$x_1^{a_1}+\dots+x_k^{a_k}\equiv 0 \pmod{m}$$ has no solution for integers $a_1,\dots,a_k\geq0$ ? The following proves the affirmative if $x_1=\ldots=x_k=:x$ . Assume without loss of generality $x\geq2$ . Consider the modulus $m=x^\ell-1$ . Let $k\geq1$ be the minimum number such that $x^{a_1}+\dots+x^{a_k}\equiv0$ has a solution modulo $m$ . Since $x^\ell\equiv1\ (\mbox{mod}\ m)$ , we can take the $a_i$ values modulo $\ell$ . If $x$ of the $a_i$ values are equal, then they could have been replaced with a single $a_i$ value that is one larger. So every $a_i$ value appears at most $x-1$ times, giving $x^{a_1}+\dots+x^{a_k}\equiv c_0+c_1x+\dots+c_{\ell-1}x^{\ell-1}\equiv0$ for some $c_0,\dots,c_{\ell-1}\leq x-1$ summing to $k\geq1$ . We find the only solution is $c_0=\ldots=c_{\ell-1}=x-1$ giving $k=\ell(x-1)$ . Thus, if $k<\ell(x-1)$ then there is no solution. If there is some prime $p$ that divides all $x_i$ but one, then there is no solution modulo $p$ . So if $k=2$ then we can assume without loss of generality that $\mbox{rad}(x_1)=\mbox{rad}(x_2)=:r$ . The following uses this to prove the affirmative if $k=2$ . If we assume $m\equiv7\ (\mbox{mod}\ 8)$ , then $2$ is a quadratic residue modulo $m$ and $-1$ is not. Due to the Chinese remainder theorem, we can furthermore assume that $m$ is not a quadratic residue modulo all odd primes $p|r$ . Due to Dirichlet's prime number theorem on arithmetic progression we can furthermore assume that $m$ is prime. By the law of quadratic reciprocity, it follows that all odd primes $p|r$ are quadratic residues modulo $m$ . It follows that $x_1^{a_1}\equiv-x_2^{a_2}$ has no solution modulo $m$ , having a quadratic residue on the left hand side but not the right hand side. To give a bit of context, I saw a video about the equation $2^a+2^b=n!$ . I decided to generalize this to $x^{a_1}+\dots+x^{a_k}=n!$ and proved it always has finitely many solutions by showing there exists $m$ such that $x^{a_1}+\dots+x^{a_k}$ is never divisible by $m$ . As a bonus, I was also wondering if the equation $$1^{a_1}+\dots+n^{a_n}=n!$$ has a solution for infinitely many values of $n$ . This is probably highly non-trivial, for example with $n=6$ we find $1^1+2^3+4^1+3^4+5^4+6^0=6!$ . Solving this is not necessary to get your answer accepted, but I am thinking of giving a bounty if you do solve it, depending on how hard it was.","Let be fixed integers. Does there always exist an integer so that has no solution for integers ? The following proves the affirmative if . Assume without loss of generality . Consider the modulus . Let be the minimum number such that has a solution modulo . Since , we can take the values modulo . If of the values are equal, then they could have been replaced with a single value that is one larger. So every value appears at most times, giving for some summing to . We find the only solution is giving . Thus, if then there is no solution. If there is some prime that divides all but one, then there is no solution modulo . So if then we can assume without loss of generality that . The following uses this to prove the affirmative if . If we assume , then is a quadratic residue modulo and is not. Due to the Chinese remainder theorem, we can furthermore assume that is not a quadratic residue modulo all odd primes . Due to Dirichlet's prime number theorem on arithmetic progression we can furthermore assume that is prime. By the law of quadratic reciprocity, it follows that all odd primes are quadratic residues modulo . It follows that has no solution modulo , having a quadratic residue on the left hand side but not the right hand side. To give a bit of context, I saw a video about the equation . I decided to generalize this to and proved it always has finitely many solutions by showing there exists such that is never divisible by . As a bonus, I was also wondering if the equation has a solution for infinitely many values of . This is probably highly non-trivial, for example with we find . Solving this is not necessary to get your answer accepted, but I am thinking of giving a bounty if you do solve it, depending on how hard it was.","x_1,\dots,x_k\geq1 m \ge 1 x_1^{a_1}+\dots+x_k^{a_k}\equiv 0 \pmod{m} a_1,\dots,a_k\geq0 x_1=\ldots=x_k=:x x\geq2 m=x^\ell-1 k\geq1 x^{a_1}+\dots+x^{a_k}\equiv0 m x^\ell\equiv1\ (\mbox{mod}\ m) a_i \ell x a_i a_i a_i x-1 x^{a_1}+\dots+x^{a_k}\equiv c_0+c_1x+\dots+c_{\ell-1}x^{\ell-1}\equiv0 c_0,\dots,c_{\ell-1}\leq x-1 k\geq1 c_0=\ldots=c_{\ell-1}=x-1 k=\ell(x-1) k<\ell(x-1) p x_i p k=2 \mbox{rad}(x_1)=\mbox{rad}(x_2)=:r k=2 m\equiv7\ (\mbox{mod}\ 8) 2 m -1 m p|r m p|r m x_1^{a_1}\equiv-x_2^{a_2} m 2^a+2^b=n! x^{a_1}+\dots+x^{a_k}=n! m x^{a_1}+\dots+x^{a_k} m 1^{a_1}+\dots+n^{a_n}=n! n n=6 1^1+2^3+4^1+3^4+5^4+6^0=6!","['abstract-algebra', 'number-theory', 'modular-arithmetic']"
58,Prove certain ring is noetherian.,Prove certain ring is noetherian.,,"Let $R$ be a commutative ring with the property that for each nonzero ideal $I$ and element $a ∈ I$ there exists a unique ideal $J$ such that $IJ = (a)$ . Show that $R$ is Noetherian. I am looking for some elementary proof to use really basic ring and generators property to solve the problem. My attempt is to show $J$ is finitely generated and use $IJ=JI$ to show $I$ is finitely generated. I suppose for $a\in I$ , there exists $b_1,b_2,...,b_n$ , such that $\sum_{i=1}^na_ib_i=a$ for some $a_i\in I$ , where $n$ is the minimal number of such kind of summation holds. I decided to show $J=(b_1,b_2,...,b_n)$ such that $IJ=(a)$ . But I couldn't go further, any hint or suggestions are appreciated. Thanks. Edit: The assumption that for all $a\in I$ , there exists a unique $J$ , such that $IJ=(a)$ is used in this proof. If it is not the case, we can't conclude that for the same $a\in I\cap J$ , $IJ=JI=(a)$ .","Let be a commutative ring with the property that for each nonzero ideal and element there exists a unique ideal such that . Show that is Noetherian. I am looking for some elementary proof to use really basic ring and generators property to solve the problem. My attempt is to show is finitely generated and use to show is finitely generated. I suppose for , there exists , such that for some , where is the minimal number of such kind of summation holds. I decided to show such that . But I couldn't go further, any hint or suggestions are appreciated. Thanks. Edit: The assumption that for all , there exists a unique , such that is used in this proof. If it is not the case, we can't conclude that for the same , .","R I a ∈ I J IJ = (a) R J IJ=JI I a\in I b_1,b_2,...,b_n \sum_{i=1}^na_ib_i=a a_i\in I n J=(b_1,b_2,...,b_n) IJ=(a) a\in I J IJ=(a) a\in I\cap J IJ=JI=(a)","['abstract-algebra', 'ring-theory', 'commutative-algebra', 'noetherian']"
59,Non-abelian simple groups of odd order less than $10000$.,Non-abelian simple groups of odd order less than .,10000,"I am trying to solve problem 6.2.16 from Dummit and Foote, namely Prove there are no non-abelian simple groups of odd order $< 10000$ . I did something similar for order $<100$ , where I showed the only non-abelian simple group of order $< 100$ is $A_5$ , but this was done by looking at many different forms of order in terms of prime arrangements and picking a few special cases aside to rule everything out but order $60$ . The order is too big to do this here. The only thought I have is that $10000=100^2$ and so taking $G$ to be a minimal counterexample with $|G|>100$ we can write $|G|=ab$ with $a<100, b\geq 100$ . Apart from that not much that I feel could be constructive. Any help would be appreciated for getting to an elegant proof that doesn't grind through every prime arrangement.","I am trying to solve problem 6.2.16 from Dummit and Foote, namely Prove there are no non-abelian simple groups of odd order . I did something similar for order , where I showed the only non-abelian simple group of order is , but this was done by looking at many different forms of order in terms of prime arrangements and picking a few special cases aside to rule everything out but order . The order is too big to do this here. The only thought I have is that and so taking to be a minimal counterexample with we can write with . Apart from that not much that I feel could be constructive. Any help would be appreciated for getting to an elegant proof that doesn't grind through every prime arrangement.","< 10000 <100 < 100 A_5 60 10000=100^2 G |G|>100 |G|=ab a<100, b\geq 100","['abstract-algebra', 'group-theory', 'finite-groups', 'simple-groups']"
60,Do there exist groups with polynomial word problem and NP-complete conjugacy problem?,Do there exist groups with polynomial word problem and NP-complete conjugacy problem?,,"Suppose $G$ is a finitely-generated group and $A$ is a finite symmetric set of its generators. Define $\pi: A^* \to G$ using the following recurrence: $$\pi(\Lambda) = e$$ $$\pi(a \alpha) = a\pi(\alpha)$$ Then we have two computational problems: Word problem : Given a word $a \in A^*$ determine whether $\pi(a) = e$ ( $e$ is the group identity element). Conjugacy problem : Given two word $a, b \in A^*$ determine whether $\pi(a)$ and $pi(b)$ are conjugate. There are known quite many groups $G$ , for which the word problem can be solved in polynomial time (in respect to the length of the word). This class of groups includes all hyperbolic groups, automorphism groups of all free groups, all automatic groups and also many groups, that do not fall under any of these categories  (like, for example, the Baumslag-Gersten group $\langle a, b | a^{a^b}= a^2 \rangle$ ). For all such groups, the conjugacy problem belongs to NP. Indeed, if we take a word $c \in A^*$ such that $\pi(c)^{-1}\pi(a)\pi(c) = \pi(b)$ as a witness, we can verify it in polynomial time by solving the word problem for $c^{-1}acb^{-1}$ . However, does there exist a group, in which the word problem is polynomial and the conjugacy problem is NP-complete? Such group (if it exists) will be neither hyperbolic nor automatic (assuming $P \neq NP$ ), because for all groups from those classes the conjugacy problem is also solvable in polynomial time.","Suppose is a finitely-generated group and is a finite symmetric set of its generators. Define using the following recurrence: Then we have two computational problems: Word problem : Given a word determine whether ( is the group identity element). Conjugacy problem : Given two word determine whether and are conjugate. There are known quite many groups , for which the word problem can be solved in polynomial time (in respect to the length of the word). This class of groups includes all hyperbolic groups, automorphism groups of all free groups, all automatic groups and also many groups, that do not fall under any of these categories  (like, for example, the Baumslag-Gersten group ). For all such groups, the conjugacy problem belongs to NP. Indeed, if we take a word such that as a witness, we can verify it in polynomial time by solving the word problem for . However, does there exist a group, in which the word problem is polynomial and the conjugacy problem is NP-complete? Such group (if it exists) will be neither hyperbolic nor automatic (assuming ), because for all groups from those classes the conjugacy problem is also solvable in polynomial time.","G A \pi: A^* \to G \pi(\Lambda) = e \pi(a \alpha) = a\pi(\alpha) a \in A^* \pi(a) = e e a, b \in A^* \pi(a) pi(b) G \langle a, b | a^{a^b}= a^2 \rangle c \in A^* \pi(c)^{-1}\pi(a)\pi(c) = \pi(b) c^{-1}acb^{-1} P \neq NP","['abstract-algebra', 'group-theory', 'discrete-mathematics', 'computational-complexity', 'finitely-generated']"
61,"If $f$ is a surjective normal endomorphism of group $G$, then $f=1_G+g$ where $g$ send $G$ to its center.","If  is a surjective normal endomorphism of group , then  where  send  to its center.",f G f=1_G+g g G,"This is a question from Jacobson's BA 2. Explanation: An endomorphism of a group is normal if it commutes with every inner-automorphism of a group $G$ wrt the composition of functions. In following context, $1_G$ is identity endomorphism. And for any two endomorphism of G, say $f,g$ set $f+g:x\mapsto f(x)g(x)$ (p.s. I don't really like this notation why it uses plus sign even though it is not necessarily commutative) Anyway, I try to prove: If $f$ is a surjective normal endomorphism of group $G$ , then $f=1_G+g$ where $g$ send $G$ to its center Update: I have already proved the above proposition. I thought maybe some people do the same question(or homework) in the future and may be confused once as I do, so I leave my answer here rather than delete the post. if it can be proved that $f$ is idempotent on the derived group $G'$ of group G(i.e. the group generated by the commutator of $G$ ), then $f\circ f=1_G\circ f $ forall $x$ in $G'$ . Due to $f$ is surjective, It would be apparent that $f=1_G$ when $x\in f(G')=G'$ (to see $f(G')=G'$ , since $f(G')\subseteq G'$ , and let $f^{-1}$ be a choice function, then for any $xyx^{-1}y^{-1}=f(f^{-1}(x)f^{-1}(y)(f^{-1}(x))^{-1}(f^{-1}(y))^{-1})$ , there element acted by $f$ is apparently in $G'$ so $f(G')\supseteq G'$ ) Having that, it became much easier since forall $x,y\in G$ we have $$ xyx^{-1}y^{-1}=f(xyx^{-1}y^{-1})\\ xyx^{-1}y^{-1}f(y)=f(xyx^{-1})=xf(y)x^{-1} \implies x^{-1}y^{-1}f(y)=y^{-1}x^{-1}xf(y)x^{-1}\\\implies x^{-1}(y^{-1}f(y))x=y^{-1}f(y) $$ The fact that $f$ is idempotent on $G'$ (or all commutators of $G$ ) relies on $f$ being normal and being homomorphism: $$ f(xyx^{-1}y^{-1})=f(x)f(y)f(x^{-1})f(y^{})^{-1}=f(x)f(f(y)x^{-1}f(y^{-1}))=f(x)f^2(y)f(x)^{-1}f^2(y^{-1}) \\ =f(f(x)f(y)f(x)^{-1})f^2(y^{-1})=f^2(xyx^{-1}y^{-1})  $$","This is a question from Jacobson's BA 2. Explanation: An endomorphism of a group is normal if it commutes with every inner-automorphism of a group wrt the composition of functions. In following context, is identity endomorphism. And for any two endomorphism of G, say set (p.s. I don't really like this notation why it uses plus sign even though it is not necessarily commutative) Anyway, I try to prove: If is a surjective normal endomorphism of group , then where send to its center Update: I have already proved the above proposition. I thought maybe some people do the same question(or homework) in the future and may be confused once as I do, so I leave my answer here rather than delete the post. if it can be proved that is idempotent on the derived group of group G(i.e. the group generated by the commutator of ), then forall in . Due to is surjective, It would be apparent that when (to see , since , and let be a choice function, then for any , there element acted by is apparently in so ) Having that, it became much easier since forall we have The fact that is idempotent on (or all commutators of ) relies on being normal and being homomorphism:","G 1_G f,g f+g:x\mapsto f(x)g(x) f G f=1_G+g g G f G' G f\circ f=1_G\circ f  x G' f f=1_G x\in f(G')=G' f(G')=G' f(G')\subseteq G' f^{-1} xyx^{-1}y^{-1}=f(f^{-1}(x)f^{-1}(y)(f^{-1}(x))^{-1}(f^{-1}(y))^{-1}) f G' f(G')\supseteq G' x,y\in G  xyx^{-1}y^{-1}=f(xyx^{-1}y^{-1})\\ xyx^{-1}y^{-1}f(y)=f(xyx^{-1})=xf(y)x^{-1}
\implies x^{-1}y^{-1}f(y)=y^{-1}x^{-1}xf(y)x^{-1}\\\implies x^{-1}(y^{-1}f(y))x=y^{-1}f(y)  f G' G f  f(xyx^{-1}y^{-1})=f(x)f(y)f(x^{-1})f(y^{})^{-1}=f(x)f(f(y)x^{-1}f(y^{-1}))=f(x)f^2(y)f(x)^{-1}f^2(y^{-1})
\\
=f(f(x)f(y)f(x)^{-1})f^2(y^{-1})=f^2(xyx^{-1}y^{-1})
 ","['abstract-algebra', 'group-theory', 'group-homomorphism']"
62,surjective group homomorphism between $\mathbb{Z}^2$ and $\mathbb{Z}_{30}$,surjective group homomorphism between  and,\mathbb{Z}^2 \mathbb{Z}_{30},"Let $H = \langle (6,2), (3,6)\rangle$ , which is a subgroup of $\mathbb{Z}^2$ (denoted $H\leq \mathbb{Z}^2$ ). Show that $|\mathbb{Z}^2/H| = 30$ and that $\mathbb{Z}^2/ H$ is cyclic, and then find a surjective group homomorphism $\phi : \mathbb{Z}^2 \to \mathbb{Z}_{30}$ with $Ker(\phi) = H.$ I think that $\mathbb{Z}^2/ H = \{(r,s) + H : 0\leq r < 15, 0\leq s < 2\} =: T$ and $T = \langle (2,1) + H\rangle$ . Indeed $(2,1) + H \in T$ and $T\leq \mathbb{Z}^2/H$ so $\langle (2,1) + H\rangle \subseteq T.$ Also, every element in $T$ is a multiple of $(2,1) + H$ so $T\subseteq \langle (2,1) + H\rangle$ (one can show this using the fact that $(6,2), (15,0)\in H$ ). Also, by repeated use of the division algorithm, one can show that every coset is in $T.$ To show they're distinct one can obtain a contradiction from assuming $(r_1, s_1) + H = (r_2, s_2) + H$ if $(r_1, s_1)\neq (r_2, s_2) $ . One can show $s_1 = s_2$ and if $r_1 \neq r_2$ then they must differ by a multiple of $15.$ Also, $\phi : \mathbb{Z}^2/ H \to \mathbb{Z}_{30}, \phi((r,s) + H) = s \cdot 15 + r$ is a group isomorphism, which shows $\mathbb{Z}^2/ H\cong Z_{30}$ and hence $|\mathbb{Z}^2/ H| = 30$ . However, I’m not sure how to find a surjective group homomorphism $f: \mathbb{Z}^2 \to \mathbb{Z}_{30}$ with $Ker(\phi) = H.$ I tried determining the values of $f(1,0)$ and $f(0,1)$ but apparently I seem to get a contradiction.","Let , which is a subgroup of (denoted ). Show that and that is cyclic, and then find a surjective group homomorphism with I think that and . Indeed and so Also, every element in is a multiple of so (one can show this using the fact that ). Also, by repeated use of the division algorithm, one can show that every coset is in To show they're distinct one can obtain a contradiction from assuming if . One can show and if then they must differ by a multiple of Also, is a group isomorphism, which shows and hence . However, I’m not sure how to find a surjective group homomorphism with I tried determining the values of and but apparently I seem to get a contradiction.","H = \langle (6,2), (3,6)\rangle \mathbb{Z}^2 H\leq \mathbb{Z}^2 |\mathbb{Z}^2/H| = 30 \mathbb{Z}^2/ H \phi : \mathbb{Z}^2 \to \mathbb{Z}_{30} Ker(\phi) = H. \mathbb{Z}^2/ H = \{(r,s) + H : 0\leq r < 15, 0\leq s < 2\} =: T T = \langle (2,1) + H\rangle (2,1) + H \in T T\leq \mathbb{Z}^2/H \langle (2,1) + H\rangle \subseteq T. T (2,1) + H T\subseteq \langle (2,1) + H\rangle (6,2), (15,0)\in H T. (r_1, s_1) + H = (r_2, s_2) + H (r_1, s_1)\neq (r_2, s_2)  s_1 = s_2 r_1 \neq r_2 15. \phi : \mathbb{Z}^2/ H \to \mathbb{Z}_{30}, \phi((r,s) + H) = s \cdot 15 + r \mathbb{Z}^2/ H\cong Z_{30} |\mathbb{Z}^2/ H| = 30 f: \mathbb{Z}^2 \to \mathbb{Z}_{30} Ker(\phi) = H. f(1,0) f(0,1)","['abstract-algebra', 'group-theory', 'group-homomorphism']"
63,"Is there a universal property for homogeneous maps $ \phi(ax,ay) = a^k\phi(x,y) $?",Is there a universal property for homogeneous maps ?," \phi(ax,ay) = a^k\phi(x,y) ","Let $E,F$ and $G$ be vector spaces over the field $\Gamma$ and let $\phi:E\times F \to G$ be homogeneous map of degree $k \in \mathbb N$ , i.e., $$ \phi(ax,ay) = a^k\phi(x,y), \qquad \forall x \in E, y \in F a \in \Gamma $$ Is there universal property for such maps ? i.e., is there a pair $(\odot,H)$ where $\odot$ is a homogeneous map of degree $k$ on $E\times F$ into $H$ (a vector space) such that for every homogeneous map (degree $k$ ) $\phi$ there is a linear $f$ such that $f\circ \odot = \phi$ ? My attempt: Let $C(E\times F)$ be the free vector space over the $E\times F$ and let the $N$ be the subspace generated by all elements of the form $$ (ax,ay) - a^k(x,y) $$ Now consider the canonical projection $\pi:C(E\times F) \to C(E\times F)/N$ ,  then define the linear map $h:C(E\times F) \to G$ such that $h((x,y)) = \phi(x,y)$ . It can be shown that $N \subset \ker h$ . Then by the universal property of quotient maps there is a unique linear map $f:C(E\times F)/N \to G$ such that $f \circ \pi = h$ . If the restriction of $\pi$ to $E\times F$ is denoted $\odot$ , then this a homogeneous map of degree $k$ , and it follows that $f\circ\odot=\phi$ , and if $C(E\times F)/N$ is denoted $H$ then we have the pair $(\odot,H)$ . Please comment!, I would like to know whether there is any mistake, or if something like this universal property of homogeneous maps does exist at all. Thanks in advance! Added I have the following two comments on this construction: I . This construction can be carried out for homogeneous maps $\phi:V_1\times\cdots\times V_n \to W$ provided the subspace $N$ of $C(V_1\times\cdots\times V_n)$ is modified accordingly, i.e., to be generated by all elements of the form $(av_1,\cdots,av_n)-a^k(v_1,\cdots,v_n)$ II . The basis of $H$ for the case of one Vector space $n=1$ with finite dimension $d > 1$ and over $\mathbb R$ or $\mathbb C$ , is uncountably infinite. Since such maps are determined by their action on all lines through the origin. To see this, take a basis in $V$ , then each direction is determined by $d-1$ numbers, and there is uncountably infinite directions to determine the action of $\phi$ on them. In contrast to ( $p$ -)linear maps on $V$ into $V$ (for example), which requires only $d^{(p)}\cdot d$ numbers to determine a ( $p$ -) linear map, (finite dimensional tensor product space.)","Let and be vector spaces over the field and let be homogeneous map of degree , i.e., Is there universal property for such maps ? i.e., is there a pair where is a homogeneous map of degree on into (a vector space) such that for every homogeneous map (degree ) there is a linear such that ? My attempt: Let be the free vector space over the and let the be the subspace generated by all elements of the form Now consider the canonical projection ,  then define the linear map such that . It can be shown that . Then by the universal property of quotient maps there is a unique linear map such that . If the restriction of to is denoted , then this a homogeneous map of degree , and it follows that , and if is denoted then we have the pair . Please comment!, I would like to know whether there is any mistake, or if something like this universal property of homogeneous maps does exist at all. Thanks in advance! Added I have the following two comments on this construction: I . This construction can be carried out for homogeneous maps provided the subspace of is modified accordingly, i.e., to be generated by all elements of the form II . The basis of for the case of one Vector space with finite dimension and over or , is uncountably infinite. Since such maps are determined by their action on all lines through the origin. To see this, take a basis in , then each direction is determined by numbers, and there is uncountably infinite directions to determine the action of on them. In contrast to ( -)linear maps on into (for example), which requires only numbers to determine a ( -) linear map, (finite dimensional tensor product space.)","E,F G \Gamma \phi:E\times F \to G k \in \mathbb N 
\phi(ax,ay) = a^k\phi(x,y), \qquad \forall x \in E, y \in F a \in \Gamma
 (\odot,H) \odot k E\times F H k \phi f f\circ \odot = \phi C(E\times F) E\times F N 
(ax,ay) - a^k(x,y)
 \pi:C(E\times F) \to C(E\times F)/N h:C(E\times F) \to G h((x,y)) = \phi(x,y) N \subset \ker h f:C(E\times F)/N \to G f \circ \pi = h \pi E\times F \odot k f\circ\odot=\phi C(E\times F)/N H (\odot,H) \phi:V_1\times\cdots\times V_n \to W N C(V_1\times\cdots\times V_n) (av_1,\cdots,av_n)-a^k(v_1,\cdots,v_n) H n=1 d > 1 \mathbb R \mathbb C V d-1 \phi p V V d^{(p)}\cdot d p","['abstract-algebra', 'solution-verification', 'quotient-spaces', 'universal-property']"
64,A group $G$ has a finite number of subgroups if and only if $G$ is finite.,A group  has a finite number of subgroups if and only if  is finite.,G G,"I found this as an exercise, and wrote my own solution but am interested in a shorter/easier one. So here it goes: Statement: $G$ is a group $G$ has a finite number of subgroups <=> $G$ is finite. Proof: Suppose $G$ has an infinite number of elements but a finite number of subgroups. Let's look at the cyclic subgroups of $x$ where $x \in G$ . $A_G=\{\langle x\rangle : x \in G\}$ Since the elements of $A_g$ are subgroups of $G$ => $A_G$ has a finite number of elements. Obviously $\cup_{A \in A_G}{A} = G$ .(since every $x \in G$ would belong to $\langle x\rangle$ which is in $A_G$ . So it's a given that for some $x \in G$ , $\langle x\rangle$ must have an infinite number of elements. But then we can make infinitely many subgroups of $\langle x\rangle$ like: $\langle x^2\rangle$ , $\langle x^3\rangle$ , $\langle x^4\rangle$ ,etc.(which are all different, but to convince oneself, we can only look at $\langle x^p\rangle$ where p is prime.) Hence G has an infinite amount of subgroups which is a contradiction, so G has to be finite. Now in the opposite direction: Suppose G is finite. Let $|G|=n$ . $P(G)$ (the powerset of G) will have only $2^n$ elements. But the set of subgroups of G is a subset of $P(G)$ . Hence G has a finite number of subgroups.","I found this as an exercise, and wrote my own solution but am interested in a shorter/easier one. So here it goes: Statement: is a group has a finite number of subgroups <=> is finite. Proof: Suppose has an infinite number of elements but a finite number of subgroups. Let's look at the cyclic subgroups of where . Since the elements of are subgroups of => has a finite number of elements. Obviously .(since every would belong to which is in . So it's a given that for some , must have an infinite number of elements. But then we can make infinitely many subgroups of like: , , ,etc.(which are all different, but to convince oneself, we can only look at where p is prime.) Hence G has an infinite amount of subgroups which is a contradiction, so G has to be finite. Now in the opposite direction: Suppose G is finite. Let . (the powerset of G) will have only elements. But the set of subgroups of G is a subset of . Hence G has a finite number of subgroups.",G G G G x x \in G A_G=\{\langle x\rangle : x \in G\} A_g G A_G \cup_{A \in A_G}{A} = G x \in G \langle x\rangle A_G x \in G \langle x\rangle \langle x\rangle \langle x^2\rangle \langle x^3\rangle \langle x^4\rangle \langle x^p\rangle |G|=n P(G) 2^n P(G),"['abstract-algebra', 'group-theory', 'finite-groups', 'solution-verification']"
65,irreducibility of a certain polynomial,irreducibility of a certain polynomial,,"$\alpha =\sqrt{-1+\sqrt{-2}}$ and $\beta =\sqrt{-1-\sqrt{-2}}$ are roots of irreducible polynomial: $$f(x)=x^{4}+2x^2+3$$ over $\mathbb{Q}$ . Since $\alpha ^{2}+\beta ^{2}=-2$ , I want to prove that $$irr(\beta, \mathbb{Q}(\alpha ))=x^{2}+\alpha ^{2}+2$$ But I can't prove irreducibility of $x^{2}+\alpha ^{2}+2$ over $\mathbb{Q}(\alpha )$ . Help me please. Thank you in advance!","and are roots of irreducible polynomial: over . Since , I want to prove that But I can't prove irreducibility of over . Help me please. Thank you in advance!","\alpha =\sqrt{-1+\sqrt{-2}} \beta =\sqrt{-1-\sqrt{-2}} f(x)=x^{4}+2x^2+3 \mathbb{Q} \alpha ^{2}+\beta ^{2}=-2 irr(\beta, \mathbb{Q}(\alpha ))=x^{2}+\alpha ^{2}+2 x^{2}+\alpha ^{2}+2 \mathbb{Q}(\alpha )","['abstract-algebra', 'galois-theory']"
66,A lemma in Tensor Categories (Etingof et al),A lemma in Tensor Categories (Etingof et al),,"Lemma 8.10.5 in EGNO's Tensor Categories basically states Let $\mathcal{C}$ be a tensor category over an algebraically closed field $\mathbb{k}$ with braiding $c$ . For any nonzero simple object $X$ the composition \begin{align}     t(X) := \operatorname{ev}_X \circ c_{X, X^\vee} \circ \operatorname{coev}_X \in \operatorname{End}_{\mathcal{C}}(\mathbf{1})  \end{align} is nonzero. I feel very conflicted. On the one hand, the one line proof given in the book seems plausible: Since $X$ is simple, the corresponding composition \begin{align} \operatorname{End}(\mathbf{1}) \to \operatorname{Hom}(\mathbf{1}, X\otimes X^\vee) \to \operatorname{End}(\mathbf{1}) \end{align} consists of nonzero maps between 1-dimensional spaces, and is thus non-zero. On the other hand, suppose that the lemma holds and that $X$ is projective. Then $P = X \otimes X^\vee$ is projective. Set $f = t(X)^{-1} \operatorname{coev}_X$ and $g = \operatorname{ev}_X \circ c_{X, X^\vee}  $ . But then \begin{align}     \mathbf{1} \xrightarrow{f} P \xrightarrow{g} \mathbf{1} = \operatorname{id}_{\mathbf{1}} \ , \end{align} so that $\mathbf{1}$ , being a direct summand in a projective, is projective. But then $\mathcal{C}$ is semisimple. A contradiction to the existence of non-semisimple finite tensor categories with simple projective objects. Note that in fact the general heuristic in this last part implies that in a non-semisimple (finite) tensor category there exists no nonzero endomorphism of the tensor unit factoring through a projective object. For this heuristic, see also the proof of Theorem 6.6.1 in the book. So, where is the mistake? Edit: Here are two examples for non-semisimple finite tensor categories with simple projective objects: The symplectic fermions . This category is even factorizable, i.e. ribbon with a certain non-degeneracy condition on the braiding. The category of representations over the restricted quantum group $\overline{U}_q(sl_2)$ . Edit 2: The mistake is in the proof in the book. Namely, as I prove, the map $\operatorname{Hom}(\mathbf{1}, X \otimes X^\vee) \to \operatorname{End}(\mathbf{1})$ is zero if $X$ is projective.","Lemma 8.10.5 in EGNO's Tensor Categories basically states Let be a tensor category over an algebraically closed field with braiding . For any nonzero simple object the composition is nonzero. I feel very conflicted. On the one hand, the one line proof given in the book seems plausible: Since is simple, the corresponding composition consists of nonzero maps between 1-dimensional spaces, and is thus non-zero. On the other hand, suppose that the lemma holds and that is projective. Then is projective. Set and . But then so that , being a direct summand in a projective, is projective. But then is semisimple. A contradiction to the existence of non-semisimple finite tensor categories with simple projective objects. Note that in fact the general heuristic in this last part implies that in a non-semisimple (finite) tensor category there exists no nonzero endomorphism of the tensor unit factoring through a projective object. For this heuristic, see also the proof of Theorem 6.6.1 in the book. So, where is the mistake? Edit: Here are two examples for non-semisimple finite tensor categories with simple projective objects: The symplectic fermions . This category is even factorizable, i.e. ribbon with a certain non-degeneracy condition on the braiding. The category of representations over the restricted quantum group . Edit 2: The mistake is in the proof in the book. Namely, as I prove, the map is zero if is projective.","\mathcal{C} \mathbb{k} c X \begin{align}
    t(X) := \operatorname{ev}_X \circ c_{X, X^\vee} \circ \operatorname{coev}_X \in \operatorname{End}_{\mathcal{C}}(\mathbf{1}) 
\end{align} X \begin{align}
\operatorname{End}(\mathbf{1}) \to \operatorname{Hom}(\mathbf{1}, X\otimes X^\vee) \to \operatorname{End}(\mathbf{1})
\end{align} X P = X \otimes X^\vee f = t(X)^{-1} \operatorname{coev}_X g = \operatorname{ev}_X \circ c_{X, X^\vee}   \begin{align}
    \mathbf{1} \xrightarrow{f} P \xrightarrow{g} \mathbf{1} = \operatorname{id}_{\mathbf{1}}
\ ,
\end{align} \mathbf{1} \mathcal{C} \overline{U}_q(sl_2) \operatorname{Hom}(\mathbf{1}, X \otimes X^\vee) \to \operatorname{End}(\mathbf{1}) X","['abstract-algebra', 'category-theory', 'abelian-categories', 'projective-module', 'monoidal-categories']"
67,If Gal$(K/\mathbb{Q}) = S_5$ then $K$ is the splitting field of a degree $5$ polynomial,If Gal then  is the splitting field of a degree  polynomial,(K/\mathbb{Q}) = S_5 K 5,"Let $K$ be a Galois extension of $\mathbb{Q}$ whose Galois group is isomorphic to $S_5$ . Prove that $K$ is the splitting field of some polynomial of degree $5$ over $\mathbb{Q}$ . Since $K$ is a finite Galios extension over $\mathbb{Q}$ we know that $K$ is the splitting field of a separable polynomial $f$ over $\mathbb{Q}$ . Let $n$ be the degree of this separable polynomial. Since the Galois group acts on the roots $f$ via permutation we know that the Galois group is isomorphic to a subgroup of $S_n$ and hence $n \geq 5$ . Let $\alpha$ be a root of $f$ . Since since $|K : \mathbb{Q}| = |K :\mathbb{Q}(\alpha)| |\mathbb{Q}(\alpha):\mathbb{Q}| = n|K :\mathbb{Q}(\alpha)|$ , we have that $n|5! = 120$ . Therefore $n \in \{5, 6, 8, 10, 12, 15, 20, 24, 30, 40, 60, 120\}$ Any help would be appreciated.","Let be a Galois extension of whose Galois group is isomorphic to . Prove that is the splitting field of some polynomial of degree over . Since is a finite Galios extension over we know that is the splitting field of a separable polynomial over . Let be the degree of this separable polynomial. Since the Galois group acts on the roots via permutation we know that the Galois group is isomorphic to a subgroup of and hence . Let be a root of . Since since , we have that . Therefore Any help would be appreciated.","K \mathbb{Q} S_5 K 5 \mathbb{Q} K \mathbb{Q} K f \mathbb{Q} n f S_n n \geq 5 \alpha f |K : \mathbb{Q}| = |K :\mathbb{Q}(\alpha)| |\mathbb{Q}(\alpha):\mathbb{Q}| = n|K :\mathbb{Q}(\alpha)| n|5! = 120 n \in \{5, 6, 8, 10, 12, 15, 20, 24, 30, 40, 60, 120\}","['abstract-algebra', 'galois-theory', 'extension-field', 'symmetric-groups', 'galois-extensions']"
68,"If endomorphisms embed, does the vector space itself embed?","If endomorphisms embed, does the vector space itself embed?",,"This statement is known to be independent of ZFC: $ \def\qq{\mathbb{Q}} \def\pow{\mathcal{P}} \def\end{\mathrm{End}} \def\inj{\hookrightarrow} $ Given any sets $S,T$ , if $\pow(S) ≈ \pow(T)$ then $S ≈ T$ . So I was wondering whether the following 'algebraic' statement is also independent of ZFC: (★) Given any infinite-dimensional $\qq$ -vector spaces $V,W$ , if $\end(V)$ embeds into $\end(W)$ as a ring then $V$ embeds into $W$ . The motivation is to find some algebraic statement that has no set-theoretic flavour but is sensitive to set-theoretic assumptions. I know that if GCH holds, then (★) also holds. Proof : For every $\qq$ -vector space $V$ with basis $B$ we have that $\end(V)$ has the same cardinality as $(B×\qq)^B ≈ \qq^B$ (because $(B×\qq)^B$ $\inj (2×\qq)^{B×B}$ $≈ \qq^{B×B}$ , and $B×B ≈ B$ if $B$ is infinite). Now take any infinite-dimensional $\qq$ -vector spaces $V,W$ such that $\end(V)$ embeds into $\end(W)$ . Let $B,C$ be bases for $V,W$ respectively. Then $\qq^B \inj \qq^C$ and so $2^B \inj \qq^B \inj \qq^C \inj 2^C$ , and hence $B \inj C$ by GCH, yielding an embedding of $V$ into $W$ . But does ZFC already prove (★)? If not, is there some well-known set-theoretic axiom weaker than GCH that implies (★) over ZFC?","This statement is known to be independent of ZFC: Given any sets , if then . So I was wondering whether the following 'algebraic' statement is also independent of ZFC: (★) Given any infinite-dimensional -vector spaces , if embeds into as a ring then embeds into . The motivation is to find some algebraic statement that has no set-theoretic flavour but is sensitive to set-theoretic assumptions. I know that if GCH holds, then (★) also holds. Proof : For every -vector space with basis we have that has the same cardinality as (because , and if is infinite). Now take any infinite-dimensional -vector spaces such that embeds into . Let be bases for respectively. Then and so , and hence by GCH, yielding an embedding of into . But does ZFC already prove (★)? If not, is there some well-known set-theoretic axiom weaker than GCH that implies (★) over ZFC?","
\def\qq{\mathbb{Q}}
\def\pow{\mathcal{P}}
\def\end{\mathrm{End}}
\def\inj{\hookrightarrow}
 S,T \pow(S) ≈ \pow(T) S ≈ T \qq V,W \end(V) \end(W) V W \qq V B \end(V) (B×\qq)^B ≈ \qq^B (B×\qq)^B \inj (2×\qq)^{B×B} ≈ \qq^{B×B} B×B ≈ B B \qq V,W \end(V) \end(W) B,C V,W \qq^B \inj \qq^C 2^B \inj \qq^B \inj \qq^C \inj 2^C B \inj C V W","['abstract-algebra', 'ring-theory', 'vector-spaces', 'set-theory', 'provability']"
69,Reflective subcategories of monoids,Reflective subcategories of monoids,,"An exercise in The Joy of Cats, p. 59, is as follows: Show that no finite monoid, considered as a category, has a proper reflective subcategory. The obvious idea is to let $r : \cdot \to \cdot$ be a reflector. Then by assumption every arrow $f$ factors as $f' \circ r$ , where $f'$ is in the subcategory. Now if we can show that $r$ is itself in the subcategory, then we win... But this is giving me some trouble. The theorem is false for infinite monoids (and the second part of this problem, which I've done, gives a counterexample), but I'm not sure how to leverage finiteness without knowing my monoid is cancellative. We can conclude lots of things by pigeonhole, but I'm not sure how to apply them. Any help is appreciated ^_^","An exercise in The Joy of Cats, p. 59, is as follows: Show that no finite monoid, considered as a category, has a proper reflective subcategory. The obvious idea is to let be a reflector. Then by assumption every arrow factors as , where is in the subcategory. Now if we can show that is itself in the subcategory, then we win... But this is giving me some trouble. The theorem is false for infinite monoids (and the second part of this problem, which I've done, gives a counterexample), but I'm not sure how to leverage finiteness without knowing my monoid is cancellative. We can conclude lots of things by pigeonhole, but I'm not sure how to apply them. Any help is appreciated ^_^",r : \cdot \to \cdot f f' \circ r f' r,"['abstract-algebra', 'category-theory', 'monoid']"
70,A ring with $8$ elements,A ring with  elements,8,"Let $(A,+,\cdot)$ be a unitary ring with 8 elements. Prove that : $a)8=0$ and $k\neq 0$ , for any odd $k$ . b)If $\exists a\in A$ such that $a^3+a+1=0$ , then $a\neq 0$ , $a\neq 1$ , $2=0$ , $a^7=1$ and $A$ is a field. a) is pretty straightforward, it is obvious that $8=0$ from Lagrange's theorem in the additive group $(A,+)$ , and then if we had some odd $k$ such that $k=0$ then we would have that $1=0$ , which is a contradiction to $|A|=8$ . For b) the most difficult part is showing that $\operatorname{char}A=2$ . The fact that $a\neq 0$ and $a\neq 1$ is really trivial. Proving that $a^7=1$ is easy as well if we know that $2=0$ . From here we easily get that $A$ is a field and we are done. The only progress I made towards showing that $2=0$ was that $a$ is invertible(I guess it might help) because we have that $a(-a^2-1)=(-a^2-1)a=1$ . Obviously, we also have that $\operatorname{char}A\in \{2,4,8\}$ , but I didn't get any further.","Let be a unitary ring with 8 elements. Prove that : and , for any odd . b)If such that , then , , , and is a field. a) is pretty straightforward, it is obvious that from Lagrange's theorem in the additive group , and then if we had some odd such that then we would have that , which is a contradiction to . For b) the most difficult part is showing that . The fact that and is really trivial. Proving that is easy as well if we know that . From here we easily get that is a field and we are done. The only progress I made towards showing that was that is invertible(I guess it might help) because we have that . Obviously, we also have that , but I didn't get any further.","(A,+,\cdot) a)8=0 k\neq 0 k \exists a\in A a^3+a+1=0 a\neq 0 a\neq 1 2=0 a^7=1 A 8=0 (A,+) k k=0 1=0 |A|=8 \operatorname{char}A=2 a\neq 0 a\neq 1 a^7=1 2=0 A 2=0 a a(-a^2-1)=(-a^2-1)a=1 \operatorname{char}A\in \{2,4,8\}","['abstract-algebra', 'ring-theory']"
71,Rationalizing denominator containing a root of a polynomial - why is this possible / why does it work? [duplicate],Rationalizing denominator containing a root of a polynomial - why is this possible / why does it work? [duplicate],,"This question already has answers here : Intermediate ring between a field and an algebraic extension. (4 answers) Algebraic field extensions: Why $k(\alpha)=k[\alpha]$. (3 answers) Closed 4 years ago . The problem is to express the number $$1\over x^5 + 2x^4 + 3x^3 + 3x^2 + 2$$ using only rational numbers in the denominator, knowing that $x^5 + 2 = -2(x + 1)(x^3 + x^2 + x)$ . (This is an example from a final exam of an algebra course from the past year (which is freely available to students).) I know how to solve this problem, but I do not understand why this is possible. The trick comes from knowing that this number can be expressed using addition and multiplication of $x$ and rational numbers, ie. as $$r_0 + r_1x + r_2x^2 + r_3x^3 + r_4x^4 + r_5x^5 + \dots$$ My solution follows , skip to questions if you are not interested. From the polynomial equation it follows that $x^5$ equals $-2 - 2x - 4x^2 - 4x^3 - 2x^4$ , therefore an expression containing $x$ with exponent 5 or greater can be rewritten as an expression with only $x^0$ to $x^4$ . So my number can be expressed as $$ax^4 + bx^3 + cx^2 + dx + e$$ Putting an equals sign between these two expressions and multiplying by the denominator gives $$1 = (x^5 + 2x^4 + 3x^3 + 3x^2 + 2)(ax^4 + bx^3 + cx^2 + dx + e)$$ I can multiply the right hand side and ""normalize"" it (by replacing $x^{\ge5}$ ) to get $$1 = (e - f)x^4 + \dots + (2e - 2d)$$ From this formula I can create a system of linear equations, because the coefficient of $x^n$ has to be one if n equals zero, and zero otherwise. Solving this system will let me find the values of the coefficients. Question 0 : Why does this problem even have a solution of the form I used? Question 1 : Is there a solution like this for any expression $1 \over f(x)$ , where f is a polynomial of $x$ , knowing $x$ is a root of another polynomial? Will my method always work? Why? Apologies if this is a duplicate, but everything I could find about rationalizing the denominator was about simple fractions like $1 \over 2 + \sqrt 6$ . I'd appreciate even just comments directing me towards what I can study because I am out of keywords.","This question already has answers here : Intermediate ring between a field and an algebraic extension. (4 answers) Algebraic field extensions: Why $k(\alpha)=k[\alpha]$. (3 answers) Closed 4 years ago . The problem is to express the number using only rational numbers in the denominator, knowing that . (This is an example from a final exam of an algebra course from the past year (which is freely available to students).) I know how to solve this problem, but I do not understand why this is possible. The trick comes from knowing that this number can be expressed using addition and multiplication of and rational numbers, ie. as My solution follows , skip to questions if you are not interested. From the polynomial equation it follows that equals , therefore an expression containing with exponent 5 or greater can be rewritten as an expression with only to . So my number can be expressed as Putting an equals sign between these two expressions and multiplying by the denominator gives I can multiply the right hand side and ""normalize"" it (by replacing ) to get From this formula I can create a system of linear equations, because the coefficient of has to be one if n equals zero, and zero otherwise. Solving this system will let me find the values of the coefficients. Question 0 : Why does this problem even have a solution of the form I used? Question 1 : Is there a solution like this for any expression , where f is a polynomial of , knowing is a root of another polynomial? Will my method always work? Why? Apologies if this is a duplicate, but everything I could find about rationalizing the denominator was about simple fractions like . I'd appreciate even just comments directing me towards what I can study because I am out of keywords.",1\over x^5 + 2x^4 + 3x^3 + 3x^2 + 2 x^5 + 2 = -2(x + 1)(x^3 + x^2 + x) x r_0 + r_1x + r_2x^2 + r_3x^3 + r_4x^4 + r_5x^5 + \dots x^5 -2 - 2x - 4x^2 - 4x^3 - 2x^4 x x^0 x^4 ax^4 + bx^3 + cx^2 + dx + e 1 = (x^5 + 2x^4 + 3x^3 + 3x^2 + 2)(ax^4 + bx^3 + cx^2 + dx + e) x^{\ge5} 1 = (e - f)x^4 + \dots + (2e - 2d) x^n 1 \over f(x) x x 1 \over 2 + \sqrt 6,"['abstract-algebra', 'algebra-precalculus']"
72,Can a reduced ring have (# idempotents) $\in 3 \mathbb{Z}$?,Can a reduced ring have (# idempotents) ?,\in 3 \mathbb{Z},"This is a follow-up question to Is there a reduced ring with exactly $3$ idempotents? , to which the answer was ""no."" Note: In this question, 'ring' means ring with unity, but not necessarily commutative In fact, in a (non-trivial) reduced ring, the number of idempotents is either even or $\infty$ . The reason is that the idempotents come in pairs $e,1-e$ . And $e \neq1-e$ , otherwise $ee=e-ee$ and $e^2=0$ , implying (since the ring is reduced) that $e=0$ , which can't happen if $e=1-e$ . My next question is, does there exist a reduced ring whose number of idempotents is a multiple of $3$ ? (For example, can we find a reduced ring with $6$ idempotent elements? $12$ ? $18$ ? $3000$ ?) What about rings in general? (i.e. not necessarily reduced) Attempting the easiest case first, assume $R$ is a reduced ring and the idempotent elements are $\{0,1,a,(1-a),b,(1-b)\}$ (all distinct). I see that the product of two idempotents must be idempotent (since the idempotents commute with everything). Also, I see that the square of the difference of two idempotents must also be idempotent. So $ab \in \{0,1,a,(1-a),b,(1-b)\}$ . (I suspect that there might be a way to derive a contradiction from this, although I don't see how to do so yet.)","This is a follow-up question to Is there a reduced ring with exactly $3$ idempotents? , to which the answer was ""no."" Note: In this question, 'ring' means ring with unity, but not necessarily commutative In fact, in a (non-trivial) reduced ring, the number of idempotents is either even or . The reason is that the idempotents come in pairs . And , otherwise and , implying (since the ring is reduced) that , which can't happen if . My next question is, does there exist a reduced ring whose number of idempotents is a multiple of ? (For example, can we find a reduced ring with idempotent elements? ? ? ?) What about rings in general? (i.e. not necessarily reduced) Attempting the easiest case first, assume is a reduced ring and the idempotent elements are (all distinct). I see that the product of two idempotents must be idempotent (since the idempotents commute with everything). Also, I see that the square of the difference of two idempotents must also be idempotent. So . (I suspect that there might be a way to derive a contradiction from this, although I don't see how to do so yet.)","\infty e,1-e e \neq1-e ee=e-ee e^2=0 e=0 e=1-e 3 6 12 18 3000 R \{0,1,a,(1-a),b,(1-b)\} ab \in \{0,1,a,(1-a),b,(1-b)\}","['abstract-algebra', 'ring-theory', 'idempotents']"
73,"Degree of extension $\mathbb{Q}(\cos(2\pi/p),\sin(2\pi/p))/\mathbb{Q}(\cos(2\pi/p))$.",Degree of extension .,"\mathbb{Q}(\cos(2\pi/p),\sin(2\pi/p))/\mathbb{Q}(\cos(2\pi/p))","Let $p$ be an odd prime number. I want to compute the degree $$ [\mathbb{Q}(\cos(2\pi/p),\sin(2\pi/p)):\mathbb{Q}(\cos(2\pi/p))]. $$ I already showed that $$ [\mathbb{Q}(\cos(2\pi/p)):\mathbb{Q}] = \frac{p-1}{2}, $$ so it will be helpful to know $$ [\mathbb{Q}(\cos(2\pi/p),\sin(2\pi/p)):\mathbb{Q}]. $$ We know that the polynomial $$ X^2+\cos^2(2\pi/p)-1 \in \mathbb{Q}(\cos(2\pi/p))[X] $$ vanish on $\sin(2\pi/p)$ , but I don't how to show that this polynomial is irreducible over $\mathbb{Q}(\cos(2\pi/p))$ . I believe that $$ [\mathbb{Q}(\cos(2\pi/p),\sin(2\pi/p)):\mathbb{Q}(\cos(2\pi/p))]=2, $$ but I cannot see why it is possible or not to write $$ \sin(2\pi/p) = \sum_{k=0}^{n} a_k\cos^k(2\pi/p) $$ for some $a_0,a_1,\dots,a_n\in\mathbb{Q}$ . Any help will be appreciated!","Let be an odd prime number. I want to compute the degree I already showed that so it will be helpful to know We know that the polynomial vanish on , but I don't how to show that this polynomial is irreducible over . I believe that but I cannot see why it is possible or not to write for some . Any help will be appreciated!","p 
[\mathbb{Q}(\cos(2\pi/p),\sin(2\pi/p)):\mathbb{Q}(\cos(2\pi/p))].
 
[\mathbb{Q}(\cos(2\pi/p)):\mathbb{Q}] = \frac{p-1}{2},
 
[\mathbb{Q}(\cos(2\pi/p),\sin(2\pi/p)):\mathbb{Q}].
 
X^2+\cos^2(2\pi/p)-1 \in \mathbb{Q}(\cos(2\pi/p))[X]
 \sin(2\pi/p) \mathbb{Q}(\cos(2\pi/p)) 
[\mathbb{Q}(\cos(2\pi/p),\sin(2\pi/p)):\mathbb{Q}(\cos(2\pi/p))]=2,
 
\sin(2\pi/p) = \sum_{k=0}^{n} a_k\cos^k(2\pi/p)
 a_0,a_1,\dots,a_n\in\mathbb{Q}","['abstract-algebra', 'field-theory', 'extension-field']"
74,Question about $\operatorname{Aut}(S_6)$ and $\operatorname{Aut}(A_6)$,Question about  and,\operatorname{Aut}(S_6) \operatorname{Aut}(A_6),"From (1) , (2) , (3) , $[\operatorname{Aut}(S_6):\operatorname{Inn}(S_6)]=2$ . My question : $1$ . How to prove $\operatorname{Aut}(S_6)\cong S_6\rtimes_\varphi \mathbb Z_2$ ? $2$ . How to prove $\operatorname{Aut}(S_6)\not\cong S_6\times \mathbb Z_2$ ? $3$ . How to prove $\operatorname{Aut}(A_6)\cong \operatorname{Aut}(S_6)$ ? My effort : $1$ . For 1, it remains to show there exists $\sigma\in \operatorname{Aut}(S_6)\setminus \operatorname{Inn}(S_6)$ s.t. $\sigma^2=\text{id}$ . $2$ . For 2, $Z(S_6\times\mathbb Z_2)=\mathbb Z_2$ , it's sufficient to show $Z(\operatorname{Aut}(S_6))\neq\mathbb Z_2$ . $3$ . For 3, I proved $\operatorname{Aut}(S_n)\leqslant\operatorname{Aut}(A_n)$ (Is this correct?) and $[\operatorname{Aut}(A_6):\operatorname{Inn}(S_6)]\leqslant 2$ . Update: I wrote my answer below, but there still remain three questions: $1$ . I copied the result from a book to give an explicit element $\psi\in\operatorname{Aut}(S_6)\setminus \operatorname{Inn}(S_6)$ of order $2$ , and I wonder if there's a way to avoid doing so, i.e. find an element of order $2$ in $\operatorname{Aut}(S_6)\setminus \operatorname{Inn}(S_6)$ without writing it out explicitly. $2$ . I used the specific element $\psi$ to show $\mathbb Z_2\cong \langle \psi\rangle$ is not normal in $\operatorname{Aut}(S_6)$ , I wonder if we can analysis the center of $\operatorname{Aut}(S_6)$ instead. And what is center of $\operatorname{Aut}(S_6)$ ? $3$ . Is there a better way to prove $\operatorname{Aut}(A_6)\cong \operatorname{Aut}(S_6)$ ? Thanks for your time and effort!","From (1) , (2) , (3) , . My question : . How to prove ? . How to prove ? . How to prove ? My effort : . For 1, it remains to show there exists s.t. . . For 2, , it's sufficient to show . . For 3, I proved (Is this correct?) and . Update: I wrote my answer below, but there still remain three questions: . I copied the result from a book to give an explicit element of order , and I wonder if there's a way to avoid doing so, i.e. find an element of order in without writing it out explicitly. . I used the specific element to show is not normal in , I wonder if we can analysis the center of instead. And what is center of ? . Is there a better way to prove ? Thanks for your time and effort!",[\operatorname{Aut}(S_6):\operatorname{Inn}(S_6)]=2 1 \operatorname{Aut}(S_6)\cong S_6\rtimes_\varphi \mathbb Z_2 2 \operatorname{Aut}(S_6)\not\cong S_6\times \mathbb Z_2 3 \operatorname{Aut}(A_6)\cong \operatorname{Aut}(S_6) 1 \sigma\in \operatorname{Aut}(S_6)\setminus \operatorname{Inn}(S_6) \sigma^2=\text{id} 2 Z(S_6\times\mathbb Z_2)=\mathbb Z_2 Z(\operatorname{Aut}(S_6))\neq\mathbb Z_2 3 \operatorname{Aut}(S_n)\leqslant\operatorname{Aut}(A_n) [\operatorname{Aut}(A_6):\operatorname{Inn}(S_6)]\leqslant 2 1 \psi\in\operatorname{Aut}(S_6)\setminus \operatorname{Inn}(S_6) 2 2 \operatorname{Aut}(S_6)\setminus \operatorname{Inn}(S_6) 2 \psi \mathbb Z_2\cong \langle \psi\rangle \operatorname{Aut}(S_6) \operatorname{Aut}(S_6) \operatorname{Aut}(S_6) 3 \operatorname{Aut}(A_6)\cong \operatorname{Aut}(S_6),"['abstract-algebra', 'group-theory', 'permutations', 'solution-verification']"
75,Question for an abelian extension over $\mathbb{Q}$,Question for an abelian extension over,\mathbb{Q},"Let $\alpha$ and $\beta$ be real algebraic numbers satisfying $\alpha+\beta\notin\mathbb{Q}$ and $\alpha^{3}+\beta^{3}\in\mathbb{Q}$ . Let $K\subseteq\mathbb{C}$ be the splitting field of the minimal polynomial of $\alpha+\beta$ over $\mathbb{Q}$ . Suppose that the Galois group $G:=\textrm{Gal}(K/\mathbb{Q})$ is abelian. Show that: $x := \alpha\omega+\beta\omega^{2}$ $y := \alpha\omega^{2}+\beta\omega$ are elements of $K$ where $\omega\in\mathbb{C}$ with $\omega^{3}=1,\omega\neq1$ . We find that $x + y = -(\alpha + \beta)$ $x \cdot y = \alpha^2 - \alpha \beta + \beta^2 = \frac{\alpha^3 + \beta^3}{\alpha + \beta}$ $\alpha\beta=\frac{(\alpha+\beta)^{3}-(\alpha^{3}+\beta^{3})}{3(\alpha+\beta)}\in K$ . The polynomial $P(X)=X^{3}-3\alpha\beta\cdot X-(\alpha^{3}+\beta^{3})$ has $\alpha+\beta$ as a root. The terms $x$ and $y$ look like the cubic Lagrange Resolvent. but how can we finish?",Let and be real algebraic numbers satisfying and . Let be the splitting field of the minimal polynomial of over . Suppose that the Galois group is abelian. Show that: are elements of where with . We find that . The polynomial has as a root. The terms and look like the cubic Lagrange Resolvent. but how can we finish?,"\alpha \beta \alpha+\beta\notin\mathbb{Q} \alpha^{3}+\beta^{3}\in\mathbb{Q} K\subseteq\mathbb{C} \alpha+\beta \mathbb{Q} G:=\textrm{Gal}(K/\mathbb{Q}) x := \alpha\omega+\beta\omega^{2} y := \alpha\omega^{2}+\beta\omega K \omega\in\mathbb{C} \omega^{3}=1,\omega\neq1 x + y = -(\alpha + \beta) x \cdot y = \alpha^2 - \alpha \beta + \beta^2 = \frac{\alpha^3 + \beta^3}{\alpha + \beta} \alpha\beta=\frac{(\alpha+\beta)^{3}-(\alpha^{3}+\beta^{3})}{3(\alpha+\beta)}\in K P(X)=X^{3}-3\alpha\beta\cdot X-(\alpha^{3}+\beta^{3}) \alpha+\beta x y","['abstract-algebra', 'galois-theory', 'splitting-field', 'galois-extensions']"
76,Fermat-like polynomials are irreducible,Fermat-like polynomials are irreducible,,"I am attempting to prove the following result. Let $K$ be a field, $n \ge 3$ , and $e_1,\ldots, e_n$ be positive integers which are not multiples of the characteristic. Then the polynomial $$ p(x) = \sum_{i=1}^n x_i^{e_i} $$ is irreducible in $K[x_1,\ldots, x_n]$ . I am attempting to expand the outline given in this answer into a proof. My attempt to do so is below. Proof by induction. For the base case, $n = 3$ , we claim that irreducibility of $x^l + y^m + z^n$ in $k[x,y,z]$ is equivalent to irreducibility in $k(z)[x,y]$ . Since the coefficients of our polynomial are all $1$ , this follows from Gauss' Lemma; in particular, both are equivalent to irreducibility in $k(y,z)[x]$ . So we view our polynomial as $x^l + (y^m + z^n)$ in $k(z)[y][x]$ and want to satisfy Eisenstein's criterion. In particular, we can check by the derivative test that $a_0 \equiv (y^m + z^n)$ in $k(z)[y]$ is separable, since it is of the form $y^m + c$ for $c\neq 0$ and the characteristic does not divide $m$ . Since $a_0$ is separable, its prime factors in $k(z)[y]$ are all distinct. Let $\mathfrak{p}$ be the prime ideal generated by one of these prime factors. The conditions of Eisenstein are satisfied, so our polynomial is irreducible in $k(z)[y][x]$ and in $k[x,y,z]$ . Now for the inductive step, also by Eisenstein. We write $$    p(x) = x_n^{e_n} + (x_1^{e_1} + \cdots + x_{n-1}^{e_{n-1}}) \in k[x_1, \ldots, x_{n-1}][x_n]. $$ Since by the inductive hypothesis the constant term is irreducible, it generates a prime ideal $(x_1^{e_1} + \cdots + x_{n-1}^{e_{n-1}})$ in the polynomial ring. We need to check that $$     x_1^{e_1} + \cdots + x_{n-1}^{e_{n-1}} \not\in \langle x_1^{e_1} + \cdots + x_{n-1}^{e_{n-1}}\rangle^2 = \langle(x_1^{e_1} + \cdots + x_{n-1}^{e_{n-1}})^2\rangle. $$ (Hagen von Eitzen) If it were, then setting $x_2,\ldots, x_{n-1}$ all to zero gives $$    x_1^{e_1} = x_1^{2e_1} f(x_1) $$ for some $f \in k[x_1]$ , which is absurd. Hence the inductive step holds by Eisenstein. (Edit: Answered by Hagen von Eitzen) This last fact seems like it should be true, at least in characteristic not 2, but I don't see how to prove it. Or do I need another technique to do the inductive step? This proof does not use the full strength of the assumptions, as far as I can see. I have so far only needed one of the $e_i$ to not be multiples of the characteristic. Am I making a mistake in the base case, or is the statement true that generally?","I am attempting to prove the following result. Let be a field, , and be positive integers which are not multiples of the characteristic. Then the polynomial is irreducible in . I am attempting to expand the outline given in this answer into a proof. My attempt to do so is below. Proof by induction. For the base case, , we claim that irreducibility of in is equivalent to irreducibility in . Since the coefficients of our polynomial are all , this follows from Gauss' Lemma; in particular, both are equivalent to irreducibility in . So we view our polynomial as in and want to satisfy Eisenstein's criterion. In particular, we can check by the derivative test that in is separable, since it is of the form for and the characteristic does not divide . Since is separable, its prime factors in are all distinct. Let be the prime ideal generated by one of these prime factors. The conditions of Eisenstein are satisfied, so our polynomial is irreducible in and in . Now for the inductive step, also by Eisenstein. We write Since by the inductive hypothesis the constant term is irreducible, it generates a prime ideal in the polynomial ring. We need to check that (Hagen von Eitzen) If it were, then setting all to zero gives for some , which is absurd. Hence the inductive step holds by Eisenstein. (Edit: Answered by Hagen von Eitzen) This last fact seems like it should be true, at least in characteristic not 2, but I don't see how to prove it. Or do I need another technique to do the inductive step? This proof does not use the full strength of the assumptions, as far as I can see. I have so far only needed one of the to not be multiples of the characteristic. Am I making a mistake in the base case, or is the statement true that generally?","K n \ge 3 e_1,\ldots, e_n 
p(x) = \sum_{i=1}^n x_i^{e_i}
 K[x_1,\ldots, x_n] n = 3 x^l + y^m + z^n k[x,y,z] k(z)[x,y] 1 k(y,z)[x] x^l + (y^m + z^n) k(z)[y][x] a_0 \equiv (y^m + z^n) k(z)[y] y^m + c c\neq 0 m a_0 k(z)[y] \mathfrak{p} k(z)[y][x] k[x,y,z] 
   p(x) = x_n^{e_n} + (x_1^{e_1} + \cdots + x_{n-1}^{e_{n-1}}) \in k[x_1, \ldots, x_{n-1}][x_n].
 (x_1^{e_1} + \cdots + x_{n-1}^{e_{n-1}}) 
    x_1^{e_1} + \cdots + x_{n-1}^{e_{n-1}} \not\in \langle x_1^{e_1} + \cdots + x_{n-1}^{e_{n-1}}\rangle^2 = \langle(x_1^{e_1} + \cdots + x_{n-1}^{e_{n-1}})^2\rangle.
 x_2,\ldots, x_{n-1} 
   x_1^{e_1} = x_1^{2e_1} f(x_1)
 f \in k[x_1] e_i","['abstract-algebra', 'polynomials', 'field-theory', 'irreducible-polynomials']"
77,"Let $p,q > 2$ be distinct primes. Show that $\mathbb{Z}_{pq}^*$ is not cyclic.",Let  be distinct primes. Show that  is not cyclic.,"p,q > 2 \mathbb{Z}_{pq}^*","I recognize this question has been asked many times before ( here , here , and here ) and in other forms (that $pq$ does not have a primitive root for example). I am also self-studying Aluffi's Algebra and am wondering specifically about the last solution above I have linked. We know that $\mathbb{Z}_{pq}^*$ has order $pq$ less the multiples of $p$ or $q$ between $1$ and $pq$ . These multiples of $p$ are $p,2p,...,(q-1)p$ , of $q$ are $q,2q,...,(p-1)q$ , and of course their least common multiple $pq$ . So the order is $pq - (p-1) - (q-1) - 1 = (p-1)(q-1)$ . We aim to show that there is no element of this order so that the group cannot be cyclic. Put $n = (p-1)(q-1) /2$ . Both factors in the numerator are even so we can do this and still have both factors dividing $n$ . The last solution linked above uses that for $m\in \mathbb{Z}_{pq}^{*}$ we have $m$ in both $\mathbb{Z}_{p}^{*}$ and $\mathbb{Z}_{q}^{*}$ , with $$ m^{n} \equiv m^{p-1} \equiv 1 \mod p  \\ m^{n} \equiv m^{q-1} \equiv 1 \mod q.$$ From which we get $p,q | m^{n} - 1$ so $pq | (m^{n}-1)$ since $p,q$ are coprime. But how do we know that $$ m^{p-1} \equiv 1 \mod p  \\ m^{q-1} \equiv 1 \mod q$$ without something like Lagrange's theorem? Aluffi doesn't develop a lot for us to work with...","I recognize this question has been asked many times before ( here , here , and here ) and in other forms (that does not have a primitive root for example). I am also self-studying Aluffi's Algebra and am wondering specifically about the last solution above I have linked. We know that has order less the multiples of or between and . These multiples of are , of are , and of course their least common multiple . So the order is . We aim to show that there is no element of this order so that the group cannot be cyclic. Put . Both factors in the numerator are even so we can do this and still have both factors dividing . The last solution linked above uses that for we have in both and , with From which we get so since are coprime. But how do we know that without something like Lagrange's theorem? Aluffi doesn't develop a lot for us to work with...","pq \mathbb{Z}_{pq}^* pq p q 1 pq p p,2p,...,(q-1)p q q,2q,...,(p-1)q pq pq - (p-1) - (q-1) - 1 = (p-1)(q-1) n = (p-1)(q-1) /2 n m\in \mathbb{Z}_{pq}^{*} m \mathbb{Z}_{p}^{*} \mathbb{Z}_{q}^{*}  m^{n} \equiv m^{p-1} \equiv 1 \mod p  \\
m^{n} \equiv m^{q-1} \equiv 1 \mod q. p,q | m^{n} - 1 pq | (m^{n}-1) p,q  m^{p-1} \equiv 1 \mod p  \\
m^{q-1} \equiv 1 \mod q",['abstract-algebra']
78,"Let $G$ be a group and $A \subseteq G$. Assume $\{aA\mid a \in G\}$ form a partition of $G$. For some $a_0\in G$, $H=a_0A$ is a subgroup of $G$.","Let  be a group and . Assume  form a partition of . For some ,  is a subgroup of .",G A \subseteq G \{aA\mid a \in G\} G a_0\in G H=a_0A G,"This is an extra problem that my Algebra teacher suggested working through for practice. Let $G$ be a group and let $A \subseteq  G$ . Assume that $K = \{aA\mid a \in  G\} $ form a partition of G. Show that there exists $a_0\in G$ such that H = $a_0$ A is a subgroup of G and $\{aA\mid a  \in  G\} = \{bH\mid b  \in  G\}$ . I know that if $K$ is a partition of $G$ , then either $aA = bA$ or $aA  \cap  bA =  \emptyset $ for all $a,b \in G$ and that there must exist $a_0\in A$ such that for $x \in G$ , $a_0a = x$ which is equivalent to $a_0 = xa^{-1}$ , since $G$ is a group. This format is similar to $H =  a_0A$ . I'm struggling with forming this subgroup $H$ and showing it's a subgroup. As for the second part, I think I could just show that each set is a subset of the other. I am new to Abstract Algebra and struggle with the concepts. I am looking for very straightforward, simple suggestions. Thank you!","This is an extra problem that my Algebra teacher suggested working through for practice. Let be a group and let . Assume that form a partition of G. Show that there exists such that H = A is a subgroup of G and . I know that if is a partition of , then either or for all and that there must exist such that for , which is equivalent to , since is a group. This format is similar to . I'm struggling with forming this subgroup and showing it's a subgroup. As for the second part, I think I could just show that each set is a subset of the other. I am new to Abstract Algebra and struggle with the concepts. I am looking for very straightforward, simple suggestions. Thank you!","G A \subseteq  G K = \{aA\mid a \in  G\}  a_0\in G a_0 \{aA\mid a  \in  G\} = \{bH\mid b  \in  G\} K G aA = bA aA  \cap  bA =  \emptyset  a,b \in G a_0\in A x \in G a_0a = x a_0 = xa^{-1} G H =  a_0A H","['abstract-algebra', 'group-theory']"
79,Frattini subgroup is normal-monotone,Frattini subgroup is normal-monotone,,"In the exercise 6.1.22 of Dummit and Foote's Abstract Algebra (Here $\Phi(G)$ is the Frattini subgroup of $G$ ): If $N\unlhd G$ , then $\Phi(N)\subseteq\Phi(G)$ . When every proper subgroup of $N$ is contained in a maximal subgroup of $N$ , the statement can be proved. (By taking $M$ as a maximal subgroup of $G$ that fails to contain $\Phi(N)$ and deriving $N=\Phi(N)(N\cap M)$ , a contradiction.) But, as there may not exist a maximal subgroup of $N$ containing $N\cap M$ , the case is different when some proper subgroup of $N$ is not contained in a maximal subgroup of $N$ . Hence I'd like to ask that if $N\unlhd G$ and $M$ a maximal subgroup of $G$ , could the case that there does not exist a maximal subgroup of $N$ containing $N\cap M$ happen? Or is there another way to prove the statement? (Otherwise, is there a counterexample that the statement does not hold when some proper subgroup of $N$ is not contained in a maximal subgroup of $N$ ?) In fact , I wonder that if a group satisfies the condition that every proper subgroup is contained in a maximal subgroup, could it be possible that the condition does not apply to its normal subgroup?","In the exercise 6.1.22 of Dummit and Foote's Abstract Algebra (Here is the Frattini subgroup of ): If , then . When every proper subgroup of is contained in a maximal subgroup of , the statement can be proved. (By taking as a maximal subgroup of that fails to contain and deriving , a contradiction.) But, as there may not exist a maximal subgroup of containing , the case is different when some proper subgroup of is not contained in a maximal subgroup of . Hence I'd like to ask that if and a maximal subgroup of , could the case that there does not exist a maximal subgroup of containing happen? Or is there another way to prove the statement? (Otherwise, is there a counterexample that the statement does not hold when some proper subgroup of is not contained in a maximal subgroup of ?) In fact , I wonder that if a group satisfies the condition that every proper subgroup is contained in a maximal subgroup, could it be possible that the condition does not apply to its normal subgroup?",\Phi(G) G N\unlhd G \Phi(N)\subseteq\Phi(G) N N M G \Phi(N) N=\Phi(N)(N\cap M) N N\cap M N N N\unlhd G M G N N\cap M N N,"['abstract-algebra', 'group-theory']"
80,How many equivalence classes of squares are there?,How many equivalence classes of squares are there?,,"Let's say we have a $3×3$ square where $3$ of the cells are labeled $a$ , $b$ , $c$ and the rest are blank. Two such squares are considered ""equivalent"" if one square can be obtained from another square by 1) rotation on 90, 180, and 270 degrees, 2) reflection (through the horizontal, vertical, or either diagonal axis). I need to find equivalence classes of squares (maybe groups or patterns?). My attempt is :  1) put the $a$ , $b$ and $c$ in the 1-st row: $A=\left(% \begin{array}{ccc}   a & b & c \\   .. & .. & .. \\   .. & .. & .. \\ \end{array} \right)$ , then we can rotate the square $A$ on 90, 180 and 270 degrees: $A_{90}=\left(% \begin{array}{ccc}   .. & .. & a \\   .. & .. & b\\   .. & .. & c \\ \end{array} \right)$ , $A_{180}=\left(% \begin{array}{ccc}   .. & .. & .. \\   .. & .. & ..\\   c & b & a \\ \end{array} \right)$ , $A_{270}=\left(% \begin{array}{ccc}   c & .. & .. \\   b & .. & ..\\   a & .. & .. \\ \end{array}. \right)$ . Four square $A$ , $A_{90}$ , $A_{180}$ and $A_{270}$ are equvalent. This is the first equivalence class. 2) put the $a$ , $b$ and $c$ in the main diagonal: $$A=\left(% \begin{array}{ccc}   a & .. & .. \\   .. & b & .. \\   .. & .. & c \\ \end{array}% \right) $$ and rotate on 90 degree $$A_{90}=\left(% \begin{array}{ccc}   .. & .. & a \\   .. & b & .. \\   c & .. & .. \\ \end{array}% \right) $$ Two square $A$ and $A_{90}$ are equvalent. This is the second equivalence class. Edit 2. Here I have found the 16 patterns. Question. How many equivalence classes for three elements in a square are there?","Let's say we have a square where of the cells are labeled , , and the rest are blank. Two such squares are considered ""equivalent"" if one square can be obtained from another square by 1) rotation on 90, 180, and 270 degrees, 2) reflection (through the horizontal, vertical, or either diagonal axis). I need to find equivalence classes of squares (maybe groups or patterns?). My attempt is :  1) put the , and in the 1-st row: , then we can rotate the square on 90, 180 and 270 degrees: , , . Four square , , and are equvalent. This is the first equivalence class. 2) put the , and in the main diagonal: and rotate on 90 degree Two square and are equvalent. This is the second equivalence class. Edit 2. Here I have found the 16 patterns. Question. How many equivalence classes for three elements in a square are there?","3×3 3 a b c a b c A=\left(%
\begin{array}{ccc}
  a & b & c \\
  .. & .. & .. \\
  .. & .. & .. \\
\end{array}
\right) A A_{90}=\left(%
\begin{array}{ccc}
  .. & .. & a \\
  .. & .. & b\\
  .. & .. & c \\
\end{array}
\right) A_{180}=\left(%
\begin{array}{ccc}
  .. & .. & .. \\
  .. & .. & ..\\
  c & b & a \\
\end{array}
\right) A_{270}=\left(%
\begin{array}{ccc}
  c & .. & .. \\
  b & .. & ..\\
  a & .. & .. \\
\end{array}.
\right) A A_{90} A_{180} A_{270} a b c A=\left(%
\begin{array}{ccc}
  a & .. & .. \\
  .. & b & .. \\
  .. & .. & c \\
\end{array}%
\right)
 A_{90}=\left(%
\begin{array}{ccc}
  .. & .. & a \\
  .. & b & .. \\
  c & .. & .. \\
\end{array}%
\right)
 A A_{90}","['abstract-algebra', 'combinatorics', 'group-theory', 'symmetric-groups', 'class-field-theory']"
81,Determine the maximal ideals of $\mathbb R^2$ by determining **all** its ideals.,Determine the maximal ideals of  by determining **all** its ideals.,\mathbb R^2,"This is a letter of an exericse in Artin Algebra and has been asked and answered here as well as by Brian Bi here and Takumi Murayama here . I had a different approach here and have yet another approach. $\mathbb R$ is a field so its only ideals are $(1)$ and $(0)$ . Thus, by Structure of ideals in the product of two rings all the ideals of $\mathbb R^2$ are $$(0)\times(0),(1)\times(1),(0)\times(1),(1)\times(0)$$ . Then it's obvious $$(0)\times(1),(1)\times(0)$$ are the maximal ideals. Is this correct also?","This is a letter of an exericse in Artin Algebra and has been asked and answered here as well as by Brian Bi here and Takumi Murayama here . I had a different approach here and have yet another approach. is a field so its only ideals are and . Thus, by Structure of ideals in the product of two rings all the ideals of are . Then it's obvious are the maximal ideals. Is this correct also?","\mathbb R (1) (0) \mathbb R^2 (0)\times(0),(1)\times(1),(0)\times(1),(1)\times(0) (0)\times(1),(1)\times(0)","['abstract-algebra', 'proof-verification']"
82,$\overline{f}$ is isomorphism in abelian category,is isomorphism in abelian category,\overline{f},"Suppose $f: A \longrightarrow B$ is a morphism in an abelian category $\mathcal{C}$ . What I consider an abelian category: $\mathcal{C}$ is additive. Every morphism has a kernel and a cokernel. Every monomorphism is a kernel and every epimorphism is a cokernel. With that, we can define: $Im(f)= kernel(cokernel(f))$ $Coim(f)=cokernel(kernel(f))$ where $k: K \longrightarrow A$ is a kernel of $f$ if $ k \circ f = 0_{K,B}$ and whenever $h \circ f = 0$ , $h$ factors uniquely through $k$ . (i.e. $h= k \circ h'$ ). And $q:B \longrightarrow C$ is a cokernel of $f$ if $ f \circ q = 0_{A,C}$ and whenever $f \circ h = 0$ , $h$ factors uniquely through $q$ (i.e. $h = h' \circ q$ ). Notation : $0_{A,B}$ is the zero morphism obtained composing $A \longrightarrow 0$ and $0 \longrightarrow B$ . Once I have defined $Im(f)$ and $Coim(f)$ , I want to check that there exists a natural map between them, called $\overline{f}$ which is isomorphism. I have been working with epimorphisms and monomorphisms notions but I am a little bit lost. Any help/hint? Related but do not understand: Equivalent conditions for a preabelian category to be abelian","Suppose is a morphism in an abelian category . What I consider an abelian category: is additive. Every morphism has a kernel and a cokernel. Every monomorphism is a kernel and every epimorphism is a cokernel. With that, we can define: where is a kernel of if and whenever , factors uniquely through . (i.e. ). And is a cokernel of if and whenever , factors uniquely through (i.e. ). Notation : is the zero morphism obtained composing and . Once I have defined and , I want to check that there exists a natural map between them, called which is isomorphism. I have been working with epimorphisms and monomorphisms notions but I am a little bit lost. Any help/hint? Related but do not understand: Equivalent conditions for a preabelian category to be abelian","f: A \longrightarrow B \mathcal{C} \mathcal{C} Im(f)= kernel(cokernel(f)) Coim(f)=cokernel(kernel(f)) k: K \longrightarrow A f  k \circ f = 0_{K,B} h \circ f = 0 h k h= k \circ h' q:B \longrightarrow C f  f \circ q = 0_{A,C} f \circ h = 0 h q h = h' \circ q 0_{A,B} A \longrightarrow 0 0 \longrightarrow B Im(f) Coim(f) \overline{f}","['abstract-algebra', 'commutative-algebra', 'category-theory', 'abelian-categories']"
83,"Suppose $ H\leqslant G $, prove that if $ (H, G')=\langle e \rangle $, then $ (H', G)=\langle e \rangle $.","Suppose , prove that if , then ."," H\leqslant G   (H, G')=\langle e \rangle   (H', G)=\langle e \rangle ","I am working on this Exercise from Algebra by Hungerford (Exercise II.7.3(b)). It states If $ H $ and $ K $ are subgroups of a group $ G $ , let $ (H, K) $ be the subgroup of $ G $ generated by the elements $ \{ hkh^{-1}k^{-1}|h\in H, k\in K \} $ . Show that If $ (H, G')=\langle e \rangle $ , then $ (H', G)=\langle e \rangle $ . $ G' $ is the commutator subgroup of $ G $ . My attempt: $ (H', G)= \langle e \rangle $ is the same thing as $ H' $ is in the center of $ G $ . Then I am stuck... I couldn't find any useful tool to simplify the problem. Can someone give me a hint? Thank you.","I am working on this Exercise from Algebra by Hungerford (Exercise II.7.3(b)). It states If and are subgroups of a group , let be the subgroup of generated by the elements . Show that If , then . is the commutator subgroup of . My attempt: is the same thing as is in the center of . Then I am stuck... I couldn't find any useful tool to simplify the problem. Can someone give me a hint? Thank you."," H   K   G   (H, K)   G   \{ hkh^{-1}k^{-1}|h\in H, k\in K \}   (H, G')=\langle e \rangle   (H', G)=\langle e \rangle   G'   G   (H', G)= \langle e \rangle   H'   G ","['abstract-algebra', 'group-theory']"
84,The semidirect product $(C_7\times C_{13})\rtimes C_3$,The semidirect product,(C_7\times C_{13})\rtimes C_3,"Following this page , in the classification of groups of order $273$ , the product of the Sylow group $S:=C_7C_{13}\simeq C_{7}\times C_{13}$ is normal, hence can be acted on by the Sylow $C_3$ to produce semidirect products. Since $\operatorname{Aut}(S)\simeq C_6\times C_{12}$ , one only needs to find an order 3 element there to produce the homomorphism $C_3\rightarrow \operatorname{Aut}(S)$ . In particular, if $C_6=\langle x\rangle$ and $C_{12}=\langle y\rangle$ , up to choice of generators of $C_3$ , the group is classified by the 5 actions corresponding to: $(1,1), (x^2,1), (1,y^4), (x^2,y^4), (x^{-2},y^4)$ . It is easy to see that the center of the corresponding groups has order $273, 13, 7, 1, 1$ respectively. So my question is: how do we show that the last two groups are actually non-isomorphic? P.S. In a similar example with $(C_7\times C_7)\rtimes C_3$ on the same page , the last two groups can be distinguished since every subgroup of order $7$ can be shown to be normal in one but not the other.","Following this page , in the classification of groups of order , the product of the Sylow group is normal, hence can be acted on by the Sylow to produce semidirect products. Since , one only needs to find an order 3 element there to produce the homomorphism . In particular, if and , up to choice of generators of , the group is classified by the 5 actions corresponding to: . It is easy to see that the center of the corresponding groups has order respectively. So my question is: how do we show that the last two groups are actually non-isomorphic? P.S. In a similar example with on the same page , the last two groups can be distinguished since every subgroup of order can be shown to be normal in one but not the other.","273 S:=C_7C_{13}\simeq C_{7}\times C_{13} C_3 \operatorname{Aut}(S)\simeq C_6\times C_{12} C_3\rightarrow \operatorname{Aut}(S) C_6=\langle x\rangle C_{12}=\langle y\rangle C_3 (1,1), (x^2,1), (1,y^4), (x^2,y^4), (x^{-2},y^4) 273, 13, 7, 1, 1 (C_7\times C_7)\rtimes C_3 7","['abstract-algebra', 'group-theory', 'finite-groups', 'semidirect-product']"
85,Show that a semigroup is semisimple iff $A^2 = A$ for every two-sided ideal $A$,Show that a semigroup is semisimple iff  for every two-sided ideal,A^2 = A A,"Let $S$ be a semigroup, a subset $I\subseteq S$ is called an ideal if $SI \subseteq I$ and $IS \subseteq S$. We denote by $S^1$ the semigroup $S$ adjoined with a identity if it does not contains one, and for $a \in S$ we set $J(a) = S^1aS^1$, the principal ideal generated by $a$. Also we set for $a,b \in S$ $$   a \mathcal J b :\Leftrightarrow S^1 a S^1 = S^1 b S^1. $$ For an ideal we can form the Rees factor semigroup denotes by $S / I$, which essentially means collapsing everything in $I$ to a zero in the factor semigroup. Denote the $\mathcal J$-equivalence class of some $a \in S$ by $J_a$. A principal factor is a factor semigroup of the form $J(a) / (J(a) \setminus J_a)$, see the Ecyclopdia of mathematics . The unique minimal ideal, called the kernel of $S$, is among the principal factors. A null semigroup (or semigroup with zero multiplication) is a semigroup $S$ with a zero $0 \in S$ such that $ab = 0$ for all $a,b \in S$. A semigroup is called semisimple if none of the principal factors is a null semigroup. I want to show that a semigroup $S$ is semisimple if and onyl if $A^2 = A$ for every ideal $A \subseteq S$. This is an exercise from the book Fundamentals of Semigroup Theory by J. Howie, page 95. What I do not understand is that if a semigroup contains a zero $0$, then the minimal ideal must be $\{0\}$, in particular this is a null semigroup. But I can very well find semigroups such that $A^2 = A$ for every ideal that have a zero. For example take any group $G$, adjoin a zero $0$ by setting $g0 = 0g =0$ and observe that the only ideals are $\{0\}$ and $G \cup\{0\}$ itself, and they both fulfill the condition on ideals... So any hints on this exercise, or what I have understood wrong here? EDIT : Maybe there is a typo in the exercise, and just the principal factors not equal to the kernel are meant. But the same exercise appears in the classic Algebraic Theory of Semigroups by Clifford/Preston, and the above link to the Encyclopedia does not excludes the kernel. But if we exclude the kernel, than $A^2 = A$ for every ideal $A$ iff no principal factor not equal the kernel is null. For $A^2 \subsetneq A$ choose $a \in A \setminus A^2$, then $J(a) / N(a)$ is null, for if $x,y \in J(a)$ with $xy \notin N(a)$ would imply $a = u(xy)v$ for some $u,v \in S^1$ and $(ux)(yv) \in A^2$. Conversely, as $S^2 / I = (S/I)^2$ for every semigroup $S$ and ideal $I$ we have that $(J(a)/N(a))^2 = J(a)^2 / N(a) = J(a) / N(a)$ and as $|J(a)/N(a)| > 1$ if $N(a) \ne\emptyset$ those principal factors not equal the kernel are not null. But in case is is not a typo I ask for feedback/clarification...","Let $S$ be a semigroup, a subset $I\subseteq S$ is called an ideal if $SI \subseteq I$ and $IS \subseteq S$. We denote by $S^1$ the semigroup $S$ adjoined with a identity if it does not contains one, and for $a \in S$ we set $J(a) = S^1aS^1$, the principal ideal generated by $a$. Also we set for $a,b \in S$ $$   a \mathcal J b :\Leftrightarrow S^1 a S^1 = S^1 b S^1. $$ For an ideal we can form the Rees factor semigroup denotes by $S / I$, which essentially means collapsing everything in $I$ to a zero in the factor semigroup. Denote the $\mathcal J$-equivalence class of some $a \in S$ by $J_a$. A principal factor is a factor semigroup of the form $J(a) / (J(a) \setminus J_a)$, see the Ecyclopdia of mathematics . The unique minimal ideal, called the kernel of $S$, is among the principal factors. A null semigroup (or semigroup with zero multiplication) is a semigroup $S$ with a zero $0 \in S$ such that $ab = 0$ for all $a,b \in S$. A semigroup is called semisimple if none of the principal factors is a null semigroup. I want to show that a semigroup $S$ is semisimple if and onyl if $A^2 = A$ for every ideal $A \subseteq S$. This is an exercise from the book Fundamentals of Semigroup Theory by J. Howie, page 95. What I do not understand is that if a semigroup contains a zero $0$, then the minimal ideal must be $\{0\}$, in particular this is a null semigroup. But I can very well find semigroups such that $A^2 = A$ for every ideal that have a zero. For example take any group $G$, adjoin a zero $0$ by setting $g0 = 0g =0$ and observe that the only ideals are $\{0\}$ and $G \cup\{0\}$ itself, and they both fulfill the condition on ideals... So any hints on this exercise, or what I have understood wrong here? EDIT : Maybe there is a typo in the exercise, and just the principal factors not equal to the kernel are meant. But the same exercise appears in the classic Algebraic Theory of Semigroups by Clifford/Preston, and the above link to the Encyclopedia does not excludes the kernel. But if we exclude the kernel, than $A^2 = A$ for every ideal $A$ iff no principal factor not equal the kernel is null. For $A^2 \subsetneq A$ choose $a \in A \setminus A^2$, then $J(a) / N(a)$ is null, for if $x,y \in J(a)$ with $xy \notin N(a)$ would imply $a = u(xy)v$ for some $u,v \in S^1$ and $(ux)(yv) \in A^2$. Conversely, as $S^2 / I = (S/I)^2$ for every semigroup $S$ and ideal $I$ we have that $(J(a)/N(a))^2 = J(a)^2 / N(a) = J(a) / N(a)$ and as $|J(a)/N(a)| > 1$ if $N(a) \ne\emptyset$ those principal factors not equal the kernel are not null. But in case is is not a typo I ask for feedback/clarification...",,"['abstract-algebra', 'group-theory', 'semigroups']"
86,Prove that the ideal of $p\in \mathbb Z[x]$ with $p(1)$ even is not principal,Prove that the ideal of  with  even is not principal,p\in \mathbb Z[x] p(1),"Consider the set $I$ of polynomials $p(x)$ in $\mathbb Z[x]$ such that $p(1)$ is even. Prove that this is a non-principal ideal. That this is an ideal is clear. I was wondering whether my proof that $I$ is non-principal correct? Assume $I=(f)$. Since $2\in I$, $2=f(x)g(x)$ for $f,g\in \mathbb Z[x]$, and this implies that $f$ must be constant. This constant can only be equal to $2$ because otherwise $2\notin I$. But on the other hand, $x^2+1\in I$, so $x^2+1=2g(x)$. This is impossible because the LHS is not divisible by $2$.","Consider the set $I$ of polynomials $p(x)$ in $\mathbb Z[x]$ such that $p(1)$ is even. Prove that this is a non-principal ideal. That this is an ideal is clear. I was wondering whether my proof that $I$ is non-principal correct? Assume $I=(f)$. Since $2\in I$, $2=f(x)g(x)$ for $f,g\in \mathbb Z[x]$, and this implies that $f$ must be constant. This constant can only be equal to $2$ because otherwise $2\notin I$. But on the other hand, $x^2+1\in I$, so $x^2+1=2g(x)$. This is impossible because the LHS is not divisible by $2$.",,"['abstract-algebra', 'ring-theory', 'ideals']"
87,Realizing subring as ring of invariants?,Realizing subring as ring of invariants?,,"Let $R$ be a finite type integral domain over field $k$, $S$ be a subring of $R$, such that $[f.f(R)\colon f.f(S)]=d$, does there exist a group $G$ with $|G|=d$ such that $S=R^G$? (Here $f.f(R)$ means the fraction field of $R$) (I am confused by [ACGH] Geometry of algebraic curves Vol II, P.262 Line 14-16. How shall I understand the claim in [ACGH]? If $S=R^G$ and both $R,S$ are smooth, and the extension is not etale, can we show $\mathrm{Spec}(R)\to\mathrm{Spec}(S)$ has to be a cyclic cover over smooth divisor? I think the following is a counterexample.) (Original question: Let $k[x,y]$ be the polynomial ring over $k$ with two variables. Does there exist an action of some finite group $G$ on $k[x,y]$ such that $k[x,y]^G=k[x^2,y^2]$? The naive $x\to -x,y\to -y$ has a larger invariant subring $k[x^2,xy,y^2]$.)","Let $R$ be a finite type integral domain over field $k$, $S$ be a subring of $R$, such that $[f.f(R)\colon f.f(S)]=d$, does there exist a group $G$ with $|G|=d$ such that $S=R^G$? (Here $f.f(R)$ means the fraction field of $R$) (I am confused by [ACGH] Geometry of algebraic curves Vol II, P.262 Line 14-16. How shall I understand the claim in [ACGH]? If $S=R^G$ and both $R,S$ are smooth, and the extension is not etale, can we show $\mathrm{Spec}(R)\to\mathrm{Spec}(S)$ has to be a cyclic cover over smooth divisor? I think the following is a counterexample.) (Original question: Let $k[x,y]$ be the polynomial ring over $k$ with two variables. Does there exist an action of some finite group $G$ on $k[x,y]$ such that $k[x,y]^G=k[x^2,y^2]$? The naive $x\to -x,y\to -y$ has a larger invariant subring $k[x^2,xy,y^2]$.)",,"['abstract-algebra', 'algebraic-geometry']"
88,"Why the Skyscraper Sheaf is Named as Such, Intuition and Interpretation","Why the Skyscraper Sheaf is Named as Such, Intuition and Interpretation",,"I have reviewed all the other questions on SE regarding the skyscraper sheaf and I believe my question is not a duplicate. I am also aware that the questions I pose below may be considered ill defined since there is not 'one right answer' - if my question gets closed, so be it - but I just started learning sheaves and they are complicated. With a complicated object like a sheaf, I like to discuss these sorts of things early on with people who have more maturity on the topic to make sure I am interpreting the object in a good way. Please at least skim my high lighted questions to avoid just answering the title, which does not encompass all of my related questions. I have mostly been working with sheaves of abelian groups, but answers involving sheaves that take values in any concrete category should be understandable to me. Probably best to avoid examples and answers involving abstract sheaves that take values in complicated categories like Cat . First of all, there are two definitions of a sheaf. The more modern/common is the functorial definition given on wikipedia, and then there is the étalé space definition given by Serre on the first page of FAC . I give this definition in the questions I asked here , and here . The definition Serre gives is the one I have been using, but the discussion in the comments on the second link briefly discusses why they are 'equivalent'. Some of the explicit sheaves I have encountered that have names seem to make more sense using one definition than the other. For example, the constant sheaf makes more sense to me using the étalé definition, and the skyscraper sheaf, so far, seems to make more sense using the functorial definition. The skyscraper sheaf is the one I would like to discuss. The standard definition given for the skyscraper sheaf (using a sheaf of sets for simplicity), such as in Vakil's notes, is as follows. For a topological space $X$, and a fixed point $x \in X$, and some set $S$, for each open set $U \subset X$, we assign the set of sections $$F(U) := \begin{cases} S, &\text{ if } x \in U,\\ \{e\}, &\text{ if } x \notin U. \end{cases}$$ For sheaves of algebraic objects replace $\{e\}$ by the appropriate terminal object in the category of choice. One can check that the stalks of this sheaf are $$\mathscr{F}_y := \begin{cases} S, &\text{ if } y \in \overline{\{x\}},\\ \{e\}, &\text{ if } y \notin \overline{ \{x\}}. \end{cases}$$ Now we are prepared for questions. 1. Do some skyscraper sheaves not live up to the name? How I think of a sheaf intuitively, or decide what a sheaf should look like, is pretty much determined by its stalks. This makes sense since I use the étalé definition where the actual object that is called the sheaf is the disjoint union of stalks, as opposed to being the functor that determines such data. Now if $X$ is T1, or more generally $x$ is a closed point of $X$, I see that if $S \neq \{b\}$ we get one non-zero stalk $\mathscr{F}_x$ and all other stalks are $\{e\}$. However, if $S = \{b\}$, or if we are in the algebro-geometric case where the space $X$ is usually not T1, $x$ could be a generic point so that its closure was the whole space. Then no matter what $S$ was, every stalk would be $S$. These are two different ways that a, by definition, skyscraper sheaf could turn out to be a constant sheaf. Is there a way I am missing to still interpret these sheaves as skyscrapers, or are they just perverse examples that don't live up to the name? In my next question, for simplicity, I will speak as though $X$ is T1 so that the skyscraper sheaf has exactly one non zero stalk (so $S \neq \{b\}$ also). 2. Is a skyscraper sheaf actually 'tall', or is it just tall compared to everything else around it? When I first heard the name 'skyscraper sheaf' I automatically thought I was about to encounter a massive object like the monster group . However, since I 'picture' a sheaf as a collection of stalks, it doesn't seem that it's one non-zero stalk at $x$ needs to be necessarily 'tall' or 'big'. Since the stalk is a direct limit, and direct limits tend to make bigger things out of smaller things, I first thought maybe I would be getting lots of copies of $S$ for the stalk at $x$, perhaps one for each open neighborhood, which will (maybe) make that stalk very tall. But I quickly thought through this and realized the direct system determining that direct limit is just the identity maps and the same object over and over, yielding the same object for the direct limit, so this can't be why the one non-zero stalk would be a 'skyscraper'. I tend to think of the 'size' or 'height' of a stalk as its size as a set, or group, etc. So if I chose $S = \{a,b\}$ for my one non-zero stalk, by definition I have a sky scraper sheaf, but really to me this is more like a mole hill sheaf because the stalk corresponding to $S$ would not be very tall, even compared to all the 0 stalks around it. But maybe if I quit thinking about a sheafs physical interpretations as only being determined by stalks, I could also consider stalks and sections. Then, even if I choose $S = \{a,b\}$, I have a copy of $S$ at $x$, then as I transition from local to global, taking nested open subsets containing $x$, I get a copy of $S$ for each one. Maybe this is how the sheaf is 'tall' around one point? Then if I do this process anywhere else, my sequence of sections would be 0 until the open subsets were big enough that they also included $x$ (yes, I am not abstractly thinking of the topology, and am thinking of it as being 'like' a euclidean topology or a Hausdorff space, but oh well). 3. Is it possible to define a skyscraper sheaf and end up with something more like an “NYC Sheaf""? As I mentioned above, if $x$ is a non closed point, we could get more non-zero stalks on the closure of $x$. Above I mentioned an extreme case from the Zariski topology where a point may have the entire space as its closure. Are there any naturally arising cases or examples where the closure of a single point is not the whole space? In this case we should have non-zero stalks on some closed neighborhood around $x,$ and then 0 everywhere else. I imagine if I stood back and looked at this sheaf it would look kind of like this arial photograph of NYC where the angle and distance of the photograph make it look like a ton of action packed into one (seemingly) small area, then flat everywhere around it for as far as you can see. This sheaf should be called the NYC Sheaf! Lastly, it is a perfectly acceptable answer that I will gladly upvote (if it is true) to say ""It seems your understanding of a sheaf, and a skyscraper sheaf, is adequate and you are horribly overthinking the name. Get back to proving theorems.","I have reviewed all the other questions on SE regarding the skyscraper sheaf and I believe my question is not a duplicate. I am also aware that the questions I pose below may be considered ill defined since there is not 'one right answer' - if my question gets closed, so be it - but I just started learning sheaves and they are complicated. With a complicated object like a sheaf, I like to discuss these sorts of things early on with people who have more maturity on the topic to make sure I am interpreting the object in a good way. Please at least skim my high lighted questions to avoid just answering the title, which does not encompass all of my related questions. I have mostly been working with sheaves of abelian groups, but answers involving sheaves that take values in any concrete category should be understandable to me. Probably best to avoid examples and answers involving abstract sheaves that take values in complicated categories like Cat . First of all, there are two definitions of a sheaf. The more modern/common is the functorial definition given on wikipedia, and then there is the étalé space definition given by Serre on the first page of FAC . I give this definition in the questions I asked here , and here . The definition Serre gives is the one I have been using, but the discussion in the comments on the second link briefly discusses why they are 'equivalent'. Some of the explicit sheaves I have encountered that have names seem to make more sense using one definition than the other. For example, the constant sheaf makes more sense to me using the étalé definition, and the skyscraper sheaf, so far, seems to make more sense using the functorial definition. The skyscraper sheaf is the one I would like to discuss. The standard definition given for the skyscraper sheaf (using a sheaf of sets for simplicity), such as in Vakil's notes, is as follows. For a topological space $X$, and a fixed point $x \in X$, and some set $S$, for each open set $U \subset X$, we assign the set of sections $$F(U) := \begin{cases} S, &\text{ if } x \in U,\\ \{e\}, &\text{ if } x \notin U. \end{cases}$$ For sheaves of algebraic objects replace $\{e\}$ by the appropriate terminal object in the category of choice. One can check that the stalks of this sheaf are $$\mathscr{F}_y := \begin{cases} S, &\text{ if } y \in \overline{\{x\}},\\ \{e\}, &\text{ if } y \notin \overline{ \{x\}}. \end{cases}$$ Now we are prepared for questions. 1. Do some skyscraper sheaves not live up to the name? How I think of a sheaf intuitively, or decide what a sheaf should look like, is pretty much determined by its stalks. This makes sense since I use the étalé definition where the actual object that is called the sheaf is the disjoint union of stalks, as opposed to being the functor that determines such data. Now if $X$ is T1, or more generally $x$ is a closed point of $X$, I see that if $S \neq \{b\}$ we get one non-zero stalk $\mathscr{F}_x$ and all other stalks are $\{e\}$. However, if $S = \{b\}$, or if we are in the algebro-geometric case where the space $X$ is usually not T1, $x$ could be a generic point so that its closure was the whole space. Then no matter what $S$ was, every stalk would be $S$. These are two different ways that a, by definition, skyscraper sheaf could turn out to be a constant sheaf. Is there a way I am missing to still interpret these sheaves as skyscrapers, or are they just perverse examples that don't live up to the name? In my next question, for simplicity, I will speak as though $X$ is T1 so that the skyscraper sheaf has exactly one non zero stalk (so $S \neq \{b\}$ also). 2. Is a skyscraper sheaf actually 'tall', or is it just tall compared to everything else around it? When I first heard the name 'skyscraper sheaf' I automatically thought I was about to encounter a massive object like the monster group . However, since I 'picture' a sheaf as a collection of stalks, it doesn't seem that it's one non-zero stalk at $x$ needs to be necessarily 'tall' or 'big'. Since the stalk is a direct limit, and direct limits tend to make bigger things out of smaller things, I first thought maybe I would be getting lots of copies of $S$ for the stalk at $x$, perhaps one for each open neighborhood, which will (maybe) make that stalk very tall. But I quickly thought through this and realized the direct system determining that direct limit is just the identity maps and the same object over and over, yielding the same object for the direct limit, so this can't be why the one non-zero stalk would be a 'skyscraper'. I tend to think of the 'size' or 'height' of a stalk as its size as a set, or group, etc. So if I chose $S = \{a,b\}$ for my one non-zero stalk, by definition I have a sky scraper sheaf, but really to me this is more like a mole hill sheaf because the stalk corresponding to $S$ would not be very tall, even compared to all the 0 stalks around it. But maybe if I quit thinking about a sheafs physical interpretations as only being determined by stalks, I could also consider stalks and sections. Then, even if I choose $S = \{a,b\}$, I have a copy of $S$ at $x$, then as I transition from local to global, taking nested open subsets containing $x$, I get a copy of $S$ for each one. Maybe this is how the sheaf is 'tall' around one point? Then if I do this process anywhere else, my sequence of sections would be 0 until the open subsets were big enough that they also included $x$ (yes, I am not abstractly thinking of the topology, and am thinking of it as being 'like' a euclidean topology or a Hausdorff space, but oh well). 3. Is it possible to define a skyscraper sheaf and end up with something more like an “NYC Sheaf""? As I mentioned above, if $x$ is a non closed point, we could get more non-zero stalks on the closure of $x$. Above I mentioned an extreme case from the Zariski topology where a point may have the entire space as its closure. Are there any naturally arising cases or examples where the closure of a single point is not the whole space? In this case we should have non-zero stalks on some closed neighborhood around $x,$ and then 0 everywhere else. I imagine if I stood back and looked at this sheaf it would look kind of like this arial photograph of NYC where the angle and distance of the photograph make it look like a ton of action packed into one (seemingly) small area, then flat everywhere around it for as far as you can see. This sheaf should be called the NYC Sheaf! Lastly, it is a perfectly acceptable answer that I will gladly upvote (if it is true) to say ""It seems your understanding of a sheaf, and a skyscraper sheaf, is adequate and you are horribly overthinking the name. Get back to proving theorems.",,"['abstract-algebra', 'general-topology', 'algebraic-geometry', 'sheaf-theory']"
89,A Lie Group and Lie Algebra Problem,A Lie Group and Lie Algebra Problem,,"I am a physicist doing some research and I come across with the following Lie algebra problem. Consider the Lie Group $G$ (compact and connected, if you wish), and two generators in the corresponding Lie algebra $X$ and $Y$. By successive action of exponential map you can get the following element in the Lie group $$e^{\alpha_1 X}e^{\beta_1 Y}...e^{\alpha_n X}e^{\beta_n Y} \in G.  $$ The question is: when will the whole Lie group be generated by the action above? And if some part of the Lie group cannot be generated, what is the subgroup that can be generated? Extensions: What is the closure of the generated subgroup? Can you extend the above results to 3 or more generators? Example:  (1) Consider $SU(2)$ and $X=i\sigma_x, Y=i\sigma_y$ ($\sigma$ are Pauli matrices), then they can generate the whole $SU(2)$. (2) Consider $S^1\times S^1$ as a Lie group and $X=Y=i(a,b)$ (in the naturally chosen coordinate system). Of course only a one dimensional subgroup can be generated. However, if $\frac{a}{b}$ is irrational, the closure is the whole group. Thank you for your attention!","I am a physicist doing some research and I come across with the following Lie algebra problem. Consider the Lie Group $G$ (compact and connected, if you wish), and two generators in the corresponding Lie algebra $X$ and $Y$. By successive action of exponential map you can get the following element in the Lie group $$e^{\alpha_1 X}e^{\beta_1 Y}...e^{\alpha_n X}e^{\beta_n Y} \in G.  $$ The question is: when will the whole Lie group be generated by the action above? And if some part of the Lie group cannot be generated, what is the subgroup that can be generated? Extensions: What is the closure of the generated subgroup? Can you extend the above results to 3 or more generators? Example:  (1) Consider $SU(2)$ and $X=i\sigma_x, Y=i\sigma_y$ ($\sigma$ are Pauli matrices), then they can generate the whole $SU(2)$. (2) Consider $S^1\times S^1$ as a Lie group and $X=Y=i(a,b)$ (in the naturally chosen coordinate system). Of course only a one dimensional subgroup can be generated. However, if $\frac{a}{b}$ is irrational, the closure is the whole group. Thank you for your attention!",,"['abstract-algebra', 'group-theory', 'lie-groups', 'lie-algebras', 'topological-groups']"
90,Structure theorem for modules implies Smith Normal Form,Structure theorem for modules implies Smith Normal Form,,"I am working on a homework problem for an undergrad abstract algebra course and I have been stuck on something for a while. I am not looking for a full proof, but any help would be much appreciated. Apologies if this is something simple! The problem: Show that for any matrix $B \in M_{n,m}(R)$, the orbit $GL_n(R) \cdot B \cdot GL_m(R)$ contains a matrix $A$ in Smith normal form, and moreover the integer $k$ and the ideals $(A_{11}),\ldots,(A_{kk})$ are uniquely determined. Edit: $R$ is a PID, and $k$ is the highest integer $i$ such that $A_{ii} \neq 0$. I have the structure theorem at my disposal, and I have shown that $GL_n(R) \cdot B \cdot GL_m(R)$ corresponds to the set of matrices for the linear transformation $f: R^m \to R^n$ given by $f(m) = Bm$ for all possible bases of $R^m$ and $R^n$. It is suggested that I start with the $\ker(B) = \{0\}$ case and modify from there; I have figured out how to prove the full theorem assuming this case. Does anyone have any suggestions for going about this case? Again, I am not necessarily looking for a full proof, just something to help me get unstuck. Thank you for your time!","I am working on a homework problem for an undergrad abstract algebra course and I have been stuck on something for a while. I am not looking for a full proof, but any help would be much appreciated. Apologies if this is something simple! The problem: Show that for any matrix $B \in M_{n,m}(R)$, the orbit $GL_n(R) \cdot B \cdot GL_m(R)$ contains a matrix $A$ in Smith normal form, and moreover the integer $k$ and the ideals $(A_{11}),\ldots,(A_{kk})$ are uniquely determined. Edit: $R$ is a PID, and $k$ is the highest integer $i$ such that $A_{ii} \neq 0$. I have the structure theorem at my disposal, and I have shown that $GL_n(R) \cdot B \cdot GL_m(R)$ corresponds to the set of matrices for the linear transformation $f: R^m \to R^n$ given by $f(m) = Bm$ for all possible bases of $R^m$ and $R^n$. It is suggested that I start with the $\ker(B) = \{0\}$ case and modify from there; I have figured out how to prove the full theorem assuming this case. Does anyone have any suggestions for going about this case? Again, I am not necessarily looking for a full proof, just something to help me get unstuck. Thank you for your time!",,"['abstract-algebra', 'ring-theory', 'modules', 'principal-ideal-domains']"
91,If $[K(\alpha):K]=p\neq q=[K(\beta):K]$ then $[K(\alpha+\beta):K]=pq$,If  then,[K(\alpha):K]=p\neq q=[K(\beta):K] [K(\alpha+\beta):K]=pq,"I am having some trouble with the following problem: Let $K\subseteq L$ be fields. Suppose $\alpha,\beta\in L$ are algebraic elements over $K$ of degrees $p,q$ respectively, where $p$ and $q$ are distinct primes. Show that $\alpha+\beta$ is algebraic over $K$ with degree $pq$. I have shown that $\alpha+\beta$ is algebraic over $K$, but I am having trouble showing that the degree is $pq$. I have previously shown that $[K(\alpha,\beta):K]=pq$, so I have been trying to use this result. I thought that if I could show that $K(\alpha,\beta)=K(\alpha+\beta)$ then I'd be done. To show this I first noted that clearly $K(\alpha+\beta)\subseteq K(\alpha,\beta)$. Then we have $$pq=[K(\alpha,\beta):K]=[K(\alpha,\beta):K(\alpha+\beta)][K(\alpha+\beta):K]$$ so that $[K(\alpha+\beta):K]=1,p,q,pq$. I tried looking at the cases when it is equal to $1,p,q$ and deriving a contradiction, but have been unsuccessful. Another way I thought of solving this is to show that $\alpha,\beta\in K(\alpha+\beta)$, which would allow me to conclude that $K(\alpha+\beta)=K(\alpha,\beta)$, but I am not sure how to complete this either. I am looking for some assistance to show that $[K(\alpha+\beta):K]\neq 1,p,q$, or that $\alpha,\beta\in K(\alpha,\beta)$. If you have any other solutions to this problem I would like to see those too.","I am having some trouble with the following problem: Let $K\subseteq L$ be fields. Suppose $\alpha,\beta\in L$ are algebraic elements over $K$ of degrees $p,q$ respectively, where $p$ and $q$ are distinct primes. Show that $\alpha+\beta$ is algebraic over $K$ with degree $pq$. I have shown that $\alpha+\beta$ is algebraic over $K$, but I am having trouble showing that the degree is $pq$. I have previously shown that $[K(\alpha,\beta):K]=pq$, so I have been trying to use this result. I thought that if I could show that $K(\alpha,\beta)=K(\alpha+\beta)$ then I'd be done. To show this I first noted that clearly $K(\alpha+\beta)\subseteq K(\alpha,\beta)$. Then we have $$pq=[K(\alpha,\beta):K]=[K(\alpha,\beta):K(\alpha+\beta)][K(\alpha+\beta):K]$$ so that $[K(\alpha+\beta):K]=1,p,q,pq$. I tried looking at the cases when it is equal to $1,p,q$ and deriving a contradiction, but have been unsuccessful. Another way I thought of solving this is to show that $\alpha,\beta\in K(\alpha+\beta)$, which would allow me to conclude that $K(\alpha+\beta)=K(\alpha,\beta)$, but I am not sure how to complete this either. I am looking for some assistance to show that $[K(\alpha+\beta):K]\neq 1,p,q$, or that $\alpha,\beta\in K(\alpha,\beta)$. If you have any other solutions to this problem I would like to see those too.",,"['abstract-algebra', 'field-theory', 'extension-field']"
92,"finding a bijective map from $ F(m,n)\times F(k,n)$ to $F(m+k,n)$",finding a bijective map from  to," F(m,n)\times F(k,n) F(m+k,n)","Let $E$ be a complex Hilbert space and $\mathcal{L}(E)$ be the algebra of all bounded linear operators on $E$. For $A=(A_1,...,A_n) \in \mathcal{L}(E)^n$ and $m,k\in \mathbb{N}^*$. I want to show that     $$\sum_{h\in F(m,n)} A_h^*\left(\sum_{g\in F(k,n)} A_g^* A_{g}\right) A_{h}=\sum_{f\in F(m+k,n)} A_f^* A_{f}\; ??$$   where $F(m,n)$  the set of all functions $f$ from $\{1,2, ... , m\} $ to the set  $\{1.2...,n\}$. For $g\in F(m,n)$, we set $A_g:=A_{g(1)}\cdots A_{g(m)}$. I try as follows: $$ \sum_{h\in F(m,n)} A_h^*\left(\sum_{g\in F(k,n)} A_g^* A_{g}\right) A_{h}=\sum_{h \in F(m,n), g \in F(k,n)} (A_{g\ast h})^* A_{g \ast h}, $$ where $g \ast h$ is a function from $ F(m,n)\times F(k,n)$ to $F(m+k,n)$. I'm facing difficulties to give the expression of $g \ast h$ in order to show that it is a bijection.","Let $E$ be a complex Hilbert space and $\mathcal{L}(E)$ be the algebra of all bounded linear operators on $E$. For $A=(A_1,...,A_n) \in \mathcal{L}(E)^n$ and $m,k\in \mathbb{N}^*$. I want to show that     $$\sum_{h\in F(m,n)} A_h^*\left(\sum_{g\in F(k,n)} A_g^* A_{g}\right) A_{h}=\sum_{f\in F(m+k,n)} A_f^* A_{f}\; ??$$   where $F(m,n)$  the set of all functions $f$ from $\{1,2, ... , m\} $ to the set  $\{1.2...,n\}$. For $g\in F(m,n)$, we set $A_g:=A_{g(1)}\cdots A_{g(m)}$. I try as follows: $$ \sum_{h\in F(m,n)} A_h^*\left(\sum_{g\in F(k,n)} A_g^* A_{g}\right) A_{h}=\sum_{h \in F(m,n), g \in F(k,n)} (A_{g\ast h})^* A_{g \ast h}, $$ where $g \ast h$ is a function from $ F(m,n)\times F(k,n)$ to $F(m+k,n)$. I'm facing difficulties to give the expression of $g \ast h$ in order to show that it is a bijection.",,"['abstract-algebra', 'functional-analysis', 'functions', 'hilbert-spaces', 'inverse']"
93,Prove that there is no subgroup of index $6$ in a simple group of order $240$,Prove that there is no subgroup of index  in a simple group of order,6 240,"Let $G$ be a group of order $240=2^4\cdot 3\cdot 5$ . Assume $G$ is simple, then show that there is no subgroup of index 2, 3, 4 or 5 show that there is no subgroup of index 6 For the first item, the usual method seems to work: Let $H\leq G$ where $[G:H] = 5$ then let $G$ act on the set of right cosets of $H$ by right multiplication. Assume $\phi: G\to S_5$ to be its permutation representation. Since $G$ is simple this $\phi$ must be one-to-one which implies $G\cong \mathrm{im}(\phi) \leq G$ . But this is impossible since $|S_5| = 5!=120$ while $|G|=240$ . For the second item. Let once again $H\leq G$ such that $[G:H] = 6$ . Repeating the same argument does not work here, since $240 = |G| \leq |S_6| = 720$ . Any ideas on how to proceed? I prefer hints to complete solutions. Perhaps I could use the theorem $[G:H] = [G:K]\cdot [K:H]$ if I would find a $K$ such that $H\leq K\leq G$ . (I doubt it though)","Let be a group of order . Assume is simple, then show that there is no subgroup of index 2, 3, 4 or 5 show that there is no subgroup of index 6 For the first item, the usual method seems to work: Let where then let act on the set of right cosets of by right multiplication. Assume to be its permutation representation. Since is simple this must be one-to-one which implies . But this is impossible since while . For the second item. Let once again such that . Repeating the same argument does not work here, since . Any ideas on how to proceed? I prefer hints to complete solutions. Perhaps I could use the theorem if I would find a such that . (I doubt it though)",G 240=2^4\cdot 3\cdot 5 G H\leq G [G:H] = 5 G H \phi: G\to S_5 G \phi G\cong \mathrm{im}(\phi) \leq G |S_5| = 5!=120 |G|=240 H\leq G [G:H] = 6 240 = |G| \leq |S_6| = 720 [G:H] = [G:K]\cdot [K:H] K H\leq K\leq G,"['abstract-algebra', 'simple-groups']"
94,Prime/Maximal Ideals in $\mathbb{Z}[\sqrt d]$,Prime/Maximal Ideals in,\mathbb{Z}[\sqrt d],"Let $d \in \mathbb{Z}$ be a square free integer.  $R=\mathbb{Z}[\sqrt d]$ =$\{a+b\sqrt d | a,b \in \mathbb{Z} \}$. Overall, I'm trying to show that every prime ideal $P \subset R$ is a maximal ideal. So far I showed that $I \subset R$ is finitely generated.  $I=\{(x,s+y \sqrt d)\}$ And now I'm trying to show that R/P, for some prime ideal is a finite ring with no zero divisors.  From there it would follow that R/P is a field and any prime ideal is maximal. I know R could also be written as $R= \mathbb{Z}[x]/(x^2-d)$.  How could I show that R/P is a quotient of $\mathbb{Z}/n\mathbb{Z}[x]/(x^2-d)$?","Let $d \in \mathbb{Z}$ be a square free integer.  $R=\mathbb{Z}[\sqrt d]$ =$\{a+b\sqrt d | a,b \in \mathbb{Z} \}$. Overall, I'm trying to show that every prime ideal $P \subset R$ is a maximal ideal. So far I showed that $I \subset R$ is finitely generated.  $I=\{(x,s+y \sqrt d)\}$ And now I'm trying to show that R/P, for some prime ideal is a finite ring with no zero divisors.  From there it would follow that R/P is a field and any prime ideal is maximal. I know R could also be written as $R= \mathbb{Z}[x]/(x^2-d)$.  How could I show that R/P is a quotient of $\mathbb{Z}/n\mathbb{Z}[x]/(x^2-d)$?",,"['abstract-algebra', 'ring-theory', 'ideals', 'maximal-and-prime-ideals']"
95,Is my professor's explanation of direct sum accurate?,Is my professor's explanation of direct sum accurate?,,"This is coming from a graduate-level abstract algebra class, for reference. My professor says that given two groups $G,H$ we say $D$ is the direct sum of $G$ and $H$ and write $C = G \oplus H$ if $G$ and $H$ are disjoint except for zero and $C = G+H = \{g+h | g \in G, h \in H\}$. My issue is that this isn't really a definition, since for arbitrary groups $G$ and $H$ that aren't necessarily disjoint , this isn't defined. For example, I have an assignment question to show that the direct sum of two modules with a certain property still has that property, but I don't see how the direct sum is defined for arbitrary modules. For example, what would $\mathbb{Z}_3 \oplus \mathbb{Z_2}$ be?","This is coming from a graduate-level abstract algebra class, for reference. My professor says that given two groups $G,H$ we say $D$ is the direct sum of $G$ and $H$ and write $C = G \oplus H$ if $G$ and $H$ are disjoint except for zero and $C = G+H = \{g+h | g \in G, h \in H\}$. My issue is that this isn't really a definition, since for arbitrary groups $G$ and $H$ that aren't necessarily disjoint , this isn't defined. For example, I have an assignment question to show that the direct sum of two modules with a certain property still has that property, but I don't see how the direct sum is defined for arbitrary modules. For example, what would $\mathbb{Z}_3 \oplus \mathbb{Z_2}$ be?",,"['abstract-algebra', 'direct-sum']"
96,Zariski Topology on Spec A,Zariski Topology on Spec A,,"I am solving an exercise of Reid's Undergraduate Commutative Algebra, which asks the following. Let $A$ be Noetherian, and $X\subseteq \text{Spec A}$ be Zariski closed. Then $X$ is irreducible (i.e. not the union of two distinct closed subsets) if and only if $I(X)=\cap_{P\in X} P$ is prime. My problem is that I wrote a proof in which I don't see flaws, but I did not use the fact that $A$ was Noetherian. Could it be that Noetherianity is not needed? Here's a sketch. For the if part, by contraposition: if $X$ is reducible, there are closed $X_1,X_2$ such that $X=X_1\cup X_2$, both being non-nested and distinct. This implies that $I(X_1)$ and $I(X_2)$ are also distinct and non-nested, so there are $f\in I(X_1)\setminus I(X_2)$ and $g\in I(X_2)\setminus I(X_1)$, while obviously $fg\in I(X)$, which therefore is not prime. For the only if part, again by contraposition. Say $I(X)=I$ is not prime, so there is $fg\in I$ with $f,g\notin I$.  The proof then relies on the claim\begin{equation*} X=V(I,f)\cup V(I,g)\end{equation*} Both directions are obvious: if $P\in X$, then $P\supseteq I$ so $fg\in P$ and thus either $f$ or $g$ are in $P$ so that $P\in V(I,f)$ or $P\in V(I,g)$. On the other hand if $P$ is in the RHS, say $P\in V(I,f)$, then obviously $P\supseteq I$, so that $P\in LHS$. Can anybody see flaws?","I am solving an exercise of Reid's Undergraduate Commutative Algebra, which asks the following. Let $A$ be Noetherian, and $X\subseteq \text{Spec A}$ be Zariski closed. Then $X$ is irreducible (i.e. not the union of two distinct closed subsets) if and only if $I(X)=\cap_{P\in X} P$ is prime. My problem is that I wrote a proof in which I don't see flaws, but I did not use the fact that $A$ was Noetherian. Could it be that Noetherianity is not needed? Here's a sketch. For the if part, by contraposition: if $X$ is reducible, there are closed $X_1,X_2$ such that $X=X_1\cup X_2$, both being non-nested and distinct. This implies that $I(X_1)$ and $I(X_2)$ are also distinct and non-nested, so there are $f\in I(X_1)\setminus I(X_2)$ and $g\in I(X_2)\setminus I(X_1)$, while obviously $fg\in I(X)$, which therefore is not prime. For the only if part, again by contraposition. Say $I(X)=I$ is not prime, so there is $fg\in I$ with $f,g\notin I$.  The proof then relies on the claim\begin{equation*} X=V(I,f)\cup V(I,g)\end{equation*} Both directions are obvious: if $P\in X$, then $P\supseteq I$ so $fg\in P$ and thus either $f$ or $g$ are in $P$ so that $P\in V(I,f)$ or $P\in V(I,g)$. On the other hand if $P$ is in the RHS, say $P\in V(I,f)$, then obviously $P\supseteq I$, so that $P\in LHS$. Can anybody see flaws?",,"['abstract-algebra', 'zariski-topology']"
97,Direct sums in projective modules,Direct sums in projective modules,,"Let $P$ be a projective module and $P=P_1+N$, where $P_1$ is a direct summand of $P$ and $N$ is a submodule. Show that there is $P_2\subseteq N$ such that $P=P_1\oplus P_2$. I know that there is a submodule $P'$ of $P$ such that $P=P_1\oplus P'$. I wanted to consider the projection from this to $P_1$ and use the definition of being projective. But I would also need a map from $P=P_1+N$ to $P_1$ and I don't know how to get a well defined map there because it is not a direct sum.","Let $P$ be a projective module and $P=P_1+N$, where $P_1$ is a direct summand of $P$ and $N$ is a submodule. Show that there is $P_2\subseteq N$ such that $P=P_1\oplus P_2$. I know that there is a submodule $P'$ of $P$ such that $P=P_1\oplus P'$. I wanted to consider the projection from this to $P_1$ and use the definition of being projective. But I would also need a map from $P=P_1+N$ to $P_1$ and I don't know how to get a well defined map there because it is not a direct sum.",,"['abstract-algebra', 'ring-theory', 'projective-module']"
98,Maximal ideals of $\mathbb{Z}[x]$ containing $30$ and $x^2 + 1$.,Maximal ideals of  containing  and .,\mathbb{Z}[x] 30 x^2 + 1,"I want to find the maximal ideals of the ring $\mathbb{Z}[x]$ containing $30$ and $x^2+1$ . Any such ideal will contain the ideal $(30, x^2+1)$ , so we are searching for maximal ideals in the ring $$\mathbb{Z}[x] / (30,x^2+1) \cong \mathbb{Z}_{30}[x] /(x^2+1) \cong \mathbb{Z}_5[x] /(x^2+1) \oplus\mathbb{Z}_3[x] /(x^2+1) \oplus \mathbb{Z}_2[x] /(x^2+1)$$ Now, we search for the ideals in the summands. Factorizing, $x^2 + 1 = (x+3)(x+2) \bmod 5$ , $x^2 + 1$ is irreducible $\bmod 3$ , and $x^2 + 1 = (x+1)^2 \bmod 2$ . From this, we see that $\mathbb{Z}_3[x] /(x^2+1)$ is a field - there are no nonzero ideals. Moreover, we get ideals corresponding to the factored components in the other summands. How do I find the maximal ideals in $\mathbb{Z}_{30}[x] /(x^2+1)$ from here? Furthermore, how do I find the generators of the corresponding ideals in $\mathbb{Z}_{30}$ ?","I want to find the maximal ideals of the ring containing and . Any such ideal will contain the ideal , so we are searching for maximal ideals in the ring Now, we search for the ideals in the summands. Factorizing, , is irreducible , and . From this, we see that is a field - there are no nonzero ideals. Moreover, we get ideals corresponding to the factored components in the other summands. How do I find the maximal ideals in from here? Furthermore, how do I find the generators of the corresponding ideals in ?","\mathbb{Z}[x] 30 x^2+1 (30, x^2+1) \mathbb{Z}[x] / (30,x^2+1) \cong \mathbb{Z}_{30}[x] /(x^2+1) \cong \mathbb{Z}_5[x] /(x^2+1) \oplus\mathbb{Z}_3[x] /(x^2+1) \oplus \mathbb{Z}_2[x] /(x^2+1) x^2 + 1 = (x+3)(x+2) \bmod 5 x^2 + 1 \bmod 3 x^2 + 1 = (x+1)^2 \bmod 2 \mathbb{Z}_3[x] /(x^2+1) \mathbb{Z}_{30}[x] /(x^2+1) \mathbb{Z}_{30}",['abstract-algebra']
99,Why is the group cohomology for a profinite group always torsion?,Why is the group cohomology for a profinite group always torsion?,,"Let $G$ be a profinite group, $A$ be a discrete $G$-module, and $n>0$ be an integer. Why is the cohomology group $H^n(G;A)$ a torsion abelian group? Here $H^n$ denotes the continuous cohomology groups. This thread is related, but I didn't find the answer to my question. — I know that any (continuous) cocycle $f : G^n \to A$ has finite image, for $G$ is compact and $A$ is discrete. If the subgroup generated by the image of $f$ inside $A$ is also finite (say of cardinality $k$), then $k \cdot f = 0 : G^n \to A$ so that the class of $f$ in $H^n(G;A)$ has order at most $k$.  If $f$ is also a group morphism, which holds if $n=1$ and $A$ is a trivial $G$-module, then the image of $f$ inside $A$ is already a subgroup, so the aforementioned condition is satisfied. – But in general, we only want to find a multiple of $f$ which is a coboundary (without this multiple to be the zero map itself, as it was the case above). I'm not sure how to proceed. Is there may a smarter way to do it? Thank you!","Let $G$ be a profinite group, $A$ be a discrete $G$-module, and $n>0$ be an integer. Why is the cohomology group $H^n(G;A)$ a torsion abelian group? Here $H^n$ denotes the continuous cohomology groups. This thread is related, but I didn't find the answer to my question. — I know that any (continuous) cocycle $f : G^n \to A$ has finite image, for $G$ is compact and $A$ is discrete. If the subgroup generated by the image of $f$ inside $A$ is also finite (say of cardinality $k$), then $k \cdot f = 0 : G^n \to A$ so that the class of $f$ in $H^n(G;A)$ has order at most $k$.  If $f$ is also a group morphism, which holds if $n=1$ and $A$ is a trivial $G$-module, then the image of $f$ inside $A$ is already a subgroup, so the aforementioned condition is satisfied. – But in general, we only want to find a multiple of $f$ which is a coboundary (without this multiple to be the zero map itself, as it was the case above). I'm not sure how to proceed. Is there may a smarter way to do it? Thank you!",,"['abstract-algebra', 'group-cohomology', 'profinite-groups', 'torsion-groups']"
