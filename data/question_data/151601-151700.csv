,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,extending the convergence of measures,extending the convergence of measures,,"Let $F$ the real vector space of all applications $\phi: X \times Y \rightarrow \mathbb{R}$ where  $(X,\mathcal{B}_1, \mu)$, $(Y,\mathcal{B}_2 )$ measurable spaces with $X$ and $Y$ are compact metric space provided with the borel sigma algebra $\mathcal{B}_1$, $\mathcal{B}_2$ such that $\phi$ is measurable. for all $x \in X$, $\phi_x=\phi(x,-): Y \rightarrow \mathbb{R}$ is continuous i.e. $\phi_x \in C(Y)$ $x\in X \mapsto \Vert \phi_x\Vert_{C(Y)} \in L^1(\mu)$ identifying $\phi \backsim \psi \Leftrightarrow \phi_x =\psi_x$ for $\mu -a.e $  and endow $ F $ of a norm $$\Vert \phi\Vert_F=\int_X \Vert \phi_x\Vert_{C(Y)} d\mu$$ then $ (F, \Vert \phi\Vert_F) $ is a separable Banach space where the dense set is the set of continuous functions $\varphi: X \times Y \rightarrow \mathbb{R}$. With this definition now present my question: Let $P(X\times Y)$ is the space of probability. Know $\nu_n \rightarrow \nu$ in $P(X\times Y)$ sss $\int \varphi d\nu_n\rightarrow \int \varphi d\nu$ for all continuous functions  $\varphi: X \times Y \rightarrow \mathbb{R}$. Then $\int \phi d\nu_n\rightarrow \int \phi d\nu$ for all functions  $\phi \in F$ ? Appreciate any help on how to test this assertion.","Let $F$ the real vector space of all applications $\phi: X \times Y \rightarrow \mathbb{R}$ where  $(X,\mathcal{B}_1, \mu)$, $(Y,\mathcal{B}_2 )$ measurable spaces with $X$ and $Y$ are compact metric space provided with the borel sigma algebra $\mathcal{B}_1$, $\mathcal{B}_2$ such that $\phi$ is measurable. for all $x \in X$, $\phi_x=\phi(x,-): Y \rightarrow \mathbb{R}$ is continuous i.e. $\phi_x \in C(Y)$ $x\in X \mapsto \Vert \phi_x\Vert_{C(Y)} \in L^1(\mu)$ identifying $\phi \backsim \psi \Leftrightarrow \phi_x =\psi_x$ for $\mu -a.e $  and endow $ F $ of a norm $$\Vert \phi\Vert_F=\int_X \Vert \phi_x\Vert_{C(Y)} d\mu$$ then $ (F, \Vert \phi\Vert_F) $ is a separable Banach space where the dense set is the set of continuous functions $\varphi: X \times Y \rightarrow \mathbb{R}$. With this definition now present my question: Let $P(X\times Y)$ is the space of probability. Know $\nu_n \rightarrow \nu$ in $P(X\times Y)$ sss $\int \varphi d\nu_n\rightarrow \int \varphi d\nu$ for all continuous functions  $\varphi: X \times Y \rightarrow \mathbb{R}$. Then $\int \phi d\nu_n\rightarrow \int \phi d\nu$ for all functions  $\phi \in F$ ? Appreciate any help on how to test this assertion.",,"['analysis', 'measure-theory', 'convergence-divergence']"
1,$\int_{\mathbb R^{2}} |\int_{\mathbb R} (f(t-y)- f(t)) g(t-x) e^{-2\pi i w\cdot t} dt|dx dw \to 0 $ as $y\to 0$?,as ?,\int_{\mathbb R^{2}} |\int_{\mathbb R} (f(t-y)- f(t)) g(t-x) e^{-2\pi i w\cdot t} dt|dx dw \to 0  y\to 0,"Fact: It is well-known that translation is continuous in the $L^{1}$ norm, that is,  if $f\in L^{1}(\mathbb R)$  then $\lim_{y\to 0} \|f_{y}-f\|_{L^{1}(\mathbb R)}=0;$ (where, $f_{y}(x)= f(x-y)$, $x\in \mathbb R$ ). (Bit roughly speaking, we consider the analogous case for the Fourier transform)   Suppose $f, \hat{f} \in L^{1}(\mathbb R).$ Consider, $\widehat{(f_{y}-f)} (\xi)= \int_{\mathbb R} (f_{y}-f)(x) e^{-2\pi i \xi \cdot x} dx= \int_{\mathbb R} (f(y-x)-f(x)) e^{-2\pi i \xi \cdot x} dx = (e^{-2\pi i y \cdot \xi }-1) \hat{f}(\xi); (\xi \in \mathbb R);$ therefore, by dominated convergence theorem, it follows that, $\|\widehat{f_{y}- f}\|_{L^{1}(\mathbb R)}\to 0$ as $y\to 0.$(Please correct me, If I am wrong here). We define the short-time Fourier transform, of $f$ with respect to non zero window function $g\in \mathcal{S}(\mathbb R)$(Schwartz space), as follows: $$V_{g}f(x, w)=\int_{\mathbb R} f(t) g(t-x) e^{-2\pi i w\cdot t} dt= \widehat{fg_{x}}(w); (x,w)\in \mathbb R^{2}.$$ My Question is : Suppose $f, \hat{f} \in L^{1}(\mathbb R),$ and $\|V_{g}f\|_{L^{1}(\mathbb R^{2})} <\infty.$    Can we expect $\|V_{g}f_{y}- V_{g}f\|_{L^{1}(\mathbb R \times \mathbb R)} \to 0$ as $y\to 0,$ that is, $\int_{\mathbb R^{2}} |\int_{\mathbb R} (f(t-y)- f(t)) g(t-x) e^{-2\pi i w\cdot t} dt|dx dw \to 0 $ as $y\to 0$ ? Thanks,","Fact: It is well-known that translation is continuous in the $L^{1}$ norm, that is,  if $f\in L^{1}(\mathbb R)$  then $\lim_{y\to 0} \|f_{y}-f\|_{L^{1}(\mathbb R)}=0;$ (where, $f_{y}(x)= f(x-y)$, $x\in \mathbb R$ ). (Bit roughly speaking, we consider the analogous case for the Fourier transform)   Suppose $f, \hat{f} \in L^{1}(\mathbb R).$ Consider, $\widehat{(f_{y}-f)} (\xi)= \int_{\mathbb R} (f_{y}-f)(x) e^{-2\pi i \xi \cdot x} dx= \int_{\mathbb R} (f(y-x)-f(x)) e^{-2\pi i \xi \cdot x} dx = (e^{-2\pi i y \cdot \xi }-1) \hat{f}(\xi); (\xi \in \mathbb R);$ therefore, by dominated convergence theorem, it follows that, $\|\widehat{f_{y}- f}\|_{L^{1}(\mathbb R)}\to 0$ as $y\to 0.$(Please correct me, If I am wrong here). We define the short-time Fourier transform, of $f$ with respect to non zero window function $g\in \mathcal{S}(\mathbb R)$(Schwartz space), as follows: $$V_{g}f(x, w)=\int_{\mathbb R} f(t) g(t-x) e^{-2\pi i w\cdot t} dt= \widehat{fg_{x}}(w); (x,w)\in \mathbb R^{2}.$$ My Question is : Suppose $f, \hat{f} \in L^{1}(\mathbb R),$ and $\|V_{g}f\|_{L^{1}(\mathbb R^{2})} <\infty.$    Can we expect $\|V_{g}f_{y}- V_{g}f\|_{L^{1}(\mathbb R \times \mathbb R)} \to 0$ as $y\to 0,$ that is, $\int_{\mathbb R^{2}} |\int_{\mathbb R} (f(t-y)- f(t)) g(t-x) e^{-2\pi i w\cdot t} dt|dx dw \to 0 $ as $y\to 0$ ? Thanks,",,"['real-analysis', 'analysis', 'fourier-analysis', 'continuity', 'banach-algebras']"
2,"How to prove that $F(x,y)=(f(x)h(y),g(y))$ is a diffeomorphism?",How to prove that  is a diffeomorphism?,"F(x,y)=(f(x)h(y),g(y))","Let $F:\mathbb{R}^2\to\mathbb{R}^2$ be given by $F(x,y)=(f(x)h(y),g(y))$, where $h:\mathbb{R}\to\mathbb{R}$ is a diferentiable function and $f,g:\mathbb{R}\to\mathbb{R}$ are diffeomorphisms. Definition: a diffeomorphism is a bijective differentiable mapping whose inverse is also differentiable. The problem is to show that the following sentences are equivalent. (a) $F$ is a diffeomorphism. (b) $0\notin h(\mathbb{R})$. If $f,g,h$ are $C^1$ functions, then $F$ is a $C^1$ function and thus we can use the following fact. Theorem: Let $U\subset\mathbb{R}^m$ be an open set and $F:U\to\mathbb{R}^m$ a $C^1$ function. Then, $F$ is a local diffeomorphism if, and only if, $\det J_F(x)\neq 0$ for all $x\in U$. In that case, I think the solution can be done as below. (a) $\Rightarrow$ (b) Since $F$ is a diffeomorphism, it's a local diffeomorphism. Since $F$ is $C^1$, it follows from theorem above that $\det J_F(x,y)\neq 0$ for all $(x,y)\in\mathbb{R}^2$. But  $$\det J_F(x,y)\neq 0\quad\Rightarrow\quad h(y)f'(x)g'(y)\neq0\quad\Rightarrow\quad h(y)\neq 0$$ so that $h(y)\neq 0$ for all $y\in\mathbb{R}$, that is, $0\notin h(\mathbb{R})$. (b) $\Rightarrow$ (a) Since $f,g$ are diffeomorphisms, $f'(x)g'(y)\neq 0$ for all $(x,y)\in\mathbb{R}^2$. Since $0\notin h(\mathbb{R}^2)$, it follows that $\det J_F(x,y)=h(y)f'(x)g'(y)\neq0$ for all $(x,y)\in\mathbb{R}^2$. As $F$ is $C^1$, we conclude from theorem above that $F$ is a local diffeomorphism. But we can prove that $F$ is bijective (because $f,g$ are bijective). So, $F$ is indeed a diffeomorphism. However, the hypothesis $f,g,h\in C^1$ is not a part of the exercise. So, how can we solve it without that hypothesis? Thanks.","Let $F:\mathbb{R}^2\to\mathbb{R}^2$ be given by $F(x,y)=(f(x)h(y),g(y))$, where $h:\mathbb{R}\to\mathbb{R}$ is a diferentiable function and $f,g:\mathbb{R}\to\mathbb{R}$ are diffeomorphisms. Definition: a diffeomorphism is a bijective differentiable mapping whose inverse is also differentiable. The problem is to show that the following sentences are equivalent. (a) $F$ is a diffeomorphism. (b) $0\notin h(\mathbb{R})$. If $f,g,h$ are $C^1$ functions, then $F$ is a $C^1$ function and thus we can use the following fact. Theorem: Let $U\subset\mathbb{R}^m$ be an open set and $F:U\to\mathbb{R}^m$ a $C^1$ function. Then, $F$ is a local diffeomorphism if, and only if, $\det J_F(x)\neq 0$ for all $x\in U$. In that case, I think the solution can be done as below. (a) $\Rightarrow$ (b) Since $F$ is a diffeomorphism, it's a local diffeomorphism. Since $F$ is $C^1$, it follows from theorem above that $\det J_F(x,y)\neq 0$ for all $(x,y)\in\mathbb{R}^2$. But  $$\det J_F(x,y)\neq 0\quad\Rightarrow\quad h(y)f'(x)g'(y)\neq0\quad\Rightarrow\quad h(y)\neq 0$$ so that $h(y)\neq 0$ for all $y\in\mathbb{R}$, that is, $0\notin h(\mathbb{R})$. (b) $\Rightarrow$ (a) Since $f,g$ are diffeomorphisms, $f'(x)g'(y)\neq 0$ for all $(x,y)\in\mathbb{R}^2$. Since $0\notin h(\mathbb{R}^2)$, it follows that $\det J_F(x,y)=h(y)f'(x)g'(y)\neq0$ for all $(x,y)\in\mathbb{R}^2$. As $F$ is $C^1$, we conclude from theorem above that $F$ is a local diffeomorphism. But we can prove that $F$ is bijective (because $f,g$ are bijective). So, $F$ is indeed a diffeomorphism. However, the hypothesis $f,g,h\in C^1$ is not a part of the exercise. So, how can we solve it without that hypothesis? Thanks.",,"['analysis', 'multivariable-calculus']"
3,Baby Rudin without knowing multivariate?,Baby Rudin without knowing multivariate?,,"I have read Spivak's Calculus and it has went well. I didn't have any problem with the rigorosity of the book at all. Now, I have never had any experience in multivariate. I only have experience with basic high school calculus, linear algebra and the calculus from Spivak. Can I learn multivariate calculus directly from the Baby Rudin analysis textbook (does it cover that)?","I have read Spivak's Calculus and it has went well. I didn't have any problem with the rigorosity of the book at all. Now, I have never had any experience in multivariate. I only have experience with basic high school calculus, linear algebra and the calculus from Spivak. Can I learn multivariate calculus directly from the Baby Rudin analysis textbook (does it cover that)?",,"['calculus', 'analysis', 'vector-analysis']"
4,Show that the limit of $ f(x)$ as $x\rightarrow 0$ is $0$,Show that the limit of  as  is, f(x) x\rightarrow 0 0,"Show directly from the definition of a limit of a function that lim x->0 (x^(1/3) * sin(1/x)) = 0. The definition is The limit of f as x goes to p is q if for every e>0 there exists a d>0 s.t. dx(x,p)<d for x belongs to E -> dy(f(x), q)< e with the notation limi x->p f(x) = q My approach We want to show that dx(x,0)<d, x belongs to E -> dy( (x^(1/3) * sin(1/x)),0) < e which is | x-0| < d - > | x |<d and |(x^(1/3) * sin(1/x))|< e Now the solution I have takes e^3=d and shows that |(x^(1/3) * sin(1/x))|< d^(1/3)=e which makes no sense... I suppose the steps start from |x^(1/3)|<= d^(1/3)  |x^(1/3)| * | sin(1/x)|<= d^(1/3) * | 1| (Does he multiply by one because the max value of sin is 1 and hence it will always be smaller? ) Then takes e^3=d and somehow it is proven. It makes much more sense that I explained it now but I am looking to see what people think.... Please edit the notation if you know how. Thanks a lot.","Show directly from the definition of a limit of a function that lim x->0 (x^(1/3) * sin(1/x)) = 0. The definition is The limit of f as x goes to p is q if for every e>0 there exists a d>0 s.t. dx(x,p)<d for x belongs to E -> dy(f(x), q)< e with the notation limi x->p f(x) = q My approach We want to show that dx(x,0)<d, x belongs to E -> dy( (x^(1/3) * sin(1/x)),0) < e which is | x-0| < d - > | x |<d and |(x^(1/3) * sin(1/x))|< e Now the solution I have takes e^3=d and shows that |(x^(1/3) * sin(1/x))|< d^(1/3)=e which makes no sense... I suppose the steps start from |x^(1/3)|<= d^(1/3)  |x^(1/3)| * | sin(1/x)|<= d^(1/3) * | 1| (Does he multiply by one because the max value of sin is 1 and hence it will always be smaller? ) Then takes e^3=d and somehow it is proven. It makes much more sense that I explained it now but I am looking to see what people think.... Please edit the notation if you know how. Thanks a lot.",,"['real-analysis', 'analysis']"
5,CW complex adjunction map,CW complex adjunction map,,"In topology we defined a quotient topology for glueing in the following way: Let $(X,O)$ and $(Y,O)$ be topological spaces and $f:A \subseteq X \rightarrow Y$ a continuous map, then we have that $X \cup_f Y = (X \cup Y)/(a \tilde \  f(a))$ is a quotient space. So far so good. Now, we had to deal with CW complexes. I know the definition, but I don't know how this concept is used to build up a CW complex. Let's take the n-sphere. The only thing I know is that this concept of adjunction is used to build CW-structure, but how exactly is that done? Wikipedia says that we have one 0-cell $e_0$ and one n-cell $e_n$. It does not say how exactly these two look like. I guess $e_0$ is just the northpole and $e_n$ the rest. Actually, if I am correct so far, we don't need the concept of adjunction because we could just write down a homeomorphism $\phi : B(0,1) \subset \mathbb{R}^n \rightarrow S^{n}\backslash\{(0,...,0,1\} \subset \mathbb{R}^{n+1}.$ Something like a stereographic projection should do this(although I did not really think about the details), but how would one build up this complex with the use of adjunction maps?","In topology we defined a quotient topology for glueing in the following way: Let $(X,O)$ and $(Y,O)$ be topological spaces and $f:A \subseteq X \rightarrow Y$ a continuous map, then we have that $X \cup_f Y = (X \cup Y)/(a \tilde \  f(a))$ is a quotient space. So far so good. Now, we had to deal with CW complexes. I know the definition, but I don't know how this concept is used to build up a CW complex. Let's take the n-sphere. The only thing I know is that this concept of adjunction is used to build CW-structure, but how exactly is that done? Wikipedia says that we have one 0-cell $e_0$ and one n-cell $e_n$. It does not say how exactly these two look like. I guess $e_0$ is just the northpole and $e_n$ the rest. Actually, if I am correct so far, we don't need the concept of adjunction because we could just write down a homeomorphism $\phi : B(0,1) \subset \mathbb{R}^n \rightarrow S^{n}\backslash\{(0,...,0,1\} \subset \mathbb{R}^{n+1}.$ Something like a stereographic projection should do this(although I did not really think about the details), but how would one build up this complex with the use of adjunction maps?",,"['calculus', 'real-analysis']"
6,Tricks to solve inequalities,Tricks to solve inequalities,,I am wondering if there are some tricks to solve inequalities which are not manageable analytically. For example consider the inequality (say we restrict on positive $x$): $\displaystyle \frac {\text e^{-x^2}}{x^2}<1 $ Let's say I want to find a good lower bound for $x$ which solves this inequality. But I can not solve it analytically to end up with a statement like: For all $x>...$ the inequality is fulfilled. Unfortunately I just know the two usual tricks like estimate $\frac {\text e^{-x^2}}{x^2}$ against some other function which is manageable. For example I know that $\frac {\text e^{-x^2}}{x^2}<1/x^2$ and $1/x^2<1$ for all $x>1$. So $x>1$ gives me a lower bound for $x$. try some value and use the fact that $\frac {\text e^{-x^2}}{x^2}$ is strictly decreasing. So I find that $x=0.8$ solves the inequality which gives me the slightly improved bound: $x>0.8$ Are there some more tricks? Some general treatments how to deal with inequalities where I have a precise expression of $x$ but can not solve it for $x>...$ analytically?,I am wondering if there are some tricks to solve inequalities which are not manageable analytically. For example consider the inequality (say we restrict on positive $x$): $\displaystyle \frac {\text e^{-x^2}}{x^2}<1 $ Let's say I want to find a good lower bound for $x$ which solves this inequality. But I can not solve it analytically to end up with a statement like: For all $x>...$ the inequality is fulfilled. Unfortunately I just know the two usual tricks like estimate $\frac {\text e^{-x^2}}{x^2}$ against some other function which is manageable. For example I know that $\frac {\text e^{-x^2}}{x^2}<1/x^2$ and $1/x^2<1$ for all $x>1$. So $x>1$ gives me a lower bound for $x$. try some value and use the fact that $\frac {\text e^{-x^2}}{x^2}$ is strictly decreasing. So I find that $x=0.8$ solves the inequality which gives me the slightly improved bound: $x>0.8$ Are there some more tricks? Some general treatments how to deal with inequalities where I have a precise expression of $x$ but can not solve it for $x>...$ analytically?,,"['real-analysis', 'analysis', 'inequality', 'estimation']"
7,can $L^p$ norm convergence and pointwise monotonic imply pointwise convergence?,can  norm convergence and pointwise monotonic imply pointwise convergence?,L^p,"Let $(f_n)_{n=1}^\infty$ be a sequence of measurable function such that $\lim_{n\to\infty}||f_n-f||_p=0$. If for any $x\in \Omega$, $\{f_{n}(x)\}_{n=1}^\infty$ is a monotonic sequence, can we deduce that $f_n\to f$ almost everywhere?","Let $(f_n)_{n=1}^\infty$ be a sequence of measurable function such that $\lim_{n\to\infty}||f_n-f||_p=0$. If for any $x\in \Omega$, $\{f_{n}(x)\}_{n=1}^\infty$ is a monotonic sequence, can we deduce that $f_n\to f$ almost everywhere?",,"['real-analysis', 'analysis', 'measure-theory', 'convergence-divergence', 'lp-spaces']"
8,Equivalent norm in Sobolev space,Equivalent norm in Sobolev space,,"Let $\rho\in H^{1}(0,\pi)$ be a function, and consider the functional $$ I(\rho)=\bigg(\int_{0}^{\pi}{\sqrt{\rho^2(t)+\dot\rho^2(t)}\,dt}\bigg)^2. $$  I'm asking if it is equivalent to the norm  $$ \lVert \rho \rVert_{H^1}=\lVert \rho \rVert_{L^2}+\lVert \dot\rho \rVert_{L^2}   $$ on $H^{1}(0,\pi)$. Obviously $I(\rho)\leq \lVert \rho \rVert_{H^1}^2$, i'm asking if the other inequalities holds.","Let $\rho\in H^{1}(0,\pi)$ be a function, and consider the functional $$ I(\rho)=\bigg(\int_{0}^{\pi}{\sqrt{\rho^2(t)+\dot\rho^2(t)}\,dt}\bigg)^2. $$  I'm asking if it is equivalent to the norm  $$ \lVert \rho \rVert_{H^1}=\lVert \rho \rVert_{L^2}+\lVert \dot\rho \rVert_{L^2}   $$ on $H^{1}(0,\pi)$. Obviously $I(\rho)\leq \lVert \rho \rVert_{H^1}^2$, i'm asking if the other inequalities holds.",,"['real-analysis', 'analysis', 'functional-analysis', 'sobolev-spaces', 'calculus-of-variations']"
9,Existence of fixed point,Existence of fixed point,,"I will copy the definition I am using just to make things clearer. Def. Let $(X,d)$ be a metric space and let $F:A(\subset X)\rightarrow X$. We say F is a contraction if there exists $\lambda$ where $ 0\leq \lambda <1$ such that: $$ d(F(x),F(y))\leq \lambda d(x,y) $$ for all $x,y\in X$. Now the contraction mappping theorem states that if $(X,d)$ is a complete metric space, then $F$ has a unique fixed point. If we let $x_0$ be any point in $X$, we can define a sequence $(x_n)_{n=1}^{\infty}$ such that $x_1=F(x_0)$, $x_2=F(x_1)$ $\dots$ I understood why it follows that $x_n$ is Cauchy. Then since $(X,d)$ is complete, $(x_n)$ converges to a limit in $(X,d)$, say $x$. The second claim of the theorem is that $x$ is a fixed point. But it was not totally clear to me why this is the case. The proof I have seen says: $x$ is a fixed point iff $d(F(x),x)=0$ (so far so good). Then for any $n$: $$ d(x,F(x)) \leq d(x,x_n)+d(x_n,F(x)) \\= d(x,x_n) + d(F(x_{n-1}),F(x))\\ \leq d(x,x_n)+\lambda d(x_{n-1},x) \rightarrow 0 \text{ as } n\rightarrow \infty $$ But this only proves that $d(F(x),x)\rightarrow0$ and not $d(F(x),x)=0$. How can I be certain that $x$ is indeed a fixed point?","I will copy the definition I am using just to make things clearer. Def. Let $(X,d)$ be a metric space and let $F:A(\subset X)\rightarrow X$. We say F is a contraction if there exists $\lambda$ where $ 0\leq \lambda <1$ such that: $$ d(F(x),F(y))\leq \lambda d(x,y) $$ for all $x,y\in X$. Now the contraction mappping theorem states that if $(X,d)$ is a complete metric space, then $F$ has a unique fixed point. If we let $x_0$ be any point in $X$, we can define a sequence $(x_n)_{n=1}^{\infty}$ such that $x_1=F(x_0)$, $x_2=F(x_1)$ $\dots$ I understood why it follows that $x_n$ is Cauchy. Then since $(X,d)$ is complete, $(x_n)$ converges to a limit in $(X,d)$, say $x$. The second claim of the theorem is that $x$ is a fixed point. But it was not totally clear to me why this is the case. The proof I have seen says: $x$ is a fixed point iff $d(F(x),x)=0$ (so far so good). Then for any $n$: $$ d(x,F(x)) \leq d(x,x_n)+d(x_n,F(x)) \\= d(x,x_n) + d(F(x_{n-1}),F(x))\\ \leq d(x,x_n)+\lambda d(x_{n-1},x) \rightarrow 0 \text{ as } n\rightarrow \infty $$ But this only proves that $d(F(x),x)\rightarrow0$ and not $d(F(x),x)=0$. How can I be certain that $x$ is indeed a fixed point?",,"['analysis', 'metric-spaces']"
10,the differentiability from an oscillation estimate,the differentiability from an oscillation estimate,,"If we have an oscillation estimate of $u\in W^{2,n} (B^{+}) \cap C^0(\bar B^{+})$, and $u=0$ on $T = B \cap \partial R^{n}_{+}$, that $$osc_{B^{+}} \frac{u}{x_{n}} \leq C.$$ Then how comes that $u$ is differential on $T$ ? Here $B = B_{1}(0)$. This is a question I met when reading the remark of Theorem 9.31 in the book ""Elliptic Partial Differential Equations of Second Order"" by Gilbarg and Trudinger, version of revision of the 1983 second edition(P254).","If we have an oscillation estimate of $u\in W^{2,n} (B^{+}) \cap C^0(\bar B^{+})$, and $u=0$ on $T = B \cap \partial R^{n}_{+}$, that $$osc_{B^{+}} \frac{u}{x_{n}} \leq C.$$ Then how comes that $u$ is differential on $T$ ? Here $B = B_{1}(0)$. This is a question I met when reading the remark of Theorem 9.31 in the book ""Elliptic Partial Differential Equations of Second Order"" by Gilbarg and Trudinger, version of revision of the 1983 second edition(P254).",,"['analysis', 'partial-differential-equations']"
11,Computing $\int_{\gamma} |z|^{-4}\ |dz|$,Computing,\int_{\gamma} |z|^{-4}\ |dz|,Let $\gamma = \rho e^{it}$ on $0 \le t \le 2\pi$ be the parameterization of the curve $|z| = \rho$. Goal: Compute $\int_{\gamma} |z|^{-4}\ |dz|$. Edited Attempt: $$ \int_{\gamma} {1 \over |z|^{4}}\ |dz| = \int_0^{2 \pi} {1 \over |(\rho e^{ it})|^4 } |i\rho e^{it}|\ dt = \int_0^{2 \pi} {1 \over \rho^4 } \rho\ dt = \int_0^{2 \pi} {1 \over \rho^3}\ dt = {2 \pi \over \rho^3} $$ Is this correct?,Let $\gamma = \rho e^{it}$ on $0 \le t \le 2\pi$ be the parameterization of the curve $|z| = \rho$. Goal: Compute $\int_{\gamma} |z|^{-4}\ |dz|$. Edited Attempt: $$ \int_{\gamma} {1 \over |z|^{4}}\ |dz| = \int_0^{2 \pi} {1 \over |(\rho e^{ it})|^4 } |i\rho e^{it}|\ dt = \int_0^{2 \pi} {1 \over \rho^4 } \rho\ dt = \int_0^{2 \pi} {1 \over \rho^3}\ dt = {2 \pi \over \rho^3} $$ Is this correct?,,"['complex-analysis', 'analysis']"
12,How was this sequence discovered?,How was this sequence discovered?,,"Let $N$ be a positive integer and consider the following rational sequence for $n \ge 0$: $$ a_{n+1} = \frac{N a_n + N}{a_n + N}, a_0 \in \Bbb{Q}. $$ If $-\sqrt{N} < a_0 < \sqrt{N}$, then $\{a_n\}$ is a monotone increasing rational sequence and converges to $\sqrt{N}$. If $\sqrt{N} < a_0$, then $\{a_n\}$ is a monotone decreasing rational sequence and converges to $\sqrt{N}$. By using this sequence, we can easily prove that both $\max\{r |r \lt \sqrt{N}, r \in \Bbb{Q}\}$ and $\min\{r |r \gt \sqrt{N}, r \in \Bbb{Q}\}$ do not exist if N is not a square integer. This sequence is nice. How was this sequence discovered? Bill Trok, thank you very much. I cannot prove the fact about your sequence but thanks to your answer, I know that the above sequence is not special and I could find another similar sequence. $$ a_{n+1} = \frac{-4 N^3 -4 N^2 -4 N}{(a_n + 2 N)^2 + 3 N} + N + 1, a_0 \in \Bbb{Q}. $$ If $-\sqrt{N} < a_0 < \sqrt{N}$, then $\{a_n\}$ is a monotone increasing rational sequence and converges to $\sqrt{N}$. If $\sqrt{N} < a_0$, then $\{a_n\}$ is a monotone decreasing rational sequence and converges to $\sqrt{N}$. http://wolfr.am/1hoilfq","Let $N$ be a positive integer and consider the following rational sequence for $n \ge 0$: $$ a_{n+1} = \frac{N a_n + N}{a_n + N}, a_0 \in \Bbb{Q}. $$ If $-\sqrt{N} < a_0 < \sqrt{N}$, then $\{a_n\}$ is a monotone increasing rational sequence and converges to $\sqrt{N}$. If $\sqrt{N} < a_0$, then $\{a_n\}$ is a monotone decreasing rational sequence and converges to $\sqrt{N}$. By using this sequence, we can easily prove that both $\max\{r |r \lt \sqrt{N}, r \in \Bbb{Q}\}$ and $\min\{r |r \gt \sqrt{N}, r \in \Bbb{Q}\}$ do not exist if N is not a square integer. This sequence is nice. How was this sequence discovered? Bill Trok, thank you very much. I cannot prove the fact about your sequence but thanks to your answer, I know that the above sequence is not special and I could find another similar sequence. $$ a_{n+1} = \frac{-4 N^3 -4 N^2 -4 N}{(a_n + 2 N)^2 + 3 N} + N + 1, a_0 \in \Bbb{Q}. $$ If $-\sqrt{N} < a_0 < \sqrt{N}$, then $\{a_n\}$ is a monotone increasing rational sequence and converges to $\sqrt{N}$. If $\sqrt{N} < a_0$, then $\{a_n\}$ is a monotone decreasing rational sequence and converges to $\sqrt{N}$. http://wolfr.am/1hoilfq",,"['calculus', 'sequences-and-series', 'analysis']"
13,Computing $\int_{\gamma} {dz \over (1-z)^3}$,Computing,\int_{\gamma} {dz \over (1-z)^3},"(a) Let $\gamma$ be the circle of radius ${1 \over 2}$ centered at the   origin, oriented counter-clockwise. Compute $$ \int_{\gamma} {dz \over (1-z)^3} $$ (b) Same as above, except $\gamma$ is centered at $1$ with radius ${1 \over 2}$. Part (a) attempt: Let $\gamma$ be the counter-clockwise circle of radius ${1 \over 2}$ centered at the origin parameterized by $\gamma(t) = {1 \over 2}e^{-it}$ on $0 \le t \le 2 \pi$. Then we have $$ \int_\gamma {dz \over (1 - z)^3} = \int_{-\gamma} {(-1)^2 dz \over (z-1)^{2+1}} = {2 \pi i \over 2!} f^{(2)}(1) \text{ s.t. }f(z) = 1 $$ and since $f^{(2)}(1) = 0$ we have that our integral is equal to zero. Another way of seeing this integral is equal to $0$ is to observe that since $1$ is outside the circle $-\gamma$, we have that $n(-\gamma, 1) = 0$ by Cauchy's Theorem in a disk (i.e., $-\gamma$ is a closed curve inside an open disk of radius greater than $1/2$ s.t. $f(z)$ is completely analytic on this disk; hence $\int_{-\gamma} {dz \over 1 - z} = 0$ so that the integral above must also equal $0$). Part (b) attempt: Now if we assume that $\gamma$ is centered at $1$ with radius $1/2$, then we have that $n(-\gamma,1) = 1$ so that $-n(\gamma,1) = -1$.  This makes me suspect our integral in question isn't equal to zero. Yet we still seem to have $$ \int_\gamma {dz \over (1 - z)^3} = \int_{-\gamma} {(-1)^2 dz \over (z-1)^{2+1}} = {2 \pi i \over 2!} f^{(2)}(1) \text{ s.t. }f(z) = 1 $$ But it seems like this value is still $0$ since $f^{(2)}(1) = 0$ regardless of where $\gamma$ is centered. This doesn't seem right -- so I think I'm misapplying Cauchy's Integral Formula.  Is this the case?","(a) Let $\gamma$ be the circle of radius ${1 \over 2}$ centered at the   origin, oriented counter-clockwise. Compute $$ \int_{\gamma} {dz \over (1-z)^3} $$ (b) Same as above, except $\gamma$ is centered at $1$ with radius ${1 \over 2}$. Part (a) attempt: Let $\gamma$ be the counter-clockwise circle of radius ${1 \over 2}$ centered at the origin parameterized by $\gamma(t) = {1 \over 2}e^{-it}$ on $0 \le t \le 2 \pi$. Then we have $$ \int_\gamma {dz \over (1 - z)^3} = \int_{-\gamma} {(-1)^2 dz \over (z-1)^{2+1}} = {2 \pi i \over 2!} f^{(2)}(1) \text{ s.t. }f(z) = 1 $$ and since $f^{(2)}(1) = 0$ we have that our integral is equal to zero. Another way of seeing this integral is equal to $0$ is to observe that since $1$ is outside the circle $-\gamma$, we have that $n(-\gamma, 1) = 0$ by Cauchy's Theorem in a disk (i.e., $-\gamma$ is a closed curve inside an open disk of radius greater than $1/2$ s.t. $f(z)$ is completely analytic on this disk; hence $\int_{-\gamma} {dz \over 1 - z} = 0$ so that the integral above must also equal $0$). Part (b) attempt: Now if we assume that $\gamma$ is centered at $1$ with radius $1/2$, then we have that $n(-\gamma,1) = 1$ so that $-n(\gamma,1) = -1$.  This makes me suspect our integral in question isn't equal to zero. Yet we still seem to have $$ \int_\gamma {dz \over (1 - z)^3} = \int_{-\gamma} {(-1)^2 dz \over (z-1)^{2+1}} = {2 \pi i \over 2!} f^{(2)}(1) \text{ s.t. }f(z) = 1 $$ But it seems like this value is still $0$ since $f^{(2)}(1) = 0$ regardless of where $\gamma$ is centered. This doesn't seem right -- so I think I'm misapplying Cauchy's Integral Formula.  Is this the case?",,"['complex-analysis', 'analysis']"
14,Properties of a Mehler's type integral,Properties of a Mehler's type integral,,"When computing the resolvent of the Laplace beltrami opetator on $S^n$ for even dimension, $n=2k$, I came across the following integral $$ F(\theta)=\int_{-\theta}^{\theta}{\frac{e^{(i\lambda-\mu)\phi}}{(2\cos \phi-2\cos \theta)^{1/2}} d\phi}, ~~~~~ \theta\in(0,\pi) $$ and I need to estimate $(\frac{1}{\sin \theta}\frac{\partial}{\partial\theta})^kF(\theta)$. I know that there is a similar formula called Mehler's integral $$ P_n(\cos\theta)=\int_{-\theta}^{\theta}{\frac{\cos (n+\frac{1}{2})\phi}{(2\cos \phi-2\cos \theta)^{1/2}} d\phi} $$ I want to know if there is a similar expression for $F(\theta)$, and do we have a  expansion of $F(\theta)$ like $\sum a_k(\theta,\lambda,\mu)e^{(i\lambda-\mu)\theta}$? The reason I ask the second question is that the resolvent in the odd dimension can be written as such forms, so I wonder if it's also the case in even dimention.","When computing the resolvent of the Laplace beltrami opetator on $S^n$ for even dimension, $n=2k$, I came across the following integral $$ F(\theta)=\int_{-\theta}^{\theta}{\frac{e^{(i\lambda-\mu)\phi}}{(2\cos \phi-2\cos \theta)^{1/2}} d\phi}, ~~~~~ \theta\in(0,\pi) $$ and I need to estimate $(\frac{1}{\sin \theta}\frac{\partial}{\partial\theta})^kF(\theta)$. I know that there is a similar formula called Mehler's integral $$ P_n(\cos\theta)=\int_{-\theta}^{\theta}{\frac{\cos (n+\frac{1}{2})\phi}{(2\cos \phi-2\cos \theta)^{1/2}} d\phi} $$ I want to know if there is a similar expression for $F(\theta)$, and do we have a  expansion of $F(\theta)$ like $\sum a_k(\theta,\lambda,\mu)e^{(i\lambda-\mu)\theta}$? The reason I ask the second question is that the resolvent in the odd dimension can be written as such forms, so I wonder if it's also the case in even dimention.",,"['integration', 'analysis', 'special-functions']"
15,bessel function maximizer,bessel function maximizer,,I try to find global maximum for $ \frac{J_2(x)}{x^2} $  I suspect it happens at x=0 ( plotting the graph) where the value of the function is $ \frac{1}{8} $  I know local maximizers are at   zeros of $ J_3(x) $ since $ \frac{d}{dx}\frac{J_2(x)}{x^2}=-\frac{J_3(x)}{x^2} $.  My problem is to evaluate $ \frac{J_2(x)}{x^2} $  at the zeros of  $ J_3(x) $ or compare it to  $ \frac{1}{8} $ . thanks.,I try to find global maximum for $ \frac{J_2(x)}{x^2} $  I suspect it happens at x=0 ( plotting the graph) where the value of the function is $ \frac{1}{8} $  I know local maximizers are at   zeros of $ J_3(x) $ since $ \frac{d}{dx}\frac{J_2(x)}{x^2}=-\frac{J_3(x)}{x^2} $.  My problem is to evaluate $ \frac{J_2(x)}{x^2} $  at the zeros of  $ J_3(x) $ or compare it to  $ \frac{1}{8} $ . thanks.,,"['analysis', 'special-functions']"
16,translation of open set problem,translation of open set problem,,"Suppose U is an open set in an Euclidean space. Then any point in U is contained in all but finitely many open sets that is translated by vectors converging to zero. It is easily proved in one dimensional euclidean space, but i can't prove it generally...","Suppose U is an open set in an Euclidean space. Then any point in U is contained in all but finitely many open sets that is translated by vectors converging to zero. It is easily proved in one dimensional euclidean space, but i can't prove it generally...",,"['general-topology', 'analysis']"
17,compactness in topology of pointwise convergence,compactness in topology of pointwise convergence,,"I started reading about the topology of pointwise convergence. So far I do not feel quite comfortable with this theory. Maybe one can help me out in a more concrete example case. Let's consider sequences, so $a : \mathbb{N} \to \mathbb{R},x \mapsto a(x)$. In the topology of pointwise convergence it holds now that $$(a)_n \to (A) \iff (a(x))_n \to A(x),\forall x \in \mathbb{N} \quad (1)$$ I hope so far I somehow boiled it down correctly to that concrete case. Now I wonder what it means for a set $\mathcal{S} \subseteq \mathbb{R}^\mathbb{N}$ of sequences to be compact in the topology of pointwise convergence. First, I need to get a feeling about ""open sets"" in that topology and how (1) defines those open sets -- I do not quite get the link. After that I would need to show, that every open cover of $\mathcal{S}$ has a finite subcover, right? What would that mean precisely, I mean in doing.. ADDED STUFF Consider we want to show, that $\mathcal{S} \subseteq \mathbb{R}^\mathbb{N}$ is compact in the topology of pointwise convergence. By, $S_n$ we denote the projections. Assume we know, that all $S_n$ are bounded. Further assume, that for every sequence in $\mathcal{S}$ we know, that the resulting projected sequences have a convergent subsequence of which the limit lies in the corresponding $S_n$. Does this suffice to show that $\mathcal{S}$ is compact?","I started reading about the topology of pointwise convergence. So far I do not feel quite comfortable with this theory. Maybe one can help me out in a more concrete example case. Let's consider sequences, so $a : \mathbb{N} \to \mathbb{R},x \mapsto a(x)$. In the topology of pointwise convergence it holds now that $$(a)_n \to (A) \iff (a(x))_n \to A(x),\forall x \in \mathbb{N} \quad (1)$$ I hope so far I somehow boiled it down correctly to that concrete case. Now I wonder what it means for a set $\mathcal{S} \subseteq \mathbb{R}^\mathbb{N}$ of sequences to be compact in the topology of pointwise convergence. First, I need to get a feeling about ""open sets"" in that topology and how (1) defines those open sets -- I do not quite get the link. After that I would need to show, that every open cover of $\mathcal{S}$ has a finite subcover, right? What would that mean precisely, I mean in doing.. ADDED STUFF Consider we want to show, that $\mathcal{S} \subseteq \mathbb{R}^\mathbb{N}$ is compact in the topology of pointwise convergence. By, $S_n$ we denote the projections. Assume we know, that all $S_n$ are bounded. Further assume, that for every sequence in $\mathcal{S}$ we know, that the resulting projected sequences have a convergent subsequence of which the limit lies in the corresponding $S_n$. Does this suffice to show that $\mathcal{S}$ is compact?",,"['general-topology', 'analysis', 'convergence-divergence', 'compactness']"
18,Connected Subset of Finite Topological Space,Connected Subset of Finite Topological Space,,"What I want to know is this: Am I correct in thinking that if we have a finite topological space (so no reals or anything here), that any connected subset contained in this space has only one element? For example, {x} is connected but {x,y} is disconnected.","What I want to know is this: Am I correct in thinking that if we have a finite topological space (so no reals or anything here), that any connected subset contained in this space has only one element? For example, {x} is connected but {x,y} is disconnected.",,['analysis']
19,Alternatives to Rudin,Alternatives to Rudin,,I'm taking an advanced calculus class this semester and we've been using Rudin's Principles of Mathematical Analysis. I was wondering if anyone could suggest some good analysis textbooks aimed toward undergrad students that I can use as a supplement.,I'm taking an advanced calculus class this semester and we've been using Rudin's Principles of Mathematical Analysis. I was wondering if anyone could suggest some good analysis textbooks aimed toward undergrad students that I can use as a supplement.,,['analysis']
20,Signed measure defined by an integral,Signed measure defined by an integral,,"Let $ (X,\mathcal{M},\mu)$ be a measure space and let $ f:X\to[-\infty,+\infty]$ be an integrable function (i.e. at least one of $ f_+ $ and $ f_-$ is integrable). I want to prove that $ \lambda:\mathcal{M}\to[-\infty,+\infty]$ defined as $$ \lambda(E)=\int_{E}f d\mu $$ is a signed measure . When I prove the countable additivity, how can I prove that the series $$ \sum_{i=1}^{+\infty}\int_{E_i}f d\mu$$ may only converge absolutely or diverge?","Let $ (X,\mathcal{M},\mu)$ be a measure space and let $ f:X\to[-\infty,+\infty]$ be an integrable function (i.e. at least one of $ f_+ $ and $ f_-$ is integrable). I want to prove that $ \lambda:\mathcal{M}\to[-\infty,+\infty]$ defined as $$ \lambda(E)=\int_{E}f d\mu $$ is a signed measure . When I prove the countable additivity, how can I prove that the series $$ \sum_{i=1}^{+\infty}\int_{E_i}f d\mu$$ may only converge absolutely or diverge?",,"['real-analysis', 'integration', 'analysis', 'measure-theory']"
21,Showing that the space of Hilbert-Schmidt operators form a Banach space.,Showing that the space of Hilbert-Schmidt operators form a Banach space.,,"How do i show that the set of Hilbert-Schmidt operators $HS(H) = \{T \in B(H) \; : \;  \sum^{\infty}_{n=1}\|Te_n\|^2 < \infty \}$ for some countable ONB $\{e_n\}$,  on a separable Hilbert Space $H$, form a Banach space when equipped with the HS norm? $$\|T\|_{HS} = \sqrt{\sum^{\infty}_{n=1}\|Te_n\|^2}$$","How do i show that the set of Hilbert-Schmidt operators $HS(H) = \{T \in B(H) \; : \;  \sum^{\infty}_{n=1}\|Te_n\|^2 < \infty \}$ for some countable ONB $\{e_n\}$,  on a separable Hilbert Space $H$, form a Banach space when equipped with the HS norm? $$\|T\|_{HS} = \sqrt{\sum^{\infty}_{n=1}\|Te_n\|^2}$$",,"['analysis', 'functional-analysis', 'operator-theory', 'hilbert-spaces', 'banach-spaces']"
22,Understanding a Proof for Why $\ell^2$ is Complete,Understanding a Proof for Why  is Complete,\ell^2,"Setting: Let $(x_n)$ be Cauchy in $\ell^2$ over $\mathbb{F} = \mathbb{C}$ or $\mathbb{R}$.  I'm trying to show that $(x_n) \rightarrow x \in \ell^2$.  That is, I'm trying to show that $\ell^2$ is complete in a particular way outlined below.  I only used the first few steps of the proof because once I understand the third step I think I can understand how the rest of the proof unfolds. Attempt: View $(x_k) = (x_1, x_2, \ldots)$ s.t. $$ x_k = x_{k,1}, x_{k,2}, \ldots $$ For any $i \in \mathbb{N}$ we have that for large enough $n$ and $m$ that $$  \left| x_{i,m} - x_{i,n} \right| < \varepsilon $$ Therefore $$ \sum_{i=1}^\infty \left| x_{i,m} - x_{i,n} \right| < \varepsilon $$ Question: Why is this last step true?  I can see how if it is true, then for $k$ large enough we would have that $\{x_{i,k}\}_{i=1}^\infty \rightarrow y_i$ for some $y_i \in \mathbb{F}$ (since $\mathbb{F}$ is complete).","Setting: Let $(x_n)$ be Cauchy in $\ell^2$ over $\mathbb{F} = \mathbb{C}$ or $\mathbb{R}$.  I'm trying to show that $(x_n) \rightarrow x \in \ell^2$.  That is, I'm trying to show that $\ell^2$ is complete in a particular way outlined below.  I only used the first few steps of the proof because once I understand the third step I think I can understand how the rest of the proof unfolds. Attempt: View $(x_k) = (x_1, x_2, \ldots)$ s.t. $$ x_k = x_{k,1}, x_{k,2}, \ldots $$ For any $i \in \mathbb{N}$ we have that for large enough $n$ and $m$ that $$  \left| x_{i,m} - x_{i,n} \right| < \varepsilon $$ Therefore $$ \sum_{i=1}^\infty \left| x_{i,m} - x_{i,n} \right| < \varepsilon $$ Question: Why is this last step true?  I can see how if it is true, then for $k$ large enough we would have that $\{x_{i,k}\}_{i=1}^\infty \rightarrow y_i$ for some $y_i \in \mathbb{F}$ (since $\mathbb{F}$ is complete).",,"['linear-algebra', 'complex-analysis', 'analysis']"
23,Question on a derivative on a Hilbert space,Question on a derivative on a Hilbert space,,"I have this functional $J(u)=\frac12 \|u\|^2+\int_0^1 F(t,Ku(t))dt$ where $F(t,u)=\int_0^u f(t,\xi) d\xi$ , $\displaystyle Ku(t)=\int_0^1 G(t,s)u(s) ds$ with $G(t,s)=\begin{cases} s(1-t),&0\leq s \leq t\leq 1\\t(1-s), &0\leq t\leq s \leq 1\end{cases}$ we define the Nemytskii operator $N_f : H\rightarrow H$ where $N_{f}u(t)=f(t,u(t)),\ t\in [0,1]$ and we have $J'(u)v=(u-KN_fK u,v)$ , For $\tau>0$ I'm trying to prove that $\frac{d}{d\tau} J(\tau u)=(J'(\tau u),u)$ it's written like that, and it gives the area that we applied the derivative of a composed function! But whene i used the definition (Gateaux) i found: $$\displaystyle\lim_{s\rightarrow 0} \frac{J(\tau u+sh)-J(\tau u)}{s}=(J'(\tau u),h)$$ for all $h\in H$ so $\nabla J(\tau u)=J'(\tau u)$ how to find that $\frac{d}{d\tau}J(\tau u)=(J'(\tau u),u)$ ?? Thank you.","I have this functional where , with we define the Nemytskii operator where and we have , For I'm trying to prove that it's written like that, and it gives the area that we applied the derivative of a composed function! But whene i used the definition (Gateaux) i found: for all so how to find that ?? Thank you.","J(u)=\frac12 \|u\|^2+\int_0^1 F(t,Ku(t))dt F(t,u)=\int_0^u f(t,\xi) d\xi \displaystyle Ku(t)=\int_0^1 G(t,s)u(s) ds G(t,s)=\begin{cases} s(1-t),&0\leq s \leq t\leq 1\\t(1-s), &0\leq t\leq s \leq 1\end{cases} N_f : H\rightarrow H N_{f}u(t)=f(t,u(t)),\ t\in [0,1] J'(u)v=(u-KN_fK u,v) \tau>0 \frac{d}{d\tau} J(\tau u)=(J'(\tau u),u) \displaystyle\lim_{s\rightarrow 0} \frac{J(\tau u+sh)-J(\tau u)}{s}=(J'(\tau u),h) h\in H \nabla J(\tau u)=J'(\tau u) \frac{d}{d\tau}J(\tau u)=(J'(\tau u),u)","['analysis', 'derivatives', 'hilbert-spaces', 'gateaux-derivative']"
24,When is a function differentiable,When is a function differentiable,,"Let $f$ be a function  $$ f(x,y)=\begin{cases}\quad0&(x,y)=(0,0)\\\dfrac{x^3y^2}{\left(x^2+y^2\right)^2} & (x,y)\neq(0,0). \end{cases}$$ I know what $f$ is continous at the point because the limit of $f$ when $(x,y)\to(0,0)$ exist: did go to the origin by the $x$- and $y$-axis and all lines through origin. Since the limit exists, then $f$ is continuous and the partial derivatives exist in this point. But why isn't $f$ differentiable in $(0,0)$?","Let $f$ be a function  $$ f(x,y)=\begin{cases}\quad0&(x,y)=(0,0)\\\dfrac{x^3y^2}{\left(x^2+y^2\right)^2} & (x,y)\neq(0,0). \end{cases}$$ I know what $f$ is continous at the point because the limit of $f$ when $(x,y)\to(0,0)$ exist: did go to the origin by the $x$- and $y$-axis and all lines through origin. Since the limit exists, then $f$ is continuous and the partial derivatives exist in this point. But why isn't $f$ differentiable in $(0,0)$?",,['analysis']
25,"Showing $f: \prod_{i \ge 1} \{0,2\}_i \rightarrow C$ is an Open Mapping into the Cantor Set",Showing  is an Open Mapping into the Cantor Set,"f: \prod_{i \ge 1} \{0,2\}_i \rightarrow C","Setting: Let $C$ denote the Cantor Set and $X = \prod_{i \ge 1} \{0,2\}_i$.  Let $X$ be given the product topology.  Consider $f : X \rightarrow C$ s.t. if $p = \left\langle x_1, x_2 , \ldots \right\rangle  \in X$, then $f(p) = \sum_{n \ge 1} x_n / 3^n  \in  C$.  My ultimate goal is to show that $f$ is a homeomorphism.  I've already shown that $f$ is continuous and bijective.  It remains to be shown that $f^{-1}$  is continuous. Goal: Show that $f^{-1}$ is continuous (or equivalently, show that $f$ is an open mapping). Attempt: Let $W$ be non-empty and open in $X$ under the product topology. Then $f^{-1}$ is continuous if $(f^{-1})^{-1}(W) = f(W)$ is open in $C$. Since $W$ is open in $X$, we have that $W = \prod_{i \ge 1} W_i$ s.t. (i) $W_i \ne X \implies W_i = U \cap \{0,2\}$ s.t. $U$ is open in $\mathbb{R}$ and (ii) all but a finite number of the $W_i$ are equal to $\{0,2\}$. Then the last step yields that any index of $W$ could be $\{0\}$, $\{2\}$, or $\{0,2\}$ (with the former two options only occurring at most a finite number of times). Then $f(W) = \{\sum_{n \ge 1} a_n / 3^n :$ a finite number of the $a_n$ are fixed as either $0$ or $2\}$. But then why must $f(W)$ be open in $C$?","Setting: Let $C$ denote the Cantor Set and $X = \prod_{i \ge 1} \{0,2\}_i$.  Let $X$ be given the product topology.  Consider $f : X \rightarrow C$ s.t. if $p = \left\langle x_1, x_2 , \ldots \right\rangle  \in X$, then $f(p) = \sum_{n \ge 1} x_n / 3^n  \in  C$.  My ultimate goal is to show that $f$ is a homeomorphism.  I've already shown that $f$ is continuous and bijective.  It remains to be shown that $f^{-1}$  is continuous. Goal: Show that $f^{-1}$ is continuous (or equivalently, show that $f$ is an open mapping). Attempt: Let $W$ be non-empty and open in $X$ under the product topology. Then $f^{-1}$ is continuous if $(f^{-1})^{-1}(W) = f(W)$ is open in $C$. Since $W$ is open in $X$, we have that $W = \prod_{i \ge 1} W_i$ s.t. (i) $W_i \ne X \implies W_i = U \cap \{0,2\}$ s.t. $U$ is open in $\mathbb{R}$ and (ii) all but a finite number of the $W_i$ are equal to $\{0,2\}$. Then the last step yields that any index of $W$ could be $\{0\}$, $\{2\}$, or $\{0,2\}$ (with the former two options only occurring at most a finite number of times). Then $f(W) = \{\sum_{n \ge 1} a_n / 3^n :$ a finite number of the $a_n$ are fixed as either $0$ or $2\}$. But then why must $f(W)$ be open in $C$?",,"['real-analysis', 'general-topology', 'analysis']"
26,"supA, infA, maxA, minA","supA, infA, maxA, minA",,"Having the set $$A=\{x: x \leq 0, x^2+x-1<0\}$$ I am asked to find $\sup A, \inf A, \max A, \min A$. I found the roots of the equation $x^2+x-1=0$, which are $$\frac{-1+ \sqrt{5}}{2} \text{ and } \frac{-1- \sqrt{5}}{2}.$$ Since $x \leq 0$, we have $x \in \left( \frac{-1- \sqrt{5}}{2},0 \right]$. So $$A= \left\{x:x  \in \left( \frac{-1- \sqrt{5}}{2},0 \right] \right\}.$$ So $\sup A=0, \max A=0, \inf A= \frac{-1- \sqrt{5}}{2}, \nexists \min A$. Is this correct?","Having the set $$A=\{x: x \leq 0, x^2+x-1<0\}$$ I am asked to find $\sup A, \inf A, \max A, \min A$. I found the roots of the equation $x^2+x-1=0$, which are $$\frac{-1+ \sqrt{5}}{2} \text{ and } \frac{-1- \sqrt{5}}{2}.$$ Since $x \leq 0$, we have $x \in \left( \frac{-1- \sqrt{5}}{2},0 \right]$. So $$A= \left\{x:x  \in \left( \frac{-1- \sqrt{5}}{2},0 \right] \right\}.$$ So $\sup A=0, \max A=0, \inf A= \frac{-1- \sqrt{5}}{2}, \nexists \min A$. Is this correct?",,"['real-analysis', 'analysis']"
27,Compact Domain and Inverse Image,Compact Domain and Inverse Image,,"I am trying to show that given $f:M \rightarrow N$, where $M$ is compact, $f$ is continuous and onto, then given $A \subset N$: $$ f^{-1}(A) \text{ closed} \implies A\text{ closed}  $$ I am dealing here with any metric space, although I feel that the approach is identical to the $\mathbb{R}$eal case (am I right?). So my attempt is the following. My attempt: $f^{-1}(A)$ closed and $f^{-1}(A) \subset M \implies f^{-1}(A)$ bounded and hence, compact. Hence, I can consider $\{x_n\} \rightarrow x, x \in f^{-1}(A)$. Using continuity, now we have a $\{[f(x)]_n\} \subset N$ and $\{[f(x)]_n\} \rightarrow f(x)$. Since $x \in f^{-1}(A)$, $f(x)=y \in A$. But how do I know that all limit points of $A$ are generated by convergent subsequences in $f^{-1}(A)$? Thanks.","I am trying to show that given $f:M \rightarrow N$, where $M$ is compact, $f$ is continuous and onto, then given $A \subset N$: $$ f^{-1}(A) \text{ closed} \implies A\text{ closed}  $$ I am dealing here with any metric space, although I feel that the approach is identical to the $\mathbb{R}$eal case (am I right?). So my attempt is the following. My attempt: $f^{-1}(A)$ closed and $f^{-1}(A) \subset M \implies f^{-1}(A)$ bounded and hence, compact. Hence, I can consider $\{x_n\} \rightarrow x, x \in f^{-1}(A)$. Using continuity, now we have a $\{[f(x)]_n\} \subset N$ and $\{[f(x)]_n\} \rightarrow f(x)$. Since $x \in f^{-1}(A)$, $f(x)=y \in A$. But how do I know that all limit points of $A$ are generated by convergent subsequences in $f^{-1}(A)$? Thanks.",,"['general-topology', 'analysis', 'continuity', 'compactness']"
28,Remainder of Taylor series,Remainder of Taylor series,,The Taylor series of the function $$f(x) = \int_{1}^{\sqrt{x}} \ln(xy)+ y^{3x} dy + e^{2x}$$ at the point $x = 1$ is $$e^2 + (x-1)\left(2e^2+\frac{1}{2}\right) + \frac{(x-1)^2}{2}\left(4e^2+\frac{7}{4}\right)$$ which I calculated using the Leibniz rule. How can I estimate the remainder term of second order for f(2) ? (The second derivate is already very complicated). Is there a method to calculate higher derivatives of parameter integrals easier than simply applying the Leibniz rule repeatedly?,The Taylor series of the function $$f(x) = \int_{1}^{\sqrt{x}} \ln(xy)+ y^{3x} dy + e^{2x}$$ at the point $x = 1$ is $$e^2 + (x-1)\left(2e^2+\frac{1}{2}\right) + \frac{(x-1)^2}{2}\left(4e^2+\frac{7}{4}\right)$$ which I calculated using the Leibniz rule. How can I estimate the remainder term of second order for f(2) ? (The second derivate is already very complicated). Is there a method to calculate higher derivatives of parameter integrals easier than simply applying the Leibniz rule repeatedly?,,"['analysis', 'taylor-expansion']"
29,How to find the inverse function?,How to find the inverse function?,,"For the function $f:\mathbb{N}\to\mathbb{N}$ the descrete derivative for $f$ in $n\in \mathbb{N}$ is defined as follows: $$f'(n) := f(n+1)-f(n)$$ Find the chain rule for the descrete derivative. Given that $f:\mathbb{N} \to \mathbb{N}$ is bijective. Is it possible to create a rule for descrete derivative of the inverse function of $f$? Hello, could someone give me a reasonable solution and explain the approach...thx","For the function $f:\mathbb{N}\to\mathbb{N}$ the descrete derivative for $f$ in $n\in \mathbb{N}$ is defined as follows: $$f'(n) := f(n+1)-f(n)$$ Find the chain rule for the descrete derivative. Given that $f:\mathbb{N} \to \mathbb{N}$ is bijective. Is it possible to create a rule for descrete derivative of the inverse function of $f$? Hello, could someone give me a reasonable solution and explain the approach...thx",,"['calculus', 'real-analysis', 'analysis']"
30,"Is $f \in W^{1,1}[a,b]$ equivalent to $f$ absolutely continuous on $[a,b]$?",Is  equivalent to  absolutely continuous on ?,"f \in W^{1,1}[a,b] f [a,b]","$f$ is a function defined on $[a,b]$. Then $f \in W^{1,1}$ is equivalent to $f$ is absolutely continuous?","$f$ is a function defined on $[a,b]$. Then $f \in W^{1,1}$ is equivalent to $f$ is absolutely continuous?",,"['real-analysis', 'analysis', 'functional-analysis', 'partial-differential-equations', 'sobolev-spaces']"
31,Energy norm: what is the intuition behind?,Energy norm: what is the intuition behind?,,"Last week I've read the following definition of Energy Norm (along with the definitions of a distance, Euclidean norm, p-norm, etc..: we were talking about metric spaces). Specifically, for $A\in \mathbb{R}^{n\times n}$ positive definite, the energy norm of $x\in \mathbb{R}^{n}$ is defined as follows: $$ ||x||_{A} := \sqrt{x^T A x}  $$ I felt completely lost. Is there any intuition / geometrical representation behind this concept that may help me to understand it? Thank you in advance!","Last week I've read the following definition of Energy Norm (along with the definitions of a distance, Euclidean norm, p-norm, etc..: we were talking about metric spaces). Specifically, for $A\in \mathbb{R}^{n\times n}$ positive definite, the energy norm of $x\in \mathbb{R}^{n}$ is defined as follows: $$ ||x||_{A} := \sqrt{x^T A x}  $$ I felt completely lost. Is there any intuition / geometrical representation behind this concept that may help me to understand it? Thank you in advance!",,"['analysis', 'metric-spaces']"
32,How prove $h(x)=f(x)-g(x)$,How prove,h(x)=f(x)-g(x),"show that: every function $h(x):R\to R$ can be written as $$h(x)=f(x)-g(x)$$ where $f(x),g(x)$ are satisfying the   intermediate value property http://en.wikipedia.org/wiki/Intermediate_value_theorem My try: since  $f(x),g(x)$ are all  such Intermediate value theorem mean that: so for $f(x)$,and for  any $[a,b]$, there exsit $\xi\in(a,b)$,such $$f(\xi)=\eta,$$ where $f(a)<\eta<f(b)$ similar for $g(x)$,and for any $[c,d]$,there $\xi_{1}\in[c,d]$,such $$f(\xi_{1})=\eta_{1}$$ where $\eta_{1}\in (f(c),f(d))$ and Now How can for any function $h(x)$,then  we always can  $$h(x)=f(x)-g(x)$$ Thank you","show that: every function $h(x):R\to R$ can be written as $$h(x)=f(x)-g(x)$$ where $f(x),g(x)$ are satisfying the   intermediate value property http://en.wikipedia.org/wiki/Intermediate_value_theorem My try: since  $f(x),g(x)$ are all  such Intermediate value theorem mean that: so for $f(x)$,and for  any $[a,b]$, there exsit $\xi\in(a,b)$,such $$f(\xi)=\eta,$$ where $f(a)<\eta<f(b)$ similar for $g(x)$,and for any $[c,d]$,there $\xi_{1}\in[c,d]$,such $$f(\xi_{1})=\eta_{1}$$ where $\eta_{1}\in (f(c),f(d))$ and Now How can for any function $h(x)$,then  we always can  $$h(x)=f(x)-g(x)$$ Thank you",,['analysis']
33,"For a continuous function $f :\mathbb{R}\rightarrow \mathbb{R}$ satisfying $\int_{\mathbb{R}}|f(x)|dx<\infty$, which of the following is true?","For a continuous function  satisfying , which of the following is true?",f :\mathbb{R}\rightarrow \mathbb{R} \int_{\mathbb{R}}|f(x)|dx<\infty,"Problem: For a continuous function $f :\mathbb{R}\rightarrow \mathbb{R}$ satisfying $$\int_{\mathbb{R}}|f(x)|dx<\infty$$ and for some $\alpha >0$ let $d_f(\alpha)$ be the Lebesgue measure of the set $$\{x\in \mathbb{R} : |f(x)|>\alpha\}$$ Then, for all $\alpha \geq 0$ which of the following is true? $\alpha d_f(\alpha)\leq \int_{\mathbb{R}}|f(x)|dx$ $\alpha^2 d_f(\alpha)\leq \int_{\mathbb{R}}|f(x)|dx$ $d_f(\alpha)\leq \alpha \int_{\mathbb{R}}|f(x)|dx$ $d_f(\alpha)\leq \alpha^2 \int_{\mathbb{R}}|f(x)|dx$ My solution: As $\alpha<|f(x)|$ on $\{x\in \mathbb{R} : |f(x)|>\alpha\}$ we have $$\int_{\{x: \alpha<|f(x)|\}}\alpha<\int_{\{x: \alpha<|f(x)|\}}|f(x)|<\int_{\mathbb{R}}|f(x)|dx$$ i.e., $$\alpha.d_f(\alpha)<\int_{\mathbb{R}}|f(x)|dx$$ So, I can see that first option is true. I belive all the other three options are not necessarily true but i could not come up with an example. Please help me to see if my argument for first bullet is sufficient/clear and help me to see other in detail. Thank you.","Problem: For a continuous function satisfying and for some let be the Lebesgue measure of the set Then, for all which of the following is true? My solution: As on we have i.e., So, I can see that first option is true. I belive all the other three options are not necessarily true but i could not come up with an example. Please help me to see if my argument for first bullet is sufficient/clear and help me to see other in detail. Thank you.",f :\mathbb{R}\rightarrow \mathbb{R} \int_{\mathbb{R}}|f(x)|dx<\infty \alpha >0 d_f(\alpha) \{x\in \mathbb{R} : |f(x)|>\alpha\} \alpha \geq 0 \alpha d_f(\alpha)\leq \int_{\mathbb{R}}|f(x)|dx \alpha^2 d_f(\alpha)\leq \int_{\mathbb{R}}|f(x)|dx d_f(\alpha)\leq \alpha \int_{\mathbb{R}}|f(x)|dx d_f(\alpha)\leq \alpha^2 \int_{\mathbb{R}}|f(x)|dx \alpha<|f(x)| \{x\in \mathbb{R} : |f(x)|>\alpha\} \int_{\{x: \alpha<|f(x)|\}}\alpha<\int_{\{x: \alpha<|f(x)|\}}|f(x)|<\int_{\mathbb{R}}|f(x)|dx \alpha.d_f(\alpha)<\int_{\mathbb{R}}|f(x)|dx,"['real-analysis', 'analysis']"
34,"How to prove $f'(\xi)=a$ ,if $ f(0)=0,f(a)=a,f'(x_{0})=0$?","How to prove  ,if ?","f'(\xi)=a  f(0)=0,f(a)=a,f'(x_{0})=0","let $a\in (0,1)$,and $f(x)$ is continuous on $[0,a]$, and is  differentiable on $(0,a)$, and $x_{0}\in (0,a)$,such $f'(x_{0})=0$,and  such $$f(0)=0, f(a)=a$$ show that there exsit $\xi\in(0,a)$,such $$f'(\xi)=a$$ My try: I want to put $$F(x)=f(x)-ax$$ since $$F(0)=f(0)-0=0$$ so if we find another point $c\in(0,a)$ such that $F(c)=f(c)-ac$ then we can use Rolle theorem to prove it. Now How to prove there exist $c\in(0,a)$,such that $f(c)=ac$ maybe this method is not useful,Thank you.","let $a\in (0,1)$,and $f(x)$ is continuous on $[0,a]$, and is  differentiable on $(0,a)$, and $x_{0}\in (0,a)$,such $f'(x_{0})=0$,and  such $$f(0)=0, f(a)=a$$ show that there exsit $\xi\in(0,a)$,such $$f'(\xi)=a$$ My try: I want to put $$F(x)=f(x)-ax$$ since $$F(0)=f(0)-0=0$$ so if we find another point $c\in(0,a)$ such that $F(c)=f(c)-ac$ then we can use Rolle theorem to prove it. Now How to prove there exist $c\in(0,a)$,such that $f(c)=ac$ maybe this method is not useful,Thank you.",,['analysis']
35,How prove this $f(x)=\sum_{n=1}^{\infty}\left(\frac{\sin \frac{1}{x-r_n} }{2}\right)^n$ such follow condition?,How prove this  such follow condition?,f(x)=\sum_{n=1}^{\infty}\left(\frac{\sin \frac{1}{x-r_n} }{2}\right)^n,"Question: let $r_{1},r_{2},\cdots.$ is the interval $[0, 1]$ rational sequence  in a line, and define: $$f(x)=\begin{cases} \displaystyle\sum_{n=1}^{\infty}\left(\dfrac{\sin{\left(\dfrac{1}{x-r_{n}}\right)}}{2}\right)^n &x\in R-Q\\ \displaystyle\sum_{n\neq m}^{\infty}\left(\dfrac{\sin{\left(\dfrac{1}{x-r_{n}}\right)}}{2}\right)^n&x=r_{m}\in Q \end{cases}$$   show that: Then function $f(x)$ such this follow three conditions: (1): $x\in [0,1]$ (2):   such $f(x)$ is continuous at all irrationals, discontinuous at all rationals; (3):and $f$   have many  infinite number discontinuity point of the second kind in $[0,1]$ the problem is from this: How take example such this three conditions.is continuous at all irrationals, discontinuous at all rationals I feel my example is such this three conditions, But I can't prove it (2),(3),especially (3). can you someone can help me? Thank you very much!","Question: let $r_{1},r_{2},\cdots.$ is the interval $[0, 1]$ rational sequence  in a line, and define: $$f(x)=\begin{cases} \displaystyle\sum_{n=1}^{\infty}\left(\dfrac{\sin{\left(\dfrac{1}{x-r_{n}}\right)}}{2}\right)^n &x\in R-Q\\ \displaystyle\sum_{n\neq m}^{\infty}\left(\dfrac{\sin{\left(\dfrac{1}{x-r_{n}}\right)}}{2}\right)^n&x=r_{m}\in Q \end{cases}$$   show that: Then function $f(x)$ such this follow three conditions: (1): $x\in [0,1]$ (2):   such $f(x)$ is continuous at all irrationals, discontinuous at all rationals; (3):and $f$   have many  infinite number discontinuity point of the second kind in $[0,1]$ the problem is from this: How take example such this three conditions.is continuous at all irrationals, discontinuous at all rationals I feel my example is such this three conditions, But I can't prove it (2),(3),especially (3). can you someone can help me? Thank you very much!",,['analysis']
36,How to integrate this partly defined function?,How to integrate this partly defined function?,,"I have a function $\psi(t):=1$ for $t \in [0,R]$, $\psi(t):=1 + \frac{R-t}{\epsilon}$ for $t \in [R,R+\epsilon]$ and $\psi(t):=0$ for $t > R+ \epsilon$ and a function $\phi(t):=\psi(t+\epsilon)$. And now I am asked what the integral $\int_0^{\infty} \psi(t) t^{n-1}dt $ is and the same for $\int_0^{\infty} \phi(t)t^{n-1}dt$? The problem is that as far as I see, if you first integrate this and then take the limit $\epsilon \rightarrow 0$, both integra do not coincide. Could anybody here please help me with the integrals, since I must have made a mistake. Especially the second one, if you do not have much time. Thanks to anybody here in advance!!!","I have a function $\psi(t):=1$ for $t \in [0,R]$, $\psi(t):=1 + \frac{R-t}{\epsilon}$ for $t \in [R,R+\epsilon]$ and $\psi(t):=0$ for $t > R+ \epsilon$ and a function $\phi(t):=\psi(t+\epsilon)$. And now I am asked what the integral $\int_0^{\infty} \psi(t) t^{n-1}dt $ is and the same for $\int_0^{\infty} \phi(t)t^{n-1}dt$? The problem is that as far as I see, if you first integrate this and then take the limit $\epsilon \rightarrow 0$, both integra do not coincide. Could anybody here please help me with the integrals, since I must have made a mistake. Especially the second one, if you do not have much time. Thanks to anybody here in advance!!!",,"['calculus', 'real-analysis']"
37,Interesting inequality $\|F\|_p\le \frac{\pi}{\sin(\pi/p)}\|f\|_p$ over $L^p$,Interesting inequality  over,\|F\|_p\le \frac{\pi}{\sin(\pi/p)}\|f\|_p L^p,"Consider the function $$F(x)=\int_0^\infty \frac{f(y)}{x+y} \, dy, \quad0<x<\infty$$ Prove that if $1<p<\infty$, $$\|F\|_p\le \frac{\pi}{\sin(\pi/p)}\|f\|_p$$ and show that the constant is the best possible. Since this problem came from a chapter on convolution, I think it might help to rewrite the integral in the form of a convolution over $\mathbb{R}$. I'm thinking of setting $z=x+y$ so that $y=z-x$. Also, to evaluate the constant, this might be helpful: for $0<a<1$, $$\int_0^\infty \frac1{(1+x)x^a} \, dx=\frac{\pi}{\sin\pi a}$$ Somewhat related: Prove $ F(x)=\int_0^{\infty}\frac{f(y)}{x+y}dy $ is continuous on $(0,\infty)$ and differentiable, and have $\lim\limits_{x\to \infty} F(x)=0$.","Consider the function $$F(x)=\int_0^\infty \frac{f(y)}{x+y} \, dy, \quad0<x<\infty$$ Prove that if $1<p<\infty$, $$\|F\|_p\le \frac{\pi}{\sin(\pi/p)}\|f\|_p$$ and show that the constant is the best possible. Since this problem came from a chapter on convolution, I think it might help to rewrite the integral in the form of a convolution over $\mathbb{R}$. I'm thinking of setting $z=x+y$ so that $y=z-x$. Also, to evaluate the constant, this might be helpful: for $0<a<1$, $$\int_0^\infty \frac1{(1+x)x^a} \, dx=\frac{\pi}{\sin\pi a}$$ Somewhat related: Prove $ F(x)=\int_0^{\infty}\frac{f(y)}{x+y}dy $ is continuous on $(0,\infty)$ and differentiable, and have $\lim\limits_{x\to \infty} F(x)=0$.",,"['real-analysis', 'analysis', 'measure-theory', 'inequality', 'convolution']"
38,Intuition tells me this function doesn't converge uniformly but not sure how to put it formally?,Intuition tells me this function doesn't converge uniformly but not sure how to put it formally?,,"$\mathbb{R}$ is the domain. Let $$f_n(x) = \frac{4n}{n+x^2}$$ As $n$ becomes large the $x^2$ term becomes insignificant and the function converges to $4$ pointwise. Now it seems to me that no matter what $N$ I choose, $$|f_n(x) - f(x)| < \epsilon$$ doesn't hold when $n \ge N$ for every $x$ as I can choose 'a very large $x$' to such that the $n$ values become insignificant and $f_n(x)$ approaches $0$. Is that correct? And how would I put this formally?","$\mathbb{R}$ is the domain. Let $$f_n(x) = \frac{4n}{n+x^2}$$ As $n$ becomes large the $x^2$ term becomes insignificant and the function converges to $4$ pointwise. Now it seems to me that no matter what $N$ I choose, $$|f_n(x) - f(x)| < \epsilon$$ doesn't hold when $n \ge N$ for every $x$ as I can choose 'a very large $x$' to such that the $n$ values become insignificant and $f_n(x)$ approaches $0$. Is that correct? And how would I put this formally?",,"['analysis', 'measure-theory']"
39,The 2nd total derivative (Hessian) of a composite function -Version 1,The 2nd total derivative (Hessian) of a composite function -Version 1,,"Let $f\in C^2(\mathbb R^n,\mathbb R)$ and $Df:A\subset\mathbb R^n\to L(\mathbb R^n,\mathbb R)$ so that $Df_x:\mathbb R^n\to\mathbb R$ is $f$ 's total derivative at $x\in\mathbb R^n$ . Let $f$ 's Hessian at the point $x$ be $Hf_x: \mathbb R^n \times  \mathbb R^n\to\mathbb R$ , defined (as a bilinear form) by $$Hf_x(x_1,x_2)=[[D(Df)_x] (x_2)](x_1).$$ Further, let $a\in\mathbb R^n$ and define $g:\mathbb R\to\mathbb R$ by $$g(t)=f(a+xt).$$ In a proof that $f(a)$ is a local max implies that $Hf_a$ is negative semidefinite, I encountered the following assertion, but I aren't able to see why: $$Hg_0(1,1)=Hf_a(x,x).$$","Let and so that is 's total derivative at . Let 's Hessian at the point be , defined (as a bilinear form) by Further, let and define by In a proof that is a local max implies that is negative semidefinite, I encountered the following assertion, but I aren't able to see why:","f\in C^2(\mathbb R^n,\mathbb R) Df:A\subset\mathbb R^n\to L(\mathbb R^n,\mathbb R) Df_x:\mathbb R^n\to\mathbb R f x\in\mathbb R^n f x Hf_x: \mathbb R^n \times  \mathbb R^n\to\mathbb R Hf_x(x_1,x_2)=[[D(Df)_x] (x_2)](x_1). a\in\mathbb R^n g:\mathbb R\to\mathbb R g(t)=f(a+xt). f(a) Hf_a Hg_0(1,1)=Hf_a(x,x).","['real-analysis', 'analysis', 'multivariable-calculus', 'derivatives', 'optimization']"
40,"Find an example of a sequence $\{f_k\}$ such that $f_k\in L^p$ for $1\le p <\infty$, $f_k\to0$ in $L^p$ for $1\le p < p_0$","Find an example of a sequence  such that  for ,  in  for",\{f_k\} f_k\in L^p 1\le p <\infty f_k\to0 L^p 1\le p < p_0,"Let $1<p_0<\infty$. Find an example of a sequence $\{f_k\}$ such that $f_k\in L^p$ for $1\le p <\infty$, $f_k\to0$ in $L^p$ for $1\le p < p_0$, but $f_k$ does not converge in $L^{p_0}$. I thought of $f_k = k^{1/p_0}\chi_{(0,1/k)}$. Then $||f_k||_p=k^{1/p_0- 1/p}$. So $f_k\in L^p$ for $1\le p <\infty$. Also for $1\le p <p_0$, $||f_k||_p\to0$. For $p_0<p$, $||f_k||_p\to\infty$. So suppose there exists $f$ such that $||f_k-f||_p\to 0$. Then by Minkowski's inequality, $||f_k||_p \le ||f_k-f||_p +||f||_p$. That means $||f||_p$ is unbounded. A contradiction. Is my proof correct? Is there any flaw? Thanks! EDIT: OK. I realized this shows $f_k$ doesn't converge in $L^p$ for $p>p_0$. But it's inconclusive when $p=p_0$. Any suggestion? Thanks.","Let $1<p_0<\infty$. Find an example of a sequence $\{f_k\}$ such that $f_k\in L^p$ for $1\le p <\infty$, $f_k\to0$ in $L^p$ for $1\le p < p_0$, but $f_k$ does not converge in $L^{p_0}$. I thought of $f_k = k^{1/p_0}\chi_{(0,1/k)}$. Then $||f_k||_p=k^{1/p_0- 1/p}$. So $f_k\in L^p$ for $1\le p <\infty$. Also for $1\le p <p_0$, $||f_k||_p\to0$. For $p_0<p$, $||f_k||_p\to\infty$. So suppose there exists $f$ such that $||f_k-f||_p\to 0$. Then by Minkowski's inequality, $||f_k||_p \le ||f_k-f||_p +||f||_p$. That means $||f||_p$ is unbounded. A contradiction. Is my proof correct? Is there any flaw? Thanks! EDIT: OK. I realized this shows $f_k$ doesn't converge in $L^p$ for $p>p_0$. But it's inconclusive when $p=p_0$. Any suggestion? Thanks.",,"['real-analysis', 'analysis', 'measure-theory', 'lebesgue-integral', 'lp-spaces']"
41,Why below sequence is diverge?,Why below sequence is diverge?,,"This problem maybe simple for you,but i dont know that why below sequence is diverge?please help me about this: why $\lbrace\mid x_{k}\mid\rbrace$ with below definition is diverge? $x_{k+1}:= x_{k}-(1+x_{k}^2)\arctan x_{k}$ ; $\arctan \mid x_{0}\mid>\dfrac{2\mid x_{0}\mid}{1+x_{0}^2}$","This problem maybe simple for you,but i dont know that why below sequence is diverge?please help me about this: why $\lbrace\mid x_{k}\mid\rbrace$ with below definition is diverge? $x_{k+1}:= x_{k}-(1+x_{k}^2)\arctan x_{k}$ ; $\arctan \mid x_{0}\mid>\dfrac{2\mid x_{0}\mid}{1+x_{0}^2}$",,"['real-analysis', 'sequences-and-series', 'analysis', 'numerical-methods']"
42,"The symmetry of mixed partials, for derivatives of order > 2","The symmetry of mixed partials, for derivatives of order > 2",,"Let $f\in C^r(A\subset \mathbb R^n,\mathbb R^m)$ , $Df:A\subset\mathbb R^n\to L(\mathbb R^n,\mathbb R^m)$ so that $Df(x):\mathbb R^n\to\mathbb R^m$ is $f$ 's total derivative, (abusing notation) $D^2f(x): \mathbb R^n \times \mathbb R^n\to\mathbb R^m$ be the bilinear map defined by $$[D^2f(x)](x_1,x_2)=[[D(Df)(x)] (x_2)](x_1),$$ (still abusing notation) $D^3f(x): \mathbb R^n \times \mathbb R^n\times \mathbb R^n\to\mathbb R^m$ be the trilinear map defined by $$[D^3f(x)](x_1,x_2,x_3)=[[D(D^2f)(x)] (x_3)](x_1,x_2),$$ and so on. For $ r\geq 3:$ How do I show that $D^rf(x)$ is symmetric (i.e. returns the same value for every permutation of inputs)? And why is $D^rf(x)$ being symmetric equivalent to the fact that for each component, $f$ 's $r$ th partials can be taken in any order? P.S. I have read the proof for the case of second-order derivatives. Reference: Marsden's Elementary Classical Analysis","Let , so that is 's total derivative, (abusing notation) be the bilinear map defined by (still abusing notation) be the trilinear map defined by and so on. For How do I show that is symmetric (i.e. returns the same value for every permutation of inputs)? And why is being symmetric equivalent to the fact that for each component, 's th partials can be taken in any order? P.S. I have read the proof for the case of second-order derivatives. Reference: Marsden's Elementary Classical Analysis","f\in C^r(A\subset \mathbb R^n,\mathbb R^m) Df:A\subset\mathbb R^n\to L(\mathbb R^n,\mathbb R^m) Df(x):\mathbb R^n\to\mathbb R^m f D^2f(x): \mathbb R^n \times \mathbb R^n\to\mathbb R^m [D^2f(x)](x_1,x_2)=[[D(Df)(x)] (x_2)](x_1), D^3f(x): \mathbb R^n \times \mathbb R^n\times \mathbb R^n\to\mathbb R^m [D^3f(x)](x_1,x_2,x_3)=[[D(D^2f)(x)] (x_3)](x_1,x_2),  r\geq 3: D^rf(x) D^rf(x) f r","['real-analysis', 'analysis', 'multivariable-calculus', 'partial-derivative']"
43,Calculating a derivative using the chain rule vs. differentiating the composite,Calculating a derivative using the chain rule vs. differentiating the composite,,"I have two functions $f:\mathbb{R} \rightarrow \mathbb{R}^2$ , $t \mapsto (t^3,t^2)$ and $g : \mathbb{R}^2 \rightarrow \mathbb{R}$ $(x,y) \mapsto (x^2+y^2)^{\alpha}$ Then we are asked to calculate $ (g \circ f)'(t)$ by using two different methods: First, the chain rule and second: by directly inserting f in g and then just differentiating with respect to t. After that, we are supposed to say, what the difference is, but unfortunately I get in both cases $(g \circ f)'(t) = \alpha (t^6 + t ^4 )^{\alpha - 1} (6 t^5+ 4 t^3)$ Maybe it is something about the existence of the derivatives... Unfortunately nobody here left a comment about the choice of $\alpha$. Can anybody comment on this, whether this could be a problem?","I have two functions $f:\mathbb{R} \rightarrow \mathbb{R}^2$ , $t \mapsto (t^3,t^2)$ and $g : \mathbb{R}^2 \rightarrow \mathbb{R}$ $(x,y) \mapsto (x^2+y^2)^{\alpha}$ Then we are asked to calculate $ (g \circ f)'(t)$ by using two different methods: First, the chain rule and second: by directly inserting f in g and then just differentiating with respect to t. After that, we are supposed to say, what the difference is, but unfortunately I get in both cases $(g \circ f)'(t) = \alpha (t^6 + t ^4 )^{\alpha - 1} (6 t^5+ 4 t^3)$ Maybe it is something about the existence of the derivatives... Unfortunately nobody here left a comment about the choice of $\alpha$. Can anybody comment on this, whether this could be a problem?",,['real-analysis']
44,Is $e^{it\Delta} g(x)$ continuous in $\mathcal{S}(\mathbb{R}^d)$?,Is  continuous in ?,e^{it\Delta} g(x) \mathcal{S}(\mathbb{R}^d),"In a paper, I see the following inequality: $$\vert e^{it\Delta} g(x)- e^{it'\Delta} g(x') \vert\leqslant C (\vert t-t'\vert + \vert x-x'\vert),$$ where $C$ depends on $d$ and $\Vert g \Vert_{L^{\infty}}$ ($g\in\mathcal{S}(\mathbb{R}^d)$). Here the $\Delta$ means the Laplacian. What's more, $e^{it\Delta}g(x)=\mathcal{F}^{-1}(e^{-t\vert\eta\vert^2}\mathcal{F}(g)(\eta))(x),$ where $\mathcal{F}$ means the Fourier transform in $\mathbb{R}^d$. I do not know how to prove this? Can anybody give me some suggestions?","In a paper, I see the following inequality: $$\vert e^{it\Delta} g(x)- e^{it'\Delta} g(x') \vert\leqslant C (\vert t-t'\vert + \vert x-x'\vert),$$ where $C$ depends on $d$ and $\Vert g \Vert_{L^{\infty}}$ ($g\in\mathcal{S}(\mathbb{R}^d)$). Here the $\Delta$ means the Laplacian. What's more, $e^{it\Delta}g(x)=\mathcal{F}^{-1}(e^{-t\vert\eta\vert^2}\mathcal{F}(g)(\eta))(x),$ where $\mathcal{F}$ means the Fourier transform in $\mathbb{R}^d$. I do not know how to prove this? Can anybody give me some suggestions?",,"['real-analysis', 'analysis', 'partial-differential-equations']"
45,How to distinguish a connected set or a disconnected set?,How to distinguish a connected set or a disconnected set?,,"I have some problems with this question? How to distinguish a connected set or a disconnected set ? Let $A=\left\{(x,y):0<x\le 1,y=\sin\frac1x\right\}$, $B=\{(x,y):y=0,-1\le x\le 0\}$, and let $S=A\cup B$. Prove that $S$ is connected but not arcwise connected.","I have some problems with this question? How to distinguish a connected set or a disconnected set ? Let $A=\left\{(x,y):0<x\le 1,y=\sin\frac1x\right\}$, $B=\{(x,y):y=0,-1\le x\le 0\}$, and let $S=A\cup B$. Prove that $S$ is connected but not arcwise connected.",,"['general-topology', 'analysis']"
46,Determining if a sequence of functions is a Cauchy sequence?,Determining if a sequence of functions is a Cauchy sequence?,,"Show that the space $C([a,b])$ equipped with the $L^1$-norm $||\cdot||_1$ defined by $$ ||f||_1 = \int_a^b|f(x)|dx ,$$ is incomplete. I was given a counter example to disprove the statement: Let $f_n$ be the sequence of functions: $$f_n(x) = \begin{cases} 0 & x\left[a,\frac{b-a}{2}\right)\\ nx-n\frac{(b-a)}{2} & x\in\left[\frac{b-a}{2},\frac{b-a}{2}+\frac{1}{n}\right)\\ 1 & x\in \left[\frac{b-a}{2}+\frac{1}{n},b\right] \end{cases}.$$ This is a cauchy sequence that converges to a discontinuous function. My question is: How do I see that such a sequence of functions is cauchy? My thought was that the $||\cdot||_1$ will determine the differences in area under the curve for each function, so that $||f_n-f_m||\leq \frac{(b-a)}{2}$. Is this correct?","Show that the space $C([a,b])$ equipped with the $L^1$-norm $||\cdot||_1$ defined by $$ ||f||_1 = \int_a^b|f(x)|dx ,$$ is incomplete. I was given a counter example to disprove the statement: Let $f_n$ be the sequence of functions: $$f_n(x) = \begin{cases} 0 & x\left[a,\frac{b-a}{2}\right)\\ nx-n\frac{(b-a)}{2} & x\in\left[\frac{b-a}{2},\frac{b-a}{2}+\frac{1}{n}\right)\\ 1 & x\in \left[\frac{b-a}{2}+\frac{1}{n},b\right] \end{cases}.$$ This is a cauchy sequence that converges to a discontinuous function. My question is: How do I see that such a sequence of functions is cauchy? My thought was that the $||\cdot||_1$ will determine the differences in area under the curve for each function, so that $||f_n-f_m||\leq \frac{(b-a)}{2}$. Is this correct?",,"['sequences-and-series', 'analysis', 'functional-analysis', 'self-learning', 'cauchy-sequences']"
47,Derivative $f : GL_n(\mathbb{R}) \to GL_n(\mathbb{R}) : A \mapsto A^{-1}$,Derivative,f : GL_n(\mathbb{R}) \to GL_n(\mathbb{R}) : A \mapsto A^{-1},"Given a continuous $f : GL_n(\mathbb{R}) \to GL_n(\mathbb{R}) : A \mapsto A^{-1}$, I want to show that for $g \in GL_n(\mathbb{R})$ the derivative $Df(g)$ equals $$ H \mapsto -g^{-1}Hg^{-1} $$ for $H \in Mat(n, \mathbb{R})$. I tried proving  $$ \lim_{X \to A} \frac{||f(X) - f(A) - Df(A)(X-A)||}{||X - A||} =  \lim_{X \to A} \frac{||X^{-1} - A^{-1} + A^{-1}(X - A)A||}{||X - A||} = 0, $$ (with $g = A$) but did not succeed. The syllabus suggests I use the chain-rule, but I don't see how.","Given a continuous $f : GL_n(\mathbb{R}) \to GL_n(\mathbb{R}) : A \mapsto A^{-1}$, I want to show that for $g \in GL_n(\mathbb{R})$ the derivative $Df(g)$ equals $$ H \mapsto -g^{-1}Hg^{-1} $$ for $H \in Mat(n, \mathbb{R})$. I tried proving  $$ \lim_{X \to A} \frac{||f(X) - f(A) - Df(A)(X-A)||}{||X - A||} =  \lim_{X \to A} \frac{||X^{-1} - A^{-1} + A^{-1}(X - A)A||}{||X - A||} = 0, $$ (with $g = A$) but did not succeed. The syllabus suggests I use the chain-rule, but I don't see how.",,['analysis']
48,A query about countability,A query about countability,,"Suppose we are working in an elementary  context where we want to keep background assumptions modest (not take a stand on fancy issues in set theory, say). What should our attitude be to the idea of countability? Countability is defined by a quantification  $X$ are countable if there is a function $f \colon N \to X$ which enumerates them. But quantification over which functions? Even if you fully buy into a rich set-theoretic background, taken at face value, different set theories will supply different enumerating functions. Thus the so-called constructible reals are uncountable according to the theory ZFC + V = L but countable according to the theory ZFC + there exists a Ramsey cardinal. More needs to be said even by the orthodox who identify functions with sets, if it isnt to be left somewhat indeterminate what objects are countable.  But suppose we fall short of  endorsing the set-thereotic orthodoxy because we dont (in the context, anyway) want definitely to buy into the wildly infinitary assumptions of this or that set theory: then that leaves more indeterminacy in what counts as countable. (If we don't settle just which functions we are prepared to countenance, we leave it correspondingly open which enumerating functions we are aiming to quantify over when we say that some given objects are countable.) Yet mathematicians  at least when writing in fairly elementary contexts  cheerfully talk about the countable as if thats unproblematic. How come? Is that just carelessness?? Or worse??? Well, elementary talk about the countable tends (doesnt it?) to feature  in three sorts of context: There are claims that certain objects are indeed countable, defended by showing that the objects in question are unproblematically counted by producing a nice enumerating function. (Consider, for a familiar simple example, how we show that the positive rationals $m/n$ are countable by actually constructing the zig-zag enumerating function for ordered pairs $m, n$, and so counting them.) There are claims that certain objects are uncountable, defended by reducing the assumption that they can be counted to absurdity. (Consider, for another familiar simple example, the usual diagonal argument that the infinite binary sequences are uncountable.) There are conditional claims of the kind if X are countable, then ... , supported by general arguments that are insensitive to how exactly we delimit (or fail to delimit) the countable. (The second is a special case of the third, of course, but perhaps worth highlighting separately.) In none of these kinds of case, at any rate, does such indeterminacy as we might be leaving in the extent of the countable become problematic. So if we proceed with due caution  restrict ourselves to these cases  we can continue to talk about the (un)countable safely enough. And in elementary contexts we do exercise such caution. So we are not in trouble after all. Or at least, so it seems. Query: is that a fair description of ordinary mathematical practice in elementary, non-set-theoretic, areas? If not, what is a better description of what is going on? While if Im right, can you think of some elementary texts which overtly fess up to the need for this element of caution?","Suppose we are working in an elementary  context where we want to keep background assumptions modest (not take a stand on fancy issues in set theory, say). What should our attitude be to the idea of countability? Countability is defined by a quantification  $X$ are countable if there is a function $f \colon N \to X$ which enumerates them. But quantification over which functions? Even if you fully buy into a rich set-theoretic background, taken at face value, different set theories will supply different enumerating functions. Thus the so-called constructible reals are uncountable according to the theory ZFC + V = L but countable according to the theory ZFC + there exists a Ramsey cardinal. More needs to be said even by the orthodox who identify functions with sets, if it isnt to be left somewhat indeterminate what objects are countable.  But suppose we fall short of  endorsing the set-thereotic orthodoxy because we dont (in the context, anyway) want definitely to buy into the wildly infinitary assumptions of this or that set theory: then that leaves more indeterminacy in what counts as countable. (If we don't settle just which functions we are prepared to countenance, we leave it correspondingly open which enumerating functions we are aiming to quantify over when we say that some given objects are countable.) Yet mathematicians  at least when writing in fairly elementary contexts  cheerfully talk about the countable as if thats unproblematic. How come? Is that just carelessness?? Or worse??? Well, elementary talk about the countable tends (doesnt it?) to feature  in three sorts of context: There are claims that certain objects are indeed countable, defended by showing that the objects in question are unproblematically counted by producing a nice enumerating function. (Consider, for a familiar simple example, how we show that the positive rationals $m/n$ are countable by actually constructing the zig-zag enumerating function for ordered pairs $m, n$, and so counting them.) There are claims that certain objects are uncountable, defended by reducing the assumption that they can be counted to absurdity. (Consider, for another familiar simple example, the usual diagonal argument that the infinite binary sequences are uncountable.) There are conditional claims of the kind if X are countable, then ... , supported by general arguments that are insensitive to how exactly we delimit (or fail to delimit) the countable. (The second is a special case of the third, of course, but perhaps worth highlighting separately.) In none of these kinds of case, at any rate, does such indeterminacy as we might be leaving in the extent of the countable become problematic. So if we proceed with due caution  restrict ourselves to these cases  we can continue to talk about the (un)countable safely enough. And in elementary contexts we do exercise such caution. So we are not in trouble after all. Or at least, so it seems. Query: is that a fair description of ordinary mathematical practice in elementary, non-set-theoretic, areas? If not, what is a better description of what is going on? While if Im right, can you think of some elementary texts which overtly fess up to the need for this element of caution?",,"['analysis', 'elementary-set-theory', 'logic']"
49,Parametrization of level sets of a smooth function,Parametrization of level sets of a smooth function,,"Let $H:\mathbb{R}^2\rightarrow\mathbb{R}$ be given by $H(q,p)=p^2/2+3q^2/2$ (single-well potential). This function has a critical point at $(0,0)$. Define $T:\mathbb{R}^+\rightarrow \mathbb{R}$ by, $T(h)=\int\limits_{H^{-1}(h)}\frac{dx}{|\nabla H(x)|}$. Question: Is $T$ continuous? And if possible can this be shown by the intuitive idea below? My intuition suggests that I should somehow convert this integral over a level set into an integral over a fixed surface by creating some sort of parametrization of level sets. This seems plausible because $H$ is smooth enough. Searching over internet esp. in Differential Geometry based references I have  found the idea of a tubular neighbourhood (I don't have much background in geometry). Can this be used somehow to convert the integral above to an integral over a fixed domain? Apologies if the question and the idea is too vague. Would be grateful for any references in this direction as well. PS. Not too sure about tags for this question.","Let $H:\mathbb{R}^2\rightarrow\mathbb{R}$ be given by $H(q,p)=p^2/2+3q^2/2$ (single-well potential). This function has a critical point at $(0,0)$. Define $T:\mathbb{R}^+\rightarrow \mathbb{R}$ by, $T(h)=\int\limits_{H^{-1}(h)}\frac{dx}{|\nabla H(x)|}$. Question: Is $T$ continuous? And if possible can this be shown by the intuitive idea below? My intuition suggests that I should somehow convert this integral over a level set into an integral over a fixed surface by creating some sort of parametrization of level sets. This seems plausible because $H$ is smooth enough. Searching over internet esp. in Differential Geometry based references I have  found the idea of a tubular neighbourhood (I don't have much background in geometry). Can this be used somehow to convert the integral above to an integral over a fixed domain? Apologies if the question and the idea is too vague. Would be grateful for any references in this direction as well. PS. Not too sure about tags for this question.",,"['analysis', 'differential-geometry', 'dynamical-systems', 'morse-theory']"
50,Gaussian function,Gaussian function,,"I want to scale the Gaussian function $\exp(-x^2)$ to the unit disc.  In particular, I wish to represent $\int_0^\infty \exp(-x^2) dx$ as $\int_0^1 g(x) dx$, where $g$ should be the rescaled Gaussian function. I read that $g(x)=\exp(-(1-x^2)^{-1})$ should do the job, but I can not see how to derive that or why it should be true. I mean, a homeomorphism between the positive real line and the unit line is given by $x\mapsto \frac{x}{1+x}$. But how can we conclude the above expression? I would be glad if you could help me out, best regards","I want to scale the Gaussian function $\exp(-x^2)$ to the unit disc.  In particular, I wish to represent $\int_0^\infty \exp(-x^2) dx$ as $\int_0^1 g(x) dx$, where $g$ should be the rescaled Gaussian function. I read that $g(x)=\exp(-(1-x^2)^{-1})$ should do the job, but I can not see how to derive that or why it should be true. I mean, a homeomorphism between the positive real line and the unit line is given by $x\mapsto \frac{x}{1+x}$. But how can we conclude the above expression? I would be glad if you could help me out, best regards",,"['analysis', 'integration', 'normal-distribution']"
51,Compactness of Hilbert-Schmidt Operator,Compactness of Hilbert-Schmidt Operator,,"I'm trying to show that a certain Hilbert-Schmidt operator is compact following some exercises in Rudin's Functional Analysis (exercise 15 on page 112): If $X, \mu$ is a finite measure space and $K \in L^2 (X \times X)$, define $T: L^2 (X) \rightarrow L^2 (X)$ by $$T(f)(s) = \int_X K(s,t)f(t) d\mu (t)$$. I think I'm supposed to show $T$ is compact following this outline: Approximate $K$ (in some sense) by a sequence $$K_n (s,t) = \sum _{i \leq n} a^n_i (s) b^n_i (t)$$ Where $a^n_i, b^n_i  \in L^2 (X)$.  Then the operators $$T_n (f) (s) :=   \int_X K_n(s,t)f(t) d\mu (t) $$ will hopefully converge to $T$ in the operator norm.  Then the fact that every $T_n$ has finite dimensional range will imply that $T$ is compact. The issue is that I don't know how to find  $a^n_i$ and $b^n_i$ .  Thanks for reading my question.","I'm trying to show that a certain Hilbert-Schmidt operator is compact following some exercises in Rudin's Functional Analysis (exercise 15 on page 112): If $X, \mu$ is a finite measure space and $K \in L^2 (X \times X)$, define $T: L^2 (X) \rightarrow L^2 (X)$ by $$T(f)(s) = \int_X K(s,t)f(t) d\mu (t)$$. I think I'm supposed to show $T$ is compact following this outline: Approximate $K$ (in some sense) by a sequence $$K_n (s,t) = \sum _{i \leq n} a^n_i (s) b^n_i (t)$$ Where $a^n_i, b^n_i  \in L^2 (X)$.  Then the operators $$T_n (f) (s) :=   \int_X K_n(s,t)f(t) d\mu (t) $$ will hopefully converge to $T$ in the operator norm.  Then the fact that every $T_n$ has finite dimensional range will imply that $T$ is compact. The issue is that I don't know how to find  $a^n_i$ and $b^n_i$ .  Thanks for reading my question.",,"['analysis', 'functional-analysis']"
52,Why is there this contradiction or what is wrong,Why is there this contradiction or what is wrong,,"In the second paragraph on Page 30 of this published paper , it says that the intersection of the convex hull of points $(\alpha_{1}+\beta_{P_{1}},\alpha_{2}+\beta_{P_{2}})$ with the convex hull of points $(\beta_{1}+\alpha_{P_{1}},\beta_{2}+\alpha_{P_{2}})$ contains the point $(\gamma_{1},\gamma_{2})$. For example, if $\alpha_{1}=2$, $\alpha_{2}=0$, $\beta_{1}=3$, and $\beta_{2}=2$, then the intersection is a line segment between $(3,4)$ and $(4,3)$ . But don't the eigenvalues of the the sum matrix actually lie on the line segment between $(1,5)$and $(3,4)$? or what is wrong?","In the second paragraph on Page 30 of this published paper , it says that the intersection of the convex hull of points $(\alpha_{1}+\beta_{P_{1}},\alpha_{2}+\beta_{P_{2}})$ with the convex hull of points $(\beta_{1}+\alpha_{P_{1}},\beta_{2}+\alpha_{P_{2}})$ contains the point $(\gamma_{1},\gamma_{2})$. For example, if $\alpha_{1}=2$, $\alpha_{2}=0$, $\beta_{1}=3$, and $\beta_{2}=2$, then the intersection is a line segment between $(3,4)$ and $(4,3)$ . But don't the eigenvalues of the the sum matrix actually lie on the line segment between $(1,5)$and $(3,4)$? or what is wrong?",,"['linear-algebra', 'geometry', 'analysis', 'matrices']"
53,Why base analysis on the natural numbers rather than on sets?,Why base analysis on the natural numbers rather than on sets?,,"I'm looking Bishop's Foundations of Constructive Analysis . I decided to look at the Amazon's page searching for some elucidative review about if it's a worthy read, I've found this review . The first phrase of the second paragraph says: The point is that, if you want, you can base analysis on the natural numbers rather than on sets. I don't get the specific meaning of this nor what difference it would make, we build sets of natural numbers on analysis, right? What's the point? A total avoidance of the concept of sets? For what purpose? Bishop mentions a lot about computational existence and computational ambiguity - I'm not really sure but I feel that they are connected with the basing of analysis on natural numbers (instead of sets) somehow.","I'm looking Bishop's Foundations of Constructive Analysis . I decided to look at the Amazon's page searching for some elucidative review about if it's a worthy read, I've found this review . The first phrase of the second paragraph says: The point is that, if you want, you can base analysis on the natural numbers rather than on sets. I don't get the specific meaning of this nor what difference it would make, we build sets of natural numbers on analysis, right? What's the point? A total avoidance of the concept of sets? For what purpose? Bishop mentions a lot about computational existence and computational ambiguity - I'm not really sure but I feel that they are connected with the basing of analysis on natural numbers (instead of sets) somehow.",,"['real-analysis', 'complex-analysis', 'analysis', 'foundations']"
54,"Can somebody correct my proof? : if f is continuously differentiable, then f is differentiable.","Can somebody correct my proof? : if f is continuously differentiable, then f is differentiable.",,"Theorem: Let $U\subseteq \Bbb R^n$ be open. If $f$ has continuous first partial derivatives in $U$ then $f$ is differentiable in $U$. Proof: Let's prove that if $f$ is differentiable at $a$ then $T(h)=Df(a).h$ ,where T(h) is total derivative. Let's set $T(h)= (\alpha_1.h+\alpha_2.h+...+\alpha_n.h)=(\alpha_1,...,\alpha_n).h $ Since $f$ has continuous derivatives, $$\lim_{h\to0}\frac{f(a+h)-f(a)-T(h)}{\Vert h\Vert}=0 \tag 1$$ by the first order approximation theorem. Set $h:=te_i$ for $t>0$ $$\begin{align} \lim_{h\to0}\frac{f(a+h)-f(a)-T(h)}{\Vert h\Vert}&=\lim_{h\to0}\frac{f(a+te_i)-f(a)-\alpha_it}{t} \\ &=\frac{f(a+te_i)-f(a)}{t}-\alpha_i \end{align}$$ $\lim_{t\to 0^+}\frac{f(a+te_i)-f(a)}{t}=\alpha_i$ by the equation (1) We also need to check this limit for $t<0$ Do I prove the theorem? Is this proof enough? Do I have mistakes ? Please can one correct my proof? Thank you:)","Theorem: Let $U\subseteq \Bbb R^n$ be open. If $f$ has continuous first partial derivatives in $U$ then $f$ is differentiable in $U$. Proof: Let's prove that if $f$ is differentiable at $a$ then $T(h)=Df(a).h$ ,where T(h) is total derivative. Let's set $T(h)= (\alpha_1.h+\alpha_2.h+...+\alpha_n.h)=(\alpha_1,...,\alpha_n).h $ Since $f$ has continuous derivatives, $$\lim_{h\to0}\frac{f(a+h)-f(a)-T(h)}{\Vert h\Vert}=0 \tag 1$$ by the first order approximation theorem. Set $h:=te_i$ for $t>0$ $$\begin{align} \lim_{h\to0}\frac{f(a+h)-f(a)-T(h)}{\Vert h\Vert}&=\lim_{h\to0}\frac{f(a+te_i)-f(a)-\alpha_it}{t} \\ &=\frac{f(a+te_i)-f(a)}{t}-\alpha_i \end{align}$$ $\lim_{t\to 0^+}\frac{f(a+te_i)-f(a)}{t}=\alpha_i$ by the equation (1) We also need to check this limit for $t<0$ Do I prove the theorem? Is this proof enough? Do I have mistakes ? Please can one correct my proof? Thank you:)",,"['calculus', 'real-analysis', 'analysis']"
55,Limiting value of functions - From Tao Analysis II,Limiting value of functions - From Tao Analysis II,,"Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces,$E \subseteq X$ and $f:E \rightarrow Y$ a function. If $x_0 \in \overline E$ we define $$  \lim_{x \rightarrow x_0;x \in E} f(x) = L  $$ iff $$ \exists L \in Y \forall \epsilon > 0 \exists \delta > 0 \forall x\in E: d_X(x,x_0) < \delta \rightarrow d_Y(f(x),L) < \epsilon \qquad (1) $$ He then gives the following exercise: Let $x_0 \in E$. Then $\lim_{x \rightarrow x_0; x \in E} f(x)$ exists $\iff$ $\lim_{x \rightarrow x_0; x \in E \setminus \{x_0\}} f(x)$ exists. Further we have that if $\lim_{x \rightarrow x_0; x \in E} f(x)$ exists, then the limit equals $f(x_0)$. First the implications: From left to right is trivial. But what if $x \in E \setminus \{x_0\}$ ? Why does then (1) also hold for $x_0$ itself ? I got some edits: The implication $\Leftarrow )$ also requires that the limit equals $f(x_0)$. Sorry for that. Then the equivalence is true. Further I see that Tao doesn't intend that for example the function $f(x)= 1$ if $x = x_0$ and $f(x) = 0$ otherwise should have a limit where $E = \mathbb R$. First, if we consider $E \setminus \{x_0\}$ we have that the limit equals $0$. In fact if the limit exists and $x_0 \in E$ then the function must be continous.","Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces,$E \subseteq X$ and $f:E \rightarrow Y$ a function. If $x_0 \in \overline E$ we define $$  \lim_{x \rightarrow x_0;x \in E} f(x) = L  $$ iff $$ \exists L \in Y \forall \epsilon > 0 \exists \delta > 0 \forall x\in E: d_X(x,x_0) < \delta \rightarrow d_Y(f(x),L) < \epsilon \qquad (1) $$ He then gives the following exercise: Let $x_0 \in E$. Then $\lim_{x \rightarrow x_0; x \in E} f(x)$ exists $\iff$ $\lim_{x \rightarrow x_0; x \in E \setminus \{x_0\}} f(x)$ exists. Further we have that if $\lim_{x \rightarrow x_0; x \in E} f(x)$ exists, then the limit equals $f(x_0)$. First the implications: From left to right is trivial. But what if $x \in E \setminus \{x_0\}$ ? Why does then (1) also hold for $x_0$ itself ? I got some edits: The implication $\Leftarrow )$ also requires that the limit equals $f(x_0)$. Sorry for that. Then the equivalence is true. Further I see that Tao doesn't intend that for example the function $f(x)= 1$ if $x = x_0$ and $f(x) = 0$ otherwise should have a limit where $E = \mathbb R$. First, if we consider $E \setminus \{x_0\}$ we have that the limit equals $0$. In fact if the limit exists and $x_0 \in E$ then the function must be continous.",,[]
56,When $f^{(n)}\to g$ uniformly?,When  uniformly?,f^{(n)}\to g,"Let $f\in C^\infty([0,1])$ and consider the sequence $$f_n=f^{(n)}$$ where $f^{(n)}$ denote the derivative of order $n$ of $f$. My question is: What is a necessary condition to impose on $f$, such that $f_n$ converges uniformly to some $g\in C([0,1])$, i.e. $\|f_n-g\|_\infty\to 0$? I could find some examples that maybe can help. I - If $f$ is a polynomious, then $f_n\to 0$, II - If $f(x)=e^{\lambda x}$, then $f_n\to 0$ if $\lambda\in [0,1)$ and $f_n\to f$ if $\lambda =1$. Thank you","Let $f\in C^\infty([0,1])$ and consider the sequence $$f_n=f^{(n)}$$ where $f^{(n)}$ denote the derivative of order $n$ of $f$. My question is: What is a necessary condition to impose on $f$, such that $f_n$ converges uniformly to some $g\in C([0,1])$, i.e. $\|f_n-g\|_\infty\to 0$? I could find some examples that maybe can help. I - If $f$ is a polynomious, then $f_n\to 0$, II - If $f(x)=e^{\lambda x}$, then $f_n\to 0$ if $\lambda\in [0,1)$ and $f_n\to f$ if $\lambda =1$. Thank you",,"['analysis', 'uniform-convergence']"
57,Distinguishing between the different eigenvalues,Distinguishing between the different eigenvalues,,"Consider the symmetric matrix $$A=\begin{pmatrix} 2 & t & \cos t-1 \\ t & 2 & 0 \\ \cos t-1 & 0 & 2  \end{pmatrix}. $$ The (real) eigenvalues of $A$ can be found easily using the quadratic formula: they are $$\lambda_1,\lambda_2,\lambda_3=2,\frac{1}{2} \left(4-\sqrt{2} \sqrt{2 t^2-4 \cos (t)+\cos (2 t)+3}\right),\frac{1}{2} \left(4+\sqrt{2} \sqrt{2 t^2-4 \cos (t)+\cos (2 t)+3}\right) $$ Plotting them as functions of the real parameter $t$, I obtain this graph What bothers me, is that we could have defined the eigenvalues differently as $$\lambda_1,\lambda_2,\lambda_3=2,\frac{1}{2} \left(4-\text{sgn } t\sqrt{2} \sqrt{2 t^2-4 \cos (t)+\cos (2 t)+3}\right),\frac{1}{2} \left(4+ \text{sgn } t\sqrt{2} \sqrt{2 t^2-4 \cos (t)+\cos (2 t)+3}\right), $$ so that the graph would look like this: This way, the functions $\lambda_1(t),\lambda_2(t),\lambda_3(t)$ are of class $C^\infty$ (smooth). I have a few questions regarding this: Given a symmetric matrix which depends (smoothly) on a real parameter $t$, is it possible to define its eigenvalues $\lambda_1(t), \dots, \lambda_n(t)$ is such a way that they will be smooth functions of $t$? Is there a natural way of finding these ""smooth branches""? (The quadratic formula has failed me.) A little more generally, given the equation $f(x,t)=0$, under what conditions is there a choice for all solutions $x(t)$, so that they are as regular as $f$ is WRT $t$? Thank you","Consider the symmetric matrix $$A=\begin{pmatrix} 2 & t & \cos t-1 \\ t & 2 & 0 \\ \cos t-1 & 0 & 2  \end{pmatrix}. $$ The (real) eigenvalues of $A$ can be found easily using the quadratic formula: they are $$\lambda_1,\lambda_2,\lambda_3=2,\frac{1}{2} \left(4-\sqrt{2} \sqrt{2 t^2-4 \cos (t)+\cos (2 t)+3}\right),\frac{1}{2} \left(4+\sqrt{2} \sqrt{2 t^2-4 \cos (t)+\cos (2 t)+3}\right) $$ Plotting them as functions of the real parameter $t$, I obtain this graph What bothers me, is that we could have defined the eigenvalues differently as $$\lambda_1,\lambda_2,\lambda_3=2,\frac{1}{2} \left(4-\text{sgn } t\sqrt{2} \sqrt{2 t^2-4 \cos (t)+\cos (2 t)+3}\right),\frac{1}{2} \left(4+ \text{sgn } t\sqrt{2} \sqrt{2 t^2-4 \cos (t)+\cos (2 t)+3}\right), $$ so that the graph would look like this: This way, the functions $\lambda_1(t),\lambda_2(t),\lambda_3(t)$ are of class $C^\infty$ (smooth). I have a few questions regarding this: Given a symmetric matrix which depends (smoothly) on a real parameter $t$, is it possible to define its eigenvalues $\lambda_1(t), \dots, \lambda_n(t)$ is such a way that they will be smooth functions of $t$? Is there a natural way of finding these ""smooth branches""? (The quadratic formula has failed me.) A little more generally, given the equation $f(x,t)=0$, under what conditions is there a choice for all solutions $x(t)$, so that they are as regular as $f$ is WRT $t$? Thank you",,"['real-analysis', 'analysis', 'eigenvalues-eigenvectors', 'implicit-differentiation']"
58,"showing $f(x_1,x_2)=\sqrt[3]{x_1x_2}$ is differentiable",showing  is differentiable,"f(x_1,x_2)=\sqrt[3]{x_1x_2}","Given $f\colon\mathbb R^{>0}\times\mathbb R^{>0}\rightarrow\mathbb R, (x_1,x_2)\mapsto \sqrt[3]{x_1\cdot x_2}$ I want to prove that $f$ is differentiable. I know $f$ is partial differentiable and all partial derivatives are continuous, so $f$ is differentiable. But I am wondering if I could show it by the definition of differentiability. E.g. consider the point $1=(1,1)$. So there has to be a linear map $A$ such  that $\lim_{h\rightarrow0}\frac{f(1+h)-f(1)-Ah}{\|h\|}=0$ Let $h=(h_1,h_2)$. It's $A=(\frac13,\frac13)$ and so $\lim_{h\rightarrow0}\frac{\sqrt[3]{(1+h_1)(1+h_2)}-1-\frac13h_1-\frac13h_2}{\|h\|}=0$ ? I know how to show it using polar coordinates but is there any way by doing it without any big 'help functions/relations' etc.?","Given $f\colon\mathbb R^{>0}\times\mathbb R^{>0}\rightarrow\mathbb R, (x_1,x_2)\mapsto \sqrt[3]{x_1\cdot x_2}$ I want to prove that $f$ is differentiable. I know $f$ is partial differentiable and all partial derivatives are continuous, so $f$ is differentiable. But I am wondering if I could show it by the definition of differentiability. E.g. consider the point $1=(1,1)$. So there has to be a linear map $A$ such  that $\lim_{h\rightarrow0}\frac{f(1+h)-f(1)-Ah}{\|h\|}=0$ Let $h=(h_1,h_2)$. It's $A=(\frac13,\frac13)$ and so $\lim_{h\rightarrow0}\frac{\sqrt[3]{(1+h_1)(1+h_2)}-1-\frac13h_1-\frac13h_2}{\|h\|}=0$ ? I know how to show it using polar coordinates but is there any way by doing it without any big 'help functions/relations' etc.?",,"['calculus', 'real-analysis']"
59,Need help with difficult extra credit analysis question,Need help with difficult extra credit analysis question,,"I don't really understand this problem from my analysis class: Let $BC([0,1],R)$ be the metric space of functions which are bounded, continuous on $[0,1]$, and in $C^\infty$, meaning that all of their derivatives exist and are continuous. We're using the infinity metric on $BC([0,1],R)$. Notice that there's an issue about defining the derivative at $0$ or $1$. We'll take ""differentiable"" to mean that, for every $f$ in $BC([0,1],R)$, there is some $a<0$ and $b>1$ so that $f$ is infinitely differentiable on $(a,b)$. Let $F$ be the set of all $f$ in $BC([0,1],R)$ such that the absolute value of the $K$-th derivative of $f(x)$ is less than or equal to $M$ for all $x$ between $0$ and $1$, inclusive, and also such that $0$ is in the $j$-th derivative of $f([0,1])$ for all $j$ between $0$ and $K$, inclusive. That is, $F$ is the set of all functions whose $K$-th derivative is uniformly bounded by $M$ and whose image includes $0$. Note only the $K$-th derivative is bounded, not the $(K - 1)$-th or anything. Prove that for any fixed $K$ in the natural numbers and $M > 0$, we have that the closure of $F$ is compact. Here the closure is taken in $BC([0,1],R)$. I have been advised to use the Arzel-Ascoli theorem (Take $K$ to be a closed subset of $C[0,1]$.  Then the following are equivalent: $K$ is compact; $K$ is uniformly bounded and equicontinuous). So basically we need to prove that the closure of $F$ is uniformly bounded and equicontinuous.  Can I prove that $F$ is uniformly bounded by saying that the $K$-th derivative is obviously bounded by $M$ for $x$ between $0$ and $1$, so the $(K - 1)$-th derivative must be bounded because it only changes by a finite amount (only achieves at maximum a velocity of $M$) and so must be bounded as well, and then extending this logic all the way to the original $f$? I also don't really know how to go about proving that the function is equicontinuous.  Any help is greatly appreciated!","I don't really understand this problem from my analysis class: Let $BC([0,1],R)$ be the metric space of functions which are bounded, continuous on $[0,1]$, and in $C^\infty$, meaning that all of their derivatives exist and are continuous. We're using the infinity metric on $BC([0,1],R)$. Notice that there's an issue about defining the derivative at $0$ or $1$. We'll take ""differentiable"" to mean that, for every $f$ in $BC([0,1],R)$, there is some $a<0$ and $b>1$ so that $f$ is infinitely differentiable on $(a,b)$. Let $F$ be the set of all $f$ in $BC([0,1],R)$ such that the absolute value of the $K$-th derivative of $f(x)$ is less than or equal to $M$ for all $x$ between $0$ and $1$, inclusive, and also such that $0$ is in the $j$-th derivative of $f([0,1])$ for all $j$ between $0$ and $K$, inclusive. That is, $F$ is the set of all functions whose $K$-th derivative is uniformly bounded by $M$ and whose image includes $0$. Note only the $K$-th derivative is bounded, not the $(K - 1)$-th or anything. Prove that for any fixed $K$ in the natural numbers and $M > 0$, we have that the closure of $F$ is compact. Here the closure is taken in $BC([0,1],R)$. I have been advised to use the Arzel-Ascoli theorem (Take $K$ to be a closed subset of $C[0,1]$.  Then the following are equivalent: $K$ is compact; $K$ is uniformly bounded and equicontinuous). So basically we need to prove that the closure of $F$ is uniformly bounded and equicontinuous.  Can I prove that $F$ is uniformly bounded by saying that the $K$-th derivative is obviously bounded by $M$ for $x$ between $0$ and $1$, so the $(K - 1)$-th derivative must be bounded because it only changes by a finite amount (only achieves at maximum a velocity of $M$) and so must be bounded as well, and then extending this logic all the way to the original $f$? I also don't really know how to go about proving that the function is equicontinuous.  Any help is greatly appreciated!",,['analysis']
60,Show $\{f_n'(x)\}$ is uniformly bounded given that $\{f_n''(x)\}$ is and $f_n \rightarrow 0$ uniformly,Show  is uniformly bounded given that  is and  uniformly,\{f_n'(x)\} \{f_n''(x)\} f_n \rightarrow 0,"Suppose that $f_n : \mathbb{R} \rightarrow \mathbb{R}$, $n=1,2,\dots$ is a sequence of functions with continuous second derivatives. Further, let $f_n \rightarrow 0$ uniformly as $n \rightarrow \infty$ and let $M$ be a constant such that $|f_n '' (x)| \leq M$ for all $n$ and $x \in \mathbb{R}$. Show that there exists a constant $K$ such that $|f_n '(x)| \leq K$ for all $n$ and $x \in \mathbb{R}$. I am stuck on how to relate the above information. I know that given $a,b\in \mathbb{R}$, with $a<b$, we have that for some $c,d\in(a,b)$ $$ f_n(b) = f_n(a) + f_n'(a)(b-a) + \frac{f_n''(c)}{2}(b-a)^2 \leq f_n(a) + f_n'(b-a) + \frac{M}{2}(b-a)^2  $$ and $$ f_n'(b) - f_n'(a) = f''(d)(b-a) \leq M(b-a) $$ But I can't see how to combine this with the uniform convergence of $\{f_n\}$. Edit: After reading Hans' comment below, I think the connection comes from $f_n$ being uniformly bounded. Since $f_n \rightarrow 0$ uniformly, there exists a sequence $L_n \rightarrow 0$ where $$ L_n = \sup_{x} |f_n(x)| $$ Thus, if we let $L$ be the LUB of the $L_n$, we can combine this with Hans' trick below to get $$ |f_n'(x)| \leq |f_n(x+1) - f_n(x)| + M/2 \leq 2L + M/2 $$","Suppose that $f_n : \mathbb{R} \rightarrow \mathbb{R}$, $n=1,2,\dots$ is a sequence of functions with continuous second derivatives. Further, let $f_n \rightarrow 0$ uniformly as $n \rightarrow \infty$ and let $M$ be a constant such that $|f_n '' (x)| \leq M$ for all $n$ and $x \in \mathbb{R}$. Show that there exists a constant $K$ such that $|f_n '(x)| \leq K$ for all $n$ and $x \in \mathbb{R}$. I am stuck on how to relate the above information. I know that given $a,b\in \mathbb{R}$, with $a<b$, we have that for some $c,d\in(a,b)$ $$ f_n(b) = f_n(a) + f_n'(a)(b-a) + \frac{f_n''(c)}{2}(b-a)^2 \leq f_n(a) + f_n'(b-a) + \frac{M}{2}(b-a)^2  $$ and $$ f_n'(b) - f_n'(a) = f''(d)(b-a) \leq M(b-a) $$ But I can't see how to combine this with the uniform convergence of $\{f_n\}$. Edit: After reading Hans' comment below, I think the connection comes from $f_n$ being uniformly bounded. Since $f_n \rightarrow 0$ uniformly, there exists a sequence $L_n \rightarrow 0$ where $$ L_n = \sup_{x} |f_n(x)| $$ Thus, if we let $L$ be the LUB of the $L_n$, we can combine this with Hans' trick below to get $$ |f_n'(x)| \leq |f_n(x+1) - f_n(x)| + M/2 \leq 2L + M/2 $$",,"['real-analysis', 'analysis']"
61,continuous function on a set of lines through a point,continuous function on a set of lines through a point,,"This came up in class once: Suppose we have a set $R \subset \mathbb{R}^2$ and $x \in  \mathbb{R}^2$. If we define a function $A$ on all the   lines $l$ going through $x$ such that $A(l)$ gives how much of the area   of $R$ is ""above"" (or to the right left of) the line $l$, must   $A$ be continuous? I do think it is continuous. If we take a horizontal line through $x$ and rotate it by $\theta$, we have a continuous function giving the area of the part of $R$ whose pre-image (under this rotation) is the part of $R$ initially under the horizontal line. However, I'm at loss how to rigorously show continuity using pre-image of open sets is an open set. In particular, how should an open set be defined on the set of lines through $x$, and thus allowing me to rework a proof. EDIT: What we mean by the ``above'': For a non-vertical line $l$ passing through $x$, we rotate $S$ clockwise until $l$ becomes horizontal. Then $A(l)$ gives the area of $R$ which is above this horizontal line (the image of $l$ after the rotation). If $l$ were vertical, we take the area to the left of $l$,","This came up in class once: Suppose we have a set $R \subset \mathbb{R}^2$ and $x \in  \mathbb{R}^2$. If we define a function $A$ on all the   lines $l$ going through $x$ such that $A(l)$ gives how much of the area   of $R$ is ""above"" (or to the right left of) the line $l$, must   $A$ be continuous? I do think it is continuous. If we take a horizontal line through $x$ and rotate it by $\theta$, we have a continuous function giving the area of the part of $R$ whose pre-image (under this rotation) is the part of $R$ initially under the horizontal line. However, I'm at loss how to rigorously show continuity using pre-image of open sets is an open set. In particular, how should an open set be defined on the set of lines through $x$, and thus allowing me to rework a proof. EDIT: What we mean by the ``above'': For a non-vertical line $l$ passing through $x$, we rotate $S$ clockwise until $l$ becomes horizontal. Then $A(l)$ gives the area of $R$ which is above this horizontal line (the image of $l$ after the rotation). If $l$ were vertical, we take the area to the left of $l$,",,"['real-analysis', 'analysis', 'continuity']"
62,Tails of Fourier Transformed family of functions,Tails of Fourier Transformed family of functions,,"I am reading a thesis where on page 39, Definition 4, $\epsilon$-oscillatory is defined as a property for a family of functions $\{f_{\epsilon}\}_{0<\epsilon<1}$ in $L^2(\mathbb{R}^d)$ to have if $$\lim_{R\to\infty}\limsup_{\epsilon\to 0}\int_{\{\left|\xi\right|>R\}}\frac{1}{\epsilon^d}\left|\widehat{f_{\epsilon}}\left(\frac{\xi}{\epsilon}\right)\right|^2d\xi=0.$$ It is mentioned soon after that for this property to hold it suffices that the weak derivative $\{\epsilon\nabla f_{\epsilon}\}$ is bounded in $L^2$. I cannot connect the two. The most I could do was the following $$\sup_{\epsilon}||\epsilon\nabla f_{\epsilon}||_{L^2}<C \implies \sup_{\epsilon}||\epsilon\widehat{\nabla f_{\epsilon}}||_{L^2}<C\implies\sup_{\epsilon}||\epsilon\xi \widehat{f_{\epsilon}}||_{L^2}<C$$ $$\implies \sup_{\epsilon}\int_{\mathbb{R}^d}\frac{1}{\epsilon^d}\left|p\widehat{f_{\epsilon}}\left(\frac{p}{\epsilon}\right)\right|^2dp<C$$ after a change of variables $\epsilon\xi=p$. Could someone please help me in figuring out the rest?","I am reading a thesis where on page 39, Definition 4, $\epsilon$-oscillatory is defined as a property for a family of functions $\{f_{\epsilon}\}_{0<\epsilon<1}$ in $L^2(\mathbb{R}^d)$ to have if $$\lim_{R\to\infty}\limsup_{\epsilon\to 0}\int_{\{\left|\xi\right|>R\}}\frac{1}{\epsilon^d}\left|\widehat{f_{\epsilon}}\left(\frac{\xi}{\epsilon}\right)\right|^2d\xi=0.$$ It is mentioned soon after that for this property to hold it suffices that the weak derivative $\{\epsilon\nabla f_{\epsilon}\}$ is bounded in $L^2$. I cannot connect the two. The most I could do was the following $$\sup_{\epsilon}||\epsilon\nabla f_{\epsilon}||_{L^2}<C \implies \sup_{\epsilon}||\epsilon\widehat{\nabla f_{\epsilon}}||_{L^2}<C\implies\sup_{\epsilon}||\epsilon\xi \widehat{f_{\epsilon}}||_{L^2}<C$$ $$\implies \sup_{\epsilon}\int_{\mathbb{R}^d}\frac{1}{\epsilon^d}\left|p\widehat{f_{\epsilon}}\left(\frac{p}{\epsilon}\right)\right|^2dp<C$$ after a change of variables $\epsilon\xi=p$. Could someone please help me in figuring out the rest?",,"['analysis', 'functional-analysis']"
63,Proof of a theorem with upper/lower limits.,Proof of a theorem with upper/lower limits.,,"Theorem: If $s_n \le t_n$ for all $n$ greater than a fixed integer $N$, then $$\lim_{n \to \infty} \inf s_n \le \lim_{n \to \infty} \inf t_n$$ I would like to prove this and it would be nice if someone could check my work. Proof: Letting $$\lim_{n \to \infty} \inf s_n = s_*$$ and $$\lim_{n \to \infty} \inf t_n = t_*$$ assume that $s_* > t_* $ while there exists $N$ such that for all $n$ >$N$, $s_n \le t_n$.   Then there must be an integer $M$(particularly larger than $N$) such that if $m > M$, $t_m < s_*$. This contradicts the fact that for all $n > N$, $s_n \le t_n$.","Theorem: If $s_n \le t_n$ for all $n$ greater than a fixed integer $N$, then $$\lim_{n \to \infty} \inf s_n \le \lim_{n \to \infty} \inf t_n$$ I would like to prove this and it would be nice if someone could check my work. Proof: Letting $$\lim_{n \to \infty} \inf s_n = s_*$$ and $$\lim_{n \to \infty} \inf t_n = t_*$$ assume that $s_* > t_* $ while there exists $N$ such that for all $n$ >$N$, $s_n \le t_n$.   Then there must be an integer $M$(particularly larger than $N$) such that if $m > M$, $t_m < s_*$. This contradicts the fact that for all $n > N$, $s_n \le t_n$.",,['analysis']
64,Modified Arithmetic-Geometric Mean,Modified Arithmetic-Geometric Mean,,"Let $\{x_n\}$ and $\{y_n\}$ be defined iteratively, $x_0:=\beta >1, \ y_0:= 1$ and $x_{n+1}= \frac{x_n+y_n}{2}$, $y_{n+1} = (x_n.y_n)^{\frac{1}{2}}$; i.e. they are respectively the arithmetic and geometric mean of the previous terms. We know that their limit is called the arithmetic-geometric mean of $\beta$ and $1$ (denoted by $AGM(\beta,1)$). Now, let's define $\xi_0:= \beta^2$, $\eta_0:= 1$ and $\zeta_0:=0$. Then let's define iteratively, $\xi_{n+1}:= \frac{\xi_n + \eta_n}{2}$, $\eta_{n+1}:= \zeta_n + ((\xi_n-\zeta_n)(\eta_n-\zeta_n))^{\frac{1}{2}}$ and finally $\zeta_{n+1}:= \zeta_n - ((\xi_n-\zeta_n)(\eta_n-\zeta_n))^{\frac{1}{2}}$. The common limit of $\xi_n$ and $\eta_n$ is called the modified arithmetic-geometric mean of $\beta^2$ and $1$ (denoted by $MAGM(\beta^2, 1)$). My question is wheter there is an easy way to prove the equality $\xi_n = \beta^2-\sum_{m=0}^{n-1}2^m\frac{x_m^2-y_m^2}{2}$ and if there is, how? Thank you very for any help. Note: This is from an article in Notices of the AMS, Volume 59, Number 8.","Let $\{x_n\}$ and $\{y_n\}$ be defined iteratively, $x_0:=\beta >1, \ y_0:= 1$ and $x_{n+1}= \frac{x_n+y_n}{2}$, $y_{n+1} = (x_n.y_n)^{\frac{1}{2}}$; i.e. they are respectively the arithmetic and geometric mean of the previous terms. We know that their limit is called the arithmetic-geometric mean of $\beta$ and $1$ (denoted by $AGM(\beta,1)$). Now, let's define $\xi_0:= \beta^2$, $\eta_0:= 1$ and $\zeta_0:=0$. Then let's define iteratively, $\xi_{n+1}:= \frac{\xi_n + \eta_n}{2}$, $\eta_{n+1}:= \zeta_n + ((\xi_n-\zeta_n)(\eta_n-\zeta_n))^{\frac{1}{2}}$ and finally $\zeta_{n+1}:= \zeta_n - ((\xi_n-\zeta_n)(\eta_n-\zeta_n))^{\frac{1}{2}}$. The common limit of $\xi_n$ and $\eta_n$ is called the modified arithmetic-geometric mean of $\beta^2$ and $1$ (denoted by $MAGM(\beta^2, 1)$). My question is wheter there is an easy way to prove the equality $\xi_n = \beta^2-\sum_{m=0}^{n-1}2^m\frac{x_m^2-y_m^2}{2}$ and if there is, how? Thank you very for any help. Note: This is from an article in Notices of the AMS, Volume 59, Number 8.",,"['analysis', 'number-theory']"
65,"Differentiability of function $f(x,y) = |x|^a + |x-y|$.",Differentiability of function .,"f(x,y) = |x|^a + |x-y|","I am trying to figure out the points at which the function $f(x,y) = |x|^a + |x-y|$ is differentiable. Could you please help me out. I have considered the cases x>0, y>0 etc, but am having difficulty working out what the conditions on x,y have to be such that this function is not differentiable. Many thanks.","I am trying to figure out the points at which the function $f(x,y) = |x|^a + |x-y|$ is differentiable. Could you please help me out. I have considered the cases x>0, y>0 etc, but am having difficulty working out what the conditions on x,y have to be such that this function is not differentiable. Many thanks.",,"['real-analysis', 'analysis']"
66,A robust convex optimization problem,A robust convex optimization problem,,"Consider a function $ f: \mathbb{R}^n \times \mathbb{R}^m \rightarrow \mathbb{R}  $ such that $\forall x \in \mathbb{R}^n$ the map $f(x,\cdot)$ is convex, and $\forall y\in \mathbb{R}^m$ the map $f(\cdot,y)$ is convex as well. We assume that $m<n$. Given $y^1, ..., y^N \in \mathbb{R}^m$, and $c \in \mathbb{R}^n$, consider the following convex optimization problem. $$ \min_{ x \in \mathbb{R}^n } c^\top x \ \ \ \text{ sub. to: } \ f(x,y^i) \leq 0 \ \ \forall i \in [1,N] $$ Let $x^\star$ be its optimizer, supposed to be unique. Notice that the above optimization problem is equivalent to the following one. $$ \min_{ x \in \mathbb{R}^n } c^\top x \ \ \ \text{ sub. to: } \ f(x,y) \leq 0 \ \ \forall y \in \text{conv}( \{y^1, ..., y^N\} ) $$ We now say that $f(x,y^k) \leq 0$ is a ""true"" constraint if it ""really matters"", i.e. any optimizer of the problem $$ \min_{ x \in \mathbb{R}^n } c^\top x \ \ \ \text{ sub. to: } \ f(x,y^i) \leq 0 \ \ \forall i \in ([1,N]\setminus\{k\}), $$ which we denote by $x_{ \neg k }^\star$, is such that $c^\top x_{ \neg k }^\star < c^\top x^\star$. What is the number of true constraints?","Consider a function $ f: \mathbb{R}^n \times \mathbb{R}^m \rightarrow \mathbb{R}  $ such that $\forall x \in \mathbb{R}^n$ the map $f(x,\cdot)$ is convex, and $\forall y\in \mathbb{R}^m$ the map $f(\cdot,y)$ is convex as well. We assume that $m<n$. Given $y^1, ..., y^N \in \mathbb{R}^m$, and $c \in \mathbb{R}^n$, consider the following convex optimization problem. $$ \min_{ x \in \mathbb{R}^n } c^\top x \ \ \ \text{ sub. to: } \ f(x,y^i) \leq 0 \ \ \forall i \in [1,N] $$ Let $x^\star$ be its optimizer, supposed to be unique. Notice that the above optimization problem is equivalent to the following one. $$ \min_{ x \in \mathbb{R}^n } c^\top x \ \ \ \text{ sub. to: } \ f(x,y) \leq 0 \ \ \forall y \in \text{conv}( \{y^1, ..., y^N\} ) $$ We now say that $f(x,y^k) \leq 0$ is a ""true"" constraint if it ""really matters"", i.e. any optimizer of the problem $$ \min_{ x \in \mathbb{R}^n } c^\top x \ \ \ \text{ sub. to: } \ f(x,y^i) \leq 0 \ \ \forall i \in ([1,N]\setminus\{k\}), $$ which we denote by $x_{ \neg k }^\star$, is such that $c^\top x_{ \neg k }^\star < c^\top x^\star$. What is the number of true constraints?",,"['real-analysis', 'analysis', 'optimization', 'convex-analysis', 'convex-optimization']"
67,"assume $f:[a,b] \to \mathbb R$ such that $f '(a)=f '(b)$ how prove $\exists t\in(a,b) : f(t)-f(a)=f '(t)(t-a)$?",assume  such that  how prove ?,"f:[a,b] \to \mathbb R f '(a)=f '(b) \exists t\in(a,b) : f(t)-f(a)=f '(t)(t-a)","let $f:[a,b] \to \mathbb R$ such that $f '(a)=f '(b)$ how prove $$\exists t\in(a,b) : f(t)-f(a)=f '(t)(t-a)$$ Thanks in advance","let $f:[a,b] \to \mathbb R$ such that $f '(a)=f '(b)$ how prove $$\exists t\in(a,b) : f(t)-f(a)=f '(t)(t-a)$$ Thanks in advance",,"['analysis', 'contest-math']"
68,"Prove for continuous $f$ and $g$,$f(x)<g(x)$ there exists $k$ such that $f(x)+k<g(x)$","Prove for continuous  and , there exists  such that",f g f(x)<g(x) k f(x)+k<g(x),"Suppose that $f$ and $g$ are continuous on $[a,b]$ and for each $x$, it holds that $f(x)<g(x)$. Prove that there exists $\alpha>0$ such that for each $x$, it holds that $f(x) + \alpha <g(x)$ (Notice that $\alpha$ may not depend on $x$). We have done a similar problem with $f(x)>0$ and proving $\alpha>0$ exists such that $f(x)>\alpha$, but I'm not exactly sure how to use supremum and infimum to solve the above version.","Suppose that $f$ and $g$ are continuous on $[a,b]$ and for each $x$, it holds that $f(x)<g(x)$. Prove that there exists $\alpha>0$ such that for each $x$, it holds that $f(x) + \alpha <g(x)$ (Notice that $\alpha$ may not depend on $x$). We have done a similar problem with $f(x)>0$ and proving $\alpha>0$ exists such that $f(x)>\alpha$, but I'm not exactly sure how to use supremum and infimum to solve the above version.",,"['calculus', 'analysis']"
69,$f(x) = e^{-x^2}$ series representation,series representation,f(x) = e^{-x^2},"Let $f(x) = e^{-x^2}$, defined for $x \in \mathbb{R}$. Find a series representation of a function $F:\mathbb{R} \to \mathbb{R}$ such that $F(0)=0$ and $F'(x)=f(x)$ for each $x$. I know that the answer to this is in some form of a Taylor series, but I don't even know where to start beyond that...","Let $f(x) = e^{-x^2}$, defined for $x \in \mathbb{R}$. Find a series representation of a function $F:\mathbb{R} \to \mathbb{R}$ such that $F(0)=0$ and $F'(x)=f(x)$ for each $x$. I know that the answer to this is in some form of a Taylor series, but I don't even know where to start beyond that...",,"['calculus', 'analysis']"
70,Question about theorem 3.2 from Morse theory by Milnor,Question about theorem 3.2 from Morse theory by Milnor,,"The demonstration of the theorem 3.2 in the book Morse theory by Milnor THEOREM $\mathbf{3.2.}$ Let $f:M\to\bf R$ be a smooth function, and let $p$ be a non-degenerate critical point with index $\lambda$. Setting $f(p)=c$; suppose that $f^{-1}[c-\epsilon,c+\epsilon]$ is compact, and contains no critical point of $f$ other then $p$, for some $\epsilon\gt0$. Then, for all sufficiently small $\epsilon$, the set $M^{C+\epsilon}$ has the homotopy type of $M^{C-\epsilon}$ with a $\lambda$-cell attached. is given in the special case whene the manifold is the Torus , My question is : can i prove it in the case where the manifold is a  manifold with dimension  1 ? this is a part of proof , in dimension 1 we chose only $(u^1)$ as a coordinate systeme, the index of the critical point is 0 or 1 ,so $f=c(u^1)^2$ my first probleme is when i must define $e^{\lambda}$ ,I can't say that is the set of points in $U$ with : $u^1)^2\leq \varepsilon$ ,and $u^1 = 0$ ! so how to define $e^{\lambda}$ and in the book ,they applicated this to the torus , how to applicated it on a manifold with dimension 1 ? Please Thank you @Yvoz","The demonstration of the theorem 3.2 in the book Morse theory by Milnor THEOREM $\mathbf{3.2.}$ Let $f:M\to\bf R$ be a smooth function, and let $p$ be a non-degenerate critical point with index $\lambda$. Setting $f(p)=c$; suppose that $f^{-1}[c-\epsilon,c+\epsilon]$ is compact, and contains no critical point of $f$ other then $p$, for some $\epsilon\gt0$. Then, for all sufficiently small $\epsilon$, the set $M^{C+\epsilon}$ has the homotopy type of $M^{C-\epsilon}$ with a $\lambda$-cell attached. is given in the special case whene the manifold is the Torus , My question is : can i prove it in the case where the manifold is a  manifold with dimension  1 ? this is a part of proof , in dimension 1 we chose only $(u^1)$ as a coordinate systeme, the index of the critical point is 0 or 1 ,so $f=c(u^1)^2$ my first probleme is when i must define $e^{\lambda}$ ,I can't say that is the set of points in $U$ with : $u^1)^2\leq \varepsilon$ ,and $u^1 = 0$ ! so how to define $e^{\lambda}$ and in the book ,they applicated this to the torus , how to applicated it on a manifold with dimension 1 ? Please Thank you @Yvoz",,"['analysis', 'functional-analysis', 'manifolds', 'morse-theory']"
71,Probability of failure of a campaign,Probability of failure of a campaign,,"This is a simplistic question. It is about the issue of running a campaign in a newspaper. There are two issues here: (i) The chances of failure vs success of a well-planned, well-written, and well-excuted campaign page (in terms of leaving an impact on a reader) is 25 vs 75. (ii) The chances that a page (any page, not necessarily campaign) littered with ads will put off the reader is 90 vs 10. So, what are the chances of a failure if a campaign page is littered with ads? (Ceteris paribus; all other things remaining constant) Thanks. Subir","This is a simplistic question. It is about the issue of running a campaign in a newspaper. There are two issues here: (i) The chances of failure vs success of a well-planned, well-written, and well-excuted campaign page (in terms of leaving an impact on a reader) is 25 vs 75. (ii) The chances that a page (any page, not necessarily campaign) littered with ads will put off the reader is 90 vs 10. So, what are the chances of a failure if a campaign page is littered with ads? (Ceteris paribus; all other things remaining constant) Thanks. Subir",,['analysis']
72,Metric spaces problem,Metric spaces problem,,"In notes I have come across, it is stated that for a Hausdorff space induced by countably many semi-norms $p_n$ that, $$d(x,y) = \sum_{n=1}^{\infty}2^{-n}\frac{p_n(x-y)}{1+p_n(x-y)}$$ is a metric. But I can't see why this metric satisfies the triangle inequality. As I believe it satisfies the triangle inequality iff $p_n(x-y)(1+p_n(x-z)p_n(z-y)) \leq p_n(x-z) + p_n(z-y)$ If you shed some light on this, it would be much appreciated!","In notes I have come across, it is stated that for a Hausdorff space induced by countably many semi-norms $p_n$ that, $$d(x,y) = \sum_{n=1}^{\infty}2^{-n}\frac{p_n(x-y)}{1+p_n(x-y)}$$ is a metric. But I can't see why this metric satisfies the triangle inequality. As I believe it satisfies the triangle inequality iff $p_n(x-y)(1+p_n(x-z)p_n(z-y)) \leq p_n(x-z) + p_n(z-y)$ If you shed some light on this, it would be much appreciated!",,['real-analysis']
73,How to recover a measure from its Fourier transform?,How to recover a measure from its Fourier transform?,,"Let $f$ be the complex function defined on $\mathbb{R}$ by  $$ f(t)=\frac{1-it}{1+it}. $$ 1) Does there exist a complex bounded measure $\mu \in M(\mathbb{R})$ such that $\hat{\mu}=f$ (where $\hat{}$ denotes the Fourier transform)? 2) If the answer is positive, do we have $||\mu||\leq 1$?","Let $f$ be the complex function defined on $\mathbb{R}$ by  $$ f(t)=\frac{1-it}{1+it}. $$ 1) Does there exist a complex bounded measure $\mu \in M(\mathbb{R})$ such that $\hat{\mu}=f$ (where $\hat{}$ denotes the Fourier transform)? 2) If the answer is positive, do we have $||\mu||\leq 1$?",,"['analysis', 'measure-theory', 'integration', 'fourier-analysis']"
74,"Set of Continuous Functions, Functionals, and Equicontinuity","Set of Continuous Functions, Functionals, and Equicontinuity",,"Define the subset of $C^0[0,1]$ to be: $P = \{F(x) = \int_0^x f(t) \, dt : f \in C^0[0,1], \|f\|_\infty \le 1\}$ 1) Show that $P$ is not closed. 2) Show that $P$ is bounded and equicontinuous (using the infinity norm) 3) Show that the functional $J: C^0[0,1] \to R$ given by $J(F) = \int_0^1 F(x)\, dx$ achieves its max value on $P$. Thanks in advance for your help and explanations. Thus far I have been working on the first two parts and have an intuition for why they are true, but I am struggling to construct a formal proof. I see that the functions are all differentiable and have been thinking about pointwise convergence. For part 2, I have been considering using the mean value theorem. Any help would be greatly appreciated.","Define the subset of $C^0[0,1]$ to be: $P = \{F(x) = \int_0^x f(t) \, dt : f \in C^0[0,1], \|f\|_\infty \le 1\}$ 1) Show that $P$ is not closed. 2) Show that $P$ is bounded and equicontinuous (using the infinity norm) 3) Show that the functional $J: C^0[0,1] \to R$ given by $J(F) = \int_0^1 F(x)\, dx$ achieves its max value on $P$. Thanks in advance for your help and explanations. Thus far I have been working on the first two parts and have an intuition for why they are true, but I am struggling to construct a formal proof. I see that the functions are all differentiable and have been thinking about pointwise convergence. For part 2, I have been considering using the mean value theorem. Any help would be greatly appreciated.",,"['general-topology', 'analysis', 'functional-analysis']"
75,Does uniform convergence imply,Does uniform convergence imply,,"Let $U$ be a bounded open set in $\mathbb{R}^n$. Consider $C(U)$, the space of the continuous functions on $U$. I am confused with ""uniform convergence"" and ""$L^{\infty}$-convergence"" on $C(U)$. It seems to me that the two convergences are equivalent. If I am wrong, is one of them stronger than the other? Thank you in advance.","Let $U$ be a bounded open set in $\mathbb{R}^n$. Consider $C(U)$, the space of the continuous functions on $U$. I am confused with ""uniform convergence"" and ""$L^{\infty}$-convergence"" on $C(U)$. It seems to me that the two convergences are equivalent. If I am wrong, is one of them stronger than the other? Thank you in advance.",,"['real-analysis', 'analysis', 'convergence-divergence', 'uniform-convergence']"
76,A statement equivalent to the definition of limits at infinity?,A statement equivalent to the definition of limits at infinity?,,I was fiddling around with the definition of limits at infinity and believe I have found a statement that is equivalent to the definition. So the question is this: are the following two statements equivalent? (1) $\lim_{x\rightarrow\infty}f\left(x\right)=L$ (2) $\exists c>0\exists M>0\left(\sup\left\{ \left|x\left(f\left(x\right)-L\right)\right|:x\geq c\right\} \leq M\right)$,I was fiddling around with the definition of limits at infinity and believe I have found a statement that is equivalent to the definition. So the question is this: are the following two statements equivalent? (1) $\lim_{x\rightarrow\infty}f\left(x\right)=L$ (2) $\exists c>0\exists M>0\left(\sup\left\{ \left|x\left(f\left(x\right)-L\right)\right|:x\geq c\right\} \leq M\right)$,,['analysis']
77,Find adjoint operator,Find adjoint operator,,"Let $\mathcal{H}$ be a separable Hilbert space with orthonormal basis $(e_n)$. Denote by $V$ the subspace of finite linear combinations of the basis-vectors. Define $T$ on $\mathcal{H}$ with $D(T) = V$ by $$T\left(\sum_{j=1}^n c_j e_j\right) = \sum_{j=1}^n c_j e_1$$ I want to find the adjoint operator $T^*$ and the closure of $G(T)$ the graph. My problem is this: So for the first part. $\mathcal{H}^* = B(\mathcal{H}, \mathbb{C})$, $$D(T^*) =  \left\{y^*\in \mathcal{H}^*: |y^*(Tx)|\leq C_{y^*}\left\|x\right\|_{\mathcal{H}}, x\in V  \right\}$$ (This is what i got from my definitions: So $D(T^*)$ consists of functionals for which the map $x\mapsto y^*(Tx)$ is continuous for all $x\in V$) The adjoint transformation is defined by $$T^*y^* = x^* $$ for which $y^*(Tx)= x^*(x)$ for all $x\in V$. So then I first need to determine which functionals $y^*$ are in $D(T^*)$ right? But i havent been given a norm $\left\|\cdot\right\|_{\mathcal{H}}$...could someone help me out?","Let $\mathcal{H}$ be a separable Hilbert space with orthonormal basis $(e_n)$. Denote by $V$ the subspace of finite linear combinations of the basis-vectors. Define $T$ on $\mathcal{H}$ with $D(T) = V$ by $$T\left(\sum_{j=1}^n c_j e_j\right) = \sum_{j=1}^n c_j e_1$$ I want to find the adjoint operator $T^*$ and the closure of $G(T)$ the graph. My problem is this: So for the first part. $\mathcal{H}^* = B(\mathcal{H}, \mathbb{C})$, $$D(T^*) =  \left\{y^*\in \mathcal{H}^*: |y^*(Tx)|\leq C_{y^*}\left\|x\right\|_{\mathcal{H}}, x\in V  \right\}$$ (This is what i got from my definitions: So $D(T^*)$ consists of functionals for which the map $x\mapsto y^*(Tx)$ is continuous for all $x\in V$) The adjoint transformation is defined by $$T^*y^* = x^* $$ for which $y^*(Tx)= x^*(x)$ for all $x\in V$. So then I first need to determine which functionals $y^*$ are in $D(T^*)$ right? But i havent been given a norm $\left\|\cdot\right\|_{\mathcal{H}}$...could someone help me out?",,"['real-analysis', 'analysis', 'functional-analysis']"
78,Measurable Maps,Measurable Maps,,"Let $X$, $M$ be two metric spaces and $\nu:X\rightarrow \mathcal{M}_1(M)$, $x\mapsto \nu_x$ a map, where $\mathcal{M}_1(M)$ is the space of all probabilities over $M$ with the Boral $\sigma$-algebra, $\mathcal{M}_1(M)$ with the weak* topology. The following statements are equivalent $\nu$ is measurable with respsect to the Borel $\sigma$-algebras of $X$ and $\mathcal{M}_1(M)$; The map $X\rightarrow \mathbb{R}$, $x\mapsto \int \varphi d\nu_x$ is measurable, for all continuous bounded map $\varphi:M\rightarrow \mathbb{R}$; The map $X\rightarrow \mathbb{R}$, $x\mapsto \int \psi d\nu_x$ is measurable, for all measurable bounded map $\psi>M\rightarrow \mathbb{R}$; The map $X\rightarrow \mathbb{R}$, $x\mapsto \nu_x(E)$ is measurable, for all $E\subset M$ measurable. I managed to prove some of them: $(2)\Rightarrow(1)$, $(4)\Rightarrow(3)$ and $(3)\Rightarrow(2)$. I was trying to prove $(2)\Rightarrow(1)$ and $(2)\Rightarrow(4)$ because I think it's easier! Here are my ideas: $(2)\Rightarrow (4):$ Given a measurable set $E\subset M$ and its characteristic map $\chi_E$, the natural approach is to approximate $\chi_E$ by a sequence of continuous maps. The problem is that it converges a.e. and I can't determine for which measure! I just just don't know how to solve this problem! $(1)\Rightarrow (2):$ For this one I'm kind of lost! I tried to prove that the map $\mathcal{M}_1(M)\rightarrow \mathbb{R}$, $\mu\mapsto \int \varphi d\mu$, for a giver $\varphi:M\rightarrow\mathbb{R}$ continuous bounded map, is measurable but I couldn't manage to do that! (or at least measurable in $\operatorname{Im}(\nu)\cap \mathcal{M}_1(M)$) Do you guys have any kind of idea? Any hint? Thank you so much in advance, guys!","Let $X$, $M$ be two metric spaces and $\nu:X\rightarrow \mathcal{M}_1(M)$, $x\mapsto \nu_x$ a map, where $\mathcal{M}_1(M)$ is the space of all probabilities over $M$ with the Boral $\sigma$-algebra, $\mathcal{M}_1(M)$ with the weak* topology. The following statements are equivalent $\nu$ is measurable with respsect to the Borel $\sigma$-algebras of $X$ and $\mathcal{M}_1(M)$; The map $X\rightarrow \mathbb{R}$, $x\mapsto \int \varphi d\nu_x$ is measurable, for all continuous bounded map $\varphi:M\rightarrow \mathbb{R}$; The map $X\rightarrow \mathbb{R}$, $x\mapsto \int \psi d\nu_x$ is measurable, for all measurable bounded map $\psi>M\rightarrow \mathbb{R}$; The map $X\rightarrow \mathbb{R}$, $x\mapsto \nu_x(E)$ is measurable, for all $E\subset M$ measurable. I managed to prove some of them: $(2)\Rightarrow(1)$, $(4)\Rightarrow(3)$ and $(3)\Rightarrow(2)$. I was trying to prove $(2)\Rightarrow(1)$ and $(2)\Rightarrow(4)$ because I think it's easier! Here are my ideas: $(2)\Rightarrow (4):$ Given a measurable set $E\subset M$ and its characteristic map $\chi_E$, the natural approach is to approximate $\chi_E$ by a sequence of continuous maps. The problem is that it converges a.e. and I can't determine for which measure! I just just don't know how to solve this problem! $(1)\Rightarrow (2):$ For this one I'm kind of lost! I tried to prove that the map $\mathcal{M}_1(M)\rightarrow \mathbb{R}$, $\mu\mapsto \int \varphi d\mu$, for a giver $\varphi:M\rightarrow\mathbb{R}$ continuous bounded map, is measurable but I couldn't manage to do that! (or at least measurable in $\operatorname{Im}(\nu)\cap \mathcal{M}_1(M)$) Do you guys have any kind of idea? Any hint? Thank you so much in advance, guys!",,"['probability', 'analysis', 'measure-theory']"
79,for all $a>0$ find supremum of $f(x)=a^{-x} + a^{\frac{-1}{x}} \in (0;+\infty)$,for all  find supremum of,a>0 f(x)=a^{-x} + a^{\frac{-1}{x}} \in (0;+\infty),"As in topic, this quest is marked as a rather difficult. All I suppose is that it has something to do with maybe defining a helpful function like $$g(x):= \ln {f(x)}$$ and then doing something with $g'(x)$, but I am not quite sure why do I have to act that way. Could anyone give a hint? Thank you for your time.","As in topic, this quest is marked as a rather difficult. All I suppose is that it has something to do with maybe defining a helpful function like $$g(x):= \ln {f(x)}$$ and then doing something with $g'(x)$, but I am not quite sure why do I have to act that way. Could anyone give a hint? Thank you for your time.",,"['calculus', 'analysis']"
80,$f(n)=2f(\lfloor{n\over2}\rfloor)+1$ . Prove that $f(n)=O(n)$.,. Prove that .,f(n)=2f(\lfloor{n\over2}\rfloor)+1 f(n)=O(n),"$f(n)=2f(\lfloor{n\over2}\rfloor)+1$ where $n$ is positive integer and $f:Z^+\to Z^+$. Prove that $f(n)=O(n)$. Attempts: I have figured the case that for $n=2^k$, $f(n)=2n-1$ which can be obtainly by simple recurrance. But $n$ is any positive integer, i think only the case $n=2^k$ can't really help to the prove. Any hints or solution are welcome.","$f(n)=2f(\lfloor{n\over2}\rfloor)+1$ where $n$ is positive integer and $f:Z^+\to Z^+$. Prove that $f(n)=O(n)$. Attempts: I have figured the case that for $n=2^k$, $f(n)=2n-1$ which can be obtainly by simple recurrance. But $n$ is any positive integer, i think only the case $n=2^k$ can't really help to the prove. Any hints or solution are welcome.",,"['real-analysis', 'analysis', 'functions']"
81,Showing Solution to Some Random PDE Tends to Zero Uniformly,Showing Solution to Some Random PDE Tends to Zero Uniformly,,"Another qual problem that is causing me some difficulty... Consider the PDE $$ \left\{\begin{array}{rl} u_{xxt}+u_{xx}-u^{3}=0&\text{in}\;[0,1]\times(0,\infty)\\ u(0,t)=u(1,t)&\text{on}\{x=0\}\times(0,\infty)\bigcup\{x=1\}\times(0,\infty)\\ u(x,0)=g(x).\end{array}\right. $$ Take $g(x)=x(x-1)$ and show that solutions tend to zero uniformly in $t$ as $t\infty$. To solve the problem I used the usual energy argument, multiplying the PDE by $u$ and integrating both sides in space (applying to the periodic boundary conditions to eliminate boundary terms from the integration by parts) in order to easily obtain $$\frac{de}{dt}(t):=\frac{d}{dt}\int\limits_{0}^{1}|u_{x}(x,t)|^{2}\;dx=-2e(t)-2\int\limits_{0}^{1}|u(x,t)|^{4}\;dx\leq0$$ so that the energy decreases.  Then, using the initial condition, I also have $$0\leq e(t)\leq e(0)=\frac{1}{3}$$ for all $t>0$.  So I have a fairly sharp bound and the fact that $e(t)$ is decreasing in $t$.  If I could show further that $e(t)$ actually tends to $0$ uniformly, then that would imply $u_{x}(x,t)\to0\;\text{as}\;t\to\infty$ uniformly and that would give me $u(x,t)\to\;\text{C}\;\text{as}\;t\to\infty$.  Applying the periodic boundary conditions once more would then give me $C=0$ as required. However, I do not know how to show $e(t)\to0$ as $t\to\infty$.  I've tried computing additional derivatives of $e$ to see if I could glean any additional information on the decay of $e$, but the computations did not seem to lead to anything fruitful (even after playing around/substituting terms using the PDE). Any hints are appreciated (a fully fleshed answer is not needed).  Thanks!","Another qual problem that is causing me some difficulty... Consider the PDE $$ \left\{\begin{array}{rl} u_{xxt}+u_{xx}-u^{3}=0&\text{in}\;[0,1]\times(0,\infty)\\ u(0,t)=u(1,t)&\text{on}\{x=0\}\times(0,\infty)\bigcup\{x=1\}\times(0,\infty)\\ u(x,0)=g(x).\end{array}\right. $$ Take $g(x)=x(x-1)$ and show that solutions tend to zero uniformly in $t$ as $t\infty$. To solve the problem I used the usual energy argument, multiplying the PDE by $u$ and integrating both sides in space (applying to the periodic boundary conditions to eliminate boundary terms from the integration by parts) in order to easily obtain $$\frac{de}{dt}(t):=\frac{d}{dt}\int\limits_{0}^{1}|u_{x}(x,t)|^{2}\;dx=-2e(t)-2\int\limits_{0}^{1}|u(x,t)|^{4}\;dx\leq0$$ so that the energy decreases.  Then, using the initial condition, I also have $$0\leq e(t)\leq e(0)=\frac{1}{3}$$ for all $t>0$.  So I have a fairly sharp bound and the fact that $e(t)$ is decreasing in $t$.  If I could show further that $e(t)$ actually tends to $0$ uniformly, then that would imply $u_{x}(x,t)\to0\;\text{as}\;t\to\infty$ uniformly and that would give me $u(x,t)\to\;\text{C}\;\text{as}\;t\to\infty$.  Applying the periodic boundary conditions once more would then give me $C=0$ as required. However, I do not know how to show $e(t)\to0$ as $t\to\infty$.  I've tried computing additional derivatives of $e$ to see if I could glean any additional information on the decay of $e$, but the computations did not seem to lead to anything fruitful (even after playing around/substituting terms using the PDE). Any hints are appreciated (a fully fleshed answer is not needed).  Thanks!",,"['analysis', 'partial-differential-equations']"
82,"If $f_n(x)=x^n$ converges to $f$, why is $f$ not continuous?","If  converges to , why is  not continuous?",f_n(x)=x^n f f,"I was reading my Analysis course notes and had some trouble. I hope you can help me. Let $C(X)=\{ f | f:X \longrightarrow \mathbb{R} \text{ is a continuous function}\}$. It was already stated and proved in the notes that, whenever $X$ is a compact metric space, then $C(X)$ is a complete metric space regarding the uniform metric $d_\infty(f,g)= \displaystyle \sup_{x \in X} \lvert f(x)-g(x) \rvert$. What I thought: if $C(X)$ is complete, then every Cauchy sequence of functions $f_n$ in $C(X)$ must converge to a function $f \in C(X)$. But then it came to me that the sequence $f_n(x)=x^n$ is such that $f_n \in C([0,1])$ for all  $n \in \mathbb{N}$, but $f_n$ converges to $f \in l^\infty(X)$, where $$ f(x) =   \begin{cases}     0, \text{ if } x \in [0,1)\\     1, \text{ if } x = 1   \end{cases} $$ and $l^\infty(X)$ is the metric space of all the limited real functions defined in $X$ with the metric above. If $f_n \longrightarrow f \in l^\infty(X)$, then $f_n$ is a Cauchy sequence in $C(X)$. As $C(X)$ is complete, why is it that $f \notin C(X) \subset l^\infty(X)$? I'm sure my mistake is as silly as a wrong assumption, but I can't spot where it is. Could someone give me a clue? Thanks in advance.","I was reading my Analysis course notes and had some trouble. I hope you can help me. Let $C(X)=\{ f | f:X \longrightarrow \mathbb{R} \text{ is a continuous function}\}$. It was already stated and proved in the notes that, whenever $X$ is a compact metric space, then $C(X)$ is a complete metric space regarding the uniform metric $d_\infty(f,g)= \displaystyle \sup_{x \in X} \lvert f(x)-g(x) \rvert$. What I thought: if $C(X)$ is complete, then every Cauchy sequence of functions $f_n$ in $C(X)$ must converge to a function $f \in C(X)$. But then it came to me that the sequence $f_n(x)=x^n$ is such that $f_n \in C([0,1])$ for all  $n \in \mathbb{N}$, but $f_n$ converges to $f \in l^\infty(X)$, where $$ f(x) =   \begin{cases}     0, \text{ if } x \in [0,1)\\     1, \text{ if } x = 1   \end{cases} $$ and $l^\infty(X)$ is the metric space of all the limited real functions defined in $X$ with the metric above. If $f_n \longrightarrow f \in l^\infty(X)$, then $f_n$ is a Cauchy sequence in $C(X)$. As $C(X)$ is complete, why is it that $f \notin C(X) \subset l^\infty(X)$? I'm sure my mistake is as silly as a wrong assumption, but I can't spot where it is. Could someone give me a clue? Thanks in advance.",,"['analysis', 'metric-spaces']"
83,Integration of hyperreal functions / Intermediate Value Theorem,Integration of hyperreal functions / Intermediate Value Theorem,,"Here's a statement on hyperreal function I've been trying to prove (I came up with it but I think it is true): Suppose $f(x)$ is a continuous real-valued function and $h(x)$ is a continuous hyperreal-valued function such that $f(x) \approx h(x)$ for all $x$ (even infinite). If $\int_a^\infty f(x)\,dx$ and $\int_a^\infty h(x)\,dx$ exist and are finite, then $ \int_a^\infty f(x)\,dx = \int_a^\infty h(x)\,dx$ . Note: $\int_a^\infty g(x)\,dx = st\left(\sum_a^Ng(x)\,dx\right)$ for any infinite $N$ . Using notions of standard analysis, I can show that $\int_a^b f(x)\,dx - \int_a^b h(x)\,dx=0$ for all real $b$ and therefore remains $0$ as $b$ grows. However, I am unable to use nonstandard analysis on this, that is, to show $\sum_a^N f(x)\,dx \approx \sum_a^N h(x)\,dx$ for all positive infinite $N$ . Just because the statement holds for all real $b$ , I can't see why it must hold for infinite hyperreals and so I feel like my proof is not rigorous enough. The transfer principle will not apply since $h(x)$ does not have to be a natural extension of any real-valued function. Even more, the statement implies that if $h(x) \approx 0$ and $\sum_0^N h(x)\,dx \approx \sum_0^{M} h(x)\,dx \approx  r \in \mathbb{R}$ for all $N,M$ , then $r= 0$ . So we can't have a sort of infinite series of infinitesimals that equals $1$ . This seems surprising to me. Edit: Would it be valid to argue that $$\delta(b) = st\left(\sum_a^b f(x)\,dx - \sum_a^b h(x)\,dx\right) = \begin{cases} 0, & b \text{ is finite} \\ r \neq 0, &b \text{ is infinite} \end{cases}$$ is impossible since $\delta(b)$ is a continuous function? $\delta(b)$ is constant for infinite $b$ because the integrals exist and are finite and thus $\delta(N)=\delta(M)$ for any infinite $N, M$ . Edit 2: Justification of this approach has brought me to another problem altogether. The function $l(x)=st(x)$ is defined on all finite hyperreals and continuous but does not satisfy the Intermediate Value Theorem (IVT) in ${}^*\mathbb{R}$ . For example, $l(0)=0$ and $l(1)=1$ but there is no $x \in [0,1]$ such that $l(x)=\epsilon$ for any infinitesimal $\epsilon$ . However, there is an $x$ such that $l(x) \approx \epsilon$ . In this second sense, $l(x)$ passes the IVT but $\delta(b)$ does not. I still need to work out the details to understand why I had to reword the theorem for hyperreal-valued functions. Edit 3: I think I've finally resolved the matter. I tried to see if the hyperreal number line is complete, and as I suspected, the nonstandard metric is defined in terms of $\approx$ or ""halos."" So instead of points converging, it is the halos that do. That is why I had to reword the Intermediate Value Theorem to use $\approx$ instead of $=$ . So yes, $\delta(b)$ can only be continuous if it is $0$ everywhere.","Here's a statement on hyperreal function I've been trying to prove (I came up with it but I think it is true): Suppose is a continuous real-valued function and is a continuous hyperreal-valued function such that for all (even infinite). If and exist and are finite, then . Note: for any infinite . Using notions of standard analysis, I can show that for all real and therefore remains as grows. However, I am unable to use nonstandard analysis on this, that is, to show for all positive infinite . Just because the statement holds for all real , I can't see why it must hold for infinite hyperreals and so I feel like my proof is not rigorous enough. The transfer principle will not apply since does not have to be a natural extension of any real-valued function. Even more, the statement implies that if and for all , then . So we can't have a sort of infinite series of infinitesimals that equals . This seems surprising to me. Edit: Would it be valid to argue that is impossible since is a continuous function? is constant for infinite because the integrals exist and are finite and thus for any infinite . Edit 2: Justification of this approach has brought me to another problem altogether. The function is defined on all finite hyperreals and continuous but does not satisfy the Intermediate Value Theorem (IVT) in . For example, and but there is no such that for any infinitesimal . However, there is an such that . In this second sense, passes the IVT but does not. I still need to work out the details to understand why I had to reword the theorem for hyperreal-valued functions. Edit 3: I think I've finally resolved the matter. I tried to see if the hyperreal number line is complete, and as I suspected, the nonstandard metric is defined in terms of or ""halos."" So instead of points converging, it is the halos that do. That is why I had to reword the Intermediate Value Theorem to use instead of . So yes, can only be continuous if it is everywhere.","f(x) h(x) f(x) \approx h(x) x \int_a^\infty f(x)\,dx \int_a^\infty h(x)\,dx  \int_a^\infty f(x)\,dx = \int_a^\infty h(x)\,dx \int_a^\infty g(x)\,dx = st\left(\sum_a^Ng(x)\,dx\right) N \int_a^b f(x)\,dx - \int_a^b h(x)\,dx=0 b 0 b \sum_a^N f(x)\,dx \approx \sum_a^N h(x)\,dx N b h(x) h(x) \approx 0 \sum_0^N h(x)\,dx \approx \sum_0^{M} h(x)\,dx \approx  r \in \mathbb{R} N,M r= 0 1 \delta(b) = st\left(\sum_a^b f(x)\,dx - \sum_a^b h(x)\,dx\right) = \begin{cases} 0, & b \text{ is finite} \\ r \neq 0, &b \text{ is infinite} \end{cases} \delta(b) \delta(b) b \delta(N)=\delta(M) N, M l(x)=st(x) {}^*\mathbb{R} l(0)=0 l(1)=1 x \in [0,1] l(x)=\epsilon \epsilon x l(x) \approx \epsilon l(x) \delta(b) \approx \approx = \delta(b) 0","['analysis', 'improper-integrals', 'nonstandard-analysis']"
84,Is there a simple smooth non-piecewise function that could replace this piecewise one,Is there a simple smooth non-piecewise function that could replace this piecewise one,,"I need a function which satisfies these conditions: $f(0) = 0,$ $f(x)$ is monotonically increasing for all $x > 0,$ for some specified $x_2 > x_1 > 0$ and $y_2 > y_1 > 0,$ $f(x_1) = y_1,$ and $f(x_2) = y_2.$ I can construct $f(x)$ easily using a piecewise linear function that passes from the origin to the points $(x_1,y_1)$ and $(x_2,y_2)$ in turn, then continues to monotonically increase at some arbitrary rate. However, I am concerned at possible effects of the non-continuous curvature. Is there a better alternative to using spline curves that might give both a smooth curve and a non-piecewise function?","I need a function which satisfies these conditions: $f(0) = 0,$ $f(x)$ is monotonically increasing for all $x > 0,$ for some specified $x_2 > x_1 > 0$ and $y_2 > y_1 > 0,$ $f(x_1) = y_1,$ and $f(x_2) = y_2.$ I can construct $f(x)$ easily using a piecewise linear function that passes from the origin to the points $(x_1,y_1)$ and $(x_2,y_2)$ in turn, then continues to monotonically increase at some arbitrary rate. However, I am concerned at possible effects of the non-continuous curvature. Is there a better alternative to using spline curves that might give both a smooth curve and a non-piecewise function?",,"['real-analysis', 'analysis']"
85,Elementary application of Brouwer's fixed point Theorem,Elementary application of Brouwer's fixed point Theorem,,"A professor of mine has suggested to me to look at this theorem and to find a problem related to it to explain in a future class. I found an understandable proof in ""Linear operators"" by Dunford-Schwartz and I think I studied it, so now I know how to probe Brouwer's Theorem. Now I was thinking of some interesting related problem I could try to solve, do you have any suggestion of something not too hard (I am a second year undergraduate student!) ? Thank you very much! EDIT: I put PDE in the tags as this professor is mainly interested in this area so if you have any idea on that then even better (I guess most of the applications to PDEs will be very hard though!! )","A professor of mine has suggested to me to look at this theorem and to find a problem related to it to explain in a future class. I found an understandable proof in ""Linear operators"" by Dunford-Schwartz and I think I studied it, so now I know how to probe Brouwer's Theorem. Now I was thinking of some interesting related problem I could try to solve, do you have any suggestion of something not too hard (I am a second year undergraduate student!) ? Thank you very much! EDIT: I put PDE in the tags as this professor is mainly interested in this area so if you have any idea on that then even better (I guess most of the applications to PDEs will be very hard though!! )",,"['analysis', 'partial-differential-equations']"
86,Sine not a Rational Function Spivak,Sine not a Rational Function Spivak,,"This is Chapter 15 Question 31 in Spivak: a) Show sin is not a rational function. By definition of a rational function, a rational function cannot be $0$ at infinite points unless it is $0$ everywhere. Obviously, sin have infinite points that are 0 and infinite points that are not zero, thus not a rational function. b) Show that there do not exist rational functions $f_0, \ldots, f_{n-1}$ such that $(\sin x)^n + f_{n-1}(x)(\sin x)^{n-1} + \ldots + f_0({x}) = 0$ for all x First choose $x = 2k\pi$, so $f_0(x) = 0$ for $x = 2k\pi$. Since $f_0$ is rational $\implies f_0(x) = 0$ for all x.  Thus can write $\sin x[(\sin x)^{n-1} + f_{n-1}(\sin x)^{n-2} ... +f_1(x)] = 0$ Question: The second factor is $0$ for all $x \neq 2k\pi$ How does this imply that it is 0 for all x? And how does this lead to the result?","This is Chapter 15 Question 31 in Spivak: a) Show sin is not a rational function. By definition of a rational function, a rational function cannot be $0$ at infinite points unless it is $0$ everywhere. Obviously, sin have infinite points that are 0 and infinite points that are not zero, thus not a rational function. b) Show that there do not exist rational functions $f_0, \ldots, f_{n-1}$ such that $(\sin x)^n + f_{n-1}(x)(\sin x)^{n-1} + \ldots + f_0({x}) = 0$ for all x First choose $x = 2k\pi$, so $f_0(x) = 0$ for $x = 2k\pi$. Since $f_0$ is rational $\implies f_0(x) = 0$ for all x.  Thus can write $\sin x[(\sin x)^{n-1} + f_{n-1}(\sin x)^{n-2} ... +f_1(x)] = 0$ Question: The second factor is $0$ for all $x \neq 2k\pi$ How does this imply that it is 0 for all x? And how does this lead to the result?",,['analysis']
87,$ \log_{\frac 32x_{1}}\left(\frac{1}{2}-\frac{1}{36x_{2}^{2}}\right)+\cdots+ \log_{\frac 32x_{n}}\left(\frac{1}{2}-\frac{1}{36x_{1}^{2}}\right).$,, \log_{\frac 32x_{1}}\left(\frac{1}{2}-\frac{1}{36x_{2}^{2}}\right)+\cdots+ \log_{\frac 32x_{n}}\left(\frac{1}{2}-\frac{1}{36x_{1}^{2}}\right).,"Let $x_{1}$, $x_{2}$, $\ldots$, $x_{n}$ be $n$ real numbers in $\left(\frac{1}{4},\frac{2}{3}\right)$. Find the minimal value of the expression: $ \log_{\frac 32x_{1}}\left(\frac{1}{2}-\frac{1}{36x_{2}^{2}}\right)+\log_{\frac 32x_{2}}\left(\frac{1}{2}-\frac{1}{36x_{3}^{2}}\right)+\cdots+ \log_{\frac 32x_{n}}\left(\frac{1}{2}-\frac{1}{36x_{1}^{2}}\right). $","Let $x_{1}$, $x_{2}$, $\ldots$, $x_{n}$ be $n$ real numbers in $\left(\frac{1}{4},\frac{2}{3}\right)$. Find the minimal value of the expression: $ \log_{\frac 32x_{1}}\left(\frac{1}{2}-\frac{1}{36x_{2}^{2}}\right)+\log_{\frac 32x_{2}}\left(\frac{1}{2}-\frac{1}{36x_{3}^{2}}\right)+\cdots+ \log_{\frac 32x_{n}}\left(\frac{1}{2}-\frac{1}{36x_{1}^{2}}\right). $",,"['real-analysis', 'algebra-precalculus', 'analysis', 'optimization', 'contest-math']"
88,"The area of the set in which a polynomial is ""small""","The area of the set in which a polynomial is ""small""",,"Prove that there exist a constant $C$ such that for every monic polynomial $P$, the area of the set $A=\{x : |P(x)|<1\}$ is at most $C$. Remarks: This puzzle holds for both the real and the complex field (with possibly different $C$'s) Monic polynomial means that the leading coefficient is 1","Prove that there exist a constant $C$ such that for every monic polynomial $P$, the area of the set $A=\{x : |P(x)|<1\}$ is at most $C$. Remarks: This puzzle holds for both the real and the complex field (with possibly different $C$'s) Monic polynomial means that the leading coefficient is 1",,"['analysis', 'polynomials']"
89,Taylor expansion of an integral,Taylor expansion of an integral,,"I am interested in the Taylor series expansion around $t=0$ of the following expression: $$I(t)=\int_{0}^{\infty}e^{-x^2}\log\left(e^{-(x-t)^2}+e^{-(x+t)^2}\right)dx$$ Normally, I would proceed by taking the derivatives inside the integral using the Leibniz Integral Rule, however, in this case I am not sure if I can do this, since I can't find the dominating function $g(x)$ for the integrand such that $|e^{-x^2}\log\left(e^{-(x-t)^2}+e^{-(x+t)^2}\right)|\leq g(x)$ that is independent of $t$.  Such dominating function is a condition for interchanging the order of differential and integration . Re-arranging the equation above using arithmetic, interchanging the differential and the integral in the following expression would be very useful: $$\frac{\partial}{\partial t}\int_0^\infty e^{-x^2}\log \operatorname{cosh}(xt)dx$$ where $\operatorname{cosh}(x)=\frac{e^{x}+e^{-x}}{2}$ is the hyperbolic cosine function.  However, I cannot find a dominating function for the integrand in this case that does not depend on $t$. Any help would be appreciated.  I think restricting $t$ to $0\leq t \leq t_{\max} <\infty$ would work, but I am wondering if I can do this without the restriction on $t$ (other than being a real number).","I am interested in the Taylor series expansion around $t=0$ of the following expression: $$I(t)=\int_{0}^{\infty}e^{-x^2}\log\left(e^{-(x-t)^2}+e^{-(x+t)^2}\right)dx$$ Normally, I would proceed by taking the derivatives inside the integral using the Leibniz Integral Rule, however, in this case I am not sure if I can do this, since I can't find the dominating function $g(x)$ for the integrand such that $|e^{-x^2}\log\left(e^{-(x-t)^2}+e^{-(x+t)^2}\right)|\leq g(x)$ that is independent of $t$.  Such dominating function is a condition for interchanging the order of differential and integration . Re-arranging the equation above using arithmetic, interchanging the differential and the integral in the following expression would be very useful: $$\frac{\partial}{\partial t}\int_0^\infty e^{-x^2}\log \operatorname{cosh}(xt)dx$$ where $\operatorname{cosh}(x)=\frac{e^{x}+e^{-x}}{2}$ is the hyperbolic cosine function.  However, I cannot find a dominating function for the integrand in this case that does not depend on $t$. Any help would be appreciated.  I think restricting $t$ to $0\leq t \leq t_{\max} <\infty$ would work, but I am wondering if I can do this without the restriction on $t$ (other than being a real number).",,"['real-analysis', 'analysis', 'derivatives', 'taylor-expansion']"
90,Showing a function is not Riemann integrable,Showing a function is not Riemann integrable,,"Let $m,n \in \mathbb{Z_{+}}$ and let $f(x)=\begin{cases} x^m+x^n \text{ if } x \in [0,1]\cap\mathbb{Q}\\ 0 \text{ if } x \in [0,1] \setminus \mathbb{Q} \end{cases}$. I thought of a similar function $f(x)=\begin{cases} x^m \ \text{ if } x \in [0,1]\cap\mathbb{Q}\\ 0 \text{ if } x \in [0,1] \setminus \mathbb{Q} \end{cases}$. I've shown the second function is not Riemann integrable.  I was trying to produce a similar argument, but I was told it was not so straightforward.","Let $m,n \in \mathbb{Z_{+}}$ and let $f(x)=\begin{cases} x^m+x^n \text{ if } x \in [0,1]\cap\mathbb{Q}\\ 0 \text{ if } x \in [0,1] \setminus \mathbb{Q} \end{cases}$. I thought of a similar function $f(x)=\begin{cases} x^m \ \text{ if } x \in [0,1]\cap\mathbb{Q}\\ 0 \text{ if } x \in [0,1] \setminus \mathbb{Q} \end{cases}$. I've shown the second function is not Riemann integrable.  I was trying to produce a similar argument, but I was told it was not so straightforward.",,['analysis']
91,Proving continuity of an integral,Proving continuity of an integral,,"I have the following function: $$I_n(a)=\int_{-\infty}^{\infty}x^6e^{-x^2}\operatorname{sech}^n(ax)dx$$ where $\operatorname{sech}(x)=\frac{2}{e^x+e^{-x}}$ is the hyperbolic secant. Clearly, the integral is a beast to evaluate.  However, all I need is to prove the continuity of $I_n(a)$ at $n=2,4,6$; I don't need the closed form of $I_n(a)$.  In particular, I am interested in the continuity of $I_n(a)$ in the neighborhood of $a$ around zero, i.e. $a\in[-\epsilon,\epsilon]$ for some small positive $\epsilon$ (I plotted it in that region and it ""looks"" continuous to me, however, I'd like a rigorous proof). This seems like a simple problem, but I have no idea where to start.  Someone suggested the Lebesgue Dominated Convergence Theorem , but unfortunately, I am shaky on the measure theory, having never taken a course on the subject.  I read the wikipedia page , but have no idea how to use.  Is there perhaps a simpler way?  If not, can someone help?","I have the following function: $$I_n(a)=\int_{-\infty}^{\infty}x^6e^{-x^2}\operatorname{sech}^n(ax)dx$$ where $\operatorname{sech}(x)=\frac{2}{e^x+e^{-x}}$ is the hyperbolic secant. Clearly, the integral is a beast to evaluate.  However, all I need is to prove the continuity of $I_n(a)$ at $n=2,4,6$; I don't need the closed form of $I_n(a)$.  In particular, I am interested in the continuity of $I_n(a)$ in the neighborhood of $a$ around zero, i.e. $a\in[-\epsilon,\epsilon]$ for some small positive $\epsilon$ (I plotted it in that region and it ""looks"" continuous to me, however, I'd like a rigorous proof). This seems like a simple problem, but I have no idea where to start.  Someone suggested the Lebesgue Dominated Convergence Theorem , but unfortunately, I am shaky on the measure theory, having never taken a course on the subject.  I read the wikipedia page , but have no idea how to use.  Is there perhaps a simpler way?  If not, can someone help?",,"['analysis', 'measure-theory', 'continuity']"
92,What is the norm of the gradient of $f$ in normal coordinate?,What is the norm of the gradient of  in normal coordinate?,f,"Let $M$ be a Riemannian manifold and $f$ a smooth function on $M$. The Bochner formula proved in Schoen-Yau's book ""lectures in Differential Geometry"": (prop. 2.2) $$ \Delta |\nabla f|^2(p)=2\sum _{ij} |f_{ij}|^2 +2 R_{ij}f_i f_j +2 \sum f_i(\Delta f)_i $$ The proof assumes that ${x_i}$ are normal coordinates around $p$. and $f_i$ is the co variant differential of $f$ with respect to $\partial/\partial x_i$. $R_{ij}$ is the Ricci tensor. Then the proof proceed as follows: Since $|\nabla f|^2=\sum f_i^2$. Hence at $p$ one has $$ \Delta |\nabla f|^2 =\sum_j(\sum f_i^2)_{jj}=..... $$ My question is actually the first claim $$|\nabla f|^2=\sum f_i^2$$ It seems for my that in general $$\nabla f=f_sg^{si}\frac{\partial}{\partial x_i}$$ In normal coordinate we can only get the expression $|\nabla f|^2=\sum f_i^2$ only holds at the point $p$, not other points. But clearly we need to take derivative, the value at the point $p$ is not enough. It's most likely I miss some point here, but I can't figure out why. Anyone can help?","Let $M$ be a Riemannian manifold and $f$ a smooth function on $M$. The Bochner formula proved in Schoen-Yau's book ""lectures in Differential Geometry"": (prop. 2.2) $$ \Delta |\nabla f|^2(p)=2\sum _{ij} |f_{ij}|^2 +2 R_{ij}f_i f_j +2 \sum f_i(\Delta f)_i $$ The proof assumes that ${x_i}$ are normal coordinates around $p$. and $f_i$ is the co variant differential of $f$ with respect to $\partial/\partial x_i$. $R_{ij}$ is the Ricci tensor. Then the proof proceed as follows: Since $|\nabla f|^2=\sum f_i^2$. Hence at $p$ one has $$ \Delta |\nabla f|^2 =\sum_j(\sum f_i^2)_{jj}=..... $$ My question is actually the first claim $$|\nabla f|^2=\sum f_i^2$$ It seems for my that in general $$\nabla f=f_sg^{si}\frac{\partial}{\partial x_i}$$ In normal coordinate we can only get the expression $|\nabla f|^2=\sum f_i^2$ only holds at the point $p$, not other points. But clearly we need to take derivative, the value at the point $p$ is not enough. It's most likely I miss some point here, but I can't figure out why. Anyone can help?",,"['analysis', 'differential-geometry', 'riemannian-geometry']"
93,product of measures and radon-nikodym,product of measures and radon-nikodym,,"$\mu, \nu$ measures $(X,M)$ with $\nu \ll \mu$ and $\lambda = \mu + \nu$. Show: $f=\frac{d\nu}{d\lambda}$ then $0 \leq f < 1$ $\mu$-a.e. and $\frac{d\nu}{d\mu} = \frac{f}{1-f}$. In our context a measure is positive if not stated differently. So in this case $\mu,\nu$ and $\lambda$ are positiv. Let $A\in M$ with $\lambda(A)=0$ then it follows from $\lambda(A)=\mu(A)+\nu(A)=0$ that $\nu(A) =0$ and therefore it is $\nu \ll \lambda$ (and also $\mu \ll \lambda$). Then according to Radon-Nikodym it follows $\nu(A) = \int_A f d\lambda$. That $f \geq 0$ $\mu$-a.e. follows from $\mu \ll \lambda$ and $0 < \nu(A) = \int_A f d\lambda$ for all $A$ (therefore $f \geq 0$ $\lambda$-a.e.). For the second part I tried proof by contradiction. Assume $\mu(A) > 0$ and $f(x) > 1$ for all $x\in A$. From $\lambda(A) = \mu(A) + \nu(A)$ it follows then, that $\lambda(A) > 0$. Then $\nu(A) = \int_A f d\lambda > \int_A 1 d\lambda = \lambda(A)$. So that $\lambda(A) = \mu(A) + \nu(A) > \mu(A) + \lambda(A)$ which contradicts $\mu(A) > 0$. But then how to show the last part? Since $\nu \ll \mu$ it is (already proven) $\frac{d\nu}{d\lambda} = \frac{d\nu}{d\mu}\cdot\frac{d\mu}{d\lambda}$ So $\int_A f\cdot\frac{1}{1-f} d\mu \overset{!}{=} \nu(A) = \int_A \frac{d\nu}{d\lambda} d\lambda = \int_A \frac{d\nu}{d\mu}\cdot\frac{d\mu}{d\lambda} d\lambda$ So I need to show $1-f=\frac{d\lambda}{d\mu}$ (since $\lambda \ll \mu$ and $\mu \ll \lambda$ it is proven that $\frac{d\lambda}{d\mu} = \left( \frac{d\mu}{d\lambda} \right) ^{-1}$) $\int_A 1-f d\mu = \mu(A) - \int_A \frac{d\nu}{d\lambda}d\mu = \cdots$","$\mu, \nu$ measures $(X,M)$ with $\nu \ll \mu$ and $\lambda = \mu + \nu$. Show: $f=\frac{d\nu}{d\lambda}$ then $0 \leq f < 1$ $\mu$-a.e. and $\frac{d\nu}{d\mu} = \frac{f}{1-f}$. In our context a measure is positive if not stated differently. So in this case $\mu,\nu$ and $\lambda$ are positiv. Let $A\in M$ with $\lambda(A)=0$ then it follows from $\lambda(A)=\mu(A)+\nu(A)=0$ that $\nu(A) =0$ and therefore it is $\nu \ll \lambda$ (and also $\mu \ll \lambda$). Then according to Radon-Nikodym it follows $\nu(A) = \int_A f d\lambda$. That $f \geq 0$ $\mu$-a.e. follows from $\mu \ll \lambda$ and $0 < \nu(A) = \int_A f d\lambda$ for all $A$ (therefore $f \geq 0$ $\lambda$-a.e.). For the second part I tried proof by contradiction. Assume $\mu(A) > 0$ and $f(x) > 1$ for all $x\in A$. From $\lambda(A) = \mu(A) + \nu(A)$ it follows then, that $\lambda(A) > 0$. Then $\nu(A) = \int_A f d\lambda > \int_A 1 d\lambda = \lambda(A)$. So that $\lambda(A) = \mu(A) + \nu(A) > \mu(A) + \lambda(A)$ which contradicts $\mu(A) > 0$. But then how to show the last part? Since $\nu \ll \mu$ it is (already proven) $\frac{d\nu}{d\lambda} = \frac{d\nu}{d\mu}\cdot\frac{d\mu}{d\lambda}$ So $\int_A f\cdot\frac{1}{1-f} d\mu \overset{!}{=} \nu(A) = \int_A \frac{d\nu}{d\lambda} d\lambda = \int_A \frac{d\nu}{d\mu}\cdot\frac{d\mu}{d\lambda} d\lambda$ So I need to show $1-f=\frac{d\lambda}{d\mu}$ (since $\lambda \ll \mu$ and $\mu \ll \lambda$ it is proven that $\frac{d\lambda}{d\mu} = \left( \frac{d\mu}{d\lambda} \right) ^{-1}$) $\int_A 1-f d\mu = \mu(A) - \int_A \frac{d\nu}{d\lambda}d\mu = \cdots$",,"['real-analysis', 'analysis', 'measure-theory']"
94,Continuity of the function $g(e^{it})=f(t)$ defined on $f:\mathbb{R}\rightarrow \mathbb{C}$,Continuity of the function  defined on,g(e^{it})=f(t) f:\mathbb{R}\rightarrow \mathbb{C},"$f:\mathbb{R}\rightarrow \mathbb{C}$ continuous and $2\pi $ periodic. In Harro Heuser: Analysis 2 (page 65) it is left to the reader to show that the function $g(e^{it})=f(t)$ defined by : $$g: S\rightarrow \mathbb{C}; S=\{z\in \mathbb{C} ||z|=1 \}  $$ is continuous What I have tried: continuous maps under continuous functions are again continuous so it is enough if one shows that $y(t) = e^{it}$ is continuous on $t\in[0,2\pi]$ $\lim_{t\rightarrow a } e^{it} = e^{ia}$ is true because : $$|e^{it}-e^{ia}|= |e^{ia}|\cdot|e^{(t-a)}-1| = |t-a+\frac{(t-a)^2}{2!}+...| $$$$ = |t-a|\cdot|1+\frac{t-a}{2!}+\frac{(t-a)^2}{3!}...| \le |t-a|\cdot |e^{t-a}|$$ so in a compact neighbourhood we can bound $e^{it}$  and $|t-a|$ and $|e^{it}-e^{ia}|$ can be chosen arbitrarily small Is this correct? How can one show the continuity of this function more directly?","$f:\mathbb{R}\rightarrow \mathbb{C}$ continuous and $2\pi $ periodic. In Harro Heuser: Analysis 2 (page 65) it is left to the reader to show that the function $g(e^{it})=f(t)$ defined by : $$g: S\rightarrow \mathbb{C}; S=\{z\in \mathbb{C} ||z|=1 \}  $$ is continuous What I have tried: continuous maps under continuous functions are again continuous so it is enough if one shows that $y(t) = e^{it}$ is continuous on $t\in[0,2\pi]$ $\lim_{t\rightarrow a } e^{it} = e^{ia}$ is true because : $$|e^{it}-e^{ia}|= |e^{ia}|\cdot|e^{(t-a)}-1| = |t-a+\frac{(t-a)^2}{2!}+...| $$$$ = |t-a|\cdot|1+\frac{t-a}{2!}+\frac{(t-a)^2}{3!}...| \le |t-a|\cdot |e^{t-a}|$$ so in a compact neighbourhood we can bound $e^{it}$  and $|t-a|$ and $|e^{it}-e^{ia}|$ can be chosen arbitrarily small Is this correct? How can one show the continuity of this function more directly?",,"['real-analysis', 'analysis', 'multivariable-calculus']"
95,Construction of a sequence of simple functions converging pointwise to a given function,Construction of a sequence of simple functions converging pointwise to a given function,,Q1: How to construct a sequence of $\{f_n\}$ of simple function for a function $f$  such that $f_n\to f$ converges pointwise? Q2: If $f$ is measurable is $f_n$ also measurable for each $n$? Q2 is by Q1,Q1: How to construct a sequence of $\{f_n\}$ of simple function for a function $f$  such that $f_n\to f$ converges pointwise? Q2: If $f$ is measurable is $f_n$ also measurable for each $n$? Q2 is by Q1,,"['real-analysis', 'analysis', 'measure-theory', 'convergence-divergence', 'lebesgue-integral']"
96,On the relation of a certain set function with the outer measure,On the relation of a certain set function with the outer measure,,"Let $A$ be a set and define $m^{**}(A) \in [0,\infty] $ by: $m^{**} (A)= \inf\{m^{*}(\mathcal{O}) \ | \ A \subset \mathcal{O}, \mathcal{O} \ \text{open}  \} $ How is $m^{**}$ related to the outer measure $m^{*}$? I would appreciate any help on this!!","Let $A$ be a set and define $m^{**}(A) \in [0,\infty] $ by: $m^{**} (A)= \inf\{m^{*}(\mathcal{O}) \ | \ A \subset \mathcal{O}, \mathcal{O} \ \text{open}  \} $ How is $m^{**}$ related to the outer measure $m^{*}$? I would appreciate any help on this!!",,['real-analysis']
97,Does there exist a differentiable function $\Bbb R^2$ to $\Bbb R$ with certain partial derivative properties?,Does there exist a differentiable function  to  with certain partial derivative properties?,\Bbb R^2 \Bbb R,"Does there exist a counter-example to the following claim: For a function $f: \mathbb{R}^2 \to \mathbb{R}$, if: $D_1f$ exists in some ball around the origin and is continuous at the origin (but not necessarily anywhere else) $D_2f$ exists at the origin (but not necessarily anywhere else) then $f$ is differentiable at the origin. I know that if we add either the condition ""$D_2f$ is continuous at the origin"" or ""$D_1f$ is continuous in the open ball around the origin"" then $f$ must be differentiable at the origin, but I can't find a non-differentiable function just satisfying the above two properties.","Does there exist a counter-example to the following claim: For a function $f: \mathbb{R}^2 \to \mathbb{R}$, if: $D_1f$ exists in some ball around the origin and is continuous at the origin (but not necessarily anywhere else) $D_2f$ exists at the origin (but not necessarily anywhere else) then $f$ is differentiable at the origin. I know that if we add either the condition ""$D_2f$ is continuous at the origin"" or ""$D_1f$ is continuous in the open ball around the origin"" then $f$ must be differentiable at the origin, but I can't find a non-differentiable function just satisfying the above two properties.",,"['real-analysis', 'analysis']"
98,"A sequence in $C([-1,1])$ and $C^1([-1,1])$ with star-weak convergence w.r.t. to one space, but not the other","A sequence in  and  with star-weak convergence w.r.t. to one space, but not the other","C([-1,1]) C^1([-1,1])","The functionals $$   \phi_n(x) = \int_{\frac{1}{n} \le |t| \le 1} \frac{x(t)}{t} \mathrm{d} t $$ define a sequence of functionls in $C([-1,1])$ and $C^1([-1,1])$. a) Show that $(\phi_n)$ converges *-weakly in $C^1([-1,1])'$. b) Does $(\phi_n)$ converges *-weakly in $C([-1,1])'$? For me the limit functional $$  \int_{0 \le |t| \le 1} \frac{x(t)}{t} \mathrm{d} t $$ is not well defined so i have trouble evaluating the condition of convergence? Do you have any hints?","The functionals $$   \phi_n(x) = \int_{\frac{1}{n} \le |t| \le 1} \frac{x(t)}{t} \mathrm{d} t $$ define a sequence of functionls in $C([-1,1])$ and $C^1([-1,1])$. a) Show that $(\phi_n)$ converges *-weakly in $C^1([-1,1])'$. b) Does $(\phi_n)$ converges *-weakly in $C([-1,1])'$? For me the limit functional $$  \int_{0 \le |t| \le 1} \frac{x(t)}{t} \mathrm{d} t $$ is not well defined so i have trouble evaluating the condition of convergence? Do you have any hints?",,"['analysis', 'functional-analysis', 'convergence-divergence', 'banach-spaces']"
99,calculating a multivariate integral via level sets,calculating a multivariate integral via level sets,,"I'm considering the possibility of calculating an integral of the form $\int_{S_n} f(x_1,\dots,x_n) dx_1\dots dx_n$ via level sets, where $S_n$ is the domain of integration. In my problem everything is real valued, and $f:R^n\to R$. Also, consider $f$ smooth, with no holes, etc. Now, consider $g$ a real number that solves the equation $g=f(x_1,\dots,x_n)$, i.e., a level set. Then, I think one can write the relation \begin{equation} \int_{S_n}f(x_1,\dots,x_n) dx_1\dots dx_n=\int g\rho(g)dg \end{equation} where $\rho(g)dg$ is the density distribution of $g$, or roughly speaking the number of solutions of the level set equation for $g$. A simple example: consider calculating the volume of a half sphere (radius $r$) centered at $(a,a,0)$. Then, the integral on the left hand of the identity is $\int dx \int dy \sqrt{r^2-(x-a)^2-(y-a)^2}$, and the right hand is $\int dz 2\pi\sqrt{r^2-z^2}z$ where $g=z$ and $\rho(g=z)=2\pi\sqrt{r^2-z^2}$. First, is this statement roughly correct? I suspect it's related to the coarea formula, but am not entirely sure as this is not my area of expertise. Does this result have a particular name? Also, it seems to me that $\rho(g)$ would relate to the Jacobian (modulus) in some way, although I don't think it's just equal. Any ideas? Finally, even if all of this is correct, in a concrete case it seems to me that unless $f$ is a very simple example, it is going to be difficult to make concrete calculations in most cases. However, I should still whether there are some techniques out there to deal with this analytically. Or should one consider numerical techniques? Thanks","I'm considering the possibility of calculating an integral of the form $\int_{S_n} f(x_1,\dots,x_n) dx_1\dots dx_n$ via level sets, where $S_n$ is the domain of integration. In my problem everything is real valued, and $f:R^n\to R$. Also, consider $f$ smooth, with no holes, etc. Now, consider $g$ a real number that solves the equation $g=f(x_1,\dots,x_n)$, i.e., a level set. Then, I think one can write the relation \begin{equation} \int_{S_n}f(x_1,\dots,x_n) dx_1\dots dx_n=\int g\rho(g)dg \end{equation} where $\rho(g)dg$ is the density distribution of $g$, or roughly speaking the number of solutions of the level set equation for $g$. A simple example: consider calculating the volume of a half sphere (radius $r$) centered at $(a,a,0)$. Then, the integral on the left hand of the identity is $\int dx \int dy \sqrt{r^2-(x-a)^2-(y-a)^2}$, and the right hand is $\int dz 2\pi\sqrt{r^2-z^2}z$ where $g=z$ and $\rho(g=z)=2\pi\sqrt{r^2-z^2}$. First, is this statement roughly correct? I suspect it's related to the coarea formula, but am not entirely sure as this is not my area of expertise. Does this result have a particular name? Also, it seems to me that $\rho(g)$ would relate to the Jacobian (modulus) in some way, although I don't think it's just equal. Any ideas? Finally, even if all of this is correct, in a concrete case it seems to me that unless $f$ is a very simple example, it is going to be difficult to make concrete calculations in most cases. However, I should still whether there are some techniques out there to deal with this analytically. Or should one consider numerical techniques? Thanks",,"['calculus', 'probability', 'analysis']"
