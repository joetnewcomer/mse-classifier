,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Outer measure and Caratheodory's criterion,Outer measure and Caratheodory's criterion,,"Suppose $m^*$ is an outer measure in Caratheodory's sense on the space $X$, which satisfies $m^*(\emptyset)=0$, $A\subseteq B\implies m^*(A)\le m^*(B)$, and $m^*(\bigcup_n A_n)\le\sum m^*(A_n)$. We define $E$ measurable iff $m^*(E\cap A)+m^*(E^c\cap A)=m^*(A)$ for all $A\subseteq X$, and denote $m(E)=m^*(E)$. Is it true that $m^*$ is always really an outer measure, i.e. $$m^*(A)=\inf\{\,m(E)\,\vert\,\text{measurable }E\supseteq A\,\}$$ If not, can we put some simple axioms to $m^*$ such that the preceding equality takes place? I don't know any example of outer measure except Lebesgue outer measure, so I need some help. Thanks.","Suppose $m^*$ is an outer measure in Caratheodory's sense on the space $X$, which satisfies $m^*(\emptyset)=0$, $A\subseteq B\implies m^*(A)\le m^*(B)$, and $m^*(\bigcup_n A_n)\le\sum m^*(A_n)$. We define $E$ measurable iff $m^*(E\cap A)+m^*(E^c\cap A)=m^*(A)$ for all $A\subseteq X$, and denote $m(E)=m^*(E)$. Is it true that $m^*$ is always really an outer measure, i.e. $$m^*(A)=\inf\{\,m(E)\,\vert\,\text{measurable }E\supseteq A\,\}$$ If not, can we put some simple axioms to $m^*$ such that the preceding equality takes place? I don't know any example of outer measure except Lebesgue outer measure, so I need some help. Thanks.",,"['real-analysis', 'measure-theory']"
1,Transpose of continuous operator bounded below is surjective,Transpose of continuous operator bounded below is surjective,,Suppose $T: X\rightarrow Y$ is a continuous linear transformation between 2 normed vector spaces such tha $\|Tx\| \geq C\|x\|$ for some $C > 0$. Why must the transpose $T^{\ast}: Y^{\ast} \rightarrow X^{\ast}$ be surjective?,Suppose $T: X\rightarrow Y$ is a continuous linear transformation between 2 normed vector spaces such tha $\|Tx\| \geq C\|x\|$ for some $C > 0$. Why must the transpose $T^{\ast}: Y^{\ast} \rightarrow X^{\ast}$ be surjective?,,"['real-analysis', 'analysis', 'functional-analysis']"
2,"Showing that the Lebesgue measure is ""continuous""","Showing that the Lebesgue measure is ""continuous""",,"Problem : Let $m$ denote the Lebesgue measure on $\mathbb{R}$. Let $E$ be a Lebesgue measurable set of $\mathbb{R}$ such that $m(E \cap (E+t)) = 0$ for all $t \neq 0$. Prove that $m(E)=0$. I am aware of several solutions to this problem, and some of them we can find here on Math.SE,  but I am curious if I can also apply continuity of measure directly. In particular, we know by the continuity of measure that if $\{B_k\}$ is an ascending sequence of measurable sets, then $m\left( \bigcup^{\infty}_{k=1} B_k\right)=\lim_{k\rightarrow \infty} m(B_k)$. To specialize this for our case, suppose we consider the sequence $A_n = E \cap (E+\frac{1}{n})$ of sets. Clearly the conditions of the theorem are satisfied, since $m(A_n) < \infty$ and $A_1 \subseteq A_2 \subseteq \cdots$, and so we have that $m\left( \bigcup^{\infty}_{k=1} A_k\right)=\lim_{k\rightarrow \infty} m(A_k) = \lim_{k\rightarrow \infty} m(E \cap (E+\frac{1}{k})) = m(E)$, but this must be zero since unions of sets of measure zero is zero (left-most left-hand side of the equation). Is this one way to go about this, and if not, where is the fault? Thank you.","Problem : Let $m$ denote the Lebesgue measure on $\mathbb{R}$. Let $E$ be a Lebesgue measurable set of $\mathbb{R}$ such that $m(E \cap (E+t)) = 0$ for all $t \neq 0$. Prove that $m(E)=0$. I am aware of several solutions to this problem, and some of them we can find here on Math.SE,  but I am curious if I can also apply continuity of measure directly. In particular, we know by the continuity of measure that if $\{B_k\}$ is an ascending sequence of measurable sets, then $m\left( \bigcup^{\infty}_{k=1} B_k\right)=\lim_{k\rightarrow \infty} m(B_k)$. To specialize this for our case, suppose we consider the sequence $A_n = E \cap (E+\frac{1}{n})$ of sets. Clearly the conditions of the theorem are satisfied, since $m(A_n) < \infty$ and $A_1 \subseteq A_2 \subseteq \cdots$, and so we have that $m\left( \bigcup^{\infty}_{k=1} A_k\right)=\lim_{k\rightarrow \infty} m(A_k) = \lim_{k\rightarrow \infty} m(E \cap (E+\frac{1}{k})) = m(E)$, but this must be zero since unions of sets of measure zero is zero (left-most left-hand side of the equation). Is this one way to go about this, and if not, where is the fault? Thank you.",,"['real-analysis', 'solution-verification']"
3,Derive Fourier transform from what it should do?,Derive Fourier transform from what it should do?,,"I was wondering about the following: Imagine you want to figure out whether there is a transform that exchanges differentiation with multiplication and convoution with pairwise transformation for $L^2$ functions. Is there an analytical way to derive from this the Fourier transform? Why am I asking this? In many practical examples, you want to have a transform that has certain properties like: get rid of differentiation. But in mostly all books, the Fourier transform is just introduced and the properties are derived afterwards. Is it a posteriori possible to derive an integral transform just by looking for something with these properties?","I was wondering about the following: Imagine you want to figure out whether there is a transform that exchanges differentiation with multiplication and convoution with pairwise transformation for $L^2$ functions. Is there an analytical way to derive from this the Fourier transform? Why am I asking this? In many practical examples, you want to have a transform that has certain properties like: get rid of differentiation. But in mostly all books, the Fourier transform is just introduced and the properties are derived afterwards. Is it a posteriori possible to derive an integral transform just by looking for something with these properties?",,"['real-analysis', 'analysis']"
4,Convergence to half Euler's constant,Convergence to half Euler's constant,,"Euler's constant is defined by $$\gamma=\lim_{N\rightarrow\infty}\sum_{n=1}^N\dfrac1n-\log N.$$ So we can write it as $$1+\dfrac12+\dfrac13+\ldots+\dfrac1n-\log n\rightarrow \gamma.$$ How can we show that $$1+\dfrac13+\dfrac15+\ldots+\dfrac{1}{2n-1}-\dfrac12\log n\rightarrow\dfrac{\gamma}{2}+\log 2?$$ The fractions are similar, but when they skip the even terms, it's not clear how to relate.","Euler's constant is defined by $$\gamma=\lim_{N\rightarrow\infty}\sum_{n=1}^N\dfrac1n-\log N.$$ So we can write it as $$1+\dfrac12+\dfrac13+\ldots+\dfrac1n-\log n\rightarrow \gamma.$$ How can we show that $$1+\dfrac13+\dfrac15+\ldots+\dfrac{1}{2n-1}-\dfrac12\log n\rightarrow\dfrac{\gamma}{2}+\log 2?$$ The fractions are similar, but when they skip the even terms, it's not clear how to relate.",,"['real-analysis', 'convergence-divergence']"
5,Product of submanifolds is a submanifold,Product of submanifolds is a submanifold,,"Suppose that $X_1$ is an $n_1$-dimensional submanifold of $\mathbb{R}^{N_1}$, and $X_2$ is an $n_2$-dimensional submanifold of $\mathbb{R}^{N_2}$. Prove that $X_1\times X_2\subseteq \mathbb{R}^{N_1}\times\mathbb{R}^{N_2}$ is an $(n_1+n_2)$-dimensional submanifold of $\mathbb{R}^{N_1}\times\mathbb{R}^{N_2}$. Suppose $q=(r,s)\in X_1\times X_2$, where $r\in X_1$ and $s\in X_2$. By definition of a submanifold, there exists a neighborhood $V_r\in\mathbb{R}^{N_1}$, and an open subset $U_1\in\mathbb{R}^{n_1}$ and a diffeomorphism $\phi_1:U_1\rightarrow X_1\cap V_r$. Similarly, there exists a neighborhood $V_s\in\mathbb{R}^{N_2}$, and an open subset $U_2\in\mathbb{R}^{n_2}$ and a diffeomorphism $\phi_2:U_2\rightarrow X_2\cap V_s$. (Edited to take user103402's comment into account) Now, I could take the neighborhood $V_r\times V_s\in\mathbb{R}^{N_1}\times\mathbb{R}^{N_2}$ and the open subset $U_1\times U_2\in\mathbb{R}^{n_1}\times\mathbb{R}^{n_2}$. I want to define the map $\phi:U_1\times U_2\rightarrow(X_1\times X_2)\cap (V_r\times V_s)$. Note that  $$(X_1\times X_2)\cap (V_r\times V_s)=(X_1\cap V_r)\times(X_2\cap V_s).$$ So the map $\phi$ is simply given by $\phi(u_1,u_2)=(\phi_1(u_1),\phi_2(u_2))$ for $u_1\in U_1,u_2\in U_2$. Now, why is $\phi$ a diffeomorphism between $U_1\times U_2$ and $(X_1\cap V_r)\times(X_2\cap V_s)$? Even though $\phi_1$ is a diffeomorphism between $U_1$ and $X_1\cap V_r$, and $\phi_2$ is a diffeomorphism between $U_2$ and $X_2\cap V_s$, it does not seem clear to me that $\phi$ should be a diffeomorphism.","Suppose that $X_1$ is an $n_1$-dimensional submanifold of $\mathbb{R}^{N_1}$, and $X_2$ is an $n_2$-dimensional submanifold of $\mathbb{R}^{N_2}$. Prove that $X_1\times X_2\subseteq \mathbb{R}^{N_1}\times\mathbb{R}^{N_2}$ is an $(n_1+n_2)$-dimensional submanifold of $\mathbb{R}^{N_1}\times\mathbb{R}^{N_2}$. Suppose $q=(r,s)\in X_1\times X_2$, where $r\in X_1$ and $s\in X_2$. By definition of a submanifold, there exists a neighborhood $V_r\in\mathbb{R}^{N_1}$, and an open subset $U_1\in\mathbb{R}^{n_1}$ and a diffeomorphism $\phi_1:U_1\rightarrow X_1\cap V_r$. Similarly, there exists a neighborhood $V_s\in\mathbb{R}^{N_2}$, and an open subset $U_2\in\mathbb{R}^{n_2}$ and a diffeomorphism $\phi_2:U_2\rightarrow X_2\cap V_s$. (Edited to take user103402's comment into account) Now, I could take the neighborhood $V_r\times V_s\in\mathbb{R}^{N_1}\times\mathbb{R}^{N_2}$ and the open subset $U_1\times U_2\in\mathbb{R}^{n_1}\times\mathbb{R}^{n_2}$. I want to define the map $\phi:U_1\times U_2\rightarrow(X_1\times X_2)\cap (V_r\times V_s)$. Note that  $$(X_1\times X_2)\cap (V_r\times V_s)=(X_1\cap V_r)\times(X_2\cap V_s).$$ So the map $\phi$ is simply given by $\phi(u_1,u_2)=(\phi_1(u_1),\phi_2(u_2))$ for $u_1\in U_1,u_2\in U_2$. Now, why is $\phi$ a diffeomorphism between $U_1\times U_2$ and $(X_1\cap V_r)\times(X_2\cap V_s)$? Even though $\phi_1$ is a diffeomorphism between $U_1$ and $X_1\cap V_r$, and $\phi_2$ is a diffeomorphism between $U_2$ and $X_2\cap V_s$, it does not seem clear to me that $\phi$ should be a diffeomorphism.",,"['real-analysis', 'manifolds']"
6,Prove that $1/f$ is uniformly continuous on ...,Prove that  is uniformly continuous on ...,1/f,"I need hints for this particular question: Prove that if a function $f$ is uniformly continuous on $A\subseteq \mathbb{R}$ and $|f(x)|\geq k>0$ for all $x\in A$, then the function $\frac{1}{f(x)}$ is also uniformly continuous on $A$. My attempt: From the rough work given that $f(x)$ is uniformly continuous on $A$ for all $\epsilon> 0$, there exists a $\delta$ such that $|f(x)-f(y)|<\epsilon k^2$ when $x\in A$ and $|x-y|< \delta$, which impiles $\frac{1}{k^2} |f(x)-f(y)|< \epsilon$ for all $\epsilon >0$ and $|x-y|<\delta$. Taking the hint from Ayman Hourieh into consideration we have using the same $\delta$ from rough work, $\left |\frac{1}{f(x)} - \frac{1}{f(y)}  \right | $ = $\left |\frac{1}{f(x)f(y)}  \right | \left | f(x) - f(y) \right |\leq \frac{1}{k^2} \left | f(x)-f(y)  \right |< \epsilon$ when $\left | x-y \right |<\delta$. Is this proof okay?","I need hints for this particular question: Prove that if a function $f$ is uniformly continuous on $A\subseteq \mathbb{R}$ and $|f(x)|\geq k>0$ for all $x\in A$, then the function $\frac{1}{f(x)}$ is also uniformly continuous on $A$. My attempt: From the rough work given that $f(x)$ is uniformly continuous on $A$ for all $\epsilon> 0$, there exists a $\delta$ such that $|f(x)-f(y)|<\epsilon k^2$ when $x\in A$ and $|x-y|< \delta$, which impiles $\frac{1}{k^2} |f(x)-f(y)|< \epsilon$ for all $\epsilon >0$ and $|x-y|<\delta$. Taking the hint from Ayman Hourieh into consideration we have using the same $\delta$ from rough work, $\left |\frac{1}{f(x)} - \frac{1}{f(y)}  \right | $ = $\left |\frac{1}{f(x)f(y)}  \right | \left | f(x) - f(y) \right |\leq \frac{1}{k^2} \left | f(x)-f(y)  \right |< \epsilon$ when $\left | x-y \right |<\delta$. Is this proof okay?",,['real-analysis']
7,Solution to integral curve for compactly supported functions,Solution to integral curve for compactly supported functions,,"Let $U\subseteq\mathbb{R}^n$ be an open subset, and let $g:U\rightarrow\mathbb{R^n}$ be a $C^1$ function. Let $x_1(t),\ldots,x_n(t)$ be $C^1$ functions on an open interval $I\subseteq\mathbb{R}$. Write $x(t)=(x_1(t),\ldots,x_n(t))$. Consider the equation $$\dfrac{dx}{dt}(t)=g(x(t))$$ Call that equation $(*)$. Suppose that there exists a compact set $W$ such that $g(x)=0$ for all $x\not\in W$, and let $x_0\in U$. Prove that there exists a solution $x(t)$ to $(*)$ for $t\in(-\infty,\infty)$ such that $x(0)=x_0$. Fix $x_0\in U$. If $g(x_0)=0$, we could just take the constant solution $x(t)=x_0$ which clearly satisfies $(*)$. So we can consider the case $g(x)\neq 0$. I don't know what to do in this case.","Let $U\subseteq\mathbb{R}^n$ be an open subset, and let $g:U\rightarrow\mathbb{R^n}$ be a $C^1$ function. Let $x_1(t),\ldots,x_n(t)$ be $C^1$ functions on an open interval $I\subseteq\mathbb{R}$. Write $x(t)=(x_1(t),\ldots,x_n(t))$. Consider the equation $$\dfrac{dx}{dt}(t)=g(x(t))$$ Call that equation $(*)$. Suppose that there exists a compact set $W$ such that $g(x)=0$ for all $x\not\in W$, and let $x_0\in U$. Prove that there exists a solution $x(t)$ to $(*)$ for $t\in(-\infty,\infty)$ such that $x(0)=x_0$. Fix $x_0\in U$. If $g(x_0)=0$, we could just take the constant solution $x(t)=x_0$ which clearly satisfies $(*)$. So we can consider the case $g(x)\neq 0$. I don't know what to do in this case.",,['real-analysis']
8,Show $m(A \cap I) \leq (1-\epsilon)m(I)$ for every $I$ implies $m(A) = 0$,Show  for every  implies,m(A \cap I) \leq (1-\epsilon)m(I) I m(A) = 0,"The problem I stuck was following. Suppose $m$ is Lebesgue measure and $A$ is a Borel measurable subset of $\mathbb{R}$. Prove that if    $$  m(A\cap I) \leq (1-\epsilon)m(I) $$   for every open interval $I$, then $m(A) = 0$. I asked about this problem to the TA, and they just suggested me to use open interval with outer measure. The outer measure I am using is  $$ m^\star (E) = \inf  \{ \sum_{i=1}^{\infty} l(A_{i}) : E \subset \cup_{i=1}^{\infty}A_{i} \} $$ Thanks in advance.","The problem I stuck was following. Suppose $m$ is Lebesgue measure and $A$ is a Borel measurable subset of $\mathbb{R}$. Prove that if    $$  m(A\cap I) \leq (1-\epsilon)m(I) $$   for every open interval $I$, then $m(A) = 0$. I asked about this problem to the TA, and they just suggested me to use open interval with outer measure. The outer measure I am using is  $$ m^\star (E) = \inf  \{ \sum_{i=1}^{\infty} l(A_{i}) : E \subset \cup_{i=1}^{\infty}A_{i} \} $$ Thanks in advance.",,"['real-analysis', 'measure-theory']"
9,"Prove that the interval $[a,b]$ in $\Bbb R$ is the same as the segment $[a,b]$ in $\Bbb R^1$",Prove that the interval  in  is the same as the segment  in,"[a,b] \Bbb R [a,b] \Bbb R^1","Prove that the interval $[a,b]$ in $\Bbb R$ is the same as the segment $[a,b]$ in $\Bbb R^1$.  That is, $\{x\in\Bbb R: a\le x\le b\} = \{y\in\Bbb R: \exists s,t\in [0,1], s+t=1\text{ and } y=sa+tb\}$ I was trying to solve it as follows. Let's denote  $A= \{x\in\Bbb R: a\le x\le b\}$ and  $B= \{y\in\Bbb R: \exists s,t\in [0,1], s+t=1\text{ and }y=sa+tb\}$. Then I wanted to show, that $A\subseteq B$ and $B\subseteq A$ so $A = B$. For the $B\subseteq A$ I did the following: $y=sa+tb=sa+(1-s)b=s(a-b)+b$ that is obviously in $[a,b]$ (as when $s=0 $ then $y=b $ and when $s=1 $ then $ y=a$). So $B\subseteq A$. For $A\subseteq B$ I tried to show that if $x\in [a,b] $ then we can express $x=a+(b-a)k, k\in [0,1]$ then $x=a(1-k)+bk$ Saying that $(1-k)=t $ and $ k=s$ we can conclude that $A\subseteq B$. It seems to me that the second part is incorrect, but, unfortunately I can't figure out what should be done here.","Prove that the interval $[a,b]$ in $\Bbb R$ is the same as the segment $[a,b]$ in $\Bbb R^1$.  That is, $\{x\in\Bbb R: a\le x\le b\} = \{y\in\Bbb R: \exists s,t\in [0,1], s+t=1\text{ and } y=sa+tb\}$ I was trying to solve it as follows. Let's denote  $A= \{x\in\Bbb R: a\le x\le b\}$ and  $B= \{y\in\Bbb R: \exists s,t\in [0,1], s+t=1\text{ and }y=sa+tb\}$. Then I wanted to show, that $A\subseteq B$ and $B\subseteq A$ so $A = B$. For the $B\subseteq A$ I did the following: $y=sa+tb=sa+(1-s)b=s(a-b)+b$ that is obviously in $[a,b]$ (as when $s=0 $ then $y=b $ and when $s=1 $ then $ y=a$). So $B\subseteq A$. For $A\subseteq B$ I tried to show that if $x\in [a,b] $ then we can express $x=a+(b-a)k, k\in [0,1]$ then $x=a(1-k)+bk$ Saying that $(1-k)=t $ and $ k=s$ we can conclude that $A\subseteq B$. It seems to me that the second part is incorrect, but, unfortunately I can't figure out what should be done here.",,"['real-analysis', 'elementary-set-theory']"
10,proof that $f(x)=\sum_{n=1}^\infty \frac{\ln(x+n)}{x^2 + n^2}$ converges uniformily,proof that  converges uniformily,f(x)=\sum_{n=1}^\infty \frac{\ln(x+n)}{x^2 + n^2},"Prove that $f(x)=\sum_{n=1}^\infty \frac{\ln(x+n)}{x^2 + n^2}$  converges uniformly for $x\geq 0$. This exercise is in a text about uniform convergence, I've tried to use Weierstrass test with no success. trying to quote with $a_n=\sqrt{x+n}/(x^2 + n^2)$ so that $\ln(x+n)/(x^2 + n^2)\leq\sqrt{x+n}/(x^2 + n^2)\,,$ but then I can't find a criteria to prove that  $\sqrt{x+n}/(x^2 + n^2)$ converges (and if I prove this then $f(x)$ converges uniformly as Weierstrass test claims)","Prove that $f(x)=\sum_{n=1}^\infty \frac{\ln(x+n)}{x^2 + n^2}$  converges uniformly for $x\geq 0$. This exercise is in a text about uniform convergence, I've tried to use Weierstrass test with no success. trying to quote with $a_n=\sqrt{x+n}/(x^2 + n^2)$ so that $\ln(x+n)/(x^2 + n^2)\leq\sqrt{x+n}/(x^2 + n^2)\,,$ but then I can't find a criteria to prove that  $\sqrt{x+n}/(x^2 + n^2)$ converges (and if I prove this then $f(x)$ converges uniformly as Weierstrass test claims)",,"['real-analysis', 'sequences-and-series', 'convergence-divergence', 'summation']"
11,"If $f:\mathbb{R}\to\mathbb{R}$ is infinitely differentiable, then for all $x\in[0,1]$, $f^{(m)}(x)\neq 0$ for some $m$?","If  is infinitely differentiable, then for all ,  for some ?","f:\mathbb{R}\to\mathbb{R} x\in[0,1] f^{(m)}(x)\neq 0 m","This is question #2 from the Fall 2003 qualifying exam at my university. Let $f\colon\mathbb{R}\to\mathbb{R}$ be an infinitely differentiable function such that for all $x\in[0,1]$, there exists $m>0$ such that $f^{(m)}(x)\neq 0$. Prove that in fact there exists $M$ such that for all $x\in[0,1]$ there exists $0<m\leq M$ such that $f^{(m)}(x)\neq 0$. I had this idea; for each $x_i\in[0,1]$, there exists $m_i$ such that $f^{(m_i)}(x_i)\neq 0$. Since $f$ is infinitely differentiable, $f^{(m_i)}$ is continuous, so there exists $\epsilon_i$ such that $f^{(m_i)}\neq 0$ on $(x_i-\epsilon_i,x_i+\epsilon_i)$. Then $$ [0,1]\subseteq\bigcup_{i\in I}(x_i-\epsilon_i,x_i+\epsilon_i)$$ is an open covering of $[0,1]$, so by compactness,  $$ [0,1]\subseteq\bigcup_{i=1}^n(x_i-\epsilon_i,x_i+\epsilon_i)$$ after relabelling. Set $M=\max\{m_1,\dots,m_n\}$. For arbitrary $x\in [0,1]$, $x\in (x_j-\epsilon_j,x_j+\epsilon_j)$ for some $j$. Then $f^{(m_j)}(x)\neq 0$ on $(x_j-\epsilon_j,x_j+\epsilon_j)$, and $m_j\leq M$. Is this solution correct or can it be made better?","This is question #2 from the Fall 2003 qualifying exam at my university. Let $f\colon\mathbb{R}\to\mathbb{R}$ be an infinitely differentiable function such that for all $x\in[0,1]$, there exists $m>0$ such that $f^{(m)}(x)\neq 0$. Prove that in fact there exists $M$ such that for all $x\in[0,1]$ there exists $0<m\leq M$ such that $f^{(m)}(x)\neq 0$. I had this idea; for each $x_i\in[0,1]$, there exists $m_i$ such that $f^{(m_i)}(x_i)\neq 0$. Since $f$ is infinitely differentiable, $f^{(m_i)}$ is continuous, so there exists $\epsilon_i$ such that $f^{(m_i)}\neq 0$ on $(x_i-\epsilon_i,x_i+\epsilon_i)$. Then $$ [0,1]\subseteq\bigcup_{i\in I}(x_i-\epsilon_i,x_i+\epsilon_i)$$ is an open covering of $[0,1]$, so by compactness,  $$ [0,1]\subseteq\bigcup_{i=1}^n(x_i-\epsilon_i,x_i+\epsilon_i)$$ after relabelling. Set $M=\max\{m_1,\dots,m_n\}$. For arbitrary $x\in [0,1]$, $x\in (x_j-\epsilon_j,x_j+\epsilon_j)$ for some $j$. Then $f^{(m_j)}(x)\neq 0$ on $(x_j-\epsilon_j,x_j+\epsilon_j)$, and $m_j\leq M$. Is this solution correct or can it be made better?",,"['real-analysis', 'solution-verification']"
12,Non-rigorous (maybe a good pop-math) book about real analysis?,Non-rigorous (maybe a good pop-math) book about real analysis?,,"I am reading a real analysis book at the moment, but I feel the lack of something I believe that could be a good feature: Historical remarks about the concepts and also some chat about how those concepts are interconnected. I'm self-studying and most of the time I don't have someone to talk about those concepts - I would like to read some insights about the introduced concepts. The only book I know that is similar to what I'm look is Analysis by Its history . Are there more books such as this one? I'm reading Derbyshire's Unknown Quantity - for example. It would be nice to have a resource that talks about the concepts in analysis such as this book. I'm not looking specifically for a textbook on real analysis, it could be a non-serious book to be read together with a textbook. It's nice to have an idea of how the things are interconnected without spending hours trying to grasp the precise meaning of the concept.","I am reading a real analysis book at the moment, but I feel the lack of something I believe that could be a good feature: Historical remarks about the concepts and also some chat about how those concepts are interconnected. I'm self-studying and most of the time I don't have someone to talk about those concepts - I would like to read some insights about the introduced concepts. The only book I know that is similar to what I'm look is Analysis by Its history . Are there more books such as this one? I'm reading Derbyshire's Unknown Quantity - for example. It would be nice to have a resource that talks about the concepts in analysis such as this book. I'm not looking specifically for a textbook on real analysis, it could be a non-serious book to be read together with a textbook. It's nice to have an idea of how the things are interconnected without spending hours trying to grasp the precise meaning of the concept.",,"['real-analysis', 'reference-request', 'book-recommendation']"
13,An estimate for $\ln(1+f(x))$ using Taylor expansion,An estimate for  using Taylor expansion,\ln(1+f(x)),"A crucial skill for every aspiring analyst (like myself) is confidence in estimation - knowing when, where, and how to use tools like Big-and-little-O to gain quick upper bounds.  I'm trying to push myself to get better at it, but I'm still a little hesitant sometimes.  Here's an example: I need to determine if the following series converges uniformly $$ \sum_{n=1}^\infty f_n(x)=\sum_{n=1}^\infty\frac{\ln\left(1+\frac{\sin^2(nx)}{n^2}\right)}{n} $$ I'm pretty sure it does, and my reasoning is as follows: if $n>1$, $\sin^2(nx)/n^2<1$ for all $x\in\Bbb{R}$, and hence we can use the Taylor series for $\ln(1+z)$ about $z=0$ to obtain the following: $$ \left\vert\ln\left(1+\frac{\sin^2(nx)}{n^2}\right)\right\vert\leq\frac{1}{n^2}+\frac{C}{n^4}\tag{$\dagger$} $$ where $C$ is a constant independent of $x$.  Thus, $$ \left|\,f_n(x)\right|\leq\frac{1}{n^3}+\frac{C}{n^5} $$and since  $$ \sum_{n=1}^\infty\left( \frac{1}{n^3}+\frac{1}{n^5}\right)<\infty, $$the series converges uniformly. My question is: is $(\dagger)$ correct, and do I have the constant in the right place (or does it matter)?  I used the fact that $\ln(1+z)=z+O(z^2)$ to obtain $(\dagger)$.","A crucial skill for every aspiring analyst (like myself) is confidence in estimation - knowing when, where, and how to use tools like Big-and-little-O to gain quick upper bounds.  I'm trying to push myself to get better at it, but I'm still a little hesitant sometimes.  Here's an example: I need to determine if the following series converges uniformly $$ \sum_{n=1}^\infty f_n(x)=\sum_{n=1}^\infty\frac{\ln\left(1+\frac{\sin^2(nx)}{n^2}\right)}{n} $$ I'm pretty sure it does, and my reasoning is as follows: if $n>1$, $\sin^2(nx)/n^2<1$ for all $x\in\Bbb{R}$, and hence we can use the Taylor series for $\ln(1+z)$ about $z=0$ to obtain the following: $$ \left\vert\ln\left(1+\frac{\sin^2(nx)}{n^2}\right)\right\vert\leq\frac{1}{n^2}+\frac{C}{n^4}\tag{$\dagger$} $$ where $C$ is a constant independent of $x$.  Thus, $$ \left|\,f_n(x)\right|\leq\frac{1}{n^3}+\frac{C}{n^5} $$and since  $$ \sum_{n=1}^\infty\left( \frac{1}{n^3}+\frac{1}{n^5}\right)<\infty, $$the series converges uniformly. My question is: is $(\dagger)$ correct, and do I have the constant in the right place (or does it matter)?  I used the fact that $\ln(1+z)=z+O(z^2)$ to obtain $(\dagger)$.",,"['real-analysis', 'sequences-and-series', 'analysis']"
14,State-of-art of the Discrete Fourier Transform,State-of-art of the Discrete Fourier Transform,,"I would like to know what is the state-of-art in the research of the discrete Fourier transform. I have listed some questions to help answering, please add your own to make the list more comprehensive. 1) Is it possible to compute DFT faster than in $O(N\log N)$? Is there proofs for or against this? 2) For sparse inputs, I heard there is a faster than FFT algorithm now. Are there any other faster than FFT algorithms for some spesific input types? 3) Is it known how many additions and multiplications, respectively, is needed to compute the DFT for inputs of size $2^P$? Has a lower bound been proven (for either or both)? 4) What other improvements has been made during the past ten years or so? Proofs, explanations, and URLs to recent good papers are welcome. I know that I have several questions here, but I think it serves a good purpose. For someone who would like to enter the field, it is really difficult to find the correct things from the thousand's of publications and other materials in the internet.","I would like to know what is the state-of-art in the research of the discrete Fourier transform. I have listed some questions to help answering, please add your own to make the list more comprehensive. 1) Is it possible to compute DFT faster than in $O(N\log N)$? Is there proofs for or against this? 2) For sparse inputs, I heard there is a faster than FFT algorithm now. Are there any other faster than FFT algorithms for some spesific input types? 3) Is it known how many additions and multiplications, respectively, is needed to compute the DFT for inputs of size $2^P$? Has a lower bound been proven (for either or both)? 4) What other improvements has been made during the past ten years or so? Proofs, explanations, and URLs to recent good papers are welcome. I know that I have several questions here, but I think it serves a good purpose. For someone who would like to enter the field, it is really difficult to find the correct things from the thousand's of publications and other materials in the internet.",,"['real-analysis', 'reference-request', 'soft-question', 'fourier-analysis', 'fourier-series']"
15,Bounded slope for each point in compact subset,Bounded slope for each point in compact subset,,"Let $f$ be a function on $[a,b]$ . Let $K$ be a compact subset of $[a,b]$ on which $f$ is continuous. Suppose there exists $c>0$ such that for each $x\in K$ , there exists $h_x>0$ with $$\left|\frac{f(x+h_x)-f(x)}{h_x}\right|<c$$ Prove that there exists a finite subset $\{x_1,\ldots,x_n\}\subset K$ and positive numbers $h_1,\ldots,h_n$ such that (a) $x_1<x_1+h_1\leq x_2<x_2+h_2\leq x_3<\ldots$ (b) $\left|\dfrac{f(x_i+h_i)-f(x_i)}{h_i}\right|<c$ for $i=1,\ldots,n$ (c) $K\subset\bigcup_{i=1}^n[x_i,x_i+h_i]$ The function $f$ is continuous on the compact set $K$ , and so is bounded, and has a maximum and minimum on $K$ . I would like to find an open cover for $K$ , so that I can use the compactness property. I'm thinking about a set like $\{h\mid h>0$ and $ x+h\in K$ and $\left|\dfrac{f(x+h)-f(x)}{h}\right|<c\}$ for each $x\in K$ . But I can't seem to get something to work.","Let be a function on . Let be a compact subset of on which is continuous. Suppose there exists such that for each , there exists with Prove that there exists a finite subset and positive numbers such that (a) (b) for (c) The function is continuous on the compact set , and so is bounded, and has a maximum and minimum on . I would like to find an open cover for , so that I can use the compactness property. I'm thinking about a set like and and for each . But I can't seem to get something to work.","f [a,b] K [a,b] f c>0 x\in K h_x>0 \left|\frac{f(x+h_x)-f(x)}{h_x}\right|<c \{x_1,\ldots,x_n\}\subset K h_1,\ldots,h_n x_1<x_1+h_1\leq x_2<x_2+h_2\leq x_3<\ldots \left|\dfrac{f(x_i+h_i)-f(x_i)}{h_i}\right|<c i=1,\ldots,n K\subset\bigcup_{i=1}^n[x_i,x_i+h_i] f K K K \{h\mid h>0  x+h\in K \left|\dfrac{f(x+h)-f(x)}{h}\right|<c\} x\in K","['real-analysis', 'compactness']"
16,Definition simply connected in $\Bbb C$,Definition simply connected in,\Bbb C,I recently saw a different definition for simply connected which I had never seen before. A connected subset $\Omega\subset\Bbb C$ is called simply connected if the boundary is the image of a simple closed curve. Is this equivalent to the usual definitions? Thanks,I recently saw a different definition for simply connected which I had never seen before. A connected subset $\Omega\subset\Bbb C$ is called simply connected if the boundary is the image of a simple closed curve. Is this equivalent to the usual definitions? Thanks,,"['real-analysis', 'general-topology', 'complex-analysis']"
17,derivative of fourier transform,derivative of fourier transform,,"Let $f\in C^k$ and $f^{(k)}$ be absolutely integrable. I want to show for the fourier transform:  $$\widehat{f^{(k)}}(z)=(iz)^k\widehat{f}(z)$$ I want to prove it for $k=1$ and did the following: partial integration:  $$\begin{align*} \int_a^bf'(w)\exp(-iwz)dw&=f(b)\exp(-ibz)-f(a)\exp(-iaz)+iz\int_a^b f(w)\exp(-iwz)dw \end{align*}$$ The last term for $a,b\rightarrow\infty$ is the fourier transform of $f(z)$. But why are $\lim_{b\rightarrow\infty}f(b)=0$ and the same for $a$?","Let $f\in C^k$ and $f^{(k)}$ be absolutely integrable. I want to show for the fourier transform:  $$\widehat{f^{(k)}}(z)=(iz)^k\widehat{f}(z)$$ I want to prove it for $k=1$ and did the following: partial integration:  $$\begin{align*} \int_a^bf'(w)\exp(-iwz)dw&=f(b)\exp(-ibz)-f(a)\exp(-iaz)+iz\int_a^b f(w)\exp(-iwz)dw \end{align*}$$ The last term for $a,b\rightarrow\infty$ is the fourier transform of $f(z)$. But why are $\lim_{b\rightarrow\infty}f(b)=0$ and the same for $a$?",,['calculus']
18,Horn and spindle tori,Horn and spindle tori,,"I was trying to prove that the horn torus and the spindle torus are not manifolds by definition(locally diffeomorphic to some Euclidean space.). I have no idea how to do this, but I attempted it in the following way: I failed to show that a ""slice"" of manifold is a manifold itself. By a slice I mean, if the manifold $X\in \mathbf R^n$, then we set the coordinates $x_i=0$ for some $i$ where $0\leq i\leq n$. I feel this would work but I have no idea how to prove it. Then I look at the cross section for the tori I mentioned(this is equivalent to taking a slice.). For the horn torus, you have two circles touching each other. And the fore spindle torus, you have two circles intersecting each other. Since a circle is locally diffeomorphic to $\mathbf R^1$, the two circles better have to be diffeomorphic to $R^1$. Otherwise we get the result we want. I tried to proof that a neighbourhood around the point they touch(or intersect) cannot be locally diffeomorphic to $\mathbf R^1$. I tried to do it by contradiction. However I'm stuck on finding a contradiction... Any thoughts? P.S. Under the request of Sam Lisi, here are the definitions or horn and spindle tori: A torus is the set of points in $\mathbf R^3$ at a distance $b$ from the circle of radius a in the $xy$ plane. This is like you put a circle with radius $b$ in the $yz$ plane centred at $(a,0,0)$. Then you make it orbit around the origin and you get a torus. A horn torus is when $a=b$. If you take a cross-section, you'll find that it's two circles touching each other at one point. When $a<b$, it's called a spindle torus. The cross-section would look like two circles intersecting with each other at two pionts. There are pictures in this wikipedia article that might help you visualise the horn and spindle tori: http://en.wikipedia.org/wiki/Torus","I was trying to prove that the horn torus and the spindle torus are not manifolds by definition(locally diffeomorphic to some Euclidean space.). I have no idea how to do this, but I attempted it in the following way: I failed to show that a ""slice"" of manifold is a manifold itself. By a slice I mean, if the manifold $X\in \mathbf R^n$, then we set the coordinates $x_i=0$ for some $i$ where $0\leq i\leq n$. I feel this would work but I have no idea how to prove it. Then I look at the cross section for the tori I mentioned(this is equivalent to taking a slice.). For the horn torus, you have two circles touching each other. And the fore spindle torus, you have two circles intersecting each other. Since a circle is locally diffeomorphic to $\mathbf R^1$, the two circles better have to be diffeomorphic to $R^1$. Otherwise we get the result we want. I tried to proof that a neighbourhood around the point they touch(or intersect) cannot be locally diffeomorphic to $\mathbf R^1$. I tried to do it by contradiction. However I'm stuck on finding a contradiction... Any thoughts? P.S. Under the request of Sam Lisi, here are the definitions or horn and spindle tori: A torus is the set of points in $\mathbf R^3$ at a distance $b$ from the circle of radius a in the $xy$ plane. This is like you put a circle with radius $b$ in the $yz$ plane centred at $(a,0,0)$. Then you make it orbit around the origin and you get a torus. A horn torus is when $a=b$. If you take a cross-section, you'll find that it's two circles touching each other at one point. When $a<b$, it's called a spindle torus. The cross-section would look like two circles intersecting with each other at two pionts. There are pictures in this wikipedia article that might help you visualise the horn and spindle tori: http://en.wikipedia.org/wiki/Torus",,"['real-analysis', 'general-topology', 'geometry', 'differential-geometry', 'manifolds']"
19,"Continuity of a function defined differently on $\mathbb Q,\mathbb R\setminus \mathbb Q$",Continuity of a function defined differently on,"\mathbb Q,\mathbb R\setminus \mathbb Q","Let's say I define the function $f(x)=2^x$ for rational $x$, and $f(x)=1$ for irrational $x$. My question is: is this function continuous everywhere? I think it's not, because for any $2$ irrational numbers you can find a rational in between, and for any $2$ rationals you can find an irrational in between. My second question is: is the function continuous and differentiable at $x=0$? I think it is, and I think that $f'(0)=0$ because of the picture . Am I correct? (By the way please keep the answers at a pretty low level, I'm a calculus AB student.) Thanks!","Let's say I define the function $f(x)=2^x$ for rational $x$, and $f(x)=1$ for irrational $x$. My question is: is this function continuous everywhere? I think it's not, because for any $2$ irrational numbers you can find a rational in between, and for any $2$ rationals you can find an irrational in between. My second question is: is the function continuous and differentiable at $x=0$? I think it is, and I think that $f'(0)=0$ because of the picture . Am I correct? (By the way please keep the answers at a pretty low level, I'm a calculus AB student.) Thanks!",,"['calculus', 'real-analysis', 'analysis']"
20,"Is the Cauchy principal value ""invariant"" under change of variables?","Is the Cauchy principal value ""invariant"" under change of variables?",,"Let $f \in C^{\gamma}_c(\mathbb{R}) $. Let $K:\mathbb{R}^n \backslash \{\vec{0}\} \rightarrow \mathbb{R}^n$ be a singular integral kernel with the following properties: 1) K smooth everywhere except at $\vec{0}$ 2) K homogeneous of degree $-n$, in particular $|K(x)| \leq \frac{c}{|x|^{n}}$ 3) K has mean value zero on the unit sphere, ie $\int_{|x|=1}K(x)dS=0$ I was wondering if the Cauchy principal value of the convolution of $K$ with $f$ is ""invariant"" under a change of variables. That is, for a $C^1$ diffeomorphism $G: \mathbb{R}^n \rightarrow \mathbb{R}^n$, denoting $y=G(w)$ and $x=G(v)$, do we have: \begin{eqnarray} \text{P.V.} \int_{\mathbb{R}^n} K(x-y)f(y)dy &\equiv& \lim_{\delta \searrow 0} \int_{|x-y|> \delta} K(x-y)f(y)dy \\ &=& \lim_{\delta \searrow 0} \int_{|v-w|> \delta} K \left(x-G(w) \right)f \left( G(w) \right) \left|\det \nabla G(w) \right| dw \quad \text{?} \end{eqnarray}","Let $f \in C^{\gamma}_c(\mathbb{R}) $. Let $K:\mathbb{R}^n \backslash \{\vec{0}\} \rightarrow \mathbb{R}^n$ be a singular integral kernel with the following properties: 1) K smooth everywhere except at $\vec{0}$ 2) K homogeneous of degree $-n$, in particular $|K(x)| \leq \frac{c}{|x|^{n}}$ 3) K has mean value zero on the unit sphere, ie $\int_{|x|=1}K(x)dS=0$ I was wondering if the Cauchy principal value of the convolution of $K$ with $f$ is ""invariant"" under a change of variables. That is, for a $C^1$ diffeomorphism $G: \mathbb{R}^n \rightarrow \mathbb{R}^n$, denoting $y=G(w)$ and $x=G(v)$, do we have: \begin{eqnarray} \text{P.V.} \int_{\mathbb{R}^n} K(x-y)f(y)dy &\equiv& \lim_{\delta \searrow 0} \int_{|x-y|> \delta} K(x-y)f(y)dy \\ &=& \lim_{\delta \searrow 0} \int_{|v-w|> \delta} K \left(x-G(w) \right)f \left( G(w) \right) \left|\det \nabla G(w) \right| dw \quad \text{?} \end{eqnarray}",,"['real-analysis', 'functional-analysis', 'singular-integrals']"
21,Discontinuous for rationals,Discontinuous for rationals,,"Show that $f\left(x\right):=\sum_{n=1}^{\infty}\frac{\left\{nx\right\}}{n^2}$, where $\left\{nx\right\}$ is the fractional part of $nx$, is discontinuous for all rationals. I guess it would be nice to somehow show that for any arbitrarily small irrational $r$, $f\left(\frac{a}{b}\right)<f\left(\frac{a}{b}+r\right)$, but I am not sure how to do that precisely (though it makes sense intuitively since $\left\{n\frac{a}{b}\right\}=0$ for all $n=mb$ for some $m\in\mathbb{N}$ whereas $\left\{n\left(\frac{a}{b}+r\right)\right\}\ne0\,\forall\,n\in\mathbb{N}$.) Out of curiosity I tried beginning by computing the value of $f\left(x\right)$ for $x\in\mathbb{Q}$. My analysis showed that if $x=\frac{a}{b}$, where (without loss of generality) $\frac{a}{b}\in\left[0,\,1\right]$ and (again, without loss of generality) $\gcd\left(a,\,b\right)=1$ then: $$f\left(\frac{a}{b}\right)=\frac{a}{b^3}\sum_{l=1}^{b-1}l\frac{d}{dx}\left(\ln\left(\Gamma\left(\frac{l}{b}\right)\right)\right)$$ But I don't think that's relevant. Note: this question comes from Rudin's Principles of Mathematical Analysis Exercise 7.10.","Show that $f\left(x\right):=\sum_{n=1}^{\infty}\frac{\left\{nx\right\}}{n^2}$, where $\left\{nx\right\}$ is the fractional part of $nx$, is discontinuous for all rationals. I guess it would be nice to somehow show that for any arbitrarily small irrational $r$, $f\left(\frac{a}{b}\right)<f\left(\frac{a}{b}+r\right)$, but I am not sure how to do that precisely (though it makes sense intuitively since $\left\{n\frac{a}{b}\right\}=0$ for all $n=mb$ for some $m\in\mathbb{N}$ whereas $\left\{n\left(\frac{a}{b}+r\right)\right\}\ne0\,\forall\,n\in\mathbb{N}$.) Out of curiosity I tried beginning by computing the value of $f\left(x\right)$ for $x\in\mathbb{Q}$. My analysis showed that if $x=\frac{a}{b}$, where (without loss of generality) $\frac{a}{b}\in\left[0,\,1\right]$ and (again, without loss of generality) $\gcd\left(a,\,b\right)=1$ then: $$f\left(\frac{a}{b}\right)=\frac{a}{b^3}\sum_{l=1}^{b-1}l\frac{d}{dx}\left(\ln\left(\Gamma\left(\frac{l}{b}\right)\right)\right)$$ But I don't think that's relevant. Note: this question comes from Rudin's Principles of Mathematical Analysis Exercise 7.10.",,"['calculus', 'real-analysis', 'sequences-and-series', 'analysis', 'continuity']"
22,"Prove if $g$ has a fixed point in $(0,1)$, then $g^{\prime}(1) > 1$.","Prove if  has a fixed point in , then .","g (0,1) g^{\prime}(1) > 1","Let $g : [0, 1] \rightarrow \mathbb{R}$ be twice differentiable with $g^{\prime \prime}(x) > 0$ for all $x \in [0,1]$. Suppose that $g(0) > 0$ and $g(1) = 1$. Prove if $g$ has a fixed point in $(0,1)$, then $g^{\prime}(1) > 1$. My attempt: Define a function $h(x)=g(x)-x$. Since $g$ has a fixed point, say $c \in (0,1)$, we have $h(c)=g(c)-c=0$. Notice that we have $h(c)=h(1)=0$, by Rolle's Theorem, there exists $d \in (c,1)$ such that $h^{\prime}(d)=0$ Applying Mean Value Theorem on $h$ on $[d,1]$, there exists $e \in (d,1)$ such that $h^{\prime \prime}(e)=\frac{h^{\prime}(d)-h^{\prime}(1)}{c-1}$. Notice that we have $h^{\prime \prime}(x)=g^{\prime \prime}(x) >0 $ for all $x \in [0,1]$. Hence, $-h^{\prime}(1)<0 \implies g^{\prime}(1) > 1$ Can anyone explain to me why we need to use Rolle's theorem here?","Let $g : [0, 1] \rightarrow \mathbb{R}$ be twice differentiable with $g^{\prime \prime}(x) > 0$ for all $x \in [0,1]$. Suppose that $g(0) > 0$ and $g(1) = 1$. Prove if $g$ has a fixed point in $(0,1)$, then $g^{\prime}(1) > 1$. My attempt: Define a function $h(x)=g(x)-x$. Since $g$ has a fixed point, say $c \in (0,1)$, we have $h(c)=g(c)-c=0$. Notice that we have $h(c)=h(1)=0$, by Rolle's Theorem, there exists $d \in (c,1)$ such that $h^{\prime}(d)=0$ Applying Mean Value Theorem on $h$ on $[d,1]$, there exists $e \in (d,1)$ such that $h^{\prime \prime}(e)=\frac{h^{\prime}(d)-h^{\prime}(1)}{c-1}$. Notice that we have $h^{\prime \prime}(x)=g^{\prime \prime}(x) >0 $ for all $x \in [0,1]$. Hence, $-h^{\prime}(1)<0 \implies g^{\prime}(1) > 1$ Can anyone explain to me why we need to use Rolle's theorem here?",,['real-analysis']
23,Rational approximations to $\sqrt 2$,Rational approximations to,\sqrt 2,"I find this problem is very interesting, but now I can't solve it. Given $n$ a positive integer, let $$f(n)=\min_{m\in\Bbb Z}{\left\lvert\sqrt{2}-\dfrac{m}{n}\right\rvert}.$$ If there is a sequence of positive integers $n_{1}<n_{2}<\cdots<n_{l}<\cdots$ and there is a constant $C$ such that $$f(n_{i})<\dfrac{C}{n^2_{i}},i=1,2,\cdots,$$ prove that there exists some $q>1$ such that $$n_{i}\ge q^{i-1},i=1,2,\cdots.$$","I find this problem is very interesting, but now I can't solve it. Given $n$ a positive integer, let $$f(n)=\min_{m\in\Bbb Z}{\left\lvert\sqrt{2}-\dfrac{m}{n}\right\rvert}.$$ If there is a sequence of positive integers $n_{1}<n_{2}<\cdots<n_{l}<\cdots$ and there is a constant $C$ such that $$f(n_{i})<\dfrac{C}{n^2_{i}},i=1,2,\cdots,$$ prove that there exists some $q>1$ such that $$n_{i}\ge q^{i-1},i=1,2,\cdots.$$",,"['real-analysis', 'number-theory']"
24,"If $f:\mathbb{R}\to\mathbb{R}$ is continuous and $[a,b]\subset f([c,d])$, how to prove there is some $[r,s]$ such that $f([r,s])=[a,b]$?","If  is continuous and , how to prove there is some  such that ?","f:\mathbb{R}\to\mathbb{R} [a,b]\subset f([c,d]) [r,s] f([r,s])=[a,b]","Let  $f:\mathbb R\to\mathbb R$ satisfy the following: $f$ is continuous there exist closed intervals $[a,b]$ and $[c,d]$ such that $[a,b]\subset f([c,d])$ How to prove that there exists $[r,s]\subset [c,d]$ such that $f([r,s])=[a,b]$ ? Thanks in advance.","Let  $f:\mathbb R\to\mathbb R$ satisfy the following: $f$ is continuous there exist closed intervals $[a,b]$ and $[c,d]$ such that $[a,b]\subset f([c,d])$ How to prove that there exists $[r,s]\subset [c,d]$ such that $f([r,s])=[a,b]$ ? Thanks in advance.",,"['real-analysis', 'general-topology', 'analysis', 'contest-math']"
25,Alternative proof of Monotone Convergence Theorem,Alternative proof of Monotone Convergence Theorem,,"Let $(X, \mathfrak{A}, \mu)$ be a measure space. Show that the Monotone Convergence Theorem holds if $f$, $f_{1}$, $f_{2}$, ... are real-valued measurable functions and $f_{1}$ is integrable. Further, (1) $f_{1} \leq f_{2}...$ and (2) f = $\lim_{n} f_{n}$ hold almost everywhere. My try: Assume (1) and (2) hold everywhere. From the monotonicity of the integral we have $\int f_{1} d\mu \leq \int f_{2} d\mu \leq$ .. $\leq \int f d\mu$. We need to show the reverse inequality. Let $g^{+}_{n,k}$  and $g^{-}_{n,k}$ be two sequences of positive valued simple functions such that $\lim_{k} g^{+}_{n,k} - g^{-}_{n,k} = f_{n}$ and $g^{+}_{n,k} - g^{-}_{n,k} \leq f_{n}$ for all $k$. Define $h_{n}^{+}$ and $h_{n}^{-}$ as $h^{+}_{n} = \max_{k}(g_{k,n}^{+}$) and $h^{-}_{n} = \max_{k}(-g^{-}_{k,n})$, then both $h_{n}^{+}$ and $h^{-}_{n}$ are non-decreasing measurable simple functions and $\lim_{n} h^{+}_{n} + h^{-}_{n } = f^{+} - f^{-} = f$. Also $h^{+}_{n} \leq f^{+}_{n}$ and $h^{-}_{n} \leq -f^{-}_{n}.$ Since $h^{+}_{n}$ and $h^{-}_{n}$ are simple functions, it follows $\lim_n \int h_{n}^{+} d\mu = \int f^{+} d\mu$ and $ \lim_{n} \int h^{-}_{n} d\mu = -\int f^{-} d\mu$ and $\lim_{n} \int h^{+}_{n} + h^{-}_{n} d\mu = \int f d\mu$. Since $\int f^{+} d\mu = \lim_{n} h^{+}_{n} d\mu \leq \lim_{n} \int f^{+}_{n}d\mu$ and similary $-\int f^{-}d\mu = \lim_{n} \int h^{-}_{n} d\mu \leq \lim_{n} -\int f^{-}_{n}d\mu$ $\int f d\mu = \lim_{n} \int h^{+}_{n} + h^{-}_{n} d\mu \leq \lim_{n} \int f^{+}_{n} - f^{-}_{n} d\mu = \int f_{n}d\mu$. Assume now (1) and (2) hold almost everywhere, let $N$ be the set consisting all $x\in X$ for which at least one of (1) and (2) fails. Then the functions $f\chi_{n^{C}}$ and $f_{n}\chi_{N^{C}}$ fulfills (1) and (2) and thus $\int f\chi_{N^{C}} d\mu = \lim_{n} \int f_{n}\chi_{N^{C}}d\mu$ and from this we can conclude that $\int f d\mu = \lim_{n} \int f_{n}d\mu$. This is very similar to a proof for MCT in the book Measure Theory by Donald Cohn, however, the book is for positive valued functions only. My question is whether this is correct or if I have missed something. Secondly, why is the constraint that $f_{1}$ is integrable necessary?","Let $(X, \mathfrak{A}, \mu)$ be a measure space. Show that the Monotone Convergence Theorem holds if $f$, $f_{1}$, $f_{2}$, ... are real-valued measurable functions and $f_{1}$ is integrable. Further, (1) $f_{1} \leq f_{2}...$ and (2) f = $\lim_{n} f_{n}$ hold almost everywhere. My try: Assume (1) and (2) hold everywhere. From the monotonicity of the integral we have $\int f_{1} d\mu \leq \int f_{2} d\mu \leq$ .. $\leq \int f d\mu$. We need to show the reverse inequality. Let $g^{+}_{n,k}$  and $g^{-}_{n,k}$ be two sequences of positive valued simple functions such that $\lim_{k} g^{+}_{n,k} - g^{-}_{n,k} = f_{n}$ and $g^{+}_{n,k} - g^{-}_{n,k} \leq f_{n}$ for all $k$. Define $h_{n}^{+}$ and $h_{n}^{-}$ as $h^{+}_{n} = \max_{k}(g_{k,n}^{+}$) and $h^{-}_{n} = \max_{k}(-g^{-}_{k,n})$, then both $h_{n}^{+}$ and $h^{-}_{n}$ are non-decreasing measurable simple functions and $\lim_{n} h^{+}_{n} + h^{-}_{n } = f^{+} - f^{-} = f$. Also $h^{+}_{n} \leq f^{+}_{n}$ and $h^{-}_{n} \leq -f^{-}_{n}.$ Since $h^{+}_{n}$ and $h^{-}_{n}$ are simple functions, it follows $\lim_n \int h_{n}^{+} d\mu = \int f^{+} d\mu$ and $ \lim_{n} \int h^{-}_{n} d\mu = -\int f^{-} d\mu$ and $\lim_{n} \int h^{+}_{n} + h^{-}_{n} d\mu = \int f d\mu$. Since $\int f^{+} d\mu = \lim_{n} h^{+}_{n} d\mu \leq \lim_{n} \int f^{+}_{n}d\mu$ and similary $-\int f^{-}d\mu = \lim_{n} \int h^{-}_{n} d\mu \leq \lim_{n} -\int f^{-}_{n}d\mu$ $\int f d\mu = \lim_{n} \int h^{+}_{n} + h^{-}_{n} d\mu \leq \lim_{n} \int f^{+}_{n} - f^{-}_{n} d\mu = \int f_{n}d\mu$. Assume now (1) and (2) hold almost everywhere, let $N$ be the set consisting all $x\in X$ for which at least one of (1) and (2) fails. Then the functions $f\chi_{n^{C}}$ and $f_{n}\chi_{N^{C}}$ fulfills (1) and (2) and thus $\int f\chi_{N^{C}} d\mu = \lim_{n} \int f_{n}\chi_{N^{C}}d\mu$ and from this we can conclude that $\int f d\mu = \lim_{n} \int f_{n}d\mu$. This is very similar to a proof for MCT in the book Measure Theory by Donald Cohn, however, the book is for positive valued functions only. My question is whether this is correct or if I have missed something. Secondly, why is the constraint that $f_{1}$ is integrable necessary?",,"['real-analysis', 'measure-theory']"
26,Integration theory,Integration theory,,"Any help with this problem is appreciated. Given the $f$ is measurable and finite a.e. on $[0,1]$. Then prove the following statements $$ \int_E f = 0 \text{ for all measurable $E \subset [0,1]$ with $\mu(E) = 1/2$ }\Rightarrow f = 0 \text{ a.e. on } [0,1]$$ $$ f > 0 \text{ a.e. } \Rightarrow \inf ~ \left\{\int_E f : \mu(E) \geq 1/2\right\} > 0 $$","Any help with this problem is appreciated. Given the $f$ is measurable and finite a.e. on $[0,1]$. Then prove the following statements $$ \int_E f = 0 \text{ for all measurable $E \subset [0,1]$ with $\mu(E) = 1/2$ }\Rightarrow f = 0 \text{ a.e. on } [0,1]$$ $$ f > 0 \text{ a.e. } \Rightarrow \inf ~ \left\{\int_E f : \mu(E) \geq 1/2\right\} > 0 $$",,"['real-analysis', 'measure-theory', 'integration']"
27,Proving {$b_n$}$_{n=1}^\infty$ converges given {$a_n$}$_{n=1}^\infty$ and {$a_n b_n$}$_{n=1}^\infty$,Proving {} converges given {} and {},b_n _{n=1}^\infty a_n _{n=1}^\infty a_n b_n _{n=1}^\infty,"Suppose {$a_n$}$_{n=1}^\infty$ and  {$b_n$}$_{n=1}^\infty$ are sequences such that {$a_n$}$_{n=1}^\infty$ coverges to A$\neq$0 and {$a_n b_n$}$_{n=1}^\infty$ converges.  Prove that {$b_n$}$_{n=1}^\infty$ converges. What I have so far: $b_n = {a_n b_n \over a_n}$ $\to$ $C \over A$, $A\neq0$ |$b_n - {C \over A}$| = |${a_n b_n \over a_n} - {C \over A}$| = |${Aa_nb_n - Ca_n \over Aa_n}$| $ \leq $ |${1 \over Aa_n}||Aa_nb_n - Ca_n$|=|${1 \over Aa_n}||a_n(Ab_n - C)$| $\leq |{1 \over Aa_n}||a_n||(Ab_n - C)$|.  Note: since $a_n$ converges, there is M>0 such that |$a_n| \leq$M for all n $ \in\Bbb N$. Thus, |${1 \over Aa_n}||a_n||(Ab_n - C)$| = |${1 \over M}||M||(Ab_n - C)$|.  And this is where I get lost.  Any thoughts? Or am I completely wrong to begin with?","Suppose {$a_n$}$_{n=1}^\infty$ and  {$b_n$}$_{n=1}^\infty$ are sequences such that {$a_n$}$_{n=1}^\infty$ coverges to A$\neq$0 and {$a_n b_n$}$_{n=1}^\infty$ converges.  Prove that {$b_n$}$_{n=1}^\infty$ converges. What I have so far: $b_n = {a_n b_n \over a_n}$ $\to$ $C \over A$, $A\neq0$ |$b_n - {C \over A}$| = |${a_n b_n \over a_n} - {C \over A}$| = |${Aa_nb_n - Ca_n \over Aa_n}$| $ \leq $ |${1 \over Aa_n}||Aa_nb_n - Ca_n$|=|${1 \over Aa_n}||a_n(Ab_n - C)$| $\leq |{1 \over Aa_n}||a_n||(Ab_n - C)$|.  Note: since $a_n$ converges, there is M>0 such that |$a_n| \leq$M for all n $ \in\Bbb N$. Thus, |${1 \over Aa_n}||a_n||(Ab_n - C)$| = |${1 \over M}||M||(Ab_n - C)$|.  And this is where I get lost.  Any thoughts? Or am I completely wrong to begin with?",,"['real-analysis', 'proof-writing']"
28,Equivalent characterization of chain connectedness of a metric space,Equivalent characterization of chain connectedness of a metric space,,"I'm having difficulty with proof. It is that the following is an equivalent characterization of chain connectedness for a metric space $M$: Point-wise boundedness at a point of an equicontinuous family of functions (from M to the real numbers) implies pointwise boundedness at all points. Pointwise boundedness means boundedness of the set of values of all functions at a given point. Now, my attempt was to construct an equicontinuous family of functions that could be useful. For instance, you can take $A$ be the set of all points chain connected to the point $a\in M$. Then you can take $f_n$ defined as $0$ on $A$ and $n$ otherwise. The functions $f_n$ are bounded at $a$, so equicontinuity would give us the desired result. The challenge I'm facing is showing equicontinuity. Plus, it does not seem like a good choice of functions. For instance, take $$M=\{r\in\mathbb R\, |\, r<0, r=0 \text{ or } r=1/n \text{ for all }n \text{  natural}\}.$$ Use the standard Euclidean metric on $\mathbb R$. Take $a=0$. It seems like you can find $x$ and $y$ arbitrarily close such that $f_n(x)$ and $f_n(y)$ still differ by $n$. Thank you! So I think the example I gave shows that the distances between A and the rest of the chain connected components can have a zero infimum, which is what makes me skeptical that this will work. So do you think the choice functions is bad? EDIT: Could a better choice of A, i.e. a better choice of a, be helpful?","I'm having difficulty with proof. It is that the following is an equivalent characterization of chain connectedness for a metric space $M$: Point-wise boundedness at a point of an equicontinuous family of functions (from M to the real numbers) implies pointwise boundedness at all points. Pointwise boundedness means boundedness of the set of values of all functions at a given point. Now, my attempt was to construct an equicontinuous family of functions that could be useful. For instance, you can take $A$ be the set of all points chain connected to the point $a\in M$. Then you can take $f_n$ defined as $0$ on $A$ and $n$ otherwise. The functions $f_n$ are bounded at $a$, so equicontinuity would give us the desired result. The challenge I'm facing is showing equicontinuity. Plus, it does not seem like a good choice of functions. For instance, take $$M=\{r\in\mathbb R\, |\, r<0, r=0 \text{ or } r=1/n \text{ for all }n \text{  natural}\}.$$ Use the standard Euclidean metric on $\mathbb R$. Take $a=0$. It seems like you can find $x$ and $y$ arbitrarily close such that $f_n(x)$ and $f_n(y)$ still differ by $n$. Thank you! So I think the example I gave shows that the distances between A and the rest of the chain connected components can have a zero infimum, which is what makes me skeptical that this will work. So do you think the choice functions is bad? EDIT: Could a better choice of A, i.e. a better choice of a, be helpful?",,"['real-analysis', 'general-topology', 'metric-spaces']"
29,"$f:(0,\infty)\rightarrow (0,\infty)$ is continuous, $\lim_{n\rightarrow\infty}f(nx)=0$ then $\lim_{x\rightarrow\infty}f(x)=0$","is continuous,  then","f:(0,\infty)\rightarrow (0,\infty) \lim_{n\rightarrow\infty}f(nx)=0 \lim_{x\rightarrow\infty}f(x)=0","Could any one tell me how to solve this one? $f:(0,\infty)\rightarrow (0,\infty)$ is continuos, $\lim_{n\rightarrow\infty}f(nx)=0$ then $\lim_{x\rightarrow\infty}f(x)=0$ I have no clue how to solve this one. Thank you.","Could any one tell me how to solve this one? $f:(0,\infty)\rightarrow (0,\infty)$ is continuos, $\lim_{n\rightarrow\infty}f(nx)=0$ then $\lim_{x\rightarrow\infty}f(x)=0$ I have no clue how to solve this one. Thank you.",,['real-analysis']
30,What's the relationship between interior/exterior/boundary point and limit point?,What's the relationship between interior/exterior/boundary point and limit point?,,I'm learning real analysis. I found that there are two classification of points: interior/exterior/boundary point and limit point. What's the relationship between interior/exterior/boundary point and limit point ?,I'm learning real analysis. I found that there are two classification of points: interior/exterior/boundary point and limit point. What's the relationship between interior/exterior/boundary point and limit point ?,,['real-analysis']
31,Am I missing something in Rudin' PMA Theorems 2.38 - 2.40?,Am I missing something in Rudin' PMA Theorems 2.38 - 2.40?,,"So I'm reviewing Rudin's Principles of Mathematical Analysis in studying for an Intro Exam for PhD program. I'm in Chapter 2, specifically theorems 2.38 - 2.40. Having independently done the proofs prior to these theorems, I am wondering why a simpler approach is not warranted. Consider Theorem 2.38: If $\{I_n\}$ is a sequence of intervals in $R^1$ , such that $I_n$ contains $I_{n+1}$ ( $n = 1, 2, 3, \dots$ ), then infinite intersection of the sets $I_n$ is not empty. By Rudin's definition of interval in definition 2.17, the interval is a closed set in $R^1$ . Since $R^1$ is a metric space, the intervals $I_n$ are thus compact sets. So isn't Theorem 2.38 just the corollary to Theorem 2.36 directly above on the page (see below)? Why go through the proof Rudin uses to prove Theorem 2.38. 2.36  Theorem : If $\{K_\alpha\}$ is a collection of compact subsets of a metric space $X$ such that the intersection of every finite subcollection of $\{K_\alpha\}$ is nonempty, then $\bigcap K_\alpha$ is nonempty. [proof omitted] Corollary : If $\{K_n\}$ is a sequence of nonempty compact sets such that $K_n \supset K_{n+1}\, (n = 1, 2, 3, \dots)$ , then $\bigcap K_n$ is not empty. I must be missing something basic?","So I'm reviewing Rudin's Principles of Mathematical Analysis in studying for an Intro Exam for PhD program. I'm in Chapter 2, specifically theorems 2.38 - 2.40. Having independently done the proofs prior to these theorems, I am wondering why a simpler approach is not warranted. Consider Theorem 2.38: If is a sequence of intervals in , such that contains ( ), then infinite intersection of the sets is not empty. By Rudin's definition of interval in definition 2.17, the interval is a closed set in . Since is a metric space, the intervals are thus compact sets. So isn't Theorem 2.38 just the corollary to Theorem 2.36 directly above on the page (see below)? Why go through the proof Rudin uses to prove Theorem 2.38. 2.36  Theorem : If is a collection of compact subsets of a metric space such that the intersection of every finite subcollection of is nonempty, then is nonempty. [proof omitted] Corollary : If is a sequence of nonempty compact sets such that , then is not empty. I must be missing something basic?","\{I_n\} R^1 I_n I_{n+1} n = 1, 2, 3, \dots I_n R^1 R^1 I_n \{K_\alpha\} X \{K_\alpha\} \bigcap K_\alpha \{K_n\} K_n \supset K_{n+1}\, (n = 1, 2, 3, \dots) \bigcap K_n","['real-analysis', 'general-topology']"
32,For which p the series converge?,For which p the series converge?,,$$\sum_{n=0}^{\infty}\left(\frac{1}{n!}\right)^{p}$$ Please verify answer below,$$\sum_{n=0}^{\infty}\left(\frac{1}{n!}\right)^{p}$$ Please verify answer below,,"['real-analysis', 'convergence-divergence']"
33,a problem on continuity on rational and irrational point,a problem on continuity on rational and irrational point,,"[NBHM_2006_PhD Screening Test_Topology] Let $f$ be the function on $\mathbb{R}$ defined by $f(t) = \frac{p+\sqrt{2}}{q+\sqrt{2}}−\frac{p}{q}$ if $t = \frac{p}{q}$ with $p, q \in \mathbb{Z}$ and $p$ and $q$ coprime to each other, and $f(t) = 0$ if $t$ is irrational. Answer the following: At which irrational numbers $t$ is $f$ is continuous? At which rational numbers $t$ is $f$ continuous? I am totally stuck in this problem. How should I able to solve this problem?","[NBHM_2006_PhD Screening Test_Topology] Let be the function on defined by if with and and coprime to each other, and if is irrational. Answer the following: At which irrational numbers is is continuous? At which rational numbers is continuous? I am totally stuck in this problem. How should I able to solve this problem?","f \mathbb{R} f(t) = \frac{p+\sqrt{2}}{q+\sqrt{2}}−\frac{p}{q} t = \frac{p}{q} p, q \in \mathbb{Z} p q f(t) = 0 t t f t f",['real-analysis']
34,Solve $y''+(1-2x \cos x \cos 2x)y=0$,Solve,y''+(1-2x \cos x \cos 2x)y=0,Solve the differential equation $$y''+(1-2x \cos x \cos 2x)y=0 \space $$,Solve the differential equation $$y''+(1-2x \cos x \cos 2x)y=0 \space $$,,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
35,is it possible to find the closest rational number to an irrational number?,is it possible to find the closest rational number to an irrational number?,,"given an irrational number is it possible to find the closest rational number to the irrational number? If so, how?","given an irrational number is it possible to find the closest rational number to the irrational number? If so, how?",,['real-analysis']
36,Conditions for the mean value theorem,Conditions for the mean value theorem,,"the mean value theorem which most of us know starts with the conditions that $f$ is continuous on the closed interval $[a,b]$ and differentiable on the opened interval $(a,b)$, then there exists a $c \in (a,b)$, where  $\frac{f(b)-f(a)}{b-a} = f'(c)$. I'm guessing we're then able to define $\forall a', b' \in [a,b]$ where $c \in (a', b')$ and the mean value theorem is correspondingly valid. However, are we then able to start with $\forall c \in (a,b)$ and then claim that there exist $a',b' \in [a,b]$, where the mean value theorem is still valid?","the mean value theorem which most of us know starts with the conditions that $f$ is continuous on the closed interval $[a,b]$ and differentiable on the opened interval $(a,b)$, then there exists a $c \in (a,b)$, where  $\frac{f(b)-f(a)}{b-a} = f'(c)$. I'm guessing we're then able to define $\forall a', b' \in [a,b]$ where $c \in (a', b')$ and the mean value theorem is correspondingly valid. However, are we then able to start with $\forall c \in (a,b)$ and then claim that there exist $a',b' \in [a,b]$, where the mean value theorem is still valid?",,"['real-analysis', 'derivatives']"
37,"Determine conditions for $a,b>0$ such that $f(x)=\sum b^n\sin(a^nx)$ be continuous but nowhere differentiable in $\mathbb{R}$",Determine conditions for  such that  be continuous but nowhere differentiable in,"a,b>0 f(x)=\sum b^n\sin(a^nx) \mathbb{R}","Determine conditions for $a,b>0$ such that $f(x)=\sum b^n\sin(a^nx)$ be continuous but nowhere differentiable in $\mathbb{R}$. Attempt: If $0<b<1$ the function is clearly continuous by M-test and uniform convergence of continuous functions.","Determine conditions for $a,b>0$ such that $f(x)=\sum b^n\sin(a^nx)$ be continuous but nowhere differentiable in $\mathbb{R}$. Attempt: If $0<b<1$ the function is clearly continuous by M-test and uniform convergence of continuous functions.",,['real-analysis']
38,Showing a function is not of a bounded variation.,Showing a function is not of a bounded variation.,,"$V_a^b(P,f):=\sum_{i=1}^k|f(x_i)-f(x_{i-1})|$, where $P$ is a partition. $$f(x)= \begin{cases}        x^2\sin(\frac{1}{x^2}), &\text{if } x\neq0, \\       0, &\text{if } x=0     \end{cases} $$ does not have bounded variation over $[-1,1]$. I'm trying to show this without using the fact that it may not have bounded variation over some subset of $[-1,1]$ . I want to find a partition $\lbrace x_0,\dotsc,x_n \rbrace$ of $[-1,1]$ for which  $$\sum_{i=1}^n|f(x_i)-f(x_{i-1})|$$  gives a partial sum of the Harmonic Series. My partition is $P=\lbrace -1,0,\frac{1}{\sqrt{\frac{\pi}{2}+\pi n}},\frac{1}{\sqrt{\frac{\pi}{2}+\pi(n-1)}},\dotsc,\frac{1}{\sqrt{\frac{\pi}{2}}},1\rbrace$. $$\begin{align} V_{-1} ^1(P,f)    &=|f(0)-f(-1)| + \left|(f\Bigl(\frac1{\sqrt{\smash[b]{\frac\pi2}+\pi n}}\Bigr)-f(0)\right|+\left|f(1)-f\Bigl(\frac1{\sqrt{\smash[b]{\frac\pi2}}}\Bigr)\right| \\   &\mathrel{\phantom=} +\sum_{i=0}^{n}\left|f\Bigl(\frac1{\sqrt{\smash[b]{\frac\pi2}+\pi (n+1)}}\Bigr)-f\Bigl(\frac1{\sqrt{\smash[b]{\frac\pi2}+\pi n}}\Bigr)\right| \\   &= |f(0)-f(-1)|+\left|f\Bigl(\frac1{\sqrt{\smash[b]{\frac\pi2}+\pi n}}\Bigr)-f(0)\right| \\   &\mathrel{\phantom=}+\left|f(1)-f\Bigl(\frac1{\sqrt{\smash[b]{\frac\pi2}}}\Bigr)\right|+\sum_{i=0}^n\left(\frac1{\smash[b]{\frac\pi2}+\pi n}+\frac1{\smash[b]{\frac\pi2}+\pi (n-1)}\right). \end{align}$$ I don't know what to do next with the summation. Am I on  to something?","$V_a^b(P,f):=\sum_{i=1}^k|f(x_i)-f(x_{i-1})|$, where $P$ is a partition. $$f(x)= \begin{cases}        x^2\sin(\frac{1}{x^2}), &\text{if } x\neq0, \\       0, &\text{if } x=0     \end{cases} $$ does not have bounded variation over $[-1,1]$. I'm trying to show this without using the fact that it may not have bounded variation over some subset of $[-1,1]$ . I want to find a partition $\lbrace x_0,\dotsc,x_n \rbrace$ of $[-1,1]$ for which  $$\sum_{i=1}^n|f(x_i)-f(x_{i-1})|$$  gives a partial sum of the Harmonic Series. My partition is $P=\lbrace -1,0,\frac{1}{\sqrt{\frac{\pi}{2}+\pi n}},\frac{1}{\sqrt{\frac{\pi}{2}+\pi(n-1)}},\dotsc,\frac{1}{\sqrt{\frac{\pi}{2}}},1\rbrace$. $$\begin{align} V_{-1} ^1(P,f)    &=|f(0)-f(-1)| + \left|(f\Bigl(\frac1{\sqrt{\smash[b]{\frac\pi2}+\pi n}}\Bigr)-f(0)\right|+\left|f(1)-f\Bigl(\frac1{\sqrt{\smash[b]{\frac\pi2}}}\Bigr)\right| \\   &\mathrel{\phantom=} +\sum_{i=0}^{n}\left|f\Bigl(\frac1{\sqrt{\smash[b]{\frac\pi2}+\pi (n+1)}}\Bigr)-f\Bigl(\frac1{\sqrt{\smash[b]{\frac\pi2}+\pi n}}\Bigr)\right| \\   &= |f(0)-f(-1)|+\left|f\Bigl(\frac1{\sqrt{\smash[b]{\frac\pi2}+\pi n}}\Bigr)-f(0)\right| \\   &\mathrel{\phantom=}+\left|f(1)-f\Bigl(\frac1{\sqrt{\smash[b]{\frac\pi2}}}\Bigr)\right|+\sum_{i=0}^n\left(\frac1{\smash[b]{\frac\pi2}+\pi n}+\frac1{\smash[b]{\frac\pi2}+\pi (n-1)}\right). \end{align}$$ I don't know what to do next with the summation. Am I on  to something?",,"['real-analysis', 'problem-solving']"
39,Infinitely times differentiable function,Infinitely times differentiable function,,"Let $f$ belong to $ C^{\infty}[0,1]$ and for each $x \in [0,1]$ there exists $n \in \mathbb{N}$ so that $f^{(n)}(x)=0$. Prove that $f$ is a polynomial in $[0,1]$. I am trying to use Baire Category Theorem , but cannot perfectly complete the proof . Thanks for any help.","Let $f$ belong to $ C^{\infty}[0,1]$ and for each $x \in [0,1]$ there exists $n \in \mathbb{N}$ so that $f^{(n)}(x)=0$. Prove that $f$ is a polynomial in $[0,1]$. I am trying to use Baire Category Theorem , but cannot perfectly complete the proof . Thanks for any help.",,"['real-analysis', 'functions', 'baire-category']"
40,"Is $\mathcal P(X)$ connected when $(X,\mathcal P(X),m)$ is a measure space and $P(X)$ is equipped with the metric $d(A,B) =m(A\Delta B)$?",Is  connected when  is a measure space and  is equipped with the metric ?,"\mathcal P(X) (X,\mathcal P(X),m) P(X) d(A,B) =m(A\Delta B)","Is $\mathcal P(X)$ connected when $(X,\mathcal P(X),m)$ is a measure space and $P(X)$ is equipped with the metric $d(A,B) =m(A\Delta B)$?  Think when we look at the equivalence classes of almost everywhere relation . What about lebegu measure and X be a subset of Real number. Also look at Problem14.12 of chapter3 of ""Aliprantis-Burkinshaw-Principles of real analysis-3ed.1998"" ; 12. Let A be the collection of all measurable subsets of X of finite measure. That is,  A = {B in X: m(A) < \infty}.  a. Show that A is a semiring.  b. Define a relation ~ on A by B ~ C if m(B \Delta C) = 0. Show that ~ is an  equivalence relation on A.  c. Let D denote the set of ail equivalence classes of A. For B in A let B* denote  the equivalence class of B in D. Now for B*, C* in D define d(B*, C*) = m(B \Delta C).  Show that d is well defined and that (D, d) is a complete metric space. (For this  part, see also Exercise [3] of Section [31].) I just bring above problem to show readers places that d is meter. I need to know if m be lebegu measure and X be a subset of Real line, is {A in P(X) | m(A) is finite} for example when X=[a,b] that a,b are real numbers, a connected space with topology induced with meter d? Some of you have read this question and here is the last manner, I had written question with lots of ambiguity. Let me to say it again( with no ambiguity ): look at Problem14.12 of chapter3 of ""Aliprantis-Burkinshaw-Principles of real analysis-3ed.1998"" ; 12. Let A be the collection of all measurable subsets of X of finite measure. That is, A = {B in X: m(A) < \infty}. a. Show that A is a semiring. b. Define a relation ~ on A by B ~ C if m(B \Delta C) = 0. Show that ~ is an equivalence relation on A. c. Let D denote the set of ail equivalence classes of A. For B in A let B* denote the equivalence class of B in D. Now for B*, C* in D define d(B*, C*) = m(B \Delta C). Show that d is well defined and that (D, d) is a complete metric space. (For this part, see also Exercise [3] of Section [31].) Now let m be lebesgu measure on Real numbers and X be a subset of Real line, is (D,d) a connected space with topology induced with meter d?","Is $\mathcal P(X)$ connected when $(X,\mathcal P(X),m)$ is a measure space and $P(X)$ is equipped with the metric $d(A,B) =m(A\Delta B)$?  Think when we look at the equivalence classes of almost everywhere relation . What about lebegu measure and X be a subset of Real number. Also look at Problem14.12 of chapter3 of ""Aliprantis-Burkinshaw-Principles of real analysis-3ed.1998"" ; 12. Let A be the collection of all measurable subsets of X of finite measure. That is,  A = {B in X: m(A) < \infty}.  a. Show that A is a semiring.  b. Define a relation ~ on A by B ~ C if m(B \Delta C) = 0. Show that ~ is an  equivalence relation on A.  c. Let D denote the set of ail equivalence classes of A. For B in A let B* denote  the equivalence class of B in D. Now for B*, C* in D define d(B*, C*) = m(B \Delta C).  Show that d is well defined and that (D, d) is a complete metric space. (For this  part, see also Exercise [3] of Section [31].) I just bring above problem to show readers places that d is meter. I need to know if m be lebegu measure and X be a subset of Real line, is {A in P(X) | m(A) is finite} for example when X=[a,b] that a,b are real numbers, a connected space with topology induced with meter d? Some of you have read this question and here is the last manner, I had written question with lots of ambiguity. Let me to say it again( with no ambiguity ): look at Problem14.12 of chapter3 of ""Aliprantis-Burkinshaw-Principles of real analysis-3ed.1998"" ; 12. Let A be the collection of all measurable subsets of X of finite measure. That is, A = {B in X: m(A) < \infty}. a. Show that A is a semiring. b. Define a relation ~ on A by B ~ C if m(B \Delta C) = 0. Show that ~ is an equivalence relation on A. c. Let D denote the set of ail equivalence classes of A. For B in A let B* denote the equivalence class of B in D. Now for B*, C* in D define d(B*, C*) = m(B \Delta C). Show that d is well defined and that (D, d) is a complete metric space. (For this part, see also Exercise [3] of Section [31].) Now let m be lebesgu measure on Real numbers and X be a subset of Real line, is (D,d) a connected space with topology induced with meter d?",,"['real-analysis', 'general-topology', 'measure-theory']"
41,How many limit points in $\{\sin(2^n)\}$? How many can there be in a general sequence?,How many limit points in ? How many can there be in a general sequence?,\{\sin(2^n)\},"Analysis question - given a sequence $\{a_n\}_{n=1}^\infty$, how many limit points can $\{a_n\}$ have? Initially I thought only $\aleph_0$, or countably many, because there are only countably many terms in such a sequence. But then I thought about the sequence where $a_n = \sin(2^n)$, which looks like this for the first 1000 terms. This sequence has no repeating terms, or else $\pi$ is rational. However I do not think every real number in $[0,1]$ appears in this sequence, as then the reals are countable, an absurd conclusion. It would not surprise me if this sequence has limit points, but how many? I do not immediately see any reason why $\sin(2^n)\notin (r-\epsilon,r+\epsilon)\subset[0,1]$, for $n$ large enough, because $r$ is arbitrary. This seems to imply that this sequence could have every point in $[0,1]\subset \mathbb R$ as a limit point, which slightly frightens me. So, How many limit points does $\{\sin(2^n)\}_{n=1}^\infty$ have? And in general, How many limit points can a sequence with countably many terms have? What is an example of a sequence with the maximum number of limit points? I would really like to see a proof of the answer to this last statement. I can easily come up with a sequence with $\aleph_0$ limit points, but I can't prove that is the upper bound. Thanks for any help and tips on this topic.","Analysis question - given a sequence $\{a_n\}_{n=1}^\infty$, how many limit points can $\{a_n\}$ have? Initially I thought only $\aleph_0$, or countably many, because there are only countably many terms in such a sequence. But then I thought about the sequence where $a_n = \sin(2^n)$, which looks like this for the first 1000 terms. This sequence has no repeating terms, or else $\pi$ is rational. However I do not think every real number in $[0,1]$ appears in this sequence, as then the reals are countable, an absurd conclusion. It would not surprise me if this sequence has limit points, but how many? I do not immediately see any reason why $\sin(2^n)\notin (r-\epsilon,r+\epsilon)\subset[0,1]$, for $n$ large enough, because $r$ is arbitrary. This seems to imply that this sequence could have every point in $[0,1]\subset \mathbb R$ as a limit point, which slightly frightens me. So, How many limit points does $\{\sin(2^n)\}_{n=1}^\infty$ have? And in general, How many limit points can a sequence with countably many terms have? What is an example of a sequence with the maximum number of limit points? I would really like to see a proof of the answer to this last statement. I can easily come up with a sequence with $\aleph_0$ limit points, but I can't prove that is the upper bound. Thanks for any help and tips on this topic.",,"['real-analysis', 'convergence-divergence']"
42,Proof of continuity for a real function!,Proof of continuity for a real function!,,"Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a real function, and satisfy that: for all $x\in\mathbb{R}$  $$\lim_{r\to x,r\in\mathbb{Q}}f(r)=f(x).$$ Show that $f$ is continuous on $\mathbb{R}.$","Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a real function, and satisfy that: for all $x\in\mathbb{R}$  $$\lim_{r\to x,r\in\mathbb{Q}}f(r)=f(x).$$ Show that $f$ is continuous on $\mathbb{R}.$",,['real-analysis']
43,Show that the integrator in a Lebesgue-Stieltjes integration is zero,Show that the integrator in a Lebesgue-Stieltjes integration is zero,,"Let's consider a continuous function of bounded variation that is defined on a finite interval: $f: [0,t] \rightarrow \mathbb{R}$. Assume that we have a positive, continuous function that is bounded away from zero: $g: [0,t] \rightarrow \mathbb{R}$. Then we have the Lebesgue-Stieltjes integral of g with respect to f, well defined as a function of the upper limit of integration: $$I(s)=\int_{0}^{s}g(u)df(u), \ 0\leq s \leq t.$$ If we assume that $f(0)=0$ and that $I(s)=0, \forall 0 \leq s \leq t$, can we conclude that $f(s)=0, \forall s \in [0,t]$ ? Any help or comment is greatly appreciated, thanks!","Let's consider a continuous function of bounded variation that is defined on a finite interval: $f: [0,t] \rightarrow \mathbb{R}$. Assume that we have a positive, continuous function that is bounded away from zero: $g: [0,t] \rightarrow \mathbb{R}$. Then we have the Lebesgue-Stieltjes integral of g with respect to f, well defined as a function of the upper limit of integration: $$I(s)=\int_{0}^{s}g(u)df(u), \ 0\leq s \leq t.$$ If we assume that $f(0)=0$ and that $I(s)=0, \forall 0 \leq s \leq t$, can we conclude that $f(s)=0, \forall s \in [0,t]$ ? Any help or comment is greatly appreciated, thanks!",,"['real-analysis', 'integration']"
44,Level sets of convex functions,Level sets of convex functions,,"Let $f:\mathbb{R}^2\rightarrow\mathbb{R}$ be a convex function. For $t\in\mathbb{R}$, consider the corresponding level set $$f^{-1}\{t\}=\{(x,y)\in\mathbb{R}^2: f(x,y)=t\}.$$ For the application I have in mind, it will suffice to assume that all the level sets of $f$ are compact curves . In that case, denoting arc length measure on the plane by $\ell(\cdot)$, when does an estimate of the form $$\Big|\ell(f^{-1}\{t\})-\ell(f^{-1}\{t'\})\Big|\leq C |t-t'|^{\delta}$$ hold for some $C<\infty$ and $\delta>0$? And how does the optimal exponent $\delta$ depend on $f$? The intuition is that the curves $f^{-1}\{t\}$ and $f^{-1}\{t'\}$ should be ""close to each other""  whenever $t$ is close to $t'$. In which other ways can this be made rigorous? Thank you.","Let $f:\mathbb{R}^2\rightarrow\mathbb{R}$ be a convex function. For $t\in\mathbb{R}$, consider the corresponding level set $$f^{-1}\{t\}=\{(x,y)\in\mathbb{R}^2: f(x,y)=t\}.$$ For the application I have in mind, it will suffice to assume that all the level sets of $f$ are compact curves . In that case, denoting arc length measure on the plane by $\ell(\cdot)$, when does an estimate of the form $$\Big|\ell(f^{-1}\{t\})-\ell(f^{-1}\{t'\})\Big|\leq C |t-t'|^{\delta}$$ hold for some $C<\infty$ and $\delta>0$? And how does the optimal exponent $\delta$ depend on $f$? The intuition is that the curves $f^{-1}\{t\}$ and $f^{-1}\{t'\}$ should be ""close to each other""  whenever $t$ is close to $t'$. In which other ways can this be made rigorous? Thank you.",,"['real-analysis', 'convex-analysis', 'plane-curves']"
45,Gagliardo-Nirenberg inequality?,Gagliardo-Nirenberg inequality?,,"For $1\leq p<n$, we define the Sobolev conjugate of$p$ as $p^{\ast}=\frac{np}{n-p}$. Recall the Gagliardo-Nirenberg inequality $$||u||_{L^{p^{\ast}}(\mathbb{R}^n)}\leq C ||\nabla u||_{L^p(\mathbb{R}^n)}$$ for some constant $C$ depending only on $n$ and $p$ for any function $u\in C_c^1(\mathbb{R}^n)$. From this, one derives easily the following estimate for $W^{1,p}$: let $U$ be a bounded open subset of $\mathbb{r}^n$, an open disk say. Assume that $1\leq p<n$ and let $u\in W^{1,p}(U)$. Then $u\in L^{p^{\ast}}(U)$ with $$||u||_{L^{p^{\ast}}(U)}\leq C||u||_{W^{1,p}(U)}$$ I came accross the following inequality $$||u||_{L^2(\partial U)}\leq C||u||_{L^p(U)}^{1-1/p} ||u||_{W^{1,p}(U)}^{1/p}, \quad u\in W^{1,p}(U)$$ Is this a consequence of the previous inequalities? I am even confused about the appropriateness of the 'u' in the left hand side: I would expect the restriction to $\partial U$ of a function $v\in W^{1,p}(U)$. Since the restriction operator $\tau:W^{1,p}(U)\rightarrow L^2(\partial U)$ is continuous, then $$||v_{|\partial U}||_{L^2(\partial U)}=||\tau(u)||_{L^2(\partial U)}\leq C||u||_{W^{1,2}(U)}$$ and from here I am wondering if the inequalities above shed some light.","For $1\leq p<n$, we define the Sobolev conjugate of$p$ as $p^{\ast}=\frac{np}{n-p}$. Recall the Gagliardo-Nirenberg inequality $$||u||_{L^{p^{\ast}}(\mathbb{R}^n)}\leq C ||\nabla u||_{L^p(\mathbb{R}^n)}$$ for some constant $C$ depending only on $n$ and $p$ for any function $u\in C_c^1(\mathbb{R}^n)$. From this, one derives easily the following estimate for $W^{1,p}$: let $U$ be a bounded open subset of $\mathbb{r}^n$, an open disk say. Assume that $1\leq p<n$ and let $u\in W^{1,p}(U)$. Then $u\in L^{p^{\ast}}(U)$ with $$||u||_{L^{p^{\ast}}(U)}\leq C||u||_{W^{1,p}(U)}$$ I came accross the following inequality $$||u||_{L^2(\partial U)}\leq C||u||_{L^p(U)}^{1-1/p} ||u||_{W^{1,p}(U)}^{1/p}, \quad u\in W^{1,p}(U)$$ Is this a consequence of the previous inequalities? I am even confused about the appropriateness of the 'u' in the left hand side: I would expect the restriction to $\partial U$ of a function $v\in W^{1,p}(U)$. Since the restriction operator $\tau:W^{1,p}(U)\rightarrow L^2(\partial U)$ is continuous, then $$||v_{|\partial U}||_{L^2(\partial U)}=||\tau(u)||_{L^2(\partial U)}\leq C||u||_{W^{1,2}(U)}$$ and from here I am wondering if the inequalities above shed some light.",,"['real-analysis', 'inequality', 'partial-differential-equations']"
46,Proving every positive natural number has unique predecessor,Proving every positive natural number has unique predecessor,,"I am independently working through Tao's Analysis I , and one of the exercises is to prove that every positive natural number has a unique predecessor. The actual lemma is (where $n++$ denotes the successor of $n$): Let $a$ be a positive [natural] number. Then there exists exactly one natural number $b$ such that $b++=a$. The hint given is to use induction, and the book uses the Peano axioms with $0\in\mathbb{N}$. Here is my attempt at a proof, but I am not sure if I applied induction properly (I didn't seem to need the inductive hypothesis in proving the $k+1$ case), and I am even more unsure if uniqueness is proven: The statement to prove is $(\forall a\not=0)(\exists!b)(b++=a)$, which is the same as $\forall a\exists!b(a\not=0\rightarrow (b++=a))$. Induction is used on $a$. The basis step is to prove for $a=0$, which is $\exists!b(0\not=0\rightarrow (b++=0))$. Since the antecedent of the conditional is false, the statement is vacuously true. [But is $b$ unique...?] For the inductive step, it is assumed that for an arbitrary $k$, $\exists!b(k\not=0\rightarrow (b++=k))$. Then $\exists!b((k++\not=0)\rightarrow (b++=k++))$ is to be derived. The second of these (the one being derived) can be rewritten as $(k++\not=0)\rightarrow (c++=k++)$, for some $c$. $k++\not=0$ is true even without assuming it, since $0$ does not have a predecessor. But $c++=k++$ implies $c=k$ (because the successor function is injective), which shows that a [unique?] $c$ can be chosen such that it is equal to $k$. This closes the induction and thus every positive natural number has a unique successor. My questions are: (1) Is this proof correct? (2) If the answer to the first question is 'no', is the general idea for the proof correct? (3) If the answers to both of the preceding questions are 'no', could someone point me in the right direction for a correct proof? Any help would be appreciated. Thanks.","I am independently working through Tao's Analysis I , and one of the exercises is to prove that every positive natural number has a unique predecessor. The actual lemma is (where $n++$ denotes the successor of $n$): Let $a$ be a positive [natural] number. Then there exists exactly one natural number $b$ such that $b++=a$. The hint given is to use induction, and the book uses the Peano axioms with $0\in\mathbb{N}$. Here is my attempt at a proof, but I am not sure if I applied induction properly (I didn't seem to need the inductive hypothesis in proving the $k+1$ case), and I am even more unsure if uniqueness is proven: The statement to prove is $(\forall a\not=0)(\exists!b)(b++=a)$, which is the same as $\forall a\exists!b(a\not=0\rightarrow (b++=a))$. Induction is used on $a$. The basis step is to prove for $a=0$, which is $\exists!b(0\not=0\rightarrow (b++=0))$. Since the antecedent of the conditional is false, the statement is vacuously true. [But is $b$ unique...?] For the inductive step, it is assumed that for an arbitrary $k$, $\exists!b(k\not=0\rightarrow (b++=k))$. Then $\exists!b((k++\not=0)\rightarrow (b++=k++))$ is to be derived. The second of these (the one being derived) can be rewritten as $(k++\not=0)\rightarrow (c++=k++)$, for some $c$. $k++\not=0$ is true even without assuming it, since $0$ does not have a predecessor. But $c++=k++$ implies $c=k$ (because the successor function is injective), which shows that a [unique?] $c$ can be chosen such that it is equal to $k$. This closes the induction and thus every positive natural number has a unique successor. My questions are: (1) Is this proof correct? (2) If the answer to the first question is 'no', is the general idea for the proof correct? (3) If the answers to both of the preceding questions are 'no', could someone point me in the right direction for a correct proof? Any help would be appreciated. Thanks.",,['real-analysis']
47,The definition of the logarithm.,The definition of the logarithm.,,"One usually gets several definitions of the logarithm along his studies. You might be first introduced to the exponential and then told that the logarithm is its inverse. You might be given $$\log x = \int\limits_1^x {\frac{{du}}{u}} $$ Like Landau does. Let $k = 2^n$, then: $$\log x =\mathop {\lim }\limits_{k \to \infty } k\left( {\root k \of x  - 1} \right)$$ And last, if you ever read, Euler famously wrote: $$ - \log x = \frac{{1 - {x^0}}}{0}$$ Landau's definition (although I find it the most usefull to work with) really baffled me untill just now. Since $$\int\limits_1^x {\frac{{du}}{{{u^{\alpha  + 1}}}}}  = \frac{{{x^{ - \alpha }} - 1}}{{ - \alpha }}$$ Then being $\frac{1}{k} = -\alpha$ one hopes to have: $$\mathop {\lim }\limits_{\alpha  \to 0} \int\limits_1^x {\frac{{du}}{{{u^{\alpha  + 1}}}}}  = \int\limits_1^x {\frac{{du}}{u}}  = \log x = \mathop {\lim }\limits_{k \to \infty } k\left( {\root k \of x  - 1} \right)$$ How can one justify taking the limit before integration? Continuity suffices?","One usually gets several definitions of the logarithm along his studies. You might be first introduced to the exponential and then told that the logarithm is its inverse. You might be given $$\log x = \int\limits_1^x {\frac{{du}}{u}} $$ Like Landau does. Let $k = 2^n$, then: $$\log x =\mathop {\lim }\limits_{k \to \infty } k\left( {\root k \of x  - 1} \right)$$ And last, if you ever read, Euler famously wrote: $$ - \log x = \frac{{1 - {x^0}}}{0}$$ Landau's definition (although I find it the most usefull to work with) really baffled me untill just now. Since $$\int\limits_1^x {\frac{{du}}{{{u^{\alpha  + 1}}}}}  = \frac{{{x^{ - \alpha }} - 1}}{{ - \alpha }}$$ Then being $\frac{1}{k} = -\alpha$ one hopes to have: $$\mathop {\lim }\limits_{\alpha  \to 0} \int\limits_1^x {\frac{{du}}{{{u^{\alpha  + 1}}}}}  = \int\limits_1^x {\frac{{du}}{u}}  = \log x = \mathop {\lim }\limits_{k \to \infty } k\left( {\root k \of x  - 1} \right)$$ How can one justify taking the limit before integration? Continuity suffices?",,"['real-analysis', 'integration', 'logarithms', 'definition']"
48,Interchanging integration and differentiation in heat equation,Interchanging integration and differentiation in heat equation,,"I'm trying to solve the following problem: Assume that we are given $f(t,x) \in C^{\infty}(\mathbb R \times \mathbb R^n)$ such that $f(t, \cdot) \in \mathcal S(\mathbb R^n)$ for all $t>0$ (where $\mathcal S(\mathbb R^n)$ is Schwartz space ). Assume furthermore that $f$ is a solution to the heat equation on $\mathbb R^n$ with   $$ \begin{align} \partial_tf(t,x) &= \Delta f(t,x) \qquad t>0 \\ \lim_{t\to 0} f(t,x) &= g(x) \end{align} $$   for some $g\in \mathcal S(\mathbb R^n)$. I want to show that under these conditions we must have $f(t,x) = (K_t\ast g)(x)$, where $\ast$ denotes convolution and    $$ K_t(x) = \frac{1}{(4\pi t)^{n/2}}e^{-\langle x, x\rangle/4t}$$   is the heat kernel. What I'm having trouble with is the following: The idea is to consider the Fourier transform $\hat f$ of $f$ and to derive the ordinary differential equation $$\partial_t \hat f(t,k) = - k^2 \hat f(t,k)$$ But how do I even know that $\hat f(t,k)$ is differentiable with respect to $t$? What I would like to do is differentiate under the integral sign here: $$\partial_t \hat f(t,k) =\partial_t \int_{\mathbb R^n} f(t,x)e^{-ikx}\, dx$$ But I don't know how to justify it. Could anyone help me out? Thanks a lot! =)","I'm trying to solve the following problem: Assume that we are given $f(t,x) \in C^{\infty}(\mathbb R \times \mathbb R^n)$ such that $f(t, \cdot) \in \mathcal S(\mathbb R^n)$ for all $t>0$ (where $\mathcal S(\mathbb R^n)$ is Schwartz space ). Assume furthermore that $f$ is a solution to the heat equation on $\mathbb R^n$ with   $$ \begin{align} \partial_tf(t,x) &= \Delta f(t,x) \qquad t>0 \\ \lim_{t\to 0} f(t,x) &= g(x) \end{align} $$   for some $g\in \mathcal S(\mathbb R^n)$. I want to show that under these conditions we must have $f(t,x) = (K_t\ast g)(x)$, where $\ast$ denotes convolution and    $$ K_t(x) = \frac{1}{(4\pi t)^{n/2}}e^{-\langle x, x\rangle/4t}$$   is the heat kernel. What I'm having trouble with is the following: The idea is to consider the Fourier transform $\hat f$ of $f$ and to derive the ordinary differential equation $$\partial_t \hat f(t,k) = - k^2 \hat f(t,k)$$ But how do I even know that $\hat f(t,k)$ is differentiable with respect to $t$? What I would like to do is differentiate under the integral sign here: $$\partial_t \hat f(t,k) =\partial_t \int_{\mathbb R^n} f(t,x)e^{-ikx}\, dx$$ But I don't know how to justify it. Could anyone help me out? Thanks a lot! =)",,"['real-analysis', 'partial-differential-equations', 'fourier-analysis']"
49,Applications of Weierstrass Theorem & Stone Weierstrass Theorem,Applications of Weierstrass Theorem & Stone Weierstrass Theorem,,"Problem: Prove that if $f:\left [ 0,1 \right ]\rightarrow \mathbb{R}$ is a continuous function such that: $\int_{0}^{1}f(x)e^{nx}dx=0$ for all $n=0,1,2,...$ . Prove that $f(x)=0$ for all $0\leqslant x\leqslant 1$ using two methods: Change of variables and then apply Weierstrass Theorem. Apply Stone–Weierstrass Theorem. I already know how to prove that $f(x)=0$ for all $0\leqslant x\leqslant 1$ if $\int_{0}^{1}f(x)x^{n}dx=0$ . I did the following change of variable: $e^{x}=y$ and then I got $\int_{1}^{e}f(y)y^{n-1}dy$ . since $\int_{0}^{1}f(y)x^{n-1}dy=0$ , we are left with $\int_{1}^{e}f(y)x^{n-1}dy$ which I want to prove equal to zero. Any help with this? For the second part, I don't have any idea how to use the Stone–Weierstrass Theorem to prove it. I have never used this theorem before in solving problems, so I appreciate if someone helps with details for this part.","Problem: Prove that if is a continuous function such that: for all . Prove that for all using two methods: Change of variables and then apply Weierstrass Theorem. Apply Stone–Weierstrass Theorem. I already know how to prove that for all if . I did the following change of variable: and then I got . since , we are left with which I want to prove equal to zero. Any help with this? For the second part, I don't have any idea how to use the Stone–Weierstrass Theorem to prove it. I have never used this theorem before in solving problems, so I appreciate if someone helps with details for this part.","f:\left [ 0,1 \right ]\rightarrow \mathbb{R} \int_{0}^{1}f(x)e^{nx}dx=0 n=0,1,2,... f(x)=0 0\leqslant x\leqslant 1 f(x)=0 0\leqslant x\leqslant 1 \int_{0}^{1}f(x)x^{n}dx=0 e^{x}=y \int_{1}^{e}f(y)y^{n-1}dy \int_{0}^{1}f(y)x^{n-1}dy=0 \int_{1}^{e}f(y)x^{n-1}dy","['real-analysis', 'general-topology', 'analysis']"
50,Continuous functions are Riemann-Stieltjes integrable with respect to a monotone function,Continuous functions are Riemann-Stieltjes integrable with respect to a monotone function,,"Let $g:[a,b] \to \mathbb{R}$ be a monotone function. Could you help me prove that $\mathcal{C}([a,b])\subseteq\mathcal{R}([a,b],g)$? (Here $\mathcal{R}([a,b],g)$ is the set of all functions that are Riemann-Stieltjes integrable with respect to $g$.) Definition of the Riemann-Stieltjes integral. Suppose $f,g$ are bounded on $[a,b]$. If there is an $A \in \mathbb{R}$ such that for every $\varepsilon >0$, there exists a partition $\mathcal{P}$ of $[a,b]$ such that for every refinement $\mathcal{Q}$ of $\mathcal{P}$ we have $|I(f,\mathcal{Q},X,g)-A|<\varepsilon$ (where if $\mathcal{P}=\{a=x_0<\ldots<x_n=b\}$ and $X$ is an evaluation sequence $X=\{x_1^\prime,\ldots,x_n^\prime\}$ and $I(f,\mathcal{Q},X,g)=\sum_{j=1}^n f(x_j^\prime)(g(x_j)-g(x_{j-1}))$), then $f$ is R-S integrable with respect to $g$, and the integral is $A$.","Let $g:[a,b] \to \mathbb{R}$ be a monotone function. Could you help me prove that $\mathcal{C}([a,b])\subseteq\mathcal{R}([a,b],g)$? (Here $\mathcal{R}([a,b],g)$ is the set of all functions that are Riemann-Stieltjes integrable with respect to $g$.) Definition of the Riemann-Stieltjes integral. Suppose $f,g$ are bounded on $[a,b]$. If there is an $A \in \mathbb{R}$ such that for every $\varepsilon >0$, there exists a partition $\mathcal{P}$ of $[a,b]$ such that for every refinement $\mathcal{Q}$ of $\mathcal{P}$ we have $|I(f,\mathcal{Q},X,g)-A|<\varepsilon$ (where if $\mathcal{P}=\{a=x_0<\ldots<x_n=b\}$ and $X$ is an evaluation sequence $X=\{x_1^\prime,\ldots,x_n^\prime\}$ and $I(f,\mathcal{Q},X,g)=\sum_{j=1}^n f(x_j^\prime)(g(x_j)-g(x_{j-1}))$), then $f$ is R-S integrable with respect to $g$, and the integral is $A$.",,"['real-analysis', 'analysis']"
51,Differentiating a function of matrices,Differentiating a function of matrices,,Suppose I have a function $\phi(M):=M^n$ where $M\in M_n(R)$ Does it make any sense to talk about the derivative of this function? What would it mean if it does make sense? I am also wondering what the derivative (if it exists and is meaningful) would be. (Perhaps $nM^{n-1}$?) Thanks.,Suppose I have a function $\phi(M):=M^n$ where $M\in M_n(R)$ Does it make any sense to talk about the derivative of this function? What would it mean if it does make sense? I am also wondering what the derivative (if it exists and is meaningful) would be. (Perhaps $nM^{n-1}$?) Thanks.,,"['linear-algebra', 'real-analysis', 'matrices']"
52,Smoothness of a real-valued function on $\mathbb{R}^n $,Smoothness of a real-valued function on,\mathbb{R}^n ,"Let  $$ f(x)= \begin{cases} \exp\left(\frac{-1}{1-|x|^2}\right), &\text{ if } |x| < 1, \\ 0, &\text{ if } |x|\geq  1. \end{cases} $$ Prove that $f$ is infinitely differentiable everywhere. ($x$ belongs to $\mathbb{R}^n$ for fixed $n$.) Well, this is obvious for $|x|>1$ and easy enough for the first derivative at $|x|=1$, but I can't seem to use the definition of the Gateaux derivative to show it for $|x|<1$. Any advice would be appreciated. (This is not homework.)","Let  $$ f(x)= \begin{cases} \exp\left(\frac{-1}{1-|x|^2}\right), &\text{ if } |x| < 1, \\ 0, &\text{ if } |x|\geq  1. \end{cases} $$ Prove that $f$ is infinitely differentiable everywhere. ($x$ belongs to $\mathbb{R}^n$ for fixed $n$.) Well, this is obvious for $|x|>1$ and easy enough for the first derivative at $|x|=1$, but I can't seem to use the definition of the Gateaux derivative to show it for $|x|<1$. Any advice would be appreciated. (This is not homework.)",,['real-analysis']
53,"What motivates discrepancies between the definitions of ""continuous"" and ""limit""?","What motivates discrepancies between the definitions of ""continuous"" and ""limit""?",,"I am working from Munkres' Analysis, and I've converted his definitions slightly to make them easier to compare.  In the table below, you can fill in the blanks in the top row with words from the lower rows to form either definition: It is not clear to me what motivates some of the choices when it comes to 'filling in the blanks'.  My biggest concern is the last two blanks.  The 2nd blank is essentially discussed in my old question here: Why not define 'limits' to include isolated points? And while I roughly understand the response there (letting in isolated points means that functions can approach infinitely many limits at isolated points), when I consider changes to the last two columns, I find myself also considering changes to the 2nd. My hope is that someone can construct simple examples for each column (in as few dimensions as possible!) which motivate the choice, while somehow dealing with the interconnection problem wherein choice in one column affects choice in another...","I am working from Munkres' Analysis, and I've converted his definitions slightly to make them easier to compare.  In the table below, you can fill in the blanks in the top row with words from the lower rows to form either definition: It is not clear to me what motivates some of the choices when it comes to 'filling in the blanks'.  My biggest concern is the last two blanks.  The 2nd blank is essentially discussed in my old question here: Why not define 'limits' to include isolated points? And while I roughly understand the response there (letting in isolated points means that functions can approach infinitely many limits at isolated points), when I consider changes to the last two columns, I find myself also considering changes to the 2nd. My hope is that someone can construct simple examples for each column (in as few dimensions as possible!) which motivate the choice, while somehow dealing with the interconnection problem wherein choice in one column affects choice in another...",,"['calculus', 'real-analysis', 'general-topology', 'analysis', 'definition']"
54,"The average of a bounded, decreasing-difference sequence","The average of a bounded, decreasing-difference sequence",,"(not sure if ""decreasing-difference"" is the right way to put it - please edit away if you know the proper technical term) From my analysis homework: Does there exist a bounded sequence   $\{x_n\}_{n \in \mathbb{N}}$ such that   $x_{n+1}-x_n \to 0$, but the sequence   $(\sum_{i=1}^n x_i)/n$ does not have a   limit? Give an example or show none   exist. In a previous part, I showed that there exists a bounded sequence $\{x_n\}_{n \in \mathbb{N}}$ such that $\left| x_{n+1}-x_n \right| \to 0$, yet $x_n$ does not converge, using a rearrangement of the alternating harmonic series. Intuitively, I believe none exist. I've tried showing that the sequence is Cauchy, but that does not seem to help: $$\left| \frac{ \sum_{i=1}^m x_i}{m} - \frac{ \sum_{i=1}^n x_i}{n} \right| = \left| \frac{ (n-m) \sum_{i=1}^m x_i - m \sum_{i=m+1}^n x_i}{mn} \right| $$ I feel like I can't use my normal series tools because I am looking at an average, and the series does not necessarily converge.","(not sure if ""decreasing-difference"" is the right way to put it - please edit away if you know the proper technical term) From my analysis homework: Does there exist a bounded sequence   $\{x_n\}_{n \in \mathbb{N}}$ such that   $x_{n+1}-x_n \to 0$, but the sequence   $(\sum_{i=1}^n x_i)/n$ does not have a   limit? Give an example or show none   exist. In a previous part, I showed that there exists a bounded sequence $\{x_n\}_{n \in \mathbb{N}}$ such that $\left| x_{n+1}-x_n \right| \to 0$, yet $x_n$ does not converge, using a rearrangement of the alternating harmonic series. Intuitively, I believe none exist. I've tried showing that the sequence is Cauchy, but that does not seem to help: $$\left| \frac{ \sum_{i=1}^m x_i}{m} - \frac{ \sum_{i=1}^n x_i}{n} \right| = \left| \frac{ (n-m) \sum_{i=1}^m x_i - m \sum_{i=m+1}^n x_i}{mn} \right| $$ I feel like I can't use my normal series tools because I am looking at an average, and the series does not necessarily converge.",,"['real-analysis', 'analysis', 'sequences-and-series']"
55,Sequence of measurable functions,Sequence of measurable functions,,"If $h(x,t)$ is measurable and the measures involved are $\sigma$-finite, does there exist a sequences of functions $$h_n(x,t) = \sum_{j=1}^{N_n} f_{j,n}(x) 1_{F_{j,n}}(t)$$  where the sets are pairwise disjoint (in $j$) such that $h_n \to h$ pointwise almost everywhere? I know that if we fix $x$ we can get a sequence like that but we cannot just make the coefficients depending on $x$ since the sets $F_{j,n}$ will be different. Can I somehow combine them? Do I need the $\sigma$-finiteness? If the answer is positive, we can use this to prove Minkowski's inequality for integrals quite easily.","If $h(x,t)$ is measurable and the measures involved are $\sigma$-finite, does there exist a sequences of functions $$h_n(x,t) = \sum_{j=1}^{N_n} f_{j,n}(x) 1_{F_{j,n}}(t)$$  where the sets are pairwise disjoint (in $j$) such that $h_n \to h$ pointwise almost everywhere? I know that if we fix $x$ we can get a sequence like that but we cannot just make the coefficients depending on $x$ since the sets $F_{j,n}$ will be different. Can I somehow combine them? Do I need the $\sigma$-finiteness? If the answer is positive, we can use this to prove Minkowski's inequality for integrals quite easily.",,"['real-analysis', 'measure-theory']"
56,Show $ I(k):=\lim _{R \rightarrow+\infty} \int_{-R}^R \frac{e^{i k x}}{1+|x|} d x $ exists,Show  exists, I(k):=\lim _{R \rightarrow+\infty} \int_{-R}^R \frac{e^{i k x}}{1+|x|} d x ,"I'm trying to solve this problem from a past Stanford qual, but I'm a bit stuck. (EDIT: I have now solved the problem. See the solution below.) Show that for each $k \in \mathbb{R} \backslash\{0\}$ the limit $$ I(k):=\lim _{R \rightarrow+\infty} \int_{-R}^R \frac{e^{i k x}}{1+|x|} d x $$ exists, and find all $m \in \mathbb{R}$ such that $\lim _{k \rightarrow 0}|k|^m I(k)=0$ . This looks like the inverse Fourier transform of $\frac{1}{1+|x|}$ , but I'm not sure what to do with this fact. If I expand $e^{ikx}$ , I get $$\lim_{R\to \infty}\int_{-R}^R\frac{e^{ikx}}{1 + |x|}dx =\lim_{R\to\infty}\left[\int_{-R}^R\frac{\cos(kx)}{1 + |x|}dx + i\int_{-R}^R\frac{\sin(kx)}{1 + |x|}dx \right]$$ The imaginary part is zero because $\frac{1}{1+|x|}$ is even and $\sin(kx)$ is odd over a symmetric domain. Therefore we just have to worry about $$\lim_{R\to\infty}\int_{-R}^R\frac{\cos(kx)}{1 + |x|}dx=\lim_{R\to\infty}2\int_0^R\frac{\cos(kx)}{1+x}dx$$ now integrating by parts, we may set $u=\frac{1}{1+x}$ so that $du=\frac{-1}{(1+x)^2}dx$ and $v=\frac{1}{k}\sin(kx)$ and $dv=\sin(kx)dx$ . Usng integration by parts $$2\int_0^R \frac{\cos(kx)}{1+x}dx = \left.\frac{2}{k}\frac{\sin(kx)}{1+x}\right|_0^R-2\int_0^R \frac{-1}{k(1+x)^2}\sin(kx)dx.$$ Now $$\left|\int_0^R \frac{\sin(kx)}{(1+x)^2}dx\right|\leq \int_0^R\frac{1}{(1+x)^2}dx=\left.\frac{-1}{1+x}\right|_0^R= \frac{-1}{1+R} + 1$$ Therefore, we get that the integral converges absolutely, so the limit exists. Now using the bound $-\frac{2}{|k|}\leq I(k)\leq \frac{2}{\left|k\right|}$ we get that $-2|k|^{m-1}\leq |k|^mI(k)\leq 2|k|^{m-1}$ so $ \lim _{k \rightarrow 0}|k|^m I(k)=0 $ if $m> 1$ by the squeeze theorem. Now when $m < 1$ , then since $\sin(-x)=-\sin(x)$ , we get $$\lim_{k\to 0}|k|^{m}I(x) =\lim_{k\to 0}\frac{2|k|^{m}}{k} \int_0^R \frac{\sin(kx)}{1 + |x|}dx=\lim_{k\to 0}2|k|^{m-1} \int_0^R \frac{\sin(kx)}{1 + |x|}dx = \infty$$ and when $m=1$ , we have $$\lim_{k\to 0}|k|^{m}I(x) =\lim_{k\to 0} \int_0^R \frac{\sin(kx)}{1 + |x|}dx\neq 0.$$","I'm trying to solve this problem from a past Stanford qual, but I'm a bit stuck. (EDIT: I have now solved the problem. See the solution below.) Show that for each the limit exists, and find all such that . This looks like the inverse Fourier transform of , but I'm not sure what to do with this fact. If I expand , I get The imaginary part is zero because is even and is odd over a symmetric domain. Therefore we just have to worry about now integrating by parts, we may set so that and and . Usng integration by parts Now Therefore, we get that the integral converges absolutely, so the limit exists. Now using the bound we get that so if by the squeeze theorem. Now when , then since , we get and when , we have","k \in \mathbb{R} \backslash\{0\} 
I(k):=\lim _{R \rightarrow+\infty} \int_{-R}^R \frac{e^{i k x}}{1+|x|} d x
 m \in \mathbb{R} \lim _{k \rightarrow 0}|k|^m I(k)=0 \frac{1}{1+|x|} e^{ikx} \lim_{R\to \infty}\int_{-R}^R\frac{e^{ikx}}{1 + |x|}dx =\lim_{R\to\infty}\left[\int_{-R}^R\frac{\cos(kx)}{1 + |x|}dx + i\int_{-R}^R\frac{\sin(kx)}{1 + |x|}dx \right] \frac{1}{1+|x|} \sin(kx) \lim_{R\to\infty}\int_{-R}^R\frac{\cos(kx)}{1 + |x|}dx=\lim_{R\to\infty}2\int_0^R\frac{\cos(kx)}{1+x}dx u=\frac{1}{1+x} du=\frac{-1}{(1+x)^2}dx v=\frac{1}{k}\sin(kx) dv=\sin(kx)dx 2\int_0^R \frac{\cos(kx)}{1+x}dx = \left.\frac{2}{k}\frac{\sin(kx)}{1+x}\right|_0^R-2\int_0^R \frac{-1}{k(1+x)^2}\sin(kx)dx. \left|\int_0^R \frac{\sin(kx)}{(1+x)^2}dx\right|\leq \int_0^R\frac{1}{(1+x)^2}dx=\left.\frac{-1}{1+x}\right|_0^R= \frac{-1}{1+R} + 1 -\frac{2}{|k|}\leq I(k)\leq \frac{2}{\left|k\right|} -2|k|^{m-1}\leq |k|^mI(k)\leq 2|k|^{m-1} 
\lim _{k \rightarrow 0}|k|^m I(k)=0
 m> 1 m < 1 \sin(-x)=-\sin(x) \lim_{k\to 0}|k|^{m}I(x) =\lim_{k\to 0}\frac{2|k|^{m}}{k} \int_0^R \frac{\sin(kx)}{1 + |x|}dx=\lim_{k\to 0}2|k|^{m-1} \int_0^R \frac{\sin(kx)}{1 + |x|}dx = \infty m=1 \lim_{k\to 0}|k|^{m}I(x) =\lim_{k\to 0} \int_0^R \frac{\sin(kx)}{1 + |x|}dx\neq 0.","['real-analysis', 'calculus', 'complex-analysis', 'analysis']"
57,Divisors Sum Related Interesting Approximate Relation,Divisors Sum Related Interesting Approximate Relation,,"Working on Divisors Sum Efficient calulcation topic. Accidentaly discovered one interesting relation which is accurate up to $10^{17}$ order. $$\sum_{i=1}^{\infty}{\frac{\sigma(i)}{e^{i}}}\approx\frac{\pi^2}{6}-\frac{1}{2}+\frac{1}{24}$$ To get things more clear look at the below numbers: $$\sum_{i=1}^{\infty}{\frac{\sigma(i)}{e^{i}}}=1.1866007335148928206...$$ $$\frac{\pi^2}{6}-\frac{1}{2}+\frac{1}{24}=1.1866007335148931031...$$ Just would like to share this nice relation, check if you know some paper about this and wondering if there are some similar known relations for other number theoretical functions:) Just to clarify things this is not the only realation, but one of many, for example: $$\sum_{i=1}^{\infty}{\frac{\sigma(i)}{\sqrt{e^i}}}\approx\frac{2\pi^2}{3}-1+\frac{1}{24}$$ EDITED Accorging to @Greg Martin comment just realized that this is Lambert Series example. Lets assume now $s>0$ . So the general rule is $$\sum_{i=1}^{\infty}{\frac{\sigma(i)}{e^{si}}}=\sum_{i=1}^{\infty}{\frac{i}{e^{si}-1}}$$ Using Euler-Maclaurin summation: $$\sum_{i=1}^{\infty}{\frac{i}{e^{si}-1}}=\int_{0}^{\infty}\frac{x}{e^{sx}-1}dx - \frac{1}{2}\big(\lim_{x\to0}\frac{x}{e^{sx}-1}+\lim_{x\to\infty}\frac{x}{e^{sx}-1}\big)+\frac{1}{12}\big(\lim_{x\to\infty}(\frac{x}{e^{sx}-1})'-\lim_{x\to0}(\frac{x}{e^{sx}-1})'\big)-\frac{1}{720}\big(\lim_{x\to\infty}(\frac{x}{e^{sx}-1})'''-\lim_{x\to0}(\frac{x}{e^{sx}-1})'''\big)+...$$ Here all the higher order derivatives are odd. If we look at the $\frac{x}{e^{sx}-1}$ all the odd derivatives $(\frac{x}{e^{sx}-1})^{(2k-1)}$ at $x\to0$ and $\infty$ are equal $0$ except the first derivative. Here is the list of few odd derivatives: $$\lim_{x\to0}\big(\frac{x}{e^{sx}-1}\big)=\frac{1}{s}; \lim_{x\to\infty}\big(\frac{x}{e^{sx}-1}\big)=0$$ $$\lim_{x\to0}\big(\frac{x}{e^{sx}-1}\big)'=-\frac{1}{2}; \lim_{x\to\infty}\big(\frac{x}{e^{sx}-1}\big)'=0$$ $$\lim_{x\to0}\big(\frac{x}{e^{sx}-1}\big)'''=0; \lim_{x\to\infty}\big(\frac{x}{e^{sx}-1}\big)'''=0$$ $$\lim_{x\to0}\big(\frac{x}{e^{sx}-1}\big)^{(5)}=0; \lim_{x\to\infty}\big(\frac{x}{e^{sx}-1}\big)^{(5)}=0$$ and integral equals to $$\int_{0}^{\infty}\frac{x}{e^{sx}-1}dx=\frac{\pi^2}{6s^2}$$ So finally we have: $$\sum_{i=1}^{\infty}{\frac{\sigma(i)}{e^{si}}}=\frac{\pi^2}{6s^2}-\frac{1}{2s}+\frac{1}{24}$$ Conclusion Even after above formulas, the calculation shows that the real values are not matching. For example case for $s=1$ . Any idea why this formula does not work?","Working on Divisors Sum Efficient calulcation topic. Accidentaly discovered one interesting relation which is accurate up to order. To get things more clear look at the below numbers: Just would like to share this nice relation, check if you know some paper about this and wondering if there are some similar known relations for other number theoretical functions:) Just to clarify things this is not the only realation, but one of many, for example: EDITED Accorging to @Greg Martin comment just realized that this is Lambert Series example. Lets assume now . So the general rule is Using Euler-Maclaurin summation: Here all the higher order derivatives are odd. If we look at the all the odd derivatives at and are equal except the first derivative. Here is the list of few odd derivatives: and integral equals to So finally we have: Conclusion Even after above formulas, the calculation shows that the real values are not matching. For example case for . Any idea why this formula does not work?",10^{17} \sum_{i=1}^{\infty}{\frac{\sigma(i)}{e^{i}}}\approx\frac{\pi^2}{6}-\frac{1}{2}+\frac{1}{24} \sum_{i=1}^{\infty}{\frac{\sigma(i)}{e^{i}}}=1.1866007335148928206... \frac{\pi^2}{6}-\frac{1}{2}+\frac{1}{24}=1.1866007335148931031... \sum_{i=1}^{\infty}{\frac{\sigma(i)}{\sqrt{e^i}}}\approx\frac{2\pi^2}{3}-1+\frac{1}{24} s>0 \sum_{i=1}^{\infty}{\frac{\sigma(i)}{e^{si}}}=\sum_{i=1}^{\infty}{\frac{i}{e^{si}-1}} \sum_{i=1}^{\infty}{\frac{i}{e^{si}-1}}=\int_{0}^{\infty}\frac{x}{e^{sx}-1}dx - \frac{1}{2}\big(\lim_{x\to0}\frac{x}{e^{sx}-1}+\lim_{x\to\infty}\frac{x}{e^{sx}-1}\big)+\frac{1}{12}\big(\lim_{x\to\infty}(\frac{x}{e^{sx}-1})'-\lim_{x\to0}(\frac{x}{e^{sx}-1})'\big)-\frac{1}{720}\big(\lim_{x\to\infty}(\frac{x}{e^{sx}-1})'''-\lim_{x\to0}(\frac{x}{e^{sx}-1})'''\big)+... \frac{x}{e^{sx}-1} (\frac{x}{e^{sx}-1})^{(2k-1)} x\to0 \infty 0 \lim_{x\to0}\big(\frac{x}{e^{sx}-1}\big)=\frac{1}{s}; \lim_{x\to\infty}\big(\frac{x}{e^{sx}-1}\big)=0 \lim_{x\to0}\big(\frac{x}{e^{sx}-1}\big)'=-\frac{1}{2}; \lim_{x\to\infty}\big(\frac{x}{e^{sx}-1}\big)'=0 \lim_{x\to0}\big(\frac{x}{e^{sx}-1}\big)'''=0; \lim_{x\to\infty}\big(\frac{x}{e^{sx}-1}\big)'''=0 \lim_{x\to0}\big(\frac{x}{e^{sx}-1}\big)^{(5)}=0; \lim_{x\to\infty}\big(\frac{x}{e^{sx}-1}\big)^{(5)}=0 \int_{0}^{\infty}\frac{x}{e^{sx}-1}dx=\frac{\pi^2}{6s^2} \sum_{i=1}^{\infty}{\frac{\sigma(i)}{e^{si}}}=\frac{\pi^2}{6s^2}-\frac{1}{2s}+\frac{1}{24} s=1,"['real-analysis', 'number-theory', 'approximation', 'divisor-sum']"
58,Sequence of polynomials $\{P_n\}$ of bounded degree such that $\int_{\mathbb{R}^d}P_n(x)\varphi(x){\rm d}x$ converges for all test functions $\varphi$,Sequence of polynomials  of bounded degree such that  converges for all test functions,\{P_n\} \int_{\mathbb{R}^d}P_n(x)\varphi(x){\rm d}x \varphi,"I was wondering that: Suppose that $\{P_n\}$ is a sequence of polynomials with degree $\le p$ such that for every test function $\varphi\in C^\infty_c(\mathbb{R}^d)$ , the sequence of integrals $\left\{\displaystyle\int_{\mathbb{R}^d}P_n(x)\varphi(x){\rm d}x\right\}$ converges. Must the coefficients of $\{P_n\}$ of each degree converge? Here $C^\infty_c(\mathbb{R}^d)$ is the space of smooth functions over $\mathbb{R}^d$ with compact support. I would like to find a function $\varphi$ with compact support such that, for a given multiindex $|\alpha_0|\le p$ , we have $$ \int_{\mathbb{R}^d}x^{\alpha_0}\varphi(x){\rm d}x\neq 0,\quad\int_{\mathbb{R}^d}x^\alpha\varphi(x){\rm d}x=0,\quad\forall|\alpha|\le p,\alpha\neq\alpha_0, $$ then plugging this function into the integral shows that the $\alpha$ -coefficients of $\{P_n\}$ converge. Could you please give some strategies to construct such $\varphi$ if it does exist, or is there any more elegant way to think about this question? Edit: The following result would be applicable. (Banach-Steinhaus theorem of distributions) Suppose that $\{T_n\}$ is a sequence of distributions over an open set $\Omega\subset\mathbb{R}^d$ , and that $\langle T_n,\varphi\rangle$ converges for every $\varphi\in C^\infty_c(\Omega)$ , then $\{T_n\}$ converges in $D'(\Omega)$ (the space of distributions). So we know that $\{P_n\}$ converges to some distribution $T$ . Moreover, $$ 0=\partial^{\alpha}P_n\to\partial^{\alpha}T,\quad\forall|\alpha|=p+1, $$ so $\partial^{\alpha}T=0$ for every $|\alpha|=p+1$ . It is standard (can be shown by induction on $p$ ) that $T$ is itself defined by a polynomial $P$ with degree $\le p$ . By subtracting $P$ we can suppose that $$ \lim_{n\to\infty}\displaystyle\int_{\mathbb{R}^d}P_n(x)\varphi(x){\rm d}x=0,\quad\forall\varphi\in C^\infty_c(\mathbb{R}^d), $$ and I want to show that the coefficients of $\{P_n\}$ of each degree converge to $0$ .","I was wondering that: Suppose that is a sequence of polynomials with degree such that for every test function , the sequence of integrals converges. Must the coefficients of of each degree converge? Here is the space of smooth functions over with compact support. I would like to find a function with compact support such that, for a given multiindex , we have then plugging this function into the integral shows that the -coefficients of converge. Could you please give some strategies to construct such if it does exist, or is there any more elegant way to think about this question? Edit: The following result would be applicable. (Banach-Steinhaus theorem of distributions) Suppose that is a sequence of distributions over an open set , and that converges for every , then converges in (the space of distributions). So we know that converges to some distribution . Moreover, so for every . It is standard (can be shown by induction on ) that is itself defined by a polynomial with degree . By subtracting we can suppose that and I want to show that the coefficients of of each degree converge to .","\{P_n\} \le p \varphi\in C^\infty_c(\mathbb{R}^d) \left\{\displaystyle\int_{\mathbb{R}^d}P_n(x)\varphi(x){\rm d}x\right\} \{P_n\} C^\infty_c(\mathbb{R}^d) \mathbb{R}^d \varphi |\alpha_0|\le p 
\int_{\mathbb{R}^d}x^{\alpha_0}\varphi(x){\rm d}x\neq 0,\quad\int_{\mathbb{R}^d}x^\alpha\varphi(x){\rm d}x=0,\quad\forall|\alpha|\le p,\alpha\neq\alpha_0,
 \alpha \{P_n\} \varphi \{T_n\} \Omega\subset\mathbb{R}^d \langle T_n,\varphi\rangle \varphi\in C^\infty_c(\Omega) \{T_n\} D'(\Omega) \{P_n\} T 
0=\partial^{\alpha}P_n\to\partial^{\alpha}T,\quad\forall|\alpha|=p+1,
 \partial^{\alpha}T=0 |\alpha|=p+1 p T P \le p P 
\lim_{n\to\infty}\displaystyle\int_{\mathbb{R}^d}P_n(x)\varphi(x){\rm d}x=0,\quad\forall\varphi\in C^\infty_c(\mathbb{R}^d),
 \{P_n\} 0","['real-analysis', 'sequences-and-series', 'polynomials', 'distribution-theory']"
59,"Proof verification: Does $\sup_{n\in\mathbb N} \int_{\mathbb R}|F(x, u_n)|^2 dx \le M_1$ hold?",Proof verification: Does  hold?,"\sup_{n\in\mathbb N} \int_{\mathbb R}|F(x, u_n)|^2 dx \le M_1","I am going back to a question I posted yesterday. This is because I think I have made some steps forward. Let $(E, \|\cdot\|)$ denote a Hilbert space such that $E$ is continuously embedded in $L^q(\mathbb R)$ for any $q\in [2, 4]$ . Let $(u_n)$ be a bounded sequence in $E$ , i.e. there exists $M>0$ such that $\|u_n\|_{E}\le M$ . Let $p\in (2, 3)$ and $F:\mathbb R\times \mathbb R\to\mathbb R$ be a continuous function such that $$|F(x, u)|\le c_1 |u| +c_2 |u|^{p-1}\quad\mbox{ for a.e. } x\in\mathbb R, \ \forall u\in\mathbb R,$$ for some $c_1, c_2>0$ . Under these assumptions, I think that it is possible to conclude that $$\sup_{n\in\mathbb N} \int_{\mathbb R}|F(x, u_n)|^2 dx \le M_1,$$ for a constant $M_1>0$ . Indeed: $$ \begin{split} \int_{\mathbb R}|F(x, u_n)|^2 dx &\le c_1 \int_{\mathbb R} |u_n|^2 dx + c_1 c_2 \int_{\mathbb R} |u_n|^p dx + c_2^2 \int_{\mathbb R} |u_n|^{2(p-1)} dx \\ &\le k_1 \|u_n\|_E \le k_1 M =:M_1. \end{split} $$ Hence, passing to the supremum, one has $$\sup_{n\in\mathbb N} \int_{\mathbb R}|F(x, u_n)|^2 dx \le M_1.$$ Anyone could please tell me if I argued it right? Thank you in advance.","I am going back to a question I posted yesterday. This is because I think I have made some steps forward. Let denote a Hilbert space such that is continuously embedded in for any . Let be a bounded sequence in , i.e. there exists such that . Let and be a continuous function such that for some . Under these assumptions, I think that it is possible to conclude that for a constant . Indeed: Hence, passing to the supremum, one has Anyone could please tell me if I argued it right? Thank you in advance.","(E, \|\cdot\|) E L^q(\mathbb R) q\in [2, 4] (u_n) E M>0 \|u_n\|_{E}\le M p\in (2, 3) F:\mathbb R\times \mathbb R\to\mathbb R |F(x, u)|\le c_1 |u| +c_2 |u|^{p-1}\quad\mbox{ for a.e. } x\in\mathbb R, \ \forall u\in\mathbb R, c_1, c_2>0 \sup_{n\in\mathbb N} \int_{\mathbb R}|F(x, u_n)|^2 dx \le M_1, M_1>0 
\begin{split}
\int_{\mathbb R}|F(x, u_n)|^2 dx &\le c_1 \int_{\mathbb R} |u_n|^2 dx + c_1 c_2 \int_{\mathbb R} |u_n|^p dx + c_2^2 \int_{\mathbb R} |u_n|^{2(p-1)} dx \\
&\le k_1 \|u_n\|_E \le k_1 M =:M_1.
\end{split}
 \sup_{n\in\mathbb N} \int_{\mathbb R}|F(x, u_n)|^2 dx \le M_1.","['real-analysis', 'calculus', 'solution-verification', 'lp-spaces']"
60,An inductive proof for the Heine-Borel theorem in $\mathbb{R}^n$,An inductive proof for the Heine-Borel theorem in,\mathbb{R}^n,"The Heine-Borel theorem for $\mathbb{R}^n$ states that a subset $K\subset\mathbb{R}^n$ is closed and bounded if and only if it is compact . (A set is said to be compact if every open cover of the set has a finite sub-cover). The standard method to show that closed- and boundedness implies compactness starts by enclosing $K$ in a box, and continually bisecting it until some or other contradiction is obtained. I have an alternative approach, but I am not entirely sure it works, and I would like to know if it seems possible to extend this to an arbitrary metric space (by replacing closed and bounded by sequentially compact) beyond that, I think its interesting that an inductive proof for it exists, so here it is: Firstly, it is sufficient to prove that every closed ball centered at $0$ is compact, since one can always enclose a bounded set $K$ inside such a closed ball of radius $r$ , $U(r)$ , and append to any cover of $K$ the set $\mathbb{R}^n \setminus K$ , and finding a finite subcover of the new cover is then equivalent to finding a finite subcover of the original cover. The base case is trivial since $\mathbb{R}^{0}=\{0\}$ has only compact subsets. If we then assume that this theorem holds in $\mathbb{R}^k$ , we will show that it holds in $\mathbb{R}^{k+1}$ . Now let $U(r)\subseteq \mathbb{R}^{k+1}$ be the closed ball of radius $r$ centered at the origin, and let $G$ be a cover for $U(r)$ . Then we say that $l\in \mathbb{R}^+$ is reached if there is some finite subset $G'$ of $G$ such that $$U(l) \subseteq \bigcup G'  $$ Now define $w$ as supremum of those $l\leq r$ which are reached. Now we assume that $w<r$ and obtain a contradiction. Note that since $$\partial U(w) \subseteq U(r) \subseteq \bigcup G $$ We have that $\partial U(w)$ is covered by $G$ . But $\partial U(w)$ is a $k$ -dimensional sphere, so it's upper (inclusive) hemisphere is homeomorphic to the closed ball in $\mathbb{R}^k$ , which is compact by assumption, so there is a finite subset of $G$ which covers the upper hemisphere of $\partial U(w) $ , the same is of course true for the lower hemisphere, so there is a finite subset of $G$ , call it $G_0$ , which covers $\partial U(w) $ . It is then easy to show that $G_0\cup G'$ , where $G'$ is a finite subset of $G$ , covers an open ball with a radius larger than $w$ (the proof for this is included at the end) and thus it covers a closed ball with a radius larger than $w$ , which is a contradiction, since $w$ was assumed to be the supremum of reached radii. Some of my questions are, is this a known proof? Is it correct? Proof that an open ball of radius larger than $w$ is covered by a finite subset of $G$ : To see why, it is sufficient to show that there is some $\delta>0$ such that for any $x\in\partial U(w)$ , the open ball of radius $\delta$ is contained in some member of $G_0$ . Suppose this is not true, then for any $\delta>0$ there is some $x\in\partial U(w)$ such that the open ball of radius $\delta$ is not contained in any member of $G_0$ , we can in particular take the convergent sequence of $\delta_n=\frac{1}{n}$ , which induces a sequence $(x_n)$ with elements in $\partial U(w)$ , but because of sequential compactness, this sequence converges to some point $x_0\in\partial U(w)$ but this is a contradiction since $x_0$ is contained in some open ball of radius $\delta_0$ (because $G_0$ is an open cover for $\partial U(w)$ ) and so eventually the $x_n$ 's will be within $\frac{\delta_0}{2}$ of $x_0$ , but then each $x_n$ beyond this point is contained in an open ball of radius $\frac{\delta_0}{2}$ , a contradiction, so there is some $\delta>0$ such that every point in $\partial U(w)$ is contained in the open ball of radius $\delta$ around that point. A routine application of the triangle inequality then shows that for any $|\varepsilon|\leq \frac{\delta}{2}$ we have $$\partial U(w+\varepsilon) \in \bigcup G_0$$ Since there is some finite subset $G'\subseteq G$ such that $$U\left(w-\frac{\delta}{2}\right)\subseteq \bigcup G' $$ So $G'\cup G_0$ covers $U(w+\frac{\delta}{2})$ which is the contradiction we required in the original proof","The Heine-Borel theorem for states that a subset is closed and bounded if and only if it is compact . (A set is said to be compact if every open cover of the set has a finite sub-cover). The standard method to show that closed- and boundedness implies compactness starts by enclosing in a box, and continually bisecting it until some or other contradiction is obtained. I have an alternative approach, but I am not entirely sure it works, and I would like to know if it seems possible to extend this to an arbitrary metric space (by replacing closed and bounded by sequentially compact) beyond that, I think its interesting that an inductive proof for it exists, so here it is: Firstly, it is sufficient to prove that every closed ball centered at is compact, since one can always enclose a bounded set inside such a closed ball of radius , , and append to any cover of the set , and finding a finite subcover of the new cover is then equivalent to finding a finite subcover of the original cover. The base case is trivial since has only compact subsets. If we then assume that this theorem holds in , we will show that it holds in . Now let be the closed ball of radius centered at the origin, and let be a cover for . Then we say that is reached if there is some finite subset of such that Now define as supremum of those which are reached. Now we assume that and obtain a contradiction. Note that since We have that is covered by . But is a -dimensional sphere, so it's upper (inclusive) hemisphere is homeomorphic to the closed ball in , which is compact by assumption, so there is a finite subset of which covers the upper hemisphere of , the same is of course true for the lower hemisphere, so there is a finite subset of , call it , which covers . It is then easy to show that , where is a finite subset of , covers an open ball with a radius larger than (the proof for this is included at the end) and thus it covers a closed ball with a radius larger than , which is a contradiction, since was assumed to be the supremum of reached radii. Some of my questions are, is this a known proof? Is it correct? Proof that an open ball of radius larger than is covered by a finite subset of : To see why, it is sufficient to show that there is some such that for any , the open ball of radius is contained in some member of . Suppose this is not true, then for any there is some such that the open ball of radius is not contained in any member of , we can in particular take the convergent sequence of , which induces a sequence with elements in , but because of sequential compactness, this sequence converges to some point but this is a contradiction since is contained in some open ball of radius (because is an open cover for ) and so eventually the 's will be within of , but then each beyond this point is contained in an open ball of radius , a contradiction, so there is some such that every point in is contained in the open ball of radius around that point. A routine application of the triangle inequality then shows that for any we have Since there is some finite subset such that So covers which is the contradiction we required in the original proof",\mathbb{R}^n K\subset\mathbb{R}^n K 0 K r U(r) K \mathbb{R}^n \setminus K \mathbb{R}^{0}=\{0\} \mathbb{R}^k \mathbb{R}^{k+1} U(r)\subseteq \mathbb{R}^{k+1} r G U(r) l\in \mathbb{R}^+ G' G U(l) \subseteq \bigcup G'   w l\leq r w<r \partial U(w) \subseteq U(r) \subseteq \bigcup G  \partial U(w) G \partial U(w) k \mathbb{R}^k G \partial U(w)  G G_0 \partial U(w)  G_0\cup G' G' G w w w w G \delta>0 x\in\partial U(w) \delta G_0 \delta>0 x\in\partial U(w) \delta G_0 \delta_n=\frac{1}{n} (x_n) \partial U(w) x_0\in\partial U(w) x_0 \delta_0 G_0 \partial U(w) x_n \frac{\delta_0}{2} x_0 x_n \frac{\delta_0}{2} \delta>0 \partial U(w) \delta |\varepsilon|\leq \frac{\delta}{2} \partial U(w+\varepsilon) \in \bigcup G_0 G'\subseteq G U\left(w-\frac{\delta}{2}\right)\subseteq \bigcup G'  G'\cup G_0 U(w+\frac{\delta}{2}),"['real-analysis', 'solution-verification', 'metric-spaces', 'induction']"
61,Orthonormal basis of $L^2(X \times Y)$,Orthonormal basis of,L^2(X \times Y),"Let $(X, \mathscr{S}, \mu)$ and $(Y, \mathscr{T}, \lambda)$ be two $\sigma$ -finite measure spaces. Let $\{ \phi_i\}_{\in I}$ , $\{\psi_j\}_{j \in J}$ be two orthonormal bases for $L^2(X,\mu)$ and $L^2(Y, \lambda)$ respectively. For $i \in I$ , $j \in J$ , let $$\theta_{ij}(x,y):=\phi_i(x)\psi_j(y), \,\,\,\, (x\in X, y \in Y).$$ Is it true that $\{\theta_{ij}\}_{i \in I, j \in J}$ is an orthonormal basis for $L^2(X \times Y, \mu \times \lambda)$ ? Answer is yes whenever either $L^2(X)$ or $L^2(Y)$ is separable. See for instance the following. Orthonormal basis for product $L^2$ space The question is precisely for the case when both $L^2(X)$ and $L^2(Y)$ are not separable i.e. both $I$ and $J$ are uncountable.","Let and be two -finite measure spaces. Let , be two orthonormal bases for and respectively. For , , let Is it true that is an orthonormal basis for ? Answer is yes whenever either or is separable. See for instance the following. Orthonormal basis for product $L^2$ space The question is precisely for the case when both and are not separable i.e. both and are uncountable.","(X, \mathscr{S}, \mu) (Y, \mathscr{T}, \lambda) \sigma \{ \phi_i\}_{\in I} \{\psi_j\}_{j \in J} L^2(X,\mu) L^2(Y, \lambda) i \in I j \in J \theta_{ij}(x,y):=\phi_i(x)\psi_j(y), \,\,\,\, (x\in X, y \in Y). \{\theta_{ij}\}_{i \in I, j \in J} L^2(X \times Y, \mu \times \lambda) L^2(X) L^2(Y) L^2(X) L^2(Y) I J","['real-analysis', 'functional-analysis', 'measure-theory']"
62,Approximating a function of bounded variation function by a stepfunction obtained by averaging over measurable sets,Approximating a function of bounded variation function by a stepfunction obtained by averaging over measurable sets,,"Inspired by this question, suppose that we have a function $f\in\mathrm{BV}([0,1])\cap L^1([0,1])$ , take a measurable partition $\mathcal P^N$ of $[0,1]=\bigcup_{i=1}^NI_i$ , and obtain a new function $f^N$ obtained by averaging over the sets $I_i$ : when $x\in I_i$ , $$f^N(x)=\frac1{|I_i|}\int_{I_i}f(y)\ \mathrm dy.$$ As in the linked question, we have $f^N\in L^1[0,1]$ , and if the mesh of the partition tends to $0$ when $N\to\infty$ , we can argue that $f^N\to f$ a.e. as $N\to\infty$ . I was wondering whether it is possible in this case to prove that $$\|f^N-f\|_{L^1[0,1]}\leq\|f\|_{TV}\mathrm{mesh}(\mathcal P^N),$$ though the approach of the linked question didn't seem to work here directly. I hope someone can help. EDIT: as Alex Ravsky's counterexample shows, we cannot just take $\mathcal P^N$ to be any measurable partition. Instead, let's assume that we have partitions of the form $0=t_0<t_1<\cdots<t_{n-1}<t_n=1$ of the unit interval in disjoint, consecutive intervals $[t_{k-1}, t_k)$ . In this case, I'm still wondering if the result holds. A similar bound is not hard to prove if we assume that the size of the smallest and largest set in the $N$ th partition do not differ by more than a factor $C$ , independent of $N$ . However, I'm particularly interested in not making that assumption.","Inspired by this question, suppose that we have a function , take a measurable partition of , and obtain a new function obtained by averaging over the sets : when , As in the linked question, we have , and if the mesh of the partition tends to when , we can argue that a.e. as . I was wondering whether it is possible in this case to prove that though the approach of the linked question didn't seem to work here directly. I hope someone can help. EDIT: as Alex Ravsky's counterexample shows, we cannot just take to be any measurable partition. Instead, let's assume that we have partitions of the form of the unit interval in disjoint, consecutive intervals . In this case, I'm still wondering if the result holds. A similar bound is not hard to prove if we assume that the size of the smallest and largest set in the th partition do not differ by more than a factor , independent of . However, I'm particularly interested in not making that assumption.","f\in\mathrm{BV}([0,1])\cap L^1([0,1]) \mathcal P^N [0,1]=\bigcup_{i=1}^NI_i f^N I_i x\in I_i f^N(x)=\frac1{|I_i|}\int_{I_i}f(y)\ \mathrm dy. f^N\in L^1[0,1] 0 N\to\infty f^N\to f N\to\infty \|f^N-f\|_{L^1[0,1]}\leq\|f\|_{TV}\mathrm{mesh}(\mathcal P^N), \mathcal P^N 0=t_0<t_1<\cdots<t_{n-1}<t_n=1 [t_{k-1}, t_k) N C N","['real-analysis', 'analysis', 'measure-theory', 'bounded-variation']"
63,Uniqueness of the root for a DoG function (Difference of Gaussian),Uniqueness of the root for a DoG function (Difference of Gaussian),,"I am struggling with the following problem: Let $f$ be a real function such that: $f\in\mathcal{C}^\infty(\mathbb{R},\mathbb{R})$ , $f$ is strictly convex on $(-\infty,0)$ , strictly concave on $(0,\infty)$ , strictly increasing on $\mathbb{R}$ , (exponential growth ( $f(x) = o(e^{D|x|})$ )? just to suppose the following integral well defined). Let $a_1>a_2>0$ two real numbers. Can one prove (or give a counter-example to) the following statment: The function $g$ defined by $$g(x) := \int_\mathbb{R}\left(f(x+a_1s)-f(x+a_2s)\right)e^{-s^2/2}ds$$ has a unique $0$ on $\mathbb{R}$ . I am (numerically) convinced that the statement is true. Any hint, counter-example or help will be highly appreciated ! Thank you very much. Numerical example Here a picture showing the function $f = \arctan$ and the function $g$ . The difficulty is that $g$ is not increasing function... Note that the function $f$ is not necessarly odd. Possible hints The function $g$ can be as a DoG (difference of gaussians) function: $$g = f*(G_{a_1}-G_{a_2}).$$ DoG is known to be a possible approximation of gaussian laplacian, which is a smoothed version of the Laplacian. If there is one inflection point, we could imagine that the DoG function will have one zero point. One way to proceed is to show two points: If the Gaussian of Laplacian of $f$ (denoted $\text{LoG}(f)$ ) has two zeros, the Laplacian of $f$ $\Delta f$ has two zeros. ( This point is already proved ) If the $\text{DoG}$ function has two zeros, $\text{LoG}(f)$ must have two zeros. (This has to be proved). If one manage to prove the second point, the result follows by contradiction (single 0 of $\Delta f$ ).","I am struggling with the following problem: Let be a real function such that: , is strictly convex on , strictly concave on , strictly increasing on , (exponential growth ( )? just to suppose the following integral well defined). Let two real numbers. Can one prove (or give a counter-example to) the following statment: The function defined by has a unique on . I am (numerically) convinced that the statement is true. Any hint, counter-example or help will be highly appreciated ! Thank you very much. Numerical example Here a picture showing the function and the function . The difficulty is that is not increasing function... Note that the function is not necessarly odd. Possible hints The function can be as a DoG (difference of gaussians) function: DoG is known to be a possible approximation of gaussian laplacian, which is a smoothed version of the Laplacian. If there is one inflection point, we could imagine that the DoG function will have one zero point. One way to proceed is to show two points: If the Gaussian of Laplacian of (denoted ) has two zeros, the Laplacian of has two zeros. ( This point is already proved ) If the function has two zeros, must have two zeros. (This has to be proved). If one manage to prove the second point, the result follows by contradiction (single 0 of ).","f f\in\mathcal{C}^\infty(\mathbb{R},\mathbb{R}) f (-\infty,0) (0,\infty) \mathbb{R} f(x) = o(e^{D|x|}) a_1>a_2>0 g g(x) := \int_\mathbb{R}\left(f(x+a_1s)-f(x+a_2s)\right)e^{-s^2/2}ds 0 \mathbb{R} f = \arctan g g f g g = f*(G_{a_1}-G_{a_2}). f \text{LoG}(f) f \Delta f \text{DoG} \text{LoG}(f) \Delta f","['real-analysis', 'integration', 'convex-analysis', 'convolution']"
64,"If I've learned real analysis, should I go back and (re)learn calculus properly?","If I've learned real analysis, should I go back and (re)learn calculus properly?",,"If you've learned real analysis, what, if any, is the value in going back and learning calculus ""properly"", via, let's say, Spivak or Courant? I've learned real analysis using the texts of Ross and of Abbott (not yet Rudin).  My initial study of calculus was Stewart-style: Here's the procedure, do it, get an A.  That leaves many gaps, both in depth of understanding and even in fluency (it's hard to memorize a procedure that you don't really understand).  Should I invest time in relearning single variable calculus from a classic text such as Spivak or Courant? If so, how should I approach it, and what should my goals be? Similarly for multivariable calculus: I learned it using Stewart and did very well in the class.  Which means I learned next to nothing.  And I certainly didn't encounter things like the Implicit Function Theorem.  Should I go back and relearn multivariable calculus, using, e.g. Hubbard or Shifrin? Do I move forward (Rudin) or back (single or multivariable calculus)? Update littleO asks ""Once you’ve learned real analysis... doesn’t that mean you’ve already learned single variable calculus properly... [since it] proves the main results of single variable calculus"".  In one sense that's true.  But, in another sense, the focus of a real analysis is on proving theorems, whereas a book like Spivak has many (challenging) computational problems.  Would you believe that someone can prove the Cauchy sequences converge implies the Nested Interval Theorem, but perhaps get confused when doing integration by substitution? Or that they find a fresh presentation of the development of the integral of $x^n$ using first principles (Courant develops it using limits of series, even before introducing the Fundamental Theorem of Calculus), to be very enlightening? So even if you've proven the main results, there still a lot to be gained from learning, the ""right"" way, how to develop the basic procedures from fundamentals, and how to use them to solve problems. Put another way, we have three things: Procedure focused.  E.g. Stewart. Theorem & proof focused.  E.g. real analysis. Using procedures, but a) developing them from first principles and b) using them masterfully.  E.g. Spivak, Apostol, Courant. I've found that #1 alone is insufficient: it leaves big gaps, and even the procedures don't stick.  #2 is excellent - and there's no limit on how far and deep you can go.  But something tells me that before I do so, I need to go back and master the procedures , including their development and usage.  I'm not sure if that's correct and if so how I should do it without being stuck in repetition and without failing to advance.","If you've learned real analysis, what, if any, is the value in going back and learning calculus ""properly"", via, let's say, Spivak or Courant? I've learned real analysis using the texts of Ross and of Abbott (not yet Rudin).  My initial study of calculus was Stewart-style: Here's the procedure, do it, get an A.  That leaves many gaps, both in depth of understanding and even in fluency (it's hard to memorize a procedure that you don't really understand).  Should I invest time in relearning single variable calculus from a classic text such as Spivak or Courant? If so, how should I approach it, and what should my goals be? Similarly for multivariable calculus: I learned it using Stewart and did very well in the class.  Which means I learned next to nothing.  And I certainly didn't encounter things like the Implicit Function Theorem.  Should I go back and relearn multivariable calculus, using, e.g. Hubbard or Shifrin? Do I move forward (Rudin) or back (single or multivariable calculus)? Update littleO asks ""Once you’ve learned real analysis... doesn’t that mean you’ve already learned single variable calculus properly... [since it] proves the main results of single variable calculus"".  In one sense that's true.  But, in another sense, the focus of a real analysis is on proving theorems, whereas a book like Spivak has many (challenging) computational problems.  Would you believe that someone can prove the Cauchy sequences converge implies the Nested Interval Theorem, but perhaps get confused when doing integration by substitution? Or that they find a fresh presentation of the development of the integral of using first principles (Courant develops it using limits of series, even before introducing the Fundamental Theorem of Calculus), to be very enlightening? So even if you've proven the main results, there still a lot to be gained from learning, the ""right"" way, how to develop the basic procedures from fundamentals, and how to use them to solve problems. Put another way, we have three things: Procedure focused.  E.g. Stewart. Theorem & proof focused.  E.g. real analysis. Using procedures, but a) developing them from first principles and b) using them masterfully.  E.g. Spivak, Apostol, Courant. I've found that #1 alone is insufficient: it leaves big gaps, and even the procedures don't stick.  #2 is excellent - and there's no limit on how far and deep you can go.  But something tells me that before I do so, I need to go back and master the procedures , including their development and usage.  I'm not sure if that's correct and if so how I should do it without being stuck in repetition and without failing to advance.",x^n,"['real-analysis', 'calculus', 'soft-question', 'self-learning']"
65,"Given two unbounded subsets $X,Y$ of $\mathbb{R},$ do there exist three points of $X$ whose translation and stretch approximates three points of $Y?$",Given two unbounded subsets  of  do there exist three points of  whose translation and stretch approximates three points of,"X,Y \mathbb{R}, X Y?","Suppose $X$ and $Y$ each are subsets of $\mathbb{R}$ that are bounded below and unbounded above (and therefore infinite). Given $\varepsilon>0,\ $ do there exist $\ x_1,\ x_2,\ x_3 \in X;\ x_1 < x_2 < x_3;\ \quad y_1,\ y_2,\ y_3 \in Y;\ y_1 < y_2 < y_3,\ $ such that $$ 1 - \varepsilon < \left\lvert \frac{ \frac{x_2 - x_1}{x_3 - x_1}}{\frac{y_2 - y_1}{y_3 - y_1}} \right\rvert < 1+\varepsilon\quad ?$$ This is equivalent to saying that we can find (many) pairs of triplets $(x_1,x_2,x_3)$ and $(y_1,y_2,y_3),$ such that $\ \frac{x_2 - x_1}{x_3 - x_1}$ and $ \frac{y_2 - y_1}{y_3 - y_1}\ $ are relatively arbitrarily close, which means that each pair of triplets $(x_1,x_2,x_3)$ and $(y_1,y_2,y_3),$ ""look similar to each other"" albeit for a scaling and translation. Is $X = \{2^n:\ n\in\mathbb{N} \};\ Y = \{3^n:\ n\in\mathbb{N} \}\ $ a counter-example? I am not sure... And maybe you construct a back-and-forth counter-example if $X$ and $Y$ are finite sets of arbitrary size, but I'm not sure this construction would extend to $X$ and $Y$ being infinite sets. On the other hand, it seems plausible that there are not good back-and-forth counter-examples if $X$ and $Y$ are finite sets of arbitrary size by some application of the Pigeonhole principle. But I'm not sure about what's true, so it will require more thought. Also, Stolz-Cesàro theorem might play a role.","Suppose and each are subsets of that are bounded below and unbounded above (and therefore infinite). Given do there exist such that This is equivalent to saying that we can find (many) pairs of triplets and such that and are relatively arbitrarily close, which means that each pair of triplets and ""look similar to each other"" albeit for a scaling and translation. Is a counter-example? I am not sure... And maybe you construct a back-and-forth counter-example if and are finite sets of arbitrary size, but I'm not sure this construction would extend to and being infinite sets. On the other hand, it seems plausible that there are not good back-and-forth counter-examples if and are finite sets of arbitrary size by some application of the Pigeonhole principle. But I'm not sure about what's true, so it will require more thought. Also, Stolz-Cesàro theorem might play a role.","X Y \mathbb{R} \varepsilon>0,\  \ x_1,\ x_2,\ x_3 \in X;\ x_1 < x_2 < x_3;\ \quad y_1,\ y_2,\ y_3 \in Y;\ y_1 < y_2 < y_3,\   1 - \varepsilon < \left\lvert \frac{ \frac{x_2 - x_1}{x_3 - x_1}}{\frac{y_2 - y_1}{y_3 - y_1}} \right\rvert < 1+\varepsilon\quad ? (x_1,x_2,x_3) (y_1,y_2,y_3), \ \frac{x_2 - x_1}{x_3 - x_1}  \frac{y_2 - y_1}{y_3 - y_1}\  (x_1,x_2,x_3) (y_1,y_2,y_3), X = \{2^n:\ n\in\mathbb{N} \};\ Y = \{3^n:\ n\in\mathbb{N} \}\  X Y X Y X Y","['real-analysis', 'approximation', 'real-numbers', 'pigeonhole-principle', 'diophantine-approximation']"
66,Minimizing Dirichlet Energy with Constraints,Minimizing Dirichlet Energy with Constraints,,"I am interested in minimizing the Dirichlet energy $$E[f]=\iint_{D_1(\mathbf{0})} \|\nabla f\|^2 \mathrm{d}A ,$$ over the unit disk $D_1(\mathbf{0})=\{(x_1,x_2) \in \mathbf{R}^2:x_1^2+x_2^2 \leq 1\}$ given equality constraints of the form $\{f(\mathbf{x}_n)=y_n:1 \leq n \leq N \}$ with $\|\mathbf{x}_n\|=1$ and $y_n \in \mathbf{R}$ . I know that specifying $f$ on the boundary in full, in terms of a continuous function, determines the solution entirely as the harmonic function given by Poisson's formula $$f(r \cos \theta, r \sin \theta)=\frac{1}{2\pi} \int_{0}^{2\pi} \frac{1-r^2}{1-2r\cos(\theta-t)+r^2}f(\cos t,\sin t) \mathrm{d}t. $$ However, I'm not sure what happens when only a select few boundary values are provided. In summary, I want to solve the following optimization problem: $$\operatorname{argmin} _f\iint_{x_1^2+x_2^2\leq 1} \|\nabla f(x_1,x_2)\|^2 \mathrm{d}x_1 \mathrm{d}x_2\\ s.t. f(\cos(t_n),\sin(t_n))=y_n, \quad 1 \leq n \leq N. $$ My attempt: Obviously if there is only one constraint $f(\mathbf{x}_1)=y_1$ the solution is the constant function $f(x_1,x_2) \equiv y_1$ . I've also managed to express the Dirichlet energy in smaller disks as a quadratic form of the boundary values: $$E_R[f]=\iint_{D_R(\mathbf{0})} \|\nabla f\|^2 \mathrm{d}A = \frac{1}{4\pi^2} \int_0^{2\pi} \int_0^{2\pi} \frac{4 \pi R^2 \left( (1+R^4) \cos(t_1-t_2)-2R^2 \right)}{\left(1-2R^2 \cos(t_1-t_2)+R^4\right)^2} f(\cos t_1, \sin t_1) f(\cos t_2, \sin t_2)\mathrm{d}t_1 \mathrm{d}t_2. $$ Here $0 \leq R < 1$ , and unfortunately a straightforward limit $R \to 1$ appears to make the integral divergent. Letting $$K(t_1-t_2):=\frac{1}{4 \pi^2} \frac{4 \pi R^2 \left( (1+R^4) \cos(t_1-t_2)-2R^2 \right)}{\left(1-2R^2 \cos(t_1-t_2)+R^4\right)^2},$$ I computed the variational derivative with respect to $f$ and found the condition $$\int_0^{2\pi} K(\tau-t) f(\cos t, \sin t) \equiv 0, \quad \text{ for all } \tau.$$ I am stuck here as I don't know how to use the constraints in this variational approach. Any help on solving the problem with more than 1 constraint would be greatly appreciated. I would love to have the solution of the most general problem, but if that's too hard, special cases such as boundary values given on the vertices of a regular polygon would also be informative. Thanks.","I am interested in minimizing the Dirichlet energy over the unit disk given equality constraints of the form with and . I know that specifying on the boundary in full, in terms of a continuous function, determines the solution entirely as the harmonic function given by Poisson's formula However, I'm not sure what happens when only a select few boundary values are provided. In summary, I want to solve the following optimization problem: My attempt: Obviously if there is only one constraint the solution is the constant function . I've also managed to express the Dirichlet energy in smaller disks as a quadratic form of the boundary values: Here , and unfortunately a straightforward limit appears to make the integral divergent. Letting I computed the variational derivative with respect to and found the condition I am stuck here as I don't know how to use the constraints in this variational approach. Any help on solving the problem with more than 1 constraint would be greatly appreciated. I would love to have the solution of the most general problem, but if that's too hard, special cases such as boundary values given on the vertices of a regular polygon would also be informative. Thanks.","E[f]=\iint_{D_1(\mathbf{0})} \|\nabla f\|^2 \mathrm{d}A , D_1(\mathbf{0})=\{(x_1,x_2) \in \mathbf{R}^2:x_1^2+x_2^2 \leq 1\} \{f(\mathbf{x}_n)=y_n:1 \leq n \leq N \} \|\mathbf{x}_n\|=1 y_n \in \mathbf{R} f f(r \cos \theta, r \sin \theta)=\frac{1}{2\pi} \int_{0}^{2\pi} \frac{1-r^2}{1-2r\cos(\theta-t)+r^2}f(\cos t,\sin t) \mathrm{d}t.  \operatorname{argmin} _f\iint_{x_1^2+x_2^2\leq 1} \|\nabla f(x_1,x_2)\|^2 \mathrm{d}x_1 \mathrm{d}x_2\\ s.t. f(\cos(t_n),\sin(t_n))=y_n, \quad 1 \leq n \leq N.  f(\mathbf{x}_1)=y_1 f(x_1,x_2) \equiv y_1 E_R[f]=\iint_{D_R(\mathbf{0})} \|\nabla f\|^2 \mathrm{d}A = \frac{1}{4\pi^2} \int_0^{2\pi} \int_0^{2\pi} \frac{4 \pi R^2 \left( (1+R^4) \cos(t_1-t_2)-2R^2 \right)}{\left(1-2R^2 \cos(t_1-t_2)+R^4\right)^2} f(\cos t_1, \sin t_1) f(\cos t_2, \sin t_2)\mathrm{d}t_1 \mathrm{d}t_2.  0 \leq R < 1 R \to 1 K(t_1-t_2):=\frac{1}{4 \pi^2} \frac{4 \pi R^2 \left( (1+R^4) \cos(t_1-t_2)-2R^2 \right)}{\left(1-2R^2 \cos(t_1-t_2)+R^4\right)^2}, f \int_0^{2\pi} K(\tau-t) f(\cos t, \sin t) \equiv 0, \quad \text{ for all } \tau.","['real-analysis', 'calculus-of-variations', 'harmonic-functions', 'boundary-value-problem']"
67,to find Infimum of set $ \{\frac{1}{2^n} : n \in \mathbb{N}\}$,to find Infimum of set, \{\frac{1}{2^n} : n \in \mathbb{N}\},"to find infimum  of set $ \{\frac{1}{2^n} : n \in \mathbb{N}\}$ Clearly $0$ is lower bound of set. Let $l$ be another lower bound such that $ l > 0 $ .Now by Archimedian property we have $\frac{1}{n} < l $ for some $n$ . Also we have for all $n \in \mathbb{N}$ , $2^n> n$ . so $\frac{1}{2^n} < \frac{1}{n}$ . In particular we have , $\frac{1}{2^n} < l$ . hence a contradiction I am not entirely sure about my proof and wondering what other ways can be used to prove this ?","to find infimum  of set Clearly is lower bound of set. Let be another lower bound such that .Now by Archimedian property we have for some . Also we have for all , . so . In particular we have , . hence a contradiction I am not entirely sure about my proof and wondering what other ways can be used to prove this ?", \{\frac{1}{2^n} : n \in \mathbb{N}\} 0 l  l > 0  \frac{1}{n} < l  n n \in \mathbb{N} 2^n> n \frac{1}{2^n} < \frac{1}{n} \frac{1}{2^n} < l,['real-analysis']
68,What is the right definition for the limits from the left and the right?,What is the right definition for the limits from the left and the right?,,"While I am reading some books about analysis, i found that the definitions used for the notion of the limit from the right and the limit from the left are not always the same in all the references. Some references give the definitions as follows: Note that the definition above takes in accounting that $x$ may take the value of $a$ which guarantee the equivalence given in the following theorem: Whereas some other references use the following definitions: but in this case the previous theorem will not hold, we can for example define a function $f$ to be $1$ at $x_{0}=0$ and to be $x$ at $x\neq0$ , the limit from the right and the limit from the left at $0$ for this function will $0$ but $f$ doesn't have a limit! and if we choose the first definition then I think that many examples have been solved in the wrong way, for instance let's take the following function: usually we say that the limit from the right of $0$ is $3$ but in fact if we apply the first definition we will find that the limit from the right of $0$ doesn't exist! Do you have any suggestions to deal with this situation?","While I am reading some books about analysis, i found that the definitions used for the notion of the limit from the right and the limit from the left are not always the same in all the references. Some references give the definitions as follows: Note that the definition above takes in accounting that may take the value of which guarantee the equivalence given in the following theorem: Whereas some other references use the following definitions: but in this case the previous theorem will not hold, we can for example define a function to be at and to be at , the limit from the right and the limit from the left at for this function will but doesn't have a limit! and if we choose the first definition then I think that many examples have been solved in the wrong way, for instance let's take the following function: usually we say that the limit from the right of is but in fact if we apply the first definition we will find that the limit from the right of doesn't exist! Do you have any suggestions to deal with this situation?",x a f 1 x_{0}=0 x x\neq0 0 0 f 0 3 0,"['real-analysis', 'limits', 'functions']"
69,Choosing a $\lambda$ s.t. $ \left(\frac{1}{1-\lambda}\right)^n \exp(-\lambda\varepsilon) \leq \left(\frac\varepsilon n\right)^n \exp(n-\varepsilon). $,Choosing a  s.t.,\lambda  \left(\frac{1}{1-\lambda}\right)^n \exp(-\lambda\varepsilon) \leq \left(\frac\varepsilon n\right)^n \exp(n-\varepsilon). ,"Context: The motivation for the following inequality comes from bounding tail probabilities of sums of i.i.d. random variables where one applies Markov's Inequality. For a fixed $\varepsilon>0$ and $n \in \mathbb{N}$ , I want to prove that I can choose a $\lambda \in (0,1)$ such that $$ \left(\frac{1}{1-\lambda}\right)^n \exp(-\lambda\varepsilon) \leq \left(\frac\varepsilon n\right)^n \exp(n-\varepsilon). $$ Some initial observations / things to keep in mind: Note $\lambda$ can (and certainly must) depend on both $n$ and $\varepsilon$ . We know nothing about the relation of $n$ and $\epsilon$ . The left hand side is a product of an increasing function and a decresing function in $\lambda \in(0,1)$ . Hence, optimization could cause some trouble. It is not necessary to compute the exact value of $\lambda$ . It would suffice to show there exists such a $\lambda$ as specified Solution Attempt 1 Here I try to find the value of $\lambda$ . I do not know how to systematically tackle this, so I have tried different values of $\lambda$ to see if anything would work. For instance choosing $$ \lambda = 1-\frac{\text{min}(\varepsilon,n)}{\varepsilon} \in (0,1) $$ yields $$ \left(\frac{1}{1-\lambda}\right)^n \exp(-\lambda\varepsilon) = \left(\frac{\varepsilon}{\text{min}(\varepsilon,n)}\right)^n \exp\left(\frac{\text{min}(\varepsilon,n)}{n}-\varepsilon\right) \leq \left(\frac{\varepsilon}{\text{min}(\varepsilon,n)}\right)^n \exp(n-\varepsilon). $$ The problem is know as mentioned above that it is not so easy to get the the upper bound from here as the other factor becomes smaller if the minimum is replaced by one of the elements. I have tried to extend this idea further by considering ""nested"" max/min on the form $\text{max}(\text{min}(...,...),\text{min}(...,...))$ but I have become convinced that this is a dead end and not something that can ever work. Solution Attempt 2 Here I will simply try to show the existence of $\lambda$ . The left hand side is continuous in $\lambda$ (and based on plots also seem convex, but I have not shown this (yet)). For $\lambda \to 0$ the right-hand side goes to 1 and for $\lambda \to 1$ the right hand side goes to $\infty$ . One idea would then be to use the intermediate value theorem to get the existence of $\lambda$ . Note we could further upper-bound the right-hand side if need be before using the IVT. But I have not managed to achieve anything here either as the left hand side easily can become less than 1. Can anyone help me show this inequality?","Context: The motivation for the following inequality comes from bounding tail probabilities of sums of i.i.d. random variables where one applies Markov's Inequality. For a fixed and , I want to prove that I can choose a such that Some initial observations / things to keep in mind: Note can (and certainly must) depend on both and . We know nothing about the relation of and . The left hand side is a product of an increasing function and a decresing function in . Hence, optimization could cause some trouble. It is not necessary to compute the exact value of . It would suffice to show there exists such a as specified Solution Attempt 1 Here I try to find the value of . I do not know how to systematically tackle this, so I have tried different values of to see if anything would work. For instance choosing yields The problem is know as mentioned above that it is not so easy to get the the upper bound from here as the other factor becomes smaller if the minimum is replaced by one of the elements. I have tried to extend this idea further by considering ""nested"" max/min on the form but I have become convinced that this is a dead end and not something that can ever work. Solution Attempt 2 Here I will simply try to show the existence of . The left hand side is continuous in (and based on plots also seem convex, but I have not shown this (yet)). For the right-hand side goes to 1 and for the right hand side goes to . One idea would then be to use the intermediate value theorem to get the existence of . Note we could further upper-bound the right-hand side if need be before using the IVT. But I have not managed to achieve anything here either as the left hand side easily can become less than 1. Can anyone help me show this inequality?","\varepsilon>0 n \in \mathbb{N} \lambda \in (0,1) 
\left(\frac{1}{1-\lambda}\right)^n \exp(-\lambda\varepsilon) \leq \left(\frac\varepsilon n\right)^n \exp(n-\varepsilon).
 \lambda n \varepsilon n \epsilon \lambda \in(0,1) \lambda \lambda \lambda \lambda 
\lambda = 1-\frac{\text{min}(\varepsilon,n)}{\varepsilon} \in (0,1)
 
\left(\frac{1}{1-\lambda}\right)^n \exp(-\lambda\varepsilon) = \left(\frac{\varepsilon}{\text{min}(\varepsilon,n)}\right)^n \exp\left(\frac{\text{min}(\varepsilon,n)}{n}-\varepsilon\right) \leq
\left(\frac{\varepsilon}{\text{min}(\varepsilon,n)}\right)^n \exp(n-\varepsilon).
 \text{max}(\text{min}(...,...),\text{min}(...,...)) \lambda \lambda \lambda \to 0 \lambda \to 1 \infty \lambda","['real-analysis', 'inequality', 'exponential-function']"
70,Sum of i.i.d. variables can be bounded a.s.,Sum of i.i.d. variables can be bounded a.s.,,"If $\{X_n\}_{n=1}^\infty$ are i.i.d. random variables where $E[X_n] = 0$ for all $n \geq 1$ . Then there is $K > 0$ such that $P(|X_1 + \cdots + X_n| \leq K \text{ i.o.}) = 1$ . I thought we could use contradiction. Say for all $K > 0$ we get that $$P(|X_1 + \cdots + X_n| \leq K \text{ i.o.}) < 1,$$ then $$0 < P(|X_1 + \cdots + X_n| > K \text{ for large n}) \leq \liminf_{n \to \infty} P(|X_1 + \cdots + X_n| > K) \\ \leq \frac{1}{K} \liminf_{n \to \infty} E(|X_1 + \cdots + X_n|).$$ If I could bound the limit we are done since $K \to 0$ finishes the contradiction. But I do not think we can calculate this easily. Edit: Do I need another condition on my $X_n$ ?",If are i.i.d. random variables where for all . Then there is such that . I thought we could use contradiction. Say for all we get that then If I could bound the limit we are done since finishes the contradiction. But I do not think we can calculate this easily. Edit: Do I need another condition on my ?,"\{X_n\}_{n=1}^\infty E[X_n] = 0 n \geq 1 K > 0 P(|X_1 + \cdots + X_n| \leq K \text{ i.o.}) = 1 K > 0 P(|X_1 + \cdots + X_n| \leq K \text{ i.o.}) < 1, 0 < P(|X_1 + \cdots + X_n| > K \text{ for large n}) \leq \liminf_{n \to \infty} P(|X_1 + \cdots + X_n| > K) \\
\leq \frac{1}{K} \liminf_{n \to \infty} E(|X_1 + \cdots + X_n|). K \to 0 X_n","['real-analysis', 'probability', 'probability-theory', 'random-variables']"
71,Prove that $\lim\limits_{n\to\infty} a_n$ exists.,Prove that  exists.,\lim\limits_{n\to\infty} a_n,"Let $a_1,\cdots, a_{2022}$ be real numbers in $(0,1)$ . Define for $n\ge 2023$ , $a_n = (a_{n-1}+\cdots + a_{n-2022})^{1/2023}$ . Prove that $\lim\limits_{n\to\infty} a_n$ exists. Clearly it suffices to show that $\lim\limits_{n\to\infty} a_n^{2023}$ exists since $x\mapsto x^{1/2023}$ is a continuous function (on $\mathbb{R}$ ). I initially thought of using the Cesaro stolz theorem, but it seems hard to prove that the limit $\lim\limits_{n\to\infty} \dfrac{a_{n+1}^{2023}-a_n^{2023}}{1} = \lim\limits_{n\to\infty} a_n - a_{n-2022}$ exists. But  it should be possible to prove it indeed exists and is equal to zero. The Squeeze theorem or some sort of telescoping sum might help. I'm not sure if inequalities like the AM-GM inequality could be useful for this problem.","Let be real numbers in . Define for , . Prove that exists. Clearly it suffices to show that exists since is a continuous function (on ). I initially thought of using the Cesaro stolz theorem, but it seems hard to prove that the limit exists. But  it should be possible to prove it indeed exists and is equal to zero. The Squeeze theorem or some sort of telescoping sum might help. I'm not sure if inequalities like the AM-GM inequality could be useful for this problem.","a_1,\cdots, a_{2022} (0,1) n\ge 2023 a_n = (a_{n-1}+\cdots + a_{n-2022})^{1/2023} \lim\limits_{n\to\infty} a_n \lim\limits_{n\to\infty} a_n^{2023} x\mapsto x^{1/2023} \mathbb{R} \lim\limits_{n\to\infty} \dfrac{a_{n+1}^{2023}-a_n^{2023}}{1} = \lim\limits_{n\to\infty} a_n - a_{n-2022}","['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'contest-math']"
72,Is there a general method to find the asymptotic order for this sequence?,Is there a general method to find the asymptotic order for this sequence?,,"Given $$a_{n+1}=a_n+\frac{n}{a_1+\dots+a_n},\qquad a_1>0$$ The answer is $$\lim_{n\to\infty} a_n\sim\sqrt{3}\cdot\sqrt{n}-\frac{\sqrt{3}}{4}\cdot\frac{1}{\sqrt{n}}$$ It is easy to show this sequence is increasing, and is divergent, because if assume the opposite $A=\lim_{n\to\infty} a_n$ , we will get $A=A+\frac{1}A$ , which gives contradictions. We also have $$a_{n+1}\ge a_n +\frac{1}{a_n}\ge2$$ From here I can show: $$a_1+\frac{n}{a_n}\le a_{n+1}\le a_1+\frac{n}{a_1}$$ Update.(1) This equation can be also written as: $$\begin{align} a_n&=\sum_{k=1}^n a_k-\sum_{k=1}^{n-1} a_k\\ \\ a_n&=\frac{n}{a_{n+1}-a_n}-\frac{n-1}{a_{n}-a_{n-1}}\\ \\ a_n(a_{n+1}-a_n)(a_{n}-a_{n-1})&=n(a_{n}-a_{n-1})-(n-1)(a_{n+1}-a_n)\tag{1} \end{align}$$ If assume $a_n=c\cdot n^p$ $$a_{n+1}-a_n\sim cp\cdot n^{p-1},~~~a_{n}-a_{n-1}\sim cp\cdot n^{p-1}$$ Plug into $(1)$ and only keep the leading order: $$c^2p\cdot n^{2p-1}=1$$ So we get $p=\frac{1}2$ and $c=\sqrt{2}$ Why the leading order coefficient is $\sqrt{2}$ , not $\sqrt{3}$ ? The bottom line is, if I take the answer as template, let $$a_n=c\sqrt{n}+t\frac{1}{\sqrt{n}}+o(\frac{1}{\sqrt{n}})$$ Pretend we don't know coefficients $c$ and $t$ . Now we want to compute $c$ and $t$ . To the leading order approximation, We have $$a_{n+1}-a_n= c\cdot \frac{1}{2\sqrt{n}}+O(\frac{1}{n^{3/2}})$$ Next, plug into Eq. $(1)$ and we can solve for $c=\sqrt{2}$ . Why does this give a contradiction? Update.(2) Thank you to @Sangchul Lee , @Somos and @Youem I put the computation part in the answer box below, and it works for asymptotic approximation at arbitrary order.","Given The answer is It is easy to show this sequence is increasing, and is divergent, because if assume the opposite , we will get , which gives contradictions. We also have From here I can show: Update.(1) This equation can be also written as: If assume Plug into and only keep the leading order: So we get and Why the leading order coefficient is , not ? The bottom line is, if I take the answer as template, let Pretend we don't know coefficients and . Now we want to compute and . To the leading order approximation, We have Next, plug into Eq. and we can solve for . Why does this give a contradiction? Update.(2) Thank you to @Sangchul Lee , @Somos and @Youem I put the computation part in the answer box below, and it works for asymptotic approximation at arbitrary order.","a_{n+1}=a_n+\frac{n}{a_1+\dots+a_n},\qquad a_1>0 \lim_{n\to\infty} a_n\sim\sqrt{3}\cdot\sqrt{n}-\frac{\sqrt{3}}{4}\cdot\frac{1}{\sqrt{n}} A=\lim_{n\to\infty} a_n A=A+\frac{1}A a_{n+1}\ge a_n +\frac{1}{a_n}\ge2 a_1+\frac{n}{a_n}\le a_{n+1}\le a_1+\frac{n}{a_1} \begin{align}
a_n&=\sum_{k=1}^n a_k-\sum_{k=1}^{n-1} a_k\\
\\
a_n&=\frac{n}{a_{n+1}-a_n}-\frac{n-1}{a_{n}-a_{n-1}}\\
\\
a_n(a_{n+1}-a_n)(a_{n}-a_{n-1})&=n(a_{n}-a_{n-1})-(n-1)(a_{n+1}-a_n)\tag{1}
\end{align} a_n=c\cdot n^p a_{n+1}-a_n\sim cp\cdot n^{p-1},~~~a_{n}-a_{n-1}\sim cp\cdot n^{p-1} (1) c^2p\cdot n^{2p-1}=1 p=\frac{1}2 c=\sqrt{2} \sqrt{2} \sqrt{3} a_n=c\sqrt{n}+t\frac{1}{\sqrt{n}}+o(\frac{1}{\sqrt{n}}) c t c t a_{n+1}-a_n= c\cdot \frac{1}{2\sqrt{n}}+O(\frac{1}{n^{3/2}}) (1) c=\sqrt{2}","['real-analysis', 'sequences-and-series', 'analysis', 'recurrence-relations', 'recursion']"
73,Characterization of Riesz sequences,Characterization of Riesz sequences,,"Let $X \subseteq \mathbb{R}$ be a compact set. Show that $\mathcal{E} = \{e^{int}\}_{n \in \mathbb{Z}}$ is a Riesz sequence in $L^2(X)$ if and only if $X+2\pi \mathbb{Z} = \mathbb{R}$ . We say that $\mathcal{E}$ is a Riesz sequence in $L^2(X)$ if we have the following chain of inequalities: $$A \sum_n |c_n|^2 \leq \left\|\sum_n c_n e^{int}\right\|_{L^2(X)}^2 \leq B \sum|c_n|^2,$$ where $c = (c_n)$ is a finite sequence. Suppose that $X+2\pi\mathbb{Z} = \mathbb{R}$ . Then since $X$ is bounded there is $N \in \mathbb{N}$ such that $X \subseteq [-\pi N, \pi N]$ . With this we get the the latter inequality by using the fact that $\mathcal{E}$ forms an orthogonal basis in $L^2[-\pi,\pi]$ and the periodicity of its elements. However I am unsure how to achieve the fitst inequality. One idea I had (which may be wrong) is to cover the interval $[-\pi,\pi]$ by a finite number of translates of the form $\{X+2\pi k\}_{k=1}^N$ . However, I cannot convince myself that I can do this only using a finite number of translates. If this is true, then this direction is done. Otherwise, I am unsure. As for the other direction, I have no clue on how to start. Of course one only needs to show that every real number $r \in \mathbb{R}$ can be written in the form $r = x+2\pi m$ for some $m \in \mathbb{Z}$ . Any help is appreciated, thanks!","Let be a compact set. Show that is a Riesz sequence in if and only if . We say that is a Riesz sequence in if we have the following chain of inequalities: where is a finite sequence. Suppose that . Then since is bounded there is such that . With this we get the the latter inequality by using the fact that forms an orthogonal basis in and the periodicity of its elements. However I am unsure how to achieve the fitst inequality. One idea I had (which may be wrong) is to cover the interval by a finite number of translates of the form . However, I cannot convince myself that I can do this only using a finite number of translates. If this is true, then this direction is done. Otherwise, I am unsure. As for the other direction, I have no clue on how to start. Of course one only needs to show that every real number can be written in the form for some . Any help is appreciated, thanks!","X \subseteq \mathbb{R} \mathcal{E} = \{e^{int}\}_{n \in \mathbb{Z}} L^2(X) X+2\pi \mathbb{Z} = \mathbb{R} \mathcal{E} L^2(X) A \sum_n |c_n|^2 \leq \left\|\sum_n c_n e^{int}\right\|_{L^2(X)}^2 \leq B \sum|c_n|^2, c = (c_n) X+2\pi\mathbb{Z} = \mathbb{R} X N \in \mathbb{N} X \subseteq [-\pi N, \pi N] \mathcal{E} L^2[-\pi,\pi] [-\pi,\pi] \{X+2\pi k\}_{k=1}^N r \in \mathbb{R} r = x+2\pi m m \in \mathbb{Z}","['real-analysis', 'hilbert-spaces', 'harmonic-analysis']"
74,"Calculate $\lim_{n\to\infty}\int_0^1 f(\{nx^k\})g(x^n)\,dx$.",Calculate .,"\lim_{n\to\infty}\int_0^1 f(\{nx^k\})g(x^n)\,dx","Let $k\geq 1$ be an integer, $f:[0,1]\to\mathbb R$ be a Riemannian integrable function and $g:[0,1]\to \mathbb R$ be a continuous function. Calculate $$\lim_{n\to\infty}\int_0^1 f(\{nx^k\})g(x^n)\,dx,$$ where $\{x\}$ denotes the fractional part of the real number $x$ . This is W33 of József Wildt International Mathematical Competition, 2021 . Yesterday, I planned to ask this question here because I could only handle the case where $k=1$ at that time. However, when I was ready to click the button ""Review your question"", an idea suddenly came to my mind: Why not consider $g\equiv 1$ first? After 1 hour or 2, that idea became a full answer to this problem. Now I write it here. It is a little bit long, because I proved two lemmas that is useful for us. My question. Is my proof presented below correct or not? Does there exist a shorter and more elegant proof? Lemma 1. If $f:[0,1]\to\mathbb R$ is measurable and bounded, then $$\lim_{n\to\infty}\int_0^1 f(\{nx^k\})\,dx=\int_0^1f(x)\,dx.$$ Proof of Lemma 1. Firstly, $$\int_0^1 f(\{nx^k\})\,dx=\sum_{m=0}^{n-1}\int_{\left(\frac mn\right)^{1/k}}^{\left(\frac {m+1}n\right)^{1/k}}f(nx^k-m)\,dx=\int_0^1\sum_{m=0}^{n-1}\frac1{nk}f(t)\left(\frac{m+t}n\right)^{\frac1k-1}\,dt.$$ Let $$f_n(t)=\sum_{m=0}^{n-1}\frac1{nk}f(t)\left(\frac{m+t}n\right)^{\frac1k-1},\qquad n\geq 1, t\in(0,1].$$ For fixed $t\in(0,1]$ , we have $$f_n(t)\to f(t)\int_0^1 \frac1k u^{\frac1k-1}\,du=f(t),\qquad \text{as }\ \ n\to\infty.$$ On the other hand, \begin{align*} 	|f_n(t)|&\leq |f(t)|\frac1{nk}\left(\frac tn\right)^{\frac1k-1}+|f(t)|\sum_{m=1}^{n-1}\frac1{nk}\left(\frac{m}n\right)^{\frac1k-1}\\ 	&\leq |f(t)|\frac1{k}t^{\frac1k-1}+|f(t)|\frac1k\int_{1/n}^1\left(u-\frac1n\right)^{\frac1k-1}\,du\\ 	&\leq \frac{\|f\|_{L^\infty}}{k}t^{\frac1k-1}+|f(t)|. 	\end{align*} Therefore, DCT implies that $\int_0^1 f_n(t)\,dt\to\int_0^1 f(t)\,dt$ , concluding the proof of Lemma 1. Lemma 2. If $f\in L^\infty(0,1)$ , and $0<\eta\ll1$ , then there exists a positive cosntant relying only on $k$ , $C(k)>0$ such that $$\int_{1-\eta}^1|f(\{nx^k\})|\,dx\leq \left(\eta+\frac{C(k)}{n}\right)\|f\|_{L^1}.$$ Proof of Lemma 2. Assume that $\left(\frac mn\right)^{1/k}\leq 1-\eta<\left(\frac {m+1}n\right)^{1/k}$ for some $m\geq \frac n2+1$ , then \begin{align*} 		\int_{1-\eta}^1|f(\{nx^k\})|\,dx&\leq \int_{\left(\frac mn\right)^{1/k}}^1|f(\{nx^k\})|\,dx\\ 		&=\int_0^1|f(t)|\sum_{p=m}^{n-1}\frac1{nk}\left(\frac{p+t}n\right)^{\frac1k-1}\,dt\\ 		&\leq \|f\|_{L^1}\sum_{p=m}^{n-1}\frac1{nk}\left(\frac{p}n\right)^{\frac1k-1}\\ 		&\leq \|f\|_{L^1}\frac1k\int_{\frac mn}^1\left(u-\frac1n\right)^{\frac1k-1}\,du\\ 		&\leq \|f\|_{L^1}\left[\left(1-\frac1n\right)^{\frac1k}-\left(\frac{m-1}n\right)^{\frac1k}\right]. 	\end{align*} Let $\phi(s)=s^{1/k}$ for $s\in(0,1]$ , then \begin{align*} 		\phi\left(\frac mn+\frac1n\right)-\phi\left(\frac mn-\frac1n\right)&\leq \frac2n\max_{1/2\leq s\leq 1}|\phi'(s)|=\frac{2^{2-\frac1k}}{nk}\\ 		&=\frac1{nk}+\frac{C(k)}{n}=\frac1n\min_{1/2\leq s\leq 1}|\phi'(s)|+\frac{C(k)}{n}\\ 		&\leq \phi(1)-\phi\left(1-\frac1n\right)+\frac{C(k)}{n}. 	\end{align*} Hence \begin{align*} 	\int_{1-\eta}^1|f(\{nx^k\})|\,dx&\leq \left(\phi\left(1-\frac1n\right)-\phi\left(\frac mn-\frac1n\right)\right)\|f\|_{L^1}\\ 	&\leq \left(\phi(1)-\phi\left(\frac mn+\frac1n\right)+\frac{C(k)}{n}\right)\|f\|_{L^1}\\ 	&\leq \left(\eta+\frac{C(k)}{n}\right)\|f\|_{L^1}. 	\end{align*} This completes the proof of Lemma 2. Proof of the main problem. We prove $$\lim_{n\to\infty}\int_0^1 f(\{nx^k\})g(x^n)\,dx=g(0)\int_0^1f(x)\,dx.\tag{1}$$ For each $\epsilon>0$ and $0<\eta\ll1$ , since $g$ is continuous at $x=0$ , we can find $\delta>0$ such that $|g(x)-g(0)|\leq \epsilon$ for all $x\in[0,\delta]$ . For $n$ large enough, we have $(1-\eta)^n<\epsilon$ and thus $$|g(x^n)-g(0)|\leq \epsilon,\qquad \forall x\in[0,1-\eta].$$ Therefore, for all large $n$ , we have \begin{align*} 		&\ \ \ \left|\int_0^1 f(\{nx^k\})g(x^n)\,dx-g(0)\int_0^1f(x)\,dx\right|\\ 		&\leq\left|g(0)\int_0^1f(\{nx^k\})\,dx-g(0)\int_0^1f(x)\,dx\right|+\int_0^1|f(\{nx^k\})||g(x^n)-g(0)|\,dx\\ 		&\leq|g(0)|\left|\int_0^1f(\{nx^k\})\,dx-\int_0^1f(x)\,dx\right|+\epsilon \int_0^{1-\eta}|f(\{nx^k\})|\,dx+2\|g\|_{L^\infty}\int_{1-\eta}^1|f(\{nx^k\})|\,dx\\ 		&\leq|g(0)|\left|\int_0^1f(\{nx^k\})\,dx-\int_0^1f(x)\,dx\right|+\epsilon \int_0^{1}|f(\{nx^k\})|\,dx+2\|g\|_{L^\infty}\int_{1-\eta}^1|f(\{nx^k\})|\,dx. 	\end{align*} By Lemma 1 and Lemma 2, taking $\limsup_{n\to\infty}$ on each side gives that $$\limsup_{n\to\infty}\left|\int_0^1 f(\{nx^k\})g(x^n)\,dx-g(0)\int_0^1f(x)\,dx\right|\leq \epsilon\|f\|_{L^1}+2\eta\|g\|_{L^\infty}\|f\|_{L^1}.$$ Letting $\epsilon,\eta\to0$ , we get $(1)$ . I would like to express my gratitude to anyone who are reading this long post. Comments or remarks on a easier proof, or improved results ( e.g. convergence rates, the same conclusion under weaker conditions), or anything else related to this problem, are very welcome.","Let be an integer, be a Riemannian integrable function and be a continuous function. Calculate where denotes the fractional part of the real number . This is W33 of József Wildt International Mathematical Competition, 2021 . Yesterday, I planned to ask this question here because I could only handle the case where at that time. However, when I was ready to click the button ""Review your question"", an idea suddenly came to my mind: Why not consider first? After 1 hour or 2, that idea became a full answer to this problem. Now I write it here. It is a little bit long, because I proved two lemmas that is useful for us. My question. Is my proof presented below correct or not? Does there exist a shorter and more elegant proof? Lemma 1. If is measurable and bounded, then Proof of Lemma 1. Firstly, Let For fixed , we have On the other hand, Therefore, DCT implies that , concluding the proof of Lemma 1. Lemma 2. If , and , then there exists a positive cosntant relying only on , such that Proof of Lemma 2. Assume that for some , then Let for , then Hence This completes the proof of Lemma 2. Proof of the main problem. We prove For each and , since is continuous at , we can find such that for all . For large enough, we have and thus Therefore, for all large , we have By Lemma 1 and Lemma 2, taking on each side gives that Letting , we get . I would like to express my gratitude to anyone who are reading this long post. Comments or remarks on a easier proof, or improved results ( e.g. convergence rates, the same conclusion under weaker conditions), or anything else related to this problem, are very welcome.","k\geq 1 f:[0,1]\to\mathbb R g:[0,1]\to \mathbb R \lim_{n\to\infty}\int_0^1 f(\{nx^k\})g(x^n)\,dx, \{x\} x k=1 g\equiv 1 f:[0,1]\to\mathbb R \lim_{n\to\infty}\int_0^1 f(\{nx^k\})\,dx=\int_0^1f(x)\,dx. \int_0^1 f(\{nx^k\})\,dx=\sum_{m=0}^{n-1}\int_{\left(\frac mn\right)^{1/k}}^{\left(\frac {m+1}n\right)^{1/k}}f(nx^k-m)\,dx=\int_0^1\sum_{m=0}^{n-1}\frac1{nk}f(t)\left(\frac{m+t}n\right)^{\frac1k-1}\,dt. f_n(t)=\sum_{m=0}^{n-1}\frac1{nk}f(t)\left(\frac{m+t}n\right)^{\frac1k-1},\qquad n\geq 1, t\in(0,1]. t\in(0,1] f_n(t)\to f(t)\int_0^1 \frac1k u^{\frac1k-1}\,du=f(t),\qquad \text{as }\ \ n\to\infty. \begin{align*}
	|f_n(t)|&\leq |f(t)|\frac1{nk}\left(\frac tn\right)^{\frac1k-1}+|f(t)|\sum_{m=1}^{n-1}\frac1{nk}\left(\frac{m}n\right)^{\frac1k-1}\\
	&\leq |f(t)|\frac1{k}t^{\frac1k-1}+|f(t)|\frac1k\int_{1/n}^1\left(u-\frac1n\right)^{\frac1k-1}\,du\\
	&\leq \frac{\|f\|_{L^\infty}}{k}t^{\frac1k-1}+|f(t)|.
	\end{align*} \int_0^1 f_n(t)\,dt\to\int_0^1 f(t)\,dt f\in L^\infty(0,1) 0<\eta\ll1 k C(k)>0 \int_{1-\eta}^1|f(\{nx^k\})|\,dx\leq \left(\eta+\frac{C(k)}{n}\right)\|f\|_{L^1}. \left(\frac mn\right)^{1/k}\leq 1-\eta<\left(\frac {m+1}n\right)^{1/k} m\geq \frac n2+1 \begin{align*}
		\int_{1-\eta}^1|f(\{nx^k\})|\,dx&\leq \int_{\left(\frac mn\right)^{1/k}}^1|f(\{nx^k\})|\,dx\\
		&=\int_0^1|f(t)|\sum_{p=m}^{n-1}\frac1{nk}\left(\frac{p+t}n\right)^{\frac1k-1}\,dt\\
		&\leq \|f\|_{L^1}\sum_{p=m}^{n-1}\frac1{nk}\left(\frac{p}n\right)^{\frac1k-1}\\
		&\leq \|f\|_{L^1}\frac1k\int_{\frac mn}^1\left(u-\frac1n\right)^{\frac1k-1}\,du\\
		&\leq \|f\|_{L^1}\left[\left(1-\frac1n\right)^{\frac1k}-\left(\frac{m-1}n\right)^{\frac1k}\right].
	\end{align*} \phi(s)=s^{1/k} s\in(0,1] \begin{align*}
		\phi\left(\frac mn+\frac1n\right)-\phi\left(\frac mn-\frac1n\right)&\leq \frac2n\max_{1/2\leq s\leq 1}|\phi'(s)|=\frac{2^{2-\frac1k}}{nk}\\
		&=\frac1{nk}+\frac{C(k)}{n}=\frac1n\min_{1/2\leq s\leq 1}|\phi'(s)|+\frac{C(k)}{n}\\
		&\leq \phi(1)-\phi\left(1-\frac1n\right)+\frac{C(k)}{n}.
	\end{align*} \begin{align*}
	\int_{1-\eta}^1|f(\{nx^k\})|\,dx&\leq \left(\phi\left(1-\frac1n\right)-\phi\left(\frac mn-\frac1n\right)\right)\|f\|_{L^1}\\
	&\leq \left(\phi(1)-\phi\left(\frac mn+\frac1n\right)+\frac{C(k)}{n}\right)\|f\|_{L^1}\\
	&\leq \left(\eta+\frac{C(k)}{n}\right)\|f\|_{L^1}.
	\end{align*} \lim_{n\to\infty}\int_0^1 f(\{nx^k\})g(x^n)\,dx=g(0)\int_0^1f(x)\,dx.\tag{1} \epsilon>0 0<\eta\ll1 g x=0 \delta>0 |g(x)-g(0)|\leq \epsilon x\in[0,\delta] n (1-\eta)^n<\epsilon |g(x^n)-g(0)|\leq \epsilon,\qquad \forall x\in[0,1-\eta]. n \begin{align*}
		&\ \ \ \left|\int_0^1 f(\{nx^k\})g(x^n)\,dx-g(0)\int_0^1f(x)\,dx\right|\\
		&\leq\left|g(0)\int_0^1f(\{nx^k\})\,dx-g(0)\int_0^1f(x)\,dx\right|+\int_0^1|f(\{nx^k\})||g(x^n)-g(0)|\,dx\\
		&\leq|g(0)|\left|\int_0^1f(\{nx^k\})\,dx-\int_0^1f(x)\,dx\right|+\epsilon \int_0^{1-\eta}|f(\{nx^k\})|\,dx+2\|g\|_{L^\infty}\int_{1-\eta}^1|f(\{nx^k\})|\,dx\\
		&\leq|g(0)|\left|\int_0^1f(\{nx^k\})\,dx-\int_0^1f(x)\,dx\right|+\epsilon \int_0^{1}|f(\{nx^k\})|\,dx+2\|g\|_{L^\infty}\int_{1-\eta}^1|f(\{nx^k\})|\,dx.
	\end{align*} \limsup_{n\to\infty} \limsup_{n\to\infty}\left|\int_0^1 f(\{nx^k\})g(x^n)\,dx-g(0)\int_0^1f(x)\,dx\right|\leq \epsilon\|f\|_{L^1}+2\eta\|g\|_{L^\infty}\|f\|_{L^1}. \epsilon,\eta\to0 (1)","['real-analysis', 'limits', 'solution-verification', 'contest-math', 'alternative-proof']"
75,"Prove that $A =\{x \in X\mid f(x) \ge 2022\}$ is closed in $X$, by means of sequences.","Prove that  is closed in , by means of sequences.",A =\{x \in X\mid f(x) \ge 2022\} X,"Let $f:X\to \mathbb{R}$ be a continuous function, where $(X,d)$ is a metric space, and $\mathbb{R}$ has the usual metric, $|x-y|$ . Prove that the set $A=\{x\in X:f(x)\geq 2022\}$ is a closed set in $X$ , by means of sequences. I was thinking of proving that the complement of $A$ is open. So, I did the following: Let $0<r<x_0-2022$ . If $x\in B(x,r)\implies x\in A^c$ $$d(x,x_0)=|x-x_0|<r \implies x_0-r<x<r+x_0$$ But $$\begin{align*} 0<r<x_0-2022 &\implies -r>2022-x_0 \\ &\implies -r-x_0>2022\\ &\implies x>2022\\ &\therefore A^c \text{ is open} \\ &\therefore A \text{ is closed} \end{align*}$$ But I think I am not using the sequences.","Let be a continuous function, where is a metric space, and has the usual metric, . Prove that the set is a closed set in , by means of sequences. I was thinking of proving that the complement of is open. So, I did the following: Let . If But But I think I am not using the sequences.","f:X\to \mathbb{R} (X,d) \mathbb{R} |x-y| A=\{x\in X:f(x)\geq 2022\} X A 0<r<x_0-2022 x\in B(x,r)\implies x\in A^c d(x,x_0)=|x-x_0|<r \implies x_0-r<x<r+x_0 \begin{align*}
0<r<x_0-2022
&\implies -r>2022-x_0 \\
&\implies -r-x_0>2022\\
&\implies x>2022\\
&\therefore A^c \text{ is open} \\
&\therefore A \text{ is closed}
\end{align*}","['real-analysis', 'sequences-and-series', 'continuity', 'metric-spaces']"
76,monotone functions agreeing with Holder functions on a large set,monotone functions agreeing with Holder functions on a large set,,"Let $\alpha \in (0,1)$ , $f:[0,1]\rightarrow \mathbb{R}$ be a continuous monotone function and $\varepsilon>0$ . Does there exist a function $\phi_{\varepsilon} \in \mathcal{C}^{\alpha}$ such that $\lambda(\{t \in [0,1]: f(t)=\phi_{\varepsilon}(t)\})\geqslant 1-\varepsilon$ ? Ideas: A monotone continuous function is a.e. differentiable and therefore by regularity of the Lebesgue measure we can find an arbitrarily small open set that includes all the points where the function is not differentiable. Then one can write this open set as a countable disjoint union of open sets, on which we could ""smoothly interpolate"". Then we have a function that is smooth on this open set, but I don't know how to ensure regularity on the whole interval... The prototype of a counterexample, the Cantor-function, does not work as we can take an approximation with Lipschitz-functions agreeing up to small measure with the Cantor-function. Maybe the ""fat-Cantor-function"" could be an idea of a counterexample.","Let , be a continuous monotone function and . Does there exist a function such that ? Ideas: A monotone continuous function is a.e. differentiable and therefore by regularity of the Lebesgue measure we can find an arbitrarily small open set that includes all the points where the function is not differentiable. Then one can write this open set as a countable disjoint union of open sets, on which we could ""smoothly interpolate"". Then we have a function that is smooth on this open set, but I don't know how to ensure regularity on the whole interval... The prototype of a counterexample, the Cantor-function, does not work as we can take an approximation with Lipschitz-functions agreeing up to small measure with the Cantor-function. Maybe the ""fat-Cantor-function"" could be an idea of a counterexample.","\alpha \in (0,1) f:[0,1]\rightarrow \mathbb{R} \varepsilon>0 \phi_{\varepsilon} \in \mathcal{C}^{\alpha} \lambda(\{t \in [0,1]: f(t)=\phi_{\varepsilon}(t)\})\geqslant 1-\varepsilon","['real-analysis', 'measure-theory', 'monotone-functions', 'bounded-variation', 'holder-spaces']"
77,How to prove a lemma about $\lim _{\lambda \rightarrow \infty} e^{-\lambda t} \sum_{k \leq \lambda x} \frac{(\lambda t)^{k}}{k !}$? [duplicate],How to prove a lemma about ? [duplicate],\lim _{\lambda \rightarrow \infty} e^{-\lambda t} \sum_{k \leq \lambda x} \frac{(\lambda t)^{k}}{k !},"This question already has answers here : The proof of an elementary equality (2 answers) Closed 2 years ago . I recently came across the following  lemma while learning harmonic analysis, but don't know how to prove it by using analytical methods. Lemma: For all $t \geq 0, x>0$ , we have that \begin{equation} \lim _{\lambda \rightarrow \infty} e^{-\lambda t} \sum_{k \leq \lambda x} \frac{(\lambda t)^{k}}{k !}=\mathbf{1}_{[0, x)}(t)+\frac{1}{2} \mathbf{1}_{\{x\}}(t), \end{equation} where $\mathbf{1}_{[0, x)}(t)$ is the indicator function on set $[0, x)$ . If anyone can provide the proof, I would like to thank you here in advance.","This question already has answers here : The proof of an elementary equality (2 answers) Closed 2 years ago . I recently came across the following  lemma while learning harmonic analysis, but don't know how to prove it by using analytical methods. Lemma: For all , we have that where is the indicator function on set . If anyone can provide the proof, I would like to thank you here in advance.","t \geq 0, x>0 \begin{equation}
\lim _{\lambda \rightarrow \infty} e^{-\lambda t} \sum_{k \leq \lambda x} \frac{(\lambda t)^{k}}{k !}=\mathbf{1}_{[0, x)}(t)+\frac{1}{2} \mathbf{1}_{\{x\}}(t),
\end{equation} \mathbf{1}_{[0, x)}(t) [0, x)","['real-analysis', 'analysis', 'harmonic-analysis']"
78,Proof that this isn't a complete metric space.,Proof that this isn't a complete metric space.,,"Given $d:\mathbb{R}\times\mathbb{R},d(x,y):=|e^{-x}-e^{-y}|$ , $(\mathbb{R},d)$ is a, metric space (we don't have to prove this). Show that $(\mathbb{R},d)$ is not complete. So the idea for my proof was: Let $x_n:=n, n\in\mathbb{N}$ be a sequence of natural numbers. This sequence is a Cauchy-Sequence because let $\epsilon=N,N<n$ $|e^{-n}-e^{-m}|=e^{-n}-e^{-m}<e^{-n}<e^{-N}=\epsilon$ so $\forall\epsilon>0\exists N_{\epsilon}\in\mathbb{N}\forall n,m>N_{\epsilon}:d(x_n,x_m)<\epsilon$ . The second part is to show that $x_n$ doesn't converge in $(\mathbb{R},d)$ : Take a look at $e^{-n}\to 0$ as $n\to\infty$ which means for $\epsilon>0$ , $|e^{-n}|<\epsilon\Leftrightarrow |e^{-n}-0|<\epsilon$ but $\forall x\in\mathbb{R}:e^{-x}\neq 0$ which means that $x_n$ doesn't converge in this metric space. In conclusion $x_n$ is a Cauchy-Sequence that doesn't converge in $(\mathbb{R},d)$ which means that $(\mathbb{R},d)$ isn't a complete metric space. I know that my proof is very poorly written but I'm not a native english speaker and therefore don't usually write proofs in english. Is this example right though and is this proof valid?","Given , is a, metric space (we don't have to prove this). Show that is not complete. So the idea for my proof was: Let be a sequence of natural numbers. This sequence is a Cauchy-Sequence because let so . The second part is to show that doesn't converge in : Take a look at as which means for , but which means that doesn't converge in this metric space. In conclusion is a Cauchy-Sequence that doesn't converge in which means that isn't a complete metric space. I know that my proof is very poorly written but I'm not a native english speaker and therefore don't usually write proofs in english. Is this example right though and is this proof valid?","d:\mathbb{R}\times\mathbb{R},d(x,y):=|e^{-x}-e^{-y}| (\mathbb{R},d) (\mathbb{R},d) x_n:=n, n\in\mathbb{N} \epsilon=N,N<n |e^{-n}-e^{-m}|=e^{-n}-e^{-m}<e^{-n}<e^{-N}=\epsilon \forall\epsilon>0\exists N_{\epsilon}\in\mathbb{N}\forall n,m>N_{\epsilon}:d(x_n,x_m)<\epsilon x_n (\mathbb{R},d) e^{-n}\to 0 n\to\infty \epsilon>0 |e^{-n}|<\epsilon\Leftrightarrow |e^{-n}-0|<\epsilon \forall x\in\mathbb{R}:e^{-x}\neq 0 x_n x_n (\mathbb{R},d) (\mathbb{R},d)","['real-analysis', 'sequences-and-series', 'metric-spaces']"
79,The outer measure of a closed interval is its length without Heine-Borel Theorem.,The outer measure of a closed interval is its length without Heine-Borel Theorem.,,"I tried to prove that the outer measure of a closed interval is its length without Heine-Borel Theorem. Proving the first part $m^\star ([a,b])\leq a-b$ is easy and does not require Heine-Borel Theorem. Proving the other part  is summarized as follows: (1)-Clearly we have $[a,b]\subset(a-\epsilon ,b+\epsilon ) $ for any scalar $\epsilon>0$ . (2)- $a$ and $b$ are interior points of  any open coverng since $[a,b]$ $\subset$ $\cup_{k=1}^\infty I_k$ . (3)- The covering $\cup_{k=1}^\infty I_k$ is open. Hence there exists $\epsilon_1>0$ and $\epsilon_2>0$ such that  the intervals $(a-\epsilon_1,a+\epsilon_1)$ and $(b-\epsilon_2,b+\epsilon_2)\subset  \cup_{k=1}^\infty I_k$ . (4)- Steps (1), (2), and (3) imply that    there exists $\epsilon=min\{\epsilon_1,\epsilon_2\}$ such that $(a-\epsilon ,b+\epsilon )\subset  \cup_{k=1}^\infty I_k$ for every open covering of $[a,b]$ , i.e. for any open covering $\cup_{k=1}^\infty I_k$ we have an open cover (not subcover) that contains $[a,b]$ . In other words, for any open cover, there exists $\epsilon>0$ such that $(a-\epsilon ,b+\epsilon )$ is an open cover of $[a,b]$ with the following property: $$[a,b]\subset(a-\epsilon ,b+\epsilon ) \subset  \cup_{k=1}^\infty I_k$$ I know that the interval $(a-\epsilon ,b+\epsilon )$ is not a subcover but it is an open cover that is tighter than the open cover itself. We can take the infimum over $\epsilon$ which is the same as taking the infimum overall covering. $\textbf{EDIT: We have}$ $b-a\leq l((a-\epsilon,b-\epsilon)\leq\sum_kl(I_k))$ , $\textbf{which implies:}$ $$b-a= \inf_{\epsilon>0}\{ b -a+2\epsilon \}\leq \inf\bigg\{\sum_{k=0}^\infty l(I_k): [a,b]\subset\cup_{k=0}^\infty I_k\bigg\}= m^\star ([a,b])$$ which completes the proof. Note that I did not use the subcovering property. I used the fact that every open covering of a closed interval contains an open interval (which is not a subcover) that covers the closed set. Is there any flaw in my proof?","I tried to prove that the outer measure of a closed interval is its length without Heine-Borel Theorem. Proving the first part is easy and does not require Heine-Borel Theorem. Proving the other part  is summarized as follows: (1)-Clearly we have for any scalar . (2)- and are interior points of  any open coverng since . (3)- The covering is open. Hence there exists and such that  the intervals and . (4)- Steps (1), (2), and (3) imply that    there exists such that for every open covering of , i.e. for any open covering we have an open cover (not subcover) that contains . In other words, for any open cover, there exists such that is an open cover of with the following property: I know that the interval is not a subcover but it is an open cover that is tighter than the open cover itself. We can take the infimum over which is the same as taking the infimum overall covering. , which completes the proof. Note that I did not use the subcovering property. I used the fact that every open covering of a closed interval contains an open interval (which is not a subcover) that covers the closed set. Is there any flaw in my proof?","m^\star ([a,b])\leq a-b [a,b]\subset(a-\epsilon ,b+\epsilon )  \epsilon>0 a b [a,b] \subset \cup_{k=1}^\infty I_k \cup_{k=1}^\infty I_k \epsilon_1>0 \epsilon_2>0 (a-\epsilon_1,a+\epsilon_1) (b-\epsilon_2,b+\epsilon_2)\subset  \cup_{k=1}^\infty I_k \epsilon=min\{\epsilon_1,\epsilon_2\} (a-\epsilon ,b+\epsilon )\subset  \cup_{k=1}^\infty I_k [a,b] \cup_{k=1}^\infty I_k [a,b] \epsilon>0 (a-\epsilon ,b+\epsilon ) [a,b] [a,b]\subset(a-\epsilon ,b+\epsilon ) \subset  \cup_{k=1}^\infty I_k (a-\epsilon ,b+\epsilon ) \epsilon \textbf{EDIT: We have} b-a\leq l((a-\epsilon,b-\epsilon)\leq\sum_kl(I_k)) \textbf{which implies:} b-a= \inf_{\epsilon>0}\{ b -a+2\epsilon \}\leq \inf\bigg\{\sum_{k=0}^\infty l(I_k): [a,b]\subset\cup_{k=0}^\infty I_k\bigg\}= m^\star ([a,b])","['real-analysis', 'measure-theory', 'lebesgue-measure']"
80,"Proving that a function $f:[0,\infty)\to[0,\infty)$ satisfying these conditions is necessarily non-decreasing",Proving that a function  satisfying these conditions is necessarily non-decreasing,"f:[0,\infty)\to[0,\infty)","I have a function $f: [0, \infty) \to [0, \infty)$ which is smooth. I also have that $f(0) = 0$ $f'(0) > 0$ $f''(x) \leq 0$ , for all $x \in [0, \infty)$ It intuitively makes sense then that $f$ on $[0, \infty)$ must be strictly non-decreasing (since if it was ever decreasing, we would have to eventually ""pull up"" and contradict clause 3). I want to prove this, and I was able to derive contradictions if at some point the slope was negative. If at point $c$ in $(0, infty)$ , $f'(c) < 0$ , take some point $c + h$ further on in $[0, \infty)$ , $h > 0$ , and three cases occur: a. $f(c + h) = f(c)$ -> got a contradiction b. $f(c + h) > f(c)$ -> got a contradiction But for $f(c + h) < f(c)$ , this leads nowhere since it can still technically happen. I am at a complete loss on how to go forward. I was able to show that $f$ can never intersect $0$ . Edit: My main goal is to claim that $f(a) \leq f(b)$ for any $a < b$ on $[0, \infty)$","I have a function which is smooth. I also have that , for all It intuitively makes sense then that on must be strictly non-decreasing (since if it was ever decreasing, we would have to eventually ""pull up"" and contradict clause 3). I want to prove this, and I was able to derive contradictions if at some point the slope was negative. If at point in , , take some point further on in , , and three cases occur: a. -> got a contradiction b. -> got a contradiction But for , this leads nowhere since it can still technically happen. I am at a complete loss on how to go forward. I was able to show that can never intersect . Edit: My main goal is to claim that for any on","f: [0, \infty) \to [0, \infty) f(0) = 0 f'(0) > 0 f''(x) \leq 0 x \in [0, \infty) f [0, \infty) c (0, infty) f'(c) < 0 c + h [0, \infty) h > 0 f(c + h) = f(c) f(c + h) > f(c) f(c + h) < f(c) f 0 f(a) \leq f(b) a < b [0, \infty)","['real-analysis', 'calculus', 'monotone-functions']"
81,Is the set of the upper Riemann sums convex for an arbitrary bounded function $f(x)$?,Is the set of the upper Riemann sums convex for an arbitrary bounded function ?,f(x),"If $f$ is bounded on $[a,b]$ and $P= (x_0,x_1.\cdots,x_n)$ is a partition of $[a,b]$ , let $M_j = \sup_{x_{j-1}\le x \le x_j}\text{$f(x)$}$ . The upper Riemann sums of $f$ over $P$ is $S(P)= \sum_{j=1}^{n} M_j(x_j-x_{j-1}) $ . Now, let's consider the set of all $S(P)$ : $X_f$ = { $S(P)$ | for all possible $P$ }. Is $X_f$ convex for arbitrary bounded functions? If not, what conditions $f$ should have so that $X_f$ can be convex? By convex I mean: if $S(P_1) \in X_f$ and $S(P_2) \in X_f$ , then $\lambda S(P_1)+(1-\lambda)S(P_2) \in X_f$ for all $\lambda \in [0,1]$ .","If is bounded on and is a partition of , let . The upper Riemann sums of over is . Now, let's consider the set of all : = { | for all possible }. Is convex for arbitrary bounded functions? If not, what conditions should have so that can be convex? By convex I mean: if and , then for all .","f [a,b] P= (x_0,x_1.\cdots,x_n) [a,b] M_j = \sup_{x_{j-1}\le x \le x_j}\text{f(x)} f P S(P)= \sum_{j=1}^{n} M_j(x_j-x_{j-1})  S(P) X_f S(P) P X_f f X_f S(P_1) \in X_f S(P_2) \in X_f \lambda S(P_1)+(1-\lambda)S(P_2) \in X_f \lambda \in [0,1]","['real-analysis', 'riemann-integration', 'riemann-sum']"
82,Baby rudin theorem 10.7,Baby rudin theorem 10.7,,"We need this definition for the proof of the theorem: Here is the theorem: Suppose $F$ is a $\mathscr C'$ - mapping ( that means continuously differentiability) of an open set E $\subset R^n$ into $R^n$ , $0 \in E $ , $F(0) = 0$ , and $F'(0)$ is invertible. Then there is a neighborhood of $0$ in $R^n$ in which a representation: $$\mathbf{F}(\mathbf{x})=B_1\cdots B_{n-1}\mathbf{G}_n\circ \cdots \mathbf{G}_1(\mathbf{x})$$ . is valid. with each $\mathbf{G}_i$ being a primitive $\mathscr{C'}$ mapping in some neighborhood of $0$ , $\mathbf{G}_i(\mathbf{0})=0$ , and $\mathbf{G'}_i(0)$ is invertible, and each $B_i$ is either a flip or the identity operator. Here is the proof: Put $F = F_1$ . Assume $1 \leq m \leq n - 1,$ and make the following induction hypothesis ( which evidently holds for $m$ = 1): $V_m$ is a neighborhood of $0$ , $F_m$ $\in$ $\mathscr C'(V_m)$ , $F_m(0)$ = $0$ , $F_m'(0)$ is invertible, and $$P_{m-1}F_m(x) = P_{m-1}x (  x \in V_m). \tag{$\star$}$$ by ( $\star$ ), we  have: $$F_m(x) = P_{m-1}x + \sum_{i=m}^n \alpha_i(x)e_i\tag{$\ast$}$$ where $\alpha_m,...,\alpha_n$ are real $\mathscr C'$ -functions in $V_m$ . I don't understand from where does the last equality ( $\ast$ ) comes. I would be grateful for any kind of help.","We need this definition for the proof of the theorem: Here is the theorem: Suppose is a - mapping ( that means continuously differentiability) of an open set E into , , , and is invertible. Then there is a neighborhood of in in which a representation: . is valid. with each being a primitive mapping in some neighborhood of , , and is invertible, and each is either a flip or the identity operator. Here is the proof: Put . Assume and make the following induction hypothesis ( which evidently holds for = 1): is a neighborhood of , , = , is invertible, and by ( ), we  have: where are real -functions in . I don't understand from where does the last equality ( ) comes. I would be grateful for any kind of help.","F \mathscr C' \subset R^n R^n 0 \in E  F(0) = 0 F'(0) 0 R^n \mathbf{F}(\mathbf{x})=B_1\cdots B_{n-1}\mathbf{G}_n\circ \cdots \mathbf{G}_1(\mathbf{x}) \mathbf{G}_i \mathscr{C'} 0 \mathbf{G}_i(\mathbf{0})=0 \mathbf{G'}_i(0) B_i F = F_1 1 \leq m \leq n - 1, m V_m 0 F_m \in \mathscr C'(V_m) F_m(0) 0 F_m'(0) P_{m-1}F_m(x) = P_{m-1}x (  x \in V_m). \tag{\star} \star F_m(x) = P_{m-1}x + \sum_{i=m}^n \alpha_i(x)e_i\tag{\ast} \alpha_m,...,\alpha_n \mathscr C' V_m \ast","['real-analysis', 'linear-algebra', 'complex-analysis', 'functional-analysis', 'analysis']"
83,"How can I convert ""norms"" using the bijection between $\mathbb{N}$ and $\mathbb{Z}^{d}$?","How can I convert ""norms"" using the bijection between  and ?",\mathbb{N} \mathbb{Z}^{d},"Suppose $\varphi$ is a sequence $\varphi = \{\varphi(x)\}_{x\in \mathbb{Z}^{d}}$ satisfying the following condition. There exists $k \in \mathbb{N}$ and $C \ge 0$ such that: $$|\varphi(x)| \le C ||x||^{k} \tag{1}\label{1}$$ for every $x \in \mathbb{Z}^{d}$ . Here, $||x|| = \sqrt{x_{1}^{2}+\cdots x_{d}^{2}}$ . Since $\mathbb{Z}^{d}$ is countable, I can actuallt treat $\varphi$ to be a sequence indexed by $\mathbb{N}$ instead of $\mathbb{Z}$ , say, $\varphi = \{\varphi_{n}\}_{n\in \mathbb{N}}$ . This is what I'm trying to prove. Claim 1: There exists $m \in \mathbb{Z}$ such that: $$\sum_{n\in \mathbb{N}}n^{2m}|\varphi_{n}|^{2} < \infty \tag{2}\label{2}.$$ Claim 2: If (\ref{2}) holds, then (\ref{1}) also hold. Of course, I have to use condition (\ref{1}) to get (\ref{2}) and vice-versa, but I don't know how exactly does the $||x||$ is afected by the bijection $T: \mathbb{N} \to \mathbb{Z}^{d}$ . In other words, when $\{\varphi(x)\}_{x\in \mathbb{Z}^{d}}$ becomes $\{\varphi_{n}\}_{n\in \mathbb{N}}$ , how is condition (\ref{1}) changed, so I can use it to prove (\ref{2})?","Suppose is a sequence satisfying the following condition. There exists and such that: for every . Here, . Since is countable, I can actuallt treat to be a sequence indexed by instead of , say, . This is what I'm trying to prove. Claim 1: There exists such that: Claim 2: If (\ref{2}) holds, then (\ref{1}) also hold. Of course, I have to use condition (\ref{1}) to get (\ref{2}) and vice-versa, but I don't know how exactly does the is afected by the bijection . In other words, when becomes , how is condition (\ref{1}) changed, so I can use it to prove (\ref{2})?",\varphi \varphi = \{\varphi(x)\}_{x\in \mathbb{Z}^{d}} k \in \mathbb{N} C \ge 0 |\varphi(x)| \le C ||x||^{k} \tag{1}\label{1} x \in \mathbb{Z}^{d} ||x|| = \sqrt{x_{1}^{2}+\cdots x_{d}^{2}} \mathbb{Z}^{d} \varphi \mathbb{N} \mathbb{Z} \varphi = \{\varphi_{n}\}_{n\in \mathbb{N}} m \in \mathbb{Z} \sum_{n\in \mathbb{N}}n^{2m}|\varphi_{n}|^{2} < \infty \tag{2}\label{2}. ||x|| T: \mathbb{N} \to \mathbb{Z}^{d} \{\varphi(x)\}_{x\in \mathbb{Z}^{d}} \{\varphi_{n}\}_{n\in \mathbb{N}},"['real-analysis', 'sequences-and-series', 'functional-analysis', 'convergence-divergence', 'locally-convex-spaces']"
84,Simplified estimate for derivatives of $f\circ g$?,Simplified estimate for derivatives of ?,f\circ g,"Let $g:\mathbb R\to \mathbb R$ be a smooth compactly supported function, and let $f:\mathbb R\to \mathbb R$ be a smooth function with $f(0)=0$ . Let $\|f\|_m := \|f\|_{C^m}=\sum_{0\le k\le m}\sup_{x\in\mathbb R} |f^{(k)}(x)|$ and let $[f]_m:= \|  f^{(m)} \|_{0}$ . I am able to prove the following estimates- \begin{align} [f \circ g]_{m} & \lesssim_m \sum_{i=1}^{m}[f]_{i}\|g\|_{0}^{i-1}[g]_{m}, \tag{$A_1$}\label{A1}\\ [f \circ g]_{m} & \lesssim_m \sum_{i=1}^{m}[f]_{i}[g]_{1}^{(i-1) \frac{m}{m-1}}[g]_{m}^{\frac{m-i}{m-1}},\tag{$A_2$}\label{A2} \end{align} ( $a\lesssim_m b$ means that $a\le Cb$ with a constant $C>0$ depending on $m$ .) Their proofs are just Faà di Bruno's formula followed up by either of the interpolation inequalities \begin{align} [g]_i \lesssim \|g\|_{0}^{1-i/m} [g]_m^{i/m}, \quad\text{or}\quad [g]_i \lesssim [g]_{1}^{1-\frac{i-1}{m-1}} [g]_m^{\frac{i-1}{m-1}} . \quad(i\ge 1) \end{align} Question I'm wondering however if its possible to get the following simpler looking bounds? \begin{align} [f \circ g]_{m} & \overset{\color{red} ?}\lesssim_m  [f]_1[g]_{m} +\|f'\|_{m-1}\|g\|_0^{m-1}[g]_m, \tag{$B_1$}\label{B1}\\ [f \circ g]_{m} & \overset{\color{blue} ?}\lesssim_m  [f]_1[g]_m + \|f'\|_{m-1}[g]_1^m.\tag{$B_2$}\label{B2} \end{align} These are something like only taking the $i=1$ and $i=m$ terms in the sums of ( $A_{1,2}$ ), but some seminorms are replaced with norms. I came across this in a certain Arxiv paper but I have found a number of other (minor) mistakes so my confidence in exactly this inequality is not the highest. ( $B_{1,2}$ ) are obvious in the simplest case $f(x)=x$ , and 'feel' like the above interpolation inequalities (specifically after further applying Young's inequality) $ [g]_j \lesssim \|g\|_0 + [g]_m$ , but I am unable to see how to estimate a general term of $(A_{1,2})$ , because it seems $f$ and $g$ cannot be estimated separately without decoupling them, hence getting a worse estimate, like e.g. $$ [f\circ g]_m \lesssim_m [f]_1 [g]_m + \|f'\|_{m-1}\|g\|^m_{m}.$$ I did manage to get a counterexample to a stronger estimate $[f \circ g]_{m}  \lesssim_m  [f]_1[g]_m + [f]_m[g]_1^m$ , indicating that $\|Df\|_{m-1}$ cannot be replaced with $[f]_m$ . (Just take $f(x)=x^2$ , $g(x)=\lambda g_0(x)$ with $g_0\in C^\infty_c$ , then send $\lambda\to\infty$ .)","Let be a smooth compactly supported function, and let be a smooth function with . Let and let . I am able to prove the following estimates- ( means that with a constant depending on .) Their proofs are just Faà di Bruno's formula followed up by either of the interpolation inequalities Question I'm wondering however if its possible to get the following simpler looking bounds? These are something like only taking the and terms in the sums of ( ), but some seminorms are replaced with norms. I came across this in a certain Arxiv paper but I have found a number of other (minor) mistakes so my confidence in exactly this inequality is not the highest. ( ) are obvious in the simplest case , and 'feel' like the above interpolation inequalities (specifically after further applying Young's inequality) , but I am unable to see how to estimate a general term of , because it seems and cannot be estimated separately without decoupling them, hence getting a worse estimate, like e.g. I did manage to get a counterexample to a stronger estimate , indicating that cannot be replaced with . (Just take , with , then send .)","g:\mathbb R\to \mathbb R f:\mathbb R\to \mathbb R f(0)=0 \|f\|_m := \|f\|_{C^m}=\sum_{0\le k\le m}\sup_{x\in\mathbb R} |f^{(k)}(x)| [f]_m:= \|  f^{(m)} \|_{0} \begin{align}
[f \circ g]_{m} & \lesssim_m \sum_{i=1}^{m}[f]_{i}\|g\|_{0}^{i-1}[g]_{m}, \tag{A_1}\label{A1}\\
[f \circ g]_{m} & \lesssim_m \sum_{i=1}^{m}[f]_{i}[g]_{1}^{(i-1) \frac{m}{m-1}}[g]_{m}^{\frac{m-i}{m-1}},\tag{A_2}\label{A2}
\end{align} a\lesssim_m b a\le Cb C>0 m \begin{align}
[g]_i \lesssim \|g\|_{0}^{1-i/m} [g]_m^{i/m}, \quad\text{or}\quad [g]_i \lesssim [g]_{1}^{1-\frac{i-1}{m-1}} [g]_m^{\frac{i-1}{m-1}} . \quad(i\ge 1)
\end{align} \begin{align}
[f \circ g]_{m} & \overset{\color{red} ?}\lesssim_m  [f]_1[g]_{m} +\|f'\|_{m-1}\|g\|_0^{m-1}[g]_m, \tag{B_1}\label{B1}\\
[f \circ g]_{m} & \overset{\color{blue} ?}\lesssim_m  [f]_1[g]_m + \|f'\|_{m-1}[g]_1^m.\tag{B_2}\label{B2}
\end{align} i=1 i=m A_{1,2} B_{1,2} f(x)=x  [g]_j \lesssim \|g\|_0 + [g]_m (A_{1,2}) f g  [f\circ g]_m \lesssim_m [f]_1 [g]_m + \|f'\|_{m-1}\|g\|^m_{m}. [f \circ g]_{m}  \lesssim_m  [f]_1[g]_m + [f]_m[g]_1^m \|Df\|_{m-1} [f]_m f(x)=x^2 g(x)=\lambda g_0(x) g_0\in C^\infty_c \lambda\to\infty","['real-analysis', 'analysis', 'derivatives', 'inequality', 'chain-rule']"
85,Proof of limit that involves integral,Proof of limit that involves integral,,"I was trying to prove the following statement but I am not sure whether my proof is correct or not. Prove that for any $a > 0$ the following holds: $\lim_{t \to \infty}  \int_t^{t+a}\frac{sin(x)}{x}dx = 0$ My approach: Denote $f(t) = \int_t^{t+a}\frac{sin(x)}{x}dx$ , Then we need to prove that for any $\epsilon > 0$ there is $M \in R$ such that if $ t > M$ then $|f(t) - 0| < \epsilon$ Let $\epsilon = \frac{\epsilon}{a}$ , from the fact that $\lim_{x \to \infty} \frac{sin(x)}{x} = 0$ we know that there is an element $M1 \in R$ such that if $x>M1$ then $|\frac{sin(x)}{x}| < \frac{\epsilon}{a}$ Therefore for the same $M1$ if $t>M1$ then for any $x \in [t,t+a]$ : $|\frac{sin(x)}{x}| < \frac{\epsilon}{a} \Longleftrightarrow -\frac{\epsilon}{a} < \frac{sin(x)}{x} < \frac{\epsilon}{a}$ Therefore $f$ is bounded between $-\frac{\epsilon}{a}$ and $\frac{\epsilon}{a}$ , so: $-\frac{\epsilon}{a} (t+a-t) < \int_t^{t+a}\frac{sin(x)}{x}dx < \frac{\epsilon}{a} *(t+a-t) \Longleftrightarrow -\epsilon < \int_t^{t+a}\frac{sin(x)}{x}dx < \epsilon \Longleftrightarrow $ $|\int_t^{t+a}\frac{sin(x)}{x}dx| < \epsilon$ This is what I did but I am not sure that this is true. I will appreciate any help, thanks in advance.","I was trying to prove the following statement but I am not sure whether my proof is correct or not. Prove that for any the following holds: My approach: Denote , Then we need to prove that for any there is such that if then Let , from the fact that we know that there is an element such that if then Therefore for the same if then for any : Therefore is bounded between and , so: This is what I did but I am not sure that this is true. I will appreciate any help, thanks in advance.","a > 0 \lim_{t \to \infty}  \int_t^{t+a}\frac{sin(x)}{x}dx = 0 f(t) = \int_t^{t+a}\frac{sin(x)}{x}dx \epsilon > 0 M \in R  t > M |f(t) - 0| < \epsilon \epsilon = \frac{\epsilon}{a} \lim_{x \to \infty} \frac{sin(x)}{x} = 0 M1 \in R x>M1 |\frac{sin(x)}{x}| < \frac{\epsilon}{a} M1 t>M1 x \in [t,t+a] |\frac{sin(x)}{x}| < \frac{\epsilon}{a} \Longleftrightarrow -\frac{\epsilon}{a} < \frac{sin(x)}{x} < \frac{\epsilon}{a} f -\frac{\epsilon}{a} \frac{\epsilon}{a} -\frac{\epsilon}{a} (t+a-t) < \int_t^{t+a}\frac{sin(x)}{x}dx < \frac{\epsilon}{a} *(t+a-t) \Longleftrightarrow -\epsilon < \int_t^{t+a}\frac{sin(x)}{x}dx < \epsilon \Longleftrightarrow  |\int_t^{t+a}\frac{sin(x)}{x}dx| < \epsilon","['real-analysis', 'calculus', 'integration', 'limits']"
86,Distance of scaled point on the unit sphere to an integer,Distance of scaled point on the unit sphere to an integer,,"Let $S:=\{ x\in \mathbb{R}^d:||x||_2=1\}$ be the d-dimensional unit sphere, where $||x||_2$ is the euclidean norm. Let $\epsilon>0$ and $s\in S$ be an arbitrary point on the sphere. Is it correct that there exists an $\alpha>0$ and a $k\in \mathbb{Z}^d\setminus\{0\}$ such that the distance between $\alpha s-k$ is less than $\epsilon$ ? In other words can i scale every point on the unit sphere such that the distance to an non-zero integer is arbitrarily small","Let be the d-dimensional unit sphere, where is the euclidean norm. Let and be an arbitrary point on the sphere. Is it correct that there exists an and a such that the distance between is less than ? In other words can i scale every point on the unit sphere such that the distance to an non-zero integer is arbitrarily small",S:=\{ x\in \mathbb{R}^d:||x||_2=1\} ||x||_2 \epsilon>0 s\in S \alpha>0 k\in \mathbb{Z}^d\setminus\{0\} \alpha s-k \epsilon,['real-analysis']
87,Why are dense set useful in analysis,Why are dense set useful in analysis,,"Consider the following theorem from Rudin's Real-and complex analysis. 3.14 Theorem For $1 \leq p < \infty, C_c(X)$ is dense in $L^p(\mu)$ . What I don't understand is why this result is useful in proofs. I heard few examples of where in order to define functionals in $L^p(\mu)$ you can firstly define them in $C_c(X)$ and then extend them using Hahn Banach theorem. Is this the main application? definition of maps and functionals? Update : If anyone could point to the proofs of results that actually use density of $C_c(X)$ in $L^p(\mathbb{\mu})$ that would be useful.",Consider the following theorem from Rudin's Real-and complex analysis. 3.14 Theorem For is dense in . What I don't understand is why this result is useful in proofs. I heard few examples of where in order to define functionals in you can firstly define them in and then extend them using Hahn Banach theorem. Is this the main application? definition of maps and functionals? Update : If anyone could point to the proofs of results that actually use density of in that would be useful.,"1 \leq p < \infty, C_c(X) L^p(\mu) L^p(\mu) C_c(X) C_c(X) L^p(\mathbb{\mu})","['real-analysis', 'functional-analysis']"
88,Non constant periodic solutions to $y''=y'-2(y')^2-3y+4y^2$,Non constant periodic solutions to,y''=y'-2(y')^2-3y+4y^2,"Does the equation $y''=y'-2(y')^2-3y+4y^2$ have non constant periodic solutions? So far I transformed this equation into a first order ODE system by setting $x=y'$ . This is what we saw in class: The ODE $y'=v(y)$ has no non constant periodic orbits if there exists a function $\mu$ such that $\omega = \mu \sum v_i dy_i$ is exact. I'm struggling to find $\mu$ and I'm wondering if there is a better method than just trial and error. I've tried rearranging the equation too and maybe isolate either x or y, but it didn't get me anywhere.","Does the equation have non constant periodic solutions? So far I transformed this equation into a first order ODE system by setting . This is what we saw in class: The ODE has no non constant periodic orbits if there exists a function such that is exact. I'm struggling to find and I'm wondering if there is a better method than just trial and error. I've tried rearranging the equation too and maybe isolate either x or y, but it didn't get me anywhere.",y''=y'-2(y')^2-3y+4y^2 x=y' y'=v(y) \mu \omega = \mu \sum v_i dy_i \mu,"['real-analysis', 'ordinary-differential-equations']"
89,Formula for $f(1) + f(2) + \cdots + f(n)$: Euler-Maclaurin summation formula,Formula for : Euler-Maclaurin summation formula,f(1) + f(2) + \cdots + f(n),"Let $f\colon \mathbb{R}\to \mathbb{R}$ be a function with $k$ continuous derivatives. We want to find an expression for $$ S=f(1)+f(2)+f(3)+\ldots+f(n). $$ I'm currently reading Analysis by Its History by Hairer and Wanner. They first consider the shifted sum and arrive at the expression $$f(n)-f(0)=\sum_{i=1}^{n} f^{\prime}(i)-\frac{1}{2 !} \sum_{i=1}^{n} f^{\prime \prime}(i)+\frac{1}{3 !} \sum_{i=1}^{n} f^{\prime \prime \prime}(i)-\frac{1}{4 !} \sum_{i=1}^{n} f^{\prime \prime \prime \prime}(i)+\ldots$$ using Taylor series (provided that the Taylor series actually converges to $f$ ). In order to turn this formula for $\sum f^{\prime}(i)$ into a formula for $\sum f(i)$ , we replace $f$ by its primitive (again denoted by $f$ ): $$\sum_{i=1}^{n} f(i)=\int_{0}^{n} f(x) d x+\frac{1}{2 !} \sum_{i=1}^{n} f^{\prime}(i)-\frac{1}{3 !} \sum_{i=1}^{n} f^{\prime \prime}(i)+\frac{1}{4 !} \sum_{i=1}^{n} f^{\prime \prime \prime}(i)-\ldots$$ The second idea is to remove the sums $\sum f^{\prime}, \sum f^{\prime \prime}, \sum f^{\prime \prime \prime}$ , on the right by using the same formula, with $f$ successively replaced by $f^{\prime}, f^{\prime \prime}, f^{\prime \prime \prime}$ etc. I don't really understand the step which replaces $f$ by its primitive. Using $F$ for denoting the primitive of $f$ I obtain $$F(n)-F(0)=\sum_{i=1}^{n} F^{}(i)-\frac{1}{2 !} \sum_{i=1}^{n} F^{\prime}(i)+\frac{1}{3 !} \sum_{i=1}^{n} F^{\prime \prime}(i)-\frac{1}{4 !} \sum_{i=1}^{n} F^{\prime \prime \prime}(i)+\ldots$$ but I don't obtain any  expression in terms of an integral. Clearly, $$ F(n) - F(0) = \int_0^n f(x) \textrm{d}x $$ but since the author mentions that he again denotes the primitive by $f$ this doesn't match up with the above formula. Can anyone explain me what my  mistake is here?","Let be a function with continuous derivatives. We want to find an expression for I'm currently reading Analysis by Its History by Hairer and Wanner. They first consider the shifted sum and arrive at the expression using Taylor series (provided that the Taylor series actually converges to ). In order to turn this formula for into a formula for , we replace by its primitive (again denoted by ): The second idea is to remove the sums , on the right by using the same formula, with successively replaced by etc. I don't really understand the step which replaces by its primitive. Using for denoting the primitive of I obtain but I don't obtain any  expression in terms of an integral. Clearly, but since the author mentions that he again denotes the primitive by this doesn't match up with the above formula. Can anyone explain me what my  mistake is here?","f\colon \mathbb{R}\to \mathbb{R} k 
S=f(1)+f(2)+f(3)+\ldots+f(n).
 f(n)-f(0)=\sum_{i=1}^{n} f^{\prime}(i)-\frac{1}{2 !} \sum_{i=1}^{n} f^{\prime \prime}(i)+\frac{1}{3 !} \sum_{i=1}^{n} f^{\prime \prime \prime}(i)-\frac{1}{4 !} \sum_{i=1}^{n} f^{\prime \prime \prime \prime}(i)+\ldots f \sum f^{\prime}(i) \sum f(i) f f \sum_{i=1}^{n} f(i)=\int_{0}^{n} f(x) d x+\frac{1}{2 !} \sum_{i=1}^{n} f^{\prime}(i)-\frac{1}{3 !} \sum_{i=1}^{n} f^{\prime \prime}(i)+\frac{1}{4 !} \sum_{i=1}^{n} f^{\prime \prime \prime}(i)-\ldots \sum f^{\prime}, \sum f^{\prime \prime}, \sum f^{\prime \prime \prime} f f^{\prime}, f^{\prime \prime}, f^{\prime \prime \prime} f F f F(n)-F(0)=\sum_{i=1}^{n} F^{}(i)-\frac{1}{2 !} \sum_{i=1}^{n} F^{\prime}(i)+\frac{1}{3 !} \sum_{i=1}^{n} F^{\prime \prime}(i)-\frac{1}{4 !} \sum_{i=1}^{n} F^{\prime \prime \prime}(i)+\ldots 
F(n) - F(0) = \int_0^n f(x) \textrm{d}x
 f","['real-analysis', 'integration', 'summation', 'euler-maclaurin']"
90,uniform convergence fn to f,uniform convergence fn to f,,"I'm given the following function sequence: $$f_n = \frac{nx}{1+nx^2}, \forall x \in A = [0,\infty].$$ I show the following that: $$\lim_{n \to \infty} \frac{nx}{1+nx^2} \le \frac{nx}{nx^2} \le \frac{1}{x}.$$ And thus my convergent function I compute is $f(x) = \frac{1}{x}.$ However the answer appears to be $f(x) = \frac{1}{2x}$ using A/G mean inequality. This leads to my next question that if $f_n \to f$ converges EITHER point wise or uniformly to $f$ , is $f$ unique?","I'm given the following function sequence: I show the following that: And thus my convergent function I compute is However the answer appears to be using A/G mean inequality. This leads to my next question that if converges EITHER point wise or uniformly to , is unique?","f_n = \frac{nx}{1+nx^2}, \forall x \in A = [0,\infty]. \lim_{n \to \infty} \frac{nx}{1+nx^2} \le \frac{nx}{nx^2} \le \frac{1}{x}. f(x) = \frac{1}{x}. f(x) = \frac{1}{2x} f_n \to f f f","['real-analysis', 'uniform-convergence']"
91,"Is map of ""deltas"" to points on closed interval continuous?","Is map of ""deltas"" to points on closed interval continuous?",,"I am having trouble properly coming up with an answer to the following: Let $f: [0, 1] \to \mathbb{R}$ be a continuous function, using the usual (Cauchy) characterization of continuity. Define the set $D_{\epsilon} (x) = \{d \in \mathbb{R}: \forall x' \in [0, 1]:|x'-x| < d \implies |f(x')-f(x)| < \epsilon\}$ for all $x \in [0,1]$ , and let $\delta(x) = \sup{D_{\epsilon}}$ . Does the fact that $f(x)$ is continuous on $[0,1]$ imply that the map $\delta(x)$ is continuous too? I have tried to derive a contradiction, by showing that if some points in the interval are close enough, but their $\delta$ differs by a finite amount, one of the $\delta$ is not actually the supremum of the set D for one of them, but I didn't manage to take my argument too far. Any help is much appreciated!","I am having trouble properly coming up with an answer to the following: Let be a continuous function, using the usual (Cauchy) characterization of continuity. Define the set for all , and let . Does the fact that is continuous on imply that the map is continuous too? I have tried to derive a contradiction, by showing that if some points in the interval are close enough, but their differs by a finite amount, one of the is not actually the supremum of the set D for one of them, but I didn't manage to take my argument too far. Any help is much appreciated!","f: [0, 1] \to \mathbb{R} D_{\epsilon} (x) = \{d \in \mathbb{R}: \forall x' \in [0, 1]:|x'-x| < d \implies |f(x')-f(x)| < \epsilon\} x \in [0,1] \delta(x) = \sup{D_{\epsilon}} f(x) [0,1] \delta(x) \delta \delta","['real-analysis', 'analysis', 'functions', 'continuity', 'epsilon-delta']"
92,This inequality holds on Hilbert Spaces?,This inequality holds on Hilbert Spaces?,,"I am trying to solve a probelm and if I conclude that the following inequality holds I finish it. Consider $H$ an Hilbert space over $\mathbb{R}$ . Let $\alpha_1, ..., \alpha_n$ be real numbers such that $\alpha_i\geq 0$ , for all $i=1,...,n$ and $\sum_{i=1}^n\alpha_i=1$ . Let $x_1, ...,x_n \in H$ and define $$x:=\sum_{i=1}^n\alpha_ix_i$$ My question is: the following inequality holds for all $i,j$ ? $$\lVert x-x_i\rVert ^2+\lVert x-x_j\rVert ^2 \leq \lVert x_i-x_j\rVert ^2$$ I can easily see that it holds when $\{x_1, ..., x_n\}$ is orthogonal, but I'm not sure in the general case. I would appreciate any hint. Thanks!","I am trying to solve a probelm and if I conclude that the following inequality holds I finish it. Consider an Hilbert space over . Let be real numbers such that , for all and . Let and define My question is: the following inequality holds for all ? I can easily see that it holds when is orthogonal, but I'm not sure in the general case. I would appreciate any hint. Thanks!","H \mathbb{R} \alpha_1, ..., \alpha_n \alpha_i\geq 0 i=1,...,n \sum_{i=1}^n\alpha_i=1 x_1, ...,x_n \in H x:=\sum_{i=1}^n\alpha_ix_i i,j \lVert x-x_i\rVert ^2+\lVert x-x_j\rVert ^2 \leq \lVert x_i-x_j\rVert ^2 \{x_1, ..., x_n\}","['real-analysis', 'functional-analysis', 'analysis', 'hilbert-spaces']"
93,Walter Rudin Theorem 1.20 (a) Proof,Walter Rudin Theorem 1.20 (a) Proof,,"Theorem 1.20 (a) If $x\in\mathbb{R}$ , $y\in\mathbb{R}$ and $x > 0$ , then there is a positive integer $n$ such that $nx > y$ . In the proof, set $A$ is assumed to have a least upper bound in $\mathbb{R}$ that implies set $A$ is bounded above. But I don't known how set $A$ is bounded above, given the constraint, $x$ is a positive real number and $n$ is a natural number. More clarification: $$A = \{nx\in\mathbb{R}_{>0} \mid (n\in\mathbb{N})\wedge(x\in\mathbb{R}_{>0})\}$$ My question is: how this set $A$ have a upper bound? And the whole theorem and proof didn't really make sense to me. I mean, if this theorem were reversed, i.e. $nx < y$ , I wouldn't be able to make difference between these two scenario. Potential duplicate post: is referring to possibility of set A being empty. In this post, I’m asking(implicitly) about contradiction statement of theorem and bdd property of set. Non empty set is just one of the condition must satisfy to talk about bddness of sets.","Theorem 1.20 (a) If , and , then there is a positive integer such that . In the proof, set is assumed to have a least upper bound in that implies set is bounded above. But I don't known how set is bounded above, given the constraint, is a positive real number and is a natural number. More clarification: My question is: how this set have a upper bound? And the whole theorem and proof didn't really make sense to me. I mean, if this theorem were reversed, i.e. , I wouldn't be able to make difference between these two scenario. Potential duplicate post: is referring to possibility of set A being empty. In this post, I’m asking(implicitly) about contradiction statement of theorem and bdd property of set. Non empty set is just one of the condition must satisfy to talk about bddness of sets.",x\in\mathbb{R} y\in\mathbb{R} x > 0 n nx > y A \mathbb{R} A A x n A = \{nx\in\mathbb{R}_{>0} \mid (n\in\mathbb{N})\wedge(x\in\mathbb{R}_{>0})\} A nx < y,"['real-analysis', 'upper-lower-bounds']"
94,Prove that $\lim_{n\to\infty} \frac{a_n}{a_{n+1}} = p$ [duplicate],Prove that  [duplicate],\lim_{n\to\infty} \frac{a_n}{a_{n+1}} = p,"This question already has answers here : How prove this limit $\lim\limits_{n\rightarrow \infty} \frac{f_n}{f_{n+1}}=a$ given two other limits related to $f_n$ (5 answers) Closed 3 years ago . Given a sequence $\{a_n\}_{n\in \mathbb N}$ of real numbers such that $$\begin{align} \lim_{n\to\infty}\frac{a_na_{n+1} - a_{n-1}a_{n+2}}{a_{n+1}^2 - a_na_{n+2}} &= p + q && (1)\\[1mm] \lim_{n\to\infty} \frac{a_n^2 - a_{n-1}a_{n+1}}{a_{n+1}^2 - a_na_{n+2}} &= pq && (2)\end{align}$$ where $|p| < |q|$ , prove that $$\lim_{n\to\infty} \frac{a_n}{a_{n+1}} = p$$ Attempts: Idea #1: Let us denote $b_n = \dfrac{a_n}{a_{n+1}}$ . If we divide both numerators and denominators of $(1)$ and $(2)$ by $a_na_{n+2}$ and $a_{n+1}a_n$ , respectively, we have $$ \begin{align} \lim_{n\to\infty}\frac{\dfrac{a_{n+1}}{a_{n+2}} - \dfrac{a_{n-1}}{a_{n}}}{\dfrac{a_{n+1}}{a_n}\dfrac{a_{n+1}}{a_{n+2}} - 1} &= p+q && (1') \\[1mm] \lim_{n\to\infty} \frac{\dfrac{a_n}{a_{a+1}} - \dfrac{a_{n-1}}{a_n}}{\dfrac{a_{n+1}}{a_n} - \dfrac{a_{n+2}}{a_{n+1}}} &= pq && (2') \end{align}$$ which now can be written as $$\begin{align} \lim_{n\to\infty}\frac{b_{n+1}b_n - b_nb_{n-1}}{b_{n+1} - b_n} &= p+q  && (1'') \\[1mm] \lim_{n\to\infty} \frac{b_n - b_{n-1}}{\dfrac{1}{b_{n+1}} - \dfrac{1}{b_{n}}} &= -pq && (2'') \end{align}$$ Now, every numerator and denominator contains the difference of consecutive terms of some sequence (reminds me of Cesaro-Stolz, but that cannot be applied here). Idea #2: The given conditions can be written as $$\begin{align} \lim_{n\to\infty}\frac{\begin{vmatrix}a_n & a_{n-1} \\ a_{n+2} & a_{n+1}\end{vmatrix}}{\begin{vmatrix}a_{n+1} & a_{n} \\ a_{n+2} & a_{n+1}\end{vmatrix}} &= p + q && (1')\\[2mm] \lim_{n\to\infty} \frac{\begin{vmatrix}a_n & a_{n-1} \\ a_{n+1} & a_{n}\end{vmatrix}}{\begin{vmatrix}a_{n+1} & a_{n} \\ a_{n+2} & a_{n+1}\end{vmatrix}} &= pq && (2')\end{align}$$ Now, maybe a bit of Linear Algebra could be incorporated somehow. If we set (B. Grossman) $$\begin{align} x_n = \frac{\begin{vmatrix}a_n & a_{n-1} \\ a_{n+2} & a_{n+1}\end{vmatrix}}{\begin{vmatrix}a_{n+1} & a_{n} \\ a_{n+2} & a_{n+1}\end{vmatrix}}, \quad  y_n =  \frac{\begin{vmatrix}a_n & a_{n-1} \\ a_{n+1} & a_{n}\end{vmatrix}}{\begin{vmatrix}a_{n+1} & a_{n} \\ a_{n+2} & a_{n+1}\end{vmatrix}} \end{align}$$ Then, we have $$\pmatrix{a_{n+1} & a_{n+2}\\a_n & a_{n+1}} \pmatrix{x_n\\y_n} = \pmatrix{a_n\\a_{n-1}}$$ This gives $$\frac{a_{n+1}x_n + a_{n+2}y_n}{a_nx_n + a_{n+1}y_n} = \frac{a_n}{a_{n-1}}$$ Dividing the numerator and the denominator by $a_{n+1}$ , we get $$\frac{x_n + \dfrac{a_{n+2}}{a_{n+1}}y_n}{\dfrac{a_n}{a_{n+1}}x_n + y_n} = \frac{a_n}{a_{n-1}}$$ Noting that $x_n \to p+q$ , $y_n \to pq$ and assuming that $\dfrac{a_n}{a_{n-1}} \to A$ , and sending $n$ to infinity, we get $$\frac{p+q + Apq}{\dfrac{1}{A}(p+q) + pq} = A$$ which simplifies to $$0=0$$ I think I did something wrong somewhere. Any help is appreciated.","This question already has answers here : How prove this limit $\lim\limits_{n\rightarrow \infty} \frac{f_n}{f_{n+1}}=a$ given two other limits related to $f_n$ (5 answers) Closed 3 years ago . Given a sequence of real numbers such that where , prove that Attempts: Idea #1: Let us denote . If we divide both numerators and denominators of and by and , respectively, we have which now can be written as Now, every numerator and denominator contains the difference of consecutive terms of some sequence (reminds me of Cesaro-Stolz, but that cannot be applied here). Idea #2: The given conditions can be written as Now, maybe a bit of Linear Algebra could be incorporated somehow. If we set (B. Grossman) Then, we have This gives Dividing the numerator and the denominator by , we get Noting that , and assuming that , and sending to infinity, we get which simplifies to I think I did something wrong somewhere. Any help is appreciated.","\{a_n\}_{n\in \mathbb N} \begin{align}
\lim_{n\to\infty}\frac{a_na_{n+1} - a_{n-1}a_{n+2}}{a_{n+1}^2 - a_na_{n+2}} &= p + q && (1)\\[1mm]
\lim_{n\to\infty} \frac{a_n^2 - a_{n-1}a_{n+1}}{a_{n+1}^2 - a_na_{n+2}} &= pq && (2)\end{align} |p| < |q| \lim_{n\to\infty} \frac{a_n}{a_{n+1}} = p b_n = \dfrac{a_n}{a_{n+1}} (1) (2) a_na_{n+2} a_{n+1}a_n 
\begin{align}
\lim_{n\to\infty}\frac{\dfrac{a_{n+1}}{a_{n+2}} - \dfrac{a_{n-1}}{a_{n}}}{\dfrac{a_{n+1}}{a_n}\dfrac{a_{n+1}}{a_{n+2}} - 1} &= p+q && (1') \\[1mm]
\lim_{n\to\infty} \frac{\dfrac{a_n}{a_{a+1}} - \dfrac{a_{n-1}}{a_n}}{\dfrac{a_{n+1}}{a_n} - \dfrac{a_{n+2}}{a_{n+1}}} &= pq && (2')
\end{align} \begin{align}
\lim_{n\to\infty}\frac{b_{n+1}b_n - b_nb_{n-1}}{b_{n+1} - b_n} &= p+q  && (1'') \\[1mm]
\lim_{n\to\infty} \frac{b_n - b_{n-1}}{\dfrac{1}{b_{n+1}} - \dfrac{1}{b_{n}}} &= -pq && (2'')
\end{align} \begin{align}
\lim_{n\to\infty}\frac{\begin{vmatrix}a_n & a_{n-1} \\ a_{n+2} & a_{n+1}\end{vmatrix}}{\begin{vmatrix}a_{n+1} & a_{n} \\ a_{n+2} & a_{n+1}\end{vmatrix}} &= p + q && (1')\\[2mm]
\lim_{n\to\infty} \frac{\begin{vmatrix}a_n & a_{n-1} \\ a_{n+1} & a_{n}\end{vmatrix}}{\begin{vmatrix}a_{n+1} & a_{n} \\ a_{n+2} & a_{n+1}\end{vmatrix}} &= pq && (2')\end{align} \begin{align} x_n = \frac{\begin{vmatrix}a_n & a_{n-1} \\ a_{n+2} & a_{n+1}\end{vmatrix}}{\begin{vmatrix}a_{n+1} & a_{n} \\ a_{n+2} & a_{n+1}\end{vmatrix}}, \quad  y_n =  \frac{\begin{vmatrix}a_n & a_{n-1} \\ a_{n+1} & a_{n}\end{vmatrix}}{\begin{vmatrix}a_{n+1} & a_{n} \\ a_{n+2} & a_{n+1}\end{vmatrix}} \end{align} \pmatrix{a_{n+1} & a_{n+2}\\a_n & a_{n+1}} \pmatrix{x_n\\y_n} = \pmatrix{a_n\\a_{n-1}} \frac{a_{n+1}x_n + a_{n+2}y_n}{a_nx_n + a_{n+1}y_n} = \frac{a_n}{a_{n-1}} a_{n+1} \frac{x_n + \dfrac{a_{n+2}}{a_{n+1}}y_n}{\dfrac{a_n}{a_{n+1}}x_n + y_n} = \frac{a_n}{a_{n-1}} x_n \to p+q y_n \to pq \dfrac{a_n}{a_{n-1}} \to A n \frac{p+q + Apq}{\dfrac{1}{A}(p+q) + pq} = A 0=0","['real-analysis', 'calculus', 'sequences-and-series', 'limits']"
95,"Existence of a subsequence in a set of positive measure for any sequence in [0,1]","Existence of a subsequence in a set of positive measure for any sequence in [0,1]",,"The following is an exercise from Bruckner's Real Analysis: 5:12.2 Let $E$ be a Lebesgue measurable set of positive measure, and let ${\{x_n}\}$ be any sequence of points in the interval $[0, 1]$ . Show that there must exist a point $y$ and a subsequence ${\{x_{n_k}}\}$ so that $y + x_{n_k} \in  E$ for all $k$ . [Hint: Consider the functions $f_n(t)=χ_E(t − x_n)$ and their integrals.] I tried the hint of the book but it seems to be not useful: $\int_{[0, 1]} χ_E(t − x_n) d \mu = \mu (x_n+E) = \mu(E)$ . By LDCT, $\lim_n \int_{[0, 1]} χ_E(t − x_n) d \mu = \int_{[0, 1]} \lim_n χ_E(t − x_n) d \mu = \int_{[0, 1]} χ_E(t − x) = \mu (x+E) = \mu(E)$ ; as expected. But how does it guide to the existence of $y$ and a sub-sequence ${\{x_{n_k}}\}$ such that $y + x_{n_k} \in  E$ for all $k$ ? Also how there can be a $y \in [0, 1]$ if the set $E$ 'is spread enough' and includes both points ${\{0,1}\}$ ?","The following is an exercise from Bruckner's Real Analysis: 5:12.2 Let be a Lebesgue measurable set of positive measure, and let be any sequence of points in the interval . Show that there must exist a point and a subsequence so that for all . [Hint: Consider the functions and their integrals.] I tried the hint of the book but it seems to be not useful: . By LDCT, ; as expected. But how does it guide to the existence of and a sub-sequence such that for all ? Also how there can be a if the set 'is spread enough' and includes both points ?","E {\{x_n}\} [0, 1] y {\{x_{n_k}}\} y + x_{n_k} \in  E k f_n(t)=χ_E(t − x_n) \int_{[0, 1]} χ_E(t − x_n) d \mu = \mu (x_n+E) = \mu(E) \lim_n \int_{[0, 1]} χ_E(t − x_n) d \mu = \int_{[0, 1]} \lim_n χ_E(t − x_n) d \mu = \int_{[0, 1]} χ_E(t − x) = \mu (x+E) = \mu(E) y {\{x_{n_k}}\} y + x_{n_k} \in  E k y \in [0, 1] E {\{0,1}\}",['real-analysis']
96,Are there any $f(x)$ whose $\xi_n$ of the Lagrange's remainder does not converge to $0$?,Are there any  whose  of the Lagrange's remainder does not converge to ?,f(x) \xi_n 0,"Consider the Maclaurin's series of an analytic function $f$ with Lagrange's remainder. Define $\xi_n$ as $$f(1)=\sum_{k=0}^{n-1}\frac{f^{(k)}(0)}{k!}+\frac{f^{(n)}(\xi_n)}{n!}$$ where $0<\xi_n<1$ , and if there are several that satisfy this, choose the one closest to $0$ (or the infimum if there are infinitely many). Is there an example of $f$ such that $\xi_n$ doesn't converge to $0$ ? I'm not sure why, but all the ones I tried seemed to go to zero. Edit: $\frac{f^{(n)}(\xi_n)}{f^{(n)}(0)}=1+\frac{1}{n+1}\frac{f^{(n+1)}(\xi_{n+1})}{f^{(n)}(0)}$ , so, if $\frac{f^{(n+1)}(\xi_{n+1})}{f^{(n)}(0)}=o(n)$ then $f^{(n)}(\xi_n)\to f^{(n)}(0)$ . This implies that $\xi_n$ converges to $0$ for some functions such that $e^{ax}, \sin ax$ . From the results of numerical methods for other functions, I suppose that the same is true for all analytic functions.","Consider the Maclaurin's series of an analytic function with Lagrange's remainder. Define as where , and if there are several that satisfy this, choose the one closest to (or the infimum if there are infinitely many). Is there an example of such that doesn't converge to ? I'm not sure why, but all the ones I tried seemed to go to zero. Edit: , so, if then . This implies that converges to for some functions such that . From the results of numerical methods for other functions, I suppose that the same is true for all analytic functions.","f \xi_n f(1)=\sum_{k=0}^{n-1}\frac{f^{(k)}(0)}{k!}+\frac{f^{(n)}(\xi_n)}{n!} 0<\xi_n<1 0 f \xi_n 0 \frac{f^{(n)}(\xi_n)}{f^{(n)}(0)}=1+\frac{1}{n+1}\frac{f^{(n+1)}(\xi_{n+1})}{f^{(n)}(0)} \frac{f^{(n+1)}(\xi_{n+1})}{f^{(n)}(0)}=o(n) f^{(n)}(\xi_n)\to f^{(n)}(0) \xi_n 0 e^{ax}, \sin ax","['real-analysis', 'limits', 'taylor-expansion']"
97,Convergence between $\sum |a_n|$ and $\sum a_n z^n$ for all $z\in\mathbb{T}$,Convergence between  and  for all,\sum |a_n| \sum a_n z^n z\in\mathbb{T},"Let $\{a_n\}_{n=0}^{\infty}$ be a sequence of complex numbers. $\mathbb{T}$ is the unit circle over complex plane. It is obvious that if $\sum |a_n|$ converges, then $\sum a_n z^n$ converges for each $z\in \mathbb{T}$ fixed. Is the converse also true? If the answer is “no”, what will happen if we strength the condition by assuming $\sum a_n z^n$ converges pointwise and defined a continuous function on $\mathbb{T}$ ? Here are some observations. 1.Suppose $\sum a_n z^n$ converges pointwise on $\mathbb{T}$ . Denote the limit function by $f$ , then by Baire category theorem, $f$ is continuous except a set of first category. 2.I think this question may have some relations with the following question: find a function in disc algebra $A(\mathbb{D})$ , and there is no analytic continuation of $f$ .","Let be a sequence of complex numbers. is the unit circle over complex plane. It is obvious that if converges, then converges for each fixed. Is the converse also true? If the answer is “no”, what will happen if we strength the condition by assuming converges pointwise and defined a continuous function on ? Here are some observations. 1.Suppose converges pointwise on . Denote the limit function by , then by Baire category theorem, is continuous except a set of first category. 2.I think this question may have some relations with the following question: find a function in disc algebra , and there is no analytic continuation of .",\{a_n\}_{n=0}^{\infty} \mathbb{T} \sum |a_n| \sum a_n z^n z\in \mathbb{T} \sum a_n z^n \mathbb{T} \sum a_n z^n \mathbb{T} f f A(\mathbb{D}) f,"['real-analysis', 'calculus', 'sequences-and-series', 'functional-analysis']"
98,"Why does $S = ([0,1] \times [0,1]) \cap \mathbb{Q}^2$ have no area?",Why does  have no area?,"S = ([0,1] \times [0,1]) \cap \mathbb{Q}^2","I am currently in an Advanced Calculus 2 class and am using the C. H. Edwards ""Advanced Calculus of Several Variables"" text. In chapter 4 when discussing area in $\mathbb{R}^2$ , the text says that the set $S = ([0,1] \times [0,1]) \cap \mathbb{Q}^2$ has no area. The reasoning the book gives is that given any set of non-overlapping rectangles $\{R_i'\}$ for $i=1,\dots,n$ where $R_i' \subseteq S$ for all $i$ , $\sum_{i=1}^na(R_i') = 0$ , and for any set of rectangles $\{R_i''\}$ for $i = 1,\dots, m$ where $S \subseteq \bigcup_{i = 1}^nR_i''$ , $\sum_{i = 1}^ma(R_i'') \geq 1$ . I understand if these statements are true, there can be no area for the set $S$ , but I am having trouble understanding why for any $i$ , $a(R_i') = 0$ , and I also don't understand how $\sum_{i = 1}^ma(R_i'') \geq 1$ . Any help would be appreciated! Clarification: This is the definition of the area of a bounded set $S \subseteq \mathbb{R}^2$ given in our book: Given a bounded set $S \subseteq \mathbb{R}^2$ , we say that its area is $\alpha$ if and only if given $\epsilon > 0$ , there exists both A finite collection $R_1',\dots,R_k'$ of nonoverlapping rectangles, each contained in $S$ , with $\sum_{i=1}^ka(R_i') > \alpha - \epsilon$ A finite collection $R_1'',\dots,R_l''$ of rectangles whose union contain $S$ , with $\sum_{i=1}^la(R_i'') < \alpha + \epsilon$ If there exists no such number $\alpha$ , we say that the set $S$ does not have area, or that its area is not defined.","I am currently in an Advanced Calculus 2 class and am using the C. H. Edwards ""Advanced Calculus of Several Variables"" text. In chapter 4 when discussing area in , the text says that the set has no area. The reasoning the book gives is that given any set of non-overlapping rectangles for where for all , , and for any set of rectangles for where , . I understand if these statements are true, there can be no area for the set , but I am having trouble understanding why for any , , and I also don't understand how . Any help would be appreciated! Clarification: This is the definition of the area of a bounded set given in our book: Given a bounded set , we say that its area is if and only if given , there exists both A finite collection of nonoverlapping rectangles, each contained in , with A finite collection of rectangles whose union contain , with If there exists no such number , we say that the set does not have area, or that its area is not defined.","\mathbb{R}^2 S = ([0,1] \times [0,1]) \cap \mathbb{Q}^2 \{R_i'\} i=1,\dots,n R_i' \subseteq S i \sum_{i=1}^na(R_i') = 0 \{R_i''\} i = 1,\dots, m S \subseteq \bigcup_{i = 1}^nR_i'' \sum_{i = 1}^ma(R_i'') \geq 1 S i a(R_i') = 0 \sum_{i = 1}^ma(R_i'') \geq 1 S \subseteq \mathbb{R}^2 S \subseteq \mathbb{R}^2 \alpha \epsilon > 0 R_1',\dots,R_k' S \sum_{i=1}^ka(R_i') > \alpha - \epsilon R_1'',\dots,R_l'' S \sum_{i=1}^la(R_i'') < \alpha + \epsilon \alpha S",['real-analysis']
99,The inverse type of Bernhard Leeb's solution for IMO‐1983–inequality,The inverse type of Bernhard Leeb's solution for IMO‐1983–inequality,,"Given three side-lengths $a, b, c$ of a triangle. Prove that $$a^{2}b\left ( a- b \right )+ b^{2}c\left ( b- c \right )+ c^{2}a\left ( c- a \right )\geq 3\left ( a+ b- c \right )c\left ( a- b \right )\left ( b- c \right )$$ Source: StackMath/@haidangel ft.@tthnew I used discriminant to create this inequality, also $constant\!:\!=\!\!3$ is the best here. See_ on.StackMath , that's what I'm doing research on, of course this inequality is a result. Not an answer. I think we use Bernhard Leeb's result to help a real lot $$a^{2}b\left ( a- b \right )+ b^{2}c\left ( b- c \right )+ c^{2}a\left ( c- a \right ):= \left ( c+ a- b \right )\left ( c- a \right )^{2}b- \left ( b+ c- a \right )\left ( a- b \right )\left ( b- c \right )c$$","Given three side-lengths of a triangle. Prove that Source: StackMath/@haidangel ft.@tthnew I used discriminant to create this inequality, also is the best here. See_ on.StackMath , that's what I'm doing research on, of course this inequality is a result. Not an answer. I think we use Bernhard Leeb's result to help a real lot","a, b, c a^{2}b\left ( a- b \right )+ b^{2}c\left ( b- c \right )+ c^{2}a\left ( c- a \right )\geq 3\left ( a+ b- c \right )c\left ( a- b \right )\left ( b- c \right ) constant\!:\!=\!\!3 a^{2}b\left ( a- b \right )+ b^{2}c\left ( b- c \right )+ c^{2}a\left ( c- a \right ):= \left ( c+ a- b \right )\left ( c- a \right )^{2}b- \left ( b+ c- a \right )\left ( a- b \right )\left ( b- c \right )c","['real-analysis', 'optimization']"
