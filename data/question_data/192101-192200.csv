,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Example of function that is Gâteaux-differentiable but not Fréchet-differentiable,Example of function that is Gâteaux-differentiable but not Fréchet-differentiable,,"I am looking for an example of a function that is Gateaux-differentiable but not Fréchet-differentiable. I know that there is a lot of example of function $f: \mathbb R^2 \to \mathbb R$ that satisfies this property. An example is $$f(x, y) =  \begin{cases} \frac{x^3}{x^2 + y^2} & \text{if } (x, y) \neq 0,\\ 0 & \text{otherwise.} \end{cases}$$ But since the Fréchet and Gâteaux derivative are defined for Banach spaces in general, I am looking for a more fancy example such as a function from $L^p$ to $L^q$ for instance. Any idea ?","I am looking for an example of a function that is Gateaux-differentiable but not Fréchet-differentiable. I know that there is a lot of example of function that satisfies this property. An example is But since the Fréchet and Gâteaux derivative are defined for Banach spaces in general, I am looking for a more fancy example such as a function from to for instance. Any idea ?","f: \mathbb R^2 \to \mathbb R f(x, y) = 
\begin{cases}
\frac{x^3}{x^2 + y^2} & \text{if } (x, y) \neq 0,\\
0 & \text{otherwise.}
\end{cases} L^p L^q","['derivatives', 'banach-spaces', 'frechet-derivative', 'gateaux-derivative']"
1,Last step in the proof of second derivative test,Last step in the proof of second derivative test,,"I came across a problem studying the proof of the Second Derivative Test theorem from Spivak's Calculus (Chapter 11, Theorem 5, p199, 3rd edition): Suppose $\operatorname{f}^\prime(a) = 0$ . If $\operatorname{f}^{\prime\prime}(a) > 0$ , then $\operatorname{f}$ has a local minimum at $a$ . I'm able to follow the proof until the very last point, where the author concludes after observing the sign of $\operatorname{f}^\prime$ around $a$ , that since $\operatorname{f}$ is increasing in some interval to the right of $a$ and decreasing in some interval to the left of $a$ , [...] f has a local minimum at a. The way I understood this, is that $\operatorname{f}$ decreases on $(a-h, a)$ and increases on $(a, a+h)$ for some $h > 0$ , but there is nothing said about the point $a$ itself. I managed to come up with two proofs, none of them seems straightforward enough just to be omitted from the book, so I have the impression that I'm missing a point. For any $b \in (a-\delta, a)$ consider $c = \frac{b + a}2$ . $\operatorname{f}$ is decreasing, so $ \epsilon = \frac{\operatorname{f}(b) -\operatorname{f}(c)}{2} > 0 $ . From the (left-)continuity of $\operatorname{f}$ at $a$ for this $\epsilon$ there is some $\delta > 0$ such that $\forall x: 0 \le a - x \lt \delta : \left| \operatorname{f}(x) - \operatorname{f}(a) \right| < \epsilon \Leftrightarrow \operatorname{f}(x) - \epsilon < \operatorname{f}(a) < \operatorname{f}(x) + \epsilon $ . For any $x$ that satisfies both $0 \le a - x \lt \delta \text{ and } c < x $ , we get from the previous inequality together with the fact that f is decreasing: $\operatorname{f}(a) < \operatorname{f}(x) + \epsilon < \operatorname{f}(c) + \epsilon < \operatorname{f}(c) + \frac{\operatorname{f}(b) -\operatorname{f}(c)}{2} < \frac{\operatorname{f}(b) +\operatorname{f}(c)}{2} < \operatorname{f}(b)$ . The proof that $\operatorname{f}(a) < \operatorname{f}(b)$ for all $b \in (a, a+\delta)$ is similar. I know, that if $\operatorname{f}$ was continuous on some $(a-\delta, a+\delta)$ interval, then there would be a closed interval inside this open interval (e.g $ [a-\frac{\delta}{2}, a+\frac{\delta}{2}]$ ), where f takes on its minimum value at some point $b$ . By way of contradiction we can see that $b = a$ , otherwise $\operatorname{f}(\frac{a + b}{2}) < \operatorname{f}(b)$ would contradict the conclusion that $b$ is a minimum place. Since $\exists \lim_{h\to 0} \frac{\operatorname{f}^\prime(a + h) - \operatorname{f}^\prime(a)}{h}  = \operatorname{f}^{\prime\prime}(a)$ , $\operatorname{f}^\prime(x)$ must exist $\forall x \in (a-\delta, a+\delta)$ for some small $\delta > 0$ , thus f is continuous on $(a-\delta, a+\delta)$ , thus the first part of this proof holds. My questions: Are these proofs correct? Is there a more direct way to conclude from the last part of the proof provided in the book that $\operatorname{f}$ has a local minimum at $a$ ?","I came across a problem studying the proof of the Second Derivative Test theorem from Spivak's Calculus (Chapter 11, Theorem 5, p199, 3rd edition): Suppose . If , then has a local minimum at . I'm able to follow the proof until the very last point, where the author concludes after observing the sign of around , that since is increasing in some interval to the right of and decreasing in some interval to the left of , [...] f has a local minimum at a. The way I understood this, is that decreases on and increases on for some , but there is nothing said about the point itself. I managed to come up with two proofs, none of them seems straightforward enough just to be omitted from the book, so I have the impression that I'm missing a point. For any consider . is decreasing, so . From the (left-)continuity of at for this there is some such that . For any that satisfies both , we get from the previous inequality together with the fact that f is decreasing: . The proof that for all is similar. I know, that if was continuous on some interval, then there would be a closed interval inside this open interval (e.g ), where f takes on its minimum value at some point . By way of contradiction we can see that , otherwise would contradict the conclusion that is a minimum place. Since , must exist for some small , thus f is continuous on , thus the first part of this proof holds. My questions: Are these proofs correct? Is there a more direct way to conclude from the last part of the proof provided in the book that has a local minimum at ?","\operatorname{f}^\prime(a) = 0 \operatorname{f}^{\prime\prime}(a) > 0 \operatorname{f} a \operatorname{f}^\prime a \operatorname{f} a a \operatorname{f} (a-h, a) (a, a+h) h > 0 a b \in (a-\delta, a) c = \frac{b + a}2 \operatorname{f}  \epsilon = \frac{\operatorname{f}(b) -\operatorname{f}(c)}{2} > 0  \operatorname{f} a \epsilon \delta > 0 \forall x: 0 \le a - x \lt \delta : \left| \operatorname{f}(x) - \operatorname{f}(a) \right| < \epsilon \Leftrightarrow \operatorname{f}(x) - \epsilon < \operatorname{f}(a) < \operatorname{f}(x) + \epsilon  x 0 \le a - x \lt \delta \text{ and } c < x  \operatorname{f}(a) < \operatorname{f}(x) + \epsilon < \operatorname{f}(c) + \epsilon < \operatorname{f}(c) + \frac{\operatorname{f}(b) -\operatorname{f}(c)}{2} < \frac{\operatorname{f}(b) +\operatorname{f}(c)}{2} < \operatorname{f}(b) \operatorname{f}(a) < \operatorname{f}(b) b \in (a, a+\delta) \operatorname{f} (a-\delta, a+\delta)  [a-\frac{\delta}{2}, a+\frac{\delta}{2}] b b = a \operatorname{f}(\frac{a + b}{2}) < \operatorname{f}(b) b \exists \lim_{h\to 0} \frac{\operatorname{f}^\prime(a + h) - \operatorname{f}^\prime(a)}{h}  = \operatorname{f}^{\prime\prime}(a) \operatorname{f}^\prime(x) \forall x \in (a-\delta, a+\delta) \delta > 0 (a-\delta, a+\delta) \operatorname{f} a","['real-analysis', 'derivatives', 'continuity']"
2,Show that $f''(\xi)≥8$,Show that,f''(\xi)≥8,"Let $f:[0,1]\rightarrow\mathbb{R}$ be a $\mathcal{C}^{2}([0,1])$ function such that $f(0)=f(1)=1$ and such that the minimum of $f$ is negative. Prove that there must exist a point $\xi\in [0,1]$ such that $f''(\xi)≥8$ Maybe I can use the fact that there exists maximum and minimum in $[0,1]$ so there are two critical points. But I don't know how to proceed. Can someone give me any advice? Thanks before!",Let be a function such that and such that the minimum of is negative. Prove that there must exist a point such that Maybe I can use the fact that there exists maximum and minimum in so there are two critical points. But I don't know how to proceed. Can someone give me any advice? Thanks before!,"f:[0,1]\rightarrow\mathbb{R} \mathcal{C}^{2}([0,1]) f(0)=f(1)=1 f \xi\in [0,1] f''(\xi)≥8 [0,1]","['real-analysis', 'derivatives', 'maxima-minima']"
3,Find all the points where a function is not differentiable,Find all the points where a function is not differentiable,,"Let $f\left( x \right)$ be a differentiable function $\mathbb{R}\to \mathbb{R}$ . Find all the points where the function $\left| f\left( x \right) \right|$ is not differentiable. Of course, I understand that $\left| f\left( x \right) \right|$ is not differentiable where its graph has a corner because of the modulus, e.g., ${{\delta }_{+}}f\left( a \right)+{{\delta }_{-}}f\left( a \right)=0$ . Obviously, it is only possible where $f\left( x \right)$ changes its sign since $f\left( x \right)$ is be a differentiable function. Is there any way to write this informal idea in a more formal manner?","Let be a differentiable function . Find all the points where the function is not differentiable. Of course, I understand that is not differentiable where its graph has a corner because of the modulus, e.g., . Obviously, it is only possible where changes its sign since is be a differentiable function. Is there any way to write this informal idea in a more formal manner?",f\left( x \right) \mathbb{R}\to \mathbb{R} \left| f\left( x \right) \right| \left| f\left( x \right) \right| {{\delta }_{+}}f\left( a \right)+{{\delta }_{-}}f\left( a \right)=0 f\left( x \right) f\left( x \right),"['real-analysis', 'derivatives']"
4,Non-standard five-point formula for second derivative used in Tracker,Non-standard five-point formula for second derivative used in Tracker,,"Tracker is an open-source program used to analyze object trajectories from video. The typical data sets Tracker produces are time series of object positions at various times, i.e., data pairs (t_i,x_i) for $i=1$ to $N$ . The time samples are assumed to have uniform time step $t_{i}-t_{i-1}=\Delta t$ . Tracker then uses this data to generates estimates for the first and second derivatives, corresponding to the velocity $v=dx/dt$ and acceleration $a=d^2x/dt^2$ respectively. By default, it does this using the following finite difference schemes: $$v_i = \frac{x_{i+1}-x_{i-1}}{\Delta t},\qquad  a_i = \frac{1}{7(\Delta t)^2}(2x_{i+2} - x_{i+1} - 2x_i - x_{i-1} + 2x_{i-2}) $$ The first is a standard two-point formula which requires no comment. However, the standard 5-point formula for the second derivative is $$f''(x)\approx \frac{-f(x+2h)+16 f(x+h)-30 f(x)+16f(x-h)-f(x-2h)}{12h^2} \tag{1}$$ which has error of order $O(h^4)$ . (Note that this formula is exact for quartic polynomials but not quintic.) The formula used by Tracker, by contrast, would correspond to $$f''(x)\approx \frac{2f(x+2h)- f(x+h)-2 f(x)-f(x-h)+2f(x-2h)}{12h^2}. \tag{2}$$ Both sides do match in the limit $h\to 0$ , but the error is instead $O(h^2)$ . So Tracker uses a scheme which, on the face of it, is not as precise. Tracker's documentation doesn't address this point in detail, but does state the following: ""Note: there are many other finite difference algorithms. Tracker's algorithms define the velocity for a step to be the average velocity over a 2-step interval, and the acceleration to be the second derivative of a parabolic fit over a 4-step interval, with the step at the center. Tracker's acceleration algorithm is less sensitive to position uncertainties than others."" The last sentence is especially notable. So my question as a whole is: What's the precise motivation behind Tracker's use of approximation (2), and in what sense (if any) is it ""less sensitive to position uncertainties?""","Tracker is an open-source program used to analyze object trajectories from video. The typical data sets Tracker produces are time series of object positions at various times, i.e., data pairs (t_i,x_i) for to . The time samples are assumed to have uniform time step . Tracker then uses this data to generates estimates for the first and second derivatives, corresponding to the velocity and acceleration respectively. By default, it does this using the following finite difference schemes: The first is a standard two-point formula which requires no comment. However, the standard 5-point formula for the second derivative is which has error of order . (Note that this formula is exact for quartic polynomials but not quintic.) The formula used by Tracker, by contrast, would correspond to Both sides do match in the limit , but the error is instead . So Tracker uses a scheme which, on the face of it, is not as precise. Tracker's documentation doesn't address this point in detail, but does state the following: ""Note: there are many other finite difference algorithms. Tracker's algorithms define the velocity for a step to be the average velocity over a 2-step interval, and the acceleration to be the second derivative of a parabolic fit over a 4-step interval, with the step at the center. Tracker's acceleration algorithm is less sensitive to position uncertainties than others."" The last sentence is especially notable. So my question as a whole is: What's the precise motivation behind Tracker's use of approximation (2), and in what sense (if any) is it ""less sensitive to position uncertainties?""","i=1 N t_{i}-t_{i-1}=\Delta t v=dx/dt a=d^2x/dt^2 v_i = \frac{x_{i+1}-x_{i-1}}{\Delta t},\qquad  a_i = \frac{1}{7(\Delta t)^2}(2x_{i+2} - x_{i+1} - 2x_i - x_{i-1} + 2x_{i-2})  f''(x)\approx \frac{-f(x+2h)+16 f(x+h)-30 f(x)+16f(x-h)-f(x-2h)}{12h^2} \tag{1} O(h^4) f''(x)\approx \frac{2f(x+2h)- f(x+h)-2 f(x)-f(x-h)+2f(x-2h)}{12h^2}. \tag{2} h\to 0 O(h^2)","['calculus', 'derivatives', 'numerical-methods', 'approximation']"
5,Suppose $g(u) = \int_{2}^{u} \sqrt{1 + t^3} \ dt$. Find the value of $(g^{-1})'(0)$ if it exists.,Suppose . Find the value of  if it exists.,g(u) = \int_{2}^{u} \sqrt{1 + t^3} \ dt (g^{-1})'(0),"I want some verification to see if I am doing this question right. Here is the question, and my attempt: Suppose $g(u) = \int_{2}^{u} \sqrt{1 + t^3} \ dt$ . Find the value of $(g^{-1})'(0)$ if it exists. So first I found the derivative of $g(u)$ , which was $$g'(u) = \sqrt{1 + u^3}$$ Then I let $v = g'(u)$ . The domain of $g'$ is $u\in[-1, \infty)$ and the range of $g'$ is $v\in[0,\infty)$ . Now solve for $u$ , so \begin{align*} v &= \sqrt{1 + u^3} \\ v^2 &= 1 + u^3 \\ u^3 &= v^2 - 1 \\ u &= \sqrt[3]{v^2 - 1} \end{align*} So the inverse is $u = \sqrt[3]{v^2 - 1}$ , or $(g^{-1})'(v) = \sqrt[3]{v^2 - 1}$ . The domain of $(g^{-1})'$ is $v\in[0,\infty)$ , and the range for $(g^{-1})'$ is $u\in[-1,\infty)$ . Now find $(g^{-1})'(0)$ , so sub $v = 0$ . \begin{align*} (g^{-1})'(v) &= \sqrt[3]{v^2 - 1} \\ (g^{-1})'(0) &= \sqrt[3]{0^2 - 1} \\ (g^{-1})'(0) &= \sqrt[3]{-1} \\ (g^{-1})'(0) &= -1 \\ \end{align*} Since the value $-1$ is within the range, this value exists. Therefore, the value of $(g^-1)'(0)$ is $-1$ . This is my attempt, not sure if it is correct...Would like some suggestions or advice, if it is off somewhere.","I want some verification to see if I am doing this question right. Here is the question, and my attempt: Suppose . Find the value of if it exists. So first I found the derivative of , which was Then I let . The domain of is and the range of is . Now solve for , so So the inverse is , or . The domain of is , and the range for is . Now find , so sub . Since the value is within the range, this value exists. Therefore, the value of is . This is my attempt, not sure if it is correct...Would like some suggestions or advice, if it is off somewhere.","g(u) = \int_{2}^{u} \sqrt{1 + t^3} \ dt (g^{-1})'(0) g(u) g'(u) = \sqrt{1 + u^3} v = g'(u) g' u\in[-1, \infty) g' v\in[0,\infty) u \begin{align*}
v &= \sqrt{1 + u^3} \\
v^2 &= 1 + u^3 \\
u^3 &= v^2 - 1 \\
u &= \sqrt[3]{v^2 - 1}
\end{align*} u = \sqrt[3]{v^2 - 1} (g^{-1})'(v) = \sqrt[3]{v^2 - 1} (g^{-1})' v\in[0,\infty) (g^{-1})' u\in[-1,\infty) (g^{-1})'(0) v = 0 \begin{align*}
(g^{-1})'(v) &= \sqrt[3]{v^2 - 1} \\
(g^{-1})'(0) &= \sqrt[3]{0^2 - 1} \\
(g^{-1})'(0) &= \sqrt[3]{-1} \\
(g^{-1})'(0) &= -1 \\
\end{align*} -1 (g^-1)'(0) -1","['calculus', 'integration', 'derivatives', 'inverse-function']"
6,Find points at which tangent intersects curve for variable x and y values,Find points at which tangent intersects curve for variable x and y values,,"I am not sure if I titled this correctly. I am working through Calculus With Analytic Geometry by Simmons. Question $\# 17$ on page $87$ reads Find the equation of the tangent to the curve $y = x^3$ at the point $(a, a^3)$ . For what values of a does this tangent intersect the curve at another point? I can calculate the equation of the tangent line to be $y=3a^2x-2a^3$ but I am not sure how to calculate the values of $a$ where the tangent intersects the curve at another point. Even a hint would be great, thank you.","I am not sure if I titled this correctly. I am working through Calculus With Analytic Geometry by Simmons. Question on page reads Find the equation of the tangent to the curve at the point . For what values of a does this tangent intersect the curve at another point? I can calculate the equation of the tangent line to be but I am not sure how to calculate the values of where the tangent intersects the curve at another point. Even a hint would be great, thank you.","\# 17 87 y = x^3 (a, a^3) y=3a^2x-2a^3 a","['calculus', 'derivatives', 'tangent-line']"
7,How to prove the existence of exactly one root of a function in between two consecutive roots of another function,How to prove the existence of exactly one root of a function in between two consecutive roots of another function,,"I have the following question before me: $f$ and $g$ are two functions derivable in $(a,b)$ such that $f(x) g'(x)-f'(x)g(x)>0$ for all $x$ in $(a,b)$ then prove that there lies exactly one root of $g(x)=0$ in between two consecutive roots of $f(x)=0$ . I assumed the existence of two roots of $g(x)=0$ in between two consecutive roots of $f(x)=0$ in order to reach a contradiction. Here is how I attempted: Let $c$ and $d$ be the two consecutive roots of $f(x)=0$ . Now for all $x$ in between $c$ and $d$ , $f(x)$ is non-zero. Consider the function $g(x)/f(x)$ in the interval $(c,d)$ . The derivative of this function is $ (f(x)g'(x)-g(x)f'(x))/((f(x))^2$ . Due to the given condition, this derivative is always positive which implies that the function is strictly increasing in nature and thus can't have equal values at any two different points in its domain. Now I consider the existence of two consecutive roots of $g(x)=0$ in $(c,d)$ which makes the value of the function $g(x)/f(x)$ $0$ at two different points which is impermissible. Thus I am able to arrive at a contradiction. From here I can conclude that there lies at most one root of $g(x)=0$ in between two consecutive roots of $f(x)=0$ . But the question asks me to show the existence of exactly one such root. How do I proceed from here to prove this? Please suggest.","I have the following question before me: and are two functions derivable in such that for all in then prove that there lies exactly one root of in between two consecutive roots of . I assumed the existence of two roots of in between two consecutive roots of in order to reach a contradiction. Here is how I attempted: Let and be the two consecutive roots of . Now for all in between and , is non-zero. Consider the function in the interval . The derivative of this function is . Due to the given condition, this derivative is always positive which implies that the function is strictly increasing in nature and thus can't have equal values at any two different points in its domain. Now I consider the existence of two consecutive roots of in which makes the value of the function at two different points which is impermissible. Thus I am able to arrive at a contradiction. From here I can conclude that there lies at most one root of in between two consecutive roots of . But the question asks me to show the existence of exactly one such root. How do I proceed from here to prove this? Please suggest.","f g (a,b) f(x) g'(x)-f'(x)g(x)>0 x (a,b) g(x)=0 f(x)=0 g(x)=0 f(x)=0 c d f(x)=0 x c d f(x) g(x)/f(x) (c,d)  (f(x)g'(x)-g(x)f'(x))/((f(x))^2 g(x)=0 (c,d) g(x)/f(x) 0 g(x)=0 f(x)=0","['real-analysis', 'derivatives', 'rolles-theorem']"
8,Why do we take the derivative as zero in this question?,Why do we take the derivative as zero in this question?,,"Today, I was solving this KVPY-SA 2017 Question : In an isosceles trapezium, the length of one of the parallel sides, and the lengths of the non-parallel sides are all equal to $30$ . In order to maximize the area of the trapezium, the smallest angle should be: A) $π/6$ B) $π/4$ C) $π/3$ D) $π/2$ So, I took the angle as $θ$ and after that I found the Area by using the $sin$ and $cos$ functions like this: $$A=900\sin\theta(\cos\theta + 1)$$ And after that I found the derivative of area like this: $$\begin{align*}\frac{dA}{d\theta} &= \frac{d}{d\theta}[900(\sin\theta)(1+\cos\theta)]\\ &= 900\left[(\sin\theta)\frac{d}{d\theta}(1+\cos\theta)+(1+\cos\theta)\frac{d}{d\theta}(\sin\theta)\right]\end{align*}$$ And after many steps I reached here where I put the derivative as equal to $0$ : $$\frac{dA}{d\theta} =900[\sin\theta\cdot(-\sin\theta)+(1+\cos\theta)\cdot(\cos\theta)]=0$$ I don't know why I should put it equal to $0$ but that's how you find the answer. When I searched for answers on google I found this solved paper by Byju's. In their answer, you would see that they've written: Differentiating both sides with respect to $\theta$ , $$\frac{dA}{d\theta} = 900[{\sin\theta\cdot(–\sin \theta) +(1 + \cos\theta)\cdot\cos\theta }] = 0\text{ (for critical points)}$$ What did they mean by differentiation = $0$ for critical points? You know, I'm very bad at calculus so please forgive me if this is a dumb question. Edit: What I did after putting the derivative= $0$ :- $$-sin²\theta+cos\theta+cos²\theta=0$$ $$⇒cos²\theta-1+cos\theta+cos ²\theta=0$$ $$⇒2cos²\theta+cos\theta-1=0$$ $$⇒2cos²\theta+2cos\theta-cos\theta-1=0$$ $$⇒2cos\theta(cos\theta+1)-1(cos\theta+1)=0$$ $$⇒(2cos\theta-1)(cos\theta+1)=0$$","Today, I was solving this KVPY-SA 2017 Question : In an isosceles trapezium, the length of one of the parallel sides, and the lengths of the non-parallel sides are all equal to . In order to maximize the area of the trapezium, the smallest angle should be: A) B) C) D) So, I took the angle as and after that I found the Area by using the and functions like this: And after that I found the derivative of area like this: And after many steps I reached here where I put the derivative as equal to : I don't know why I should put it equal to but that's how you find the answer. When I searched for answers on google I found this solved paper by Byju's. In their answer, you would see that they've written: Differentiating both sides with respect to , What did they mean by differentiation = for critical points? You know, I'm very bad at calculus so please forgive me if this is a dumb question. Edit: What I did after putting the derivative= :-","30 π/6 π/4 π/3 π/2 θ sin cos A=900\sin\theta(\cos\theta + 1) \begin{align*}\frac{dA}{d\theta} &= \frac{d}{d\theta}[900(\sin\theta)(1+\cos\theta)]\\
&= 900\left[(\sin\theta)\frac{d}{d\theta}(1+\cos\theta)+(1+\cos\theta)\frac{d}{d\theta}(\sin\theta)\right]\end{align*} 0 \frac{dA}{d\theta} =900[\sin\theta\cdot(-\sin\theta)+(1+\cos\theta)\cdot(\cos\theta)]=0 0 \theta \frac{dA}{d\theta} = 900[{\sin\theta\cdot(–\sin \theta) +(1 + \cos\theta)\cdot\cos\theta }] = 0\text{ (for critical points)} 0 0 -sin²\theta+cos\theta+cos²\theta=0 ⇒cos²\theta-1+cos\theta+cos ²\theta=0 ⇒2cos²\theta+cos\theta-1=0 ⇒2cos²\theta+2cos\theta-cos\theta-1=0 ⇒2cos\theta(cos\theta+1)-1(cos\theta+1)=0 ⇒(2cos\theta-1)(cos\theta+1)=0","['calculus', 'derivatives']"
9,Solution verification: Derivative of the infinite power tower $y(x) = x^{x^⋰}$,Solution verification: Derivative of the infinite power tower,y(x) = x^{x^⋰},I was doing the problem $(x^{x^{⋰}})'$ and I would like someone to verify my solution: \begin{align*} &\left(y=x^{x^{^{⋰}}}\right)'\\ \implies & \;\left(y=x^{y}\right)'\\ \implies & \;\left(y^{\frac{1}{y}}=x\right)'\\ \implies & \;\left(\frac{\ln\left(y\right)}{y}=\ln\left(x\right)\right)'\\ \implies & \;\frac{y'-y'\ln\left(y\right)}{y^{2}}=\frac{1}{x}\\ \implies & \; y'-y'\ln\left(y\right)=\frac{y^{2}}{x}\\ \implies & \; y'=\frac{y^{2}}{\left(1-\ln\left(y\right)\right)x}\\ \implies & \; \left(x^{x^{⋰}}\right)'=\frac{\left(x^{x^{⋰}}\right)^{2}}{\left(1-\ln\left(x^{x^{⋰}}\right)\right)x}  \end{align*},I was doing the problem and I would like someone to verify my solution:,"(x^{x^{⋰}})' \begin{align*}
&\left(y=x^{x^{^{⋰}}}\right)'\\
\implies & \;\left(y=x^{y}\right)'\\
\implies & \;\left(y^{\frac{1}{y}}=x\right)'\\
\implies & \;\left(\frac{\ln\left(y\right)}{y}=\ln\left(x\right)\right)'\\
\implies & \;\frac{y'-y'\ln\left(y\right)}{y^{2}}=\frac{1}{x}\\
\implies & \; y'-y'\ln\left(y\right)=\frac{y^{2}}{x}\\
\implies & \; y'=\frac{y^{2}}{\left(1-\ln\left(y\right)\right)x}\\
\implies & \; \left(x^{x^{⋰}}\right)'=\frac{\left(x^{x^{⋰}}\right)^{2}}{\left(1-\ln\left(x^{x^{⋰}}\right)\right)x} 
\end{align*}","['calculus', 'derivatives', 'solution-verification', 'power-towers']"
10,Infimum of right derivative and infimum of left derivative are equal?,Infimum of right derivative and infimum of left derivative are equal?,,"Suppose we have continuous function $f: [a,b] \to \mathbb{R}$ with right and left derivatives on $(a,b)$ . So would $$m_-=\inf\{f_+'(x):x \in (a,b)\}= \inf\{f_-'(x):x \in (a,b)\}=m_+ $$ take place? I tried to prove by contradiction. Suppose $m_-<m_+$ . Then exists $x_0 \in (a,b)$ , such that $f'_-(x_0)<0.5(m_++m_-)$ . Then if we take $\epsilon=0.25(m_+-m_-)$ , then exists $\delta > 0$ such that if $0<x_0-x<\delta$ then $|\frac{f(x)-f(x_0)}{x-x_0}-f'_-(x_0)|<\epsilon$ . Then I choose $y \in (x_0-\delta; x_0)$ . We have $$\frac{f(y)-f(x_0)}{y-x_0}<\epsilon+ f'_-(x_0)<0.75m_++0.25m_-$$ Then if $x_0 \to y+0$ , it appears that $f'_+(y)<m_+$ , which is contradiction. I don't know if it's correct, so I would be glad if you point me in the right direction.","Suppose we have continuous function with right and left derivatives on . So would take place? I tried to prove by contradiction. Suppose . Then exists , such that . Then if we take , then exists such that if then . Then I choose . We have Then if , it appears that , which is contradiction. I don't know if it's correct, so I would be glad if you point me in the right direction.","f: [a,b] \to \mathbb{R} (a,b) m_-=\inf\{f_+'(x):x \in (a,b)\}= \inf\{f_-'(x):x \in (a,b)\}=m_+  m_-<m_+ x_0 \in (a,b) f'_-(x_0)<0.5(m_++m_-) \epsilon=0.25(m_+-m_-) \delta > 0 0<x_0-x<\delta |\frac{f(x)-f(x_0)}{x-x_0}-f'_-(x_0)|<\epsilon y \in (x_0-\delta; x_0) \frac{f(y)-f(x_0)}{y-x_0}<\epsilon+ f'_-(x_0)<0.75m_++0.25m_- x_0 \to y+0 f'_+(y)<m_+","['real-analysis', 'derivatives', 'supremum-and-infimum']"
11,On locating inflection points,On locating inflection points,,"From what I have learnt, a point of inflection of a curve is, by definition, a point where the curve changes concavity. The Simple Case Thus, if, for a point, $c$ , on a given function, $f(x)$ , $f'(c) = f''(c) = 0$ and $f'''(c) \neq 0$ , then $c$ is a point of inflection. I believe I understand the explanation for this, as, by definition, the second derivative describes concavity, so the third would necessarily describe the rate of change of concavity. Then, since $f'(c) = f''(c) = 0$ and $f'''(c) \neq 0$ , we can conclude that the rate of change of the second derivative is non-zero, so concavity is changing and $c$ is a point of inflection. Please feel free to correct me if my explanation for this is wrong! The General Case However, on doing a little more research, I found out that this phenomenon can actually be generalised as follows: If $f(x)$ is $k$ times continuously differentiable in a certain neighborhood of a point $x$ with $k$ odd and $k ≥ 3$ , while $f^{(n)}(x_0) = 0$ for $n = 2, …, k − 1$ and $f^{(k)}(x_0) ≠ 0$ , then $f(x)$ has a point of inflection at $x_0$ . Questions I do not understand how to explain this, since I thought that only the third derivative (and not other higher-order derivatives) describes rate of change of concavity, so I have the following four questions: How do we generalise my observation about the feature of the third derivative to any odd-numbered derivative (below the second)? Why does this generalisation only apply to odd-numbered derivatives (below the second)? In other words, why does it not apply to even-numbered derivatives (below the second)? I also know that there can be inflection points where the second derivative is undefined. How, then, do we confirm that there is an inflection point there? Is the fact that the second derivative is undefined a sufficient condition? As an extension to my third question, what if the second derivative is defined and equals zero at the particular point, but the third derivative is undefined? How, then, do we confirm that there is an inflection point there? Background Perhaps I might add that I am currently taking a introductory module in calculus at university level, so my level of knowledge about calculus at present may not be in-depth enough to understand the sophisticated explanations that I expect would be coming my way. I have learnt IVT, EVT, Rolle's Theorem, MVT, Cauchy's MVT and L'Hopital's Rule, but that is about it as far as theorems are concerned, so I would greatly appreciate it if there are any intuitive/""lower-level"" explanations to this :)","From what I have learnt, a point of inflection of a curve is, by definition, a point where the curve changes concavity. The Simple Case Thus, if, for a point, , on a given function, , and , then is a point of inflection. I believe I understand the explanation for this, as, by definition, the second derivative describes concavity, so the third would necessarily describe the rate of change of concavity. Then, since and , we can conclude that the rate of change of the second derivative is non-zero, so concavity is changing and is a point of inflection. Please feel free to correct me if my explanation for this is wrong! The General Case However, on doing a little more research, I found out that this phenomenon can actually be generalised as follows: If is times continuously differentiable in a certain neighborhood of a point with odd and , while for and , then has a point of inflection at . Questions I do not understand how to explain this, since I thought that only the third derivative (and not other higher-order derivatives) describes rate of change of concavity, so I have the following four questions: How do we generalise my observation about the feature of the third derivative to any odd-numbered derivative (below the second)? Why does this generalisation only apply to odd-numbered derivatives (below the second)? In other words, why does it not apply to even-numbered derivatives (below the second)? I also know that there can be inflection points where the second derivative is undefined. How, then, do we confirm that there is an inflection point there? Is the fact that the second derivative is undefined a sufficient condition? As an extension to my third question, what if the second derivative is defined and equals zero at the particular point, but the third derivative is undefined? How, then, do we confirm that there is an inflection point there? Background Perhaps I might add that I am currently taking a introductory module in calculus at university level, so my level of knowledge about calculus at present may not be in-depth enough to understand the sophisticated explanations that I expect would be coming my way. I have learnt IVT, EVT, Rolle's Theorem, MVT, Cauchy's MVT and L'Hopital's Rule, but that is about it as far as theorems are concerned, so I would greatly appreciate it if there are any intuitive/""lower-level"" explanations to this :)","c f(x) f'(c) = f''(c) = 0 f'''(c) \neq 0 c f'(c) = f''(c) = 0 f'''(c) \neq 0 c f(x) k x k k ≥ 3 f^{(n)}(x_0) = 0 n = 2, …, k − 1 f^{(k)}(x_0) ≠ 0 f(x) x_0","['real-analysis', 'calculus', 'derivatives', 'stationary-point']"
12,Bound on difference of derivatives of convex functions,Bound on difference of derivatives of convex functions,,"Let $f:[0, \infty) \to \mathbb{R}$ be a convex function. Let us assume further that $f(0) = 0, f'(x) \geq 0$ and that for every $x > 0$ $$|f(x) -x^2| \leq \varepsilon,$$ for some $\varepsilon > 0$ . Can we uniformly bound $$\sup_{x\geq 0}|f'(x)-2x|$$ in terms of $\varepsilon?$ If this is not possible? what can be said about $$\sup_{T\geq x\geq 0}|f'(x)-2x|,$$ for some fixed $T$ . Note that uniformly one cannot deduce uniform bounds on derivatives from uniform bounds on the functions. But I'm hoping that convexity can help here.",Let be a convex function. Let us assume further that and that for every for some . Can we uniformly bound in terms of If this is not possible? what can be said about for some fixed . Note that uniformly one cannot deduce uniform bounds on derivatives from uniform bounds on the functions. But I'm hoping that convexity can help here.,"f:[0, \infty) \to \mathbb{R} f(0) = 0, f'(x) \geq 0 x > 0 |f(x) -x^2| \leq \varepsilon, \varepsilon > 0 \sup_{x\geq 0}|f'(x)-2x| \varepsilon? \sup_{T\geq x\geq 0}|f'(x)-2x|, T","['derivatives', 'inequality', 'convex-analysis', 'uniform-convergence']"
13,"Given two real numbers $x,y$ so that $x^{2}+y^{2}+xy+4=4y+3x$. Prove that $3\left(x^{3}-y^{3}\right)+20x^{2}+2xy+5y^{2}+39x\leq 100$.",Given two real numbers  so that . Prove that .,"x,y x^{2}+y^{2}+xy+4=4y+3x 3\left(x^{3}-y^{3}\right)+20x^{2}+2xy+5y^{2}+39x\leq 100","Given two real numbers $x, y$ so that $x^{2}+ y^{2}+ xy+ 4= 4y+ 3x$ . Prove that $$3\left ( x^{3}- y^{3} \right )+ 20x^{2}+ 2xy+ 5y^{2}+ 39x\leq 100$$ I used derivative and Wolfram|Alpha but only the minimum value found $$\min\{3\left ( x^{3}- y^{3} \right )+ 20x^{2}+ 2xy+ 5y^{2}+ 39x\}\Leftrightarrow \left\{\begin{matrix} x\cong 0.0320241\\ y\cong 2.16078\\ \left ( z\cong 2.19235 \right ) \end{matrix}\right.$$ where $z$ is a root of $472z^{3}- 449z^{2}- 689z- 1305= 0$ , $y$ is a root of $27468y- 11800z^{2}+ 17833z- 41733= 0$ , $x$ is a root of $27468x+ 3304z^{2}- 11167z+ 7722= 0$ . Why is unsuccessful ? Here are two examples of my claim .","Given two real numbers so that . Prove that I used derivative and Wolfram|Alpha but only the minimum value found where is a root of , is a root of , is a root of . Why is unsuccessful ? Here are two examples of my claim .","x, y x^{2}+ y^{2}+ xy+ 4= 4y+ 3x 3\left ( x^{3}- y^{3} \right )+ 20x^{2}+ 2xy+ 5y^{2}+ 39x\leq 100 \min\{3\left ( x^{3}- y^{3} \right )+ 20x^{2}+ 2xy+ 5y^{2}+ 39x\}\Leftrightarrow \left\{\begin{matrix} x\cong 0.0320241\\ y\cong 2.16078\\ \left ( z\cong 2.19235 \right ) \end{matrix}\right. z 472z^{3}- 449z^{2}- 689z- 1305= 0 y 27468y- 11800z^{2}+ 17833z- 41733= 0 x 27468x+ 3304z^{2}- 11167z+ 7722= 0","['derivatives', 'inequality']"
14,"How to prove that $F(x) = \int_0^x f(t)$ is differentiable at $0$. Here $f(t)$ is continuous on $[-2 , 2]$.",How to prove that  is differentiable at . Here  is continuous on .,"F(x) = \int_0^x f(t) 0 f(t) [-2 , 2]","How to prove that $F(x) = \int_0^x f(t)$ is differentiable at $0$ .  Here $f(t)$ is continuous on $[-2 , 2]$ . My Attempt : For a positive number $e$ , we will get a $\delta > 0$ such that $ -e < f(t) - f(0) < e$ for all $t \in (-\delta , \delta).$ Now $\int_0^h (-e) < \int_0^h (f(t) - f(0)) < \int_0^h e \implies   -e < \frac{F(h) - F(0)}{h} - f(0)<  e $ for all $h \in (0 , \delta). $ So we can say $F_+'(0) = f(0)$ Now $\int_0^h (e) < \int_0^h (f(t) - f(0)) < \int_0^h -e \implies   -e < \frac{F(h) - F(0)}{h} - f(0) <  e $ for all $h \in (- \delta , 0). $ So we can say $F_-'(0) = f(0)$ $\therefore$ $F'(0) = f(0)$ Can anyone please check my attempt?","How to prove that is differentiable at .  Here is continuous on . My Attempt : For a positive number , we will get a such that for all Now for all So we can say Now for all So we can say Can anyone please check my attempt?","F(x) = \int_0^x f(t) 0 f(t) [-2 , 2] e \delta > 0  -e < f(t) - f(0) < e t \in (-\delta , \delta). \int_0^h (-e) < \int_0^h (f(t) - f(0)) < \int_0^h e \implies   -e < \frac{F(h) - F(0)}{h} - f(0)<  e  h \in (0 , \delta).  F_+'(0) = f(0) \int_0^h (e) < \int_0^h (f(t) - f(0)) < \int_0^h -e \implies   -e < \frac{F(h) - F(0)}{h} - f(0) <  e  h \in (- \delta , 0).  F_-'(0) = f(0) \therefore F'(0) = f(0)","['real-analysis', 'integration', 'derivatives', 'solution-verification']"
15,Proving: $\lim_{x\to\infty}\frac{f(x)f''(x)}{(f'(x))^2}=\frac{1}{2-c}$,Proving:,\lim_{x\to\infty}\frac{f(x)f''(x)}{(f'(x))^2}=\frac{1}{2-c},"let f be a function three times differentiable on $\mathbb{R}_{+}^{*}$ such that $f(x)> 0, f'(x)>0$ and $f''(x)>0$ for $x>0$ prove that if: $$\lim_{x\to\infty}\frac{f'(x)f'''(x)}{(f''(x))^2}=c ,\quad  c\neq 1$$ So: $$\lim_{x\to\infty}\frac{f(x)f''(x)}{(f'(x))^2}=\frac{1}{2-c}$$ using the hospital rule we get: $(1)\quad $ $$\lim_{x\to\infty}\frac{f'(x)f'''(x)}{(f''(x))^2}=\lim_{x\to\infty}\left(1-\frac{f'(x)}{xf''(x)}\right)=c$$ proof $(1)$ : $$\lim_{x\to\infty}\left(1-\frac{f'(x)}{xf''(x)}\right)=\lim_{x\to\infty}\frac{\left(x-\frac{f'(x)}{f''(x)}\right)}{x'}=\lim_{x\to\infty}\frac{f'(x)f'''(x)}{(f''(x))^2}=c$$ So we find : $$ \lim_{x\to \infty}\frac{f'(x)}{xf''(x)}=1-c$$ This is what I found now, how can I use this result in finding the required or is there any other way to prove","let f be a function three times differentiable on such that and for prove that if: So: using the hospital rule we get: proof : So we find : This is what I found now, how can I use this result in finding the required or is there any other way to prove","\mathbb{R}_{+}^{*} f(x)> 0, f'(x)>0 f''(x)>0 x>0 \lim_{x\to\infty}\frac{f'(x)f'''(x)}{(f''(x))^2}=c ,\quad  c\neq 1 \lim_{x\to\infty}\frac{f(x)f''(x)}{(f'(x))^2}=\frac{1}{2-c} (1)\quad  \lim_{x\to\infty}\frac{f'(x)f'''(x)}{(f''(x))^2}=\lim_{x\to\infty}\left(1-\frac{f'(x)}{xf''(x)}\right)=c (1) \lim_{x\to\infty}\left(1-\frac{f'(x)}{xf''(x)}\right)=\lim_{x\to\infty}\frac{\left(x-\frac{f'(x)}{f''(x)}\right)}{x'}=\lim_{x\to\infty}\frac{f'(x)f'''(x)}{(f''(x))^2}=c 
\lim_{x\to \infty}\frac{f'(x)}{xf''(x)}=1-c","['real-analysis', 'calculus', 'derivatives']"
16,Proof of differential operator identity in DLMF $16.3.5$,Proof of differential operator identity in DLMF,16.3.5,"DLMF $16.3.5$ gives the differential operator identity $$ (z\partial_z z)^nf(z)=z^n\partial_z^nz^nf(z),\quad n=1,2,\dots $$ where $\partial_z$ is differentiation w.r.t. $z$ . I am having some issues proving this identity.  It seems like this is a candidate for induction.  Clearly, the identity holds for $n=1$ so assuming for $n$ we have $$ (z\partial_z z)^{n+1}=z\partial_z z^{n+1}\partial_z^nz^n. $$ At this point all I had to go on was trying to somehow use the product rule.  I wrote $$ (z\partial_z z)^{n+1}=z\partial_z (z^{n+1}\partial_z^nz^n)=z((n+1)z^n\partial_z^nz^n+z^{n+1}\partial_z^{n+1}z^n), $$ which seems to lead to a dead end. How do I prove this identity? Perhaps a different proof technique altogether?","DLMF gives the differential operator identity where is differentiation w.r.t. . I am having some issues proving this identity.  It seems like this is a candidate for induction.  Clearly, the identity holds for so assuming for we have At this point all I had to go on was trying to somehow use the product rule.  I wrote which seems to lead to a dead end. How do I prove this identity? Perhaps a different proof technique altogether?","16.3.5 
(z\partial_z z)^nf(z)=z^n\partial_z^nz^nf(z),\quad n=1,2,\dots
 \partial_z z n=1 n 
(z\partial_z z)^{n+1}=z\partial_z z^{n+1}\partial_z^nz^n.
 
(z\partial_z z)^{n+1}=z\partial_z (z^{n+1}\partial_z^nz^n)=z((n+1)z^n\partial_z^nz^n+z^{n+1}\partial_z^{n+1}z^n),
","['calculus', 'derivatives', 'differential-operators']"
17,Why distance function differentiable for sufficiently smooth boundary?,Why distance function differentiable for sufficiently smooth boundary?,,"Suppose that $\Omega$ is bounded domain in $\mathbb{R}^n$ with $C^k$ boundary. Why is it that for points sufficiently close to the boundary the distance function $d = d( \cdot, \partial \Omega)$ is $C^k$ ? I can see it is enough to take a small ball around the boundary, and take a $C^k$ function $\psi$ defining the boundary and show that this give me a $C^k$ distance function, then use compactness. But, I don't see how to do this. Thoughts? Thanks","Suppose that is bounded domain in with boundary. Why is it that for points sufficiently close to the boundary the distance function is ? I can see it is enough to take a small ball around the boundary, and take a function defining the boundary and show that this give me a distance function, then use compactness. But, I don't see how to do this. Thoughts? Thanks","\Omega \mathbb{R}^n C^k d = d( \cdot, \partial \Omega) C^k C^k \psi C^k","['real-analysis', 'derivatives', 'differential-geometry', 'partial-differential-equations', 'elliptic-equations']"
18,Using a solution to show no others are possible in positive odd integer,Using a solution to show no others are possible in positive odd integer,,"If I have an equation say $$3(1+x+x^2)(1+y+y^2)(1+z+z^2)+1=4x^2y^2z^2 \quad (1)$$ and I know a non negative integer solution $x=4, y=64, z=262144$ then no odd positive integer solutions can possibly exist. I know that one way to show it for this example would be to compute but I am looking for a proof that does not rely on computation like using wolfram as I want to generalize this. I will happily award the bounty to anyone who can prove it. What I have tried is minimal. I suppose that $p,q,r \in \mathbb N$ and they satisfy equation $(1)$ . I think if we let $p$ be the smallest integer of the solution $p<4$ is impossible so assume that $p\ge 5$ but this might lead to a contradiction since the coefficient of $4p^2q^2r^2 \quad $ is $4<5$ and that might be impossible?",If I have an equation say and I know a non negative integer solution then no odd positive integer solutions can possibly exist. I know that one way to show it for this example would be to compute but I am looking for a proof that does not rely on computation like using wolfram as I want to generalize this. I will happily award the bounty to anyone who can prove it. What I have tried is minimal. I suppose that and they satisfy equation . I think if we let be the smallest integer of the solution is impossible so assume that but this might lead to a contradiction since the coefficient of is and that might be impossible?,"3(1+x+x^2)(1+y+y^2)(1+z+z^2)+1=4x^2y^2z^2 \quad (1) x=4, y=64, z=262144 p,q,r \in \mathbb N (1) p p<4 p\ge 5 4p^2q^2r^2 \quad  4<5","['number-theory', 'derivatives', 'diophantine-equations']"
19,Prove that $f(x)\leq\cosh(x)$ $\forall$ $ x\in\mathbb{R^+} $,Prove that,f(x)\leq\cosh(x) \forall  x\in\mathbb{R^+} ,"Let $ f:\mathbb{R^+}\to(1,+\infty) $ be continuously differentiable function such that $ f^{2}(x) -(f^{'})^{2}(x)\geq 1$ $\forall$ $ x\in\mathbb{R^+} $ and $ f(0) =1$ . Prove that $f(x)\leq\cosh(x)$ $\forall$ $ x\in\mathbb{R^+} $ My proof : Put $f(x)=\cosh(g(x))$ with $g(0)=0$ we get : $$\cosh^2(g(x))-(g'(x))^2\sinh^2(g(x))\geq 1$$ Or : $$-(g'(x))^2\sinh^2(g(x))\geq 1-\cosh^2(g(x))$$ Or: $$-(g'(x))^2\sinh^2(g(x))\geq -\sinh^2(g(x))$$ Or: $$(g'(x))^2\leq 1$$ Integrating we get : $$|g(x)|\leq |x|$$ So we get the desired result since $\cosh(x)$ is increasing $\forall$ $ x\in\mathbb{R^+} $ My question I'm really curious to see an alternative proof so : Have you another proof ? Thanks a lot for all your contributions",Let be continuously differentiable function such that and . Prove that My proof : Put with we get : Or : Or: Or: Integrating we get : So we get the desired result since is increasing My question I'm really curious to see an alternative proof so : Have you another proof ? Thanks a lot for all your contributions," f:\mathbb{R^+}\to(1,+\infty)   f^{2}(x) -(f^{'})^{2}(x)\geq 1 \forall  x\in\mathbb{R^+}   f(0) =1 f(x)\leq\cosh(x) \forall  x\in\mathbb{R^+}  f(x)=\cosh(g(x)) g(0)=0 \cosh^2(g(x))-(g'(x))^2\sinh^2(g(x))\geq 1 -(g'(x))^2\sinh^2(g(x))\geq 1-\cosh^2(g(x)) -(g'(x))^2\sinh^2(g(x))\geq -\sinh^2(g(x)) (g'(x))^2\leq 1 |g(x)|\leq |x| \cosh(x) \forall  x\in\mathbb{R^+} ","['integration', 'derivatives', 'inequality', 'exponential-function', 'alternative-proof']"
20,Finding the point on $f(x)=\sqrt{2x+1}$ where the tangent line is perpendicular to $3x + y = 5$,Finding the point on  where the tangent line is perpendicular to,f(x)=\sqrt{2x+1} 3x + y = 5,"$$f(x)=\sqrt{2x+1}$$ Find the point on the radical function where the tangent line is perpendicular to $3x + y = 5$ . I worked out the $y= -3x+5$ , and therefore the slope was $-3$ . I then used the negative reciprocal to find the slope of a perpendicular line, $m= 1/3$ . I set the derivative equal to $1/3$ to find $x$ which I found to be $4$ , $x=4$ . I then put $x=4$ back into the original equation, and found the $y=3$ when $x$ is $4$ . Using the slope I had, and the point $(4,3)$ , I inserted these numbers in the equation $y=mx+b$ , to find that $b= 5/3$ . So my final equation was $y=\frac13x+\frac53$ . I put this and $y= -3x+5$ into my graphing calculator. The lines are not perpendicular to each other. I am not sure what I did wrong. If someone could assist me that would be awesome.","Find the point on the radical function where the tangent line is perpendicular to . I worked out the , and therefore the slope was . I then used the negative reciprocal to find the slope of a perpendicular line, . I set the derivative equal to to find which I found to be , . I then put back into the original equation, and found the when is . Using the slope I had, and the point , I inserted these numbers in the equation , to find that . So my final equation was . I put this and into my graphing calculator. The lines are not perpendicular to each other. I am not sure what I did wrong. If someone could assist me that would be awesome.","f(x)=\sqrt{2x+1} 3x + y = 5 y= -3x+5 -3 m= 1/3 1/3 x 4 x=4 x=4 y=3 x 4 (4,3) y=mx+b b= 5/3 y=\frac13x+\frac53 y= -3x+5","['calculus', 'derivatives']"
21,Derivative of log of Gaussian pdf?,Derivative of log of Gaussian pdf?,,"I am trying to find the derivative of the logarithm of a Gaussian pdf with mean $\mu$ and standard deviation $\sigma$ defined over $x \in \mathbb{R}$ , but I am not certain whether my derivation is correct. Could you please verify whether this is fine, and if not, tell me where the error lies? I would start with the Gaussian pdf: $$p(x)=\frac{1}{\sqrt{2 \pi \sigma^2}}exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)$$ Then apply the log and the derivative operator to both sides: $$\frac{d}{dx}log\left(p(x)\right)=\frac{d}{dx}log\left(\frac{1}{\sqrt{2 \pi \sigma^2}}exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)\right)$$ Here we can split the innermost argument on the RHS into two separate logarithms: $$\frac{d}{dx}log\left(p(x)\right)=\frac{d}{dx}log\left(\frac{1}{\sqrt{2 \pi \sigma^2}}\right)+\frac{d}{dx}log\left(exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)\right)$$ Recognizing that the first RHS term is constant, its derivative becomes zero. In the second RHS term, the $log$ and $exp$ cancel out. $$\frac{d}{dx}log\left(p(x)\right)=\frac{d}{dx}\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)$$ We can expand the numerator of the term in the RHS brackets: $$\frac{d}{dx}log\left(p(x)\right)=\frac{d}{dx}\left(-\frac{x^2-2x\mu-\mu^2}{2 \sigma^2}\right)$$ Which yields, after derivation: $$\frac{d}{dx}log\left(p(x)\right)=-\frac{2x-2\mu}{2 \sigma^2}$$ which can be simplified to: $$\frac{d}{dx}log\left(p(x)\right)=-\frac{x-\mu}{\sigma^2}$$ Is this correct? Or did I make any errors?","I am trying to find the derivative of the logarithm of a Gaussian pdf with mean and standard deviation defined over , but I am not certain whether my derivation is correct. Could you please verify whether this is fine, and if not, tell me where the error lies? I would start with the Gaussian pdf: Then apply the log and the derivative operator to both sides: Here we can split the innermost argument on the RHS into two separate logarithms: Recognizing that the first RHS term is constant, its derivative becomes zero. In the second RHS term, the and cancel out. We can expand the numerator of the term in the RHS brackets: Which yields, after derivation: which can be simplified to: Is this correct? Or did I make any errors?",\mu \sigma x \in \mathbb{R} p(x)=\frac{1}{\sqrt{2 \pi \sigma^2}}exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right) \frac{d}{dx}log\left(p(x)\right)=\frac{d}{dx}log\left(\frac{1}{\sqrt{2 \pi \sigma^2}}exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)\right) \frac{d}{dx}log\left(p(x)\right)=\frac{d}{dx}log\left(\frac{1}{\sqrt{2 \pi \sigma^2}}\right)+\frac{d}{dx}log\left(exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)\right) log exp \frac{d}{dx}log\left(p(x)\right)=\frac{d}{dx}\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right) \frac{d}{dx}log\left(p(x)\right)=\frac{d}{dx}\left(-\frac{x^2-2x\mu-\mu^2}{2 \sigma^2}\right) \frac{d}{dx}log\left(p(x)\right)=-\frac{2x-2\mu}{2 \sigma^2} \frac{d}{dx}log\left(p(x)\right)=-\frac{x-\mu}{\sigma^2},"['derivatives', 'logarithms', 'normal-distribution']"
22,Does kernel regression preserve monotonicity?,Does kernel regression preserve monotonicity?,,"Consider the Kernel regression estimator: $$\hat{y}(x)=\frac{\sum_{i=1}^n{K(x-x_i)y_i}}{\sum_{i=1}^n{K(x-x_i)}},$$ where $x,x_1,\dots,x_n\in\mathbb{R}^d$ , $y_1,\dots,y_n\in\mathbb{R}$ , where $K:\mathbb{R}^d\rightarrow(0,\infty)$ is a strictly positive valued, differentiable kernel function, with a unique maximum at $0$ . Suppose further that for all $i,j\in\{1,\dots,n\}$ , if $x_i\le x_j$ then $y_i \le y_j$ . Is it the case that for all $x\in\mathbb{R}^d$ : $$\frac{\partial\hat{y}(x)}{\partial x} \ge 0?$$ It seems obvious in the $d=1$ case, but even there I haven't been able to prove it. It's unclear to me if it holds for $d>1$ . If it only holds under additional assumptions on $K$ I'd be interested in them. Notation: $\frac{\partial \hat{y}(x)}{\partial x}$ is the column vector of partial derivatives of $\hat{y}(x)$ , i.e. the (transposed) Jacobian. For vectors $a=[a_1,\dots,a_d]^\top$ and $b=[b_1,\dots,b_d]^\top$ , $a\le b$ if and only if $a_i \le b_i$ for all $i\in\{1,\dots,d\}$ .","Consider the Kernel regression estimator: where , , where is a strictly positive valued, differentiable kernel function, with a unique maximum at . Suppose further that for all , if then . Is it the case that for all : It seems obvious in the case, but even there I haven't been able to prove it. It's unclear to me if it holds for . If it only holds under additional assumptions on I'd be interested in them. Notation: is the column vector of partial derivatives of , i.e. the (transposed) Jacobian. For vectors and , if and only if for all .","\hat{y}(x)=\frac{\sum_{i=1}^n{K(x-x_i)y_i}}{\sum_{i=1}^n{K(x-x_i)}}, x,x_1,\dots,x_n\in\mathbb{R}^d y_1,\dots,y_n\in\mathbb{R} K:\mathbb{R}^d\rightarrow(0,\infty) 0 i,j\in\{1,\dots,n\} x_i\le x_j y_i \le y_j x\in\mathbb{R}^d \frac{\partial\hat{y}(x)}{\partial x} \ge 0? d=1 d>1 K \frac{\partial \hat{y}(x)}{\partial x} \hat{y}(x) a=[a_1,\dots,a_d]^\top b=[b_1,\dots,b_d]^\top a\le b a_i \le b_i i\in\{1,\dots,d\}","['derivatives', 'inequality', 'regression', 'monotone-functions']"
23,Derivative of function with the Kronecker product of a Matrix with respect to vech,Derivative of function with the Kronecker product of a Matrix with respect to vech,,"I have $\Sigma$ a symmetric $2 \times 2$ matrix, and $\Sigma^{-1}$ is its inverse. Now, $\tilde{\Sigma}^{-1}=\Sigma^{-1} \otimes I_{n \times n}$ (Kronecker product). I have a function $Y=f(\tilde{\Sigma}^{-1})$ that gives a value in $\mathbb R$ . Let's define $\Phi_{\Sigma}=vech(\Sigma)$ Now, I am trying to get $\frac{\partial Y}{\partial \Phi_{\Sigma}}$ . So far I have $\frac{\partial Y}{\partial \Phi_{\Sigma}^T} = \Bigg( \frac{\partial vec(\tilde{\Sigma}^{-1})}{\partial \Phi_{\Sigma}^T} \Bigg)^T \Bigg( \frac{\partial vec(Y)}{\partial vec(\tilde{\Sigma}^{-1})^T} \Bigg)$ I've been working on this an got that $\Bigg( \frac{\partial vec(Y)}{\partial vec(\tilde{\Sigma}^{-1})^T} \Bigg)$ is a vector with $n \times n$ elements.  Now, working with the first part of the derivative $\Bigg( \frac{\partial vec(\tilde{\Sigma}^{-1})}{\partial \Phi_{\Sigma}^T} \Bigg)^T  = \Bigg( \frac{\partial vec(\tilde{\Sigma}^{-1})}{\partial vec(\Sigma)^T}  D_2 \Bigg)^T  = \Bigg( vec \Big( \frac{\partial \tilde{\Sigma}^{-1}}{\partial \Sigma}\Big)  D_2 \Bigg)^T = \Bigg( vec \Big( \frac{\partial \tilde{\Sigma}^{-1}}{\partial \Sigma^{-1}} \frac{\partial \Sigma^{-1}}{\partial \Sigma} \Big)  D_2 \Bigg)^T$ $= \Bigg( vec \Big( (I_2 \otimes I_n) (-\Sigma^{-1} \Sigma^{-1}) \Big)  D_2 \Bigg)^T$ where $D_2$ is the duplication matrix However, the matrices $(I_2 \otimes I_n)$ and $-\Sigma^{-1} \Sigma^{-1}$ are not conformable. So it is wrong. Also, since $\Bigg( \frac{\partial vec(Y)}{\partial vec(\tilde{\Sigma}^{-1})^T} \Bigg)$ is a vector with $n \times n$ elements, and $\frac{\partial Y}{\partial \Phi_{\Sigma}}$ is $3 \times 1$ , so $\Bigg( \frac{\partial vec(\tilde{\Sigma}^{-1})}{\partial \Phi_{\Sigma}^T} \Bigg)^T$ should be $3 \times (n \times n)$ . May I ask for advice on solving this task?","I have a symmetric matrix, and is its inverse. Now, (Kronecker product). I have a function that gives a value in . Let's define Now, I am trying to get . So far I have I've been working on this an got that is a vector with elements.  Now, working with the first part of the derivative where is the duplication matrix However, the matrices and are not conformable. So it is wrong. Also, since is a vector with elements, and is , so should be . May I ask for advice on solving this task?",\Sigma 2 \times 2 \Sigma^{-1} \tilde{\Sigma}^{-1}=\Sigma^{-1} \otimes I_{n \times n} Y=f(\tilde{\Sigma}^{-1}) \mathbb R \Phi_{\Sigma}=vech(\Sigma) \frac{\partial Y}{\partial \Phi_{\Sigma}} \frac{\partial Y}{\partial \Phi_{\Sigma}^T} = \Bigg( \frac{\partial vec(\tilde{\Sigma}^{-1})}{\partial \Phi_{\Sigma}^T} \Bigg)^T \Bigg( \frac{\partial vec(Y)}{\partial vec(\tilde{\Sigma}^{-1})^T} \Bigg) \Bigg( \frac{\partial vec(Y)}{\partial vec(\tilde{\Sigma}^{-1})^T} \Bigg) n \times n \Bigg( \frac{\partial vec(\tilde{\Sigma}^{-1})}{\partial \Phi_{\Sigma}^T} \Bigg)^T  = \Bigg( \frac{\partial vec(\tilde{\Sigma}^{-1})}{\partial vec(\Sigma)^T}  D_2 \Bigg)^T  = \Bigg( vec \Big( \frac{\partial \tilde{\Sigma}^{-1}}{\partial \Sigma}\Big)  D_2 \Bigg)^T = \Bigg( vec \Big( \frac{\partial \tilde{\Sigma}^{-1}}{\partial \Sigma^{-1}} \frac{\partial \Sigma^{-1}}{\partial \Sigma} \Big)  D_2 \Bigg)^T = \Bigg( vec \Big( (I_2 \otimes I_n) (-\Sigma^{-1} \Sigma^{-1}) \Big)  D_2 \Bigg)^T D_2 (I_2 \otimes I_n) -\Sigma^{-1} \Sigma^{-1} \Bigg( \frac{\partial vec(Y)}{\partial vec(\tilde{\Sigma}^{-1})^T} \Bigg) n \times n \frac{\partial Y}{\partial \Phi_{\Sigma}} 3 \times 1 \Bigg( \frac{\partial vec(\tilde{\Sigma}^{-1})}{\partial \Phi_{\Sigma}^T} \Bigg)^T 3 \times (n \times n),"['matrices', 'derivatives', 'matrix-calculus', 'kronecker-product', 'vectorization']"
24,$f:\Bbb{R}\mapsto\Bbb{R}$ and $f$ is twice differentiable.,and  is twice differentiable.,f:\Bbb{R}\mapsto\Bbb{R} f,"$f:\Bbb{R}\mapsto\Bbb{R}$ and $f$ is twice differentiable such that $f(0)=2, f(1)=1, f'(0)=-2$ . Prove that $\exists$ a $\varepsilon \in (0,1)$ such that $f(\varepsilon)f'(\varepsilon)+f''(\varepsilon)=0$ . Suppose $g(x) = \frac{2}{f(x)}-1-x$ . Then $g(0)=g(1)=0\implies \exists$ $c \in (0, 1)$ such that $g'(c)=0$ by Rolle's theorem. $g'(x) = \frac{-2f'(x)-f(x)^2}{f(x)^2}$ . Then $g'(0)=g'(c)=0$ $2f'(x)+f(x)^2=0$ for $x=0, c$ . Suppose $h(x)=2f'(x)+f(x)^2$ . Then $h(0)=h(c)=0\implies \exists$ $k \in (0, c)$ such that $h'(k)=0$ by Rolle's theorem. $h'(k) = 2f''(k)+2f(k)f'(k)=0 \implies f''(k)+f(k)f'(k)=0$ for $k \in (0, c) \subset (0, 1)$ . Hence, Proved. But what to do if $f(k)=0$ for some $k \in (0, 1)$ . Either a new function is to be found out or we need to prove that $f(x)$ is non-zero in the interval.","and is twice differentiable such that . Prove that a such that . Suppose . Then such that by Rolle's theorem. . Then for . Suppose . Then such that by Rolle's theorem. for . Hence, Proved. But what to do if for some . Either a new function is to be found out or we need to prove that is non-zero in the interval.","f:\Bbb{R}\mapsto\Bbb{R} f f(0)=2, f(1)=1, f'(0)=-2 \exists \varepsilon \in (0,1) f(\varepsilon)f'(\varepsilon)+f''(\varepsilon)=0 g(x) = \frac{2}{f(x)}-1-x g(0)=g(1)=0\implies \exists c \in (0, 1) g'(c)=0 g'(x) = \frac{-2f'(x)-f(x)^2}{f(x)^2} g'(0)=g'(c)=0 2f'(x)+f(x)^2=0 x=0, c h(x)=2f'(x)+f(x)^2 h(0)=h(c)=0\implies \exists k \in (0, c) h'(k)=0 h'(k) = 2f''(k)+2f(k)f'(k)=0 \implies f''(k)+f(k)f'(k)=0 k \in (0, c) \subset (0, 1) f(k)=0 k \in (0, 1) f(x)","['calculus', 'derivatives', 'rolles-theorem']"
25,differentiability and continuity of $f: \mathbb{R}^2 \to \mathbb{R}$,differentiability and continuity of,f: \mathbb{R}^2 \to \mathbb{R},"I have a problem with this function. I have to study where the function is continuous  and where is differentiable. The function is $f : \Bbb R^2 → \Bbb R$ : $$ f(x, y) = \begin{cases}    x + \sin(y)& x\le y \\        y + \sin(x) & x>y \\  \end{cases} $$ Continuity (i think that is wrong): I know that $$ sin(y) + x $$ is continuos in $\mathbb{R}^2$ because $sin(y)$ and $x$ are continuos. (for $sin(x)+ y$ is the same) My problem is about the point $(x = y)$ . What i have to do? I tried with: $$ \lim_{(x,y)\to (a,a)}f(x,y) $$ Differentiability I compute the Partial derivative: $$ \frac{\partial f}{\partial x} = \begin{cases}    1& x\le y \\        cos(x) & x>y \\  \end{cases} $$ and $$ \frac{\partial f}{\partial y} = \begin{cases}    cos(y)& x\le y \\        1 & x>y \\  \end{cases} $$ So i think that is differentiable in $\mathbb{R}^2$ \{(x,y): x=y}. For (x = y) I used the definition of differentiability: $$ \lim_{(h,k)\to (0,0)}(f(x+h,y+k) - f(x,y) - \frac{\partial f}{\partial y}(x,y) *h- \frac{\partial f}{\partial x}(x,y)*k)/\sqrt((h^2 + k^2)) $$ Is it correct?","I have a problem with this function. I have to study where the function is continuous  and where is differentiable. The function is : Continuity (i think that is wrong): I know that is continuos in because and are continuos. (for is the same) My problem is about the point . What i have to do? I tried with: Differentiability I compute the Partial derivative: and So i think that is differentiable in \{(x,y): x=y}. For (x = y) I used the definition of differentiability: Is it correct?","f : \Bbb R^2 → \Bbb R 
f(x, y) = \begin{cases}
   x + \sin(y)& x\le y \\
       y + \sin(x) & x>y \\ 
\end{cases}
  sin(y) + x  \mathbb{R}^2 sin(y) x sin(x)+ y (x = y) 
\lim_{(x,y)\to (a,a)}f(x,y)
 
\frac{\partial f}{\partial x} = \begin{cases}
   1& x\le y \\
       cos(x) & x>y \\ 
\end{cases}
 
\frac{\partial f}{\partial y} = \begin{cases}
   cos(y)& x\le y \\
       1 & x>y \\ 
\end{cases}
 \mathbb{R}^2 
\lim_{(h,k)\to (0,0)}(f(x+h,y+k) - f(x,y) - \frac{\partial f}{\partial y}(x,y) *h- \frac{\partial f}{\partial x}(x,y)*k)/\sqrt((h^2 + k^2))
","['derivatives', 'continuity']"
26,Optimize area of football field within running track of 400 meters in perimeter,Optimize area of football field within running track of 400 meters in perimeter,,"I'm trying to solve the following problem: We are projecting a running track. The running area consists of two   parallel lines and two semicircles connecting them. The perimeter of   the running track is 400 meters. We want to have a football playground   (a rectangle) inside the running track with the biggest possible area.   What dimensions do we have to choose in order to have the biggest   area? I know how to optimize using the derivative, however I don't know how to form the function. Could you help me with that? Thanks","I'm trying to solve the following problem: We are projecting a running track. The running area consists of two   parallel lines and two semicircles connecting them. The perimeter of   the running track is 400 meters. We want to have a football playground   (a rectangle) inside the running track with the biggest possible area.   What dimensions do we have to choose in order to have the biggest   area? I know how to optimize using the derivative, however I don't know how to form the function. Could you help me with that? Thanks",,['calculus']
27,Understanding Gradient Policy Deriving,Understanding Gradient Policy Deriving,,"I'm stuck with understanding pretty simple expression and would appreciate some help on this. The most interesting part for algorithms, it's way how we can come here. Using the original resources from Andrej Karpathy blog about Policy Gradient. Everything is clear with Monte Carlo credit assignments and supervised algorithms vs reinforcement. We have next expression, how we came up this optimization objective and gradient for it (images from another resources): 1) I'm familiar with derivation I think, but what was a point for taking log in this case? It's called likehood ratio trick sometimes and also explained here (where I still cannot get it). What is the point of using it here? 2) Can somebody show few Very simple examples of using it with numbers and how it works? Is there anything else about math I need to find or this could exist on Khan Academy? References : 1) Deep Reinforcement Learning: Pong from Pixels 2) An introduction to Policy Gradients with Cartpole and Doom 3) Deriving Policy Gradients and Implementing REINFORCE 4) Machine Learning Trick of the Day (5): Log Derivative Trick 12 UPDATE Please consider answering above two points. I don't need to find derivative of softmax and complicated output. I would appreciate some new explanation (different from articles above). And let's say that action space it's continues value and probability of taking action it's liner activation within very simple example.","I'm stuck with understanding pretty simple expression and would appreciate some help on this. The most interesting part for algorithms, it's way how we can come here. Using the original resources from Andrej Karpathy blog about Policy Gradient. Everything is clear with Monte Carlo credit assignments and supervised algorithms vs reinforcement. We have next expression, how we came up this optimization objective and gradient for it (images from another resources): 1) I'm familiar with derivation I think, but what was a point for taking log in this case? It's called likehood ratio trick sometimes and also explained here (where I still cannot get it). What is the point of using it here? 2) Can somebody show few Very simple examples of using it with numbers and how it works? Is there anything else about math I need to find or this could exist on Khan Academy? References : 1) Deep Reinforcement Learning: Pong from Pixels 2) An introduction to Policy Gradients with Cartpole and Doom 3) Deriving Policy Gradients and Implementing REINFORCE 4) Machine Learning Trick of the Day (5): Log Derivative Trick 12 UPDATE Please consider answering above two points. I don't need to find derivative of softmax and complicated output. I would appreciate some new explanation (different from articles above). And let's say that action space it's continues value and probability of taking action it's liner activation within very simple example.",,"['integration', 'derivatives', 'logarithms']"
28,Name of analytic function identity,Name of analytic function identity,,"Doing partial integration on $\int 1\times f(x)dx$ , one gets $$\int f(x)dx=xf(x)-\int xf'(x)dx+C$$ $$=xf(x)-\frac{x^2}2 f'(x)+\int \frac{x^2}2 f''(x)+C$$ $$=...=C+x\sum_{n=0}^\infty \frac{(-x)^n}{(n+1)!} f^{(n)}(x)$$ Replacing $f(x)$ by $f'(x)$ and bringing the right hand side to the left, this becomes $$\sum_{n=0}^\infty \frac{(-x)^n}{n!}f^{(n)}(x)=C$$ My questions: A) Is this correct? B) This identity was not hard to show and looks pretty nice, so I'm sure it has been found by other people, but I could not find any reference to it. Has it appeared anywhere before? If yes, what is its name?","Doing partial integration on , one gets Replacing by and bringing the right hand side to the left, this becomes My questions: A) Is this correct? B) This identity was not hard to show and looks pretty nice, so I'm sure it has been found by other people, but I could not find any reference to it. Has it appeared anywhere before? If yes, what is its name?",\int 1\times f(x)dx \int f(x)dx=xf(x)-\int xf'(x)dx+C =xf(x)-\frac{x^2}2 f'(x)+\int \frac{x^2}2 f''(x)+C =...=C+x\sum_{n=0}^\infty \frac{(-x)^n}{(n+1)!} f^{(n)}(x) f(x) f'(x) \sum_{n=0}^\infty \frac{(-x)^n}{n!}f^{(n)}(x)=C,"['calculus', 'integration', 'sequences-and-series', 'derivatives']"
29,Why does $\int \frac{-1}{\sqrt{1-x^2}}dx = -\sin^{-1}(x) + C$ and not $\cos^{-1}(x) + C?$,Why does  and not,\int \frac{-1}{\sqrt{1-x^2}}dx = -\sin^{-1}(x) + C \cos^{-1}(x) + C?,"Since $\displaystyle \frac{\mathrm{d}}{\mathrm{d}x} \arccos(x) = \frac{-1}{\sqrt{1-x^2}}$ , it seems odd that $\displaystyle\int \frac{-1}{\sqrt{1-x^2}}dx = -\arcsin(x) + C$ . Why is this the case?","Since , it seems odd that . Why is this the case?",\displaystyle \frac{\mathrm{d}}{\mathrm{d}x} \arccos(x) = \frac{-1}{\sqrt{1-x^2}} \displaystyle\int \frac{-1}{\sqrt{1-x^2}}dx = -\arcsin(x) + C,"['calculus', 'integration', 'derivatives']"
30,How to apply chain rule to $\log$ function,How to apply chain rule to  function,\log,I'm doing my economics reading and I find this equation for elasticity of substitution: $$\sigma \equiv \frac{F_K \cdot F_L}{F \cdot F_{KL}}$$ and then $$\frac{d(\log(F_K/F_L))}{d(\log(K/L))}=-\frac{1}{\sigma}.$$ How do I do the later derivative? By chain rule or anything else? Many thanks.,I'm doing my economics reading and I find this equation for elasticity of substitution: and then How do I do the later derivative? By chain rule or anything else? Many thanks.,\sigma \equiv \frac{F_K \cdot F_L}{F \cdot F_{KL}} \frac{d(\log(F_K/F_L))}{d(\log(K/L))}=-\frac{1}{\sigma}.,"['derivatives', 'economics', 'chain-rule']"
31,The $k$th derivative of $\sin^n x$ as $n \ \sin^{n - k} x$ times a polynomial in $\cos x$,The th derivative of  as  times a polynomial in,k \sin^n x n \ \sin^{n - k} x \cos x,"It seems the $k$ th derivative of the function $x \mapsto \sin^n x$ can be expressed as $n \ \sin^{n - k} x$ times a polynomial with only even or only odd powers of $\cos x$ (depending on the parity of $k$ ), with polynomials in $n$ as coefficients, for natural $k$ and integer $n$ and complex $x$ . The highest term is $n^{k-1} \cos^k x$ ; the ranks of the other polynomial-in- $n$ coefficients for $\cos^{k-2j} x$ seem to be (maximally) $(k-1-j)$ . $$ \frac{\mathrm{d}^0}{\mathrm{d}x^0} \sin^n x = n \ \sin^{n - 0} x \ \left(n^{-1} \cos^0 x\right)\text{ for }n \neq 0 $$ $$ \frac{\mathrm{d}^1}{\mathrm{d}x^1} \sin^n x = n \ \sin^{n - 1} x \ \left(n^0 \cos x\right) $$ $$ \frac{\mathrm{d}^2}{\mathrm{d}x^2} \sin^n x = n \ \sin^{n - 2} x \ \left(n^1 \cos^2 x - 1\right) $$ $$ \frac{\mathrm{d}^3}{\mathrm{d}x^3} \sin^n x = n \ \sin^{n - 3} x \ \left(n^2 \cos^3 x + (-3 n + 2) \cos x\right) $$ $$ \frac{\mathrm{d}^4}{\mathrm{d}x^4} \sin^n x = n \ \sin^{n - 4} x \ \left(n^3 \cos^4 x + (-6 n^2 + 8 n - 4) \cos^2 x + (3 n - 2)\right) $$ $$ \frac{\mathrm{d}^5}{\mathrm{d}x^5} \sin^n x = n \ \sin^{n - 5} x $$ $$ \left(n^4 \cos^5 x + (-10 n^3 + 20 n^2 - 20 n + 8) \cos^3 x + (15 n^2 - 30 n + 16) \cos x\right) $$ $$ \frac{\mathrm{d}^6}{\mathrm{d}x^6} \sin^n x = n \ \sin^{n - 6} x $$ $$\left(n^5 \cos^6 x + (-15 n^4 + 40 n^3 - 60 n^2 + 48 n - 16) \cos^4 x + (45 n^3 - 150 n^2 + 196 n - 88) \cos^2 x + (-15 n^2 + 30 n - 16)\right) $$ Is there a general rule to determine the polynomial-in- $n$ coefficients? Edit: The OEIS sequence A133341 seems to determine the second polynomial-in- $n$ coefficients (when one alternates the signs, and discards the italic values): 1 , 2 , 1, 3 , 3, 2, 4 , 6, 8, 4, 5 , 10, 20, 20, 8, 6 , 15, 40, 60, 48, 16, 7 , 21, 70, 140, 168, 112, 32, 8 , 28, 112, 280, 448, 448, 256, 64, 9 , 36, 168, 504, 1008, 1344, 1152, 576, 128, 10 , 45, 240, 840, 2016, 3360, 3840, 2880, 1280, 256, …","It seems the th derivative of the function can be expressed as times a polynomial with only even or only odd powers of (depending on the parity of ), with polynomials in as coefficients, for natural and integer and complex . The highest term is ; the ranks of the other polynomial-in- coefficients for seem to be (maximally) . Is there a general rule to determine the polynomial-in- coefficients? Edit: The OEIS sequence A133341 seems to determine the second polynomial-in- coefficients (when one alternates the signs, and discards the italic values): 1 , 2 , 1, 3 , 3, 2, 4 , 6, 8, 4, 5 , 10, 20, 20, 8, 6 , 15, 40, 60, 48, 16, 7 , 21, 70, 140, 168, 112, 32, 8 , 28, 112, 280, 448, 448, 256, 64, 9 , 36, 168, 504, 1008, 1344, 1152, 576, 128, 10 , 45, 240, 840, 2016, 3360, 3840, 2880, 1280, 256, …","k x \mapsto \sin^n x n \ \sin^{n - k} x \cos x k n k n x n^{k-1} \cos^k x n \cos^{k-2j} x (k-1-j) 
\frac{\mathrm{d}^0}{\mathrm{d}x^0} \sin^n x = n \ \sin^{n - 0} x \ \left(n^{-1} \cos^0 x\right)\text{ for }n \neq 0
 
\frac{\mathrm{d}^1}{\mathrm{d}x^1} \sin^n x = n \ \sin^{n - 1} x \ \left(n^0 \cos x\right)
 
\frac{\mathrm{d}^2}{\mathrm{d}x^2} \sin^n x = n \ \sin^{n - 2} x \ \left(n^1 \cos^2 x - 1\right)
 
\frac{\mathrm{d}^3}{\mathrm{d}x^3} \sin^n x = n \ \sin^{n - 3} x \ \left(n^2 \cos^3 x + (-3 n + 2) \cos x\right)
 
\frac{\mathrm{d}^4}{\mathrm{d}x^4} \sin^n x = n \ \sin^{n - 4} x \ \left(n^3 \cos^4 x + (-6 n^2 + 8 n - 4) \cos^2 x + (3 n - 2)\right)
 
\frac{\mathrm{d}^5}{\mathrm{d}x^5} \sin^n x = n \ \sin^{n - 5} x
 
\left(n^4 \cos^5 x + (-10 n^3 + 20 n^2 - 20 n + 8) \cos^3 x + (15 n^2 - 30 n + 16) \cos x\right)
 
\frac{\mathrm{d}^6}{\mathrm{d}x^6} \sin^n x = n \ \sin^{n - 6} x
 \left(n^5 \cos^6 x + (-15 n^4 + 40 n^3 - 60 n^2 + 48 n - 16) \cos^4 x + (45 n^3 - 150 n^2 + 196 n - 88) \cos^2 x + (-15 n^2 + 30 n - 16)\right)
 n n",['derivatives']
32,$\frac{d}{dt}$ represents an operator or infinitesimal change?,represents an operator or infinitesimal change?,\frac{d}{dt},"In high school physics, they teach us a little bit Calculus as it would be done months later in maths. And many times my physics teacher wrote things like $$\frac{dv}{dt} =\frac{dv}{dx}\frac{dx}{dt}$$ That seemed ok, as we were taught $\frac{dv}{dt}$ represents ratio of two infinitesimal changes. But in maths we were taught $\frac{d}{dt}$ is an operator and so can't be separated as that. I'm confused on this. Which is correct, and if $\frac{d}{dt}$ is an operator (can't be separated) then how can relations like the one mentioned above can be derived?","In high school physics, they teach us a little bit Calculus as it would be done months later in maths. And many times my physics teacher wrote things like That seemed ok, as we were taught represents ratio of two infinitesimal changes. But in maths we were taught is an operator and so can't be separated as that. I'm confused on this. Which is correct, and if is an operator (can't be separated) then how can relations like the one mentioned above can be derived?",\frac{dv}{dt} =\frac{dv}{dx}\frac{dx}{dt} \frac{dv}{dt} \frac{d}{dt} \frac{d}{dt},"['calculus', 'derivatives', 'notation']"
33,Does there exist an algorithm which detects the starting and stop points of valleys in a time-series?,Does there exist an algorithm which detects the starting and stop points of valleys in a time-series?,,"I was able to loosely code something which does this in Python, but I was just wondering if there was a known mathematical formula or proper algorithm which estimates these points. I've marked ""starting points"" in green and ""ending points"" in red. My intuition led me to estimating the second-derivative and doing peak detection on it, but I'm sure that my hunch isn't some novel invention, so I'd just like to make sure.","I was able to loosely code something which does this in Python, but I was just wondering if there was a known mathematical formula or proper algorithm which estimates these points. I've marked ""starting points"" in green and ""ending points"" in red. My intuition led me to estimating the second-derivative and doing peak detection on it, but I'm sure that my hunch isn't some novel invention, so I'd just like to make sure.",,"['derivatives', 'maxima-minima', 'data-analysis']"
34,Implicit and explicit differentiation give contradictory answers in related rates problem.,Implicit and explicit differentiation give contradictory answers in related rates problem.,,"The problem statement: Oil spreads on a frying pan so that it's radius is proportional to $t^{1/2}$ , where $t$ represents the time from the moment when the oil   is poured. Find the change $dT/dt$ of the thickness T of the oil. From the solution I can tell that we should assume that the oil is in the shape of a cylinder and that the volume is constant. Implicit solution (according to textbook): $r = kt^{1/2}$ $r^2T = \frac{V}{\pi} = c$ (definition of the volume of a cylinder) $2r\frac{dr}{dt}T + r^2\frac{dT}{dt} = 0$ $\frac{dT}{dt} = -\frac{2}{r}\frac{dr}{dt}$ $\frac{dT}{dt} = -\frac{1}{t}$ (after computing the derivative of r) According to the textbook this is the correct solution, but if we differentiate explicitly: $r = kt^{1/2}$ $T = \frac{V}{r^2\pi} = \frac{V}{k^2t\pi}$ $\frac{dT}{dt} = -\frac{V}{k^2\pi} \frac{1}{t^2} = -\frac{r^2\pi T}{k^2\pi} \frac{1}{t^2} = -\frac{k^2tT}{k^2} \frac{1}{t^2} = -\frac{T}{t}$ If both of the solution are correct then $\frac{dT}{dt} = -\frac{1}{t} = -\frac{T}{t}$ and $T = 1$ , which can't be possible. Why does differentiating implicitly and explicitly give different contradictory solutions?","The problem statement: Oil spreads on a frying pan so that it's radius is proportional to , where represents the time from the moment when the oil   is poured. Find the change of the thickness T of the oil. From the solution I can tell that we should assume that the oil is in the shape of a cylinder and that the volume is constant. Implicit solution (according to textbook): (definition of the volume of a cylinder) (after computing the derivative of r) According to the textbook this is the correct solution, but if we differentiate explicitly: If both of the solution are correct then and , which can't be possible. Why does differentiating implicitly and explicitly give different contradictory solutions?",t^{1/2} t dT/dt r = kt^{1/2} r^2T = \frac{V}{\pi} = c 2r\frac{dr}{dt}T + r^2\frac{dT}{dt} = 0 \frac{dT}{dt} = -\frac{2}{r}\frac{dr}{dt} \frac{dT}{dt} = -\frac{1}{t} r = kt^{1/2} T = \frac{V}{r^2\pi} = \frac{V}{k^2t\pi} \frac{dT}{dt} = -\frac{V}{k^2\pi} \frac{1}{t^2} = -\frac{r^2\pi T}{k^2\pi} \frac{1}{t^2} = -\frac{k^2tT}{k^2} \frac{1}{t^2} = -\frac{T}{t} \frac{dT}{dt} = -\frac{1}{t} = -\frac{T}{t} T = 1,"['calculus', 'derivatives', 'related-rates']"
35,"Given a positive number $t$, prove that $7>\frac{4t^{3}+56t^{2}+191t+188}{\sqrt{t^{2}+2t+8}\left (7\sqrt{t^{2}+2t+8}+2(6t-2+t^{2})\right )}$ .","Given a positive number , prove that  .",t 7>\frac{4t^{3}+56t^{2}+191t+188}{\sqrt{t^{2}+2t+8}\left (7\sqrt{t^{2}+2t+8}+2(6t-2+t^{2})\right )},"Given a positive number $t$ , prove that $$7> \frac{4\,t^{3}+ 56\,t^{2}+ 191\,t+ 188}{\sqrt{t^{2}+ 2\,t+ 8}\left ( 7\sqrt{t^{2}+ 2\,t+ 8}+ 2(6\,t- 2+ t^{2}) \right )}$$ Original problem is: Given two positive numbers $x,\,y$ so that $x^{2}+ 2\,y^{2}= \frac{8}{3}$ . Prove that $$v(x)\equiv v= 7(x+ 2\,y)- 4\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}}\leqq 8$$ 1st solution: (with Cauchy-Schwarz) We need to prove $v(\!x\!)\leqq 7(x+ 2\,y)- (3 x+ 10\,y)= 4(x+ y)$ . Then $4(x+ y)\leqq 4\sqrt{\left ( 1+ \frac{1}{2} \right )(x^{2}+ 2\,y^{2})}= 8\,\therefore\,v(x)\leqq 8$ / q.e.d The 2rd solution is my one without C-S : $$x^{2}+ 2\,y^{2}= \frac{8}{3}\,\therefore\,(x- 2\,y)\left ( x- \frac{4}{3} \right )\geqq 0,\,2\,{y}'= -\,\frac{x}{y}$$ Thus, we have $${v}'(x)= 7(1+ 2\,{y}')- \frac{4(2\,x+ 2\,y+ 2\,x{y}'+ 16\,y{y}')}{2\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}}}=$$ $$= 7\left ( 1- \frac{x}{y} \right )- \frac{2\left ( 2\,x+ 2\,y- \frac{x^{2}}{y}- 8\,x \right )}{\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}}}= 7\left ( 2- \frac{x}{y} \right )+ \frac{2(\,6xy- 2\,y^{2}+ x^{2})}{y\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}}}- 7=$$ $$= \frac{1}{y}\left \{ -7(x- 2\,y)+ \frac{4(6\,xy- 2\,y^{2}+ x^{2})^{2}- 49\,y^{2}(x^{2}+ 2\,xy+ 8\,y^{2})^{2}}{\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}}\left ( 2(6\,xy- 2\,y^{2}+ x^{2})+ 7\,y\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}} \right )} \right \}=$$ $$= -\,\frac{x- 2\,y}{y}\left \{ \underbrace{7- \frac{4\,x^{3}+ 56\,x^{2}y+ 191\,xy^{2}+ 188\,y^{3}}{\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}}\left ( 2(6\,xy- 2\,y^{2}+ x^{2})+ 7\,y\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}} \right )}}_{> 0\,(we\,need\,to\,prove\,that\,it\,holds\,!)} \right \}$$ Indeed, for $t= -\,2\,{y}'= \frac{x}{y}> 0$ , we have $$\frac{\mathrm{d} }{\mathrm{d} t}\left ( \underbrace{2(6\,t- 2+ t^{2})+ 7\sqrt{t^{2}+ 2\,t+ 8}}_{> 0\,\because\,its\,derivative\,is\,positve\,and\,t> 0} \right )= \frac{14(t+ 1)}{2\sqrt{t^{2}+ 2\,t+ 8}}+ 4(t+ 3)> 0$$ Or $$14(6 t- 2+ t^{2})\sqrt{t^{2}+ 2 t+ 8}> 4 t^{3}+ 56 t^{2}+ 191 t+ 188- 49(t^{2}+ 2 t+ 8)\,(\!original.problem\!)$$ My solution in VMF : (and I'm looking forward to seeing a nicer one(s), thanks for your interests !) For $0< t\leqq 2$ $$14(6\,t- 2+ t^{2})\sqrt{t^{2}+ 2\,t+ 8}\geqq 4\,t^{3}+ 241\,t^{2}+ 192\,t+ 188- 49(t^{2}+ 2\,t+ 8)>$$ $$> 4\,t^{3}+ 56\,t^{2}+ 191\,t+ 188- 49(t^{2}+ 2\,t+ 8)$$ For $t\geqq 2$ $$14(6\,t- 2+ t^{2})\sqrt{t^{2}+ 2\,t+ 8}\geqq 4\,t^{3}+ 56\,t^{2}+ 562\,t+ 188- 49(t^{2}+ 2\,t+ 8)>$$ $$> 4\,t^{3}+ 56\,t^{2}+ 191\,t+ 188- 49(t^{2}+ 2\,t+ 8)$$ Therefore $(x- 2\,y){v}'(x)\leqq 0\,\therefore\,\left ( x- \frac{4}{3} \right ){v}'(x)\leqq 0\,\therefore\,v(x)\leqq v\left ( \frac{4}{3} \right )= 8$ / q.e.d","Given a positive number , prove that Original problem is: Given two positive numbers so that . Prove that 1st solution: (with Cauchy-Schwarz) We need to prove . Then / q.e.d The 2rd solution is my one without C-S : Thus, we have Indeed, for , we have Or My solution in VMF : (and I'm looking forward to seeing a nicer one(s), thanks for your interests !) For For Therefore / q.e.d","t 7> \frac{4\,t^{3}+ 56\,t^{2}+ 191\,t+ 188}{\sqrt{t^{2}+ 2\,t+ 8}\left ( 7\sqrt{t^{2}+ 2\,t+ 8}+ 2(6\,t- 2+ t^{2}) \right )} x,\,y x^{2}+ 2\,y^{2}= \frac{8}{3} v(x)\equiv v= 7(x+ 2\,y)- 4\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}}\leqq 8 v(\!x\!)\leqq 7(x+ 2\,y)- (3 x+ 10\,y)= 4(x+ y) 4(x+ y)\leqq 4\sqrt{\left ( 1+ \frac{1}{2} \right )(x^{2}+ 2\,y^{2})}= 8\,\therefore\,v(x)\leqq 8 x^{2}+ 2\,y^{2}= \frac{8}{3}\,\therefore\,(x- 2\,y)\left ( x- \frac{4}{3} \right )\geqq 0,\,2\,{y}'= -\,\frac{x}{y} {v}'(x)= 7(1+ 2\,{y}')- \frac{4(2\,x+ 2\,y+ 2\,x{y}'+ 16\,y{y}')}{2\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}}}= = 7\left ( 1- \frac{x}{y} \right )- \frac{2\left ( 2\,x+ 2\,y- \frac{x^{2}}{y}- 8\,x \right )}{\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}}}= 7\left ( 2- \frac{x}{y} \right )+ \frac{2(\,6xy- 2\,y^{2}+ x^{2})}{y\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}}}- 7= = \frac{1}{y}\left \{ -7(x- 2\,y)+ \frac{4(6\,xy- 2\,y^{2}+ x^{2})^{2}- 49\,y^{2}(x^{2}+ 2\,xy+ 8\,y^{2})^{2}}{\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}}\left ( 2(6\,xy- 2\,y^{2}+ x^{2})+ 7\,y\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}} \right )} \right \}= = -\,\frac{x- 2\,y}{y}\left \{ \underbrace{7- \frac{4\,x^{3}+ 56\,x^{2}y+ 191\,xy^{2}+ 188\,y^{3}}{\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}}\left ( 2(6\,xy- 2\,y^{2}+ x^{2})+ 7\,y\sqrt{x^{2}+ 2\,xy+ 8\,y^{2}} \right )}}_{> 0\,(we\,need\,to\,prove\,that\,it\,holds\,!)} \right \} t= -\,2\,{y}'= \frac{x}{y}> 0 \frac{\mathrm{d} }{\mathrm{d} t}\left ( \underbrace{2(6\,t- 2+ t^{2})+ 7\sqrt{t^{2}+ 2\,t+ 8}}_{> 0\,\because\,its\,derivative\,is\,positve\,and\,t> 0} \right )= \frac{14(t+ 1)}{2\sqrt{t^{2}+ 2\,t+ 8}}+ 4(t+ 3)> 0 14(6 t- 2+ t^{2})\sqrt{t^{2}+ 2 t+ 8}> 4 t^{3}+ 56 t^{2}+ 191 t+ 188- 49(t^{2}+ 2 t+ 8)\,(\!original.problem\!) 0< t\leqq 2 14(6\,t- 2+ t^{2})\sqrt{t^{2}+ 2\,t+ 8}\geqq 4\,t^{3}+ 241\,t^{2}+ 192\,t+ 188- 49(t^{2}+ 2\,t+ 8)> > 4\,t^{3}+ 56\,t^{2}+ 191\,t+ 188- 49(t^{2}+ 2\,t+ 8) t\geqq 2 14(6\,t- 2+ t^{2})\sqrt{t^{2}+ 2\,t+ 8}\geqq 4\,t^{3}+ 56\,t^{2}+ 562\,t+ 188- 49(t^{2}+ 2\,t+ 8)> > 4\,t^{3}+ 56\,t^{2}+ 191\,t+ 188- 49(t^{2}+ 2\,t+ 8) (x- 2\,y){v}'(x)\leqq 0\,\therefore\,\left ( x- \frac{4}{3} \right ){v}'(x)\leqq 0\,\therefore\,v(x)\leqq v\left ( \frac{4}{3} \right )= 8","['derivatives', 'inequality']"
36,Inverse of the fundamental theorem of calculus,Inverse of the fundamental theorem of calculus,,"The fundumental theorem of calculus states: if : 1) $f$ was integrable over an interval like $[a,b]$ . 2) $f$ was continuous at $x=c$ , $a<c<b$ . 3) $F(x)= \int_a ^x{f(t)}$ . then: $F'(c)=f(c)$ I wonder if there is any function with following properties : 1) $f$ was integrable over an interval like $[a,b]$ . 2) $f$ was continuous over $[a,b]$ but at $x=c$ , where isn't continuous. 3) $F(x)=\int_a^x{f(t)}$ , and $F$ is differentiable over $[a,b]$ , and $F'(x)=f(x)$ Note: if there was any function like $f$ with above properties, then, $L_1$ and/or $L_2$ wouldn't exist ((event in the cases $L_1=\pm\infty$ and/or $L_2=\pm\infty$ )),  where: $$L_1=\lim_{x \rightarrow c^-} f(x) \quad L_2=\lim_{x \rightarrow c^+}f(x)$$ any response, would be appreciated.","The fundumental theorem of calculus states: if : 1) was integrable over an interval like . 2) was continuous at , . 3) . then: I wonder if there is any function with following properties : 1) was integrable over an interval like . 2) was continuous over but at , where isn't continuous. 3) , and is differentiable over , and Note: if there was any function like with above properties, then, and/or wouldn't exist ((event in the cases and/or )),  where: any response, would be appreciated.","f [a,b] f x=c a<c<b F(x)= \int_a ^x{f(t)} F'(c)=f(c) f [a,b] f [a,b] x=c F(x)=\int_a^x{f(t)} F [a,b] F'(x)=f(x) f L_1 L_2 L_1=\pm\infty L_2=\pm\infty L_1=\lim_{x \rightarrow c^-} f(x) \quad L_2=\lim_{x \rightarrow c^+}f(x)","['calculus', 'integration', 'derivatives']"
37,How to show Jacobian of a composite function is the product of Jacobians?,How to show Jacobian of a composite function is the product of Jacobians?,,"Let $f: \mathbb{R}^m \rightarrow \mathbb{R^n}$ and $g : \mathbb{R^n} \rightarrow \mathbb{R^m}$ be two vector-valued functions. We want to show that $$J_{f\circ g}(a)=J_f(g(a))Jg(a)$$ where $J$ is the Jacobian and $a$ is a point in $\mathbb{R^n}$ . Wikipedia Chain Rule page, at Higher dimensions section, has the following: Let $D_a(g)$ denote the total derivative of $g$ at $a$ and $D_{g(a)}(f)$ denote the total derivative of $f$ at $g(a)$ . These two derivatives are linear transformations. The chain rule for total derivatives says that their composite is the total derivative of $f \circ g$ at $a$ , that is: $$ D_a(f \circ g) = D_{g(a)}(f) \circ D_a(g) \tag{1} $$ Concluding by the fact that the total derivative is Jacobian and since it is linear transformation, the composite of two total derivative becomes product of them. My questions: 1- Why $(1)$ is true? prove this? 2- Alternative prove that start from scratch and shows $J_{f\circ g}(a)=J_f(g(a))Jg(a)$ .","Let and be two vector-valued functions. We want to show that where is the Jacobian and is a point in . Wikipedia Chain Rule page, at Higher dimensions section, has the following: Let denote the total derivative of at and denote the total derivative of at . These two derivatives are linear transformations. The chain rule for total derivatives says that their composite is the total derivative of at , that is: Concluding by the fact that the total derivative is Jacobian and since it is linear transformation, the composite of two total derivative becomes product of them. My questions: 1- Why is true? prove this? 2- Alternative prove that start from scratch and shows .","f: \mathbb{R}^m \rightarrow \mathbb{R^n} g : \mathbb{R^n} \rightarrow \mathbb{R^m} J_{f\circ g}(a)=J_f(g(a))Jg(a) J a \mathbb{R^n} D_a(g) g a D_{g(a)}(f) f g(a) f \circ g a 
D_a(f \circ g) = D_{g(a)}(f) \circ D_a(g) \tag{1}
 (1) J_{f\circ g}(a)=J_f(g(a))Jg(a)",['derivatives']
38,Matrix derivative of structured matrix (with constraint),Matrix derivative of structured matrix (with constraint),,"Let $$c (A,B) := \log | ABA' + I - \mbox{diag}(ABA')|$$ where matrix $A$ is not necessarily square, matrix $B$ is symmetric, and $I$ is the identity matrix. How to obtain the derivatives $\frac{dc}{dA}$ and $\frac{dc}{dB}$ ?","Let where matrix is not necessarily square, matrix is symmetric, and is the identity matrix. How to obtain the derivatives and ?","c (A,B) := \log | ABA' + I - \mbox{diag}(ABA')| A B I \frac{dc}{dA} \frac{dc}{dB}","['derivatives', 'determinant', 'matrix-calculus']"
39,How to take the derivative of minimum of a norm?,How to take the derivative of minimum of a norm?,,"Suppose $f: \mathbb{R}^n \rightarrow \mathbb{R}$ where $f$ is the following: $$ f(z) =  \begin{cases} 0 \,\,\,\, z \in C \\ \min_{x\in C} \frac{1}{2} \|x-z\|_2^2 \,\,\,\ z\notin C \end{cases} $$ where $C$ is a closed convex $C$ in $\mathbb{R}^n$ and $x$ is a point in $C$ . How can we find derivative of $f$ . I know the answer is $$ f'(z) =  \begin{cases} 0 \,\,\,\, z \in C \\ z-x \,\,\,\ z\notin C \end{cases} $$ but how would we find this derivative using the definition? Is there any way to get that using: $$ f'(z;d) = \lim_{d \rightarrow 0^{+}} \frac{\min_{x\in C} \frac{1}{2} \|x-(z+td)\|_2^2 - \min_{x\in C} \frac{1}{2} \|x-z\|_2^2}{t} $$",Suppose where is the following: where is a closed convex in and is a point in . How can we find derivative of . I know the answer is but how would we find this derivative using the definition? Is there any way to get that using:,"f: \mathbb{R}^n \rightarrow \mathbb{R} f 
f(z) = 
\begin{cases}
0 \,\,\,\, z \in C \\
\min_{x\in C} \frac{1}{2} \|x-z\|_2^2 \,\,\,\ z\notin C
\end{cases}
 C C \mathbb{R}^n x C f 
f'(z) = 
\begin{cases}
0 \,\,\,\, z \in C \\
z-x \,\,\,\ z\notin C
\end{cases}
 
f'(z;d) = \lim_{d \rightarrow 0^{+}} \frac{\min_{x\in C} \frac{1}{2} \|x-(z+td)\|_2^2 - \min_{x\in C} \frac{1}{2} \|x-z\|_2^2}{t}
",['derivatives']
40,Continuous extension from Cantor set to unit interval,Continuous extension from Cantor set to unit interval,,"I have the following assignment: Let $C \subset [0, 1]$ be the Cantor set, $x ∈ C$ satisfying $$x = \sum_{n = 1} ^ \infty \frac{a_n}{3^n} \; ,$$ and $\varphi: C \to [0,1]$ a function defined as $$\varphi(x) = \sum_{n=1} ^ \infty \frac{a_n}{2^{n+1}} .$$ I want to prove: a) The function $\varphi: C \to [0,1]$ is continuous, surjective and monotone . b) There exists a continuous extension $\hat{\varphi}:[0,1] \to [0,1]$ of $\varphi$ , such that $\hat{\varphi}$ is constant in any open set $U$ of $[0,1] \setminus C$ . c) The function $\hat{\varphi}$ is almost everywhere differentiable . My thoughts a) The function $\hat{\varphi}$ seems to be an identification of the Cantor set with the binary representation of elements in the unit interval. From this, I don't know how to prove continuity. Which topology of $C$ should I consider? How to prove continuity? b) $[0,1]$ is a complete metric space. I was thinking that if $\hat{\varphi}$ is uniformly continuous, this could show the existence of a continuous extension over $[0,1]$ . However, this fails since the Cantor set is not dense in $[0,1]$ . c) Should I work with the weak derivative of $\hat{\varphi}$ ? Thanks for any help","I have the following assignment: Let be the Cantor set, satisfying and a function defined as I want to prove: a) The function is continuous, surjective and monotone . b) There exists a continuous extension of , such that is constant in any open set of . c) The function is almost everywhere differentiable . My thoughts a) The function seems to be an identification of the Cantor set with the binary representation of elements in the unit interval. From this, I don't know how to prove continuity. Which topology of should I consider? How to prove continuity? b) is a complete metric space. I was thinking that if is uniformly continuous, this could show the existence of a continuous extension over . However, this fails since the Cantor set is not dense in . c) Should I work with the weak derivative of ? Thanks for any help","C \subset [0, 1] x ∈ C x = \sum_{n = 1} ^ \infty \frac{a_n}{3^n} \; , \varphi: C \to [0,1] \varphi(x) = \sum_{n=1} ^ \infty \frac{a_n}{2^{n+1}} . \varphi: C \to [0,1] \hat{\varphi}:[0,1] \to [0,1] \varphi \hat{\varphi} U [0,1] \setminus C \hat{\varphi} \hat{\varphi} C [0,1] \hat{\varphi} [0,1] [0,1] \hat{\varphi}","['functional-analysis', 'derivatives', 'continuity', 'cantor-set']"
41,"If $C_0, C_1, C_2, .., C_n$ are the binomial coefficients in the expansion of $(1+x)^n$",If  are the binomial coefficients in the expansion of,"C_0, C_1, C_2, .., C_n (1+x)^n","If $C_0, C_1, C_2,...,C_n$ are the binomial coefficients in the expansion of $(1+x)^n$ , prove that: $$C_{r}.C_{n} + C_{r+1}.C_{n-1} +......+ C_{n}.C_{r} = C(2n, n+r) =\dfrac {(2n)!}{(n-r)! (n+r)!}$$ Is there any way to approach this sort of questions using calculus (derivatives or integration)?","If are the binomial coefficients in the expansion of , prove that: Is there any way to approach this sort of questions using calculus (derivatives or integration)?","C_0, C_1, C_2,...,C_n (1+x)^n C_{r}.C_{n} + C_{r+1}.C_{n-1} +......+ C_{n}.C_{r} = C(2n, n+r) =\dfrac {(2n)!}{(n-r)! (n+r)!}","['calculus', 'integration', 'derivatives', 'binomial-coefficients', 'binomial-theorem']"
42,Definition of differentiability,Definition of differentiability,,"I can't figure out the intuition and motivation behind the following definition: Function $f$ , defined in some neighborhood of $x_0$ , is said to be differentiable, if: $$f(x_0 + \Delta x)-f(x_0)=A\Delta x+o(\Delta x), \Delta x \rightarrow 0$$ where $A$ is some real number, or equivalently: $$\Delta y=A\Delta x +\epsilon(\Delta x)\Delta x$$ where $\epsilon(\Delta x)$ is infinitely small as $\Delta x \rightarrow 0$ . I understand the limit definition of differentiability, and I understand the proof that two definitions are equivalent, but I just don't get the above one. At first look it says that we can approximate the change of function by some linear(?) function ? And if it's saying that we can approximate the value of $f$ at point $a$ by some linear function, doesn't this definition make every function differentiable, as we can chose constant function $f(x)=a$ for approximation .. ? Also, I don't get the $o(\Delta x)$ part .. Can this be shown visually ? (But then, visualization would be the same as for limit definition, right?)","I can't figure out the intuition and motivation behind the following definition: Function , defined in some neighborhood of , is said to be differentiable, if: where is some real number, or equivalently: where is infinitely small as . I understand the limit definition of differentiability, and I understand the proof that two definitions are equivalent, but I just don't get the above one. At first look it says that we can approximate the change of function by some linear(?) function ? And if it's saying that we can approximate the value of at point by some linear function, doesn't this definition make every function differentiable, as we can chose constant function for approximation .. ? Also, I don't get the part .. Can this be shown visually ? (But then, visualization would be the same as for limit definition, right?)","f x_0 f(x_0 + \Delta x)-f(x_0)=A\Delta x+o(\Delta x), \Delta x \rightarrow 0 A \Delta y=A\Delta x +\epsilon(\Delta x)\Delta x \epsilon(\Delta x) \Delta x \rightarrow 0 f a f(x)=a o(\Delta x)",['real-analysis']
43,Interpretation of $\nabla$ operator in an expression,Interpretation of  operator in an expression,\nabla,"Let a tensor (3x3) be of the form $U = \mathbf{u}\mathbf{v}$ ( $\mathbf{u}$ and $\mathbf{v}$ ) being two fluid velocity vectors (of dimension 1x3). In my analysis, for such a tensor $U$ , following expression arises, \begin{equation} E = \frac{1}{2}\nabla^2(\mathbf{u}\cdot\mathbf{v}) + \frac{1}{2}\left(\nabla\cdot(\mathbf{u}\cdot\nabla\mathbf{v}) + \nabla\cdot(\mathbf{v}\cdot\nabla\mathbf{u})\right)  \end{equation} Above expression remains the same when we replace $\mathbf{v}$ with $\mathbf{u}$ , although the original tensor $U$ changes to its transpose with such replacement. Now, I would like to write down the expression $E$ above for the case where $\mathbf{u}$ is replaced with gradient operator $\nabla$ . In this case $U = \nabla\mathbf{v}$ . I proceeded with blindly replacing $\mathbf{u}$ with $\nabla$ to arrive at, \begin{eqnarray} E &=& \frac{1}{2}\nabla^2(\nabla\cdot\mathbf{v}) + \frac{1}{2}\left(\nabla\cdot(\nabla\cdot\nabla\mathbf{v}) + \nabla\cdot(\mathbf{v}\cdot\nabla\nabla)\right) \\ &=& \frac{1}{2}\nabla^2(\nabla\cdot\mathbf{v}) + \frac{1}{2}\nabla^2(\nabla\cdot\mathbf{v}) + \nabla\cdot(\mathbf{v}\cdot\nabla\nabla) \\ &=& \nabla^2(\nabla\cdot\mathbf{v})+\nabla\cdot(\mathbf{v}\cdot\nabla\nabla) \end{eqnarray} I am not sure how to interpret the last term in the above expression $\nabla\cdot(\mathbf{v}\cdot\nabla\nabla)$ . This term does not have an argument on the right hand side of tensor $\nabla\nabla$ . Any help in the interpretation is highly appreciated.","Let a tensor (3x3) be of the form ( and ) being two fluid velocity vectors (of dimension 1x3). In my analysis, for such a tensor , following expression arises, Above expression remains the same when we replace with , although the original tensor changes to its transpose with such replacement. Now, I would like to write down the expression above for the case where is replaced with gradient operator . In this case . I proceeded with blindly replacing with to arrive at, I am not sure how to interpret the last term in the above expression . This term does not have an argument on the right hand side of tensor . Any help in the interpretation is highly appreciated.","U = \mathbf{u}\mathbf{v} \mathbf{u} \mathbf{v} U \begin{equation}
E = \frac{1}{2}\nabla^2(\mathbf{u}\cdot\mathbf{v}) + \frac{1}{2}\left(\nabla\cdot(\mathbf{u}\cdot\nabla\mathbf{v}) + \nabla\cdot(\mathbf{v}\cdot\nabla\mathbf{u})\right) 
\end{equation} \mathbf{v} \mathbf{u} U E \mathbf{u} \nabla U = \nabla\mathbf{v} \mathbf{u} \nabla \begin{eqnarray}
E &=& \frac{1}{2}\nabla^2(\nabla\cdot\mathbf{v}) + \frac{1}{2}\left(\nabla\cdot(\nabla\cdot\nabla\mathbf{v}) + \nabla\cdot(\mathbf{v}\cdot\nabla\nabla)\right) \\
&=& \frac{1}{2}\nabla^2(\nabla\cdot\mathbf{v}) + \frac{1}{2}\nabla^2(\nabla\cdot\mathbf{v}) + \nabla\cdot(\mathbf{v}\cdot\nabla\nabla) \\
&=& \nabla^2(\nabla\cdot\mathbf{v})+\nabla\cdot(\mathbf{v}\cdot\nabla\nabla)
\end{eqnarray} \nabla\cdot(\mathbf{v}\cdot\nabla\nabla) \nabla\nabla","['derivatives', 'tensor-products', 'tensors']"
44,"Finding $F'(1)$ when $F(x)=\int_{x^3}^4 \sqrt{4+t^2}\,dt$ for all $x\in\mathbb R$",Finding  when  for all,"F'(1) F(x)=\int_{x^3}^4 \sqrt{4+t^2}\,dt x\in\mathbb R","Differentiation under integral sign Jam 2019: If $F(x)=\int_{x^3}^{4}\sqrt{4+t^2}\,dt$ for all $ x\in \mathbb R$ then $F'(1)$ equals ? $F'(x)=  -\sqrt{4+x^6}(3x^2)$ $F'(1)=-3\sqrt{5}$ Is there any mistake ?",Differentiation under integral sign Jam 2019: If for all then equals ? Is there any mistake ?,"F(x)=\int_{x^3}^{4}\sqrt{4+t^2}\,dt  x\in \mathbb R F'(1) F'(x)=  -\sqrt{4+x^6}(3x^2) F'(1)=-3\sqrt{5}","['derivatives', 'definite-integrals']"
45,Derivative of $\mathbf{XX}^T$ with respect to $\mathbf{X}$,Derivative of  with respect to,\mathbf{XX}^T \mathbf{X},"Problem $$\nabla_{\mathbf{X}}\mathbf{XX}^T$$ What I Have Done I checked matrix cookbook , but there is no luck. So I tried to derive it from scratch. I have $$(\mathbf{XX}^T)_{kl}= \mathbf{X}_k^T\mathbf{X}_l=\sum_{q=1}^n \mathbf{X}_{kl}\mathbf{X}_{ql}$$ where $\mathbf{X}_i$ is the $i$ -th column of $\mathbf{X}$ . However, when I tried to get $\nabla_{\mathbf{X}_{ij}} (\mathbf{XX}^T)_{kl}$ , I am lost in the indices $i,j,k$ and $l$ and did not how to resolve this issue. Could anyone help me, thank you in advance.","Problem What I Have Done I checked matrix cookbook , but there is no luck. So I tried to derive it from scratch. I have where is the -th column of . However, when I tried to get , I am lost in the indices and and did not how to resolve this issue. Could anyone help me, thank you in advance.","\nabla_{\mathbf{X}}\mathbf{XX}^T (\mathbf{XX}^T)_{kl}= \mathbf{X}_k^T\mathbf{X}_l=\sum_{q=1}^n \mathbf{X}_{kl}\mathbf{X}_{ql} \mathbf{X}_i i \mathbf{X} \nabla_{\mathbf{X}_{ij}} (\mathbf{XX}^T)_{kl} i,j,k l","['matrices', 'derivatives', 'matrix-calculus']"
46,Why can't I reduce the total differential?,Why can't I reduce the total differential?,,"I have encountered the following equation: $g: \mathbb{R}^m \rightarrow \mathbb{R}$ $u: \mathbb{R}^n \rightarrow \mathbb{R}^m$ $z = g(\mathbf{y})$ , $\mathbf{y} = u(\mathbf{x})$ then using numerator layout $\frac{\partial z}{\partial \mathbf{x}}  = \frac{\partial z}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}$ translates to: $$ \left(\begin{array}{ccc}     \frac{\partial z}{\partial x_{1}} & \cdots & \frac{\partial z}{\partial x_{n}}     \end{array}\right)=     \left(\begin{array}{ccc}     \frac{\partial z}{\partial y_{1}} & \cdots & \frac{\partial z}{\partial y_{m}}     \end{array}\right)     \left(\begin{array}{ccc}     \frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{n}}\\     \vdots & \ddots & \vdots\\     \frac{\partial y_{m}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}}     \end{array}\right) $$ (From https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py presented in denominator notation there) When calculating the matrix multiplication on paper I came to think that the equation must be wrong because for any element in the result vector: $ \frac{\partial z}{\partial x_i} =        \left(\begin{array}{ccc}     \frac{\partial z}{\partial y_{1}} & \cdots & \frac{\partial z}{\partial y_{m}}     \end{array}\right) \left(\begin{array}{c}     \frac{\partial y_{1}}{\partial x_i}\\     \vdots \\     \frac{\partial y_{m}}{\partial x_i}     \end{array}\right)= \sum_{j=1}^m \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_i} \overset{chain \\rule}{=} \sum_{j=1}^m \frac{\partial z}{\partial x_i} = m \frac{\partial z}{\partial x_i}$ But maybe I'm wrong? After all I've never seen the chain rule with $\partial$ but always with $d$ . But even after research they seem to be 2 ways of expressing the same thing: $df/dx$ is the derivative of a term e.g. $2xy$ wrt. $x$ and $\partial f/\partial x$ the derivative of a function e.g. $f(x,y) = 2xy$ . Maybe this can be explained by the law of total derivatives. However this law is sometimes presented using only $\partial$ s, sometimes using a mix like this $ \frac{dz}{dx_i} = \sum_{j=1}^m \frac{\partial z}{\partial y_j} \frac{dy_j}{dx_i} $ , further implying that $d$ , $\partial$ is really just a style choice. But if it is, what's keeping me from replacing all $\partial$ with $d$ and applying the chain rule like this: $$ \frac{dz}{dx_i} = \sum_{j=1}^m \frac{dz}{dy_j} \frac{dy_j}{dx_i} = \sum_{j=1}^m \frac{dz}{dx_i} = m \frac{dz}{dx_i} $$ Questions: Is the Equation $\frac{\partial z}{\partial \mathbf{x}}  = \frac{\partial z}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}$ correct? If so, is $\frac{\partial y}{\partial x}$ equivalent to $\frac{dy}{dx}$ ? If so, what am I doing wrong when applying the chain rule?","I have encountered the following equation: , then using numerator layout translates to: (From https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py presented in denominator notation there) When calculating the matrix multiplication on paper I came to think that the equation must be wrong because for any element in the result vector: But maybe I'm wrong? After all I've never seen the chain rule with but always with . But even after research they seem to be 2 ways of expressing the same thing: is the derivative of a term e.g. wrt. and the derivative of a function e.g. . Maybe this can be explained by the law of total derivatives. However this law is sometimes presented using only s, sometimes using a mix like this , further implying that , is really just a style choice. But if it is, what's keeping me from replacing all with and applying the chain rule like this: Questions: Is the Equation correct? If so, is equivalent to ? If so, what am I doing wrong when applying the chain rule?","g: \mathbb{R}^m \rightarrow \mathbb{R} u: \mathbb{R}^n \rightarrow \mathbb{R}^m z = g(\mathbf{y}) \mathbf{y} = u(\mathbf{x}) \frac{\partial z}{\partial \mathbf{x}}  = \frac{\partial z}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}  \left(\begin{array}{ccc}
    \frac{\partial z}{\partial x_{1}} & \cdots & \frac{\partial z}{\partial x_{n}}
    \end{array}\right)=
    \left(\begin{array}{ccc}
    \frac{\partial z}{\partial y_{1}} & \cdots & \frac{\partial z}{\partial y_{m}}
    \end{array}\right)
    \left(\begin{array}{ccc}
    \frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{n}}\\
    \vdots & \ddots & \vdots\\
    \frac{\partial y_{m}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}}
    \end{array}\right)   \frac{\partial z}{\partial x_i} =   
    \left(\begin{array}{ccc}
    \frac{\partial z}{\partial y_{1}} & \cdots & \frac{\partial z}{\partial y_{m}}
    \end{array}\right) \left(\begin{array}{c}
    \frac{\partial y_{1}}{\partial x_i}\\
    \vdots \\
    \frac{\partial y_{m}}{\partial x_i}
    \end{array}\right)=
\sum_{j=1}^m \frac{\partial z}{\partial y_j} \frac{\partial y_j}{\partial x_i} \overset{chain \\rule}{=} \sum_{j=1}^m \frac{\partial z}{\partial x_i} = m \frac{\partial z}{\partial x_i} \partial d df/dx 2xy x \partial f/\partial x f(x,y) = 2xy \partial  \frac{dz}{dx_i} = \sum_{j=1}^m \frac{\partial z}{\partial y_j} \frac{dy_j}{dx_i}  d \partial \partial d  \frac{dz}{dx_i} = \sum_{j=1}^m \frac{dz}{dy_j} \frac{dy_j}{dx_i} = \sum_{j=1}^m \frac{dz}{dx_i} = m \frac{dz}{dx_i}  \frac{\partial z}{\partial \mathbf{x}}  = \frac{\partial z}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}} \frac{\partial y}{\partial x} \frac{dy}{dx}","['derivatives', 'matrix-calculus', 'chain-rule']"
47,Computing the Laplacian in Polar Coordinates [duplicate],Computing the Laplacian in Polar Coordinates [duplicate],,"This question already has an answer here : Computing second partial derivative with polar coordinates (1 answer) Closed 5 years ago . Similar questions have been asked on this site but none of them seemed to help me. I'm asked to compute the Laplacian $$\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}$$ in terms of polar coordinates. I did do it, but I don't understand why what I did is correct, and I don't understand the more ""brute force"" way to do it at all. Here is what I did: I calculated $\frac{\partial}{\partial r}$ and $\frac{\partial}{\partial \theta}$ in terms of $r,$ $\theta,$ $\frac{\partial}{\partial x}$ and $\frac{\partial}{\partial y}.$ This gave me a system of linear equations which I wrote as $$\begin{pmatrix}\frac{\partial}{\partial r} \\\frac{\partial}{\partial \theta}\end{pmatrix} = \begin{pmatrix}\cos\theta & \sin\theta \\\ -r\sin\theta & r\cos\theta \end{pmatrix}\begin{pmatrix}\frac{\partial}{\partial x} \\\frac{\partial}{\partial y}\end{pmatrix}.$$ I inverted to get $$\begin{pmatrix}\frac{\partial}{\partial x} \\\frac{\partial}{\partial y}\end{pmatrix} = \frac{1}{r}\begin{pmatrix}r\cos\theta & -\sin\theta \\\ r\sin\theta & \cos\theta \end{pmatrix}\begin{pmatrix}\frac{\partial}{\partial r} \\\frac{\partial}{\partial \theta}\end{pmatrix},$$ and then I simply wrote \begin{align}\frac{\partial^2}{\partial x^2} &= \frac{\partial}{\partial x} \left( \frac{\partial}{\partial x}\right)\\ &= \left(\cos\theta \frac{\partial}{\partial r}-\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\right)\left(\cos\theta \frac{\partial}{\partial r}-\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\right)\\ &= \cos\theta\frac{\partial}{\partial r}\left(\cos\theta \frac{\partial}{\partial r}-\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\right) -\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\left(\cos\theta \frac{\partial}{\partial r}-\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\right)\\ &= \cos^2\theta \frac{\partial^2}{\partial r^2} - \frac{2}{r}\sin\theta\cos\theta\frac{\partial^2}{\partial r \partial\theta} +\frac{1}{r^2}\sin^2\theta\frac{\partial^2}{\partial \theta^2}.\end{align} I similarly got $$\frac{\partial^2}{\partial y^2} = \sin^2\theta \frac{\partial^2}{\partial r^2} + \frac{2}{r}\sin\theta\cos\theta\frac{\partial^2}{\partial r \partial\theta} +\frac{1}{r^2}\cos^2\theta\frac{\partial^2}{\partial \theta^2}.$$ Adding the two yields $$\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2} = \frac{\partial^2}{\partial r^2}+\frac{1}{r^2}\frac{\partial^2}{\partial \theta^2},$$ which Spivak says is correct. Explicitly, here are my questions: In my solution, when I found $\frac{\partial^2}{\partial x^2},$ I simply ""multiplied"" the expressions in the second line of the large aligned equation (treating multiplication of the partial operators as composition). Why am I allowed to do this? Why does the expression on the left not act on the thing on the right, forcing me to do the product rule and other nonsense to get the answer? My original idea was just to compute the Laplacian using the chain rule. That is, write $\frac{\partial}{\partial x}=\frac{\partial}{\partial r} \frac{\partial r}{\partial x}$ and compute $\frac{\partial^2}{\partial x^2}$ from there. My problem with this is that I keep getting confused about in which variables I should be writing everything, and how the partial derivative operators act on these other expressions. For example, I compute $\frac{\partial}{\partial x}=\frac{\partial}{\partial r} \frac{\partial r}{\partial x} + \frac{\partial}{\partial \theta} \frac{\partial \theta}{\partial x} = \frac{x}{\sqrt{x^2+y^2}}\frac{\partial}{\partial r} - \frac{y}{x^2+y^2} \frac{\partial}{\partial \theta},$ but then I don't know where to go. Help understanding this method would be greatly appreciated. If anything is unclear let me know and I'll make the necessary edits.","This question already has an answer here : Computing second partial derivative with polar coordinates (1 answer) Closed 5 years ago . Similar questions have been asked on this site but none of them seemed to help me. I'm asked to compute the Laplacian in terms of polar coordinates. I did do it, but I don't understand why what I did is correct, and I don't understand the more ""brute force"" way to do it at all. Here is what I did: I calculated and in terms of and This gave me a system of linear equations which I wrote as I inverted to get and then I simply wrote I similarly got Adding the two yields which Spivak says is correct. Explicitly, here are my questions: In my solution, when I found I simply ""multiplied"" the expressions in the second line of the large aligned equation (treating multiplication of the partial operators as composition). Why am I allowed to do this? Why does the expression on the left not act on the thing on the right, forcing me to do the product rule and other nonsense to get the answer? My original idea was just to compute the Laplacian using the chain rule. That is, write and compute from there. My problem with this is that I keep getting confused about in which variables I should be writing everything, and how the partial derivative operators act on these other expressions. For example, I compute but then I don't know where to go. Help understanding this method would be greatly appreciated. If anything is unclear let me know and I'll make the necessary edits.","\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2} \frac{\partial}{\partial r} \frac{\partial}{\partial \theta} r, \theta, \frac{\partial}{\partial x} \frac{\partial}{\partial y}. \begin{pmatrix}\frac{\partial}{\partial r} \\\frac{\partial}{\partial \theta}\end{pmatrix} = \begin{pmatrix}\cos\theta & \sin\theta \\\ -r\sin\theta & r\cos\theta \end{pmatrix}\begin{pmatrix}\frac{\partial}{\partial x} \\\frac{\partial}{\partial y}\end{pmatrix}. \begin{pmatrix}\frac{\partial}{\partial x} \\\frac{\partial}{\partial y}\end{pmatrix} = \frac{1}{r}\begin{pmatrix}r\cos\theta & -\sin\theta \\\ r\sin\theta & \cos\theta \end{pmatrix}\begin{pmatrix}\frac{\partial}{\partial r} \\\frac{\partial}{\partial \theta}\end{pmatrix}, \begin{align}\frac{\partial^2}{\partial x^2} &= \frac{\partial}{\partial x} \left( \frac{\partial}{\partial x}\right)\\ &= \left(\cos\theta \frac{\partial}{\partial r}-\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\right)\left(\cos\theta \frac{\partial}{\partial r}-\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\right)\\ &= \cos\theta\frac{\partial}{\partial r}\left(\cos\theta \frac{\partial}{\partial r}-\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\right) -\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\left(\cos\theta \frac{\partial}{\partial r}-\frac{1}{r}\sin\theta\frac{\partial}{\partial \theta}\right)\\ &= \cos^2\theta \frac{\partial^2}{\partial r^2} - \frac{2}{r}\sin\theta\cos\theta\frac{\partial^2}{\partial r
\partial\theta} +\frac{1}{r^2}\sin^2\theta\frac{\partial^2}{\partial \theta^2}.\end{align} \frac{\partial^2}{\partial y^2} = \sin^2\theta \frac{\partial^2}{\partial r^2} + \frac{2}{r}\sin\theta\cos\theta\frac{\partial^2}{\partial r
\partial\theta} +\frac{1}{r^2}\cos^2\theta\frac{\partial^2}{\partial \theta^2}. \frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2} = \frac{\partial^2}{\partial r^2}+\frac{1}{r^2}\frac{\partial^2}{\partial \theta^2}, \frac{\partial^2}{\partial x^2}, \frac{\partial}{\partial x}=\frac{\partial}{\partial r} \frac{\partial r}{\partial x} \frac{\partial^2}{\partial x^2} \frac{\partial}{\partial x}=\frac{\partial}{\partial r} \frac{\partial r}{\partial x} + \frac{\partial}{\partial \theta} \frac{\partial \theta}{\partial x} = \frac{x}{\sqrt{x^2+y^2}}\frac{\partial}{\partial r} - \frac{y}{x^2+y^2} \frac{\partial}{\partial \theta},","['derivatives', 'partial-derivative', 'polar-coordinates']"
48,Prove an inequality concerning $\ln$ function,Prove an inequality concerning  function,\ln,"For any real number $p \in (0,1)$ , $\delta \in (0,1)$ and $p+\delta <1$ , prove the following inequality: $$(p+\delta)\ln \frac {p+\delta}{p} +(1-p-\delta)\ln \frac {1-p-\delta}{1-p} \ge 2\delta^2$$ I have tried some appoximation on $\ln$ function such as $\ln(1+x) \le x$ and $\ln(1+x)\ge x-x^2/2$ but it doesn't work.","For any real number , and , prove the following inequality: I have tried some appoximation on function such as and but it doesn't work.","p \in (0,1) \delta \in (0,1) p+\delta <1 (p+\delta)\ln \frac {p+\delta}{p} +(1-p-\delta)\ln \frac {1-p-\delta}{1-p} \ge 2\delta^2 \ln \ln(1+x) \le x \ln(1+x)\ge x-x^2/2","['real-analysis', 'derivatives', 'inequality']"
49,I need help with a problem involving the nth derivative of arcsin x,I need help with a problem involving the nth derivative of arcsin x,,"I need help with a problem. For context, the section of the textbook the problem is in is about power series. Note that the textbook uses the convention that $f^{(n)}$ represents the $n$ th derivative of $f$ , and $f^{(0)}(x) = f(x).$ I'll now state the problem exactly as stated in the textbook: Consider the function $f$ defined by $f(x) = \arcsin x$ , for $\lvert x \rvert \leq 1$ . The derivatives of $f(x)$ satisfy the equation $ (1 - x^2)f^{(n + 2)}(x) - (2n + 1)xf^{(n + 1)}(x) - n^2 f^{(n)}(x) = 0$ , for $n \geq 1. $ The coefficient of $x^n$ in the Maclaurin series for $f(x)$ is denoted by $a_n$ . You may assume that the series only contains odd powers of $x$ . $\textbf{a.1)}$ Show that, for $n \geq 1, (n+1)(n+2)a_{n+2} = n^2 a_n.$ $\textbf{a.2})$ Given that $a_1 = 1$ , find an expression for $a_n$ in terms of $n$ , valid for odd $n \geq 3.$ $\textbf{b})$ Find the radius of convergence of this Maclaurin series. $\textbf{c})$ Find an approximate value for $\pi$ by putting $x = \frac{1}{2}$ and summing the first three non-zero terms of this series. Give your answer to $\textbf{four}$ significant figures. I'm stuck on $\textbf{a.1}$ . The way the question is formulated makes me think you're not supposed to use the actual derivatives of $\arcsin$ to solve it, but I can't figure out how to do it. I know that $a_n = \frac{f^{(n)}(0)}{n!}$ ,so I was thinking that if I can find a formula for the nth derivative of $f(x)$ , I should be good to go.  I know the derivative of $f(x)$ : $f^\prime(x) = \frac{d}{dx}\arcsin x = \frac{1}{\sqrt{1- x^2}}$ . From here, I can easily also find the second, third, etc. derivatives. However, when I try to come up with a formla for the $\textit{nth}$ derivative, I have a problem. I came up with the following formula: $\frac{d^n}{dx^n}\arcsin x = (-1)^n \prod\limits_{k = 0}^n \left(\frac{1}{2} - k\right)$ . Unfortunately, I have no idea how to proveed from here, as I don't know how to evaluate the product $\prod\limits_{k = 0}^n \left(\frac{1}{2} - k\right)$ . Anyway, I don't think this is the right approrach, as my textbook hasn't dealt with products yet, only sums. Can anyone help with $\textbf{a.1}$ ?","I need help with a problem. For context, the section of the textbook the problem is in is about power series. Note that the textbook uses the convention that represents the th derivative of , and I'll now state the problem exactly as stated in the textbook: Consider the function defined by , for . The derivatives of satisfy the equation , for The coefficient of in the Maclaurin series for is denoted by . You may assume that the series only contains odd powers of . Show that, for Given that , find an expression for in terms of , valid for odd Find the radius of convergence of this Maclaurin series. Find an approximate value for by putting and summing the first three non-zero terms of this series. Give your answer to significant figures. I'm stuck on . The way the question is formulated makes me think you're not supposed to use the actual derivatives of to solve it, but I can't figure out how to do it. I know that ,so I was thinking that if I can find a formula for the nth derivative of , I should be good to go.  I know the derivative of : . From here, I can easily also find the second, third, etc. derivatives. However, when I try to come up with a formla for the derivative, I have a problem. I came up with the following formula: . Unfortunately, I have no idea how to proveed from here, as I don't know how to evaluate the product . Anyway, I don't think this is the right approrach, as my textbook hasn't dealt with products yet, only sums. Can anyone help with ?","f^{(n)} n f f^{(0)}(x) = f(x). f f(x) = \arcsin x \lvert x \rvert \leq 1 f(x) 
(1 - x^2)f^{(n + 2)}(x) - (2n + 1)xf^{(n + 1)}(x) - n^2 f^{(n)}(x) = 0 n \geq 1.
 x^n f(x) a_n x \textbf{a.1)} n \geq 1, (n+1)(n+2)a_{n+2} = n^2 a_n. \textbf{a.2}) a_1 = 1 a_n n n \geq 3. \textbf{b}) \textbf{c}) \pi x = \frac{1}{2} \textbf{four} \textbf{a.1} \arcsin a_n = \frac{f^{(n)}(0)}{n!} f(x) f(x) f^\prime(x) = \frac{d}{dx}\arcsin x = \frac{1}{\sqrt{1- x^2}} \textit{nth} \frac{d^n}{dx^n}\arcsin x = (-1)^n \prod\limits_{k = 0}^n \left(\frac{1}{2} - k\right) \prod\limits_{k = 0}^n \left(\frac{1}{2} - k\right) \textbf{a.1}","['calculus', 'sequences-and-series', 'derivatives', 'taylor-expansion']"
50,Leibniz integral rule for higher order derivatives,Leibniz integral rule for higher order derivatives,,"Just wondering if there is a version of Leibniz integral rule for higher order derivatives. Specifically, I want to evaluate the differential $$ \frac{d^k}{dt^k}\int_0^t f(t - \tau) g(\tau) d\tau $$ at $t = 0$ for arbitrary integer $k$ . Edit The Leibniz rule provides $$ \frac{d}{dt}\int_0^t f(t - \tau) g(\tau) d\tau = f(0) g(t) + \int_0^t f'(t - \tau) g(\tau) d\tau. $$ Then, $$ \frac{d^2}{dt^2}\int_0^t f(t - \tau) g(\tau) d\tau = f(0) g'(t) + f'(0) g(t) + \int_0^t f''(t - \tau) g(\tau) d\tau, $$ $$ \frac{d^3}{dt^3}\int_0^t f(t - \tau) g(\tau) d\tau = f(0) g''(t) + f'(0) g'(t) + f''(0) g(t) + \int_0^t f'''(t - \tau) g(\tau) d\tau, $$ etc. Eventually, $$ \frac{d^k}{dt^k}\int_0^t f(t - \tau) g(\tau) d\tau = f(0) g^{(k - 1)}(t) + f'(0) g^{(k - 2)}(t) + ... + f^{(k - 1)}(0)g(t) + \int_0^t f^{(k)}(t - \tau) g(\tau) d\tau. $$ Does $f(0) g^{(k - 1)}(t) + f'(0) g^{(k - 2)}(t) + ... + f^{(k - 1)}(0)g(t)$ stand for a simple expression. Apparently, $[f(t - \tau) g(\tau)]^{(k - 1)}_{t = \tau}$ is not what it should look like. Edit 2 What I have encountered so far is as follows: $$ \frac{d^k}{dt^k}\int_0^t f(t - \tau) g(\tau) d\tau = \sum_{n = 1}^k f^{(n - 1)}(0) g^{(k - n)}(t) + \int_0^t f^{(k)}(t - \tau) g(\tau) d\tau. $$ Thus, evaluating both sides at $t = 0$ , we get $$ \left[\frac{d^k}{dt^k}\int_0^t f(t - \tau) g(\tau) d\tau\right]\bigg|_{t = 0} = \sum_{n = 1}^k f^{(n - 1)}(0) g^{(k - n)}(0). $$ Can't encounter what the right hand side is.","Just wondering if there is a version of Leibniz integral rule for higher order derivatives. Specifically, I want to evaluate the differential at for arbitrary integer . Edit The Leibniz rule provides Then, etc. Eventually, Does stand for a simple expression. Apparently, is not what it should look like. Edit 2 What I have encountered so far is as follows: Thus, evaluating both sides at , we get Can't encounter what the right hand side is.","
\frac{d^k}{dt^k}\int_0^t f(t - \tau) g(\tau) d\tau
 t = 0 k  \frac{d}{dt}\int_0^t f(t - \tau) g(\tau) d\tau = f(0) g(t) + \int_0^t f'(t - \tau) g(\tau) d\tau.   \frac{d^2}{dt^2}\int_0^t f(t - \tau) g(\tau) d\tau = f(0) g'(t) + f'(0) g(t) + \int_0^t f''(t - \tau) g(\tau) d\tau,   \frac{d^3}{dt^3}\int_0^t f(t - \tau) g(\tau) d\tau = f(0) g''(t) + f'(0) g'(t) + f''(0) g(t) + \int_0^t f'''(t - \tau) g(\tau) d\tau,   \frac{d^k}{dt^k}\int_0^t f(t - \tau) g(\tau) d\tau = f(0) g^{(k - 1)}(t) + f'(0) g^{(k - 2)}(t) + ... + f^{(k - 1)}(0)g(t) + \int_0^t f^{(k)}(t - \tau) g(\tau) d\tau.  f(0) g^{(k - 1)}(t) + f'(0) g^{(k - 2)}(t) + ... + f^{(k - 1)}(0)g(t) [f(t - \tau) g(\tau)]^{(k - 1)}_{t = \tau} 
\frac{d^k}{dt^k}\int_0^t f(t - \tau) g(\tau) d\tau = \sum_{n = 1}^k f^{(n - 1)}(0) g^{(k - n)}(t) + \int_0^t f^{(k)}(t - \tau) g(\tau) d\tau.
 t = 0 
\left[\frac{d^k}{dt^k}\int_0^t f(t - \tau) g(\tau) d\tau\right]\bigg|_{t = 0} = \sum_{n = 1}^k f^{(n - 1)}(0) g^{(k - n)}(0).
","['integration', 'derivatives', 'definite-integrals']"
51,Finding the derivative of $y= (\ln x)^2$,Finding the derivative of,y= (\ln x)^2,"In my math class, we are beginning to find derivatives of more complex functions. I’ve been trying questions from my textbook as practice. Here are two of them that I’m trying out: $y=(\ln x)^2$ . First, we take the power rule. This would make it $2(\ln x)$ . Then you multiply it by the derivative of the inside function right? $\ln x$ ’s derivative is $1/x$ . So, multiplied by $1/x$ . This gives us $\frac{2\ln x}x$ . This doesn’t seem correct so I’m a little confused. $y=\frac{3}{\sqrt{2x+1}}$ First, we get the denominator to the top. We get $y=3(2x+1)^{-1/2}$ . Then, we use the power rule. $y=3(x+1/2)$ . Then we multiply by the derivative again. Which is $2$ , I believe. This gives us $y=6(2x+1)$ . Again, this does not seem correct and I’m confused. I feel that my mistakes may be from a mix up of steps but I’m not exactly sure where in my process I went wrong.","In my math class, we are beginning to find derivatives of more complex functions. I’ve been trying questions from my textbook as practice. Here are two of them that I’m trying out: . First, we take the power rule. This would make it . Then you multiply it by the derivative of the inside function right? ’s derivative is . So, multiplied by . This gives us . This doesn’t seem correct so I’m a little confused. First, we get the denominator to the top. We get . Then, we use the power rule. . Then we multiply by the derivative again. Which is , I believe. This gives us . Again, this does not seem correct and I’m confused. I feel that my mistakes may be from a mix up of steps but I’m not exactly sure where in my process I went wrong.",y=(\ln x)^2 2(\ln x) \ln x 1/x 1/x \frac{2\ln x}x y=\frac{3}{\sqrt{2x+1}} y=3(2x+1)^{-1/2} y=3(x+1/2) 2 y=6(2x+1),['derivatives']
52,Partial derivative of the likelihood function respect to $\sigma^2$,Partial derivative of the likelihood function respect to,\sigma^2,"I am having problem doing the partial derivative of the likelihood function which is $L(\mu,\sigma^2)=\frac{1}{\sigma\sqrt{2\pi}^n}\times \exp{(-\frac{1}{2\sigma^2}\sum(x_i-\mu)^2)}$ If the first part has solved that the $\hat{\mu}$ is $\bar{x}$ and plug this to the $L(x,\mu,\sigma)$ I wonder may I ask how to make the partial derivative to the $\sigma^2$ and the answer is $-n/2\sigma^2+1/2\sigma^4\times\sum(x_i-\mu)^2$ Thank you! I guess I did something wrong so I tried two times but did not get the above answer. Appreciated!",I am having problem doing the partial derivative of the likelihood function which is If the first part has solved that the is and plug this to the I wonder may I ask how to make the partial derivative to the and the answer is Thank you! I guess I did something wrong so I tried two times but did not get the above answer. Appreciated!,"L(\mu,\sigma^2)=\frac{1}{\sigma\sqrt{2\pi}^n}\times \exp{(-\frac{1}{2\sigma^2}\sum(x_i-\mu)^2)} \hat{\mu} \bar{x} L(x,\mu,\sigma) \sigma^2 -n/2\sigma^2+1/2\sigma^4\times\sum(x_i-\mu)^2",['derivatives']
53,Chain rule for derivative of a norm,Chain rule for derivative of a norm,,"Suppose that $A$ is an $M \times N$ matrix, $x$ is an $N \times 1$ vector and $b$ is an $M \times1$ vector. I want to compute $\frac{d}{dx}||Ax+b||^2_{2}$ . According to this link , the answer should be: $$2A^{T}(Ax+b)$$ . However, the chain rule gives me the transpose of this expression: $$\frac{d\|Ax+b\|^2_{2}}{dAx+b}. \frac{d(Ax+b)}{dx}=2(Ax+b)^{T}A$$ Which answer is the correct one?","Suppose that is an matrix, is an vector and is an vector. I want to compute . According to this link , the answer should be: . However, the chain rule gives me the transpose of this expression: Which answer is the correct one?",A M \times N x N \times 1 b M \times1 \frac{d}{dx}||Ax+b||^2_{2} 2A^{T}(Ax+b) \frac{d\|Ax+b\|^2_{2}}{dAx+b}. \frac{d(Ax+b)}{dx}=2(Ax+b)^{T}A,"['calculus', 'derivatives', 'normed-spaces', 'chain-rule']"
54,Find the radius of a cylinder of given volume V if its surface area is a minimum.,Find the radius of a cylinder of given volume V if its surface area is a minimum.,,"this question is driving me crazy as I'm not sure how they've got the answer. The surface area is given as $S = 2\pi r^2 + \frac {1}{50r} $ and they are asking for the value of r for which S is minimum. The derivative of this (I hope!) is $4\pi r - \frac {1}{50r^2}$ Then to find the value of r when S is a minimum I presume you set the derivative to equal $0$. The book shows the value $200π - \frac 13 $ but I'm not sure how they've got this figure from setting the derivative to $0$. Any insight would be appreciated! Edit: My bad, answer is raised to negative $ \frac {1}{3}$, you live, you learn. Thanks for pointing this out.","this question is driving me crazy as I'm not sure how they've got the answer. The surface area is given as $S = 2\pi r^2 + \frac {1}{50r} $ and they are asking for the value of r for which S is minimum. The derivative of this (I hope!) is $4\pi r - \frac {1}{50r^2}$ Then to find the value of r when S is a minimum I presume you set the derivative to equal $0$. The book shows the value $200π - \frac 13 $ but I'm not sure how they've got this figure from setting the derivative to $0$. Any insight would be appreciated! Edit: My bad, answer is raised to negative $ \frac {1}{3}$, you live, you learn. Thanks for pointing this out.",,['derivatives']
55,Is $\limsup |C_n|^{\frac{1}{n}} = \limsup |C_{n+j}|^{\frac{1}{n}}? $ where $j\geq0 $,Is  where,\limsup |C_n|^{\frac{1}{n}} = \limsup |C_{n+j}|^{\frac{1}{n}}?  j\geq0 ,$C_n$ is a sequence of complex numbers. I would like to prove that the radius of convergence of a differentiated power series is the same as the original series. i.e. $\limsup |(n+j)(\cdots)(n+1)C_{n+j}|^{\frac{1}{n}} = \limsup |C_n|^{\frac{1}{n}}$ I know that $\limsup |(n+j)(\cdots)(n+1)C_{n+j}|^{\frac{1}{n}} = \limsup|C_{n+j}|^{\frac{1}{n}}$ since $\lim((n+j)(\cdots)(n+1))^{\frac{1}{n}}=1$ But I am stuck at the above question. Any help?,$C_n$ is a sequence of complex numbers. I would like to prove that the radius of convergence of a differentiated power series is the same as the original series. i.e. $\limsup |(n+j)(\cdots)(n+1)C_{n+j}|^{\frac{1}{n}} = \limsup |C_n|^{\frac{1}{n}}$ I know that $\limsup |(n+j)(\cdots)(n+1)C_{n+j}|^{\frac{1}{n}} = \limsup|C_{n+j}|^{\frac{1}{n}}$ since $\lim((n+j)(\cdots)(n+1))^{\frac{1}{n}}=1$ But I am stuck at the above question. Any help?,,"['sequences-and-series', 'derivatives', 'power-series', 'limsup-and-liminf']"
56,Finding the $n$-th derivative of $a_nx^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0$,Finding the -th derivative of,n a_nx^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0,"Let $f(x)= a_nx^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0$.   I am trying to find $f^n(x)$. By applying the power rule $n$ times, I get this $$f^n(x)=a_{n}(n\cdot n)x^{n-n}+\cdots+ a_1$$ which I think can be simplified to $$f^n(x)=a_{n}n^2+\cdots+ a_1$$ However, I don't think I have the correct answer, as my exercise book is telling me the answer is. $$a_n\,n\cdot(n-1)\cdot\,\cdots\,\cdot 1$$ What did I do wrong? I'm under the impression I have done multiple mistakes.","Let $f(x)= a_nx^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0$.   I am trying to find $f^n(x)$. By applying the power rule $n$ times, I get this $$f^n(x)=a_{n}(n\cdot n)x^{n-n}+\cdots+ a_1$$ which I think can be simplified to $$f^n(x)=a_{n}n^2+\cdots+ a_1$$ However, I don't think I have the correct answer, as my exercise book is telling me the answer is. $$a_n\,n\cdot(n-1)\cdot\,\cdots\,\cdot 1$$ What did I do wrong? I'm under the impression I have done multiple mistakes.",,['derivatives']
57,anti-derivative not differentiable at any point,anti-derivative not differentiable at any point,,"Reading about primitives and anti-derivatives, I noticed that primitive functions of non-continuous functions are not differentiable at some point, but the set of non-differentiability is often negligible. I tried to think of a function horrible enough to  get a non-differentiable antiderivative, and I found some with non-negligible set of non-differentiability points. But I never found an antiderivative that is nowhere differentiable.  Can you find one?","Reading about primitives and anti-derivatives, I noticed that primitive functions of non-continuous functions are not differentiable at some point, but the set of non-differentiability is often negligible. I tried to think of a function horrible enough to  get a non-differentiable antiderivative, and I found some with non-negligible set of non-differentiability points. But I never found an antiderivative that is nowhere differentiable.  Can you find one?",,"['integration', 'derivatives', 'lebesgue-measure', 'continuity']"
58,Trigonometric functions differentiation with number inside the argument part.,Trigonometric functions differentiation with number inside the argument part.,,"I had a question regarding the differentiation of trigonometric functions. To be a bit more specific, I'm currently studying a chapter about vector functions and vector calculus for an engineering mathematics course I'm taking at school. There is a rather simple detail that I don't understand though for an example question in the textbook. In the particular example question, the vector function is given as: $$\mathbf{r}(t) = \cos 2t\ \mathbf{i} + \sin t\ \mathbf{j}$$ And the differentiation of the function is apparently as follows: $$\mathbf{r^\prime}(t) = -2\sin 2t\ \mathbf{i} + \cos t\ \mathbf{j}$$ What I'm having trouble understanding is why the $-2$ has been added to the front of the $\sin$ for the differential equation? I thought that $$\frac{d}{dx}(\cos{x}) = -\sin{x}$$ Thank you.","I had a question regarding the differentiation of trigonometric functions. To be a bit more specific, I'm currently studying a chapter about vector functions and vector calculus for an engineering mathematics course I'm taking at school. There is a rather simple detail that I don't understand though for an example question in the textbook. In the particular example question, the vector function is given as: $$\mathbf{r}(t) = \cos 2t\ \mathbf{i} + \sin t\ \mathbf{j}$$ And the differentiation of the function is apparently as follows: $$\mathbf{r^\prime}(t) = -2\sin 2t\ \mathbf{i} + \cos t\ \mathbf{j}$$ What I'm having trouble understanding is why the $-2$ has been added to the front of the $\sin$ for the differential equation? I thought that $$\frac{d}{dx}(\cos{x}) = -\sin{x}$$ Thank you.",,"['trigonometry', 'derivatives']"
59,Differentiate an exponential operator,Differentiate an exponential operator,,"I have an operator defined as: $$ \exp \Bigg[  \lambda \ \frac{\partial}{\partial R} \Bigg] $$ I'm trying to optimize this with respect to the parameter $\lambda$. In other words, I'm trying to get an expression for $$ \frac{\partial}{\partial \lambda} \Bigg[\exp \Big[  \lambda \  \frac{\partial}{\partial R} \Big] \Bigg]$$ I read about Sneddon's formula but that doesn't seem to solve the problem. Any suggestions on how I should approach this?","I have an operator defined as: $$ \exp \Bigg[  \lambda \ \frac{\partial}{\partial R} \Bigg] $$ I'm trying to optimize this with respect to the parameter $\lambda$. In other words, I'm trying to get an expression for $$ \frac{\partial}{\partial \lambda} \Bigg[\exp \Big[  \lambda \  \frac{\partial}{\partial R} \Big] \Bigg]$$ I read about Sneddon's formula but that doesn't seem to solve the problem. Any suggestions on how I should approach this?",,"['derivatives', 'operator-algebras', 'chain-rule']"
60,Derivative of the trace of the product of a matrix and its transpose,Derivative of the trace of the product of a matrix and its transpose,,"I googled and found that the derivative of the trace of the product: $$\frac{d}{dX} \mbox{Trace} (X^TX) = 2X$$ But I can't find: $$\frac{d}{dX} \mbox{Trace} (XX^T)$$ I don't major in mathematics, so I don't know how to derive this. Could anyone help me out?","I googled and found that the derivative of the trace of the product: $$\frac{d}{dX} \mbox{Trace} (X^TX) = 2X$$ But I can't find: $$\frac{d}{dX} \mbox{Trace} (XX^T)$$ I don't major in mathematics, so I don't know how to derive this. Could anyone help me out?",,"['matrices', 'derivatives', 'matrix-calculus', 'trace', 'scalar-fields']"
61,Does equality of (weak) gradients imply equality of functions up to a constant?,Does equality of (weak) gradients imply equality of functions up to a constant?,,"This seems like a pretty basic question, but I can't figure it out. Suppose we have two measurable functions, $u$ and $v$, whose (weak) partial derivatives are all well defined, and we have $\partial_i u = \partial_i v$ for each $i$. Can we then conclude that $u=v+C$ for some constant $C$? Note: I'm mainly interested in the weak case but I couldn't think of a proof in the classical setting either.","This seems like a pretty basic question, but I can't figure it out. Suppose we have two measurable functions, $u$ and $v$, whose (weak) partial derivatives are all well defined, and we have $\partial_i u = \partial_i v$ for each $i$. Can we then conclude that $u=v+C$ for some constant $C$? Note: I'm mainly interested in the weak case but I couldn't think of a proof in the classical setting either.",,"['derivatives', 'partial-derivative', 'weak-derivatives']"
62,"Expansion of $ e^{\int f(t)\,dt} $",Expansion of," e^{\int f(t)\,dt} ","What is the correct expansion of $ e^{\int f(t)\,dt} $ ? Is it the following? $$ \begin{split} e^{\int f(t)\,dt}  & \simeq 1+\int f(t)\,dt + \frac{1}{2}\left( \int f(t)\,dt\right)^2\\  & =1+\int f(t)\,dt + \frac{1}{2}\iint f(t) f(t')\,dt\,dt' \end{split} $$ If yes, what are the conditions in the integral that this is true?","What is the correct expansion of $ e^{\int f(t)\,dt} $ ? Is it the following? $$ \begin{split} e^{\int f(t)\,dt}  & \simeq 1+\int f(t)\,dt + \frac{1}{2}\left( \int f(t)\,dt\right)^2\\  & =1+\int f(t)\,dt + \frac{1}{2}\iint f(t) f(t')\,dt\,dt' \end{split} $$ If yes, what are the conditions in the integral that this is true?",,"['calculus', 'integration', 'derivatives', 'taylor-expansion', 'exponential-function']"
63,Does dy/dx change depending on the setup of the equation?,Does dy/dx change depending on the setup of the equation?,,So I noticed that when I found $dy/dx$ of $$x\sin(y)=1$$ I got $$dy/dx=-\tan(y)/x$$ where as when I try to find $dy/dx$ of $$\sin(y)=1/x$$ I get $$dy/dx=-\frac1{x^2\cos(y)}$$Shouldn't these two equations yield the same result since they are the same equation but algebraically manipulated before differentiating. When I drew the slope fields of the two graphs using a computer they yielded two different graphs. Why is this the case?,So I noticed that when I found $dy/dx$ of $$x\sin(y)=1$$ I got $$dy/dx=-\tan(y)/x$$ where as when I try to find $dy/dx$ of $$\sin(y)=1/x$$ I get $$dy/dx=-\frac1{x^2\cos(y)}$$Shouldn't these two equations yield the same result since they are the same equation but algebraically manipulated before differentiating. When I drew the slope fields of the two graphs using a computer they yielded two different graphs. Why is this the case?,,"['derivatives', 'implicit-differentiation']"
64,"Show that $f$ is differentiable and determine $f'(t)$, for all $t \in I$. (Differentiable path)","Show that  is differentiable and determine , for all . (Differentiable path)",f f'(t) t \in I,"Let $X: I \to \mathbb{R}^{n^{2}}$ a differentiable path whose values ​​are matrices $n \times n$. Fixed $k \in \mathbb{N}$, let $f: I \to \mathbb{R}^{n^{2}}$ the path given by the rule $f(t) = X(t)^{k}$. Show that $f$ is differentiable and determine $f'(t)$, for all $t \in I$. Since $X$ is differentiable, we have $$X(a+t) - X(a) = X'(a) + r(t)$$ where $\lim_{t \to 0} \frac{r(t)}{t} = 0$, with $a \in I$. My idea is find $v$ such that $$r(t) = f(a+t) - f(a) - v$$ implies $\lim_{t \to 0} \frac{r(t)}{t} = 0$ for show that $$\lim_{t \to 0} \frac{X(a+t)^{k} - X(a)^{k}-v}{t} = 0.$$ I thought $v$ could be something that would allow you to write $X(a+t)^{k} - X(a)^{k} - v = (X(a+t) - X(a))^{k}$ or $v = kX(a+t)^{k-1}\cdot X'(t)$, but I couldn't conclude anything. Thanks for the any hint!","Let $X: I \to \mathbb{R}^{n^{2}}$ a differentiable path whose values ​​are matrices $n \times n$. Fixed $k \in \mathbb{N}$, let $f: I \to \mathbb{R}^{n^{2}}$ the path given by the rule $f(t) = X(t)^{k}$. Show that $f$ is differentiable and determine $f'(t)$, for all $t \in I$. Since $X$ is differentiable, we have $$X(a+t) - X(a) = X'(a) + r(t)$$ where $\lim_{t \to 0} \frac{r(t)}{t} = 0$, with $a \in I$. My idea is find $v$ such that $$r(t) = f(a+t) - f(a) - v$$ implies $\lim_{t \to 0} \frac{r(t)}{t} = 0$ for show that $$\lim_{t \to 0} \frac{X(a+t)^{k} - X(a)^{k}-v}{t} = 0.$$ I thought $v$ could be something that would allow you to write $X(a+t)^{k} - X(a)^{k} - v = (X(a+t) - X(a))^{k}$ or $v = kX(a+t)^{k-1}\cdot X'(t)$, but I couldn't conclude anything. Thanks for the any hint!",,"['real-analysis', 'matrices', 'derivatives']"
65,Another corollary of the Lagrange's Mean Value Theorem,Another corollary of the Lagrange's Mean Value Theorem,,"I'm having trouble at proving the following theorem, stated as a corollary of the Lagrange's Mean Value Theorem in the book I'm reading (""Curso de Análise"", Elon Lages Lima): Let $f: \left(a, b\right) \rightarrow \mathbb{R}$ be a differentiable function except, possibly, at $c \in \left(a, b\right)$, with $f$ continuous at $c$. If the limit: $$\lim_{x \rightarrow c} f'(x)=L$$ exists, then $f'(c)$ exists and its value is $f'(c)=L$. My attempt at proving this was: ""As long as $c \neq a$ and $c\neq b$, then there exists $A \in \left(a, c\right), B \in \left(c, b\right)$ such that $f$ is continuous at $(A, c)\cup(c,B)$ (because $f$ is differentiable in these open intervals), $f$ is continuous at $A$, $B$ and $c$ (because $f$ is differentiable at $A$ and $B$, when the intervals under consideration are $(a,c)$ and $(c,b)$ and $f$ is supposedly continuous at $c$). Then $f$ is (uniformly) continuous in $\left[A,B\right]$. In particular, $f$ is uniformly continuous in every closed interval contained in $\left[A,B\right]$. Let $\epsilon > 0$ be given, and let's keep fixed a corresponding $\delta > 0$ which satisfies the uniform continuity. Then $f$ is (uniformly) continuous at $[c-\delta, c]$ and $[c, c+ \delta]$ and differentiable at $(c-\delta, c)$ and $(c, c+ \delta)$, for $\epsilon$ small enough (which makes $\delta$ to be small enough and the intervals $[c-\delta, c]$ and $[c, c+ \delta]$ to be contained in $[A,B]$). By the Lagrange's Mean Value Theorem, we have: $$\exists c_1 \in (c- \delta, c): \ f'(c_1)[c-(c-\delta)]=\delta f'(c_1)=f(c)-f(c - \delta)$$ $$\exists c_2 \in (c, c+ \delta): \ f'(c_2)[(c+\delta)-c]=\delta f'(c_2)=f(c + \delta)- f(c)$$ By uniform continuity of $f$, $\epsilon \rightarrow 0 \implies \delta \rightarrow 0$ and, as $c\in (a,b)$ is fixed, $c_1=c_1(\delta(\epsilon), f), c_2=c_2(\delta(\epsilon),f )$, with: $$\lim_{\epsilon \rightarrow 0} c_1(\delta(\epsilon), f)= c=\lim_{\epsilon \rightarrow 0} c_2(\delta(\epsilon),f)$$ Then, we finally have: $$L=\lim_{x \rightarrow c^{-}} f'(x)=\lim_{\epsilon \rightarrow 0} f'(c_1(\delta(\epsilon), f))=\lim_{\epsilon \rightarrow 0} \frac{f(c) - f(c-\delta(\epsilon))}{\delta}=\lim_{x \rightarrow c^{-}} \frac{f(c) - f(x)}{c-x}$$ $$L=\lim_{x \rightarrow c^{+}} f'(x)=\lim_{\epsilon \rightarrow 0} f'(c_2(\delta(\epsilon), f))=\lim_{\epsilon \rightarrow 0} \frac{f(c+\delta(\epsilon))-f(c)}{\delta}=\lim_{x \rightarrow c^{-}} \frac{f(x) - f(c)}{x-c}$$ Since both lateral limits exist and are equal, the theorem is proved and: $$L=\lim_{x \rightarrow c} \frac{f(x)-f(c)}{x-c}\equiv f'(c)$$ q.e.d."" Am I missing anything? Or is the theorem correctly proved?","I'm having trouble at proving the following theorem, stated as a corollary of the Lagrange's Mean Value Theorem in the book I'm reading (""Curso de Análise"", Elon Lages Lima): Let $f: \left(a, b\right) \rightarrow \mathbb{R}$ be a differentiable function except, possibly, at $c \in \left(a, b\right)$, with $f$ continuous at $c$. If the limit: $$\lim_{x \rightarrow c} f'(x)=L$$ exists, then $f'(c)$ exists and its value is $f'(c)=L$. My attempt at proving this was: ""As long as $c \neq a$ and $c\neq b$, then there exists $A \in \left(a, c\right), B \in \left(c, b\right)$ such that $f$ is continuous at $(A, c)\cup(c,B)$ (because $f$ is differentiable in these open intervals), $f$ is continuous at $A$, $B$ and $c$ (because $f$ is differentiable at $A$ and $B$, when the intervals under consideration are $(a,c)$ and $(c,b)$ and $f$ is supposedly continuous at $c$). Then $f$ is (uniformly) continuous in $\left[A,B\right]$. In particular, $f$ is uniformly continuous in every closed interval contained in $\left[A,B\right]$. Let $\epsilon > 0$ be given, and let's keep fixed a corresponding $\delta > 0$ which satisfies the uniform continuity. Then $f$ is (uniformly) continuous at $[c-\delta, c]$ and $[c, c+ \delta]$ and differentiable at $(c-\delta, c)$ and $(c, c+ \delta)$, for $\epsilon$ small enough (which makes $\delta$ to be small enough and the intervals $[c-\delta, c]$ and $[c, c+ \delta]$ to be contained in $[A,B]$). By the Lagrange's Mean Value Theorem, we have: $$\exists c_1 \in (c- \delta, c): \ f'(c_1)[c-(c-\delta)]=\delta f'(c_1)=f(c)-f(c - \delta)$$ $$\exists c_2 \in (c, c+ \delta): \ f'(c_2)[(c+\delta)-c]=\delta f'(c_2)=f(c + \delta)- f(c)$$ By uniform continuity of $f$, $\epsilon \rightarrow 0 \implies \delta \rightarrow 0$ and, as $c\in (a,b)$ is fixed, $c_1=c_1(\delta(\epsilon), f), c_2=c_2(\delta(\epsilon),f )$, with: $$\lim_{\epsilon \rightarrow 0} c_1(\delta(\epsilon), f)= c=\lim_{\epsilon \rightarrow 0} c_2(\delta(\epsilon),f)$$ Then, we finally have: $$L=\lim_{x \rightarrow c^{-}} f'(x)=\lim_{\epsilon \rightarrow 0} f'(c_1(\delta(\epsilon), f))=\lim_{\epsilon \rightarrow 0} \frac{f(c) - f(c-\delta(\epsilon))}{\delta}=\lim_{x \rightarrow c^{-}} \frac{f(c) - f(x)}{c-x}$$ $$L=\lim_{x \rightarrow c^{+}} f'(x)=\lim_{\epsilon \rightarrow 0} f'(c_2(\delta(\epsilon), f))=\lim_{\epsilon \rightarrow 0} \frac{f(c+\delta(\epsilon))-f(c)}{\delta}=\lim_{x \rightarrow c^{-}} \frac{f(x) - f(c)}{x-c}$$ Since both lateral limits exist and are equal, the theorem is proved and: $$L=\lim_{x \rightarrow c} \frac{f(x)-f(c)}{x-c}\equiv f'(c)$$ q.e.d."" Am I missing anything? Or is the theorem correctly proved?",,"['real-analysis', 'derivatives', 'uniform-continuity']"
66,Why is $\tan^2(y)=x^2$?,Why is ?,\tan^2(y)=x^2,"I'm following a proof in a book about the derivative of an arc tangent as follows: By definition, $y = \tan^{-1}(x) \Rightarrow \tan(y)=x$ Therefore, using implicit derivation: $$ \sec^2(y)\frac{dy}{dx}=1 \\ \frac{dy}{dx}=\frac{1}{\sec^2(y)}=\frac{1}{1+\tan^2(y)}=\frac{1}{1+x^2} $$ Thus completing the proof. I follow well the implicit derivation, and the trigonometric identity $\sec^2(x)=1+\tan^2(x)$, but I don't follow how did it jump to the conclusion that $\tan^2(y)=x^2$. I can't find a reference for this. What would be the reason for this? Thanks.","I'm following a proof in a book about the derivative of an arc tangent as follows: By definition, $y = \tan^{-1}(x) \Rightarrow \tan(y)=x$ Therefore, using implicit derivation: $$ \sec^2(y)\frac{dy}{dx}=1 \\ \frac{dy}{dx}=\frac{1}{\sec^2(y)}=\frac{1}{1+\tan^2(y)}=\frac{1}{1+x^2} $$ Thus completing the proof. I follow well the implicit derivation, and the trigonometric identity $\sec^2(x)=1+\tan^2(x)$, but I don't follow how did it jump to the conclusion that $\tan^2(y)=x^2$. I can't find a reference for this. What would be the reason for this? Thanks.",,"['calculus', 'trigonometry', 'derivatives']"
67,Derivative of $\mathrm{tr}(X^\top A) A$,Derivative of,\mathrm{tr}(X^\top A) A,"Let A be a constant matrix. Define $$ Y:= \mathrm{tr}(X^\top A) A $$ I want to find \begin{align} \frac{\partial Y}{\partial X} \label{A} \end{align} I know that  $$ \frac{\partial\,\mathrm{tr}(X^\top A)}{\partial X} = A \label{B} $$ Of course, I should be able to extend this to the case I want. But I am a bit confused as to how I should notate the result. What I did to find \eqref{A} was to vectorize both sides and then use the fact in \eqref{B}. $$ \mathrm{vec(Y)} = \mathrm{tr}(X^\top A)\mathrm{vec}(A) $$ I also knwo that I can write  $$ \mathrm{tr}(X^\top A) = \mathrm{vec}(A)^\top \mathrm{vec}(X) $$  So the desired the derivative should be $$ \mathrm{vec}(A) \otimes \mathrm{vec}(A) $$ Is this correct?.","Let A be a constant matrix. Define $$ Y:= \mathrm{tr}(X^\top A) A $$ I want to find \begin{align} \frac{\partial Y}{\partial X} \label{A} \end{align} I know that  $$ \frac{\partial\,\mathrm{tr}(X^\top A)}{\partial X} = A \label{B} $$ Of course, I should be able to extend this to the case I want. But I am a bit confused as to how I should notate the result. What I did to find \eqref{A} was to vectorize both sides and then use the fact in \eqref{B}. $$ \mathrm{vec(Y)} = \mathrm{tr}(X^\top A)\mathrm{vec}(A) $$ I also knwo that I can write  $$ \mathrm{tr}(X^\top A) = \mathrm{vec}(A)^\top \mathrm{vec}(X) $$  So the desired the derivative should be $$ \mathrm{vec}(A) \otimes \mathrm{vec}(A) $$ Is this correct?.",,"['matrices', 'derivatives', 'matrix-calculus']"
68,Are vectors and the derivative of $\left|x\right|$ related?,Are vectors and the derivative of  related?,\left|x\right|,"So, in our lesson about derivatives, we learnt that $$\frac{\mathrm{d}\left|x\right|}{\mathrm{d}x}=\frac{x}{\left|x\right|}$$ This is just an application of the chain rule, since $\left|x\right|=\sqrt{x^2}$. The above looks suspiciously similar to the formula for a unit vector, which is$$\hat{x}=\frac{\vec{x}}{\left|\vec{x}\right|}$$ and in fact, if $\vec{x}$ is a one dimensional vector (on the real number line), $x$ can be taken to be just a number, which gives us the first relation. Is it right to reason that this can be extended to the complex numbers as well? That would mean if $$x=a+bi$$ then $$\frac{\mathrm{d}\left|x\right|}{\mathrm{d}x}=\frac{a+bi}{\sqrt{a^2+b^2}}$$ or in general, $$\frac{\mathrm{d}\left|\vec{x}\right|}{\mathrm{d}x}=\hat{x}$$ Am I right? Or is there a flaw in my reasoning? I'm asking because it's just an observation. I would be glad if someone could explain this.","So, in our lesson about derivatives, we learnt that $$\frac{\mathrm{d}\left|x\right|}{\mathrm{d}x}=\frac{x}{\left|x\right|}$$ This is just an application of the chain rule, since $\left|x\right|=\sqrt{x^2}$. The above looks suspiciously similar to the formula for a unit vector, which is$$\hat{x}=\frac{\vec{x}}{\left|\vec{x}\right|}$$ and in fact, if $\vec{x}$ is a one dimensional vector (on the real number line), $x$ can be taken to be just a number, which gives us the first relation. Is it right to reason that this can be extended to the complex numbers as well? That would mean if $$x=a+bi$$ then $$\frac{\mathrm{d}\left|x\right|}{\mathrm{d}x}=\frac{a+bi}{\sqrt{a^2+b^2}}$$ or in general, $$\frac{\mathrm{d}\left|\vec{x}\right|}{\mathrm{d}x}=\hat{x}$$ Am I right? Or is there a flaw in my reasoning? I'm asking because it's just an observation. I would be glad if someone could explain this.",,"['calculus', 'derivatives', 'vectors']"
69,How can I push the derivative through the following sum?,How can I push the derivative through the following sum?,,"Let $f$ be a probability density and $F$ the corresponding distribution function. In other words $f$ is non-negative, integrates to 1, and $F' = f$. I'm interested in the following: $$ \frac{d}{dx}\sum_{i\in\mathbb{Z}}F(i+x)-F(i)\stackrel{?}{=}\sum_{i\in\mathbb{Z}}f(i+x) $$ I'm doubtful that this result holds in general. But what if I add assumptions such as that $f$ be Lipschitz-continuous?","Let $f$ be a probability density and $F$ the corresponding distribution function. In other words $f$ is non-negative, integrates to 1, and $F' = f$. I'm interested in the following: $$ \frac{d}{dx}\sum_{i\in\mathbb{Z}}F(i+x)-F(i)\stackrel{?}{=}\sum_{i\in\mathbb{Z}}f(i+x) $$ I'm doubtful that this result holds in general. But what if I add assumptions such as that $f$ be Lipschitz-continuous?",,"['probability', 'sequences-and-series', 'derivatives']"
70,Find derivative of $y=\sin^{-1}\frac{2x}{1+x^2}$,Find derivative of,y=\sin^{-1}\frac{2x}{1+x^2},"Find $\frac{dy}{dx}$ if $y=\sin^{-1}\frac{2x}{1+x^2}$ The solution is given as $\frac{2}{1+x^2}$. But is it a complete solution ? My Attempt $$ 2\tan^{-1}x=\begin{cases}\sin^{-1}\frac{2x}{1+x^2},\quad |x|\leq 1\\ \pi-\sin^{-1}\frac{2x}{1+x^2},\quad |x|>1 \;\&\; x>0\\ -\pi-\sin^{-1}\frac{2x}{1+x^2},\quad |x|>1 \;\&\;x>0\\ \end{cases}\\ \sin^{-1}\frac{2x}{1+x^2}=\begin{cases}2\tan^{-1}x,\quad |x|\leq 1\\ \pi-2\tan^{-1}x,\quad |x|>1 \;\&\; x>0\\ -\pi-2\tan^{-1}x,\quad |x|>1 \;\&\;x>0\\ \end{cases}\\ $$ Thus, $$ \frac{dy}{dx}=\frac{d}{dx}\bigg[\sin^{-1}\frac{2x}{1+x^2}\bigg]=\begin{cases} \frac{d}{dx}[2\tan^{-1}x]=\frac{2}{1+x^2},\quad |x|\leq 1\\ \frac{d}{dx}[\pm\pi-2\tan^{-1}x]=\frac{-2}{1+x^2},|x|>1  \end{cases} $$ Is it correct ?","Find $\frac{dy}{dx}$ if $y=\sin^{-1}\frac{2x}{1+x^2}$ The solution is given as $\frac{2}{1+x^2}$. But is it a complete solution ? My Attempt $$ 2\tan^{-1}x=\begin{cases}\sin^{-1}\frac{2x}{1+x^2},\quad |x|\leq 1\\ \pi-\sin^{-1}\frac{2x}{1+x^2},\quad |x|>1 \;\&\; x>0\\ -\pi-\sin^{-1}\frac{2x}{1+x^2},\quad |x|>1 \;\&\;x>0\\ \end{cases}\\ \sin^{-1}\frac{2x}{1+x^2}=\begin{cases}2\tan^{-1}x,\quad |x|\leq 1\\ \pi-2\tan^{-1}x,\quad |x|>1 \;\&\; x>0\\ -\pi-2\tan^{-1}x,\quad |x|>1 \;\&\;x>0\\ \end{cases}\\ $$ Thus, $$ \frac{dy}{dx}=\frac{d}{dx}\bigg[\sin^{-1}\frac{2x}{1+x^2}\bigg]=\begin{cases} \frac{d}{dx}[2\tan^{-1}x]=\frac{2}{1+x^2},\quad |x|\leq 1\\ \frac{d}{dx}[\pm\pi-2\tan^{-1}x]=\frac{-2}{1+x^2},|x|>1  \end{cases} $$ Is it correct ?",,"['derivatives', 'inverse-function']"
71,Fourier series of the inverse of a function,Fourier series of the inverse of a function,,"Suppose you have a function $F:[-\pi,\pi]\to [-\pi,\pi]$ that is strictly increasing, has a continuous derivative, and we know its Fourier coefficients (so we know also the Fourier coefficients of the derivative). Is there a way to find the Fourier series of the inverse function $F^{-1}$?  (Note: the inverse, not the reciprocal) I tried to compute them, but I get nested trigonometrical function like $\cos(n\cos(kx))$ and there seem not to be a wayout... My approach: suppose that  $$F(x) = a_0 + \sum a_n\cos(nx) + b_n\sin(nx) $$ and try to compute the coefficients of the inverse. If $F[-\pi,\pi] = [a,b]\subseteq [-\pi,\pi]$, then you get (up to constants) $$\int_a^b F^{-1}(x)\cos(nx) dx = \int_a^b F^{-1}(F(y))\cos(nF(y)) dF(y)$$ $$ = \int_{-\pi}^\pi y\cos(nF(y)) F'(y)dy. $$ Using the linearity of the integral, one can split the $F'$ into cosines and sines, so we have to compute, for example (and still up to constants) $$ \int_{-\pi}^\pi y\cos(nF(y)) \cos(my)dy. $$ and here is where I get stuck","Suppose you have a function $F:[-\pi,\pi]\to [-\pi,\pi]$ that is strictly increasing, has a continuous derivative, and we know its Fourier coefficients (so we know also the Fourier coefficients of the derivative). Is there a way to find the Fourier series of the inverse function $F^{-1}$?  (Note: the inverse, not the reciprocal) I tried to compute them, but I get nested trigonometrical function like $\cos(n\cos(kx))$ and there seem not to be a wayout... My approach: suppose that  $$F(x) = a_0 + \sum a_n\cos(nx) + b_n\sin(nx) $$ and try to compute the coefficients of the inverse. If $F[-\pi,\pi] = [a,b]\subseteq [-\pi,\pi]$, then you get (up to constants) $$\int_a^b F^{-1}(x)\cos(nx) dx = \int_a^b F^{-1}(F(y))\cos(nF(y)) dF(y)$$ $$ = \int_{-\pi}^\pi y\cos(nF(y)) F'(y)dy. $$ Using the linearity of the integral, one can split the $F'$ into cosines and sines, so we have to compute, for example (and still up to constants) $$ \int_{-\pi}^\pi y\cos(nF(y)) \cos(my)dy. $$ and here is where I get stuck",,"['derivatives', 'fourier-series', 'inverse-function']"
72,Once Continuously Differentiable?,Once Continuously Differentiable?,,"I'm hoping someone can clarify this because I can't seem to find a solid answer. I know functions can be continuously differentiable, but I just read in a textbook ""this applies to once continuously differentiable..."" but they don't give an example and google seems unhelpful. Am I right in assuming that once continuously differentiable is equivalent to continuously differentiable? If not can you please provide an example that clearly demonstrates the difference.","I'm hoping someone can clarify this because I can't seem to find a solid answer. I know functions can be continuously differentiable, but I just read in a textbook ""this applies to once continuously differentiable..."" but they don't give an example and google seems unhelpful. Am I right in assuming that once continuously differentiable is equivalent to continuously differentiable? If not can you please provide an example that clearly demonstrates the difference.",,"['real-analysis', 'derivatives']"
73,Prove $f'(\frac{x_1+2x_2}3)<1-a$ for two zeros $x_1<x_2$ of $f(x)=\ln x+x-ax^2$,Prove  for two zeros  of,f'(\frac{x_1+2x_2}3)<1-a x_1<x_2 f(x)=\ln x+x-ax^2,"Let $f(x) = \ln x + x - ax^2$, where $a > 0$. Suppose $x_1 < x_2$ are two zeros of $f(x)$, prove that$$ f'\left( \frac{x_1 + 2x_2}{3} \right) < 1 - a. $$ My try: Denote $x_3 = \dfrac{x_1 + 2x_2}{3}$. From $f(x_1) = f(x_2) = 0$, there is$$ \frac{\ln x_1 + x_1}{x_1^2} = \frac{\ln x_2 + x_2}{x_2^2} = a. $$ Also,$$ f'(x) = \frac{1}{x} + 1 - 2ax. $$ Thus\begin{align*} &\mathrel{\phantom{\Longleftrightarrow}} f'(x_3) < 1 - a \Longleftrightarrow 2ax_3^2 - ax_3 - 1 > 0 \Longleftrightarrow ax_3 (2x_3 - 1) > 1\\ &\Longleftrightarrow \left( \frac{1}{3} ax_1 + \frac{2}{3} ax_2 \right) \left( \frac{2}{3} x_1 + \frac{4}{3} x_2 - 1 \right) > 1\\ &\Longleftrightarrow \left( \frac{1}{3} \frac{\ln x_1 + x_1}{x_1} + \frac{2}{3} \frac{\ln x_2 + x_2}{x_2} \right) \left( \frac{2}{3} x_1 + \frac{4}{3} x_2 - 1 \right) > 1\\ &\Longleftrightarrow \left( \frac{\ln x_1}{x_1} + \frac{2\ln x_2}{x_2} + 3 \right) (2x_1 + 4x_2 - 3) > 9. \end{align*} Then I tried to prove the last inequality for arbitrary $x_1 < x_2$, but the expression has gotten really complicated so far and the logarithm terms make it even worse to handle. Is there a better approach to this question? Thanks in advance. Edit: From the comment of @MartinR and some experiment with the graph of $f$, it seems promising to prove that $0 < a < 1$ and $f'\left( \dfrac{x_1 + 2x_2}{3} \right) < 0$. Furthermore, it also seems true that $f'\left( \dfrac{x_1 + x_2}{2} \right) < 0$, which is stronger than the original inequality to be proved in that $f'$ is decreasing. So it needs proving that$$ f'\left( \frac{x_1 + x_2}{2} \right) = \frac{2}{x_1 + x_2} + 1 - a(x_1 + x_2) < 0. $$ Any ideas?","Let $f(x) = \ln x + x - ax^2$, where $a > 0$. Suppose $x_1 < x_2$ are two zeros of $f(x)$, prove that$$ f'\left( \frac{x_1 + 2x_2}{3} \right) < 1 - a. $$ My try: Denote $x_3 = \dfrac{x_1 + 2x_2}{3}$. From $f(x_1) = f(x_2) = 0$, there is$$ \frac{\ln x_1 + x_1}{x_1^2} = \frac{\ln x_2 + x_2}{x_2^2} = a. $$ Also,$$ f'(x) = \frac{1}{x} + 1 - 2ax. $$ Thus\begin{align*} &\mathrel{\phantom{\Longleftrightarrow}} f'(x_3) < 1 - a \Longleftrightarrow 2ax_3^2 - ax_3 - 1 > 0 \Longleftrightarrow ax_3 (2x_3 - 1) > 1\\ &\Longleftrightarrow \left( \frac{1}{3} ax_1 + \frac{2}{3} ax_2 \right) \left( \frac{2}{3} x_1 + \frac{4}{3} x_2 - 1 \right) > 1\\ &\Longleftrightarrow \left( \frac{1}{3} \frac{\ln x_1 + x_1}{x_1} + \frac{2}{3} \frac{\ln x_2 + x_2}{x_2} \right) \left( \frac{2}{3} x_1 + \frac{4}{3} x_2 - 1 \right) > 1\\ &\Longleftrightarrow \left( \frac{\ln x_1}{x_1} + \frac{2\ln x_2}{x_2} + 3 \right) (2x_1 + 4x_2 - 3) > 9. \end{align*} Then I tried to prove the last inequality for arbitrary $x_1 < x_2$, but the expression has gotten really complicated so far and the logarithm terms make it even worse to handle. Is there a better approach to this question? Thanks in advance. Edit: From the comment of @MartinR and some experiment with the graph of $f$, it seems promising to prove that $0 < a < 1$ and $f'\left( \dfrac{x_1 + 2x_2}{3} \right) < 0$. Furthermore, it also seems true that $f'\left( \dfrac{x_1 + x_2}{2} \right) < 0$, which is stronger than the original inequality to be proved in that $f'$ is decreasing. So it needs proving that$$ f'\left( \frac{x_1 + x_2}{2} \right) = \frac{2}{x_1 + x_2} + 1 - a(x_1 + x_2) < 0. $$ Any ideas?",,"['derivatives', 'inequality']"
74,"What is the approach to showing the differentiability of $F(s,f) = \int_{0}^{1}f(x+s)g'(x)dx$?",What is the approach to showing the differentiability of ?,"F(s,f) = \int_{0}^{1}f(x+s)g'(x)dx","Let $E$ be the set of functions $f: \mathbb{R} \rightarrow \mathbb{R}$, that are $\mathcal{C}^1$ and such that $\forall x \in \mathbb{R}$, $f(x+1)=f(x)$. Let $g \in E$ and $F: \mathbb{R} \times E \rightarrow \mathbb{R}$ such that $$F(s,f)= \int^{1}_{0}f(x+s)g'(x)dx $$   Show that $F$ is $\mathcal{C}^1$. Now, I found this exercise in past terms exams, so it might be possible that in order to solve it I require some notions that I haven't seen yet in my course. Anyway, I wonder, how should I approach this exercise? Is there a way to apply the Leibniz integral rule?","Let $E$ be the set of functions $f: \mathbb{R} \rightarrow \mathbb{R}$, that are $\mathcal{C}^1$ and such that $\forall x \in \mathbb{R}$, $f(x+1)=f(x)$. Let $g \in E$ and $F: \mathbb{R} \times E \rightarrow \mathbb{R}$ such that $$F(s,f)= \int^{1}_{0}f(x+s)g'(x)dx $$   Show that $F$ is $\mathcal{C}^1$. Now, I found this exercise in past terms exams, so it might be possible that in order to solve it I require some notions that I haven't seen yet in my course. Anyway, I wonder, how should I approach this exercise? Is there a way to apply the Leibniz integral rule?",,"['real-analysis', 'integration', 'derivatives', 'partial-derivative', 'differential']"
75,How to find the mean value for $\frac{1}{x^2+1}$?,How to find the mean value for ?,\frac{1}{x^2+1},"I am reading Courant's Differential and Integral Calculus. Here: Find the intermediate value $\xi $ of the mean value theorem for the following functions, and illustrate graphically: (d) $1/(x^2+1)$ I am having a little bit of trouble to find $\xi$ in $1/(x^2+1)$. I did the following: $$\frac{f(x_1)-f(x_2)}{x_2 - x_1}=-\frac{x_1+x_2}{\left(x_1^2+1\right) \left(x_2^2+1\right)}$$ So, there exists an $\xi\in [x_1,x_2]$ such that: $$-\frac{x_1+x_2}{\left(x_1^2+1\right) \left(x_2^2+1\right)}=f'(\xi)$$ And then, obviously I could compute the derivative for $1/(x^2+1)$ which gives me $-\frac{2 x}{\left(x^2+1\right)^2}$ and attempt to solve: $$-\frac{x_1+x_2}{\left(x_1^2+1\right) \left(x_2^2+1\right)}=-\frac{2 \xi}{\left(\xi^2+1\right)^2}$$ For $\xi$, but perhaps this would be ""too hard"" to do by hand, I suspect that there is something much simpler. I also guess that in general, any function which the derivative looks as the derivative of the function I provided, differentiating and solving for $\xi$ could be a nightmare. I have even tried on Mathematica and it gave me this: I may be missing something truly silly here, Courant gives a very simple answer in his book.","I am reading Courant's Differential and Integral Calculus. Here: Find the intermediate value $\xi $ of the mean value theorem for the following functions, and illustrate graphically: (d) $1/(x^2+1)$ I am having a little bit of trouble to find $\xi$ in $1/(x^2+1)$. I did the following: $$\frac{f(x_1)-f(x_2)}{x_2 - x_1}=-\frac{x_1+x_2}{\left(x_1^2+1\right) \left(x_2^2+1\right)}$$ So, there exists an $\xi\in [x_1,x_2]$ such that: $$-\frac{x_1+x_2}{\left(x_1^2+1\right) \left(x_2^2+1\right)}=f'(\xi)$$ And then, obviously I could compute the derivative for $1/(x^2+1)$ which gives me $-\frac{2 x}{\left(x^2+1\right)^2}$ and attempt to solve: $$-\frac{x_1+x_2}{\left(x_1^2+1\right) \left(x_2^2+1\right)}=-\frac{2 \xi}{\left(\xi^2+1\right)^2}$$ For $\xi$, but perhaps this would be ""too hard"" to do by hand, I suspect that there is something much simpler. I also guess that in general, any function which the derivative looks as the derivative of the function I provided, differentiating and solving for $\xi$ could be a nightmare. I have even tried on Mathematica and it gave me this: I may be missing something truly silly here, Courant gives a very simple answer in his book.",,"['real-analysis', 'derivatives']"
76,"Minimum distance from the points of the function $\frac{1}{4xy}$ to the point $(0, 0, 0)$",Minimum distance from the points of the function  to the point,"\frac{1}{4xy} (0, 0, 0)","I am trying to find the minimum distance from the points of the function $\large{\frac{1}{4xy}}$ to the point $(0, 0, 0)$. This appears to be a problem of Lagrange in which my condition: $C(x,y,z) = z - \frac{1}{4xy} = 0$, and my function would be $f(x,y,z) = \sqrt{x^2+y^2+z^2}$ or if i'm correct, it would be the same as the minimum value I get from using thee function as $f(x,y,z) = x^2+y^2+z^2$. If I do this, I would then have: $$ f(x,y,z,\lambda) = x^2 + y^2 + z^2 + \lambda(z-\frac{1}{4xy}) \\ = x^2 + y^2 + z^2 + \lambda z-\frac{1}{4xy} \lambda $$ Then findind the partial derivatives ($\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}, \frac{\partial f}{\partial \lambda}$) and solving for the values of $x, y, z, \lambda$ and the minimum value I get at the end would be my answer. Would that be the correct solution?","I am trying to find the minimum distance from the points of the function $\large{\frac{1}{4xy}}$ to the point $(0, 0, 0)$. This appears to be a problem of Lagrange in which my condition: $C(x,y,z) = z - \frac{1}{4xy} = 0$, and my function would be $f(x,y,z) = \sqrt{x^2+y^2+z^2}$ or if i'm correct, it would be the same as the minimum value I get from using thee function as $f(x,y,z) = x^2+y^2+z^2$. If I do this, I would then have: $$ f(x,y,z,\lambda) = x^2 + y^2 + z^2 + \lambda(z-\frac{1}{4xy}) \\ = x^2 + y^2 + z^2 + \lambda z-\frac{1}{4xy} \lambda $$ Then findind the partial derivatives ($\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}, \frac{\partial f}{\partial z}, \frac{\partial f}{\partial \lambda}$) and solving for the values of $x, y, z, \lambda$ and the minimum value I get at the end would be my answer. Would that be the correct solution?",,"['derivatives', 'partial-derivative', 'extreme-value-theorem']"
77,Chain rule proof by definition,Chain rule proof by definition,,"So i was trying to prove the following: Let $f \colon \mathbb{R}^n \rightarrow  \mathbb{R}^m $, $g \colon \mathbb{R}^m \rightarrow  \mathbb{R}^k $ and $h \colon \mathbb{R}^n \rightarrow  \mathbb{R}^k $ such that $h = g \circ f$. If $f$ is differentiable  a   t $p \in \mathbb{R}^n$ and $g$ is differentiable at $q = f(p) \in \mathbb{R}^m$ then $h$ is differentiable at $p$ and $Dh(p) = Dg(q) \circ Df(p)$ My attempt: As $f$ is differentiable at $p$ and $g$ so it is at $q$, we have \begin{align*} f(x) &= f(p) + Df(x - p) + R_f(x),\text{ with }\frac{\Vert R_f(x)\Vert}{\Vert x-p\Vert} \rightarrow 0\text{ as } x \rightarrow p \\ g(x) &= g(q) + Dg(x - q) + R_g(x),\text{ with }\frac{\Vert R_g(x)\Vert}{\Vert x-q\Vert} \rightarrow 0\text{ as } x \rightarrow q \end{align*} Then, \begin{align*} h(x)&= g(f(x)) = g(f(p)) + Dg(f(x) - f(p)) + R_g(f(x))   \\ &= g(f(p)) + Dg(Df(x-p) + R_f(x)) + R_g(f(x))  \\ &= (g \circ f)(p) + (Dg \circ Df)(x-p) + Dg(R_f(x)) + R_g(f(x))\end{align*} If $R(x) = Dg(R_f(x)) + R_g(f(x))$, proving $\frac{\Vert R(x)\Vert}{\Vert x - p\Vert} \rightarrow 0$ as $x \rightarrow p$ would be enough to complete the proof. \begin{align*} \frac{\Vert R(x)\Vert}{\Vert x - p\Vert} = \frac{\Vert Dg(R_f(x)) + R_g(f(x))\Vert}{\Vert x - p\Vert} \leq \frac{\Vert Dg(R_f(x))\Vert}{\Vert x - p\Vert} + \frac{\Vert R_g(f(x))\Vert}{\Vert x - p\Vert} \end{align*} So, i need to bound both terms. For the first one, i was thinking of $\frac{\Vert Dg(R_f(x))\Vert}{\Vert x - p\Vert} \leq \frac{\Vert D_g\Vert \Vert R_f(x)\Vert}{\Vert x - p\Vert} \leq \frac{\Vert D_g + \epsilon_1 \Vert \Vert R_f(x)\Vert}{\Vert x - p\Vert}$ for some $\epsilon_1 > 0$, and thus, taking $\delta_1 > 0$ such that $\frac{\Vert R_f(x)\Vert}{\Vert x - p\Vert} < \frac{\epsilon_2}{2(\Vert Dg + \epsilon_1\Vert)}$ if $\Vert x - p\Vert < \delta_1$ so $\frac{\Vert D_g + \epsilon_1 \Vert \Vert R_f(x)\Vert}{\Vert x - p\Vert} \leq \frac{\epsilon_2}{2}$ if $\Vert x - p \Vert < \delta_1$. Yet, i don't know how to bound the second term. Any help? Thanks in advance.","So i was trying to prove the following: Let $f \colon \mathbb{R}^n \rightarrow  \mathbb{R}^m $, $g \colon \mathbb{R}^m \rightarrow  \mathbb{R}^k $ and $h \colon \mathbb{R}^n \rightarrow  \mathbb{R}^k $ such that $h = g \circ f$. If $f$ is differentiable  a   t $p \in \mathbb{R}^n$ and $g$ is differentiable at $q = f(p) \in \mathbb{R}^m$ then $h$ is differentiable at $p$ and $Dh(p) = Dg(q) \circ Df(p)$ My attempt: As $f$ is differentiable at $p$ and $g$ so it is at $q$, we have \begin{align*} f(x) &= f(p) + Df(x - p) + R_f(x),\text{ with }\frac{\Vert R_f(x)\Vert}{\Vert x-p\Vert} \rightarrow 0\text{ as } x \rightarrow p \\ g(x) &= g(q) + Dg(x - q) + R_g(x),\text{ with }\frac{\Vert R_g(x)\Vert}{\Vert x-q\Vert} \rightarrow 0\text{ as } x \rightarrow q \end{align*} Then, \begin{align*} h(x)&= g(f(x)) = g(f(p)) + Dg(f(x) - f(p)) + R_g(f(x))   \\ &= g(f(p)) + Dg(Df(x-p) + R_f(x)) + R_g(f(x))  \\ &= (g \circ f)(p) + (Dg \circ Df)(x-p) + Dg(R_f(x)) + R_g(f(x))\end{align*} If $R(x) = Dg(R_f(x)) + R_g(f(x))$, proving $\frac{\Vert R(x)\Vert}{\Vert x - p\Vert} \rightarrow 0$ as $x \rightarrow p$ would be enough to complete the proof. \begin{align*} \frac{\Vert R(x)\Vert}{\Vert x - p\Vert} = \frac{\Vert Dg(R_f(x)) + R_g(f(x))\Vert}{\Vert x - p\Vert} \leq \frac{\Vert Dg(R_f(x))\Vert}{\Vert x - p\Vert} + \frac{\Vert R_g(f(x))\Vert}{\Vert x - p\Vert} \end{align*} So, i need to bound both terms. For the first one, i was thinking of $\frac{\Vert Dg(R_f(x))\Vert}{\Vert x - p\Vert} \leq \frac{\Vert D_g\Vert \Vert R_f(x)\Vert}{\Vert x - p\Vert} \leq \frac{\Vert D_g + \epsilon_1 \Vert \Vert R_f(x)\Vert}{\Vert x - p\Vert}$ for some $\epsilon_1 > 0$, and thus, taking $\delta_1 > 0$ such that $\frac{\Vert R_f(x)\Vert}{\Vert x - p\Vert} < \frac{\epsilon_2}{2(\Vert Dg + \epsilon_1\Vert)}$ if $\Vert x - p\Vert < \delta_1$ so $\frac{\Vert D_g + \epsilon_1 \Vert \Vert R_f(x)\Vert}{\Vert x - p\Vert} \leq \frac{\epsilon_2}{2}$ if $\Vert x - p \Vert < \delta_1$. Yet, i don't know how to bound the second term. Any help? Thanks in advance.",,"['calculus', 'derivatives', 'chain-rule']"
78,Prove reflection property of parabola,Prove reflection property of parabola,,"Problem from Calculus with Analytic Geometry 2nd ed. from Simmons, page 87/21(c). Let $p$ be a positive constant and consider the parabola $x^2 = 4py$ with vertex at the origin and focus at the point $(0, p)$. Let $(x_0, y_0)$ be a point on this parabola other than the vertex. Assume we've proved tangent at $(x_0, y_0)$ has $y$-intercept $(0, -y_0)$ triangle with vertices $(x_0, y_0), (0, -y_0), (0, p)$ is isosceles. Now suppose that a source of light is placed at the focus of a parabola, and assume that each ray of light leaving the focus is reflected off the parabola in such a way that it makes equal angles with the tangent line at the point of reflection (the angle of incidence equals the angle of reflection). Use (b) to show that after reflection each ray points vertically upward, parallel to the axis. My analysis is here. But I don't know how to prove that angle $b = 90 \deg$:","Problem from Calculus with Analytic Geometry 2nd ed. from Simmons, page 87/21(c). Let $p$ be a positive constant and consider the parabola $x^2 = 4py$ with vertex at the origin and focus at the point $(0, p)$. Let $(x_0, y_0)$ be a point on this parabola other than the vertex. Assume we've proved tangent at $(x_0, y_0)$ has $y$-intercept $(0, -y_0)$ triangle with vertices $(x_0, y_0), (0, -y_0), (0, p)$ is isosceles. Now suppose that a source of light is placed at the focus of a parabola, and assume that each ray of light leaving the focus is reflected off the parabola in such a way that it makes equal angles with the tangent line at the point of reflection (the angle of incidence equals the angle of reflection). Use (b) to show that after reflection each ray points vertically upward, parallel to the axis. My analysis is here. But I don't know how to prove that angle $b = 90 \deg$:",,"['derivatives', 'conic-sections']"
79,Show $x\mapsto x/\sqrt{1+\vert x\vert^2}$ is diffeomorphism,Show  is diffeomorphism,x\mapsto x/\sqrt{1+\vert x\vert^2},"Let $H$ be an euclidean Hilbert space, and consider $$f\colon H\to H,\quad x\mapsto\frac{x}{\sqrt{1+\vert x\vert^2}}.$$ What are $Y:=\operatorname{im}(f)$, $f^{-1}$, and $\partial f$? Show that $f\in\operatorname{Diff}^\infty(H,Y)$. So, it should be $$\partial f(x)=\frac{\sqrt{1+\vert x\vert^2}\operatorname{id}-1/\sqrt{1+\vert x\vert^2}\langle\cdot,x\rangle x}{1+\vert x\vert^2}.$$ Is showing $(\partial f(x)h=0\Leftrightarrow h=0)$ directly, using the numerator, the right way to continue? I wasn't able to... Next, what are $Y$ and $f^{-1}$?","Let $H$ be an euclidean Hilbert space, and consider $$f\colon H\to H,\quad x\mapsto\frac{x}{\sqrt{1+\vert x\vert^2}}.$$ What are $Y:=\operatorname{im}(f)$, $f^{-1}$, and $\partial f$? Show that $f\in\operatorname{Diff}^\infty(H,Y)$. So, it should be $$\partial f(x)=\frac{\sqrt{1+\vert x\vert^2}\operatorname{id}-1/\sqrt{1+\vert x\vert^2}\langle\cdot,x\rangle x}{1+\vert x\vert^2}.$$ Is showing $(\partial f(x)h=0\Leftrightarrow h=0)$ directly, using the numerator, the right way to continue? I wasn't able to... Next, what are $Y$ and $f^{-1}$?",,"['real-analysis', 'derivatives', 'hilbert-spaces', 'diffeomorphism']"
80,Proving Identity for Derivative of Determinant,Proving Identity for Derivative of Determinant,,"For a square matrix $A$ and identity matrix $I$, how does one prove that $$\frac{d}{dt}\det(tI-A)=\sum_{i=1}^n\det(tI-A_i)$$ Where $A_i$ is the matrix $A$ with the $i^{th}$ row and $i^{th}$ column vectors removed?","For a square matrix $A$ and identity matrix $I$, how does one prove that $$\frac{d}{dt}\det(tI-A)=\sum_{i=1}^n\det(tI-A_i)$$ Where $A_i$ is the matrix $A$ with the $i^{th}$ row and $i^{th}$ column vectors removed?",,"['linear-algebra', 'matrices', 'derivatives', 'determinant']"
81,Derivative of trace distance,Derivative of trace distance,,"If we have two time-dependent density matrices $\rho(t)$ and $\sigma(t)$. The trace distance is $$ D(\rho(t),\sigma(t))=\frac{1}{2} \operatorname{Tr} \vert \rho(t)-\sigma(t) \vert. $$ Does an explicit formula for $$\frac{d D(\rho(t),\sigma(t))}{dt}$$ exist? Only the domain where $\rho \neq \sigma$ is asked for.","If we have two time-dependent density matrices $\rho(t)$ and $\sigma(t)$. The trace distance is $$ D(\rho(t),\sigma(t))=\frac{1}{2} \operatorname{Tr} \vert \rho(t)-\sigma(t) \vert. $$ Does an explicit formula for $$\frac{d D(\rho(t),\sigma(t))}{dt}$$ exist? Only the domain where $\rho \neq \sigma$ is asked for.",,"['linear-algebra', 'matrices', 'derivatives', 'matrix-equations', 'matrix-calculus']"
82,Introduction of $ln$ in defining derivative of $a^x$,Introduction of  in defining derivative of,ln a^x,"I am reading R. Shankar's Basic Training in Mathematics . In the first chapter, the author is attempting to give meaning to what $a^x$ means for any (potentially irrational) x. As the very first step, he presents the following method of determining the derivative of $a^x$ (before either $a^x$ or $ln$ have been defined): [1] $\Delta a^x$ = $a^{(x + \Delta x)} - a^x$ [2] = $a^x(a^{\Delta x} - 1)$ [3] = $a^x(1 + \ln(a)\Delta x + ... - 1)$ [4] $\frac{da^x}{dx}$ = $a^x\ln(a)$ He justifies step [3] by saying that it is obvious that $a^{\Delta x}$ is very close to 1, and that: The deviation from 1 has a term linear in $\Delta x$, with a coefficient that depends on $a$, and we call it the function $\ln(a)$... How is he justified in asserting that, for any choice of $a$, the deviation from $1$ must be represented by a term linear in $\Delta x$? I can see that, if there were any higher-order terms $a_2(\Delta x)^2$, $a_3(\Delta x)^3$, ..., they would all disappear as $\Delta x \rightarrow 0$ in the derivative, but how do we know that a suitable linear function even exists?","I am reading R. Shankar's Basic Training in Mathematics . In the first chapter, the author is attempting to give meaning to what $a^x$ means for any (potentially irrational) x. As the very first step, he presents the following method of determining the derivative of $a^x$ (before either $a^x$ or $ln$ have been defined): [1] $\Delta a^x$ = $a^{(x + \Delta x)} - a^x$ [2] = $a^x(a^{\Delta x} - 1)$ [3] = $a^x(1 + \ln(a)\Delta x + ... - 1)$ [4] $\frac{da^x}{dx}$ = $a^x\ln(a)$ He justifies step [3] by saying that it is obvious that $a^{\Delta x}$ is very close to 1, and that: The deviation from 1 has a term linear in $\Delta x$, with a coefficient that depends on $a$, and we call it the function $\ln(a)$... How is he justified in asserting that, for any choice of $a$, the deviation from $1$ must be represented by a term linear in $\Delta x$? I can see that, if there were any higher-order terms $a_2(\Delta x)^2$, $a_3(\Delta x)^3$, ..., they would all disappear as $\Delta x \rightarrow 0$ in the derivative, but how do we know that a suitable linear function even exists?",,"['calculus', 'algebra-precalculus', 'derivatives', 'exponentiation']"
83,"Determine $\frac{dy}{dx}$ for $y=\csc(x)$ for $x(0,p)$",Determine  for  for,"\frac{dy}{dx} y=\csc(x) x(0,p)","Determine $\frac{dy}{dx}$ for $y=\csc(x)$ for $x(0,p)$. Find the points where $\frac{dy}{dx}$ exists; and where $\frac{dy}{dx} = 0$. I obtained $$\frac{dy}{dx}=-\csc(x)\cot(x)$$ Then I used limits to find where $\frac{dy}{dx}$ exists and I get $-\infty$ to $+\infty$. I am stuck as they asked for points. Also, I can't find where $\frac{dy}{dx}=-\csc(x)\cot(x)$ is $0$.","Determine $\frac{dy}{dx}$ for $y=\csc(x)$ for $x(0,p)$. Find the points where $\frac{dy}{dx}$ exists; and where $\frac{dy}{dx} = 0$. I obtained $$\frac{dy}{dx}=-\csc(x)\cot(x)$$ Then I used limits to find where $\frac{dy}{dx}$ exists and I get $-\infty$ to $+\infty$. I am stuck as they asked for points. Also, I can't find where $\frac{dy}{dx}=-\csc(x)\cot(x)$ is $0$.",,"['calculus', 'derivatives']"
84,Derivative Solution,Derivative Solution,,I understand the first step but at the second step how do they come to negative one in the numerator? Also why do they show the definition of derivative again for step 2? How does this produce a negative one?,I understand the first step but at the second step how do they come to negative one in the numerator? Also why do they show the definition of derivative again for step 2? How does this produce a negative one?,,['derivatives']
85,an optimisation problem folding paper,an optimisation problem folding paper,,"A standard $8.5$ inches by $11$ inches piece of paper is folded so that one corner touches the opposite long side and the crease runs from the adjacent short side to the other long side, as shown in the picture below. What is the minimum length of the crease? So I put the shorter leg of the folded portion (since it's a right triangle) as $x$ and the longer leg $y$. Then the crease is $\sqrt{x^2+y^2}$ and deriving it (which is what I'm supposed to be doing for optimisation) in terms of x to minimise the crease length, I got $\frac{x+yy'}{\sqrt{x^2+y^2}} = 0$, so $x+yy' = 0$. Then I was able to find some similar triangles - one with hypotenuse $x$ and one leg $8.5-x$ and the other with hypotenuse $y$ and the not-corresponding leg $8.5$. From there I was able to get the equation $y=\sqrt{y^2-8.5^2} + \sqrt{x^2-(8.5-x)^2}$. Then I started squaring the equation but it got quite messy, and I'm not even sure if the problem is supposed to be so messy... Can someone help me solve this? Disclaimer: I do know that there is a post with the same problem on this site, but unfortunately the answers there didn't really help me, and I don't get what I am doing wrong.","A standard $8.5$ inches by $11$ inches piece of paper is folded so that one corner touches the opposite long side and the crease runs from the adjacent short side to the other long side, as shown in the picture below. What is the minimum length of the crease? So I put the shorter leg of the folded portion (since it's a right triangle) as $x$ and the longer leg $y$. Then the crease is $\sqrt{x^2+y^2}$ and deriving it (which is what I'm supposed to be doing for optimisation) in terms of x to minimise the crease length, I got $\frac{x+yy'}{\sqrt{x^2+y^2}} = 0$, so $x+yy' = 0$. Then I was able to find some similar triangles - one with hypotenuse $x$ and one leg $8.5-x$ and the other with hypotenuse $y$ and the not-corresponding leg $8.5$. From there I was able to get the equation $y=\sqrt{y^2-8.5^2} + \sqrt{x^2-(8.5-x)^2}$. Then I started squaring the equation but it got quite messy, and I'm not even sure if the problem is supposed to be so messy... Can someone help me solve this? Disclaimer: I do know that there is a post with the same problem on this site, but unfortunately the answers there didn't really help me, and I don't get what I am doing wrong.",,"['calculus', 'derivatives', 'optimization', 'triangles', 'congruences']"
86,A differentiable function intersecting every vertical plane $y = \lambda x$ must be linear,A differentiable function intersecting every vertical plane  must be linear,y = \lambda x,"This question is from a midterm that I just took and wasn't able to solve. If someone could provide a sketch for a solution that would be really helpful, also currently I'm studying analysis by Munkres, could someone suggest books that would help me be better prepared for the next midterm. Let $f:\mathbb{R}^2 \rightarrow \mathbb{R}$, such that f(x,y) = z is differentiable and intersects every vertical plane $y = \lambda x$ in a straight line.  Show that f is a linear function $f = ax + by + c$. Can the differentiabilty condition be reduced to just continuity? Thank you","This question is from a midterm that I just took and wasn't able to solve. If someone could provide a sketch for a solution that would be really helpful, also currently I'm studying analysis by Munkres, could someone suggest books that would help me be better prepared for the next midterm. Let $f:\mathbb{R}^2 \rightarrow \mathbb{R}$, such that f(x,y) = z is differentiable and intersects every vertical plane $y = \lambda x$ in a straight line.  Show that f is a linear function $f = ax + by + c$. Can the differentiabilty condition be reduced to just continuity? Thank you",,"['real-analysis', 'derivatives', 'continuity']"
87,Making sense of matrix derivative formula for determinant of symmetric matrix as a Fréchet derivative?,Making sense of matrix derivative formula for determinant of symmetric matrix as a Fréchet derivative?,,"I am familiar with the derivation of $D_A(\det(A)) := \dfrac{\partial \det(A)}{\partial A}$ as a Fréchet derivative by considering $$D_A[H] = \det(A+H) - \det(A) \; as \; \|H \| \rightarrow 0 = \det(A) \mathrm{tr} (A^{-1}H) = \det(A) A^{-T} \cdot H$$ for a general square matrix $A$. I have used the Frobenius inner product for writing the trace as an inner product. This leads to the same answer as that given in The Matrix Cookbook . But if $A$ is symmetric , how do I compute the Fréchet derivative  to be $$ \det(A) (2A^{-1} - \operatorname{diag}(A^{-1}) )$$ I am curious to know if we can derive this formula  that is quoted from The Matrix Cookbook in various posts, see for example -- What is the derivative of the determinant of a symmetric positive definite matrix?","I am familiar with the derivation of $D_A(\det(A)) := \dfrac{\partial \det(A)}{\partial A}$ as a Fréchet derivative by considering $$D_A[H] = \det(A+H) - \det(A) \; as \; \|H \| \rightarrow 0 = \det(A) \mathrm{tr} (A^{-1}H) = \det(A) A^{-T} \cdot H$$ for a general square matrix $A$. I have used the Frobenius inner product for writing the trace as an inner product. This leads to the same answer as that given in The Matrix Cookbook . But if $A$ is symmetric , how do I compute the Fréchet derivative  to be $$ \det(A) (2A^{-1} - \operatorname{diag}(A^{-1}) )$$ I am curious to know if we can derive this formula  that is quoted from The Matrix Cookbook in various posts, see for example -- What is the derivative of the determinant of a symmetric positive definite matrix?",,"['derivatives', 'determinant', 'matrix-calculus', 'frechet-derivative']"
88,Question about maximizing pyramids volume,Question about maximizing pyramids volume,,"So i have sphere with radius $R$, and inside there is a  4 sided regualar pyramid with $a$ and $h$ as it's data. So now i have to find $a and h$ such  that volume $V=\frac{1}{3} a^2h$ will maximized. So because the base is a square and 2R is the diagonal i came to this: $$2R=a\sqrt{2}$$ $$ a=\sqrt{2}R$$ hence: $$V(h)=\frac{1}{3}2R^2 h$$ So i managed to get the volume as a fucntion of $h$ I could use multivariable functions as well, i know, but this one is suppose to be solved using only the one variable functions. Did i do it correctly though, since if i calculate its derivative, it doesn't have a zero. So i probably did something wrong? Is it okay to use this: $2R=a\sqrt{2}$. Thank you for any help.","So i have sphere with radius $R$, and inside there is a  4 sided regualar pyramid with $a$ and $h$ as it's data. So now i have to find $a and h$ such  that volume $V=\frac{1}{3} a^2h$ will maximized. So because the base is a square and 2R is the diagonal i came to this: $$2R=a\sqrt{2}$$ $$ a=\sqrt{2}R$$ hence: $$V(h)=\frac{1}{3}2R^2 h$$ So i managed to get the volume as a fucntion of $h$ I could use multivariable functions as well, i know, but this one is suppose to be solved using only the one variable functions. Did i do it correctly though, since if i calculate its derivative, it doesn't have a zero. So i probably did something wrong? Is it okay to use this: $2R=a\sqrt{2}$. Thank you for any help.",,"['calculus', 'derivatives']"
89,Derivative of ratio of exponential functions: $ \frac{d}{dk}\left(\frac{x_{i}^{k}}{\sum_{j}x_{j}^{k}}\right). $,Derivative of ratio of exponential functions:, \frac{d}{dk}\left(\frac{x_{i}^{k}}{\sum_{j}x_{j}^{k}}\right). ,"Suppose we have a row vector $$X = [x_1\; \cdots \; x_n],$$ where $x_i>0$ for all $i$. I have the function \begin{equation} \frac{d}{dk}\left(\frac{x_{i}^{k}}{\sum_{j}x_{j}^{k}}\right).\quad (1) \end{equation} I'm trying to prove that (1) is negative for all $x_i\neq \max\{x_j\}$. This seems somewhat intuitive to me. However, my attempts to `prove' it have not been successful. The obvious approach is to use the quotient rule, i.e.,  \begin{align*} \frac{d}{dk}\left(\frac{x_{i}^{k}}{\sum_{j}x_{j}^{k}}\right)&=\frac{x_{i}^{k}}{\left(\sum_{j}x_{j}^{k}\right)^{2}}\sum_{j}\frac{dx_{j}^{k}}{dk}-\frac{dx_{i}^{k}}{dk}\frac{\sum_{j}x_{j}^{k}}{\left(\sum_{j}x_{j}^{k}\right)^{2}} , \end{align*} which could be carried forward as follows \begin{align*} \frac{d}{dk}\left(\frac{x_{i}^{k}}{\sum_{j}x_{j}^{k}}\right)&=\frac{x_{i}^{k}}{\left(\sum_{j}x_{j}^{k}\right)^{2}}\sum_{j}\frac{dx_{j}^{k}}{dk}-\frac{dx_{i}^{k}}{dk}\frac{\sum_{j}x_{j}^{k}}{\left(\sum_{j}x_{j}^{k}\right)^{2}} \\ & =\frac{x_{i}^{k}}{\left(\sum_{j}x_{j}^{k}\right)^{2}}\sum_{j}\frac{dx_{j}^{k}}{dk}-\frac{1}{\sum_{j}x_{j}^{k}}\frac{dx_{i}^{k}}{dk}\\  & =\frac{1}{\sum_{j}x_{j}^{k}}\left(\frac{x_{i}^{k}\sum_{j}\frac{dx_{j}^{k}}{dk}}{\sum_{j}x_{j}^{k}}-\frac{dx_{i}^{k}}{dk}\right)\\  & =\frac{1}{\sum_{j}x_{j}^{k}}\left(\frac{x_{i}^{k}\frac{d}{dk}\left(\sum_{j}x_{j}^{k}\right)}{\sum_{j}x_{j}^{k}}-\frac{dx_{i}^{k}}{dk}\right). \end{align*} It's not clear to me whether this is the right track to be going down, or how the conclusion could be drawn from this. Can anyone help?","Suppose we have a row vector $$X = [x_1\; \cdots \; x_n],$$ where $x_i>0$ for all $i$. I have the function \begin{equation} \frac{d}{dk}\left(\frac{x_{i}^{k}}{\sum_{j}x_{j}^{k}}\right).\quad (1) \end{equation} I'm trying to prove that (1) is negative for all $x_i\neq \max\{x_j\}$. This seems somewhat intuitive to me. However, my attempts to `prove' it have not been successful. The obvious approach is to use the quotient rule, i.e.,  \begin{align*} \frac{d}{dk}\left(\frac{x_{i}^{k}}{\sum_{j}x_{j}^{k}}\right)&=\frac{x_{i}^{k}}{\left(\sum_{j}x_{j}^{k}\right)^{2}}\sum_{j}\frac{dx_{j}^{k}}{dk}-\frac{dx_{i}^{k}}{dk}\frac{\sum_{j}x_{j}^{k}}{\left(\sum_{j}x_{j}^{k}\right)^{2}} , \end{align*} which could be carried forward as follows \begin{align*} \frac{d}{dk}\left(\frac{x_{i}^{k}}{\sum_{j}x_{j}^{k}}\right)&=\frac{x_{i}^{k}}{\left(\sum_{j}x_{j}^{k}\right)^{2}}\sum_{j}\frac{dx_{j}^{k}}{dk}-\frac{dx_{i}^{k}}{dk}\frac{\sum_{j}x_{j}^{k}}{\left(\sum_{j}x_{j}^{k}\right)^{2}} \\ & =\frac{x_{i}^{k}}{\left(\sum_{j}x_{j}^{k}\right)^{2}}\sum_{j}\frac{dx_{j}^{k}}{dk}-\frac{1}{\sum_{j}x_{j}^{k}}\frac{dx_{i}^{k}}{dk}\\  & =\frac{1}{\sum_{j}x_{j}^{k}}\left(\frac{x_{i}^{k}\sum_{j}\frac{dx_{j}^{k}}{dk}}{\sum_{j}x_{j}^{k}}-\frac{dx_{i}^{k}}{dk}\right)\\  & =\frac{1}{\sum_{j}x_{j}^{k}}\left(\frac{x_{i}^{k}\frac{d}{dk}\left(\sum_{j}x_{j}^{k}\right)}{\sum_{j}x_{j}^{k}}-\frac{dx_{i}^{k}}{dk}\right). \end{align*} It's not clear to me whether this is the right track to be going down, or how the conclusion could be drawn from this. Can anyone help?",,"['calculus', 'real-analysis', 'functional-analysis', 'derivatives', 'functional-inequalities']"
90,One-sided derivative for a Lipschitz function and its limit,One-sided derivative for a Lipschitz function and its limit,,"Let $f(t)$ be a Lipschitz continuous function and let $ f'(t)=\lim_{\delta\downarrow 0 } \frac{f(t+\delta)-f(t)}{\delta}$ denote its one-sided right derivative in $t$. Assume that $f'(0)$ exists. My question is about the equation: $$ f'(0) = \lim_{t\downarrow 0} f'(t). $$ Is this equation necessarily true provided that the limit in the RHS exists? In general, the equation above does not make sense because the RHS may not exists even when $f'(0)$ does, for instance $f(x)=x^2\sin(1/x)$, but if also the RHS exists, does it need to be equal to $f'(0)$?","Let $f(t)$ be a Lipschitz continuous function and let $ f'(t)=\lim_{\delta\downarrow 0 } \frac{f(t+\delta)-f(t)}{\delta}$ denote its one-sided right derivative in $t$. Assume that $f'(0)$ exists. My question is about the equation: $$ f'(0) = \lim_{t\downarrow 0} f'(t). $$ Is this equation necessarily true provided that the limit in the RHS exists? In general, the equation above does not make sense because the RHS may not exists even when $f'(0)$ does, for instance $f(x)=x^2\sin(1/x)$, but if also the RHS exists, does it need to be equal to $f'(0)$?",,"['real-analysis', 'derivatives', 'continuity', 'lipschitz-functions']"
91,Evaluating $\frac{d^{100}}{dx^{100}}\left(\frac{p(x)}{x^3-x}\right)$,Evaluating,\frac{d^{100}}{dx^{100}}\left(\frac{p(x)}{x^3-x}\right),"I am given that $\dfrac{d^{100}}{dx^{100}}\left(\dfrac{p(x)}{x^3-x}\right) = \dfrac{f(x)}{g(x)}$ for some polynomials $f(x)$ and $g(x).$ $p(x)$ doesn't have the factor $x^3-x$ and I need to find the least possible degree of $f(x)$ . My Attempt: I am describing in short what I did. Used partial fraction to break up $\dfrac{1}{x^3-x}=\dfrac{A}{x+1}+\dfrac{B}{x}+\dfrac{C}{x-1}$ Now differentiating this $100$ times gave after simplification the denominator $[x(x+1)(x-1)]^{303}$ whle the numerator of Pairwise product of the factors of denominator. That is, $\dfrac{d^{100}}{dx^{100}}\left(\dfrac{p(x)}{x+1}\right) =-A(100)!\left( \dfrac{a_0+a_1 x+\cdots +a_m x^m}{(x+1)^{101}}\right)$ . where degree of $p(x)$ is $m$ . The other two factors also produced a similar result and adding them the final expression had in numerator degree of $101+101+m=202+m$ . Now the least possible degree is achieved if $m=0$ that is $p$ is a constant polynomial. So the answer is $202$ . I felt this solution was ok but I do need advices to make sure how much I would get out of 15. Thanks i Advance!","I am given that for some polynomials and doesn't have the factor and I need to find the least possible degree of . My Attempt: I am describing in short what I did. Used partial fraction to break up Now differentiating this times gave after simplification the denominator whle the numerator of Pairwise product of the factors of denominator. That is, . where degree of is . The other two factors also produced a similar result and adding them the final expression had in numerator degree of . Now the least possible degree is achieved if that is is a constant polynomial. So the answer is . I felt this solution was ok but I do need advices to make sure how much I would get out of 15. Thanks i Advance!",\dfrac{d^{100}}{dx^{100}}\left(\dfrac{p(x)}{x^3-x}\right) = \dfrac{f(x)}{g(x)} f(x) g(x). p(x) x^3-x f(x) \dfrac{1}{x^3-x}=\dfrac{A}{x+1}+\dfrac{B}{x}+\dfrac{C}{x-1} 100 [x(x+1)(x-1)]^{303} \dfrac{d^{100}}{dx^{100}}\left(\dfrac{p(x)}{x+1}\right) =-A(100)!\left( \dfrac{a_0+a_1 x+\cdots +a_m x^m}{(x+1)^{101}}\right) p(x) m 101+101+m=202+m m=0 p 202,"['algebra-precalculus', 'derivatives']"
92,Proof of an inverse function inequality.,Proof of an inverse function inequality.,,"It is given that $f$ is a monotonically increasing function  $\forall x\in\mathbb R$ such that $f''(x) \gt 0$. Provided that $f^{-1}$ exists, It is to be shown that,  $$\frac{f^{-1}(x_1) + f^{-1}(x_2) +f^{-1}(x_3)}{3} \lt f^{-1}\left(\frac{x_1 +x_2 +x_3}{3}\right)$$ I assumed that $f^{-1}(x)$ is the mirror image of $f(x)$ about the origin, it must have all values of its derivatives negative, since $f$ is monotonically increasing. Is this correct? If not, how can the above be proved?","It is given that $f$ is a monotonically increasing function  $\forall x\in\mathbb R$ such that $f''(x) \gt 0$. Provided that $f^{-1}$ exists, It is to be shown that,  $$\frac{f^{-1}(x_1) + f^{-1}(x_2) +f^{-1}(x_3)}{3} \lt f^{-1}\left(\frac{x_1 +x_2 +x_3}{3}\right)$$ I assumed that $f^{-1}(x)$ is the mirror image of $f(x)$ about the origin, it must have all values of its derivatives negative, since $f$ is monotonically increasing. Is this correct? If not, how can the above be proved?",,"['derivatives', 'inverse-function']"
93,Minimize sum of quadratic form and regularizer with respect to a matrix,Minimize sum of quadratic form and regularizer with respect to a matrix,,"I have an optimization problem where I am trying to minimize  $$x^ \top A^{-1} x + Tr(A)$$ with respect to A, on the set of symmetric positive definite matrices. I can see the problem is convex (the first term is jointly convex in $x$ and $A$, the second one is linear) and that $Tr(A)$ acts as a regularizer to prevent the solution from being ""$+ \infty * Id$"", but otherwise I am stuck: I do not know how to apply Fermat's rule to the first term. I thought about diagonalizing $A$, but since this diagonalization depends on $A$ I also get stuck. Any ideas?","I have an optimization problem where I am trying to minimize  $$x^ \top A^{-1} x + Tr(A)$$ with respect to A, on the set of symmetric positive definite matrices. I can see the problem is convex (the first term is jointly convex in $x$ and $A$, the second one is linear) and that $Tr(A)$ acts as a regularizer to prevent the solution from being ""$+ \infty * Id$"", but otherwise I am stuck: I do not know how to apply Fermat's rule to the first term. I thought about diagonalizing $A$, but since this diagonalization depends on $A$ I also get stuck. Any ideas?",,"['linear-algebra', 'derivatives', 'optimization', 'convex-optimization']"
94,Proving the sum rule for derivatives using the sequential definition,Proving the sum rule for derivatives using the sequential definition,,"Using this definition: If $f$ is a function and has derivative $f'(c)$ at the point $c$ in the domain of $f$ means that if ($a_n$)$_{n=1}^{\infty}$ is any sequence converging to $c$ such that $a_n$ $\not= c$is in the domain of $f$ for all $n \in \mathbb{N},$ then: $$\left[ \frac{f(x_n)-f(c)}{x_n-c}\right]_{n=1}^{\infty}$$converges to $f'(c)$ Assuming $f$ and $g$ are differentiable functions on (a,b) with $h(x)=f(x)+g(x),$ prove that $h'(x)=f'(x)+g'(x).$ Attempt so far: Using the definition above,  $\left[ \frac{h(x_n)-h(c)}{x_n-c}\right]_{n=1}^{\infty}$ converges to $h'(c)$ <=> $\left[ \frac{f(x_n)-f(c)}{x_n-c}\right]_{n=1}^{\infty}$ + $\left[ \frac{g(x_n)-g(c)}{x_n-c}\right]_{n=1}^{\infty}$ since $h(x)=f(x)+g(x).$ I'm not sure if using the fact that If ($a_n$) converges to $L$ and ($b_n$) converges to $K$ then ($a_n+b_n$) converges to $L+K$  would help at all? I feel like this proof should be somewhat simple looking at how it is proven using the normal derivative definition, but I am having trouble coming up with an actual proof.","Using this definition: If $f$ is a function and has derivative $f'(c)$ at the point $c$ in the domain of $f$ means that if ($a_n$)$_{n=1}^{\infty}$ is any sequence converging to $c$ such that $a_n$ $\not= c$is in the domain of $f$ for all $n \in \mathbb{N},$ then: $$\left[ \frac{f(x_n)-f(c)}{x_n-c}\right]_{n=1}^{\infty}$$converges to $f'(c)$ Assuming $f$ and $g$ are differentiable functions on (a,b) with $h(x)=f(x)+g(x),$ prove that $h'(x)=f'(x)+g'(x).$ Attempt so far: Using the definition above,  $\left[ \frac{h(x_n)-h(c)}{x_n-c}\right]_{n=1}^{\infty}$ converges to $h'(c)$ <=> $\left[ \frac{f(x_n)-f(c)}{x_n-c}\right]_{n=1}^{\infty}$ + $\left[ \frac{g(x_n)-g(c)}{x_n-c}\right]_{n=1}^{\infty}$ since $h(x)=f(x)+g(x).$ I'm not sure if using the fact that If ($a_n$) converges to $L$ and ($b_n$) converges to $K$ then ($a_n+b_n$) converges to $L+K$  would help at all? I feel like this proof should be somewhat simple looking at how it is proven using the normal derivative definition, but I am having trouble coming up with an actual proof.",,"['real-analysis', 'derivatives', 'proof-verification']"
95,Proving differentiability implies continuity using sequential definition of derivatives,Proving differentiability implies continuity using sequential definition of derivatives,,"I have seen many proofs using this approach: Let us suppose that $f$ is differentiable at $x_0$. Then $$ \lim_{x\to x_0} \frac{f(x) - f(x_0)}{x-x_0} =  ‎f^{\prime} ‎(x) $$ and hence $$ \lim_{x\to x_0} f(x) - f(x_0) = lim_{x\to x_0} \left[ \frac{f(x) - f(x_0)}{x-x_0} \right] \cdot lim_{x\to x_0} (x-x_0) = 0$$ We have therefore shown that, using the definition of continuous, if the function is differentiable at $x_0$, it must also be continuous. However, I was wondering if you can use this same proof using the sequential definition of differentiability that states: If $f$ is a function and has derivative $f'(c)$ at the point $c$ in the domain of $f$ means that if ($a_n$)$_{n=1}^{\infty}$ is any sequence converging to $c$ such that $a_n$ $\not= c$is in the domain of $f$ for all $n \in \mathbb{N},$ then: $$\left[ \frac{f(x_n)-f(c)}{x_n-c}\right]_{n=1}^{\infty}$$converges to $f'(c)$ My attempt using this definition: $\left(\frac{f(x_n)-f(c)}{x_n-c}\right)_{n=1}^{\infty}$. Let $\epsilon >0.$ Then $|\frac{f(x_n)-f(c)}{x_n-c}-$$f'(c)$$| < \epsilon$ <=> |$f(a_n)-f(c)$|<($\epsilon + |f'(c)|$)|$a_n-c$| I thought this could be the start to a proof similar to the one above, but I am stuck after this point. I'm not sure if I have to use the delta-epsilon or sequential definition of continuity to continue with this proof, or if there is another way. Any suggestions would be appreciated.","I have seen many proofs using this approach: Let us suppose that $f$ is differentiable at $x_0$. Then $$ \lim_{x\to x_0} \frac{f(x) - f(x_0)}{x-x_0} =  ‎f^{\prime} ‎(x) $$ and hence $$ \lim_{x\to x_0} f(x) - f(x_0) = lim_{x\to x_0} \left[ \frac{f(x) - f(x_0)}{x-x_0} \right] \cdot lim_{x\to x_0} (x-x_0) = 0$$ We have therefore shown that, using the definition of continuous, if the function is differentiable at $x_0$, it must also be continuous. However, I was wondering if you can use this same proof using the sequential definition of differentiability that states: If $f$ is a function and has derivative $f'(c)$ at the point $c$ in the domain of $f$ means that if ($a_n$)$_{n=1}^{\infty}$ is any sequence converging to $c$ such that $a_n$ $\not= c$is in the domain of $f$ for all $n \in \mathbb{N},$ then: $$\left[ \frac{f(x_n)-f(c)}{x_n-c}\right]_{n=1}^{\infty}$$converges to $f'(c)$ My attempt using this definition: $\left(\frac{f(x_n)-f(c)}{x_n-c}\right)_{n=1}^{\infty}$. Let $\epsilon >0.$ Then $|\frac{f(x_n)-f(c)}{x_n-c}-$$f'(c)$$| < \epsilon$ <=> |$f(a_n)-f(c)$|<($\epsilon + |f'(c)|$)|$a_n-c$| I thought this could be the start to a proof similar to the one above, but I am stuck after this point. I'm not sure if I have to use the delta-epsilon or sequential definition of continuity to continue with this proof, or if there is another way. Any suggestions would be appreciated.",,"['real-analysis', 'derivatives', 'proof-verification', 'continuity']"
96,A proof that $\displaystyle\sum_{i=1}^{n}P''(r_i)/P'(r_i)=0$ for a degree $n$ polynomial,A proof that  for a degree  polynomial,\displaystyle\sum_{i=1}^{n}P''(r_i)/P'(r_i)=0 n,"Let $P(x)$ be a degree $n$ polynomial with distinct roots $r_1, r_2, \cdots r_n$. Prove that $$\displaystyle\sum_{i=1}^{n}\frac{P''(r_i)}{P'(r_i)}=0$$ My proof: We can equivalently rewrite the polynomial as $(x-r_1)(x-r_2)\cdots(x-r_n)$ and if we plug in the roots in the polynomial, we get that all of them are equal to $0$ so their first derivatives are equal to $1$ and then their second derivatives are equal to $0$ which means that all quotients are equal to $0$ and there for the sum is as well, $Q.E.D$ Don't show the solution please, but if you have a hint, feel free. Thank you.","Let $P(x)$ be a degree $n$ polynomial with distinct roots $r_1, r_2, \cdots r_n$. Prove that $$\displaystyle\sum_{i=1}^{n}\frac{P''(r_i)}{P'(r_i)}=0$$ My proof: We can equivalently rewrite the polynomial as $(x-r_1)(x-r_2)\cdots(x-r_n)$ and if we plug in the roots in the polynomial, we get that all of them are equal to $0$ so their first derivatives are equal to $1$ and then their second derivatives are equal to $0$ which means that all quotients are equal to $0$ and there for the sum is as well, $Q.E.D$ Don't show the solution please, but if you have a hint, feel free. Thank you.",,"['derivatives', 'polynomials', 'summation']"
97,Fourier Transform of a Derivative [duplicate],Fourier Transform of a Derivative [duplicate],,"This question already has answers here : Fourier Transform of Derivative (2 answers) Closed 7 years ago . I'm trying to prove that: $$F\,\{f'(x)\} = -i\omega F(\omega) \qquad (1) $$ where $\, F(\omega) = F\,\{f(x)\}$ This is my procedure so far: $$F\,\{f'(x)\} = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} f'(t)e^{i\omega t} dt$$ Integrating by parts I obtained: $$=\frac{1}{\sqrt{2\pi}} \big[ \space f(t) e^{i\omega t} \space \big|_{-\infty}^{\infty} -  \int_{-\infty}^{\infty} i\omega f(t)e^{i\omega t} dt \space \big]$$ Now, in order to (1) to be true I need to get: $$ f(t)e^{i\omega t} \space \big|_{-\infty}^{\infty} =0 \qquad (2)$$ I developed it and got the following: $$ f(\infty)e^{i\omega \infty} - f(-\infty)e^{-i\omega \infty } = f(\infty)e^{i\omega \infty}  - 0= f(t)e^{i\omega \infty} $$ I undarstand that the second term is $0$ , since f(t) must have a value to accomplish Dirichlet conditions (and $e^{-\infty} = 0$ ),  but I don't see how $ f(\infty)e^{i\omega \infty}$ is $0$ .","This question already has answers here : Fourier Transform of Derivative (2 answers) Closed 7 years ago . I'm trying to prove that: where This is my procedure so far: Integrating by parts I obtained: Now, in order to (1) to be true I need to get: I developed it and got the following: I undarstand that the second term is , since f(t) must have a value to accomplish Dirichlet conditions (and ),  but I don't see how is .","F\,\{f'(x)\} = -i\omega F(\omega) \qquad (1)  \, F(\omega) = F\,\{f(x)\} F\,\{f'(x)\} = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} f'(t)e^{i\omega t} dt =\frac{1}{\sqrt{2\pi}} \big[ \space f(t) e^{i\omega t} \space \big|_{-\infty}^{\infty} -  \int_{-\infty}^{\infty} i\omega f(t)e^{i\omega t} dt \space \big]  f(t)e^{i\omega t} \space \big|_{-\infty}^{\infty} =0 \qquad (2)  f(\infty)e^{i\omega \infty} - f(-\infty)e^{-i\omega \infty } = f(\infty)e^{i\omega \infty}  - 0= f(t)e^{i\omega \infty}  0 e^{-\infty} = 0  f(\infty)e^{i\omega \infty} 0","['derivatives', 'fourier-transform']"
98,$f(0)=0$ with $\left| f'(x)\right| \leq \left| f(x)\right|$ for $x\in\mathbb{R}$. Show that $f$ is identically zero.,with  for . Show that  is identically zero.,f(0)=0 \left| f'(x)\right| \leq \left| f(x)\right| x\in\mathbb{R} f,"Let $f:\mathbb{R} \rightarrow \mathbb{R}$ diff. $f(0)=0$ with $\left| f'(x)\right| \leq \left| f(x)\right|$ for $x\in\mathbb{R}$. Show that $f$ is identically zero. My proof-trying. Let $x\in\mathbb{R^{>0}}$. Consider $f:\left[ 0,x\right] \rightarrow \mathbb{R}$ ; continuous, $f:\left( 0,x\right) \rightarrow \mathbb{R}$; differentiable. By the mean value theorem, there exist a $c$ in $(0,x)$ such that $\dfrac {f\left( x\right) -f(0)} {x-0}=\dfrac {f(x)} {x}=f'(c)$. Hence, $f'(c) x=f(x)$. So, what should I do?","Let $f:\mathbb{R} \rightarrow \mathbb{R}$ diff. $f(0)=0$ with $\left| f'(x)\right| \leq \left| f(x)\right|$ for $x\in\mathbb{R}$. Show that $f$ is identically zero. My proof-trying. Let $x\in\mathbb{R^{>0}}$. Consider $f:\left[ 0,x\right] \rightarrow \mathbb{R}$ ; continuous, $f:\left( 0,x\right) \rightarrow \mathbb{R}$; differentiable. By the mean value theorem, there exist a $c$ in $(0,x)$ such that $\dfrac {f\left( x\right) -f(0)} {x-0}=\dfrac {f(x)} {x}=f'(c)$. Hence, $f'(c) x=f(x)$. So, what should I do?",,['real-analysis']
99,Factor a simple linear differential operator,Factor a simple linear differential operator,,"Let $\frac{d}{dx}=D$, one of my exercises shows that the operator $(D^2-1)$ of a function $f(x)$ can be factor as either $(D-1)(D+1)$ or $(D+\tanh x)(D-\tanh x)$. I understand the first factorization but do not get why it can be factorized as $(D-\tanh x)(D+\tanh x)$? My verification is as follows. $$(D+\tanh x)(D-\tanh x)=D^2-D(\tanh x)+\tanh x * D-\tanh^2 x=D^2-(1-\tanh^2x)+\tanh x * D-\tanh^2 x=D^2-1+\color{red}{\tanh x*D}.$$ As you can see, there is an additional term (in red). Where I made a mistake?","Let $\frac{d}{dx}=D$, one of my exercises shows that the operator $(D^2-1)$ of a function $f(x)$ can be factor as either $(D-1)(D+1)$ or $(D+\tanh x)(D-\tanh x)$. I understand the first factorization but do not get why it can be factorized as $(D-\tanh x)(D+\tanh x)$? My verification is as follows. $$(D+\tanh x)(D-\tanh x)=D^2-D(\tanh x)+\tanh x * D-\tanh^2 x=D^2-(1-\tanh^2x)+\tanh x * D-\tanh^2 x=D^2-1+\color{red}{\tanh x*D}.$$ As you can see, there is an additional term (in red). Where I made a mistake?",,"['derivatives', 'operator-algebras']"
