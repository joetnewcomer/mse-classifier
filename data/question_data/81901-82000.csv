,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Derivative of $c^TX^TXc$ with respect to $X$,Derivative of  with respect to,c^TX^TXc X,"What is the derivative of $c^TX^TXc$ with respect to $X$ ? Here, all the entries are real and $X$ is a matrix while $c$ is a vector. I keep getting confused with the left and right multiplication. Hence, if I have these answers in both these angles, I might remember this better, based on the simple one or two line approach you would take. The following shows these results in the appendix of a book: \begin{gather*} \nabla _X a^T X^T X a = 2X a a^T\\ \nabla _X a^T X X^T a = 2 a a^T X \end{gather*} Let me know how we get there.","What is the derivative of with respect to ? Here, all the entries are real and is a matrix while is a vector. I keep getting confused with the left and right multiplication. Hence, if I have these answers in both these angles, I might remember this better, based on the simple one or two line approach you would take. The following shows these results in the appendix of a book: Let me know how we get there.","c^TX^TXc X X c \begin{gather*}
\nabla _X a^T X^T X a = 2X a a^T\\
\nabla _X a^T X X^T a = 2 a a^T X
\end{gather*}","['calculus', 'linear-algebra', 'matrices', 'multivariable-calculus', 'derivatives']"
1,Link between the norm $1$ of a matrix and its biggest eigenvalue,Link between the norm  of a matrix and its biggest eigenvalue,1,"I am working on a set of matrices for a project, studying their highest eigenvalue, let's call it $\lambda_{1}$. I was curious and plotted the norm 1 of the matrix, ie $ \frac{1}{n^{2}}\sum_{i,j} |a_{i,j}|$. I observed a similar comportement for these quantities, is there any explanation ? Is this due to the structure of my matrices ? Is there a general results ? Thanks in advance More information about my matrix: It's a correlation matrix, symetric and positive. The first eigenvalues is approximately $10$ times bigger than the others. I also forgot an important thing: all variables are normalized. (and also the norm 1) $\lambda1$ in red, the norm 1 in green.","I am working on a set of matrices for a project, studying their highest eigenvalue, let's call it $\lambda_{1}$. I was curious and plotted the norm 1 of the matrix, ie $ \frac{1}{n^{2}}\sum_{i,j} |a_{i,j}|$. I observed a similar comportement for these quantities, is there any explanation ? Is this due to the structure of my matrices ? Is there a general results ? Thanks in advance More information about my matrix: It's a correlation matrix, symetric and positive. The first eigenvalues is approximately $10$ times bigger than the others. I also forgot an important thing: all variables are normalized. (and also the norm 1) $\lambda1$ in red, the norm 1 in green.",,"['matrices', 'eigenvalues-eigenvectors', 'normed-spaces']"
2,Can an alternating sign matrix that is not a permutation matrix be non-singular?,Can an alternating sign matrix that is not a permutation matrix be non-singular?,,"An alternating sign matrix is a $n\times n$ matrix with entries in the set $\{-1,0,1\}$ such that for each row and column, the non-zero entries alternate between $1$ and $-1$, starting and ending with a $1$. Permutation matrices are alternating sign matrices with no $-1$ entries. The first non-permutation alternating sign matrix is $$ \left(\begin{array}{ccc} 0 & 1 & 0\\ 1 & -1 & 1\\ 0 & 1 & 0\\ \end{array}\right). $$ Can an alternating sign matrix that is not a permutation matrix be non-singular?","An alternating sign matrix is a $n\times n$ matrix with entries in the set $\{-1,0,1\}$ such that for each row and column, the non-zero entries alternate between $1$ and $-1$, starting and ending with a $1$. Permutation matrices are alternating sign matrices with no $-1$ entries. The first non-permutation alternating sign matrix is $$ \left(\begin{array}{ccc} 0 & 1 & 0\\ 1 & -1 & 1\\ 0 & 1 & 0\\ \end{array}\right). $$ Can an alternating sign matrix that is not a permutation matrix be non-singular?",,"['combinatorics', 'matrices']"
3,Using Neumann series to compute $T^{-1}$,Using Neumann series to compute,T^{-1},"Need help on how to show that $S$ satisfies the necessary condition for Neumann series. Here is what is given. $T\in B(X,X)$ where $X$ is a Banach space .  Let $T: \mathbb R^3 \rightarrow \mathbb R^3$ be a bounded linear operator with matrix representation $$         \begin{pmatrix}         1 & 1/2 & 1/3 \\         0 & 1 & 1/2 \\         0 & 0 & 1 \\         \end{pmatrix} $$ Use the Neumann series to compute $T^{-1}$. The operator $T$ can be written as $I-S$. Show that S satisfies the necessary condition for the Neumann series. So here is my plan of attack: I know I have to use the norm $$\left\|\begin{pmatrix}          a \\         b \\         c \\         \end{pmatrix}\right\| = |a|+|b|+|c|$$ to bound $\|S\|$. I also know I have to use the the Neumann series $$(I-S)^{-1}=\sum_{j=0}^\infty S^j=I+S+S^2+\cdots$$ But how can I get $T^{-1}$ from this formula? Any help will be appreciated!","Need help on how to show that $S$ satisfies the necessary condition for Neumann series. Here is what is given. $T\in B(X,X)$ where $X$ is a Banach space .  Let $T: \mathbb R^3 \rightarrow \mathbb R^3$ be a bounded linear operator with matrix representation $$         \begin{pmatrix}         1 & 1/2 & 1/3 \\         0 & 1 & 1/2 \\         0 & 0 & 1 \\         \end{pmatrix} $$ Use the Neumann series to compute $T^{-1}$. The operator $T$ can be written as $I-S$. Show that S satisfies the necessary condition for the Neumann series. So here is my plan of attack: I know I have to use the norm $$\left\|\begin{pmatrix}          a \\         b \\         c \\         \end{pmatrix}\right\| = |a|+|b|+|c|$$ to bound $\|S\|$. I also know I have to use the the Neumann series $$(I-S)^{-1}=\sum_{j=0}^\infty S^j=I+S+S^2+\cdots$$ But how can I get $T^{-1}$ from this formula? Any help will be appreciated!",,"['matrices', 'functional-analysis', 'banach-spaces']"
4,"For this matrix $A$, what is $A^n$?","For this matrix , what is ?",A A^n,$$A = \begin{pmatrix}0 & a & b \\ 0& 0 & c \\ 0& 0 &0\end{pmatrix}$$ What is $A^n$ (for $n\geq 1)$?,$$A = \begin{pmatrix}0 & a & b \\ 0& 0 & c \\ 0& 0 &0\end{pmatrix}$$ What is $A^n$ (for $n\geq 1)$?,,"['linear-algebra', 'matrices']"
5,Fit a quadratic form given covariant derivatives on the sphere?,Fit a quadratic form given covariant derivatives on the sphere?,,"I am trying to solve for a particular vector given covariant first and second derivative for a function on a sphere.  If you have a quadratic form restricted to the sphere: $f(x) = \frac{1}{2}x^T\Gamma x + p^T x + c,\, x\in S^{n-1}$ then the covariant first and second derivatives at $x$ take the form: $\nabla f(x) = P_x(\Gamma x + p)$ $\nabla^2 f(x) = P_x\Gamma P_x-(p^Tx+x^T\Gamma x)P_x$ where $P_x = (I-xx^T)$ is the projection operator onto the tangent space at $x$.  Suppose $\Gamma$ is symmetric, $\Gamma p = 0$ and $\mathrm{Tr}(\Gamma) = 0$.  If we know $f, \nabla f$ and $ \nabla^2 f$ at a particular point, can we solve for $p$?  Is the solution unique? Edit After some numerical experimentation I realized the expression I gave for $\nabla^2 f$ is wrong.  Since the second derivative should map one point in the tangent space to another point in the tangent space, the factor of $I$ in the second term should be replaced with $P_x$.  Since $\mathrm{Tr}(P_x) = n-1$, $\mathrm{Tr}(\nabla^2 f) = -(n-1)p^Tx - nx^T\Gamma x$, but now $x^T\nabla^2 f x = 0$ and the argument below fails. Technically I believe this makes $\nabla^2 f$ the Hessian and not the second covariant derivative.","I am trying to solve for a particular vector given covariant first and second derivative for a function on a sphere.  If you have a quadratic form restricted to the sphere: $f(x) = \frac{1}{2}x^T\Gamma x + p^T x + c,\, x\in S^{n-1}$ then the covariant first and second derivatives at $x$ take the form: $\nabla f(x) = P_x(\Gamma x + p)$ $\nabla^2 f(x) = P_x\Gamma P_x-(p^Tx+x^T\Gamma x)P_x$ where $P_x = (I-xx^T)$ is the projection operator onto the tangent space at $x$.  Suppose $\Gamma$ is symmetric, $\Gamma p = 0$ and $\mathrm{Tr}(\Gamma) = 0$.  If we know $f, \nabla f$ and $ \nabla^2 f$ at a particular point, can we solve for $p$?  Is the solution unique? Edit After some numerical experimentation I realized the expression I gave for $\nabla^2 f$ is wrong.  Since the second derivative should map one point in the tangent space to another point in the tangent space, the factor of $I$ in the second term should be replaced with $P_x$.  Since $\mathrm{Tr}(P_x) = n-1$, $\mathrm{Tr}(\nabla^2 f) = -(n-1)p^Tx - nx^T\Gamma x$, but now $x^T\nabla^2 f x = 0$ and the argument below fails. Technically I believe this makes $\nabla^2 f$ the Hessian and not the second covariant derivative.",,"['linear-algebra', 'matrices', 'differential-geometry']"
6,Matrix set determination,Matrix set determination,,"Let $ H=\{ A\in M_2(\mathbb{R}) | A^2=A \},x \in \mathbb{R} $ a) Prove that if $M \in H$ and $\det(M) \neq 0$ then $\det(M)=1$. I tried this using the Hamilton-Cayley relationship, but didn't really help. $ M^2- \operatorname{Tr}(M)M- \det(M)I_2=O_2 \Leftrightarrow M-\operatorname{Tr}(M)\cdot M-\det(M)I_2=O_2$ Also, supposing $\det(M)=1$ the equation is even harder to prove in my opinion, because it is $M(1-\operatorname{Tr}(M))-I_2=O_2$. b) Prove that the set $H$ is infinite. I have no idea how to actually prove b.","Let $ H=\{ A\in M_2(\mathbb{R}) | A^2=A \},x \in \mathbb{R} $ a) Prove that if $M \in H$ and $\det(M) \neq 0$ then $\det(M)=1$. I tried this using the Hamilton-Cayley relationship, but didn't really help. $ M^2- \operatorname{Tr}(M)M- \det(M)I_2=O_2 \Leftrightarrow M-\operatorname{Tr}(M)\cdot M-\det(M)I_2=O_2$ Also, supposing $\det(M)=1$ the equation is even harder to prove in my opinion, because it is $M(1-\operatorname{Tr}(M))-I_2=O_2$. b) Prove that the set $H$ is infinite. I have no idea how to actually prove b.",,"['linear-algebra', 'matrices', 'determinant']"
7,Invertible Adjoint matrix,Invertible Adjoint matrix,,"How do you show that if the adjoint of a matrix $A$, adj$A$, is invertible then $A$ is invertible? I know how to prove the opposite, but I can't figure out how to prove it. Eden Segal","How do you show that if the adjoint of a matrix $A$, adj$A$, is invertible then $A$ is invertible? I know how to prove the opposite, but I can't figure out how to prove it. Eden Segal",,"['linear-algebra', 'matrices']"
8,Can we always recover a matrix from its eigenvalues and eigenvectors?,Can we always recover a matrix from its eigenvalues and eigenvectors?,,"If we're given all the eigenvalues of a square matrix $A$ and the corresponding eigenvectors of each eigenvalue, then in what case(s) is it possible theoretically to recover $A$ from this much information? And how exactly when it is possible?","If we're given all the eigenvalues of a square matrix $A$ and the corresponding eigenvectors of each eigenvalue, then in what case(s) is it possible theoretically to recover $A$ from this much information? And how exactly when it is possible?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
9,Positive definite matrix inequality,Positive definite matrix inequality,,Can someone help me with the following proof involving positive definite matrices: Suppose $X\succ  0$ positive definite.  Show that $X-v{v^T}\succ 0$ if and only if ${v^T}X^{-1}v \le 1$. Thanks in advance.,Can someone help me with the following proof involving positive definite matrices: Suppose $X\succ  0$ positive definite.  Show that $X-v{v^T}\succ 0$ if and only if ${v^T}X^{-1}v \le 1$. Thanks in advance.,,"['matrices', 'inequality']"
10,"A matrix $A$ such that $Ax = b$ has either 1 or infinitely many solutions, depending on $b$","A matrix  such that  has either 1 or infinitely many solutions, depending on",A Ax = b b,How can I find a matrix like this: Find a matrix $A$ such that $Ax = b$ that has either 1 or infinitely many solutions depending on $b$. Thank you.,How can I find a matrix like this: Find a matrix $A$ such that $Ax = b$ that has either 1 or infinitely many solutions depending on $b$. Thank you.,,"['linear-algebra', 'matrices']"
11,Colleague Matrix,Colleague Matrix,,Can someone explain to me the concept of a Colleague Matrix. I tried to find some information online and I haven't been able to find anything. Example.. Given the function $$f (x) = x\bigg(x − {1\over4}\bigg)\bigg(x − {1\over2}\bigg)$$ show that its colleague matrix is given by $C = \begin{bmatrix}0 & 1 & 0\\{1\over2} & 0 & {1\over2}\\{3\over4} & -{5\over4} & {3\over4}\end{bmatrix}$,Can someone explain to me the concept of a Colleague Matrix. I tried to find some information online and I haven't been able to find anything. Example.. Given the function $$f (x) = x\bigg(x − {1\over4}\bigg)\bigg(x − {1\over2}\bigg)$$ show that its colleague matrix is given by $C = \begin{bmatrix}0 & 1 & 0\\{1\over2} & 0 & {1\over2}\\{3\over4} & -{5\over4} & {3\over4}\end{bmatrix}$,,"['linear-algebra', 'matrices', 'numerical-methods', 'orthogonal-polynomials']"
12,Svd decomposition and rotationally invariance,Svd decomposition and rotationally invariance,,Is SVD decomposition rotationally invariant? If so can you provide the underlying intuition or a demonstration?,Is SVD decomposition rotationally invariant? If so can you provide the underlying intuition or a demonstration?,,"['linear-algebra', 'matrices']"
13,Rotate a vector in higher dimension,Rotate a vector in higher dimension,,"I am trying to figure out way to rotate a vector $X=[x_1,x_2,\dots\dots,x_n]$ taking another point $p$ as a reference, where the vector is in higher dimension with $n>3$ something like $n=30$. At this point, I am not sure what are the things that I need to know if I have to rotate it. I think i need to rotate it in each of the plane, but not sure, how to do it. Thank you for your time!","I am trying to figure out way to rotate a vector $X=[x_1,x_2,\dots\dots,x_n]$ taking another point $p$ as a reference, where the vector is in higher dimension with $n>3$ something like $n=30$. At this point, I am not sure what are the things that I need to know if I have to rotate it. I think i need to rotate it in each of the plane, but not sure, how to do it. Thank you for your time!",,"['matrices', 'vector-spaces', 'rotations']"
14,Prove the inverse of an integer matrix has only integer entries?,Prove the inverse of an integer matrix has only integer entries?,,"Prove that if $A$ is a square matrix with integer entries and $\det(A)=\pm 1$, then the inverse of $A$ contains all integer entries.","Prove that if $A$ is a square matrix with integer entries and $\det(A)=\pm 1$, then the inverse of $A$ contains all integer entries.",,"['linear-algebra', 'matrices']"
15,find the function of a matrix,find the function of a matrix,,"Given $$ J=\begin{bmatrix} \frac{\pi}{2}&0&0\\ 1&\frac{\pi}{2}&0\\ 0&1&\frac{\pi}{2}\\ \end{bmatrix} $$ find $\sin(J) \text{ and } \cos(J)$ I know I need to find the spectral decomposition, but I am not sure what to do because the examples and exercises in the textbook I have all have more than one eigenvalue, here there is only one eigenvalue.","Given $$ J=\begin{bmatrix} \frac{\pi}{2}&0&0\\ 1&\frac{\pi}{2}&0\\ 0&1&\frac{\pi}{2}\\ \end{bmatrix} $$ find $\sin(J) \text{ and } \cos(J)$ I know I need to find the spectral decomposition, but I am not sure what to do because the examples and exercises in the textbook I have all have more than one eigenvalue, here there is only one eigenvalue.",,['matrices']
16,Maximum and minimum ratio of matrix calculation,Maximum and minimum ratio of matrix calculation,,"Suppose you have a matrix : $$A = \begin{pmatrix} 13 & -4 & 2 \\ -4 & 13 & -2 \\ 2 & -2 & 10 \end{pmatrix}.$$ I want to find the maximum and minimum values of the ratio $$\frac{x'Ax}{x'x}$$ where $x =(x_1,x_2,x_3)$ is nonzero. Is there a way you can figure this out manually?","Suppose you have a matrix : $$A = \begin{pmatrix} 13 & -4 & 2 \\ -4 & 13 & -2 \\ 2 & -2 & 10 \end{pmatrix}.$$ I want to find the maximum and minimum values of the ratio $$\frac{x'Ax}{x'x}$$ where $x =(x_1,x_2,x_3)$ is nonzero. Is there a way you can figure this out manually?",,"['linear-algebra', 'matrices']"
17,Counting the number of matrices which cause collision,Counting the number of matrices which cause collision,,"Let $m,n \in \mathbb{N}$, and $q$ be a prime number. Let $\mathbf{A} \in \mathbb{Z}^{m \times n}_q$ be a matrix. In the following, assume that all additions and multiplications are performed modulo $q$. We call $\mathbf{A}$ a ""bad"" matrix if there exist two distinct binary vectors $\mathbf{x}, \mathbf{y} \in \{0,1\}^{n \times 1}$ such that $\mathbf{Ax} \equiv \mathbf{Ay} \pmod q$. In other words, a matrix for which there exists a collision is bad. Otherwise, $\mathbf{A}$ is called a ""good"" matrix. Example: For $m=n=2$ and $q=11$, the matrices    $\mathbf{A_1} = \left[\begin{smallmatrix} 0&0\\ 0&0 \end{smallmatrix} \right]$ and   $\mathbf{A_2} = \left[\begin{smallmatrix} 1&2\\ 3&4 \end{smallmatrix} \right]$ are bad and good, respectively. There are a total of $q^{mn}$ possible matrices $\mathbf{A}  \in \mathbb{Z}^{m \times n}_q$. The question is to count the number of bad matrices. Question: How many matrices $\mathbf{A}  \in \mathbb{Z}^{m \times n}_q$ are bad? A tight upper-bound will do as well. PS: Programming experiments with $2 \times 2$ matrices show that about $4q^2$ of the matrices are bad. Edit (8/9/2012): In my problem, I have the following requirement on the parameters $n \ge 3m \lceil \lg q \rceil$. For a practical implementation, one can take $m=100$ and $q=257$, resulting in $n=2700$.","Let $m,n \in \mathbb{N}$, and $q$ be a prime number. Let $\mathbf{A} \in \mathbb{Z}^{m \times n}_q$ be a matrix. In the following, assume that all additions and multiplications are performed modulo $q$. We call $\mathbf{A}$ a ""bad"" matrix if there exist two distinct binary vectors $\mathbf{x}, \mathbf{y} \in \{0,1\}^{n \times 1}$ such that $\mathbf{Ax} \equiv \mathbf{Ay} \pmod q$. In other words, a matrix for which there exists a collision is bad. Otherwise, $\mathbf{A}$ is called a ""good"" matrix. Example: For $m=n=2$ and $q=11$, the matrices    $\mathbf{A_1} = \left[\begin{smallmatrix} 0&0\\ 0&0 \end{smallmatrix} \right]$ and   $\mathbf{A_2} = \left[\begin{smallmatrix} 1&2\\ 3&4 \end{smallmatrix} \right]$ are bad and good, respectively. There are a total of $q^{mn}$ possible matrices $\mathbf{A}  \in \mathbb{Z}^{m \times n}_q$. The question is to count the number of bad matrices. Question: How many matrices $\mathbf{A}  \in \mathbb{Z}^{m \times n}_q$ are bad? A tight upper-bound will do as well. PS: Programming experiments with $2 \times 2$ matrices show that about $4q^2$ of the matrices are bad. Edit (8/9/2012): In my problem, I have the following requirement on the parameters $n \ge 3m \lceil \lg q \rceil$. For a practical implementation, one can take $m=100$ and $q=257$, resulting in $n=2700$.",,"['combinatorics', 'matrices', 'number-theory', 'modular-arithmetic', 'inclusion-exclusion']"
18,Presentation of discrete upper triangular group,Presentation of discrete upper triangular group,,"Let $G$ be the nilpotent Lie group consisting of matrices $$ \begin{pmatrix} 1 & a_{12} & \cdots & a_{1,n}\\ 0 & 1 & \ddots & \vdots\\ \vdots & \ddots & \ddots & a_{n-1,n}\\ 0 & \cdots & 0 & 1 \end{pmatrix} $$ where $a_{ij}\in\mathbb R$. I would like to find a presentation of the group $\Gamma=G\cap\mathrm{GL}_n\mathbb Z$. The entries on the superdiagonal play a crucial role, in that the $n-1$ matrices with a single $1$ on the superdiagonal ($1$s on the diagonal and $0$s elsewhere) generate all of $\Gamma$. Trying to write down a presentation, I would start by choosing $n-1$ generators, $a_{12},\dotsc,a_{n-1,n}$ (named after the matrix entries). It is also clear that we need commutation relations, like $$ [\cdots[[[a_{12},a_{23}],[a_{23},a_{34}]],\ldots]\cdots]=\cdots=e. $$ (If $n=3$, this would just be $[[a_{12},a_{23}],a_{12}]=[[a_{12},a_{23}],a_{23}]=e$.) I am just not sure, though, if I explicitly need to require that, say, $$ [a_{12},a_{34}]=e, $$ which seems to be directly related to the embedding I am thinking of. I would like a presentation in which any generators can be mapped to any of the matrices with a $1$ in the $(i,i+1)$ position. Is this possible?","Let $G$ be the nilpotent Lie group consisting of matrices $$ \begin{pmatrix} 1 & a_{12} & \cdots & a_{1,n}\\ 0 & 1 & \ddots & \vdots\\ \vdots & \ddots & \ddots & a_{n-1,n}\\ 0 & \cdots & 0 & 1 \end{pmatrix} $$ where $a_{ij}\in\mathbb R$. I would like to find a presentation of the group $\Gamma=G\cap\mathrm{GL}_n\mathbb Z$. The entries on the superdiagonal play a crucial role, in that the $n-1$ matrices with a single $1$ on the superdiagonal ($1$s on the diagonal and $0$s elsewhere) generate all of $\Gamma$. Trying to write down a presentation, I would start by choosing $n-1$ generators, $a_{12},\dotsc,a_{n-1,n}$ (named after the matrix entries). It is also clear that we need commutation relations, like $$ [\cdots[[[a_{12},a_{23}],[a_{23},a_{34}]],\ldots]\cdots]=\cdots=e. $$ (If $n=3$, this would just be $[[a_{12},a_{23}],a_{12}]=[[a_{12},a_{23}],a_{23}]=e$.) I am just not sure, though, if I explicitly need to require that, say, $$ [a_{12},a_{34}]=e, $$ which seems to be directly related to the embedding I am thinking of. I would like a presentation in which any generators can be mapped to any of the matrices with a $1$ in the $(i,i+1)$ position. Is this possible?",,"['group-theory', 'matrices', 'lie-groups']"
19,Invariant subspace under orthogonal matrix,Invariant subspace under orthogonal matrix,,"Let $V=\mathbb{R}^{n}$ and $T\,:V\to V$ be defined by $Tv=Av$ where $A\in M_{n}(\mathbb{R})$ is an orthogonal matrix. My lecture wrote that if $W\subset V$ is a subspace of $V$ then if $W$ is $A$ invariant then $W^{\perp}$ is also $A$ invariant. What I do know is that if $W$ is $A$ invariant then $W^{\perp}$ is also $A^*=A^{t}$ invariant, but I could not deduce from this that it is also $A$ invariant. Is this 'fact' true ? I couldn't prove it (I tried writing a proof similar to the case I know, using inner products and failed), help is appreciated!","Let $V=\mathbb{R}^{n}$ and $T\,:V\to V$ be defined by $Tv=Av$ where $A\in M_{n}(\mathbb{R})$ is an orthogonal matrix. My lecture wrote that if $W\subset V$ is a subspace of $V$ then if $W$ is $A$ invariant then $W^{\perp}$ is also $A$ invariant. What I do know is that if $W$ is $A$ invariant then $W^{\perp}$ is also $A^*=A^{t}$ invariant, but I could not deduce from this that it is also $A$ invariant. Is this 'fact' true ? I couldn't prove it (I tried writing a proof similar to the case I know, using inner products and failed), help is appreciated!",,"['linear-algebra', 'matrices', 'inner-products']"
20,"Rank and determinant of $D$ , an $n\times n$ real matrix, $n\ge 2$","Rank and determinant of  , an  real matrix,",D n\times n n\ge 2,"Let $D$ be a $n\times n$ real matrix, $n\ge 2$. Which of the following is valid? $\det(D)=0\Rightarrow \mathrm{rank}(D)=0$ $\det(D)=1\Rightarrow \mathrm{rank}(D)\neq 1$ $\det(D)=1\Rightarrow \mathrm{rank}(D)\neq0$ $\det(D)=n\Rightarrow \mathrm{rank}(D)\neq 1$ Well, (1) is wrong because there is a $3\times 3$ matrix with rank $2$ and determinant $0$, namely $$\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix}. $$ I am confused about the other three: please help!","Let $D$ be a $n\times n$ real matrix, $n\ge 2$. Which of the following is valid? $\det(D)=0\Rightarrow \mathrm{rank}(D)=0$ $\det(D)=1\Rightarrow \mathrm{rank}(D)\neq 1$ $\det(D)=1\Rightarrow \mathrm{rank}(D)\neq0$ $\det(D)=n\Rightarrow \mathrm{rank}(D)\neq 1$ Well, (1) is wrong because there is a $3\times 3$ matrix with rank $2$ and determinant $0$, namely $$\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix}. $$ I am confused about the other three: please help!",,"['linear-algebra', 'matrices', 'determinant']"
21,The form of the states on an algebra of $n\times n$ matrices with complex entries,The form of the states on an algebra of  matrices with complex entries,n\times n,"I have a question concerning $n \times n$ matrices. Denote by $M_n(\mathbb{C})$ the algebra of all $n \times n$ matrices with complex entries. Let $\phi$ be a state on $M_n(\mathbb{C})$ i.e. a linear functional $\phi \colon M_n(\mathbb{C}) \rightarrow \mathbb{C}$ such that $\phi(Id)=1$ and $\phi(A^*A) \geq 0$, for each $A \in M_n(\mathbb{C})$. Show that $\phi$ must be of the form $\phi(A) = \text{tr}(\rho A)$, where $\rho$ is a positive-definite matrix such that $\mbox{tr}(\rho)=1$. Thank you for any help.","I have a question concerning $n \times n$ matrices. Denote by $M_n(\mathbb{C})$ the algebra of all $n \times n$ matrices with complex entries. Let $\phi$ be a state on $M_n(\mathbb{C})$ i.e. a linear functional $\phi \colon M_n(\mathbb{C}) \rightarrow \mathbb{C}$ such that $\phi(Id)=1$ and $\phi(A^*A) \geq 0$, for each $A \in M_n(\mathbb{C})$. Show that $\phi$ must be of the form $\phi(A) = \text{tr}(\rho A)$, where $\rho$ is a positive-definite matrix such that $\mbox{tr}(\rho)=1$. Thank you for any help.",,"['linear-algebra', 'matrices', 'c-star-algebras']"
22,Measure to compare matrix $A$ and permuted matrix $B$,Measure to compare matrix  and permuted matrix,A B,"We have matrices $A$ and $B$ of the same dimension. Generally, we use the Frobenius norm of the difference, $\|A-B\|_F^2$ , to compare these two matrices and measure how close they are to each other. Here we assume the $\mbox{col}_i$ of matrix $A$ corresponds to $\mbox{col}_i$ of matrix $B$ and so on and, hence, the subtraction makes sense. If matrices $A$ and $B$ are equal, the Frobenius norm is zero. When matrices $A$ and $B$ deviate, the Frobenius norm is positive. In our case, $\mbox{col}_i$ of matrix $A$ may corresponds to $\mbox{col}_j$ of matrix $B$ . Is there any measure to compare these two matrices where the columns are permuted in one matrix? One can say that, we can find all the permuted matrix $B$ and evaluate $\|A -\operatorname{perm}(B)\|_F^2$ and choose the best $B$ . However, this gets problematic when the number of columns increases, as we need to evaluate $n!$ permutations. Is there any other measure (not necessarily using the Frobenius norm) to evaluate the closeness of matrices $A$ and $B$ where $B$ is a version of $A$ in which the columns have been permuted?","We have matrices and of the same dimension. Generally, we use the Frobenius norm of the difference, , to compare these two matrices and measure how close they are to each other. Here we assume the of matrix corresponds to of matrix and so on and, hence, the subtraction makes sense. If matrices and are equal, the Frobenius norm is zero. When matrices and deviate, the Frobenius norm is positive. In our case, of matrix may corresponds to of matrix . Is there any measure to compare these two matrices where the columns are permuted in one matrix? One can say that, we can find all the permuted matrix and evaluate and choose the best . However, this gets problematic when the number of columns increases, as we need to evaluate permutations. Is there any other measure (not necessarily using the Frobenius norm) to evaluate the closeness of matrices and where is a version of in which the columns have been permuted?",A B \|A-B\|_F^2 \mbox{col}_i A \mbox{col}_i B A B A B \mbox{col}_i A \mbox{col}_j B B \|A -\operatorname{perm}(B)\|_F^2 B n! A B B A,"['matrices', 'optimization', 'permutations', 'discrete-optimization']"
23,Is there a classic Matrix Algebra reference?,Is there a classic Matrix Algebra reference?,,"I'm looking for a classic matrix algebra reference, either introductory or advanced. In fact, I'm looking for ways to factorize elements of a matrix, and its appropriate determinant implications. Your help is greatly appreciated.","I'm looking for a classic matrix algebra reference, either introductory or advanced. In fact, I'm looking for ways to factorize elements of a matrix, and its appropriate determinant implications. Your help is greatly appreciated.",,"['linear-algebra', 'matrices', 'reference-request']"
24,Hadamard Matrix,Hadamard Matrix,,"Prove that if $H$ is a (normalized) Hadamard matrix, then so is the matrix $\pmatrix{ H& H\\\ H& -H}$. I have been working on this and I know this statement is true. My book just simply says that this is true. Does it have to do with the order of the Hadamard matrix?","Prove that if $H$ is a (normalized) Hadamard matrix, then so is the matrix $\pmatrix{ H& H\\\ H& -H}$. I have been working on this and I know this statement is true. My book just simply says that this is true. Does it have to do with the order of the Hadamard matrix?",,"['abstract-algebra', 'combinatorics', 'matrices']"
25,Idempotency of difference of two idempotent matrices,Idempotency of difference of two idempotent matrices,,"Define $$ \mathbf{H}=\mathbf{X}\left(\mathbf{X}^{\prime}\mathbf{X}\right)^{-1}\mathbf{X}^{\prime} $$ where $\mathbf{X}$ is of order $n \times k$ and $$ \overline{\mathbf{J}}=\frac{1}{n}\mathbf{J}=\frac{1}{n}\mathbf{1}\mathbf{1}^{\prime} $$ where $\mathbf{1}$ is a unit vector of order $n \times 1$. Now $$ \mathbf{H}\mathbf{H}=\mathbf{H} $$ and $$ \overline{\mathbf{J}}\overline{\mathbf{J}}=\overline{\mathbf{J}} $$ Thus both $\mathbf{H}$ and $\overline{\mathbf{J}}$ are idempotent matrices. My question is whether $\mathbf{H}-\overline{\mathbf{J}}$ would be idempotent. If so then $$ \left(\mathbf{H}-\mathbf{\overline{\mathbf{J}}}\right)\mathbf{\left(\mathbf{H}-\mathbf{\overline{\mathbf{J}}}\right)}=\mathbf{H}-\mathbf{H\overline{\mathbf{J}}-\overline{\mathbf{J}}H}+\overline{\mathbf{J}}=\mathbf{H}-\mathbf{\overline{\mathbf{J}}-\overline{\mathbf{J}}}+\overline{\mathbf{J}}=\mathbf{H}-\overline{\mathbf{J}} $$ But I'm not able to show that $$ \mathbf{H\overline{\mathbf{J}}=\overline{\mathbf{J}}H}=\overline{\mathbf{J}} $$ I'd highly appreciate if you guide me to figure this. Thanks for your time and help.","Define $$ \mathbf{H}=\mathbf{X}\left(\mathbf{X}^{\prime}\mathbf{X}\right)^{-1}\mathbf{X}^{\prime} $$ where $\mathbf{X}$ is of order $n \times k$ and $$ \overline{\mathbf{J}}=\frac{1}{n}\mathbf{J}=\frac{1}{n}\mathbf{1}\mathbf{1}^{\prime} $$ where $\mathbf{1}$ is a unit vector of order $n \times 1$. Now $$ \mathbf{H}\mathbf{H}=\mathbf{H} $$ and $$ \overline{\mathbf{J}}\overline{\mathbf{J}}=\overline{\mathbf{J}} $$ Thus both $\mathbf{H}$ and $\overline{\mathbf{J}}$ are idempotent matrices. My question is whether $\mathbf{H}-\overline{\mathbf{J}}$ would be idempotent. If so then $$ \left(\mathbf{H}-\mathbf{\overline{\mathbf{J}}}\right)\mathbf{\left(\mathbf{H}-\mathbf{\overline{\mathbf{J}}}\right)}=\mathbf{H}-\mathbf{H\overline{\mathbf{J}}-\overline{\mathbf{J}}H}+\overline{\mathbf{J}}=\mathbf{H}-\mathbf{\overline{\mathbf{J}}-\overline{\mathbf{J}}}+\overline{\mathbf{J}}=\mathbf{H}-\overline{\mathbf{J}} $$ But I'm not able to show that $$ \mathbf{H\overline{\mathbf{J}}=\overline{\mathbf{J}}H}=\overline{\mathbf{J}} $$ I'd highly appreciate if you guide me to figure this. Thanks for your time and help.",,"['linear-algebra', 'matrices']"
26,Solving $\mathrm{A}=\mathrm{B}^{-1}\mathrm{C}$ for $\mathrm{B}$ when $\mathrm{C}$ is not invertible,Solving  for  when  is not invertible,\mathrm{A}=\mathrm{B}^{-1}\mathrm{C} \mathrm{B} \mathrm{C},I need to solve the following equation for $\boldsymbol{\mathrm{B}}$ $$\boldsymbol{\mathrm{A}}_{m\times p}=\boldsymbol{\mathrm{B}}^{-1}_{m\times m}\boldsymbol{\mathrm{C}}_{m\times p}$$ The problem is that the matrix $\boldsymbol{\mathrm{C}}$ is not invertible.  Any way to approximate or solve this equation for $\boldsymbol{\mathrm{B}}$. Thanks for your time and help. Note $p$ could be 1 and in that case $\mathrm{C}\mathrm{C}^{T}$ is not invertible.,I need to solve the following equation for $\boldsymbol{\mathrm{B}}$ $$\boldsymbol{\mathrm{A}}_{m\times p}=\boldsymbol{\mathrm{B}}^{-1}_{m\times m}\boldsymbol{\mathrm{C}}_{m\times p}$$ The problem is that the matrix $\boldsymbol{\mathrm{C}}$ is not invertible.  Any way to approximate or solve this equation for $\boldsymbol{\mathrm{B}}$. Thanks for your time and help. Note $p$ could be 1 and in that case $\mathrm{C}\mathrm{C}^{T}$ is not invertible.,,['matrices']
27,Is this determinant equal to 1,Is this determinant equal to 1,,"Let $V$ be a finite dimensional vector space over $\mathbf{C}$ with a hermitian inner product. Let $e=(e_1,\ldots,e_n)^t$ and $f=(f_1,\ldots,f_n)^t$ be orthonormal bases for $V$. There is a matrix $A$ such that $e =A f$. Is $\det A = 1$?","Let $V$ be a finite dimensional vector space over $\mathbf{C}$ with a hermitian inner product. Let $e=(e_1,\ldots,e_n)^t$ and $f=(f_1,\ldots,f_n)^t$ be orthonormal bases for $V$. There is a matrix $A$ such that $e =A f$. Is $\det A = 1$?",,"['linear-algebra', 'matrices', 'determinant']"
28,Always solving systems of linear equations wrong - what do I do wrong?,Always solving systems of linear equations wrong - what do I do wrong?,,"Dear ladies and gentlemen, over time I noticed I (and other) again and again have problems solving ""systems of linear equations"". It seems depending of the steps one chooses, we get different results!! How can that be? Should one not always get the same results no matter which path he goes down? What am I missing? Are there maybe rules I don't know and use even though I shouldn't? I want to give you an example. The set of equations is from a state-price security calculation (see ""State Preference Approach"") which we shall solve using the Gaussian elimination: I: 2P(1) + 2P(2) + 2P(3) =1,6 II: 3P(1) + 0P(2) + 1P(3) =1,0 III: 0P(1) + 2P(2) + 1P(3) =0,8 The solution shall be: P(1) = 0,2 P(2)=0,2 and P(3)=0,4 Not only got I different numbers on the first try that totally went in ""into space"", I want to write down for you my second approach to check for mistakes: II-III = IV = 3P(1) - 2P(2) = 0,2 --> 2P(2) = 3P(1) - 0,2 (this far I'm with the solution) then I simply plug in the result in the following lines: in III: 4P(1) = 1 --> P(1) = 0,25 in II: 0,75 + P(3) = 1,0 --> P(3) = 0,25 in I: 0,5 + 2P(2) + 0,5 = 1,6 --> P(2) = 0,3 But these results seem to differ. So it seems I clearly miss some important rule! Must I not ""plug in"" results into other rows, as the ""Gaussian"" system seems to avoid? But how can there be a difference / can I be forced to avoid ""plugging in"" results? This should normally be allowed, shouldn't it? Can you help me find my blind spot? Thanks for your help in advance.","Dear ladies and gentlemen, over time I noticed I (and other) again and again have problems solving ""systems of linear equations"". It seems depending of the steps one chooses, we get different results!! How can that be? Should one not always get the same results no matter which path he goes down? What am I missing? Are there maybe rules I don't know and use even though I shouldn't? I want to give you an example. The set of equations is from a state-price security calculation (see ""State Preference Approach"") which we shall solve using the Gaussian elimination: I: 2P(1) + 2P(2) + 2P(3) =1,6 II: 3P(1) + 0P(2) + 1P(3) =1,0 III: 0P(1) + 2P(2) + 1P(3) =0,8 The solution shall be: P(1) = 0,2 P(2)=0,2 and P(3)=0,4 Not only got I different numbers on the first try that totally went in ""into space"", I want to write down for you my second approach to check for mistakes: II-III = IV = 3P(1) - 2P(2) = 0,2 --> 2P(2) = 3P(1) - 0,2 (this far I'm with the solution) then I simply plug in the result in the following lines: in III: 4P(1) = 1 --> P(1) = 0,25 in II: 0,75 + P(3) = 1,0 --> P(3) = 0,25 in I: 0,5 + 2P(2) + 0,5 = 1,6 --> P(2) = 0,3 But these results seem to differ. So it seems I clearly miss some important rule! Must I not ""plug in"" results into other rows, as the ""Gaussian"" system seems to avoid? But how can there be a difference / can I be forced to avoid ""plugging in"" results? This should normally be allowed, shouldn't it? Can you help me find my blind spot? Thanks for your help in advance.",,['matrices']
29,matrix of all possible differences,matrix of all possible differences,,"I need to compute the matrix of differences of a vector, just like here in section ""Matrix of differences"". Is that actually correct? I thought that, in order to add up (or subtract) two vector/matrices they had to have the same dimensions.","I need to compute the matrix of differences of a vector, just like here in section ""Matrix of differences"". Is that actually correct? I thought that, in order to add up (or subtract) two vector/matrices they had to have the same dimensions.",,"['linear-algebra', 'matrices']"
30,Efficient multiplication of super-size matrices in matlab,Efficient multiplication of super-size matrices in matlab,,"I don't have enough memory to simply create diagonal matrix D x D, getting an 'out of memory' error. Instead of performing M x D x D operations in the first multiplication, I do M x D operations, but still my code takes ages to run. Can anybody find a more effective way to multiply these matrices in matlab? D=20000 M=25  A = floor(rand(D,M)*10); B = floor(rand(1,M)*10);   for i=1:D  for j=1:M   result(i,j) = A(i,j) * B(1,j);  end end   manual = result * A'; auto = A*diag(B)*A'; isequal(manual,auto)","I don't have enough memory to simply create diagonal matrix D x D, getting an 'out of memory' error. Instead of performing M x D x D operations in the first multiplication, I do M x D operations, but still my code takes ages to run. Can anybody find a more effective way to multiply these matrices in matlab? D=20000 M=25  A = floor(rand(D,M)*10); B = floor(rand(1,M)*10);   for i=1:D  for j=1:M   result(i,j) = A(i,j) * B(1,j);  end end   manual = result * A'; auto = A*diag(B)*A'; isequal(manual,auto)",,"['linear-algebra', 'matrices', 'matlab']"
31,Symmetric Matrix without $LDL^T$ Decomposition,Symmetric Matrix without  Decomposition,LDL^T,What would be an example of a matrix that would not have a $LDL^T$ decomposition? This is a trivial case but I was thinking of a zero matrix which would result in L being an identity matrix and D would be a zero matrix. When you compute $LDL^T$ it still gives you the original zero matrix so I don't believe this is correct. What about a singular matrix?,What would be an example of a matrix that would not have a $LDL^T$ decomposition? This is a trivial case but I was thinking of a zero matrix which would result in L being an identity matrix and D would be a zero matrix. When you compute $LDL^T$ it still gives you the original zero matrix so I don't believe this is correct. What about a singular matrix?,,"['linear-algebra', 'matrices']"
32,Formal notation for number of rows/columns in a matrix,Formal notation for number of rows/columns in a matrix,,"Is there a generally accepted formal notation for denoting the number of columns a matrix has (e.g to use in pseudocode/algorithm environment in LaTeX)? Something I could use in the description of an algorithm like: if horizontaldim(V) > x then end if or if size(V,2) > x then  end if or should I just use a description like if number of columns in V > x then end if","Is there a generally accepted formal notation for denoting the number of columns a matrix has (e.g to use in pseudocode/algorithm environment in LaTeX)? Something I could use in the description of an algorithm like: if horizontaldim(V) > x then end if or if size(V,2) > x then  end if or should I just use a description like if number of columns in V > x then end if",,"['notation', 'matrices']"
33,"Find values of $a,b$. such that a matrix is diagonalizable",Find values of . such that a matrix is diagonalizable,"a,b","Find all values of $a,b\in\mathbb{R}$ such that $A$ is diagonalizable. $$A=\begin{pmatrix} -1 & a & b\\ 0 & 1 & 2\\ 0 & 2 & 1\\ \end{pmatrix}.$$ So far, I have that: $$det(A-\lambda I)=\begin{vmatrix}  -1-\lambda & a & b\\ 0 & 1-\lambda & 2\\ 0 & 2 & 1-\lambda\\ \end{vmatrix}=-(\lambda +1)^2(\lambda -3).$$ So $A$ has eigenvalues $\lambda=-1$ , $\lambda=3$ Now, let $v=(x,y,z)^T$ . Then $v\in ker(A--1I)\iff$ $$\begin{pmatrix}  0&a&b\\ 0&2&2\\ 0&2&2 \end{pmatrix} \begin{pmatrix} x\\ y\\ z \end{pmatrix}=\begin{pmatrix} 0\\ 0\\ 0 \end{pmatrix}$$ $$\iff ay+bz=0, 2y+2z=0, 2y+2z=0.$$ $$\iff y=(-bz)/a, y=-z, z=-y$$ So, $v\in ker(A--1I) \iff v=\begin{pmatrix} x\\ (-b•-y)/a\\ -y \end{pmatrix}=x\begin{pmatrix}  1\\ 0\\ 0 \end{pmatrix}+y\begin{pmatrix} 0\\ b/a\\ -1 \end{pmatrix}$ . Similarly for $\lambda=3$ , $v\in ker(A-3I)\iff$ $x=(ay)/4+(bz)4, y=z, z=y \iff$ $$ v=\begin{pmatrix}  (ay)/4 + (bz)/4\\ y\\ z \end{pmatrix}$$ $$=y\begin{pmatrix} a/4\\ 1\\ 0 \end{pmatrix}+z\begin{pmatrix} b/4\\ 0\\ 1 \end{pmatrix}.$$ But after this, I don't know how to find the values of $a,b$ such that $A$ is diagonalizable.","Find all values of such that is diagonalizable. So far, I have that: So has eigenvalues , Now, let . Then So, . Similarly for , But after this, I don't know how to find the values of such that is diagonalizable.","a,b\in\mathbb{R} A A=\begin{pmatrix}
-1 & a & b\\
0 & 1 & 2\\
0 & 2 & 1\\
\end{pmatrix}. det(A-\lambda I)=\begin{vmatrix} 
-1-\lambda & a & b\\
0 & 1-\lambda & 2\\
0 & 2 & 1-\lambda\\
\end{vmatrix}=-(\lambda +1)^2(\lambda -3). A \lambda=-1 \lambda=3 v=(x,y,z)^T v\in ker(A--1I)\iff \begin{pmatrix} 
0&a&b\\
0&2&2\\
0&2&2
\end{pmatrix}
\begin{pmatrix}
x\\
y\\
z
\end{pmatrix}=\begin{pmatrix}
0\\
0\\
0
\end{pmatrix} \iff ay+bz=0, 2y+2z=0, 2y+2z=0. \iff y=(-bz)/a, y=-z, z=-y v\in ker(A--1I) \iff v=\begin{pmatrix}
x\\
(-b•-y)/a\\
-y
\end{pmatrix}=x\begin{pmatrix} 
1\\
0\\
0
\end{pmatrix}+y\begin{pmatrix}
0\\
b/a\\
-1
\end{pmatrix} \lambda=3 v\in ker(A-3I)\iff x=(ay)/4+(bz)4, y=z, z=y \iff  v=\begin{pmatrix} 
(ay)/4 + (bz)/4\\
y\\
z
\end{pmatrix} =y\begin{pmatrix}
a/4\\
1\\
0
\end{pmatrix}+z\begin{pmatrix}
b/4\\
0\\
1
\end{pmatrix}. a,b A","['linear-algebra', 'matrices', 'linear-transformations', 'diagonalization']"
34,How can I calculate the determinant without knowing every element of the matrix? [closed],How can I calculate the determinant without knowing every element of the matrix? [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question I was given the following matrix and asked only to find the determinant. As far as I know we need to know every element of the matrix in order to do that. But maybe I'm missing something. Any help is appreciated. Thanks in advance.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question I was given the following matrix and asked only to find the determinant. As far as I know we need to know every element of the matrix in order to do that. But maybe I'm missing something. Any help is appreciated. Thanks in advance.",,"['linear-algebra', 'matrices', 'determinant']"
35,Which conditions make an associative algebra a matrix algebra?,Which conditions make an associative algebra a matrix algebra?,,"If I have an algebra over a field, which additional conditions would make this algebra uniquely the matrix algebra? I'm already assuming that I have an $n^2$ dimensional vector-space equipped with an associative bilinear product. I think this still allows for more spaces than just matrices? Which additional conditions would fix this to be the matrix algebra over the field?","If I have an algebra over a field, which additional conditions would make this algebra uniquely the matrix algebra? I'm already assuming that I have an dimensional vector-space equipped with an associative bilinear product. I think this still allows for more spaces than just matrices? Which additional conditions would fix this to be the matrix algebra over the field?",n^2,"['abstract-algebra', 'matrices']"
36,Product of a positive semi-definite matrix and a diagonal matrix [closed],Product of a positive semi-definite matrix and a diagonal matrix [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question Let $A$ be a positive semi-definite matrix. $B$ is a diagonal matrix with positive diagonal entries. Let $x$ and $y$ be vectors. Is it true that $$ x'y > (<) 0 \Longrightarrow x'ABy \geq (\leq) 0 $$","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question Let be a positive semi-definite matrix. is a diagonal matrix with positive diagonal entries. Let and be vectors. Is it true that","A B x y 
x'y > (<) 0 \Longrightarrow x'ABy \geq (\leq) 0
","['linear-algebra', 'matrices', 'positive-semidefinite']"
37,Is the subset of symmetric $ 3\times 3 $ matrices with given eigenvalues a manifold?,Is the subset of symmetric  matrices with given eigenvalues a manifold?, 3\times 3 ,"For $ \mu_1>\mu_2>\mu_3 $ are three real numbers. Consider th set $$ S(\mu_1,\mu_2,\mu_3)=\{A\in M(3,\mathbb{R}):,A^T=A,\,\,\lambda_1(A)=\mu_1,\lambda_2(A)=\mu_2,\lambda_3(A)=\mu_3\}, $$ where $ \lambda_1(A)>\lambda_2(A)>\lambda_3(A) $ are eigenvalues of $ A $ and $ M(3,\mathbb{R}) $ denotes the sets of $ 3\times 3 $ matrices. Then the questions is that if $ S(\mu_1,\mu_2,\mu_3) $ is a manifold. Here is my try, define $$ f:\{A\in M(3,\mathbb{R}):A^T=A\}\to\mathbb{R}^3\quad(f(A)=(\lambda_1(A),\lambda_2(A),\lambda_3(A))). $$ I want to show that for any $ \mu_1>\mu_2>\mu_3 $ the point $ (\mu_1,\mu_2,\mu_3) $ is a regular point of $ f $ and then $ S(\mu_1,\mu_2,\mu_3) $ is a manifold. However I cannot deal with the derivatives of $ f $ . Can you give me some hints or references?","For are three real numbers. Consider th set where are eigenvalues of and denotes the sets of matrices. Then the questions is that if is a manifold. Here is my try, define I want to show that for any the point is a regular point of and then is a manifold. However I cannot deal with the derivatives of . Can you give me some hints or references?"," \mu_1>\mu_2>\mu_3  
S(\mu_1,\mu_2,\mu_3)=\{A\in M(3,\mathbb{R}):,A^T=A,\,\,\lambda_1(A)=\mu_1,\lambda_2(A)=\mu_2,\lambda_3(A)=\mu_3\},
  \lambda_1(A)>\lambda_2(A)>\lambda_3(A)   A   M(3,\mathbb{R})   3\times 3   S(\mu_1,\mu_2,\mu_3)  
f:\{A\in M(3,\mathbb{R}):A^T=A\}\to\mathbb{R}^3\quad(f(A)=(\lambda_1(A),\lambda_2(A),\lambda_3(A))).
  \mu_1>\mu_2>\mu_3   (\mu_1,\mu_2,\mu_3)   f   S(\mu_1,\mu_2,\mu_3)   f ","['linear-algebra', 'matrices', 'algebraic-topology', 'manifolds', 'differential-topology']"
38,Derivation of inverse matrix [duplicate],Derivation of inverse matrix [duplicate],,This question already has answers here : Derivative of the inverse of a matrix (5 answers) Closed 7 months ago . My question is about how can reach to the formula $$\frac{dA^{-1}}{dt}=-A^{-1}\frac{dA}{dt}A^{-1}$$ when $A$ is a matrix. It is very similar to $$\frac{dx^{-1}}{dt}=-x^{-2}\frac{dx}{dt}$$ And I know we can write $$A^{-1}=A^{-1}I\\A^{-1}=A^{-1}AA^{-1}$$ then it seems to be $$\frac{dA^{-1}}{dt}=\frac{dA^{-1}}{dt}AA^{-1} +A^{-1}\frac{dA}{dt}A^{-1}+A^{-1}A\frac{dA^{-1}}{dt}$$ in this part: how can I simplify right hand side? to obtain $\frac{dA^{-1}}{dt}=-A^{-1}\frac{dA}{dt}A^{-1}$ Any hint will be appreciated.,This question already has answers here : Derivative of the inverse of a matrix (5 answers) Closed 7 months ago . My question is about how can reach to the formula when is a matrix. It is very similar to And I know we can write then it seems to be in this part: how can I simplify right hand side? to obtain Any hint will be appreciated.,\frac{dA^{-1}}{dt}=-A^{-1}\frac{dA}{dt}A^{-1} A \frac{dx^{-1}}{dt}=-x^{-2}\frac{dx}{dt} A^{-1}=A^{-1}I\\A^{-1}=A^{-1}AA^{-1} \frac{dA^{-1}}{dt}=\frac{dA^{-1}}{dt}AA^{-1} +A^{-1}\frac{dA}{dt}A^{-1}+A^{-1}A\frac{dA^{-1}}{dt} \frac{dA^{-1}}{dt}=-A^{-1}\frac{dA}{dt}A^{-1},"['linear-algebra', 'matrices', 'derivatives', 'inverse', 'matrix-decomposition']"
39,"Linear Programming: What exactly is the basis matrix B, and how do we find its inverse from a given simplex tableau?","Linear Programming: What exactly is the basis matrix B, and how do we find its inverse from a given simplex tableau?",,"The explanation given in my textbook, where $A$ is the coefficient matrix of the linear program: Let $\mathbf{A}_j$ denote the $j^{\text{th}}$ column of $A$ and $\mathscr{B} = \{B_1,B_2,\dots,B_m\} ⊆ \{1,2,\dots,n\}$ be such that the columns $\mathbf{A}_{B_1}, \mathbf{A}_{B_2},\dots,\mathbf{A}_{B_m}$ are linearly independent. Then the $m×m$ matrix $B = [\mathbf{A}_{B_1}, \mathbf{A}_{B_2},\dots, \mathbf{A}_{B_m}]$ is invertible. Let $\mathscr{N} = \{1,2,\dots,n\} − \mathscr{B} = \{ℓ_1, ℓ_2,\dots, ℓ_{n−m}\}$ and $N = [\mathbf{A}_{ℓ_1}, \mathbf{A}_{ℓ_2} ,\dots, \mathbf{A}_{ℓ_{n−m}}]$ . I'm struggling to understand how $B$ isn't just always the identity matrix (if it's the linearly independent columns of the coefficient matrix), and I have so many questions. How exactly is $B$ , or $B^{-1}$ , obtained from a simplex tableau? I've had a ton of abstract stuff in this book so I'm looking for something at least mildly concrete and intuitive. I'm starting learning about the revised simplex method, so apparently this requires knowing exactly what $B^{-1}$ is on every iteration of the revised method.","The explanation given in my textbook, where is the coefficient matrix of the linear program: Let denote the column of and be such that the columns are linearly independent. Then the matrix is invertible. Let and . I'm struggling to understand how isn't just always the identity matrix (if it's the linearly independent columns of the coefficient matrix), and I have so many questions. How exactly is , or , obtained from a simplex tableau? I've had a ton of abstract stuff in this book so I'm looking for something at least mildly concrete and intuitive. I'm starting learning about the revised simplex method, so apparently this requires knowing exactly what is on every iteration of the revised method.","A \mathbf{A}_j j^{\text{th}} A \mathscr{B} = \{B_1,B_2,\dots,B_m\} ⊆ \{1,2,\dots,n\} \mathbf{A}_{B_1},
\mathbf{A}_{B_2},\dots,\mathbf{A}_{B_m} m×m B = [\mathbf{A}_{B_1}, \mathbf{A}_{B_2},\dots,
\mathbf{A}_{B_m}] \mathscr{N} = \{1,2,\dots,n\} − \mathscr{B} = \{ℓ_1, ℓ_2,\dots, ℓ_{n−m}\} N = [\mathbf{A}_{ℓ_1}, \mathbf{A}_{ℓ_2} ,\dots, \mathbf{A}_{ℓ_{n−m}}] B B B^{-1} B^{-1}","['linear-algebra', 'matrices', 'linear-programming']"
40,Element of order $p + 1$ in $\mathrm{GL}_2(\mathbb{F}_p)$,Element of order  in,p + 1 \mathrm{GL}_2(\mathbb{F}_p),"Is there a way to explicitly construct a matrix of (multiplicative) order $p + 1$ in $\mathrm{GL}_2(\mathbb{F}_p)$ ? I am aware that by considering the cyclic multiplicative group $\mathbb{F}_{p^2}^\times \subseteq \mathrm{GL}_2(\mathbb{F}_p)$ , there is in fact a matrix of order $p^2 - 1$ , which gives us a matrix of order $p + 1$ (see e.g. this ). However, this method is not exactly constructive, and I wonder if there's an easier way to get one of order $p + 1$ .","Is there a way to explicitly construct a matrix of (multiplicative) order in ? I am aware that by considering the cyclic multiplicative group , there is in fact a matrix of order , which gives us a matrix of order (see e.g. this ). However, this method is not exactly constructive, and I wonder if there's an easier way to get one of order .",p + 1 \mathrm{GL}_2(\mathbb{F}_p) \mathbb{F}_{p^2}^\times \subseteq \mathrm{GL}_2(\mathbb{F}_p) p^2 - 1 p + 1 p + 1,"['linear-algebra', 'matrices', 'finite-fields']"
41,"Show that $K(t) = \int_0^t \mathrm{e}^{-Cs} \, D \, \mathrm{e}^{-C^\intercal s} \,\mathrm{d} s$ satisfies a given equation involving $K(\infty)$",Show that  satisfies a given equation involving,"K(t) = \int_0^t \mathrm{e}^{-Cs} \, D \, \mathrm{e}^{-C^\intercal s} \,\mathrm{d} s K(\infty)","Assume that for each $t \geq 0$ , we have a matrix $$K(t) = \int_0^t \mathrm{e}^{-Cs} D\, \mathrm{e}^{-C^\intercal s} \mathrm{d} s$$ where $D \in \mathbb{R}^{d \times d}$ is constant, symmetric, and positive semi-definite and that $C \in \mathbb{R}^{d \times d}$ . It is claimed in some papers (without details unfortunately) that we have $$K_\infty = K(t) + \mathrm{e}^{-Ct}\,K_\infty\, \mathrm{e}^{-C^\intercal t} \label{1}\tag{1}$$ for all $t \geq 0$ , where $$K_\infty = \int_0^\infty \mathrm{e}^{-Cs} D\, \mathrm{e}^{-C^\intercal s} \mathrm{d} s.$$ May I know how can we check the validity of equation \eqref{1} from basic matrix calculus? Remark: For a given matrix $A \in \mathbb{R}^{d \times d}$ , we denote its transpose by $A^\intercal$ .","Assume that for each , we have a matrix where is constant, symmetric, and positive semi-definite and that . It is claimed in some papers (without details unfortunately) that we have for all , where May I know how can we check the validity of equation \eqref{1} from basic matrix calculus? Remark: For a given matrix , we denote its transpose by .","t \geq 0 K(t) = \int_0^t \mathrm{e}^{-Cs} D\, \mathrm{e}^{-C^\intercal s} \mathrm{d} s D \in \mathbb{R}^{d \times d} C \in \mathbb{R}^{d \times d} K_\infty = K(t) + \mathrm{e}^{-Ct}\,K_\infty\, \mathrm{e}^{-C^\intercal t} \label{1}\tag{1} t \geq 0 K_\infty = \int_0^\infty \mathrm{e}^{-Cs} D\, \mathrm{e}^{-C^\intercal s} \mathrm{d} s. A \in \mathbb{R}^{d \times d} A^\intercal","['matrices', 'matrix-equations', 'matrix-calculus']"
42,Positive-definite matrix with simple structure,Positive-definite matrix with simple structure,,"The $n\times n$ matrix $A$ has this specific structure: The diagonal entries $a_{ii}$ are strictly increasing and strictly positive real numbers; The non-diagonal entries are calculated as $a_{ij}=a_{ji}=a_{ii}$ where $i$ is the smaller index. In other words, $a_{ij}=a_{ji}=\min(a_{ii},a_{jj})$ . Is the matrix $A$ positive-definite? For the $1\times1$ case, the result is trivial. For the $2\times2$ case, we can use Sylvester's criterion and see that $a_{11}a_{22}-a_{11}^2>0$ is true, so $A$ is positive-definite. Is there an argument that can be used to extend this result to larger matrices? Some empirical results I generated one million $20\times20$ matrices using an exponential random variable for the diagonal. Every random matrix is positive-definite. I expect this result to be true in general but I just don't find a proof.","The matrix has this specific structure: The diagonal entries are strictly increasing and strictly positive real numbers; The non-diagonal entries are calculated as where is the smaller index. In other words, . Is the matrix positive-definite? For the case, the result is trivial. For the case, we can use Sylvester's criterion and see that is true, so is positive-definite. Is there an argument that can be used to extend this result to larger matrices? Some empirical results I generated one million matrices using an exponential random variable for the diagonal. Every random matrix is positive-definite. I expect this result to be true in general but I just don't find a proof.","n\times n A a_{ii} a_{ij}=a_{ji}=a_{ii} i a_{ij}=a_{ji}=\min(a_{ii},a_{jj}) A 1\times1 2\times2 a_{11}a_{22}-a_{11}^2>0 A 20\times20","['linear-algebra', 'matrices', 'positive-definite']"
43,How to prove $\det{AB}=\det{A}\det{B}$ with Leibniz formula in terms of Levi-Civita symbol and Einstein summation notation? [closed],How to prove  with Leibniz formula in terms of Levi-Civita symbol and Einstein summation notation? [closed],\det{AB}=\det{A}\det{B},"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 11 months ago . Improve this question Prove that $\det{AB}=\det{A}\det{B}$ with Leibniz formula in terms of Levi-Civita symbol and Einstein summation notation Here is a similar question asked 6 years ago. The OP answered in the question that ""expanding will give the answer which is then trivial by inspection"", but I still can't understand why it is ""trivial"".","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 11 months ago . Improve this question Prove that with Leibniz formula in terms of Levi-Civita symbol and Einstein summation notation Here is a similar question asked 6 years ago. The OP answered in the question that ""expanding will give the answer which is then trivial by inspection"", but I still can't understand why it is ""trivial"".",\det{AB}=\det{A}\det{B},"['linear-algebra', 'matrices', 'determinant', 'tensors', 'index-notation']"
44,orthogonal vs orthonormal matrices - what are simplest possible definitions and examples of each ??,orthogonal vs orthonormal matrices - what are simplest possible definitions and examples of each ??,,"I'm trying to understand orthogonal and orthonormal matrices and I'm very confused.  Unfortunately most sources I've found have unclear definitions, and many have conflicting definitions! Some sites like for example https://en.wikipedia.org/wiki/Orthogonal_matrix seem to imply these are the same thing, but most others at least imply if not directly state that they are different. Before someone marks this question as a duplicate I've already consulted Difference between orthogonal and orthonormal matrices and the provided answers do not make the definitions of orthogonal and orthonormal matrices clear, at least not to me. Also, I've been able to find various definitions and verbiage, but I've been able to find very few, if any, examples of matrices that are or aren't orthogonal/orthonormal/both/neither. Additionally I should clarify I'm concerned with the definition of orthogonal and orthonormal matrices, not orthogonal/orthonormal vectors. Based on the sources available and my current understanding of them, this seems to be the definition of each: def. of orthogonal matrix: $AA^T = I$ and: $A^{-1} = A^T$ def. of orthonormal matrix: meets the definition of orthogonal (above) and also: $determinant(A) = 1$ So my questions are: Are these definitions correct?  Please don't throw complicated math formulas at me here, I'm ideally looking for simple logic that I could code in a Python/NumPy if statement (I'm an engineer not a math person!).  If these definitions are not correct, what should they be? What would be examples of matrices that are: a) not orthogonal or orthonormal b) orthogonal but not orthonormal c) orthonormal but not orthogonal (if this is possible, my current understanding is it's not) d) orthogonal and orthonormal","I'm trying to understand orthogonal and orthonormal matrices and I'm very confused.  Unfortunately most sources I've found have unclear definitions, and many have conflicting definitions! Some sites like for example https://en.wikipedia.org/wiki/Orthogonal_matrix seem to imply these are the same thing, but most others at least imply if not directly state that they are different. Before someone marks this question as a duplicate I've already consulted Difference between orthogonal and orthonormal matrices and the provided answers do not make the definitions of orthogonal and orthonormal matrices clear, at least not to me. Also, I've been able to find various definitions and verbiage, but I've been able to find very few, if any, examples of matrices that are or aren't orthogonal/orthonormal/both/neither. Additionally I should clarify I'm concerned with the definition of orthogonal and orthonormal matrices, not orthogonal/orthonormal vectors. Based on the sources available and my current understanding of them, this seems to be the definition of each: def. of orthogonal matrix: and: def. of orthonormal matrix: meets the definition of orthogonal (above) and also: So my questions are: Are these definitions correct?  Please don't throw complicated math formulas at me here, I'm ideally looking for simple logic that I could code in a Python/NumPy if statement (I'm an engineer not a math person!).  If these definitions are not correct, what should they be? What would be examples of matrices that are: a) not orthogonal or orthonormal b) orthogonal but not orthonormal c) orthonormal but not orthogonal (if this is possible, my current understanding is it's not) d) orthogonal and orthonormal",AA^T = I A^{-1} = A^T determinant(A) = 1,"['matrices', 'orthogonality', 'orthonormal']"
45,Why does Wolfram say this symmetric matrix has complex eigenvalues?,Why does Wolfram say this symmetric matrix has complex eigenvalues?,,"According to Wolfram , the following matrix has complex eigenvalues. Symmetric matrices have real eigenvalues, so I’m not sure what I’m failing to understand. The matrix is the shape operator of the hypersurface $f(x,y,z) = (x,y,z,xyz)$ . I don’t see why this manifold would have complex-valued principal curvatures.","According to Wolfram , the following matrix has complex eigenvalues. Symmetric matrices have real eigenvalues, so I’m not sure what I’m failing to understand. The matrix is the shape operator of the hypersurface . I don’t see why this manifold would have complex-valued principal curvatures.","f(x,y,z) = (x,y,z,xyz)","['matrices', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
46,Prove or disprove $\det(6(A^3 + B^3 + C^3)+(A+B+C)^3) \ge 5^2\det(A^2 + B^2 + C^2)\cdot\det(A + B+ C)$,Prove or disprove,\det(6(A^3 + B^3 + C^3)+(A+B+C)^3) \ge 5^2\det(A^2 + B^2 + C^2)\cdot\det(A + B+ C),"Let $A, B, C$ be $2\times 2$ Hermitian positive semidefinite (PSD) matrices. Prove or disprove that $$\det(6(A^3 + B^3 + C^3)+(A+B+C)^3) \ge 5^2\det(A^2 + B^2 + C^2)\det(A + B+ C).$$ This problem is inspired by these two problems: P1 , P2 . When I tried to deal with P2, I want to eliminate the constraint $A+B+C = I_2$ . So I propose this problem. I just do some numerical experiments and do not find an counterexample yet. We can use the fact that for $2\times 2$ matrix $S$ , $$\det S = \frac12 (\mathrm{tr}(S))^2 - \frac12\mathrm{tr}(S^2).$$ An idea is to use $A = u_1u_1^H + u_2u_2^H, B = v_1v_1^H + v_2v_2^H, C = w_1w_1^H + w_2w_2^H$ . Then we have a unconstrained optimization of $f(u_1, u_2, v_1, v_2, w_1, w_2)$ . I also consider the following: Let $A, B$ be $2\times 2$ Hermitian positive semidefinite (PSD) matrices. Prove or disprove that $$\det(6(A^3 + B^3 + I_2)+(A+B+I_2)^3) \ge 5^2\det(A^2 + B^2 + I_2)\det(A + B+ I_2).$$","Let be Hermitian positive semidefinite (PSD) matrices. Prove or disprove that This problem is inspired by these two problems: P1 , P2 . When I tried to deal with P2, I want to eliminate the constraint . So I propose this problem. I just do some numerical experiments and do not find an counterexample yet. We can use the fact that for matrix , An idea is to use . Then we have a unconstrained optimization of . I also consider the following: Let be Hermitian positive semidefinite (PSD) matrices. Prove or disprove that","A, B, C 2\times 2 \det(6(A^3 + B^3 + C^3)+(A+B+C)^3)
\ge 5^2\det(A^2 + B^2 + C^2)\det(A + B+ C). A+B+C = I_2 2\times 2 S \det S = \frac12 (\mathrm{tr}(S))^2 - \frac12\mathrm{tr}(S^2). A = u_1u_1^H + u_2u_2^H, B = v_1v_1^H + v_2v_2^H,
C = w_1w_1^H + w_2w_2^H f(u_1, u_2, v_1, v_2, w_1, w_2) A, B 2\times 2 \det(6(A^3 + B^3 + I_2)+(A+B+I_2)^3)
\ge 5^2\det(A^2 + B^2 + I_2)\det(A + B+ I_2).","['linear-algebra', 'matrices', 'inequality', 'determinant', 'hermitian-matrices']"
47,Determinant of all possible matrices with elements $\pm 1$,Determinant of all possible matrices with elements,\pm 1,"Given that A is an element of set $\mathbb{B}$ consisting of all $N\times N$ matrices with elements $\pm 1$ , how can I find the following sum: \begin{equation} \sum_{A\in \mathbb{B}} \det(A)  \end{equation} My initial approach was to use elementary row operations to reduce the matrix to the following form: \begin{equation} \begin{bmatrix} 1 & 0 \\ 0 & A^{N-1} \end{bmatrix} \end{equation} where $A^{N-1}$ consists of elements $\{-2,0,2\}$ . Since each row has those elements, we can divide each row to 2. That will reduce the determinant to $2^{N-1}A'$ . But the road ended here. My insight is that, as we have $\pm 1$ for each element, determinants should cancel out and give $0$ .","Given that A is an element of set consisting of all matrices with elements , how can I find the following sum: My initial approach was to use elementary row operations to reduce the matrix to the following form: where consists of elements . Since each row has those elements, we can divide each row to 2. That will reduce the determinant to . But the road ended here. My insight is that, as we have for each element, determinants should cancel out and give .","\mathbb{B} N\times N \pm 1 \begin{equation}
\sum_{A\in \mathbb{B}} \det(A) 
\end{equation} \begin{equation}
\begin{bmatrix}
1 & 0 \\
0 & A^{N-1}
\end{bmatrix}
\end{equation} A^{N-1} \{-2,0,2\} 2^{N-1}A' \pm 1 0","['linear-algebra', 'matrices', 'determinant']"
48,Why is gradient symmetric at optimal point for convex functions on the positive semidefinite cone?,Why is gradient symmetric at optimal point for convex functions on the positive semidefinite cone?,,"For a convex function $f: \mathbb{R}^{n \times n} \to \mathbb{R}$ , if $X^{*} = \underset{ X\succeq 0 }{ \operatorname{arg min}}f(X)$ , then we have $$\nabla f(X^{*})\succeq 0, \qquad X^{*} \succeq  0, \qquad \left\langle \nabla f(X^{*}),X^{*} \right\rangle = 0$$ How can we show $\nabla f(X^{*})$ is symmetric? I have proved everything else (see below). However, I'm running out of techniques to show that $\nabla f(X^{*})$ is symmetric, I have tried to use orthogonal component of skew symmetric matrix, and some other techniques. My intuition is that this is quite easy, and can be proven by contradiction by finding a direction to decrease in the positive semidefinite range. However, I have not come up with anything solid. Proof: For $\nabla f(X^{*}) \succeq 0$ , suppose not, $\exists u$ s.t. $u^{T} \nabla f(X^{*})u = u^{T}\otimes u^{T}vec(\nabla f(X^{*})) < 0$ . $u^{T}\otimes u^{T} = uu^{T}$ which is a positive semidefinite matrix. Therefore, $\exists t > 0$ s.t. $f(X^{*}+tuu^{T}) < f(X^{*})$ , which contradicts that $X^{*}$ is the global minimum. Next we show $\langle \nabla f(X^{*}),X^{*} \rangle = 0$ . Suppose not, if $\langle \nabla f(X^{*}),X^{*} \rangle < 0$ , $\exists t> 0$ , s.t. $f(X^{*}+tX^{*}) < f(X^{*})$ . Similarly, if $\langle \nabla f(X^{*}),X^{*} \rangle > 0$ , $\langle \nabla f(X^{*}),X^{*} \rangle < 0$ , since we know for $0 < t < 1$ , $tX^{*} \succeq  0$ , we know $\exists 0<t<1$ s.t. $f(X^{*}-tX^{*})) < f(X^{*})$ . Both cases contradict that $X^{*}$ is the global minimum.","For a convex function , if , then we have How can we show is symmetric? I have proved everything else (see below). However, I'm running out of techniques to show that is symmetric, I have tried to use orthogonal component of skew symmetric matrix, and some other techniques. My intuition is that this is quite easy, and can be proven by contradiction by finding a direction to decrease in the positive semidefinite range. However, I have not come up with anything solid. Proof: For , suppose not, s.t. . which is a positive semidefinite matrix. Therefore, s.t. , which contradicts that is the global minimum. Next we show . Suppose not, if , , s.t. . Similarly, if , , since we know for , , we know s.t. . Both cases contradict that is the global minimum.","f: \mathbb{R}^{n \times n} \to \mathbb{R} X^{*} = \underset{ X\succeq 0 }{ \operatorname{arg min}}f(X) \nabla f(X^{*})\succeq 0, \qquad X^{*} \succeq  0, \qquad \left\langle \nabla f(X^{*}),X^{*} \right\rangle = 0 \nabla f(X^{*}) \nabla f(X^{*}) \nabla f(X^{*}) \succeq 0 \exists u u^{T} \nabla f(X^{*})u = u^{T}\otimes u^{T}vec(\nabla f(X^{*})) < 0 u^{T}\otimes u^{T} = uu^{T} \exists t > 0 f(X^{*}+tuu^{T}) < f(X^{*}) X^{*} \langle \nabla f(X^{*}),X^{*} \rangle = 0 \langle \nabla f(X^{*}),X^{*} \rangle < 0 \exists t> 0 f(X^{*}+tX^{*}) < f(X^{*}) \langle \nabla f(X^{*}),X^{*} \rangle > 0 \langle \nabla f(X^{*}),X^{*} \rangle < 0 0 < t < 1 tX^{*} \succeq  0 \exists 0<t<1 f(X^{*}-tX^{*})) < f(X^{*}) X^{*}","['linear-algebra', 'matrices', 'optimization', 'convex-optimization', 'positive-semidefinite']"
49,Linear Algebra Question On Augmented Matrix,Linear Algebra Question On Augmented Matrix,,"I read the book Linear Algebra by David Lay and stumbled upon a paragraph on page $40$ that read: If an augmented matrix [A b ] has a pivot position in every row, then the equation Ax = b may or may not be consistent. I can't seem to provide myself with an example for each case, and would appreciate any help. Thanks WY. Note: There is a similar question that has been posted here but it is really not the same as this one.","I read the book Linear Algebra by David Lay and stumbled upon a paragraph on page that read: If an augmented matrix [A b ] has a pivot position in every row, then the equation Ax = b may or may not be consistent. I can't seem to provide myself with an example for each case, and would appreciate any help. Thanks WY. Note: There is a similar question that has been posted here but it is really not the same as this one.",40,"['linear-algebra', 'matrices']"
50,Partial derivatives and composition with matrices,Partial derivatives and composition with matrices,,"Given $f$ and $g$ are each functions sending $(0,0)$ to $(0,0)$ , if you know that $$ [Df]_{(0,0)} =  \begin{pmatrix} 2 & -1 \\ 1 & 0 \\ \end{pmatrix} $$ and $$ [D(f \circ g)]_{(0,0)} =  \begin{pmatrix} -1 & 4 \\ 2 & 3  \end{pmatrix} $$ What is the derivative of $g$ at $(0,0)$ ? I believe the strategy is to replace two elements in the equation below (recognizing that $g$ and $f$ are switched from normal convention in the given problem): $ [D(g \circ f)]_{a} = [Dg]_{f(a)} \cdot [Df]_{a} $ Via substitution, I get: $$ \begin{pmatrix} -1 & 4 \\ 2 & 3  \end{pmatrix} = \begin{pmatrix} 2 & -1 \\ 1 & 0  \end{pmatrix} \begin{pmatrix} ? & ? \\ ? & ?  \end{pmatrix} $$ However, I don't know how to solve the unknown matrix from here. In a more generalized sense, is there a way to find matrix A when AB = C and we know B and C? Or, is there another way to solve this problem? I've only so far seen $Ax = b$ used when $x$ is a column vector. I tried to take the inverse of $A$ and multiply it with $C$ , but reverse checking my work, that didn't work out. (This is problem 2 in Robert Ghrist's textbook in multivariable calculus, volume 2, chapter 5.)","Given and are each functions sending to , if you know that and What is the derivative of at ? I believe the strategy is to replace two elements in the equation below (recognizing that and are switched from normal convention in the given problem): Via substitution, I get: However, I don't know how to solve the unknown matrix from here. In a more generalized sense, is there a way to find matrix A when AB = C and we know B and C? Or, is there another way to solve this problem? I've only so far seen used when is a column vector. I tried to take the inverse of and multiply it with , but reverse checking my work, that didn't work out. (This is problem 2 in Robert Ghrist's textbook in multivariable calculus, volume 2, chapter 5.)","f g (0,0) (0,0)  [Df]_{(0,0)} = 
\begin{pmatrix}
2 & -1 \\
1 & 0 \\
\end{pmatrix}   [D(f \circ g)]_{(0,0)} = 
\begin{pmatrix}
-1 & 4 \\
2 & 3 
\end{pmatrix}  g (0,0) g f  [D(g \circ f)]_{a} = [Dg]_{f(a)} \cdot [Df]_{a}  
\begin{pmatrix}
-1 & 4 \\
2 & 3 
\end{pmatrix} =
\begin{pmatrix}
2 & -1 \\
1 & 0 
\end{pmatrix}
\begin{pmatrix}
? & ? \\
? & ? 
\end{pmatrix}
 Ax = b x A C","['matrices', 'multivariable-calculus', 'partial-derivative', 'matrix-calculus', 'jacobian']"
51,A suspicion on orthonormal basis coefficients,A suspicion on orthonormal basis coefficients,,"Let's assume I have a $2$ dimensional vector space with inner product, and a basis where the inner product can be represented as $$ \begin{bmatrix}x &y\end{bmatrix} \begin{bmatrix}E &F\\F &G\end{bmatrix} \begin{bmatrix}x\\y\end{bmatrix}$$ In this basis, I express an orthonormal basis $\{e_1;e_2\}$ as $\{(e_{1x},e_{1y});(e_{2x},e_{2y})\}$ . I have the ""suspicion"", and I could not find any counterexample, that the inverse of the matrix above can be written as $$ \begin{bmatrix} e_{1x}^2+e_{2x}^2 & e_{1x}e_{1y}+e_{2x}e_{2y}\\ e_{1x}e_{1y}+e_{2x}e_{2y} &  e_{1y}^2+e_{2y}^2  \end{bmatrix} $$ for any orthonormal basis $\{e_1;e_2\}$ . I tried to set up the equalities that derive from orthogonality and normality of the various vectors and arrange them in a identity matrix, but I am stuck there and I can't find any way to extraxt an inverse of my $EFFG$ matrix. I feel there may be a straightforward way to prove this (or I am wrong upfront). What could be a method to prove this? Also, how would this extend to $n>2$ ? thanks","Let's assume I have a dimensional vector space with inner product, and a basis where the inner product can be represented as In this basis, I express an orthonormal basis as . I have the ""suspicion"", and I could not find any counterexample, that the inverse of the matrix above can be written as for any orthonormal basis . I tried to set up the equalities that derive from orthogonality and normality of the various vectors and arrange them in a identity matrix, but I am stuck there and I can't find any way to extraxt an inverse of my matrix. I feel there may be a straightforward way to prove this (or I am wrong upfront). What could be a method to prove this? Also, how would this extend to ? thanks","2 
\begin{bmatrix}x &y\end{bmatrix}
\begin{bmatrix}E &F\\F &G\end{bmatrix}
\begin{bmatrix}x\\y\end{bmatrix} \{e_1;e_2\} \{(e_{1x},e_{1y});(e_{2x},e_{2y})\} 
\begin{bmatrix}
e_{1x}^2+e_{2x}^2 & e_{1x}e_{1y}+e_{2x}e_{2y}\\
e_{1x}e_{1y}+e_{2x}e_{2y} & 
e_{1y}^2+e_{2y}^2 
\end{bmatrix}
 \{e_1;e_2\} EFFG n>2","['linear-algebra', 'matrices', 'orthonormal']"
52,How to solve this system of linear equations using Gaussian elimination?,How to solve this system of linear equations using Gaussian elimination?,,"I'm having trouble with this problem from my linear algebra course. The problem is: A new restaurant owner decides to have 20 tables for her guests, a certain number of tables with space for 4 people, some with space for 6 people and also one or more tables with space for 8 guests. In total, there are 108 seats available, but if only half of the seats at the 4-tables and 6-tables, and only a quarter of the seats at the 8-tables, are occupied, the restaurant will have 46 guests. How many tables with space for 4, 6 and 8 seats does she need to set up in her premises? I tried to solve it by writing the system of equations as a matrix: $$\left( \begin{array}{cccc} 1&1&1&20\\  2&3&2&46\\  4&6&8&108 \end{array} \right) $$ Then I applied Gaussian elimination by doing the following steps: Subtracting 2 times the first row from the second row Swapping the first and third rows Multiplying the first row by 1/4 Subtracting the second row from the third row Subtracting 2/3 times the second row from the first row Subtracting 2 times the third row from the first row Adding the first row to the third row This gave me the following matrix: $$ \left( \begin{array}{cccc} -1&0&0&-\frac{5}{2} \\  0&1&0&6\\  0&0&1&\frac{23}{2}  \end{array} \right) $$ However, this does not match the answer given in the textbook, which is: $$ \left(\begin{array}{cccc} 1 & 0 & 0 & 10 \\ 0 & 1 & 0 & 6 \\ 0 & 0 & 1 & 4 \end{array}\right) $$ Can someone please explain where I went wrong and how to get the correct answer? I appreciate any help or hints. Thank you!","I'm having trouble with this problem from my linear algebra course. The problem is: A new restaurant owner decides to have 20 tables for her guests, a certain number of tables with space for 4 people, some with space for 6 people and also one or more tables with space for 8 guests. In total, there are 108 seats available, but if only half of the seats at the 4-tables and 6-tables, and only a quarter of the seats at the 8-tables, are occupied, the restaurant will have 46 guests. How many tables with space for 4, 6 and 8 seats does she need to set up in her premises? I tried to solve it by writing the system of equations as a matrix: Then I applied Gaussian elimination by doing the following steps: Subtracting 2 times the first row from the second row Swapping the first and third rows Multiplying the first row by 1/4 Subtracting the second row from the third row Subtracting 2/3 times the second row from the first row Subtracting 2 times the third row from the first row Adding the first row to the third row This gave me the following matrix: However, this does not match the answer given in the textbook, which is: Can someone please explain where I went wrong and how to get the correct answer? I appreciate any help or hints. Thank you!","\left( \begin{array}{cccc}
1&1&1&20\\ 
2&3&2&46\\ 
4&6&8&108
\end{array} \right)
 
\left( \begin{array}{cccc}
-1&0&0&-\frac{5}{2} \\ 
0&1&0&6\\ 
0&0&1&\frac{23}{2} 
\end{array} \right)
 
\left(\begin{array}{cccc}
1 & 0 & 0 & 10 \\
0 & 1 & 0 & 6 \\
0 & 0 & 1 & 4
\end{array}\right)
","['linear-algebra', 'matrices', 'systems-of-equations', 'gaussian-elimination']"
53,Geometric interpretation of vector and transpose?,Geometric interpretation of vector and transpose?,,"$v^Tv$ is the dot product of a vector with itself, which is just its norm squared, an intuitive geometric quantity. Is there something to be said about $vv^T$ ? Is there some kind of relationship between this matrix and some geometric object the way the cross product relates to the area of the parallelogram for example?","is the dot product of a vector with itself, which is just its norm squared, an intuitive geometric quantity. Is there something to be said about ? Is there some kind of relationship between this matrix and some geometric object the way the cross product relates to the area of the parallelogram for example?",v^Tv vv^T,"['linear-algebra', 'matrices', 'vectors', 'intuition', 'geometric-interpretation']"
54,Problem Proof of matrix Invertibility,Problem Proof of matrix Invertibility,,Let 2 $v \times 1$ column matrices X and Y such that the $v \times v$ matrix $A = I +XY^{T}$ to be invertible.Show that $A^{-1}= I - \frac{1}{1+Y^{T}X}XY^{T}$ My effort: I have to show that $AA^{-1}=I$ . So: $$AA^{-1}=(I +XY^{T})\left(I - \frac{1}{1+Y^{T}X}XY^{T}\right)$$ \begin{align*} AA^{-1}&=(I +XY^{T})\left(I - \frac{1}{1+Y^{T}X}XY^{T}\right)\\ &= I\left(I - \frac{1}{1+Y^{T}X}XY^{T}\right) + XY^{T}\left(I - \frac{1}{1+Y^{T}X}XY^{T}\right)\\ &= I - \frac{1}{1+Y^{T}X}XY^{T} + XY^{T} - \frac{1}{1+Y^{T}X}XY^{T}XY^{T}\\ \end{align*} How I proceed further ?,Let 2 column matrices X and Y such that the matrix to be invertible.Show that My effort: I have to show that . So: How I proceed further ?,"v \times 1 v \times v A = I +XY^{T} A^{-1}= I - \frac{1}{1+Y^{T}X}XY^{T} AA^{-1}=I AA^{-1}=(I +XY^{T})\left(I - \frac{1}{1+Y^{T}X}XY^{T}\right) \begin{align*}
AA^{-1}&=(I +XY^{T})\left(I - \frac{1}{1+Y^{T}X}XY^{T}\right)\\
&= I\left(I - \frac{1}{1+Y^{T}X}XY^{T}\right) + XY^{T}\left(I - \frac{1}{1+Y^{T}X}XY^{T}\right)\\
&= I - \frac{1}{1+Y^{T}X}XY^{T} + XY^{T} - \frac{1}{1+Y^{T}X}XY^{T}XY^{T}\\
\end{align*}","['linear-algebra', 'matrices', 'vectors']"
55,Invariant subspaces in the context of representation theory,Invariant subspaces in the context of representation theory,,"In what follows, let $G$ be a finite group, $F$ a field and $V$ a vector space over $F$ . First we fix some definitions: Representation: A homomorphism $\phi:G\to GL(V)$ . In this sense, if $V$ is a finite- $n$ -dimensional vector space over $F$ , we can conflate $\rho$ with the image of $\rho(G)$ a finite collection of $n\times n$ matrices over $F$ corresponding to the action of $g$ on $v\in V$ . Sometimes, we will conflate $\rho(G)$ with $V$ itself. Subrepresentation: If $W$ is a subspace of $V$ then we say $W$ is a subrepresentation of $V$ if it is $G$ -invariant. i.e. the action of $g\in G$ on vectors in $W$ map to other vectors in $W$ . It is clear to me from this that $V,\{0\}$ are $G$ -invariant. Now, my confusion with these concepts arises when trying to justify why we can conflate $V$ with $\rho(G)$ . This becomes particularly apparent when I try to prove Schur's lemma. First, I assume the following form of Maschke's Theorem: If $V$ is a representation of some finite group (by which we mean that there is a $rho:G\to GL(V)$ ), and there exists $U$ a subrepresentation of $V$ (taken to mean that there is a $G$ -invariant subspace $U$ of $V$ ), then there is another subrepresentation $W$ of $V$ so $V=U\oplus W$ . i.e. $V$ is the direct sum of $U, W$ . The version of Schur's lemma I am trying to understand in light of the above is as follows: If $M,N$ are irreducible representations of $G$ and $\phi$ is a $G$ -linear transformation in the sense that $\phi$ commutes with the action of $g\in G$ on vectors $V$ in the representation $M$ , then $\phi$ is either the zero map or an isomorphism. Furthermore, $\phi$ must take the form $\lambda I_V$ for $\lambda\in F$ . Here is proof attempt/understanding of the proof: We will show that $\ker{\phi},\text{Im}{\phi}$ are $G$ -invariant since they are subspaces of $V, W$ respectively, where $V,W$ are the vector spaces we conflate with the representations $M,N$ . Let $v\in\ker{\phi}$ . Then $\phi(v)=0$ . So $\phi(gv)=\rho(g)\phi(v)=\rho(0)=0$ , as required. Similarly for the image, let $y\in\text{Im}{\phi}$ then there is $x\in V$ so $\phi(x)=y$ . So $\phi(gx)=g\phi(x)=\rho(g)y$ . I think in this context $\rho$ is associated with $N$ since otherwise $\rho(g)y$ makes no sense. Please correct me if I am wrong. But anyway, this immediately implies that the kernel and image are subrepresentations. By the irreducibility of $M,N$ we conclude that $\phi$ is an isomorphism. A question which occurs to me immediately at this point in the proof is the necessity of all of this. i.e. could we not also prove this statement by the following reasoning: if $\phi$ is linear from $V\to W$ , then $V,W$ have the same dimension, otherwise the kernel fails to be non-trivial? Why the need to show $G$ -invariance? Does this statement alone not already give us the fact that $\phi$ is bijective and hence an isomorphism since certainly it is a homomorphism by its linearity which is assumed? As for the second bit of the proof, the logic seems to go completely over my head. We would like that show that $\phi$ takes the form $\lambda I_V$ . Now, in particular, I guess that $\phi$ is really a homomorphism of linear transformations since $M,N$ are representations which are simply a collection of transformations on some ambient vector spaces $V,W$ . i.e. It seems to me that $\phi$ maps elements of $L(V,V)$ onto elements of $L(W,W)$ . Am I mistaken in thinking this? Now since we just established that $V$ and $W$ have the same cardinality, $L(V,V)$ should be isomorphic to $L(W,W)$ ? I guess in some sense since $\phi$ is a map from $L(V,V)\to L(W,W)$ , we can identify it with a map from $V\to W$ ? Is this line of reasoning correct? If so, then I guess if $\phi$ was not of the form $\lambda I_v$ for some lambda, then it would have a non-trivial proper eigenspace in which case $W$ wouldn't be irreducible? Is this moving in the right direction? I guess my largest concern lies in understanding the justification behind why we can conflate all of these objects. Apologies for the long-winded question. Any help clearing all this up for me is immensely appreciated as I've been muddling around with it for a good while now. Thanks in advance! Edit: As a comment kindly pointed out, I should specify $F$ is an algebraically closed field. I figured out my point of confusion thanks to the poster below. As for the second part of Schur's Lemma, a proof is as follows. If $\phi$ is a $G$ -linear map over a complete field, it has eigenvalues in this field. Say one of them is $\lambda$ . Then consider the transformation $\phi-\lambda I$ . If this does not have the trivial kernel, then since $V$ is irreducible the kernel must be all of $V$ . i.e. $\phi-\lambda I=0$ . The result follows.","In what follows, let be a finite group, a field and a vector space over . First we fix some definitions: Representation: A homomorphism . In this sense, if is a finite- -dimensional vector space over , we can conflate with the image of a finite collection of matrices over corresponding to the action of on . Sometimes, we will conflate with itself. Subrepresentation: If is a subspace of then we say is a subrepresentation of if it is -invariant. i.e. the action of on vectors in map to other vectors in . It is clear to me from this that are -invariant. Now, my confusion with these concepts arises when trying to justify why we can conflate with . This becomes particularly apparent when I try to prove Schur's lemma. First, I assume the following form of Maschke's Theorem: If is a representation of some finite group (by which we mean that there is a ), and there exists a subrepresentation of (taken to mean that there is a -invariant subspace of ), then there is another subrepresentation of so . i.e. is the direct sum of . The version of Schur's lemma I am trying to understand in light of the above is as follows: If are irreducible representations of and is a -linear transformation in the sense that commutes with the action of on vectors in the representation , then is either the zero map or an isomorphism. Furthermore, must take the form for . Here is proof attempt/understanding of the proof: We will show that are -invariant since they are subspaces of respectively, where are the vector spaces we conflate with the representations . Let . Then . So , as required. Similarly for the image, let then there is so . So . I think in this context is associated with since otherwise makes no sense. Please correct me if I am wrong. But anyway, this immediately implies that the kernel and image are subrepresentations. By the irreducibility of we conclude that is an isomorphism. A question which occurs to me immediately at this point in the proof is the necessity of all of this. i.e. could we not also prove this statement by the following reasoning: if is linear from , then have the same dimension, otherwise the kernel fails to be non-trivial? Why the need to show -invariance? Does this statement alone not already give us the fact that is bijective and hence an isomorphism since certainly it is a homomorphism by its linearity which is assumed? As for the second bit of the proof, the logic seems to go completely over my head. We would like that show that takes the form . Now, in particular, I guess that is really a homomorphism of linear transformations since are representations which are simply a collection of transformations on some ambient vector spaces . i.e. It seems to me that maps elements of onto elements of . Am I mistaken in thinking this? Now since we just established that and have the same cardinality, should be isomorphic to ? I guess in some sense since is a map from , we can identify it with a map from ? Is this line of reasoning correct? If so, then I guess if was not of the form for some lambda, then it would have a non-trivial proper eigenspace in which case wouldn't be irreducible? Is this moving in the right direction? I guess my largest concern lies in understanding the justification behind why we can conflate all of these objects. Apologies for the long-winded question. Any help clearing all this up for me is immensely appreciated as I've been muddling around with it for a good while now. Thanks in advance! Edit: As a comment kindly pointed out, I should specify is an algebraically closed field. I figured out my point of confusion thanks to the poster below. As for the second part of Schur's Lemma, a proof is as follows. If is a -linear map over a complete field, it has eigenvalues in this field. Say one of them is . Then consider the transformation . If this does not have the trivial kernel, then since is irreducible the kernel must be all of . i.e. . The result follows.","G F V F \phi:G\to GL(V) V n F \rho \rho(G) n\times n F g v\in V \rho(G) V W V W V G g\in G W W V,\{0\} G V \rho(G) V rho:G\to GL(V) U V G U V W V V=U\oplus W V U, W M,N G \phi G \phi g\in G V M \phi \phi \lambda I_V \lambda\in F \ker{\phi},\text{Im}{\phi} G V, W V,W M,N v\in\ker{\phi} \phi(v)=0 \phi(gv)=\rho(g)\phi(v)=\rho(0)=0 y\in\text{Im}{\phi} x\in V \phi(x)=y \phi(gx)=g\phi(x)=\rho(g)y \rho N \rho(g)y M,N \phi \phi V\to W V,W G \phi \phi \lambda I_V \phi M,N V,W \phi L(V,V) L(W,W) V W L(V,V) L(W,W) \phi L(V,V)\to L(W,W) V\to W \phi \lambda I_v W F \phi G \lambda \phi-\lambda I V V \phi-\lambda I=0","['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors', 'representation-theory']"
56,Find Jordan Basis of triple differentiation operator in $\mathbb{R}_8[x]$,Find Jordan Basis of triple differentiation operator in,\mathbb{R}_8[x],"General information and notation at first. Let $A$ be matrix of $T$ and let $T$ be such operator, that $(1, x, x^2, \cdots x^8) \to (0, 0, 0, 6, 24x, 60x^2, 120x^3, 210x^4, 336x^5)$ $T^2$ : $(1, x, x^2 \cdots x^8) \to (0, 0, \cdots, 720, 5040x, 20160x^2)$ Transformation is nilpotent, since $T^3$ leads to zeroes. Also the only non-zero values are at the diagonal that is above of the main diagonal. Hence, all 9 eigenvalues equal to $0$ . In Jordan normal form of $A$ we have three Jordan blocks of size $3$ with $0$ at diagonal. Now back to the question. I suspect, there is some pretty interesting way to find Jordan basis. Something that involves playing around with coefficients. Factorials (perhaps, not completely) involved too, since, for example $x^8 \to 336x^5$ , where $336 = 8\cdot 7 \cdot 6$ . But I can't grasp this idea, unfortunately. I am acquainted with some general algorithms for finding basis but seems like I lack understanding of this theme. Anyway, what can we do here?","General information and notation at first. Let be matrix of and let be such operator, that : Transformation is nilpotent, since leads to zeroes. Also the only non-zero values are at the diagonal that is above of the main diagonal. Hence, all 9 eigenvalues equal to . In Jordan normal form of we have three Jordan blocks of size with at diagonal. Now back to the question. I suspect, there is some pretty interesting way to find Jordan basis. Something that involves playing around with coefficients. Factorials (perhaps, not completely) involved too, since, for example , where . But I can't grasp this idea, unfortunately. I am acquainted with some general algorithms for finding basis but seems like I lack understanding of this theme. Anyway, what can we do here?","A T T (1, x, x^2, \cdots x^8) \to (0, 0, 0, 6, 24x, 60x^2, 120x^3, 210x^4, 336x^5) T^2 (1, x, x^2 \cdots x^8) \to (0, 0, \cdots, 720, 5040x, 20160x^2) T^3 0 A 3 0 x^8 \to 336x^5 336 = 8\cdot 7 \cdot 6","['linear-algebra', 'matrices', 'jordan-normal-form']"
57,How to find ${\rm rank}(2I_n-A)$ where A is a square matrix of size $n$ and $A^3 - 6A^2 + 12A = 0_n$?,How to find  where A is a square matrix of size  and ?,{\rm rank}(2I_n-A) n A^3 - 6A^2 + 12A = 0_n,"I think the rank has to be $n$ since anything else would be impossible to prove with so little information about the matrix. $$\det(2I_n-A) = -P_A(2) = -\det(A - 2I_n) \ .$$ So, if I can show the characteristic polynomial is non-zero at $2$ , it would prove the rank is $n$ . I have no idea how to do that though. Maybe the characteristic polynomial can be deduced from $$A^3 - 6A^2 + 12A = 0$$ somehow? Not sure how, since the Hamilton-Cayley theorem doesn't work in the opposite direction. If I factorize the polynomial I get $$(A^2 - 6A + 12I_n)A = 0_n \ .$$ So, at least one of the factors has to have the determinant $0$ , but I want to prove that $A$ 's determinant isn't $0$ , so I'm not sure if factorizing helps here.","I think the rank has to be since anything else would be impossible to prove with so little information about the matrix. So, if I can show the characteristic polynomial is non-zero at , it would prove the rank is . I have no idea how to do that though. Maybe the characteristic polynomial can be deduced from somehow? Not sure how, since the Hamilton-Cayley theorem doesn't work in the opposite direction. If I factorize the polynomial I get So, at least one of the factors has to have the determinant , but I want to prove that 's determinant isn't , so I'm not sure if factorizing helps here.",n \det(2I_n-A) = -P_A(2) = -\det(A - 2I_n) \ . 2 n A^3 - 6A^2 + 12A = 0 (A^2 - 6A + 12I_n)A = 0_n \ . 0 A 0,"['linear-algebra', 'matrices', 'matrix-equations', 'matrix-rank', 'characteristic-polynomial']"
58,Prove that $A$ is invertible if $BA^2+A^2=I$,Prove that  is invertible if,A BA^2+A^2=I,"I was asked this question: There's $A,B \in M_3 (\mathbb{R})$ a square matrices from order $3\times3$ from the real numbers, that satisifes: $BA^2+A^2=I$ . Prove that $A$ is invertible and that the matrices $A$ and $B$ have the commutative property. $(AB=BA=I)$ Given that $B+A^2B=O$ . Prove that $B$ isn't invertible and calculate $|A|$ , write the two possibilities. My first attempt (But then I saw that determinant does not preserve the addition, then this attempt is completely wrong): Suppose $A$ isn't invertible, meaning $|A|=0$ . $BA^2+A^2=I \rightarrow A^2=I-BA^2$ And since we assumed that A isn't invertible: $|A^2|=|I|-B|A|^2 \rightarrow 0=1-B*0 \rightarrow 0=1$ Contradiction, therefore $A$ is invertible. My second attempt : $BA^2+A^2=I \rightarrow (B+I)A^2=I$ Now, we need to show that $A^2(B+I)=(B+I)A^2=I$ , which will prove that $A$ is invertible and found an inverse for it. But I wasn't sure how to continue from here... And since $A$ is invertible then: $BA^2+A^2=AB^2+A^2=I \rightarrow A^2(B+I)=(B+I)A^2=I$ Which proved that the commutative property exists. My attempt on the second question: (But it also relies on adding determinants, then it must be incorrect too) We'll prove it by contradiction, $B$ is inverse, meaning $|B|\neq 0$ : $B+A^2B=O \rightarrow B=O-A^2B \rightarrow |B|=-|A^2||B|$ And from $1$ . we know that $|A|\neq 0$ , and since we assumed that $|B| \neq 0$ then the determinant of $B$ isn't necessarily always $0$ , therefore it is a contradiction. I'd love for guidance on how to continue. Thanks for the help!","I was asked this question: There's a square matrices from order from the real numbers, that satisifes: . Prove that is invertible and that the matrices and have the commutative property. Given that . Prove that isn't invertible and calculate , write the two possibilities. My first attempt (But then I saw that determinant does not preserve the addition, then this attempt is completely wrong): Suppose isn't invertible, meaning . And since we assumed that A isn't invertible: Contradiction, therefore is invertible. My second attempt : Now, we need to show that , which will prove that is invertible and found an inverse for it. But I wasn't sure how to continue from here... And since is invertible then: Which proved that the commutative property exists. My attempt on the second question: (But it also relies on adding determinants, then it must be incorrect too) We'll prove it by contradiction, is inverse, meaning : And from . we know that , and since we assumed that then the determinant of isn't necessarily always , therefore it is a contradiction. I'd love for guidance on how to continue. Thanks for the help!","A,B \in M_3 (\mathbb{R}) 3\times3 BA^2+A^2=I A A B (AB=BA=I) B+A^2B=O B |A| A |A|=0 BA^2+A^2=I \rightarrow A^2=I-BA^2 |A^2|=|I|-B|A|^2 \rightarrow 0=1-B*0 \rightarrow 0=1 A BA^2+A^2=I \rightarrow (B+I)A^2=I A^2(B+I)=(B+I)A^2=I A A BA^2+A^2=AB^2+A^2=I \rightarrow A^2(B+I)=(B+I)A^2=I B |B|\neq 0 B+A^2B=O \rightarrow B=O-A^2B \rightarrow |B|=-|A^2||B| 1 |A|\neq 0 |B| \neq 0 B 0","['linear-algebra', 'matrices']"
59,Finding the eigenvalues of a 9x9 matrix,Finding the eigenvalues of a 9x9 matrix,,"Is there any way that I can find the eigenvalues of this matrix!? I've used SymPy and tried the M.eigenvals() function but it always runs out of time. My main goal is to diagonalise it so if anyone can help that's be great. Thanks! $$ \begin{bmatrix}     -e^{-ik_{1}} & e^{-ik_{1}} & e^{-ik_{1}} & e^{-ik_{1}} & 0 & 2\sqrt{3}ie^{ik_{1}} & 0 & 0 & 0 \\     e^{ik_{1}} & -e^{ik_{1}} & e^{ik_{1}} & e^{ik_{1}} & 0 & 0 & 2\sqrt{3}ie^{-ik_{1}} & 0 & 0 \\     e^{-ik_{2}} & e^{-ik_{2}} & -e^{-ik_{2}} & e^{-ik_{2}} & 0 & 0 & 0 & 2\sqrt{3}ie^{-ik_{2}}  & 0 \\     e^{ik_{2}} & e^{ik_{2}} & e^{ik_{2}} & -e^{ik_{2}} & 0 & 0 & 0 & 0 & 2\sqrt{3}ie^{ik_{2}} \\     0 & 0 & 0 & 0 & 4 & 0 & 0 & 0 & 0\\     2\sqrt{3}ie^{i(k_{2}+k_{1})} & 0 & 0 & 0 & 0 & -e^{-i(k_{2}+k_{1})} & e^{-i(k_{2}+k_{1})} & e^{-i(k_{2}+k_{1})} & e^{-i(k_{2}+k_{1})}\\     0 & 2\sqrt{3}ie^{i(k_{1}-k_{2})} & 0 & 0 & 0 & e^{i(k_{1}-k_{2})} & -e^{i(k_{1}-k_{2})} & e^{i(k_{1}-k_{2})} & e^{i(k_{1}-k_{2})}\\     0 & 0 & 2\sqrt{3}ie^{i(k_{2}-k_{1})} & 0 & 0 & e^{i(k_{2}-k_{1})} & e^{i(k_{2}-k_{1})} & -e^{i(k_{2}-k_{1})} & e^{i(k_{2}+k_{1})}\\     0 & 0 & 0 & 2\sqrt{3}ie^{i(k_{1}+k_{2})} & 0 & e^{i(k_{1}+k_{2})} & e^{i(k_{1}+k_{2})} & e^{i(k_{1}+k_{2})} & -e^{i(k_{1}+k_{2})}\\ \end{bmatrix}$$ Edit: For context, I'm trying to do a quantum walk on a D2Q9 lattice. I have done this sucessfully. Quantum walks use coin opertors and I want to transform my coin into fourier space. The above matrix is the coin transformed into fourier space, however I need the eigenvalues and eigenvectors of this coin to get a useable operator. An approximation may not scale well when using it as a coin operator in a quantum walk which is why I'm trying to find an exact solution and not an approximation.","Is there any way that I can find the eigenvalues of this matrix!? I've used SymPy and tried the M.eigenvals() function but it always runs out of time. My main goal is to diagonalise it so if anyone can help that's be great. Thanks! Edit: For context, I'm trying to do a quantum walk on a D2Q9 lattice. I have done this sucessfully. Quantum walks use coin opertors and I want to transform my coin into fourier space. The above matrix is the coin transformed into fourier space, however I need the eigenvalues and eigenvectors of this coin to get a useable operator. An approximation may not scale well when using it as a coin operator in a quantum walk which is why I'm trying to find an exact solution and not an approximation.","
\begin{bmatrix}
    -e^{-ik_{1}} & e^{-ik_{1}} & e^{-ik_{1}} & e^{-ik_{1}} & 0 & 2\sqrt{3}ie^{ik_{1}} & 0 & 0 & 0 \\
    e^{ik_{1}} & -e^{ik_{1}} & e^{ik_{1}} & e^{ik_{1}} & 0 & 0 & 2\sqrt{3}ie^{-ik_{1}} & 0 & 0 \\
    e^{-ik_{2}} & e^{-ik_{2}} & -e^{-ik_{2}} & e^{-ik_{2}} & 0 & 0 & 0 & 2\sqrt{3}ie^{-ik_{2}}  & 0 \\
    e^{ik_{2}} & e^{ik_{2}} & e^{ik_{2}} & -e^{ik_{2}} & 0 & 0 & 0 & 0 & 2\sqrt{3}ie^{ik_{2}} \\
    0 & 0 & 0 & 0 & 4 & 0 & 0 & 0 & 0\\
    2\sqrt{3}ie^{i(k_{2}+k_{1})} & 0 & 0 & 0 & 0 & -e^{-i(k_{2}+k_{1})} & e^{-i(k_{2}+k_{1})} & e^{-i(k_{2}+k_{1})} & e^{-i(k_{2}+k_{1})}\\
    0 & 2\sqrt{3}ie^{i(k_{1}-k_{2})} & 0 & 0 & 0 & e^{i(k_{1}-k_{2})} & -e^{i(k_{1}-k_{2})} & e^{i(k_{1}-k_{2})} & e^{i(k_{1}-k_{2})}\\
    0 & 0 & 2\sqrt{3}ie^{i(k_{2}-k_{1})} & 0 & 0 & e^{i(k_{2}-k_{1})} & e^{i(k_{2}-k_{1})} & -e^{i(k_{2}-k_{1})} & e^{i(k_{2}+k_{1})}\\
    0 & 0 & 0 & 2\sqrt{3}ie^{i(k_{1}+k_{2})} & 0 & e^{i(k_{1}+k_{2})} & e^{i(k_{1}+k_{2})} & e^{i(k_{1}+k_{2})} & -e^{i(k_{1}+k_{2})}\\
\end{bmatrix}","['matrices', 'eigenvalues-eigenvectors', 'diagonalization']"
60,Show that $G$ is a subgroup of $\operatorname{GL}_3(\mathbb R)$ and $G \cong \mathbb R^2 \rtimes (\mathbb R^\times \times \mathbb R^\times)$,Show that  is a subgroup of  and,G \operatorname{GL}_3(\mathbb R) G \cong \mathbb R^2 \rtimes (\mathbb R^\times \times \mathbb R^\times),"This is exercise $3-5$ from Milne's group theory book. $\DeclareMathOperator{\GL}{GL}$ $\newcommand{\R}{\mathbb R}$ Problem. Let $G$ be all matrices in $\GL_3 (\R)$ of the form $$\begin{pmatrix} a & 0 & b\\ 0 & a & c \\ 0 & 0 & d \\ \end{pmatrix},$$ where $ad\neq 0$ . Check that $G$ is a subgroup of $\GL_3 (\R)$ and that it is a semidirect product of $\R^2$ (additive group) and $\R^\times \times \R^\times$ . Is it a direct product of these two groups? Attempt. First, to check $G$ is a subgroup of $\GL_3 (\R)$ we can take a matrix $A \in G$ and find that its inverse is given by $$A^{-1} = \begin{pmatrix} a^{-1} & 0 & e\\ 0 & a^{-1} & f \\ 0 & 0 & d^{-1}\end{pmatrix},$$ where $e=-a^{-1}bd^{-1}$ and $f= -a^{-1}cd^{-1}$ . Then straightforward matrix multiplication shows if $A,B \in G$ then $AB^{-1}\in G$ , so by the subgroup test $G$ is a subgroup. Now to show $G  \cong \R^2 \rtimes (\R^\times \times \R^\times)$ it suffices to find $N \cong \R^2$ and $Q \cong \R^\times \times \R^\times$ such that \begin{align*}     N &\vartriangleleft G;\\     NQ &= G;\\     N \cap Q &= \{1\}.\\ \end{align*} Let $N$ be all matrices of the form $$\begin{pmatrix} a & 0 & 0 \\ 0 & a & 0 \\ 0 & 0 & d \end{pmatrix}$$ such that $ad \neq 0$ , and let $Q$ be all matrices of the form $$\begin{pmatrix} 1 & 0 & b \\ 0 & 1 & c \\ 0 & 0 & 1\end{pmatrix},$$ such that $b,c \in \R^\times$ . Then we can identify $N$ with $\R^2$ and $Q$ with $\R^\times \times \R^\times$ in the obvious way, and we see that the three conditions for semidirect product are also satisfied. Therefore $G \cong \R^2 \rtimes (\R^\times \times \R^\times)$ as desired. Note that $G$ cannot be a direct product of these two subgroups because, e.g., $Q$ is not normal in $G$ . Is this correct?","This is exercise from Milne's group theory book. Problem. Let be all matrices in of the form where . Check that is a subgroup of and that it is a semidirect product of (additive group) and . Is it a direct product of these two groups? Attempt. First, to check is a subgroup of we can take a matrix and find that its inverse is given by where and . Then straightforward matrix multiplication shows if then , so by the subgroup test is a subgroup. Now to show it suffices to find and such that Let be all matrices of the form such that , and let be all matrices of the form such that . Then we can identify with and with in the obvious way, and we see that the three conditions for semidirect product are also satisfied. Therefore as desired. Note that cannot be a direct product of these two subgroups because, e.g., is not normal in . Is this correct?","3-5 \DeclareMathOperator{\GL}{GL} \newcommand{\R}{\mathbb R} G \GL_3 (\R) \begin{pmatrix} a & 0 & b\\ 0 & a & c \\ 0 & 0 & d \\ \end{pmatrix}, ad\neq 0 G \GL_3 (\R) \R^2 \R^\times \times \R^\times G \GL_3 (\R) A \in G A^{-1} = \begin{pmatrix} a^{-1} & 0 & e\\ 0 & a^{-1} & f \\ 0 & 0 & d^{-1}\end{pmatrix}, e=-a^{-1}bd^{-1} f= -a^{-1}cd^{-1} A,B \in G AB^{-1}\in G G G  \cong \R^2 \rtimes (\R^\times \times \R^\times) N \cong \R^2 Q \cong \R^\times \times \R^\times \begin{align*}
    N &\vartriangleleft G;\\
    NQ &= G;\\
    N \cap Q &= \{1\}.\\
\end{align*} N \begin{pmatrix} a & 0 & 0 \\ 0 & a & 0 \\ 0 & 0 & d \end{pmatrix} ad \neq 0 Q \begin{pmatrix} 1 & 0 & b \\ 0 & 1 & c \\ 0 & 0 & 1\end{pmatrix}, b,c \in \R^\times N \R^2 Q \R^\times \times \R^\times G \cong \R^2 \rtimes (\R^\times \times \R^\times) G Q G","['linear-algebra', 'abstract-algebra', 'matrices', 'group-theory', 'solution-verification']"
61,Compute the matrix representation of a linear map with respect to a basis,Compute the matrix representation of a linear map with respect to a basis,,"I'm trying to solve this exercise but I'm stuck: Let $V$ be a vector space over $K$ and let $\mathcal{B} = \{v_1, v_2, v_3, v_4\}$ be a basis of $V$ . Let $f: V \rightarrow V$ be a linear map such that: $$f(v_1) = v_2 - v_3, \quad f(v_2) = 3v_1 - 2v_4, \quad f(v_3) = v_3, \quad f(v_4) = v_1 + v_4.$$ Calculate the matrix $A$ which represents $f$ in the basis $\mathcal{B}$ . (Extra: is $A$ nilpotent?) I tried multiple things. Intuitively, I thought the answer would look something like this: \begin{bmatrix}\mid&\mid&\mid&\mid\\\ v_2-v_3&3v_1-2v_4&v_3&v_1+v_4\\\ \mid&\mid&\mid&\mid\end{bmatrix} Which of course works for our usual basis vectors $e_1, e_2, e_3, e_4$ , but unfortunately not for other basis vectors. What I tried next was to calculate the matrix by hand in the case of a $2 \times 2$ matrix and then a specific example with some made-up numbers for a $3 \times 3$ matrix but I wasn't able to see any easy patterns. Surely the intention here is not to solve multiple, big systems of equations to figure out all the entries in the matrix one by one, right? What am I missing here? Oh and for the extra, my guess is that $A$ is not nilpotent, since no matter how many times we apply $A$ , we will always get $v_3$ for $v_3$ and never the zero vector. Is this correct? Thanks to a comment I came to this solution: $A = \begin{bmatrix} 0 & 3 & 0 & 1 \\ 1 & 0 & 0 & 0 \\ -1 & 0 & 1 & 0 \\ 0 & -2 & 0 & 1 \end{bmatrix}$ , and I see that this works with the basis $e_1, e_2, e_3, e_4$ . But if I try it with other vectors, obviously it doesn't work. For example, let $v_1 = \begin{pmatrix} 1\\\ 1\\\ 1\\\ 1\end{pmatrix}, \quad v_2 = \begin{pmatrix} 1\\\ 1\\\ 1\\\ 0\end{pmatrix}, \quad v_3 = \begin{pmatrix} 1\\\ 1\\\ 0\\\ 0\end{pmatrix}, \quad v_4 = \begin{pmatrix} 1\\\ 0\\\ 0\\\ 0\end{pmatrix}$ But then $Av_1 = \begin{pmatrix} 4\\\ 1\\\ 0\\\ -1\end{pmatrix} \neq v_2 - v_3$ What am I not understanding here? Shouldn't the solution have to be in terms of the given vectors? Doesn't the solution change if you have different specific vectors?","I'm trying to solve this exercise but I'm stuck: Let be a vector space over and let be a basis of . Let be a linear map such that: Calculate the matrix which represents in the basis . (Extra: is nilpotent?) I tried multiple things. Intuitively, I thought the answer would look something like this: Which of course works for our usual basis vectors , but unfortunately not for other basis vectors. What I tried next was to calculate the matrix by hand in the case of a matrix and then a specific example with some made-up numbers for a matrix but I wasn't able to see any easy patterns. Surely the intention here is not to solve multiple, big systems of equations to figure out all the entries in the matrix one by one, right? What am I missing here? Oh and for the extra, my guess is that is not nilpotent, since no matter how many times we apply , we will always get for and never the zero vector. Is this correct? Thanks to a comment I came to this solution: , and I see that this works with the basis . But if I try it with other vectors, obviously it doesn't work. For example, let But then What am I not understanding here? Shouldn't the solution have to be in terms of the given vectors? Doesn't the solution change if you have different specific vectors?","V K \mathcal{B} = \{v_1, v_2, v_3, v_4\} V f: V \rightarrow V f(v_1) = v_2 - v_3, \quad f(v_2) = 3v_1 - 2v_4, \quad f(v_3) = v_3, \quad f(v_4) = v_1 + v_4. A f \mathcal{B} A \begin{bmatrix}\mid&\mid&\mid&\mid\\\ v_2-v_3&3v_1-2v_4&v_3&v_1+v_4\\\ \mid&\mid&\mid&\mid\end{bmatrix} e_1, e_2, e_3, e_4 2 \times 2 3 \times 3 A A v_3 v_3 A = \begin{bmatrix}
0 & 3 & 0 & 1 \\
1 & 0 & 0 & 0 \\
-1 & 0 & 1 & 0 \\
0 & -2 & 0 & 1
\end{bmatrix} e_1, e_2, e_3, e_4 v_1 = \begin{pmatrix} 1\\\ 1\\\ 1\\\ 1\end{pmatrix}, \quad v_2 = \begin{pmatrix} 1\\\ 1\\\ 1\\\ 0\end{pmatrix}, \quad v_3 = \begin{pmatrix} 1\\\ 1\\\ 0\\\ 0\end{pmatrix}, \quad v_4 = \begin{pmatrix} 1\\\ 0\\\ 0\\\ 0\end{pmatrix} Av_1 = \begin{pmatrix} 4\\\ 1\\\ 0\\\ -1\end{pmatrix} \neq v_2 - v_3","['linear-algebra', 'matrices']"
62,What is the largest eigenvalue of the special matrix?,What is the largest eigenvalue of the special matrix?,,"The question is a simplified question to my last question. Let $I_m$ denote the $m\times m$ identity matrix, and $\alpha\in\mathbb{R}^{n-1}$ . Let $$ \mathbf{B}=\left[\begin{array}{ccccc} -\alpha & 0 & &  &\\ \alpha & -\alpha & 0 & \\ 0 & \alpha & \ddots & \ddots \\ & & & & &-\alpha\\ \end{array}\right]\in \mathbb{R}^{(mn-m)\times (m-1)} $$ I want to calculate the largest eigenvalue of $$ \left[\begin{array}{ccccc} I_{mn-m} & B \\  B^{\top} & I_{m-1} \end{array}\right]. $$ The answer should be $\sim3- O(\frac{1}{n m^2})$ , but I want to know the exact value.","The question is a simplified question to my last question. Let denote the identity matrix, and . Let I want to calculate the largest eigenvalue of The answer should be , but I want to know the exact value.","I_m m\times m \alpha\in\mathbb{R}^{n-1} 
\mathbf{B}=\left[\begin{array}{ccccc}
-\alpha & 0 & &  &\\
\alpha & -\alpha & 0 & \\
0 & \alpha & \ddots & \ddots \\
& & & & &-\alpha\\
\end{array}\right]\in \mathbb{R}^{(mn-m)\times (m-1)}
 
\left[\begin{array}{ccccc}
I_{mn-m} & B \\ 
B^{\top} & I_{m-1}
\end{array}\right].
 \sim3- O(\frac{1}{n m^2})","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'matrix-equations']"
63,Find the symmetric matrix given its eigenvalues and eigenvector.,Find the symmetric matrix given its eigenvalues and eigenvector.,,"$A$ is a $3 \times 3$ symmetric matrix. It has the eigenvalue $\lambda_1 = 3$ with the eigenvector $$ \begin{bmatrix}  1 \\ 0 \\ -1  \end{bmatrix} $$ and a double eigenvalue $\lambda_2 = -1$ . Find the matrix $A$ . Since $A$ is symmetric, it can be diagonalized orthogonally, with an orthogonal matrix $P$ as $A = PDP^T$ . The matrix is made of the eigenvectors that are orthogonal to each other. What I then did was with the help of inner product, I got a vector that is orthogonal to $v_1$ and with the help of cross product I got another orthogonal vector. Those vectors are $$ \begin{bmatrix}  1 \\ 1 \\ 1  \end{bmatrix} \quad\text{and}\quad  \begin{bmatrix}  1 \\ -2 \\ 1  \end{bmatrix} $$ so $$ A = PDP^{-1}  = \begin{bmatrix}   1 &  1 & -5 \\   1 & -5 &  1 \\  -5 &  1 & -5  \end{bmatrix}.  $$ But my answer is wrong obviously since $Av \neq \lambda v$ . The answer is $$ \begin{bmatrix}   1 &  0 & -2 \\   0 & -1 &  0 \\  -2 &  0 & 1  \end{bmatrix} $$ where the vectors are $v_2 = \begin{bmatrix} 1 & 0 & 1 \end{bmatrix}^T$ and $\begin{bmatrix} 0 & 1 & 0 \end{bmatrix}^T$ . The answer sheet says that these vectors are orthogonal to $v_1$ . Also they didn't do $A = PDP^T$ instead they did $A = PDP^{-1}$ . Can anyone help me understand?","is a symmetric matrix. It has the eigenvalue with the eigenvector and a double eigenvalue . Find the matrix . Since is symmetric, it can be diagonalized orthogonally, with an orthogonal matrix as . The matrix is made of the eigenvectors that are orthogonal to each other. What I then did was with the help of inner product, I got a vector that is orthogonal to and with the help of cross product I got another orthogonal vector. Those vectors are so But my answer is wrong obviously since . The answer is where the vectors are and . The answer sheet says that these vectors are orthogonal to . Also they didn't do instead they did . Can anyone help me understand?","A 3 \times 3 \lambda_1 = 3 
\begin{bmatrix} 
1 \\ 0 \\ -1 
\end{bmatrix}
 \lambda_2 = -1 A A P A = PDP^T v_1 
\begin{bmatrix} 
1 \\ 1 \\ 1 
\end{bmatrix}
\quad\text{and}\quad 
\begin{bmatrix} 
1 \\ -2 \\ 1 
\end{bmatrix}
 
A = PDP^{-1} 
= \begin{bmatrix} 
 1 &  1 & -5 \\ 
 1 & -5 &  1 \\ 
-5 &  1 & -5 
\end{bmatrix}. 
 Av \neq \lambda v 
\begin{bmatrix} 
 1 &  0 & -2 \\ 
 0 & -1 &  0 \\ 
-2 &  0 & 1 
\end{bmatrix}
 v_2 = \begin{bmatrix} 1 & 0 & 1 \end{bmatrix}^T \begin{bmatrix} 0 & 1 & 0 \end{bmatrix}^T v_1 A = PDP^T A = PDP^{-1}","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'orthogonality', 'symmetric-matrices']"
64,Trace of log of matrix,Trace of log of matrix,,"How to prove that $$\operatorname{Tr}(\log B) = \sum_i \log b_i \tag{1}$$ where the $b_i$ are the eigenvalues of matrix $B$ ? Question background: This question arises from quantum field theory in particle physics. Where $B$ is $i\not\partial - m$ in continuous Hilbert space(this space of four dimensional spacetime). And $\not\partial=\partial_{\mu}\gamma^{\mu}$ , $\gamma^{\mu}$ is four dimensional Dirac Gamma matrix , there also a four dimensional identity matrix with $m$ . You can specify this as each point in Hilbert space, there is a four dimensional matrix， or each element of B is a four dimensional matrix in Hilbert space; About logarithm of matrix definition, I use Peskin and Schroeder’s QFT book, they define the logarithm of matrix as Taylor series expansion. Below is my navie attempt (sorry for my insufficient math knowledge), suppose $B$ can be diagonalized as: $$B = U^{-1}\Lambda U \tag{2}$$ where $U$ is some unitary matrix and $\Lambda$ is diagonal matrix. Then $$\operatorname{Tr} (\log B) = \operatorname{Tr} (\log (U^{-1}\Lambda U)) \tag{3}$$ I expect that $$ \operatorname{Tr} (\log (U^{-1}\Lambda U)) = \operatorname{Tr} (\log (\Lambda)) \tag{4}$$ But how to proceed from ( $3$ )?","How to prove that where the are the eigenvalues of matrix ? Question background: This question arises from quantum field theory in particle physics. Where is in continuous Hilbert space(this space of four dimensional spacetime). And , is four dimensional Dirac Gamma matrix , there also a four dimensional identity matrix with . You can specify this as each point in Hilbert space, there is a four dimensional matrix， or each element of B is a four dimensional matrix in Hilbert space; About logarithm of matrix definition, I use Peskin and Schroeder’s QFT book, they define the logarithm of matrix as Taylor series expansion. Below is my navie attempt (sorry for my insufficient math knowledge), suppose can be diagonalized as: where is some unitary matrix and is diagonal matrix. Then I expect that But how to proceed from ( )?",\operatorname{Tr}(\log B) = \sum_i \log b_i \tag{1} b_i B B i\not\partial - m \not\partial=\partial_{\mu}\gamma^{\mu} \gamma^{\mu} m B B = U^{-1}\Lambda U \tag{2} U \Lambda \operatorname{Tr} (\log B) = \operatorname{Tr} (\log (U^{-1}\Lambda U)) \tag{3}  \operatorname{Tr} (\log (U^{-1}\Lambda U)) = \operatorname{Tr} (\log (\Lambda)) \tag{4} 3,"['linear-algebra', 'matrices']"
65,matrix derivation,matrix derivation,,"Given $$ \mathbf{Z}=\mathbf{A}\mathbf{H}\mathbf{W} $$ where $\mathbf{Z}\in\mathbb{R}^{V\times d_n}$ , $\mathbf{A}\in\mathbb{R}^{V\times V}$ , $\mathbf{H}\in\mathbb{R}^{V\times d_{n-1}}$ , and $\mathbf{W}\in\mathbb{R}^{d_{n-1}\times d_n}$ , what is the matrix representation of $$\frac{\partial \mathbf{Z}}{\partial \mathbf{H}}$$ ? My understanding is that it should be $\mathbf{A}\mathbf{W}$ , but their dimensions don't match.","Given where , , , and , what is the matrix representation of ? My understanding is that it should be , but their dimensions don't match.","
\mathbf{Z}=\mathbf{A}\mathbf{H}\mathbf{W}
 \mathbf{Z}\in\mathbb{R}^{V\times d_n} \mathbf{A}\in\mathbb{R}^{V\times V} \mathbf{H}\in\mathbb{R}^{V\times d_{n-1}} \mathbf{W}\in\mathbb{R}^{d_{n-1}\times d_n} \frac{\partial \mathbf{Z}}{\partial \mathbf{H}} \mathbf{A}\mathbf{W}","['matrices', 'matrix-calculus']"
66,Invertibility of a Rank-One Update of a Symmetric Matrix,Invertibility of a Rank-One Update of a Symmetric Matrix,,"I want to find a necessary and sufficient condition for $B = A + x \, y^\top$ to be invertible, where $A$ is assumed to be symmetric and $x, \, y \, \in \mathbb{R}^n-\{0\}$ . I know that the following holds Theorem . Let $E = I - \alpha \, u \, v^\top$ be a rank-one modification of the identity $I$ , where $\alpha \in \mathbb{R} - \{0\}$ and $u, \, v \, \in \mathbb{R}^n-\{0\}$ . Then, $E$ is invertible if and only if $\alpha \, u^\top v -1 \ne 0$ . Furthermore, $E^{-1} = I - \beta \, u \, v^\top$ with $\beta = \alpha/(\alpha \, u^\top v - 1) $ . Now, I want to find a necessary and sufficient condition for invertiblity of $B$ by using this theorem. I suspect that it should be the positive definiteness of $A$ , but I am having a little bit of difficulty to prove this.","I want to find a necessary and sufficient condition for to be invertible, where is assumed to be symmetric and . I know that the following holds Theorem . Let be a rank-one modification of the identity , where and . Then, is invertible if and only if . Furthermore, with . Now, I want to find a necessary and sufficient condition for invertiblity of by using this theorem. I suspect that it should be the positive definiteness of , but I am having a little bit of difficulty to prove this.","B = A + x \, y^\top A x, \, y \, \in \mathbb{R}^n-\{0\} E = I - \alpha \, u \, v^\top I \alpha \in \mathbb{R} - \{0\} u, \, v \, \in \mathbb{R}^n-\{0\} E \alpha \, u^\top v -1 \ne 0 E^{-1} = I - \beta \, u \, v^\top \beta = \alpha/(\alpha \, u^\top v - 1)  B A","['linear-algebra', 'matrices', 'inverse', 'matrix-rank']"
67,Commutative matrix multiplication,Commutative matrix multiplication,,Given two 3x3 matrix: $$ V= \begin{bmatrix} 1 & 0 & 9 \cr 6 & 4 & -18 \cr -3 & 0 & 13 \cr \end{bmatrix}\quad W= \begin{bmatrix} 13 & 9 & 3 \cr -14 & -8 & 2 \cr 5 & 3 & -1 \cr \end{bmatrix}  $$ Is there any way to predict that $ V * W = W * V $ without actually calculating both multiplications,Given two 3x3 matrix: Is there any way to predict that without actually calculating both multiplications,"
V=
\begin{bmatrix}
1 & 0 & 9 \cr
6 & 4 & -18 \cr
-3 & 0 & 13 \cr
\end{bmatrix}\quad
W=
\begin{bmatrix}
13 & 9 & 3 \cr
-14 & -8 & 2 \cr
5 & 3 & -1 \cr
\end{bmatrix} 
  V * W = W * V ",['matrices']
68,High first singular value for random matrix,High first singular value for random matrix,,I have a 100 by 100 matrix of random entries. The SVD of the matrix shows that the first singular value is quite high and the rest are substantially lower. I know that this first singular value is the best rank one approximation of the matrix but why is it so much higher than the other values?,I have a 100 by 100 matrix of random entries. The SVD of the matrix shows that the first singular value is quite high and the rest are substantially lower. I know that this first singular value is the best rank one approximation of the matrix but why is it so much higher than the other values?,,"['linear-algebra', 'matrices', 'svd']"
69,Loewner order and rank-$1$ matrices,Loewner order and rank- matrices,1,"The Loewner order is defined over the set of Hermitian matrices as $A \leq B$ if and only if $B - A$ is positive semidefinite. If $B$ is a rank- $1$ , positive semidefinite matrix, what are the matrices $A$ such that $0 \leq A \leq B$ ? Are there other solutions than $A = \lambda B$ for $0 \leq \lambda \leq 1$ ?","The Loewner order is defined over the set of Hermitian matrices as if and only if is positive semidefinite. If is a rank- , positive semidefinite matrix, what are the matrices such that ? Are there other solutions than for ?",A \leq B B - A B 1 A 0 \leq A \leq B A = \lambda B 0 \leq \lambda \leq 1,"['linear-algebra', 'matrices', 'symmetric-matrices', 'positive-semidefinite', 'rank-1-matrices']"
70,Am I right? Building matrix from polynomial and derivative,Am I right? Building matrix from polynomial and derivative,,"I'm just really unsure whether I'm correct about building a system of equations since there are derivatives. What is known: $$f(x) = a_1x^3 + a_2x^2 + a_3x + a_4$$ $$f(−1) = 5, f(1) = 3, f'(−1) =−7, f'(1) = 13$$ So we have: $f(-1) = a_1(-1)^3 + a_2(-1)^2 + a_3(-1) + a_4 = -a_1 + a_2 -a_3 + a_4 = 5$ $f(1) = a_1\times1^3 + a_2\times1^2 + a_3\times1 + a_4 = a_1 + a_2 + a_3 + a_4 = 3$ The derivative form of this function is the following (I guess): $f'(x) = 3a_1x^2 + 2a_2x + a_3 $ As a result, $f'(-1) = 3a_1(-1)^2 + 2a_2(-1) + a_3 = 3a_1 -2a_2 + a_3 =-7$ $f'(1) = 3a*1^2 + 2b*1 + c = 3a_1 + 2a_2 + a_3 = 13$ So we have system of equations: $-x_1 + x_2 - x_3 + x_4 = 5$ $x_1 + x_2 + x_3 + x_4 = 3$ $3x_1 - 2x_2 +x_3 + 0 \times x_4 = -7$ $3x_1 + 2x_2 +x_3 + 0 \times x_4 = 13$ And from this system I can build matrix. Is everything correct?","I'm just really unsure whether I'm correct about building a system of equations since there are derivatives. What is known: So we have: The derivative form of this function is the following (I guess): As a result, So we have system of equations: And from this system I can build matrix. Is everything correct?","f(x) = a_1x^3 + a_2x^2 + a_3x + a_4 f(−1) = 5, f(1) = 3, f'(−1) =−7, f'(1) = 13 f(-1) = a_1(-1)^3 + a_2(-1)^2 + a_3(-1) + a_4 = -a_1 + a_2 -a_3 + a_4 = 5 f(1) = a_1\times1^3 + a_2\times1^2 + a_3\times1 + a_4 = a_1 + a_2 + a_3 + a_4 = 3 f'(x) = 3a_1x^2 + 2a_2x + a_3  f'(-1) = 3a_1(-1)^2 + 2a_2(-1) + a_3 = 3a_1 -2a_2 + a_3 =-7 f'(1) = 3a*1^2 + 2b*1 + c = 3a_1 + 2a_2 + a_3 = 13 -x_1 + x_2 - x_3 + x_4 = 5 x_1 + x_2 + x_3 + x_4 = 3 3x_1 - 2x_2 +x_3 + 0 \times x_4 = -7 3x_1 + 2x_2 +x_3 + 0 \times x_4 = 13","['linear-algebra', 'matrices', 'derivatives', 'polynomials']"
71,Is it that easy to be Zariski dense?,Is it that easy to be Zariski dense?,,"I've just heard about diagonizable matrices being Zariski dense as a consequence of Cayley-Hamilton, and this is the proof I came up with (in $\mathbb{C}$ ): Let $O_{\cal F} = \{M\in{\cal M}_n(\mathbb C);\ \exists f \in {\cal F}, f(M)\neq 0\}$ be a non-empty open set, and $f\in {\cal F}$ . Let $\lambda$ be such that $P_\lambda = X - \lambda$ does not divide $f$ . The matrix $\lambda I_n$ has $P_\lambda$ as its minimal polynomial, which does not divide $f$ , so $f(\lambda I_n) \neq 0$ and it belongs to $O_{\cal F}$ . We showed that there are diagonizable matrices in every open set, Q.E.D. This proof seems strange to me, we proved it quite easily, without directly involving Cayley-Hamilton, and in fact proved something much stronger that is to say that $\mathbb C I_n$ , a line, is Zariski dense. Moreover every other proof I've found in 5 minutes of googling looked much more complicated and involved more advanced concepts. Basically my proofs generalizes to any set that contains matrices with minimal polynomials with arbitrary roots. Did I understand Zariski's topology wrong, made a mistake in my proof, or is it really that easy to be Zariski dense ?","I've just heard about diagonizable matrices being Zariski dense as a consequence of Cayley-Hamilton, and this is the proof I came up with (in ): Let be a non-empty open set, and . Let be such that does not divide . The matrix has as its minimal polynomial, which does not divide , so and it belongs to . We showed that there are diagonizable matrices in every open set, Q.E.D. This proof seems strange to me, we proved it quite easily, without directly involving Cayley-Hamilton, and in fact proved something much stronger that is to say that , a line, is Zariski dense. Moreover every other proof I've found in 5 minutes of googling looked much more complicated and involved more advanced concepts. Basically my proofs generalizes to any set that contains matrices with minimal polynomials with arbitrary roots. Did I understand Zariski's topology wrong, made a mistake in my proof, or is it really that easy to be Zariski dense ?","\mathbb{C} O_{\cal F} = \{M\in{\cal M}_n(\mathbb C);\ \exists f \in {\cal F}, f(M)\neq 0\} f\in {\cal F} \lambda P_\lambda = X - \lambda f \lambda I_n P_\lambda f f(\lambda I_n) \neq 0 O_{\cal F} \mathbb C I_n","['linear-algebra', 'general-topology', 'matrices']"
72,How to interpret symmetrical $2\times2$ matrix geometrically as a composition of simple transformation?,How to interpret symmetrical  matrix geometrically as a composition of simple transformation?,2\times2,"Let's say I have this matrix: $$A = \left[ \begin{matrix}   1&3\\   3&1 \end{matrix} \right]$$ I only know simple transformation like reflection, rotation, scaling, and shear. Can I interpret matrix $A$ as a composition of these simple transformation? How? I have tried to imagine the transformation of matrix $A$ and intuitively, it looks some kind of reflection or rotation with some scaling, but I got stuck because it has -2 and 4 scaling, which makes me confused.","Let's say I have this matrix: I only know simple transformation like reflection, rotation, scaling, and shear. Can I interpret matrix as a composition of these simple transformation? How? I have tried to imagine the transformation of matrix and intuitively, it looks some kind of reflection or rotation with some scaling, but I got stuck because it has -2 and 4 scaling, which makes me confused.","A = \left[
\begin{matrix}
  1&3\\
  3&1
\end{matrix}
\right] A A","['matrices', 'geometry', 'linear-transformations', 'affine-geometry']"
73,Diagonalization of a symmetric bilinear form over the integers,Diagonalization of a symmetric bilinear form over the integers,,"Suppose $H$ is an $n\times n$ symmetric matrix with integer entries. We can view $H$ as a symmetric bilinear form $H:\Bbb R^n\times \Bbb R^n\to \Bbb R$ defined by $H(v,w)=v^tAw$ . It is then well known that $H$ is diagonalizable ( http://www.math.toronto.edu/~jkamnitz/courses/mat247_2014/bilinearforms2.pdf , Theorem 1.4, for example), i.e. there is an invertible matrix $P$ such that $P^t HP$ is diagonal. On the other hand, we can view $H$ as a symmetric bilinear form $\Bbb Z^n\times \Bbb Z^n\to \Bbb Z$ . Suppose $\det H=\pm 1$ and that $H$ is negative definite. Then can we find an invertible matrix $P\in GL(n,\Bbb Z)$ such that $P^t HP$ is diagonal?","Suppose is an symmetric matrix with integer entries. We can view as a symmetric bilinear form defined by . It is then well known that is diagonalizable ( http://www.math.toronto.edu/~jkamnitz/courses/mat247_2014/bilinearforms2.pdf , Theorem 1.4, for example), i.e. there is an invertible matrix such that is diagonal. On the other hand, we can view as a symmetric bilinear form . Suppose and that is negative definite. Then can we find an invertible matrix such that is diagonal?","H n\times n H H:\Bbb R^n\times \Bbb R^n\to \Bbb R H(v,w)=v^tAw H P P^t HP H \Bbb Z^n\times \Bbb Z^n\to \Bbb Z \det H=\pm 1 H P\in GL(n,\Bbb Z) P^t HP","['linear-algebra', 'matrices', 'symmetric-matrices', 'positive-definite', 'bilinear-form']"
74,Show absolute value of determinant of a prime matrix is a perfect square.,Show absolute value of determinant of a prime matrix is a perfect square.,,"Assume we have a matrix $A$ of the following form $$ 1\leq i,j \leq n \hspace{0.5cm} A = [a_{ij}] \in M_n(\{0,1\}) \hspace{0.4cm}a_{ij} = \begin{cases} 1 \hspace{0.3cm}i+j \in  \mathbb{P} \\ 0 \hspace{0.3cm} i + j \notin  \mathbb{P} \end{cases} $$ where $\mathbb{P}$ denotes the set of prime numbers. How can I prove the absolute value of its determinant is a perfect square? So far, I've managed to figure out it's a symmetric matrix and has zeros on its diagonal except at $a_{11}$ , which isn't much, and I haven't even made use of the prime property.","Assume we have a matrix of the following form where denotes the set of prime numbers. How can I prove the absolute value of its determinant is a perfect square? So far, I've managed to figure out it's a symmetric matrix and has zeros on its diagonal except at , which isn't much, and I haven't even made use of the prime property.","A  1\leq i,j \leq n \hspace{0.5cm} A = [a_{ij}] \in M_n(\{0,1\}) \hspace{0.4cm}a_{ij} = \begin{cases} 1 \hspace{0.3cm}i+j \in  \mathbb{P} \\ 0 \hspace{0.3cm} i + j \notin  \mathbb{P} \end{cases}  \mathbb{P} a_{11}","['linear-algebra', 'matrices', 'prime-numbers', 'determinant', 'square-numbers']"
75,Does SVD(SINGULAR VALUE DECOMPOSITION) tell if a matrix is singular or not?,Does SVD(SINGULAR VALUE DECOMPOSITION) tell if a matrix is singular or not?,,"We know that the singular value decomposition of a matrix A is the factorization of A into the product of three matrices A = UDV T where the columns of U and V are orthonormal and the matrix D is diagonal with positive real entries. Now, if we use SVD, does it tell if a matrix is singular or not? If this is not the case, then how should I know if a matrix is singular or not? Example: Find all values of 𝑎 for which the matrix is singular: matrix: (just a verification of terms question) Should I use gaussian for this?","We know that the singular value decomposition of a matrix A is the factorization of A into the product of three matrices A = UDV T where the columns of U and V are orthonormal and the matrix D is diagonal with positive real entries. Now, if we use SVD, does it tell if a matrix is singular or not? If this is not the case, then how should I know if a matrix is singular or not? Example: Find all values of 𝑎 for which the matrix is singular: matrix: (just a verification of terms question) Should I use gaussian for this?",,"['linear-algebra', 'matrices', 'soft-question', 'matrix-equations', 'matrix-decomposition']"
76,Derivative of dot-product involving a matrix function,Derivative of dot-product involving a matrix function,,"I am struggling with the following derivative $$\frac{\partial }{\partial \mathbf{x}}(\mathbf{b}\cdot(\mathbf{A}(\mathbf{x})\,\mathbf{c}))$$ with $\mathbf{b}\in \mathbb{R}^n$ , $\mathbf{c}\in \mathbb{R}^m$ constants, and $\mathbf{A}\in \mathbb{R}^{n\times m}$ function of $\mathbf{x}\in \mathbb{R}^l$ . I know that $$\frac{\partial \mathbf{A}(\mathbf{x})}{\partial \mathbf{x}}=\mathbf{M}$$ with $\mathbf{M}$ a $3$ D tensor, such that $$M_{n,m,l}=\frac{\partial A_{n,m}}{\partial x_l}$$ but I am not able to find the results of the original problem starting from this result. I am primarly having trouble in retrieving the correct dimensions. Can you give me a hint? Thanks","I am struggling with the following derivative with , constants, and function of . I know that with a D tensor, such that but I am not able to find the results of the original problem starting from this result. I am primarly having trouble in retrieving the correct dimensions. Can you give me a hint? Thanks","\frac{\partial }{\partial \mathbf{x}}(\mathbf{b}\cdot(\mathbf{A}(\mathbf{x})\,\mathbf{c})) \mathbf{b}\in \mathbb{R}^n \mathbf{c}\in \mathbb{R}^m \mathbf{A}\in \mathbb{R}^{n\times m} \mathbf{x}\in \mathbb{R}^l \frac{\partial \mathbf{A}(\mathbf{x})}{\partial \mathbf{x}}=\mathbf{M} \mathbf{M} 3 M_{n,m,l}=\frac{\partial A_{n,m}}{\partial x_l}","['linear-algebra', 'matrices', 'derivatives', 'partial-derivative', 'matrix-calculus']"
77,$\begin{bmatrix}\mathbf O & U\\ V^T & 0\end{bmatrix}$ is diagonalizable?,is diagonalizable?,\begin{bmatrix}\mathbf O & U\\ V^T & 0\end{bmatrix},"I was trying to solve this problem: find all vectors $U, V\in \mathbb R^{n}$ such that $$\begin{bmatrix}\mathbf{O} & U \\ V^T & 0\end{bmatrix}$$ is a diagonalisable matrix. I was able to see that the above matrix is of rank at most $2$ so I tried to find the remainning two eigenvalues of the matrix, but I did not find them in terms of $U$ and $V$ . Is there any way to do it?","I was trying to solve this problem: find all vectors such that is a diagonalisable matrix. I was able to see that the above matrix is of rank at most so I tried to find the remainning two eigenvalues of the matrix, but I did not find them in terms of and . Is there any way to do it?","U, V\in \mathbb R^{n} \begin{bmatrix}\mathbf{O} & U \\
V^T & 0\end{bmatrix} 2 U V","['matrices', 'diagonalization', 'matrix-rank']"
78,Dimensions in group theory,Dimensions in group theory,,"Remark . My previous post about it has been linked to a post that apparently solves my problem -- yet, said post does not satisfy me. I haven't learned the techniques used in my class. As a physics student, I am currently learning about group theory. I have trouble understanding how to find the dimension of a (sub)group. Let us consider the orthogonal group $O(n)$ . Before talking about its dimension, one needs to show that $O(n)$ is indeed a group. We assume throughout that $O(n)\subset GL(n,\mathbb{R})$ . A matrix is said to be orthogonal whenever it preserves the Euclidian norm - that is if \begin{equation} x^Ty = \left(Mx\right)^T\left(My\right) = x^TM^TMx \end{equation} This equation holds true if and only if $M^TM = Id$ . Closure . Let $A,B\in O(n)$ . Let $C = AB$ . \begin{equation} C^TC = (AB)^T(AB) = B^TA^TAB = B^TB = Id. \end{equation} Identity . One easily sees that $Id^T = Id$ , such that $Id^T Id = Id$ . Inverse . Because $O(n)\subset GL(n,\mathbb{R})$ , every $F\in O(n)\subset GL(n,\mathbb{R})$ , is non-singular. Therefore $F^{-1}$ exists. Let us show that $F^{-1}\in O(n)$ . \begin{equation} (F^{-1})^TF^{-1} = (F^T)^{-1}F^{-1} = (F^TF)^{-1} = Id. \end{equation} Associativity . Matrix multiplication being associative, any two matrices that happen to be a members of $O(n)$ will still be associative. We now know for sure that $O(n)$ is a group. In fact, we even know that it is a subgroup of $GL(n,\mathbb{R})$ . Now, how does one work out the dimension of that group? Here is how I tried to solve that problem. Let $K\in O(n)$ . For $n=2$ , I find 3 conditions on the matrix elements of $K$ . For $n=3$ , I find 6 such conditions. We know that for a general matrix in $GL(n,\mathbb{R})$ , the dimension is $n^2$ . Hence, the dimension of $O(2)$ should be 2^2-3 = 1. Equivalently, for $O(3)$ one finds that the dimensions should be $9-6=3$ . How does one find out from this that the dimension of $O(n)$ follows $\frac{n(n-1)}{2}$ ?","Remark . My previous post about it has been linked to a post that apparently solves my problem -- yet, said post does not satisfy me. I haven't learned the techniques used in my class. As a physics student, I am currently learning about group theory. I have trouble understanding how to find the dimension of a (sub)group. Let us consider the orthogonal group . Before talking about its dimension, one needs to show that is indeed a group. We assume throughout that . A matrix is said to be orthogonal whenever it preserves the Euclidian norm - that is if This equation holds true if and only if . Closure . Let . Let . Identity . One easily sees that , such that . Inverse . Because , every , is non-singular. Therefore exists. Let us show that . Associativity . Matrix multiplication being associative, any two matrices that happen to be a members of will still be associative. We now know for sure that is a group. In fact, we even know that it is a subgroup of . Now, how does one work out the dimension of that group? Here is how I tried to solve that problem. Let . For , I find 3 conditions on the matrix elements of . For , I find 6 such conditions. We know that for a general matrix in , the dimension is . Hence, the dimension of should be 2^2-3 = 1. Equivalently, for one finds that the dimensions should be . How does one find out from this that the dimension of follows ?","O(n) O(n) O(n)\subset GL(n,\mathbb{R}) \begin{equation}
x^Ty = \left(Mx\right)^T\left(My\right) = x^TM^TMx
\end{equation} M^TM = Id A,B\in O(n) C = AB \begin{equation}
C^TC = (AB)^T(AB) = B^TA^TAB = B^TB = Id.
\end{equation} Id^T = Id Id^T Id = Id O(n)\subset GL(n,\mathbb{R}) F\in O(n)\subset GL(n,\mathbb{R}) F^{-1} F^{-1}\in O(n) \begin{equation}
(F^{-1})^TF^{-1} = (F^T)^{-1}F^{-1} = (F^TF)^{-1} = Id.
\end{equation} O(n) O(n) GL(n,\mathbb{R}) K\in O(n) n=2 K n=3 GL(n,\mathbb{R}) n^2 O(2) O(3) 9-6=3 O(n) \frac{n(n-1)}{2}","['abstract-algebra', 'matrices', 'group-theory']"
79,Prove that $\operatorname{adj} (kA) = k ^ {n - 1} \operatorname{adj} A$,Prove that,\operatorname{adj} (kA) = k ^ {n - 1} \operatorname{adj} A,"Prove that $\operatorname{adj} (kA) = k ^ {n - 1} \operatorname{adj} A$ where $A$ is $n \times n$ matrix. Hello, I know the proof for this when $\det A \neq 0$ : For $k = 0$ , this holds. Now, for $k \neq 0$ $$(kA) \operatorname{adj} (kA) = k ^ n \det (A) I_n$$ $$A \operatorname{adj} (kA) = k ^ {n - 1}$$ Pre-multiply both sides by $\operatorname{adj} A$ (this is possible) $$\underbrace{\operatorname{adj} (A) A}\operatorname{adj}(kA) = \det (A)k^{n - 1}\operatorname{adj} A$$ $$\det A \cdot I_n \operatorname{adj}(kA) = \det (A)k^{n - 1} \operatorname{adj} A$$ Cancel $\det A$ : $$\operatorname{adj} (kA) = k^{n - 1} \operatorname{adj} A$$ But, what if $\det A = 0$ ? Thanks","Prove that where is matrix. Hello, I know the proof for this when : For , this holds. Now, for Pre-multiply both sides by (this is possible) Cancel : But, what if ? Thanks",\operatorname{adj} (kA) = k ^ {n - 1} \operatorname{adj} A A n \times n \det A \neq 0 k = 0 k \neq 0 (kA) \operatorname{adj} (kA) = k ^ n \det (A) I_n A \operatorname{adj} (kA) = k ^ {n - 1} \operatorname{adj} A \underbrace{\operatorname{adj} (A) A}\operatorname{adj}(kA) = \det (A)k^{n - 1}\operatorname{adj} A \det A \cdot I_n \operatorname{adj}(kA) = \det (A)k^{n - 1} \operatorname{adj} A \det A \operatorname{adj} (kA) = k^{n - 1} \operatorname{adj} A \det A = 0,"['linear-algebra', 'matrices']"
80,How to represent $\mathbb{R}^n$ as a matrix?,How to represent  as a matrix?,\mathbb{R}^n,"Is there a basis in which Hadamard product in $\mathbb R^n$ is isomorphic to the matrix product, and on the matrix main diagonal all elements be equal to the mean value of the elements of the vector in $\mathbb R^n$ ? For instance, in $2$ -dimensional case, we have $$ (a,b) \to \begin{pmatrix} \dfrac{a+b}{2} & \dfrac{b-a}{2} \\ \dfrac{b-a}{2} & \dfrac{a+b}{2} \end{pmatrix} $$ What would be in the $n$ -dimensional case?","Is there a basis in which Hadamard product in is isomorphic to the matrix product, and on the matrix main diagonal all elements be equal to the mean value of the elements of the vector in ? For instance, in -dimensional case, we have What would be in the -dimensional case?","\mathbb R^n \mathbb R^n 2  (a,b) \to \begin{pmatrix} \dfrac{a+b}{2} & \dfrac{b-a}{2} \\ \dfrac{b-a}{2} & \dfrac{a+b}{2} \end{pmatrix}  n","['linear-algebra', 'matrices']"
81,Are matrices more easier to work with than linear maps? [closed],Are matrices more easier to work with than linear maps? [closed],,"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 1 year ago . Improve this question I never liked doing computations with matrices. This was before I learnt that matrices are used to represent linear maps. Also that the definition of matrix multiplication stems from trying to find the matrix of the composition of two linear maps. Another property of a matrix of some linear map is that a matrix multiplied by a vector is the same as evaluating the linear map for that specific vector. What I don't understand is that why is the use of matrices in applications so emphasised. I could be wrong, but wouldn't it be easier to evaluate a function than to multiply a matrix with a vector. The only useful way I see matrices making computations easier is when one wants to find the inverse of a linear map that is hard to find just by the expression of the map. Even while learning about matrices for the first time, a lot of problems phrased in terms of matrices looked like they would be phrased better in terms of functions or just equations. My question is that, how is it more useful to use matrices than it is to use linear maps?","Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 1 year ago . Improve this question I never liked doing computations with matrices. This was before I learnt that matrices are used to represent linear maps. Also that the definition of matrix multiplication stems from trying to find the matrix of the composition of two linear maps. Another property of a matrix of some linear map is that a matrix multiplied by a vector is the same as evaluating the linear map for that specific vector. What I don't understand is that why is the use of matrices in applications so emphasised. I could be wrong, but wouldn't it be easier to evaluate a function than to multiply a matrix with a vector. The only useful way I see matrices making computations easier is when one wants to find the inverse of a linear map that is hard to find just by the expression of the map. Even while learning about matrices for the first time, a lot of problems phrased in terms of matrices looked like they would be phrased better in terms of functions or just equations. My question is that, how is it more useful to use matrices than it is to use linear maps?",,"['linear-algebra', 'matrices', 'soft-question']"
82,Does $v^Tw>0$ imply that $\exists A>0$ such that $v=Aw$?,Does  imply that  such that ?,v^Tw>0 \exists A>0 v=Aw,"Let $v,w \in \mathbb{R}^n$ satisfy $v^Tw>0$ . Does there exist a symmetric positive-definite matrix $g$ such that $v=gw$ ? The condition $v^Tw>0$ is necessary for the existence of such $g$ : $$ v^Tw=w^Tg^Tw=w^Tgw>0. $$ I guess there should be an easy argument for the existence of such $g$ , but I don't see it immediatley. Motivation: I am trying to understand whenever two vectors can be gradients w.r.t different metrics.","Let satisfy . Does there exist a symmetric positive-definite matrix such that ? The condition is necessary for the existence of such : I guess there should be an easy argument for the existence of such , but I don't see it immediatley. Motivation: I am trying to understand whenever two vectors can be gradients w.r.t different metrics.","v,w \in \mathbb{R}^n v^Tw>0 g v=gw v^Tw>0 g 
v^Tw=w^Tg^Tw=w^Tgw>0.
 g","['linear-algebra', 'matrices', 'matrix-decomposition', 'positive-definite', 'symmetry']"
83,Decomposition of trace zero real matrix with purely imaginary eigenvalues,Decomposition of trace zero real matrix with purely imaginary eigenvalues,,"I would like to prove that for a matrix $A\in sl(2,\mathbb{R})$ , i.e. a real matrix with $\text{tr}(A)=0$ , if the eigenvalues of $A$ are $\pm i\alpha$ , for $\alpha\in \mathbb{R}$ , then there is a real matrix $M$ with $\det(M)>0$ and $$ MAM^{-1}=\alpha J $$ where $J=\begin{pmatrix}0&1\\-1&0\end{pmatrix}$ . It seems like it should be easy, but I am having some difficulty proving it by hand. I am wondering if there is a more abstract solution (I don't know any Lie theory). Thanks for any help!","I would like to prove that for a matrix , i.e. a real matrix with , if the eigenvalues of are , for , then there is a real matrix with and where . It seems like it should be easy, but I am having some difficulty proving it by hand. I am wondering if there is a more abstract solution (I don't know any Lie theory). Thanks for any help!","A\in sl(2,\mathbb{R}) \text{tr}(A)=0 A \pm i\alpha \alpha\in \mathbb{R} M \det(M)>0 
MAM^{-1}=\alpha J
 J=\begin{pmatrix}0&1\\-1&0\end{pmatrix}","['linear-algebra', 'matrices', 'lie-algebras']"
84,Fields over which matrix is diagonalizable,Fields over which matrix is diagonalizable,,"I recently came across diagonalizations of certain special matricies in my Linear Algebra lecture (real symmetric, self-adjoint, orthogonal, etc), and have a couple questions about knowing which field(s) a matrix is diagonalizable over: If a matrix with complex entries is diagonalizable, is it only diagonalizable over $\mathbb{C}$ , or are there such matricies that are also diagonalizable over $\mathbb{R}$ ? If a matrix with complex entries has only real eigenvalues (which is the case for self-adjoint matricies), is it diagonalizable only over $\mathbb{C}$ ? If a matrix with real entries is diagonalizable, is it always diagonalizable over $\mathbb{R}$ and not just $\mathbb{C}$ ?","I recently came across diagonalizations of certain special matricies in my Linear Algebra lecture (real symmetric, self-adjoint, orthogonal, etc), and have a couple questions about knowing which field(s) a matrix is diagonalizable over: If a matrix with complex entries is diagonalizable, is it only diagonalizable over , or are there such matricies that are also diagonalizable over ? If a matrix with complex entries has only real eigenvalues (which is the case for self-adjoint matricies), is it diagonalizable only over ? If a matrix with real entries is diagonalizable, is it always diagonalizable over and not just ?",\mathbb{C} \mathbb{R} \mathbb{C} \mathbb{R} \mathbb{C},"['linear-algebra', 'matrices', 'field-theory', 'diagonalization']"
85,Centralizer/normalizer of degree $n$ extension embedded in $\mathrm{GL}_n(k)$,Centralizer/normalizer of degree  extension embedded in,n \mathrm{GL}_n(k),"Let $k$ be a finite field, and let $\ell/k$ be the unique degree $n$ extension of $k$ . Then, by choosing a basis for $\ell$ , we can identify $\ell^\times$ with a subgroup of $G=\mathrm{GL}_n(k)$ . Such an embedding is uniquely determined up to conjugacy, so we may ask: Question: What is the centralizer/normalizer of $\ell^\times\subset G$ ? A couple of observations: For each $g\in N_G(\ell^\times)$ , the conjugation action $g-g^{-1}\colon\ell\to\ell$ fixes the subfield $k$ (since $Z(G)=k^\times$ ), so there is a homomorphism $N_G(\ell^\times)/C_G(\ell^\times)\to\mathrm{Gal}(\ell/k)$ . Moreover, this is surjective, since if $\ell=k(\alpha)$ and $\sigma\in\mathrm{Gal}(\ell/k)$ is the generator, then $\alpha,\sigma(\alpha)\in G$ are conjugate (they are clearly conjugate over $\ell$ ). If $g\in C_G(\ell^\times)$ , then $\ell[g]\supset\ell$ is a commutative algebra. Maybe exploring properties of this algebra tells us something?? When $\ell/k$ is quadratic and $k$ has characteristic $>2$ , then $\ell=k(\sqrt a)$ for some $a\in k$ , so $\ell^\times$ can be identified with $$\{uI_2+v\begin{pmatrix}&a\\1&\end{pmatrix}:u,v\in k,(u,v)\ne(0,0)\}.$$ Now, by direct computation, we see that $C_G(\ell^\times)=\ell^\times$ , but $\begin{pmatrix}-1&\\&1\end{pmatrix}\in N_G(\ell^\times)$ corresponds to the generator of the Galois group of $\ell/k$ . In general, I believe $C_G(\ell^\times)=\ell^\times$ , and $N_G(\ell^\times)$ is some $C_n$ -extension, but I cannot prove it.","Let be a finite field, and let be the unique degree extension of . Then, by choosing a basis for , we can identify with a subgroup of . Such an embedding is uniquely determined up to conjugacy, so we may ask: Question: What is the centralizer/normalizer of ? A couple of observations: For each , the conjugation action fixes the subfield (since ), so there is a homomorphism . Moreover, this is surjective, since if and is the generator, then are conjugate (they are clearly conjugate over ). If , then is a commutative algebra. Maybe exploring properties of this algebra tells us something?? When is quadratic and has characteristic , then for some , so can be identified with Now, by direct computation, we see that , but corresponds to the generator of the Galois group of . In general, I believe , and is some -extension, but I cannot prove it.","k \ell/k n k \ell \ell^\times G=\mathrm{GL}_n(k) \ell^\times\subset G g\in N_G(\ell^\times) g-g^{-1}\colon\ell\to\ell k Z(G)=k^\times N_G(\ell^\times)/C_G(\ell^\times)\to\mathrm{Gal}(\ell/k) \ell=k(\alpha) \sigma\in\mathrm{Gal}(\ell/k) \alpha,\sigma(\alpha)\in G \ell g\in C_G(\ell^\times) \ell[g]\supset\ell \ell/k k >2 \ell=k(\sqrt a) a\in k \ell^\times \{uI_2+v\begin{pmatrix}&a\\1&\end{pmatrix}:u,v\in k,(u,v)\ne(0,0)\}. C_G(\ell^\times)=\ell^\times \begin{pmatrix}-1&\\&1\end{pmatrix}\in N_G(\ell^\times) \ell/k C_G(\ell^\times)=\ell^\times N_G(\ell^\times) C_n","['matrices', 'group-theory', 'finite-groups', 'finite-fields']"
86,Moore-Penrose pseudoinverse solves the least squares problem (SVD framework) [duplicate],Moore-Penrose pseudoinverse solves the least squares problem (SVD framework) [duplicate],,"This question already has answers here : Prove uniqueness of solutions of different OLS matrix cases (2 answers) Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved I am a computer science researcher who has to learn some numerical linear algebra for my work. I have been struggling with the SVD and Moore-Penrose pseudoinverse as of late. I am trying to solve some problems to get more comfortable with what should probably be routine manipulations. First of all, I have gone through similar questions on Stack Exchange but I believe they were more general and are not equivalent. I am working in the framework where $A^{\dagger} = V\Lambda^{\dagger}U^T$ . So, basically, I am using SVD's. The matrix $A$ of course is identified with $U\Lambda V^T$ Problem Consider the matrix equation $Ax=y$ , where $A\in R^{m\times n}$ . The corresponding least squares problem is to find a least squares solution $x_{\text{LS}}$ that minimizes the Euclidean norm of the residual, i.e., $$\|Ax_{\text{LS}}-y\| = \min_{x \in \Bbb R^n} \|Ax-y\| = \min_{z \in \mbox{Ran}(A)}\|z-y\|$$ a) Show that $A^{\dagger}y$ is a least-squares solution and satisfies the normal equation $A^TAx=A^Ty$ . Why is this solution special? b) Show that $\ker(A^TA) = \ker(A)$ . c) Use the above results to deduce that $x \in \Bbb R^n$ is a least-squares solution if and only if it satisfies the normal equation. Help on any or all of these parts is appreciated. I'd also appreciate links to relevant posts. Like I said, I've read similar questions but did not understand them as they were in a more general framework. Edit: I have solved b). It didn't depend on a) as I had initially thought and it is pretty straightforward to solve, see eg. here: Prove that for a real matrix $A$, $\ker(A) = \ker(A^TA)$ Edit: I realize that part a) might be more involved than I had expected... Assuming parts a) and b), can someone help me with part c)?","This question already has answers here : Prove uniqueness of solutions of different OLS matrix cases (2 answers) Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved I am a computer science researcher who has to learn some numerical linear algebra for my work. I have been struggling with the SVD and Moore-Penrose pseudoinverse as of late. I am trying to solve some problems to get more comfortable with what should probably be routine manipulations. First of all, I have gone through similar questions on Stack Exchange but I believe they were more general and are not equivalent. I am working in the framework where . So, basically, I am using SVD's. The matrix of course is identified with Problem Consider the matrix equation , where . The corresponding least squares problem is to find a least squares solution that minimizes the Euclidean norm of the residual, i.e., a) Show that is a least-squares solution and satisfies the normal equation . Why is this solution special? b) Show that . c) Use the above results to deduce that is a least-squares solution if and only if it satisfies the normal equation. Help on any or all of these parts is appreciated. I'd also appreciate links to relevant posts. Like I said, I've read similar questions but did not understand them as they were in a more general framework. Edit: I have solved b). It didn't depend on a) as I had initially thought and it is pretty straightforward to solve, see eg. here: Prove that for a real matrix $A$, $\ker(A) = \ker(A^TA)$ Edit: I realize that part a) might be more involved than I had expected... Assuming parts a) and b), can someone help me with part c)?",A^{\dagger} = V\Lambda^{\dagger}U^T A U\Lambda V^T Ax=y A\in R^{m\times n} x_{\text{LS}} \|Ax_{\text{LS}}-y\| = \min_{x \in \Bbb R^n} \|Ax-y\| = \min_{z \in \mbox{Ran}(A)}\|z-y\| A^{\dagger}y A^TAx=A^Ty \ker(A^TA) = \ker(A) x \in \Bbb R^n,"['linear-algebra', 'matrices', 'least-squares', 'svd', 'pseudoinverse']"
87,Finding the Inverse of a Matrix using Row Operations,Finding the Inverse of a Matrix using Row Operations,,Problem: Let $A$ be the following matrix. Find $A^{-1}$ . $$ \begin{bmatrix} 	-1 & 2 & 3 \\ 	4 & 5 & 6 \\ 	7 & 8 & 9 \\ \end{bmatrix} $$ Answer: \begin{align*} \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	4 & 5 & 6 & 0 & 1 & 0 \\ 	7 & 8 & 9 & 0 & 0 & 1 \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & 13 & 18 & 4 & 1 & 0 \\ 	0 & 22 & 30 & 7 & 0 & 1 \\ \end{bmatrix} \\ \end{align*} Now I multiply the second row by $-22$ and the third row by $13$ . \begin{align*} \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & 13(-22) & 18(-22) &- 88 & -22 & 0 \\ 	0 & 22(13) & 390 & 91 & 0 & 13 \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & 13(-22) & 18(-22) &- 88 & -22 & 0 \\ 	0 & 0 & 390 - 18(22) & 91 - 88 & -22 & 13 \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & -286 & -396 & -88 & -22 & 0 \\ 	0 & 0 & -6 & 3 & -22 & 13 \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & -286 & -396 & -88 & -22 & 0 \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \end{align*} Now we multiply the third row by $396$ and add it to the second row. \begin{align*} \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & -286 & 0 & -88 -198 & -22 + \frac{396(11)}{2} & -\frac{396(13)}{6} \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & -286 & 0 & -286 & 2156 & -858 \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & 1 & 0 & 1 & -\dfrac{2156}{286} & \dfrac{858}{286} \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & 1 & 0 & 1 & -\dfrac{98}{13} & \dfrac{66}{13} \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \end{align*} Now we need to work on the first row. \begin{align*} \begin{bmatrix} 	-1 & 0 & 3 & 1 & \dfrac{2(98)}{13} & \dfrac{-2(66)}{13} \\ 	0 & 1 & 0 & 1 & -\dfrac{98}{13} & \dfrac{66}{13} \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 0 & 3 & 1 & \dfrac{196}{13} & -\dfrac{132}{13} \\ 	0 & 1 & 0 & 1 & -\dfrac{98}{13} & \dfrac{66}{13} \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \end{align*} I have reason to believe the correct answer is: $$  \begin{bmatrix} 	-\dfrac{1}{2} & 1 & -\dfrac{1}{2} \\ 	1 & -5 & 3 \\ 	-\dfrac{1}{2} & \dfrac{11}{3} & -\dfrac{13}{6} \\ \end{bmatrix} $$ As such I am confident I made a mistake. Where did I go wrong? Based upon the feed back I got I updated my solution. Here is an updated but still wrong solution. \begin{align*} \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	4 & 5 & 6 & 0 & 1 & 0 \\ 	7 & 8 & 9 & 0 & 0 & 1 \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & 13 & 18 & 4 & 1 & 0 \\ 	0 & 22 & 30 & 7 & 0 & 1 \\ \end{bmatrix} \\ \end{align*} Now I multiply the second row by $-22$ and the third row by $13$ . \begin{align*} \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & 13(-22) & 18(-22) &- 88 & -22 & 0 \\ 	0 & 22(13) & 390 & 91 & 0 & 13 \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & 13(-22) & 18(-22) &- 88 & -22 & 0 \\ 	0 & 0 & 390 - 18(22) & 91 - 88 & -22 & 13 \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & -286 & -396 & -88 & -22 & 0 \\ 	0 & 0 & -6 & 3 & -22 & 13 \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & -286 & -396 & -88 & -22 & 0 \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \end{align*} Now we multiply the third row by $396$ and add it to the second row. \begin{align*} \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & -286 & 0 & -88 -198 & -22 + \frac{396(11)}{2} & -\frac{396(13)}{6} \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & -286 & 0 & -286 & 2156 & -858 \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & 1 & 0 & 1 & -\dfrac{2156}{286} & \dfrac{858}{286} \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \begin{bmatrix} 	-1 & 2 & 3 & 1 & 0 & 0 \\ 	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \end{align*} Now we need to work on the first row. \begin{align*} \begin{bmatrix} 	-1 & 0 & 3 & -1 & -\dfrac{ 98}{26} & -6 \\ 	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \begin{bmatrix} 	1 & 0 & -3 & 1 & \dfrac{ 49}{13} & 6 \\ 	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \begin{bmatrix} 	1 & 0 & 0 & 1 -\frac{3}{2} & \dfrac{ 49}{13} + 3 & 6 - \frac{3(13)}{6} \\ 	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \begin{bmatrix} 	1 & 0 & 0 & -\frac{1}{2} & 9 & - \frac{1}{2} \\ 	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\ 	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\ \end{bmatrix} \\ \end{align*} Where did I go wrong?,Problem: Let be the following matrix. Find . Answer: Now I multiply the second row by and the third row by . Now we multiply the third row by and add it to the second row. Now we need to work on the first row. I have reason to believe the correct answer is: As such I am confident I made a mistake. Where did I go wrong? Based upon the feed back I got I updated my solution. Here is an updated but still wrong solution. Now I multiply the second row by and the third row by . Now we multiply the third row by and add it to the second row. Now we need to work on the first row. Where did I go wrong?,"A A^{-1}  \begin{bmatrix}
	-1 & 2 & 3 \\
	4 & 5 & 6 \\
	7 & 8 & 9 \\
\end{bmatrix}  \begin{align*}
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	4 & 5 & 6 & 0 & 1 & 0 \\
	7 & 8 & 9 & 0 & 0 & 1 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 13 & 18 & 4 & 1 & 0 \\
	0 & 22 & 30 & 7 & 0 & 1 \\
\end{bmatrix} \\
\end{align*} -22 13 \begin{align*}
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 13(-22) & 18(-22) &- 88 & -22 & 0 \\
	0 & 22(13) & 390 & 91 & 0 & 13 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 13(-22) & 18(-22) &- 88 & -22 & 0 \\
	0 & 0 & 390 - 18(22) & 91 - 88 & -22 & 13 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & -396 & -88 & -22 & 0 \\
	0 & 0 & -6 & 3 & -22 & 13 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & -396 & -88 & -22 & 0 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\end{align*} 396 \begin{align*}
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & 0 & -88 -198 & -22 + \frac{396(11)}{2} & -\frac{396(13)}{6} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & 0 & -286 & 2156 & -858 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 1 & 0 & 1 & -\dfrac{2156}{286} & \dfrac{858}{286} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & \dfrac{66}{13} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\end{align*} \begin{align*}
\begin{bmatrix}
	-1 & 0 & 3 & 1 & \dfrac{2(98)}{13} & \dfrac{-2(66)}{13} \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & \dfrac{66}{13} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 0 & 3 & 1 & \dfrac{196}{13} & -\dfrac{132}{13} \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & \dfrac{66}{13} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\end{align*}   \begin{bmatrix}
	-\dfrac{1}{2} & 1 & -\dfrac{1}{2} \\
	1 & -5 & 3 \\
	-\dfrac{1}{2} & \dfrac{11}{3} & -\dfrac{13}{6} \\
\end{bmatrix}  \begin{align*}
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	4 & 5 & 6 & 0 & 1 & 0 \\
	7 & 8 & 9 & 0 & 0 & 1 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 13 & 18 & 4 & 1 & 0 \\
	0 & 22 & 30 & 7 & 0 & 1 \\
\end{bmatrix} \\
\end{align*} -22 13 \begin{align*}
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 13(-22) & 18(-22) &- 88 & -22 & 0 \\
	0 & 22(13) & 390 & 91 & 0 & 13 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 13(-22) & 18(-22) &- 88 & -22 & 0 \\
	0 & 0 & 390 - 18(22) & 91 - 88 & -22 & 13 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & -396 & -88 & -22 & 0 \\
	0 & 0 & -6 & 3 & -22 & 13 \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & -396 & -88 & -22 & 0 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\end{align*} 396 \begin{align*}
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & 0 & -88 -198 & -22 + \frac{396(11)}{2} & -\frac{396(13)}{6} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & -286 & 0 & -286 & 2156 & -858 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 1 & 0 & 1 & -\dfrac{2156}{286} & \dfrac{858}{286} \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	-1 & 2 & 3 & 1 & 0 & 0 \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\end{align*} \begin{align*}
\begin{bmatrix}
	-1 & 0 & 3 & -1 & -\dfrac{ 98}{26} & -6 \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	1 & 0 & -3 & 1 & \dfrac{ 49}{13} & 6 \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	1 & 0 & 0 & 1 -\frac{3}{2} & \dfrac{ 49}{13} + 3 & 6 - \frac{3(13)}{6} \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\begin{bmatrix}
	1 & 0 & 0 & -\frac{1}{2} & 9 & - \frac{1}{2} \\
	0 & 1 & 0 & 1 & -\dfrac{98}{13} & 3 \\
	0 & 0 & 1 & -\frac{1}{2} & \frac{11}{3} & -\frac{13}{6} \\
\end{bmatrix} \\
\end{align*}","['linear-algebra', 'matrices', 'inverse']"
88,Making a matrix with an unknown diagonalizable,Making a matrix with an unknown diagonalizable,,"I'm stuck on a question about diagonalizability of matrices and need some help moving forward.I think I made some progress but I'm also not sure if its any good.  Let $a \in \mathbb{R}$ , $A$ is a matrix. For which values of $a$ is the matrix $A$ diagonalizable. $$A:= \begin{pmatrix} 1 & 2 & 3 & 4\\ 0 & 2 & a & 5\\ 0 & 0 & 2 & 6\\ 0 & 0 & 0 & 7 \\ \end{pmatrix} \in \mathbb{R^{4\times4}}$$ Firstly, if I'm not mistaken, any matrix whose charachteristic polynomial is a product of distinct linear factors can be diagonalized. So I simply tried to find the characteristic polynomial first. $$ p_A(x)= \det(xE_n-A)=\det \begin{pmatrix} x-1 & -2 & -3 & -4\\ 0 & x-2 & -a & -5\\ 0 & 0 & x-2 & -6\\ 0 & 0 & 0 & x-7 \\ \end{pmatrix}= (x-2) \times \det \begin{pmatrix} -2 & -a & -5 \\ x-2 & x-2 & -6 \\ 0 & 0 & x-7 \\ \end{pmatrix}.$$ $$ (x-2) \times \det \begin{pmatrix} -2 & -a & -5 \\ x-2 & x-2 & -6 \\ 0 & 0 & x-7 \\ \end{pmatrix}=-a^2 x^3 + 2 a^2 x^2 - 9 a x^2 + 32 a x - 28 a - 2 x^3 + 23 x^2 - 66 x + 56.$$ Unless there is an error, this should be the characteristic polynomial. From here I just checked some simple cases like what if $a=0,a=1,a=2$ and found that if $a=2$ then $p_A(x)=-6x^3+13x^2-2x=(-x)(x-2)(6x-1).$ From here however I don't know how to generalize this for any other $a$ or if there is a way to check whether this is the only possible value for $a$ .","I'm stuck on a question about diagonalizability of matrices and need some help moving forward.I think I made some progress but I'm also not sure if its any good.  Let , is a matrix. For which values of is the matrix diagonalizable. Firstly, if I'm not mistaken, any matrix whose charachteristic polynomial is a product of distinct linear factors can be diagonalized. So I simply tried to find the characteristic polynomial first. Unless there is an error, this should be the characteristic polynomial. From here I just checked some simple cases like what if and found that if then From here however I don't know how to generalize this for any other or if there is a way to check whether this is the only possible value for .","a \in \mathbb{R} A a A A:= \begin{pmatrix}
1 & 2 & 3 & 4\\
0 & 2 & a & 5\\
0 & 0 & 2 & 6\\
0 & 0 & 0 & 7 \\
\end{pmatrix} \in \mathbb{R^{4\times4}}  p_A(x)= \det(xE_n-A)=\det \begin{pmatrix}
x-1 & -2 & -3 & -4\\
0 & x-2 & -a & -5\\
0 & 0 & x-2 & -6\\
0 & 0 & 0 & x-7 \\
\end{pmatrix}= (x-2) \times \det \begin{pmatrix}
-2 & -a & -5 \\
x-2 & x-2 & -6 \\
0 & 0 & x-7 \\
\end{pmatrix}.  (x-2) \times \det \begin{pmatrix}
-2 & -a & -5 \\
x-2 & x-2 & -6 \\
0 & 0 & x-7 \\
\end{pmatrix}=-a^2 x^3 + 2 a^2 x^2 - 9 a x^2 + 32 a x - 28 a - 2 x^3 + 23 x^2 - 66 x + 56. a=0,a=1,a=2 a=2 p_A(x)=-6x^3+13x^2-2x=(-x)(x-2)(6x-1). a a","['linear-algebra', 'matrices', 'diagonalization']"
89,How do you go about Gaussian elimination modulo p?,How do you go about Gaussian elimination modulo p?,,"The problem at hand is specifically to find the inverse of a matrix using Gaussian elimination modulo 29. I am familiar with the process of regular Gaussian elimination and modular arithmetic but not the combination of the two. The matrix you are given is: $$A=\begin{pmatrix} 12 & 3 & 23 \\ 28 & 1 & 2 \\ 11 & 0 & 7 \end{pmatrix}$$ I first start out the way I would when doing a normal Gaussian elimination to find the inverse by writing out the following matrix: $$A=\begin{pmatrix} 12 & 3 & 23 & 1 & 0 & 0 \\ 28 & 1 & 2 & 0 & 1 & 0\\ 11 & 0 & 7 & 0 & 0 & 1 \end{pmatrix}$$ From here, I first tried solving it how I would normally and then taking the elements modulo 29 at the end but unfortunately you end up with fractions at the end. After that I've tried working with only integers and tried to reduce numbers using modulo 29 as I go but haven't found success and I'm not sure if that is how you're supposed to do it. My question more generally would be how you go about Gaussian elimination modulo p for any prime number. Thanks in advance!","The problem at hand is specifically to find the inverse of a matrix using Gaussian elimination modulo 29. I am familiar with the process of regular Gaussian elimination and modular arithmetic but not the combination of the two. The matrix you are given is: I first start out the way I would when doing a normal Gaussian elimination to find the inverse by writing out the following matrix: From here, I first tried solving it how I would normally and then taking the elements modulo 29 at the end but unfortunately you end up with fractions at the end. After that I've tried working with only integers and tried to reduce numbers using modulo 29 as I go but haven't found success and I'm not sure if that is how you're supposed to do it. My question more generally would be how you go about Gaussian elimination modulo p for any prime number. Thanks in advance!","A=\begin{pmatrix}
12 & 3 & 23 \\
28 & 1 & 2 \\
11 & 0 & 7
\end{pmatrix} A=\begin{pmatrix}
12 & 3 & 23 & 1 & 0 & 0 \\
28 & 1 & 2 & 0 & 1 & 0\\
11 & 0 & 7 & 0 & 0 & 1
\end{pmatrix}","['matrices', 'modular-arithmetic', 'gaussian-elimination']"
90,Can $\mathbb{R}^3$ with Hadamard product be represented as matrices?,Can  with Hadamard product be represented as matrices?,\mathbb{R}^3,"It is known that $\mathbb{R}^2$ with Hadamard product, represented as pairs or numbers $(a,b)$ with element-wise operations is isomorphic to real matrices of the form $\left( \begin{array}{cc}  \frac{a+b}{2} & \frac{a-b}{2} \\  \frac{a-b}{2} & \frac{a+b}{2} \\ \end{array} \right)$ , and also to the split-complex numbers. But I wonder, whether it is possible to represent $\mathbb{R}^3$ with Hadamard product and element-wise operations as real matrices in a similar way?","It is known that with Hadamard product, represented as pairs or numbers with element-wise operations is isomorphic to real matrices of the form , and also to the split-complex numbers. But I wonder, whether it is possible to represent with Hadamard product and element-wise operations as real matrices in a similar way?","\mathbb{R}^2 (a,b) \left(
\begin{array}{cc}
 \frac{a+b}{2} & \frac{a-b}{2} \\
 \frac{a-b}{2} & \frac{a+b}{2} \\
\end{array}
\right) \mathbb{R}^3","['matrices', 'ring-theory', 'hypercomplex-numbers']"
91,Assume that $A$ has the minimal polynomial as $(x-1)(x-2)$ and c.p. as $(x-2)^2(x-1)$,Assume that  has the minimal polynomial as  and c.p. as,A (x-1)(x-2) (x-2)^2(x-1),"Let $$A := \begin{pmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1\\ \end {pmatrix}$$ Find the number of matrices similar to $A$ whose entries are from $\mathbb{Z}/\mathbb{3Z}$ . Since $A$ is a $3 \times 3$ matrix and $A$ has the minimal polynomial as $(x-1)(x-2)$ and c.p. as $(x-2)^2(x-1)$ .Then any matrix simialr to $A$ will have the same minimal and characteristic polynomial. Any matrix similar to $A$ will be an upper triangular matrix or a lower triangular matrix with diagonals as $\{1,2,2\}$ and $\{2,2,1\}$ . Then number of upper triangular matrix with diagonals as $\{1,2,2\}$ is $3^3$ . Similarly number of lower triangular matrix is $3^3$ . Then the total numbers is $54$ . So the total number of matrices as $2 \times 54 = 108$ .The answer is $117$ .What am i missing?",Let Find the number of matrices similar to whose entries are from . Since is a matrix and has the minimal polynomial as and c.p. as .Then any matrix simialr to will have the same minimal and characteristic polynomial. Any matrix similar to will be an upper triangular matrix or a lower triangular matrix with diagonals as and . Then number of upper triangular matrix with diagonals as is . Similarly number of lower triangular matrix is . Then the total numbers is . So the total number of matrices as .The answer is .What am i missing?,"A := \begin{pmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1\\ \end {pmatrix} A \mathbb{Z}/\mathbb{3Z} A 3 \times 3 A (x-1)(x-2) (x-2)^2(x-1) A A \{1,2,2\} \{2,2,1\} \{1,2,2\} 3^3 3^3 54 2 \times 54 = 108 117","['linear-algebra', 'matrices', 'finite-fields', 'minimal-polynomials', 'characteristic-polynomial']"
92,Solution to Linear Time-invariant Matrix ODE $\frac{d\mathbf{P}}{dt} = \mathbf{A}\mathbf{P} + \mathbf{B}$,Solution to Linear Time-invariant Matrix ODE,\frac{d\mathbf{P}}{dt} = \mathbf{A}\mathbf{P} + \mathbf{B},Do we have a solution for the following matrix ODE: $$\frac{d\mathbf{P}}{dt} = \mathbf{A}\mathbf{P} + \mathbf{B}$$ where all matrices are square. My guess is: $$ \mathbf{P}(t) = \exp{(\mathbf{A}t)}\mathbf{P}(0) + \mathbf{A}^{-1}[\exp{(\mathbf{A}t)}\mathbf{B} - \mathbf{B}] $$ when $\mathbf{A}$ is invertible. I am not sure it is correct and not clear when $\mathbf{A}$ is not full rank.,Do we have a solution for the following matrix ODE: where all matrices are square. My guess is: when is invertible. I am not sure it is correct and not clear when is not full rank.,"\frac{d\mathbf{P}}{dt} = \mathbf{A}\mathbf{P} + \mathbf{B} 
\mathbf{P}(t) = \exp{(\mathbf{A}t)}\mathbf{P}(0) + \mathbf{A}^{-1}[\exp{(\mathbf{A}t)}\mathbf{B} - \mathbf{B}]
 \mathbf{A} \mathbf{A}","['linear-algebra', 'matrices', 'ordinary-differential-equations', 'analysis', 'matrix-equations']"
93,Prove that the set of skew-symmetric matrices is closed under addition,Prove that the set of skew-symmetric matrices is closed under addition,,"I am trying to prove that W is a subspace of V with: $V = M_{n\times n}$ , $W = \{A \in M_{n\times n} : A = -A^T\}$ I am fairly sure $W$ is closed under addition, but am not sure how to prove it for all $M_{n\times n}$ I can prove it for $M_{2\times 2}$ : Let $A = \begin{bmatrix}0 & a\\-a & 0\end{bmatrix} \in W$ Let $B = \begin{bmatrix}0 & b\\-b & 0\end{bmatrix} \in W$ $A + B = \begin{bmatrix}0 & a\\-a & 0\end{bmatrix} + \begin{bmatrix}0 & b\\-b & 0\end{bmatrix}$ $ = \begin{bmatrix}0 & a + b\\-a-b & 0\end{bmatrix} = \begin{bmatrix}0 & a + b\\-(a+b) & 0\end{bmatrix} \in W$ How could I generalise this to all $A, B \in M_{n\times n}$","I am trying to prove that W is a subspace of V with: , I am fairly sure is closed under addition, but am not sure how to prove it for all I can prove it for : Let Let How could I generalise this to all","V = M_{n\times n} W = \{A \in M_{n\times n} : A = -A^T\} W M_{n\times n} M_{2\times 2} A = \begin{bmatrix}0 & a\\-a & 0\end{bmatrix} \in W B = \begin{bmatrix}0 & b\\-b & 0\end{bmatrix} \in W A + B = \begin{bmatrix}0 & a\\-a & 0\end{bmatrix} + \begin{bmatrix}0 & b\\-b & 0\end{bmatrix}  = \begin{bmatrix}0 & a + b\\-a-b & 0\end{bmatrix} = \begin{bmatrix}0 & a + b\\-(a+b) & 0\end{bmatrix} \in W A, B \in M_{n\times n}","['matrices', 'vector-spaces', 'skew-symmetric-matrices']"
94,"For every square matrix $U$ there exist a diagonal matrix $E$ with $e_{i,i}=-1 , 1 $ such that $E U - I_n$ is non-singular",For every square matrix  there exist a diagonal matrix  with  such that  is non-singular,"U E e_{i,i}=-1 , 1  E U - I_n","For every square matrix $U$ there exist a diagonal matrix $E$ with $e_{i,i} = \pm 1$ such that $E U - I_n$ is non-singular. If $U$ is unitary then $EU$ is also unitary Note that all computation is over coefficient modulo $3$ . I want to prove this statement, but have no idea how and where to start.","For every square matrix there exist a diagonal matrix with such that is non-singular. If is unitary then is also unitary Note that all computation is over coefficient modulo . I want to prove this statement, but have no idea how and where to start.","U E e_{i,i} = \pm 1 E U - I_n U EU 3","['linear-algebra', 'matrices']"
95,Determinant of a $4 \times 4$ matrix,Determinant of a  matrix,4 \times 4,"If $$ A = \begin{pmatrix}a&b&c&d\\-b&a&-d&c\\-c&d&a&-b\\-d&-c&b&a\end{pmatrix} $$ calculate $\det(A)$ . If you calculate $$AA^t=\begin{pmatrix}a^2+b^2+c^2+d^2&0&0&0\\0&a^2+b^2+c^2+d^2&0&0\\ 0&0&a^2+b^2+c^2+d^2&0\\0&0&0&a^2+b^2+c^2+d^2\end{pmatrix}$$ $$\det(AA^t)=(a^2+b^2+c^2+d^2)^4\Leftrightarrow (\det(A))^2=(a^2+b^2+c^2+d^2)^4$$ The answer is $\det(A)=(a^2+b^2+c^2+d^2)^2$ and not $\det(A)=-(a^2+b^2+c^2+d^2)^2$ . Short of calculating it by hand, why is it not negative?","If calculate . If you calculate The answer is and not . Short of calculating it by hand, why is it not negative?", A = \begin{pmatrix}a&b&c&d\\-b&a&-d&c\\-c&d&a&-b\\-d&-c&b&a\end{pmatrix}  \det(A) AA^t=\begin{pmatrix}a^2+b^2+c^2+d^2&0&0&0\\0&a^2+b^2+c^2+d^2&0&0\\ 0&0&a^2+b^2+c^2+d^2&0\\0&0&0&a^2+b^2+c^2+d^2\end{pmatrix} \det(AA^t)=(a^2+b^2+c^2+d^2)^4\Leftrightarrow (\det(A))^2=(a^2+b^2+c^2+d^2)^4 \det(A)=(a^2+b^2+c^2+d^2)^2 \det(A)=-(a^2+b^2+c^2+d^2)^2,"['linear-algebra', 'matrices', 'determinant']"
96,"On existence of a nonzero integer vector $x$ such that $\|Bx\|_1\leq \sqrt[n]{n!\,|\det(B)|}$",On existence of a nonzero integer vector  such that,"x \|Bx\|_1\leq \sqrt[n]{n!\,|\det(B)|}","Let $B=(b_{ij})$ be a real invertible $n\times n$ matrix for some $n\ge2$ . Prove that there exists an integer vector $(x_1,\cdots, x_n)^T\ne0$ such that $\|Bx\|_1\leq\sqrt[n]{n!\,|\det(B)|}$ . There are several inequalities that might be useful for this problem: Holder's inequality, the Cauchy-Schwarz inequality for inner products, the AM-GM inequality, the power-mean inequality, and Jensen's inequality. I also know that $\det(B) = \sum_{\sigma \in \Pi_n}(-1)^\sigma b_{1, \sigma(1)}b_{2,\sigma(2)}\cdots b_{n,\sigma(n)},$ where $\Pi(n)$ is the set of permutations of $\{1,\cdots, n\}$ and that it can be obtained from cofactor expansion along any row or column, which might be useful for proving this inequality.","Let be a real invertible matrix for some . Prove that there exists an integer vector such that . There are several inequalities that might be useful for this problem: Holder's inequality, the Cauchy-Schwarz inequality for inner products, the AM-GM inequality, the power-mean inequality, and Jensen's inequality. I also know that where is the set of permutations of and that it can be obtained from cofactor expansion along any row or column, which might be useful for proving this inequality.","B=(b_{ij}) n\times n n\ge2 (x_1,\cdots, x_n)^T\ne0 \|Bx\|_1\leq\sqrt[n]{n!\,|\det(B)|} \det(B) = \sum_{\sigma \in \Pi_n}(-1)^\sigma b_{1, \sigma(1)}b_{2,\sigma(2)}\cdots b_{n,\sigma(n)}, \Pi(n) \{1,\cdots, n\}","['real-analysis', 'calculus', 'linear-algebra', 'matrices', 'inequality']"
97,Inverse of $A-I$,Inverse of,A-I,"I am trying to understand one detail of one solution of below question Let $A$ be n-by-n matrix ( $n \geq 2$ ), and $$ A= \begin{pmatrix} 0 & 1 & 1 & \cdots & 1 \\ 1 & 0 & 1 & \cdots & 1 \\ \vdots & \vdots & \vdots &  & \vdots \\ 1 & 1 & 1 & \cdots & 0 \end{pmatrix} $$ calculate $A^{-1}$ . The solution: Let $A = B - I$ , $B$ is matrix that every element is $1$ . Then let $A^{-1} = aB + bI$ , we  can construct the equation $$ \begin{array}{ll} I &= (B-I)(aB + bI) \\   &= aB^2+(b-a)B -bI \\   &= anB + (b-a)B - bI \\   &= (an+b-a)B - bI \end{array} $$ so $b = -1, a = \frac{1}{n-1}$ . But why it can let $A^{-1} = aB + bI$ directly at the beginning of the solution? Is there some tricks for calculate inverse of such matrix?","I am trying to understand one detail of one solution of below question Let be n-by-n matrix ( ), and calculate . The solution: Let , is matrix that every element is . Then let , we  can construct the equation so . But why it can let directly at the beginning of the solution? Is there some tricks for calculate inverse of such matrix?","A n \geq 2  A=
\begin{pmatrix}
0 & 1 & 1 & \cdots & 1 \\
1 & 0 & 1 & \cdots & 1 \\
\vdots & \vdots & \vdots &  & \vdots \\
1 & 1 & 1 & \cdots & 0
\end{pmatrix}
 A^{-1} A = B - I B 1 A^{-1} = aB + bI 
\begin{array}{ll}
I &= (B-I)(aB + bI) \\
  &= aB^2+(b-a)B -bI \\
  &= anB + (b-a)B - bI \\
  &= (an+b-a)B - bI
\end{array}
 b = -1, a = \frac{1}{n-1} A^{-1} = aB + bI","['linear-algebra', 'matrices', 'inverse']"
98,Problem on rank of a matrix-Mathematics Competition,Problem on rank of a matrix-Mathematics Competition,,"Hi, this problem is from a previous year question paper of a math competition which is held in our college annually. I hope it is  allowed to discuss these problems here, if not please let me know. Let $m,n\in \mathbb N$ . Find the rank of the block matrix $C$ , where $C=\begin{pmatrix} (J-I)_{m\times m} & J_{m\times n}\\ J^T_{n\times m} & A_{n\times n}\end{pmatrix}$ . Here $J$ is the matrix with all $1$ , and $A$ has rank $n$ with $A\neq J, I, J-I$ . My try : The eigenvalues of $(J-I)_{m\times m} $ are $m-1$ with multiplicity $1$ and $-1$ with multiplicity $m-1$ . I am guessing that Rank( $C)=m+n$ , but I dont know any means to prove it. Does there exist any particular result by which rank of block matrices can be determined. I have just started reading matrix theory, so I am not quite sure on how to proceed with these type of problems. Can someone please point me towards any text or provide me some hints on how to complete this problem? Apologies for giving the problem wrong. It has been edited now.","Hi, this problem is from a previous year question paper of a math competition which is held in our college annually. I hope it is  allowed to discuss these problems here, if not please let me know. Let . Find the rank of the block matrix , where . Here is the matrix with all , and has rank with . My try : The eigenvalues of are with multiplicity and with multiplicity . I am guessing that Rank( , but I dont know any means to prove it. Does there exist any particular result by which rank of block matrices can be determined. I have just started reading matrix theory, so I am not quite sure on how to proceed with these type of problems. Can someone please point me towards any text or provide me some hints on how to complete this problem? Apologies for giving the problem wrong. It has been edited now.","m,n\in \mathbb N C C=\begin{pmatrix} (J-I)_{m\times m} & J_{m\times n}\\
J^T_{n\times m} & A_{n\times n}\end{pmatrix} J 1 A n A\neq J, I, J-I (J-I)_{m\times m}  m-1 1 -1 m-1 C)=m+n","['matrices', 'eigenvalues-eigenvectors', 'matrix-rank']"
99,$3\times3$ invertible matrix in $\mathbb{F}_2$ such that $M^7 = I$ [duplicate],invertible matrix in  such that  [duplicate],3\times3 \mathbb{F}_2 M^7 = I,This question already has answers here : Show that there exists a $3 × 3$ invertible matrix $M$ with entries in $\mathbb{Z}/2\mathbb{Z}$ such that $M^7 = I_3$. (2 answers) Closed 2 years ago . Show that there exists a $3\times 3$ invertible matrix $M\neq I_3$ with entries in the field $\mathbb{F}_2$ such that $M^7 =I_3$ . Attempt: $M^7 =I$ can be factorized as $(M- I) ( M^6 + M^5 +...+M+I)=0$ but I don't think it will prove the existence of required M. Trying to find an M by hit and trial method seems a bad idea. Can you please suggest some elegant way of proving it?,This question already has answers here : Show that there exists a $3 × 3$ invertible matrix $M$ with entries in $\mathbb{Z}/2\mathbb{Z}$ such that $M^7 = I_3$. (2 answers) Closed 2 years ago . Show that there exists a invertible matrix with entries in the field such that . Attempt: can be factorized as but I don't think it will prove the existence of required M. Trying to find an M by hit and trial method seems a bad idea. Can you please suggest some elegant way of proving it?,3\times 3 M\neq I_3 \mathbb{F}_2 M^7 =I_3 M^7 =I (M- I) ( M^6 + M^5 +...+M+I)=0,['linear-algebra']
