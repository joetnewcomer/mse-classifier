,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Derivative of an even function is odd and vice versa,Derivative of an even function is odd and vice versa,,"This is the question: ""Show that the derivative of an even function is odd and that the derivative of an odd function is even. (Write the equation that says $f$ is even, and differentiate both sides, using the chain rule.)"" I already read numerous solutions online. The following block shows the official solution, but I didn't quite understand it. (Particularly, I'm not convinced why exactly $dz/dx=-1$ if $z=-x$ .) (1F-6). Following the hint, let $z=-x$ . If $f$ is even, then $f(x)=f(z).$ Differentiating and using the chain rule: $$f'(x)=f'(z) \frac{dz}{dx}=-f'(z),$$ because $\frac{dz}{dx}=-1.$ But this means that $f'$ is odd. Similarly, if $g$ is odd, then $g(x)=-g(z)$ . Differentiating and using the chain rule: $$g'(x)=-g'(z) \frac{dz}{dx}=g'(z),$$ because $\frac{dz}{dx}=-1.$ Thanks in advance =]","This is the question: ""Show that the derivative of an even function is odd and that the derivative of an odd function is even. (Write the equation that says is even, and differentiate both sides, using the chain rule.)"" I already read numerous solutions online. The following block shows the official solution, but I didn't quite understand it. (Particularly, I'm not convinced why exactly if .) (1F-6). Following the hint, let . If is even, then Differentiating and using the chain rule: because But this means that is odd. Similarly, if is odd, then . Differentiating and using the chain rule: because Thanks in advance =]","f dz/dx=-1 z=-x z=-x f f(x)=f(z). f'(x)=f'(z) \frac{dz}{dx}=-f'(z), \frac{dz}{dx}=-1. f' g g(x)=-g(z) g'(x)=-g'(z) \frac{dz}{dx}=g'(z), \frac{dz}{dx}=-1.","['derivatives', 'chain-rule']"
1,Showing $f'(x) = f(x)$ implies an exponential function [duplicate],Showing  implies an exponential function [duplicate],f'(x) = f(x),"This question already has answers here : Closed 12 years ago . Possible Duplicate: Proof that $\exp(x)$ is the only function for which $f(x) = f'(x)$ How can I show the statement $f'(x) = f(x)$ implies the function is defined as $f: \mathbb{R} \rightarrow \mathbb{R} : x \rightarrow a\cdot \exp(x)$ without using integrals. My attempt at a solution: I tried to show that the Taylor series of $f$ has the same structure as $a\cdot \exp(x)$ , but I fail at showing that the error on the series converges to zero.","This question already has answers here : Closed 12 years ago . Possible Duplicate: Proof that is the only function for which How can I show the statement implies the function is defined as without using integrals. My attempt at a solution: I tried to show that the Taylor series of has the same structure as , but I fail at showing that the error on the series converges to zero.",\exp(x) f(x) = f'(x) f'(x) = f(x) f: \mathbb{R} \rightarrow \mathbb{R} : x \rightarrow a\cdot \exp(x) f a\cdot \exp(x),['derivatives']
2,Continuity of Derivative at a point.,Continuity of Derivative at a point.,,Is it possible that derivative of a function exists at a point but derivative does not exist in neighbourhood of that point. If this happens then how is it possible. I feel that if derivative exists at a point then the left hand derivative is equal to the right hand derivative so derivative should exist in neighbourhood of that point.,Is it possible that derivative of a function exists at a point but derivative does not exist in neighbourhood of that point. If this happens then how is it possible. I feel that if derivative exists at a point then the left hand derivative is equal to the right hand derivative so derivative should exist in neighbourhood of that point.,,"['derivatives', 'differential']"
3,Obtaining derivative of log of sigmoid function,Obtaining derivative of log of sigmoid function,,I saw the following result: $$ \dfrac{\mathrm{d}}{\mathrm{d}x} \left( \log\left( \dfrac{1}{1+\mathrm{e}^{-x}} \right) \right) = \dfrac{1}{\mathrm{e}^x+1} $$ What are the intermediary steps for obtaining this result?,I saw the following result: $$ \dfrac{\mathrm{d}}{\mathrm{d}x} \left( \log\left( \dfrac{1}{1+\mathrm{e}^{-x}} \right) \right) = \dfrac{1}{\mathrm{e}^x+1} $$ What are the intermediary steps for obtaining this result?,,"['derivatives', 'logarithms', 'exponential-function']"
4,How is implicit differentiation formally defined?,How is implicit differentiation formally defined?,,"I get that differentiation is an operation used on a function, so if a function is defined $x\mapsto x^2$, the derivative is  $$ (x\mapsto x^2)'    = x \mapsto \lim_{h\to 0} \frac{x^2+2xh+h^2-x^2}{h} = x\mapsto 2x. $$ But how can you extend the definition $f' = \dfrac{f_h-f}{h}$ in such a way that it works with implicit functions/multifunctions? I know that it works, but I don't understand how it works for equations like $y^2 = 4-x^2$.","I get that differentiation is an operation used on a function, so if a function is defined $x\mapsto x^2$, the derivative is  $$ (x\mapsto x^2)'    = x \mapsto \lim_{h\to 0} \frac{x^2+2xh+h^2-x^2}{h} = x\mapsto 2x. $$ But how can you extend the definition $f' = \dfrac{f_h-f}{h}$ in such a way that it works with implicit functions/multifunctions? I know that it works, but I don't understand how it works for equations like $y^2 = 4-x^2$.",,"['derivatives', 'implicit-differentiation']"
5,Use a set of data points from a graph to find a derivative,Use a set of data points from a graph to find a derivative,,"I have a data logger that is recording the temperature readings from thermocouples at a specific interval.  This gives me data points that I can graph where the x-coordinate is time and the y-coordinate is temperature.  For each set of data points that I graph, I can connect the points and make a line - usually curved.  I need to find the derivative of each line and graph those as well.  There is no known function that creates these curves, so I can't simply find the derivative of a function.  All I have is a huge list of (x,y) coordinates.  How do I take a derivative and graph it in this case?","I have a data logger that is recording the temperature readings from thermocouples at a specific interval.  This gives me data points that I can graph where the x-coordinate is time and the y-coordinate is temperature.  For each set of data points that I graph, I can connect the points and make a line - usually curved.  I need to find the derivative of each line and graph those as well.  There is no known function that creates these curves, so I can't simply find the derivative of a function.  All I have is a huge list of (x,y) coordinates.  How do I take a derivative and graph it in this case?",,"['derivatives', 'data-analysis']"
6,Derivative of the binomial $\binom x n$ with respect to $x$,Derivative of the binomial  with respect to,\binom x n x,"My background is not mathematics and I need to implement (in C++) the derivative of a binomial, with wxMaxima and wolfram.alpha as a helper. So far, the binomial can be written as: $$\binom x n = \frac 1 {n!}\prod_{k=1}^n (x-k+1)$$ This reduces to a continued convolution. For my specific needs, the binomial needs to be of the form: $$\binom{\frac{n+1}{2}x+\frac{n-1}{2}}{n}$$ But I also need the derivative of it, which wxMaxima solves as $$-\frac{1}{2}(n+1) \,\left( \psi_0\left( \frac{(n+1) x-n+1} 2 \right) -\psi_0 \left( \frac{(n+1) \,(x+1) }{2}\right) \right) \,\begin{pmatrix}\frac{( n+1) x+n-1}{2}\\ n \end{pmatrix}$$ while wolfram goes a bit further and, instead of $\psi_0$ gives $H_n$, which they call harmonic number . ( this link ). That $\psi$ seems to have quite an involved formula, but $H_n$,as functions.wolfram has it, is a simple $\sum_{k=1}^n 1/k$, which is a lot simpler in terms of C++. Now, because I have trust issues, I went on to verify the answer given by wolfram, in wxMaxima, for $n=4$. Here's the code: n:4$ g:diff(binomial((n+1)/2*(x+1)-1,n),x),expand,numer$ h:-(n+1)/2*binomial((n+1)/2*(x+1)-1,n)*(sum(1/k,k,1,(n+1)/2*(x-1))-sum(1/k,k,1,1/2*(n*(x+1)+x-1))); wxplot2d([g,h],[x,0,1]); and here's the output of it: plot As you can see, they don't match; plotting wxMaxima's derivation is a match, but that involves $\psi$ as an infinite sum. So I'm left wondering what's wrong: is the implementation of the harmonic number? Is the derivation formula? Is it the way I transcribed it? TL;DR: I need a derivation formula (not the actual code, that's up to me) for the binomial that is (fairly) simple to implement and doesn't take ages to compute, in C++, as the whole function will be called in a bracketed root-finding algorithm. And I'm also using GMP from gmplib dot org (need 10 rep to post more than 2 links). Following G Cab 's excellent post, and modifying the formulas according to my needs, I managed to come up (with a bit of hammering) to this formula: $$\frac{d}{dx}\binom{\frac{n+1}{2}x+\frac{n-1}{2}}{n}=(-1)^n \frac{n+1}{2} \binom{\frac{n+1}{2}x + \frac{n-1}{2}}{n} \sum_{k=0}^{n-1} \frac{1}{\frac{n+1}{2}x +\frac{n-1}{2}-k}$$ The $(-1)^n$ takes care of odd $n$. Thank you very much everyone that answered.","My background is not mathematics and I need to implement (in C++) the derivative of a binomial, with wxMaxima and wolfram.alpha as a helper. So far, the binomial can be written as: $$\binom x n = \frac 1 {n!}\prod_{k=1}^n (x-k+1)$$ This reduces to a continued convolution. For my specific needs, the binomial needs to be of the form: $$\binom{\frac{n+1}{2}x+\frac{n-1}{2}}{n}$$ But I also need the derivative of it, which wxMaxima solves as $$-\frac{1}{2}(n+1) \,\left( \psi_0\left( \frac{(n+1) x-n+1} 2 \right) -\psi_0 \left( \frac{(n+1) \,(x+1) }{2}\right) \right) \,\begin{pmatrix}\frac{( n+1) x+n-1}{2}\\ n \end{pmatrix}$$ while wolfram goes a bit further and, instead of $\psi_0$ gives $H_n$, which they call harmonic number . ( this link ). That $\psi$ seems to have quite an involved formula, but $H_n$,as functions.wolfram has it, is a simple $\sum_{k=1}^n 1/k$, which is a lot simpler in terms of C++. Now, because I have trust issues, I went on to verify the answer given by wolfram, in wxMaxima, for $n=4$. Here's the code: n:4$ g:diff(binomial((n+1)/2*(x+1)-1,n),x),expand,numer$ h:-(n+1)/2*binomial((n+1)/2*(x+1)-1,n)*(sum(1/k,k,1,(n+1)/2*(x-1))-sum(1/k,k,1,1/2*(n*(x+1)+x-1))); wxplot2d([g,h],[x,0,1]); and here's the output of it: plot As you can see, they don't match; plotting wxMaxima's derivation is a match, but that involves $\psi$ as an infinite sum. So I'm left wondering what's wrong: is the implementation of the harmonic number? Is the derivation formula? Is it the way I transcribed it? TL;DR: I need a derivation formula (not the actual code, that's up to me) for the binomial that is (fairly) simple to implement and doesn't take ages to compute, in C++, as the whole function will be called in a bracketed root-finding algorithm. And I'm also using GMP from gmplib dot org (need 10 rep to post more than 2 links). Following G Cab 's excellent post, and modifying the formulas according to my needs, I managed to come up (with a bit of hammering) to this formula: $$\frac{d}{dx}\binom{\frac{n+1}{2}x+\frac{n-1}{2}}{n}=(-1)^n \frac{n+1}{2} \binom{\frac{n+1}{2}x + \frac{n-1}{2}}{n} \sum_{k=0}^{n-1} \frac{1}{\frac{n+1}{2}x +\frac{n-1}{2}-k}$$ The $(-1)^n$ takes care of odd $n$. Thank you very much everyone that answered.",,"['derivatives', 'binomial-coefficients']"
7,Solution to $\frac{d}{d\frac{1}{x}} x$,Solution to,\frac{d}{d\frac{1}{x}} x,"If I want to solve  $$\frac{d}{d\frac{1}{x}} x$$ is my approach correct? As $$\begin{align*} \frac{d}{d\frac{1}{x}}x&=\\ \text{with }\frac{1}{x}&=y\\ \frac{d}{dy}\frac{1}{y}&=-\frac{1}{y^2}\\ &=-\frac{1}{\left(\frac{1}{x}\right)^2}\\ &=-x^2 \end{align*}$$ Is this approach correct, or did I miss something?","If I want to solve  $$\frac{d}{d\frac{1}{x}} x$$ is my approach correct? As $$\begin{align*} \frac{d}{d\frac{1}{x}}x&=\\ \text{with }\frac{1}{x}&=y\\ \frac{d}{dy}\frac{1}{y}&=-\frac{1}{y^2}\\ &=-\frac{1}{\left(\frac{1}{x}\right)^2}\\ &=-x^2 \end{align*}$$ Is this approach correct, or did I miss something?",,"['derivatives', 'proof-verification']"
8,Lie derivative along time-dependent vector fields,Lie derivative along time-dependent vector fields,,"In ""Lectures on Symplectic Geometry"" by A. C. da Silva ( http://www.math.ist.utl.pt/~acannas/Books/lsg.pdf ) the author gives the following definition: $$ \mathcal{L}_{v_t}\omega := \frac{\mathrm d }{\mathrm d t} (\rho_t)^*\omega\big|_{t=0} $$ where $\rho_t$ satisfies $$ \frac{\mathrm d \rho_t}{\mathrm d t} = v_t\circ\rho_t \qquad \text{and} \qquad \rho_0 = \mathrm{id}. $$ I wonder if this actually makes sense. For time-independent vector fields $v_t=v$ it totally does, but in the time dependent case I have the following objections: The right-hand side of the definition of $\mathcal{L}_{v_t}$ does not use the parameter $t$ . Or is the $t$ in the left-hand side just to denote that we have a time-dependent vector field? But on page 40 the author uses the Cartan formula $$ \mathcal{L}_{v_t}\omega = i_{v_t}\mathrm{d\omega} + \mathrm{d}i_{v_t}\omega $$ where the right-hand side certainly depends on the parameter $t$ . The formula $$ \frac{\mathrm d}{\mathrm dt}\rho_t^*\omega = \rho_t^*\mathcal{L}_{v_t}\omega$$ given on page 36 seems to be wrong when you use the definition of $\mathcal{L}_{v_t}$ given above. For me everything works when I define instead $$ \mathcal{L}_{v_s} \omega:= \frac{\mathrm d }{\mathrm d t} (\rho_{s,t})^*\omega\big|_{t=s} $$ where $\rho_{s,t}$ satisfies $$ \frac{\mathrm d \rho_{s,t}}{\mathrm d t} = v_t\circ\rho_{s,t} \qquad \text{and} \qquad \rho_{s,s} = \mathrm{id}. $$ Does this make sense to you?","In ""Lectures on Symplectic Geometry"" by A. C. da Silva ( http://www.math.ist.utl.pt/~acannas/Books/lsg.pdf ) the author gives the following definition: where satisfies I wonder if this actually makes sense. For time-independent vector fields it totally does, but in the time dependent case I have the following objections: The right-hand side of the definition of does not use the parameter . Or is the in the left-hand side just to denote that we have a time-dependent vector field? But on page 40 the author uses the Cartan formula where the right-hand side certainly depends on the parameter . The formula given on page 36 seems to be wrong when you use the definition of given above. For me everything works when I define instead where satisfies Does this make sense to you?"," \mathcal{L}_{v_t}\omega := \frac{\mathrm d }{\mathrm d t} (\rho_t)^*\omega\big|_{t=0}  \rho_t  \frac{\mathrm d \rho_t}{\mathrm d t} = v_t\circ\rho_t \qquad \text{and} \qquad \rho_0 = \mathrm{id}.  v_t=v \mathcal{L}_{v_t} t t  \mathcal{L}_{v_t}\omega = i_{v_t}\mathrm{d\omega} + \mathrm{d}i_{v_t}\omega  t  \frac{\mathrm d}{\mathrm dt}\rho_t^*\omega = \rho_t^*\mathcal{L}_{v_t}\omega \mathcal{L}_{v_t}  \mathcal{L}_{v_s} \omega:= \frac{\mathrm d }{\mathrm d t} (\rho_{s,t})^*\omega\big|_{t=s}  \rho_{s,t}  \frac{\mathrm d \rho_{s,t}}{\mathrm d t} = v_t\circ\rho_{s,t} \qquad \text{and} \qquad \rho_{s,s} = \mathrm{id}. ","['derivatives', 'definition', 'differential-forms']"
9,notation of differentiation in differential geometry,notation of differentiation in differential geometry,,"I can't wrap my head around notation in differential geometry especially the abundant versions of differentiation. Peter Petersen: Riemannian Geometry defines a lot of notation to be equal but I don't really know when one tends to use which version and how to memorize the definitions and properties/identities. Directional derivative or equivalently the action of a vector field $X$ on a function ($f:M\to\mathbb R$): $X\cdot f=D_Xf=df\cdot X\ $, which is also denoted as $L_Xf$ This is mostly clear except why the notation $D_Xf\ $ exists. $grad(f)=\nabla f\ $ the gradiant of $f:M\to\mathbb R$ Has $\nabla$ something to do with the Levi-Civita connection? Lie derivative of vector fields: $L_XY:=[X,Y]= X\cdot Y - X\cdot Y\ $, where the action of one vector field on one another is given by: $X\cdot Y:=D_XY\ $ the directional derivative of $Y$ along an integral curve of the vector field $X$. Also mostly clear. The covariant derivative or Levi-Civita connection $\nabla_XY$ Here my understanding stops and my brain starts dripping out of my ears… Are there mnemonics or other ways to get into all those ways of thinking about differentiating on manifolds. And why do most books use coordinates - are they necessary I rather like not using $X=\sum_ia^i\partial_i$ for vector fields especially if the author (ab)uses Einstein sum convention.","I can't wrap my head around notation in differential geometry especially the abundant versions of differentiation. Peter Petersen: Riemannian Geometry defines a lot of notation to be equal but I don't really know when one tends to use which version and how to memorize the definitions and properties/identities. Directional derivative or equivalently the action of a vector field $X$ on a function ($f:M\to\mathbb R$): $X\cdot f=D_Xf=df\cdot X\ $, which is also denoted as $L_Xf$ This is mostly clear except why the notation $D_Xf\ $ exists. $grad(f)=\nabla f\ $ the gradiant of $f:M\to\mathbb R$ Has $\nabla$ something to do with the Levi-Civita connection? Lie derivative of vector fields: $L_XY:=[X,Y]= X\cdot Y - X\cdot Y\ $, where the action of one vector field on one another is given by: $X\cdot Y:=D_XY\ $ the directional derivative of $Y$ along an integral curve of the vector field $X$. Also mostly clear. The covariant derivative or Levi-Civita connection $\nabla_XY$ Here my understanding stops and my brain starts dripping out of my ears… Are there mnemonics or other ways to get into all those ways of thinking about differentiating on manifolds. And why do most books use coordinates - are they necessary I rather like not using $X=\sum_ia^i\partial_i$ for vector fields especially if the author (ab)uses Einstein sum convention.",,"['differential-geometry', 'notation', 'derivatives']"
10,"Jacobian of (f,g) is identically zero if and only if f = h ∘ g?","Jacobian of (f,g) is identically zero if and only if f = h ∘ g?",,"Suppose you have smooth functions $f,g : \mathbb{R}^2 \rightarrow \mathbb{R}$. I am wondering whether the following conjecture is true: Conjecture : The Jacobian determinant $\left|\frac{\partial(f,g)}{\partial(u,v)}\right|$ is zero everywhere if and only if there exists a function $h:\mathbb{R}\rightarrow \mathbb{R}$ such that $f \equiv h\circ g$. This direction $(\Leftarrow)$ is easy using the chain rule. I'm wondering whether the converse direction $(\Rightarrow)$ holds as well. I'm not sure how to proceed, but I note that one immediate consequence of the Jacobian being zero everywhere is that the gradients of $f$ and $g$ are parallel everywhere: $$\nabla f = \alpha(u,v) \nabla g.$$ It seems to follow that the level curves of $f$ and $g$ are parallel as well. So, intuitively, to make f and g coincide, maybe it is possible to simply scale $f$ by an amount that depends on the value of $g$ at the point (i.e. the value of the level curve passing through the point). This would mean that there exists a function $h$ such that f = h ∘ g? Update:  I would like to update the conjecture to exclude trivial reasons for the determinant vanishing, e.g. because exactly one of $f$ or $g$ is constant. Later, it may also be advantageous to exclude cases such as $f_v = g_v = 0$. Conjecture (Revised) : Let $f,g:\mathbb{R}^2\rightarrow \mathbb{R}$ be smooth functions, and furthermore suppose that the gradients of $f$ and $g$ exist everywhere. The Jacobian determinant $\left|\frac{\partial(f,g)}{\partial(u,v)}\right|$ is zero everywhere if and only if there exists a function $h:\mathbb{R}\rightarrow \mathbb{R}$ such that $f \equiv h\circ g$. Here is what I've tried so far. Because the partial derivatives of $f$ and $g$ are nonzero everywhere, their gradient is well-defined everywhere and hence at each point the space of vectors $\vec{n}$ such that the partial derivative of $f$ and $g$ in the direction $\vec{n}$ vanishes is one-dimensional. Because the Jacobian determinant vanishes everywhere, the gradients of $f$ and $g$ are parallel, and hence $f$ and $g$ are locally constant in the same direction at each point. This suggests that the level  curves of $f$ and $g$ coincide everywhere; that is, for each point $\vec{p}\in \mathbb{R}^2$, $$f^{-1}(f(\{\vec{p}\})) = g^{-1}(g(\{\vec{p}\}))$$ Hence if you give me the value of $q = g(\vec{p})$, I should be able to find the value $r = f(\vec{p})$ without knowing $\vec{p}$ itself---the level sets coincide. The existence of a function $h: q \mapsto r$ would establish the proof. More formally, let $L_f$ be the collection of level sets of $f$, i.e. $L_f \equiv \{ f^{-1}(f(p)) : p \in \mathbb{R}^2\}$, and let $L_g$ be the level sets of $g$. Evidently, there are maps $C_f : L_f \rightarrow \mathbb{R}$ and $C_g : L_g \rightarrow \mathbb{R}$ sending each level set to its corresponding value in $\mathbb{R}$. What's special is that if we claim the level sets are equal, then there is an isomorphism $s : L_f \leftrightarrow L_g$. In that case, our desired function is $$h \equiv C_f \circ s^{-1}\circ C_g^{-1}$$ This is a correct definition because we have: \begin{align*} p\in \mathbb{R}^2 &\quad \text{a point in }\mathbb{R}^2\\ g(p) &\quad \text{its image under $g$}\\ C_g^{-1}(g(p)) &\quad \text{the level set in $\mathbb{R}^2$ corresponding to $g(p)$}\\ s^{-1}C_g^{-1}(g(p)) &\quad \text{that same level set viewed as a level set of $f$}\\ C_f s^{-1}C_g^{-1}(g(p)) &\quad \text{the value corresponding to that $f$ level set}\\ = f(p) & \quad\text{as demonstrated here}\\ = (h)(g(p)) &\quad\text{definition of $h$}\\ \end{align*} My only remaining question is whether we can confirm that the level sets are all in fact equal with our given assumptions?","Suppose you have smooth functions $f,g : \mathbb{R}^2 \rightarrow \mathbb{R}$. I am wondering whether the following conjecture is true: Conjecture : The Jacobian determinant $\left|\frac{\partial(f,g)}{\partial(u,v)}\right|$ is zero everywhere if and only if there exists a function $h:\mathbb{R}\rightarrow \mathbb{R}$ such that $f \equiv h\circ g$. This direction $(\Leftarrow)$ is easy using the chain rule. I'm wondering whether the converse direction $(\Rightarrow)$ holds as well. I'm not sure how to proceed, but I note that one immediate consequence of the Jacobian being zero everywhere is that the gradients of $f$ and $g$ are parallel everywhere: $$\nabla f = \alpha(u,v) \nabla g.$$ It seems to follow that the level curves of $f$ and $g$ are parallel as well. So, intuitively, to make f and g coincide, maybe it is possible to simply scale $f$ by an amount that depends on the value of $g$ at the point (i.e. the value of the level curve passing through the point). This would mean that there exists a function $h$ such that f = h ∘ g? Update:  I would like to update the conjecture to exclude trivial reasons for the determinant vanishing, e.g. because exactly one of $f$ or $g$ is constant. Later, it may also be advantageous to exclude cases such as $f_v = g_v = 0$. Conjecture (Revised) : Let $f,g:\mathbb{R}^2\rightarrow \mathbb{R}$ be smooth functions, and furthermore suppose that the gradients of $f$ and $g$ exist everywhere. The Jacobian determinant $\left|\frac{\partial(f,g)}{\partial(u,v)}\right|$ is zero everywhere if and only if there exists a function $h:\mathbb{R}\rightarrow \mathbb{R}$ such that $f \equiv h\circ g$. Here is what I've tried so far. Because the partial derivatives of $f$ and $g$ are nonzero everywhere, their gradient is well-defined everywhere and hence at each point the space of vectors $\vec{n}$ such that the partial derivative of $f$ and $g$ in the direction $\vec{n}$ vanishes is one-dimensional. Because the Jacobian determinant vanishes everywhere, the gradients of $f$ and $g$ are parallel, and hence $f$ and $g$ are locally constant in the same direction at each point. This suggests that the level  curves of $f$ and $g$ coincide everywhere; that is, for each point $\vec{p}\in \mathbb{R}^2$, $$f^{-1}(f(\{\vec{p}\})) = g^{-1}(g(\{\vec{p}\}))$$ Hence if you give me the value of $q = g(\vec{p})$, I should be able to find the value $r = f(\vec{p})$ without knowing $\vec{p}$ itself---the level sets coincide. The existence of a function $h: q \mapsto r$ would establish the proof. More formally, let $L_f$ be the collection of level sets of $f$, i.e. $L_f \equiv \{ f^{-1}(f(p)) : p \in \mathbb{R}^2\}$, and let $L_g$ be the level sets of $g$. Evidently, there are maps $C_f : L_f \rightarrow \mathbb{R}$ and $C_g : L_g \rightarrow \mathbb{R}$ sending each level set to its corresponding value in $\mathbb{R}$. What's special is that if we claim the level sets are equal, then there is an isomorphism $s : L_f \leftrightarrow L_g$. In that case, our desired function is $$h \equiv C_f \circ s^{-1}\circ C_g^{-1}$$ This is a correct definition because we have: \begin{align*} p\in \mathbb{R}^2 &\quad \text{a point in }\mathbb{R}^2\\ g(p) &\quad \text{its image under $g$}\\ C_g^{-1}(g(p)) &\quad \text{the level set in $\mathbb{R}^2$ corresponding to $g(p)$}\\ s^{-1}C_g^{-1}(g(p)) &\quad \text{that same level set viewed as a level set of $f$}\\ C_f s^{-1}C_g^{-1}(g(p)) &\quad \text{the value corresponding to that $f$ level set}\\ = f(p) & \quad\text{as demonstrated here}\\ = (h)(g(p)) &\quad\text{definition of $h$}\\ \end{align*} My only remaining question is whether we can confirm that the level sets are all in fact equal with our given assumptions?",,"['derivatives', 'vector-analysis', 'jacobian']"
11,"Intuitive meaning of high order Fréchet derivative $D^k f_p(v_1, \cdots, v_l)$",Intuitive meaning of high order Fréchet derivative,"D^k f_p(v_1, \cdots, v_l)","Let $f:V \to W$ be a map between two Banach spaces $V$ and $W$. Let's denote the $k$-th Fréchet derivative of $f$ at $p$ as $D^kf_p$. Then $D^kf_p(v_1, v_2, \cdots, v_l)$ is a $(k-l)$-linear map from $l$ product of $V$ to $W$. Are there any intuitive meaning of the map? If $l = 1$, $D^kf_p(v)$ is a linear approximation of $D^{k-1}f_{p+v} - D^{k-1}f_{p}$ by definition. If $l = k$ and $v_1 = \cdots = v_k = v$, then it gives the order $k$ term of the Taylor's expansion of $f(p +v)$. However, for other cases, what meaning can we give to the quantity in general?","Let $f:V \to W$ be a map between two Banach spaces $V$ and $W$. Let's denote the $k$-th Fréchet derivative of $f$ at $p$ as $D^kf_p$. Then $D^kf_p(v_1, v_2, \cdots, v_l)$ is a $(k-l)$-linear map from $l$ product of $V$ to $W$. Are there any intuitive meaning of the map? If $l = 1$, $D^kf_p(v)$ is a linear approximation of $D^{k-1}f_{p+v} - D^{k-1}f_{p}$ by definition. If $l = k$ and $v_1 = \cdots = v_k = v$, then it gives the order $k$ term of the Taylor's expansion of $f(p +v)$. However, for other cases, what meaning can we give to the quantity in general?",,"['derivatives', 'banach-spaces', 'frechet-derivative']"
12,A differentiable approximation of modulus?,A differentiable approximation of modulus?,,"I'm trying to find a differentiable approximation of the ""fract"" function, which returns the fractional portion of a real number. $y = x-\lfloor x\rfloor$ I have something that works ""ok"", that I got by adapting a bandlimited saw wave. $y=0.5-\frac{sin(2\pi x)+sin(4\pi x)/2+sin(6\pi x)/3+sin(8\pi x)/4+sin(10\pi x)/5}{\pi}$ I can add more harmonics to make the band limited saw wave closer to the actual ""fract"" function, but for my usage case, all these trig function calls are getting pretty expensive. I was curious, are there other (better quality / lower computational complexity) ways to differentiably approximate this function?","I'm trying to find a differentiable approximation of the ""fract"" function, which returns the fractional portion of a real number. $y = x-\lfloor x\rfloor$ I have something that works ""ok"", that I got by adapting a bandlimited saw wave. $y=0.5-\frac{sin(2\pi x)+sin(4\pi x)/2+sin(6\pi x)/3+sin(8\pi x)/4+sin(10\pi x)/5}{\pi}$ I can add more harmonics to make the band limited saw wave closer to the actual ""fract"" function, but for my usage case, all these trig function calls are getting pretty expensive. I was curious, are there other (better quality / lower computational complexity) ways to differentiably approximate this function?",,"['derivatives', 'approximation']"
13,What's wrong with these equations? [duplicate],What's wrong with these equations? [duplicate],,"This question already has answers here : A proof that $1=2$. May I know why it’s false? [duplicate] (2 answers) Closed 10 years ago . My friend Boris (Boryan) gave me a task, and completely refuses to give the answer what's wrong here. $$x^2=\overbrace{x+\cdots+x}   ^{x\text{ times}}$$ $$(x^2)'=(x+\cdots+x)'$$ $$2x=1+\cdots+1$$ $$2x=x$$ $$2=1$$ Yeah! I've succesfully copypasted latex formulas! I think the problem is in non-formal symbols. It brings me to question, what is the result for $(\sum_{i=1}^{x}x)' = ?$ That's usually an obstacle for those who memorised many things without clear understanding of definitions. So, I'm interested in fundamental mistake of this equations, because I want to get out of this mess)","This question already has answers here : A proof that $1=2$. May I know why it’s false? [duplicate] (2 answers) Closed 10 years ago . My friend Boris (Boryan) gave me a task, and completely refuses to give the answer what's wrong here. $$x^2=\overbrace{x+\cdots+x}   ^{x\text{ times}}$$ $$(x^2)'=(x+\cdots+x)'$$ $$2x=1+\cdots+1$$ $$2x=x$$ $$2=1$$ Yeah! I've succesfully copypasted latex formulas! I think the problem is in non-formal symbols. It brings me to question, what is the result for $(\sum_{i=1}^{x}x)' = ?$ That's usually an obstacle for those who memorised many things without clear understanding of definitions. So, I'm interested in fundamental mistake of this equations, because I want to get out of this mess)",,"['derivatives', 'fake-proofs']"
14,Lipschitz continuity implies differentiability almost everywhere.,Lipschitz continuity implies differentiability almost everywhere.,,"I am running into some troubles with Lipschitz continuous functions. Suppose I have some one-dimensional Lipschitz continuous function $f : \mathbb{R} \to \mathbb{R}$. How do I prove that its derivative exists almost everywhere, with respect to the Lebesgue measure? I found on other places on the internet that any Lipschitz continuous function is absolutely continuous, and that this directly implies that the functions is differentiable almost everywhere. I don't quite see how this argument goes, though. Any help with giving such a proof, or redirecting me to a source where I can find one, would be greatly appreciated.","I am running into some troubles with Lipschitz continuous functions. Suppose I have some one-dimensional Lipschitz continuous function $f : \mathbb{R} \to \mathbb{R}$. How do I prove that its derivative exists almost everywhere, with respect to the Lebesgue measure? I found on other places on the internet that any Lipschitz continuous function is absolutely continuous, and that this directly implies that the functions is differentiable almost everywhere. I don't quite see how this argument goes, though. Any help with giving such a proof, or redirecting me to a source where I can find one, would be greatly appreciated.",,"['derivatives', 'lipschitz-functions', 'almost-everywhere']"
15,Product rule for scalar-vector product,Product rule for scalar-vector product,,"Let $\mathbf F : \mathbb R^p \to \mathbb R^s$ and $\phi : \mathbb R^p \to \mathbb R$ be differentiable functions. Let the function $\mathbf G$ be defined as follows: $$\mathbf G : \mathbb R^p \to \mathbb R^s \qquad  \mathbf G(\mathbf y) = \phi(\mathbf y)\mathbf F(\mathbf y)$$ Furthermore, let $y_0$ be a point in $\mathbb R^p$. Then the Jacobian of $\mathbf G$ and of $\mathbf F$ at $y_0$, denoted respectively $D\mathbf G(y_0)$ and $D\mathbf F(y_0)$ are $s \times p$ matrices, whereas the Jacobian of $\phi$ at $y_0$, denoted $D\phi(y_0)$, is a row vector $p$ entries long and may thus be turned into a gradient: $$\nabla \phi(y_0) \doteq D\phi(y_0)^\top$$ Now the question is, how can I express $D\mathbf G(y_0)$ in terms of the other two Jacobians? I tried recklessly applying the product rule, $$D\mathbf g(y_0) \stackrel{?}{=} \phi(y_0) D\mathbf F(y_0) + D\phi(y_0) \mathbf F(y_0) $$ but the dimensions of the matrices do not match up correctly. What am I doing wrong?","Let $\mathbf F : \mathbb R^p \to \mathbb R^s$ and $\phi : \mathbb R^p \to \mathbb R$ be differentiable functions. Let the function $\mathbf G$ be defined as follows: $$\mathbf G : \mathbb R^p \to \mathbb R^s \qquad  \mathbf G(\mathbf y) = \phi(\mathbf y)\mathbf F(\mathbf y)$$ Furthermore, let $y_0$ be a point in $\mathbb R^p$. Then the Jacobian of $\mathbf G$ and of $\mathbf F$ at $y_0$, denoted respectively $D\mathbf G(y_0)$ and $D\mathbf F(y_0)$ are $s \times p$ matrices, whereas the Jacobian of $\phi$ at $y_0$, denoted $D\phi(y_0)$, is a row vector $p$ entries long and may thus be turned into a gradient: $$\nabla \phi(y_0) \doteq D\phi(y_0)^\top$$ Now the question is, how can I express $D\mathbf G(y_0)$ in terms of the other two Jacobians? I tried recklessly applying the product rule, $$D\mathbf g(y_0) \stackrel{?}{=} \phi(y_0) D\mathbf F(y_0) + D\phi(y_0) \mathbf F(y_0) $$ but the dimensions of the matrices do not match up correctly. What am I doing wrong?",,"['derivatives', 'vector-analysis']"
16,Simplified form for $\frac{\operatorname d^n}{\operatorname dx^n}\left(\frac{x}{e^x-1}\right)$?,Simplified form for ?,\frac{\operatorname d^n}{\operatorname dx^n}\left(\frac{x}{e^x-1}\right),"I have found the following formula: $$\frac{\operatorname  d^n}{\operatorname dx^n}\left(\frac{x}{e^x-1}\right)=(-1)^n\,\frac{n\sum\limits_{k=0}^{n}e^{kx}\sum\limits_{i=0}^{k}(-1)^i\binom{n+1}{i}(k-i)^{n-1}+x\sum\limits_{k=0}^{n}e^{kx}\sum\limits_{i=0}^{k}(-1)^i\binom{n+1}{i}(k-i)^n}{\left(e^x-1\right)^{n+1}}. $$ My proof of this formula is complicated. Can somebody find some simple proof?","I have found the following formula: $$\frac{\operatorname  d^n}{\operatorname dx^n}\left(\frac{x}{e^x-1}\right)=(-1)^n\,\frac{n\sum\limits_{k=0}^{n}e^{kx}\sum\limits_{i=0}^{k}(-1)^i\binom{n+1}{i}(k-i)^{n-1}+x\sum\limits_{k=0}^{n}e^{kx}\sum\limits_{i=0}^{k}(-1)^i\binom{n+1}{i}(k-i)^n}{\left(e^x-1\right)^{n+1}}. $$ My proof of this formula is complicated. Can somebody find some simple proof?",,"['derivatives', 'alternative-proof']"
17,Derivative of $x^{x^{\cdot^{\cdot}}}$?,Derivative of ?,x^{x^{\cdot^{\cdot}}},The infinite tetration is defined as $$f(x)=x^{x^{\cdot^{\cdot}}}$$ This function is defined for $e^{-e} \leq x \leq e^{e-1}$. (Wikipedia image) Can one determine the derivative of this function?,The infinite tetration is defined as $$f(x)=x^{x^{\cdot^{\cdot}}}$$ This function is defined for $e^{-e} \leq x \leq e^{e-1}$. (Wikipedia image) Can one determine the derivative of this function?,,"['derivatives', 'exponentiation', 'tetration']"
18,Question about a function that is a ratio of gamma functions and appears to be strictly increasing for $x\ge 2$,Question about a function that is a ratio of gamma functions and appears to be strictly increasing for,x\ge 2,I was surprised to discover that the following function appears to be strictly increasing for $x \ge 2$ : $$f(x) = \frac{\Gamma(x+1)}{\Gamma(\frac{x}{2}+1)\Gamma(\frac{x}{3}+1)\Gamma(\frac{x}{5}+1)}$$ when I tested out different values of $x$ using Excel. I had assumed that it would be decreasing since $x < \frac{x}{2} + \frac{x}{3} + \frac{x}{5}$ . I wanted to verify this is true by checking the derivative. It seemed to me that the right way to do this is to use this series of the digamma function so that: $$\frac{d}{dx}\left(\ln \Gamma(x+1) - \ln \Gamma(\frac{x}{2}+1) - \ln\Gamma(\frac{x}{3}+1) - \ln\Gamma(\frac{x}{5}+1)\right) =$$ $$ \psi(x+1) - \frac{\psi(\frac{x}{2}+1)}{2} - \frac{\psi(\frac{x}{3}+1)}{3}-\frac{\psi(\frac{x}{5}+1}{5}=$$ $$\frac{\gamma}{6}+ \sum_{k=0}^{\infty}\left(\frac{-1}{30k+30}-\frac{1}{k+x+1} + \frac{1}{2k+x+2} + \frac{1}{3k+x+3} + \frac{1}{5k+x+5}\right)$$ It is not obvious to me that this derivative is greater than $0$ for $x \ge 2$ How would I complete the derivative to reach my conclusion about whether this function is strictly increasing or not for $x \ge 2$ ?,I was surprised to discover that the following function appears to be strictly increasing for : when I tested out different values of using Excel. I had assumed that it would be decreasing since . I wanted to verify this is true by checking the derivative. It seemed to me that the right way to do this is to use this series of the digamma function so that: It is not obvious to me that this derivative is greater than for How would I complete the derivative to reach my conclusion about whether this function is strictly increasing or not for ?,x \ge 2 f(x) = \frac{\Gamma(x+1)}{\Gamma(\frac{x}{2}+1)\Gamma(\frac{x}{3}+1)\Gamma(\frac{x}{5}+1)} x x < \frac{x}{2} + \frac{x}{3} + \frac{x}{5} \frac{d}{dx}\left(\ln \Gamma(x+1) - \ln \Gamma(\frac{x}{2}+1) - \ln\Gamma(\frac{x}{3}+1) - \ln\Gamma(\frac{x}{5}+1)\right) =  \psi(x+1) - \frac{\psi(\frac{x}{2}+1)}{2} - \frac{\psi(\frac{x}{3}+1)}{3}-\frac{\psi(\frac{x}{5}+1}{5}= \frac{\gamma}{6}+ \sum_{k=0}^{\infty}\left(\frac{-1}{30k+30}-\frac{1}{k+x+1} + \frac{1}{2k+x+2} + \frac{1}{3k+x+3} + \frac{1}{5k+x+5}\right) 0 x \ge 2 x \ge 2,"['derivatives', 'gamma-function']"
19,Is $x^{1/3}$ differentiable at $0$?,Is  differentiable at ?,x^{1/3} 0,"It occurred to me that functions that are ""smooth"" but have ""infinite slope"" may not be considered differentiable at the points where their slope is infinite. An easy example of this is $x^{1/3}$, which is ""smooth"" in a visual sense (i.e. there are no jumps in the slope, as in a function like $$ f(x) = \begin{cases} x & x\ge 0 \\ 2x & x< 0 \end{cases} $$ which is continuous at the origin but has different slopes as we approach $0$ in different directions), however at the origin the derivative approaches infinity. So there's a discrepancy, between the intuitive sense of ""differentiable"" and the mathematically rigorous definition. Intuitively I would like to say yes, $x^{1/3}$ is differentiable at $0$, but mathematically I would be forced to say no because by definition the limit of the newton quotient of this function approaches $\infty$, and so does not exist. Another example is the function $(xy)^{1/3}$ on $\mathbb{R}^2$. Is this function differentiable on the $x$ and $y$ axes? (with $(x,y)\neq(0,0)$, where it is actually not differentiable) Is there a notion of differentiability that reconciles this discrepancy?","It occurred to me that functions that are ""smooth"" but have ""infinite slope"" may not be considered differentiable at the points where their slope is infinite. An easy example of this is $x^{1/3}$, which is ""smooth"" in a visual sense (i.e. there are no jumps in the slope, as in a function like $$ f(x) = \begin{cases} x & x\ge 0 \\ 2x & x< 0 \end{cases} $$ which is continuous at the origin but has different slopes as we approach $0$ in different directions), however at the origin the derivative approaches infinity. So there's a discrepancy, between the intuitive sense of ""differentiable"" and the mathematically rigorous definition. Intuitively I would like to say yes, $x^{1/3}$ is differentiable at $0$, but mathematically I would be forced to say no because by definition the limit of the newton quotient of this function approaches $\infty$, and so does not exist. Another example is the function $(xy)^{1/3}$ on $\mathbb{R}^2$. Is this function differentiable on the $x$ and $y$ axes? (with $(x,y)\neq(0,0)$, where it is actually not differentiable) Is there a notion of differentiability that reconciles this discrepancy?",,['derivatives']
20,"An identity on $\small{}_pF_q\left(\left.\begin{array}{c} a_1+1,a_2+1,\dots ,a_p+1\\ b_1+1,b_2+1,\dots ,b_q+1\end{array}\right| z\right)$",An identity on,"\small{}_pF_q\left(\left.\begin{array}{c} a_1+1,a_2+1,\dots ,a_p+1\\ b_1+1,b_2+1,\dots ,b_q+1\end{array}\right| z\right)","I stumbled upon this relation while trying to answer this post . I was trying to find a relation between the two generalized hypergeometric functions, $$A=\,_3F_2\left(\color{blue}{\tfrac12,\tfrac12},\tfrac12;\color{red}{\tfrac32,\tfrac32};\color{fuchsia}{\tfrac12}\right)$$ $$B=\,_3F_2\left(\tfrac32,\tfrac32,\tfrac32;\tfrac52,\tfrac52;\tfrac12\right)$$ It seems, $$A+\tfrac1{18}B = \,_2F_1\left(\tfrac12,\tfrac12;\tfrac32;\tfrac12\right) =\frac{\pi}{2\sqrt2}$$ Note that from a $_3F_2$ , the sum reduces to a $_2F_1$ , and $\tfrac1{18}= \color{blue}{\tfrac12\tfrac12} \color{red}{\tfrac23\tfrac23} \color{fuchsia}{\tfrac12} $ . Question : In general, let $$p=q+1\\c_n = a_n+1\\d_n = b_n+1$$ where $a_n, b_n$ are arbitrary but the last pair must satisty $a_p+1=b_q$ . Is it true that, $$ {}_pF_q\left(\left.\begin{array}{c} a_1,a_2,\dots ,a_p\\ b_1,b_2,\dots ,b_q  \end{array}\right| z\right)+z\,\frac{a_1a_2\dots a_{p-1}}{b_1b_2\dots b_q}{}_pF_q\left(\left.\begin{array}{c} c_1,c_2,\dots ,c_p\\ d_1,d_2,\dots ,d_q  \end{array}\right| z\right)\\={}_{p-1}F_{q-1}\left(\left.\begin{array}{c} a_1,a_2,\dots ,a_{p-1}\\ b_1,b_2,\dots ,b_{q-1}  \end{array}\right| z\right)\\ {} \\ $$ (Note: The pair $a_p,b_q$ disappears in the $\text{RHS}$ .)","I stumbled upon this relation while trying to answer this post . I was trying to find a relation between the two generalized hypergeometric functions, It seems, Note that from a , the sum reduces to a , and . Question : In general, let where are arbitrary but the last pair must satisty . Is it true that, (Note: The pair disappears in the .)","A=\,_3F_2\left(\color{blue}{\tfrac12,\tfrac12},\tfrac12;\color{red}{\tfrac32,\tfrac32};\color{fuchsia}{\tfrac12}\right) B=\,_3F_2\left(\tfrac32,\tfrac32,\tfrac32;\tfrac52,\tfrac52;\tfrac12\right) A+\tfrac1{18}B = \,_2F_1\left(\tfrac12,\tfrac12;\tfrac32;\tfrac12\right) =\frac{\pi}{2\sqrt2} _3F_2 _2F_1 \tfrac1{18}= \color{blue}{\tfrac12\tfrac12} \color{red}{\tfrac23\tfrac23} \color{fuchsia}{\tfrac12}  p=q+1\\c_n = a_n+1\\d_n = b_n+1 a_n, b_n a_p+1=b_q 
{}_pF_q\left(\left.\begin{array}{c} a_1,a_2,\dots ,a_p\\ b_1,b_2,\dots ,b_q  \end{array}\right| z\right)+z\,\frac{a_1a_2\dots a_{p-1}}{b_1b_2\dots b_q}{}_pF_q\left(\left.\begin{array}{c} c_1,c_2,\dots ,c_p\\ d_1,d_2,\dots ,d_q  \end{array}\right| z\right)\\={}_{p-1}F_{q-1}\left(\left.\begin{array}{c} a_1,a_2,\dots ,a_{p-1}\\ b_1,b_2,\dots ,b_{q-1}  \end{array}\right| z\right)\\
{}
\\
 a_p,b_q \text{RHS}","['derivatives', 'hypergeometric-function']"
21,Looking for a closed form solution $P_{n+2}(x)=x^2 P'_{n+1}(x)+(n+1)xP_{n+1}(x)-(n+1)P'_n(x) $,Looking for a closed form solution,P_{n+2}(x)=x^2 P'_{n+1}(x)+(n+1)xP_{n+1}(x)-(n+1)P'_n(x) ,"$$P_{n+2}(x)=x^2 P'_{n+1}(x)+(n+1)xP_{n+1}(x)-(n+1)P'_n(x) \tag{1}$$ where $P_{0}(x)=x$ , $P_{1}(x)=x^2$ are the given initial conditions. I calculated few terms: $$P_{2}(x)=3x^3-1$$ $$P_{3}(x)=3.5x^4-6x$$ $$P_{4}(x)=3.5.7x^5-51x^2$$ $$P_{5}(x)=3.5.7.9x^6-546x^3+24$$ We can estimate the $P_n(x)$ form from a few terms , $P_{n}(x)=a_{n}x^{n+1}+b_{n}x^{n-2}+c_{n}x^{n-5}+d_{n}x^{n-8}+\cdots.$ If we put these terms in Equation 1, We get; $$a_{n+2}x^{n+3}+b_{n+2}x^{n}+c_{n+2}x^{n-3}+....=   a_{n+1}(2n+3)x^{n+3}+[b_{n+1}2n-a_{n}(n+1)^2]x^{n}+[c_{n+1}(2n-3)-b_{n}(n+1)(n-2)]x^{n-3}+...... $$ We can write that $$a_{n+2}=(2n+3)a_{n+1}$$ $$b_{n+2}=2nb_{n+1}-a_{n}(n+1)(n+1)$$ $$c_{n+2}=(2n-3)c_{n+1}-b_{n}(n+1)(n-2)$$ Next term can be gotten as  $$d_{n+2}=(2n-6)d_{n+1}-c_{n}(n+1)(n-5)$$ $a_{n}$ can be expressed as for $n>0$ $a_{n}=1.3.5.7.....(2n-1)=\frac{(2n-1)!}{2^{n-1}(n-1)!}$ I am looking for a closed form $P_n(x)$ or a generating function. Please help me to find a similar Rodrigues formula of Legendre Polynomials for the defined $P_{n}(x)$ above (if it is possible)? Note: Legendre Polynomials, Rodrigues formula is $$\frac{1}{2^nn!}\frac{d^n}{dx^n}(x^2-1)^n$$ Edit: $P_n(x)$ satisfy the relation $$\frac{x^2-y}{1-xy}=\cfrac{\sum_{n=0}^\infty \frac{y^{n}P_{n+1}(x)}{n!}}{\sum_{n=0}^\infty \frac{y^{n}P'_{n}(x)}{n!}} $$ $$\sum_{n=0}^\infty \frac{x^{2n}P_{n+1}(x)}{n!}=0 $$ My atempt to find generating function: $$G(x,t)=\sum_{n=0}^\infty P_{n}(x) t^n =P_0(x)+P_1(x)t+P_2(x)t^2+P_3(x)t^3+..... \tag {2}$$ $$xt\frac {\partial G(x,t)}{\partial t}=xP_1(x)t+2xP_2(x)t^2+3xP_3(x)t^3+.....\tag {3}$$ $$x^2\frac {\partial G(x,t)}{\partial x}=x^2P'_0(x)+x^2P'_1(x)t+x^2P'_2(x)t^2+x^3P'_3(x)t^3+.....\tag {4}$$ $$t\frac {\partial G(x,t)}{\partial x}+t^2\frac {\partial^2 G(x,t)}{\partial x \partial t}=P'_0(x)t+2P'_1(x)t^2+3P'_2(x)t^3+4P'_3(x)t^4+.....\tag {5}$$ $$xt\frac {\partial G(x,t)}{\partial t}+x^2\frac {\partial G(x,t)}{\partial x}-t\frac {\partial G(x,t)}{\partial x}-t^2\frac {\partial^2 G(x,t)}{\partial x \partial t}=x^2P'_0(x)+(x^2P'_1(x)+xP_1(x)-P'_0(x))t+(x^2P'_2(x)+2xP_2(x)-2P'_1(x))t^2+(x^2P'_3(x)+3xP_3(x)-3P'_2(x))t^3+....\tag {6}$$ $$xt\frac {\partial G(x,t)}{\partial t}+x^2\frac {\partial G(x,t)}{\partial x}-t\frac {\partial G(x,t)}{\partial x}-t^2\frac {\partial^2 G(x,t)}{\partial x \partial t}=x^2+P_2(x)t+P_3(x)t^2+P_4(x)t^3+....\tag {7}$$ $$xt^2\frac {\partial G(x,t)}{\partial t}+x^2t\frac {\partial G(x,t)}{\partial x}-t^2\frac {\partial G(x,t)}{\partial x}-t^3\frac {\partial^2 G(x,t)}{\partial x \partial t}=x^2t+P_2(x)t^2+P_3(x)t^3+P_4(x)t^4+....\tag {8}$$ $$xt^2\frac {\partial G(x,t)}{\partial t}+x^2t\frac {\partial G(x,t)}{\partial x}-t^2\frac {\partial G(x,t)}{\partial x}-t^3\frac {\partial^2 G(x,t)}{\partial x \partial t}=x^2t+G(x,t)-x-x^2t\tag {9}$$ $$G(x,t)-xt^2\frac {\partial G(x,t)}{\partial t}+(t^2-x^2t)\frac {\partial G(x,t)}{\partial x}+t^3\frac {\partial^2 G(x,t)}{\partial x \partial t}=x\tag {10}$$ I could not get progress after here. Please help me how to get the generating function after this step. UPDATE (03/23/2018): Thanks a lot for all answers. All answers are very helpful to solve the problem. I would like to write other property of the $P_{n}(x)$. I believe that it will be helpful to find the closed form of $P_n(x)$. $$U(x,y)=\sum_{n=0}^\infty \frac{(xy)^{n}P_{n}(x+y)}{n!} $$ $$\frac {\partial U(x,y)}{\partial x}=\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}+y \sum_{n=0}^\infty \frac{(xy)^{n}P_{n+1}(x+y)}{n!} $$ $$\frac {\partial U(x,y)}{\partial y}=\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}+x \sum_{n=0}^\infty \frac{(xy)^{n}P_{n+1}(x+y)}{n!} $$ We know that $$\frac{x^2-y}{1-xy}=\cfrac{\sum_{n=0}^\infty \frac{y^{n}P_{n+1}(x)}{n!}}{\sum_{n=0}^\infty \frac{y^{n}P'_{n}(x)}{n!}} $$ Thus We can rewrite it as $$\frac{(x+y)^2-xy}{1-xy(x+y)}=\frac{x^2+xy+y^2}{1-xy(x+y)}=\cfrac{\sum_{n=0}^\infty \frac{(xy)^{n}P_{n+1}(x+y)}{n!}}{\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}} $$ $$\frac {\partial U(x,y)}{\partial x}=\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}+y \frac{x^2+xy+y^2}{1-xy(x+y)}\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!} $$ $$\frac {\partial U(x,y)}{\partial y}=\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}+x \frac{x^2+xy+y^2}{1-xy(x+y)}\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}  $$ $$\frac {\partial U(x,y)}{\partial x}/\frac {\partial U(x,y)}{\partial y}=\frac {1-xy(x+y)+y(x^2+xy+y^2)}{1-xy(x+y)+x(x^2+xy+y^2)}$$ $$(1+x^3)\frac {\partial U(x,y)}{\partial x}=(1+y^3)\frac {\partial U(x,y)}{\partial y}$$ $$\int_0^{U(x,y)}  F(z)\mathrm dz=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}+\int_0^{y} \frac{\;\mathrm dz}{1+z^3}$$ If we derivative both side for x $$\frac{\partial U(x,y)}{\partial x} F(U(x,y))=\frac{1}{1+x^3}$$ $$\frac{1}{F(U(x,y))}=(1+x^3)\frac{\partial U(x,y)}{\partial x}$$ If we derivative both side for y $$\int_0^{U(x,y)} F(z)\mathrm dz=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}+\int_0^{y} \frac{\;\mathrm dz}{1+z^3}$$ $$\frac{\partial U(x,y)}{\partial y} F(U(x,y))=\frac{1}{1+y^3}$$ $$\frac{1}{F(U(x,y))}=(1+y^3)\frac{\partial U(x,y)}{\partial y}$$ To find $F(z)$,We can put $y=0$ $$\int_0^{U(x,0)}  F(z)\mathrm dz=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}$$ $$U(x,0)=P_0(x)=x $$ $$\int_0^{x}  F(z)\mathrm dz=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}$$ $$F(z)=\frac{1}{1+z^3}$$ Thus the solution of $U(x,y)$ is  $$\int_0^{U(x,y)} \frac{\;\mathrm dz}{1+z^3}=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}+\int_0^{y} \frac{\;\mathrm dz}{1+z^3}$$ Let's define $g(x)$ , $g(0)=0$; $$x=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}$$ $$y=\int_0^{g(y)} \frac{\;\mathrm dz}{1+z^3}$$ $$x+y=\int_0^{g(x+y)} \frac{\;\mathrm dz}{1+z^3}$$ $$\int_0^{g(x+y)} \frac{\;\mathrm dz}{1+z^3}=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}+\int_0^{g(y)} \frac{\;\mathrm dz}{1+z^3}$$ $$g(x+y)=U(g(x),g(y))$$ $$\int_0^{U(g(x),g(y))} \frac{\;\mathrm dz}{1+z^3}=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}+\int_0^{g(y)} \frac{\;\mathrm dz}{1+z^3}$$ $$x=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}$$ $$g^{-1}(x)=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}$$ $$g'(x)=1+g^3(x)$$ So we can write that  $$g(g^{-1}(x)+g^{-1}(y))=U(x,y)=\sum_{n=0}^\infty \frac{(xy)^{n}P_{n}(x+y)}{n!} $$ Addition formula of $g(x)$ can be written as: $$g(x+y)=\sum_{n=0}^\infty \frac{g^n(x)g^n(y)P_{n}(g(x)+g(y))}{n!} $$ $$\int_0^{g(x+y)} \frac{\;\mathrm dz}{1+z^3}=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}+\int_0^{g(y)} \frac{\;\mathrm dz}{1+z^3}$$ If a closed form of $P_n(x)$ can be found, We can write a closed form addition formula for $g(x)$ like $\tan(x)$","$$P_{n+2}(x)=x^2 P'_{n+1}(x)+(n+1)xP_{n+1}(x)-(n+1)P'_n(x) \tag{1}$$ where $P_{0}(x)=x$ , $P_{1}(x)=x^2$ are the given initial conditions. I calculated few terms: $$P_{2}(x)=3x^3-1$$ $$P_{3}(x)=3.5x^4-6x$$ $$P_{4}(x)=3.5.7x^5-51x^2$$ $$P_{5}(x)=3.5.7.9x^6-546x^3+24$$ We can estimate the $P_n(x)$ form from a few terms , $P_{n}(x)=a_{n}x^{n+1}+b_{n}x^{n-2}+c_{n}x^{n-5}+d_{n}x^{n-8}+\cdots.$ If we put these terms in Equation 1, We get; $$a_{n+2}x^{n+3}+b_{n+2}x^{n}+c_{n+2}x^{n-3}+....=   a_{n+1}(2n+3)x^{n+3}+[b_{n+1}2n-a_{n}(n+1)^2]x^{n}+[c_{n+1}(2n-3)-b_{n}(n+1)(n-2)]x^{n-3}+...... $$ We can write that $$a_{n+2}=(2n+3)a_{n+1}$$ $$b_{n+2}=2nb_{n+1}-a_{n}(n+1)(n+1)$$ $$c_{n+2}=(2n-3)c_{n+1}-b_{n}(n+1)(n-2)$$ Next term can be gotten as  $$d_{n+2}=(2n-6)d_{n+1}-c_{n}(n+1)(n-5)$$ $a_{n}$ can be expressed as for $n>0$ $a_{n}=1.3.5.7.....(2n-1)=\frac{(2n-1)!}{2^{n-1}(n-1)!}$ I am looking for a closed form $P_n(x)$ or a generating function. Please help me to find a similar Rodrigues formula of Legendre Polynomials for the defined $P_{n}(x)$ above (if it is possible)? Note: Legendre Polynomials, Rodrigues formula is $$\frac{1}{2^nn!}\frac{d^n}{dx^n}(x^2-1)^n$$ Edit: $P_n(x)$ satisfy the relation $$\frac{x^2-y}{1-xy}=\cfrac{\sum_{n=0}^\infty \frac{y^{n}P_{n+1}(x)}{n!}}{\sum_{n=0}^\infty \frac{y^{n}P'_{n}(x)}{n!}} $$ $$\sum_{n=0}^\infty \frac{x^{2n}P_{n+1}(x)}{n!}=0 $$ My atempt to find generating function: $$G(x,t)=\sum_{n=0}^\infty P_{n}(x) t^n =P_0(x)+P_1(x)t+P_2(x)t^2+P_3(x)t^3+..... \tag {2}$$ $$xt\frac {\partial G(x,t)}{\partial t}=xP_1(x)t+2xP_2(x)t^2+3xP_3(x)t^3+.....\tag {3}$$ $$x^2\frac {\partial G(x,t)}{\partial x}=x^2P'_0(x)+x^2P'_1(x)t+x^2P'_2(x)t^2+x^3P'_3(x)t^3+.....\tag {4}$$ $$t\frac {\partial G(x,t)}{\partial x}+t^2\frac {\partial^2 G(x,t)}{\partial x \partial t}=P'_0(x)t+2P'_1(x)t^2+3P'_2(x)t^3+4P'_3(x)t^4+.....\tag {5}$$ $$xt\frac {\partial G(x,t)}{\partial t}+x^2\frac {\partial G(x,t)}{\partial x}-t\frac {\partial G(x,t)}{\partial x}-t^2\frac {\partial^2 G(x,t)}{\partial x \partial t}=x^2P'_0(x)+(x^2P'_1(x)+xP_1(x)-P'_0(x))t+(x^2P'_2(x)+2xP_2(x)-2P'_1(x))t^2+(x^2P'_3(x)+3xP_3(x)-3P'_2(x))t^3+....\tag {6}$$ $$xt\frac {\partial G(x,t)}{\partial t}+x^2\frac {\partial G(x,t)}{\partial x}-t\frac {\partial G(x,t)}{\partial x}-t^2\frac {\partial^2 G(x,t)}{\partial x \partial t}=x^2+P_2(x)t+P_3(x)t^2+P_4(x)t^3+....\tag {7}$$ $$xt^2\frac {\partial G(x,t)}{\partial t}+x^2t\frac {\partial G(x,t)}{\partial x}-t^2\frac {\partial G(x,t)}{\partial x}-t^3\frac {\partial^2 G(x,t)}{\partial x \partial t}=x^2t+P_2(x)t^2+P_3(x)t^3+P_4(x)t^4+....\tag {8}$$ $$xt^2\frac {\partial G(x,t)}{\partial t}+x^2t\frac {\partial G(x,t)}{\partial x}-t^2\frac {\partial G(x,t)}{\partial x}-t^3\frac {\partial^2 G(x,t)}{\partial x \partial t}=x^2t+G(x,t)-x-x^2t\tag {9}$$ $$G(x,t)-xt^2\frac {\partial G(x,t)}{\partial t}+(t^2-x^2t)\frac {\partial G(x,t)}{\partial x}+t^3\frac {\partial^2 G(x,t)}{\partial x \partial t}=x\tag {10}$$ I could not get progress after here. Please help me how to get the generating function after this step. UPDATE (03/23/2018): Thanks a lot for all answers. All answers are very helpful to solve the problem. I would like to write other property of the $P_{n}(x)$. I believe that it will be helpful to find the closed form of $P_n(x)$. $$U(x,y)=\sum_{n=0}^\infty \frac{(xy)^{n}P_{n}(x+y)}{n!} $$ $$\frac {\partial U(x,y)}{\partial x}=\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}+y \sum_{n=0}^\infty \frac{(xy)^{n}P_{n+1}(x+y)}{n!} $$ $$\frac {\partial U(x,y)}{\partial y}=\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}+x \sum_{n=0}^\infty \frac{(xy)^{n}P_{n+1}(x+y)}{n!} $$ We know that $$\frac{x^2-y}{1-xy}=\cfrac{\sum_{n=0}^\infty \frac{y^{n}P_{n+1}(x)}{n!}}{\sum_{n=0}^\infty \frac{y^{n}P'_{n}(x)}{n!}} $$ Thus We can rewrite it as $$\frac{(x+y)^2-xy}{1-xy(x+y)}=\frac{x^2+xy+y^2}{1-xy(x+y)}=\cfrac{\sum_{n=0}^\infty \frac{(xy)^{n}P_{n+1}(x+y)}{n!}}{\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}} $$ $$\frac {\partial U(x,y)}{\partial x}=\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}+y \frac{x^2+xy+y^2}{1-xy(x+y)}\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!} $$ $$\frac {\partial U(x,y)}{\partial y}=\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}+x \frac{x^2+xy+y^2}{1-xy(x+y)}\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}  $$ $$\frac {\partial U(x,y)}{\partial x}/\frac {\partial U(x,y)}{\partial y}=\frac {1-xy(x+y)+y(x^2+xy+y^2)}{1-xy(x+y)+x(x^2+xy+y^2)}$$ $$(1+x^3)\frac {\partial U(x,y)}{\partial x}=(1+y^3)\frac {\partial U(x,y)}{\partial y}$$ $$\int_0^{U(x,y)}  F(z)\mathrm dz=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}+\int_0^{y} \frac{\;\mathrm dz}{1+z^3}$$ If we derivative both side for x $$\frac{\partial U(x,y)}{\partial x} F(U(x,y))=\frac{1}{1+x^3}$$ $$\frac{1}{F(U(x,y))}=(1+x^3)\frac{\partial U(x,y)}{\partial x}$$ If we derivative both side for y $$\int_0^{U(x,y)} F(z)\mathrm dz=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}+\int_0^{y} \frac{\;\mathrm dz}{1+z^3}$$ $$\frac{\partial U(x,y)}{\partial y} F(U(x,y))=\frac{1}{1+y^3}$$ $$\frac{1}{F(U(x,y))}=(1+y^3)\frac{\partial U(x,y)}{\partial y}$$ To find $F(z)$,We can put $y=0$ $$\int_0^{U(x,0)}  F(z)\mathrm dz=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}$$ $$U(x,0)=P_0(x)=x $$ $$\int_0^{x}  F(z)\mathrm dz=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}$$ $$F(z)=\frac{1}{1+z^3}$$ Thus the solution of $U(x,y)$ is  $$\int_0^{U(x,y)} \frac{\;\mathrm dz}{1+z^3}=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}+\int_0^{y} \frac{\;\mathrm dz}{1+z^3}$$ Let's define $g(x)$ , $g(0)=0$; $$x=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}$$ $$y=\int_0^{g(y)} \frac{\;\mathrm dz}{1+z^3}$$ $$x+y=\int_0^{g(x+y)} \frac{\;\mathrm dz}{1+z^3}$$ $$\int_0^{g(x+y)} \frac{\;\mathrm dz}{1+z^3}=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}+\int_0^{g(y)} \frac{\;\mathrm dz}{1+z^3}$$ $$g(x+y)=U(g(x),g(y))$$ $$\int_0^{U(g(x),g(y))} \frac{\;\mathrm dz}{1+z^3}=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}+\int_0^{g(y)} \frac{\;\mathrm dz}{1+z^3}$$ $$x=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}$$ $$g^{-1}(x)=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}$$ $$g'(x)=1+g^3(x)$$ So we can write that  $$g(g^{-1}(x)+g^{-1}(y))=U(x,y)=\sum_{n=0}^\infty \frac{(xy)^{n}P_{n}(x+y)}{n!} $$ Addition formula of $g(x)$ can be written as: $$g(x+y)=\sum_{n=0}^\infty \frac{g^n(x)g^n(y)P_{n}(g(x)+g(y))}{n!} $$ $$\int_0^{g(x+y)} \frac{\;\mathrm dz}{1+z^3}=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}+\int_0^{g(y)} \frac{\;\mathrm dz}{1+z^3}$$ If a closed form of $P_n(x)$ can be found, We can write a closed form addition formula for $g(x)$ like $\tan(x)$",,"['derivatives', 'polynomials', 'partial-differential-equations', 'recurrence-relations', 'generating-functions']"
22,Prove that for $(1-x)^m (1+x)^n$ there is no $k$ such that the coefficients of $x^k$ and $x^{k+1}$ are both $0$,Prove that for  there is no  such that the coefficients of  and  are both,(1-x)^m (1+x)^n k x^k x^{k+1} 0,"Prove that for   $$(1-x)^m (1+x)^n$$   there is no $k$ such that the coefficients of $x^k$ and $x^{k+1}$ are both $0$. I thought about this problem several times and can't make even a tentative answer I thought of two ways in breaking through so far thinking of it as $f(x)$ and the coefficient can be given as $f^{(k)}(0)$ (I mean the $k$-th order derivative and $0$ inserted on the derivative function) Then there may be some $k$ so that the $k$-th derivative can be divided by $x^2$. I can't get further from this point. expand the combination function as when $r$ is negative or is above $n$, $C(n,r)=0$ then the coefficient can be written as $$\sum_{i=0}^k (-1)^iC(m,i)C(n,k-i)$$ This is my best.","Prove that for   $$(1-x)^m (1+x)^n$$   there is no $k$ such that the coefficients of $x^k$ and $x^{k+1}$ are both $0$. I thought about this problem several times and can't make even a tentative answer I thought of two ways in breaking through so far thinking of it as $f(x)$ and the coefficient can be given as $f^{(k)}(0)$ (I mean the $k$-th order derivative and $0$ inserted on the derivative function) Then there may be some $k$ so that the $k$-th derivative can be divided by $x^2$. I can't get further from this point. expand the combination function as when $r$ is negative or is above $n$, $C(n,r)=0$ then the coefficient can be written as $$\sum_{i=0}^k (-1)^iC(m,i)C(n,k-i)$$ This is my best.",,"['derivatives', 'binomial-theorem']"
23,Curiosity: Wouldn't the definition of the derivative always be 1 if it exists?,Curiosity: Wouldn't the definition of the derivative always be 1 if it exists?,,"I'm pretty sure I have the wrong intuition here but I have a slight confusion about the way we could calculate the derivative at a certain point using (one of) the definition(s) of the derivative. See example bellow: $$\frac{df(x)}{dx}= \lim_{h\to0}\frac{f(x+h)-f(x)}{h}$$ let's see the case of $f(x) = \sqrt{5x+10}$ $$\frac{df(x)}{dx}=\lim_{h\to0}\frac{\sqrt{5h+5x+10}-\sqrt{5x+10}}{h}$$ If we want to calculate $f'(5)$ $$\left.\frac{df(x)}{dx}\right\rvert_5=\lim_{h\to0}\frac{\sqrt{5h+35}-\sqrt{35}}{h}$$ if we try to find the limits when $h\to0^+$: The numerator would be only slightly superior to 0 The denominator would be only slightly superior to 0 $$\frac{\text{very small number above zero}}        {\text{very small number above zero}}\approx 1$$ It should be the same for $h\to 0^-$ Hence:  $f'(5)= 1$? N.B: I know this result is wrong, I just want to know how the logic I used is faulty.","I'm pretty sure I have the wrong intuition here but I have a slight confusion about the way we could calculate the derivative at a certain point using (one of) the definition(s) of the derivative. See example bellow: $$\frac{df(x)}{dx}= \lim_{h\to0}\frac{f(x+h)-f(x)}{h}$$ let's see the case of $f(x) = \sqrt{5x+10}$ $$\frac{df(x)}{dx}=\lim_{h\to0}\frac{\sqrt{5h+5x+10}-\sqrt{5x+10}}{h}$$ If we want to calculate $f'(5)$ $$\left.\frac{df(x)}{dx}\right\rvert_5=\lim_{h\to0}\frac{\sqrt{5h+35}-\sqrt{35}}{h}$$ if we try to find the limits when $h\to0^+$: The numerator would be only slightly superior to 0 The denominator would be only slightly superior to 0 $$\frac{\text{very small number above zero}}        {\text{very small number above zero}}\approx 1$$ It should be the same for $h\to 0^-$ Hence:  $f'(5)= 1$? N.B: I know this result is wrong, I just want to know how the logic I used is faulty.",,"['derivatives', 'definition']"
24,Implicit differentiation involving a sliding ladder,Implicit differentiation involving a sliding ladder,,"A $5$-foot long ladder is resting on a wall, so that the top of the ladder is 4 feet above the ground and the bottom of the ladder is $3$ feet from the wall. At some time, the ladder is slipping so that the top of the ladder falls at a constant rate of $1 \,\frac{\text{ft}}{\text{min}}$. How fast is the bottom of the ladder slipping away from the wall? I let $y=$ distance along the wall, and $x=$ distance along the ground, so that, $$x^2 + y^2 = 25$$ Therefore, $$2y\frac{dy}{dt}+2x\frac{dx}{dt}=0$$ Initally, $$2(4)(-1)+2(3)\frac{dx}{dt}=0$$ $$\therefore\frac{dx}{dt}=\frac{4}{3}$$ This is all well and good, but my confusion lies in the following analysis: We've established $dy/dt=-1$ and $dx/dt=4/3$, and we know that initially, $$4^2 + 3^2=25$$ Let's say a minute passes, then the vertical distance should decrease by 1 ft, the horizontal distance should increase by $4/3$ ft, and the length of the ladder is always 5 ft. $$(4-1)^2 + (3+4/3)^2 = 27.77...\neq 25$$ I would have thought the ladder would always be 5 ft long, but according to the above, it is now ~5.27 ft long. A second iteration leads to a length of ~6 ft. This ladder seems to be growing in length, which does not make sense. What is going on?","A $5$-foot long ladder is resting on a wall, so that the top of the ladder is 4 feet above the ground and the bottom of the ladder is $3$ feet from the wall. At some time, the ladder is slipping so that the top of the ladder falls at a constant rate of $1 \,\frac{\text{ft}}{\text{min}}$. How fast is the bottom of the ladder slipping away from the wall? I let $y=$ distance along the wall, and $x=$ distance along the ground, so that, $$x^2 + y^2 = 25$$ Therefore, $$2y\frac{dy}{dt}+2x\frac{dx}{dt}=0$$ Initally, $$2(4)(-1)+2(3)\frac{dx}{dt}=0$$ $$\therefore\frac{dx}{dt}=\frac{4}{3}$$ This is all well and good, but my confusion lies in the following analysis: We've established $dy/dt=-1$ and $dx/dt=4/3$, and we know that initially, $$4^2 + 3^2=25$$ Let's say a minute passes, then the vertical distance should decrease by 1 ft, the horizontal distance should increase by $4/3$ ft, and the length of the ladder is always 5 ft. $$(4-1)^2 + (3+4/3)^2 = 27.77...\neq 25$$ I would have thought the ladder would always be 5 ft long, but according to the above, it is now ~5.27 ft long. A second iteration leads to a length of ~6 ft. This ladder seems to be growing in length, which does not make sense. What is going on?",,"['derivatives', 'implicit-differentiation']"
25,"Derivatives 101: what does ""with respect to"" mean?","Derivatives 101: what does ""with respect to"" mean?",,"I'm studying derivatives 101 and I can't get my head around the phrasing ""with respect to"" something. Eg in chain rule we calculate the derivative of outer function with respect to inner + derivative of inner with respect to x. But what does it actually mean (in human language) to say a derivative is ""with respect to"" anything at all? Thanks a ton.","I'm studying derivatives 101 and I can't get my head around the phrasing ""with respect to"" something. Eg in chain rule we calculate the derivative of outer function with respect to inner + derivative of inner with respect to x. But what does it actually mean (in human language) to say a derivative is ""with respect to"" anything at all? Thanks a ton.",,['derivatives']
26,Left Hand Derivative Definition,Left Hand Derivative Definition,,What is the actual definition of Left Hand Derivative? I bumped into this site and the second white box on their site gives the definition. Is that wrong? What is the correct one then?,What is the actual definition of Left Hand Derivative? I bumped into this site and the second white box on their site gives the definition. Is that wrong? What is the correct one then?,,"['derivatives', 'definition']"
27,Proving convexity of a function whose Hessian is positive semidefinite over a convex set,Proving convexity of a function whose Hessian is positive semidefinite over a convex set,,"Let $C$ be a convex set in $\mathbb{R}^n$ and let $f:{\mathbb{R}}^n \rightarrow \mathbb{R}$ be twice continuously differentiable over $C$ . The Hessian of $f$ is positive semidefinite over $C$ , and I want to show that $f$ is therefore a convex function. I am currently trying to apply Taylor's Theorem to replace $f(x)$ with an expression that includes its Hessian.","Let be a convex set in and let be twice continuously differentiable over . The Hessian of is positive semidefinite over , and I want to show that is therefore a convex function. I am currently trying to apply Taylor's Theorem to replace with an expression that includes its Hessian.",C \mathbb{R}^n f:{\mathbb{R}}^n \rightarrow \mathbb{R} C f C f f(x),"['derivatives', 'optimization', 'continuity', 'convex-optimization']"
28,What is the derivative of $x!^{x!^{x!^{x!^{x!^{x!^{x!^{.{^{.^{.}}}}}}}}}}$,What is the derivative of,x!^{x!^{x!^{x!^{x!^{x!^{x!^{.{^{.^{.}}}}}}}}}},"What is the derivative of $$x!^{x!^{x!^{x!^{x!^{x!^{x!^{.{^{.^{.}}}}}}}}}}$$ My effort: Let $$g(x)=x!^{x!^{x!^{x!^{x!^{x!^{x!^{.{^{.^{.}}}}}}}}}}\implies g(x)=x!^{g(x)}$$ Taking natrual log on both sides, $$\ln(g(x))=g(x)\cdot\ln(x!)$$ Differentiating, $$\frac{1}{g(x)}\cdot g'(x)=g'(x)\cdot\ln(x!)+g(x)\cdot\frac{1}{x!}\cdot x!\cdot\psi^{(0)}(x+1)$$ $$\implies g'(x)\left[\frac{1}{g(x)}-ln(x!)\right]=g(x)\cdot\psi^{(0)}(x+1)$$ So does isolating $g'(x)$ give me the correct solution? If not, how can I solve for the differential? Edit: The gamma function is indeed implicitly assumed when the factorial function is used.","What is the derivative of $$x!^{x!^{x!^{x!^{x!^{x!^{x!^{.{^{.^{.}}}}}}}}}}$$ My effort: Let $$g(x)=x!^{x!^{x!^{x!^{x!^{x!^{x!^{.{^{.^{.}}}}}}}}}}\implies g(x)=x!^{g(x)}$$ Taking natrual log on both sides, $$\ln(g(x))=g(x)\cdot\ln(x!)$$ Differentiating, $$\frac{1}{g(x)}\cdot g'(x)=g'(x)\cdot\ln(x!)+g(x)\cdot\frac{1}{x!}\cdot x!\cdot\psi^{(0)}(x+1)$$ $$\implies g'(x)\left[\frac{1}{g(x)}-ln(x!)\right]=g(x)\cdot\psi^{(0)}(x+1)$$ So does isolating $g'(x)$ give me the correct solution? If not, how can I solve for the differential? Edit: The gamma function is indeed implicitly assumed when the factorial function is used.",,"['derivatives', 'power-towers']"
29,Why do we need a Lie derivative of a vector field?,Why do we need a Lie derivative of a vector field?,,"Lie derivative of a smooth vector field $Y$ in the direction of a smooth vector field $X$ is defined (at least in our geometry course) as $L_X Y = \frac{d}{dt}\mid_{t=0} (\psi_\star Y)$ where $({\psi_t}_{t \in I})$ is the local flow of $X$ and $\psi_\star$ is the pushforward of $\psi$ , which is defined as the differential $d\psi$ . To my understanding, the definition says that we take the ""small change"" (derivative) of the local flow corresponding to ""small change"" in $Y$ (this is my interpretation of the $(\psi_\star Y)$ . And that we make one more derivative of this, that is, one ""small change"", this time corresponding to change in $t$ (this part is the $\frac{d}{dt}$ .) My question is: Do I understand this correctly? Or what is a better intuition behind this? And also, why do we even need to see how the $L_X Y$ look? What does it tell us to know how one vector field changes and it is somehow connected to changing the other vector field? I don´t understand why this is useful. Thank you very much!","Lie derivative of a smooth vector field in the direction of a smooth vector field is defined (at least in our geometry course) as where is the local flow of and is the pushforward of , which is defined as the differential . To my understanding, the definition says that we take the ""small change"" (derivative) of the local flow corresponding to ""small change"" in (this is my interpretation of the . And that we make one more derivative of this, that is, one ""small change"", this time corresponding to change in (this part is the .) My question is: Do I understand this correctly? Or what is a better intuition behind this? And also, why do we even need to see how the look? What does it tell us to know how one vector field changes and it is somehow connected to changing the other vector field? I don´t understand why this is useful. Thank you very much!",Y X L_X Y = \frac{d}{dt}\mid_{t=0} (\psi_\star Y) ({\psi_t}_{t \in I}) X \psi_\star \psi d\psi Y (\psi_\star Y) t \frac{d}{dt} L_X Y,"['derivatives', 'riemannian-geometry', 'vector-fields', 'differential', 'lie-derivative']"
30,What is the average?,What is the average?,,"When I was first introduced to the concept of average (mean), I was confused. What does average mean? How does one number $\sum_{i=1}^{n} a_{i}$ represent the ""central tendency"" of a set of data points $a_i$. Then I found a way to deal with this concept. I thought that the average (mean) is ""the closest to all the data points at the same time"". Now, I want to prove this. Concisely: Let $f$ $:$ $\Bbb{R}$$\to$$\Bbb{R}$ be defined by: $$f(x) = \sum_{i=1}^{n}|x-a_i|$$ To prove: $f(x)$ hits a minimum at $x=\bar a$, where $\bar a$ is the mean of the discrete data points $a_i$. This would mean that the sum of the distances of the mean from the various data points is minimum as compared to any other number. My attempt: Clearly $f(x)$ is continuous since it is a sum of continuous functions and piece-wise differentiable since it is a sum of such functions. So, I find $f'(x)$. Before that, let's assume $a_1<a_2<\ldots<a_n$ [clearly, no loss of generality, here]: $$ f'(x) =  \begin{cases}  -n, & \text {$x<a_1$} \\  -n+2, & \text{$a_1<x<a_2$} \\ -n+4, & \text{$a_2<x<a_3$} \\ . & . \\ . & . \\ . & . \\ -n+2n = n, & \text{$x>a_n$} \\  \end{cases} $$ Now, the problem arises: $f'(x)=0$ has no solutions for $n$ is odd. For n is even, it has the solution as an entire interval: $$x \in (a_{n/2},a_{n/2+1})$$ This means i failed, my intuition was wrong from the very beginning. It can be proven [i think] that for $n$ is even, $\bar a$ lies in the above interval, but still: It means that there are more real numbers that are as much the ""mean"" of the data points as the mean itself [if my ""definition"" was right]. So, two questions: Why was my intuition wrong? Which intuition is right for averages (mean)?","When I was first introduced to the concept of average (mean), I was confused. What does average mean? How does one number $\sum_{i=1}^{n} a_{i}$ represent the ""central tendency"" of a set of data points $a_i$. Then I found a way to deal with this concept. I thought that the average (mean) is ""the closest to all the data points at the same time"". Now, I want to prove this. Concisely: Let $f$ $:$ $\Bbb{R}$$\to$$\Bbb{R}$ be defined by: $$f(x) = \sum_{i=1}^{n}|x-a_i|$$ To prove: $f(x)$ hits a minimum at $x=\bar a$, where $\bar a$ is the mean of the discrete data points $a_i$. This would mean that the sum of the distances of the mean from the various data points is minimum as compared to any other number. My attempt: Clearly $f(x)$ is continuous since it is a sum of continuous functions and piece-wise differentiable since it is a sum of such functions. So, I find $f'(x)$. Before that, let's assume $a_1<a_2<\ldots<a_n$ [clearly, no loss of generality, here]: $$ f'(x) =  \begin{cases}  -n, & \text {$x<a_1$} \\  -n+2, & \text{$a_1<x<a_2$} \\ -n+4, & \text{$a_2<x<a_3$} \\ . & . \\ . & . \\ . & . \\ -n+2n = n, & \text{$x>a_n$} \\  \end{cases} $$ Now, the problem arises: $f'(x)=0$ has no solutions for $n$ is odd. For n is even, it has the solution as an entire interval: $$x \in (a_{n/2},a_{n/2+1})$$ This means i failed, my intuition was wrong from the very beginning. It can be proven [i think] that for $n$ is even, $\bar a$ lies in the above interval, but still: It means that there are more real numbers that are as much the ""mean"" of the data points as the mean itself [if my ""definition"" was right]. So, two questions: Why was my intuition wrong? Which intuition is right for averages (mean)?",,"['derivatives', 'summation', 'average', 'means']"
31,We can define the derivative of a function whose domain is a subset of rational numbers?,We can define the derivative of a function whose domain is a subset of rational numbers?,,"Usually the derivative is defined for a function $f:A\to \mathbb{R}$ where $A \subset \mathbb{R}$, and the usual definition of the derivative at a point $a$ require the existence of an open neighborhood of $a$ where the function is defined. So, if $A\subset \mathbb{Q}$ it seems that we cannot define a derivative, since $A$ is totally disconnected. But the definition $$ f'(a)=\lim_{h \to 0}\dfrac{f(a+h)-f(a)}{h} $$ require only the existence of the limit that, with the $\epsilon -\delta$ definition, can be found using only rational values of $h$. So it seams that a ''derivative'' can be defined. Or there is something that does not works? This question is suggested by Clarification if a disconnected function has a derivative at defined points. , where the OP asks for the derivability of the function $$ f:\{x=\dfrac{n}{2k+1} | n,k \in \mathbb{Z}\} \to \mathbb{R} \quad;\quad f(x)=(-2)^x $$","Usually the derivative is defined for a function $f:A\to \mathbb{R}$ where $A \subset \mathbb{R}$, and the usual definition of the derivative at a point $a$ require the existence of an open neighborhood of $a$ where the function is defined. So, if $A\subset \mathbb{Q}$ it seems that we cannot define a derivative, since $A$ is totally disconnected. But the definition $$ f'(a)=\lim_{h \to 0}\dfrac{f(a+h)-f(a)}{h} $$ require only the existence of the limit that, with the $\epsilon -\delta$ definition, can be found using only rational values of $h$. So it seams that a ''derivative'' can be defined. Or there is something that does not works? This question is suggested by Clarification if a disconnected function has a derivative at defined points. , where the OP asks for the derivability of the function $$ f:\{x=\dfrac{n}{2k+1} | n,k \in \mathbb{Z}\} \to \mathbb{R} \quad;\quad f(x)=(-2)^x $$",,"['derivatives', 'definition']"
32,Derivable doesn't exist in english?,Derivable doesn't exist in english?,,"I have a question about terminology. See this is what happens: someone says ""this function is derivable"", and then another, more experienced Anglo-Saxon mathematician goes on to correct this someone, saying that the term is ""differentiable"". Or at least in my experience. However, in Spanish (and other romance languages I believe), the situation would be: ""this function is derivable"", then the more experienced, romantic-language mathematician goes ""yes, but that doesn't make it differentiable"". This is because, in Spanish, the term derivable means just that, the directional derivatives all exist, they just may not be a linear map of the direction. For example, in 1D ""derivable"" and ""differentiable"" would be the same thing (in fact this is a common stress point in lessons, how ""it's not the same in higher dimensions!""). Furthermore, in 1-var. calculus I was urged to use the term ""derivable"", as opposed to ""differentiable"", so as to not gain a false understanding of the meaning of differentiability. Call it a trifle, but I've really been wondering about this for my whole math education (and other things about real math, not to worry). So I ask if there is an analogue to ""derivable"" in English or if you have to work up a long phrase every time.","I have a question about terminology. See this is what happens: someone says ""this function is derivable"", and then another, more experienced Anglo-Saxon mathematician goes on to correct this someone, saying that the term is ""differentiable"". Or at least in my experience. However, in Spanish (and other romance languages I believe), the situation would be: ""this function is derivable"", then the more experienced, romantic-language mathematician goes ""yes, but that doesn't make it differentiable"". This is because, in Spanish, the term derivable means just that, the directional derivatives all exist, they just may not be a linear map of the direction. For example, in 1D ""derivable"" and ""differentiable"" would be the same thing (in fact this is a common stress point in lessons, how ""it's not the same in higher dimensions!""). Furthermore, in 1-var. calculus I was urged to use the term ""derivable"", as opposed to ""differentiable"", so as to not gain a false understanding of the meaning of differentiability. Call it a trifle, but I've really been wondering about this for my whole math education (and other things about real math, not to worry). So I ask if there is an analogue to ""derivable"" in English or if you have to work up a long phrase every time.",,"['derivatives', 'terminology']"
33,How is the differential of a function on a manifold defined if $f(x)$ and $f(x+h)$ cannot be compared?,How is the differential of a function on a manifold defined if  and  cannot be compared?,f(x) f(x+h),"I'm self-teaching differential geometry, and a friend perplexed me saying that it's not correct to take a derivative of a function $M \rightarrow \mathbb R$ on a manifold $M$ because the two terms of the difference quotient (say, $f(x)$ and $f(x+h)$ ) lay in different spaces, which just in ordinary calculus are implicitly identified to the same space (the $y$ -axis). This seemed weird to me because the concept of directional derivative of a function applies regularly to manifolds too...but what if also in this case there's an implicit identification at work?","I'm self-teaching differential geometry, and a friend perplexed me saying that it's not correct to take a derivative of a function on a manifold because the two terms of the difference quotient (say, and ) lay in different spaces, which just in ordinary calculus are implicitly identified to the same space (the -axis). This seemed weird to me because the concept of directional derivative of a function applies regularly to manifolds too...but what if also in this case there's an implicit identification at work?",M \rightarrow \mathbb R M f(x) f(x+h) y,"['differential-geometry', 'derivatives', 'smooth-manifolds']"
34,Do different methods of calculating fractional derivatives have to be equal?,Do different methods of calculating fractional derivatives have to be equal?,,"Do different methods of calculating fractional derivatives have to be equal?  Or do they sometimes end up differently? An example would be nice, and if possible, an explanation as too why such formulas can disagree with one another would be exceptional. The main reason behind this is because I noted that if we could take the fractional derivative of a function through its Taylor series, this would imply that $\frac{d^q}{dx^q}e^x-\frac{d^{q+1}}{dx^{q+1}}e^x={x^{-q-1}\over\Gamma(-q)}$, which tends to $0$ as $q$ tends to become a whole number, but still, this goes against what I would expect. And of course, explanation on the role of constants of integration would be nice.","Do different methods of calculating fractional derivatives have to be equal?  Or do they sometimes end up differently? An example would be nice, and if possible, an explanation as too why such formulas can disagree with one another would be exceptional. The main reason behind this is because I noted that if we could take the fractional derivative of a function through its Taylor series, this would imply that $\frac{d^q}{dx^q}e^x-\frac{d^{q+1}}{dx^{q+1}}e^x={x^{-q-1}\over\Gamma(-q)}$, which tends to $0$ as $q$ tends to become a whole number, but still, this goes against what I would expect. And of course, explanation on the role of constants of integration would be nice.",,"['derivatives', 'fractional-calculus']"
35,Del operator in Cylindrical coordinates (problem in partial differentiation),Del operator in Cylindrical coordinates (problem in partial differentiation),,"I am currently reviewing basic vector analysis and trying to understand every single detail, however, I got stuck in some derivation. What I want to show is the following: Given the del operator (i.e., vector differential operator) in   Cartesian coordinates $(x,y,z)$ $$\nabla=\frac{\partial }{\partial x}\mathbf{a}_x+\frac{\partial  }{\partial y}\mathbf{a}_y+\frac{\partial }{\partial z}\mathbf{a}_z$$ show that the corrseponding operator in Cylindrical coordinates   $(\rho, \phi ,z)$ is given by$$\nabla=\frac{\partial }{\partial\rho}\mathbf{a}_\rho+\frac{1}{\rho}\frac{\partial }{\partial  \phi}\mathbf{a}_\phi+\frac{\partial }{\partial z}\mathbf{a}_z$$ I tried one approach. However, for curiosity I tried a different method but I couldn't get it right. Approach #1: From the point-to-point transformation $$\rho=\sqrt{x^2+y^2}, \; \phi=\text{tan}\frac{y}{x}$$ partial differentiation with respect to $x$ and $y$ yields \begin{align} \frac{\partial \rho}{\partial x} &=\frac{x}{\sqrt{x^2+y^2}}=\frac{\rho \, \text{cos}\phi}{\rho}=\text{cos}\phi \\                 \frac{\partial \rho}{\partial y}&=\frac{y}{\sqrt{x^2+y^2}}=\frac{\rho \, \text{sin}\phi}{\rho}=\text{sin}\phi  \end{align} and \begin{align} \frac{\partial \phi}{\partial x}&=\frac{-y}{x^2}\frac{1}{1+(\frac{y}{x})^2}=\frac{-y}{x^2+y^2}=\frac{-\rho \, \text{sin}\phi}{\rho^2}=\frac{-\text{sin}\phi}{\rho} \\                 \frac{\partial \phi}{\partial y}&=\frac{1}{x}\frac{1}{1+(\frac{y}{x})^2}=\frac{x}{x^2+y^2}=\frac{\rho \, \text{cos}\phi}{\rho^2}=\frac{\text{cos}\phi}{\rho} \end{align} Now, plugging these in the chain rule differentiation formulas  \begin{align} \frac{\partial }{\partial x}&=\frac{\partial }{\partial \rho}\;\frac{\partial \rho}{\partial x}+\frac{\partial }{\partial \phi}\;\frac{\partial \phi}{\partial x} \\ \frac{\partial }{\partial y}&=\frac{\partial }{\partial \rho}\;\frac{\partial \rho}{\partial y}+\frac{\partial }{\partial \phi}\;\frac{\partial \phi}{\partial y} \end{align} and making use of the unit vector transformation from Cartesian to Cylindrical  \begin{align} \mathbf{a}_x&=\text{cos}\phi\;\mathbf{a}_\rho-\text{sin}\phi\;\mathbf{a}_\phi\\ \mathbf{a}_y&=\text{sin}\phi\;\mathbf{a}_\rho+\text{cos}\phi\;\mathbf{a}_\phi \end{align} We get  \begin{align} \nabla&=\frac{\partial }{\partial x}\mathbf{a}_x+\frac{\partial  }{\partial y}\mathbf{a}_y+\frac{\partial }{\partial z}\mathbf{a}_z  \\                &=\left (\frac{\partial }{\partial \rho}\;\frac{\partial \rho}{\partial x}+\frac{\partial }{\partial \phi}\;\frac{\partial \phi}{\partial x}  \right )\left ( \text{cos}\phi\;\mathbf{a}_\rho-\text{sin}\phi\;\mathbf{a}_\phi \right )\\ &+\left ( \frac{\partial }{\partial \rho}\;\frac{\partial \rho}{\partial y}+\frac{\partial }{\partial \phi}\;\frac{\partial \phi}{\partial y} \right )\left ( \text{sin}\phi\;\mathbf{a}_\rho+\text{cos}\phi\;\mathbf{a}_\phi \right )+\frac{\partial }{\partial z}\mathbf{a}_z  \\                &=\left (\frac{\partial }{\partial \rho}\;\text{cos}\phi+\frac{\partial }{\partial \phi}\;\frac{-\text{sin}\phi}{\rho}  \right )\left ( \text{cos}\phi\;\mathbf{a}_\rho-\text{sin}\phi\;\mathbf{a}_\phi \right )\\ &+\left ( \frac{\partial }{\partial \rho}\;\text{sin}\phi+\frac{\partial }{\partial \phi}\;\frac{\text{cos}\phi}{\rho} \right )\left ( \text{sin}\phi\;\mathbf{a}_\rho+\text{cos}\phi\;\mathbf{a}_\phi \right )+\frac{\partial }{\partial z}\mathbf{a}_z\\ &=\left ( \text{sin}^2\phi+\text{cos}^2\phi \right )\frac{\partial }{\partial \rho}\mathbf{a}_\rho+\frac{1}{\rho}\left ( \text{sin}^2\phi+\text{cos}^2\phi \right )\frac{\partial }{\partial \phi}\mathbf{a}_\phi+\frac{\partial }{\partial z}\mathbf{a}_z\\ &=\frac{\partial }{\partial\rho}\mathbf{a}_\rho+\frac{1}{\rho}\frac{\partial }{\partial  \phi}\mathbf{a}_\phi+\frac{\partial }{\partial z}\mathbf{a}_z  \end{align} which is the desired result. Approach #2: How can I get the same result starting from the point-to-point transformation $$x=\rho \, \text{cos}\phi,\; y=\rho \, \text{sin}\phi$$ by using partial differentiation? Maybe implicit differentiation?","I am currently reviewing basic vector analysis and trying to understand every single detail, however, I got stuck in some derivation. What I want to show is the following: Given the del operator (i.e., vector differential operator) in   Cartesian coordinates $(x,y,z)$ $$\nabla=\frac{\partial }{\partial x}\mathbf{a}_x+\frac{\partial  }{\partial y}\mathbf{a}_y+\frac{\partial }{\partial z}\mathbf{a}_z$$ show that the corrseponding operator in Cylindrical coordinates   $(\rho, \phi ,z)$ is given by$$\nabla=\frac{\partial }{\partial\rho}\mathbf{a}_\rho+\frac{1}{\rho}\frac{\partial }{\partial  \phi}\mathbf{a}_\phi+\frac{\partial }{\partial z}\mathbf{a}_z$$ I tried one approach. However, for curiosity I tried a different method but I couldn't get it right. Approach #1: From the point-to-point transformation $$\rho=\sqrt{x^2+y^2}, \; \phi=\text{tan}\frac{y}{x}$$ partial differentiation with respect to $x$ and $y$ yields \begin{align} \frac{\partial \rho}{\partial x} &=\frac{x}{\sqrt{x^2+y^2}}=\frac{\rho \, \text{cos}\phi}{\rho}=\text{cos}\phi \\                 \frac{\partial \rho}{\partial y}&=\frac{y}{\sqrt{x^2+y^2}}=\frac{\rho \, \text{sin}\phi}{\rho}=\text{sin}\phi  \end{align} and \begin{align} \frac{\partial \phi}{\partial x}&=\frac{-y}{x^2}\frac{1}{1+(\frac{y}{x})^2}=\frac{-y}{x^2+y^2}=\frac{-\rho \, \text{sin}\phi}{\rho^2}=\frac{-\text{sin}\phi}{\rho} \\                 \frac{\partial \phi}{\partial y}&=\frac{1}{x}\frac{1}{1+(\frac{y}{x})^2}=\frac{x}{x^2+y^2}=\frac{\rho \, \text{cos}\phi}{\rho^2}=\frac{\text{cos}\phi}{\rho} \end{align} Now, plugging these in the chain rule differentiation formulas  \begin{align} \frac{\partial }{\partial x}&=\frac{\partial }{\partial \rho}\;\frac{\partial \rho}{\partial x}+\frac{\partial }{\partial \phi}\;\frac{\partial \phi}{\partial x} \\ \frac{\partial }{\partial y}&=\frac{\partial }{\partial \rho}\;\frac{\partial \rho}{\partial y}+\frac{\partial }{\partial \phi}\;\frac{\partial \phi}{\partial y} \end{align} and making use of the unit vector transformation from Cartesian to Cylindrical  \begin{align} \mathbf{a}_x&=\text{cos}\phi\;\mathbf{a}_\rho-\text{sin}\phi\;\mathbf{a}_\phi\\ \mathbf{a}_y&=\text{sin}\phi\;\mathbf{a}_\rho+\text{cos}\phi\;\mathbf{a}_\phi \end{align} We get  \begin{align} \nabla&=\frac{\partial }{\partial x}\mathbf{a}_x+\frac{\partial  }{\partial y}\mathbf{a}_y+\frac{\partial }{\partial z}\mathbf{a}_z  \\                &=\left (\frac{\partial }{\partial \rho}\;\frac{\partial \rho}{\partial x}+\frac{\partial }{\partial \phi}\;\frac{\partial \phi}{\partial x}  \right )\left ( \text{cos}\phi\;\mathbf{a}_\rho-\text{sin}\phi\;\mathbf{a}_\phi \right )\\ &+\left ( \frac{\partial }{\partial \rho}\;\frac{\partial \rho}{\partial y}+\frac{\partial }{\partial \phi}\;\frac{\partial \phi}{\partial y} \right )\left ( \text{sin}\phi\;\mathbf{a}_\rho+\text{cos}\phi\;\mathbf{a}_\phi \right )+\frac{\partial }{\partial z}\mathbf{a}_z  \\                &=\left (\frac{\partial }{\partial \rho}\;\text{cos}\phi+\frac{\partial }{\partial \phi}\;\frac{-\text{sin}\phi}{\rho}  \right )\left ( \text{cos}\phi\;\mathbf{a}_\rho-\text{sin}\phi\;\mathbf{a}_\phi \right )\\ &+\left ( \frac{\partial }{\partial \rho}\;\text{sin}\phi+\frac{\partial }{\partial \phi}\;\frac{\text{cos}\phi}{\rho} \right )\left ( \text{sin}\phi\;\mathbf{a}_\rho+\text{cos}\phi\;\mathbf{a}_\phi \right )+\frac{\partial }{\partial z}\mathbf{a}_z\\ &=\left ( \text{sin}^2\phi+\text{cos}^2\phi \right )\frac{\partial }{\partial \rho}\mathbf{a}_\rho+\frac{1}{\rho}\left ( \text{sin}^2\phi+\text{cos}^2\phi \right )\frac{\partial }{\partial \phi}\mathbf{a}_\phi+\frac{\partial }{\partial z}\mathbf{a}_z\\ &=\frac{\partial }{\partial\rho}\mathbf{a}_\rho+\frac{1}{\rho}\frac{\partial }{\partial  \phi}\mathbf{a}_\phi+\frac{\partial }{\partial z}\mathbf{a}_z  \end{align} which is the desired result. Approach #2: How can I get the same result starting from the point-to-point transformation $$x=\rho \, \text{cos}\phi,\; y=\rho \, \text{sin}\phi$$ by using partial differentiation? Maybe implicit differentiation?",,"['derivatives', 'partial-derivative', 'transformation', 'vector-analysis']"
36,"If $f$ is bounded and twice differentiable in $\mathbb{R}$, show that there exists $\xi\in\mathbb{R}$, s.t. $f''(\xi)=0$.","If  is bounded and twice differentiable in , show that there exists , s.t. .",f \mathbb{R} \xi\in\mathbb{R} f''(\xi)=0,"My idea: If $f$ has maximum and minimum, then $f'=0$ at these two points, and the conclusion is further derived using Mean Value Theorem. But what if $f$ has no maximum/minimum, like $f=\frac{1}{1+e^{-x}}$? $f$ is twice differentiable in $\mathbb{R}$, so $f'$ is continuous in $\mathbb{R}$. If $\forall x,y,~f'(x)\neq f'(y)$, then $f'$ is a monotonic function (how to prove this?).  Then use second-order Taylor approximation to show $f$ is unbounded (inspired by Simon S), which contradicts the assumption. So $\exists x,y,~f'(x)=f'(y)$ and $\exists \xi,~ f''(\xi)=0$ [Mean Value Theorem].","My idea: If $f$ has maximum and minimum, then $f'=0$ at these two points, and the conclusion is further derived using Mean Value Theorem. But what if $f$ has no maximum/minimum, like $f=\frac{1}{1+e^{-x}}$? $f$ is twice differentiable in $\mathbb{R}$, so $f'$ is continuous in $\mathbb{R}$. If $\forall x,y,~f'(x)\neq f'(y)$, then $f'$ is a monotonic function (how to prove this?).  Then use second-order Taylor approximation to show $f$ is unbounded (inspired by Simon S), which contradicts the assumption. So $\exists x,y,~f'(x)=f'(y)$ and $\exists \xi,~ f''(\xi)=0$ [Mean Value Theorem].",,['derivatives']
37,Use of a substitution to prove that $e^{2xt-t^2}$ is the exponential generating function of the Hermite polynomials,Use of a substitution to prove that  is the exponential generating function of the Hermite polynomials,e^{2xt-t^2},"The generating function encodes all the Hermite polynomials in one formula. It is a function of $x$ and a dummy variable $t$ of the the form: $e^{2xt-t^2}=\sum^\infty_{n=0}\frac{H_n(x)}{n!}t^n. $ We begin by considering $f(t)=e^{-(x-t)^2}=e^{-x^2}e^{2xt-t^2}.$ The Taylor series for this function is  $f(t)=\sum^\infty_{n=0}\frac{f^{(n)}(0)}{n!}t^n.$ Here by using a substitution, $x-t=u$, we have $f^{(n)}(0)=\bigg[ \frac{d^n}{dt^n}e^{-(x-t)^2} \bigg]_{t=0}=(-1)^n\bigg[ \frac{d^n}{dt^n}e^{-u^2} \bigg]_{u=x}=(-1)^n\frac{d^n}{dx^n}(e^{-x^2})=e^{-x^2}H_n(x).$ I am struggling to see how the substitution $x-t=u$ is implemented here and where the $(-1)^n$ arises from? Any help would be much appreciated. Thank you","The generating function encodes all the Hermite polynomials in one formula. It is a function of $x$ and a dummy variable $t$ of the the form: $e^{2xt-t^2}=\sum^\infty_{n=0}\frac{H_n(x)}{n!}t^n. $ We begin by considering $f(t)=e^{-(x-t)^2}=e^{-x^2}e^{2xt-t^2}.$ The Taylor series for this function is  $f(t)=\sum^\infty_{n=0}\frac{f^{(n)}(0)}{n!}t^n.$ Here by using a substitution, $x-t=u$, we have $f^{(n)}(0)=\bigg[ \frac{d^n}{dt^n}e^{-(x-t)^2} \bigg]_{t=0}=(-1)^n\bigg[ \frac{d^n}{dt^n}e^{-u^2} \bigg]_{u=x}=(-1)^n\frac{d^n}{dx^n}(e^{-x^2})=e^{-x^2}H_n(x).$ I am struggling to see how the substitution $x-t=u$ is implemented here and where the $(-1)^n$ arises from? Any help would be much appreciated. Thank you",,"['derivatives', 'orthogonal-polynomials', 'hermite-polynomials']"
38,What actually is a differential?,What actually is a differential?,,"I am a bit confused about differentials, and this is probably partly due to what I find to be a rather confusing teaching approach. (I know there are a bunch of similar questions around, but none of them clarified my confusion). When first meeting derivatives in calculus, the magic $d$ symbol first appeared in the Leibniz notation for derivatives as $\frac{df}{dx}$ , and got told that it's a symbol for differentiation, not a fraction. The chain rule, saying that $\frac{df}{dx} = \frac{df}{du}\frac{du}{dx}$ was taught along the lines ""looks like fraction simplification, but be careful"". Then integrals came around, with not much being said about the $dx$ at the end, until the substitution rule. Then, I made contact with differentials, but merely saying that when changing the variable, one needs also to change the differential $dx$ to $du=f'dx$ . Now, it seems to me that $du=f'dx$ comes a bit from $f'=\frac{df}{dx}$ . When getting a bit into higher maths, there is more and more operations with functions/differentials. When doing surface areas, we talk about area differential, with $ds^2 = dx^2 + dy^2$ , or differential of a multivariable function as $df = \sum\frac{\partial f}{\partial xi}dx_i$ . It seems like, in some cases, we do operate with the differential as its a simple real value, where the idea of an infinitesimal (basically $\Delta x = x - x_1$ as $x_1 \rightarrow x$ . Possibly the most important thing for me now, it seems like I can see the differential $df$ as a local, linear approximation of $f$ and $ \sum\frac{\partial f}{\partial xi}dx_i$ as a decomposition of the tangent along the basis directions. But, at the same time, I remain stuck with the lack of an exact definition of the differential and a bit of fear of using it due to warnings such as ""chain rule is not really a fraction simplification"".","I am a bit confused about differentials, and this is probably partly due to what I find to be a rather confusing teaching approach. (I know there are a bunch of similar questions around, but none of them clarified my confusion). When first meeting derivatives in calculus, the magic symbol first appeared in the Leibniz notation for derivatives as , and got told that it's a symbol for differentiation, not a fraction. The chain rule, saying that was taught along the lines ""looks like fraction simplification, but be careful"". Then integrals came around, with not much being said about the at the end, until the substitution rule. Then, I made contact with differentials, but merely saying that when changing the variable, one needs also to change the differential to . Now, it seems to me that comes a bit from . When getting a bit into higher maths, there is more and more operations with functions/differentials. When doing surface areas, we talk about area differential, with , or differential of a multivariable function as . It seems like, in some cases, we do operate with the differential as its a simple real value, where the idea of an infinitesimal (basically as . Possibly the most important thing for me now, it seems like I can see the differential as a local, linear approximation of and as a decomposition of the tangent along the basis directions. But, at the same time, I remain stuck with the lack of an exact definition of the differential and a bit of fear of using it due to warnings such as ""chain rule is not really a fraction simplification"".",d \frac{df}{dx} \frac{df}{dx} = \frac{df}{du}\frac{du}{dx} dx dx du=f'dx du=f'dx f'=\frac{df}{dx} ds^2 = dx^2 + dy^2 df = \sum\frac{\partial f}{\partial xi}dx_i \Delta x = x - x_1 x_1 \rightarrow x df f  \sum\frac{\partial f}{\partial xi}dx_i,"['derivatives', 'differential']"
39,Pretty conjecture $x^{\left(\frac{y}{x}\right)^n}+y^{\left(\frac{x}{y}\right)^n}\leq 1$,Pretty conjecture,x^{\left(\frac{y}{x}\right)^n}+y^{\left(\frac{x}{y}\right)^n}\leq 1,"inspired (again) by an inequality of Vasile Cirtoaje I propose my own conjecture : Let $x,y>0$ such that $x+y=1$ and $n\geq 1$ a natural number then we have : $$x^{\left(\frac{y}{x}\right)^n}+y^{\left(\frac{x}{y}\right)^n}\leq 1$$ First I find it very nice because all the coefficient are $1$ . I have tested with Geogebra until $n=50$ without any counter-examples. Furthermore we have an equality case as $x=y=0.5$ or $x=1$ and $y=0$ and vice versa . To solve it I have tried all the ideas here My main idea was to make a link with this inequality (my inspiration) see here So if you can help me to solve it or give me an approach... ...Thanks for all your contributions ! Little update I think there is also an invariance as in question here Conjecture $a^{(\frac{a}{b})^p}+b^{(\frac{b}{a})^p}+c\geq 1$ Theoretical method Well,Well this method is very simple but the result is a little bit crazy (for me (and you ?)) Well ,I know that if we put $n=2$ we can find (using parabola) an upper bound like $$x^{\left(\frac{1-x}{x}\right)^2}\leq ax^2+bx+c=p(x)$$ And $$(1-x)^{\left(\frac{x}{1-x}\right)^2}\leq ux^2+vx+w=q(x)$$ on $[\alpha,\frac{1}{2}]$ with $\alpha>0$ and such that $p(x)+q(x)<1$ In the neightborhood of $0$ we can use a cubic . Well,now we have (summing) : $$x^{\left(\frac{1-x}{x}\right)^2}+(1-x)^{\left(\frac{x}{1-x}\right)^2}\leq p(x)+q(x)$$ We add a variable $\varepsilon$ such that $(p(x)+\varepsilon)+q(x)=1$ Now we want an inequality of the kind ( $k\geq 2$ ): $$x^{\left(\frac{1-x}{x}\right)^{2k}}+(1-x)^{\left(\frac{x}{1-x}\right)^{2k}}\leq (p(x)+\varepsilon)^{\left(\frac{1-x}{x}\right)^{2k-2}}+q(x)^{\left(\frac{x}{1-x}\right)^{2k-2}}$$ Now and it's a crucial idea we want something like : $$\left(\frac{x}{1-x}\right)^{2k-2}\geq \left(\frac{1-(p(x)+\varepsilon)}{q(x)}\right)^y$$ AND : $$\left(\frac{1-x}{x}\right)^{2k-2}\geq \left(\frac{1-q(x)}{p(x)+\varepsilon}\right)^y$$ Now it's not hard to find a such $y$ using logarithm . We get someting like : $$x^{\left(\frac{1-x}{x}\right)^{2k}}+(1-x)^{\left(\frac{x}{1-x}\right)^{2k}}\leq q(x)^{\left(\frac{1-q(x)}{q(x)}\right)^{y}}+(1-q(x))^{\left(\frac{q(x)}{1-q(x)}\right)^{y}}$$ Furthermore the successive iterations of this method conducts to $1$ because the values of the differents polynomials (wich are an approximation of the initial curve) tend to zero or one (as abscissa). The extra-thing (and a little bit crazy) we can make an order on all the values. My second question Is it unusable as theoretical\practical method ?","inspired (again) by an inequality of Vasile Cirtoaje I propose my own conjecture : Let such that and a natural number then we have : First I find it very nice because all the coefficient are . I have tested with Geogebra until without any counter-examples. Furthermore we have an equality case as or and and vice versa . To solve it I have tried all the ideas here My main idea was to make a link with this inequality (my inspiration) see here So if you can help me to solve it or give me an approach... ...Thanks for all your contributions ! Little update I think there is also an invariance as in question here Conjecture $a^{(\frac{a}{b})^p}+b^{(\frac{b}{a})^p}+c\geq 1$ Theoretical method Well,Well this method is very simple but the result is a little bit crazy (for me (and you ?)) Well ,I know that if we put we can find (using parabola) an upper bound like And on with and such that In the neightborhood of we can use a cubic . Well,now we have (summing) : We add a variable such that Now we want an inequality of the kind ( ): Now and it's a crucial idea we want something like : AND : Now it's not hard to find a such using logarithm . We get someting like : Furthermore the successive iterations of this method conducts to because the values of the differents polynomials (wich are an approximation of the initial curve) tend to zero or one (as abscissa). The extra-thing (and a little bit crazy) we can make an order on all the values. My second question Is it unusable as theoretical\practical method ?","x,y>0 x+y=1 n\geq 1 x^{\left(\frac{y}{x}\right)^n}+y^{\left(\frac{x}{y}\right)^n}\leq 1 1 n=50 x=y=0.5 x=1 y=0 n=2 x^{\left(\frac{1-x}{x}\right)^2}\leq ax^2+bx+c=p(x) (1-x)^{\left(\frac{x}{1-x}\right)^2}\leq ux^2+vx+w=q(x) [\alpha,\frac{1}{2}] \alpha>0 p(x)+q(x)<1 0 x^{\left(\frac{1-x}{x}\right)^2}+(1-x)^{\left(\frac{x}{1-x}\right)^2}\leq p(x)+q(x) \varepsilon (p(x)+\varepsilon)+q(x)=1 k\geq 2 x^{\left(\frac{1-x}{x}\right)^{2k}}+(1-x)^{\left(\frac{x}{1-x}\right)^{2k}}\leq (p(x)+\varepsilon)^{\left(\frac{1-x}{x}\right)^{2k-2}}+q(x)^{\left(\frac{x}{1-x}\right)^{2k-2}} \left(\frac{x}{1-x}\right)^{2k-2}\geq \left(\frac{1-(p(x)+\varepsilon)}{q(x)}\right)^y \left(\frac{1-x}{x}\right)^{2k-2}\geq \left(\frac{1-q(x)}{p(x)+\varepsilon}\right)^y y x^{\left(\frac{1-x}{x}\right)^{2k}}+(1-x)^{\left(\frac{x}{1-x}\right)^{2k}}\leq q(x)^{\left(\frac{1-q(x)}{q(x)}\right)^{y}}+(1-q(x))^{\left(\frac{q(x)}{1-q(x)}\right)^{y}} 1","['derivatives', 'inequality', 'examples-counterexamples', 'exponentiation', 'conjectures']"
40,Why is the derivative important? [duplicate],Why is the derivative important? [duplicate],,"This question already has answers here : Why do we differentiate? (6 answers) Closed 6 years ago . Derivatives, both ordinary and partial, appear often in my mathematics courses. However, my teachers have never really given a good example of why the derivative is useful. My questions: Other than the usual instantaneous rate of change, what are some common uses of the derivative? What does the partial derivative tell us? And what does the total derivative tell us? I find that often times, the derivative is simply explained as ""the instantaneous rate of change"". I am thinking about switching my major, because the applications of math at such an elementary level seem trivial when professors just push symbols and don't have any real world motivation included in their lectures. P.S. This question is not a duplicate of Why do we differentiate? I do not want to know why we differentiate. I want to know why it is important past our undergraduate learning. What are the applications beyond Calculus 3? Beyond academia, what makes the derivative important in complex situations?","This question already has answers here : Why do we differentiate? (6 answers) Closed 6 years ago . Derivatives, both ordinary and partial, appear often in my mathematics courses. However, my teachers have never really given a good example of why the derivative is useful. My questions: Other than the usual instantaneous rate of change, what are some common uses of the derivative? What does the partial derivative tell us? And what does the total derivative tell us? I find that often times, the derivative is simply explained as ""the instantaneous rate of change"". I am thinking about switching my major, because the applications of math at such an elementary level seem trivial when professors just push symbols and don't have any real world motivation included in their lectures. P.S. This question is not a duplicate of Why do we differentiate? I do not want to know why we differentiate. I want to know why it is important past our undergraduate learning. What are the applications beyond Calculus 3? Beyond academia, what makes the derivative important in complex situations?",,"['derivatives', 'soft-question']"
41,Why use 95% confidence interval?,Why use 95% confidence interval?,,"May I ask why $95\%$ confidence is so commonly used? Does it have anything to do with $\frac{d}{d\alpha}e_n(\alpha)$, where $e_n(\alpha) = Z_{\alpha/2}\frac{S_n}{\sqrt n}$? (My professor asks me to evaluate this derivative at $\alpha = 0.05$, given $S_n = 4.7, n = 100$.)","May I ask why $95\%$ confidence is so commonly used? Does it have anything to do with $\frac{d}{d\alpha}e_n(\alpha)$, where $e_n(\alpha) = Z_{\alpha/2}\frac{S_n}{\sqrt n}$? (My professor asks me to evaluate this derivative at $\alpha = 0.05$, given $S_n = 4.7, n = 100$.)",,"['derivatives', 'normal-distribution', 'confidence-interval']"
42,Derivative of quaternions,Derivative of quaternions,,I am trying to calculate the Jacobian of a function that has quaternions and 3D points in it. I refer to quaternions as $q$ and 3D points as $p$ $$h_1(q)=A C(q)p $$ $$h_2(q)=q_1\otimes q \otimes q_2 $$ where $A\in R^{3x3}$ and $C(q)$ is Direction cosine matrix . I am using the Hamilton form for the quaternions. I would like to calculate the following Jacobians: $$H_1 = \frac{\partial h_1(q)}{\partial q} $$ $$H_2 = \frac{\partial h_2(q)}{\partial q} $$ Following Joan Solà's reference eq. 18 what I have is $$H_1 = A^TC(q)^T[p]_x $$ $$H_2 = [q_1]_L[q_2]_R $$ Where $[q]_R$ and $[q]_L$ are the right and left handed conversion of quaternion to matrix form as defined in Joan Solà's reference eq. 18. All rotations are body centric. Is this correct?  Is there a better way to do this?  Can the expression be easily simplified?,I am trying to calculate the Jacobian of a function that has quaternions and 3D points in it. I refer to quaternions as and 3D points as where and is Direction cosine matrix . I am using the Hamilton form for the quaternions. I would like to calculate the following Jacobians: Following Joan Solà's reference eq. 18 what I have is Where and are the right and left handed conversion of quaternion to matrix form as defined in Joan Solà's reference eq. 18. All rotations are body centric. Is this correct?  Is there a better way to do this?  Can the expression be easily simplified?,q p h_1(q)=A C(q)p  h_2(q)=q_1\otimes q \otimes q_2  A\in R^{3x3} C(q) H_1 = \frac{\partial h_1(q)}{\partial q}  H_2 = \frac{\partial h_2(q)}{\partial q}  H_1 = A^TC(q)^T[p]_x  H_2 = [q_1]_L[q_2]_R  [q]_R [q]_L,"['derivatives', 'quaternions']"
43,BAC─CAB rule used on del operators,BAC─CAB rule used on del operators,,"In my textbook, it is stated that $\nabla\times(\nabla\times A)=\nabla(\nabla\cdot A)-\nabla^2A $ So, I thought that del can be treated as if it were a vector (although it was an operator). However,  When solving $\nabla\times (k\times r)$ where $r= x \hat{i}+y\hat{j}+z\hat{k}    $, the results I obtained from using the BAC CAB rule (while treating $\nabla$ as a constant) was different from the results obtained by doing it in the normal order. Why is this the case? Is it because, since $\nabla$ is an 'operator', you can't use it like a vector, and therefore cannot use the BAC CAB rule? But in the textbook, it uses the BAC CAB rule as the following: $$\nabla\times(\nabla\times A)=\nabla(\nabla\cdot A)-\nabla^2A $$ So does this mean that the textbook's derivation of $\nabla\times(\nabla\times A)$ was incorrect, and I should use Levi-Civita definition of cross products? In summary, my question is: when is it safe to regard $\nabla$ as a vector?","In my textbook, it is stated that $\nabla\times(\nabla\times A)=\nabla(\nabla\cdot A)-\nabla^2A $ So, I thought that del can be treated as if it were a vector (although it was an operator). However,  When solving $\nabla\times (k\times r)$ where $r= x \hat{i}+y\hat{j}+z\hat{k}    $, the results I obtained from using the BAC CAB rule (while treating $\nabla$ as a constant) was different from the results obtained by doing it in the normal order. Why is this the case? Is it because, since $\nabla$ is an 'operator', you can't use it like a vector, and therefore cannot use the BAC CAB rule? But in the textbook, it uses the BAC CAB rule as the following: $$\nabla\times(\nabla\times A)=\nabla(\nabla\cdot A)-\nabla^2A $$ So does this mean that the textbook's derivation of $\nabla\times(\nabla\times A)$ was incorrect, and I should use Levi-Civita definition of cross products? In summary, my question is: when is it safe to regard $\nabla$ as a vector?",,"['notation', 'derivatives', 'vector-fields']"
44,Is $e^{|x|}$ differentiable?,Is  differentiable?,e^{|x|},"My thoughts go as follows: For $x > 0$, $e^{|x|} = e^x $ For $x < 0$, $e^{|x|} = e^{-x}$ Both $e^x$ and $e^{-x}$ are differentiable at every point in their domains, so $e^{|x|}$ will be differentiable for all $x \ne 0$ $e^{|x|}$ is certainly continuous everywhere, so I can't rule out differentiability with that criterion. I know the derivative of $e^x$ at $x=0$ is $1$, and the derivative of $e^{-x}$ at $x = 0$ is $-1$, so to me this indicates that the right hand limit and left hand limit of  $\frac{e^{|x|} - 1}{x}$ approach different values as $x$ approaches $0$, so it cannot be differentiable at $0$. This seems logically correct to me, but I'm not completely certain, and it feels a little weak. Any advice?","My thoughts go as follows: For $x > 0$, $e^{|x|} = e^x $ For $x < 0$, $e^{|x|} = e^{-x}$ Both $e^x$ and $e^{-x}$ are differentiable at every point in their domains, so $e^{|x|}$ will be differentiable for all $x \ne 0$ $e^{|x|}$ is certainly continuous everywhere, so I can't rule out differentiability with that criterion. I know the derivative of $e^x$ at $x=0$ is $1$, and the derivative of $e^{-x}$ at $x = 0$ is $-1$, so to me this indicates that the right hand limit and left hand limit of  $\frac{e^{|x|} - 1}{x}$ approach different values as $x$ approaches $0$, so it cannot be differentiable at $0$. This seems logically correct to me, but I'm not completely certain, and it feels a little weak. Any advice?",,"['derivatives', 'exponential-function', 'absolute-value']"
45,Is $d^2y/dx$ a valid mathematical notation?,Is  a valid mathematical notation?,d^2y/dx,"I have often seen ""the second derivative of y with respect to x"" written as $${d^2y\over dx^2},$$ but I don't understand the reason for this notation. I have always seen it written as $${d^2y\over dx^2},$$ and never as $${d^2y\over dx}.$$ Would $d^2y\over dx$ be a valid notation as well, and would it have a different meaning from $d^2y\over dx^2$?","I have often seen ""the second derivative of y with respect to x"" written as $${d^2y\over dx^2},$$ but I don't understand the reason for this notation. I have always seen it written as $${d^2y\over dx^2},$$ and never as $${d^2y\over dx}.$$ Would $d^2y\over dx$ be a valid notation as well, and would it have a different meaning from $d^2y\over dx^2$?",,"['derivatives', 'notation']"
46,The diffential of commutator map in a Lie group,The diffential of commutator map in a Lie group,,"Leb $G$ be a Lie group and $f:G\times G\rightarrow G$ be the commutator map $:(x,y)\mapsto xyx^{-1}y^{-1}$. How to obtain the Lie bracket in the associated Lie algebra of $G$ from the derivatives of $f$? (We know that the Lie bracket is defined via the adjoint representation.) (I saw $df_{(e,e)}(X,Y)=[X,Y]$ somewhere. Thanks to @John for remarking this is false. But I think there is indeed a relation between the Lie bracket and derivatives of $f$.  )","Leb $G$ be a Lie group and $f:G\times G\rightarrow G$ be the commutator map $:(x,y)\mapsto xyx^{-1}y^{-1}$. How to obtain the Lie bracket in the associated Lie algebra of $G$ from the derivatives of $f$? (We know that the Lie bracket is defined via the adjoint representation.) (I saw $df_{(e,e)}(X,Y)=[X,Y]$ somewhere. Thanks to @John for remarking this is false. But I think there is indeed a relation between the Lie bracket and derivatives of $f$.  )",,"['differential-geometry', 'derivatives']"
47,Finite differences second derivative as successive application of the first derivative,Finite differences second derivative as successive application of the first derivative,,"The finite difference expressions for the first, second and higher derivatives in the first, second or higher order of accuracy can be easily derived from Taylor's expansions. But, numerically, the successive application of the first derivative, in general, is not same as application of the second derivative. First, a case where it works. Let's say that we want to compute second derivative of function $f$ given on 3-points stencil $(i-1, i, i+1)$ . The finite difference formula is: $$\left(\frac{\partial^2 f}{\partial x^2}\right)_i = \frac{1}{h^2}(f_{i-1} - 2f_i + f_{i+1})$$ This result is derived from Taylor's expansions, but it can also be interpreted in the following way. The first derivatives of the first order accuracy at the intervals $(i-1, i)$ and $(i, i+1)$ are: $$\left(\frac{\partial f}{\partial x}\right)_{i-1/2} = \frac{1}{h}(f_i - f_{i-1})$$ and $$\left(\frac{\partial f}{\partial x}\right)_{i+1/2} = \frac{1}{h}(f_{i+1} - f_{i})$$ where I use $i-1/2$ and $i+1/2$ because these derivatives are representative for the cell faces (In the first order I have actually approximated my function as piece-wise linear between the grid points $x_i$ . Therefore, in every grid point the slope on the left and the right hand side of it is not the same.) The second derivative in point $i$ is now: $$\left(\frac{\partial^2 f}{\partial x^2}\right)_{i} = \frac{1}{h}(f'_{i+1/2} - f'_{i-1/2}) = \frac{1}{h^2}(f_{i+1} - f_{i} - (f_i - f_{i-1})) $$ And this is identical to the finite difference expression for the second derivative in the second order of accuracy. I wonder if there is a similar procedure to represent the second derivative in the 4th order accuracy (on 5-points stencil) as successive application of two first order derivative of the lower accuracy (on shorter stencils)? A naive approach would be to apply first derivatives of the second order accuracy to the stencils $(i-2, i-i, i)$ and $(i, i+1, i+2)$ : $$\left(\frac{\partial u}{\partial x}\right)_{i-1} = \frac{1}{2h}(u_i - u_{i-2})$$ and $$\left(\frac{\partial u}{\partial x}\right)_{i+1} = \frac{1}{2h}(u_{i+2} - u_{i})$$ and then to find the second derivative as the first derivative of the previous two: $$\left(\frac{\partial^2 u}{\partial x^2}\right)_{i} = \frac{1}{4h^2}(u_{i+2} - 2u_{i} - u_{i-2})$$ This is obviously not correct or, at least, not the same as application of the second derivative of the 4th order straight away: $$\left(\frac{\partial^2 u}{\partial x^2}\right)_{i} = \frac{1}{12h^2}(-u_{i-2} + 16u_{i-1} + 30 u_i + 16 u_{i+1} - u_{i+2})$$ So, is there a way to reproduce the last equation as a successive combination of first derivatives of the lower accuracy order? If not, why not? Many thanks for help! This is driving me crazy!","The finite difference expressions for the first, second and higher derivatives in the first, second or higher order of accuracy can be easily derived from Taylor's expansions. But, numerically, the successive application of the first derivative, in general, is not same as application of the second derivative. First, a case where it works. Let's say that we want to compute second derivative of function given on 3-points stencil . The finite difference formula is: This result is derived from Taylor's expansions, but it can also be interpreted in the following way. The first derivatives of the first order accuracy at the intervals and are: and where I use and because these derivatives are representative for the cell faces (In the first order I have actually approximated my function as piece-wise linear between the grid points . Therefore, in every grid point the slope on the left and the right hand side of it is not the same.) The second derivative in point is now: And this is identical to the finite difference expression for the second derivative in the second order of accuracy. I wonder if there is a similar procedure to represent the second derivative in the 4th order accuracy (on 5-points stencil) as successive application of two first order derivative of the lower accuracy (on shorter stencils)? A naive approach would be to apply first derivatives of the second order accuracy to the stencils and : and and then to find the second derivative as the first derivative of the previous two: This is obviously not correct or, at least, not the same as application of the second derivative of the 4th order straight away: So, is there a way to reproduce the last equation as a successive combination of first derivatives of the lower accuracy order? If not, why not? Many thanks for help! This is driving me crazy!","f (i-1, i, i+1) \left(\frac{\partial^2 f}{\partial x^2}\right)_i = \frac{1}{h^2}(f_{i-1} - 2f_i + f_{i+1}) (i-1, i) (i, i+1) \left(\frac{\partial f}{\partial x}\right)_{i-1/2} = \frac{1}{h}(f_i - f_{i-1}) \left(\frac{\partial f}{\partial x}\right)_{i+1/2} = \frac{1}{h}(f_{i+1} - f_{i}) i-1/2 i+1/2 x_i i \left(\frac{\partial^2 f}{\partial x^2}\right)_{i} = \frac{1}{h}(f'_{i+1/2} - f'_{i-1/2}) = \frac{1}{h^2}(f_{i+1} - f_{i} - (f_i - f_{i-1}))  (i-2, i-i, i) (i, i+1, i+2) \left(\frac{\partial u}{\partial x}\right)_{i-1} = \frac{1}{2h}(u_i - u_{i-2}) \left(\frac{\partial u}{\partial x}\right)_{i+1} = \frac{1}{2h}(u_{i+2} - u_{i}) \left(\frac{\partial^2 u}{\partial x^2}\right)_{i} = \frac{1}{4h^2}(u_{i+2} - 2u_{i} - u_{i-2}) \left(\frac{\partial^2 u}{\partial x^2}\right)_{i} = \frac{1}{12h^2}(-u_{i-2} + 16u_{i-1} + 30 u_i + 16 u_{i+1} - u_{i+2})","['derivatives', 'numerical-methods', 'finite-differences', 'numerical-calculus', 'finite-difference-methods']"
48,Deriving the Normalization formula for Associated Legendre functions: Stage $1$ of $4$,Deriving the Normalization formula for Associated Legendre functions: Stage  of,1 4,"The question that follows is needed as part of a derivation of the Associated Legendre Functions Normalization Formula: $$\color{blue}{\displaystyle\int_{x=-1}^{1}[{P_{L}}^m(x)]^2\,\mathrm{d}x=\left(\frac{2}{2L+1}\right)\frac{(L+m)!}{(L-m)!}}$$ where for each $m$ , the functions $${P_L}^m(x)=\frac{1}{2^LL!}\left(1-x^2\right)^{m/2}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L$$ are the associated Legendre functions on $[−1, 1]$ . The question in my textbook asks me to Show that $$\begin{align}\require{enclose}\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}\left(x^2-1\right)^L&=\frac{(L-m)!}{(L+m)!}\left(x^2-1\right)^m\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\end{align}$$ Where $L\,\text&\,m\, \text{are constants}$ and $0\leq  m\leq L$ . Hint : Write $$(x^2-1)^L=(x-1)^L(x+1)^L$$ and find the derivatives by Leibniz' rule. So this is what I have tried: $$\begin{align}\require{enclose}\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}\left(x^2-1\right)^L&=\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}(x^2-1)^L \\&=\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}(x-1)^L(x+1)^L\quad\quad\longleftarrow\bbox[#F8A]{\text{Using the Hint}} \\&=(x-1)^L\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}(x+1)^L +(L-m)\frac{\mathrm{d}}{\mathrm{d}x}\left(x-1\right)^L\frac{\mathrm{d}^{L-(m+1)}}{\mathrm{d}x^{L-(m+1)}}(x+1)^L \\&\phantom{Abcde}+\frac{(L-m)(L-[m+1])}{2}\frac{\mathrm{d}^2}{\mathrm{d}x^2}\left(x-1\right)^L\frac{\mathrm{d}^{L-(m+2)}}{\mathrm{d}x^{L-(m+2)}}(x+1)^L+\ldots\,.\end{align}$$ But this could go on forever and I have no idea how to evaluate (or simplify) terms like $$\frac{\mathrm{d}^{L-(m+2)}}{\mathrm{d}x^{L-(m+2)}}(x+1)^L\,.$$ Is there any chance someone could please give me some hints or advice on how to show that $$\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}\left(x^2-1\right)^L=\color{#180}{\fbox{$\frac{(L-m)!}{(L+m)!}\left(x^2-1\right)^m\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L$}}$$ by starting at one side of the equation and showing that it is equal to the other side? Best Regards.","The question that follows is needed as part of a derivation of the Associated Legendre Functions Normalization Formula: where for each , the functions are the associated Legendre functions on . The question in my textbook asks me to Show that Where and . Hint : Write and find the derivatives by Leibniz' rule. So this is what I have tried: But this could go on forever and I have no idea how to evaluate (or simplify) terms like Is there any chance someone could please give me some hints or advice on how to show that by starting at one side of the equation and showing that it is equal to the other side? Best Regards.","\color{blue}{\displaystyle\int_{x=-1}^{1}[{P_{L}}^m(x)]^2\,\mathrm{d}x=\left(\frac{2}{2L+1}\right)\frac{(L+m)!}{(L-m)!}} m {P_L}^m(x)=\frac{1}{2^LL!}\left(1-x^2\right)^{m/2}\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L [−1, 1] \begin{align}\require{enclose}\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}\left(x^2-1\right)^L&=\frac{(L-m)!}{(L+m)!}\left(x^2-1\right)^m\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L\end{align} L\,\text&\,m\, \text{are constants} 0\leq  m\leq L (x^2-1)^L=(x-1)^L(x+1)^L \begin{align}\require{enclose}\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}\left(x^2-1\right)^L&=\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}(x^2-1)^L
\\&=\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}(x-1)^L(x+1)^L\quad\quad\longleftarrow\bbox[#F8A]{\text{Using the Hint}}
\\&=(x-1)^L\frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}(x+1)^L
+(L-m)\frac{\mathrm{d}}{\mathrm{d}x}\left(x-1\right)^L\frac{\mathrm{d}^{L-(m+1)}}{\mathrm{d}x^{L-(m+1)}}(x+1)^L
\\&\phantom{Abcde}+\frac{(L-m)(L-[m+1])}{2}\frac{\mathrm{d}^2}{\mathrm{d}x^2}\left(x-1\right)^L\frac{\mathrm{d}^{L-(m+2)}}{\mathrm{d}x^{L-(m+2)}}(x+1)^L+\ldots\,.\end{align} \frac{\mathrm{d}^{L-(m+2)}}{\mathrm{d}x^{L-(m+2)}}(x+1)^L\,. \frac{\mathrm{d}^{L-m}}{\mathrm{d}x^{L-m}}\left(x^2-1\right)^L=\color{#180}{\fbox{\frac{(L-m)!}{(L+m)!}\left(x^2-1\right)^m\frac{\mathrm{d}^{L+m}}{\mathrm{d}x^{L+m}}\left(x^2-1\right)^L}}","['derivatives', 'polynomials', 'infinite-product', 'legendre-polynomials']"
49,Derivative of the Inverse Cumulative Distribution Function for the Standard Normal Distribution,Derivative of the Inverse Cumulative Distribution Function for the Standard Normal Distribution,,"As the title says, I am trying to find the derivative of the inverse cumulative distribution function for the standard normal distribution. I have this figured out for one particular case, but there is an extra layer of complexity that has be stumped. Let $0 \le p \le 1$ and let $z = \Phi^{-1}(p)$, where $\Phi^{-1}(p)$ is the inverse cumulative distribution function for the standard normal distribution. Then: $$\frac{\partial \Phi^{-1}(p)}{\partial p} = \left(\frac{\partial \Phi(z)}{\partial z}\right)^{-1},$$ where $\Phi(z)$ is the cumulative distribution function for the standard normal distribution. This yields: $$= \left(\frac{1}{\sqrt{2\pi}} \exp(-z^2/2) \right)^{-1} = \frac{\sqrt{2\pi}}{\exp(-z^2/2)}.$$ I think/hope this is right so far. But now I have $p_1$ and $p_2$ and I need to find the derivative of $$\frac{\partial \Phi^{-1}\left(\frac{p_1}{p_1+p_2}\right)}{\partial p_1}$$ and $$\frac{\partial \Phi^{-1}\left(\frac{p_1}{p_1+p_2}\right)}{\partial p_2}.$$ Any help would be appreciated.","As the title says, I am trying to find the derivative of the inverse cumulative distribution function for the standard normal distribution. I have this figured out for one particular case, but there is an extra layer of complexity that has be stumped. Let $0 \le p \le 1$ and let $z = \Phi^{-1}(p)$, where $\Phi^{-1}(p)$ is the inverse cumulative distribution function for the standard normal distribution. Then: $$\frac{\partial \Phi^{-1}(p)}{\partial p} = \left(\frac{\partial \Phi(z)}{\partial z}\right)^{-1},$$ where $\Phi(z)$ is the cumulative distribution function for the standard normal distribution. This yields: $$= \left(\frac{1}{\sqrt{2\pi}} \exp(-z^2/2) \right)^{-1} = \frac{\sqrt{2\pi}}{\exp(-z^2/2)}.$$ I think/hope this is right so far. But now I have $p_1$ and $p_2$ and I need to find the derivative of $$\frac{\partial \Phi^{-1}\left(\frac{p_1}{p_1+p_2}\right)}{\partial p_1}$$ and $$\frac{\partial \Phi^{-1}\left(\frac{p_1}{p_1+p_2}\right)}{\partial p_2}.$$ Any help would be appreciated.",,"['derivatives', 'normal-distribution', 'inverse']"
50,Divergence in Definition of Laplace-Beltrami Operator,Divergence in Definition of Laplace-Beltrami Operator,,"I am trying to derive an explicit formula for Laplace-Beltrami operator in global Cartesian coordinates for a special case of plane curve. I have found this article , and I would like to match their expression (6) for LB on a curve with the standard definition in terms of metric tensor . According to formula $(6)$ in the paper , Laplace-Beltrami operator on plane curve can be written as \begin{align} \Delta_{LB}\, u & = \Delta u + \kappa\,u_{n} - u_{nn} \\ & = \tag{$\star$} \Delta u + \kappa\,\vec{n}\cdot\nabla u - \vec{n}\cdot\nabla\left(\vec{n}\cdot\nabla u\right)  \end{align} $\,\vec{n}\,$ is unit normal vector, $\,\kappa=-\nabla\cdot\vec{n}\,$ is curvature , $\,u_{n} = \vec{n}\cdot\nabla u\,$ and $\,u_{nn} = \vec{n}\cdot\nabla \left(\vec{n}\cdot\nabla u\right)\,$ are first and second normal derivatives , $\,\nabla u\,$ and $\,\Delta u\,$  are respectively gradient and Laplacian of $\,u\,$. I am having troubles deriving $(\star)$ or matching it with metric tensor expression for LB operator \begin{align}\tag{$\ast$} \Delta_{LB}\, u = \dfrac{1}{\sqrt{\left\lvert g\right\rvert}}\,\partial_i\,\Big(\sqrt{\left\lvert g\right\rvert} \,g^{ij}\,\partial_j \,u \Big) \end{align} I can derive $(\star)$ from the Laplace-Beltrami expression $\,\Delta_{LB}\,u = \nabla_{s}\cdot\big(\nabla_{s}\,u\big)\,$ assuming surface divergence of a vector equals to the regular divergence of its projection to the curve . This is a BIG assumption, and I do not know how to justify it. I will appreciate if someone could help me to justify my assumption, or to derive $(\star)$ without assumptions on (surface) divergence. My attempt to derive $(\star)$: let $\,\nabla_{s}\,$, and $P$ denote surface gradient and projecting operator , then \begin{align} \Delta_{LB}\, u & = \nabla_{s}\cdot\big(\nabla_{s}\,u\big) \stackrel{\color{red}{\huge ?}}{=} \nabla\cdot\big(\nabla_{s}\,u\big)  \\ & = \nabla\cdot\big(P\;\nabla \,u\big)       = \nabla\cdot\Big(\nabla\,u-\big(\vec{n}\cdot\nabla\,u\big)\,\vec{n}\Big) \\ & = \Delta\,u-\left(\nabla\cdot\vec{n}\right)\left(\vec{n}\cdot\nabla u\right)-             \vec{n}\cdot\nabla\left(\vec{n}\cdot\nabla u\right)  \\ & = \Delta u + \kappa\,u_{n} - u_{nn} \end{align}","I am trying to derive an explicit formula for Laplace-Beltrami operator in global Cartesian coordinates for a special case of plane curve. I have found this article , and I would like to match their expression (6) for LB on a curve with the standard definition in terms of metric tensor . According to formula $(6)$ in the paper , Laplace-Beltrami operator on plane curve can be written as \begin{align} \Delta_{LB}\, u & = \Delta u + \kappa\,u_{n} - u_{nn} \\ & = \tag{$\star$} \Delta u + \kappa\,\vec{n}\cdot\nabla u - \vec{n}\cdot\nabla\left(\vec{n}\cdot\nabla u\right)  \end{align} $\,\vec{n}\,$ is unit normal vector, $\,\kappa=-\nabla\cdot\vec{n}\,$ is curvature , $\,u_{n} = \vec{n}\cdot\nabla u\,$ and $\,u_{nn} = \vec{n}\cdot\nabla \left(\vec{n}\cdot\nabla u\right)\,$ are first and second normal derivatives , $\,\nabla u\,$ and $\,\Delta u\,$  are respectively gradient and Laplacian of $\,u\,$. I am having troubles deriving $(\star)$ or matching it with metric tensor expression for LB operator \begin{align}\tag{$\ast$} \Delta_{LB}\, u = \dfrac{1}{\sqrt{\left\lvert g\right\rvert}}\,\partial_i\,\Big(\sqrt{\left\lvert g\right\rvert} \,g^{ij}\,\partial_j \,u \Big) \end{align} I can derive $(\star)$ from the Laplace-Beltrami expression $\,\Delta_{LB}\,u = \nabla_{s}\cdot\big(\nabla_{s}\,u\big)\,$ assuming surface divergence of a vector equals to the regular divergence of its projection to the curve . This is a BIG assumption, and I do not know how to justify it. I will appreciate if someone could help me to justify my assumption, or to derive $(\star)$ without assumptions on (surface) divergence. My attempt to derive $(\star)$: let $\,\nabla_{s}\,$, and $P$ denote surface gradient and projecting operator , then \begin{align} \Delta_{LB}\, u & = \nabla_{s}\cdot\big(\nabla_{s}\,u\big) \stackrel{\color{red}{\huge ?}}{=} \nabla\cdot\big(\nabla_{s}\,u\big)  \\ & = \nabla\cdot\big(P\;\nabla \,u\big)       = \nabla\cdot\Big(\nabla\,u-\big(\vec{n}\cdot\nabla\,u\big)\,\vec{n}\Big) \\ & = \Delta\,u-\left(\nabla\cdot\vec{n}\right)\left(\vec{n}\cdot\nabla u\right)-             \vec{n}\cdot\nabla\left(\vec{n}\cdot\nabla u\right)  \\ & = \Delta u + \kappa\,u_{n} - u_{nn} \end{align}",,"['differential-geometry', 'derivatives', 'plane-curves', 'laplacian', 'differential-operators']"
51,Measure of curve smoothness,Measure of curve smoothness,,"Could someone please give me the intuition behind using integral of squared second derivative as a measure of curve smoothness? I was thinking that since curvature measures how fast a curve changes, should we not be integrating the square of curvature? Basically why are we ignoring the denominator from the definition of curvature before even checking if first derivative is small enough. This is also used in Smoothing Splines so I guess there is something to it then just being a mere approximation.","Could someone please give me the intuition behind using integral of squared second derivative as a measure of curve smoothness? I was thinking that since curvature measures how fast a curve changes, should we not be integrating the square of curvature? Basically why are we ignoring the denominator from the definition of curvature before even checking if first derivative is small enough. This is also used in Smoothing Splines so I guess there is something to it then just being a mere approximation.",,"['derivatives', 'plane-curves', 'spline']"
52,$\sin(40^\circ)<\sqrt{\frac{3}7}$,,\sin(40^\circ)<\sqrt{\frac{3}7},"Prove without using of calculator, that $\sin40^\circ<\sqrt{\frac{3}7}$. My attempt. Since $$\sin(40^\circ)=2\sin(20^\circ)\cos(20^\circ)<2\sin(20^\circ)$$ $$=2\sin(60^\circ-40^\circ)=\sqrt{3} \cos(40^\circ)-\sin(40^\circ),$$ $$2\sin(40^\circ)<\sqrt{3} \cos(40^\circ).$$ Hence, $$4\sin^2(40^\circ)<3\cos^2(40^\circ)=3(1-\sin^2(40^\circ))$$ $$7\sin^2(40^\circ)<3$$ $$\sin(40^\circ)<\sqrt{\frac{3}7}$$ Is there another way to prove this inequality?","Prove without using of calculator, that $\sin40^\circ<\sqrt{\frac{3}7}$. My attempt. Since $$\sin(40^\circ)=2\sin(20^\circ)\cos(20^\circ)<2\sin(20^\circ)$$ $$=2\sin(60^\circ-40^\circ)=\sqrt{3} \cos(40^\circ)-\sin(40^\circ),$$ $$2\sin(40^\circ)<\sqrt{3} \cos(40^\circ).$$ Hence, $$4\sin^2(40^\circ)<3\cos^2(40^\circ)=3(1-\sin^2(40^\circ))$$ $$7\sin^2(40^\circ)<3$$ $$\sin(40^\circ)<\sqrt{\frac{3}7}$$ Is there another way to prove this inequality?",,"['trigonometry', 'derivatives', 'inequality', 'fractions', 'number-comparison']"
53,Let $f : \mathbb{R} \to \mathbb{R}$ be measurable and let $Z = {\{x : f'(x)=0}\}$. Prove that $λ(f(Z)) = 0$.,Let  be measurable and let . Prove that .,f : \mathbb{R} \to \mathbb{R} Z = {\{x : f'(x)=0}\} λ(f(Z)) = 0,"The following is an exercise from Bruckner's Real Analysis: Let $f : \mathbb{R} \to \mathbb{R}$ be measurable and let $Z = {\{x : f'(x)=0}\}$ . Prove  that $λ(f(Z)) = 0$ . For the case $f$ being nondecreasing / nonincreasing, we can defined $g=f^{-1}$ and then $g'$ and then use the following theorem from the book : Let $f$ be nondecreasing / nonincreasing / of bounded variation on $[a,b]$ . Then $f$ has a finite derivative almost everywhere. Is it possible to ""reduce"" evaluation of any measurable $f$ to a nondecreasing one or otherwise how the claim can be proved for any measurable $f$ ?","The following is an exercise from Bruckner's Real Analysis: Let be measurable and let . Prove  that . For the case being nondecreasing / nonincreasing, we can defined and then and then use the following theorem from the book : Let be nondecreasing / nonincreasing / of bounded variation on . Then has a finite derivative almost everywhere. Is it possible to ""reduce"" evaluation of any measurable to a nondecreasing one or otherwise how the claim can be proved for any measurable ?","f : \mathbb{R} \to \mathbb{R} Z = {\{x : f'(x)=0}\} λ(f(Z)) = 0 f g=f^{-1} g' f [a,b] f f f",['derivatives']
54,Total Derivatives and Total Differential,Total Derivatives and Total Differential,,I am confused between total derivatives and total differential. What is  the difference between total derivatives and  total differential?,I am confused between total derivatives and total differential. What is  the difference between total derivatives and  total differential?,,['derivatives']
55,Ratio of CDF to PDF increasing?,Ratio of CDF to PDF increasing?,,Let $\Phi(x)$ be a cumulative normal distribution function and $\phi(x)$ the associated probability density function. Is the ratio $\frac{\Phi(x)}{\phi(x)}$ increasing in x? Numerically it seems to be true. Is there any ways to prove it analytically? Thanks,Let $\Phi(x)$ be a cumulative normal distribution function and $\phi(x)$ the associated probability density function. Is the ratio $\frac{\Phi(x)}{\phi(x)}$ increasing in x? Numerically it seems to be true. Is there any ways to prove it analytically? Thanks,,"['derivatives', 'normal-distribution']"
56,Derivative of a vector with Respect to scalar?,Derivative of a vector with Respect to scalar?,,I have a function $f=w\begin{bmatrix} a \\b\\c \end{bmatrix}$. So what is the $\frac{\partial f}{\partial w}$?  I have never seen this. Thank you!,I have a function $f=w\begin{bmatrix} a \\b\\c \end{bmatrix}$. So what is the $\frac{\partial f}{\partial w}$?  I have never seen this. Thank you!,,"['derivatives', 'partial-derivative']"
57,How to deduce the recursive derivative formula of B-spline basis?,How to deduce the recursive derivative formula of B-spline basis?,,"Description Let $\vec{U}=\{u_0,u_1,\ldots,u_m\}$ denotes a non-decreasing sequence of real numbers, i.e, $u_i\leq u_{i+1} \quad i=0,1,2\ldots m-1$. and the $i$-th B-spline basis function of $p$-degree, denoted by $N_{i,p}(u)$, is defined as below: $$ N_{i,0}(u)= \begin{cases}    1 & u_i\leq u<u_{i+1}\\    0 & otherwise \end{cases} $$ $$ N_{i,p}(u)= \frac{u-u_i}{u_{i+p}-u_i}N_{i,p-1}(u)+\frac{u_{i+p+1}-u}{u_{i+p+1}-u_{i+1}}N_{i+1,p-1}(u) $$ By reading the textbook The NURBS Book, I can know the following recursive formula about the derivetive of $N_{i,p}(u)$. $$ \frac{d}{du}N_{i,p}(u)=p\left[ \frac{N_{i,p-1}(u)}{u_{i+p}-u_i}-\frac{N_{i+1,p-1}(u)}{u_{i+p+1}-u_{i+1}} \right] \qquad (1) $$ In addition, I can also understand the verification process by mathematical induction that the author given in the textbook pp.59-60. Namely, (1) Varifying the correctness of this recursive formula for $p=1$; (2) Assuming this formula is right for $p=k$, then proved that this formula is also right for $p=k+1$ with help of the assumption. However, I would like to know where this formula came from . The author just given the conclusion and proved it by mathematical induction . QUESTION How to deduce the derivative formmula of the B-spline basis function $N_{i,p}(u)$? Although the author has given a related reference The Computation of all the Derivatives of a B-spline Basis in the bibliography, I cannot download that paper by the libriary of our university. In addition, the reference just for another recursive formula(please see Eq.(2) ), not for Eq.(1) . $$ N_{i,p}^{(k)}=\frac{p}{p-k}\left(\frac{u-u_i}{u_{i+p}-u_i}N_{i,p-1}^{(k)}+\frac{u_{i+p+1}-u}{u_{i+p+1}-u_{i+1}}N_{i,p-1}^{(k)}\right) \quad (2) $$ where $k=0,1,\cdots,p-1$ Lastly, I discovered that Eq.(1) was useful than Eq.(2) , and it was implemented in Wolfram Mathematica . For instance, knots = {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}; D[BSplineBasis[{3, knots}, 2, x], x] (*9/2 BSplineBasis[{2, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 2, x] -      3 BSplineBasis[{2, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 3, x]*) D[BSplineBasis[{3, knots}, 2, x], {x, 2}] (*9/2 (6 BSplineBasis[{1, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 2, x] -         3 BSplineBasis[{1, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 3, x]) -    3 (3 BSplineBasis[{1, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 3, x] -       3 BSplineBasis[{1, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 4, x])*) Thanks a lot! :)","Description Let $\vec{U}=\{u_0,u_1,\ldots,u_m\}$ denotes a non-decreasing sequence of real numbers, i.e, $u_i\leq u_{i+1} \quad i=0,1,2\ldots m-1$. and the $i$-th B-spline basis function of $p$-degree, denoted by $N_{i,p}(u)$, is defined as below: $$ N_{i,0}(u)= \begin{cases}    1 & u_i\leq u<u_{i+1}\\    0 & otherwise \end{cases} $$ $$ N_{i,p}(u)= \frac{u-u_i}{u_{i+p}-u_i}N_{i,p-1}(u)+\frac{u_{i+p+1}-u}{u_{i+p+1}-u_{i+1}}N_{i+1,p-1}(u) $$ By reading the textbook The NURBS Book, I can know the following recursive formula about the derivetive of $N_{i,p}(u)$. $$ \frac{d}{du}N_{i,p}(u)=p\left[ \frac{N_{i,p-1}(u)}{u_{i+p}-u_i}-\frac{N_{i+1,p-1}(u)}{u_{i+p+1}-u_{i+1}} \right] \qquad (1) $$ In addition, I can also understand the verification process by mathematical induction that the author given in the textbook pp.59-60. Namely, (1) Varifying the correctness of this recursive formula for $p=1$; (2) Assuming this formula is right for $p=k$, then proved that this formula is also right for $p=k+1$ with help of the assumption. However, I would like to know where this formula came from . The author just given the conclusion and proved it by mathematical induction . QUESTION How to deduce the derivative formmula of the B-spline basis function $N_{i,p}(u)$? Although the author has given a related reference The Computation of all the Derivatives of a B-spline Basis in the bibliography, I cannot download that paper by the libriary of our university. In addition, the reference just for another recursive formula(please see Eq.(2) ), not for Eq.(1) . $$ N_{i,p}^{(k)}=\frac{p}{p-k}\left(\frac{u-u_i}{u_{i+p}-u_i}N_{i,p-1}^{(k)}+\frac{u_{i+p+1}-u}{u_{i+p+1}-u_{i+1}}N_{i,p-1}^{(k)}\right) \quad (2) $$ where $k=0,1,\cdots,p-1$ Lastly, I discovered that Eq.(1) was useful than Eq.(2) , and it was implemented in Wolfram Mathematica . For instance, knots = {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}; D[BSplineBasis[{3, knots}, 2, x], x] (*9/2 BSplineBasis[{2, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 2, x] -      3 BSplineBasis[{2, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 3, x]*) D[BSplineBasis[{3, knots}, 2, x], {x, 2}] (*9/2 (6 BSplineBasis[{1, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 2, x] -         3 BSplineBasis[{1, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 3, x]) -    3 (3 BSplineBasis[{1, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 3, x] -       3 BSplineBasis[{1, {0, 0, 0, 0, 1/3, 2/3, 1, 1, 1, 1}}, 4, x])*) Thanks a lot! :)",,"['derivatives', 'recursion']"
58,general formula for the nth derivative of $f(x)=\frac{1}{1+e^{x}}$,general formula for the nth derivative of,f(x)=\frac{1}{1+e^{x}},"consider the function : $$f(x)=\frac{1}{1+e^{x}}$$ the nth derivative of the function is given by the following formula: $$f^{(n)} (x)=\sum_{k=1}^{n+1}a_{n,k}\frac{1}{\left(1+e^{x}\right)^{k}}$$ where $$a_{n,k}=\left(-1\right)^{n}\sum_{j=0}^{k-1}\left(-1\right)^{j}{{k-1}\choose{j}}\left(j+1\right)^{n}$$ my question is that:how the formula can be derived without using induction? I have no idea about that, so any hint or full proof would be highly appreciated.","consider the function : the nth derivative of the function is given by the following formula: where my question is that:how the formula can be derived without using induction? I have no idea about that, so any hint or full proof would be highly appreciated.","f(x)=\frac{1}{1+e^{x}} f^{(n)} (x)=\sum_{k=1}^{n+1}a_{n,k}\frac{1}{\left(1+e^{x}\right)^{k}} a_{n,k}=\left(-1\right)^{n}\sum_{j=0}^{k-1}\left(-1\right)^{j}{{k-1}\choose{j}}\left(j+1\right)^{n}","['derivatives', 'proof-writing']"
59,General formula for the higher order derivatives of composition with exponential function,General formula for the higher order derivatives of composition with exponential function,,"Suppose I have a function $x:\mathbb{R} \to \mathbb{R}$ and consider: $$g(t) = e^{x(t)}$$ When I start differentiating with respect to $t$ I obtain: \begin{align} g'&=e^xx'\\ g''&=e^x((x')^2+x'')\\ g'''&=e^x((x')^3+3x'x''+x''')\\ &... \end{align} My question is whether there is a reasonable expression for higher derivatives of $g$ as a function of $x$. I'm pretty sure this kind of a thing has its own name, but I cannot find anything... Thank you in advance!","Suppose I have a function $x:\mathbb{R} \to \mathbb{R}$ and consider: $$g(t) = e^{x(t)}$$ When I start differentiating with respect to $t$ I obtain: \begin{align} g'&=e^xx'\\ g''&=e^x((x')^2+x'')\\ g'''&=e^x((x')^3+3x'x''+x''')\\ &... \end{align} My question is whether there is a reasonable expression for higher derivatives of $g$ as a function of $x$. I'm pretty sure this kind of a thing has its own name, but I cannot find anything... Thank you in advance!",,"['derivatives', 'exponential-function', 'closed-form']"
60,Chain rule characterizes the derivative?,Chain rule characterizes the derivative?,,"To what extend does the chain rule $(f\circ g)'=(f'\circ g)\cdot g'$ characterize the derivative $(\ldots)'$? More precisely: If a non-constant operator $T$ defined on all differentiable functions, satisfies the relation  $$T(f\circ g)=((Tf)\circ g)\cdot T g\,,$$ does this imply that $T$ is the derivative? Motivation: I am particularly interested in the following questions: What kind of operators are obtained when the defining relation is something like  $$(T(f\circ g))(x)=((Tf)(g(x))^{(T g)(x)}\,?$$ Is there a somewhat standard way to solve these equations for $T$? What branch of mathematics deals with these kinds of equations? Is there a standard representation we can fill in for $T$, to solve these equations? (like substituting a Taylor series into a differential equation)","To what extend does the chain rule $(f\circ g)'=(f'\circ g)\cdot g'$ characterize the derivative $(\ldots)'$? More precisely: If a non-constant operator $T$ defined on all differentiable functions, satisfies the relation  $$T(f\circ g)=((Tf)\circ g)\cdot T g\,,$$ does this imply that $T$ is the derivative? Motivation: I am particularly interested in the following questions: What kind of operators are obtained when the defining relation is something like  $$(T(f\circ g))(x)=((Tf)(g(x))^{(T g)(x)}\,?$$ Is there a somewhat standard way to solve these equations for $T$? What branch of mathematics deals with these kinds of equations? Is there a standard representation we can fill in for $T$, to solve these equations? (like substituting a Taylor series into a differential equation)",,"['derivatives', 'chain-rule']"
61,Derivatives in differential geometry,Derivatives in differential geometry,,"I am really attracted by the field of differential geometry which generalize analysis on euclidean spaces that I've been working with my whole life. However by learning the field I encountered different notion of derivatives, namely : The tangent vector ( directional derivative of a function ) The ehresman connection ( derivative of a section ) The covariant derivative The exterior derivative ( derivative on the graded antisymmetric multilinear algebra of differential forms ) The exterior covariant derivative (Used to define curvature from a one form ? The Lie derivative ( derivative along curves of vector and tensors fields ) First of all I know I lack knowledge and intuition on all of them. I know the definition of them, I know their difference, to what object they act on, what info we need to use them etc. I am not looking here for definitions. I would like to know why do we need so many definition and approach to derivatives. I would hope to have a general version of derivatives that will apply to all objects but that doesn't seem to be the case here. What are the need for all of them? In what case do we need one and not the others ? Also I would really appreciate a references that would list all of thoses derivatives, minor intuition on them and also explain why and when do we have to use one or the other. PS: I have already saw most of the other responses on this forum thanks.","I am really attracted by the field of differential geometry which generalize analysis on euclidean spaces that I've been working with my whole life. However by learning the field I encountered different notion of derivatives, namely : The tangent vector ( directional derivative of a function ) The ehresman connection ( derivative of a section ) The covariant derivative The exterior derivative ( derivative on the graded antisymmetric multilinear algebra of differential forms ) The exterior covariant derivative (Used to define curvature from a one form ? The Lie derivative ( derivative along curves of vector and tensors fields ) First of all I know I lack knowledge and intuition on all of them. I know the definition of them, I know their difference, to what object they act on, what info we need to use them etc. I am not looking here for definitions. I would like to know why do we need so many definition and approach to derivatives. I would hope to have a general version of derivatives that will apply to all objects but that doesn't seem to be the case here. What are the need for all of them? In what case do we need one and not the others ? Also I would really appreciate a references that would list all of thoses derivatives, minor intuition on them and also explain why and when do we have to use one or the other. PS: I have already saw most of the other responses on this forum thanks.",,"['differential-geometry', 'derivatives', 'reference-request']"
62,Names of higher-order derivatives,Names of higher-order derivatives,,"Specific derivatives have specific names. First order is often called tangency/velocity, second order is curvature/acceleration. I've also come across words like Jerk, Yank, Jounce, Jolt, Surge and Lurch for 3rd and 4th order derivatives. Is there a widely agreed list of names for these? How many orders have specific names? In this case, I'm dealing with NURBs curves, so the ""tangency"" and ""curvature"" related words are to be preferred over ""velocity"" and ""acceleration"" words.","Specific derivatives have specific names. First order is often called tangency/velocity, second order is curvature/acceleration. I've also come across words like Jerk, Yank, Jounce, Jolt, Surge and Lurch for 3rd and 4th order derivatives. Is there a widely agreed list of names for these? How many orders have specific names? In this case, I'm dealing with NURBs curves, so the ""tangency"" and ""curvature"" related words are to be preferred over ""velocity"" and ""acceleration"" words.",,"['derivatives', 'terminology']"
63,differentiation of 1-norm of a vector,differentiation of 1-norm of a vector,,Assume you want to find the minimum of the following expression $\|x\|_1 + \alpha \|Ax-b\|_2$ where $x\in R^N$. So basically I want to calculate the derivative of $\|x\|_1$ so I could finally set the equation to zero and solve for $x^*$. What is the best approach to tackle this problem?,Assume you want to find the minimum of the following expression $\|x\|_1 + \alpha \|Ax-b\|_2$ where $x\in R^N$. So basically I want to calculate the derivative of $\|x\|_1$ so I could finally set the equation to zero and solve for $x^*$. What is the best approach to tackle this problem?,,"['derivatives', 'normed-spaces', 'matrix-calculus']"
64,"Is $\max(0, x)$ a differentiable function?",Is  a differentiable function?,"\max(0, x)","It appears that $\max(x, y)$ isn't differentiable according to this question . However, the explanation is due to the fact that $\max(x, -x) = \lvert x\rvert$, and since there won't be the case $\max(0, -0)$, does this mean that this function is differentiable?","It appears that $\max(x, y)$ isn't differentiable according to this question . However, the explanation is due to the fact that $\max(x, -x) = \lvert x\rvert$, and since there won't be the case $\max(0, -0)$, does this mean that this function is differentiable?",,['derivatives']
65,What do you get when you differentiate a $e^{f(x)}$-like function,What do you get when you differentiate a -like function,e^{f(x)},"I need help with exponential functions. I know that the derivative of $e^x$ is $e^x$, but wolfram alpha shows a different answer to my function below. If you, for example, take the derivative of $e^{-2x}$ do you get $-2e^{-2x}$ or $e^{-2x}$?","I need help with exponential functions. I know that the derivative of $e^x$ is $e^x$, but wolfram alpha shows a different answer to my function below. If you, for example, take the derivative of $e^{-2x}$ do you get $-2e^{-2x}$ or $e^{-2x}$?",,"['derivatives', 'exponential-function']"
66,Can exponential functions be thought of as eigenfunctions for the derivative operator?,Can exponential functions be thought of as eigenfunctions for the derivative operator?,,"I.e. is the function $y=b^{kx}$ an eigenfunction for the derivative operator $\frac{dy}{dx}$, where k is a constant because the derivative of such a function is ${k\ln(b)}b^{kx}$, which is a constant ($k\ln(b)$) times the original function ($b^{kx}$.) Does it even make sense to say this? If it does, are there any other such eigenfunctions for the derivative operator?","I.e. is the function $y=b^{kx}$ an eigenfunction for the derivative operator $\frac{dy}{dx}$, where k is a constant because the derivative of such a function is ${k\ln(b)}b^{kx}$, which is a constant ($k\ln(b)$) times the original function ($b^{kx}$.) Does it even make sense to say this? If it does, are there any other such eigenfunctions for the derivative operator?",,"['derivatives', 'eigenfunctions']"
67,derivative of indicator function,derivative of indicator function,,"I have an indicator function $I(D\leq Q)$which is equal to $1$ if $D\leq Q$ and $0$ otherwise. What would be derivative of this function with respect to different variables such as $D$ or $Q$ or $P$ ($D$ is a function of $P$). Clarification to what I am trying to do: $D$ represents demand which is a function of price, assume $D=a-bp$ $Q$ represents quantity or supply, which is assumed to be fixed $$\text{profit} = p\min(D,Q)= PDI(D\lt Q)+PQI(Q\leq D)$$ I want to take derivative of profit with respect to price. Thanks in advance","I have an indicator function $I(D\leq Q)$which is equal to $1$ if $D\leq Q$ and $0$ otherwise. What would be derivative of this function with respect to different variables such as $D$ or $Q$ or $P$ ($D$ is a function of $P$). Clarification to what I am trying to do: $D$ represents demand which is a function of price, assume $D=a-bp$ $Q$ represents quantity or supply, which is assumed to be fixed $$\text{profit} = p\min(D,Q)= PDI(D\lt Q)+PQI(Q\leq D)$$ I want to take derivative of profit with respect to price. Thanks in advance",,['derivatives']
68,Why is central difference preferred over backward and forward difference in convolution?,Why is central difference preferred over backward and forward difference in convolution?,,It is mentioned in some literature that we should always use central difference when computing the derivatives of an image instead of forward or backward difference. Does anyone knows why is that? Central difference = $\frac{df(x)}{dx} = \frac{f(x+h) - f(x-h)}{2h}$ Forward difference = $\frac{df(x)}{dx} = \frac{f(x+h) - f(x)}{h}$ Backward difference = $\frac{df(x)}{dx} = \frac{f(x) - f(x-h)}{h}$,It is mentioned in some literature that we should always use central difference when computing the derivatives of an image instead of forward or backward difference. Does anyone knows why is that? Central difference = Forward difference = Backward difference =,\frac{df(x)}{dx} = \frac{f(x+h) - f(x-h)}{2h} \frac{df(x)}{dx} = \frac{f(x+h) - f(x)}{h} \frac{df(x)}{dx} = \frac{f(x) - f(x-h)}{h},"['derivatives', 'numerical-methods', 'image-processing']"
69,Derivative of infinite sum,Derivative of infinite sum,,"I was thinking about derivative of infinite sum of functions, i.e. $$f(x) = \sum_{i = 0}^\infty g_i(x)$$ $g(x)$ is continuous in domain of $f$ Because if $(f+g)'(x) = f'(x) + g'(x)$ then $\left(\sum\limits_{i = 0}^{\infty} g_i(x)\right)' = \sum\limits_{i = 0}^{\infty} g_i'(x)$ isn't it?","I was thinking about derivative of infinite sum of functions, i.e. $$f(x) = \sum_{i = 0}^\infty g_i(x)$$ $g(x)$ is continuous in domain of $f$ Because if $(f+g)'(x) = f'(x) + g'(x)$ then $\left(\sum\limits_{i = 0}^{\infty} g_i(x)\right)' = \sum\limits_{i = 0}^{\infty} g_i'(x)$ isn't it?",,['derivatives']
70,Show by hand $\int_{0}^{\sqrt{5}}x^{x}dx>4$,Show by hand,\int_{0}^{\sqrt{5}}x^{x}dx>4,It's a very Challenging question perhaps a kind of olympiad question : Prove that : $$\int_{0}^{\sqrt{5}}x^{x}dx>4$$ It's pretty sharp since the left hand side is almost $4.0005$ . I recall the Sophomore dream : $$\int_{0}^{1}x^{x}dx=-\sum_{n=1}^{\infty}\left(-n\right)^{-n}$$ For the other part we have : $$\int_{1}^{\sqrt{5}}x^{x}dx\geq \int_{1}^{\sqrt{5}}\left(1-\sqrt{x}+x\right)^{1+x}dx$$ Because I show in another question the inequality $x>0$ : $$\left(1-\sqrt{x}+x\right)^{1+x}\leq x^x$$ But we are far from the goal . We have numerically : $$\int_{1}^{1.5}x^{x}dx>\int_{1}^{1.5}\left(e^{\left(-1+\frac{\left(1+x+x^{2}\right)}{3}\right)}+\frac{1}{6}\left(x-1\right)^{2}+\frac{1}{26}\left(x-1\right)^{4}\right)dx$$ How to show it by hand or make a proof where the final steps is the calculus of some constant ?,It's a very Challenging question perhaps a kind of olympiad question : Prove that : It's pretty sharp since the left hand side is almost . I recall the Sophomore dream : For the other part we have : Because I show in another question the inequality : But we are far from the goal . We have numerically : How to show it by hand or make a proof where the final steps is the calculus of some constant ?,\int_{0}^{\sqrt{5}}x^{x}dx>4 4.0005 \int_{0}^{1}x^{x}dx=-\sum_{n=1}^{\infty}\left(-n\right)^{-n} \int_{1}^{\sqrt{5}}x^{x}dx\geq \int_{1}^{\sqrt{5}}\left(1-\sqrt{x}+x\right)^{1+x}dx x>0 \left(1-\sqrt{x}+x\right)^{1+x}\leq x^x \int_{1}^{1.5}x^{x}dx>\int_{1}^{1.5}\left(e^{\left(-1+\frac{\left(1+x+x^{2}\right)}{3}\right)}+\frac{1}{6}\left(x-1\right)^{2}+\frac{1}{26}\left(x-1\right)^{4}\right)dx,"['derivatives', 'inequality', 'definite-integrals', 'constants']"
71,Derivative of Euclidean norm (L2 norm),Derivative of Euclidean norm (L2 norm),,"Let:$x=[x_1, x_2]$ and $y = [y_1, y_2]$ What is the derivative of the square of the Euclidean norm of $y-x $? I'm not sure if I've worded the question correctly, but this is what I'm trying to solve: $$ \frac{d}{dx}(||y-x||^2) $$ It has been a long time since I've taken a math class, but this is what I've done so far: $$ \frac{d}{dx}(||y-x||^2)=\frac{d}{dx}(||[y_1,y_2]-[x_1,x_2]||^2) $$ Subtracting $x $ from $y$: $$ \frac{d}{dx}(||y-x||^2)=\frac{d}{dx}(||[y_1-x_1,y_2-x_2]||^2) $$ Taking the norm: $$ \frac{d}{dx}(||y-x||^2)=\frac{d}{dx}((y_1-x_1)^2+(y_2-x_2)^2) $$ Then at this point do I take the derivative independently for $x_1$ and $x_2$? This is where I am guessing: $$ \frac{d}{dx}(||y-x||^2)=[\frac{d}{dx_1}((y_1-x_1)^2+(y_2-x_2)^2),\frac{d}{dx_2}((y_1-x_1)^2+(y_2-x_2)^2)] $$ Which would result in: $$ \frac{d}{dx}(||y-x||^2)=[2x_1-2y_1,2x_2-2y_2] $$ Is this correct? Thank you for your time.","Let:$x=[x_1, x_2]$ and $y = [y_1, y_2]$ What is the derivative of the square of the Euclidean norm of $y-x $? I'm not sure if I've worded the question correctly, but this is what I'm trying to solve: $$ \frac{d}{dx}(||y-x||^2) $$ It has been a long time since I've taken a math class, but this is what I've done so far: $$ \frac{d}{dx}(||y-x||^2)=\frac{d}{dx}(||[y_1,y_2]-[x_1,x_2]||^2) $$ Subtracting $x $ from $y$: $$ \frac{d}{dx}(||y-x||^2)=\frac{d}{dx}(||[y_1-x_1,y_2-x_2]||^2) $$ Taking the norm: $$ \frac{d}{dx}(||y-x||^2)=\frac{d}{dx}((y_1-x_1)^2+(y_2-x_2)^2) $$ Then at this point do I take the derivative independently for $x_1$ and $x_2$? This is where I am guessing: $$ \frac{d}{dx}(||y-x||^2)=[\frac{d}{dx_1}((y_1-x_1)^2+(y_2-x_2)^2),\frac{d}{dx_2}((y_1-x_1)^2+(y_2-x_2)^2)] $$ Which would result in: $$ \frac{d}{dx}(||y-x||^2)=[2x_1-2y_1,2x_2-2y_2] $$ Is this correct? Thank you for your time.",,"['derivatives', 'normed-spaces']"
72,Neural Network - Why use Derivative,Neural Network - Why use Derivative,,"Good Day I am trying to get an understanding of Neural Network. Have gone through few web sites. Came to know the following: 1)  One of main objective of neural network is to “predict” based on data. 2)  To predict a.  Train the network with known data  b.  Calculate weights by finding difference between “Target Output” and “Calculated Output”. c.  To do that we use derivative, partial derivative(chain rule etc..) I can understand the overall concept of neural network  a)  I can also understand “Derivative” is nothing but Rate of change of one quantity over another(at a given point). b)  Partial derivative is Rate of change of one quantity over another, irrespective of another quantity , if more than two factors are in equation. The point that I canNOT relate or understand clearly is, a)  why should we use derivative in neural network, how exactly does it help b)  Why should we activation function, in most cases its Sigmoid function. c)  I could not get a complete picture of how derivatives helps neural network. Can you guys please help me understand the complete picture, iff possible try not to use mathematical terms, so that it will be easy for me to grasp. Thanks, Satheesh","Good Day I am trying to get an understanding of Neural Network. Have gone through few web sites. Came to know the following: 1)  One of main objective of neural network is to “predict” based on data. 2)  To predict a.  Train the network with known data  b.  Calculate weights by finding difference between “Target Output” and “Calculated Output”. c.  To do that we use derivative, partial derivative(chain rule etc..) I can understand the overall concept of neural network  a)  I can also understand “Derivative” is nothing but Rate of change of one quantity over another(at a given point). b)  Partial derivative is Rate of change of one quantity over another, irrespective of another quantity , if more than two factors are in equation. The point that I canNOT relate or understand clearly is, a)  why should we use derivative in neural network, how exactly does it help b)  Why should we activation function, in most cases its Sigmoid function. c)  I could not get a complete picture of how derivatives helps neural network. Can you guys please help me understand the complete picture, iff possible try not to use mathematical terms, so that it will be easy for me to grasp. Thanks, Satheesh",,"['derivatives', 'neural-networks']"
73,Math Joke about Differentiation,Math Joke about Differentiation,,"So I recently read a joke that goes like this: A constant function and $e^x$ are walking on Broadway. Then suddenly the constant function sees a differential operator approaching and runs away. So $e^x$ follows him and asks why the hurry. ""Well, you see, there's this differential operator coming this way, and when we meet, he'll differentiate me and nothing will be left of me...!"" ""Ah,"" says $e^x$ , ""he won't bother ME, I'm e to the x!"" and he walks on. Of course he meets the differential operator after a short distance. ex: ""Hi, I'm $e^x$ "" diff.op.: ""Hi, I'm $d\over dy$ "" What I dont understand is, Isn't ${d\over dy}\left(e^x\right)$ supposed to be $e^x {dx\over dy}$ because of the Chain rule? So what does the joke mean?","So I recently read a joke that goes like this: A constant function and are walking on Broadway. Then suddenly the constant function sees a differential operator approaching and runs away. So follows him and asks why the hurry. ""Well, you see, there's this differential operator coming this way, and when we meet, he'll differentiate me and nothing will be left of me...!"" ""Ah,"" says , ""he won't bother ME, I'm e to the x!"" and he walks on. Of course he meets the differential operator after a short distance. ex: ""Hi, I'm "" diff.op.: ""Hi, I'm "" What I dont understand is, Isn't supposed to be because of the Chain rule? So what does the joke mean?",e^x e^x e^x e^x d\over dy {d\over dy}\left(e^x\right) e^x {dx\over dy},"['soft-question', 'derivatives']"
74,How to derive the equation for a bézier curve,How to derive the equation for a bézier curve,,"So, I remember a while back there was a maths competition and we were given a curve that we needed to write an equation for. I just skipped the question since I didn't even know where to begin. I remember it was one among the last few questions of the paper and it was worth a lot of points. I don't really remember what the curve looked like; it was something spirally, but I can't recall it to save my life right now. So, I drew this curve in Inkscape (it's a Bézier curve. Or a few of them linked together, according to Wikipedia. If it's required I will post the whole path). And I would like to write the equation for it (with someone's help, obviously). I was always a bit bad with curves, graphs and lines, but I want to understand them better. So, I was hoping someone could explain the process of deriving the equation for a curve. P.S: I'd like it if you could use another curve (it can be something simpler, but try avoiding something overly complicated) so I can crack this one on my own, but if you feel like using this curve as an example I won't mind. EDIT So  have been browsing the internet, read a few Wikipedia entries about Bazier curves, and I understand how they're drawn (mostly the GIFs helped, haha), but I am still stumped when it comes to mathematically representing a Bézier curve. Also, I will add this image, which is the path and its control points (at the end of the blue lines; I didn't paint them in): And also, the contents of the .tex file for the shape. %LaTeX with PSTricks extensions %%Creator: 0.48.2 %%Please note this file requires PSTricks extensions \psset{xunit=.5pt,yunit=.5pt,runit=.5pt} \begin{pspicture}(451.46875,34.25392151) 	{ 	\newrgbcolor{curcolor}{1 0 0} 	\pscustom[linewidth=3,linecolor=curcolor] 	{ 	\newpath 	\moveto(450.48448,1.10834551) 	\curveto(404.89404,41.45133951)(333.34998,42.21654151)(281.90128,9.03018551) 	\curveto(258.09407,-6.32636849)(228.42388,9.91159551)(202.75741,15.38398551) 	\curveto(145.68728,27.55199551)(85.852286,40.32786151)(28.08402514,26.23698551) 	\curveto(18.5710181,23.91656551)(9.403556,20.24334551)(0.681686,15.78116551) 	} 	} 	\end{pspicture} Thanks!","So, I remember a while back there was a maths competition and we were given a curve that we needed to write an equation for. I just skipped the question since I didn't even know where to begin. I remember it was one among the last few questions of the paper and it was worth a lot of points. I don't really remember what the curve looked like; it was something spirally, but I can't recall it to save my life right now. So, I drew this curve in Inkscape (it's a Bézier curve. Or a few of them linked together, according to Wikipedia. If it's required I will post the whole path). And I would like to write the equation for it (with someone's help, obviously). I was always a bit bad with curves, graphs and lines, but I want to understand them better. So, I was hoping someone could explain the process of deriving the equation for a curve. P.S: I'd like it if you could use another curve (it can be something simpler, but try avoiding something overly complicated) so I can crack this one on my own, but if you feel like using this curve as an example I won't mind. EDIT So  have been browsing the internet, read a few Wikipedia entries about Bazier curves, and I understand how they're drawn (mostly the GIFs helped, haha), but I am still stumped when it comes to mathematically representing a Bézier curve. Also, I will add this image, which is the path and its control points (at the end of the blue lines; I didn't paint them in): And also, the contents of the .tex file for the shape. %LaTeX with PSTricks extensions %%Creator: 0.48.2 %%Please note this file requires PSTricks extensions \psset{xunit=.5pt,yunit=.5pt,runit=.5pt} \begin{pspicture}(451.46875,34.25392151) 	{ 	\newrgbcolor{curcolor}{1 0 0} 	\pscustom[linewidth=3,linecolor=curcolor] 	{ 	\newpath 	\moveto(450.48448,1.10834551) 	\curveto(404.89404,41.45133951)(333.34998,42.21654151)(281.90128,9.03018551) 	\curveto(258.09407,-6.32636849)(228.42388,9.91159551)(202.75741,15.38398551) 	\curveto(145.68728,27.55199551)(85.852286,40.32786151)(28.08402514,26.23698551) 	\curveto(18.5710181,23.91656551)(9.403556,20.24334551)(0.681686,15.78116551) 	} 	} 	\end{pspicture} Thanks!",,"['derivatives', 'graphing-functions', 'bezier-curve']"
75,Differentiation Chain Rule for Quaternion Functions of Quaternions,Differentiation Chain Rule for Quaternion Functions of Quaternions,,"I am trying to calculate the time derivative of a quaternion function: $$\mathbf{p}\left(t\right)=\left(\mathbf{q}\left(t\right)\right)^{\tau}$$ Where I've used bold font to indicate that $\mathbf{p}$ and $\mathbf{q}$ are each quaternions, while $\tau$ is just a scalar (not limited to integers). What I would like to be able to do is apply some sort of effective chain rule for quaternionic differentiation to calculate the time derivative: $$\dot{\mathbf{p}}=\frac{\partial\mathbf{p}}{\partial\mathbf{q}}\cdot\dot{\mathbf{q}}$$ Where the dot operator ($\cdot$) represents the usual quaternion multiplication. With some effort, I believe that I have shown that $\mathbf{p}$ is quaternionic holomorphic and that its quaternionic derivative with respect to $\mathbf{q}$ is exactly what you'd expect: $$\frac{\partial\mathbf{p}}{\partial\mathbf{q}} = \tau\mathbf{q}^{-1}\cdot\mathbf{p} = \tau\mathbf{p}\cdot\mathbf{q}^{-1}=\tau\mathbf{q}^{\tau-1}$$ Unfortunately, when I try different functions for $\mathbf{q}\left(t\right)$ and the above chain rule, I don't get the actual rate of change of the function $\mathbf{p}\left(t\right)$, which I am estimating numerically. I understand that, if I really want to, I should be able to calculate the full time derivative of $\mathbf{p}$ without using the chain rule. But life would be a lot easier for me if I didn't have to resort to that in all cases. Is there a quaternionic version of the chain rule that I can use to simplify some of my research? Please help! If you're interested, here's a link to a PDF where I verify the quaternionic holomorphicity of $\mathbf{p}$: https://www.scribd.com/document/352235082/Quaternionic-Holomorphicity-Verification","I am trying to calculate the time derivative of a quaternion function: $$\mathbf{p}\left(t\right)=\left(\mathbf{q}\left(t\right)\right)^{\tau}$$ Where I've used bold font to indicate that $\mathbf{p}$ and $\mathbf{q}$ are each quaternions, while $\tau$ is just a scalar (not limited to integers). What I would like to be able to do is apply some sort of effective chain rule for quaternionic differentiation to calculate the time derivative: $$\dot{\mathbf{p}}=\frac{\partial\mathbf{p}}{\partial\mathbf{q}}\cdot\dot{\mathbf{q}}$$ Where the dot operator ($\cdot$) represents the usual quaternion multiplication. With some effort, I believe that I have shown that $\mathbf{p}$ is quaternionic holomorphic and that its quaternionic derivative with respect to $\mathbf{q}$ is exactly what you'd expect: $$\frac{\partial\mathbf{p}}{\partial\mathbf{q}} = \tau\mathbf{q}^{-1}\cdot\mathbf{p} = \tau\mathbf{p}\cdot\mathbf{q}^{-1}=\tau\mathbf{q}^{\tau-1}$$ Unfortunately, when I try different functions for $\mathbf{q}\left(t\right)$ and the above chain rule, I don't get the actual rate of change of the function $\mathbf{p}\left(t\right)$, which I am estimating numerically. I understand that, if I really want to, I should be able to calculate the full time derivative of $\mathbf{p}$ without using the chain rule. But life would be a lot easier for me if I didn't have to resort to that in all cases. Is there a quaternionic version of the chain rule that I can use to simplify some of my research? Please help! If you're interested, here's a link to a PDF where I verify the quaternionic holomorphicity of $\mathbf{p}$: https://www.scribd.com/document/352235082/Quaternionic-Holomorphicity-Verification",,"['derivatives', 'quaternions', 'chain-rule']"
76,Why is the exterior derivative called exterior derivative,Why is the exterior derivative called exterior derivative,,"I am studying exterior calculus, and I think I have some grasp of what is the exterior derivative. However its name still eludes me - why is it called a derivative? Is it just because the operator $d$ satisfies the Leibniz rule? I feel am looking for a way to grasp it as I understand most derivation operators - as measuring some ""infinitesimal change"" in some sense. Am I completely misguided?","I am studying exterior calculus, and I think I have some grasp of what is the exterior derivative. However its name still eludes me - why is it called a derivative? Is it just because the operator $d$ satisfies the Leibniz rule? I feel am looking for a way to grasp it as I understand most derivation operators - as measuring some ""infinitesimal change"" in some sense. Am I completely misguided?",,"['derivatives', 'terminology', 'differential-forms', 'exterior-algebra', 'exterior-derivative']"
77,Prove that $a^{4b}+b^{4a}\geq \frac{1}{2}$,Prove that,a^{4b}+b^{4a}\geq \frac{1}{2},"Inspired by a problem of Vasile Cirtoaje I propose this : Let $a,b>0$ such that $a+b=1$ then we have : $$a^{4b}+b^{4a}\geq \frac{1}{2}$$ I compute the derivative of $f(x)=x^{4(1-x)}+(1-x)^{4x}$ on $]0,1]$ we get : $$ f'(x)=x^{4 (1 - x)} (\frac{4 (1 - x)}{x} - 4 \log(x)) + (1 - x)^{(4 x)} (4 \log(1 - x) - \frac{4 x}{1 - x})$$ If we denote by $g(x)$ the function : $$g(x)=x^{4 (1 - x)} (\frac{4 (1 - x)}{x} - 4 \log(x))$$ We can rewrite the derivative as : $$f'(x)=g(x)-g(1-x)$$ So it's remains to show that $g(x)\geq g(1-x)$ or $g(x)\leq g(1-x)$ So it remains to show that $g(x)$ is increasing or decreasing . After that I'm stuck... Any helps are very appreciated ! Thanks a lot for your time .",Inspired by a problem of Vasile Cirtoaje I propose this : Let such that then we have : I compute the derivative of on we get : If we denote by the function : We can rewrite the derivative as : So it's remains to show that or So it remains to show that is increasing or decreasing . After that I'm stuck... Any helps are very appreciated ! Thanks a lot for your time .,"a,b>0 a+b=1 a^{4b}+b^{4a}\geq \frac{1}{2} f(x)=x^{4(1-x)}+(1-x)^{4x} ]0,1]  f'(x)=x^{4 (1 - x)} (\frac{4 (1 - x)}{x} - 4 \log(x)) + (1 - x)^{(4 x)} (4 \log(1 - x) - \frac{4 x}{1 - x}) g(x) g(x)=x^{4 (1 - x)} (\frac{4 (1 - x)}{x} - 4 \log(x)) f'(x)=g(x)-g(1-x) g(x)\geq g(1-x) g(x)\leq g(1-x) g(x)","['derivatives', 'inequality', 'exponentiation']"
78,Derivative of norm 2,Derivative of norm 2,,I'm struggling a bit using the chain rule. Given the function $\phi$ defined as: $\phi(x) = ||{A\bf{x}-b}||_2$ where $A$ is a matrix and $b$ is a vector. What is the gradient $\nabla\phi$ and how should I proceed to compute it?,I'm struggling a bit using the chain rule. Given the function $\phi$ defined as: $\phi(x) = ||{A\bf{x}-b}||_2$ where $A$ is a matrix and $b$ is a vector. What is the gradient $\nabla\phi$ and how should I proceed to compute it?,,"['derivatives', 'normed-spaces', 'chain-rule']"
79,What does it take for a smooth homeomorphism to be a diffeomorphism?,What does it take for a smooth homeomorphism to be a diffeomorphism?,,"I have an open subset $A$ of $\mathbb{R}^k$ and a subset $B$ of $\mathbb{R}^n$, $n>k$, that are homeomorphic and $f:A\longrightarrow B$ is a smooth homeomorphism between two sets. I'm wondering if you know any results as to what additional properties of $f$ (other than its inverse being smooth) would ensure that it is a diffeomorphism. Such result would be in the spirit of ""a continuous bijection is a homeomorphism if and only if it is open (closed)"" which lets one prove a function is a homeomorphism without directly proving that its inverse is continuous. My end goal is to prove that my concrete function $f$ has its Jacobi determinant positive everywhere on $A$ or at least that Jacobian is zero only at isolated points. So if you know any results that would let me reason about the set on which the Jacobian vanishes using the facts (smooth homeomorphism) that I stated, I would very much appreciate it.","I have an open subset $A$ of $\mathbb{R}^k$ and a subset $B$ of $\mathbb{R}^n$, $n>k$, that are homeomorphic and $f:A\longrightarrow B$ is a smooth homeomorphism between two sets. I'm wondering if you know any results as to what additional properties of $f$ (other than its inverse being smooth) would ensure that it is a diffeomorphism. Such result would be in the spirit of ""a continuous bijection is a homeomorphism if and only if it is open (closed)"" which lets one prove a function is a homeomorphism without directly proving that its inverse is continuous. My end goal is to prove that my concrete function $f$ has its Jacobi determinant positive everywhere on $A$ or at least that Jacobian is zero only at isolated points. So if you know any results that would let me reason about the set on which the Jacobian vanishes using the facts (smooth homeomorphism) that I stated, I would very much appreciate it.",,"['differential-geometry', 'derivatives', 'differential-topology', 'smooth-manifolds']"
80,Proving that $\frac{1}{a^3(b+c)}+\frac{1}{b^3(c+a)}+\frac{1}{c^3(a+b)}\ge\frac32$ using derivatives,Proving that  using derivatives,\frac{1}{a^3(b+c)}+\frac{1}{b^3(c+a)}+\frac{1}{c^3(a+b)}\ge\frac32,"Let $a,b,c\in\mathbb{R}^+$ and $abc=1$. Prove that   $$\frac{1}{a^3(b+c)}+\frac{1}{b^3(c+a)}+\frac{1}{c^3(a+b)}\ge\frac32$$ This isn't hard problem. I have already solved it in following way: Let $x=\frac1a,y=\frac1b,z=\frac1c$, then $xyz=1$. Now, it is enought to prove that $$L\equiv\frac{x^2}{y+z}+\frac{y^2}{z+x}+\frac{z^2}{x+y}\ge\frac32$$ Now using Cauchy-Schwarz inequality on numbers $a_1=\sqrt{y+z},a_2=\sqrt{z+x},a_3=\sqrt{x+y},b_1=\frac{x}{a_1},b_2=\frac{y}{a_2},b_3=\frac{z}{a_3}$ I got $$(x+y+z)^2\le((x+y)+(y+z)+(z+x))\cdot L$$ From this $$L\ge\frac{x+y+z}2\ge\frac32\sqrt[3]{xyz}=\frac32$$ Then I tried to prove it using derivatives. Let $x=a,y=b$ and $$f(x,y)=\frac1{x^3\left({y+\frac1{xy}}\right)}+\frac1{y^3\left({x+\frac1{xy}}\right)}+\frac1{\left({\frac1{xy}}\right)^3(x+y)}$$ So, I need to find minimum value of this function. It will be true when $$\frac{df}{dx}=0\land\frac{df}{dy}=0$$ After simplifying $\frac{df}{dx}=0$ I got $$\frac{-y(3xy^2+2)}{x^3\left({xy^2+1}\right)^2}+\frac{1-x^2y}{y^2\left({x^2y+1}\right)^2}+\frac{x^2y^3(2x+3y)}{\left({x+y}\right)^2}=0$$ Is there any easy way to write $x$ in term of $y$ from this equation?","Let $a,b,c\in\mathbb{R}^+$ and $abc=1$. Prove that   $$\frac{1}{a^3(b+c)}+\frac{1}{b^3(c+a)}+\frac{1}{c^3(a+b)}\ge\frac32$$ This isn't hard problem. I have already solved it in following way: Let $x=\frac1a,y=\frac1b,z=\frac1c$, then $xyz=1$. Now, it is enought to prove that $$L\equiv\frac{x^2}{y+z}+\frac{y^2}{z+x}+\frac{z^2}{x+y}\ge\frac32$$ Now using Cauchy-Schwarz inequality on numbers $a_1=\sqrt{y+z},a_2=\sqrt{z+x},a_3=\sqrt{x+y},b_1=\frac{x}{a_1},b_2=\frac{y}{a_2},b_3=\frac{z}{a_3}$ I got $$(x+y+z)^2\le((x+y)+(y+z)+(z+x))\cdot L$$ From this $$L\ge\frac{x+y+z}2\ge\frac32\sqrt[3]{xyz}=\frac32$$ Then I tried to prove it using derivatives. Let $x=a,y=b$ and $$f(x,y)=\frac1{x^3\left({y+\frac1{xy}}\right)}+\frac1{y^3\left({x+\frac1{xy}}\right)}+\frac1{\left({\frac1{xy}}\right)^3(x+y)}$$ So, I need to find minimum value of this function. It will be true when $$\frac{df}{dx}=0\land\frac{df}{dy}=0$$ After simplifying $\frac{df}{dx}=0$ I got $$\frac{-y(3xy^2+2)}{x^3\left({xy^2+1}\right)^2}+\frac{1-x^2y}{y^2\left({x^2y+1}\right)^2}+\frac{x^2y^3(2x+3y)}{\left({x+y}\right)^2}=0$$ Is there any easy way to write $x$ in term of $y$ from this equation?",,['inequality']
81,Coincidence? : $d(ax^2+bx+c)/dx=\pm \sqrt{\Delta}$,Coincidence? :,d(ax^2+bx+c)/dx=\pm \sqrt{\Delta},"As the title says, is it just a coincidence that $d(ax^2+bx+c)/dx=\pm \sqrt{\Delta}$? (where $\Delta=b^2-4ac$, i.e. discriminant of the quadratic). We can get this easily from rearranging the quadratic formula: $$x=\frac{-b\pm \sqrt{b^2-4ac}}{2a}$$ $$\iff 2ax+b=\pm \sqrt{b^2-4ac}$$ $$\iff \frac{d(ax^2+bx+c)}{dx}=\pm \sqrt{\Delta}$$ but that doesn't explain why it's true, i.e. why the derivative of a quadratic equals $\pm$ square root of $\Delta$. Seems  a touch mysterious. EDIT: I found this in a book entitled ""Vedic Mathematics"" by Bharati.","As the title says, is it just a coincidence that $d(ax^2+bx+c)/dx=\pm \sqrt{\Delta}$? (where $\Delta=b^2-4ac$, i.e. discriminant of the quadratic). We can get this easily from rearranging the quadratic formula: $$x=\frac{-b\pm \sqrt{b^2-4ac}}{2a}$$ $$\iff 2ax+b=\pm \sqrt{b^2-4ac}$$ $$\iff \frac{d(ax^2+bx+c)}{dx}=\pm \sqrt{\Delta}$$ but that doesn't explain why it's true, i.e. why the derivative of a quadratic equals $\pm$ square root of $\Delta$. Seems  a touch mysterious. EDIT: I found this in a book entitled ""Vedic Mathematics"" by Bharati.",,"['polynomials', 'derivatives', 'roots', 'quadratics']"
82,Quantify the similarity between a polynomial roots and the roots of its derivatives,Quantify the similarity between a polynomial roots and the roots of its derivatives,,"On $\mathbb{C}[X]$ , many theorems and conjectures deal with relations between a polynomial roots and the roots of its derivatives. When looking at a graph, the derivative roots distribution somewhat mimics the distribution of the polynomial roots. It is this ""somewhat mimics"" that I would like to look at in this question. Examples. Let $P$ be a degree $n$ polynomial in $\mathbb{C}[X]$ , then: (Well-known) The mean of $P$ roots is also the mean of $P$ successive derivatives roots. And so it is the only root of $P^{(n-1)}$ which has degree $1$ . The mean being the first cumulant, what about the other cumulants of the roots? The second cumulant is the variance $\sigma^2$ , same as second central moment. We find that $\sigma^2/(n-1)$ is conserved: if $\sigma'^2$ is the variance of $P'$ roots, $\sigma'^2 = \frac {n-2} {n-1} \sigma^2$ (proof at the end). Similarly, for the third cumulant $\kappa_3$ , which is also the third central moment $\mu_3$ : if $\kappa_3'$ is the third cumulant of $P'$ roots, $\kappa_3'= \frac {n-3} {n-1} \kappa_3$ (proof at the end). However this does not extend to $\kappa_4$ nor to $\mu_4$ (which by the way are different). $\sigma'^2 = \frac {n-2} {n-1} \sigma^2$ has the following consequence (proof at the end): distance between the two roots of $P^{(n-2)}$ is proportional to $\sigma$ : $\frac 2 {\sqrt{n-1}} \sigma$ . Question : Are there other quantities that characterize the roots distribution of a polynomial, that are conserved (possibly with a factor only depending upon $n$ ) in the polynomial derivative? These quantities should have an established statistical meaning, or a geometric interpretation. E.g. what about the PCA (principal component analysis) of the roots? Proofs : Use the following relations between cumulants $\kappa_i$ , elementary symmetrical polynomials $e_i$ , elementary symmetric polynomials for the derivative $e'_i$ , central moments $\mu_i$ , raw moments $\mu'_i$ (sorry for the notation clash: not the central moments of the derivative), power sums $p_i$ , polynomial roots $a_i$ : $\mu'_i=\frac 1 n p_i=\frac 1 n \sum_{k=1}^n a_k^i$ $p_1=e_1$ $p_2=e_1^2-2e_2$ $p_3=e_1^3-3e_1e_2+3e_3$ $\kappa_2=\mu'_2-\mu_1'^2$ $\kappa_3=\mu'_3-3\mu'_2\mu'_1+2\mu_1'^3$ $e'_i = \frac {n-i} n e_i$ Second cumulant (variance): $\kappa_2=\mu'_2-\mu_1'^2=\frac 1 n p_2 - \frac 1 {n^2} p_1^2$ $=\frac {n-1} {n^2} e_1^2 - \frac 2 n e_2$ $\kappa'_2=\frac {n-2} {(n-1)^2} e_1'^2 - \frac 2 {n-1} e'_2$ then replace $e'_i$ with $\frac {n-i} n e_i$ , gives $\kappa'_2=\frac {n-2} {n-1} \kappa_2$ . Third cumulant: $\kappa_3=\mu'_3-3\mu'_2\mu'_1+2\mu_1'^3$ $=\frac 1 n p_3 - \frac 3 {n^2} p_2p_1 + \frac 2 {n^3} p_1^3$ $= \frac 1 n (e_1^3-3e_1e_2+3e_3) - \frac 3 {n^2}(e_1^2-2e_2)e_1 + \frac 2 {n^3}e_1^3$ $=\frac {(n-1)(n-2)} {n^3} e_1^3 - 3 \frac {n-2} {n^2}e_1e_2 + \frac 3 n e_3$ $\kappa'_3= \frac {(n-2)(n-3)} {(n-1)^3} e_1'^3 - 3 \frac {n-3} {(n-1)^2}e'_1e'_2 + \frac 3 {n-1} e'_3$ then replace $e'_i$ with $\frac {n-i} n e_i$ , gives $\kappa'_3=\frac {n-3} {n-1} \kappa_3$ . Distance between roots of the $(n-2)$ th derivative: this can be proven using the variance conservation relation, or directly: Let $P(Z) = \sum_{j=0}^n (-1)^j e_j \; Z^{n-j}$ . $P^{(n-2)}(Z)=\frac {n!} 2 Z^2-(n-1)! \; e_1 \; Z+(n-2)! \; e_2.$ Its two roots are $\frac 1 n {e_1} \pm \frac 1 {n!} \sqrt{(n-1)!(n-2)!((n-1)e_1^2-2ne_2)}$ . The quantity $(n-1)e_1^2 - 2n e_2 = (n-1)(\sum a_j)^2 - 2n \sum_{j<k}a_j a_k$ $= n (\sum a_j)^2 - 2n \sum_{j<k}a_j a_k - (\sum a_j)^2 = n \sum a_j^2 - (\sum a_j)^2$ $= n^2 (\frac 1 n \sum a_j^2 - (\frac {\sum a_j} n)^2) = n^2 \sigma^2$ , with $\sigma^2$ the variance of $P$ roots. So distance between the two roots $= \frac 2 {n!} \sqrt{(n-1)!(n-2)!n^2 \sigma^2} = \frac 2 {\sqrt{n-1}} \sigma$","On , many theorems and conjectures deal with relations between a polynomial roots and the roots of its derivatives. When looking at a graph, the derivative roots distribution somewhat mimics the distribution of the polynomial roots. It is this ""somewhat mimics"" that I would like to look at in this question. Examples. Let be a degree polynomial in , then: (Well-known) The mean of roots is also the mean of successive derivatives roots. And so it is the only root of which has degree . The mean being the first cumulant, what about the other cumulants of the roots? The second cumulant is the variance , same as second central moment. We find that is conserved: if is the variance of roots, (proof at the end). Similarly, for the third cumulant , which is also the third central moment : if is the third cumulant of roots, (proof at the end). However this does not extend to nor to (which by the way are different). has the following consequence (proof at the end): distance between the two roots of is proportional to : . Question : Are there other quantities that characterize the roots distribution of a polynomial, that are conserved (possibly with a factor only depending upon ) in the polynomial derivative? These quantities should have an established statistical meaning, or a geometric interpretation. E.g. what about the PCA (principal component analysis) of the roots? Proofs : Use the following relations between cumulants , elementary symmetrical polynomials , elementary symmetric polynomials for the derivative , central moments , raw moments (sorry for the notation clash: not the central moments of the derivative), power sums , polynomial roots : Second cumulant (variance): then replace with , gives . Third cumulant: then replace with , gives . Distance between roots of the th derivative: this can be proven using the variance conservation relation, or directly: Let . Its two roots are . The quantity , with the variance of roots. So distance between the two roots",\mathbb{C}[X] P n \mathbb{C}[X] P P P^{(n-1)} 1 \sigma^2 \sigma^2/(n-1) \sigma'^2 P' \sigma'^2 = \frac {n-2} {n-1} \sigma^2 \kappa_3 \mu_3 \kappa_3' P' \kappa_3'= \frac {n-3} {n-1} \kappa_3 \kappa_4 \mu_4 \sigma'^2 = \frac {n-2} {n-1} \sigma^2 P^{(n-2)} \sigma \frac 2 {\sqrt{n-1}} \sigma n \kappa_i e_i e'_i \mu_i \mu'_i p_i a_i \mu'_i=\frac 1 n p_i=\frac 1 n \sum_{k=1}^n a_k^i p_1=e_1 p_2=e_1^2-2e_2 p_3=e_1^3-3e_1e_2+3e_3 \kappa_2=\mu'_2-\mu_1'^2 \kappa_3=\mu'_3-3\mu'_2\mu'_1+2\mu_1'^3 e'_i = \frac {n-i} n e_i \kappa_2=\mu'_2-\mu_1'^2=\frac 1 n p_2 - \frac 1 {n^2} p_1^2 =\frac {n-1} {n^2} e_1^2 - \frac 2 n e_2 \kappa'_2=\frac {n-2} {(n-1)^2} e_1'^2 - \frac 2 {n-1} e'_2 e'_i \frac {n-i} n e_i \kappa'_2=\frac {n-2} {n-1} \kappa_2 \kappa_3=\mu'_3-3\mu'_2\mu'_1+2\mu_1'^3 =\frac 1 n p_3 - \frac 3 {n^2} p_2p_1 + \frac 2 {n^3} p_1^3 = \frac 1 n (e_1^3-3e_1e_2+3e_3) - \frac 3 {n^2}(e_1^2-2e_2)e_1 + \frac 2 {n^3}e_1^3 =\frac {(n-1)(n-2)} {n^3} e_1^3 - 3 \frac {n-2} {n^2}e_1e_2 + \frac 3 n e_3 \kappa'_3= \frac {(n-2)(n-3)} {(n-1)^3} e_1'^3 - 3 \frac {n-3} {(n-1)^2}e'_1e'_2 + \frac 3 {n-1} e'_3 e'_i \frac {n-i} n e_i \kappa'_3=\frac {n-3} {n-1} \kappa_3 (n-2) P(Z) = \sum_{j=0}^n (-1)^j e_j \; Z^{n-j} P^{(n-2)}(Z)=\frac {n!} 2 Z^2-(n-1)! \; e_1 \; Z+(n-2)! \; e_2. \frac 1 n {e_1} \pm \frac 1 {n!} \sqrt{(n-1)!(n-2)!((n-1)e_1^2-2ne_2)} (n-1)e_1^2 - 2n e_2 = (n-1)(\sum a_j)^2 - 2n \sum_{j<k}a_j a_k = n (\sum a_j)^2 - 2n \sum_{j<k}a_j a_k - (\sum a_j)^2 = n \sum a_j^2 - (\sum a_j)^2 = n^2 (\frac 1 n \sum a_j^2 - (\frac {\sum a_j} n)^2) = n^2 \sigma^2 \sigma^2 P = \frac 2 {n!} \sqrt{(n-1)!(n-2)!n^2 \sigma^2} = \frac 2 {\sqrt{n-1}} \sigma,"['derivatives', 'polynomials', 'cumulants']"
83,Infinite number of Derivatives,Infinite number of Derivatives,,"Is there a kind of function (other than trigonometric) that you can take infinite amount of derivatives without it ever becoming 0. Algebraic functions now matter how long, or how many powers  it has it can eventually be derived to 0. I am not including trigonometric functions because they are circular in nature. I mean a function that will not go on a circle like trigonometric do; But will have infinite derivatives.","Is there a kind of function (other than trigonometric) that you can take infinite amount of derivatives without it ever becoming 0. Algebraic functions now matter how long, or how many powers  it has it can eventually be derived to 0. I am not including trigonometric functions because they are circular in nature. I mean a function that will not go on a circle like trigonometric do; But will have infinite derivatives.",,['derivatives']
84,why there is no derivative in sharp turns?,why there is no derivative in sharp turns?,,"why there is no derivative in sharp turns in functions? I understand that it may be difficult or impossible to actually draw a tangent at that point, but is there a mathematical proof that there is no derivative in sharp turns? thanks!","why there is no derivative in sharp turns in functions? I understand that it may be difficult or impossible to actually draw a tangent at that point, but is there a mathematical proof that there is no derivative in sharp turns? thanks!",,['derivatives']
85,Is the derivative of a periodic function always periodic?,Is the derivative of a periodic function always periodic?,,"True or False : The derivative of a periodic functions is always periodic. I thought it to be true , as everything about a periodic function repeats itself at regular intervals, and so should it's derivative . But , to my surprise it is given false , which suggests that it might be true most of the time but not always , I have given all my thoughts to finding a counter example but I just can't seem to find even one counter example . One possibility was $\{x\}$ , which is not differentiable at every integer , and I am confused about whether I should call it periodic or not , because it's graph will be a straight line with holes at every integer , so in a sense it is periodically not defined , just like $\tan x$ wich is not defined at every odd multiple of $\pi\over 2$ but still it is said to be periodic . Could someone please help me find a counter example and clarify about periodicity of derivative of $\{x\}$ . Thanks ! $\{x\}$ is fractional part of x .","True or False : The derivative of a periodic functions is always periodic. I thought it to be true , as everything about a periodic function repeats itself at regular intervals, and so should it's derivative . But , to my surprise it is given false , which suggests that it might be true most of the time but not always , I have given all my thoughts to finding a counter example but I just can't seem to find even one counter example . One possibility was , which is not differentiable at every integer , and I am confused about whether I should call it periodic or not , because it's graph will be a straight line with holes at every integer , so in a sense it is periodically not defined , just like wich is not defined at every odd multiple of but still it is said to be periodic . Could someone please help me find a counter example and clarify about periodicity of derivative of . Thanks ! is fractional part of x .",\{x\} \tan x \pi\over 2 \{x\} \{x\},"['derivatives', 'periodic-functions']"
86,derivative of $\left(\frac{\ 1}{3x}\right)$ using limit definition.,derivative of  using limit definition.,\left(\frac{\ 1}{3x}\right),I'm trying find derivative of $\left(\frac{\ 1}{3x}\right)$ using limit definition. I do this: $(\left(\frac{\ 1}{3x+h}\right) - \left(\frac{\ 1}{3x}\right))/h$ $(\left(\frac{\ 1}{3x+h}\right)(3x) - \left(\frac{\ 1}{3x}\right)(3x+h))/h$ $(\left(\frac{\ 3x}{3x(3x+h)}\right) - \left(\frac{\ 3x+h}{3x(3x+h)}\right))/h$ $\left(\frac{\ 3x - 3x - h}{3x(3x+h)}\right)/h$ $\left(\frac{\  - h}{3x(3x+h)}\right)/h$ $\left(\frac{\  - h}{3x(3x+h)}\right)* \left(\frac{\   1}{h}\right)$ $\left(\frac{\  - 1}{3x(3x+h)}\right)$ Answer: $\left(\frac{\  - 1}{9x^2}\right)$ But in https://www.symbolab.com/solver/derivative-calculator the answer is $\left(\frac{\  - 1}{3x^2}\right)$ I think my calculations are correct. How can symbolad answer be different? Thanks!,I'm trying find derivative of $\left(\frac{\ 1}{3x}\right)$ using limit definition. I do this: $(\left(\frac{\ 1}{3x+h}\right) - \left(\frac{\ 1}{3x}\right))/h$ $(\left(\frac{\ 1}{3x+h}\right)(3x) - \left(\frac{\ 1}{3x}\right)(3x+h))/h$ $(\left(\frac{\ 3x}{3x(3x+h)}\right) - \left(\frac{\ 3x+h}{3x(3x+h)}\right))/h$ $\left(\frac{\ 3x - 3x - h}{3x(3x+h)}\right)/h$ $\left(\frac{\  - h}{3x(3x+h)}\right)/h$ $\left(\frac{\  - h}{3x(3x+h)}\right)* \left(\frac{\   1}{h}\right)$ $\left(\frac{\  - 1}{3x(3x+h)}\right)$ Answer: $\left(\frac{\  - 1}{9x^2}\right)$ But in https://www.symbolab.com/solver/derivative-calculator the answer is $\left(\frac{\  - 1}{3x^2}\right)$ I think my calculations are correct. How can symbolad answer be different? Thanks!,,['derivatives']
87,Difference between strictly increasing and increasing functions,Difference between strictly increasing and increasing functions,,What is the precise difference between strictly increasing and increasing functions?? I see these terms being thrown around a lot My guess is that strictly increasing mean that derivative is only greater than 0 and in case of just increasing derivative can be greater than or equal or 0?,What is the precise difference between strictly increasing and increasing functions?? I see these terms being thrown around a lot My guess is that strictly increasing mean that derivative is only greater than 0 and in case of just increasing derivative can be greater than or equal or 0?,,['derivatives']
88,What is the exact meaning of the following sentence?,What is the exact meaning of the following sentence?,,"I have read the following sentence in my book. Let $\Phi$ be a convex function which is finite on interval $I$ containing the range of the function $g$ and infinite elsewhere, and let $\phi$ be an arbitrary determination of its derivative (i.e. having at a ""corner"" any value between the left and right derivatives), defined and finite on $I$. What is the meaning of "" having at a 'corner' any value between the left and right derivatives ""?","I have read the following sentence in my book. Let $\Phi$ be a convex function which is finite on interval $I$ containing the range of the function $g$ and infinite elsewhere, and let $\phi$ be an arbitrary determination of its derivative (i.e. having at a ""corner"" any value between the left and right derivatives), defined and finite on $I$. What is the meaning of "" having at a 'corner' any value between the left and right derivatives ""?",,"['derivatives', 'terminology']"
89,Calculating value of $1000^{th}$ derivative at $0$.,Calculating value of  derivative at .,1000^{th} 0,I need to calculate value of $1000^{th}$ derivate of the following function at $0$: $$ f(x) = \frac{x+1}{(x-1)(x-2)} $$ I've done similar problems before (e.g. $f(x)= \dfrac{x}{e^{x}}$) but the approach I've used would not work in this case and I believe I should expand this function into a power series. Could you please give me any hints on how to do it?,I need to calculate value of $1000^{th}$ derivate of the following function at $0$: $$ f(x) = \frac{x+1}{(x-1)(x-2)} $$ I've done similar problems before (e.g. $f(x)= \dfrac{x}{e^{x}}$) but the approach I've used would not work in this case and I believe I should expand this function into a power series. Could you please give me any hints on how to do it?,,"['derivatives', 'power-series']"
90,"Partial vs. total differentiation,basic distinction","Partial vs. total differentiation,basic distinction",,Suppose that $f:\mathbb{R}\to {\mathbb R}^n$ is a smooth function with value $f(t)$ at $t$ . What is the difference between $$\frac{df}{dt}$$ and $$\frac{\partial f}{\partial t}?$$ When are partial and total differentiation different?,Suppose that is a smooth function with value at . What is the difference between and When are partial and total differentiation different?,f:\mathbb{R}\to {\mathbb R}^n f(t) t \frac{df}{dt} \frac{\partial f}{\partial t}?,"['derivatives', 'partial-derivative']"
91,what does $(A\cdot\nabla)B$ mean?,what does  mean?,(A\cdot\nabla)B,I was studying a physics book and I saw this expression $$(A\cdot\nabla)B$$ where $A$ and $B$ are vectors. What's the definition of this? I've also seen this in some identities,I was studying a physics book and I saw this expression $$(A\cdot\nabla)B$$ where $A$ and $B$ are vectors. What's the definition of this? I've also seen this in some identities,,"['derivatives', 'vectors', 'vector-analysis', 'products', 'divergence-operator']"
92,Differentiate equation with parenthesis,Differentiate equation with parenthesis,,"I have a problem. I'm studying calculus, but I don't have a good math background, so I have a problem: I don't know well how to differentiate an equation with parenthesis. The equation is the following: $f(x) = 25x^3(x-1)^2$ Is it correct to use the Differentiation Product Rule in this way: $f'(x)=75x^2*(x-1)^2+25x^3*2(x-1)$ or before I have to solve $(x-1)^2$ in this way: f(x) = $25x^3*(x^2+1-2x)$ and then      = $25x^5+25x^3-50x^4$ ? Thanks in advance","I have a problem. I'm studying calculus, but I don't have a good math background, so I have a problem: I don't know well how to differentiate an equation with parenthesis. The equation is the following: $f(x) = 25x^3(x-1)^2$ Is it correct to use the Differentiation Product Rule in this way: $f'(x)=75x^2*(x-1)^2+25x^3*2(x-1)$ or before I have to solve $(x-1)^2$ in this way: f(x) = $25x^3*(x^2+1-2x)$ and then      = $25x^5+25x^3-50x^4$ ? Thanks in advance",,['derivatives']
93,Is there any function continuous in $R$ and differentiable in rational numbers with zero derivative?,Is there any function continuous in  and differentiable in rational numbers with zero derivative?,R,I'm looking for a function continuous in $R$ and differentiable in all rational numbers and it's derivative should be $0$.But not the constant function. And there is a same question about irrational numbers. Can any one help??,I'm looking for a function continuous in $R$ and differentiable in all rational numbers and it's derivative should be $0$.But not the constant function. And there is a same question about irrational numbers. Can any one help??,,['derivatives']
94,Numerical force due to Lennard Jones potential,Numerical force due to Lennard Jones potential,,"I am stuck with a problem related to simulating a Lennard-Jones system. The Lennard Jones potential is  $U(r) = 4\epsilon [ \frac{\sigma^{12}}{r^{12}} - \frac{\sigma^6}{r^6} ]$. Hence the force will be $F(r)=-\frac{\partial{U(r)}}{\partial{r}}= 48 \epsilon [ \frac{\sigma^{12}}{r^{13}} - \frac{\sigma^6}{r^7}]$. But to do computer simulation I need to know the force componentwise i.e. all the three components $F_x$, $F_y$, $F_z$ are to be known. Is the $x$ component simply $F_x = 48 \epsilon [ \frac{\sigma^{12}}{\Delta x^{13}} - \frac{\sigma^6}{\Delta x^7}]$, where $\Delta x$ is the distance between the pair of particles concerned? (or simply $\Delta x =x_i - x_j$) And let's say that we are calculating the force on the ith particle due to the $j$th particle. How can I express the direction of this force in terms of $x_i$ and $x_j$ ?","I am stuck with a problem related to simulating a Lennard-Jones system. The Lennard Jones potential is  $U(r) = 4\epsilon [ \frac{\sigma^{12}}{r^{12}} - \frac{\sigma^6}{r^6} ]$. Hence the force will be $F(r)=-\frac{\partial{U(r)}}{\partial{r}}= 48 \epsilon [ \frac{\sigma^{12}}{r^{13}} - \frac{\sigma^6}{r^7}]$. But to do computer simulation I need to know the force componentwise i.e. all the three components $F_x$, $F_y$, $F_z$ are to be known. Is the $x$ component simply $F_x = 48 \epsilon [ \frac{\sigma^{12}}{\Delta x^{13}} - \frac{\sigma^6}{\Delta x^7}]$, where $\Delta x$ is the distance between the pair of particles concerned? (or simply $\Delta x =x_i - x_j$) And let's say that we are calculating the force on the ith particle due to the $j$th particle. How can I express the direction of this force in terms of $x_i$ and $x_j$ ?",,['derivatives']
95,"Studying the extrema of $f(x,y) = x^4 + y^4 -2(x-y)^2$",Studying the extrema of,"f(x,y) = x^4 + y^4 -2(x-y)^2","Let $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ such that $f(x,y) = x^4 + y^4 - 2(x-y)^2$. Study its extrema. So here was my approach. We have $$\frac{\partial f}{\partial x}(x,y) = 4(x^3 -x + y),\frac{\partial f}{\partial y}(x,y)= 4(y^3 -y + x) $$ I have to find $(x_0,y_0) \in \mathbb{R}^2$ such that $ \frac{\partial f}{\partial x}(x_0,y_0)= \frac{\partial f}{\partial y}(x_0,y_0) = 0 $ So we have: $$\left\{\begin{matrix} x(x^2-1)+ y  =0\\  y(y^2 -1) + x =0 \end{matrix}\right. $$ Thus we have $x-x^3 = y$, and by replacing $y$ with $x-x^3$ in the second line, we get: $$ y(y^2 -1) + x =0 = (x-x^3)((x-x^3)^2 -1)+x  =x^5(-x^4 -x^2 -3) = 0 $$ And the only solution for this is $x=0$. As $y = x^3 -x$ we immediately have $y=0$. So the only extremum possible is at $(0,0)$. Now, I need to study its Hessian matrix. We have: $$\frac{\partial ^2f}{\partial x^2}(x,y) = 12x^2 -4 , \frac{\partial ^2f}{\partial y^2}(x,y) = 12y^2 -4, \frac{\partial^2 f}{\partial x \partial y}(x,y) = \frac{\partial^2 f}{\partial y \partial x}(x,y) = 4$$ Thus $$H(x,y) \begin{bmatrix}  12x^2 -4&4 \\   4& 12y^2 -4 \end{bmatrix} $$ At $(x,y)=(0,0)$ we have  $$H(0,0) \begin{bmatrix}  -4&4 \\   4&  -4 \end{bmatrix} $$ As we have $\text{det}(H(0,0)) = 0$ and $\text{Tr}(H(0,0)) = -8$ the eigenvalues are $0$ and $-8$. As it has $0$ as eigenvalue, I need to study the differential at a higher order. But here I lack understanding. What exactly should I do? Should I compute the third order partial differentials and then restudy their Hessian Matrix?","Let $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ such that $f(x,y) = x^4 + y^4 - 2(x-y)^2$. Study its extrema. So here was my approach. We have $$\frac{\partial f}{\partial x}(x,y) = 4(x^3 -x + y),\frac{\partial f}{\partial y}(x,y)= 4(y^3 -y + x) $$ I have to find $(x_0,y_0) \in \mathbb{R}^2$ such that $ \frac{\partial f}{\partial x}(x_0,y_0)= \frac{\partial f}{\partial y}(x_0,y_0) = 0 $ So we have: $$\left\{\begin{matrix} x(x^2-1)+ y  =0\\  y(y^2 -1) + x =0 \end{matrix}\right. $$ Thus we have $x-x^3 = y$, and by replacing $y$ with $x-x^3$ in the second line, we get: $$ y(y^2 -1) + x =0 = (x-x^3)((x-x^3)^2 -1)+x  =x^5(-x^4 -x^2 -3) = 0 $$ And the only solution for this is $x=0$. As $y = x^3 -x$ we immediately have $y=0$. So the only extremum possible is at $(0,0)$. Now, I need to study its Hessian matrix. We have: $$\frac{\partial ^2f}{\partial x^2}(x,y) = 12x^2 -4 , \frac{\partial ^2f}{\partial y^2}(x,y) = 12y^2 -4, \frac{\partial^2 f}{\partial x \partial y}(x,y) = \frac{\partial^2 f}{\partial y \partial x}(x,y) = 4$$ Thus $$H(x,y) \begin{bmatrix}  12x^2 -4&4 \\   4& 12y^2 -4 \end{bmatrix} $$ At $(x,y)=(0,0)$ we have  $$H(0,0) \begin{bmatrix}  -4&4 \\   4&  -4 \end{bmatrix} $$ As we have $\text{det}(H(0,0)) = 0$ and $\text{Tr}(H(0,0)) = -8$ the eigenvalues are $0$ and $-8$. As it has $0$ as eigenvalue, I need to study the differential at a higher order. But here I lack understanding. What exactly should I do? Should I compute the third order partial differentials and then restudy their Hessian Matrix?",,"['derivatives', 'optimization', 'partial-derivative', 'differential', 'hessian-matrix']"
96,What is a good reference for automatic (algorithmic) differentiation?,What is a good reference for automatic (algorithmic) differentiation?,,"Could someone kindly provide me with a good reference about automatic  differentiation? I have already had a look at the wikipedia article , but I would need a book or article, which gives a better theoretical understanding. It would also be helpful if I had a step by step guide to understand the concepts.","Could someone kindly provide me with a good reference about automatic  differentiation? I have already had a look at the wikipedia article , but I would need a book or article, which gives a better theoretical understanding. It would also be helpful if I had a step by step guide to understand the concepts.",,"['derivatives', 'reference-request']"
97,"Difference in use between $d$, $\partial$, $\operatorname d$, $\varDelta$ and $D$ for derivatives.","Difference in use between , , ,  and  for derivatives.",d \partial \operatorname d \varDelta D,"While reading different sources on implicit differentiation (and thereafter differentiation in general), I came across many different ""d's"" being used for (or similar to) the familiar $$\frac{dy}{dx}$$ The variant $d$ appears in the majority of cases, and also with integrals: $$\int x~dx$$ Now, it appears like the convention on Wikipedia is to use $d$ or $\operatorname d$ for ""normal"" and $\partial$ for partial (which is also its name in Tex) derivatives. Is there an additional difference between $d$ and $\operatorname d$ (except the amount of characters needed), and are there any general conventions?","While reading different sources on implicit differentiation (and thereafter differentiation in general), I came across many different ""d's"" being used for (or similar to) the familiar $$\frac{dy}{dx}$$ The variant $d$ appears in the majority of cases, and also with integrals: $$\int x~dx$$ Now, it appears like the convention on Wikipedia is to use $d$ or $\operatorname d$ for ""normal"" and $\partial$ for partial (which is also its name in Tex) derivatives. Is there an additional difference between $d$ and $\operatorname d$ (except the amount of characters needed), and are there any general conventions?",,"['derivatives', 'notation', 'indefinite-integrals']"
98,Why doesn't my derivation of the arcsec derivative formula not work?,Why doesn't my derivation of the arcsec derivative formula not work?,,"We would like to find the derivative of $\operatorname{arcsec}(x) = y$ . Rearranging this, we get $x = \sec(y)$ . Taking the derivative of both sides, we get $1 = \sec(y)\tan(y)y^\prime$ . Thus, $$y^\prime = \frac{1}{\sec(y)\tan(y)}.$$ Now, draw a right triangle with an acute angle $y$ , hypotenuse $x$ , and a side adjacent to angle $y$ with length $1$ . By the Pythagorean theorem, the side opposite angle $y$ is equal to $\sqrt{x^2-1}$ . Thus, we have $x = \sec(y)$ , which we already knew and got from the triangle, and $$\tan(y) = \frac{1}{\sqrt{x^2-1}}$$ from the triangle we drew. We substitute these values in to our derivative expression to get $$y' = \frac{1}{x\sqrt{x^2-1}}.$$ However, $y^\prime$ should equal $$\frac1{|x|\sqrt{x^2-1}}$$ (note the absolute value). Since the difference occurred in the $|x|$ part, that means I must've done something wrong when I said $\sec(y) = x$ , but I do not know why this assumption is wrong. Please correct my proof.","We would like to find the derivative of . Rearranging this, we get . Taking the derivative of both sides, we get . Thus, Now, draw a right triangle with an acute angle , hypotenuse , and a side adjacent to angle with length . By the Pythagorean theorem, the side opposite angle is equal to . Thus, we have , which we already knew and got from the triangle, and from the triangle we drew. We substitute these values in to our derivative expression to get However, should equal (note the absolute value). Since the difference occurred in the part, that means I must've done something wrong when I said , but I do not know why this assumption is wrong. Please correct my proof.",\operatorname{arcsec}(x) = y x = \sec(y) 1 = \sec(y)\tan(y)y^\prime y^\prime = \frac{1}{\sec(y)\tan(y)}. y x y 1 y \sqrt{x^2-1} x = \sec(y) \tan(y) = \frac{1}{\sqrt{x^2-1}} y' = \frac{1}{x\sqrt{x^2-1}}. y^\prime \frac1{|x|\sqrt{x^2-1}} |x| \sec(y) = x,"['derivatives', 'trigonometry', 'solution-verification']"
99,Sum of $1+4\epsilon +9\epsilon^2 +16\epsilon^3+...+2018^2 \epsilon^{2017}$,Sum of,1+4\epsilon +9\epsilon^2 +16\epsilon^3+...+2018^2 \epsilon^{2017},"I have this problem from a college exam: Let $\epsilon$ be a non-real root of unity of order 2018, find the sum$$S=1+4\epsilon +9\epsilon^2 +16\epsilon^3+...+2018^2 \epsilon^{2017}$$Here is my try. First I considered $$S_1=\sum_{k=0}^{2018} x^k=\frac{1-x^{2019}}{1-x}$$ Now I derivate one time then I multiply again by x to get: $$\sum_{k=0}^{2018} kx^k=\frac{x-x^{2020}-2019x^{2019}}{(1-x)^2}$$ And now I must derivate one more time and set $x=\epsilon$ in order to get the desired sum: $$S=\sum_{k=0}^{2018} k^2\epsilon^{k-1}=\frac{2019^2\epsilon^{2018}-2020\epsilon^{2020}-(2019^2-3\cdot2019-1)\epsilon^{2019}-\epsilon-1}{(1-\epsilon)^3}$$ And my final simplification to the numerator was:$$\epsilon(2019^2-2021)-2018\epsilon^2-2019^2-1$$ Now there were 5 answers given, and not a single one  was even close to this one. Out of luck because only 2 answer had $1-\epsilon$ in the denominator I have choosen the correct one, which was: $$S=\frac{2018(2018\epsilon-2020)}{(1-\epsilon)^2}$$ Can you help me to get that  answer?","I have this problem from a college exam: Let $\epsilon$ be a non-real root of unity of order 2018, find the sum$$S=1+4\epsilon +9\epsilon^2 +16\epsilon^3+...+2018^2 \epsilon^{2017}$$Here is my try. First I considered $$S_1=\sum_{k=0}^{2018} x^k=\frac{1-x^{2019}}{1-x}$$ Now I derivate one time then I multiply again by x to get: $$\sum_{k=0}^{2018} kx^k=\frac{x-x^{2020}-2019x^{2019}}{(1-x)^2}$$ And now I must derivate one more time and set $x=\epsilon$ in order to get the desired sum: $$S=\sum_{k=0}^{2018} k^2\epsilon^{k-1}=\frac{2019^2\epsilon^{2018}-2020\epsilon^{2020}-(2019^2-3\cdot2019-1)\epsilon^{2019}-\epsilon-1}{(1-\epsilon)^3}$$ And my final simplification to the numerator was:$$\epsilon(2019^2-2021)-2018\epsilon^2-2019^2-1$$ Now there were 5 answers given, and not a single one  was even close to this one. Out of luck because only 2 answer had $1-\epsilon$ in the denominator I have choosen the correct one, which was: $$S=\frac{2018(2018\epsilon-2020)}{(1-\epsilon)^2}$$ Can you help me to get that  answer?",,"['derivatives', 'summation', 'roots-of-unity']"
