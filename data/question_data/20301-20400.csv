,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Given a matrix A, how to find  B such that AB=BA [duplicate]","Given a matrix A, how to find  B such that AB=BA [duplicate]",,"This question already has answers here : Given a matrix, is there always another matrix which commutes with it? (5 answers) Closed 8 years ago . Let $A = \begin{pmatrix} 1 &  1& 1\\  1 & 2 &3 \\  1 &4  & 5 \end{pmatrix}$ and  $D = \begin{pmatrix} 2 &  0& 0\\  0 & 3 &0 \\  0 &0  & 5 \end{pmatrix}$. It is found that right-multiplication by D multiplies each column of A by the corresponding diagonal entry of D, whereas left-multiplication by D multiplies each row of A by the corresponding diagonal entry of D. Construct a 3 x 3 matrix B, not the identity matrix or zero matrix, such that $AB=BA$.","This question already has answers here : Given a matrix, is there always another matrix which commutes with it? (5 answers) Closed 8 years ago . Let $A = \begin{pmatrix} 1 &  1& 1\\  1 & 2 &3 \\  1 &4  & 5 \end{pmatrix}$ and  $D = \begin{pmatrix} 2 &  0& 0\\  0 & 3 &0 \\  0 &0  & 5 \end{pmatrix}$. It is found that right-multiplication by D multiplies each column of A by the corresponding diagonal entry of D, whereas left-multiplication by D multiplies each row of A by the corresponding diagonal entry of D. Construct a 3 x 3 matrix B, not the identity matrix or zero matrix, such that $AB=BA$.",,"['linear-algebra', 'matrices']"
1,Endomorphisms of $V$ and the dual space,Endomorphisms of  and the dual space,V,I was told that $V\otimes V^{*}\simeq\mbox{End}\left(V\right)$. I can't find the isomorphism itself though. Can anyone tell me what it is with a proof? Thanks!,I was told that $V\otimes V^{*}\simeq\mbox{End}\left(V\right)$. I can't find the isomorphism itself though. Can anyone tell me what it is with a proof? Thanks!,,"['linear-algebra', 'lie-algebras', 'tensor-products']"
2,Matrix norm of Kronecker product,Matrix norm of Kronecker product,,"Is it true that $ \| A \otimes B \| = \|A\|\|B\| $ for any matrix norm $ \|\cdot \| $? If not, does this identity hold for matrix norms induced by $ \ell_p $ vector norms?","Is it true that $ \| A \otimes B \| = \|A\|\|B\| $ for any matrix norm $ \|\cdot \| $? If not, does this identity hold for matrix norms induced by $ \ell_p $ vector norms?",,"['linear-algebra', 'matrices', 'normed-spaces', 'kronecker-product']"
3,Why cant I row reduce and get the same Eigen values?,Why cant I row reduce and get the same Eigen values?,,A example is: $$A=  \begin{bmatrix}     3 & 2   \\     1 & 4      \end{bmatrix} \leftrightarrow  -\begin{bmatrix}     1 & 4   \\     3 & 2      \end{bmatrix} \leftrightarrow  -\begin{bmatrix}     1 & 4   \\     0 & -10      \end{bmatrix} = H $$ The row operations I used are $$(1)   R_1 \leftrightarrow R_2$$ $$(2) -3R_1 + R_2 \to R_2$$ The original matrix $$|A - \lambda I| = (\lambda-2)(\lambda-5) $$ The reduced matrix: $$|H - \lambda I| = -(1-\lambda)(-10-\lambda) $$ Why are they different? For a couple other questions this was working EDIT: A matrix that gave the same eigen value as the one reduced: $$A=  \begin{bmatrix}     2 & 0 & 1  \\     6 & 4 & -3 \\     2 & 0 & 3     \end{bmatrix} \leftrightarrow  \begin{bmatrix}     1 & 0 & 1  \\     9 & 4 & -3 \\     -1 & 0 & 3     \end{bmatrix} \leftrightarrow \begin{bmatrix}     1 & 0 & 0  \\     9 & 4 & -12 \\     -1 & 0 & 4    \end{bmatrix} = H $$ Column operations were $$-C_3 + C_1 \to C_1$$ $$-C_1 + C_3 \to C_3$$ The eigen values are the same for A and H,A example is: $$A=  \begin{bmatrix}     3 & 2   \\     1 & 4      \end{bmatrix} \leftrightarrow  -\begin{bmatrix}     1 & 4   \\     3 & 2      \end{bmatrix} \leftrightarrow  -\begin{bmatrix}     1 & 4   \\     0 & -10      \end{bmatrix} = H $$ The row operations I used are $$(1)   R_1 \leftrightarrow R_2$$ $$(2) -3R_1 + R_2 \to R_2$$ The original matrix $$|A - \lambda I| = (\lambda-2)(\lambda-5) $$ The reduced matrix: $$|H - \lambda I| = -(1-\lambda)(-10-\lambda) $$ Why are they different? For a couple other questions this was working EDIT: A matrix that gave the same eigen value as the one reduced: $$A=  \begin{bmatrix}     2 & 0 & 1  \\     6 & 4 & -3 \\     2 & 0 & 3     \end{bmatrix} \leftrightarrow  \begin{bmatrix}     1 & 0 & 1  \\     9 & 4 & -3 \\     -1 & 0 & 3     \end{bmatrix} \leftrightarrow \begin{bmatrix}     1 & 0 & 0  \\     9 & 4 & -12 \\     -1 & 0 & 4    \end{bmatrix} = H $$ Column operations were $$-C_3 + C_1 \to C_1$$ $$-C_1 + C_3 \to C_3$$ The eigen values are the same for A and H,,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
4,Diagonal entries of inverse larger than 1 over those for the original matrix,Diagonal entries of inverse larger than 1 over those for the original matrix,,"A friend asked me if, given $A \in \mathcal{S}^n_{++}$ (positive definite), is it true $\forall$ j=1,...,n that $$(A^{-1})_{jj} \geq \frac{1}{A_{jj}}$$ Not sure if it's true but we haven't found counterexamples yet. Attempt (assuming true): Let $A_{-ij} \in \mathbb{R}^{n-1 \times n-1}$ be A w/ the ith row and jth column removed.  It's equivalent to show $A_{jj}(A^{-1})_{jj} \geq 1$. Using this , it's equivalent to show $$\frac{A_{jj} det(A_{-jj})}{\sum_{i=1}^n (-1)^{i+j}A_{ij}det(A_{-ij})} \geq 1$$ Note the numerator and denominator both have terms $A_{jj} det(A_{-jj})$ so it's equivalent to show $\sum_{i \neq j}(-1)^{i+j}A_{ij}det(A_{-ij}) \leq 0$, and here is where I'm stuck. I'm not sure if the attempt is the best approach, so alternate proposals are also much appreciated.","A friend asked me if, given $A \in \mathcal{S}^n_{++}$ (positive definite), is it true $\forall$ j=1,...,n that $$(A^{-1})_{jj} \geq \frac{1}{A_{jj}}$$ Not sure if it's true but we haven't found counterexamples yet. Attempt (assuming true): Let $A_{-ij} \in \mathbb{R}^{n-1 \times n-1}$ be A w/ the ith row and jth column removed.  It's equivalent to show $A_{jj}(A^{-1})_{jj} \geq 1$. Using this , it's equivalent to show $$\frac{A_{jj} det(A_{-jj})}{\sum_{i=1}^n (-1)^{i+j}A_{ij}det(A_{-ij})} \geq 1$$ Note the numerator and denominator both have terms $A_{jj} det(A_{-jj})$ so it's equivalent to show $\sum_{i \neq j}(-1)^{i+j}A_{ij}det(A_{-ij}) \leq 0$, and here is where I'm stuck. I'm not sure if the attempt is the best approach, so alternate proposals are also much appreciated.",,"['linear-algebra', 'matrices']"
5,Frobenius Norm and Relation to Eigenvalues,Frobenius Norm and Relation to Eigenvalues,,"I've been working on this problem, and I think that I almost have the solution, but I'm not quite there. Suppose that $A \in M_n(\mathbb C)$ has $n$ distinct eigenvalues $\lambda_1... \lambda_n$ . Show that $$\sqrt{\sum _{j=1}^{n} \left | {\lambda_j} \right |^2 } \leq \left \| A \right \|_F\,.$$ I tried using the Schur decomposition of $A$ and got that $\left \| A \right \|_F = \sqrt{TT^*}$ , where $A=QTQ^*$ with $Q$ unitary and $T$ triangular, but I'm not sure how to relate this back to eigenvalues and where the inequality comes from.","I've been working on this problem, and I think that I almost have the solution, but I'm not quite there. Suppose that has distinct eigenvalues . Show that I tried using the Schur decomposition of and got that , where with unitary and triangular, but I'm not sure how to relate this back to eigenvalues and where the inequality comes from.","A \in M_n(\mathbb C) n \lambda_1... \lambda_n \sqrt{\sum _{j=1}^{n} \left | {\lambda_j} \right |^2 } \leq \left \| A \right \|_F\,. A \left \| A \right \|_F = \sqrt{TT^*} A=QTQ^* Q T","['linear-algebra', 'matrices', 'inequality', 'eigenvalues-eigenvectors', 'normed-spaces']"
6,Any two bases of a finite dimensional vector space must have the same number of elements.,Any two bases of a finite dimensional vector space must have the same number of elements.,,"Prove that any two bases of a finite dimensional vector space must have the same number of elements. By considering the following two bases $$S_1 = \{ \alpha_1, \alpha_2 , \ldots, \alpha_n \},$$ $$S_2 = \{ \beta_1, \beta_2, \ldots, \beta_m \},$$ how do I show that $m = n$ ? Hints to get started. Thanks very much.",Prove that any two bases of a finite dimensional vector space must have the same number of elements. By considering the following two bases how do I show that ? Hints to get started. Thanks very much.,"S_1 = \{ \alpha_1, \alpha_2 , \ldots, \alpha_n \}, S_2 = \{ \beta_1, \beta_2, \ldots, \beta_m \}, m = n","['linear-algebra', 'vector-spaces', 'self-learning']"
7,"If $A^TA$ is invertible, then $A$ has linearly independent column vectors","If  is invertible, then  has linearly independent column vectors",A^TA A,"Question: Prove that for a $m \times n$ matrix $A$, if $A^TA$ is invertible, then $A$ has linearly independent column vectors. I am hitting a complete blank with this proof, I have the following jotted down so far about stuff that I know. What I know so far: Let $A$ be an $m \times n$ matrix and suppose $A^TA$ is invertible. We know $A^T$ is an $n\times m$ matrix, hence $A^TA$ is an $n\times n$ square matrix with nonzero determinant. We also know that $A^TA\bar{x}=\bar{0}$ has only the trivial solution and $A^TA = \bar{b}$ is consistent and has exactly one solution. The column and row vectors of $A^TA$ are linearly independent. How can I use some of the above to show that $A$ has linearly independent column vectors?","Question: Prove that for a $m \times n$ matrix $A$, if $A^TA$ is invertible, then $A$ has linearly independent column vectors. I am hitting a complete blank with this proof, I have the following jotted down so far about stuff that I know. What I know so far: Let $A$ be an $m \times n$ matrix and suppose $A^TA$ is invertible. We know $A^T$ is an $n\times m$ matrix, hence $A^TA$ is an $n\times n$ square matrix with nonzero determinant. We also know that $A^TA\bar{x}=\bar{0}$ has only the trivial solution and $A^TA = \bar{b}$ is consistent and has exactly one solution. The column and row vectors of $A^TA$ are linearly independent. How can I use some of the above to show that $A$ has linearly independent column vectors?",,['linear-algebra']
8,"Proving if A and B are matrices such that A, B, and AB are normal, then BA is also normal.","Proving if A and B are matrices such that A, B, and AB are normal, then BA is also normal.",,"If A and B are matrices such that A, B, and AB are normal, then BA is also normal. I've seen this statement around, although I've only seen the site/publication/etc... state that it was proven by Wiegmann, but nobody actually gives the proof. Could somebody help me wrap my head around why this is true? Thank you.","If A and B are matrices such that A, B, and AB are normal, then BA is also normal. I've seen this statement around, although I've only seen the site/publication/etc... state that it was proven by Wiegmann, but nobody actually gives the proof. Could somebody help me wrap my head around why this is true? Thank you.",,"['linear-algebra', 'matrices', 'reference-request']"
9,Continuity of eigenvalues and spectral radius for a general matrix,Continuity of eigenvalues and spectral radius for a general matrix,,"Given a general matrix $A(t), t>0$, with real entries, I would like to know if the eigenvalues of $A(t)$ are continuous functions of $t$. These eigenvalues may be real or complex. What about the spectral radius? A classical result from complex analysis states that the roots of a polynomial vary continuously with the coefficients. Can we use the theorem directly to prove the above? or there are other cases where the eigenvalues are actually discontinuous? What I'm actually doing is trying to prove that there exists a $t$ for which the spectral radius of $A(t)$ is in $(0,1)$, and I'm doing that by proving that the spectral radius is 1 if $t\rightarrow 0$ and 0 if $t\rightarrow \infty$ (which I already know). Then I would invoke the continuity of the spectral radius to say that there must exist a value of $t$ for which the spectral radius is in $(0,1)$ Thank you.","Given a general matrix $A(t), t>0$, with real entries, I would like to know if the eigenvalues of $A(t)$ are continuous functions of $t$. These eigenvalues may be real or complex. What about the spectral radius? A classical result from complex analysis states that the roots of a polynomial vary continuously with the coefficients. Can we use the theorem directly to prove the above? or there are other cases where the eigenvalues are actually discontinuous? What I'm actually doing is trying to prove that there exists a $t$ for which the spectral radius of $A(t)$ is in $(0,1)$, and I'm doing that by proving that the spectral radius is 1 if $t\rightarrow 0$ and 0 if $t\rightarrow \infty$ (which I already know). Then I would invoke the continuity of the spectral radius to say that there must exist a value of $t$ for which the spectral radius is in $(0,1)$ Thank you.",,"['linear-algebra', 'analysis']"
10,Is it true that the whole space is the direct sum of a subspace and its orthogonal space?,Is it true that the whole space is the direct sum of a subspace and its orthogonal space?,,"Problem The ground field is $K$, $\operatorname{char}K\neq2$. Suppose $W$ is a (maybe infinite dimensional) subspace of a vector space $V$ with a symmetric/symplectic form $\langle\cdot,\cdot\rangle$. The orthogonal space of $W$, denoted as $W^\perp$, is defined as $W^\perp=\{\,v\in V\,\colon\,\langle v,w\rangle=0,\forall w\in W\,\}$. If the form is nondegenerate on $W$, say $W\cap W^\perp=\{0\}$, is it true that $V=W\oplus W^\perp$? What if the form is nondegenerate on $V$? Background The statement is true when $W$ is finite dimensional. One can choose an orthogonal basis for $W$, then apply the projection formula to determine a orthogonal projection. However, such a process couldn't be applied in the infinite dimensional case. I'm interested in how far we can generalize our results of psuedo-Euclidean or symplectic forms on a finite dimensional vector space to the infinite-dimensional case.","Problem The ground field is $K$, $\operatorname{char}K\neq2$. Suppose $W$ is a (maybe infinite dimensional) subspace of a vector space $V$ with a symmetric/symplectic form $\langle\cdot,\cdot\rangle$. The orthogonal space of $W$, denoted as $W^\perp$, is defined as $W^\perp=\{\,v\in V\,\colon\,\langle v,w\rangle=0,\forall w\in W\,\}$. If the form is nondegenerate on $W$, say $W\cap W^\perp=\{0\}$, is it true that $V=W\oplus W^\perp$? What if the form is nondegenerate on $V$? Background The statement is true when $W$ is finite dimensional. One can choose an orthogonal basis for $W$, then apply the projection formula to determine a orthogonal projection. However, such a process couldn't be applied in the infinite dimensional case. I'm interested in how far we can generalize our results of psuedo-Euclidean or symplectic forms on a finite dimensional vector space to the infinite-dimensional case.",,"['linear-algebra', 'geometry', 'quadratic-forms', 'bilinear-form', 'symplectic-linear-algebra']"
11,"In a matrix ring, no zero divisors may have an inverse","In a matrix ring, no zero divisors may have an inverse",,"In a general ring with 1, a right (left) zero divisor cannot have a right (left) inverse. In a matrix ring over a field, a stronger condition is satisfied: a (right or left) zero divisor cannot have a (right or left) inverse, with the proof I know invoking the rank plus nullity theorem (which makes sense, because infinite dimensional linear transformations can have right-inverses and be right-zero divisors). Is this more or less a peculiarity of matrices, or am I missing a more general property that they (and perhaps other spaces) possess?","In a general ring with 1, a right (left) zero divisor cannot have a right (left) inverse. In a matrix ring over a field, a stronger condition is satisfied: a (right or left) zero divisor cannot have a (right or left) inverse, with the proof I know invoking the rank plus nullity theorem (which makes sense, because infinite dimensional linear transformations can have right-inverses and be right-zero divisors). Is this more or less a peculiarity of matrices, or am I missing a more general property that they (and perhaps other spaces) possess?",,"['linear-algebra', 'ring-theory']"
12,Why is a matrix of indeterminates diagonalizable?,Why is a matrix of indeterminates diagonalizable?,,"Fix $n^2$ indeterminates $t_1,\dots, t_{n^2}$. Let $A$ be the algebraic closure of $\mathbb C(t_1,\dots, t_{n^2})$. Consider the $n\times n$ matrix over $A$ whose entries are precisely $t_1, t_2,\dots, t_{n^2}$. Why is this diagonalizable? I feel I am missing something obvious. My motivation for asking this question comes from a comment on this Mathoverflow answer , which gives a slick proof of the Cayley-Hamilton theorem. I will try to fill in the details; please alert me if there is a gap. If we know this is true, then since the Cayley-Hamilton theorem is easy to verify for diagonalizable operators, we know the matrix $M$ above annihilates its characteristic polynomial. If the characteristic polynomial over $A$ is $p(T)$, then we know that $P(M)=0$. In particular, each entry of $P(M)$ is $0$. But each entry is a polynomial in the indeterminates, which we just saw equals $0$. So no matter which values of $\mathbb C$ we put in for the $t_i$, each entry must vanish. This means every matrix over $\mathbb C$ annihilates its characteristic polynomial. Maybe there is a more elegant way to finish; my justification seems inadequate, but I can't quite put my finger on why. I would appreciate any comments on this, too. Thank you.","Fix $n^2$ indeterminates $t_1,\dots, t_{n^2}$. Let $A$ be the algebraic closure of $\mathbb C(t_1,\dots, t_{n^2})$. Consider the $n\times n$ matrix over $A$ whose entries are precisely $t_1, t_2,\dots, t_{n^2}$. Why is this diagonalizable? I feel I am missing something obvious. My motivation for asking this question comes from a comment on this Mathoverflow answer , which gives a slick proof of the Cayley-Hamilton theorem. I will try to fill in the details; please alert me if there is a gap. If we know this is true, then since the Cayley-Hamilton theorem is easy to verify for diagonalizable operators, we know the matrix $M$ above annihilates its characteristic polynomial. If the characteristic polynomial over $A$ is $p(T)$, then we know that $P(M)=0$. In particular, each entry of $P(M)$ is $0$. But each entry is a polynomial in the indeterminates, which we just saw equals $0$. So no matter which values of $\mathbb C$ we put in for the $t_i$, each entry must vanish. This means every matrix over $\mathbb C$ annihilates its characteristic polynomial. Maybe there is a more elegant way to finish; my justification seems inadequate, but I can't quite put my finger on why. I would appreciate any comments on this, too. Thank you.",,"['linear-algebra', 'cayley-hamilton']"
13,Why is kernel important? [closed],Why is kernel important? [closed],,"It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened, visit the help center . Closed 11 years ago . Why are we interested in looking at the kernel and range (image) of a linear transformation on a linear algebra course?","It's difficult to tell what is being asked here. This question is ambiguous, vague, incomplete, overly broad, or rhetorical and cannot be reasonably answered in its current form. For help clarifying this question so that it can be reopened, visit the help center . Closed 11 years ago . Why are we interested in looking at the kernel and range (image) of a linear transformation on a linear algebra course?",,"['linear-algebra', 'soft-question']"
14,Looking for orthogonal basis of eigenvectors using Gram Schmidt process,Looking for orthogonal basis of eigenvectors using Gram Schmidt process,,"Question Find an orthogonal basis of eigenvectors for the following matrix. The matrix has a repeated eigenvalue so you will need to use the Gram-Schmidt process. $$\begin{bmatrix}5 & 4 & 2\\ 4 & 5 & 2 \\ 2 & 2 & 2 \end{bmatrix}$$ ($\lambda = 1$ is a double eigenvalue.) Answer Well here's what I found for eigenvalues and eigenvectors - For $\lambda = 1$ I found eigenvectors $\begin{bmatrix}-1 \\ 1 \\ 0 \end{bmatrix}$, $\begin{bmatrix}-1 \\ 0 \\ 2 \end{bmatrix}$ For $\lambda = 10$ I found eigenvector $\begin{bmatrix}2 \\ 2 \\ 1 \end{bmatrix}$ Now I can see that the eigenvectors of $\lambda = 1$ are not orthogonal to each other. But how do I do the rest of the question? It says to use the Gram-Schmidt process but if I use that on one of the vectors it will change it's orientation and it will no longer be an eigenvector. So how do I get an orthogonal basis while getting to keep the basis as eigenvectors?","Question Find an orthogonal basis of eigenvectors for the following matrix. The matrix has a repeated eigenvalue so you will need to use the Gram-Schmidt process. $$\begin{bmatrix}5 & 4 & 2\\ 4 & 5 & 2 \\ 2 & 2 & 2 \end{bmatrix}$$ ($\lambda = 1$ is a double eigenvalue.) Answer Well here's what I found for eigenvalues and eigenvectors - For $\lambda = 1$ I found eigenvectors $\begin{bmatrix}-1 \\ 1 \\ 0 \end{bmatrix}$, $\begin{bmatrix}-1 \\ 0 \\ 2 \end{bmatrix}$ For $\lambda = 10$ I found eigenvector $\begin{bmatrix}2 \\ 2 \\ 1 \end{bmatrix}$ Now I can see that the eigenvectors of $\lambda = 1$ are not orthogonal to each other. But how do I do the rest of the question? It says to use the Gram-Schmidt process but if I use that on one of the vectors it will change it's orientation and it will no longer be an eigenvector. So how do I get an orthogonal basis while getting to keep the basis as eigenvectors?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
15,Taylor series of a polynomial,Taylor series of a polynomial,,"Given a polynomial $y=C_0+C_1 x+C_2 x^2+C_3 x^3 + \ldots$ of some order $N$, I can easily calculate the polynomial of reduced order $M$ by taking only the first $M+1$ terms. This is equivalent to doing a Taylor series expansion with $M<=N$ around $x=0$. But what if I want to take the Taylor series expansion around a different point $x_c$. In the end, I want the polynomial coefficients of $y_2=K_0+K_1 x + K_2 x^2 + K_3 x^3 + \ldots$ which represents the Taylor's expansion of $y$ around point $x_c$ such that $y(x_c)=y_2(x_c)$ including the first $M$ derivatives. So given the coefficients $C_i$ with $i=0 \ldots N$, and a location $x_c$ I want to calculate the coefficients $K_j$ with $j=0 \ldots M$. Example Given $y=C_0+C_1 x+C_2 x^2$ ( $N=2$ ) then the tangent line ($M=1$) through $x_c$ is $$ y_2 = (C_0-C_2 x_c^2) + (C_1+2 C_2 x_c) x $$ or $K_0 = C_0-C_2 x_c^2$, and $K_1 =C_1+2 C_2 x_c$ There must be a way to construct a ($M+1$ by $N+1$ ) matrix that transforms the coefficients $C_i$ into $K_j$. For the above example this matrix is $$ \begin{bmatrix}K_{0}\\ K_{1}\end{bmatrix}=\begin{bmatrix}1 & 0 & -x_{c}^{2}\\ 0 & 1 & 2\, x_{c}\end{bmatrix}\begin{bmatrix}C_{0}\\ C_{1}\\ C_{2}\end{bmatrix} $$ Example #2 The reduction of a $5$-th order polynomial to a $3$-rd order around $x_c$ is $$ \begin{bmatrix}K_{0}\\ K_{1}\\ K_{2}\\ K_{3}\end{bmatrix}=\left[\begin{array}{cccc|cc} 1 &  &  &  & -x_{c}^{4} & -4\, x_{c}^{5}\\  & 1 &  &  & 4\, x_{c}^{3} & 15\, x_{c}^{4}\\  &  & 1 &  & -6\, x_{c}^{2} & -20\, x_{c}^{3}\\  &  &  & 1 & 4\, x_{c} & 10\, x_{c}^{2}\end{array}\right]\begin{bmatrix}C_{0}\\ C_{1}\\ C_{2}\\ C_{3}\\ C_{4}\\ C_{5}\end{bmatrix} $$ which is a block matrix, and not an upper diagonal one as some of the answers have indicated.","Given a polynomial $y=C_0+C_1 x+C_2 x^2+C_3 x^3 + \ldots$ of some order $N$, I can easily calculate the polynomial of reduced order $M$ by taking only the first $M+1$ terms. This is equivalent to doing a Taylor series expansion with $M<=N$ around $x=0$. But what if I want to take the Taylor series expansion around a different point $x_c$. In the end, I want the polynomial coefficients of $y_2=K_0+K_1 x + K_2 x^2 + K_3 x^3 + \ldots$ which represents the Taylor's expansion of $y$ around point $x_c$ such that $y(x_c)=y_2(x_c)$ including the first $M$ derivatives. So given the coefficients $C_i$ with $i=0 \ldots N$, and a location $x_c$ I want to calculate the coefficients $K_j$ with $j=0 \ldots M$. Example Given $y=C_0+C_1 x+C_2 x^2$ ( $N=2$ ) then the tangent line ($M=1$) through $x_c$ is $$ y_2 = (C_0-C_2 x_c^2) + (C_1+2 C_2 x_c) x $$ or $K_0 = C_0-C_2 x_c^2$, and $K_1 =C_1+2 C_2 x_c$ There must be a way to construct a ($M+1$ by $N+1$ ) matrix that transforms the coefficients $C_i$ into $K_j$. For the above example this matrix is $$ \begin{bmatrix}K_{0}\\ K_{1}\end{bmatrix}=\begin{bmatrix}1 & 0 & -x_{c}^{2}\\ 0 & 1 & 2\, x_{c}\end{bmatrix}\begin{bmatrix}C_{0}\\ C_{1}\\ C_{2}\end{bmatrix} $$ Example #2 The reduction of a $5$-th order polynomial to a $3$-rd order around $x_c$ is $$ \begin{bmatrix}K_{0}\\ K_{1}\\ K_{2}\\ K_{3}\end{bmatrix}=\left[\begin{array}{cccc|cc} 1 &  &  &  & -x_{c}^{4} & -4\, x_{c}^{5}\\  & 1 &  &  & 4\, x_{c}^{3} & 15\, x_{c}^{4}\\  &  & 1 &  & -6\, x_{c}^{2} & -20\, x_{c}^{3}\\  &  &  & 1 & 4\, x_{c} & 10\, x_{c}^{2}\end{array}\right]\begin{bmatrix}C_{0}\\ C_{1}\\ C_{2}\\ C_{3}\\ C_{4}\\ C_{5}\end{bmatrix} $$ which is a block matrix, and not an upper diagonal one as some of the answers have indicated.",,"['linear-algebra', 'polynomials', 'taylor-expansion']"
16,"If $A$ is a matrix such that $A^T = A^2$, what are eigenvalues of $A$?","If  is a matrix such that , what are eigenvalues of ?",A A^T = A^2 A,"If $A$ is a matrix such that $A^T = A^2$ , what are eigenvalues of $A$ ? Now I read somewhere that changing the matrix by taking a transpose does not change the characteristic polynomial. So it is safe to say that the annihilating polynomial in this case is $x^2 - x$ . If this is the case then I think the answer is quite easy. But if there is some caveat I am missing then I am lost as to how to approach this problem.","If is a matrix such that , what are eigenvalues of ? Now I read somewhere that changing the matrix by taking a transpose does not change the characteristic polynomial. So it is safe to say that the annihilating polynomial in this case is . If this is the case then I think the answer is quite easy. But if there is some caveat I am missing then I am lost as to how to approach this problem.",A A^T = A^2 A x^2 - x,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'minimal-polynomials', 'transpose']"
17,Are $f$ satisfying $|f(y)| = |f(x+y) - f(x)|$ additive?,Are  satisfying  additive?,f |f(y)| = |f(x+y) - f(x)|,"(This question is inspired by this question , and in particular the comment by Charlie Cunningham.) Let $(V, \|\cdot\|)$ be a normed real vector space. Let $f: V \to V$ be a function satisfying, for all $x, y \in V$ , $$ \|f(y)\| = \|f(x+y) - f(x)\|. $$ Does it follow that $f$ is additive, i.e., that $f(x + y) = f(x) + f(y)$ for all $x, y \in V$ ? This seems like it should be a fairly easy question, but I haven't quite figured it out yet. Some fairly trivial observations: Taking $y=0$ gives us $\|f(0)\| = \|f(x) - f(x)\| = 0$ , so $f(0) = 0$ . Taking $y=-x$ gives us $\|f(-x)\| = \|-f(x)\|$ . If we could prove that $f(-x) = -f(x)$ , we could take $y=-2x$ to prove that $\|f(2y)\| = 2\|f(y)\|$ . Using the equation for both $y$ and $-y$ gives us $$ \|f(x+y) - f(x)\| = \|f(x-y) - f(x)\|. $$ It does not matter that $f$ is an endofunction (it could go $f: V \to W$ too) for the answer of the question, but this was a simpler formulation.","(This question is inspired by this question , and in particular the comment by Charlie Cunningham.) Let be a normed real vector space. Let be a function satisfying, for all , Does it follow that is additive, i.e., that for all ? This seems like it should be a fairly easy question, but I haven't quite figured it out yet. Some fairly trivial observations: Taking gives us , so . Taking gives us . If we could prove that , we could take to prove that . Using the equation for both and gives us It does not matter that is an endofunction (it could go too) for the answer of the question, but this was a simpler formulation.","(V, \|\cdot\|) f: V \to V x, y \in V 
\|f(y)\| = \|f(x+y) - f(x)\|.
 f f(x + y) = f(x) + f(y) x, y \in V y=0 \|f(0)\| = \|f(x) - f(x)\| = 0 f(0) = 0 y=-x \|f(-x)\| = \|-f(x)\| f(-x) = -f(x) y=-2x \|f(2y)\| = 2\|f(y)\| y -y 
\|f(x+y) - f(x)\| = \|f(x-y) - f(x)\|.
 f f: V \to W","['linear-algebra', 'functional-equations']"
18,Let $A$ be a $4\times2$ matrix and $B$ be a $2\times4$ matrix. Find $BA$ when $AB$ is given.,Let  be a  matrix and  be a  matrix. Find  when  is given.,A 4\times2 B 2\times4 BA AB,"Let $A$ be a $4\times2$ matrix and $B$ be a $2\times4$ matrix so that $$AB=   \begin{pmatrix}     1 & 0 & -1 & 0 \\     0 & 1 & 0 & -1 \\     -1 & 0 & 1 & 0 \\     0 & -1 & 0 & 1   \end{pmatrix}. $$ Find $BA$ . I have the answer, which should be $BA=  \begin{pmatrix}     2 & 0 \\     0 & 2   \end{pmatrix} $ , but how do I show that this is the only possible solution or is it sufficient (according to some property which I'm not aware of) to have one match for $A$ and $B$ and therefore no other outcome for $BA$ is possible.","Let be a matrix and be a matrix so that Find . I have the answer, which should be , but how do I show that this is the only possible solution or is it sufficient (according to some property which I'm not aware of) to have one match for and and therefore no other outcome for is possible.","A 4\times2 B 2\times4 AB=
  \begin{pmatrix}
    1 & 0 & -1 & 0 \\
    0 & 1 & 0 & -1 \\
    -1 & 0 & 1 & 0 \\
    0 & -1 & 0 & 1
  \end{pmatrix}.
 BA BA=
 \begin{pmatrix}
    2 & 0 \\
    0 & 2
  \end{pmatrix}
 A B BA","['linear-algebra', 'matrices']"
19,"If matrix is diagonalizable, eigenvalue?","If matrix is diagonalizable, eigenvalue?",,"Let $A$ be an $n \times n$ matrix and suppose $A$ is diagonalizable and the only eigenvalue is $\lambda = k$, what can you say about matrix $D$ where $A = P^{-1} D P$, for invertible matrix $P$. So if the only eigenvalue of $A$ is $\lambda = k$, what can I say about $D$? I know that $D$ is a diagonal matrix, but is it necessarily true that $D = \text{diag } (k, k, ... , k)$ ?","Let $A$ be an $n \times n$ matrix and suppose $A$ is diagonalizable and the only eigenvalue is $\lambda = k$, what can you say about matrix $D$ where $A = P^{-1} D P$, for invertible matrix $P$. So if the only eigenvalue of $A$ is $\lambda = k$, what can I say about $D$? I know that $D$ is a diagonal matrix, but is it necessarily true that $D = \text{diag } (k, k, ... , k)$ ?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
20,Are the eigenvalues of the sum of two positive definite matrices increased?,Are the eigenvalues of the sum of two positive definite matrices increased?,,"Let $A$ and $B$ be two $n \times n$ (symmetric) positive definite matrices, and denote the $k$ th smallest eigenvalue of a general $n \times n$ matrix by $\lambda_k(X)$ , $k = 1, 2, \ldots, n$ so that $$\lambda_1(X) \leq \lambda_2(X) \leq \cdots \leq \lambda_n(X).$$ I guess the following relation holds: $$\lambda_k(A + B) > \max\{\lambda_k(A), \lambda_k(B)\}, \; k = 1, 2, \ldots, n.$$ This looks intuitive but I have difficulty to prove it, any hints?","Let and be two (symmetric) positive definite matrices, and denote the th smallest eigenvalue of a general matrix by , so that I guess the following relation holds: This looks intuitive but I have difficulty to prove it, any hints?","A B n \times n k n \times n \lambda_k(X) k = 1, 2, \ldots, n \lambda_1(X) \leq \lambda_2(X) \leq \cdots \leq \lambda_n(X). \lambda_k(A + B) > \max\{\lambda_k(A), \lambda_k(B)\}, \; k = 1, 2, \ldots, n.","['linear-algebra', 'eigenvalues-eigenvectors']"
21,Why does $\det A=0$ imply that there exists a non-zero solution to the homogeneous linear equations determined by $A$?,Why does  imply that there exists a non-zero solution to the homogeneous linear equations determined by ?,\det A=0 A,"Why does $\det A=0$ imply that there exists a non-zero solution to the homogeneous linear equations determined by $A$ ? Moreover, is it an ""if and only if"" case? That is, if there's a non-zero solution to the set of linear equations, does the matrix determined by the coefficients automatically have a determinant of zero?","Why does imply that there exists a non-zero solution to the homogeneous linear equations determined by ? Moreover, is it an ""if and only if"" case? That is, if there's a non-zero solution to the set of linear equations, does the matrix determined by the coefficients automatically have a determinant of zero?",\det A=0 A,"['linear-algebra', 'matrices', 'systems-of-equations', 'determinant']"
22,Group action on a category,Group action on a category,,"Motivating example: We get a functor from the category of real vector spaces to the category of complex vector spaces by complexifying (i.e. tensoring over $\mathbb{R}$ with $\mathbb{C}$). Let $\sigma$ denote complex conjugation. This induces a functor, $\sigma^*$, from the category of complex vector spaces into itself, also by tensoring with $\mathbb{C}$, but over $\mathbb{C}$ and via $\sigma$. The image of this can then be identified with the pairs $(V,f)$ of complex vector spaces $V$ together with a morphism $\sigma^*V\rightarrow V$, s.t. $f\circ \sigma^*f=\operatorname{id}_V$ where we secretely identify $(\sigma^*)^2V$ with $V$. Question :  What would be the good notion of a group acting on a category to say in the above example that the fixed points of that action are precisely the real vector spaces? Remarks: In particular this notion should also work for an arbitrary galois field extension $L|K$ and the group $Gal(L|K)$ acting on the category of $L$ vector spaces (Or a similar category which can be constructed from it) The question came up, when I was reading about some abstract aspects of Hodge theory, where one sometimes considers (the category of) ""complex Hodge structures"", has $Gal(\mathbb{C}|\mathbb{R})$ and wants to identify the real Hodge structures as fixed points under a $Gal(\mathbb{C}|\mathbb{R})$. This can be done similarly as in the motivating example but I was wondering if there was a general framework for doing this.","Motivating example: We get a functor from the category of real vector spaces to the category of complex vector spaces by complexifying (i.e. tensoring over $\mathbb{R}$ with $\mathbb{C}$). Let $\sigma$ denote complex conjugation. This induces a functor, $\sigma^*$, from the category of complex vector spaces into itself, also by tensoring with $\mathbb{C}$, but over $\mathbb{C}$ and via $\sigma$. The image of this can then be identified with the pairs $(V,f)$ of complex vector spaces $V$ together with a morphism $\sigma^*V\rightarrow V$, s.t. $f\circ \sigma^*f=\operatorname{id}_V$ where we secretely identify $(\sigma^*)^2V$ with $V$. Question :  What would be the good notion of a group acting on a category to say in the above example that the fixed points of that action are precisely the real vector spaces? Remarks: In particular this notion should also work for an arbitrary galois field extension $L|K$ and the group $Gal(L|K)$ acting on the category of $L$ vector spaces (Or a similar category which can be constructed from it) The question came up, when I was reading about some abstract aspects of Hodge theory, where one sometimes considers (the category of) ""complex Hodge structures"", has $Gal(\mathbb{C}|\mathbb{R})$ and wants to identify the real Hodge structures as fixed points under a $Gal(\mathbb{C}|\mathbb{R})$. This can be done similarly as in the motivating example but I was wondering if there was a general framework for doing this.",,"['linear-algebra', 'category-theory']"
23,"Insights about $Tv_j=w_j$, the linear maps and basis of domain.","Insights about , the linear maps and basis of domain.",Tv_j=w_j,"I'm now self-studying the book Linear Algebra Done Right. In chapter 3, I learned the theorem 3.5, Linear maps and basis of domain: Suppose $v_1,...,v_n$ is a basis of V and $w_1,...,w_n∈W$. Then there exists a unique linear map $T:V→W$ such that $$Tv_j=w_j$$ for each $j=1,...,n$. ""The existence part of the next result means that we can find a linear map that takes on whatever values we wish on the vectors in a basis. The uniqueness part of the next result means that a linear map is completely determined by its values on a basis."" -- a paragraph before this theorem. I'm still wondering how to make use of this theorem. It confused me quite a while, so would you please help me, really really appreciate it.","I'm now self-studying the book Linear Algebra Done Right. In chapter 3, I learned the theorem 3.5, Linear maps and basis of domain: Suppose $v_1,...,v_n$ is a basis of V and $w_1,...,w_n∈W$. Then there exists a unique linear map $T:V→W$ such that $$Tv_j=w_j$$ for each $j=1,...,n$. ""The existence part of the next result means that we can find a linear map that takes on whatever values we wish on the vectors in a basis. The uniqueness part of the next result means that a linear map is completely determined by its values on a basis."" -- a paragraph before this theorem. I'm still wondering how to make use of this theorem. It confused me quite a while, so would you please help me, really really appreciate it.",,"['linear-algebra', 'linear-transformations']"
24,Show that the additive inverse condition can be replaced by $0v = v$ for all $v \in V$,Show that the additive inverse condition can be replaced by  for all,0v = v v \in V,"In the definition of a vector space, the additive inverse condition   requires that for every $v \in V$ (where $V$ is a vector space over   $\mathbf{F} = \mathbb{R}$ or $\mathbb{C}$), there exists $w \in V$   such that $$v + w = 0$$  Show that this condition can be replaced with   the condition that $$0v = 0$$ for all $v \in V$. The $0$ on the left side is the number $0$ in   $\mathbf{F}$ and the $0$ on the right side is the additive identity of   $V$. (The phrase ""a condition can be replaced"" in a definition means that the >collection of objects satisfying the definition is unchanged if the >original condition is replaced with a the new condition) (Taken from ""Linear Algebra Done Right (3rd edition), by S. Axler) I am new to abstract algebra and have close to no clue how to tackle this problem. Here is my futile ""attempt"" nonetheless: I tried some substitution and got $$v + w = 0v$$ but am unable to make any sense out of this, much less determine if I am headed towards the right direction.","In the definition of a vector space, the additive inverse condition   requires that for every $v \in V$ (where $V$ is a vector space over   $\mathbf{F} = \mathbb{R}$ or $\mathbb{C}$), there exists $w \in V$   such that $$v + w = 0$$  Show that this condition can be replaced with   the condition that $$0v = 0$$ for all $v \in V$. The $0$ on the left side is the number $0$ in   $\mathbf{F}$ and the $0$ on the right side is the additive identity of   $V$. (The phrase ""a condition can be replaced"" in a definition means that the >collection of objects satisfying the definition is unchanged if the >original condition is replaced with a the new condition) (Taken from ""Linear Algebra Done Right (3rd edition), by S. Axler) I am new to abstract algebra and have close to no clue how to tackle this problem. Here is my futile ""attempt"" nonetheless: I tried some substitution and got $$v + w = 0v$$ but am unable to make any sense out of this, much less determine if I am headed towards the right direction.",,"['linear-algebra', 'abstract-algebra']"
25,Geometric proof for triple vector product Jacobi identity,Geometric proof for triple vector product Jacobi identity,,I believe the vector identity $\vec{a} \times (\vec{b} \times \vec{c}) + \vec{b} \times (\vec{c} \times \vec{a}) + \vec{c} \times (\vec{a} \times \vec{b}) = 0$ is called the Jacobi identity and I know the proof. Does anybody know of some elegant geometrical picture to illustrate why the identity is true?,I believe the vector identity $\vec{a} \times (\vec{b} \times \vec{c}) + \vec{b} \times (\vec{c} \times \vec{a}) + \vec{c} \times (\vec{a} \times \vec{b}) = 0$ is called the Jacobi identity and I know the proof. Does anybody know of some elegant geometrical picture to illustrate why the identity is true?,,"['linear-algebra', 'vector-analysis', 'cross-product']"
26,Is every element of a complex semisimple Lie algebra a commutator?,Is every element of a complex semisimple Lie algebra a commutator?,,"Let $L$ be a (finite-dimensional) complex semisimple Lie algebra. Then we know that $L = [L,L]$. Is it true that every element of $L$ must be a commutator? Since a complex semisimple Lie algebra is a direct sum of simple Lie algebras, this question reduces to the case where $L$ is simple. For example, we know that every complex matrix with trace zero is a commutator, see this question . So this is true when $L = \mathfrak{sl}_n(\mathbb{C})$. But what about other families of simple Lie algebras? Is this known for classical simple Lie algebras? What about the exceptional Lie algebras?","Let $L$ be a (finite-dimensional) complex semisimple Lie algebra. Then we know that $L = [L,L]$. Is it true that every element of $L$ must be a commutator? Since a complex semisimple Lie algebra is a direct sum of simple Lie algebras, this question reduces to the case where $L$ is simple. For example, we know that every complex matrix with trace zero is a commutator, see this question . So this is true when $L = \mathfrak{sl}_n(\mathbb{C})$. But what about other families of simple Lie algebras? Is this known for classical simple Lie algebras? What about the exceptional Lie algebras?",,"['linear-algebra', 'abstract-algebra', 'reference-request', 'lie-algebras', 'semisimple-lie-algebras']"
27,What does it mean to pivot (linear algebra)?,What does it mean to pivot (linear algebra)?,,"So I'm told that if the matrix is symmetric positive definitive (as the one below is), pivoting is not required when using Gaussian elimination. $A = \begin{bmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix}$ So I've been googling around trying to actually get a definition of what pivoting is and I can't find a straightforward answer. I've read that it means to do a row swap. I've read that it means to ""make an element above or below leading one into a zero."" I guess what I've always just done is: make $a_{1,1} = 1$, make zeros below. Make $a_{2,2} = 1$, make zeros below. etc.","So I'm told that if the matrix is symmetric positive definitive (as the one below is), pivoting is not required when using Gaussian elimination. $A = \begin{bmatrix} 2 & -1 & 0 \\ -1 & 2 & -1 \\ 0 & -1 & 2 \end{bmatrix}$ So I've been googling around trying to actually get a definition of what pivoting is and I can't find a straightforward answer. I've read that it means to do a row swap. I've read that it means to ""make an element above or below leading one into a zero."" I guess what I've always just done is: make $a_{1,1} = 1$, make zeros below. Make $a_{2,2} = 1$, make zeros below. etc.",,"['linear-algebra', 'numerical-methods']"
28,"Prove that if $\operatorname{rank}A=n$, then $\operatorname{rank}AB=\operatorname{rank}B$","Prove that if , then",\operatorname{rank}A=n \operatorname{rank}AB=\operatorname{rank}B,"Let $A \in M_{m\times n}(\mathbb{R})$ and $B \in M_{n\times p}(\mathbb{R})$ . Prove that if $\operatorname{rank}(A)=n$ then $\operatorname{rank}(AB)=\operatorname{rank}(B)$ . I tried to start with definitions finding that $n \le m$ , but didn't know what to do with $AB$ . Please help, thank you!","Let and . Prove that if then . I tried to start with definitions finding that , but didn't know what to do with . Please help, thank you!",A \in M_{m\times n}(\mathbb{R}) B \in M_{n\times p}(\mathbb{R}) \operatorname{rank}(A)=n \operatorname{rank}(AB)=\operatorname{rank}(B) n \le m AB,"['linear-algebra', 'matrices', 'matrix-calculus']"
29,Is the set of all invertible $n \times n$ matrices a vector space?,Is the set of all invertible  matrices a vector space?,n \times n,"I'm studying Algebra and I'm asked to prove or disprove ""Is the set of all invertible $n \times n$ matrices a vector space?"" I assume with respect to the usual matrix-sum and scalar multiplication.  I found that is true, but I'm not sure how to prove it. My problem here is that this statement is too ""broad"", i.e. I cannot create a matrix with arbitrary values a,b,c,d,(...) considering that I don't know the size of the matrix. My idea was to prove in a first time that the set of all invertible 2 x 2 matrices of real number is a vector space and then to show that this property could be extended to bigger matrices. However I don't know how to do it and it's precisely here that I need a little help ; how can I show that we can extend our statement? Is it enough to say that the matrix's size doesn't affect in any way properties we need to check?  Is my overall strategy wrong? Any form of help would be very appreciated on this dubious answer.","I'm studying Algebra and I'm asked to prove or disprove ""Is the set of all invertible $n \times n$ matrices a vector space?"" I assume with respect to the usual matrix-sum and scalar multiplication.  I found that is true, but I'm not sure how to prove it. My problem here is that this statement is too ""broad"", i.e. I cannot create a matrix with arbitrary values a,b,c,d,(...) considering that I don't know the size of the matrix. My idea was to prove in a first time that the set of all invertible 2 x 2 matrices of real number is a vector space and then to show that this property could be extended to bigger matrices. However I don't know how to do it and it's precisely here that I need a little help ; how can I show that we can extend our statement? Is it enough to say that the matrix's size doesn't affect in any way properties we need to check?  Is my overall strategy wrong? Any form of help would be very appreciated on this dubious answer.",,"['linear-algebra', 'matrices', 'proof-writing', 'matrix-calculus']"
30,equivalence between matrix multiplication and matrix inversion,equivalence between matrix multiplication and matrix inversion,,"I am a bit confused with this wikipedia article , hoping someone can clarify it. Looking at the Strassen algorithm page it is clear that this is an algorithm for reducing multiplication operations. Why is a matrix multiplication algorithm touted as a matrix inversion algorithm? last time i checked it was totally odd to apply Gauss-Jordan elimination to multiply matrices thanks","I am a bit confused with this wikipedia article , hoping someone can clarify it. Looking at the Strassen algorithm page it is clear that this is an algorithm for reducing multiplication operations. Why is a matrix multiplication algorithm touted as a matrix inversion algorithm? last time i checked it was totally odd to apply Gauss-Jordan elimination to multiply matrices thanks",,"['linear-algebra', 'matrices', 'computational-complexity']"
31,"Relationship between tuples, vectors and column/row matrices","Relationship between tuples, vectors and column/row matrices",,"I am taking a course in linear algebra at the moment, and the book I have uses $1\times n$ matrices, $n\times 1$ matrices and $n$-tuples to represent vectors. In condition I have been taught that $1\times n$ and $n\times 1$ matrices are vectors. What, then, is the difference between an $n$-tuple and a $1\times n$ matrix? What do we need tuples for, that we can't use matrices for? I see that the product between tuples are defined in another way than products between matrices. The product between two $1\times n$ matrices isn't even defined (if $n\neq 1$). But this we can easily solve with trasposing one of them. I hope you can help to clarify this for me.","I am taking a course in linear algebra at the moment, and the book I have uses $1\times n$ matrices, $n\times 1$ matrices and $n$-tuples to represent vectors. In condition I have been taught that $1\times n$ and $n\times 1$ matrices are vectors. What, then, is the difference between an $n$-tuple and a $1\times n$ matrix? What do we need tuples for, that we can't use matrices for? I see that the product between tuples are defined in another way than products between matrices. The product between two $1\times n$ matrices isn't even defined (if $n\neq 1$). But this we can easily solve with trasposing one of them. I hope you can help to clarify this for me.",,"['linear-algebra', 'definition']"
32,Has there been a rigorous analysis of Strassen's algorithm?,Has there been a rigorous analysis of Strassen's algorithm?,,"According to Wikipedia, Strassen's Algorithm runs in $O(N^{2.807})$ time.  Has anyone seen a more rigorous analysis displaying constants, possibly in a specific language such as C or Java? I realize this will vary from language to language, machine to machine etc, but does anyone know of an approximate input size where Strassen's algorithm starts to outperform regular matrix multiplication? I think this may belong on the computer science stack exchange but since it is somewhat mathematical I thought I would post it here.","According to Wikipedia, Strassen's Algorithm runs in $O(N^{2.807})$ time.  Has anyone seen a more rigorous analysis displaying constants, possibly in a specific language such as C or Java? I realize this will vary from language to language, machine to machine etc, but does anyone know of an approximate input size where Strassen's algorithm starts to outperform regular matrix multiplication? I think this may belong on the computer science stack exchange but since it is somewhat mathematical I thought I would post it here.",,"['linear-algebra', 'matrices', 'algorithms', 'computational-complexity']"
33,Does the association $V \mapsto GL(V)$ define a functor?,Does the association  define a functor?,V \mapsto GL(V),"As is stated in the title: Question: for $k$ a field, does there exist a functor $F: k$ - $\mathrm{v.s.} \to \mathrm{Grp}$ which on objects is $V \mapsto GL(V)$ ? My guess is no, because it doesn't look to me like there's a sensible way to map the morphisms. However, I can't come up with a counterexample in a similar way to Arturo Magidin in Why is there no functor $\mathsf{Group}\to\mathsf{AbGroup}$ sending groups to their centers? . Here is my attempt at imitating his answer: take $f: k \to k^2$ , $f = \begin{pmatrix} 1 \\ 0 \end{pmatrix}$ , $g: k^2 \to k$ , $g = \begin{pmatrix} 1 & 0 \end{pmatrix}$ . Then $g \circ f = 1_k$ , so $F(f):k^{\times} \to GL_2(k)$ is injective and $F(g): GL_2(k) \to k^\times$ is surjective. On the other hand, $f \circ g = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}$ , but this doesn't really give me anything.","As is stated in the title: Question: for a field, does there exist a functor - which on objects is ? My guess is no, because it doesn't look to me like there's a sensible way to map the morphisms. However, I can't come up with a counterexample in a similar way to Arturo Magidin in Why is there no functor $\mathsf{Group}\to\mathsf{AbGroup}$ sending groups to their centers? . Here is my attempt at imitating his answer: take , , , . Then , so is injective and is surjective. On the other hand, , but this doesn't really give me anything.",k F: k \mathrm{v.s.} \to \mathrm{Grp} V \mapsto GL(V) f: k \to k^2 f = \begin{pmatrix} 1 \\ 0 \end{pmatrix} g: k^2 \to k g = \begin{pmatrix} 1 & 0 \end{pmatrix} g \circ f = 1_k F(f):k^{\times} \to GL_2(k) F(g): GL_2(k) \to k^\times f \circ g = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix},"['linear-algebra', 'category-theory', 'linear-transformations', 'general-linear-group']"
34,Do complex eigenvalues for planar systems with a real valued matrix always have complex-valued eigenvectors?,Do complex eigenvalues for planar systems with a real valued matrix always have complex-valued eigenvectors?,,"sorry if this is a dumb question, but I was doing some homework and I noticed that whenever I solved a planar system which had complex eigenvalues, I would always end up with complex eigenvectors. I was wondering whether I could ever somehow get a totally real-valued eigenvector from a complex eigenvalue.","sorry if this is a dumb question, but I was doing some homework and I noticed that whenever I solved a planar system which had complex eigenvalues, I would always end up with complex eigenvectors. I was wondering whether I could ever somehow get a totally real-valued eigenvector from a complex eigenvalue.",,['linear-algebra']
35,"Armed with a sword and a shield, a proud knight combats a monstrous hydra with 100 heads.","Armed with a sword and a shield, a proud knight combats a monstrous hydra with 100 heads.",,"Hi so I am battling with this question at the moment. ""Armed with a sword and a shield, a proud knight combats a monstrous hydra with 100 heads. When he slashes, he removes 17 heads and 5 heads grow back. When he slices, he removes 6 heads and 33 grow back. When he cuts, 14 heads fall and 8 grow back. When he stabs, 2 heads fall and 23 grow back. In order to kill the hydra, all its heads must be removed at some point, in which case no heads will grow back. Can the knight’s sword and courage triumph against this mythological monster?"" I am assuming the hydra cannot grow more than the original 100 heads, and that you must cut off the exact amount (you can't slash 9 times). My current working consists of: rule a = shrink 6 rule b = shrink 12 rule c = grow 21 rule d = grow 27 b can be discounted as it's just aa as 100 is the start point, and 6 is the minimum reduction some sum of 100 + c(x) + d(y) = a(z), or 100 + (21 * x) + (27 * y) must equal a multiple of 6. 100 + (21 * x) + (27 * y) = 6 * z where x, y, z are positive integers simplifies to z = (7x/2) + (9y/2) + (50/3). For all positive integers x, y, there is no integer solution for z I am not convinced this is enough proof.","Hi so I am battling with this question at the moment. ""Armed with a sword and a shield, a proud knight combats a monstrous hydra with 100 heads. When he slashes, he removes 17 heads and 5 heads grow back. When he slices, he removes 6 heads and 33 grow back. When he cuts, 14 heads fall and 8 grow back. When he stabs, 2 heads fall and 23 grow back. In order to kill the hydra, all its heads must be removed at some point, in which case no heads will grow back. Can the knight’s sword and courage triumph against this mythological monster?"" I am assuming the hydra cannot grow more than the original 100 heads, and that you must cut off the exact amount (you can't slash 9 times). My current working consists of: rule a = shrink 6 rule b = shrink 12 rule c = grow 21 rule d = grow 27 b can be discounted as it's just aa as 100 is the start point, and 6 is the minimum reduction some sum of 100 + c(x) + d(y) = a(z), or 100 + (21 * x) + (27 * y) must equal a multiple of 6. 100 + (21 * x) + (27 * y) = 6 * z where x, y, z are positive integers simplifies to z = (7x/2) + (9y/2) + (50/3). For all positive integers x, y, there is no integer solution for z I am not convinced this is enough proof.",,"['linear-algebra', 'proof-verification', 'puzzle']"
36,Is there a semi-norm that respects matrix similitary?,Is there a semi-norm that respects matrix similitary?,,"It is not possible to have a norm on $M_n(\mathbb C)$ that respects similitary for $n > 1$ since, for example, if $A \sim 2A$ and $A \neq 0_n$ then $N(A) = 2N(A)$ contradicts the separating property. For example, $A$ s.t. $A_{1,2} = 1$ and with $0$ elsewhere. So the question is: Is there a semi-norm (i.e. without the separating property) on $M_n(\mathbb C)$ that respects matrix similitary?","It is not possible to have a norm on $M_n(\mathbb C)$ that respects similitary for $n > 1$ since, for example, if $A \sim 2A$ and $A \neq 0_n$ then $N(A) = 2N(A)$ contradicts the separating property. For example, $A$ s.t. $A_{1,2} = 1$ and with $0$ elsewhere. So the question is: Is there a semi-norm (i.e. without the separating property) on $M_n(\mathbb C)$ that respects matrix similitary?",,"['linear-algebra', 'matrices']"
37,Definition of an affine set,Definition of an affine set,,"Some resource says that a set $A \subset V$ is affine if $\forall x,y \in A, t \in F$, $tx+(1-t)y \in A$ while others say that $A$  is affine if $\forall x_1,\dotsb,x_n \in A, a_1, \dotsb a_n, \in F, a_1+\dotsb+a_n = 1$, $a_1x_1+\dotsb+a_nx_n \in A$. I am trying to show that this two is equivalent, suppose that I have the first one holds. Then I want to show $\forall x_1,\dotsb,x_n \in A, a_1, \dotsb a_n, \in F, a_1+\dotsb+a_n = 1$, $a_1x_1+\dotsb+a_nx_n \in A$. I first consider a simple case where there are only three elements, ${x_1,x_2,x_3}$,but I do not think it is viable. So maybe the first one is wrong? Because the second definition includes the interior of a triangle while the first does not.","Some resource says that a set $A \subset V$ is affine if $\forall x,y \in A, t \in F$, $tx+(1-t)y \in A$ while others say that $A$  is affine if $\forall x_1,\dotsb,x_n \in A, a_1, \dotsb a_n, \in F, a_1+\dotsb+a_n = 1$, $a_1x_1+\dotsb+a_nx_n \in A$. I am trying to show that this two is equivalent, suppose that I have the first one holds. Then I want to show $\forall x_1,\dotsb,x_n \in A, a_1, \dotsb a_n, \in F, a_1+\dotsb+a_n = 1$, $a_1x_1+\dotsb+a_nx_n \in A$. I first consider a simple case where there are only three elements, ${x_1,x_2,x_3}$,but I do not think it is viable. So maybe the first one is wrong? Because the second definition includes the interior of a triangle while the first does not.",,"['linear-algebra', 'analysis']"
38,Geometrical interpretations of SVD,Geometrical interpretations of SVD,,"I'm a bit confused by the various geometrical/visual interpretations of SVD or better I'm wondering how to reconcile them. Transformations : As explained here , the 3 matrices produced by the SVD can be interpreted as rotation, scaling, rotation. Projection on an axis (dimensionality reduction): As explained here , SVD enables to capture the most of the variance in the data by selecting the right orthogonal axes. My (very naive) question is: are these 2 interpretations related visually or not at all, and if yes, how?  Or are they 2 unrelated applications of SVD? As far as I understand, the transformations of the matrices (1) can be applied to anything/any object, so it's not about the data mentioned in (2) so I would say it's not related but I may be wrong (because in (2) we also talk about rotations...). Please note that I'm looking for the intuition with as few math as possible (I don't know much linear algebra as you may have guessed). Many thanks.","I'm a bit confused by the various geometrical/visual interpretations of SVD or better I'm wondering how to reconcile them. Transformations : As explained here , the 3 matrices produced by the SVD can be interpreted as rotation, scaling, rotation. Projection on an axis (dimensionality reduction): As explained here , SVD enables to capture the most of the variance in the data by selecting the right orthogonal axes. My (very naive) question is: are these 2 interpretations related visually or not at all, and if yes, how?  Or are they 2 unrelated applications of SVD? As far as I understand, the transformations of the matrices (1) can be applied to anything/any object, so it's not about the data mentioned in (2) so I would say it's not related but I may be wrong (because in (2) we also talk about rotations...). Please note that I'm looking for the intuition with as few math as possible (I don't know much linear algebra as you may have guessed). Many thanks.",,"['linear-algebra', 'geometry', 'intuition', 'svd']"
39,Dual basis with respect to bilinear form,Dual basis with respect to bilinear form,,"Let $V, W$ be $n$-dimensional $K$-vector spaces with a non-degenerate bilinear form $(\cdot, \cdot) : V \times W \to K$. We call a basis $(\beta_1, \dots, \beta_n)$ for $W$ dual to a basis $(\alpha_1, \dots, \alpha_n)$ for $V$ with respect to $(\cdot, \cdot)$ if for all $i, j$ we have $(\alpha_i, \beta_j) = \delta_{i j}$, where $\delta_{i j}$ denotes the Kronecker delta. The above I've found is fairly standard terminology. I've seen assertions that such a dual basis always exists and it is unique, but I haven't really seen a proof. Can anyone point me in the right direction for this proof? Is it constructive?","Let $V, W$ be $n$-dimensional $K$-vector spaces with a non-degenerate bilinear form $(\cdot, \cdot) : V \times W \to K$. We call a basis $(\beta_1, \dots, \beta_n)$ for $W$ dual to a basis $(\alpha_1, \dots, \alpha_n)$ for $V$ with respect to $(\cdot, \cdot)$ if for all $i, j$ we have $(\alpha_i, \beta_j) = \delta_{i j}$, where $\delta_{i j}$ denotes the Kronecker delta. The above I've found is fairly standard terminology. I've seen assertions that such a dual basis always exists and it is unique, but I haven't really seen a proof. Can anyone point me in the right direction for this proof? Is it constructive?",,['linear-algebra']
40,The number of $3\times 3$ nilpotent matrices over $\mathbb{F}_q$ using the orbit-stabilizer theorem [duplicate],The number of  nilpotent matrices over  using the orbit-stabilizer theorem [duplicate],3\times 3 \mathbb{F}_q,"This question already has an answer here : How many matrices in $M_n(\mathbb{F}_q)$ are nilpotent? (1 answer) Closed 10 months ago . The Fine-Herstein theorem says that the number of nilpotent $n\times n$ matrices over $\mathbb{F}_q$ is $q^{n^2-n}$ . I am trying to verify this for the case $n=3$ using the orbit-stabilizer theorem. Here is my method: Knowing that the minimal polynomial is $x^m$ for $m\leq n$ , I determine the possible rational canonical forms: $$\left( \begin{array}{ccc} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{array} \right), \qquad \left( \begin{array}{ccc} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 1 & 0 \end{array} \right), \qquad \left( \begin{array}{ccc} 0 & 0 & 0 \\ 1 & 0 & 0 \\ 0 & 1 & 0 \end{array} \right)$$ Let $\mbox{GL}_n(\mathbb{F}_q)$ act by conjugation on each of these matrices, then the sum of the sizes of the three orbits is the number of nilpotent matrices. We know that $\left| \mbox{GL}_n (\mathbb{F}_q) \right| =(q^n-1)\cdots (q^n-q^{n-1})$ , so that all the work goes in to finding the size of the stabilizer. This method worked like a charm for the $n=2$ case, but I am having trouble finding what an element of the stabilizer looks like in each case for $n=3$ . Is this possible to do just by conjugating by a general matrix and setting it equal to the class representative?","This question already has an answer here : How many matrices in $M_n(\mathbb{F}_q)$ are nilpotent? (1 answer) Closed 10 months ago . The Fine-Herstein theorem says that the number of nilpotent matrices over is . I am trying to verify this for the case using the orbit-stabilizer theorem. Here is my method: Knowing that the minimal polynomial is for , I determine the possible rational canonical forms: Let act by conjugation on each of these matrices, then the sum of the sizes of the three orbits is the number of nilpotent matrices. We know that , so that all the work goes in to finding the size of the stabilizer. This method worked like a charm for the case, but I am having trouble finding what an element of the stabilizer looks like in each case for . Is this possible to do just by conjugating by a general matrix and setting it equal to the class representative?","n\times n \mathbb{F}_q q^{n^2-n} n=3 x^m m\leq n \left( \begin{array}{ccc}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0 \end{array} \right), \qquad \left( \begin{array}{ccc}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 1 & 0 \end{array} \right), \qquad \left( \begin{array}{ccc}
0 & 0 & 0 \\
1 & 0 & 0 \\
0 & 1 & 0 \end{array} \right) \mbox{GL}_n(\mathbb{F}_q) \left| \mbox{GL}_n (\mathbb{F}_q) \right| =(q^n-1)\cdots (q^n-q^{n-1}) n=2 n=3","['linear-algebra', 'abstract-algebra', 'matrices', 'finite-fields', 'nilpotence']"
41,"Linear independence of $\sqrt{2}$, $\sqrt[3]{2}$, $\sqrt[4]{2}, \dots$ over the rationals","Linear independence of , ,  over the rationals","\sqrt{2} \sqrt[3]{2} \sqrt[4]{2}, \dots","I was trying to prove that $\sqrt{2}, \sqrt[3]{2}, ... $ are linearly independent over the rationals using elementary knowledge of rational numbers. But I could not come up with any proof using simple arguments.  How to prove that the set $$\{\sqrt[n]{2}\; :\; n=2,3,4,...\}$$ is linearly independent over the field $\mathbb{Q}$ ?",I was trying to prove that are linearly independent over the rationals using elementary knowledge of rational numbers. But I could not come up with any proof using simple arguments.  How to prove that the set is linearly independent over the field ?,"\sqrt{2}, \sqrt[3]{2}, ...  \{\sqrt[n]{2}\; :\; n=2,3,4,...\} \mathbb{Q}","['linear-algebra', 'radicals']"
42,Is $O_n$ isomorphic to $SO_n \times \{\pm I\}$?,Is  isomorphic to ?,O_n SO_n \times \{\pm I\},"This question is taken directly from Artin's ""Algebra"", on page 150: Is $O_{n}$ isomorphic to the product group of $SO_{n} \times \{\pm1\}$? Here, $O_{n}$ is defined as the group of orthogonal matrices, $SO_{n}$ is the special group of orthogonal matrices (i.e. orthogonal matrices which have determinant or $1$ or $-1$).","This question is taken directly from Artin's ""Algebra"", on page 150: Is $O_{n}$ isomorphic to the product group of $SO_{n} \times \{\pm1\}$? Here, $O_{n}$ is defined as the group of orthogonal matrices, $SO_{n}$ is the special group of orthogonal matrices (i.e. orthogonal matrices which have determinant or $1$ or $-1$).",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'orthogonal-matrices']"
43,Eigenvalues of product of two hermitian matrices,Eigenvalues of product of two hermitian matrices,,"I know that if $A$ and $B$ are hermitian matrices, then it doesn't follow that the eigenvalues of $AB$ are real, because of the following counter-example: $A=\begin{bmatrix} 0 &1 \\   1& 0 \end{bmatrix}$ and $B=\begin{bmatrix} 1 & 0\\   0& -1 \end{bmatrix}$ On the other hand, I came across the following problem, which says that if $A$ is hermitian and positive definite, and $B$ is hermitian, then $AB$ has real eigenvalues. Why if we add the property ""positive definite"" to $A$, the eigenvalues of $AB$ become real? The proof I read in the book says: Let $\lambda $ be an eigenvalue of the hermitian matrix $AB$ with non zero eigenvector $x$. Then: $$\left \langle BABx,x \right \rangle=\left \langle ABx,Bx \right \rangle=\left \langle \lambda x,Bx \right \rangle=\lambda \left \langle x,Bx \right \rangle$$ Since $$ \left \langle BABx,x \right \rangle$$ and $$\left \langle x,Bx \right \rangle,$$ then $\lambda$ is real. However, I can't see where in the proof the fact that $A$ is positive definite is used. Can anyone explain, please?","I know that if $A$ and $B$ are hermitian matrices, then it doesn't follow that the eigenvalues of $AB$ are real, because of the following counter-example: $A=\begin{bmatrix} 0 &1 \\   1& 0 \end{bmatrix}$ and $B=\begin{bmatrix} 1 & 0\\   0& -1 \end{bmatrix}$ On the other hand, I came across the following problem, which says that if $A$ is hermitian and positive definite, and $B$ is hermitian, then $AB$ has real eigenvalues. Why if we add the property ""positive definite"" to $A$, the eigenvalues of $AB$ become real? The proof I read in the book says: Let $\lambda $ be an eigenvalue of the hermitian matrix $AB$ with non zero eigenvector $x$. Then: $$\left \langle BABx,x \right \rangle=\left \langle ABx,Bx \right \rangle=\left \langle \lambda x,Bx \right \rangle=\lambda \left \langle x,Bx \right \rangle$$ Since $$ \left \langle BABx,x \right \rangle$$ and $$\left \langle x,Bx \right \rangle,$$ then $\lambda$ is real. However, I can't see where in the proof the fact that $A$ is positive definite is used. Can anyone explain, please?",,"['linear-algebra', 'eigenvalues-eigenvectors']"
44,Inner Product vs Metric,Inner Product vs Metric,,"So I know the properties of a metric are: $	{\displaystyle d(x,y)\geq 0}$ ${\displaystyle d(x,y)=0\Leftrightarrow x=y}	$ ${\displaystyle d(x,y)=d(y,x)}$ ${\displaystyle d(x,z)\leq d(x,y)+d(y,z)}$ ( triangle inequality) Inner products have analogous properties to 1. and 2. They have a conjugate symmety, which is analogous to 3. if we ignore complex numbers/spaces They don't have a triangle inequality, but they do have linearity in the first arugment. So they both have a property that allows us to deal with a third vector/point from the space. So, since both metrics and inner product are defined to somehow ""measure distance"", and have similar properties, I was wondering if a metric is in some sense a broader version of an inner product? The norm in R $^n$ is both an inner product and a metric, but there are other metrics that can be defined on R $^n$ as well. And if an inner product is just a metric, why do we bother with inner products in the first place?","So I know the properties of a metric are: ( triangle inequality) Inner products have analogous properties to 1. and 2. They have a conjugate symmety, which is analogous to 3. if we ignore complex numbers/spaces They don't have a triangle inequality, but they do have linearity in the first arugment. So they both have a property that allows us to deal with a third vector/point from the space. So, since both metrics and inner product are defined to somehow ""measure distance"", and have similar properties, I was wondering if a metric is in some sense a broader version of an inner product? The norm in R is both an inner product and a metric, but there are other metrics that can be defined on R as well. And if an inner product is just a metric, why do we bother with inner products in the first place?","	{\displaystyle d(x,y)\geq 0} {\displaystyle d(x,y)=0\Leftrightarrow x=y}	 {\displaystyle d(x,y)=d(y,x)} {\displaystyle d(x,z)\leq d(x,y)+d(y,z)} ^n ^n","['linear-algebra', 'metric-spaces', 'inner-products']"
45,"Over a commutative unital ring, does $\det A = 0$ imply that $A\mathbf{x} = 0$ has a non-zero solution?","Over a commutative unital ring, does  imply that  has a non-zero solution?",\det A = 0 A\mathbf{x} = 0,"This is a standard theorem in linear algebra over fields that is still true when working over an integral domain . For a commutative unital integral domain $R$, let $A$ be an $n\times n$ matrix with entries in $R$. The system of linear equations $A\mathbf{x}=0$ has a non-zero solution if and only if $\det(A) = 0$. The forward direction of this statement is not true though if we are working over a ring $R$ that is not an integral domain, a counter example being  $$ R = \boldsymbol{Z}_6  \qquad  A = \begin{pmatrix} 3 & 0  \\ 0 & 3 \end{pmatrix}  \qquad  \mathbf{x} = \begin{pmatrix} 2 \\ 2 \end{pmatrix} \,. $$ But what about the other direction? Does $\det(A) = 0$ imply that $A\mathbf{x} = 0$ has a non-zero solution over an arbitrary commutative unital ring? I've played around for a bit, and have yet to find a counterexample.","This is a standard theorem in linear algebra over fields that is still true when working over an integral domain . For a commutative unital integral domain $R$, let $A$ be an $n\times n$ matrix with entries in $R$. The system of linear equations $A\mathbf{x}=0$ has a non-zero solution if and only if $\det(A) = 0$. The forward direction of this statement is not true though if we are working over a ring $R$ that is not an integral domain, a counter example being  $$ R = \boldsymbol{Z}_6  \qquad  A = \begin{pmatrix} 3 & 0  \\ 0 & 3 \end{pmatrix}  \qquad  \mathbf{x} = \begin{pmatrix} 2 \\ 2 \end{pmatrix} \,. $$ But what about the other direction? Does $\det(A) = 0$ imply that $A\mathbf{x} = 0$ has a non-zero solution over an arbitrary commutative unital ring? I've played around for a bit, and have yet to find a counterexample.",,"['linear-algebra', 'abstract-algebra', 'commutative-algebra', 'determinant']"
46,A connection between vector space operations and intuitionistic connectives?,A connection between vector space operations and intuitionistic connectives?,,"As I was working my way through Steven Roman's Advanced Linear Algebra , I came across this in Chapter 11, exercise 1. Let $U,W$ be subspaces of a metric vector space $V$. Show that (a) $U\subseteq W \implies W^\bot\subseteq U^\bot$ (b) $U\subseteq U^{\bot\bot}$ (c) $U^\bot = U^{\bot\bot\bot}$ Additionally, $(U+W)^\bot = U^\bot \cap W^\bot$ and $U^\bot + W^\bot \subseteq (U\cap W)^\bot$. I'm having some trouble with the reverse inclusion (at least it's not as obvious as the other ones were). All of these properties look strikingly similar to the behavior of connectives in intuitionistic logic, e.g. $p\rightarrow q \vdash \neg q\rightarrow \neg p$, $\vdash p \rightarrow \neg\neg p$, $\vdash \neg p \leftrightarrow \neg\neg\neg p$, $\vdash \neg(p \lor q) \leftrightarrow \neg p \land \neg q$, $\vdash \neg p \lor \neg q \rightarrow \neg(p\land q)$. Is there a connection here? If so, what is it?","As I was working my way through Steven Roman's Advanced Linear Algebra , I came across this in Chapter 11, exercise 1. Let $U,W$ be subspaces of a metric vector space $V$. Show that (a) $U\subseteq W \implies W^\bot\subseteq U^\bot$ (b) $U\subseteq U^{\bot\bot}$ (c) $U^\bot = U^{\bot\bot\bot}$ Additionally, $(U+W)^\bot = U^\bot \cap W^\bot$ and $U^\bot + W^\bot \subseteq (U\cap W)^\bot$. I'm having some trouble with the reverse inclusion (at least it's not as obvious as the other ones were). All of these properties look strikingly similar to the behavior of connectives in intuitionistic logic, e.g. $p\rightarrow q \vdash \neg q\rightarrow \neg p$, $\vdash p \rightarrow \neg\neg p$, $\vdash \neg p \leftrightarrow \neg\neg\neg p$, $\vdash \neg(p \lor q) \leftrightarrow \neg p \land \neg q$, $\vdash \neg p \lor \neg q \rightarrow \neg(p\land q)$. Is there a connection here? If so, what is it?",,"['linear-algebra', 'logic']"
47,Prove that the Sylvester equation has a unique solution when $A$ and $-B$ share no eigenvalues,Prove that the Sylvester equation has a unique solution when  and  share no eigenvalues,A -B,"We are given the Sylvester equation $AX+XB=C$ with complex matrices. I am trying to understand the proof that if $A$ and $-B$ share no eigenvalues, then there is a unique solution $X$ for any $C$. The proof is on Wikipedia and reads like this: Suppose that $A$ and $-B$ have no common eigenvalues. Then their characteristic polynomials $f(z)$ and $g(z)$ have highest common factor $1$. Hence there exist complex polynomials $p(z)$ and $q(z)$ such that $p(z)f(z)+q(z)g(z)=1$. By the Cayley–Hamilton theorem, $f(A)=0=g(-B)$; hence $g(A)q(A)=I$. Let $X$ be any solution of $S(X)=0$; so $AX=-XB$ and repeating this one sees that $X=q(A)g(A)X=q(A)Xg(-B)=0$. Hence by the rank plus nullity theorem $S$ is invertible, so for all $C$ there exists a unique solution $X$. Firstly, I don't understand how it concludes that there exist $p(z)$ and $q(z)$ such that $p(z)f(z) + q(z)g(z)=1$. If this follows from the previous statement, I don't see how. Secondly, I don't understand how it concludes that $q(A)g(A)X=q(A)Xg(-B)$. Again, if it follows from a previous statement, it is not clear how. If anyone can explain these steps, or provide a different proof, it would be greatly appreciated.","We are given the Sylvester equation $AX+XB=C$ with complex matrices. I am trying to understand the proof that if $A$ and $-B$ share no eigenvalues, then there is a unique solution $X$ for any $C$. The proof is on Wikipedia and reads like this: Suppose that $A$ and $-B$ have no common eigenvalues. Then their characteristic polynomials $f(z)$ and $g(z)$ have highest common factor $1$. Hence there exist complex polynomials $p(z)$ and $q(z)$ such that $p(z)f(z)+q(z)g(z)=1$. By the Cayley–Hamilton theorem, $f(A)=0=g(-B)$; hence $g(A)q(A)=I$. Let $X$ be any solution of $S(X)=0$; so $AX=-XB$ and repeating this one sees that $X=q(A)g(A)X=q(A)Xg(-B)=0$. Hence by the rank plus nullity theorem $S$ is invertible, so for all $C$ there exists a unique solution $X$. Firstly, I don't understand how it concludes that there exist $p(z)$ and $q(z)$ such that $p(z)f(z) + q(z)g(z)=1$. If this follows from the previous statement, I don't see how. Secondly, I don't understand how it concludes that $q(A)g(A)X=q(A)Xg(-B)$. Again, if it follows from a previous statement, it is not clear how. If anyone can explain these steps, or provide a different proof, it would be greatly appreciated.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'proof-explanation', 'matrix-equations', 'sylvester-equation']"
48,help understanding a paragraph from Linear Algebra Done Right,help understanding a paragraph from Linear Algebra Done Right,,"I am attempting to work my way through the 3rd edition of ""Linear Algebra Done Right"", but there's a paragraph on page 14 that I don't understand.  I have struggled with it myself for a few hours and have come to the conclusion that I need some help.  But first some background. Axler lets $\textbf{F}$ stand for either the set of real or complex numbers.  (He mentions fields but doesn't use them directly.)  He also uses the term ""list"" instead of ""tuple"". The problematic paragraph is preceded by this: If $S$ is a set, then $\textbf{F}^S$ denotes the set of functions from $S$ to $\textbf{F}$. For $f, g \in \textbf{F}^S$, the sum $f + g \in \textbf{F}^S$ is the function defined by   $$(f + g)(x) = f(x) + g(x)$$   for all $x \in S$. For $\lambda \in \textbf{F}$ and $f \in \textbf{F}^S$, the product $\lambda f \in \textbf{F}^S$ is the function defined by   $$(\lambda f)(x) = \lambda f(x)$$   for all $x \in S$. As an example of the notation above, if $S$ is the interval [0,1] and $\textbf{F} = \textbf{R}$, then $\textbf{R}^{[0,1]}$ is the set of real-valued functions on the interval [0,1]. So far, so good.  This is where I get lost: Our previous examples of vector spaces, $\textbf{F}^n$ and $\textbf{F}^\infty$, are special cases of the vector space $\textbf{F}^S$ because a list of length $n$ of numbers in $\textbf{F}$ can be thought of as a function from {1, 2, ..., $n$} to $\textbf{F}$ and a sequence of numbers in $\textbf{F}$ can be thought of as a function from the set of positive integers to $\textbf{F}$.  In other words, we can think of $\textbf{F}^n$ as $\textbf{F}^{\{1,2,...,n\}}$ and we can think of $\textbf{F}^\infty$ as $\textbf{F}^{\{1,2,...\}}$. The general idea seems straightforward to me.  Let $S$ be the set of all tuples in $\textbf{F}^n$ or $\textbf{F}^\infty$, and now $\textbf{F}^S$ is a vector space.  Is it really that simple?  But what is he saying? [My background: I am a computer programmer who took a ""normal"" linear algebra class in college, who has a new-found love for higher mathematics, but not enough time and money to go back to school.]","I am attempting to work my way through the 3rd edition of ""Linear Algebra Done Right"", but there's a paragraph on page 14 that I don't understand.  I have struggled with it myself for a few hours and have come to the conclusion that I need some help.  But first some background. Axler lets $\textbf{F}$ stand for either the set of real or complex numbers.  (He mentions fields but doesn't use them directly.)  He also uses the term ""list"" instead of ""tuple"". The problematic paragraph is preceded by this: If $S$ is a set, then $\textbf{F}^S$ denotes the set of functions from $S$ to $\textbf{F}$. For $f, g \in \textbf{F}^S$, the sum $f + g \in \textbf{F}^S$ is the function defined by   $$(f + g)(x) = f(x) + g(x)$$   for all $x \in S$. For $\lambda \in \textbf{F}$ and $f \in \textbf{F}^S$, the product $\lambda f \in \textbf{F}^S$ is the function defined by   $$(\lambda f)(x) = \lambda f(x)$$   for all $x \in S$. As an example of the notation above, if $S$ is the interval [0,1] and $\textbf{F} = \textbf{R}$, then $\textbf{R}^{[0,1]}$ is the set of real-valued functions on the interval [0,1]. So far, so good.  This is where I get lost: Our previous examples of vector spaces, $\textbf{F}^n$ and $\textbf{F}^\infty$, are special cases of the vector space $\textbf{F}^S$ because a list of length $n$ of numbers in $\textbf{F}$ can be thought of as a function from {1, 2, ..., $n$} to $\textbf{F}$ and a sequence of numbers in $\textbf{F}$ can be thought of as a function from the set of positive integers to $\textbf{F}$.  In other words, we can think of $\textbf{F}^n$ as $\textbf{F}^{\{1,2,...,n\}}$ and we can think of $\textbf{F}^\infty$ as $\textbf{F}^{\{1,2,...\}}$. The general idea seems straightforward to me.  Let $S$ be the set of all tuples in $\textbf{F}^n$ or $\textbf{F}^\infty$, and now $\textbf{F}^S$ is a vector space.  Is it really that simple?  But what is he saying? [My background: I am a computer programmer who took a ""normal"" linear algebra class in college, who has a new-found love for higher mathematics, but not enough time and money to go back to school.]",,['linear-algebra']
49,"What exactly is a ""vector-space structure?"" (Linear Alg)","What exactly is a ""vector-space structure?"" (Linear Alg)",,"I am supplementing my linear algebra book with wikipedia, and I came across an interesting term that isnt mentioned precisely by that name in my textbook. The full sentence is, ""Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure."" This section was discussing linear transformation. My question is, what exactly is a vector-space structure at its most fundamental level and how can I really understand it? I have an okay grasp on the concept of a vector space, however is a structure something that is distinct from a space?","I am supplementing my linear algebra book with wikipedia, and I came across an interesting term that isnt mentioned precisely by that name in my textbook. The full sentence is, ""Similarly as in the theory of other algebraic structures, linear algebra studies mappings between vector spaces that preserve the vector-space structure."" This section was discussing linear transformation. My question is, what exactly is a vector-space structure at its most fundamental level and how can I really understand it? I have an okay grasp on the concept of a vector space, however is a structure something that is distinct from a space?",,"['linear-algebra', 'matrices']"
50,Equation to place points equidistantly on an Archimedian Spiral using arc-length,Equation to place points equidistantly on an Archimedian Spiral using arc-length,,"I am looking for a way to place points equidistantly along an Archimedes spiral according to arch-length (or an approximation) given the following parameters: Max Radius, Fixed distance between the points, Number of points I have been lurking around on this site for a few days and have found a lot of great advice but am struggling with the syntax of some of the proposed responses (not a native coder but I have had small exposure to Python and Matlab). This example 1 seems to be exactly what I am looking for but I am just struggling with the code, it is not clear to me what variables are used or how the program executes. Example 2 and example 3 were also helpful but I am definitely missing something when it comes to solving the equation numerically as the resulting spiral does not have equal spacing. My goal is to use a spreadsheet (MS Excel) to drive a solid modeling program to generate a hole pattern per the parameters above. Cheers!","I am looking for a way to place points equidistantly along an Archimedes spiral according to arch-length (or an approximation) given the following parameters: Max Radius, Fixed distance between the points, Number of points I have been lurking around on this site for a few days and have found a lot of great advice but am struggling with the syntax of some of the proposed responses (not a native coder but I have had small exposure to Python and Matlab). This example 1 seems to be exactly what I am looking for but I am just struggling with the code, it is not clear to me what variables are used or how the program executes. Example 2 and example 3 were also helpful but I am definitely missing something when it comes to solving the equation numerically as the resulting spiral does not have equal spacing. My goal is to use a spreadsheet (MS Excel) to drive a solid modeling program to generate a hole pattern per the parameters above. Cheers!",,"['linear-algebra', 'geometry', 'polar-coordinates']"
51,How to construct a line with a given equal distance from 3 Points in 3 Dimensions?,How to construct a line with a given equal distance from 3 Points in 3 Dimensions?,,"Important: I'm now convinced that 4 points are needes in order to reduce the solutions to a finite number. (Which is necessary because I need ALL solutions) In a computer science context I need to solve a geometrical problem which states: Given three points in three-dimensional space, find a line $L$ (in any form, e.g. specify two points which lie on the line) so that the distance between each of the points and $L$ are equal to a given distance $d$, if possible. By distance between a point $P$ and a line $L$, the euclidean distance between $P$ and the foot of the perpendicular on $L$ that passes through $P$ is meant. In two dimensions (given two points) this is rather simple as it involves only a few trigonometric functions, altough I really struggle with it in three dimensions. The reason surely is that I'm not from a math background and don't know too much about linear algebra (which I believe is involved here). It would be ideal if there is a solution for $N$ dimensions (I think that $N$ Points are needed then), altough I would be very happy if someone could give at least some hints about the three dimensional problem (maybe some sort of heuristic that i didn't thought of could also work). :) EDIT: Clarification The distance between the points and the line is a given constant . Example:  Given three Points and a distance $d=3$, I want to find the line which has a distance of $d$ (in this case $3$) to every given point, if possible (of course there are many cases where such a line does not exist). And I am aware of the fact that this line is not unique (several, or in case of colinearity of the three points, infinitely many lines exist) EDIT: Clarification II It seems that my wording causes much confusion about exactly WHAT properties the line should have. A picture showing the two-dimensional case follows: In this case the Points $P_1$ and $P_2$ are given and the task was to find the line $g$ such that every given Point has the same shortest distance $r_B$ (which is preset) to $g$. (In this context the line was specified by a point $C$ and the angle $\alpha$, altough I'm happy with any kind of parameterization). Now I have given 3 three-dimensional points and want to find a line with the described properties and I do not have any idea how to do this. EDIT III: I should also have mentioned that I should be able to find all solutions (I am 99% convinced that there is only a finite number of solutions for ordinary cases)","Important: I'm now convinced that 4 points are needes in order to reduce the solutions to a finite number. (Which is necessary because I need ALL solutions) In a computer science context I need to solve a geometrical problem which states: Given three points in three-dimensional space, find a line $L$ (in any form, e.g. specify two points which lie on the line) so that the distance between each of the points and $L$ are equal to a given distance $d$, if possible. By distance between a point $P$ and a line $L$, the euclidean distance between $P$ and the foot of the perpendicular on $L$ that passes through $P$ is meant. In two dimensions (given two points) this is rather simple as it involves only a few trigonometric functions, altough I really struggle with it in three dimensions. The reason surely is that I'm not from a math background and don't know too much about linear algebra (which I believe is involved here). It would be ideal if there is a solution for $N$ dimensions (I think that $N$ Points are needed then), altough I would be very happy if someone could give at least some hints about the three dimensional problem (maybe some sort of heuristic that i didn't thought of could also work). :) EDIT: Clarification The distance between the points and the line is a given constant . Example:  Given three Points and a distance $d=3$, I want to find the line which has a distance of $d$ (in this case $3$) to every given point, if possible (of course there are many cases where such a line does not exist). And I am aware of the fact that this line is not unique (several, or in case of colinearity of the three points, infinitely many lines exist) EDIT: Clarification II It seems that my wording causes much confusion about exactly WHAT properties the line should have. A picture showing the two-dimensional case follows: In this case the Points $P_1$ and $P_2$ are given and the task was to find the line $g$ such that every given Point has the same shortest distance $r_B$ (which is preset) to $g$. (In this context the line was specified by a point $C$ and the angle $\alpha$, altough I'm happy with any kind of parameterization). Now I have given 3 three-dimensional points and want to find a line with the described properties and I do not have any idea how to do this. EDIT III: I should also have mentioned that I should be able to find all solutions (I am 99% convinced that there is only a finite number of solutions for ordinary cases)",,"['linear-algebra', 'geometry', 'trigonometry', 'euclidean-geometry']"
52,Row reduction and the characteristic polynomial of a matrix,Row reduction and the characteristic polynomial of a matrix,,Can you row reduce the matrix before computing $\det(\lambda I-A)$? Will this still give an equivalent characteristic polynomial?,Can you row reduce the matrix before computing $\det(\lambda I-A)$? Will this still give an equivalent characteristic polynomial?,,"['linear-algebra', 'matrices']"
53,Proof: Show there is set of $n+1$ points in $\mathbb{R}^n$ such that distance between any two distinct points is $1$?,Proof: Show there is set of  points in  such that distance between any two distinct points is ?,n+1 \mathbb{R}^n 1,"Argh, I hate to ask a question again so soon, especially one I feel like I should know.  Linear algebra is taking its toll, and I am not quite used to the theory side of mathematics. Anyways, I want/need (more of a want) to show that there is a set of $n+1$ points in $\mathbb{R}^n$ such that the distance between any two distinct points is $1$. I am also interested in proving that a set of $n+2$ points does not exist, but I imagine that should not be difficult to do after. Thank you in advance. PS. I really do apologize if this is a duplicate, I spent a lot of time trying to figure this out to no avail.","Argh, I hate to ask a question again so soon, especially one I feel like I should know.  Linear algebra is taking its toll, and I am not quite used to the theory side of mathematics. Anyways, I want/need (more of a want) to show that there is a set of $n+1$ points in $\mathbb{R}^n$ such that the distance between any two distinct points is $1$. I am also interested in proving that a set of $n+2$ points does not exist, but I imagine that should not be difficult to do after. Thank you in advance. PS. I really do apologize if this is a duplicate, I spent a lot of time trying to figure this out to no avail.",,"['linear-algebra', 'proof-writing']"
54,What is the relationship between $(u\times v)\times w$ and $u\times(v\times w)$?,What is the relationship between  and ?,(u\times v)\times w u\times(v\times w),"Given three vectors $u$, $v$, and $w$, $(u\times v)\times w\neq u\times(v\times w)$. This has been a stated fact in my recent class. But what is the ultimate relationship between them? I would presume that one is a scalar multiple of the other?","Given three vectors $u$, $v$, and $w$, $(u\times v)\times w\neq u\times(v\times w)$. This has been a stated fact in my recent class. But what is the ultimate relationship between them? I would presume that one is a scalar multiple of the other?",,"['linear-algebra', 'vector-spaces']"
55,Finding the order of the automorphism group of the abelian group of order 8.,Finding the order of the automorphism group of the abelian group of order 8.,,"So I am given an abelian group of order $8$ such that for all non-identity elements $x^2 = e$ (all elements have order two). So I know the answer is gonna be $168$, but I gotta prove this. So far I have that there are exactly $7$ subgroups of order $4$, and each one has $6$ automorphisms. That gives me a total of $42$ automorphisms. Now there must be a justification for multiplying this number by $4$ to get the $168$. But I can't seem to find a justification for this.","So I am given an abelian group of order $8$ such that for all non-identity elements $x^2 = e$ (all elements have order two). So I know the answer is gonna be $168$, but I gotta prove this. So far I have that there are exactly $7$ subgroups of order $4$, and each one has $6$ automorphisms. That gives me a total of $42$ automorphisms. Now there must be a justification for multiplying this number by $4$ to get the $168$. But I can't seem to find a justification for this.",,"['linear-algebra', 'abstract-algebra', 'group-theory', 'finite-groups', 'abelian-groups']"
56,"Relation between Interior Product, Inner Product, Exterior Product, Outer Product","Relation between Interior Product, Inner Product, Exterior Product, Outer Product",,"Following my previous question Relationship between cross product and outer product where I learnt that the Exterior Product generalises the Cross Product whereas the Inner Product generalises the Dot Product, I was wondering if the simple map that I have drawn below is at all an accurate representation of the links between these different products? Vertical lines denote generalisation-specification, horizontal lines denote ""in opposition to"". I'm just trying to get a quick overview before I dive in. Thanks.","Following my previous question Relationship between cross product and outer product where I learnt that the Exterior Product generalises the Cross Product whereas the Inner Product generalises the Dot Product, I was wondering if the simple map that I have drawn below is at all an accurate representation of the links between these different products? Vertical lines denote generalisation-specification, horizontal lines denote ""in opposition to"". I'm just trying to get a quick overview before I dive in. Thanks.",,"['linear-algebra', 'matrices', 'terminology', 'vector-spaces']"
57,How to invert this symmetric tridiagonal Toeplitz matrix?,How to invert this symmetric tridiagonal Toeplitz matrix?,,What's the best way to invert a simple symmetric tridiagonal Toeplitz matrix of the following form? $$ A = \begin{bmatrix} 1 & a & 0 & \ldots & \ldots & 0 \\\  a & 1 & a & \ddots & & \vdots \\\ 0 & a & 1 & \ddots & \ddots& \vdots \\\ \vdots & \ddots & \ddots & \ddots & a & 0\\\ \vdots & & \ddots & a & 1 & a\\\  0 & \ldots & \ldots & 0 & a & 1 \end{bmatrix}  $$,What's the best way to invert a simple symmetric tridiagonal Toeplitz matrix of the following form?,"
A = \begin{bmatrix} 1 & a & 0 & \ldots & \ldots & 0 \\\ 
a & 1 & a & \ddots & & \vdots \\\
0 & a & 1 & \ddots & \ddots& \vdots \\\
\vdots & \ddots & \ddots & \ddots & a & 0\\\
\vdots & & \ddots & a & 1 & a\\\ 
0 & \ldots & \ldots & 0 & a & 1 \end{bmatrix} 
","['linear-algebra', 'matrices', 'inverse', 'tridiagonal-matrices', 'toeplitz-matrices']"
58,Necessary and sufficient conditions for a matrix to be a valid correlation matrix.,Necessary and sufficient conditions for a matrix to be a valid correlation matrix.,,"It's not too hard to see that any correlation matrix must have certain properties, such as all entries in the range -1 to 1, symmetric, positive semi-definite (excluding pathological cases like singular matrices for the moment). But I'm wondering about the other direction. If I write down some matrix that is positive semi-definite, is symmetric, has 1's along the main diagonal, and all entries are between -1 and 1, does this guarantee that there exists a set of random variables giving rise to that correlation matrix? If the answer is easy to see, a helpful hint about how to define the set of random variables that would give rise to a given matrix would be appreciated.","It's not too hard to see that any correlation matrix must have certain properties, such as all entries in the range -1 to 1, symmetric, positive semi-definite (excluding pathological cases like singular matrices for the moment). But I'm wondering about the other direction. If I write down some matrix that is positive semi-definite, is symmetric, has 1's along the main diagonal, and all entries are between -1 and 1, does this guarantee that there exists a set of random variables giving rise to that correlation matrix? If the answer is easy to see, a helpful hint about how to define the set of random variables that would give rise to a given matrix would be appreciated.",,"['linear-algebra', 'probability', 'matrices', 'correlation']"
59,What is the difference between finding a basis for a complex and a real space?,What is the difference between finding a basis for a complex and a real space?,,"Given that $V = \mathbb{C}^{3}$ how does one determine if a set of vectors $A \subset V$ is a basis? Am I allowed to assume that the the field is now $\mathbb{C}$ and so I am allowed to use all $\lambda \in \mathbb{C}$ as scalars? If so, $E = \{ (1,0,0), (0,1,0), (0,0,1)\}$ is still the usual basis for $\mathbb{C}^{3}$, right? Basically my approach right now would be to immediately rule out any sets of vectors that were not exactly $3$ elements. Then I would put the three vectors in row echelon form and if all rows were non-zero I would conclude that the given set were a basis (exactly as I would if $V= \mathbb{R}^{3}$), is this OK? And then in general, I am now thinking that any basis of $\mathbb{R}^{n}$ would also be a basis for $\mathbb{C}^{n}$, but not the other way around, is that right? Assuming that I am not already way off at this point, is there a big difference between $\mathbb{C}^{n}$ and $\mathbb{R}^{n}$ when it comes to bases?","Given that $V = \mathbb{C}^{3}$ how does one determine if a set of vectors $A \subset V$ is a basis? Am I allowed to assume that the the field is now $\mathbb{C}$ and so I am allowed to use all $\lambda \in \mathbb{C}$ as scalars? If so, $E = \{ (1,0,0), (0,1,0), (0,0,1)\}$ is still the usual basis for $\mathbb{C}^{3}$, right? Basically my approach right now would be to immediately rule out any sets of vectors that were not exactly $3$ elements. Then I would put the three vectors in row echelon form and if all rows were non-zero I would conclude that the given set were a basis (exactly as I would if $V= \mathbb{R}^{3}$), is this OK? And then in general, I am now thinking that any basis of $\mathbb{R}^{n}$ would also be a basis for $\mathbb{C}^{n}$, but not the other way around, is that right? Assuming that I am not already way off at this point, is there a big difference between $\mathbb{C}^{n}$ and $\mathbb{R}^{n}$ when it comes to bases?",,['linear-algebra']
60,"Let $A$ be a binary $n \times n$ matrix, such that $A^2=0$. What is the max num of $1$'s that $A$ could have?","Let  be a binary  matrix, such that . What is the max num of 's that  could have?",A n \times n A^2=0 1 A,"I noticed: Fixing $A_{ij}=1$ would imply $i$ -th column and $j$ -th row are all $0$ 's From there, I constructed a few matrices with small $n$ and hypothesized $f(n) = \lfloor{n/2}\rfloor \cdot \lceil{n/2}\rceil$ Interpreting $A$ as an adjacency matrix: $A^2=0$ implies you can't get from $i$ to $j$ in 2 hops. This reminds me of the max number of edges in a bipartite graph of n nodes; if you consider the edges as directed and the two sets as source/sink sets. I wonder if this relates to max flow. Searching ""nilpotent binary matrix"" got me to Nilpotent binary matrices over finite fields . I realized directed edges are not necessary in bipartite graph interpretation -- the problem could be considered as Maximum number of edges in a bipartite graph I feel there are multiple proof approaches. There is something very classic and familiar going on which I can't put my finger to, and I wonder what non-graph approaches would be especially.","I noticed: Fixing would imply -th column and -th row are all 's From there, I constructed a few matrices with small and hypothesized Interpreting as an adjacency matrix: implies you can't get from to in 2 hops. This reminds me of the max number of edges in a bipartite graph of n nodes; if you consider the edges as directed and the two sets as source/sink sets. I wonder if this relates to max flow. Searching ""nilpotent binary matrix"" got me to Nilpotent binary matrices over finite fields . I realized directed edges are not necessary in bipartite graph interpretation -- the problem could be considered as Maximum number of edges in a bipartite graph I feel there are multiple proof approaches. There is something very classic and familiar going on which I can't put my finger to, and I wonder what non-graph approaches would be especially.",A_{ij}=1 i j 0 n f(n) = \lfloor{n/2}\rfloor \cdot \lceil{n/2}\rceil A A^2=0 i j,"['linear-algebra', 'matrices', 'graph-theory', 'extremal-graph-theory']"
61,Intuitive explanation of the rank-nullity theorem [duplicate],Intuitive explanation of the rank-nullity theorem [duplicate],,"This question already has answers here : Intuitive explanation of why $\dim\operatorname{Im} T + \dim\operatorname{Ker} T = \dim V$ (4 answers) Closed 5 years ago . I understand that if you have a linear transformation from $U$ to $V$ with, say, $\operatorname{dim} U = 3$ , $\operatorname{rank} T = 2$ , then the set of points that map onto the $0$ vector will lie along a straight line, and therefore $\operatorname{nullity}T = 1$ . Can anyone offer an intuitive explanation of why this is always true?","This question already has answers here : Intuitive explanation of why $\dim\operatorname{Im} T + \dim\operatorname{Ker} T = \dim V$ (4 answers) Closed 5 years ago . I understand that if you have a linear transformation from to with, say, , , then the set of points that map onto the vector will lie along a straight line, and therefore . Can anyone offer an intuitive explanation of why this is always true?",U V \operatorname{dim} U = 3 \operatorname{rank} T = 2 0 \operatorname{nullity}T = 1,"['linear-algebra', 'matrix-rank']"
62,Can the conjugate $\phi^{-1} \circ A \circ \phi$ be linear for nonlinear $\phi$?,Can the conjugate  be linear for nonlinear ?,\phi^{-1} \circ A \circ \phi \phi,"More specifically, let $f：ℂ^n\to ℂ^n, f(z)=Az$ be linear and $ϕ：ℂ^n → ℂ^n$ entire and bijective, i.e. $ϕ∈\text{Aut}(ℂ^n)$ . Is it possible that $ϕ^{-1}∘f∘ϕ$ is linear again when $ϕ$ is non-linear? For example if $ϕ(z) = Lz+c$ is affine, then $$ ϕ^{-1}∘f∘ϕ (z) = L^{-1}ALz + L^{-1}(A-I)c $$ is linear if and only if $c \in \ker(A-I)$ . Are there more sophisticated examples where $\phi$ is truly non-linear, in the sense of having non-constant derivative? Question: Does there exist a triple $(A, B, ϕ)$ , such that $A≠B$ are complex $n×n$ matrices. $ϕ∈\text{Aut}(ℂ^n)$ with $ϕ'$ non-constant $ ϕ^{-1} ∘ A ∘ ϕ=B $ $A∘ϕ$ is nonlinear (this avoids certain trivial examples) In particular, is there an example where $A$ and $B$ are non-similar? EDIT 2022: Non-similarity is impossible as per Dap's comment ✔ I added the 4th condition to rule out certain trivial examples (see comments in Qiaochu Yuan's answer) Dap provided an example in the comments with $A=B$ Note that if $(A, B, ϕ)$ is a valid triple, with $B=S^{-1}AS$ , then we can actually non-linearly conjugate $A$ to any matrix $C=T^{-1}AT$ similar to $A$ via $ψ = ϕ∘S^{-1}∘T$ . So we immediately get an example with $A≠B$ out of Dap's contraction. An interesting follow-up question would be: Can we construct examples for generic $A$ ? So far we only have an example where $A$ is similar to a diagonal matrix and all eigenvalues have magnitude $1$ .","More specifically, let be linear and entire and bijective, i.e. . Is it possible that is linear again when is non-linear? For example if is affine, then is linear if and only if . Are there more sophisticated examples where is truly non-linear, in the sense of having non-constant derivative? Question: Does there exist a triple , such that are complex matrices. with non-constant is nonlinear (this avoids certain trivial examples) In particular, is there an example where and are non-similar? EDIT 2022: Non-similarity is impossible as per Dap's comment ✔ I added the 4th condition to rule out certain trivial examples (see comments in Qiaochu Yuan's answer) Dap provided an example in the comments with Note that if is a valid triple, with , then we can actually non-linearly conjugate to any matrix similar to via . So we immediately get an example with out of Dap's contraction. An interesting follow-up question would be: Can we construct examples for generic ? So far we only have an example where is similar to a diagonal matrix and all eigenvalues have magnitude .","f：ℂ^n\to ℂ^n, f(z)=Az ϕ：ℂ^n → ℂ^n ϕ∈\text{Aut}(ℂ^n) ϕ^{-1}∘f∘ϕ ϕ ϕ(z) = Lz+c  ϕ^{-1}∘f∘ϕ (z) = L^{-1}ALz + L^{-1}(A-I)c  c \in \ker(A-I) \phi (A, B, ϕ) A≠B n×n ϕ∈\text{Aut}(ℂ^n) ϕ'  ϕ^{-1} ∘ A ∘ ϕ=B  A∘ϕ A B A=B (A, B, ϕ) B=S^{-1}AS A C=T^{-1}AT A ψ = ϕ∘S^{-1}∘T A≠B A A 1","['linear-algebra', 'complex-analysis']"
63,Do these higher-dimensional analogues of Möbius transformations have a name?,Do these higher-dimensional analogues of Möbius transformations have a name?,,"Do maps of the form $$ x \in \mathbb{R}^n \mapsto \frac{Ax+b}{c^Tx+d} \in \mathbb{R}^n, $$ where $A \in \mathbb{R}^{n\times n}, b, c \in \mathbb{R}^n, d\in \mathbb{R}$ have a name? Have they been studied anywhere? It looks somehow familiar to Möbius-transformation but it is different as $A, b, c, d$ are not complex numbers. It is easy to see that the above maps form a group. I am interested in this because of an application in optics where I found that for a thin lense the map which maps image to object points is of the above form. I am especially interested in the $n=2$ and $n=3$ cases.","Do maps of the form $$ x \in \mathbb{R}^n \mapsto \frac{Ax+b}{c^Tx+d} \in \mathbb{R}^n, $$ where $A \in \mathbb{R}^{n\times n}, b, c \in \mathbb{R}^n, d\in \mathbb{R}$ have a name? Have they been studied anywhere? It looks somehow familiar to Möbius-transformation but it is different as $A, b, c, d$ are not complex numbers. It is easy to see that the above maps form a group. I am interested in this because of an application in optics where I found that for a thin lense the map which maps image to object points is of the above form. I am especially interested in the $n=2$ and $n=3$ cases.",,['reference-request']
64,Why is the dot product of two vectors defined the way in which it is? [duplicate],Why is the dot product of two vectors defined the way in which it is? [duplicate],,"This question already has answers here : why don't we define vector multiplication component-wise? (6 answers) Closed 6 years ago . Most people who’ve sat through any lesson involving vectors will know about the vector dot product If $\displaystyle \mathbf p=\left[{v_1\atop v_2}\right]$ and $\displaystyle \mathbf q=\left[{w_1\atop w_2}\right]$, then $$\mathbf p \cdot \mathbf q=v_1w_1+v_2w_2$$ Obviously this is the special case where the vectors lie in the two dimensional plane, and this formula does extend to $n$ dimensions, my typesetting abilities just aren’t advanced enough to know how to represent such vectors. In any case, I’m just wondering, so that I don’t go through the rest of any future linear algebra courses blind, why do we define the dot product of vectors in this way? Instead of, for example, multiplying each component of one vector by the corresponding component of a second vector to produce a third vector, whose components are these products? Any input is appreciated, thank you.","This question already has answers here : why don't we define vector multiplication component-wise? (6 answers) Closed 6 years ago . Most people who’ve sat through any lesson involving vectors will know about the vector dot product If $\displaystyle \mathbf p=\left[{v_1\atop v_2}\right]$ and $\displaystyle \mathbf q=\left[{w_1\atop w_2}\right]$, then $$\mathbf p \cdot \mathbf q=v_1w_1+v_2w_2$$ Obviously this is the special case where the vectors lie in the two dimensional plane, and this formula does extend to $n$ dimensions, my typesetting abilities just aren’t advanced enough to know how to represent such vectors. In any case, I’m just wondering, so that I don’t go through the rest of any future linear algebra courses blind, why do we define the dot product of vectors in this way? Instead of, for example, multiplying each component of one vector by the corresponding component of a second vector to produce a third vector, whose components are these products? Any input is appreciated, thank you.",,"['linear-algebra', 'soft-question', 'vectors']"
65,$V$ finite-dimensional vector space and isomorphic to $\mathbb{R}^n$?,finite-dimensional vector space and isomorphic to ?,V \mathbb{R}^n,"If $V$ is a finite-dimensional vector space, does it mean that $V$ is also isomorphic to $\mathbb{R}^n$ for some $n$? I am having a hard time trying to picture this. I was wondering if someone could explain this to me.","If $V$ is a finite-dimensional vector space, does it mean that $V$ is also isomorphic to $\mathbb{R}^n$ for some $n$? I am having a hard time trying to picture this. I was wondering if someone could explain this to me.",,"['linear-algebra', 'vector-spaces', 'vector-space-isomorphism']"
66,What is the geometric interpretation of the rowspace?,What is the geometric interpretation of the rowspace?,,"I am missing something essential in my understanding of matrices as linear mappings (or matrices in general). To explain, I think I have a decent intuition for columnspace of a matrix. Basically, the columnspace can be thought of as the set of points other than the origin which can be mapped by the matrix of a linear mapping (I like to think of matrices in general as linear mappings). What I have trouble with is the significance and geometric interpretation of the rowspace of a matrix. By the definition, I can see that the rowspace and columnspace are analagous; the rowspace is merely the columnspace of the transpose of the matrix of the linear mapping. However, I cannot explain this anymore intuitively than above. To quote Albert Einstein, ""if you don't understand it simply, you don't understand it well enough"". I am looking for a simple explanation (or geometric interpretation that will contribute to my imagination of linear mappings). How would you explain this concept? To give you some context, I ran into problems when my course started to discuss the following: For an $n \times n$ matrix $P$, the following are equivalent: (1) The columns of $P$ form an orthonormal basis for $\mathbb{R}^n$ (2) $P^T = P^{-1}$ (3) The rows of $P$ form an orthonormal basis for $\mathbb{R}^n$. I understand the algebra perfectly, but I seek a level of intuition so that I would not need to fiddle around with algebra to discover these facts. How would you explain the above concepts intuitively or geometrically, without the need for algebra to make a case? Many thanks in advance.","I am missing something essential in my understanding of matrices as linear mappings (or matrices in general). To explain, I think I have a decent intuition for columnspace of a matrix. Basically, the columnspace can be thought of as the set of points other than the origin which can be mapped by the matrix of a linear mapping (I like to think of matrices in general as linear mappings). What I have trouble with is the significance and geometric interpretation of the rowspace of a matrix. By the definition, I can see that the rowspace and columnspace are analagous; the rowspace is merely the columnspace of the transpose of the matrix of the linear mapping. However, I cannot explain this anymore intuitively than above. To quote Albert Einstein, ""if you don't understand it simply, you don't understand it well enough"". I am looking for a simple explanation (or geometric interpretation that will contribute to my imagination of linear mappings). How would you explain this concept? To give you some context, I ran into problems when my course started to discuss the following: For an $n \times n$ matrix $P$, the following are equivalent: (1) The columns of $P$ form an orthonormal basis for $\mathbb{R}^n$ (2) $P^T = P^{-1}$ (3) The rows of $P$ form an orthonormal basis for $\mathbb{R}^n$. I understand the algebra perfectly, but I seek a level of intuition so that I would not need to fiddle around with algebra to discover these facts. How would you explain the above concepts intuitively or geometrically, without the need for algebra to make a case? Many thanks in advance.",,"['linear-algebra', 'vector-spaces', 'linear-transformations', 'orthonormal']"
67,Find a matrix with given row and column sums,Find a matrix with given row and column sums,,"Please forgive my intrusion. I've been working for days on this problem and it's vexing me. It doesn't seem to have a solution and I could really use some help. I have a ""math square"" (not a magic square) that looks like this when filled in $$ \begin{array}{ccccc|c} 9 & 4 & 8 & 4 & 7 & 32 \\ 7 & 9 & 15 & 7 & 5 & 43 \\ 3 & 2 & 9 & 10 & 9 & 33 \\ 5 & 3 & 5 & 6 & 4 & 23 \\ \hline 24 & 18 & 37 & 27 & 25 & 131 \\ \end{array} $$ However, the puzzle when empty looks like this. $$ \begin{array}{ccccc|c} x & x & x & x & x & 32 \\ x & x & x & x & x & 43 \\ x & x & x & x & x & 33 \\ x & x & x & x & x & 23 \\ \hline 24 & 18 & 37 & 27 & 25 & 131 \\ \end{array} $$ I need an equation of some sort to fill in the unknowns ($X$'s) and recreate the missing values. There appears to be some symmetry with the puzzle as the columns and row all add up to be the same which in this case is $131$. If you separate them by every other row to perhaps break it down to make it easier to solve you get. You could also do this with the columns but for simplicity I haven't written it here. $$ \begin{array}{ccccc|c} 9 & 4 & 8 & 4 & 7 & 32 \\ 3 & 2 & 9 & 10 & 9 & 33 \\ \hline 12 & 6 & 17 & 14 & 16 & 65 \\ \end{array} \dots \begin{array}{ccccc|c} 7 & 9 & 15 & 7 & 5 & 43 \\ 5 & 3 & 5 & 6 & 4 & 23\\ \hline 12 & 12 & 20 & 13 & 9 & 86   \\ \end{array} $$ using these givens in the puzzle is allowed, but not the $X$'s If it is indeed unsolvable I would like to know but if it can what missing component can be add to achieve that goal? Your help is very much appreciated and thank you! Regards, Tony p.s. sorry about the formatting.","Please forgive my intrusion. I've been working for days on this problem and it's vexing me. It doesn't seem to have a solution and I could really use some help. I have a ""math square"" (not a magic square) that looks like this when filled in $$ \begin{array}{ccccc|c} 9 & 4 & 8 & 4 & 7 & 32 \\ 7 & 9 & 15 & 7 & 5 & 43 \\ 3 & 2 & 9 & 10 & 9 & 33 \\ 5 & 3 & 5 & 6 & 4 & 23 \\ \hline 24 & 18 & 37 & 27 & 25 & 131 \\ \end{array} $$ However, the puzzle when empty looks like this. $$ \begin{array}{ccccc|c} x & x & x & x & x & 32 \\ x & x & x & x & x & 43 \\ x & x & x & x & x & 33 \\ x & x & x & x & x & 23 \\ \hline 24 & 18 & 37 & 27 & 25 & 131 \\ \end{array} $$ I need an equation of some sort to fill in the unknowns ($X$'s) and recreate the missing values. There appears to be some symmetry with the puzzle as the columns and row all add up to be the same which in this case is $131$. If you separate them by every other row to perhaps break it down to make it easier to solve you get. You could also do this with the columns but for simplicity I haven't written it here. $$ \begin{array}{ccccc|c} 9 & 4 & 8 & 4 & 7 & 32 \\ 3 & 2 & 9 & 10 & 9 & 33 \\ \hline 12 & 6 & 17 & 14 & 16 & 65 \\ \end{array} \dots \begin{array}{ccccc|c} 7 & 9 & 15 & 7 & 5 & 43 \\ 5 & 3 & 5 & 6 & 4 & 23\\ \hline 12 & 12 & 20 & 13 & 9 & 86   \\ \end{array} $$ using these givens in the puzzle is allowed, but not the $X$'s If it is indeed unsolvable I would like to know but if it can what missing component can be add to achieve that goal? Your help is very much appreciated and thank you! Regards, Tony p.s. sorry about the formatting.",,"['linear-algebra', 'linear-programming']"
68,On the determinant of a tridiagonal Toeplitz matrix,On the determinant of a tridiagonal Toeplitz matrix,,"I'm a bit confused with this determinant. We have the determinant $$\Delta_n=\left\vert\begin{matrix} 5&3&0&\cdots&\cdots&0\\ 2&5&3&\ddots& &\vdots\\ 0&2&5&\ddots&\ddots&\vdots\\ \vdots&\ddots&\ddots&\ddots&\ddots&0\\ \vdots& &\ddots&\ddots&\ddots&3\\ 0&\cdots&\cdots&0&2&5\end{matrix} \right\vert$$ I compute $\Delta_2=19$ , $\Delta_3=65$ . Then I would like to find a relation for $n\geq 4$ which links $\Delta_n, \Delta_{n-1}$ and $\Delta_{n-2}$ and thus find an expression of $\Delta_n$ . How could we do that for $n\geq 4$ ?","I'm a bit confused with this determinant. We have the determinant I compute , . Then I would like to find a relation for which links and and thus find an expression of . How could we do that for ?","\Delta_n=\left\vert\begin{matrix}
5&3&0&\cdots&\cdots&0\\
2&5&3&\ddots& &\vdots\\
0&2&5&\ddots&\ddots&\vdots\\
\vdots&\ddots&\ddots&\ddots&\ddots&0\\
\vdots& &\ddots&\ddots&\ddots&3\\
0&\cdots&\cdots&0&2&5\end{matrix}
\right\vert \Delta_2=19 \Delta_3=65 n\geq 4 \Delta_n, \Delta_{n-1} \Delta_{n-2} \Delta_n n\geq 4","['linear-algebra', 'matrices', 'determinant', 'tridiagonal-matrices', 'toeplitz-matrices']"
69,Minimal polynomials and cyclic subspaces,Minimal polynomials and cyclic subspaces,,"I'm trying to make my way through two problems in Curtis's Linear Algebra , chapter 25.  One of the two problems is this one, #5: Prove that $V$ is cyclic relative to a linear transformation $T \in \mathcal{L}(V)$ if and only if the minimal polynomial of $T$ is equal to the characteristic polynomial. Part of the issue is that I'm not really sure what it's asking. The statement that $V$ is cyclic seems to suggest that the entire vector space (say it has dimension $n$) is generated by some vector $v \in V$, along with $Tv, T^2v, \ldots T^{n-1}v$; Curtis's definition of a cyclic subspace (as opposed to a space, though I can't see why there should be a difference) seems to reinforce this: A $T$-invariant subspace $V_1$ of $V$ is called cyclic relative to $T$ if $V_1 \neq 0$ and there exists a vector $v_1 \in V_1$ such that $V_1$ is generated by $\{ v_1, Tv_1, T^2v_1, \ldots T^kv_1 \}$ for some $k$. However, I've seen several examples that seem to directly contradict this interpretation. For example, we recently had a take-home midterm that required we find elementary divisors, rational canonical form, etc., of a $6\times 6$ matrix; a classmate who received a perfect score on her test found that her matrix's characteristic polynomial was equal to its minimal polynomial — it was $(x+8)^2 (x+14)^2 (x^2+61x+80)$ — and yet the vector space could be written as $\left< v_6 \right> \oplus \left< v_4 \right> \oplus \left< -v_2+v_3+v_6 \right>$ — i.e., as the direct sum of three cyclic subspaces, not as one cyclic subspace. So that suggests that ""$V$ is cyclic"" might mean the direct sum of cyclic subspaces. However, while working on the exercise that follows (#6), I came across this: $$S= \left(\begin{array}{ccc} -1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 2 \end{array}\right)$$ Here, the characteristic polynomial is not equal to the minimal polynomial, which is $m(x) = (x-1)(x+1)(x+2)$. The eigenspace associated with $\lambda = 1$ has dimension $2$. And so it seems as though, if $V$ has basis $\{ v_1, v_2, v_3, v_4 \}$, we can write $V = \left< v_1 \right> \oplus \left< v_2 \right> \oplus \left< v_3 \right> \oplus \left< v_4 \right> $. This would seem to contradict my second interpretation of the original question. Obviously I'm missing something. Could someone fill me in on what it might be? (I've tried to be thorough, but I'm also trying not to make this ridiculously long; if I've left out necessary information, I'll be happy to append it.)","I'm trying to make my way through two problems in Curtis's Linear Algebra , chapter 25.  One of the two problems is this one, #5: Prove that $V$ is cyclic relative to a linear transformation $T \in \mathcal{L}(V)$ if and only if the minimal polynomial of $T$ is equal to the characteristic polynomial. Part of the issue is that I'm not really sure what it's asking. The statement that $V$ is cyclic seems to suggest that the entire vector space (say it has dimension $n$) is generated by some vector $v \in V$, along with $Tv, T^2v, \ldots T^{n-1}v$; Curtis's definition of a cyclic subspace (as opposed to a space, though I can't see why there should be a difference) seems to reinforce this: A $T$-invariant subspace $V_1$ of $V$ is called cyclic relative to $T$ if $V_1 \neq 0$ and there exists a vector $v_1 \in V_1$ such that $V_1$ is generated by $\{ v_1, Tv_1, T^2v_1, \ldots T^kv_1 \}$ for some $k$. However, I've seen several examples that seem to directly contradict this interpretation. For example, we recently had a take-home midterm that required we find elementary divisors, rational canonical form, etc., of a $6\times 6$ matrix; a classmate who received a perfect score on her test found that her matrix's characteristic polynomial was equal to its minimal polynomial — it was $(x+8)^2 (x+14)^2 (x^2+61x+80)$ — and yet the vector space could be written as $\left< v_6 \right> \oplus \left< v_4 \right> \oplus \left< -v_2+v_3+v_6 \right>$ — i.e., as the direct sum of three cyclic subspaces, not as one cyclic subspace. So that suggests that ""$V$ is cyclic"" might mean the direct sum of cyclic subspaces. However, while working on the exercise that follows (#6), I came across this: $$S= \left(\begin{array}{ccc} -1 & 0 & 0 & 0\\ 0 & 1 & 0 & 0\\ 0 & 0 & 1 & 0\\ 0 & 0 & 0 & 2 \end{array}\right)$$ Here, the characteristic polynomial is not equal to the minimal polynomial, which is $m(x) = (x-1)(x+1)(x+2)$. The eigenspace associated with $\lambda = 1$ has dimension $2$. And so it seems as though, if $V$ has basis $\{ v_1, v_2, v_3, v_4 \}$, we can write $V = \left< v_1 \right> \oplus \left< v_2 \right> \oplus \left< v_3 \right> \oplus \left< v_4 \right> $. This would seem to contradict my second interpretation of the original question. Obviously I'm missing something. Could someone fill me in on what it might be? (I've tried to be thorough, but I'm also trying not to make this ridiculously long; if I've left out necessary information, I'll be happy to append it.)",,['linear-algebra']
70,Must a normed vector space be over $\mathbb{R}$ or $\mathbb{C}$?,Must a normed vector space be over  or ?,\mathbb{R} \mathbb{C},"If it must be, why is this so? In the maths courses I have taken normed vector spaces always have been over $\mathbb{R}$ or $\mathbb{C}$, but I don't see that this has to be so. I am asking because I was proving that if a normed vector space X has a Schauder basis then it is separable, and I had a proof along the lines of this question How to prove that if a normed space has Schauder basis, then it is separable? What about the converse? The problem I had is this proof requires that we have a sequence of rationals converging to the scalars of the vector space. But if the vector space is not $\mathbb{R}$ or $\mathbb{C}$ then why should this work? Someone has asked this in a comment to the second answer of the question above, but I do not quite understand the response given: ""The fiel has to be restricted, notably because of the definition of a normed space: an absolute value is needed"" I can see that we do use absolute values on the scalars of the vector space but I'm not then sure why this restricts them to being in $\mathbb{R}$ or $\mathbb{C}$, other than I don't know how to define an absolute value on something other than a real or complex number (but wikipedia seems to suggest it can be done).","If it must be, why is this so? In the maths courses I have taken normed vector spaces always have been over $\mathbb{R}$ or $\mathbb{C}$, but I don't see that this has to be so. I am asking because I was proving that if a normed vector space X has a Schauder basis then it is separable, and I had a proof along the lines of this question How to prove that if a normed space has Schauder basis, then it is separable? What about the converse? The problem I had is this proof requires that we have a sequence of rationals converging to the scalars of the vector space. But if the vector space is not $\mathbb{R}$ or $\mathbb{C}$ then why should this work? Someone has asked this in a comment to the second answer of the question above, but I do not quite understand the response given: ""The fiel has to be restricted, notably because of the definition of a normed space: an absolute value is needed"" I can see that we do use absolute values on the scalars of the vector space but I'm not then sure why this restricts them to being in $\mathbb{R}$ or $\mathbb{C}$, other than I don't know how to define an absolute value on something other than a real or complex number (but wikipedia seems to suggest it can be done).",,"['linear-algebra', 'vector-spaces', 'normed-spaces']"
71,Matrix-version of $\frac1a+\frac1b=\frac{a+b}{ab}$?,Matrix-version of ?,\frac1a+\frac1b=\frac{a+b}{ab},In my Linear Algebra Textbook one exercise is to find the matrix analogue of $$ \frac1a+\frac1b=\frac{a+b}{ab} $$ my immediate response was $$ A^{-1}+B^{-1}=A^{-1}(A+B)B^{-1} $$ is that a reasonable answer or do someone have better suggestions? NOTE: $A$ and $B$ are assumed invertible $n\times n$ matrices.,In my Linear Algebra Textbook one exercise is to find the matrix analogue of $$ \frac1a+\frac1b=\frac{a+b}{ab} $$ my immediate response was $$ A^{-1}+B^{-1}=A^{-1}(A+B)B^{-1} $$ is that a reasonable answer or do someone have better suggestions? NOTE: $A$ and $B$ are assumed invertible $n\times n$ matrices.,,['linear-algebra']
72,Prove that every unitary matrix $U$ is unitarily diagonalizable,Prove that every unitary matrix  is unitarily diagonalizable,U,"I just can't show that a unitary matrix $U$ is unitarily diagonizable. I know I need to show that $U$ is unitarily similar to a diagonal matrix, and this result is presumably a consequence of the spectral theorem. EDIT: I was reading this wrong, and I am supposed to be proving this result without the use of the spectral theorem. I have written a proof using the notion of block matrices which I believe is correct. Can anyone help me prove this result without the spectral theorem, using inner products? I suppose the question reduces to: Prove that a normal matrix is unitarily diagonalizable (using inner products).","I just can't show that a unitary matrix is unitarily diagonizable. I know I need to show that is unitarily similar to a diagonal matrix, and this result is presumably a consequence of the spectral theorem. EDIT: I was reading this wrong, and I am supposed to be proving this result without the use of the spectral theorem. I have written a proof using the notion of block matrices which I believe is correct. Can anyone help me prove this result without the spectral theorem, using inner products? I suppose the question reduces to: Prove that a normal matrix is unitarily diagonalizable (using inner products).",U U,"['linear-algebra', 'matrices', 'diagonalization', 'unitary-matrices']"
73,Singular Matrix Problem,Singular Matrix Problem,,"I am working to implement a Least Squares Estimate using Matrices.  However, I seem to produce a Singular Matrix which means I cannot solve the equation.  I'm struggling to understand why the matrix is singular (I know about determinants, etc) - is there something obvious that I am missing? The system I am working with is as follows: $$ X = \begin{pmatrix}\frac{x_1^2}{y_1^2} & \frac{x_1y_1}{y_1^2} & \frac{x_1}{y_1^2} & \frac{y_1}{y_1^2} & \frac{1}{y_1^2} \\ \frac{x_2^2}{y_2^2} & \frac{x_2y_2}{y_2^2} & \frac{x_2}{y_2^2} & \frac{y_2}{y_2^2} & \frac{1}{y_2^2} \\ \vdots \\ \frac{x_k^2}{y_k^2} & \frac{x_ky_k}{y_k^2} & \frac{x_k}{y_k^2} & \frac{y_k}{y_k^2} & \frac{1}{y_k^2} \end{pmatrix} \\ P =  \begin{pmatrix} A \\ B \\ C \\ D \\ E  \end{pmatrix}  $$ $ P $ is a vector of unknowns, I am trying to find estimates for $ P $ with the following equation: $$ P_{est} = \left(X^TX\right)^{-1}X $$ Now, the Matrix $ \left(X^TX\right)^{-1} $ is always singular for my data, I have even tried putting in random data which also gives a singular Matrix.  With this in mind I suspect there is a property of the Matrix definition itself which is causing the issue but I can't put my finger on it. Do any of you clever people know why this is happening?  I can and will provide data once I return to work. Thankyou. Update - I have scaled the data and I still get a Singular Matrix - what is causing this? Given the following data: $$ \begin{matrix} X & Y \\ 13.7284650 & 12.2143600 \\ 13.7284650 & 12.2143600 \\ 12.2198090 & 12.2098100 \\ 12.2077710 & 13.7148230 \\ \end{matrix} $$ Will produce the following Matrix: $$ X = \begin{pmatrix} 1.263 & 1.124 & 0.092 & 0.082 & 6.703E-3 \\ 1.002 & 1.001 & 0.082 & 0.082 & 6.708E-3 \\ 0.792 & 0.89 & 0.065 & 0.073 & 5.316E-3 \\ 0.723 & 0.85 & 0.059 & 0.07 & 4.863E-3 \\ \end{pmatrix} $$ Transposing gives: $$ X^T = \begin{pmatrix} 1.263 & 1.002 & 0.792 & 0.723 \\ 1.124 & 1.001 & 0.89 & 0.85 \\ 0.092 & 0.082 & 0.065 & 0.059 \\ 0.082 & 0.082 & 0.073 & 0.07 \\ 6.703E-3 & 6.708E-3 & 5.316E-3 & 4.863E-3 \\ \end{pmatrix} $$ Therefore $ X^TX $ gives: $$ X^TX = \begin{pmatrix} 3.7499 & 3.7425 & 0.2927 & 0.2937 & 0.0229 \\ 3.7425 & 3.7804 & 0.2937 & 0.2982 & 0.0231 \\ 0.2927 & 0.2937 & 0.0229 & 0.0231 & 1.8001E-3 \\ 0.2937 & 0.2982 & 0.0231 & 0.0236 & 1.8249E-3 \\ 0.0229 & 0.0231 & 1.8001E-3 & 1.8249E-3 & 1.4184E-4 \\ \end{pmatrix} $$ Which has has determinant 0, and therefore cannot be inverted. Why is this?  I suspect it's something to do with diagonality?  Is there a way around this to solve the equation? Any help/advice appreciated, thankyou.","I am working to implement a Least Squares Estimate using Matrices.  However, I seem to produce a Singular Matrix which means I cannot solve the equation.  I'm struggling to understand why the matrix is singular (I know about determinants, etc) - is there something obvious that I am missing? The system I am working with is as follows: $$ X = \begin{pmatrix}\frac{x_1^2}{y_1^2} & \frac{x_1y_1}{y_1^2} & \frac{x_1}{y_1^2} & \frac{y_1}{y_1^2} & \frac{1}{y_1^2} \\ \frac{x_2^2}{y_2^2} & \frac{x_2y_2}{y_2^2} & \frac{x_2}{y_2^2} & \frac{y_2}{y_2^2} & \frac{1}{y_2^2} \\ \vdots \\ \frac{x_k^2}{y_k^2} & \frac{x_ky_k}{y_k^2} & \frac{x_k}{y_k^2} & \frac{y_k}{y_k^2} & \frac{1}{y_k^2} \end{pmatrix} \\ P =  \begin{pmatrix} A \\ B \\ C \\ D \\ E  \end{pmatrix}  $$ $ P $ is a vector of unknowns, I am trying to find estimates for $ P $ with the following equation: $$ P_{est} = \left(X^TX\right)^{-1}X $$ Now, the Matrix $ \left(X^TX\right)^{-1} $ is always singular for my data, I have even tried putting in random data which also gives a singular Matrix.  With this in mind I suspect there is a property of the Matrix definition itself which is causing the issue but I can't put my finger on it. Do any of you clever people know why this is happening?  I can and will provide data once I return to work. Thankyou. Update - I have scaled the data and I still get a Singular Matrix - what is causing this? Given the following data: $$ \begin{matrix} X & Y \\ 13.7284650 & 12.2143600 \\ 13.7284650 & 12.2143600 \\ 12.2198090 & 12.2098100 \\ 12.2077710 & 13.7148230 \\ \end{matrix} $$ Will produce the following Matrix: $$ X = \begin{pmatrix} 1.263 & 1.124 & 0.092 & 0.082 & 6.703E-3 \\ 1.002 & 1.001 & 0.082 & 0.082 & 6.708E-3 \\ 0.792 & 0.89 & 0.065 & 0.073 & 5.316E-3 \\ 0.723 & 0.85 & 0.059 & 0.07 & 4.863E-3 \\ \end{pmatrix} $$ Transposing gives: $$ X^T = \begin{pmatrix} 1.263 & 1.002 & 0.792 & 0.723 \\ 1.124 & 1.001 & 0.89 & 0.85 \\ 0.092 & 0.082 & 0.065 & 0.059 \\ 0.082 & 0.082 & 0.073 & 0.07 \\ 6.703E-3 & 6.708E-3 & 5.316E-3 & 4.863E-3 \\ \end{pmatrix} $$ Therefore $ X^TX $ gives: $$ X^TX = \begin{pmatrix} 3.7499 & 3.7425 & 0.2927 & 0.2937 & 0.0229 \\ 3.7425 & 3.7804 & 0.2937 & 0.2982 & 0.0231 \\ 0.2927 & 0.2937 & 0.0229 & 0.0231 & 1.8001E-3 \\ 0.2937 & 0.2982 & 0.0231 & 0.0236 & 1.8249E-3 \\ 0.0229 & 0.0231 & 1.8001E-3 & 1.8249E-3 & 1.4184E-4 \\ \end{pmatrix} $$ Which has has determinant 0, and therefore cannot be inverted. Why is this?  I suspect it's something to do with diagonality?  Is there a way around this to solve the equation? Any help/advice appreciated, thankyou.",,"['linear-algebra', 'matrices']"
74,Orthogonal projection on the Hilbert space .,Orthogonal projection on the Hilbert space .,,"I want to prove the following: If $X$ is a Hilbert space and $Y$ is a closed subspace of $X$, then   every $x\in X$ can be written as $x=y+z $ where $y\in Y$,   $z \in Y^\perp$. The projection (into $Y$) map $P:X\to Y$, given by $P(x)=y$ is linear,   bounded, $P^2=P$, and $\langle x_1 , Px_2\rangle =\langle Px_1 , x_2\rangle$. Here I have avoided subscripts (but the projection is always onto the $Y$ space): Consider $x$ in $X$  , then there is a closest point to $x$ in $Y$ . Let us say that point as $Px$ , now we prove that $x-Px$ is orthogonal to $Y$ .   Choose  $y \in Y$ and $ |y|=1$ Now $|x-(Px+ y)|^2 =|x-Px|^2-2Re \alpha(x-Px,  y) + |\alpha|^2 y^2$   Let us choose $\alpha = (y, x-Px)$ then it becomes ,  $ := |x-Px|^2 - 2|(x-Px,  y) |^2 + |(y, x-Px)|^2 =|x-Px|^2 - |(x-Px, y) |^2 $ Which implies that distance of $x$ from $Px+y \in Y$ is less than the $|x-Px|$ , unless $|(x-Px, y) |^2 =0$ which gives us that $x$ and $x-Px$ are orthogonal . Now let us see if $P : x \to Px$ is linear ,  Define $Qx =x-Px$ , We have already shown that $Qx$ is orthogonal to $Y$ Then $P(ax+by)+Q(ax+by) =ax+by =a(Px+Qx)+b(Py+Qy$, moving $P$ and $Q$ on two sides we get  $P(ax+by)-(aPx +bPy) = Q(ax+by) -(aQx+bQy)$, since the right side is in $Y$ and the left side is not in $Y$, both the side should be equal to $0$ , $P(ax+by)-(aPx +bPy)=0$ , hence we show that $P$ is linear . And the boundedness follows because $|x|^2=|Px|^2+|Qx|^2$ , is that right ? Am i right so far ? I am having a bit of difficulty in proving rest of the stuff.  Thanks for your help.","I want to prove the following: If $X$ is a Hilbert space and $Y$ is a closed subspace of $X$, then   every $x\in X$ can be written as $x=y+z $ where $y\in Y$,   $z \in Y^\perp$. The projection (into $Y$) map $P:X\to Y$, given by $P(x)=y$ is linear,   bounded, $P^2=P$, and $\langle x_1 , Px_2\rangle =\langle Px_1 , x_2\rangle$. Here I have avoided subscripts (but the projection is always onto the $Y$ space): Consider $x$ in $X$  , then there is a closest point to $x$ in $Y$ . Let us say that point as $Px$ , now we prove that $x-Px$ is orthogonal to $Y$ .   Choose  $y \in Y$ and $ |y|=1$ Now $|x-(Px+ y)|^2 =|x-Px|^2-2Re \alpha(x-Px,  y) + |\alpha|^2 y^2$   Let us choose $\alpha = (y, x-Px)$ then it becomes ,  $ := |x-Px|^2 - 2|(x-Px,  y) |^2 + |(y, x-Px)|^2 =|x-Px|^2 - |(x-Px, y) |^2 $ Which implies that distance of $x$ from $Px+y \in Y$ is less than the $|x-Px|$ , unless $|(x-Px, y) |^2 =0$ which gives us that $x$ and $x-Px$ are orthogonal . Now let us see if $P : x \to Px$ is linear ,  Define $Qx =x-Px$ , We have already shown that $Qx$ is orthogonal to $Y$ Then $P(ax+by)+Q(ax+by) =ax+by =a(Px+Qx)+b(Py+Qy$, moving $P$ and $Q$ on two sides we get  $P(ax+by)-(aPx +bPy) = Q(ax+by) -(aQx+bQy)$, since the right side is in $Y$ and the left side is not in $Y$, both the side should be equal to $0$ , $P(ax+by)-(aPx +bPy)=0$ , hence we show that $P$ is linear . And the boundedness follows because $|x|^2=|Px|^2+|Qx|^2$ , is that right ? Am i right so far ? I am having a bit of difficulty in proving rest of the stuff.  Thanks for your help.",,"['linear-algebra', 'functional-analysis', 'hilbert-spaces']"
75,Why are linear transformations important?,Why are linear transformations important?,,How important are linear transformations in linear algebra? In some texts linear transformations are introduced first and then the idea of a matrix. In other books linear transformations are relegated to being an application of matrices. What is the best way of introducing linear transformation on a linear algebra course? How do we motivate students to study transformations as part of linear algebra? What is their real impact?,How important are linear transformations in linear algebra? In some texts linear transformations are introduced first and then the idea of a matrix. In other books linear transformations are relegated to being an application of matrices. What is the best way of introducing linear transformation on a linear algebra course? How do we motivate students to study transformations as part of linear algebra? What is their real impact?,,"['linear-algebra', 'soft-question']"
76,Proof of uniqueness of LU factorization,Proof of uniqueness of LU factorization,,"The first question: what is the proof that LU factorization of matrix is unique? Or am I mistaken? The second question is, how can theentries of L below the main diagonal be obtained from the matrix $A$ and $A_1$ that results from the row echelon reduction to $U$? ($A=LU$)","The first question: what is the proof that LU factorization of matrix is unique? Or am I mistaken? The second question is, how can theentries of L below the main diagonal be obtained from the matrix $A$ and $A_1$ that results from the row echelon reduction to $U$? ($A=LU$)",,"['linear-algebra', 'matrices', 'matrix-decomposition', 'gaussian-elimination', 'lu-decomposition']"
77,Proving a basic trace inequality,Proving a basic trace inequality,,"Let $X\in M(\mathbb R)$ and $k\in\mathbb Z^+$, show:   $$\left| \operatorname{tr}(X^{2k})\right| \leq \operatorname{tr}((XX^T)^k)$$ I can do the $k=1$ case, but then I'm stumped.","Let $X\in M(\mathbb R)$ and $k\in\mathbb Z^+$, show:   $$\left| \operatorname{tr}(X^{2k})\right| \leq \operatorname{tr}((XX^T)^k)$$ I can do the $k=1$ case, but then I'm stumped.",,"['linear-algebra', 'inequality']"
78,Isomorphism of Vector spaces over $\mathbb{Q}$,Isomorphism of Vector spaces over,\mathbb{Q},"From this post we see that $\mathbb{R}$ over $\mathbb{Q}$ is infinite dimensional. Similarly $\mathbb{C}$ over $\mathbb{Q}$ is also infinite dimensional, and I rememeber having solved a problem that $\mathbb{R} \cong \mathbb{C}$ when considered as vector spaces over $\mathbb{Q}$. So I would like to extend this question. Suppose $V$ and $W$ are two vector spaces, over the field $\mathbb{Q}$. Can we say that $V \cong W$ if they have the same dimension over $\mathbb{Q}$. I really don't know how to prove or disprove this!","From this post we see that $\mathbb{R}$ over $\mathbb{Q}$ is infinite dimensional. Similarly $\mathbb{C}$ over $\mathbb{Q}$ is also infinite dimensional, and I rememeber having solved a problem that $\mathbb{R} \cong \mathbb{C}$ when considered as vector spaces over $\mathbb{Q}$. So I would like to extend this question. Suppose $V$ and $W$ are two vector spaces, over the field $\mathbb{Q}$. Can we say that $V \cong W$ if they have the same dimension over $\mathbb{Q}$. I really don't know how to prove or disprove this!",,['linear-algebra']
79,Understanding Spectral Theorem,Understanding Spectral Theorem,,"I got the proof from mentioned here . Spectral Theorem : Let $T$ be a normal operator on a finite-dimensional complex inner product space $V$ or a self-adjoint operator on a finite-dimensional real inner product space $V$ .   Let $c_1, \dotsc, c_k$ be the distinct characteristic values of $T$ .   Let $W_j$ be the characteristic space associated with $c_j$ and $E_j$ the orthogonal projection of $V$ on $W_j$ .   Then $W_j$ is orthogonal to $W_i$ when $i \neq j$ , $V$ is the direct sum of $W_1, \dotsc, W_k$ , and $$   T = c_1 E_1 + \dotsb + c_k E_k$$ Proof : Let $\alpha$ be a vector in $W_j$ , $\beta$ a vector in $W_i$ , and suppose $i\neq j$ . Then $c_j⟨\alpha,\beta⟩=⟨T\alpha,\beta⟩=⟨\alpha,T^*\beta⟩=⟨\alpha,\overline{c_j}\beta⟩$ . Hence $(c_j-c_i)⟨\alpha,\beta⟩=0$ , and since $c_j-c_i\neq 0$ , it follows that $⟨\alpha,\beta⟩=0$ . Thus $W_j$ is orthogonal to $W_i$ when when $i\neq j$ . $\color{red}{\text{From the fact that } V \text{ has an orthonormal basis consisting of characteristic vectors, it}}$ $\color{red}{\text{follows that }V=W_1+\cdots+W_k}$ . If $\alpha_j$ belongs to $V_j(1\leq j\leq k)$ and $\color{red}{\alpha_1+\cdots+\alpha_k\stackrel{?}{=}0}$ , then \begin{align} 0&\color{red}{\stackrel{?}{=}}(\alpha_i,\sum_j\alpha_j)=\sum_j(\alpha_i,\alpha_j)\qquad(1)\\ &=\|\alpha_i\|^2 \end{align} For every $i$ , so that $V$ is the direct sum of $W_1,\cdots,W_k$ . Therefore $\color{red}{E_1+\cdots+E_k=I}$ and \begin{align} T&=TE_1+\cdots+TE_k\\ &=c_1E_1+\cdots+c_kE_k \end{align} I am not a follower of Hoffman and Kunze's linear algebra but for seeking my proof of Spectral Theorem I read that part. And I didn't manage to understand the red marked line. Like: $(1)$ Why $V$ space can be written as a sum of the $W_i$ space $?$ $(2)$ Why the sum of the vectors $\alpha_i$ is zero $?$ $(3)$ Why the equality hold for $(1)?$ $(4)$ Why the sum of eigenvectors $E_i$ is $I?$ Maybe all of my question require another reference theorem but I really want to know all of those Why . Thanks for your time and thanks in advance.","I got the proof from mentioned here . Spectral Theorem : Let be a normal operator on a finite-dimensional complex inner product space or a self-adjoint operator on a finite-dimensional real inner product space .   Let be the distinct characteristic values of .   Let be the characteristic space associated with and the orthogonal projection of on .   Then is orthogonal to when , is the direct sum of , and Proof : Let be a vector in , a vector in , and suppose . Then . Hence , and since , it follows that . Thus is orthogonal to when when . . If belongs to and , then For every , so that is the direct sum of . Therefore and I am not a follower of Hoffman and Kunze's linear algebra but for seeking my proof of Spectral Theorem I read that part. And I didn't manage to understand the red marked line. Like: Why space can be written as a sum of the space Why the sum of the vectors is zero Why the equality hold for Why the sum of eigenvectors is Maybe all of my question require another reference theorem but I really want to know all of those Why . Thanks for your time and thanks in advance.","T V V c_1, \dotsc, c_k T W_j c_j E_j V W_j W_j W_i i \neq j V W_1, \dotsc, W_k 
  T = c_1 E_1 + \dotsb + c_k E_k \alpha W_j \beta W_i i\neq j c_j⟨\alpha,\beta⟩=⟨T\alpha,\beta⟩=⟨\alpha,T^*\beta⟩=⟨\alpha,\overline{c_j}\beta⟩ (c_j-c_i)⟨\alpha,\beta⟩=0 c_j-c_i\neq 0 ⟨\alpha,\beta⟩=0 W_j W_i i\neq j \color{red}{\text{From the fact that } V \text{ has an orthonormal basis consisting of characteristic vectors, it}} \color{red}{\text{follows that }V=W_1+\cdots+W_k} \alpha_j V_j(1\leq j\leq k) \color{red}{\alpha_1+\cdots+\alpha_k\stackrel{?}{=}0} \begin{align}
0&\color{red}{\stackrel{?}{=}}(\alpha_i,\sum_j\alpha_j)=\sum_j(\alpha_i,\alpha_j)\qquad(1)\\
&=\|\alpha_i\|^2
\end{align} i V W_1,\cdots,W_k \color{red}{E_1+\cdots+E_k=I} \begin{align}
T&=TE_1+\cdots+TE_k\\
&=c_1E_1+\cdots+c_kE_k
\end{align} (1) V W_i ? (2) \alpha_i ? (3) (1)? (4) E_i I?","['linear-algebra', 'inner-products']"
80,Coefficients of the characteristic polynomial,Coefficients of the characteristic polynomial,,"Let $A$ be an $n\times n$ matrix. Then its characteristic polynomial is $$\phi_A(x) = \det(xI - A) = x^n - (\operatorname{tr} A)x^{n-1} + \cdots + (-1)^{n-1}(\operatorname{tr}(\operatorname{adj}A))x + (-1)^n\det A,$$ where $\operatorname{adj}A$ denotes the adjugate matrix of $A$ . My question: Why are the coefficients of $\phi_A$ those shown above? I can see why $(-1)^n\det A$ is the constant term, since $$\det(A) = \phi_A(0) = (0-\mu_1)\cdots(0-\mu_n) = (-1)^n\mu_1\cdots\mu_n,$$ where $\mu_i$ are the roots ( $i=1,\dots,n$ ).  I also see why the coefficient of $x^{n-1}$ is minus the trace, since by Laplace expansion (along the first row): \begin{align*}     &\det(xI-A)\\[10pt]    &= \begin{vmatrix}          x-a_{11} & -a_{12} & \cdots & -a_{1n}\\          -a_{21} & x-a_{22} & \cdots & -a_{2n}\\          \vdots & \vdots & \ddots & \vdots\\          -a_{n1} & -a_{n2} & \cdots & x-a_{nn}     \end{vmatrix}\\[10pt]     &= (x-a_{11})\begin{vmatrix}          x-a_{22} & \cdots & -a_{2n}\\          \vdots & \ddots & \vdots\\          -a_{n2} & \cdots & x-a_{nn}     \end{vmatrix} + \underbrace{a_{12} \begin{vmatrix}          -a_{21} & \cdots & -a_{2n}\\          \vdots & \ddots & \vdots\\          -a_{n1} & \cdots & x-a_{nn}     \end{vmatrix} + \cdots}_{\text{at most polynomial of degree $n-2$}}\\[10pt]    &=(x-a_{11})\bigg((x-a_{22})\begin{vmatrix}         x-a_{33} & \cdots & -a_{3n}\\        \vdots & \ddots & \vdots\\        -a_{n3} & \cdots & x-a_{nn}     \end{vmatrix}     +\underbrace{\underbrace{a_{23}\begin{vmatrix}         -a_{32} & \cdots & -a_{3n}\\        \vdots & \ddots & \vdots\\        -a_{n2} & \cdots & x-a_{nn}     \end{vmatrix}+\cdots}_{\text{at most polynomial of degree $n-3$}}      \bigg) + \cdots}_{\text{at most polynomial of degree $n-2$}}\\     &\kern10pt\vdots\\[10pt]     &=(x-a_{11})(x-a_{22})\cdots(x-a_{nn}) + \underbrace{\cdots}_{\text{at most polynomial of degree $n-2$}}\\[10pt]     &= x^n - (a_{11}+a_{22}+\cdots + a_{nn})x^{n-1} + \cdots\\[10pt]     &= x^n - (\operatorname{tr}A)x^{n-1} + \cdots \end{align*} (Let me know if there is a more elegant way to obtain this). But what I cannot seem to prove is that the coefficient of $x$ is $ (-1)^{n-1}\operatorname{tr}(\operatorname{adj}A)$ . Attempting to work with the Laplace expansion (as I did to obtain the trace above) is leading me nowhere.","Let be an matrix. Then its characteristic polynomial is where denotes the adjugate matrix of . My question: Why are the coefficients of those shown above? I can see why is the constant term, since where are the roots ( ).  I also see why the coefficient of is minus the trace, since by Laplace expansion (along the first row): (Let me know if there is a more elegant way to obtain this). But what I cannot seem to prove is that the coefficient of is . Attempting to work with the Laplace expansion (as I did to obtain the trace above) is leading me nowhere.","A n\times n \phi_A(x) = \det(xI - A) = x^n - (\operatorname{tr} A)x^{n-1} + \cdots + (-1)^{n-1}(\operatorname{tr}(\operatorname{adj}A))x + (-1)^n\det A, \operatorname{adj}A A \phi_A (-1)^n\det A \det(A) = \phi_A(0) = (0-\mu_1)\cdots(0-\mu_n) = (-1)^n\mu_1\cdots\mu_n, \mu_i i=1,\dots,n x^{n-1} \begin{align*}
    &\det(xI-A)\\[10pt]
   &= \begin{vmatrix}
         x-a_{11} & -a_{12} & \cdots & -a_{1n}\\
         -a_{21} & x-a_{22} & \cdots & -a_{2n}\\
         \vdots & \vdots & \ddots & \vdots\\
         -a_{n1} & -a_{n2} & \cdots & x-a_{nn}
    \end{vmatrix}\\[10pt]
    &= (x-a_{11})\begin{vmatrix}
         x-a_{22} & \cdots & -a_{2n}\\
         \vdots & \ddots & \vdots\\
         -a_{n2} & \cdots & x-a_{nn}
    \end{vmatrix} + \underbrace{a_{12} \begin{vmatrix}
         -a_{21} & \cdots & -a_{2n}\\
         \vdots & \ddots & \vdots\\
         -a_{n1} & \cdots & x-a_{nn}
    \end{vmatrix} + \cdots}_{\text{at most polynomial of degree n-2}}\\[10pt]
   &=(x-a_{11})\bigg((x-a_{22})\begin{vmatrix} 
       x-a_{33} & \cdots & -a_{3n}\\
       \vdots & \ddots & \vdots\\
       -a_{n3} & \cdots & x-a_{nn}
    \end{vmatrix}
    +\underbrace{\underbrace{a_{23}\begin{vmatrix} 
       -a_{32} & \cdots & -a_{3n}\\
       \vdots & \ddots & \vdots\\
       -a_{n2} & \cdots & x-a_{nn}
    \end{vmatrix}+\cdots}_{\text{at most polynomial of degree n-3}}
     \bigg) + \cdots}_{\text{at most polynomial of degree n-2}}\\
    &\kern10pt\vdots\\[10pt]
    &=(x-a_{11})(x-a_{22})\cdots(x-a_{nn}) + \underbrace{\cdots}_{\text{at most polynomial of degree n-2}}\\[10pt]
    &= x^n - (a_{11}+a_{22}+\cdots + a_{nn})x^{n-1} + \cdots\\[10pt]
    &= x^n - (\operatorname{tr}A)x^{n-1} + \cdots
\end{align*} x  (-1)^{n-1}\operatorname{tr}(\operatorname{adj}A)","['linear-algebra', 'matrices', 'determinant']"
81,Burning a rope to count time,Burning a rope to count time,,"A rope burns irregularly in 16 minutes and costs 32 rupees, while a second rope burns also irregularly in 7 minutes and costs 14 rupees. Both can be lit only at one end and can be turned off and lit again as many times we want, until they are completely burned. In what way can we count 1 minute by using such ropes and how much will it cost? (Multiple ropes can be used but we are looking for the combination that will cost less). My attempt: We light the two ropes simultaneously so when the second is fully burnt, 7 minutes have passed, so the remaining from the first one will be burnt in 9 minutes. Then we light a second of 7 mins and this way we have one of 2 minutes. Following the same procedure twice, we can have another one of 2 minutes. Then we light another of 7 minutes, along with the two of 2 minutes one after the other, and we have one of 3 minutes left (7-2-2). Then with the one of 3 minutes and another one of 2 minutes we can count 1 minute. ...but this would be a disaster, cost-wise :) Any other ideas are most welcome!!!","A rope burns irregularly in 16 minutes and costs 32 rupees, while a second rope burns also irregularly in 7 minutes and costs 14 rupees. Both can be lit only at one end and can be turned off and lit again as many times we want, until they are completely burned. In what way can we count 1 minute by using such ropes and how much will it cost? (Multiple ropes can be used but we are looking for the combination that will cost less). My attempt: We light the two ropes simultaneously so when the second is fully burnt, 7 minutes have passed, so the remaining from the first one will be burnt in 9 minutes. Then we light a second of 7 mins and this way we have one of 2 minutes. Following the same procedure twice, we can have another one of 2 minutes. Then we light another of 7 minutes, along with the two of 2 minutes one after the other, and we have one of 3 minutes left (7-2-2). Then with the one of 3 minutes and another one of 2 minutes we can count 1 minute. ...but this would be a disaster, cost-wise :) Any other ideas are most welcome!!!",,['linear-algebra']
82,Rational function field over uncountable field is uncountably dimensional,Rational function field over uncountable field is uncountably dimensional,,"Here is the problem and I'm a bit stuck in my proof. We have an uncountable field $k$ and a transcendental element $t$ over $k$ so we consider the field $k(t)$ and want to prove that $\dim(k(t)/k)$ is uncountable. The idea is to consider the vectors $\frac{1}{t-\lambda}$ for $\lambda \in k$ and to show that they are linearly independent over $k$. Let us take $c_1,c_2,...,c_n$ elements of $k$ such that : $$\sum_{i=1}^{n}\frac{c_i}{t-\lambda_i}=0$$ So we get that: $$\sum_{i=1}^{n}c_i\prod_{j\neq i}(t-\lambda_j)=0$$ But $t$ being transcendental this polynomial in $t$ must be $0$ and all the coefficients must be $0$. From here I don't know how to conclude so any help would be very welcomed.","Here is the problem and I'm a bit stuck in my proof. We have an uncountable field $k$ and a transcendental element $t$ over $k$ so we consider the field $k(t)$ and want to prove that $\dim(k(t)/k)$ is uncountable. The idea is to consider the vectors $\frac{1}{t-\lambda}$ for $\lambda \in k$ and to show that they are linearly independent over $k$. Let us take $c_1,c_2,...,c_n$ elements of $k$ such that : $$\sum_{i=1}^{n}\frac{c_i}{t-\lambda_i}=0$$ So we get that: $$\sum_{i=1}^{n}c_i\prod_{j\neq i}(t-\lambda_j)=0$$ But $t$ being transcendental this polynomial in $t$ must be $0$ and all the coefficients must be $0$. From here I don't know how to conclude so any help would be very welcomed.",,"['linear-algebra', 'abstract-algebra', 'field-theory']"
83,Why does the eigenvalues of $AA^T$ are the squares of the singular values of $A$?,Why does the eigenvalues of  are the squares of the singular values of ?,AA^T A,"Let $A$ where it's non-zero singular values are $\sigma_1,\ldots,\sigma_r$. $A$ can be written as $A=U\Sigma V$ where both $U,V$ are unitary matrices. Let's look at $$AA^T = U\Sigma V^TV\Sigma U^T= U\Sigma^2 U^T$$ Now, from this point one should infer that the eigenvalues of $AA^T$ are $\sigma_1^2,\ldots,\sigma_r^2$ but I'm not sure how exactly. Is that has something to do with the fact that $U$ is unitary?","Let $A$ where it's non-zero singular values are $\sigma_1,\ldots,\sigma_r$. $A$ can be written as $A=U\Sigma V$ where both $U,V$ are unitary matrices. Let's look at $$AA^T = U\Sigma V^TV\Sigma U^T= U\Sigma^2 U^T$$ Now, from this point one should infer that the eigenvalues of $AA^T$ are $\sigma_1^2,\ldots,\sigma_r^2$ but I'm not sure how exactly. Is that has something to do with the fact that $U$ is unitary?",,"['linear-algebra', 'matrices', 'numerical-methods', 'svd']"
84,Projecting data onto a vector,Projecting data onto a vector,,"I have learned that projecting a vector a onto a vector b is done by multiplying the orthogonal projection of a (say $\mathbf{a_o}$) with the unit vector $\mathbf{\hat{b}}$ in the direction of b = Orthogonal projection of a onto b = $\mathbf{a_o}$ * $\mathbf{\hat{b}}$ However in the context of principal component analysis, I often read that the projection of the data D onto an eigenvector v is simply $\mathbf{v}^{T}$*$D$ (and so we don't take the orthogonal components of the data vectors of D..?) - why is that? Thanks","I have learned that projecting a vector a onto a vector b is done by multiplying the orthogonal projection of a (say $\mathbf{a_o}$) with the unit vector $\mathbf{\hat{b}}$ in the direction of b = Orthogonal projection of a onto b = $\mathbf{a_o}$ * $\mathbf{\hat{b}}$ However in the context of principal component analysis, I often read that the projection of the data D onto an eigenvector v is simply $\mathbf{v}^{T}$*$D$ (and so we don't take the orthogonal components of the data vectors of D..?) - why is that? Thanks",,['linear-algebra']
85,Do infinite dimensional Hermitian operators admit a complete basis of eigenvectors?,Do infinite dimensional Hermitian operators admit a complete basis of eigenvectors?,,"I'm currently taking a quantum mechanics course. We have proven that hermitian operators always have real eigenvalues, that we can choose the eigenvectors to be orthonormal, and that finite dimensional hermitian operators are diagonalizable (i.e., admit a complete basis of eigenvectors). Can this last result be generalized to the infinite dimensional case? The standard proof seems to use induction on the dimension of the operator, so this proof certainly doesn't carry over.","I'm currently taking a quantum mechanics course. We have proven that hermitian operators always have real eigenvalues, that we can choose the eigenvectors to be orthonormal, and that finite dimensional hermitian operators are diagonalizable (i.e., admit a complete basis of eigenvectors). Can this last result be generalized to the infinite dimensional case? The standard proof seems to use induction on the dimension of the operator, so this proof certainly doesn't carry over.",,"['linear-algebra', 'eigenvalues-eigenvectors', 'quantum-mechanics']"
86,matrix operator norm and inner product,matrix operator norm and inner product,,"Is it true that $\Vert A\Vert:=\sup_{\Vert x\Vert=1}\Vert Ax\Vert=\sup_{\Vert x\Vert=\Vert y\Vert=1}\vert\langle y,Ax\rangle\vert$ for arbitrary matrices $A$? Showing $""\geq""$ seems to be straightforward using Cauchy-Schwarz.","Is it true that $\Vert A\Vert:=\sup_{\Vert x\Vert=1}\Vert Ax\Vert=\sup_{\Vert x\Vert=\Vert y\Vert=1}\vert\langle y,Ax\rangle\vert$ for arbitrary matrices $A$? Showing $""\geq""$ seems to be straightforward using Cauchy-Schwarz.",,"['linear-algebra', 'normed-spaces']"
87,When does a matrix have a cyclic decomposition and when a rational form?,When does a matrix have a cyclic decomposition and when a rational form?,,"I'm reading Hoffman's ""Linear Algebra"" and this question comes to my mind: suppose $V$ is a finite-dimensional vector space on a field $F$, and $T$ a linear operator on $V$, when does a matrix has Cyclic Decomposition, and when does it has a Rational Form? The Cyclic Decomposition Theorem (Hoffman, Thm 7.3) says: Let $T$ be a linear operator on a finite-dimensional vector space $V$   and let $W_0$ be a proper $T$-admissible subspace of $V$. There exist   non-zero vectors $a_1, \dots , a_r$ in $V$ with respective   $T$-annihilators $p_1, \dots , p_r$ such that $V = W_0 \oplus  Z(\alpha_1; T) \oplus \dots \oplus Z (\alpha_r ; T)$ ; So the Cyclic Decomposition exists iff there is a $T$-admissible proper subspace $W_0 \subsetneq V$ to start from. Then what is the the condition for $W_0$'s existence? A Rational Form is actually the Cyclic Decomposition with $W_0=\{0\}$. So does this mean the condition for the existence of a rational form is that $T$'s character values are all in field $F$? For example a $n\times n$ matrix $T$ on real number field $\mathbb R$ has a rational form, so long as all the characteristic values of $T$ are all real numbers?","I'm reading Hoffman's ""Linear Algebra"" and this question comes to my mind: suppose $V$ is a finite-dimensional vector space on a field $F$, and $T$ a linear operator on $V$, when does a matrix has Cyclic Decomposition, and when does it has a Rational Form? The Cyclic Decomposition Theorem (Hoffman, Thm 7.3) says: Let $T$ be a linear operator on a finite-dimensional vector space $V$   and let $W_0$ be a proper $T$-admissible subspace of $V$. There exist   non-zero vectors $a_1, \dots , a_r$ in $V$ with respective   $T$-annihilators $p_1, \dots , p_r$ such that $V = W_0 \oplus  Z(\alpha_1; T) \oplus \dots \oplus Z (\alpha_r ; T)$ ; So the Cyclic Decomposition exists iff there is a $T$-admissible proper subspace $W_0 \subsetneq V$ to start from. Then what is the the condition for $W_0$'s existence? A Rational Form is actually the Cyclic Decomposition with $W_0=\{0\}$. So does this mean the condition for the existence of a rational form is that $T$'s character values are all in field $F$? For example a $n\times n$ matrix $T$ on real number field $\mathbb R$ has a rational form, so long as all the characteristic values of $T$ are all real numbers?",,['linear-algebra']
88,Negative sign incorrect when finding determinant by applying Gaussian Elimination,Negative sign incorrect when finding determinant by applying Gaussian Elimination,,"(I don't know how to make a matrix here, someone please correct it into a better format, thanks~) So I'm applying the Gaussian Elimination to find the determinant for this matrix: $\begin{pmatrix} 0&1&3\\ 1&2&0\\ 0&3&4\\ \end{pmatrix}$ So, I switched row $1$ and $2$:  $\begin{pmatrix} 1&2&0\\ 0&1&3\\ 0&3&4\\ \end{pmatrix}$ Then, add the multiple of $-3$ of row $2$ to the third row: $\begin{pmatrix} 1&2&0\\ 0&1&3\\ 0&0&-5\\ \end{pmatrix}$ So the determinant I got is $-5$, however the answer key said it's $5$.  Some1 point out what I have done wrong?  Thank you!!","(I don't know how to make a matrix here, someone please correct it into a better format, thanks~) So I'm applying the Gaussian Elimination to find the determinant for this matrix: $\begin{pmatrix} 0&1&3\\ 1&2&0\\ 0&3&4\\ \end{pmatrix}$ So, I switched row $1$ and $2$:  $\begin{pmatrix} 1&2&0\\ 0&1&3\\ 0&3&4\\ \end{pmatrix}$ Then, add the multiple of $-3$ of row $2$ to the third row: $\begin{pmatrix} 1&2&0\\ 0&1&3\\ 0&0&-5\\ \end{pmatrix}$ So the determinant I got is $-5$, however the answer key said it's $5$.  Some1 point out what I have done wrong?  Thank you!!",,"['linear-algebra', 'matrices']"
89,Matrix Determinant Identity,Matrix Determinant Identity,,"I have come across an observation about the determinant of a matrix, but I don't know how to prove it in general. Let me demonstrate it through an example. $$ \begin{align} \left| \begin{matrix}  1 & 2 & 3 & 4 \\  5 & 6 & 7 & 8 \\  9 & 10 & 11 & 12 \\  13 & 14 & 15 & 16 \\ \end{matrix} \right| \cdot \left| \begin{matrix}  6 & 7 \\  10 & 11 \\ \end{matrix} \right| &= \left| \begin{matrix}  1 & 2 & 3 \\  5 & 6 & 7 \\  9 & 10 & 11 \\ \end{matrix} \right| \cdot\left| \begin{matrix}  6 & 7 & 8 \\  10 & 11 & 12 \\  14 & 15 & 16 \\ \end{matrix} \right|\\ &- \left| \begin{matrix}  2 & 3 & 4 \\  6 & 7 & 8 \\  10 & 11 & 12 \\ \end{matrix} \right| \cdot \left| \begin{matrix}  5 & 6 & 7 \\  9 & 10 & 11 \\  13 & 14 & 15 \\ \end{matrix} \right| \end{align} $$ What this essentially says is that you can find the determinant of a larger matrix by breaking it down into this ""ad-bc""-style of smaller determinants, however I can't work out why this should be true. I've expanded out the determinant for a 3x3 example in full generality and confirmed that it works, but I'd like to know if there's a more straightforward reason for this observation.","I have come across an observation about the determinant of a matrix, but I don't know how to prove it in general. Let me demonstrate it through an example. $$ \begin{align} \left| \begin{matrix}  1 & 2 & 3 & 4 \\  5 & 6 & 7 & 8 \\  9 & 10 & 11 & 12 \\  13 & 14 & 15 & 16 \\ \end{matrix} \right| \cdot \left| \begin{matrix}  6 & 7 \\  10 & 11 \\ \end{matrix} \right| &= \left| \begin{matrix}  1 & 2 & 3 \\  5 & 6 & 7 \\  9 & 10 & 11 \\ \end{matrix} \right| \cdot\left| \begin{matrix}  6 & 7 & 8 \\  10 & 11 & 12 \\  14 & 15 & 16 \\ \end{matrix} \right|\\ &- \left| \begin{matrix}  2 & 3 & 4 \\  6 & 7 & 8 \\  10 & 11 & 12 \\ \end{matrix} \right| \cdot \left| \begin{matrix}  5 & 6 & 7 \\  9 & 10 & 11 \\  13 & 14 & 15 \\ \end{matrix} \right| \end{align} $$ What this essentially says is that you can find the determinant of a larger matrix by breaking it down into this ""ad-bc""-style of smaller determinants, however I can't work out why this should be true. I've expanded out the determinant for a 3x3 example in full generality and confirmed that it works, but I'd like to know if there's a more straightforward reason for this observation.",,"['linear-algebra', 'matrices', 'determinant']"
90,Graphs with commuting adjacency matrices,Graphs with commuting adjacency matrices,,Let A and B be adjacency matrix of two undirected simple graphs. Can we assign some combinatorial interpretations to this pair of graphs if A and B commute?,Let A and B be adjacency matrix of two undirected simple graphs. Can we assign some combinatorial interpretations to this pair of graphs if A and B commute?,,"['linear-algebra', 'matrices', 'graph-theory']"
91,Relationship between dual space and adjoint of a linear operator,Relationship between dual space and adjoint of a linear operator,,"I am having a hard time understanding the concept of adjoint of a linear operator. Given a finite dimensional Hilbert space $H$ over a field $F$, I know the dual space is the vector space $H^*$ of all linear forms $f:H\rightarrow F$. Is the adjoint of a linear map $A$ on $H$ a member of $H^*$? Or what is the relationship between a linear map $f:H\rightarrow K$ and the dual space, generally? What confuses me even more is  this special case: take the bra-ket notation. It says that for any ket $|\psi\rangle\in H$ there is a linear form from $H^*$ called a bra ($|\psi\rangle:=\langle\psi,-\rangle$-also weird, is $\psi$ a function or not?...), such that bra is the adjoint of ket. It's a bit of a mess, any help would be appreciated.","I am having a hard time understanding the concept of adjoint of a linear operator. Given a finite dimensional Hilbert space $H$ over a field $F$, I know the dual space is the vector space $H^*$ of all linear forms $f:H\rightarrow F$. Is the adjoint of a linear map $A$ on $H$ a member of $H^*$? Or what is the relationship between a linear map $f:H\rightarrow K$ and the dual space, generally? What confuses me even more is  this special case: take the bra-ket notation. It says that for any ket $|\psi\rangle\in H$ there is a linear form from $H^*$ called a bra ($|\psi\rangle:=\langle\psi,-\rangle$-also weird, is $\psi$ a function or not?...), such that bra is the adjoint of ket. It's a bit of a mess, any help would be appreciated.",,"['linear-algebra', 'quantum-mechanics']"
92,Why is the dimension of the zero subspace 0 and not 1?,Why is the dimension of the zero subspace 0 and not 1?,,"A related question: If a single non-zero vector serves as a basis for a subspace, then is the dimension of that subspace 1 or 0? I'm almost certain the answer to the above question is 1. But my confusion lies with the fact that the dimension of the zero subspace (which consists only of the zero vector) is zero. There is one vector in this subspace (namely, the zero vector), so shouldn't the dimension be 1? Is there an intuitive way to think about this, or is this just how it's been defined as this is the only way everything works? On this similar post, a commenter said: ""The zero vector itself does not have a dimension. The vector space consisting of only the zero vector has dimension 0. This is because a basis for that vector space is the empty set, and the dimension of a vector space is the cardinality of any basis for that vector space."" I don't see see how this is true, unless the empty set is not counted when counting cardinality (i.e. a set of vectors with 5 vectors as well as the empty set would be counted as having 5 vectors and not 6). Again, is this just how we've defined things?","A related question: If a single non-zero vector serves as a basis for a subspace, then is the dimension of that subspace 1 or 0? I'm almost certain the answer to the above question is 1. But my confusion lies with the fact that the dimension of the zero subspace (which consists only of the zero vector) is zero. There is one vector in this subspace (namely, the zero vector), so shouldn't the dimension be 1? Is there an intuitive way to think about this, or is this just how it's been defined as this is the only way everything works? On this similar post, a commenter said: ""The zero vector itself does not have a dimension. The vector space consisting of only the zero vector has dimension 0. This is because a basis for that vector space is the empty set, and the dimension of a vector space is the cardinality of any basis for that vector space."" I don't see see how this is true, unless the empty set is not counted when counting cardinality (i.e. a set of vectors with 5 vectors as well as the empty set would be counted as having 5 vectors and not 6). Again, is this just how we've defined things?",,"['linear-algebra', 'vector-spaces']"
93,Second directional derivative and Hessian matrix,Second directional derivative and Hessian matrix,,"I am reading the following from the book Deep Learning , and I have the following questions. I don't quite understand second directional derivatives. The first directional derivative of a function $f:\mathbb{R}^m\to\mathbb{R}$ in the direction $u$ represents the slope of $f$ in the direction $u$. So what does the second directional derivative along the direction $u$ represent? In the above paragraph, I understood that $d^THd$, the second directional derivative of $f$ in the direction $d$ ($||d||_2=1$), is given by the corresponding eigenvalue when $d$ is an eigenvector of $H$, because if $d$ is an eigenvector of $H$ then $d^THd=d^T\lambda_d d=\lambda_d d^Td=\lambda_d$. However, I don't understand the statement ""For other directions of $d$, the directional second derivative is a weighted average of all the eigenvalues, with weights between $0$ and $1$""::--Since $H$ is real symmetric, $H$ has $m$ independent orthogonal eigenvectors, which form a basis for $\mathbb{R}^m$. Thus, if $d$ is not an eigenvector, then $d=c_1x_1+\cdots +c_mx_m$ for some scalars $c_i$s and eigenvectors $x_i$s. Thus, $$d^THd=d^TH(c_1x_1+\cdots +c_mx_m)\\=d^T(c_1\lambda_1x_1+\cdots +c_m\lambda_mx_m)\\=c_1^2||x_1||^2\lambda_1 +\cdots +c_m^2||x_m||^2\lambda_m$$, which is ofcourse the weighted average of all the eigenvalues of $H$. But I don't understand why the weights lie between $0$ and $1$ as given. In fact, there is no reason to believe that the weights $c_i^2||x_i||^2$ to be in the range $(0,1)$. Also, I don't understand the statement ""The maximum eigenvalue determines the maximum second derivative, and the minimum eigenvalue determines the minimum second derivative"". Can you explain this?","I am reading the following from the book Deep Learning , and I have the following questions. I don't quite understand second directional derivatives. The first directional derivative of a function $f:\mathbb{R}^m\to\mathbb{R}$ in the direction $u$ represents the slope of $f$ in the direction $u$. So what does the second directional derivative along the direction $u$ represent? In the above paragraph, I understood that $d^THd$, the second directional derivative of $f$ in the direction $d$ ($||d||_2=1$), is given by the corresponding eigenvalue when $d$ is an eigenvector of $H$, because if $d$ is an eigenvector of $H$ then $d^THd=d^T\lambda_d d=\lambda_d d^Td=\lambda_d$. However, I don't understand the statement ""For other directions of $d$, the directional second derivative is a weighted average of all the eigenvalues, with weights between $0$ and $1$""::--Since $H$ is real symmetric, $H$ has $m$ independent orthogonal eigenvectors, which form a basis for $\mathbb{R}^m$. Thus, if $d$ is not an eigenvector, then $d=c_1x_1+\cdots +c_mx_m$ for some scalars $c_i$s and eigenvectors $x_i$s. Thus, $$d^THd=d^TH(c_1x_1+\cdots +c_mx_m)\\=d^T(c_1\lambda_1x_1+\cdots +c_m\lambda_mx_m)\\=c_1^2||x_1||^2\lambda_1 +\cdots +c_m^2||x_m||^2\lambda_m$$, which is ofcourse the weighted average of all the eigenvalues of $H$. But I don't understand why the weights lie between $0$ and $1$ as given. In fact, there is no reason to believe that the weights $c_i^2||x_i||^2$ to be in the range $(0,1)$. Also, I don't understand the statement ""The maximum eigenvalue determines the maximum second derivative, and the minimum eigenvalue determines the minimum second derivative"". Can you explain this?",,"['linear-algebra', 'multivariable-calculus', 'eigenvalues-eigenvectors', 'hessian-matrix']"
94,Pairwise distance matrix,Pairwise distance matrix,,"Suppose $x_1,\dots,x_n \in \mathbb{R}^d$. Is there a vectorized way of representing the square distance matrix $D_{ij} = \lVert x_i - x_j \rVert^2$?","Suppose $x_1,\dots,x_n \in \mathbb{R}^d$. Is there a vectorized way of representing the square distance matrix $D_{ij} = \lVert x_i - x_j \rVert^2$?",,"['linear-algebra', 'optimization']"
95,The Determinant of an Operator Equals the Product of Determinants of its Restrictions,The Determinant of an Operator Equals the Product of Determinants of its Restrictions,,"Consider the following definitions and proved theorems Definitions $1$. The field $\Bbb{F}$ is $\Bbb{R}$ or $\Bbb{C}$. $2$. $V$ is a vector space over $\Bbb{F}$. $3$. $\mathcal{L}(V)$ is the vector space of all operators on $V$ (that is, linear maps $V \to V$) and $T \in \mathcal{L}(V)$. $4$. $T|_{U}$ is the restriction of $T$ to the invariant subspace $U$. $5$. $G(\lambda_i,T)$ is the generalized eigen-space of $T$ corresponding to eigen-value $\lambda_i$. The multiplicity of $\lambda_i$ is defined to be $d_i=\dim G(\lambda_i,T)$. $6$. If $\Bbb{F}=\Bbb{C}$ then $\lambda_1,\cdots,\lambda_m$ are distinct eigen-values of $T$.  If $\Bbb{F}=\Bbb{R}$ then $\lambda_1,\cdots,\lambda_m$ are distinct eigen-values of  the complexification of $T$, denoted by $T_{\Bbb{C}}$. Then $\det T = \lambda_1^{d_1}\cdots\lambda_m^{d_m}$ where each $d_i$ is the multiplicity of $\lambda_i$. Proved Theorems $1$. Suppose $V$ is a vector space over $\Bbb{C}$ and $T \in \mathcal{L}(V)$. Let $\lambda_1,\cdots,\lambda_m$ be distinct eigen values of $T$. Then   $V=G(\lambda_1,T) \oplus \cdots \oplus G(\lambda_m,T)$ where each $G(\lambda_i,T)$ is invariant under $T$ . Furthermore, $\dim V = d_1+\cdots+d_m$ where $d_i$ is the multiplicity of $\lambda_i$. Now, I want to prove the following theorem just using the above tools. Any hint or help is appreciated. Question If $T \in \mathcal{L}(V)$ and $V=V_1 \oplus \cdots \oplus V_M$ with each $V_j,\,j=1,\cdots,M$ invariant under $T$ then $\det T = \det T|_{V_1} \cdots \det T|_{V_M}$. This problem happened in example $10.28$ of the book Linear Algebra Done Right . There was just one sentence for the proof of this in the text. Because the dimensions of generalized eigen-spaces in $V_j$ add up to $\dim V$. but I don't understand it!","Consider the following definitions and proved theorems Definitions $1$. The field $\Bbb{F}$ is $\Bbb{R}$ or $\Bbb{C}$. $2$. $V$ is a vector space over $\Bbb{F}$. $3$. $\mathcal{L}(V)$ is the vector space of all operators on $V$ (that is, linear maps $V \to V$) and $T \in \mathcal{L}(V)$. $4$. $T|_{U}$ is the restriction of $T$ to the invariant subspace $U$. $5$. $G(\lambda_i,T)$ is the generalized eigen-space of $T$ corresponding to eigen-value $\lambda_i$. The multiplicity of $\lambda_i$ is defined to be $d_i=\dim G(\lambda_i,T)$. $6$. If $\Bbb{F}=\Bbb{C}$ then $\lambda_1,\cdots,\lambda_m$ are distinct eigen-values of $T$.  If $\Bbb{F}=\Bbb{R}$ then $\lambda_1,\cdots,\lambda_m$ are distinct eigen-values of  the complexification of $T$, denoted by $T_{\Bbb{C}}$. Then $\det T = \lambda_1^{d_1}\cdots\lambda_m^{d_m}$ where each $d_i$ is the multiplicity of $\lambda_i$. Proved Theorems $1$. Suppose $V$ is a vector space over $\Bbb{C}$ and $T \in \mathcal{L}(V)$. Let $\lambda_1,\cdots,\lambda_m$ be distinct eigen values of $T$. Then   $V=G(\lambda_1,T) \oplus \cdots \oplus G(\lambda_m,T)$ where each $G(\lambda_i,T)$ is invariant under $T$ . Furthermore, $\dim V = d_1+\cdots+d_m$ where $d_i$ is the multiplicity of $\lambda_i$. Now, I want to prove the following theorem just using the above tools. Any hint or help is appreciated. Question If $T \in \mathcal{L}(V)$ and $V=V_1 \oplus \cdots \oplus V_M$ with each $V_j,\,j=1,\cdots,M$ invariant under $T$ then $\det T = \det T|_{V_1} \cdots \det T|_{V_M}$. This problem happened in example $10.28$ of the book Linear Algebra Done Right . There was just one sentence for the proof of this in the text. Because the dimensions of generalized eigen-spaces in $V_j$ add up to $\dim V$. but I don't understand it!",,['linear-algebra']
96,Visualization of the dual space of a vector space,Visualization of the dual space of a vector space,,"I am wondering what the motivation was for defining a dual space of a vector space, and how to visualize the dual space. I'm asking since it doesn't seem to me to be intuitive to deal with such a space. In particular, I'm looking for questions where it would be natural to consider the space of linear functionals in order to answer these questions.","I am wondering what the motivation was for defining a dual space of a vector space, and how to visualize the dual space. I'm asking since it doesn't seem to me to be intuitive to deal with such a space. In particular, I'm looking for questions where it would be natural to consider the space of linear functionals in order to answer these questions.",,['linear-algebra']
97,Why is it bad to pick basis for a vector space?,Why is it bad to pick basis for a vector space?,,"Reading `This Week's Finds', http://math.ucr.edu/home/baez/week247.html , I'm informed that one should avoid picking coordinate systems and I'm unsure why that is the case. Any help on the matter is appreciated. Linear algebra is all about vector spaces and linear maps. One of the lessons that gets drummed into you when you study this subject is that it's good to avoid picking bases for your vector spaces until you need them. It's good to keep the freedom to do coordinate transformations... and not just keep it in reserve, but keep it manifest! Why? Is there a simple example of when a choice of coordinates makes a difference in a result? ---thereby giving reason to avoid basis. As Hermann Weyl wrote, ""The introduction of a coordinate system to geometry is an act of violence"". Why? Some googling has informed me that this issue is part of the larger notion of categorical naturality...","Reading `This Week's Finds', http://math.ucr.edu/home/baez/week247.html , I'm informed that one should avoid picking coordinate systems and I'm unsure why that is the case. Any help on the matter is appreciated. Linear algebra is all about vector spaces and linear maps. One of the lessons that gets drummed into you when you study this subject is that it's good to avoid picking bases for your vector spaces until you need them. It's good to keep the freedom to do coordinate transformations... and not just keep it in reserve, but keep it manifest! Why? Is there a simple example of when a choice of coordinates makes a difference in a result? ---thereby giving reason to avoid basis. As Hermann Weyl wrote, ""The introduction of a coordinate system to geometry is an act of violence"". Why? Some googling has informed me that this issue is part of the larger notion of categorical naturality...",,"['linear-algebra', 'vector-spaces', 'category-theory', 'coordinate-systems', 'change-of-basis']"
98,Book for studying Linear Algebra,Book for studying Linear Algebra,,"So I'm taking Linear Algebra in college. However, I'm not getting the grades I want and I have sort of difficulties using my teacher's book: it has very formal explanations and a strong lack of examples. I'm looking for a book that has a good explanation of the content and also solved exercises (which is a very important thing that I'm missing). So here is a list of books my college has: Calculus: T. M. Apostol 1994 Vol. I. and Vol.II Reverté Linear Algebra and Its Applications: G. Strang 1988 3rd ed. Academic Press Linear Algebra: S. Lipschutz 1994 Schaum's Outline Series. McGraw-Hill What is in your opinion the best book for self-study? (I'm going to repeat the examinations next semester but I'll be studying on my own.) If there is a better book than the ones on this list please tell me. Thanks!! EDIT:  I study in a Portuguese-speaking country and we use a Portuguese book. The contents of the course are: Systems of linear equations. Gaussian elimination. Vectors and matrices. Inverse matrices. Linear spaces and linear transformations. Linear independence, bases and dimension. Kernel and range of a linear transformation. Applications to linear differential equations. Inner products and norms, orthogonal bases and Gram-Schmidt orthogonalization, orthogonal complements and projection onto subspaces. Applications to equations of straight lines and planes. Least squares approximations. Determinants and their applications. Eigenvalues and eigenvectors. Invariant subspaces. Diagonalization of matrices. Jordan forms. Hermitian, skew Hermitian, and unitary transformations. Quadratic forms.","So I'm taking Linear Algebra in college. However, I'm not getting the grades I want and I have sort of difficulties using my teacher's book: it has very formal explanations and a strong lack of examples. I'm looking for a book that has a good explanation of the content and also solved exercises (which is a very important thing that I'm missing). So here is a list of books my college has: Calculus: T. M. Apostol 1994 Vol. I. and Vol.II Reverté Linear Algebra and Its Applications: G. Strang 1988 3rd ed. Academic Press Linear Algebra: S. Lipschutz 1994 Schaum's Outline Series. McGraw-Hill What is in your opinion the best book for self-study? (I'm going to repeat the examinations next semester but I'll be studying on my own.) If there is a better book than the ones on this list please tell me. Thanks!! EDIT:  I study in a Portuguese-speaking country and we use a Portuguese book. The contents of the course are: Systems of linear equations. Gaussian elimination. Vectors and matrices. Inverse matrices. Linear spaces and linear transformations. Linear independence, bases and dimension. Kernel and range of a linear transformation. Applications to linear differential equations. Inner products and norms, orthogonal bases and Gram-Schmidt orthogonalization, orthogonal complements and projection onto subspaces. Applications to equations of straight lines and planes. Least squares approximations. Determinants and their applications. Eigenvalues and eigenvectors. Invariant subspaces. Diagonalization of matrices. Jordan forms. Hermitian, skew Hermitian, and unitary transformations. Quadratic forms.",,"['linear-algebra', 'reference-request', 'book-recommendation']"
99,Suggest a follow up book to Axler's Linear Algebra Done Right?,Suggest a follow up book to Axler's Linear Algebra Done Right?,,"So I know that a similar question has probably been asked about alternatives or compliments to this book, but I think my situation is different enough to warrant slightly different advice. So I've just completed a course that used this text and I've read it pretty thoroughly for the most part, but I didn't learn everything as well as I wanted (maybe that's normal), probably because some days I was slightly less motivated than others and so my knowledge is better in some areas than others. I want to close the gaps obviously, but I'm not sure I want to just re-read the text, not for a few weeks anyway. Could anybody suggest another book that treats linear algebra in a similar theoretical fashion (less emphasis on matrices), or should I just wait a week or two after the semester is over to re-charge and just re-read sections from this text? Also, I've had two semesters of real analysis, and am planning to self study functional analysis over the summer, will this afford me a chance to plug a few of the holes? As in, do a lot of the more fundamental results in linear algebra also pop up in a functional analysis text? That way I could possibly kill two birds with one stone?","So I know that a similar question has probably been asked about alternatives or compliments to this book, but I think my situation is different enough to warrant slightly different advice. So I've just completed a course that used this text and I've read it pretty thoroughly for the most part, but I didn't learn everything as well as I wanted (maybe that's normal), probably because some days I was slightly less motivated than others and so my knowledge is better in some areas than others. I want to close the gaps obviously, but I'm not sure I want to just re-read the text, not for a few weeks anyway. Could anybody suggest another book that treats linear algebra in a similar theoretical fashion (less emphasis on matrices), or should I just wait a week or two after the semester is over to re-charge and just re-read sections from this text? Also, I've had two semesters of real analysis, and am planning to self study functional analysis over the summer, will this afford me a chance to plug a few of the holes? As in, do a lot of the more fundamental results in linear algebra also pop up in a functional analysis text? That way I could possibly kill two birds with one stone?",,"['linear-algebra', 'reference-request', 'soft-question', 'book-recommendation']"
