,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Why is $\mathbb{C}_p$ isomorphic to $\mathbb{C}$?,Why is  isomorphic to ?,\mathbb{C}_p \mathbb{C},I know that two closed fields of caracteristic $0$ and uncountable are isomorphic iff they have the same cardinality. But I don't know why $\mathbb{C}_p$ has the same cardinality as $\mathbb{C}$. Can anyone give me some reference or hint ?,I know that two closed fields of caracteristic $0$ and uncountable are isomorphic iff they have the same cardinality. But I don't know why $\mathbb{C}_p$ has the same cardinality as $\mathbb{C}$. Can anyone give me some reference or hint ?,,"['abstract-algebra', 'field-theory', 'valuation-theory']"
1,Can we construct $\Bbb C$ without first identifying $\Bbb R$?,Can we construct  without first identifying ?,\Bbb C \Bbb R,"Sometimes it is useful to consider $\Bbb C$ as our primitive and identify $\Bbb R$ as a subset of $\Bbb C$.  Thus we can define $\Bbb R$ (or at least a set with all of the interesting properties of $\Bbb R$) from $\Bbb C$. This suggests to me that there is some way of constructing $\Bbb C$ without first constructing (or taking as a primitive) $\Bbb R$.  However, I've never seen such a construction of $\Bbb C$ (a quick Google search didn't provide me one, either).  I've the Cayley-Dickson construction and the matrix construction many times, but are they the only known ways of constructing $\Bbb C$? My question: Is there a way to construct the set of complex numbers without already having (or first constructing) the real numbers?","Sometimes it is useful to consider $\Bbb C$ as our primitive and identify $\Bbb R$ as a subset of $\Bbb C$.  Thus we can define $\Bbb R$ (or at least a set with all of the interesting properties of $\Bbb R$) from $\Bbb C$. This suggests to me that there is some way of constructing $\Bbb C$ without first constructing (or taking as a primitive) $\Bbb R$.  However, I've never seen such a construction of $\Bbb C$ (a quick Google search didn't provide me one, either).  I've the Cayley-Dickson construction and the matrix construction many times, but are they the only known ways of constructing $\Bbb C$? My question: Is there a way to construct the set of complex numbers without already having (or first constructing) the real numbers?",,"['abstract-algebra', 'field-theory', 'extension-field']"
2,Exactness of the Tensor Functor,Exactness of the Tensor Functor,,"This might turn out to be a very stupid question, so I apologize in advance, but it is confusing me a little bit. I know in general that if $$M'\rightarrow M\rightarrow M''\rightarrow 0$$ is an exact sequence of $A-$modules, then $$M'\otimes_AN\rightarrow M\otimes_AN \rightarrow M''\otimes_AN \rightarrow 0$$ is an exact sequence for any $A-$module $N$. If instead we are given an exact sequence of the form:  $$M'''\rightarrow M'\rightarrow M\rightarrow M''\rightarrow 0$$ where $M'''$ is not the zero module (because I know that tensoring with a module doesn't necessarily take injective maps to injective maps), then I guess my question is why isn't the following an exact sequence as well? $$M'''\otimes_A N\rightarrow M'\otimes_AN\rightarrow M\otimes_AN \rightarrow M''\otimes_AN \rightarrow 0$$ What is going morally wrong here? I guess i'm wondering where exactly is the proof of the first case failing to show that the second sequence is exact at $M'\otimes_AN$? I would appreciate very much if anyone could clarify my confusion. I am probably just being stupid about this :/","This might turn out to be a very stupid question, so I apologize in advance, but it is confusing me a little bit. I know in general that if $$M'\rightarrow M\rightarrow M''\rightarrow 0$$ is an exact sequence of $A-$modules, then $$M'\otimes_AN\rightarrow M\otimes_AN \rightarrow M''\otimes_AN \rightarrow 0$$ is an exact sequence for any $A-$module $N$. If instead we are given an exact sequence of the form:  $$M'''\rightarrow M'\rightarrow M\rightarrow M''\rightarrow 0$$ where $M'''$ is not the zero module (because I know that tensoring with a module doesn't necessarily take injective maps to injective maps), then I guess my question is why isn't the following an exact sequence as well? $$M'''\otimes_A N\rightarrow M'\otimes_AN\rightarrow M\otimes_AN \rightarrow M''\otimes_AN \rightarrow 0$$ What is going morally wrong here? I guess i'm wondering where exactly is the proof of the first case failing to show that the second sequence is exact at $M'\otimes_AN$? I would appreciate very much if anyone could clarify my confusion. I am probably just being stupid about this :/",,"['abstract-algebra', 'homological-algebra', 'tensor-products']"
3,Testing whether an element of a tensor product of modules is zero,Testing whether an element of a tensor product of modules is zero,,"Let $A$ be a commutative ring and let $F$, $E$ be $A$-modules. Then $E$ is the direct limit of its finitely generated submodules. Serge Lang, in his Algebra, p. 604, remarks that a technique to test whether an element of $F \otimes E$ is zero, is to examine its image in $F \otimes N$, as $N$ varies over the finitely generated submodules of $E$. Could somebody please explain why this is a valid technique? What is the image of an element of $F \otimes E$ in $F \otimes N$?","Let $A$ be a commutative ring and let $F$, $E$ be $A$-modules. Then $E$ is the direct limit of its finitely generated submodules. Serge Lang, in his Algebra, p. 604, remarks that a technique to test whether an element of $F \otimes E$ is zero, is to examine its image in $F \otimes N$, as $N$ varies over the finitely generated submodules of $E$. Could somebody please explain why this is a valid technique? What is the image of an element of $F \otimes E$ in $F \otimes N$?",,"['abstract-algebra', 'tensor-products']"
4,Showing that $R(x)$ is a proper subset of $R((x))$ if $R$ is a field,Showing that  is a proper subset of  if  is a field,R(x) R((x)) R,"I would like to show that if $R$ is a field, then $R(x)$ is a proper subset of $R((x))$, where $R(x)$ is the ring of rational functions, and $R((x))$ is the ring of formal Laurent series. If $f \in R(x)$, then $f(x) = f_1(x)f_2^{-1}(x)$, where $f_1(x), f_2(x) \in R[x]$. So I wrote this as $$f(x) = \frac{\sum_{i=0}^{n}a_ix^i}{\sum_{j=0}^{m}b_jx^j}\;,$$ and I would like to show that I can write $f$ in the form $\sum_{k=r}^{\infty}c_kx^k$. However, I am unsure how to manipulate $f$ in order to show this. What I was thinking was to find some formal power series expansion for $f_2^{-1}(x)$, multiply out the summation with $f_1(x)$, then rearrange the coefficients and terms to obtain the desired form. However, I can't seem to derive a formula for the inverse of a polynomial in general that I could use for this. How can I go about manipulating $f_2^{-1}(x)$ to show this? Any suggestions? Thanks!","I would like to show that if $R$ is a field, then $R(x)$ is a proper subset of $R((x))$, where $R(x)$ is the ring of rational functions, and $R((x))$ is the ring of formal Laurent series. If $f \in R(x)$, then $f(x) = f_1(x)f_2^{-1}(x)$, where $f_1(x), f_2(x) \in R[x]$. So I wrote this as $$f(x) = \frac{\sum_{i=0}^{n}a_ix^i}{\sum_{j=0}^{m}b_jx^j}\;,$$ and I would like to show that I can write $f$ in the form $\sum_{k=r}^{\infty}c_kx^k$. However, I am unsure how to manipulate $f$ in order to show this. What I was thinking was to find some formal power series expansion for $f_2^{-1}(x)$, multiply out the summation with $f_1(x)$, then rearrange the coefficients and terms to obtain the desired form. However, I can't seem to derive a formula for the inverse of a polynomial in general that I could use for this. How can I go about manipulating $f_2^{-1}(x)$ to show this? Any suggestions? Thanks!",,"['abstract-algebra', 'field-theory', 'power-series']"
5,Ideal as a projective module,Ideal as a projective module,,"I am sorry, this may not be a good question here. I am looking a good reference about when the ideal $I$ of a given commutative ring $R$ (local or may not be local) with identity is a projective module.","I am sorry, this may not be a good question here. I am looking a good reference about when the ideal $I$ of a given commutative ring $R$ (local or may not be local) with identity is a projective module.",,"['abstract-algebra', 'reference-request', 'commutative-algebra', 'ring-theory', 'modules']"
6,Field Extension,Field Extension,,"Let $F$ be an extension field of $K$. let $L$ and $M$ be intermediate fields,  with both finite algebraic extensions of $K$.   Suppose {$a_1,...,a_n$} is a basis for $L$ over $K$ and {$b_1, ...,b_m$} is a basis for $M$ over $K$. Show that {$a_ib_j$} is a spanning set for the field $LM$ ($LM$ is the smallest field between $K$ and $F$ containing both $L$ and $M$) as a vector space over $K$. What I have done so far is this: Let $x$ $\in$ $L$. Then $x$ can be written as a linear combination of $a_i$. Also $y$ $\in$ $M$ implies that $y$ can be written as a linear combination of $b_j$. This is where I'm stuck.","Let $F$ be an extension field of $K$. let $L$ and $M$ be intermediate fields,  with both finite algebraic extensions of $K$.   Suppose {$a_1,...,a_n$} is a basis for $L$ over $K$ and {$b_1, ...,b_m$} is a basis for $M$ over $K$. Show that {$a_ib_j$} is a spanning set for the field $LM$ ($LM$ is the smallest field between $K$ and $F$ containing both $L$ and $M$) as a vector space over $K$. What I have done so far is this: Let $x$ $\in$ $L$. Then $x$ can be written as a linear combination of $a_i$. Also $y$ $\in$ $M$ implies that $y$ can be written as a linear combination of $b_j$. This is where I'm stuck.",,['abstract-algebra']
7,Is $\Bbb Z^3$ a one-relator group?,Is  a one-relator group?,\Bbb Z^3,"I understand that: $\Bbb Z^0 = \langle a  \mid a \rangle$ $\Bbb Z^1 = \langle a, b \mid b \rangle$ $\Bbb Z^2 = \langle a, b  \mid aba^{-1}b^{-1} \rangle$ but is it possible for $\Bbb Z^3$ to be represented with just one relator? I can’t think of a way in which it would be possible but I am also unsure how to prove it would be impossible.",I understand that: but is it possible for to be represented with just one relator? I can’t think of a way in which it would be possible but I am also unsure how to prove it would be impossible.,"\Bbb Z^0 = \langle a  \mid a \rangle \Bbb Z^1 = \langle a, b \mid b \rangle \Bbb Z^2 = \langle a, b  \mid aba^{-1}b^{-1} \rangle \Bbb Z^3","['abstract-algebra', 'group-theory', 'free-groups', 'group-presentation', 'combinatorial-group-theory']"
8,Is there a surjective morphism from an infinite direct product of copies of $\mathbb{Z}$ to an infinite direct sum of copies of $\mathbb{Z}$?,Is there a surjective morphism from an infinite direct product of copies of  to an infinite direct sum of copies of ?,\mathbb{Z} \mathbb{Z},"Is there a surjective morphism $\mathbb{Z}^I\to \mathbb{Z}^{(J)}$ for some $I,J$ ? i) I'm asking about group morphisms ii) $\mathbb{Z}^{(J)}$ denotes the direct sum of $J$ copies of $\mathbb Z$ iii) Of course, I want $J$ , and thus $I$ to be infinite, otherwise it's trivial. I want to know if there is such a surjection for any $J$ , so it suffices to find one $J$ with no such surjection and it'll be done. However it seems that the answer won't really depend on $J$ . Thoughts : I think there is no surjective morphism, and so was trying to find a contradiction. Of course such a morphism is split, so I was trying to analyze summands of $\mathbb{Z}^I$ but got essentially nowhere.  I also tried studying maps $\mathbb{Z}^I \to \mathbb{Z}$ , to study the projections. I know that if such a map vanishes on almost zero sequences, then it vanishes, so I was trying to prove that it would do that here, but with no success. I don't really know how to proceed further. EDIT: the link that was given in the comments can help. Indeed, if there were such a surjection, then by passing to hom's into $\mathbb{Z}$ , we would have an injection $\mathbb{Z}^J \to \hom (\mathbb{Z}^I, \mathbb{Z})$ . Now I don't know how general the quoted Baer's result is, but if it generalizes to any exponent, then this would provide an injection $\mathbb{Z}^J\to \mathbb{Z}^{(I)}$ , which is clearly contradictory, by looking for instance at ""almost"" $2^\infty$ -divisible elements of $\mathbb{Z}^J$ (if this isn't clear I can sketch the proof here, but it's not that complicated) Alternatively, if Baer's result isn't that general, it might be possible to use some trickery to reduce to $I= \mathbb{N}$ , at least when assuming that $J=\mathbb{N}$ .","Is there a surjective morphism for some ? i) I'm asking about group morphisms ii) denotes the direct sum of copies of iii) Of course, I want , and thus to be infinite, otherwise it's trivial. I want to know if there is such a surjection for any , so it suffices to find one with no such surjection and it'll be done. However it seems that the answer won't really depend on . Thoughts : I think there is no surjective morphism, and so was trying to find a contradiction. Of course such a morphism is split, so I was trying to analyze summands of but got essentially nowhere.  I also tried studying maps , to study the projections. I know that if such a map vanishes on almost zero sequences, then it vanishes, so I was trying to prove that it would do that here, but with no success. I don't really know how to proceed further. EDIT: the link that was given in the comments can help. Indeed, if there were such a surjection, then by passing to hom's into , we would have an injection . Now I don't know how general the quoted Baer's result is, but if it generalizes to any exponent, then this would provide an injection , which is clearly contradictory, by looking for instance at ""almost"" -divisible elements of (if this isn't clear I can sketch the proof here, but it's not that complicated) Alternatively, if Baer's result isn't that general, it might be possible to use some trickery to reduce to , at least when assuming that .","\mathbb{Z}^I\to \mathbb{Z}^{(J)} I,J \mathbb{Z}^{(J)} J \mathbb Z J I J J J \mathbb{Z}^I \mathbb{Z}^I \to \mathbb{Z} \mathbb{Z} \mathbb{Z}^J \to \hom (\mathbb{Z}^I, \mathbb{Z}) \mathbb{Z}^J\to \mathbb{Z}^{(I)} 2^\infty \mathbb{Z}^J I= \mathbb{N} J=\mathbb{N}","['abstract-algebra', 'group-theory', 'abelian-groups']"
9,Does $A\oplus \mathbb{Z}\cong B\oplus \mathbb{Z}$ imply $A\cong B$?,Does  imply ?,A\oplus \mathbb{Z}\cong B\oplus \mathbb{Z} A\cong B,"If $A$ and $B$ are abelian groups, do we have that $A\oplus \mathbb{Z}\cong B\oplus \mathbb{Z}$ implies $A\cong B$? Motivation: I was just thinking about different ways of deducing equality from expressions by quotienting, then realized I didn't know the answer in this case.","If $A$ and $B$ are abelian groups, do we have that $A\oplus \mathbb{Z}\cong B\oplus \mathbb{Z}$ implies $A\cong B$? Motivation: I was just thinking about different ways of deducing equality from expressions by quotienting, then realized I didn't know the answer in this case.",,"['abstract-algebra', 'group-theory', 'abelian-groups', 'direct-sum']"
10,Torsion-free but not free,Torsion-free but not free,,"The question is asking me to give an example of a finitely generated $R$-module that is torsion-free but not free. I remember in lecture, lecturer say something about the ideal $(2,X)$ in $\mathbb{Z}[X]$considered as a $\mathbb{Z}[X]$-module is torsion-free but not free. But I don't know why, what I know about these guys are: $(2,X)$ is the ideal that contain all polynomial with even constant $(2,X)$ is not principle ideal, so $\mathbb{Z}[X]$ is not a PID I am not sure will these facts help to answer to question.","The question is asking me to give an example of a finitely generated $R$-module that is torsion-free but not free. I remember in lecture, lecturer say something about the ideal $(2,X)$ in $\mathbb{Z}[X]$considered as a $\mathbb{Z}[X]$-module is torsion-free but not free. But I don't know why, what I know about these guys are: $(2,X)$ is the ideal that contain all polynomial with even constant $(2,X)$ is not principle ideal, so $\mathbb{Z}[X]$ is not a PID I am not sure will these facts help to answer to question.",,"['abstract-algebra', 'modules']"
11,Factoring the ideal $(8)$ into a product of prime ideals in $\mathbb{Q}(\sqrt{-7})$,Factoring the ideal  into a product of prime ideals in,(8) \mathbb{Q}(\sqrt{-7}),"I am trying to factor the ideal $(8)$ into a product of prime ideals in $\mathbb{Q}(\sqrt{-7})$. I am not exactly sure how to go about doing this, and I feel I am missing some theory in the matter. What I have noted is that we have $$8=(1+\sqrt{-7})(1-\sqrt{-7})=2^3$$ and so I surmise the factorization will involve these numbers somehow. I also noted that $N(8)=64$, so the prime ideal factors will have norms that multiply out to $64.$ Unfortunately I cannot proceed further than this. Any hints or references are appreciated.","I am trying to factor the ideal $(8)$ into a product of prime ideals in $\mathbb{Q}(\sqrt{-7})$. I am not exactly sure how to go about doing this, and I feel I am missing some theory in the matter. What I have noted is that we have $$8=(1+\sqrt{-7})(1-\sqrt{-7})=2^3$$ and so I surmise the factorization will involve these numbers somehow. I also noted that $N(8)=64$, so the prime ideal factors will have norms that multiply out to $64.$ Unfortunately I cannot proceed further than this. Any hints or references are appreciated.",,"['abstract-algebra', 'algebraic-number-theory', 'ideals', 'factoring']"
12,Find the number of homomorphisms between cyclic groups.,Find the number of homomorphisms between cyclic groups.,,"In each of the following examples determine the number of homomorphisms between the given groups: $(a)$ from $\mathbb{Z}$ to $\mathbb{Z}_{10}$ ; $(b)$ from $\mathbb{Z}_{10}$ to $\mathbb{Z}_{10}$ ; $(c)$ from $\mathbb{Z}_{8}$ to $\mathbb{Z}_{10}$ . Could anyone just give me hints for the problem? Well, let $f:\mathbb{Z}\rightarrow \mathbb{Z}_{10}$ be homo, then $f(1)=[n]$ for any $[n]\in \mathbb{Z}_{10}$ will give a homomorphism hence there are $10$ for (a)?","In each of the following examples determine the number of homomorphisms between the given groups: from to ; from to ; from to . Could anyone just give me hints for the problem? Well, let be homo, then for any will give a homomorphism hence there are for (a)?",(a) \mathbb{Z} \mathbb{Z}_{10} (b) \mathbb{Z}_{10} \mathbb{Z}_{10} (c) \mathbb{Z}_{8} \mathbb{Z}_{10} f:\mathbb{Z}\rightarrow \mathbb{Z}_{10} f(1)=[n] [n]\in \mathbb{Z}_{10} 10,"['abstract-algebra', 'group-theory', 'finite-groups', 'cyclic-groups']"
13,Example of nonabelian group with all proper subgroups normal,Example of nonabelian group with all proper subgroups normal,,How do we find an example of nonabelian group for which all proper subgroups are normal?? It's one of the questions on my study-guide sheet.  Thank you,How do we find an example of nonabelian group for which all proper subgroups are normal?? It's one of the questions on my study-guide sheet.  Thank you,,['abstract-algebra']
14,Can we always find a primitive element that is a square?,Can we always find a primitive element that is a square?,,Let $L/\mathbb Q$ be a finite field extension. The Primitive Element Theorem says that there is an element $\alpha \in L$ so that $L=\mathbb Q(\alpha)$. Can I always find an element $\beta \in L$ so that $L=\mathbb Q(\beta^2)$ ?,Let $L/\mathbb Q$ be a finite field extension. The Primitive Element Theorem says that there is an element $\alpha \in L$ so that $L=\mathbb Q(\alpha)$. Can I always find an element $\beta \in L$ so that $L=\mathbb Q(\beta^2)$ ?,,"['abstract-algebra', 'field-theory', 'extension-field']"
15,Is a completion of an algebraically closed field with respect to a norm also algebraically closed?,Is a completion of an algebraically closed field with respect to a norm also algebraically closed?,,"Assume we have an algebraically closed field $F$ with a norm (where $F$ is considered as a vector space over itself), so that $F$ is not complete as a normed space. Let $\overline F$ be its completion with respect to the norm. Is $\overline F$ necessarily algebraically closed? Thanks.","Assume we have an algebraically closed field $F$ with a norm (where $F$ is considered as a vector space over itself), so that $F$ is not complete as a normed space. Let $\overline F$ be its completion with respect to the norm. Is $\overline F$ necessarily algebraically closed? Thanks.",,"['abstract-algebra', 'field-theory', 'normed-spaces', 'banach-spaces']"
16,Elementary proof that there is no field with 6 elements,Elementary proof that there is no field with 6 elements,,"I am a T.A at an introductory linear algebra course this semester, and before getting into vector spaces, we give them some basic examples and properties of fields. Since this is a first semester course, everything is very elementary and we do not have too many tools. I'd be happy to give them some elementary proof that there is no field of size $6$ (or some other small, concrete number, like 10... I just do not really care about the general case). They already know that $\mathbb Z _6$ is not a field, but I want to show that there is no other possible field of this size. By elementary, I mean something which can be taught to math students on the second week to their first semester (so no arguments using groups and such). I can start talking about characteristics of fields and then show that the size has to be some power of the characteristic, but I'd prefer not to get into all of this. I am simply wondering if anyone here knows of an elementary proof for some concrete size (like $6$ ). Thank in advance!","I am a T.A at an introductory linear algebra course this semester, and before getting into vector spaces, we give them some basic examples and properties of fields. Since this is a first semester course, everything is very elementary and we do not have too many tools. I'd be happy to give them some elementary proof that there is no field of size (or some other small, concrete number, like 10... I just do not really care about the general case). They already know that is not a field, but I want to show that there is no other possible field of this size. By elementary, I mean something which can be taught to math students on the second week to their first semester (so no arguments using groups and such). I can start talking about characteristics of fields and then show that the size has to be some power of the characteristic, but I'd prefer not to get into all of this. I am simply wondering if anyone here knows of an elementary proof for some concrete size (like ). Thank in advance!",6 \mathbb Z _6 6,"['abstract-algebra', 'field-theory', 'finite-fields', 'alternative-proof']"
17,When is a class function the character of a representation?,When is a class function the character of a representation?,,"The representations of a finite group can be understood by their irreducible characters. A class function is a function from the group to the complex numbers that is constant on the conjugacy classes. I know that any linear combination of the irreducible characters is the character of some representation. I also know that not all class functions are characters of a representation. Let's say that I don't know all the irreducible characters of a group, but I come across a class function whose inner product with itself is 1. My question is: How do I know whether this function is actually the character of an irreducible representation? More generally: How do I know whether a given class function is the character of some representation of a group without knowing all the irreducible representations? EDIT: I see this question with answers: Class function as a character . This almost answers my question. To clarify what I am specifically interested in knowing, if I have found some irreducible representations of a group $G$ . Say I have $\chi_1, \dots, \chi_m$ . I know I haven't found all of them because I know the number of conjugacy classes. Then, say, I some other non-irreducible character $\chi$ and I know, say, that this is the character of some representation. Then I subtract a linear combination of $\chi_1, \dots, \chi_m$ , and define the class function $\psi = \chi - (a_1\chi_1 + \dots + a_m\chi_m)$ . How do I know whether this $\psi$ is the character of some representation?","The representations of a finite group can be understood by their irreducible characters. A class function is a function from the group to the complex numbers that is constant on the conjugacy classes. I know that any linear combination of the irreducible characters is the character of some representation. I also know that not all class functions are characters of a representation. Let's say that I don't know all the irreducible characters of a group, but I come across a class function whose inner product with itself is 1. My question is: How do I know whether this function is actually the character of an irreducible representation? More generally: How do I know whether a given class function is the character of some representation of a group without knowing all the irreducible representations? EDIT: I see this question with answers: Class function as a character . This almost answers my question. To clarify what I am specifically interested in knowing, if I have found some irreducible representations of a group . Say I have . I know I haven't found all of them because I know the number of conjugacy classes. Then, say, I some other non-irreducible character and I know, say, that this is the character of some representation. Then I subtract a linear combination of , and define the class function . How do I know whether this is the character of some representation?","G \chi_1, \dots, \chi_m \chi \chi_1, \dots, \chi_m \psi = \chi - (a_1\chi_1 + \dots + a_m\chi_m) \psi","['abstract-algebra', 'group-theory', 'finite-groups', 'representation-theory']"
18,Difference between $\mathbb{Z}/n\mathbb{Z}$ and $\mathbb{Z}_n$,Difference between  and,\mathbb{Z}/n\mathbb{Z} \mathbb{Z}_n,"In every Modern Algebra book I've read, I've seen that the groups $\mathbb{Z}/n\mathbb{Z}$ and $\mathbb{Z}_n$ are isomorphic, but not equal. I understand the difference between ""isomorphic"" and ""equal,"" but this particular example raises a couple of questions for me. I know the first group consists of cosets $k + n\mathbb{Z}$, and the second group consists of equivalence classes $[k]_n$, but isn't it true that $k + n\mathbb{Z} = [k]_n$ for every integer $k$? (They are both sets containing the same elements from $\mathbb{Z}$.) And if so, then can't we say that $\mathbb{Z}/n\mathbb{Z} = \mathbb{Z}_n$? Thanks in advance!","In every Modern Algebra book I've read, I've seen that the groups $\mathbb{Z}/n\mathbb{Z}$ and $\mathbb{Z}_n$ are isomorphic, but not equal. I understand the difference between ""isomorphic"" and ""equal,"" but this particular example raises a couple of questions for me. I know the first group consists of cosets $k + n\mathbb{Z}$, and the second group consists of equivalence classes $[k]_n$, but isn't it true that $k + n\mathbb{Z} = [k]_n$ for every integer $k$? (They are both sets containing the same elements from $\mathbb{Z}$.) And if so, then can't we say that $\mathbb{Z}/n\mathbb{Z} = \mathbb{Z}_n$? Thanks in advance!",,"['abstract-algebra', 'integers', 'group-isomorphism', 'quotient-group']"
19,"Rings with a given number of (prime, maximal) ideals","Rings with a given number of (prime, maximal) ideals",,"Given three cardinal numbers $1≤a≤b<c$ , I would like to know if we can find a commutative ring $R$ (with unit) such that the number of ideals of $R$ is $c$ the number of prime ideals of $R$ is $b$ the number of maximal ideals of $R$ is $a$ For instance, for $a=b=c=\aleph_0$ , the ring $R=\Bbb Z$ provides an example. I'm particularly interested in the case where $a,b,c$ may be infinite. If $c$ is finite, then either $a=b=1$ and $c=2$ or $R$ is not an integral domain (see here ). In any finite ring, prime ideals are maximal (because a finite integral domain is a field, see here ), so that possible examples for $a \neq b$ must be infinite. If $c=3$ , then $a=1$ (see here ). If $a=b$ and $c=2^a$ is finite, then I think that we can consider $R=\Bbb Z/p_1 \cdots p_a \Bbb Z$ where $p_1, \dots, p_a$ are distinct primes. I wasn't able to solve the case $c=5,a=b=3$ for instance. Maybe there are conditions on $a,b,c$ for such a ring to exist, at least when $c$ is finite. Possibly related: (1) , (2) , (3) . Thank you very much for your help!","Given three cardinal numbers , I would like to know if we can find a commutative ring (with unit) such that the number of ideals of is the number of prime ideals of is the number of maximal ideals of is For instance, for , the ring provides an example. I'm particularly interested in the case where may be infinite. If is finite, then either and or is not an integral domain (see here ). In any finite ring, prime ideals are maximal (because a finite integral domain is a field, see here ), so that possible examples for must be infinite. If , then (see here ). If and is finite, then I think that we can consider where are distinct primes. I wasn't able to solve the case for instance. Maybe there are conditions on for such a ring to exist, at least when is finite. Possibly related: (1) , (2) , (3) . Thank you very much for your help!","1≤a≤b<c R R c R b R a a=b=c=\aleph_0 R=\Bbb Z a,b,c c a=b=1 c=2 R a \neq b c=3 a=1 a=b c=2^a R=\Bbb Z/p_1 \cdots p_a \Bbb Z p_1, \dots, p_a c=5,a=b=3 a,b,c c","['abstract-algebra', 'ring-theory', 'commutative-algebra', 'ideals']"
20,Why doesn't Fermat's Last Theorem for polynomials entail that for integers?,Why doesn't Fermat's Last Theorem for polynomials entail that for integers?,,"Fermat's Last Theorem for polynomials follows from the Stothers-Mason theorem, that is: For any integer $n\geq 3$, there do not exist polynomials $x(t), y(t), z(t)$ not all constant such that   $x(t)^n + y(t)^n = z(t)^n$ for all $t\neq 0$. But since we can always find suitable polynomials in $t$ such that $(x(t), y(t), z(t)) = (a,b,c)$, why can't FLT for polynomials entail that for integers ? For example, suppose we had $7^n + 8^n = 15^n$ for some integer $n\geq 3$, we would have $t=2$ such that $(7, 8, 15) = (3t+1, 4t, 8t-1)$, which is impossible by FLT for polynomials ?","Fermat's Last Theorem for polynomials follows from the Stothers-Mason theorem, that is: For any integer $n\geq 3$, there do not exist polynomials $x(t), y(t), z(t)$ not all constant such that   $x(t)^n + y(t)^n = z(t)^n$ for all $t\neq 0$. But since we can always find suitable polynomials in $t$ such that $(x(t), y(t), z(t)) = (a,b,c)$, why can't FLT for polynomials entail that for integers ? For example, suppose we had $7^n + 8^n = 15^n$ for some integer $n\geq 3$, we would have $t=2$ such that $(7, 8, 15) = (3t+1, 4t, 8t-1)$, which is impossible by FLT for polynomials ?",,"['abstract-algebra', 'number-theory']"
21,Show that $x^{n-1}+\cdots +x+1$ is irreducible over $\mathbb Z$ if and only if $n$ is a prime.,Show that  is irreducible over  if and only if  is a prime.,x^{n-1}+\cdots +x+1 \mathbb Z n,"I proved that if $n$ is a prime, then $p(x)=x^{n-1}+\cdots+x+1$ is irreducible over $\mathbb Z$. But, I don't know how to prove that if $p(x)$ is irreducible over $\mathbb Z$, then $n$ is prime. Can you give me a hints?","I proved that if $n$ is a prime, then $p(x)=x^{n-1}+\cdots+x+1$ is irreducible over $\mathbb Z$. But, I don't know how to prove that if $p(x)$ is irreducible over $\mathbb Z$, then $n$ is prime. Can you give me a hints?",,"['abstract-algebra', 'polynomials', 'ring-theory', 'factoring', 'irreducible-polynomials']"
22,"Smallest example of a group that is not isomorphic to a cyclic group, a direct product of cyclic groups or a semi direct product of cyclic groups.","Smallest example of a group that is not isomorphic to a cyclic group, a direct product of cyclic groups or a semi direct product of cyclic groups.",,"What is the smallest example of a group that is not isomorphic to a cyclic group, a direct product of cyclic groups or a semi-direct product of cyclic groups? So finite abelian groups are ruled out, as are the groups up to order $7$ I believe, since $1,2,3$ are all isomorphic to cyclic groups, $4$ has two non-isomorphic groups one cyclic and one isomorphic to the direct product $\Bbb{Z/2Z\times Z/2Z}$. $5,7$ are prime and so must be cyclic, and there's only one nonabelian group of order $6$ which is isomorphic to $\Bbb{Z/3Z\rtimes Z/2Z}$. Is it the quaternion group?","What is the smallest example of a group that is not isomorphic to a cyclic group, a direct product of cyclic groups or a semi-direct product of cyclic groups? So finite abelian groups are ruled out, as are the groups up to order $7$ I believe, since $1,2,3$ are all isomorphic to cyclic groups, $4$ has two non-isomorphic groups one cyclic and one isomorphic to the direct product $\Bbb{Z/2Z\times Z/2Z}$. $5,7$ are prime and so must be cyclic, and there's only one nonabelian group of order $6$ which is isomorphic to $\Bbb{Z/3Z\rtimes Z/2Z}$. Is it the quaternion group?",,"['abstract-algebra', 'group-theory', 'finite-groups']"
23,"In $\mathbb{Z}/(n)$, does $(a) = (b)$ imply that $a$ and $b$ are associates?","In , does  imply that  and  are associates?",\mathbb{Z}/(n) (a) = (b) a b,"[ Update : Based on the hints provided by @zcn and @whacka, I believe I have found a solution. See my answer below.] Below, $R$ is a commutative ring with $1$. In John J. Watkins' Topics in Commutative Ring Theory , the author observes that if $a$ and $b$ are associates in $R$, then they generate the same ideal: $(a) = (b)$. This is clearly true, because $a = ub$ and $b = va$ for some units $u$ and $v$, and so $a \in (b)$ and $b \in (a)$, whence $(a) \subset (b)$ and $(b) \subset (a)$. Indeed, we didn't even require $u$ and $v$ to be units to reach this conclusion. The author then claims that if $R$ is not a domain, then the converse may not be true: we may have $(a) = (b)$ even if $a$ and $b$ are not associates. As a ""counterexample"" he gives $R = \mathbb{Z}/(6)$, with $a = 2 + (6)$ and $b = 4 + (6)$. Clearly these elements generate the same ideal: $(a) = (b) = \{0, 2 + (6), 4 + (6)\}$. The author asserts that $a$ and $b$ are not associates, but this is false: $5 + (6)$ is a unit, and $(2 + (6))(5 + (6)) = 4 + (6)$, so $a$ and $b$ actually are associates. I figured that the author botched the example but that surely the basic fact must be true: if $R = \mathbb{Z}/(n)$ then there may be nonassociates $a,b \in R$ with $(a) = (b)$. However, I was not able to find a counterexample and eventually resorted to writing a computer program and found that there are no counterexamples in $\mathbb{Z}/(n)$ for $n \leq 100$. So now I'm starting to think that there is no counterexample in $\mathbb{Z}/(n)$ at all, but I have not yet been able to prove this. My questions: If I am correct that $(a) = (b)$ in $\mathbb{Z}/(n)$ implies that $a$ and $b$ are associates, I would appreciate a nudge in the right direction toward a proof. (But if the proof is elementary, please give only a hint, not a solution :-) I'll post my solution once I have one. In either case, could you please point me to a genuine counterexample? If there is one in $\mathbb{Z}/(n)$ for some $n$, that would be ideal (no pun intended), otherwise a counterexample in any commutative ring with $1$ would be great.","[ Update : Based on the hints provided by @zcn and @whacka, I believe I have found a solution. See my answer below.] Below, $R$ is a commutative ring with $1$. In John J. Watkins' Topics in Commutative Ring Theory , the author observes that if $a$ and $b$ are associates in $R$, then they generate the same ideal: $(a) = (b)$. This is clearly true, because $a = ub$ and $b = va$ for some units $u$ and $v$, and so $a \in (b)$ and $b \in (a)$, whence $(a) \subset (b)$ and $(b) \subset (a)$. Indeed, we didn't even require $u$ and $v$ to be units to reach this conclusion. The author then claims that if $R$ is not a domain, then the converse may not be true: we may have $(a) = (b)$ even if $a$ and $b$ are not associates. As a ""counterexample"" he gives $R = \mathbb{Z}/(6)$, with $a = 2 + (6)$ and $b = 4 + (6)$. Clearly these elements generate the same ideal: $(a) = (b) = \{0, 2 + (6), 4 + (6)\}$. The author asserts that $a$ and $b$ are not associates, but this is false: $5 + (6)$ is a unit, and $(2 + (6))(5 + (6)) = 4 + (6)$, so $a$ and $b$ actually are associates. I figured that the author botched the example but that surely the basic fact must be true: if $R = \mathbb{Z}/(n)$ then there may be nonassociates $a,b \in R$ with $(a) = (b)$. However, I was not able to find a counterexample and eventually resorted to writing a computer program and found that there are no counterexamples in $\mathbb{Z}/(n)$ for $n \leq 100$. So now I'm starting to think that there is no counterexample in $\mathbb{Z}/(n)$ at all, but I have not yet been able to prove this. My questions: If I am correct that $(a) = (b)$ in $\mathbb{Z}/(n)$ implies that $a$ and $b$ are associates, I would appreciate a nudge in the right direction toward a proof. (But if the proof is elementary, please give only a hint, not a solution :-) I'll post my solution once I have one. In either case, could you please point me to a genuine counterexample? If there is one in $\mathbb{Z}/(n)$ for some $n$, that would be ideal (no pun intended), otherwise a counterexample in any commutative ring with $1$ would be great.",,"['abstract-algebra', 'ring-theory']"
24,categorical generalizations of familiar objects,categorical generalizations of familiar objects,,"A couple of days ago I've learned that you can define trace in a very abstract setting. Namely, suppose $F\colon A\to B$ is a functor between two categories. Suppose $E,G\colon B\to A$ are two functors, that are adjoint to $F$, such that $E$ is left adjoint and $G$ is right adjoint. Suppose moreover that $\nu\colon G\to E$ is a natural transformation. Then having this data $\forall x,y\in A$ you can define the trace map $$Tr\colon Hom(F(x),F(y))\to Hom(x,y)$$ for any $f\in Hom(F(x),F(y))$ to be the composition $$x\to GF(x)\to EF(x)\to EF(y)\to y,$$ where the first map is given by the adjunction unit $id_A\to GF$, second map is given by $\nu$, the third map is given by $E(f)$ and the last map is given by the adjunction counit $EF\to id_A$. If we take $A=B=Vect_k$, $F=-\otimes V$ and $G=E=-\otimes V^*$ for some finite dimension vector space $V$ over $k$, and take $x=y=k$, then this construction gives the usual trace of a linear map. From the top of my head I remembered a couple of more examples. For example, if $C$ is a category, the Bernstein center of $C$ is defined to be $Z(C)=End(id_C)$, the (commutative monoid) of endomorphism of the identity functor. If the category $C$ is additive, then $Z(C)$ is a commutative ring. If we take $C$ to be the category of modules over some ring $R$, then $Z(C)$ is isomorphic to the center of $R$. Another one, you can talk about open, closed subfunctors on the category of commutative rings into $Sets$. If your functors are representable, then you get the usual definition of open or closed subschemes. So my question is: What are other nice examples of such categorical generalizations that you now?","A couple of days ago I've learned that you can define trace in a very abstract setting. Namely, suppose $F\colon A\to B$ is a functor between two categories. Suppose $E,G\colon B\to A$ are two functors, that are adjoint to $F$, such that $E$ is left adjoint and $G$ is right adjoint. Suppose moreover that $\nu\colon G\to E$ is a natural transformation. Then having this data $\forall x,y\in A$ you can define the trace map $$Tr\colon Hom(F(x),F(y))\to Hom(x,y)$$ for any $f\in Hom(F(x),F(y))$ to be the composition $$x\to GF(x)\to EF(x)\to EF(y)\to y,$$ where the first map is given by the adjunction unit $id_A\to GF$, second map is given by $\nu$, the third map is given by $E(f)$ and the last map is given by the adjunction counit $EF\to id_A$. If we take $A=B=Vect_k$, $F=-\otimes V$ and $G=E=-\otimes V^*$ for some finite dimension vector space $V$ over $k$, and take $x=y=k$, then this construction gives the usual trace of a linear map. From the top of my head I remembered a couple of more examples. For example, if $C$ is a category, the Bernstein center of $C$ is defined to be $Z(C)=End(id_C)$, the (commutative monoid) of endomorphism of the identity functor. If the category $C$ is additive, then $Z(C)$ is a commutative ring. If we take $C$ to be the category of modules over some ring $R$, then $Z(C)$ is isomorphic to the center of $R$. Another one, you can talk about open, closed subfunctors on the category of commutative rings into $Sets$. If your functors are representable, then you get the usual definition of open or closed subschemes. So my question is: What are other nice examples of such categorical generalizations that you now?",,"['abstract-algebra', 'soft-question', 'category-theory', 'big-list']"
25,Units (invertibles) of polynomial rings over a field,Units (invertibles) of polynomial rings over a field,,"If $R$ is a field, what are the units of $R[X]$? My attempt: Let $f,g \in R[X]$ and $f(X)g(X)=1$. Then the only solution for the equation is both $f,g \in {R}$. So $U(R[X])=R$, exclude zero elements of $R$. Is this correct ?","If $R$ is a field, what are the units of $R[X]$? My attempt: Let $f,g \in R[X]$ and $f(X)g(X)=1$. Then the only solution for the equation is both $f,g \in {R}$. So $U(R[X])=R$, exclude zero elements of $R$. Is this correct ?",,"['abstract-algebra', 'polynomials']"
26,Equivalent characterizations of faithfully exact functors of abelian categories,Equivalent characterizations of faithfully exact functors of abelian categories,,"Let $F\colon \mathcal{A} \rightarrow \mathcal{B}$ be a functor of abelian categories. We will define some properties of $F$ before we state a question. Let $X \rightarrow Y \rightarrow Z$ be a sequence in $\mathcal{A}$. We say $F$ reflects exact sequences if $X \rightarrow Y \rightarrow Z$ is exact whenever $F(X) \rightarrow F(Y) \rightarrow F(Z)$ is exact. We say $F$ is faithfully exact if it is exact and reflects exact sequences. Let $X$ be an object of $\mathcal{A}$. We say $F$ reflects zero objects if $X = 0$ whenever $F(X) = 0$. Let $f\colon X \rightarrow Y$ be a morphism of $\mathcal{A}$. We say $F$ reflects zero morphisms if $f = 0$ whenever $F(f) = 0$. Question. Let $F\colon \mathcal{A} \rightarrow \mathcal{B}$ be an exact functor of abelian categories. Are the following conditions equivalent? (1) $F$ reflects exact sequences, i.e., $F$ is faithfully exact. (2) Let $f\colon X \rightarrow Y$ and $g\colon Y \rightarrow Z$ be morphisms of $\mathcal{A}$ such that $gf = 0$. Suppose $F(X) \rightarrow F(Y) \rightarrow F(Z)$ is exact. Then $X \rightarrow Y \rightarrow Z$ is exact. (3) $F$ reflects zero objects. (4) $F$ reflects zero morphisms.","Let $F\colon \mathcal{A} \rightarrow \mathcal{B}$ be a functor of abelian categories. We will define some properties of $F$ before we state a question. Let $X \rightarrow Y \rightarrow Z$ be a sequence in $\mathcal{A}$. We say $F$ reflects exact sequences if $X \rightarrow Y \rightarrow Z$ is exact whenever $F(X) \rightarrow F(Y) \rightarrow F(Z)$ is exact. We say $F$ is faithfully exact if it is exact and reflects exact sequences. Let $X$ be an object of $\mathcal{A}$. We say $F$ reflects zero objects if $X = 0$ whenever $F(X) = 0$. Let $f\colon X \rightarrow Y$ be a morphism of $\mathcal{A}$. We say $F$ reflects zero morphisms if $f = 0$ whenever $F(f) = 0$. Question. Let $F\colon \mathcal{A} \rightarrow \mathcal{B}$ be an exact functor of abelian categories. Are the following conditions equivalent? (1) $F$ reflects exact sequences, i.e., $F$ is faithfully exact. (2) Let $f\colon X \rightarrow Y$ and $g\colon Y \rightarrow Z$ be morphisms of $\mathcal{A}$ such that $gf = 0$. Suppose $F(X) \rightarrow F(Y) \rightarrow F(Z)$ is exact. Then $X \rightarrow Y \rightarrow Z$ is exact. (3) $F$ reflects zero objects. (4) $F$ reflects zero morphisms.",,"['abstract-algebra', 'category-theory', 'abelian-categories', 'exact-sequence']"
27,"If $\phi(g)=g^3$ is a homomorphism and $3 \nmid |G|$, $G$ is abelian.","If  is a homomorphism and ,  is abelian.",\phi(g)=g^3 3 \nmid |G| G,"As the title suggests. Let $G$ be a group, and suppose the function $\phi: G \to G$ with $\phi(g)=g^3$ for $g \in G$ is a homomorphism. Show that if $3 \nmid |G|$, $G$ must be abelian. By considering $\ker(\phi)$ and Lagrange's Theorem, we have $\phi$ must be an isomorphism (right?), but I'm not really sure where to go after that. This is a problem from Alperin and Bell, and it is not for homework.","As the title suggests. Let $G$ be a group, and suppose the function $\phi: G \to G$ with $\phi(g)=g^3$ for $g \in G$ is a homomorphism. Show that if $3 \nmid |G|$, $G$ must be abelian. By considering $\ker(\phi)$ and Lagrange's Theorem, we have $\phi$ must be an isomorphism (right?), but I'm not really sure where to go after that. This is a problem from Alperin and Bell, and it is not for homework.",,"['abstract-algebra', 'group-theory']"
28,Field Extensions and their dimensions,Field Extensions and their dimensions,,There is a powerful theorem with respect to a field $F$ extensions and their dimensions. $F<E<K \ \Rightarrow [K:F] = [K:E][E:F] $ This is analogous to the famous Lagrange's theorem with respect to groups. Is there any relationship between these two theorems.,There is a powerful theorem with respect to a field $F$ extensions and their dimensions. $F<E<K \ \Rightarrow [K:F] = [K:E][E:F] $ This is analogous to the famous Lagrange's theorem with respect to groups. Is there any relationship between these two theorems.,,"['abstract-algebra', 'group-theory', 'category-theory', 'field-theory']"
29,Sufficiently many idempotents and commutativity,Sufficiently many idempotents and commutativity,,"It is a well-known result that if a ring $R$ satisfies $a^2=a$ for each $a\in R$, then $R$ must be commutative. See here for proof. I am wondering whether the same result holds for finite rings if we only assume sufficiently many (but not necessarily all) elements of the ring are idempotents. Recall that an element $a\in R$ is called idempotent if $a^2=a$. For example, suppose $R$ is a finite ring in which at least $80$% elements are idempotents. Can we conclude that $R$ is commutative? More generally, Does there exist an absolute constant $0<k<1$, such that whenever a finite ring $R$ satisfies   $$\frac{\textrm{Number of idempotents}}{|R|}\ge k$$ then $R$ is   commutative. Motivation: We know that if every element of a group $G$ satisfies $a^2=1$, then $G$ must be abelian. This is a relatively easy exercise. However, it turns out that we only need 75% percent of elements to satisfy $a^2=1$ in order force $G$ to be abelian. See here. For reference, $a\in G$ is called involution if $a^2=1$.","It is a well-known result that if a ring $R$ satisfies $a^2=a$ for each $a\in R$, then $R$ must be commutative. See here for proof. I am wondering whether the same result holds for finite rings if we only assume sufficiently many (but not necessarily all) elements of the ring are idempotents. Recall that an element $a\in R$ is called idempotent if $a^2=a$. For example, suppose $R$ is a finite ring in which at least $80$% elements are idempotents. Can we conclude that $R$ is commutative? More generally, Does there exist an absolute constant $0<k<1$, such that whenever a finite ring $R$ satisfies   $$\frac{\textrm{Number of idempotents}}{|R|}\ge k$$ then $R$ is   commutative. Motivation: We know that if every element of a group $G$ satisfies $a^2=1$, then $G$ must be abelian. This is a relatively easy exercise. However, it turns out that we only need 75% percent of elements to satisfy $a^2=1$ in order force $G$ to be abelian. See here. For reference, $a\in G$ is called involution if $a^2=1$.",,"['abstract-algebra', 'ring-theory', 'idempotents']"
30,If $|G|=36$ then $G$ has either a normal $2$-Sylow or a normal $3$-Sylow,If  then  has either a normal -Sylow or a normal -Sylow,|G|=36 G 2 3,"As many, I'm trying to classify all groups of order 36. I've seen many posts and in them, they claim this is true, but I can't find why. I know because of this , that $G$ is not simple. But I can't understand why the normal subgroup has to be a Sylow. I know that the number of $2$ -Sylows is either $1, 3$ or $9$ , and the number of $3$ -Sylows is either $1$ or $4$ .","As many, I'm trying to classify all groups of order 36. I've seen many posts and in them, they claim this is true, but I can't find why. I know because of this , that is not simple. But I can't understand why the normal subgroup has to be a Sylow. I know that the number of -Sylows is either or , and the number of -Sylows is either or .","G 2 1, 3 9 3 1 4","['abstract-algebra', 'group-theory', 'finite-groups', 'sylow-theory']"
31,"Prove that the augmentation ideal in the group ring $\mathbb{Z}/p\mathbb{Z}G$ is a nilpotent ideal ($p$ is a prime, $G$ is a $p$-group)","Prove that the augmentation ideal in the group ring  is a nilpotent ideal ( is a prime,  is a -group)",\mathbb{Z}/p\mathbb{Z}G p G p,"Let $p$ be a prime and let $G$ be a finite group of order a power of $p$ (i.e., a $p$-group). Prove that the augmentation ideal in the group ring $\mathbb{Z}/p\mathbb{Z}G$ (to be read as $\left( \mathbb{Z}/p\mathbb{Z} \right) G$) is a nilpotent ideal. (Note that this ring may be noncommutative.) Let  $I_G$ be the augmentation ideal of the group ring $\mathbb{Z}/p\mathbb{Z}G$, i.e. $I_G$ consists of formal linear combinations $\sum_i n_i g_i$ ($n_i\in \mathbb{Z}/p\mathbb{Z}$, $g_i\in G$) such that $\sum_i n_i=0$. I cannot show that there is $m \in \mathbb{Z}_+$ such that $I_G^m=0$.","Let $p$ be a prime and let $G$ be a finite group of order a power of $p$ (i.e., a $p$-group). Prove that the augmentation ideal in the group ring $\mathbb{Z}/p\mathbb{Z}G$ (to be read as $\left( \mathbb{Z}/p\mathbb{Z} \right) G$) is a nilpotent ideal. (Note that this ring may be noncommutative.) Let  $I_G$ be the augmentation ideal of the group ring $\mathbb{Z}/p\mathbb{Z}G$, i.e. $I_G$ consists of formal linear combinations $\sum_i n_i g_i$ ($n_i\in \mathbb{Z}/p\mathbb{Z}$, $g_i\in G$) such that $\sum_i n_i=0$. I cannot show that there is $m \in \mathbb{Z}_+$ such that $I_G^m=0$.",,"['abstract-algebra', 'ring-theory', 'finite-groups', 'representation-theory', 'group-rings']"
32,Ring of formal power series over a principal ideal domain is a unique factorisation domain,Ring of formal power series over a principal ideal domain is a unique factorisation domain,,"An exercise in my algebra course book asks to prove that if $R$ is a PID , then $R[[x]]$ is a UFD , where $R[[x]]$ is the ring of formal power series over $R$. After some failed attempts at  proving the ACC I visited Wikipedia, which comments : If $R$ is Noetherian, then so is $R[[x]]$; this is a version of the Hilbert basis theorem. This is very useful, as $R$ is a PID and hence Noetherian. Unfortunately we only saw (without proof) Hilbert's basis theorem in the form If $R$ is Noetherian, then so is $R[x]$. I'm not sure how to conclude the Noetherianity of $R[[x]]$ from this. I know that $R[x]$ is a UFD because $R$ is (we saw this without proof), and have been trying to conclude the ACC in $R[[x]]$ from the ACC in $R[x]$, without success. I can prove the ACC if $R$ is a field, because then every $a_kx^k+a_{k+1}x^{k+1}+\cdots$ with $a_k\neq0$ is associate with $x^k$, hence every ideal is of the form $(x^k)$. (In fact this readily proves that $R[[x]]$ is a UFD.) If $R$ is not a field, say it has some non-invertible element $r$, then there are many more ideals such as $(r)$ or $(r+x)$. I'm also facing difficulties at identifying the irreducible elements of $R[[x]]$. (I wish to prove that they are all primes in order to conclude the uniqueness of factorisation.) I have figured that they either take the form $ux+Px^2$ for some unit $u\in R^\times$ and some $P\in R[[x]]$ or have a non-zero constant term which is not invertible in $R$. The elements of the form $ux+Px^2$ are indeed primes, the difficulty lies in those with non-zero constant term. Any hints at proving $R[[x]]$ is a UFD?","An exercise in my algebra course book asks to prove that if $R$ is a PID , then $R[[x]]$ is a UFD , where $R[[x]]$ is the ring of formal power series over $R$. After some failed attempts at  proving the ACC I visited Wikipedia, which comments : If $R$ is Noetherian, then so is $R[[x]]$; this is a version of the Hilbert basis theorem. This is very useful, as $R$ is a PID and hence Noetherian. Unfortunately we only saw (without proof) Hilbert's basis theorem in the form If $R$ is Noetherian, then so is $R[x]$. I'm not sure how to conclude the Noetherianity of $R[[x]]$ from this. I know that $R[x]$ is a UFD because $R$ is (we saw this without proof), and have been trying to conclude the ACC in $R[[x]]$ from the ACC in $R[x]$, without success. I can prove the ACC if $R$ is a field, because then every $a_kx^k+a_{k+1}x^{k+1}+\cdots$ with $a_k\neq0$ is associate with $x^k$, hence every ideal is of the form $(x^k)$. (In fact this readily proves that $R[[x]]$ is a UFD.) If $R$ is not a field, say it has some non-invertible element $r$, then there are many more ideals such as $(r)$ or $(r+x)$. I'm also facing difficulties at identifying the irreducible elements of $R[[x]]$. (I wish to prove that they are all primes in order to conclude the uniqueness of factorisation.) I have figured that they either take the form $ux+Px^2$ for some unit $u\in R^\times$ and some $P\in R[[x]]$ or have a non-zero constant term which is not invertible in $R$. The elements of the form $ux+Px^2$ are indeed primes, the difficulty lies in those with non-zero constant term. Any hints at proving $R[[x]]$ is a UFD?",,"['abstract-algebra', 'ring-theory', 'principal-ideal-domains', 'noetherian', 'unique-factorization-domains']"
33,Zero divisors in ring of real valued functions.,Zero divisors in ring of real valued functions.,,"I'm working though Pinter's A book of Abstract Algebra and would like a quick verification on a simple problem. Exercise 17.B2 asks Describe the divisors of zero in $\mathcal{F}(\mathbb{R})$. Note that $\mathcal{F}(\mathbb{R})$ denotes the ring of all real valued functions endowed with pointwise multiplication and addition. My attempt If a non-zero element $f$ is a zero divisor of $\mathcal{F}(\mathbb{R})$, then there exists a non-zero $g$ such that $$\forall x \in \mathbb{R}: \quad f(x)g(x) = 0.$$ Because $f$ is non-zero, there exists an $x_1 \in \mathbb{R}$ such that $f(x_1) \neq 0$. I claim that if, in addition, there is an $x_2 \in \mathbb{R}$ such that $f(x_2) = 0$, then $f$ is a zero-divisor. To see this, define $g$ by $$ g(x) = \begin{cases} 0 &\text{if}\;\;\; x \neq x_2 \\  1 &\text{if}\;\;\; x = x_2 \end{cases}$$ Then it is clear that $g$ is non-zero and $fg=0$. Thus, $f$ is a zero-divisor. I claim, further, that such $f$ are the only zero divisors. To see this, suppose that there does not exist an $x_2 \in \mathbb{R}$ such that $f(x_2) = 0$. Then $f(x) \neq 0$ for every $x \in \mathbb{R}$. But then for any $x \in \mathbb{R}$, we can divide though by $f(x)$ in  $f(x)g(x) = 0$ to obtain $$g(x) = \frac{0}{f(x)} = 0.$$ Which means that all $g$ that satisfy $fg=0$ must have $g(x) = 0$ for all $x \in \mathbb{R}$ contradicting the requirement that $g$ is non-zero. I'm trying to learn this on my own, and am having some trouble understanding and gaining intuition regarding zero-divisors. So even though this may seem simple, I would like some help and criticism :)","I'm working though Pinter's A book of Abstract Algebra and would like a quick verification on a simple problem. Exercise 17.B2 asks Describe the divisors of zero in $\mathcal{F}(\mathbb{R})$. Note that $\mathcal{F}(\mathbb{R})$ denotes the ring of all real valued functions endowed with pointwise multiplication and addition. My attempt If a non-zero element $f$ is a zero divisor of $\mathcal{F}(\mathbb{R})$, then there exists a non-zero $g$ such that $$\forall x \in \mathbb{R}: \quad f(x)g(x) = 0.$$ Because $f$ is non-zero, there exists an $x_1 \in \mathbb{R}$ such that $f(x_1) \neq 0$. I claim that if, in addition, there is an $x_2 \in \mathbb{R}$ such that $f(x_2) = 0$, then $f$ is a zero-divisor. To see this, define $g$ by $$ g(x) = \begin{cases} 0 &\text{if}\;\;\; x \neq x_2 \\  1 &\text{if}\;\;\; x = x_2 \end{cases}$$ Then it is clear that $g$ is non-zero and $fg=0$. Thus, $f$ is a zero-divisor. I claim, further, that such $f$ are the only zero divisors. To see this, suppose that there does not exist an $x_2 \in \mathbb{R}$ such that $f(x_2) = 0$. Then $f(x) \neq 0$ for every $x \in \mathbb{R}$. But then for any $x \in \mathbb{R}$, we can divide though by $f(x)$ in  $f(x)g(x) = 0$ to obtain $$g(x) = \frac{0}{f(x)} = 0.$$ Which means that all $g$ that satisfy $fg=0$ must have $g(x) = 0$ for all $x \in \mathbb{R}$ contradicting the requirement that $g$ is non-zero. I'm trying to learn this on my own, and am having some trouble understanding and gaining intuition regarding zero-divisors. So even though this may seem simple, I would like some help and criticism :)",,"['abstract-algebra', 'proof-verification', 'self-learning']"
34,Projective and injective modules; direct sums and products,Projective and injective modules; direct sums and products,,"I need two counterexamples. First, a direct sum of $R$-modules is projective iff each one is projective.  But I need an example to show that, “an arbitrary direct product of projective modules need not be a projective module.” If I let $R= \mathbb Z$  then $\mathbb Z$ is a projective $R$-module, but the direct product $\mathbb Z \times \mathbb Z \times \cdots$ is not free, hence it is not a projective module. We have a theorem which says that every free module over a ring $R$ is projective. Am I correct? Second, a direct product of $R$-modules is injective iff each one is injective  but I need an example to show that the direct sum of injective modules need not be injective.","I need two counterexamples. First, a direct sum of $R$-modules is projective iff each one is projective.  But I need an example to show that, “an arbitrary direct product of projective modules need not be a projective module.” If I let $R= \mathbb Z$  then $\mathbb Z$ is a projective $R$-module, but the direct product $\mathbb Z \times \mathbb Z \times \cdots$ is not free, hence it is not a projective module. We have a theorem which says that every free module over a ring $R$ is projective. Am I correct? Second, a direct product of $R$-modules is injective iff each one is injective  but I need an example to show that the direct sum of injective modules need not be injective.",,"['abstract-algebra', 'modules']"
35,"Recommendations for an ""illuminating"" (explained in the post) group theory/abstract algebra resource?","Recommendations for an ""illuminating"" (explained in the post) group theory/abstract algebra resource?",,"I recently asked a question regarding why homomorphisms and isomorphisms are important. The best answer to that question was actually a comment, which referred me to Brian M. Scott's answer here: https://math.stackexchange.com/a/242370/115703 That answer was mind blowingly insightful for me. I finally began to understand why someone would care about homomorphisms, and why the ""kernel"" might actually be called a kernel. Revelation upon revelation. Frisson all over. Why isn't this sort of an explanation easy to find in algebra textbooks though? (I am reading Dummit and Foote, and Rotman) Shouldn't this be the first thing a textbook says. Example: Say we are interested in studying the structure of the odd and even numbers. If we look at it from the perspective of the set $\mathbb{Z}$ , then we are likely to carry on a lot of extra baggage since $\mathbb{Z}$ has more structure in it than just ""odd and even"". What if studied the structure $\mathbb{Z}/2\mathbb{Z}$ instead? Well, it would be useful then to have some sort of a mapping between $\mathbb{Z}$ and $\mathbb{Z}/\mathbb{2Z}$ , since we are really studying integers, in a ""reduced structure"" setting. What sort of mappings might we be interested in... (ellipsis for a better way to explain the argument between where I have left off, and where I am going to, which eludes me right now) -- so the concept of a homomorphism . However, we might also be interested in asking how a homomorphism between $\mathbb{Z}$ and $\mathbb{Z}/2\mathbb{Z}$ changes/preserves the structure of $\mathbb{Z}$ . For instance, we might be curious about which elements in $\mathbb{Z}$ essentially become the ""same"" in $\mathbb{Z}/2\mathbb{Z}$ ...--so, the concept of a kernel . On the other hand, which elements retain some sense of ""difference""...--so, the concept of image . An isomorphism is just a homomorphism which preserves detail exactly -- i.e. it doesn't collapse any elements in $\mathbb{Z}$ into the ""same"" element in $\mathbb{Z}/2\mathbb{Z}$ ..-- which is why its kernel is just the identity . Actually, maybe instead of viewing homomorphisms and isomorphisms as structure preserving maps between groups, we should view them as generators of groups ? i.e. given some group, and we construct something that is a homomorphism in order to explore a new group related to the old group, given that homomorphism ? A lot of what I have written is very ""soft"" and not formally fleshed out. Some parts are outright skipped over (ellipsis) because I still lack the wisdom to explain it well. Regardless, the point is that, when first learning these topics, in order to understand the definitions well, it would be very illuminating to read the ""big picture"" behind all the details that are about to follow. Is there an abstract algebra text that provides this sort of illumination? Ideally, it such a text would also contain all the necessary proofs for formally defining a topic, but perhaps that is asking for too much? Even more ideally, such a text would deal with most of abstract algebra (at least groups, rings and fields), but that might be asking for too much, again. So, perhaps recommendations can be split up into categories depending on which aspect of abstract algebra they tackle in particular.","I recently asked a question regarding why homomorphisms and isomorphisms are important. The best answer to that question was actually a comment, which referred me to Brian M. Scott's answer here: https://math.stackexchange.com/a/242370/115703 That answer was mind blowingly insightful for me. I finally began to understand why someone would care about homomorphisms, and why the ""kernel"" might actually be called a kernel. Revelation upon revelation. Frisson all over. Why isn't this sort of an explanation easy to find in algebra textbooks though? (I am reading Dummit and Foote, and Rotman) Shouldn't this be the first thing a textbook says. Example: Say we are interested in studying the structure of the odd and even numbers. If we look at it from the perspective of the set , then we are likely to carry on a lot of extra baggage since has more structure in it than just ""odd and even"". What if studied the structure instead? Well, it would be useful then to have some sort of a mapping between and , since we are really studying integers, in a ""reduced structure"" setting. What sort of mappings might we be interested in... (ellipsis for a better way to explain the argument between where I have left off, and where I am going to, which eludes me right now) -- so the concept of a homomorphism . However, we might also be interested in asking how a homomorphism between and changes/preserves the structure of . For instance, we might be curious about which elements in essentially become the ""same"" in ...--so, the concept of a kernel . On the other hand, which elements retain some sense of ""difference""...--so, the concept of image . An isomorphism is just a homomorphism which preserves detail exactly -- i.e. it doesn't collapse any elements in into the ""same"" element in ..-- which is why its kernel is just the identity . Actually, maybe instead of viewing homomorphisms and isomorphisms as structure preserving maps between groups, we should view them as generators of groups ? i.e. given some group, and we construct something that is a homomorphism in order to explore a new group related to the old group, given that homomorphism ? A lot of what I have written is very ""soft"" and not formally fleshed out. Some parts are outright skipped over (ellipsis) because I still lack the wisdom to explain it well. Regardless, the point is that, when first learning these topics, in order to understand the definitions well, it would be very illuminating to read the ""big picture"" behind all the details that are about to follow. Is there an abstract algebra text that provides this sort of illumination? Ideally, it such a text would also contain all the necessary proofs for formally defining a topic, but perhaps that is asking for too much? Even more ideally, such a text would deal with most of abstract algebra (at least groups, rings and fields), but that might be asking for too much, again. So, perhaps recommendations can be split up into categories depending on which aspect of abstract algebra they tackle in particular.",\mathbb{Z} \mathbb{Z} \mathbb{Z}/2\mathbb{Z} \mathbb{Z} \mathbb{Z}/\mathbb{2Z} \mathbb{Z} \mathbb{Z}/2\mathbb{Z} \mathbb{Z} \mathbb{Z} \mathbb{Z}/2\mathbb{Z} \mathbb{Z} \mathbb{Z}/2\mathbb{Z},"['abstract-algebra', 'reference-request', 'soft-question', 'big-picture']"
36,What is an $R$-algebra?,What is an -algebra?,R,"In the following, assume that rings are rings with unity. Here is the definition of $R$ -algebra from Wikipedia: Let $R$ be a commutative ring and $(M,+,\cdot)$ an $R$ -module. Assume $\ast$ is a binary operation on $M$ , such that: $x\ast (y+z)= x\ast y + x\ast z$ $\forall x,y,z \in M.(x+y)\ast z = x\ast z + y\ast z$ $\forall r,s \in R,x,y \in M.(rx)\ast (sy)=(rs)(x\ast y) (r,s\in R, x,y\in M)$ then $(M,+,\cdot,\ast)$ is called an $R$ -algebra. This definition has the same form as my definition for ""algebra over a field"". Note that this definition does not require $\ast$ to be associative. Here is an equivalent definition given in Dummit&Foote (original form is given using a ring homomorphism): Let $(M,+,\cdot)$ be a ring and $R$ a commutive ring. If $M$ is an $R$ -module and the multiplication on $M$ is bilinear, then $M$ is called ""an $R$ -algebra"". Note that this definition requires $\ast$ to be associative. What is the usual definition for $R$ -algebra? References Abstract Algebra, Dummit and Foote, 3rd edition, page 355.","In the following, assume that rings are rings with unity. Here is the definition of -algebra from Wikipedia: Let be a commutative ring and an -module. Assume is a binary operation on , such that: then is called an -algebra. This definition has the same form as my definition for ""algebra over a field"". Note that this definition does not require to be associative. Here is an equivalent definition given in Dummit&Foote (original form is given using a ring homomorphism): Let be a ring and a commutive ring. If is an -module and the multiplication on is bilinear, then is called ""an -algebra"". Note that this definition requires to be associative. What is the usual definition for -algebra? References Abstract Algebra, Dummit and Foote, 3rd edition, page 355.","R R (M,+,\cdot) R \ast M x\ast (y+z)= x\ast y + x\ast z \forall x,y,z \in M.(x+y)\ast z = x\ast z + y\ast z \forall r,s \in R,x,y \in M.(rx)\ast (sy)=(rs)(x\ast y) (r,s\in R, x,y\in M) (M,+,\cdot,\ast) R \ast (M,+,\cdot) R M R M M R \ast R","['abstract-algebra', 'ring-theory', 'modules', 'definition', 'algebras']"
37,"Confusion about ""horizontal composition"" of natural transformations","Confusion about ""horizontal composition"" of natural transformations",,"I'm having trouble with an exercise from Rotman's Homological Algebra. It has to do with what Wikipedia calls "" horizontal composition "" of natural transformations. Namely, given $F, G:\mathcal{A}\to\mathcal{B}$ and $F^\prime, G^\prime:\mathcal{B}\to\mathcal{C}$ covariant functors and $\sigma:F\to G$ and $\tau:F^\prime\to G^\prime$ natural transformations, the goal is to show that there is a composite natural transformation $\tau\sigma: F^\prime F\to G^\prime G$. There should be a ""natural choice"" for the morphism $(\tau\sigma)_A$ associated to any $A\in\operatorname{obj}\mathcal{A}$, and this is where I'm confused. The problem says to define $$(\tau\sigma)_A=\tau_{FA}\sigma_A : F^\prime F(A)\to G^\prime G(A)$$ but this doesn't make sense to me since $\tau_{FA}\in\operatorname{Hom}(F^\prime F(A),G^\prime F(A))$ and $\sigma_A\in\operatorname{Hom}(FA,GA)$ (right?). It's not clear to me why this is the right composition, or that it even is a composition. If I had to guess, I would say $(\tau\sigma)_A:=\tau_{GA}F^\prime(\sigma_A)$ because this is the only thing I can come up with that actually is a morphism $F^\prime F(A)\to G^\prime G(A)$. Can anyone clear up my confusion?","I'm having trouble with an exercise from Rotman's Homological Algebra. It has to do with what Wikipedia calls "" horizontal composition "" of natural transformations. Namely, given $F, G:\mathcal{A}\to\mathcal{B}$ and $F^\prime, G^\prime:\mathcal{B}\to\mathcal{C}$ covariant functors and $\sigma:F\to G$ and $\tau:F^\prime\to G^\prime$ natural transformations, the goal is to show that there is a composite natural transformation $\tau\sigma: F^\prime F\to G^\prime G$. There should be a ""natural choice"" for the morphism $(\tau\sigma)_A$ associated to any $A\in\operatorname{obj}\mathcal{A}$, and this is where I'm confused. The problem says to define $$(\tau\sigma)_A=\tau_{FA}\sigma_A : F^\prime F(A)\to G^\prime G(A)$$ but this doesn't make sense to me since $\tau_{FA}\in\operatorname{Hom}(F^\prime F(A),G^\prime F(A))$ and $\sigma_A\in\operatorname{Hom}(FA,GA)$ (right?). It's not clear to me why this is the right composition, or that it even is a composition. If I had to guess, I would say $(\tau\sigma)_A:=\tau_{GA}F^\prime(\sigma_A)$ because this is the only thing I can come up with that actually is a morphism $F^\prime F(A)\to G^\prime G(A)$. Can anyone clear up my confusion?",,"['category-theory', 'abstract-algebra']"
38,Separability and tensor product of fields,Separability and tensor product of fields,,"Is it true that a finite degree field extension $L/k$ is separable if and only if $L\otimes_{k}L$ is a reduced $L$-algebra? Surely the ""only if"" part is true because if the extension is separable, we have the Primitive Element Theorem and everything follows. But I'm asking if it's true, and how to prove the ""if"" part. Thanks.","Is it true that a finite degree field extension $L/k$ is separable if and only if $L\otimes_{k}L$ is a reduced $L$-algebra? Surely the ""only if"" part is true because if the extension is separable, we have the Primitive Element Theorem and everything follows. But I'm asking if it's true, and how to prove the ""if"" part. Thanks.",,"['abstract-algebra', 'commutative-algebra', 'galois-theory', 'extension-field']"
39,How to prove surjectivity part of Short Five Lemma for short exact sequences.,How to prove surjectivity part of Short Five Lemma for short exact sequences.,,"Suppose we have a homomorphism $\alpha, \beta, \gamma$ of short exact sequences: $$ \begin{matrix} 0 & \to & A & \xrightarrow{\psi} & B & \xrightarrow{\phi} & C & \to & 0 \\ \ & \   & \downarrow^{\alpha} & \ & \downarrow^{\beta} \ & \ & \downarrow^{\gamma} \\ 0 & \to & A' & \xrightarrow{\psi'} & B' & \xrightarrow{\phi'} & C' & \to & 0 \end{matrix} $$ If both $\alpha, \gamma$ are surjective then so is $\beta$.  This can be proved using the properties of the diagram somehow. I've tried several things.","Suppose we have a homomorphism $\alpha, \beta, \gamma$ of short exact sequences: $$ \begin{matrix} 0 & \to & A & \xrightarrow{\psi} & B & \xrightarrow{\phi} & C & \to & 0 \\ \ & \   & \downarrow^{\alpha} & \ & \downarrow^{\beta} \ & \ & \downarrow^{\gamma} \\ 0 & \to & A' & \xrightarrow{\psi'} & B' & \xrightarrow{\phi'} & C' & \to & 0 \end{matrix} $$ If both $\alpha, \gamma$ are surjective then so is $\beta$.  This can be proved using the properties of the diagram somehow. I've tried several things.",,"['abstract-algebra', 'modules', 'exact-sequence']"
40,Multivariate coprime polynomials in field extensions,Multivariate coprime polynomials in field extensions,,"Suppose $f$ and $g$ are polynomials in $n$ variables, $n\ge 2$, over a field $E$. Suppose further that $f$ and $g$ are relatively prime over $E$. If $F$ is a field extension of $E$, are $f$ and $g$ still relatively prime over $F$?","Suppose $f$ and $g$ are polynomials in $n$ variables, $n\ge 2$, over a field $E$. Suppose further that $f$ and $g$ are relatively prime over $E$. If $F$ is a field extension of $E$, are $f$ and $g$ still relatively prime over $F$?",,"['abstract-algebra', 'polynomials', 'field-theory']"
41,Uniformly solvable families of polynomials,Uniformly solvable families of polynomials,,"It is a famous theorem that there is no ""quintic formula"", i.e. there is no formula which expresses the roots of a quintic polynomial $x^5+a_4x^4+\cdots+a_0$ in terms of $a_4,\ldots,a_0$ and rational constants using the field operations and taking $n$th roots. But the standard proof of this actually proves something which on its face seems stronger: that there are particular quintics with roots that cannot be expressed in terms of rational numbers using these operations. The non-existence of a quintic formula merely requires that there be no ""uniform"" way of doing so. I'm wondering if this is actually a stronger statement. To make this rigorous: Given $f(a_0,\ldots,a_k,x)\in \mathbb Q(a_0,\ldots,a_k)[x]$, we can define the family of polynomials associated to $f$ to be the set of polynomials $f(t_0,\ldots,t_k,x)\in \mathbb Q[x]$ where $t_0,\ldots,t_k\in \mathbb Q$ are such that this expression is defined. For example, the family of polynomials associated to $x^2+a_1x+a_0$ is the set of monic quadratic polynomials. If there an $f\in \mathbb Q(a_0,\ldots,a_k)[x]$ which does not admit a formula for its roots (in the variable $t$) in terms of elements of $\mathbb Q(a_0,\ldots,a_k)$, field operations and taking $n$th roots, yet every polynomial in the family of polynomials associated to $f$ does? From the point of view of Galois theory, the question becomes whether there exist polynomials $f\in \mathbb Q(a_0,\ldots,a_k)[x]$ which are not solvable over $\mathbb Q(a_0,\ldots,a_k)$, but evaluating them at any point $(t_0,\ldots,t_k)\in\mathbb Q$ (for which the result is defined) gives a polynomial solvable over $\mathbb Q$?","It is a famous theorem that there is no ""quintic formula"", i.e. there is no formula which expresses the roots of a quintic polynomial $x^5+a_4x^4+\cdots+a_0$ in terms of $a_4,\ldots,a_0$ and rational constants using the field operations and taking $n$th roots. But the standard proof of this actually proves something which on its face seems stronger: that there are particular quintics with roots that cannot be expressed in terms of rational numbers using these operations. The non-existence of a quintic formula merely requires that there be no ""uniform"" way of doing so. I'm wondering if this is actually a stronger statement. To make this rigorous: Given $f(a_0,\ldots,a_k,x)\in \mathbb Q(a_0,\ldots,a_k)[x]$, we can define the family of polynomials associated to $f$ to be the set of polynomials $f(t_0,\ldots,t_k,x)\in \mathbb Q[x]$ where $t_0,\ldots,t_k\in \mathbb Q$ are such that this expression is defined. For example, the family of polynomials associated to $x^2+a_1x+a_0$ is the set of monic quadratic polynomials. If there an $f\in \mathbb Q(a_0,\ldots,a_k)[x]$ which does not admit a formula for its roots (in the variable $t$) in terms of elements of $\mathbb Q(a_0,\ldots,a_k)$, field operations and taking $n$th roots, yet every polynomial in the family of polynomials associated to $f$ does? From the point of view of Galois theory, the question becomes whether there exist polynomials $f\in \mathbb Q(a_0,\ldots,a_k)[x]$ which are not solvable over $\mathbb Q(a_0,\ldots,a_k)$, but evaluating them at any point $(t_0,\ldots,t_k)\in\mathbb Q$ (for which the result is defined) gives a polynomial solvable over $\mathbb Q$?",,"['abstract-algebra', 'algebraic-geometry', 'commutative-algebra', 'galois-theory']"
42,Which families of groups have interesting formulas for the number of elements of given order?,Which families of groups have interesting formulas for the number of elements of given order?,,"Suppose that $G$ is a group and that $n$ is a positive integer diving the order of $G$. Let $f_n(G)$ be the number of elements satisfying $x^n = 1$ in $G$. According to a theorem of Frobenius, then we have $f_n(G) \equiv 0 \mod{n}$. Hence if we have a family of groups and a formula for the number of solutions to $x^n = 1$, an application of Frobenius theorem proves that the formula is $\equiv 0 \mod{n}$. One family of groups where we can find a formula are the symmetric and alternating groups, since the order of a permutation is determined by cycle structure. For example, let $p$ be a prime number and $n \geq p$ some integer. Then the number of elements satisfying $x^p = 1$ in $S_n$ is $$\sum_{k = 1}^{\lfloor \frac{n}{p} \rfloor} \frac{n!}{p^k (n-pk)! k!} + 1 \equiv 0 \mod{p}$$ by Frobenius theorem. It does not seem immediately obvious without Frobenius theorem that the sum on the left should be $\equiv -1 \mod{p}$. Also, notice that $n = p$ gives us Wilson's theorem: $$(p-1)! \equiv -1 \mod{p}$$ More congruences can be found by calculating the number of elements of some order in the symmetric groups $S_n$ and alternating groups $A_n$. My question is this: Besides $S_n$ and $A_n$, for what other families of finite groups can we find congruences like the one above by applying Frobenius theorem? Of course, if you feel like there's some noteworthy/interesting/useful congruence in $S_n$ to be found with Frobenius theorem, feel free to answer or comment with those too.","Suppose that $G$ is a group and that $n$ is a positive integer diving the order of $G$. Let $f_n(G)$ be the number of elements satisfying $x^n = 1$ in $G$. According to a theorem of Frobenius, then we have $f_n(G) \equiv 0 \mod{n}$. Hence if we have a family of groups and a formula for the number of solutions to $x^n = 1$, an application of Frobenius theorem proves that the formula is $\equiv 0 \mod{n}$. One family of groups where we can find a formula are the symmetric and alternating groups, since the order of a permutation is determined by cycle structure. For example, let $p$ be a prime number and $n \geq p$ some integer. Then the number of elements satisfying $x^p = 1$ in $S_n$ is $$\sum_{k = 1}^{\lfloor \frac{n}{p} \rfloor} \frac{n!}{p^k (n-pk)! k!} + 1 \equiv 0 \mod{p}$$ by Frobenius theorem. It does not seem immediately obvious without Frobenius theorem that the sum on the left should be $\equiv -1 \mod{p}$. Also, notice that $n = p$ gives us Wilson's theorem: $$(p-1)! \equiv -1 \mod{p}$$ More congruences can be found by calculating the number of elements of some order in the symmetric groups $S_n$ and alternating groups $A_n$. My question is this: Besides $S_n$ and $A_n$, for what other families of finite groups can we find congruences like the one above by applying Frobenius theorem? Of course, if you feel like there's some noteworthy/interesting/useful congruence in $S_n$ to be found with Frobenius theorem, feel free to answer or comment with those too.",,"['abstract-algebra', 'group-theory', 'elementary-number-theory', 'finite-groups', 'q-analogs']"
43,Proving formula involving Euler's totient function,Proving formula involving Euler's totient function,,"This question is motivated by lhf's comment here . ""It'd be nice to relate this formula with the natural mapping $U_{mn} \to U_m \times U_n$ by proving that the kernel has size $d$ and the image has index $\varphi(d)$."" Here, $U_k$ denotes the group of units of the ring $\mathbb{Z} / k \mathbb{Z}$ whenever $k$ is a positive integer. I'm trying to prove the formula $$   \varphi(mn) = \varphi(m)\varphi(n) \frac{d}{\varphi(d)} $$ by considering the natural map $\eta\colon U_{mn} \to U_m \times U_n$ (i.e. the map sending $\overline{x} \mapsto (\overline{x},\overline{x})$, where the bar denotes reduction mod $mn$, $m$, or $n$, respectively). I've been able to show that the kernel has the right size as follows: The kernel of $\eta$ consists of the elements $\overline{x} \in U_{mn}$ with $x \equiv 1 \bmod m$ and $x \equiv 1 \bmod n$. The integers $x$ which satisfy these conditions are those of the form $x = \frac{mn}{d}k + 1$ for $k \in \mathbb Z$. On the other hand, any such integer $x$ is relatively prime to $mn$, and hence gives and element $\overline{x} \in U_{mn}$. Therefore, $\ker \eta$ consists of the $d$ distinct elements $\overline{x}$, where $x = \frac{mn}{d}k + 1$ and $k \in \{1,\ldots,d\}$. Once it has been shown that the image has index $\varphi(d)$, the first isomorphism theorem gives $$   \frac{U_{mn}}{\ker \eta} \cong Im(\eta), $$ and so $$   \frac{\varphi(mn)}{d} = \frac{|U_{mn}|}{|\ker \eta|} = |Im(\eta)| = \frac{|U_m \times U_n|}{|U_m \times U_n:Im(\eta)|} = \frac{\varphi(m)\varphi(n)}{\varphi(d)}, $$ or $$   \varphi(mn) = \varphi(m)\varphi(n) \frac{d}{\varphi(d)}. $$ I'm having trouble showing the image has the right index. I've noticed that $\eta(\overline{x}) = \eta(\overline{x + \frac{mn}{d}})$, so the image consists the images of the elements $\overline{x}$ with $1 \leq x < \frac{mn}{d}$. I'm not sure if this is going anywhere, though.  Any suggestions?","This question is motivated by lhf's comment here . ""It'd be nice to relate this formula with the natural mapping $U_{mn} \to U_m \times U_n$ by proving that the kernel has size $d$ and the image has index $\varphi(d)$."" Here, $U_k$ denotes the group of units of the ring $\mathbb{Z} / k \mathbb{Z}$ whenever $k$ is a positive integer. I'm trying to prove the formula $$   \varphi(mn) = \varphi(m)\varphi(n) \frac{d}{\varphi(d)} $$ by considering the natural map $\eta\colon U_{mn} \to U_m \times U_n$ (i.e. the map sending $\overline{x} \mapsto (\overline{x},\overline{x})$, where the bar denotes reduction mod $mn$, $m$, or $n$, respectively). I've been able to show that the kernel has the right size as follows: The kernel of $\eta$ consists of the elements $\overline{x} \in U_{mn}$ with $x \equiv 1 \bmod m$ and $x \equiv 1 \bmod n$. The integers $x$ which satisfy these conditions are those of the form $x = \frac{mn}{d}k + 1$ for $k \in \mathbb Z$. On the other hand, any such integer $x$ is relatively prime to $mn$, and hence gives and element $\overline{x} \in U_{mn}$. Therefore, $\ker \eta$ consists of the $d$ distinct elements $\overline{x}$, where $x = \frac{mn}{d}k + 1$ and $k \in \{1,\ldots,d\}$. Once it has been shown that the image has index $\varphi(d)$, the first isomorphism theorem gives $$   \frac{U_{mn}}{\ker \eta} \cong Im(\eta), $$ and so $$   \frac{\varphi(mn)}{d} = \frac{|U_{mn}|}{|\ker \eta|} = |Im(\eta)| = \frac{|U_m \times U_n|}{|U_m \times U_n:Im(\eta)|} = \frac{\varphi(m)\varphi(n)}{\varphi(d)}, $$ or $$   \varphi(mn) = \varphi(m)\varphi(n) \frac{d}{\varphi(d)}. $$ I'm having trouble showing the image has the right index. I've noticed that $\eta(\overline{x}) = \eta(\overline{x + \frac{mn}{d}})$, so the image consists the images of the elements $\overline{x}$ with $1 \leq x < \frac{mn}{d}$. I'm not sure if this is going anywhere, though.  Any suggestions?",,"['abstract-algebra', 'number-theory', 'elementary-number-theory', 'totient-function']"
44,$f(x)= {}^tx^{-1}$ is an automorphism of GL$_n(\mathbb{R})$,is an automorphism of GL,f(x)= {}^tx^{-1} _n(\mathbb{R}),"This is a question from the free Harvard online abstract algebra lectures .  I'm posting my solutions here to get some feedback on them.  For a fuller explanation, see this post. This problem is assigned in the 4th lecture. Define $f:GL_n(\mathbb{R}) → GL_n(\mathbb{R})$ by $f(A)=^tA^{-1}$ (where $^tA$ is the transpose of $A$ ).  Show that $f$ is an automorphism, but not an inner automorphism for n ≥ 1. Let $A,B\in GL_n(\mathbb{R})$ .  Then $f(AB)={}^t(AB)^{-1}=({}^tB^tA)^{-1}={}^tA^{-1}\cdot{}^tB^{-1}=f(A)\cdot f(B)$ .  So $f$ is a homomorphism.  For any $A \in GL_n(\mathbb{R}), f(f(A))=f({}^tA^{-1})={}^t({}^tA^{-1})^{-1}=A$ .  So $f^{-1}=f$ .  Since $f$ has an inverse for all $A\in GL_n(\mathbb{R})$ , $f$ is bijective.  Therefore, $f$ is an automorphism of $GL_n(\mathbb{R})$ . Assume $f$ is an inner automorphism.  Let $A=\lambda I_n$ .  Since $A$ is in the center of $GL_n(\mathbb{R})$ , there is some $B \in GL_n(\mathbb{R})$ such that $f(A)=BAB^{-1} = BB^{-1}A=A$ .  But ${}^tA^{-1}=\frac{1}{\lambda}I_n\neq A$ for $\lambda\neq1$ .  Therefore, $f$ is not an inner automorphism. Again, I welcome any critique of my reasoning and/or my style as well as alternative solutions to the problem. Thanks.","This is a question from the free Harvard online abstract algebra lectures .  I'm posting my solutions here to get some feedback on them.  For a fuller explanation, see this post. This problem is assigned in the 4th lecture. Define by (where is the transpose of ).  Show that is an automorphism, but not an inner automorphism for n ≥ 1. Let .  Then .  So is a homomorphism.  For any .  So .  Since has an inverse for all , is bijective.  Therefore, is an automorphism of . Assume is an inner automorphism.  Let .  Since is in the center of , there is some such that .  But for .  Therefore, is not an inner automorphism. Again, I welcome any critique of my reasoning and/or my style as well as alternative solutions to the problem. Thanks.","f:GL_n(\mathbb{R}) → GL_n(\mathbb{R}) f(A)=^tA^{-1} ^tA A f A,B\in GL_n(\mathbb{R}) f(AB)={}^t(AB)^{-1}=({}^tB^tA)^{-1}={}^tA^{-1}\cdot{}^tB^{-1}=f(A)\cdot f(B) f A \in GL_n(\mathbb{R}), f(f(A))=f({}^tA^{-1})={}^t({}^tA^{-1})^{-1}=A f^{-1}=f f A\in GL_n(\mathbb{R}) f f GL_n(\mathbb{R}) f A=\lambda I_n A GL_n(\mathbb{R}) B \in GL_n(\mathbb{R}) f(A)=BAB^{-1} = BB^{-1}A=A {}^tA^{-1}=\frac{1}{\lambda}I_n\neq A \lambda\neq1 f","['abstract-algebra', 'group-theory', 'finite-groups']"
45,Which groups have precisely two automorphisms,Which groups have precisely two automorphisms,,"Which groups $G$ have precisely two automorphisms, i.e., precisely one non-trivial automorphism? Examples: $G= C_3, \mathbf{Z},\ldots$. I think $G$ has to be abelian. In fact, we have $ \vert G\vert \geq 3$. Therefore, if $G$ is not abelian, we have at least two non-trivial inner automorphisms. If we can show that $G$ is cyclic the above examples are all of them.","Which groups $G$ have precisely two automorphisms, i.e., precisely one non-trivial automorphism? Examples: $G= C_3, \mathbf{Z},\ldots$. I think $G$ has to be abelian. In fact, we have $ \vert G\vert \geq 3$. Therefore, if $G$ is not abelian, we have at least two non-trivial inner automorphisms. If we can show that $G$ is cyclic the above examples are all of them.",,"['abstract-algebra', 'group-theory', 'automorphism-group']"
46,Is $[G_p \cap G_q:G_{pq}]$ always finite?,Is  always finite?,[G_p \cap G_q:G_{pq}],"Suppose $G$ is a group, $G_n = \langle\{g^n| g \in G\}\rangle$. Suppose $p$ and $q$ are coprime integers. It is not hard to notice, that $G_{pq} \leq G_p \cap G_q$ (""$\leq$"" sign means here ""Is a subgroup of"") and there are cases where inequality holds ($G_{pq}$ is a proper subgroup). For example in a free group $G = F[a, b]$ the element $a^3(ab^2)^3$ is an element of both $G_2$ and $G_3$ but not of $G_6$. However, I failed to find any example where $G_{pq}$ is of infinite index in $G_q \cap G_p$. The proof that there isn't one, didn't come to my mind either.  And thus I am asking a question: Is the index of $G_{pq}$ in $G_p \cap G_q$ always finite? Any help will be appreciated.","Suppose $G$ is a group, $G_n = \langle\{g^n| g \in G\}\rangle$. Suppose $p$ and $q$ are coprime integers. It is not hard to notice, that $G_{pq} \leq G_p \cap G_q$ (""$\leq$"" sign means here ""Is a subgroup of"") and there are cases where inequality holds ($G_{pq}$ is a proper subgroup). For example in a free group $G = F[a, b]$ the element $a^3(ab^2)^3$ is an element of both $G_2$ and $G_3$ but not of $G_6$. However, I failed to find any example where $G_{pq}$ is of infinite index in $G_q \cap G_p$. The proof that there isn't one, didn't come to my mind either.  And thus I am asking a question: Is the index of $G_{pq}$ in $G_p \cap G_q$ always finite? Any help will be appreciated.",,"['abstract-algebra', 'group-theory']"
47,Find the splitting field of $x^4+1$ over $\mathbb Q$.,Find the splitting field of  over .,x^4+1 \mathbb Q,"Solution:Let $\mathbb E$ be the splitting field of $x^4+1$ over $\mathbb Q$ .Then $x^4+1$ splits into linear factors in $\mathbb E$ . $$x^4+1=(x^2-i)(x^2+i)=(x-\sqrt i)(x+\sqrt i)(x-\sqrt {-i})(x+\sqrt {-i})$$ $$=(x-e^{i\pi/4})(x+e^{i\pi/4})(x-e^{i3\pi/4})(x+\ e^{i3\pi/4}).$$ Since $\mathbb E$ is the splitting field of $x^4+1$ over $\mathbb Q$ then $\mathbb E$ is the smallest field containing $\mathbb Q$ and the roots of $x^4+1$ . Thus, $$\mathbb E=\mathbb Q(e^{i3\pi/4},-e^{i3\pi/4},e^{i\pi/4},-e^{i3\pi/4}).$$ Since $e^{i3\pi/4}$ can be obtained by taking the cube of $e^{i\pi/4}$ we have $$\mathbb E=\mathbb Q(e^{i\pi/4})=\mathbb Q\left(\frac{1+i}{\sqrt 2}\right).$$ Thus, $$\mathbb E=\mathbb Q\left(\frac{1+i}{\sqrt 2}\right)=\mathbb Q(i,\sqrt 2)=\mathbb Q(\sqrt {-2}).$$ Please check my solution. Any suggestions are heartly welcome!! Thank you!!","Solution:Let be the splitting field of over .Then splits into linear factors in . Since is the splitting field of over then is the smallest field containing and the roots of . Thus, Since can be obtained by taking the cube of we have Thus, Please check my solution. Any suggestions are heartly welcome!! Thank you!!","\mathbb E x^4+1 \mathbb Q x^4+1 \mathbb E x^4+1=(x^2-i)(x^2+i)=(x-\sqrt i)(x+\sqrt i)(x-\sqrt {-i})(x+\sqrt {-i}) =(x-e^{i\pi/4})(x+e^{i\pi/4})(x-e^{i3\pi/4})(x+\ e^{i3\pi/4}). \mathbb E x^4+1 \mathbb Q \mathbb E \mathbb Q x^4+1 \mathbb E=\mathbb Q(e^{i3\pi/4},-e^{i3\pi/4},e^{i\pi/4},-e^{i3\pi/4}). e^{i3\pi/4} e^{i\pi/4} \mathbb E=\mathbb Q(e^{i\pi/4})=\mathbb Q\left(\frac{1+i}{\sqrt 2}\right). \mathbb E=\mathbb Q\left(\frac{1+i}{\sqrt 2}\right)=\mathbb Q(i,\sqrt 2)=\mathbb Q(\sqrt {-2}).","['abstract-algebra', 'field-theory', 'splitting-field']"
48,Structure theorem (PIDs) from Smith Normal Form,Structure theorem (PIDs) from Smith Normal Form,,"How exactly does the structure theorem follow from Smith Normal Form? ( Wikipedia statement ) It is said that a presentation (map from relations to generators) is put into Smith Normal form. Now, I see that the Smith Normal form applies to such a presentation, but I dont see how the structure theorem follows. I must say that the term presentation is new for me, I read through Free Presentation , Group presentation and looked up the terms relators and generators which I think I understood. Smith Normal Form A linear transformation $f$ between free modules $M$ and $N$ over a PID $R$ can be represented by the $m \times n$ following matrix where $a_i | a_{i+1} \forall 1 \leq i \lt r$. $$ \begin{bmatrix} a_1 & 0 &0 &0 &... \\ 0 &a_2 &0 &0 &... \\ 0 &0 &a_3 &0 &... \\ ... \end{bmatrix} $$ Structure theorem for modules over PIDs For every finitely generated module $M$ over a PID $R$ there exist unique ideals $(d_1) \supset (d_2) \supset (d_3) ... (d_n)$ with $M \cong R/(d_1) \oplus R/(d_2) \oplus ... R/(d_n)$","How exactly does the structure theorem follow from Smith Normal Form? ( Wikipedia statement ) It is said that a presentation (map from relations to generators) is put into Smith Normal form. Now, I see that the Smith Normal form applies to such a presentation, but I dont see how the structure theorem follows. I must say that the term presentation is new for me, I read through Free Presentation , Group presentation and looked up the terms relators and generators which I think I understood. Smith Normal Form A linear transformation $f$ between free modules $M$ and $N$ over a PID $R$ can be represented by the $m \times n$ following matrix where $a_i | a_{i+1} \forall 1 \leq i \lt r$. $$ \begin{bmatrix} a_1 & 0 &0 &0 &... \\ 0 &a_2 &0 &0 &... \\ 0 &0 &a_3 &0 &... \\ ... \end{bmatrix} $$ Structure theorem for modules over PIDs For every finitely generated module $M$ over a PID $R$ there exist unique ideals $(d_1) \supset (d_2) \supset (d_3) ... (d_n)$ with $M \cong R/(d_1) \oplus R/(d_2) \oplus ... R/(d_n)$",,"['abstract-algebra', 'modules', 'principal-ideal-domains', 'smith-normal-form']"
49,"Any Set with Associativity, Left Identity, Left Inverse is a Group","Any Set with Associativity, Left Identity, Left Inverse is a Group",,"Related Link: Right identity and Right inverse implies a group Reference: Fraleigh p. 49 Question 4.38 in A First Course in Abstract Algebra I will present my proof (distinct from those in the link) for critique and then ask my question.  $G$ is a set and $ \times $ is an associative binary operation.  Suppose that there exists a $e \in G$ such that, for all $a \in G$, $ea = a$ and $a^{-1}a = e$ for some $a^{-1} \in G$.  Show that for the same $e$, $ae = a$ and $aa^{-1} = e$. $a^{-1}(aa^{-1})a = (a^{-1}a)(a^{-1}a) = ee = e = a^{-1}a = a^{-1}(a^{-1}a)a$ Since $a^{-1} \in G$, it has a left inverse; apply it to both ends, and we have $(aa^{-1})a = (a^{-1}a)a$. As a result, $ae$ = $a(a^{-1}a) = (aa^{-1})a = (a^{-1}a)a = ea$. For the right inverse, start with $aa^{-1} = a(a^{-1}a)a^{-1} = (aa^{-1})(aa^{-1})$. Since $\times$ is a binary operation, $aa^{-1} \in G$ and has a left inverse; apply it to both ends, and we have $e = aa^{-1}$. In second comment following the question in the link, Mr. Derek Holt pointed out that the requester did not word his/her question correctly.  Specifically, the identity in the second axiom is not well-defined. Let $(G, *)$ be a semi-group. Suppose 1. $ \exists e \in G$ such that $\forall a \in G,\  ae = a$; 2. $\forall a \in G, \exists a^{-1} \in G$ such that $aa^{-1} = e$. How can we prove that $(G,*)$ is a group? This formulation makes the same technical error as many textbooks. The $e$ in your second axiom is not well-defined. ""But obviously it's intended to be the same $e$ as in the first axiom"" you reply. But the first axiom does not necessarily specify a unique element $e$. So should we interpret the second axiom as meaning ""for some $e$ as in 1"" or ""for all $e$ as in 1""? –  Derek Holt Sep 17 '11 at 15:31 Was he saying that if, in axiom 1, we have $ae_1 = a, ae_2 = a$, but $e_1 \neq e_2$, when we get to axiom 2, do we have $aa^{-1} = e_1, aa^{-1} = e_2$, or two different inverses so that $aa_1^{-1} =  e_1, aa^{-1}_2 = e_2$?  I think my wording eliminated the ambiguity.  It does not imply that $e$ is unique, but if $e$ is a left identity and produces left inverses, then it is also a right identity and produces right inverses. I tried really hard on this one; please kindly point out my mistakes.","Related Link: Right identity and Right inverse implies a group Reference: Fraleigh p. 49 Question 4.38 in A First Course in Abstract Algebra I will present my proof (distinct from those in the link) for critique and then ask my question.  $G$ is a set and $ \times $ is an associative binary operation.  Suppose that there exists a $e \in G$ such that, for all $a \in G$, $ea = a$ and $a^{-1}a = e$ for some $a^{-1} \in G$.  Show that for the same $e$, $ae = a$ and $aa^{-1} = e$. $a^{-1}(aa^{-1})a = (a^{-1}a)(a^{-1}a) = ee = e = a^{-1}a = a^{-1}(a^{-1}a)a$ Since $a^{-1} \in G$, it has a left inverse; apply it to both ends, and we have $(aa^{-1})a = (a^{-1}a)a$. As a result, $ae$ = $a(a^{-1}a) = (aa^{-1})a = (a^{-1}a)a = ea$. For the right inverse, start with $aa^{-1} = a(a^{-1}a)a^{-1} = (aa^{-1})(aa^{-1})$. Since $\times$ is a binary operation, $aa^{-1} \in G$ and has a left inverse; apply it to both ends, and we have $e = aa^{-1}$. In second comment following the question in the link, Mr. Derek Holt pointed out that the requester did not word his/her question correctly.  Specifically, the identity in the second axiom is not well-defined. Let $(G, *)$ be a semi-group. Suppose 1. $ \exists e \in G$ such that $\forall a \in G,\  ae = a$; 2. $\forall a \in G, \exists a^{-1} \in G$ such that $aa^{-1} = e$. How can we prove that $(G,*)$ is a group? This formulation makes the same technical error as many textbooks. The $e$ in your second axiom is not well-defined. ""But obviously it's intended to be the same $e$ as in the first axiom"" you reply. But the first axiom does not necessarily specify a unique element $e$. So should we interpret the second axiom as meaning ""for some $e$ as in 1"" or ""for all $e$ as in 1""? –  Derek Holt Sep 17 '11 at 15:31 Was he saying that if, in axiom 1, we have $ae_1 = a, ae_2 = a$, but $e_1 \neq e_2$, when we get to axiom 2, do we have $aa^{-1} = e_1, aa^{-1} = e_2$, or two different inverses so that $aa_1^{-1} =  e_1, aa^{-1}_2 = e_2$?  I think my wording eliminated the ambiguity.  It does not imply that $e$ is unique, but if $e$ is a left identity and produces left inverses, then it is also a right identity and produces right inverses. I tried really hard on this one; please kindly point out my mistakes.",,"['abstract-algebra', 'group-theory']"
50,What's are all the prime elements in Gaussian integers $\mathbb{Z}[i]$,What's are all the prime elements in Gaussian integers,\mathbb{Z}[i],"So far, I know if $p$ is a rational prime, then $(1)$ if $p\equiv 3\mod4$, then $p$ is prime in $\mathbb{Z}[i]$. $(2)$ If $p\equiv1\mod4$  then $p=π_1 π_2$ where $π_1 $ and $π_2$ are conjugate, Then $π_1 $ and $π_2$ are primes in $\mathbb{Z}[i]$. $(3)$ $2=(1+i)(1-i)$, then $(1+i)$and$(1-i)$ are primes in $\mathbb{Z}[i]$. What's are all the prime elements in Gaussian integers $\mathbb{Z}[i]$? For example, $-3$ are prime in $\mathbb{Z}[i]$, but not in the above $3$ cases.","So far, I know if $p$ is a rational prime, then $(1)$ if $p\equiv 3\mod4$, then $p$ is prime in $\mathbb{Z}[i]$. $(2)$ If $p\equiv1\mod4$  then $p=π_1 π_2$ where $π_1 $ and $π_2$ are conjugate, Then $π_1 $ and $π_2$ are primes in $\mathbb{Z}[i]$. $(3)$ $2=(1+i)(1-i)$, then $(1+i)$and$(1-i)$ are primes in $\mathbb{Z}[i]$. What's are all the prime elements in Gaussian integers $\mathbb{Z}[i]$? For example, $-3$ are prime in $\mathbb{Z}[i]$, but not in the above $3$ cases.",,"['abstract-algebra', 'number-theory', 'ring-theory']"
51,What can be said about two groups with isomorphic derived factors?,What can be said about two groups with isomorphic derived factors?,,"The third isomorphism theorem states that we can relate an isomorphic relation between two normal subgroups of a group $G$. My question is can we infer anything about the two groups structures itself given that the factor/quotient groups in the derived series of two solvable groups are isomorphic to one another? In other words, if we have two solvable groups $G=\langle g_1,g_2,\ldots, g_n \rangle$ and $H=\langle h_1,h_2,\ldots h_p \rangle$, given that $G^{j}/G^{j+1}$ is isomorphic to $H^{j}/H^{j+1}$, what can be said about $G$ and $H$? (I don't think they are isomorphic but anything less strong be said?)","The third isomorphism theorem states that we can relate an isomorphic relation between two normal subgroups of a group $G$. My question is can we infer anything about the two groups structures itself given that the factor/quotient groups in the derived series of two solvable groups are isomorphic to one another? In other words, if we have two solvable groups $G=\langle g_1,g_2,\ldots, g_n \rangle$ and $H=\langle h_1,h_2,\ldots h_p \rangle$, given that $G^{j}/G^{j+1}$ is isomorphic to $H^{j}/H^{j+1}$, what can be said about $G$ and $H$? (I don't think they are isomorphic but anything less strong be said?)",,"['abstract-algebra', 'group-theory', 'solvable-groups']"
52,Is there a name for this ring-like object?,Is there a name for this ring-like object?,,"Let $S$ be an abelian group under an operation denoted by $+$. Suppose further that $S$ is closed under a commutative, associative law of multiplication denoted by $\cdot$. Say that $\cdot$ distributes over $+$ in the usual way. Finally, for every $s\in S$, suppose there exists some element $t$, not necessarily unique, such that $s\cdot t=s$. Essentially, $S$ is one step removed from being a ring; the only problem is that the multiplicative identity is not unique. Here is an example. Let $S=\{\text{Continuous functions} f: \mathbb{R}\rightarrow \mathbb{R} \ \text{with compact support}\}$ with addition and multiplication defined pointwise. It is clear that this is an abelian group with the necessary law of multiplication. Now, let $f\in S$ be supported on $[a,b]$. Let $S'\subset S$ be the set of continuous functions compactly supported on intervals containing $[a,b]$ that are identically 1 on $[a,b]$. Clearly, if $g\in S'$, then $f\cdot g=f$ for all $x$. Also, there is no unique multiplicative identity in this collection since the constant function 1 is not compactly supported. I've observed that this example is an increasing union of rings, but I don't know if  this holds for every set with the property I've defined.","Let $S$ be an abelian group under an operation denoted by $+$. Suppose further that $S$ is closed under a commutative, associative law of multiplication denoted by $\cdot$. Say that $\cdot$ distributes over $+$ in the usual way. Finally, for every $s\in S$, suppose there exists some element $t$, not necessarily unique, such that $s\cdot t=s$. Essentially, $S$ is one step removed from being a ring; the only problem is that the multiplicative identity is not unique. Here is an example. Let $S=\{\text{Continuous functions} f: \mathbb{R}\rightarrow \mathbb{R} \ \text{with compact support}\}$ with addition and multiplication defined pointwise. It is clear that this is an abelian group with the necessary law of multiplication. Now, let $f\in S$ be supported on $[a,b]$. Let $S'\subset S$ be the set of continuous functions compactly supported on intervals containing $[a,b]$ that are identically 1 on $[a,b]$. Clearly, if $g\in S'$, then $f\cdot g=f$ for all $x$. Also, there is no unique multiplicative identity in this collection since the constant function 1 is not compactly supported. I've observed that this example is an increasing union of rings, but I don't know if  this holds for every set with the property I've defined.",,"['abstract-algebra', 'ring-theory', 'terminology', 'rngs']"
53,"Why is the (-1)-th coefficient of $f^n f'$ equal to 0, without dividing by $n+1$?","Why is the (-1)-th coefficient of  equal to 0, without dividing by ?",f^n f' n+1,"Let $R$ be a commutative ring, and $n$ be a nonnegative integer. Let $f\in R\left[t,t^{-1}\right]$ be a Laurent polynomial in one variable $t$ over $R$ (this means a formal $R$-linear combination of terms of the form $f^m$ with $m\in\mathbb Z$ such that only finitely many of these terms have nonzero coefficients). We let $f^{\prime}$ denote the derivative of $f$ with respect to $t$, defined in the formal way: $f^{\prime} = \sum\limits_{m\in \mathbb Z} mf_m t^{m-1}$, where $f_m$ is the coefficient of $f$ before $t^m$. Theorem: The coefficient of the Laurent polynomial $f^n f^{\prime}$ before $t^{-1}$ is zero. Let me sketch the standard proof of this theorem, to show what I want to avoid: First of all, it is very easy to verify the Theorem in the case $n=0$. Hence, we can apply the Theorem to $f^{n+1}$ and $0$ instead of $f$ and $n$, and conclude that the coefficient of the Laurent polynomial $\left(f^{n+1}\right)^0 \left(f^{n+1}\right)^{\prime}$ before $t^{-1}$ is zero. Since $\left(f^{n+1}\right)^0 \left(f^{n+1}\right)^{\prime} = \left(f^{n+1}\right)^{\prime} = \left(n+1\right)f^n f^{\prime}$ (by the Leibniz identity or the chain rule, as you wish), this yields that the coefficient of the Laurent polynomial $ \left(n+1\right)f^n f^{\prime}$ before $t^{-1}$ is zero. Now, if $n+1$ is not a zero-divisor in $R$, then this immediately yields that the coefficient of the Laurent polynomial $f^n f^{\prime}$ before $t^{-1}$ is zero. Thus, the Theorem is proven in the case when $n+1$ is not a zero-divisor in $R$. In particular, the Theorem is proven in the case when $R$ is a polynomial ring over $\mathbb Z$. But since the statement of the Theorem (for a given value of $n$ and for a given up-degree and low-degree of the Laurent polynomial $f$) is a polynomial identity in the coefficients of $f$, proving it when $R$ is a polynomial ring over $\mathbb Z$ automatically entails that it holds for any commutative ring $R$ (by an elementary fact which has many names, among them ""principle of permanence of identities"" ). Thus, the Theorem is proven. Question: Is there a (not too long or ugly) proof of the Theorem which avoids the use of the principle of permanence of identities? For $n=0$ and $n=1$, the Theorem can be shown by ""expanding"" the polynomial, but this seems to become messy for higher $n$'s. I believe there should be some smart induction-over-$n$ argument (maybe through generalization of the Theorem).","Let $R$ be a commutative ring, and $n$ be a nonnegative integer. Let $f\in R\left[t,t^{-1}\right]$ be a Laurent polynomial in one variable $t$ over $R$ (this means a formal $R$-linear combination of terms of the form $f^m$ with $m\in\mathbb Z$ such that only finitely many of these terms have nonzero coefficients). We let $f^{\prime}$ denote the derivative of $f$ with respect to $t$, defined in the formal way: $f^{\prime} = \sum\limits_{m\in \mathbb Z} mf_m t^{m-1}$, where $f_m$ is the coefficient of $f$ before $t^m$. Theorem: The coefficient of the Laurent polynomial $f^n f^{\prime}$ before $t^{-1}$ is zero. Let me sketch the standard proof of this theorem, to show what I want to avoid: First of all, it is very easy to verify the Theorem in the case $n=0$. Hence, we can apply the Theorem to $f^{n+1}$ and $0$ instead of $f$ and $n$, and conclude that the coefficient of the Laurent polynomial $\left(f^{n+1}\right)^0 \left(f^{n+1}\right)^{\prime}$ before $t^{-1}$ is zero. Since $\left(f^{n+1}\right)^0 \left(f^{n+1}\right)^{\prime} = \left(f^{n+1}\right)^{\prime} = \left(n+1\right)f^n f^{\prime}$ (by the Leibniz identity or the chain rule, as you wish), this yields that the coefficient of the Laurent polynomial $ \left(n+1\right)f^n f^{\prime}$ before $t^{-1}$ is zero. Now, if $n+1$ is not a zero-divisor in $R$, then this immediately yields that the coefficient of the Laurent polynomial $f^n f^{\prime}$ before $t^{-1}$ is zero. Thus, the Theorem is proven in the case when $n+1$ is not a zero-divisor in $R$. In particular, the Theorem is proven in the case when $R$ is a polynomial ring over $\mathbb Z$. But since the statement of the Theorem (for a given value of $n$ and for a given up-degree and low-degree of the Laurent polynomial $f$) is a polynomial identity in the coefficients of $f$, proving it when $R$ is a polynomial ring over $\mathbb Z$ automatically entails that it holds for any commutative ring $R$ (by an elementary fact which has many names, among them ""principle of permanence of identities"" ). Thus, the Theorem is proven. Question: Is there a (not too long or ugly) proof of the Theorem which avoids the use of the principle of permanence of identities? For $n=0$ and $n=1$, the Theorem can be shown by ""expanding"" the polynomial, but this seems to become messy for higher $n$'s. I believe there should be some smart induction-over-$n$ argument (maybe through generalization of the Theorem).",,"['commutative-algebra', 'polynomials', 'residue-calculus', 'abstract-algebra']"
54,A graded ring $R$ is graded-local iff $R_0$ is a local ring?,A graded ring  is graded-local iff  is a local ring?,R R_0,"Update: I've copied this question over to mathoverflow.net: https://mathoverflow.net/questions/100755/a-graded-ring-r-is-graded-local-iff-r-0-is-a-local-ring to see if I get any answers there. Let $R$ be a $\bf{Z}$-graded ring (with no commutativity assumptions whatsoever). Recall that $R$ is ""graded-local"" if the following equivalent conditions hold: $R$ has a unique homogeneous left ideal that is maximal among the proper homogeneous left ideals; $R$ has a unique homogeneous right ideal that is maximal among the proper homogeneous right ideals; $R\neq 0$ and the sum of two homogeneous non-units is again a non-unit. Note, in particular, that (3) tells us that $R_0$, the ring of degree 0 elements, is a local ring. But it seems to me that the converse is true. That is, it seems to me that a $\bf{Z}$-graded ring $R$ is ""graded-local"" if and only if $R_0$ is a local ring. Since this seems a bit suspicious, I have come here to find out if this is really the case. Here is my argument: Let $J^g(R)$ denote the intersection of all the homogeneous left ideals that are maximal among the proper homogeneous left ideals. (This is the ""graded Jacobson radical"". Note that there should be at least one such ""maximal homogeneous left ideal"" because $R\neq 0$). As an intersection of proper homogeneous left ideals, $J^g(R$) is a proper homogeneous left ideal. I claim that it is maximal among the proper homogeneous left ideals, and hence is the unique such ""maximal homogeneous left ideal"". Consider a homogeneous left ideal $I$ with $J^g(R) \subsetneq I$. Since $I$ is homogeneous there exists a homogeneous element $a\in I$ with $a \not\in J^g(R)$. Since $a$ is not in $J^g(R)$ there exists a maximal homogeneous left ideal $\frak{m}$ with $a \not \in \frak{m}$. Well, $R a + \frak{m}$ is a homogeneous left ideal, so by maximality of $\frak{m}$, $1=ra + m$ for some $r\in R$ and $m \in \frak{m}$. Taking the degree 0 components we have $1=r_0a + m_0$ where $m_0$ is a degree zero element in $\frak{m}$ and $r_0a$ is also homogeneous of degree 0. Thus, since $R_0$ is local, either $m_0$ or $r_0a$ is a unit. But $m_0$ cannot be a unit (since it would contradict the properness of $\frak{m}$). Hence $r_0a$ is a unit. In particular, $r_0a$ is left invertible, and thus $a$ is also left invertible. Thus, $I=R$. We conclude that $J^g(R)$ is a ""maximal homogeneous left ideal"" and hence is the unique such. Can you see any problems with this? Is it really true that a $\bf{Z}$-graded ring $R$ is graded-local iff $R_0$ is local? Update: It is clear that an $\bf{N}$-graded ring $R$ is ""graded local"" iff $R_0$ is local. (If ${\frak m}_0$ is the unique maximal left ideal of $R_0$ then ${\frak m} := {\frak m}_0 \oplus \bigoplus_{d>0} R_d$ is the unique maximal left ideal of $R$.) But the (easy) argument for $\bf{N}$-graded rings doesn't work for $\bf{Z}$-graded rings.","Update: I've copied this question over to mathoverflow.net: https://mathoverflow.net/questions/100755/a-graded-ring-r-is-graded-local-iff-r-0-is-a-local-ring to see if I get any answers there. Let $R$ be a $\bf{Z}$-graded ring (with no commutativity assumptions whatsoever). Recall that $R$ is ""graded-local"" if the following equivalent conditions hold: $R$ has a unique homogeneous left ideal that is maximal among the proper homogeneous left ideals; $R$ has a unique homogeneous right ideal that is maximal among the proper homogeneous right ideals; $R\neq 0$ and the sum of two homogeneous non-units is again a non-unit. Note, in particular, that (3) tells us that $R_0$, the ring of degree 0 elements, is a local ring. But it seems to me that the converse is true. That is, it seems to me that a $\bf{Z}$-graded ring $R$ is ""graded-local"" if and only if $R_0$ is a local ring. Since this seems a bit suspicious, I have come here to find out if this is really the case. Here is my argument: Let $J^g(R)$ denote the intersection of all the homogeneous left ideals that are maximal among the proper homogeneous left ideals. (This is the ""graded Jacobson radical"". Note that there should be at least one such ""maximal homogeneous left ideal"" because $R\neq 0$). As an intersection of proper homogeneous left ideals, $J^g(R$) is a proper homogeneous left ideal. I claim that it is maximal among the proper homogeneous left ideals, and hence is the unique such ""maximal homogeneous left ideal"". Consider a homogeneous left ideal $I$ with $J^g(R) \subsetneq I$. Since $I$ is homogeneous there exists a homogeneous element $a\in I$ with $a \not\in J^g(R)$. Since $a$ is not in $J^g(R)$ there exists a maximal homogeneous left ideal $\frak{m}$ with $a \not \in \frak{m}$. Well, $R a + \frak{m}$ is a homogeneous left ideal, so by maximality of $\frak{m}$, $1=ra + m$ for some $r\in R$ and $m \in \frak{m}$. Taking the degree 0 components we have $1=r_0a + m_0$ where $m_0$ is a degree zero element in $\frak{m}$ and $r_0a$ is also homogeneous of degree 0. Thus, since $R_0$ is local, either $m_0$ or $r_0a$ is a unit. But $m_0$ cannot be a unit (since it would contradict the properness of $\frak{m}$). Hence $r_0a$ is a unit. In particular, $r_0a$ is left invertible, and thus $a$ is also left invertible. Thus, $I=R$. We conclude that $J^g(R)$ is a ""maximal homogeneous left ideal"" and hence is the unique such. Can you see any problems with this? Is it really true that a $\bf{Z}$-graded ring $R$ is graded-local iff $R_0$ is local? Update: It is clear that an $\bf{N}$-graded ring $R$ is ""graded local"" iff $R_0$ is local. (If ${\frak m}_0$ is the unique maximal left ideal of $R_0$ then ${\frak m} := {\frak m}_0 \oplus \bigoplus_{d>0} R_d$ is the unique maximal left ideal of $R$.) But the (easy) argument for $\bf{N}$-graded rings doesn't work for $\bf{Z}$-graded rings.",,"['abstract-algebra', 'ring-theory', 'noncommutative-algebra']"
55,Showing a Functor is not Representable,Showing a Functor is not Representable,,"I have this old qual problem that I don't know how to do: So let $ F: Rings \to Sets$ given by $R \mapsto \{(a,b) \in R^2, aR + bR =  R\}$ . The action on the morphism is the obvious one. Now the problem asks to show that the functor is not representable. So here is what I found on the internet: According to the method on the second page of this pdf http://pi.math.cornell.edu/~zbnorwood/ucla/files/repfunctors.pdf , suppose $F  = Hom(A, \_)$ , then there is $(a, b) \in A^2, aA + bA =A$ such that for $B$ a ring and $(x, y) \in B^2, xB+ yB = B$ then there exists unique ring map $f: A \to B$ such that $f(a) = x, f(b) = y$ . I don't know how to reach a contradiction from there. Or does anyone have any great method for proving a not representable functor is not representable in general? I find this type of problems really hard.","I have this old qual problem that I don't know how to do: So let given by . The action on the morphism is the obvious one. Now the problem asks to show that the functor is not representable. So here is what I found on the internet: According to the method on the second page of this pdf http://pi.math.cornell.edu/~zbnorwood/ucla/files/repfunctors.pdf , suppose , then there is such that for a ring and then there exists unique ring map such that . I don't know how to reach a contradiction from there. Or does anyone have any great method for proving a not representable functor is not representable in general? I find this type of problems really hard."," F: Rings \to Sets R \mapsto \{(a,b) \in R^2, aR + bR =  R\} F  = Hom(A, \_) (a, b) \in A^2, aA + bA =A B (x, y) \in B^2, xB+ yB = B f: A \to B f(a) = x, f(b) = y","['abstract-algebra', 'ring-theory', 'category-theory']"
56,Central extensions versus semidirect products,Central extensions versus semidirect products,,"Consider an extension $E$ of a group $G$ by an abelian group $A$. $$1 \to A \overset{\iota}{\to} E \overset{\pi}{\to} G \to 1$$ Two special kinds of extensions are: Central Extensions:  $A$ is contained in the centre of $E$. Semidirect products: $\pi$ has a section, i.e. a homomorphism $s : G \to E$ with $\pi \circ s = \mathrm{id}_G$. It perhaps reasonable to think of these two types of extensions as ""orthogonal"" since an extension is both central and a semidirect product if and only if it is split, i.e. there is an isomorphism $E \cong A \times G$ through which $ \iota$ and $\pi$ become identified with the standard inclusion and projection. These two types of extensions are nice in the sense that we can construct all of them in terms of certain external data, namely 2-cocycles and actions. Given $\psi:G \times G \to A$ satisfying $\psi(g_1,g_2)  \psi(g_1 g_2,g_3) = \psi(g_1,g_2 g_3)\psi(g_2,g_3),$ we can devise an extension $E_\psi = A\times G$ with product $(a_1,g_1)(a_2,g_2)=(a_1a_2\psi(g_1,g_2),g_1g_2),$ and $\iota,\pi$ given by the standard inclusion and projection. Given a homomorphism $\theta:G \to \mathrm{Aut}(A)$, define $E=A \rtimes_\theta G$ to be $A \times G$ with product $(a_1,g_1)(a_2,g_2) = (a_1 \theta_{g_1}(a_2),g_1g_2)$. Since the cases of central extensions and semi-direct product are somehow ""orthogonal"", I am tempted to ask the following ill-defined question: Main Question: Can we think of all extensions $E$ of a group $G$ by an abelian group $A$ as being somehow built out of these two orthogonal cases of central extensions and semidirect products? and maybe Followup Question 1: Can arbitrary extensions be constructed out of external data in the same way as central extensions and semidirect products? The data would need to be something which mixes the notions of a 2-cocycle and an action. One further question, a bit frivolous, just popped into mind. Followup question 2: If we want to study extensions $A \to E \to G$ where $A$ is nonabelian , we can still talk about the extensions which are semidirect products. However, the notion of a central extension no long makes any sense. Is there a property $(P)$ which is an appropriate analogue of central extension in this context? Can $(P)$ be chosen so that the trivial split extension is the only extensions which satisfies $(P)$ and is also a semidirect product?","Consider an extension $E$ of a group $G$ by an abelian group $A$. $$1 \to A \overset{\iota}{\to} E \overset{\pi}{\to} G \to 1$$ Two special kinds of extensions are: Central Extensions:  $A$ is contained in the centre of $E$. Semidirect products: $\pi$ has a section, i.e. a homomorphism $s : G \to E$ with $\pi \circ s = \mathrm{id}_G$. It perhaps reasonable to think of these two types of extensions as ""orthogonal"" since an extension is both central and a semidirect product if and only if it is split, i.e. there is an isomorphism $E \cong A \times G$ through which $ \iota$ and $\pi$ become identified with the standard inclusion and projection. These two types of extensions are nice in the sense that we can construct all of them in terms of certain external data, namely 2-cocycles and actions. Given $\psi:G \times G \to A$ satisfying $\psi(g_1,g_2)  \psi(g_1 g_2,g_3) = \psi(g_1,g_2 g_3)\psi(g_2,g_3),$ we can devise an extension $E_\psi = A\times G$ with product $(a_1,g_1)(a_2,g_2)=(a_1a_2\psi(g_1,g_2),g_1g_2),$ and $\iota,\pi$ given by the standard inclusion and projection. Given a homomorphism $\theta:G \to \mathrm{Aut}(A)$, define $E=A \rtimes_\theta G$ to be $A \times G$ with product $(a_1,g_1)(a_2,g_2) = (a_1 \theta_{g_1}(a_2),g_1g_2)$. Since the cases of central extensions and semi-direct product are somehow ""orthogonal"", I am tempted to ask the following ill-defined question: Main Question: Can we think of all extensions $E$ of a group $G$ by an abelian group $A$ as being somehow built out of these two orthogonal cases of central extensions and semidirect products? and maybe Followup Question 1: Can arbitrary extensions be constructed out of external data in the same way as central extensions and semidirect products? The data would need to be something which mixes the notions of a 2-cocycle and an action. One further question, a bit frivolous, just popped into mind. Followup question 2: If we want to study extensions $A \to E \to G$ where $A$ is nonabelian , we can still talk about the extensions which are semidirect products. However, the notion of a central extension no long makes any sense. Is there a property $(P)$ which is an appropriate analogue of central extension in this context? Can $(P)$ be chosen so that the trivial split extension is the only extensions which satisfies $(P)$ and is also a semidirect product?",,"['abstract-algebra', 'group-theory', 'semidirect-product', 'group-extensions', 'central-extensions']"
57,Interview preparation for Ph.D admission,Interview preparation for Ph.D admission,,"I have recently gave a written exam for admission to Ph.D program  to an institute in India. I have done that exam well and hoping for an interview call. I would like to know what could be type of questions the panel would ask for a student to confirm that i am fit for them.. I am interested in Algebra so they would start with Algebra and then my choice would be  topology Usually the syllabus for algebra which almost all Indian universities would cover is : Algebra : Group theory : Group actions, Automorphisms, Sylow theorems, Direct Products,finitely generated abelian groups. More or less we are expected to know first five chapters of Dummit Foote Abstract Algebra Book. Ring Theory : euclidean Domains, P.I.D., U.F.D., Polynomial rings, Irreducibility criterion.More or less we are expected to know all three chapters of Ring Theory part of Dummit Foote Abstract Algebra Book. Field and Galois theory : Algebraic extensions, splitting fields, seperable extensions, cyclotomic polynomials, fundemental theorem of galois theory cyclotomic extensions, Insolvability of quintic and more or less Chapter $13$ and chapter $14$ (except transcendental extensions, Inseperable extensions, infinite galois groups)of dummit foote  Abstract Algebra Book. Topology : We are expected to know connectedness, compactness, countability and seperation axioms, Compactness/Completeness of metric spaces and more or less first four chapters and seventh chapter of Topology book by Munkres. I have tried to solve most of the problems in what all i mentioned for algebra part but then as i have only one month time i am not able to decide how should to start the preparation. should i just go on solving all those questions again or do something else? I tried to see http://web.math.princeton.edu/generals/ to get some idea but then it is much more advanced.. So, I would request you to suggest me to some way to prepare for the interview... May be by posting some interesting questions which are answerable in less than ten minutes... For example : give an example of an extension whose galois group is $S_3$ or $S_4$ Computing galois group of some polynomials over $\mathbb{Q}$ Please help me to collect such problems. Thank you.","I have recently gave a written exam for admission to Ph.D program  to an institute in India. I have done that exam well and hoping for an interview call. I would like to know what could be type of questions the panel would ask for a student to confirm that i am fit for them.. I am interested in Algebra so they would start with Algebra and then my choice would be  topology Usually the syllabus for algebra which almost all Indian universities would cover is : Algebra : Group theory : Group actions, Automorphisms, Sylow theorems, Direct Products,finitely generated abelian groups. More or less we are expected to know first five chapters of Dummit Foote Abstract Algebra Book. Ring Theory : euclidean Domains, P.I.D., U.F.D., Polynomial rings, Irreducibility criterion.More or less we are expected to know all three chapters of Ring Theory part of Dummit Foote Abstract Algebra Book. Field and Galois theory : Algebraic extensions, splitting fields, seperable extensions, cyclotomic polynomials, fundemental theorem of galois theory cyclotomic extensions, Insolvability of quintic and more or less Chapter $13$ and chapter $14$ (except transcendental extensions, Inseperable extensions, infinite galois groups)of dummit foote  Abstract Algebra Book. Topology : We are expected to know connectedness, compactness, countability and seperation axioms, Compactness/Completeness of metric spaces and more or less first four chapters and seventh chapter of Topology book by Munkres. I have tried to solve most of the problems in what all i mentioned for algebra part but then as i have only one month time i am not able to decide how should to start the preparation. should i just go on solving all those questions again or do something else? I tried to see http://web.math.princeton.edu/generals/ to get some idea but then it is much more advanced.. So, I would request you to suggest me to some way to prepare for the interview... May be by posting some interesting questions which are answerable in less than ten minutes... For example : give an example of an extension whose galois group is $S_3$ or $S_4$ Computing galois group of some polynomials over $\mathbb{Q}$ Please help me to collect such problems. Thank you.",,"['abstract-algebra', 'general-topology']"
58,How many $\alpha \in S_n$ are such that $\alpha^2 = 1$?,How many  are such that ?,\alpha \in S_n \alpha^2 = 1,"This is not for homework, but I am not great at counting arguments and would like some feedback.  The question asks Let $n \in \mathbb{N}$ . How many $\alpha \in S_n$ are there such that $\alpha^2 = 1$ ? I know that, if $\alpha^2 = 1$ , then either $\alpha = 1$ or $\alpha$ is the product of disjoint transpositions. If $\alpha = (i, j)$ is a single transposition, then there are $\frac{1}{2^1 \cdot 1!} \binom{n}{2}$ such $\alpha$ (the $2^1$ and $1!$ are put in the denominator to help in noticing the pattern later). If $\alpha = (i, j)(k, l)$ is the product of $2$ disjoint transpositions, then there are $\frac{1}{2^2 \cdot 2!} \binom{n}{2} \binom{n-2}{2}$ such $\alpha$ , where the $2^2$ appears in the denominator to account for the cyclic permutations of each transposition, and the $2!$ appears to account for the permutation of the transpositions themselves. If $\alpha$ is the product of $3$ disjoint transpositions, then there are $\frac{1}{2^3 \cdot 3!} \binom{n}{2} \binom{n-2}{2} \binom{n-4}{2}$ such $\alpha$ . Extrapolating from this, I find that the total number of $\alpha \in S_n$ such that $\alpha^2 = 1$ is $$ 1 + \sum_{i=1}^{\lfloor \frac{n}{2} \rfloor} \frac{1}{2^i \cdot i!} \prod_{k=0}^{i-1} \binom{n-2k}{2}. $$ Does this look OK?  It looks like a rather ugly answer to me, so I have my doubts.  Any input would be welcomed.","This is not for homework, but I am not great at counting arguments and would like some feedback.  The question asks Let . How many are there such that ? I know that, if , then either or is the product of disjoint transpositions. If is a single transposition, then there are such (the and are put in the denominator to help in noticing the pattern later). If is the product of disjoint transpositions, then there are such , where the appears in the denominator to account for the cyclic permutations of each transposition, and the appears to account for the permutation of the transpositions themselves. If is the product of disjoint transpositions, then there are such . Extrapolating from this, I find that the total number of such that is Does this look OK?  It looks like a rather ugly answer to me, so I have my doubts.  Any input would be welcomed.","n \in \mathbb{N} \alpha \in S_n \alpha^2 = 1 \alpha^2 = 1 \alpha = 1 \alpha \alpha = (i, j) \frac{1}{2^1 \cdot 1!} \binom{n}{2} \alpha 2^1 1! \alpha = (i, j)(k, l) 2 \frac{1}{2^2 \cdot 2!} \binom{n}{2} \binom{n-2}{2} \alpha 2^2 2! \alpha 3 \frac{1}{2^3 \cdot 3!} \binom{n}{2} \binom{n-2}{2} \binom{n-4}{2} \alpha \alpha \in S_n \alpha^2 = 1  1 + \sum_{i=1}^{\lfloor \frac{n}{2} \rfloor} \frac{1}{2^i \cdot i!} \prod_{k=0}^{i-1} \binom{n-2k}{2}. ","['abstract-algebra', 'combinatorics', 'group-theory', 'permutations', 'symmetric-groups']"
59,The Galois group of a composite of Galois extensions,The Galois group of a composite of Galois extensions,,"Morandi's Field and Galois Theory, exercise 5.19b Let $K$ and $L$ be Galois extensions of $F$ . The restriction of function map, namely, $\sigma\mapsto(\sigma\vert_K,\sigma\vert_L)$ induces an injective group homomorphism $\varphi\colon\operatorname{Gal}(KL/F)\to\operatorname{Gal}(K/F)\times\operatorname{Gal}(L/F)$ . Show that $\varphi$ is surjective if and only if $K\cap L=F$ . It's not hard to show that $\varphi$ is a monomorphism. If it's surjective, it's not hard to show that $K\cap L=F$ as follow: Fix $\alpha\in K\cap L$ , let $\beta$ be a root of the minimal polynomial of $\alpha$ over $F$ . Since $K,L$ are normal, $\beta\in K\cap L$ . By isomorphism extension theorem, we can choose $\tau_1\in\operatorname{Gal}(K/F)$ such that $\tau_1(\alpha)=\beta$ . For surjectivity of the map, there's $\sigma$ such that $\sigma\vert_K=\tau_1$ and $\sigma_L=\mathrm{id}$ , which forces $\alpha=\beta$ , therefore $\alpha\in F$ , since $K,L$ are separable over $F$ . The converse seems hard. I cannot show that when $K,L$ are arbitrary Galois extensions. If they are both finite dimensional, the statement follows from natural irrationality: $\operatorname{Gal}(KL/L)\cong\operatorname{Gal}(K/K\cap L)$ , which implies that $[KL:L]=[K:K\cap L]=[K:F]$ , therefore $[KL:F]=[K:F][L:F]$ , and note that $\varphi$ is injective, thus surjective. Any help? Thanks!","Morandi's Field and Galois Theory, exercise 5.19b Let and be Galois extensions of . The restriction of function map, namely, induces an injective group homomorphism . Show that is surjective if and only if . It's not hard to show that is a monomorphism. If it's surjective, it's not hard to show that as follow: Fix , let be a root of the minimal polynomial of over . Since are normal, . By isomorphism extension theorem, we can choose such that . For surjectivity of the map, there's such that and , which forces , therefore , since are separable over . The converse seems hard. I cannot show that when are arbitrary Galois extensions. If they are both finite dimensional, the statement follows from natural irrationality: , which implies that , therefore , and note that is injective, thus surjective. Any help? Thanks!","K L F \sigma\mapsto(\sigma\vert_K,\sigma\vert_L) \varphi\colon\operatorname{Gal}(KL/F)\to\operatorname{Gal}(K/F)\times\operatorname{Gal}(L/F) \varphi K\cap L=F \varphi K\cap L=F \alpha\in K\cap L \beta \alpha F K,L \beta\in K\cap L \tau_1\in\operatorname{Gal}(K/F) \tau_1(\alpha)=\beta \sigma \sigma\vert_K=\tau_1 \sigma_L=\mathrm{id} \alpha=\beta \alpha\in F K,L F K,L \operatorname{Gal}(KL/L)\cong\operatorname{Gal}(K/K\cap L) [KL:L]=[K:K\cap L]=[K:F] [KL:F]=[K:F][L:F] \varphi","['abstract-algebra', 'field-theory', 'galois-theory']"
60,"$K/E$, $E/F$ are separable field extensions $\implies$ $K/F $ is separable",",  are separable field extensions   is separable",K/E E/F \implies K/F ,"I wish to prove that if $K/E$, $E/F$ are algebraic separable field extensions then $K/F$ is separable. I tried taking $a\in K$ and said that if $a\in E$ it is clear, otherwise I looked at the minimal polynomials over $E,F$ and called them $f(x),g(x)$ respectively, then I said that $f(x)$ is separable and since there exist $h(x)$ s.t $f(x)h(x)=g(x)$ it remains to see that $h(x)$ is separable — here I am stuck. Can someone please help proving this claim or can say how to continue ?","I wish to prove that if $K/E$, $E/F$ are algebraic separable field extensions then $K/F$ is separable. I tried taking $a\in K$ and said that if $a\in E$ it is clear, otherwise I looked at the minimal polynomials over $E,F$ and called them $f(x),g(x)$ respectively, then I said that $f(x)$ is separable and since there exist $h(x)$ s.t $f(x)h(x)=g(x)$ it remains to see that $h(x)$ is separable — here I am stuck. Can someone please help proving this claim or can say how to continue ?",,"['abstract-algebra', 'field-theory', 'galois-theory']"
61,"Is it possible to construct $GF(4)$ with two different multiplication operations mod n and mod m as ""addition"" and ""multiplication""?","Is it possible to construct  with two different multiplication operations mod n and mod m as ""addition"" and ""multiplication""?",GF(4),"I found that it's possible to construct multiplication and addition tables for $GF(2)$ and $GF(3)$ in that way, but I still can't find ones for $GF(4)$ . Is it possible at all? $GF(2)$ can be constructed with elements $\{21, 7\}$ and operations $* \bmod42$ and $* \bmod28$ $GF(2)$ multiplication: \begin{array}{c|cc} \ * \bmod42 & 21 & 7  \\  \hline 21 & 21 & 21  \\  7 & 21 & 7  \\  \end{array} $GF(2)$ addition: \begin{array}{c|cc} \ * \bmod28 & 21 & 7  \\  \hline 21 & 21 & 7  \\  7 & 7 & 21  \\  \end{array} $GF(3)$ can be constructed with elements $\{10, 16, 4\}$ and operations $* \bmod30$ and $* \bmod18$ $GF(3)$ multiplication: \begin{array}{c|ccc} \ * \bmod30 & 10 & 16 & 4    \\  \hline 10 & 10 & 10 & 10   \\  16 & 10 & 16 & 4\\ 4  & 10 & 4 & 16\\  \end{array} $GF(3)$ addition: \begin{array}{c|ccc} \ * \bmod 18 & 10 & 16 & 4    \\  \hline 10 & 10 & 16 & 4   \\  16 & 16 & 4 & 10\\ 4  & 4 & 10 & 16\\  \end{array} [Edit] Adding the following for clarity because users well versed in algebra can make mistakes here. Judging from the OP's examples and comments, mod should be the binary mod . In other words, $ab\bmod A$ is the remainder of the product $ab$ when divided by a positive integer $A$ . This translates to the usual congruence only when both moduli are larger than all the elements of the ""field"", JL [/Edit]","I found that it's possible to construct multiplication and addition tables for and in that way, but I still can't find ones for . Is it possible at all? can be constructed with elements and operations and multiplication: addition: can be constructed with elements and operations and multiplication: addition: [Edit] Adding the following for clarity because users well versed in algebra can make mistakes here. Judging from the OP's examples and comments, mod should be the binary mod . In other words, is the remainder of the product when divided by a positive integer . This translates to the usual congruence only when both moduli are larger than all the elements of the ""field"", JL [/Edit]","GF(2) GF(3) GF(4) GF(2) \{21, 7\} * \bmod42 * \bmod28 GF(2) \begin{array}{c|cc}
\ * \bmod42 & 21 & 7  \\ 
\hline
21 & 21 & 21  \\ 
7 & 21 & 7  \\ 
\end{array} GF(2) \begin{array}{c|cc}
\ * \bmod28 & 21 & 7  \\ 
\hline
21 & 21 & 7  \\ 
7 & 7 & 21  \\ 
\end{array} GF(3) \{10, 16, 4\} * \bmod30 * \bmod18 GF(3) \begin{array}{c|ccc}
\ * \bmod30 & 10 & 16 & 4    \\ 
\hline
10 & 10 & 10 & 10   \\ 
16 & 10 & 16 & 4\\
4  & 10 & 4 & 16\\ 
\end{array} GF(3) \begin{array}{c|ccc}
\ * \bmod 18 & 10 & 16 & 4    \\ 
\hline
10 & 10 & 16 & 4   \\ 
16 & 16 & 4 & 10\\
4  & 4 & 10 & 16\\ 
\end{array} ab\bmod A ab A","['abstract-algebra', 'finite-fields']"
62,Is every submodule of cyclic module over PID cyclic?,Is every submodule of cyclic module over PID cyclic?,,"I needed to check if any subgroup of cyclic abelian group is cyclic and successfully proved that ""yes"" using the fact that $\mathbb{Z}$ is Euclidian domain. It's easy to give an example that it doesn't hold for modules over arbitrary ring, but I am pretty sure that it does for PID. How can I prove it?","I needed to check if any subgroup of cyclic abelian group is cyclic and successfully proved that ""yes"" using the fact that $\mathbb{Z}$ is Euclidian domain. It's easy to give an example that it doesn't hold for modules over arbitrary ring, but I am pretty sure that it does for PID. How can I prove it?",,"['abstract-algebra', 'modules']"
63,Why are metrics defined as functions in $\mathbb{R}^{+}$?,Why are metrics defined as functions in ?,\mathbb{R}^{+},"A metric on a set $S$ is a function $d: S^2 \to \mathbb{R}^{+}$ that is symmetric, sub-additive, non-negative, and takes $(x,y)$ to $0$ iff $x=y$. My question is: what makes $\mathbb{R}^{+}$ so special that metrics are universally defined in terms of it? Why don't we use some other totally-ordered set with a least element $m$ and an abelian operator $+$ which preserves order and under which $m$ is neutral? To take it one step further, why aren't metrics defined, more generally, to map elements of $S^2$ to any totally-ordered set that satisfies these conditions?","A metric on a set $S$ is a function $d: S^2 \to \mathbb{R}^{+}$ that is symmetric, sub-additive, non-negative, and takes $(x,y)$ to $0$ iff $x=y$. My question is: what makes $\mathbb{R}^{+}$ so special that metrics are universally defined in terms of it? Why don't we use some other totally-ordered set with a least element $m$ and an abelian operator $+$ which preserves order and under which $m$ is neutral? To take it one step further, why aren't metrics defined, more generally, to map elements of $S^2$ to any totally-ordered set that satisfies these conditions?",,"['abstract-algebra', 'general-topology', 'metric-spaces']"
64,Show that $A[X]/(aX+b)$ is an integral domain,Show that  is an integral domain,A[X]/(aX+b),"Let $A$ be an integral domain, $a$ and $b \in A-\{0\}$, and let $B = A[X]/(aX+b)$. Show that, if $Aa \cap Ab=Aab$, then $B$ is an integral domain. My attempt at proof (following a hint). Denote by $K$ the field of fractions of $A$. Let $\phi: A[X] \to K$, with $\phi(X)=-b/a$ and $\phi(y)=y, y \in A$. Then $\phi(aX+b)=-b+b=0$. If $p(X) \in A[X]$ and $p(X) \not \in (aX+b)$, then $p(X)=q(X)(aX+b)+r$, $r \in A$. Therefore, $\phi(p(X))=\phi(r)=r$, so $p(X) \in \ker(\phi) \iff r=0 \iff p(X) \in (aX+b)$. Hence $\ker(\phi)=(aX+b)$; by the first isomorphism theorem, $A[X]/(aX+b)$ must be isomorphic to Im$(\phi)$. In conclusion $B$ is isomorphic to a subfield of $K$, so it's an integral domain. The problem is, I don't know where I used the condition $Aa \cap Ab=Aab$. Am I doing something wrong?","Let $A$ be an integral domain, $a$ and $b \in A-\{0\}$, and let $B = A[X]/(aX+b)$. Show that, if $Aa \cap Ab=Aab$, then $B$ is an integral domain. My attempt at proof (following a hint). Denote by $K$ the field of fractions of $A$. Let $\phi: A[X] \to K$, with $\phi(X)=-b/a$ and $\phi(y)=y, y \in A$. Then $\phi(aX+b)=-b+b=0$. If $p(X) \in A[X]$ and $p(X) \not \in (aX+b)$, then $p(X)=q(X)(aX+b)+r$, $r \in A$. Therefore, $\phi(p(X))=\phi(r)=r$, so $p(X) \in \ker(\phi) \iff r=0 \iff p(X) \in (aX+b)$. Hence $\ker(\phi)=(aX+b)$; by the first isomorphism theorem, $A[X]/(aX+b)$ must be isomorphic to Im$(\phi)$. In conclusion $B$ is isomorphic to a subfield of $K$, so it's an integral domain. The problem is, I don't know where I used the condition $Aa \cap Ab=Aab$. Am I doing something wrong?",,"['abstract-algebra', 'polynomials', 'ring-theory', 'divisibility', 'integral-domain']"
65,The infinite Direct Sum in the category Ring,The infinite Direct Sum in the category Ring,,"If you don't have strong personal feelings about it already, most of you have at least witnessed the opposing factions on how we should define a ring and, by extension, how we should define a ring homomorphism. The strong majority define a ring to have a unity and require that ring homomorphisms preserve it. Others do not require these things of a ring. The general pattern I've seen which determines into which camp any given mathematician falls is how 'good' they want their category of rings to be. Those who want zero morphisms and want to be able to define the kernel of a morphism categorically often opt to go without unity. There is a printed semi-famous comment about this problem in the Radical Theory of Rings by Gardner and Wiegandt: This is where my question begins. I have searched for why the category of Rings (with unity) does not have infinite coproducts. And basically everywhere that comments on it does not prove that there is no object in Ring which satisfies the universal property of a coproduct for an infinite family . What they do explain is that the thing we would expect to be the coproduct (namely the set-theoretic direct sum with pointwise addition and multiplication) is not a coproduct in Ring . I have attempted to prove the non-existence of such an object for myself but have not gotten far. One would have to show that either every infinite family of rings cannot all be embedded in another ring; or in the case that there is such an object, there is always a 'smaller' (non-isomorphic) object in Ring which has the same property. Thus my question is can someone prove to me that Ring (the category of rings with unity and unity preserving homomorphisms) does not have infinite coproducts? Or can you direct me to a reference that may have my answer?","If you don't have strong personal feelings about it already, most of you have at least witnessed the opposing factions on how we should define a ring and, by extension, how we should define a ring homomorphism. The strong majority define a ring to have a unity and require that ring homomorphisms preserve it. Others do not require these things of a ring. The general pattern I've seen which determines into which camp any given mathematician falls is how 'good' they want their category of rings to be. Those who want zero morphisms and want to be able to define the kernel of a morphism categorically often opt to go without unity. There is a printed semi-famous comment about this problem in the Radical Theory of Rings by Gardner and Wiegandt: This is where my question begins. I have searched for why the category of Rings (with unity) does not have infinite coproducts. And basically everywhere that comments on it does not prove that there is no object in Ring which satisfies the universal property of a coproduct for an infinite family . What they do explain is that the thing we would expect to be the coproduct (namely the set-theoretic direct sum with pointwise addition and multiplication) is not a coproduct in Ring . I have attempted to prove the non-existence of such an object for myself but have not gotten far. One would have to show that either every infinite family of rings cannot all be embedded in another ring; or in the case that there is such an object, there is always a 'smaller' (non-isomorphic) object in Ring which has the same property. Thus my question is can someone prove to me that Ring (the category of rings with unity and unity preserving homomorphisms) does not have infinite coproducts? Or can you direct me to a reference that may have my answer?",,"['abstract-algebra', 'reference-request']"
66,Rings that are isomorphic to the endomorphism ring of their additive group.,Rings that are isomorphic to the endomorphism ring of their additive group.,,"Every ring is isomorphic to a subring of the endomorphism ring of it's underlying group. That's Cayley's theorem for rings . What can we say about rings that are isomorphic to the endomorphism ring of their underlying additive group? (are they always integral domains? ADDED: no, $Z/nZ$ is a counter example. from the comment by egreg)","Every ring is isomorphic to a subring of the endomorphism ring of it's underlying group. That's Cayley's theorem for rings . What can we say about rings that are isomorphic to the endomorphism ring of their underlying additive group? (are they always integral domains? ADDED: no, $Z/nZ$ is a counter example. from the comment by egreg)",,['abstract-algebra']
67,"What is the smallest $n$ for which the usual ""counting sizes of conjugacy classes"" proof of simplicity fails for $A_n$?","What is the smallest  for which the usual ""counting sizes of conjugacy classes"" proof of simplicity fails for ?",n A_n,"A common proof of the simplicity of $A_5$ proceeds as follows. First, note that a normal subgroup is always a union of conjugacy classes. Also, a subgroup has order dividing the size of the group by Lagrange's theorem. Any normal subgroup will contain the conjugacy class with the identity. Now if we look at unions of conjugacy classes of $A_5$ where we include $\{(1)\}$ in the union, there only two cases where the size of the union divides the order $A_5$. These are the cases where the union is all of $A_5$ or just $\{(1)\}$. Hence $A_5$ must be simple. This same proof also works for $A_6$. But in general we cannot deduce the simplicity of $A_n$ for $n \geq 5$ with the same proof. According to this article it turns out that for example,  $1 + \frac{n(n-1)(n-2)}{3}$ divides $n!/2$ when $n = 68$. Note that $\frac{n(n-1)(n-2)}{3}$ is the size of the conjugacy class of $3$-cycles in $A_n$ when $n \geq 5$. Question: Is $n = 68$ the smallest positive integer where the proof fails? In the article they say that this ""seems to be the smallest counterexample"". However, counting all the possible sums of conjugacy classes of $A_n$ becomes computationally difficult very quickly (as is noted in the article). Hence actually demonstrating this might be hard.","A common proof of the simplicity of $A_5$ proceeds as follows. First, note that a normal subgroup is always a union of conjugacy classes. Also, a subgroup has order dividing the size of the group by Lagrange's theorem. Any normal subgroup will contain the conjugacy class with the identity. Now if we look at unions of conjugacy classes of $A_5$ where we include $\{(1)\}$ in the union, there only two cases where the size of the union divides the order $A_5$. These are the cases where the union is all of $A_5$ or just $\{(1)\}$. Hence $A_5$ must be simple. This same proof also works for $A_6$. But in general we cannot deduce the simplicity of $A_n$ for $n \geq 5$ with the same proof. According to this article it turns out that for example,  $1 + \frac{n(n-1)(n-2)}{3}$ divides $n!/2$ when $n = 68$. Note that $\frac{n(n-1)(n-2)}{3}$ is the size of the conjugacy class of $3$-cycles in $A_n$ when $n \geq 5$. Question: Is $n = 68$ the smallest positive integer where the proof fails? In the article they say that this ""seems to be the smallest counterexample"". However, counting all the possible sums of conjugacy classes of $A_n$ becomes computationally difficult very quickly (as is noted in the article). Hence actually demonstrating this might be hard.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'symmetric-groups']"
68,Is $\Bbb Z[\sqrt{6}]$ a UFD?,Is  a UFD?,\Bbb Z[\sqrt{6}],"I want to show that $\Bbb Z[\sqrt{6}]$ is not a UFD. We know that this is a Dedekind domain. (Hint: For the ideals $\mathfrak{p}=(2,4+\sqrt{6})$ , $\mathfrak{q}=(5,4+\sqrt{6})$ , compute $\mathfrak{p}^2$ , $\mathfrak{q}\bar{\mathfrak{q}}$ , $\mathfrak{p}\mathfrak{q}$ and $\mathfrak{p}\bar{\mathfrak{q}}$ .) Note that $\mathfrak{p}^2=(4,8+2\sqrt{6},22+8\sqrt{6})$ , $\mathfrak{q}\bar{\mathfrak{q}}=(5, 20-5\sqrt{6},20+5\sqrt{6})$ , $\mathfrak{p}\mathfrak{q}=(10,8+2\sqrt{6},20+5\sqrt{6},22+8\sqrt{6})$ , and $\mathfrak{p}\bar{\mathfrak{q}}=(10,8-2\sqrt{6},20+5\sqrt{6})$ . Also, we observe that $10=(4+\sqrt{6})(4-\sqrt{6})=(-1+\sqrt{6})(2+\sqrt{6})(1+\sqrt{6})(-2+\sqrt{6})$ and $10=2.5=(2+\sqrt{6})(-2+\sqrt{6})(-1+\sqrt{6})(1+\sqrt{6})$ . But, this doesn't imply that $\Bbb Z[\sqrt{6}]$ is not a UFD. I am not sure how to use the hint. Thanks!","I want to show that is not a UFD. We know that this is a Dedekind domain. (Hint: For the ideals , , compute , , and .) Note that , , , and . Also, we observe that and . But, this doesn't imply that is not a UFD. I am not sure how to use the hint. Thanks!","\Bbb Z[\sqrt{6}] \mathfrak{p}=(2,4+\sqrt{6}) \mathfrak{q}=(5,4+\sqrt{6}) \mathfrak{p}^2 \mathfrak{q}\bar{\mathfrak{q}} \mathfrak{p}\mathfrak{q} \mathfrak{p}\bar{\mathfrak{q}} \mathfrak{p}^2=(4,8+2\sqrt{6},22+8\sqrt{6}) \mathfrak{q}\bar{\mathfrak{q}}=(5, 20-5\sqrt{6},20+5\sqrt{6}) \mathfrak{p}\mathfrak{q}=(10,8+2\sqrt{6},20+5\sqrt{6},22+8\sqrt{6}) \mathfrak{p}\bar{\mathfrak{q}}=(10,8-2\sqrt{6},20+5\sqrt{6}) 10=(4+\sqrt{6})(4-\sqrt{6})=(-1+\sqrt{6})(2+\sqrt{6})(1+\sqrt{6})(-2+\sqrt{6}) 10=2.5=(2+\sqrt{6})(-2+\sqrt{6})(-1+\sqrt{6})(1+\sqrt{6}) \Bbb Z[\sqrt{6}]","['abstract-algebra', 'ring-theory', 'algebraic-number-theory']"
69,"Examples of calculus on ""strange"" spaces","Examples of calculus on ""strange"" spaces",,"I am interested in examples of calculus on ""strange"" spaces. For example, you can take the derivative of a regular expression[1][2]. Also the concept extends past regular languages, to more general formal languages[3]. You can also do calculus on abstract data types, here is an example in Haskell[4]. Differential equations are type-inference equations. You can also taylor-expand types[5]. I am looking at more examples of this. Note that I am interested where calculus is similar enough to ""normal"" calculi (e.g. calculus on functions of complex variables, functional calculus, etc). At least operators must be linear, for example the arithmetic derivative is not interesting to me because the operators are not linear. The examples I gave are all from computer-science, but I am interested in more general answers. Brzozowski: Derivatives of Regular Expressions Owens: Regular-expression derivatives reexamined Might: Parsing with Derivatives The Algebra of Algebraic Data Types, Part 3 The Algebra of Algebraic Data Types, Part 2","I am interested in examples of calculus on ""strange"" spaces. For example, you can take the derivative of a regular expression[1][2]. Also the concept extends past regular languages, to more general formal languages[3]. You can also do calculus on abstract data types, here is an example in Haskell[4]. Differential equations are type-inference equations. You can also taylor-expand types[5]. I am looking at more examples of this. Note that I am interested where calculus is similar enough to ""normal"" calculi (e.g. calculus on functions of complex variables, functional calculus, etc). At least operators must be linear, for example the arithmetic derivative is not interesting to me because the operators are not linear. The examples I gave are all from computer-science, but I am interested in more general answers. Brzozowski: Derivatives of Regular Expressions Owens: Regular-expression derivatives reexamined Might: Parsing with Derivatives The Algebra of Algebraic Data Types, Part 3 The Algebra of Algebraic Data Types, Part 2",,"['abstract-algebra', 'analysis', 'soft-question', 'computer-science', 'recreational-mathematics']"
70,Prove that any subfield of $\mathbb C$ must contain $\mathbb Q$,Prove that any subfield of  must contain,\mathbb C \mathbb Q,"I just started reading Linear Algebra by Hoffman and Kunze, and I came across the following line: The interested reader should verify that any subfield of $\mathbb C$   must contain every rational number. Of course, I am interested, so I tried to come up with a proof. I just need verification on the correctness of my proof, and any feedback on how I can improve it. So here it goes. Proof A subfield is simply a subset of a field. A field $\mathbb F$ must obey the following axioms: 1. Addition is commutative, i.e. $x + y = y + x$ for all $x, y$ in $ \mathbb F$. 2. Addition is associative, i.e. $x + (y + z) = (x + y) + z$ for all $x, y, z$ in $\mathbb F$. 3. There exists a unique additive identity $0$ such that $x + 0 = x$ for all $x$ in $\mathbb F$. 4. There exists a unique additive inverse $-x$ such that $x + (-x) = 0$ for all $x$ in $\mathbb F$. 5. Multiplication is commutative, i.e. $x \cdot y = y \cdot x$ for all $x, y$ in $\mathbb F$. 6. Multiplication is associative, i.e. $x \cdot (y \cdot z) = (x \cdot y) \cdot z$ for all $x, y, z$ in $\mathbb F$. 7. There exists a unique multiplicative identity $1$ such that $x \cdot 1 = x$ for all $x$ in $\mathbb F$. 8. For every nonzero $x$ in $\mathbb F$, there exists a unique multiplicative inverse $x^{-1}$ such that $x \cdot x^{-1} = 1$. 9. Multiplication is distributive over addition, i.e. $x \cdot (y + z) = x \cdot y + x \cdot z$ for all $x, y, z$ in $\mathbb F$. 10. Closure under addition, i.e. for all $x, y$ in $\mathbb F$, $x + y$ must also be in $\mathbb F$. 11. Closure under multiplication, i.e. for all $x, y$ in $\mathbb F$, $x \cdot y$ must also be in $\mathbb F$. From the above it follows that any subfield $\mathbb F$ of $\mathbb C$ must contain the elements $0$ and $1$ (as the additive and multiplicative identity respectively). This satisfies axioms $3$ and $7$. By our axiomatic definition of fields, any subfield of $\mathbb C$ must also satisfy axioms $1, 2, 5, 6, 9$. To satisfy axiom $10$, we see that $\mathbb F$ must contain $\mathbb Z^+$ by induction (I do not know how to prove this). To satisfy axiom $4$, it follows that $\mathbb F$ must contain $\mathbb Z^-$ as well. Hence $\mathbb F$ contains $\mathbb Z$. To satisfy axiom $8$, we need to include all scalars of the form $\frac 1x$, where $x \in \mathbb Z \setminus\{0\}$. To accomodate axiom $11$, we must include all scalars of the form $\frac 1x \cdot y$, where $x \in \mathbb Z \setminus \{0\}, y \in \mathbb Z$. Hence $\mathbb F$ must contain $\mathbb Q$. Now every axiom is satisfied and we see that any subfield of $\mathbb C$ must at least contain $\mathbb Q$.","I just started reading Linear Algebra by Hoffman and Kunze, and I came across the following line: The interested reader should verify that any subfield of $\mathbb C$   must contain every rational number. Of course, I am interested, so I tried to come up with a proof. I just need verification on the correctness of my proof, and any feedback on how I can improve it. So here it goes. Proof A subfield is simply a subset of a field. A field $\mathbb F$ must obey the following axioms: 1. Addition is commutative, i.e. $x + y = y + x$ for all $x, y$ in $ \mathbb F$. 2. Addition is associative, i.e. $x + (y + z) = (x + y) + z$ for all $x, y, z$ in $\mathbb F$. 3. There exists a unique additive identity $0$ such that $x + 0 = x$ for all $x$ in $\mathbb F$. 4. There exists a unique additive inverse $-x$ such that $x + (-x) = 0$ for all $x$ in $\mathbb F$. 5. Multiplication is commutative, i.e. $x \cdot y = y \cdot x$ for all $x, y$ in $\mathbb F$. 6. Multiplication is associative, i.e. $x \cdot (y \cdot z) = (x \cdot y) \cdot z$ for all $x, y, z$ in $\mathbb F$. 7. There exists a unique multiplicative identity $1$ such that $x \cdot 1 = x$ for all $x$ in $\mathbb F$. 8. For every nonzero $x$ in $\mathbb F$, there exists a unique multiplicative inverse $x^{-1}$ such that $x \cdot x^{-1} = 1$. 9. Multiplication is distributive over addition, i.e. $x \cdot (y + z) = x \cdot y + x \cdot z$ for all $x, y, z$ in $\mathbb F$. 10. Closure under addition, i.e. for all $x, y$ in $\mathbb F$, $x + y$ must also be in $\mathbb F$. 11. Closure under multiplication, i.e. for all $x, y$ in $\mathbb F$, $x \cdot y$ must also be in $\mathbb F$. From the above it follows that any subfield $\mathbb F$ of $\mathbb C$ must contain the elements $0$ and $1$ (as the additive and multiplicative identity respectively). This satisfies axioms $3$ and $7$. By our axiomatic definition of fields, any subfield of $\mathbb C$ must also satisfy axioms $1, 2, 5, 6, 9$. To satisfy axiom $10$, we see that $\mathbb F$ must contain $\mathbb Z^+$ by induction (I do not know how to prove this). To satisfy axiom $4$, it follows that $\mathbb F$ must contain $\mathbb Z^-$ as well. Hence $\mathbb F$ contains $\mathbb Z$. To satisfy axiom $8$, we need to include all scalars of the form $\frac 1x$, where $x \in \mathbb Z \setminus\{0\}$. To accomodate axiom $11$, we must include all scalars of the form $\frac 1x \cdot y$, where $x \in \mathbb Z \setminus \{0\}, y \in \mathbb Z$. Hence $\mathbb F$ must contain $\mathbb Q$. Now every axiom is satisfied and we see that any subfield of $\mathbb C$ must at least contain $\mathbb Q$.",,"['abstract-algebra', 'proof-verification', 'field-theory']"
71,Another completion of $\overline{\Bbb{Q}}$,Another completion of,\overline{\Bbb{Q}},"$\overline{\Bbb{Q}}$ is the field of algebraic numbers, let $E$ be the set of embeddings $\overline{\Bbb{Q}}\to \Bbb{C}$ and consider the following norm on $\overline{\Bbb{Q}}$ $$\|\alpha\|=\sup_{\sigma\in E} |\sigma(\alpha)|$$ making it a topological field. The completion for that norm is a ring containing $\overline{\Bbb{Q}}\otimes_\Bbb{Q}\Bbb{R}$ (including some zero divisors such as $(1\otimes \sqrt2)^2=(\sqrt2\otimes 1)^2$ ) as well as things such as $$\sum_{n\ge 1} 2^{-n} \sqrt{n}$$ Is there a simple description of this completion?","is the field of algebraic numbers, let be the set of embeddings and consider the following norm on making it a topological field. The completion for that norm is a ring containing (including some zero divisors such as ) as well as things such as Is there a simple description of this completion?",\overline{\Bbb{Q}} E \overline{\Bbb{Q}}\to \Bbb{C} \overline{\Bbb{Q}} \|\alpha\|=\sup_{\sigma\in E} |\sigma(\alpha)| \overline{\Bbb{Q}}\otimes_\Bbb{Q}\Bbb{R} (1\otimes \sqrt2)^2=(\sqrt2\otimes 1)^2 \sum_{n\ge 1} 2^{-n} \sqrt{n},"['abstract-algebra', 'field-theory', 'complete-spaces']"
72,How can a non-mathematician intuitively understand the importance of algebraic varieties?,How can a non-mathematician intuitively understand the importance of algebraic varieties?,,"In my splintered readings, I have come to understand that algebraic varieties/ideals and their investigations/extensions/unifications dominated a large part of 20th century mathematics. Many profound results were achieved and prized awards given out for discoveries in this field. But as an engineer it still escapes me why they are the quantities of central interest and how they fit into the big picture (i.e. how they provide connections between different fields of mathematics, which I think is why they are so widely investigated?). In my case I think the main issue is the vast terminology that one has to internalize before one can begin to understand even basic results. A Wikipedia reading inevitably turns to multi-hour link-fest. To be more precise, my interest as an engineer arises specifically in their connection to dynamical systems theory. An algebraic approach to dynamical systems has been sporadically attempted since the 60s; and I think was initiated by Kalman in his study of dynamical systems over rings. Recently, far more general approaches have also been adopted incorporating category theory, sheaves etc.  For example here and here . So I am trying to find a coherent picture of their importance to (1) modern mathematics, (2) ODE's and differential geometry (3) systems theory. Understandably, the question might be too wide to answer in a single answer, so multiple answers are invited. As for an idea of what I am looking for see this Quora answer. The answer beautifully breaks down what is probably a one line rejoinder for a mathematician into something even a sufficiently advanced high school student can understand (but the answer doesn't have to be that simple or long, intuition is more important). I am not opposed to taking courses in Abstract Algebra to fully understand them, but from an engineering stand-point, a motivation for that type of commitment is hard to bring about unless I first get a big-picture idea of why/if it will be useful.","In my splintered readings, I have come to understand that algebraic varieties/ideals and their investigations/extensions/unifications dominated a large part of 20th century mathematics. Many profound results were achieved and prized awards given out for discoveries in this field. But as an engineer it still escapes me why they are the quantities of central interest and how they fit into the big picture (i.e. how they provide connections between different fields of mathematics, which I think is why they are so widely investigated?). In my case I think the main issue is the vast terminology that one has to internalize before one can begin to understand even basic results. A Wikipedia reading inevitably turns to multi-hour link-fest. To be more precise, my interest as an engineer arises specifically in their connection to dynamical systems theory. An algebraic approach to dynamical systems has been sporadically attempted since the 60s; and I think was initiated by Kalman in his study of dynamical systems over rings. Recently, far more general approaches have also been adopted incorporating category theory, sheaves etc.  For example here and here . So I am trying to find a coherent picture of their importance to (1) modern mathematics, (2) ODE's and differential geometry (3) systems theory. Understandably, the question might be too wide to answer in a single answer, so multiple answers are invited. As for an idea of what I am looking for see this Quora answer. The answer beautifully breaks down what is probably a one line rejoinder for a mathematician into something even a sufficiently advanced high school student can understand (but the answer doesn't have to be that simple or long, intuition is more important). I am not opposed to taking courses in Abstract Algebra to fully understand them, but from an engineering stand-point, a motivation for that type of commitment is hard to bring about unless I first get a big-picture idea of why/if it will be useful.",,"['abstract-algebra', 'soft-question', 'dynamical-systems']"
73,Algebraic extension of $\Bbb Q$ with exactly one extension of given degree $n$,Algebraic extension of  with exactly one extension of given degree,\Bbb Q n,"Let $n \geq 2$ be any integer. Is there an algebraic extension $F_n$ of $\Bbb Q$ such that $F_n$ has exactly one field extension $K/F_n$ of degree $n$? Here I mean ""exactly one"" in a strict sense, i.e. I don't allow ""up to (field / $F$-algebra) isomorphisms"". But a solution with ""exactly one up to field (or $F$-algebra) isomorphisms"" would also be welcome. I'm very interested in the case where $n$ has two distinct prime factors. My thoughts: This answer provides a construction for $n=2$. I was able to generalize it for $n=p^r$ where $p$ is an odd prime. Let  $S = \left\{\zeta_{p^r}^j\sqrt[p^r]{2} \mid 0 \leq j < p^r \right\}$. Then $$\mathscr F_S =  \left\{L/\Bbb Q \text{ algebraic extension} \mid \forall x \in S,\; x \not \in L \text{ and } \zeta_{p^r} \in L \right\} =\left\{L/\Bbb Q \text{ algebraic extension} \mid \sqrt[p^r]{2} \not \in L \text{ and } \zeta_{p^r} \in L \right\} $$ has a maximal element $F$, by Zorn's lemma. In particular, we have $$ F \subsetneq K \text{ and } K/\Bbb Q \text{ algebraic extension}  \implies  \exists x \in S,\; x \in K \implies  \exists x \in S,\; F \subsetneq F(x) \subseteq K $$ But $X^{p^r}-2$ is the minimal polynomial of any $x \in S$ over $F$ : it is irreducible over $F$ because $2$ is not a $p$-th power in $F$. Therefore $F(x)$ has degree $p^r$ over $F$ and using the implications above, we conclude that $F(x) = F(\sqrt[p^r]{2})$ is the only extension of degree $p^r$ of $F$, when $x \in S$. Assume now that we want to build a field $F$ with the desired property for some $n=\prod_{i=1}^r p_i^{n_i}$. I tried to do some kind of compositum, without any success. I have some trouble with the irreducibility over $F$ of the minimal polynomial of some $x \in S$ ($S$ suitably chosen) over $\Bbb Q$... I know that $\mathbf C((t))$ is quasi-finite and embeds abstractly in $\bf C$, so there is an uncountable subfield of $\bf C$ having exactly one field extension of degree $n$ for any $n \geq 1$.","Let $n \geq 2$ be any integer. Is there an algebraic extension $F_n$ of $\Bbb Q$ such that $F_n$ has exactly one field extension $K/F_n$ of degree $n$? Here I mean ""exactly one"" in a strict sense, i.e. I don't allow ""up to (field / $F$-algebra) isomorphisms"". But a solution with ""exactly one up to field (or $F$-algebra) isomorphisms"" would also be welcome. I'm very interested in the case where $n$ has two distinct prime factors. My thoughts: This answer provides a construction for $n=2$. I was able to generalize it for $n=p^r$ where $p$ is an odd prime. Let  $S = \left\{\zeta_{p^r}^j\sqrt[p^r]{2} \mid 0 \leq j < p^r \right\}$. Then $$\mathscr F_S =  \left\{L/\Bbb Q \text{ algebraic extension} \mid \forall x \in S,\; x \not \in L \text{ and } \zeta_{p^r} \in L \right\} =\left\{L/\Bbb Q \text{ algebraic extension} \mid \sqrt[p^r]{2} \not \in L \text{ and } \zeta_{p^r} \in L \right\} $$ has a maximal element $F$, by Zorn's lemma. In particular, we have $$ F \subsetneq K \text{ and } K/\Bbb Q \text{ algebraic extension}  \implies  \exists x \in S,\; x \in K \implies  \exists x \in S,\; F \subsetneq F(x) \subseteq K $$ But $X^{p^r}-2$ is the minimal polynomial of any $x \in S$ over $F$ : it is irreducible over $F$ because $2$ is not a $p$-th power in $F$. Therefore $F(x)$ has degree $p^r$ over $F$ and using the implications above, we conclude that $F(x) = F(\sqrt[p^r]{2})$ is the only extension of degree $p^r$ of $F$, when $x \in S$. Assume now that we want to build a field $F$ with the desired property for some $n=\prod_{i=1}^r p_i^{n_i}$. I tried to do some kind of compositum, without any success. I have some trouble with the irreducibility over $F$ of the minimal polynomial of some $x \in S$ ($S$ suitably chosen) over $\Bbb Q$... I know that $\mathbf C((t))$ is quasi-finite and embeds abstractly in $\bf C$, so there is an uncountable subfield of $\bf C$ having exactly one field extension of degree $n$ for any $n \geq 1$.",,"['abstract-algebra', 'field-theory', 'extension-field']"
74,Geometric Intuition for Dihedral Group Automorphisms,Geometric Intuition for Dihedral Group Automorphisms,,"I noticed the other day that the automorphism group of the dihedral group $D_{2n}$ (of order $2n$) is $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$, the group of affine transformations of the $\mathbb Z$-module $\mathbb Z/n\mathbb Z$. By ""affine transformation"" I mean an invertible function $\mathbb Z/n\mathbb Z\to \mathbb Z/n\mathbb Z$ of the form $x\mapsto ax + b$ (so $a\in(\mathbb Z/n\mathbb Z)^\times$ and $b\in\mathbb Z/n\mathbb Z$); the group multiplication in $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$ is function composition. Both $D_{2n}$ and $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$ have a natural geometric interpretation: $D_{2n}$ represents the symmetries of the $n$-gon and $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$ represents a nice class of transformations of the ""space"" $\mathbb Z/n\mathbb Z$. The geometric interpretation of $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$ is admittedly more abstract than the first, but I don't think it should be discounted. At any rate, I understand the algebraic reasons why $\operatorname{Aut}(D_{2n}) \cong \operatorname{Aff}(\mathbb Z/n\mathbb Z)$, but it seems like there might also be geometric reasons. Is there an intuitive geometric explanation for why we should expect the automorphism group of a dihedral group to be an affine group? Does anyone know of an insightful way to visualize automorphisms of the dihedral group as affine transformations?","I noticed the other day that the automorphism group of the dihedral group $D_{2n}$ (of order $2n$) is $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$, the group of affine transformations of the $\mathbb Z$-module $\mathbb Z/n\mathbb Z$. By ""affine transformation"" I mean an invertible function $\mathbb Z/n\mathbb Z\to \mathbb Z/n\mathbb Z$ of the form $x\mapsto ax + b$ (so $a\in(\mathbb Z/n\mathbb Z)^\times$ and $b\in\mathbb Z/n\mathbb Z$); the group multiplication in $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$ is function composition. Both $D_{2n}$ and $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$ have a natural geometric interpretation: $D_{2n}$ represents the symmetries of the $n$-gon and $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$ represents a nice class of transformations of the ""space"" $\mathbb Z/n\mathbb Z$. The geometric interpretation of $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$ is admittedly more abstract than the first, but I don't think it should be discounted. At any rate, I understand the algebraic reasons why $\operatorname{Aut}(D_{2n}) \cong \operatorname{Aff}(\mathbb Z/n\mathbb Z)$, but it seems like there might also be geometric reasons. Is there an intuitive geometric explanation for why we should expect the automorphism group of a dihedral group to be an affine group? Does anyone know of an insightful way to visualize automorphisms of the dihedral group as affine transformations?",,"['abstract-algebra', 'group-theory', 'geometry', 'intuition']"
75,"Does ""zero dimensional domains are fields"" require the Boolean Prime Ideal theorem?","Does ""zero dimensional domains are fields"" require the Boolean Prime Ideal theorem?",,"Consider the following simple lemma: Lemma 1. Let $R$ be a commutative domain, then $R$ is a field iff $\dim(R)=0$ . N.B. By ""commutative domain"" I mean ''nonzero commutative ring with 1 such that $xy=0$ implies either $x=0$ or $y=0$ '' and ""dimension"" means ""Krull dimension"". $(\!\!\implies\!\!)$ is unconditionally true. The proof I know for $(\!\!\impliedby\!\!)$ involves the existence of maximal ideals and thus Zorn's lemma. This initally led me to wonder: Wrong question: is the axiom of choice (in the form that every proper ideal is contained in a maximal ideal) required to prove Lemma 1 ? Is Lemma 1 even equivalent to the axiom of choice? After searching the internet for clues I stumbled on this MathOverflow question on the existence of prime ideals and godelian's answer . The following doesn't require any choice principle: Lemma 2. Let $R$ be a nonzero commutative ring with $1$ , then $R$ is a field iff its only ideals are $(0)$ and $(1)$ . So to prove that zero dimensional domains are fields it is enough to prove that their only ideals are $(0)$ and $(1)$ . If there were some proper ideal $(0)<I<(1)$ then, by the Boolean Prime Ideal (BPI) theorem, the quotient ring $R/I$ would have a prime ideal which pulls back to a prime $(0)<I\leq \mathfrak{p}$ in $R$ . Thus $\dim(R)\geq 1$ which is a contradiction. Better question: is BPI required to prove Lemma 1 ? Is Lemma 1 strictly weaker than the BPI? One can slightly broaden the scope of the question by dropping the domain condition and asking Slightly broader question. Does ""prime ideals in zero dimensional rings are maximal"" require the BPI? Is it possibly even equivalent to the BPI? Or is it weaker? One can ask a broader question still without mentioning dimensions: Alternative phrasing. What choice principle is required to prove that prime ideals which are maximal among prime ideals are maximal among all proper ideals.","Consider the following simple lemma: Lemma 1. Let be a commutative domain, then is a field iff . N.B. By ""commutative domain"" I mean ''nonzero commutative ring with 1 such that implies either or '' and ""dimension"" means ""Krull dimension"". is unconditionally true. The proof I know for involves the existence of maximal ideals and thus Zorn's lemma. This initally led me to wonder: Wrong question: is the axiom of choice (in the form that every proper ideal is contained in a maximal ideal) required to prove Lemma 1 ? Is Lemma 1 even equivalent to the axiom of choice? After searching the internet for clues I stumbled on this MathOverflow question on the existence of prime ideals and godelian's answer . The following doesn't require any choice principle: Lemma 2. Let be a nonzero commutative ring with , then is a field iff its only ideals are and . So to prove that zero dimensional domains are fields it is enough to prove that their only ideals are and . If there were some proper ideal then, by the Boolean Prime Ideal (BPI) theorem, the quotient ring would have a prime ideal which pulls back to a prime in . Thus which is a contradiction. Better question: is BPI required to prove Lemma 1 ? Is Lemma 1 strictly weaker than the BPI? One can slightly broaden the scope of the question by dropping the domain condition and asking Slightly broader question. Does ""prime ideals in zero dimensional rings are maximal"" require the BPI? Is it possibly even equivalent to the BPI? Or is it weaker? One can ask a broader question still without mentioning dimensions: Alternative phrasing. What choice principle is required to prove that prime ideals which are maximal among prime ideals are maximal among all proper ideals.",R R \dim(R)=0 xy=0 x=0 y=0 (\!\!\implies\!\!) (\!\!\impliedby\!\!) R 1 R (0) (1) (0) (1) (0)<I<(1) R/I (0)<I\leq \mathfrak{p} R \dim(R)\geq 1,"['abstract-algebra', 'ring-theory', 'axiom-of-choice', 'maximal-and-prime-ideals']"
76,Can a group have a cyclical derived series?,Can a group have a cyclical derived series?,,"Given any group $G$ , one can consider its derived series $$G = G^{(0)}\rhd G^{(1)}\rhd G^{(2)}\rhd\dots$$ where $G^{(k)}$ is the commutator subgroup of $G^{(k-1)}$ . A group is perfect if $G=G^{(1)}$ and thus has constant derived series, and solvable if its derived series reaches the trivial group after finitely many steps. Is it possible for a group’s derived series to be cyclical, i.e. that $G \cong G^{(n)}$ for some $n>1$ and $G\not\cong G^{(k)}$ for all positive $k<n$ ? Note that such a group could not be finite, solvable, nor co-Hopfian.","Given any group , one can consider its derived series where is the commutator subgroup of . A group is perfect if and thus has constant derived series, and solvable if its derived series reaches the trivial group after finitely many steps. Is it possible for a group’s derived series to be cyclical, i.e. that for some and for all positive ? Note that such a group could not be finite, solvable, nor co-Hopfian.",G G = G^{(0)}\rhd G^{(1)}\rhd G^{(2)}\rhd\dots G^{(k)} G^{(k-1)} G=G^{(1)} G \cong G^{(n)} n>1 G\not\cong G^{(k)} k<n,"['abstract-algebra', 'group-theory', 'infinite-groups', 'derived-subgroup']"
77,Question on a homomorphism of a set G.,Question on a homomorphism of a set G.,,"I'm having difficulty showing the given a map, say $\phi(z)=z^k$, is surjective. This question is from D & F section 1.6 - #19 Let $G$ =$\{z \in \mathbb C|z^n=1 \text{ for some } n \in \mathbb Z^+\}$. Prove that for any fixed integer $k>1$ the map from $G$ to itself defined by $z \to z^k$ is a surjective homomorphism, but is not an isomorphism. To show $\phi(z)=z^k$ is a homomorphism let $z_a=e^\frac{2\pi ia}{n}$ and $z_b=e^\frac{2\pi ib}{n}$ where $a,b\in \mathbb Z^+$ and $a\neq b$ then $\phi(z_az_b)=(e^\frac{2\pi ia}{n}$$\cdot$$e^\frac{2\pi ib}{n}$)=$e^\frac{2\pi ik(a+b)}{n}$=$e^\frac{2\pi ika}{n}$$\cdot$$e^\frac{2\pi ikb}{n}$=$\phi(z_a)\phi(z_b)$. I had what I thought was a proof for $\phi$ being surjective until I thought about this specific example. Here's the example: Suppose $z_a$ is a root of unity then so is a power $z^t_a$. My thoughts on $G$ are that it consists of all integer powers of the ""basic"" roots of unity for a partiuclar $n$. Let $n=3$ and $k=3$, then $\phi(e^\frac{2t\pi ik_i}{3})=(e^\frac{2t\pi ik_i}{3})^3 =e^{2t\pi ik_i}= 1$ for any integer $t\in \mathbb Z$ and integer $k_i$ such that $0\leq$$k_i$$\leq$ $2$. So we have $\phi[G]=\{1\}$, $\phi$ is not surjective in this instance. What have I misunderstood about the question? Are there any glaring errors I'm making? Is $n$ supposed to be a particular value and not just any integer value? Your help is appreciated.","I'm having difficulty showing the given a map, say $\phi(z)=z^k$, is surjective. This question is from D & F section 1.6 - #19 Let $G$ =$\{z \in \mathbb C|z^n=1 \text{ for some } n \in \mathbb Z^+\}$. Prove that for any fixed integer $k>1$ the map from $G$ to itself defined by $z \to z^k$ is a surjective homomorphism, but is not an isomorphism. To show $\phi(z)=z^k$ is a homomorphism let $z_a=e^\frac{2\pi ia}{n}$ and $z_b=e^\frac{2\pi ib}{n}$ where $a,b\in \mathbb Z^+$ and $a\neq b$ then $\phi(z_az_b)=(e^\frac{2\pi ia}{n}$$\cdot$$e^\frac{2\pi ib}{n}$)=$e^\frac{2\pi ik(a+b)}{n}$=$e^\frac{2\pi ika}{n}$$\cdot$$e^\frac{2\pi ikb}{n}$=$\phi(z_a)\phi(z_b)$. I had what I thought was a proof for $\phi$ being surjective until I thought about this specific example. Here's the example: Suppose $z_a$ is a root of unity then so is a power $z^t_a$. My thoughts on $G$ are that it consists of all integer powers of the ""basic"" roots of unity for a partiuclar $n$. Let $n=3$ and $k=3$, then $\phi(e^\frac{2t\pi ik_i}{3})=(e^\frac{2t\pi ik_i}{3})^3 =e^{2t\pi ik_i}= 1$ for any integer $t\in \mathbb Z$ and integer $k_i$ such that $0\leq$$k_i$$\leq$ $2$. So we have $\phi[G]=\{1\}$, $\phi$ is not surjective in this instance. What have I misunderstood about the question? Are there any glaring errors I'm making? Is $n$ supposed to be a particular value and not just any integer value? Your help is appreciated.",,"['group-theory', 'abstract-algebra', 'roots-of-unity']"
78,Is there a theory that combines category theory/abstract algebra and computational complexity?,Is there a theory that combines category theory/abstract algebra and computational complexity?,,"Category theory and abstract algebra deal with the way functions can be combined with other functions.  Complexity theory deals with how hard a function is to compute.  It's weird to me that I haven't seen anyone merge these fields of study, since they seem like such natural pairs.  Has anyone done this before? As a motivating example, let's take a look at semigroups.  Semigroups have an associative binary operation.  It's well known that operations in semigroups can be parallelized due to the associativity.  For example, if there's a billion numbers we want added together, we could break the problem into 10 problems of adding 100 million numbers together, then add the results. But parallelizing the addition operator makes sense only because it can be computed in constant time and space.  What if this weren't the case?  For example, lists and the append operation form a common semigroup in functional programming, but the append operator takes time and space $O(n)$. It seems like there should be a convenient way to describe the differences between these two semigroups.  In general, it seems like algebras that took into account the complexity of their operations would be very useful to computer scientists like myself.","Category theory and abstract algebra deal with the way functions can be combined with other functions.  Complexity theory deals with how hard a function is to compute.  It's weird to me that I haven't seen anyone merge these fields of study, since they seem like such natural pairs.  Has anyone done this before? As a motivating example, let's take a look at semigroups.  Semigroups have an associative binary operation.  It's well known that operations in semigroups can be parallelized due to the associativity.  For example, if there's a billion numbers we want added together, we could break the problem into 10 problems of adding 100 million numbers together, then add the results. But parallelizing the addition operator makes sense only because it can be computed in constant time and space.  What if this weren't the case?  For example, lists and the append operation form a common semigroup in functional programming, but the append operator takes time and space $O(n)$. It seems like there should be a convenient way to describe the differences between these two semigroups.  In general, it seems like algebras that took into account the complexity of their operations would be very useful to computer scientists like myself.",,"['abstract-algebra', 'category-theory', 'computational-complexity']"
79,How many different groups of order $15$ there are?,How many different groups of order  there are?,15,"I wanted to share with you my resolution of this exercise. How many different groups of order $15$ there are? My resolution: We're looking for groups such that $|G|=15=3\cdot 5$. Then: $G$ has an unique Sylow $3$-subgroup of order $3$ ($P_3$) and an unique Sylow $5$-subgroup of order $5$ ($P_5$). Furthermore, $$|P_3 \cap P_5|\  \Large\mid\ \normalsize|P_3|,\ |P_5|$$   So $$|P_3 \cap P_5|=1\implies P_3 \cap P_5=\{e\}.$$ Then:$$|P_3\cdot P_5|=\displaystyle\frac{|P_3|\cdot |P_5|}{|P_3 \cap P_5|}=|P_3|\cdot |P_5|=3\cdot 5=|G|,$$ and $$G=P_3\cdot P_5 \simeq P_3 \times P_5 \simeq \mathbb{Z}/3\mathbb{Z} \times \mathbb{Z}/5\mathbb{Z}.$$ It's everything correct and well justified? There's some things that I didn't say, for example that a subgroup a prime $p$ as order is cyclic, and thus isomoprhic to $\mathbb{Z}/p\mathbb{Z}$. Maybe I did something wrong. I would appreciate someone to look and criticize my exercise. Thank you.","I wanted to share with you my resolution of this exercise. How many different groups of order $15$ there are? My resolution: We're looking for groups such that $|G|=15=3\cdot 5$. Then: $G$ has an unique Sylow $3$-subgroup of order $3$ ($P_3$) and an unique Sylow $5$-subgroup of order $5$ ($P_5$). Furthermore, $$|P_3 \cap P_5|\  \Large\mid\ \normalsize|P_3|,\ |P_5|$$   So $$|P_3 \cap P_5|=1\implies P_3 \cap P_5=\{e\}.$$ Then:$$|P_3\cdot P_5|=\displaystyle\frac{|P_3|\cdot |P_5|}{|P_3 \cap P_5|}=|P_3|\cdot |P_5|=3\cdot 5=|G|,$$ and $$G=P_3\cdot P_5 \simeq P_3 \times P_5 \simeq \mathbb{Z}/3\mathbb{Z} \times \mathbb{Z}/5\mathbb{Z}.$$ It's everything correct and well justified? There's some things that I didn't say, for example that a subgroup a prime $p$ as order is cyclic, and thus isomoprhic to $\mathbb{Z}/p\mathbb{Z}$. Maybe I did something wrong. I would appreciate someone to look and criticize my exercise. Thank you.",,"['abstract-algebra', 'group-theory', 'finite-groups', 'groups-enumeration']"
80,Is a linear combination of minors irreducible?,Is a linear combination of minors irreducible?,,"Let $X=(X_{ij})_{1\le i,j\le n}$ be a matrix of indeterminates over $\mathbb C$. For choices $I,J\subseteq\{1,\ldots,n\}$ with $|I|=|J|=k$ denote by $X_{I\times J}$ the matrix $(X_{ij})_{i\in I,j\in J}$. I will call the polynomials $\delta_{I\times J}:=\det(X_{I\times J})$ the $k$-minors of $X$, for all choices of $I$ and $J$ as above. It is well-known that each of the $\delta_{I\times J}$ is an irreducible polynomial, but assume we have a linear combination $$ f = \sum_{\substack{I,J\subseteq\{1,\ldots,n\}\\|I|=|J|=k}} \lambda_{IJ} \cdot \delta_{I\times J} \quad\in \mathbb C[X_{ij}]_k $$ with certain $\lambda_{IJ}\in\mathbb C$, and $f\ne 0$. My question is: Is $f$ always irreducible? If no, I would be curious to see a counterexample. I somehow can't manage to come up with one. Are there well-known conditions under which $f$ is irreducible?","Let $X=(X_{ij})_{1\le i,j\le n}$ be a matrix of indeterminates over $\mathbb C$. For choices $I,J\subseteq\{1,\ldots,n\}$ with $|I|=|J|=k$ denote by $X_{I\times J}$ the matrix $(X_{ij})_{i\in I,j\in J}$. I will call the polynomials $\delta_{I\times J}:=\det(X_{I\times J})$ the $k$-minors of $X$, for all choices of $I$ and $J$ as above. It is well-known that each of the $\delta_{I\times J}$ is an irreducible polynomial, but assume we have a linear combination $$ f = \sum_{\substack{I,J\subseteq\{1,\ldots,n\}\\|I|=|J|=k}} \lambda_{IJ} \cdot \delta_{I\times J} \quad\in \mathbb C[X_{ij}]_k $$ with certain $\lambda_{IJ}\in\mathbb C$, and $f\ne 0$. My question is: Is $f$ always irreducible? If no, I would be curious to see a counterexample. I somehow can't manage to come up with one. Are there well-known conditions under which $f$ is irreducible?",,"['abstract-algebra', 'algebraic-geometry']"
81,"What are such ""off-by-a-factor"" ring maps called?","What are such ""off-by-a-factor"" ring maps called?",,"Suppose $R,S$ are ( non -unital) rings. What is the term for a function $f:R\rightarrow S$ such that $f(a+b)=f(a)+f(b)$ for all $a,b\in R$ , and there is some $u\in S$ such that for all $a,b\in R$ we have $uf(ab)=f(a)f(b)$ ? I'm currently calling these ""weak homomorphisms"" (and ""weak embeddings"" in the injective case), but I suspect they have an actual name. For example, let $R=\mathbb{Q}$ and let $S$ be the ring of polynomials with rational coefficients whose constant terms are integers. The map $R\rightarrow S:q\mapsto qx$ is of course not a homomorphism, and indeed there is no homomorphism from $R$ to $S$ , but it does satisfy the weaker property above via $u=x$ . (I'm primarily running into this notion in the context of certain models of Robinson arithmetic , the idea being that ""sufficiently generic"" such models admit weak embeddings from lots of rings and this leads to some interesting structural properties, but I'm also interested in them in other contexts - including non-unital ones.)","Suppose are ( non -unital) rings. What is the term for a function such that for all , and there is some such that for all we have ? I'm currently calling these ""weak homomorphisms"" (and ""weak embeddings"" in the injective case), but I suspect they have an actual name. For example, let and let be the ring of polynomials with rational coefficients whose constant terms are integers. The map is of course not a homomorphism, and indeed there is no homomorphism from to , but it does satisfy the weaker property above via . (I'm primarily running into this notion in the context of certain models of Robinson arithmetic , the idea being that ""sufficiently generic"" such models admit weak embeddings from lots of rings and this leads to some interesting structural properties, but I'm also interested in them in other contexts - including non-unital ones.)","R,S f:R\rightarrow S f(a+b)=f(a)+f(b) a,b\in R u\in S a,b\in R uf(ab)=f(a)f(b) R=\mathbb{Q} S R\rightarrow S:q\mapsto qx R S u=x","['abstract-algebra', 'ring-theory', 'terminology']"
82,Artin conjecture for soluble extensions,Artin conjecture for soluble extensions,,"Towards the end of ch. VII §10 of Neukirch's Algebraische Zahlentheorie , he proves that the Artin $L$ -series of Abelian extensions coincide with Hecke $L$ -series, thereby proving Artin's conjecture for Abelian extensions. He then mentions in passing that this also settles the Artin conjecture for all soluble extensions. I have been trying to convince myself why this is the case. Let $E/K$ be a soluble extension of algebraic number fields, and let $G':=\textrm{Gal}(E/K)$ . Then there exists a subgroup $N \vartriangleleft G'$ such that $G:=G'/N$ is Abelian. Let $M$ be the subfield fixed by $N$ . Then we have $G \cong \textrm{Gal}(M/K)$ , and for any non-trivial simple character $\chi$ of $G$ , we have, by inflation: $$L(E/K,\chi',s) = L(M/K,\chi,s)$$ $\chi ' = \chi \circ \pi$ , where $\pi:G' \to G \cong G'/N$ is the canonical projection. As $M/K$ is an Abelian extension, this proves that $L(E/K,\chi',s)$ is holomorphic on $\mathbb{C}$ . The problem is that not every simple character of $\textrm{Gal}(E/K)$ can be expressed as $\chi \circ \pi$ , where $\chi$ is a simple character of $\textrm{Gal}(M/K)$ . So why does this prove the Artin conjecture for soluble groups? Thank you for your attention. $\textbf{Addendum:}$ I thought I had a solution. I include it here, as it might contain some useful idea. I begin by listing three properties, the third of which is not true: 1) The ""converse of inflation"" (deflation?) mentioned here : If $G:=\textrm{Gal}(E/K)$ and $\chi$ is a simple character of $G$ , then $L(E/K,\chi,s) = L(E_{\chi}/K,\chi',s)$ , where $\chi': G/\textrm{Ker}(\chi) \cong \textrm{Gal}(E_{\chi}/K) \to \mathbb{C}^{\times}$ is a faithful simple character given by $\chi'(g\ \textrm{Ker}(\chi)):=\chi(g)$ . 2) That every normal subgroup $N \vartriangleleft G$ can be expressed as the kernel of some simple character of $G$ . 3) The quotient of a soluble group by a normal subgroup is always Abelian (NB: This is not true. Take $\textit{e.g.}$ $S_4/(C_2 \times C_2) \cong S_3$ ). We then have that as $G$ is soluble, there exists a simple character $\chi$ of $G$ , so that $G/\textrm{Ker}(\chi)$ is Abelian, and by the above $$ L(E/K,\chi,s) = L(E_{\chi}/K,\chi',s) $$ is entire. My next idea was to replace the erroneous property 3) with the fact that the quotient of a soluble group by a normal subgroup is a soluble group, and then apply the process iteratively, but this does not seem to work. $\textbf{Addendum II:}$ I have still not found a solution and hence add a bounty. $\textbf{Addendum III:}$ I think I have a solution. Correct me if I am wrong. I think the answer is just to keep ""quotienting"". By 1) above, for any character $\chi$ of $\textrm{Gal}(E/K)$ ,  we get a simple character $\chi'$ of $\textrm{Gal}(E'/K)$ and $L(E/K,\chi,s) = L(E'/K,\chi',s)$ , where $E'$ is the subfield fixed by $\textrm{Ker}(\chi)$ . Applying this again w.r.t. the group $\textrm{Gal}(E'/K)/\textrm{Ker}(\chi') = \textrm{Gal}(E''/K)$ , we get a simple character $\chi''$ , and $L(E/K,\chi,s) = L(E'/K,\chi',s) = L(E''/K,\chi'',s)$ , where $E''$ is the subfield of $E'$ fixed by $\textrm{Ker}(\chi')$ . Applying this iteratively, we find that $L(E/K,\chi,s) = L(E^{(n)}/K,\chi^{(n)},s)$ , where $E^{(n)}/K$ is an Abelian extension for some $n \in \mathbb{N}$ . Note that this process must terminate after a finite number of iterations, as the groups in question are finite - by ""terminate"", I mean that eventually the resulting quotient group is Abelian. $\textbf{Addendum IV:}$ The above is not a solution. See the comment by Lukas Heger below. $\textbf{Addendum V:}$ Would the Artin conjecture for all soluble extensions not trivially imply the proofs of Langlands and Tunnell of the Artin conjecture for tetra- and octahedral representations? What I mean is the following: What they proved is that if $(V,\rho)$ is a degree 2 representation of a finite group $G$ and $\rho(G)/Z(\rho(G))$ is isomorphic to either $A_4$ or $S_4$ , then the corresponding Artin $L$ -series is entire. But we know that if $G/Z(G)$ is soluble, then so is $G$ . And $A_4$ and $S_4$ are soluble groups! On the other hand, $\rho(G)$ soluble does not necessarily imply that $G$ is soluble, so I suppose the work of Langlands and Tunnell is only non-trivial for degree 2 representations of insoluble extensions whose images in the projective general linear group are isomorphic to either $A_4$ or $S_4$ . In particular, these representations must be unfaithful. This is of course assuming that we have a proof of the Artin conjecture for all soluble extensions. I should also add that when I wrote $\textrm{Ker}(\chi)$ above, I meant $\textrm{Ker}(\rho)$ , where $(V,\rho)$ is the representations corresponding to the character $\chi$ . I realise this is poor notation, but hopefully unambiguous. $\textbf{Addendum VI:}$ The same is true for unfaithful representations! We assume the Artin conjecture for soluble extensions. Let $G$ be insoluble and $(V,\rho)$ an unfaithful representation of $G$ so that $\rho(G)$ is soluble. Let $E_{\rho}$ be the subfield fixed by the kernel of $\rho$ . We then have $\rho(G) \cong G/\textrm{Ker}(\rho) \cong \textrm{Gal}(E_{\rho}/K)$ , and: $$ L(E/K,\rho,s) = L(E_{\rho}/K,\rho',s) $$ where $\rho' = \rho \circ \pi$ . As the RHS is an $L$ -series of a soluble extension, it is entire by our assumption. Thus the Artin conjecture for all soluble extensions does indeed imply the proofs of Langlands and Tunnell. My conclusion: Neukirch was mistaken, and proving the Artin conjecture for all soluble extensions is not as simple as he seems to have imagined. It seems that the Artin conjecture for soluble extensions is in fact an open problem. Addendum VII: I should add that the credit for discovering the falseness of the above claim goes entirely to my friend and colleague O. Justinussen. If anyone is able to prove him wrong by providing an elementary proof of the Artin conjecture for all soluble extensions, please let me know.","Towards the end of ch. VII §10 of Neukirch's Algebraische Zahlentheorie , he proves that the Artin -series of Abelian extensions coincide with Hecke -series, thereby proving Artin's conjecture for Abelian extensions. He then mentions in passing that this also settles the Artin conjecture for all soluble extensions. I have been trying to convince myself why this is the case. Let be a soluble extension of algebraic number fields, and let . Then there exists a subgroup such that is Abelian. Let be the subfield fixed by . Then we have , and for any non-trivial simple character of , we have, by inflation: , where is the canonical projection. As is an Abelian extension, this proves that is holomorphic on . The problem is that not every simple character of can be expressed as , where is a simple character of . So why does this prove the Artin conjecture for soluble groups? Thank you for your attention. I thought I had a solution. I include it here, as it might contain some useful idea. I begin by listing three properties, the third of which is not true: 1) The ""converse of inflation"" (deflation?) mentioned here : If and is a simple character of , then , where is a faithful simple character given by . 2) That every normal subgroup can be expressed as the kernel of some simple character of . 3) The quotient of a soluble group by a normal subgroup is always Abelian (NB: This is not true. Take ). We then have that as is soluble, there exists a simple character of , so that is Abelian, and by the above is entire. My next idea was to replace the erroneous property 3) with the fact that the quotient of a soluble group by a normal subgroup is a soluble group, and then apply the process iteratively, but this does not seem to work. I have still not found a solution and hence add a bounty. I think I have a solution. Correct me if I am wrong. I think the answer is just to keep ""quotienting"". By 1) above, for any character of ,  we get a simple character of and , where is the subfield fixed by . Applying this again w.r.t. the group , we get a simple character , and , where is the subfield of fixed by . Applying this iteratively, we find that , where is an Abelian extension for some . Note that this process must terminate after a finite number of iterations, as the groups in question are finite - by ""terminate"", I mean that eventually the resulting quotient group is Abelian. The above is not a solution. See the comment by Lukas Heger below. Would the Artin conjecture for all soluble extensions not trivially imply the proofs of Langlands and Tunnell of the Artin conjecture for tetra- and octahedral representations? What I mean is the following: What they proved is that if is a degree 2 representation of a finite group and is isomorphic to either or , then the corresponding Artin -series is entire. But we know that if is soluble, then so is . And and are soluble groups! On the other hand, soluble does not necessarily imply that is soluble, so I suppose the work of Langlands and Tunnell is only non-trivial for degree 2 representations of insoluble extensions whose images in the projective general linear group are isomorphic to either or . In particular, these representations must be unfaithful. This is of course assuming that we have a proof of the Artin conjecture for all soluble extensions. I should also add that when I wrote above, I meant , where is the representations corresponding to the character . I realise this is poor notation, but hopefully unambiguous. The same is true for unfaithful representations! We assume the Artin conjecture for soluble extensions. Let be insoluble and an unfaithful representation of so that is soluble. Let be the subfield fixed by the kernel of . We then have , and: where . As the RHS is an -series of a soluble extension, it is entire by our assumption. Thus the Artin conjecture for all soluble extensions does indeed imply the proofs of Langlands and Tunnell. My conclusion: Neukirch was mistaken, and proving the Artin conjecture for all soluble extensions is not as simple as he seems to have imagined. It seems that the Artin conjecture for soluble extensions is in fact an open problem. Addendum VII: I should add that the credit for discovering the falseness of the above claim goes entirely to my friend and colleague O. Justinussen. If anyone is able to prove him wrong by providing an elementary proof of the Artin conjecture for all soluble extensions, please let me know.","L L E/K G':=\textrm{Gal}(E/K) N \vartriangleleft G' G:=G'/N M N G \cong \textrm{Gal}(M/K) \chi G L(E/K,\chi',s) = L(M/K,\chi,s) \chi ' = \chi \circ \pi \pi:G' \to G \cong G'/N M/K L(E/K,\chi',s) \mathbb{C} \textrm{Gal}(E/K) \chi \circ \pi \chi \textrm{Gal}(M/K) \textbf{Addendum:} G:=\textrm{Gal}(E/K) \chi G L(E/K,\chi,s) = L(E_{\chi}/K,\chi',s) \chi': G/\textrm{Ker}(\chi) \cong \textrm{Gal}(E_{\chi}/K) \to \mathbb{C}^{\times} \chi'(g\ \textrm{Ker}(\chi)):=\chi(g) N \vartriangleleft G G \textit{e.g.} S_4/(C_2 \times C_2) \cong S_3 G \chi G G/\textrm{Ker}(\chi) 
L(E/K,\chi,s) = L(E_{\chi}/K,\chi',s)
 \textbf{Addendum II:} \textbf{Addendum III:} \chi \textrm{Gal}(E/K) \chi' \textrm{Gal}(E'/K) L(E/K,\chi,s) = L(E'/K,\chi',s) E' \textrm{Ker}(\chi) \textrm{Gal}(E'/K)/\textrm{Ker}(\chi') = \textrm{Gal}(E''/K) \chi'' L(E/K,\chi,s) = L(E'/K,\chi',s) = L(E''/K,\chi'',s) E'' E' \textrm{Ker}(\chi') L(E/K,\chi,s) = L(E^{(n)}/K,\chi^{(n)},s) E^{(n)}/K n \in \mathbb{N} \textbf{Addendum IV:} \textbf{Addendum V:} (V,\rho) G \rho(G)/Z(\rho(G)) A_4 S_4 L G/Z(G) G A_4 S_4 \rho(G) G A_4 S_4 \textrm{Ker}(\chi) \textrm{Ker}(\rho) (V,\rho) \chi \textbf{Addendum VI:} G (V,\rho) G \rho(G) E_{\rho} \rho \rho(G) \cong G/\textrm{Ker}(\rho) \cong \textrm{Gal}(E_{\rho}/K)  L(E/K,\rho,s) = L(E_{\rho}/K,\rho',s)  \rho' = \rho \circ \pi L","['abstract-algebra', 'group-theory', 'representation-theory', 'galois-theory', 'algebraic-number-theory']"
83,Applications of the Dedekind-Hasse criterion,Applications of the Dedekind-Hasse criterion,,"It is a fact that an integral domain $R$ is a principal ideal domain if and only if there is a Dedekind-Hasse function $|R|\setminus\{0\}\xrightarrow{\ \ \delta\ \ }\mathbb{N}$ on $R$, i.e. a function such that for $0\not=a\in R$ and $b\in R$ arbitrary either $a\mid b$ or $\delta(c)<\delta(a)$ for some $0\not=c\in(a,b)$. Proof. Let $\delta$ be a Dedekind-Hasse function, $0\not=I\trianglelefteq R$ an ideal. Chose $0\not=a\in I$ of minimal degree. Clearly, $(a)\le I$. Conversely suppose $b\in I$, $b\notin (a)$. Then there exists $0\not=c\in(a,b)\le I$ of degree less then $a$, a contradiction. If $R$ is a principal ideal domain, then it is factorial, so the definition $$a\longmapsto\text{Number of prime factors of }a$$ is meaningful. If $0\not=a,b\in R$ and $a\nmid b$, then $\gcd(a,b)$ properly divides $a$, hence has strictly less prime factors. But $\gcd(a,b)\in (a,b)$ by Bézout's Lemma. $\square$ Now it can be shown with some bit of commutative algebra that $\mathbb{R}[X,Y]/(X^2+Y^2+1)$ is a principal ideal domain, see for example here . Is it possible to give a concrete description of a Dedekind-Hasse function on the ring $R=\mathbb{R}[X,Y]/(X^2+Y^2+1)$? The most well-known application of the Dedekind-Hasse criterion is probably to some rings of integers of quadratic number fields, e.g. the algebraic norm is a Dedekind-Hasse function on $\mathcal{O}_{\mathbb{Q}(\sqrt{-19})} $. Are there other applications of the Dedekind-Hasse criterion to find some non-obvious examples of principal ideal domains?","It is a fact that an integral domain $R$ is a principal ideal domain if and only if there is a Dedekind-Hasse function $|R|\setminus\{0\}\xrightarrow{\ \ \delta\ \ }\mathbb{N}$ on $R$, i.e. a function such that for $0\not=a\in R$ and $b\in R$ arbitrary either $a\mid b$ or $\delta(c)<\delta(a)$ for some $0\not=c\in(a,b)$. Proof. Let $\delta$ be a Dedekind-Hasse function, $0\not=I\trianglelefteq R$ an ideal. Chose $0\not=a\in I$ of minimal degree. Clearly, $(a)\le I$. Conversely suppose $b\in I$, $b\notin (a)$. Then there exists $0\not=c\in(a,b)\le I$ of degree less then $a$, a contradiction. If $R$ is a principal ideal domain, then it is factorial, so the definition $$a\longmapsto\text{Number of prime factors of }a$$ is meaningful. If $0\not=a,b\in R$ and $a\nmid b$, then $\gcd(a,b)$ properly divides $a$, hence has strictly less prime factors. But $\gcd(a,b)\in (a,b)$ by Bézout's Lemma. $\square$ Now it can be shown with some bit of commutative algebra that $\mathbb{R}[X,Y]/(X^2+Y^2+1)$ is a principal ideal domain, see for example here . Is it possible to give a concrete description of a Dedekind-Hasse function on the ring $R=\mathbb{R}[X,Y]/(X^2+Y^2+1)$? The most well-known application of the Dedekind-Hasse criterion is probably to some rings of integers of quadratic number fields, e.g. the algebraic norm is a Dedekind-Hasse function on $\mathcal{O}_{\mathbb{Q}(\sqrt{-19})} $. Are there other applications of the Dedekind-Hasse criterion to find some non-obvious examples of principal ideal domains?",,"['abstract-algebra', 'ring-theory']"
84,If $\gamma\in \Bbb A$ then there exists a $\pm$quadratic coefficiented polynomial for which $\gamma$ is a root.,If  then there exists a quadratic coefficiented polynomial for which  is a root.,\gamma\in \Bbb A \pm \gamma,"$\mathbf{1.\space Proposition}$ Let $\gamma$ be a solution to the equation: $$ \sum_{i=0}^n \rm a_i\rm x^i=0, \rm a_i\in\Bbb Z, a_n=1. $$ Then, there exists a polynomial $p\in \Bbb Z[x]$ such that:   $$ p=\sum _{i=0}^m\rm s_ix^i $$ Where $\rm s_i=(-1)^{k_i}\tau_i^2 \space\space\forall \rm i\in \{0,..,m\}, k_i\in \Bbb Z, \tau_i\in \Bbb Z$, for which $\gamma$ is a root. $\mathbf{1.1\space Generalization}$. There exist a monic polynomial satisfying the above conditions. $\mathbf{1.2\space Generalization}$ Same conditions as $\rm1$, but $\rm s_i=(-1)^{k_i}\tau_i^r$, for all natural $\rm r$. $\mathbf{1.3\space Generalization}$ There exists a monic polynomial satisfying $\rm1.2$. $\mathbf{Important \space implication \space of \space 1.1 }$ If $\rm 1.1$ is true, it means that the condition of $\gamma$ being the root of a monic polynomial $p\in \Bbb Z[x]$ is actually equivalent to the 'stronger' condition of being a solution to a $\pm$quadratic coefficiented polynomial $q\in \Bbb Z[x]$.","$\mathbf{1.\space Proposition}$ Let $\gamma$ be a solution to the equation: $$ \sum_{i=0}^n \rm a_i\rm x^i=0, \rm a_i\in\Bbb Z, a_n=1. $$ Then, there exists a polynomial $p\in \Bbb Z[x]$ such that:   $$ p=\sum _{i=0}^m\rm s_ix^i $$ Where $\rm s_i=(-1)^{k_i}\tau_i^2 \space\space\forall \rm i\in \{0,..,m\}, k_i\in \Bbb Z, \tau_i\in \Bbb Z$, for which $\gamma$ is a root. $\mathbf{1.1\space Generalization}$. There exist a monic polynomial satisfying the above conditions. $\mathbf{1.2\space Generalization}$ Same conditions as $\rm1$, but $\rm s_i=(-1)^{k_i}\tau_i^r$, for all natural $\rm r$. $\mathbf{1.3\space Generalization}$ There exists a monic polynomial satisfying $\rm1.2$. $\mathbf{Important \space implication \space of \space 1.1 }$ If $\rm 1.1$ is true, it means that the condition of $\gamma$ being the root of a monic polynomial $p\in \Bbb Z[x]$ is actually equivalent to the 'stronger' condition of being a solution to a $\pm$quadratic coefficiented polynomial $q\in \Bbb Z[x]$.",,"['abstract-algebra', 'number-theory', 'polynomials', 'roots']"
85,Why are cochains in group cohomology exact as a functor of the coefficients?,Why are cochains in group cohomology exact as a functor of the coefficients?,,"I am stuck with exercise $1$ of section $3$ of chapter $1$ in the book Cohomology of number fields by Neukirch. The exercise is to show that the functor from $A \rightarrow C^n(G,A)$ is exact, where $G$ is a profinite group and $A$ is a $G$ module. By definition $C^n(G,A)=X^n(G,A)^G$. Here $X^n(G,A)$ is defined to be continuous map from $G^{n+1}$ to $A$ with discrete topology on $A$. Now from page $32$ of the same book  using  proposition $1.3.7$, I deduce that $X^n(G,A)=X^n(G,Ind_G(A))^G$ that is I get that $C^n(G,A)=({X^n(G,Ind_G(A))^G})^G$. But $(-)^G$ is left exact and I know that $A \rightarrow Ind_G(A)$ is exact. I am lost after that. Any help will be very favorable.","I am stuck with exercise $1$ of section $3$ of chapter $1$ in the book Cohomology of number fields by Neukirch. The exercise is to show that the functor from $A \rightarrow C^n(G,A)$ is exact, where $G$ is a profinite group and $A$ is a $G$ module. By definition $C^n(G,A)=X^n(G,A)^G$. Here $X^n(G,A)$ is defined to be continuous map from $G^{n+1}$ to $A$ with discrete topology on $A$. Now from page $32$ of the same book  using  proposition $1.3.7$, I deduce that $X^n(G,A)=X^n(G,Ind_G(A))^G$ that is I get that $C^n(G,A)=({X^n(G,Ind_G(A))^G})^G$. But $(-)^G$ is left exact and I know that $A \rightarrow Ind_G(A)$ is exact. I am lost after that. Any help will be very favorable.",,"['abstract-algebra', 'number-theory', 'category-theory', 'homological-algebra', 'group-cohomology']"
86,Proving that the number of integer solutions of $x^2-Ny^2=1$ is infinite,Proving that the number of integer solutions of  is infinite,x^2-Ny^2=1,I am trying to prove that the number of integer solutions of $x^2-Ny^2=1$ is infinite whenever N is a squarefree integer. For this I define norm of $a+b\sqrt N=a^2-Nb^2$. Now I prove that $a+b \sqrt N$ forms a ring.Then I showed that a number is a unit iff its norm is one. So if there is one solution say $c+d\sqrt N$ which is a unit then I will raise it to higher powers and I will hopefully get infinite units. $(a+b\sqrt N)^2=a^2+Nb^2+2ab\sqrt N$ Now if $N>2$ then the integer $a$ keeps increasing if the unit I start out with is not $+1$ or $-1$. But I am unable to show the existence of one such unit . If anyone has any ideas it would be great. Also I am interested in a proof which is essentially algebraic rather a number theoretic proof.Thanks,I am trying to prove that the number of integer solutions of $x^2-Ny^2=1$ is infinite whenever N is a squarefree integer. For this I define norm of $a+b\sqrt N=a^2-Nb^2$. Now I prove that $a+b \sqrt N$ forms a ring.Then I showed that a number is a unit iff its norm is one. So if there is one solution say $c+d\sqrt N$ which is a unit then I will raise it to higher powers and I will hopefully get infinite units. $(a+b\sqrt N)^2=a^2+Nb^2+2ab\sqrt N$ Now if $N>2$ then the integer $a$ keeps increasing if the unit I start out with is not $+1$ or $-1$. But I am unable to show the existence of one such unit . If anyone has any ideas it would be great. Also I am interested in a proof which is essentially algebraic rather a number theoretic proof.Thanks,,"['abstract-algebra', 'number-theory', 'algebraic-number-theory']"
87,Let $f$ and $g$ be two group homomorphisms from $G$ to $G'$. Is $H$ a subgroup of $G$?,Let  and  be two group homomorphisms from  to . Is  a subgroup of ?,f g G G' H G,"Can someone please verify this? Let $f$ and $g$ be two group homomorphisms from $G$ to $G'$. Let $H \subset G$ be the subset $\{x \in G: f(x)=g(x)\}$. Is $H$ a subgroup of $G$? Let $e$ and $e'$ denote the identity elements of $G$ and $G'$, respectively. Using the fact that a homomorphism maps the identity to the identity, we have \begin{eqnarray} f(e) = g(e) = e' \end{eqnarray} Therefore, $e \in H$. Now, let $x \in H$. Then, \begin{eqnarray} f(x) &=& g(x) \\ \end{eqnarray} Since we proved that $e \in H$, \begin{eqnarray} f(x \cdot x^{-1}) &=& g(x \cdot x^{-1}) \\ f(x)*f(x^{-1})&=&g(x)*g(x^{-1}) \\ f(x)*f(x^{-1}) &=& f(x)*g(x^{-1}) \\ f(x^{-1})&=&g(x^{-1}) \end{eqnarray} This implies that $x^{-1} \in H$. Now, let $x$ and $y$ be elements in $H$. Then, \begin{eqnarray} f(x \cdot y) &=& f(x)*f(y)\\ &=& g(x)*g(y)\\ &=& g(x \cdot y) \end{eqnarray} So, $x \cdot y \in H$. Since $H$ is closed under the operation $\cdot$, contains the identity and inverse elements, it is a subgroup of G. The associativity of $\cdot$ in $H$ carries over from the associativity of $\cdot$ in $G$.","Can someone please verify this? Let $f$ and $g$ be two group homomorphisms from $G$ to $G'$. Let $H \subset G$ be the subset $\{x \in G: f(x)=g(x)\}$. Is $H$ a subgroup of $G$? Let $e$ and $e'$ denote the identity elements of $G$ and $G'$, respectively. Using the fact that a homomorphism maps the identity to the identity, we have \begin{eqnarray} f(e) = g(e) = e' \end{eqnarray} Therefore, $e \in H$. Now, let $x \in H$. Then, \begin{eqnarray} f(x) &=& g(x) \\ \end{eqnarray} Since we proved that $e \in H$, \begin{eqnarray} f(x \cdot x^{-1}) &=& g(x \cdot x^{-1}) \\ f(x)*f(x^{-1})&=&g(x)*g(x^{-1}) \\ f(x)*f(x^{-1}) &=& f(x)*g(x^{-1}) \\ f(x^{-1})&=&g(x^{-1}) \end{eqnarray} This implies that $x^{-1} \in H$. Now, let $x$ and $y$ be elements in $H$. Then, \begin{eqnarray} f(x \cdot y) &=& f(x)*f(y)\\ &=& g(x)*g(y)\\ &=& g(x \cdot y) \end{eqnarray} So, $x \cdot y \in H$. Since $H$ is closed under the operation $\cdot$, contains the identity and inverse elements, it is a subgroup of G. The associativity of $\cdot$ in $H$ carries over from the associativity of $\cdot$ in $G$.",,"['abstract-algebra', 'group-theory', 'proof-verification']"
88,Intuition about the class equation (and flowers).,Intuition about the class equation (and flowers).,,"I apologize in advance for the size of the images I've devoted a lot of time and effort to draw them on the computer and i didn’t manage to re-size. I'd be really thankful if anyone would edit this post and re-size them. I have this image in my head of a flower whenever I'm thinking about the class equation and I’d like to make sure that there's nothing wrong with it. For this first diagram let $G$ be a group and let $x, y, z \in G$. I'll denote the normalizers of $x, y$ and $z$ as $N(x), N(y), N(z)$ respectively and the center as the usual $Z(G)$. There's a normalizer for every element of the group (I've put only 3 for convenience). diagram shows that the center is contained in every normalizer . Additionally, If i would to draw the normalizers of all the elements in $G$ their intersection would be precisely $Z(G)$ (is that right?). For the second diagram imagine we pick one of the elements of $G$, say x, and look at the partition of $G$ to cosets of $N(x)$. I've chosen for convenience that the index of $N(x)$ will be $5$. The unique cosets of $N(x)$ are represented as $aN(x), bN(x), cN(x)$ and $dN(x)$. The arrows represent conjugation relation. The unique conjugates of $x$ are given by conjugation with the elements $a, b, c, d$. ADDED: As PavelC mentioned in the comments there is a mistake in the labelling of the cosets. No two unique conjugates can be in the same coset and The fact that no element of the center is a conjugate of any other element (apart from itself) can be seen as a consequence of the center being contained in all the normalizers (or maybe more obvious than that is the fact that the normalizer of each of them is the whole group) Is there a mistake in anything I’ve written?","I apologize in advance for the size of the images I've devoted a lot of time and effort to draw them on the computer and i didn’t manage to re-size. I'd be really thankful if anyone would edit this post and re-size them. I have this image in my head of a flower whenever I'm thinking about the class equation and I’d like to make sure that there's nothing wrong with it. For this first diagram let $G$ be a group and let $x, y, z \in G$. I'll denote the normalizers of $x, y$ and $z$ as $N(x), N(y), N(z)$ respectively and the center as the usual $Z(G)$. There's a normalizer for every element of the group (I've put only 3 for convenience). diagram shows that the center is contained in every normalizer . Additionally, If i would to draw the normalizers of all the elements in $G$ their intersection would be precisely $Z(G)$ (is that right?). For the second diagram imagine we pick one of the elements of $G$, say x, and look at the partition of $G$ to cosets of $N(x)$. I've chosen for convenience that the index of $N(x)$ will be $5$. The unique cosets of $N(x)$ are represented as $aN(x), bN(x), cN(x)$ and $dN(x)$. The arrows represent conjugation relation. The unique conjugates of $x$ are given by conjugation with the elements $a, b, c, d$. ADDED: As PavelC mentioned in the comments there is a mistake in the labelling of the cosets. No two unique conjugates can be in the same coset and The fact that no element of the center is a conjugate of any other element (apart from itself) can be seen as a consequence of the center being contained in all the normalizers (or maybe more obvious than that is the fact that the normalizer of each of them is the whole group) Is there a mistake in anything I’ve written?",,"['abstract-algebra', 'group-theory', 'intuition']"
89,Semialgebraic conditions that convey properties of Galois group,Semialgebraic conditions that convey properties of Galois group,,"Let $f \in \mathbb{Z}[x]$ be a polynomial of degree $n$ with integer coefficients and let $G_f$ be the Galois group of $f$ over $\mathbb{Q}$. I am trying to collect results that convey some information about $G_f$ that only involve the coefficients of $f$ in semialgebraic conditions.  Let me explain what I mean by also giving the few such results that I have found. Example 1 When trying to determine $G_f$, the first thing to check is if $f$ is irreducible over $\mathbb{Q}$ since this holds if and only if $G_f$ is transitive.  However, the condition ""$f$ is irreducible over $\mathbb{Q}$"" is not a semialgebraic condition of the coefficients of $f$.  There are many popular methods to show that a polynomial is irreducible, including Eisenstein's criterion (a special case of Dumas's criterion, which uses Newton diagrams, see Section 2.2.1 in Polynomials ), irreducibility over $\mathbb{F}_p$ for some prime $p$, and Pólya-type criteria (also see Section 2.2.2 in Polynomials).  However, these criteria are still not semialgebraic conditions of the coefficients of $f$. Instead, an irreducibility criterion that only involves the coefficients in a semialgebraic expression is Perron's criterion: Let $f(x) = \sum_{i=0}^n a_i x^i$ be a polynomial with integer coefficients such that $a_n = 1$ and $a_0 \ne 0$. a) If $|a_{n-1}| > 1 + |a_{n-2}| + \dotsb + a_0$, then f is irreducible. b) If $|a_{n-1}| \ge 1 + |a_{n-2}| + \dotsb + a_0$ and $f(\pm 1) \ne 0$, then f is irreducible. The only other results of this kind that I know of are by Brauer (either Theorem 2.2.6 in Polynomials or results in On the Irreducibility of Polynomials with Large Third Coefficient ). Example 2 The method of resolvents for Galois groups seems quite strong to me.  If I understand it correctly, for each subgroup of $H \le S_n$, there exists a polynomial $\varphi_H$ such that $G_f \le H$ if and only if $\operatorname{Res}(f,\varphi_H)$ contains a rational root.  The simplest example of this is when $H = A_n$ is the alternating group, in which case $\varphi_H = f'$ is the derivative of $f$ and $\operatorname{Res}(f,\varphi_H) = x^2 - \Delta(f)$, where $\Delta(f)$ is the discriminant of $f$.  Once again, the condition that some polynomial does (or doesn't) contain a rational root is not a semialgebraic condition of the roots of $f$. For this simplest case of Galois resolvent, it is easy to obtain the kind of results that I am looking for. If $\Delta(f) < 0$, then $G_f \not\le A_n$. In this case (as well as in the stronger iff condition), there are an odd number of nonreal complex conjugate pairs, so the Galois group contains an odd permutation of order 2, namely complex conjugation. Question To explicitly state my question: What other results are known that convey information about the Galois group of $f$ but only involve the coefficients of $f$ in semialgebraic conditions?","Let $f \in \mathbb{Z}[x]$ be a polynomial of degree $n$ with integer coefficients and let $G_f$ be the Galois group of $f$ over $\mathbb{Q}$. I am trying to collect results that convey some information about $G_f$ that only involve the coefficients of $f$ in semialgebraic conditions.  Let me explain what I mean by also giving the few such results that I have found. Example 1 When trying to determine $G_f$, the first thing to check is if $f$ is irreducible over $\mathbb{Q}$ since this holds if and only if $G_f$ is transitive.  However, the condition ""$f$ is irreducible over $\mathbb{Q}$"" is not a semialgebraic condition of the coefficients of $f$.  There are many popular methods to show that a polynomial is irreducible, including Eisenstein's criterion (a special case of Dumas's criterion, which uses Newton diagrams, see Section 2.2.1 in Polynomials ), irreducibility over $\mathbb{F}_p$ for some prime $p$, and Pólya-type criteria (also see Section 2.2.2 in Polynomials).  However, these criteria are still not semialgebraic conditions of the coefficients of $f$. Instead, an irreducibility criterion that only involves the coefficients in a semialgebraic expression is Perron's criterion: Let $f(x) = \sum_{i=0}^n a_i x^i$ be a polynomial with integer coefficients such that $a_n = 1$ and $a_0 \ne 0$. a) If $|a_{n-1}| > 1 + |a_{n-2}| + \dotsb + a_0$, then f is irreducible. b) If $|a_{n-1}| \ge 1 + |a_{n-2}| + \dotsb + a_0$ and $f(\pm 1) \ne 0$, then f is irreducible. The only other results of this kind that I know of are by Brauer (either Theorem 2.2.6 in Polynomials or results in On the Irreducibility of Polynomials with Large Third Coefficient ). Example 2 The method of resolvents for Galois groups seems quite strong to me.  If I understand it correctly, for each subgroup of $H \le S_n$, there exists a polynomial $\varphi_H$ such that $G_f \le H$ if and only if $\operatorname{Res}(f,\varphi_H)$ contains a rational root.  The simplest example of this is when $H = A_n$ is the alternating group, in which case $\varphi_H = f'$ is the derivative of $f$ and $\operatorname{Res}(f,\varphi_H) = x^2 - \Delta(f)$, where $\Delta(f)$ is the discriminant of $f$.  Once again, the condition that some polynomial does (or doesn't) contain a rational root is not a semialgebraic condition of the roots of $f$. For this simplest case of Galois resolvent, it is easy to obtain the kind of results that I am looking for. If $\Delta(f) < 0$, then $G_f \not\le A_n$. In this case (as well as in the stronger iff condition), there are an odd number of nonreal complex conjugate pairs, so the Galois group contains an odd permutation of order 2, namely complex conjugation. Question To explicitly state my question: What other results are known that convey information about the Galois group of $f$ but only involve the coefficients of $f$ in semialgebraic conditions?",,"['abstract-algebra', 'reference-request', 'polynomials', 'galois-theory']"
90,Book recommendations for self-study at the level of 3rd-4th year undergraduate,Book recommendations for self-study at the level of 3rd-4th year undergraduate,,"I have only recently discovered an interested in mathematics and I could only take a year off work to be back at school. Needless to say, for financial reasons (couple of mortgages) I will need to return to work soon. Luckily though, I will be setting up a nice office with whiteboard etc to do some serious thinking in! I have found some examples as well as a lot of resources for first and second year courses, though by time I return to work basically any 2nd year math class that my university offers as well as most of the third year ones I will have taken. I know there are a ton of math professors on here, so I'm wondering if anyone can recommend a good textbook for learning. PDE (never done, save for some physics related stuff) Topology (never done but I am extremely interested in this topic) Complex Calculus (complex being imaginary numbers) Abstract Algebra (done second year only) Chaos (done intro only) Advanced ODE As well as any other texts/books you find interesting (Graph theory etc) most things involving logic I find very intriguing basically anything but statistics. I will have also completed a third year level analysis course so hopefully I can get through the math language. That being said, as dummy-proof of a layout as possible is preferred. Anyone out there that happens to have some material lying around to a related course that wouldn't mind emailing it to me or leaving there website below so I could steal some assignments and solutions I would really appreciate that as well. Most stuff I find on the internet isn't at the 3rd/4th year undergraduate level.","I have only recently discovered an interested in mathematics and I could only take a year off work to be back at school. Needless to say, for financial reasons (couple of mortgages) I will need to return to work soon. Luckily though, I will be setting up a nice office with whiteboard etc to do some serious thinking in! I have found some examples as well as a lot of resources for first and second year courses, though by time I return to work basically any 2nd year math class that my university offers as well as most of the third year ones I will have taken. I know there are a ton of math professors on here, so I'm wondering if anyone can recommend a good textbook for learning. PDE (never done, save for some physics related stuff) Topology (never done but I am extremely interested in this topic) Complex Calculus (complex being imaginary numbers) Abstract Algebra (done second year only) Chaos (done intro only) Advanced ODE As well as any other texts/books you find interesting (Graph theory etc) most things involving logic I find very intriguing basically anything but statistics. I will have also completed a third year level analysis course so hopefully I can get through the math language. That being said, as dummy-proof of a layout as possible is preferred. Anyone out there that happens to have some material lying around to a related course that wouldn't mind emailing it to me or leaving there website below so I could steal some assignments and solutions I would really appreciate that as well. Most stuff I find on the internet isn't at the 3rd/4th year undergraduate level.",,"['abstract-algebra', 'general-topology', 'complex-analysis', 'reference-request', 'partial-differential-equations']"
91,Working with Morphisms in Local Coordinates,Working with Morphisms in Local Coordinates,,"In light of the holiday, I would like to air a grievance. I have no good way to recoordinatize a morphism of varieties as I move between coordinate neighborhoods. Let me explain what I mean with an example. Consider the affine variety $E_Y$ cut out by the equation $$Z + Z^2 - (X^3 + ZX^2) = 0 \mbox{   }\mbox{   }\mbox{   }\mbox{   } (1)$$ The projective closure of $E_Y$, which we'll denote by $E,$ is an elliptic curve with identity element $O:=(0,0)$ in $X,Z$-coordinates. So there is addition morphism $\mu:E\times E \rightarrow E$ and an inverse morphism $-:E \rightarrow E.$ A calulation shows, $-\mu$ is given by $$X(-\mu) = \frac{(z_1 -z_0)^2 - (x_1 -x_0)(x_1z_0 - x_0z_1)}{(x_1 -x_0)^2 + (x_1 - x_0)(z_1 - z_0)} - x_0 - x_1$$ $$Z(-\mu) = \frac{z_1 - z_0}{x_1 -x_0}\left(\frac{(z_1 -z_0)^2 - (x_1 -x_0)(x_1z_0 - x_0z_1)}{(x_1 -x_0)^2 + (x_1 - x_0)(z_1 - z_0)} - x_0 - x_1\right) + \frac{x_1z_0 - x_0z_1}{x_1 -x_0} $$ on $E_Y \times E_Y$ outside of the locus $(x_1 -x_0)^2 + (x_1 - x_0)(z_1 - z_0) = 0.$ I would like to express the morphism $-\mu$ in terms of regular functions on some open neighborhood of $O \times O,$ but my current method of obtaining such an expression is ""to move symbols around"" in my expression for $-\mu$ using the relation of the curve, $(1),$ until I obtain a regular expression at $(0,0) \times (0,0).$ This is often a huge waste of time and becomes nearly impossible as the equations defining the variety become more complicated. So I'm wondering if there is a more methodical way to approach this problem? How does one do this in practice?","In light of the holiday, I would like to air a grievance. I have no good way to recoordinatize a morphism of varieties as I move between coordinate neighborhoods. Let me explain what I mean with an example. Consider the affine variety $E_Y$ cut out by the equation $$Z + Z^2 - (X^3 + ZX^2) = 0 \mbox{   }\mbox{   }\mbox{   }\mbox{   } (1)$$ The projective closure of $E_Y$, which we'll denote by $E,$ is an elliptic curve with identity element $O:=(0,0)$ in $X,Z$-coordinates. So there is addition morphism $\mu:E\times E \rightarrow E$ and an inverse morphism $-:E \rightarrow E.$ A calulation shows, $-\mu$ is given by $$X(-\mu) = \frac{(z_1 -z_0)^2 - (x_1 -x_0)(x_1z_0 - x_0z_1)}{(x_1 -x_0)^2 + (x_1 - x_0)(z_1 - z_0)} - x_0 - x_1$$ $$Z(-\mu) = \frac{z_1 - z_0}{x_1 -x_0}\left(\frac{(z_1 -z_0)^2 - (x_1 -x_0)(x_1z_0 - x_0z_1)}{(x_1 -x_0)^2 + (x_1 - x_0)(z_1 - z_0)} - x_0 - x_1\right) + \frac{x_1z_0 - x_0z_1}{x_1 -x_0} $$ on $E_Y \times E_Y$ outside of the locus $(x_1 -x_0)^2 + (x_1 - x_0)(z_1 - z_0) = 0.$ I would like to express the morphism $-\mu$ in terms of regular functions on some open neighborhood of $O \times O,$ but my current method of obtaining such an expression is ""to move symbols around"" in my expression for $-\mu$ using the relation of the curve, $(1),$ until I obtain a regular expression at $(0,0) \times (0,0).$ This is often a huge waste of time and becomes nearly impossible as the equations defining the variety become more complicated. So I'm wondering if there is a more methodical way to approach this problem? How does one do this in practice?",,"['algebraic-geometry', 'arithmetic-geometry', 'abstract-algebra']"
92,Solving $|\operatorname{Aut}(H)|=|\operatorname{Aut}(G)|$,Solving,|\operatorname{Aut}(H)|=|\operatorname{Aut}(G)|,"Couple of days ago, i actually asked this question : Existence of a normal subgroup with $\lvert\operatorname{Aut}{(H)}\rvert>\vert\operatorname{Aut}{(G)}\rvert$ . I was thinking about the converse of this statement. Suppose $G$ is a finite group, with subgroups $H$ and satisfies $$|\operatorname{Aut}(G)|=|\operatorname{Aut}(H)|,$$ then can anything be said about, $H$ or $G$ in terms of their structure. Will $H$ be normal or ....? Or one can even ask, that find all finite groups $G$ such that $|\operatorname{Aut}(H)|=|\operatorname{Aut}(G)|$ for every subgroup $H$ of $G$ ! I am not interested that much in this question as there seems one can have lot of Groups of this type, but i am more curious to know the behaviour of $H$ . Also, anyone interested can very well read this article in MO: https://mathoverflow.net/questions/1075/order-of-an-automorphism-of-a-finite-group","Couple of days ago, i actually asked this question : Existence of a normal subgroup with $\lvert\operatorname{Aut}{(H)}\rvert>\vert\operatorname{Aut}{(G)}\rvert$ . I was thinking about the converse of this statement. Suppose is a finite group, with subgroups and satisfies then can anything be said about, or in terms of their structure. Will be normal or ....? Or one can even ask, that find all finite groups such that for every subgroup of ! I am not interested that much in this question as there seems one can have lot of Groups of this type, but i am more curious to know the behaviour of . Also, anyone interested can very well read this article in MO: https://mathoverflow.net/questions/1075/order-of-an-automorphism-of-a-finite-group","G H |\operatorname{Aut}(G)|=|\operatorname{Aut}(H)|, H G H G |\operatorname{Aut}(H)|=|\operatorname{Aut}(G)| H G H",['abstract-algebra']
93,Fermat's Last Theorem ($n=3$) using the Eisenstein integers,Fermat's Last Theorem () using the Eisenstein integers,n=3,"I'm doing the first part of the following exercise in Miles Reid's Undergraduate Commutative Algebra : Exercise 0.18 : Prove the cases $n=3$ and $n=4$ of Fermat's last theorem. I'm assuming I should prove it using the Eisenstein integers $\Bbb Z[\omega]$ , where $\omega^2+\omega+1=0$ , since the previous exercise asked me to prove that $\Bbb Z[\omega]$ is a UFD. I would like to know if my proof is correct. Here we go: Let $$z^3=x^3+y^3\tag{1}$$ where $x,y,z\in\Bbb Z^+$ have no common factor. Eq. $(1)$ may be factored as $$z^3=(x+y)(x+\omega y)(x+\omega^2 y)\tag{2}$$ Notice also that we have the relations $$\begin{align}x+y&=x+y\\x+\omega y&=(x+y)-(1-\omega)y\\x+\omega^2 y&=(x+y)-(1+\omega)(1-\omega)y\end{align}\tag{3}$$ Therefore if there exists a prime $\pi\in\Bbb Z[\omega]$ that divides any two of the three factors, we must have $\pi\mid 1-\omega$ . We cannot have $\pi\mid y$ , since this would lead to $\pi\mid x,z$ , which again would lead to $N(\pi)\mid x,y,z$ . The element $1-\omega$ is itself a prime, since $N(1-\omega)=3$ is a prime, therefore we may assume $\pi=1-\omega$ . If $1-\omega$ divides every factor then we may write $$\begin{align}x+y&=u_1(1-\omega)\alpha^3\\x+\omega y&=u_2(1-\omega)\beta^3\\x+\omega^2y&=u_3(1-\omega)\gamma^3\end{align}\tag{4}$$ With $\alpha,\beta,\gamma$ having no common factor. Taking norms we get $(x+y)^2=3N(\alpha)^3$ , therefore $3\mid N(\alpha)$ , and so $9\mid x+y$ . Let $k$ be the highest power such that $3^k\mid x+y$ , $k\geq 2$ . We can then write $$\begin{align}(x+y)^3&\equiv_{3^{3k}} 0\\x^3+3xy(x+y)+y^3&\equiv_{3^{3k}} 0\\xy(x+y)&\equiv_{3^{3k-1}} 0\end{align}\tag{5}$$ Therefore either $3\mid x$ or $3\mid y$ . This implies that $3\mid x,y,z$ . Contradiction. Therefore all three factors are relatively prime, and we may write $$x+y=u_1\alpha^3\\x+\omega y=u_2\beta^3\\x+\omega^2 y= u_3\gamma^3\tag{6}$$ Notice that since $(x+y)+\omega (x+\omega y)+\omega^2(x+\omega^2 y)=0$ , we also have $$u_1\alpha^3+\omega u_2\beta^3+\omega^2 u_3\gamma^3=0\tag{7}$$ Now, reduce eq. $(7)$ modulo $(1-\omega)^2$ . We're then working over the ring $\Bbb Z[\omega]/(1-\omega)^2$ . Since $(1-\omega)^2=(3)$ , we have $$\Bbb Z[\omega]/(1-\omega)^2\cong\Bbb Z_3[\omega]\cong\Bbb Z_9\tag{8}$$ In this ring we observe: A unit $u\in\Bbb Z[\omega]$ is still a unit $\Bbb Z[\omega]/(1-\omega)^2$ Proof : We prove a stronger result. Let $R$ be any ring with multiplicative identity $1\in R$ , and $I\subseteq R$ any ideal of $R$ . Let $\phi$ be the canonical map $$\phi: R\twoheadrightarrow R/I$$ Suppose $u\in R^\times$ is a unit. Then $1=\phi(1)=\phi(uu^{-1})=\phi(u)\phi(u^{-1})$ , so the image of $u$ , $\phi(u)$ , is a unit in $R/I$ . Suppose $u\in\Bbb Z_9$ is a unit. Then $u^3=\pm 1$ . Proof : Either note that for every unit $u\in\Bbb Z_9$ we have $$u^{\phi(9)}-1\equiv_9 u^6-1\equiv_9 (u^3-1)(u^3+1)\equiv_9 0$$ so it's impossible for both factors to be divisible by $3$ , since that would imply $(u^3+1)-(u^3-1)\equiv_3 0$ . Or make a table of all the units in $\Bbb Z_9$ and calculate the corresponding cubes: $$\begin{array} {|r|r|}\hline u & 1 & 2 & 4 & 5 & 7 & 8 \\ \hline u^3 & 1 & -1 & 1 & -1 & 1 & -1\\\hline\end{array}$$ We can now conclude: Notice in eq. $(6)$ that $\alpha,\beta,\gamma\not\in (1-\omega)$ , so they must be units in $\Bbb Z[\omega]/(1-\omega)^2$ . Eq. $(7)$ now reduces to the sum of three terms who with value $\pm 1$ modulo $9$ $$(-1)^a+(-1)^b+(-1)^c\equiv_9 0$$ This is impossible.","I'm doing the first part of the following exercise in Miles Reid's Undergraduate Commutative Algebra : Exercise 0.18 : Prove the cases and of Fermat's last theorem. I'm assuming I should prove it using the Eisenstein integers , where , since the previous exercise asked me to prove that is a UFD. I would like to know if my proof is correct. Here we go: Let where have no common factor. Eq. may be factored as Notice also that we have the relations Therefore if there exists a prime that divides any two of the three factors, we must have . We cannot have , since this would lead to , which again would lead to . The element is itself a prime, since is a prime, therefore we may assume . If divides every factor then we may write With having no common factor. Taking norms we get , therefore , and so . Let be the highest power such that , . We can then write Therefore either or . This implies that . Contradiction. Therefore all three factors are relatively prime, and we may write Notice that since , we also have Now, reduce eq. modulo . We're then working over the ring . Since , we have In this ring we observe: A unit is still a unit Proof : We prove a stronger result. Let be any ring with multiplicative identity , and any ideal of . Let be the canonical map Suppose is a unit. Then , so the image of , , is a unit in . Suppose is a unit. Then . Proof : Either note that for every unit we have so it's impossible for both factors to be divisible by , since that would imply . Or make a table of all the units in and calculate the corresponding cubes: We can now conclude: Notice in eq. that , so they must be units in . Eq. now reduces to the sum of three terms who with value modulo This is impossible.","n=3 n=4 \Bbb Z[\omega] \omega^2+\omega+1=0 \Bbb Z[\omega] z^3=x^3+y^3\tag{1} x,y,z\in\Bbb Z^+ (1) z^3=(x+y)(x+\omega y)(x+\omega^2 y)\tag{2} \begin{align}x+y&=x+y\\x+\omega y&=(x+y)-(1-\omega)y\\x+\omega^2 y&=(x+y)-(1+\omega)(1-\omega)y\end{align}\tag{3} \pi\in\Bbb Z[\omega] \pi\mid 1-\omega \pi\mid y \pi\mid x,z N(\pi)\mid x,y,z 1-\omega N(1-\omega)=3 \pi=1-\omega 1-\omega \begin{align}x+y&=u_1(1-\omega)\alpha^3\\x+\omega y&=u_2(1-\omega)\beta^3\\x+\omega^2y&=u_3(1-\omega)\gamma^3\end{align}\tag{4} \alpha,\beta,\gamma (x+y)^2=3N(\alpha)^3 3\mid N(\alpha) 9\mid x+y k 3^k\mid x+y k\geq 2 \begin{align}(x+y)^3&\equiv_{3^{3k}} 0\\x^3+3xy(x+y)+y^3&\equiv_{3^{3k}} 0\\xy(x+y)&\equiv_{3^{3k-1}} 0\end{align}\tag{5} 3\mid x 3\mid y 3\mid x,y,z x+y=u_1\alpha^3\\x+\omega y=u_2\beta^3\\x+\omega^2 y= u_3\gamma^3\tag{6} (x+y)+\omega (x+\omega y)+\omega^2(x+\omega^2 y)=0 u_1\alpha^3+\omega u_2\beta^3+\omega^2 u_3\gamma^3=0\tag{7} (7) (1-\omega)^2 \Bbb Z[\omega]/(1-\omega)^2 (1-\omega)^2=(3) \Bbb Z[\omega]/(1-\omega)^2\cong\Bbb Z_3[\omega]\cong\Bbb Z_9\tag{8} u\in\Bbb Z[\omega] \Bbb Z[\omega]/(1-\omega)^2 R 1\in R I\subseteq R R \phi \phi: R\twoheadrightarrow R/I u\in R^\times 1=\phi(1)=\phi(uu^{-1})=\phi(u)\phi(u^{-1}) u \phi(u) R/I u\in\Bbb Z_9 u^3=\pm 1 u\in\Bbb Z_9 u^{\phi(9)}-1\equiv_9 u^6-1\equiv_9 (u^3-1)(u^3+1)\equiv_9 0 3 (u^3+1)-(u^3-1)\equiv_3 0 \Bbb Z_9 \begin{array} {|r|r|}\hline u & 1 & 2 & 4 & 5 & 7 & 8 \\ \hline u^3 & 1 & -1 & 1 & -1 & 1 & -1\\\hline\end{array} (6) \alpha,\beta,\gamma\not\in (1-\omega) \Bbb Z[\omega]/(1-\omega)^2 (7) \pm 1 9 (-1)^a+(-1)^b+(-1)^c\equiv_9 0","['abstract-algebra', 'commutative-algebra', 'solution-verification', 'algebraic-number-theory', 'eisenstein-integers']"
94,Is $\mathrm{U}(\mathfrak{a} \oplus \mathfrak{b}) \cong \mathrm{U}(\mathfrak{a}) \otimes \mathrm{U}(\mathfrak{b})$ over a commutative ring?,Is  over a commutative ring?,\mathrm{U}(\mathfrak{a} \oplus \mathfrak{b}) \cong \mathrm{U}(\mathfrak{a}) \otimes \mathrm{U}(\mathfrak{b}),"Let $\mathbb{k}$ be a commutative ring and let $\mathfrak{g}$ be a Lie-Algebra over $\mathbb{k}$ . Suppose that $\mathfrak{a}$ and $\mathfrak{b}$ are two Lie subalgebras of $\mathfrak{g}$ such that $\mathfrak{g} = \mathfrak{a} \oplus \mathfrak{b}$ as $\mathbb{k}$ -modules. We then have a homomorphism of $\operatorname{U}(\mathfrak{a})$ - $\operatorname{U}(\mathfrak{b})$ -bimodules $$   \Phi   \colon   \operatorname{U}(\mathfrak{a}) \otimes_{\mathbb{k}} \operatorname{U}(\mathfrak{b})   \to   \operatorname{U}(\mathfrak{g}) \,,   \quad   x \otimes y   \mapsto   xy \,. $$ Question. Is the homomorphism $\Phi$ an isomorphism of bimodules? My thoughts so far: If both $\mathfrak{a}$ and $\mathfrak{b}$ are free as $\mathbb{k}$ -modules (e.g. if $\mathbb{k}$ is a field), then $\mathfrak{g}$ is also free as a $\mathbb{k}$ -module. One can then use the PBW-theorem to see that $\Phi$ is a bijection on the induced bases. It is then an isomorphism of $\mathbb{k}$ -modules, and thus an isomorphism of bimodules. If $\mathfrak{a}$ and $\mathfrak{b}$ is are ideals of $\mathfrak{g}$ , then the decomposition $\mathfrak{g} = \mathfrak{a} \oplus \mathfrak{b}$ is one of Lie algebras. One can then see that both sides of $\Phi$ satisfy the same universal property as $\mathbb{k}$ -algebras, and that the above map $\Phi$ is even an isomorphism of $\mathbb{k}$ -algebras. I think that $\Phi$ is in general still surjective. One should still be able to get module generating sets by PBW-monomials for the universal enveloping algebras, and then see that $\Phi$ is surjective on the induced generators. But I’m not sure what happens with the injectivity of $\Phi$ .","Let be a commutative ring and let be a Lie-Algebra over . Suppose that and are two Lie subalgebras of such that as -modules. We then have a homomorphism of - -bimodules Question. Is the homomorphism an isomorphism of bimodules? My thoughts so far: If both and are free as -modules (e.g. if is a field), then is also free as a -module. One can then use the PBW-theorem to see that is a bijection on the induced bases. It is then an isomorphism of -modules, and thus an isomorphism of bimodules. If and is are ideals of , then the decomposition is one of Lie algebras. One can then see that both sides of satisfy the same universal property as -algebras, and that the above map is even an isomorphism of -algebras. I think that is in general still surjective. One should still be able to get module generating sets by PBW-monomials for the universal enveloping algebras, and then see that is surjective on the induced generators. But I’m not sure what happens with the injectivity of .","\mathbb{k} \mathfrak{g} \mathbb{k} \mathfrak{a} \mathfrak{b} \mathfrak{g} \mathfrak{g} = \mathfrak{a} \oplus \mathfrak{b} \mathbb{k} \operatorname{U}(\mathfrak{a}) \operatorname{U}(\mathfrak{b}) 
  \Phi
  \colon
  \operatorname{U}(\mathfrak{a}) \otimes_{\mathbb{k}} \operatorname{U}(\mathfrak{b})
  \to
  \operatorname{U}(\mathfrak{g}) \,,
  \quad
  x \otimes y
  \mapsto
  xy \,.
 \Phi \mathfrak{a} \mathfrak{b} \mathbb{k} \mathbb{k} \mathfrak{g} \mathbb{k} \Phi \mathbb{k} \mathfrak{a} \mathfrak{b} \mathfrak{g} \mathfrak{g} = \mathfrak{a} \oplus \mathfrak{b} \Phi \mathbb{k} \Phi \mathbb{k} \Phi \Phi \Phi","['abstract-algebra', 'lie-algebras']"
95,"Generic behavior amongst ""polynomialish"" models of $\mathsf{Q}$","Generic behavior amongst ""polynomialish"" models of",\mathsf{Q},"Now asked at MO . For $\mathcal{R}=(R_i)_{i\in\mathbb{N}}$ a sequence of countable commutative ordered rings such that $R_i$ is an sub-ordered ring of $R_{i+1}$ and $R_0=\mathbb{Z}$ , let $$Poly_\mathcal{R}=\{\sum_{i<n}a_ix^i\in (\bigcup_{i\in\omega}R_i)[x]: a_i\in R_i\}$$ be the set of polynomials whose $i$ th term comes from the $i$ th ring in the sequence. $Poly_\mathcal{R}$ naturally inherits the structure of an ordered ring, namely by taking as the set of positive elements all those $p$ whose leading coefficient is positive in the sense of the appropriate $R_i$ . For example, we'll have $x>1$ since the leading coefficient of $x-1$ is $1$ which is positive. The set $Poly_\mathcal{R}^+$ of nonnegative elements of $Poly_\mathcal{R}$ is a nonstandard model of Robinson's $\mathsf{Q}$ . Unsurprisingly the theory of $Poly^+_\mathcal{R}$ depends on $\mathcal{R}$ . I'm interested in the ""generic"" behavior of such models. To make this precise, let $\mathfrak{X}$ be the space of all such ordered ring sequences topologized by taking as basic open sets of those of the form $$[(R_i)_{i<n}]:=\{\mathcal{S}=(S_i)_{i\in\omega}: \forall i<n(S_i=R_i)\}$$ for each finite sequence of ordered rings $(R_i)_{i<n}$ . (Fine, this $\mathfrak{X}$ is a proper-class-sized space; do any of the usual tricks to fix this.) Say that a sentence $\varphi$ in the language of Robinson arithmetic is generically true (resp. false ) iff $\{\mathcal{R}: Poly_\mathcal{R}^+\models\varphi\}$ is comeager in $\mathfrak{X}$ (resp. meager). For example, ""Every number is either even or odd"" is neither generically true nor generically false. On the other hand, the more complicated sentence $(*)\quad$ ""There is some $k$ such that for all $n>k$ there is some $m<k$ such that $n+m$ is even"" is generically true. Given a (code for a) basic open set $(R_i)_{i<n}$ , consider the extension $(S_i)_{i<n+1}$ with $S_i=R_i$ for $i<n$ and $S_n$ be some ring containing $R_{n-1}$ as a subring in which $2x=1$ has a solution. Then every $\mathcal{R}\in[(S_i)_{i<n+1}]$ has $Poly_\mathcal{R}^+\models(*)$ : just take $k=x^n$ . (Note that this is nontrivial since $Poly_{(\mathbb{Z},\mathbb{Z},\mathbb{Z}, ...)}^+\models\neg(*)$ .) In general, many basic facts about modular arithmetic have ""perturbed"" versions a la $(*)$ above which are generically true. I'm interested in unpacking this more. Suppose we have a sentence of the form $$\varphi\equiv\forall x_1,...,x_n\exists y_1,...,y_k\theta(x_1,...,x_n,y_1,...,y_k)$$ with $\theta$ quantifier-free. Define the perturbed version of $\varphi$ , which I'll denote by "" $\varphi^\circ$ ,"" as: $$\exists u\forall x_1,...,x_n\exists x_1',...,x_n'\mbox{ such that }$$ $$\bigwedge_{i\le n}\lfloor{x_i\over u}\rfloor\le x_i'\le x_iu\quad\mbox{ and }$$ $$\quad\exists y_1,...,y_k(\theta(x_1', ..., x_n', y_1,...,y_k)).$$ The idea is that $\varphi^\circ$ doesn't require $\exists\overline{y}\theta$ to hold on every tuple of $x$ s; we're allowed to perturb the inputs by up to a fixed factor $u$ . Note that by ""disjointifying"" the variables involved we have in an appropriate sense that the perturbation of a conjunction is the conjunction of the perturbations. In the original version of this question I didn't notice that variable-disjointification did this, and since commutation with conjunctions seems an obviously desirable property I used a much nastier notion of perturbation which I now realize was silly. My question is: Suppose $\varphi$ is an $\forall\exists$ -sentence in the language of Robinson arithmetic and $\mathbb{N}\models\varphi$ . Is $\varphi^\circ$ generically true? (We can also consider ""perturbed versions"" of higher-complexity sentences, but the $\forall\exists$ -case seems already interesting. However, the particular question above is boring for higher-complexity sentences: ""There is some $k$ such that for every $n$ there is some $m\in (\lfloor {n\over k}\rfloor, nk)$ such that $m$ is not even"" is generically false .)","Now asked at MO . For a sequence of countable commutative ordered rings such that is an sub-ordered ring of and , let be the set of polynomials whose th term comes from the th ring in the sequence. naturally inherits the structure of an ordered ring, namely by taking as the set of positive elements all those whose leading coefficient is positive in the sense of the appropriate . For example, we'll have since the leading coefficient of is which is positive. The set of nonnegative elements of is a nonstandard model of Robinson's . Unsurprisingly the theory of depends on . I'm interested in the ""generic"" behavior of such models. To make this precise, let be the space of all such ordered ring sequences topologized by taking as basic open sets of those of the form for each finite sequence of ordered rings . (Fine, this is a proper-class-sized space; do any of the usual tricks to fix this.) Say that a sentence in the language of Robinson arithmetic is generically true (resp. false ) iff is comeager in (resp. meager). For example, ""Every number is either even or odd"" is neither generically true nor generically false. On the other hand, the more complicated sentence ""There is some such that for all there is some such that is even"" is generically true. Given a (code for a) basic open set , consider the extension with for and be some ring containing as a subring in which has a solution. Then every has : just take . (Note that this is nontrivial since .) In general, many basic facts about modular arithmetic have ""perturbed"" versions a la above which are generically true. I'm interested in unpacking this more. Suppose we have a sentence of the form with quantifier-free. Define the perturbed version of , which I'll denote by "" ,"" as: The idea is that doesn't require to hold on every tuple of s; we're allowed to perturb the inputs by up to a fixed factor . Note that by ""disjointifying"" the variables involved we have in an appropriate sense that the perturbation of a conjunction is the conjunction of the perturbations. In the original version of this question I didn't notice that variable-disjointification did this, and since commutation with conjunctions seems an obviously desirable property I used a much nastier notion of perturbation which I now realize was silly. My question is: Suppose is an -sentence in the language of Robinson arithmetic and . Is generically true? (We can also consider ""perturbed versions"" of higher-complexity sentences, but the -case seems already interesting. However, the particular question above is boring for higher-complexity sentences: ""There is some such that for every there is some such that is not even"" is generically false .)","\mathcal{R}=(R_i)_{i\in\mathbb{N}} R_i R_{i+1} R_0=\mathbb{Z} Poly_\mathcal{R}=\{\sum_{i<n}a_ix^i\in (\bigcup_{i\in\omega}R_i)[x]: a_i\in R_i\} i i Poly_\mathcal{R} p R_i x>1 x-1 1 Poly_\mathcal{R}^+ Poly_\mathcal{R} \mathsf{Q} Poly^+_\mathcal{R} \mathcal{R} \mathfrak{X} [(R_i)_{i<n}]:=\{\mathcal{S}=(S_i)_{i\in\omega}: \forall i<n(S_i=R_i)\} (R_i)_{i<n} \mathfrak{X} \varphi \{\mathcal{R}: Poly_\mathcal{R}^+\models\varphi\} \mathfrak{X} (*)\quad k n>k m<k n+m (R_i)_{i<n} (S_i)_{i<n+1} S_i=R_i i<n S_n R_{n-1} 2x=1 \mathcal{R}\in[(S_i)_{i<n+1}] Poly_\mathcal{R}^+\models(*) k=x^n Poly_{(\mathbb{Z},\mathbb{Z},\mathbb{Z}, ...)}^+\models\neg(*) (*) \varphi\equiv\forall x_1,...,x_n\exists y_1,...,y_k\theta(x_1,...,x_n,y_1,...,y_k) \theta \varphi \varphi^\circ \exists u\forall x_1,...,x_n\exists x_1',...,x_n'\mbox{ such that } \bigwedge_{i\le n}\lfloor{x_i\over u}\rfloor\le x_i'\le x_iu\quad\mbox{ and } \quad\exists y_1,...,y_k(\theta(x_1', ..., x_n', y_1,...,y_k)). \varphi^\circ \exists\overline{y}\theta x u \varphi \forall\exists \mathbb{N}\models\varphi \varphi^\circ \forall\exists k n m\in (\lfloor {n\over k}\rfloor, nk) m","['abstract-algebra', 'logic', 'ring-theory', 'model-theory']"
96,Are all verbal automorphisms inner power automorphisms?,Are all verbal automorphisms inner power automorphisms?,,"Suppose $G$ is a group. $\DeclareMathOperator{\Wa}{Wa}\DeclareMathOperator{\Tame}{Tame}\DeclareMathOperator{\Aut}{Aut}$ Lets call $\phi \in \Aut(G)$ verbal automorphism iff $\exists n \in \mathbb{N}, \{a_i\}_{i=0}^n \subset G, \{e_i\}_{i=0}^n \subset \{-1; 1\}$ such that $\forall t \in G$ $(\phi(t) = a_0t^{e_1}a_1…t^{e_n}a_n)$ .  One can easily see, that all the verbal automorphisms form a normal subgroup in $\Aut(G)$ . Lets denote this subgroup as $Va(G)$ . One can see, that sometimes $Va(G)$ is a proper subgroup: for example, $C_2 \times C_2$ has no nontrivial verbal automorphisms, but $\Aut(C_2 \times C_2)$ is isomorphic to $S_3$ . Also one can see that a subgroup is invariant under verbal automorphisms iff it is normal. Lets call $\phi \in \Aut(G)$ inner power automorphism iff it is a composition of an inner automorphism and a universal power automorphism. It is easy to see, that all inner power automorphisms form a normal subgroup in $\Aut(G)$ . Lets denote this subgroup as $Ip(G)$ . One can also see that $Ip(G) \leq Va(G)$ (as $\forall \phi \in Ip(G) \exists a \in G, n \in \mathbb{Z}$ such that $\forall t \in G (\phi(t) = a^{-1}t^na$ )) and that $Ip(G)$ is always isomorphic to a homomorphic image of $\frac{G}{Z(G)} \times C_{\exp(G)}$ , where $\exp(G)$ is the exponent of $G$ . Is the statement $Va(G) = Ip(G)$ always true? If $G$ is abelian, then it definitely is, as all verbal automorphisms of any abelian group are universal power automorphisms. If $G$ is complete then the statement is also true, as all automorphisms of a complete group are inner. However, I failed to find out anything other than that. Any help will be appreciated.","Suppose is a group. Lets call verbal automorphism iff such that .  One can easily see, that all the verbal automorphisms form a normal subgroup in . Lets denote this subgroup as . One can see, that sometimes is a proper subgroup: for example, has no nontrivial verbal automorphisms, but is isomorphic to . Also one can see that a subgroup is invariant under verbal automorphisms iff it is normal. Lets call inner power automorphism iff it is a composition of an inner automorphism and a universal power automorphism. It is easy to see, that all inner power automorphisms form a normal subgroup in . Lets denote this subgroup as . One can also see that (as such that )) and that is always isomorphic to a homomorphic image of , where is the exponent of . Is the statement always true? If is abelian, then it definitely is, as all verbal automorphisms of any abelian group are universal power automorphisms. If is complete then the statement is also true, as all automorphisms of a complete group are inner. However, I failed to find out anything other than that. Any help will be appreciated.","G \DeclareMathOperator{\Wa}{Wa}\DeclareMathOperator{\Tame}{Tame}\DeclareMathOperator{\Aut}{Aut} \phi \in \Aut(G) \exists n \in \mathbb{N}, \{a_i\}_{i=0}^n \subset G, \{e_i\}_{i=0}^n \subset \{-1; 1\} \forall t \in G (\phi(t) = a_0t^{e_1}a_1…t^{e_n}a_n) \Aut(G) Va(G) Va(G) C_2 \times C_2 \Aut(C_2 \times C_2) S_3 \phi \in \Aut(G) \Aut(G) Ip(G) Ip(G) \leq Va(G) \forall \phi \in Ip(G) \exists a \in G, n \in \mathbb{Z} \forall t \in G (\phi(t) = a^{-1}t^na Ip(G) \frac{G}{Z(G)} \times C_{\exp(G)} \exp(G) G Va(G) = Ip(G) G G","['abstract-algebra', 'group-theory', 'normal-subgroups', 'automorphism-group', 'power-automorphism']"
97,An equivalence between group cohomology and sheaf cohomology,An equivalence between group cohomology and sheaf cohomology,,"I'm recently reading group cohomology from Serre's book local fields, and he uses there the following terminology $H^q(G,A)$ the q-th degree cohomology of G with coefficent in $A$. So i started to think that probably he used this terminology to create an analogy with the cohomology of a space with coefficient in a sheaf. So i started to think that maybe there is a canonical way to turn a sheaf in to a module over the fundamental group, in such a way that fixed points turns out to be global section, so i got to the following question: 1)Is there a natural way to associate to each sheaf(possibly with additional structure) over a (reasonable) space a module over his fundamental group(possibly with additional structure), and possibly the other way around? That is: is there an equivalence between sheaves(possibly with more structure) and modules over the fundamental group(possibly with more structure), inducing isomorphisms on the cohomology(respectively Cech, and group cohomology)? 2)(this is a very wild guess question, but i would be amused if the answer is yes) Wondering about 1) i started to think that in many cases the obstruction to define a global section comes from the fact that after a non trivial loop(i have the complex logarithm in mind) a local object gets a different value in the starting point; so this seemed to me a possible link between the two concepts: if 1) is true, then the module in 1) should take account of all this local data, and the loops acts exactly via the change of value of the ""multi-function"" after that the loop is performed, in a way that well defined global sections are exactly those that keep fixed by this procedure. Does something like this exist? I know that 2) is not a precise question, but i hope that the link between the 2 concepts that i am asking suggest a reference to a precise statement of the form that i am hoping, from some of you. Many thanks and apologize for the ignorance(in usual life i think about arithmetic)","I'm recently reading group cohomology from Serre's book local fields, and he uses there the following terminology $H^q(G,A)$ the q-th degree cohomology of G with coefficent in $A$. So i started to think that probably he used this terminology to create an analogy with the cohomology of a space with coefficient in a sheaf. So i started to think that maybe there is a canonical way to turn a sheaf in to a module over the fundamental group, in such a way that fixed points turns out to be global section, so i got to the following question: 1)Is there a natural way to associate to each sheaf(possibly with additional structure) over a (reasonable) space a module over his fundamental group(possibly with additional structure), and possibly the other way around? That is: is there an equivalence between sheaves(possibly with more structure) and modules over the fundamental group(possibly with more structure), inducing isomorphisms on the cohomology(respectively Cech, and group cohomology)? 2)(this is a very wild guess question, but i would be amused if the answer is yes) Wondering about 1) i started to think that in many cases the obstruction to define a global section comes from the fact that after a non trivial loop(i have the complex logarithm in mind) a local object gets a different value in the starting point; so this seemed to me a possible link between the two concepts: if 1) is true, then the module in 1) should take account of all this local data, and the loops acts exactly via the change of value of the ""multi-function"" after that the loop is performed, in a way that well defined global sections are exactly those that keep fixed by this procedure. Does something like this exist? I know that 2) is not a precise question, but i hope that the link between the 2 concepts that i am asking suggest a reference to a precise statement of the form that i am hoping, from some of you. Many thanks and apologize for the ignorance(in usual life i think about arithmetic)",,"['abstract-algebra', 'algebraic-topology', 'homology-cohomology', 'group-cohomology', 'sheaf-cohomology']"
98,"Let $F_\infty=\bigcup_{n\geq1}\operatorname{Q}(2^{1/2^n})$ , $K_\infty=\bigcup_{n\geq1}\operatorname{Q}(\zeta_{2^n})$, what is the intersection?","Let  , , what is the intersection?",F_\infty=\bigcup_{n\geq1}\operatorname{Q}(2^{1/2^n}) K_\infty=\bigcup_{n\geq1}\operatorname{Q}(\zeta_{2^n}),"Let $F_\infty=\bigcup_{n\geq1}\operatorname{Q}(2^{1/2^n})$ and $K_\infty=\bigcup_{n\geq1}\operatorname{Q}(\zeta_{2^n})$. What is the intersection $F_\infty\cap K_\infty$? (Here $\zeta_{2^n}$ is a primitive $2^n$th root of unity.) Jyrki Lahtonen used the method of algebraic number theory to deduce that $$\operatorname{Q}(2^{1/p^n})\cap\operatorname{Q}(\zeta_{p^n})=\operatorname{Q},\quad p\neq2\ \text{an odd prime number}.$$ See here . I guess that $$F_\infty\cap K_\infty=\operatorname{Q}(\sqrt{2}).$$ We know that $F_\infty$ is a real field and the maximal real subfield $K^+_\infty$ of $K_\infty$ is an extension of $\operatorname{Z}_2$ ($2$-adic integers). My question is 1. For every $2^n$, does $F_\infty$ contain just one field of degree $2^n$? 2. What is the maximal sub-abelian extension of $F_\infty$? (I mean the intersection of $F_\infty$ and $\operatorname{Q}^{\operatorname{ab}}$ .) If we replace $2$ with any odd prime $p$, we can also define $F_\infty=\bigcup_{n\geq1}\operatorname{Q}(p^{1/p^n})$ and $K_\infty=\bigcup_{n\geq1}\operatorname{Q}(\zeta_{p^n})$ and can also ask the same question: what is the intersection of $F_\infty$ and $K_\infty$?","Let $F_\infty=\bigcup_{n\geq1}\operatorname{Q}(2^{1/2^n})$ and $K_\infty=\bigcup_{n\geq1}\operatorname{Q}(\zeta_{2^n})$. What is the intersection $F_\infty\cap K_\infty$? (Here $\zeta_{2^n}$ is a primitive $2^n$th root of unity.) Jyrki Lahtonen used the method of algebraic number theory to deduce that $$\operatorname{Q}(2^{1/p^n})\cap\operatorname{Q}(\zeta_{p^n})=\operatorname{Q},\quad p\neq2\ \text{an odd prime number}.$$ See here . I guess that $$F_\infty\cap K_\infty=\operatorname{Q}(\sqrt{2}).$$ We know that $F_\infty$ is a real field and the maximal real subfield $K^+_\infty$ of $K_\infty$ is an extension of $\operatorname{Z}_2$ ($2$-adic integers). My question is 1. For every $2^n$, does $F_\infty$ contain just one field of degree $2^n$? 2. What is the maximal sub-abelian extension of $F_\infty$? (I mean the intersection of $F_\infty$ and $\operatorname{Q}^{\operatorname{ab}}$ .) If we replace $2$ with any odd prime $p$, we can also define $F_\infty=\bigcup_{n\geq1}\operatorname{Q}(p^{1/p^n})$ and $K_\infty=\bigcup_{n\geq1}\operatorname{Q}(\zeta_{p^n})$ and can also ask the same question: what is the intersection of $F_\infty$ and $K_\infty$?",,"['abstract-algebra', 'field-theory', 'algebraic-number-theory', 'extension-field']"
99,Isomorphic groups but not isomorphic rings,Isomorphic groups but not isomorphic rings,,"Provide an example of two rings that have the same characteristic, are isomorphic as groups but are not isomorphic as rings. I'm confused with how to being. I know that having the same characteristic means that the concatenation is the same number to receive the zero element.","Provide an example of two rings that have the same characteristic, are isomorphic as groups but are not isomorphic as rings. I'm confused with how to being. I know that having the same characteristic means that the concatenation is the same number to receive the zero element.",,['abstract-algebra']
