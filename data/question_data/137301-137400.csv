,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Differential operators acting on the Schwartz space,Differential operators acting on the Schwartz space,,I was asking me the following question and cannot find any answer to it. Any help/suggestion is most than welcome! Let $D$ be a linear differential operator with polynomial coefficients on $\mathbb{R}^n$. Thus $$\displaystyle D=\sum_{\lvert \alpha\rvert\leqslant k} P_\alpha(x) \frac{\partial^{\lvert \alpha\rvert}}{\partial x^\alpha}$$ where the $P_\alpha$'s are polynomial functions on $\mathbb{R}^n$. Consider $D$ as an endomorphism of the space of Schwartz functions $\mathcal{S}(\mathbb{R}^n)$. My question is now the following: Is the image of $D$ a closed subspace of $\mathcal{S}(\mathbb{R}^n)$ (for its natural Frechet topology)?,I was asking me the following question and cannot find any answer to it. Any help/suggestion is most than welcome! Let $D$ be a linear differential operator with polynomial coefficients on $\mathbb{R}^n$. Thus $$\displaystyle D=\sum_{\lvert \alpha\rvert\leqslant k} P_\alpha(x) \frac{\partial^{\lvert \alpha\rvert}}{\partial x^\alpha}$$ where the $P_\alpha$'s are polynomial functions on $\mathbb{R}^n$. Consider $D$ as an endomorphism of the space of Schwartz functions $\mathcal{S}(\mathbb{R}^n)$. My question is now the following: Is the image of $D$ a closed subspace of $\mathcal{S}(\mathbb{R}^n)$ (for its natural Frechet topology)?,,"['functional-analysis', 'ordinary-differential-equations', 'topological-vector-spaces', 'schwartz-space']"
1,Ordinary differential equation with polynomial terms,Ordinary differential equation with polynomial terms,,"The original equation I had was: $$-y''+(x^2+2x^4-2\alpha)y=0$$ Where $\alpha$ is a real parameter $\geq 0$ and we require the solutions to go to $0$ at infinity. With the substitution $y=p(x)e^{-\frac{x^2}{2}}$ I got the equation: $$-p''+2xp'+p(1+2x^4-2\alpha)=0$$ I tried the power series method, getting: $$a_2=(\frac{1}{2}-\alpha)a_0$$ $$a_3=\frac{(\frac{3}{2}-\alpha)a_1}{3}$$ $$a_4=\frac{(\frac{5}{2}-\alpha)(\frac{1}{2}-\alpha)a_0}{6}$$ $$a_5=\frac{(\frac{7}{2}-\alpha)(\frac{3}{2}-\alpha)a_1}{30}$$ And after $n=4$: $$a_{n+2}=\frac{2(n+\frac{1}{2}-\alpha)a_n+2a_{n-4}}{(n+1)(n+2)}$$ Are the solutions to this equation known? What would you do?","The original equation I had was: $$-y''+(x^2+2x^4-2\alpha)y=0$$ Where $\alpha$ is a real parameter $\geq 0$ and we require the solutions to go to $0$ at infinity. With the substitution $y=p(x)e^{-\frac{x^2}{2}}$ I got the equation: $$-p''+2xp'+p(1+2x^4-2\alpha)=0$$ I tried the power series method, getting: $$a_2=(\frac{1}{2}-\alpha)a_0$$ $$a_3=\frac{(\frac{3}{2}-\alpha)a_1}{3}$$ $$a_4=\frac{(\frac{5}{2}-\alpha)(\frac{1}{2}-\alpha)a_0}{6}$$ $$a_5=\frac{(\frac{7}{2}-\alpha)(\frac{3}{2}-\alpha)a_1}{30}$$ And after $n=4$: $$a_{n+2}=\frac{2(n+\frac{1}{2}-\alpha)a_n+2a_{n-4}}{(n+1)(n+2)}$$ Are the solutions to this equation known? What would you do?",,"['calculus', 'real-analysis', 'ordinary-differential-equations']"
2,Solving an integral within an integral,Solving an integral within an integral,,"I have been trying for a while now to solve the following integral: $$ \int_0^t e^{-ct} \left(\int_0^t(1-M_r t)^a e^{ct} dt\right) (1-M_r t)^{-(a+1)} dt  $$ I know the answer is: $$ \frac {1}{M_r^2} [(1-M_r t) \ln( (1-M_r t) -1)] $$ the only approach I can think of is to use integration by parts and setting the integral within the integral to be the function which is differentiated. However, this does not seem to lead to the answer. Does anybody have ANY idea? A reference, term I could search. Literally anything that could point me to the right trail would be helpful.","I have been trying for a while now to solve the following integral: $$ \int_0^t e^{-ct} \left(\int_0^t(1-M_r t)^a e^{ct} dt\right) (1-M_r t)^{-(a+1)} dt  $$ I know the answer is: $$ \frac {1}{M_r^2} [(1-M_r t) \ln( (1-M_r t) -1)] $$ the only approach I can think of is to use integration by parts and setting the integral within the integral to be the function which is differentiated. However, this does not seem to lead to the answer. Does anybody have ANY idea? A reference, term I could search. Literally anything that could point me to the right trail would be helpful.",,"['integration', 'ordinary-differential-equations', 'definite-integrals', 'indefinite-integrals', 'integration-by-parts']"
3,Stuck on solving this PDE of 2nd Order,Stuck on solving this PDE of 2nd Order,,"I'm trying to solve this second order PDE: $$-u_{xx}+2u_{xy}+3u_{yy}-(1/3)u_x+u_y=0$$ I've got up to $48V_{st}+4V_t=0$ and this implies $48V_s+4V=f(s)$. I'm stuck on how to find the $g(t)$ solution. Could I have some help, please?","I'm trying to solve this second order PDE: $$-u_{xx}+2u_{xy}+3u_{yy}-(1/3)u_x+u_y=0$$ I've got up to $48V_{st}+4V_t=0$ and this implies $48V_s+4V=f(s)$. I'm stuck on how to find the $g(t)$ solution. Could I have some help, please?",,"['ordinary-differential-equations', 'partial-differential-equations']"
4,The meaning of dependent and independent variables in ODEs,The meaning of dependent and independent variables in ODEs,,"This thought has occured to me a few days ago, and now I am puzzled about some fundamental properties /definitions in the theory of differential equations. Suppose I have an ode $$\frac{dy}{dx}=y^{2}$$ In the above formulation, we are seeking $y$ as a function of $x$ so we have $x$ as the independent variable, $y$ is the dependent variable, and the ode is nonlinear autonomous ($x$ plays the role of time). However if we take the reciprocal, we get $\frac{dx}{dy}=\frac{1}{y^{2}}$, and now $y$ is independent variable, $x$ is dependent, and the ode is linear and nonautonomous. So given that we can interchange dependent and independent variables, what do these termns actually mean? Are they context-specific definitions, or is there some deeper intrinsic meaning? The same for linear/nonlinear and autonomous/nonautonomous?","This thought has occured to me a few days ago, and now I am puzzled about some fundamental properties /definitions in the theory of differential equations. Suppose I have an ode $$\frac{dy}{dx}=y^{2}$$ In the above formulation, we are seeking $y$ as a function of $x$ so we have $x$ as the independent variable, $y$ is the dependent variable, and the ode is nonlinear autonomous ($x$ plays the role of time). However if we take the reciprocal, we get $\frac{dx}{dy}=\frac{1}{y^{2}}$, and now $y$ is independent variable, $x$ is dependent, and the ode is linear and nonautonomous. So given that we can interchange dependent and independent variables, what do these termns actually mean? Are they context-specific definitions, or is there some deeper intrinsic meaning? The same for linear/nonlinear and autonomous/nonautonomous?",,"['calculus', 'ordinary-differential-equations', 'dynamical-systems']"
5,Construct a Lyapunov Function for the system,Construct a Lyapunov Function for the system,,"Construct a Lyapunov Function in order to show that the system \begin{align}\frac{dx}{dt}&=x(y^2 +1) +y \\ \frac{dy}{dt} &= x^2y +x\end{align} has no closed orbits (limit cycle) and hence no periodic solutions. I have absolutely no experience in constructing Lyapunov functions. Can anyone please help guide me with this? I know the conditions that the Lyapunov function $L(x,y)$ must satisfy, but I do not know how to ""create"" the function $L$.","Construct a Lyapunov Function in order to show that the system \begin{align}\frac{dx}{dt}&=x(y^2 +1) +y \\ \frac{dy}{dt} &= x^2y +x\end{align} has no closed orbits (limit cycle) and hence no periodic solutions. I have absolutely no experience in constructing Lyapunov functions. Can anyone please help guide me with this? I know the conditions that the Lyapunov function $L(x,y)$ must satisfy, but I do not know how to ""create"" the function $L$.",,"['ordinary-differential-equations', 'dynamical-systems', 'stability-in-odes']"
6,Intuitive understanding of behaivor of 2nd order ODE,Intuitive understanding of behaivor of 2nd order ODE,,"I am dealing with the equation: $$my''+y'+y=0$$ and being asked, what do the different graphs look like for varying values of m between .04 and 1. How can I intuitively figure out what the different graphs are going to look like? Is this some kind of famous function??","I am dealing with the equation: $$my''+y'+y=0$$ and being asked, what do the different graphs look like for varying values of m between .04 and 1. How can I intuitively figure out what the different graphs are going to look like? Is this some kind of famous function??",,['ordinary-differential-equations']
7,Center Manifold Exercise (small solution for small changes of the parameter),Center Manifold Exercise (small solution for small changes of the parameter),,"Hi I'm stuck with this problem at first I didn't know how to begin so I copy an argument from [Carr, Application of Centre Manifold Theory]. But I don't know how can I find the coefficient from a, b and c? and also I don't understand the theory very well, it's very complicated to me. I'd appreciate if someone can help me with a detailed answer. Thank you For the following system give an explicit formula for the centre manifold, what happening in the origin? \begin{align*} \dot{x}&=-x+y^2-2x^2\\ \dot{y}&= \epsilon y-xy\end{align*} The linearized problem has eigenvalues $-1, \epsilon$. This means that the results in the centre manifold theory does not apply directly. Now if we write the above equation as  \begin{align*} \dot{x}&=-x+y^2-2x^2\\ \dot{y}&= \epsilon y-xy\\ \dot{\epsilon} &=0\end{align*} We considered as an equation in $\mathbb{R}^3$ and the term $\epsilon x$ in the equation is nonlinear. Thus the linearized problem associated has eigenvalues $-1, 0,0$. Now the theory applies and by the centre manifold theorem for some $\delta_i>0$ for $i=1,2$ there is a map $h(y,\epsilon)=x$ for $|x|<\delta_1$ and $|\epsilon|<\delta_2$. So we have $\dot{x}=h_y(y,\epsilon) \dot{y}$, where $h_y$ is the partial derivative with respect to y. From this, it follows that $h$ satisfies the functional equation $$h_y(y,\epsilon)[\epsilon y-h(y,\epsilon)y]+h(y,\epsilon)-y^2+2h^2(y,\epsilon)=0$$ We have an expansion $h(y,\epsilon)=ay^2+by\epsilon+c\epsilon^2+\ldots+$ from here we have \begin{align*} 0&=h_{y}(y, \epsilon)\dot{y}-\dot{x}\\ &=h_{y}(y, \epsilon)(\epsilon y -xy)+h(y, \epsilon)-y^{2}+2h^{2}\\ &= h_{y}\epsilon y-h_{y}xy+h-y^{2}+2h^{2}\\ &= (2ay+b\epsilon)\epsilon y-(2ay+b\epsilon)hy+h-y^{2}+2h^{2}\\ &= 2a\epsilon y^{2}+b\epsilon ^{2}y-(2ay+b\epsilon)(ay^{2}+by\epsilon +c\epsilon ^{2})y+ay^{2}+by\epsilon +c\epsilon ^{2}-y^{2}+2h^{2}\\ &= 2a\epsilon y^{2}+b\epsilon ^{2}y-(2ay^{2}+b\epsilon y)(ay^{2}+by\epsilon +c\epsilon ^{2})+ay^{2}+by\epsilon +c\epsilon ^{2}-y^{2}+2(ay^{2}+by\epsilon +c\epsilon ^{2})(ay^{2}+by\epsilon +c\epsilon ^{2})\\ &= 2a\epsilon y^{2}+b\epsilon ^{2}y-(2a^{2}y^{4}+2ab\epsilon y^{3}+2ac\epsilon ^{2}y^{2}+ab\epsilon y^{3}+b^{2}\epsilon ^{2}y^{2}+bc\epsilon ^{3}y)+ay^{2}+by\epsilon +c\epsilon ^{2}-y^{2}+2(a^{2}y^{4}+aby^{3}\epsilon +ac\epsilon ^{2}y^{2}+aby^{3}\epsilon +b^{2}y^{2}\epsilon ^{2}+bc\epsilon ^{3}y+ac\epsilon ^{2}y^{2}+bcy\epsilon ^{3}+c^{2}\epsilon ^{4}) \end{align*} But from here we can find $b=c=0$ and $a=1$. Thus $h(y,\epsilon)=y^2 + \ldots$ and plugging this in the $y$ equation we have $\dot{y} =\epsilon y- y^3 + \ldots $ and when $\epsilon= 0$ the system is stable. Am I right? Thanks","Hi I'm stuck with this problem at first I didn't know how to begin so I copy an argument from [Carr, Application of Centre Manifold Theory]. But I don't know how can I find the coefficient from a, b and c? and also I don't understand the theory very well, it's very complicated to me. I'd appreciate if someone can help me with a detailed answer. Thank you For the following system give an explicit formula for the centre manifold, what happening in the origin? \begin{align*} \dot{x}&=-x+y^2-2x^2\\ \dot{y}&= \epsilon y-xy\end{align*} The linearized problem has eigenvalues $-1, \epsilon$. This means that the results in the centre manifold theory does not apply directly. Now if we write the above equation as  \begin{align*} \dot{x}&=-x+y^2-2x^2\\ \dot{y}&= \epsilon y-xy\\ \dot{\epsilon} &=0\end{align*} We considered as an equation in $\mathbb{R}^3$ and the term $\epsilon x$ in the equation is nonlinear. Thus the linearized problem associated has eigenvalues $-1, 0,0$. Now the theory applies and by the centre manifold theorem for some $\delta_i>0$ for $i=1,2$ there is a map $h(y,\epsilon)=x$ for $|x|<\delta_1$ and $|\epsilon|<\delta_2$. So we have $\dot{x}=h_y(y,\epsilon) \dot{y}$, where $h_y$ is the partial derivative with respect to y. From this, it follows that $h$ satisfies the functional equation $$h_y(y,\epsilon)[\epsilon y-h(y,\epsilon)y]+h(y,\epsilon)-y^2+2h^2(y,\epsilon)=0$$ We have an expansion $h(y,\epsilon)=ay^2+by\epsilon+c\epsilon^2+\ldots+$ from here we have \begin{align*} 0&=h_{y}(y, \epsilon)\dot{y}-\dot{x}\\ &=h_{y}(y, \epsilon)(\epsilon y -xy)+h(y, \epsilon)-y^{2}+2h^{2}\\ &= h_{y}\epsilon y-h_{y}xy+h-y^{2}+2h^{2}\\ &= (2ay+b\epsilon)\epsilon y-(2ay+b\epsilon)hy+h-y^{2}+2h^{2}\\ &= 2a\epsilon y^{2}+b\epsilon ^{2}y-(2ay+b\epsilon)(ay^{2}+by\epsilon +c\epsilon ^{2})y+ay^{2}+by\epsilon +c\epsilon ^{2}-y^{2}+2h^{2}\\ &= 2a\epsilon y^{2}+b\epsilon ^{2}y-(2ay^{2}+b\epsilon y)(ay^{2}+by\epsilon +c\epsilon ^{2})+ay^{2}+by\epsilon +c\epsilon ^{2}-y^{2}+2(ay^{2}+by\epsilon +c\epsilon ^{2})(ay^{2}+by\epsilon +c\epsilon ^{2})\\ &= 2a\epsilon y^{2}+b\epsilon ^{2}y-(2a^{2}y^{4}+2ab\epsilon y^{3}+2ac\epsilon ^{2}y^{2}+ab\epsilon y^{3}+b^{2}\epsilon ^{2}y^{2}+bc\epsilon ^{3}y)+ay^{2}+by\epsilon +c\epsilon ^{2}-y^{2}+2(a^{2}y^{4}+aby^{3}\epsilon +ac\epsilon ^{2}y^{2}+aby^{3}\epsilon +b^{2}y^{2}\epsilon ^{2}+bc\epsilon ^{3}y+ac\epsilon ^{2}y^{2}+bcy\epsilon ^{3}+c^{2}\epsilon ^{4}) \end{align*} But from here we can find $b=c=0$ and $a=1$. Thus $h(y,\epsilon)=y^2 + \ldots$ and plugging this in the $y$ equation we have $\dot{y} =\epsilon y- y^3 + \ldots $ and when $\epsilon= 0$ the system is stable. Am I right? Thanks",,"['ordinary-differential-equations', 'dynamical-systems', 'nonlinear-system', 'stability-in-odes']"
8,Show that given piecewise function does not have a convergent series from Picard-iteration,Show that given piecewise function does not have a convergent series from Picard-iteration,,"This is a homework question. Given an initial value problem, $\frac{dx}{dt} = f(t,x(t)), \text{ } x(0) = 0$ with the function defined as: $$ f(t,x(t)) =   \begin{cases}        \hfill 0    \hfill & t \leq 0 \\       \hfill 2t   \hfill & t>0, \text{ } x \leq 0 \\       \hfill 2t-4\frac{x}{t} \hfill & t>0, \text{ } 0<x<t^2\\       \hfill -2t  \hfill & t>0, \text{ } x \geq t^2,   \end{cases} $$ I have to show that the $x_n$ sequence(acquired by Picard-iteration) is not convergent, nor does it have a convergent subsequence which converges to a solution. I'd like to be provided the least help possible. What I've so far tried is: Showing f is not locally Lipschitz-continuous. This is part of the problem (""Notice that f is continuous, but not locally Lipschitz-continuous!""). As I remember, this is done by evaluating the partial derivatives. The derivative with respect to t is 0 in the first, and 2 in the second interval (with x being x(t), I'm not sure this is the correct method, though), so I did no further evaluation. Calculating the $x_n$ members. This goes as follows ($x_0 := x(0) = 0) $: $$ x_{n+1} = x(0) + \int_\limits{0}^t f(s, x_n (s))ds. $$ What I got is  $$ x_n(t) =   \begin{cases}        \hfill 0    \hfill & t \leq 0 \\       \hfill \frac{2t^{n+1}}{(n+1)!}   \hfill & t>0, \text{ } x<t^2 \\       \hfill -\frac{2t^{n+1}}{(n+1)!}   \hfill & t>0, \text{ } x \geq t^2.   \end{cases} $$ At this point, I ask the question: are these two steps correct (in showing f is not Lipschitz-continuous locally, and acquiring $x_n(t)$)? My next step would be first to substitute $x_n$ into the initial value problem, getting: $$   \begin{cases}        \hfill 0 = 0    \hfill & t \leq 0 \\       \hfill \frac{2t^{n}}{n!}=2t   \hfill & t>0, \text{ } x_n \leq 0 \\       \hfill \frac{2t^{n}}{n!}=2t-4\frac{\frac{2t^{n+1}}{(n+1)!}}{t} \hfill & t>0, \text{ } 0<x_n<t^2\\       \hfill -\frac{2t^{n+1}}{(n+1)!}=-2t  \hfill & t>0, \text{ } x_n \geq t^2,   \end{cases} $$ $$   \begin{cases}        \hfill 0 = 0    \hfill & t \leq 0 \\       \hfill \frac{t^{n-1}}{n!}=1   \hfill & t>0, \text{ } x_n \leq 0 \\       \hfill \frac{(n+5)t^{n-1}}{(n+1)!}=1 \hfill & t>0, \text{ } 0<x_n<t^2\\       \hfill \frac{t^{n}}{(n+1)!}=1  \hfill & t>0, \text{ } x_n \geq t^2.   \end{cases} $$ This leads to quite strange inequalities. Substituting into $x_n$ I could get contradictions on all except the first line, I believe, and thus I could show that for any n, it cannot satisfy the initial value problem. But how do I show that the limit does not satisfy it either (with t remaining as a variable)? Edit: This seems to be an exercise in the book Solving Ordinary Differential Equations I: Nonstiff Problems (I.8 / 5.).","This is a homework question. Given an initial value problem, $\frac{dx}{dt} = f(t,x(t)), \text{ } x(0) = 0$ with the function defined as: $$ f(t,x(t)) =   \begin{cases}        \hfill 0    \hfill & t \leq 0 \\       \hfill 2t   \hfill & t>0, \text{ } x \leq 0 \\       \hfill 2t-4\frac{x}{t} \hfill & t>0, \text{ } 0<x<t^2\\       \hfill -2t  \hfill & t>0, \text{ } x \geq t^2,   \end{cases} $$ I have to show that the $x_n$ sequence(acquired by Picard-iteration) is not convergent, nor does it have a convergent subsequence which converges to a solution. I'd like to be provided the least help possible. What I've so far tried is: Showing f is not locally Lipschitz-continuous. This is part of the problem (""Notice that f is continuous, but not locally Lipschitz-continuous!""). As I remember, this is done by evaluating the partial derivatives. The derivative with respect to t is 0 in the first, and 2 in the second interval (with x being x(t), I'm not sure this is the correct method, though), so I did no further evaluation. Calculating the $x_n$ members. This goes as follows ($x_0 := x(0) = 0) $: $$ x_{n+1} = x(0) + \int_\limits{0}^t f(s, x_n (s))ds. $$ What I got is  $$ x_n(t) =   \begin{cases}        \hfill 0    \hfill & t \leq 0 \\       \hfill \frac{2t^{n+1}}{(n+1)!}   \hfill & t>0, \text{ } x<t^2 \\       \hfill -\frac{2t^{n+1}}{(n+1)!}   \hfill & t>0, \text{ } x \geq t^2.   \end{cases} $$ At this point, I ask the question: are these two steps correct (in showing f is not Lipschitz-continuous locally, and acquiring $x_n(t)$)? My next step would be first to substitute $x_n$ into the initial value problem, getting: $$   \begin{cases}        \hfill 0 = 0    \hfill & t \leq 0 \\       \hfill \frac{2t^{n}}{n!}=2t   \hfill & t>0, \text{ } x_n \leq 0 \\       \hfill \frac{2t^{n}}{n!}=2t-4\frac{\frac{2t^{n+1}}{(n+1)!}}{t} \hfill & t>0, \text{ } 0<x_n<t^2\\       \hfill -\frac{2t^{n+1}}{(n+1)!}=-2t  \hfill & t>0, \text{ } x_n \geq t^2,   \end{cases} $$ $$   \begin{cases}        \hfill 0 = 0    \hfill & t \leq 0 \\       \hfill \frac{t^{n-1}}{n!}=1   \hfill & t>0, \text{ } x_n \leq 0 \\       \hfill \frac{(n+5)t^{n-1}}{(n+1)!}=1 \hfill & t>0, \text{ } 0<x_n<t^2\\       \hfill \frac{t^{n}}{(n+1)!}=1  \hfill & t>0, \text{ } x_n \geq t^2.   \end{cases} $$ This leads to quite strange inequalities. Substituting into $x_n$ I could get contradictions on all except the first line, I believe, and thus I could show that for any n, it cannot satisfy the initial value problem. But how do I show that the limit does not satisfy it either (with t remaining as a variable)? Edit: This seems to be an exercise in the book Solving Ordinary Differential Equations I: Nonstiff Problems (I.8 / 5.).",,"['sequences-and-series', 'ordinary-differential-equations']"
9,Solving IVP with differential equation $y'' - 2y' + 2y = x + 1$,Solving IVP with differential equation,y'' - 2y' + 2y = x + 1,"I need to solve the following IVP. I've just recently started to learn about differential equations, so I would appreciate if you could check my work, and point out any mistakes. Also, should problems of this type always be solved by the next method? Problem: Solve the following initial value problem: \begin{align*} y'' - 2 y' + 2y = x + 1, \qquad y(0) = a, \quad y'(0) = 0. \end{align*} Attempt: The associated homogeneous differential equation $$ y'' - 2 y' + 2y = 0 $$ has characteristic equation $$ r^2 - 2r + 2 = 0. $$ The complex roots of this equation are $$ r_{1,2} = 1 \pm i. $$ Hence the homogeneous differential equation has the following general solution: \begin{align*} y_h (x) = e^x (c_1 \cos x + c_2 \sin x), \end{align*} with $c_1, c_2$ constants. The RHS $f(x) = x + 1$ of the original ODE can be solved by the method of undetermined coefficients, and we guess a solution has the form $$ y_p(x) = Ax + B $$ with $y_p'(x) = A$ and $y_p''(x) = 0$. Substituting this in the original differential equation gives $$ 0 - 2A + 2(Ax + B) = x + 1. $$ Equating coefficients of like powers gives \begin{align*} \begin{cases} 2A &= 1  \\ -2A + 2B &= 1 \end{cases} \end{align*} so that $A = 1/2$ and $B = 1$. So a particular solution of the ODE is $$ y_p(x) = \frac{1}{2} x + 1. $$ But this does not have the required initial conditions. So we write the general solution of the ODE as \begin{align*} y(x) &= y_h(x) + y_p(x) \\ &= e^x (c_1 \cos x + c_2 \sin x) + \frac{1}{2} x + 1 \end{align*} so that \begin{align*} y'(x) = c_1 e^x (\cos x - \sin x) + c_2 e^x (\sin x + \cos x) + \frac{1}{2}. \end{align*} Then, on applying the initial conditions: \begin{align*} y(0) = a = c_1 + 1 \qquad \text{and} \qquad y'(0) = 0 = c_1 + c_2 + \frac{1}{2} \end{align*} Together this gives $c_1 = a - 1$ and $c_2 = \frac{1}{2} - a$. Hence the solution of the given IVP is $$ y(x) = (a-1) e^x \cos x + (\frac{1}{2} - a) e^x \sin x + \frac{1}{2}x + 1. $$","I need to solve the following IVP. I've just recently started to learn about differential equations, so I would appreciate if you could check my work, and point out any mistakes. Also, should problems of this type always be solved by the next method? Problem: Solve the following initial value problem: \begin{align*} y'' - 2 y' + 2y = x + 1, \qquad y(0) = a, \quad y'(0) = 0. \end{align*} Attempt: The associated homogeneous differential equation $$ y'' - 2 y' + 2y = 0 $$ has characteristic equation $$ r^2 - 2r + 2 = 0. $$ The complex roots of this equation are $$ r_{1,2} = 1 \pm i. $$ Hence the homogeneous differential equation has the following general solution: \begin{align*} y_h (x) = e^x (c_1 \cos x + c_2 \sin x), \end{align*} with $c_1, c_2$ constants. The RHS $f(x) = x + 1$ of the original ODE can be solved by the method of undetermined coefficients, and we guess a solution has the form $$ y_p(x) = Ax + B $$ with $y_p'(x) = A$ and $y_p''(x) = 0$. Substituting this in the original differential equation gives $$ 0 - 2A + 2(Ax + B) = x + 1. $$ Equating coefficients of like powers gives \begin{align*} \begin{cases} 2A &= 1  \\ -2A + 2B &= 1 \end{cases} \end{align*} so that $A = 1/2$ and $B = 1$. So a particular solution of the ODE is $$ y_p(x) = \frac{1}{2} x + 1. $$ But this does not have the required initial conditions. So we write the general solution of the ODE as \begin{align*} y(x) &= y_h(x) + y_p(x) \\ &= e^x (c_1 \cos x + c_2 \sin x) + \frac{1}{2} x + 1 \end{align*} so that \begin{align*} y'(x) = c_1 e^x (\cos x - \sin x) + c_2 e^x (\sin x + \cos x) + \frac{1}{2}. \end{align*} Then, on applying the initial conditions: \begin{align*} y(0) = a = c_1 + 1 \qquad \text{and} \qquad y'(0) = 0 = c_1 + c_2 + \frac{1}{2} \end{align*} Together this gives $c_1 = a - 1$ and $c_2 = \frac{1}{2} - a$. Hence the solution of the given IVP is $$ y(x) = (a-1) e^x \cos x + (\frac{1}{2} - a) e^x \sin x + \frac{1}{2}x + 1. $$",,['ordinary-differential-equations']
10,What is the exact meaning of self consistent?,What is the exact meaning of self consistent?,,"I've heard the term self consistent being used when referring to differential equations, etc. but I have to admit that I'm unsure as to what is exactly meant by this. Is it simply that there is at least one solution that solves the equation? For example, suppose I have a 2nd order differential equation of the form $$a(x)y''(x)+b(x)y'(x)+c(x)y(x)=f(x)$$ Would one say that this differential equation is self consistent if there is at least one solution $\tilde{y}(x)$ such that $$a(x)\tilde{y}''(x)+b(x)\tilde{y}'(x)+c(x)\tilde{y}(x)=f(x)$$ or does the term self-consistent mean something completely different?","I've heard the term self consistent being used when referring to differential equations, etc. but I have to admit that I'm unsure as to what is exactly meant by this. Is it simply that there is at least one solution that solves the equation? For example, suppose I have a 2nd order differential equation of the form $$a(x)y''(x)+b(x)y'(x)+c(x)y(x)=f(x)$$ Would one say that this differential equation is self consistent if there is at least one solution $\tilde{y}(x)$ such that $$a(x)\tilde{y}''(x)+b(x)\tilde{y}'(x)+c(x)\tilde{y}(x)=f(x)$$ or does the term self-consistent mean something completely different?",,"['ordinary-differential-equations', 'terminology']"
11,Which PDE book covers these topics best?,Which PDE book covers these topics best?,,"I have an exam in January and I want to prepare ODE and PDE section first as they carry good weightage. For ODE I have S.L. Ross' book, which I like and have always referred to. But I haven't done PDE yet and I have to prepare it now. My syllabus consists of these topics- Linear and quasilinear first order partial differential equations method of characteristics; second order linear equations in two variables and their classification Cauchy, Dirichlet and Neumann problems; solutions of Laplace wave in two dimensional Cartesian coordinates Interior and exterior Dirichlet problems in polar coordinates Separation of variables method for solving wave and diffusion equations in one space variable Fourier series and Fourier transform and Laplace transform methods of solutions for the above equations. My main and only focus is these topics with as many problems I can try on them. Can someone recommend a good book/s which covers all these topics. Thanks!","I have an exam in January and I want to prepare ODE and PDE section first as they carry good weightage. For ODE I have S.L. Ross' book, which I like and have always referred to. But I haven't done PDE yet and I have to prepare it now. My syllabus consists of these topics- Linear and quasilinear first order partial differential equations method of characteristics; second order linear equations in two variables and their classification Cauchy, Dirichlet and Neumann problems; solutions of Laplace wave in two dimensional Cartesian coordinates Interior and exterior Dirichlet problems in polar coordinates Separation of variables method for solving wave and diffusion equations in one space variable Fourier series and Fourier transform and Laplace transform methods of solutions for the above equations. My main and only focus is these topics with as many problems I can try on them. Can someone recommend a good book/s which covers all these topics. Thanks!",,"['ordinary-differential-equations', 'reference-request', 'partial-differential-equations']"
12,Time of life of a solution of a Cauchy problem,Time of life of a solution of a Cauchy problem,,"Can you help me in showing that there exist $r >0$ such that for every $\|(x_0,y_0)) \| \leqslant r$ the solution of the Cauchy Problem : \begin{align*} x'+x&=y^2\\y'+y&=x^2 \end{align*} with the boundary $(x(0),y(0))=(x_0,y_0)$ is definined on $\mathbb{R}$ (i.e the solution lives infinitely). Thank you. PS. I tried to find a ""bounded"" energy and I didn't succeed, but I want to show that $(x',y')$ is locally bounded.","Can you help me in showing that there exist $r >0$ such that for every $\|(x_0,y_0)) \| \leqslant r$ the solution of the Cauchy Problem : \begin{align*} x'+x&=y^2\\y'+y&=x^2 \end{align*} with the boundary $(x(0),y(0))=(x_0,y_0)$ is definined on $\mathbb{R}$ (i.e the solution lives infinitely). Thank you. PS. I tried to find a ""bounded"" energy and I didn't succeed, but I want to show that $(x',y')$ is locally bounded.",,"['real-analysis', 'ordinary-differential-equations']"
13,Should translation change eigenvalues? [closed],Should translation change eigenvalues? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 7 years ago . Improve this question everyone! I'm working on flow instability and just a newbie. Now, I try to employ spectral method - collocation method to solve Orr-Sommerfeld(OS) equation. I've already solved the eigenvalue problem of Poiseuille flow. And I want to move to Boundary layer instability., during which I come across a problem concerning coordinates transformation. The problem is mainly a mathematical one. Specificlly, OS equation is as follows. $$ (U-c)(D^{2}-\alpha^{2})\phi - U^{\prime \prime}\phi = \frac{1}{i \alpha Re} (D^{4}-2\alpha^{2}D^{2}+\alpha^{4})\phi $$ Mean velocity profile of Poiseuille flow is $U = 1 - x^{2}, -1 <= x <= 1$. What I concerns is c, wave speed, also eigenvalue. I treat OS equation as a general eigenvalue problem, which leads OS equation to be of the form $A \phi = c B \phi$, where both A and B are operators. $$ A = \frac{-1}{\alpha Re} (D^4 - 2 \alpha^2 D^2 + \alpha^4) + i U (D^2 - \alpha^2) - i U^{\prime \prime} $$ $$ B = i (D^2 - \alpha^2) $$ By some eigenvalue solver, eigenvalues can be solved. Mathematically, eigenvalues are determined by operators A and B. Ok, I finish the above. Then I consider coordinate transformation. Take a simple example as translation: $y = x + 1$. It translates field from $-1 \leq x \leq 1$ to $0 \leq y \leq 2$. Since the field is translated, operators A and B should be changed to be defined on the new field. Specifically, partial differential operator $D, D^2, D^4$ and velocity profile $U, U^{\prime \prime}$ should be changed. In this case, coordinate transformation is just translation. So, partial differential operator is unchanged and velocity profile becomes $U = -y^2 + 2y, U^{\prime \prime} = -2$. All in all, what changes in this eigenvalue problem $A \phi = c B \phi$ is only velocity profile. It seems operator indeed changes, which results in different eigenvalues from original ones. Then there exists contradiction. I mean, mathematically, different operators in eigenvalue problem will lead to different eigenvalues. Meanwhile, physically, coordinate translation leads to the same eigenvalues. In fact, I do get same eigenvalues numerically by spectral method - collocation method. So, where I am wrong from the above?","Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 7 years ago . Improve this question everyone! I'm working on flow instability and just a newbie. Now, I try to employ spectral method - collocation method to solve Orr-Sommerfeld(OS) equation. I've already solved the eigenvalue problem of Poiseuille flow. And I want to move to Boundary layer instability., during which I come across a problem concerning coordinates transformation. The problem is mainly a mathematical one. Specificlly, OS equation is as follows. $$ (U-c)(D^{2}-\alpha^{2})\phi - U^{\prime \prime}\phi = \frac{1}{i \alpha Re} (D^{4}-2\alpha^{2}D^{2}+\alpha^{4})\phi $$ Mean velocity profile of Poiseuille flow is $U = 1 - x^{2}, -1 <= x <= 1$. What I concerns is c, wave speed, also eigenvalue. I treat OS equation as a general eigenvalue problem, which leads OS equation to be of the form $A \phi = c B \phi$, where both A and B are operators. $$ A = \frac{-1}{\alpha Re} (D^4 - 2 \alpha^2 D^2 + \alpha^4) + i U (D^2 - \alpha^2) - i U^{\prime \prime} $$ $$ B = i (D^2 - \alpha^2) $$ By some eigenvalue solver, eigenvalues can be solved. Mathematically, eigenvalues are determined by operators A and B. Ok, I finish the above. Then I consider coordinate transformation. Take a simple example as translation: $y = x + 1$. It translates field from $-1 \leq x \leq 1$ to $0 \leq y \leq 2$. Since the field is translated, operators A and B should be changed to be defined on the new field. Specifically, partial differential operator $D, D^2, D^4$ and velocity profile $U, U^{\prime \prime}$ should be changed. In this case, coordinate transformation is just translation. So, partial differential operator is unchanged and velocity profile becomes $U = -y^2 + 2y, U^{\prime \prime} = -2$. All in all, what changes in this eigenvalue problem $A \phi = c B \phi$ is only velocity profile. It seems operator indeed changes, which results in different eigenvalues from original ones. Then there exists contradiction. I mean, mathematically, different operators in eigenvalue problem will lead to different eigenvalues. Meanwhile, physically, coordinate translation leads to the same eigenvalues. In fact, I do get same eigenvalues numerically by spectral method - collocation method. So, where I am wrong from the above?",,['ordinary-differential-equations']
14,Convergence of difference equation to differential equation,Convergence of difference equation to differential equation,,"Starting with the difference equation: $$f(x(t+dt),t+dt)= (1-a)f(x(t),t) + a f(x(t+dt),t)$$ where $x(0)$ is given and positive, $a\in(0,1)$, $f(0,t)=0$, and $f$ is increasing in both arguments.  This clearly defines a positive, decreasing sequence. I would like to know conditions such that as I decrease the step size (i.e.   $dt\to 0$), that this sequence converges to the solution of the corresponding differential equation: $$x'(t) = -\frac{f_t(x,t)}{(1-a)f_x(x,t)}$$ The domain can be taken to be $[0,x_0]×[0,1]$.  $f$ is nonnegative, bounded, and strictly increasing in both arguments, differentiable, etc. Ideally would also like to handle the case where $x_0 = \infty$. Thanks for any suggestions on how to approach this. Any references or pointers to resources (for a relative beginner!) would be appreciated.","Starting with the difference equation: $$f(x(t+dt),t+dt)= (1-a)f(x(t),t) + a f(x(t+dt),t)$$ where $x(0)$ is given and positive, $a\in(0,1)$, $f(0,t)=0$, and $f$ is increasing in both arguments.  This clearly defines a positive, decreasing sequence. I would like to know conditions such that as I decrease the step size (i.e.   $dt\to 0$), that this sequence converges to the solution of the corresponding differential equation: $$x'(t) = -\frac{f_t(x,t)}{(1-a)f_x(x,t)}$$ The domain can be taken to be $[0,x_0]×[0,1]$.  $f$ is nonnegative, bounded, and strictly increasing in both arguments, differentiable, etc. Ideally would also like to handle the case where $x_0 = \infty$. Thanks for any suggestions on how to approach this. Any references or pointers to resources (for a relative beginner!) would be appreciated.",,"['ordinary-differential-equations', 'convergence-divergence', 'numerical-methods', 'recurrence-relations']"
15,"Problem in understanding the solution of exact differential $M(x,y)dx + N(x,y)dy = du(x,y)$.",Problem in understanding the solution of exact differential .,"M(x,y)dx + N(x,y)dy = du(x,y)","If $M(x,y)dx + N(x,y)dy = du(x,y)$, then it is an exact differential equation. For that to happen, $$\left(\dfrac{\partial M(x,y)}{\partial y}\right)_x =\left(\dfrac{\partial N(x,y)}{\partial x}\right)_y \tag1.$$ I was following Differential Equations by  Balachandra Rao, S. Staff to see how to find $u(x,y)$. For most of the part but one, I could conceive. The proof goes using  the first part of $(1)$ after integrating it to get $$u(x,y) = \int M(x,y) dx + \phi (y).$$ In order to satisfy the second condition of $(1)$, there exists a certain $\phi (y)$. So, $$\frac{\partial u(x,y)}{\partial y} = \frac{\partial}{\partial y}\left[\int M(x,y) dx\right]+ \phi'(y)$$ which must be equal to $$N(x,y) = \frac{\partial}{\partial y}\left[\int M(x,y) dx\right]+ \phi'(y) \implies \phi'(y) = N(x,y) - \frac{\partial}{\partial y}\left[\int M(x,y) dx\right] .$$ LHS is independent of $x$; so RHS must also be independent of $x$; in order to verify this, we take the partial derivative of RHS w.r.t. $x$ : $$\frac{\partial}{\partial x} \left[N(x,y) - \frac{\partial}{\partial y}\left[\int M(x,y) dx\right]\right] = \dfrac{\partial N(x,y)}{\partial x} -\dfrac{\partial M(x,y)}{\partial y}= 0.$$ So, RHS is independent of $x$. Then by integrating, we get $$\phi(y) = \int\left[N(x,y) -\frac{\partial}{\partial y}\left[\int M(x,y) dx \right] \right]dy + C .$$ Putting the value of $\phi(y)$, we get $$u(x,y) = \int M(x,y) dx + \int\left[N(x,y) -\frac{\partial}{\partial y}\left[\int M(x,y) dx \right] \right]dy.$$ This is the value of $u(x,y)$ . But the book presents the working rule as A "" working rule "" convenient to apply for solving an exact differential equation is as follows: $$\underset{[y\; \text{const.}]}{\int M dx} + [\color{red}{\text{Terms of} \; N\; \text{not containing} \; x}] dy = c$$ See the red marked statement; $M(x,y)$ is not there, but in the derivation the parenthesis contained $\dfrac{\partial}{\partial y}\left[\int M(x,y) dx \right] $. As if it was excluded in the working rule without specifying any reason. If I break $$\int\left[N(x,y) -\frac{\partial}{\partial y}\left[\int M(x,y) dx \right] \right]dy$$ into $$\int N(x,y) dy - \int \frac{\partial}{\partial y}\left[\int M(x,y) dx \right] dy$$, then I get $$\int N(x,y) dy - \frac{\partial}{\partial y}\left[\int M(x,y) dx \right].$$ The second term $$-\frac{\partial}{\partial y}\left[\int M(x,y) dx \right]$$ is absent in the red-marked statement. What is the reason for its exclusion ? Please help.","If $M(x,y)dx + N(x,y)dy = du(x,y)$, then it is an exact differential equation. For that to happen, $$\left(\dfrac{\partial M(x,y)}{\partial y}\right)_x =\left(\dfrac{\partial N(x,y)}{\partial x}\right)_y \tag1.$$ I was following Differential Equations by  Balachandra Rao, S. Staff to see how to find $u(x,y)$. For most of the part but one, I could conceive. The proof goes using  the first part of $(1)$ after integrating it to get $$u(x,y) = \int M(x,y) dx + \phi (y).$$ In order to satisfy the second condition of $(1)$, there exists a certain $\phi (y)$. So, $$\frac{\partial u(x,y)}{\partial y} = \frac{\partial}{\partial y}\left[\int M(x,y) dx\right]+ \phi'(y)$$ which must be equal to $$N(x,y) = \frac{\partial}{\partial y}\left[\int M(x,y) dx\right]+ \phi'(y) \implies \phi'(y) = N(x,y) - \frac{\partial}{\partial y}\left[\int M(x,y) dx\right] .$$ LHS is independent of $x$; so RHS must also be independent of $x$; in order to verify this, we take the partial derivative of RHS w.r.t. $x$ : $$\frac{\partial}{\partial x} \left[N(x,y) - \frac{\partial}{\partial y}\left[\int M(x,y) dx\right]\right] = \dfrac{\partial N(x,y)}{\partial x} -\dfrac{\partial M(x,y)}{\partial y}= 0.$$ So, RHS is independent of $x$. Then by integrating, we get $$\phi(y) = \int\left[N(x,y) -\frac{\partial}{\partial y}\left[\int M(x,y) dx \right] \right]dy + C .$$ Putting the value of $\phi(y)$, we get $$u(x,y) = \int M(x,y) dx + \int\left[N(x,y) -\frac{\partial}{\partial y}\left[\int M(x,y) dx \right] \right]dy.$$ This is the value of $u(x,y)$ . But the book presents the working rule as A "" working rule "" convenient to apply for solving an exact differential equation is as follows: $$\underset{[y\; \text{const.}]}{\int M dx} + [\color{red}{\text{Terms of} \; N\; \text{not containing} \; x}] dy = c$$ See the red marked statement; $M(x,y)$ is not there, but in the derivation the parenthesis contained $\dfrac{\partial}{\partial y}\left[\int M(x,y) dx \right] $. As if it was excluded in the working rule without specifying any reason. If I break $$\int\left[N(x,y) -\frac{\partial}{\partial y}\left[\int M(x,y) dx \right] \right]dy$$ into $$\int N(x,y) dy - \int \frac{\partial}{\partial y}\left[\int M(x,y) dx \right] dy$$, then I get $$\int N(x,y) dy - \frac{\partial}{\partial y}\left[\int M(x,y) dx \right].$$ The second term $$-\frac{\partial}{\partial y}\left[\int M(x,y) dx \right]$$ is absent in the red-marked statement. What is the reason for its exclusion ? Please help.",,['calculus']
16,Squares and constants in the dynamical system,Squares and constants in the dynamical system,,I have  $$ \begin{eqnarray}  x'&=& x^2 - y^2 -1 \\  y'&=& 2y  \end{eqnarray} $$  How can I solve such a system? I have tried the substitution $X= x^2 - 1$ but I still get constants in the new system as well as square roots this time. I need some advice on the proper substitution to use so as to solve the system. Thank you very much.,I have  $$ \begin{eqnarray}  x'&=& x^2 - y^2 -1 \\  y'&=& 2y  \end{eqnarray} $$  How can I solve such a system? I have tried the substitution $X= x^2 - 1$ but I still get constants in the new system as well as square roots this time. I need some advice on the proper substitution to use so as to solve the system. Thank you very much.,,"['ordinary-differential-equations', 'dynamical-systems']"
17,Linear equation and linear differential equations,Linear equation and linear differential equations,,"I remember noting from an algebra class that $x$ and $y$ of a linear equation neither divide or multiply with each other which is somewhat clear from the forms of linear equations: General form of linear equation: $Ax + By + C = 0$ Slope intercept form: $y = mx + b$ Is this also true for linear differential equations ? The definition goes like this: ""A differential equation is said to be linear if the dependent variable and its differential coeficients (derivates) occur only in the first degree and not multiplied together."" ${dy \over dx} = {Py + Q}$ Where P, Q are functions of $x$ only. What exactly does this mean? Does the algebraic linear equation has something to do with linear differential equation?","I remember noting from an algebra class that $x$ and $y$ of a linear equation neither divide or multiply with each other which is somewhat clear from the forms of linear equations: General form of linear equation: $Ax + By + C = 0$ Slope intercept form: $y = mx + b$ Is this also true for linear differential equations ? The definition goes like this: ""A differential equation is said to be linear if the dependent variable and its differential coeficients (derivates) occur only in the first degree and not multiplied together."" ${dy \over dx} = {Py + Q}$ Where P, Q are functions of $x$ only. What exactly does this mean? Does the algebraic linear equation has something to do with linear differential equation?",,"['linear-algebra', 'ordinary-differential-equations']"
18,"Coupled partial differential equation, with boundaries specification","Coupled partial differential equation, with boundaries specification",,"Please, help me to find a books or samples to learn how to solve such coupled equations $$\begin{eqnarray} \frac{\partial T_1(x,t)}{\partial t}&=& \alpha_1 \frac{\partial^2 T_1(x,t)}{	\partial x^2} +f(x)\\ \frac{\partial T_2(x,t)}{\partial t}&=& \alpha_2 \frac{\partial^2 T_2(x,t)}{	\partial x^2} \end{eqnarray} $$ with f(x) an exponential source term, the initial and boundary conditions: $$\begin{eqnarray} T_1(0,0)&=&T_0,(x->0)\\ T_2(l,0)&=&T_0,(x->l)\\ T_1(l,s)&=&T_2(l,s),(x->l)\\ T_1'(l,s)&=&K_\lambda T_1'(l,s),(x->l)\\ T_2(l,0)&=&T_0,(x->l)\\ T_2(\infty,s)&=&\frac{T_0}{s},(x->\infty)\end{eqnarray} $$ Please do not hesitate with any kind of suggestions. After a Laplace transform, to both equations, I stuck to find constant C1, C2, C3. by the mean of this short code (evaluation doent give any result, I do not know why !! $$\begin{eqnarray} T_{1}[x,s]:=C_{1}exp{-\lambda x}+ c_{2}exp{-\lambda x}+\frac{A}{s}+\frac{T_{0}}{s}\\ T_{2}[x,s]:=C_{3}exp{-\beta x}+\frac{T_{0}}{s}\\\end{eqnarray}$$ $$\begin{eqnarray} BC1=k_{1}T'_{1}[x,s]==0/. x->0;\\ BC2=T_{1}[l,s]==T_{2}[l,s];\\ BC3=(k_{1}T'_1[x,s]-k_{2}T'_{2}[x,s]==0/. x->l;\\ BC4=k_{2}T'_{2}[x,s]==0 /. x->L; \end{eqnarray}$$ $$ \begin{eqnarray}eqns&=Flatten[{BC1,BC2,BC3,BC4}];\\ var&={C_{1},C_{2},C_{3}};\\ soln&=Simplify[Solve[eqns,var]]\\\end{eqnarray}$$ Where the origin is at x=0, the interface at x=l and the end bar at x=L. Have you another method ? Merci beaucoup.","Please, help me to find a books or samples to learn how to solve such coupled equations $$\begin{eqnarray} \frac{\partial T_1(x,t)}{\partial t}&=& \alpha_1 \frac{\partial^2 T_1(x,t)}{	\partial x^2} +f(x)\\ \frac{\partial T_2(x,t)}{\partial t}&=& \alpha_2 \frac{\partial^2 T_2(x,t)}{	\partial x^2} \end{eqnarray} $$ with f(x) an exponential source term, the initial and boundary conditions: $$\begin{eqnarray} T_1(0,0)&=&T_0,(x->0)\\ T_2(l,0)&=&T_0,(x->l)\\ T_1(l,s)&=&T_2(l,s),(x->l)\\ T_1'(l,s)&=&K_\lambda T_1'(l,s),(x->l)\\ T_2(l,0)&=&T_0,(x->l)\\ T_2(\infty,s)&=&\frac{T_0}{s},(x->\infty)\end{eqnarray} $$ Please do not hesitate with any kind of suggestions. After a Laplace transform, to both equations, I stuck to find constant C1, C2, C3. by the mean of this short code (evaluation doent give any result, I do not know why !! $$\begin{eqnarray} T_{1}[x,s]:=C_{1}exp{-\lambda x}+ c_{2}exp{-\lambda x}+\frac{A}{s}+\frac{T_{0}}{s}\\ T_{2}[x,s]:=C_{3}exp{-\beta x}+\frac{T_{0}}{s}\\\end{eqnarray}$$ $$\begin{eqnarray} BC1=k_{1}T'_{1}[x,s]==0/. x->0;\\ BC2=T_{1}[l,s]==T_{2}[l,s];\\ BC3=(k_{1}T'_1[x,s]-k_{2}T'_{2}[x,s]==0/. x->l;\\ BC4=k_{2}T'_{2}[x,s]==0 /. x->L; \end{eqnarray}$$ $$ \begin{eqnarray}eqns&=Flatten[{BC1,BC2,BC3,BC4}];\\ var&={C_{1},C_{2},C_{3}};\\ soln&=Simplify[Solve[eqns,var]]\\\end{eqnarray}$$ Where the origin is at x=0, the interface at x=l and the end bar at x=L. Have you another method ? Merci beaucoup.",,"['ordinary-differential-equations', 'partial-differential-equations']"
19,A discussion on fourier and laplace transforms and differential equations ...?,A discussion on fourier and laplace transforms and differential equations ...?,,"i have read many of the answers and explanations about the similarities and differences between laplace and fourier transform. Laplace can be used to analyze unstable systems. Fourier is a subset of laplace. Some signals have fourier but laplace is not defined , for instance cosine or sine from -infinity to +infinity. i have studied signals and systems and basic control theory in my undergrad. My question is related to the solution of differntial equations using these transforms. Wherever i have seen it is written that fourier is used for steady state analysis (example : in solution of circuits) whereas for transient response we resort to laplace. What exactly is the thing that enables laplace to incorporate initial conditions (hence unilateral laplace) and solve ODEs ? Similarly what prevents fourier from taking these into account? Let me explain at least what i understand. when solving for the laplace of the derivative of a function (using integration by parts) we input the initial conditions there and they usually end up appearing as decaying exponentials (at the natural modes/poles of the system) in the final response. The constants are chosen so as to satisfy the initial conditions. On a sidenote, this is also related to linearity and upon initial conditions that are not 0 (rest) the system ends up becoming non-linear because of not satisfying the zeros input zero output property. Now when the fourier is found for the derivative of a function the limits of the integral go from -infinity to + infinity and consequently the initial conditions can't be absorbed. Why cant we define the integral to be from 0 to infinity and incorporate initial conditions just as in laplace? i know that fourier is closely linked to convolution and changing these limits would wrong that but still can this formulation be used for solving an initial value ODE? if yes , kindly provide an example. here is one that i found . i don't understand one step (second step of finding the general solution'u') in it but other than that i can't find a mistake. Solving an initial value ODE problem using fourier transform i know this might be too basic for some or too narrowed down but it has bugged me my whole undergrad and now i really need an answer from  an expert. Thank u and regards khurram","i have read many of the answers and explanations about the similarities and differences between laplace and fourier transform. Laplace can be used to analyze unstable systems. Fourier is a subset of laplace. Some signals have fourier but laplace is not defined , for instance cosine or sine from -infinity to +infinity. i have studied signals and systems and basic control theory in my undergrad. My question is related to the solution of differntial equations using these transforms. Wherever i have seen it is written that fourier is used for steady state analysis (example : in solution of circuits) whereas for transient response we resort to laplace. What exactly is the thing that enables laplace to incorporate initial conditions (hence unilateral laplace) and solve ODEs ? Similarly what prevents fourier from taking these into account? Let me explain at least what i understand. when solving for the laplace of the derivative of a function (using integration by parts) we input the initial conditions there and they usually end up appearing as decaying exponentials (at the natural modes/poles of the system) in the final response. The constants are chosen so as to satisfy the initial conditions. On a sidenote, this is also related to linearity and upon initial conditions that are not 0 (rest) the system ends up becoming non-linear because of not satisfying the zeros input zero output property. Now when the fourier is found for the derivative of a function the limits of the integral go from -infinity to + infinity and consequently the initial conditions can't be absorbed. Why cant we define the integral to be from 0 to infinity and incorporate initial conditions just as in laplace? i know that fourier is closely linked to convolution and changing these limits would wrong that but still can this formulation be used for solving an initial value ODE? if yes , kindly provide an example. here is one that i found . i don't understand one step (second step of finding the general solution'u') in it but other than that i can't find a mistake. Solving an initial value ODE problem using fourier transform i know this might be too basic for some or too narrowed down but it has bugged me my whole undergrad and now i really need an answer from  an expert. Thank u and regards khurram",,"['ordinary-differential-equations', 'fourier-analysis', 'laplace-transform']"
20,Showing a bound exists,Showing a bound exists,,"I was able to derive the following differential equations I have to work with for a function $V$: $$ \begin{align*}   dV(x_1,x_2,x_3,x_4) &= \left(x_1^2-1\right)\left[(1-x_1^2-x_2^2-x_3^2-x_4^2)-c_{21}x_2^2+e_{31}x_3^2+e_{41}x_4^2 \right]\\     &+  \left(x_2^2-1\right)\left[(1-x_1^2-x_2^2-x_3^2-x_4^2)+e_{12}x_1^2-c_{32}x_3^2-c_{42}x_4^2 \right]\\     &+  \left(x_3^2-1\right)\left[(1-x_1^2-x_2^2-x_3^2-x_4^2)-c_{13}x_1^2+e_{23}x_2^2+e_{43}x_4^2 \right]\\     &+  \left(x_4^2-1\right)\left[(1-x_1^2-x_2^2-x_3^2-x_4^2)-c_{14}x_1^2+e_{24}x_2^2-c_{34}x_3^2 \right] dt\\ \end{align*}$$ What I would like to do is find constants $c_0, c_1, c_2, c_3, c_4$ so that: $$dV(x_1,x_2,x_3,x_4) \leq c_0+c_1x_1^2+c_2x_2^2+c_3x_3^2+c_4x_4^2 dt$$ All the parameters $e_{ij}$ and $c_{ij}$ are positive, and each $x_i >0$ as well. Clearly this bound may not exist if the $e_{ij}$ are too large since the equations may explode, so the suitable condition  to prevent this is suppose to be $\Pi e_{ij} < \Pi c_{ij}$. What that should do is somehow create enough push from the negative terms to prevent explosion. However, I have been unable to use that condition and show the bound I'm looking for exists. Please help!","I was able to derive the following differential equations I have to work with for a function $V$: $$ \begin{align*}   dV(x_1,x_2,x_3,x_4) &= \left(x_1^2-1\right)\left[(1-x_1^2-x_2^2-x_3^2-x_4^2)-c_{21}x_2^2+e_{31}x_3^2+e_{41}x_4^2 \right]\\     &+  \left(x_2^2-1\right)\left[(1-x_1^2-x_2^2-x_3^2-x_4^2)+e_{12}x_1^2-c_{32}x_3^2-c_{42}x_4^2 \right]\\     &+  \left(x_3^2-1\right)\left[(1-x_1^2-x_2^2-x_3^2-x_4^2)-c_{13}x_1^2+e_{23}x_2^2+e_{43}x_4^2 \right]\\     &+  \left(x_4^2-1\right)\left[(1-x_1^2-x_2^2-x_3^2-x_4^2)-c_{14}x_1^2+e_{24}x_2^2-c_{34}x_3^2 \right] dt\\ \end{align*}$$ What I would like to do is find constants $c_0, c_1, c_2, c_3, c_4$ so that: $$dV(x_1,x_2,x_3,x_4) \leq c_0+c_1x_1^2+c_2x_2^2+c_3x_3^2+c_4x_4^2 dt$$ All the parameters $e_{ij}$ and $c_{ij}$ are positive, and each $x_i >0$ as well. Clearly this bound may not exist if the $e_{ij}$ are too large since the equations may explode, so the suitable condition  to prevent this is suppose to be $\Pi e_{ij} < \Pi c_{ij}$. What that should do is somehow create enough push from the negative terms to prevent explosion. However, I have been unable to use that condition and show the bound I'm looking for exists. Please help!",,"['ordinary-differential-equations', 'inequality']"
21,Stability of origin of dynamical system,Stability of origin of dynamical system,,"Usually you can note some nice structure in the problem which enables construction of a nice Lyapunov function. But this one is just a monster. Maybe there is a trick I've missed? Investigate the stability of the origin of the following system. $$x' = yx - 5x^3 \\ y' = x^2 - 5y$$ Attempt: First note that the are no other fixed points. Also note that linearization will fail at the origin. We try some Lyapunov function $V(x,y) = G(x) + H(y)$ which gives $$V' = g(x)xy - g(x)5x^3 + h(y)x^2-h(y)5y$$ I can not figure out what kind of function would make $V'$ definite in either direction, due to the differences in order. I should also note that I've already caught a lot of sloppy errors in these exam questions, making the problems either impossible to solve or something other than the posted solution. This particular problem has no solutions posted, however.","Usually you can note some nice structure in the problem which enables construction of a nice Lyapunov function. But this one is just a monster. Maybe there is a trick I've missed? Investigate the stability of the origin of the following system. $$x' = yx - 5x^3 \\ y' = x^2 - 5y$$ Attempt: First note that the are no other fixed points. Also note that linearization will fail at the origin. We try some Lyapunov function $V(x,y) = G(x) + H(y)$ which gives $$V' = g(x)xy - g(x)5x^3 + h(y)x^2-h(y)5y$$ I can not figure out what kind of function would make $V'$ definite in either direction, due to the differences in order. I should also note that I've already caught a lot of sloppy errors in these exam questions, making the problems either impossible to solve or something other than the posted solution. This particular problem has no solutions posted, however.",,"['ordinary-differential-equations', 'dynamical-systems']"
22,Reducing a higher order nonlinear ODE to a system of first order ODEs,Reducing a higher order nonlinear ODE to a system of first order ODEs,,"The ODE that I am trying to reduce is: $$ y''' + 4\,y'' + y' + 6\,y - 2y^{2} = 0 $$ I start by letting $$ y = y_1 $$ $$ y' = y_2 $$ $$ y'' = y_3 $$ $$ y''' = y_4 = 2y_1^2 - 4y_3 - y_2 - 6y_1 $$ However, due to the term $2y_1^2$, I believe that this set of functions cannot be put into matrix form, and hence, I am stuck here. I would very much appreciate any guidance. :)","The ODE that I am trying to reduce is: $$ y''' + 4\,y'' + y' + 6\,y - 2y^{2} = 0 $$ I start by letting $$ y = y_1 $$ $$ y' = y_2 $$ $$ y'' = y_3 $$ $$ y''' = y_4 = 2y_1^2 - 4y_3 - y_2 - 6y_1 $$ However, due to the term $2y_1^2$, I believe that this set of functions cannot be put into matrix form, and hence, I am stuck here. I would very much appreciate any guidance. :)",,['ordinary-differential-equations']
23,two point block method for solving ODE,two point block method for solving ODE,,"How to solve the ordinary differential equation  $$y'(t) = -1000 y(t)+ 999 e^{-t}, \hspace{10mm} 0≤t≤5.$$  $y(t)=e^{-t}$, for $t<0$. Using two point block method  $$hf_{n+1}= \frac{1}{3} (hf_{n+2} - 2y_{n} + 2y_{n+1} )$$  and  $$y_{n+2}= \frac{1}{3} \, (2hf_{n+2}-y_{n}+4y_{n+1} )$$  step-size=0.01","How to solve the ordinary differential equation  $$y'(t) = -1000 y(t)+ 999 e^{-t}, \hspace{10mm} 0≤t≤5.$$  $y(t)=e^{-t}$, for $t<0$. Using two point block method  $$hf_{n+1}= \frac{1}{3} (hf_{n+2} - 2y_{n} + 2y_{n+1} )$$  and  $$y_{n+2}= \frac{1}{3} \, (2hf_{n+2}-y_{n}+4y_{n+1} )$$  step-size=0.01",,"['ordinary-differential-equations', 'numerical-methods']"
24,Physical applications of Chebyshev's equation.,Physical applications of Chebyshev's equation.,,"As reported by Wikipedia , Chebyshev's equation is the second order linear differential equation $$(1-x^2) {d^2 y \over d x^2} - x {d y \over d x} + p^2 y = 0 $$ where $p$ is a real constant. Has equation above physical meaning? That is, is it used to model some physical phenomena?","As reported by Wikipedia , Chebyshev's equation is the second order linear differential equation $$(1-x^2) {d^2 y \over d x^2} - x {d y \over d x} + p^2 y = 0 $$ where $p$ is a real constant. Has equation above physical meaning? That is, is it used to model some physical phenomena?",,"['ordinary-differential-equations', 'applications']"
25,Regularity of solutions to a transport equation,Regularity of solutions to a transport equation,,"Currently I am working on a transport equation and have been able to prove the existence and uniqueness of a weak measurable solution to said equation. I am now working in trying to jot down (with proof) some regularity results. Other papers have stated (without proof) that given a weak solution to the equation $$\partial_t f(t,\mathbf{x}) + div_{\mathbf{x}}(a(t,\mathbf{x})f(t,\mathbf{x})) = 0$$ $$f(0,\mathbf{x}) = f_0(\mathbf{x})$$ $$t \geq 0, \mathbf{x} \in \mathbb{R}^d$$ with initial datum $f_0$ Lipschitz and $a(t,\mathbf{x})$ (vector valued function) bounded and Lipschitz in $\mathbf{x}$ then the solution $f(t,\mathbf{x})$ is Lipschitz in $\mathbf{x}$. How is this proven? Moreover, if we impose stronger conditions on $a$ and $f_0$ will that also comply with a stronger regularity of the solution $f$?","Currently I am working on a transport equation and have been able to prove the existence and uniqueness of a weak measurable solution to said equation. I am now working in trying to jot down (with proof) some regularity results. Other papers have stated (without proof) that given a weak solution to the equation $$\partial_t f(t,\mathbf{x}) + div_{\mathbf{x}}(a(t,\mathbf{x})f(t,\mathbf{x})) = 0$$ $$f(0,\mathbf{x}) = f_0(\mathbf{x})$$ $$t \geq 0, \mathbf{x} \in \mathbb{R}^d$$ with initial datum $f_0$ Lipschitz and $a(t,\mathbf{x})$ (vector valued function) bounded and Lipschitz in $\mathbf{x}$ then the solution $f(t,\mathbf{x})$ is Lipschitz in $\mathbf{x}$. How is this proven? Moreover, if we impose stronger conditions on $a$ and $f_0$ will that also comply with a stronger regularity of the solution $f$?",,"['ordinary-differential-equations', 'partial-differential-equations', 'regularity-theory-of-pdes', 'optimal-transport']"
26,Generalized Bessel Equation?,Generalized Bessel Equation?,,"This seems like a long shot, but is there any closed form solution to  $$y''x^{2r} - {\frac{r}{2}}y'x^r - cyx^{\frac{5r}{2}} = 0 ?$$ Here, we can take $x>0,r>0$ if it helps. This sort of looks like some kind of generalized Bessel equation... but not quite?","This seems like a long shot, but is there any closed form solution to  $$y''x^{2r} - {\frac{r}{2}}y'x^r - cyx^{\frac{5r}{2}} = 0 ?$$ Here, we can take $x>0,r>0$ if it helps. This sort of looks like some kind of generalized Bessel equation... but not quite?",,"['ordinary-differential-equations', 'bessel-functions']"
27,Verify my solution of $y'=(1-y)\sqrt{y}$,Verify my solution of,y'=(1-y)\sqrt{y},"I have to solve $$y'=(1-y)\sqrt{y},\ \ \  y(0)=y_0$$ My approach: $$\begin{align} \int{\frac{1}{(1-y)\sqrt{y}}dy}=x+c\\ 2\int{\frac{1}{1-w^2}dw}=x+c\\ 2\text{arctanh}\sqrt{y}=x+c\\ y=\tanh^2{\frac{x+c}{2}} \end{align}$$ So that $$y(0)=y_0=\tanh^2{\frac{c}{2}},\  \ \ c=2 \text{arctanh}\sqrt{y_0}$$ And so the solution is $$y=\tanh^2(\text{arctanh}\sqrt{y_0}+\frac{x}{2})$$ However, I'm a bit uncomfortable with the fact that for $|y_0|>1$  the $\text{arctanh}\sqrt{y_0}$ will be imaginary. It is not quite clear to me why (or even IF) the solution will be real?","I have to solve $$y'=(1-y)\sqrt{y},\ \ \  y(0)=y_0$$ My approach: $$\begin{align} \int{\frac{1}{(1-y)\sqrt{y}}dy}=x+c\\ 2\int{\frac{1}{1-w^2}dw}=x+c\\ 2\text{arctanh}\sqrt{y}=x+c\\ y=\tanh^2{\frac{x+c}{2}} \end{align}$$ So that $$y(0)=y_0=\tanh^2{\frac{c}{2}},\  \ \ c=2 \text{arctanh}\sqrt{y_0}$$ And so the solution is $$y=\tanh^2(\text{arctanh}\sqrt{y_0}+\frac{x}{2})$$ However, I'm a bit uncomfortable with the fact that for $|y_0|>1$  the $\text{arctanh}\sqrt{y_0}$ will be imaginary. It is not quite clear to me why (or even IF) the solution will be real?",,['ordinary-differential-equations']
28,Which functions solve autonomous ODEs?,Which functions solve autonomous ODEs?,,"Fix an open set $U \subseteq \mathbb{R}$. What can be said about the set $a_n(U)$ of functions $f \in C^n(U, \mathbb{R})$ for which there exists a (sufficiently nice) nonzero function $g : \mathbb{R} \to \mathbb{R}$ such that $$g(f(x), f'(x), f''(x), \ldots, f^{(n)}(x)) = 0$$ for all $x \in U$? Or about the set $$a(U) := \bigcup_n a_n$$ of functions that satisfy some autonomous equation of some order? For instance, are they known to contain or be contained in any well-known classes of functions? (Obviously this depends on what functions $g$ you deem ""sufficiently nice.""  Feel free to make it anything reasonable that gives an interesting result.)","Fix an open set $U \subseteq \mathbb{R}$. What can be said about the set $a_n(U)$ of functions $f \in C^n(U, \mathbb{R})$ for which there exists a (sufficiently nice) nonzero function $g : \mathbb{R} \to \mathbb{R}$ such that $$g(f(x), f'(x), f''(x), \ldots, f^{(n)}(x)) = 0$$ for all $x \in U$? Or about the set $$a(U) := \bigcup_n a_n$$ of functions that satisfy some autonomous equation of some order? For instance, are they known to contain or be contained in any well-known classes of functions? (Obviously this depends on what functions $g$ you deem ""sufficiently nice.""  Feel free to make it anything reasonable that gives an interesting result.)",,['ordinary-differential-equations']
29,Galerkin methods for odes,Galerkin methods for odes,,"Could you give me some information about the multi-adaptive Galerkin methods for odes?? What does the term ""multi-adaptive"" mean?? Are there real-world problems at which we could apply these methods??","Could you give me some information about the multi-adaptive Galerkin methods for odes?? What does the term ""multi-adaptive"" mean?? Are there real-world problems at which we could apply these methods??",,"['ordinary-differential-equations', 'galerkin-methods']"
30,$\frac{dy_t}{dt} = a \frac{dx_t}{dt} + x_t +y_t$ with $x_t$ Ornstein Uhlenbeck process - what to do? [UNRESOLVED],with  Ornstein Uhlenbeck process - what to do? [UNRESOLVED],\frac{dy_t}{dt} = a \frac{dx_t}{dt} + x_t +y_t x_t,"I consider the following equation: $$\frac{dy_t}{dt}  = a \frac{dx_t}{dt} + x_t +y_t, \tag{1}$$  where $a=$ constant and where $x_t$ follows an Ornstein Uhlenbeck process (see here under Alternative representation for nonstationary processes): $$x_t=x_0 e^{-\theta t} +\mu (1-e^{-\theta t})+ {\sigma\over\sqrt{2\theta}}e^{-\theta t}W_{e^{2\theta t}-1}. \tag{2} $$ For $\sigma = 0$ there is no problem => By substituting Eq.(2) into Eq.(1), we get an ODE. For $\sigma >0$ there is a problem. The problem is that when I substitute Eq.(2) into Eq.(1), I will have to differentiate the Brownian motion term. Is it possible to do this? If so, how? Can someone please help me out? I would like to present the solution eventually in for instance in some plot. If useful, numerical methods to solve are also fine for me.","I consider the following equation: $$\frac{dy_t}{dt}  = a \frac{dx_t}{dt} + x_t +y_t, \tag{1}$$  where $a=$ constant and where $x_t$ follows an Ornstein Uhlenbeck process (see here under Alternative representation for nonstationary processes): $$x_t=x_0 e^{-\theta t} +\mu (1-e^{-\theta t})+ {\sigma\over\sqrt{2\theta}}e^{-\theta t}W_{e^{2\theta t}-1}. \tag{2} $$ For $\sigma = 0$ there is no problem => By substituting Eq.(2) into Eq.(1), we get an ODE. For $\sigma >0$ there is a problem. The problem is that when I substitute Eq.(2) into Eq.(1), I will have to differentiate the Brownian motion term. Is it possible to do this? If so, how? Can someone please help me out? I would like to present the solution eventually in for instance in some plot. If useful, numerical methods to solve are also fine for me.",,"['ordinary-differential-equations', 'stochastic-processes', 'numerical-methods', 'stochastic-calculus', 'brownian-motion']"
31,Differential Equations which involve Infinite Series,Differential Equations which involve Infinite Series,,"The problem statement is as follows: Find the general solution for the following equation for $x(t)$. $$x''+ 9x = 2 + \sum_{n=1}^\infty \cos(nt)/n^3$$ I can't find anything about this in my differential equations book; however it seems like a simple case of the linearity of solutions. Please do not feel like you must check the Algebra, etc but rather if the method of solving is correct. I will present my methodology of solving the problem; and would kindly ask if someone could tell me if I am correct as I have no solution provided. First I solve for the complementary solution of: $$x''+9x=0$$ whose solution is: $x_c = C_1\cos(3t)+C_2\sin(3t)$ I proceed to solve for the particular solution corresponding to the constant 2: $x_{p0}=A_0$ thus $9A=2$, and $A=2/9$. Next I solve for the n-th particular solution corresponding to $\cos(nt)$ keeping in mind the duplicity at $n=3$. $x_{pn}=A_n\cos(nt)+B_n\sin(nt)$ from which it follows that the second derivative is: $x_{pn}''=-n^2A_n\cos(nt)-n^2B_n\sin(nt)$ thus for the nth particular solution the differential equation is: $-n^2A_n\cos(nt)-n^2B_n\sin(nt)+9(A_n\cos(nt)+B_n\sin(nt))=\cos(nt)/n^3$ it is obvious that $B_n$ should be zero since no $\sin(nt)$ exist on the right side; furthermore, $-n^2A_n+9A_n=1/n^3$ thus $A_n=\frac 1{(9-n^2)n^3}$ for the case of n=3 the particular solution takes the form: $x_{p3}=tA_3\cos(3t)+tB_3\sin(3t)$ then: $x_{p3}''=\cos(3t)(6B_3-9tA_3)+\sin(3t)(-6A_3-9tB_3)$ putting this into the differential equation yields: $$\cos(3t)(6B_3-9tA_3)+\sin(3t)(-6A_3-9tB_3)+9(\cos(3t)A_3+\sin(3t)B_3)=\frac{\cos(3t)}{n^3}$$ Again equating coefficients yields: $B_3=0$ and $A_3=1/162$ hence would the solution be: $x=C_1\cos(3t)+C_2\sin(3t) + 2/9 + \frac {t\cos3t} {162} + \sum_{n=1,n \neq 3}^ \infty \frac {\cos(nt)} {(9-n^2)n^3}$?","The problem statement is as follows: Find the general solution for the following equation for $x(t)$. $$x''+ 9x = 2 + \sum_{n=1}^\infty \cos(nt)/n^3$$ I can't find anything about this in my differential equations book; however it seems like a simple case of the linearity of solutions. Please do not feel like you must check the Algebra, etc but rather if the method of solving is correct. I will present my methodology of solving the problem; and would kindly ask if someone could tell me if I am correct as I have no solution provided. First I solve for the complementary solution of: $$x''+9x=0$$ whose solution is: $x_c = C_1\cos(3t)+C_2\sin(3t)$ I proceed to solve for the particular solution corresponding to the constant 2: $x_{p0}=A_0$ thus $9A=2$, and $A=2/9$. Next I solve for the n-th particular solution corresponding to $\cos(nt)$ keeping in mind the duplicity at $n=3$. $x_{pn}=A_n\cos(nt)+B_n\sin(nt)$ from which it follows that the second derivative is: $x_{pn}''=-n^2A_n\cos(nt)-n^2B_n\sin(nt)$ thus for the nth particular solution the differential equation is: $-n^2A_n\cos(nt)-n^2B_n\sin(nt)+9(A_n\cos(nt)+B_n\sin(nt))=\cos(nt)/n^3$ it is obvious that $B_n$ should be zero since no $\sin(nt)$ exist on the right side; furthermore, $-n^2A_n+9A_n=1/n^3$ thus $A_n=\frac 1{(9-n^2)n^3}$ for the case of n=3 the particular solution takes the form: $x_{p3}=tA_3\cos(3t)+tB_3\sin(3t)$ then: $x_{p3}''=\cos(3t)(6B_3-9tA_3)+\sin(3t)(-6A_3-9tB_3)$ putting this into the differential equation yields: $$\cos(3t)(6B_3-9tA_3)+\sin(3t)(-6A_3-9tB_3)+9(\cos(3t)A_3+\sin(3t)B_3)=\frac{\cos(3t)}{n^3}$$ Again equating coefficients yields: $B_3=0$ and $A_3=1/162$ hence would the solution be: $x=C_1\cos(3t)+C_2\sin(3t) + 2/9 + \frac {t\cos3t} {162} + \sum_{n=1,n \neq 3}^ \infty \frac {\cos(nt)} {(9-n^2)n^3}$?",,['ordinary-differential-equations']
32,Finding conditions of non existence of Periodic orbit,Finding conditions of non existence of Periodic orbit,,"$$ x'=y \mbox{ and } y'=ax-by-x^2y-x^3 $$ I need non-existence of periodic orbits. Which conditions $a$ and $b$ in $\mathbb{R}$ must satisfy? First, one can see that if $a\leq 0$, then the system has just one equilibrium point, (0,0). If $a>0$, then there are 3 equilibrium points (0,0), $(\pm a^{\frac{1}{2}}, 0)$. I tried to use a Lyapunov function of the form $V(x,y)=\pm ax^2 +y^2$ (depending on $a$) to limit (or not) an orbit, but it is really boring because there was a lot of possibilities to analyze. Is there any simpler way to look at this problem?","$$ x'=y \mbox{ and } y'=ax-by-x^2y-x^3 $$ I need non-existence of periodic orbits. Which conditions $a$ and $b$ in $\mathbb{R}$ must satisfy? First, one can see that if $a\leq 0$, then the system has just one equilibrium point, (0,0). If $a>0$, then there are 3 equilibrium points (0,0), $(\pm a^{\frac{1}{2}}, 0)$. I tried to use a Lyapunov function of the form $V(x,y)=\pm ax^2 +y^2$ (depending on $a$) to limit (or not) an orbit, but it is really boring because there was a lot of possibilities to analyze. Is there any simpler way to look at this problem?",,"['ordinary-differential-equations', 'dynamical-systems']"
33,Homogenous equation to higher order ODE,Homogenous equation to higher order ODE,,"Hello I have a quick question in regard to general form of the solution to $$y^{(4)}-2y^{(3)}+y''=0$$ I had thought to find this solution we would consider $r^{4}-2r^{3}+r^{2}=0$ which factors as $r^2(r-1)^{2}$ that is we have $r_{1,2}=0$ and $r_{3,4}=1$ So what from what I had thought I knew, I thought this implied a solution of form $$y=c_1e^{0x}+c_2xe^{0x}+c_3e^{x}+c_4xe^{x}=c_1+c_2x+c_{3}e^{x}+c_{4}xe^{x}$$ with $c \in \mathbb{R}$ But wolfram says it is $$y(x)=e^x(c_{2}x+c_{1}-2c_{2})+c_{4}x+c_{3}$$ So where am I going wrong? Thanks all PS: I hear it is correct, and I would by wondering about solving $y^{(4)}-2y^{(3)}+y''=x$ Would I be able to use method of undetermined coefficients for this? And if so, because r=0 is a double root of the equations, would by assumed form for a particular solution need to be $x^2({A_{o}x+A_{1}})$? If I did it with this, could I get a correct answer? Im  not sure exactly","Hello I have a quick question in regard to general form of the solution to $$y^{(4)}-2y^{(3)}+y''=0$$ I had thought to find this solution we would consider $r^{4}-2r^{3}+r^{2}=0$ which factors as $r^2(r-1)^{2}$ that is we have $r_{1,2}=0$ and $r_{3,4}=1$ So what from what I had thought I knew, I thought this implied a solution of form $$y=c_1e^{0x}+c_2xe^{0x}+c_3e^{x}+c_4xe^{x}=c_1+c_2x+c_{3}e^{x}+c_{4}xe^{x}$$ with $c \in \mathbb{R}$ But wolfram says it is $$y(x)=e^x(c_{2}x+c_{1}-2c_{2})+c_{4}x+c_{3}$$ So where am I going wrong? Thanks all PS: I hear it is correct, and I would by wondering about solving $y^{(4)}-2y^{(3)}+y''=x$ Would I be able to use method of undetermined coefficients for this? And if so, because r=0 is a double root of the equations, would by assumed form for a particular solution need to be $x^2({A_{o}x+A_{1}})$? If I did it with this, could I get a correct answer? Im  not sure exactly",,['ordinary-differential-equations']
34,Third-order differential equation with initial values using Euler method,Third-order differential equation with initial values using Euler method,,"The problem I have is the initial value problem $$y''' = x + y$$ with  $$ y(1) = 3, y'(1) = 2, y''(1) = 1$$ that should be solved with Eulers method using the step length, $h = \frac{1}{2}$. The iteration step for Eulers method is $y_{n+1} = y_n + hf(x_n, y_n)$. So I should need a system of equations of my initial values I have.  I started with substituting: $$U_1 = y, U_2 = y', U_3 = y''$$ and then $$U_1' = U_2, U_2' = U_3, U_3' = x + U_1$$  And it's here I'm stuck, I don't understand how I should start iterate with the step-length from here, I have only encountered first-order problem with Eulers method so I would love if someone could point me in the right direction.","The problem I have is the initial value problem $$y''' = x + y$$ with  $$ y(1) = 3, y'(1) = 2, y''(1) = 1$$ that should be solved with Eulers method using the step length, $h = \frac{1}{2}$. The iteration step for Eulers method is $y_{n+1} = y_n + hf(x_n, y_n)$. So I should need a system of equations of my initial values I have.  I started with substituting: $$U_1 = y, U_2 = y', U_3 = y''$$ and then $$U_1' = U_2, U_2' = U_3, U_3' = x + U_1$$  And it's here I'm stuck, I don't understand how I should start iterate with the step-length from here, I have only encountered first-order problem with Eulers method so I would love if someone could point me in the right direction.",,"['ordinary-differential-equations', 'numerical-methods']"
35,upper bound of a differential equation solution,upper bound of a differential equation solution,,"Let $A(t)$ be a bounded singular values matrix that is function of time, and $f(t)$ an $L^\infty$ function of time. And consider the ODE $$ \dot x = A(t) x + f(t) $$ How we can describe qualitatively  the magnitude that the euclidean norm $\| x \|$ reach in finite time $T$? In other words, I wish to compute the upper bound of the euclidean norm a a time $T$ of the solution of an ODE without solving it. There is some related result from the Qualitative Theory of ODE's??","Let $A(t)$ be a bounded singular values matrix that is function of time, and $f(t)$ an $L^\infty$ function of time. And consider the ODE $$ \dot x = A(t) x + f(t) $$ How we can describe qualitatively  the magnitude that the euclidean norm $\| x \|$ reach in finite time $T$? In other words, I wish to compute the upper bound of the euclidean norm a a time $T$ of the solution of an ODE without solving it. There is some related result from the Qualitative Theory of ODE's??",,"['integration', 'functional-analysis', 'ordinary-differential-equations', 'inequality']"
36,Differential equations application problem,Differential equations application problem,,"I am studying differential equations, and I saw this interesting problem in another question ( here ): A destroyer is hunting a submarine in a dense fog. The fog lifts for a moment, discloses the submarine on the surface 3 miles away, and immediately descends. The speed of the destroyer is twice that of the submarine, and it is known that the latter will at once dive and depart at full speed in a straight course of unknown direction. What path should the destroyer follow to be certain of passing directly over the submarine? The problem gives a hint: establish a polar coordinate system with the origin at the point where the submarine was sighted. I honestly have no inkling as to how you can solve this problem. I am thinking the path must be some sort of spiral around the submarine's location (pursuit curve?) but I'm not sure.","I am studying differential equations, and I saw this interesting problem in another question ( here ): A destroyer is hunting a submarine in a dense fog. The fog lifts for a moment, discloses the submarine on the surface 3 miles away, and immediately descends. The speed of the destroyer is twice that of the submarine, and it is known that the latter will at once dive and depart at full speed in a straight course of unknown direction. What path should the destroyer follow to be certain of passing directly over the submarine? The problem gives a hint: establish a polar coordinate system with the origin at the point where the submarine was sighted. I honestly have no inkling as to how you can solve this problem. I am thinking the path must be some sort of spiral around the submarine's location (pursuit curve?) but I'm not sure.",,['ordinary-differential-equations']
37,What is meant by a linear SDE?,What is meant by a linear SDE?,,"I am sure this is a ridiculous question, but I can't seem to find a definition. I know the definition of linear ODE or PDE just by saying that the differential operator should be linear, but how does one define a linear SDE? Is the following correct?  The SDE (driven by Brownian motion) $$dX_t=\mu(t,X_t)\mathrm{d}t+\sigma(t,X_t)\mathrm{d}B_t$$ is called linear if for all $t$, $x\mapsto \mu(t,x)$ and $x\mapsto\sigma(t,x)$ are linear functions. And I assume we can extend this definition to path-dependent coefficients?","I am sure this is a ridiculous question, but I can't seem to find a definition. I know the definition of linear ODE or PDE just by saying that the differential operator should be linear, but how does one define a linear SDE? Is the following correct?  The SDE (driven by Brownian motion) $$dX_t=\mu(t,X_t)\mathrm{d}t+\sigma(t,X_t)\mathrm{d}B_t$$ is called linear if for all $t$, $x\mapsto \mu(t,x)$ and $x\mapsto\sigma(t,x)$ are linear functions. And I assume we can extend this definition to path-dependent coefficients?",,"['ordinary-differential-equations', 'stochastic-calculus', 'stochastic-analysis']"
38,Find all the solutions of the differential equation,Find all the solutions of the differential equation,,"I want to find all the solutions of the differential equation $y'+2y=b(x), x \in \mathbb{R}$ where $$b(x)=\left\{\begin{matrix} 1-|x| &, |x| \leq 1 \\ \\ 0 &, |x|>1  \end{matrix}\right.$$ That's what I have tried: $$a(x)=2, \text{ thus } A(x)= \int_0^x 2 dt=2x$$ So if $\phi$ is a solution of the differential equation, then: $$e^{2x} \phi'(x)+ 2 e^{2x} \phi(x)= e^{2x} b(x) \Rightarrow (e^{2x} \phi(x))'=e^{2x} b(x) \\ \Rightarrow \int_0^x (e^{2t} \phi(t))' dt=\int_0^x e^{2t} b(t) dt \Rightarrow e^{2x} \phi(x)- \phi(0)=\int_0^x e^{2t} b(t) dt$$ $$\int_0^x e^{2t} b(t) dt=\left\{\begin{matrix} \int_0^x e^{2t} (1-t)dt & , 0<x<1\\  \int_0^x e^{2t}(1+t)dt & , -1<x<0\\  \int_0^1 e^{2t} (1-t)dt & , x \geq 1\\  \int_0^{-1} e^{2t}(1+t)dt  &  , x\leq-1 \end{matrix}\right.$$ And thus: $$\phi(x)= e^{-2x} \phi(0)+ e^{-2x} g(x)$$ where: $$g(x)=\left\{\begin{matrix} \frac{3e^{2x}-2xe^{2x}-3}{4} &, 0<x<1 \\ \\ \frac{e^{2x}+2xe^{2x}-1}{4} &, -1<x<0 \\ \\ \frac{3e^{2}-2xe^{2}-3}{4} & , x \geq 1 \\ \\ \frac{e^{-2}-2e^{-2}-1}{4} & , x \leq -1 \end{matrix}\right.$$ So the general solution of the differential equation is: $$y(x)= c e^{-2x}+e^{-2x} g(x)$$ Could you tell me if that what I have tried is right?","I want to find all the solutions of the differential equation $y'+2y=b(x), x \in \mathbb{R}$ where $$b(x)=\left\{\begin{matrix} 1-|x| &, |x| \leq 1 \\ \\ 0 &, |x|>1  \end{matrix}\right.$$ That's what I have tried: $$a(x)=2, \text{ thus } A(x)= \int_0^x 2 dt=2x$$ So if $\phi$ is a solution of the differential equation, then: $$e^{2x} \phi'(x)+ 2 e^{2x} \phi(x)= e^{2x} b(x) \Rightarrow (e^{2x} \phi(x))'=e^{2x} b(x) \\ \Rightarrow \int_0^x (e^{2t} \phi(t))' dt=\int_0^x e^{2t} b(t) dt \Rightarrow e^{2x} \phi(x)- \phi(0)=\int_0^x e^{2t} b(t) dt$$ $$\int_0^x e^{2t} b(t) dt=\left\{\begin{matrix} \int_0^x e^{2t} (1-t)dt & , 0<x<1\\  \int_0^x e^{2t}(1+t)dt & , -1<x<0\\  \int_0^1 e^{2t} (1-t)dt & , x \geq 1\\  \int_0^{-1} e^{2t}(1+t)dt  &  , x\leq-1 \end{matrix}\right.$$ And thus: $$\phi(x)= e^{-2x} \phi(0)+ e^{-2x} g(x)$$ where: $$g(x)=\left\{\begin{matrix} \frac{3e^{2x}-2xe^{2x}-3}{4} &, 0<x<1 \\ \\ \frac{e^{2x}+2xe^{2x}-1}{4} &, -1<x<0 \\ \\ \frac{3e^{2}-2xe^{2}-3}{4} & , x \geq 1 \\ \\ \frac{e^{-2}-2e^{-2}-1}{4} & , x \leq -1 \end{matrix}\right.$$ So the general solution of the differential equation is: $$y(x)= c e^{-2x}+e^{-2x} g(x)$$ Could you tell me if that what I have tried is right?",,['ordinary-differential-equations']
39,the solution of $\lambda u''(x) = u(x)$ is $u(x)=\{sin(n\pi x)\}_{n=1}^\infty$,the solution of  is,\lambda u''(x) = u(x) u(x)=\{sin(n\pi x)\}_{n=1}^\infty,in my text it says: the solution of $\lambda u''(x) = u(x)$ is $u(x)=\{sin(n\pi x)\}_{n=1}^\infty$with boundary condition u(0)=u(1)=0 how do I know that this set contains all solutions? What if their is some weird function satisfying this equation.,in my text it says: the solution of $\lambda u''(x) = u(x)$ is $u(x)=\{sin(n\pi x)\}_{n=1}^\infty$with boundary condition u(0)=u(1)=0 how do I know that this set contains all solutions? What if their is some weird function satisfying this equation.,,['ordinary-differential-equations']
40,System of DEs with constant term,System of DEs with constant term,,"This is similar but not identical to standard examples in e.g. Paul's Notes , and while the math seems straightforward the results I get disagree with what I get from numerical simulation.  Given a 2D system $$x' = Ax + b$$ my solution is $$x = c_1e^{\lambda_1t}\eta_1 + c_2e^{\lambda_2t}\eta_2 + A^{-1}b$$ where $\lambda_i$ and $\eta_i$ are eigenvalues and eigenvectors of $A$, respectively.  The first two terms in the RHS of the above are straight out of the textbook and indeed my simulation is fine when $b = 0$.  Is the $A^{-1}b$ term incorrect?","This is similar but not identical to standard examples in e.g. Paul's Notes , and while the math seems straightforward the results I get disagree with what I get from numerical simulation.  Given a 2D system $$x' = Ax + b$$ my solution is $$x = c_1e^{\lambda_1t}\eta_1 + c_2e^{\lambda_2t}\eta_2 + A^{-1}b$$ where $\lambda_i$ and $\eta_i$ are eigenvalues and eigenvectors of $A$, respectively.  The first two terms in the RHS of the above are straight out of the textbook and indeed my simulation is fine when $b = 0$.  Is the $A^{-1}b$ term incorrect?",,"['ordinary-differential-equations', 'systems-of-equations']"
41,Uniqueness of Rectifying Coordinates: Question for Arnold's ODE Book,Uniqueness of Rectifying Coordinates: Question for Arnold's ODE Book,,"In section 7 of his book Ordinary Differential Equations , VI Arnold explains the `rectification theorem', that, given an ordinary differential equation $$\dot{\mathbb{x}} = \mathbb{v(x)}$$ where $\mathbb{v}$ is smooth, and given a point $\mathbb{x_0}$ such that $\mathbb{v(x_0)}\neq 0$, then in some sufficiently small neighbourhood of $\mathbb{x_0}$ one can choose a coordinate system $(y_1,\ldots, y_n)$ such that, in this coordinate system, the equation is transformed into: $$\dot{y}_1 = 1,\quad \dot{y}_2 =\cdots = \dot{y}_n=0$$ Then in problem 1 of section 7.2 he asks: Are the rectifying coordinates $y_i$ uniqely defined? Prove that in 1 dimension the coordinate $y$ is defined to within an affine transformation $\bar{y} = \alpha y + \beta$. I can't see how this can possibly be true. Suppose that $$\dot{x} = v(x)$$ and we've cleverly chosen the rectifying coordinates $$y = f(x)$$ so that $$\dot{y} = \frac{dy}{dx}\frac{dx}{dt} = \frac{df}{dx} \dot{x} = 1.$$ Then if we consider $\bar{y} = \alpha y + \beta = \alpha f(x) + \beta$, then $$\dot{\bar{y}} = \frac{d\bar{y}}{dy}\frac{dy}{dt} = \alpha \dot{y} = \alpha \neq 1.$$ so $\bar{y}$ cannot be a rectifying coordinate. Intuitively, I can see that rectifying coordinates remains such under translations (i.e. adding $\beta$), but multiplication by $\alpha$ scales the vector field, and I feel this should interfere with the $\dot{y}_1=1$ term, as the maths seems to show. Taking it as axiomatic that Arnold isn't wrong - where is my  misunderstanding? :)","In section 7 of his book Ordinary Differential Equations , VI Arnold explains the `rectification theorem', that, given an ordinary differential equation $$\dot{\mathbb{x}} = \mathbb{v(x)}$$ where $\mathbb{v}$ is smooth, and given a point $\mathbb{x_0}$ such that $\mathbb{v(x_0)}\neq 0$, then in some sufficiently small neighbourhood of $\mathbb{x_0}$ one can choose a coordinate system $(y_1,\ldots, y_n)$ such that, in this coordinate system, the equation is transformed into: $$\dot{y}_1 = 1,\quad \dot{y}_2 =\cdots = \dot{y}_n=0$$ Then in problem 1 of section 7.2 he asks: Are the rectifying coordinates $y_i$ uniqely defined? Prove that in 1 dimension the coordinate $y$ is defined to within an affine transformation $\bar{y} = \alpha y + \beta$. I can't see how this can possibly be true. Suppose that $$\dot{x} = v(x)$$ and we've cleverly chosen the rectifying coordinates $$y = f(x)$$ so that $$\dot{y} = \frac{dy}{dx}\frac{dx}{dt} = \frac{df}{dx} \dot{x} = 1.$$ Then if we consider $\bar{y} = \alpha y + \beta = \alpha f(x) + \beta$, then $$\dot{\bar{y}} = \frac{d\bar{y}}{dy}\frac{dy}{dt} = \alpha \dot{y} = \alpha \neq 1.$$ so $\bar{y}$ cannot be a rectifying coordinate. Intuitively, I can see that rectifying coordinates remains such under translations (i.e. adding $\beta$), but multiplication by $\alpha$ scales the vector field, and I feel this should interfere with the $\dot{y}_1=1$ term, as the maths seems to show. Taking it as axiomatic that Arnold isn't wrong - where is my  misunderstanding? :)",,['ordinary-differential-equations']
42,General solution of continuous function-dependent ODE,General solution of continuous function-dependent ODE,,"Given a continuous function $f:I\subseteq\mathbb R\to \mathbb R$ and an ordinary differential equation given by: $$ y''-xf(x) y' + f(x) y = 0. $$ I'd like to solve this ODE. However, I don't know how to attack the problem and I'd be very pleased if you give me a hint. Edit I It has been relatively easy to me to figure out that $y_1(x) := x$ satisfies the ode for every function $f$. If $y_2 := u \cdot x$ for some function $u:I\to\mathbb R$, then $$ 0 = u''x+2u'- x f(x) (u'x+u)+f(x) u x = x u'' + [2-x^2 f(x)]u'$$ and setting $v := u'$ $$ 0 = x v' + [2-x^2 f(x)] v \iff \frac{dv}{v} = [x^2 f(x)-2]\frac{dx}{x} $$ in its ""differential form"", so $$ \ln|v(x)| = \int \left(x f(x) - \frac{2}{x}\right) dx = -2 \ln|x|+xF(x)-\int F(s) ds \implies ...$$ where $F:I\to\mathbb R$ is a primitive of $f$. Is this right? Am I missing something?","Given a continuous function $f:I\subseteq\mathbb R\to \mathbb R$ and an ordinary differential equation given by: $$ y''-xf(x) y' + f(x) y = 0. $$ I'd like to solve this ODE. However, I don't know how to attack the problem and I'd be very pleased if you give me a hint. Edit I It has been relatively easy to me to figure out that $y_1(x) := x$ satisfies the ode for every function $f$. If $y_2 := u \cdot x$ for some function $u:I\to\mathbb R$, then $$ 0 = u''x+2u'- x f(x) (u'x+u)+f(x) u x = x u'' + [2-x^2 f(x)]u'$$ and setting $v := u'$ $$ 0 = x v' + [2-x^2 f(x)] v \iff \frac{dv}{v} = [x^2 f(x)-2]\frac{dx}{x} $$ in its ""differential form"", so $$ \ln|v(x)| = \int \left(x f(x) - \frac{2}{x}\right) dx = -2 \ln|x|+xF(x)-\int F(s) ds \implies ...$$ where $F:I\to\mathbb R$ is a primitive of $f$. Is this right? Am I missing something?",,[]
43,How can you alter the Volterra-Lotka system to obtain a model of cooperative species?,How can you alter the Volterra-Lotka system to obtain a model of cooperative species?,,"The Volterra-Lotka system for two competitive species is: \begin{equation*} \frac{dx}{dt} = x(-Ax-By+C) \\ \frac{dy}{dt} = y(-DX-Ey+F) \end{equation*} where $x,y\geq 0 $ and $ A,B,C,D,E,F$ all positive. The Model system for cooperative species is: \begin{equation*} \frac{dx}{dt} = x(-Ax+By+C) \\ \frac{dy}{dt} = y(DX-Ey+F) \end{equation*} but I dont know who to explaining  using words. what happens when $x$ decreases or when $y$ decrease. And if $x$ increases then $y$ increases.","The Volterra-Lotka system for two competitive species is: \begin{equation*} \frac{dx}{dt} = x(-Ax-By+C) \\ \frac{dy}{dt} = y(-DX-Ey+F) \end{equation*} where $x,y\geq 0 $ and $ A,B,C,D,E,F$ all positive. The Model system for cooperative species is: \begin{equation*} \frac{dx}{dt} = x(-Ax+By+C) \\ \frac{dy}{dt} = y(DX-Ey+F) \end{equation*} but I dont know who to explaining  using words. what happens when $x$ decreases or when $y$ decrease. And if $x$ increases then $y$ increases.",,['ordinary-differential-equations']
44,Show that any solution of $x' = X(x)$ it is defined for all $t > 0$.,Show that any solution of  it is defined for all .,x' = X(x) t > 0,"If $X=(X_1,X_2,...,X_n)$ is a vector field of class $C^1$ in $\mathbb{R^n}$ and $V$ it is a differentiable function in $\mathbb{R^n}$ such that  $$\sum_{i=1}^n\frac{\partial V}{\partial x_i}(x)X_i(x) \leq 0 $$ and $V(x)\geq |x|^2$ $\forall x \in \mathbb{R}^n$. Show that any solution of $x' = X(x)$ it  is defined for all $t>0$.","If $X=(X_1,X_2,...,X_n)$ is a vector field of class $C^1$ in $\mathbb{R^n}$ and $V$ it is a differentiable function in $\mathbb{R^n}$ such that  $$\sum_{i=1}^n\frac{\partial V}{\partial x_i}(x)X_i(x) \leq 0 $$ and $V(x)\geq |x|^2$ $\forall x \in \mathbb{R}^n$. Show that any solution of $x' = X(x)$ it  is defined for all $t>0$.",,"['analysis', 'ordinary-differential-equations']"
45,How to Separate Quasi-Linear PDE,How to Separate Quasi-Linear PDE,,"I'm attempting to solve the non-homogenous quasi-linear PDE below: $$z\frac{\partial z}{\partial x} - \alpha y\frac{\partial z}{\partial y} = \frac{\beta}{x^3y}$$ From what I've read in texts, the general form of a quasi-linear PDE is defined as $$a(x,y,z)\frac{\partial z}{\partial x} + b(x,y,z)\frac{\partial z}{\partial y} - c(x,y,z) = 0$$ with solutions (called characteristic curves) $\phi(x,y,z) = C_1$ and $\psi(x,y,z) = C_2$ given by the characteristic equations $$\frac{dx}{a} = \frac{dy}{b} = \frac{dz}{c}$$ When I set up these equations for my problem, I find $$a(x,y,z) = z$$ $$b(x,y,z) = -\alpha y$$ $$c(x,y,z) = \frac{\beta}{x^3 y}$$ which leads to $$\frac{dx}{z} = -\frac{dy}{\alpha y} = \frac{x^3ydz}{\beta}$$ Due to the coupling in the last term I cannot find a way to separate these to get two expressions containing a total derivative. Could anyone help?","I'm attempting to solve the non-homogenous quasi-linear PDE below: $$z\frac{\partial z}{\partial x} - \alpha y\frac{\partial z}{\partial y} = \frac{\beta}{x^3y}$$ From what I've read in texts, the general form of a quasi-linear PDE is defined as $$a(x,y,z)\frac{\partial z}{\partial x} + b(x,y,z)\frac{\partial z}{\partial y} - c(x,y,z) = 0$$ with solutions (called characteristic curves) $\phi(x,y,z) = C_1$ and $\psi(x,y,z) = C_2$ given by the characteristic equations $$\frac{dx}{a} = \frac{dy}{b} = \frac{dz}{c}$$ When I set up these equations for my problem, I find $$a(x,y,z) = z$$ $$b(x,y,z) = -\alpha y$$ $$c(x,y,z) = \frac{\beta}{x^3 y}$$ which leads to $$\frac{dx}{z} = -\frac{dy}{\alpha y} = \frac{x^3ydz}{\beta}$$ Due to the coupling in the last term I cannot find a way to separate these to get two expressions containing a total derivative. Could anyone help?",,"['ordinary-differential-equations', 'partial-differential-equations']"
46,Is there a test for tractability of nonlinear differential equations?,Is there a test for tractability of nonlinear differential equations?,,"After lengthy attempts at tackling the problem one might say that coming up with a closed form solution for a nonlinear differential equation is not possible - that the problem is intractable. But is there a more direct, universal way to determine if a problem is intractable or not?","After lengthy attempts at tackling the problem one might say that coming up with a closed form solution for a nonlinear differential equation is not possible - that the problem is intractable. But is there a more direct, universal way to determine if a problem is intractable or not?",,"['ordinary-differential-equations', 'problem-solving', 'nonlinear-system']"
47,Is this a property of commutative differential operators?,Is this a property of commutative differential operators?,,"I found this claim in a paper but the proof escapes me. I'm sure that it's simple. Suppose we have $\psi$, a solution to the ODE $LQ\psi=0$, where $L$ and $Q$ are commutative differential operators. Then it is possible to write $\psi=\psi_1+\psi_2$ such that $L\psi_1=0$ and $Q\psi_2=0$. Thanks","I found this claim in a paper but the proof escapes me. I'm sure that it's simple. Suppose we have $\psi$, a solution to the ODE $LQ\psi=0$, where $L$ and $Q$ are commutative differential operators. Then it is possible to write $\psi=\psi_1+\psi_2$ such that $L\psi_1=0$ and $Q\psi_2=0$. Thanks",,['ordinary-differential-equations']
48,Solve $x(x-1)y''+6x^2y'+3y=0$ using Frobenius's Method,Solve  using Frobenius's Method,x(x-1)y''+6x^2y'+3y=0,"Solve $x(x-1)y''+6x^2y'+3y=0$ using Frobenius's Method I can't solve this ODE. How can I get first two term? and indicial equation is also very confusing. I can solve two term recurrence relation by Frobenius Method, but it's too hard to solve three term recurrence relation. P.S Recurrence relation that I write above is correct. And Sorry for my bad English","Solve $x(x-1)y''+6x^2y'+3y=0$ using Frobenius's Method I can't solve this ODE. How can I get first two term? and indicial equation is also very confusing. I can solve two term recurrence relation by Frobenius Method, but it's too hard to solve three term recurrence relation. P.S Recurrence relation that I write above is correct. And Sorry for my bad English",,['ordinary-differential-equations']
49,Reduce PDE to ODE,Reduce PDE to ODE,,"Maybe you don't want to check all the details, but could look at a few equations here. Would you mind leaving a comment that you at least some part looks okay?- This way, I know that at least somebody checked it, too (although maybe not every step). I want to reduce the PDE $$ \sum_{i=1}^{n} \partial_i \left( \frac{\partial_i u(x)}{\sqrt{1+ \sum_{j=1}^{n} ( \partial_j u(x)^2 ) }} \right)=0$$ to an ODE by looking at solutions of the form $u(x):=f(||x||).$ We have $\partial_i u(x) = f'(||x||) \frac{x_i}{||x||}$ and $\partial_{i,j}u(x) = f''(||x||) \frac{x_i x_j}{||x||^2} + f'(||x||) \frac{\delta_{i,j}||x||^2 - x_j x_i }{||x||^3}.$ Applying the product rule to the PDE and multiplying by the square root in the demoninator, I get $$ \left(\sum_{i=1}^{n} \partial_i^2 u(x) \right) \left(1+ \sum_{j=1}^{n} ( \partial_j u(x)^2)  \right) - \left(\sum_{i,k=1}^n \partial_i u(x) \partial_k u(x) \partial_{i,k} u(x) \right) =0$$ Rewriting gives $$\left(\sum_{i=1}^{n} \partial_i^2 u(x) \right) = f''(||x||)+f'(||x||) \frac{(n-1)}{||x||}$$ $$\left(1+ \sum_{j=1}^{n} ( \partial_j u(x)^2)  \right) = 1+f'(||x||)^2$$ and finally $$ \left(\sum_{i,k=1}^n \partial_i u(x) \partial_k u(x) \partial_{i,k} u(x) \right) = f'(||x||)^2 f''(||x||).$$ Is this all correct?","Maybe you don't want to check all the details, but could look at a few equations here. Would you mind leaving a comment that you at least some part looks okay?- This way, I know that at least somebody checked it, too (although maybe not every step). I want to reduce the PDE $$ \sum_{i=1}^{n} \partial_i \left( \frac{\partial_i u(x)}{\sqrt{1+ \sum_{j=1}^{n} ( \partial_j u(x)^2 ) }} \right)=0$$ to an ODE by looking at solutions of the form $u(x):=f(||x||).$ We have $\partial_i u(x) = f'(||x||) \frac{x_i}{||x||}$ and $\partial_{i,j}u(x) = f''(||x||) \frac{x_i x_j}{||x||^2} + f'(||x||) \frac{\delta_{i,j}||x||^2 - x_j x_i }{||x||^3}.$ Applying the product rule to the PDE and multiplying by the square root in the demoninator, I get $$ \left(\sum_{i=1}^{n} \partial_i^2 u(x) \right) \left(1+ \sum_{j=1}^{n} ( \partial_j u(x)^2)  \right) - \left(\sum_{i,k=1}^n \partial_i u(x) \partial_k u(x) \partial_{i,k} u(x) \right) =0$$ Rewriting gives $$\left(\sum_{i=1}^{n} \partial_i^2 u(x) \right) = f''(||x||)+f'(||x||) \frac{(n-1)}{||x||}$$ $$\left(1+ \sum_{j=1}^{n} ( \partial_j u(x)^2)  \right) = 1+f'(||x||)^2$$ and finally $$ \left(\sum_{i,k=1}^n \partial_i u(x) \partial_k u(x) \partial_{i,k} u(x) \right) = f'(||x||)^2 f''(||x||).$$ Is this all correct?",,"['calculus', 'real-analysis', 'analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
50,Analytic approximation of $\ddot x+\gamma sign(\dot x)+x=0$,Analytic approximation of,\ddot x+\gamma sign(\dot x)+x=0,I am trying to find an analytic approximation to this non-linear differential equation. $$ \ddot x+\gamma sign(\dot x)+x=0 $$  $\gamma$ is a very small parameter. The solution I am getting is  $$ x(t)=a_{0}e^{\frac{-2\gamma t}{\pi}}\cos t $$  When I compare this to a numerically approximated solution in Mathematica it is clear that my approximation is incorrect. I can't seem to figure out what term is responsible for the ending behavior or the numerically calculated blue line. The orange line is my approximation.  Can anyone help show me where I went wrong. Any help would be appreciated.Thanks.,I am trying to find an analytic approximation to this non-linear differential equation. $$ \ddot x+\gamma sign(\dot x)+x=0 $$  $\gamma$ is a very small parameter. The solution I am getting is  $$ x(t)=a_{0}e^{\frac{-2\gamma t}{\pi}}\cos t $$  When I compare this to a numerically approximated solution in Mathematica it is clear that my approximation is incorrect. I can't seem to figure out what term is responsible for the ending behavior or the numerically calculated blue line. The orange line is my approximation.  Can anyone help show me where I went wrong. Any help would be appreciated.Thanks.,,['ordinary-differential-equations']
51,Asymptotic Behavior of Differential Equation,Asymptotic Behavior of Differential Equation,,"physicist here. I'm studying some problems that involve the use of differential equations. The professor of the course has indicated that usually variable changes used to simplify the equations come from an asymptotic analysis:  For example if I try to analize then a equation like $\frac{dx}{dt}+x=\frac{t}{x}$, I would start making  $x \rightarrow 0$, and then solving the equation $\frac{dx}{dt}=\frac{t}{x}$. After that, I'll take dominant term $u(t)$ in the solution to make a variable change $x(t)=u(t)f(t)$ and then simplify (and solve) the equation. The problem that now I'm facing is the following differential equation, at  $ x \rightarrow 1$: $$ x^2 s \frac{d^2u(x)}{dx^2} + xs\frac{du(x)}{dx}+\frac{u(x)x}{c} \left ( \frac{p}{1-x} + \frac{q}{(1-x)^2}\right )=-K u(x)$$ My try: First I tried to change all that $x$ by 1 except the denominators, then solving the differential equation. It was completely impossible anyway. I've thought on power series, but that $1-x$ are very annoying.  I've finally tried with Mathematica. I discovered that substituting the $x$ by 1 is NOT a good idea. Also now I'm pretty sure that the solution is an hypergeometric. However I could not solve the entire equation, I had to eliminate the $p/(1-x)$ term (the computer could not simplify in that case). I need a good way to attack this. Any ideas?","physicist here. I'm studying some problems that involve the use of differential equations. The professor of the course has indicated that usually variable changes used to simplify the equations come from an asymptotic analysis:  For example if I try to analize then a equation like $\frac{dx}{dt}+x=\frac{t}{x}$, I would start making  $x \rightarrow 0$, and then solving the equation $\frac{dx}{dt}=\frac{t}{x}$. After that, I'll take dominant term $u(t)$ in the solution to make a variable change $x(t)=u(t)f(t)$ and then simplify (and solve) the equation. The problem that now I'm facing is the following differential equation, at  $ x \rightarrow 1$: $$ x^2 s \frac{d^2u(x)}{dx^2} + xs\frac{du(x)}{dx}+\frac{u(x)x}{c} \left ( \frac{p}{1-x} + \frac{q}{(1-x)^2}\right )=-K u(x)$$ My try: First I tried to change all that $x$ by 1 except the denominators, then solving the differential equation. It was completely impossible anyway. I've thought on power series, but that $1-x$ are very annoying.  I've finally tried with Mathematica. I discovered that substituting the $x$ by 1 is NOT a good idea. Also now I'm pretty sure that the solution is an hypergeometric. However I could not solve the entire equation, I had to eliminate the $p/(1-x)$ term (the computer could not simplify in that case). I need a good way to attack this. Any ideas?",,"['ordinary-differential-equations', 'problem-solving', 'mathematical-physics']"
52,What type of equation is this?,What type of equation is this?,,"Is this equation an ODE or PDE $$ \frac{d^3u}{dx^3}−αxu=0, x∈R $$ The only thing given is $\int_R u(x) =\pi $ and $α>0$ is some constant. I have to find the solution using fourier transformation but  we've only ever learnt for PDE since it is a PDE class.","Is this equation an ODE or PDE $$ \frac{d^3u}{dx^3}−αxu=0, x∈R $$ The only thing given is $\int_R u(x) =\pi $ and $α>0$ is some constant. I have to find the solution using fourier transformation but  we've only ever learnt for PDE since it is a PDE class.",,"['ordinary-differential-equations', 'partial-differential-equations', 'fourier-series']"
53,Prove that the solution of an ODE can be prolonged to $\infty$,Prove that the solution of an ODE can be prolonged to,\infty,"I need an help understanding some general techniques in ordinary differential equations. I've never attended a course on ODE, so I'm quite confused on the argument, but I'm trying to improve my knowledgle. We have the following Cauchy problem: $$\begin{cases}\frac{dv}{dt}=b(v(t)+f(t))\\ v(s)=0\end{cases}$$  where $b\in C^1(\mathbb R^n, \mathbb R^n)$ and it is monotone, i.e. $\left<b(x)-b(y),x-y\right>\leq k(1+|x-y|^2)\quad \forall x,y\in \mathbb R^n.$ $f:[s,+\infty]\to \mathbb R^n$ is continuous. We know that a  unique solution $v$ exists defined on the interval $[s,T)$ (since $F(t,x):=b(x+f(t))$ is continuously differentiable with respect to $x$). We want to prove that we can prolong the solution to $+\infty$, i.e. there exists a unique solution defined on $[s,+\infty).$ Now: Using monotonicity assumption on $b$ we have that $$\frac{1}{2} \frac{d}{dt}|v(t)|^2=\left< \frac{d}{dt}v(t),v(t)\right> \leq C(1+|v(t)|^2);$$ Integrating the above expression on the interval $[s,t]$ and using Gronwall's Lemma we have $$|v(t)|^2\leq 2C(T-s)e^{2C(T-s)}, \quad \forall t\in[s,T).$$ I've written on my notes that from 1. and 2. follows $$\sup_{t\in[s,T)} \left|\frac{dv}{dt}\right|=R_T<\infty,\tag{$*$}$$ and so we can prolong the solution to $\infty$. Now, I agree with the fact that if the solution does not explode we can prolong it, but I don't get why from 1. and 2. follows $(*).$ Can someone help me? Any suggestion is really appreciated.","I need an help understanding some general techniques in ordinary differential equations. I've never attended a course on ODE, so I'm quite confused on the argument, but I'm trying to improve my knowledgle. We have the following Cauchy problem: $$\begin{cases}\frac{dv}{dt}=b(v(t)+f(t))\\ v(s)=0\end{cases}$$  where $b\in C^1(\mathbb R^n, \mathbb R^n)$ and it is monotone, i.e. $\left<b(x)-b(y),x-y\right>\leq k(1+|x-y|^2)\quad \forall x,y\in \mathbb R^n.$ $f:[s,+\infty]\to \mathbb R^n$ is continuous. We know that a  unique solution $v$ exists defined on the interval $[s,T)$ (since $F(t,x):=b(x+f(t))$ is continuously differentiable with respect to $x$). We want to prove that we can prolong the solution to $+\infty$, i.e. there exists a unique solution defined on $[s,+\infty).$ Now: Using monotonicity assumption on $b$ we have that $$\frac{1}{2} \frac{d}{dt}|v(t)|^2=\left< \frac{d}{dt}v(t),v(t)\right> \leq C(1+|v(t)|^2);$$ Integrating the above expression on the interval $[s,t]$ and using Gronwall's Lemma we have $$|v(t)|^2\leq 2C(T-s)e^{2C(T-s)}, \quad \forall t\in[s,T).$$ I've written on my notes that from 1. and 2. follows $$\sup_{t\in[s,T)} \left|\frac{dv}{dt}\right|=R_T<\infty,\tag{$*$}$$ and so we can prolong the solution to $\infty$. Now, I agree with the fact that if the solution does not explode we can prolong it, but I don't get why from 1. and 2. follows $(*).$ Can someone help me? Any suggestion is really appreciated.",,"['real-analysis', 'ordinary-differential-equations']"
54,Solving System of Boundary Value problem,Solving System of Boundary Value problem,,"The boundary value problem: $$y'' + Q(t)y = f(t)$$ satisfying $$Ay(a) +By(b) = g$$ where A, B and Q are the matrices of order n. After calculation, we can get the form of solution will be  $$y(x) = -(A+B)^{-1}\left[\int_x^bB \cdot f(s)ds + \int_a^xA \cdot f(s)ds\right]$$ $$= \int_a^xD^{-1}A \cdot f(s)ds - \int_x^b D^{-1}B \cdot f(s)ds$$ where $D=A+B$ $$= \int_a^bG(x,s) \cdot f(s)ds$$ and $$G(x,s) = \begin{cases}D^{-1}A \text{ if } x >s \\ -D^{-1}B \text{ if } x < s\end{cases}$$ So, we used the Green function to get the solution of boundary value system.  My question is what happened if the system is solvable, but $D$ is not invertible ?","The boundary value problem: $$y'' + Q(t)y = f(t)$$ satisfying $$Ay(a) +By(b) = g$$ where A, B and Q are the matrices of order n. After calculation, we can get the form of solution will be  $$y(x) = -(A+B)^{-1}\left[\int_x^bB \cdot f(s)ds + \int_a^xA \cdot f(s)ds\right]$$ $$= \int_a^xD^{-1}A \cdot f(s)ds - \int_x^b D^{-1}B \cdot f(s)ds$$ where $D=A+B$ $$= \int_a^bG(x,s) \cdot f(s)ds$$ and $$G(x,s) = \begin{cases}D^{-1}A \text{ if } x >s \\ -D^{-1}B \text{ if } x < s\end{cases}$$ So, we used the Green function to get the solution of boundary value system.  My question is what happened if the system is solvable, but $D$ is not invertible ?",,"['ordinary-differential-equations', 'numerical-methods']"
55,"If $y'=\frac{1}{x+1}$ and $y(0)=0$, find the value of $y(-2) $","If  and , find the value of",y'=\frac{1}{x+1} y(0)=0 y(-2) ,"If $y'=\dfrac{1}{x+1}$ and $y(0)=0$, find the value of $y(-2) = ?$ By integrating I am getting $$y = \ln (x+1)+C$$ I am stuck somewhat as it looks tricky from here. Any help ? Thanks!","If $y'=\dfrac{1}{x+1}$ and $y(0)=0$, find the value of $y(-2) = ?$ By integrating I am getting $$y = \ln (x+1)+C$$ I am stuck somewhat as it looks tricky from here. Any help ? Thanks!",,"['calculus', 'integration', 'ordinary-differential-equations']"
56,Laplace transforms for a pharmacokinetics multi-compartmental model,Laplace transforms for a pharmacokinetics multi-compartmental model,,"I am an anaesthetist trying to write some pharmacokinetics software as a pet project. Unfortunately the maths I need is a bit too much for my rusty high school calculus, and I am out of my depth. I am working with the following system of equations: $\frac{dC_{1}}{dt}=k_{21}C_{2}(t)-k_{12}C_{1}(t)+k_{31}C_{3}(t)-k_{13}C_{1}(t)-k_{10}C_{1}(t)+C_{inf}$ $\frac{dC_{2}}{dt}=k_{12}C_{1}(t)-k_{21}C_{2}(t)$ $\frac{dC_{3}}{dt}=k_{13}C_{1}(t)-k_{31}C_{3}(t)$ $\frac{dC_{e}}{dt}=k_{e0}C_{1}(t)-k_{e0}C_{e}(t)$ These equations model the distribution of a drug in the human body according to a multi-compartmental model where: $C_{1}$: concentration in compartment 1 (bloodstream). $C_{2}$: concentration in compartment 2 (richly perfused, mostly muscle). $C_{3}$: concentration in compartment 3 (poorly perfused, mostly fat). $C_{e}$: concentration at effector site (brain). $C_{inf}$: concentration of infusion (for a drug given intravenously at a constant rate, the rate divided by the volume of distribution of compartment 1). $k_{10}$: constant of elimination from compartment 1. $k_{12}$, $k_{13}$, etc.: constants of equilibration between compartments. $k_{e0}$: constant of equilibration to effector site (it is assumed to be the same in both directions). t: time. All the above are known except for $C_{1}$, $C_{2}$, $C_{3}$ and $C_{e}$. We also know what the 4 concentrations are at t = 0. I have done Laplace transforms of these equations (with the aid of the computer program Maxima) and solved the system for the Laplace transforms. I have not been able to calculate the inverse of the transforms, so I have used Zakian's method to obtain a numerical approximation to the inverse of the Laplace transforms. This approach works reasonably well and provides quite accurate results, but it is computationally intensive, and therefore too slow to draw graphics on the fly. Also it bugs me to use a numerical approximation when there may be an exact solution. This link seems to indicate that for a simpler two compartment model it is possible to find an exact solution by doing Laplace transforms, solving the system of equations, and then inverting the transforms. Is my system of equations solvable? If so, I would be very grateful for some pointers.","I am an anaesthetist trying to write some pharmacokinetics software as a pet project. Unfortunately the maths I need is a bit too much for my rusty high school calculus, and I am out of my depth. I am working with the following system of equations: $\frac{dC_{1}}{dt}=k_{21}C_{2}(t)-k_{12}C_{1}(t)+k_{31}C_{3}(t)-k_{13}C_{1}(t)-k_{10}C_{1}(t)+C_{inf}$ $\frac{dC_{2}}{dt}=k_{12}C_{1}(t)-k_{21}C_{2}(t)$ $\frac{dC_{3}}{dt}=k_{13}C_{1}(t)-k_{31}C_{3}(t)$ $\frac{dC_{e}}{dt}=k_{e0}C_{1}(t)-k_{e0}C_{e}(t)$ These equations model the distribution of a drug in the human body according to a multi-compartmental model where: $C_{1}$: concentration in compartment 1 (bloodstream). $C_{2}$: concentration in compartment 2 (richly perfused, mostly muscle). $C_{3}$: concentration in compartment 3 (poorly perfused, mostly fat). $C_{e}$: concentration at effector site (brain). $C_{inf}$: concentration of infusion (for a drug given intravenously at a constant rate, the rate divided by the volume of distribution of compartment 1). $k_{10}$: constant of elimination from compartment 1. $k_{12}$, $k_{13}$, etc.: constants of equilibration between compartments. $k_{e0}$: constant of equilibration to effector site (it is assumed to be the same in both directions). t: time. All the above are known except for $C_{1}$, $C_{2}$, $C_{3}$ and $C_{e}$. We also know what the 4 concentrations are at t = 0. I have done Laplace transforms of these equations (with the aid of the computer program Maxima) and solved the system for the Laplace transforms. I have not been able to calculate the inverse of the transforms, so I have used Zakian's method to obtain a numerical approximation to the inverse of the Laplace transforms. This approach works reasonably well and provides quite accurate results, but it is computationally intensive, and therefore too slow to draw graphics on the fly. Also it bugs me to use a numerical approximation when there may be an exact solution. This link seems to indicate that for a simpler two compartment model it is possible to find an exact solution by doing Laplace transforms, solving the system of equations, and then inverting the transforms. Is my system of equations solvable? If so, I would be very grateful for some pointers.",,"['integration', 'ordinary-differential-equations', 'laplace-transform']"
57,solving PDE with state-dependent boundary conditions,solving PDE with state-dependent boundary conditions,,"I am interested in solving the following PDE (heat equation): $$\frac{\partial u}{\partial t} = \kappa \frac{\partial ^2 u}{\partial x^2}$$ In order to solve it, I discretize space uniformly into $N$ segments and convert the PDE into $N+1$ ODEs: $$ \frac{du_k}{dt} = \kappa\frac{u_{k+1} + u_{k-1} - 2u_k}{(\Delta x)^2} \quad k=0,1\ldots.N$$ which I can solve using any ODE routine (such as ode45 or ode15s in MATLAB). For the particular case of my problem, the Neumann boundary conditions for the PDE are dependent on the values of $u_k$ itself, i.e  $$ \frac{du_b}{dx}= \left\{ \begin{array}{cc} \alpha & u_b < u_{critical}, \\ -\gamma u_b & u_b \geq u_{critical} \end{array} \right. $$ where $\alpha$ and $\gamma$ are constants and $b=\{0,N\}$. Assuming that I am solving the above problem using some numerical ODE solver (such as ode45 in MATLAB), and there are two routines: odefun (which I provide to the solver) to evaluate the derivatives and odestep which i define and the solver calls it after every successful integration step, where should I check and change the boundary conditions, in odefun or odestep ? I am asking this because the odefun routine might be used internally by the solver for evaluation of jacobian etc and ideally, we should not change boundary conditions in the middle of an integration step, right? PS. I was not sure which was the better place to ask this question, programming stack exchange or maths stack exchange. I asked it here because its essentially a math problem.","I am interested in solving the following PDE (heat equation): $$\frac{\partial u}{\partial t} = \kappa \frac{\partial ^2 u}{\partial x^2}$$ In order to solve it, I discretize space uniformly into $N$ segments and convert the PDE into $N+1$ ODEs: $$ \frac{du_k}{dt} = \kappa\frac{u_{k+1} + u_{k-1} - 2u_k}{(\Delta x)^2} \quad k=0,1\ldots.N$$ which I can solve using any ODE routine (such as ode45 or ode15s in MATLAB). For the particular case of my problem, the Neumann boundary conditions for the PDE are dependent on the values of $u_k$ itself, i.e  $$ \frac{du_b}{dx}= \left\{ \begin{array}{cc} \alpha & u_b < u_{critical}, \\ -\gamma u_b & u_b \geq u_{critical} \end{array} \right. $$ where $\alpha$ and $\gamma$ are constants and $b=\{0,N\}$. Assuming that I am solving the above problem using some numerical ODE solver (such as ode45 in MATLAB), and there are two routines: odefun (which I provide to the solver) to evaluate the derivatives and odestep which i define and the solver calls it after every successful integration step, where should I check and change the boundary conditions, in odefun or odestep ? I am asking this because the odefun routine might be used internally by the solver for evaluation of jacobian etc and ideally, we should not change boundary conditions in the middle of an integration step, right? PS. I was not sure which was the better place to ask this question, programming stack exchange or maths stack exchange. I asked it here because its essentially a math problem.",,"['ordinary-differential-equations', 'partial-differential-equations', 'matlab', 'boundary-value-problem', 'heat-equation']"
58,Find the trajectories that follow drops of water on a given surface.,Find the trajectories that follow drops of water on a given surface.,,"We have a saddle that have the form of the surface $z=y^2-x^2$ and is outside, under the rain. Find the trajectories that drops of water will follow if they fall on such surface. Can somebody give me a hand?","We have a saddle that have the form of the surface $z=y^2-x^2$ and is outside, under the rain. Find the trajectories that drops of water will follow if they fall on such surface. Can somebody give me a hand?",,['ordinary-differential-equations']
59,Linear system of ODE and the solution of IVP,Linear system of ODE and the solution of IVP,,"Assume that the matrix $A(t)$ is continuous for $t\in (a,b)$ and $R(\mathbf{x},t)$ is continuous for all $\mathbf{x}$ and $t\in (a,b)$. Prove that the IVP $$\frac{d\mathbf{x}}{dt}=A(t)\mathbf{x} + R(\mathbf{x},t),$$ $$\mathbf{x} (t_0)=\mathbf{x}^0$$ is equivalent to the integral equation $$\mathbf{x} (t)= \Phi (t)\Phi^{-1}(t_0)\mathbf{x}^0 +\int_{t_0}^t \Phi (t-s+t_0)\Phi^{-1}(t_0)R(\mathbf{x} (s),s)ds,$$ where $\Phi (t)$ is any fundamental matrix of the homogeneous system $$\frac{d\mathbf{x}}{dt}=A(t)\mathbf{x}.$$ I have learned a similar theorem (from Birkhoff and Rota's book) that if $A$ is a constant matrix, $\mathbf{b}(t)=(b_1(t),\cdots,b_n(t))$ is continuous, and $$\frac{d\mathbf{x}}{dt}=A\mathbf{x} + \mathbf{b}(t),$$ $$\mathbf{x} (t_0)=\mathbf{x}^0$$ then the solution of the above IVP is given by $$\mathbf{x} (t)= \Phi (t)\Phi^{-1}(t_0)\mathbf{x}^0 +\int_{t_0}^t \Phi (t-s+t_0)\Phi^{-1}(t_0)\mathbf{b}(s)ds.$$ I have tried to mimick that proof but have failed. Please helps.","Assume that the matrix $A(t)$ is continuous for $t\in (a,b)$ and $R(\mathbf{x},t)$ is continuous for all $\mathbf{x}$ and $t\in (a,b)$. Prove that the IVP $$\frac{d\mathbf{x}}{dt}=A(t)\mathbf{x} + R(\mathbf{x},t),$$ $$\mathbf{x} (t_0)=\mathbf{x}^0$$ is equivalent to the integral equation $$\mathbf{x} (t)= \Phi (t)\Phi^{-1}(t_0)\mathbf{x}^0 +\int_{t_0}^t \Phi (t-s+t_0)\Phi^{-1}(t_0)R(\mathbf{x} (s),s)ds,$$ where $\Phi (t)$ is any fundamental matrix of the homogeneous system $$\frac{d\mathbf{x}}{dt}=A(t)\mathbf{x}.$$ I have learned a similar theorem (from Birkhoff and Rota's book) that if $A$ is a constant matrix, $\mathbf{b}(t)=(b_1(t),\cdots,b_n(t))$ is continuous, and $$\frac{d\mathbf{x}}{dt}=A\mathbf{x} + \mathbf{b}(t),$$ $$\mathbf{x} (t_0)=\mathbf{x}^0$$ then the solution of the above IVP is given by $$\mathbf{x} (t)= \Phi (t)\Phi^{-1}(t_0)\mathbf{x}^0 +\int_{t_0}^t \Phi (t-s+t_0)\Phi^{-1}(t_0)\mathbf{b}(s)ds.$$ I have tried to mimick that proof but have failed. Please helps.",,['ordinary-differential-equations']
60,need help to understand the differential equation,need help to understand the differential equation,,"In one of the books, it was mentioned $\frac{d}{dx}(x^3 \tan x)= (x^2\sec^3x+3x^2\tan x)$, but i think it should be $(x^3\sec^2x+3x^2\tan x)$. I feel its a printing mistake. Just wanted to be sure, if that's not the case, can you please tell me how do we get $(x^2\sec^3x+3x^2\tan x)$? -Kamal.","In one of the books, it was mentioned $\frac{d}{dx}(x^3 \tan x)= (x^2\sec^3x+3x^2\tan x)$, but i think it should be $(x^3\sec^2x+3x^2\tan x)$. I feel its a printing mistake. Just wanted to be sure, if that's not the case, can you please tell me how do we get $(x^2\sec^3x+3x^2\tan x)$? -Kamal.",,"['ordinary-differential-equations', 'derivatives']"
61,General analytical solution for first order time varying system of ODEs,General analytical solution for first order time varying system of ODEs,,"I asked a question related to this previously, but not as explicitly as I should have, I'm restating it more concisely here. Assume we are given some matrix $\mathbf{A}(t)$, which is time dependent. This matrix is square and not invertible. A system: $$ \frac{\text{d}\mathbf{x}(t)}{\text{d}t}=\mathbf{A}(t)\mathbf{x}(t) $$ Admits the solution: $$ \mathbf{x}(t)=\mathbf{x}(0)\text{exp}\left({\int^t_0\mathbf{A}(\tau)\,\text{d}\tau}\right) $$ If the following property holds: $$ \mathbf{A}(t)\mathbf{A}(\tau)=\mathbf{A}(\tau)\mathbf{A}(t) $$ The property is also know as the matrix being semi-proper . Is there an analytical solution for the cases when $\mathbf{A}(t)$ is not semi-proper? If there is no analytical solution, how should one go about constructing an approximate solution? What methods are commonly used?","I asked a question related to this previously, but not as explicitly as I should have, I'm restating it more concisely here. Assume we are given some matrix $\mathbf{A}(t)$, which is time dependent. This matrix is square and not invertible. A system: $$ \frac{\text{d}\mathbf{x}(t)}{\text{d}t}=\mathbf{A}(t)\mathbf{x}(t) $$ Admits the solution: $$ \mathbf{x}(t)=\mathbf{x}(0)\text{exp}\left({\int^t_0\mathbf{A}(\tau)\,\text{d}\tau}\right) $$ If the following property holds: $$ \mathbf{A}(t)\mathbf{A}(\tau)=\mathbf{A}(\tau)\mathbf{A}(t) $$ The property is also know as the matrix being semi-proper . Is there an analytical solution for the cases when $\mathbf{A}(t)$ is not semi-proper? If there is no analytical solution, how should one go about constructing an approximate solution? What methods are commonly used?",,"['linear-algebra', 'ordinary-differential-equations', 'numerical-methods']"
62,Can differential equations have orders of derivatives that are dependent on the independent variable?,Can differential equations have orders of derivatives that are dependent on the independent variable?,,"I was wondering if you can construct and/or solve a differential equation of the following form, $$\frac{d^{(f(x))}y}{dx^{(f(x))}}=g(x)$$ where $y$ is dependent only on $x$ and $f(x)$ is a function whose range consists of integers only, say $f(x)=\lfloor x\rfloor $. A buddy of mine was wondering something similar by considering a DE like $$\frac{d^{(\lfloor x\rfloor\pmod2+1)}y}{dx^{(\lfloor x\rfloor\pmod2+1)}}=x$$ so that the order of the derivative would alternate between $1$ and $2$, and it struck me as an interesting question. I don't know/understand as much DE theory as I'd like to, so I would like to know if this sort of situation is at all possible, and if so, what might a physical interpretation be? My first instinct is that it's not possible, but supposing it were, I expect some piecewise definitions would be involved.","I was wondering if you can construct and/or solve a differential equation of the following form, $$\frac{d^{(f(x))}y}{dx^{(f(x))}}=g(x)$$ where $y$ is dependent only on $x$ and $f(x)$ is a function whose range consists of integers only, say $f(x)=\lfloor x\rfloor $. A buddy of mine was wondering something similar by considering a DE like $$\frac{d^{(\lfloor x\rfloor\pmod2+1)}y}{dx^{(\lfloor x\rfloor\pmod2+1)}}=x$$ so that the order of the derivative would alternate between $1$ and $2$, and it struck me as an interesting question. I don't know/understand as much DE theory as I'd like to, so I would like to know if this sort of situation is at all possible, and if so, what might a physical interpretation be? My first instinct is that it's not possible, but supposing it were, I expect some piecewise definitions would be involved.",,"['ordinary-differential-equations', 'soft-question']"
63,Existence and uniqueness of strong solution of stochastic differential equation.,Existence and uniqueness of strong solution of stochastic differential equation.,,"I am currently going through the proof of the existence of a solution of the SDE \begin{align} dX_t = bdt + \sigma dB_t \end{align} where $B_t$ is a Brownian motion with respect to a filtration $\{\mathcal{F}_t\}_{t\in[0,T]}$.  We assume the following conditions hold for $t \in [0,T]$: 1) $b(t,X_t) \text{ and } \sigma(t,X_t) \text{ are } \mathcal{B}\times\mathcal{F} \text{-measurable}$ and $\mathcal{F}_t$-adapted. 2) $|b(t,x)| \leq L(1 + |x|)$ and $\sigma(t,x) \leq L(1 + |x|)$. 3) $|b(t,x) - b(t,y)| \leq L|x - y|$ and $|\sigma(t,x) - \sigma(t,y)| \leq L|x - y|$. 4) $X_0$ is $\mathcal{F}_0$-measurable, and $E[|X_0|^2] < \infty$. Now, every proof I have seen uses the Picard iterations \begin{align} \begin{cases} X_t^0 &= X_0. \\ X_t^{n+1} &= X_0 + \int_0^tb(s,X_s^n)ds + \int_0^t \sigma(s,X_s^n)dB_s. \end{cases} \end{align} There are several things I am struggling to understand in the proof, and I am happy with any help/hints I can get. First we need to show that the iterates $X_t^n$ are elements of $L_{\text{ad}}^2([0,T]\times \Omega)$, that is, that properties a), b), and c) are satisfied: a) $X_t^n$ is continuous in $t$ b) $X_t^n$ is $\mathcal{B}\times\mathcal{F}$ -measurable and $\mathcal{F}_t$-adapted c) $E\Big[ \int_0^t |X_s^n|^2 ds \Big] < \infty$ We proceed by induction, and clearly a) - c) are satisfied for $n=0$, due to condition 4). Now, we assume that a) - c) hold for $X_t^n$. By condition 2), the Cauchy-Schwarz(C-S) inequality as well as $(x_1+x_2+ \cdots+x_n)^2 \leq n(x_1^2 + x_2^2 + \cdots + x_n^2)$ it is easy to see that \begin{align*} E\bigg[ \int_0^t \big| \sigma(s,X_s^{n})\big|^2 ds \bigg] &\leq 2 L^2 \int_0^t 1 + E\big[ |X_s^{n}|^2 \big] ds < \infty \\ \int_0^t \big| b(s,X_s^{n}) \big| ds & \leq L\sqrt{2t} \sqrt{\int_0^t 1 + |X_s^{n-1}|^2 ds} < \infty \end{align*}  where the last inequality holds almost surely. Hence the above integrals exist with probability 1. Now, the argument is that this implies that properties a) and b) are satisfied. I fail to see why. Thoughts: $T_1$ - If $\sigma(t,X_t^n)$ is measurable and adapted, we have that, since the expected integral of $\sigma(t,X_t^n)^2$ is finite, that the Ito integral is $t$-continuous a.s. (a property of processes satisfying a) - c) ). $T_2$ - Once a) and b) is settled, c) is found by using condition 2), $(x_1+x_2+ \cdots+x_n)^2 \leq n(x_1^2 + x_2^2 + \cdots + x_n^2)$ and C-S. Questions: $Q_1$ - We know from condition $1$) that for instance $b(t,X_t)$ is jointly measurable and adapted. Is this different from $b(t,X_t^n)$ satisfying these properties? I really don't see how to prove that $X_t^{n+1}$ is jointly measurable and adapted.","I am currently going through the proof of the existence of a solution of the SDE \begin{align} dX_t = bdt + \sigma dB_t \end{align} where $B_t$ is a Brownian motion with respect to a filtration $\{\mathcal{F}_t\}_{t\in[0,T]}$.  We assume the following conditions hold for $t \in [0,T]$: 1) $b(t,X_t) \text{ and } \sigma(t,X_t) \text{ are } \mathcal{B}\times\mathcal{F} \text{-measurable}$ and $\mathcal{F}_t$-adapted. 2) $|b(t,x)| \leq L(1 + |x|)$ and $\sigma(t,x) \leq L(1 + |x|)$. 3) $|b(t,x) - b(t,y)| \leq L|x - y|$ and $|\sigma(t,x) - \sigma(t,y)| \leq L|x - y|$. 4) $X_0$ is $\mathcal{F}_0$-measurable, and $E[|X_0|^2] < \infty$. Now, every proof I have seen uses the Picard iterations \begin{align} \begin{cases} X_t^0 &= X_0. \\ X_t^{n+1} &= X_0 + \int_0^tb(s,X_s^n)ds + \int_0^t \sigma(s,X_s^n)dB_s. \end{cases} \end{align} There are several things I am struggling to understand in the proof, and I am happy with any help/hints I can get. First we need to show that the iterates $X_t^n$ are elements of $L_{\text{ad}}^2([0,T]\times \Omega)$, that is, that properties a), b), and c) are satisfied: a) $X_t^n$ is continuous in $t$ b) $X_t^n$ is $\mathcal{B}\times\mathcal{F}$ -measurable and $\mathcal{F}_t$-adapted c) $E\Big[ \int_0^t |X_s^n|^2 ds \Big] < \infty$ We proceed by induction, and clearly a) - c) are satisfied for $n=0$, due to condition 4). Now, we assume that a) - c) hold for $X_t^n$. By condition 2), the Cauchy-Schwarz(C-S) inequality as well as $(x_1+x_2+ \cdots+x_n)^2 \leq n(x_1^2 + x_2^2 + \cdots + x_n^2)$ it is easy to see that \begin{align*} E\bigg[ \int_0^t \big| \sigma(s,X_s^{n})\big|^2 ds \bigg] &\leq 2 L^2 \int_0^t 1 + E\big[ |X_s^{n}|^2 \big] ds < \infty \\ \int_0^t \big| b(s,X_s^{n}) \big| ds & \leq L\sqrt{2t} \sqrt{\int_0^t 1 + |X_s^{n-1}|^2 ds} < \infty \end{align*}  where the last inequality holds almost surely. Hence the above integrals exist with probability 1. Now, the argument is that this implies that properties a) and b) are satisfied. I fail to see why. Thoughts: $T_1$ - If $\sigma(t,X_t^n)$ is measurable and adapted, we have that, since the expected integral of $\sigma(t,X_t^n)^2$ is finite, that the Ito integral is $t$-continuous a.s. (a property of processes satisfying a) - c) ). $T_2$ - Once a) and b) is settled, c) is found by using condition 2), $(x_1+x_2+ \cdots+x_n)^2 \leq n(x_1^2 + x_2^2 + \cdots + x_n^2)$ and C-S. Questions: $Q_1$ - We know from condition $1$) that for instance $b(t,X_t)$ is jointly measurable and adapted. Is this different from $b(t,X_t^n)$ satisfying these properties? I really don't see how to prove that $X_t^{n+1}$ is jointly measurable and adapted.",,"['ordinary-differential-equations', 'stochastic-calculus', 'stochastic-analysis']"
64,Is there a fiber bundle approach to nonlinear oscillations?,Is there a fiber bundle approach to nonlinear oscillations?,,"I've recently been learning about nonlinear oscillations, and I noticed a seemingly strong connection between how the equations of motion are solved/approximated, and fiber bundles (or vector bundles considering the systems use vectors). For instance: in general when looking at a nonlinear system, you usually only consider a small localization around a minimum that allows the equations of motion to be approximated as linear differential equations; however, the global system is in general nonlinear, so I figured it's like dealing with a local trivialization. But looking online I couldn't seem to find anything relating the two or giving a fiber bundle approach. I thought it might be interesting, or maybe shed light on understanding nonlinear oscillations. Though I've been told that almost all of these systems have no mathematical solutions (only numerical approximations), so is it maybe that this approach has been tried, but it didn't give anything new? Or is there a fiber/vector bundle approach, but I'm just not looking in the right places?","I've recently been learning about nonlinear oscillations, and I noticed a seemingly strong connection between how the equations of motion are solved/approximated, and fiber bundles (or vector bundles considering the systems use vectors). For instance: in general when looking at a nonlinear system, you usually only consider a small localization around a minimum that allows the equations of motion to be approximated as linear differential equations; however, the global system is in general nonlinear, so I figured it's like dealing with a local trivialization. But looking online I couldn't seem to find anything relating the two or giving a fiber bundle approach. I thought it might be interesting, or maybe shed light on understanding nonlinear oscillations. Though I've been told that almost all of these systems have no mathematical solutions (only numerical approximations), so is it maybe that this approach has been tried, but it didn't give anything new? Or is there a fiber/vector bundle approach, but I'm just not looking in the right places?",,"['ordinary-differential-equations', 'dynamical-systems', 'vector-bundles', 'nonlinear-system', 'fiber-bundles']"
65,Writing ODE system with a complex variable,Writing ODE system with a complex variable,,"I'm looking at the system of ODEs: $$\begin{cases}\dot{x} = -y + kx + xy^2\\ \dot{y} = x + ky - x^2\end{cases}$$ I'm trying to introduce a complex variable $z = x+iy$ to write this as a single first order equation so that I can then find an approximation for the limit cycle. So far I have that $\dot{z} = (k+i)z + xy^2 - ix^2$ and I'm having difficulty with the last two terms, in particular they are different order in x and y so I'm struggling to see how I could replace them with a simple function of z. Thanks","I'm looking at the system of ODEs: $$\begin{cases}\dot{x} = -y + kx + xy^2\\ \dot{y} = x + ky - x^2\end{cases}$$ I'm trying to introduce a complex variable $z = x+iy$ to write this as a single first order equation so that I can then find an approximation for the limit cycle. So far I have that $\dot{z} = (k+i)z + xy^2 - ix^2$ and I'm having difficulty with the last two terms, in particular they are different order in x and y so I'm struggling to see how I could replace them with a simple function of z. Thanks",,"['ordinary-differential-equations', 'dynamical-systems', 'systems-of-equations']"
66,Solving a Non-Exact First-Order ODE,Solving a Non-Exact First-Order ODE,,"Consider the implicit differential equation: $$(45y^3+33xy)dx+(50xy^2+18x^2)dy=0$$ Show that $x^py^q$ is an integrating factor of this equation, and find the explicit values of $p$ and $q$. Then, use the integrating factor to obtain a solution in implicit form. I have learned about the two special cases, where the integrating factor may be a function of $x$ only or $y$ only, but in this case, neither is true; thus, the integrating factor must be a function of both. We know that a non-exact ODE of the form $M(x,y)dx+N(x,y)dy=0$ can be made exact if and only if $\frac{\partial}{\partial y}[\mu(x,y)M(x,y)]=\frac{\partial}{\partial x}[\mu(x,y)N(x,y)]$, where $\mu(x,y)$ is the integrating factor. I just don't know how to apply this formula in a more general manner, without resorting to the two special cases. Any help would be appreciated.","Consider the implicit differential equation: $$(45y^3+33xy)dx+(50xy^2+18x^2)dy=0$$ Show that $x^py^q$ is an integrating factor of this equation, and find the explicit values of $p$ and $q$. Then, use the integrating factor to obtain a solution in implicit form. I have learned about the two special cases, where the integrating factor may be a function of $x$ only or $y$ only, but in this case, neither is true; thus, the integrating factor must be a function of both. We know that a non-exact ODE of the form $M(x,y)dx+N(x,y)dy=0$ can be made exact if and only if $\frac{\partial}{\partial y}[\mu(x,y)M(x,y)]=\frac{\partial}{\partial x}[\mu(x,y)N(x,y)]$, where $\mu(x,y)$ is the integrating factor. I just don't know how to apply this formula in a more general manner, without resorting to the two special cases. Any help would be appreciated.",,[]
67,A question about solving the nonlinear differential equation $\dot{x} = x(1-x)$,A question about solving the nonlinear differential equation,\dot{x} = x(1-x),"I am aware of the standard solution that makes use of partial fractions. However, I made the following manipulations, in order to be more rigorous with splitting up the differentials before integration: $$\dot{x} = f(x) = x(1-x)$$ $$\frac{\textrm{d}x}{\textrm{d}t}\frac{1}{f(x)} = 1\quad\text{...definition of $\dot{x}$}$$ $$\int \frac{\textrm{d}x}{\textrm{d}t}\frac{1}{f(x)}\;\textrm{d}t = \int 1\;\textrm{d}t$$ $$\int \frac{\textrm{d}x}{\textrm{d}t}\frac{1}{f(x)}\;\textrm{d}t = t + K$$ $$\int \frac{\textrm{d}\ln{f(x(t))}}{\textrm{d}t}\;\textrm{d}t = t + K\quad\text{...observing Chain Rule}$$ $$\ln{f(x)} = t + K\quad\text{...fundamental theorem of calculus}$$ $$\ln{x(1-x)} = t + K\quad\text{...definition of $f(x)$}$$ $$\ln{x} + \ln{(1-x)} = t + K\quad\text{ERROR}$$ I have made an error somewhere, but I do not know where. Could you teach me?","I am aware of the standard solution that makes use of partial fractions. However, I made the following manipulations, in order to be more rigorous with splitting up the differentials before integration: $$\dot{x} = f(x) = x(1-x)$$ $$\frac{\textrm{d}x}{\textrm{d}t}\frac{1}{f(x)} = 1\quad\text{...definition of $\dot{x}$}$$ $$\int \frac{\textrm{d}x}{\textrm{d}t}\frac{1}{f(x)}\;\textrm{d}t = \int 1\;\textrm{d}t$$ $$\int \frac{\textrm{d}x}{\textrm{d}t}\frac{1}{f(x)}\;\textrm{d}t = t + K$$ $$\int \frac{\textrm{d}\ln{f(x(t))}}{\textrm{d}t}\;\textrm{d}t = t + K\quad\text{...observing Chain Rule}$$ $$\ln{f(x)} = t + K\quad\text{...fundamental theorem of calculus}$$ $$\ln{x(1-x)} = t + K\quad\text{...definition of $f(x)$}$$ $$\ln{x} + \ln{(1-x)} = t + K\quad\text{ERROR}$$ I have made an error somewhere, but I do not know where. Could you teach me?",,['ordinary-differential-equations']
68,"Pursuit Curve, Parametric Equation","Pursuit Curve, Parametric Equation",,"So its a classic problem: Object $A$ starts at the origin $(0,0)$ and moves straight up the $y$ axis with a speed $v$. Object $B$ starts at point $(1,0)$, always moves towards object $A$ and has a speed of $2v$. There are answers on Stack Exchange to the question of finding $y_b (x) $ where $y_b$ is the $y$ coordinate of object $b$. But I want to find $y_b(t)$ and $x_b(t)$ where $t$ is time These, I think are the two equations you get from the problem have: Since the velocity of object $b$ is always $2v$ $$ \sqrt{x_b'(t)^2 + y_b'(t)^2} = 2v$$ Since the slope of object $B$ is always pointing toward object $A$, whose coordinates are $(0, vt)$ $$ \frac{y_b'(t)}{x_b'(t)} = \frac {y_b(t) - vt}{x_b(t)}$$ So far,  I have had no luck solving this, and I did try mathematica, but that didn't work. I'm not even sure if there is a reasonable solution. If you must, you can let $v = 1$, and bonus points if you can solve this for more generic coordinates. Note: I've only just finished calculus $AB$, so if could keep that in mind with your responses, that would be helpful :)","So its a classic problem: Object $A$ starts at the origin $(0,0)$ and moves straight up the $y$ axis with a speed $v$. Object $B$ starts at point $(1,0)$, always moves towards object $A$ and has a speed of $2v$. There are answers on Stack Exchange to the question of finding $y_b (x) $ where $y_b$ is the $y$ coordinate of object $b$. But I want to find $y_b(t)$ and $x_b(t)$ where $t$ is time These, I think are the two equations you get from the problem have: Since the velocity of object $b$ is always $2v$ $$ \sqrt{x_b'(t)^2 + y_b'(t)^2} = 2v$$ Since the slope of object $B$ is always pointing toward object $A$, whose coordinates are $(0, vt)$ $$ \frac{y_b'(t)}{x_b'(t)} = \frac {y_b(t) - vt}{x_b(t)}$$ So far,  I have had no luck solving this, and I did try mathematica, but that didn't work. I'm not even sure if there is a reasonable solution. If you must, you can let $v = 1$, and bonus points if you can solve this for more generic coordinates. Note: I've only just finished calculus $AB$, so if could keep that in mind with your responses, that would be helpful :)",,"['ordinary-differential-equations', 'parametric']"
69,Equilibrium points and linear stability,Equilibrium points and linear stability,,"Consider the nondimensional amplitude equation for $A = A(t)$ where $t$ is time given by (1): $$ \frac{dA}{dt} = \sigma A - a_1 A^3 - a_3 A^5 = f(A) \text{ with } \sigma \in \mathbb{R}, a_1 < 0, \text{ and } a_3 > 0 $$ (a) Show that this equation has three equilibrium points (2a) $$ A(t) \equiv A_e \text{ such that } f(A_e) = 0 $$ satisfying either (2b) $$ (i)\ \ A_e = 0 \text{ or } (ii,iii)\ \ 2a_3 A^2_e = \pm\sqrt{a_1^2 + 4a_3\sigma} - a_1 > 0 $$ and find the $\sigma$-range over which each of them exists. (b) Seeking a solution of (1) of the form (3a) $$ A(t) = A_e + \epsilon_1A_1(t) + O(\epsilon_1^2) \text{ where } |\epsilon_1|<<1 $$ and employing the two-term Taylor polynomial expansion (3b) $$ f(A) = f(A_e) + f'(A_e)(A-A_e) + O(A-A_e)^2 $$ obtain (3c) $$ \frac{dA_1}{dt} = f'(A_e)A_1 $$ (c) Particularizing the result of (3c) sequentially to the critical points of (2b), show that (i) is linearly stable for $\sigma < 0$ and (ii) & (iii) are linearly stable and unstable, respectively, fr the $\sigma$-range over which they are defined. (d) Determine the global stability behavior of these equilibrium points upon multiplying (1) by $A(t)$, rewriting the resulting equation in the form $(1/2)(dA^2/dt) = F(A^2)$, and plotting $F$ versus $A^2$ for $\sigma < \sigma_{-1} = -a_1^2/(4a_3)$, $\sigma_{-1}<\sigma < 0$, and $\sigma > 0$, respectively. (e) Defining $\epsilon^2 = |a_1|$, assuming $a_3 = O(1)$ as $\epsilon \to 0$, and considering $\sigma = O(\epsilon^4) > 0$, first demonstrate that $A_e^+ = O(\epsilon)$. Now introducing the rescaled variables $\tau = \sigma t$, $A'(\tau) = A(t)/A_e^+$ where $A', dA'/d\tau = O(1)$ as $\epsilon \to 0$ into the equation $dA/dt = f(A) + O(A^7)$, and taking the limit as $\epsilon \to 0$, deduce that (1) is a valid truncation for that equation. I've solved parts (a) and (b), but have no idea where to start for the rest of them. Any ideas?","Consider the nondimensional amplitude equation for $A = A(t)$ where $t$ is time given by (1): $$ \frac{dA}{dt} = \sigma A - a_1 A^3 - a_3 A^5 = f(A) \text{ with } \sigma \in \mathbb{R}, a_1 < 0, \text{ and } a_3 > 0 $$ (a) Show that this equation has three equilibrium points (2a) $$ A(t) \equiv A_e \text{ such that } f(A_e) = 0 $$ satisfying either (2b) $$ (i)\ \ A_e = 0 \text{ or } (ii,iii)\ \ 2a_3 A^2_e = \pm\sqrt{a_1^2 + 4a_3\sigma} - a_1 > 0 $$ and find the $\sigma$-range over which each of them exists. (b) Seeking a solution of (1) of the form (3a) $$ A(t) = A_e + \epsilon_1A_1(t) + O(\epsilon_1^2) \text{ where } |\epsilon_1|<<1 $$ and employing the two-term Taylor polynomial expansion (3b) $$ f(A) = f(A_e) + f'(A_e)(A-A_e) + O(A-A_e)^2 $$ obtain (3c) $$ \frac{dA_1}{dt} = f'(A_e)A_1 $$ (c) Particularizing the result of (3c) sequentially to the critical points of (2b), show that (i) is linearly stable for $\sigma < 0$ and (ii) & (iii) are linearly stable and unstable, respectively, fr the $\sigma$-range over which they are defined. (d) Determine the global stability behavior of these equilibrium points upon multiplying (1) by $A(t)$, rewriting the resulting equation in the form $(1/2)(dA^2/dt) = F(A^2)$, and plotting $F$ versus $A^2$ for $\sigma < \sigma_{-1} = -a_1^2/(4a_3)$, $\sigma_{-1}<\sigma < 0$, and $\sigma > 0$, respectively. (e) Defining $\epsilon^2 = |a_1|$, assuming $a_3 = O(1)$ as $\epsilon \to 0$, and considering $\sigma = O(\epsilon^4) > 0$, first demonstrate that $A_e^+ = O(\epsilon)$. Now introducing the rescaled variables $\tau = \sigma t$, $A'(\tau) = A(t)/A_e^+$ where $A', dA'/d\tau = O(1)$ as $\epsilon \to 0$ into the equation $dA/dt = f(A) + O(A^7)$, and taking the limit as $\epsilon \to 0$, deduce that (1) is a valid truncation for that equation. I've solved parts (a) and (b), but have no idea where to start for the rest of them. Any ideas?",,"['ordinary-differential-equations', 'stability-in-odes']"
70,Master equation of chemical reaction,Master equation of chemical reaction,,"I have about the construction of master equation for chemical reaction i.e. I have to construct differential equations for the probability mass function for the number of particles A, B and C. When I have this following reaction  $$\emptyset\rightarrow^{k_A}{A}$$ $$\emptyset\rightarrow^{k_B}{B}$$ $$A\rightarrow^{k_A}{B}$$ $$B\rightarrow^{\gamma}{\emptyset}$$ I have no problem to construct the master equation because if i consider P(n,m,t), P(n,t), P(m,t) where n is the number of particles A and m the number of particles B at time t I should have the following equations with respect to each reaction (if I'm right): $$ P'(n,t)=P(n-1,t)k_{A}-P(n,t)k_A $$ $$ P'(m,t)=P(m-1,t)k_{B}-P(m,t)k_B $$ $$ P'(m,t)=P(m-1,t)nk_{A}-P(m,t)nk_A $$ $$ P'(m,t)=P(m+1,t)(m+1)\gamma-P(m,t)\gamma $$ But for example, when I have to consider more complicated reaction, in which way should I have to procede?  For example, for the following reaction $$ A+B\rightarrow^{\gamma}\emptyset $$ is the following reaction correct? $$ P'(n,m,t)=P(n+1,m+1,t)(n+1)(m+1)\gamma-P(n,m,t)nm\gamma $$ And for other kind of reaction like these: $$ A+B\rightarrow^{\gamma_B}A $$ $$ A+A\rightarrow^{\gamma_{AA}}\emptyset $$ $$ A+B\rightarrow^{k_C}C $$ In which way should I procede?  I will be very happy if someone could help me! Thank you very much in advance!","I have about the construction of master equation for chemical reaction i.e. I have to construct differential equations for the probability mass function for the number of particles A, B and C. When I have this following reaction  $$\emptyset\rightarrow^{k_A}{A}$$ $$\emptyset\rightarrow^{k_B}{B}$$ $$A\rightarrow^{k_A}{B}$$ $$B\rightarrow^{\gamma}{\emptyset}$$ I have no problem to construct the master equation because if i consider P(n,m,t), P(n,t), P(m,t) where n is the number of particles A and m the number of particles B at time t I should have the following equations with respect to each reaction (if I'm right): $$ P'(n,t)=P(n-1,t)k_{A}-P(n,t)k_A $$ $$ P'(m,t)=P(m-1,t)k_{B}-P(m,t)k_B $$ $$ P'(m,t)=P(m-1,t)nk_{A}-P(m,t)nk_A $$ $$ P'(m,t)=P(m+1,t)(m+1)\gamma-P(m,t)\gamma $$ But for example, when I have to consider more complicated reaction, in which way should I have to procede?  For example, for the following reaction $$ A+B\rightarrow^{\gamma}\emptyset $$ is the following reaction correct? $$ P'(n,m,t)=P(n+1,m+1,t)(n+1)(m+1)\gamma-P(n,m,t)nm\gamma $$ And for other kind of reaction like these: $$ A+B\rightarrow^{\gamma_B}A $$ $$ A+A\rightarrow^{\gamma_{AA}}\emptyset $$ $$ A+B\rightarrow^{k_C}C $$ In which way should I procede?  I will be very happy if someone could help me! Thank you very much in advance!",,"['probability', 'ordinary-differential-equations', 'stochastic-processes']"
71,"The differential equation $xu_{x}+yu_{y}=2u$ satisfying the initial condition $y=xg(x),u=f(x)$",The differential equation  satisfying the initial condition,"xu_{x}+yu_{y}=2u y=xg(x),u=f(x)","I was thinking about the following problem : Find out which of the following option(s) is/are correct? The differential equation $xu_{x}+yu_{y}=2u$ satisfying the initial condition $y=xg(x),u=f(x)$ , with (a) $f(x)=2x,g(x)=1$ has no solution, (b) $f(x)=2x^2,g(x)=1$ has infinite number of solutions, (c) $f(x)=x^3,g(x)=x$ has a unique solution, (d) $f(x)=x^4,g(x)=x$ has a unique solution. My Attempt: Using lagrange's method, we see that $\frac{dx}{x}=\frac{dy}{y}=\frac{du}{2u} $ gives $y/x=c_1,$ and $y=\sqrt uc_2$ ,where $c_1,c_2$ being constants. Now we have to apply to the given initial conditions  and  check the given options.Here ,I observe that $y=xg(x)=x.1=x$ ,[as $g(x)=1$ ] and $u=f(x)=2x^2$ But,this relation does not satisfy $xu_{x}+yu_{y}=2u$ and so the choice""(a) $f(x)=2x,g(x)=1$ has no solution,"" is right. Now I am stuck here and could not progress further.Am i going in the right direction? Please help.Thanks in advance for your time.","I was thinking about the following problem : Find out which of the following option(s) is/are correct? The differential equation satisfying the initial condition , with (a) has no solution, (b) has infinite number of solutions, (c) has a unique solution, (d) has a unique solution. My Attempt: Using lagrange's method, we see that gives and ,where being constants. Now we have to apply to the given initial conditions  and  check the given options.Here ,I observe that ,[as ] and But,this relation does not satisfy and so the choice""(a) has no solution,"" is right. Now I am stuck here and could not progress further.Am i going in the right direction? Please help.Thanks in advance for your time.","xu_{x}+yu_{y}=2u y=xg(x),u=f(x) f(x)=2x,g(x)=1 f(x)=2x^2,g(x)=1 f(x)=x^3,g(x)=x f(x)=x^4,g(x)=x \frac{dx}{x}=\frac{dy}{y}=\frac{du}{2u}  y/x=c_1, y=\sqrt uc_2 c_1,c_2 y=xg(x)=x.1=x g(x)=1 u=f(x)=2x^2 xu_{x}+yu_{y}=2u f(x)=2x,g(x)=1",[]
72,Closed orbits of vector fields under perturbation,Closed orbits of vector fields under perturbation,,"Consider a vector field $V$ on an annulus $U$, say. Also, assume that the vector field $V$ has a closed orbit. I am looking for a reference that gives stability results of the following type: If the vector field $V$ satisfies properties (collectively called) A, then under small perturbations of type (collectively called) B of the vector field, the perturbed vector field would still have a closed orbit which is ""close"" in sense C to the original closed orbit. The best situation would be to find a body of results for different A, B and C. My background in dynamical systems is very rudimentary, and the prospect of reading 5 volumes to work a solution out for myself is daunting. Any help would be highly appreciated. Edit (after Robert Israel's answer): The situation I am looking at does not necessarily have a closed orbit, and now I have checked that the vector fields can indeed be tangent to the boundaries. I was wondering what we can say in such situations (I don't necessarily need existence of a closed orbit. I am more interested in the stability of such an orbit in case it exists). Also, please include a reference if you can, I really need to read the theory up myself.","Consider a vector field $V$ on an annulus $U$, say. Also, assume that the vector field $V$ has a closed orbit. I am looking for a reference that gives stability results of the following type: If the vector field $V$ satisfies properties (collectively called) A, then under small perturbations of type (collectively called) B of the vector field, the perturbed vector field would still have a closed orbit which is ""close"" in sense C to the original closed orbit. The best situation would be to find a body of results for different A, B and C. My background in dynamical systems is very rudimentary, and the prospect of reading 5 volumes to work a solution out for myself is daunting. Any help would be highly appreciated. Edit (after Robert Israel's answer): The situation I am looking at does not necessarily have a closed orbit, and now I have checked that the vector fields can indeed be tangent to the boundaries. I was wondering what we can say in such situations (I don't necessarily need existence of a closed orbit. I am more interested in the stability of such an orbit in case it exists). Also, please include a reference if you can, I really need to read the theory up myself.",,"['ordinary-differential-equations', 'reference-request', 'dynamical-systems']"
73,"Solve differential equation, $x'=x^2-2t^{-2}$","Solve differential equation,",x'=x^2-2t^{-2},Solve differential equation: $x'=x^2- \frac{2}{t^2}$ Maybe is it sth connected with homogeneous equation? I have no idea how to solve it.,Solve differential equation: $x'=x^2- \frac{2}{t^2}$ Maybe is it sth connected with homogeneous equation? I have no idea how to solve it.,,['ordinary-differential-equations']
74,"Second order differential equation, power series method","Second order differential equation, power series method",,Solve the differential equation    $$(x+2)y''-xy'+(1-x^2)y=0 ; \quad X_0=1$$   using the power series method about the point $x_0=1$. I get to this step after deriving the derivatives of the $\sum_0^\infty A_n(x-1)^n$ : $$(1+(x-1))\left[ \sum_{n=2}^{\infty} (n(n-1)A_n(x-2)^{n-2}\right]-(1+(x-1))\left[\sum_{n=1}^{\infty} A_nn(x-1)^{n-1}\right] + (1-x^2)\left(\sum_{n=0}^{\infty}A_n(x-1)^n\right)$$ My problem is I don't understand how to distribute through the last piece $(1-x^2)$ as I do for the first piece where I substituted $(1+(x-1))$ to make it possible.,Solve the differential equation    $$(x+2)y''-xy'+(1-x^2)y=0 ; \quad X_0=1$$   using the power series method about the point $x_0=1$. I get to this step after deriving the derivatives of the $\sum_0^\infty A_n(x-1)^n$ : $$(1+(x-1))\left[ \sum_{n=2}^{\infty} (n(n-1)A_n(x-2)^{n-2}\right]-(1+(x-1))\left[\sum_{n=1}^{\infty} A_nn(x-1)^{n-1}\right] + (1-x^2)\left(\sum_{n=0}^{\infty}A_n(x-1)^n\right)$$ My problem is I don't understand how to distribute through the last piece $(1-x^2)$ as I do for the first piece where I substituted $(1+(x-1))$ to make it possible.,,"['ordinary-differential-equations', 'power-series']"
75,Solution of inhomogenous ODE (4th order),Solution of inhomogenous ODE (4th order),,"Hello stackexchangers, I have an inhomogenous ODE in 4th order. This ODE is the constitutive law to describe a material by using the ""Wiechert model"" (p. 15) which is given by $p_0\sigma + p_1\frac{\text{d}\sigma}{\text{d}t} + p_2\frac{\text{d}^2\sigma}{\text{d}t^2} + p_3\frac{\text{d}^3\sigma}{\text{d}t^3} + p_4\frac{\text{d}^4\sigma}{\text{d}t^4} = q_0\epsilon + q_1\frac{\text{d}\epsilon}{\text{d}t} + q_2\frac{\text{d}^2\epsilon}{\text{d}t^2} + q_3\frac{\text{d}^3\epsilon}{\text{d}t^3} + q_4\frac{\text{d}^4\epsilon}{\text{d}t^4}$, where $p_i$ and $q_i$ a time-dependant coefficients that are known from experiments but are available in analytical form; and $\sigma$ is a time-dependant tension that is known from a previous calculation for each instant of time $t_k$ in something like a kind of table of values, like: $t_0$ : 0 $t_1$ : 0 $t_2$ : 0 $t_3$ : 0 $t_4$ : 0.017 $t_5$ : 0.030 $t_6$ : 0.039 $t_7$ : 0.046 $t_8$ : 0.052 $t_9$ : 0.055 $t_{10}$ : 0.058 $t_{11}$ : 0.060 $t_{12}$ : 0.062 $t_{13}$ : 0.063 $t_{14}$ : 0.064 $t_{15}$ : 0.065 $t_{16}$ : 0.066 $t_{k-1}$ : 0.067 $t_{k}$ : 0.068 It could also be said that there's no previous impact before $t = t_0 = 0$ and $\epsilon = 0$, $\sigma = 0$ as well as $\frac{\text{d}^n\sigma}{\text{d}t} = \frac{\text{d}^n\epsilon}{\text{d}t} = 0$ at $t = t_0 = 0$. Now I want to calculate the resulting strain $\epsilon(t_k)$ which is caused by $\sigma(t_k)$. For further calculations I need something like $\epsilon(\sigma) = \dots$ The problem is that I don't know a function to describe $\sigma(t)$ continously. And I guess that there exist no analytical solution to the equation above. That's why I am looking for an approximate solution. I know that the coefficients $q_i$ and $p_i$ are time-dependend, what makes it more difficult to find a solution. My idea now is to consider the problem in time-steps, where one time-step $t_{k-1} - t_k$ is so small that those parameters could be considered as constant. In the mentionnend script (p. 16; ""Example 8"") it is said "" It may be that the input strain function is not known as a mathematical expression, or its mathematical expression may be so complicated as to make the transform process intractable. In those cases, one may return to the differential constitutive equation and recast it in finite-difference form so as to obtain a numerical solution. [...] "" Is it somehow possible to rewrite the given 4th-order-ODE in finite-difference form? So that I can calculate an approximation of $\epsilon$? Or is there no way around numerical methods like Runge-Kutta? I hope that someone can help me ... Many thanks in advance!","Hello stackexchangers, I have an inhomogenous ODE in 4th order. This ODE is the constitutive law to describe a material by using the ""Wiechert model"" (p. 15) which is given by $p_0\sigma + p_1\frac{\text{d}\sigma}{\text{d}t} + p_2\frac{\text{d}^2\sigma}{\text{d}t^2} + p_3\frac{\text{d}^3\sigma}{\text{d}t^3} + p_4\frac{\text{d}^4\sigma}{\text{d}t^4} = q_0\epsilon + q_1\frac{\text{d}\epsilon}{\text{d}t} + q_2\frac{\text{d}^2\epsilon}{\text{d}t^2} + q_3\frac{\text{d}^3\epsilon}{\text{d}t^3} + q_4\frac{\text{d}^4\epsilon}{\text{d}t^4}$, where $p_i$ and $q_i$ a time-dependant coefficients that are known from experiments but are available in analytical form; and $\sigma$ is a time-dependant tension that is known from a previous calculation for each instant of time $t_k$ in something like a kind of table of values, like: $t_0$ : 0 $t_1$ : 0 $t_2$ : 0 $t_3$ : 0 $t_4$ : 0.017 $t_5$ : 0.030 $t_6$ : 0.039 $t_7$ : 0.046 $t_8$ : 0.052 $t_9$ : 0.055 $t_{10}$ : 0.058 $t_{11}$ : 0.060 $t_{12}$ : 0.062 $t_{13}$ : 0.063 $t_{14}$ : 0.064 $t_{15}$ : 0.065 $t_{16}$ : 0.066 $t_{k-1}$ : 0.067 $t_{k}$ : 0.068 It could also be said that there's no previous impact before $t = t_0 = 0$ and $\epsilon = 0$, $\sigma = 0$ as well as $\frac{\text{d}^n\sigma}{\text{d}t} = \frac{\text{d}^n\epsilon}{\text{d}t} = 0$ at $t = t_0 = 0$. Now I want to calculate the resulting strain $\epsilon(t_k)$ which is caused by $\sigma(t_k)$. For further calculations I need something like $\epsilon(\sigma) = \dots$ The problem is that I don't know a function to describe $\sigma(t)$ continously. And I guess that there exist no analytical solution to the equation above. That's why I am looking for an approximate solution. I know that the coefficients $q_i$ and $p_i$ are time-dependend, what makes it more difficult to find a solution. My idea now is to consider the problem in time-steps, where one time-step $t_{k-1} - t_k$ is so small that those parameters could be considered as constant. In the mentionnend script (p. 16; ""Example 8"") it is said "" It may be that the input strain function is not known as a mathematical expression, or its mathematical expression may be so complicated as to make the transform process intractable. In those cases, one may return to the differential constitutive equation and recast it in finite-difference form so as to obtain a numerical solution. [...] "" Is it somehow possible to rewrite the given 4th-order-ODE in finite-difference form? So that I can calculate an approximation of $\epsilon$? Or is there no way around numerical methods like Runge-Kutta? I hope that someone can help me ... Many thanks in advance!",,"['ordinary-differential-equations', 'discrete-mathematics', 'numerical-methods']"
76,Kinematics of gravity in a non uniform field,Kinematics of gravity in a non uniform field,,"I am a first year physics student. I am trying to figure out how to compute position in terms of time for an object falling through non uniform gravity towards the earth, and by extension towards any body. There are two formulas that can help with this: $$-{GM_E \over r^2}={d^2 r \over dt^2} $$ which is a second order differential equation. A presumably simpler approach would be to use the energy formula for velocity, that is  $$K = {GM_{E}m} {\left({1 \over r} -{1\over r_i} \right)}={1 \over2}mv^2 $$ Solving for velocity would yield the first order differential equation : $$v={dr\over dt}=\sqrt{{2GM_{E}} {\left({1 \over r} -{1\over r_i}  \right)}} $$ Apparently, however, there is no elementary function that can be used as an anti derivative for this function. I searched the internet far and wide, but I could not find any straightforward answer to this.Obviously the answer is far from straightforward, otherwise it would be taught in physics texts. However, it appears to be far too fundamental a concept to not have a known solution for. P.S. If the answer involves anything such as Lagrange multipliers and the like, if whoever answers this question doesn't mind, please provide at least a basic explanation of it, because I haven't yet taken anything beyond second year calculus. Thank you.","I am a first year physics student. I am trying to figure out how to compute position in terms of time for an object falling through non uniform gravity towards the earth, and by extension towards any body. There are two formulas that can help with this: $$-{GM_E \over r^2}={d^2 r \over dt^2} $$ which is a second order differential equation. A presumably simpler approach would be to use the energy formula for velocity, that is  $$K = {GM_{E}m} {\left({1 \over r} -{1\over r_i} \right)}={1 \over2}mv^2 $$ Solving for velocity would yield the first order differential equation : $$v={dr\over dt}=\sqrt{{2GM_{E}} {\left({1 \over r} -{1\over r_i}  \right)}} $$ Apparently, however, there is no elementary function that can be used as an anti derivative for this function. I searched the internet far and wide, but I could not find any straightforward answer to this.Obviously the answer is far from straightforward, otherwise it would be taught in physics texts. However, it appears to be far too fundamental a concept to not have a known solution for. P.S. If the answer involves anything such as Lagrange multipliers and the like, if whoever answers this question doesn't mind, please provide at least a basic explanation of it, because I haven't yet taken anything beyond second year calculus. Thank you.",,"['ordinary-differential-equations', 'physics', 'dynamical-systems']"
77,"If a differential operator $C$ factors as $AB$, then every solution of $C(y)=0$ has the form $y=y_1+y_2$ with $A(y_1)=0$ and $B(y_2)=0$","If a differential operator  factors as , then every solution of  has the form  with  and",C AB C(y)=0 y=y_1+y_2 A(y_1)=0 B(y_2)=0,"Given two constant-coefficient operators $A$ and $B$ whose characteristic polynomials have no zeros in common. Let $C = A B$. First part of question is Prove that every solution of the differential equation $C(y)=0$ has the form $y=y_1+y_2$, where $A(y_1)=0$ and $B(y_2)=0$. I proved this part using the fact that they do not have zeros in common. solution of $A$ can not be a solution of $B$ and therefore $y_1$ and $y_2$ are independent for every $y_1$ and $y_2$ Second part is Prove that the functions $y_1$ and $y_2$ are uniquely determined. That is, there is only one pair $y_1$, $y_2$. I don't really understand second part. Please help me..","Given two constant-coefficient operators $A$ and $B$ whose characteristic polynomials have no zeros in common. Let $C = A B$. First part of question is Prove that every solution of the differential equation $C(y)=0$ has the form $y=y_1+y_2$, where $A(y_1)=0$ and $B(y_2)=0$. I proved this part using the fact that they do not have zeros in common. solution of $A$ can not be a solution of $B$ and therefore $y_1$ and $y_2$ are independent for every $y_1$ and $y_2$ Second part is Prove that the functions $y_1$ and $y_2$ are uniquely determined. That is, there is only one pair $y_1$, $y_2$. I don't really understand second part. Please help me..",,"['linear-algebra', 'ordinary-differential-equations', 'polynomials']"
78,EigenFunction for $\frac{\partial f}{\partial t}+f\frac{\partial f}{\partial x} =\frac{2f^2}{x}$,EigenFunction for,\frac{\partial f}{\partial t}+f\frac{\partial f}{\partial x} =\frac{2f^2}{x},"When studying a computer vision problem I end up with a function $f(x,t)$ that satisfying  $\frac{\partial f}{\partial t}+f\frac{\partial f}{\partial x} =\frac{2f^2}{x}$. My question includes two parts: What are the solutions to the above equation in general?  I worked out three specific solutions:  (1) $ f(x,t)= \frac{x}{k-t}$. (2) $ f(x,t)= kx^2$. (3) $ f(x,t)= \frac{kx^2-x}{b+t}$ (k and b are constants). I have no idea whether there is other solutions. To resolve the equation in another point view,  we can define an operator $\Omega f=\frac{\partial f}{\partial x} - \frac{2f}{x}$. Suppose we can find the eigenfunctions of the operator $\Omega$, namly $\Omega f=\lambda f $ ($\lambda$ is constant), so that any solution $f$ can be represented by linear sum of these eigenfunctions (very similar to Schrödinger equation).  Unfortunately, no more functions other than the three above are found, though I expect that there should be some more complicated eigenfunctions, such as Fourier series. Now I am looking into wavelet theories to find a new clue. Any suggestions on the general solution to $\frac{\partial f}{\partial t}+f\frac{\partial f}{\partial x} =\frac{2f^2}{x}$, or to the eigenfunctions for operator $\Omega f=\frac{\partial f}{\partial x} - \frac{2f}{x}$?","When studying a computer vision problem I end up with a function $f(x,t)$ that satisfying  $\frac{\partial f}{\partial t}+f\frac{\partial f}{\partial x} =\frac{2f^2}{x}$. My question includes two parts: What are the solutions to the above equation in general?  I worked out three specific solutions:  (1) $ f(x,t)= \frac{x}{k-t}$. (2) $ f(x,t)= kx^2$. (3) $ f(x,t)= \frac{kx^2-x}{b+t}$ (k and b are constants). I have no idea whether there is other solutions. To resolve the equation in another point view,  we can define an operator $\Omega f=\frac{\partial f}{\partial x} - \frac{2f}{x}$. Suppose we can find the eigenfunctions of the operator $\Omega$, namly $\Omega f=\lambda f $ ($\lambda$ is constant), so that any solution $f$ can be represented by linear sum of these eigenfunctions (very similar to Schrödinger equation).  Unfortunately, no more functions other than the three above are found, though I expect that there should be some more complicated eigenfunctions, such as Fourier series. Now I am looking into wavelet theories to find a new clue. Any suggestions on the general solution to $\frac{\partial f}{\partial t}+f\frac{\partial f}{\partial x} =\frac{2f^2}{x}$, or to the eigenfunctions for operator $\Omega f=\frac{\partial f}{\partial x} - \frac{2f}{x}$?",,"['ordinary-differential-equations', 'partial-differential-equations', 'fourier-analysis', 'eigenfunctions']"
79,Continuum Approach to Modeling Cell Proliferation and Differentiation (PDE),Continuum Approach to Modeling Cell Proliferation and Differentiation (PDE),,"I'm new here so I hope this post is appropriate. I recently read in a bioengineering textbook about an approach to model cell proliferation and differentiation. They proposed the following partial differential equation: $$ \frac{\partial X}{\partial t} +  \delta  \frac{\partial X}{\partial \lambda} =  \big( \mu - \alpha \big) X $$ $X$ is number of cells. $t$ is time. $\lambda$ is the differentiation state, and can only have values between 0 and 1, which I will discuss below. $\delta$, $\mu$, and $\alpha$ are differentiation rate, proliferation rate, and death rate, respectively, and have units of inverse time. I'll assume they are all constant for the time being. I would be fine in solving this partial differential equation except for my interpretation of $\lambda$. In the population X, there are two kinds of cells, which I will call A and B. A transforms into B. Their sum is X. $$A + B = X$$ If $\lambda = 0$, then X is entirely composed of A. If $\lambda = 1$, then X is entirely composed of B. That is all my textbook tells me, but I would like to understand this further. From my understanding, $\lambda$ could therefore be defined as: $$\lambda = \frac{B}{X} = \frac{B}{A+B} $$ At first glance to me, $X$ should be a function of $t$ and $\lambda$, but if I'm saying that $\lambda$ is a function of A and B, does that mean that $X$ is only a function of time $t$? Another question I have is about the implications of $\delta$. Does $\delta$ simply imply a transformation, or transformation and increase in numbers, based solely on the differentiation equation above. I hope this question is not too biological for this forum. Thank you.","I'm new here so I hope this post is appropriate. I recently read in a bioengineering textbook about an approach to model cell proliferation and differentiation. They proposed the following partial differential equation: $$ \frac{\partial X}{\partial t} +  \delta  \frac{\partial X}{\partial \lambda} =  \big( \mu - \alpha \big) X $$ $X$ is number of cells. $t$ is time. $\lambda$ is the differentiation state, and can only have values between 0 and 1, which I will discuss below. $\delta$, $\mu$, and $\alpha$ are differentiation rate, proliferation rate, and death rate, respectively, and have units of inverse time. I'll assume they are all constant for the time being. I would be fine in solving this partial differential equation except for my interpretation of $\lambda$. In the population X, there are two kinds of cells, which I will call A and B. A transforms into B. Their sum is X. $$A + B = X$$ If $\lambda = 0$, then X is entirely composed of A. If $\lambda = 1$, then X is entirely composed of B. That is all my textbook tells me, but I would like to understand this further. From my understanding, $\lambda$ could therefore be defined as: $$\lambda = \frac{B}{X} = \frac{B}{A+B} $$ At first glance to me, $X$ should be a function of $t$ and $\lambda$, but if I'm saying that $\lambda$ is a function of A and B, does that mean that $X$ is only a function of time $t$? Another question I have is about the implications of $\delta$. Does $\delta$ simply imply a transformation, or transformation and increase in numbers, based solely on the differentiation equation above. I hope this question is not too biological for this forum. Thank you.",,"['ordinary-differential-equations', 'partial-differential-equations', 'mathematical-modeling', 'biology']"
80,Fundamental Matrices for Linear ODE,Fundamental Matrices for Linear ODE,,"Why is the following statement true?: For a matrix ODE: $\mathbf{x'=Ax}$ with special fundamental matrix, $\Phi (t)$ or $e^{\mathbf{A}}$, where $\Phi(t_0) = I$, and fundamental matrix containing the general solution, $\Psi (t)$, the following is true: $\Psi(t)$ =$\Phi (t) T$, where T contains columns that are linearly independent eigenvectors of $\mathbf{A}$. This equation comes from the statements: $e^\mathbf{A} =TQ(t)T^{-1}$ and $\Psi (t) = TQ$, where Q is a diagonal matrix with $e^{\lambda t}$ along its diagonal, where the $\lambda$ are the eigenvalues of $\mathbf{A}$.  In addition, it seems to conclude, by the process of diagonalization of a matrix, that the eigenvectors/values of $\mathbf{A}$ are the same as those of $e^{\mathbf{A}}$. Thanks.","Why is the following statement true?: For a matrix ODE: $\mathbf{x'=Ax}$ with special fundamental matrix, $\Phi (t)$ or $e^{\mathbf{A}}$, where $\Phi(t_0) = I$, and fundamental matrix containing the general solution, $\Psi (t)$, the following is true: $\Psi(t)$ =$\Phi (t) T$, where T contains columns that are linearly independent eigenvectors of $\mathbf{A}$. This equation comes from the statements: $e^\mathbf{A} =TQ(t)T^{-1}$ and $\Psi (t) = TQ$, where Q is a diagonal matrix with $e^{\lambda t}$ along its diagonal, where the $\lambda$ are the eigenvalues of $\mathbf{A}$.  In addition, it seems to conclude, by the process of diagonalization of a matrix, that the eigenvectors/values of $\mathbf{A}$ are the same as those of $e^{\mathbf{A}}$. Thanks.",,"['linear-algebra', 'ordinary-differential-equations']"
81,$f(b)-f(a) =((b-a)/2)\cdot(f'(a)+f'(b))-((b-a)³/12)\cdot f'''(c)$ [duplicate],[duplicate],f(b)-f(a) =((b-a)/2)\cdot(f'(a)+f'(b))-((b-a)³/12)\cdot f'''(c),"This question already has an answer here : Approximating a three-times differentiable function by a linear combination of derivatives (1 answer) Closed 9 years ago . Let $f$ be three times differentiable on $[a,b]$. $f'''$ is continuous. Show that there is a $c\in[a,b]$ such that $$f(b)-f(a) =((b-a)/2)\cdot(f'(a)+f'(b))-((b-a)³/12)\cdot f'''(c)$$ This looks like a combination of Mean Value Theorem and Taylor's series. but how to solve it, i don't know. Thanks for any answers!","This question already has an answer here : Approximating a three-times differentiable function by a linear combination of derivatives (1 answer) Closed 9 years ago . Let $f$ be three times differentiable on $[a,b]$. $f'''$ is continuous. Show that there is a $c\in[a,b]$ such that $$f(b)-f(a) =((b-a)/2)\cdot(f'(a)+f'(b))-((b-a)³/12)\cdot f'''(c)$$ This looks like a combination of Mean Value Theorem and Taylor's series. but how to solve it, i don't know. Thanks for any answers!",,"['calculus', 'real-analysis', 'ordinary-differential-equations', 'multivariable-calculus']"
82,"What is the ""Enstrophy Miracle""?","What is the ""Enstrophy Miracle""?",,"I read that one of the main differences between the establishing global regularity for the Navier-Stokes equations for viscous and incompressible flow in 2 spatial dimensions versus in 3 is that in 2D there is something known as the enstrophy miracle which is given as (getting this straight from arxiv), $$\int_{\Omega_{\epsilon}}\left(f\left(u\cdot\nabla f\right)\right)\,dV = 0$$ What does this mean? How does this come about? Please respond as if you're responding to somebody whose only familiar with undergraduate vector calculus, linear algebra, differential equations, and a bit of fluid dynamics and real analysis.","I read that one of the main differences between the establishing global regularity for the Navier-Stokes equations for viscous and incompressible flow in 2 spatial dimensions versus in 3 is that in 2D there is something known as the enstrophy miracle which is given as (getting this straight from arxiv), What does this mean? How does this come about? Please respond as if you're responding to somebody whose only familiar with undergraduate vector calculus, linear algebra, differential equations, and a bit of fluid dynamics and real analysis.","\int_{\Omega_{\epsilon}}\left(f\left(u\cdot\nabla f\right)\right)\,dV = 0","['real-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'fluid-dynamics']"
83,the exact graph of the general solution for $x'=\begin{bmatrix} 1 & 1\\ 4& 1 \end{bmatrix}x$,the exact graph of the general solution for,x'=\begin{bmatrix} 1 & 1\\ 4& 1 \end{bmatrix}x,"i need someone to give me exact graph  of  the general solution for $$x'=\begin{bmatrix} 1 & 1\\   4& 1 \end{bmatrix}x$$ i solved it manually , the general solution is like this $$x(t)=c_1\begin{pmatrix} 1\\  2 \end{pmatrix} e^{3t} +c_1\begin{pmatrix} 1\\  -2 \end{pmatrix} e^{-t} $$ and the graph will be something like this http://prntscr.com/52xmbe I'll appreciate any help , thanks for advance","i need someone to give me exact graph  of  the general solution for $$x'=\begin{bmatrix} 1 & 1\\   4& 1 \end{bmatrix}x$$ i solved it manually , the general solution is like this $$x(t)=c_1\begin{pmatrix} 1\\  2 \end{pmatrix} e^{3t} +c_1\begin{pmatrix} 1\\  -2 \end{pmatrix} e^{-t} $$ and the graph will be something like this http://prntscr.com/52xmbe I'll appreciate any help , thanks for advance",,"['linear-algebra', 'ordinary-differential-equations']"
84,Dixmier Conjecture,Dixmier Conjecture,,"In algebra the Dixmier conjecture, asked by Jacques Dixmier in 1968, is the conjecture that any endomorphism of a Weyl algebra is an automorphism. Which are some consequences of Dixmier conjecture which are different from Jacobian conjecture [ link ] ?","In algebra the Dixmier conjecture, asked by Jacques Dixmier in 1968, is the conjecture that any endomorphism of a Weyl algebra is an automorphism. Which are some consequences of Dixmier conjecture which are different from Jacobian conjecture [ link ] ?",,"['ordinary-differential-equations', 'ring-theory']"
85,Differential Equation (Advertising Model),Differential Equation (Advertising Model),,"In the sales response to advertising model given by $S'=-(a+ rA(t)/M)S+ rA(t)$, where a,M and r are constants. Assume that $S(0) = S_0$ and that advertising is constant A over a fixed time period T , and is then removed. That is, $A(t) =  A$ when $0 ≤ t ≤ T$  and $A(t)= 0, t > T $ Find a formula for the sales $S(t)$ for every time t. I'm a little bit lost since A is defined in two intervals.","In the sales response to advertising model given by $S'=-(a+ rA(t)/M)S+ rA(t)$, where a,M and r are constants. Assume that $S(0) = S_0$ and that advertising is constant A over a fixed time period T , and is then removed. That is, $A(t) =  A$ when $0 ≤ t ≤ T$  and $A(t)= 0, t > T $ Find a formula for the sales $S(t)$ for every time t. I'm a little bit lost since A is defined in two intervals.",,['ordinary-differential-equations']
86,Loss of stability (unphysical energy gain) for simple pendulum equation?,Loss of stability (unphysical energy gain) for simple pendulum equation?,,"I am simulating a pendulum using MATLAB and noted a curious issue. When I use zero velocity and (pi - 0.1) angular position as starting conditions for my second order ODE, the solution deviates from what physically and analytically expected after some short time (the pendulum gains energy, after Swinging back and forth it starts revolving). The trivial code I used is as follows f1 = @(t,Y) [Y(2); -sin(Y(1))]; [z1,z2] = ode45(f1,[0:0.001:80],[ pi - 0.1,0]) plot(z1,z2(:,1)) Using ODE15s actually makes the phenomenon to occur earlier. Any hint on what I am doing wrong? What is causing all this? Thanks a lot","I am simulating a pendulum using MATLAB and noted a curious issue. When I use zero velocity and (pi - 0.1) angular position as starting conditions for my second order ODE, the solution deviates from what physically and analytically expected after some short time (the pendulum gains energy, after Swinging back and forth it starts revolving). The trivial code I used is as follows f1 = @(t,Y) [Y(2); -sin(Y(1))]; [z1,z2] = ode45(f1,[0:0.001:80],[ pi - 0.1,0]) plot(z1,z2(:,1)) Using ODE15s actually makes the phenomenon to occur earlier. Any hint on what I am doing wrong? What is causing all this? Thanks a lot",,['ordinary-differential-equations']
87,List of ODE's that can be solved by Fourier transform,List of ODE's that can be solved by Fourier transform,,"I am teaching introductory level Fourier analysis and I want to give my students some basic and some not so basic examples of how to solve ordinary differential equations with the method of Fourier transform. Looking at various sources there seems to be basically just one instance $u''-u=f$ where the method is applicable (the other examples seem to be derived from it. Since there also other methods to deal with these examples and the students may be unconvinced what they actually gain by using this method, I wonder where could one find a list of ODE's (maybe with non-constant coefficients but  not PDE's) which are solvable (at least in principle) precisely by the method of Fourier transform (not Laplace transform, Fourier series etc., I know these are related, yet my question is a particular one). Preferably the list should contain cases when distribution theory is either needed or is avoided.","I am teaching introductory level Fourier analysis and I want to give my students some basic and some not so basic examples of how to solve ordinary differential equations with the method of Fourier transform. Looking at various sources there seems to be basically just one instance $u''-u=f$ where the method is applicable (the other examples seem to be derived from it. Since there also other methods to deal with these examples and the students may be unconvinced what they actually gain by using this method, I wonder where could one find a list of ODE's (maybe with non-constant coefficients but  not PDE's) which are solvable (at least in principle) precisely by the method of Fourier transform (not Laplace transform, Fourier series etc., I know these are related, yet my question is a particular one). Preferably the list should contain cases when distribution theory is either needed or is avoided.",,"['ordinary-differential-equations', 'fourier-analysis']"
88,Existence of Periodic Solution,Existence of Periodic Solution,,"I'm working with the system of equations below that represents a Pendulum with constant forcing.  \begin{align*} \theta'&=v\\ v'&=-bv-\sin(\theta)+k \end{align*} Where $\theta$ gives the angular position, $v$ is the velocity, $b>0$ measures the damping force and $k\geq0$ the applied torque. The question ask, Suppose $k>1$ Prove that there exist a periodic solution for this system. My original idea was to use the Poincaré-Bendixon theorem in our book which says that if $\Omega$ is a closed, and bounded limit set of a planar system of differential equations that contains no equilibrium point. Then $\Omega$ is a closed orbit. If we look at the $v$ nullcline, we can see that the vector field in the region above (say when $v=v_2$) points downwards. Above (say when $v=v_1$) it points upwards. We also know that when $k>1$ there are no longer any equilibrium points. But...this version of the Poincaré-Bendixon theorem is used for planar systems, where I guess in this problem we are working on a cylinder. Is there another way to look at this question, or a different theorem to use. Any help would be great!","I'm working with the system of equations below that represents a Pendulum with constant forcing.  \begin{align*} \theta'&=v\\ v'&=-bv-\sin(\theta)+k \end{align*} Where $\theta$ gives the angular position, $v$ is the velocity, $b>0$ measures the damping force and $k\geq0$ the applied torque. The question ask, Suppose $k>1$ Prove that there exist a periodic solution for this system. My original idea was to use the Poincaré-Bendixon theorem in our book which says that if $\Omega$ is a closed, and bounded limit set of a planar system of differential equations that contains no equilibrium point. Then $\Omega$ is a closed orbit. If we look at the $v$ nullcline, we can see that the vector field in the region above (say when $v=v_2$) points downwards. Above (say when $v=v_1$) it points upwards. We also know that when $k>1$ there are no longer any equilibrium points. But...this version of the Poincaré-Bendixon theorem is used for planar systems, where I guess in this problem we are working on a cylinder. Is there another way to look at this question, or a different theorem to use. Any help would be great!",,"['ordinary-differential-equations', 'dynamical-systems', 'systems-of-equations']"
89,Solve the following ODE,Solve the following ODE,,Solve the following ODE $$(y-x)\left(1+x^2 \right)^{\frac{1}{2}}\dfrac{\mathrm{d}y}{\mathrm{d}x}=n\left(1+y^2 \right)^{\frac{3}{2}}$$ I have tried substituting $y=\tan \theta$ and $x=\tan \phi$ but can't do it. Any help is appreciated.,Solve the following ODE $$(y-x)\left(1+x^2 \right)^{\frac{1}{2}}\dfrac{\mathrm{d}y}{\mathrm{d}x}=n\left(1+y^2 \right)^{\frac{3}{2}}$$ I have tried substituting $y=\tan \theta$ and $x=\tan \phi$ but can't do it. Any help is appreciated.,,[]
90,How to define a Holder seminorm of a section,How to define a Holder seminorm of a section,,"I'm reading ""Variational Problems in Geometry"",Seiki Nishikawa, in the figure below. Let $(M,g)$ be a compact $m$ dimensional Riemannian manifold with no boundary. $T>0, 0<\alpha<1, Q:=M\times[0,T]$. For a sufficiently differentiable function $u:Q\to\mathbb{R}^q$, we want to give a quantity $|u|^{(2+\alpha, 1+\frac{\alpha}{2})}_Q$ as (4.16) below. But I cannot understand what is \begin{eqnarray} \langle D^2_x u\rangle^{(\alpha)}_x \end{eqnarray} . Please tell me how to define the quantity above in usual. Thanks a lot. In the image below, the covariant derivative $\nabla$ denotes the Levi-Civita connection w.r.t. the Riemannian metric $g$.","I'm reading ""Variational Problems in Geometry"",Seiki Nishikawa, in the figure below. Let $(M,g)$ be a compact $m$ dimensional Riemannian manifold with no boundary. $T>0, 0<\alpha<1, Q:=M\times[0,T]$. For a sufficiently differentiable function $u:Q\to\mathbb{R}^q$, we want to give a quantity $|u|^{(2+\alpha, 1+\frac{\alpha}{2})}_Q$ as (4.16) below. But I cannot understand what is \begin{eqnarray} \langle D^2_x u\rangle^{(\alpha)}_x \end{eqnarray} . Please tell me how to define the quantity above in usual. Thanks a lot. In the image below, the covariant derivative $\nabla$ denotes the Levi-Civita connection w.r.t. the Riemannian metric $g$.",,"['analysis', 'functional-analysis', 'ordinary-differential-equations', 'differential-geometry', 'definition']"
91,Ordinary Differential Equation rewriting,Ordinary Differential Equation rewriting,,"I've a certainly stupid question. I have the following dynamical system of autonomous ODEs. \begin{align} \frac{dx_1}{dt}=f(x_1,x_2) \\ \frac{dx_2}{dt}=g(x_1,x_2)  \end{align} May I change the system to: $$ \frac{dx_1}{dx_2} = \frac{f(x_1,x_2)}{g(x_1,x_2)} $$ Thank you","I've a certainly stupid question. I have the following dynamical system of autonomous ODEs. \begin{align} \frac{dx_1}{dt}=f(x_1,x_2) \\ \frac{dx_2}{dt}=g(x_1,x_2)  \end{align} May I change the system to: $$ \frac{dx_1}{dx_2} = \frac{f(x_1,x_2)}{g(x_1,x_2)} $$ Thank you",,['ordinary-differential-equations']
92,Math software for plotting phase portraits,Math software for plotting phase portraits,,"I'm looking for math software which is possible to plot phase portraits for ODE and systems of differential equations. Is there a software which can create not only simple 2D phase portrait plots but also 3D or even $n$D (with $n>3$) ones? 2D are welcome, too.","I'm looking for math software which is possible to plot phase portraits for ODE and systems of differential equations. Is there a software which can create not only simple 2D phase portrait plots but also 3D or even $n$D (with $n>3$) ones? 2D are welcome, too.",,"['ordinary-differential-equations', 'graphing-functions', 'systems-of-equations', 'big-list', 'math-software']"
93,2nd order odes with non constant coeff.,2nd order odes with non constant coeff.,,I am trying to solve these two DE: $ y''+(2x)/(1+x^2)y'+1/(1+x^2)^2y=0 $ and $ xy''-y'-4x^3y=0 $ and I am looking for methods on how to find the solutions. Should I go with the series method or is there a simpler way?,I am trying to solve these two DE: $ y''+(2x)/(1+x^2)y'+1/(1+x^2)^2y=0 $ and $ xy''-y'-4x^3y=0 $ and I am looking for methods on how to find the solutions. Should I go with the series method or is there a simpler way?,,['ordinary-differential-equations']
94,Why can't solutions to Autonomous ODE intersect?,Why can't solutions to Autonomous ODE intersect?,,"Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^n $ be continuously differentiable.  Why can't solutions to $x'=f(x) $ intersect? I use a proof by contradiction: Assume solutions can, and do intersect at the point $(t_0,y_0)$.  Then, the IVP $ x'=f(x)\quad x(t_0)=y_0$ does not have a unique solution. Since $f$ is continuously differentiable, it is Lipschitz on any interval containing $t_0$. Thus, by Picard's Existence Theorem, a unique solution exists, which contradicts our assumption. How does this proof look?  Is there a better way to do it?","Let $f: \mathbb{R}^n \rightarrow \mathbb{R}^n $ be continuously differentiable.  Why can't solutions to $x'=f(x) $ intersect? I use a proof by contradiction: Assume solutions can, and do intersect at the point $(t_0,y_0)$.  Then, the IVP $ x'=f(x)\quad x(t_0)=y_0$ does not have a unique solution. Since $f$ is continuously differentiable, it is Lipschitz on any interval containing $t_0$. Thus, by Picard's Existence Theorem, a unique solution exists, which contradicts our assumption. How does this proof look?  Is there a better way to do it?",,"['real-analysis', 'ordinary-differential-equations']"
95,legendre's polynomial problem,legendre's polynomial problem,,"Let $y$ be a polynomial solution of the differential equation $$(1-x^2)y^{''}-2xy^{'}+6y=0$$ if $y(1)=2$, then the value of the integral $\displaystyle \int_{-1}^{1}e^{y^2}dx$ is? how to calculate this?","Let $y$ be a polynomial solution of the differential equation $$(1-x^2)y^{''}-2xy^{'}+6y=0$$ if $y(1)=2$, then the value of the integral $\displaystyle \int_{-1}^{1}e^{y^2}dx$ is? how to calculate this?",,['ordinary-differential-equations']
96,Second order linear ODE question,Second order linear ODE question,,"I am working on this equation: $ x^4y''+2x^3y'+y=0$ and i need a little help. Should I substitute y=exp(integrate(u)dx), and transform given equation into Riccati's: $ u'=-u^2-2u/x-1/x^4 $ (but i dont know know how to solve this either) or is there any other way? Thanks for tips and help","I am working on this equation: $ x^4y''+2x^3y'+y=0$ and i need a little help. Should I substitute y=exp(integrate(u)dx), and transform given equation into Riccati's: $ u'=-u^2-2u/x-1/x^4 $ (but i dont know know how to solve this either) or is there any other way? Thanks for tips and help",,['ordinary-differential-equations']
97,"Finding the general solution to system of linear equations: $y' = 2y,y''=4y-y'$",Finding the general solution to system of linear equations:,"y' = 2y,y''=4y-y'","Question: I want to find the general solution to the following system:   $\begin{pmatrix} \dot{y}_1 \\ \dot{y}_2 \end{pmatrix} = \begin{pmatrix} 2&0\\4&-1 \end{pmatrix}\begin{pmatrix}y_1\\y_2\end{pmatrix}$ I find working in questions makes them look cluttered and noone helps. They are hidden below. I believe I am meant to find the eigenvalues and eigenvectors here. Since we have a zero on the opposite diagonal, we know that the eigenvalues are $2$ and $-1$, and the eigenvectors corresponding to these turned out to be $(3,4)^T$ and $(0,\gamma)^T$. cont. I am fairly sure I am meant to use the form $\vec{y}= \alpha \vec{x}^{(1)} e^{\lambda_1 t} + \beta \vec{x}^{(2)} e^{\lambda_2 t}$ Attempt solution Hence it should equal $\alpha \begin{pmatrix} 3 \\ 4 \end{pmatrix} e^{2t}+  \beta \begin{pmatrix} 0 \\ \gamma \end{pmatrix} e^{-t} $, but I am not sure this is right.","Question: I want to find the general solution to the following system:   $\begin{pmatrix} \dot{y}_1 \\ \dot{y}_2 \end{pmatrix} = \begin{pmatrix} 2&0\\4&-1 \end{pmatrix}\begin{pmatrix}y_1\\y_2\end{pmatrix}$ I find working in questions makes them look cluttered and noone helps. They are hidden below. I believe I am meant to find the eigenvalues and eigenvectors here. Since we have a zero on the opposite diagonal, we know that the eigenvalues are $2$ and $-1$, and the eigenvectors corresponding to these turned out to be $(3,4)^T$ and $(0,\gamma)^T$. cont. I am fairly sure I am meant to use the form $\vec{y}= \alpha \vec{x}^{(1)} e^{\lambda_1 t} + \beta \vec{x}^{(2)} e^{\lambda_2 t}$ Attempt solution Hence it should equal $\alpha \begin{pmatrix} 3 \\ 4 \end{pmatrix} e^{2t}+  \beta \begin{pmatrix} 0 \\ \gamma \end{pmatrix} e^{-t} $, but I am not sure this is right.",,"['real-analysis', 'ordinary-differential-equations']"
98,General Solution of DE?,General Solution of DE?,,"I've got the following ODE, and I'm just having trouble coming up with the form of the general solution.  I'm really trying to find the particular solution, but in order to do that, I need to know the homogeneous solution. Here's the problem: $$ y' =  \begin{bmatrix}   3 & 1 & -1 \\   3 & 5 & 1 \\   -6 & 2 & 4  \end{bmatrix} y + \begin{bmatrix}   3\\   6\\   8  \end{bmatrix} $$ I know that my eigenvalues are r_1 = 0 and r_2=6 and r_3 = 6 My eigenvectors should be: $$ v_1 = \begin{bmatrix}   1\\   -1\\   2  \end{bmatrix}      v_2 =\begin{bmatrix}   1\\   3\\   0  \end{bmatrix}  v_3 = \begin{bmatrix}   -1\\   0\\   3  \end{bmatrix} $$ This means my homogeneous solution should take the form: $$ y(t) = c_1\begin{bmatrix}   1\\   -1\\   2  \end{bmatrix} + c_2\begin{bmatrix}   1\\   3\\   0  \end{bmatrix}e^{6t} + c_3(\begin{bmatrix}   -1\\   0\\   3  \end{bmatrix}e^{6t} +\begin{bmatrix}   1\\   3\\   0  \end{bmatrix}te^{6t}) $$ Can anyone confirm this for me?  I know that I'm right about the eigenvalues, but wasn't sure if my homogeneous component was in the correct form? Thanks!","I've got the following ODE, and I'm just having trouble coming up with the form of the general solution.  I'm really trying to find the particular solution, but in order to do that, I need to know the homogeneous solution. Here's the problem: $$ y' =  \begin{bmatrix}   3 & 1 & -1 \\   3 & 5 & 1 \\   -6 & 2 & 4  \end{bmatrix} y + \begin{bmatrix}   3\\   6\\   8  \end{bmatrix} $$ I know that my eigenvalues are r_1 = 0 and r_2=6 and r_3 = 6 My eigenvectors should be: $$ v_1 = \begin{bmatrix}   1\\   -1\\   2  \end{bmatrix}      v_2 =\begin{bmatrix}   1\\   3\\   0  \end{bmatrix}  v_3 = \begin{bmatrix}   -1\\   0\\   3  \end{bmatrix} $$ This means my homogeneous solution should take the form: $$ y(t) = c_1\begin{bmatrix}   1\\   -1\\   2  \end{bmatrix} + c_2\begin{bmatrix}   1\\   3\\   0  \end{bmatrix}e^{6t} + c_3(\begin{bmatrix}   -1\\   0\\   3  \end{bmatrix}e^{6t} +\begin{bmatrix}   1\\   3\\   0  \end{bmatrix}te^{6t}) $$ Can anyone confirm this for me?  I know that I'm right about the eigenvalues, but wasn't sure if my homogeneous component was in the correct form? Thanks!",,"['ordinary-differential-equations', 'eigenvalues-eigenvectors']"
99,finding solution to a partial integro differential equation [closed],finding solution to a partial integro differential equation [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question I want to find a function (or a set of functions) such that  $u(x,t)$ satisfies the following partial integro-differential equation with singular kernel \begin{eqnarray} &&u_x(0,t) = \int_0^t \frac{u_\lambda(0,\lambda)}{\sqrt{t-\lambda}} d\lambda, \\  &&u(0,0) = 0. \end{eqnarray} I tried the Laplace transform method but i couldn't find the solution. Some help would be greatly appreciated.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 months ago . Improve this question I want to find a function (or a set of functions) such that  $u(x,t)$ satisfies the following partial integro-differential equation with singular kernel \begin{eqnarray} &&u_x(0,t) = \int_0^t \frac{u_\lambda(0,\lambda)}{\sqrt{t-\lambda}} d\lambda, \\  &&u(0,0) = 0. \end{eqnarray} I tried the Laplace transform method but i couldn't find the solution. Some help would be greatly appreciated.",,"['ordinary-differential-equations', 'partial-differential-equations', 'integral-equations', 'integral-transforms', 'integro-differential-equations']"
