,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Does there exist a non-trivial, associative Lie algebra?","Does there exist a non-trivial, associative Lie algebra?",,"In a Linear Algebra text I'm reading the author states that a Lie Algebra $K$ is non-associative unless we have $[x, y] = 0$ for all $x, y \in K$ (i.e, if $[v, [u, w]] = [[v, u], w]$ for all $v, u, w \in K$, then $[x, y] = 0$ for all $x, y \in K$). Is this true? Because I don't see it. By the way, the text is 'The Linear Algebra a Beginning Graduate Student Ought to Know' by Johnathan S. Golan. Thanks.","In a Linear Algebra text I'm reading the author states that a Lie Algebra $K$ is non-associative unless we have $[x, y] = 0$ for all $x, y \in K$ (i.e, if $[v, [u, w]] = [[v, u], w]$ for all $v, u, w \in K$, then $[x, y] = 0$ for all $x, y \in K$). Is this true? Because I don't see it. By the way, the text is 'The Linear Algebra a Beginning Graduate Student Ought to Know' by Johnathan S. Golan. Thanks.",,"['linear-algebra', 'abstract-algebra', 'lie-algebras']"
1,"Are ""constrained linear least squares"" and ""quadratic programming"" the same thing?","Are ""constrained linear least squares"" and ""quadratic programming"" the same thing?",,"A Quadratic Programming problem is to minimize: $f(\mathbf{x}) = \tfrac{1}{2} \mathbf{x}^T Q\mathbf{x} + \mathbf{c}^T \mathbf{x}$ subject to $A\mathbf{x} \leq \mathbf b$; $C\mathbf{x} = \mathbf d$; and $ \mathbf{s} \leq \mathbf{x} \leq \mathbf t$ and $Q$ is symmetric. A Constrained Linear Least Squares problem is to minimize: $\frac{1}{2}| Q\mathbf{x} - \mathbf{c}|_2^2$ subject to $A\mathbf{x} \leq \mathbf b$; $C\mathbf{x} = \mathbf d$; and $ \mathbf{s} \leq \mathbf{x} \leq \mathbf t$. Matlab has two different functions for solving these, quadprog and lsqlin , hinting that these are different problems; but they seem like the same thing under the hood. Could someone explain whether these are the same problem, in particular is it correct to describe a ""Constrained Linear Least Squares"" problem as a ""Quadratic Programming"" problem? If not, what is an example of a problem expressible in one form but not the other?","A Quadratic Programming problem is to minimize: $f(\mathbf{x}) = \tfrac{1}{2} \mathbf{x}^T Q\mathbf{x} + \mathbf{c}^T \mathbf{x}$ subject to $A\mathbf{x} \leq \mathbf b$; $C\mathbf{x} = \mathbf d$; and $ \mathbf{s} \leq \mathbf{x} \leq \mathbf t$ and $Q$ is symmetric. A Constrained Linear Least Squares problem is to minimize: $\frac{1}{2}| Q\mathbf{x} - \mathbf{c}|_2^2$ subject to $A\mathbf{x} \leq \mathbf b$; $C\mathbf{x} = \mathbf d$; and $ \mathbf{s} \leq \mathbf{x} \leq \mathbf t$. Matlab has two different functions for solving these, quadprog and lsqlin , hinting that these are different problems; but they seem like the same thing under the hood. Could someone explain whether these are the same problem, in particular is it correct to describe a ""Constrained Linear Least Squares"" problem as a ""Quadratic Programming"" problem? If not, what is an example of a problem expressible in one form but not the other?",,"['linear-algebra', 'matlab', 'quadratic-programming']"
2,What's an example of a vector space that doesn't have a basis if we don't accept Choice?,What's an example of a vector space that doesn't have a basis if we don't accept Choice?,,"I've read that the fact that all vector spaces have a basis is dependent on the axiom of choice, I'd like to see an example of a vector space that doesn't have a basis if we don't accept AoC. I'm also interested in knowing why this happens. Thanks!","I've read that the fact that all vector spaces have a basis is dependent on the axiom of choice, I'd like to see an example of a vector space that doesn't have a basis if we don't accept AoC. I'm also interested in knowing why this happens. Thanks!",,"['linear-algebra', 'vector-spaces', 'set-theory', 'axiom-of-choice', 'hamel-basis']"
3,What's the use of quadratic forms?,What's the use of quadratic forms?,,"Starting with the abstract concept of a vector space, I can see why we'd want to add some structure to be able to perform useful operations.  For instance if we add a metric/ norm to a vector space we can talk about distances.  If we add an inner product to a vector space we can talk about angles.  These two operations also give us a bunch of inequalities (like Cauchy-Schwarz) that we get as well. But I don't see the point of equipping our vector space with some degree two polynomial.  What does that get us?  Is there some geometric meaning to it (like how we got distance and angle from norm and inner product)?","Starting with the abstract concept of a vector space, I can see why we'd want to add some structure to be able to perform useful operations.  For instance if we add a metric/ norm to a vector space we can talk about distances.  If we add an inner product to a vector space we can talk about angles.  These two operations also give us a bunch of inequalities (like Cauchy-Schwarz) that we get as well. But I don't see the point of equipping our vector space with some degree two polynomial.  What does that get us?  Is there some geometric meaning to it (like how we got distance and angle from norm and inner product)?",,"['linear-algebra', 'vector-spaces', 'quadratic-forms']"
4,Change of basis = similarity?,Change of basis = similarity?,,"Today, the teacher in my class said that any similarity transform of a matrix is essentially a change of basis. So as a result, we end up with the same transformation, just with respect to a different basis. I did not follow that, could someone please explain the intuition behind how multiplying a matrix by $S$ from the left and $S^{-1}$ produces the same transformation but wrt to a different basis?","Today, the teacher in my class said that any similarity transform of a matrix is essentially a change of basis. So as a result, we end up with the same transformation, just with respect to a different basis. I did not follow that, could someone please explain the intuition behind how multiplying a matrix by $S$ from the left and $S^{-1}$ produces the same transformation but wrt to a different basis?",,['linear-algebra']
5,Span of permutation matrices,Span of permutation matrices,,"The set $P$ of $n \times n$ permutation matrices spans a subspace of dimension $(n-1)^2+1$ within, say, the $n \times n$ complex matrices.  Is there another description of this space?  In particular, I am interested in a description of a subset of the permutation matrices which will form a basis. For $n=1$ and $2$ , this is completely trivial -- the set of all permutation matrices is linearly independent.  For $n=3$ , the dimension of their span is $5$ , and any five of the six permutation matrices are linearly independent, as can be seen from the following dependence relation: $$ \sum_{M \in P} \det (M) \ M = 0 $$ So even in the case $n=4$ , is there a natural description of a $10$ matrix basis?","The set of permutation matrices spans a subspace of dimension within, say, the complex matrices.  Is there another description of this space?  In particular, I am interested in a description of a subset of the permutation matrices which will form a basis. For and , this is completely trivial -- the set of all permutation matrices is linearly independent.  For , the dimension of their span is , and any five of the six permutation matrices are linearly independent, as can be seen from the following dependence relation: So even in the case , is there a natural description of a matrix basis?",P n \times n (n-1)^2+1 n \times n n=1 2 n=3 5  \sum_{M \in P} \det (M) \ M = 0  n=4 10,"['linear-algebra', 'matrices', 'permutations', 'permutation-matrices']"
6,Isometry group of a norm is always contained in some Isometry group of an inner product?,Isometry group of a norm is always contained in some Isometry group of an inner product?,,"$\newcommand{\<}{\langle} \newcommand{\>}{\rangle} $Let $||\cdot||$ be a norm on a finite dimensional real vector space $V$. Does there always exist some inner product $\<,\>$ on $V$ such that $\text{ISO}(|| \cdot ||)\subseteq \text{ISO}(\<,\>)$ ? Update: As pointed by Qiaochu Yuan the answer is positive. This raises the question of uniqueness of the inner product $\<,\>$ which satisfies $\text{ISO}(|| \cdot ||)\subseteq \text{ISO}(\<,\>)$. Is it unique (up to scalar multiple)? Remarks: 1) Determining $\<,\>$ (up to scalar multiple) is equivalent to determining $\text{ISO}(\<,\>)$. Clearly if we know the inner product we know all its isometries. The other direction follows as a corollary from an argument given here which shows which inner products are preserved by a given automorphism. 2) Since there are ""rigid"" norms (whose only isometries are $\pm Id$ ) the uniqueness certainly doesn't hold in general. One could hope for that in the case of ""rich enough norms"" ( norms with many isometries, see this question ) the subset $\text{ISO}(|| \cdot ||)\subseteq \text{ISO}(\<,\>)$ will be large enough to determine $\text{ISO}(\<,\>)$. (which by remark 1) determines $(\<,\>)$).","$\newcommand{\<}{\langle} \newcommand{\>}{\rangle} $Let $||\cdot||$ be a norm on a finite dimensional real vector space $V$. Does there always exist some inner product $\<,\>$ on $V$ such that $\text{ISO}(|| \cdot ||)\subseteq \text{ISO}(\<,\>)$ ? Update: As pointed by Qiaochu Yuan the answer is positive. This raises the question of uniqueness of the inner product $\<,\>$ which satisfies $\text{ISO}(|| \cdot ||)\subseteq \text{ISO}(\<,\>)$. Is it unique (up to scalar multiple)? Remarks: 1) Determining $\<,\>$ (up to scalar multiple) is equivalent to determining $\text{ISO}(\<,\>)$. Clearly if we know the inner product we know all its isometries. The other direction follows as a corollary from an argument given here which shows which inner products are preserved by a given automorphism. 2) Since there are ""rigid"" norms (whose only isometries are $\pm Id$ ) the uniqueness certainly doesn't hold in general. One could hope for that in the case of ""rich enough norms"" ( norms with many isometries, see this question ) the subset $\text{ISO}(|| \cdot ||)\subseteq \text{ISO}(\<,\>)$ will be large enough to determine $\text{ISO}(\<,\>)$. (which by remark 1) determines $(\<,\>)$).",,"['linear-algebra', 'normed-spaces', 'inner-products', 'isometry', 'metric-geometry']"
7,Intuition behind functional dependence,Intuition behind functional dependence,,"What is the intuition behind functional independence ? (This is defined in the following way: Let $k\leq n$. The $C^1$ functions $F_1,\ldots,F_k:\mathbb{R}^n\rightarrow \mathbb{R}$ are functionally independent if the matrix whose columns are the gradients  $\nabla F_1,\ldots,\nabla F_k$ has full rank, i.e. rank $k$, on the whole domain of definition. From what I gather from this answer, this is the same as saying that $F:=(F_1,\ldots,F_k):\mathbb{R}^n\rightarrow \mathbb{R}^k$ is submersion, but that doesn't help me much either, because I also don't have any intuition concerning submersions.) So what does it really mean if the functions are functional indepedent - or conversely, dependent ? Is there, in the latter case, then also a relationship like $g(\nabla F_1,\ldots,\nabla F_k)=0$ -- or maybe like $g(F_1(x),\ldots F_k (x))=0$ for some $x$ -- for $g$ ranging in some specific set, similar to the case of linear independence ( in which $g$ would be from the set $\{g:\mathbb{R}^k\rightarrow \mathbb{R}:g(x_1,\ldots,x_k)=\sum \lambda_i x_i \text{ for some nonzero } \lambda_i \in \mathbb{R}\}$).","What is the intuition behind functional independence ? (This is defined in the following way: Let $k\leq n$. The $C^1$ functions $F_1,\ldots,F_k:\mathbb{R}^n\rightarrow \mathbb{R}$ are functionally independent if the matrix whose columns are the gradients  $\nabla F_1,\ldots,\nabla F_k$ has full rank, i.e. rank $k$, on the whole domain of definition. From what I gather from this answer, this is the same as saying that $F:=(F_1,\ldots,F_k):\mathbb{R}^n\rightarrow \mathbb{R}^k$ is submersion, but that doesn't help me much either, because I also don't have any intuition concerning submersions.) So what does it really mean if the functions are functional indepedent - or conversely, dependent ? Is there, in the latter case, then also a relationship like $g(\nabla F_1,\ldots,\nabla F_k)=0$ -- or maybe like $g(F_1(x),\ldots F_k (x))=0$ for some $x$ -- for $g$ ranging in some specific set, similar to the case of linear independence ( in which $g$ would be from the set $\{g:\mathbb{R}^k\rightarrow \mathbb{R}:g(x_1,\ldots,x_k)=\sum \lambda_i x_i \text{ for some nonzero } \lambda_i \in \mathbb{R}\}$).",,['linear-algebra']
8,How to generate $3 \times 3$ integer matrices with integer eigenvalues?,How to generate  integer matrices with integer eigenvalues?,3 \times 3,"I am looking for an easy way to generate non-trivial (i.e., not just diagonal) examples of $3 \times 3$ matrices whose entries are integers and whose eigenvalues are also integers. I know how to do this for $2 \times 2$ matrices: you just choose integers so that the discriminant of the characteristic equation is a perfect square. But short of trial and error, with the help of Wolfram Alpha, I don't know a way with larger matrices. Does anyone know how to do this?","I am looking for an easy way to generate non-trivial (i.e., not just diagonal) examples of $3 \times 3$ matrices whose entries are integers and whose eigenvalues are also integers. I know how to do this for $2 \times 2$ matrices: you just choose integers so that the discriminant of the characteristic equation is a perfect square. But short of trial and error, with the help of Wolfram Alpha, I don't know a way with larger matrices. Does anyone know how to do this?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
9,Why is the determinant equal to the index?,Why is the determinant equal to the index?,,"Let $A \subset B$ be integral domains and assume $B$ is a free $A$-module of rank $m$. Define the discriminant of $m$ elements $b_1,\dots,b_m\in B$ as $D(b_1,\dots,b_m)=\det(\operatorname{Tr}_{B/A}(b_ib_j))$. A standard result says that if $c_j=\sum a_{ji}b_i$ then $$D(c_1,\dots,c_m)=\det(a_{ij})^2 D(b_1,\dots,b_m).$$ However, there's a result I don't understand. Milne states that if $A=\mathbb{Z}$, then the elements $\gamma_1,\dots, \gamma_m$ generate a submodule $N$ of finite index if and only if $D(\gamma_1, \dots, \gamma_m)\neq 0$. So far so good. However, he then claims that $$D(\gamma_1,\dots, \gamma_n)=(B:N)^2 \operatorname{disc}(B/\mathbb{Z}).$$ I don't see how the determinant of a change-of-basis matrix relates to the index of a submodule. Could someone explain this? Thanks. (NB: This is from page 28 in Milne's notes)","Let $A \subset B$ be integral domains and assume $B$ is a free $A$-module of rank $m$. Define the discriminant of $m$ elements $b_1,\dots,b_m\in B$ as $D(b_1,\dots,b_m)=\det(\operatorname{Tr}_{B/A}(b_ib_j))$. A standard result says that if $c_j=\sum a_{ji}b_i$ then $$D(c_1,\dots,c_m)=\det(a_{ij})^2 D(b_1,\dots,b_m).$$ However, there's a result I don't understand. Milne states that if $A=\mathbb{Z}$, then the elements $\gamma_1,\dots, \gamma_m$ generate a submodule $N$ of finite index if and only if $D(\gamma_1, \dots, \gamma_m)\neq 0$. So far so good. However, he then claims that $$D(\gamma_1,\dots, \gamma_n)=(B:N)^2 \operatorname{disc}(B/\mathbb{Z}).$$ I don't see how the determinant of a change-of-basis matrix relates to the index of a submodule. Could someone explain this? Thanks. (NB: This is from page 28 in Milne's notes)",,"['linear-algebra', 'algebraic-number-theory']"
10,intuition for similar matrix,intuition for similar matrix,,"If matrices $A$ and $B$ satisfy the definition of similar matrix, i.e. $B=PAP^{-1}$, then we can interpret $A$ and $B$ are the same linear transformation under different basis. But my question is how to ""grok"" this interpretation by just look at the definition of similar matrix?","If matrices $A$ and $B$ satisfy the definition of similar matrix, i.e. $B=PAP^{-1}$, then we can interpret $A$ and $B$ are the same linear transformation under different basis. But my question is how to ""grok"" this interpretation by just look at the definition of similar matrix?",,"['linear-algebra', 'matrices']"
11,Why would one eigenvalue correspond to multiple eigenvectors?,Why would one eigenvalue correspond to multiple eigenvectors?,,"Given the matrix $A = \begin{bmatrix} 0 & 1 \\ -1 & -2 \end{bmatrix}$ This guy has eigenvalues $\lambda = -1$ with algebraic multiplicity 2. Why is it that the eigenvector corresponding to this eigenvalue (as returned by computing software such as matlab) is $[-1, 1]$ , and $[1, -1]$ instead of just one eigenvector? Is this by convention? Is it for efficiency? Is there some mathematical inaccuracy if we just use one eigenvector?","Given the matrix This guy has eigenvalues with algebraic multiplicity 2. Why is it that the eigenvector corresponding to this eigenvalue (as returned by computing software such as matlab) is , and instead of just one eigenvector? Is this by convention? Is it for efficiency? Is there some mathematical inaccuracy if we just use one eigenvector?","A = \begin{bmatrix} 0 & 1 \\ -1 & -2 \end{bmatrix} \lambda = -1 [-1, 1] [1, -1]","['linear-algebra', 'matrices', 'eigenvalues-eigenvectors']"
12,Kernel of composition of linear transformations,Kernel of composition of linear transformations,,"Let $f : U \to V$ and $g : V \to W$ be linear transformations on the vector spaces $U$, $V$, and $W$. Supposedly, $$ \dim(\ker(g \circ f)) = \dim(\ker(f)) + \dim(\ker(g) \cap \operatorname{im}(f)). $$ How might I go about proving that? (Attempt:) The $\dim(\ker(g)\cap\operatorname{im}(f))$ term suggests to me that I should define a vector space $V' = \ker(g) + \operatorname{im}(f)$ to invoke the theorem that $$ \dim(V') = \dim(\ker(g)) + \dim(\operatorname{im}(f)) - \dim(\ker(g) \cap \operatorname{im}(f)), $$ but I don't really see where to go from there.","Let $f : U \to V$ and $g : V \to W$ be linear transformations on the vector spaces $U$, $V$, and $W$. Supposedly, $$ \dim(\ker(g \circ f)) = \dim(\ker(f)) + \dim(\ker(g) \cap \operatorname{im}(f)). $$ How might I go about proving that? (Attempt:) The $\dim(\ker(g)\cap\operatorname{im}(f))$ term suggests to me that I should define a vector space $V' = \ker(g) + \operatorname{im}(f)$ to invoke the theorem that $$ \dim(V') = \dim(\ker(g)) + \dim(\operatorname{im}(f)) - \dim(\ker(g) \cap \operatorname{im}(f)), $$ but I don't really see where to go from there.",,['linear-algebra']
13,Orbits of vectors under the action of $\mathrm{GL}_n(\mathbb Q)$,Orbits of vectors under the action of,\mathrm{GL}_n(\mathbb Q),"Context. While working on a larger proof, I would love to have the following lemma, but I can't even decide if it's true or not. The question. We consider the action of $\mathrm {GL}_n(\mathbb Q)$ on $\mathbb R^n$ such that $\varphi\cdot x=\varphi(x)$ for $\varphi\in \mathrm {GL}_n(\mathbb Q)$ and $x\in\mathbb R^n$ . Let $A$ be a subspace of $\mathbb R^n$ of dimension $2$ . Does there exist $u,v\in A$ linearly independent such that $v$ is in the orbit of $u$ under the action of $\mathrm{GL}_n(\mathbb Q)$ ? Remarks. We can reformulate the question this way: does there exist a rational transformation $\varphi\in\mathrm{GL}_n(\mathbb Q)$ such that $\varphi(u)=v$ ? I managed to prove this result for $n=3$ by constructing a rational rotation which sends $u$ to $v$ . With a reasoning of cardinality, we can prove that this result is false if we fix $u\in A$ , and we try to find $\varphi\in\mathrm{GL}_n(\mathbb Q)$ and $v\in A$ such that $v=\varphi(u)$ . If $A$ contains a rational vector $x$ , the symmetry with respect to $x$ will do the trick, so we can assume that $A$ does not contain any rational vector. Moreover, if $A$ intersect non-trivially a rational plane $B$ , then we can consider the rotation of axis $B^\perp$ (of dimension $n-2$ ) and of angle $\pi/2$ , and a little work shows this result. So we can assume now that for all rational planes $B$ , $A\cap B=\{0\}$ . I have no idea if this result is even true when $n\geqslant 4$ . Any hints, references or proofs would be much appreciated.","Context. While working on a larger proof, I would love to have the following lemma, but I can't even decide if it's true or not. The question. We consider the action of on such that for and . Let be a subspace of of dimension . Does there exist linearly independent such that is in the orbit of under the action of ? Remarks. We can reformulate the question this way: does there exist a rational transformation such that ? I managed to prove this result for by constructing a rational rotation which sends to . With a reasoning of cardinality, we can prove that this result is false if we fix , and we try to find and such that . If contains a rational vector , the symmetry with respect to will do the trick, so we can assume that does not contain any rational vector. Moreover, if intersect non-trivially a rational plane , then we can consider the rotation of axis (of dimension ) and of angle , and a little work shows this result. So we can assume now that for all rational planes , . I have no idea if this result is even true when . Any hints, references or proofs would be much appreciated.","\mathrm {GL}_n(\mathbb Q) \mathbb R^n \varphi\cdot x=\varphi(x) \varphi\in \mathrm {GL}_n(\mathbb Q) x\in\mathbb R^n A \mathbb R^n 2 u,v\in A v u \mathrm{GL}_n(\mathbb Q) \varphi\in\mathrm{GL}_n(\mathbb Q) \varphi(u)=v n=3 u v u\in A \varphi\in\mathrm{GL}_n(\mathbb Q) v\in A v=\varphi(u) A x x A A B B^\perp n-2 \pi/2 B A\cap B=\{0\} n\geqslant 4","['linear-algebra', 'geometry', 'linear-transformations', 'group-actions', 'rational-numbers']"
14,"How is a Generator Matrix for a (7, 4) Hamming code created?","How is a Generator Matrix for a (7, 4) Hamming code created?",,"I see that a generator matrix is created with the following formulae: $$G = \left[I_{k}|P\right]$$ I do not understand what P is in this case. In my notes, I am told that in a (7, 4) Hamming code situation my $$G = \begin{pmatrix} 1 & 0 & 0 & 0 & 1 & 0 & 1 \\ 0 & 1 & 0 & 0 & 1 & 1 & 1 \\ 0 & 0 & 1 & 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 1 & 0 & 1 & 1   \end{pmatrix}$$ where P would be $$P=\begin{pmatrix}  1 & 0 & 1 \\  1 & 1 & 1 \\  1 & 1 & 0 \\  0 & 1 & 1   \end{pmatrix}$$ How is this P generated?","I see that a generator matrix is created with the following formulae: $$G = \left[I_{k}|P\right]$$ I do not understand what P is in this case. In my notes, I am told that in a (7, 4) Hamming code situation my $$G = \begin{pmatrix} 1 & 0 & 0 & 0 & 1 & 0 & 1 \\ 0 & 1 & 0 & 0 & 1 & 1 & 1 \\ 0 & 0 & 1 & 0 & 1 & 1 & 0 \\ 0 & 0 & 0 & 1 & 0 & 1 & 1   \end{pmatrix}$$ where P would be $$P=\begin{pmatrix}  1 & 0 & 1 \\  1 & 1 & 1 \\  1 & 1 & 0 \\  0 & 1 & 1   \end{pmatrix}$$ How is this P generated?",,"['linear-algebra', 'matrices', 'coding-theory']"
15,A curious property of orthogonal matrices,A curious property of orthogonal matrices,,"Let $A$ be an $n\times n$ orthogonal matrix, i.e. $AA^T=A^TA=I$. I noticed experimentally that if we take any increasing set of indices $i_1<\cdots<i_p$, then the sum of the squares of all possible minors with rows corresponding to $\{i_1,\ldots,i_p\}$ is equal to 1. This is a bit hard to formulate, so let me try to say what I mean more explicitly. Let $$A=\begin{pmatrix}a_1^1&\ldots&a_1^n\\\vdots&\ddots&\vdots\\a_n^1&\cdots&a_n^n\end{pmatrix}$$ and let $$A_{i_1\ldots i_p}^{j_1\ldots j_p}=\begin{pmatrix}a_{i_1}^{j_1}&\ldots&a_{i_1}^{j_p}\\\vdots&\ddots&\vdots\\a_{i_p}^{j_1}&\cdots&a_{i_p}^{j_p}\end{pmatrix}$$ for $i_1<\cdots<i_p$, $j_1<\cdots<j_p$ and $1\leq p\leq n$. Conjecture: If $A$ is orthogonal, then, for any $i_1<\cdots<i_p$ we have   $$\sum_{j_1<\cdots<j_p}\left(\det A_{i_1\ldots i_p}^{j_1\ldots j_p}\right)^2=1,$$   where the sum is over all increasing sets of $p$ indices. How do we prove such a thing? Is that even true? (The answer is very likely yes, as I have checked it for many random examples.) Note: The case $p=1$ is the statement that the rows of $A$ have unit norms, and the case $p=n$ is the statement that $(\det A)^2=1$. The conjecture generalizes these two statements. In particular, the conjecture is clearly true for $n=2$. Now, here is a slightly less trivial example: Example: Take $$ A= \begin{pmatrix} 1/\sqrt{2}&0&1/\sqrt{2}\\ -1/\sqrt{6}&\sqrt{2/3}&1/\sqrt{6}\\ 1/\sqrt{3}&1/\sqrt{3}&-1/\sqrt{3} \end{pmatrix}\in O(3). $$ Then, with $p=2$ and $i_1=1$, $i_2=2$ we have $$ \begin{vmatrix} 1/\sqrt{2}&0\\ -1/\sqrt{6}&\sqrt{2/3} \end{vmatrix}^2 + \begin{vmatrix} 1/\sqrt{2}&1/\sqrt{2}\\ -1/\sqrt{6}&1/\sqrt{6} \end{vmatrix}^2 + \begin{vmatrix} 0&1/\sqrt{2}\\ \sqrt{2/3}&1/\sqrt{6} \end{vmatrix}^2 =\frac{1}{3}+\frac{1}{3}+\frac{1}{3}=1. $$","Let $A$ be an $n\times n$ orthogonal matrix, i.e. $AA^T=A^TA=I$. I noticed experimentally that if we take any increasing set of indices $i_1<\cdots<i_p$, then the sum of the squares of all possible minors with rows corresponding to $\{i_1,\ldots,i_p\}$ is equal to 1. This is a bit hard to formulate, so let me try to say what I mean more explicitly. Let $$A=\begin{pmatrix}a_1^1&\ldots&a_1^n\\\vdots&\ddots&\vdots\\a_n^1&\cdots&a_n^n\end{pmatrix}$$ and let $$A_{i_1\ldots i_p}^{j_1\ldots j_p}=\begin{pmatrix}a_{i_1}^{j_1}&\ldots&a_{i_1}^{j_p}\\\vdots&\ddots&\vdots\\a_{i_p}^{j_1}&\cdots&a_{i_p}^{j_p}\end{pmatrix}$$ for $i_1<\cdots<i_p$, $j_1<\cdots<j_p$ and $1\leq p\leq n$. Conjecture: If $A$ is orthogonal, then, for any $i_1<\cdots<i_p$ we have   $$\sum_{j_1<\cdots<j_p}\left(\det A_{i_1\ldots i_p}^{j_1\ldots j_p}\right)^2=1,$$   where the sum is over all increasing sets of $p$ indices. How do we prove such a thing? Is that even true? (The answer is very likely yes, as I have checked it for many random examples.) Note: The case $p=1$ is the statement that the rows of $A$ have unit norms, and the case $p=n$ is the statement that $(\det A)^2=1$. The conjecture generalizes these two statements. In particular, the conjecture is clearly true for $n=2$. Now, here is a slightly less trivial example: Example: Take $$ A= \begin{pmatrix} 1/\sqrt{2}&0&1/\sqrt{2}\\ -1/\sqrt{6}&\sqrt{2/3}&1/\sqrt{6}\\ 1/\sqrt{3}&1/\sqrt{3}&-1/\sqrt{3} \end{pmatrix}\in O(3). $$ Then, with $p=2$ and $i_1=1$, $i_2=2$ we have $$ \begin{vmatrix} 1/\sqrt{2}&0\\ -1/\sqrt{6}&\sqrt{2/3} \end{vmatrix}^2 + \begin{vmatrix} 1/\sqrt{2}&1/\sqrt{2}\\ -1/\sqrt{6}&1/\sqrt{6} \end{vmatrix}^2 + \begin{vmatrix} 0&1/\sqrt{2}\\ \sqrt{2/3}&1/\sqrt{6} \end{vmatrix}^2 =\frac{1}{3}+\frac{1}{3}+\frac{1}{3}=1. $$",,"['linear-algebra', 'matrices']"
16,How can I represent an N dimensional line?,How can I represent an N dimensional line?,,How can I represent a  straight line (between two points) in a N-dimensional space?,How can I represent a  straight line (between two points) in a N-dimensional space?,,['linear-algebra']
17,Probability that a random binary matrix will have full column rank?,Probability that a random binary matrix will have full column rank?,,"If $S = \{ A_{m \times n} : m > n, A_{ij} \in \{0,1\} \}, $ is the set of all $m \times n$ binary matrices, and I choose a random matrix $r \in S$, what would be the probability that $r$ would have full column rank (each of the n columns would be linearly independent)? Several papers simply claim that this probability is very high, but I'm curious as to what the exact probability would be, and how to compute it.","If $S = \{ A_{m \times n} : m > n, A_{ij} \in \{0,1\} \}, $ is the set of all $m \times n$ binary matrices, and I choose a random matrix $r \in S$, what would be the probability that $r$ would have full column rank (each of the n columns would be linearly independent)? Several papers simply claim that this probability is very high, but I'm curious as to what the exact probability would be, and how to compute it.",,"['linear-algebra', 'probability']"
18,Why does cross product tell us about clockwise or anti-clockwise rotation?,Why does cross product tell us about clockwise or anti-clockwise rotation?,,"Wikipedia link for Cross Product talks about using the cross-product to determine if $3$ points are in a clockwise or anti-clockwise rotation. I'm not able to visualize this or think of it in terms of math. Does it mean that sin of an angle made between two vectors is $0-180$ for anticlockwise and $180-360$ for clockwise? Can somebody explain, at the most fundamental level, why the cross-product tells us about an anti-clockwise/clockwise rotation? I want to understand how can you determine if $3$ points have a clockwise or anti-clockwise rotation.","Wikipedia link for Cross Product talks about using the cross-product to determine if points are in a clockwise or anti-clockwise rotation. I'm not able to visualize this or think of it in terms of math. Does it mean that sin of an angle made between two vectors is for anticlockwise and for clockwise? Can somebody explain, at the most fundamental level, why the cross-product tells us about an anti-clockwise/clockwise rotation? I want to understand how can you determine if points have a clockwise or anti-clockwise rotation.",3 0-180 180-360 3,"['linear-algebra', 'vector-spaces']"
19,Get Transformation Matrix from Points,Get Transformation Matrix from Points,,"I have built a little C# application that allows visualization of perpective transformations with a matrix, in 2D XYW space. Now I would like to be able to calculate the matrix from the four corners of the transformed square. Here is a screenshot to help you understand what I am working with: The idea is to allow the user to move the corners of the square and update the matrix. The corners are currently located at (1,1), (1,-1), (-1,-1), (-1,1). Is there an algorithm that will calculate a 3x3 matrix given four 2D points? If I understand this correctly, every matrix corresponds to a set of four points, and every set of four points corresponds to one or more equivalent matrices ('equivalent' meaning 'producing identical transformations'). I searched for an algorithm to do this, but didn't have much luck. I figured out that I could do it by creating eight equations, one for each variable in the four points, and then setting one of the matrix values to one, and solving for the other eight with algebra. However, the equations grow much too complicated to do this all successfully on pencil and paper. This is the process I used to try to get it working. So this is the basic matrix transformation formula. $\begin{pmatrix}a & b & c\\ d & e & f\\ g & h & i \end{pmatrix}\begin{pmatrix}x\\ y\\ z \end{pmatrix}=\begin{pmatrix}ax+by+cz\\ dx+ey+fz\\ gx+hy+iz \end{pmatrix}$ The resulting point is then converted from homogeneous to Euclidean coordinates. $\begin{pmatrix}x\\ y\\ z \end{pmatrix}$=$\begin{pmatrix}\frac{x}{z}\\ \frac{y}{z} \end{pmatrix}$ So given a collection of points, we transform them like this. $\begin{pmatrix}a & b & c\\ d & e & f\\ g & h & i \end{pmatrix}\begin{pmatrix}x_{n}\\ y_{n}\\ z_{n} \end{pmatrix}=\begin{pmatrix}x'_{n}\\ y'_{n} \end{pmatrix}$ These are the formulas used for the transformation. $x'_{n}=$$\frac{ax_{n}+by_{n}+cz_{n}}{gx_{n}+hy_{n}+iz_{n}}$ $y'_{n}=$$\frac{dx_{n}+ey_{n}+fz_{n}}{gx_{n}+hy_{n}+iz_{n}}$ We then define four base points, which are the corners of our square. $x'_{0}=1$ $y'_{0}=1$ $z'_{0}=1$ $x'_{1}=1$ $y'_{1}=-1$ $z'_{1}=1$ $x'_{2}=-1$ $y'_{2}=-1$ $z'_{2}=1$ $x'_{3}=-1$ $y'_{3}=1$ $z'_{3}=1$ This gives us the following system of equations, in the form that lets us determine the transformed points from the matrix. $x'_{0}=$$\frac{a+b+c}{g+h+i}$ $y'_{0}=$$\frac{d+e+f}{g+h+i}$ $x'_{1}=$$\frac{a-b+c}{g-h+i}$ $y'_{1}=$$\frac{d-e+f}{g-h+i}$ $x'_{2}=$$\frac{-a-b+c}{-g-h+i}$ $y'_{2}=$$\frac{-d-e+f}{-g-h+i}$ $x'_{3}=$$\frac{-a+b+c}{-g+h+i}$ $y'_{3}=$$\frac{-d+e+f}{-g+h+i}$ Now we want to reverse the transformation, and find the matrix that produces the above points. Since we have 9 unknowns and 8 equations, we need to add another equation. $i=1$ Now all that is left is to solve the system equations to find the formulas for the matrix values. I'm not patient enough nor good enough with algebra to do this myself, so I used an online calculator to solve the system of equations. The formulas it gave almost worked, but had some glitches with y-coordinates. I think this can be narrowed down to 2 questions: Are the above calculations wrong, or does the online calculator have a bug? Is there an easier algorithm for this?","I have built a little C# application that allows visualization of perpective transformations with a matrix, in 2D XYW space. Now I would like to be able to calculate the matrix from the four corners of the transformed square. Here is a screenshot to help you understand what I am working with: The idea is to allow the user to move the corners of the square and update the matrix. The corners are currently located at (1,1), (1,-1), (-1,-1), (-1,1). Is there an algorithm that will calculate a 3x3 matrix given four 2D points? If I understand this correctly, every matrix corresponds to a set of four points, and every set of four points corresponds to one or more equivalent matrices ('equivalent' meaning 'producing identical transformations'). I searched for an algorithm to do this, but didn't have much luck. I figured out that I could do it by creating eight equations, one for each variable in the four points, and then setting one of the matrix values to one, and solving for the other eight with algebra. However, the equations grow much too complicated to do this all successfully on pencil and paper. This is the process I used to try to get it working. So this is the basic matrix transformation formula. $\begin{pmatrix}a & b & c\\ d & e & f\\ g & h & i \end{pmatrix}\begin{pmatrix}x\\ y\\ z \end{pmatrix}=\begin{pmatrix}ax+by+cz\\ dx+ey+fz\\ gx+hy+iz \end{pmatrix}$ The resulting point is then converted from homogeneous to Euclidean coordinates. $\begin{pmatrix}x\\ y\\ z \end{pmatrix}$=$\begin{pmatrix}\frac{x}{z}\\ \frac{y}{z} \end{pmatrix}$ So given a collection of points, we transform them like this. $\begin{pmatrix}a & b & c\\ d & e & f\\ g & h & i \end{pmatrix}\begin{pmatrix}x_{n}\\ y_{n}\\ z_{n} \end{pmatrix}=\begin{pmatrix}x'_{n}\\ y'_{n} \end{pmatrix}$ These are the formulas used for the transformation. $x'_{n}=$$\frac{ax_{n}+by_{n}+cz_{n}}{gx_{n}+hy_{n}+iz_{n}}$ $y'_{n}=$$\frac{dx_{n}+ey_{n}+fz_{n}}{gx_{n}+hy_{n}+iz_{n}}$ We then define four base points, which are the corners of our square. $x'_{0}=1$ $y'_{0}=1$ $z'_{0}=1$ $x'_{1}=1$ $y'_{1}=-1$ $z'_{1}=1$ $x'_{2}=-1$ $y'_{2}=-1$ $z'_{2}=1$ $x'_{3}=-1$ $y'_{3}=1$ $z'_{3}=1$ This gives us the following system of equations, in the form that lets us determine the transformed points from the matrix. $x'_{0}=$$\frac{a+b+c}{g+h+i}$ $y'_{0}=$$\frac{d+e+f}{g+h+i}$ $x'_{1}=$$\frac{a-b+c}{g-h+i}$ $y'_{1}=$$\frac{d-e+f}{g-h+i}$ $x'_{2}=$$\frac{-a-b+c}{-g-h+i}$ $y'_{2}=$$\frac{-d-e+f}{-g-h+i}$ $x'_{3}=$$\frac{-a+b+c}{-g+h+i}$ $y'_{3}=$$\frac{-d+e+f}{-g+h+i}$ Now we want to reverse the transformation, and find the matrix that produces the above points. Since we have 9 unknowns and 8 equations, we need to add another equation. $i=1$ Now all that is left is to solve the system equations to find the formulas for the matrix values. I'm not patient enough nor good enough with algebra to do this myself, so I used an online calculator to solve the system of equations. The formulas it gave almost worked, but had some glitches with y-coordinates. I think this can be narrowed down to 2 questions: Are the above calculations wrong, or does the online calculator have a bug? Is there an easier algorithm for this?",,"['linear-algebra', 'matrices', 'algorithms', 'transformation']"
20,On computing the largest eigenvalue of a very large sparse matrix,On computing the largest eigenvalue of a very large sparse matrix,,"I am trying to compute the asymptotic growth-rate in a specific combinatorial problem depending on a parameter w, using the Transfer-Matrix method. This amounts to computing the largest eigenvalue of the corresponding matrix. For small values of w, the corresponding matrix is small and I can use the so-called power method - start with some vector, and multiply it by the matrix over and over, and under certain conditions you'll get the eigenvector corresponding to the largest eigenvalue. However, for the values of w I'm interested in, the matrix becomes to large, and so the vector becomes too large - $n>10,000,000,000$ entries or so, so it can't be contained in the computer's memory anymore and I need extra programming tricks or a very powerful computer. As for the matrix itself, I don't need to store it in memory - I can access it as a black box, i.e. given $i,j$ I can return $A_{ij}$ via a simple computation. Also, the matrix has only 0 and 1 entries, and I believe it to be sparse (i.e. only around $\log n$ of the entries are 1's, $n$ being the number of rows/columns). However, the matrix is not symmetric. Is there some method more space-effective for computation of eigenvalues for a case like this?","I am trying to compute the asymptotic growth-rate in a specific combinatorial problem depending on a parameter w, using the Transfer-Matrix method. This amounts to computing the largest eigenvalue of the corresponding matrix. For small values of w, the corresponding matrix is small and I can use the so-called power method - start with some vector, and multiply it by the matrix over and over, and under certain conditions you'll get the eigenvector corresponding to the largest eigenvalue. However, for the values of w I'm interested in, the matrix becomes to large, and so the vector becomes too large - $n>10,000,000,000$ entries or so, so it can't be contained in the computer's memory anymore and I need extra programming tricks or a very powerful computer. As for the matrix itself, I don't need to store it in memory - I can access it as a black box, i.e. given $i,j$ I can return $A_{ij}$ via a simple computation. Also, the matrix has only 0 and 1 entries, and I believe it to be sparse (i.e. only around $\log n$ of the entries are 1's, $n$ being the number of rows/columns). However, the matrix is not symmetric. Is there some method more space-effective for computation of eigenvalues for a case like this?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'numerical-linear-algebra', 'sparse-matrices']"
21,"Why does calculating matrix inverses, roots, etc. using the spectrum of a matrix work?","Why does calculating matrix inverses, roots, etc. using the spectrum of a matrix work?",,"Suppose $A$ is a $n \times n$ matrix from $M_n(\mathbb{C})$ with eigenvalues $\lambda_1, \ldots, \lambda_s$. Let $$m(\lambda) = (\lambda - \lambda_1)^{m_1} \ldots (\lambda - \lambda_s)^{m_s}$$ be the minimal polynomial of $A$. We define $f(A)$ in general to be $p(A)$ for some polynomial interpolating $f(\lambda_k), f'(\lambda_k), \ldots, f^{(m_k - 1)}(\lambda_k)$ for all $k$, assuming that $f$ is defined at these points. This is well defined because for any such two interpolating polynomials $p$ and $q$ we have $p(A) = q(A)$ and such a polynomial always exists. We can also find a polynomial $p$ which has degree less than the minimal polynomial $m$. Also, then there exist linearly independent matrices $Z_{ij}$ such that for any $f$ defined at $f(\lambda_k), f'(\lambda_k), \ldots, f^{(m_k - 1)}(\lambda_k)$ for all $k$, we have $$f(A) = \sum_{k = 1}^{s} ( f(\lambda_k)Z_{k0} + f'(\lambda_k)Z_{k1} + \ldots + f^{(m_k - 1)}(\lambda_k)Z_{k, m_k - 1})$$ Okay, with the definitions and such out of the way here's the actual question.. We can use this definition to calculate $f(A)$ for $f$ such as $f(x) = 1/x$ which gives us the inverse of $A$, or $f(x) = \sqrt{x}$ which gives us the square root of $A$, or $f(A)$ for any polynomial $f(x)$. The power series definition of $\sin(A)$, $\cos(A)$, etc also agrees with the definition. Now this is all really awesome and interesting in my opinion, but why on earth does this work? I have no idea why the properties of the inverse function $1/x$ or the square root $\sqrt{x}$ are ""passed on"" to matrices this way. Also, what is this called? Spectral something, but I'm not sure. I tried looking it up on Wikipedia and Google but couldn't find anything. Besides answers to help me understand what this is all about, I'll appreciate any references, keywords and so on.","Suppose $A$ is a $n \times n$ matrix from $M_n(\mathbb{C})$ with eigenvalues $\lambda_1, \ldots, \lambda_s$. Let $$m(\lambda) = (\lambda - \lambda_1)^{m_1} \ldots (\lambda - \lambda_s)^{m_s}$$ be the minimal polynomial of $A$. We define $f(A)$ in general to be $p(A)$ for some polynomial interpolating $f(\lambda_k), f'(\lambda_k), \ldots, f^{(m_k - 1)}(\lambda_k)$ for all $k$, assuming that $f$ is defined at these points. This is well defined because for any such two interpolating polynomials $p$ and $q$ we have $p(A) = q(A)$ and such a polynomial always exists. We can also find a polynomial $p$ which has degree less than the minimal polynomial $m$. Also, then there exist linearly independent matrices $Z_{ij}$ such that for any $f$ defined at $f(\lambda_k), f'(\lambda_k), \ldots, f^{(m_k - 1)}(\lambda_k)$ for all $k$, we have $$f(A) = \sum_{k = 1}^{s} ( f(\lambda_k)Z_{k0} + f'(\lambda_k)Z_{k1} + \ldots + f^{(m_k - 1)}(\lambda_k)Z_{k, m_k - 1})$$ Okay, with the definitions and such out of the way here's the actual question.. We can use this definition to calculate $f(A)$ for $f$ such as $f(x) = 1/x$ which gives us the inverse of $A$, or $f(x) = \sqrt{x}$ which gives us the square root of $A$, or $f(A)$ for any polynomial $f(x)$. The power series definition of $\sin(A)$, $\cos(A)$, etc also agrees with the definition. Now this is all really awesome and interesting in my opinion, but why on earth does this work? I have no idea why the properties of the inverse function $1/x$ or the square root $\sqrt{x}$ are ""passed on"" to matrices this way. Also, what is this called? Spectral something, but I'm not sure. I tried looking it up on Wikipedia and Google but couldn't find anything. Besides answers to help me understand what this is all about, I'll appreciate any references, keywords and so on.",,"['linear-algebra', 'matrices']"
22,Dimensionality of null space when Trace is Zero,Dimensionality of null space when Trace is Zero,,"This is the fourth part of a four-part problem in Charles W. Curtis's book entitled Linear Algebra, An Introductory Approach (p. 216).  I've succeeded in proving the first three parts, but the most interesting part of the problem eludes me.  Part (a) requires the reader to prove that $\operatorname{Tr}{(AB)} = \operatorname{Tr}{(BA)}$, which I was able to show by writing out each side of the equation using sigma notation.  Part (b) asks the reader to use part (a) to show that similar matrices have the same trace.  If $A$ and $B$ are similar, then $\operatorname{Tr}{(A)} = \operatorname{Tr}{(S^{-1}BS)}$ $= \operatorname{Tr}(BSS^{-1})$ $= \operatorname{Tr}(B)$, which completes part (b).  Part (c) asks the reader to show that the vector subspace of matrices with trace equal to zero have dimension $n^2 - 1$.  Curtis provides the hint that the map from $M_n(F)$ to $F$ is a linear transformation.  From this, I used the theorem that $\dim T(V) + \dim n(T) = \dim V$ to obtain the dimension of the null space.  Part (d), however, I'm stuck on.  It asks the reader to show that subspace described in part (c) is generated by matrices of the form $AB - BA$, where $A$ and $B$ are arbitrary $n \times n$ matrices.  I tried to form a basis for the subspace, but wasn't really sure what it would look like since an $n \times n$ matrix has $n^2$ entries in it, but the basis would need $n^2 - 1$ matrixes.  I also tried to think of a linear transformation whose image would have the form of $AB - BA$, but this also didn't help me.  I'm kind of stuck... Many thanks in advance!","This is the fourth part of a four-part problem in Charles W. Curtis's book entitled Linear Algebra, An Introductory Approach (p. 216).  I've succeeded in proving the first three parts, but the most interesting part of the problem eludes me.  Part (a) requires the reader to prove that $\operatorname{Tr}{(AB)} = \operatorname{Tr}{(BA)}$, which I was able to show by writing out each side of the equation using sigma notation.  Part (b) asks the reader to use part (a) to show that similar matrices have the same trace.  If $A$ and $B$ are similar, then $\operatorname{Tr}{(A)} = \operatorname{Tr}{(S^{-1}BS)}$ $= \operatorname{Tr}(BSS^{-1})$ $= \operatorname{Tr}(B)$, which completes part (b).  Part (c) asks the reader to show that the vector subspace of matrices with trace equal to zero have dimension $n^2 - 1$.  Curtis provides the hint that the map from $M_n(F)$ to $F$ is a linear transformation.  From this, I used the theorem that $\dim T(V) + \dim n(T) = \dim V$ to obtain the dimension of the null space.  Part (d), however, I'm stuck on.  It asks the reader to show that subspace described in part (c) is generated by matrices of the form $AB - BA$, where $A$ and $B$ are arbitrary $n \times n$ matrices.  I tried to form a basis for the subspace, but wasn't really sure what it would look like since an $n \times n$ matrix has $n^2$ entries in it, but the basis would need $n^2 - 1$ matrixes.  I also tried to think of a linear transformation whose image would have the form of $AB - BA$, but this also didn't help me.  I'm kind of stuck... Many thanks in advance!",,"['linear-algebra', 'matrices']"
23,Non-integral powers of a matrix,Non-integral powers of a matrix,,"Question Given a square complex matrix $A$, what ways are there to define and compute $A^p$ for non-integral scalar exponents $p\in\mathbb R$, and for what matrices do they work? My thoughts Integral exponents Defining $A^k$ for $k\in\mathbb N$ is easy in terms of repeated multiplication, and works for every matrix. This includes $A^0=I$. Using $A^{-1}$ as the inverse, $A^{-k}=\left(A^{-1}\right)^k$ is easy to define, but requires the matrix to be invertible. So much for integral exponents. Rational definition I guess for a rational exponent, one could define $$A^{\frac pq}=B\quad:\Leftrightarrow\quad A^p=B^q$$ This will allow for more than one solution, and I'm not sure if the computations I'll describe below will find all solutions satisfying the above equation. So I'm not sure whether that's a reasonable definition. For non-rational exponents, a limit using a convergent series of rational exponents might work. Diagonalizable computation If $A$ is diagonalizable , then one has $A=W\,D\,W^{-1}$ for some diagonal matrix $D$. One can simply raise all the diagonal elements to the $p$-th power, obtaining a matrix which will satisfy the above equation. For each diagonal element, I'd define $\lambda^p=e^{(p\ln\lambda)}$, and since $\ln\lambda$ is only defined up to $2\pi i\mathbb Z$, this allows for multiple possible solutions. If one requires $-\pi<\operatorname{Im}(\ln\lambda)\le\pi$, then the solution should be well defined, and I guess this definition even has a name, although I don't know it. Non-diagonalizable computation If $A$ is not diagonalizable, then there is still a Jordan normal form , so instead of raising diagonal elements to a fractional power, one could attempt to do the same with Jordan blocks. Unless I made a mistake, this appears to be possible. At least for my example of a $3\times3$ Jordan block, I was able to obtain a $k$-th root. $$ \begin{pmatrix} \lambda^{\frac1k} & \tfrac1k\lambda^{\frac1k-1} & \tfrac{1-k}{2k^2}\lambda^{\frac1k-2} & \\ 0 & \lambda^{\frac1k} & \tfrac1k\lambda^{\frac1k-1} \\ 0 & 0 & \lambda^{\frac1k} \end{pmatrix}^k = \begin{pmatrix} \lambda & 1 & 0 \\ 0 & \lambda & 1 \\ 0 & 0 & \lambda \end{pmatrix} $$ If the eigenvalue $\lambda$ of this block is zero, then the root as computed above would be the zero matrix, which doesn't result in a Jordan block. But otherwise it should work. Conclusion Edited since this question was first asked. So it seems that every invertible matrix can be raised to every rational power, as long as uniqueness is not a strong requirement. A non-invertible matrix apparently can be raised to non-negative powers as long as all Jordan blocks for eigenvalue zero have size one. Is this true? If not, where is my mistake? If it is, is there a good reference for this?","Question Given a square complex matrix $A$, what ways are there to define and compute $A^p$ for non-integral scalar exponents $p\in\mathbb R$, and for what matrices do they work? My thoughts Integral exponents Defining $A^k$ for $k\in\mathbb N$ is easy in terms of repeated multiplication, and works for every matrix. This includes $A^0=I$. Using $A^{-1}$ as the inverse, $A^{-k}=\left(A^{-1}\right)^k$ is easy to define, but requires the matrix to be invertible. So much for integral exponents. Rational definition I guess for a rational exponent, one could define $$A^{\frac pq}=B\quad:\Leftrightarrow\quad A^p=B^q$$ This will allow for more than one solution, and I'm not sure if the computations I'll describe below will find all solutions satisfying the above equation. So I'm not sure whether that's a reasonable definition. For non-rational exponents, a limit using a convergent series of rational exponents might work. Diagonalizable computation If $A$ is diagonalizable , then one has $A=W\,D\,W^{-1}$ for some diagonal matrix $D$. One can simply raise all the diagonal elements to the $p$-th power, obtaining a matrix which will satisfy the above equation. For each diagonal element, I'd define $\lambda^p=e^{(p\ln\lambda)}$, and since $\ln\lambda$ is only defined up to $2\pi i\mathbb Z$, this allows for multiple possible solutions. If one requires $-\pi<\operatorname{Im}(\ln\lambda)\le\pi$, then the solution should be well defined, and I guess this definition even has a name, although I don't know it. Non-diagonalizable computation If $A$ is not diagonalizable, then there is still a Jordan normal form , so instead of raising diagonal elements to a fractional power, one could attempt to do the same with Jordan blocks. Unless I made a mistake, this appears to be possible. At least for my example of a $3\times3$ Jordan block, I was able to obtain a $k$-th root. $$ \begin{pmatrix} \lambda^{\frac1k} & \tfrac1k\lambda^{\frac1k-1} & \tfrac{1-k}{2k^2}\lambda^{\frac1k-2} & \\ 0 & \lambda^{\frac1k} & \tfrac1k\lambda^{\frac1k-1} \\ 0 & 0 & \lambda^{\frac1k} \end{pmatrix}^k = \begin{pmatrix} \lambda & 1 & 0 \\ 0 & \lambda & 1 \\ 0 & 0 & \lambda \end{pmatrix} $$ If the eigenvalue $\lambda$ of this block is zero, then the root as computed above would be the zero matrix, which doesn't result in a Jordan block. But otherwise it should work. Conclusion Edited since this question was first asked. So it seems that every invertible matrix can be raised to every rational power, as long as uniqueness is not a strong requirement. A non-invertible matrix apparently can be raised to non-negative powers as long as all Jordan blocks for eigenvalue zero have size one. Is this true? If not, where is my mistake? If it is, is there a good reference for this?",,"['linear-algebra', 'complex-numbers', 'eigenvalues-eigenvectors', 'exponentiation', 'jordan-normal-form']"
24,Multiplicative norm on $\mathbb{R}[X]$.,Multiplicative norm on .,\mathbb{R}[X],"How to prove that : there is no function $N\colon \mathbb{R}[X] \rightarrow \mathbb{R}$, such that : $N$ is a norm of $\mathbb{R}$-vector space and $N(PQ)=N(P)N(Q)$ for all $P,Q \in \mathbb{R}[X]$. Once, my teacher asked if there is a multipicative norm on $\mathbb{R}[X]$, and one of my classmate proved that there was none. But I can't remember the proof (all I remember is that he was using integration somewhere...).","How to prove that : there is no function $N\colon \mathbb{R}[X] \rightarrow \mathbb{R}$, such that : $N$ is a norm of $\mathbb{R}$-vector space and $N(PQ)=N(P)N(Q)$ for all $P,Q \in \mathbb{R}[X]$. Once, my teacher asked if there is a multipicative norm on $\mathbb{R}[X]$, and one of my classmate proved that there was none. But I can't remember the proof (all I remember is that he was using integration somewhere...).",,"['linear-algebra', 'polynomials', 'normed-spaces', 'inner-products']"
25,We call a coloring of $3$-regular graph with $3$ colors good if for every $3$ edges incident with a vertex ...,We call a coloring of -regular graph with  colors good if for every  edges incident with a vertex ...,3 3 3,"Let $G$ be a $3$ -regular graph with $n$ vertices. Color each edge with red, blue or yellow. Now, we call a coloring of graph as good if any three edges incident with any vertex have one color or three colors. Prove that the number of good coloring of $G$ must be a power of $3$ . I have very short solution using linear algebra: If $M$ is an incidence matrix of this graph and we color each edge with colors $0,1,2$ then we have to find a number of vectors $\vec{c}\in \mathbb{Z}_3^{\varepsilon}$ such that $M\vec{c} =\vec{0} \in \mathbb{Z}_3^{n}$ where $\varepsilon$ is a number of edges $G$ . So we are interested in $|\ker(M)|$ , but $\ker(M)\leq  \mathbb{Z}_3^{\varepsilon}$ and thus $|\ker(M)| = 3^{d}$ where $d=\dim (\ker(M))$ . Even more, we can find exact number of colorings. By handshake lemma we have $\varepsilon = 3n/2$ so $n$ is even and by  rank-nullity theorem we have $$ d=\varepsilon -{\rm rank} (M) $$ There is a theorem that says if $G$ is connected then: $${\rm rank}(M)=\cases{n-1;\;{\rm if\;}G\;{\rm is\;bipartite}\\ \;\;n\;\;\;\;;\;{\rm else}}$$ So $d={n\over 2}+1$ if $G$ is bipartite and $d={n\over 2}$ else. Clearly we can extend this results to all graphs, not just connected. But I'm unable to get an elementary solution.","Let be a -regular graph with vertices. Color each edge with red, blue or yellow. Now, we call a coloring of graph as good if any three edges incident with any vertex have one color or three colors. Prove that the number of good coloring of must be a power of . I have very short solution using linear algebra: If is an incidence matrix of this graph and we color each edge with colors then we have to find a number of vectors such that where is a number of edges . So we are interested in , but and thus where . Even more, we can find exact number of colorings. By handshake lemma we have so is even and by  rank-nullity theorem we have There is a theorem that says if is connected then: So if is bipartite and else. Clearly we can extend this results to all graphs, not just connected. But I'm unable to get an elementary solution.","G 3 n G 3 M 0,1,2 \vec{c}\in \mathbb{Z}_3^{\varepsilon} M\vec{c} =\vec{0} \in \mathbb{Z}_3^{n} \varepsilon G |\ker(M)| \ker(M)\leq  \mathbb{Z}_3^{\varepsilon} |\ker(M)| = 3^{d} d=\dim (\ker(M)) \varepsilon = 3n/2 n  d=\varepsilon -{\rm rank} (M)  G {\rm rank}(M)=\cases{n-1;\;{\rm if\;}G\;{\rm is\;bipartite}\\ \;\;n\;\;\;\;;\;{\rm else}} d={n\over 2}+1 G d={n\over 2}","['linear-algebra', 'combinatorics', 'graph-theory', 'contest-math', 'coloring']"
26,"Distribution of determinants of $n\times n$ matrices with entries in $\{0,1,\ldots,q-1\}$",Distribution of determinants of  matrices with entries in,"n\times n \{0,1,\ldots,q-1\}","Consider the set $M(n,q)$ of $n\times n$ matrices with entries in $\{0,1,\ldots,q-1\}$ , where $q$ is a prime power. What can be said about the distribution of the determinant of matrices in $M(n,q)$ ? (A 'heuristic' statement of the problem: taking $\{0,1,\ldots,q-1\}$ as a basis for $F=\mathbb{Z}_q$ , what do the determinants of matrices over $F$ look like if you don't mod out $q$ ?) Obviously $|M(n,q)| = q^{n^2}$ . Since $|GL_n(\mathbb{F}_q)| = \prod_{k=0}^{n-1} q^n-q^k$ , in $\mathbb{F}_q$ we get a clean answer for how many are divisible by $q$ : the values are equally distributed (modulo $q$ , there are $\frac{1}{q-1}\prod_{k=0}^{n-1} q^n-q^k$ matrices with determinant $j$ , $1\le j\le q-1$ ). But if we do not look mod $q$ , as it were, the question becomes substantially more difficult; to be frank, I'm not sure where to start or if there are any clear patterns. Information about the limiting behavior or any upper bounds on the magnitude of the determinant would be welcome as well. I computed the distributions for several values of $n=2,3$ and $2\le q\le 5$ ; the plot labels are of the form $\{n,q\}$ . As expected, determinant zero is the most common option and a determinant of $a$ is just as likely as a determinant of $-a$ . Past that, I admit I'm a little out of my league, but it seems like an interesting problem.","Consider the set of matrices with entries in , where is a prime power. What can be said about the distribution of the determinant of matrices in ? (A 'heuristic' statement of the problem: taking as a basis for , what do the determinants of matrices over look like if you don't mod out ?) Obviously . Since , in we get a clean answer for how many are divisible by : the values are equally distributed (modulo , there are matrices with determinant , ). But if we do not look mod , as it were, the question becomes substantially more difficult; to be frank, I'm not sure where to start or if there are any clear patterns. Information about the limiting behavior or any upper bounds on the magnitude of the determinant would be welcome as well. I computed the distributions for several values of and ; the plot labels are of the form . As expected, determinant zero is the most common option and a determinant of is just as likely as a determinant of . Past that, I admit I'm a little out of my league, but it seems like an interesting problem.","M(n,q) n\times n \{0,1,\ldots,q-1\} q M(n,q) \{0,1,\ldots,q-1\} F=\mathbb{Z}_q F q |M(n,q)| = q^{n^2} |GL_n(\mathbb{F}_q)| = \prod_{k=0}^{n-1} q^n-q^k \mathbb{F}_q q q \frac{1}{q-1}\prod_{k=0}^{n-1} q^n-q^k j 1\le j\le q-1 q n=2,3 2\le q\le 5 \{n,q\} a -a","['linear-algebra', 'probability', 'matrices', 'probability-distributions', 'determinant']"
27,Who was the first to use dual space?,Who was the first to use dual space?,,Who was the first person who used the dual space? In which paper / book did he or she use the dual space? Who was the first who called it dual space and in which paper / book?,Who was the first person who used the dual space? In which paper / book did he or she use the dual space? Who was the first who called it dual space and in which paper / book?,,"['linear-algebra', 'math-history']"
28,The Intution Behind Real Symmetric Matrices and Their Real Eigenvectors,The Intution Behind Real Symmetric Matrices and Their Real Eigenvectors,,"I am wondering about the geometric intuition behind real symmetric matrices and their corresponding linear transformations. Is it possible to understand geometrically why real symmetric matrices have only real eigenvalues? That is, what do symmetric linear transformations have in common geometrically that make this true? I am NOT after a proof of this fact; what I am curious about is whether there is a geometric argument for it. I am after the sort of intuition one gets from looking at linear transformations in $\mathbb{R}^2$ and envisioning their eigenvalues and eigenvectors. For example, it is very intuitively clear why nontrivial rotation matrices cannot have real eigenvectors. It is also geometrically clear why diagonal matrices have their eigenvalues equal to their diagonal elements. EDIT: Upon further thought, I have realized that a simple consequence of symmetry is that in the singular value decomposition of a matrix $M = U \Sigma V^*$, we have $U = V$. Thus requiring symmetry means that the linear transformation must be accomplished by performing a rotation, performing a scaling along the axes, and then reversing the original rotation . The missing piece for me is why defining a transformation via a symmetric matrix means that the transformation can be decomposed in this simple way.","I am wondering about the geometric intuition behind real symmetric matrices and their corresponding linear transformations. Is it possible to understand geometrically why real symmetric matrices have only real eigenvalues? That is, what do symmetric linear transformations have in common geometrically that make this true? I am NOT after a proof of this fact; what I am curious about is whether there is a geometric argument for it. I am after the sort of intuition one gets from looking at linear transformations in $\mathbb{R}^2$ and envisioning their eigenvalues and eigenvectors. For example, it is very intuitively clear why nontrivial rotation matrices cannot have real eigenvectors. It is also geometrically clear why diagonal matrices have their eigenvalues equal to their diagonal elements. EDIT: Upon further thought, I have realized that a simple consequence of symmetry is that in the singular value decomposition of a matrix $M = U \Sigma V^*$, we have $U = V$. Thus requiring symmetry means that the linear transformation must be accomplished by performing a rotation, performing a scaling along the axes, and then reversing the original rotation . The missing piece for me is why defining a transformation via a symmetric matrix means that the transformation can be decomposed in this simple way.",,"['linear-algebra', 'intuition']"
29,Determinant of a standard magic square,Determinant of a standard magic square,,"What is the lowest positive, what the highest possible value for the determinant of a standard-magic-square-matrix of order $n$ ? Are there singular standard-magic-square-matrices of any order greater than $3$ ? First of all, the determinant of a standard-magic-square-matrix must be a multiple of $\frac{n^2(n^2+1)}{2}$ for odd $n$ . This follows easily by the following process: Add all the columns to the last column. Then every entry in the last column is $\frac{n(n^2+1)}{2}$ ,the constant of the standard-magic-square. Now extract this constant and add all the rows to the last one. Then every entry in the last row is again the constant, beside the last entry, which is $n$ . Since $n$ is for odd n a divisor of the constant, it can be extracted as well. For even $n$ , only $\frac{n}{2}$ can be extracted, so the determinant is only a multiple of $\frac{n^2(n^2+1)}{4}$ . This gives lower bounds for the absolute value of the determinant of regular magic-square-matrices. For size $3$ , the only possible determinant (ignoring the sign) is $360$ . For size $4$ , my personal minimum for the absolute non-zero determinant is $2176$ and my maximum is $17408$ . For size $5$ , my best results are $325$ and $6\ 547\ 775$ . For sizes $4$ and $5$ , I also found matrices with determinant $0$ , but for $n = 6$ I found none. OEIS claims that the magic square of order $6$ produced by Matlab has determinant $0$ (By the way, the sequence seems to contain a typo because in the list $-360$ appears for $n=2$ instead of $n=3$ ). My pascal program generating random magic squares did not find a magic square with order $6$ and determinant $0$ . Since I do not have Matlab, I cannot verify the magic square produced by it.","What is the lowest positive, what the highest possible value for the determinant of a standard-magic-square-matrix of order ? Are there singular standard-magic-square-matrices of any order greater than ? First of all, the determinant of a standard-magic-square-matrix must be a multiple of for odd . This follows easily by the following process: Add all the columns to the last column. Then every entry in the last column is ,the constant of the standard-magic-square. Now extract this constant and add all the rows to the last one. Then every entry in the last row is again the constant, beside the last entry, which is . Since is for odd n a divisor of the constant, it can be extracted as well. For even , only can be extracted, so the determinant is only a multiple of . This gives lower bounds for the absolute value of the determinant of regular magic-square-matrices. For size , the only possible determinant (ignoring the sign) is . For size , my personal minimum for the absolute non-zero determinant is and my maximum is . For size , my best results are and . For sizes and , I also found matrices with determinant , but for I found none. OEIS claims that the magic square of order produced by Matlab has determinant (By the way, the sequence seems to contain a typo because in the list appears for instead of ). My pascal program generating random magic squares did not find a magic square with order and determinant . Since I do not have Matlab, I cannot verify the magic square produced by it.",n 3 \frac{n^2(n^2+1)}{2} n \frac{n(n^2+1)}{2} n n n \frac{n}{2} \frac{n^2(n^2+1)}{4} 3 360 4 2176 17408 5 325 6\ 547\ 775 4 5 0 n = 6 6 0 -360 n=2 n=3 6 0,"['linear-algebra', 'matrices', 'determinant', 'magic-square']"
30,Can a cube always be fitted into the projection of a cube?,Can a cube always be fitted into the projection of a cube?,,"If we project the unit cube, i.e. a axis parallel cube with side length 1 centered at the origin, in $\mathbb{R}^n$ onto a $k$-dimensional subspace of $\mathbb{R}^n$ which contains the origin, can we always fit a $k$-dimensional cube of side length 1 into the projection?","If we project the unit cube, i.e. a axis parallel cube with side length 1 centered at the origin, in $\mathbb{R}^n$ onto a $k$-dimensional subspace of $\mathbb{R}^n$ which contains the origin, can we always fit a $k$-dimensional cube of side length 1 into the projection?",,"['linear-algebra', 'geometry']"
31,Theoretical link between the graph diffusion/heat kernel and spectral clustering,Theoretical link between the graph diffusion/heat kernel and spectral clustering,,"The graph diffusion kernel of a graph is the exponential of its Laplacian $\exp(-\beta L)$ (or a similar expression depending on how you define the kernel). If you have labels on some vertices, you can get labels on the rest of the vertices by a majority vote of adding the kernel contribution. In spectral clustering, the sign of the eigenvector components of the largest eigenvalue of the Laplacian determines the class assignment of the vertices. Since these techniques seem to be doing similar things based on the graph Laplacian, is there a link between spectral clustering and the diffusion or heat kernel? Is one the generalization of the other under some assumptions? What I have found so far: The paper ""The Principal Components Analysis of a Graph, and Its Relationships to Spectral Clustering"" by Saerens et al. seems to say something about this. They say that one flavor of the diffusion kernel (the Euclidean Commute Time Distance) is the same as one type of spectral clustering. The paper ""Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators"" by Nadler et al. also has a result that I am yet to parse. Following ""Graph spectral image smoothing using the heat kernel"" by Zhan and Hancock, we can observe that if the Laplacian $L = USU^T$, (with $U$ as the eigenvectors and $S$ the diagonal matrix of eigenvalues)  and the heat kernel is $H=\exp(-\beta L)$, then $H = Uexp(-\beta S)U^T$. So for some $\beta$ we just have to take the smallest eigenvalue to get the approximate $H$. Disclaimer: This is cross-posted from CV .","The graph diffusion kernel of a graph is the exponential of its Laplacian $\exp(-\beta L)$ (or a similar expression depending on how you define the kernel). If you have labels on some vertices, you can get labels on the rest of the vertices by a majority vote of adding the kernel contribution. In spectral clustering, the sign of the eigenvector components of the largest eigenvalue of the Laplacian determines the class assignment of the vertices. Since these techniques seem to be doing similar things based on the graph Laplacian, is there a link between spectral clustering and the diffusion or heat kernel? Is one the generalization of the other under some assumptions? What I have found so far: The paper ""The Principal Components Analysis of a Graph, and Its Relationships to Spectral Clustering"" by Saerens et al. seems to say something about this. They say that one flavor of the diffusion kernel (the Euclidean Commute Time Distance) is the same as one type of spectral clustering. The paper ""Diffusion Maps, Spectral Clustering and Eigenfunctions of Fokker-Planck Operators"" by Nadler et al. also has a result that I am yet to parse. Following ""Graph spectral image smoothing using the heat kernel"" by Zhan and Hancock, we can observe that if the Laplacian $L = USU^T$, (with $U$ as the eigenvectors and $S$ the diagonal matrix of eigenvalues)  and the heat kernel is $H=\exp(-\beta L)$, then $H = Uexp(-\beta S)U^T$. So for some $\beta$ we just have to take the smallest eigenvalue to get the approximate $H$. Disclaimer: This is cross-posted from CV .",,"['linear-algebra', 'graph-theory', 'heat-equation', 'spectral-graph-theory', 'clustering']"
32,Toeplitz matrices question with Fourier coefficients,Toeplitz matrices question with Fourier coefficients,,"Denote: $f(e^{i\theta})$ is continuous and strictly positive on the interval $ 0 \le \theta \le 2\pi$ with Fourier coefficients $$ t_j = \frac{1}{2\pi}\int_0^{2\pi}f(e^{i\theta})e^{-ij\theta} \quad T_k(f) = \begin{bmatrix}     t_0 & t_{-1} & \cdots &t_{-k} \\     t_1 & t_0 & \cdots & t_{1-k}\\     \vdots & \ddots & \ddots & \vdots \\     t_k & \cdots & t_1 & t_0     \end{bmatrix}$$ for $k = 0,1, ..$ The matrix $T_k(f)$ is constant on diagonals; such matrices are called Toeplitz matrices . I've already shown that for $b=\begin{bmatrix}b_0&\cdots&b_n\end{bmatrix}^T$ we have $$\frac{1}{2\pi}\int_0^{2\pi}\left|\sum_{j=0}^nb_je^{ij\theta}\right|^2f(e^{i\theta})d\theta = b^HT_nb\,.$$ And also have shown that if $T_n\succ0$ and $v^H=\begin{bmatrix}t_1&\cdots & t_n\end{bmatrix}$ , then $$T_n=\begin{bmatrix}1&v^HT_{n-1}^{-1}\\0&1\end{bmatrix}       \begin{bmatrix}\rho_n & 0^H\\0&T_{n-1}\end{bmatrix}       \begin{bmatrix}1&0^H\\ T^{-1}_{n-1}v & I_{n}\end{bmatrix} \,.$$ Where $\rho_n^{-1} = (T_n^{-1})_{00}$ . [ Note : This is a special case of a connection between the blocks of a matrix and the blocks of its inverse that may be obtained by Schur complements. Where (Need to prove) $$\min\left\{ \frac{1}{2\pi}\int_0^{2\pi} \left|1-\sum_{j=1}^nc_je^{ij\theta}\right|^2f(e^{i\theta})d\theta: c_1,\ldots,c_n\in\mathbb{C} \right\} = \rho_n \,.\tag{*}$$ Question: I'm struggling to show $(*)$ .","Denote: is continuous and strictly positive on the interval with Fourier coefficients for The matrix is constant on diagonals; such matrices are called Toeplitz matrices . I've already shown that for we have And also have shown that if and , then Where . [ Note : This is a special case of a connection between the blocks of a matrix and the blocks of its inverse that may be obtained by Schur complements. Where (Need to prove) Question: I'm struggling to show .","f(e^{i\theta})  0 \le \theta \le 2\pi  t_j = \frac{1}{2\pi}\int_0^{2\pi}f(e^{i\theta})e^{-ij\theta} \quad T_k(f) = \begin{bmatrix}
    t_0 & t_{-1} & \cdots &t_{-k} \\
    t_1 & t_0 & \cdots & t_{1-k}\\
    \vdots & \ddots & \ddots & \vdots \\
    t_k & \cdots & t_1 & t_0
    \end{bmatrix} k = 0,1, .. T_k(f) b=\begin{bmatrix}b_0&\cdots&b_n\end{bmatrix}^T \frac{1}{2\pi}\int_0^{2\pi}\left|\sum_{j=0}^nb_je^{ij\theta}\right|^2f(e^{i\theta})d\theta = b^HT_nb\,. T_n\succ0 v^H=\begin{bmatrix}t_1&\cdots & t_n\end{bmatrix} T_n=\begin{bmatrix}1&v^HT_{n-1}^{-1}\\0&1\end{bmatrix}
      \begin{bmatrix}\rho_n & 0^H\\0&T_{n-1}\end{bmatrix}
      \begin{bmatrix}1&0^H\\ T^{-1}_{n-1}v & I_{n}\end{bmatrix} \,. \rho_n^{-1} = (T_n^{-1})_{00} \min\left\{ \frac{1}{2\pi}\int_0^{2\pi} \left|1-\sum_{j=1}^nc_je^{ij\theta}\right|^2f(e^{i\theta})d\theta: c_1,\ldots,c_n\in\mathbb{C}
\right\} = \rho_n \,.\tag{*} (*)","['linear-algebra', 'fourier-analysis', 'toeplitz-matrices']"
33,$M\cong N$ iff $[M:N]_R$ is a principal fractional ideal,iff  is a principal fractional ideal,M\cong N [M:N]_R,"Let $R$ be a Dedekind ring, $K$ its field of fractions, $U$ a finite vector space over $K$, and $M,N$ finitely generated $R$-modules that span $U$, i.e. contain a basis of $U$. For every $\mathfrak p \subset R$, define $[M:N]_{\mathfrak p} = (\det \phi_{\mathfrak p})$, where $\phi_{\mathfrak p}$ is a linear transformation that takes $M_{\mathfrak p}$ onto $N_{\mathfrak p}$. Recall here $R_{\mathfrak p}$ is a principal ideal domain $\Rightarrow$ $M_{\mathfrak p},N_{\mathfrak p}$ are free and one can indeed find such a transformation. Now define $[M:N]_R := \prod_i \mathfrak p_i^{v_{\mathfrak p_i}([M:N]_{\mathfrak p_i})}$. According to Froehlich, one can show that $M\cong N$ iff $[M:N]_R$ is a principal fractional ideal. One direction is easy. If we have a general linear transformation $\Psi$ of $U$ such that $\Psi (M)=N$, then $(\det \phi_{\mathfrak p_i}) = (\det \Psi_{\mathfrak p_i})$ for every $i$, so that $[M:N]_R =\prod_i \mathfrak p_i^{v_{\mathfrak p_i}([M:N]_{\mathfrak p_i})} = (\det \Psi)$, which is a principal fractional ideal. But how do I get the other direction?","Let $R$ be a Dedekind ring, $K$ its field of fractions, $U$ a finite vector space over $K$, and $M,N$ finitely generated $R$-modules that span $U$, i.e. contain a basis of $U$. For every $\mathfrak p \subset R$, define $[M:N]_{\mathfrak p} = (\det \phi_{\mathfrak p})$, where $\phi_{\mathfrak p}$ is a linear transformation that takes $M_{\mathfrak p}$ onto $N_{\mathfrak p}$. Recall here $R_{\mathfrak p}$ is a principal ideal domain $\Rightarrow$ $M_{\mathfrak p},N_{\mathfrak p}$ are free and one can indeed find such a transformation. Now define $[M:N]_R := \prod_i \mathfrak p_i^{v_{\mathfrak p_i}([M:N]_{\mathfrak p_i})}$. According to Froehlich, one can show that $M\cong N$ iff $[M:N]_R$ is a principal fractional ideal. One direction is easy. If we have a general linear transformation $\Psi$ of $U$ such that $\Psi (M)=N$, then $(\det \phi_{\mathfrak p_i}) = (\det \Psi_{\mathfrak p_i})$ for every $i$, so that $[M:N]_R =\prod_i \mathfrak p_i^{v_{\mathfrak p_i}([M:N]_{\mathfrak p_i})} = (\det \Psi)$, which is a principal fractional ideal. But how do I get the other direction?",,"['linear-algebra', 'commutative-algebra', 'algebraic-number-theory']"
34,Why is the inverse of a sum of matrices not the sum of their inverses?,Why is the inverse of a sum of matrices not the sum of their inverses?,,"Suppose $A + B$ is invertible, then is it true that $(A + B)^{-1} = A^{-1} + B^{-1}$? I know the answer is no, but don't get why.","Suppose $A + B$ is invertible, then is it true that $(A + B)^{-1} = A^{-1} + B^{-1}$? I know the answer is no, but don't get why.",,"['linear-algebra', 'matrices', 'inverse']"
35,"If $A^2$ is invertible, then $A$ is also invertible?","If  is invertible, then  is also invertible?",A^2 A,"True or False: If $A^2$ is invertible, then $A$ is also invertible. ($A$ is a matrix here.) The answer is true. I was trying to come up with an example that makes this false. But I couldn't. Could anybody help me prove this?","True or False: If $A^2$ is invertible, then $A$ is also invertible. ($A$ is a matrix here.) The answer is true. I was trying to come up with an example that makes this false. But I couldn't. Could anybody help me prove this?",,['linear-algebra']
36,Why is a square root not a linear transformation?,Why is a square root not a linear transformation?,,"The question says: Prove that the function $f(x)=\sqrt{x}$ is not a linear transformation (particularly $\sqrt{1+x^2}1+x$) I think that this is because the exponent of $\sqrt{x}$ is $1/2$, and with that exponent, $T(cu)=c^{1/2}T(u)$, which does not follow the rules of a linear transformation...","The question says: Prove that the function $f(x)=\sqrt{x}$ is not a linear transformation (particularly $\sqrt{1+x^2}1+x$) I think that this is because the exponent of $\sqrt{x}$ is $1/2$, and with that exponent, $T(cu)=c^{1/2}T(u)$, which does not follow the rules of a linear transformation...",,"['linear-algebra', 'self-learning', 'linear-transformations']"
37,Can an idempotent matrix be complex?,Can an idempotent matrix be complex?,,A matrix $A$ is called idempotent if $A^2 = A$. I am just wondering if such matrix can be complex. Anyone can help give an example or proof that it has to be real? Thanks!,A matrix $A$ is called idempotent if $A^2 = A$. I am just wondering if such matrix can be complex. Anyone can help give an example or proof that it has to be real? Thanks!,,['linear-algebra']
38,Are all algebraic commutative operations always associative? [duplicate],Are all algebraic commutative operations always associative? [duplicate],,This question already has answers here : Does commutativity imply Associativity? (13 answers) Closed 11 years ago . I know that there are many algebraic associative operations which are commutative and which are not commutative. for example multiplications of matrices as associative operation is not commutative. I need to know about inverse of this! I mean is there any algebraic commutative operation which is not associative? can you show me sample?,This question already has answers here : Does commutativity imply Associativity? (13 answers) Closed 11 years ago . I know that there are many algebraic associative operations which are commutative and which are not commutative. for example multiplications of matrices as associative operation is not commutative. I need to know about inverse of this! I mean is there any algebraic commutative operation which is not associative? can you show me sample?,,"['linear-algebra', 'abstract-algebra', 'examples-counterexamples', 'binary-operations', 'associativity']"
39,Matrices with $A^3+B^3=C^3$,Matrices with,A^3+B^3=C^3,"Problem: Find infinitely many triples of nonzero $3\times 3$ matrices $(A,B,C)$ over the nonnegative integers with $$A^3+B^3=C^3.$$ My proposed solution is in the answers.","Problem: Find infinitely many triples of nonzero $3\times 3$ matrices $(A,B,C)$ over the nonnegative integers with $$A^3+B^3=C^3.$$ My proposed solution is in the answers.",,"['linear-algebra', 'matrices']"
40,What is the motivation defining Matrix Similarity?,What is the motivation defining Matrix Similarity?,,"I'm taking the course Linear Algebra 1, and recently we've learned about matrix similarity. What is the motivation defining it? or, What are the uses/applications for this definition? Thanks","I'm taking the course Linear Algebra 1, and recently we've learned about matrix similarity. What is the motivation defining it? or, What are the uses/applications for this definition? Thanks",,"['linear-algebra', 'matrices']"
41,Proofs of determinants of block matrices [duplicate],Proofs of determinants of block matrices [duplicate],,"This question already has answers here : Determinant of a block lower triangular matrix (7 answers) Closed 7 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved I know that there are three important results when taking the Determinants of Block matrices $$\begin{align}\det \begin{bmatrix} A & B \\ 0 & D  \end{bmatrix} &= \det(A) \cdot \det(D) \ \ \ \ & (1) \\ \\ \det \begin{bmatrix} A & B \\ C & D  \end{bmatrix} &\neq AD - CB & (2) \\ \\ \det \begin{bmatrix} A & B \\ C & D  \end{bmatrix} &= \det \begin{bmatrix} A & B \\ 0 & D - CA^{-1}B \end{bmatrix} \\ \\ &= \underbrace{\det(A)\cdot \det\left(D-CA^{-1}B\right)}_\text{if $A^{-1}$ exists} \\ \\ &= \underbrace{\det\left(AD-CB\right)}_\text{if $AC=CA$} & (3) \end{align}$$ Now I understand in result $(3)$, that all that row operations are being performed to bring it into the form we see in $(1)$, but I can't seem to convince myself that result $(1)$ is true in the first place. Furthermore in result $(3)$, I understand that,  $\det(A)\cdot \det\left(D-CA^{-1}B\right) = \det\left(A(D-CA^{-1}B)\right)= \det(AD-CB)$, via the product rule for determinants I also understand that we need $A^{-1}$ to exist, for the initial row operation to reduce the matrix into an upper triangular form $U$, and I understand that we require $AC = CA$, to allow commutativity when we multiply $ACA^{-1}B$ to reduce it to $CB$. Can someone provide proofs for results $(1)$ and $(2)$, as I can't seem to find proofs for them in any of the textbooks I have at my disposal","This question already has answers here : Determinant of a block lower triangular matrix (7 answers) Closed 7 years ago . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved I know that there are three important results when taking the Determinants of Block matrices $$\begin{align}\det \begin{bmatrix} A & B \\ 0 & D  \end{bmatrix} &= \det(A) \cdot \det(D) \ \ \ \ & (1) \\ \\ \det \begin{bmatrix} A & B \\ C & D  \end{bmatrix} &\neq AD - CB & (2) \\ \\ \det \begin{bmatrix} A & B \\ C & D  \end{bmatrix} &= \det \begin{bmatrix} A & B \\ 0 & D - CA^{-1}B \end{bmatrix} \\ \\ &= \underbrace{\det(A)\cdot \det\left(D-CA^{-1}B\right)}_\text{if $A^{-1}$ exists} \\ \\ &= \underbrace{\det\left(AD-CB\right)}_\text{if $AC=CA$} & (3) \end{align}$$ Now I understand in result $(3)$, that all that row operations are being performed to bring it into the form we see in $(1)$, but I can't seem to convince myself that result $(1)$ is true in the first place. Furthermore in result $(3)$, I understand that,  $\det(A)\cdot \det\left(D-CA^{-1}B\right) = \det\left(A(D-CA^{-1}B)\right)= \det(AD-CB)$, via the product rule for determinants I also understand that we need $A^{-1}$ to exist, for the initial row operation to reduce the matrix into an upper triangular form $U$, and I understand that we require $AC = CA$, to allow commutativity when we multiply $ACA^{-1}B$ to reduce it to $CB$. Can someone provide proofs for results $(1)$ and $(2)$, as I can't seem to find proofs for them in any of the textbooks I have at my disposal",,"['linear-algebra', 'matrices', 'proof-explanation', 'determinant', 'block-matrices']"
42,Basis of the polynomial vector space,Basis of the polynomial vector space,,I don't understand how to find a basis for a polynomial vector space. Can someone help me with an example?,I don't understand how to find a basis for a polynomial vector space. Can someone help me with an example?,,"['linear-algebra', 'polynomials']"
43,Is the inverse operation on matrices distributive with respect to addition?,Is the inverse operation on matrices distributive with respect to addition?,,"For example, is the following true: $$(A + B)^{-1} = A^{-1} + B^{-1}$$ If $\det(A) \ne 0$, $\det(B) \ne 0$, and $\det(A + B) \ne 0$.","For example, is the following true: $$(A + B)^{-1} = A^{-1} + B^{-1}$$ If $\det(A) \ne 0$, $\det(B) \ne 0$, and $\det(A + B) \ne 0$.",,"['linear-algebra', 'matrices', 'inverse']"
44,Are all conjugacy classes in $\text{GL}_n(\mathbb R)$ path-connected?,Are all conjugacy classes in  path-connected?,\text{GL}_n(\mathbb R),"Suppose $A$ and $B$ are conjugate invertible real $n \times n$ -matrices. Does there always exist a path from $A$ to $B$ inside their conjugacy class? I thought I had an easy proof for odd $n$ which goes as follows, but it was incorrect as pointed out in this answer. To show where my misunderstanding arised, here is the wrong argument. Suppose there exists a real matrix $P$ such that $B = PAP^{-1}$ . By replacing $P$ with $-P$ if necessary, we can assume that $\det P > 0$ (this is what goes wrong in even dimensions, see this question). Then we have that $P = e^Q$ for some real matrix $Q$ (since the image of the exponential map is the path-component of the identity in $\text{GL}_n(\mathbb R)$ ). But now the path $$t \mapsto e^{tQ}Ae^{-tQ}$$ is a path connecting $A$ to $PAP^{-1} = B$ .","Suppose and are conjugate invertible real -matrices. Does there always exist a path from to inside their conjugacy class? I thought I had an easy proof for odd which goes as follows, but it was incorrect as pointed out in this answer. To show where my misunderstanding arised, here is the wrong argument. Suppose there exists a real matrix such that . By replacing with if necessary, we can assume that (this is what goes wrong in even dimensions, see this question). Then we have that for some real matrix (since the image of the exponential map is the path-component of the identity in ). But now the path is a path connecting to .",A B n \times n A B n P B = PAP^{-1} P -P \det P > 0 P = e^Q Q \text{GL}_n(\mathbb R) t \mapsto e^{tQ}Ae^{-tQ} A PAP^{-1} = B,"['linear-algebra', 'abstract-algebra', 'general-topology', 'matrices']"
45,Operator norm is equal to max eigenvalue,Operator norm is equal to max eigenvalue,,"Take a matrix $A \in M_{2 \times 2}(\mathbb{R})$ and consider the norm $\vert\vert A\vert\vert = \sup\limits_{x \in \mathbb{R}^2} \frac{ \vert\vert Ax\vert\vert}{\vert\vert x \vert\vert} = \sup\limits_{x, \vert\vert x \vert\vert = 1} \vert\vert Ax\vert\vert$ . I am unable to see that the norm must be less than or equal to the maximum eigenvalue of $A$ : $\vert\vert A\vert\vert \le \max\limits_{\lambda \in \sigma(A)} \lambda$ and I am also unable of characterizing the type of $A$ such that: $\vert\vert A\vert\vert = \max\limits_{\lambda \in \sigma(A)} \lambda$",Take a matrix and consider the norm . I am unable to see that the norm must be less than or equal to the maximum eigenvalue of : and I am also unable of characterizing the type of such that:,"A \in M_{2 \times 2}(\mathbb{R}) \vert\vert A\vert\vert = \sup\limits_{x \in \mathbb{R}^2} \frac{ \vert\vert Ax\vert\vert}{\vert\vert x \vert\vert} = \sup\limits_{x, \vert\vert x \vert\vert = 1} \vert\vert Ax\vert\vert A \vert\vert A\vert\vert \le \max\limits_{\lambda \in \sigma(A)} \lambda A \vert\vert A\vert\vert = \max\limits_{\lambda \in \sigma(A)} \lambda","['linear-algebra', 'matrices', 'normed-spaces']"
46,What are some meaningful connections between the minimal polynomial and other concepts in linear algebra?,What are some meaningful connections between the minimal polynomial and other concepts in linear algebra?,,"Ive found that the most effective way for me to deeply grasp mathematical concepts is to connect them to as many other concepts as I can. Unfortunately, Im seeing neither the importance nor the relevance of the minimal polynomial at all. Are there any significant connections between the minimal polynomial and linear algebra? Particularly regarding the relationship between the minimal polynomial of a linear operator on a vector space and other properties of that operator? I think the problem is partially due to my textbook's emphasis on crunching out as many theorems about a concept as possible, rather than explaining it on a deep level and demonstrating its importance. But part of it is definitely also that this concept is not clicking well for me. Thanks for any help.","Ive found that the most effective way for me to deeply grasp mathematical concepts is to connect them to as many other concepts as I can. Unfortunately, Im seeing neither the importance nor the relevance of the minimal polynomial at all. Are there any significant connections between the minimal polynomial and linear algebra? Particularly regarding the relationship between the minimal polynomial of a linear operator on a vector space and other properties of that operator? I think the problem is partially due to my textbook's emphasis on crunching out as many theorems about a concept as possible, rather than explaining it on a deep level and demonstrating its importance. But part of it is definitely also that this concept is not clicking well for me. Thanks for any help.",,"['linear-algebra', 'polynomials', 'operator-theory', 'linear-transformations', 'minimal-polynomials']"
47,Eigenvectors are linearly independent?,Eigenvectors are linearly independent?,,Theorem: Eigenvectors corresponding to distinct eigenvalues are linearly independent. Could someone give me a geometric interpretation of the theorem? Thanks!,Theorem: Eigenvectors corresponding to distinct eigenvalues are linearly independent. Could someone give me a geometric interpretation of the theorem? Thanks!,,"['linear-algebra', 'matrices', 'vector-spaces', 'eigenvalues-eigenvectors']"
48,Must a matrix of which all conjugates have zero diagonal be zero?,Must a matrix of which all conjugates have zero diagonal be zero?,,"Let $A$ be an $n \times n$ real matrix with the following property: All the conjugates of $A$ have only zeros on the diagonal. Does $A=0$? (By conjugates, I mean all the matrices similar to it, over $\mathbb{R}$, that is I require the conjugating matrix to be real). Of course, if $A$ is diagonalizable, then clearly it must be zero. The only idea I have is to use the Jordan form for real matrices , but after some thought I am not sure this is a good approach.","Let $A$ be an $n \times n$ real matrix with the following property: All the conjugates of $A$ have only zeros on the diagonal. Does $A=0$? (By conjugates, I mean all the matrices similar to it, over $\mathbb{R}$, that is I require the conjugating matrix to be real). Of course, if $A$ is diagonalizable, then clearly it must be zero. The only idea I have is to use the Jordan form for real matrices , but after some thought I am not sure this is a good approach.",,"['linear-algebra', 'matrices', 'matrix-equations', 'matrix-calculus']"
49,Minimal polynomial of restriction to invariant subspace divides minimal polynomial,Minimal polynomial of restriction to invariant subspace divides minimal polynomial,,I'm trying to prove this: $T : V \to V$ linear transformation. $W$ subspace of $V$. If $W$ is $T$-invariant then the minimal polynomial for the restriction operator $T|_W$ divides the minimal polynomial for $T$.,I'm trying to prove this: $T : V \to V$ linear transformation. $W$ subspace of $V$. If $W$ is $T$-invariant then the minimal polynomial for the restriction operator $T|_W$ divides the minimal polynomial for $T$.,,"['linear-algebra', 'linear-transformations', 'minimal-polynomials']"
50,"Orthogonal projections with $\sum P_i =I$, proving that $i\ne j \Rightarrow  P_{j}P_{i}=0$","Orthogonal projections with , proving that",\sum P_i =I i\ne j \Rightarrow  P_{j}P_{i}=0,"I am reading Introduction to Quantum Computing by Kaye, Laflamme, and Mosca. As an exercise, they write ""Prove that if the operators $P_{i}$ satisfy $P_{i}^{*}=P_{i}$ and $P_{i}^{2}=P_{i}$ , then $P_{i}P_{j}=0$ for $i\ne j$.'' In the context of this problem, it has been assumed that $I=\sum_{i=1}^{n} P_{i}$, where I suppose that $n$ could be infinite. I have shown that this is true in the trivial case $n=2$, but the general case has been eluding me. How should I attack this?","I am reading Introduction to Quantum Computing by Kaye, Laflamme, and Mosca. As an exercise, they write ""Prove that if the operators $P_{i}$ satisfy $P_{i}^{*}=P_{i}$ and $P_{i}^{2}=P_{i}$ , then $P_{i}P_{j}=0$ for $i\ne j$.'' In the context of this problem, it has been assumed that $I=\sum_{i=1}^{n} P_{i}$, where I suppose that $n$ could be infinite. I have shown that this is true in the trivial case $n=2$, but the general case has been eluding me. How should I attack this?",,"['linear-algebra', 'quantum-mechanics']"
51,Intuition for cross product of vector with itself and vector with zero vector,Intuition for cross product of vector with itself and vector with zero vector,,"I'm having trouble intuiting the following two vector identities for any vector $\mathbf{v}$. I'm only asking about intuition here, and not about their proofs (which follow from definition of cross product): $\color{green}{\mathbf{v}} \times \color{brown}{\mathbf{v}} = \mathbf{0} \tag{*}$ $\mathbf{v} \times \mathbf{0} = \mathbf{0} \tag{*}$ For (*), my intuition is that we want a vector that's perpendicular to both $\color{green}{\mathbf{v}}$ and $\color{brown}{\mathbf{v}}$. But this is the same vector, written out two times. Therefore, we want a vector that's perpendicular to just $\mathbf{v}$. Wouldn't there be infinitely many vectors that are perpendicular to any one vector? Why is it $\mathbf{0}$? For (**), my intuition is that we want a vector that's perpendicular to both $\mathbf{v}$ and $\mathbf{0}$. Since $\mathbf{0}$ has magnitude $0$, therefore it doesn't exist ""physically"", so no vector can be perpendicular to it. I'm not sure about this, though.","I'm having trouble intuiting the following two vector identities for any vector $\mathbf{v}$. I'm only asking about intuition here, and not about their proofs (which follow from definition of cross product): $\color{green}{\mathbf{v}} \times \color{brown}{\mathbf{v}} = \mathbf{0} \tag{*}$ $\mathbf{v} \times \mathbf{0} = \mathbf{0} \tag{*}$ For (*), my intuition is that we want a vector that's perpendicular to both $\color{green}{\mathbf{v}}$ and $\color{brown}{\mathbf{v}}$. But this is the same vector, written out two times. Therefore, we want a vector that's perpendicular to just $\mathbf{v}$. Wouldn't there be infinitely many vectors that are perpendicular to any one vector? Why is it $\mathbf{0}$? For (**), my intuition is that we want a vector that's perpendicular to both $\mathbf{v}$ and $\mathbf{0}$. Since $\mathbf{0}$ has magnitude $0$, therefore it doesn't exist ""physically"", so no vector can be perpendicular to it. I'm not sure about this, though.",,[]
52,What's the meaning of the transpose? [duplicate],What's the meaning of the transpose? [duplicate],,"This question already has answers here : What is the geometric interpretation of the transpose? (5 answers) Closed 9 years ago . I don't understand the motivation of the transpose (or better yet, I haven't even seen one). It feels like just something pulled out of a hat. Thinking about it makes it seem  like a product of being able to write a matrix using either columns or rows 'first'. E.g., when we 'reflect down the diagonal' we are really keeping all the information of our old matrix, just changing rows to columns. This is much like how $f \text{  from} \{1,2,...,m-1,m\} \times \{1,2,...,n\} \text{ to some field } \mathbf{F}$ is an $m \times n$ matrix,  if we switch the order of the product, so that we would define: $g \text{ is a map from } \{1,2,...,n\} \times \{1,2,...,m - 1, m\} \text{ to } \mathbf{F}$  but allow $f(i,j) = g(j,i)$. Then we somehow get a 'natural' map from the space of $m \times n$ matrices to $n \times m$ matrices. Is that what transpose is - a 'natural' map for those spaces? What do I even mean by natural here (serious question, I'm not being mysterious)? If I had to guess, it is a linear bijective map? Is that close enough? Moving away from this, does the transpose have any useful application in Euclidean geometry (other than orthogonal matrices being defined in terms of transposes?).","This question already has answers here : What is the geometric interpretation of the transpose? (5 answers) Closed 9 years ago . I don't understand the motivation of the transpose (or better yet, I haven't even seen one). It feels like just something pulled out of a hat. Thinking about it makes it seem  like a product of being able to write a matrix using either columns or rows 'first'. E.g., when we 'reflect down the diagonal' we are really keeping all the information of our old matrix, just changing rows to columns. This is much like how $f \text{  from} \{1,2,...,m-1,m\} \times \{1,2,...,n\} \text{ to some field } \mathbf{F}$ is an $m \times n$ matrix,  if we switch the order of the product, so that we would define: $g \text{ is a map from } \{1,2,...,n\} \times \{1,2,...,m - 1, m\} \text{ to } \mathbf{F}$  but allow $f(i,j) = g(j,i)$. Then we somehow get a 'natural' map from the space of $m \times n$ matrices to $n \times m$ matrices. Is that what transpose is - a 'natural' map for those spaces? What do I even mean by natural here (serious question, I'm not being mysterious)? If I had to guess, it is a linear bijective map? Is that close enough? Moving away from this, does the transpose have any useful application in Euclidean geometry (other than orthogonal matrices being defined in terms of transposes?).",,"['linear-algebra', 'matrices']"
53,Limit of power of Markov matrix,Limit of power of Markov matrix,,"Recently, a friend gave me the following problem: Let $$M = \begin{pmatrix} p & q & r \\ q & r & p \\ r & p & q \end{pmatrix}$$ where $p, q, r > 0$ and $p + q + r = 1$. Prove that $$\lim_{n \rightarrow \infty} M^n = \begin{pmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{pmatrix}$$ Some of my observations include that $M^{2^n}$ remains cyclic, so I tried to bound (with no luck) the maximum and minimum to see if they both converged to $1/3$. It would be nice if someone could post an elementary solution to this problem.","Recently, a friend gave me the following problem: Let $$M = \begin{pmatrix} p & q & r \\ q & r & p \\ r & p & q \end{pmatrix}$$ where $p, q, r > 0$ and $p + q + r = 1$. Prove that $$\lim_{n \rightarrow \infty} M^n = \begin{pmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{pmatrix}$$ Some of my observations include that $M^{2^n}$ remains cyclic, so I tried to bound (with no luck) the maximum and minimum to see if they both converged to $1/3$. It would be nice if someone could post an elementary solution to this problem.",,"['linear-algebra', 'matrices', 'limits', 'markov-chains']"
54,"Given a perturbation of a symmetric matrix, find an expansion for the eigenvalues","Given a perturbation of a symmetric matrix, find an expansion for the eigenvalues",,"Let $A$ be a real, symmetrix $n\times n$ matrix with $n$ distinct,   non-zero eigenvalues, and let $V$ be a real, symmetric $n\times n$   matrix. Consider $A_{\varepsilon}=A+\varepsilon V$, a perturbation of $A$, where   $\varepsilon$ is a small number. Find an expansion as an $\varepsilon$-series for the eigenvalues and   eigenvectors of $A_{\varepsilon}$ around the eigenvalues and   eigenvectors of the original matrix, $A$. Assume that $\epsilon$ is sufficiently small that one can neglect   all terms of order $\varepsilon^2$ and higher in the expansion. How   close are the eigenvalues of $A_{\varepsilon}$ to those of $A$? What   about the eigenvectors? What can we say about the eigenvalues and eigenvectors of   $A_{\varepsilon} $ if $A$ and $V$ are arbitrary $n\times n$ matrices?   You may assume that $\det A \neq 0$. Ideas : If $A$ and $V$ commute (thus are simultaneously diagonalizable), there are just the linear term $\varepsilon v_{ii}$, when we look with the right basis. I'm getting the feeling I really don't know what they want me to do with this problem. Any ideas?","Let $A$ be a real, symmetrix $n\times n$ matrix with $n$ distinct,   non-zero eigenvalues, and let $V$ be a real, symmetric $n\times n$   matrix. Consider $A_{\varepsilon}=A+\varepsilon V$, a perturbation of $A$, where   $\varepsilon$ is a small number. Find an expansion as an $\varepsilon$-series for the eigenvalues and   eigenvectors of $A_{\varepsilon}$ around the eigenvalues and   eigenvectors of the original matrix, $A$. Assume that $\epsilon$ is sufficiently small that one can neglect   all terms of order $\varepsilon^2$ and higher in the expansion. How   close are the eigenvalues of $A_{\varepsilon}$ to those of $A$? What   about the eigenvectors? What can we say about the eigenvalues and eigenvectors of   $A_{\varepsilon} $ if $A$ and $V$ are arbitrary $n\times n$ matrices?   You may assume that $\det A \neq 0$. Ideas : If $A$ and $V$ commute (thus are simultaneously diagonalizable), there are just the linear term $\varepsilon v_{ii}$, when we look with the right basis. I'm getting the feeling I really don't know what they want me to do with this problem. Any ideas?",,"['linear-algebra', 'matrices', 'power-series']"
55,What is the minimum and maximum number of eigenvectors?,What is the minimum and maximum number of eigenvectors?,,"I am given the eigenvalues of a square, 8x8, matrix. They are all non-zero. I have determined that the matrix is diagonalizable and has an inverse. In one part of the problem, I am asked to find the maximum and minimum number of eigenvectors that the matrix could possibly have? Since A is diagonalizable does that mean it will have n linearly independent eigenvectors. So, is the max and min number of eigenvectors is 8?","I am given the eigenvalues of a square, 8x8, matrix. They are all non-zero. I have determined that the matrix is diagonalizable and has an inverse. In one part of the problem, I am asked to find the maximum and minimum number of eigenvectors that the matrix could possibly have? Since A is diagonalizable does that mean it will have n linearly independent eigenvectors. So, is the max and min number of eigenvectors is 8?",,"['linear-algebra', 'numerical-linear-algebra']"
56,Can you equip every vector space with a Hilbert space structure?,Can you equip every vector space with a Hilbert space structure?,,"Suppose that we have a vector space $X$ over the field $\mathbb F \in \{ \mathbb R, \mathbb C \}$. Question: Does there exist a Hilbert space $\widehat X$ over $\mathbb F$ such that $\widehat X$, when we forget the Hilbert space structure, is the same as $X$ in the category of vector spaces? Which criteria hold for the existence of such a Hilbert space, on which choices do we have? My motivation is to better understand which structure we gain from the Hilbert space setting (and topological vector spaces in general).","Suppose that we have a vector space $X$ over the field $\mathbb F \in \{ \mathbb R, \mathbb C \}$. Question: Does there exist a Hilbert space $\widehat X$ over $\mathbb F$ such that $\widehat X$, when we forget the Hilbert space structure, is the same as $X$ in the category of vector spaces? Which criteria hold for the existence of such a Hilbert space, on which choices do we have? My motivation is to better understand which structure we gain from the Hilbert space setting (and topological vector spaces in general).",,"['functional-analysis', 'linear-algebra', 'set-theory']"
57,What's a good reference to study multilinear algebra?,What's a good reference to study multilinear algebra?,,"This semester I'm taking a course in linear algebra and now at the end of the course we came to study the tensor product and multilinear algebra in general. I've already studied this theme in the past through Kostrikin's ""Linear Algebra and Geometry"", but I'm not sure this is enough. My teacher didn't know what to recommend as textbook for this part of the course and he could just recommend one book that does everything in modules. Now, it's not that I'm not interested in modules, it's just that until today I've never dealt with them, so it's a little confusing to study the tensor product on that book. In that case, what's a good reference to study multilinear algebra done in vector spaces? Is Kostrikin's book enough, or should I get other book to study this? Thanks very much in advance!","This semester I'm taking a course in linear algebra and now at the end of the course we came to study the tensor product and multilinear algebra in general. I've already studied this theme in the past through Kostrikin's ""Linear Algebra and Geometry"", but I'm not sure this is enough. My teacher didn't know what to recommend as textbook for this part of the course and he could just recommend one book that does everything in modules. Now, it's not that I'm not interested in modules, it's just that until today I've never dealt with them, so it's a little confusing to study the tensor product on that book. In that case, what's a good reference to study multilinear algebra done in vector spaces? Is Kostrikin's book enough, or should I get other book to study this? Thanks very much in advance!",,"['linear-algebra', 'reference-request', 'book-recommendation', 'multilinear-algebra']"
58,Why the nontrivial nullspace of a functional has codimension 1?,Why the nontrivial nullspace of a functional has codimension 1?,,"The nullspace of a linear functional that is not $\equiv 0$ is a linear subspace of codimension $1$ . I don't understand this statement on page 57, Functional Analysis (Pater Lax). Does it mean the dimension of nullspace of a linear functional is either zero or the dimension of the domain of the functional minus one, which I don't see why it's necessarily true. Added: Thank you all for your valuable comments and answers. I didn't realize that it's wrong to interpret  ""The nullspace of a linear functional that is not $\equiv 0$ "" as the nullspace (of a linear functional) that is not $\equiv 0$ until I saw the answers.","The nullspace of a linear functional that is not is a linear subspace of codimension . I don't understand this statement on page 57, Functional Analysis (Pater Lax). Does it mean the dimension of nullspace of a linear functional is either zero or the dimension of the domain of the functional minus one, which I don't see why it's necessarily true. Added: Thank you all for your valuable comments and answers. I didn't realize that it's wrong to interpret  ""The nullspace of a linear functional that is not "" as the nullspace (of a linear functional) that is not until I saw the answers.",\equiv 0 1 \equiv 0 \equiv 0,"['linear-algebra', 'functional-analysis']"
59,What is the intuition behind the trace of an endomorphism?,What is the intuition behind the trace of an endomorphism?,,"Looking back on my math education, I noticed that even though the trace of an endomorphism came up a lot, I'd be hard pressed to give a good description of what the trace really means. I'm looking for some good intuition about its meaning. To elaborate on what I'm looking for: if I forgot the rigorous definition of the determinant, I could rebuild it from scratch without reference to a basis because I know that it's supposed to measure the change in volume and orientation of a parallelepiped under a linear transformation. For the same reason, I can quickly tell that it is independent of basis, multiplicative, and determines wether the endomorphism is injective or not, all without doing any calculations. I want something similar for the trace. It doesn't need to be geometric, but I want to know what the trace tells us about how the endomorphism acts .","Looking back on my math education, I noticed that even though the trace of an endomorphism came up a lot, I'd be hard pressed to give a good description of what the trace really means. I'm looking for some good intuition about its meaning. To elaborate on what I'm looking for: if I forgot the rigorous definition of the determinant, I could rebuild it from scratch without reference to a basis because I know that it's supposed to measure the change in volume and orientation of a parallelepiped under a linear transformation. For the same reason, I can quickly tell that it is independent of basis, multiplicative, and determines wether the endomorphism is injective or not, all without doing any calculations. I want something similar for the trace. It doesn't need to be geometric, but I want to know what the trace tells us about how the endomorphism acts .",,"['linear-algebra', 'intuition', 'trace']"
60,How does the homogenization of a curve using a given line work?,How does the homogenization of a curve using a given line work?,,"I am given a curve  $$C_1:2x^2 +3y^2 =5$$  and a line  $$L_1: 3x-4y=5$$  and I needed to find curve  joining the origin and the points of intersection of $C_1$ and $L_1$ so I was told to ""homogenize"" the line with the curve . They basically said that the required  curve would be $$ 2x^2 +3y^2 -5\left(\frac{3x-4y}{5}\right)^2=0$$ What is this ? And how does this give required curve??","I am given a curve  $$C_1:2x^2 +3y^2 =5$$  and a line  $$L_1: 3x-4y=5$$  and I needed to find curve  joining the origin and the points of intersection of $C_1$ and $L_1$ so I was told to ""homogenize"" the line with the curve . They basically said that the required  curve would be $$ 2x^2 +3y^2 -5\left(\frac{3x-4y}{5}\right)^2=0$$ What is this ? And how does this give required curve??",,"['linear-algebra', 'analytic-geometry', 'conic-sections', 'curves']"
61,Proving that two systems of linear equations are equivalent if they have the same solutions,Proving that two systems of linear equations are equivalent if they have the same solutions,,"I've just begun to work learn Linear Algebra on my own through Hoffman and Kunze's book and the first problem set already has a question that I can't solve: Prove that if two homogeneous systems of linear equations in two unknowns have the same solutions, then they are equivalent. I can't seem to figure out how to prove this without resorting to case work where you account for the cases where one of the coefficients are zero and when both are. Is there an elegant way to prove in general that when two systems of linear equations have the same solutions, they are equivalent? The converse is obvious enough though. Definition of equivalence from the text : Let us say that two systems of linear equations are equivalent if each equation in each system is a linear combination of the equations   in the other system.","I've just begun to work learn Linear Algebra on my own through Hoffman and Kunze's book and the first problem set already has a question that I can't solve: Prove that if two homogeneous systems of linear equations in two unknowns have the same solutions, then they are equivalent. I can't seem to figure out how to prove this without resorting to case work where you account for the cases where one of the coefficients are zero and when both are. Is there an elegant way to prove in general that when two systems of linear equations have the same solutions, they are equivalent? The converse is obvious enough though. Definition of equivalence from the text : Let us say that two systems of linear equations are equivalent if each equation in each system is a linear combination of the equations   in the other system.",,"['linear-algebra', 'systems-of-equations']"
62,"Solving very large matrices in ""pieces""","Solving very large matrices in ""pieces""",,"Say you have a very dense matrix that is 30000x30000 elements. The very dense matrix comes from the radiosity equation, which I discussed here . Say you have Ax = B.  You have B, and A is 30000x30000 elements.  Solving:  You need to find x given B. Solving a matrix like this in C code actually introduces an interesting problem:  you cannot actually store it all in memory at once. So, I want to know about ways to solve this problem ""in pieces"" -- I want to mathematically decompose the 30000x30000 matrix into smaller sub-matrices (perhaps in chunks of 30000x20?), and somehow solve that way. What algorithms exist to break down / solve a matrix ""in pieces"" or steps, that can be solved regardless of memory restrictions?  It can be an iterative technique.","Say you have a very dense matrix that is 30000x30000 elements. The very dense matrix comes from the radiosity equation, which I discussed here . Say you have Ax = B.  You have B, and A is 30000x30000 elements.  Solving:  You need to find x given B. Solving a matrix like this in C code actually introduces an interesting problem:  you cannot actually store it all in memory at once. So, I want to know about ways to solve this problem ""in pieces"" -- I want to mathematically decompose the 30000x30000 matrix into smaller sub-matrices (perhaps in chunks of 30000x20?), and somehow solve that way. What algorithms exist to break down / solve a matrix ""in pieces"" or steps, that can be solved regardless of memory restrictions?  It can be an iterative technique.",,"['linear-algebra', 'algorithms', 'matrices']"
63,Relations between matrix norm and determinant,Relations between matrix norm and determinant,,"I was wondering whether there is a way to obtain the determinant of a matrix out of its norm (when the matrix is regular otherwise it is not true). If $A$ is a square matrix of dimension $n\geq 1$ , and $\det A\neq 0$ , do we have something like? $$|Ax|_2 \leq \|A\|_{\text{op}} |x|_2 \leq C_n |\det A| \ |x|_2 \leq \cdots$$ or similar? Or maybe a similar estimate for different norms for $A$ and $x$ if needed since many matrix norms are related to eigen- or singular values which are related to the determinant. Thanks a lot! :)","I was wondering whether there is a way to obtain the determinant of a matrix out of its norm (when the matrix is regular otherwise it is not true). If is a square matrix of dimension , and , do we have something like? or similar? Or maybe a similar estimate for different norms for and if needed since many matrix norms are related to eigen- or singular values which are related to the determinant. Thanks a lot! :)",A n\geq 1 \det A\neq 0 |Ax|_2 \leq \|A\|_{\text{op}} |x|_2 \leq C_n |\det A| \ |x|_2 \leq \cdots A x,"['linear-algebra', 'matrices', 'inequality', 'determinant', 'normed-spaces']"
64,"Suppose $A,B\in M_n(\mathbb{C})$ such that $AB-BA=A $ . Prove that $A$ is not invertible .",Suppose  such that  . Prove that  is not invertible .,"A,B\in M_n(\mathbb{C}) AB-BA=A  A","Suppose that $A,B\in M_n(\mathbb{C})$ such that $AB-BA=A $. Prove that $A$ is not invertible. My work: Suppose $A$ is invertible. Then $ABA^{-1}=I+B$ . So $B$ is similar to $I+B$  .Let $B$ have eigenvalues $c_1,c_2,\ldots,c_n \in \mathbb{C}$. So $B$ has basis such that $B$ is upper triangular with respect to it and  has $c_1,\ldots,c_n$ as diagonal entries . It is easy to see that $I+B$ is upper-triangular with respect to this basis and has entries $1+c_1,\ldots, 1+c_n$ . Hence $$c_1+c_2+\ldots +c_n=\operatorname{trace}(B)$$ $$=\operatorname{trace}(I+B)=(1+c_1)+\ldots+(1+c_n)=n+c_1+\ldots +c_n .$$ So $n=0$, contradiction. I'm not sure if my solution is correct one. It seems alright. I will be very thankful if you can confirm that the proof indeed is a correct one. Any other possible solutions are welcomed.","Suppose that $A,B\in M_n(\mathbb{C})$ such that $AB-BA=A $. Prove that $A$ is not invertible. My work: Suppose $A$ is invertible. Then $ABA^{-1}=I+B$ . So $B$ is similar to $I+B$  .Let $B$ have eigenvalues $c_1,c_2,\ldots,c_n \in \mathbb{C}$. So $B$ has basis such that $B$ is upper triangular with respect to it and  has $c_1,\ldots,c_n$ as diagonal entries . It is easy to see that $I+B$ is upper-triangular with respect to this basis and has entries $1+c_1,\ldots, 1+c_n$ . Hence $$c_1+c_2+\ldots +c_n=\operatorname{trace}(B)$$ $$=\operatorname{trace}(I+B)=(1+c_1)+\ldots+(1+c_n)=n+c_1+\ldots +c_n .$$ So $n=0$, contradiction. I'm not sure if my solution is correct one. It seems alright. I will be very thankful if you can confirm that the proof indeed is a correct one. Any other possible solutions are welcomed.",,"['linear-algebra', 'abstract-algebra', 'matrices', 'proof-verification', 'eigenvalues-eigenvectors']"
65,"CSB inquality: is $\|x\|^2\|y\|^2 - \langle x,y \rangle^2$ a square in any obvious way?",CSB inquality: is  a square in any obvious way?,"\|x\|^2\|y\|^2 - \langle x,y \rangle^2","Suppose $x=(x_1,x_2),y = (y_1,y_2) \in \mathbb{R}^2$. I noticed that \begin{align*} \|x\|^2 \|y\|^2 - \langle x,y \rangle^2 &= x_1^2y_1^2 + x_1^2 y_2^2 + x_2^2 y_1^2 + x_2^2 y_2 ^2 - (x_1^2 y_1^2 + 2 x_1 y_1 x_2 y_2 + x_2^2 y_2^2) \\ &=(x_1 y_2)^2 - 2x_1 y_2 x_2 y_1 + (x_2 y_2)^2 \\ &=(x_1 y_2 - x_2 y_1)^2 \end{align*} which proves the CSB inequality in dimension two. This begs the question: If $x = (x_1,\ldots,x_n),y=(y_1,\ldots,y_n) \in \mathbb{R}^n$, is there a polynomial $p \in \mathbb{R}[x_1,\ldots,x_n;y_1,\ldots,y_n]$ such that $ \|x\|^2 \|y\|^2 - \langle x,y \rangle^2 = p^2$?","Suppose $x=(x_1,x_2),y = (y_1,y_2) \in \mathbb{R}^2$. I noticed that \begin{align*} \|x\|^2 \|y\|^2 - \langle x,y \rangle^2 &= x_1^2y_1^2 + x_1^2 y_2^2 + x_2^2 y_1^2 + x_2^2 y_2 ^2 - (x_1^2 y_1^2 + 2 x_1 y_1 x_2 y_2 + x_2^2 y_2^2) \\ &=(x_1 y_2)^2 - 2x_1 y_2 x_2 y_1 + (x_2 y_2)^2 \\ &=(x_1 y_2 - x_2 y_1)^2 \end{align*} which proves the CSB inequality in dimension two. This begs the question: If $x = (x_1,\ldots,x_n),y=(y_1,\ldots,y_n) \in \mathbb{R}^n$, is there a polynomial $p \in \mathbb{R}[x_1,\ldots,x_n;y_1,\ldots,y_n]$ such that $ \|x\|^2 \|y\|^2 - \langle x,y \rangle^2 = p^2$?",,"['linear-algebra', 'abstract-algebra', 'inequality']"
66,Notation for element-wise division of vectors,Notation for element-wise division of vectors,,"I am wondering if there is any standard notation for the element-wise division of vectors. I am going to use $\oslash$ for this purpose, similar to $\odot$ that is used in some texts for element-wise multiplication. For example, assuming $\vec{u}$ and $\vec{v}$ are vectors of length $k$, then $\vec{x} = \vec{u} \oslash \vec{v}$ if $\vec{x}(i) = \vec{u}(i) / \vec{v}(i)$. Would that be strange to use this in a scientific paper?","I am wondering if there is any standard notation for the element-wise division of vectors. I am going to use $\oslash$ for this purpose, similar to $\odot$ that is used in some texts for element-wise multiplication. For example, assuming $\vec{u}$ and $\vec{v}$ are vectors of length $k$, then $\vec{x} = \vec{u} \oslash \vec{v}$ if $\vec{x}(i) = \vec{u}(i) / \vec{v}(i)$. Would that be strange to use this in a scientific paper?",,"['linear-algebra', 'notation']"
67,Inverse of the sum of a invertible matrix with known Cholesky-decomposion and diagonal matrix,Inverse of the sum of a invertible matrix with known Cholesky-decomposion and diagonal matrix,,"I want to  ask a question about invertible matrix. Suppose there is a $n\times n$ symmetric and invertible matrix $M$, and we know its Cholesky decomposion as $M=LL'$. Then do we have an efficient way to calculate $(M+D)^{-1}$, where $D=diag(d_1,...,d_n)$ with positive diagonal entries, by taking the information of $M=LL'$ rather than calculating from scratch with $M+D$ directly? Or what if for the sepcial case $D=dI_n$? Thanks a lot!","I want to  ask a question about invertible matrix. Suppose there is a $n\times n$ symmetric and invertible matrix $M$, and we know its Cholesky decomposion as $M=LL'$. Then do we have an efficient way to calculate $(M+D)^{-1}$, where $D=diag(d_1,...,d_n)$ with positive diagonal entries, by taking the information of $M=LL'$ rather than calculating from scratch with $M+D$ directly? Or what if for the sepcial case $D=dI_n$? Thanks a lot!",,"['linear-algebra', 'matrices', 'inverse', 'matrix-decomposition', 'cholesky-decomposition']"
68,"What is the agreed upon definition of a ""positive definite matrix""?","What is the agreed upon definition of a ""positive definite matrix""?",,"In here: http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/positive-definite-matrices-and-applications/symmetric-matrices-and-positive-definiteness/MIT18_06SCF11_Ses3.1sum.pdf A positive definite matrix is a symmetric matrix A for which all   eigenvalues are positive. - Gilbert Strang I have heard of positive definite quadratic forms, but never heard of definiteness for a matrix. Because definiteness is higher dimensional analogy for whether if something is convex (opening up) or concave (opening down). It does not make sense to me to say a matrix is opening up, or matrix is opening down. Therefore it does not make sense to say that a matrix has definiteness. In addition, when we say $M \in \mathbb{R}^{n \times n}$ positive definite, what is the first thing we do? We plug $M$ into a function(al) $x^T (\cdot) x$ and check whether the function is positive for all $x    \in \mathbb{R}^n$. Clearly, that means we are defining this definiteness with respect to $x^T (\cdot) x$ and NOT $M$ itself. Furthermore, when matrix have complex eigenvalues, then we ditch the notion of definiteness property all together. Clearly, definiteness is a flimsy property for matrices if we can just throw it away when it becomes inconvenient. I will grant you that if we were to define positive definite matrices, we should only define with respect to symmetric matrices. This is the definition on Wikipedia, the definition used by numerous linear algebra books and many applied math books. But then when confronted with a matrix of the form $$\begin{bmatrix} 1 & -1 \\ 0  & 1 \end{bmatrix}$$ I still firmly believe that this matrix is not positive definite because it is not symmetric. Because to me positive definiteness implies symmetry. To what degree is it widely agreed upon in the math community that a positive definite matrix is defined strictly with respect to symmetric matrices and why only with respect to symmetric matrices?","In here: http://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/positive-definite-matrices-and-applications/symmetric-matrices-and-positive-definiteness/MIT18_06SCF11_Ses3.1sum.pdf A positive definite matrix is a symmetric matrix A for which all   eigenvalues are positive. - Gilbert Strang I have heard of positive definite quadratic forms, but never heard of definiteness for a matrix. Because definiteness is higher dimensional analogy for whether if something is convex (opening up) or concave (opening down). It does not make sense to me to say a matrix is opening up, or matrix is opening down. Therefore it does not make sense to say that a matrix has definiteness. In addition, when we say $M \in \mathbb{R}^{n \times n}$ positive definite, what is the first thing we do? We plug $M$ into a function(al) $x^T (\cdot) x$ and check whether the function is positive for all $x    \in \mathbb{R}^n$. Clearly, that means we are defining this definiteness with respect to $x^T (\cdot) x$ and NOT $M$ itself. Furthermore, when matrix have complex eigenvalues, then we ditch the notion of definiteness property all together. Clearly, definiteness is a flimsy property for matrices if we can just throw it away when it becomes inconvenient. I will grant you that if we were to define positive definite matrices, we should only define with respect to symmetric matrices. This is the definition on Wikipedia, the definition used by numerous linear algebra books and many applied math books. But then when confronted with a matrix of the form $$\begin{bmatrix} 1 & -1 \\ 0  & 1 \end{bmatrix}$$ I still firmly believe that this matrix is not positive definite because it is not symmetric. Because to me positive definiteness implies symmetry. To what degree is it widely agreed upon in the math community that a positive definite matrix is defined strictly with respect to symmetric matrices and why only with respect to symmetric matrices?",,"['linear-algebra', 'matrices', 'vector-spaces', 'definition', 'positive-definite']"
69,"Is the zero matrix the only symmetric, nilpotent matrix with real values?","Is the zero matrix the only symmetric, nilpotent matrix with real values?",,"My intuition tells me that the zero matrix is the only matrix that is symmetric and nilpotent with real values, but I'm having trouble proving it (or finding a counterexample.) I have searched for related problems, but I've found only one where nilpotent was defined as any matrix $A$ where $A^2=0$; using this definition, the problem is pretty easy.  I'm using the more general definition that $A$ is nilpotent if and only if there exists a positive integer $k$ such that $A^k=0$. Based on my observations while trying to find a counterexample, I've been trying to formulate some argument about the positive semi-definiteness of the entries on the main diagonal, but I'm not getting very far with it.  Is this the right approach?  Is my gut feeling even true?","My intuition tells me that the zero matrix is the only matrix that is symmetric and nilpotent with real values, but I'm having trouble proving it (or finding a counterexample.) I have searched for related problems, but I've found only one where nilpotent was defined as any matrix $A$ where $A^2=0$; using this definition, the problem is pretty easy.  I'm using the more general definition that $A$ is nilpotent if and only if there exists a positive integer $k$ such that $A^k=0$. Based on my observations while trying to find a counterexample, I've been trying to formulate some argument about the positive semi-definiteness of the entries on the main diagonal, but I'm not getting very far with it.  Is this the right approach?  Is my gut feeling even true?",,"['linear-algebra', 'matrices']"
70,"What makes elementary row operations ""special""?","What makes elementary row operations ""special""?",,"This is probably a stupid question, but what makes the three magical elementary row operations, as taught in elementary linear algebra courses, special? In other words, in what way are they ""natural"" (as opposed to ""arbitrary"")? It seems that they're always presented in a somewhat haphazard manner (""these are the three legendary elementary row operations, don't ask why, they just are""). From what I understand, they satisfy some nice properties, such as the inverse of each being an operation of the same type, etc. But is there something that characterizes them, i.e. is there some definition of what constitutes ""elementary"" that's only satisfied by the three types of elementary matrices, and no other matrix?","This is probably a stupid question, but what makes the three magical elementary row operations, as taught in elementary linear algebra courses, special? In other words, in what way are they ""natural"" (as opposed to ""arbitrary"")? It seems that they're always presented in a somewhat haphazard manner (""these are the three legendary elementary row operations, don't ask why, they just are""). From what I understand, they satisfy some nice properties, such as the inverse of each being an operation of the same type, etc. But is there something that characterizes them, i.e. is there some definition of what constitutes ""elementary"" that's only satisfied by the three types of elementary matrices, and no other matrix?",,"['linear-algebra', 'matrices']"
71,Can I find the connected components of a graph using matrix operations on the graph's adjacency matrix?,Can I find the connected components of a graph using matrix operations on the graph's adjacency matrix?,,"If I have an adjacency matrix for a graph, can I do a series of matrix operations on the adjacency matrix to find the connected components of the graph?","If I have an adjacency matrix for a graph, can I do a series of matrix operations on the adjacency matrix to find the connected components of the graph?",,"['linear-algebra', 'graph-theory', 'connectedness', 'algebraic-graph-theory', 'spectral-graph-theory']"
72,"Why is addition defined, and not implied, on quotient spaces?","Why is addition defined, and not implied, on quotient spaces?",,"Small question. In chapter 3, section E, page 96 of ""Linear Algebra Done Right"", addition in quotient vector spaces is defined this way: I understand why scalar multiplication has to be defined, because multiplying a subset of a vector space with a scalar was not defined. But why can addition of affine subsets be said to work this way, if the sum of subsets of a vector space was already defined? $v+U, w+U$ are both subsets of a vector space $V$ , so $(v+U)+(w+U)$ is the set containing all possible sums of elements of $v+U$ with elements of $w+U$ . Shouldn't the statement $(v+U)+(w+U)=(v+w)+U$ be a theorem?","Small question. In chapter 3, section E, page 96 of ""Linear Algebra Done Right"", addition in quotient vector spaces is defined this way: I understand why scalar multiplication has to be defined, because multiplying a subset of a vector space with a scalar was not defined. But why can addition of affine subsets be said to work this way, if the sum of subsets of a vector space was already defined? are both subsets of a vector space , so is the set containing all possible sums of elements of with elements of . Shouldn't the statement be a theorem?","v+U, w+U V (v+U)+(w+U) v+U w+U (v+U)+(w+U)=(v+w)+U","['linear-algebra', 'vector-spaces', 'quotient-spaces']"
73,Difference between the algebraic and topological dual of a topological vector space?,Difference between the algebraic and topological dual of a topological vector space?,,"What is the difference between the algebraic and the topological dual of a topological vector space, such as for example the Euclidean space $\mathbb{H}$? I am interested in intuitive as well as in detailled technical answers.","What is the difference between the algebraic and the topological dual of a topological vector space, such as for example the Euclidean space $\mathbb{H}$? I am interested in intuitive as well as in detailled technical answers.",,"['linear-algebra', 'functional-analysis', 'vector-spaces', 'topological-vector-spaces']"
74,what do free variable and leading variables mean?,what do free variable and leading variables mean?,,"What do the leading variables and free variables in a matrix mean? I have the system below and am trying to understand which are which. I searched a lot for this, please help me ! $$w + x + y + z = 6 \qquad w + y + z = 4 \qquad w + y = 2$$","What do the leading variables and free variables in a matrix mean? I have the system below and am trying to understand which are which. I searched a lot for this, please help me !",w + x + y + z = 6 \qquad w + y + z = 4 \qquad w + y = 2,"['linear-algebra', 'matrices', 'matrix-equations']"
75,Inverse of sparse matrix is not generally sparse,Inverse of sparse matrix is not generally sparse,,I have a question regarding inverse of square sparse matrices(or can be restricted to real symmetric positive definite matrices). I encountered several times the web pages which states that the inverse of the sparse matrix is not usually sparse and my experience also said so. One exception can be diagonal matrices. How theses kind of assertions can be verified?,I have a question regarding inverse of square sparse matrices(or can be restricted to real symmetric positive definite matrices). I encountered several times the web pages which states that the inverse of the sparse matrix is not usually sparse and my experience also said so. One exception can be diagonal matrices. How theses kind of assertions can be verified?,,"['linear-algebra', 'matrices', 'asymptotics', 'inverse', 'sparse-matrices']"
76,Distance between a point and a m-dimensional space in n-dimensional space ($m<n$),Distance between a point and a m-dimensional space in n-dimensional space (),m<n,"I am trying to find a method with a low computational cost to compute the distance of a point $P$ and a space $S$ that is defined by the origin $O$ and $m$ vectors $v_1, v_2, ..., v_m$ in an $n$-dimensional space ($m<n$). The vectors are not restricted by any means other than that they are not 0. Furthermore, I would like to identify the point in $S$ that is closest to $P$. This calculation is part of a 'fitting function' for a machine learning problem and thus has to be executed rather often and should be fast. The input to the function is as defined above, $P$ and $v_1, v_2, ..., v_m$. This is just for context and I am happy for a mathematical solution and can of course do the implementation myself. Thanks in advance and please let me know if I need to specify anything in more detail.","I am trying to find a method with a low computational cost to compute the distance of a point $P$ and a space $S$ that is defined by the origin $O$ and $m$ vectors $v_1, v_2, ..., v_m$ in an $n$-dimensional space ($m<n$). The vectors are not restricted by any means other than that they are not 0. Furthermore, I would like to identify the point in $S$ that is closest to $P$. This calculation is part of a 'fitting function' for a machine learning problem and thus has to be executed rather often and should be fast. The input to the function is as defined above, $P$ and $v_1, v_2, ..., v_m$. This is just for context and I am happy for a mathematical solution and can of course do the implementation myself. Thanks in advance and please let me know if I need to specify anything in more detail.",,"['linear-algebra', 'geometry', 'computer-science']"
77,Finding a matrix representation of the transpose transformation,Finding a matrix representation of the transpose transformation,,"Define $T : M_{nn}(\mathbb{R})  M_{nn}(\mathbb{R})$ by $T(A) := A^t$. I know this transformation is linear and just takes a matrix and spits out it's transpose. I also know that the transpose is just a matrix with it's columns and rows swapped; however, I don't know how to form a matrix representation of this transformation for arbitrary $n$. Any help to get me started would be appreciated!","Define $T : M_{nn}(\mathbb{R})  M_{nn}(\mathbb{R})$ by $T(A) := A^t$. I know this transformation is linear and just takes a matrix and spits out it's transpose. I also know that the transpose is just a matrix with it's columns and rows swapped; however, I don't know how to form a matrix representation of this transformation for arbitrary $n$. Any help to get me started would be appreciated!",,"['linear-algebra', 'matrices', 'transformation']"
78,Derivative of a summation in order to minimize,Derivative of a summation in order to minimize,,"I am asked to minimize $\sum^n_{i=0}(x_i - C)^2$ with respect only to C so I know I have to take the derivative  respect to C,  set it equal to 0, and then solve. I have never done summation in my life and this is very new. I have been trying to search the web for information about how to proceed in these cases but all I have found is long theorems of summation or explanations of very simple operations with the summation notation. I have found, nevertheless, the answer to my question which is as follows: $$S = \sum^n_{i=0}(x_i - C)^2$$ $$\frac{\partial S}{\partial C} = \sum^n_{i=0} 2 (x_i - C)(-1) = -2 \sum^n_{i=0} (x_i - C)$$ $$\frac{\partial S}{\partial C} = 0 \implies \sum^n_{i=0} (x_i - C) = 0$$ $$ \sum^n_{i=0} x_i - \sum^n_{i=0} C = 0$$ $$ \sum^n_{i=0} x_i = \sum^n_{i=0} C = nC$$ $$C = \frac{\sum^n_{i=0} X_i}{n}$$ First step, I don't get what this guy is doing,  so when he takes the derivative why he puts that (-1) at the end? is it because this is originally the square of a difference? That's the only thing I can think of.. Second step, where has the (-2) gone? It just vanished. Third step, I get this one. Fourth, where is this $nC$ coming from? What does it mean?  I understand that at the end what he is doing is  $ \sum^n_{i=0} x_i= nC$ so it isolates $C$ and he finally gets the result  $C = \frac{\sum^n_{i=0} X_i}{n}$. Any reasons though why he chooses to swap $\sum^n_{i=0} C$ instead of $\sum^n_{i=0} x_i$ for $nC$? Thanks a lot, I'd really appreciate if you could please refer me to any page you might know about summation which doesn't go about 1000 theorems and properties . Unfortunately I can spend much time on summation as I have many other topics in my exam that I need to cover and this is just a small part of it. Cheers!","I am asked to minimize $\sum^n_{i=0}(x_i - C)^2$ with respect only to C so I know I have to take the derivative  respect to C,  set it equal to 0, and then solve. I have never done summation in my life and this is very new. I have been trying to search the web for information about how to proceed in these cases but all I have found is long theorems of summation or explanations of very simple operations with the summation notation. I have found, nevertheless, the answer to my question which is as follows: $$S = \sum^n_{i=0}(x_i - C)^2$$ $$\frac{\partial S}{\partial C} = \sum^n_{i=0} 2 (x_i - C)(-1) = -2 \sum^n_{i=0} (x_i - C)$$ $$\frac{\partial S}{\partial C} = 0 \implies \sum^n_{i=0} (x_i - C) = 0$$ $$ \sum^n_{i=0} x_i - \sum^n_{i=0} C = 0$$ $$ \sum^n_{i=0} x_i = \sum^n_{i=0} C = nC$$ $$C = \frac{\sum^n_{i=0} X_i}{n}$$ First step, I don't get what this guy is doing,  so when he takes the derivative why he puts that (-1) at the end? is it because this is originally the square of a difference? That's the only thing I can think of.. Second step, where has the (-2) gone? It just vanished. Third step, I get this one. Fourth, where is this $nC$ coming from? What does it mean?  I understand that at the end what he is doing is  $ \sum^n_{i=0} x_i= nC$ so it isolates $C$ and he finally gets the result  $C = \frac{\sum^n_{i=0} X_i}{n}$. Any reasons though why he chooses to swap $\sum^n_{i=0} C$ instead of $\sum^n_{i=0} x_i$ for $nC$? Thanks a lot, I'd really appreciate if you could please refer me to any page you might know about summation which doesn't go about 1000 theorems and properties . Unfortunately I can spend much time on summation as I have many other topics in my exam that I need to cover and this is just a small part of it. Cheers!",,"['linear-algebra', 'derivatives', 'summation']"
79,Sum of eigenvalues and singular values,Sum of eigenvalues and singular values,,"How one can prove that for a matrix $A\in \mathbb{C}^{n\times n}$ with eigenvalues $\lambda_i$ and singular values $\sigma_i$, $i=1,\ldots,n$, the following inequality holds: $$ \sum_{i=1}^n \sigma_i(A) \geq\sum_{i=1}^n \left |\lambda_i(A) \right |$$","How one can prove that for a matrix $A\in \mathbb{C}^{n\times n}$ with eigenvalues $\lambda_i$ and singular values $\sigma_i$, $i=1,\ldots,n$, the following inequality holds: $$ \sum_{i=1}^n \sigma_i(A) \geq\sum_{i=1}^n \left |\lambda_i(A) \right |$$",,"['linear-algebra', 'matrices', 'inequality', 'eigenvalues-eigenvectors', 'singular-values']"
80,Orthogonal Projection of $ z $ onto the Affine set $ \left\{ x \mid A x = b \right\} $,Orthogonal Projection of  onto the Affine set, z   \left\{ x \mid A x = b \right\} ,"Suppose $A$ is fat (number of columns > number of rows) and full row rank . The projection of $z$ onto $\{x\mid Ax = b\}$ is (affine) $$P(z) = z - A^T(AA^T)^{-1}(Az-b)$$ How to show this? Note:  $A^T(AA^T)^{-1}$ is the pseudo-inverse of $A$ What I am thinking is from: Least square problem:  $$\text{min $\left\|\: Ax-b \,\right\|_2$}$$ The solution for this is $\hat{x} = A^T(AA^T)^{-1}b$. It seems $(Az - b )$ above is the role of $b$ here. Vector projection of $x$ onto $y$:    $$p = \frac{x^Ty}{y^Ty}y$$ But I still cannot figure out how to prove the above result.","Suppose $A$ is fat (number of columns > number of rows) and full row rank . The projection of $z$ onto $\{x\mid Ax = b\}$ is (affine) $$P(z) = z - A^T(AA^T)^{-1}(Az-b)$$ How to show this? Note:  $A^T(AA^T)^{-1}$ is the pseudo-inverse of $A$ What I am thinking is from: Least square problem:  $$\text{min $\left\|\: Ax-b \,\right\|_2$}$$ The solution for this is $\hat{x} = A^T(AA^T)^{-1}b$. It seems $(Az - b )$ above is the role of $b$ here. Vector projection of $x$ onto $y$:    $$p = \frac{x^Ty}{y^Ty}y$$ But I still cannot figure out how to prove the above result.",,"['linear-algebra', 'convex-optimization', 'inner-products', 'least-squares']"
81,Prove the determinant is the product of its diagonal entries,Prove the determinant is the product of its diagonal entries,,"Prove that the determinant of an upper triangular matrix is the product of its diagonal entries. What I have so far: We will prove this by induction for an $n \times n$ matrix. For the case of a $2 \times 2$ matrix, let $A= \left( \begin{array}{cc} a_{11} & a_{12} \\ 0 & a_{22}  \end{array} \right)$ . So $\det(A)=a_{11}a_{22}$ and the statement is true for the case of a $2 \times 2$ matrix. Now suppose that this statement is true for an $n \times n$ matrix. We will show that it also is true for an $(n + 1) \times (n + 1)$ matrix. Let $A = \left( \begin{array}{ccc} a_{11} & a_{12} & \cdots & a_{1(n+1)}\\ 0 & a_{22} & \cdots & a_{2(n+1)} \\ \vdots & \cdots &  & \vdots\\ 0 & 0 & \cdots & a_{(n+1)(n+1)}\end{array} \right)$ . I don't know what to do after this.","Prove that the determinant of an upper triangular matrix is the product of its diagonal entries. What I have so far: We will prove this by induction for an matrix. For the case of a matrix, let . So and the statement is true for the case of a matrix. Now suppose that this statement is true for an matrix. We will show that it also is true for an matrix. Let . I don't know what to do after this.","n \times n 2 \times 2 A= \left( \begin{array}{cc}
a_{11} & a_{12} \\
0 & a_{22}  \end{array} \right) \det(A)=a_{11}a_{22} 2 \times 2 n \times n (n + 1) \times (n + 1) A = \left( \begin{array}{ccc}
a_{11} & a_{12} & \cdots & a_{1(n+1)}\\
0 & a_{22} & \cdots & a_{2(n+1)} \\
\vdots & \cdots &  & \vdots\\
0 & 0 & \cdots & a_{(n+1)(n+1)}\end{array} \right)",['linear-algebra']
82,Cross product: matrix transformation identity,Cross product: matrix transformation identity,,"How can one prove the following identity of the cross product? $$(M a)\times (M b)=\det(M) (M^{\rm T})^{-1}(a\times b)$$ $a$ and $b$ are 3-vectors, and $M$ is an invertible real $3 \times 3$ matrix.","How can one prove the following identity of the cross product? $$(M a)\times (M b)=\det(M) (M^{\rm T})^{-1}(a\times b)$$ $a$ and $b$ are 3-vectors, and $M$ is an invertible real $3 \times 3$ matrix.",,"['linear-algebra', 'matrices']"
83,Why is the permanent of interest for complexity theorists?,Why is the permanent of interest for complexity theorists?,,"Studying a bit about the determinant and the permanent, I'm told that although both concepts have very similar formulas, the permanent was of not much interest historically - it was until later that complexity theorists became more curious about it. What exactly makes it interesting for complexity theorists? I heard that there is no efficient algorithm to calculate it - is that what they mean?","Studying a bit about the determinant and the permanent, I'm told that although both concepts have very similar formulas, the permanent was of not much interest historically - it was until later that complexity theorists became more curious about it. What exactly makes it interesting for complexity theorists? I heard that there is no efficient algorithm to calculate it - is that what they mean?",,"['linear-algebra', 'matrices', 'soft-question', 'math-history', 'permanent']"
84,Quick question: matrix with norm equal to spectral radius,Quick question: matrix with norm equal to spectral radius,,"For $A\in \mathcal{M}_n(\mathbb{C})$, define: the spectral radius $$ \rho(A)=\max\{|\lambda|:\lambda \mbox{ is an eigenvalue of } A\} $$ and the norm $$ \|A\|=\max_{|x|=1}|A(x)| $$ where |.| is the Euclidean norm on $\mathbb{C}^n$. Problem: Find all $A\in \mathcal{M}_n(\mathbb{C})$ such that $\rho(A)=\|A\|$. Thank you very much!","For $A\in \mathcal{M}_n(\mathbb{C})$, define: the spectral radius $$ \rho(A)=\max\{|\lambda|:\lambda \mbox{ is an eigenvalue of } A\} $$ and the norm $$ \|A\|=\max_{|x|=1}|A(x)| $$ where |.| is the Euclidean norm on $\mathbb{C}^n$. Problem: Find all $A\in \mathcal{M}_n(\mathbb{C})$ such that $\rho(A)=\|A\|$. Thank you very much!",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'spectral-radius']"
85,"Why $f(z)=\frac{az+b}{cz+d}$, $a,b,c,d \in \mathbb C$, is a linear transformation?","Why , , is a linear transformation?","f(z)=\frac{az+b}{cz+d} a,b,c,d \in \mathbb C","Now I'm confused with what ""a linear transformation"" means. In linear algebra textbook, I learned that a linear transformation is $T:V \to W$, where V,W are vector spaces, which satisfies additivity and homogeneity, in other words, $T(u+v)=Tu+Tv, T(av)=aTv$ for all $u,v \in V$ and $a \in F$ But in my complex analysis textbook, $\displaystyle f(z)=\frac{az+b}{cz+d}$, $a,b,c,d \in \mathbb C$, is introduced as an example of a linear transformation. However, this function $f$ doesn't seem to follow the definition from linear algebra. Indeed, $f(0) \neq 0$. Is it like there are two kinds of linear transformations in mathematics, or they are actually the same thing but I don't get it well?","Now I'm confused with what ""a linear transformation"" means. In linear algebra textbook, I learned that a linear transformation is $T:V \to W$, where V,W are vector spaces, which satisfies additivity and homogeneity, in other words, $T(u+v)=Tu+Tv, T(av)=aTv$ for all $u,v \in V$ and $a \in F$ But in my complex analysis textbook, $\displaystyle f(z)=\frac{az+b}{cz+d}$, $a,b,c,d \in \mathbb C$, is introduced as an example of a linear transformation. However, this function $f$ doesn't seem to follow the definition from linear algebra. Indeed, $f(0) \neq 0$. Is it like there are two kinds of linear transformations in mathematics, or they are actually the same thing but I don't get it well?",,"['linear-algebra', 'complex-analysis']"
86,"Are column vectors considered the ""default"" orientation in linear algebra?","Are column vectors considered the ""default"" orientation in linear algebra?",,"I am taking an online course where vectors are typically written out as column vectors. It seems like the only row vectors we have seen are the transposes of column vectors and labeled as such. So I'm wondering if mathematicians (at least those in linear algebra) tend to favor column vectors. This is essentially a question about convention, i.e. is it such a strong convention that if you told a mathematician about a vector without specifying its alignment, would they assume it is a column vector? Come to think of it I guess that might make sense since it is the first dimension.","I am taking an online course where vectors are typically written out as column vectors. It seems like the only row vectors we have seen are the transposes of column vectors and labeled as such. So I'm wondering if mathematicians (at least those in linear algebra) tend to favor column vectors. This is essentially a question about convention, i.e. is it such a strong convention that if you told a mathematician about a vector without specifying its alignment, would they assume it is a column vector? Come to think of it I guess that might make sense since it is the first dimension.",,"['linear-algebra', 'notation', 'vectors', 'convention']"
87,How do I prove that the trace of a matrix to its $k$th power is equal to the sum of its eigenvalues raised to the $k$th power?,How do I prove that the trace of a matrix to its th power is equal to the sum of its eigenvalues raised to the th power?,k k,"Let $A$ be an $n \times n$ matrix with eigenvalues $\lambda_{1},...\lambda_{n}$. How do I prove that tr$(A^k) = \sum_{i=1}^{n}\lambda_{i}^{k}$?","Let $A$ be an $n \times n$ matrix with eigenvalues $\lambda_{1},...\lambda_{n}$. How do I prove that tr$(A^k) = \sum_{i=1}^{n}\lambda_{i}^{k}$?",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'trace']"
88,Why does linear dependence require a *finite* linear combination to vanish?,Why does linear dependence require a *finite* linear combination to vanish?,,"By definition, $S$ is linearly independent if for all $n>0,c_i\in F,s_i\in S$, we have that $c_1s_1 + \ldots + c_n s_n \neq 0$ whenever $\{c_i\}\neq \{0\}$. Why do we restrict our attention to finite linear combinations? We could imagine a different definition: Set $S$ is ""infinitely linearly independent"" if for any indexing set $I$ and $c_i\in F,s_i\in S$, we have that $\sum_{i \in I} c_is_i \neq 0$ whenever $\{c_i\}\neq \{0\}$. Why isn't this definition, which allows arbitrary subsets of $S$, useful?","By definition, $S$ is linearly independent if for all $n>0,c_i\in F,s_i\in S$, we have that $c_1s_1 + \ldots + c_n s_n \neq 0$ whenever $\{c_i\}\neq \{0\}$. Why do we restrict our attention to finite linear combinations? We could imagine a different definition: Set $S$ is ""infinitely linearly independent"" if for any indexing set $I$ and $c_i\in F,s_i\in S$, we have that $\sum_{i \in I} c_is_i \neq 0$ whenever $\{c_i\}\neq \{0\}$. Why isn't this definition, which allows arbitrary subsets of $S$, useful?",,['linear-algebra']
89,"""Tricks"" for solving the determinant of a matrix","""Tricks"" for solving the determinant of a matrix",,"I've got an exam coming up, and I'm worried that (as I usually tend to do) I will mess up with some minor algebraic calculation in a large problem like finding the determinant of a 4x4 or larger matrix. I'm likely to be asked to use Laplace Expansion, and I'd like to know if there are any other quick and dirty ways I can try to make sure my answer was correct. Considering I'm not alowed to use a calculator, is there some other non-standard method I can use to try to verify a correct solution? I know how to find the determinant no problem, but I'm very prone to minor mistakes and I don't want to have to spend too much time reading over every problem in fine detail to make sure I didn't omit a negative sign somewhere.","I've got an exam coming up, and I'm worried that (as I usually tend to do) I will mess up with some minor algebraic calculation in a large problem like finding the determinant of a 4x4 or larger matrix. I'm likely to be asked to use Laplace Expansion, and I'd like to know if there are any other quick and dirty ways I can try to make sure my answer was correct. Considering I'm not alowed to use a calculator, is there some other non-standard method I can use to try to verify a correct solution? I know how to find the determinant no problem, but I'm very prone to minor mistakes and I don't want to have to spend too much time reading over every problem in fine detail to make sure I didn't omit a negative sign somewhere.",,"['linear-algebra', 'education']"
90,Span of a subset of a vector space is the smallest subspace containing that set,Span of a subset of a vector space is the smallest subspace containing that set,,"To Prove: If  $S=[{v_1,v_2,...,v_k}]$ is a subset of vector space $V$. Then $span(S)$ is the smallest subspace of $V$ containing set $S$. I know that $L[S]$ is a subspace of $V$.  But in most arguments for proving the $L[S]$ is the smallest subspace containing $S$ , I find that if $W$ is another subspace of $V$ containing $S$ then, proving $S \subset W$ means $S$ is the smallest. I couldn't understand that if $S \subset W$ proves that $L[S]$ is the smallest containing $S$. Please elaborate.","To Prove: If  $S=[{v_1,v_2,...,v_k}]$ is a subset of vector space $V$. Then $span(S)$ is the smallest subspace of $V$ containing set $S$. I know that $L[S]$ is a subspace of $V$.  But in most arguments for proving the $L[S]$ is the smallest subspace containing $S$ , I find that if $W$ is another subspace of $V$ containing $S$ then, proving $S \subset W$ means $S$ is the smallest. I couldn't understand that if $S \subset W$ proves that $L[S]$ is the smallest containing $S$. Please elaborate.",,['linear-algebra']
91,Double orthogonal complement is equal to topological closure,Double orthogonal complement is equal to topological closure,,"So I'm in an advanced Linear Algebra class and we just moved into Hilbert spaces and such, and I'm struggling with this question. Let $A$ be a nonempty subset of a Hilbert space $H$. Denote by $\operatorname{span}(A)$ the linear subspace of all finite linear combinations of vectors in $A$, and by $\overline{\operatorname{span}(A)}$ the topological closure of $\operatorname{span}(A)$ with respect to $\|\cdot\|$. Also, let $A^ = \{h  H : \langle h,f \rangle = 0, f  A\}$ and $A^{} = (A^)^$.  Use orthogonal projection to prove that $A^{} =\overline{\operatorname{span}(A)}$. The thing that trips me up is that we don't know much about $A$, like if I knew a little more, perhaps to show it's closed, then I can do the direct sum decomposition blah blah, but it also confuses me why we're using the complement of $\operatorname{span}(A)$. Is it possible to show that $\operatorname{span}(A)$ is closed, then go from there? I know it might look a bit like a duplicate, but all the questions I find don't refer to orthogonal projection at all. Any hints would be greatly appreciated!","So I'm in an advanced Linear Algebra class and we just moved into Hilbert spaces and such, and I'm struggling with this question. Let $A$ be a nonempty subset of a Hilbert space $H$. Denote by $\operatorname{span}(A)$ the linear subspace of all finite linear combinations of vectors in $A$, and by $\overline{\operatorname{span}(A)}$ the topological closure of $\operatorname{span}(A)$ with respect to $\|\cdot\|$. Also, let $A^ = \{h  H : \langle h,f \rangle = 0, f  A\}$ and $A^{} = (A^)^$.  Use orthogonal projection to prove that $A^{} =\overline{\operatorname{span}(A)}$. The thing that trips me up is that we don't know much about $A$, like if I knew a little more, perhaps to show it's closed, then I can do the direct sum decomposition blah blah, but it also confuses me why we're using the complement of $\operatorname{span}(A)$. Is it possible to show that $\operatorname{span}(A)$ is closed, then go from there? I know it might look a bit like a duplicate, but all the questions I find don't refer to orthogonal projection at all. Any hints would be greatly appreciated!",,"['linear-algebra', 'hilbert-spaces', 'orthogonality']"
92,What is symplectic geometry? [closed],What is symplectic geometry? [closed],,"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 4 years ago . Improve this question EDIT: Much thanks for answers. As was pointed out, the question as it stands is a little too broad. Nevertheless, I don't want to delete it, because I think that such introduction-style questions can be answered without writing a book, rather something more like an introduction to a book and fits here. Moreover, commenters have linked to great resources, and this question might help someone else. I made a follow up strictly narrower question instead. First some background, so that you know where I came from. But the question in the title stands as it is, if you want to answer without appealing to what is below, please do. I am currently learning about Lie groups. One of the first things that I've seen are the classical groups , and the classical group that I want to talk about today is the symplectic group $\mathrm{Sp}(n,\mathbb{F})$ . The definition of $\mathrm{Sp}(n,\mathbb{F})$ I am familiar with is as follows: Let $\omega$ be an skew-symmetric bilinear form on $\mathbb{F}^{2n}$ , which is unique up to change of basis. It is given by the formula $$\omega(\mathbf{x},\mathbf{y}) = \sum_{i=1}^n{x_iy_{i+n}-y_ix_{i+n}}$$ Why is this symplectic form important? We can then write out the definition $$\mathrm{Sp}(n,\mathbb{F}) = \left\{ A: \mathbb{F}^{2n} \to \mathbb{F}^{2n} \mid \omega(A\mathbf{x},A\mathbf{y}) = \omega(\mathbf{x},\mathbf{y}) \text{ for all } \mathbf{x,y} \in \mathbb{F}^{2n}\right\}$$ I can see the analogue of $O(n,\mathbb{F})$ . We also have some bilinear form that needs to be preserved, namely the inner product $\langle \cdot,\cdot\rangle$ . But more importantly, elements of $O(n,\mathbb{F})$ are really easy to visualize, because I intuitively know what a rigid transformation is. So the important question for me is How to visualize symplectic transformations? And I tried to research this question, and I stumbled upon the topic of symplectic linear spaces and symplectic manifolds . A symplectic vector space is defined analogous to Euclidean vector space, but the inner product is again substituted by symplectic form. What is a symplectic vector space, intuitively? I saw that the intuition behind these things should be that $\mathbb{R}^{2n}$ should be treated as a space of positions and velocities , a phase space . And I don't understand it. But I feel that physical intuition would be really helpful. What is the connection of classical mechanics with symplectic geometry? I don't know classical mechanics, sadly, so a quick mathematical rundown would be appreciated. All the questions that I've asked above could be summarized to one question: What is symplectic geometry?","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 4 years ago . Improve this question EDIT: Much thanks for answers. As was pointed out, the question as it stands is a little too broad. Nevertheless, I don't want to delete it, because I think that such introduction-style questions can be answered without writing a book, rather something more like an introduction to a book and fits here. Moreover, commenters have linked to great resources, and this question might help someone else. I made a follow up strictly narrower question instead. First some background, so that you know where I came from. But the question in the title stands as it is, if you want to answer without appealing to what is below, please do. I am currently learning about Lie groups. One of the first things that I've seen are the classical groups , and the classical group that I want to talk about today is the symplectic group . The definition of I am familiar with is as follows: Let be an skew-symmetric bilinear form on , which is unique up to change of basis. It is given by the formula Why is this symplectic form important? We can then write out the definition I can see the analogue of . We also have some bilinear form that needs to be preserved, namely the inner product . But more importantly, elements of are really easy to visualize, because I intuitively know what a rigid transformation is. So the important question for me is How to visualize symplectic transformations? And I tried to research this question, and I stumbled upon the topic of symplectic linear spaces and symplectic manifolds . A symplectic vector space is defined analogous to Euclidean vector space, but the inner product is again substituted by symplectic form. What is a symplectic vector space, intuitively? I saw that the intuition behind these things should be that should be treated as a space of positions and velocities , a phase space . And I don't understand it. But I feel that physical intuition would be really helpful. What is the connection of classical mechanics with symplectic geometry? I don't know classical mechanics, sadly, so a quick mathematical rundown would be appreciated. All the questions that I've asked above could be summarized to one question: What is symplectic geometry?","\mathrm{Sp}(n,\mathbb{F}) \mathrm{Sp}(n,\mathbb{F}) \omega \mathbb{F}^{2n} \omega(\mathbf{x},\mathbf{y}) = \sum_{i=1}^n{x_iy_{i+n}-y_ix_{i+n}} \mathrm{Sp}(n,\mathbb{F}) = \left\{ A: \mathbb{F}^{2n} \to \mathbb{F}^{2n} \mid \omega(A\mathbf{x},A\mathbf{y}) = \omega(\mathbf{x},\mathbf{y}) \text{ for all } \mathbf{x,y} \in \mathbb{F}^{2n}\right\} O(n,\mathbb{F}) \langle \cdot,\cdot\rangle O(n,\mathbb{F}) \mathbb{R}^{2n}","['linear-algebra', 'lie-groups', 'classical-mechanics', 'symplectic-geometry', 'symplectic-linear-algebra']"
93,"If permutation matrices are conjugate in $\operatorname{GL}(n,\mathbb{F})$ are the corresponding permutations conjugate in the symmetric group?",If permutation matrices are conjugate in  are the corresponding permutations conjugate in the symmetric group?,"\operatorname{GL}(n,\mathbb{F})","There is a standard embedding of the symmetric group $S_n$ into $\operatorname{GL}(n,\mathbb{F})$ (for any field $\mathbb{F}$) that sends each permutation in $S_n$ to the corresponding permutation matrix.  As this is a group homomorphism, certainly, conjugate permutations map to conjugate matrices.  What about the converse?  Suppose that $A$ and $B$ are similar permutation matrices in $\operatorname{GL}(n,\mathbb{F})$ coming from permutations $\alpha$ and $\beta$ in $S_n$.  Does it follow that $\alpha$ and $\beta$ are already conjugate in $S_n$?  (I thought that I had a proof, but I noticed that the argument seems to fail in positive characteristic.)","There is a standard embedding of the symmetric group $S_n$ into $\operatorname{GL}(n,\mathbb{F})$ (for any field $\mathbb{F}$) that sends each permutation in $S_n$ to the corresponding permutation matrix.  As this is a group homomorphism, certainly, conjugate permutations map to conjugate matrices.  What about the converse?  Suppose that $A$ and $B$ are similar permutation matrices in $\operatorname{GL}(n,\mathbb{F})$ coming from permutations $\alpha$ and $\beta$ in $S_n$.  Does it follow that $\alpha$ and $\beta$ are already conjugate in $S_n$?  (I thought that I had a proof, but I noticed that the argument seems to fail in positive characteristic.)",,"['linear-algebra', 'group-theory', 'permutations', 'permutation-matrices']"
94,Proof of Gaussian elimination/Why does it work,Proof of Gaussian elimination/Why does it work,,"I have just had a class on linear algebra and the professor explained how to solve matrixes. While he could explain how to solve them by using Gaussian's elimination, he failed to explain how does that work. Why does matrix before doing any operations have the same solutions as the matrix after ""changing"" a row with Gaussian elimination? Where can I read the proof?","I have just had a class on linear algebra and the professor explained how to solve matrixes. While he could explain how to solve them by using Gaussian's elimination, he failed to explain how does that work. Why does matrix before doing any operations have the same solutions as the matrix after ""changing"" a row with Gaussian elimination? Where can I read the proof?",,['linear-algebra']
95,Prove that Hermitian matrices are diagonalizable,Prove that Hermitian matrices are diagonalizable,,"I am trying to prove that Hermitian Matrices are diagonalizable. I have already proven that Hermitian Matrices have real roots and any two eigenvectors associated with two distinct eigen values are orthogonal. If $A=A^H;\;\;\lambda_1,\lambda_2$ be two distinct eigenvalues and $v_1,v_2$  be two eigenvectors associated with them. $$Av_1=\lambda_1v_1\Rightarrow v_1^HAv_1=\lambda_1v_1^Hv_1\Rightarrow \lambda_1=\lambda_1^*.$$ Similarly, $$Av_1=\lambda_1v_1\Rightarrow v_2^HAv_1=\lambda_1v_2^Hv_1\Rightarrow v_1^H\lambda_2v_2=\lambda_1v_1^Hv_2\Rightarrow v_1^Hv_2=0$$ However, I am not aware of Spectral Theorem. Given this circumstances, how can I prove that Hermitian Matrices are diagonalizable? It should follow from above but the only sufficient condition of a matrix being diagonalizable is to have $dim(A)$ distinct eigenvalues, or existance of $P$ such that $A=P^{-1}DP$. I am not sure how this follows from above conditions.","I am trying to prove that Hermitian Matrices are diagonalizable. I have already proven that Hermitian Matrices have real roots and any two eigenvectors associated with two distinct eigen values are orthogonal. If $A=A^H;\;\;\lambda_1,\lambda_2$ be two distinct eigenvalues and $v_1,v_2$  be two eigenvectors associated with them. $$Av_1=\lambda_1v_1\Rightarrow v_1^HAv_1=\lambda_1v_1^Hv_1\Rightarrow \lambda_1=\lambda_1^*.$$ Similarly, $$Av_1=\lambda_1v_1\Rightarrow v_2^HAv_1=\lambda_1v_2^Hv_1\Rightarrow v_1^H\lambda_2v_2=\lambda_1v_1^Hv_2\Rightarrow v_1^Hv_2=0$$ However, I am not aware of Spectral Theorem. Given this circumstances, how can I prove that Hermitian Matrices are diagonalizable? It should follow from above but the only sufficient condition of a matrix being diagonalizable is to have $dim(A)$ distinct eigenvalues, or existance of $P$ such that $A=P^{-1}DP$. I am not sure how this follows from above conditions.",,"['linear-algebra', 'matrices']"
96,"""An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps.""","""An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps.""",,"I was reading the Wikipedia article for complex affine spaces , which says the following: Affine geometry, broadly speaking, is the study of the geometrical properties of lines, planes, and their higher dimensional analogs, in which a notion of ""parallel"" is retained, but no metrical notions of distance or angle are. Affine spaces differ from linear spaces (that is, vector spaces) in that they do not have a distinguished choice of origin. So, in the words of Marcel Berger , ""An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps."" [1] Accordingly, a complex affine space , that is an affine space over the complex numbers, is like a complex vector space, but without a distinguished point to serve as the origin. What Marcel Berger said is what interests me: ... ""An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps."" What is meant here by ""forgetting about the origin by adding translations to the linear maps""? Can someone please explain, using basic linear algebra and geometry, and with mathematics included, what this means?","I was reading the Wikipedia article for complex affine spaces , which says the following: Affine geometry, broadly speaking, is the study of the geometrical properties of lines, planes, and their higher dimensional analogs, in which a notion of ""parallel"" is retained, but no metrical notions of distance or angle are. Affine spaces differ from linear spaces (that is, vector spaces) in that they do not have a distinguished choice of origin. So, in the words of Marcel Berger , ""An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps."" [1] Accordingly, a complex affine space , that is an affine space over the complex numbers, is like a complex vector space, but without a distinguished point to serve as the origin. What Marcel Berger said is what interests me: ... ""An affine space is nothing more than a vector space whose origin we try to forget about, by adding translations to the linear maps."" What is meant here by ""forgetting about the origin by adding translations to the linear maps""? Can someone please explain, using basic linear algebra and geometry, and with mathematics included, what this means?",,"['linear-algebra', 'geometry', 'vector-spaces', 'linear-transformations', 'affine-geometry']"
97,Prove that $0<\det(A) \le 1$,Prove that,0<\det(A) \le 1,"$A=(a_{ij})$ is a $n\times n$ symmetric real matrix such that: $a_{ii}=1$ and $\sum_{j=1}^{n}|a_{ij}|<2$ for all $i \in \{1,2,3,...,n\}$. Prove that $0< \det(A) \le 1$. My approach: That is a question that I have tried before and I am trying again but still without success. I'm trying to use spectral theorem (maybe prove that $|\lambda| \le1$) but I got nothing. I also tried brute force using the definition (using permutations) of $\det A$. Any idea?","$A=(a_{ij})$ is a $n\times n$ symmetric real matrix such that: $a_{ii}=1$ and $\sum_{j=1}^{n}|a_{ij}|<2$ for all $i \in \{1,2,3,...,n\}$. Prove that $0< \det(A) \le 1$. My approach: That is a question that I have tried before and I am trying again but still without success. I'm trying to use spectral theorem (maybe prove that $|\lambda| \le1$) but I got nothing. I also tried brute force using the definition (using permutations) of $\det A$. Any idea?",,['linear-algebra']
98,Why are the four fundamental subspaces fundamental?,Why are the four fundamental subspaces fundamental?,,"The four fundamental subspaces in linear algebra, as discussed by Gilbert Strang [ 1 ], are the kernel, image, dual space kernel, and dual space image (nullspace, column space, left nullspace, row space). He calls the relationship between these ""the fundamental theorem of linear algebra"". Of course the kernel and image are needed for the first isomorphism theorem, which is fundamental to understanding what a homomorphism is. But why are the dual space image and kernel important and ""fundamental""? A similar question was asked [ 2 ] with one answer stating In short, these four spaces (really just two spaces, with a left and a   right version of the pair) carry all the information about the image   and kernel of the linear transformation that A is affecting, whether   you are using it on the right or on the left. but this entirely avoids the issue of why the image and kernel of the adjoint of a transformation are important to understanding the transformation. Strang's paper [ 1 ] seems to suggest that these subspaces may provide intuition for the singular value decomposition, but I haven't studied the singular value decomposition (SVD) yet. I'm also not sure if the SVD is fundamental enough to justify classifying the dual space kernel and image as ""fundamental"". edit: Thanks for the replies :) To the men and women of the future that take the path I have trodden on, I'll leave some links that I found particularly helpful [ 3 , 4 , 5 ].","The four fundamental subspaces in linear algebra, as discussed by Gilbert Strang [ 1 ], are the kernel, image, dual space kernel, and dual space image (nullspace, column space, left nullspace, row space). He calls the relationship between these ""the fundamental theorem of linear algebra"". Of course the kernel and image are needed for the first isomorphism theorem, which is fundamental to understanding what a homomorphism is. But why are the dual space image and kernel important and ""fundamental""? A similar question was asked [ 2 ] with one answer stating In short, these four spaces (really just two spaces, with a left and a   right version of the pair) carry all the information about the image   and kernel of the linear transformation that A is affecting, whether   you are using it on the right or on the left. but this entirely avoids the issue of why the image and kernel of the adjoint of a transformation are important to understanding the transformation. Strang's paper [ 1 ] seems to suggest that these subspaces may provide intuition for the singular value decomposition, but I haven't studied the singular value decomposition (SVD) yet. I'm also not sure if the SVD is fundamental enough to justify classifying the dual space kernel and image as ""fundamental"". edit: Thanks for the replies :) To the men and women of the future that take the path I have trodden on, I'll leave some links that I found particularly helpful [ 3 , 4 , 5 ].",,"['linear-algebra', 'abstract-algebra', 'functional-analysis']"
99,Prove that $A\mathbf{x}=0$ has a non-zero solution $\mathbf{x}$ iff $\det(A)=0$.,Prove that  has a non-zero solution  iff .,A\mathbf{x}=0 \mathbf{x} \det(A)=0,"I was reading the wiki page for eigenvalues and eigenvectors , and I found this statement as a fundamental linear algebra theorem. $A\mathbf{x}=\mathbf{0}$ has a non-zero solution $\mathbf{x}$ iff $\det(A)=0$. I know how to prove from left to right: Assuming $\det(A)\neq 0$, the only solution for $A\mathbf{x}=\mathbf{0}$ is $\mathbf{x}=A^{-1}\mathbf{0}=\mathbf{0}$. This is a contradiction to the fact that $A\mathbf{x}=\mathbf{0}$ has a non-zero solution $\mathbf{x}$. Therefore, $A\mathbf{x}=\mathbf{0}$ has a non-zero solution $\implies$ $\det(A)=0$. Can anybody show me how to prove the other direction? $$\det(A)=0 \implies A\mathbf{x}=\mathbf{0} \;\;\text{has a non-zero solution}$$","I was reading the wiki page for eigenvalues and eigenvectors , and I found this statement as a fundamental linear algebra theorem. $A\mathbf{x}=\mathbf{0}$ has a non-zero solution $\mathbf{x}$ iff $\det(A)=0$. I know how to prove from left to right: Assuming $\det(A)\neq 0$, the only solution for $A\mathbf{x}=\mathbf{0}$ is $\mathbf{x}=A^{-1}\mathbf{0}=\mathbf{0}$. This is a contradiction to the fact that $A\mathbf{x}=\mathbf{0}$ has a non-zero solution $\mathbf{x}$. Therefore, $A\mathbf{x}=\mathbf{0}$ has a non-zero solution $\implies$ $\det(A)=0$. Can anybody show me how to prove the other direction? $$\det(A)=0 \implies A\mathbf{x}=\mathbf{0} \;\;\text{has a non-zero solution}$$",,"['linear-algebra', 'matrices', 'eigenvalues-eigenvectors', 'determinant']"
