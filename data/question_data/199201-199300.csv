,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is there a standard name for a category all of whose contravariant hom functors are sheaves?,Is there a standard name for a category all of whose contravariant hom functors are sheaves?,,"What prompted this question is the definition of a  pseudogroup in nlab : Given a X a topological space. Then a pseudogroup is a subgroupoid of the groupoid of transitions between open sets in X, contains the groupoid of identity transitions, and satisfies a sheaf condition. (Pseudogroups of continuous/smooth transitions are used to define the atlases for manifolds of the respective kind). It seems to me a pseudogroup is morally a groupoid G that satisfies the sheaf condition for each presheaf G[-,V] for V an object of G.","What prompted this question is the definition of a  pseudogroup in nlab : Given a X a topological space. Then a pseudogroup is a subgroupoid of the groupoid of transitions between open sets in X, contains the groupoid of identity transitions, and satisfies a sheaf condition. (Pseudogroups of continuous/smooth transitions are used to define the atlases for manifolds of the respective kind). It seems to me a pseudogroup is morally a groupoid G that satisfies the sheaf condition for each presheaf G[-,V] for V an object of G.",,"['differential-geometry', 'category-theory']"
1,Prove that curve with zero torsion is planar,Prove that curve with zero torsion is planar,,"I have proved that a planar curve of zero curvature is a straight line. It follows from the Frenet equations. But now I need to prove that if $\varkappa=0$, then the space curve $\mathbf{r}(t)$ is planar. From the condition and the Frenet equations it follows that $$ \left\{ \begin{aligned} \frac{d}{ds}\mathbf{v}&=k(s)\mathbf{n}(s),\\ \frac{d}{ds}\mathbf{n}&=-k(s)\mathbf{v}(s),\\ \frac{d}{ds}\mathbf{b}&=0.\\ \end{aligned} \right. $$ But how can be technically deduced from these equations that the curve is planar? Update: from a related question planar curve if and only if torsion I have realized that I need to show that $(\mathbf{r}(t)-\mathbf{r}(t_0))\cdot\mathbf{b}(t)=0$ for any $t$ and some $t_0$. The question now is how to do that. I appreciate any help.","I have proved that a planar curve of zero curvature is a straight line. It follows from the Frenet equations. But now I need to prove that if $\varkappa=0$, then the space curve $\mathbf{r}(t)$ is planar. From the condition and the Frenet equations it follows that $$ \left\{ \begin{aligned} \frac{d}{ds}\mathbf{v}&=k(s)\mathbf{n}(s),\\ \frac{d}{ds}\mathbf{n}&=-k(s)\mathbf{v}(s),\\ \frac{d}{ds}\mathbf{b}&=0.\\ \end{aligned} \right. $$ But how can be technically deduced from these equations that the curve is planar? Update: from a related question planar curve if and only if torsion I have realized that I need to show that $(\mathbf{r}(t)-\mathbf{r}(t_0))\cdot\mathbf{b}(t)=0$ for any $t$ and some $t_0$. The question now is how to do that. I appreciate any help.",,['differential-geometry']
2,Euler characteristic of sphere with a hole,Euler characteristic of sphere with a hole,,"The topologically invariant Euler characteristic of a 2-surface is given by $\chi=\frac{1}{4\pi}\int\sqrt{g}\mathcal{R}$ (where $\mathcal{R}$ is the scalar curvature) and is equivalent to $\chi=2-2g - b$ for a surface with $g$ handles and $b$ boundaries. For a sphere this is simple, $\sqrt{g}=R^2$ and $\mathcal{R}=\frac{2}{R^2}$, where $R$ is the radius. Therefore, $\chi=2$, as suggested by the second formula above. If we wanted to add a boundary, we could take the usual metric \begin{equation} \mathrm{d}s^2=\mathrm{d}\theta^2 + \sin^2{\theta} \mathrm{d}\phi^2 \end{equation} and limit $\theta$ to $0\leq\theta\leq\theta_0$, with $\theta_0<\pi$. We have added a hole in the sphere, thus adding a boundary, so the Euler charateristic should be 1. By the first formula, however, we get $\chi=(1-\cos^{-1}{\theta_0})$. Moreover, this should be topologically equivalent to a flat disc, on which we may take a flat metric, so $\mathcal{R}=0$ and $\chi=0$. Where have I gone wrong? Many thanks in advance.","The topologically invariant Euler characteristic of a 2-surface is given by $\chi=\frac{1}{4\pi}\int\sqrt{g}\mathcal{R}$ (where $\mathcal{R}$ is the scalar curvature) and is equivalent to $\chi=2-2g - b$ for a surface with $g$ handles and $b$ boundaries. For a sphere this is simple, $\sqrt{g}=R^2$ and $\mathcal{R}=\frac{2}{R^2}$, where $R$ is the radius. Therefore, $\chi=2$, as suggested by the second formula above. If we wanted to add a boundary, we could take the usual metric \begin{equation} \mathrm{d}s^2=\mathrm{d}\theta^2 + \sin^2{\theta} \mathrm{d}\phi^2 \end{equation} and limit $\theta$ to $0\leq\theta\leq\theta_0$, with $\theta_0<\pi$. We have added a hole in the sphere, thus adding a boundary, so the Euler charateristic should be 1. By the first formula, however, we get $\chi=(1-\cos^{-1}{\theta_0})$. Moreover, this should be topologically equivalent to a flat disc, on which we may take a flat metric, so $\mathcal{R}=0$ and $\chi=0$. Where have I gone wrong? Many thanks in advance.",,"['algebraic-topology', 'differential-geometry']"
3,Tangent bundle of the projective space.,Tangent bundle of the projective space.,,"I would like to know how does one imagine/write-down the tangent bundle of the real/complex projective space. Is there something simplifying that happens especially for $\mathbb{RP}^1$? Isn't there some relationship of this tangent bundle to spheres and Möbius bands? I can write down transition functions on these but that doesn't answer ""what"" is the tangent bundle. I vaguely know that Möbius band can be imagined as the anti-podal identification of the normal bundle on the circle. What is the exact statement and how to see that? Since projective spaces are easier to think of as quotients of simpler spaces like spheres I am motivated to ask the following question, In general if a group action on a manifold is such that the quotient space is again a manifold then the same action will also do a quotient of the tangent bundle. Now is the quotient of the tangent bundle (will that be again a vector bundle?) anyway related to the tangent bundle of the quotient space?","I would like to know how does one imagine/write-down the tangent bundle of the real/complex projective space. Is there something simplifying that happens especially for $\mathbb{RP}^1$? Isn't there some relationship of this tangent bundle to spheres and Möbius bands? I can write down transition functions on these but that doesn't answer ""what"" is the tangent bundle. I vaguely know that Möbius band can be imagined as the anti-podal identification of the normal bundle on the circle. What is the exact statement and how to see that? Since projective spaces are easier to think of as quotients of simpler spaces like spheres I am motivated to ask the following question, In general if a group action on a manifold is such that the quotient space is again a manifold then the same action will also do a quotient of the tangent bundle. Now is the quotient of the tangent bundle (will that be again a vector bundle?) anyway related to the tangent bundle of the quotient space?",,['differential-geometry']
4,What is the modulus of a tensor on a Riemannian 3-manifold?,What is the modulus of a tensor on a Riemannian 3-manifold?,,"Let $v^i$ be a vector on a Riemannian 3-manifold with metric $g_{ij}$ embedded inside a 3+1 space-time such that for some constant $N_M$ it satisfies the inequality $g_{ij}v^iv^j \leq  N_M ^2$. Let $K$ be a symmetric rank-2 tensor on the 3-manifold. Then apparently the following holds: $$\vert K_{ij} v^i v^j \vert \leq \vert K \vert _g N_M ^2.$$ This looks like some sort of a Cauchy-Schwarz inequality but given that $K$ is a tensor as described, I don't understand what the notation on the RHS means. For a rank-2 symmetric tensor $K$ what does $\vert K \vert _g$ mean? If one knows that for some function $N$, $g_{ij}v^iv^j \leq  N^2,$ where the function $N$ is itself bounded between constants $$N_m \leq N \leq N_M,$$ then using inequalities like the above one can apparently show the following bound: $$\int _{t_1} ^t \frac{1}{N} \Big(-v^i \partial _i N - \frac{dN}{dt} + K_{ij}v^iv^j\Big) dt  \leq -2\log N_m + \frac{1}{N_m} \int _{t_1}^t (\vert \nabla N \vert _g N_M + \vert K \vert _g N_M ^2 )dt,$$ for some fixed $t_1$ and $t$. I can't understand that first ""log"" term in the above. Also once the above bound is shown does it follow that the integral can be unbounded above or below depending solely on the property of the function $N$? If yes then what would be needed of $N$ to make the integral unbounded above or below?","Let $v^i$ be a vector on a Riemannian 3-manifold with metric $g_{ij}$ embedded inside a 3+1 space-time such that for some constant $N_M$ it satisfies the inequality $g_{ij}v^iv^j \leq  N_M ^2$. Let $K$ be a symmetric rank-2 tensor on the 3-manifold. Then apparently the following holds: $$\vert K_{ij} v^i v^j \vert \leq \vert K \vert _g N_M ^2.$$ This looks like some sort of a Cauchy-Schwarz inequality but given that $K$ is a tensor as described, I don't understand what the notation on the RHS means. For a rank-2 symmetric tensor $K$ what does $\vert K \vert _g$ mean? If one knows that for some function $N$, $g_{ij}v^iv^j \leq  N^2,$ where the function $N$ is itself bounded between constants $$N_m \leq N \leq N_M,$$ then using inequalities like the above one can apparently show the following bound: $$\int _{t_1} ^t \frac{1}{N} \Big(-v^i \partial _i N - \frac{dN}{dt} + K_{ij}v^iv^j\Big) dt  \leq -2\log N_m + \frac{1}{N_m} \int _{t_1}^t (\vert \nabla N \vert _g N_M + \vert K \vert _g N_M ^2 )dt,$$ for some fixed $t_1$ and $t$. I can't understand that first ""log"" term in the above. Also once the above bound is shown does it follow that the integral can be unbounded above or below depending solely on the property of the function $N$? If yes then what would be needed of $N$ to make the integral unbounded above or below?",,['differential-geometry']
5,Mercator Projection.,Mercator Projection.,,"I am trying to derive the metrics for the  Mercator Projection using standard spherical coordinates $f(\phi, \theta) = (\cos\phi \cos\theta, \sin\phi \cos\theta, \sin\theta)  $ . For simplicity, I consider the radius of the Earth is equal to 1. Then I am using a quadratic differential of the first fundamental form: $ds^2=Edu^2+2FDuDv+Gdv^2$ ; where $E, F, G$ are elements of the matrix representation of the first fundamental form of $f$ , that is $E = \cos^2\theta; F =0$ , and $G=1$ . So the metrics of the sphere is $ds^2=\cos^2\theta d\phi^2+d\theta^2$ . I am looking for projections: functions $x=x(\phi,\theta); y=y(\phi,\theta)$ . Next, I let loxodrome be a function $\phi=\phi(\theta)$ then the velocity vector will be $v=(\phi',1)$ . And meridian has a condition $\phi = const$ , then as a parameter, I choose $\theta$ , which gives me a velocity vector $m =(0,1)$ . Then the loxodrome should make a constant angle $\psi$ with the meridian. So I can write: $ \cos\psi =\frac{\lt v,m \gt}{\Vert v \Vert \Vert m \Vert} = \frac{1}{ \sqrt{ \cos^2\theta \phi'^2 +1}  } $ . I understand the last expression. But can someone explain to me the length of the velocity vector $v$ ? Thank you.","I am trying to derive the metrics for the  Mercator Projection using standard spherical coordinates . For simplicity, I consider the radius of the Earth is equal to 1. Then I am using a quadratic differential of the first fundamental form: ; where are elements of the matrix representation of the first fundamental form of , that is , and . So the metrics of the sphere is . I am looking for projections: functions . Next, I let loxodrome be a function then the velocity vector will be . And meridian has a condition , then as a parameter, I choose , which gives me a velocity vector . Then the loxodrome should make a constant angle with the meridian. So I can write: . I understand the last expression. But can someone explain to me the length of the velocity vector ? Thank you.","f(\phi, \theta) = (\cos\phi \cos\theta, \sin\phi \cos\theta, \sin\theta)   ds^2=Edu^2+2FDuDv+Gdv^2 E, F, G f E = \cos^2\theta; F =0 G=1 ds^2=\cos^2\theta d\phi^2+d\theta^2 x=x(\phi,\theta); y=y(\phi,\theta) \phi=\phi(\theta) v=(\phi',1) \phi = const \theta m =(0,1) \psi  \cos\psi =\frac{\lt v,m \gt}{\Vert v \Vert \Vert m \Vert} = \frac{1}{ \sqrt{ \cos^2\theta \phi'^2 +1}  }  v",['differential-geometry']
6,Differentiating curves given by curves in a Lie group acting on a point,Differentiating curves given by curves in a Lie group acting on a point,,This is a problem I keep running into and I feel it is likely a really basic fact that I am forgetting/missing but I haven't been able to see a way through. Suppose I have a curve in a Lie group: $\varphi:I\to G$ and I let that act on a point $v$ in some representation $V$ of $G$ to make a curve there. $$\sigma := \varphi \cdot v : I \to V.$$ How can I write the derivative $\sigma'$ in terms of $v$ and $\varphi$ ? For simplicity I am happy if that representation is the adjoint one $V = \mathfrak{g}$ and conversely for more complexity I would love an answer for when $v$ is replaced by a map $I \to V$ . One guess might be to use the logarithmic derivative $\varphi^{-1} \cdot \varphi'$ in some way so we can get a curve in the Lie algebra to act on $v$ : $$ \varphi^{-1}(\varphi \cdot v)' = (\varphi^{-1} \cdot \varphi')\cdot v.$$ Both sides here are curves in $V$ but I can't see why such a thing would be true. Is there some approachable way of computing $(\varphi \cdot v)'$ here or is this a foolish endeavour?,This is a problem I keep running into and I feel it is likely a really basic fact that I am forgetting/missing but I haven't been able to see a way through. Suppose I have a curve in a Lie group: and I let that act on a point in some representation of to make a curve there. How can I write the derivative in terms of and ? For simplicity I am happy if that representation is the adjoint one and conversely for more complexity I would love an answer for when is replaced by a map . One guess might be to use the logarithmic derivative in some way so we can get a curve in the Lie algebra to act on : Both sides here are curves in but I can't see why such a thing would be true. Is there some approachable way of computing here or is this a foolish endeavour?,\varphi:I\to G v V G \sigma := \varphi \cdot v : I \to V. \sigma' v \varphi V = \mathfrak{g} v I \to V \varphi^{-1} \cdot \varphi' v  \varphi^{-1}(\varphi \cdot v)' = (\varphi^{-1} \cdot \varphi')\cdot v. V (\varphi \cdot v)',"['differential-geometry', 'lie-groups', 'lie-algebras']"
7,"Symbol of $d^*$, the adjoint of the exterior derivative $d: \Omega^k(M) \to \Omega^{k+1}(M)$","Symbol of , the adjoint of the exterior derivative",d^* d: \Omega^k(M) \to \Omega^{k+1}(M),"In my Global Analysis course we are studying the symbols of differential operator. We did the example of the Laplacian $\Delta = dd^* + d^*d$ but there is something I do not really understand. Let me explain. For $d: \Omega^k(M) \to \Omega^{k+1}(M)$ it is not too hard to show that, for $\xi \in T^*M$ and $\alpha \in \Lambda^kT^*M$ we have that $$\sigma(d)(\xi)\alpha = i \xi \wedge \alpha \in \Lambda^{k+1}T^*M,$$ where $\sigma(d)$ stands for the symbol of the operator $d$ . Now we know that $$\sigma(d^*)(\xi) = \sigma(d)^*(\xi):\Lambda^{k+1}T^*M \to \Lambda^{k}T^*M$$ for $\sigma(d)^*$ the hermitian dual of $\sigma(d)$ . From there my teacher deduced that, for $\omega \in \Lambda^{k+1}T^*M$ , $$\sigma(d)^*(\xi)\omega = -i \iota_{\xi_\sharp}(\omega)$$ where $\xi_\sharp\in TM$ is the metric dual of $\xi$ , i.e. $$g(\xi_\sharp, v) = \xi(v), \quad \forall v \in TM$$ and $\iota_{\xi_\sharp}(\omega)$ is the contraction $$\iota_{\xi_\sharp}(\omega) = \omega(\xi_\sharp, \cdot, \ldots, \cdot).$$ Therefore, we can compute the symbol of $\Delta$ as \begin{align} \sigma(\Delta)(\xi)\alpha &= \sigma(d) \circ \sigma(d^*)(\xi)\alpha + \sigma(d^*) \circ \sigma(d)(\xi)\alpha\\ &= \xi \wedge \iota_{\xi_\sharp}(\alpha) + \iota_{\xi_\sharp}(\xi \wedge \alpha)\\ &= \xi \wedge \iota_{\xi_\sharp}(\alpha) + |\xi|^2 \alpha - \xi \wedge \iota_{\xi_\sharp}(\alpha)\\ &= |\xi|^2\alpha \end{align} so that $\sigma(\Delta)(\xi) = |\xi|^2 \text{Id}_{\Lambda^{k}T^*M}.$ Now the thing is that I really don't understand how to deduce the form of $\sigma(d)^*(\xi)$ , could one of you explain how  he got this ?","In my Global Analysis course we are studying the symbols of differential operator. We did the example of the Laplacian but there is something I do not really understand. Let me explain. For it is not too hard to show that, for and we have that where stands for the symbol of the operator . Now we know that for the hermitian dual of . From there my teacher deduced that, for , where is the metric dual of , i.e. and is the contraction Therefore, we can compute the symbol of as so that Now the thing is that I really don't understand how to deduce the form of , could one of you explain how  he got this ?","\Delta = dd^* + d^*d d: \Omega^k(M) \to \Omega^{k+1}(M) \xi \in T^*M \alpha \in \Lambda^kT^*M \sigma(d)(\xi)\alpha = i \xi \wedge \alpha \in \Lambda^{k+1}T^*M, \sigma(d) d \sigma(d^*)(\xi) = \sigma(d)^*(\xi):\Lambda^{k+1}T^*M \to \Lambda^{k}T^*M \sigma(d)^* \sigma(d) \omega \in \Lambda^{k+1}T^*M \sigma(d)^*(\xi)\omega = -i \iota_{\xi_\sharp}(\omega) \xi_\sharp\in TM \xi g(\xi_\sharp, v) = \xi(v), \quad \forall v \in TM \iota_{\xi_\sharp}(\omega) \iota_{\xi_\sharp}(\omega) = \omega(\xi_\sharp, \cdot, \ldots, \cdot). \Delta \begin{align}
\sigma(\Delta)(\xi)\alpha &= \sigma(d) \circ \sigma(d^*)(\xi)\alpha + \sigma(d^*) \circ \sigma(d)(\xi)\alpha\\
&= \xi \wedge \iota_{\xi_\sharp}(\alpha) + \iota_{\xi_\sharp}(\xi \wedge \alpha)\\
&= \xi \wedge \iota_{\xi_\sharp}(\alpha) + |\xi|^2 \alpha - \xi \wedge \iota_{\xi_\sharp}(\alpha)\\
&= |\xi|^2\alpha
\end{align} \sigma(\Delta)(\xi) = |\xi|^2 \text{Id}_{\Lambda^{k}T^*M}. \sigma(d)^*(\xi)","['differential-geometry', 'laplacian', 'global-analysis']"
8,Doubt in the definition of manifolds?,Doubt in the definition of manifolds?,,"Let $M$ be a Hausdorff space. Assume that there exists an open covering $\{U_{\alpha} : \alpha \in A\}$ of $M$ and homeomorphisms $\varphi_{\alpha}$ , from $U_{\alpha}$ onto an open subset $\varphi_{\alpha}(U_{\alpha})$ of $\mathbb{R}^m(\alpha)$ with $m(\alpha)$ a nonnegative integer. There exists $k \in \mathbb{N} \cup \infty$ such that whenever for $\alpha,\beta \in A$ we have then the map $U_{\alpha} \cap U_{\beta} \ne \phi$ then the map: $$\varphi_{\alpha} \circ \varphi_{\beta}^{-1}:\varphi_{\beta}(U_{\alpha} \cap U_{\beta}) \to \varphi_{\alpha}(U_{\alpha} \cap U_{\beta})$$ is $C^k$ . The intuition of the definition behind the manifold is that it behaves locally as a subset of $\mathbb{R}^n$ ( there exists a homeomorphism between $U_p$ a neihbourhood of all possible points $p \in M $ such that $\varphi(U_p)$ is a subset of $\mathbb{R}^n$ ) such that any function $f:U \to \mathbb{R}$ is $C^k$ [( $f \circ \varphi^{-1}):\varphi(U) \to \mathbb{R} $ is $C^k$ ( $k$ times differentiable)]. I can understand the first line of the definition but I am stuck with $$\varphi_{\alpha} \circ \varphi_{\beta}^{-1}:\varphi_{\beta}(U_{\alpha} \cap U_{\beta}) \to \varphi_{\alpha}(U_{\alpha} \cap U_{\beta}) is C^k $$ - how is it related?","Let be a Hausdorff space. Assume that there exists an open covering of and homeomorphisms , from onto an open subset of with a nonnegative integer. There exists such that whenever for we have then the map then the map: is . The intuition of the definition behind the manifold is that it behaves locally as a subset of ( there exists a homeomorphism between a neihbourhood of all possible points such that is a subset of ) such that any function is [( is ( times differentiable)]. I can understand the first line of the definition but I am stuck with - how is it related?","M \{U_{\alpha} : \alpha \in A\} M \varphi_{\alpha} U_{\alpha} \varphi_{\alpha}(U_{\alpha}) \mathbb{R}^m(\alpha) m(\alpha) k \in \mathbb{N} \cup \infty \alpha,\beta \in A U_{\alpha} \cap U_{\beta} \ne \phi \varphi_{\alpha} \circ \varphi_{\beta}^{-1}:\varphi_{\beta}(U_{\alpha} \cap U_{\beta}) \to \varphi_{\alpha}(U_{\alpha} \cap U_{\beta}) C^k \mathbb{R}^n U_p p \in M  \varphi(U_p) \mathbb{R}^n f:U \to \mathbb{R} C^k f \circ \varphi^{-1}):\varphi(U) \to \mathbb{R}  C^k k \varphi_{\alpha} \circ \varphi_{\beta}^{-1}:\varphi_{\beta}(U_{\alpha} \cap U_{\beta}) \to \varphi_{\alpha}(U_{\alpha} \cap U_{\beta}) is C^k ","['differential-geometry', 'manifolds']"
9,Riemannian homogeneous aspherical iff flat torus,Riemannian homogeneous aspherical iff flat torus,,"We say that a connected manifold $ M $ is aspherical if $$ \pi_n(M) = 0 $$ for all $ n \geq 2 $ . Equip $ M $ with a metric $ g $ such that $ (M,g) $ is Riemannian homogeneous (i.e. the isometry group acts transitively). If $ M $ is a compact Riemannian homogeneous aspherical manifold must $ M $ be a flat torus? I believe the answer is yes. Here is the proof: A compact aspherical manifold (indeed any finite CW complex) has torsion free fundamental group. Since $ M $ is compact Riemannian homogeneous then by Transitive action by compact Lie group implies almost abelian fundamental group the commutator subgroup of the fundamental group must be finite. But $ \pi_1(M) $ is torsion free so any finite subgroup is trivial. Thus the commutator subgroup is trivial. In other words $ \pi_1(M) $ is abelian. Since $ M $ is compact $ \pi_1(M) $ is finitely generated. So $ \pi_1(M) $ is a finitely generated torsion free abelian group $$ \pi_1(M) \cong \mathbb{Z}^n $$ Assuming that a compact Riemannian homogeneous $ K(\mathbb{Z}^n,1) $ must be a flat torus that completes the proof. But I'm not quite sure how to show that a compact Riemannian homogeneous $ K(\mathbb{Z}^n,1) $ must be a flat torus. What about the case where $ M $ is Riemannian homogenous aspherical but not compact? A Riemannian homogeneous manifold is an isometric product of a contractible piece with a Riemannian homogeneous compact piece. See https://mathoverflow.net/questions/410334/noncompact-riemannian-homogeneous-is-trivial-vector-bundle-over-compact-homogene So as long as the compact piece has dimension at least 2 then the above argument goes through and the compact piece is a flat torus so by homogeneity of the metric the whole thing is flat. But what about if the compact piece is only one dimensional?  I think the group $ H(3, \mathbb{R})/ \Gamma $ with its invariant metric (Nil geometry) is a counterexample where flatness is lost. Here $$H(3, \mathbb{R}) = \left\{\begin{bmatrix} 1 & x & z\\ 0 & 1 & y\\ 0 & 0 & 1\end{bmatrix} : x, y, z \in \mathbb{R}\right\}$$ is the three dimensional Heisenberg group, and $$\Gamma = \left\{\begin{bmatrix} 1 & 0 & c\\ 0 & 1 & 0\\ 0 & 0 & 1\end{bmatrix} :  c \in \mathbb{Z}\right\}$$ is a discrete central subgroup. Of course if there is no compact piece and $ M $ is contractible (topologically $ \mathbb{R}^n $ ) Riemannian homogeneous then there are a million different Riemannian homogeneous metrics that aren't flat. Take for example the hyperbolic metric of even the left invariant metric on any contractible Lie group (all simply connected non-abelian solvable Lie groups are good examples) This is mostly a proof verification question because this seems too general to be true but I think my proof checks out","We say that a connected manifold is aspherical if for all . Equip with a metric such that is Riemannian homogeneous (i.e. the isometry group acts transitively). If is a compact Riemannian homogeneous aspherical manifold must be a flat torus? I believe the answer is yes. Here is the proof: A compact aspherical manifold (indeed any finite CW complex) has torsion free fundamental group. Since is compact Riemannian homogeneous then by Transitive action by compact Lie group implies almost abelian fundamental group the commutator subgroup of the fundamental group must be finite. But is torsion free so any finite subgroup is trivial. Thus the commutator subgroup is trivial. In other words is abelian. Since is compact is finitely generated. So is a finitely generated torsion free abelian group Assuming that a compact Riemannian homogeneous must be a flat torus that completes the proof. But I'm not quite sure how to show that a compact Riemannian homogeneous must be a flat torus. What about the case where is Riemannian homogenous aspherical but not compact? A Riemannian homogeneous manifold is an isometric product of a contractible piece with a Riemannian homogeneous compact piece. See https://mathoverflow.net/questions/410334/noncompact-riemannian-homogeneous-is-trivial-vector-bundle-over-compact-homogene So as long as the compact piece has dimension at least 2 then the above argument goes through and the compact piece is a flat torus so by homogeneity of the metric the whole thing is flat. But what about if the compact piece is only one dimensional?  I think the group with its invariant metric (Nil geometry) is a counterexample where flatness is lost. Here is the three dimensional Heisenberg group, and is a discrete central subgroup. Of course if there is no compact piece and is contractible (topologically ) Riemannian homogeneous then there are a million different Riemannian homogeneous metrics that aren't flat. Take for example the hyperbolic metric of even the left invariant metric on any contractible Lie group (all simply connected non-abelian solvable Lie groups are good examples) This is mostly a proof verification question because this seems too general to be true but I think my proof checks out"," M  
\pi_n(M) = 0
  n \geq 2   M   g   (M,g)   M   M   M   \pi_1(M)   \pi_1(M)   M   \pi_1(M)   \pi_1(M)  
\pi_1(M) \cong \mathbb{Z}^n
  K(\mathbb{Z}^n,1)   K(\mathbb{Z}^n,1)   M   H(3, \mathbb{R})/ \Gamma  H(3, \mathbb{R}) = \left\{\begin{bmatrix} 1 & x & z\\ 0 & 1 & y\\ 0 & 0 & 1\end{bmatrix} : x, y, z \in \mathbb{R}\right\} \Gamma = \left\{\begin{bmatrix} 1 & 0 & c\\ 0 & 1 & 0\\ 0 & 0 & 1\end{bmatrix} :  c \in \mathbb{Z}\right\}  M   \mathbb{R}^n ","['differential-geometry', 'solution-verification', 'riemannian-geometry']"
10,Is every semi-Riemannian group geodesically complete?,Is every semi-Riemannian group geodesically complete?,,"I recently found out from this answer that every Lie group equipped with a left-invariant Riemannian metric is a (geodesically) complete Riemannian manifold. I wonder whether the same holds also for a semi-Riemannian group, i.e., a Lie group equipped with a bi-invariant semi-Riemannian metric. Any semisimple and any compact Lie group admits such a metric. Question . Is every semi-Riemannian group geodesically complete? If the answer is no , as I suspect, does it then change anything to assume that the group has dimension three?","I recently found out from this answer that every Lie group equipped with a left-invariant Riemannian metric is a (geodesically) complete Riemannian manifold. I wonder whether the same holds also for a semi-Riemannian group, i.e., a Lie group equipped with a bi-invariant semi-Riemannian metric. Any semisimple and any compact Lie group admits such a metric. Question . Is every semi-Riemannian group geodesically complete? If the answer is no , as I suspect, does it then change anything to assume that the group has dimension three?",,"['differential-geometry', 'riemannian-geometry']"
11,Equal area parameterization of a torus?,Equal area parameterization of a torus?,,"I am trying to parameterise a surface of revolution such that each infinitesimal area element is uniform across the surface. The cross-sections of the surface are shown in the picture below. The title of the post refers to a torus as I thought this might be an easier place to start. However, I really want to know how to do this for a surface with a general cross-section like the one below. Here is what I have tried so far... Let $l_\theta$ denote the length and coordinate along the contour shown below in the $x$ - $z$ plane. Where I am using $\theta$ here as a reference to the poloidal direction in ""Toroidal and poloidal coordinates"" . Let $R(l_\theta)$ denote the distance to the corresponding point from the $z$ -axis. Let $$\phi=\mathrm{atan2}(y, x),$$ denote the toroial angle. Let $$s_\theta = \frac{A_{tot}}{2\pi R(l_\theta)}\frac{l_\theta}{l_{tot}},$$ where $A_{tot}$ gives the total area of the surface and $l_{tot}$ give the total length of the contour. Note that $$\begin{aligned} ds_{\theta} &= dl_\theta\frac{ds_\theta}{dl_\theta} \\ &= dl_\theta\frac{A_{tot}}{2\pi l_{tot}}\left(\frac{1}{R(l_\theta)}-\frac{l_\theta}{R(l_\theta)^2}\frac{dR}{dl_\theta}\right) \end{aligned}$$ Using $\phi$ and $s_\theta$ as my coordinates nearly gives what I want but not quite. The infinitesimal area elements are given by $$\begin{aligned} R(l_\theta)\,d\phi\,ds_\theta &=\frac{A_{tot}}{2\pi l_{tot}}d\phi dl_\theta\left(1-\frac{l_\theta}{R(l_\theta)}\frac{dR}{dl_\theta}\right) \\ &\approx \frac{A_{tot}}{2\pi l_{tot}}d\phi dl_\theta \end{aligned}$$ for $$\frac{l_\theta}{R(l_\theta)}\frac{dR}{dl_\theta}\ll1.$$ Do you know if it's possible to get a better parameterization where the infinitesimal areas are completely uniform across the surface? If not, do you know how I can improve on the parameterization above?","I am trying to parameterise a surface of revolution such that each infinitesimal area element is uniform across the surface. The cross-sections of the surface are shown in the picture below. The title of the post refers to a torus as I thought this might be an easier place to start. However, I really want to know how to do this for a surface with a general cross-section like the one below. Here is what I have tried so far... Let denote the length and coordinate along the contour shown below in the - plane. Where I am using here as a reference to the poloidal direction in ""Toroidal and poloidal coordinates"" . Let denote the distance to the corresponding point from the -axis. Let denote the toroial angle. Let where gives the total area of the surface and give the total length of the contour. Note that Using and as my coordinates nearly gives what I want but not quite. The infinitesimal area elements are given by for Do you know if it's possible to get a better parameterization where the infinitesimal areas are completely uniform across the surface? If not, do you know how I can improve on the parameterization above?","l_\theta x z \theta R(l_\theta) z \phi=\mathrm{atan2}(y, x), s_\theta = \frac{A_{tot}}{2\pi R(l_\theta)}\frac{l_\theta}{l_{tot}}, A_{tot} l_{tot} \begin{aligned}
ds_{\theta} &= dl_\theta\frac{ds_\theta}{dl_\theta} \\
&= dl_\theta\frac{A_{tot}}{2\pi l_{tot}}\left(\frac{1}{R(l_\theta)}-\frac{l_\theta}{R(l_\theta)^2}\frac{dR}{dl_\theta}\right)
\end{aligned} \phi s_\theta \begin{aligned}
R(l_\theta)\,d\phi\,ds_\theta &=\frac{A_{tot}}{2\pi l_{tot}}d\phi dl_\theta\left(1-\frac{l_\theta}{R(l_\theta)}\frac{dR}{dl_\theta}\right) \\
&\approx \frac{A_{tot}}{2\pi l_{tot}}d\phi dl_\theta
\end{aligned} \frac{l_\theta}{R(l_\theta)}\frac{dR}{dl_\theta}\ll1.","['differential-geometry', 'surfaces', 'parametrization', 'solid-of-revolution']"
12,"Given a simple closed plane curve, find a relationship between the area of the original curve and the area of its parallel curve","Given a simple closed plane curve, find a relationship between the area of the original curve and the area of its parallel curve",,"For context, I have been introduced to simple plane curves from the perspective of single-variable calculus through the use of parameterization. Furthermore, my textbook only deals with plane curves that don't intersect themselves. The question from my textbook is stated as follows: Let $x = x(t)$ , $y = y(t)$ be a closed curve. A constant length $d$ is measured off along the normal to the curve. The extremity of this segment describes a curve which is called a parallel curve to the original curve. Find the area, the length of arc, and the radius of curvature of the parallel curve Now, after much rumination, I have found expressions for the length of arc and radius of curvature of the parallel curve. The only one I have been having a lot of trouble with is the area of the parallel curve. The first approach that came to mind was a simple algebraic approach: Since the question did not specify, I just assumed that the curve is arc-length parameterized to make life a little easier. Then, the parallel curve can be represented by the following parametric equations: $$x_p=\dot x - R\ddot y, \\ y_p = \dot y + R\ddot x$$ Now, the area of a parametric curve is normally given by the equation: $$A = \int^{t_1}_{t_0}y\dot xdt$$ We can obtain an alternative expression for the area using integration by parts: $$\begin{align}A &= y(t_1)x(t_1)-y(t_0)x(t_0)-\int^{t_1}_{t_0}x\dot ydt \\ &= -\int^{t_1}_{t_0}x\dot ydt, \text{since the curve is closed } x(t_1) = x(t_0),y(t_1) = y(t_0) \end{align}$$ Combining both expressions above, we obtain a nice symmetrical form for the equation for area: $$A=\frac{1}{2}\int^{t_1}_{t_0}y\dot x - x\dot y dt$$ Now, if we denote the total length of the original curve by $l$ , the area of the parallel curve is: $$\begin{align} A_p &= \frac{1}{2}\int^{l}_{0}(y-R\dot x)(\dot x + R\ddot y)-(x+R\dot y)(\dot y - R\ddot x)dt \\ &= \frac{1}{2}\int^{l}_{0} (y\dot x - x\dot y) + R(y\ddot y + x\ddot x) - R(\dot x^2 + \dot y^2) - R^2(\dot x \ddot y - \dot y \ddot x)dt \\ &= \frac{1}{2}\int^{l}_{0} (y\dot x - x\dot y) + R(y(k\dot x) + R(-k\dot y)) - R - R^2k dt, \text{ where } k \text{ refers to the curvature of the original curve} \\ &= \frac{1}{2}\int^{l}_{0} (y\dot x - x\dot y)(1+Rk)-R-R^2k dt \end{align}$$ I didn't really know how to continue simplifying this integral any further from this point, so I tried to get a better intuitive grasp of the problem by considering the simplest case of a plane curve - the circle: If you have a circle of radius $r$ and offset this circle by a distance $d$ , the relationship between the area of the offset circle and the original circle can be stated as: $$\begin{align} \pi (r+d)^2 &= \pi r^2 + 2\pi rd + \pi d^2 \\ A_p &= A + Cd + \pi d^2, \text{ where } C \text{ refers to the circumference of original circle} \end{align}$$ The equation above was interesting because it is similar in form to the integrand I obtained before. The only way I could think of to make use of this simpler case was to consider the osculating circles of the original curve and the parallel curve; however, I wasn't successful in fleshing out this idea further. I would appreciate any input on how I could either flesh out my idea of using osculating circles or any hints on simplifying the integral I got. Any other ideas on how to approach this problem are welcome as well.","For context, I have been introduced to simple plane curves from the perspective of single-variable calculus through the use of parameterization. Furthermore, my textbook only deals with plane curves that don't intersect themselves. The question from my textbook is stated as follows: Let , be a closed curve. A constant length is measured off along the normal to the curve. The extremity of this segment describes a curve which is called a parallel curve to the original curve. Find the area, the length of arc, and the radius of curvature of the parallel curve Now, after much rumination, I have found expressions for the length of arc and radius of curvature of the parallel curve. The only one I have been having a lot of trouble with is the area of the parallel curve. The first approach that came to mind was a simple algebraic approach: Since the question did not specify, I just assumed that the curve is arc-length parameterized to make life a little easier. Then, the parallel curve can be represented by the following parametric equations: Now, the area of a parametric curve is normally given by the equation: We can obtain an alternative expression for the area using integration by parts: Combining both expressions above, we obtain a nice symmetrical form for the equation for area: Now, if we denote the total length of the original curve by , the area of the parallel curve is: I didn't really know how to continue simplifying this integral any further from this point, so I tried to get a better intuitive grasp of the problem by considering the simplest case of a plane curve - the circle: If you have a circle of radius and offset this circle by a distance , the relationship between the area of the offset circle and the original circle can be stated as: The equation above was interesting because it is similar in form to the integrand I obtained before. The only way I could think of to make use of this simpler case was to consider the osculating circles of the original curve and the parallel curve; however, I wasn't successful in fleshing out this idea further. I would appreciate any input on how I could either flesh out my idea of using osculating circles or any hints on simplifying the integral I got. Any other ideas on how to approach this problem are welcome as well.","x = x(t) y = y(t) d x_p=\dot x - R\ddot y, \\ y_p = \dot y + R\ddot x A = \int^{t_1}_{t_0}y\dot xdt \begin{align}A &= y(t_1)x(t_1)-y(t_0)x(t_0)-\int^{t_1}_{t_0}x\dot ydt \\ &= -\int^{t_1}_{t_0}x\dot ydt, \text{since the curve is closed } x(t_1) = x(t_0),y(t_1) = y(t_0) \end{align} A=\frac{1}{2}\int^{t_1}_{t_0}y\dot x - x\dot y dt l \begin{align} A_p &= \frac{1}{2}\int^{l}_{0}(y-R\dot x)(\dot x + R\ddot y)-(x+R\dot y)(\dot y - R\ddot x)dt \\ &= \frac{1}{2}\int^{l}_{0} (y\dot x - x\dot y) + R(y\ddot y + x\ddot x) - R(\dot x^2 + \dot y^2) - R^2(\dot x \ddot y - \dot y \ddot x)dt \\ &= \frac{1}{2}\int^{l}_{0} (y\dot x - x\dot y) + R(y(k\dot x) + R(-k\dot y)) - R - R^2k dt, \text{ where } k \text{ refers to the curvature of the original curve} \\ &= \frac{1}{2}\int^{l}_{0} (y\dot x - x\dot y)(1+Rk)-R-R^2k dt \end{align} r d \begin{align} \pi (r+d)^2 &= \pi r^2 + 2\pi rd + \pi d^2 \\ A_p &= A + Cd + \pi d^2, \text{ where } C \text{ refers to the circumference of original circle} \end{align}","['calculus', 'differential-geometry']"
13,Proof of Brouwer's Fixed Point Theorem using Stokes' Theorem,Proof of Brouwer's Fixed Point Theorem using Stokes' Theorem,,"There is a proof of Brauwer's fixed point theorem (for functions $f:\overline{D(0,1)}\subset\mathbb{R}^n\rightarrow \overline{D(0,1)}$ ), it appears in several sources including the wikipedia page . The idea is to construct a smooth function $r:\overline{D(0,1)}\rightarrow S^{n-1}$ such that $r|_{S^{n-1}}=Id$ : You assuming that $f$ has no fixed point, and let r(x) be the intersection point of the continuation of the line $[x,f(x)]$ with $S^{n-1}$ on the side of x. Showing it's smooth is not a big deal since $r(x)$ is of the form $r(x)=f(x)+\lambda(x-f(x))$ for $\lambda\geq 1$ and it clearly maps every point on $S^{n-1}$ to itself. Then you take a volume $\omega$ form on $S^{n-1}$ , and consider the pullback $r^*\omega$ and say that on one hand we have: $$0\underset{\text{volume form}}{<}\int_{S^{n-1}}\omega=\int_{S^{n-1}}{r|_{S^{n-1}}}^*\omega$$ and on the ohter hand: $$\int_{S^{n-1}}{r|_{S^{n-1}}}^*\omega=\int_{S^{n-1}}r^*\omega\underset{Stokes}{=}\int_\overline{D(0,1)}d(r^*\omega)=\int_\overline{D(0,1)}r^*(d\omega)=0$$ since $\omega$ is a maximal form on $S^{n-1}$ , it's exterior derivative $d\omega$ is zero. We have defined the pullback of a differential form using a smooth function $g:U\rightarrow \mathcal{M}$ for an open set $U\subset{\mathbb{R}^{n-1}}$ by $$g^*\omega(x)(v_1,\dots,v_{n-1})=\omega(g(x))(Dg(x)v_1,\dots,Dg(x)v_{n-1})$$ for any $x\in U$ and any vectors $v_1,\dots,v_{n-1}\in \mathbb{R}^{n-1}$ There is also a pullback with functions between manifolds using the local coordinates so I understand why $\int_{S^{n-1}}\omega=\int_{S^{n-1}}Id^*\omega=\int_{S^{n-1}}{r|_{S^{n-1}}}^*\omega$ , but what I don't understand it is why can we say that $\int_{S^{n-1}}{r|_{S^{n-1}}}^*\omega=\int_{S^{n-1}}r^*\omega$ - how does the definition of a pullback by a smooth function extend to the boundry of the open set such that we can say state this equality. More generally I start to wonder what do we mean when we pull-back differential forms from manifolds with boundry - is it the limit of the pull-backs from the interior, or do we pull them back using the induced atlas on the bounry? If it's the latter, how do we know we're getting a smooth differential form?","There is a proof of Brauwer's fixed point theorem (for functions ), it appears in several sources including the wikipedia page . The idea is to construct a smooth function such that : You assuming that has no fixed point, and let r(x) be the intersection point of the continuation of the line with on the side of x. Showing it's smooth is not a big deal since is of the form for and it clearly maps every point on to itself. Then you take a volume form on , and consider the pullback and say that on one hand we have: and on the ohter hand: since is a maximal form on , it's exterior derivative is zero. We have defined the pullback of a differential form using a smooth function for an open set by for any and any vectors There is also a pullback with functions between manifolds using the local coordinates so I understand why , but what I don't understand it is why can we say that - how does the definition of a pullback by a smooth function extend to the boundry of the open set such that we can say state this equality. More generally I start to wonder what do we mean when we pull-back differential forms from manifolds with boundry - is it the limit of the pull-backs from the interior, or do we pull them back using the induced atlas on the bounry? If it's the latter, how do we know we're getting a smooth differential form?","f:\overline{D(0,1)}\subset\mathbb{R}^n\rightarrow \overline{D(0,1)} r:\overline{D(0,1)}\rightarrow S^{n-1} r|_{S^{n-1}}=Id f [x,f(x)] S^{n-1} r(x) r(x)=f(x)+\lambda(x-f(x)) \lambda\geq 1 S^{n-1} \omega S^{n-1} r^*\omega 0\underset{\text{volume form}}{<}\int_{S^{n-1}}\omega=\int_{S^{n-1}}{r|_{S^{n-1}}}^*\omega \int_{S^{n-1}}{r|_{S^{n-1}}}^*\omega=\int_{S^{n-1}}r^*\omega\underset{Stokes}{=}\int_\overline{D(0,1)}d(r^*\omega)=\int_\overline{D(0,1)}r^*(d\omega)=0 \omega S^{n-1} d\omega g:U\rightarrow \mathcal{M} U\subset{\mathbb{R}^{n-1}} g^*\omega(x)(v_1,\dots,v_{n-1})=\omega(g(x))(Dg(x)v_1,\dots,Dg(x)v_{n-1}) x\in U v_1,\dots,v_{n-1}\in \mathbb{R}^{n-1} \int_{S^{n-1}}\omega=\int_{S^{n-1}}Id^*\omega=\int_{S^{n-1}}{r|_{S^{n-1}}}^*\omega \int_{S^{n-1}}{r|_{S^{n-1}}}^*\omega=\int_{S^{n-1}}r^*\omega","['differential-geometry', 'fixed-point-theorems', 'stokes-theorem']"
14,Prove cylinder local isometric to the plane,Prove cylinder local isometric to the plane,,"Let $C = \{(x,y,z)\mid x^2+y^2 = 1\}\subset \Bbb{R}^3$ be the cylinder and $P = \{(x,y,z)\mid z = 0\}$ . Prove if we gives the induced metric to $C$ and $P$ from $\Bbb{R}^3$ .they are local isometry. Here local isometry means a smooth map $\varphi:P\to C$ such that $\varphi^*(g_c) = g_p$ . I have proved it clear,but seems trapped into some detail.Is there some clear proof of this exercise? I constructed as follows: $$\varphi:P\to C \\(x,y,0) \to (\cos x,\sin x ,y)$$ The hard part is to clearly show that it's local isometry. First pick local parametrization for $C$ as $X(u,v) = (\cos u,\sin u,v)$ .hence we see that :let $i_C :C\to \Bbb{R}^3$ ,then the induced metric on $C$ under the coordinate chart is $du^2 + dv^2$ .If we compute $\varphi^*(du^2 +dv^2) = d(u\circ\varphi)^2 + d(v\circ \varphi)^2$ where $u \circ \varphi (x,y,0) = x + C$ and $v \circ \varphi (x,y,0) = y + C$ ,for some constant $C$ , hence we have $\varphi^*(du^2 + dv^2) = dx^2 + dy^2$ on $P$ coinside with metric on $P$","Let be the cylinder and . Prove if we gives the induced metric to and from .they are local isometry. Here local isometry means a smooth map such that . I have proved it clear,but seems trapped into some detail.Is there some clear proof of this exercise? I constructed as follows: The hard part is to clearly show that it's local isometry. First pick local parametrization for as .hence we see that :let ,then the induced metric on under the coordinate chart is .If we compute where and ,for some constant , hence we have on coinside with metric on","C = \{(x,y,z)\mid x^2+y^2 = 1\}\subset \Bbb{R}^3 P = \{(x,y,z)\mid z = 0\} C P \Bbb{R}^3 \varphi:P\to C \varphi^*(g_c) = g_p \varphi:P\to C \\(x,y,0) \to (\cos x,\sin x ,y) C X(u,v) = (\cos u,\sin u,v) i_C :C\to \Bbb{R}^3 C du^2 + dv^2 \varphi^*(du^2 +dv^2) = d(u\circ\varphi)^2 + d(v\circ \varphi)^2 u \circ \varphi (x,y,0) = x + C v \circ \varphi (x,y,0) = y + C C \varphi^*(du^2 + dv^2) = dx^2 + dy^2 P P","['differential-geometry', 'riemannian-geometry']"
15,Projective plane does not minimally immerse into $S^3$,Projective plane does not minimally immerse into,S^3,"Can someone please give me a reference (or a proof) of the fact that the projective plane cannot be minimally immersed into the 3-sphere? My reference is: Lawson, “Complete Minimal Surfaces in $\mathbb S^3$ ”, see Corollary 1.6. In this paper, among other things, Lawson explains the relation between the zeros of the Hopf holomorphic differential form on a minimal surface in $\mathbb S^3$ and the topology of the surface, which seems to bee the idea that leads to Corollary 1.6, my desired claim. However, I miss the logical step to prove the claim, maybe somebody can explain it to me. It is anyway clear to me that the projective plane cannot be embedded in $\mathbb S^3$ and that it can be immersed in $\mathbb S^3$ .","Can someone please give me a reference (or a proof) of the fact that the projective plane cannot be minimally immersed into the 3-sphere? My reference is: Lawson, “Complete Minimal Surfaces in ”, see Corollary 1.6. In this paper, among other things, Lawson explains the relation between the zeros of the Hopf holomorphic differential form on a minimal surface in and the topology of the surface, which seems to bee the idea that leads to Corollary 1.6, my desired claim. However, I miss the logical step to prove the claim, maybe somebody can explain it to me. It is anyway clear to me that the projective plane cannot be embedded in and that it can be immersed in .",\mathbb S^3 \mathbb S^3 \mathbb S^3 \mathbb S^3,"['differential-geometry', 'minimal-surfaces']"
16,Divergence is coordinate independent,Divergence is coordinate independent,,"I am working on a project about Spectral Geometry. One of the main goals of the project is to be able to define the Laplacian on a Riemannian Manifold. As such, Let $(M,g)$ be a Riemannian Manifold, then the Laplacian is given by $\triangle_g= -\operatorname{div}_g\circ \operatorname{grad}_g$ . Given a vector field X on M, we define the divergence as being $$d(\iota_X \omega)=(\operatorname{div}_g X) \omega,$$ where $\omega$ is the volume form and $\iota_X \omega(Y_1, ..., Y_{n-1}) = \omega (X, Y_1, ..., Y_{n-1})$ . I have already shown that given coordinates $x_1,...,x_n$ then we can write the divergence of $X=\sum_{i=1}^n X_i \frac{d}{dx_i}$ as: $$\frac{1}{\sqrt{\det g}}\sum_{i=1}^n\frac{\partial}{\partial x_i}(X_i\sqrt{\det g })$$ I can clearly see that this definition $d(\iota_X \omega)=(\operatorname{div}_g X) \omega$ is independent of the choice of coordinates system. However, if instead of been given that first definition I was only given the last one, how can I prove the independence? I have tried this: Let $x_i$ and $y_i$ be two charts of $M$ . Let $g$ be the metric of $M$ with respect to the coordinates $x_i$ and $\widetilde{g}$ the one with respect to $y_i$ . Then thinking in terms of matrices we have that $\widetilde{g}=J^t g J$ , where $J$ is the jacobian associated with the change of variables from $x_i$ to $y_i$ so we get that: $$\sqrt{\det(\widetilde{g})}=\sqrt{\det (g)} \times \det (J)$$ Let $X=\sum_{i=1}^n X_i \frac{d}{dx_i}$ be a vector field in $x_i$ coordinates then it can be written as $\sum_{i,a=1}^n X_i \frac{dy_a}{dx_i}\frac{d}{dy_a}$ so let $ X_i \frac{dy_a}{dx_i}= Y_i$ .Then , \begin{align*}     \frac{1}{\sqrt{\det \widetilde{g}}}\sum_{i=1}^n \frac{\partial}{\partial y_i}(Y_i\sqrt{\det \widetilde{g}})&=\frac{1}{\sqrt{\det g} \det J}\sum_{i=1}^n \frac{\partial}{\partial y_i}(Y_i\sqrt{\det g} \det(J))\\     &=\frac{1}{\sqrt{\det g} \det J}\sum_{i,a=1}^n \frac{\partial x_a}{\partial y_i}\frac{\partial}{\partial x_a}(X_i\frac{\partial y_a}{\partial x_i}\sqrt{\det g}\det(J))\\ &=\frac{1}{\sqrt{\det g} \det J}\sum_{i,a=1}^n \det(J)\frac{\partial y_a}{\partial x_i} \frac{\partial x_a}{\partial y_i}\frac{\partial}{\partial x_a}(X_i\sqrt{\det g})+ \frac{1}{\sqrt{\det g} \det J}\sum_{i,a=1}^n X_i\sqrt{\det(g)} \frac{\partial x_a}{\partial y_i}\frac{\partial}{\partial x_a}(\frac{\partial y_a}{\partial x_i}\det J)\\ \end{align*} Now I am stuck and not sure how to proceed. On the left side, I have what I want but on the right side, I don't know how to proceed. Any ideas?","I am working on a project about Spectral Geometry. One of the main goals of the project is to be able to define the Laplacian on a Riemannian Manifold. As such, Let be a Riemannian Manifold, then the Laplacian is given by . Given a vector field X on M, we define the divergence as being where is the volume form and . I have already shown that given coordinates then we can write the divergence of as: I can clearly see that this definition is independent of the choice of coordinates system. However, if instead of been given that first definition I was only given the last one, how can I prove the independence? I have tried this: Let and be two charts of . Let be the metric of with respect to the coordinates and the one with respect to . Then thinking in terms of matrices we have that , where is the jacobian associated with the change of variables from to so we get that: Let be a vector field in coordinates then it can be written as so let .Then , Now I am stuck and not sure how to proceed. On the left side, I have what I want but on the right side, I don't know how to proceed. Any ideas?","(M,g) \triangle_g= -\operatorname{div}_g\circ \operatorname{grad}_g d(\iota_X \omega)=(\operatorname{div}_g X) \omega, \omega \iota_X \omega(Y_1, ..., Y_{n-1}) = \omega (X, Y_1, ..., Y_{n-1}) x_1,...,x_n X=\sum_{i=1}^n X_i \frac{d}{dx_i} \frac{1}{\sqrt{\det g}}\sum_{i=1}^n\frac{\partial}{\partial x_i}(X_i\sqrt{\det g }) d(\iota_X \omega)=(\operatorname{div}_g X) \omega x_i y_i M g M x_i \widetilde{g} y_i \widetilde{g}=J^t g J J x_i y_i \sqrt{\det(\widetilde{g})}=\sqrt{\det (g)} \times \det (J) X=\sum_{i=1}^n X_i \frac{d}{dx_i} x_i \sum_{i,a=1}^n X_i \frac{dy_a}{dx_i}\frac{d}{dy_a}  X_i \frac{dy_a}{dx_i}= Y_i \begin{align*}
    \frac{1}{\sqrt{\det \widetilde{g}}}\sum_{i=1}^n \frac{\partial}{\partial y_i}(Y_i\sqrt{\det \widetilde{g}})&=\frac{1}{\sqrt{\det g} \det J}\sum_{i=1}^n \frac{\partial}{\partial y_i}(Y_i\sqrt{\det g} \det(J))\\
    &=\frac{1}{\sqrt{\det g} \det J}\sum_{i,a=1}^n \frac{\partial x_a}{\partial y_i}\frac{\partial}{\partial x_a}(X_i\frac{\partial y_a}{\partial x_i}\sqrt{\det g}\det(J))\\
&=\frac{1}{\sqrt{\det g} \det J}\sum_{i,a=1}^n \det(J)\frac{\partial y_a}{\partial x_i} \frac{\partial x_a}{\partial y_i}\frac{\partial}{\partial x_a}(X_i\sqrt{\det g})+ \frac{1}{\sqrt{\det g} \det J}\sum_{i,a=1}^n X_i\sqrt{\det(g)} \frac{\partial x_a}{\partial y_i}\frac{\partial}{\partial x_a}(\frac{\partial y_a}{\partial x_i}\det J)\\
\end{align*}","['differential-geometry', 'riemannian-geometry', 'laplacian', 'divergence-operator']"
17,Hamiltonian vector field in the definition of model Dehn twists,Hamiltonian vector field in the definition of model Dehn twists,,"I am trying to understand Dehn twists along Lagrangian spheres, specifically in the context of section 2.1 in this paper. In order to define the model Dehn twist in $T^*S^n$ , which we identify with $$ \{(p,q) \in \mathbb{R}^{n+1}\times \mathbb{R}^{n+1} \mid |q| = 1, \langle p, q \rangle = 0\}, $$ one first considers the Hamiltonian $$ \mu: T^*S^n \setminus S_0 \to \mathbb{R}, \qquad (p,q) \mapsto |p|. $$ $S_0$ denotes the zero section. It is then stated that the corresponding Hamiltonian vector field is $$ X_\mu = |p|^{-1} \sum_{j=1}^{n+1}p_j \partial_{q_j} - |p| \sum_{j=1}^{n+1} q_j \partial_{p_j}. $$ One then defines the model Dehn twist using the flow of this vector field. My question now concerns how to obtain this Hamiltonian vector field - It is required to satisfy $$ \imath_{X_\mu}\omega_{can} = -d\mu, $$ where we can write $\omega_{can} = dp \wedge dq$ . Then we have $$ \imath_{X_\mu}\omega_{can} = dp(X_\mu)dq - dq(X_\mu)dp \stackrel{!}{=} -\partial_p(\mu)dp - \partial_q(\mu)dq = -d\mu. $$ Computing the partial derivatives, we have $\partial_{p_j}(\mu) = \frac{p_j}{|p|}$ and $\partial_{q_j}(\mu) = 0$ , hence the Hamiltonian vector field should only be $$ X_\mu = |p|^{-1}\sum_{j=1}^{n+1} p_j \partial_{q_j}! $$ Am I making some basic mistake? How does the second term arise? Thank you in advance.","I am trying to understand Dehn twists along Lagrangian spheres, specifically in the context of section 2.1 in this paper. In order to define the model Dehn twist in , which we identify with one first considers the Hamiltonian denotes the zero section. It is then stated that the corresponding Hamiltonian vector field is One then defines the model Dehn twist using the flow of this vector field. My question now concerns how to obtain this Hamiltonian vector field - It is required to satisfy where we can write . Then we have Computing the partial derivatives, we have and , hence the Hamiltonian vector field should only be Am I making some basic mistake? How does the second term arise? Thank you in advance.","T^*S^n 
\{(p,q) \in \mathbb{R}^{n+1}\times \mathbb{R}^{n+1} \mid |q| = 1, \langle p, q \rangle = 0\},
 
\mu: T^*S^n \setminus S_0 \to \mathbb{R}, \qquad (p,q) \mapsto |p|.
 S_0 
X_\mu = |p|^{-1} \sum_{j=1}^{n+1}p_j \partial_{q_j} - |p| \sum_{j=1}^{n+1} q_j \partial_{p_j}.
 
\imath_{X_\mu}\omega_{can} = -d\mu,
 \omega_{can} = dp \wedge dq 
\imath_{X_\mu}\omega_{can} = dp(X_\mu)dq - dq(X_\mu)dp \stackrel{!}{=} -\partial_p(\mu)dp - \partial_q(\mu)dq = -d\mu.
 \partial_{p_j}(\mu) = \frac{p_j}{|p|} \partial_{q_j}(\mu) = 0 
X_\mu = |p|^{-1}\sum_{j=1}^{n+1} p_j \partial_{q_j}!
","['differential-geometry', 'geometric-topology', 'symplectic-geometry']"
18,Local coordinate expression for the equations of motion in gauge theory,Local coordinate expression for the equations of motion in gauge theory,,"Let's assume $P$ is a principal bundle, $F^A \in \Omega^2(M,Ad(P))$ the curvature 2-form, $Ad(P)$ the adjoint bundle. $d_A$ the covariant differential. For sections in the associated bundle $E=P \times_{(G, \rho)} V$ , $d_A$ is just the covariant differential. In local coordinates it is of the form $d_A \rightarrow \partial_{\mu}+\rho_*(A_{\mu})$ . $\phi$ is a section in the associated bundle and in local coordinates takes the form $[s(x),\varphi(x)]$ where $s:U \rightarrow P$ is a section in the principal bundle and $\varphi:U \rightarrow V$ . The Yang-Mills-Higgs action is \begin{equation} \mathcal{S}_{Y K}: \mathcal{C}(P) \times \Gamma(E) \rightarrow \mathbb{R}, \quad \mathcal{S}_{Y K}[A, \phi]=\int_{M}\left(-\frac{1}{2}\left\langle F^{A}, F^{A}\right\rangle_{\mathrm{Ad}(P)}+\left\langle d_{A} \phi, d_{A} \phi\right\rangle_{E}-m^{2}\langle\phi, \phi\rangle_{E}\right) d \nu_{g} \end{equation} The variation $A\mapsto A+\omega$ gives the equations of motion \begin{equation} \delta_{A} F^{A}=j \end{equation} \begin{equation} \delta_{A} d_{A} \phi + m^{2} \phi=0 \end{equation} with the codifferential $\delta_A$ and $j \in \Omega^{1}(M , \operatorname{Ad}(P))$ implicitly defined by \begin{equation} \langle j, \omega\rangle_{\mathrm{Ad}(P)}=-2 \operatorname{Re}\left(\left\langle d_{A} \phi, \rho_{*}(\omega) \phi\right\rangle_{E}\right)\quad\text{for all }\omega. \end{equation} In physics, the current is defined by \begin{equation} j_{\nu}^{a}=-i\left(\left(D_{\nu} \varphi_{i}\right)^{\dagger}\left(T_{a}^{r} \varphi\right)_{i}-\left(T_{a}^{r} \varphi\right)_{i}^{\dagger} D_{\nu} \varphi^{j}\right) \end{equation} where $T_a$ is a basis of the Lie algebra and $T_a^r=\rho_*(T_a)$ , $D_{\nu}=\partial_{\nu}+A_{\nu}^aT_a^r$ . $\varphi_i$ is just the $i$ -th component of $\varphi$ . The $i$ 's come into play due to the definition of physicists that every Lie algebra element is multiplied with $I$ . $\mathbf{Question}$ : How exactly can one derive the physical local coordinate expression from the mathematical definition?","Let's assume is a principal bundle, the curvature 2-form, the adjoint bundle. the covariant differential. For sections in the associated bundle , is just the covariant differential. In local coordinates it is of the form . is a section in the associated bundle and in local coordinates takes the form where is a section in the principal bundle and . The Yang-Mills-Higgs action is The variation gives the equations of motion with the codifferential and implicitly defined by In physics, the current is defined by where is a basis of the Lie algebra and , . is just the -th component of . The 's come into play due to the definition of physicists that every Lie algebra element is multiplied with . : How exactly can one derive the physical local coordinate expression from the mathematical definition?","P F^A \in \Omega^2(M,Ad(P)) Ad(P) d_A E=P \times_{(G, \rho)} V d_A d_A \rightarrow \partial_{\mu}+\rho_*(A_{\mu}) \phi [s(x),\varphi(x)] s:U \rightarrow P \varphi:U \rightarrow V \begin{equation}
\mathcal{S}_{Y K}: \mathcal{C}(P) \times \Gamma(E) \rightarrow \mathbb{R}, \quad \mathcal{S}_{Y K}[A, \phi]=\int_{M}\left(-\frac{1}{2}\left\langle F^{A}, F^{A}\right\rangle_{\mathrm{Ad}(P)}+\left\langle d_{A} \phi, d_{A} \phi\right\rangle_{E}-m^{2}\langle\phi, \phi\rangle_{E}\right) d \nu_{g}
\end{equation} A\mapsto A+\omega \begin{equation}
\delta_{A} F^{A}=j
\end{equation} \begin{equation}
\delta_{A} d_{A} \phi + m^{2} \phi=0
\end{equation} \delta_A j \in \Omega^{1}(M , \operatorname{Ad}(P)) \begin{equation}
\langle j, \omega\rangle_{\mathrm{Ad}(P)}=-2 \operatorname{Re}\left(\left\langle d_{A} \phi, \rho_{*}(\omega) \phi\right\rangle_{E}\right)\quad\text{for all }\omega.
\end{equation} \begin{equation}
j_{\nu}^{a}=-i\left(\left(D_{\nu} \varphi_{i}\right)^{\dagger}\left(T_{a}^{r} \varphi\right)_{i}-\left(T_{a}^{r} \varphi\right)_{i}^{\dagger} D_{\nu} \varphi^{j}\right)
\end{equation} T_a T_a^r=\rho_*(T_a) D_{\nu}=\partial_{\nu}+A_{\nu}^aT_a^r \varphi_i i \varphi i I \mathbf{Question}","['differential-geometry', 'differential-topology', 'mathematical-physics', 'gauge-theory']"
19,Abstract idea behind the method of characteristics to solve first-order PDEs,Abstract idea behind the method of characteristics to solve first-order PDEs,,"I'm trying to understand the method of characteristics for a general first-order PDE $$F(\nabla u(x),u(x),x)=0\;\;\;\text{for all }x\in\Omega,\tag1$$ where $F:\mathbb R^d\times\mathbb R\times\overline\Omega$ is sufficiently regular and $\Omega\subseteq\mathbb R^d$ is open. I've tried to understand the motivation/derivation presented in chapter 3.2 of Evans Partial Differential Equations , but it's hard for me to understand the argumentation (and notation). Here's what I understand (but I'm unsure whether it is the correct approach to obtain the ""characteristic equations""): Assume $F\in C^1(\mathbb R^d\times\mathbb R\times\Omega)$ and $u\in C^2(\Omega)$ . Let $$E(x):=(\nabla u(x),u(x),x)\;\;\;\text{for }x\in\Omega$$ and $G:=F\circ E$ . Then $^1$ $G\in C^1(\Omega)$ and $$\nabla G(x)=\nabla^2u(x)\nabla_1F(E(x))+\partial_2F(E(x))\nabla u(x)+\nabla_3F(E(x))\tag2$$ for all $x\in\Omega$ . Now let $M:=\{G=0\}$ and $x_0\in M$ . Assuming that $G$ is a submersion at $x_0$ , the tangent space of $M$ at $x_0$ is fiven by $$T_{x_0}\:M:=\mathcal N({\rm D}G(x_0))\tag3.$$ Let $\gamma:I\to M$ be a $C^1$ -curve on $M$ through $x_0$ for some neighborhood $I$ of $0\in\mathbb R$ . To ease the following, let \begin{align}z&:=u\circ\gamma;\\p&:=\nabla u\circ\gamma;\\ E_\gamma&:=E\circ\gamma;\\ G_\gamma&:=G\circ\gamma=F\circ E_\gamma=F(p,z,\gamma).\end{align} Note that \begin{align}z'&=\langle\gamma',p\rangle;\\ p'&=\nabla^2u(\gamma)(\gamma')\tag4\end{align} Since $\gamma(I)\subseteq M$ , we've got $G_\gamma=0$ and hence \begin{equation}\begin{split}0=G_\gamma'=\langle\gamma',\nabla G(\gamma)\rangle&=\langle\gamma',\nabla^2u(\gamma)\nabla_1F(E_\gamma)+\partial_2F(E_\gamma)p+\nabla_3F(E_\gamma)\\&=\langle p',\nabla_1F(E_\gamma)\rangle+z'\partial_2F(E_\gamma)+\langle\gamma',\nabla_3F(E_\gamma)\rangle\end{split}\tag5\end{equation} by $(2)$ and $(4)$ . But this is the point where I got stuck. Instead of $(5)$ , Evans is deriving $$0=\nabla^2u(\gamma)\nabla_1F(E_\gamma)+\partial_2F(E_\gamma)p+\nabla_3F(E_\gamma)\tag{5'},$$ which can ""formally"" be obtained by inserting $x=\gamma$ into $(2)$ (hence ignoring that $\gamma$ is a function and hence ignoring that the chain rule should be applied). Then he is assuming that $$\gamma'=\nabla_1F(E_\gamma)\tag6$$ , from which we easily obtain the ""characteristic equations"" which Evans presents: \begin{align}p'&=-\partial_2F(E_\gamma)p-\nabla_3F(E_\gamma);\\ z'&=\langle p,\nabla_1F(E_\gamma)\rangle;\\\gamma'&=\nabla_1F(E_\gamma).\tag7\end{align} However, while it's clearly finite to assume $(6)$ and look what we can derive from this, I have no idea how $(5)$ is justified or why it is at least sensible to assume that it holds. And, most importantly , aren't we able to derive a sensible set of ""characteristic equations"" from $(5)$ , which - in contrast to $(5')$ - is an equation which can rigorously be derived (as I have shown) as long as we assume the stated regularity assumptions. $^1$ $\nabla_iF$ denotes the gradient of the map $F$ in the $i$ th argument and I write $\partial_2F$ for the derivative in the scalar second argument of $F$ .","I'm trying to understand the method of characteristics for a general first-order PDE where is sufficiently regular and is open. I've tried to understand the motivation/derivation presented in chapter 3.2 of Evans Partial Differential Equations , but it's hard for me to understand the argumentation (and notation). Here's what I understand (but I'm unsure whether it is the correct approach to obtain the ""characteristic equations""): Assume and . Let and . Then and for all . Now let and . Assuming that is a submersion at , the tangent space of at is fiven by Let be a -curve on through for some neighborhood of . To ease the following, let Note that Since , we've got and hence by and . But this is the point where I got stuck. Instead of , Evans is deriving which can ""formally"" be obtained by inserting into (hence ignoring that is a function and hence ignoring that the chain rule should be applied). Then he is assuming that , from which we easily obtain the ""characteristic equations"" which Evans presents: However, while it's clearly finite to assume and look what we can derive from this, I have no idea how is justified or why it is at least sensible to assume that it holds. And, most importantly , aren't we able to derive a sensible set of ""characteristic equations"" from , which - in contrast to - is an equation which can rigorously be derived (as I have shown) as long as we assume the stated regularity assumptions. denotes the gradient of the map in the th argument and I write for the derivative in the scalar second argument of .","F(\nabla u(x),u(x),x)=0\;\;\;\text{for all }x\in\Omega,\tag1 F:\mathbb R^d\times\mathbb R\times\overline\Omega \Omega\subseteq\mathbb R^d F\in C^1(\mathbb R^d\times\mathbb R\times\Omega) u\in C^2(\Omega) E(x):=(\nabla u(x),u(x),x)\;\;\;\text{for }x\in\Omega G:=F\circ E ^1 G\in C^1(\Omega) \nabla G(x)=\nabla^2u(x)\nabla_1F(E(x))+\partial_2F(E(x))\nabla u(x)+\nabla_3F(E(x))\tag2 x\in\Omega M:=\{G=0\} x_0\in M G x_0 M x_0 T_{x_0}\:M:=\mathcal N({\rm D}G(x_0))\tag3. \gamma:I\to M C^1 M x_0 I 0\in\mathbb R \begin{align}z&:=u\circ\gamma;\\p&:=\nabla u\circ\gamma;\\ E_\gamma&:=E\circ\gamma;\\ G_\gamma&:=G\circ\gamma=F\circ E_\gamma=F(p,z,\gamma).\end{align} \begin{align}z'&=\langle\gamma',p\rangle;\\ p'&=\nabla^2u(\gamma)(\gamma')\tag4\end{align} \gamma(I)\subseteq M G_\gamma=0 \begin{equation}\begin{split}0=G_\gamma'=\langle\gamma',\nabla G(\gamma)\rangle&=\langle\gamma',\nabla^2u(\gamma)\nabla_1F(E_\gamma)+\partial_2F(E_\gamma)p+\nabla_3F(E_\gamma)\\&=\langle p',\nabla_1F(E_\gamma)\rangle+z'\partial_2F(E_\gamma)+\langle\gamma',\nabla_3F(E_\gamma)\rangle\end{split}\tag5\end{equation} (2) (4) (5) 0=\nabla^2u(\gamma)\nabla_1F(E_\gamma)+\partial_2F(E_\gamma)p+\nabla_3F(E_\gamma)\tag{5'}, x=\gamma (2) \gamma \gamma'=\nabla_1F(E_\gamma)\tag6 \begin{align}p'&=-\partial_2F(E_\gamma)p-\nabla_3F(E_\gamma);\\ z'&=\langle p,\nabla_1F(E_\gamma)\rangle;\\\gamma'&=\nabla_1F(E_\gamma).\tag7\end{align} (6) (5) (5) (5') ^1 \nabla_iF F i \partial_2F F","['differential-geometry', 'partial-differential-equations', 'reference-request', 'book-recommendation', 'characteristics']"
20,Fubini's formula on fibered Riemannian manifolds.,Fubini's formula on fibered Riemannian manifolds.,,"Let $\pi:(M,g)\to(N,h)$ be a Riemannian submersion (that is, $\pi$ is a submersion satisfying $g|_{\ker(d\pi)^\perp}=\pi^*h|_{\ker(d\pi)^\perp}$ ). Each fiber of $\pi$ is a Riemannian submanifold of $M$ , given by $(F_y,g_y):=\left(\pi^{-1}(y),g|_{\pi^{-1}(y)}\right)$ . In the case that $\pi$ is a canonical projection on a Riemannian product manifold, and $f\in C^\infty_cM$ is a compactly supported smooth function, we can write Fubini's formula suggestively as $$ \int_Mf(x)d\mu_g(x)=\int_N\left(\int_{F_y}f(x)d\mu_{g_y}(x)\right)d\mu_h(y) $$ Where $\mu_g$ is the measure on $M$ induced by $g$ , etc. Intuitively, it seems that this formula also should hold for a general Riemannian submersion $\pi$ , but it is not so clear how to show this. $M$ need not look like a Riemannian product space, even locally, since $\ker(d\pi)^\perp$ need not be integrable. As a result, $g$ does not decompose nicely in adapted coordinates, making a direct computation difficult. I suspect there's a more straightforward geometrical argument, or a simple counterexample that I'm missing.","Let be a Riemannian submersion (that is, is a submersion satisfying ). Each fiber of is a Riemannian submanifold of , given by . In the case that is a canonical projection on a Riemannian product manifold, and is a compactly supported smooth function, we can write Fubini's formula suggestively as Where is the measure on induced by , etc. Intuitively, it seems that this formula also should hold for a general Riemannian submersion , but it is not so clear how to show this. need not look like a Riemannian product space, even locally, since need not be integrable. As a result, does not decompose nicely in adapted coordinates, making a direct computation difficult. I suspect there's a more straightforward geometrical argument, or a simple counterexample that I'm missing.","\pi:(M,g)\to(N,h) \pi g|_{\ker(d\pi)^\perp}=\pi^*h|_{\ker(d\pi)^\perp} \pi M (F_y,g_y):=\left(\pi^{-1}(y),g|_{\pi^{-1}(y)}\right) \pi f\in C^\infty_cM 
\int_Mf(x)d\mu_g(x)=\int_N\left(\int_{F_y}f(x)d\mu_{g_y}(x)\right)d\mu_h(y)
 \mu_g M g \pi M \ker(d\pi)^\perp g","['differential-geometry', 'riemannian-geometry', 'geometric-measure-theory']"
21,Understanding the definition of a differential operator on manifolds,Understanding the definition of a differential operator on manifolds,,"In Christian Bar's ""Geometric Wave Equations"" notes it has this definition of a differential operator. I know what $\frac{\partial f}{\partial x^i}$ means when $f:M\rightarrow \mathbb{R}^n$ is a smooth function. But I don't understand what is meant by $\frac{\partial v}{\partial x^i}$ when $v:M\rightarrow E$ is a smooth section (M and E being manifolds). Any help is appreciated, cheers.","In Christian Bar's ""Geometric Wave Equations"" notes it has this definition of a differential operator. I know what means when is a smooth function. But I don't understand what is meant by when is a smooth section (M and E being manifolds). Any help is appreciated, cheers.",\frac{\partial f}{\partial x^i} f:M\rightarrow \mathbb{R}^n \frac{\partial v}{\partial x^i} v:M\rightarrow E,['differential-geometry']
22,Basic 2 form exercise,Basic 2 form exercise,,"Given $\omega = fdx+gdy+hdz$ , such that $\omega\wedge dz=0$ , what can we conclude about $f$ , $g$ and $h$ ? Let's write what we have: $$ 0 = \omega\wedge dz = f dx\wedge dz + g dy \wedge dz. $$ We could take $d$ here and get $g_x = f_y$ . Then we could use this result and $d^2 \omega = 0$ and obtain $f_{zy}=g_{zx}$ . So I derived something, but not really understand the geometric sense or what this all was about. But it should be smth.","Given , such that , what can we conclude about , and ? Let's write what we have: We could take here and get . Then we could use this result and and obtain . So I derived something, but not really understand the geometric sense or what this all was about. But it should be smth.","\omega = fdx+gdy+hdz \omega\wedge dz=0 f g h 
0 = \omega\wedge dz = f dx\wedge dz + g dy \wedge dz.
 d g_x = f_y d^2 \omega = 0 f_{zy}=g_{zx}","['differential-geometry', 'differential', 'exterior-derivative']"
23,Making a vector field divergence-free,Making a vector field divergence-free,,"I have a feeling that this is not true, but I'm not sure how to construct a counter-example. Given a non-vanishing smooth vector field $X$ on $\mathbb{R}^n$ , is there a positive smooth function $f$ such that the re-scaled vector field $fX$ is divergence-free? This involves solving the following PDE for $f$ , with coefficients depending on $X$ and its first derivatives: $$df(X) + f\text{div}(X) = 0.$$","I have a feeling that this is not true, but I'm not sure how to construct a counter-example. Given a non-vanishing smooth vector field on , is there a positive smooth function such that the re-scaled vector field is divergence-free? This involves solving the following PDE for , with coefficients depending on and its first derivatives:",X \mathbb{R}^n f fX f X df(X) + f\text{div}(X) = 0.,"['real-analysis', 'differential-geometry', 'partial-differential-equations']"
24,How to show this is a homotopy operator between $f_0$ and $f_1$?,How to show this is a homotopy operator between  and ?,f_0 f_1,"Given $f_0, f_1: M \rightarrow N$ maps between smooth manifolds. We defined a homotopy operator between them as a linear map $$ Q: \Omega^{k} (N) \rightarrow \Omega^{k-1} (M)$$ such that $$ f_{1}^{*} - f_{0}^{*} = d \circ Q + Q \circ d$$ holds. Problem: Let $f_0, f_1: M \rightarrow N$ be smooth maps, let $H: I \times M \rightarrow N$ be a homotopy between them, i.e. $H(0,x) = f_0 (x)$ and $H(1,x) = f_1 (x)$ . Also let $I_t : M \rightarrow I \times M: x \mapsto (t,x)$ . Prove that $$Q := \int_0^{1} I_t^{*} \circ \iota_{\partial_t} \circ H^{*} dt $$ is a homotopy operator between $f_1$ and $f_0$ . Here $\iota$ denotes interior multiplication. Attempt: I think I have to use the formula $$ \frac{d}{dt} \rho_t^{*} \alpha_t = \rho_t^{*} (L_{v_t} \alpha_t + \frac{d}{dt} \alpha_t)$$ where $\alpha_t$ is the isotopy of the time-dependent vector field $v_t$ and $L$ denotes the Lie derivative. Given $\alpha $ a $k$ -form on $N$ , I wanted to calculate $$ Q (d \alpha) + d (Q \alpha) = \int_{0}^1 (I_t^{*} \circ \iota_{\partial_t} \circ H^{*}) (d \alpha) dt + d  \int_0^{1} (I_t^{*} \circ \iota_{\partial_t} \circ H^{*}) \alpha dt $$ I was not sure how to work this out. Any suggestions/advice?","Given maps between smooth manifolds. We defined a homotopy operator between them as a linear map such that holds. Problem: Let be smooth maps, let be a homotopy between them, i.e. and . Also let . Prove that is a homotopy operator between and . Here denotes interior multiplication. Attempt: I think I have to use the formula where is the isotopy of the time-dependent vector field and denotes the Lie derivative. Given a -form on , I wanted to calculate I was not sure how to work this out. Any suggestions/advice?","f_0, f_1: M \rightarrow N  Q: \Omega^{k} (N) \rightarrow \Omega^{k-1} (M)  f_{1}^{*} - f_{0}^{*} = d \circ Q + Q \circ d f_0, f_1: M \rightarrow N H: I \times M \rightarrow N H(0,x) = f_0 (x) H(1,x) = f_1 (x) I_t : M \rightarrow I \times M: x \mapsto (t,x) Q := \int_0^{1} I_t^{*} \circ \iota_{\partial_t} \circ H^{*} dt  f_1 f_0 \iota  \frac{d}{dt} \rho_t^{*} \alpha_t = \rho_t^{*} (L_{v_t} \alpha_t + \frac{d}{dt} \alpha_t) \alpha_t v_t L \alpha  k N  Q (d \alpha) + d (Q \alpha) = \int_{0}^1 (I_t^{*} \circ \iota_{\partial_t} \circ H^{*}) (d \alpha) dt + d  \int_0^{1} (I_t^{*} \circ \iota_{\partial_t} \circ H^{*}) \alpha dt ","['differential-geometry', 'algebraic-topology', 'symplectic-geometry']"
25,Geodesics on paraboloid self-interesect in an infinite number of points,Geodesics on paraboloid self-interesect in an infinite number of points,,"I'm trying to solve an exercise of Do Carmo's Riemannian geometry. Specifically, I have to prove that on a paraboloid(that is the revolution surface of a paraboloid $\{(v\cos u,v\sin u,v^2):v\in(0,\infty),u\in(0,2\pi)\}$ ) the geodesics which are not meridians (that is $u\neq $ constant)self-intersects in an infinite amount of points. Using Clairaut's relation it is possible to show that geodesics are characterized (at least locally) by the following ODE's system: \begin{cases} (1+4v(t)^2)v'(t)^2+u'(t)^2=c_0\\ u'(t)v(t)^2=c_1 \end{cases} where $c_0,c_1$ are unkown constants. This system reduces to the equation $(1+4v(t)^2)v'(t)^4-c_0v(t)^2+c_1=0$ Which I have absolutely no idea on how to solve. Any hint is very much appreciated. P.S: Do Carmo suggest using Clairaut's relation, which I've already used, but it is possible there is a more tricky application.","I'm trying to solve an exercise of Do Carmo's Riemannian geometry. Specifically, I have to prove that on a paraboloid(that is the revolution surface of a paraboloid ) the geodesics which are not meridians (that is constant)self-intersects in an infinite amount of points. Using Clairaut's relation it is possible to show that geodesics are characterized (at least locally) by the following ODE's system: where are unkown constants. This system reduces to the equation Which I have absolutely no idea on how to solve. Any hint is very much appreciated. P.S: Do Carmo suggest using Clairaut's relation, which I've already used, but it is possible there is a more tricky application.","\{(v\cos u,v\sin u,v^2):v\in(0,\infty),u\in(0,2\pi)\} u\neq  \begin{cases}
(1+4v(t)^2)v'(t)^2+u'(t)^2=c_0\\
u'(t)v(t)^2=c_1
\end{cases} c_0,c_1 (1+4v(t)^2)v'(t)^4-c_0v(t)^2+c_1=0","['differential-geometry', 'geodesic']"
26,Two way of computing blowup -- which one is correct?,Two way of computing blowup -- which one is correct?,,"I came across reading Corollary 7.15 in Hartshorne's book. A special case of this corollary is the following statement If $Y,C\subset X$ are subvarieties, $\widetilde X\to X$ is the blowup of $X$ along $C$ and $\widetilde Y\to Y$ is the blowup along $Y\cap C$ , then there is an inclusion $\widetilde Y\subset \widetilde X$ . In other words, where exists a commutative diagram $\require{AMScd}$ \begin{CD} \widetilde{Y}=Bl_{C\cap Y}Y @>>> \widetilde{X}=Bl_C X\\ @V  V V @VV V\\ Y @>>> X. \end{CD} Now assume that $X=\mathbb C^n$ and thus that $Y\subset \mathbb C^n$ is an affine variety. Let $Z\subset Y$ be a fixed subvariety. My confusion arises when comparing the following two blowups: Take as center $C$ a variety such that $C\cap Y=Z$ and compute the blowup. Take as center $C$ the variety $Z$ itself. The point is that $C$ in 1. can be defined by fewer equations than the $C$ in 2. -- that is because the intersection with $Y$ can give additional equations (coming from the equations of $Y$ ). So it seems that there are two ways of computing the blowup of of $Y$ along $Z$ and they result in different results. Which one is the correct one and why? Example: Take $X=\mathbb C^4$ with coordinates $x_1,,\dots, x_4$ , $Y=\{x_1^2-x_2^2=0\}$ , $Z=\{x_1=x_2=x_3=0\}$ . For 1. we can take $C=\{x_2=x_3=0\}$ because $C\cap Y=\{x_1^2-x_2^2=x_2=x_3=0\}=Z$ . Then, $$\widetilde X=\{a_0x_2=a_1x_3\}\subset \mathbb C^4\times \mathbb P^1$$ and the strict transform of $Y$ can be computed in the corresponding charts to be chart 1 ( $a_0\neq 0$ ): $\widetilde Y\cap \{a_0\neq 0\}=\{x_1^2-a_1^2x_3^2=x_2-a_1x_3\}\subset \mathbb C^5$ , chart 2 ( $a_1\neq 0$ ): $\widetilde Y\cap \{a_1\neq 0\}=\{x_1^2-x_2^2=x_3-a_0x_2=0\}\subset \mathbb C^5$ . On the other hand, we can also blowup $X$ along $Z$ and obtain $$\widetilde X=\{a_0x_2-a_1x_1=a_0x_3-a_2x_1=a_1x_3-a_2x_2=0\}\subset \mathbb C^4\times \mathbb P^2$$ chart 1 ( $a_0\neq 0$ ): $\widetilde Y\cap \{a_0\neq 0\}=\{1-a_1^2=x_2-a_1x_1=x_3-a_2x_1=0\}\subset \mathbb C^6$ , chart 2 ( $a_1\neq 0$ ): $\widetilde Y\cap \{a_1\neq 0\}=\{a_0^2-1=x_1-a_0x_2=x_3-a_2x_2=0\}\subset \mathbb C^6$ , chart 3 ( $a_2\neq 0$ ): $\widetilde Y\cap \{a_2\neq 0\}=\{a_0^2-a_1^2=x_1-a_0x_3=x_2-a_3x_3=0\}\subset \mathbb C^6$ . In these charts we had to remove factors corresponding to the exceptional divisor. So it seems that $\widetilde Y=\{a_0^2-a_1^2=0\}\subset \widetilde X$ . Chart 3 of the second and chart 2 of the first are isomorphic. But I don't see any isomorphism between the remaining charts and there is also one more chart for the second than for the first. Moreover, using the first method, the inverse image of $Z$ is $Z\times \mathbb P^1$ , whilst for the second method it is $Z\times \mathbb P^2$ and from the equations there is nothing like an inclusion or so that I can see. So what is the point here? With both methods I calculated $Bl_Z Y$ but the results seem to be very different from each other.","I came across reading Corollary 7.15 in Hartshorne's book. A special case of this corollary is the following statement If are subvarieties, is the blowup of along and is the blowup along , then there is an inclusion . In other words, where exists a commutative diagram Now assume that and thus that is an affine variety. Let be a fixed subvariety. My confusion arises when comparing the following two blowups: Take as center a variety such that and compute the blowup. Take as center the variety itself. The point is that in 1. can be defined by fewer equations than the in 2. -- that is because the intersection with can give additional equations (coming from the equations of ). So it seems that there are two ways of computing the blowup of of along and they result in different results. Which one is the correct one and why? Example: Take with coordinates , , . For 1. we can take because . Then, and the strict transform of can be computed in the corresponding charts to be chart 1 ( ): , chart 2 ( ): . On the other hand, we can also blowup along and obtain chart 1 ( ): , chart 2 ( ): , chart 3 ( ): . In these charts we had to remove factors corresponding to the exceptional divisor. So it seems that . Chart 3 of the second and chart 2 of the first are isomorphic. But I don't see any isomorphism between the remaining charts and there is also one more chart for the second than for the first. Moreover, using the first method, the inverse image of is , whilst for the second method it is and from the equations there is nothing like an inclusion or so that I can see. So what is the point here? With both methods I calculated but the results seem to be very different from each other.","Y,C\subset X \widetilde X\to X X C \widetilde Y\to Y Y\cap C \widetilde Y\subset \widetilde X \require{AMScd} \begin{CD}
\widetilde{Y}=Bl_{C\cap Y}Y @>>> \widetilde{X}=Bl_C X\\
@V  V V @VV V\\
Y @>>> X.
\end{CD} X=\mathbb C^n Y\subset \mathbb C^n Z\subset Y C C\cap Y=Z C Z C C Y Y Y Z X=\mathbb C^4 x_1,,\dots, x_4 Y=\{x_1^2-x_2^2=0\} Z=\{x_1=x_2=x_3=0\} C=\{x_2=x_3=0\} C\cap Y=\{x_1^2-x_2^2=x_2=x_3=0\}=Z \widetilde X=\{a_0x_2=a_1x_3\}\subset \mathbb C^4\times \mathbb P^1 Y a_0\neq 0 \widetilde Y\cap \{a_0\neq 0\}=\{x_1^2-a_1^2x_3^2=x_2-a_1x_3\}\subset \mathbb C^5 a_1\neq 0 \widetilde Y\cap \{a_1\neq 0\}=\{x_1^2-x_2^2=x_3-a_0x_2=0\}\subset \mathbb C^5 X Z \widetilde X=\{a_0x_2-a_1x_1=a_0x_3-a_2x_1=a_1x_3-a_2x_2=0\}\subset \mathbb C^4\times \mathbb P^2 a_0\neq 0 \widetilde Y\cap \{a_0\neq 0\}=\{1-a_1^2=x_2-a_1x_1=x_3-a_2x_1=0\}\subset \mathbb C^6 a_1\neq 0 \widetilde Y\cap \{a_1\neq 0\}=\{a_0^2-1=x_1-a_0x_2=x_3-a_2x_2=0\}\subset \mathbb C^6 a_2\neq 0 \widetilde Y\cap \{a_2\neq 0\}=\{a_0^2-a_1^2=x_1-a_0x_3=x_2-a_3x_3=0\}\subset \mathbb C^6 \widetilde Y=\{a_0^2-a_1^2=0\}\subset \widetilde X Z Z\times \mathbb P^1 Z\times \mathbb P^2 Bl_Z Y","['differential-geometry', 'algebraic-geometry', 'blowup']"
27,Properties of preimage of critical values of a manifold,Properties of preimage of critical values of a manifold,,"My question is very vague and poorly stated. Let $f: M \to \mathbb{R}$ be a smooth function with $M$ a smooth manifold. Let $y \in \mathbb{R}$ be a critical value which isn't a global extremum. We know in general that $f^{-1}(y)$ isn't necessarily a submanifold of $M$ , but I'm wondering if there are cases where it still is, or at least is ""almost"" a manifold. To be more clear about what I'm saying I'll give an example that's easy to visualize. Say $M$ is a compact surface in $\mathbb{R}^3$ and $f$ is the height function. For a regular value $x$ , the preimage of it will look like a set of circles (if I'm not wrong). I think of it as the intersection of the surface with the plane $z=x$ For a critical value $y$ that isn't a global max or min (say it is the peak of some hill), the preimage will consist of a point as well as at least one circle. This is because $f^{-1}(y)$ not only has a critical point, but also a set of regular points which map to the circle, let's say. How do we describe this preimage, is it still a manifold? It has components of different dimensions, does this contradict the definition? Can we say that removing the critical points of the preimage leaves us with a properly defined manifold?","My question is very vague and poorly stated. Let be a smooth function with a smooth manifold. Let be a critical value which isn't a global extremum. We know in general that isn't necessarily a submanifold of , but I'm wondering if there are cases where it still is, or at least is ""almost"" a manifold. To be more clear about what I'm saying I'll give an example that's easy to visualize. Say is a compact surface in and is the height function. For a regular value , the preimage of it will look like a set of circles (if I'm not wrong). I think of it as the intersection of the surface with the plane For a critical value that isn't a global max or min (say it is the peak of some hill), the preimage will consist of a point as well as at least one circle. This is because not only has a critical point, but also a set of regular points which map to the circle, let's say. How do we describe this preimage, is it still a manifold? It has components of different dimensions, does this contradict the definition? Can we say that removing the critical points of the preimage leaves us with a properly defined manifold?",f: M \to \mathbb{R} M y \in \mathbb{R} f^{-1}(y) M M \mathbb{R}^3 f x z=x y f^{-1}(y),"['differential-geometry', 'differential-topology']"
28,Inner product on fiber of vector bundle over manifold.,Inner product on fiber of vector bundle over manifold.,,"Fix $n\in\mathbb{N}$ . A vector bundle of rank $n$ is a smooth map $\pi:E\rightarrow B$ between manifolds such that $\forall p\in B: E_p := \pi^{-1}(p)$ is an $n$ -dimensional vector space and $\forall p\in B$ , there exists a neighborhood $U$ of $p$ and a diffeomorphism $\psi:E\mid_U:=\pi^{-1}(U)\rightarrow U\times \mathbb{R}^n$ such that $\operatorname{pr}_1\circ\psi = \pi$ and $\psi\mid_{E_q}:E_q\rightarrow\{q\}\times\mathbb{R}^n$ is a vectorspace isomorphism for all $q\in U$ . Suppose $E$ is a vector bundle over a manifold $M$ . (I suppose by this they mean there exists $\pi:E\rightarrow M$ as above). Prove that for all $x\in M$ one can construct an inner product (symmetric, positive definite, bilinear form): $g_x : E_x\times E_x\rightarrow \mathbb{R}$ which depends smoothly on $x$ . "" $g$ depends smoothly on $x$ "" means: $g(v,w)$ is a smooth function on $M$ for all smooth sections $v,w$ of $E$ . I have tried to construct this norm by using the standard norm on $\mathbb{R}$ . Take $x\in M$ . There is an open neighborhood $U$ of $x$ for which there exists a diffeomorphism $\psi$ as above. Let us define $g_x((a,b)) = <\operatorname{pr}_2(\psi(a)),\operatorname{pr}_2(\psi(b))>_\mathbb{R}$ . Since this is symmetric, $g_x$ will be symmetric. Since this is positive definite, $g_x$ will also be positive definite. Now for bilinearity. For $a_1,a_2\in E_x$ , we have $\psi(a_1)+\psi(a_2) = \psi(a_1+a_2)$ , since $\psi$ is an isomorphism. And since $\psi(a_i)\in\{x\}\times\mathbb{R}^n$ . $\operatorname{pr}_2(\psi(a_1)+\psi(a_2))=\operatorname{pr}_2(\psi(a_1))+\operatorname{pr}_2(\psi(a_2))$ . Since we have a vector space isomorphism we can also show this for scalar multiplication. I am unsure however what this field of scalars is exactly. Then for the final part I need to show that $g$ depends smoothly on $x$ . But here I am lost completely. How can we evaluate a section in $g$ ? As far as I know there is no correspondence between sections and elements of $E_x$ . A hint I was given is that we can use partitions of unity, but I have no clue how this ties in with what I have constructed.","Fix . A vector bundle of rank is a smooth map between manifolds such that is an -dimensional vector space and , there exists a neighborhood of and a diffeomorphism such that and is a vectorspace isomorphism for all . Suppose is a vector bundle over a manifold . (I suppose by this they mean there exists as above). Prove that for all one can construct an inner product (symmetric, positive definite, bilinear form): which depends smoothly on . "" depends smoothly on "" means: is a smooth function on for all smooth sections of . I have tried to construct this norm by using the standard norm on . Take . There is an open neighborhood of for which there exists a diffeomorphism as above. Let us define . Since this is symmetric, will be symmetric. Since this is positive definite, will also be positive definite. Now for bilinearity. For , we have , since is an isomorphism. And since . . Since we have a vector space isomorphism we can also show this for scalar multiplication. I am unsure however what this field of scalars is exactly. Then for the final part I need to show that depends smoothly on . But here I am lost completely. How can we evaluate a section in ? As far as I know there is no correspondence between sections and elements of . A hint I was given is that we can use partitions of unity, but I have no clue how this ties in with what I have constructed.","n\in\mathbb{N} n \pi:E\rightarrow B \forall p\in B: E_p := \pi^{-1}(p) n \forall p\in B U p \psi:E\mid_U:=\pi^{-1}(U)\rightarrow U\times \mathbb{R}^n \operatorname{pr}_1\circ\psi = \pi \psi\mid_{E_q}:E_q\rightarrow\{q\}\times\mathbb{R}^n q\in U E M \pi:E\rightarrow M x\in M g_x : E_x\times E_x\rightarrow \mathbb{R} x g x g(v,w) M v,w E \mathbb{R} x\in M U x \psi g_x((a,b)) = <\operatorname{pr}_2(\psi(a)),\operatorname{pr}_2(\psi(b))>_\mathbb{R} g_x g_x a_1,a_2\in E_x \psi(a_1)+\psi(a_2) = \psi(a_1+a_2) \psi \psi(a_i)\in\{x\}\times\mathbb{R}^n \operatorname{pr}_2(\psi(a_1)+\psi(a_2))=\operatorname{pr}_2(\psi(a_1))+\operatorname{pr}_2(\psi(a_2)) g x g E_x",['differential-geometry']
29,"Prove that if the chord length depends only on |s-t|, then it is a line or a part of a circle.","Prove that if the chord length depends only on |s-t|, then it is a line or a part of a circle.",,"Let $\alpha : (a,b) \rightarrow \mathbb{R}^2$ be a smooth map(infinitely differentiable). Show that if the chord length $\Vert{\alpha(s)-\alpha(t)}\Vert$ depends only on $|s-t|$ , then it is a line or a part of a circle. It comes from Shifrin's differential geometry notes. Here is my attempt: Since the chord length only depends on $|s-t|$ , for $s-t=\delta, -\delta$ , the chord length should be same. $\Vert \alpha(s+\delta) -\alpha(s) \Vert = \Vert \alpha(s) - \alpha(s-\delta) \Vert$ Squaring both sides and viewing both sides as an inner product of themselves, respectively. And expanding Taylor series and discarding terms with higher degree than 2 of $\delta$ 's. Then I arrived at $\alpha(s)' \cdot  \alpha(s)'' =0$ . And I have no idea to do. Could you give me a hint?","Let be a smooth map(infinitely differentiable). Show that if the chord length depends only on , then it is a line or a part of a circle. It comes from Shifrin's differential geometry notes. Here is my attempt: Since the chord length only depends on , for , the chord length should be same. Squaring both sides and viewing both sides as an inner product of themselves, respectively. And expanding Taylor series and discarding terms with higher degree than 2 of 's. Then I arrived at . And I have no idea to do. Could you give me a hint?","\alpha : (a,b) \rightarrow \mathbb{R}^2 \Vert{\alpha(s)-\alpha(t)}\Vert |s-t| |s-t| s-t=\delta, -\delta \Vert \alpha(s+\delta) -\alpha(s) \Vert = \Vert \alpha(s) - \alpha(s-\delta) \Vert \delta \alpha(s)' \cdot  \alpha(s)'' =0",['differential-geometry']
30,definition of the hyperbolic space,definition of the hyperbolic space,,"In Jurgen Jost's book, there is an example about Hyperbolic space in Chapter 1.The author mentions that we can equip $\Bbb R^{n+1}$ with an inner product $\langle x,y\rangle=x^1y^1+\cdots + x^ny^n-x^{n+1}y^{n+1}$ ,where $x=(x^1,\cdots, x^{n+1}),y=(y^1,\cdots, y^{n+1})$ . According to the definition of inner product,the following property should be satisfied: $\langle x,x\rangle\geq 0$ ,but the inner product defined above does not have positivity.","In Jurgen Jost's book, there is an example about Hyperbolic space in Chapter 1.The author mentions that we can equip with an inner product ,where . According to the definition of inner product,the following property should be satisfied: ,but the inner product defined above does not have positivity.","\Bbb R^{n+1} \langle x,y\rangle=x^1y^1+\cdots + x^ny^n-x^{n+1}y^{n+1} x=(x^1,\cdots, x^{n+1}),y=(y^1,\cdots, y^{n+1}) \langle x,x\rangle\geq 0","['differential-geometry', 'manifolds', 'riemannian-geometry', 'smooth-manifolds']"
31,Taylor expansion of a function on the unit sphere,Taylor expansion of a function on the unit sphere,,"I would like to understand how Taylor expansion works on the unit sphere. Similar questions have already been asked here and here , but I did not quite understand the answers. What stopped me from understanding the answers was mostly my lack of knowledge about differential geometry and manifolds (I guess). Maybe someone could guide me through an example. Assume we are given a function $f: \mathbb{S}^2 \rightarrow \mathbb{R}$ . To make it more precise I just assume a polynomial $f(x,y,z) = x^1y^2 z^3$ . Let $p$ be the north pole, e.g. $p=(0,0,1)^T$ . Let $q$ be close to $p$ and given by $q=\epsilon \cdot (a,b,c)^T \in \mathbb{S}^2$ for $\epsilon$ small. The previous answers were mentioning the tangent space, which, to my understanding, is the plane $\{(x,y,1) \text{ for } x,y \in\mathbb{R}\}$ . Now the part where I am unsure. Do I simply compute $\partial_x f(x,y,z) = y^2z^3$ and $\partial_y f(x,y,z) = 2xyz^3$ And obtain $f(q) \approx f(p) + q_1\partial_x f(p) + q_2 \partial_y f(p)$ where $q_i$ represents the $i$ -th entry of $q$ . What about the difference in $z$ ? I somehow need to incorporate the curvature of my space, or (and maybe equivalently) the mapping from the sphere to the tangent space. How would this work? What would this mapping look like for the unit sphere? Instead of $f(q) \approx f(p) + q_1\partial_x f(p) + q_2 \partial_y f(p)$ , do I maybe have to use $f(q) \approx f(p) + \tilde{q}_1\partial_x f(p) + \tilde{q}_2 \partial_y f(p)$ where $\tilde{q}$ is the projection of $q$ onto the tangent space?","I would like to understand how Taylor expansion works on the unit sphere. Similar questions have already been asked here and here , but I did not quite understand the answers. What stopped me from understanding the answers was mostly my lack of knowledge about differential geometry and manifolds (I guess). Maybe someone could guide me through an example. Assume we are given a function . To make it more precise I just assume a polynomial . Let be the north pole, e.g. . Let be close to and given by for small. The previous answers were mentioning the tangent space, which, to my understanding, is the plane . Now the part where I am unsure. Do I simply compute and And obtain where represents the -th entry of . What about the difference in ? I somehow need to incorporate the curvature of my space, or (and maybe equivalently) the mapping from the sphere to the tangent space. How would this work? What would this mapping look like for the unit sphere? Instead of , do I maybe have to use where is the projection of onto the tangent space?","f: \mathbb{S}^2 \rightarrow \mathbb{R} f(x,y,z) = x^1y^2 z^3 p p=(0,0,1)^T q p q=\epsilon \cdot (a,b,c)^T \in \mathbb{S}^2 \epsilon \{(x,y,1) \text{ for } x,y \in\mathbb{R}\} \partial_x f(x,y,z) = y^2z^3 \partial_y f(x,y,z) = 2xyz^3 f(q) \approx f(p) + q_1\partial_x f(p) + q_2 \partial_y f(p) q_i i q z f(q) \approx f(p) + q_1\partial_x f(p) + q_2 \partial_y f(p) f(q) \approx f(p) + \tilde{q}_1\partial_x f(p) + \tilde{q}_2 \partial_y f(p) \tilde{q} q","['differential-geometry', 'taylor-expansion']"
32,Help in proof from Riemannian Geometry by Docarmo.,Help in proof from Riemannian Geometry by Docarmo.,,"I have been working on ${\it Lemma\,5.2}$ from Riemannian Geometry by DoCarmo which establishes the existence and uniqueness of the vector field $Zf=(XY-YX)f$ , given $X$ and $Y$ as differenciable vector fields. On this proof we have expressions for $XYf$ and $YXf$ as follows: $XYf=\sum_{i,j}a_{i}\frac{\partial b_{j}}{\partial x_{i}}\frac{\partial f}{\partial x_{j}}+\sum_{i,j}a_{i}b_{j}\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}}$ $YXf=\sum_{i,j}b_{j}\frac{\partial a_{i}}{\partial x_{i}}\frac{\partial f}{\partial x_{j}}+\sum_{i,j}a_{i}b_{j}\frac{\partial^{2}f}{\partial x_{j}\partial x_{i}}$ Where $Xf=\sum_{i}a_{i}\frac{\partial f}{\partial x_{i}}$ and $Yf=\sum_{j}b_{j}\frac{\partial f}{\partial x_{j}}$ . If I substract expressions of the items I obtain $$Zf=XYf-YXf=\sum_{i,j}\left(a_{i}\frac{\partial b_{j}}{\partial x_{i}}\frac{\partial f}{\partial x_{j}}-b_{j}\frac{\partial a_{i}}{\partial x_{j}}\frac{\partial f}{\partial x_{i}}\right).$$ But DoCarmos says that this turns out to be $$Zf=XYf-YXf=\sum_{i,j}\left(a_{i}\frac{\partial b_{j}}{\partial x_{i}}-b_{i}\frac{\partial a_{j}}{\partial x_{i}}\right)\frac{\partial f}{\partial x_{j}}$$ as if $\frac{\partial f}{\partial x_{i}}$ and $\frac{\partial f}{\partial x_{j}}$ were the same.","I have been working on from Riemannian Geometry by DoCarmo which establishes the existence and uniqueness of the vector field , given and as differenciable vector fields. On this proof we have expressions for and as follows: Where and . If I substract expressions of the items I obtain But DoCarmos says that this turns out to be as if and were the same.","{\it Lemma\,5.2} Zf=(XY-YX)f X Y XYf YXf XYf=\sum_{i,j}a_{i}\frac{\partial b_{j}}{\partial x_{i}}\frac{\partial f}{\partial x_{j}}+\sum_{i,j}a_{i}b_{j}\frac{\partial^{2}f}{\partial x_{i}\partial x_{j}} YXf=\sum_{i,j}b_{j}\frac{\partial a_{i}}{\partial x_{i}}\frac{\partial f}{\partial x_{j}}+\sum_{i,j}a_{i}b_{j}\frac{\partial^{2}f}{\partial x_{j}\partial x_{i}} Xf=\sum_{i}a_{i}\frac{\partial f}{\partial x_{i}} Yf=\sum_{j}b_{j}\frac{\partial f}{\partial x_{j}} Zf=XYf-YXf=\sum_{i,j}\left(a_{i}\frac{\partial b_{j}}{\partial x_{i}}\frac{\partial f}{\partial x_{j}}-b_{j}\frac{\partial a_{i}}{\partial x_{j}}\frac{\partial f}{\partial x_{i}}\right). Zf=XYf-YXf=\sum_{i,j}\left(a_{i}\frac{\partial b_{j}}{\partial x_{i}}-b_{i}\frac{\partial a_{j}}{\partial x_{i}}\right)\frac{\partial f}{\partial x_{j}} \frac{\partial f}{\partial x_{i}} \frac{\partial f}{\partial x_{j}}",['differential-geometry']
33,$SU(3)$ acts transitively on $S^5$.,acts transitively on .,SU(3) S^5,"I'm told that $SU(n)$ acts transitively on $S^{2n-1} \subset \mathbb{C}^n$ by matrix multiplication. Yet I can't find a proof of this anywhere, so I was trying to construct a proof on my own by mimicking a version of the proof that I know for showing $SO(n)$ acts transitively on $S^{n-1}$ . My proof is outlined as follows: To show transitive, it suffices to show that the point $x= \begin{pmatrix}  1  \\ 0  \\ 0  \end{pmatrix} \in S^5 \subset \mathbb{C}^3$ can be taken to any other point $p=\begin{pmatrix}  p_1  \\ p_2  \\ p_3 \end{pmatrix} \in S^5 \subset \mathbb{C}^3. $ A complex matrix lies in $SU(3)$ if and only if its columns form an orthonormal basis for $\mathbb{C}^3$ . If $A \in SU(3)$ has the property that $Ax=p$ , then $A$ must have $p$ as its first column. Completing $p$ to a basis for $\mathbb{C}^3$ , then running the Gram-Schmidt algorithm on this basis (starting with $p$ in the first step of the algorithm) completes $p$ to an orthonormal basis for $\mathbb{C}^3$ . Then sticking these basis elements in as the columns of $A$ , with $p$ as the first column, yields a matrix in $U(n)$ that takes $x$ to $p$ . This proof in general shows that $U(n)$ acts transitively on $S^{2n-1}$ , but the matrix I constructed does not necessarily have determinant $1$ . In the completely analogous proof for showing $SO(n)$ acts on $S^n$ transitively, the matrix has determinant $\pm1$ so if the determinant is $-1$ , we need only interchange the last two columns to get a matrix with determinant $1$ . This proof breaks down here because we only know that the determinant lies on the unit circle in $\mathbb{C}$ , that is $|\det(A)|=1$ . Is there some way to produce a matrix with determinant dead equal to $1$ with this process? Otherwise, is there another simple proof of transitivity?","I'm told that acts transitively on by matrix multiplication. Yet I can't find a proof of this anywhere, so I was trying to construct a proof on my own by mimicking a version of the proof that I know for showing acts transitively on . My proof is outlined as follows: To show transitive, it suffices to show that the point can be taken to any other point A complex matrix lies in if and only if its columns form an orthonormal basis for . If has the property that , then must have as its first column. Completing to a basis for , then running the Gram-Schmidt algorithm on this basis (starting with in the first step of the algorithm) completes to an orthonormal basis for . Then sticking these basis elements in as the columns of , with as the first column, yields a matrix in that takes to . This proof in general shows that acts transitively on , but the matrix I constructed does not necessarily have determinant . In the completely analogous proof for showing acts on transitively, the matrix has determinant so if the determinant is , we need only interchange the last two columns to get a matrix with determinant . This proof breaks down here because we only know that the determinant lies on the unit circle in , that is . Is there some way to produce a matrix with determinant dead equal to with this process? Otherwise, is there another simple proof of transitivity?","SU(n) S^{2n-1} \subset \mathbb{C}^n SO(n) S^{n-1} x= \begin{pmatrix} 
1  \\
0  \\
0 
\end{pmatrix} \in S^5 \subset \mathbb{C}^3 p=\begin{pmatrix} 
p_1  \\
p_2  \\
p_3
\end{pmatrix} \in S^5 \subset \mathbb{C}^3.  SU(3) \mathbb{C}^3 A \in SU(3) Ax=p A p p \mathbb{C}^3 p p \mathbb{C}^3 A p U(n) x p U(n) S^{2n-1} 1 SO(n) S^n \pm1 -1 1 \mathbb{C} |\det(A)|=1 1","['differential-geometry', 'lie-groups', 'group-actions', 'linear-groups']"
34,Reference for proof of Rigid Topological Theorem,Reference for proof of Rigid Topological Theorem,,Where can I find the proof of the following theorem: Let $M$ be a compact manifold such   that $K\geq 0$ . Then there is an exact sequence $$0\to \Phi \to \pi_1(M) \to B\to 0$$ where $\Phi$ is a finite group and $\bf B$ is a crystallographic group on $\Bbb R^k$ for some $k\leq \dim M$ . I would be greatly appreciate if someone provide link of a book or paper for proof and similar theorems?,Where can I find the proof of the following theorem: Let be a compact manifold such   that . Then there is an exact sequence where is a finite group and is a crystallographic group on for some . I would be greatly appreciate if someone provide link of a book or paper for proof and similar theorems?,M K\geq 0 0\to \Phi \to \pi_1(M) \to B\to 0 \Phi \bf B \Bbb R^k k\leq \dim M,"['differential-geometry', 'reference-request', 'differential-topology', 'riemannian-geometry']"
35,Matrix of shape operator of the sphere,Matrix of shape operator of the sphere,,"Problem: Let $M$ be a sphere of radius $a$ in $\mathbb{R}^3$ , defined by $$ x^2 + y^2 + z^2 = a^2. $$ Parametrize the sphere using spherical coordinates $$x = a \sin \phi \cos \theta \\ y = a \sin \phi \sin \theta \\ z = a \cos \phi $$ where $0 \leq \phi < \pi, 0 \leq \theta < 2 \pi$ . Then for each $p \in M, e_1 = \partial/\partial\phi$ and $e_2 = \partial/ \partial \theta$ is a basis for the tangent space $T_p M$ for $p \in M$ . Let $N_p$ be the unit outward normal vector at $p$ on the sphere. Find the matrix of the shape operator of the sphere with respect to the basis $e_1, e_2$ . Attempt: I need to calculate $L(e_j) = - D_{e_j} N$ and then find the matrix $[a^{i}_{j}]$ such that $L(e_j) = \sum a^{i}_{j} e_i. $ Here $L$ denotes the shape operator. I know that in cartesian coordinates, a normal unit vector is $N = \frac{1}{a} (x, y, z)$ . But do I have to write this first in the $(\theta, \phi)$ basis? I would calculate e.g. $$ D_{\partial_{\phi}} \frac{1}{a} (a \sin \phi \cos \theta, a \sin \phi \sin \theta, a \cos \phi) \\ = ( \cos \phi \cos \theta, \cos \phi \sin \theta, - \sin \phi). $$ But I need to write this as a linear combination of the basis vectors $\partial_{\phi}$ and $\partial_{\theta}$ to find the matrix representation?","Problem: Let be a sphere of radius in , defined by Parametrize the sphere using spherical coordinates where . Then for each and is a basis for the tangent space for . Let be the unit outward normal vector at on the sphere. Find the matrix of the shape operator of the sphere with respect to the basis . Attempt: I need to calculate and then find the matrix such that Here denotes the shape operator. I know that in cartesian coordinates, a normal unit vector is . But do I have to write this first in the basis? I would calculate e.g. But I need to write this as a linear combination of the basis vectors and to find the matrix representation?","M a \mathbb{R}^3  x^2 + y^2 + z^2 = a^2.  x = a \sin \phi \cos \theta \\ y = a \sin \phi \sin \theta \\ z = a \cos \phi  0 \leq \phi < \pi, 0 \leq \theta < 2 \pi p \in M, e_1 = \partial/\partial\phi e_2 = \partial/ \partial \theta T_p M p \in M N_p p e_1, e_2 L(e_j) = - D_{e_j} N [a^{i}_{j}] L(e_j) = \sum a^{i}_{j} e_i.  L N = \frac{1}{a} (x, y, z) (\theta, \phi)  D_{\partial_{\phi}} \frac{1}{a} (a \sin \phi \cos \theta, a \sin \phi \sin \theta, a \cos \phi) \\ = ( \cos \phi \cos \theta, \cos \phi \sin \theta, - \sin \phi).  \partial_{\phi} \partial_{\theta}","['differential-geometry', 'riemannian-geometry', 'surfaces']"
36,Eigenfunctions of Laplace-Beltrami operator,Eigenfunctions of Laplace-Beltrami operator,,"Let $M$ be a compact Riemannian manifold and $$Lf:=-\operatorname{div} \nabla(f)$$ be the Laplace-Beltrami operator. Let $f$ be a smooth function on $M$ . Consider the optimization problem of minimizing $$\int_ML(f)f$$ under the constraint $\int_M |f|^2=1$ . I wonder how to prove that if $f$ minimize the integral $\int_ML(f)f$ , then it must be an eigenfunction of $L$ , i.e. $L(f)=\lambda f$ for some $\lambda$ . I know it can be shown that $\int_M\|\nabla f\|^2= \int_ML(f)f$ but this may not be helpful.","Let be a compact Riemannian manifold and be the Laplace-Beltrami operator. Let be a smooth function on . Consider the optimization problem of minimizing under the constraint . I wonder how to prove that if minimize the integral , then it must be an eigenfunction of , i.e. for some . I know it can be shown that but this may not be helpful.",M Lf:=-\operatorname{div} \nabla(f) f M \int_ML(f)f \int_M |f|^2=1 f \int_ML(f)f L L(f)=\lambda f \lambda \int_M\|\nabla f\|^2= \int_ML(f)f,"['differential-geometry', 'riemannian-geometry']"
37,About Poincaré-Hopf theorem: can we find and draw $v$ with only one zero on $S$?,About Poincaré-Hopf theorem: can we find and draw  with only one zero on ?,v S,"Disclaimer: It turns that that there is a positive answer to a more general question than my original question (see here ), but this answer isn't what I am really looking for: I would like some visual constructions. Also I don't want to make a new post because an answer has already been posted. The Poincaré-Hopf theorem (for surfaces) states that if $v$ is a smooth vector field with isolated zeros on a compact surface $S$ , then the sum of the index of these zeros is equal to the Euler characteristic of $S$ : $$\sum_{x\text{ zero of }v} ind(v,x)=\chi(S).$$ Also during the proof that I know, we constructed explicitly a vector field $v$ on $S$ with isolated zeros, namely by triangulating the surface. My original question was: In the case where $\chi(S)\neq 0$ , i.e $S$ is neither a torus nor a klein bottle, can we ask that $v$ has only one zero $x_0$ , which therefore satisfies $ind(v,x_0)=\chi(S)~?$ So it turns out that this is true in general (even in higher dimensions). What I really want to know, now that the existence of such vector field is proven, is this: Can we provide some (nice) pictures of these vector fields $v$ on $S$ ? This can be done when $S$ is the sphere $S^2$ , by pulling back the constant vector field $\frac{\partial}{\partial x}$ on $\Bbb R^2$ via the stereographic projection and extending this pulled-back vector field to $0$ on the north pole. The picture is as follows (with a zoom on the right): For the case where $S$ is the protective plane $\Bbb{RP}^2=S^2/_{\{\pm id\}}$ , this is also possible.  If we take the vector field on the sphere which is drawn below (each trajectory stays in horizontal hyperplanes basically) then this vector field has two zeros (the north pole $N$ and the south pole $S$ ), both of degree one. Moreover, this vector field is invariant under the antipodal map, and therefore it gives rise to a vector field on $\Bbb{RP}^2$ and this vector field has only one zero, namely $x_0=\{N,S\}$ , with degree $1$ . For the case $S=M_g$ of the connected sum of $g\geq 2$ torus, I tried to do drawings and write $M_g$ as a polygon with edges identified, but I can't find an example. For example at least when $g=2$ , I am hoping to find a comprehensive picture (on the embedded surface or on the polygon with edges identified). I haven't thought about the other cases yet (connected sum of projective planes), but it should be of the same difficulty. Any help would be greatly appreciated!","Disclaimer: It turns that that there is a positive answer to a more general question than my original question (see here ), but this answer isn't what I am really looking for: I would like some visual constructions. Also I don't want to make a new post because an answer has already been posted. The Poincaré-Hopf theorem (for surfaces) states that if is a smooth vector field with isolated zeros on a compact surface , then the sum of the index of these zeros is equal to the Euler characteristic of : Also during the proof that I know, we constructed explicitly a vector field on with isolated zeros, namely by triangulating the surface. My original question was: In the case where , i.e is neither a torus nor a klein bottle, can we ask that has only one zero , which therefore satisfies So it turns out that this is true in general (even in higher dimensions). What I really want to know, now that the existence of such vector field is proven, is this: Can we provide some (nice) pictures of these vector fields on ? This can be done when is the sphere , by pulling back the constant vector field on via the stereographic projection and extending this pulled-back vector field to on the north pole. The picture is as follows (with a zoom on the right): For the case where is the protective plane , this is also possible.  If we take the vector field on the sphere which is drawn below (each trajectory stays in horizontal hyperplanes basically) then this vector field has two zeros (the north pole and the south pole ), both of degree one. Moreover, this vector field is invariant under the antipodal map, and therefore it gives rise to a vector field on and this vector field has only one zero, namely , with degree . For the case of the connected sum of torus, I tried to do drawings and write as a polygon with edges identified, but I can't find an example. For example at least when , I am hoping to find a comprehensive picture (on the embedded surface or on the polygon with edges identified). I haven't thought about the other cases yet (connected sum of projective planes), but it should be of the same difficulty. Any help would be greatly appreciated!","v S S \sum_{x\text{ zero of }v} ind(v,x)=\chi(S). v S \chi(S)\neq 0 S v x_0 ind(v,x_0)=\chi(S)~? v S S S^2 \frac{\partial}{\partial x} \Bbb R^2 0 S \Bbb{RP}^2=S^2/_{\{\pm id\}} N S \Bbb{RP}^2 x_0=\{N,S\} 1 S=M_g g\geq 2 M_g g=2","['differential-geometry', 'differential-topology', 'surfaces', 'vector-fields']"
38,Cotangent lift of an action and its effect on the moment covector,Cotangent lift of an action and its effect on the moment covector,,"If I have an action of a Lie group on a configuration space。 $G\to \text{Diff}(M)$ , $g \mapsto \rho_g$ , $\rho_g : q \mapsto \rho_g(q)$ (for example a rotation). Then when we consider the phase spaces $T^*M$ , we provide it with the action : $G\to \text{Diff}(T^*M)$ , $g \mapsto \rho^*_{g^{-1}}$ , $\rho_{g^{-1}}^* \colon (q,p) \mapsto (\rho_g(q),\rho^*_{g^{-1}}(p))$ . Now I understand that the point $q$ is send to its image under the action, but I don't understand why the moment $p$ is transformed as $\rho^*_{g^{-1}}(p)$ under the action? Why is the reason we consider this weird action with a pullback on the moment? Why do we want the moment to transform in this way?","If I have an action of a Lie group on a configuration space。 , , (for example a rotation). Then when we consider the phase spaces , we provide it with the action : , , . Now I understand that the point is send to its image under the action, but I don't understand why the moment is transformed as under the action? Why is the reason we consider this weird action with a pullback on the moment? Why do we want the moment to transform in this way?","G\to \text{Diff}(M) g \mapsto \rho_g \rho_g : q \mapsto \rho_g(q) T^*M G\to \text{Diff}(T^*M) g \mapsto \rho^*_{g^{-1}} \rho_{g^{-1}}^* \colon (q,p) \mapsto (\rho_g(q),\rho^*_{g^{-1}}(p)) q p \rho^*_{g^{-1}}(p)","['differential-geometry', 'lie-groups', 'moment-map']"
39,How to solve Poisson's equation on compact Riemann surfaces of genus greater than one?,How to solve Poisson's equation on compact Riemann surfaces of genus greater than one?,,"$M$ is a compact Riemann surface, $f\in C^{\infty}(M)$ . I want to find the solution of $\Delta \varphi=f$ . When $M=T^2=\mathbb{R}^2/2\pi \mathbb{Z}^2$ , I can use Fourier series on $\mathbb{R}^2$ to solve this equation. When $M$ are compact Riemann surfaces of genus greater than one, how to solve it? By the uniformization theorem, compact Riemann surfaces of genus greater than one have simply connected universal covering surface given by the unit disk $D$ (poincare metric).  And $M=D/\Gamma$ , $\Gamma$ is a Fuchsian group of Mobius transformations. So maybe I can solve possion's equation on the Poincare disk?","is a compact Riemann surface, . I want to find the solution of . When , I can use Fourier series on to solve this equation. When are compact Riemann surfaces of genus greater than one, how to solve it? By the uniformization theorem, compact Riemann surfaces of genus greater than one have simply connected universal covering surface given by the unit disk (poincare metric).  And , is a Fuchsian group of Mobius transformations. So maybe I can solve possion's equation on the Poincare disk?",M f\in C^{\infty}(M) \Delta \varphi=f M=T^2=\mathbb{R}^2/2\pi \mathbb{Z}^2 \mathbb{R}^2 M D M=D/\Gamma \Gamma,"['differential-geometry', 'partial-differential-equations', 'riemannian-geometry', 'riemann-surfaces', 'hyperbolic-geometry']"
40,Visualizing the Lie Bracket in connection with Torsion,Visualizing the Lie Bracket in connection with Torsion,,"I have seen this kind of picture a lot, linking the Lie Bracket with the Torsion (e.g. 1 , 2 , 3 , 4 ). I will report for convenience one of such picture, from Hehl and Obukhov review article For some reason I am not able to understand how the Lie Bracket can be seen in that way. I will say what I have done so far, I would be glad if someone would point me to the missing piece or maybe if I am misunderstanding the approximation done. Let us take two vector fields $u$ and $v$ on a manifold $M$ . Choose a $P\in M$ , we can say that $$ u_P = u^k \left( \frac{\partial}{\partial x^k} \right)_P $$ $$ v_P = v^k \left( \frac{\partial}{\partial x^k} \right)_P $$ Let me say with abuse of notation that $R=P+\epsilon v$ and $Q=P+\epsilon u$ (I believe the formal way would be to go down in the chart and then coming back with the inverse). Anyway $u$ in $R$ is $$ u_R \approx \left( u^k +\epsilon v^i \frac{\partial u^k}{\partial x^i} \right) \left( \frac{\partial}{\partial x^k} \right)_R \approx \left( u^k + \epsilon v^i \frac{\partial u^k}{\partial x^i} + \epsilon v^i u^j \Gamma_{ij}^k + O(\epsilon^2)\right) \left( \frac{\partial}{\partial x^k} \right)_P $$ while the parallel transport of $u$ along $v$ $$ u_R^{//} \approx \left( u^k - \epsilon \Gamma_{ij}^k v^i u^j \right) \left( \frac{\partial}{\partial x^k} \right)_R \approx  \left( u^k + O(\epsilon^2) \right) \left( \frac{\partial}{\partial x^k} \right)_P $$ Which I reported in the tangent space of $P$ , using the fact that \begin{equation} \left( \frac{\partial}{\partial x^k} \right)_R = \left( \frac{\partial}{\partial x^k} \right)_P + \epsilon v^i \Gamma_{ik}^j \left( \frac{\partial}{\partial x^j} \right)_P \end{equation} Now that my vectors all belong to the same tangent space $T_P M$ I can finally verify $$ u_R - u_R^{//} = \nabla_{v} u  $$ If I want to get the Lie Bracket from the drawing, I can calculate the vectors $u_R$ and $v_Q$ , report them in $P$ and combine them accordingly, i.e. \begin{equation} \begin{split} (u_P + v_Q) - (v_P + u_R) &= u^k + v^k + \epsilon v^i u^j \Gamma_{ij}^k + \epsilon v^i \frac{\partial u^k}{\partial x^i} - \left( u^k + v^k + \epsilon u^i v^j \Gamma_{ij}^k + \epsilon u^i \frac{\partial v^k}{\partial x^i} \right) \\ &= \epsilon v^i u^j T_{ij}^k - \epsilon [u,v] \end{split} \end{equation} So also the torsion appears to me in that subtraction, and that is why I cannot understand that picture... What am I missing? It seems to me that I should not report the vectors back in P, but then how can I add or subtract vectors in different tangent spaces? And once this is solved, what is the connection of the Lie Bracket with the Torsion, could anybody shine light on this?","I have seen this kind of picture a lot, linking the Lie Bracket with the Torsion (e.g. 1 , 2 , 3 , 4 ). I will report for convenience one of such picture, from Hehl and Obukhov review article For some reason I am not able to understand how the Lie Bracket can be seen in that way. I will say what I have done so far, I would be glad if someone would point me to the missing piece or maybe if I am misunderstanding the approximation done. Let us take two vector fields and on a manifold . Choose a , we can say that Let me say with abuse of notation that and (I believe the formal way would be to go down in the chart and then coming back with the inverse). Anyway in is while the parallel transport of along Which I reported in the tangent space of , using the fact that Now that my vectors all belong to the same tangent space I can finally verify If I want to get the Lie Bracket from the drawing, I can calculate the vectors and , report them in and combine them accordingly, i.e. So also the torsion appears to me in that subtraction, and that is why I cannot understand that picture... What am I missing? It seems to me that I should not report the vectors back in P, but then how can I add or subtract vectors in different tangent spaces? And once this is solved, what is the connection of the Lie Bracket with the Torsion, could anybody shine light on this?","u v M P\in M 
u_P = u^k \left( \frac{\partial}{\partial x^k} \right)_P
 
v_P = v^k \left( \frac{\partial}{\partial x^k} \right)_P
 R=P+\epsilon v Q=P+\epsilon u u R 
u_R \approx \left( u^k +\epsilon v^i \frac{\partial u^k}{\partial x^i} \right) \left( \frac{\partial}{\partial x^k} \right)_R \approx
\left( u^k + \epsilon v^i \frac{\partial u^k}{\partial x^i} + \epsilon v^i u^j \Gamma_{ij}^k + O(\epsilon^2)\right) \left( \frac{\partial}{\partial x^k} \right)_P
 u v 
u_R^{//} \approx \left( u^k - \epsilon \Gamma_{ij}^k v^i u^j \right) \left( \frac{\partial}{\partial x^k} \right)_R \approx 
\left( u^k + O(\epsilon^2) \right) \left( \frac{\partial}{\partial x^k} \right)_P
 P \begin{equation}
\left( \frac{\partial}{\partial x^k} \right)_R = \left( \frac{\partial}{\partial x^k} \right)_P + \epsilon v^i \Gamma_{ik}^j \left( \frac{\partial}{\partial x^j} \right)_P
\end{equation} T_P M 
u_R - u_R^{//} = \nabla_{v} u 
 u_R v_Q P \begin{equation}
\begin{split}
(u_P + v_Q) - (v_P + u_R) &= u^k + v^k + \epsilon v^i u^j \Gamma_{ij}^k + \epsilon v^i \frac{\partial u^k}{\partial x^i} - \left( u^k + v^k + \epsilon u^i v^j \Gamma_{ij}^k + \epsilon u^i \frac{\partial v^k}{\partial x^i} \right) \\
&= \epsilon v^i u^j T_{ij}^k - \epsilon [u,v]
\end{split}
\end{equation}","['differential-geometry', 'smooth-manifolds', 'curvature', 'lie-derivative']"
41,Geodesics and minimal surfaces.,Geodesics and minimal surfaces.,,"We say that a submanifold $M$ of a Riemannian manifold $(N,h)$ is minimal if it's second fundamental form is traceless i.e (as far as I understand) Trace $B = \sum_{0}^{m}B(X_{i},X_{i})=0$ , for any orthonormal frame for the tangent bundle $TM$ . Is this in some sense multidimensional analogue of a geodesic? Minimal surfaces seams to be the minimize the area functional instead of length functional.","We say that a submanifold of a Riemannian manifold is minimal if it's second fundamental form is traceless i.e (as far as I understand) Trace , for any orthonormal frame for the tangent bundle . Is this in some sense multidimensional analogue of a geodesic? Minimal surfaces seams to be the minimize the area functional instead of length functional.","M (N,h) B = \sum_{0}^{m}B(X_{i},X_{i})=0 TM","['differential-geometry', 'soft-question', 'riemannian-geometry']"
42,Understanding the definition of an unramified morphism of schemes,Understanding the definition of an unramified morphism of schemes,,"This definition is from the book Neron Models . I want to understand what this definition is saying in the affine case.  Suppose $X = \operatorname{Spec} B, S = \operatorname{Spec A}$ , and $f$ comes from a finitely presented ring homomorphism $\phi: A \rightarrow B$ . Then $B = A[T_1, ... , T_n]/\mathfrak a$ for some $n$ and some finitely generated ideal $\mathfrak a$ , and we have a closed $A$ -immersion $j:X \rightarrow \mathbb A_A^n$ .  So in the definition we can take the open set $U$ to be all of $X$ , and the sheaf of ideals $\mathscr I$ in the definition identifies with the ideal $\mathfrak a$ . Let $\mathfrak q$ be a prime of $B$ at which $f$ is unramified, say $\mathfrak q = \mathfrak Q/\mathfrak a$ for a prime $\mathfrak Q$ of $A[\underline{T}] = A[T_1, ... , T_n]$ , contracting to a prime $\mathfrak p$ of $A$ . We have the universal $A$ -linear map $d: A[\underline{T}] \rightarrow \Omega_{A[\underline{T}]/A}$ which localizes nicely: $$d_{\mathfrak p}: A_{\mathfrak p}[\underline{T}] \rightarrow \Omega_{A_{\mathfrak p}[\underline{T}]/A_{\mathfrak p}}$$ There is also an exact sequence of $B = A[\underline{T}]/\mathfrak a$ modules: $$\mathfrak a/\mathfrak a^2 \rightarrow \Omega_{A[\underline{T}]/A} \otimes_{A[\underline{T}]} B \rightarrow \Omega_{B/A} \rightarrow 0$$ where the map on the left is induced by $d$ . I cannot quite work out what condition (b) is saying in this case.  I get the general idea that the image of some localization of $\mathfrak a$ under $d$ should generate some localization of $\Omega_{A[\underline{T}]/A}$ .","This definition is from the book Neron Models . I want to understand what this definition is saying in the affine case.  Suppose , and comes from a finitely presented ring homomorphism . Then for some and some finitely generated ideal , and we have a closed -immersion .  So in the definition we can take the open set to be all of , and the sheaf of ideals in the definition identifies with the ideal . Let be a prime of at which is unramified, say for a prime of , contracting to a prime of . We have the universal -linear map which localizes nicely: There is also an exact sequence of modules: where the map on the left is induced by . I cannot quite work out what condition (b) is saying in this case.  I get the general idea that the image of some localization of under should generate some localization of .","X = \operatorname{Spec} B, S = \operatorname{Spec A} f \phi: A \rightarrow B B = A[T_1, ... , T_n]/\mathfrak a n \mathfrak a A j:X \rightarrow \mathbb A_A^n U X \mathscr I \mathfrak a \mathfrak q B f \mathfrak q = \mathfrak Q/\mathfrak a \mathfrak Q A[\underline{T}] = A[T_1, ... , T_n] \mathfrak p A A d: A[\underline{T}] \rightarrow \Omega_{A[\underline{T}]/A} d_{\mathfrak p}: A_{\mathfrak p}[\underline{T}] \rightarrow \Omega_{A_{\mathfrak p}[\underline{T}]/A_{\mathfrak p}} B = A[\underline{T}]/\mathfrak a \mathfrak a/\mathfrak a^2 \rightarrow \Omega_{A[\underline{T}]/A} \otimes_{A[\underline{T}]} B \rightarrow \Omega_{B/A} \rightarrow 0 d \mathfrak a d \Omega_{A[\underline{T}]/A}","['differential-geometry', 'algebraic-geometry', 'commutative-algebra']"
43,How does a section of a bundle with fibre $\mathbb{P}^N$ give us a spin structure?,How does a section of a bundle with fibre  give us a spin structure?,\mathbb{P}^N,"In ""Spin Structures on Manifolds"" Milnor says that for a principal $SO(n)$ bundle $\pi:E\to X$ a spin structure can be defined as follows: Let $Spin(n)$ act on a sphere $S^N$ in such a way that the kernel $\mathbb{Z}_2$ of the cover $Spin(n) \to SO(n)$ acts freely, thus giving an action of the quotient $SO(n) $ on the quotient $\mathbb{P}^N$ . He considers the bundle with fibre $\mathbb{P}^N$ associated to the principal bundle $\pi:E\to X$ and says that a spin structure can be seen as a section of this bundle but he doesn't give any detail. How can we get a spin structure on $E$ from such a section?","In ""Spin Structures on Manifolds"" Milnor says that for a principal bundle a spin structure can be defined as follows: Let act on a sphere in such a way that the kernel of the cover acts freely, thus giving an action of the quotient on the quotient . He considers the bundle with fibre associated to the principal bundle and says that a spin structure can be seen as a section of this bundle but he doesn't give any detail. How can we get a spin structure on from such a section?",SO(n) \pi:E\to X Spin(n) S^N \mathbb{Z}_2 Spin(n) \to SO(n) SO(n)  \mathbb{P}^N \mathbb{P}^N \pi:E\to X E,"['differential-geometry', 'algebraic-topology', 'differential-topology']"
44,"How to prove Penrose ""Bianchi symmetry"" with non-zero torsion tensor using abstract indexing?","How to prove Penrose ""Bianchi symmetry"" with non-zero torsion tensor using abstract indexing?",,"I want to prove $R_{[αβγ]}^{\ \ \ \ \ \ \ δ} + ∇_{[α}T_{βγ]}^{\ \ \ \ δ} + T_{[αβ}^{\ \ \ \ ρ}\ T_{γ]ρ}^{\ \ \ \ δ} = 0$ EDIT: A brief discussion of the solution found by Matt is at the bottom of this post. The equation is called the ""Bianchi symmetry"" in Vol. 1 of ""Spinors and space-time"" by Penrose and Rindler, and is given there in equation (4.2.39) (with index $σ$ in place of the $δ$ here).  The definitions of $T$ , $R$ and all other symbols are taken from that same text, e.g: $T_{αβ}^{\ \ \ \ γ}$ = Torsion tensor, satisfying $T_{αβ}^{\ \ \ \ γ} \ ∇_γf =  ∇_α∇_βf - ∇_β∇_αf$ , for scalars $f$ $R_{αβγ}^{\ \ \ \ \ \ \ δ}$ = Curvature tensor, satisfying $∇_α∇_βV^δ - ∇_β∇_αV^δ - T_{αβ}^{\ \ \ \ γ}\ ∇_γV^δ = R_{αβγ}^{\ \ \ \ \ \ \ δ}\ V^γ$ , for vectors $V^γ$ , and for covariant vectors, $V_δ$ : $\ \ \ \ \ \ \ \ \ \ \ \ \ ∇_α∇_βV_γ - ∇_β∇_αV_γ - T_{αβ}^{\ \ \ \ ρ}\ ∇_ρV_γ = -R_{αβγ}^{\ \ \ \ \ \ \ δ}\ V_δ$ I am trying to use the manner suggested in the text that is ""along the same lines"" as , but ""more elaborate"" than how the torsion-free version (4.2.37) is derived there, but also without using (4.2.52).  The approach I have taken looks like this: The first two (identical) equations below are shown as merely modest reformulations of others published (for other purposes) in the book as indicated (noting the fairly obvious book-typo in the first), so I am virtually certain the setup preceding the third equation here is correct. What then follows are just some fairly straightforward transformations, so I am suspecting the tensor $T_{[αβ}^{\ \ \ \ δ}\  ∇_{γ]}∇_δf$ highlighted at the end must be zero, but am unable to show why, perhaps because it isn't. ( edit: In fact it isn't.  See end of question.) As in (4.2.40), but with f in place of $V^δ$ (not the $V^γ$ -typo appearing in my edition): $2∇_{[[α}∇_{β]}∇_{γ]} f = T_{[αβ}^{\ \ \ \ ρ}∇_{|ρ|}∇_{γ]}f − R_{[αβγ]}^{\ \ \ \ \ \ \ δ}\ ∇_δf$ . Equivalently, as in (4.2.35), but symmetrized in α, β, and γ and including T: $−R_{[αβγ]}^{\ \ \ \ \ \ \ δ}∇_δ f = 2∇_{[[α}∇_{β]}∇_{γ]}f − T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}f$ $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = 2∇_{[α}∇_{[β}∇_{γ]]}f − T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}f$ $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = ∇_{[α}Δ_{βγ]}f − T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}f\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $ (with $Δ_{βγ} := 2∇_{[β}∇_{γ]}$ ) $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = ∇_{[α}T_{βγ]}^{\ \ \ \ δ}\ ∇_δf − T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}f\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $ (4.2.22 applied to first term above) $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = ∇_{[α}T_{βγ]}^{\ \ \ \ ρ}\ ∇_δf + T_{[αβ}^{\ \ \ \ ρ}\ T_{γ]ρ}^{\ \ \ \ δ}\ ∇_δf − T_{[αβ}^{\ \ \ \ ρ}∇_{γ]} ∇_ρf\ \ $ (4.2.22 applied to last term above) $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = ∇_{[α}T_{βγ]}^{\ \ \ \ δ}\ ∇_δf + T_{[αβ}^{\ \ \ \ ρ}\ T_{γ]ρ}^{\ \ \ \ δ}\ ∇_δf − T_{[αβ}^{\ \ \ \ δ}∇_{γ]}∇_δf$ Comparing to (4.2.39), $T_{[αβ}^{\ \ \ \ δ}\  ∇_{γ]}∇_δf$ must be zero.  ( edit: Not so.  See discussion at end of question below.) Hints about either why this might be true, or if not where I have been misled along the way will be greatly appreciated!  ( edit: See below for why not.) P.S.:  This book is a fabulous way for getting familiar with tensor indexing.  I'm almost half way through reading it, and up until this point have figured out pretty much every single formula, minor aside and fottnote on my own (except for the curious bits on ""irreducibility"" at the end of 3.3) and savored every minute of it. P.P.S.:  This is my first post to the forum, and am very impressed by how easy it was to figure out everything I needed, especially using MathJax, a non-trivial bit of software that actually is implemented in an extremely self-explanatory way, unlike so much else that claims to be, but hardly ever is.  I also find all the suggestions for making effective posts very sensible and helpful, something else that is very rare.  I hope they are adequately reflected in this post! Making this post has been truly a lot of fun, and I look forward to much more. Cheers! EDIT: Discussion of Solution due to Matt As he points out, the Leibnitz expansion applied to the first term in the last line of my derivation cancels the last term on that line.  The reason I missed this was failing to use parentheses in the relevant expression: $\ ∇_{[α}(T_{βγ]}^{\ \ \ \ δ}\ ∇_δf) = (∇_{[α}T_{βγ]}^{\ \ \ \ δ})\ ∇_δf + T_{[αβ}^{\ \ \ \ δ}(\  ∇_{γ]}∇_δf)$ Penrose's abstract indexing is commutative and associative for the tensor objects themselves, which is one its greatest advantages.  His covariant operators $∇_δ$ on the other hand are neither commutative (as he makes very clear) nor associative when followed by two or more tensors.  While the latter is not specifically mentioned in the text, it becomes obvious with a small amount of thought, with this non-trivial example serving as a perfect illustration. From this momentary stumbling block and the resulting great discussion and help on MSE, I have learned much.  So now, because it is so much fun, I will show how to derive what Penrose calls the ""Bianchi identity"" that is actually a simpler formula than the ""Bianchi symmetry"", though more complicated to derive (it actually depends on the ""symmetry"" formula): Derivation of ""Bianchi identity"" This is equation (4.2.43) (as well as Fig. A-9) in Vol. 1 of ""Spinors and space-time"" by Penrose and Rindler, and uses the notation and other concepts and references from that text: $2∇_{[[α}∇_{β]}∇_{γ]}V^δ = T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}V^δ + R_{[αβ|ρ|}^{\ \ \ \ \ \ \ \ \ δ}\ ∇_{γ]}V^ρ - R_{[αβγ]}^{\ \ \ \ \ \ \ \ ρ}\ ∇_ρV^δ\ \ \ \ \ \ \ $ (4.2.40) (LHS expanded using ""generalized Ricci identity"" as defined by Penrose) $2∇_{[α}∇_{[β}∇_{γ]]}V^δ = ∇_{[α}(T_{βγ]}^{\ \ \ \ ρ}\ ∇_{ρ}V^δ) + R_{[αβ|ρ|}^{\ \ \ \ \ \ \ \ \ δ}\ ∇_{γ]}V^ρ + V^ρ∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ}\ \ \ \ \ \ $ (4.2.41) (LHS expanded using definition of $R$ and Leibnitz law) Subtracting the first from the second gives (since LHSs are equal): $0 = V^ρ∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ} - T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}V^δ$ $\ \ \ \ \ + ∇_{[α}(T_{βγ]}^{\ \ \ \ ρ}\ ∇_{ρ}V^δ) + R_{[αβγ]}^{\ \ \ \ \ \ \ \ ρ}\ ∇_ρV^δ$ $\ \ = V^ρ∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ} - T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}V^δ$ $\ \ \ \ \ + (∇_{[α}\ ∇_{|ρ|}V^δ)T_{βγ]}^{\ \ \ \ ρ} + (∇_{[α}T_{βγ]}^{\ \ \ \ ρ})\ ∇_ρV^δ + R_{[αβγ]}^{\ \ \ \ \ \ \ \ ρ}\ ∇_ρV^δ$ $\ \ \ \ \ \ \ $ (using Leibnitz law) $\ \ = V^ρ∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ} - T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}V^δ + T_{[αβ}^{\ \ \ \ ρ}\ ∇_{γ]}\ ∇_ρV^δ$ $\ \ \ \ \ - T_{[αβ}^{\ \ \ \ ρ}\ T_{γ]ρ}^{\ \ \ \ σ}\ ∇_σV^δ - R_{[αβγ]}^{\ \ \ \ \ \ \ \ ρ}\ ∇_ρV^δ + R_{[αβγ]}^{\ \ \ \ \ \ \ \ ρ}\ ∇_ρV^δ$ $\ \ \ \ \ \ \ $ (using Bianchi symmetry) $\ \ = V^ρ∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ} + T_{[αβ}^{\ \ \ \ ρ}\ R_{γ]ρσ}^{\ \ \ \ \ \ \ δ}\ V^σ$ $\ \ \ \ \ \ \ $ (using Ricci identity) $\ \ =\ (∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ}\ )V^ρ + T_{[αβ}^{\ \ \ \ σ}\ R_{γ]σρ}^{\ \ \ \ \ \ \ δ}\ V^ρ$ So we have arrived at the two Penrose formulas for Bianchi symmetry: $\ \ -R_{[αβγ]}^{\ \ \ \ \ \ \ δ} = ∇_{[α}T_{βγ]}^{\ \ \ \ δ} + T_{[αβ}^{\ \ \ \ ρ}\ T_{γ]ρ}^{\ \ \ \ δ}$ Bianchi identity: $\ -∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ} = T_{[αβ}^{\ \ \ \ σ}\ R_{γ]σρ}^{\ \ \ \ \ \ \ δ}$","I want to prove EDIT: A brief discussion of the solution found by Matt is at the bottom of this post. The equation is called the ""Bianchi symmetry"" in Vol. 1 of ""Spinors and space-time"" by Penrose and Rindler, and is given there in equation (4.2.39) (with index in place of the here).  The definitions of , and all other symbols are taken from that same text, e.g: = Torsion tensor, satisfying , for scalars = Curvature tensor, satisfying , for vectors , and for covariant vectors, : I am trying to use the manner suggested in the text that is ""along the same lines"" as , but ""more elaborate"" than how the torsion-free version (4.2.37) is derived there, but also without using (4.2.52).  The approach I have taken looks like this: The first two (identical) equations below are shown as merely modest reformulations of others published (for other purposes) in the book as indicated (noting the fairly obvious book-typo in the first), so I am virtually certain the setup preceding the third equation here is correct. What then follows are just some fairly straightforward transformations, so I am suspecting the tensor highlighted at the end must be zero, but am unable to show why, perhaps because it isn't. ( edit: In fact it isn't.  See end of question.) As in (4.2.40), but with f in place of (not the -typo appearing in my edition): . Equivalently, as in (4.2.35), but symmetrized in α, β, and γ and including T: (with ) (4.2.22 applied to first term above) (4.2.22 applied to last term above) Comparing to (4.2.39), must be zero.  ( edit: Not so.  See discussion at end of question below.) Hints about either why this might be true, or if not where I have been misled along the way will be greatly appreciated!  ( edit: See below for why not.) P.S.:  This book is a fabulous way for getting familiar with tensor indexing.  I'm almost half way through reading it, and up until this point have figured out pretty much every single formula, minor aside and fottnote on my own (except for the curious bits on ""irreducibility"" at the end of 3.3) and savored every minute of it. P.P.S.:  This is my first post to the forum, and am very impressed by how easy it was to figure out everything I needed, especially using MathJax, a non-trivial bit of software that actually is implemented in an extremely self-explanatory way, unlike so much else that claims to be, but hardly ever is.  I also find all the suggestions for making effective posts very sensible and helpful, something else that is very rare.  I hope they are adequately reflected in this post! Making this post has been truly a lot of fun, and I look forward to much more. Cheers! EDIT: Discussion of Solution due to Matt As he points out, the Leibnitz expansion applied to the first term in the last line of my derivation cancels the last term on that line.  The reason I missed this was failing to use parentheses in the relevant expression: Penrose's abstract indexing is commutative and associative for the tensor objects themselves, which is one its greatest advantages.  His covariant operators on the other hand are neither commutative (as he makes very clear) nor associative when followed by two or more tensors.  While the latter is not specifically mentioned in the text, it becomes obvious with a small amount of thought, with this non-trivial example serving as a perfect illustration. From this momentary stumbling block and the resulting great discussion and help on MSE, I have learned much.  So now, because it is so much fun, I will show how to derive what Penrose calls the ""Bianchi identity"" that is actually a simpler formula than the ""Bianchi symmetry"", though more complicated to derive (it actually depends on the ""symmetry"" formula): Derivation of ""Bianchi identity"" This is equation (4.2.43) (as well as Fig. A-9) in Vol. 1 of ""Spinors and space-time"" by Penrose and Rindler, and uses the notation and other concepts and references from that text: (4.2.40) (LHS expanded using ""generalized Ricci identity"" as defined by Penrose) (4.2.41) (LHS expanded using definition of and Leibnitz law) Subtracting the first from the second gives (since LHSs are equal): (using Leibnitz law) (using Bianchi symmetry) (using Ricci identity) So we have arrived at the two Penrose formulas for Bianchi symmetry: Bianchi identity:",R_{[αβγ]}^{\ \ \ \ \ \ \ δ} + ∇_{[α}T_{βγ]}^{\ \ \ \ δ} + T_{[αβ}^{\ \ \ \ ρ}\ T_{γ]ρ}^{\ \ \ \ δ} = 0 σ δ T R T_{αβ}^{\ \ \ \ γ} T_{αβ}^{\ \ \ \ γ} \ ∇_γf =  ∇_α∇_βf - ∇_β∇_αf f R_{αβγ}^{\ \ \ \ \ \ \ δ} ∇_α∇_βV^δ - ∇_β∇_αV^δ - T_{αβ}^{\ \ \ \ γ}\ ∇_γV^δ = R_{αβγ}^{\ \ \ \ \ \ \ δ}\ V^γ V^γ V_δ \ \ \ \ \ \ \ \ \ \ \ \ \ ∇_α∇_βV_γ - ∇_β∇_αV_γ - T_{αβ}^{\ \ \ \ ρ}\ ∇_ρV_γ = -R_{αβγ}^{\ \ \ \ \ \ \ δ}\ V_δ T_{[αβ}^{\ \ \ \ δ}\  ∇_{γ]}∇_δf V^δ V^γ 2∇_{[[α}∇_{β]}∇_{γ]} f = T_{[αβ}^{\ \ \ \ ρ}∇_{|ρ|}∇_{γ]}f − R_{[αβγ]}^{\ \ \ \ \ \ \ δ}\ ∇_δf −R_{[αβγ]}^{\ \ \ \ \ \ \ δ}∇_δ f = 2∇_{[[α}∇_{β]}∇_{γ]}f − T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}f \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = 2∇_{[α}∇_{[β}∇_{γ]]}f − T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}f \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = ∇_{[α}Δ_{βγ]}f − T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}f\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  Δ_{βγ} := 2∇_{[β}∇_{γ]} \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = ∇_{[α}T_{βγ]}^{\ \ \ \ δ}\ ∇_δf − T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}f\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = ∇_{[α}T_{βγ]}^{\ \ \ \ ρ}\ ∇_δf + T_{[αβ}^{\ \ \ \ ρ}\ T_{γ]ρ}^{\ \ \ \ δ}\ ∇_δf − T_{[αβ}^{\ \ \ \ ρ}∇_{γ]} ∇_ρf\ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = ∇_{[α}T_{βγ]}^{\ \ \ \ δ}\ ∇_δf + T_{[αβ}^{\ \ \ \ ρ}\ T_{γ]ρ}^{\ \ \ \ δ}\ ∇_δf − T_{[αβ}^{\ \ \ \ δ}∇_{γ]}∇_δf T_{[αβ}^{\ \ \ \ δ}\  ∇_{γ]}∇_δf \ ∇_{[α}(T_{βγ]}^{\ \ \ \ δ}\ ∇_δf) = (∇_{[α}T_{βγ]}^{\ \ \ \ δ})\ ∇_δf + T_{[αβ}^{\ \ \ \ δ}(\  ∇_{γ]}∇_δf) ∇_δ 2∇_{[[α}∇_{β]}∇_{γ]}V^δ = T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}V^δ + R_{[αβ|ρ|}^{\ \ \ \ \ \ \ \ \ δ}\ ∇_{γ]}V^ρ - R_{[αβγ]}^{\ \ \ \ \ \ \ \ ρ}\ ∇_ρV^δ\ \ \ \ \ \ \  2∇_{[α}∇_{[β}∇_{γ]]}V^δ = ∇_{[α}(T_{βγ]}^{\ \ \ \ ρ}\ ∇_{ρ}V^δ) + R_{[αβ|ρ|}^{\ \ \ \ \ \ \ \ \ δ}\ ∇_{γ]}V^ρ + V^ρ∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ}\ \ \ \ \ \  R 0 = V^ρ∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ} - T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}V^δ \ \ \ \ \ + ∇_{[α}(T_{βγ]}^{\ \ \ \ ρ}\ ∇_{ρ}V^δ) + R_{[αβγ]}^{\ \ \ \ \ \ \ \ ρ}\ ∇_ρV^δ \ \ = V^ρ∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ} - T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}V^δ \ \ \ \ \ + (∇_{[α}\ ∇_{|ρ|}V^δ)T_{βγ]}^{\ \ \ \ ρ} + (∇_{[α}T_{βγ]}^{\ \ \ \ ρ})\ ∇_ρV^δ + R_{[αβγ]}^{\ \ \ \ \ \ \ \ ρ}\ ∇_ρV^δ \ \ \ \ \ \ \  \ \ = V^ρ∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ} - T_{[αβ}^{\ \ \ \ ρ}\ ∇_{|ρ|}∇_{γ]}V^δ + T_{[αβ}^{\ \ \ \ ρ}\ ∇_{γ]}\ ∇_ρV^δ \ \ \ \ \ - T_{[αβ}^{\ \ \ \ ρ}\ T_{γ]ρ}^{\ \ \ \ σ}\ ∇_σV^δ - R_{[αβγ]}^{\ \ \ \ \ \ \ \ ρ}\ ∇_ρV^δ + R_{[αβγ]}^{\ \ \ \ \ \ \ \ ρ}\ ∇_ρV^δ \ \ \ \ \ \ \  \ \ = V^ρ∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ} + T_{[αβ}^{\ \ \ \ ρ}\ R_{γ]ρσ}^{\ \ \ \ \ \ \ δ}\ V^σ \ \ \ \ \ \ \  \ \ =\ (∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ}\ )V^ρ + T_{[αβ}^{\ \ \ \ σ}\ R_{γ]σρ}^{\ \ \ \ \ \ \ δ}\ V^ρ \ \ -R_{[αβγ]}^{\ \ \ \ \ \ \ δ} = ∇_{[α}T_{βγ]}^{\ \ \ \ δ} + T_{[αβ}^{\ \ \ \ ρ}\ T_{γ]ρ}^{\ \ \ \ δ} \ -∇_{[α}R_{βγ]ρ}^{\ \ \ \ \ \ \ δ} = T_{[αβ}^{\ \ \ \ σ}\ R_{γ]σρ}^{\ \ \ \ \ \ \ δ},"['differential-geometry', 'tensors', 'general-relativity', 'index-notation']"
45,Riemannian holonomy of a covering,Riemannian holonomy of a covering,,"Suppose I have a connected Riemannian manifold $X$ and a covering $\pi:Y\to X$ with the pulled back metric on $Y$, making $\pi$ into a local isometry between Riemannian manifolds. Suppose we have a closed loop in $Y$ based at a point $y\in Y$. This will give rise to an element of the holonomy group $\mathrm{Hol}_y(Y)$, which after we fix a basis of $T_yY$ becomes an element of $\mathrm{GL}(n)$ (or $\mathrm O(n)$ really). If we project the path to $X$, we get a closed loop again, hence an element of $\mathrm{Hol}_xX$, where $x = \pi(y)$, and by using $\pi$ to map the basis of $T_yY$ to one of $T_xX$ this becomes an element of $\mathrm{GL}(n)$ as well, and we can talk about equality in a sensible way. Since all along the paths the geometry is identical (around every point there is a neighbourhood on which $\pi$ induces an isometry) it seems clear that they must induce the same element in holonomy. This would mean that the holonomy of $Y$ is a subgroup of the holonomy of $X$. This inclusion can be strict, if $X$ is not orientable for example, its holonomy group might be $\mathrm O(n)$, but if we take $Y$ to be its oriented double cover its holonomy must be contained in $\mathrm{SO}(n)$. Can anything more precise be said about the relation between the holonomy groups of $X$ and of $Y$? I would expect the fundamental group or rather the group of deck transformations to play a role, but I don't know in what form. Thanks!","Suppose I have a connected Riemannian manifold $X$ and a covering $\pi:Y\to X$ with the pulled back metric on $Y$, making $\pi$ into a local isometry between Riemannian manifolds. Suppose we have a closed loop in $Y$ based at a point $y\in Y$. This will give rise to an element of the holonomy group $\mathrm{Hol}_y(Y)$, which after we fix a basis of $T_yY$ becomes an element of $\mathrm{GL}(n)$ (or $\mathrm O(n)$ really). If we project the path to $X$, we get a closed loop again, hence an element of $\mathrm{Hol}_xX$, where $x = \pi(y)$, and by using $\pi$ to map the basis of $T_yY$ to one of $T_xX$ this becomes an element of $\mathrm{GL}(n)$ as well, and we can talk about equality in a sensible way. Since all along the paths the geometry is identical (around every point there is a neighbourhood on which $\pi$ induces an isometry) it seems clear that they must induce the same element in holonomy. This would mean that the holonomy of $Y$ is a subgroup of the holonomy of $X$. This inclusion can be strict, if $X$ is not orientable for example, its holonomy group might be $\mathrm O(n)$, but if we take $Y$ to be its oriented double cover its holonomy must be contained in $\mathrm{SO}(n)$. Can anything more precise be said about the relation between the holonomy groups of $X$ and of $Y$? I would expect the fundamental group or rather the group of deck transformations to play a role, but I don't know in what form. Thanks!",,"['differential-geometry', 'curvature', 'covering-spaces', 'holonomy']"
46,Pullback of an Immersion $j: \mathbb{R}^3 \to \mathbb{R}^4$,Pullback of an Immersion,j: \mathbb{R}^3 \to \mathbb{R}^4,"I'm preparing for some exams and this problem is from an older exam: ""On $\mathbb{R}^4$, with standard coordinates $(x, y, z, t)$, consider the 1-form $\theta = x \, dy − y \, dx + z \, dt − t \, dz$. Is there a smooth immersion $j : \mathbb{R}^3 \to \mathbb{R}^4$ such that $j^∗ \theta = 0$ everywhere?"" I'm not sure of where to start besides of course supposing there is such an immersion and seeing if it leads to a contradiction. I've also differentiated the 1-form just to see if any patterns come of it but there are none as far as I can tell. However, the 1-form does look very symmetric and I'm sure it was chosen with care. I was told to look at some integrability conditions such as the kernel of some map and apply Frobenius' Theorem. However, I only know of the version of Frobenius' theorem in the context of taking the Lie bracket of vector fields. Any help or hints are helpful, thank you!","I'm preparing for some exams and this problem is from an older exam: ""On $\mathbb{R}^4$, with standard coordinates $(x, y, z, t)$, consider the 1-form $\theta = x \, dy − y \, dx + z \, dt − t \, dz$. Is there a smooth immersion $j : \mathbb{R}^3 \to \mathbb{R}^4$ such that $j^∗ \theta = 0$ everywhere?"" I'm not sure of where to start besides of course supposing there is such an immersion and seeing if it leads to a contradiction. I've also differentiated the 1-form just to see if any patterns come of it but there are none as far as I can tell. However, the 1-form does look very symmetric and I'm sure it was chosen with care. I was told to look at some integrability conditions such as the kernel of some map and apply Frobenius' Theorem. However, I only know of the version of Frobenius' theorem in the context of taking the Lie bracket of vector fields. Any help or hints are helpful, thank you!",,"['differential-geometry', 'differential-topology']"
47,Normal coordinate charts in Riemannian manifolds and relationship between two definitions of tangent space.,Normal coordinate charts in Riemannian manifolds and relationship between two definitions of tangent space.,,"Let $M$ be a Riemannian manifold and $p\in M$ a point. Let $E_{i}$ be an orthonormal basis of $T_{p}M$ and let $E\colon \mathbb{R}^{n} \to T_{p}M$ be the isomorphism sending $(x_{1},\ldots,x_{n})\mapsto \sum_{i=1}^{n}x_{i}E_{i}$. On a normal neighborhood $U$ of $p$ in $M$ (that is, the image under the exponential map $\exp_{p}$ of a star shaped open neighborhood of $0\in T_{p}M$) we can define a coordinate chart by $\varphi =E^{-1}\circ \exp_{p}^{-1}\colon U\to \mathbb{R}^{n}$. As with any other chart, the basis $\frac{\partial }{\partial x_{i}}$ of $T_{0}\mathbb{R}^{n}$ pulls back to a basis $\frac{\partial }{\partial x_{i} }|_{p}$ of $T_{p}M$. Is this pullback the orthnormal basis that we started from? My attempt: let $f\in C^{\infty }(M)$. We have to show that $E_{i}(f)=\frac{\partial }{\partial x_{i} }|_{p}(f)$. By definition: $$\frac{\partial }{\partial x_{i} }|_{p}(f)=\frac{\partial( f\circ \exp_{p}\circ E)}{\partial x_{i} }(0)$$ I guess now I need to deal with the definition of the exponential map explicitly, but I am not sure on how to do this. So first $E$ sends $(x_{1},\ldots,x_{n})\mapsto \sum_{i=1}^{n}x_{i}E_{i}$. Then the exponential map sends a vector $v\in T_{p}M$ to $\gamma_{v}(1)$, where $\gamma_{v}$ is a geodesic with $\gamma_{v}(0)=p$ and $\dot{\gamma_{v} }(0)=v$. How do I use this now? I don't even know how geodesics look like without knowing the Riemannian metric $g$. For example, the point $(1,0,\ldots,0)$ is sent first to $E_{1}$ and then we take the geodesic $\gamma$ with $\gamma(0)=p=(0,\ldots,0)$ (using the expression of the point in the normal coordinate chart) and $\dot{ \gamma}(0)=E_{1}$. Now $\gamma(1)$ should be the point in the trajectory of $\gamma $ at time 1, and I guess if we track $\gamma$ on $\mathbb{R}^{n}$ via the normal coordinates then this point should correspond to $(1,0,\ldots,0)$, but I am already not sure of these claims. This also led me to the following question: In a chart $(V,\psi)$ with $\psi(p)=0$, is it true that $\frac{\partial }{\partial x_{i} }|_{p}=\dot{ \gamma }(0)$ for any curve $\gamma $ in $M$ with $\gamma(0)=p$ and such that $\psi(\gamma)$ has speed $1$ in the positive direction of the $x_{i}$ axis? EDIT (current thoughts on the questions) : I suspect now that the answer to the first question is positive and I suspect it will follow from the differential of the exponential map at $p$ being the identity on $T_{p}M$. But I still couldn't manage to write down a formal argument. Regarding the second question, I still didn't find an answer and I still very interested in finding one. I think that there are at least two ways of defining the tangent space of $M$ at a point $p$, one with this abstract derivations and the other one by considering derivatives of curves in the manifold passing through $p$ at time $0$. I have never worked with this definition, but after thinking about the second question I got to the following conclusion: isn't that second question precisely the relation between these two definitions of the tangent space?","Let $M$ be a Riemannian manifold and $p\in M$ a point. Let $E_{i}$ be an orthonormal basis of $T_{p}M$ and let $E\colon \mathbb{R}^{n} \to T_{p}M$ be the isomorphism sending $(x_{1},\ldots,x_{n})\mapsto \sum_{i=1}^{n}x_{i}E_{i}$. On a normal neighborhood $U$ of $p$ in $M$ (that is, the image under the exponential map $\exp_{p}$ of a star shaped open neighborhood of $0\in T_{p}M$) we can define a coordinate chart by $\varphi =E^{-1}\circ \exp_{p}^{-1}\colon U\to \mathbb{R}^{n}$. As with any other chart, the basis $\frac{\partial }{\partial x_{i}}$ of $T_{0}\mathbb{R}^{n}$ pulls back to a basis $\frac{\partial }{\partial x_{i} }|_{p}$ of $T_{p}M$. Is this pullback the orthnormal basis that we started from? My attempt: let $f\in C^{\infty }(M)$. We have to show that $E_{i}(f)=\frac{\partial }{\partial x_{i} }|_{p}(f)$. By definition: $$\frac{\partial }{\partial x_{i} }|_{p}(f)=\frac{\partial( f\circ \exp_{p}\circ E)}{\partial x_{i} }(0)$$ I guess now I need to deal with the definition of the exponential map explicitly, but I am not sure on how to do this. So first $E$ sends $(x_{1},\ldots,x_{n})\mapsto \sum_{i=1}^{n}x_{i}E_{i}$. Then the exponential map sends a vector $v\in T_{p}M$ to $\gamma_{v}(1)$, where $\gamma_{v}$ is a geodesic with $\gamma_{v}(0)=p$ and $\dot{\gamma_{v} }(0)=v$. How do I use this now? I don't even know how geodesics look like without knowing the Riemannian metric $g$. For example, the point $(1,0,\ldots,0)$ is sent first to $E_{1}$ and then we take the geodesic $\gamma$ with $\gamma(0)=p=(0,\ldots,0)$ (using the expression of the point in the normal coordinate chart) and $\dot{ \gamma}(0)=E_{1}$. Now $\gamma(1)$ should be the point in the trajectory of $\gamma $ at time 1, and I guess if we track $\gamma$ on $\mathbb{R}^{n}$ via the normal coordinates then this point should correspond to $(1,0,\ldots,0)$, but I am already not sure of these claims. This also led me to the following question: In a chart $(V,\psi)$ with $\psi(p)=0$, is it true that $\frac{\partial }{\partial x_{i} }|_{p}=\dot{ \gamma }(0)$ for any curve $\gamma $ in $M$ with $\gamma(0)=p$ and such that $\psi(\gamma)$ has speed $1$ in the positive direction of the $x_{i}$ axis? EDIT (current thoughts on the questions) : I suspect now that the answer to the first question is positive and I suspect it will follow from the differential of the exponential map at $p$ being the identity on $T_{p}M$. But I still couldn't manage to write down a formal argument. Regarding the second question, I still didn't find an answer and I still very interested in finding one. I think that there are at least two ways of defining the tangent space of $M$ at a point $p$, one with this abstract derivations and the other one by considering derivatives of curves in the manifold passing through $p$ at time $0$. I have never worked with this definition, but after thinking about the second question I got to the following conclusion: isn't that second question precisely the relation between these two definitions of the tangent space?",,"['differential-geometry', 'riemannian-geometry', 'smooth-manifolds']"
48,"On polar normal coordinates, Jacobi field and parallel transport.","On polar normal coordinates, Jacobi field and parallel transport.",,"Setup. Let $(M,g)$ be a Riemannian manifold of dimension $2$, let $m\in M$ and let $U$ be an open neighborhood of $0$ in $T_mM$ such that $\exp_m\colon U\rightarrow M$ is an embedding and let $h:={\exp_m}^*g$. According to Gauss' lemma, in polar coordinates $[r,\theta]$ on $U\setminus\{0\}$, one has the following decomposition: $$h=\mathrm{d}r^2+f(r,\theta)^2\mathrm{d}\theta^2.$$ Let $u$ and $v$ be two vectors of $T_mM$ which are orthogonal for $h_0$ and assume that polar coordinates have been chosen such that the angle associated to $u$ is $0$. Question. Let $c\colon r\mapsto ru$, let $r\mapsto V(r)$ be the parallel transport of $v$ along $c$ and let $\cdot'$ be the connection induced on $c^*TM$ by $\nabla$ the Levi-Civita connection of $M$, then:   $$J(r):=f(r,0)V(r)$$   is the Jacobi field along $c$ with initial values $J(0)=0$ and $J'(0)=v$. My attempts. I have to prove that $J$ satisfies the following second order linear differential equation: $$J''-R(\dot{c},J)\dot{c}=0.$$ The first term is easy to compute, as $V$ is parallel along $c$, one has: $$J''(r)=\frac{\partial^2f}{\partial r^2}(r,0)V(r).$$ However, I am having a hard time with the curvature term, I am aware that by definition, one has: $$R(\dot{c},J)\dot{c}=\nabla_{\dot{c}}\nabla_J\dot{c}-\underbrace{\nabla_{J}\nabla_{\dot{c}}{\dot{c}}}_{=0}-\nabla_{[\dot{c},J]}\dot{c}$$ and the middle term is vanishing since $c$ is a geodesic path and here I am stuck. An other strategy I have in mind is finding a smooth map $\ell\colon I\times]-\varepsilon,\varepsilon[\rightarrow M$ such that: $$J(r)=\frac{\partial\ell}{\partial s}(r,0)$$ and for all $s$, $t\mapsto\ell(t,s)$ is a geodesic of $M$. I have not dig in depth this approach yet. Another idea could be to prove that one has $J(r)=T_{ru}\exp_m(v)$, but it does not seem too doable. Any enlightenment will be greatly appreciated.","Setup. Let $(M,g)$ be a Riemannian manifold of dimension $2$, let $m\in M$ and let $U$ be an open neighborhood of $0$ in $T_mM$ such that $\exp_m\colon U\rightarrow M$ is an embedding and let $h:={\exp_m}^*g$. According to Gauss' lemma, in polar coordinates $[r,\theta]$ on $U\setminus\{0\}$, one has the following decomposition: $$h=\mathrm{d}r^2+f(r,\theta)^2\mathrm{d}\theta^2.$$ Let $u$ and $v$ be two vectors of $T_mM$ which are orthogonal for $h_0$ and assume that polar coordinates have been chosen such that the angle associated to $u$ is $0$. Question. Let $c\colon r\mapsto ru$, let $r\mapsto V(r)$ be the parallel transport of $v$ along $c$ and let $\cdot'$ be the connection induced on $c^*TM$ by $\nabla$ the Levi-Civita connection of $M$, then:   $$J(r):=f(r,0)V(r)$$   is the Jacobi field along $c$ with initial values $J(0)=0$ and $J'(0)=v$. My attempts. I have to prove that $J$ satisfies the following second order linear differential equation: $$J''-R(\dot{c},J)\dot{c}=0.$$ The first term is easy to compute, as $V$ is parallel along $c$, one has: $$J''(r)=\frac{\partial^2f}{\partial r^2}(r,0)V(r).$$ However, I am having a hard time with the curvature term, I am aware that by definition, one has: $$R(\dot{c},J)\dot{c}=\nabla_{\dot{c}}\nabla_J\dot{c}-\underbrace{\nabla_{J}\nabla_{\dot{c}}{\dot{c}}}_{=0}-\nabla_{[\dot{c},J]}\dot{c}$$ and the middle term is vanishing since $c$ is a geodesic path and here I am stuck. An other strategy I have in mind is finding a smooth map $\ell\colon I\times]-\varepsilon,\varepsilon[\rightarrow M$ such that: $$J(r)=\frac{\partial\ell}{\partial s}(r,0)$$ and for all $s$, $t\mapsto\ell(t,s)$ is a geodesic of $M$. I have not dig in depth this approach yet. Another idea could be to prove that one has $J(r)=T_{ru}\exp_m(v)$, but it does not seem too doable. Any enlightenment will be greatly appreciated.",,"['differential-geometry', 'riemannian-geometry', 'vector-fields']"
49,"What makes a Riemannian manifold ""directionally homogeneous""?","What makes a Riemannian manifold ""directionally homogeneous""?",,"Consider the three Riemannian 2-manifolds: Euclidean plane, 2-sphere and Poincaré disk. Each of them has the following property (D): For any point $p$ on the surface $S$ and any two unit tangent vectors $v,w\in T_pS$, there exists an isometry $f:S\to S$ such that $f(p)=p$ and the pushforward $(f_*)_p:T_pS\to T_pS$ sends $v$ to $w$. This property is not seen in arbitrary smooth Riemannian 2-manifolds. For instance, consider the infinite cylinder $S=\{(x,y,z)\in\Bbb R^3:x^2+y^2=1\}$ with metric inherited from $\Bbb R^3$. Even though the space is homogeneous in the sense that for any two points $p_1,p_2\in S$, there exists an isometry $f:S\to S$ sending $p_1$ to $p_2$, there is no isometry whose pushfoward sends a unit tangent vector that goes horizontally (along the direction of $xy$-plane) to a unit tangent vector that goes vertically (parallel the direction of $z$-axis). If there is, such an isometry would send a horizontal circle on $S$ (which is a geodesic generated by a horizontal tangent vector) to a vertical line on $S$ (which is a geodesic generated by a vertical tangent vector), and this is absurd. On the cylinder, directions at a point are not ""equal"" to each other, and I say the cylinder is not ""directionally homogeneous"". So what are some necessary and/or sufficient conditions for a smooth Riemannian manifold to have property (D)? Please note that I have only taken a course on differential geometry at the level of do Carmo's Differential Geometry of Curves and Surfaces .","Consider the three Riemannian 2-manifolds: Euclidean plane, 2-sphere and Poincaré disk. Each of them has the following property (D): For any point $p$ on the surface $S$ and any two unit tangent vectors $v,w\in T_pS$, there exists an isometry $f:S\to S$ such that $f(p)=p$ and the pushforward $(f_*)_p:T_pS\to T_pS$ sends $v$ to $w$. This property is not seen in arbitrary smooth Riemannian 2-manifolds. For instance, consider the infinite cylinder $S=\{(x,y,z)\in\Bbb R^3:x^2+y^2=1\}$ with metric inherited from $\Bbb R^3$. Even though the space is homogeneous in the sense that for any two points $p_1,p_2\in S$, there exists an isometry $f:S\to S$ sending $p_1$ to $p_2$, there is no isometry whose pushfoward sends a unit tangent vector that goes horizontally (along the direction of $xy$-plane) to a unit tangent vector that goes vertically (parallel the direction of $z$-axis). If there is, such an isometry would send a horizontal circle on $S$ (which is a geodesic generated by a horizontal tangent vector) to a vertical line on $S$ (which is a geodesic generated by a vertical tangent vector), and this is absurd. On the cylinder, directions at a point are not ""equal"" to each other, and I say the cylinder is not ""directionally homogeneous"". So what are some necessary and/or sufficient conditions for a smooth Riemannian manifold to have property (D)? Please note that I have only taken a course on differential geometry at the level of do Carmo's Differential Geometry of Curves and Surfaces .",,"['differential-geometry', 'riemannian-geometry']"
50,Show that for a scalar field the following relation is true,Show that for a scalar field the following relation is true,,"The FLRW line element is $ds^2=dt^2 -a^2(t)[(dx^1)^2+(dx^2)^2+(dx^3)^2]$. For a homogeneous scalar field $\psi$ show that $$g^{\mu\nu}\nabla_{\mu}\nabla_{\nu} \psi = \ddot{\psi} + 3\frac{\dot{a}}{a}\dot{\psi}$$ The dot denotes a time derivative, the Einstein summation convention applies. I think I get close but I'm going wrong somewhere! Here's what I've tried: Expand the second covariant derivative $$g^{\mu\nu}\nabla_{\mu}\nabla_{\nu} \psi = g^{\mu\nu}\nabla_{\mu}(\partial_{\nu}\psi)$$ $$=g^{\mu\nu}(\partial_{\mu}\partial_{\nu}\psi-\partial_{\sigma}\psi\Gamma^{\sigma}_{\mu\nu})$$ $$=g^{\mu\nu} \partial_{\mu}\partial_{\nu}\psi - g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu}\partial_{\sigma}\psi$$ Homogeneous means $\psi$ is a function of $t$ only, and so for the first term to be non-zero sub in $\mu=\nu=0$. Because they're dummy indices that doesn't mean I also have to set them to zero in the second term.  $$=g^{00}\frac{\partial ^2\psi}{\partial t^2} - g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu}\partial_{\sigma}\psi$$ From the line element, $g^{00}=1$. Now looking at the second term, I can write out the Christoffel symbol: $$g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu} = \frac{1}{2}g^{\sigma\epsilon}g^{\mu\nu}\left(\frac{\partial g_{\mu\epsilon}}{\partial x^{\nu}}+\frac{\partial g_{\nu\epsilon}}{\partial x^{\mu}} - \frac{\partial g_{\mu\nu}}{\partial x^{\epsilon}}\right)$$ Contract with the metric: $$\color{red}{\frac{1}{2}g^{\sigma\epsilon}\left(\frac{\partial g^{\nu}_{\epsilon}}{\partial x^{\nu}}+\frac{\partial g^{\mu}_{\epsilon}}{\partial x^{\mu}} - 0\right)}$$  Because the last term is either one or zero so the derivative is always zero. Contract again:  $$\frac{1}{2}\left(\frac{\partial g^{\sigma\nu}}{\partial x^{\nu}}+\frac{\partial g^{\sigma\mu}}{\partial x^{\mu}}\right)$$ Relabel the dummy index and get  $$g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu}=\frac{\partial g^{\sigma\nu}}{\partial x^{\nu}}$$ If I substitute that back in to the original equation,  $$\frac{\partial ^2\psi}{\partial t^2} - g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu}\partial_{\sigma}\psi = \frac{\partial ^2\psi}{\partial t^2} - \frac{\partial g^{\sigma\nu}}{\partial x^{\nu}}\partial_{\sigma}\psi$$ Looking at that I would conclude that only $\sigma = 0$ in the second term is non-zero, because I'd have $\partial_{\sigma}\psi$ which is zero unless the derivative is with respect to $t$. But the metric is diagonal so having $g^{0\nu}$ would mean I can't ever have the $a^2$ term and I won't get the $\dot{a}/a$ that I need. Where have I gone wrong?","The FLRW line element is $ds^2=dt^2 -a^2(t)[(dx^1)^2+(dx^2)^2+(dx^3)^2]$. For a homogeneous scalar field $\psi$ show that $$g^{\mu\nu}\nabla_{\mu}\nabla_{\nu} \psi = \ddot{\psi} + 3\frac{\dot{a}}{a}\dot{\psi}$$ The dot denotes a time derivative, the Einstein summation convention applies. I think I get close but I'm going wrong somewhere! Here's what I've tried: Expand the second covariant derivative $$g^{\mu\nu}\nabla_{\mu}\nabla_{\nu} \psi = g^{\mu\nu}\nabla_{\mu}(\partial_{\nu}\psi)$$ $$=g^{\mu\nu}(\partial_{\mu}\partial_{\nu}\psi-\partial_{\sigma}\psi\Gamma^{\sigma}_{\mu\nu})$$ $$=g^{\mu\nu} \partial_{\mu}\partial_{\nu}\psi - g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu}\partial_{\sigma}\psi$$ Homogeneous means $\psi$ is a function of $t$ only, and so for the first term to be non-zero sub in $\mu=\nu=0$. Because they're dummy indices that doesn't mean I also have to set them to zero in the second term.  $$=g^{00}\frac{\partial ^2\psi}{\partial t^2} - g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu}\partial_{\sigma}\psi$$ From the line element, $g^{00}=1$. Now looking at the second term, I can write out the Christoffel symbol: $$g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu} = \frac{1}{2}g^{\sigma\epsilon}g^{\mu\nu}\left(\frac{\partial g_{\mu\epsilon}}{\partial x^{\nu}}+\frac{\partial g_{\nu\epsilon}}{\partial x^{\mu}} - \frac{\partial g_{\mu\nu}}{\partial x^{\epsilon}}\right)$$ Contract with the metric: $$\color{red}{\frac{1}{2}g^{\sigma\epsilon}\left(\frac{\partial g^{\nu}_{\epsilon}}{\partial x^{\nu}}+\frac{\partial g^{\mu}_{\epsilon}}{\partial x^{\mu}} - 0\right)}$$  Because the last term is either one or zero so the derivative is always zero. Contract again:  $$\frac{1}{2}\left(\frac{\partial g^{\sigma\nu}}{\partial x^{\nu}}+\frac{\partial g^{\sigma\mu}}{\partial x^{\mu}}\right)$$ Relabel the dummy index and get  $$g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu}=\frac{\partial g^{\sigma\nu}}{\partial x^{\nu}}$$ If I substitute that back in to the original equation,  $$\frac{\partial ^2\psi}{\partial t^2} - g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu}\partial_{\sigma}\psi = \frac{\partial ^2\psi}{\partial t^2} - \frac{\partial g^{\sigma\nu}}{\partial x^{\nu}}\partial_{\sigma}\psi$$ Looking at that I would conclude that only $\sigma = 0$ in the second term is non-zero, because I'd have $\partial_{\sigma}\psi$ which is zero unless the derivative is with respect to $t$. But the metric is diagonal so having $g^{0\nu}$ would mean I can't ever have the $a^2$ term and I won't get the $\dot{a}/a$ that I need. Where have I gone wrong?",,"['differential-geometry', 'tensors']"
51,Inverse image of a sub manifold - Transversal intersection,Inverse image of a sub manifold - Transversal intersection,,"Suppose $N,M$ are smooth manifolds and  $f:N\rightarrow M$ is a smooth map intersecting transversally with a submanifold $S$ of $M$. The question is to prove that $f^{-1}(S)$ is a smooth submanifold of $N$. There is a proof in Lee’s smooth manifolds book but that seems to be incomplete or I am misunderstanding something. The idea given there is to somehow show that $f^{-1}(S)$ is a regular level set of  some smooth map $N\rightarrow M’$ for some smooth manifold $M’$. As $S$ is an embedded submanifold, it is locally a regular level set i.e., given $p\in S$ there exists open $U$ in $M$ containing $p$ and a smooth map $\varphi :U\rightarrow \mathbb{R}^k$ such that $U\cap S=\varphi^{-1}(0)$. We then have $f^{-1}(U\cap S)=f^{-1}\varphi^{-1}(0)=(\varphi\circ f)^{-1}(0)$. By $f$ in $\varphi\circ f$ I mean restriction of $f$ to $\varphi^{-1}(U)$. This would only tell me (after proving that $0$ is a regular value for composition) that $f^{-1}(U\cap S)=f^{-1}(U)\cap f^{-1}(S)$ is a submanifold of $f^{-1}(U)$. I do not see why this would imply $f^{-1}(S)$ is a submanifold of $N$. Is it that straightforward? Any suggestion is welcome.","Suppose $N,M$ are smooth manifolds and  $f:N\rightarrow M$ is a smooth map intersecting transversally with a submanifold $S$ of $M$. The question is to prove that $f^{-1}(S)$ is a smooth submanifold of $N$. There is a proof in Lee’s smooth manifolds book but that seems to be incomplete or I am misunderstanding something. The idea given there is to somehow show that $f^{-1}(S)$ is a regular level set of  some smooth map $N\rightarrow M’$ for some smooth manifold $M’$. As $S$ is an embedded submanifold, it is locally a regular level set i.e., given $p\in S$ there exists open $U$ in $M$ containing $p$ and a smooth map $\varphi :U\rightarrow \mathbb{R}^k$ such that $U\cap S=\varphi^{-1}(0)$. We then have $f^{-1}(U\cap S)=f^{-1}\varphi^{-1}(0)=(\varphi\circ f)^{-1}(0)$. By $f$ in $\varphi\circ f$ I mean restriction of $f$ to $\varphi^{-1}(U)$. This would only tell me (after proving that $0$ is a regular value for composition) that $f^{-1}(U\cap S)=f^{-1}(U)\cap f^{-1}(S)$ is a submanifold of $f^{-1}(U)$. I do not see why this would imply $f^{-1}(S)$ is a submanifold of $N$. Is it that straightforward? Any suggestion is welcome.",,['differential-geometry']
52,On the vector bundles associated to the eigenspaces of an almost complex structure.,On the vector bundles associated to the eigenspaces of an almost complex structure.,,"Let $(M,J)$ be an almost complex manifold, that is $M$ is a smooth differentiable manifold of even dimension and $J$ is an endomorphism of the tangent vector bundle $TM$ such that $J^2=-\textrm{id}_{TM}$. Let $T^{\mathbb{C}}M$ be the complexification of $TM$, that is: $$T^{\mathbb{C}}M:=TM\otimes_{\mathbb{R}}\mathbb{C}=TM\oplus iTM,$$ where the tensor product is done fiber by fiber. Then, the endomorphism $J$ is diagonalizable on each fiber. Let $T^{(1,0)}M$, respectively $T^{(0,1)}M$ be the disjoint union of eigenspaces associated with $+i$, respectively $-i$. For example, one has: $$T^{(1,0)}M:=\coprod_{x\in M}E_i(J_{\vert T_x^{\mathbb{C}}M}).$$ Why are $T^{(1,0)}M$ and $T^{(0,1)}M$ vector bundles? Roughly speaking, why the vector space structure of $E_{\pm i}(J_{\vert T_x^\mathbb{C}M})$ depends continuously on $x$? More generally the following question arises: Question. Let $\xi$ be a vector bundle and let $F$ be an endomorphism of $\xi$, when is $\ker(F)$ a vector bundle? It seems that the question is non-empty since the dimension of $\ker(F_{\vert E_b})$ can change with the point $b\in B$, where $p\colon E\rightarrow B$ is the projection of $\xi$, which obviously prevents $\ker(F)$ from being a vector bundle. Any enlightenment will be greatly appreciated.","Let $(M,J)$ be an almost complex manifold, that is $M$ is a smooth differentiable manifold of even dimension and $J$ is an endomorphism of the tangent vector bundle $TM$ such that $J^2=-\textrm{id}_{TM}$. Let $T^{\mathbb{C}}M$ be the complexification of $TM$, that is: $$T^{\mathbb{C}}M:=TM\otimes_{\mathbb{R}}\mathbb{C}=TM\oplus iTM,$$ where the tensor product is done fiber by fiber. Then, the endomorphism $J$ is diagonalizable on each fiber. Let $T^{(1,0)}M$, respectively $T^{(0,1)}M$ be the disjoint union of eigenspaces associated with $+i$, respectively $-i$. For example, one has: $$T^{(1,0)}M:=\coprod_{x\in M}E_i(J_{\vert T_x^{\mathbb{C}}M}).$$ Why are $T^{(1,0)}M$ and $T^{(0,1)}M$ vector bundles? Roughly speaking, why the vector space structure of $E_{\pm i}(J_{\vert T_x^\mathbb{C}M})$ depends continuously on $x$? More generally the following question arises: Question. Let $\xi$ be a vector bundle and let $F$ be an endomorphism of $\xi$, when is $\ker(F)$ a vector bundle? It seems that the question is non-empty since the dimension of $\ker(F_{\vert E_b})$ can change with the point $b\in B$, where $p\colon E\rightarrow B$ is the projection of $\xi$, which obviously prevents $\ker(F)$ from being a vector bundle. Any enlightenment will be greatly appreciated.",,"['differential-geometry', 'vector-bundles', 'almost-complex']"
53,How to rigorously differentiate the square of the geodesic distance?,How to rigorously differentiate the square of the geodesic distance?,,"Let $(M,g)$ be a Lorentzian 4-dimensional manifold, in other words, $g$ has signature $(-,+,+,+)$. If $U$ is a geodesically convex set, one defines $\sigma : U\times U\to \mathbb{R}$ in the following way: since $U$ is geodesically convex, pick $x$, then there is a unique geodesic $\gamma_{x,y} : [a,b]\to U$ connecting it to $y\in U$. In that case, $$\sigma(x,y)=\dfrac{1}{2}\int_{a}^b g(\gamma_{x,y}(t))(\gamma_{x,y}'(t),\gamma_{x,y}'(t))dt.$$ Now I want to differentiate this $\sigma$ rigorously with respect to the two parameters, namely, as a function $C^\infty(M\times M)$, in order to yield a two-point vector field. In Physics works there is a rather non-rigorous way, taking about infinitesimal variations $\delta x^\mu$ of the curve. It is clumsy, it is not rigorous, and I really don't like it. I want one rigorous way to differentiate this. My main problem is that the dependence on $x,y$ is implicit in $\gamma_{x,y}$. The result in the end will be: let $\phi^\mu$ be coordinate functions on $U$. Since this is a function on $M\times M$ we have to make a difference between the first factor and the second. Adopting the admitedely bad notation of using the first half of the greek alphabet to refer to the first factor and the second half to the second factor, differentiating along $\phi^\alpha$, we have $$g^{\alpha\beta}\nabla_\alpha \sigma(x,y)= (b-a)v^\beta$$ Where $v^\beta$ are the components of the tangent vector to $\gamma_{x,y}$ in the coordinate basis. This is terribly odd, I'm differentiating a map, and end up with dependence on tangent vector to a curve and also to the interval of parametrization of the curve. I need some explanation on how all this can be put on rigorous grounds to make some sense. What is going on here? How does one differentiate this two-point scalar correctly? I believe the way uses calculus of variations, but I'm unsure how.","Let $(M,g)$ be a Lorentzian 4-dimensional manifold, in other words, $g$ has signature $(-,+,+,+)$. If $U$ is a geodesically convex set, one defines $\sigma : U\times U\to \mathbb{R}$ in the following way: since $U$ is geodesically convex, pick $x$, then there is a unique geodesic $\gamma_{x,y} : [a,b]\to U$ connecting it to $y\in U$. In that case, $$\sigma(x,y)=\dfrac{1}{2}\int_{a}^b g(\gamma_{x,y}(t))(\gamma_{x,y}'(t),\gamma_{x,y}'(t))dt.$$ Now I want to differentiate this $\sigma$ rigorously with respect to the two parameters, namely, as a function $C^\infty(M\times M)$, in order to yield a two-point vector field. In Physics works there is a rather non-rigorous way, taking about infinitesimal variations $\delta x^\mu$ of the curve. It is clumsy, it is not rigorous, and I really don't like it. I want one rigorous way to differentiate this. My main problem is that the dependence on $x,y$ is implicit in $\gamma_{x,y}$. The result in the end will be: let $\phi^\mu$ be coordinate functions on $U$. Since this is a function on $M\times M$ we have to make a difference between the first factor and the second. Adopting the admitedely bad notation of using the first half of the greek alphabet to refer to the first factor and the second half to the second factor, differentiating along $\phi^\alpha$, we have $$g^{\alpha\beta}\nabla_\alpha \sigma(x,y)= (b-a)v^\beta$$ Where $v^\beta$ are the components of the tangent vector to $\gamma_{x,y}$ in the coordinate basis. This is terribly odd, I'm differentiating a map, and end up with dependence on tangent vector to a curve and also to the interval of parametrization of the curve. I need some explanation on how all this can be put on rigorous grounds to make some sense. What is going on here? How does one differentiate this two-point scalar correctly? I believe the way uses calculus of variations, but I'm unsure how.",,"['differential-geometry', 'riemannian-geometry', 'mathematical-physics', 'general-relativity', 'semi-riemannian-geometry']"
54,Understanding a proof about Riemannian metrics in three dimensions always being diagonalizable,Understanding a proof about Riemannian metrics in three dimensions always being diagonalizable,,"I've recently been working through Deturck's and Yang's Existence of elastic deformations with prescribed principal strains . First and formost, I'm interested in it's proof that Riemannian metrics can in three dimensions always be diagonalized, that is, we can always find an atlas of coordinate functions so that the metric components become $g_{i j} = 0$ for $i \neq j$ when represented with regards to this metric, but I have some trouble fully understanding it. Now the general idea of the proof is to choose some orthonormal frames $\{\overline{e_1}, \overline{e_2}, \overline{e_3}\}$ of vector fields on $M$, a corresponding dual basis $\{\overline{\omega}^1, \overline{\omega}^2, \overline{\omega}^3\}$ of one-forms (which he calls the reference frame), and solving them for a set of coordinate functions $\{x^1, x^2, x^3\}$ in which the metric becomes diagonal, with the dual coframe $\{\omega^1, \omega^2, \omega^3\}$. Now a large part of the proof comes down to some tedious but simple calculations involving the Frobenius theorem, the structure equations and some other fundamental constructs, but it's the final step of the proof that I cannot wrap my head around so far and that I'm hoping someone can explain to me. I hope it suffices to only reproduce the final part of the proof aswell as an outline of the preliminary steps of the proof here; if anyone shall require more of the proof, feel free to let me know. Now let, as mentioned above, $\omega^i, i = 1, 2, 3$ be the orthonormal coframe to the desired coordinate functions $(x^1, x^2, x^3)$ to which our metric becomes diagonal of which we want to show the existence. The first major step in the proof is that they show it's an equivalent condition for the existence of such a coframe $\omega^i$ with the property that the corresponding frame diagonalizes the metric is that $$\omega^1 \wedge \omega^2 \wedge \omega_2^1 = 0, \omega^1 \wedge \omega^3 \wedge \omega_3^1 = 0, \omega^2 \wedge \omega^3 \wedge \omega_3^2 = 0 \tag{1} $$ where $\omega_i^j$ is the connection form. Given the existence of such a coframe, one would be able to represent it with respect to the reference frame $\omega^j$ via $\omega^i = \sum_{j=1}^n b_j^i \overline{\omega}^j$ for some coefficients $b_i^j$ where $(b_i^j) = b \in C^\infty(M, SO(3))$. In other words, it's sufficient to find such a matrix-valued function $b$ with this property. Using some more calculations, they then rewrite the equation $(1)$ without dependency on the desired coframe $\omega^i$ but instead with the matrix components $b_i^j$; namely, they show that the existence of $\omega^i$ that satisfy equation $(1)$ is equivalent to the existence of a $b \in C^\infty(M, SO(3))$ that satisfy: $$0 = \sum_{p, q, j, k} b_p^i b_q^l \overline{\omega}^p \wedge \overline{\omega}^q \wedge \left( \frac 12 \left( b_k^l \overline{e}_k (b_j^i) - b_k^i \overline{e}_k (b_j^l) \right) \overline{\omega}^j + b_k^l b_j^i \overline{\omega}_k^j \right) \tag{2}$$ I'm sorry for this lengthy lead-up; now my actual question starts here as this is where I can't follow anymore. I'll just quote the rest of the proof step-by-step: Proposition 4.8. The linearization of [$(2)$] is diagonal hyperbolic. This is one of my main issues so far: Deturck and Yang go on to proof this statement, but they don't lose a word about how the existence from such functions $b_i^j$ can be seen from this fact. Why does the fact that the linearization of $(2)$ is diagonal hyperbolic guarantee the existence of a solution? What exactly are our $b_i^j$'s, why do they exist because of it? Proof. It suffices to linearize [(2)] around the frame where $b_j^i(x) \equiv \delta_j^i$, since we can choose the reference frame $\{\overline{\omega}^i\}$ to be equal to the frame $\{\omega^i\}$ around which we are linearizing. Now... what exactly is happening here? Why can we choose the desired frame $\overline{\omega}^i$ to be (locally, I assume?) equal to the reference frame? One of them is (supposed to be) diagonal whereas the other one is not, so why can they be equal here? Let $\beta_j^i = (d b)_j^i$ be the variation in $b - \beta_j^i$ is a skew-symmetric matrix-valued function. My next problem here is: which variance exactly are they talking about here? How is this variance defined? I'm sorry but I'm not familiar with any definition of variance in this context and couldn't find a definition that makes sense so far. Do they mean something like the (exterior) derivative of $b$ since they write $d b$? Or is it something else entirely? I otherwise only know a variance from a stochastic context and I highly doubt that's what they mean here. The linearization of [(2)] is thus: $$\frac 12 \left( \overline{e}_i \left(\beta_j^i\right) - \overline{e}_i \left(\beta_j^l\right) \right) \overline{\omega}^i \wedge \overline{\omega}^l \wedge \overline{\omega}^j + \text{ lower order terms in } \beta = 0$$ for $(j, j, l ) = (1, 2, 3), (2, 3, 1),$ and $(3, 1, 2)$ I guess I could understand why this is the linearization if I knew what exactly the variance here is. We write out the three equations for the linearization: $$\frac 12 \left( \overline{e}_1 \left( \beta_3^2 \right) - \overline{e}_2 \left(\beta_3^1 \right) \right) = \text{lower order terms in } \beta = 0 $$ $$\frac 12 \left( \overline{e}_2 \left( \beta_1^3 \right) - \overline{e}_3 \left(\beta_1^2 \right) \right) = \text{lower order terms in } \beta = 0 $$   $$\frac 12 \left( \overline{e}_3 \left( \beta_2^1 \right) - \overline{e}_1 \left(\beta_2^3 \right) \right) = \text{lower order terms in } \beta = 0 $$ By alternately adding two of the equations together and subtracting the other, we obtain a system of the form: $$\overline{e}_1(\beta_3^2) = \text{lower order terms in } \beta = 0 $$ $$\overline{e}_2(\beta_1^3) = \text{lower order terms in } \beta = 0$$ $$\overline{e}_3(\beta_2^1) = \text{lower order terms in } \beta = 0$$ This is obviously diagonal form. q.e.d. This final part I can follow again: given the linearization he derived, I think I can see that the linearization here is diagonal. But, as mentioned above, how does the fact that this linearization is diagonal prove that a function $b \in C^\infty(M, SO(3))$ exists so that $(3)$ is satisfied? What's the connection here that I'm missing? I realize that this is a lengthy question, and I hope I phrased it in a manner that makes it clear what I can and what I cannot understand, and what answer(s) I seek here. To resume, my main issue is understanding how the linearization of system $(2)$ being diagonal is sufficient for the existence of a matrix function $b$ that satisfies $(2)$, why the desired coframe and the reference coframe can be chosen equal locally, and what exactly this variance ""$\beta_j^i = (d b)_j^i$"" is and how it plays into the linearization of the system $(2)$. Any help would be greatly appreciated. If anyone desires more details about the paper or the preliminary steps of the proof, I'm happy to provide them.","I've recently been working through Deturck's and Yang's Existence of elastic deformations with prescribed principal strains . First and formost, I'm interested in it's proof that Riemannian metrics can in three dimensions always be diagonalized, that is, we can always find an atlas of coordinate functions so that the metric components become $g_{i j} = 0$ for $i \neq j$ when represented with regards to this metric, but I have some trouble fully understanding it. Now the general idea of the proof is to choose some orthonormal frames $\{\overline{e_1}, \overline{e_2}, \overline{e_3}\}$ of vector fields on $M$, a corresponding dual basis $\{\overline{\omega}^1, \overline{\omega}^2, \overline{\omega}^3\}$ of one-forms (which he calls the reference frame), and solving them for a set of coordinate functions $\{x^1, x^2, x^3\}$ in which the metric becomes diagonal, with the dual coframe $\{\omega^1, \omega^2, \omega^3\}$. Now a large part of the proof comes down to some tedious but simple calculations involving the Frobenius theorem, the structure equations and some other fundamental constructs, but it's the final step of the proof that I cannot wrap my head around so far and that I'm hoping someone can explain to me. I hope it suffices to only reproduce the final part of the proof aswell as an outline of the preliminary steps of the proof here; if anyone shall require more of the proof, feel free to let me know. Now let, as mentioned above, $\omega^i, i = 1, 2, 3$ be the orthonormal coframe to the desired coordinate functions $(x^1, x^2, x^3)$ to which our metric becomes diagonal of which we want to show the existence. The first major step in the proof is that they show it's an equivalent condition for the existence of such a coframe $\omega^i$ with the property that the corresponding frame diagonalizes the metric is that $$\omega^1 \wedge \omega^2 \wedge \omega_2^1 = 0, \omega^1 \wedge \omega^3 \wedge \omega_3^1 = 0, \omega^2 \wedge \omega^3 \wedge \omega_3^2 = 0 \tag{1} $$ where $\omega_i^j$ is the connection form. Given the existence of such a coframe, one would be able to represent it with respect to the reference frame $\omega^j$ via $\omega^i = \sum_{j=1}^n b_j^i \overline{\omega}^j$ for some coefficients $b_i^j$ where $(b_i^j) = b \in C^\infty(M, SO(3))$. In other words, it's sufficient to find such a matrix-valued function $b$ with this property. Using some more calculations, they then rewrite the equation $(1)$ without dependency on the desired coframe $\omega^i$ but instead with the matrix components $b_i^j$; namely, they show that the existence of $\omega^i$ that satisfy equation $(1)$ is equivalent to the existence of a $b \in C^\infty(M, SO(3))$ that satisfy: $$0 = \sum_{p, q, j, k} b_p^i b_q^l \overline{\omega}^p \wedge \overline{\omega}^q \wedge \left( \frac 12 \left( b_k^l \overline{e}_k (b_j^i) - b_k^i \overline{e}_k (b_j^l) \right) \overline{\omega}^j + b_k^l b_j^i \overline{\omega}_k^j \right) \tag{2}$$ I'm sorry for this lengthy lead-up; now my actual question starts here as this is where I can't follow anymore. I'll just quote the rest of the proof step-by-step: Proposition 4.8. The linearization of [$(2)$] is diagonal hyperbolic. This is one of my main issues so far: Deturck and Yang go on to proof this statement, but they don't lose a word about how the existence from such functions $b_i^j$ can be seen from this fact. Why does the fact that the linearization of $(2)$ is diagonal hyperbolic guarantee the existence of a solution? What exactly are our $b_i^j$'s, why do they exist because of it? Proof. It suffices to linearize [(2)] around the frame where $b_j^i(x) \equiv \delta_j^i$, since we can choose the reference frame $\{\overline{\omega}^i\}$ to be equal to the frame $\{\omega^i\}$ around which we are linearizing. Now... what exactly is happening here? Why can we choose the desired frame $\overline{\omega}^i$ to be (locally, I assume?) equal to the reference frame? One of them is (supposed to be) diagonal whereas the other one is not, so why can they be equal here? Let $\beta_j^i = (d b)_j^i$ be the variation in $b - \beta_j^i$ is a skew-symmetric matrix-valued function. My next problem here is: which variance exactly are they talking about here? How is this variance defined? I'm sorry but I'm not familiar with any definition of variance in this context and couldn't find a definition that makes sense so far. Do they mean something like the (exterior) derivative of $b$ since they write $d b$? Or is it something else entirely? I otherwise only know a variance from a stochastic context and I highly doubt that's what they mean here. The linearization of [(2)] is thus: $$\frac 12 \left( \overline{e}_i \left(\beta_j^i\right) - \overline{e}_i \left(\beta_j^l\right) \right) \overline{\omega}^i \wedge \overline{\omega}^l \wedge \overline{\omega}^j + \text{ lower order terms in } \beta = 0$$ for $(j, j, l ) = (1, 2, 3), (2, 3, 1),$ and $(3, 1, 2)$ I guess I could understand why this is the linearization if I knew what exactly the variance here is. We write out the three equations for the linearization: $$\frac 12 \left( \overline{e}_1 \left( \beta_3^2 \right) - \overline{e}_2 \left(\beta_3^1 \right) \right) = \text{lower order terms in } \beta = 0 $$ $$\frac 12 \left( \overline{e}_2 \left( \beta_1^3 \right) - \overline{e}_3 \left(\beta_1^2 \right) \right) = \text{lower order terms in } \beta = 0 $$   $$\frac 12 \left( \overline{e}_3 \left( \beta_2^1 \right) - \overline{e}_1 \left(\beta_2^3 \right) \right) = \text{lower order terms in } \beta = 0 $$ By alternately adding two of the equations together and subtracting the other, we obtain a system of the form: $$\overline{e}_1(\beta_3^2) = \text{lower order terms in } \beta = 0 $$ $$\overline{e}_2(\beta_1^3) = \text{lower order terms in } \beta = 0$$ $$\overline{e}_3(\beta_2^1) = \text{lower order terms in } \beta = 0$$ This is obviously diagonal form. q.e.d. This final part I can follow again: given the linearization he derived, I think I can see that the linearization here is diagonal. But, as mentioned above, how does the fact that this linearization is diagonal prove that a function $b \in C^\infty(M, SO(3))$ exists so that $(3)$ is satisfied? What's the connection here that I'm missing? I realize that this is a lengthy question, and I hope I phrased it in a manner that makes it clear what I can and what I cannot understand, and what answer(s) I seek here. To resume, my main issue is understanding how the linearization of system $(2)$ being diagonal is sufficient for the existence of a matrix function $b$ that satisfies $(2)$, why the desired coframe and the reference coframe can be chosen equal locally, and what exactly this variance ""$\beta_j^i = (d b)_j^i$"" is and how it plays into the linearization of the system $(2)$. Any help would be greatly appreciated. If anyone desires more details about the paper or the preliminary steps of the proof, I'm happy to provide them.",,"['differential-geometry', 'riemannian-geometry', 'diagonalization', 'linearization']"
55,Understanding definition of differentiable manifold,Understanding definition of differentiable manifold,,"I am currently studying a basic course on differentiable manifolds.I have read the following definition of differentiable atlas and manifolds: Definition. Let $ \mathcal{A} = {(x_{\alpha},U_{\alpha})}_{\alpha \in A}$ be an atlas on a topological manifold $M$. Whenever the overlap $U_\alpha \cap U_\beta$ between two chart domains is nonempty we have the change of coordinates map $x_\beta \circ x_\alpha^{-1} : x_\alpha(U_\alpha \cap U_\beta) \to x_\beta(U_\alpha \cap U_\beta)$. If all such change of coordinates maps are $C^r$-diffeomorphisms, then $\mathcal {A}$ is called a $C^r$-atlas and a manifold endowed with maximal differentiable atlas is called differentiable manifold. I really find it hard to understand the following: Is there any intuitive idea from which the definition of diffferentiable atlas and manifolds emerge? How the local diffeomorphisms $x_\beta \circ x_\alpha^{-1}   : x_\alpha(U_\alpha \cap U_\beta) \to x_\beta(U_\alpha \cap U_\beta)$ allow us to define a differential structure on manifold globally? Moreover,why is it required for atlas to be maximal in order to define differentiable manifold","I am currently studying a basic course on differentiable manifolds.I have read the following definition of differentiable atlas and manifolds: Definition. Let $ \mathcal{A} = {(x_{\alpha},U_{\alpha})}_{\alpha \in A}$ be an atlas on a topological manifold $M$. Whenever the overlap $U_\alpha \cap U_\beta$ between two chart domains is nonempty we have the change of coordinates map $x_\beta \circ x_\alpha^{-1} : x_\alpha(U_\alpha \cap U_\beta) \to x_\beta(U_\alpha \cap U_\beta)$. If all such change of coordinates maps are $C^r$-diffeomorphisms, then $\mathcal {A}$ is called a $C^r$-atlas and a manifold endowed with maximal differentiable atlas is called differentiable manifold. I really find it hard to understand the following: Is there any intuitive idea from which the definition of diffferentiable atlas and manifolds emerge? How the local diffeomorphisms $x_\beta \circ x_\alpha^{-1}   : x_\alpha(U_\alpha \cap U_\beta) \to x_\beta(U_\alpha \cap U_\beta)$ allow us to define a differential structure on manifold globally? Moreover,why is it required for atlas to be maximal in order to define differentiable manifold",,['differential-geometry']
56,Lie algebra of vector fields,Lie algebra of vector fields,,"I have some conceptual confusion when thinking about the Lie algebra of a set of vector fields. Below, there are two questions which refer to the same problem, but from different viewpoints. Any comments/suggestions are greatly appreciated. Sorry if my questions appear silly. Let $V=\{V_i\}$, $i=1,\dots,m$ be a set of vector fields on a smooth manifold $M$, $dim(M)=n\ge m$. Let $L(V)$ be a non-involutive Lie algebra of vector fields $V$. For any $p\in M$, $L(V)$ is an infinite-dimensional group acting locally on some neighborhood $U(p)\subset M$ of $p$. Any group action can be seen as $\exp(t\cdot v):U(p)\rightarrow U(p)$, where $v\in L(V)$. Here comes my first problem.  When we look at elements of $L(V)$, these are all vector fields. If we consider them is being elements of a vector space, these form an infinite dimensional set. However, vector fields belong to a more general object: a module over $C^\infty(M)$. As such, they form a finite-dimensional set. Thant is, we can always find a set of basis vector fields and express the remaining ones as linear combinations (over $C^\infty$) of the basis v.f.'s. I wonder, how infinite-dimensionality enters the picture? Is there something that cannot be expressed using a finite-dim. basis? When we think about the Lie algebra of a Lie group $G$, it is basically the same at each point $g\in G$. This is not true for the Lie algebra of vector fields. Would it make sense to speak about rank of the Lie algebra $L(V)$ at some point $p\in M$, to say that $p$ is a critical point of the Lie algebra etc? If so, how should we consider the rank of $L(V)$ at $p$? As the rank of vector fields $v\in L(V)$ at $p$? Obviously, it cannot be higher than $n$. So, again, we have an inf-dim set whose rank does not exceed $n$ locally.","I have some conceptual confusion when thinking about the Lie algebra of a set of vector fields. Below, there are two questions which refer to the same problem, but from different viewpoints. Any comments/suggestions are greatly appreciated. Sorry if my questions appear silly. Let $V=\{V_i\}$, $i=1,\dots,m$ be a set of vector fields on a smooth manifold $M$, $dim(M)=n\ge m$. Let $L(V)$ be a non-involutive Lie algebra of vector fields $V$. For any $p\in M$, $L(V)$ is an infinite-dimensional group acting locally on some neighborhood $U(p)\subset M$ of $p$. Any group action can be seen as $\exp(t\cdot v):U(p)\rightarrow U(p)$, where $v\in L(V)$. Here comes my first problem.  When we look at elements of $L(V)$, these are all vector fields. If we consider them is being elements of a vector space, these form an infinite dimensional set. However, vector fields belong to a more general object: a module over $C^\infty(M)$. As such, they form a finite-dimensional set. Thant is, we can always find a set of basis vector fields and express the remaining ones as linear combinations (over $C^\infty$) of the basis v.f.'s. I wonder, how infinite-dimensionality enters the picture? Is there something that cannot be expressed using a finite-dim. basis? When we think about the Lie algebra of a Lie group $G$, it is basically the same at each point $g\in G$. This is not true for the Lie algebra of vector fields. Would it make sense to speak about rank of the Lie algebra $L(V)$ at some point $p\in M$, to say that $p$ is a critical point of the Lie algebra etc? If so, how should we consider the rank of $L(V)$ at $p$? As the rank of vector fields $v\in L(V)$ at $p$? Obviously, it cannot be higher than $n$. So, again, we have an inf-dim set whose rank does not exceed $n$ locally.",,"['differential-geometry', 'lie-groups', 'lie-algebras', 'vector-bundles']"
57,Linear connection and covariant derivative: help needed to clear up confusion in extension of definitions,Linear connection and covariant derivative: help needed to clear up confusion in extension of definitions,,"Let $\nabla$ be a linear connection defined on the tangent bundle of a manifold $M$ . We have, with $X(M)$ being the global sections module of $TM$ , $$\nabla: X(M)\to \Omega^1(M)\otimes X(M)$$ We can extend it as a derivation of degree 1 on the $\Omega^*(M)\otimes X(M)$ complex by the formula $$\nabla(\omega\otimes X)=d\omega\otimes X+(-1)^{r}\omega\wedge\nabla(X)$$ where $\omega\in\Omega^r(M)$ and $X\in X(M)$ On the other hand, a covariant derivative defined on $TM$ extends in a unique way to the duals of vector fields (i.e., covector fields), and to arbitrary tensor fields, that ensures compatibility with the tensor product and trace operations (tensor contraction). For instance, for $\omega\in \Omega^r(M)$ and $X, Y\in X(M)$ , we get $$\nabla_Y(\omega\otimes X)=\nabla_Y(\omega)\otimes X+\omega\otimes\nabla_Y(X)$$ I am confused here because I cannot reconcile the two formulas. What am I misunderstanding?","Let be a linear connection defined on the tangent bundle of a manifold . We have, with being the global sections module of , We can extend it as a derivation of degree 1 on the complex by the formula where and On the other hand, a covariant derivative defined on extends in a unique way to the duals of vector fields (i.e., covector fields), and to arbitrary tensor fields, that ensures compatibility with the tensor product and trace operations (tensor contraction). For instance, for and , we get I am confused here because I cannot reconcile the two formulas. What am I misunderstanding?","\nabla M X(M) TM \nabla: X(M)\to \Omega^1(M)\otimes X(M) \Omega^*(M)\otimes X(M) \nabla(\omega\otimes X)=d\omega\otimes X+(-1)^{r}\omega\wedge\nabla(X) \omega\in\Omega^r(M) X\in X(M) TM \omega\in \Omega^r(M) X, Y\in X(M) \nabla_Y(\omega\otimes X)=\nabla_Y(\omega)\otimes X+\omega\otimes\nabla_Y(X)",['differential-geometry']
58,Differential of a Map,Differential of a Map,,"I have the following map that embeds the Torus $T^2$ into $\mathbb{R}^3$: $$f(\theta, \phi)=(cos\theta(R+rcos(\phi)),sin\theta(R+rcos(\phi)), rsin\phi)$$ noting that $0<r<R$. I want to compute the differential of $f$, $f_*$, that maps $T_P(T^2)$ to $T_{f(p)}(\mathbb{R}^3)$. This topic is extremely confusing to me.  I am not sure how to really approach the problem at all.  I believe that if $v\in T_p(T^2)$, then I choose a smooth curve $g:\mathbb{R}\to T^2$ s.t. $g(0)=p$ and $g'(0)=v$, then $df(p)v=\frac{d}{dt}f(g(t))$ at $t=0$. I don't really know what to do with all this.  I don't know where to go.  If someone has a good example or a good source to look at that would help explain this problem, or if someone could help me with this problem that would be greatly appreciated. Thank you in advance.","I have the following map that embeds the Torus $T^2$ into $\mathbb{R}^3$: $$f(\theta, \phi)=(cos\theta(R+rcos(\phi)),sin\theta(R+rcos(\phi)), rsin\phi)$$ noting that $0<r<R$. I want to compute the differential of $f$, $f_*$, that maps $T_P(T^2)$ to $T_{f(p)}(\mathbb{R}^3)$. This topic is extremely confusing to me.  I am not sure how to really approach the problem at all.  I believe that if $v\in T_p(T^2)$, then I choose a smooth curve $g:\mathbb{R}\to T^2$ s.t. $g(0)=p$ and $g'(0)=v$, then $df(p)v=\frac{d}{dt}f(g(t))$ at $t=0$. I don't really know what to do with all this.  I don't know where to go.  If someone has a good example or a good source to look at that would help explain this problem, or if someone could help me with this problem that would be greatly appreciated. Thank you in advance.",,['differential-geometry']
59,$M$ is orientable $\Leftrightarrow$ determinant bundle $\det(TM)$ is trivial,is orientable  determinant bundle  is trivial,M \Leftrightarrow \det(TM),"Let $M$ be a differentiable manifold and $TM$ be its tangent bundle. I need to prove the following: $M$ is orientable if and only if $\det(TM)$ is trivial. The definition of determinant bundle I'm using is the following: Given a vector bundle $E$ over $M$ with transition functions $g_{\alpha\beta}$, then the determinant vector bundle $\det(TM)$ over $M$ is the line vector bundle whose transition functions are $\det(g_{\alpha\beta})$. I get the orientability and $\det(TM)$ are closely related since the transition functions of $\det(TM)$ are just the determinant of the Jacobian matrix of the change of chart for an atlas of $M$.","Let $M$ be a differentiable manifold and $TM$ be its tangent bundle. I need to prove the following: $M$ is orientable if and only if $\det(TM)$ is trivial. The definition of determinant bundle I'm using is the following: Given a vector bundle $E$ over $M$ with transition functions $g_{\alpha\beta}$, then the determinant vector bundle $\det(TM)$ over $M$ is the line vector bundle whose transition functions are $\det(g_{\alpha\beta})$. I get the orientability and $\det(TM)$ are closely related since the transition functions of $\det(TM)$ are just the determinant of the Jacobian matrix of the change of chart for an atlas of $M$.",,['differential-geometry']
60,"Explanation of a proof in Colding-Minicozzi's ""A Course in Minimal Surfaces"".","Explanation of a proof in Colding-Minicozzi's ""A Course in Minimal Surfaces"".",,"I have just started reading ""A Course in Minimal Surfaces"" by Colding-Minicozzi on my own. I have to clarify some points in the proof of a lemma given in the book. On page $30$ of the book, they prove the following lemma (Lemma $1.19$) Now I understand that since the image of the Gauss map $N$, in this case is the upper hemisphere, which is contractible and exterior derivative commutes with pullback, hence $N^*\omega =N^*(d\alpha)=d(N^*\alpha)$ What I don't understand are the following two points :- 1) Why is $|A|^2d$Area=$-2Kd$Area=$2N^*\omega$ ? Here $A$ is the Second Fundamental form and $K$ is the Gaussian curvature. I know that for a minimal graph $|A|^2=-2K$ so the first equality is fine. But why the second equality ? 2) It's written ""since $\alpha$ is a one form, hence $\exists$ a constant $C_{\alpha}$ so that $|N^*\alpha|\leq C_{\alpha}|dN|$"" ? Why is this true ? Also, on what quantities does this constant $C_{\alpha}$ depends ? Thank you for your help.","I have just started reading ""A Course in Minimal Surfaces"" by Colding-Minicozzi on my own. I have to clarify some points in the proof of a lemma given in the book. On page $30$ of the book, they prove the following lemma (Lemma $1.19$) Now I understand that since the image of the Gauss map $N$, in this case is the upper hemisphere, which is contractible and exterior derivative commutes with pullback, hence $N^*\omega =N^*(d\alpha)=d(N^*\alpha)$ What I don't understand are the following two points :- 1) Why is $|A|^2d$Area=$-2Kd$Area=$2N^*\omega$ ? Here $A$ is the Second Fundamental form and $K$ is the Gaussian curvature. I know that for a minimal graph $|A|^2=-2K$ so the first equality is fine. But why the second equality ? 2) It's written ""since $\alpha$ is a one form, hence $\exists$ a constant $C_{\alpha}$ so that $|N^*\alpha|\leq C_{\alpha}|dN|$"" ? Why is this true ? Also, on what quantities does this constant $C_{\alpha}$ depends ? Thank you for your help.",,"['differential-geometry', 'riemannian-geometry', 'proof-explanation', 'minimal-surfaces']"
61,How to determine if $M\subset\mathbb R^n$ is contained in an $m$-plane?,How to determine if  is contained in an -plane?,M\subset\mathbb R^n m,"Let $M$ be a $k$ dimensional manifold in $\mathbb R^n$. How to determine if $M$ is contained in some $m$ dimesnional plane ($m\geq k$) ? I know that one way is to guess a $p\in \mathbb R^n$ and $(n-m)$ vectors $v_1,...,v_{n-m}\in \mathbb R^n$ such that $<x-p,v_i>=0$ for each $x\in\mathbb R^n$ and each $v_i$. But I want a more systematic method. (e.g. I know that for $k=1$, $m=2$, we have to check if $M$ has zero torsion.) Thanks.","Let $M$ be a $k$ dimensional manifold in $\mathbb R^n$. How to determine if $M$ is contained in some $m$ dimesnional plane ($m\geq k$) ? I know that one way is to guess a $p\in \mathbb R^n$ and $(n-m)$ vectors $v_1,...,v_{n-m}\in \mathbb R^n$ such that $<x-p,v_i>=0$ for each $x\in\mathbb R^n$ and each $v_i$. But I want a more systematic method. (e.g. I know that for $k=1$, $m=2$, we have to check if $M$ has zero torsion.) Thanks.",,"['differential-geometry', 'riemannian-geometry']"
62,How to explicitly perform the circle eversion in the $3$-dimensional space?,How to explicitly perform the circle eversion in the -dimensional space?,3,"The following claim is a well-known consequence of the Whitney-Graustein theorem : Claim. It does not exist $H\colon\mathbb{S}^1\times[0,1]\overset{C^1}{\rightarrow}\mathbb{R}^2$ such that for all $t\in [0,1]$, $H(\cdot,t)\colon\mathbb{S}^1\rightarrow\mathbb{R}^2$ is an immersion, $H(\cdot,0)=(\cos(2\pi\cdot),\sin(2\pi\cdot))$ and $H(\cdot,1)=(\cos(2\pi \cdot),-\sin(2\pi\cdot))$. In other words, it is impossible to perform a circle eversion in the plane, namely it is impossible to continuously and regularly change the orientation of the circle while sticking to the plane. However, I want to illustrate that it is possible to realize the circle eversion in the $3$-dimensional space. The idea is to thicken the circle into a cylinder, perform a $\pi$-twist on the cylinder in order to put its inside out and finally to retract the everted cylinder onto its equatorial circle. My main concern is to graphically represent the above process using a mathematical software, e.g. SageMath. I tried in vain to write down explicit formulas for it and here I am stuck. Please note that the following homotopy did not seem to do any good: $$\forall x\in\mathbb{S}^1\times [-1,1],\forall t\in [0,1],H(x,t)=\frac{x}{\|x\|^{2t}}.$$ Any enlightenment will be greatly appreciated!","The following claim is a well-known consequence of the Whitney-Graustein theorem : Claim. It does not exist $H\colon\mathbb{S}^1\times[0,1]\overset{C^1}{\rightarrow}\mathbb{R}^2$ such that for all $t\in [0,1]$, $H(\cdot,t)\colon\mathbb{S}^1\rightarrow\mathbb{R}^2$ is an immersion, $H(\cdot,0)=(\cos(2\pi\cdot),\sin(2\pi\cdot))$ and $H(\cdot,1)=(\cos(2\pi \cdot),-\sin(2\pi\cdot))$. In other words, it is impossible to perform a circle eversion in the plane, namely it is impossible to continuously and regularly change the orientation of the circle while sticking to the plane. However, I want to illustrate that it is possible to realize the circle eversion in the $3$-dimensional space. The idea is to thicken the circle into a cylinder, perform a $\pi$-twist on the cylinder in order to put its inside out and finally to retract the everted cylinder onto its equatorial circle. My main concern is to graphically represent the above process using a mathematical software, e.g. SageMath. I tried in vain to write down explicit formulas for it and here I am stuck. Please note that the following homotopy did not seem to do any good: $$\forall x\in\mathbb{S}^1\times [-1,1],\forall t\in [0,1],H(x,t)=\frac{x}{\|x\|^{2t}}.$$ Any enlightenment will be greatly appreciated!",,"['differential-geometry', 'circles', 'homotopy-theory', 'orientation']"
63,Calculation of Connection on regular submanifold,Calculation of Connection on regular submanifold,,"I am studying Riemannian Geometry from the book by M.P. do Carmo and I am trying to get a complete picture of Connections by working out some examples and in particular I to calculate for the class of sub-manifolds given as a regular level set of a regular value, I'm trying to calculate it for that case. As a corollary, I wish to see the case of sphere, or some other simple manifold. If $f:\Bbb{R}^n \to \Bbb{R}^k$  where $ (k<n)$ and $p\in\Bbb{R}^k$ be a regular value, then $M = f^{-1}(p)$ will be a regular submanifold of $\Bbb{R}^n$ of dimension $n-k$. Now how to give a connection on $M$? Any hint will be appreciated. It will be really helpful if one can give a reference to a book, online material or online lecture notes in which similar calculations are done or where sufficient hints are provided.","I am studying Riemannian Geometry from the book by M.P. do Carmo and I am trying to get a complete picture of Connections by working out some examples and in particular I to calculate for the class of sub-manifolds given as a regular level set of a regular value, I'm trying to calculate it for that case. As a corollary, I wish to see the case of sphere, or some other simple manifold. If $f:\Bbb{R}^n \to \Bbb{R}^k$  where $ (k<n)$ and $p\in\Bbb{R}^k$ be a regular value, then $M = f^{-1}(p)$ will be a regular submanifold of $\Bbb{R}^n$ of dimension $n-k$. Now how to give a connection on $M$? Any hint will be appreciated. It will be really helpful if one can give a reference to a book, online material or online lecture notes in which similar calculations are done or where sufficient hints are provided.",,['differential-geometry']
64,"What is the radius of torsion, geometrically?","What is the radius of torsion, geometrically?",,"In studying calculus of space curves, we calculate the quantities ""curvature"" ($\kappa$) and ""torsion""($\tau$). Both have inverse-length as units, so their reciprocals $\frac{1}{\kappa}$ and $\frac{1}{\tau}$ have units of length, and are called ""radius of curvature"" and ""radius of torsion"". I understand that radius of curvature is the radius of a curve's osculating circle at a point. That's is a pretty clear notion geometrically, but I struggle to obtain a corresponding notion for radius of torsion. Can anyone share a geometric intuition behind this length, and what it tells us about a non-planar curve? According to http://mathworld.wolfram.com/OsculatingSphere.html , the osculating sphere does not have radius $\frac{1}{\tau}$, so it's not that. Calling it a ""radius"" seems to imply that it's a radius of something. Thanks in advance for any insight. Edit: If a helix is given by $\left<a\cos t,a\sin t,bt\right>$, ($a$ and $b$ positive), then the curvature is $\frac{a}{a^2+b^2}$ and the torsion is $\frac{b}{a^2+b^2}$. There's a lovely duality there, and if you define a dual helix by swapping $a$ and $b$, then the curvature of one is the torsion of the other, and vice versa. Thus, we could say that the radius of torsion is the radius of curvature for a ""dual"" helix, but I hesitate to define a whole new kind of duality just to awkwardly impart meaning to a phrase I saw in a book. I don't know whether people who know a lot about helices think this way. I'm still hoping there's a more natural answer out there.","In studying calculus of space curves, we calculate the quantities ""curvature"" ($\kappa$) and ""torsion""($\tau$). Both have inverse-length as units, so their reciprocals $\frac{1}{\kappa}$ and $\frac{1}{\tau}$ have units of length, and are called ""radius of curvature"" and ""radius of torsion"". I understand that radius of curvature is the radius of a curve's osculating circle at a point. That's is a pretty clear notion geometrically, but I struggle to obtain a corresponding notion for radius of torsion. Can anyone share a geometric intuition behind this length, and what it tells us about a non-planar curve? According to http://mathworld.wolfram.com/OsculatingSphere.html , the osculating sphere does not have radius $\frac{1}{\tau}$, so it's not that. Calling it a ""radius"" seems to imply that it's a radius of something. Thanks in advance for any insight. Edit: If a helix is given by $\left<a\cos t,a\sin t,bt\right>$, ($a$ and $b$ positive), then the curvature is $\frac{a}{a^2+b^2}$ and the torsion is $\frac{b}{a^2+b^2}$. There's a lovely duality there, and if you define a dual helix by swapping $a$ and $b$, then the curvature of one is the torsion of the other, and vice versa. Thus, we could say that the radius of torsion is the radius of curvature for a ""dual"" helix, but I hesitate to define a whole new kind of duality just to awkwardly impart meaning to a phrase I saw in a book. I don't know whether people who know a lot about helices think this way. I'm still hoping there's a more natural answer out there.",,"['differential-geometry', 'curves']"
65,Hyperbolic metric geodesically complete,Hyperbolic metric geodesically complete,,"Consider the upper half plane model of the hyperbolic space ($\mathbb{H}$ with the riemannian metric $g=\frac{dx^2+dy^2}{y^2}$). It is known that $(\mathbb{H},g)$ is geodesically complete, which means that no geodesic can reach the border $\partial \mathbb{H}$ in a finite time. Why is that? Of course if I consider particular geodesics such as $t\mapsto (0,e^{-t})$ this is true, but I can't figure out a general proof for this fact which doesn't rely on the particular form of the geodesic considered. My intuition is that it must depend on the fact that, as a geodesic approaches the border, the denominator of $g$ goes to $0$. Can you point me out an explanatory proof which shows that the length of a hyperbolic geodesic which goes to the border can't be finite? EDIT: Thank you so much for the answers, I realized I didn't make myself clear. What I mean is: is there a way to prove directly for any hyperbolic geodesic (using the fact that $g$ has $y^2$ at the denominator) that its length is infinite?","Consider the upper half plane model of the hyperbolic space ($\mathbb{H}$ with the riemannian metric $g=\frac{dx^2+dy^2}{y^2}$). It is known that $(\mathbb{H},g)$ is geodesically complete, which means that no geodesic can reach the border $\partial \mathbb{H}$ in a finite time. Why is that? Of course if I consider particular geodesics such as $t\mapsto (0,e^{-t})$ this is true, but I can't figure out a general proof for this fact which doesn't rely on the particular form of the geodesic considered. My intuition is that it must depend on the fact that, as a geodesic approaches the border, the denominator of $g$ goes to $0$. Can you point me out an explanatory proof which shows that the length of a hyperbolic geodesic which goes to the border can't be finite? EDIT: Thank you so much for the answers, I realized I didn't make myself clear. What I mean is: is there a way to prove directly for any hyperbolic geodesic (using the fact that $g$ has $y^2$ at the denominator) that its length is infinite?",,"['differential-geometry', 'riemannian-geometry', 'riemann-surfaces', 'hyperbolic-geometry']"
66,The tangent space of the moduli space of connection?,The tangent space of the moduli space of connection?,,"I'm reading one of Floer's paper. (An Instanton-Invariant for 3-Manifold). Let $M$ be a $3$-manifold. A principal $SU_2$-bundle P over $M$ must be trivial. Fixed a trivialization $P \cong M \times SU_2$  and we can regard the space of connection as $su_2$-valued $1$-forms on $M$. Denote $L_1^4(\Omega^1(M)\otimes su_2)$ by $\mathcal{A}(M)$. The gauge group $L_1^4(M,SU_2)$ (denoted by $\mathcal{G}(M)$) atcs on $\mathcal{A}(M)$ by $$ g(a) = g a g^{-1} + (dg) g^{-1}.$$ Denote $\mathcal{A}(M)/\mathcal{G}(M)$ by $\mathcal{B}(M)$. For $a \in \mathcal{A}(M)$ such that $\operatorname{Fix}(a)$ is the center $\mathbb{Z}_2$. There is an identification for the tangent space $$ T_{[a]} \mathcal{B}(M) = \{ \alpha \in L(\Omega(M) \otimes su_2) | d_a^\ast \alpha = 0 \}$$ where $d_a$ extend $d$ on $M$ by the connection $a$ and $d_a^\ast$ is its $L^2$-adjoint. My question is that how we can get the identification for the tangent space? Or is there any more detailed text about this type of objects? (Moduli spaces, Banach manifolds, etc.) Thank you.","I'm reading one of Floer's paper. (An Instanton-Invariant for 3-Manifold). Let $M$ be a $3$-manifold. A principal $SU_2$-bundle P over $M$ must be trivial. Fixed a trivialization $P \cong M \times SU_2$  and we can regard the space of connection as $su_2$-valued $1$-forms on $M$. Denote $L_1^4(\Omega^1(M)\otimes su_2)$ by $\mathcal{A}(M)$. The gauge group $L_1^4(M,SU_2)$ (denoted by $\mathcal{G}(M)$) atcs on $\mathcal{A}(M)$ by $$ g(a) = g a g^{-1} + (dg) g^{-1}.$$ Denote $\mathcal{A}(M)/\mathcal{G}(M)$ by $\mathcal{B}(M)$. For $a \in \mathcal{A}(M)$ such that $\operatorname{Fix}(a)$ is the center $\mathbb{Z}_2$. There is an identification for the tangent space $$ T_{[a]} \mathcal{B}(M) = \{ \alpha \in L(\Omega(M) \otimes su_2) | d_a^\ast \alpha = 0 \}$$ where $d_a$ extend $d$ on $M$ by the connection $a$ and $d_a^\ast$ is its $L^2$-adjoint. My question is that how we can get the identification for the tangent space? Or is there any more detailed text about this type of objects? (Moduli spaces, Banach manifolds, etc.) Thank you.",,"['differential-geometry', 'differential-topology', 'connections', 'moduli-space']"
67,Embedding of Kähler manifolds into $\Bbb C^n$,Embedding of Kähler manifolds into,\Bbb C^n,Consider $\Bbb C^n$ with its standard hermitian product. This space produces many example of Kähler manifolds simply by taking a smooth affine variety $X\subseteq\Bbb C^n$ with the induced metric. Now I am wondering about the converse: Question: Suppose that $X$ is a Kähler manifold that is also a smooth affine variety over $\Bbb C$. Can we assume that $X$ has an embedding $X\subseteq\Bbb C^n$ such that the metric of $X$ is the one inherited from the hermitian product on $\Bbb C^n$? Note that I assume that $X$ is algebraic to begin with. It would false for a general Kähler manifold.,Consider $\Bbb C^n$ with its standard hermitian product. This space produces many example of Kähler manifolds simply by taking a smooth affine variety $X\subseteq\Bbb C^n$ with the induced metric. Now I am wondering about the converse: Question: Suppose that $X$ is a Kähler manifold that is also a smooth affine variety over $\Bbb C$. Can we assume that $X$ has an embedding $X\subseteq\Bbb C^n$ such that the metric of $X$ is the one inherited from the hermitian product on $\Bbb C^n$? Note that I assume that $X$ is algebraic to begin with. It would false for a general Kähler manifold.,,"['algebraic-geometry', 'differential-geometry', 'complex-geometry', 'kahler-manifolds']"
68,Prove this : $\left(a\cos\alpha\right)^n + \left(b\sin\alpha\right)^n = p^n$,Prove this :,\left(a\cos\alpha\right)^n + \left(b\sin\alpha\right)^n = p^n,I have this question:  If the line $x\cos\alpha + y\sin\alpha = p$ touches the curve $\left(\frac{x}{a}\right)^\frac{n}{n - 1} + \left(\frac{y}{b}\right)^\frac{n}{n - 1} = 1$ then prove that $\left(a\cos\alpha\right)^n + \left(b\sin\alpha\right)^n = p^n$ I know that the equation given is an equation of the line in normal form with perpendicular distance $p$ from origin. Also the slope of given line is $-\cot\alpha$ and this slope of line will be equal to the slope of the curve. But equating both is not yielding the desired result. The only little progress I seem to make after substituting $x$  and $y$ from the equation of line to the equation of curve seems futile to prove this further. I seem to make no further progress in this question. What should I do?,I have this question:  If the line $x\cos\alpha + y\sin\alpha = p$ touches the curve $\left(\frac{x}{a}\right)^\frac{n}{n - 1} + \left(\frac{y}{b}\right)^\frac{n}{n - 1} = 1$ then prove that $\left(a\cos\alpha\right)^n + \left(b\sin\alpha\right)^n = p^n$ I know that the equation given is an equation of the line in normal form with perpendicular distance $p$ from origin. Also the slope of given line is $-\cot\alpha$ and this slope of line will be equal to the slope of the curve. But equating both is not yielding the desired result. The only little progress I seem to make after substituting $x$  and $y$ from the equation of line to the equation of curve seems futile to prove this further. I seem to make no further progress in this question. What should I do?,,"['calculus', 'differential-geometry', 'plane-curves', 'curves']"
69,"example of a subset of a smooth manifold admitting a unique smooth structure making the inclusion an immersion, which is not a weak embedding.","example of a subset of a smooth manifold admitting a unique smooth structure making the inclusion an immersion, which is not a weak embedding.",,"A subset $S$ of a smooth manifold $M$ is called a weakly embedded submanifold (at least in Lee ) if it admits a smooth structure making the inclusion an immersion, and such that for any other smooth manifold $N$, a  map $N \to S$ is smooth iff its composition with $S \hookrightarrow M$ is smooth. Such a smooth structure on $S$ is clearly unique. To get a better feel for this property I am asking for the following: Is there an example of a subset $S$ of smooth manifold $M$ which admits a unique smooth structure making the inclusion $S \hookrightarrow M$ an immersion, but which is not a weak embedding?","A subset $S$ of a smooth manifold $M$ is called a weakly embedded submanifold (at least in Lee ) if it admits a smooth structure making the inclusion an immersion, and such that for any other smooth manifold $N$, a  map $N \to S$ is smooth iff its composition with $S \hookrightarrow M$ is smooth. Such a smooth structure on $S$ is clearly unique. To get a better feel for this property I am asking for the following: Is there an example of a subset $S$ of smooth manifold $M$ which admits a unique smooth structure making the inclusion $S \hookrightarrow M$ an immersion, but which is not a weak embedding?",,"['differential-geometry', 'manifolds', 'smooth-manifolds']"
70,Holomorphic Frobenius Theorem,Holomorphic Frobenius Theorem,,"I'm trying to understand a proof of the Holomorphic Frobenius Theorem using the smooth version as seen in Voisin's Complex Geometry book: (pg 51) http://www.amazon.com/Hodge-Theory-Complex-Algebraic-Geometry/dp/0521718015 She starts with a holomorphic distribution $E$ of dimension $k$ on a complex manifold which is closed under bracket. So, $[E,E]\subseteq E$. Then, to reduce to the real case, we take the real part of the distribution to get another distribution $\Re(E)$ of dimension $2k$ in the real tangent bundle. What I can't understand is why this real distribution, $\Re(E)$, also satisfies the bracket condition. The books says this follows since $E$ is holomorphic and satisfies the bracket condition. My guess is that a local frame for it is given by the real and imaginary parts of a local frame $E$ but i'm not sure how to proceed from there. Any help is much appreciated.","I'm trying to understand a proof of the Holomorphic Frobenius Theorem using the smooth version as seen in Voisin's Complex Geometry book: (pg 51) http://www.amazon.com/Hodge-Theory-Complex-Algebraic-Geometry/dp/0521718015 She starts with a holomorphic distribution $E$ of dimension $k$ on a complex manifold which is closed under bracket. So, $[E,E]\subseteq E$. Then, to reduce to the real case, we take the real part of the distribution to get another distribution $\Re(E)$ of dimension $2k$ in the real tangent bundle. What I can't understand is why this real distribution, $\Re(E)$, also satisfies the bracket condition. The books says this follows since $E$ is holomorphic and satisfies the bracket condition. My guess is that a local frame for it is given by the real and imaginary parts of a local frame $E$ but i'm not sure how to proceed from there. Any help is much appreciated.",,"['differential-geometry', 'complex-geometry']"
71,Canonical Bundle Isomorphism $T_v(E)\cong \pi^* E$,Canonical Bundle Isomorphism,T_v(E)\cong \pi^* E,"Given a smooth vector bundle $\pi:E\to M$, the vertical bundle $T_v(E)$ is by definition $\ker T\pi$, which is a subbundle of $T(E)$. It is asserted in this answer that there is a canonical isomorphism $$T_v(E)\cong \pi^* E.$$ Georges Elencwajg writes the following: Recalling  that for a finite-dimensional vector space  $V$ (seen as a   manifold) we have for each $a\in V$ a canonical isomorphism $T_a(V)=V$   we obtain the canonical isomorphism $T_v(E)=\pi^*(E)$ I am still unable to see how one obtains the claimed isomorphism and would be thankful for elucidation.","Given a smooth vector bundle $\pi:E\to M$, the vertical bundle $T_v(E)$ is by definition $\ker T\pi$, which is a subbundle of $T(E)$. It is asserted in this answer that there is a canonical isomorphism $$T_v(E)\cong \pi^* E.$$ Georges Elencwajg writes the following: Recalling  that for a finite-dimensional vector space  $V$ (seen as a   manifold) we have for each $a\in V$ a canonical isomorphism $T_a(V)=V$   we obtain the canonical isomorphism $T_v(E)=\pi^*(E)$ I am still unable to see how one obtains the claimed isomorphism and would be thankful for elucidation.",,"['differential-geometry', 'differential-topology', 'vector-bundles']"
72,"Question about creating a volume form for $SL(2,\mathbb{R})$",Question about creating a volume form for,"SL(2,\mathbb{R})","This problem comes out of R.W.R. Darling (Differential Forms and Connections) ch.8. In the chapter he shows that if $M$ is an $n$-dimensional differential manifold immersed in $\mathbb{R}^{n+k}$, and $\Psi$ is an immersion from $\mathbb{R}^n \rightarrow \mathbb{R}^{n+k}$ that parametrizes the manifold, and $f$ is a submersion from $\mathbb{R}^{n+k} \rightarrow \mathbb{R}^k$ such that $f^{-1}(0) = M$, then we can construct a volume form $(\star df)$ on $M$ using the Hodge star, and that $(\star df)\Lambda^n \Psi_{*}$, which parametrizes the volume form, is given for $k=1$, and $\Psi(0) = r$ by  $$ \begin{vmatrix} D_1 f(r) & D_1 \Psi_1(0) & \cdots & D_n \Psi_1(0) \\                          \vdots & \vdots      & \ddots & \vdots      \\                          D_n f(r) & D_1 \Psi_n(0) & \cdots & D_n \Psi_n(0) \\ \end{vmatrix}. $$ The exercise is to do this with the submanifold $SL(2,\mathbb{R}) \subset GL(2,\mathbb{R})$ regarded as equivalent to $\mathbb{R}^{nxn}$, with $\Psi$ parametrizing $\begin{pmatrix} x & y \\ z & w \\ \end{pmatrix}$ as the image of $(x,y,z)$ and $f(x,y,z,w) = xw - yz - 1.$ I calculated this, and got:  $$ \begin{vmatrix} w & 1 & 0 & 0 \\                   -z & 0 & 1 & 0 \\                   -y & 0 & 0 & 1 \\                    x & \frac{-w}{x} & \frac{z}{x} & \frac{y}{x} \\ \end{vmatrix}, $$ where $w = \frac{1+yz}{x}$, which correctly evaluates at $I$ to give $-2dx\land dy\land dz$ as the parametrized volume operator. The second part is where I have a problem, it says to extend this volume form in a left-invariant manner to $SL(2,\mathbb{R})$ by calculating $(L_A^{*}(\star df))(A^{-1})$, where $L_A$ is the left shift operator on $GL(2,\mathbb{R})$, with $L_A \begin{pmatrix} s & t \\ u & v \\ \end{pmatrix} = \begin{pmatrix} x & y\\ z & w\\ \end{pmatrix} \begin{pmatrix} s & t \\ u & v \\ \end{pmatrix}$ when $A = \begin{pmatrix} x & y\\ z & w\\ \end{pmatrix}.$ I sense that I should take $(L_A^{*}(\star df)) = ((\star df)L_{A{*}})$ to start the process, but am confused as to how the push-forward fits into the calculation with respect to the parametrization $\Psi$.  Could someone help me with how that works? And do I calculate $L_{A{*}}$ as an element of $\mathbb{R}^{nxn}$ which gives a differential as a 4x4 matrix? And if so, does it pre-multiply or post multiply which of the various pieces of the matrix determinant needed to form the volume form? (The problem is 8.4.5 in Darling, p.173, and this is a self-study question.)","This problem comes out of R.W.R. Darling (Differential Forms and Connections) ch.8. In the chapter he shows that if $M$ is an $n$-dimensional differential manifold immersed in $\mathbb{R}^{n+k}$, and $\Psi$ is an immersion from $\mathbb{R}^n \rightarrow \mathbb{R}^{n+k}$ that parametrizes the manifold, and $f$ is a submersion from $\mathbb{R}^{n+k} \rightarrow \mathbb{R}^k$ such that $f^{-1}(0) = M$, then we can construct a volume form $(\star df)$ on $M$ using the Hodge star, and that $(\star df)\Lambda^n \Psi_{*}$, which parametrizes the volume form, is given for $k=1$, and $\Psi(0) = r$ by  $$ \begin{vmatrix} D_1 f(r) & D_1 \Psi_1(0) & \cdots & D_n \Psi_1(0) \\                          \vdots & \vdots      & \ddots & \vdots      \\                          D_n f(r) & D_1 \Psi_n(0) & \cdots & D_n \Psi_n(0) \\ \end{vmatrix}. $$ The exercise is to do this with the submanifold $SL(2,\mathbb{R}) \subset GL(2,\mathbb{R})$ regarded as equivalent to $\mathbb{R}^{nxn}$, with $\Psi$ parametrizing $\begin{pmatrix} x & y \\ z & w \\ \end{pmatrix}$ as the image of $(x,y,z)$ and $f(x,y,z,w) = xw - yz - 1.$ I calculated this, and got:  $$ \begin{vmatrix} w & 1 & 0 & 0 \\                   -z & 0 & 1 & 0 \\                   -y & 0 & 0 & 1 \\                    x & \frac{-w}{x} & \frac{z}{x} & \frac{y}{x} \\ \end{vmatrix}, $$ where $w = \frac{1+yz}{x}$, which correctly evaluates at $I$ to give $-2dx\land dy\land dz$ as the parametrized volume operator. The second part is where I have a problem, it says to extend this volume form in a left-invariant manner to $SL(2,\mathbb{R})$ by calculating $(L_A^{*}(\star df))(A^{-1})$, where $L_A$ is the left shift operator on $GL(2,\mathbb{R})$, with $L_A \begin{pmatrix} s & t \\ u & v \\ \end{pmatrix} = \begin{pmatrix} x & y\\ z & w\\ \end{pmatrix} \begin{pmatrix} s & t \\ u & v \\ \end{pmatrix}$ when $A = \begin{pmatrix} x & y\\ z & w\\ \end{pmatrix}.$ I sense that I should take $(L_A^{*}(\star df)) = ((\star df)L_{A{*}})$ to start the process, but am confused as to how the push-forward fits into the calculation with respect to the parametrization $\Psi$.  Could someone help me with how that works? And do I calculate $L_{A{*}}$ as an element of $\mathbb{R}^{nxn}$ which gives a differential as a 4x4 matrix? And if so, does it pre-multiply or post multiply which of the various pieces of the matrix determinant needed to form the volume form? (The problem is 8.4.5 in Darling, p.173, and this is a self-study question.)",,"['differential-geometry', 'differential-forms']"
73,Why is an embedding an injective immersion?,Why is an embedding an injective immersion?,,"My course on manifolds defines an embedding as follows: 'A smooth map $f:\mathcal{M}\rightarrow\mathcal{N}$ between manifolds $\mathcal{M}$ of dimension $m$ and $\mathcal{N}$ of dimension $n$ is an embedding if it is a diffeomorphism onto its range. We refer to the range, $f(\mathcal{M})$, of such a map, as a submanifold of $\mathcal{N}$.' Below the definition, it states 'clearly, any embedding is an injective immersion', but I am struggling to see why this is. Injectivity follows from the fact that any diffeomorphism is bijective, but why does the immersion part follow? Why does $f$ being a diffeomorphism onto its range imply that the derivative $d_xf$ is injective for all $x\in\mathcal{M}$? Is it to do with $d_xf$ having a left inverse? I've tried to consider a dimension argument but this has got me even more confused, so I have a second question: we view the $d_xf$ as a linear map from $T_x\mathcal{M}$ to $T_{f(x)}\mathcal{N}$, which can be represented as an $n\times{m}$ matrix since the dimension of a tangent space is equal to the dimension of its corresponding manifold. But couldn't we equally view $d_xf$ as a linear map from $T_x\mathcal{M}$ to $T_{f(x)}f(\mathcal{M})$? This can then be represented as an $m\times{m}$ matrix, since the dimension of $f(\mathcal{M})$ is equal to the dimension of $\mathcal{M}$ (diffeomorphisms preserve dimension?). Which is the correct way to look at it?","My course on manifolds defines an embedding as follows: 'A smooth map $f:\mathcal{M}\rightarrow\mathcal{N}$ between manifolds $\mathcal{M}$ of dimension $m$ and $\mathcal{N}$ of dimension $n$ is an embedding if it is a diffeomorphism onto its range. We refer to the range, $f(\mathcal{M})$, of such a map, as a submanifold of $\mathcal{N}$.' Below the definition, it states 'clearly, any embedding is an injective immersion', but I am struggling to see why this is. Injectivity follows from the fact that any diffeomorphism is bijective, but why does the immersion part follow? Why does $f$ being a diffeomorphism onto its range imply that the derivative $d_xf$ is injective for all $x\in\mathcal{M}$? Is it to do with $d_xf$ having a left inverse? I've tried to consider a dimension argument but this has got me even more confused, so I have a second question: we view the $d_xf$ as a linear map from $T_x\mathcal{M}$ to $T_{f(x)}\mathcal{N}$, which can be represented as an $n\times{m}$ matrix since the dimension of a tangent space is equal to the dimension of its corresponding manifold. But couldn't we equally view $d_xf$ as a linear map from $T_x\mathcal{M}$ to $T_{f(x)}f(\mathcal{M})$? This can then be represented as an $m\times{m}$ matrix, since the dimension of $f(\mathcal{M})$ is equal to the dimension of $\mathcal{M}$ (diffeomorphisms preserve dimension?). Which is the correct way to look at it?",,"['differential-geometry', 'manifolds', 'smooth-manifolds']"
74,Sign of first eigenvalue of conformal Laplacian,Sign of first eigenvalue of conformal Laplacian,,"Let $(M^n,g)$ be some manifold of dimension $n \geq 3$. The conformal Laplacian is given by $L=-4 \frac{n-1}{n-2} \Delta+ R$, where $R$ is the scalar curvature of $M$ and $\Delta= -\operatorname{div}\circ \operatorname{grad}$. Now assume that $g$ is a Riemannian metric, $M$ is orientable, compact and has no boundary.   Show that the sign of the first eigenvalue of $L$ is a conformal invariant. Why is that so? I was able to show some transformation law for $L$. Namely, if $\tilde{g}=v^{4/(n-2)}g$, then for some function $\varphi >0$ we have $L(\varphi)=v^{\frac{n+2}{n-2}} \tilde{L} (v^{-1} \varphi)$. (Here $\tilde{L}$ is the conformal Laplacian with respect to the metric $\tilde{g}$) I originaly hoped I could conclude something similar to: $(\varphi, \lambda)$ eigenpair for $L$ $\Leftrightarrow$ $(v^{-1} \varphi, \lambda \cdot \max v)$ eigenpair for $\tilde{L}$. However I did not have any luck with that so far. (And obviously the above line is NOT true. I just wanted to give an impression what kind of statement I was looking for) How could I go about this exercise?","Let $(M^n,g)$ be some manifold of dimension $n \geq 3$. The conformal Laplacian is given by $L=-4 \frac{n-1}{n-2} \Delta+ R$, where $R$ is the scalar curvature of $M$ and $\Delta= -\operatorname{div}\circ \operatorname{grad}$. Now assume that $g$ is a Riemannian metric, $M$ is orientable, compact and has no boundary.   Show that the sign of the first eigenvalue of $L$ is a conformal invariant. Why is that so? I was able to show some transformation law for $L$. Namely, if $\tilde{g}=v^{4/(n-2)}g$, then for some function $\varphi >0$ we have $L(\varphi)=v^{\frac{n+2}{n-2}} \tilde{L} (v^{-1} \varphi)$. (Here $\tilde{L}$ is the conformal Laplacian with respect to the metric $\tilde{g}$) I originaly hoped I could conclude something similar to: $(\varphi, \lambda)$ eigenpair for $L$ $\Leftrightarrow$ $(v^{-1} \varphi, \lambda \cdot \max v)$ eigenpair for $\tilde{L}$. However I did not have any luck with that so far. (And obviously the above line is NOT true. I just wanted to give an impression what kind of statement I was looking for) How could I go about this exercise?",,"['differential-geometry', 'partial-differential-equations', 'eigenvalues-eigenvectors', 'conformal-geometry']"
75,Classifying left invariant metrics on the 3-dimensional heisenberg group,Classifying left invariant metrics on the 3-dimensional heisenberg group,,"Recently I read that all left invariant metrics on the Heisenberg group are equivalent up to scaling,however no reference was given for this result. I've made some attempt to prove this myself. In particular the Heisenberg group H can be represented as, $$H=\left\{ \begin{bmatrix}1&x&y\\0&1&z\\0&0&1 \end{bmatrix} \Big\vert\, x,y,z\in\mathbb{R}\right\}\tag{1}$$with $$\mathfrak{g}=\left\{\begin{bmatrix}0&x&y\\0&0&z&\\0&0&0\end{bmatrix}\Big| \,x,y,z\in\mathbb{R}\right\}\tag{2}$$its associated Lie algebra. Then we can define a left invariant metric $g$ by choosing a basis for $\mathfrak{g}$ and declaring it orthonormal and then translating. I've made a attempts at this but am not really sure where to start. I've tried starting with two choices of basis $\{E_1,E_2,E_3\}$ and $\{F_1,F_2,F_3\}$ with metrics $g_1,g_2$ respectively. I like to then say that if $\phi:\mathfrak{g}\rightarrow\mathfrak{g}$ is an automorphism I could extend that to an automorphism $\Phi:H\rightarrow H$ which will hopefully be an isometry. If you can point me in the right direction with either a reference or on the proof itself I would appreciate it.","Recently I read that all left invariant metrics on the Heisenberg group are equivalent up to scaling,however no reference was given for this result. I've made some attempt to prove this myself. In particular the Heisenberg group H can be represented as, $$H=\left\{ \begin{bmatrix}1&x&y\\0&1&z\\0&0&1 \end{bmatrix} \Big\vert\, x,y,z\in\mathbb{R}\right\}\tag{1}$$with $$\mathfrak{g}=\left\{\begin{bmatrix}0&x&y\\0&0&z&\\0&0&0\end{bmatrix}\Big| \,x,y,z\in\mathbb{R}\right\}\tag{2}$$its associated Lie algebra. Then we can define a left invariant metric $g$ by choosing a basis for $\mathfrak{g}$ and declaring it orthonormal and then translating. I've made a attempts at this but am not really sure where to start. I've tried starting with two choices of basis $\{E_1,E_2,E_3\}$ and $\{F_1,F_2,F_3\}$ with metrics $g_1,g_2$ respectively. I like to then say that if $\phi:\mathfrak{g}\rightarrow\mathfrak{g}$ is an automorphism I could extend that to an automorphism $\Phi:H\rightarrow H$ which will hopefully be an isometry. If you can point me in the right direction with either a reference or on the proof itself I would appreciate it.",,"['differential-geometry', 'lie-groups', 'riemannian-geometry']"
76,Map Laplacian in terms of covariant derivatives,Map Laplacian in terms of covariant derivatives,,"I stumbled upon the following definition: Let $\mathcal{M}$ be a manifold, $g_{ij}$, $h_{ij}$ be two Riemannian metrics on $\mathcal{M}$, $\psi : \mathcal{M} \to \mathcal{M}$ be a twice differentiable map, $C_1$, $C_2$ be two charts on $\mathcal{M}$, $\Psi := C_2 \circ \psi \circ C_1^{-1}$. The map Laplacian $\Delta_{g,h}$ is defined by $$ (C_2 \circ  (\Delta_{g,h} \psi) \circ C_1^{-1})^q := g^{ij}\left( \partial_i  \partial_j \Psi^q   -  \Gamma(g)^k_{ij} \, \partial_k \Psi^q  + (\Gamma(h)^q_{mn} \circ \Psi) \, \partial_i \Psi^m \, \partial_j \Psi^n \right) . $$ Basically, my question is how to make sense of the above beast, but since this is probably too broad, I would like to narrow it down to: can the above formula be expressed in terms of covariant derivatives? What I got so far: I recognise the term $\partial_i  \partial_j \Psi^q -  \Gamma(g)^k_{ij}\, \partial_k \Psi^q = \nabla_i \nabla_j \Psi^q$ as the second covariant derivative of $\Psi^q$ such that $g^{ij} \nabla_i \nabla_j \Psi^q = \nabla^i \nabla_j \Psi^q$ can be interpreted as the Laplacian of $\Psi^q$. Since $\Psi^q$ depends on the chart $C_2$, I also expect there to be some term to correct for that, which is probably the last one, but I cannot make that last statement any more precise. Update: Another question would be whether it is possible to give a physical intuition for this operator. For example, if $\mathcal{M} = \mathbb{R}^3$, the map Laplacian becomes the vector Laplacian appearing in the Navier-Stokes equation where it describes the friction in the fluid (diffusion of momentum). This intuition does not generalise to the situation at hand, however, because $\psi$ is a map from the manifold onto itself, not from the tangent space (which is $T\mathcal{M} = \mathbb{R}^3 = \mathcal{M}$ for $\mathcal{M} = \mathbb{R}^3$) onto itself.","I stumbled upon the following definition: Let $\mathcal{M}$ be a manifold, $g_{ij}$, $h_{ij}$ be two Riemannian metrics on $\mathcal{M}$, $\psi : \mathcal{M} \to \mathcal{M}$ be a twice differentiable map, $C_1$, $C_2$ be two charts on $\mathcal{M}$, $\Psi := C_2 \circ \psi \circ C_1^{-1}$. The map Laplacian $\Delta_{g,h}$ is defined by $$ (C_2 \circ  (\Delta_{g,h} \psi) \circ C_1^{-1})^q := g^{ij}\left( \partial_i  \partial_j \Psi^q   -  \Gamma(g)^k_{ij} \, \partial_k \Psi^q  + (\Gamma(h)^q_{mn} \circ \Psi) \, \partial_i \Psi^m \, \partial_j \Psi^n \right) . $$ Basically, my question is how to make sense of the above beast, but since this is probably too broad, I would like to narrow it down to: can the above formula be expressed in terms of covariant derivatives? What I got so far: I recognise the term $\partial_i  \partial_j \Psi^q -  \Gamma(g)^k_{ij}\, \partial_k \Psi^q = \nabla_i \nabla_j \Psi^q$ as the second covariant derivative of $\Psi^q$ such that $g^{ij} \nabla_i \nabla_j \Psi^q = \nabla^i \nabla_j \Psi^q$ can be interpreted as the Laplacian of $\Psi^q$. Since $\Psi^q$ depends on the chart $C_2$, I also expect there to be some term to correct for that, which is probably the last one, but I cannot make that last statement any more precise. Update: Another question would be whether it is possible to give a physical intuition for this operator. For example, if $\mathcal{M} = \mathbb{R}^3$, the map Laplacian becomes the vector Laplacian appearing in the Navier-Stokes equation where it describes the friction in the fluid (diffusion of momentum). This intuition does not generalise to the situation at hand, however, because $\psi$ is a map from the manifold onto itself, not from the tangent space (which is $T\mathcal{M} = \mathbb{R}^3 = \mathcal{M}$ for $\mathcal{M} = \mathbb{R}^3$) onto itself.",,"['differential-geometry', 'tensors']"
77,What is a Killing Vector Field?,What is a Killing Vector Field?,,"What is the definition of Killing vector field?. The one my professor told me is : a smooth vector field $V$ on $M$ is called a Killing vector field for $g$ if the flow of $V$ acts by isometries of $g$. So what does it mean by the flow of $V$ acts by isometries? I suspect this definition is equivalent to saying the lie derivative of $g$ along a vector field is 0, then the vector field is a Killing field. But first I need to figure out the meaning of the first definition. $g$ is just the Riemannian Metric.","What is the definition of Killing vector field?. The one my professor told me is : a smooth vector field $V$ on $M$ is called a Killing vector field for $g$ if the flow of $V$ acts by isometries of $g$. So what does it mean by the flow of $V$ acts by isometries? I suspect this definition is equivalent to saying the lie derivative of $g$ along a vector field is 0, then the vector field is a Killing field. But first I need to figure out the meaning of the first definition. $g$ is just the Riemannian Metric.",,[]
78,Geodesic axis and displacement function,Geodesic axis and displacement function,,"So let us assume our manifold is complete, simply connected, and has nonpositive sectional curvature. If we assume that the displacement function $f(x)=d(x,\phi(x))$, for an isometry $\phi:M\rightarrow M$, is bounded from below by a positive value, show that there exists a geodesic axis. That is there exists a geodesic $\gamma$ such that $\phi\circ \gamma(t)=\gamma(t+t_0)$ assuming that $f(x)=\inf(f)$ for some $x\in M$. So here is what I have so far. It seems that I should select the geodesic that goes from $x$ to $\phi(x)$ where $f(x)=\inf(f)$. I had two ideas for showing that this is an axis. Namely we could show that $\phi\circ\gamma(-t_0)=\gamma(0)$, where $\gamma(0)=x$ and $\gamma(t_0)=\phi(x)$, then we could use that geodesics are unique in this type of manifold, or we could show that $f(\gamma)$ is constant which could also establish our result, but I'm not sure how to go about with either. Any suggestions? Thanks.","So let us assume our manifold is complete, simply connected, and has nonpositive sectional curvature. If we assume that the displacement function $f(x)=d(x,\phi(x))$, for an isometry $\phi:M\rightarrow M$, is bounded from below by a positive value, show that there exists a geodesic axis. That is there exists a geodesic $\gamma$ such that $\phi\circ \gamma(t)=\gamma(t+t_0)$ assuming that $f(x)=\inf(f)$ for some $x\in M$. So here is what I have so far. It seems that I should select the geodesic that goes from $x$ to $\phi(x)$ where $f(x)=\inf(f)$. I had two ideas for showing that this is an axis. Namely we could show that $\phi\circ\gamma(-t_0)=\gamma(0)$, where $\gamma(0)=x$ and $\gamma(t_0)=\phi(x)$, then we could use that geodesics are unique in this type of manifold, or we could show that $f(\gamma)$ is constant which could also establish our result, but I'm not sure how to go about with either. Any suggestions? Thanks.",,"['differential-geometry', 'riemannian-geometry']"
79,How do I derive this formula from gauge theory?,How do I derive this formula from gauge theory?,,"This is Exercise 3.4.14 in R. W. Sharpe's Differential Geometry . Suppose $G$ is a Lie group with Lie algebra $\mathfrak{g}$ and $H$ is a Lie subgroup of $G$. Let $\theta$ be a $\mathfrak{g}$-valued $1$-form on a manifold $M$ and let $k:M\to H$. Then, $$\mathrm{d}\!\left(\mathrm{Ad}_k^{-1}\theta\right)=\mathrm{Ad}_k^{-1}(\mathrm{d}\theta)-[\mathrm{Ad}_k^{-1}\theta,k^*\omega_H],$$ where $\omega_H$ is the Maurer-Cartan form on $H$ and $\mathrm{Ad}_k^{-1}\theta(X)=\mathrm{Ad}_k^{-1}(\theta(X))$. In other words, for vector fields $X,Y\in\Gamma(TM)$, $$\mathrm{d}\!\left(\mathrm{Ad}_k^{-1}\theta\right)(X\wedge Y)=\mathrm{Ad}_k^{-1}(\mathrm{d}\theta(X\wedge Y))-[\mathrm{Ad}_k^{-1}(\theta(X)),L_{k\,*}^{-1}k_*Y]-[L_{k\,*}^{-1}k_*X,\mathrm{Ad}_k^{-1}(\theta(Y))].$$ I've managed to reduce this to $$\mathrm{Ad}_k\!\left(X(\mathrm{Ad}_k^{-1}\theta(Y))-Y(\mathrm{Ad}_k^{-1}\theta(X))\right)=X(\theta(Y))-Y(\theta(X))-[\theta(X),R_{k\,*}^{-1}k_*Y]-[R_{k\,*}^{-1}k_*X,\theta(Y)],$$ but I'm not sure whether this is even helpful. Can anyone suggest how to prove this? Sharpe goes on to say that this exercise is ""fundamental for the elementary properties of Cartan gauges,"" so I'm including the gauge-theory tag.","This is Exercise 3.4.14 in R. W. Sharpe's Differential Geometry . Suppose $G$ is a Lie group with Lie algebra $\mathfrak{g}$ and $H$ is a Lie subgroup of $G$. Let $\theta$ be a $\mathfrak{g}$-valued $1$-form on a manifold $M$ and let $k:M\to H$. Then, $$\mathrm{d}\!\left(\mathrm{Ad}_k^{-1}\theta\right)=\mathrm{Ad}_k^{-1}(\mathrm{d}\theta)-[\mathrm{Ad}_k^{-1}\theta,k^*\omega_H],$$ where $\omega_H$ is the Maurer-Cartan form on $H$ and $\mathrm{Ad}_k^{-1}\theta(X)=\mathrm{Ad}_k^{-1}(\theta(X))$. In other words, for vector fields $X,Y\in\Gamma(TM)$, $$\mathrm{d}\!\left(\mathrm{Ad}_k^{-1}\theta\right)(X\wedge Y)=\mathrm{Ad}_k^{-1}(\mathrm{d}\theta(X\wedge Y))-[\mathrm{Ad}_k^{-1}(\theta(X)),L_{k\,*}^{-1}k_*Y]-[L_{k\,*}^{-1}k_*X,\mathrm{Ad}_k^{-1}(\theta(Y))].$$ I've managed to reduce this to $$\mathrm{Ad}_k\!\left(X(\mathrm{Ad}_k^{-1}\theta(Y))-Y(\mathrm{Ad}_k^{-1}\theta(X))\right)=X(\theta(Y))-Y(\theta(X))-[\theta(X),R_{k\,*}^{-1}k_*Y]-[R_{k\,*}^{-1}k_*X,\theta(Y)],$$ but I'm not sure whether this is even helpful. Can anyone suggest how to prove this? Sharpe goes on to say that this exercise is ""fundamental for the elementary properties of Cartan gauges,"" so I'm including the gauge-theory tag.",,"['differential-geometry', 'lie-groups', 'lie-algebras', 'differential-forms', 'gauge-theory']"
80,Is there a natural Riemannian structure on the total space of a vector bundle?,Is there a natural Riemannian structure on the total space of a vector bundle?,,"Suppose $B$ is a Riemannian manifold and $\pi: E \to B$ is a smooth vector bundle equipped with a metric. Is there a natural Riemannian metric on $E$, i.e. a bundle metric on $TE\to E$? It seems like there should at least be metrics on $TE$ which restrict to the original Riemannian metric on $TB \subset TE$. I tried to show that the metrics on $E \to B$ and $TB \to B$ give a splitting of the short exact sequence $VE \to TE \to \pi^* TB$ but ended up swimming in symbols.","Suppose $B$ is a Riemannian manifold and $\pi: E \to B$ is a smooth vector bundle equipped with a metric. Is there a natural Riemannian metric on $E$, i.e. a bundle metric on $TE\to E$? It seems like there should at least be metrics on $TE$ which restrict to the original Riemannian metric on $TB \subset TE$. I tried to show that the metrics on $E \to B$ and $TB \to B$ give a splitting of the short exact sequence $VE \to TE \to \pi^* TB$ but ended up swimming in symbols.",,"['differential-geometry', 'riemannian-geometry', 'vector-bundles']"
81,Algebraic vs Geometric Picard groups,Algebraic vs Geometric Picard groups,,"Given a smooth manifold $P$ . I found two definition of a Picard group. The first one is algebraic, $Pic(C^{\infty}(P))$ is defined as the set of self-equivalence bimodules (Morita equivalence) with tensor product of bimodules as group operation. The second one is geometric, $Pic(P)$ is defined as the set of isomorphism classes of line bundles over $P$ with tensor product of vector bundles as group operation. I know there is some relation between the two concepts via Serre-Swan Theorem. Yet I don't know what is the exact relation, I mean, are they isomorphic? Can one of them be seen as a (normal) subgroup of the other? I would like an answer avoiding use of algebraic geometric terminologies, like affine spaces or schemes, if possible.","Given a smooth manifold . I found two definition of a Picard group. The first one is algebraic, is defined as the set of self-equivalence bimodules (Morita equivalence) with tensor product of bimodules as group operation. The second one is geometric, is defined as the set of isomorphism classes of line bundles over with tensor product of vector bundles as group operation. I know there is some relation between the two concepts via Serre-Swan Theorem. Yet I don't know what is the exact relation, I mean, are they isomorphic? Can one of them be seen as a (normal) subgroup of the other? I would like an answer avoiding use of algebraic geometric terminologies, like affine spaces or schemes, if possible.",P Pic(C^{\infty}(P)) Pic(P) P,"['differential-geometry', 'smooth-manifolds', 'line-bundles']"
82,Can a volume form on a submanifold be extended to a parallel form in a neighbourhood?,Can a volume form on a submanifold be extended to a parallel form in a neighbourhood?,,"Let $(M^{n+1},g)$ be a Riemannian manifold and let $\Sigma^n \hookrightarrow M$ be a smooth, closed, embedded submanifold. Let $\Omega$ be the volume form of $\Sigma$. It is well-known that a volume form is parallel, i.e. $$ \nabla^{\Sigma}\Omega \equiv 0 $$ on $\Sigma$. Can we always extend $\Omega$ to an $n$-form $\tilde{\Omega}$ defined on a neighborhood $U$ of $\Sigma$ such that $$ \nabla \tilde{\Omega} \equiv 0 $$ on $U$?","Let $(M^{n+1},g)$ be a Riemannian manifold and let $\Sigma^n \hookrightarrow M$ be a smooth, closed, embedded submanifold. Let $\Omega$ be the volume form of $\Sigma$. It is well-known that a volume form is parallel, i.e. $$ \nabla^{\Sigma}\Omega \equiv 0 $$ on $\Sigma$. Can we always extend $\Omega$ to an $n$-form $\tilde{\Omega}$ defined on a neighborhood $U$ of $\Sigma$ such that $$ \nabla \tilde{\Omega} \equiv 0 $$ on $U$?",,"['differential-geometry', 'riemannian-geometry', 'differential-forms']"
83,principal curvature of the flat torus,principal curvature of the flat torus,,"I am looking at the Hopf-fibration and I am looking at the preimage of the equator in $\mathbb{S}^2$. I think that I have proved that this is just the flat torus and now I want to calculate the principal curvatures of this torus. My general approach to the problem has been: I consider $\mathbb{R}^4$ as $\mathbb{C}^2$ and $\mathbb{S}^3$ to be ""the unit circle"" in this ""plane"", every point on the circle determines a line through the origion. And the lines through the origion intersects the sphere in a unique circle. Mapping $\mathbb{S^3}$ to the space of lines through the origion gives me an onto map to $\mathbb{C}\mathbb{P}^1$ which I diffeomorphically identify with $\mathbb{S}^2$. The preimage of any point $(z_1:z_2)$ in $\mathbb{S}^3$ is just the circle $\frac{z_1}{z_2}=z$. Hence the preimage of the equator is all circles satisfying $\frac{|z_1|}{|z_2|}=1$, i.e $|z_1|=|z_2|$, hence the preimage is a product of two circles, i.e a torus. Since my torus is just $S^1\times S^1$ in $\mathbb{C}\times \mathbb{C}$ it is equipped with the product metric, and hence in this case flat. I am now stuck, here is two ways I would like to go forward: Calculating the Weingarten map.  Using the parametrization $(e^{i\theta },e^{i\psi })$, what is the normal direction inside of $\mathbb{S}^3$? I have also tried to look at the normal curvature of curves in $S^1\times S^1$ are these just ""circles wrapping around the torus"" the smaller ones curving more then the bigger? This way of thinking would give me principal curvature $1$ and something depending on where on the torus I am being $-1$ in ""the inner circle""?","I am looking at the Hopf-fibration and I am looking at the preimage of the equator in $\mathbb{S}^2$. I think that I have proved that this is just the flat torus and now I want to calculate the principal curvatures of this torus. My general approach to the problem has been: I consider $\mathbb{R}^4$ as $\mathbb{C}^2$ and $\mathbb{S}^3$ to be ""the unit circle"" in this ""plane"", every point on the circle determines a line through the origion. And the lines through the origion intersects the sphere in a unique circle. Mapping $\mathbb{S^3}$ to the space of lines through the origion gives me an onto map to $\mathbb{C}\mathbb{P}^1$ which I diffeomorphically identify with $\mathbb{S}^2$. The preimage of any point $(z_1:z_2)$ in $\mathbb{S}^3$ is just the circle $\frac{z_1}{z_2}=z$. Hence the preimage of the equator is all circles satisfying $\frac{|z_1|}{|z_2|}=1$, i.e $|z_1|=|z_2|$, hence the preimage is a product of two circles, i.e a torus. Since my torus is just $S^1\times S^1$ in $\mathbb{C}\times \mathbb{C}$ it is equipped with the product metric, and hence in this case flat. I am now stuck, here is two ways I would like to go forward: Calculating the Weingarten map.  Using the parametrization $(e^{i\theta },e^{i\psi })$, what is the normal direction inside of $\mathbb{S}^3$? I have also tried to look at the normal curvature of curves in $S^1\times S^1$ are these just ""circles wrapping around the torus"" the smaller ones curving more then the bigger? This way of thinking would give me principal curvature $1$ and something depending on where on the torus I am being $-1$ in ""the inner circle""?",,['differential-geometry']
84,Connection form on a frame bundle,Connection form on a frame bundle,,"Let $(E(M),\pi,M)$ be the frame bundle over a manifold $M$ of rank $n$. Consider a covering of $M$ by open neighborhoods $U_{\alpha}$. Let $s_{i}$ and $t_{j}$ where $i,j\in {1,...,n}$ be frames of $M$ over $U_{\alpha}$ and $U_{\beta}$, respectively. Then there is a $g_{ij}\in GL(n,\mathbb{R})$ such that $t_{j}= s_{i}.g_{ij}$. My question: Why does  connection $1-$form $\omega_{U_{\alpha}}$ over $U_{\alpha}$ with values in the Lie algebra of the group $GL(n,\mathbb{R})$ satisfy the following condition, locally? $$\omega_{U_{\beta}}= g_{ij}^{-1}\omega_{U_{\alpha}}g_{ij}+ g_{ij}^{-1} dg_{ij}$$ Second question:Is there any book on frame bundles that can help me? Thanks.","Let $(E(M),\pi,M)$ be the frame bundle over a manifold $M$ of rank $n$. Consider a covering of $M$ by open neighborhoods $U_{\alpha}$. Let $s_{i}$ and $t_{j}$ where $i,j\in {1,...,n}$ be frames of $M$ over $U_{\alpha}$ and $U_{\beta}$, respectively. Then there is a $g_{ij}\in GL(n,\mathbb{R})$ such that $t_{j}= s_{i}.g_{ij}$. My question: Why does  connection $1-$form $\omega_{U_{\alpha}}$ over $U_{\alpha}$ with values in the Lie algebra of the group $GL(n,\mathbb{R})$ satisfy the following condition, locally? $$\omega_{U_{\beta}}= g_{ij}^{-1}\omega_{U_{\alpha}}g_{ij}+ g_{ij}^{-1} dg_{ij}$$ Second question:Is there any book on frame bundles that can help me? Thanks.",,"['differential-geometry', 'fiber-bundles']"
85,Exterior derivative of a form and $d(d\omega)=0$?,Exterior derivative of a form and ?,d(d\omega)=0,"We know that in differential geometry, $d^2\omega=0$, where $\omega $ is a form and $d$ is the exterior derivative. However if this form happens to be the exterior derivative of another form $\theta$ such that $\omega =d\theta $ then won't $d\omega $ always be zero since we could also write $d\omega =d(d\theta)$? Forgive me if I'm missing something elementary!","We know that in differential geometry, $d^2\omega=0$, where $\omega $ is a form and $d$ is the exterior derivative. However if this form happens to be the exterior derivative of another form $\theta$ such that $\omega =d\theta $ then won't $d\omega $ always be zero since we could also write $d\omega =d(d\theta)$? Forgive me if I'm missing something elementary!",,"['differential-geometry', 'differential-forms', 'exterior-algebra']"
86,Boundary of the boundary of a manifold with corners,Boundary of the boundary of a manifold with corners,,"A point of a manifold with corners is a boundary point by definition if one of its coordinates is $0$ by some (hence in all) chart with corners (see here ). In the same page one can read: The boundary of a smooth manifold with corners, however, is in general   not a smooth manifold with corners (e.g., think of the boundary of a   cube). ... It is, however, a union of finitely many such But how is the boundary of such an object defined? What is the definition of the boundary of the unions of manifolds with corners? In topological sense it is itself (or at least a subset of itself), but I think we should expect here a definition so, that the boundary of the boundary of a manifold with corner is empty. Second attempt to formulate my question without error Is there a definition of  the boundary of the boundary of a manifold with corners so, that we can state, that the boundary of the boundary of a manifold with corners is empty? (Similarly as we can say that the boundary of the boundary of a manifold with boundary is empty)","A point of a manifold with corners is a boundary point by definition if one of its coordinates is $0$ by some (hence in all) chart with corners (see here ). In the same page one can read: The boundary of a smooth manifold with corners, however, is in general   not a smooth manifold with corners (e.g., think of the boundary of a   cube). ... It is, however, a union of finitely many such But how is the boundary of such an object defined? What is the definition of the boundary of the unions of manifolds with corners? In topological sense it is itself (or at least a subset of itself), but I think we should expect here a definition so, that the boundary of the boundary of a manifold with corner is empty. Second attempt to formulate my question without error Is there a definition of  the boundary of the boundary of a manifold with corners so, that we can state, that the boundary of the boundary of a manifold with corners is empty? (Similarly as we can say that the boundary of the boundary of a manifold with boundary is empty)",,"['differential-geometry', 'smooth-manifolds', 'manifolds-with-boundary']"
87,"If $\alpha$ is a unit speed curve of constant curvature lying in a sphere, then $\alpha$ is a circle.","If  is a unit speed curve of constant curvature lying in a sphere, then  is a circle.",\alpha \alpha,"I'm trying to solve the following problem but got stuck along the way. I would like some help on getting this through. Prove that if $\alpha$ is a unit speed curve of constant curvature lying in a sphere, then $\alpha$ is a circle. Solution: My goal is to show that the torsion is zero. We have $\alpha \cdot \alpha =r^2$, so taking the derivatives, $T \cdot \alpha =0$. So we can let $\alpha=xN+yB$ for some functions $x$ and $y$. Now differentiating the previous equation again, we get $T' \cdot \alpha + T\cdot T=0$, so using Frenet Formula, we get $\kappa N \cdot \alpha =-1$, and another differentiation and Frenet Formula yield $\tau B\cdot \alpha =0$. So by the assumption on $\alpha$, we get $\tau y=0$. However, here is where I have a problem. If I know that $y\neq0$, then I'm done. But I cannot guarantee that $y$ is nonzero, so I can't show that $\tau$ must be zero. How can I solve this? I would greatly appreciate any solutions or suggestions.","I'm trying to solve the following problem but got stuck along the way. I would like some help on getting this through. Prove that if $\alpha$ is a unit speed curve of constant curvature lying in a sphere, then $\alpha$ is a circle. Solution: My goal is to show that the torsion is zero. We have $\alpha \cdot \alpha =r^2$, so taking the derivatives, $T \cdot \alpha =0$. So we can let $\alpha=xN+yB$ for some functions $x$ and $y$. Now differentiating the previous equation again, we get $T' \cdot \alpha + T\cdot T=0$, so using Frenet Formula, we get $\kappa N \cdot \alpha =-1$, and another differentiation and Frenet Formula yield $\tau B\cdot \alpha =0$. So by the assumption on $\alpha$, we get $\tau y=0$. However, here is where I have a problem. If I know that $y\neq0$, then I'm done. But I cannot guarantee that $y$ is nonzero, so I can't show that $\tau$ must be zero. How can I solve this? I would greatly appreciate any solutions or suggestions.",,"['differential-geometry', 'vector-analysis']"
88,Characterize the sphere using mean curvature.,Characterize the sphere using mean curvature.,,"We know the following result: if $\Sigma$ is a compact surface than $$ \int_{\Sigma}H^2 \ge 4 \pi, $$ where with $H= \frac{1}{2}(\kappa_1+\kappa_2)$ we denote the main curvature. I have to prove that $\int_{\Sigma} H^2 = 4 \pi$ if and only if $\Sigma$ is a sphere $\mathbb{S}_R$. Now, if $\Sigma=\mathbb{S}_R $ then we know that $\kappa_1=\kappa_2=\frac{1}{R}$, so $$ \int_{\mathbb{S}_R}H^2 d\mathcal{A}_{\mathbb{S}_R}=\frac{1}{R^2}  \int_{\mathbb{S}_R} 1 \,\,d\mathcal{A}_{\mathbb{S}_R} = \frac{4\pi R^2}{R^2}= 4 \pi .$$ How can I prove the other part of the proposition? Thank you!","We know the following result: if $\Sigma$ is a compact surface than $$ \int_{\Sigma}H^2 \ge 4 \pi, $$ where with $H= \frac{1}{2}(\kappa_1+\kappa_2)$ we denote the main curvature. I have to prove that $\int_{\Sigma} H^2 = 4 \pi$ if and only if $\Sigma$ is a sphere $\mathbb{S}_R$. Now, if $\Sigma=\mathbb{S}_R $ then we know that $\kappa_1=\kappa_2=\frac{1}{R}$, so $$ \int_{\mathbb{S}_R}H^2 d\mathcal{A}_{\mathbb{S}_R}=\frac{1}{R^2}  \int_{\mathbb{S}_R} 1 \,\,d\mathcal{A}_{\mathbb{S}_R} = \frac{4\pi R^2}{R^2}= 4 \pi .$$ How can I prove the other part of the proposition? Thank you!",,"['differential-geometry', 'curvature']"
89,"How to show that $\{(x,y,z)\in\mathbb{R}^3:x^4+y^4+z^4=1\}$ is diffeomorphic to the $2$-sphere.",How to show that  is diffeomorphic to the -sphere.,"\{(x,y,z)\in\mathbb{R}^3:x^4+y^4+z^4=1\} 2","How to show that the ""squared sphere""   $$\tilde{S}^2=\{(x,y,z)\in\mathbb{R}^3:x^4+y^4+z^4=1\}$$   is diffeomorphism to the standard $2$-sphere   $$S^2=\{(x,y,z)\in\mathbb{R}^3:x^2+y^2+z^2=1\}?$$ One obvious map between the two is $$F:S^2\to\tilde{S}^2,\quad F(x,y,z)=(x^2,y^2,z^2),$$ but this is clearly not surjective since it maps only to non-negative numbers. What to do? Here is how it looks:","How to show that the ""squared sphere""   $$\tilde{S}^2=\{(x,y,z)\in\mathbb{R}^3:x^4+y^4+z^4=1\}$$   is diffeomorphism to the standard $2$-sphere   $$S^2=\{(x,y,z)\in\mathbb{R}^3:x^2+y^2+z^2=1\}?$$ One obvious map between the two is $$F:S^2\to\tilde{S}^2,\quad F(x,y,z)=(x^2,y^2,z^2),$$ but this is clearly not surjective since it maps only to non-negative numbers. What to do? Here is how it looks:",,"['calculus', 'differential-geometry']"
90,Question on calculating curvature of a surface given implicitly,Question on calculating curvature of a surface given implicitly,,"I want to find, as an exercise, an expression for the curvature of a surface given by the zero set of a function. I reached a final expression, but when I test it for a sphere I get a non-constant expression. I know I'm doing a step wrong, but I don't know why. Below is what I have. Say we have a good enough $f:\Bbb R^3 \to \Bbb R$. This defines a surface in $\Bbb R^3$ as $$S = \{(x,y,z)\in\Bbb R^3 : f(x,y,z) = 0\}$$ Suppose that $f_z = \frac{\partial f}{\partial z}\neq 0$. Then by the implicit function theorem it's easy enough to see we have a local parametrization of $S$ (in fact, the graph of a function): $$h:\Bbb R^2\to \Bbb R^3 \atop (x,y)\mapsto (x,y,h(x,y))$$ and $h$ satisfies (found using the chain rule on $f(x,y,h(x,y)) = 0$) $$\begin{align}h_x = \frac{-f_x}{f_z} \\ h_y = \frac{-f_y}{f_z}\end{align}$$ This way we have a basis for $T_pS$ (at a point $p$ which is omitted in the expressions) $(1,0,-f_x/f_z), (0,1,-f_y/f_z)$. With the usual notation, the coefficients of the first fundamental form $I$ can be calculated as $$E = 1+(f_x/f_z)^2, F = \frac{f_xf_y}{f_z^2}, G = 1 + (f_y/f_z)^2$$ My problem, it seems, comes with the second fundamental form $II$. For example, my calculation of $h_{xx}$ results in $$h_{xx} = \frac{f_xf_{xz}-f_{xx}f_z}{f_z^2}$$ while on this page they get $$h_{xx} = \frac{2f_xf_zf_{xz}-f_x^2f_{zz}-f_z^2f_{xx}}{f_z^3}$$ so either something is horribly wrong and I've forgotten how to differentiate a quotient, or there's something else in the calculation I'm not including. Note that this is before doing anything with the normal vector, just partial derivatives of the parametrization. Could someone clear this up?","I want to find, as an exercise, an expression for the curvature of a surface given by the zero set of a function. I reached a final expression, but when I test it for a sphere I get a non-constant expression. I know I'm doing a step wrong, but I don't know why. Below is what I have. Say we have a good enough $f:\Bbb R^3 \to \Bbb R$. This defines a surface in $\Bbb R^3$ as $$S = \{(x,y,z)\in\Bbb R^3 : f(x,y,z) = 0\}$$ Suppose that $f_z = \frac{\partial f}{\partial z}\neq 0$. Then by the implicit function theorem it's easy enough to see we have a local parametrization of $S$ (in fact, the graph of a function): $$h:\Bbb R^2\to \Bbb R^3 \atop (x,y)\mapsto (x,y,h(x,y))$$ and $h$ satisfies (found using the chain rule on $f(x,y,h(x,y)) = 0$) $$\begin{align}h_x = \frac{-f_x}{f_z} \\ h_y = \frac{-f_y}{f_z}\end{align}$$ This way we have a basis for $T_pS$ (at a point $p$ which is omitted in the expressions) $(1,0,-f_x/f_z), (0,1,-f_y/f_z)$. With the usual notation, the coefficients of the first fundamental form $I$ can be calculated as $$E = 1+(f_x/f_z)^2, F = \frac{f_xf_y}{f_z^2}, G = 1 + (f_y/f_z)^2$$ My problem, it seems, comes with the second fundamental form $II$. For example, my calculation of $h_{xx}$ results in $$h_{xx} = \frac{f_xf_{xz}-f_{xx}f_z}{f_z^2}$$ while on this page they get $$h_{xx} = \frac{2f_xf_zf_{xz}-f_x^2f_{zz}-f_z^2f_{xx}}{f_z^3}$$ so either something is horribly wrong and I've forgotten how to differentiate a quotient, or there's something else in the calculation I'm not including. Note that this is before doing anything with the normal vector, just partial derivatives of the parametrization. Could someone clear this up?",,"['differential-geometry', 'partial-derivative', 'implicit-differentiation', 'curvature', 'implicit-function-theorem']"
91,Bijective isometry which fixes origin from $\mathbb{R}^{n} \longrightarrow \mathbb{R}^{n}$ is linear,Bijective isometry which fixes origin from  is linear,\mathbb{R}^{n} \longrightarrow \mathbb{R}^{n},I was  going through Hall's book about Lie groups.While presenting Euler groups $E(n)$ and on the way to prove that they form a matrix Lie group hee made a proposition that Every one one onto distance preserving map from $\mathbb{R}^{n}$ to $\mathbb{R}^{n}$ which fixes origin will be linear.He has not proved it.For $n=1$ it is clear.But how to prove it generally ?,I was  going through Hall's book about Lie groups.While presenting Euler groups $E(n)$ and on the way to prove that they form a matrix Lie group hee made a proposition that Every one one onto distance preserving map from $\mathbb{R}^{n}$ to $\mathbb{R}^{n}$ which fixes origin will be linear.He has not proved it.For $n=1$ it is clear.But how to prove it generally ?,,"['differential-geometry', 'lie-groups']"
92,Proving one version of equivariant formality,Proving one version of equivariant formality,,"Let $G$ be a compact, connected Lie group acting smoothly on a compact, connected and oriented smooth manifold $M$. We denote by $H_G^*(M)$ the corresponding equivariant cohomology. We have a canonical map, the characteristic map , $$ c:H^*(BG)\rightarrow H_G^*(M) $$ that endows $H_G^*(M)$ with the structure of an $H^*(BG)$-module. There's also a canonical restriction map $$ r:H_G^*(M)\rightarrow H^*(M). $$ We say that $M$ is equivariantly formal if $r$ is onto. Any hints on how to prove the following will be appreciated: Propositon: If $M$ is equivariantly formal, then $H_G^*(M)\simeq H^*(M)\otimes H^*(BG)$ as $H^*(BG)$-modules. N.B.: I am aware that there are many different ways to define equivariant formality, but I'd like to use only the given definitions, if possible.","Let $G$ be a compact, connected Lie group acting smoothly on a compact, connected and oriented smooth manifold $M$. We denote by $H_G^*(M)$ the corresponding equivariant cohomology. We have a canonical map, the characteristic map , $$ c:H^*(BG)\rightarrow H_G^*(M) $$ that endows $H_G^*(M)$ with the structure of an $H^*(BG)$-module. There's also a canonical restriction map $$ r:H_G^*(M)\rightarrow H^*(M). $$ We say that $M$ is equivariantly formal if $r$ is onto. Any hints on how to prove the following will be appreciated: Propositon: If $M$ is equivariantly formal, then $H_G^*(M)\simeq H^*(M)\otimes H^*(BG)$ as $H^*(BG)$-modules. N.B.: I am aware that there are many different ways to define equivariant formality, but I'd like to use only the given definitions, if possible.",,"['differential-geometry', 'algebraic-topology', 'equivariant-cohomology']"
93,how to calculate a vector in a left invariant vector field?,how to calculate a vector in a left invariant vector field?,,"I would like to understand the left invariant vector field by using a numerical example. Now we consider a Lie group $G=SE(3)$, and the associated Lie algebra is $\mathfrak{g}=se(3)$. We suppose: $$g=\pmatrix{1 & 0 & 0 & 1\\ 0 & 1 & 0 & 2\\0 & 0 & 1 & 3\\0 & 0 & 0 & 1}\in G$$ and $$v=\pmatrix{0 & 0 & 0 & 1\\ 0 & 0 & 0 & 2\\0 & 0 & 0 & 0\\0 & 0 & 0 &0}\in\mathfrak{g}$$ $v$ is a vector at the identity element $I$ in a left invariant vector field $X$ of the Lie Group $G$. Then my questions: 1) how to calculate the vector $v_g$ at the point $g$ in the vector field $X$? 2) Now we consider a map: $\phi:G\rightarrow G,x\rightarrow gx$, where $g$ is defined as above. Then $\quad$ i) how to calculate the vector at the identity element $I$ in the new Lie Group $\phi(G)$? (Is this equal to $v$?) $\quad$ ii) how to calculate the vector at the point $g=\phi(I)$ in the Lie Group $\phi(G)$? $\quad$ iii) how to calculate the vector at the point $\phi(g)$ in the Lie Group $\phi(G)$?","I would like to understand the left invariant vector field by using a numerical example. Now we consider a Lie group $G=SE(3)$, and the associated Lie algebra is $\mathfrak{g}=se(3)$. We suppose: $$g=\pmatrix{1 & 0 & 0 & 1\\ 0 & 1 & 0 & 2\\0 & 0 & 1 & 3\\0 & 0 & 0 & 1}\in G$$ and $$v=\pmatrix{0 & 0 & 0 & 1\\ 0 & 0 & 0 & 2\\0 & 0 & 0 & 0\\0 & 0 & 0 &0}\in\mathfrak{g}$$ $v$ is a vector at the identity element $I$ in a left invariant vector field $X$ of the Lie Group $G$. Then my questions: 1) how to calculate the vector $v_g$ at the point $g$ in the vector field $X$? 2) Now we consider a map: $\phi:G\rightarrow G,x\rightarrow gx$, where $g$ is defined as above. Then $\quad$ i) how to calculate the vector at the identity element $I$ in the new Lie Group $\phi(G)$? (Is this equal to $v$?) $\quad$ ii) how to calculate the vector at the point $g=\phi(I)$ in the Lie Group $\phi(G)$? $\quad$ iii) how to calculate the vector at the point $\phi(g)$ in the Lie Group $\phi(G)$?",,"['differential-geometry', 'lie-groups', 'lie-algebras']"
94,explicit (holomorphic) map revealing blow-up as a connected sum with $\overline{\mathbb{CP}}^n$,explicit (holomorphic) map revealing blow-up as a connected sum with,\overline{\mathbb{CP}}^n,"I am trying to understand the statement that a blow-up of a complex manifold $M$ at a point $p$ is equivalent to the connected sum of $M$ with $\overline{\mathbb{CP^n}}$ and, being a physicist, I need the explicit map between charts. I have looked at a number of sources, but found Broens' thesis particularly useful. Using this one may construct a transition function from the blow-up of $M$ , $\tilde{M}$ , to $\overline{\mathbb{CP^n}}$ . The problem I have is that it doesn't look very holomorphic. the set-up If $M$ has co-ordinates $z'$ with $z'(p)=0$ , then the blow-up $\tilde{M}$ is given by the set of points $\{ (z', [w]) \in \mathbb{C}^n \times \mathbb{CP^{n-1}} \;|\; z'_iw_j=z'_jw_i\}$ Now define a map $\psi:\overline{\mathbb{CP^n}}\to\tilde{M}$ by $\psi(\overline{Z}_0:Z_1:Z_2…:Z_n)=(Z_0Z/|Z|^2,\;[Z])$ where $Z=(Z_1,Z_2,…Z_n)\neq0$ . The inverse of this map is, I think, $\psi^{-1}(z',[z'])=(|z'|^2:z'_1:z'_2:…z'_n)$ A ball in $\mathbb{CP^n}$ about the point $q$ with co-ordinates $(1:0:0…:0)$ is defined by the set $B_\epsilon=\{ (\overline {Z}_0:Z)\;|\;\;|Z|/|Z_0|\;\leq\;1/\epsilon  \}$ which is then removed to give the set $V_\epsilon$ that will be glued on $V_\epsilon=\{ (\overline {Z}_0:Z)\;|\;\;|Z|/|Z_0|\;>\;1/\epsilon  \}$ the image of which is $\psi(V_\epsilon)=\{ (Z_0Z/|Z|^2,\;[Z])\;|\;\;|Z|/|Z_0|\;>\;1/\epsilon \}$ so if we define the co-ordinates on $M$ to be $z'=Z_0Z/|Z|^2$ we find the set of points $U_\epsilon=\psi(V_\epsilon)=\{  (z',[z'])\;|\;\;|z'|<\epsilon \}$ now make the annulus for the connected sum by constructing $U_\delta$ , $\delta<\epsilon$ , and the corresponding annulus on $V_\epsilon$ . So, given a point on the annulus in $M$ , $z'\in U_\epsilon-U_\delta$ , we get a point $(|z'|^2:z'_1:z'_2:…z'_n)=(1:z'_1/|z'|^2:z'_2/|z'|^2:…z'_n/|z'|^2)$ . on $\overline{\mathbb{CP^n}}$ with inhomogeneous co-ordinates $z'_i/|z|^2$ the problem There's a good chance that I have totally misunderstood something, but if we take $\zeta_i$ to be the inhomogeneous co-ordinates on $\overline{\mathbb{CP^n}}$ , then the above relates the charts of $M$ and $\overline{\mathbb{CP^n}}$ via $\zeta_i=z'_i/|z|^2$ which is not holomorphic. Given that, for example, the del Pezzo surfaces are blow-ups, and also complex manifolds, I would have expected a holomorphic map between charts.","I am trying to understand the statement that a blow-up of a complex manifold at a point is equivalent to the connected sum of with and, being a physicist, I need the explicit map between charts. I have looked at a number of sources, but found Broens' thesis particularly useful. Using this one may construct a transition function from the blow-up of , , to . The problem I have is that it doesn't look very holomorphic. the set-up If has co-ordinates with , then the blow-up is given by the set of points Now define a map by where . The inverse of this map is, I think, A ball in about the point with co-ordinates is defined by the set which is then removed to give the set that will be glued on the image of which is so if we define the co-ordinates on to be we find the set of points now make the annulus for the connected sum by constructing , , and the corresponding annulus on . So, given a point on the annulus in , , we get a point . on with inhomogeneous co-ordinates the problem There's a good chance that I have totally misunderstood something, but if we take to be the inhomogeneous co-ordinates on , then the above relates the charts of and via which is not holomorphic. Given that, for example, the del Pezzo surfaces are blow-ups, and also complex manifolds, I would have expected a holomorphic map between charts.","M p M \overline{\mathbb{CP^n}} M \tilde{M} \overline{\mathbb{CP^n}} M z' z'(p)=0 \tilde{M} \{ (z', [w]) \in \mathbb{C}^n \times \mathbb{CP^{n-1}} \;|\; z'_iw_j=z'_jw_i\} \psi:\overline{\mathbb{CP^n}}\to\tilde{M} \psi(\overline{Z}_0:Z_1:Z_2…:Z_n)=(Z_0Z/|Z|^2,\;[Z]) Z=(Z_1,Z_2,…Z_n)\neq0 \psi^{-1}(z',[z'])=(|z'|^2:z'_1:z'_2:…z'_n) \mathbb{CP^n} q (1:0:0…:0) B_\epsilon=\{ (\overline {Z}_0:Z)\;|\;\;|Z|/|Z_0|\;\leq\;1/\epsilon  \} V_\epsilon V_\epsilon=\{ (\overline {Z}_0:Z)\;|\;\;|Z|/|Z_0|\;>\;1/\epsilon  \} \psi(V_\epsilon)=\{ (Z_0Z/|Z|^2,\;[Z])\;|\;\;|Z|/|Z_0|\;>\;1/\epsilon \} M z'=Z_0Z/|Z|^2 U_\epsilon=\psi(V_\epsilon)=\{  (z',[z'])\;|\;\;|z'|<\epsilon \} U_\delta \delta<\epsilon V_\epsilon M z'\in U_\epsilon-U_\delta (|z'|^2:z'_1:z'_2:…z'_n)=(1:z'_1/|z'|^2:z'_2/|z'|^2:…z'_n/|z'|^2) \overline{\mathbb{CP^n}} z'_i/|z|^2 \zeta_i \overline{\mathbb{CP^n}} M \overline{\mathbb{CP^n}} \zeta_i=z'_i/|z|^2","['algebraic-geometry', 'differential-geometry', 'complex-geometry']"
95,Some questions about synthetic differential geometry,Some questions about synthetic differential geometry,,"I've been trying to read Kock's text on synthetic differential geometry but I am getting a bit confused. For example, what does it mean to ""interpret set theory in a topos""? What is a model of a theory? Why does Kock use [[ ]] rather than { } for sets? Does it serve to indicate that these sets are not ""classical""? As a side question, are there any drawbacks to synthetic differential geometry compared to the usual approach? Are there any aspects of classical differential geometry that cannot be done synthetically, or require more effort and machinery? Can physical theories like general relativity be expressed synthetically? If so, does this make it easier or more difficult to perform calculations and simulations based on the synthetic formulation? With regards to my background, I'm educated in ""classical"" differential geometry at the level of John Lee's series, I know a bit of general relativity from O'Neill, I'm familiar with elementary category theory at the level of Simmons' book, and I know the definition of a topos, but I don't know any categorical logic or model theory.","I've been trying to read Kock's text on synthetic differential geometry but I am getting a bit confused. For example, what does it mean to ""interpret set theory in a topos""? What is a model of a theory? Why does Kock use [[ ]] rather than { } for sets? Does it serve to indicate that these sets are not ""classical""? As a side question, are there any drawbacks to synthetic differential geometry compared to the usual approach? Are there any aspects of classical differential geometry that cannot be done synthetically, or require more effort and machinery? Can physical theories like general relativity be expressed synthetically? If so, does this make it easier or more difficult to perform calculations and simulations based on the synthetic formulation? With regards to my background, I'm educated in ""classical"" differential geometry at the level of John Lee's series, I know a bit of general relativity from O'Neill, I'm familiar with elementary category theory at the level of Simmons' book, and I know the definition of a topos, but I don't know any categorical logic or model theory.",,"['differential-geometry', 'category-theory', 'topos-theory', 'general-relativity', 'synthetic-differential-geometry']"
96,A compact connected solvable Lie group is a torus,A compact connected solvable Lie group is a torus,,"I am looking for the proof of the following statement. A compact connected solvable Lie group of dimension $n\geq 1$ is a torus, i.e., it isomorphic to the product of $n$ copies of $S^1$. A search with Google revealed that pages 51-52 of ""Lie Groups and Lie Algebras III: Structure of Lie Groups and Lie Algebras"" (A.L. Onishchik) deal with this problem but the proofs are not very detailed and I'm not an expert. Thanks, Gis","I am looking for the proof of the following statement. A compact connected solvable Lie group of dimension $n\geq 1$ is a torus, i.e., it isomorphic to the product of $n$ copies of $S^1$. A search with Google revealed that pages 51-52 of ""Lie Groups and Lie Algebras III: Structure of Lie Groups and Lie Algebras"" (A.L. Onishchik) deal with this problem but the proofs are not very detailed and I'm not an expert. Thanks, Gis",,"['differential-geometry', 'lie-groups', 'lie-algebras']"
97,why can i differentiate this term-by-term?,why can i differentiate this term-by-term?,,"What's the best way to justify the following computation: For $A, B$ symmetric real matrices, $$\frac{d}{dt}|_{t=0}e^{A+tB}= \frac{d}{dt}|_{t=0}(1+(A+tB)+\frac{1}{2!}(A+tB)^2+...) = (B+\frac{1}{2!}(BA+AB)+...+\frac{1}{(n+1)!}\sum _{k=0}^n A^kBA^{n-k}+...)$$ The only thing I don't understand is the how to justify term-by-term differentiation. In the case of an expression like $\frac{d}{dt}e^{tB}$ it is easy that each coordinate is a power series in t with infinite radius of convergence.  Is that what I should do here or is there an easier way?","What's the best way to justify the following computation: For $A, B$ symmetric real matrices, $$\frac{d}{dt}|_{t=0}e^{A+tB}= \frac{d}{dt}|_{t=0}(1+(A+tB)+\frac{1}{2!}(A+tB)^2+...) = (B+\frac{1}{2!}(BA+AB)+...+\frac{1}{(n+1)!}\sum _{k=0}^n A^kBA^{n-k}+...)$$ The only thing I don't understand is the how to justify term-by-term differentiation. In the case of an expression like $\frac{d}{dt}e^{tB}$ it is easy that each coordinate is a power series in t with infinite radius of convergence.  Is that what I should do here or is there an easier way?",,"['differential-geometry', 'lie-groups']"
98,Symplectomorphism Preserves Cotangent fibrations,Symplectomorphism Preserves Cotangent fibrations,,"Let $M$ be a manifold with local coordinates $x^1,\dots,x^n$ and $T^\ast M$ the cotangent bundle with induced coordinates $x^1,\dots,x^n,\xi_1,\dots,\xi_n$ . Let $\alpha$ be the tautological one form on $T^\ast M$. That is, $\alpha\in \Gamma(T(T^\ast M))$ where $\alpha=\xi_idx^i$. Let $\omega=-d\alpha=dx^i\wedge d\xi_i$. We have that $(T^\ast M,\omega)$ is a symplectic manifold. Let $g:T^\ast M\to T^\ast M$ be a symplectomorphism that preserves $\alpha$ (i.e. $g^\ast\alpha=\alpha$). I'm trying to show that if for some $(p,\sigma_p)\in T^\ast_p M$ we have $g(p,\sigma_p)=(q,\eta_q)$, then $g(T^\ast_p M)=T^\ast_q M$. So far this is what I have: Let $V\in\Gamma(T(T^\ast M))$ be the symplectic dual of $\alpha$. That is, $\omega(V,\cdot)=\alpha$. Let $\theta$ denote the flow of $V$. I can show that $g\circ\theta_t=\theta_t\circ g$ for all $t\in\mathbb{R}$ or equivalently, that $g_\ast V=V$. I can show that each $\theta_t$ preserves $T_p^\ast M$ for arbitrary $p\in M$. That is, I can show that $\theta_t(T^\ast_p M)=T^\ast_p M$. This follows from actually computing the flow, which turns out to be $\theta_t(p,\mu_p)=(p,e^t\mu_p)$. How do I use these facts to show what I wrote above? Am I missing something obvious? Any help would be greatly appreciated.","Let $M$ be a manifold with local coordinates $x^1,\dots,x^n$ and $T^\ast M$ the cotangent bundle with induced coordinates $x^1,\dots,x^n,\xi_1,\dots,\xi_n$ . Let $\alpha$ be the tautological one form on $T^\ast M$. That is, $\alpha\in \Gamma(T(T^\ast M))$ where $\alpha=\xi_idx^i$. Let $\omega=-d\alpha=dx^i\wedge d\xi_i$. We have that $(T^\ast M,\omega)$ is a symplectic manifold. Let $g:T^\ast M\to T^\ast M$ be a symplectomorphism that preserves $\alpha$ (i.e. $g^\ast\alpha=\alpha$). I'm trying to show that if for some $(p,\sigma_p)\in T^\ast_p M$ we have $g(p,\sigma_p)=(q,\eta_q)$, then $g(T^\ast_p M)=T^\ast_q M$. So far this is what I have: Let $V\in\Gamma(T(T^\ast M))$ be the symplectic dual of $\alpha$. That is, $\omega(V,\cdot)=\alpha$. Let $\theta$ denote the flow of $V$. I can show that $g\circ\theta_t=\theta_t\circ g$ for all $t\in\mathbb{R}$ or equivalently, that $g_\ast V=V$. I can show that each $\theta_t$ preserves $T_p^\ast M$ for arbitrary $p\in M$. That is, I can show that $\theta_t(T^\ast_p M)=T^\ast_p M$. This follows from actually computing the flow, which turns out to be $\theta_t(p,\mu_p)=(p,e^t\mu_p)$. How do I use these facts to show what I wrote above? Am I missing something obvious? Any help would be greatly appreciated.",,"['differential-geometry', 'symplectic-geometry']"
99,Product neighborhood theorem with boundary,Product neighborhood theorem with boundary,,"The Product Neighborhood Theorem states that if $N\subseteq M$ is a smooth compact submanifold without boundary of codimension $n$ and there is a trivialization of the normal bundle (wrt. some smooth metric) of $N$ in $M$, then some neighborhood of $N$ is diffeomorphic to $N\times \mathbb{R}^n$ s.t. the diffeomorphism takes the framing vectors to the canonical basis of $\mathbb{R}^n$. Is the same true if $N,M$ are smooth manifolds with boundary and $N$ is a neat submanifold of $M$? For the proof, in most texts I found the idea to map $(n,x)\in N\times\mathbb{R}^n$ to $\varphi(1)$ for a geodesics $\varphi$ starting in $n$ with tangent vector $\epsilon x$ for $\epsilon$ small enough. This probably doesn't work on the boundary: for example, if $M$ is the closed unit 2-disc and $N:=\{0\}\times [-1,1]$ with the horizontal framing $(1,0)$ then I don't see how to generalize the above construction. I can hardly have a geodesics in $M$ starting in $(0,1)$ in the $(1,0)$ direction. Remark: What confuses me is that the boundary-version of PNT is used in the proof of the Thom-Pontryiagin construction -- namely, that $[M,S^n]$ is isomorphic to the framed cobordism group $\Omega^{fr}_{n;M}$ (a framed cobordism has boundary) -- but in all books, I have only seen the statement of PNT for the boundaryless case.","The Product Neighborhood Theorem states that if $N\subseteq M$ is a smooth compact submanifold without boundary of codimension $n$ and there is a trivialization of the normal bundle (wrt. some smooth metric) of $N$ in $M$, then some neighborhood of $N$ is diffeomorphic to $N\times \mathbb{R}^n$ s.t. the diffeomorphism takes the framing vectors to the canonical basis of $\mathbb{R}^n$. Is the same true if $N,M$ are smooth manifolds with boundary and $N$ is a neat submanifold of $M$? For the proof, in most texts I found the idea to map $(n,x)\in N\times\mathbb{R}^n$ to $\varphi(1)$ for a geodesics $\varphi$ starting in $n$ with tangent vector $\epsilon x$ for $\epsilon$ small enough. This probably doesn't work on the boundary: for example, if $M$ is the closed unit 2-disc and $N:=\{0\}\times [-1,1]$ with the horizontal framing $(1,0)$ then I don't see how to generalize the above construction. I can hardly have a geodesics in $M$ starting in $(0,1)$ in the $(1,0)$ direction. Remark: What confuses me is that the boundary-version of PNT is used in the proof of the Thom-Pontryiagin construction -- namely, that $[M,S^n]$ is isomorphic to the framed cobordism group $\Omega^{fr}_{n;M}$ (a framed cobordism has boundary) -- but in all books, I have only seen the statement of PNT for the boundaryless case.",,"['differential-geometry', 'differential-topology', 'riemannian-geometry']"
