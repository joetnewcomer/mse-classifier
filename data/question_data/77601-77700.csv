,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Is there any matrix notation the summation of a vector?,Is there any matrix notation the summation of a vector?,,"I have a simple question please. I want a mathematical notation to express sum(A[i]) where A is a vector (one dimensional matrix). I want to rewrite a simple formula in matrix form (compact form) and need your help. For example, if you want to rewrite $\sum_i x^2[i]$, it may be simply denoted by $X^T X$. Thanks","I have a simple question please. I want a mathematical notation to express sum(A[i]) where A is a vector (one dimensional matrix). I want to rewrite a simple formula in matrix form (compact form) and need your help. For example, if you want to rewrite $\sum_i x^2[i]$, it may be simply denoted by $X^T X$. Thanks",,['matrices']
1,What is the geometrical action of a skew-symmetric matrix on an arbitrary vector?,What is the geometrical action of a skew-symmetric matrix on an arbitrary vector?,,"What is the geometrical action of a skew-symmetric matrix on an arbitrary vector? The rotation matrix is a skew-symmetric matrix when $\theta$ is some multiple of $\frac{\pi}{2}$. But it cannot be true that every skew-symmetric matrix represents a rotation? Also, since the leading diagonal is zero, it cannot represent a scaling nor a shear. In fact, none of the standard transformation matrices on Wikipedia seem to fit the pattern of an arbitrary skew-symmetric matrix. So can anything be said about the geometrical action of a skew-symmetric matrix on an arbitrary vector?","What is the geometrical action of a skew-symmetric matrix on an arbitrary vector? The rotation matrix is a skew-symmetric matrix when $\theta$ is some multiple of $\frac{\pi}{2}$. But it cannot be true that every skew-symmetric matrix represents a rotation? Also, since the leading diagonal is zero, it cannot represent a scaling nor a shear. In fact, none of the standard transformation matrices on Wikipedia seem to fit the pattern of an arbitrary skew-symmetric matrix. So can anything be said about the geometrical action of a skew-symmetric matrix on an arbitrary vector?",,"['geometry', 'matrices']"
2,"To ""subtract"" two matrices with different dimensions in Octave (Matlab)","To ""subtract"" two matrices with different dimensions in Octave (Matlab)",,"I have matrix and need to subtract another matrix element by element on each row. Something like this: $$ \begin{pmatrix} x_{1} &  x_{2}\\ x_{3} &  x_{4}\\ \vdots & \vdots\\ x_{n-1} &  x_{n}\\ \end{pmatrix} - \begin{pmatrix} y_{1} &  y_{2}\\ \end{pmatrix} $$ So end result should be something like: $$ \begin{pmatrix} x_{1} - y_{1} &  x_{2} - y_{2}\\ x_{3} - y_{1} &  x_{4} - y_{2}\\ \vdots & \vdots\\ x_{n-1} - y_{1} &  x_{n} - y_{2}\\ \end{pmatrix} $$ How to do this? How to do this in Octave, Matlab? Sorry for noob question. Also would be very kind if you pint me where to read about this.","I have matrix and need to subtract another matrix element by element on each row. Something like this: $$ \begin{pmatrix} x_{1} &  x_{2}\\ x_{3} &  x_{4}\\ \vdots & \vdots\\ x_{n-1} &  x_{n}\\ \end{pmatrix} - \begin{pmatrix} y_{1} &  y_{2}\\ \end{pmatrix} $$ So end result should be something like: $$ \begin{pmatrix} x_{1} - y_{1} &  x_{2} - y_{2}\\ x_{3} - y_{1} &  x_{4} - y_{2}\\ \vdots & \vdots\\ x_{n-1} - y_{1} &  x_{n} - y_{2}\\ \end{pmatrix} $$ How to do this? How to do this in Octave, Matlab? Sorry for noob question. Also would be very kind if you pint me where to read about this.",,"['matrices', 'matlab']"
3,Is it possible to link the eigenvalues of a matrix to the Fourier transform of the matrix?,Is it possible to link the eigenvalues of a matrix to the Fourier transform of the matrix?,,"I'm trying to get insight into the eigenvalue spectrum of a square matrix (large N, symmetric, positive semi-definite matrix) using Fourier transforms (I've tried transforming a bunch of things: the diagonal, the skew diagonal, the entire matrix, each eigenvector, etc. . Is there anything I should be looking at specifically?","I'm trying to get insight into the eigenvalue spectrum of a square matrix (large N, symmetric, positive semi-definite matrix) using Fourier transforms (I've tried transforming a bunch of things: the diagonal, the skew diagonal, the entire matrix, each eigenvector, etc. . Is there anything I should be looking at specifically?",,['matrices']
4,Why does the space of SPD matrices form a differentiable manifold?,Why does the space of SPD matrices form a differentiable manifold?,,"Disclaimer: I am not a mathematician, just a young neuroscientist trying to understand a paper. So, please forgive me if I have horribly misunderstood something. In order to understand this paper $^\color{magenta}{\star}$ that involves using a Riemannian kernel for a support vector machine, I am trying to understand some of the maths behind part of the methods. I am still not entirely sure what a differentiable manifold is, despite having watched a number of video lectures on the topic (It's a topological space which is locally homomorphic to Euclidean space - with continous transfer functions?). But more importantly: the paper states that the space of symmetric positive definite matrices forms a differentiable manifold. This seems important as the we need to calculate the distances between covariance matrices (which by definition live in the SPD space) in order for the SVM to perform well. So, to me, it seems that the author's of the paper achieved impressive SVM classification results because their similarity (or distance) measure was more suited to the space in which they were working. ie a Riemannian kernel in a differentiable manifold.  Is it just a happy coincidence that the space of SPD matrices forms this.. mathematical object within which we can perform better similarity measures? I suppose my entire confusion can be summed up as: Why does a collection of a specific type of matrix (SPD) form this other topological space with specific properties that make it more efficient for measuring distances between points (matrices) in the space? $\color{magenta}{\star}$ Alexandre Barachant, Stéphane Bonnet, Marco Congedo, Christian Jutten, Classification of covariance matrices using a Riemannian-based kernel for BCI applications [ hal-00820475 ], Neurocomputing, Volume 112, July 2013.","Disclaimer: I am not a mathematician, just a young neuroscientist trying to understand a paper. So, please forgive me if I have horribly misunderstood something. In order to understand this paper that involves using a Riemannian kernel for a support vector machine, I am trying to understand some of the maths behind part of the methods. I am still not entirely sure what a differentiable manifold is, despite having watched a number of video lectures on the topic (It's a topological space which is locally homomorphic to Euclidean space - with continous transfer functions?). But more importantly: the paper states that the space of symmetric positive definite matrices forms a differentiable manifold. This seems important as the we need to calculate the distances between covariance matrices (which by definition live in the SPD space) in order for the SVM to perform well. So, to me, it seems that the author's of the paper achieved impressive SVM classification results because their similarity (or distance) measure was more suited to the space in which they were working. ie a Riemannian kernel in a differentiable manifold.  Is it just a happy coincidence that the space of SPD matrices forms this.. mathematical object within which we can perform better similarity measures? I suppose my entire confusion can be summed up as: Why does a collection of a specific type of matrix (SPD) form this other topological space with specific properties that make it more efficient for measuring distances between points (matrices) in the space? Alexandre Barachant, Stéphane Bonnet, Marco Congedo, Christian Jutten, Classification of covariance matrices using a Riemannian-based kernel for BCI applications [ hal-00820475 ], Neurocomputing, Volume 112, July 2013.",^\color{magenta}{\star} \color{magenta}{\star},"['matrices', 'manifolds', 'riemannian-geometry', 'symmetric-matrices', 'positive-definite']"
5,What is a Dominant Eigenvalue?,What is a Dominant Eigenvalue?,,I know this is a rather simple questions. I've searched for a while and from what i gather about the dominant Eiganvalue. If i have $\lambda_1$ = 1 $\lambda_2$ = 3 and $\lambda_3$=5 (hypothetically) would 5 be the dominant Eiganvalue? And from that can i assume that given any set of eigenvalues the one with highest weight is always the Dominant?,I know this is a rather simple questions. I've searched for a while and from what i gather about the dominant Eiganvalue. If i have $\lambda_1$ = 1 $\lambda_2$ = 3 and $\lambda_3$=5 (hypothetically) would 5 be the dominant Eiganvalue? And from that can i assume that given any set of eigenvalues the one with highest weight is always the Dominant?,,"['matrices', 'eigenvalues-eigenvectors']"
6,Determinants of triangular matrices,Determinants of triangular matrices,,Does a lower triangular matrix have a determinant that is equal to the product of the elements in the diagonal similar to an upper triangular matrix.,Does a lower triangular matrix have a determinant that is equal to the product of the elements in the diagonal similar to an upper triangular matrix.,,"['matrices', 'determinant']"
7,'H' symbol as a power of matrix,'H' symbol as a power of matrix,,Here is capture of problem That is what I saw while I was studying maximum ratio combining (MRC) in communication. It is probably very simple and stupid thing to ask.  I guess it is Hermitian of a matrix but i am not sure.. Here is entire content: http://www.dsplog.com/2008/09/28/maximal-ratio-combining/,Here is capture of problem That is what I saw while I was studying maximum ratio combining (MRC) in communication. It is probably very simple and stupid thing to ask.  I guess it is Hermitian of a matrix but i am not sure.. Here is entire content: http://www.dsplog.com/2008/09/28/maximal-ratio-combining/,,"['matrices', 'notation']"
8,Square root of a specific 3x3 matrix,Square root of a specific 3x3 matrix,,"From a problem set I'm working on: (Edit 04/11 - I fudged a sign in my matrix...) Let $A(t) \in M_3(\mathbb{R})$ be defined: $$ A(t) =  \left( \begin{array}{crc} 1 & 2 & 0 \\  0 & -1 & 0 \\ t-1 & -2 & t \end{array}  \right).$$ For which $t$ does there exist a $B \in M_3(\mathbb{R})$   such that $B^2 = A$? In a previous part of the problem, I showed that $A(t)$ could be diagonalized into a real diagonal matrix for all $t \in \mathbb{R}$, with eigenvalues $1,-1,t$. A few things I've thought of: The matrix is not positive-semidefinite, so the general form of the square root does not work. (Is positive-definiteness a necessary condition for the existence of a square root?) Since $A = B^2$, then $\det(B^2) = (\det B)^2 = \det A$. So $\det A \geq 0$ for there to be a real-valued square root, forcing $t \leq 0$ to be necessary. My professor suggested that, since $B^2$ fits the characteristic polynomial of $A$, $\mu_A(x) = (x-1)(x+1)(x-a)$, then the minimal polynomial of $B$ must divide $\mu_A(x^2) = (x^2-1)(x^2+1)(x^2-a) = (x-1)(x+1)(x^2+1)(x^2-a)$. Examining the possible minimal polynomials, one can find the rational canonical form, square it, and check whether the eigenvalues match. This probably could get me the right answer, but I am fairly sure that there is an alternative to a ""proof by exhaustion"".","From a problem set I'm working on: (Edit 04/11 - I fudged a sign in my matrix...) Let $A(t) \in M_3(\mathbb{R})$ be defined: $$ A(t) =  \left( \begin{array}{crc} 1 & 2 & 0 \\  0 & -1 & 0 \\ t-1 & -2 & t \end{array}  \right).$$ For which $t$ does there exist a $B \in M_3(\mathbb{R})$   such that $B^2 = A$? In a previous part of the problem, I showed that $A(t)$ could be diagonalized into a real diagonal matrix for all $t \in \mathbb{R}$, with eigenvalues $1,-1,t$. A few things I've thought of: The matrix is not positive-semidefinite, so the general form of the square root does not work. (Is positive-definiteness a necessary condition for the existence of a square root?) Since $A = B^2$, then $\det(B^2) = (\det B)^2 = \det A$. So $\det A \geq 0$ for there to be a real-valued square root, forcing $t \leq 0$ to be necessary. My professor suggested that, since $B^2$ fits the characteristic polynomial of $A$, $\mu_A(x) = (x-1)(x+1)(x-a)$, then the minimal polynomial of $B$ must divide $\mu_A(x^2) = (x^2-1)(x^2+1)(x^2-a) = (x-1)(x+1)(x^2+1)(x^2-a)$. Examining the possible minimal polynomials, one can find the rational canonical form, square it, and check whether the eigenvalues match. This probably could get me the right answer, but I am fairly sure that there is an alternative to a ""proof by exhaustion"".",,['matrices']
9,Matrices with a more general indexing than by integers,Matrices with a more general indexing than by integers,,"A matrix is usually understood as an rectangular array of objects (for the most part numbers) arranged in rows and columns. If the objects belong to a set $\mathcal S$ , then one can write $M(m,n;\mathcal S)$ for the set of $(m \times n)$ -matrices $A$ with entries in $\mathcal S$ . Such a matrix $A$ has $m$ rows and $n$ columns. The entry of $A$ occurring in row $i$ and column $j$ is often denoted as $a_{ij}$ and one writes $A = (a_{ij})$ . In linear algebra matrices are used to represent linear maps $f : V \to W$ between finite-dimensional vector spaces $V,W$ with respect to bases $\mathfrak V  =\{v_1,\ldots, v_n\}$ of $V$ and $\mathfrak W  =\{w_1,\ldots, w_m\}$ of $W$ . We have $f(v_j) = \sum_{i=1}^m a_{ij}w_i$ with unique $a_{ij}$ and therefore $f(\sum_{j=1}^n\lambda_jv_j) =  \sum_{j=1}^n \lambda_jf(v_j) = \sum_{i,j} \lambda_j a_{ij}w_i$ . The $(m\times n)$ -matrix $(a_{ij})$ is the matrix representation of $f$ with respect to $\mathfrak V, \mathfrak W$ . For such matrix representations we work only with bases indexed by sets of the special form $I_k = \{1,\ldots,k\}$ . Here is my question: Wouldn't it be more flexible to allow arbitrary finite index sets ? That is, to consider matrix sets of the form $M(I, J; \mathcal S) = \mathcal S^{I \times J}$ with arbitrary finite sets $I, J$ . A matrix $A \in M(I, J; \mathcal S)$ is then an indexed collection $A    =(a_{(i,j)}) \in \mathcal S^{I \times J}$ . We can still regard it as a rectangular array of objects of $\mathcal S$ arranged in rows and columns, although these do not have integer row numbers and column numbers . Of course this concept is not a big innovation. But does is occur somewhere in the literature? This question was motivated by the answers to Chain rule for differentiation yields conflicting dimensions . The question deals with a matrix valued function $f : \mathbb R \to M(n,n;\mathbb R)$ . Clearly $M(n,n;\mathbb R)$ is isomorphic to $\mathbb R^{n^2}$ , but it does not have a canonical basis indexed by $\{1\ldots,n^2\}$ . Instead it has a natural base consisting of the matrices $(E_{ij}) \in M(n,n;\mathbb R)$ where the $E_{ij}$ have an entry $1$ in row $i$ and column $j$ , all other entries being $0$ . The index set of this natural basis is $I_{m,n} = \{1,\ldots,m\} \times \{1,\ldots,n\}$ . When considering the derivative $Df \mid_x$ at $x \in \mathbb R$ , which naturally is a linear map $\mathbb R \to M(n,n;\mathbb R)$ , the mentioned answers are wavering around by saying that we have to ""flatten matrices"" or to ""identify $M(n,n;\mathbb R)$ with $\mathbb R^{n^2}$ "" to get a matrix in $M(n^2,1;\mathbb R)$ . One can do this, but I think it is uncessary and may even cause confusion (the OP of the above question seems to mix up $M(n^2,1;\mathbb R)$ with $M(n,n;\mathbb R)$ when flattening $Df\mid_x)$ ). In my opinion it is much more transparent to say that the Jacobian of $f$ at $x$ is a matrix in $M(I_{m,n},1;\mathbb R)$ .","A matrix is usually understood as an rectangular array of objects (for the most part numbers) arranged in rows and columns. If the objects belong to a set , then one can write for the set of -matrices with entries in . Such a matrix has rows and columns. The entry of occurring in row and column is often denoted as and one writes . In linear algebra matrices are used to represent linear maps between finite-dimensional vector spaces with respect to bases of and of . We have with unique and therefore . The -matrix is the matrix representation of with respect to . For such matrix representations we work only with bases indexed by sets of the special form . Here is my question: Wouldn't it be more flexible to allow arbitrary finite index sets ? That is, to consider matrix sets of the form with arbitrary finite sets . A matrix is then an indexed collection . We can still regard it as a rectangular array of objects of arranged in rows and columns, although these do not have integer row numbers and column numbers . Of course this concept is not a big innovation. But does is occur somewhere in the literature? This question was motivated by the answers to Chain rule for differentiation yields conflicting dimensions . The question deals with a matrix valued function . Clearly is isomorphic to , but it does not have a canonical basis indexed by . Instead it has a natural base consisting of the matrices where the have an entry in row and column , all other entries being . The index set of this natural basis is . When considering the derivative at , which naturally is a linear map , the mentioned answers are wavering around by saying that we have to ""flatten matrices"" or to ""identify with "" to get a matrix in . One can do this, but I think it is uncessary and may even cause confusion (the OP of the above question seems to mix up with when flattening ). In my opinion it is much more transparent to say that the Jacobian of at is a matrix in .","\mathcal S M(m,n;\mathcal S) (m \times n) A \mathcal S A m n A i j a_{ij} A = (a_{ij}) f : V \to W V,W \mathfrak V  =\{v_1,\ldots, v_n\} V \mathfrak W  =\{w_1,\ldots, w_m\} W f(v_j) = \sum_{i=1}^m a_{ij}w_i a_{ij} f(\sum_{j=1}^n\lambda_jv_j) =  \sum_{j=1}^n \lambda_jf(v_j) = \sum_{i,j} \lambda_j a_{ij}w_i (m\times n) (a_{ij}) f \mathfrak V, \mathfrak W I_k = \{1,\ldots,k\} M(I, J; \mathcal S) = \mathcal S^{I \times J} I, J A \in M(I, J; \mathcal S) A 
  =(a_{(i,j)}) \in \mathcal S^{I \times J} \mathcal S f : \mathbb R \to M(n,n;\mathbb R) M(n,n;\mathbb R) \mathbb R^{n^2} \{1\ldots,n^2\} (E_{ij}) \in M(n,n;\mathbb R) E_{ij} 1 i j 0 I_{m,n} = \{1,\ldots,m\} \times \{1,\ldots,n\} Df \mid_x x \in \mathbb R \mathbb R \to M(n,n;\mathbb R) M(n,n;\mathbb R) \mathbb R^{n^2} M(n^2,1;\mathbb R) M(n^2,1;\mathbb R) M(n,n;\mathbb R) Df\mid_x) f x M(I_{m,n},1;\mathbb R)","['matrices', 'reference-request']"
10,Gradient of $A \mapsto \sigma_i (A)$,Gradient of,A \mapsto \sigma_i (A),"Let $ A $ be an $m \times n$ matrix of rank $ k \le \min(m,n) $ . Then we decompose $ A = USV^T $ , where: $U$ is $m \times k$ is a semi-orthogonal matrix. $S$ is $k \times k$ diagonal matrix , of which its diagonal entries are called singular values of $ A $ . we denote them by $ \sigma _i = S_{ii} $ . $V$ is $n \times k$ semi-orthogonal matrix. Definition: a semi-orthogonal matrix $ Q $ is a non-square matrix where $ Q^{T}Q=I $ . This is the singular value decomposition (SVD) of matrix $ A $ . We define a function $ f_i: \mathbb R^{ m \times n} \to \mathbb R $ by $ f_i (A) = \sigma_i (A) $ . I am interested in finding the gradient of $ f_i $ in order to practice matrix defferentiation. I hope you can help me starting with the first steps. Here are the hints that I have been given in order to find the solution, and feel free to use them: Use the product rule of differentials to calculate $ dA $ where A is considered as function of $ U $ , $ S $ and $ V $ . The entries of the diagonal of anti-symmetric matrix are all zeros. The Hadamard product of two matrices $ A,B $ of the same size , is denoted by $$ (A \circ B )_{ij} = A_{ij} \cdot B_{ij} $$ Use the cyclic property of the trace operator. That is: $$\mbox{Tr}(ABC) = \mbox{Tr}(CAB) = \mbox{Tr}(BCA)$$ The trace of a scalar is a scalar. That is, given $ a \in \mathbb R $ : $$ \mbox{Tr}(a) = a $$ I stuck right at the beginning, I found that the product rule is: $$ dA = dUSV^{T} +  UdSV^{T}  +  USdV^{T}  $$ Also, I have tried to calculate $ A^{T}A $ as trying to find a useful manipulation where I can use it for the solution, and I got that it is equal to: $ VS^{T} SV^{T} $ . First of all, is this what they meant by the product rule? And, second, how do I continue from here?","Let be an matrix of rank . Then we decompose , where: is is a semi-orthogonal matrix. is diagonal matrix , of which its diagonal entries are called singular values of . we denote them by . is semi-orthogonal matrix. Definition: a semi-orthogonal matrix is a non-square matrix where . This is the singular value decomposition (SVD) of matrix . We define a function by . I am interested in finding the gradient of in order to practice matrix defferentiation. I hope you can help me starting with the first steps. Here are the hints that I have been given in order to find the solution, and feel free to use them: Use the product rule of differentials to calculate where A is considered as function of , and . The entries of the diagonal of anti-symmetric matrix are all zeros. The Hadamard product of two matrices of the same size , is denoted by Use the cyclic property of the trace operator. That is: The trace of a scalar is a scalar. That is, given : I stuck right at the beginning, I found that the product rule is: Also, I have tried to calculate as trying to find a useful manipulation where I can use it for the solution, and I got that it is equal to: . First of all, is this what they meant by the product rule? And, second, how do I continue from here?"," A  m \times n  k \le \min(m,n)   A = USV^T  U m \times k S k \times k  A   \sigma _i = S_{ii}  V n \times k  Q   Q^{T}Q=I   A   f_i: \mathbb R^{ m \times n} \to \mathbb R   f_i (A) = \sigma_i (A)   f_i   dA   U   S   V   A,B   (A \circ B )_{ij} = A_{ij} \cdot B_{ij}  \mbox{Tr}(ABC) = \mbox{Tr}(CAB) = \mbox{Tr}(BCA)  a \in \mathbb R   \mbox{Tr}(a) = a   dA = dUSV^{T} +  UdSV^{T}  +  USdV^{T}    A^{T}A   VS^{T} SV^{T} ","['matrices', 'derivatives', 'matrix-calculus', 'singular-values', 'non-smooth-analysis']"
11,Using Chain Rule in Matrix Differentiation,Using Chain Rule in Matrix Differentiation,,"I have the following parameters and their respective dimension: $X:2\times 1$, $W_1:7\times2$, $W_2:1\times7$, $B_1: 7\times 1$ and $B_2:1\times1$, with the following formulation: $Y=W_2H+B_2$ where $H=\verb+ReLU+(W_1X+B_1)$, the rectified linear unit applied element-wise ($\verb+ReLU+(x)=\max(0,x)$). I want to compute $$\frac{\partial Y}{\partial W_1},$$ by using the chain rule. Hence, I compute $$\frac{\partial Y}{\partial W_1}=\frac{\partial Y}{\partial H}\cdot\frac{\partial H}{\partial W_1}$$ which is equal to $$W_2\cdot\frac{\partial H}{\partial W_1}.$$ My problem is computing $\frac{\partial H}{\partial W_1}$. I take out $X^T$ from this, by using chain rule, but then it doesn't match the dimesnion for multiplication. How do we go about taking the derivative of $H$ w.r.t. $W_1$, which is a $7\times 2$ matrix? I was told that the final result has dimesnion $7\times2$, but no matter how I arrange things, I can't come up with the correct result.","I have the following parameters and their respective dimension: $X:2\times 1$, $W_1:7\times2$, $W_2:1\times7$, $B_1: 7\times 1$ and $B_2:1\times1$, with the following formulation: $Y=W_2H+B_2$ where $H=\verb+ReLU+(W_1X+B_1)$, the rectified linear unit applied element-wise ($\verb+ReLU+(x)=\max(0,x)$). I want to compute $$\frac{\partial Y}{\partial W_1},$$ by using the chain rule. Hence, I compute $$\frac{\partial Y}{\partial W_1}=\frac{\partial Y}{\partial H}\cdot\frac{\partial H}{\partial W_1}$$ which is equal to $$W_2\cdot\frac{\partial H}{\partial W_1}.$$ My problem is computing $\frac{\partial H}{\partial W_1}$. I take out $X^T$ from this, by using chain rule, but then it doesn't match the dimesnion for multiplication. How do we go about taking the derivative of $H$ w.r.t. $W_1$, which is a $7\times 2$ matrix? I was told that the final result has dimesnion $7\times2$, but no matter how I arrange things, I can't come up with the correct result.",,"['matrices', 'derivatives', 'matrix-calculus', 'neural-networks']"
12,Upper bound for the sum of absolute values of the eigenvalues,Upper bound for the sum of absolute values of the eigenvalues,,"Let $\mathbf A = (a_{ij})$ be an $n\times n$ real or complex matrix with eigenvalues $\lambda_j$, for $j=1,...,n$. It is known that $\max|\lambda_j|$ is bounded above by the maximum row sum of $A$ (using entrywise absolute values).  Does the following related bound also hold? Question: Is it true that $$ \sum_{j=1}^n|\lambda_j| \le \sum_{i,j=1}^{n}|a_{ij}|, $$ where the right hand side is summed over all matrix elements$\,$? For example, this inequality is obvious for triangular matrices as well as positive definite matrices and slightly less obvious for unitary matrices (using the fact that the rows are unit vectors).  It can also be proved in other special cases but is it true in general? Thanks.","Let $\mathbf A = (a_{ij})$ be an $n\times n$ real or complex matrix with eigenvalues $\lambda_j$, for $j=1,...,n$. It is known that $\max|\lambda_j|$ is bounded above by the maximum row sum of $A$ (using entrywise absolute values).  Does the following related bound also hold? Question: Is it true that $$ \sum_{j=1}^n|\lambda_j| \le \sum_{i,j=1}^{n}|a_{ij}|, $$ where the right hand side is summed over all matrix elements$\,$? For example, this inequality is obvious for triangular matrices as well as positive definite matrices and slightly less obvious for unitary matrices (using the fact that the rows are unit vectors).  It can also be proved in other special cases but is it true in general? Thanks.",,"['matrices', 'eigenvalues-eigenvectors']"
13,"Searching two matrix A and B, such that exp(A+B)=exp(A)exp(B) but AB is not equal to BA. [duplicate]","Searching two matrix A and B, such that exp(A+B)=exp(A)exp(B) but AB is not equal to BA. [duplicate]",,This question already has answers here : $\exp(A+B)$ and Baker-Campbell-Hausdorff (3 answers) Closed 9 years ago . We know that if two matrix $A$ and $B$ commutes then $\exp(A+B)=\exp(A)\exp(B)$. I am trying to find two matrix that does not commute but $\exp(A+B)=\exp(A)\exp(B)$ is true for them. Can anybody give exact example. Thanks,This question already has answers here : $\exp(A+B)$ and Baker-Campbell-Hausdorff (3 answers) Closed 9 years ago . We know that if two matrix $A$ and $B$ commutes then $\exp(A+B)=\exp(A)\exp(B)$. I am trying to find two matrix that does not commute but $\exp(A+B)=\exp(A)\exp(B)$ is true for them. Can anybody give exact example. Thanks,,"['matrices', 'exponential-function', 'functional-calculus']"
14,Show that $(AB-BA)^2=O_2$,Show that,(AB-BA)^2=O_2,"Let $A,B$ be two $2 \times 2 $ matrices with real elements and $a,b \in \Bbb R$ such that $a^2 \ne b^2$ and $A(A-aB)+B(B-bA)=O_2$. Show that $(AB-BA)^2=O_2.$ MY TRY: Using Hamilton-Cayley Theorem, the problem reduces to showing that $\det (AB-BA)=0$ because the trace of $AB-BA=0$. This is where I am stuck.","Let $A,B$ be two $2 \times 2 $ matrices with real elements and $a,b \in \Bbb R$ such that $a^2 \ne b^2$ and $A(A-aB)+B(B-bA)=O_2$. Show that $(AB-BA)^2=O_2.$ MY TRY: Using Hamilton-Cayley Theorem, the problem reduces to showing that $\det (AB-BA)=0$ because the trace of $AB-BA=0$. This is where I am stuck.",,"['matrices', 'determinant']"
15,Is there a symbol for the antidiagonal matrix that has 1 as every entry?,Is there a symbol for the antidiagonal matrix that has 1 as every entry?,,"Is there a symbol for the antidiagonal matrix that has 1 as every entry? For example, in two dimensions  \begin{equation} \left( \begin{array}{cc} 0 & 1 \\ 1 & 0 \end{array} \right) \end{equation}","Is there a symbol for the antidiagonal matrix that has 1 as every entry? For example, in two dimensions  \begin{equation} \left( \begin{array}{cc} 0 & 1 \\ 1 & 0 \end{array} \right) \end{equation}",,"['matrices', 'notation']"
16,How to prove the cyclic property of the trace?,How to prove the cyclic property of the trace?,,"I need to prove that in general case (for every possible combination of square matrices) trace of the product of said matrices stays the same after some permutation iff that permutation is cyclic. I can prove it one way: $\text{tr}(A_1A_2 \ldots A_{n-1}A_n) = \text{tr}(A_nA_1A_2 \ldots A_{n-1})$, but I'm having problems with proving the other part: ""if the trace didn't change, then the permutation was cyclic"". If someone could at least point me in the right direction, I would be very grateful.","I need to prove that in general case (for every possible combination of square matrices) trace of the product of said matrices stays the same after some permutation iff that permutation is cyclic. I can prove it one way: $\text{tr}(A_1A_2 \ldots A_{n-1}A_n) = \text{tr}(A_nA_1A_2 \ldots A_{n-1})$, but I'm having problems with proving the other part: ""if the trace didn't change, then the permutation was cyclic"". If someone could at least point me in the right direction, I would be very grateful.",,"['matrices', 'trace']"
17,How does Maurer-Cartan form work,How does Maurer-Cartan form work,,"I have seen similar post asking for interpretation of the Maurer-Cartan form, but I am still struggling to understand it, so let me try to work a specific example and pose a specific question. Let $G$ be the group containing all matrices of the form \begin{bmatrix} x &y  \\  0 &1/x \end{bmatrix} where $x \neq 0$. I read that the Maurer-Cartan form is defined as $\Omega = g^{-1} dg$ so I computed $\Omega= \begin{bmatrix} 1/x &-y  \\  0 &x \end{bmatrix} \begin{bmatrix} dx &dy  \\  0 &-dx/x^2 \end{bmatrix}= \begin{bmatrix} dx/x &ydx/x^2+dy/x  \\  0 &-dx/x \end{bmatrix}$ So, for example, at the point  $g= \begin{bmatrix} 5 &3  \\  0 &1/5 \end{bmatrix} $ we have  $\Omega_g =  \begin{bmatrix} dx/5 &3dx/25+dy/5  \\  0 &-dx/5 \end{bmatrix} $ But I read that at a specific point $g\in G$, the entity $\Omega_g$ is supposed to be a linear map from $T_gG$ to $T_eG$. Looking at what I computed above, I don't see how $\Omega_g$ is a linear map on $T_gG$ . For example, what is $\Omega_g (\frac{\partial}{\partial x})$? I'm also interested in the claim that $\Omega$ is left-invariant. I am able to show that the 1-forms appearing as entries of $\Omega$ are each left-invariant via explicit computation. --Proof: However, I have seen the following ""direct"" proof, applicable in general, which appears similar in spirit: --Proof: However, because $dh$ is not a 1-form in the usual sense, and we are not really working in some coordinates, I am not sure how I am supposed to interpret this ""proof"" besides recognizing the analogy. Why is the above proof true? As for background, you may assume I understand Lee's Smooth Manifold text (2nd edition) chapters 1-14. Lee's treatment of 1-form is always real-valued, so this different kind of 1-form is throwing me off.","I have seen similar post asking for interpretation of the Maurer-Cartan form, but I am still struggling to understand it, so let me try to work a specific example and pose a specific question. Let $G$ be the group containing all matrices of the form \begin{bmatrix} x &y  \\  0 &1/x \end{bmatrix} where $x \neq 0$. I read that the Maurer-Cartan form is defined as $\Omega = g^{-1} dg$ so I computed $\Omega= \begin{bmatrix} 1/x &-y  \\  0 &x \end{bmatrix} \begin{bmatrix} dx &dy  \\  0 &-dx/x^2 \end{bmatrix}= \begin{bmatrix} dx/x &ydx/x^2+dy/x  \\  0 &-dx/x \end{bmatrix}$ So, for example, at the point  $g= \begin{bmatrix} 5 &3  \\  0 &1/5 \end{bmatrix} $ we have  $\Omega_g =  \begin{bmatrix} dx/5 &3dx/25+dy/5  \\  0 &-dx/5 \end{bmatrix} $ But I read that at a specific point $g\in G$, the entity $\Omega_g$ is supposed to be a linear map from $T_gG$ to $T_eG$. Looking at what I computed above, I don't see how $\Omega_g$ is a linear map on $T_gG$ . For example, what is $\Omega_g (\frac{\partial}{\partial x})$? I'm also interested in the claim that $\Omega$ is left-invariant. I am able to show that the 1-forms appearing as entries of $\Omega$ are each left-invariant via explicit computation. --Proof: However, I have seen the following ""direct"" proof, applicable in general, which appears similar in spirit: --Proof: However, because $dh$ is not a 1-form in the usual sense, and we are not really working in some coordinates, I am not sure how I am supposed to interpret this ""proof"" besides recognizing the analogy. Why is the above proof true? As for background, you may assume I understand Lee's Smooth Manifold text (2nd edition) chapters 1-14. Lee's treatment of 1-form is always real-valued, so this different kind of 1-form is throwing me off.",,"['matrices', 'differential-geometry', 'lie-groups', 'lie-algebras']"
18,Existence of some type matrix,Existence of some type matrix,,"Is there square matrix $A$ of size $3$ with real entries such that  $$ \operatorname{tr}(A)=0\text{ and }A^2+A^T=I. $$  I have proved that there is not with size $2$ using definition of  ""trace"", but for size $3$ it becomes complicated. Here is the sketch of the proof for $2$. $$ a_{11}+a_{22}=0,\\ a_{11}^2+a_{12}a_{21}+a_{11}=1,\\ a_{11}a_{12}+a_{12}a_{22}+a_{21}=0,\\ a_{21}a_{11}+a_{21}a_{22}+a_{12}=0,\\ a_{12}a_{21}+a_{22}^2+a_{22}=1. $$ Putting $a_{11}=-a_{22}$ it is easy to see that above inequalities can't be true together. Thanks!","Is there square matrix $A$ of size $3$ with real entries such that  $$ \operatorname{tr}(A)=0\text{ and }A^2+A^T=I. $$  I have proved that there is not with size $2$ using definition of  ""trace"", but for size $3$ it becomes complicated. Here is the sketch of the proof for $2$. $$ a_{11}+a_{22}=0,\\ a_{11}^2+a_{12}a_{21}+a_{11}=1,\\ a_{11}a_{12}+a_{12}a_{22}+a_{21}=0,\\ a_{21}a_{11}+a_{21}a_{22}+a_{12}=0,\\ a_{12}a_{21}+a_{22}^2+a_{22}=1. $$ Putting $a_{11}=-a_{22}$ it is easy to see that above inequalities can't be true together. Thanks!",,"['matrices', 'matrix-equations']"
19,"Exact meaning of ""Not every matrix is a tensor"".","Exact meaning of ""Not every matrix is a tensor"".",,"I've recently begun reading about tensors and am trying to understand the second order variety in the context of euclidean $\mathbb{R}^n$ with orthonormal basis {$e_1, e_2,\ldots, e_n$}.  This seems like a simple starting point that leaves topics like covariance and contravariance for another day.  My question is w.r.t. this limited context. Concerning the relationship between n$\times n$ matrices and second order tensors, I've read that ""not every matrix is a tensor"" and I'm trying to find a concise statement of which n$\times n$ matrices are second order tensors. Is it true that every n$\times n$ matrix $M$ is a tensor if and only if: $$M_{ij}=S_{ij}e_i\otimes e_j$$ and $S_{ij} \in \mathbb{R}$ If ""yes"", is it true that there aren't any other constraints on $S$ in order for $M$ to be considered a tensor?","I've recently begun reading about tensors and am trying to understand the second order variety in the context of euclidean $\mathbb{R}^n$ with orthonormal basis {$e_1, e_2,\ldots, e_n$}.  This seems like a simple starting point that leaves topics like covariance and contravariance for another day.  My question is w.r.t. this limited context. Concerning the relationship between n$\times n$ matrices and second order tensors, I've read that ""not every matrix is a tensor"" and I'm trying to find a concise statement of which n$\times n$ matrices are second order tensors. Is it true that every n$\times n$ matrix $M$ is a tensor if and only if: $$M_{ij}=S_{ij}e_i\otimes e_j$$ and $S_{ij} \in \mathbb{R}$ If ""yes"", is it true that there aren't any other constraints on $S$ in order for $M$ to be considered a tensor?",,"['matrices', 'tensors']"
20,Geometric interpretation of the Kronecker product?,Geometric interpretation of the Kronecker product?,,Is there any geometric interpretation of the Kronecker product of two $2\times 2$ matrices?,Is there any geometric interpretation of the Kronecker product of two $2\times 2$ matrices?,,"['geometry', 'matrices']"
21,Relationship between two Cauchy interlacing formulas,Relationship between two Cauchy interlacing formulas,,Let $A_n$ be an $n \times n$ Hermitian matrix and let $A_{n-1}$ be the $(n-1) \times (n-1)$ top-left minor.  The Cauchy Interlacing Formula says that the eigenvalues of $A_{n-1}$ interlace those of $A_n$.  (See Exercise 14 at https://terrytao.wordpress.com/2010/01/12/254a-notes-3a-eigenvalues-and-sums-of-hermitian-matrices/ ) The Cauchy Interlacing Formula for rank-one updates (See Theorem 6.7 at https://math.berkeley.edu/~nikhil/courses/270/lec6.pdf ) says that the eigenvalues of a Hermitian matrix $A$ interlace those of $A + v v^*$ for any vector $v$. Is there any relationship between these two theorems?,Let $A_n$ be an $n \times n$ Hermitian matrix and let $A_{n-1}$ be the $(n-1) \times (n-1)$ top-left minor.  The Cauchy Interlacing Formula says that the eigenvalues of $A_{n-1}$ interlace those of $A_n$.  (See Exercise 14 at https://terrytao.wordpress.com/2010/01/12/254a-notes-3a-eigenvalues-and-sums-of-hermitian-matrices/ ) The Cauchy Interlacing Formula for rank-one updates (See Theorem 6.7 at https://math.berkeley.edu/~nikhil/courses/270/lec6.pdf ) says that the eigenvalues of a Hermitian matrix $A$ interlace those of $A + v v^*$ for any vector $v$. Is there any relationship between these two theorems?,,"['matrices', 'symmetric-matrices']"
22,Fast algorithm for approximating eigenvalue distribution of large sparse matrix,Fast algorithm for approximating eigenvalue distribution of large sparse matrix,,"I am interested in the eigenvalue distribution of a huge $2^{16} \times 2^{16}$ Hermitian sparse matrix with spectrum contained in $[-1,1]$ . That is I don't need to know all eigenvalues exactly, but rather the approximate number of eigenvalues in, say, the intervals $[-1,-0.99],\dots,[0.99,1]$ . The Matlab command eig fails since it doesn't accept sparse matrices and the matrix is too big for being stored as a normal matrix. The command eigs doesn't help me very much since it only gives me the $k$ biggest eigenvalues and takes forever. Are the fast approximate algorithms for approximating the spectral density?","I am interested in the eigenvalue distribution of a huge Hermitian sparse matrix with spectrum contained in . That is I don't need to know all eigenvalues exactly, but rather the approximate number of eigenvalues in, say, the intervals . The Matlab command eig fails since it doesn't accept sparse matrices and the matrix is too big for being stored as a normal matrix. The command eigs doesn't help me very much since it only gives me the biggest eigenvalues and takes forever. Are the fast approximate algorithms for approximating the spectral density?","2^{16} \times 2^{16} [-1,1] [-1,-0.99],\dots,[0.99,1] k","['matrices', 'eigenvalues-eigenvectors', 'matlab', 'numerical-linear-algebra', 'sparse-matrices']"
23,How to tell if two matrices are similar? [duplicate],How to tell if two matrices are similar? [duplicate],,"This question already has answers here : How do I tell if matrices are similar? (6 answers) Closed 10 years ago . Two n-by-n matrices A and B are called similar if $$      \! B = P^{-1} A P $$ for some invertible n-by-n matrix P. Similar matrices share many properties: Rank Determinant Trace Eigenvalues (though the eigenvectors will in general be different) Characteristic polynomial Minimal polynomial (among the other similarity invariants in the Smith normal form) Elementary divisors Given two square matrices A and B, how would you tell if they are similar? Constructing a $P$ in the definition seems difficult even if we know they are similar, does it? Not to mention, use this way to tell if they are similar. Are there some properties of similar matrices that can characterize similar matrices? Thanks!","This question already has answers here : How do I tell if matrices are similar? (6 answers) Closed 10 years ago . Two n-by-n matrices A and B are called similar if $$      \! B = P^{-1} A P $$ for some invertible n-by-n matrix P. Similar matrices share many properties: Rank Determinant Trace Eigenvalues (though the eigenvectors will in general be different) Characteristic polynomial Minimal polynomial (among the other similarity invariants in the Smith normal form) Elementary divisors Given two square matrices A and B, how would you tell if they are similar? Constructing a $P$ in the definition seems difficult even if we know they are similar, does it? Not to mention, use this way to tell if they are similar. Are there some properties of similar matrices that can characterize similar matrices? Thanks!",,['matrices']
24,Matrix algorithm convergence,Matrix algorithm convergence,,"Suppose I start with a $n \times n$ matrix of zeros and ones: $$ \begin{bmatrix} 0 &	0 &	0 &	1 &	1\\ 1 &	1 &	1 &	1 &	1\\ 1 &	1 &	1 &	1 &	1\\ 1 &	1 &	1 &	1 &	1\\ 1 &	1 &	1 &	1 &	1\\ \end{bmatrix} $$ Then I normalize each row such that it sums to $1$: $$\begin{bmatrix}     0.&	0.&	0.&	0.5&	0.5\\     0.2&	0.2&	0.2&	0.2&	0.2\\     0.2&	0.2&	0.2&	0.2&	0.2\\     0.2&	0.2&	0.2&	0.2&	0.2\\     0.2&	0.2&	0.2&	0.2&	0.2\\ \end{bmatrix} $$ And then do the same for each column: $$\begin{bmatrix}     0. & 	0.  &	0.  &	0.384615  &	0.384615\\     0.25&	0.25&	0.25&	0.153846&	0.153846\\     0.25&	0.25&	0.25&	0.153846&	0.153846\\     0.25&	0.25&	0.25&	0.153846&	0.153846\\     0.25&	0.25&	0.25&	0.153846&	0.153846\\ \end{bmatrix}$$ Repeat this process 15 times, and I have: $$\begin{bmatrix}     0.   &	0. &  	0.   &	0.5  & 	0.5\\     0.25&	0.25&	0.25&	0.125&	0.125\\     0.25&	0.25&	0.25&	0.125&	0.125\\     0.25&	0.25&	0.25&	0.125&	0.125\\     0.25&	0.25&	0.25&	0.125&	0.125\\ \end{bmatrix}$$ Assuming that the original matrix is such that this process is stable, each row and column in the final matrix should sum to $1$. My questions: Is there a name for what this algorithm converges to, or something closely related? What algorithm will produce the same result but converge faster?","Suppose I start with a $n \times n$ matrix of zeros and ones: $$ \begin{bmatrix} 0 &	0 &	0 &	1 &	1\\ 1 &	1 &	1 &	1 &	1\\ 1 &	1 &	1 &	1 &	1\\ 1 &	1 &	1 &	1 &	1\\ 1 &	1 &	1 &	1 &	1\\ \end{bmatrix} $$ Then I normalize each row such that it sums to $1$: $$\begin{bmatrix}     0.&	0.&	0.&	0.5&	0.5\\     0.2&	0.2&	0.2&	0.2&	0.2\\     0.2&	0.2&	0.2&	0.2&	0.2\\     0.2&	0.2&	0.2&	0.2&	0.2\\     0.2&	0.2&	0.2&	0.2&	0.2\\ \end{bmatrix} $$ And then do the same for each column: $$\begin{bmatrix}     0. & 	0.  &	0.  &	0.384615  &	0.384615\\     0.25&	0.25&	0.25&	0.153846&	0.153846\\     0.25&	0.25&	0.25&	0.153846&	0.153846\\     0.25&	0.25&	0.25&	0.153846&	0.153846\\     0.25&	0.25&	0.25&	0.153846&	0.153846\\ \end{bmatrix}$$ Repeat this process 15 times, and I have: $$\begin{bmatrix}     0.   &	0. &  	0.   &	0.5  & 	0.5\\     0.25&	0.25&	0.25&	0.125&	0.125\\     0.25&	0.25&	0.25&	0.125&	0.125\\     0.25&	0.25&	0.25&	0.125&	0.125\\     0.25&	0.25&	0.25&	0.125&	0.125\\ \end{bmatrix}$$ Assuming that the original matrix is such that this process is stable, each row and column in the final matrix should sum to $1$. My questions: Is there a name for what this algorithm converges to, or something closely related? What algorithm will produce the same result but converge faster?",,"['matrices', 'algorithms', 'convergence-divergence']"
25,What is the adjoint representation of $\operatorname{GL}_n(\mathbb{R})$?,What is the adjoint representation of ?,\operatorname{GL}_n(\mathbb{R}),"This question has been asked several times, and virtually every introductory textbook on Lie groups and Lie algebras will cover this. But every single explanation goes beyond me. Let $G$ be a Lie group. Let $\Psi : G \to \operatorname{Aut}(G)$ be the map sending $g \in G$ to the conjugation map $h \mapsto g h g^{-1}$ . For any $g \in G$ , the conjugation map $\Psi(g) : G \to G$ admits a total derivative $d\Psi(g)_e : T_e G \to T_e G$ . Varying $g$ , this may be expressed as a representation $\operatorname{Ad} : G \to \operatorname{Aut}(T_e G)$ . Then, we define $\operatorname{ad} : T_e G \to \operatorname{End}(T_e G)$ to be the total derivative $d \operatorname{Ad}_e$ . Question. What is $\operatorname{ad}$ when $G = \operatorname{GL}_n(\mathbb{R})$ ? What I know. Suppose $A$ is a matrix in $\operatorname{GL}_n(\mathbb{R})$ . Then $\Psi(A)$ is the map sending a matrix $M$ to $AMA^{-1}$ . We can take the derivative of $\Psi(A)$ , which is a map $d \Psi(A)_e : T_e \operatorname{GL}_n(\mathbb{R}) \to T_e \operatorname{GL}_n(\mathbb{R})$ . As $\operatorname{GL}_n(\mathbb{R})$ is an open submanifold of $\operatorname{Mat}_n(\mathbb{R})$ , I can identify the tangent space of $\operatorname{GL}_n(\mathbb{R})$ with $\operatorname{Mat}_n(\mathbb{R})$ . Moreover, for every matrix $X$ , I have a particularly simple choice of a path $\gamma : I \to \operatorname{GL}_n(\mathbb{R})$ passing through $I_n$ with derivative $X$ at $t = 0$ , namely, $\gamma(t) = I_n + tX$ . So to find out what $\operatorname{Ad}(A)$ is, I can write $d\Psi(A)_e(X)$ , for any matrix $A$ in $\operatorname{GL}_n(\mathbb{R})$ and for any element $X \in T_e \operatorname{GL}_n(\mathbb{R}) = \operatorname{Mat}_n(\mathbb{R})$ , as $$d \Psi(A)_e(X) = \frac{d}{dt} \Psi(A)(I + tX) = \frac{d}{dt} A(I + tX)A^{-1} = \frac{d}{dt} (I + AtX A^{-1}) = AXA^{-1}.$$ So, $\operatorname{Ad}(A)$ is $d\Psi(A)_e$ , which is the map $X \mapsto AXA^{-1}$ . But then you have to take a further derivative. And here the limit of my capacity is reached. The complexity of the objects and maps involved simply impede me from seeing what it even means to take a derivative anymore, let alone being able to calculate it.","This question has been asked several times, and virtually every introductory textbook on Lie groups and Lie algebras will cover this. But every single explanation goes beyond me. Let be a Lie group. Let be the map sending to the conjugation map . For any , the conjugation map admits a total derivative . Varying , this may be expressed as a representation . Then, we define to be the total derivative . Question. What is when ? What I know. Suppose is a matrix in . Then is the map sending a matrix to . We can take the derivative of , which is a map . As is an open submanifold of , I can identify the tangent space of with . Moreover, for every matrix , I have a particularly simple choice of a path passing through with derivative at , namely, . So to find out what is, I can write , for any matrix in and for any element , as So, is , which is the map . But then you have to take a further derivative. And here the limit of my capacity is reached. The complexity of the objects and maps involved simply impede me from seeing what it even means to take a derivative anymore, let alone being able to calculate it.",G \Psi : G \to \operatorname{Aut}(G) g \in G h \mapsto g h g^{-1} g \in G \Psi(g) : G \to G d\Psi(g)_e : T_e G \to T_e G g \operatorname{Ad} : G \to \operatorname{Aut}(T_e G) \operatorname{ad} : T_e G \to \operatorname{End}(T_e G) d \operatorname{Ad}_e \operatorname{ad} G = \operatorname{GL}_n(\mathbb{R}) A \operatorname{GL}_n(\mathbb{R}) \Psi(A) M AMA^{-1} \Psi(A) d \Psi(A)_e : T_e \operatorname{GL}_n(\mathbb{R}) \to T_e \operatorname{GL}_n(\mathbb{R}) \operatorname{GL}_n(\mathbb{R}) \operatorname{Mat}_n(\mathbb{R}) \operatorname{GL}_n(\mathbb{R}) \operatorname{Mat}_n(\mathbb{R}) X \gamma : I \to \operatorname{GL}_n(\mathbb{R}) I_n X t = 0 \gamma(t) = I_n + tX \operatorname{Ad}(A) d\Psi(A)_e(X) A \operatorname{GL}_n(\mathbb{R}) X \in T_e \operatorname{GL}_n(\mathbb{R}) = \operatorname{Mat}_n(\mathbb{R}) d \Psi(A)_e(X) = \frac{d}{dt} \Psi(A)(I + tX) = \frac{d}{dt} A(I + tX)A^{-1} = \frac{d}{dt} (I + AtX A^{-1}) = AXA^{-1}. \operatorname{Ad}(A) d\Psi(A)_e X \mapsto AXA^{-1},"['matrices', 'differential-geometry']"
26,Cesàro limit of a stochastic matrix,Cesàro limit of a stochastic matrix,,"Let $A$ be a stochastic matrix . Then \begin{align*} \lim_{t \rightarrow\infty} A^t \end{align*} may not exist. For example: \begin{align*} A &= \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \\ A^{2t} &= I \\ A^{2t+1} &= A \end{align*} Now define the Cesàro limit $A^\infty$ of $A$ to be \begin{align*} \lim_{t \rightarrow \infty} \frac{1}{t} \sum_{k=0}^{t-1} A^k \end{align*} Then, for the above example, \begin{align*} A^\infty = \begin{bmatrix}   \frac{1}{2} & \frac{1}{2} \\   \frac{1}{2} & \frac{1}{2} \end{bmatrix} \end{align*} Intuitively, $A^\infty$ represents the long-run average amount of time spent in each state of the Markov chain described by $A$. My question is this: Does every (finite) stochastic matrix have a Cesàro limit? If so, what is the most efficient algorithm for finding this limit? According to this article , $R^2 = R = RA = AR$ and rank $R \geq $ rank $A^\infty$ implies $R = A^\infty$. It appears that the rows of $A^\infty$ are the normalized eigenvectors of $A^\top$ that have a corresponding eigenvalue of 1. How does one determine the correct order and repetition of such eigenvectors, algorithmically? EDIT: According to this article , the Cesàro limit is guaranteed to exist and is equal to the eigenprojection for the eigenvalue 1 of $A$. EDIT 2: According to this article , $$A^\infty = X (Y^* X)^{-1} Y^*$$ where $X$ are the eigenvectors of $A$ with eigenvalue 1 and $Y$ are the eigenvectors of $A^\top$ with eigenvalue 1. I generally get the right result with this approach but sometimes numerical errors seem to result in the wrong Cesàro limit. Is there a more numerically stable approach?","Let $A$ be a stochastic matrix . Then \begin{align*} \lim_{t \rightarrow\infty} A^t \end{align*} may not exist. For example: \begin{align*} A &= \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix} \\ A^{2t} &= I \\ A^{2t+1} &= A \end{align*} Now define the Cesàro limit $A^\infty$ of $A$ to be \begin{align*} \lim_{t \rightarrow \infty} \frac{1}{t} \sum_{k=0}^{t-1} A^k \end{align*} Then, for the above example, \begin{align*} A^\infty = \begin{bmatrix}   \frac{1}{2} & \frac{1}{2} \\   \frac{1}{2} & \frac{1}{2} \end{bmatrix} \end{align*} Intuitively, $A^\infty$ represents the long-run average amount of time spent in each state of the Markov chain described by $A$. My question is this: Does every (finite) stochastic matrix have a Cesàro limit? If so, what is the most efficient algorithm for finding this limit? According to this article , $R^2 = R = RA = AR$ and rank $R \geq $ rank $A^\infty$ implies $R = A^\infty$. It appears that the rows of $A^\infty$ are the normalized eigenvectors of $A^\top$ that have a corresponding eigenvalue of 1. How does one determine the correct order and repetition of such eigenvectors, algorithmically? EDIT: According to this article , the Cesàro limit is guaranteed to exist and is equal to the eigenprojection for the eigenvalue 1 of $A$. EDIT 2: According to this article , $$A^\infty = X (Y^* X)^{-1} Y^*$$ where $X$ are the eigenvectors of $A$ with eigenvalue 1 and $Y$ are the eigenvectors of $A^\top$ with eigenvalue 1. I generally get the right result with this approach but sometimes numerical errors seem to result in the wrong Cesàro limit. Is there a more numerically stable approach?",,"['matrices', 'limits', 'matrix-calculus', 'regularization', 'stochastic-matrices']"
27,Efficient way to compute $(A+D)^{-1}$ when $A^{-1}$ is known,Efficient way to compute  when  is known,(A+D)^{-1} A^{-1},"I need to compute the inverse of a matrix sum $A+D$, where the inverse of $A\in\mathbb{R}^{n\times n}$ is known. The matrix $D\in\mathbb{R}^{n\times n}$ is a diagonal matrix which can be thought of as a perturbation to $A$. Assuming that $A+D$ is still full-rank, is there any efficient way to compute $(A+D)^{-1}$ using the known quantity $A^{-1}$?","I need to compute the inverse of a matrix sum $A+D$, where the inverse of $A\in\mathbb{R}^{n\times n}$ is known. The matrix $D\in\mathbb{R}^{n\times n}$ is a diagonal matrix which can be thought of as a perturbation to $A$. Assuming that $A+D$ is still full-rank, is there any efficient way to compute $(A+D)^{-1}$ using the known quantity $A^{-1}$?",,"['matrices', 'inverse']"
28,Radical of $\mathfrak{gl}_n$,Radical of,\mathfrak{gl}_n,"I find it intuitive enough that the radical of $\mathfrak{gl}_n\mathbb F$ is the scalar matrices, but I have trouble finding an easy, but complete proof: Proof . Let $\mathfrak s$ denote the scalar matrices. Clearly $\mathfrak s\subset\mathrm{rad}(\mathfrak{gl}_n\mathbb F)$. Suppose that $\mathrm{rad}(\mathfrak{gl}_n\mathbb F)$ is generated by more than one element, so that $X\in\mathrm{rad}(\mathfrak{gl}_n\mathbb F)$, but $X\notin\mathfrak s$. We can change basis such that $\mathrm{rad}(\mathfrak{gl}_n\mathbb F)$ is upper-triangular. (The change of basis leaves the scalar matrices invariant.) Then $X$ is upper-triangular. I want to conclude that there exists a $Y\in\mathfrak{gl}_n\mathbb F$ s.t. $[X,Y]$ is not upper-triangular. How can I see that $Y$ always exists, except by waving hands?","I find it intuitive enough that the radical of $\mathfrak{gl}_n\mathbb F$ is the scalar matrices, but I have trouble finding an easy, but complete proof: Proof . Let $\mathfrak s$ denote the scalar matrices. Clearly $\mathfrak s\subset\mathrm{rad}(\mathfrak{gl}_n\mathbb F)$. Suppose that $\mathrm{rad}(\mathfrak{gl}_n\mathbb F)$ is generated by more than one element, so that $X\in\mathrm{rad}(\mathfrak{gl}_n\mathbb F)$, but $X\notin\mathfrak s$. We can change basis such that $\mathrm{rad}(\mathfrak{gl}_n\mathbb F)$ is upper-triangular. (The change of basis leaves the scalar matrices invariant.) Then $X$ is upper-triangular. I want to conclude that there exists a $Y\in\mathfrak{gl}_n\mathbb F$ s.t. $[X,Y]$ is not upper-triangular. How can I see that $Y$ always exists, except by waving hands?",,"['matrices', 'lie-algebras']"
29,Minimizing $L_1$ Regularization,Minimizing  Regularization,L_1,"I have given a high dimensional input $x \in \mathbb{R}^m$ where $m$ is a big number. Linear regression can be applied, but in generel it is expected, that a lot of these dimensions are actually irrelevant. I ought to find a method to model the function $y = f(x)$ and at the same time uncover which dimensions contribute to the output. The overall hint is to apply the $L_1$-norm Lasso regularization . $$L^{\text lasso}(\beta) = \sum_{i=1}^n (y_i - \phi(x_i)^T \beta)^2 + \lambda \sum_{j = 1}^k | \beta_j |$$ Minimizing $L^{\text lasso}$ is in general hard, for that reason I should apply gradient descent. My approach so far is the following: In order to minimize the term, I chose to compute the gradient and set it $0$, i.e. $$\frac{\partial}{\partial \beta} L^{\text lasso}(\beta) = -2 \sum_{i = 1}^n \phi(x_i)(y_i - \beta)$$ Since this cannot be computed simply, I want to apply the gradient descent here. The gradient descent takes a function $f(x)$ as input, so in place of that will be put the $L(\beta)$ function. My problems are: is the gradient I computed correct? I left out the regularizationt erm in the end, which I guess is a problem, but I also have problems to compute the gradient for this one In addition I may replace $| \beta_j |$ by the function $l(x)$, which is $l(x) = x - \varepsilon/2$ if $|x| \geq \epsilon$, else $x^2/(2 \varepsilon)$ one step in the gradient descent is: $\beta \leftarrow \beta - \alpha \cdot \frac{g}{|g|}$, where $g = \frac{\partial}{\partial \beta}f(x))^T$ I don't get this one, when I transpose the gradient I have computed the term cannot be resolved due to dimension mismatch","I have given a high dimensional input $x \in \mathbb{R}^m$ where $m$ is a big number. Linear regression can be applied, but in generel it is expected, that a lot of these dimensions are actually irrelevant. I ought to find a method to model the function $y = f(x)$ and at the same time uncover which dimensions contribute to the output. The overall hint is to apply the $L_1$-norm Lasso regularization . $$L^{\text lasso}(\beta) = \sum_{i=1}^n (y_i - \phi(x_i)^T \beta)^2 + \lambda \sum_{j = 1}^k | \beta_j |$$ Minimizing $L^{\text lasso}$ is in general hard, for that reason I should apply gradient descent. My approach so far is the following: In order to minimize the term, I chose to compute the gradient and set it $0$, i.e. $$\frac{\partial}{\partial \beta} L^{\text lasso}(\beta) = -2 \sum_{i = 1}^n \phi(x_i)(y_i - \beta)$$ Since this cannot be computed simply, I want to apply the gradient descent here. The gradient descent takes a function $f(x)$ as input, so in place of that will be put the $L(\beta)$ function. My problems are: is the gradient I computed correct? I left out the regularizationt erm in the end, which I guess is a problem, but I also have problems to compute the gradient for this one In addition I may replace $| \beta_j |$ by the function $l(x)$, which is $l(x) = x - \varepsilon/2$ if $|x| \geq \epsilon$, else $x^2/(2 \varepsilon)$ one step in the gradient descent is: $\beta \leftarrow \beta - \alpha \cdot \frac{g}{|g|}$, where $g = \frac{\partial}{\partial \beta}f(x))^T$ I don't get this one, when I transpose the gradient I have computed the term cannot be resolved due to dimension mismatch",,"['matrices', 'derivatives', 'vector-analysis', 'machine-learning']"
30,Smallest Non-negative number in a matrix,Smallest Non-negative number in a matrix,,"There is a question I encountered which said to fill an $N \times N$ matrix such that each entry in the matrix is the smallest non-negative number which does not appear  either above the entry or to its left. That is for $N = 6$ the matrix looks like this: $$\begin{array}{} 0&1&2&3&4&5\\ 1&0&3&2&5&4\\ 2&3&0&1&6&7\\ 3&2&1&0&7&6\\ 4&5&6&7&0&1\\ 5&4&7&6&1&0 \end{array}$$ I was asked to find a function such that given row and column I can calculate the value at that point i.e $$f(\text{row}, \text{column}) = \text{Matrix}[\text{row}][\text{column}]$$ I was looking at Nimbers and found the matrix in it exactly similar to it. There was also given a formula to calculate to calculate Matrix[row][column] which was row XOR column (XOR is bitwise exor). However I was able to get the answer I am still unable as to how to arrive at the solution i.e. the proof that each entry in the matrix is the smallest non-negative number which does not appear either above the entry or to its left is equal to row XOR column.","There is a question I encountered which said to fill an $N \times N$ matrix such that each entry in the matrix is the smallest non-negative number which does not appear  either above the entry or to its left. That is for $N = 6$ the matrix looks like this: $$\begin{array}{} 0&1&2&3&4&5\\ 1&0&3&2&5&4\\ 2&3&0&1&6&7\\ 3&2&1&0&7&6\\ 4&5&6&7&0&1\\ 5&4&7&6&1&0 \end{array}$$ I was asked to find a function such that given row and column I can calculate the value at that point i.e $$f(\text{row}, \text{column}) = \text{Matrix}[\text{row}][\text{column}]$$ I was looking at Nimbers and found the matrix in it exactly similar to it. There was also given a formula to calculate to calculate Matrix[row][column] which was row XOR column (XOR is bitwise exor). However I was able to get the answer I am still unable as to how to arrive at the solution i.e. the proof that each entry in the matrix is the smallest non-negative number which does not appear either above the entry or to its left is equal to row XOR column.",,['matrices']
31,How to prove this inequality for determinant of Hermitian block matrix?,How to prove this inequality for determinant of Hermitian block matrix?,,"I am given an Hermitian positive definite matrix $$D=\left(\begin{matrix}A&\overline{C}^T\\C&B\end{matrix}\right)$$ $A$ and $B$ are square matrices. The task is to prove the following inequalities: $\det(D)\leq\det(A)\det(B)\\\det(D)\leq\prod_{i=1}^{n}(d_{ii})$ All I can see is that $A,B$ are also Hermitian, and that determinant is equal to product of eigenvalues for $D, A, B$ since they are Hermitian, and those are positive since matrix is positive definite, and I have no further idea. Maybe you could give an advice or kind of hint. Thanks in advance!","I am given an Hermitian positive definite matrix $$D=\left(\begin{matrix}A&\overline{C}^T\\C&B\end{matrix}\right)$$ $A$ and $B$ are square matrices. The task is to prove the following inequalities: $\det(D)\leq\det(A)\det(B)\\\det(D)\leq\prod_{i=1}^{n}(d_{ii})$ All I can see is that $A,B$ are also Hermitian, and that determinant is equal to product of eigenvalues for $D, A, B$ since they are Hermitian, and those are positive since matrix is positive definite, and I have no further idea. Maybe you could give an advice or kind of hint. Thanks in advance!",,"['matrices', 'inequality', 'determinant', 'block-matrices', 'hermitian-matrices']"
32,Log-likelihood gradient and Hessian,Log-likelihood gradient and Hessian,,"Considering a binary classification problem with data $D = \left\{ (x_i, y_i) \right\}_{i=1}^n$ , $x_i \in \mathbb{R}^d$ and $y_i \in \{0,1\}$ . Given the following definitions, $$f(x) = x^T \beta$$ $$p(x) = \sigma(f(x)) \quad \text{with}  \quad\sigma(z) = 1/(1 + e^{-z})$$ $$L(\beta) = \sum_{i=1}^n \Bigl[ y_i \log p(x_i) + (1 - y_i) \log [1 - p(x_i)] \Bigr]$$ where $\beta \in \mathbb{R}^d$ is a vector. $p(x)$ is a short-hand for $p(y = 1\ |\ x)$ . The task is to compute the derivative $\frac{\partial}{\partial \beta} L(\beta)$ . A tip is to use the fact $$\frac{\partial}{\partial z} \sigma(z) = \sigma(z) (1 - \sigma(z))$$ So here is my approach so far: \begin{align*} L(\beta) & = \sum_{i=1}^n \Bigl[ y_i \log p(x_i) + (1 - y_i) \log [1 - p(x_i)] \Bigr]\\ \frac{\partial}{\partial \beta} L(\beta) & =  \sum_{i=1}^n \Bigl[ \Bigl( \frac{\partial}{\partial \beta} y_i \log p(x_i) \Bigr) + \Bigl( \frac{\partial}{\partial \beta} (1 - y_i) \log [1 - p(x_i)] \Bigr) \Bigr]\\ \end{align*} \begin{align*} \frac{\partial}{\partial \beta} y_i  \log p(x_i) &= (\frac{\partial}{\partial \beta} y_i) \cdot \log p(x_i) + y_i \cdot  (\frac{\partial}{\partial \beta} p(x_i))\\ &= 0 \cdot \log p(x_i) + y_i \cdot (\frac{\partial}{\partial \beta} p(x_i))\\ &= y_i \cdot (p(x_i) \cdot (1 - p(x_i))) \end{align*} \begin{align*} \frac{\partial}{\partial \beta} (1 - y_i) \log [1 - p(x_i)] &= (1 - y_i) \cdot (\frac{\partial}{\partial \beta} \log [1 - p(x_i)])\\ & = (1 - y_i) \cdot \frac{1}{1 - p(x_i)} \cdot p(x_i) \cdot (1 - p(x_i))\\ & = (1 - y_i) \cdot p(x_i) \end{align*} $$\frac{\partial}{\partial \beta} L(\beta) = \sum_{i=1}^n \Bigl[ y_i \cdot (p(x_i) \cdot (1 - p(x_i))) + (1 - y_i) \cdot p(x_i)  \Bigr]$$ So basically I used the product and chain rule to compute the derivative. I am afraid, that my solution is wrong, because on page 120 of The Elements of Statistical Learning it says the gradient is $$\sum_{i = 1}^N x_i(y_i - p(x_i;\beta))$$ I don't know what could have possibly gone wrong. Any advice on this?","Considering a binary classification problem with data , and . Given the following definitions, where is a vector. is a short-hand for . The task is to compute the derivative . A tip is to use the fact So here is my approach so far: So basically I used the product and chain rule to compute the derivative. I am afraid, that my solution is wrong, because on page 120 of The Elements of Statistical Learning it says the gradient is I don't know what could have possibly gone wrong. Any advice on this?","D = \left\{ (x_i, y_i) \right\}_{i=1}^n x_i \in \mathbb{R}^d y_i \in \{0,1\} f(x) = x^T \beta p(x) = \sigma(f(x)) \quad \text{with}  \quad\sigma(z) = 1/(1 + e^{-z}) L(\beta) = \sum_{i=1}^n \Bigl[ y_i \log p(x_i) + (1 - y_i) \log [1 - p(x_i)] \Bigr] \beta \in \mathbb{R}^d p(x) p(y = 1\ |\ x) \frac{\partial}{\partial \beta} L(\beta) \frac{\partial}{\partial z} \sigma(z) = \sigma(z) (1 - \sigma(z)) \begin{align*}
L(\beta) & = \sum_{i=1}^n \Bigl[ y_i \log p(x_i) + (1 - y_i) \log [1 - p(x_i)] \Bigr]\\
\frac{\partial}{\partial \beta} L(\beta) & =  \sum_{i=1}^n \Bigl[ \Bigl( \frac{\partial}{\partial \beta} y_i \log p(x_i) \Bigr) + \Bigl( \frac{\partial}{\partial \beta} (1 - y_i) \log [1 - p(x_i)] \Bigr) \Bigr]\\
\end{align*} \begin{align*}
\frac{\partial}{\partial \beta} y_i  \log p(x_i) &= (\frac{\partial}{\partial \beta} y_i) \cdot \log p(x_i) + y_i \cdot  (\frac{\partial}{\partial \beta} p(x_i))\\
&= 0 \cdot \log p(x_i) + y_i \cdot (\frac{\partial}{\partial \beta} p(x_i))\\
&= y_i \cdot (p(x_i) \cdot (1 - p(x_i)))
\end{align*} \begin{align*}
\frac{\partial}{\partial \beta} (1 - y_i) \log [1 - p(x_i)] &= (1 - y_i) \cdot (\frac{\partial}{\partial \beta} \log [1 - p(x_i)])\\
& = (1 - y_i) \cdot \frac{1}{1 - p(x_i)} \cdot p(x_i) \cdot (1 - p(x_i))\\
& = (1 - y_i) \cdot p(x_i)
\end{align*} \frac{\partial}{\partial \beta} L(\beta) = \sum_{i=1}^n \Bigl[ y_i \cdot (p(x_i) \cdot (1 - p(x_i))) + (1 - y_i) \cdot p(x_i)  \Bigr] \sum_{i = 1}^N x_i(y_i - p(x_i;\beta))","['matrices', 'multivariable-calculus', 'derivatives', 'vector-analysis', 'regression']"
33,Ways to calculate the derivative of the matrix exponential,Ways to calculate the derivative of the matrix exponential,,"Could someone provide me with a rigorous proof as to why the derivative of the function $f:t \ni \mathbb{R} \mapsto e^{tA}\in \textrm{Mat}_n (\mathbb{R})$ is $t \mapsto A\cdot e^{tA}$ ? I didn't understand the ""elementwise"" arguments, as to why the above should hold and when trying to evaluate $f'$ by hand I got stuck at evaluating $$ \lim _{h\rightarrow 0} \frac{||e^{xA}\cdot e^{hA} - e^{xA} -A\cdot e^{xA} \cdot h||}{|h|},$$ where $||\cdot ||$ denotes any norm on $\textrm{Mat}_n (\mathbb{R}) $ that is multiplicative (so that $( \textrm{Mat}_n (\mathbb{R}) , || \cdot ||)$ becomes a Banach algebra) - which is what I have do to, I think (please correct me, if I'm wrong, or using an unnecessary abstract level of discourse), because in the setting of matrix-valued functions the derivative of a matrix becomes the derivative between the Banach spaces $\mathbb{R}$ and $\textrm{Mat}_n (\mathbb{R}) $ (using an isomorphism $\phi:\textrm{Mat}_n (\mathbb{R}) \rightarrow \mathbb{R}^{n^2} $ to do the derivative there and then transporting everything back to  $\textrm{Mat}_n (\mathbb{R}) $ seems rather ugly to me - although I tried to do it this way and failed). Could I replace $\mathbb{R}$ with $\mathbb{C}$ ?","Could someone provide me with a rigorous proof as to why the derivative of the function $f:t \ni \mathbb{R} \mapsto e^{tA}\in \textrm{Mat}_n (\mathbb{R})$ is $t \mapsto A\cdot e^{tA}$ ? I didn't understand the ""elementwise"" arguments, as to why the above should hold and when trying to evaluate $f'$ by hand I got stuck at evaluating $$ \lim _{h\rightarrow 0} \frac{||e^{xA}\cdot e^{hA} - e^{xA} -A\cdot e^{xA} \cdot h||}{|h|},$$ where $||\cdot ||$ denotes any norm on $\textrm{Mat}_n (\mathbb{R}) $ that is multiplicative (so that $( \textrm{Mat}_n (\mathbb{R}) , || \cdot ||)$ becomes a Banach algebra) - which is what I have do to, I think (please correct me, if I'm wrong, or using an unnecessary abstract level of discourse), because in the setting of matrix-valued functions the derivative of a matrix becomes the derivative between the Banach spaces $\mathbb{R}$ and $\textrm{Mat}_n (\mathbb{R}) $ (using an isomorphism $\phi:\textrm{Mat}_n (\mathbb{R}) \rightarrow \mathbb{R}^{n^2} $ to do the derivative there and then transporting everything back to  $\textrm{Mat}_n (\mathbb{R}) $ seems rather ugly to me - although I tried to do it this way and failed). Could I replace $\mathbb{R}$ with $\mathbb{C}$ ?",,"['matrices', 'exponentiation']"
34,Who first noted that entries in the powers of an adjacency matrix of a graph count the number of walks on the graph?,Who first noted that entries in the powers of an adjacency matrix of a graph count the number of walks on the graph?,,"The Wikipedia article on powers of an adjacency matrix presently (as of 2022) notes the neat combinatorial fact that, given an adjacency matrix $A$ of some graph, entries of the $n$ th power of the adjacency matrix, $A^n_{ij}$ , count the number of $n$ -length walks from $i$ to $j$ .  The Wikipedia article further relates this to the problem of counting the number of triangles in said graph by dividing the trace of $A^3$ by $6$ , and determining the distance between two nodes in an undirected, unweighted graph. The Wikipedia article does not name these as lemmas or theorems or corollaries, and states them with only the briefest outline of the proof. Are these folk results, or is there any other interesting history behind who first formulated and proved them? These are very nice and fun statements to prove, but I'm curious to know if there's anything else to say about their history. (I am especially interested in how quantum computers could efficiently explore spectral properties of certain large adjacency matrices of certain large graphs, as, by linearity, exponentiation of the spectrum corresponds to exponentiation of the given adjacency matrix - which then corresponds to counting various walks on the graph.)","The Wikipedia article on powers of an adjacency matrix presently (as of 2022) notes the neat combinatorial fact that, given an adjacency matrix of some graph, entries of the th power of the adjacency matrix, , count the number of -length walks from to .  The Wikipedia article further relates this to the problem of counting the number of triangles in said graph by dividing the trace of by , and determining the distance between two nodes in an undirected, unweighted graph. The Wikipedia article does not name these as lemmas or theorems or corollaries, and states them with only the briefest outline of the proof. Are these folk results, or is there any other interesting history behind who first formulated and proved them? These are very nice and fun statements to prove, but I'm curious to know if there's anything else to say about their history. (I am especially interested in how quantum computers could efficiently explore spectral properties of certain large adjacency matrices of certain large graphs, as, by linearity, exponentiation of the spectrum corresponds to exponentiation of the given adjacency matrix - which then corresponds to counting various walks on the graph.)",A n A^n_{ij} n i j A^3 6,"['matrices', 'graph-theory', 'math-history', 'algebraic-graph-theory', 'adjacency-matrix']"
35,Sudoku grid guaranteed to be solvable?,Sudoku grid guaranteed to be solvable?,,"I want to generate random sudoku grids. My approach is to fill the three 3x3 groups on a diagonal of the grid, each with the numbers 1-9 randomly shuffled. That looks e.g. like the grid below: +-------+-------+-------+ | 5 6 4 | · · · | · · · | | 1 7 2 | · · · | · · · | | 9 8 3 | · · · | · · · | +-------+-------+-------+ | · · · | 3 2 5 | · · · | | · · · | 7 9 6 | · · · | | · · · | 8 1 4 | · · · | +-------+-------+-------+ | · · · | · · · | 1 5 9 | | · · · | · · · | 3 2 7 | | · · · | · · · | 6 4 8 | +-------+-------+-------+ Then I let my sudoku solver process it until it finds a solution to fill all remaining gaps. The example above resulted in the filled grid below: +-------+-------+-------+ | 5 6 4 | 9 3 7 | 2 8 1 | | 1 7 2 | 5 4 8 | 9 6 3 | | 9 8 3 | 1 6 2 | 4 7 5 | +-------+-------+-------+ | 7 4 9 | 3 2 5 | 8 1 6 | | 2 1 8 | 7 9 6 | 5 3 4 | | 6 3 5 | 8 1 4 | 7 9 2 | +-------+-------+-------+ | 8 2 6 | 4 7 3 | 1 5 9 | | 4 5 1 | 6 8 9 | 3 2 7 | | 3 9 7 | 2 5 1 | 6 4 8 | +-------+-------+-------+ My question is if this approach is mathematically safe, i.e. can you prove me that when I fill my grid using the described approach (randomly assigning the first 27 numbers to fields in the groups on a diagonal line), there will be always at least one possible way to complete the grid from there on? Or are there chances that the randomly placed numbers can interfere with each other and result in an impossible grid?","I want to generate random sudoku grids. My approach is to fill the three 3x3 groups on a diagonal of the grid, each with the numbers 1-9 randomly shuffled. That looks e.g. like the grid below: +-------+-------+-------+ | 5 6 4 | · · · | · · · | | 1 7 2 | · · · | · · · | | 9 8 3 | · · · | · · · | +-------+-------+-------+ | · · · | 3 2 5 | · · · | | · · · | 7 9 6 | · · · | | · · · | 8 1 4 | · · · | +-------+-------+-------+ | · · · | · · · | 1 5 9 | | · · · | · · · | 3 2 7 | | · · · | · · · | 6 4 8 | +-------+-------+-------+ Then I let my sudoku solver process it until it finds a solution to fill all remaining gaps. The example above resulted in the filled grid below: +-------+-------+-------+ | 5 6 4 | 9 3 7 | 2 8 1 | | 1 7 2 | 5 4 8 | 9 6 3 | | 9 8 3 | 1 6 2 | 4 7 5 | +-------+-------+-------+ | 7 4 9 | 3 2 5 | 8 1 6 | | 2 1 8 | 7 9 6 | 5 3 4 | | 6 3 5 | 8 1 4 | 7 9 2 | +-------+-------+-------+ | 8 2 6 | 4 7 3 | 1 5 9 | | 4 5 1 | 6 8 9 | 3 2 7 | | 3 9 7 | 2 5 1 | 6 4 8 | +-------+-------+-------+ My question is if this approach is mathematically safe, i.e. can you prove me that when I fill my grid using the described approach (randomly assigning the first 27 numbers to fields in the groups on a diagonal line), there will be always at least one possible way to complete the grid from there on? Or are there chances that the randomly placed numbers can interfere with each other and result in an impossible grid?",,"['matrices', 'puzzle']"
36,Do almost commuting matrices almost commute with the square root?,Do almost commuting matrices almost commute with the square root?,,"If $A$ and $B$ are $n\times n$ self-adjoint matrices with $B$ positive semidefinite and $\|B\|_F\leq1$ such that $$\|AB-BA\|_F\leq \epsilon,$$ where $\|A\|^2_F:=tr(A^*A)$ is the Frobenius (or Hilbert-Schmidt) norm. I'd like to know if there is a constant $C>0$ such that $\|A\sqrt{B}-\sqrt{B}A\|_F<C\epsilon$ is independent of $n$ . Using the solution to this question one can use the fact that there is a polynomial of degree at most $n$ such that $p(B)=\sqrt{B}$ and it follows that there is a constant $K>0$ such that for any polynomial $p(B)$ we see that $$\|Ap(B)-p(B)A\|_F\leq Kn\epsilon,$$ just using the submutliplicativity of $\|\cdot\|_F$ and the triangle inequality. However, in the case where $p(B)=\sqrt{B}$ , I would like to know if there is a bound that depends on $\epsilon$ but not $n$ . Edit: in the comments of the linked question, the author of the answer has noted that the degree of the polynomial is at least the number of distinct eigenvalues of $B$ , which makes sense if you are using the polynomial that interpolates the eigenvalues.","If and are self-adjoint matrices with positive semidefinite and such that where is the Frobenius (or Hilbert-Schmidt) norm. I'd like to know if there is a constant such that is independent of . Using the solution to this question one can use the fact that there is a polynomial of degree at most such that and it follows that there is a constant such that for any polynomial we see that just using the submutliplicativity of and the triangle inequality. However, in the case where , I would like to know if there is a bound that depends on but not . Edit: in the comments of the linked question, the author of the answer has noted that the degree of the polynomial is at least the number of distinct eigenvalues of , which makes sense if you are using the polynomial that interpolates the eigenvalues.","A B n\times n B \|B\|_F\leq1 \|AB-BA\|_F\leq \epsilon, \|A\|^2_F:=tr(A^*A) C>0 \|A\sqrt{B}-\sqrt{B}A\|_F<C\epsilon n n p(B)=\sqrt{B} K>0 p(B) \|Ap(B)-p(B)A\|_F\leq Kn\epsilon, \|\cdot\|_F p(B)=\sqrt{B} \epsilon n B","['matrices', 'normed-spaces']"
37,Modifying unitary matrix eigenvalues by right multiplication by orthogonal matrix,Modifying unitary matrix eigenvalues by right multiplication by orthogonal matrix,,"I have a matrix $U \in U(n)$ ( $U^* U=Id$ ), with eigenvalues $\lambda_1, \dots \lambda_n \in S^1$ . I would like to know if its always possible to find a matrix $O \in O(n)$ such that the eigenvalues $\lambda^{'}_1, \dots, \lambda^{'}_n$ of $UO \in U(n)$ can be written as: $$\lambda^{'}_j=e^{i \pi \alpha_j}$$ Where $\alpha_j \in [0,1)$ . There is an easy case. If $U$ is a diagonal matrix and $J \subset \{1, \dots , n \}$ is the subset such that for $j \in J$ we have $\lambda_j=x_j + iy_j$ with $x,y \in \mathbb{R}$ and $(y<0 \vee x=-1)$ . Then choosing: $$\{O\}_{mn}=\begin{cases} 0 & m\ne n \\ -1 & m=n\in J \\ 1 & else \end{cases}$$ This will define an orthogonal matrix  and $UO$ will be as desired.  The problem is that when $U$ is not a diagonal matrix the matrix $S=P^*OP$ for $P \in U(n)$ and $O \in O(n)$ is not generally an orthogonal matrix so there is no simple algorithm as far as I can see to find $O \in O(n)$","I have a matrix ( ), with eigenvalues . I would like to know if its always possible to find a matrix such that the eigenvalues of can be written as: Where . There is an easy case. If is a diagonal matrix and is the subset such that for we have with and . Then choosing: This will define an orthogonal matrix  and will be as desired.  The problem is that when is not a diagonal matrix the matrix for and is not generally an orthogonal matrix so there is no simple algorithm as far as I can see to find","U \in U(n) U^* U=Id \lambda_1, \dots \lambda_n \in S^1 O \in O(n) \lambda^{'}_1, \dots, \lambda^{'}_n UO \in U(n) \lambda^{'}_j=e^{i \pi \alpha_j} \alpha_j \in [0,1) U J \subset \{1, \dots , n \} j \in J \lambda_j=x_j + iy_j x,y \in \mathbb{R} (y<0 \vee x=-1) \{O\}_{mn}=\begin{cases}
0 & m\ne n \\
-1 & m=n\in J \\
1 & else
\end{cases} UO U S=P^*OP P \in U(n) O \in O(n) O \in O(n)","['matrices', 'eigenvalues-eigenvectors', 'lie-groups', 'lie-algebras', 'unitary-matrices']"
38,What are the diagonals in the matrix called?,What are the diagonals in the matrix called?,,"How correctly to designate these diagonals? They are highlighted in different colors. \begin{pmatrix}\color{green}1&\color{orange}2&\color{red}3&\color{blue}4\\ \color{orange}5&\color{red}6&\color{blue}7&\color{green}8\\ \color{red}9&\color{blue}{10}&\color{green}{11}&\color{orange}{12}\\ \color{blue}{13}&\color{green}{14}&\color{orange}{15}&\color{red}{16}\end{pmatrix} $\color{green}{1, 14, 11, 8}$ $\color{orange}{5, 2, 15, 12}$ $\color{red}{9, 6, 3, 16}$ $\color{blue}{13,10, 7, 4}$ - secondary diagonal","How correctly to designate these diagonals? They are highlighted in different colors. \begin{pmatrix}\color{green}1&\color{orange}2&\color{red}3&\color{blue}4\\ \color{orange}5&\color{red}6&\color{blue}7&\color{green}8\\ \color{red}9&\color{blue}{10}&\color{green}{11}&\color{orange}{12}\\ \color{blue}{13}&\color{green}{14}&\color{orange}{15}&\color{red}{16}\end{pmatrix} $\color{green}{1, 14, 11, 8}$ $\color{orange}{5, 2, 15, 12}$ $\color{red}{9, 6, 3, 16}$ $\color{blue}{13,10, 7, 4}$ - secondary diagonal",,"['matrices', 'terminology']"
39,An inequality concerning restricted isometry property,An inequality concerning restricted isometry property,,"Let $A\in \mathbb{R}^{m\times n}$ be a matrix and let us denote by $A_S$ the submatrix of $A$ with the columns restricted to a set $S\subset [n]:=\{1,2,\cdots, \ n\}$. Then one says that the matrix $A$ has restricted isometry property of order $K$ with constant $\delta_K$ if the matrix $A$ has the property $$\delta_K=\max_{S\subset[n]:|S|\le K}\|A_S^TA_S-I\|_{2\to 2}$$ There is a very nice Cauchy-Scwartz like inequality concerning this constant which can be stated as below If $x,y\in \mathbb{R}^n$ with $supp(x)\subset S_1,\ supp(y)\subset S_2,\ S_1\cap S_2=\emptyset$, then,    $$|\langle Ax,Ay\rangle |< \delta_{|S_1|+|S_2|} \|x\|_2\|y\|_2$$ Emmanuel Candes proved this inequality here . The proof is rather simple. I am trying to find a similar inequality, for the quantity in the LHS, but now a lower bound and also with the constraint $S_1\cap S_2\ne \emptyset$. I do not know if it is trivially $0$, but I hope it is not. I have still not succeeded to find it. I have searched for such an inequality in the literature but failed to get one. So it will be really helpful if anyone can direct me either towards some relevant literature or giving some helpful tips to come up with a non-trivial lower bound. Thanks in advance.","Let $A\in \mathbb{R}^{m\times n}$ be a matrix and let us denote by $A_S$ the submatrix of $A$ with the columns restricted to a set $S\subset [n]:=\{1,2,\cdots, \ n\}$. Then one says that the matrix $A$ has restricted isometry property of order $K$ with constant $\delta_K$ if the matrix $A$ has the property $$\delta_K=\max_{S\subset[n]:|S|\le K}\|A_S^TA_S-I\|_{2\to 2}$$ There is a very nice Cauchy-Scwartz like inequality concerning this constant which can be stated as below If $x,y\in \mathbb{R}^n$ with $supp(x)\subset S_1,\ supp(y)\subset S_2,\ S_1\cap S_2=\emptyset$, then,    $$|\langle Ax,Ay\rangle |< \delta_{|S_1|+|S_2|} \|x\|_2\|y\|_2$$ Emmanuel Candes proved this inequality here . The proof is rather simple. I am trying to find a similar inequality, for the quantity in the LHS, but now a lower bound and also with the constraint $S_1\cap S_2\ne \emptyset$. I do not know if it is trivially $0$, but I hope it is not. I have still not succeeded to find it. I have searched for such an inequality in the literature but failed to get one. So it will be really helpful if anyone can direct me either towards some relevant literature or giving some helpful tips to come up with a non-trivial lower bound. Thanks in advance.",,"['matrices', 'inequality', 'eigenvalues-eigenvectors', 'operator-theory']"
40,Polynomial approximation to formal power series matrix,Polynomial approximation to formal power series matrix,,"I noticed that starting with a $2 {\times} 2$ matrix $M$ with a handful of polynomial entries in two variables such that $\det M$ is invertible in $\mathbb C [x] [[y]]$ I can add infinitely many terms of higher orders in $y$ to the entries of $M$ to make $\det M$ a constant. Further, I noticed that with a bit of tinkering it's actually possible to add finitely many terms to $M$ and still achieve that $\det M$ is constant. I am now wondering if this is true in general: Let $M$ be a $2 {\times} 2$ matrix with entries in the ring $\mathbb C [x][[y]]$ and constant determinant. Given $k > 0$, does there exist a matrix $N$ with entries in $\mathbb C [x, y]$ such that $M \,\mathrm{mod} \, (y^{k+1}) = N \,\mathrm{mod} \, (y^{k+1})$ and $\det M = \det N$? (If it helps, I would be content with assuming that one of the entries is a multiple of $y$ — e.g. if the $(1,2)$ entry is a multiple of $y$, then the determinant being constant implies that the $(1,1)$ and $(2,2)$ entries have a constant term and are both of the form $1 + O (y)$.) I give a simple example to illustrate: Example . Let   $$ M = \begin{pmatrix} 1 - y & 0 \\ 0 & 1 + y + y^2 + \dotsb \end{pmatrix} $$   Then $\det M = 1$ (as the $(1,1)$ entry is invertible in $\mathbb C [[y]]$ with inverse $(2,2)$). Now let   $$ N = \begin{pmatrix} 1-y & -y^{k+1} \\ y^{k+1} & 1 + y + \dotsb + y^{2k+1} \end{pmatrix} $$   Then $\det M = \det N = 1$ and   $$ M \, \mathrm{mod} (y^{k+1}) = N \, \mathrm{mod} (y^{k+1}) =  \begin{pmatrix} 1 - y & 0 \\ 0 & 1 + y + \dotsb + y^k \end{pmatrix}. \qquad \diamond $$ This example works equally well if you replace $y$ by a polynomial $p = p (x,y)$ which is a multiple of $y$. I would also be interested to hear about (textbook/paper) examples computing with matrices of formal power series to see what kind of techniques can be used. Edit. I have discovered a truly tedious proof of this, which this edit section is too narrow to contain. It is a ""proof by brute force"" (solving linear equations for each monomial that may appear in the expression of $\det M \mod (y^{n+1})$), and I think writing up the details should best be left as an Exercise To The Reader. There ought to be a more elegant proof and although I haven't disclosed the full proof, at least I now believe the result to be true, which might be a reason to look for such a proof.","I noticed that starting with a $2 {\times} 2$ matrix $M$ with a handful of polynomial entries in two variables such that $\det M$ is invertible in $\mathbb C [x] [[y]]$ I can add infinitely many terms of higher orders in $y$ to the entries of $M$ to make $\det M$ a constant. Further, I noticed that with a bit of tinkering it's actually possible to add finitely many terms to $M$ and still achieve that $\det M$ is constant. I am now wondering if this is true in general: Let $M$ be a $2 {\times} 2$ matrix with entries in the ring $\mathbb C [x][[y]]$ and constant determinant. Given $k > 0$, does there exist a matrix $N$ with entries in $\mathbb C [x, y]$ such that $M \,\mathrm{mod} \, (y^{k+1}) = N \,\mathrm{mod} \, (y^{k+1})$ and $\det M = \det N$? (If it helps, I would be content with assuming that one of the entries is a multiple of $y$ — e.g. if the $(1,2)$ entry is a multiple of $y$, then the determinant being constant implies that the $(1,1)$ and $(2,2)$ entries have a constant term and are both of the form $1 + O (y)$.) I give a simple example to illustrate: Example . Let   $$ M = \begin{pmatrix} 1 - y & 0 \\ 0 & 1 + y + y^2 + \dotsb \end{pmatrix} $$   Then $\det M = 1$ (as the $(1,1)$ entry is invertible in $\mathbb C [[y]]$ with inverse $(2,2)$). Now let   $$ N = \begin{pmatrix} 1-y & -y^{k+1} \\ y^{k+1} & 1 + y + \dotsb + y^{2k+1} \end{pmatrix} $$   Then $\det M = \det N = 1$ and   $$ M \, \mathrm{mod} (y^{k+1}) = N \, \mathrm{mod} (y^{k+1}) =  \begin{pmatrix} 1 - y & 0 \\ 0 & 1 + y + \dotsb + y^k \end{pmatrix}. \qquad \diamond $$ This example works equally well if you replace $y$ by a polynomial $p = p (x,y)$ which is a multiple of $y$. I would also be interested to hear about (textbook/paper) examples computing with matrices of formal power series to see what kind of techniques can be used. Edit. I have discovered a truly tedious proof of this, which this edit section is too narrow to contain. It is a ""proof by brute force"" (solving linear equations for each monomial that may appear in the expression of $\det M \mod (y^{n+1})$), and I think writing up the details should best be left as an Exercise To The Reader. There ought to be a more elegant proof and although I haven't disclosed the full proof, at least I now believe the result to be true, which might be a reason to look for such a proof.",,"['matrices', 'polynomials', 'commutative-algebra', 'formal-power-series']"
41,Does the inverse of this matrix of size $n \times n$ approach the zero-matrix in the limit as $\small n \to \infty$?,Does the inverse of this matrix of size  approach the zero-matrix in the limit as ?,n \times n \small n \to \infty,"Fiddling with another (older) question here I constructed an example-matrix of the type $\small M_n: m_{n:r,c} = {1 \over (1+r)^c } \quad \text{ for } r,c=0 \ldots n-1 $     . I considered the inverse W :  $\small W_n=M_n^{-1} $ for some small n and observed, that the entries in W from the left columns on tend to zero. Using the LDU-decomposition and inverting that L,D,U -factors, one can observe a simple pattern for the coefficients in the matrices or better: for the terms of the dot-products of that inverses in the leftmost columns which is independent of the matrixsize. For instance, if we denote the inverted L,D,U -factors as K , C , T such that for any n $\small W_n = M_n^{-1} = T \cdot C \cdot K = U^{-1} \cdot D^{-1} \cdot L^{-1}$ then the top-left entry in $\small W_n  $ can be computed by the dot-product of the first row of T , C and the first column of K which shows a simple pattern such that we assume the following type of sum:  $$\small w_{n:0,0} = \sum_{j=0}^{n-1} (-1)^j (j+1)/j! \qquad \text{ and } \qquad \lim_{n\to \infty} w_{n:00}=0 $$  Now for the two first columns it seems, that indeed all dot-products vanish when n increases and that this can be shown by relatively simple modification and linear composition of the formal exponential-series. But for the next columns this becomes more difficult because the patterns are complicated (but seem to be recursive) and the matrices need increasing size to actually approximate the limiting values. So the question:a) does W indeed approach the Null-matrix as n goes to infinity? or in more detail: what are the patterns for the dot-products which occur in the evaluation of the single entries in W ? [update] I seem to have solved the pattern for the matrix of the partial product $\small C \cdot K $ by a good heuristic. Let's denote the entries with the small letter g then $$ g_{r,c} = (-1)^c \cdot (1+c)^r \binom{r+1}{c+1} $$ Because at least the first two rows in T are simple to decode  $$ \begin{eqnarray}  t_{0,c} &=& (-1)^{r+c} {1 \over c!} \\   t_{1,c} &=& (-1)^{r+c} \binom{c+1}{2}{1 \over c!}  \end{eqnarray} $$ the results by the two dot-products for rows r=0 and r=1 $$ \begin{eqnarray}  w_{0,c} = \sum_{k=0}^{\infty} t_{0,k} \cdot g_{k,c} = 0  \\   w_{1,c} = \sum_{k=1}^{\infty} t_{1,k} \cdot g_{k,c} = 0  \\  \end{eqnarray} $$ give for the first two rows in W zeros for the first couple of columns (with index c=0..4 checked using wolframalpha and the formulae) Wolframalpha could even give answers for the general column (colindex=c kept indeterminate) For the rows r=0 and r=1 in W for all columns c the value $\small w_{r,c}=0 $ Here are the Wolframalpha formulae: $$ \sum_{k=c}^{\infty} \left[ \left({(-1)^k \over k!}\right) * \left((-1)^ c (1 + c )^k \binom{k+1}{ c +1}\right)  \right] $$ ( input ) $$ \sum_{k=c}^{\infty} \left[ \left({(-1)^k \over k! }\binom{k+1}{2} \right) * \left((-1)^ c (1 + c )^k \binom{k+1}{c +1} \right)  \right] $$ ( input )","Fiddling with another (older) question here I constructed an example-matrix of the type $\small M_n: m_{n:r,c} = {1 \over (1+r)^c } \quad \text{ for } r,c=0 \ldots n-1 $     . I considered the inverse W :  $\small W_n=M_n^{-1} $ for some small n and observed, that the entries in W from the left columns on tend to zero. Using the LDU-decomposition and inverting that L,D,U -factors, one can observe a simple pattern for the coefficients in the matrices or better: for the terms of the dot-products of that inverses in the leftmost columns which is independent of the matrixsize. For instance, if we denote the inverted L,D,U -factors as K , C , T such that for any n $\small W_n = M_n^{-1} = T \cdot C \cdot K = U^{-1} \cdot D^{-1} \cdot L^{-1}$ then the top-left entry in $\small W_n  $ can be computed by the dot-product of the first row of T , C and the first column of K which shows a simple pattern such that we assume the following type of sum:  $$\small w_{n:0,0} = \sum_{j=0}^{n-1} (-1)^j (j+1)/j! \qquad \text{ and } \qquad \lim_{n\to \infty} w_{n:00}=0 $$  Now for the two first columns it seems, that indeed all dot-products vanish when n increases and that this can be shown by relatively simple modification and linear composition of the formal exponential-series. But for the next columns this becomes more difficult because the patterns are complicated (but seem to be recursive) and the matrices need increasing size to actually approximate the limiting values. So the question:a) does W indeed approach the Null-matrix as n goes to infinity? or in more detail: what are the patterns for the dot-products which occur in the evaluation of the single entries in W ? [update] I seem to have solved the pattern for the matrix of the partial product $\small C \cdot K $ by a good heuristic. Let's denote the entries with the small letter g then $$ g_{r,c} = (-1)^c \cdot (1+c)^r \binom{r+1}{c+1} $$ Because at least the first two rows in T are simple to decode  $$ \begin{eqnarray}  t_{0,c} &=& (-1)^{r+c} {1 \over c!} \\   t_{1,c} &=& (-1)^{r+c} \binom{c+1}{2}{1 \over c!}  \end{eqnarray} $$ the results by the two dot-products for rows r=0 and r=1 $$ \begin{eqnarray}  w_{0,c} = \sum_{k=0}^{\infty} t_{0,k} \cdot g_{k,c} = 0  \\   w_{1,c} = \sum_{k=1}^{\infty} t_{1,k} \cdot g_{k,c} = 0  \\  \end{eqnarray} $$ give for the first two rows in W zeros for the first couple of columns (with index c=0..4 checked using wolframalpha and the formulae) Wolframalpha could even give answers for the general column (colindex=c kept indeterminate) For the rows r=0 and r=1 in W for all columns c the value $\small w_{r,c}=0 $ Here are the Wolframalpha formulae: $$ \sum_{k=c}^{\infty} \left[ \left({(-1)^k \over k!}\right) * \left((-1)^ c (1 + c )^k \binom{k+1}{ c +1}\right)  \right] $$ ( input ) $$ \sum_{k=c}^{\infty} \left[ \left({(-1)^k \over k! }\binom{k+1}{2} \right) * \left((-1)^ c (1 + c )^k \binom{k+1}{c +1} \right)  \right] $$ ( input )",,"['number-theory', 'matrices', 'limits']"
42,SVD of concatenate matrices,SVD of concatenate matrices,,"Let $X \in \mathbf{R}^{n\times m}$ be split into two sub matrices, $X =    \begin{bmatrix}     X_0 \\     X_1   \end{bmatrix} $, if $X_0 = U_0 \Sigma_0 V_0^T$ and $X_1 = U_1 \Sigma_1 V_1^T$ can we say anything about the SVD of $X$, or at least have a good starting point for an iterative method?","Let $X \in \mathbf{R}^{n\times m}$ be split into two sub matrices, $X =    \begin{bmatrix}     X_0 \\     X_1   \end{bmatrix} $, if $X_0 = U_0 \Sigma_0 V_0^T$ and $X_1 = U_1 \Sigma_1 V_1^T$ can we say anything about the SVD of $X$, or at least have a good starting point for an iterative method?",,"['matrices', 'eigenvalues-eigenvectors', 'svd']"
43,Can the Riemann hypothesis be relaxed to say that this matrix A consists of square roots?,Can the Riemann hypothesis be relaxed to say that this matrix A consists of square roots?,,"I realize that asking this question is like presenting to a patent attorney a wheel-less skateboard while asking to patent a hoverboard. Anyways. Lagarias version of the Riemann hypothesis sets a bound on the sum of divisors: $$\sigma(n) \le H_n + e^{H_n} \ln H_n$$ where $H_n$ is a harmonic number. The von Mangoldt function matrix, as I call it, can be generated from the matrix product of two matrices: $$A = \left( \begin{array}{ccccccc}  1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots \\  1 & \sqrt{2} & 0 & 0 & 0 & 0 & 0 \\  1 & 0 & \sqrt{3} & 0 & 0 & 0 & 0 \\  1 & \sqrt{2} & 0 & \sqrt{4} & 0 & 0 & 0 \\  1 & 0 & 0 & 0 & \sqrt{5} & 0 & 0 \\  1 & \sqrt{2} & \sqrt{3} & 0 & 0 & \sqrt{6} & 0 \\  1 & 0 & 0 & 0 & 0 & 0 & \sqrt{7} \\  \vdots&&&&&&&\ddots \end{array} \right)$$ which is equal to $A(n,k)=\sqrt{k}$ if $k$ divides $n$, else $A(n,k)=0$ The matrix inverse of $A$ is by its terms essentially equal to the matrix: $$B = \left( \begin{array}{ccccccc}  1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\  0 & -\sqrt{2} & 0 & -\sqrt{2} & 0 & -\sqrt{2} & 0 \\  0 & 0 & -\sqrt{3} & 0 & 0 & -\sqrt{3} & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & -\sqrt{5} & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & \sqrt{6} & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & -\sqrt{7} \\  \vdots&&&&&&&\ddots \end{array} \right)$$ which is equal to $B(n,k)=\mu(n)\sqrt{n}$ if $n$ divides $k$, else $A(n,k)=0$ where $\mu(n)$ is the Möbius function defined by: $$\mu(n)=\begin{cases} (-1)^{\omega(n)}=(-1)^{\Omega(n)} &\text{if }\; \omega(n) = \Omega(n)\\ 0&\text{if }\;\omega(n) \ne \Omega(n).\end{cases}$$ or as in the Wikipedia page: $\mu(n) =  1$ if $n$ is a square-free positive integer with an even number of prime factors. $\mu(n) =  -1$ if $n$ is a square-free positive integer with an odd number of prime factors. $\mu(n) =  0$ if $n$ has a squared prime factor. The von Mangoldt function matrix is then the matrix product $A$ times $B$: $$T = A.B = \left( \begin{array}{ccccccc}  +1&+1&+1&+1&+1&+1&+1&\cdots \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&-2&+1&+1&-2&+1 \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&+1&+1&-4&+1&+1 \\ +1&-1&-2&-1&+1&+2&+1 \\ +1&+1&+1&+1&+1&+1&-6 \\ \vdots&&&&&&&\ddots \end{array} \right)$$ And the von Mangoldt function is then: $$\Lambda(n) = \sum\limits_{k=1}^{k=\infty}\frac{T(n,k)}{k}$$ as proven by joriki here . or as the Dirichlet generating functions of the columns as proven here by GH from MO at Mathoverflow: $$\Lambda(n)=\lim\limits_{s \rightarrow 1} \zeta(s)\sum\limits_{d|n} \frac{\mu(d)}{d^{(s-1)}}$$ Here comes the hoverboard / wheel-less skateboard: Since according to the explicit formula, the von Mangoldt function is a sum of logarithmic square root waves as follows: $$\sum_{n=1}^{n=k} \Lambda(n) = \Re\left(-\sum _{j=1}^{\infty} \left(\frac{x^{1-\rho _j}}{1-\rho _j}+\frac{x^{\rho _j}}{\rho _j}\right)-\frac{1}{2} \log \left(1-\frac{1}{x^2}\right)+x-\log (2 \pi )\right)$$ or as a Mathematica one-liner from Alex Kontorovich web page: Plot[Re[X - Log[2 Pi] - Log[1 - 1/X^2]/2 -     Sum[X^(N[ZetaZero[j]])/(N[ZetaZero[j]]) +       X^(1 - N[ZetaZero[j]])/(1 - N[ZetaZero[j]]), {j, 1, 30}]], {X,    1.1, 30}] Can the Riemann hypothesis be relaxed/be made precise to say that the so called von Mangoldt function matrix $T$ is a matrix product $T=A.B$ as in the example above? (*Matrix T Mathematica  8*) nn = 32; A = Table[    Table[If[Mod[n, k] == 0, k^(ZetaZero[k]), 0], {k, 1, nn}], {n, 1,      nn}]; B = Table[    Table[If[Mod[k, n] == 0, MoebiusMu[n]*n^(ZetaZero[-n]), 0], {k, 1,       nn}], {n, 1, nn}]; MatrixForm[T=N[A.B]] It appears to work for any complex number sequence in the exponents as long as the sum of the two matrices $A$ and $B$'s respective real parts is equal to 1, and the imaginary parts are each others negatives. In other words a condition that applies to any two complex number sequences of that form, of which the zeta zeros are a subset, so no progress. To demonstrate this I have made this variant of the program above: (*Matrix T Mathematica 8 start*)nn = 32; a = Table[RandomComplex[], {n, 1, 32}] A = Table[    Table[If[Mod[n, k] == 0, k^(a[[k]]), 0], {k, 1, nn}], {n, 1, nn}]; B = Table[    Table[If[Mod[k, n] == 0, MoebiusMu[n]*n^(1 - a[[n]]), 0], {k, 1,       nn}], {n, 1, nn}]; MatrixForm[T = Chop[N[A.B]]] (*end*) which produces matrix $T$. This probably has to do with the elementary fact that: $$n=n^{a} n^{1-a}$$ $n$ is here a substitute for the terms in matrix $T$. So for some arbitrary complex number sequence $a$ like for example: $$a=0.771518+0.640552I,0.192739+0.923147I,0.931096+0.758704I,...$$ or the non-trivial Riemann zeta zeros: $$a=0.5 + 14.1347 I, 0.5 + 21.022 I, 0.5 + 25.0109 I,...$$ we have in general the matrices: $$A = \left( \begin{array}{ccccccc}  1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots \\  1 & 2^{a_2} & 0 & 0 & 0 & 0 & 0 \\  1 & 0 & 3^{a_3} & 0 & 0 & 0 & 0 \\  1 & 2^{a_2} & 0 & 4^{a_4} & 0 & 0 & 0 \\  1 & 0 & 0 & 0 & 5^{a_5} & 0 & 0 \\  1 & 2^{a_2} & 3^{a_3} & 0 & 0 & 6^{a_6} & 0 \\  1 & 0 & 0 & 0 & 0 & 0 & 7^{a_7} \\  \vdots&&&&&&&\ddots \end{array} \right)$$ $$B = \left( \begin{array}{ccccccc}  1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\  0 & -2^{1-a_2} & 0 & -2^{1-a_2} & 0 & -2^{1-a_2} & 0 \\  0 & 0 & -3^{1-a_3} & 0 & 0 & -3^{1-a_3} & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & -5^{1-a_5} & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 6^{1-a_6} & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & -7^{1-a_7} \\  \vdots&&&&&&&\ddots \end{array} \right)$$ Which have the same property as the earlier matrices $A$ and $B$ producing matrix $T$ as the matrix product: $$T = A.B = \left( \begin{array}{ccccccc}  +1&+1&+1&+1&+1&+1&+1&\cdots \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&-2&+1&+1&-2&+1 \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&+1&+1&-4&+1&+1 \\ +1&-1&-2&-1&+1&+2&+1 \\ +1&+1&+1&+1&+1&+1&-6 \\ \vdots&&&&&&&\ddots \end{array} \right)$$","I realize that asking this question is like presenting to a patent attorney a wheel-less skateboard while asking to patent a hoverboard. Anyways. Lagarias version of the Riemann hypothesis sets a bound on the sum of divisors: $$\sigma(n) \le H_n + e^{H_n} \ln H_n$$ where $H_n$ is a harmonic number. The von Mangoldt function matrix, as I call it, can be generated from the matrix product of two matrices: $$A = \left( \begin{array}{ccccccc}  1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots \\  1 & \sqrt{2} & 0 & 0 & 0 & 0 & 0 \\  1 & 0 & \sqrt{3} & 0 & 0 & 0 & 0 \\  1 & \sqrt{2} & 0 & \sqrt{4} & 0 & 0 & 0 \\  1 & 0 & 0 & 0 & \sqrt{5} & 0 & 0 \\  1 & \sqrt{2} & \sqrt{3} & 0 & 0 & \sqrt{6} & 0 \\  1 & 0 & 0 & 0 & 0 & 0 & \sqrt{7} \\  \vdots&&&&&&&\ddots \end{array} \right)$$ which is equal to $A(n,k)=\sqrt{k}$ if $k$ divides $n$, else $A(n,k)=0$ The matrix inverse of $A$ is by its terms essentially equal to the matrix: $$B = \left( \begin{array}{ccccccc}  1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\  0 & -\sqrt{2} & 0 & -\sqrt{2} & 0 & -\sqrt{2} & 0 \\  0 & 0 & -\sqrt{3} & 0 & 0 & -\sqrt{3} & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & -\sqrt{5} & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & \sqrt{6} & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & -\sqrt{7} \\  \vdots&&&&&&&\ddots \end{array} \right)$$ which is equal to $B(n,k)=\mu(n)\sqrt{n}$ if $n$ divides $k$, else $A(n,k)=0$ where $\mu(n)$ is the Möbius function defined by: $$\mu(n)=\begin{cases} (-1)^{\omega(n)}=(-1)^{\Omega(n)} &\text{if }\; \omega(n) = \Omega(n)\\ 0&\text{if }\;\omega(n) \ne \Omega(n).\end{cases}$$ or as in the Wikipedia page: $\mu(n) =  1$ if $n$ is a square-free positive integer with an even number of prime factors. $\mu(n) =  -1$ if $n$ is a square-free positive integer with an odd number of prime factors. $\mu(n) =  0$ if $n$ has a squared prime factor. The von Mangoldt function matrix is then the matrix product $A$ times $B$: $$T = A.B = \left( \begin{array}{ccccccc}  +1&+1&+1&+1&+1&+1&+1&\cdots \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&-2&+1&+1&-2&+1 \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&+1&+1&-4&+1&+1 \\ +1&-1&-2&-1&+1&+2&+1 \\ +1&+1&+1&+1&+1&+1&-6 \\ \vdots&&&&&&&\ddots \end{array} \right)$$ And the von Mangoldt function is then: $$\Lambda(n) = \sum\limits_{k=1}^{k=\infty}\frac{T(n,k)}{k}$$ as proven by joriki here . or as the Dirichlet generating functions of the columns as proven here by GH from MO at Mathoverflow: $$\Lambda(n)=\lim\limits_{s \rightarrow 1} \zeta(s)\sum\limits_{d|n} \frac{\mu(d)}{d^{(s-1)}}$$ Here comes the hoverboard / wheel-less skateboard: Since according to the explicit formula, the von Mangoldt function is a sum of logarithmic square root waves as follows: $$\sum_{n=1}^{n=k} \Lambda(n) = \Re\left(-\sum _{j=1}^{\infty} \left(\frac{x^{1-\rho _j}}{1-\rho _j}+\frac{x^{\rho _j}}{\rho _j}\right)-\frac{1}{2} \log \left(1-\frac{1}{x^2}\right)+x-\log (2 \pi )\right)$$ or as a Mathematica one-liner from Alex Kontorovich web page: Plot[Re[X - Log[2 Pi] - Log[1 - 1/X^2]/2 -     Sum[X^(N[ZetaZero[j]])/(N[ZetaZero[j]]) +       X^(1 - N[ZetaZero[j]])/(1 - N[ZetaZero[j]]), {j, 1, 30}]], {X,    1.1, 30}] Can the Riemann hypothesis be relaxed/be made precise to say that the so called von Mangoldt function matrix $T$ is a matrix product $T=A.B$ as in the example above? (*Matrix T Mathematica  8*) nn = 32; A = Table[    Table[If[Mod[n, k] == 0, k^(ZetaZero[k]), 0], {k, 1, nn}], {n, 1,      nn}]; B = Table[    Table[If[Mod[k, n] == 0, MoebiusMu[n]*n^(ZetaZero[-n]), 0], {k, 1,       nn}], {n, 1, nn}]; MatrixForm[T=N[A.B]] It appears to work for any complex number sequence in the exponents as long as the sum of the two matrices $A$ and $B$'s respective real parts is equal to 1, and the imaginary parts are each others negatives. In other words a condition that applies to any two complex number sequences of that form, of which the zeta zeros are a subset, so no progress. To demonstrate this I have made this variant of the program above: (*Matrix T Mathematica 8 start*)nn = 32; a = Table[RandomComplex[], {n, 1, 32}] A = Table[    Table[If[Mod[n, k] == 0, k^(a[[k]]), 0], {k, 1, nn}], {n, 1, nn}]; B = Table[    Table[If[Mod[k, n] == 0, MoebiusMu[n]*n^(1 - a[[n]]), 0], {k, 1,       nn}], {n, 1, nn}]; MatrixForm[T = Chop[N[A.B]]] (*end*) which produces matrix $T$. This probably has to do with the elementary fact that: $$n=n^{a} n^{1-a}$$ $n$ is here a substitute for the terms in matrix $T$. So for some arbitrary complex number sequence $a$ like for example: $$a=0.771518+0.640552I,0.192739+0.923147I,0.931096+0.758704I,...$$ or the non-trivial Riemann zeta zeros: $$a=0.5 + 14.1347 I, 0.5 + 21.022 I, 0.5 + 25.0109 I,...$$ we have in general the matrices: $$A = \left( \begin{array}{ccccccc}  1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots \\  1 & 2^{a_2} & 0 & 0 & 0 & 0 & 0 \\  1 & 0 & 3^{a_3} & 0 & 0 & 0 & 0 \\  1 & 2^{a_2} & 0 & 4^{a_4} & 0 & 0 & 0 \\  1 & 0 & 0 & 0 & 5^{a_5} & 0 & 0 \\  1 & 2^{a_2} & 3^{a_3} & 0 & 0 & 6^{a_6} & 0 \\  1 & 0 & 0 & 0 & 0 & 0 & 7^{a_7} \\  \vdots&&&&&&&\ddots \end{array} \right)$$ $$B = \left( \begin{array}{ccccccc}  1 & 1 & 1 & 1 & 1 & 1 & 1 & \cdots \\  0 & -2^{1-a_2} & 0 & -2^{1-a_2} & 0 & -2^{1-a_2} & 0 \\  0 & 0 & -3^{1-a_3} & 0 & 0 & -3^{1-a_3} & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 \\  0 & 0 & 0 & 0 & -5^{1-a_5} & 0 & 0 \\  0 & 0 & 0 & 0 & 0 & 6^{1-a_6} & 0 \\  0 & 0 & 0 & 0 & 0 & 0 & -7^{1-a_7} \\  \vdots&&&&&&&\ddots \end{array} \right)$$ Which have the same property as the earlier matrices $A$ and $B$ producing matrix $T$ as the matrix product: $$T = A.B = \left( \begin{array}{ccccccc}  +1&+1&+1&+1&+1&+1&+1&\cdots \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&-2&+1&+1&-2&+1 \\ +1&-1&+1&-1&+1&-1&+1 \\ +1&+1&+1&+1&-4&+1&+1 \\ +1&-1&-2&-1&+1&+2&+1 \\ +1&+1&+1&+1&+1&+1&-6 \\ \vdots&&&&&&&\ddots \end{array} \right)$$",,"['matrices', 'number-theory', 'riemann-hypothesis']"
44,Expressing the values of a matrix at pow N,Expressing the values of a matrix at pow N,,"I have a square matrix (that comes from a Markov Chain) that looks like that: $$Q = \begin{bmatrix} 0 & 1& 0    & 0 & .. & 0 & 0\\ 0 & a & 1-a & 0 & .. & 0 & 0\\ 0 & 0 & b   & 1 - b & .. & 0 & 0\\ .. & .. & .. & .. & .. & .. & .. \\ 0 & 0 & 0   & 0 & .. & 0 & 1 \end{bmatrix}$$ with $a, b, c,$ etc. real numbers between $0$ and $1$ included. I am interested in the values of the first line of the matrix $Q^N$. Currently, I am using a library (numpy) that allows me to compute $Q^N$ and then I read the first line of this matrix. But with large matrices ($> 500 \times 500$) and large values of $N$ (~ 10000), it is a bit too slow for my usage. By curiosity, I have plotted $Q^N_{0,j}$ for $N$ between 1 to 1000 and I found that they follow something that looks like a Poisson distribution or similar (but I don't know if it's only due to my specific input matrix $Q$ or not). My question is, given such a matrix Q, is there a way to get the values of $Q^N_{0,j}$ without having to compute $Q^N$? Edit: the terms on the diagonal are such as: 0 <= a <= b <= c <= ... < 1 Edit2: It appears that if I can diagonalize $Q$, I can use $Q^n = P D^n P^{-1}$ which is faster for large values of $n$ than an exponentiation by squaring (as used by numpy). Problem is that I am not sure it is possible for any matrix $Q$. And if it's not possible, I'd accept a slight modification of $Q$, $P$ or $D$ if the result is close enough. Edit3: The values of the first line of $Q^N$ for $N \in [1,100]$: Edit4: it's $a <= b$ and not $a < b$, sorry!","I have a square matrix (that comes from a Markov Chain) that looks like that: $$Q = \begin{bmatrix} 0 & 1& 0    & 0 & .. & 0 & 0\\ 0 & a & 1-a & 0 & .. & 0 & 0\\ 0 & 0 & b   & 1 - b & .. & 0 & 0\\ .. & .. & .. & .. & .. & .. & .. \\ 0 & 0 & 0   & 0 & .. & 0 & 1 \end{bmatrix}$$ with $a, b, c,$ etc. real numbers between $0$ and $1$ included. I am interested in the values of the first line of the matrix $Q^N$. Currently, I am using a library (numpy) that allows me to compute $Q^N$ and then I read the first line of this matrix. But with large matrices ($> 500 \times 500$) and large values of $N$ (~ 10000), it is a bit too slow for my usage. By curiosity, I have plotted $Q^N_{0,j}$ for $N$ between 1 to 1000 and I found that they follow something that looks like a Poisson distribution or similar (but I don't know if it's only due to my specific input matrix $Q$ or not). My question is, given such a matrix Q, is there a way to get the values of $Q^N_{0,j}$ without having to compute $Q^N$? Edit: the terms on the diagonal are such as: 0 <= a <= b <= c <= ... < 1 Edit2: It appears that if I can diagonalize $Q$, I can use $Q^n = P D^n P^{-1}$ which is faster for large values of $n$ than an exponentiation by squaring (as used by numpy). Problem is that I am not sure it is possible for any matrix $Q$. And if it's not possible, I'd accept a slight modification of $Q$, $P$ or $D$ if the result is close enough. Edit3: The values of the first line of $Q^N$ for $N \in [1,100]$: Edit4: it's $a <= b$ and not $a < b$, sorry!",,"['matrices', 'exponentiation']"
45,Is there general formula for the exponential of a tridiagonal matrix?,Is there general formula for the exponential of a tridiagonal matrix?,,"For an arbitrary tridiagonal matrix of the form $$ A = \begin{pmatrix} b_1 & c_1 & 0 & 0 & ... \\ a_2 & b_2 & c_2 & 0 & ... \\ 0 & a_3 & b_3 & c_3 & ... \\ \vdots &&\ddots&\ddots&\ddots\end{pmatrix} $$ is there a formula to calculate $\exp(A)$? Or at least for some special tridiagonal matrices? The special case I am most interested in is a $(2n+1)^2$ matrix with $b_k = i(k-n-1)$ and $c_k = (a_{2n+2-k})^*$, i.e. $$\begin{pmatrix} -in & c_1 & 0 & \\ c_{2n}^* & -i(n-1) & c_2 & \\ 0 & c_{2n-1}^* & -i(n-2) & \ddots \\ &&\ddots&\ddots \end{pmatrix}$$","For an arbitrary tridiagonal matrix of the form $$ A = \begin{pmatrix} b_1 & c_1 & 0 & 0 & ... \\ a_2 & b_2 & c_2 & 0 & ... \\ 0 & a_3 & b_3 & c_3 & ... \\ \vdots &&\ddots&\ddots&\ddots\end{pmatrix} $$ is there a formula to calculate $\exp(A)$? Or at least for some special tridiagonal matrices? The special case I am most interested in is a $(2n+1)^2$ matrix with $b_k = i(k-n-1)$ and $c_k = (a_{2n+2-k})^*$, i.e. $$\begin{pmatrix} -in & c_1 & 0 & \\ c_{2n}^* & -i(n-1) & c_2 & \\ 0 & c_{2n-1}^* & -i(n-2) & \ddots \\ &&\ddots&\ddots \end{pmatrix}$$",,"['matrices', 'exponentiation', 'matrix-exponential', 'tridiagonal-matrices']"
46,Determinant of a 5 × 5 matrix,Determinant of a 5 × 5 matrix,,"I have a little problem with a determinant. Let $A = (a_{ij}) \in \mathbb{R}^{(n, n)}, n \ge 4$ with $$a_{ij} =  \begin{cases}    x \quad \mbox{for } \,i = 2, \,\, j \ge 4,\\   d \quad \mbox{for } \,i \ge j, \\   0 \quad \mbox{else.} \end{cases}$$ So for example, if we choose $n = 5$, the matrix would look like this: $$ A =  \begin{pmatrix}  d &0 &0 &0 &0 \\ d &d &0 &x &x \\ d &d &d &0 &0 \\ d &d &d &d &0 \\ d &d &d &d &d \\ \end{pmatrix}$$ How can I find the determinant of this matrix? My first idea was to split this matrix into a product of a triangular matrix $T$ and a rest matrix $R$ so that $A = T \cdot R$. Then I wanted to use $$\det(A) = \det(T \cdot R) = \det(T) \cdot \det(R).$$ to figure out the determinant. This would be something like $$ d^n \cdot \det(R)$$ But is this approach even possible (I don't think so)? Is there any intelligent way of solving this? Thanks in advance.","I have a little problem with a determinant. Let $A = (a_{ij}) \in \mathbb{R}^{(n, n)}, n \ge 4$ with $$a_{ij} =  \begin{cases}    x \quad \mbox{for } \,i = 2, \,\, j \ge 4,\\   d \quad \mbox{for } \,i \ge j, \\   0 \quad \mbox{else.} \end{cases}$$ So for example, if we choose $n = 5$, the matrix would look like this: $$ A =  \begin{pmatrix}  d &0 &0 &0 &0 \\ d &d &0 &x &x \\ d &d &d &0 &0 \\ d &d &d &d &0 \\ d &d &d &d &d \\ \end{pmatrix}$$ How can I find the determinant of this matrix? My first idea was to split this matrix into a product of a triangular matrix $T$ and a rest matrix $R$ so that $A = T \cdot R$. Then I wanted to use $$\det(A) = \det(T \cdot R) = \det(T) \cdot \det(R).$$ to figure out the determinant. This would be something like $$ d^n \cdot \det(R)$$ But is this approach even possible (I don't think so)? Is there any intelligent way of solving this? Thanks in advance.",,['matrices']
47,Calculating the matrix exponential of rotation matrix,Calculating the matrix exponential of rotation matrix,,I have the matrix: $$A=\begin {pmatrix} 0 &-1\\ 1 & 0\\ \end{pmatrix}$$ And I want to try to calculate its exponential using this formula $$\ e^{M} = \sum_{k=0}^\infty \frac{1}{k!}\ M^{k}\\$$ I have worked out that $$A^{2} = -I$$ $$A^{3}=-A$$ and $$A^{4}=I$$ where $I$ is the identity matrix. I have then tried to use the fact that the sum will cycle through these matrices to separate the sum and then recombine it into one matrix. However what I get out cannot easily be expressed as a sum. Any help would be greatly appreciated!,I have the matrix: And I want to try to calculate its exponential using this formula I have worked out that and where is the identity matrix. I have then tried to use the fact that the sum will cycle through these matrices to separate the sum and then recombine it into one matrix. However what I get out cannot easily be expressed as a sum. Any help would be greatly appreciated!,"A=\begin {pmatrix}
0 &-1\\
1 & 0\\
\end{pmatrix} \ e^{M} = \sum_{k=0}^\infty \frac{1}{k!}\ M^{k}\\ A^{2} = -I A^{3}=-A A^{4}=I I","['matrices', 'matrix-calculus', 'matrix-exponential']"
48,Matrix inverses - Why are they derived the way they are?,Matrix inverses - Why are they derived the way they are?,,"Note that this is not a question of how, but why. I know the mechanics of it, but this is the first thing i've come across that truly seems like magic, rather than a rigorous mathematical process. There are questions on SE about proofs for the inverse, but niether here nor anywhere else on the internet, can I find a decent explanation of why it works the way it does: So, basically, suppose the matrix is $\begin{bmatrix} a && b \\ c && d \end{bmatrix}$ Why do we swap values $a$ and $d$? Why do $c$ and $b$ become negative?. Why do we divide $a$, $b$, $c$ and $d$ by $ad-bc$? Once again, this truly seems like magic. It's like ''hey, swap these fellers around, twiddle these signs a little, and poof, you're inverse!''. Thanks in advance for any help :)","Note that this is not a question of how, but why. I know the mechanics of it, but this is the first thing i've come across that truly seems like magic, rather than a rigorous mathematical process. There are questions on SE about proofs for the inverse, but niether here nor anywhere else on the internet, can I find a decent explanation of why it works the way it does: So, basically, suppose the matrix is $\begin{bmatrix} a && b \\ c && d \end{bmatrix}$ Why do we swap values $a$ and $d$? Why do $c$ and $b$ become negative?. Why do we divide $a$, $b$, $c$ and $d$ by $ad-bc$? Once again, this truly seems like magic. It's like ''hey, swap these fellers around, twiddle these signs a little, and poof, you're inverse!''. Thanks in advance for any help :)",,"['matrices', 'inverse', 'matrix-equations']"
49,Why is the determinant of the all one matrix minus the identity matrix n-1? [duplicate],Why is the determinant of the all one matrix minus the identity matrix n-1? [duplicate],,"This question already has answers here : How to calculate the determinant of all-ones matrix minus the identity? [duplicate] (8 answers) Determinant of a matrix with diagonal entries $a$ and off-diagonal entries $b$ [duplicate] (9 answers) Closed 6 years ago . Context (skippable) I was asked (by a friend who is preparing for an exam) whether there was a special trick to compute the determinant of the following matrix. I didn't see anything beyond using the standard computations (like using ""Gauss"" to compute the value). Then I asked another math student who, while quite bright, is a bit rusty in linear algebra and using sagemath we empirically found the below formula. Of course we were both confused as to a) whether it actually always holds and b) why it holds. Actual question Let $n\in\mathbb N$ be a positive integer. Let $I_n\in\mathbb R^{n\times n}$ be the identity matrix and let $1_n\in\mathbb R^{n\times n}$ be the all-one matrix, that is, the matrix for which every entry is $1$. Now I am confused as to why the following (empirically found) statement holds (or does not): $$\forall n\in\mathbb N:\det(1_n-I_n)=(-1)^{n-1}(n-1)$$ For illustration purposes, here is the matrix for $n=4$ (with the determinant being $-3$): \begin{pmatrix} 0&1&1&1\\ 1&0&1&1\\ 1&1&0&1\\ 1&1&1&0 \end{pmatrix}","This question already has answers here : How to calculate the determinant of all-ones matrix minus the identity? [duplicate] (8 answers) Determinant of a matrix with diagonal entries $a$ and off-diagonal entries $b$ [duplicate] (9 answers) Closed 6 years ago . Context (skippable) I was asked (by a friend who is preparing for an exam) whether there was a special trick to compute the determinant of the following matrix. I didn't see anything beyond using the standard computations (like using ""Gauss"" to compute the value). Then I asked another math student who, while quite bright, is a bit rusty in linear algebra and using sagemath we empirically found the below formula. Of course we were both confused as to a) whether it actually always holds and b) why it holds. Actual question Let $n\in\mathbb N$ be a positive integer. Let $I_n\in\mathbb R^{n\times n}$ be the identity matrix and let $1_n\in\mathbb R^{n\times n}$ be the all-one matrix, that is, the matrix for which every entry is $1$. Now I am confused as to why the following (empirically found) statement holds (or does not): $$\forall n\in\mathbb N:\det(1_n-I_n)=(-1)^{n-1}(n-1)$$ For illustration purposes, here is the matrix for $n=4$ (with the determinant being $-3$): \begin{pmatrix} 0&1&1&1\\ 1&0&1&1\\ 1&1&0&1\\ 1&1&1&0 \end{pmatrix}",,"['matrices', 'determinant', 'symmetric-matrices']"
50,Inverse of an upper triangular matrix with all entries 1,Inverse of an upper triangular matrix with all entries 1,,I want to find the inverse of a large upper triangular matrix where all its entries are 1. Is there some trick to it or do I have to compute it using the usual way?,I want to find the inverse of a large upper triangular matrix where all its entries are 1. Is there some trick to it or do I have to compute it using the usual way?,,"['matrices', 'inverse']"
51,Why must the determinant of a matrix with with integer entries be an integer?,Why must the determinant of a matrix with with integer entries be an integer?,,"Why must the determinant of a matrix with integer entries be an integer? Note: I know what a determinant of a matrix is, not sure how to explain this question. Is that because if the matrix is made with integers the determinant has to be an integer as well?","Why must the determinant of a matrix with integer entries be an integer? Note: I know what a determinant of a matrix is, not sure how to explain this question. Is that because if the matrix is made with integers the determinant has to be an integer as well?",,"['matrices', 'determinant']"
52,Can a symmetric matrix always be represented as the sum of a positive-definite and negative-definite matrix?,Can a symmetric matrix always be represented as the sum of a positive-definite and negative-definite matrix?,,I was wondering if it is possible to decompose any symmetric matrix into a positive definite and a negative definite component. I can't seem to think of a counterexample if the statement is false.,I was wondering if it is possible to decompose any symmetric matrix into a positive definite and a negative definite component. I can't seem to think of a counterexample if the statement is false.,,['matrices']
53,Is there a simple method to do LU decomposition by hand?,Is there a simple method to do LU decomposition by hand?,,"Today my professor in numerical analysis pointed out that in the exam we will probably have to do LU decomposition by hand. I understand how the decomposition works theoretically, but when it comes actually getting my hands dirty, I'm never sure, if I'm writing the row operation at the right place in the L matrix. Do you know a mnemonic, which allows one to efficiently compute the LU decomposition by hand?","Today my professor in numerical analysis pointed out that in the exam we will probably have to do LU decomposition by hand. I understand how the decomposition works theoretically, but when it comes actually getting my hands dirty, I'm never sure, if I'm writing the row operation at the right place in the L matrix. Do you know a mnemonic, which allows one to efficiently compute the LU decomposition by hand?",,"['matrices', 'algorithms']"
54,What does $A^{-1}=A^T$ have to do with “orthogonality”?,What does  have to do with “orthogonality”?,A^{-1}=A^T,"Whenever I read some use of the term “orthogonal”, I have been able to find some way in which it is at least metaphorically similar to the idea of two orthogonal lines in euclidean space. E.g. orthogonal random variables, etc. But I cannot see how $A^{-1}=A^T$ captures the idea of “orthogonality”. What is “orthogonal” about a matrix that satisfies this property?","Whenever I read some use of the term “orthogonal”, I have been able to find some way in which it is at least metaphorically similar to the idea of two orthogonal lines in euclidean space. E.g. orthogonal random variables, etc. But I cannot see how $A^{-1}=A^T$ captures the idea of “orthogonality”. What is “orthogonal” about a matrix that satisfies this property?",,"['matrices', 'terminology', 'intuition', 'orthogonality', 'orthogonal-matrices']"
55,Any complex number can be the eigenvalue of some non-negative matrix,Any complex number can be the eigenvalue of some non-negative matrix,,"Let $z\in\Bbb C$. Show that there exists a non-negative matrix $A$ (with entries $\geq 0$) such that $z$ is an eigenvalue of $A$. If $z$ is real, it is easy. Since, $a\geq 0$ is an eigenvalue of  $$\begin{pmatrix} a&0\\ 0&a \end{pmatrix};$$ while $a<0$ is an eigenvalue of  $$\begin{pmatrix} 0&-a\\ -a&0 \end{pmatrix}.$$ For complex $z$, we should need the rows of $A$ is greater than $3$...Con we construct it? Or could we prove the statement above by using some facts of non-negative matrices...","Let $z\in\Bbb C$. Show that there exists a non-negative matrix $A$ (with entries $\geq 0$) such that $z$ is an eigenvalue of $A$. If $z$ is real, it is easy. Since, $a\geq 0$ is an eigenvalue of  $$\begin{pmatrix} a&0\\ 0&a \end{pmatrix};$$ while $a<0$ is an eigenvalue of  $$\begin{pmatrix} 0&-a\\ -a&0 \end{pmatrix}.$$ For complex $z$, we should need the rows of $A$ is greater than $3$...Con we construct it? Or could we prove the statement above by using some facts of non-negative matrices...",,['matrices']
56,Is there an analytic expression for a number of elements inside a triangular matrix (with and without items on diagonal),Is there an analytic expression for a number of elements inside a triangular matrix (with and without items on diagonal),,"Is there an analytic expression for a number of items inside a triangular matrix (with and without items on diagonal)? I tried to solve this with a combinatorial analysis using this ""representation"" of the problem: ( _, _ ) = ( n, m ) where: ""n"" is the matrix row index, ""m"" is the matrix column index. m = { 0, 1, 2, ... n } but I don't have any further idea (I do not understand combinatorial analysis very well)","Is there an analytic expression for a number of items inside a triangular matrix (with and without items on diagonal)? I tried to solve this with a combinatorial analysis using this ""representation"" of the problem: ( _, _ ) = ( n, m ) where: ""n"" is the matrix row index, ""m"" is the matrix column index. m = { 0, 1, 2, ... n } but I don't have any further idea (I do not understand combinatorial analysis very well)",,['matrices']
57,Operator norm calculation for simple matrix [closed],Operator norm calculation for simple matrix [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Suppose $$ A = \left( \begin{array}{cc} 1 & 4 \\ 5 & 6 \end{array}\right) $$ How do I calculate $\|A\|_{\text{OP}}$? I know the definition of operator norm, but I am clueless on how to calculate it for real example like this. Can somebody please give me a step-by-step instruction on how to do this?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Suppose $$ A = \left( \begin{array}{cc} 1 & 4 \\ 5 & 6 \end{array}\right) $$ How do I calculate $\|A\|_{\text{OP}}$? I know the definition of operator norm, but I am clueless on how to calculate it for real example like this. Can somebody please give me a step-by-step instruction on how to do this?",,"['matrices', 'normed-spaces', 'matrix-norms', 'spectral-norm']"
58,trace of product of a diagonal and a matrix,trace of product of a diagonal and a matrix,,"I would like to know if anything can be said about the trace of a product of two matrices, where one matrix is a diagonal matrix, i.e., $$\text{trace}(DA)=...$$ Are there some bounds in terms of $\text{trace}(D)$ and $\text{trace}(A)$ ?","I would like to know if anything can be said about the trace of a product of two matrices, where one matrix is a diagonal matrix, i.e., $$\text{trace}(DA)=...$$ Are there some bounds in terms of $\text{trace}(D)$ and $\text{trace}(A)$ ?",,"['matrices', 'trace']"
59,An application of Vandermonde determinant,An application of Vandermonde determinant,,"Let $\lambda_1,\ldots,\lambda_n$ be complex numbers such that for each positive integer $k\geq 0$, $$\sum_{i=1}^n \lambda_i^k=0.$$ Here I am supposed to show that $\lambda_i=0$ for each $i\in 1,\ldots,n$. One of my friends said it sounds that I should use Vandermonde determinant. I tried to find an appropriate version of Vandermonde determinant that I may apply here, but I could not. I appreciate if you let me know a suitable version.","Let $\lambda_1,\ldots,\lambda_n$ be complex numbers such that for each positive integer $k\geq 0$, $$\sum_{i=1}^n \lambda_i^k=0.$$ Here I am supposed to show that $\lambda_i=0$ for each $i\in 1,\ldots,n$. One of my friends said it sounds that I should use Vandermonde determinant. I tried to find an appropriate version of Vandermonde determinant that I may apply here, but I could not. I appreciate if you let me know a suitable version.",,"['matrices', 'polynomials', 'complex-numbers']"
60,How valid is this concept or does this already have a name?,How valid is this concept or does this already have a name?,,"I was going through my school papers and found an interesting question, so I experimented a bit more and found out a pattern, so I made a formula for such matrices. $$A = \begin{bmatrix}x&-(x-1)\\x+1&-x\end{bmatrix}$$ where $x > 0$ is an integer $$A^n = \begin{cases} I,  & \text{if $n$ is even} \\ A, & \text{if $n$ is odd} \end{cases}$$ where $I$ is identity matrix of order 2. I just wanted to know if this has been found before or whether it has a name too or if there are some cases that does not obey this. Hope someone can format my question properly, I'm new to this community. Hope this is the correct way of putting things together too. Thanks!!","I was going through my school papers and found an interesting question, so I experimented a bit more and found out a pattern, so I made a formula for such matrices. where is an integer where is identity matrix of order 2. I just wanted to know if this has been found before or whether it has a name too or if there are some cases that does not obey this. Hope someone can format my question properly, I'm new to this community. Hope this is the correct way of putting things together too. Thanks!!","A = \begin{bmatrix}x&-(x-1)\\x+1&-x\end{bmatrix} x > 0 A^n =
\begin{cases}
I,  & \text{if n is even} \\
A, & \text{if n is odd}
\end{cases} I",['matrices']
61,Logarithm and tensor products,Logarithm and tensor products,,"We define Von Neumann Entropy for a density matrix $\rho$ (hermitian, positively defined, with trace 1) as : $S(\rho)=-tr(\rho \ln(\rho))$ Considering $\rho = \rho_1 \bigotimes \rho_2$, I want to show that $S(\rho)=S(\rho_1)+ S(\rho_2)$. I do not see how the following equality can be (where $\mathbb{Id}$ stands for the identity matrix with appropriate dimension): $\ln(\rho_1\bigotimes \mathbb{Id})= \ln(\rho_1)\bigotimes\mathbb{Id}$ Actually I don't understand how the definition of ln for matrices applies here. Could someone help me on this step ?","We define Von Neumann Entropy for a density matrix $\rho$ (hermitian, positively defined, with trace 1) as : $S(\rho)=-tr(\rho \ln(\rho))$ Considering $\rho = \rho_1 \bigotimes \rho_2$, I want to show that $S(\rho)=S(\rho_1)+ S(\rho_2)$. I do not see how the following equality can be (where $\mathbb{Id}$ stands for the identity matrix with appropriate dimension): $\ln(\rho_1\bigotimes \mathbb{Id})= \ln(\rho_1)\bigotimes\mathbb{Id}$ Actually I don't understand how the definition of ln for matrices applies here. Could someone help me on this step ?",,"['matrices', 'tensor-products']"
62,"If $A^2$ is diagonalizable, must $A$ be such as well?","If  is diagonalizable, must  be such as well?",A^2 A,"Given a diagonalizable matrix $A^2$, must the matrix $A$ be diagonalizable as well? I can prove that this is true for when $A\in M_{n\times n} (\mathbb{C})$ by using the theorem that the Minimal polynomial for $A^2$ is expressed as a multiplication of linear attributes, and we can simply take $\pm \sqrt{\lambda_i}$ and show that $A$'s minimal polynomial is also a multiplication of linear attributes, thus making $A$ diagonalizable as well. The problem is that I do now know whether or not this statement is correct for $A\in M_{n\times n} (\mathbb{R})$, I know my proof won't work for when $\lambda_i < 0$, but perhaps there is another proof for this? Or a counterexample?","Given a diagonalizable matrix $A^2$, must the matrix $A$ be diagonalizable as well? I can prove that this is true for when $A\in M_{n\times n} (\mathbb{C})$ by using the theorem that the Minimal polynomial for $A^2$ is expressed as a multiplication of linear attributes, and we can simply take $\pm \sqrt{\lambda_i}$ and show that $A$'s minimal polynomial is also a multiplication of linear attributes, thus making $A$ diagonalizable as well. The problem is that I do now know whether or not this statement is correct for $A\in M_{n\times n} (\mathbb{R})$, I know my proof won't work for when $\lambda_i < 0$, but perhaps there is another proof for this? Or a counterexample?",,['matrices']
63,Using QR algorithm to compute the SVD of a matrix,Using QR algorithm to compute the SVD of a matrix,,How to use the QR algorithm to compute the SVD of a matrix $X\in R^{m\times n}$? Is there any algorithm for doing that?,How to use the QR algorithm to compute the SVD of a matrix $X\in R^{m\times n}$? Is there any algorithm for doing that?,,"['matrices', 'eigenvalues-eigenvectors']"
64,$\det\left(I + A^TA^{-1}\right) = 2\left(1 + \operatorname{tr}\left(A^TA^{-1}\right)\right)$,,\det\left(I + A^TA^{-1}\right) = 2\left(1 + \operatorname{tr}\left(A^TA^{-1}\right)\right),Let $A$ be an invertible $3\times3$ matrix with complex values. Prove that: $$\det\left(I + A^TA^{-1}\right) = 2\left(1 + \operatorname{tr}\left(A^TA^{-1}\right)\right)$$ I've tried to solve this problem with characteristic polynomial but in the end I got stuck in an expression involvind Newton's identities regarding that polynomial. I cannot relate $A^TA^{-1}$ with characteristic polynomial.,Let $A$ be an invertible $3\times3$ matrix with complex values. Prove that: $$\det\left(I + A^TA^{-1}\right) = 2\left(1 + \operatorname{tr}\left(A^TA^{-1}\right)\right)$$ I've tried to solve this problem with characteristic polynomial but in the end I got stuck in an expression involvind Newton's identities regarding that polynomial. I cannot relate $A^TA^{-1}$ with characteristic polynomial.,,"['matrices', 'eigenvalues-eigenvectors', 'determinant']"
65,Least squares / residual sum of squares in closed form,Least squares / residual sum of squares in closed form,,In finding the Residual Sum of Squares (RSS) We have: \begin{equation} \hat{Y} = X^T\hat{\beta} \end{equation} where the parameter $\hat{\beta}$ will be used in estimating the output value of input vector $X^T$ as $\hat{Y}$ \begin{equation} RSS(\beta) = \sum_{i=1}^n (y_i - x_i^T\beta)^2 \end{equation} which in matrix form would be \begin{equation} RSS(\beta) = (y - X \beta)^T (y - X \beta) \end{equation} differentiating w.r.t $\beta$ we get \begin{equation} X^T(y - X\beta) = 0 \end{equation} My question is how is the last step done? How did the derivative get the last equation?,In finding the Residual Sum of Squares (RSS) We have: \begin{equation} \hat{Y} = X^T\hat{\beta} \end{equation} where the parameter $\hat{\beta}$ will be used in estimating the output value of input vector $X^T$ as $\hat{Y}$ \begin{equation} RSS(\beta) = \sum_{i=1}^n (y_i - x_i^T\beta)^2 \end{equation} which in matrix form would be \begin{equation} RSS(\beta) = (y - X \beta)^T (y - X \beta) \end{equation} differentiating w.r.t $\beta$ we get \begin{equation} X^T(y - X\beta) = 0 \end{equation} My question is how is the last step done? How did the derivative get the last equation?,,"['least-squares', 'matrices']"
66,Matrix exponential: $e^A\ge 0\iff a_{ij}\ge 0$ when $i\neq j$,Matrix exponential:  when,e^A\ge 0\iff a_{ij}\ge 0 i\neq j,Let $A$ be a $n$ by $n$ matrix. Prove that that $$a_{ij}\ge 0 \text{ whenever }i\neq j\iff e^A\text{ has all entries }\ge 0.$$ I'd like just a hint for now please.,Let $A$ be a $n$ by $n$ matrix. Prove that that $$a_{ij}\ge 0 \text{ whenever }i\neq j\iff e^A\text{ has all entries }\ge 0.$$ I'd like just a hint for now please.,,['matrices']
67,Approximate inverse matrix,Approximate inverse matrix,,"Does a method exist to calculate the approximate inverse matrix for a matrix whose elements are small? To be clearer, let's suppose the elements of the matrix $A$ to be function of a small parameter, i.e. $a_{ij} = a_{ij} (\epsilon)$. Is it possible to obtain the elements of the inverse $A^{-1}$ avoiding to calculate it before and then to study the limit for $\epsilon \rightarrow0$. For example could we expand the inverse in a series of $\epsilon$ whose terms can be calculated from $A$?","Does a method exist to calculate the approximate inverse matrix for a matrix whose elements are small? To be clearer, let's suppose the elements of the matrix $A$ to be function of a small parameter, i.e. $a_{ij} = a_{ij} (\epsilon)$. Is it possible to obtain the elements of the inverse $A^{-1}$ avoiding to calculate it before and then to study the limit for $\epsilon \rightarrow0$. For example could we expand the inverse in a series of $\epsilon$ whose terms can be calculated from $A$?",,"['matrices', 'approximation']"
68,If $A^2=B^2$ and $AB=BA$ then $\det(A+B)=0$,If  and  then,A^2=B^2 AB=BA \det(A+B)=0,"Let $A, B \in M_n(\mathbb{C})$ distinct matrices such that $A^2=B^2$ and $AB=BA$. Prove   $\det(A+B)=0$ From $A^2=B^2$ and $AB=BA$ we get $(A-B)(A+B)=0_n$ hence $\det(A+B)=0$ or $\det(A-B)=0$. Now suppose $A+B$ invertible. Then ${(A+B)}^2$ invertible therefore $A^2 + 2AB + B^2$ invertible and, from here, $A^2 + AB$ invertible, hence $A, B$ invertible. I can't get a contradiction for this supposition.","Let $A, B \in M_n(\mathbb{C})$ distinct matrices such that $A^2=B^2$ and $AB=BA$. Prove   $\det(A+B)=0$ From $A^2=B^2$ and $AB=BA$ we get $(A-B)(A+B)=0_n$ hence $\det(A+B)=0$ or $\det(A-B)=0$. Now suppose $A+B$ invertible. Then ${(A+B)}^2$ invertible therefore $A^2 + 2AB + B^2$ invertible and, from here, $A^2 + AB$ invertible, hence $A, B$ invertible. I can't get a contradiction for this supposition.",,['matrices']
69,Non-negative determinant of a block matrix,Non-negative determinant of a block matrix,,"Here's the problem I've been stuck on for some time now. Let $A,B \in M_n(\mathbb{R})$. Let $C=         \begin{bmatrix}         A & B \\         -B & A \\         \end{bmatrix} $ be a real $2n \times 2n$ matrix. Prove $\det(C) \geq 0$. What I've tried so far are: First I tried to write determinant of $C$ as the sum of $2^{2n}$ matrix determinants by expanding all rows of $C$ such that for each binary sequence of length $2n$ like $a = (a_1, a_2, ..., a_{2n})$, if $a_i = 0$ then the first $n$ entries of $i$-th row are zero and if $a_i = 1$ then the second $n$ entries of $i$-th row are zero. But couldn't come close to any answer. Second we know that determinant is the product of eigenvalues. The characteristic polynomial of $C$ has real coefficients hence its complex roots come in conjugate pairs and have positive product. What remains is to prove that each negative eigenvalue has even multiplicity which I couldn't prove. Any sort of hints and/or ideas are appreciated.","Here's the problem I've been stuck on for some time now. Let $A,B \in M_n(\mathbb{R})$. Let $C=         \begin{bmatrix}         A & B \\         -B & A \\         \end{bmatrix} $ be a real $2n \times 2n$ matrix. Prove $\det(C) \geq 0$. What I've tried so far are: First I tried to write determinant of $C$ as the sum of $2^{2n}$ matrix determinants by expanding all rows of $C$ such that for each binary sequence of length $2n$ like $a = (a_1, a_2, ..., a_{2n})$, if $a_i = 0$ then the first $n$ entries of $i$-th row are zero and if $a_i = 1$ then the second $n$ entries of $i$-th row are zero. But couldn't come close to any answer. Second we know that determinant is the product of eigenvalues. The characteristic polynomial of $C$ has real coefficients hence its complex roots come in conjugate pairs and have positive product. What remains is to prove that each negative eigenvalue has even multiplicity which I couldn't prove. Any sort of hints and/or ideas are appreciated.",,"['matrices', 'determinant']"
70,Are there any fields with a matrix representation other than $\mathbb{C}$?,Are there any fields with a matrix representation other than ?,\mathbb{C},"There has been a lot of research into matrix groups, and even matrix rings, but the only field that I have heard of that has a matrix representation is $\mathbb{C}$. Are there any other fields with a matrix representation? Also, if so, what is the name of the study of matrix representations for fields?","There has been a lot of research into matrix groups, and even matrix rings, but the only field that I have heard of that has a matrix representation is $\mathbb{C}$. Are there any other fields with a matrix representation? Also, if so, what is the name of the study of matrix representations for fields?",,"['matrices', 'field-theory', 'representation-theory']"
71,Computing explicitly the Riemannian Distance on $GL_n^+$,Computing explicitly the Riemannian Distance on,GL_n^+,"$\newcommand{\ep}{\epsilon}$ Let  $GL_n^+$ be the Lie group of invertible $n \times n$ matrices with positive determinant. In particular it's a connected open submanifold of the Euclidean space $\mathbb{R}^{n^2}$. Now consider it with the induced metric from $\mathbb{R}^{n^2}$. (So it's a Riemannian submanifold of $\mathbb{R}^{n^2}$). For clarity, this means endowing $GL_n^+ $ with the pullback metric of the Euclidean metric $e$ along the inclusion $ i:GL_n^+ \to (\mathbb{R}^{n^2},e) $. Explicitly: $g_z(X,Y) =tr(X^TY)$ , where $X,Y \in T_z(GL_n^+)$. Questions: (1) Is there an explicit formula for the Riemannian distance between two matrices $A,B \in  GL_n^+$? Conjecture: The Riemannian distance equals the Euclidean one. Attempted proof: Since $GL_n^+$ is open in $\mathbb{R}^{n^2}$, it follows that it's a totally geodesic submanifold. (That is, all it's geodesics are geodesics in $(\mathbb{R}^{n^2},e)$, i.e they are the usual straight lines in Euclidean space). $GL^+(\mathbb{R}^n)$ is open $\Rightarrow$ for any $A \in GL^+(\mathbb{R}^n)$, there is an Euclidean ball centered around it which is contained in $GL_n^+$. Hence, for all matrices close enough to $A$ their distance from $A$ is just the Euclidean one. (since the straight line beteen them is in our submanifold). Now consider $A,B \in GL_n^+$. Let $\alpha:[0,1] \to GL_n^+$ be the straight line path between them. Then: $$ \det(\alpha(t))=\det(A+t(B-A)) $$ is a polynomial in $t$ of degree $\le n$ . Hence, it has only finitely many zeroes. This implies there are no more than $n$ points $t_i$ where $\alpha(t_i)$ is not invertible. Hence, we only need to show we can make arbitrary small perturbations around each such 'bad' non-invertible matrix. This would imply the Riemannian distance equals the Euclidean one. It would be nice if someone could find a neat argument to show this maneuver is indeed possible. Update: This conjecture is false. The key point (as noted by Jason DeVito and loup blanc) is that the determinant is negative for non-negligible parts of the straight-line path $\alpha$. Now, by continuity argument, any path which approximates too closely the straight path must enter a region of negative determinant. It turns out that the behaviour depends on the number of sign changes of the determinant. Example for a case where the distance is Euclidean (a ""jump"" is possible): take $n=2$,$A,B=\pm Id$. Start with a path from $Id$ to $\begin{bmatrix}\ep & 0 \\ 0 & \ep\end{bmatrix}$. Then go via (1) $t \to \begin{bmatrix}\ep & -t \\ t & \ep\end{bmatrix}$ to $\begin{bmatrix}\ep & -\ep \\ \ep & \ep\end{bmatrix}$ ($t$ goes $0 \to \ep)$ . (2) $t \to \begin{bmatrix}t & -\ep \\ \ep & t\end{bmatrix}$ to $\begin{bmatrix}-\ep & -\ep \\ \ep & -\ep\end{bmatrix}$ ($t$ goes $\ep \to -\ep)$. (3) $t \to \begin{bmatrix}-\ep & -t \\ t & -\ep\end{bmatrix}$ to $\begin{bmatrix}-\ep & 0 \\ 0 & -\ep\end{bmatrix}$ ($t$ goes $\ep \to 0)$ . Now continue with straight line until reaching $-Id$. How much this maneuver cost us? The derivatives of the 3 broken straight paths we took were: $\begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix}, Id , \begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix}$. their norms are $\sqrt 2$. Hence, the total lenght is $\sqrt2 \cdot 4\ep$ which is arbitrarily small, as required. (Also, note that the determinant was always $t^2 + \ep^2 > 0$ so we stayed in $GL_n^+$). (2) Can we compute explicitly for a given $A \in GL_n^+$, it's distance from $SO(n)$? $dist(A,SO(n)) =\underset{X \in SO(n)}{\text{min}} d(A,X)$ ($SO(n)$ is the special orthogonal group and the minimum exists since $SO(n)$ is compact and $d$ is continuous) And who is the minimizer (the closest matrix to $A$ in $SO(n)$)? Is it unique? Note: Right or Left multiplication by elements of $SO(n)$ are isometries of $GL_n^+$ with the induced metric. Thus, $d$ is left (right)-$SO(n)$ invariant. In particular, if $ A = U\Sigma V^T $ is the SVD-dscomposition of $A$, then: $dist(A,SO(n)) = dist(\Sigma,SO(n)) $ , where $\Sigma$ is a square, diagonal matrix whose diagonal elements are the (strictly positive) singular values of $A$. So for the question of computing the distance from $SO(n)$ (and the minimizer) is reduced to matrices of this type. (i.e diagonal + positive entries).","$\newcommand{\ep}{\epsilon}$ Let  $GL_n^+$ be the Lie group of invertible $n \times n$ matrices with positive determinant. In particular it's a connected open submanifold of the Euclidean space $\mathbb{R}^{n^2}$. Now consider it with the induced metric from $\mathbb{R}^{n^2}$. (So it's a Riemannian submanifold of $\mathbb{R}^{n^2}$). For clarity, this means endowing $GL_n^+ $ with the pullback metric of the Euclidean metric $e$ along the inclusion $ i:GL_n^+ \to (\mathbb{R}^{n^2},e) $. Explicitly: $g_z(X,Y) =tr(X^TY)$ , where $X,Y \in T_z(GL_n^+)$. Questions: (1) Is there an explicit formula for the Riemannian distance between two matrices $A,B \in  GL_n^+$? Conjecture: The Riemannian distance equals the Euclidean one. Attempted proof: Since $GL_n^+$ is open in $\mathbb{R}^{n^2}$, it follows that it's a totally geodesic submanifold. (That is, all it's geodesics are geodesics in $(\mathbb{R}^{n^2},e)$, i.e they are the usual straight lines in Euclidean space). $GL^+(\mathbb{R}^n)$ is open $\Rightarrow$ for any $A \in GL^+(\mathbb{R}^n)$, there is an Euclidean ball centered around it which is contained in $GL_n^+$. Hence, for all matrices close enough to $A$ their distance from $A$ is just the Euclidean one. (since the straight line beteen them is in our submanifold). Now consider $A,B \in GL_n^+$. Let $\alpha:[0,1] \to GL_n^+$ be the straight line path between them. Then: $$ \det(\alpha(t))=\det(A+t(B-A)) $$ is a polynomial in $t$ of degree $\le n$ . Hence, it has only finitely many zeroes. This implies there are no more than $n$ points $t_i$ where $\alpha(t_i)$ is not invertible. Hence, we only need to show we can make arbitrary small perturbations around each such 'bad' non-invertible matrix. This would imply the Riemannian distance equals the Euclidean one. It would be nice if someone could find a neat argument to show this maneuver is indeed possible. Update: This conjecture is false. The key point (as noted by Jason DeVito and loup blanc) is that the determinant is negative for non-negligible parts of the straight-line path $\alpha$. Now, by continuity argument, any path which approximates too closely the straight path must enter a region of negative determinant. It turns out that the behaviour depends on the number of sign changes of the determinant. Example for a case where the distance is Euclidean (a ""jump"" is possible): take $n=2$,$A,B=\pm Id$. Start with a path from $Id$ to $\begin{bmatrix}\ep & 0 \\ 0 & \ep\end{bmatrix}$. Then go via (1) $t \to \begin{bmatrix}\ep & -t \\ t & \ep\end{bmatrix}$ to $\begin{bmatrix}\ep & -\ep \\ \ep & \ep\end{bmatrix}$ ($t$ goes $0 \to \ep)$ . (2) $t \to \begin{bmatrix}t & -\ep \\ \ep & t\end{bmatrix}$ to $\begin{bmatrix}-\ep & -\ep \\ \ep & -\ep\end{bmatrix}$ ($t$ goes $\ep \to -\ep)$. (3) $t \to \begin{bmatrix}-\ep & -t \\ t & -\ep\end{bmatrix}$ to $\begin{bmatrix}-\ep & 0 \\ 0 & -\ep\end{bmatrix}$ ($t$ goes $\ep \to 0)$ . Now continue with straight line until reaching $-Id$. How much this maneuver cost us? The derivatives of the 3 broken straight paths we took were: $\begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix}, Id , \begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix}$. their norms are $\sqrt 2$. Hence, the total lenght is $\sqrt2 \cdot 4\ep$ which is arbitrarily small, as required. (Also, note that the determinant was always $t^2 + \ep^2 > 0$ so we stayed in $GL_n^+$). (2) Can we compute explicitly for a given $A \in GL_n^+$, it's distance from $SO(n)$? $dist(A,SO(n)) =\underset{X \in SO(n)}{\text{min}} d(A,X)$ ($SO(n)$ is the special orthogonal group and the minimum exists since $SO(n)$ is compact and $d$ is continuous) And who is the minimizer (the closest matrix to $A$ in $SO(n)$)? Is it unique? Note: Right or Left multiplication by elements of $SO(n)$ are isometries of $GL_n^+$ with the induced metric. Thus, $d$ is left (right)-$SO(n)$ invariant. In particular, if $ A = U\Sigma V^T $ is the SVD-dscomposition of $A$, then: $dist(A,SO(n)) = dist(\Sigma,SO(n)) $ , where $\Sigma$ is a square, diagonal matrix whose diagonal elements are the (strictly positive) singular values of $A$. So for the question of computing the distance from $SO(n)$ (and the minimizer) is reduced to matrices of this type. (i.e diagonal + positive entries).",,"['matrices', 'metric-spaces', 'riemannian-geometry']"
72,How to compute time ordered Exponential?,How to compute time ordered Exponential?,,"So say you have a matrix dependent on a variable t: $A(t)$ How do you compute $e^{A(t)}$ ? It seems Sylvester's formula, my standard method of computing matrix exponentials can't be applied here given the varying nature of the matrix and furthermore the fact that it may not always have distinct eigenvalues.","So say you have a matrix dependent on a variable t: $A(t)$ How do you compute $e^{A(t)}$ ? It seems Sylvester's formula, my standard method of computing matrix exponentials can't be applied here given the varying nature of the matrix and furthermore the fact that it may not always have distinct eigenvalues.",,"['matrices', 'ordinary-differential-equations', 'multivariable-calculus', 'exponential-function', 'matrix-calculus']"
73,What kind of matrix $A$ satisfies $Ax\geq 0\Rightarrow x\geq 0$?,What kind of matrix  satisfies ?,A Ax\geq 0\Rightarrow x\geq 0,"$A\in \mathbb{R}^{n\times n}$ is an n-by-n matrix. $x=(x_1,x_2,\ldots ,x_n)\in \mathbb{R}^n$ is a vector. $x\geq 0$ means $x_i\geq 0,\forall i$. Q1: When $A$ satisfies what conditions, $\forall x\in\mathbb{R}^n ,x\geq 0\Rightarrow Ax\geq 0$? Q2: When $A$ satisfies what conditions, $\forall x\in\mathbb{R}^n ,Ax\geq 0\Rightarrow x\geq 0$? Q1 is solved by Martin Argerami(The matrix's entries are all positive). But what are the matrices in Q2? In Q2, the answer is not ""entries all positive"". The counterexample is $A=\begin{pmatrix} 1 & 1 \\ 1 & 1\end{pmatrix}$, $A\begin{pmatrix} -1 \\ 2 \end{pmatrix}\geq 0$. Let's see an example $A=\begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix}$.$A\begin{pmatrix} x \\ y \end{pmatrix} \geq 0\Rightarrow x\geq 0,y\geq 0$. So $A$ is in answer to Q2. What general properties of matrix $A$ would ensure Q2 be satisfied? Another obvious class of matrix in Q2 is diagnoal matrices with positive diagonal entries. Q2 is settled by Robert Israel($A^{-1}$ has entries all positive).","$A\in \mathbb{R}^{n\times n}$ is an n-by-n matrix. $x=(x_1,x_2,\ldots ,x_n)\in \mathbb{R}^n$ is a vector. $x\geq 0$ means $x_i\geq 0,\forall i$. Q1: When $A$ satisfies what conditions, $\forall x\in\mathbb{R}^n ,x\geq 0\Rightarrow Ax\geq 0$? Q2: When $A$ satisfies what conditions, $\forall x\in\mathbb{R}^n ,Ax\geq 0\Rightarrow x\geq 0$? Q1 is solved by Martin Argerami(The matrix's entries are all positive). But what are the matrices in Q2? In Q2, the answer is not ""entries all positive"". The counterexample is $A=\begin{pmatrix} 1 & 1 \\ 1 & 1\end{pmatrix}$, $A\begin{pmatrix} -1 \\ 2 \end{pmatrix}\geq 0$. Let's see an example $A=\begin{pmatrix} 2 & -1 \\ -1 & 2 \end{pmatrix}$.$A\begin{pmatrix} x \\ y \end{pmatrix} \geq 0\Rightarrow x\geq 0,y\geq 0$. So $A$ is in answer to Q2. What general properties of matrix $A$ would ensure Q2 be satisfied? Another obvious class of matrix in Q2 is diagnoal matrices with positive diagonal entries. Q2 is settled by Robert Israel($A^{-1}$ has entries all positive).",,['matrices']
74,How to construct a $2\times 2$ real matrix $A$ not equal to Identity such that $A^3=I$?,How to construct a  real matrix  not equal to Identity such that ?,2\times 2 A A^3=I,"How to construct a $2\times 2$ real matrix $A$ not equal to Identity such that $A^3=I$? There is a correspondence between the ring of complex numbers and the ring  of  $2\times2$ matrices (0 matrix is included!) i.e.,$$a+ib\leftrightarrow\begin{pmatrix}a&-b\\b&a\end{pmatrix}$$ Can I apply this result and construct such matrix?","How to construct a $2\times 2$ real matrix $A$ not equal to Identity such that $A^3=I$? There is a correspondence between the ring of complex numbers and the ring  of  $2\times2$ matrices (0 matrix is included!) i.e.,$$a+ib\leftrightarrow\begin{pmatrix}a&-b\\b&a\end{pmatrix}$$ Can I apply this result and construct such matrix?",,"['matrices', 'ring-theory']"
75,What is a big condition number for a matrix?,What is a big condition number for a matrix?,,"The condition number of a matrix is a measure of how close a matrix is to being singular. But, what is considered a big condition number?","The condition number of a matrix is a measure of how close a matrix is to being singular. But, what is considered a big condition number?",,"['matrices', 'inverse', 'numerical-linear-algebra', 'condition-number']"
76,Operator norm in Hilbert space [closed],Operator norm in Hilbert space [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $\mathcal{B}(F)$ the algebra of all bounded linear operators on an infinite-dimensional complex Hilbert space $F$. If $A\in \mathcal{B}(F)$, why $(\|A^n\|^{1/n})_n$ is a decreasing sequence?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Let $\mathcal{B}(F)$ the algebra of all bounded linear operators on an infinite-dimensional complex Hilbert space $F$. If $A\in \mathcal{B}(F)$, why $(\|A^n\|^{1/n})_n$ is a decreasing sequence?",,"['matrices', 'functional-analysis']"
77,When can we say that $A^{\mathrm T} B = B^{\mathrm T} A$?,When can we say that ?,A^{\mathrm T} B = B^{\mathrm T} A,"I was looking at the derivation of the normal equation from here . Now, the author has used the fact that $A^{\mathrm T} B = B^{\mathrm T} A$ to reach the step shown in the below image. Can anyone provide some information like, when is it true, or how we can prove it? $$J(\theta) = ((X\theta)^{\mathrm T} -y^{\mathrm T})(X\theta -y)$$ $$J(\theta) = (X\theta)^{\mathrm T} X\theta -\color{blue}{(X\theta)^{\mathrm T} y \color{black}{-} y^{\mathrm T} (X\theta)} +y^{\mathrm T} y$$ Note that $X\theta$ is a vector, and so is $y$ . So when we multiply one by another, it doesn't matter what the order is (as long as the dimensions work out). So we can further simplify: $$J(\theta) = \theta^{\mathrm T} X^{\mathrm T} X \theta -\color{blue}{2(X\theta)^{\mathrm T} y} +y^{\mathrm T} y$$","I was looking at the derivation of the normal equation from here . Now, the author has used the fact that to reach the step shown in the below image. Can anyone provide some information like, when is it true, or how we can prove it? Note that is a vector, and so is . So when we multiply one by another, it doesn't matter what the order is (as long as the dimensions work out). So we can further simplify:",A^{\mathrm T} B = B^{\mathrm T} A J(\theta) = ((X\theta)^{\mathrm T} -y^{\mathrm T})(X\theta -y) J(\theta) = (X\theta)^{\mathrm T} X\theta -\color{blue}{(X\theta)^{\mathrm T} y \color{black}{-} y^{\mathrm T} (X\theta)} +y^{\mathrm T} y X\theta y J(\theta) = \theta^{\mathrm T} X^{\mathrm T} X \theta -\color{blue}{2(X\theta)^{\mathrm T} y} +y^{\mathrm T} y,"['matrices', 'matrix-calculus', 'linear-regression', 'transpose']"
78,Is the zero matrix in reduced row echelon form?,Is the zero matrix in reduced row echelon form?,,Is this matrix in reduced row echelon form?  $3\times3$ matrix is: 0 0 0   0 0 0   0 0 0 I can say for other matrices but this one without 1s confuses me. Are $1$s optional in reduced row echelon form? I think they aren't. What do you think?,Is this matrix in reduced row echelon form?  $3\times3$ matrix is: 0 0 0   0 0 0   0 0 0 I can say for other matrices but this one without 1s confuses me. Are $1$s optional in reduced row echelon form? I think they aren't. What do you think?,,['matrices']
79,The definition of Affine Invariant Riemannian Metric (AIRM),The definition of Affine Invariant Riemannian Metric (AIRM),,"For any two symmetric positive definite (SPD) matrices $A$ and $B$ , the Affine Invariant Riemannian Metric (AIRM) between them is defined as [1], [2]: $d(A,B)=||\log (A^{-1/2}B A^{-1/2}) ||_F$ , where $\log(A)$ is the matrix logrithm of $A$ , and $||A||_F$ is the Frobenius norm of $A$ . On the other hand, it is also shown that (e.g., see [3]) $d(A,B)=||\log (A^{-1/2}B A^{-1/2}) ||_F = ||\log (A^{-1}B) ||_F$ . However, when I used Matlab function logm , I found that $||\log (A^{-1/2}B A^{-1/2}) ||_F \ne ||\log (A^{-1}B) ||_F$ at all (but their eigenvalues are the same). I was wondering if there is anything wrong with my understanding the definition of AIRM? Thanks very much! [1] R. Bhatia, Positive Definite Matrices. Princeton University Press, 2009. [2] X. Pennec, P. Fillard, and N. Ayache, “A Riemannian framework for tensor computing,” International Journal of Computer Vision, vol. 66, no. 1, pp. 41–66, 2006. [3] M. Moakher, “A differential geometric approach to the geometric mean of symmetric Positive-Definite matrices,” SIAM J. Matrix Anal. Appl., vol. 26, no. 3, pp. 735–747, 2005. [4] I. Horev, F. Yger, and M. Sugiyama, “Geometry-aware principal component analysis for symmetric positive definite matrices,” Machine Learning, 2017.","For any two symmetric positive definite (SPD) matrices and , the Affine Invariant Riemannian Metric (AIRM) between them is defined as [1], [2]: , where is the matrix logrithm of , and is the Frobenius norm of . On the other hand, it is also shown that (e.g., see [3]) . However, when I used Matlab function logm , I found that at all (but their eigenvalues are the same). I was wondering if there is anything wrong with my understanding the definition of AIRM? Thanks very much! [1] R. Bhatia, Positive Definite Matrices. Princeton University Press, 2009. [2] X. Pennec, P. Fillard, and N. Ayache, “A Riemannian framework for tensor computing,” International Journal of Computer Vision, vol. 66, no. 1, pp. 41–66, 2006. [3] M. Moakher, “A differential geometric approach to the geometric mean of symmetric Positive-Definite matrices,” SIAM J. Matrix Anal. Appl., vol. 26, no. 3, pp. 735–747, 2005. [4] I. Horev, F. Yger, and M. Sugiyama, “Geometry-aware principal component analysis for symmetric positive definite matrices,” Machine Learning, 2017.","A B d(A,B)=||\log (A^{-1/2}B A^{-1/2}) ||_F \log(A) A ||A||_F A d(A,B)=||\log (A^{-1/2}B A^{-1/2}) ||_F = ||\log (A^{-1}B) ||_F ||\log (A^{-1/2}B A^{-1/2}) ||_F \ne ||\log (A^{-1}B) ||_F","['matrices', 'riemannian-geometry']"
80,Is Kruskal rank and rank of a matrix the same?,Is Kruskal rank and rank of a matrix the same?,,"The definition of Kruskal rank : maximum value of $k$ such that any $k$ columns of a matrix $\textbf{A}$ are linearly independent, then $k$ is the Kruskal rank of matrix $\textbf{A}$. How is it different from rank of the matrix $\textbf{A}$ ? Can you give an example where Kruskal rank and rank of a matrix are different.","The definition of Kruskal rank : maximum value of $k$ such that any $k$ columns of a matrix $\textbf{A}$ are linearly independent, then $k$ is the Kruskal rank of matrix $\textbf{A}$. How is it different from rank of the matrix $\textbf{A}$ ? Can you give an example where Kruskal rank and rank of a matrix are different.",,"['matrices', 'matrix-rank']"
81,How to find a function mapping matrix indices?,How to find a function mapping matrix indices?,,"There are several posts dealing with this kind of topic, but I so much would like to know, how to tackle such little problems. This seems so logical and still for me it's so hard to crack. Given a n x n Matrix I would like to map indices using a function. For instance, mapping indices for the upper triangular matrix (without diagonal). Boundary conditions are $$     i = 0, …, n-2\\     j = 1, …, n-1\\     i < j $$ where i and j are the row and column indices. I'm looking for the function $f(i, j)$ or $f(i, j, n)$ which maps them to a flat vector. I tried to figure out, how the function should behave: f(i, j, n) = y 0, 1   → 0 0, 2   → 1 0, 3   → 2 0, n-1 → n-2  1, 2   → n-2 + 1 1, n-1 → n-2 + n-3  2, 3   → n-2 + n-3 + 1 2, 4   → n-2 + n-3 + 2 2, n-1 → n-2 + n-3 + n-4  n-3, n-2 → ? n-2, n-1 → ? but I'm just stuck somehow. How to get $f(i, j, n)$? Redirect me, if you got smth good, comprehensible on this. Important is the point of how, as in which steps to take, to get the desired function . Edit: Since in this case I want to map the upper triangular to a continuous order of indices y, i and j satisfy the condition i < j The desired function in this case is $$f(i, j, n) = {n \choose 2} - {n-i \choose 2} + (j-i-1)$$ but I dont know how to get there..","There are several posts dealing with this kind of topic, but I so much would like to know, how to tackle such little problems. This seems so logical and still for me it's so hard to crack. Given a n x n Matrix I would like to map indices using a function. For instance, mapping indices for the upper triangular matrix (without diagonal). Boundary conditions are $$     i = 0, …, n-2\\     j = 1, …, n-1\\     i < j $$ where i and j are the row and column indices. I'm looking for the function $f(i, j)$ or $f(i, j, n)$ which maps them to a flat vector. I tried to figure out, how the function should behave: f(i, j, n) = y 0, 1   → 0 0, 2   → 1 0, 3   → 2 0, n-1 → n-2  1, 2   → n-2 + 1 1, n-1 → n-2 + n-3  2, 3   → n-2 + n-3 + 1 2, 4   → n-2 + n-3 + 2 2, n-1 → n-2 + n-3 + n-4  n-3, n-2 → ? n-2, n-1 → ? but I'm just stuck somehow. How to get $f(i, j, n)$? Redirect me, if you got smth good, comprehensible on this. Important is the point of how, as in which steps to take, to get the desired function . Edit: Since in this case I want to map the upper triangular to a continuous order of indices y, i and j satisfy the condition i < j The desired function in this case is $$f(i, j, n) = {n \choose 2} - {n-i \choose 2} + (j-i-1)$$ but I dont know how to get there..",,['matrices']
82,matrix multiplication by columns,matrix multiplication by columns,,"Yo, I need some help with understanding matrix multiplication by columns. Consider the two matrices: $\left( \begin{array}{ccc} 1 & 2 & 3 \\ 6 & 5 & 4 \\ 7 & 8 & 9 \end{array} \right)  \left( \begin{array}{ccc} 3 & 2 & 1 \\ 4 & 5 & 6 \\ 9 & 8 & 7 \end{array} \right)  $ So I'm familiar with the standard algorithm where element $AB_{ij}$ is found by multiplying the $i^{th}$ row of A with the $j^{th}$ column of B. Apparently there is another way to multiply matrices where you work with whole columns of A to get the product AB. Does anyone know how to do that? If so, could you please provide a general algorithm? I've never heard of it and I can't find it anywhere.","Yo, I need some help with understanding matrix multiplication by columns. Consider the two matrices: $\left( \begin{array}{ccc} 1 & 2 & 3 \\ 6 & 5 & 4 \\ 7 & 8 & 9 \end{array} \right)  \left( \begin{array}{ccc} 3 & 2 & 1 \\ 4 & 5 & 6 \\ 9 & 8 & 7 \end{array} \right)  $ So I'm familiar with the standard algorithm where element $AB_{ij}$ is found by multiplying the $i^{th}$ row of A with the $j^{th}$ column of B. Apparently there is another way to multiply matrices where you work with whole columns of A to get the product AB. Does anyone know how to do that? If so, could you please provide a general algorithm? I've never heard of it and I can't find it anywhere.",,['matrices']
83,Arbitrary non-integer power of a matrix,Arbitrary non-integer power of a matrix,,"does there exist the notion of a non-integer power of a matrix? This seems to be accessible via semigroup-theory, yet I have not seen an actual definition so far. I am not too firm at this right now, but I am curious. Can you give me a sketch of the definition and provide with some introductory information?","does there exist the notion of a non-integer power of a matrix? This seems to be accessible via semigroup-theory, yet I have not seen an actual definition so far. I am not too firm at this right now, but I am curious. Can you give me a sketch of the definition and provide with some introductory information?",,"['analysis', 'matrices']"
84,Derivative of $\sqrt{AA^T}$ with respect to $A$,Derivative of  with respect to,\sqrt{AA^T} A,"How to find the derivative of the function $f: M_n(\mathbb{R})\to M_n(\mathbb{R}), A\mapsto \sqrt{AA^T},$ where $A^T$ is the transpose of the matrix $A$ ? \begin{align}     Df_V(A) & = \lim_{h\to 0}\dfrac{f(A+hV)-f(A)}{h}\\             & = \lim_{h\to 0}\dfrac{\sqrt{(A+hV)(A+hV)^T}-\sqrt{AA^T}}{h}\\             & = \lim_{h\to 0} \dfrac{\sqrt{AA^T+hAV^T+hVA^T+h^2VV^T}-\sqrt{AA^T}}{h}   \end{align} Now, what should I do?","How to find the derivative of the function where is the transpose of the matrix ? Now, what should I do?","f: M_n(\mathbb{R})\to M_n(\mathbb{R}), A\mapsto \sqrt{AA^T}, A^T A \begin{align}
    Df_V(A) & = \lim_{h\to 0}\dfrac{f(A+hV)-f(A)}{h}\\
            & = \lim_{h\to 0}\dfrac{\sqrt{(A+hV)(A+hV)^T}-\sqrt{AA^T}}{h}\\
            & = \lim_{h\to 0} \dfrac{\sqrt{AA^T+hAV^T+hVA^T+h^2VV^T}-\sqrt{AA^T}}{h}
  \end{align}","['matrices', 'derivatives', 'matrix-calculus']"
85,Matrix equation $A^2+A=I$ when $\det(A) = 1$,Matrix equation  when,A^2+A=I \det(A) = 1,I have to solve the following problem: find the matrix $A \in M_{n \times n}(\mathbb{R})$ such that: $$A^2+A=I$$ and $\det(A)=1$. How many of these matrices can be found when $n$ is given? Thanks in advance.,I have to solve the following problem: find the matrix $A \in M_{n \times n}(\mathbb{R})$ such that: $$A^2+A=I$$ and $\det(A)=1$. How many of these matrices can be found when $n$ is given? Thanks in advance.,,"['matrices', 'determinant', 'matrix-equations']"
86,Is the controllability Gramian always positive definite?,Is the controllability Gramian always positive definite?,,"I am trying to understand the balanced truncation algorithm and have some trouble distinguishing between controllability matrix and controllability Gramian. If my understanding is correct, a linear time invariant system $\dot x(t) = Ax(t) + Bu$ is controllable if the controllability matrix $$P = \begin{bmatrix} B & AB & A^2B & \dots & A^{n-1}B\end{bmatrix}$$ has full row rank. Then we have the controllability Gramian $$\textit{P} = \int^{t_1}_{t_0} e^{At}BB^Te^{A^Tt} \, \mathrm d t$$ If the controllability matrix $P$ does not satisfy the full row rank requirement and the system is not controllable, is the Gramian still positive definite, nonsingular, hermitian and all that? In other words, can I proceed to the Cholesky decomposition with a non-controllable system's controllability Gramian? Any literature stating this would also be much appreciated. Thank you.","I am trying to understand the balanced truncation algorithm and have some trouble distinguishing between controllability matrix and controllability Gramian. If my understanding is correct, a linear time invariant system is controllable if the controllability matrix has full row rank. Then we have the controllability Gramian If the controllability matrix does not satisfy the full row rank requirement and the system is not controllable, is the Gramian still positive definite, nonsingular, hermitian and all that? In other words, can I proceed to the Cholesky decomposition with a non-controllable system's controllability Gramian? Any literature stating this would also be much appreciated. Thank you.","\dot x(t) = Ax(t) + Bu P = \begin{bmatrix} B & AB & A^2B & \dots & A^{n-1}B\end{bmatrix} \textit{P} = \int^{t_1}_{t_0} e^{At}BB^Te^{A^Tt} \, \mathrm d t P","['matrices', 'control-theory', 'positive-definite', 'optimal-control', 'linear-control']"
87,Eigenvalues of two related symmetric matrices,Eigenvalues of two related symmetric matrices,,"What are the eigenvalues of following $n \times n$ matrices $A$ and $B$? $A=\begin{bmatrix} 0 & 0 & 1 & 1 & 1 & 1 & 1 & \cdots & 1 & 0 \\  0 & 0 & 1 & 0 & 0 & 0 & 0 & \cdots & 0 & 0 \\  1 & 1 & 0 & 1 & 0 & 0 & 0 & \cdots & 0 & 0 \\  1 & 0 & 1 & 0 & 1 & 0 & 0 & \cdots & 0 & 0 \\  1 & 0 & 0 & 1 & 0 & 1 & 0 & \cdots & 0 & 0 \\  1 & 0 & 0 & 0 & 1 & 0 & 1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots &  \vdots & \ddots & \vdots & \vdots & \vdots \\  1 & 0 & 0 & 0 & 0 & 0 & 0 & \ddots & 1 & 0 \\  1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 1 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 1 & 0 \end{bmatrix} $ Where, $a_{ij}=1$,if $i=j-1$ or $i=j+1$ for all $i=3,4,5...n-1$ $a_{13}=a_{14}=...=a_{1n-1}=1$, $a_{31}=a_{41}=...=a_{(n-1)1}=1$ $a_{23}=a_{n(n-1)}=1$ As it is symmertic matrix all of its eigenvalues are real. $B=\begin{bmatrix} 0 & 0 & 1 & 1 & 1 & 1 & 1 & \cdots & 1 & 1 \\  0 & 0 & 1 & 0 & 0 & 0 & 0 & \cdots & 0 & 0 \\  1 & 1 & 0 & 1 & 0 & 0 & 0 & \cdots & 0 & 0 \\  1 & 0 & 1 & 0 & 1 & 0 & 0 & \cdots & 0 & 0 \\  1 & 0 & 0 & 1 & 0 & 1 & 0 & \cdots & 0 & 0 \\  1 & 0 & 0 & 0 & 1 & 0 & 1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots &  \vdots & \ddots & \vdots & \vdots & \vdots \\  1 & 0 & 0 & 0 & 0 & 0 & 0 & \ddots & 1 & 0 \\  1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 1 \\  1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 1 & 0 \end{bmatrix} $ Where, $b_{ij}=1$,if $i=j-1$ or $i=j+1$ for all $i=3,4,5...n-1$ $b_{13}=b_{14}=...=b_{1n}=1$, $b_{31}=b_{41}=...=a_{(n-1)1}=1$ $b_{23}=b_{n(n-1)}=b_{n1}=1$ As it is symmertic matrix all of its eigenvalues are real.","What are the eigenvalues of following $n \times n$ matrices $A$ and $B$? $A=\begin{bmatrix} 0 & 0 & 1 & 1 & 1 & 1 & 1 & \cdots & 1 & 0 \\  0 & 0 & 1 & 0 & 0 & 0 & 0 & \cdots & 0 & 0 \\  1 & 1 & 0 & 1 & 0 & 0 & 0 & \cdots & 0 & 0 \\  1 & 0 & 1 & 0 & 1 & 0 & 0 & \cdots & 0 & 0 \\  1 & 0 & 0 & 1 & 0 & 1 & 0 & \cdots & 0 & 0 \\  1 & 0 & 0 & 0 & 1 & 0 & 1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots &  \vdots & \ddots & \vdots & \vdots & \vdots \\  1 & 0 & 0 & 0 & 0 & 0 & 0 & \ddots & 1 & 0 \\  1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 1 \\  0 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 1 & 0 \end{bmatrix} $ Where, $a_{ij}=1$,if $i=j-1$ or $i=j+1$ for all $i=3,4,5...n-1$ $a_{13}=a_{14}=...=a_{1n-1}=1$, $a_{31}=a_{41}=...=a_{(n-1)1}=1$ $a_{23}=a_{n(n-1)}=1$ As it is symmertic matrix all of its eigenvalues are real. $B=\begin{bmatrix} 0 & 0 & 1 & 1 & 1 & 1 & 1 & \cdots & 1 & 1 \\  0 & 0 & 1 & 0 & 0 & 0 & 0 & \cdots & 0 & 0 \\  1 & 1 & 0 & 1 & 0 & 0 & 0 & \cdots & 0 & 0 \\  1 & 0 & 1 & 0 & 1 & 0 & 0 & \cdots & 0 & 0 \\  1 & 0 & 0 & 1 & 0 & 1 & 0 & \cdots & 0 & 0 \\  1 & 0 & 0 & 0 & 1 & 0 & 1 & \cdots & 0 & 0 \\ \vdots & \vdots & \vdots & \vdots & \ddots &  \vdots & \ddots & \vdots & \vdots & \vdots \\  1 & 0 & 0 & 0 & 0 & 0 & 0 & \ddots & 1 & 0 \\  1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 0 & 1 \\  1 & 0 & 0 & 0 & 0 & 0 & 0 & \cdots & 1 & 0 \end{bmatrix} $ Where, $b_{ij}=1$,if $i=j-1$ or $i=j+1$ for all $i=3,4,5...n-1$ $b_{13}=b_{14}=...=b_{1n}=1$, $b_{31}=b_{41}=...=a_{(n-1)1}=1$ $b_{23}=b_{n(n-1)}=b_{n1}=1$ As it is symmertic matrix all of its eigenvalues are real.",,"['matrices', 'eigenvalues-eigenvectors']"
88,"With matrices, why does the Lie bracket agree with the matrix commutator?","With matrices, why does the Lie bracket agree with the matrix commutator?",,"I gave an answer to Is there a group between $SO(2,\mathbb{R})$ and $SL(2,\mathbb{R})$? which was not popular. Meanwhile, I found myself at a loss when wishing to explain why a matrix Lie group had, as a tangent space at $I,$ not only a vector space but a Lie algebra, with bracket agreeing with $[A,B] = AB-BA$ from matrix multiplication, that the matrix exponential of a bracket puts us back in the same group, and so on. I got what I think is a proof in one direction using this: http://en.wikipedia.org/wiki/Baker%E2%80%93Campbell%E2%80%93Hausdorff_formula#Matrix_Lie_group_illustration http://en.wikipedia.org/wiki/Matrix_group I do have Matrix Groups by Curtis, he does not go this far. I also have Foundations of Differentiable Manifolds and Lie Groups by Warner, which seems to do large portions of this. Still, I am not entirely satisfied: with matrices, why does the Lie bracket agree with the matrix bracket, given a matrix group why is the identity tangent space both a vector space and a Lie algebra, given a tangent space at the identity that is a Lie algebra under matrix bracket, why do we get a group under matrix exponential?","I gave an answer to Is there a group between $SO(2,\mathbb{R})$ and $SL(2,\mathbb{R})$? which was not popular. Meanwhile, I found myself at a loss when wishing to explain why a matrix Lie group had, as a tangent space at not only a vector space but a Lie algebra, with bracket agreeing with from matrix multiplication, that the matrix exponential of a bracket puts us back in the same group, and so on. I got what I think is a proof in one direction using this: http://en.wikipedia.org/wiki/Baker%E2%80%93Campbell%E2%80%93Hausdorff_formula#Matrix_Lie_group_illustration http://en.wikipedia.org/wiki/Matrix_group I do have Matrix Groups by Curtis, he does not go this far. I also have Foundations of Differentiable Manifolds and Lie Groups by Warner, which seems to do large portions of this. Still, I am not entirely satisfied: with matrices, why does the Lie bracket agree with the matrix bracket, given a matrix group why is the identity tangent space both a vector space and a Lie algebra, given a tangent space at the identity that is a Lie algebra under matrix bracket, why do we get a group under matrix exponential?","I, [A,B] = AB-BA","['matrices', 'reference-request', 'lie-groups', 'lie-algebras']"
89,"Does anyone remember a paper or talk: ""Some matrices I have known""?","Does anyone remember a paper or talk: ""Some matrices I have known""?",,"Quite a few years ago I seem to have read a paper or heard a good talk with the title ""Some matrices I have known"". Does anyone recall that, or can give a reference?","Quite a few years ago I seem to have read a paper or heard a good talk with the title ""Some matrices I have known"". Does anyone recall that, or can give a reference?",,"['matrices', 'reference-request']"
90,Irreducible Representations of Matrix Algebras,Irreducible Representations of Matrix Algebras,,"I am currently reading the book Spin Geometry by Lawson/Michelsohn to understand Dirac Operators and related topics. At some point it uses representation theory to classify Clifford Algebras. In particular this book states in I,§5. Theorem 5.6 on p.31: Let $K=\mathbb{R}, \mathbb{C}$ or $\mathbb{H}$ and consider the ring $K(n)$ of $n \times n$ $K$-matrices as an algebra over $\mathbb{R}$. Then the natural representation $\rho$ of $K(n)$ on the vector space $K^n$ is, up to equivalence, the only irreducible representation of $K(n)$. The algebra $K(n) \oplus K(n)$ has exactly two equivalence classes of irreducible representations. They are given by    $$ \rho_1(\varphi_1,\varphi_2) = \rho(\varphi_1) \text{ and } \rho_2(\varphi_1,\varphi_2) =\rho(\varphi_2) $$   acting on $K(n)$. I am perfectly fine with the statement, but I would like to see a proof of that. Lawson/Michelsohn claim that this follows from the fact that the algebras $K(n)$ are simple and that simple algebras have only one irreducible representation up to equivalence. For the details the reader is referred to Lang: Algebra . Since this book as nearly 1000 pages I would like to ask, if someone could be a little more precise? Of course I would also be happy with a direct proof of the claim or a reference to any other readably textbook containing a proof. I must point out that I have no prior experience in the field of representation theory.","I am currently reading the book Spin Geometry by Lawson/Michelsohn to understand Dirac Operators and related topics. At some point it uses representation theory to classify Clifford Algebras. In particular this book states in I,§5. Theorem 5.6 on p.31: Let $K=\mathbb{R}, \mathbb{C}$ or $\mathbb{H}$ and consider the ring $K(n)$ of $n \times n$ $K$-matrices as an algebra over $\mathbb{R}$. Then the natural representation $\rho$ of $K(n)$ on the vector space $K^n$ is, up to equivalence, the only irreducible representation of $K(n)$. The algebra $K(n) \oplus K(n)$ has exactly two equivalence classes of irreducible representations. They are given by    $$ \rho_1(\varphi_1,\varphi_2) = \rho(\varphi_1) \text{ and } \rho_2(\varphi_1,\varphi_2) =\rho(\varphi_2) $$   acting on $K(n)$. I am perfectly fine with the statement, but I would like to see a proof of that. Lawson/Michelsohn claim that this follows from the fact that the algebras $K(n)$ are simple and that simple algebras have only one irreducible representation up to equivalence. For the details the reader is referred to Lang: Algebra . Since this book as nearly 1000 pages I would like to ask, if someone could be a little more precise? Of course I would also be happy with a direct proof of the claim or a reference to any other readably textbook containing a proof. I must point out that I have no prior experience in the field of representation theory.",,"['matrices', 'representation-theory']"
91,Show that a matrix is invertible with norm less than one,Show that a matrix is invertible with norm less than one,,"If $\|G\| < 1$, then show that $I-G$ is invertible. This can be proved by contradiction: If $I-G$ is singular, then 1 is an eigenvalue of $I-I+G$. So if the matrix norm is induced the 2-norm, $\| G\|$ is at least 1 since the largest singular value of a matrix is not less than its eigenvalue in absolute value. I have two questions: 1) Why does 1 have to be an eigenvalue of $I-A$ if A is singular? and why is the largest singular value of a matrix not less than its eigenvalue in absolute value? 2) I am learning iterative methods in class and i don't believe we have come acrossed eigenvalues yet; is there any other ways to prove this? Thanks!","If $\|G\| < 1$, then show that $I-G$ is invertible. This can be proved by contradiction: If $I-G$ is singular, then 1 is an eigenvalue of $I-I+G$. So if the matrix norm is induced the 2-norm, $\| G\|$ is at least 1 since the largest singular value of a matrix is not less than its eigenvalue in absolute value. I have two questions: 1) Why does 1 have to be an eigenvalue of $I-A$ if A is singular? and why is the largest singular value of a matrix not less than its eigenvalue in absolute value? 2) I am learning iterative methods in class and i don't believe we have come acrossed eigenvalues yet; is there any other ways to prove this? Thanks!",,"['matrices', 'numerical-methods', 'eigenvalues-eigenvectors', 'normed-spaces', 'euclidean-algorithm']"
92,When is the equicorrelation matrix positive-definite?,When is the equicorrelation matrix positive-definite?,,"Show that the matrix, $$ A= \begin{bmatrix} 1	& \rho	& \dots	 & \rho      \\ \rho	& 1 	& \dots  & \rho 	  \\ \vdots	& \rho 	& \ddots & \vdots \\ \rho 	& \dots & \rho	 & 1 \end{bmatrix} $$ is positive-definite if and only if $\frac{-1}{n-1}<\rho<1$. I know that one way would be to show that all eigenvalues are positive but I couldn't produce anything usefuol yet. I would be glad for some ideas or rough proof sketch.","Show that the matrix, $$ A= \begin{bmatrix} 1	& \rho	& \dots	 & \rho      \\ \rho	& 1 	& \dots  & \rho 	  \\ \vdots	& \rho 	& \ddots & \vdots \\ \rho 	& \dots & \rho	 & 1 \end{bmatrix} $$ is positive-definite if and only if $\frac{-1}{n-1}<\rho<1$. I know that one way would be to show that all eigenvalues are positive but I couldn't produce anything usefuol yet. I would be glad for some ideas or rough proof sketch.",,"['matrices', 'statistics', 'eigenvalues-eigenvectors', 'matrix-decomposition', 'positive-definite']"
93,determinant of a matrix with binomial coefficient entries,determinant of a matrix with binomial coefficient entries,,"I trying to prove a statement, which boils down to showing that the determinant of a specific matrix is nonzero. I use the convention that $\binom{n}{k} = 0$ if $k > n$ or $k < 0$. Let $k,l$ be natural numbers such that $k \le l$. Then the $n\times n$-Matrix $A$ is defined to have the entries $a_{ij} = \binom{l}{k +i - j}$. So it looks like  $A = \left( \begin{array}{cccccc} \binom{l}{k} &  \cdots & \binom{l}{0} & 0 & \cdots & 0 \\  \vdots & \ddots &  & \ddots & \ddots &  \vdots \\ \binom{l}{l} & & \ddots & & \ddots   & 0  \\ 0& \ddots && \ddots & & \binom{l}{0}\\ \vdots&\ddots&\ddots&&\ddots&\vdots\\ 0&\cdots&0& \binom{l}{l} & \cdots & \binom{l}{k}  \end{array}\right)$.  Clearly the cases $k = l$ and $k = 0$ are trivial, since $A$ is then triangular. My first idea was to use the formula  $\binom{r}{s} = \binom{r-1}{s-1} + \binom{r}{s}$ and add columns/rows to each other. But that does not work out that well... So if anyone has any ideas, or this matrix is known to be invertible, I would be very thankful.","I trying to prove a statement, which boils down to showing that the determinant of a specific matrix is nonzero. I use the convention that $\binom{n}{k} = 0$ if $k > n$ or $k < 0$. Let $k,l$ be natural numbers such that $k \le l$. Then the $n\times n$-Matrix $A$ is defined to have the entries $a_{ij} = \binom{l}{k +i - j}$. So it looks like  $A = \left( \begin{array}{cccccc} \binom{l}{k} &  \cdots & \binom{l}{0} & 0 & \cdots & 0 \\  \vdots & \ddots &  & \ddots & \ddots &  \vdots \\ \binom{l}{l} & & \ddots & & \ddots   & 0  \\ 0& \ddots && \ddots & & \binom{l}{0}\\ \vdots&\ddots&\ddots&&\ddots&\vdots\\ 0&\cdots&0& \binom{l}{l} & \cdots & \binom{l}{k}  \end{array}\right)$.  Clearly the cases $k = l$ and $k = 0$ are trivial, since $A$ is then triangular. My first idea was to use the formula  $\binom{r}{s} = \binom{r-1}{s-1} + \binom{r}{s}$ and add columns/rows to each other. But that does not work out that well... So if anyone has any ideas, or this matrix is known to be invertible, I would be very thankful.",,['matrices']
94,Adding a diagonal matrix to a product of transpose of a matrix and itself is always invertible,Adding a diagonal matrix to a product of transpose of a matrix and itself is always invertible,,"I am asking this question in context to Regularization/Ridge Regression Let's say that there is a Matrix A of dimension n x d , where n is the number of rows and d is the number of columns ( n may or may not be larger than d ) Consequently, we cannot say if A T A is Invertible or not, irrespective of what is n or d Let's say we have a diagonal matrix D (with diagonal elements > 0) and if we add it to A T A as follows - D + A T A Can we say that the resulting matrix will always be invertible, irrespective whether n is larger or smaller than d ? OR can we say that adding a diagonal matrix to any matrix converts it into a full rank matrix? I have read in the literature that the addition of a diagonal matrix D to A T A 'regularizes' A T A and it becomes invertible, irrespective whether n is larger or smaller than d , but I am looking for a formal proof to it.","I am asking this question in context to Regularization/Ridge Regression Let's say that there is a Matrix A of dimension n x d , where n is the number of rows and d is the number of columns ( n may or may not be larger than d ) Consequently, we cannot say if A T A is Invertible or not, irrespective of what is n or d Let's say we have a diagonal matrix D (with diagonal elements > 0) and if we add it to A T A as follows - D + A T A Can we say that the resulting matrix will always be invertible, irrespective whether n is larger or smaller than d ? OR can we say that adding a diagonal matrix to any matrix converts it into a full rank matrix? I have read in the literature that the addition of a diagonal matrix D to A T A 'regularizes' A T A and it becomes invertible, irrespective whether n is larger or smaller than d , but I am looking for a formal proof to it.",,"['matrices', 'transpose', 'regularization']"
95,Finding the logarithm of a matrix?,Finding the logarithm of a matrix?,,"Find $B$ if $A=e^B$ and  $A=\begin{bmatrix} 2&1&0\\ 0&2&0\\ 0&0&4\\ \end{bmatrix}$. Besides, I would be very happy if give some general remark(Best approach). I have seen the wiki article on log of a matrix but it was too complicated(for me).","Find $B$ if $A=e^B$ and  $A=\begin{bmatrix} 2&1&0\\ 0&2&0\\ 0&0&4\\ \end{bmatrix}$. Besides, I would be very happy if give some general remark(Best approach). I have seen the wiki article on log of a matrix but it was too complicated(for me).",,"['matrices', 'logarithms']"
96,What exacty is the role played by Jacobian or Wronskian?,What exacty is the role played by Jacobian or Wronskian?,,"In many of our derivations or in differential equations we come across the terms Jacobian or Wronskian. For example, to check the linear independence of solutions of differential equations, we ensure that the Wronskian is non zero. What role do these play? What do they actually account for?","In many of our derivations or in differential equations we come across the terms Jacobian or Wronskian. For example, to check the linear independence of solutions of differential equations, we ensure that the Wronskian is non zero. What role do these play? What do they actually account for?",,"['matrices', 'ordinary-differential-equations', 'determinant']"
97,A Fermat's FLT look-like for matrices.,A Fermat's FLT look-like for matrices.,,"I was wonderwing about a matrix equation (in some way similar to a very well known theorem :P). Find all positive integers $n,m$ such that there exists $X$, $Y$, $Z$ matrices $n\times n$, non-singular, such that: $$X^m+Y^m=Z^m$$ Disclaimer: $X$, $Y$, $Z$ must have integer entries. Also, they may have an inverse which is not necessarily in $\mathbb{Z}^{n\times n}$. I made some approachs (I don't know how to spoil, so if you want to think it yourself stop reading): As my intuition says to me that there will always be solutions for $n>1$, then, it suffices to show there will be solutions for $n=2$ and $n=3$ (because then you can form a matrix composed by matrices $2\times 2$ and $3\times 3$ in its diagonal). I showed that for all $m$ odd, and $n=2$ there is a solution. Can someone give me a hand with the other cases? I would be very grateful.","I was wonderwing about a matrix equation (in some way similar to a very well known theorem :P). Find all positive integers $n,m$ such that there exists $X$, $Y$, $Z$ matrices $n\times n$, non-singular, such that: $$X^m+Y^m=Z^m$$ Disclaimer: $X$, $Y$, $Z$ must have integer entries. Also, they may have an inverse which is not necessarily in $\mathbb{Z}^{n\times n}$. I made some approachs (I don't know how to spoil, so if you want to think it yourself stop reading): As my intuition says to me that there will always be solutions for $n>1$, then, it suffices to show there will be solutions for $n=2$ and $n=3$ (because then you can form a matrix composed by matrices $2\times 2$ and $3\times 3$ in its diagonal). I showed that for all $m$ odd, and $n=2$ there is a solution. Can someone give me a hand with the other cases? I would be very grateful.",,"['matrices', 'number-theory']"
98,Matrix Equation $A^3-3A=\begin{pmatrix}-7 & -9\\ 3 & 2\end{pmatrix}$,Matrix Equation,A^3-3A=\begin{pmatrix}-7 & -9\\ 3 & 2\end{pmatrix},How can I solve in $\mathcal{M}_{2}(\mathbb{Z})$ the equation $$A^3-3A=\begin{pmatrix}-7 & -9\\ 3 & 2\end{pmatrix}?$$ I try to use $$A^2-Tr(A)A+detA\cdot I_2=O_2$$ but I don't still obtain anything. thanks.,How can I solve in $\mathcal{M}_{2}(\mathbb{Z})$ the equation $$A^3-3A=\begin{pmatrix}-7 & -9\\ 3 & 2\end{pmatrix}?$$ I try to use $$A^2-Tr(A)A+detA\cdot I_2=O_2$$ but I don't still obtain anything. thanks.,,"['matrices', 'matrix-equations']"
99,Vector Matrix Differentiation (to maximize function),Vector Matrix Differentiation (to maximize function),,"how would I calculate the derivative of the following. I want to know the derivative so that I can maximise it. $$     \frac{x^TAx}{x^TBx} $$ Both the matricies A and B are symmetric. I know the derivative of $\frac{d}{dx}x^TAx = 2Ax$. Haven't been very successful applying the quotient rule to the above though. Appreciate the help. Thanks! EDIT: In response to ""What goes wrong when applying the chain rule"". We know that: $$     \frac{d}{dx}\frac{u}{v} = \frac{vu' - uv'}{v^2} $$ Which would give me:  $$    \frac{2x^TBxAx - 2x^TAxBx}{x^TBx^2} \, or \, \frac{2Axx^TBx - 2Bxx^TAx}{(x^TBx)^2} $$ In the first case the dimensions don't agree. In the second they do, but I don't want to assume that it's correct just because the dimensions agree. If it is correct then please do let me know!","how would I calculate the derivative of the following. I want to know the derivative so that I can maximise it. $$     \frac{x^TAx}{x^TBx} $$ Both the matricies A and B are symmetric. I know the derivative of $\frac{d}{dx}x^TAx = 2Ax$. Haven't been very successful applying the quotient rule to the above though. Appreciate the help. Thanks! EDIT: In response to ""What goes wrong when applying the chain rule"". We know that: $$     \frac{d}{dx}\frac{u}{v} = \frac{vu' - uv'}{v^2} $$ Which would give me:  $$    \frac{2x^TBxAx - 2x^TAxBx}{x^TBx^2} \, or \, \frac{2Axx^TBx - 2Bxx^TAx}{(x^TBx)^2} $$ In the first case the dimensions don't agree. In the second they do, but I don't want to assume that it's correct just because the dimensions agree. If it is correct then please do let me know!",,"['matrices', 'multivariable-calculus']"
