,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Dealing with absolute value in the process of solving differential equations,Dealing with absolute value in the process of solving differential equations,,"Currently learning how to solve linear differential equations using the idea of the Product Rule of differentiation and finding the integrating factor. I keep encountering the same problem of not being sure how to deal with absolute values that appear in the process of reaching to a solution. For example: $$\frac{dy}{dt}-\frac{1}{t+1}y(t)=4t^2+4t$$ Integrating factor:  $$g(t)=-\frac{1}{t+1}$$ $$M(t)=e^{\int g(t)dt}$$ $$M(t)=e^{\int-\frac{1}{t+1}dt}$$ $$M(t)=e^{-\ln |t+1|}$$ $$M(t)=\frac{1}{|t+1|}$$ Solving the equation: $$\frac{dy}{dt}-\frac{1}{t+1}y(t)=4t^2+4t$$ $$M(t)\frac{dy}{dt}-M(t)\frac{1}{t+1}y(t)=M(t)(4t^2+4t)$$ $$\left(\frac{1}{|t+1|}\right)\frac{dy}{dt}-\left(\frac{1}{|t+1|}\right)\left(\frac{1}{t+1}\right)y(t)=\left(\frac{1}{|t+1|}\right)(4t^2+4t)$$ How do I finish solving this equation? (I've never completely understood the concept of absolute value, so when dealing with problems like this I don't know how to get rid of it.)","Currently learning how to solve linear differential equations using the idea of the Product Rule of differentiation and finding the integrating factor. I keep encountering the same problem of not being sure how to deal with absolute values that appear in the process of reaching to a solution. For example: $$\frac{dy}{dt}-\frac{1}{t+1}y(t)=4t^2+4t$$ Integrating factor:  $$g(t)=-\frac{1}{t+1}$$ $$M(t)=e^{\int g(t)dt}$$ $$M(t)=e^{\int-\frac{1}{t+1}dt}$$ $$M(t)=e^{-\ln |t+1|}$$ $$M(t)=\frac{1}{|t+1|}$$ Solving the equation: $$\frac{dy}{dt}-\frac{1}{t+1}y(t)=4t^2+4t$$ $$M(t)\frac{dy}{dt}-M(t)\frac{1}{t+1}y(t)=M(t)(4t^2+4t)$$ $$\left(\frac{1}{|t+1|}\right)\frac{dy}{dt}-\left(\frac{1}{|t+1|}\right)\left(\frac{1}{t+1}\right)y(t)=\left(\frac{1}{|t+1|}\right)(4t^2+4t)$$ How do I finish solving this equation? (I've never completely understood the concept of absolute value, so when dealing with problems like this I don't know how to get rid of it.)",,"['integration', 'ordinary-differential-equations', 'derivatives', 'absolute-value']"
1,Calculus problem on functions for finding $a$,Calculus problem on functions for finding,a,"Question: Given a function $$f(x)=(60073-x^{10})^{1/10}$$ and $$f'(2)=\frac{1}{f'(a)},$$ where $a$ is a positive integer, find $a$. My approach: I can proceed with the problem by differentiating the equation and finding $f'(x)$ and then put $2$ and $a$. But I was wondering if there is any shorter method to solve these problems since these problems was generally given in competitive examinations. Any help would be appreciated.","Question: Given a function $$f(x)=(60073-x^{10})^{1/10}$$ and $$f'(2)=\frac{1}{f'(a)},$$ where $a$ is a positive integer, find $a$. My approach: I can proceed with the problem by differentiating the equation and finding $f'(x)$ and then put $2$ and $a$. But I was wondering if there is any shorter method to solve these problems since these problems was generally given in competitive examinations. Any help would be appreciated.",,['calculus']
2,Finding a power series for $\frac{1}{(1+x)^2}$,Finding a power series for,\frac{1}{(1+x)^2},"I'm trying to find a power series representation of: $$\frac{1}{(1+x)^2}$$ I know the following: $$\frac{1}{1+x} = \sum_{n=0}^{\infty}(-1)^nx^n$$ My understanding is that if I differentiate the left and right sides of the equation, I'll get (close to) my answer. Left side: $$\frac{d}{dx}\frac{1}{1+x} = \frac{-1}{(1+x)^2}$$ Right side: $$\frac{d}{dx}\sum_{n=0}^{\infty}(-1)^nx^{n} = \sum_{n=0}^{\infty}(-1)^nnx^{n-1}$$ Since the left side has a $-1$ on top and the original equation doesn't, my understanding is that I need to multiply the right side (the summation) by $-1$ to compensate: $$-\sum_{n=0}^{\infty}(-1)^nnx^{n-1}$$ When I check the answer to the original equation in my text (and Wolfram Alpha), it gives the power series as: $$\sum_{n=0}^{\infty}(-1)^n(n+1)x^{n}$$ Can anyone help me understand where I've gone astray?","I'm trying to find a power series representation of: $$\frac{1}{(1+x)^2}$$ I know the following: $$\frac{1}{1+x} = \sum_{n=0}^{\infty}(-1)^nx^n$$ My understanding is that if I differentiate the left and right sides of the equation, I'll get (close to) my answer. Left side: $$\frac{d}{dx}\frac{1}{1+x} = \frac{-1}{(1+x)^2}$$ Right side: $$\frac{d}{dx}\sum_{n=0}^{\infty}(-1)^nx^{n} = \sum_{n=0}^{\infty}(-1)^nnx^{n-1}$$ Since the left side has a $-1$ on top and the original equation doesn't, my understanding is that I need to multiply the right side (the summation) by $-1$ to compensate: $$-\sum_{n=0}^{\infty}(-1)^nnx^{n-1}$$ When I check the answer to the original equation in my text (and Wolfram Alpha), it gives the power series as: $$\sum_{n=0}^{\infty}(-1)^n(n+1)x^{n}$$ Can anyone help me understand where I've gone astray?",,"['derivatives', 'power-series']"
3,Minimum length of axes intercept of a line passing through a fixed point,Minimum length of axes intercept of a line passing through a fixed point,,"I encountered this problem: A straight line passes through a fixed point $(h,k) \ (h,k>0)$ and intersects the coordinate axes at $P(a,0)$ and $Q(0,b)$. Show that the minimum length of $PQ$ is $(h^{2/3}+k^{2/3})^{3/2}$. Now, I proceeded like this: If a line passes through $(h,k)$ then its equation is $$\frac{y-k}{x-h}=m.$$ If it passes through $(a,0)$ and $(0,b)$ then we have $a=h-\frac{k}{m}$ and $b=k+mh$. Length of $PQ$ is $\sqrt{a^2+b^2}$. We can put the value of $a$ and $b$ in the expression and then differentiate w.r.t. $m$, to find out the minimum value. But after equating the first differential to $0$, I found two roots of $m$. As the values of $h$ and $k$ can be positive or negative, I cannot put them in the second differential to find out which one is minimum and which one is maximum. What am I doing wrong? and, Is there any other, simpler method that can be used?","I encountered this problem: A straight line passes through a fixed point $(h,k) \ (h,k>0)$ and intersects the coordinate axes at $P(a,0)$ and $Q(0,b)$. Show that the minimum length of $PQ$ is $(h^{2/3}+k^{2/3})^{3/2}$. Now, I proceeded like this: If a line passes through $(h,k)$ then its equation is $$\frac{y-k}{x-h}=m.$$ If it passes through $(a,0)$ and $(0,b)$ then we have $a=h-\frac{k}{m}$ and $b=k+mh$. Length of $PQ$ is $\sqrt{a^2+b^2}$. We can put the value of $a$ and $b$ in the expression and then differentiate w.r.t. $m$, to find out the minimum value. But after equating the first differential to $0$, I found two roots of $m$. As the values of $h$ and $k$ can be positive or negative, I cannot put them in the second differential to find out which one is minimum and which one is maximum. What am I doing wrong? and, Is there any other, simpler method that can be used?",,"['geometry', 'derivatives']"
4,What is the derivative of $(2x)^{4x}$?,What is the derivative of ?,(2x)^{4x},"This is quite simple. I know. I am having a problem when comparing my answer to online calculators like Symbolab and such. $$\begin{array}{rll} y &= **(2x)^{4x}** & \text{equation}\\ \ln(y) &= \ln((2x)^{4x}) &\text{ take ln of both sides to bring 4x out front}\\ \ln(y) &= 4x  \ln(2x) & \text{ use log property}\\ 1/y * y' &= 4\ln(2x) + 4x  (1/2x)  2 & \text{ use product and chain rule}\\ y' &= y ( 4\ln(2x) + 4x (1/2x)  2 ) &\text{ multiply both sides by y}\\ y' &= 2x^{4x}( 4\ln(2x) + 4) & \text{simplify }4x*2 = 8x / 2x = 4 \\ y' &= 8x^{4x}\ln(2x) + 8x^{4x} &\text{further simplify}\\ y' &= 8x^{4x}(\ln(2x) + 1).& \end{array}$$ However, the answer on symbolab gives $8x^{4x}(\ln(x) + 1)$. <--- Disregard that. That was based on my incorrect input of the first line. Am I wrong? If so, how?","This is quite simple. I know. I am having a problem when comparing my answer to online calculators like Symbolab and such. $$\begin{array}{rll} y &= **(2x)^{4x}** & \text{equation}\\ \ln(y) &= \ln((2x)^{4x}) &\text{ take ln of both sides to bring 4x out front}\\ \ln(y) &= 4x  \ln(2x) & \text{ use log property}\\ 1/y * y' &= 4\ln(2x) + 4x  (1/2x)  2 & \text{ use product and chain rule}\\ y' &= y ( 4\ln(2x) + 4x (1/2x)  2 ) &\text{ multiply both sides by y}\\ y' &= 2x^{4x}( 4\ln(2x) + 4) & \text{simplify }4x*2 = 8x / 2x = 4 \\ y' &= 8x^{4x}\ln(2x) + 8x^{4x} &\text{further simplify}\\ y' &= 8x^{4x}(\ln(2x) + 1).& \end{array}$$ However, the answer on symbolab gives $8x^{4x}(\ln(x) + 1)$. <--- Disregard that. That was based on my incorrect input of the first line. Am I wrong? If so, how?",,"['calculus', 'derivatives', 'logarithms']"
5,Deciding monotonic nature of $h(x)$ given another function $f(x)$,Deciding monotonic nature of  given another function,h(x) f(x),"Question: If $f(x)$ is a strictly increasing function, then $h(x)=f(x)-a(f(x))^2+a(f(x))^3$ is a non-monotonic function for what set of values of $a$? Here both $f(x)$ and $h(x)$ take only real values. My attempt: For $h(x)$ to be monotonic, it's derivative should either be increasing or decreasing for all $x$. So, by putting the derivative of the function as positive for all $x$ and considering the fact that $f'(x)$ is also positive, I got the set of values of a between $(0,3)$. However the answer excludes all of these values. Please provide a detailed solution. All help is appreciated.","Question: If $f(x)$ is a strictly increasing function, then $h(x)=f(x)-a(f(x))^2+a(f(x))^3$ is a non-monotonic function for what set of values of $a$? Here both $f(x)$ and $h(x)$ take only real values. My attempt: For $h(x)$ to be monotonic, it's derivative should either be increasing or decreasing for all $x$. So, by putting the derivative of the function as positive for all $x$ and considering the fact that $f'(x)$ is also positive, I got the set of values of a between $(0,3)$. However the answer excludes all of these values. Please provide a detailed solution. All help is appreciated.",,"['calculus', 'derivatives']"
6,"How to evaluate $\frac{1}{2\pi}\int \frac{1}{(1-\alpha \cos \omega)^2 +\alpha^2\sin^2 \omega } \, d\omega$",How to evaluate,"\frac{1}{2\pi}\int \frac{1}{(1-\alpha \cos \omega)^2 +\alpha^2\sin^2 \omega } \, d\omega","May I please get help to solve the following expression, $$\frac 1 {2\pi}\int \frac 1 {(1-\alpha \cos \omega)^2 +\alpha^2\sin^2 \omega} \, d\omega$$ I have tried several ways, but couldn't get through. Thanks in advance.","May I please get help to solve the following expression, $$\frac 1 {2\pi}\int \frac 1 {(1-\alpha \cos \omega)^2 +\alpha^2\sin^2 \omega} \, d\omega$$ I have tried several ways, but couldn't get through. Thanks in advance.",,"['integration', 'derivatives', 'definite-integrals', 'trigonometric-integrals']"
7,Values of local maxima are countable for a $C^1$ function,Values of local maxima are countable for a  function,C^1,"Prove or disprove: Let $f:[a,b]\rightarrow \mathbb{R}$ be a $C^1$ function, $C:=\{f(x) \mid f'(x)=0\}$. So $|C| \le \aleph_0$. My try: I've shown that the set of the x-values (and the set of the points) of the strict local maxima is countable by selecting rational numbers and using their cardinality. However, I cannot use this fact for the set of the values...What can I do? Thanks in advance.","Prove or disprove: Let $f:[a,b]\rightarrow \mathbb{R}$ be a $C^1$ function, $C:=\{f(x) \mid f'(x)=0\}$. So $|C| \le \aleph_0$. My try: I've shown that the set of the x-values (and the set of the points) of the strict local maxima is countable by selecting rational numbers and using their cardinality. However, I cannot use this fact for the set of the values...What can I do? Thanks in advance.",,"['calculus', 'derivatives', 'cardinals', 'maxima-minima']"
8,4-point-like central finite difference for second partial derivatives,4-point-like central finite difference for second partial derivatives,,"To numerically approximate the second partial derivatives, and get the Hessian matrix, one may use ""2-point"" central finite difference formulas: $$H_{xy}(x,y) \approx \frac{f(x+h,y+h)-f(x+h,y-h)-f(x-h,y+h)+f(x-h,y-h)}{4h^2}\tag{1}$$ for the off-diagonal elements, and $$H_{xx}(x,y) \approx \frac{f(x+h,y)-2f(x,y)+f(x-h,y)}{h^2}\tag{2}$$ formulas for the diagonal elements. (I am simplifying here, this generalizes for any number of dimensions, and uses the same displacement for all variables) It may be sometimes desirable to further improve the accuracy of these central difference formulas. For univariate functions this is conveniently done via the 4-point formula (aka. 5-point stencil in one dimension): $$f''(x)     \approx \frac{-f(x+2 h)+16 f(x+h)-30 f(x) + 16 f(x-h) - f(x-2h)}{12 h^2}$$ However, I have been unable the find the ""4-point"" equivalent of $(1)$ and $(2)$ anywhere online.","To numerically approximate the second partial derivatives, and get the Hessian matrix, one may use ""2-point"" central finite difference formulas: $$H_{xy}(x,y) \approx \frac{f(x+h,y+h)-f(x+h,y-h)-f(x-h,y+h)+f(x-h,y-h)}{4h^2}\tag{1}$$ for the off-diagonal elements, and $$H_{xx}(x,y) \approx \frac{f(x+h,y)-2f(x,y)+f(x-h,y)}{h^2}\tag{2}$$ formulas for the diagonal elements. (I am simplifying here, this generalizes for any number of dimensions, and uses the same displacement for all variables) It may be sometimes desirable to further improve the accuracy of these central difference formulas. For univariate functions this is conveniently done via the 4-point formula (aka. 5-point stencil in one dimension): $$f''(x)     \approx \frac{-f(x+2 h)+16 f(x+h)-30 f(x) + 16 f(x-h) - f(x-2h)}{12 h^2}$$ However, I have been unable the find the ""4-point"" equivalent of $(1)$ and $(2)$ anywhere online.",,"['derivatives', 'numerical-methods', 'partial-derivative', 'finite-differences']"
9,What's the relationship between $\frac{\text{d}^n\gamma}{\text{d}s^n}$ and $\left(\gamma\circ s^{-1}\right)^{(n)}|_{s(t)}$?,What's the relationship between  and ?,\frac{\text{d}^n\gamma}{\text{d}s^n} \left(\gamma\circ s^{-1}\right)^{(n)}|_{s(t)},"Let $\gamma : I\subset\mathbb{R}\to\mathbb{R}^n$ be a $C^\infty$ curve such that $\|\gamma'\|>0$ everywhere. We let $s(t)$ be the arclength of $\gamma$ at time $t$, which has a smooth inverse everywhere (where defined), meaning $\gamma\circ s^{-1}$ is a unit-speed reparametrization of $\gamma$ (that is, $\|(\gamma\circ s^{-1})'\| = 1$). Disclaimer: Everywhere I use a $'$ symbol, that indicates differentiation with respect to $t$. Now, it's easy to see that $\left( \gamma\circ s^{-1} \right)'|_{s(t)}$ (that is, the derivative of $(\gamma\circ s^{-1})$ with respect to $t$, evaluated at $t=s(t)$) is equal to $\frac{\text{d}\gamma}{\text{d}s}$. In fact both are simply $1$, but for good measure $$ \left.(\gamma\circ s^{-1})'\right|_{s(t)} = \left.(\gamma'\circ s^{-1})(s^{-1})'\right|_{s(t)} = \left.\frac{\gamma'\circ s^{-1}}{s'\circ s^{-1}}\right|_{s(t)} = \frac{\gamma'}{s'} = \frac{\text{d}\gamma/\text{d}t}{\text{d}s/\text{d}t} = \frac{\text{d}\gamma}{\text{d}s} $$ Above we used the inverse function theorem to write $(s^{-1})' = \frac1{s'\circ s^{-1}}$. Similarly, $(s^{-1})'' = -\frac{s''\circ s^{-1}}{(s'\circ s^{-1})^3}$, and $s'' = \frac{\text{d}}{\text{d}t}\|\gamma'\| = \frac{\left\langle\gamma',\gamma''\right\rangle}{\|\gamma'\|}$. We can try to extend this for higher order derivatives. First we get $$ \left.(\gamma\circ s^{-1})''\right|_{s(t)} = \kappa(t) = \frac{\gamma''\left\langle\gamma',\gamma'\right\rangle - \gamma'\left\langle\gamma',\gamma''\right\rangle}{\|\gamma'\|^4} $$ (in $\mathbb{R}^3$ you can simplify this significantly using the triple vector product formula, but we will leave it as this). For $\frac{\text{d}^2\gamma}{\text{d}s^2}$ we have $$ \frac{\text{d}}{\text{d}s}\left(\frac{\text{d}\gamma}{\text{d}s}\right) = \frac{\text{d}}{\text{d}s}\left(\frac{\text{d}\gamma/\text{d}t}{\text{d}s/\text{d}t}\right) = \frac{\text{d}\left(\frac{\text{d}\gamma/\text{d}t}{\text{d}s/\text{d}t}\right)/\text{d}t}{\text{d}s/\text{d}t} $$ The numerator is given by $$ \begin{aligned} \frac{\text{d}}{\text{d}t}\left(\frac{\text{d}\gamma/\text{d}t}{\text{d}s/\text{d}t}\right) & = \frac{(\text{d}\gamma/\text{d}t)'(\text{d}s/\text{d}t)-(\text{d}\gamma/\text{d}t)(\text{d}s/\text{d}t)'}{(\text{d}s/\text{d}t)^2} \\ & = \frac{\gamma''\|\gamma'\|-\gamma'\left\langle\gamma',\gamma''\right\rangle/\|\gamma'\|}{\|\gamma'\|^2} \\ & = \frac{\gamma''\left\langle\gamma',\gamma'\right\rangle - \gamma'\left\langle\gamma',\gamma''\right\rangle}{\|\gamma'\|^3} \end{aligned} $$ and so we see that $$ \frac{\text{d}^2\gamma}{\text{d}s^2} = \frac{\gamma''\left\langle\gamma',\gamma'\right\rangle - \gamma'\left\langle\gamma',\gamma''\right\rangle}{\|\gamma'\|^4} = \left.(\gamma\circ s^{-1})''\right|_{s(t)} $$ I imagine this continues for higher order derivatives, and I'm almost certain I'm going about this in a really, really, really roundabout way, but I don't have an intuition for what the relationship between the objects $\frac{\text{d}^n\gamma}{\text{d}s^n}$ and $(\gamma\circ s^{-1})^{(n)}(s(t))$ actually are . Can someone give me a thorough explanation as to why these two objects should be the same (or if they aren't, why they aren't, and how they're related), so that I can skip the heavy computation each time I need to use either of the two?","Let $\gamma : I\subset\mathbb{R}\to\mathbb{R}^n$ be a $C^\infty$ curve such that $\|\gamma'\|>0$ everywhere. We let $s(t)$ be the arclength of $\gamma$ at time $t$, which has a smooth inverse everywhere (where defined), meaning $\gamma\circ s^{-1}$ is a unit-speed reparametrization of $\gamma$ (that is, $\|(\gamma\circ s^{-1})'\| = 1$). Disclaimer: Everywhere I use a $'$ symbol, that indicates differentiation with respect to $t$. Now, it's easy to see that $\left( \gamma\circ s^{-1} \right)'|_{s(t)}$ (that is, the derivative of $(\gamma\circ s^{-1})$ with respect to $t$, evaluated at $t=s(t)$) is equal to $\frac{\text{d}\gamma}{\text{d}s}$. In fact both are simply $1$, but for good measure $$ \left.(\gamma\circ s^{-1})'\right|_{s(t)} = \left.(\gamma'\circ s^{-1})(s^{-1})'\right|_{s(t)} = \left.\frac{\gamma'\circ s^{-1}}{s'\circ s^{-1}}\right|_{s(t)} = \frac{\gamma'}{s'} = \frac{\text{d}\gamma/\text{d}t}{\text{d}s/\text{d}t} = \frac{\text{d}\gamma}{\text{d}s} $$ Above we used the inverse function theorem to write $(s^{-1})' = \frac1{s'\circ s^{-1}}$. Similarly, $(s^{-1})'' = -\frac{s''\circ s^{-1}}{(s'\circ s^{-1})^3}$, and $s'' = \frac{\text{d}}{\text{d}t}\|\gamma'\| = \frac{\left\langle\gamma',\gamma''\right\rangle}{\|\gamma'\|}$. We can try to extend this for higher order derivatives. First we get $$ \left.(\gamma\circ s^{-1})''\right|_{s(t)} = \kappa(t) = \frac{\gamma''\left\langle\gamma',\gamma'\right\rangle - \gamma'\left\langle\gamma',\gamma''\right\rangle}{\|\gamma'\|^4} $$ (in $\mathbb{R}^3$ you can simplify this significantly using the triple vector product formula, but we will leave it as this). For $\frac{\text{d}^2\gamma}{\text{d}s^2}$ we have $$ \frac{\text{d}}{\text{d}s}\left(\frac{\text{d}\gamma}{\text{d}s}\right) = \frac{\text{d}}{\text{d}s}\left(\frac{\text{d}\gamma/\text{d}t}{\text{d}s/\text{d}t}\right) = \frac{\text{d}\left(\frac{\text{d}\gamma/\text{d}t}{\text{d}s/\text{d}t}\right)/\text{d}t}{\text{d}s/\text{d}t} $$ The numerator is given by $$ \begin{aligned} \frac{\text{d}}{\text{d}t}\left(\frac{\text{d}\gamma/\text{d}t}{\text{d}s/\text{d}t}\right) & = \frac{(\text{d}\gamma/\text{d}t)'(\text{d}s/\text{d}t)-(\text{d}\gamma/\text{d}t)(\text{d}s/\text{d}t)'}{(\text{d}s/\text{d}t)^2} \\ & = \frac{\gamma''\|\gamma'\|-\gamma'\left\langle\gamma',\gamma''\right\rangle/\|\gamma'\|}{\|\gamma'\|^2} \\ & = \frac{\gamma''\left\langle\gamma',\gamma'\right\rangle - \gamma'\left\langle\gamma',\gamma''\right\rangle}{\|\gamma'\|^3} \end{aligned} $$ and so we see that $$ \frac{\text{d}^2\gamma}{\text{d}s^2} = \frac{\gamma''\left\langle\gamma',\gamma'\right\rangle - \gamma'\left\langle\gamma',\gamma''\right\rangle}{\|\gamma'\|^4} = \left.(\gamma\circ s^{-1})''\right|_{s(t)} $$ I imagine this continues for higher order derivatives, and I'm almost certain I'm going about this in a really, really, really roundabout way, but I don't have an intuition for what the relationship between the objects $\frac{\text{d}^n\gamma}{\text{d}s^n}$ and $(\gamma\circ s^{-1})^{(n)}(s(t))$ actually are . Can someone give me a thorough explanation as to why these two objects should be the same (or if they aren't, why they aren't, and how they're related), so that I can skip the heavy computation each time I need to use either of the two?",,"['derivatives', 'differential-geometry', 'curves', 'arc-length']"
10,"Show that $f(x,y) = (x^2y-\frac13y^3, \frac13x^3-xy^2)$ is totally differentiable and calculate its derivative.",Show that  is totally differentiable and calculate its derivative.,"f(x,y) = (x^2y-\frac13y^3, \frac13x^3-xy^2)","I'm working on the problem Show that $f(x,y) = (x^2y-\frac13y^3, \frac13x^3-xy^2)$ is totally differentiable and calculate its derivative. And arived at the answer $Df(x,y)=(x^2+2xy-y^2, x^2-2xy-y^2)$. But I have doubts about the method I used to get there so I'd love someone to verify my work. (In specific, I'm not entirely certain I'm allowed to just casually ignore all terms with an $h$ in them.) Solution: Consider $$\lim_{h\rightarrow 0} \frac{|f(x+h, y+h) - f(x,y) - A(h,h)|}{|(h,h)|}=0.$$ For conveinience, we'll focus our attention to the enumerator of this fraction and leave out the limit. (Leaving out a few algebraic steps for conciseness) We can then write this as $$\left|(x^2y+2xyh+yh^2+hx^2+h^3-\frac13y^3-y^2h-yh^2-\frac13h^3, \frac13 x^3+x^2h+xh^2+h^3-xy^2-2xyh-xh^2-hy^2-2yh^2-h^3) - (x^2y-\frac13y^3, \frac13x^3-xy^2)-A(h,h)\right|.$$ Note how all terms which are not multiplied by $h$ at least once cancel. This means that we can write this as $$\left|(h,h)\right|\left|(2xy+yh+x^2+h^2-y^2-yh-\frac13h^2, x^2+h^2-2xy-y^2-2yh-h^2)-  A\right|$$ But remembering our limit, we know that we're going to have to divide by $|(h,h)|$ and that $h\rightarrow 0$ so that we have that $$|(2xy+x^2-y^2, x^2-2xy-y^2) - A| = 0,$$ so $A = (x^2+2xy-y^2, x^2-2xy-y^2)$. Another question: The place I got this from says that the problem should take anywhere from 10 to 15 minutes to solve, but with this method it takes longer. What alternative methods should I consider in solving these kinds of problems, or what should my first line of attack be? For context, another they had listed within the same time frame was: Show that $f(x,y) = |x+\pi|^3e^{3y}$ is totally differentiable and calculate its gradient.","I'm working on the problem Show that $f(x,y) = (x^2y-\frac13y^3, \frac13x^3-xy^2)$ is totally differentiable and calculate its derivative. And arived at the answer $Df(x,y)=(x^2+2xy-y^2, x^2-2xy-y^2)$. But I have doubts about the method I used to get there so I'd love someone to verify my work. (In specific, I'm not entirely certain I'm allowed to just casually ignore all terms with an $h$ in them.) Solution: Consider $$\lim_{h\rightarrow 0} \frac{|f(x+h, y+h) - f(x,y) - A(h,h)|}{|(h,h)|}=0.$$ For conveinience, we'll focus our attention to the enumerator of this fraction and leave out the limit. (Leaving out a few algebraic steps for conciseness) We can then write this as $$\left|(x^2y+2xyh+yh^2+hx^2+h^3-\frac13y^3-y^2h-yh^2-\frac13h^3, \frac13 x^3+x^2h+xh^2+h^3-xy^2-2xyh-xh^2-hy^2-2yh^2-h^3) - (x^2y-\frac13y^3, \frac13x^3-xy^2)-A(h,h)\right|.$$ Note how all terms which are not multiplied by $h$ at least once cancel. This means that we can write this as $$\left|(h,h)\right|\left|(2xy+yh+x^2+h^2-y^2-yh-\frac13h^2, x^2+h^2-2xy-y^2-2yh-h^2)-  A\right|$$ But remembering our limit, we know that we're going to have to divide by $|(h,h)|$ and that $h\rightarrow 0$ so that we have that $$|(2xy+x^2-y^2, x^2-2xy-y^2) - A| = 0,$$ so $A = (x^2+2xy-y^2, x^2-2xy-y^2)$. Another question: The place I got this from says that the problem should take anywhere from 10 to 15 minutes to solve, but with this method it takes longer. What alternative methods should I consider in solving these kinds of problems, or what should my first line of attack be? For context, another they had listed within the same time frame was: Show that $f(x,y) = |x+\pi|^3e^{3y}$ is totally differentiable and calculate its gradient.",,"['real-analysis', 'derivatives', 'proof-verification', 'alternative-proof']"
11,Second-derivative finite-difference approximation: What is the correct order?,Second-derivative finite-difference approximation: What is the correct order?,,"The ""standard"" second-derivative centered finite-difference approximation is given by LeVeque as \begin{equation} u''(x)\approx\frac{u(x+h)+u(x-h)-2u(x)}{h^2}\,. \end{equation} So if I insert \begin{equation} u(x+h)=u(x)+h u'(x)+\frac{1}{2} h^2 u''(x)+\frac{1}{6} h^3 u^{(3)}(x)+\frac{1}{24} h^4 u^{(4)}(x)+\mathcal{O}\left(h^5\right) \end{equation} and \begin{equation} u(x-h)=u(x)-h u'(x)+\frac{1}{2} h^2 u''(x)-\frac{1}{6} h^3 u^{(3)}(x)+\frac{1}{24} h^4 u^{(4)}(x)+\mathcal{O}\left(h^5\right) \end{equation} into the first equation, the $u(x)$, $u'(x)$, and $u'''(x)$ terms cancel, and the rest are divided by $h^2$ to give \begin{equation} u''(x)+\frac{1}{12} h^2 u^{(4)}(x)+\mathcal{O}\left(h^3\right)\,. \end{equation} Here I am assuming that \begin{equation} \frac{\mathcal{O}(h^5)}{h^2}=\mathcal{O}(h^3)\,. \end{equation} But LeVeque says the result should be \begin{equation} u''(x)+\frac{1}{12} h^2 u^{(4)}(x)+\mathcal{O}\left(h^4\right)\,. \end{equation} Where have I gone wrong?","The ""standard"" second-derivative centered finite-difference approximation is given by LeVeque as \begin{equation} u''(x)\approx\frac{u(x+h)+u(x-h)-2u(x)}{h^2}\,. \end{equation} So if I insert \begin{equation} u(x+h)=u(x)+h u'(x)+\frac{1}{2} h^2 u''(x)+\frac{1}{6} h^3 u^{(3)}(x)+\frac{1}{24} h^4 u^{(4)}(x)+\mathcal{O}\left(h^5\right) \end{equation} and \begin{equation} u(x-h)=u(x)-h u'(x)+\frac{1}{2} h^2 u''(x)-\frac{1}{6} h^3 u^{(3)}(x)+\frac{1}{24} h^4 u^{(4)}(x)+\mathcal{O}\left(h^5\right) \end{equation} into the first equation, the $u(x)$, $u'(x)$, and $u'''(x)$ terms cancel, and the rest are divided by $h^2$ to give \begin{equation} u''(x)+\frac{1}{12} h^2 u^{(4)}(x)+\mathcal{O}\left(h^3\right)\,. \end{equation} Here I am assuming that \begin{equation} \frac{\mathcal{O}(h^5)}{h^2}=\mathcal{O}(h^3)\,. \end{equation} But LeVeque says the result should be \begin{equation} u''(x)+\frac{1}{12} h^2 u^{(4)}(x)+\mathcal{O}\left(h^4\right)\,. \end{equation} Where have I gone wrong?",,['derivatives']
12,"Whether $f(x)= x^3 \sin \frac{1}{\sqrt{|x|}}$, $x\neq 0$ and $0$ if $x=0$ is differentiable everywhere.","Whether ,  and  if  is differentiable everywhere.",f(x)= x^3 \sin \frac{1}{\sqrt{|x|}} x\neq 0 0 x=0,"I think that this function is not differentiable at points where $\frac{1}{\sqrt{|x|}}=n\pi$, $n \in \mathbb{Z}$. But I don't know whether it is correct or not. Of course, the function is differentiable at $x=0$. $\lim_{x\to 0+} \frac{f(x)-f(0)}{x-0}= \lim_{x\to 0+} x^{2}\sin \frac{1}{x} =0 $ Similarly,  $\lim_{x\to 0-} \frac{f(x)-f(0)}{x-0}=0.$ How to discuss the differentiability of this function at other points. Thanks in advance.","I think that this function is not differentiable at points where $\frac{1}{\sqrt{|x|}}=n\pi$, $n \in \mathbb{Z}$. But I don't know whether it is correct or not. Of course, the function is differentiable at $x=0$. $\lim_{x\to 0+} \frac{f(x)-f(0)}{x-0}= \lim_{x\to 0+} x^{2}\sin \frac{1}{x} =0 $ Similarly,  $\lim_{x\to 0-} \frac{f(x)-f(0)}{x-0}=0.$ How to discuss the differentiability of this function at other points. Thanks in advance.",,"['calculus', 'real-analysis', 'derivatives']"
13,Understanding partial derivative of logistic regression cost function,Understanding partial derivative of logistic regression cost function,,"I'm following along in Andrew Ng's great lecture series on machine learning, and he presents the following as the cost function for a logistic regression model [ link ]: $$L(a,y) = -(y \log(a) + (1 - y) \log(1 - a)) $$ He then builds a little math graph, or series of equations, that can be used as helpers for computing the partial derivatives of $L$ with respect to various variables [ link ]: $$ z = w_1x_1 + w_2x_2 + b $$ $$ \hat{y} = a = \sigma(z) $$ Next he says that the following represents the derivative of $L$ wrt $a$ [ link ]: $$ \frac{\partial L}{\partial a} = -\frac{y}{a} + \frac{1-y}{1-a} $$ Unfortunately, he doesn't give any clues as to how this can be derived. Does anyone here know how to derive this partial derivative given the equations above? I'd be very grateful for any insights others can offer on this question!","I'm following along in Andrew Ng's great lecture series on machine learning, and he presents the following as the cost function for a logistic regression model [ link ]: $$L(a,y) = -(y \log(a) + (1 - y) \log(1 - a)) $$ He then builds a little math graph, or series of equations, that can be used as helpers for computing the partial derivatives of $L$ with respect to various variables [ link ]: $$ z = w_1x_1 + w_2x_2 + b $$ $$ \hat{y} = a = \sigma(z) $$ Next he says that the following represents the derivative of $L$ wrt $a$ [ link ]: $$ \frac{\partial L}{\partial a} = -\frac{y}{a} + \frac{1-y}{1-a} $$ Unfortunately, he doesn't give any clues as to how this can be derived. Does anyone here know how to derive this partial derivative given the equations above? I'd be very grateful for any insights others can offer on this question!",,"['calculus', 'derivatives', 'partial-derivative', 'logistic-regression']"
14,"Compute the higher derivatives of $B(h,h)$",Compute the higher derivatives of,"B(h,h)","I'm trying compute the higher derivatives of $B(h,h)$ , but I'm stuck when I tried compute the second derivative of this question. I would like to know if my attempt is correct and how can I found the second derivative of $B(h,h)$ $\textbf{Problem:}$ Let $f: \mathbb{R}^n \longrightarrow \mathbb{R}^m$ defined by $f(x) := B(x,x)$ , where $B: \mathbb{R}^n \times \mathbb{R}^n \longrightarrow \mathbb{R}^m$ is a bilinear form. Compute the higher derivatives of $f$ . $\textbf{My attempt:}$ I know that $B$ is differentiable and $DB(a,b) \cdot (h,k) = B(a,k) + B(h,b)$ for every $(a,b), (h,k) \in \mathbb{R}^n \times \mathbb{R}^n$ . Defining $C: \mathbb{R}^n \longrightarrow \mathbb{R}^n \times \mathbb{R}^n$ by $C(h) := (h,h)$ e noting that $C$ is linear, I know that $DC(h) \cdot k = C(k)$ for every $h, k \in \mathbb{R}^n$ . By the Chain of Rule, $Df(x) \cdot h = D \left( B \circ C \right)(x) = DB(C(x)) \circ DC(x) \cdot h = DB((x,x)) \ \circ \ C(h)$ $ = DB((x,x)) \cdot (h,h) = B(x,h) + B(h,x) = 2 B(x,h)$ for every $h, k \in \mathbb{R}^n$ . Using the definition of differentiability to compute $D^2f(h)$ , I found $Df(x+h) \cdot h = Df(x) \cdot h + D^2f(x) \cdot (h,h) + r(h)$ $2 B(x+h,h) = 2 B(x,h) + D^2f(x) \cdot (h,h) + r(h)$ $2 B(x,h) + 2 B(h,h) = 2 B(x,h) + D^2f(x) \cdot (h,h) + r(h)$ $2 B(h,h) = D^2f(x) \cdot (h,h) + r(h)$ By the continuity of $B$ and $D^2f(x)$ , I have that $\lim_{h \rightarrow 0} r(h) = 0$ , which implies that $\lim_{h \rightarrow 0} \frac{r(h)}{||h||} = 0$ , then $Df(x)$ is differentiable, but this don't give me the second derivative of $f$ . How can I found the second derivative of $f$ ? Is it easy find the $n$ -th order derivative of $f$ for $n \geq 3$ once I have the second derivative of $f$ ? Thanks in advance!","I'm trying compute the higher derivatives of , but I'm stuck when I tried compute the second derivative of this question. I would like to know if my attempt is correct and how can I found the second derivative of Let defined by , where is a bilinear form. Compute the higher derivatives of . I know that is differentiable and for every . Defining by e noting that is linear, I know that for every . By the Chain of Rule, for every . Using the definition of differentiability to compute , I found By the continuity of and , I have that , which implies that , then is differentiable, but this don't give me the second derivative of . How can I found the second derivative of ? Is it easy find the -th order derivative of for once I have the second derivative of ? Thanks in advance!","B(h,h) B(h,h) \textbf{Problem:} f: \mathbb{R}^n \longrightarrow \mathbb{R}^m f(x) := B(x,x) B: \mathbb{R}^n \times \mathbb{R}^n \longrightarrow \mathbb{R}^m f \textbf{My attempt:} B DB(a,b) \cdot (h,k) = B(a,k) + B(h,b) (a,b), (h,k) \in \mathbb{R}^n \times \mathbb{R}^n C: \mathbb{R}^n \longrightarrow \mathbb{R}^n \times \mathbb{R}^n C(h) := (h,h) C DC(h) \cdot k = C(k) h, k \in \mathbb{R}^n Df(x) \cdot h = D \left( B \circ C \right)(x) = DB(C(x)) \circ DC(x) \cdot h = DB((x,x)) \ \circ \ C(h)  = DB((x,x)) \cdot (h,h) = B(x,h) + B(h,x) = 2 B(x,h) h, k \in \mathbb{R}^n D^2f(h) Df(x+h) \cdot h = Df(x) \cdot h + D^2f(x) \cdot (h,h) + r(h) 2 B(x+h,h) = 2 B(x,h) + D^2f(x) \cdot (h,h) + r(h) 2 B(x,h) + 2 B(h,h) = 2 B(x,h) + D^2f(x) \cdot (h,h) + r(h) 2 B(h,h) = D^2f(x) \cdot (h,h) + r(h) B D^2f(x) \lim_{h \rightarrow 0} r(h) = 0 \lim_{h \rightarrow 0} \frac{r(h)}{||h||} = 0 Df(x) f f n f n \geq 3 f","['analysis', 'derivatives', 'proof-verification']"
15,derivative of $\frac 2x \sin(x^3)$ by definition,derivative of  by definition,\frac 2x \sin(x^3),"The function: $$\frac 2x \sin(x^3)$$ when $x\neq0$, and $0$ while $x=0$. I need to find if the function is derivation at $x=1$. First step was to check if the function is continuous. there is just 1 side of limit to check (since its the same function around $x=1$, so I compared the limit of the function with $f(1)$: $$\lim_{x\to 1}\space \frac{2}{x}\sin(x^3) = 2\sin(1) = f(1)$$ first question: was this step necessary? next: $$f'(1) =\lim_{h\to 0}\space \frac{ \frac2{h+1}\sin((h+1)^3)-2\sin(1)}h$$ $$=\lim_{h\to 0}\space \frac{ \frac{2\sin((h+1)^3)}{h+1}-2\sin(1)}h$$ and I know about $\lim_{x\to 0}\frac {\sin(x)}x = 1$, so: $$=\lim_{h\to 0}\space \frac{ \frac{2\sin((h+1)^3) \cdot (h+1)^2}{(h+1)^3}-2\sin(1)}h$$ and then: $$=\lim_{h\to 0}\space \frac{ 2(h+1)^2-2\sin(1)}h$$ and now I don't know how to get rid of the $h$ denominator","The function: $$\frac 2x \sin(x^3)$$ when $x\neq0$, and $0$ while $x=0$. I need to find if the function is derivation at $x=1$. First step was to check if the function is continuous. there is just 1 side of limit to check (since its the same function around $x=1$, so I compared the limit of the function with $f(1)$: $$\lim_{x\to 1}\space \frac{2}{x}\sin(x^3) = 2\sin(1) = f(1)$$ first question: was this step necessary? next: $$f'(1) =\lim_{h\to 0}\space \frac{ \frac2{h+1}\sin((h+1)^3)-2\sin(1)}h$$ $$=\lim_{h\to 0}\space \frac{ \frac{2\sin((h+1)^3)}{h+1}-2\sin(1)}h$$ and I know about $\lim_{x\to 0}\frac {\sin(x)}x = 1$, so: $$=\lim_{h\to 0}\space \frac{ \frac{2\sin((h+1)^3) \cdot (h+1)^2}{(h+1)^3}-2\sin(1)}h$$ and then: $$=\lim_{h\to 0}\space \frac{ 2(h+1)^2-2\sin(1)}h$$ and now I don't know how to get rid of the $h$ denominator",,['derivatives']
16,Proving that a polynomial over $K[t]$ has a multiple zero if and only if $f$ shares a factor with $f'$.,Proving that a polynomial over  has a multiple zero if and only if  shares a factor with .,K[t] f f',"The author is trying to prove that a polynomial $f$ has a repeated zero in it's splitting field over $K$ if and only if $f$ shares a common factor with $f'$ of degree $≥1$ . He first proves that if $f$ has a repeated zero, then $f$ and $f'$ have a common factor of degree $≥1$ in $K[t]$ , in the following way: $$f=(t-a)^2g$$ $$f'=(t-a)^2g'+2(t-a)g$$ At the end of the proof he states "" $f$ and $f'$ share a common factor in $S[t]$ , so they share a common factor in $K[t]$ "". Where $S$ is the splitting field of $f$ over $K$ . I understand how these polynomials share a common factor in $S[t]$ , but how is it that the author infeere from this that they share a common factor in $K[t]$ ? He then continues to prove that if $f$ has no repeated roots, then it doesn't share a common factor with $f'$ . This part of the proof is understandable since it doesn't depend on $K[t]$ . I would really appreciate any help/thoughts.","The author is trying to prove that a polynomial has a repeated zero in it's splitting field over if and only if shares a common factor with of degree . He first proves that if has a repeated zero, then and have a common factor of degree in , in the following way: At the end of the proof he states "" and share a common factor in , so they share a common factor in "". Where is the splitting field of over . I understand how these polynomials share a common factor in , but how is it that the author infeere from this that they share a common factor in ? He then continues to prove that if has no repeated roots, then it doesn't share a common factor with . This part of the proof is understandable since it doesn't depend on . I would really appreciate any help/thoughts.",f K f f' ≥1 f f f' ≥1 K[t] f=(t-a)^2g f'=(t-a)^2g'+2(t-a)g f f' S[t] K[t] S f K S[t] K[t] f f' K[t],"['abstract-algebra', 'derivatives', 'polynomials', 'field-theory']"
17,Deriving the cartesian del operator from cylindrical del operator,Deriving the cartesian del operator from cylindrical del operator,,"I'm having trouble going from the cylindrical form of the del operator to the cartesian form. Here is my attempt so far: $\rho = \sqrt{x^2 + y^2}$ $\theta = \arctan(\frac{y}{x})$ $\nabla = \frac{\partial}{\partial \rho} \hat{\rho} + \frac{1}{\rho} \frac{\partial}{\partial \theta} \hat{\theta} + \frac{\partial}{\partial z} \hat{z} $ $\frac{\partial}{\partial \rho} = \frac{\partial x}{\partial \rho}\frac{\partial}{\partial x} + \frac{\partial y}{\partial \rho}\frac{\partial}{\partial y}$ $\frac{\partial x}{\partial \rho} = (\frac{\partial \rho}{\partial x})^{-1} = (\frac{x}{\rho})^{-1} = \frac{\rho}{x} = \frac{1}{\cos(\theta)}$ $\frac{\partial y}{\partial \rho} = (\frac{\partial \rho}{\partial y})^{-1} = (\frac{y}{\rho})^{-1} = \frac{\rho}{y} = \frac{1}{\sin(\theta)}$ thus, $\frac{\partial}{\partial \rho} = \frac{1}{\cos(\theta)}\frac{\partial}{\partial x} + \frac{1}{\sin(\theta)}\frac{\partial}{\partial y}$ Also: $\hat{\rho} = \cos(\theta) \hat{x} + \sin(\theta) \hat{y}$ next, $\frac{\partial}{\partial \theta} = \frac{\partial x}{\partial \theta}\frac{\partial}{\partial x} + \frac{\partial y}{\partial \theta}\frac{\partial}{\partial y}$ $\frac{\partial x}{\partial \theta} = (\frac{\partial \theta}{\partial x})^{-1} = (\frac{x^2}{y^2 + x^2} \frac{-y}{x^2})^{-1} = -\frac{\rho^2}{y} = -\frac{\rho}{\sin(\theta)}$ $\frac{\partial y}{\partial \theta} = (\frac{\partial \theta}{\partial y})^{-1} = (\frac{x^2}{y^2 + x^2} \frac{1}{x})^{-1} = \frac{\rho^2}{x} = \frac{\rho}{\cos(\theta)}$ thus, $\frac{\partial}{\partial \theta} = -\frac{\rho}{\sin(\theta)}\frac{\partial}{\partial x} + \frac{\rho}{\cos(\theta)}\frac{\partial}{\partial y}$ also: $\hat{\theta} = -\sin(\theta) \hat{x} + \cos(\theta) \hat{y}$ so: $\nabla = \frac{\partial}{\partial \rho} \hat{\rho} + \frac{1}{\rho}\frac{\partial}{\partial \theta} \hat{\theta} + \frac{\partial}{\partial z} \hat{z} = (\frac{1}{\cos(\theta)}\frac{\partial}{\partial x} + \frac{1}{\sin(\theta)}\frac{\partial}{\partial y}) (\cos(\theta) \hat{x} + \sin(\theta) \hat{y}) + (-\frac{1}{\sin(\theta)}\frac{\partial}{\partial x} + \frac{1}{\cos(\theta)}\frac{\partial}{\partial y}) (-\sin(\theta) \hat{x} + \cos(\theta) \hat{y}) + \frac{\partial}{\partial z} \hat{z}$ Simplifying this: $\nabla = \frac{\partial}{\partial x} \hat{x} + \tan(\theta)\frac{\partial}{\partial x} \hat{y} + \cot(\theta)\frac{\partial}{\partial y} \hat{x} + \frac{\partial}{\partial y} \hat{y} + \frac{\partial}{\partial x} \hat{x} - \cot(\theta)\frac{\partial}{\partial x} \hat{y} - \tan(\theta)\frac{\partial}{\partial y} \hat{x} + \frac{\partial}{\partial y} \hat{y} + \frac{\partial}{\partial z} \hat{z}$ $ = 2\frac{\partial}{\partial x} \hat{x} + 2\frac{\partial}{\partial y} \hat{y} + (\tan(\theta) - \cot(\theta))\frac{\partial}{\partial x} \hat{y} + (\cot(\theta) - \tan(\theta))\frac{\partial}{\partial y} \hat{x} + \frac{\partial}{\partial z} \hat{z}$ but this isn't the cartesian del operator. Where did I go wrong?","I'm having trouble going from the cylindrical form of the del operator to the cartesian form. Here is my attempt so far: $\rho = \sqrt{x^2 + y^2}$ $\theta = \arctan(\frac{y}{x})$ $\nabla = \frac{\partial}{\partial \rho} \hat{\rho} + \frac{1}{\rho} \frac{\partial}{\partial \theta} \hat{\theta} + \frac{\partial}{\partial z} \hat{z} $ $\frac{\partial}{\partial \rho} = \frac{\partial x}{\partial \rho}\frac{\partial}{\partial x} + \frac{\partial y}{\partial \rho}\frac{\partial}{\partial y}$ $\frac{\partial x}{\partial \rho} = (\frac{\partial \rho}{\partial x})^{-1} = (\frac{x}{\rho})^{-1} = \frac{\rho}{x} = \frac{1}{\cos(\theta)}$ $\frac{\partial y}{\partial \rho} = (\frac{\partial \rho}{\partial y})^{-1} = (\frac{y}{\rho})^{-1} = \frac{\rho}{y} = \frac{1}{\sin(\theta)}$ thus, $\frac{\partial}{\partial \rho} = \frac{1}{\cos(\theta)}\frac{\partial}{\partial x} + \frac{1}{\sin(\theta)}\frac{\partial}{\partial y}$ Also: $\hat{\rho} = \cos(\theta) \hat{x} + \sin(\theta) \hat{y}$ next, $\frac{\partial}{\partial \theta} = \frac{\partial x}{\partial \theta}\frac{\partial}{\partial x} + \frac{\partial y}{\partial \theta}\frac{\partial}{\partial y}$ $\frac{\partial x}{\partial \theta} = (\frac{\partial \theta}{\partial x})^{-1} = (\frac{x^2}{y^2 + x^2} \frac{-y}{x^2})^{-1} = -\frac{\rho^2}{y} = -\frac{\rho}{\sin(\theta)}$ $\frac{\partial y}{\partial \theta} = (\frac{\partial \theta}{\partial y})^{-1} = (\frac{x^2}{y^2 + x^2} \frac{1}{x})^{-1} = \frac{\rho^2}{x} = \frac{\rho}{\cos(\theta)}$ thus, $\frac{\partial}{\partial \theta} = -\frac{\rho}{\sin(\theta)}\frac{\partial}{\partial x} + \frac{\rho}{\cos(\theta)}\frac{\partial}{\partial y}$ also: $\hat{\theta} = -\sin(\theta) \hat{x} + \cos(\theta) \hat{y}$ so: $\nabla = \frac{\partial}{\partial \rho} \hat{\rho} + \frac{1}{\rho}\frac{\partial}{\partial \theta} \hat{\theta} + \frac{\partial}{\partial z} \hat{z} = (\frac{1}{\cos(\theta)}\frac{\partial}{\partial x} + \frac{1}{\sin(\theta)}\frac{\partial}{\partial y}) (\cos(\theta) \hat{x} + \sin(\theta) \hat{y}) + (-\frac{1}{\sin(\theta)}\frac{\partial}{\partial x} + \frac{1}{\cos(\theta)}\frac{\partial}{\partial y}) (-\sin(\theta) \hat{x} + \cos(\theta) \hat{y}) + \frac{\partial}{\partial z} \hat{z}$ Simplifying this: $\nabla = \frac{\partial}{\partial x} \hat{x} + \tan(\theta)\frac{\partial}{\partial x} \hat{y} + \cot(\theta)\frac{\partial}{\partial y} \hat{x} + \frac{\partial}{\partial y} \hat{y} + \frac{\partial}{\partial x} \hat{x} - \cot(\theta)\frac{\partial}{\partial x} \hat{y} - \tan(\theta)\frac{\partial}{\partial y} \hat{x} + \frac{\partial}{\partial y} \hat{y} + \frac{\partial}{\partial z} \hat{z}$ $ = 2\frac{\partial}{\partial x} \hat{x} + 2\frac{\partial}{\partial y} \hat{y} + (\tan(\theta) - \cot(\theta))\frac{\partial}{\partial x} \hat{y} + (\cot(\theta) - \tan(\theta))\frac{\partial}{\partial y} \hat{x} + \frac{\partial}{\partial z} \hat{z}$ but this isn't the cartesian del operator. Where did I go wrong?",,"['derivatives', 'vector-analysis']"
18,"Related Rates - two airplanes, both rates given. Find rate of distance","Related Rates - two airplanes, both rates given. Find rate of distance",,"The question reads ""Two aircraft are in the same airspace with Plane A 500km south of Plane B. If Plane A is traveling 600 km/h due south while Plane B is travelling 800km/h due west, determine how quickly the distance between the planes is changing."" My solution: $\frac{dA}{dt} = 600km/h$ $\frac{dB}{dt} = 800 km/h$ $A = 500km$ $B = 0km$ $d = \sqrt{A^2 + B^2}$ $\frac{dd}{dt} = \frac{A\frac{dA}{dt} + B\frac{dB}{dt}}{\sqrt{A^2+B^2}}$ Plugging in the values from above: $\frac{dd}{dt} = \frac{0 + 500(600)}{\sqrt{500^2}}$ $\frac{dd}{dt} = 600 km/h $ Did I do this the correct way?","The question reads ""Two aircraft are in the same airspace with Plane A 500km south of Plane B. If Plane A is traveling 600 km/h due south while Plane B is travelling 800km/h due west, determine how quickly the distance between the planes is changing."" My solution: $\frac{dA}{dt} = 600km/h$ $\frac{dB}{dt} = 800 km/h$ $A = 500km$ $B = 0km$ $d = \sqrt{A^2 + B^2}$ $\frac{dd}{dt} = \frac{A\frac{dA}{dt} + B\frac{dB}{dt}}{\sqrt{A^2+B^2}}$ Plugging in the values from above: $\frac{dd}{dt} = \frac{0 + 500(600)}{\sqrt{500^2}}$ $\frac{dd}{dt} = 600 km/h $ Did I do this the correct way?",,"['calculus', 'derivatives']"
19,The graph of the derivative of a function $f(x)$ is shown and $f(-3)=-1$. How many local minima of the function $g(x) = \left | 2f(x)+x^2 \right |$?,The graph of the derivative of a function  is shown and . How many local minima of the function ?,f(x) f(-3)=-1 g(x) = \left | 2f(x)+x^2 \right |,"The graph of the derivative $f'(x)$ of a function $f(x)$ is shown in the figure below. If $g$ is a function defined for all $x$ by $g(x) = \left | 2f(x)+x^2 \right |$ and $f(-3)=-1$ , how many local minima does the function $g$ have? This is what I have done. From the figure, I can draw this table We have $g(x)=\sqrt{( 2f(x)+x^2 )^2}$ so its derivative is $g'(x)=\frac{2(2f(x)+x^2)(2f'(x)+2x)}{2\sqrt{(2f(x)+x^2)^2}}$ . I see that $2f'(-3)+2(-3)=0$ and $2f'(-1)+2(-1)=0$ so $g'(-3)=0$ and $g'(-1)=0$ . I believe that there exists an $-3<x_0<-1$ such that $2f'(x_0)+2(x_0)=0$ which implies $g'(x_0)=0$ . I'm stuck here, I don't know the sign of $g'(x)$ in any interval. This is my prediction. We have $g(x)=\left | 2\left [ f\left ( x \right )-\left ( \frac{-x^2}{2} \right ) \right ] \right |$ , consider the graph of the function $y=f(x)$ and the function $y=\frac{-x^2}{2}$ . From the table above, I think that the graph of the function $y=f(x)$ is always ""higher"" (I don't know the exact word) than the graph of the function $y=\frac{-x^2}{2}$ for all $x$ , which implies $\left [ f\left ( x \right )-\left ( \frac{-x^2}{2} \right ) \right ] >0$ for all $x$ , so $g(x) = 2f(x)+x^2$ and its derivative is $g'(x)=2f'(x)+2x=2[f'(x)-(-x)]$ . Now from the figure above, draw the graph of the function $y=-x$ . We will see that the two graphs have 3 intersection points, which are $(-3,3)$ , $(-1,1)$ and $(x_0,-x_0)$ where $-3<x_0<-1$ , we notice that $f'(x)-(-x)>0$ for $-3<x<x_0$ (the graph of $y=f'(x)$ is higher) and $f'(x)-(-x)<0$ for $x_0<x<-1$ (the graph of $y=-x$ is higher). I draw this table I conclude that $g$ has 2 local minima. If my prediction is right please show me how to prove this, or show me another way to finish this exercise. Thank you.","The graph of the derivative of a function is shown in the figure below. If is a function defined for all by and , how many local minima does the function have? This is what I have done. From the figure, I can draw this table We have so its derivative is . I see that and so and . I believe that there exists an such that which implies . I'm stuck here, I don't know the sign of in any interval. This is my prediction. We have , consider the graph of the function and the function . From the table above, I think that the graph of the function is always ""higher"" (I don't know the exact word) than the graph of the function for all , which implies for all , so and its derivative is . Now from the figure above, draw the graph of the function . We will see that the two graphs have 3 intersection points, which are , and where , we notice that for (the graph of is higher) and for (the graph of is higher). I draw this table I conclude that has 2 local minima. If my prediction is right please show me how to prove this, or show me another way to finish this exercise. Thank you.","f'(x) f(x) g x g(x) = \left | 2f(x)+x^2 \right | f(-3)=-1 g g(x)=\sqrt{( 2f(x)+x^2 )^2} g'(x)=\frac{2(2f(x)+x^2)(2f'(x)+2x)}{2\sqrt{(2f(x)+x^2)^2}} 2f'(-3)+2(-3)=0 2f'(-1)+2(-1)=0 g'(-3)=0 g'(-1)=0 -3<x_0<-1 2f'(x_0)+2(x_0)=0 g'(x_0)=0 g'(x) g(x)=\left | 2\left [ f\left ( x \right )-\left ( \frac{-x^2}{2} \right ) \right ] \right | y=f(x) y=\frac{-x^2}{2} y=f(x) y=\frac{-x^2}{2} x \left [ f\left ( x \right )-\left ( \frac{-x^2}{2} \right ) \right ] >0 x g(x) = 2f(x)+x^2 g'(x)=2f'(x)+2x=2[f'(x)-(-x)] y=-x (-3,3) (-1,1) (x_0,-x_0) -3<x_0<-1 f'(x)-(-x)>0 -3<x<x_0 y=f'(x) f'(x)-(-x)<0 x_0<x<-1 y=-x g","['calculus', 'derivatives', 'graphing-functions', 'maxima-minima']"
20,"If graph of $f(x) \cdot f'(x)$ is given, answer the following","If graph of  is given, answer the following",f(x) \cdot f'(x),"If $f(x)$ is a continuous and differentiable function. Given that $f(x)$ takes values of the type $\pm \sqrt{W}$ for $x=a$ and $x=b$ (where $W$ denotes set of whole numbers). For all other $x$, $f(x)$ can take any real value. Also $f(c)=-\frac{3}{2}$ and $|f(a)| \leq |f(b)|$ and graph of $f(x) \cdot f'(x)$ is given below: Answer the following questions: Ques: Find number of rational values $f(a)+f(b)+f(c)$ can take? Ques: Find number of values $(f(a))^2+(f(b))^2+(f(c))^2$ can take? It is clear from given data that $f'(c)=0$ . I thought of taking  $g(x)=(f(x))^2/2$ Hence from given graph, $g(x)$ increases from $x=a$ to $x=c$ and then decreases Also $g(c)=9/8$ but I am not able to proceed from here. Could someone please help me in this?","If $f(x)$ is a continuous and differentiable function. Given that $f(x)$ takes values of the type $\pm \sqrt{W}$ for $x=a$ and $x=b$ (where $W$ denotes set of whole numbers). For all other $x$, $f(x)$ can take any real value. Also $f(c)=-\frac{3}{2}$ and $|f(a)| \leq |f(b)|$ and graph of $f(x) \cdot f'(x)$ is given below: Answer the following questions: Ques: Find number of rational values $f(a)+f(b)+f(c)$ can take? Ques: Find number of values $(f(a))^2+(f(b))^2+(f(c))^2$ can take? It is clear from given data that $f'(c)=0$ . I thought of taking  $g(x)=(f(x))^2/2$ Hence from given graph, $g(x)$ increases from $x=a$ to $x=c$ and then decreases Also $g(c)=9/8$ but I am not able to proceed from here. Could someone please help me in this?",,"['calculus', 'derivatives']"
21,Two derivatives on a ring with two metrics?,Two derivatives on a ring with two metrics?,,"In real analysis, the derivative can be defined by the limit formula: $$D[f](x)v := \lim_{t\to 0} \frac{f(x+tv) - f(x)}{t}$$ and we ask that as a function, $D[f] : \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ is continuous. Sometimes, one wants to do analysis over a more general commutative ring with a metric, or even more generally, a topological ring.  This is done for example by Bertram et al in ""Differential calculus of general base fields and rings"". Here a continuous function $f : R \to R$ has a derivative when there is a  continuous function $f^{[1]} : R \times R \times R \to R$ such that $$f(x+tv)-f(x) = t\cdot f^{[1]}(x,v,t) \qquad (1)$$ The $f^{[1]}$ can be classically (over $\mathbb{R}$) described as $$   f^{[1]}(x,v,t) =      \begin{cases}       \frac{f(x+tv) - f(x)}{t} & t \ne 0\\       D[f](x)v & t = 0     \end{cases}   $$ But in general, as $R$ is not a field, an $f^{[1]}$ satisfying $(1)$ will not be defined as above. When a continuous $f$ has an associated $f^{[1]}$ that satisfies $(1)$, and that $f^{[1]}$ is linear in $v$, we may  define its differential as $$D[f](x)v := f^{[1]}(x,v,0)$$ Bertram makes an assumption that leads to $f^{[1]}$ being unique, and thus, is a ""Fermat ring"" (in the sense of Dubuc and Kock).  One aspect of uniqueness is that it makes the derivative well defined.  It also ensures that the collection of differentiable functions satisfies the chain rule, and other usual properties of differentiation. But I am curious -- what if one changes the metric or the topology on the ring, but leaves the operations unchanged.  Is it possible for some fixed CRing $R$ to have two metrics and where the induced derivatives are different? Link to Bertram et al: http://www.sciencedirect.com/science/article/pii/S0723086904800069 Dubuc and Kock: ""On 1-form classifiers""  E. Dubuc and A. Kock.  Communications in Algebra.  Volume 12, Issue 12, 1984. pp 1471-1531. -- Edits to clarify --","In real analysis, the derivative can be defined by the limit formula: $$D[f](x)v := \lim_{t\to 0} \frac{f(x+tv) - f(x)}{t}$$ and we ask that as a function, $D[f] : \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ is continuous. Sometimes, one wants to do analysis over a more general commutative ring with a metric, or even more generally, a topological ring.  This is done for example by Bertram et al in ""Differential calculus of general base fields and rings"". Here a continuous function $f : R \to R$ has a derivative when there is a  continuous function $f^{[1]} : R \times R \times R \to R$ such that $$f(x+tv)-f(x) = t\cdot f^{[1]}(x,v,t) \qquad (1)$$ The $f^{[1]}$ can be classically (over $\mathbb{R}$) described as $$   f^{[1]}(x,v,t) =      \begin{cases}       \frac{f(x+tv) - f(x)}{t} & t \ne 0\\       D[f](x)v & t = 0     \end{cases}   $$ But in general, as $R$ is not a field, an $f^{[1]}$ satisfying $(1)$ will not be defined as above. When a continuous $f$ has an associated $f^{[1]}$ that satisfies $(1)$, and that $f^{[1]}$ is linear in $v$, we may  define its differential as $$D[f](x)v := f^{[1]}(x,v,0)$$ Bertram makes an assumption that leads to $f^{[1]}$ being unique, and thus, is a ""Fermat ring"" (in the sense of Dubuc and Kock).  One aspect of uniqueness is that it makes the derivative well defined.  It also ensures that the collection of differentiable functions satisfies the chain rule, and other usual properties of differentiation. But I am curious -- what if one changes the metric or the topology on the ring, but leaves the operations unchanged.  Is it possible for some fixed CRing $R$ to have two metrics and where the induced derivatives are different? Link to Bertram et al: http://www.sciencedirect.com/science/article/pii/S0723086904800069 Dubuc and Kock: ""On 1-form classifiers""  E. Dubuc and A. Kock.  Communications in Algebra.  Volume 12, Issue 12, 1984. pp 1471-1531. -- Edits to clarify --",,"['calculus', 'derivatives', 'ring-theory', 'metric-spaces']"
22,Lipschitz Continuous Diffeomorphism,Lipschitz Continuous Diffeomorphism,,"Denote the real line by $\mathcal{R}$. I look for non-trivial examples of functions $f:\mathcal{R}\longrightarrow\mathcal{R}$ that are Lipschitz continuous, differentiable and with non-vanishing derivative. By trivial examples I mean the ones of the form $f(x):=ax+b$ for $a,b$ real numbers.","Denote the real line by $\mathcal{R}$. I look for non-trivial examples of functions $f:\mathcal{R}\longrightarrow\mathcal{R}$ that are Lipschitz continuous, differentiable and with non-vanishing derivative. By trivial examples I mean the ones of the form $f(x):=ax+b$ for $a,b$ real numbers.",,"['calculus', 'real-analysis', 'derivatives']"
23,Properties of Transpose Matrices... Flippable?,Properties of Transpose Matrices... Flippable?,,"I'm learning linear regression using ""A Primer on Linear Models"" by John F. Monahan. On page 14, it says $$Q(b) = (y-Xb)^T(y-Xb)=y^Ty − 2y^T Xb + b^T X^T Xb$$ Where $y$ is a $N ×1$ vector of observed responses, $X$ is a $N × p$ matrix of fixed constants, $b$ is a $p × 1$ vector of fixed but unknown parameters. by developing brackets I can get $-y^TXb-(Xb)^Ty$. But I don't know from this how to get $− 2y^T Xb$ term. Why is it possible to just flip $(Xb)^Ty$ to $y^TXb$? Also, on the same page 14, $$\frac{d Q}{d b}=-2X^Ty + 2X^TXb = -2X^T(y-Xb)$$ I don't understand how I can get this result.","I'm learning linear regression using ""A Primer on Linear Models"" by John F. Monahan. On page 14, it says $$Q(b) = (y-Xb)^T(y-Xb)=y^Ty − 2y^T Xb + b^T X^T Xb$$ Where $y$ is a $N ×1$ vector of observed responses, $X$ is a $N × p$ matrix of fixed constants, $b$ is a $p × 1$ vector of fixed but unknown parameters. by developing brackets I can get $-y^TXb-(Xb)^Ty$. But I don't know from this how to get $− 2y^T Xb$ term. Why is it possible to just flip $(Xb)^Ty$ to $y^TXb$? Also, on the same page 14, $$\frac{d Q}{d b}=-2X^Ty + 2X^TXb = -2X^T(y-Xb)$$ I don't understand how I can get this result.",,"['matrices', 'derivatives', 'transpose']"
24,Elementary question on notation in implicit differentiation,Elementary question on notation in implicit differentiation,,"I've been watching a video series called ""The Essence of Calculus"" on the YouTube channel $3$Blue$1$Brown, and I'm familiar with taking derivates, even moderately advanced derivatives, but on this occasion I come across a use of the notation I hadn't seen before. In this particular case, the following was said: ""The derivative of the equation $x^2+y^2=5^2$ can be taken by the following means: the derivative of $x^2$ is given as $2x~dx$ and the derivative of $y^2$ is given as $2y~dy$, therefore $2x~dx+2y~dy=0$ [etc]"" Now, I'm familiar with the chain rule, but only very briefly familiar with implicit differentiation. I understand how, when differentiation something like the equation for a circle, you end up with $$2x+2y\frac{dy}{dx}=0$$ $$\frac{dy}{dx}=-\frac{x}{y}$$ but even so, I'm unsure as to where the individual $dx$ and $dy$ tags came from. Provided we follow a slower approach, and we consider $x^2+y^2$ to be a multivariable function, $S$, of $x$ and $y$, then $$S=x^2+y^2$$ $$S+dS=(x+dx)^2+(y+dy)^2$$ $$dS=x^2+2x~dx+dx^2+y^2+2y~dy+dy^2-x^2-y^2$$ $$dS=2x~dx+dx^2+2y~dy+dy^2$$ we know that, since this is the derivative of a circle equation, that $dS=0$, since the radius of the circle will remain the same everywhere, but even we set the current expression to equal $0$, i.e. $0=2x~dx+dx^2+2y~dy+dy^2$, this I'm still far from where I want to be, which stokes a few questions. One is, is the $dx$ and $dy$ a notational trick, or is there more than this behind it? And what is it that we are even differentiating with respect to? Given the situation, this question has led me to believe that $x$ and $y$ are in fact functions of some other variable, as if I wasn't lost before! Having followed the symbolic (non-intuitive) method of differentiating implicit functions has left me a little stumped by this situation. Also, I'm aware that my question and this question are very similar, however I have not managed to find an answer that has explained my particular situation in a satisfactory way to myself in it. Any help is appreciated, thank you.","I've been watching a video series called ""The Essence of Calculus"" on the YouTube channel $3$Blue$1$Brown, and I'm familiar with taking derivates, even moderately advanced derivatives, but on this occasion I come across a use of the notation I hadn't seen before. In this particular case, the following was said: ""The derivative of the equation $x^2+y^2=5^2$ can be taken by the following means: the derivative of $x^2$ is given as $2x~dx$ and the derivative of $y^2$ is given as $2y~dy$, therefore $2x~dx+2y~dy=0$ [etc]"" Now, I'm familiar with the chain rule, but only very briefly familiar with implicit differentiation. I understand how, when differentiation something like the equation for a circle, you end up with $$2x+2y\frac{dy}{dx}=0$$ $$\frac{dy}{dx}=-\frac{x}{y}$$ but even so, I'm unsure as to where the individual $dx$ and $dy$ tags came from. Provided we follow a slower approach, and we consider $x^2+y^2$ to be a multivariable function, $S$, of $x$ and $y$, then $$S=x^2+y^2$$ $$S+dS=(x+dx)^2+(y+dy)^2$$ $$dS=x^2+2x~dx+dx^2+y^2+2y~dy+dy^2-x^2-y^2$$ $$dS=2x~dx+dx^2+2y~dy+dy^2$$ we know that, since this is the derivative of a circle equation, that $dS=0$, since the radius of the circle will remain the same everywhere, but even we set the current expression to equal $0$, i.e. $0=2x~dx+dx^2+2y~dy+dy^2$, this I'm still far from where I want to be, which stokes a few questions. One is, is the $dx$ and $dy$ a notational trick, or is there more than this behind it? And what is it that we are even differentiating with respect to? Given the situation, this question has led me to believe that $x$ and $y$ are in fact functions of some other variable, as if I wasn't lost before! Having followed the symbolic (non-intuitive) method of differentiating implicit functions has left me a little stumped by this situation. Also, I'm aware that my question and this question are very similar, however I have not managed to find an answer that has explained my particular situation in a satisfactory way to myself in it. Any help is appreciated, thank you.",,"['derivatives', 'implicit-differentiation']"
25,Why two derivatives do not match,Why two derivatives do not match,,"Let $$ \begin{align} u = \begin{cases} \sqrt{r} \cosh(t) & \text{if } r\gt 0\\ \sqrt{-r} \sinh(t) & \text{if } r \lt 0\\ \end{cases} \end{align} \tag 1 $$ and $$   \begin{align}  v = \begin{cases} \sqrt{r} \sinh(t) & \text{if } r\gt 0\\ \sqrt{-r} \cosh(t) & \text{if } r \lt 0\\ \end{cases} \end{align} \tag 2 $$ Combining both $(1)$ and $(2)$ and using $\cosh^2-\sinh^2=1$, $ u^2 - v^2=r \tag 3$ Using $(1)$, $$\frac{\partial{u}}{\partial{r}}=\frac{\cosh(t)}{2\sqrt{r}} \text{ if } r\gt 0 \quad \lor \quad  -\frac{\sinh(t)}{2\sqrt{-r}} \text{ if } r\lt 0$$ However, using $(3)$,  $$ \frac{\partial{u}}{\partial{r}}= \frac{1}{2u} = \frac{1}{2\sqrt{r}\cosh(t)}\text{ if } r\gt 0 \quad \lor \quad \frac{1}{2\sqrt{-r}\sinh(t)}\text{ if } r\lt 0$$ Why don't both of the derivatives of $\frac{\partial{u}}{\partial{r}}$ match? What did I do wrong?","Let $$ \begin{align} u = \begin{cases} \sqrt{r} \cosh(t) & \text{if } r\gt 0\\ \sqrt{-r} \sinh(t) & \text{if } r \lt 0\\ \end{cases} \end{align} \tag 1 $$ and $$   \begin{align}  v = \begin{cases} \sqrt{r} \sinh(t) & \text{if } r\gt 0\\ \sqrt{-r} \cosh(t) & \text{if } r \lt 0\\ \end{cases} \end{align} \tag 2 $$ Combining both $(1)$ and $(2)$ and using $\cosh^2-\sinh^2=1$, $ u^2 - v^2=r \tag 3$ Using $(1)$, $$\frac{\partial{u}}{\partial{r}}=\frac{\cosh(t)}{2\sqrt{r}} \text{ if } r\gt 0 \quad \lor \quad  -\frac{\sinh(t)}{2\sqrt{-r}} \text{ if } r\lt 0$$ However, using $(3)$,  $$ \frac{\partial{u}}{\partial{r}}= \frac{1}{2u} = \frac{1}{2\sqrt{r}\cosh(t)}\text{ if } r\gt 0 \quad \lor \quad \frac{1}{2\sqrt{-r}\sinh(t)}\text{ if } r\lt 0$$ Why don't both of the derivatives of $\frac{\partial{u}}{\partial{r}}$ match? What did I do wrong?",,"['derivatives', 'partial-derivative', 'implicit-differentiation']"
26,Solutions of $0 = A\sin^2(\alpha) + B\sin(2\alpha) - C$?,Solutions of ?,0 = A\sin^2(\alpha) + B\sin(2\alpha) - C,I am facing a problem from physics class involving a projectile motion which can be described with such an given equation: $$ h = -\frac{1}{2} \frac{g}{v_{0}^2 \cos^2\alpha} d^2 + \frac{\sin\alpha}{\cos\alpha} d + y. $$ The goal is to find the minimum value of (rearranging above equation) $$ v_0(\alpha) = \frac{d}{\cos\alpha} \cdot \sqrt{\frac{1}{2} \cdot \frac{g}{\tan\alpha \cdot d + y - h}}. $$ This involves finding solutions to $v_0'(\alpha) = 0$. I was able to find the derivative ($t := d\cdot \tan\alpha +y-h$): $$ v_0'(\alpha) = \frac{\tan \alpha}{\cos\alpha \cdot \sqrt{t}} - \frac{d}{2\cos^3\alpha \cdot (\sqrt{t})^3} $$ Because of the condition $v_0'(\alpha) = 0$ this simplifies to (it is know that the solution is around $50^\circ$) $$ 0 = \sin\alpha - \frac{d}{2\sin\alpha \cos\alpha \cdot (d\cdot \tan\alpha + y - h)} $$ or $$ 0 = d\sin(2\alpha)\tan\alpha + (y-h)\sin(2\alpha) - d. $$ or $$ 0 = 2d\sin^2(\alpha) + (y-h)\sin(2\alpha) - d $$ or with some constants $$ 0 = 2d\sin^2(\alpha) + B\sin(2\alpha) - d. $$ How can I find the   solutions from here?,I am facing a problem from physics class involving a projectile motion which can be described with such an given equation: $$ h = -\frac{1}{2} \frac{g}{v_{0}^2 \cos^2\alpha} d^2 + \frac{\sin\alpha}{\cos\alpha} d + y. $$ The goal is to find the minimum value of (rearranging above equation) $$ v_0(\alpha) = \frac{d}{\cos\alpha} \cdot \sqrt{\frac{1}{2} \cdot \frac{g}{\tan\alpha \cdot d + y - h}}. $$ This involves finding solutions to $v_0'(\alpha) = 0$. I was able to find the derivative ($t := d\cdot \tan\alpha +y-h$): $$ v_0'(\alpha) = \frac{\tan \alpha}{\cos\alpha \cdot \sqrt{t}} - \frac{d}{2\cos^3\alpha \cdot (\sqrt{t})^3} $$ Because of the condition $v_0'(\alpha) = 0$ this simplifies to (it is know that the solution is around $50^\circ$) $$ 0 = \sin\alpha - \frac{d}{2\sin\alpha \cos\alpha \cdot (d\cdot \tan\alpha + y - h)} $$ or $$ 0 = d\sin(2\alpha)\tan\alpha + (y-h)\sin(2\alpha) - d. $$ or $$ 0 = 2d\sin^2(\alpha) + (y-h)\sin(2\alpha) - d $$ or with some constants $$ 0 = 2d\sin^2(\alpha) + B\sin(2\alpha) - d. $$ How can I find the   solutions from here?,,"['trigonometry', 'derivatives', 'mathematical-physics']"
27,How can I find the inverse of $f(x) = 7 + 5x^3 + x^7$?,How can I find the inverse of ?,f(x) = 7 + 5x^3 + x^7,Let $f(x) = 7 + 5x^3 + x^7$. What will the inverse of $f(x)$ be? How do I isolate $x$? I'm not being able to group $x$ together And then how do I find  $(f^{-1})''(1)$?,Let $f(x) = 7 + 5x^3 + x^7$. What will the inverse of $f(x)$ be? How do I isolate $x$? I'm not being able to group $x$ together And then how do I find  $(f^{-1})''(1)$?,,"['calculus', 'derivatives', 'inverse-function']"
28,"Show that $m\ddot q+\nabla U(q)=0\implies m\frac{|\dot q|^2}2+U(q)=K,\quad K\in\Bbb R$",Show that,"m\ddot q+\nabla U(q)=0\implies m\frac{|\dot q|^2}2+U(q)=K,\quad K\in\Bbb R","This is the exercise 12 in page 211 of Analysis II of Amann and Escher. Suppose $T(\dot q)=m\frac{|\dot q|^2}2$ and $U(t,q)=U(q)$ for $q\in\Bbb R^3$. Prove that the total energy defined as $E:=T+U$ is constant along every solution $q$ of the Euler-Lagrange equation for the variational problem $$\int_{t_0}^{t_1}[T(\dot q)-U(q)]\,\mathrm dt\implies\text{Min},\quad q\in C^2([t_0,t_1],\Bbb R^3)\tag1$$ I dont follow exactly what I must do in this exercise. My work so far: The Euler-Lagrange equation for $L:=T(\dot q)-U(q)$ is $m\ddot q=-\nabla U(q)$. Then we want to show that if $q$ holds the previous equation then $E=T+U=K$ where $K$ is a constant. If Im not wrong the exercise can be rewritten as $$ m\ddot q+\nabla U(q)=0\implies m\frac{|\dot q|^2}2+U(q)=K,\quad K\in\Bbb R \tag2$$ However from $E=K$ I find that $\frac{\partial}{\partial q} E=\nabla U(q)=0$ and $\frac\partial{\partial \dot q} E=m\dot q=0$, that is, it seems that Im proving the opposite direction of $(2)$. Some help will be appreciated, thank you.","This is the exercise 12 in page 211 of Analysis II of Amann and Escher. Suppose $T(\dot q)=m\frac{|\dot q|^2}2$ and $U(t,q)=U(q)$ for $q\in\Bbb R^3$. Prove that the total energy defined as $E:=T+U$ is constant along every solution $q$ of the Euler-Lagrange equation for the variational problem $$\int_{t_0}^{t_1}[T(\dot q)-U(q)]\,\mathrm dt\implies\text{Min},\quad q\in C^2([t_0,t_1],\Bbb R^3)\tag1$$ I dont follow exactly what I must do in this exercise. My work so far: The Euler-Lagrange equation for $L:=T(\dot q)-U(q)$ is $m\ddot q=-\nabla U(q)$. Then we want to show that if $q$ holds the previous equation then $E=T+U=K$ where $K$ is a constant. If Im not wrong the exercise can be rewritten as $$ m\ddot q+\nabla U(q)=0\implies m\frac{|\dot q|^2}2+U(q)=K,\quad K\in\Bbb R \tag2$$ However from $E=K$ I find that $\frac{\partial}{\partial q} E=\nabla U(q)=0$ and $\frac\partial{\partial \dot q} E=m\dot q=0$, that is, it seems that Im proving the opposite direction of $(2)$. Some help will be appreciated, thank you.",,"['analysis', 'derivatives', 'calculus-of-variations']"
29,Finding the Taylor polynomial of $f(x) = \frac{1}{x}$ with induction,Finding the Taylor polynomial of  with induction,f(x) = \frac{1}{x},"So I am asked to find the Taylor polynomial of $f(x) = \frac{1}{x}$ about the point $a=1$ for ever n$\in{N}$, and then use induction to justify the answer. I got the Taylor polynomial which was simple enough: $$T_{n}(x)=\sum_{n=0}^{\infty} \frac{f^n(x)(a)}{n!}(x-a)^n$$ $$f(x) = 1-(x-1)+(x-1)^2-(x-1)^3+(x-1)^4+...$$ $$T_{n}(x)=\sum_{n=0}^{\infty} (-1)^n(x-1)^n$$ That wasn't too bad. How do I justify this with induction though? I am a little confused as to how I would start this. I tried writing out the terms as such: $$1-(x-1)+(x-1)^2-(x-1)^3+(x-1)^4+...+(-1)^k(x-1)^k = \frac{1}{x}$$ I am not really sure how to go about doing this though... Is my step valid? I would appreciate is someone could guide me in the right direction.","So I am asked to find the Taylor polynomial of $f(x) = \frac{1}{x}$ about the point $a=1$ for ever n$\in{N}$, and then use induction to justify the answer. I got the Taylor polynomial which was simple enough: $$T_{n}(x)=\sum_{n=0}^{\infty} \frac{f^n(x)(a)}{n!}(x-a)^n$$ $$f(x) = 1-(x-1)+(x-1)^2-(x-1)^3+(x-1)^4+...$$ $$T_{n}(x)=\sum_{n=0}^{\infty} (-1)^n(x-1)^n$$ That wasn't too bad. How do I justify this with induction though? I am a little confused as to how I would start this. I tried writing out the terms as such: $$1-(x-1)+(x-1)^2-(x-1)^3+(x-1)^4+...+(-1)^k(x-1)^k = \frac{1}{x}$$ I am not really sure how to go about doing this though... Is my step valid? I would appreciate is someone could guide me in the right direction.",,"['derivatives', 'induction', 'taylor-expansion']"
30,Cauchy–Riemann equations and complex differentiability at origin,Cauchy–Riemann equations and complex differentiability at origin,,"Consider the function defined by $$f(z)=\begin{cases}0&\text{ if }\operatorname{Re}(z)=0\vee\operatorname{Im}(z)=0\\1&\text{ otherwise.}\end{cases}$$ I would like to show that the real and imaginary parts of $f$ satisfy the Cauchy-Riemann equations at the origin but am not sure if just saying $0=0$ and $0=-0$ is enough or not also is $f$ complex differentiable at the origin, I am not sure on how to show whether it is or not, thank you","Consider the function defined by $$f(z)=\begin{cases}0&\text{ if }\operatorname{Re}(z)=0\vee\operatorname{Im}(z)=0\\1&\text{ otherwise.}\end{cases}$$ I would like to show that the real and imaginary parts of $f$ satisfy the Cauchy-Riemann equations at the origin but am not sure if just saying $0=0$ and $0=-0$ is enough or not also is $f$ complex differentiable at the origin, I am not sure on how to show whether it is or not, thank you",,"['complex-analysis', 'derivatives', 'cauchy-riemann-equations']"
31,"Using the definition of derivative, find $f'(x)$ where $f(x) = \frac{\cos x}{x}$","Using the definition of derivative, find  where",f'(x) f(x) = \frac{\cos x}{x},"I have attempted to solve the problem, but got stuck on the way, see below. \begin{align*} f'(x)&=\lim_{h\to 0} \frac{f(x+h)-f(x)}{h}\\ &= \lim_{h\to 0}\frac{\frac{\cos(x+h)}{x+h}-\frac{\cos x}{x}}{h}\\ &=\lim_{h\to 0}\frac{x\cos(x+h)-(h+x)\cos x}{xh(x+h)}\\ &=\lim_{h\to 0}\frac{x\cos h\cos x-x\sin h\sin x-(h+x)\cos x}{xh(x+h)}\\ &=\lim_{h\to 0}\frac{1}{x(x+h)}\left(\frac{x\cos x(\cos h-1)-h\cos x}{h}-\frac{x\sin x\sin h}{h}\right)\\ &=\lim_{h\to 0}\frac{1}{x(x+h)}\left(\frac{x\cos x(\cos h-1)}{h}-\cos x-\left(\frac{\sin h}{h}\right)x\sin x\right) \end{align*} From here I cannot solve $$\lim_{h\to 0}\frac{x\cos x(\cos h-1)}{h}.$$ Any suggestions? Or maybe I have taken the wrong route.","I have attempted to solve the problem, but got stuck on the way, see below. \begin{align*} f'(x)&=\lim_{h\to 0} \frac{f(x+h)-f(x)}{h}\\ &= \lim_{h\to 0}\frac{\frac{\cos(x+h)}{x+h}-\frac{\cos x}{x}}{h}\\ &=\lim_{h\to 0}\frac{x\cos(x+h)-(h+x)\cos x}{xh(x+h)}\\ &=\lim_{h\to 0}\frac{x\cos h\cos x-x\sin h\sin x-(h+x)\cos x}{xh(x+h)}\\ &=\lim_{h\to 0}\frac{1}{x(x+h)}\left(\frac{x\cos x(\cos h-1)-h\cos x}{h}-\frac{x\sin x\sin h}{h}\right)\\ &=\lim_{h\to 0}\frac{1}{x(x+h)}\left(\frac{x\cos x(\cos h-1)}{h}-\cos x-\left(\frac{\sin h}{h}\right)x\sin x\right) \end{align*} From here I cannot solve $$\lim_{h\to 0}\frac{x\cos x(\cos h-1)}{h}.$$ Any suggestions? Or maybe I have taken the wrong route.",,"['calculus', 'limits', 'derivatives']"
32,Uniqueness of minimum within interval,Uniqueness of minimum within interval,,"Assuming a smooth real valued function $$ f(x): \mathbb{R} \rightarrow \mathbb{R} $$ with $x^{*}$ being a local minimum of $f$. Further let $$ f^{(k)}(x) := \frac{d^k}{d x^k}f(x) $$ be the first non-zero derivative of $f$ at $x^{*}$. Question : Say I can show that $f^{(k)}$ is strictly positive on an interval containing $x^{*}$, does this imply that $x^{*}$ is the only minimum in this interval? If yes, does this extend to the multidimensional case with $f(x):\mathbb{R}^n \rightarrow\mathbb{R}$?","Assuming a smooth real valued function $$ f(x): \mathbb{R} \rightarrow \mathbb{R} $$ with $x^{*}$ being a local minimum of $f$. Further let $$ f^{(k)}(x) := \frac{d^k}{d x^k}f(x) $$ be the first non-zero derivative of $f$ at $x^{*}$. Question : Say I can show that $f^{(k)}$ is strictly positive on an interval containing $x^{*}$, does this imply that $x^{*}$ is the only minimum in this interval? If yes, does this extend to the multidimensional case with $f(x):\mathbb{R}^n \rightarrow\mathbb{R}$?",,"['calculus', 'derivatives', 'optimization']"
33,How to differentiate one function with respect to another? [closed],How to differentiate one function with respect to another? [closed],,"Closed. This question is off-topic . It is not currently accepting answers. Closed 6 years ago . This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. This question is not about mathematics, within the scope defined in the help center . Improve this question I have two functions, u(x,y) and v(x,y), that are both intractable analytically and estimated numerically in MATLAB on a grid for x and y. How can I estimate du(x,y)/dv(x,y) at all points (x,y) on the grid? I want to plot du(x,y)/dv(x,y) against x and y.","Closed. This question is off-topic . It is not currently accepting answers. Closed 6 years ago . This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. This question is not about mathematics, within the scope defined in the help center . Improve this question I have two functions, u(x,y) and v(x,y), that are both intractable analytically and estimated numerically in MATLAB on a grid for x and y. How can I estimate du(x,y)/dv(x,y) at all points (x,y) on the grid? I want to plot du(x,y)/dv(x,y) against x and y.",,['derivatives']
34,The Mean Value Theorem without an equation,The Mean Value Theorem without an equation,,"I've done this question a few times but I can't seem to figure out what I'm doing wrong...  In the question, there is a statement and then a graph.  The question states: Applying the Mean Value Theorem with $a = 2$, $b = 7$, and $c = 4$. What is  the equation of the tangent line at 4? Then the figure below the question shows part of circle with the points, $(2, 3)$, $(4, 6)$, $(7, 7)$ and a tangent line passing the point, $(4, 6)$, which, from what I understand, is point $c$. The answer is a fill in the blanks, $y =$ ______ So, first I wrote out the equation for The Mean Value Theorem: $${f'(c) = \frac{f(b) - f(a)}{b - a}}$$ Then I plugged in the values: $${f'(4) = \frac{f(7) - f(2)}{7 - 2}}$$ Using the points on the graph, I plug in the f(x) values $${f'(4) = \frac{7 - 3}{5}}$$ $${f'(4) = \frac{4}{5}}$$ $\frac{4}{5}$, however, is not the answer because I am not looking for $y'$, I'm looking for $y$. What do I do next? How do I find y?","I've done this question a few times but I can't seem to figure out what I'm doing wrong...  In the question, there is a statement and then a graph.  The question states: Applying the Mean Value Theorem with $a = 2$, $b = 7$, and $c = 4$. What is  the equation of the tangent line at 4? Then the figure below the question shows part of circle with the points, $(2, 3)$, $(4, 6)$, $(7, 7)$ and a tangent line passing the point, $(4, 6)$, which, from what I understand, is point $c$. The answer is a fill in the blanks, $y =$ ______ So, first I wrote out the equation for The Mean Value Theorem: $${f'(c) = \frac{f(b) - f(a)}{b - a}}$$ Then I plugged in the values: $${f'(4) = \frac{f(7) - f(2)}{7 - 2}}$$ Using the points on the graph, I plug in the f(x) values $${f'(4) = \frac{7 - 3}{5}}$$ $${f'(4) = \frac{4}{5}}$$ $\frac{4}{5}$, however, is not the answer because I am not looking for $y'$, I'm looking for $y$. What do I do next? How do I find y?",,"['calculus', 'derivatives']"
35,Can I compute $\frac{dy}{dx}$ of $y=x\sqrt{3x+1}\sqrt{x+1}$ by taking $ln$ on both sides,Can I compute  of  by taking  on both sides,\frac{dy}{dx} y=x\sqrt{3x+1}\sqrt{x+1} ln,"My teacher has suggested that to compute $\frac{dy}{dx}$ of $y=x\sqrt{3x+1}\sqrt{x+1}$, it's better to take $ln$ on both sides of the equation $y=x\sqrt{3x+1}\sqrt{x+1}$, and try taking derivative of logarithms on both sides, and then solve for $\frac{dy}{dx}$. But I am confused because when you consider the function $y=x\sqrt{3x+1}\sqrt{x+1}$,  $y \lt 0$ for $-1/3 \lt x \lt 0$, so $ln(y)$ is not defined for $-1/3 \lt x \lt 0$, i.e. we cannot take logarithm for entire domain in which $y=x\sqrt{3x+1}\sqrt{x+1}$ is defined for. In this case, can we still compute $\frac{dy}{dx}$ by taking $ln$ on both sides, like my teacher is suggesting? Thank you,","My teacher has suggested that to compute $\frac{dy}{dx}$ of $y=x\sqrt{3x+1}\sqrt{x+1}$, it's better to take $ln$ on both sides of the equation $y=x\sqrt{3x+1}\sqrt{x+1}$, and try taking derivative of logarithms on both sides, and then solve for $\frac{dy}{dx}$. But I am confused because when you consider the function $y=x\sqrt{3x+1}\sqrt{x+1}$,  $y \lt 0$ for $-1/3 \lt x \lt 0$, so $ln(y)$ is not defined for $-1/3 \lt x \lt 0$, i.e. we cannot take logarithm for entire domain in which $y=x\sqrt{3x+1}\sqrt{x+1}$ is defined for. In this case, can we still compute $\frac{dy}{dx}$ by taking $ln$ on both sides, like my teacher is suggesting? Thank you,",,"['calculus', 'derivatives']"
36,Comparing $\|f\|$ with $\|f''\|$ on an open interval,Comparing  with  on an open interval,\|f\| \|f''\|,"So I was looking at one of my dad's old math exams and came across the following question: Suppose $f\in C^2((a,b))$ (open interval) and for some $a <x_1 <x_2 <b$ we have $f(x_1) =0 =f(x_2)$. Prove $\|f\| \le (b-a)^2 \|f''\|$ where $\|g\| = \sup_{x \in (a,b)} |g(x)|$. My first inclination was a FTC argument, but of course this can't work (for example, $1/x$ on $(0,1)$). I also considered the following: By Rolle's, there exists $c \in (x_1, x_2)$ such that $f'(c) =0$. Then Taylor expanding, we have $f(x) = f(c) + \displaystyle \frac12 (x-c)^2 f''(c) +g(x-c)$, where $g(x-c) \to 0$ as $x\to c$, but this was also fruitless. Any ideas? I feel like the problem must use Rolle's theorem somehow, but I'm not sure how.","So I was looking at one of my dad's old math exams and came across the following question: Suppose $f\in C^2((a,b))$ (open interval) and for some $a <x_1 <x_2 <b$ we have $f(x_1) =0 =f(x_2)$. Prove $\|f\| \le (b-a)^2 \|f''\|$ where $\|g\| = \sup_{x \in (a,b)} |g(x)|$. My first inclination was a FTC argument, but of course this can't work (for example, $1/x$ on $(0,1)$). I also considered the following: By Rolle's, there exists $c \in (x_1, x_2)$ such that $f'(c) =0$. Then Taylor expanding, we have $f(x) = f(c) + \displaystyle \frac12 (x-c)^2 f''(c) +g(x-c)$, where $g(x-c) \to 0$ as $x\to c$, but this was also fruitless. Any ideas? I feel like the problem must use Rolle's theorem somehow, but I'm not sure how.",,"['calculus', 'real-analysis', 'derivatives']"
37,Derivative of matrix inverse from the definition,Derivative of matrix inverse from the definition,,"How to compute (directly from the definition) the derivative of matrix-valued function $M^{-1}(t)$ with respect to $t$ and recover the standard result $-M^{-1}(t)\frac{dM}{dt}M^{-1}(t)$? A similar question has been asked on this site before involving this computation several times before, but without the restriction the proof come directly from the definition. In this case, I know how to give a proof using the fact that $M^{-1}(t)M(t) = I$, and applying the product rule. However, I would like to give an argument directly from the definition if possible. A few hours of tinkering have led me nowhere fast - the issue is that the known formula is in terms of the derivative of $M$, and all methods I know of relating $M$ to $M^{-1}$, such as through the adjugate formula seem to be too ugly to recover the formula in question. Either I'm missing something, or this problem is difficult without the slick product-rule approach. Since this is homework (of course, why else would such an arbitrary restriction be imposed on an otherwise fine argument?), a full solution is probably not necessary.","How to compute (directly from the definition) the derivative of matrix-valued function $M^{-1}(t)$ with respect to $t$ and recover the standard result $-M^{-1}(t)\frac{dM}{dt}M^{-1}(t)$? A similar question has been asked on this site before involving this computation several times before, but without the restriction the proof come directly from the definition. In this case, I know how to give a proof using the fact that $M^{-1}(t)M(t) = I$, and applying the product rule. However, I would like to give an argument directly from the definition if possible. A few hours of tinkering have led me nowhere fast - the issue is that the known formula is in terms of the derivative of $M$, and all methods I know of relating $M$ to $M^{-1}$, such as through the adjugate formula seem to be too ugly to recover the formula in question. Either I'm missing something, or this problem is difficult without the slick product-rule approach. Since this is homework (of course, why else would such an arbitrary restriction be imposed on an otherwise fine argument?), a full solution is probably not necessary.",,"['calculus', 'derivatives', 'matrix-calculus']"
38,What kind of convergence is in Gateaux derivative?,What kind of convergence is in Gateaux derivative?,,"Let $F$ map $X$ to another Banach space $Y$. The usual (one sided) directional derivative of $F$ at $x$ in the direction $v$ is  $$F'(x;v)=\lim_{t\downarrow0}\frac{F(x+tv)-F(x)}{t}\qquad(1)$$ when this limit exists. If $F'(x;\cdot)\in\mathcal{L}(X,Y)$ then (denoting $DF(x)=F'(x;\cdot)$) $DF(x)$ is Gâteaux derivative. My question is: what kind of convergence is in (1)? Is it pointwise convergence? If we consider convergence with respect to the norm $\|\cdot\|_{Y}$, i.e., $$\lim_{t\downarrow0}\|F'(x;v)-\frac{F(x+tv)-F(x)}{t}\|_{Y}=0$$  and once again $F'(x;\cdot)\in\mathcal{L}(X,Y)$, I think we obtain Fréchet derivative. Am I correct?","Let $F$ map $X$ to another Banach space $Y$. The usual (one sided) directional derivative of $F$ at $x$ in the direction $v$ is  $$F'(x;v)=\lim_{t\downarrow0}\frac{F(x+tv)-F(x)}{t}\qquad(1)$$ when this limit exists. If $F'(x;\cdot)\in\mathcal{L}(X,Y)$ then (denoting $DF(x)=F'(x;\cdot)$) $DF(x)$ is Gâteaux derivative. My question is: what kind of convergence is in (1)? Is it pointwise convergence? If we consider convergence with respect to the norm $\|\cdot\|_{Y}$, i.e., $$\lim_{t\downarrow0}\|F'(x;v)-\frac{F(x+tv)-F(x)}{t}\|_{Y}=0$$  and once again $F'(x;\cdot)\in\mathcal{L}(X,Y)$, I think we obtain Fréchet derivative. Am I correct?",,"['real-analysis', 'derivatives']"
39,Help needed to begin with a proof,Help needed to begin with a proof,,"Let $f(x),g (x) $ be two continuous and differentiable functions on [a,b] then prove that there exists a c such that $a <c <b $ and $\frac {f (b)-f (a)}{g (b)-g (a)}=\frac {f'(c)g^2 (c)}{g'(c)g (a)(g (b)} $. The part without $\frac {g^2 (c)}{g (a)g (b)} $ is a known Cauchy MVT which can be proved using $h (x)=f (x)+Ag (x) $ where $A\in R $ and using help of Rolles theorem.  But bringing that extra part in the proof is not easy.Any hints on how to start?Just the first step or an idea would do. Thanks!","Let $f(x),g (x) $ be two continuous and differentiable functions on [a,b] then prove that there exists a c such that $a <c <b $ and $\frac {f (b)-f (a)}{g (b)-g (a)}=\frac {f'(c)g^2 (c)}{g'(c)g (a)(g (b)} $. The part without $\frac {g^2 (c)}{g (a)g (b)} $ is a known Cauchy MVT which can be proved using $h (x)=f (x)+Ag (x) $ where $A\in R $ and using help of Rolles theorem.  But bringing that extra part in the proof is not easy.Any hints on how to start?Just the first step or an idea would do. Thanks!",,"['calculus', 'derivatives']"
40,Derivative of $u ^3$,Derivative of,u ^3,"In Gilbert Strang's Calculus, section 2.5, the author explains the power rule of derivative by giving the example of change in volume of a cube. For cube of side u i.e. $u^3$, the derivative is $3u^2$. However for a change in side of $\Delta u$, the actual change in volume is $3u^2 \Delta u + 3u(\Delta u)^2 + \Delta u ^3$. So, what happens to the last two terms when we look at the rate of change i.e. the derivative. Are they ignored as $\Delta u$ is small and any powers of it would be smaller still?","In Gilbert Strang's Calculus, section 2.5, the author explains the power rule of derivative by giving the example of change in volume of a cube. For cube of side u i.e. $u^3$, the derivative is $3u^2$. However for a change in side of $\Delta u$, the actual change in volume is $3u^2 \Delta u + 3u(\Delta u)^2 + \Delta u ^3$. So, what happens to the last two terms when we look at the rate of change i.e. the derivative. Are they ignored as $\Delta u$ is small and any powers of it would be smaller still?",,"['calculus', 'derivatives']"
41,How to compute the Gateaux's derivative of a smooth but not analytic function?,How to compute the Gateaux's derivative of a smooth but not analytic function?,,"Let $f(x)=\begin{cases}x^2~\mbox{if $x\geq 0$}\\ 0~\mbox{otherwise}\end{cases}$ Let $A, B$ be two Hermitian matrices. We treat $f$ as a matrix function where $f(A)$ can be obtained from $A$ by applying $f$ to all the eigenvalues of $A$. My question is if the following derivative well defined. If yes, how to compute it? $\frac{d}{dt}f(A+tB)|_{t=0}$","Let $f(x)=\begin{cases}x^2~\mbox{if $x\geq 0$}\\ 0~\mbox{otherwise}\end{cases}$ Let $A, B$ be two Hermitian matrices. We treat $f$ as a matrix function where $f(A)$ can be obtained from $A$ by applying $f$ to all the eigenvalues of $A$. My question is if the following derivative well defined. If yes, how to compute it? $\frac{d}{dt}f(A+tB)|_{t=0}$",,"['linear-algebra', 'matrices', 'derivatives', 'matrix-calculus', 'frechet-derivative']"
42,Finding the Derivative of $\arctan \frac{x}{a - \sqrt{a^2 - x^2}}$,Finding the Derivative of,\arctan \frac{x}{a - \sqrt{a^2 - x^2}},"I am trying to simplify the derivative of $\arctan \frac{x}{a - \sqrt{a^2 - x^2}}$. My work: Everybody knows that $\frac{d}{dx} (\arctan \space u) = \frac{1}{1 + u^2} \frac{du}{dx}$ We let $u =  \frac{x}{a - \sqrt{a^2 - x^2}} .$ To get the $du,$ I remember that $\frac{d}{dx} \left( \frac{u}{v} \right)= \frac{v \frac{du}{dx} - u \frac{dv}{dx}}{v^2}.$ So:  $$\frac{d}{dx} \left ( \frac{x}{a - \sqrt{a^2 - x^2}}  \right) = \frac{a-\sqrt{a^2 - x^2} \frac{d}{dx} (x) - x \frac{d}{dx} (a-\sqrt{a^2 - x^2)}}{(a-\sqrt{a^2 - x^2)^2}} $$ $$ = \frac{a - \sqrt{a^2 - x^2} -x \left(\frac{x}{(\sqrt{a^2 - x^2})}\right)}{a^2 - 2\sqrt{a^2 - x^2} +a^2 - x^2}$$ $$ = \frac{ \frac{a - \sqrt{a^2 - x^2}}{1} + \frac{-x^2}{ \sqrt{a^2 - x^2} }}{2a^2 - 2\sqrt{a^2 - x^2} - x^2}$$ $$ = \frac{\frac{(a^2 - x^2)^{\frac{1}{2}} (a - (a^2 - x^2)^\frac{1}{2}) - x^2}{(a^2 - x^2)^{\frac{1}{2}}}}{2a^2 - x^2 - 2\sqrt{a^2 - x^2} }$$ $$ \frac{d}{dx} \left( \frac{x}{a - \sqrt{a^2 - x^2}}\right) = \frac{a(a^2- x^2)^\frac{1}{2} - (a^2- x^2) - x^2}{2a^2 - x^2 - 2\sqrt{a^2 - x^2} }$$ Then: $$\frac{d}{dx} \left( \arctan \frac{x}{a - \sqrt{a^2 - x^2}} \right) = \frac{1}{1 + \left( \frac{x}{a - \sqrt{a^2 - x^2}} \right)^2}  \left( \frac{a(a^2- x^2)^{\frac{1}{2}} - (a^2- x^2) - x^2}{2a^2 - x^2 - 2\sqrt{a^2 - x^2} } \right) $$ At this point, simplifying it is difficult. How do you get the derivative of $\arctan \frac{x}{a - \sqrt{a^2 - x^2}}?$","I am trying to simplify the derivative of $\arctan \frac{x}{a - \sqrt{a^2 - x^2}}$. My work: Everybody knows that $\frac{d}{dx} (\arctan \space u) = \frac{1}{1 + u^2} \frac{du}{dx}$ We let $u =  \frac{x}{a - \sqrt{a^2 - x^2}} .$ To get the $du,$ I remember that $\frac{d}{dx} \left( \frac{u}{v} \right)= \frac{v \frac{du}{dx} - u \frac{dv}{dx}}{v^2}.$ So:  $$\frac{d}{dx} \left ( \frac{x}{a - \sqrt{a^2 - x^2}}  \right) = \frac{a-\sqrt{a^2 - x^2} \frac{d}{dx} (x) - x \frac{d}{dx} (a-\sqrt{a^2 - x^2)}}{(a-\sqrt{a^2 - x^2)^2}} $$ $$ = \frac{a - \sqrt{a^2 - x^2} -x \left(\frac{x}{(\sqrt{a^2 - x^2})}\right)}{a^2 - 2\sqrt{a^2 - x^2} +a^2 - x^2}$$ $$ = \frac{ \frac{a - \sqrt{a^2 - x^2}}{1} + \frac{-x^2}{ \sqrt{a^2 - x^2} }}{2a^2 - 2\sqrt{a^2 - x^2} - x^2}$$ $$ = \frac{\frac{(a^2 - x^2)^{\frac{1}{2}} (a - (a^2 - x^2)^\frac{1}{2}) - x^2}{(a^2 - x^2)^{\frac{1}{2}}}}{2a^2 - x^2 - 2\sqrt{a^2 - x^2} }$$ $$ \frac{d}{dx} \left( \frac{x}{a - \sqrt{a^2 - x^2}}\right) = \frac{a(a^2- x^2)^\frac{1}{2} - (a^2- x^2) - x^2}{2a^2 - x^2 - 2\sqrt{a^2 - x^2} }$$ Then: $$\frac{d}{dx} \left( \arctan \frac{x}{a - \sqrt{a^2 - x^2}} \right) = \frac{1}{1 + \left( \frac{x}{a - \sqrt{a^2 - x^2}} \right)^2}  \left( \frac{a(a^2- x^2)^{\frac{1}{2}} - (a^2- x^2) - x^2}{2a^2 - x^2 - 2\sqrt{a^2 - x^2} } \right) $$ At this point, simplifying it is difficult. How do you get the derivative of $\arctan \frac{x}{a - \sqrt{a^2 - x^2}}?$",,"['calculus', 'trigonometry', 'derivatives', 'fractions', 'inverse-function']"
43,Is it correct to use partial derivative notation even when we know that function is dependent on only one variable?,Is it correct to use partial derivative notation even when we know that function is dependent on only one variable?,,"This is a question about notation. I am trying to understand when it is appropriate to rewrite $ \frac{\partial f}{\partial x} $ as $ \frac{df}{dx} $ while performing a derivation. Say we are given, $$ f(x, y) = 4x^2 + 27y^3. $$ Let us define, $$ u(x) = 2x. $$ Then, $$ f(x, y) = f(u(x), y) = (u(x))^2 + 27y^3. $$ If I now want to find the partial derivative $ \frac{\partial f(x, y)}{\partial x} $, I could do the following. $$ \frac{\partial f(x, y)}{\partial x} = \frac{\partial f(u, y)}{\partial u} \cdot \frac{\partial u(x)}{\partial x} $$ Is the above expression correct? Or should I have written it as the following, $$ \frac{\partial f(x, y)}{\partial x} = \frac{\partial f(u, y)}{\partial u} \cdot \frac{d u(x)}{dx} $$ Or are both the above expressions correct?","This is a question about notation. I am trying to understand when it is appropriate to rewrite $ \frac{\partial f}{\partial x} $ as $ \frac{df}{dx} $ while performing a derivation. Say we are given, $$ f(x, y) = 4x^2 + 27y^3. $$ Let us define, $$ u(x) = 2x. $$ Then, $$ f(x, y) = f(u(x), y) = (u(x))^2 + 27y^3. $$ If I now want to find the partial derivative $ \frac{\partial f(x, y)}{\partial x} $, I could do the following. $$ \frac{\partial f(x, y)}{\partial x} = \frac{\partial f(u, y)}{\partial u} \cdot \frac{\partial u(x)}{\partial x} $$ Is the above expression correct? Or should I have written it as the following, $$ \frac{\partial f(x, y)}{\partial x} = \frac{\partial f(u, y)}{\partial u} \cdot \frac{d u(x)}{dx} $$ Or are both the above expressions correct?",,"['derivatives', 'notation', 'partial-derivative']"
44,How to calculate the limit after solving a differential equation?,How to calculate the limit after solving a differential equation?,,"The solution of $$\frac{\partial c}{\partial t} = D\frac{\partial^2c}{\partial x^2}$$ with boundary conditions $$\begin{cases} c(x,0) = C^\infty\\ c(0,t)=0\\ c(\infty,t)=C^\infty \end{cases}$$ is $$c(x,t) = C^\infty \operatorname{erfc}\Big(\frac{x}{2\sqrt{Dt}}\Big)$$ and then the flux is, $$J_{(x=0)} = D\frac{\partial c(x,t)}{\partial t} = \frac{C^\infty\sqrt{D}}{\sqrt{\pi t}}$$ what is the limit (maximum) of $J$? When $t \to 0$ (at short times) then $J \to \infty$. It doesn't make sense.","The solution of $$\frac{\partial c}{\partial t} = D\frac{\partial^2c}{\partial x^2}$$ with boundary conditions $$\begin{cases} c(x,0) = C^\infty\\ c(0,t)=0\\ c(\infty,t)=C^\infty \end{cases}$$ is $$c(x,t) = C^\infty \operatorname{erfc}\Big(\frac{x}{2\sqrt{Dt}}\Big)$$ and then the flux is, $$J_{(x=0)} = D\frac{\partial c(x,t)}{\partial t} = \frac{C^\infty\sqrt{D}}{\sqrt{\pi t}}$$ what is the limit (maximum) of $J$? When $t \to 0$ (at short times) then $J \to \infty$. It doesn't make sense.",,"['ordinary-differential-equations', 'limits', 'derivatives', 'partial-differential-equations']"
45,"How to minimize the integral of the functional of a function, with respect to that function?","How to minimize the integral of the functional of a function, with respect to that function?",,"I need to obtain the function $f(x)$ for which the following integral has  its minimum value: $I=\int F(f(x))dx= \int [A (B^2-f(x)^2)^2-Cf(x)f''(x)]dx$ One special solution is $f(x)=constant=\pm B$, but I need the general solution such that $f(x) \ne constant$. Then the systematic approach is to minimize '$I$' with respect to $f(x)$. I started with $\dfrac{dI}{df(x)}=0 $. Then I differentiate both side with respect to $x$ so that I get rid of the integral and end up with $\dfrac{dF(f(x))}{df(x)}=0 $ This step gives me:  $2A(B^2-f(x)^2).[-2f(x)] -C\dfrac{d}{df(x)}f(x)f''(x)=0$ At this point how to carry out the second part? Shall I consider the $f''(x)$ be constant with respect to $f(x)$? Doing so would give a differential equation to solve for $f(x)$. But I am not sure whether this is the right way or not. Thanks in advance","I need to obtain the function $f(x)$ for which the following integral has  its minimum value: $I=\int F(f(x))dx= \int [A (B^2-f(x)^2)^2-Cf(x)f''(x)]dx$ One special solution is $f(x)=constant=\pm B$, but I need the general solution such that $f(x) \ne constant$. Then the systematic approach is to minimize '$I$' with respect to $f(x)$. I started with $\dfrac{dI}{df(x)}=0 $. Then I differentiate both side with respect to $x$ so that I get rid of the integral and end up with $\dfrac{dF(f(x))}{df(x)}=0 $ This step gives me:  $2A(B^2-f(x)^2).[-2f(x)] -C\dfrac{d}{df(x)}f(x)f''(x)=0$ At this point how to carry out the second part? Shall I consider the $f''(x)$ be constant with respect to $f(x)$? Doing so would give a differential equation to solve for $f(x)$. But I am not sure whether this is the right way or not. Thanks in advance",,"['calculus', 'derivatives', 'calculus-of-variations', 'maxima-minima']"
46,general form of functions whose derivatives can be written in terms of the function itself?,general form of functions whose derivatives can be written in terms of the function itself?,,"A classic example for this would be the exponential function, or sigmoid function, or even tanh function. But is there a general form for all these functions, in such a way that the general form follows the same property? (ie the general form function's derivative can be written in terms of the function itself...)","A classic example for this would be the exponential function, or sigmoid function, or even tanh function. But is there a general form for all these functions, in such a way that the general form follows the same property? (ie the general form function's derivative can be written in terms of the function itself...)",,"['calculus', 'derivatives', 'exponential-function', 'hyperbolic-functions']"
47,Is there such a thing as the derivative of an integer?,Is there such a thing as the derivative of an integer?,,"A polynomial has a square divisor (i.e., a double root) if and only if that root divides the derivative of that polynomial. Is there a corresponding algebraic definition of the derivative of an integer, so that an integer $n$ has a square divisor $p^2$ if an only if $p$ divides the derivative of $n$? Another way of putting it: $gcd(n,n')$ gives you the square factors of $n$. If such a derivative existed, it would be a useful tool in factoring integers.","A polynomial has a square divisor (i.e., a double root) if and only if that root divides the derivative of that polynomial. Is there a corresponding algebraic definition of the derivative of an integer, so that an integer $n$ has a square divisor $p^2$ if an only if $p$ divides the derivative of $n$? Another way of putting it: $gcd(n,n')$ gives you the square factors of $n$. If such a derivative existed, it would be a useful tool in factoring integers.",,"['derivatives', 'polynomials', 'integers']"
48,Matrix differentiation: Combination of vectors and matrices,Matrix differentiation: Combination of vectors and matrices,,"I want to differentiate: $f(w) = w^TF^TFw - w^TF^Tt- t^TFw$ with respect to w. F is a $n*d$ matrix, w is a $d*1$ vector, y is a $n*1$ vector. I read sometimes that $(w^TF^Tt)'$ = $(F^Tt)^T$, and sometimes that it is  $(F^Tt)$ - why is that? Furthermore, I know that generally $(w^TAw)' = w^T(A+A^T)$. Should it not follow that $w^TF^TFw = w^T(F^TF+F^TF)$?","I want to differentiate: $f(w) = w^TF^TFw - w^TF^Tt- t^TFw$ with respect to w. F is a $n*d$ matrix, w is a $d*1$ vector, y is a $n*1$ vector. I read sometimes that $(w^TF^Tt)'$ = $(F^Tt)^T$, and sometimes that it is  $(F^Tt)$ - why is that? Furthermore, I know that generally $(w^TAw)' = w^T(A+A^T)$. Should it not follow that $w^TF^TFw = w^T(F^TF+F^TF)$?",,"['linear-algebra', 'matrices']"
49,"Differential 1-form for $f(x,y) = \sin(x^2 + y^2)$ on vector field $\mathbf{A}=x\partial_x + y\partial_y$",Differential 1-form for  on vector field,"f(x,y) = \sin(x^2 + y^2) \mathbf{A}=x\partial_x + y\partial_y","I was doing some research into differential 1-forms and came across a problem which asks for the differential 1-form $\omega = df$ on the vector field $\mathbf{A}=x\partial_x + y\partial_y$ if $f(x,y) = \sin(x^2 + y^2)$. So I undertand that if $\omega$ is a differential 1-form then it can be expressed as $\omega=df=F(x,y)dx+G(x,y)dy=2x\cos(x^2+y^2)dx + 2y\cos(x^2+y^2)dy$. But $dx$ and $dy$ have vector arguments, so as a function of $\textbf{A}$ this is $\omega(\textbf{A})=2x\cos(x^2+y^2)dx(\textbf{A}) + 2y\cos(x^2+y^2)dy(\textbf{A})=2x\cos(x^2+y^2)A_x + 2y\cos(x^2+y^2)A_y$ In this case, the solution to the problem reads that $A_x = x$ and $A_y = y$, but why is this so, and what is the meaning of $\partial_x$ and $\partial_y$ in this problem (i.e. what are we taking partial derivatives of)?","I was doing some research into differential 1-forms and came across a problem which asks for the differential 1-form $\omega = df$ on the vector field $\mathbf{A}=x\partial_x + y\partial_y$ if $f(x,y) = \sin(x^2 + y^2)$. So I undertand that if $\omega$ is a differential 1-form then it can be expressed as $\omega=df=F(x,y)dx+G(x,y)dy=2x\cos(x^2+y^2)dx + 2y\cos(x^2+y^2)dy$. But $dx$ and $dy$ have vector arguments, so as a function of $\textbf{A}$ this is $\omega(\textbf{A})=2x\cos(x^2+y^2)dx(\textbf{A}) + 2y\cos(x^2+y^2)dy(\textbf{A})=2x\cos(x^2+y^2)A_x + 2y\cos(x^2+y^2)A_y$ In this case, the solution to the problem reads that $A_x = x$ and $A_y = y$, but why is this so, and what is the meaning of $\partial_x$ and $\partial_y$ in this problem (i.e. what are we taking partial derivatives of)?",,"['calculus', 'geometry', 'derivatives', 'differential-geometry']"
50,Derive the labour demand function.,Derive the labour demand function.,,"$$Y = 4[(K^α)(L^{(1-α)})]$$ I took the derivative with respect to $L$ , and ended up with: $$Y'= 4[(K^α)(1-α)L^{(-α)}$$ But the correct answer is something like: $$[4(1-a)K^a/w]^{(1/a)}$$ I'm not totally sure where the $w$ came from but my guess is MP of labor $= w$ , where $w =$ wages. Please show me the correct method.","I took the derivative with respect to , and ended up with: But the correct answer is something like: I'm not totally sure where the came from but my guess is MP of labor , where wages. Please show me the correct method.",Y = 4[(K^α)(L^{(1-α)})] L Y'= 4[(K^α)(1-α)L^{(-α)} [4(1-a)K^a/w]^{(1/a)} w = w w =,"['derivatives', 'partial-derivative', 'economics']"
51,Proving the composition of a convex function is subharmonic,Proving the composition of a convex function is subharmonic,,"Background Information: Def - If $U$ is a bounded, open set, we say that $v\in C^2(U)\cap C(\overline{U})$ is subharmonic if  $$-\Delta v \leq 0 \ \ \ \text{in} \ U$$ Question: Let $\phi:\mathbb{R}\to \mathbb{R}$ be smooth and convex (i.e., $\phi^{\prime \prime}(x) \geq 0 $). Suppose that $u$ is harmonic and define $v(x):= \phi(u(x))$. Prove that $v$ is subharmonic. Attempted proof - We have  $$\Delta v(x) = \Delta\left[\phi(u(x))\right] = \phi'(u(x))\Delta u(x) + \phi^{\prime \prime}(u(x))|\nabla u|^2$$ Before I continue I really do not understand at all where $\phi^{\prime \prime}(u(x))|\nabla u|^2$ comes in by the rules of differentiation. Please explain this clearly as possible using necessary definitions of $\Delta$ if needed.","Background Information: Def - If $U$ is a bounded, open set, we say that $v\in C^2(U)\cap C(\overline{U})$ is subharmonic if  $$-\Delta v \leq 0 \ \ \ \text{in} \ U$$ Question: Let $\phi:\mathbb{R}\to \mathbb{R}$ be smooth and convex (i.e., $\phi^{\prime \prime}(x) \geq 0 $). Suppose that $u$ is harmonic and define $v(x):= \phi(u(x))$. Prove that $v$ is subharmonic. Attempted proof - We have  $$\Delta v(x) = \Delta\left[\phi(u(x))\right] = \phi'(u(x))\Delta u(x) + \phi^{\prime \prime}(u(x))|\nabla u|^2$$ Before I continue I really do not understand at all where $\phi^{\prime \prime}(u(x))|\nabla u|^2$ comes in by the rules of differentiation. Please explain this clearly as possible using necessary definitions of $\Delta$ if needed.",,"['derivatives', 'proof-verification', 'partial-differential-equations', 'harmonic-functions']"
52,Limit Definition for Half-Derivative,Limit Definition for Half-Derivative,,"The derivative of a function $f$ is defined as $$\lim_{h\to 0} \frac{f(x+h)-f(x)}{h}$$ Let $$d_1(f,h)=\frac{f(x+h)-f(x)}{h}$$ and, in fact, let all $d_n$ be defined by $$\lim_{h\to 0} d_n(f,h)=f^{(n)}(x)$$ In order to obtain $d_2$, we can plug $d_1$ into itself to get $$\frac{\frac{f(x+2h)-f(x+h)}{h}-\frac{f(x+h)-f(x)}{h}}{h}$$ $$\frac{f(x+2h)-f(x+h)-f(x+h)-f(x)}{h^2}$$ $$\frac{f(x+2h)-2f(x+h)-f(x)}{h^2}$$ and, in general, we can use induction to prove that, for natural $n$, $$d_n(f,h)=\frac{1}{h^n}\sum_{k=0}^n (-1)^k \binom{n}{k}f(x+(n-k)h)$$ However, I am interested in finding $d_\frac{1}{2}$. It should satisfy $$d_\frac{1}{2}(d_\frac{1}{2}(f,h),h)=d_1(f,h)=\frac{f(x+h)-f(x)}{h}$$ So that when it is composed with itself as shown, $d_1$ is the result. It can possibly be obtained by figuring out how to extend the expression $$\frac{1}{h^n}\sum_{k=0}^n (-1)^k \binom{n}{k}f(x+(n-k)h)$$ to non-integer $n$. Does anybody have any ideas about how to do this?","The derivative of a function $f$ is defined as $$\lim_{h\to 0} \frac{f(x+h)-f(x)}{h}$$ Let $$d_1(f,h)=\frac{f(x+h)-f(x)}{h}$$ and, in fact, let all $d_n$ be defined by $$\lim_{h\to 0} d_n(f,h)=f^{(n)}(x)$$ In order to obtain $d_2$, we can plug $d_1$ into itself to get $$\frac{\frac{f(x+2h)-f(x+h)}{h}-\frac{f(x+h)-f(x)}{h}}{h}$$ $$\frac{f(x+2h)-f(x+h)-f(x+h)-f(x)}{h^2}$$ $$\frac{f(x+2h)-2f(x+h)-f(x)}{h^2}$$ and, in general, we can use induction to prove that, for natural $n$, $$d_n(f,h)=\frac{1}{h^n}\sum_{k=0}^n (-1)^k \binom{n}{k}f(x+(n-k)h)$$ However, I am interested in finding $d_\frac{1}{2}$. It should satisfy $$d_\frac{1}{2}(d_\frac{1}{2}(f,h),h)=d_1(f,h)=\frac{f(x+h)-f(x)}{h}$$ So that when it is composed with itself as shown, $d_1$ is the result. It can possibly be obtained by figuring out how to extend the expression $$\frac{1}{h^n}\sum_{k=0}^n (-1)^k \binom{n}{k}f(x+(n-k)h)$$ to non-integer $n$. Does anybody have any ideas about how to do this?",,"['limits', 'derivatives', 'fractional-calculus']"
53,Z transform of a polynomial signal,Z transform of a polynomial signal,,"$$x(k)=\begin{cases} k^n \qquad k\ge 0 \\ 0 \ \  \qquad k<0 \end{cases}$$ I have this formula to calculate the Z transform: $$\mathscr{Z} \{x(k) \}=\Big(-z \ \frac{d}{dz}\Big)^n \Big[ \frac{z}{z-1} \Big]$$ I have considered this particular case: $$x(k)=\begin{cases} k^2 \qquad k\ge 0 \\ 0 \ \  \qquad k<0 \end{cases}$$ $$f(z)=\frac{z}{z-1}$$ $$f'(z)=\frac{-1}{(z-1)^2}$$ $$-z f'(z)=\frac{z}{(z-1)^2}$$ $$g(z)=\frac{z}{(z-1)^2}$$ $$g'(z)=\frac{(z-1)^2-2z(z-1)}{(z-1)^4}=\frac{-z-1}{(z-1)^3}$$ $$-z g'(z)=\frac{z^2+z}{(z-1)^3}$$ So $$\mathscr{Z} \{ x(t) \}=\frac{z (z+1)}{(z-1)^3}$$ But, I have tried to calculate the inverse Z transform: $$\mathscr{Z}^{-1} \Big\{\frac{z (z+1)}{(z-1)^3} \Big\}=\mathscr{Z}^{-1} \Big\{H(z) \Big\}$$ Partial fraction decomposition of $\frac{H(z)}{z}$: $$\frac{z+1}{(z-1)^3}=\frac{1}{(z-1)^2}+\frac{2}{(z-1)^3}$$ $$\mathscr{Z}^{-1} \Big\{\frac{z}{(z-1)^2} \Big\}=k \ u(k)$$ $$\mathscr{Z}^{-1} \Big\{\frac{2z}{(z-1)^3} \Big\}=k^2 \ u(k)$$ $$\mathscr{Z}^{-1} \Big\{H(z) \Big\}=(k+k^2) \ u(k)$$ Where is the mistake? I know this formula to calculate the inverse Z transform: $$\mathscr{Z^{-1}}\Big[ \frac{C_{i,j} \ z}{(z-p_i)^j}\Big]=C_{i,j} \ \frac{k^{(j-1)}}{(j-1)!} \ p_i^{k-j+1} \ u(k)$$ Is it not applicable to $\frac{z(z+1)}{(z-1)^3}$? Thanks","$$x(k)=\begin{cases} k^n \qquad k\ge 0 \\ 0 \ \  \qquad k<0 \end{cases}$$ I have this formula to calculate the Z transform: $$\mathscr{Z} \{x(k) \}=\Big(-z \ \frac{d}{dz}\Big)^n \Big[ \frac{z}{z-1} \Big]$$ I have considered this particular case: $$x(k)=\begin{cases} k^2 \qquad k\ge 0 \\ 0 \ \  \qquad k<0 \end{cases}$$ $$f(z)=\frac{z}{z-1}$$ $$f'(z)=\frac{-1}{(z-1)^2}$$ $$-z f'(z)=\frac{z}{(z-1)^2}$$ $$g(z)=\frac{z}{(z-1)^2}$$ $$g'(z)=\frac{(z-1)^2-2z(z-1)}{(z-1)^4}=\frac{-z-1}{(z-1)^3}$$ $$-z g'(z)=\frac{z^2+z}{(z-1)^3}$$ So $$\mathscr{Z} \{ x(t) \}=\frac{z (z+1)}{(z-1)^3}$$ But, I have tried to calculate the inverse Z transform: $$\mathscr{Z}^{-1} \Big\{\frac{z (z+1)}{(z-1)^3} \Big\}=\mathscr{Z}^{-1} \Big\{H(z) \Big\}$$ Partial fraction decomposition of $\frac{H(z)}{z}$: $$\frac{z+1}{(z-1)^3}=\frac{1}{(z-1)^2}+\frac{2}{(z-1)^3}$$ $$\mathscr{Z}^{-1} \Big\{\frac{z}{(z-1)^2} \Big\}=k \ u(k)$$ $$\mathscr{Z}^{-1} \Big\{\frac{2z}{(z-1)^3} \Big\}=k^2 \ u(k)$$ $$\mathscr{Z}^{-1} \Big\{H(z) \Big\}=(k+k^2) \ u(k)$$ Where is the mistake? I know this formula to calculate the inverse Z transform: $$\mathscr{Z^{-1}}\Big[ \frac{C_{i,j} \ z}{(z-p_i)^j}\Big]=C_{i,j} \ \frac{k^{(j-1)}}{(j-1)!} \ p_i^{k-j+1} \ u(k)$$ Is it not applicable to $\frac{z(z+1)}{(z-1)^3}$? Thanks",,"['derivatives', 'transformation', 'z-transform']"
54,What happens when you change the variable in differentiation?,What happens when you change the variable in differentiation?,,"I was asked a somewhat elementary question, but it got me a bit confused. Suppose I have a function $f(k)$ and I change variables, so now $k = \frac{c}{y}$ where $c$ is constant. What happens to $\frac{df(k)}{dk}$ under this change? Is this correct: $dk = -\frac{c}{y^2} dy$, so $$\frac{df(k)}{dk} = \frac{df(\frac{c}{y})}{-\frac{c}{y^2} dy} =-\frac{y^2}{c} \frac{df(\frac{c}{y})}{dy}$$","I was asked a somewhat elementary question, but it got me a bit confused. Suppose I have a function $f(k)$ and I change variables, so now $k = \frac{c}{y}$ where $c$ is constant. What happens to $\frac{df(k)}{dk}$ under this change? Is this correct: $dk = -\frac{c}{y^2} dy$, so $$\frac{df(k)}{dk} = \frac{df(\frac{c}{y})}{-\frac{c}{y^2} dy} =-\frac{y^2}{c} \frac{df(\frac{c}{y})}{dy}$$",,"['calculus', 'derivatives']"
55,Error range for the Taylor polynomial Lorentz factor $γ$,Error range for the Taylor polynomial Lorentz factor,γ,"Consider the Lorentz factor (in special theory of relativity) as the function $$γ(x)=\frac{c}{\sqrt{c^2-x^2}},\;x\in[ 0 , c \rangle$$ Where $x=$ is the velocity of an object moving relative to another at rest. $C=$ speed of light in the vacuum. Wikipedia says that if γ (x) approaches with the Taylor polynomial centered on $ x = 0 $ of second degree $ P_2 (x) $, then the approximation error is as follows: The approximation $γ ≈ 1 + (1/2)β^2$ may be used to calculate relativistic effects at low speeds. It holds to within 1% error for $x < 0.4 c$, donde $β=x/c$. I suppose that refers to the error that is obtained with the rest of Lagrange. According to my own calculations this is: $$R_3(α,x)=\frac{{{\gamma ^{(3)}}(\alpha )}}{{3!}} \cdot {x^3}=\frac{{c\!\cdot\!\alpha\!\cdot\!\left( {{\alpha ^2} + \frac{3}{2}{c^2}} \right)}}{{{{\left( {{c^2} - {\alpha ^2}} \right)}^{7/2}}}} \cdot {x^3},\quad 0<α<x$$ Then the question is: How can I prove that $$\color{blue}{R(α,x)<1\%, \;\,\textrm{if}\; x < 0.4 c\,?}$$","Consider the Lorentz factor (in special theory of relativity) as the function $$γ(x)=\frac{c}{\sqrt{c^2-x^2}},\;x\in[ 0 , c \rangle$$ Where $x=$ is the velocity of an object moving relative to another at rest. $C=$ speed of light in the vacuum. Wikipedia says that if γ (x) approaches with the Taylor polynomial centered on $ x = 0 $ of second degree $ P_2 (x) $, then the approximation error is as follows: The approximation $γ ≈ 1 + (1/2)β^2$ may be used to calculate relativistic effects at low speeds. It holds to within 1% error for $x < 0.4 c$, donde $β=x/c$. I suppose that refers to the error that is obtained with the rest of Lagrange. According to my own calculations this is: $$R_3(α,x)=\frac{{{\gamma ^{(3)}}(\alpha )}}{{3!}} \cdot {x^3}=\frac{{c\!\cdot\!\alpha\!\cdot\!\left( {{\alpha ^2} + \frac{3}{2}{c^2}} \right)}}{{{{\left( {{c^2} - {\alpha ^2}} \right)}^{7/2}}}} \cdot {x^3},\quad 0<α<x$$ Then the question is: How can I prove that $$\color{blue}{R(α,x)<1\%, \;\,\textrm{if}\; x < 0.4 c\,?}$$",,"['derivatives', 'taylor-expansion', 'special-relativity']"
56,Proof verification - A specific function $f$ has non-zero derivative at $0$ but is not monotone in any interval around $0$,Proof verification - A specific function  has non-zero derivative at  but is not monotone in any interval around,f 0 0,"Here's a problem on a function that has non-zero derivative at the point $0,$ but is not monotone in any interval around the point $0$. I'm stating the problem and presenting my solution to it. I'd greatly appreciate if someone checks the solution and tell me if there's any gap in my arguments. Also, is there any method/trick that may enable me produce a shorter solution? Thank you. The Problem : Let $f:\mathbb{R} \to \mathbb{R}$ be given by   $f(x)= \begin{cases}        x+2x^2sin\Big(\frac{1}{x}\Big) & x\neq 0 \\       0 & x=0 \end{cases} $ To show that $f'(0)=1,$ but $f$ is not monotonic in any interval around $0$. My Solution : We see that $\lim_{x \to 0} \frac{f(x)-f(0)}{x-0}=\lim_{x \to 0;~x \neq 0} \frac{f(x)}{x}=\lim_{x \to 0;~x \neq 0} \frac{x+2x^2sin\big(\frac{1}{x}\big)}{x}=1+2\lim_{x \to 0}xsin\Big(\frac{1}{x}\Big)$ We first find out $\lim_{x \to 0}xsin\Big(\frac{1}{x}\Big)$. Let $\epsilon>0.$ Choose $\delta=\epsilon$. Then $$x \in (-\delta,\delta)\setminus\{0\} \implies \left| xsin\Big(\frac{1}{x}\Big)-0 \right| \leq |x| < \delta=\epsilon$$ Since $\epsilon>0$ is arbitrary, $\lim_{x \to 0}xsin\Big(\frac{1}{x}\Big)=0$. And hence, $\lim_{x \to 0} \frac{f(x)-f(0)}{x-0}$ exists and is equal to $1+(2 \times 0)=1$. Thus $f'(0)=1$. Now, let $\alpha>0$ be arbitrary. By Archimedean Property $2,$ $\exists N \in \mathbb{N}$ such that $\frac{1}{N}<2\pi\alpha,$ i.e. $\frac{1}{2\pi N}<\alpha$. Choose $x_1=\frac{1}{2\pi N},~x_2=\frac{1}{2\pi N+\frac{\pi}{2}},~x_3=\frac{1}{2\pi N+\frac{3\pi}{2}}$. Then, $x_1>x_2>x_3$. Now, $f(x_1)=\frac{1}{2\pi N}+2\Big(\frac{1}{2\pi N}\Big)^2sin\Big(2\pi N\Big)=\frac{1}{2\pi N}$ $f(x_2)=\frac{1}{2\pi N+\frac{\pi}{2}}+2\Big(\frac{1}{2\pi N+\frac{\pi}{2}}\Big)^2sin\Big(2\pi N+\frac{\pi}{2}\Big)=\frac{1}{2\pi N+\frac{\pi}{2}}+2\Big(\frac{1}{2\pi N+\frac{\pi}{2}}\Big)^2$ $f(x_3)=\frac{1}{2\pi N+\frac{3\pi}{2}}+2\Big(\frac{1}{2\pi N+\frac{3\pi}{2}}\Big)^2sin\Big(2\pi N+\frac{3\pi}{2}\Big)=\frac{1}{2\pi N+\frac{3\pi}{2}}-2\Big(\frac{1}{2\pi N+\frac{3\pi}{2}}\Big)^2$ Claim : $f(x_3)<f(x_1)<f(x_2)$ We see that $f(x_3)=\frac{1}{2\pi N+\frac{3\pi}{2}}-2\Big(\frac{1}{2\pi N+\frac{3\pi}{2}}\Big)^2 < \frac{1}{2\pi N+\frac{3\pi}{2}} < \frac{1}{2\pi N} = f(x_1)$ Suppose $f(x_1) \geq f(x_2)$. Then, \begin{align} f(x_1) \geq f(x_2) &\iff \frac{1}{2\pi N} \geq \frac{1}{2\pi N+\frac{\pi}{2}}+2\Big(\frac{1}{2\pi N+\frac{\pi}{2}}\Big)^2\\ &\iff \frac{1}{2\pi N} - \frac{1}{2\pi N+\frac{\pi}{2}} \geq 2\Big(\frac{1}{2\pi N+\frac{\pi}{2}}\Big)^2\\ &\iff \frac{\frac{\pi}{2}}{(2\pi N)(2\pi N+\frac{\pi}{2})} \geq 2\Big(\frac{1}{2\pi N+\frac{\pi}{2}}\Big)^2\\ &\iff \frac{\frac{\pi}{2}}{2\pi N} \geq \frac{2}{2\pi N+\frac{\pi}{2}}\\ &\iff \frac{2\pi N+\frac{\pi}{2}}{2\pi N} \geq \frac{2}{\frac{\pi}{2}}\\ &\iff 1+\frac{1}{4N} \geq \frac{4}{\pi}\\ &\iff \frac{1}{4N} \geq \frac{4}{\pi}-1=\frac{4-\pi}{\pi}\\ &\iff 4N \leq \frac{\pi}{4-\pi} < 4\\ &[\text{If not, let} \frac{\pi}{4-\pi} \geq 4 \iff \pi \geq 16-4\pi \iff 5\pi \geq 16 \iff \pi \geq 3.2, \text{ contradiction.}]\\ & \implies N<1 \end{align} And we reach a contradiction. Hence $f(x_1)<f(x_2)$. Thus the claim is true. We have $x_1,x_2,x_3 \in (0,\alpha)$ such that $x_1>x_2>x_3$ and $f(x_3)<f(x_1)<f(x_2)$. We conclude that $f$ is not monotone in $(0,\alpha),$ and hence in $(-\alpha,\alpha)$. Since $\alpha>0$ is arbitrary, $f$ is not monotone in any interval around $0$. $\blacksquare$","Here's a problem on a function that has non-zero derivative at the point $0,$ but is not monotone in any interval around the point $0$. I'm stating the problem and presenting my solution to it. I'd greatly appreciate if someone checks the solution and tell me if there's any gap in my arguments. Also, is there any method/trick that may enable me produce a shorter solution? Thank you. The Problem : Let $f:\mathbb{R} \to \mathbb{R}$ be given by   $f(x)= \begin{cases}        x+2x^2sin\Big(\frac{1}{x}\Big) & x\neq 0 \\       0 & x=0 \end{cases} $ To show that $f'(0)=1,$ but $f$ is not monotonic in any interval around $0$. My Solution : We see that $\lim_{x \to 0} \frac{f(x)-f(0)}{x-0}=\lim_{x \to 0;~x \neq 0} \frac{f(x)}{x}=\lim_{x \to 0;~x \neq 0} \frac{x+2x^2sin\big(\frac{1}{x}\big)}{x}=1+2\lim_{x \to 0}xsin\Big(\frac{1}{x}\Big)$ We first find out $\lim_{x \to 0}xsin\Big(\frac{1}{x}\Big)$. Let $\epsilon>0.$ Choose $\delta=\epsilon$. Then $$x \in (-\delta,\delta)\setminus\{0\} \implies \left| xsin\Big(\frac{1}{x}\Big)-0 \right| \leq |x| < \delta=\epsilon$$ Since $\epsilon>0$ is arbitrary, $\lim_{x \to 0}xsin\Big(\frac{1}{x}\Big)=0$. And hence, $\lim_{x \to 0} \frac{f(x)-f(0)}{x-0}$ exists and is equal to $1+(2 \times 0)=1$. Thus $f'(0)=1$. Now, let $\alpha>0$ be arbitrary. By Archimedean Property $2,$ $\exists N \in \mathbb{N}$ such that $\frac{1}{N}<2\pi\alpha,$ i.e. $\frac{1}{2\pi N}<\alpha$. Choose $x_1=\frac{1}{2\pi N},~x_2=\frac{1}{2\pi N+\frac{\pi}{2}},~x_3=\frac{1}{2\pi N+\frac{3\pi}{2}}$. Then, $x_1>x_2>x_3$. Now, $f(x_1)=\frac{1}{2\pi N}+2\Big(\frac{1}{2\pi N}\Big)^2sin\Big(2\pi N\Big)=\frac{1}{2\pi N}$ $f(x_2)=\frac{1}{2\pi N+\frac{\pi}{2}}+2\Big(\frac{1}{2\pi N+\frac{\pi}{2}}\Big)^2sin\Big(2\pi N+\frac{\pi}{2}\Big)=\frac{1}{2\pi N+\frac{\pi}{2}}+2\Big(\frac{1}{2\pi N+\frac{\pi}{2}}\Big)^2$ $f(x_3)=\frac{1}{2\pi N+\frac{3\pi}{2}}+2\Big(\frac{1}{2\pi N+\frac{3\pi}{2}}\Big)^2sin\Big(2\pi N+\frac{3\pi}{2}\Big)=\frac{1}{2\pi N+\frac{3\pi}{2}}-2\Big(\frac{1}{2\pi N+\frac{3\pi}{2}}\Big)^2$ Claim : $f(x_3)<f(x_1)<f(x_2)$ We see that $f(x_3)=\frac{1}{2\pi N+\frac{3\pi}{2}}-2\Big(\frac{1}{2\pi N+\frac{3\pi}{2}}\Big)^2 < \frac{1}{2\pi N+\frac{3\pi}{2}} < \frac{1}{2\pi N} = f(x_1)$ Suppose $f(x_1) \geq f(x_2)$. Then, \begin{align} f(x_1) \geq f(x_2) &\iff \frac{1}{2\pi N} \geq \frac{1}{2\pi N+\frac{\pi}{2}}+2\Big(\frac{1}{2\pi N+\frac{\pi}{2}}\Big)^2\\ &\iff \frac{1}{2\pi N} - \frac{1}{2\pi N+\frac{\pi}{2}} \geq 2\Big(\frac{1}{2\pi N+\frac{\pi}{2}}\Big)^2\\ &\iff \frac{\frac{\pi}{2}}{(2\pi N)(2\pi N+\frac{\pi}{2})} \geq 2\Big(\frac{1}{2\pi N+\frac{\pi}{2}}\Big)^2\\ &\iff \frac{\frac{\pi}{2}}{2\pi N} \geq \frac{2}{2\pi N+\frac{\pi}{2}}\\ &\iff \frac{2\pi N+\frac{\pi}{2}}{2\pi N} \geq \frac{2}{\frac{\pi}{2}}\\ &\iff 1+\frac{1}{4N} \geq \frac{4}{\pi}\\ &\iff \frac{1}{4N} \geq \frac{4}{\pi}-1=\frac{4-\pi}{\pi}\\ &\iff 4N \leq \frac{\pi}{4-\pi} < 4\\ &[\text{If not, let} \frac{\pi}{4-\pi} \geq 4 \iff \pi \geq 16-4\pi \iff 5\pi \geq 16 \iff \pi \geq 3.2, \text{ contradiction.}]\\ & \implies N<1 \end{align} And we reach a contradiction. Hence $f(x_1)<f(x_2)$. Thus the claim is true. We have $x_1,x_2,x_3 \in (0,\alpha)$ such that $x_1>x_2>x_3$ and $f(x_3)<f(x_1)<f(x_2)$. We conclude that $f$ is not monotone in $(0,\alpha),$ and hence in $(-\alpha,\alpha)$. Since $\alpha>0$ is arbitrary, $f$ is not monotone in any interval around $0$. $\blacksquare$",,"['real-analysis', 'derivatives']"
57,How to differentiate the ideal gas equation?,How to differentiate the ideal gas equation?,,"The equation states that $$p \cdot V=m \cdot R \cdot T$$ where R is a constant, m is mass, T is temperature, p is pressure, V is volume. My textbook says ""By differentiating the ideal gas equation we get: $$p \cdot dV + V \cdot dp=m \cdot R \cdot dT$$ Later on it differentiates this: $$p1 \cdot V1=p2 \cdot V2=const$$ into this: $$\frac{dp}{dV}=-\frac{p}{V}$$ Now I don't understand how they are getting this. We learnt differentiation in math class but it looks nothing like this (?) so I'm hoping someone can explain this a little.","The equation states that $$p \cdot V=m \cdot R \cdot T$$ where R is a constant, m is mass, T is temperature, p is pressure, V is volume. My textbook says ""By differentiating the ideal gas equation we get: $$p \cdot dV + V \cdot dp=m \cdot R \cdot dT$$ Later on it differentiates this: $$p1 \cdot V1=p2 \cdot V2=const$$ into this: $$\frac{dp}{dV}=-\frac{p}{V}$$ Now I don't understand how they are getting this. We learnt differentiation in math class but it looks nothing like this (?) so I'm hoping someone can explain this a little.",,"['integration', 'ordinary-differential-equations', 'derivatives']"
58,How does $y[x] = a x^r$ when $y'[x] = \frac{(r y[x])}{x}$ and $y[1] = a$,How does  when  and,y[x] = a x^r y'[x] = \frac{(r y[x])}{x} y[1] = a,When \begin{eqnarray*} y'[x] &=& \frac{r y[x] }{ x } \; \;\text{and} \;\; y[1] = a \\ y[x] &=& a x^r \end{eqnarray*} Use derivative formulas to explain the output. What I have so far: We know that  \begin{eqnarray*} y'[x] = r y[x] = k e^{rx} \end{eqnarray*} So we can rewrite as  \begin{eqnarray*} y[x] = \frac{k e^{rx} } { x} \end{eqnarray*} To get the value of k: \begin{eqnarray*} a = \frac{k e^{r} }{ 1} \\ k = \frac{a}{e^r} \end{eqnarray*} So now we have \begin{eqnarray*} \frac{(a/(e^r)) e^{rx} } { x} \end{eqnarray*} But when I plug this into a derivative calculator I get a solution much different than $y[x] = a x^r$. Perhaps I am approaching this wrong. What is the correct way to approach this problem using derivative formulas?,When \begin{eqnarray*} y'[x] &=& \frac{r y[x] }{ x } \; \;\text{and} \;\; y[1] = a \\ y[x] &=& a x^r \end{eqnarray*} Use derivative formulas to explain the output. What I have so far: We know that  \begin{eqnarray*} y'[x] = r y[x] = k e^{rx} \end{eqnarray*} So we can rewrite as  \begin{eqnarray*} y[x] = \frac{k e^{rx} } { x} \end{eqnarray*} To get the value of k: \begin{eqnarray*} a = \frac{k e^{r} }{ 1} \\ k = \frac{a}{e^r} \end{eqnarray*} So now we have \begin{eqnarray*} \frac{(a/(e^r)) e^{rx} } { x} \end{eqnarray*} But when I plug this into a derivative calculator I get a solution much different than $y[x] = a x^r$. Perhaps I am approaching this wrong. What is the correct way to approach this problem using derivative formulas?,,"['calculus', 'ordinary-differential-equations', 'derivatives']"
59,How to check the differentiability of the following function?,How to check the differentiability of the following function?,,"For an odd integer $k \geq 1$, let $F$ be the set of all entire functions $f$ such that $$f(x)= |x^k|$$ for all $x \in (-1,1)$. Then the cardinality of $F$ is $0$ $1$ $> 1$ but finite Infinite. I think for all integers $k \geq 3$, $f$ is entire. But the answer given is $0$.","For an odd integer $k \geq 1$, let $F$ be the set of all entire functions $f$ such that $$f(x)= |x^k|$$ for all $x \in (-1,1)$. Then the cardinality of $F$ is $0$ $1$ $> 1$ but finite Infinite. I think for all integers $k \geq 3$, $f$ is entire. But the answer given is $0$.",,"['derivatives', 'entire-functions']"
60,On constructing the definition of differentiation,On constructing the definition of differentiation,,"In the text that I'm following, the construction of the definition of differentiation is given as follows: My problem : In the second paragraph of point-2 , we see that if the condition $\lim_{x \to 0}\frac{E(x)}{x}=0$ holds, then it is easy to see that $a=f(0)$. I'm encountering a problem here. The limit in the condition has absolutely nothing to do with what happens to $f$ at the point $0$. If $f$ is continuous at $0$, then we can enforce the claim easily from the definition of continuity . But we're not given that $f$ is continuous (of course, differentiability implies continuity, but we cannot use that fact before defining differentiation, otherwise circularity creeps in). What I thought about it : Well one possibility is that I'm missing something obvious and I'll greatly appreciate if someone points it out. Otherwise, I think we need an initial condition that $E(0)=0$, which enforces $a$ to be $f(0)$. Now we can impose the previously stated limit condition to try to compute $b$. So in total, we need two conditions: \begin{align} &1.\,\, E(0)=0\\ &2. \,\, \lim_{x \to 0}\frac{E(x)}{x}=0 \end{align} Is this extra initial condition really needed? Or the limit condition alone does the job? Any help would be greatly appreciated. Thank you.","In the text that I'm following, the construction of the definition of differentiation is given as follows: My problem : In the second paragraph of point-2 , we see that if the condition $\lim_{x \to 0}\frac{E(x)}{x}=0$ holds, then it is easy to see that $a=f(0)$. I'm encountering a problem here. The limit in the condition has absolutely nothing to do with what happens to $f$ at the point $0$. If $f$ is continuous at $0$, then we can enforce the claim easily from the definition of continuity . But we're not given that $f$ is continuous (of course, differentiability implies continuity, but we cannot use that fact before defining differentiation, otherwise circularity creeps in). What I thought about it : Well one possibility is that I'm missing something obvious and I'll greatly appreciate if someone points it out. Otherwise, I think we need an initial condition that $E(0)=0$, which enforces $a$ to be $f(0)$. Now we can impose the previously stated limit condition to try to compute $b$. So in total, we need two conditions: \begin{align} &1.\,\, E(0)=0\\ &2. \,\, \lim_{x \to 0}\frac{E(x)}{x}=0 \end{align} Is this extra initial condition really needed? Or the limit condition alone does the job? Any help would be greatly appreciated. Thank you.",,['real-analysis']
61,Sequence of functions in $L^p$ and derivative,Sequence of functions in  and derivative,L^p,"if we have a sequence of functions,  $(f_n)_n \subset C^1(\mathbb{R})$ such that $f_n, f'_n \in L^p(\mathbb{R}) $ and $$f_n\rightarrow \phi_0 \, in \,L^p, $$ and $$f'_n\rightarrow \phi_1 \, in \,L^p.$$ Also, we can show that $(f_n)_n $ and $(f'_n)_n $ converge uniformly on $\mathbb{R}$. Can we conclude that $ \phi_0$ and $ \phi_1$ are in fact this uniform limits and in this case, $$ \phi_0 \in C^1(\mathbb{R})$$and $$\phi'_0=\phi_1.$$ Thank you for your help.","if we have a sequence of functions,  $(f_n)_n \subset C^1(\mathbb{R})$ such that $f_n, f'_n \in L^p(\mathbb{R}) $ and $$f_n\rightarrow \phi_0 \, in \,L^p, $$ and $$f'_n\rightarrow \phi_1 \, in \,L^p.$$ Also, we can show that $(f_n)_n $ and $(f'_n)_n $ converge uniformly on $\mathbb{R}$. Can we conclude that $ \phi_0$ and $ \phi_1$ are in fact this uniform limits and in this case, $$ \phi_0 \in C^1(\mathbb{R})$$and $$\phi'_0=\phi_1.$$ Thank you for your help.",,"['derivatives', 'lebesgue-integral']"
62,"integral, matrices, exponentiation, upper bound, time,linear system, unclear derivation","integral, matrices, exponentiation, upper bound, time,linear system, unclear derivation",,"How do I derive from $\dot{x}(t)=Ax(t)+Bu(t), x(0)=p$ that $x(t;p)=e^{At}p+\int_0^t e^{A(t-s)}Bu(s)ds$ ? Here $A$ and $B$ are matrices of suitable sizes: $A$ is $n\times n$ and $B$ is $n \times m$,both constant real matrices, and $\dot{x}$ is derivation of $x(t)$ by $t$.","How do I derive from $\dot{x}(t)=Ax(t)+Bu(t), x(0)=p$ that $x(t;p)=e^{At}p+\int_0^t e^{A(t-s)}Bu(s)ds$ ? Here $A$ and $B$ are matrices of suitable sizes: $A$ is $n\times n$ and $B$ is $n \times m$,both constant real matrices, and $\dot{x}$ is derivation of $x(t)$ by $t$.",,"['linear-algebra', 'derivatives', 'definite-integrals', 'initial-value-problems']"
63,what is the difference between derivative and differentiation?,what is the difference between derivative and differentiation?,,I want to know more about what is the difference， both in single variable and multivariable， I want to know about the general definition if there is one. I will appreciate it if someone can help me!! thx!!,I want to know more about what is the difference， both in single variable and multivariable， I want to know about the general definition if there is one. I will appreciate it if someone can help me!! thx!!,,"['derivatives', 'definition']"
64,"Given distance covered as a function of time, find acceleration.","Given distance covered as a function of time, find acceleration.",,"Since this question covers more aspects of differentiation and mathematical manipulation than kinematics, I am posting it here. My attempt : On differentiating position(x) once, we get velocity(v) and on differentiating position twice, we get acceleration(A). Therefore, $xv=at+b$ and $A x^1/3= (vx) (x-vx) $ are the two equations I got. But, I can't seem to express acceleration completely in terms of position, as given in the options below. Are the options wrong or am I missing something?","Since this question covers more aspects of differentiation and mathematical manipulation than kinematics, I am posting it here. My attempt : On differentiating position(x) once, we get velocity(v) and on differentiating position twice, we get acceleration(A). Therefore, $xv=at+b$ and $A x^1/3= (vx) (x-vx) $ are the two equations I got. But, I can't seem to express acceleration completely in terms of position, as given in the options below. Are the options wrong or am I missing something?",,['derivatives']
65,Is this question about differentiability wrong?,Is this question about differentiability wrong?,,"In my textbook, I found the following exercise: Show that the following vector field is differentiable at $(0,0,0)$:   $$G(x,y)=\left(\frac {x^3 + y^3}{\sqrt{x^2 + y^2}},\frac {1}{x^2 + y^2 + 1}\right)$$ However, I see that it is not even continuous at the origin, so it cannot be, in any way, differentiable. Is this right or am I missing something? All help will be greatly appreciated.","In my textbook, I found the following exercise: Show that the following vector field is differentiable at $(0,0,0)$:   $$G(x,y)=\left(\frac {x^3 + y^3}{\sqrt{x^2 + y^2}},\frac {1}{x^2 + y^2 + 1}\right)$$ However, I see that it is not even continuous at the origin, so it cannot be, in any way, differentiable. Is this right or am I missing something? All help will be greatly appreciated.",,"['derivatives', 'continuity', 'vector-analysis']"
66,Differentiation -,Differentiation -,,Finding derivatives of this function - $h(y)= (y^{-3}+2y)(2+3y^3)^{-1/2} $ I tried and weren't sure on how to simplify it to get the answer $\frac{8-27y^{-1} - 12y^4-6y^3}{2(2+3y^3)^{3/2}} $ This is my try - $ (y^{-3} + 2y) (\frac{-1}{2} (2+3y^3)^{-3/2} (0+9y^2)) + (2+3y^3)(-3y^{-4} + 2) $ I simplified it till - $ (2+3y^3)^{-3/2}(-9/2 (y^{-3} + 2y) + (2+3y^3)^{5/2}(-3y^{-4} +2) $ $ \frac{(-9/2 (y^{-3} + 2y) + (2+3y^3)^{5/2}(-3y^{-4} +2)}{ (2+3y^3)^{3/2}}$ I'm not too sure how to simplify this further .. thanks !,Finding derivatives of this function - $h(y)= (y^{-3}+2y)(2+3y^3)^{-1/2} $ I tried and weren't sure on how to simplify it to get the answer $\frac{8-27y^{-1} - 12y^4-6y^3}{2(2+3y^3)^{3/2}} $ This is my try - $ (y^{-3} + 2y) (\frac{-1}{2} (2+3y^3)^{-3/2} (0+9y^2)) + (2+3y^3)(-3y^{-4} + 2) $ I simplified it till - $ (2+3y^3)^{-3/2}(-9/2 (y^{-3} + 2y) + (2+3y^3)^{5/2}(-3y^{-4} +2) $ $ \frac{(-9/2 (y^{-3} + 2y) + (2+3y^3)^{5/2}(-3y^{-4} +2)}{ (2+3y^3)^{3/2}}$ I'm not too sure how to simplify this further .. thanks !,,"['calculus', 'derivatives']"
67,"Prob. 3, Chap. 5 in Baby Rudin: Any positive real number and any real function with bounded derivative together give rise to an injective function","Prob. 3, Chap. 5 in Baby Rudin: Any positive real number and any real function with bounded derivative together give rise to an injective function",,"Here is Prob. 3, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $g$ is a real function on $\mathbb{R}^1$, with bounded derivative (say $| g^\prime | \leq M$). Fix $\varepsilon > 0$, and define $f(x) = x + \varepsilon g(x)$. Prove that $f$ is one-to-one if $\varepsilon$ is small enough. (A set of admissible values of $\varepsilon$ can be determined which depends only on $M$.) My Attempt: Suppose $x, y \in \mathbb{R}$ such that $x < y$. Then $g$ is continuous on $[x, y]$ and differentiable on $(x, y)$. So, by the Mean Value Theorem, there is some point $p \in (x, y)$ such that $$ g(y) - g(x) = (y-x) g^\prime(p).$$   And so,    $$ f(y) - f(x) = y-x + \varepsilon \left( g(y) - g(x) \right) = (y-x)\left( 1 + \varepsilon g^\prime(p) \right).$$   Since $-M \leq g^\prime(t) \leq M$ for all $t \in \mathbb{R}$ and since $y-x > 0$, therefore,    $$ (y-x) \left( 1 - M \varepsilon \right) \leq f(y)-f(x) \leq  (y-x) \left( 1 + M \varepsilon \right). $$   So if we choose $\varepsilon$ so that $$0 < \varepsilon < \frac{1}{M},$$ then we note that    $$0 < (y-x) \left( 1 - M \varepsilon \right) \leq f(y)-f(x) \leq  (y-x) \left( 1 + M \varepsilon \right) $$   for any real numbers $x$ and $y$ such that $x < y$; that is, $f(x) < f(y)$ for any real numbers $x$ and $y$ such that $x < y$, which implies that $f$ is strictly increasing and thus one-to-one. Is this proof correct?","Here is Prob. 3, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $g$ is a real function on $\mathbb{R}^1$, with bounded derivative (say $| g^\prime | \leq M$). Fix $\varepsilon > 0$, and define $f(x) = x + \varepsilon g(x)$. Prove that $f$ is one-to-one if $\varepsilon$ is small enough. (A set of admissible values of $\varepsilon$ can be determined which depends only on $M$.) My Attempt: Suppose $x, y \in \mathbb{R}$ such that $x < y$. Then $g$ is continuous on $[x, y]$ and differentiable on $(x, y)$. So, by the Mean Value Theorem, there is some point $p \in (x, y)$ such that $$ g(y) - g(x) = (y-x) g^\prime(p).$$   And so,    $$ f(y) - f(x) = y-x + \varepsilon \left( g(y) - g(x) \right) = (y-x)\left( 1 + \varepsilon g^\prime(p) \right).$$   Since $-M \leq g^\prime(t) \leq M$ for all $t \in \mathbb{R}$ and since $y-x > 0$, therefore,    $$ (y-x) \left( 1 - M \varepsilon \right) \leq f(y)-f(x) \leq  (y-x) \left( 1 + M \varepsilon \right). $$   So if we choose $\varepsilon$ so that $$0 < \varepsilon < \frac{1}{M},$$ then we note that    $$0 < (y-x) \left( 1 - M \varepsilon \right) \leq f(y)-f(x) \leq  (y-x) \left( 1 + M \varepsilon \right) $$   for any real numbers $x$ and $y$ such that $x < y$; that is, $f(x) < f(y)$ for any real numbers $x$ and $y$ such that $x < y$, which implies that $f$ is strictly increasing and thus one-to-one. Is this proof correct?",,"['calculus', 'real-analysis', 'analysis', 'derivatives']"
68,Not sure how to use taylor theorem to show something is true for all real numbers.,Not sure how to use taylor theorem to show something is true for all real numbers.,,Let $f:\mathbb R \to \mathbb R$ be a function which is differentiable at every $x \in \mathbb R$. a) Assume that $f'(x) = f(x)$ for all $x \in \mathbb R$ and $f(0) = 0$. Prove that $f(x) = 0$ for all $x \in \mathbb R$. (Hint: Taylor's theorem.) b) Assume that $f'(x) = f(x)$ for all $x \in \mathbb R$. Prove that $f(x) = f(0)e^x$.,Let $f:\mathbb R \to \mathbb R$ be a function which is differentiable at every $x \in \mathbb R$. a) Assume that $f'(x) = f(x)$ for all $x \in \mathbb R$ and $f(0) = 0$. Prove that $f(x) = 0$ for all $x \in \mathbb R$. (Hint: Taylor's theorem.) b) Assume that $f'(x) = f(x)$ for all $x \in \mathbb R$. Prove that $f(x) = f(0)e^x$.,,"['analysis', 'derivatives']"
69,Proving the derivative of a certain point using the sequence definition,Proving the derivative of a certain point using the sequence definition,,"Using this derivative definition If $f$ is a function and has derivative $f'(c)$ at the point $c$ in the domain of $f$ means that if ($a_n$)$_{n=1}^{\infty}$ is any sequence converging to $c$ such that $a_n$ $\not= c$is in the domain of $f$ for all $n \in \mathbb{N},$ then: $$(\,\frac{f(x_n)-f(c)}{x_n-c}) \,_{n=1}^{\infty}$$converges to $f'(c)$ prove that $f'(c)=-6,$ where $f(x)=x^2 +2$ I feel like this should be fairly straight forward, but I'm having trouble. My attempt: Plugging in $a_n$ for $x,$ and using $f(c)=f(-3)=11,$ $(\,\frac{f(x_n)-f(c)}{x_n-c}) \,_{n=1}^{\infty}$<=> $\frac{[(a_n)^2+2]-11}{a_n-3}$<=>$\frac{(a_n)^2-9}{a_n-3}$<=>$a_n-3$ Because we are using sequences, I'm not sure if this is a correct approach, and I am not sure how to complete the proof. Do I need to choose a certain epsilon to show that this sequence converges to $f'(c)$?","Using this derivative definition If $f$ is a function and has derivative $f'(c)$ at the point $c$ in the domain of $f$ means that if ($a_n$)$_{n=1}^{\infty}$ is any sequence converging to $c$ such that $a_n$ $\not= c$is in the domain of $f$ for all $n \in \mathbb{N},$ then: $$(\,\frac{f(x_n)-f(c)}{x_n-c}) \,_{n=1}^{\infty}$$converges to $f'(c)$ prove that $f'(c)=-6,$ where $f(x)=x^2 +2$ I feel like this should be fairly straight forward, but I'm having trouble. My attempt: Plugging in $a_n$ for $x,$ and using $f(c)=f(-3)=11,$ $(\,\frac{f(x_n)-f(c)}{x_n-c}) \,_{n=1}^{\infty}$<=> $\frac{[(a_n)^2+2]-11}{a_n-3}$<=>$\frac{(a_n)^2-9}{a_n-3}$<=>$a_n-3$ Because we are using sequences, I'm not sure if this is a correct approach, and I am not sure how to complete the proof. Do I need to choose a certain epsilon to show that this sequence converges to $f'(c)$?",,"['real-analysis', 'sequences-and-series', 'derivatives', 'convergence-divergence']"
70,"By using d'Alembert's formula, substitute $P(z)$ and $Q(z)$ into the general solution to obtain an expression for $u(x, t)$","By using d'Alembert's formula, substitute  and  into the general solution to obtain an expression for","P(z) Q(z) u(x, t)","We will define a motion that satisfies the equation: $$u_{tt} = c^2u_{xx}\qquad x ∈ (0, 1),\: t > 0$$We have the displacement of a string being $u(x, t)$, at position $x$ and time $t$, which is stretched between two fixed points at $x = 0$ and $x = 1$. where $c$ is a real positive constant, and has boundary conditions $$u(0, t) = u(1, t) = 0 \qquad t > 0$$ The string has an initial displacement $u(x, 0) = f(x), x ∈ (0, 1)$ and is initially at rest. Starting with the general solution to the wave equation $$u(x, t) = P(x − ct) + Q(x + ct)$$ for arbitrary functions $P(z)$ and $Q(z)$, By obtaining expressions for $P(z)$ and $Q(z)$ in terms of $f(z)$ for all $z$, substitute these into the general solution to obtain an expression for $u(x, t)$ in the form $$u(x, t) = \frac{1}{2}(\overset{ˆ}{f}(x − ct) + \overset{ˆ}{f}(x + ct))$$ where $\overset{ˆ}{f}(z)$ is a suitable periodic extension of the initial data which should be defined. Im really not sure how to approach this question even though ive being trying for a couple of hours now to solve it so any help will be apprecated.","We will define a motion that satisfies the equation: $$u_{tt} = c^2u_{xx}\qquad x ∈ (0, 1),\: t > 0$$We have the displacement of a string being $u(x, t)$, at position $x$ and time $t$, which is stretched between two fixed points at $x = 0$ and $x = 1$. where $c$ is a real positive constant, and has boundary conditions $$u(0, t) = u(1, t) = 0 \qquad t > 0$$ The string has an initial displacement $u(x, 0) = f(x), x ∈ (0, 1)$ and is initially at rest. Starting with the general solution to the wave equation $$u(x, t) = P(x − ct) + Q(x + ct)$$ for arbitrary functions $P(z)$ and $Q(z)$, By obtaining expressions for $P(z)$ and $Q(z)$ in terms of $f(z)$ for all $z$, substitute these into the general solution to obtain an expression for $u(x, t)$ in the form $$u(x, t) = \frac{1}{2}(\overset{ˆ}{f}(x − ct) + \overset{ˆ}{f}(x + ct))$$ where $\overset{ˆ}{f}(z)$ is a suitable periodic extension of the initial data which should be defined. Im really not sure how to approach this question even though ive being trying for a couple of hours now to solve it so any help will be apprecated.",,"['calculus', 'ordinary-differential-equations', 'derivatives']"
71,Solution of a system of second-order differential equations,Solution of a system of second-order differential equations,,"Let $\alpha$ be a real number and let $u,v$ be constant vectors. Find the conditions on $\alpha$ , $u$ and $v$ such that $$x(t)=\cos(\alpha t)u +\sin(\alpha t)v$$ is a solution of the system $\ddot x=Ax$ . This is my attempt $\ddot x= -\alpha^2\cos(\alpha t)u-\alpha^2\sin(\alpha t)v$ Now, $$\ddot x-Ax=-\alpha^2\cos(\alpha t)u-\alpha^2\sin(\alpha t)v-A\cos(\alpha t)u-A\sin(\alpha t)v=0$$ $$\implies\ -(\alpha^2I+A)\big(\cos(\alpha t)u+\sin(\alpha t)v\big)=0$$ This implies $\cos(\alpha t)u+\sin(\alpha t)v=0\implies\ \frac{-u}{v}=\tan(\alpha t)$ , which is impossible since $u$ and $v$ are constant vectors. OR $\alpha^2I+A=0 \implies \alpha^2I=-A$ . Is my proof until here correct? And how can I complete it?","Let be a real number and let be constant vectors. Find the conditions on , and such that is a solution of the system . This is my attempt Now, This implies , which is impossible since and are constant vectors. OR . Is my proof until here correct? And how can I complete it?","\alpha u,v \alpha u v x(t)=\cos(\alpha t)u +\sin(\alpha t)v \ddot x=Ax \ddot x= -\alpha^2\cos(\alpha t)u-\alpha^2\sin(\alpha t)v \ddot x-Ax=-\alpha^2\cos(\alpha t)u-\alpha^2\sin(\alpha t)v-A\cos(\alpha t)u-A\sin(\alpha t)v=0 \implies\ -(\alpha^2I+A)\big(\cos(\alpha t)u+\sin(\alpha t)v\big)=0 \cos(\alpha t)u+\sin(\alpha t)v=0\implies\ \frac{-u}{v}=\tan(\alpha t) u v \alpha^2I+A=0 \implies \alpha^2I=-A",['matrices']
72,Theorem 5.8 in Baby Rudin: Does the function have to be defined at the endpoints as well?,Theorem 5.8 in Baby Rudin: Does the function have to be defined at the endpoints as well?,,"Here is Theorem 5.8 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $f$ be defined on $[a, b]$; if $f$ has a local maximum at a point $x \in (a, b)$, and if $f^\prime(x)$ exists, then $f^\prime(x)=0$. The analogous statement for local minima is of course also true. And, here is Rudin's proof: Choose $\delta$ in accordance with Definition 5.7, so that $$ a < x-\delta < x < x+\delta < b.$$   If $x-\delta < t < x$, then $$\frac{ f(t)-f(x)}{t-x} \geq 0.$$   Letting $t \to x$, we see that $f^\prime(x) \geq 0$. If $x < t < x+\delta$, then $$ \frac{ f(t) - f(x) }{ t-x} \leq 0,$$   which shows that $f^\prime(x) \leq 0$. Hence $f^\prime(x) = 0$. And, here is Definition 5.7 in Baby Rudin: Let $f$ be a real function defined on a metric space $X$. We say that $f$ has a local maximum at a point $p \in X$ if there exists $\delta > 0$ such that $f(q) \leq f(p)$ for all $q \in X$ with $d(p, q) < \delta$. Local minima are defined likewise. Now my question is, in Theorem 5.8, do we have to assume that the function $f$ is defined at the endpoints $a$ and $b$ as well? Or, is it sufficient for $f$ to be defined only in the segment $(a, b)$?","Here is Theorem 5.8 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Let $f$ be defined on $[a, b]$; if $f$ has a local maximum at a point $x \in (a, b)$, and if $f^\prime(x)$ exists, then $f^\prime(x)=0$. The analogous statement for local minima is of course also true. And, here is Rudin's proof: Choose $\delta$ in accordance with Definition 5.7, so that $$ a < x-\delta < x < x+\delta < b.$$   If $x-\delta < t < x$, then $$\frac{ f(t)-f(x)}{t-x} \geq 0.$$   Letting $t \to x$, we see that $f^\prime(x) \geq 0$. If $x < t < x+\delta$, then $$ \frac{ f(t) - f(x) }{ t-x} \leq 0,$$   which shows that $f^\prime(x) \leq 0$. Hence $f^\prime(x) = 0$. And, here is Definition 5.7 in Baby Rudin: Let $f$ be a real function defined on a metric space $X$. We say that $f$ has a local maximum at a point $p \in X$ if there exists $\delta > 0$ such that $f(q) \leq f(p)$ for all $q \in X$ with $d(p, q) < \delta$. Local minima are defined likewise. Now my question is, in Theorem 5.8, do we have to assume that the function $f$ is defined at the endpoints $a$ and $b$ as well? Or, is it sufficient for $f$ to be defined only in the segment $(a, b)$?",,"['calculus', 'real-analysis', 'analysis', 'derivatives', 'optimization']"
73,Nowhere differentiable continuous functions and local extrema,Nowhere differentiable continuous functions and local extrema,,"Consider any continuous function which is nowhere differentiable (such as Weierstarss function). If this function would be monotone on some interval $(a,b)$ then there is a result which states that in this interval this function would be almost everywhere differentiable which is contradiction. Therefore any such function cannot be differentiable in any interval. However this not settles the issue of having local extrema. Is it possible to construct everywhere continuous, nowhere differentiable function $f:[-1,1] \to \mathbb{R}$ with the following property: $f$ has local minimum at $x_0=0$, $f$ has local maximum at $x_1=\frac12$, $f(0)>f(\frac12)$ and $x_0,x_1$ are only extremas of $f$","Consider any continuous function which is nowhere differentiable (such as Weierstarss function). If this function would be monotone on some interval $(a,b)$ then there is a result which states that in this interval this function would be almost everywhere differentiable which is contradiction. Therefore any such function cannot be differentiable in any interval. However this not settles the issue of having local extrema. Is it possible to construct everywhere continuous, nowhere differentiable function $f:[-1,1] \to \mathbb{R}$ with the following property: $f$ has local minimum at $x_0=0$, $f$ has local maximum at $x_1=\frac12$, $f(0)>f(\frac12)$ and $x_0,x_1$ are only extremas of $f$",,"['real-analysis', 'analysis', 'derivatives', 'continuity']"
74,Can the Laurent series be found in region where there is singularity?,Can the Laurent series be found in region where there is singularity?,,"In class my teacher showed that we can use the expansion of $\frac{1}{1-z}$ to find an expansion for $\frac{1}{(z+1)(z+2)}$ in the regions $|z|<1$, $1<|z|<2$ and $|z|>2$ My question is can this method be used to find the expansion in the regions $|z|>1$ and $|z|<2$, if not then what method can be used?","In class my teacher showed that we can use the expansion of $\frac{1}{1-z}$ to find an expansion for $\frac{1}{(z+1)(z+2)}$ in the regions $|z|<1$, $1<|z|<2$ and $|z|>2$ My question is can this method be used to find the expansion in the regions $|z|>1$ and $|z|<2$, if not then what method can be used?",,"['sequences-and-series', 'ordinary-differential-equations', 'derivatives', 'laurent-series']"
75,Find points of tangency for a given function.,Find points of tangency for a given function.,,"I have spent 3 hours or so trying to tackle this problem but the answers I get arent the ones written on the page , I do not want to assume that the asnwers on the page are incorrect so here I go , The Problem The graph : $y=\frac{5x}{4} + \frac{1}{x}$ (In the first quarter) , has a tangent in point A , and a tangent in point B (both tangent to the function graph) that are perpendicular to each other . (90 deagrees). It is given that the rate of x of point A is bigger by times 3 than point B (Xa = $t$ while Xb = $\frac{1}{3}t$). Find the credentials (x , y) of point A , and point B if it is given that the X of point A , is a natural number (bigger than zero , and not a fraction). (Didnt finish 1. so didnt get to it) What I did : I wrote that the x of point a is t , then I found the derivative of  $f{(t)}$ which is equal to the angle of tangence (or in other words $m$ of $t$) , derivative : $f'{(t)} = 1.25 + -\frac{1}{t^2}$ after that I found the derivative of point B $(\frac{1}{3}t , f{(\frac{1}{3}t)})$ Which was $f'(\frac{1}{3}t) =  \frac{5}{12} + 3 == 3.416$ I was taught that $m1 * m2 = -1$ . so I took the two m's and followed through : Ma = $1.25 + -\frac{1}{t^2}$ , Mb = $3.416$ . Then Ma * Mb == -1 (Right ?) $(1.25 + -\frac{1}{t^2}) * 3.416 =  -1$ Which led me to $5.27t^2 = 3.416$ led me to this --> $t = 0.7726$ The answers in the book show : A(2,3) and B(0.666 , 2.666) , I tried finding my mistake  , but it seems that this also leads me to this. what did I miss here? Would love some help and insight , The problem was translated , took me a while , hope you understand it (my English isn't perfect to say the least).","I have spent 3 hours or so trying to tackle this problem but the answers I get arent the ones written on the page , I do not want to assume that the asnwers on the page are incorrect so here I go , The Problem The graph : $y=\frac{5x}{4} + \frac{1}{x}$ (In the first quarter) , has a tangent in point A , and a tangent in point B (both tangent to the function graph) that are perpendicular to each other . (90 deagrees). It is given that the rate of x of point A is bigger by times 3 than point B (Xa = $t$ while Xb = $\frac{1}{3}t$). Find the credentials (x , y) of point A , and point B if it is given that the X of point A , is a natural number (bigger than zero , and not a fraction). (Didnt finish 1. so didnt get to it) What I did : I wrote that the x of point a is t , then I found the derivative of  $f{(t)}$ which is equal to the angle of tangence (or in other words $m$ of $t$) , derivative : $f'{(t)} = 1.25 + -\frac{1}{t^2}$ after that I found the derivative of point B $(\frac{1}{3}t , f{(\frac{1}{3}t)})$ Which was $f'(\frac{1}{3}t) =  \frac{5}{12} + 3 == 3.416$ I was taught that $m1 * m2 = -1$ . so I took the two m's and followed through : Ma = $1.25 + -\frac{1}{t^2}$ , Mb = $3.416$ . Then Ma * Mb == -1 (Right ?) $(1.25 + -\frac{1}{t^2}) * 3.416 =  -1$ Which led me to $5.27t^2 = 3.416$ led me to this --> $t = 0.7726$ The answers in the book show : A(2,3) and B(0.666 , 2.666) , I tried finding my mistake  , but it seems that this also leads me to this. what did I miss here? Would love some help and insight , The problem was translated , took me a while , hope you understand it (my English isn't perfect to say the least).",,"['linear-algebra', 'derivatives', 'tangent-line']"
76,Linear independence of derivatives at a point implies trivial intersection of image,Linear independence of derivatives at a point implies trivial intersection of image,,"Let $f$ and $g$ be differentiable paths from $\mathbb{R}$ to $\mathbb{R^{n}}$ such that $f(a)=g(b)=p$ for some pair $a,b \in \mathbb{R}$. If $f'(a)$ and $g'(b)$ are linear independent then $\exists k>0$ such that $f(B(a,k))\cap g(B(b,k))=${$p$}. Here $B(x,k)$ stands for the open ball with radidus $k$ and center at $x$. It's easy to see that the images of $(a,a+k)$ and $(b,b+k)$ shouldn't have a point in commom because the paths are ""growing"" to different directions so the inclination may guarantee what I need,still I don't know how to go further.","Let $f$ and $g$ be differentiable paths from $\mathbb{R}$ to $\mathbb{R^{n}}$ such that $f(a)=g(b)=p$ for some pair $a,b \in \mathbb{R}$. If $f'(a)$ and $g'(b)$ are linear independent then $\exists k>0$ such that $f(B(a,k))\cap g(B(b,k))=${$p$}. Here $B(x,k)$ stands for the open ball with radidus $k$ and center at $x$. It's easy to see that the images of $(a,a+k)$ and $(b,b+k)$ shouldn't have a point in commom because the paths are ""growing"" to different directions so the inclination may guarantee what I need,still I don't know how to go further.",,"['real-analysis', 'derivatives']"
77,Differentiating function times indicator function,Differentiating function times indicator function,,"I have $$C=\int\limits_{\lambda=0}^{\lambda=2\pi}\int\limits_{\mu=0}^{\mu=1}hq\chi_{q\geq Q}\mathrm{d}\lambda \mathrm{d}\mu$$ where $Q$ is some constant, and $h$ and $q$ are functions of $\lambda$ and $\mu$. I would like to evaluate $\partial_q C$. Am I correct in saying that it is $$\int\limits_{\lambda=0}^{\lambda=2\pi}\int\limits_{\mu=0}^{\mu=1}h\chi_{q\geq Q}\mathrm{d}\lambda \mathrm{d}\mu - \int\limits_{\lambda=0}^{\lambda=2\pi}\int\limits_{\mu=0}^{\mu=1}hq\delta_{q- Q}\mathrm{d}\lambda \mathrm{d}\mu,$$ where $\delta$ is Dirac's delta?","I have $$C=\int\limits_{\lambda=0}^{\lambda=2\pi}\int\limits_{\mu=0}^{\mu=1}hq\chi_{q\geq Q}\mathrm{d}\lambda \mathrm{d}\mu$$ where $Q$ is some constant, and $h$ and $q$ are functions of $\lambda$ and $\mu$. I would like to evaluate $\partial_q C$. Am I correct in saying that it is $$\int\limits_{\lambda=0}^{\lambda=2\pi}\int\limits_{\mu=0}^{\mu=1}h\chi_{q\geq Q}\mathrm{d}\lambda \mathrm{d}\mu - \int\limits_{\lambda=0}^{\lambda=2\pi}\int\limits_{\mu=0}^{\mu=1}hq\delta_{q- Q}\mathrm{d}\lambda \mathrm{d}\mu,$$ where $\delta$ is Dirac's delta?",,"['derivatives', 'distribution-theory', 'dirac-delta']"
78,Prove $\det[D f (x)] \neq 0$,Prove,\det[D f (x)] \neq 0,"Let $U, V ⊆ \mathbb R^n$ be open sets and $f : U → V$ a differentiable bijection with a differentiable inverse. Show that $\det[D f (x)] \neq 0$ for any $x ∈ U$. This shows that the converse of the Inverse Function Theorem holds, but how should I prove it? Thank you.","Let $U, V ⊆ \mathbb R^n$ be open sets and $f : U → V$ a differentiable bijection with a differentiable inverse. Show that $\det[D f (x)] \neq 0$ for any $x ∈ U$. This shows that the converse of the Inverse Function Theorem holds, but how should I prove it? Thank you.",,"['real-analysis', 'derivatives']"
79,What are the Second Order Cauchy-Riemann equations in terms of polar co-ordinates?,What are the Second Order Cauchy-Riemann equations in terms of polar co-ordinates?,,"For a complex function $f(x + i y) = u(x,y) + i v(x,y)$, in Cartesian co-ordinates the Cauchy-Riemann equations are $u_x = v_y$ and $u_y = -v_x$, which can be used to find the second order Cauchy-Riemann equations: $u_{xx} = -u_{yy}$ and $v_{xx} = -v_{yy}$. In polar co-ordinates, the Cauchy-Riemann equations are $u_r = \frac{1}{r}v_{\theta}$ and $v_r = -\frac{1}{r}u_{\theta}$. In an attempt to find the second order Cauchy-Riemann equations for polar co-ordinates, I came out with two separate answers depending on whether I differentiated the polar Cauchy-Riemann equations with respect to $r$ or  $\theta$. With respect to $\theta$, I attained: $u_{rr} = -\frac{1}{r^2}u_{\theta \theta}$ and $v_{rr} = -\frac{1}{r^2}v_{\theta \theta}$, but when I differentiate with respect to $r$, I get $u_{rr} = -\frac{1}{r^2}v_{\theta} - \frac{1}{r^2}u_{\theta \theta}$ and $v_{rr} = \frac{1}{r^2}u_{\theta} -\frac{1}{r^2}v_{\theta \theta}$, due to using the product rule. Are either of these versions correct, and why does one, if not both, of them not give the correct answer?","For a complex function $f(x + i y) = u(x,y) + i v(x,y)$, in Cartesian co-ordinates the Cauchy-Riemann equations are $u_x = v_y$ and $u_y = -v_x$, which can be used to find the second order Cauchy-Riemann equations: $u_{xx} = -u_{yy}$ and $v_{xx} = -v_{yy}$. In polar co-ordinates, the Cauchy-Riemann equations are $u_r = \frac{1}{r}v_{\theta}$ and $v_r = -\frac{1}{r}u_{\theta}$. In an attempt to find the second order Cauchy-Riemann equations for polar co-ordinates, I came out with two separate answers depending on whether I differentiated the polar Cauchy-Riemann equations with respect to $r$ or  $\theta$. With respect to $\theta$, I attained: $u_{rr} = -\frac{1}{r^2}u_{\theta \theta}$ and $v_{rr} = -\frac{1}{r^2}v_{\theta \theta}$, but when I differentiate with respect to $r$, I get $u_{rr} = -\frac{1}{r^2}v_{\theta} - \frac{1}{r^2}u_{\theta \theta}$ and $v_{rr} = \frac{1}{r^2}u_{\theta} -\frac{1}{r^2}v_{\theta \theta}$, due to using the product rule. Are either of these versions correct, and why does one, if not both, of them not give the correct answer?",,"['complex-analysis', 'derivatives', 'complex-numbers', 'polar-coordinates']"
80,Implicit Differentiation Quesrion,Implicit Differentiation Quesrion,,$$x^y = y^x$$ is the equation My solution is :$$\frac { dy }{ dx } =\left( \ln { y-\frac { y }{ x }  }  \right) /\left( \ln { x-\frac { x }{ y }  }  \right) $$ Just wondering if this is correct!,$$x^y = y^x$$ is the equation My solution is :$$\frac { dy }{ dx } =\left( \ln { y-\frac { y }{ x }  }  \right) /\left( \ln { x-\frac { x }{ y }  }  \right) $$ Just wondering if this is correct!,,"['calculus', 'derivatives']"
81,Understanding the fundamental theorem of calculus via squeeze theorem,Understanding the fundamental theorem of calculus via squeeze theorem,,"I was just taught ""integration is the opposite of differentation, this will give the area"" While I can integrate, I am not sure why it works and the ""anti derivate"" magically is the area. I really need some help getting this idea in my head. I am using https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus for reference. If we want A(x), I am not sure how adding an extra red section helps. I think I understand the red area is defined as approximateley A(x)-A(x-h), which equals f(x)h. Then I have read, this is between hf(x)< RED AREA < f(x+h). I then see we divide by h, and are left with the defintion of differentation when h>>0. However I have 2 questions' 1) Surely we are not concerned with the red area, we want the blue area 2) When we take the limit as h>>0 this leaves an infitesimal slice (I understand that integration is the summation of an infinate amount of infiniately small sliced) but the concept of adding +h to the area (thus creating a red area) only to remove it doesn't seem to have any effect on the blue area. Thanks a lot for your help. Hopefully its something easy I missed, and I can get there!","I was just taught ""integration is the opposite of differentation, this will give the area"" While I can integrate, I am not sure why it works and the ""anti derivate"" magically is the area. I really need some help getting this idea in my head. I am using https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus for reference. If we want A(x), I am not sure how adding an extra red section helps. I think I understand the red area is defined as approximateley A(x)-A(x-h), which equals f(x)h. Then I have read, this is between hf(x)< RED AREA < f(x+h). I then see we divide by h, and are left with the defintion of differentation when h>>0. However I have 2 questions' 1) Surely we are not concerned with the red area, we want the blue area 2) When we take the limit as h>>0 this leaves an infitesimal slice (I understand that integration is the summation of an infinate amount of infiniately small sliced) but the concept of adding +h to the area (thus creating a red area) only to remove it doesn't seem to have any effect on the blue area. Thanks a lot for your help. Hopefully its something easy I missed, and I can get there!",,"['calculus', 'integration', 'derivatives']"
82,Local inversion theorem problem,Local inversion theorem problem,,Let $f:$ $M_2(\mathbb{R})\rightarrow M_2(\mathbb{R})$ defined by $f(A)=A^2$. In the neighbourhoods of what diagonal matrix is f a local diffeomorphism?,Let $f:$ $M_2(\mathbb{R})\rightarrow M_2(\mathbb{R})$ defined by $f(A)=A^2$. In the neighbourhoods of what diagonal matrix is f a local diffeomorphism?,,"['calculus', 'derivatives']"
83,Verify that f(z) = z(e^x) is analytical,Verify that f(z) = z(e^x) is analytical,,"iI have to show that the derivative of $f (z) = z (e ^ z)$ is $f '(z) = (e ^ z) + z (e ^ z)$ Then the solution would be something like: $f(z) = (x+yi)[(e^x) \cos(y) + i(e^x) \sin(y)]$ $f(z) = x(e^x)\cos(y) + ix(e^x)\sin(y) + iy(e^x)\cos(y) - y(e^x)\sin(y)$ We organize $f(z) = [x(e^x)\cos(y) - y(e^x)\sin(y)] + i [x(e^x)\sin(y) + y(e^x)cos(y)]$ We derive $du/dx = (e^x)\cos(y) + x(e^x)\cos(y) - y(e^x)\sin(y)$ $dv/dy = (e^x)\cos(y) + x(e^x)\cos(y) - y(e^x)\sin(y)$ The first rule is met $dv/dx = (e^x)\sin(y) + x(e^x)\sin(y) + y(e^x)\cos(y)$ $dv/dy = -(e^x)\sin(y) - x(e^x)\sin(y) - y(e^x)\cos(y)$ The second rule is met. Now we have to check the answers $f'(z) = du/dx + dv/dx$ $f'(z) = [(e^x)\cos(y) + x(e^x)\cos(y) - y(e^x)\sin(y)] + [(e^x)\sin(y) + x(e^x)\sin(y) + y(e^x)\cos(y)]$ $f'(z) = (e^x)[\cos(y) + x\cos(y) - y\sin(y)] + (e^x)[(\sin(y) + x \sin(y) + y \cos(y)]$ From this point I really do not know what I should do to demonstrate the derivative... The answer I have to get is ""$f'(z) = (e^z) + z(e^z)$"" Thanks for your help","iI have to show that the derivative of $f (z) = z (e ^ z)$ is $f '(z) = (e ^ z) + z (e ^ z)$ Then the solution would be something like: $f(z) = (x+yi)[(e^x) \cos(y) + i(e^x) \sin(y)]$ $f(z) = x(e^x)\cos(y) + ix(e^x)\sin(y) + iy(e^x)\cos(y) - y(e^x)\sin(y)$ We organize $f(z) = [x(e^x)\cos(y) - y(e^x)\sin(y)] + i [x(e^x)\sin(y) + y(e^x)cos(y)]$ We derive $du/dx = (e^x)\cos(y) + x(e^x)\cos(y) - y(e^x)\sin(y)$ $dv/dy = (e^x)\cos(y) + x(e^x)\cos(y) - y(e^x)\sin(y)$ The first rule is met $dv/dx = (e^x)\sin(y) + x(e^x)\sin(y) + y(e^x)\cos(y)$ $dv/dy = -(e^x)\sin(y) - x(e^x)\sin(y) - y(e^x)\cos(y)$ The second rule is met. Now we have to check the answers $f'(z) = du/dx + dv/dx$ $f'(z) = [(e^x)\cos(y) + x(e^x)\cos(y) - y(e^x)\sin(y)] + [(e^x)\sin(y) + x(e^x)\sin(y) + y(e^x)\cos(y)]$ $f'(z) = (e^x)[\cos(y) + x\cos(y) - y\sin(y)] + (e^x)[(\sin(y) + x \sin(y) + y \cos(y)]$ From this point I really do not know what I should do to demonstrate the derivative... The answer I have to get is ""$f'(z) = (e^z) + z(e^z)$"" Thanks for your help",,"['derivatives', 'riemann-surfaces']"
84,"Given a continuous function $f : I → R$, show that if $f$ is diﬀerentiable at $c$ and $c$ is a local extremum, then $f'(c) = 0$.","Given a continuous function , show that if  is diﬀerentiable at  and  is a local extremum, then .",f : I → R f c c f'(c) = 0,"Let $f : I → R$ be a continuous function on a open interval which has a local extremum at some $c\in I$. If $f$ is diﬀerentiable at $c$ then $f'(c) = 0$. Is the following proof valid? We study the diﬀerence quotient of $f$ at $x_0$: $$\frac {f(x)−f(c)}{x−c}$$ Since $f$ is diﬀerentiable, we know that its limit exists for $x → c$. Since $f$ is continuous and has a local extremum at $c$ by assumption, we get: $$f(x)−f(c) = 0 \;(\forall x \in I)$$ Thus, by the quotient rule for limits, we ﬁnd: $$f '(c) = \lim_{ x\to c} \,\frac {f(x)−f(c)}{x−c} = \lim_{ x \to c} (0) = 0$$","Let $f : I → R$ be a continuous function on a open interval which has a local extremum at some $c\in I$. If $f$ is diﬀerentiable at $c$ then $f'(c) = 0$. Is the following proof valid? We study the diﬀerence quotient of $f$ at $x_0$: $$\frac {f(x)−f(c)}{x−c}$$ Since $f$ is diﬀerentiable, we know that its limit exists for $x → c$. Since $f$ is continuous and has a local extremum at $c$ by assumption, we get: $$f(x)−f(c) = 0 \;(\forall x \in I)$$ Thus, by the quotient rule for limits, we ﬁnd: $$f '(c) = \lim_{ x\to c} \,\frac {f(x)−f(c)}{x−c} = \lim_{ x \to c} (0) = 0$$",,"['real-analysis', 'derivatives', 'proof-verification', 'alternative-proof']"
85,Critical value of a piecewise function,Critical value of a piecewise function,,"Consider the following function $f(x)$. $$f(x)= \left\{ \begin{array}{ll} x^2 & , x≤1 \\ |x-2| & , x>1 \end{array}\right.$$ to find the critical value I did the following steps: Redefine the function without absolute value $$f(x)= \left\{ \begin{array}{ll} x^2 & , x≤1 \\ x-2 & , x>2 \\ -x+2 & , 2>x>1 \end{array}\right.$$ Take the derivative of $f(x)$ $$f'(x)= \left\{ \begin{array}{ll} 2x & , x≤1 \\ 1 & , x>2 \\ -1 & , 2>x>1 \end{array}\right.$$ Find where $f'(x)=0$ or is $undefined$ $f'(x)=0$ when $x=0$, therefore​ $x=0$ is a critical value. $f'(2)=undefined$ , therefore $x=2$ is also a critical value. $f'(1)=undefined$ , since $\lim\limits_{x\to 1^-} $ doesn't equal to $\lim\limits_{x\to 1^+} $ , $x=1$ is a critical value. is this reasoning mathematically correct for proving? If there's anything incorrect let me know,and if you have a better approach for proving please provide your answers​, specially for proving $2$ and $1$ as critical values.","Consider the following function $f(x)$. $$f(x)= \left\{ \begin{array}{ll} x^2 & , x≤1 \\ |x-2| & , x>1 \end{array}\right.$$ to find the critical value I did the following steps: Redefine the function without absolute value $$f(x)= \left\{ \begin{array}{ll} x^2 & , x≤1 \\ x-2 & , x>2 \\ -x+2 & , 2>x>1 \end{array}\right.$$ Take the derivative of $f(x)$ $$f'(x)= \left\{ \begin{array}{ll} 2x & , x≤1 \\ 1 & , x>2 \\ -1 & , 2>x>1 \end{array}\right.$$ Find where $f'(x)=0$ or is $undefined$ $f'(x)=0$ when $x=0$, therefore​ $x=0$ is a critical value. $f'(2)=undefined$ , therefore $x=2$ is also a critical value. $f'(1)=undefined$ , since $\lim\limits_{x\to 1^-} $ doesn't equal to $\lim\limits_{x\to 1^+} $ , $x=1$ is a critical value. is this reasoning mathematically correct for proving? If there's anything incorrect let me know,and if you have a better approach for proving please provide your answers​, specially for proving $2$ and $1$ as critical values.",,"['calculus', 'derivatives']"
86,Bernstein polynomial derivative,Bernstein polynomial derivative,,"I am having trouble visualizing where the negative sign comes from in the final answer when differentiating the Bernstein polynomial : $$\frac{\mathrm d}{\mathrm du}\left[B_{i,n}(u)\right] = \frac{\mathrm d}{\mathrm du}\left[{\frac{n!}{i!(n-i)!}u^i(1-u)^{n-i}}\right]$$ This simplifies to: $$n\left[\frac{(n-1)!}{(i-1)!(n-i)!}u^{i-1}(1-u)^{n-i} + \frac{(n-1)!}{i!(n-i-1)!}u^i(1-u)^{n-i-1}\right]$$ $$\implies B_{i,n}'(u)= n[B_{i-1,n-1}(u)-B_{i,n-1}(u)]$$ From the definition of a Bernstein polynomial, where does the negative sign come from behind the second term?","I am having trouble visualizing where the negative sign comes from in the final answer when differentiating the Bernstein polynomial : $$\frac{\mathrm d}{\mathrm du}\left[B_{i,n}(u)\right] = \frac{\mathrm d}{\mathrm du}\left[{\frac{n!}{i!(n-i)!}u^i(1-u)^{n-i}}\right]$$ This simplifies to: $$n\left[\frac{(n-1)!}{(i-1)!(n-i)!}u^{i-1}(1-u)^{n-i} + \frac{(n-1)!}{i!(n-i-1)!}u^i(1-u)^{n-i-1}\right]$$ $$\implies B_{i,n}'(u)= n[B_{i-1,n-1}(u)-B_{i,n-1}(u)]$$ From the definition of a Bernstein polynomial, where does the negative sign come from behind the second term?",,['derivatives']
87,Derivative of double integral with direct and inverse functions,Derivative of double integral with direct and inverse functions,,"I found a couple of similar question, but I am struggling applying their logic to my example. Derivative of double integral with respect to upper limits Differentiation under the double integral sign Derivative of double integral with respect to upper limits Derivative of double integral using Leibniz integral rule I have the following differentiation of the double integral: $$\frac{d}{dc}\int_{z^{-1}(c)}^{1}\int_{z(x)}^{z^{-1}(c)} (v-y)f(x)f(y)dydx$$ where $f(x)$ and $f(y)$ and probability density functions. Is it possible to apply Leibniz rule right away here to somehow simplify it? Substituting the internal integral for anti-derivatives as in the examples does not seem possible because that's a product of functions. The best I can do is to ""open up"" the internal integral by integrating by parts and then apply the rule: $$\int_{z(x)}^{z^{-1}(c)} (v-y)f(y)dy=(v-z^{-1}(c))F(z^{-1}(c))-(v-z(x))F(z(x))+\int_{z(x)}^{z^{-1}(c)}F(y)dy$$ Then my expression becomes something like: $$\frac{d}{dc}\int_{z^{-1}(c)}^{1}(v-z^{-1}(c))F(z^{-1}(c))f(x)dx-\frac{d}{dc}\int_{z^{-1}(c)}^{1}(v-z(x))F(z(x))f(x)dx+$$ $$\frac{d}{dc}\int_{z^{-1}(c)}^{1}\int_{z(x)}^{z^{-1}(c)}F(y)f(x)dydx$$ where the first term seems to be feasible to take, and even the second one, but the third one - back to square one. Is there any way around it?","I found a couple of similar question, but I am struggling applying their logic to my example. Derivative of double integral with respect to upper limits Differentiation under the double integral sign Derivative of double integral with respect to upper limits Derivative of double integral using Leibniz integral rule I have the following differentiation of the double integral: $$\frac{d}{dc}\int_{z^{-1}(c)}^{1}\int_{z(x)}^{z^{-1}(c)} (v-y)f(x)f(y)dydx$$ where $f(x)$ and $f(y)$ and probability density functions. Is it possible to apply Leibniz rule right away here to somehow simplify it? Substituting the internal integral for anti-derivatives as in the examples does not seem possible because that's a product of functions. The best I can do is to ""open up"" the internal integral by integrating by parts and then apply the rule: $$\int_{z(x)}^{z^{-1}(c)} (v-y)f(y)dy=(v-z^{-1}(c))F(z^{-1}(c))-(v-z(x))F(z(x))+\int_{z(x)}^{z^{-1}(c)}F(y)dy$$ Then my expression becomes something like: $$\frac{d}{dc}\int_{z^{-1}(c)}^{1}(v-z^{-1}(c))F(z^{-1}(c))f(x)dx-\frac{d}{dc}\int_{z^{-1}(c)}^{1}(v-z(x))F(z(x))f(x)dx+$$ $$\frac{d}{dc}\int_{z^{-1}(c)}^{1}\int_{z(x)}^{z^{-1}(c)}F(y)f(x)dydx$$ where the first term seems to be feasible to take, and even the second one, but the third one - back to square one. Is there any way around it?",,"['derivatives', 'definite-integrals', 'inverse-function']"
88,If the Gateaux derivative is not linear does this mean the Frechet derivative doesn't exist?,If the Gateaux derivative is not linear does this mean the Frechet derivative doesn't exist?,,It seems to be true as if the frechet exists then it is the same as the gateaux which would then be nonlinear (contradiction) but this seems useful yet I can't really find it anywhere and the lecturer hasn't mentioned it. Thanks.,It seems to be true as if the frechet exists then it is the same as the gateaux which would then be nonlinear (contradiction) but this seems useful yet I can't really find it anywhere and the lecturer hasn't mentioned it. Thanks.,,"['real-analysis', 'derivatives']"
89,Multivariable dual numbers and multivariable automatic differentiation,Multivariable dual numbers and multivariable automatic differentiation,,"I understand the basics of how dual numbers work, as well as how they are used for automatic differentiation, as described here: Dual Numbers & Automatic Differentiation I was wondering, how would you extend this concept to get partial derivatives of a function? Basically I have a multivariable function, and I'd like to calculate it's value and gradient for a specific input. I started off by looking at how multiplication of dual numbers is derived for a function of a single variable of the form $y=f(x)$. (Note the $\epsilon^2$ in the last step turns to 0 which makes that term disappear): $(a+b\epsilon)*(c+d\epsilon) = \\ ac+(bc+ad)\epsilon+bd\epsilon^2 = \\ ac + (bc+ad)\epsilon$ That made me think that maybe I could just have an $\epsilon$ defined per variable in a $z=f(x,y)$ function, so I gave it a shot. (Note that the $x^2$ and $y^2$ terms disappear below for the same reason as above): $ x=\epsilon_x \\ y=\epsilon_y \\ (a+bx+cy)*(d+ex+fy)= \\ ad+(ae+bd)x+(af+cd)y+(bf+ce)xy+bex^2+cfy^2= \\ ad+(ae+bd)x+(af+cd)y+(bf+ce)xy $ This looks pretty good except for the $xy$ term, which I have no idea how to account for in the gradient, or how to interpret. Can anyone help me out towards understanding how to do multivariable automatic differentiation?","I understand the basics of how dual numbers work, as well as how they are used for automatic differentiation, as described here: Dual Numbers & Automatic Differentiation I was wondering, how would you extend this concept to get partial derivatives of a function? Basically I have a multivariable function, and I'd like to calculate it's value and gradient for a specific input. I started off by looking at how multiplication of dual numbers is derived for a function of a single variable of the form $y=f(x)$. (Note the $\epsilon^2$ in the last step turns to 0 which makes that term disappear): $(a+b\epsilon)*(c+d\epsilon) = \\ ac+(bc+ad)\epsilon+bd\epsilon^2 = \\ ac + (bc+ad)\epsilon$ That made me think that maybe I could just have an $\epsilon$ defined per variable in a $z=f(x,y)$ function, so I gave it a shot. (Note that the $x^2$ and $y^2$ terms disappear below for the same reason as above): $ x=\epsilon_x \\ y=\epsilon_y \\ (a+bx+cy)*(d+ex+fy)= \\ ad+(ae+bd)x+(af+cd)y+(bf+ce)xy+bex^2+cfy^2= \\ ad+(ae+bd)x+(af+cd)y+(bf+ce)xy $ This looks pretty good except for the $xy$ term, which I have no idea how to account for in the gradient, or how to interpret. Can anyone help me out towards understanding how to do multivariable automatic differentiation?",,"['derivatives', 'partial-derivative']"
90,Continuity ( and differentiability) of a function specified by the functional equation $f\bigl(xf(y)\bigr)f(y)=f(x+y)$,Continuity ( and differentiability) of a function specified by the functional equation,f\bigl(xf(y)\bigr)f(y)=f(x+y),"I was studying the functional equation $f\bigl(xf(y)\bigr)f(y)=f(x+y)$ and observed that for $x=\delta y$ we can write $f(y+\delta y)-f(y) = f(y)\Bigl(f\bigl(\delta yf(y)\bigr)-1\Bigr)$ . I then reasoned that, as $f(0)=1$ we can make $f(y+\delta y)-f(y)$ arbitrarily small by decreasing $\delta y$ so the function ought to be continuous. I want to know whether this proves the function is continuous or the proof requires $f$ to be continuous at $x=0$ as well. Also what can I do to prove this (apart from solving the equation)? The solution, by the way, is $f(x)= \frac{2}{2-x}$ for $x\ne2$ .","I was studying the functional equation and observed that for we can write . I then reasoned that, as we can make arbitrarily small by decreasing so the function ought to be continuous. I want to know whether this proves the function is continuous or the proof requires to be continuous at as well. Also what can I do to prove this (apart from solving the equation)? The solution, by the way, is for .",f\bigl(xf(y)\bigr)f(y)=f(x+y) x=\delta y f(y+\delta y)-f(y) = f(y)\Bigl(f\bigl(\delta yf(y)\bigr)-1\Bigr) f(0)=1 f(y+\delta y)-f(y) \delta y f x=0 f(x)= \frac{2}{2-x} x\ne2,"['real-analysis', 'analysis', 'derivatives', 'continuity', 'functional-equations']"
91,What would be the Lyapunov exponent of this map,What would be the Lyapunov exponent of this map,,"$p_i$ is the probability of the occurence of unique symbol $i$. In the case of Tent map, the Lyapunov exponent (LE) is log of the derivative of the tent map which is almost always 2. So, $\lambda = log(2)$. For the probabilistic Bernoulli map, I tried to find the derivative like this: $f'(x) = 1/p_1$ for $0<x<p_1$, or $f'(x) = 1-p_1/p_2$ for  $p_1<x\le p_2$ and so on. Please correct me if the derivative and limits are wrong. In K Feltekh, D Fournier-Prunaret, S Belghith, Analytical expressions for power spectral density issued from one-dimensional continuous piecewise linear maps with three slopes. Signal Process. 94:, 149–157 (2014). the anlytical expression for LE for any piecewise linear map in general is $\lambda = (1-p)\ln(2/(1-p)) + p\ln(1/p)$ where $p$ is a constant and $p \in (0,1)$. Then, How can I apply the above result and calculate the LE for the map in Eq(6)? Since, Tent map is conjugate to Bernoulli map, so would the probabilistic Bernoulli map have the same LE as Tent Map which is log(2)?","$p_i$ is the probability of the occurence of unique symbol $i$. In the case of Tent map, the Lyapunov exponent (LE) is log of the derivative of the tent map which is almost always 2. So, $\lambda = log(2)$. For the probabilistic Bernoulli map, I tried to find the derivative like this: $f'(x) = 1/p_1$ for $0<x<p_1$, or $f'(x) = 1-p_1/p_2$ for  $p_1<x\le p_2$ and so on. Please correct me if the derivative and limits are wrong. In K Feltekh, D Fournier-Prunaret, S Belghith, Analytical expressions for power spectral density issued from one-dimensional continuous piecewise linear maps with three slopes. Signal Process. 94:, 149–157 (2014). the anlytical expression for LE for any piecewise linear map in general is $\lambda = (1-p)\ln(2/(1-p)) + p\ln(1/p)$ where $p$ is a constant and $p \in (0,1)$. Then, How can I apply the above result and calculate the LE for the map in Eq(6)? Since, Tent map is conjugate to Bernoulli map, so would the probabilistic Bernoulli map have the same LE as Tent Map which is log(2)?",,"['probability', 'derivatives', 'eigenvalues-eigenvectors', 'dynamical-systems', 'chaos-theory']"
92,Estimation of an expression involving Riemann zeta function,Estimation of an expression involving Riemann zeta function,,"Let $\zeta$ be the Riemann zeta function and let $\zeta^{(k)}$ be the $k-$ th derivative  of the Riemann zeta function. For $s$ on the vertical line $\Re(s)=1+\kappa,$ where $\kappa$ is a positive real number, can we have an explicit value for $\frac{\zeta^{(k)}(1-s)}{\zeta(s)}$ or  are there  some estimations for the term  $\frac{\zeta^{(k)}(1-s)}{\zeta(s)}$? Many thanks.","Let $\zeta$ be the Riemann zeta function and let $\zeta^{(k)}$ be the $k-$ th derivative  of the Riemann zeta function. For $s$ on the vertical line $\Re(s)=1+\kappa,$ where $\kappa$ is a positive real number, can we have an explicit value for $\frac{\zeta^{(k)}(1-s)}{\zeta(s)}$ or  are there  some estimations for the term  $\frac{\zeta^{(k)}(1-s)}{\zeta(s)}$? Many thanks.",,"['number-theory', 'derivatives', 'analytic-number-theory', 'riemann-zeta']"
93,How strong continuity is needed for the values of derivative on rational numbers to contain all information about the derivative?,How strong continuity is needed for the values of derivative on rational numbers to contain all information about the derivative?,,"Consider these different notions of continuity, listed in order of decreasing strength: Continuously differentiable Lipschitz continuous Absolutely continuous Uniformly continuous Continuous I was wondering how far up the list we have to go before it is ensured that a function $f:\mathbb R\to\mathbb R$ must be such that $f'(x_0)$ for an irrational $x_0$ can always be recovered from information about only the values of $f'$ for rational numbers, i.e., that $f'(x_0)=\lim_{x\to x_0}f'|_{\mathbb Q}(x)$ whenever the right-hand side of that equation is defined. I know that uniform continuity is not sufficient: Minkowski's_question_mark_function is a counter-example (for all rational $x$, $f'(x)$ is $0$, so the RHS of the equation is also always $0$, but there are irrational $a$ such that the LHS is not). On the other hand, $f$ being continuously differentiable is obviously sufficient. But what about absolute and Lipschitz continuity?","Consider these different notions of continuity, listed in order of decreasing strength: Continuously differentiable Lipschitz continuous Absolutely continuous Uniformly continuous Continuous I was wondering how far up the list we have to go before it is ensured that a function $f:\mathbb R\to\mathbb R$ must be such that $f'(x_0)$ for an irrational $x_0$ can always be recovered from information about only the values of $f'$ for rational numbers, i.e., that $f'(x_0)=\lim_{x\to x_0}f'|_{\mathbb Q}(x)$ whenever the right-hand side of that equation is defined. I know that uniform continuity is not sufficient: Minkowski's_question_mark_function is a counter-example (for all rational $x$, $f'(x)$ is $0$, so the RHS of the equation is also always $0$, but there are irrational $a$ such that the LHS is not). On the other hand, $f$ being continuously differentiable is obviously sufficient. But what about absolute and Lipschitz continuity?",,"['derivatives', 'continuity', 'lipschitz-functions', 'absolute-continuity']"
94,Derivation practical problem rate of change pumping water from cylinder to reservoir,Derivation practical problem rate of change pumping water from cylinder to reservoir,,"I'm stuck at this problem and the solution doesn't make sense to me, I would like to understand what's wrong with my reasoning. There is a cylindrical tank with height 15m and radius 10m, fluid is being pumped into a biddon, which is composed of a rectangle and a prism (see picture). We know that the liquid level is decreasing with 2cm/sec in te cylindrical tank, and want the rate the level increases in the biddon at a level of 6m My way of solving is first calculate $V^\prime$. $V=hr^2\pi$ so $V^\prime=\pi r^2h^\prime\, \to\, V^\prime=10000(2\pi)=20000\pi$ Then for the body I split up in two parts,  First  $V=1000(1500h)$ Second $V=(800\cdot1500h)/2$ (I did $\frac{30}{10}=\frac{x}{6}\,\to\,x=18$ and $b=18-10$) Added them together $V=15\cdot10^5h+6\cdot10^5h \,\to\, 21\cdot10^5h$ Then  $\frac{V^\prime}{21\cdot10^5}=h^\prime$ so $h^\prime=0.01496\,$cm/sec However the result should be 0.019 cm/s, and it involves the height of the cylinder, but I don't see how that matters, or why my way is wrong. I would really appreciate your help :)","I'm stuck at this problem and the solution doesn't make sense to me, I would like to understand what's wrong with my reasoning. There is a cylindrical tank with height 15m and radius 10m, fluid is being pumped into a biddon, which is composed of a rectangle and a prism (see picture). We know that the liquid level is decreasing with 2cm/sec in te cylindrical tank, and want the rate the level increases in the biddon at a level of 6m My way of solving is first calculate $V^\prime$. $V=hr^2\pi$ so $V^\prime=\pi r^2h^\prime\, \to\, V^\prime=10000(2\pi)=20000\pi$ Then for the body I split up in two parts,  First  $V=1000(1500h)$ Second $V=(800\cdot1500h)/2$ (I did $\frac{30}{10}=\frac{x}{6}\,\to\,x=18$ and $b=18-10$) Added them together $V=15\cdot10^5h+6\cdot10^5h \,\to\, 21\cdot10^5h$ Then  $\frac{V^\prime}{21\cdot10^5}=h^\prime$ so $h^\prime=0.01496\,$cm/sec However the result should be 0.019 cm/s, and it involves the height of the cylinder, but I don't see how that matters, or why my way is wrong. I would really appreciate your help :)",,"['calculus', 'derivatives']"
95,Find the third derivative of $xe^{(x^3)}$ and show points where the derivative is $0$ [closed],Find the third derivative of  and show points where the derivative is  [closed],xe^{(x^3)} 0,"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Find the third derivative of $f(x)=xe^{(x^3)}$. Along with the graph of the third derivative show the points where the third derivative is $0$. This is a problem that  I am supposed to solve in Wolfram Mathematica. I have begun with finding the derivative using D[[f[x],{x,3}] but I am having trouble finding the points and showing them on the graph. In the presentations from my school it says that I am supposed to use the function Last[Last[FindRoot[f'[x], {x,0}]]], but I don't understand why I should use it because it's not really showing anything on my graph. By the way, {x,0} in the FindRoot[] function is supposed to be find the solutions for the equation in the area around 0 or ..? I'm guessing that since the graph of the derivative looks like this: The whole code that I have written for this problem is this: GRAFIK[x_] = x*E^(x^3) r1[x_] = D[x*E^(x^3), {x, 3}] Simplify[3 (6 E^x^3 x + 9 E^x^3 x^4) +    x (6 E^x^3 + 54 E^x^3 x^3 + 27 E^x^3 x^6)] a = Plot[r1[x], {x, -1.5, 1}] rese1 = Last[Last[FindRoot[r1[x] == 0, {x, 0}]]] rese2 = Last[Last[FindRoot[r1[x] == 0, {x, -1.5}]]] rese3 = Last[Last[FindRoot[r1[x] == 0, {x, -0.5}]]] b = ListPlot[{rese1, GRAFIK[rese1]}, PlotStyle -> PointSize[0.02]]; c = ListPlot[{rese2, GRAFIK[rese2]}, PlotStyle -> PointSize[0.02]]; d = ListPlot[{rese3, GRAFIK[rese3]}, PlotStyle -> PointSize[0.02]]; Show[{a, b}, {a, c}, {a, d}]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Find the third derivative of $f(x)=xe^{(x^3)}$. Along with the graph of the third derivative show the points where the third derivative is $0$. This is a problem that  I am supposed to solve in Wolfram Mathematica. I have begun with finding the derivative using D[[f[x],{x,3}] but I am having trouble finding the points and showing them on the graph. In the presentations from my school it says that I am supposed to use the function Last[Last[FindRoot[f'[x], {x,0}]]], but I don't understand why I should use it because it's not really showing anything on my graph. By the way, {x,0} in the FindRoot[] function is supposed to be find the solutions for the equation in the area around 0 or ..? I'm guessing that since the graph of the derivative looks like this: The whole code that I have written for this problem is this: GRAFIK[x_] = x*E^(x^3) r1[x_] = D[x*E^(x^3), {x, 3}] Simplify[3 (6 E^x^3 x + 9 E^x^3 x^4) +    x (6 E^x^3 + 54 E^x^3 x^3 + 27 E^x^3 x^6)] a = Plot[r1[x], {x, -1.5, 1}] rese1 = Last[Last[FindRoot[r1[x] == 0, {x, 0}]]] rese2 = Last[Last[FindRoot[r1[x] == 0, {x, -1.5}]]] rese3 = Last[Last[FindRoot[r1[x] == 0, {x, -0.5}]]] b = ListPlot[{rese1, GRAFIK[rese1]}, PlotStyle -> PointSize[0.02]]; c = ListPlot[{rese2, GRAFIK[rese2]}, PlotStyle -> PointSize[0.02]]; d = ListPlot[{rese3, GRAFIK[rese3]}, PlotStyle -> PointSize[0.02]]; Show[{a, b}, {a, c}, {a, d}]",,"['derivatives', 'mathematica']"
96,Show that $f$ have a zero of $m\ge n$ multiplicity iff exists some $g$ such that $f(x)=(x-x_0)^n g(x)$,Show that  have a zero of  multiplicity iff exists some  such that,f m\ge n g f(x)=(x-x_0)^n g(x),"It is the exercise 10 on page 348 of Analysis I of Amann and Escher. Please, comment if this lacks something. Thank you. Let $X\subseteq\Bbb K$ be perfect and $f\in C^n(X,\Bbb K)$ for some $n\in\Bbb N_{>0}$ . A number $x_0\in X$ is called a zero of multiplicity $n$ of $f$ if $f(x_0)=f'(x_0)=\ldots=f^{(n-1)}(x_0)=0$ and $f^{(n)}(x_0)\neq 0$ . Show that if $X$ is convex, then $f$ have a zero of multiplicity $\ge n$ at $x_0$ if and only if there is some $g\in C(X,\Bbb K)$ such that $f(x)=(x-x_0)^n g(x)$ for all $x\in X$ . $(\implies)$ From the characteristics of $f$ if we define $$g(x):=\begin{cases}\frac{f(x)}{(x-x_0)^n}, &x\in X\setminus\{x_0\}\\f^{(n)}(x_0)/n!,& x=x_0\end{cases}$$ the function is continuous and defined in $X$ since $$\lim_{x\to x_0}\frac{f(x)}{(x-x_0)^n}=\lim_{x\to x_0}\frac{f^{(n)}(x)}{n!}=\frac{f^{(n)}(x_0)}{n!}$$ The limit is well-defined because $X$ is perfect. Because $f$ is $n$ -times differentiable then $g$ is $n$ -times differentiable at least in $X\setminus\{x_0\}$ . $(\impliedby)$ If we have a function $g\in C^n(X,\Bbb K)$ then the function defined by $$f(x):=(x-x_0)^n g(x)$$ have a zero of multiplicity $m\ge n$ in $x_0$ . Certainly $g\in C(X,\Bbb K)$ , as required. Question: I dont know how to complete the discrepancy about $g$ in one implication or the other, I mean, from the first implication we need that $g$ would be $n$ -times differentiable at least in $X\setminus\{x_0\}$ . But from the second implication I only can define $g$ as $n$ -times differentiable in $X$ .","It is the exercise 10 on page 348 of Analysis I of Amann and Escher. Please, comment if this lacks something. Thank you. Let be perfect and for some . A number is called a zero of multiplicity of if and . Show that if is convex, then have a zero of multiplicity at if and only if there is some such that for all . From the characteristics of if we define the function is continuous and defined in since The limit is well-defined because is perfect. Because is -times differentiable then is -times differentiable at least in . If we have a function then the function defined by have a zero of multiplicity in . Certainly , as required. Question: I dont know how to complete the discrepancy about in one implication or the other, I mean, from the first implication we need that would be -times differentiable at least in . But from the second implication I only can define as -times differentiable in .","X\subseteq\Bbb K f\in C^n(X,\Bbb K) n\in\Bbb N_{>0} x_0\in X n f f(x_0)=f'(x_0)=\ldots=f^{(n-1)}(x_0)=0 f^{(n)}(x_0)\neq 0 X f \ge n x_0 g\in C(X,\Bbb K) f(x)=(x-x_0)^n g(x) x\in X (\implies) f g(x):=\begin{cases}\frac{f(x)}{(x-x_0)^n}, &x\in X\setminus\{x_0\}\\f^{(n)}(x_0)/n!,& x=x_0\end{cases} X \lim_{x\to x_0}\frac{f(x)}{(x-x_0)^n}=\lim_{x\to x_0}\frac{f^{(n)}(x)}{n!}=\frac{f^{(n)}(x_0)}{n!} X f n g n X\setminus\{x_0\} (\impliedby) g\in C^n(X,\Bbb K) f(x):=(x-x_0)^n g(x) m\ge n x_0 g\in C(X,\Bbb K) g g n X\setminus\{x_0\} g n X","['real-analysis', 'derivatives', 'proof-verification']"
97,Large slopes in small intervals imply non-differentiability,Large slopes in small intervals imply non-differentiability,,"Let $f:\mathbb R\to\mathbb R$ be a function and suppose that for all $n\in\mathbb N$, there exist some $a_n,b_n\in\mathbb R$ such that $$0\leq a_n<b_n\leq\frac{1}{n}\qquad\text{and}\qquad \frac{f(b_n)-f(a_n)}{b_n-a_n}>n.$$ I want to show that $f$ is not differentiable at $x=0$ in this case. Of course, the non-existence of a right-hand derivative would be sufficient, yet I seem to be stuck with proving this. Any hints (or counterexamples, for that matter) would be greatly appreciated.","Let $f:\mathbb R\to\mathbb R$ be a function and suppose that for all $n\in\mathbb N$, there exist some $a_n,b_n\in\mathbb R$ such that $$0\leq a_n<b_n\leq\frac{1}{n}\qquad\text{and}\qquad \frac{f(b_n)-f(a_n)}{b_n-a_n}>n.$$ I want to show that $f$ is not differentiable at $x=0$ in this case. Of course, the non-existence of a right-hand derivative would be sufficient, yet I seem to be stuck with proving this. Any hints (or counterexamples, for that matter) would be greatly appreciated.",,"['calculus', 'real-analysis', 'derivatives']"
98,How to calculate $\int_{e^{-2n\pi }}^{1}\left | \frac{d }{dx}\cos\left ( \ln\frac{1}{x} \right ) \right |dx$,How to calculate,\int_{e^{-2n\pi }}^{1}\left | \frac{d }{dx}\cos\left ( \ln\frac{1}{x} \right ) \right |dx,I have no idea on how to calculate $$\int_{e^{-2n\pi }}^{1}\left | \frac{d }{dx}\cos\left ( \ln\frac{1}{x} \right ) \right |dx$$ any hint would be appreciate.,I have no idea on how to calculate $$\int_{e^{-2n\pi }}^{1}\left | \frac{d }{dx}\cos\left ( \ln\frac{1}{x} \right ) \right |dx$$ any hint would be appreciate.,,['calculus']
99,Differentiable $f\colon I\to\mathbb{C}$ with bounded derivative is Lipschitz continuous,Differentiable  with bounded derivative is Lipschitz continuous,f\colon I\to\mathbb{C},"Prove that a differentiable function $f\colon I\to\mathbb{C}$ on an interval $I$ with bounded derivative is Lipschitz continuous, i.e. If $\lvert f'\rvert\leq L $ for some $L\in\mathbb{R}$, then for any $x_1,x_2\in I$ we have $$ \lvert f(x_1)-f(x_2)\rvert\leqslant L\lvert x_1-x_2\rvert. $$ Despite two little things, I think the proof should work as follows: I think, we can choose some $c\in\mathbb{C}$ with $\lvert c\rvert =1$ such that $$ \lvert f(x_2)-f(x_1)\rvert = c\cdot (f(x_2)-f(x_1))~~(*) $$ since for  $$ \frac{f(x_2)-f(x_1)}{\lvert f(x_2)-f(x_1)\rvert}=:v $$ we have $\lvert v\rvert =1$ and then we can define $c:=v^{-1}$. Next, in order to have equation $(*)$, the LHS has to be the real part of the RHS, i.e. $$ \lvert f(x_2)-f(x_1)\rvert =\Re(c\cdot (f(x_2)-f(x_1)))=\varphi(x_2)-\varphi(x_1), $$ where $$ \varphi:=\Re(cf). $$ Now, since $f$ is differentiable on $I$, it is, in particular, continuous on $[x_1,x_2]$, hence $\varphi$ is also continuous on $[x_1,x_2]$. I am not sure about the following question: Do we also have that $\varphi$ is differentiable on $(x_1,x_2)$? (Q) Assuming that we can answer question (Q) with YES, we could apply the mean value Theorem on $\varphi$, telling us that  $$ \varphi(x_2)-\varphi(x_1)=(x_2-x_1)\varphi'(\xi) $$  for some $\xi\in (x_1,x_2)$. By assumption, $\varphi'(\xi)=\Re(cf'(\xi))\leq\lvert cf'(\xi)\rvert\leq L$ and hence $$ \lvert f(x_2)-f(x_1)\rvert = \varphi(x_2)-\varphi(x_1)=(x_2-x_1)\varphi'(\xi)\leqslant L\lvert x_2-x_1\rvert. $$ I am also not completely sure if     $$ \varphi' = \Re(cf') $$     is correct. Despite the two things in the two yellow boxes, I am pretty sure the proof should work. It would be nice if you could give me some hints.","Prove that a differentiable function $f\colon I\to\mathbb{C}$ on an interval $I$ with bounded derivative is Lipschitz continuous, i.e. If $\lvert f'\rvert\leq L $ for some $L\in\mathbb{R}$, then for any $x_1,x_2\in I$ we have $$ \lvert f(x_1)-f(x_2)\rvert\leqslant L\lvert x_1-x_2\rvert. $$ Despite two little things, I think the proof should work as follows: I think, we can choose some $c\in\mathbb{C}$ with $\lvert c\rvert =1$ such that $$ \lvert f(x_2)-f(x_1)\rvert = c\cdot (f(x_2)-f(x_1))~~(*) $$ since for  $$ \frac{f(x_2)-f(x_1)}{\lvert f(x_2)-f(x_1)\rvert}=:v $$ we have $\lvert v\rvert =1$ and then we can define $c:=v^{-1}$. Next, in order to have equation $(*)$, the LHS has to be the real part of the RHS, i.e. $$ \lvert f(x_2)-f(x_1)\rvert =\Re(c\cdot (f(x_2)-f(x_1)))=\varphi(x_2)-\varphi(x_1), $$ where $$ \varphi:=\Re(cf). $$ Now, since $f$ is differentiable on $I$, it is, in particular, continuous on $[x_1,x_2]$, hence $\varphi$ is also continuous on $[x_1,x_2]$. I am not sure about the following question: Do we also have that $\varphi$ is differentiable on $(x_1,x_2)$? (Q) Assuming that we can answer question (Q) with YES, we could apply the mean value Theorem on $\varphi$, telling us that  $$ \varphi(x_2)-\varphi(x_1)=(x_2-x_1)\varphi'(\xi) $$  for some $\xi\in (x_1,x_2)$. By assumption, $\varphi'(\xi)=\Re(cf'(\xi))\leq\lvert cf'(\xi)\rvert\leq L$ and hence $$ \lvert f(x_2)-f(x_1)\rvert = \varphi(x_2)-\varphi(x_1)=(x_2-x_1)\varphi'(\xi)\leqslant L\lvert x_2-x_1\rvert. $$ I am also not completely sure if     $$ \varphi' = \Re(cf') $$     is correct. Despite the two things in the two yellow boxes, I am pretty sure the proof should work. It would be nice if you could give me some hints.",,"['real-analysis', 'derivatives', 'continuity']"
