,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Weighted average intuition,Weighted average intuition,,"Recently I came across the weighed average. I get how it works on a technical level. Maybe I'm a bit thick, but somehow I can't get a ""feel"" for it on an intuitive level. Let's just say we have two points on a line: |-----*-----------*---------> 0     A           B Now, we want to put a point $M$ somewhere between $A, B$ inclusive. If we want to put it at exactly at $A$, we can say $M = 1 \times A + 0 \times B$ If we want to put it at exactly at $B$, we can say $M = 0 \times A + 1 \times B$ That seems kind of obvious. What seems to somehow surprise me is, that for example, if we want to put it exactly between $A, B$, we can say $M = 0.5 \times A + 0.5 \times B$ Somehow I have a hard time grokking that this will put the point $M$ at the middle.","Recently I came across the weighed average. I get how it works on a technical level. Maybe I'm a bit thick, but somehow I can't get a ""feel"" for it on an intuitive level. Let's just say we have two points on a line: |-----*-----------*---------> 0     A           B Now, we want to put a point $M$ somewhere between $A, B$ inclusive. If we want to put it at exactly at $A$, we can say $M = 1 \times A + 0 \times B$ If we want to put it at exactly at $B$, we can say $M = 0 \times A + 1 \times B$ That seems kind of obvious. What seems to somehow surprise me is, that for example, if we want to put it exactly between $A, B$, we can say $M = 0.5 \times A + 0.5 \times B$ Somehow I have a hard time grokking that this will put the point $M$ at the middle.",,"['statistics', 'intuition']"
1,Understanding the density function and expected value of an estimator.,Understanding the density function and expected value of an estimator.,,"Suppose that we have a sample $X_1,\ldots,X_n$ from the distribution with density function $$f(x\mid\theta) = \dfrac{2x}{\theta^2}\mathbb{1}_{(0,\theta)}(x)$$ w.r.t. the Lebesgue measure. I recently learned that if we use $T(X) =\max(X_1,\ldots,X_n)$ as an estimator for $\theta$, the density function of $T(X)$ equals $$f_n(x\mid\theta) = n(F(x\mid\theta))^{n-1}f(x\mid\theta) = \dfrac{2n}{\theta^{2n}}x^{n-1}\mathbb{1}_{(0,\theta)}(x)$$ and thus that $$\operatorname{E}_\theta(T(X)) = \displaystyle\int_0^\theta x\dfrac{2n}{\theta^{2n}}x^{n-1}\mathbb{1}_{(0,\theta)}(x) \, dx.$$ What I don't understand is when the density function of an estimator differs from the density function of the data. In my book the following example is given: Suppose $X\sim\operatorname{Geom}(\theta)$, with $\theta\in(0,1)$. We seek for an unbiased estimator for $\theta$. We have that $$\operatorname{E}_\theta d(X) = \sum\limits_{i = 1}^\infty d(i)\theta(1-\theta)^{i-1}$$ where $d(X)$ is an estimator for $\theta$. Here the density function of the data is used to calculate the expected value of the estimator. Why is this case different to the case with the order statistic? This computation of the expected value would surely be wrong if $d(X)$ equals the order statistic right? Question: When calculating the expected value of an estimator, how do you determine which density function you should use? Is the order statistic a special case? I would really like to know what the underlying theory behind all this is, because now it comes across as quite random. Thanks in advance!","Suppose that we have a sample $X_1,\ldots,X_n$ from the distribution with density function $$f(x\mid\theta) = \dfrac{2x}{\theta^2}\mathbb{1}_{(0,\theta)}(x)$$ w.r.t. the Lebesgue measure. I recently learned that if we use $T(X) =\max(X_1,\ldots,X_n)$ as an estimator for $\theta$, the density function of $T(X)$ equals $$f_n(x\mid\theta) = n(F(x\mid\theta))^{n-1}f(x\mid\theta) = \dfrac{2n}{\theta^{2n}}x^{n-1}\mathbb{1}_{(0,\theta)}(x)$$ and thus that $$\operatorname{E}_\theta(T(X)) = \displaystyle\int_0^\theta x\dfrac{2n}{\theta^{2n}}x^{n-1}\mathbb{1}_{(0,\theta)}(x) \, dx.$$ What I don't understand is when the density function of an estimator differs from the density function of the data. In my book the following example is given: Suppose $X\sim\operatorname{Geom}(\theta)$, with $\theta\in(0,1)$. We seek for an unbiased estimator for $\theta$. We have that $$\operatorname{E}_\theta d(X) = \sum\limits_{i = 1}^\infty d(i)\theta(1-\theta)^{i-1}$$ where $d(X)$ is an estimator for $\theta$. Here the density function of the data is used to calculate the expected value of the estimator. Why is this case different to the case with the order statistic? This computation of the expected value would surely be wrong if $d(X)$ equals the order statistic right? Question: When calculating the expected value of an estimator, how do you determine which density function you should use? Is the order statistic a special case? I would really like to know what the underlying theory behind all this is, because now it comes across as quite random. Thanks in advance!",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'statistical-inference']"
2,A Problem on Beta distribution .,A Problem on Beta distribution .,,"In this problem i know that $X\sim B(m,n)$ and $(1-X)\sim B(n,m)$ After putting values in $Y_i$ i got this $\dfrac{x^2}{1-x^2}$  I have not idea what do further . I am confused.","In this problem i know that $X\sim B(m,n)$ and $(1-X)\sim B(n,m)$ After putting values in $Y_i$ i got this $\dfrac{x^2}{1-x^2}$  I have not idea what do further . I am confused.",,"['probability', 'statistics', 'probability-distributions']"
3,Mean and variance of a scaled Poisson random variable,Mean and variance of a scaled Poisson random variable,,"Suppose I have a variable, $Y$, that is a scaled Poisson random variable. That is: $$ Y = kX $$ and $$ X \sim \mathrm{Poisson}(\mu) $$ This means that PMF of $Y$, $P(y)$, is given by: $$ P(y) = e^{-\mu} \frac{\mu^{\frac{y}{k}}}{\left(\frac{y}{k}\right)!} $$ What would the mean and variance of $Y$ be in this situation? (Of course, the mean and variance of $X$ is just $\mu$.)","Suppose I have a variable, $Y$, that is a scaled Poisson random variable. That is: $$ Y = kX $$ and $$ X \sim \mathrm{Poisson}(\mu) $$ This means that PMF of $Y$, $P(y)$, is given by: $$ P(y) = e^{-\mu} \frac{\mu^{\frac{y}{k}}}{\left(\frac{y}{k}\right)!} $$ What would the mean and variance of $Y$ be in this situation? (Of course, the mean and variance of $X$ is just $\mu$.)",,"['statistics', 'poisson-distribution']"
4,Show joint cdf is continuous,Show joint cdf is continuous,,"Let $X$ have a standard normal distribution and let $Y=2X$. Show the cdf $F(x, y)$ of $(X, Y)$ is continuous. I have attempted this below, but I think I am pretty far off track. $$F_{XY}(x, y) = P(X \le x, 2X \le y) = P(X \le (x \wedge \frac{y}{2}))$$ $$ \Phi(x) = F_{XY}(x, y) = \begin{cases} \frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^\frac{-z^2}{2}\text{d}z, \quad\text{ if } x \le \frac{y}{2}\\ \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^\frac{-z^2}{8}\text{d}z, \quad\text{ if } x > \frac{y}{2} \end{cases} $$ $$ \frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^\frac{-z^2}{2}\text{d}z = \frac{\operatorname{erf}(\frac{x}{\sqrt{2}})}{2}+1 $$ $$ \frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^\frac{-z^2}{8}\,\text{d}z = \operatorname{erf} \left(\frac{x}{2^\frac{3}{2}}\right) $$ $$ \operatorname{erf}(x) \text{ continuous } \implies F_{XY}(x,y) \text{ continuous } $$","Let $X$ have a standard normal distribution and let $Y=2X$. Show the cdf $F(x, y)$ of $(X, Y)$ is continuous. I have attempted this below, but I think I am pretty far off track. $$F_{XY}(x, y) = P(X \le x, 2X \le y) = P(X \le (x \wedge \frac{y}{2}))$$ $$ \Phi(x) = F_{XY}(x, y) = \begin{cases} \frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^\frac{-z^2}{2}\text{d}z, \quad\text{ if } x \le \frac{y}{2}\\ \frac{1}{\sqrt{2\pi}}\int_{-\infty}^x e^\frac{-z^2}{8}\text{d}z, \quad\text{ if } x > \frac{y}{2} \end{cases} $$ $$ \frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^\frac{-z^2}{2}\text{d}z = \frac{\operatorname{erf}(\frac{x}{\sqrt{2}})}{2}+1 $$ $$ \frac{1}{\sqrt{2\pi}}\int_{-\infty}^xe^\frac{-z^2}{8}\,\text{d}z = \operatorname{erf} \left(\frac{x}{2^\frac{3}{2}}\right) $$ $$ \operatorname{erf}(x) \text{ continuous } \implies F_{XY}(x,y) \text{ continuous } $$",,"['statistics', 'continuity', 'normal-distribution']"
5,Derive the posterior distribution and compute the posterior mean.,Derive the posterior distribution and compute the posterior mean.,,"Exercise: Suppose $X_1,\ldots,X_n|\Theta = \theta \stackrel{iid}{\sim} \operatorname{Pois}(\theta)$ and $\Omega\sim\operatorname{Ga}(\alpha, \beta)$. Derive the posterior distribution and compute the posterior mean. What I've tried: I know that the posterior distribution is equal to $$f_{\Theta|X}(\theta|x) = \dfrac{f_{X|\Theta}(x|\theta)\,f_\Theta(\theta)}{\int f_{X|\Theta}(x|\theta)\,f_\Theta(\theta)d\theta}$$ so with $f_{X|\Theta}(x|\theta)\,f_{\Theta}(\theta) = \dfrac{\theta^{\sum x_i}e^{-n\theta}}{\prod x_i !}\dfrac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1}e^{-\beta\theta}$ this means that $$f_{\Theta|X}(\theta|x)=\dfrac{\dfrac{\theta^{\sum x_i}e^{-n\theta}}{\prod x_i !}\dfrac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1}e^{-\beta\theta}}{\displaystyle\int\dfrac{\theta^{\sum x_i}e^{-n\theta}}{\prod x_i !}\dfrac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1}e^{-\beta\theta}d\theta}.$$ I think this is some sort of Beta distribution. However, I'm not sure which one and how to get there. Question: How do I solve this exercise? Thanks in advance!","Exercise: Suppose $X_1,\ldots,X_n|\Theta = \theta \stackrel{iid}{\sim} \operatorname{Pois}(\theta)$ and $\Omega\sim\operatorname{Ga}(\alpha, \beta)$. Derive the posterior distribution and compute the posterior mean. What I've tried: I know that the posterior distribution is equal to $$f_{\Theta|X}(\theta|x) = \dfrac{f_{X|\Theta}(x|\theta)\,f_\Theta(\theta)}{\int f_{X|\Theta}(x|\theta)\,f_\Theta(\theta)d\theta}$$ so with $f_{X|\Theta}(x|\theta)\,f_{\Theta}(\theta) = \dfrac{\theta^{\sum x_i}e^{-n\theta}}{\prod x_i !}\dfrac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1}e^{-\beta\theta}$ this means that $$f_{\Theta|X}(\theta|x)=\dfrac{\dfrac{\theta^{\sum x_i}e^{-n\theta}}{\prod x_i !}\dfrac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1}e^{-\beta\theta}}{\displaystyle\int\dfrac{\theta^{\sum x_i}e^{-n\theta}}{\prod x_i !}\dfrac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1}e^{-\beta\theta}d\theta}.$$ I think this is some sort of Beta distribution. However, I'm not sure which one and how to get there. Question: How do I solve this exercise? Thanks in advance!",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'statistical-inference']"
6,Transformation of random variable and distribution function,Transformation of random variable and distribution function,,"I have task ""Let X by random varaible with exponential distribution and parameter $\lambda$"" We have to find distribution function of random variable $Y=e^x$ My solution is quite easy $1) F_{y}(Y) = P( y <= Y ) = P(y <= e^x ) = P( ln(y) <= x )$ as the last step i used inversion function on both side ( ln ) 2) $P(ln(y) <= x ) = 1 - P( x <= ln(y) = 1 - F_{x}(ln(y))$ And we know that distribution function of x is exponential distribution function so it should be $F_{y} = 1 - 1-e^{-\lambda x} = 1 - 1 - e^{-\lambda ln(y)}$ But i am not sure if i can assume, that when i have x in function its automaticly distribution function of X. Did i make mistake or is this way correct? Thank you very much for help.","I have task ""Let X by random varaible with exponential distribution and parameter $\lambda$"" We have to find distribution function of random variable $Y=e^x$ My solution is quite easy $1) F_{y}(Y) = P( y <= Y ) = P(y <= e^x ) = P( ln(y) <= x )$ as the last step i used inversion function on both side ( ln ) 2) $P(ln(y) <= x ) = 1 - P( x <= ln(y) = 1 - F_{x}(ln(y))$ And we know that distribution function of x is exponential distribution function so it should be $F_{y} = 1 - 1-e^{-\lambda x} = 1 - 1 - e^{-\lambda ln(y)}$ But i am not sure if i can assume, that when i have x in function its automaticly distribution function of X. Did i make mistake or is this way correct? Thank you very much for help.",,"['probability', 'statistics']"
7,How do we show that minimal expected length for confidence interval for normal with unknown mean is the following?,How do we show that minimal expected length for confidence interval for normal with unknown mean is the following?,,"How do we show that the $(1-\alpha)100\%$ confidence interval $(\bar{Y}-z_{\alpha-\alpha_1} \frac{\sigma}{\sqrt{n}}, \bar{Y}+z_{\alpha_1} \frac{\sigma}{\sqrt{n}})$ for the unknown normal mean $\mu$ has a minimal expected length when it is symmetric, that is, $\alpha_1 =\frac{\alpha}{2}$ Intuitively the middle has the largest percentages. But how do we formally prove it?","How do we show that the $(1-\alpha)100\%$ confidence interval $(\bar{Y}-z_{\alpha-\alpha_1} \frac{\sigma}{\sqrt{n}}, \bar{Y}+z_{\alpha_1} \frac{\sigma}{\sqrt{n}})$ for the unknown normal mean $\mu$ has a minimal expected length when it is symmetric, that is, $\alpha_1 =\frac{\alpha}{2}$ Intuitively the middle has the largest percentages. But how do we formally prove it?",,"['statistics', 'probability-distributions', 'normal-distribution', 'confidence-interval']"
8,A probability concentration inequality,A probability concentration inequality,,"Suppose $\mathbb{E}(X)=0 , \text{Var}(X)=1$ and $|X|\leq M$ on $\Omega$. Prove that $$P(|X|\geq t)\geq \frac{1-t^2}{M^2-t^2} \quad\forall\,0<t<1.$$ How I can prove , I attempted to use Markov's inequality but this does not hold. Please help me to prove this, thanks in advance.","Suppose $\mathbb{E}(X)=0 , \text{Var}(X)=1$ and $|X|\leq M$ on $\Omega$. Prove that $$P(|X|\geq t)\geq \frac{1-t^2}{M^2-t^2} \quad\forall\,0<t<1.$$ How I can prove , I attempted to use Markov's inequality but this does not hold. Please help me to prove this, thanks in advance.",,"['probability', 'probability-theory', 'statistics', 'inequality', 'concentration-of-measure']"
9,Is it correct to say that the second derivative at the maximum likelihood estimator can never be positive?,Is it correct to say that the second derivative at the maximum likelihood estimator can never be positive?,,"The maximum likelihood estimator is found to be the critical value of the likelihood function, that is also the local maximum. I understand that if the function is differentiable then you can analyse the second derivative at this point and if it is negative then it is indeed the local maximum. I was wondering if there were any cases that contradict this? I understand that some cases the function isn't differentiable or you need to calculate the hessian matrix to see that the function isn't just a saddle point. I am talking just in the simple single variable case. So is it okay to say: If $\frac{d^2L(\hat{\theta)}}{d\theta^2} > 0$ then $\hat{\theta}$ CANNOT be the MLE of $L(\theta)$?","The maximum likelihood estimator is found to be the critical value of the likelihood function, that is also the local maximum. I understand that if the function is differentiable then you can analyse the second derivative at this point and if it is negative then it is indeed the local maximum. I was wondering if there were any cases that contradict this? I understand that some cases the function isn't differentiable or you need to calculate the hessian matrix to see that the function isn't just a saddle point. I am talking just in the simple single variable case. So is it okay to say: If $\frac{d^2L(\hat{\theta)}}{d\theta^2} > 0$ then $\hat{\theta}$ CANNOT be the MLE of $L(\theta)$?",,"['statistics', 'fixed-point-theorems', 'maxima-minima', 'maximum-likelihood']"
10,Isn't the Martingale betting system completely rational?,Isn't the Martingale betting system completely rational?,,"https://en.wikipedia.org/wiki/Martingale_(betting_system) I don't think it's based on gambler's fallacy. Suppose I have $1+2+4+8+.....+2^{9}=1023$ Dollars. I'm about to bet on heads in the tosses of a fair coin. If the coin lands heads, I receive an amount double my bet (my original bet + profit equal to my original bet) My strategy will be: 1.Bet 1 Dollar on the first toss. 2.Double my bet on each successive toss. 3.Walk away on my first win. So, all the possible outcomes are: $$H, TH, TTH, TTTH, TTTTH, TTTTTH, TTTTTTH, TTTTTTTH, TTTTTTTTH, TTTTTTTTTH, TTTTTTTTTT$$ I end up making a 1 dollar profit in the first 10 of these outcomes. Only in the last case I lose 1023 dollars. But it would take a miracle for $TTTTTTTTTT$ to happen with a fair coin. I know that the expected value is negative but the expected value comes into play only when I keep on betting till the end of time. My strategy is to walk away on the first win. So, before I begin to bet, isn't it reasonable for me to believe that I'll most likely make a profit?","https://en.wikipedia.org/wiki/Martingale_(betting_system) I don't think it's based on gambler's fallacy. Suppose I have $1+2+4+8+.....+2^{9}=1023$ Dollars. I'm about to bet on heads in the tosses of a fair coin. If the coin lands heads, I receive an amount double my bet (my original bet + profit equal to my original bet) My strategy will be: 1.Bet 1 Dollar on the first toss. 2.Double my bet on each successive toss. 3.Walk away on my first win. So, all the possible outcomes are: $$H, TH, TTH, TTTH, TTTTH, TTTTTH, TTTTTTH, TTTTTTTH, TTTTTTTTH, TTTTTTTTTH, TTTTTTTTTT$$ I end up making a 1 dollar profit in the first 10 of these outcomes. Only in the last case I lose 1023 dollars. But it would take a miracle for $TTTTTTTTTT$ to happen with a fair coin. I know that the expected value is negative but the expected value comes into play only when I keep on betting till the end of time. My strategy is to walk away on the first win. So, before I begin to bet, isn't it reasonable for me to believe that I'll most likely make a profit?",,"['probability', 'probability-theory']"
11,"Show that $T(X) = (X_{(1)}, X_{(n)})$ is sufficient.",Show that  is sufficient.,"T(X) = (X_{(1)}, X_{(n)})","Exercise: Let $\phi$ be a positive (Borel) function on $\mathbb{R}$ such that $\int\limits_{a}^b\phi(x)dx < \infty$ for a pair $\theta = (a,b)$, with $-\infty<a<b<\infty$. Let $\Omega = \{\theta \in \mathbb{R^2}:a<b\}$. Define $$f(x|\theta) = c(\theta)\phi(x)\mathbb{1}{(a,b)},$$ with $c(\theta)$ such that $\int f(x|\theta)dx = 1$. Then $\{f(\cdot|\theta), \theta\in\Omega\}$ is called a truncation family. Suppose that $X_1,...,X_n \stackrel{iid}{\sim} f(\cdot|\theta)$. Let $X = (X_1,...,X_n)$. Show that $T(X) = (X_{(1)},X_{(n)})$ is a sufficient statistic. Question: How do I show that $T(X)$ is a sufficient statistic? I know that according to the mathematical definition $T(X)$ is sufficient if the distribution of $X$ given $T$ is known (does not depend on $\theta$). However, I think it's quite hard to find the conditional distribution and show that it's not dependent on $\theta$. I also know that $T(X)$ is sufficient if no other statistic that can be calculated from the same sample provides more information regarding $\theta$. So I need to show that $T(X)$ gives us just as much about $\theta$ as $S(X) = (X_1,...,X_n)$ does. Intuitively I feel this can be done by looking at $f(x_1|\theta)$ and $f(x_2|\theta)$ and then inspecting what happens with $\mathbb{1}_{(a,b)}(x)$, but I'm not sure how. Thanks in advance!","Exercise: Let $\phi$ be a positive (Borel) function on $\mathbb{R}$ such that $\int\limits_{a}^b\phi(x)dx < \infty$ for a pair $\theta = (a,b)$, with $-\infty<a<b<\infty$. Let $\Omega = \{\theta \in \mathbb{R^2}:a<b\}$. Define $$f(x|\theta) = c(\theta)\phi(x)\mathbb{1}{(a,b)},$$ with $c(\theta)$ such that $\int f(x|\theta)dx = 1$. Then $\{f(\cdot|\theta), \theta\in\Omega\}$ is called a truncation family. Suppose that $X_1,...,X_n \stackrel{iid}{\sim} f(\cdot|\theta)$. Let $X = (X_1,...,X_n)$. Show that $T(X) = (X_{(1)},X_{(n)})$ is a sufficient statistic. Question: How do I show that $T(X)$ is a sufficient statistic? I know that according to the mathematical definition $T(X)$ is sufficient if the distribution of $X$ given $T$ is known (does not depend on $\theta$). However, I think it's quite hard to find the conditional distribution and show that it's not dependent on $\theta$. I also know that $T(X)$ is sufficient if no other statistic that can be calculated from the same sample provides more information regarding $\theta$. So I need to show that $T(X)$ gives us just as much about $\theta$ as $S(X) = (X_1,...,X_n)$ does. Intuitively I feel this can be done by looking at $f(x_1|\theta)$ and $f(x_2|\theta)$ and then inspecting what happens with $\mathbb{1}_{(a,b)}(x)$, but I'm not sure how. Thanks in advance!",,"['statistics', 'probability-distributions', 'statistical-inference']"
12,Finding an expression for $\prod_{x=1}^n(1-f(x))$,Finding an expression for,\prod_{x=1}^n(1-f(x)),"Question: Say I have $\prod_{x=1}^n(1-f(x))$ where $1 > f(x) > 0$. I am looking to find a way to express this product using $f(x)$. Expanding gives me $1-\sum(\text{odd pairs})+\sum(\text{even pairs})$, however I am not sure how to reduce further. Context: For the case: $$f(x)=\frac{1}{\sqrt{x^2-3x+2a+\frac{1}{4}}}$$ this product gives the probability that $a$ isn't eliminated by any $x<a$, related to my question here . Any advice/pointers to useful topics are appreciated!","Question: Say I have $\prod_{x=1}^n(1-f(x))$ where $1 > f(x) > 0$. I am looking to find a way to express this product using $f(x)$. Expanding gives me $1-\sum(\text{odd pairs})+\sum(\text{even pairs})$, however I am not sure how to reduce further. Context: For the case: $$f(x)=\frac{1}{\sqrt{x^2-3x+2a+\frac{1}{4}}}$$ this product gives the probability that $a$ isn't eliminated by any $x<a$, related to my question here . Any advice/pointers to useful topics are appreciated!",,"['elementary-number-theory', 'statistics']"
13,An honest die is thrown 8 times; let X be the number of twos and let Y be the number of fours. Find the joint pmf of X and Y and calculate P(X=Y).,An honest die is thrown 8 times; let X be the number of twos and let Y be the number of fours. Find the joint pmf of X and Y and calculate P(X=Y).,,"An honest die is thrown 8 times; let $X$ be the number of twos and let $Y$ be the number of fours. Find the joint pmf of $X$ and $Y$ and calculate $\mathbb{P}(X=Y)$. This question is too big to manually draw a table for so I'm having trouble solving it. Can anyone help with the joint pmf? Calculating $\mathbb{P}(X=Y)$ should be straightforward once I have the joint pmf. I know that for either $X$ and $Y$ alone, the pmf will be ${{8}\choose{k}}(1/6)^k(5/6)^{8-k}$","An honest die is thrown 8 times; let $X$ be the number of twos and let $Y$ be the number of fours. Find the joint pmf of $X$ and $Y$ and calculate $\mathbb{P}(X=Y)$. This question is too big to manually draw a table for so I'm having trouble solving it. Can anyone help with the joint pmf? Calculating $\mathbb{P}(X=Y)$ should be straightforward once I have the joint pmf. I know that for either $X$ and $Y$ alone, the pmf will be ${{8}\choose{k}}(1/6)^k(5/6)^{8-k}$",,"['probability', 'statistics', 'dice']"
14,Deriving the maximum likelihood estimator,Deriving the maximum likelihood estimator,,"Suppose $X_1, X_2, X_3 \stackrel{\text{i.i.d.}} \sim \operatorname{Exp}(\theta)$. Exercise: derive the maximum likelihood estimator based on $X = (X_1, X_2, X_3)$. What I've tried: the likelihood is given by $\prod\limits_{i = 1}^3 f(X_i\mid \theta) \, d\theta = \prod\limits_{i = 1}^3 \theta e^{-3\theta x} \, d\theta$. The log-likelihood is given by $\log L = 3\log\theta - 3\theta x \log(e) = 3\log\theta - 3\theta x.$ Take the derivative and set it equal to $0$ and I get $\hat{\theta} = \frac{1}{x}$. My question: How do I derive the maximum likelihood estimator based on $X = (X_1, X_2, X_3)$? I know my solution is probably not correct, but I don't know what else I should try.","Suppose $X_1, X_2, X_3 \stackrel{\text{i.i.d.}} \sim \operatorname{Exp}(\theta)$. Exercise: derive the maximum likelihood estimator based on $X = (X_1, X_2, X_3)$. What I've tried: the likelihood is given by $\prod\limits_{i = 1}^3 f(X_i\mid \theta) \, d\theta = \prod\limits_{i = 1}^3 \theta e^{-3\theta x} \, d\theta$. The log-likelihood is given by $\log L = 3\log\theta - 3\theta x \log(e) = 3\log\theta - 3\theta x.$ Take the derivative and set it equal to $0$ and I get $\hat{\theta} = \frac{1}{x}$. My question: How do I derive the maximum likelihood estimator based on $X = (X_1, X_2, X_3)$? I know my solution is probably not correct, but I don't know what else I should try.",,"['probability', 'probability-theory', 'statistics', 'maximum-likelihood']"
15,Given a random variable $Y=Ab+DX$. How do you compute the covariance of $Y$?,Given a random variable . How do you compute the covariance of ?,Y=Ab+DX Y,"Given a random variable $Y=Ab+DX$ where $X$ is a random variable with mean $\mu$ and covariance matrix $\Sigma$. For a fixed $b$ and $D,A$. How do you compute the covariance of $Y$? I know that $\mu_{Y} = Ab + D\mu$ and $Cov(Y) = E\{(Y-\mu_{Y})(Y-\mu_{Y})^{T}\}$. In my notes I have that $(Y-\mu_{Y}) = D(X-\mu)$ which I don't understand. Shoudln't it be $(Y-\mu_{Y}) = (Y-D(X-\mu))$? Or is it supposed to be $E\{(Y-\mu_{Y})\} = D(X-\mu)$? I also don't understand why $XX^{T} = \Sigma$.","Given a random variable $Y=Ab+DX$ where $X$ is a random variable with mean $\mu$ and covariance matrix $\Sigma$. For a fixed $b$ and $D,A$. How do you compute the covariance of $Y$? I know that $\mu_{Y} = Ab + D\mu$ and $Cov(Y) = E\{(Y-\mu_{Y})(Y-\mu_{Y})^{T}\}$. In my notes I have that $(Y-\mu_{Y}) = D(X-\mu)$ which I don't understand. Shoudln't it be $(Y-\mu_{Y}) = (Y-D(X-\mu))$? Or is it supposed to be $E\{(Y-\mu_{Y})\} = D(X-\mu)$? I also don't understand why $XX^{T} = \Sigma$.",,"['linear-algebra', 'probability', 'statistics', 'random-variables', 'means']"
16,Marginal probability density function of joint pdf,Marginal probability density function of joint pdf,,"Let X has a uniform distribution on the interval (0,1).  Given that X=x, Y has a uniform distribution on the interval (0,x).  Find the marginal p.d.f. of Y. My thought: marginal pdf $f_Y(y)=\int_0^1f(x,y)dx=\int_0^1f_{Y|X=x}(y)f_X(x)dx=\int_0^1\frac{1}{x}dx$.But the last integral goes to negative infinity.What's wrong?Could anyone help?","Let X has a uniform distribution on the interval (0,1).  Given that X=x, Y has a uniform distribution on the interval (0,x).  Find the marginal p.d.f. of Y. My thought: marginal pdf $f_Y(y)=\int_0^1f(x,y)dx=\int_0^1f_{Y|X=x}(y)f_X(x)dx=\int_0^1\frac{1}{x}dx$.But the last integral goes to negative infinity.What's wrong?Could anyone help?",,[]
17,"Min and Max of two Uniform, Independent, Non-identical random variables","Min and Max of two Uniform, Independent, Non-identical random variables",,"Let $X \sim U[0, 1]$ and $Y \sim U[0, 2]$ and assume they are independent. What is the CDF of $J = \min\{X, Y\}$ and $S = \max\{X, Y\}$? My attempted solution for min was $$CDF(J) = 1 - P[J ≥ j] = 1 - P[X \ge j]P[Y \ge j] = 1 - (1-j)(2-j).$$ Obviously this doesn't work since CDF is not non-decreasing from $0 \to 2$. My attempted solution for max was $$CDF(S) = P[S \le s] = P[X \le s]  P[Y \le s] = F_x(s)F_y(s) = (s)(s/2)$$ (As $F_X(x) = x$ and $F_Y(y) = \frac{y}2$). However, this solution does not work since the $CDF(S)$ evaluated as $S=2$, is $2$, which is greater than one.","Let $X \sim U[0, 1]$ and $Y \sim U[0, 2]$ and assume they are independent. What is the CDF of $J = \min\{X, Y\}$ and $S = \max\{X, Y\}$? My attempted solution for min was $$CDF(J) = 1 - P[J ≥ j] = 1 - P[X \ge j]P[Y \ge j] = 1 - (1-j)(2-j).$$ Obviously this doesn't work since CDF is not non-decreasing from $0 \to 2$. My attempted solution for max was $$CDF(S) = P[S \le s] = P[X \le s]  P[Y \le s] = F_x(s)F_y(s) = (s)(s/2)$$ (As $F_X(x) = x$ and $F_Y(y) = \frac{y}2$). However, this solution does not work since the $CDF(S)$ evaluated as $S=2$, is $2$, which is greater than one.",,"['probability', 'probability-theory', 'statistics', 'probability-distributions']"
18,"Finding $cdf$ of the sample minimum, $X_{(1)}$","Finding  of the sample minimum,",cdf X_{(1)},"Consider iid random variables $X_1$ and $X_2$, having $pdf$ $$f_X(x) =  4(1−2x)I_{(0,1/2)}(x)$$ Give the $cdf$ of the sample minimum,   $X_{(1)}$. $$\begin{align*} F_{X(1)}(x)  &= P(X_{(1)} \leq x) \\\\ &= 1 - P(min{\{X_1, X_2}\} \gt x) \\\\ &= 1 - P(X_1 \gt x, X_2 \gt x) \\\\ &= 1 - P(X_1 \gt x)\cdot P(X_2 \gt x) \\\\ &= 1 - [1-F_X(x)]^2 \\\\ &= 1 - [1-\int4(1-2x)]^2 \\\\ &= 1 - [1-(4x-4x^2)]^2 \\\\ \end{align*}$$ Did I do this correctly?","Consider iid random variables $X_1$ and $X_2$, having $pdf$ $$f_X(x) =  4(1−2x)I_{(0,1/2)}(x)$$ Give the $cdf$ of the sample minimum,   $X_{(1)}$. $$\begin{align*} F_{X(1)}(x)  &= P(X_{(1)} \leq x) \\\\ &= 1 - P(min{\{X_1, X_2}\} \gt x) \\\\ &= 1 - P(X_1 \gt x, X_2 \gt x) \\\\ &= 1 - P(X_1 \gt x)\cdot P(X_2 \gt x) \\\\ &= 1 - [1-F_X(x)]^2 \\\\ &= 1 - [1-\int4(1-2x)]^2 \\\\ &= 1 - [1-(4x-4x^2)]^2 \\\\ \end{align*}$$ Did I do this correctly?",,"['probability', 'statistics', 'proof-verification', 'random-variables']"
19,Question about p-value for one-sided hypothesis testing,Question about p-value for one-sided hypothesis testing,,"Is it true that the p-value is symmetric? Let's say for a null hypothesis $H_{0}$: p = 0.75. Given that the z-score test statistics equals 1.44. Using the R command, I found out that pnorm(1.44, lower=F)  ## [1] 0.0749337 I want to test $H_{\alpha}: p > 0.75$. I concluded that p-value is equal to 0.0749337 because by definition, assume H0 is true, then the p-value is the probability that the test statistic z takes a value (in support of $H_{\alpha}$) as or more extreme than the one we observed. For a two-sided hypothesis testing, $H_{0}: p \neq 0.75$, the p-value would be 2*0.0749337. Now, my quesiton is: can I claim that the p-value for $H_{\alpha}: p < 0.75$ is also 0.0749337? Thank you!","Is it true that the p-value is symmetric? Let's say for a null hypothesis $H_{0}$: p = 0.75. Given that the z-score test statistics equals 1.44. Using the R command, I found out that pnorm(1.44, lower=F)  ## [1] 0.0749337 I want to test $H_{\alpha}: p > 0.75$. I concluded that p-value is equal to 0.0749337 because by definition, assume H0 is true, then the p-value is the probability that the test statistic z takes a value (in support of $H_{\alpha}$) as or more extreme than the one we observed. For a two-sided hypothesis testing, $H_{0}: p \neq 0.75$, the p-value would be 2*0.0749337. Now, my quesiton is: can I claim that the p-value for $H_{\alpha}: p < 0.75$ is also 0.0749337? Thank you!",,"['statistics', 'statistical-inference', 'hypothesis-testing']"
20,Proving the Markov inequality for a non-negative random variable,Proving the Markov inequality for a non-negative random variable,,"If U is a non-negative random variable and it has pdf $f_U(u)$, how can we prove the Markov inequality $$E[U]\geq b P(U\geq b),$$ where b is a constant? Not sure how to prove this and haven't really gotten anywhere. Thanks for any help.","If U is a non-negative random variable and it has pdf $f_U(u)$, how can we prove the Markov inequality $$E[U]\geq b P(U\geq b),$$ where b is a constant? Not sure how to prove this and haven't really gotten anywhere. Thanks for any help.",,"['probability-theory', 'statistics', 'random-variables']"
21,Calculate risk (under squared error loss) of MSE estimator of normal variance,Calculate risk (under squared error loss) of MSE estimator of normal variance,,"I am given that $X_i\stackrel{\text{iid}}{\sim}N(\mu,\sigma^2)$. As part of a larger problem, I am to calculate the risk function of $\widehat{\sigma^2}_\text{MSE} = \frac1{n-1}\sum_{i=1}^n(X_i-\bar X)^2$, using the loss function $L(\sigma^2,\widehat{\sigma^2}) = (\sigma^2-\widehat{\sigma^2})^2$. I get stuck just trying to simplify down the horrible expression one gets when applying the definitions of risk and loss functions. Maybe I'm missing some shortcut or something. Any tips would be appreciated. So far I have: \begin{align*} R(\sigma^2,\widehat{\sigma^2}_\text{MSE}) = {} & \mathbb E[(\sigma^2 - \frac1{n-1}\sum_{i=1}^n(X_i-\bar X)^2)^2]\\ = {} & \cdots\\ = {} & \sigma^4 - 2\sigma^2\frac1{n-1} \left( \sum_{i=1}^n \mathbb E(X_i^2-\bar X^2) \right) \\ & {} + \frac1{(n-1)^2}\mathbb E\left[ \sum_{i=1}^n(X_i-\bar X)^4 + 2\sum_{i<j} (X_i-\bar X)^2(X_j-\bar X)^2 \right] \end{align*} Any attempts to expand out that last big term seem to end in disaster.","I am given that $X_i\stackrel{\text{iid}}{\sim}N(\mu,\sigma^2)$. As part of a larger problem, I am to calculate the risk function of $\widehat{\sigma^2}_\text{MSE} = \frac1{n-1}\sum_{i=1}^n(X_i-\bar X)^2$, using the loss function $L(\sigma^2,\widehat{\sigma^2}) = (\sigma^2-\widehat{\sigma^2})^2$. I get stuck just trying to simplify down the horrible expression one gets when applying the definitions of risk and loss functions. Maybe I'm missing some shortcut or something. Any tips would be appreciated. So far I have: \begin{align*} R(\sigma^2,\widehat{\sigma^2}_\text{MSE}) = {} & \mathbb E[(\sigma^2 - \frac1{n-1}\sum_{i=1}^n(X_i-\bar X)^2)^2]\\ = {} & \cdots\\ = {} & \sigma^4 - 2\sigma^2\frac1{n-1} \left( \sum_{i=1}^n \mathbb E(X_i^2-\bar X^2) \right) \\ & {} + \frac1{(n-1)^2}\mathbb E\left[ \sum_{i=1}^n(X_i-\bar X)^4 + 2\sum_{i<j} (X_i-\bar X)^2(X_j-\bar X)^2 \right] \end{align*} Any attempts to expand out that last big term seem to end in disaster.",,"['statistics', 'estimation', 'decision-theory']"
22,Distribution of the sample mean with multiple population definitions,Distribution of the sample mean with multiple population definitions,,"The time taken by a randomly selected applicant for a mortgage to fill out a certain form has a normal distribution with mean value 10 min and standard deviation 2 min. If five individuals fill out a form on one day and six on another, what is the probability that the sample average amount of time taken on each day is at most 11 min? In an attempt to work out the problem, I started by identifying the known variables: $X$: the time taken by a randomly selected applicant for a mortgage to fill out a certain form (that is, one form) $\mu_{X}=10$ $\sigma_{X}=2$ Now, there are obviously more variables but this is where it starts to get a bit confusing for me. In the first part of the problem, $X$ clearly refers to the time it takes to fill out a single form. But in the second part of the problem, the population ($n=5$) fills out a different number of papers on each day, changing the population the question at the end of the problem is addressing. Presumably, there is some other population $Y$ and I'm looking for $P(\overline{Y}\leq11)$. Figuring out what population $Y$ encompasses and how to set up the solution is where I'm stumped. Any nudge in the right direction is greatly appreciated.","The time taken by a randomly selected applicant for a mortgage to fill out a certain form has a normal distribution with mean value 10 min and standard deviation 2 min. If five individuals fill out a form on one day and six on another, what is the probability that the sample average amount of time taken on each day is at most 11 min? In an attempt to work out the problem, I started by identifying the known variables: $X$: the time taken by a randomly selected applicant for a mortgage to fill out a certain form (that is, one form) $\mu_{X}=10$ $\sigma_{X}=2$ Now, there are obviously more variables but this is where it starts to get a bit confusing for me. In the first part of the problem, $X$ clearly refers to the time it takes to fill out a single form. But in the second part of the problem, the population ($n=5$) fills out a different number of papers on each day, changing the population the question at the end of the problem is addressing. Presumably, there is some other population $Y$ and I'm looking for $P(\overline{Y}\leq11)$. Figuring out what population $Y$ encompasses and how to set up the solution is where I'm stumped. Any nudge in the right direction is greatly appreciated.",,"['statistics', 'probability-distributions', 'normal-distribution', 'means']"
23,How to determine a sample size to get accurate estimates of a given data set?,How to determine a sample size to get accurate estimates of a given data set?,,"I have a question with a statistical nature; I think there should be some standard theory about this issue. Suppose I have a large data set of size $N$ items, which has an amount of $K<N$ unwanted items. I am interested in finding the value of $K$. Testing all items takes too much time, so I want to determine a suitable sample size $n<N$ of randomly selected items in the data set. Suppose I just pick a value for $n$ Then, of a randomly sample data of size $n$, I search for the unwanted items of which there are some amount of $k\leq n$. Let this amount be a test statistic $T$, i.e. I will test on the probability $P(T \geq k)$. I can now find a smallest integer value $K_\min$ such that for the estimation $K = K_\min$ we have $P(T \geq k) \geq \alpha$. That is, for any smaller integer estimation $K<K_\min$ we have $P(T \geq k) < \alpha$. If I am correct, I can now state that with a significance level $\alpha$ we have that $K \geq K_\min$. Is that true? If this is true, the question now is: How accurate is this lower bound? This is also my main question. Based on the amount $n$ and accuracy level $\alpha$, what can we say about the accuracy of $K_\min$. In other words, can we determine some confidence interval on $K$ in relationship to $K_\min$ and $\alpha$? Any tips or other approaches are very much appreciated! Best, Koen Edit 26 November: Another formulation of the problem as mentioned by David K is as follows: Given some ""error"" tolerance $\varepsilon$, how do we choose $n$ for a given $\alpha$ such that we can guarantee that $|K_\min−K|/N\leq \varepsilon$ (or some assurance like that)?","I have a question with a statistical nature; I think there should be some standard theory about this issue. Suppose I have a large data set of size $N$ items, which has an amount of $K<N$ unwanted items. I am interested in finding the value of $K$. Testing all items takes too much time, so I want to determine a suitable sample size $n<N$ of randomly selected items in the data set. Suppose I just pick a value for $n$ Then, of a randomly sample data of size $n$, I search for the unwanted items of which there are some amount of $k\leq n$. Let this amount be a test statistic $T$, i.e. I will test on the probability $P(T \geq k)$. I can now find a smallest integer value $K_\min$ such that for the estimation $K = K_\min$ we have $P(T \geq k) \geq \alpha$. That is, for any smaller integer estimation $K<K_\min$ we have $P(T \geq k) < \alpha$. If I am correct, I can now state that with a significance level $\alpha$ we have that $K \geq K_\min$. Is that true? If this is true, the question now is: How accurate is this lower bound? This is also my main question. Based on the amount $n$ and accuracy level $\alpha$, what can we say about the accuracy of $K_\min$. In other words, can we determine some confidence interval on $K$ in relationship to $K_\min$ and $\alpha$? Any tips or other approaches are very much appreciated! Best, Koen Edit 26 November: Another formulation of the problem as mentioned by David K is as follows: Given some ""error"" tolerance $\varepsilon$, how do we choose $n$ for a given $\alpha$ such that we can guarantee that $|K_\min−K|/N\leq \varepsilon$ (or some assurance like that)?",,"['statistics', 'sampling', 'data-analysis', 'sampling-theory']"
24,Two players alternately flip biased coin. What is bias of coin?,Two players alternately flip biased coin. What is bias of coin?,,"Two players, A and B, alternately and independently flip a biased coin and the first player to get a head wins. Assume player A flips first. Player A wins the game 12/23 times. If the coin is biased, what is the bias of the coin? I am using the format from here Two players alternately flip a coin; what is the probability of winning by getting a head? except my equation looks 12/23 = p + (1-p)(11/23)  and solving for p.  I am getting p = 1/12. I am not understanding the answer or if it is correct. If player A is more likely to win and has first flip, why is the chance of getting heads 1/12??","Two players, A and B, alternately and independently flip a biased coin and the first player to get a head wins. Assume player A flips first. Player A wins the game 12/23 times. If the coin is biased, what is the bias of the coin? I am using the format from here Two players alternately flip a coin; what is the probability of winning by getting a head? except my equation looks 12/23 = p + (1-p)(11/23)  and solving for p.  I am getting p = 1/12. I am not understanding the answer or if it is correct. If player A is more likely to win and has first flip, why is the chance of getting heads 1/12??",,"['probability', 'combinatorics', 'statistics']"
25,Integral between max and 2nd max closing up over time?,Integral between max and 2nd max closing up over time?,,"Suppose you repeatedly sample from continuous distribution F with convex support Let's say you drew 2 4 3 5 in order. Denote the biggest number at $t$th sampling by $b(t)$, second biggest number at  $t$th sampling by $a(t)$ So we have $a(4)=4$ $b(4)=5$ My question is whether $$A=\int_{a(t)}^{b(t)}dF(x)$$ will be decreasing, at least in expectation sense, over time. Again, we repeatedly sample from continuous distribution F with convex support. Intuitively, this must be true. Just imagine uniform distribution, then the distance between $b(t)$ and $a(t)$ will likely shrink. But I can't seem to prove mathematically. In fact, I don't even know how to mathematically express the concept of ""second biggest"". How should I even proceed? ============================================== My tentative approach is as follows. If  $b(t+1)>b(t)$, then $a(t+1)=b(t)$ $$A(1)=\int_{}^{b(1)}dF(x)$$ $$A(2)=\int_{b(1)}^{b(2)}dF(x)=F(b(2))-F(b(1))$$  if $b(2)>b(1)$ $$A(t)=\int_{a(t)}^{b(t)}dF(x)$$ $$A(t+1)=\int_{b(t)}^{b(t+1)}dF(x)=F(b(t+1))-F(b(t))$$","Suppose you repeatedly sample from continuous distribution F with convex support Let's say you drew 2 4 3 5 in order. Denote the biggest number at $t$th sampling by $b(t)$, second biggest number at  $t$th sampling by $a(t)$ So we have $a(4)=4$ $b(4)=5$ My question is whether $$A=\int_{a(t)}^{b(t)}dF(x)$$ will be decreasing, at least in expectation sense, over time. Again, we repeatedly sample from continuous distribution F with convex support. Intuitively, this must be true. Just imagine uniform distribution, then the distance between $b(t)$ and $a(t)$ will likely shrink. But I can't seem to prove mathematically. In fact, I don't even know how to mathematically express the concept of ""second biggest"". How should I even proceed? ============================================== My tentative approach is as follows. If  $b(t+1)>b(t)$, then $a(t+1)=b(t)$ $$A(1)=\int_{}^{b(1)}dF(x)$$ $$A(2)=\int_{b(1)}^{b(2)}dF(x)=F(b(2))-F(b(1))$$  if $b(2)>b(1)$ $$A(t)=\int_{a(t)}^{b(t)}dF(x)$$ $$A(t+1)=\int_{b(t)}^{b(t+1)}dF(x)=F(b(t+1))-F(b(t))$$",,"['integration', 'statistics', 'sampling', 'sampling-theory']"
26,Convergence of higher absolute moments given convergence in distribution,Convergence of higher absolute moments given convergence in distribution,,"Assume $i.i.d$ random variables $X_i$ with $E(X_i)=0$ and $E(|X_i|^m)<\infty$ for all $m$. The central limit theorem states that $$ Y_n :=\frac{1}{\sqrt{n}}\sum_{i=1}^n X_i \stackrel{d}{\to} Z  $$ where $Z\sim N(0,1)$.  Let $p>0$. The continous mapping theorem then implies $$|Y_n|^p \stackrel{d}{\to} |Z|^p.$$ I need to show that for any $p>0$. I have, as $n\to \infty$, $$E(|Y_n|^p) \to E(|Z|^p).$$ I know that we need to show uniform integrability, which is implied if the following condition hold $$\sup_{n}E(|Y_n|^{p+\epsilon})< \infty.$$ Does this hold for any $p>0$ in the given $i.i.d.$ case? Note that it is sufficient to check the condition either for even values of $p+\epsilon$ (eliminating the absolute value). For $p<2$ the statement is correct. We have \begin{align*} E(|Y_n|^2)& = E(\frac{1}{n} (\sum_{i=1}^n X_i)^2)\\           & = \frac{1}{n} E( \sum_i X_i^2 + 2 \sum_{i\neq j}X_iX_j )\\           & = E(X_i^2) \end{align*}  since $E(X_iX_j)= E(X_i)E(X_j)=0$ for $p<4$ it should hold too:  $(\sum_{i=1}^n X_i)^4$ will contain $n$ summands of the form $X_i^4$, and $n(n-1)$ summands of the form $X_i^2X_j^2$  for all other summands there exists a $j$ such that the summand contains $X_j^1$ (hence the expectation of these terms will vanish again). It then would follow  $$E(|Y_n|^4) \leq C \frac{n(n-1)}{n^2} < \infty$$ I tried a similar argument for $p=10$ and if I did not made any mistakes by looking at the cases, the largest numbers of terms entering the expectation of $(\sum_{i=1}^n Y_i)^{10}$ should occure when any $5$ of the $n$ variables enter as a product of the form  $X_i^2 X_j^2 X_k^2 X_l^2 X_m^2$ for which there exists $n (n-1)(n-2)(n-3)(n-4) \leq n^5$ possibilities. Hence:  $$E(|Y_n|^{10}) \leq C \frac{n^5}{n^5} < \infty$$ My observations suggest that the claim holds for all $p>0$. And, chosing $p$ as an even number,  the largest number of terms entering the sum seem to be given by a factor of $n(n-1)(n-2) \cdots (n-p/2-1)$.","Assume $i.i.d$ random variables $X_i$ with $E(X_i)=0$ and $E(|X_i|^m)<\infty$ for all $m$. The central limit theorem states that $$ Y_n :=\frac{1}{\sqrt{n}}\sum_{i=1}^n X_i \stackrel{d}{\to} Z  $$ where $Z\sim N(0,1)$.  Let $p>0$. The continous mapping theorem then implies $$|Y_n|^p \stackrel{d}{\to} |Z|^p.$$ I need to show that for any $p>0$. I have, as $n\to \infty$, $$E(|Y_n|^p) \to E(|Z|^p).$$ I know that we need to show uniform integrability, which is implied if the following condition hold $$\sup_{n}E(|Y_n|^{p+\epsilon})< \infty.$$ Does this hold for any $p>0$ in the given $i.i.d.$ case? Note that it is sufficient to check the condition either for even values of $p+\epsilon$ (eliminating the absolute value). For $p<2$ the statement is correct. We have \begin{align*} E(|Y_n|^2)& = E(\frac{1}{n} (\sum_{i=1}^n X_i)^2)\\           & = \frac{1}{n} E( \sum_i X_i^2 + 2 \sum_{i\neq j}X_iX_j )\\           & = E(X_i^2) \end{align*}  since $E(X_iX_j)= E(X_i)E(X_j)=0$ for $p<4$ it should hold too:  $(\sum_{i=1}^n X_i)^4$ will contain $n$ summands of the form $X_i^4$, and $n(n-1)$ summands of the form $X_i^2X_j^2$  for all other summands there exists a $j$ such that the summand contains $X_j^1$ (hence the expectation of these terms will vanish again). It then would follow  $$E(|Y_n|^4) \leq C \frac{n(n-1)}{n^2} < \infty$$ I tried a similar argument for $p=10$ and if I did not made any mistakes by looking at the cases, the largest numbers of terms entering the expectation of $(\sum_{i=1}^n Y_i)^{10}$ should occure when any $5$ of the $n$ variables enter as a product of the form  $X_i^2 X_j^2 X_k^2 X_l^2 X_m^2$ for which there exists $n (n-1)(n-2)(n-3)(n-4) \leq n^5$ possibilities. Hence:  $$E(|Y_n|^{10}) \leq C \frac{n^5}{n^5} < \infty$$ My observations suggest that the claim holds for all $p>0$. And, chosing $p$ as an even number,  the largest number of terms entering the sum seem to be given by a factor of $n(n-1)(n-2) \cdots (n-p/2-1)$.",,"['combinatorics', 'probability-theory', 'statistics', 'central-limit-theorem', 'probability-limit-theorems']"
27,Martingales and random walk,Martingales and random walk,,"Suppose jumps $X_1,X_2,....$ of a simple random walk and i.i.d distributed having beroulli random variables i.e $P(X_1 = 1) = p ; P(X_1 = -1) = 1-p = q$ Random walk is the process ${[Y_n, n\ge0}]$ where $Y_0 = 0$ and $Y_n = Y_{n-1} + X_n  $ I now have to show that the process $Y_n - n(p-q)$ is a martingale with respect to $[X_n, n=1,2,....]$ $$$$I started with the usual show that: $$$$$E[Y_{n+1} - (n+1)(p-q)|X_1, X_2,.....,X_n] = Y_n -n(p-q) $ $$$$and i did this as follows: $$$$$E[Y_{n+1} - (n+1)(p-q)|X_1, X_2,.....,X_n] = E[Y_{n+1}|X_1,...,X_n] -(n+1)(p-q)$ $= E[Y_n + X_{n+1}|X_1,...,X_n] -(n+1)(p-q)$ $=E[Y_n|X_1,...,X_n] + E[X_{n+1}] -(n+1)(p-q)$ $=Y_n +(1*p) + (-1*q) - (n+1)(p-q)$ $=Y_n - n(p-q)$ Which is the correct reslut, but i am not sure that my steps are correct. Mainly I am not 100% sure that $E[Y_n|X_1,...,X_n] $is really equal to $Y_n$. Any help or tips?","Suppose jumps $X_1,X_2,....$ of a simple random walk and i.i.d distributed having beroulli random variables i.e $P(X_1 = 1) = p ; P(X_1 = -1) = 1-p = q$ Random walk is the process ${[Y_n, n\ge0}]$ where $Y_0 = 0$ and $Y_n = Y_{n-1} + X_n  $ I now have to show that the process $Y_n - n(p-q)$ is a martingale with respect to $[X_n, n=1,2,....]$ $$$$I started with the usual show that: $$$$$E[Y_{n+1} - (n+1)(p-q)|X_1, X_2,.....,X_n] = Y_n -n(p-q) $ $$$$and i did this as follows: $$$$$E[Y_{n+1} - (n+1)(p-q)|X_1, X_2,.....,X_n] = E[Y_{n+1}|X_1,...,X_n] -(n+1)(p-q)$ $= E[Y_n + X_{n+1}|X_1,...,X_n] -(n+1)(p-q)$ $=E[Y_n|X_1,...,X_n] + E[X_{n+1}] -(n+1)(p-q)$ $=Y_n +(1*p) + (-1*q) - (n+1)(p-q)$ $=Y_n - n(p-q)$ Which is the correct reslut, but i am not sure that my steps are correct. Mainly I am not 100% sure that $E[Y_n|X_1,...,X_n] $is really equal to $Y_n$. Any help or tips?",,"['probability-theory', 'statistics', 'stochastic-processes', 'martingales', 'random-walk']"
28,"If a complete sufficient statistic exists, is every minimal sufficient statistic complete?","If a complete sufficient statistic exists, is every minimal sufficient statistic complete?",,"Bahadur's theorem says that every bounded complete sufficient statistic is also minimal sufficient. But any minimal sufficient statistic is a one-to-one function of any other minimal sufficient statistic，which implies any minimal sufficient statistic is also a one-to-one function of a bounded complete sufficient statistic. Thus if a bounded complete sufficient statistic exists, then every MSS is a one-to-one function of it, and thus every MSS is also complete. Is this right? I feel it is wrong, but I don't know where the flaw is.","Bahadur's theorem says that every bounded complete sufficient statistic is also minimal sufficient. But any minimal sufficient statistic is a one-to-one function of any other minimal sufficient statistic，which implies any minimal sufficient statistic is also a one-to-one function of a bounded complete sufficient statistic. Thus if a bounded complete sufficient statistic exists, then every MSS is a one-to-one function of it, and thus every MSS is also complete. Is this right? I feel it is wrong, but I don't know where the flaw is.",,"['statistics', 'statistical-inference']"
29,Probabilistic Interpretation of Linear Regression: Why is the hypothesis function considered the mean of random variable y?,Probabilistic Interpretation of Linear Regression: Why is the hypothesis function considered the mean of random variable y?,,"In his machine learning notes (Chapter 3) http://cs229.stanford.edu/notes/cs229-notes1.pdf ,  Andrew Ng glosses over some math. In quantifying the random variable y :  $$y^{(i)}=\theta ^{T}x^{(i)}+e^{(i)}$$ e is conjured to represent random noise/factors not accounted by the hypothesis model. If this is the case why must x also be counted a random variable? Also, why is e assumed to have mean 0? Would not a more general case assume a mean of μ ? Furthermore, I understand the mathematical form of the gaussian distribution, $$\frac{1}{\sqrt{2\pi \sigma }}e^{-\frac{(q-\mu)^{2}}{2\sigma ^{2}}}$$ but I do not understand how one can conclude that the hypothesis function, $$\theta ^{T}x^{(i)}$$ constiutes the mean of the random variable y , therefore implying: $$p(y^{(i)}|x^{(i)};\theta )=\frac{1}{\sqrt{2\pi \sigma }}e^{-\frac{[y^{(i)}-\theta ^{T}x^{(i)}]^{2}}{2\sigma ^{2}}}$$ Thank you for your help in understanding these subtleties.","In his machine learning notes (Chapter 3) http://cs229.stanford.edu/notes/cs229-notes1.pdf ,  Andrew Ng glosses over some math. In quantifying the random variable y :  $$y^{(i)}=\theta ^{T}x^{(i)}+e^{(i)}$$ e is conjured to represent random noise/factors not accounted by the hypothesis model. If this is the case why must x also be counted a random variable? Also, why is e assumed to have mean 0? Would not a more general case assume a mean of μ ? Furthermore, I understand the mathematical form of the gaussian distribution, $$\frac{1}{\sqrt{2\pi \sigma }}e^{-\frac{(q-\mu)^{2}}{2\sigma ^{2}}}$$ but I do not understand how one can conclude that the hypothesis function, $$\theta ^{T}x^{(i)}$$ constiutes the mean of the random variable y , therefore implying: $$p(y^{(i)}|x^{(i)};\theta )=\frac{1}{\sqrt{2\pi \sigma }}e^{-\frac{[y^{(i)}-\theta ^{T}x^{(i)}]^{2}}{2\sigma ^{2}}}$$ Thank you for your help in understanding these subtleties.",,"['statistics', 'machine-learning', 'linear-regression']"
30,"Given $\operatorname{E}[Y\mid X]=1$, then to show that $\operatorname{Var}(XY)>\operatorname{Var}(X)$","Given , then to show that",\operatorname{E}[Y\mid X]=1 \operatorname{Var}(XY)>\operatorname{Var}(X),"Given $\operatorname{E}[Y\mid X]=1$, then to show that $\operatorname{Var}(XY)>\operatorname{Var}(X)$. Tried apply a few inequality such as Schwartz and Jensen, but none seem to be working. If not entire solution some useful inequality would also work as a suggestion","Given $\operatorname{E}[Y\mid X]=1$, then to show that $\operatorname{Var}(XY)>\operatorname{Var}(X)$. Tried apply a few inequality such as Schwartz and Jensen, but none seem to be working. If not entire solution some useful inequality would also work as a suggestion",,"['probability', 'statistics', 'random-variables', 'conditional-expectation']"
31,Card Probability using inclusion exclusion,Card Probability using inclusion exclusion,,"You are dealt $13$ cards from a shuffled deck of $52$ cards. Compute the probability that you get all four cards of at least one denomination (all Aces, or all Kings, or all Queens, . . . , or all Twos). The answer to this is $\frac{{13\choose 1}{48 \choose 9}}{{52 \choose 13}} + \frac{{13\choose 2}{44 \choose 5}}{{52 \choose 13}} + \frac{{13\choose 3}{40\choose 1}}{{52 \choose 13}}$ I'm having a hard time understanding this solution. I think it depicts the probability of choosing all four of a first denomination plus all four of a first and second denomination plus all 4 of a first, second and third denomination, but that would mean that the third fraction should be smaller than the first two, but it isn't Where am I going wrong? Perhaps this is the probability of the first four of a first domination plus four of a second plus four of  a third, but in that case I do not understand why we do ${48\choose 9}$, ${44\choose 5}$ and ${40\choose 1}$ because isn't that populating the rest of our selection of $13$ choices? And if that's the case then why are we populating it $3$ times?","You are dealt $13$ cards from a shuffled deck of $52$ cards. Compute the probability that you get all four cards of at least one denomination (all Aces, or all Kings, or all Queens, . . . , or all Twos). The answer to this is $\frac{{13\choose 1}{48 \choose 9}}{{52 \choose 13}} + \frac{{13\choose 2}{44 \choose 5}}{{52 \choose 13}} + \frac{{13\choose 3}{40\choose 1}}{{52 \choose 13}}$ I'm having a hard time understanding this solution. I think it depicts the probability of choosing all four of a first denomination plus all four of a first and second denomination plus all 4 of a first, second and third denomination, but that would mean that the third fraction should be smaller than the first two, but it isn't Where am I going wrong? Perhaps this is the probability of the first four of a first domination plus four of a second plus four of  a third, but in that case I do not understand why we do ${48\choose 9}$, ${44\choose 5}$ and ${40\choose 1}$ because isn't that populating the rest of our selection of $13$ choices? And if that's the case then why are we populating it $3$ times?",,"['probability', 'statistics', 'inclusion-exclusion']"
32,"Histogram, box plot and probability plot - which is better for assessing normality?","Histogram, box plot and probability plot - which is better for assessing normality?",,"Which method of the three: histogram, box plot and probability plot is best at determining whether a distribution is approximately normally distributed? Why?","Which method of the three: histogram, box plot and probability plot is best at determining whether a distribution is approximately normally distributed? Why?",,"['statistics', 'probability-distributions', 'descriptive-statistics']"
33,Lottery and win with less numbers - Hypergeometric Distribution,Lottery and win with less numbers - Hypergeometric Distribution,,"I got a problem in showing this proof in paper. One of my coworkers insists that using a sequential number in a Brazilian lottery increases the chances of winning the prizes with less odds I pretty certain it's not true but I can't come up with an adequate proof. Here we have a special lottery that works as the following. One must choose 15 numbers from 25. With this ticket, you'll be able to win the biggest prize (hitting the 15 numbers from 25) but you can also win the smaller prize with 14. This continues until 11, where you get the smallest. His theory is that betting with a sequential ticket from 1-15, 2-16 and 3-17 he'll have more chances to win instead of just picking 3 random sequences. The problem gets worse when others coworkers go gambling together. They insist that combining sequences like that in different tickets also increases their chances. I'm thinking about using Hypergeometric distribution with conditional probability but this is not working since they don't have this mathematical background. Thanks!","I got a problem in showing this proof in paper. One of my coworkers insists that using a sequential number in a Brazilian lottery increases the chances of winning the prizes with less odds I pretty certain it's not true but I can't come up with an adequate proof. Here we have a special lottery that works as the following. One must choose 15 numbers from 25. With this ticket, you'll be able to win the biggest prize (hitting the 15 numbers from 25) but you can also win the smaller prize with 14. This continues until 11, where you get the smallest. His theory is that betting with a sequential ticket from 1-15, 2-16 and 3-17 he'll have more chances to win instead of just picking 3 random sequences. The problem gets worse when others coworkers go gambling together. They insist that combining sequences like that in different tickets also increases their chances. I'm thinking about using Hypergeometric distribution with conditional probability but this is not working since they don't have this mathematical background. Thanks!",,"['statistics', 'proof-explanation', 'hypergeometric-function']"
34,Inequality originated from order statistics,Inequality originated from order statistics,,"The following question is given in V.K Rohatgi problem 7.2.4. Let $x_1,x_2,...,x_n$ be real numbers, and let $x_{(n)} = \max(x_1,x_2,...,x_n) $ for $n\geq 2$, and $x_{(1)} = \min(x_1,x_2,...,x_n)$. Show that for any set of real numbers $a_1,a_2,...,a_n$ such that $\sum_{i=1}^n a_i = 0$, the following inequality holds:         $$\left|\sum_{i=1}^n a_i.x_i\right| \leq \frac{1}{2}(x_{(n)} - x_{(1)})\sum_{i=1}^n |a_i|$$","The following question is given in V.K Rohatgi problem 7.2.4. Let $x_1,x_2,...,x_n$ be real numbers, and let $x_{(n)} = \max(x_1,x_2,...,x_n) $ for $n\geq 2$, and $x_{(1)} = \min(x_1,x_2,...,x_n)$. Show that for any set of real numbers $a_1,a_2,...,a_n$ such that $\sum_{i=1}^n a_i = 0$, the following inequality holds:         $$\left|\sum_{i=1}^n a_i.x_i\right| \leq \frac{1}{2}(x_{(n)} - x_{(1)})\sum_{i=1}^n |a_i|$$",,"['statistics', 'inequality', 'order-statistics']"
35,How is the Markov Chain generated by the Metropolis-Hastings Algorithm Ergodic?,How is the Markov Chain generated by the Metropolis-Hastings Algorithm Ergodic?,,"I understand that the Markov Chain generated by the Metropolis-Hastings algorithm satisfies the detailed balance condition, thus implying that the chain has a stationary distribution, $\pi(.)$, say. However, I'm not sure how we can then apply the ergodic theorem to this Markov Chain? The ergodic theorem states that the chain must be positive recurrent, aperiodic and irreducible; but all we know about the Markov Chain generated by the MH algorithm is that it has $\pi(.)$ as its stationary distribution? Thank you for your help!","I understand that the Markov Chain generated by the Metropolis-Hastings algorithm satisfies the detailed balance condition, thus implying that the chain has a stationary distribution, $\pi(.)$, say. However, I'm not sure how we can then apply the ergodic theorem to this Markov Chain? The ergodic theorem states that the chain must be positive recurrent, aperiodic and irreducible; but all we know about the Markov Chain generated by the MH algorithm is that it has $\pi(.)$ as its stationary distribution? Thank you for your help!",,"['statistics', 'markov-chains', 'monte-carlo']"
36,Control Limits (stat),Control Limits (stat),,"An $\bar{x}$ chart is to be established based on standard values: $μ=600, σ=12, n=9.$ The control limits are based on $α$-risk of $0.01$. What are the appropriate control limits? $$\\$$ This is what I have so far: I know the mean/centerline = $600$. I don't know which formula to use to find the control limits: μ $\pm  z_\frac{α}{2}$$(\frac{σ}{\sqrt{n}})$ OR μ $\pm  3(\frac{σ}{\sqrt{n}})$ I would appreciate your help, thanks!","An $\bar{x}$ chart is to be established based on standard values: $μ=600, σ=12, n=9.$ The control limits are based on $α$-risk of $0.01$. What are the appropriate control limits? $$\\$$ This is what I have so far: I know the mean/centerline = $600$. I don't know which formula to use to find the control limits: μ $\pm  z_\frac{α}{2}$$(\frac{σ}{\sqrt{n}})$ OR μ $\pm  3(\frac{σ}{\sqrt{n}})$ I would appreciate your help, thanks!",,['statistics']
37,Proof marginal distribution of multivariate normal with mgf,Proof marginal distribution of multivariate normal with mgf,,"How I can proof that if $\bf{Y}$ is a random vector with distribution   multivariate normal $N_p(\bf{\mu},\bf{\Sigma}$) each $Y_i\sim  N(\mu_i,\sigma_i^2)$. I know that $$M_\bf{Y}(\bf{t})=\exp\{\bf{\mu}'\bf{t}+\frac{1}{2}\bf{t}'\bf{\Sigma}t\}$$ If I take $\bf{t}=(0,\dots,0,1,\dots,0)=t_i$ then $$M_\bf{Y}(\bf{t})=\exp\{\mu_i+\frac{1}{2}t_i^2\sigma_i^2\}=M_{Y_i}(t_i)=M_\bf{Y}(0,\dots,0,1,\dots,0)$$ and $M_{Y_i}(t_i)$ is the moment generating function of a normal random variable so $Y_i\sim N(\mu_i,\sigma_i^2)$. Is it valid? I mean it proofs anything?","How I can proof that if $\bf{Y}$ is a random vector with distribution   multivariate normal $N_p(\bf{\mu},\bf{\Sigma}$) each $Y_i\sim  N(\mu_i,\sigma_i^2)$. I know that $$M_\bf{Y}(\bf{t})=\exp\{\bf{\mu}'\bf{t}+\frac{1}{2}\bf{t}'\bf{\Sigma}t\}$$ If I take $\bf{t}=(0,\dots,0,1,\dots,0)=t_i$ then $$M_\bf{Y}(\bf{t})=\exp\{\mu_i+\frac{1}{2}t_i^2\sigma_i^2\}=M_{Y_i}(t_i)=M_\bf{Y}(0,\dots,0,1,\dots,0)$$ and $M_{Y_i}(t_i)$ is the moment generating function of a normal random variable so $Y_i\sim N(\mu_i,\sigma_i^2)$. Is it valid? I mean it proofs anything?",,"['probability', 'statistics', 'normal-distribution']"
38,Inconsistent interpretations for the second parameter of the Gamma distribution,Inconsistent interpretations for the second parameter of the Gamma distribution,,"The following question is motivated by material from pages 358-364 of Blitzstein and Hwang's Introduction to Probability .  (NB: I have modified the book's notation in various places to make my question clearer.) First, suppose that $\{N_t\}_{t \in \mathbb{R}^+}$ is a family of r.v.s indexed by the set $\mathbb{R}^+$ of positive reals, and that for each $t \in \mathbb{R}^+$, we have $N_t \sim \mathrm{Poisson}(\lambda t)$, where $\lambda$ is an unknown parameter, independent of $t$. Or, to put it in Bayesian terms, we have $$     N_t\mid\lambda \sim \mathrm{Poisson}(\lambda t) $$ Now, take 1 $\mathrm{Gamma}(n_0, t_0)$ as the (conjugate) prior distribution for $\lambda$.  I.e. $$     \lambda \sim \mathrm{Gamma}(n_0, t_0) $$ Starting from these assumptions, the authors derive (p. 362-364) the posterior distribution for $\lambda$ as $$     \lambda \mid (N_t = n) \sim \mathrm{Gamma}(n_0 + n, t_0 + t) $$ Furthermore, authors derive (p. 364) the posterior expectation for $\lambda$ as $$     \operatorname{E}(\lambda \mid N_t = n) = \frac{n_0 + n}{t_0 + t} $$ These results suggest that the first parameter of a $\mathrm{Gamma}$ distribution is akin to a count (of occurrences), and the second one is akin to an length of time . Under this interpretation, the prior distribution $\mathrm{Gamma}(n_0, t_0)$ would be obtained from the total number of occurrences ($n_0$) observed over one or more intervals of time adding up to a total of $t_0$ (time units).  To get the posterior distribution, we update the parameters of the prior's $\mathrm{Gamma}$ by adding a number of occurrences ($n$) observed during a new interval of time of length $t$, so that now we have a total of $n_0 + n$ occurrences observed in a cumulative interval of length $t_0 + t$. On the other hand, on p. 358 the authors note that a $\mathrm{Gamma}(1, \nu)$ distribution is equivalent to an $\mathrm{Exponential}(\nu)$ distribution, defined as the distribution whose PDF is $f(t) = \nu e^{-\nu t}$.  In this case, the second parameter of the $\mathrm{Gamma}$ distribution seems to be behaving like a rate (occurrences per unit time), rather than a length of time. I'm puzzled by these two radically different interpretations of the $\mathrm{Gamma}$ distribution's second parameter (first as a length of time, and later as a rate). Is my reasoning above wrong?  If not, is there some way to unify or rationalize such divergent interpretations? 1 The book uses the convention that $\mathrm{Gamma}(a, b)$ is the distrubtion whose PDF is $f(x) = \frac{(b x)^a e^{-b x}}{x \Gamma(a)},\; x > 0$.","The following question is motivated by material from pages 358-364 of Blitzstein and Hwang's Introduction to Probability .  (NB: I have modified the book's notation in various places to make my question clearer.) First, suppose that $\{N_t\}_{t \in \mathbb{R}^+}$ is a family of r.v.s indexed by the set $\mathbb{R}^+$ of positive reals, and that for each $t \in \mathbb{R}^+$, we have $N_t \sim \mathrm{Poisson}(\lambda t)$, where $\lambda$ is an unknown parameter, independent of $t$. Or, to put it in Bayesian terms, we have $$     N_t\mid\lambda \sim \mathrm{Poisson}(\lambda t) $$ Now, take 1 $\mathrm{Gamma}(n_0, t_0)$ as the (conjugate) prior distribution for $\lambda$.  I.e. $$     \lambda \sim \mathrm{Gamma}(n_0, t_0) $$ Starting from these assumptions, the authors derive (p. 362-364) the posterior distribution for $\lambda$ as $$     \lambda \mid (N_t = n) \sim \mathrm{Gamma}(n_0 + n, t_0 + t) $$ Furthermore, authors derive (p. 364) the posterior expectation for $\lambda$ as $$     \operatorname{E}(\lambda \mid N_t = n) = \frac{n_0 + n}{t_0 + t} $$ These results suggest that the first parameter of a $\mathrm{Gamma}$ distribution is akin to a count (of occurrences), and the second one is akin to an length of time . Under this interpretation, the prior distribution $\mathrm{Gamma}(n_0, t_0)$ would be obtained from the total number of occurrences ($n_0$) observed over one or more intervals of time adding up to a total of $t_0$ (time units).  To get the posterior distribution, we update the parameters of the prior's $\mathrm{Gamma}$ by adding a number of occurrences ($n$) observed during a new interval of time of length $t$, so that now we have a total of $n_0 + n$ occurrences observed in a cumulative interval of length $t_0 + t$. On the other hand, on p. 358 the authors note that a $\mathrm{Gamma}(1, \nu)$ distribution is equivalent to an $\mathrm{Exponential}(\nu)$ distribution, defined as the distribution whose PDF is $f(t) = \nu e^{-\nu t}$.  In this case, the second parameter of the $\mathrm{Gamma}$ distribution seems to be behaving like a rate (occurrences per unit time), rather than a length of time. I'm puzzled by these two radically different interpretations of the $\mathrm{Gamma}$ distribution's second parameter (first as a length of time, and later as a rate). Is my reasoning above wrong?  If not, is there some way to unify or rationalize such divergent interpretations? 1 The book uses the convention that $\mathrm{Gamma}(a, b)$ is the distrubtion whose PDF is $f(x) = \frac{(b x)^a e^{-b x}}{x \Gamma(a)},\; x > 0$.",,"['probability', 'statistics', 'probability-distributions', 'poisson-process', 'gamma-distribution']"
39,Is it possible to find dependence between $X$ and $Y$ from distribution of $X-Y$?,Is it possible to find dependence between  and  from distribution of ?,X Y X-Y,"Consider $X,Y \sim \mathcal N(0,1)$, I am exploring all the possible ways to find dependence. Correlation, Eye Balling from scatter plot are I think obvious methods. I am trying to think in a different direction just to have a better understanding. So, When $Y$ is dependent on $X$ what would $X-Y$ distribution look like? From what I am aware of $X-Y \sim \mathcal N(0,2)$ when $X$ and $Y$ are independent. Consider a distribution show in the graph below, I want to comment on dependence just from the graph dist If this is not possible how can joint probability tell be about dependence.","Consider $X,Y \sim \mathcal N(0,1)$, I am exploring all the possible ways to find dependence. Correlation, Eye Balling from scatter plot are I think obvious methods. I am trying to think in a different direction just to have a better understanding. So, When $Y$ is dependent on $X$ what would $X-Y$ distribution look like? From what I am aware of $X-Y \sim \mathcal N(0,2)$ when $X$ and $Y$ are independent. Consider a distribution show in the graph below, I want to comment on dependence just from the graph dist If this is not possible how can joint probability tell be about dependence.",,"['statistics', 'probability-distributions', 'normal-distribution', 'independence']"
40,descriptive statistics as generative models,descriptive statistics as generative models,,"It is common to distinguish between descriptive and inferential statistics. However, it seems to me that any descriptive statistic can be immediately used to generate realizations of the data. In the simplest (often uncomputable) case this can be done through uniform sampling of all possible data samples which exactly match the value of the statistic. Such a simple approach, while reminiscent of Monte Carlo simulations or maximum-entropy models, actually has no parameters or mechanistic specifications (at least not in an explicitly defined sense). But it seems that this above approach is still a generative model of the data -- albeit one for which it is hard, or may be impossible, to compute the likelihood. Is this correct? And if yes, does this imply that there is no fundamental distinction between descriptive and inferential statistics? Here is a simple example. Suppose I have a data sample of $m$ real numbers, such that each number is in the range of $[0, b]$, where $m$ and $b$ represent some known constraints of the system in question. I also know that the mean of my data sample is $\bar{x}$, but I don't know anything else about the distribution of these numbers. In other words, I can make no assumptions that the numbers are distributed uniformly or in any other way. I can construct a generative model which samples uniformly all sets $S=\{a_1, a_2, ..., a_m\}$, such that all $a_i \in [0, b]$ and $\frac{1}{m}\sum_{a_i \in S} a_i = \bar{x}$. This model will, by definition, match the mean of my data but may otherwise be a poor representation of its distribution. Now suppose I gradually acquire additional arbitrary descriptive statistics about my data (e.g. the first quartile, the kurtosis, or any other statistic). I can incorporate these statistics into my generative model in exactly the same way as above (i.e. through uniform sampling of all sets that match these statistics). With more descriptive statistics, my model becomes a more accurate representation of the data. Here is again my question. It seems that the descriptive statistics (and the initial constraints of the system) allow me to construct an arbitrarily precise generative model of my data without performing any inference . Is this correct? And if yes, does this imply that there is no fundamental distinction between descriptive and inferential statistics? Any pointers to the relevant literature would also be greatly appreciated.","It is common to distinguish between descriptive and inferential statistics. However, it seems to me that any descriptive statistic can be immediately used to generate realizations of the data. In the simplest (often uncomputable) case this can be done through uniform sampling of all possible data samples which exactly match the value of the statistic. Such a simple approach, while reminiscent of Monte Carlo simulations or maximum-entropy models, actually has no parameters or mechanistic specifications (at least not in an explicitly defined sense). But it seems that this above approach is still a generative model of the data -- albeit one for which it is hard, or may be impossible, to compute the likelihood. Is this correct? And if yes, does this imply that there is no fundamental distinction between descriptive and inferential statistics? Here is a simple example. Suppose I have a data sample of $m$ real numbers, such that each number is in the range of $[0, b]$, where $m$ and $b$ represent some known constraints of the system in question. I also know that the mean of my data sample is $\bar{x}$, but I don't know anything else about the distribution of these numbers. In other words, I can make no assumptions that the numbers are distributed uniformly or in any other way. I can construct a generative model which samples uniformly all sets $S=\{a_1, a_2, ..., a_m\}$, such that all $a_i \in [0, b]$ and $\frac{1}{m}\sum_{a_i \in S} a_i = \bar{x}$. This model will, by definition, match the mean of my data but may otherwise be a poor representation of its distribution. Now suppose I gradually acquire additional arbitrary descriptive statistics about my data (e.g. the first quartile, the kurtosis, or any other statistic). I can incorporate these statistics into my generative model in exactly the same way as above (i.e. through uniform sampling of all sets that match these statistics). With more descriptive statistics, my model becomes a more accurate representation of the data. Here is again my question. It seems that the descriptive statistics (and the initial constraints of the system) allow me to construct an arbitrarily precise generative model of my data without performing any inference . Is this correct? And if yes, does this imply that there is no fundamental distinction between descriptive and inferential statistics? Any pointers to the relevant literature would also be greatly appreciated.",,"['probability', 'statistics', 'probability-distributions', 'statistical-inference', 'descriptive-statistics']"
41,Number of samples needed before sample maximum greater than some value,Number of samples needed before sample maximum greater than some value,,"Let's say you have a standard normal distribution, and you are sampling from this $N$ times. How many samples will it take before the maximum observed value will be at least 3 (or in general some value $K$)? To solve this problem I considered the CDF of the normal distribution for when $x >= 3 $. This gives that the probability of finding a value of $x >= 3$ in one sample is $0.0013499$. Since we know all our samples are independent of each other, the answer would appear to be the mean of the geometric distribution that results with $p=0.0013499$, which is $740.97$. However, by simulating a large number of trials I found that the true answer is around 444 trials. (Here's the mathematica code to show this Table[Table[RandomVariate[NormalDistribution[]], {x, 1, 444}] //     Max, {k, 1, 1000}] // Mean This can also be verified mathematically by solving the reverse problem: the expected sample maximum from $N$ trials. Note that $[Pr(x <= K)]^{444}$ — the probability that the results from all 444 trials are less than k — constitutes a CDF for all 444 trials. From this the corresponding PDF (albeit in terms of Erf function) can be found by differentiating, and finding the expected value of this PDF (or letting mathematica approximate the integral numerically) indeed gives that 444 trials is sufficient to have an expected sample maximum of 3. So why did my attempt to solve the problem overshoot the answer?","Let's say you have a standard normal distribution, and you are sampling from this $N$ times. How many samples will it take before the maximum observed value will be at least 3 (or in general some value $K$)? To solve this problem I considered the CDF of the normal distribution for when $x >= 3 $. This gives that the probability of finding a value of $x >= 3$ in one sample is $0.0013499$. Since we know all our samples are independent of each other, the answer would appear to be the mean of the geometric distribution that results with $p=0.0013499$, which is $740.97$. However, by simulating a large number of trials I found that the true answer is around 444 trials. (Here's the mathematica code to show this Table[Table[RandomVariate[NormalDistribution[]], {x, 1, 444}] //     Max, {k, 1, 1000}] // Mean This can also be verified mathematically by solving the reverse problem: the expected sample maximum from $N$ trials. Note that $[Pr(x <= K)]^{444}$ — the probability that the results from all 444 trials are less than k — constitutes a CDF for all 444 trials. From this the corresponding PDF (albeit in terms of Erf function) can be found by differentiating, and finding the expected value of this PDF (or letting mathematica approximate the integral numerically) indeed gives that 444 trials is sufficient to have an expected sample maximum of 3. So why did my attempt to solve the problem overshoot the answer?",,"['probability', 'statistics', 'density-function']"
42,"In a multivariate normal model, what are the consequences of having a large marginal variance (diagonal element) for each random variable?","In a multivariate normal model, what are the consequences of having a large marginal variance (diagonal element) for each random variable?",,"Suppose that we have that $X_1, \ldots, X_n \sim N(\mathbf{0}, \mathbf{\Sigma})$ where $\mathbf{0}$ is an $n \times 1$ vector and $\mathbf{\Sigma}$ is a covariance matrix. Suppose that we let the diagonal elements of $\mathbf{\Sigma}$ be large, like $25$, while keeping the rest of the off-diagonals some correlation parameter $\rho$. I am wondering what the difference is between having a large diagonal element in $\mathbf{\Sigma}$ vs having a smaller diagonal element, like $1$ in the diagonals instead. It seems asymptotically it might lead to slower convergence when calculating the MLE and associated estimators. However, are there any other reasons why usually people don't have a large variance element and like to keep it at $1$?","Suppose that we have that $X_1, \ldots, X_n \sim N(\mathbf{0}, \mathbf{\Sigma})$ where $\mathbf{0}$ is an $n \times 1$ vector and $\mathbf{\Sigma}$ is a covariance matrix. Suppose that we let the diagonal elements of $\mathbf{\Sigma}$ be large, like $25$, while keeping the rest of the off-diagonals some correlation parameter $\rho$. I am wondering what the difference is between having a large diagonal element in $\mathbf{\Sigma}$ vs having a smaller diagonal element, like $1$ in the diagonals instead. It seems asymptotically it might lead to slower convergence when calculating the MLE and associated estimators. However, are there any other reasons why usually people don't have a large variance element and like to keep it at $1$?",,"['probability', 'probability-theory', 'statistics']"
43,find minimum variance unbiased estimator,find minimum variance unbiased estimator,,"Question Suppose that the random variables $Y_1,\dots,Y_n$ are such that $\quad E(Y_j)=\mu, var(Y_j) = \sigma^2, cov(Y_j,Y_k) = \rho \sigma_j\sigma_k$ where $\mu$ is unknown and the $\sigma_j^2$'s and $\rho$ are known. Consider the linear combination estimator $Y_L$ of the $Y_j$'s giving an unbiased estimator of $\mu$. Find its minimum variance and the optimal weight. Here is my idea : Let $Y_L = \sum_{j=1}^n a_j Y_j$, and since $Y_L$ is unbiased of $\mu$, we find $E(\sum_{j=1}^n a_j Y_j) = \sum_{j=1}^n \mu = \mu$, i.e., $\quad\sum_{j=1}^n a_j = 1$ and the variance of $Y_L$ is $\quad var(Y_L) =var(\sum_{j=1}^n a_jY_j) = \sum_{j=1}^n a_j^2\sigma_j^2 + \sum_{j\neq i}\rho a_ia_j\sigma_i\sigma_j$ and then define the Lagrangian $L = \sum_{j=1}^n a_j^2\sigma_j^2 + \sum_{j\neq i}\rho a_ia_j\sigma_i\sigma_j - \lambda(\sum_{j=1}^n a_j - 1)$ use the usual way to find the minimum variance. However, I don't know how to solve this Lagrangian. Any suggestion would be appreciated.","Question Suppose that the random variables $Y_1,\dots,Y_n$ are such that $\quad E(Y_j)=\mu, var(Y_j) = \sigma^2, cov(Y_j,Y_k) = \rho \sigma_j\sigma_k$ where $\mu$ is unknown and the $\sigma_j^2$'s and $\rho$ are known. Consider the linear combination estimator $Y_L$ of the $Y_j$'s giving an unbiased estimator of $\mu$. Find its minimum variance and the optimal weight. Here is my idea : Let $Y_L = \sum_{j=1}^n a_j Y_j$, and since $Y_L$ is unbiased of $\mu$, we find $E(\sum_{j=1}^n a_j Y_j) = \sum_{j=1}^n \mu = \mu$, i.e., $\quad\sum_{j=1}^n a_j = 1$ and the variance of $Y_L$ is $\quad var(Y_L) =var(\sum_{j=1}^n a_jY_j) = \sum_{j=1}^n a_j^2\sigma_j^2 + \sum_{j\neq i}\rho a_ia_j\sigma_i\sigma_j$ and then define the Lagrangian $L = \sum_{j=1}^n a_j^2\sigma_j^2 + \sum_{j\neq i}\rho a_ia_j\sigma_i\sigma_j - \lambda(\sum_{j=1}^n a_j - 1)$ use the usual way to find the minimum variance. However, I don't know how to solve this Lagrangian. Any suggestion would be appreciated.",,"['linear-algebra', 'statistics', 'linear-programming']"
44,"Why $x^Tx>x^Tz$ with high probability, for independent standard normal high dimensional vectors $x$ and $z$","Why  with high probability, for independent standard normal high dimensional vectors  and",x^Tx>x^Tz x z,"Let's say we sample two random vectors from the same multivariate Gaussian distribution $N(0, I)$. We are interested in the relationship between $x^Tx$ and $x^Tz$. At first, I thought this question is purely wasting time because, for a sampled vector $x$, we can easily construct different $z$ that can be legally sampled, to make any of the three signs ($>$, $<$ or $=$) holds. However, my simulation surprised me. For millions of random seeds with the vector length equals to 100, $x^Tx>x^Tz$. It looks like this is a guaranteed behavior while we all know it's not. I wonder if there could be any explanation from the statistics point of view, like something stating the unlikeliness of sampling out a $z$ that can make $x^Tx<x^Tz$? Please forgive me if you think this question is silly. But try it with your favorite programming language, you may come back to upvote me.","Let's say we sample two random vectors from the same multivariate Gaussian distribution $N(0, I)$. We are interested in the relationship between $x^Tx$ and $x^Tz$. At first, I thought this question is purely wasting time because, for a sampled vector $x$, we can easily construct different $z$ that can be legally sampled, to make any of the three signs ($>$, $<$ or $=$) holds. However, my simulation surprised me. For millions of random seeds with the vector length equals to 100, $x^Tx>x^Tz$. It looks like this is a guaranteed behavior while we all know it's not. I wonder if there could be any explanation from the statistics point of view, like something stating the unlikeliness of sampling out a $z$ that can make $x^Tx<x^Tz$? Please forgive me if you think this question is silly. But try it with your favorite programming language, you may come back to upvote me.",,['probability-theory']
45,Variance of Chi Square Distribution as the Sum of Unit Normal Random Variables,Variance of Chi Square Distribution as the Sum of Unit Normal Random Variables,,"Okay, so I am interested if there is a way to derive the variance for a Chi-Square distribution using the property that it is the sum of independent unit normal distributions squared. For example, if $X$ is a Chi-Square random variable with $n$ degrees of freedom, it has the distribution: $\displaystyle\operatorname{}\left(\sum_{i=1}^n Z_i^2\right)$ where $Z$ is Normal$(0,1)$ I know that $Var(X)= E(X^{2}) -[E(X)]^{2}$ To start finding $E(X)$ I begin with the fact that each $Z$ has $E(Z)=0$ and $Var(Z)=1$. This implies that $E(Z^2)=1$ since $Var(Z)=E(Z^{2})-[E(Z)]^2$ Since $X = \displaystyle\operatorname{}\left(\sum_{i=1}^n Z_i^2\right)$ then $E(X)=\displaystyle\operatorname{}\left(\sum_{i=1}^n 1\right)=n$ I am lost on what the next step would be. I have this start, but don't where to go next. Any thoughts on how to find $E(X^{2})$? $\begin{align} X^2 =& (\sum_{i=1}^nZ_i^2)^2\\ =& \sum_{i=1}^nZ_i^4+\sum_{i \neq j}^nZ_i^2Z_j^2 \end{align}$ I can't see any way this sum not getting nasty. Note: I can solve this using integration of the PDF for the Chi-Square distribution, but I was wondering if there is any way to do it using the property that Chi-Squared is sum of Squared Normal.","Okay, so I am interested if there is a way to derive the variance for a Chi-Square distribution using the property that it is the sum of independent unit normal distributions squared. For example, if $X$ is a Chi-Square random variable with $n$ degrees of freedom, it has the distribution: $\displaystyle\operatorname{}\left(\sum_{i=1}^n Z_i^2\right)$ where $Z$ is Normal$(0,1)$ I know that $Var(X)= E(X^{2}) -[E(X)]^{2}$ To start finding $E(X)$ I begin with the fact that each $Z$ has $E(Z)=0$ and $Var(Z)=1$. This implies that $E(Z^2)=1$ since $Var(Z)=E(Z^{2})-[E(Z)]^2$ Since $X = \displaystyle\operatorname{}\left(\sum_{i=1}^n Z_i^2\right)$ then $E(X)=\displaystyle\operatorname{}\left(\sum_{i=1}^n 1\right)=n$ I am lost on what the next step would be. I have this start, but don't where to go next. Any thoughts on how to find $E(X^{2})$? $\begin{align} X^2 =& (\sum_{i=1}^nZ_i^2)^2\\ =& \sum_{i=1}^nZ_i^4+\sum_{i \neq j}^nZ_i^2Z_j^2 \end{align}$ I can't see any way this sum not getting nasty. Note: I can solve this using integration of the PDF for the Chi-Square distribution, but I was wondering if there is any way to do it using the property that Chi-Squared is sum of Squared Normal.",,"['probability', 'statistics', 'probability-distributions', 'random-variables', 'covariance']"
46,"Linear interpolation to find the median confusion - two methods, two different answers","Linear interpolation to find the median confusion - two methods, two different answers",,"I am given the following in a worked example (see picture). That is the method used to find the median by interpolation. However, they say the fractions $\frac{a}{b}$ and $\frac{c}{d}$ are equivalent. I thus tried to use $$\frac{b-a}{b} = \frac{d-c}{d}$$ to find the median, but this gave a different answer: $$\frac{36.5-m}{3} = \frac{22}{20}\Rightarrow m = 33.2$$ Where am I going wrong ? What is the issue?","I am given the following in a worked example (see picture). That is the method used to find the median by interpolation. However, they say the fractions $\frac{a}{b}$ and $\frac{c}{d}$ are equivalent. I thus tried to use $$\frac{b-a}{b} = \frac{d-c}{d}$$ to find the median, but this gave a different answer: $$\frac{36.5-m}{3} = \frac{22}{20}\Rightarrow m = 33.2$$ Where am I going wrong ? What is the issue?",,"['statistics', 'interpolation', 'median']"
47,What is a population minimizer?,What is a population minimizer?,,"I am reading into statistics in combination with machine learning and I came across the expression ""population minimizer"". I found no good explanation on what a population minimizer exactly means, so what exactly is it?","I am reading into statistics in combination with machine learning and I came across the expression ""population minimizer"". I found no good explanation on what a population minimizer exactly means, so what exactly is it?",,['statistics']
48,Two approaches to a probability/combinatorics problem,Two approaches to a probability/combinatorics problem,,"The statement: A city with $6$ districts has $6$ robberies in a particular week. Assume the robberies are located randomly, with all possibilities for which robbery occurred where equally likely. What is the probability that some district had more than $1$ robbery? The answer: There are $6^6$ possible configurations for which robbery occurred where. There are $6!$ configurations where each district had exactly $1$ of the $6$, so the probability of the complement of the desired event is $6!/6^6$. So the probability of some district having more than $1$ robbery is $1 - 6!/6^6$. The problem: I was thinking about this problem and found another approach. Of course, I know this approach is incorrect, but I can't find why . The number of ways to distribute k balls into n boxes is: $\binom{n + k - 1}{k}$. So I thought that the boxes could be the districts, and the balls could be the robberies. So there is $\binom{6 + 6 - 1}{6} = \binom{11}{6}$ different configuration of robberies, out of which, just one is the configuration in which every bank has exactly one robbery. The problem is that $1 - 1 / \binom{11}{6}$ is not equal to $1 - 6! / 6^6$. What is wrong with this reasoning?","The statement: A city with $6$ districts has $6$ robberies in a particular week. Assume the robberies are located randomly, with all possibilities for which robbery occurred where equally likely. What is the probability that some district had more than $1$ robbery? The answer: There are $6^6$ possible configurations for which robbery occurred where. There are $6!$ configurations where each district had exactly $1$ of the $6$, so the probability of the complement of the desired event is $6!/6^6$. So the probability of some district having more than $1$ robbery is $1 - 6!/6^6$. The problem: I was thinking about this problem and found another approach. Of course, I know this approach is incorrect, but I can't find why . The number of ways to distribute k balls into n boxes is: $\binom{n + k - 1}{k}$. So I thought that the boxes could be the districts, and the balls could be the robberies. So there is $\binom{6 + 6 - 1}{6} = \binom{11}{6}$ different configuration of robberies, out of which, just one is the configuration in which every bank has exactly one robbery. The problem is that $1 - 1 / \binom{11}{6}$ is not equal to $1 - 6! / 6^6$. What is wrong with this reasoning?",,"['probability', 'combinatorics', 'statistics']"
49,"Given a joint characteristic function, find $P(X<Y)$","Given a joint characteristic function, find",P(X<Y),"This question was asked in here before for a given MGF with discrete r.v.. A partial solution for the generalized version of the problem was given in this link by expiTTp1z0. But that is not nearly enough. Question: A joint characteristic function of $(X,Y)$ is given, find $P(X<Y)$ .","This question was asked in here before for a given MGF with discrete r.v.. A partial solution for the generalized version of the problem was given in this link by expiTTp1z0. But that is not nearly enough. Question: A joint characteristic function of is given, find .","(X,Y) P(X<Y)","['probability-theory', 'statistics']"
50,Probability/ statistics bracket notation?,Probability/ statistics bracket notation?,,"Say $X(t)$ is a random variable for each $t \in \mathbb R$. In various papers I've seen the notation $$\langle X(t') X(t'+t)\rangle = C(t)$$ where $C(t)$ is referred to as the correlation function, $$\langle X(t) X(t)\rangle = C(0) \equiv M^2$$ where $M$ is referred to as the typical magnitude, and $$\langle f(X(t))\rangle$$ (where $f$ is some function, say $\exp(\int X(t) dt)$) referred to as an ""ensemble average."" What does the bracket notation $\langle \rangle$ mean? How is it consistent? Could you direct me to a book/ source that explains this?","Say $X(t)$ is a random variable for each $t \in \mathbb R$. In various papers I've seen the notation $$\langle X(t') X(t'+t)\rangle = C(t)$$ where $C(t)$ is referred to as the correlation function, $$\langle X(t) X(t)\rangle = C(0) \equiv M^2$$ where $M$ is referred to as the typical magnitude, and $$\langle f(X(t))\rangle$$ (where $f$ is some function, say $\exp(\int X(t) dt)$) referred to as an ""ensemble average."" What does the bracket notation $\langle \rangle$ mean? How is it consistent? Could you direct me to a book/ source that explains this?",,"['probability', 'statistics']"
51,Calculate Quarter profit probability if you know daily average and standard deviation,Calculate Quarter profit probability if you know daily average and standard deviation,,"A company's daily profit is on average is €$100,000$ And the standard deviation is  €$10,000$. What is the probability of the company in Quarter to show a profit of more than € $9.2$ million? The Quarter average will be $100,000 * 90=9,000,000$ What about the standard deviation it will be $10,000*8=90$ or something else.","A company's daily profit is on average is €$100,000$ And the standard deviation is  €$10,000$. What is the probability of the company in Quarter to show a profit of more than € $9.2$ million? The Quarter average will be $100,000 * 90=9,000,000$ What about the standard deviation it will be $10,000*8=90$ or something else.",,"['probability', 'statistics', 'standard-deviation']"
52,Maximum Likelihood Estimate With Factorial,Maximum Likelihood Estimate With Factorial,,How do you find the maximum likelihood estimate of this function? $$P_x(k;\theta) = \frac{\theta^{2k} e^{-\theta^2}}{k!}$$ I'm really just having trouble with the factorial part... really not sure how to approach this problem. Thanks,How do you find the maximum likelihood estimate of this function? $$P_x(k;\theta) = \frac{\theta^{2k} e^{-\theta^2}}{k!}$$ I'm really just having trouble with the factorial part... really not sure how to approach this problem. Thanks,,"['statistics', 'maximum-likelihood']"
53,Example of a Covariance Matrix?,Example of a Covariance Matrix?,,"Can someone provide an example of a covariance matrix for any set of data? For example, if given: 2 3 4 5 1 8 9 7 6 how would I take this 3x3 matrix and convert it to the covariance matrix? I see the formula involves taking the means, but I'm not quite sure how that works in this case...","Can someone provide an example of a covariance matrix for any set of data? For example, if given: 2 3 4 5 1 8 9 7 6 how would I take this 3x3 matrix and convert it to the covariance matrix? I see the formula involves taking the means, but I'm not quite sure how that works in this case...",,"['matrices', 'statistics', 'covariance']"
54,Bernoulli estimation under different parameter representation,Bernoulli estimation under different parameter representation,,"Suppose that $Y_1,\ldots,Y_n\stackrel{\text{iid}}{\sim} \text{Ber}(p)$. I would like to estimate $c = g(p) = (1-p)/p$. Naively one can derive  $$\tilde{c}_\text{mle} = g(\tilde{p}_\text{mle}) = g\left(\frac{1}{n} \sum_i Y_i \right) = \frac{n-\sum_i Y_i}{\sum_i Y_i}$$ and in order for that to make any sense I set $$\tilde{c} = \mathbf{1}_{\{\sum_i Y_i > 0\}}\frac{n-\sum_i Y_i}{\sum_i Y_i}.$$ Now I'm interested in bounding $\mathbb{E}|c-\tilde{c}|$ from above, but things like Hoeffding's Inequality don't seem to work. One other thing I was thinking about was to write out $\mathbb{E}|c-\tilde{c}|$ as a sum since $\tilde{c}$ can only be a finite amount of distinct values but I don't see this expression making it easier. Another difficulty is that $g'(p)$ isn't bounded, so approaches similar to the Delta Method won't work as well. I thought there would be a bit more literature about such issues, but I haven't managed to find any of it. Is anyone familiar with such problems? Pointers are also appreciated!","Suppose that $Y_1,\ldots,Y_n\stackrel{\text{iid}}{\sim} \text{Ber}(p)$. I would like to estimate $c = g(p) = (1-p)/p$. Naively one can derive  $$\tilde{c}_\text{mle} = g(\tilde{p}_\text{mle}) = g\left(\frac{1}{n} \sum_i Y_i \right) = \frac{n-\sum_i Y_i}{\sum_i Y_i}$$ and in order for that to make any sense I set $$\tilde{c} = \mathbf{1}_{\{\sum_i Y_i > 0\}}\frac{n-\sum_i Y_i}{\sum_i Y_i}.$$ Now I'm interested in bounding $\mathbb{E}|c-\tilde{c}|$ from above, but things like Hoeffding's Inequality don't seem to work. One other thing I was thinking about was to write out $\mathbb{E}|c-\tilde{c}|$ as a sum since $\tilde{c}$ can only be a finite amount of distinct values but I don't see this expression making it easier. Another difficulty is that $g'(p)$ isn't bounded, so approaches similar to the Delta Method won't work as well. I thought there would be a bit more literature about such issues, but I haven't managed to find any of it. Is anyone familiar with such problems? Pointers are also appreciated!",,"['statistics', 'probability-distributions', 'parameter-estimation']"
55,Sending one broken taxi each to three different airports.,Sending one broken taxi each to three different airports.,,"Suppose we have $9$ taxis, and three airports, aiport $A$, airport $B$, and airport $C$. We want to send $3$ taxis to airport $A$, $5$ taxis to airport $B$, and $1$ taxi to airport $C$. If exactly three of the taxis are in need of repair, what is the probability   that every airport receives one of the taxis requiring repairs? I'm struggling to solve this problem. There is a theorem that I want to use, listed below: THEOREM : The number of ways of partitioning $n$ distinct objects into $k$ distinct groups containing $n_1, n_2, \ldots , n_k$ objects, respectively, where each object appears in exactly one group and $\sum_{i=1}^k n_i = n$, is $$N = {n\choose n_1 \;n_2 \;\cdots\; n_k} = {n!\over n_1!\;n_2!\;\cdots\;n_k!}.$$ The following is a solution I found here on page 83 for this problem. SOLUTION : Using the theorem, there are $${9\choose 3\;5\;1} = {9!\over 3!\;5!\;1!} = 504\text{ ways}$$ to send the taxis to all the airports. Let $W$ denote the event that the $3$ taxis that are in need of repair are sent to each airport. The number of ways we can send each of the $3$ broken taxis to the three airports is given by $${3\choose 1\;1\;1} = 3! = 6. \tag{Why is this important?}$$ Also, the number of ways we can send the remaining $6$ taxis to the three airports is given by $${6\choose 2\;4\;0} = {6!\over2!\;4!} = 15$$ (since each of the three broken taxis are already taking up a spot). So, by the $mn$-rule, the number of points in the sample space for $W$ is given by $$N_W = \underbrace{6\times15}_\text{Why?} = 90.$$ Thus the probability that each of the three airports receives a broken taxi is $$P(\text{broken taxi to each } A,B,C) = {N_W\over N} = {90\over504}.$$ Can someone please explain the two Why s? I thought that by holding each of the three broken taxis constant that we would only need to consider the remaining $6$, and I originally thought the probability would be $15\over504$. However, this probability seems unusually low . Thus, I went and found the solution written above. However, I was not able to comprehend the logic behind the solution. Here's what I'm thinking is wrong with my logic process and how we get the correct logic: Holding each of the broken taxis to a particular airport is where the logic is incorrect. We also want to be able to permute the broken taxis among the three airports. This is why we need the ${3\choose1\;1\;1}$. We can then permute the remaining $6$ taxis among the three airports, giving us the ${6\choose 2\;4\;0}$. Is this correct thinking? An earlier problem asked us to find the probability that exactly one broken taxi is sent to airport $C$. I did the following approach: If we're sending $1$ broken taxi to airport $C$, then we can permute the remaining $8$ like so $${8\choose 3 \; 5\; 0} = 56,$$ which this answer was correct according to the back of the book. However, I applied the same principle to the problem at-hand and only received $15$ ways. Is the ""correct"" way to use the theorem and the $mn$-rule to say: There is $${1\choose 0\;0\;1} = 1$$ way to send $1$ broken taxi to airport $C$, and $56$ ways to distinctly send the other $8$ to airports $B$ and $C$. So, by the $mn$-rule, there are $1\times56$ sample points such that a broken taxi gets sent to airport $C$. This makes sense to me, but I might just be making up a solution that yields the correct answer, but is not logically correct.","Suppose we have $9$ taxis, and three airports, aiport $A$, airport $B$, and airport $C$. We want to send $3$ taxis to airport $A$, $5$ taxis to airport $B$, and $1$ taxi to airport $C$. If exactly three of the taxis are in need of repair, what is the probability   that every airport receives one of the taxis requiring repairs? I'm struggling to solve this problem. There is a theorem that I want to use, listed below: THEOREM : The number of ways of partitioning $n$ distinct objects into $k$ distinct groups containing $n_1, n_2, \ldots , n_k$ objects, respectively, where each object appears in exactly one group and $\sum_{i=1}^k n_i = n$, is $$N = {n\choose n_1 \;n_2 \;\cdots\; n_k} = {n!\over n_1!\;n_2!\;\cdots\;n_k!}.$$ The following is a solution I found here on page 83 for this problem. SOLUTION : Using the theorem, there are $${9\choose 3\;5\;1} = {9!\over 3!\;5!\;1!} = 504\text{ ways}$$ to send the taxis to all the airports. Let $W$ denote the event that the $3$ taxis that are in need of repair are sent to each airport. The number of ways we can send each of the $3$ broken taxis to the three airports is given by $${3\choose 1\;1\;1} = 3! = 6. \tag{Why is this important?}$$ Also, the number of ways we can send the remaining $6$ taxis to the three airports is given by $${6\choose 2\;4\;0} = {6!\over2!\;4!} = 15$$ (since each of the three broken taxis are already taking up a spot). So, by the $mn$-rule, the number of points in the sample space for $W$ is given by $$N_W = \underbrace{6\times15}_\text{Why?} = 90.$$ Thus the probability that each of the three airports receives a broken taxi is $$P(\text{broken taxi to each } A,B,C) = {N_W\over N} = {90\over504}.$$ Can someone please explain the two Why s? I thought that by holding each of the three broken taxis constant that we would only need to consider the remaining $6$, and I originally thought the probability would be $15\over504$. However, this probability seems unusually low . Thus, I went and found the solution written above. However, I was not able to comprehend the logic behind the solution. Here's what I'm thinking is wrong with my logic process and how we get the correct logic: Holding each of the broken taxis to a particular airport is where the logic is incorrect. We also want to be able to permute the broken taxis among the three airports. This is why we need the ${3\choose1\;1\;1}$. We can then permute the remaining $6$ taxis among the three airports, giving us the ${6\choose 2\;4\;0}$. Is this correct thinking? An earlier problem asked us to find the probability that exactly one broken taxi is sent to airport $C$. I did the following approach: If we're sending $1$ broken taxi to airport $C$, then we can permute the remaining $8$ like so $${8\choose 3 \; 5\; 0} = 56,$$ which this answer was correct according to the back of the book. However, I applied the same principle to the problem at-hand and only received $15$ ways. Is the ""correct"" way to use the theorem and the $mn$-rule to say: There is $${1\choose 0\;0\;1} = 1$$ way to send $1$ broken taxi to airport $C$, and $56$ ways to distinctly send the other $8$ to airports $B$ and $C$. So, by the $mn$-rule, there are $1\times56$ sample points such that a broken taxi gets sent to airport $C$. This makes sense to me, but I might just be making up a solution that yields the correct answer, but is not logically correct.",,"['probability-theory', 'statistics', 'proof-verification', 'permutations']"
56,Direct proof that$ Var(cX) = c^2Var(X)$,Direct proof that, Var(cX) = c^2Var(X),"I'm trying to prove that $Var(cX) = c^2Var(X)$ and this is what I have so far: $Var(cX) = E((cX - \mu)^2)$ $ = E(c^2X^2 - 2cX\mu + \mu^2)$ $ = c^2E(X)-2c\mu E(X)+\mu^2$ by linearity of expectation $ = c^2E(X)-2c\mu^2+\mu^2$ by $E(X) =\mu$ Here, I am stuck, I know that I have a $c^2$ now but I'm having trouble making the right side of $c^2$ equal to $Var(X)$. EDIT: With the help of below commenters, this is the correct proof. $Var(cX) = E((cX - c\mu)^2)$ $ = E(c^2(X^2-2x\mu+\mu^2))$ $ = c^2E(X^2-2x\mu+\mu^2)$ by linearity $=c^2E((X -\mu)^2)$ $= c^2Var(X)$","I'm trying to prove that $Var(cX) = c^2Var(X)$ and this is what I have so far: $Var(cX) = E((cX - \mu)^2)$ $ = E(c^2X^2 - 2cX\mu + \mu^2)$ $ = c^2E(X)-2c\mu E(X)+\mu^2$ by linearity of expectation $ = c^2E(X)-2c\mu^2+\mu^2$ by $E(X) =\mu$ Here, I am stuck, I know that I have a $c^2$ now but I'm having trouble making the right side of $c^2$ equal to $Var(X)$. EDIT: With the help of below commenters, this is the correct proof. $Var(cX) = E((cX - c\mu)^2)$ $ = E(c^2(X^2-2x\mu+\mu^2))$ $ = c^2E(X^2-2x\mu+\mu^2)$ by linearity $=c^2E((X -\mu)^2)$ $= c^2Var(X)$",,"['statistics', 'proof-writing', 'variance']"
57,What is meant by the Fisher information of a particular of a particular quantity for a quartile function?,What is meant by the Fisher information of a particular of a particular quantity for a quartile function?,,"My provided definition of the Fisher information $\mathcal{I}(\theta)$ is the expected value of the observed information $I(\theta)$, where $I(\theta)$ is the second derivative of the log-likelihood function of some distribution $F(x|\theta)$. There is an exercise in my notes which mentions that the median of the exponential distribution can be estimated using maximum likelihood, using $$ Q(p=0.5|\hat{\theta}) $$ where $Q$ is the quartile function. The exercise asks for me to find the Fisher information for this quantity . I can't really understand what this means. Clearly  $$ Q(p=0.5|\hat{\theta}) = \theta^{-1} \ln{2} $$ which I understand to be the median, but now what is the Fisher information? I don't think I have a distribution , do I? $\theta$ is the true value of the parameter, so it seems to me like I have an unknown quantity here... which leaves me with no idea how to derive the Fisher information. I'm not looking for a drawn out answer if that's what is required here, but rather just an idea of what is meant by the Fisher information in this context. Thanks!","My provided definition of the Fisher information $\mathcal{I}(\theta)$ is the expected value of the observed information $I(\theta)$, where $I(\theta)$ is the second derivative of the log-likelihood function of some distribution $F(x|\theta)$. There is an exercise in my notes which mentions that the median of the exponential distribution can be estimated using maximum likelihood, using $$ Q(p=0.5|\hat{\theta}) $$ where $Q$ is the quartile function. The exercise asks for me to find the Fisher information for this quantity . I can't really understand what this means. Clearly  $$ Q(p=0.5|\hat{\theta}) = \theta^{-1} \ln{2} $$ which I understand to be the median, but now what is the Fisher information? I don't think I have a distribution , do I? $\theta$ is the true value of the parameter, so it seems to me like I have an unknown quantity here... which leaves me with no idea how to derive the Fisher information. I'm not looking for a drawn out answer if that's what is required here, but rather just an idea of what is meant by the Fisher information in this context. Thanks!",,"['statistics', 'quantile']"
58,Find a sufficient statistic for $\theta$ and show that a UMP test of $H_0: \theta = 6$ against $H_1:\theta <6$ is based on this statistic. [duplicate],Find a sufficient statistic for  and show that a UMP test of  against  is based on this statistic. [duplicate],\theta H_0: \theta = 6 H_1:\theta <6,"This question already has an answer here : (Uniformly) Most Powerful test (1 answer) Closed 7 years ago . (20%) Let $X_1, \dotsc, X_n$ be an iid sample from a distribution with pdf $f(x;\theta) = \theta x^{\theta-1}, 0< x< 1$, zero elsewhere, where $\theta >0$. Find a sufficient statistic for $\theta$ and show that a UMP test of $H_0: \theta = 6$ against $H_1:\theta <6$ is based on this statistic. I used the exponential family form to get that the summation of $\ln(x)$ is a sufficient statistic for $\theta$, but I do not know how to find a UMP Test based on this statistic. I believe it has something to do with likelihood ratios. Thanks in advance for your help. My question is different from that question because I do not already have a MP test, these are different numbers, the pdf is different, the significance level is not given, and the alternative hypothesis is an inequality.","This question already has an answer here : (Uniformly) Most Powerful test (1 answer) Closed 7 years ago . (20%) Let $X_1, \dotsc, X_n$ be an iid sample from a distribution with pdf $f(x;\theta) = \theta x^{\theta-1}, 0< x< 1$, zero elsewhere, where $\theta >0$. Find a sufficient statistic for $\theta$ and show that a UMP test of $H_0: \theta = 6$ against $H_1:\theta <6$ is based on this statistic. I used the exponential family form to get that the summation of $\ln(x)$ is a sufficient statistic for $\theta$, but I do not know how to find a UMP Test based on this statistic. I believe it has something to do with likelihood ratios. Thanks in advance for your help. My question is different from that question because I do not already have a MP test, these are different numbers, the pdf is different, the significance level is not given, and the alternative hypothesis is an inequality.",,"['statistics', 'probability-distributions', 'hypothesis-testing']"
59,Uniformly Most Powerful test for normal distribution,Uniformly Most Powerful test for normal distribution,,"Let ${Y_1,...,Y_n}$ be independent random variables and $Y_i$~$N(\beta x_i, 1)$ where $x_1,...,x_n$ are fixed known constants, and $\beta$ is an unknown parameter. I'm trying to find a uniformly most powerful level $\alpha$ test for $$H_0: \beta=0 \quad \text{vs} \quad H_1:\beta>0$$ First I found the Likelihood Ratio statistic $$\begin{align}\frac{L(0)}{L(\beta_1)}&=\frac{e^{-\frac{1}{2}\sum^n_{i=1}(y_i-0x_i)^2}}{e^{-\frac{1}{2}\sum^n_{i=1}(y_i-\beta_1x_i)^2}} \\[15pt] & = \frac{e^{-\frac{1}{2}\sum^n y_i^2}}{e^{-\frac{1}{2}(y_i^2-2y_i\beta_1x_i+\beta_1^2x_i^2)}}   \\[15pt] & =e^{\beta_1\sum^n y_ix_i-\frac{1}{2}\beta_1^2\sum^nx_i^2}\end{align}$$ Then the rejection region of the LRT is $$e^{\beta_1\sum^n y_ix_i-\frac{1}{2}\beta_1^2\sum^nx_i^2} \le k \\$$ Now moving the constants to the right side $$\\[15pt]\Rightarrow\beta_1\sum^n y_ix_i-\frac{1}{2}\beta_1^2\sum^nx_i^2 \;\le\; \text{ln}(k)$$ $$\\[15pt]\Rightarrow\sum^ny_ix_i \;\le\; \frac{\text{ln}(k)+1/2\sum^n\beta_1^2x_i^2}{\beta_1}$$ Here is where I get confused because I'm not sure what to leave on the left side. From what I have read I'm supposed to move constants to the right. I'm thinking that only $y_i$'s should remain on the left. Since $x_i$'s are constants shouldn't I move it? The steps that follow to find the rejection region by using the distribution of the left of the inequality, then equating to a quantile, puzzle me because of the $x_i$ I'm not even sure I'm going about this correctly or if there are more straight forward methods.","Let ${Y_1,...,Y_n}$ be independent random variables and $Y_i$~$N(\beta x_i, 1)$ where $x_1,...,x_n$ are fixed known constants, and $\beta$ is an unknown parameter. I'm trying to find a uniformly most powerful level $\alpha$ test for $$H_0: \beta=0 \quad \text{vs} \quad H_1:\beta>0$$ First I found the Likelihood Ratio statistic $$\begin{align}\frac{L(0)}{L(\beta_1)}&=\frac{e^{-\frac{1}{2}\sum^n_{i=1}(y_i-0x_i)^2}}{e^{-\frac{1}{2}\sum^n_{i=1}(y_i-\beta_1x_i)^2}} \\[15pt] & = \frac{e^{-\frac{1}{2}\sum^n y_i^2}}{e^{-\frac{1}{2}(y_i^2-2y_i\beta_1x_i+\beta_1^2x_i^2)}}   \\[15pt] & =e^{\beta_1\sum^n y_ix_i-\frac{1}{2}\beta_1^2\sum^nx_i^2}\end{align}$$ Then the rejection region of the LRT is $$e^{\beta_1\sum^n y_ix_i-\frac{1}{2}\beta_1^2\sum^nx_i^2} \le k \\$$ Now moving the constants to the right side $$\\[15pt]\Rightarrow\beta_1\sum^n y_ix_i-\frac{1}{2}\beta_1^2\sum^nx_i^2 \;\le\; \text{ln}(k)$$ $$\\[15pt]\Rightarrow\sum^ny_ix_i \;\le\; \frac{\text{ln}(k)+1/2\sum^n\beta_1^2x_i^2}{\beta_1}$$ Here is where I get confused because I'm not sure what to leave on the left side. From what I have read I'm supposed to move constants to the right. I'm thinking that only $y_i$'s should remain on the left. Since $x_i$'s are constants shouldn't I move it? The steps that follow to find the rejection region by using the distribution of the left of the inequality, then equating to a quantile, puzzle me because of the $x_i$ I'm not even sure I'm going about this correctly or if there are more straight forward methods.",,"['statistics', 'probability-distributions', 'statistical-inference', 'hypothesis-testing']"
60,Normal approximation to a binomial probability,Normal approximation to a binomial probability,,"I am studying for the final tomorrow and have been using a quiz as reference. A fair die is rolled 720 times. Let the random variable Y be the number of 6's obtained. Approximate the probabilities. Since n is such a high number we can treat it like a normal distribution using the Central Limit Theorem. In my professor's answer he changes the Y to X and add/sub a .5 depending on which ""tail"" we are trying to find. Ex. P(Y<98) -> P(X<98.5)       P(Y>115) -> P(X > 114.5) What exactly is happening here?","I am studying for the final tomorrow and have been using a quiz as reference. A fair die is rolled 720 times. Let the random variable Y be the number of 6's obtained. Approximate the probabilities. Since n is such a high number we can treat it like a normal distribution using the Central Limit Theorem. In my professor's answer he changes the Y to X and add/sub a .5 depending on which ""tail"" we are trying to find. Ex. P(Y<98) -> P(X<98.5)       P(Y>115) -> P(X > 114.5) What exactly is happening here?",,"['probability', 'statistics', 'normal-distribution', 'approximation', 'central-limit-theorem']"
61,Limit distribution of Markov Chain,Limit distribution of Markov Chain,,I have to determine the limiting distribution for the MC so I determine the equations first: $\Pi_0 +\Pi_1 + \Pi_2 + \Pi_3 +\Pi_4 =1$ $\Pi_0 = \Pi_0q + \Pi_1q + \Pi_2q +\Pi_3q +\Pi_4$ $\Pi_1 = \Pi_0p$ $\Pi_2 = \Pi_1p$ $\Pi_3 = \Pi_2p$ $\Pi_4 = \Pi_3p$ and then I tried to solve the system and I got this: $0= \Pi_0 (-1+q+pq+p^2q+p^3q+p^4) $ So I conclude $0= \Pi_0$ therefore $\Pi_1 = \Pi_2 = \Pi_3 =\Pi_4 =0$ but this doesn't match the answer of the book. Can somebody tell me where is my mistake?,I have to determine the limiting distribution for the MC so I determine the equations first: $\Pi_0 +\Pi_1 + \Pi_2 + \Pi_3 +\Pi_4 =1$ $\Pi_0 = \Pi_0q + \Pi_1q + \Pi_2q +\Pi_3q +\Pi_4$ $\Pi_1 = \Pi_0p$ $\Pi_2 = \Pi_1p$ $\Pi_3 = \Pi_2p$ $\Pi_4 = \Pi_3p$ and then I tried to solve the system and I got this: $0= \Pi_0 (-1+q+pq+p^2q+p^3q+p^4) $ So I conclude $0= \Pi_0$ therefore $\Pi_1 = \Pi_2 = \Pi_3 =\Pi_4 =0$ but this doesn't match the answer of the book. Can somebody tell me where is my mistake?,,"['statistics', 'stochastic-processes', 'markov-chains', 'markov-process', 'transition-matrix']"
62,$O_P(1) o_P(1) = o_P(1)$,,O_P(1) o_P(1) = o_P(1),"There is something written in the book ""Mathematische statistiek"" from van der Vaart which I don't see: ""it is short for: if $X_n$ is bounded in probability and $Y_n \rightarrow^P 0$ then $X_nY_n\rightarrow^P 0$. If $X_n$ would also converge in distribution, this would be Slutsky's lemma (with $c=0$). But by Prohorov's theorem $X_n$ converges in distribution 'along subsequences' if it is bounded in probability, so that this rule can still be deduced by arguing 'along subsequences'."" Now some subsequence $X_{n_j}$ converges in distribution and $Y_{n_j}$ converges in probability to $0$, so $X_{n_j}Y_{n_j}$ converges in distrubition to $0$ (Slutsky). I think we cannot conclude that also $X_nY_n$ converges to $0$ in distribution. But van der Vaart thinks differently. Can anyone say what he probably thinks?","There is something written in the book ""Mathematische statistiek"" from van der Vaart which I don't see: ""it is short for: if $X_n$ is bounded in probability and $Y_n \rightarrow^P 0$ then $X_nY_n\rightarrow^P 0$. If $X_n$ would also converge in distribution, this would be Slutsky's lemma (with $c=0$). But by Prohorov's theorem $X_n$ converges in distribution 'along subsequences' if it is bounded in probability, so that this rule can still be deduced by arguing 'along subsequences'."" Now some subsequence $X_{n_j}$ converges in distribution and $Y_{n_j}$ converges in probability to $0$, so $X_{n_j}Y_{n_j}$ converges in distrubition to $0$ (Slutsky). I think we cannot conclude that also $X_nY_n$ converges to $0$ in distribution. But van der Vaart thinks differently. Can anyone say what he probably thinks?",,"['probability', 'statistics', 'weak-convergence']"
63,"What F-test is performed by $\texttt{lm()}$ function in R, at the end of the output?","What F-test is performed by  function in R, at the end of the output?",\texttt{lm()},"I was wondering what is the F test (in general) that is done by the function $\texttt{lm()}$ in R. I mean you can  do different F tests, which one does it chose and how? For instance in a linear regression setting, I can fit a certain model with $\texttt{lm()}$ , but (before running the code) how do we know against which model it will be compared to? For instance look at this: For instance my model here is of the form $$Y_i= \beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3x_{i1}^2+\beta_4x_{i2}^2+\beta_5x_{i1}x_{i2}+\epsilon_i$$ but, before running this code, what F-test could I have expected to get out of $\texttt{lm()}$? And what test (here) has actually been performed?","I was wondering what is the F test (in general) that is done by the function $\texttt{lm()}$ in R. I mean you can  do different F tests, which one does it chose and how? For instance in a linear regression setting, I can fit a certain model with $\texttt{lm()}$ , but (before running the code) how do we know against which model it will be compared to? For instance look at this: For instance my model here is of the form $$Y_i= \beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3x_{i1}^2+\beta_4x_{i2}^2+\beta_5x_{i1}x_{i2}+\epsilon_i$$ but, before running this code, what F-test could I have expected to get out of $\texttt{lm()}$? And what test (here) has actually been performed?",,"['statistics', 'regression', 'hypothesis-testing']"
64,Find likelihood function and MLE for hypothesis,Find likelihood function and MLE for hypothesis,,"I'm currently reading about statistics and I'm trying to understand likelihood function and MLE (maximum likelihood estimation). I have come across an exercise which is basically like this: I'm given four numbers including $x_1, x_2, x_3$ and $n$, where $n$ is the total number of trials. The numbers for $x_1, x_2$ and $x_3$ are all different. Then I'm given a model $M_0$ which states that $(X_1, X_2, X_3)$ is distributed as the multinomial distribution with parameters $n$ and $(\pi_1, \pi_2, \pi_3)$ where $(\pi_1, \pi_2, \pi_3) \in \Pi^3$. Finally, I have to consider the hypothesis $H_1: (\pi_1, \pi_2, \pi_3) = (1 - 2p, p, p)$, $p \in (0, 0.5)$ My task is then to show that the likelihood function for the hypothesis $H_1$ is given by: $$ L(p) ={{n} \choose {x_1,x_2,x_3}}\cdot (1 - 2p)^{x_1}\cdot p^{x_2+x_3} $$ and show that the maximum likelihood estimation for p is given by $$ \hat{p} = \frac{x_2 +x_3}{2n}. $$ I have been reading and watching a lot of videos about the likelihood function and MLE in order to try to understand it, but I don't even know where to begin in this case. Also, I'm a bit confused whether $p \in (0, 0.5)$ means that $p$ is either $0$ or $0.5$ since the brackets used aren't the normal set brackets { and }. Can someone point me in the right direction?","I'm currently reading about statistics and I'm trying to understand likelihood function and MLE (maximum likelihood estimation). I have come across an exercise which is basically like this: I'm given four numbers including $x_1, x_2, x_3$ and $n$, where $n$ is the total number of trials. The numbers for $x_1, x_2$ and $x_3$ are all different. Then I'm given a model $M_0$ which states that $(X_1, X_2, X_3)$ is distributed as the multinomial distribution with parameters $n$ and $(\pi_1, \pi_2, \pi_3)$ where $(\pi_1, \pi_2, \pi_3) \in \Pi^3$. Finally, I have to consider the hypothesis $H_1: (\pi_1, \pi_2, \pi_3) = (1 - 2p, p, p)$, $p \in (0, 0.5)$ My task is then to show that the likelihood function for the hypothesis $H_1$ is given by: $$ L(p) ={{n} \choose {x_1,x_2,x_3}}\cdot (1 - 2p)^{x_1}\cdot p^{x_2+x_3} $$ and show that the maximum likelihood estimation for p is given by $$ \hat{p} = \frac{x_2 +x_3}{2n}. $$ I have been reading and watching a lot of videos about the likelihood function and MLE in order to try to understand it, but I don't even know where to begin in this case. Also, I'm a bit confused whether $p \in (0, 0.5)$ means that $p$ is either $0$ or $0.5$ since the brackets used aren't the normal set brackets { and }. Can someone point me in the right direction?",,"['probability', 'statistics', 'mathematical-modeling', 'maximum-likelihood', 'descriptive-statistics']"
65,The change in rate and Poisson distribution,The change in rate and Poisson distribution,,"I was given the births in a country follow a Poisson process in which on average number of babies born in $24$ hours is $11.7$. I figured out this indicates $0.4875$ babies are born per hour. So how can I find the probability of more than $3$ hours between births? Is this a change in rate? I'm new to probability and statistics. The answer is $0.2317$, but how was it calculated ?","I was given the births in a country follow a Poisson process in which on average number of babies born in $24$ hours is $11.7$. I figured out this indicates $0.4875$ babies are born per hour. So how can I find the probability of more than $3$ hours between births? Is this a change in rate? I'm new to probability and statistics. The answer is $0.2317$, but how was it calculated ?",,"['probability', 'statistics', 'poisson-distribution']"
66,What is the intuition behind the definition of a Markov Chain being $\pi$-invariant?,What is the intuition behind the definition of a Markov Chain being -invariant?,\pi,"In Markov Chain literature, one definition is that a chain admits $\pi$ as an invariant distribution if: $$ \forall \ \theta^t \in \mathcal{H} \ \ \int_{\mathcal{H}}\pi(\theta^{t-1})K(\theta^t|\theta^{t-1})d\theta^{t-1} = \pi(\theta^t) $$ where $\theta^1, \theta_2, \ldots$ are sequences of a Markov Chain and $K(\theta^t|\theta^{t-1})$ is a transition or Markov Kernel. In this case, we say that the chain is $\pi$-invariant. I am wondering if anyone has any intuition here as to why this is the case. My understanding from Stochastic Processes is that if $\lambda P = \lambda$, then $\lambda$ is an invariant distribution for $P$ a transition matrix. However, the above definition confuses me. Does anyone have any ideas how to interpret it?","In Markov Chain literature, one definition is that a chain admits $\pi$ as an invariant distribution if: $$ \forall \ \theta^t \in \mathcal{H} \ \ \int_{\mathcal{H}}\pi(\theta^{t-1})K(\theta^t|\theta^{t-1})d\theta^{t-1} = \pi(\theta^t) $$ where $\theta^1, \theta_2, \ldots$ are sequences of a Markov Chain and $K(\theta^t|\theta^{t-1})$ is a transition or Markov Kernel. In this case, we say that the chain is $\pi$-invariant. I am wondering if anyone has any intuition here as to why this is the case. My understanding from Stochastic Processes is that if $\lambda P = \lambda$, then $\lambda$ is an invariant distribution for $P$ a transition matrix. However, the above definition confuses me. Does anyone have any ideas how to interpret it?",,"['probability-theory', 'statistics', 'stochastic-processes']"
67,"Markov chains $(Z_n)$, how to find the probability $\mathbb{P} (Z_{n+1} = i+1 \mid Z_n = i)$?","Markov chains , how to find the probability ?",(Z_n) \mathbb{P} (Z_{n+1} = i+1 \mid Z_n = i),"Sorry for my bad english. We have a Markov chains $(Z_n)$ which follows this process : At first, $Z_1 = 0$. Then, $Z_2 = 1$ with probability $1/2$ or $Z_2 = -1$ with probability $1/2$. If $Z_2 = 1$, $Z_3 = 2$ with probability $1/2$ or $Z_3 = 0$ with probability $1/2$. If $Z_2 = -1$, $Z_3 = 0$ with probability $1/2$ or $Z_3 = -2$ with probability $1/2$. Etc. I have done a drawing until the step 8 : We would like to show that : $\mathbb{P} (Z_{n+1} = i+1 \mid Z_n = i) = \dfrac{n+2-i}{2(n+2)}$, and $\mathbb{P} (Z_{n+1} = i-1 \mid Z_n = i) = \dfrac{n+2+i}{2(n+2)}$ I don't see how to do it at all. Someone could help me ? Thank you in advance...","Sorry for my bad english. We have a Markov chains $(Z_n)$ which follows this process : At first, $Z_1 = 0$. Then, $Z_2 = 1$ with probability $1/2$ or $Z_2 = -1$ with probability $1/2$. If $Z_2 = 1$, $Z_3 = 2$ with probability $1/2$ or $Z_3 = 0$ with probability $1/2$. If $Z_2 = -1$, $Z_3 = 0$ with probability $1/2$ or $Z_3 = -2$ with probability $1/2$. Etc. I have done a drawing until the step 8 : We would like to show that : $\mathbb{P} (Z_{n+1} = i+1 \mid Z_n = i) = \dfrac{n+2-i}{2(n+2)}$, and $\mathbb{P} (Z_{n+1} = i-1 \mid Z_n = i) = \dfrac{n+2+i}{2(n+2)}$ I don't see how to do it at all. Someone could help me ? Thank you in advance...",,"['probability', 'statistics', 'markov-chains']"
68,Joint density with restrain,Joint density with restrain,,"I can't solve this problem...tried several areas for limits of integration but solution just doesn't match with solution from end of textbook (Stirzaker:Probability and random variables). Exercise 6.6.2 Let X and Y have joint density $$f(x,y)=e^{-y}$$  $$0<x<y<\infty$$ Find joint density for Z=X+Y. I just can't figure it up and became really frustrated, especially being self-learner. I haven't write any of my trials because I only ask for someone to please help me with bounds of integration(I know formula for joint density etc.) Many thanks!","I can't solve this problem...tried several areas for limits of integration but solution just doesn't match with solution from end of textbook (Stirzaker:Probability and random variables). Exercise 6.6.2 Let X and Y have joint density $$f(x,y)=e^{-y}$$  $$0<x<y<\infty$$ Find joint density for Z=X+Y. I just can't figure it up and became really frustrated, especially being self-learner. I haven't write any of my trials because I only ask for someone to please help me with bounds of integration(I know formula for joint density etc.) Many thanks!",,"['probability', 'statistics', 'probability-distributions']"
69,Proof that sum over autocorrelations is -1/2,Proof that sum over autocorrelations is -1/2,,"I am trying to understand a proof that shows, that the sum over the autocorrelation starting with lag=1 is always equal -1/2, for a stationay time series. The sum looks like this: $$ S_{\rm{afc}}=\sum_{h=1}^{T-1} \hat \rho(h)=\sum^{T-1}_{h=1}\Big( \frac{\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {\sum^{T}_{t=1} (y_t - \overline y)^2 }\Big)$$ with $\hat \rho(h)=\frac{\hat \gamma(h)}{\hat \gamma(0)}=\frac{\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {\sum^{T}_{t=1} (y_t - \overline y)^2 }$ the autocorrelation function defined in terms of an estimate $\hat \gamma = \sum^{T-h}_{h=1} (y_t - \overline y)(y_{t+h} - \overline y)$ of the the autocovariance and $\overline y= { 1  \over T} \sum^T_{t=1} y_t$ the sample mean. The proof is short:  $$ S_{\rm{afc}}=\sum^{T-1}_{h=1}\Big( \frac{\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {\sum^{T}_{t=1} (y_t - \overline y)^2 }\Big)=\Big( \frac{ \sum^{T-1}_{h=1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {\sum^{T}_{t=1} (y_t - \overline y)^2 }\Big)\\=\Big( \frac{ \sum^{T-1}_{h=1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {\big(\sum^{T}_{t=1} (y_t - \overline y)\big)^2-2 \sum_{h=1}^{T-1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y)}\Big)$$ until here I can understand everything but the last step is a mystery to me: $$\Big( \frac{ \sum^{T-1}_{h=1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {\big(\sum^{T}_{t=1} (y_t - \overline y)\big)^2-2 \sum_{h=1}^{T-1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y)}\Big)=\Big( \frac{ \sum^{T-1}_{h=1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {-2 \sum_{h=1}^{T-1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y)}\Big)$$ which is of course -1/2. Why would $\big(\sum^{T}_{t=1}(y_t - \overline y)\big)^2$ vanish? I have no earthly Idea how this might be. In case you want to look at the paper is  ""Sum of the sample autocorrelation function"" by Hossein Hassani DOI: https://doi.org/10.1515/ROSE.2009.008","I am trying to understand a proof that shows, that the sum over the autocorrelation starting with lag=1 is always equal -1/2, for a stationay time series. The sum looks like this: $$ S_{\rm{afc}}=\sum_{h=1}^{T-1} \hat \rho(h)=\sum^{T-1}_{h=1}\Big( \frac{\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {\sum^{T}_{t=1} (y_t - \overline y)^2 }\Big)$$ with $\hat \rho(h)=\frac{\hat \gamma(h)}{\hat \gamma(0)}=\frac{\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {\sum^{T}_{t=1} (y_t - \overline y)^2 }$ the autocorrelation function defined in terms of an estimate $\hat \gamma = \sum^{T-h}_{h=1} (y_t - \overline y)(y_{t+h} - \overline y)$ of the the autocovariance and $\overline y= { 1  \over T} \sum^T_{t=1} y_t$ the sample mean. The proof is short:  $$ S_{\rm{afc}}=\sum^{T-1}_{h=1}\Big( \frac{\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {\sum^{T}_{t=1} (y_t - \overline y)^2 }\Big)=\Big( \frac{ \sum^{T-1}_{h=1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {\sum^{T}_{t=1} (y_t - \overline y)^2 }\Big)\\=\Big( \frac{ \sum^{T-1}_{h=1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {\big(\sum^{T}_{t=1} (y_t - \overline y)\big)^2-2 \sum_{h=1}^{T-1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y)}\Big)$$ until here I can understand everything but the last step is a mystery to me: $$\Big( \frac{ \sum^{T-1}_{h=1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {\big(\sum^{T}_{t=1} (y_t - \overline y)\big)^2-2 \sum_{h=1}^{T-1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y)}\Big)=\Big( \frac{ \sum^{T-1}_{h=1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y) } {-2 \sum_{h=1}^{T-1}\sum^{T-h}_{t=1} (y_t - \overline y)(y_{t+h} - \overline y)}\Big)$$ which is of course -1/2. Why would $\big(\sum^{T}_{t=1}(y_t - \overline y)\big)^2$ vanish? I have no earthly Idea how this might be. In case you want to look at the paper is  ""Sum of the sample autocorrelation function"" by Hossein Hassani DOI: https://doi.org/10.1515/ROSE.2009.008",,"['statistics', 'summation', 'covariance', 'correlation']"
70,Finding a sufficient statistic for $\beta$,Finding a sufficient statistic for,\beta,"Let ${Y_1,...,Y_n}$ be independent random variables and $Y_i\sim N(\beta x_i, 1)$ where $x_1,...,x_n$ are fixed known constants, and $\beta$ is an unknown parameter. I'm trying to find a sufficient statistic for $\beta$ The issue I'm running into  is that after I find the likelihood I'm left with 2 betas in separate terms in the exponent. I'm not sure how to use the factorization criterion when there are different terms containing the parameter.","Let be independent random variables and where are fixed known constants, and is an unknown parameter. I'm trying to find a sufficient statistic for The issue I'm running into  is that after I find the likelihood I'm left with 2 betas in separate terms in the exponent. I'm not sure how to use the factorization criterion when there are different terms containing the parameter.","{Y_1,...,Y_n} Y_i\sim N(\beta x_i, 1) x_1,...,x_n \beta \beta","['statistics', 'probability-distributions', 'normal-distribution', 'statistical-inference']"
71,Intuitive interpretation on an expectation equation,Intuitive interpretation on an expectation equation,,"It's provably correct that for a nonnegative random variable denoted as $Z$. The expectation of $Z$ can be written as follows: $$\mathbb{E}[Z] = \int_{x=0}^{\infty}\Pr[Z\geq x]dx.$$ Well, it can be proved by methods ""integration by part"", however, I think there may exist a more intuitive interpretation and a direct connection with ordinary definition of expectation  $$\mathbb{E}[Z] = \int_{x=0}^{\infty}f(x)xdx$$ Hope someone could give some hints, thx. Also, for discrete case, is there also intuitive interpretation?","It's provably correct that for a nonnegative random variable denoted as $Z$. The expectation of $Z$ can be written as follows: $$\mathbb{E}[Z] = \int_{x=0}^{\infty}\Pr[Z\geq x]dx.$$ Well, it can be proved by methods ""integration by part"", however, I think there may exist a more intuitive interpretation and a direct connection with ordinary definition of expectation  $$\mathbb{E}[Z] = \int_{x=0}^{\infty}f(x)xdx$$ Hope someone could give some hints, thx. Also, for discrete case, is there also intuitive interpretation?",,"['probability', 'statistics', 'expectation']"
72,AP Statistics practice test question about residual graphs,AP Statistics practice test question about residual graphs,,"I was hoping someone might explain why answer C in the picture is better than D or E? It would appear that in answer D, the larger x becomes the closer the regression line fits the data. With E, the smaller x is the better the regression line fits. Thanks a lot!","I was hoping someone might explain why answer C in the picture is better than D or E? It would appear that in answer D, the larger x becomes the closer the regression line fits the data. With E, the smaller x is the better the regression line fits. Thanks a lot!",,"['statistics', 'linear-regression']"
73,What minimum number (A) can be taken so that (A)^N is larger than the product of N numbers?,What minimum number (A) can be taken so that (A)^N is larger than the product of N numbers?,,"Given a sequence of N numbers say 2,8,4,7,6,5. How can we calculate a minimum number say A such that A N is greater than the product of 2*8*4*7*6*5 = 13440 ? So the minimum number satisfying the above condition is 5 . As 5 6 = 15625 which is greater than 13440 . But 4 6 = 4096 which is less than 13440 .","Given a sequence of N numbers say 2,8,4,7,6,5. How can we calculate a minimum number say A such that A N is greater than the product of 2*8*4*7*6*5 = 13440 ? So the minimum number satisfying the above condition is 5 . As 5 6 = 15625 which is greater than 13440 . But 4 6 = 4096 which is less than 13440 .",,"['number-theory', 'statistics', 'inequality', 'algorithms']"
74,Poisson Process with Randomly Distributed Time,Poisson Process with Randomly Distributed Time,,"If I have a Poisson process, $N_t$ with intensity $\lambda$, then we let $X$ be a random variable that is exponentially distributed and independent of the poisson process. I am trying to find the distribution of $N_X$. I thought the best way to go about this would be to compute $P(N_X \geq k)$ and then write  $$ P(N_X = k)=P(N_X \geq k)-P(N_X \geq k+1) $$ Do you guys think this is the best way to go about it? If so I am having a bit of trouble in computing $P(N_X \geq k)$. Thankyou for any help!","If I have a Poisson process, $N_t$ with intensity $\lambda$, then we let $X$ be a random variable that is exponentially distributed and independent of the poisson process. I am trying to find the distribution of $N_X$. I thought the best way to go about this would be to compute $P(N_X \geq k)$ and then write  $$ P(N_X = k)=P(N_X \geq k)-P(N_X \geq k+1) $$ Do you guys think this is the best way to go about it? If so I am having a bit of trouble in computing $P(N_X \geq k)$. Thankyou for any help!",,"['statistics', 'poisson-process']"
75,"Let $Y=\frac{X^{2}}{2}$, Determine $F_{Y}(y)$ [closed]","Let , Determine  [closed]",Y=\frac{X^{2}}{2} F_{Y}(y),"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question https://i.sstatic.net/KGHPW.jpg I have figured out that $F_{Y}(y)=1/8$ for $0<y<8$. The only problem is that shouldn't P(Y=2)=P(X=2)+P(X=-2). If i sub the values into their respective densities, they give unequal values. Why is this the case?","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question https://i.sstatic.net/KGHPW.jpg I have figured out that $F_{Y}(y)=1/8$ for $0<y<8$. The only problem is that shouldn't P(Y=2)=P(X=2)+P(X=-2). If i sub the values into their respective densities, they give unequal values. Why is this the case?",,"['statistics', 'density-function']"
76,Alpha-Trimmed Mean and Median,Alpha-Trimmed Mean and Median,,"I'm currently working on a proof for my stochastics course, and I am a bit stuck. I tried to find some hints or help online, but so far I haven't found anything. So I figured I'd see if someone could help me here. So the problem is the following: We have to use a proof by cases, one with an even number of observations and one with an odd number of observations in our sample, that the $\alpha$-truncated mean converges to the median, as $\alpha \rightarrow \frac{1}{2}$. As the course is in German, and I'm not sure about the English term for this mean, I'll also leave the definition here. It also contains the problem I have with this proof: $$x_{t,\alpha} := \frac{1}{n-2k}\sum^{n-k}_{j=k+1}x_{[j]} $$ with $0 < \alpha < 1/2$ and $k:= \lfloor n\alpha \rfloor$, and where our sample $(x_{[1]},\dots,x_{[n]})$ is already ordered. Now, I am just looking for a starting point, not a complete solution, after all I would really love to figure it out by myself - at least partially. I'm just not sure how to deal with the floor function and the change in the size of the sum. Any good hints or references? Thanks in advance!","I'm currently working on a proof for my stochastics course, and I am a bit stuck. I tried to find some hints or help online, but so far I haven't found anything. So I figured I'd see if someone could help me here. So the problem is the following: We have to use a proof by cases, one with an even number of observations and one with an odd number of observations in our sample, that the $\alpha$-truncated mean converges to the median, as $\alpha \rightarrow \frac{1}{2}$. As the course is in German, and I'm not sure about the English term for this mean, I'll also leave the definition here. It also contains the problem I have with this proof: $$x_{t,\alpha} := \frac{1}{n-2k}\sum^{n-k}_{j=k+1}x_{[j]} $$ with $0 < \alpha < 1/2$ and $k:= \lfloor n\alpha \rfloor$, and where our sample $(x_{[1]},\dots,x_{[n]})$ is already ordered. Now, I am just looking for a starting point, not a complete solution, after all I would really love to figure it out by myself - at least partially. I'm just not sure how to deal with the floor function and the change in the size of the sum. Any good hints or references? Thanks in advance!",,"['statistics', 'ceiling-and-floor-functions', 'descriptive-statistics', 'median']"
77,The Normal Distribution,The Normal Distribution,,"I'd like to correct some of my misconceptions about The Normal Distribution. Firstly, I don't know how to interpret the following formula: $ \displaystyle Z = \frac{X-\mu}{\sigma}$ I was first introduced to this formula in a earlier chapter and it was introduced to me as the Z-Score. Apparently this was useful since it could find the number of standard deviations a value in a data set $X$ is from the mean. However, upon starting The Normal Distriubution it seems $Z$ is interpreted differently. From what I can understand, it seems $Z$ is treated as another data set with $\mu = 0$ and $\sigma=1$ which can be transformed onto by taking all data values in $X$ and applying $ \displaystyle \frac{X-\mu}{\sigma}$ I'm struggling to accept this concept and don't understand how the formula $ \frac{X-\mu}{\sigma} $ maps any normal distribution X to the Standard Normal distribution $Z$. Secondly, I understand the Standard Normal Distribution has the equation $ \displaystyle f(x) = \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} $. Suppose we wanted to find the $P(Z=1)$, I understand this can be found by just calculating $f(1)$, since the vertical axis outputs probabilities for $x\in X$. However, when looking for probabilities such as $P(Z\leq a)$, I don't understand how this equals the area under the curve over the interval $(-\infty,a]$. Wouldn't it be some type of sum of all the f(x) values between $(-\infty,a]$, so $f(a) + f(a-1)+f(a-2)+f(a-3) + ... $","I'd like to correct some of my misconceptions about The Normal Distribution. Firstly, I don't know how to interpret the following formula: $ \displaystyle Z = \frac{X-\mu}{\sigma}$ I was first introduced to this formula in a earlier chapter and it was introduced to me as the Z-Score. Apparently this was useful since it could find the number of standard deviations a value in a data set $X$ is from the mean. However, upon starting The Normal Distriubution it seems $Z$ is interpreted differently. From what I can understand, it seems $Z$ is treated as another data set with $\mu = 0$ and $\sigma=1$ which can be transformed onto by taking all data values in $X$ and applying $ \displaystyle \frac{X-\mu}{\sigma}$ I'm struggling to accept this concept and don't understand how the formula $ \frac{X-\mu}{\sigma} $ maps any normal distribution X to the Standard Normal distribution $Z$. Secondly, I understand the Standard Normal Distribution has the equation $ \displaystyle f(x) = \frac{e^{-\frac{x^2}{2}}}{\sqrt{2\pi}} $. Suppose we wanted to find the $P(Z=1)$, I understand this can be found by just calculating $f(1)$, since the vertical axis outputs probabilities for $x\in X$. However, when looking for probabilities such as $P(Z\leq a)$, I don't understand how this equals the area under the curve over the interval $(-\infty,a]$. Wouldn't it be some type of sum of all the f(x) values between $(-\infty,a]$, so $f(a) + f(a-1)+f(a-2)+f(a-3) + ... $",,['statistics']
78,Probability whether all characters would appear at least once?,Probability whether all characters would appear at least once?,,"If I'm generating a random string of length $n$ over the English alphabet, what is the probability that each character from the English alphabet would appear at least once?  Assume $n \geq 26$.","If I'm generating a random string of length $n$ over the English alphabet, what is the probability that each character from the English alphabet would appear at least once?  Assume $n \geq 26$.",,"['probability', 'statistics']"
79,Expectation of Truncated Random Variable,Expectation of Truncated Random Variable,,"So this is a pretty basic question but it is tripping me up. Suppose we have a random variable $X$ so that $X\sim F$. Now, consider $Y$ where $Y=X$ if $X<T$ but $Y=T$ if $X\ge T$. What is $E[Y]$? I have $E[Y]=E[X|X<T] \cdot Pr(X<T) + T \cdot Pr(X\ge T) = F(T) \cdot \int _0^T x dF(x)+T\cdot \bar{F}(T)$, which I believe should be right (as $T\to \infty$ for example this works out). However, looking at this wikipedia article, it says that $E[X|X>y] = \frac{\int_y^\infty xdF(x)}{\bar F (x)}$ which instead has some normalizing thing in the denominator which would change my answer. Is this even correct?","So this is a pretty basic question but it is tripping me up. Suppose we have a random variable $X$ so that $X\sim F$. Now, consider $Y$ where $Y=X$ if $X<T$ but $Y=T$ if $X\ge T$. What is $E[Y]$? I have $E[Y]=E[X|X<T] \cdot Pr(X<T) + T \cdot Pr(X\ge T) = F(T) \cdot \int _0^T x dF(x)+T\cdot \bar{F}(T)$, which I believe should be right (as $T\to \infty$ for example this works out). However, looking at this wikipedia article, it says that $E[X|X>y] = \frac{\int_y^\infty xdF(x)}{\bar F (x)}$ which instead has some normalizing thing in the denominator which would change my answer. Is this even correct?",,"['statistics', 'probability-distributions', 'expectation']"
80,Parital derivative of sigmoid function with respect to theta,Parital derivative of sigmoid function with respect to theta,,"I am attempting to calculate the partial derivative of the sigmoid function with respect to theta: $ y = \frac{1}{1+ e^{-\theta x}}$ Let: $v = -\theta x $ $u = (1 + e^{-\theta x}) = (1 + e^v)$ Then: $ \frac{\partial y}{\partial u} = -u^{-2}$ $ \frac{\partial u}{\partial v} = e^v $ $ \frac{\partial v}{\partial \theta_i} = -x_i $ So, applying the chain rule: $ \frac{\partial y}{\partial \theta_i} $ $= \frac{\partial y}{\partial u} \frac{\partial u}{\partial v} \frac{\partial v}{\partial \theta_i}$ $= -u^{-2} e^v (-x_i)$ $= -(1 + e^v)^{-2} e^v (-x_i)$ $= -(1+e^{-\theta x})^{-2} e^{-\theta x} (-x_i)$ $=\frac{-x_ie^{-\theta x}}{-(1+e^{-\theta x})^2} $ At this point, I'm trying to figure out how to get it into this form: $ \frac{\partial y}{\partial \theta_i} = y(1 - y)$ How do I accomplish this? Also, it is alleged that: $1 - \frac{1}{1+ e^{-\theta x}} = \frac{e^{-\theta x}}{1 + e^{-\theta x}}$ How is this possible?","I am attempting to calculate the partial derivative of the sigmoid function with respect to theta: $ y = \frac{1}{1+ e^{-\theta x}}$ Let: $v = -\theta x $ $u = (1 + e^{-\theta x}) = (1 + e^v)$ Then: $ \frac{\partial y}{\partial u} = -u^{-2}$ $ \frac{\partial u}{\partial v} = e^v $ $ \frac{\partial v}{\partial \theta_i} = -x_i $ So, applying the chain rule: $ \frac{\partial y}{\partial \theta_i} $ $= \frac{\partial y}{\partial u} \frac{\partial u}{\partial v} \frac{\partial v}{\partial \theta_i}$ $= -u^{-2} e^v (-x_i)$ $= -(1 + e^v)^{-2} e^v (-x_i)$ $= -(1+e^{-\theta x})^{-2} e^{-\theta x} (-x_i)$ $=\frac{-x_ie^{-\theta x}}{-(1+e^{-\theta x})^2} $ At this point, I'm trying to figure out how to get it into this form: $ \frac{\partial y}{\partial \theta_i} = y(1 - y)$ How do I accomplish this? Also, it is alleged that: $1 - \frac{1}{1+ e^{-\theta x}} = \frac{e^{-\theta x}}{1 + e^{-\theta x}}$ How is this possible?",,"['calculus', 'linear-algebra', 'algebra-precalculus', 'statistics', 'machine-learning']"
81,Method of moments when the distribution is unknown.,Method of moments when the distribution is unknown.,,"I get that method of moments (MoM) can be used to get the parameters of a specified distribution (under certain conditions). When the distribution is unknown, is it possible to use MoM on some kind of generalized distribution to narrow down to a particular distribution? Like generalized Gaussian, for instance. I am looking for something more general than Gaussians. Or is it that having a knowledge of a particular distribution is a pre-requisite for using MoM? Thanks!","I get that method of moments (MoM) can be used to get the parameters of a specified distribution (under certain conditions). When the distribution is unknown, is it possible to use MoM on some kind of generalized distribution to narrow down to a particular distribution? Like generalized Gaussian, for instance. I am looking for something more general than Gaussians. Or is it that having a knowledge of a particular distribution is a pre-requisite for using MoM? Thanks!",,"['probability', 'statistics']"
82,Finding z-score of probability,Finding z-score of probability,,I am learning the finance topic value at risk and trying to understand the number of standard deviations. I dont quite get why was 1.65 used to multiply against the std dev instead of 1.644853 as shown in the calculator below (used to retrieve the z-score from the probability),I am learning the finance topic value at risk and trying to understand the number of standard deviations. I dont quite get why was 1.65 used to multiply against the std dev instead of 1.644853 as shown in the calculator below (used to retrieve the z-score from the probability),,['statistics']
83,Compute UMVUE of $\frac{\mu}{\sigma^2}$,Compute UMVUE of,\frac{\mu}{\sigma^2},"Given $X_1,\ldots, X_7$ are i.i.d r.vs which follow $N(\mu, \sigma^2)$. (a) Find the UMVUE of $\frac{\mu}{\sigma^2}$. (b) Compute variance of UMVUE in part (a) (c) Find the lower bound of the variance of unbiased estimator of $\frac{\mu}{\sigma^2}$ for $n$ i.i.d variables $X_1, X_2,...,X_n$ follows $N(\mu, \sigma^2)$. My thought: First, we realize that $E(S^2) = \sigma^2$ where $S^2 = \frac{\sum_{i=1}^{7} (X_i - \overline{X})^2}{6}$ is a sample variance. Thus, $E(\frac{1}{S^2}) = \frac{A}{\sigma^2}$ (I could not see how to compute this constant $A$). Since the sample mean and sample variance of $X_1,\ldots, X_7$ are independent, we have: $E(\frac{\overline{X}}{S^2}) = E(\overline{X})E(\frac{1}{S^2}) = \frac{\mu}{A}$, so $\frac{\overline{X}}{AS^2}$ is the unbiased estimator of $\frac{\mu}{\sigma^2}$. Now, this unbiased estimator could be expressed as a complete sufficient statistics $(\sum_{i=1}^{7} X_i, \sum_{i=1}^{7} X_i^2)$ because $S^2 = \frac{\sum_{i=1}^{7} X_i^2 - \frac{(\sum_{i=1}^{7} X_i)^2}{7}}{6}$. This implies $\fbox{$\frac{\overline{X}}{AS^2}$}$ is a UMVUE of $\frac{\mu}{\sigma^2}$ (b) Thanks to the work by NCH below, we obtain UMVUE of $\frac{\mu}{\sigma^2}$ is $\frac{2}{3} \frac{\overline{X}}{S^2}$. Thus, to compute $Var(\frac{\overline{X}}{S^2})$, we need to compute $E(\frac{\overline{X}^2}{S^4})$. First, since $\overline{X}$ and $\frac{1}{S^2}$ are independent, $\overline{X}^2$ and $\frac{1}{S^4}$ are independent. So $E(\frac{\overline{X}^2}{S^4}) = E(\overline{X}^2)E(\frac{1}{S^4})$. Now, since $Var(\overline{X}) = \frac{Var(X_1)}{7} = \frac{\sigma^2}{7}$, $E(\overline{X}) = \frac{\sigma^2}{7} + \mu^2$ My question: Could anyone please help me determine the constant $A$ above to complete this proof? I feel so shameful not to be able to compute $E(\frac{1}{S^2})$.","Given $X_1,\ldots, X_7$ are i.i.d r.vs which follow $N(\mu, \sigma^2)$. (a) Find the UMVUE of $\frac{\mu}{\sigma^2}$. (b) Compute variance of UMVUE in part (a) (c) Find the lower bound of the variance of unbiased estimator of $\frac{\mu}{\sigma^2}$ for $n$ i.i.d variables $X_1, X_2,...,X_n$ follows $N(\mu, \sigma^2)$. My thought: First, we realize that $E(S^2) = \sigma^2$ where $S^2 = \frac{\sum_{i=1}^{7} (X_i - \overline{X})^2}{6}$ is a sample variance. Thus, $E(\frac{1}{S^2}) = \frac{A}{\sigma^2}$ (I could not see how to compute this constant $A$). Since the sample mean and sample variance of $X_1,\ldots, X_7$ are independent, we have: $E(\frac{\overline{X}}{S^2}) = E(\overline{X})E(\frac{1}{S^2}) = \frac{\mu}{A}$, so $\frac{\overline{X}}{AS^2}$ is the unbiased estimator of $\frac{\mu}{\sigma^2}$. Now, this unbiased estimator could be expressed as a complete sufficient statistics $(\sum_{i=1}^{7} X_i, \sum_{i=1}^{7} X_i^2)$ because $S^2 = \frac{\sum_{i=1}^{7} X_i^2 - \frac{(\sum_{i=1}^{7} X_i)^2}{7}}{6}$. This implies $\fbox{$\frac{\overline{X}}{AS^2}$}$ is a UMVUE of $\frac{\mu}{\sigma^2}$ (b) Thanks to the work by NCH below, we obtain UMVUE of $\frac{\mu}{\sigma^2}$ is $\frac{2}{3} \frac{\overline{X}}{S^2}$. Thus, to compute $Var(\frac{\overline{X}}{S^2})$, we need to compute $E(\frac{\overline{X}^2}{S^4})$. First, since $\overline{X}$ and $\frac{1}{S^2}$ are independent, $\overline{X}^2$ and $\frac{1}{S^4}$ are independent. So $E(\frac{\overline{X}^2}{S^4}) = E(\overline{X}^2)E(\frac{1}{S^4})$. Now, since $Var(\overline{X}) = \frac{Var(X_1)}{7} = \frac{\sigma^2}{7}$, $E(\overline{X}) = \frac{\sigma^2}{7} + \mu^2$ My question: Could anyone please help me determine the constant $A$ above to complete this proof? I feel so shameful not to be able to compute $E(\frac{1}{S^2})$.",,"['statistics', 'statistical-inference']"
84,finding joint distribution and poisson process [closed],finding joint distribution and poisson process [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Two copy editors read a $300$-page manuscript. The first found $100$ typos, the second   found $120$, and their lists contain $80$ errors in common. Suppose that the author's   typos follow a Poisson process with some unknown rate $\lambda$ per page, while the two   copy editors catch errors with unknown probabilities of success $p_1$ and $p_2$. Let $X_0$   be the number of typos that neither found. Let $X_1$ and $X_2$ be the number of typos   found only by $1$ or only by $2$, and let $X_3$ be the number of typos found by both. (a) Find the joint distribution of ($X_0;X_1;X_2;X_3$), expressed in $\lambda$, $p_1$, $p_2$. (b) Use the answer to (a) to find an estimates of $p_1$, $p_2$ and then the number of   undiscovered typos.   (Hint: Let N(s) be the Poisson process, with $T_i$ being the ""time"" when the i-th   typo occurs, and the ""time"" variable is the page number.) Will appreciate it if someone can help me with this question. I don really know how to interpret this question.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Two copy editors read a $300$-page manuscript. The first found $100$ typos, the second   found $120$, and their lists contain $80$ errors in common. Suppose that the author's   typos follow a Poisson process with some unknown rate $\lambda$ per page, while the two   copy editors catch errors with unknown probabilities of success $p_1$ and $p_2$. Let $X_0$   be the number of typos that neither found. Let $X_1$ and $X_2$ be the number of typos   found only by $1$ or only by $2$, and let $X_3$ be the number of typos found by both. (a) Find the joint distribution of ($X_0;X_1;X_2;X_3$), expressed in $\lambda$, $p_1$, $p_2$. (b) Use the answer to (a) to find an estimates of $p_1$, $p_2$ and then the number of   undiscovered typos.   (Hint: Let N(s) be the Poisson process, with $T_i$ being the ""time"" when the i-th   typo occurs, and the ""time"" variable is the page number.) Will appreciate it if someone can help me with this question. I don really know how to interpret this question.",,"['statistics', 'stochastic-processes', 'poisson-distribution', 'poisson-process']"
85,"If $\sqrt{n}(\widehat{\theta}_{n}-\theta) \to N(0, \frac{1}{I_1(\theta)})$, what does $\sqrt{n}(\widehat{\theta}_{kn}-\theta)$ converge to?","If , what does  converge to?","\sqrt{n}(\widehat{\theta}_{n}-\theta) \to N(0, \frac{1}{I_1(\theta)}) \sqrt{n}(\widehat{\theta}_{kn}-\theta)","Suppose that $$ \sqrt{n}\left(\widehat{\theta}_{n}-\theta\right) \overset{D}\to N\left(0, \frac{1}{I_1(\theta)}\right) $$ From this, is it true that: $$ \sqrt{n}\left(\widehat{\theta}_{kn}-\theta\right) \overset{D}\to N\left(0, \frac{1}{I_1(\theta)}\right) $$ where $k \in \mathbb{N}$? In other words, my sequence is further ahead in the second equation.","Suppose that $$ \sqrt{n}\left(\widehat{\theta}_{n}-\theta\right) \overset{D}\to N\left(0, \frac{1}{I_1(\theta)}\right) $$ From this, is it true that: $$ \sqrt{n}\left(\widehat{\theta}_{kn}-\theta\right) \overset{D}\to N\left(0, \frac{1}{I_1(\theta)}\right) $$ where $k \in \mathbb{N}$? In other words, my sequence is further ahead in the second equation.",,"['probability', 'statistics']"
86,convergence in distribution in $l_{2}$,convergence in distribution in,l_{2},"Consider the space $l^{2}$. Let $X_{n}$ be a sequence of random vectors in $l^{2}$, such that $X_{n} \stackrel{d}{\to} X$. Let $Z_{n}$ be another sequence of random vectors in $l^{2}$, such that $$ \mathbb{E}[\|X_{n} - Z{_n} \|_{2}^{2}] \to 0 \quad\text{as}\quad n\to\infty. $$ The question: is it true that $Z_{n} \stackrel{d}{\to} X$, as $n\to \infty$?","Consider the space $l^{2}$. Let $X_{n}$ be a sequence of random vectors in $l^{2}$, such that $X_{n} \stackrel{d}{\to} X$. Let $Z_{n}$ be another sequence of random vectors in $l^{2}$, such that $$ \mathbb{E}[\|X_{n} - Z{_n} \|_{2}^{2}] \to 0 \quad\text{as}\quad n\to\infty. $$ The question: is it true that $Z_{n} \stackrel{d}{\to} X$, as $n\to \infty$?",,"['probability', 'probability-theory', 'statistics', 'probability-distributions', 'stochastic-processes']"
87,Expectation of normalized order statistics,Expectation of normalized order statistics,,Is there a way to calculate quantities of the form $$\mu_k = \mathbf{E}\bigg[ \frac{X_{(k)}}{\sum_{i=1}^n X_i} \bigg]$$ where the $X_i$'s are independent exponentially distributed random variables with mean $\lambda=1$ and $X_{(k)}$ denotes the $k$-th order statistic ?,Is there a way to calculate quantities of the form $$\mu_k = \mathbf{E}\bigg[ \frac{X_{(k)}}{\sum_{i=1}^n X_i} \bigg]$$ where the $X_i$'s are independent exponentially distributed random variables with mean $\lambda=1$ and $X_{(k)}$ denotes the $k$-th order statistic ?,,"['probability', 'probability-theory', 'statistics', 'expectation', 'order-statistics']"
88,Statistics uniform distribution commuter bus,Statistics uniform distribution commuter bus,,"The time, $X$, it takes a commuter bus to complete its route is uniformly distributed between 85 and 109 minutes. $e)$ What is the probability the bus takes less than 90 or more than 95 minutes to complete the route? I have done the following $f(x) = \frac{1}{109-85} = \frac{1}{24}$ In order to find for the probability for more than 95 minutes to complete the route, I did the following for the uniform distribution $P(x > 95) = (109 - 95) * 1/24 = 0.583333$ $P(x < 90) = (90 - 85) * \frac{1}{24} = 0.2083333 \ldots$ Do I just add $0.583333 \ldots + 0.2083333\ldots?$","The time, $X$, it takes a commuter bus to complete its route is uniformly distributed between 85 and 109 minutes. $e)$ What is the probability the bus takes less than 90 or more than 95 minutes to complete the route? I have done the following $f(x) = \frac{1}{109-85} = \frac{1}{24}$ In order to find for the probability for more than 95 minutes to complete the route, I did the following for the uniform distribution $P(x > 95) = (109 - 95) * 1/24 = 0.583333$ $P(x < 90) = (90 - 85) * \frac{1}{24} = 0.2083333 \ldots$ Do I just add $0.583333 \ldots + 0.2083333\ldots?$",,"['probability', 'statistics', 'standard-deviation', 'means']"
89,Cant Find the Height of this Uniform Probability Distribution,Cant Find the Height of this Uniform Probability Distribution,,"The waiting times between a subway departure schedule and the arrival of a passenger are uniformly distributed between 0 and 8 minutes. Find the probability that a randomly selected passenger has a waiting time less than 4.75 minutes For a density curve to be a graph of a continuous probability​ distribution, it must have a total area under the curve equal to 1 and the curve cannot fall below the​ x-axis The length of the uniform distribution is the difference between the maximum and minimum values In this​ situation, the length of the uniform distribution is given by the following equation. 8-0= 8 min Since the uniform distribution is​ rectangular, has a length of 8  and an area of​ 1, determine the height of the uniform​ distribution, rounding to two decimal places. I am not sure how to determine the height for this question","The waiting times between a subway departure schedule and the arrival of a passenger are uniformly distributed between 0 and 8 minutes. Find the probability that a randomly selected passenger has a waiting time less than 4.75 minutes For a density curve to be a graph of a continuous probability​ distribution, it must have a total area under the curve equal to 1 and the curve cannot fall below the​ x-axis The length of the uniform distribution is the difference between the maximum and minimum values In this​ situation, the length of the uniform distribution is given by the following equation. 8-0= 8 min Since the uniform distribution is​ rectangular, has a length of 8  and an area of​ 1, determine the height of the uniform​ distribution, rounding to two decimal places. I am not sure how to determine the height for this question",,"['statistics', 'probability-distributions']"
90,Prove $ 1-\frac{\sum{(z_x - z_y)}^2}{2(n-1)} $ = $\frac{\sum{(z_x*z_y)}}{n-1}$,Prove  =, 1-\frac{\sum{(z_x - z_y)}^2}{2(n-1)}  \frac{\sum{(z_x*z_y)}}{n-1},I am trying to prove the following equality through algebraic means based on the textbook Applied multiple regression/correlation analysis or the behavioural sciences by Cohen et al. (pages 27-28) The formulas are supposedly for the Pearson Correlation Coefficient. $$ 1-\frac{\sum{(z_x - z_y)}^2}{2(n-1)}  = \frac{\sum{(z_x*z_y)}}{n-1}$$ Here $z_x$ and $z_y$ represent z scores for (scores that have been transformed by subtracting the mean from them and dividing by the standard deviation) for two different variables. You are summing over the total number of pairs of scores. I have tried by manipulating the left hand side of the equation by expanding the terms and converting the 1 to $\frac{n-1}{n-1}$ but I am having no success. Apparently the left hand side can be converted to the right hand side of the equation using only basic algebra. Can someone tell me what I am missing. Thanks,I am trying to prove the following equality through algebraic means based on the textbook Applied multiple regression/correlation analysis or the behavioural sciences by Cohen et al. (pages 27-28) The formulas are supposedly for the Pearson Correlation Coefficient. $$ 1-\frac{\sum{(z_x - z_y)}^2}{2(n-1)}  = \frac{\sum{(z_x*z_y)}}{n-1}$$ Here $z_x$ and $z_y$ represent z scores for (scores that have been transformed by subtracting the mean from them and dividing by the standard deviation) for two different variables. You are summing over the total number of pairs of scores. I have tried by manipulating the left hand side of the equation by expanding the terms and converting the 1 to $\frac{n-1}{n-1}$ but I am having no success. Apparently the left hand side can be converted to the right hand side of the equation using only basic algebra. Can someone tell me what I am missing. Thanks,,"['algebra-precalculus', 'statistics', 'correlation']"
91,how to set values in a conjugate prior,how to set values in a conjugate prior,,"Say I'm trying to set a bayesian prior for a Bernoulli trial of coin flips. The equation I'm interested in is the $p(x|I)$ from the Bayesian numerator: $$P(x|data)\propto x^{N_H}(1-x)^{N_t}p(x|I)$$ where $I$ is the background information. NOTE: The Bayes denominator takes the form (for a uniform prior $p(x|I)$): $$\int_0^1{x^{N_H}(1-x)^{N_t}p(x|I)} = \frac{\Gamma(N_H+1)\Gamma(N-N_H-1)}{\Gamma(N+2)}$$ Alternative, if I choose the conjugate prior $p(x|i)=x^{\alpha}(1-x)^\beta$, the form of the Bayes denominator stays the same: $$\int_0^1{x^{N_H}(1-x)^{N_t}p(x|I)} = \frac{\Gamma(N_H+\beta+1)\Gamma(N-N_H+\alpha+1)}{\Gamma(N+\alpha+\beta+2)}$$ Now, say I'm interested in using this conjugate prior with the following data: HTHTTHTTTHHTHTHTTTTHHTHTTTHTHTHHTTH   $$N=35, N_H=15, N_T=20$$ I feel like the way I choose the $\alpha$ and $\beta$ parameters for $p(x|I)=x^{\alpha}(1-x)^\beta$ is to calculate the mean and stdev for the given data and then somehow ""fit"" those values to the prior. But I can't seem to figure out the mechanics of doing so. Hoping someone can put me on the right track","Say I'm trying to set a bayesian prior for a Bernoulli trial of coin flips. The equation I'm interested in is the $p(x|I)$ from the Bayesian numerator: $$P(x|data)\propto x^{N_H}(1-x)^{N_t}p(x|I)$$ where $I$ is the background information. NOTE: The Bayes denominator takes the form (for a uniform prior $p(x|I)$): $$\int_0^1{x^{N_H}(1-x)^{N_t}p(x|I)} = \frac{\Gamma(N_H+1)\Gamma(N-N_H-1)}{\Gamma(N+2)}$$ Alternative, if I choose the conjugate prior $p(x|i)=x^{\alpha}(1-x)^\beta$, the form of the Bayes denominator stays the same: $$\int_0^1{x^{N_H}(1-x)^{N_t}p(x|I)} = \frac{\Gamma(N_H+\beta+1)\Gamma(N-N_H+\alpha+1)}{\Gamma(N+\alpha+\beta+2)}$$ Now, say I'm interested in using this conjugate prior with the following data: HTHTTHTTTHHTHTHTTTTHHTHTTTHTHTHHTTH   $$N=35, N_H=15, N_T=20$$ I feel like the way I choose the $\alpha$ and $\beta$ parameters for $p(x|I)=x^{\alpha}(1-x)^\beta$ is to calculate the mean and stdev for the given data and then somehow ""fit"" those values to the prior. But I can't seem to figure out the mechanics of doing so. Hoping someone can put me on the right track",,"['probability', 'statistics', 'bayesian']"
92,"If Cov(A,B) = 0, what can be said about Cov(|A|, |B|)?","If Cov(A,B) = 0, what can be said about Cov(|A|, |B|)?",,"If I have two random variables $A$ and $B$ taking values in $[-1,1]$ (where both $1$ and $-1$ have some non-zero weight), and I know that $Cov(A,B) = 0$, can anything at all be said about $Cov(|A|, |B|)$? If not, would anyone possibly be able to give me an example where $Cov(A,B)=0$, but $Cov(|A|,|B|)$ can be tuned arbitrarily?","If I have two random variables $A$ and $B$ taking values in $[-1,1]$ (where both $1$ and $-1$ have some non-zero weight), and I know that $Cov(A,B) = 0$, can anything at all be said about $Cov(|A|, |B|)$? If not, would anyone possibly be able to give me an example where $Cov(A,B)=0$, but $Cov(|A|,|B|)$ can be tuned arbitrarily?",,"['probability', 'statistics', 'covariance']"
93,Simple Linear Regression: why do we estimate conditionla mean when we can estimate the parameters?,Simple Linear Regression: why do we estimate conditionla mean when we can estimate the parameters?,,"If we can estimate $\beta_0$, $\beta_1$ and $\sigma^2$ in a simple linear regression model, why do we want to estimate the conditional mean $\beta_0+\beta_1x_0$ at a value $x_0$? I mean we have already all the information that we might possibly need. If I want to estimate the conditional mean I can just substitute the estimates $\hat{b_1}$ and $\hat{b_0}$ to obtain $\hat{y}=\hat{b_0}+\hat{b_1}x_0$ which is indeed the fitted value.","If we can estimate $\beta_0$, $\beta_1$ and $\sigma^2$ in a simple linear regression model, why do we want to estimate the conditional mean $\beta_0+\beta_1x_0$ at a value $x_0$? I mean we have already all the information that we might possibly need. If I want to estimate the conditional mean I can just substitute the estimates $\hat{b_1}$ and $\hat{b_0}$ to obtain $\hat{y}=\hat{b_0}+\hat{b_1}x_0$ which is indeed the fitted value.",,"['statistics', 'regression', 'parameter-estimation', 'linear-regression']"
94,Pdf of limiting distribution of $T_n = X_n - n$,Pdf of limiting distribution of,T_n = X_n - n,"Given $T_n$ is a discrete r.v. with pmf $\ f_{X_n}(t) =  \frac{1}{n}$ for $x = n^2$, $ 1-\frac{1}{n}$ for $x=0$,  and $\ 0$ otherwise. Define $T_n = X_n - E(X_n)$. Find the pmf (or pdf) of the limiting distribution. My attempt: First, since $X_n$ is a discrete r.v, $E(X_n) = n^2\frac{1}{n} + 0(1-\frac{1}{n}) + 0 = n$. Thus $T_n = X_n - n$. Now, $F_{T_n}(t) = P(T_n\leq t) = P(X_n\leq n+t) = 1$ if $t\geq n^2-n$, $= 1-\frac{1}{n}$ if $-n\leq t < n^2 - n$ and $= 0$ if $t< -n$. Thus, as $n\rightarrow \infty$, $F_{T_n}(t) = 1$ for $t\in (-\infty, \infty)$ (is this a correct interval?), we conclude that $T_1, T_2, \ldots$ converges in distribution to a degenerate r.v $T$ whose pmf is $f_{T}(t) = 1$. My question: Could someone please help verify if my solution above is correct?","Given $T_n$ is a discrete r.v. with pmf $\ f_{X_n}(t) =  \frac{1}{n}$ for $x = n^2$, $ 1-\frac{1}{n}$ for $x=0$,  and $\ 0$ otherwise. Define $T_n = X_n - E(X_n)$. Find the pmf (or pdf) of the limiting distribution. My attempt: First, since $X_n$ is a discrete r.v, $E(X_n) = n^2\frac{1}{n} + 0(1-\frac{1}{n}) + 0 = n$. Thus $T_n = X_n - n$. Now, $F_{T_n}(t) = P(T_n\leq t) = P(X_n\leq n+t) = 1$ if $t\geq n^2-n$, $= 1-\frac{1}{n}$ if $-n\leq t < n^2 - n$ and $= 0$ if $t< -n$. Thus, as $n\rightarrow \infty$, $F_{T_n}(t) = 1$ for $t\in (-\infty, \infty)$ (is this a correct interval?), we conclude that $T_1, T_2, \ldots$ converges in distribution to a degenerate r.v $T$ whose pmf is $f_{T}(t) = 1$. My question: Could someone please help verify if my solution above is correct?",,"['statistics', 'statistical-inference']"
95,How to find density of a given finction.,How to find density of a given finction.,,I was solving the following problem : X is a continuous random variable with density $f_X(x) = 1 - |x|$ for $-1<x<1$ and $0$ elsewhere. I need to find the density of $Y= |X|$. Can anyone help me how to proceed this problem? I am quite new to statistics subject. Thanks for the help.,I was solving the following problem : X is a continuous random variable with density $f_X(x) = 1 - |x|$ for $-1<x<1$ and $0$ elsewhere. I need to find the density of $Y= |X|$. Can anyone help me how to proceed this problem? I am quite new to statistics subject. Thanks for the help.,,"['probability', 'statistics']"
96,Standard normal variable median and quantile. [closed],Standard normal variable median and quantile. [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question $y$ is standard normal random variable and  $x=|y|$. What will be the median of $x$? $P (1 < x < 2)$? $0.80$ quantile of $x$?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question $y$ is standard normal random variable and  $x=|y|$. What will be the median of $x$? $P (1 < x < 2)$? $0.80$ quantile of $x$?",,"['probability', 'statistics', 'random-variables']"
97,Bias-variance decomposition,Bias-variance decomposition,,"I found the following in The Elements of Statistical Learning . Suppose we have 1000 training examples $x_i$ generated uniformly on $[-1,1]^p$. Assume that the true relationship between $X$ and $Y$ is  $$Y = f(X) = e^{-8||X||^2}$$ without any measurement error. We use the 1-nearest-neighbour rule to predict $y_0$ at the test-point $x_0 = 0$. Denote the training set by $\mathcal T$. We can compute the expected prediction error at $x_0$ for our procedure, averaging over all such examples of size 1000. Since the problem is deterministic, this is the mean squared error (MSE) for estimating $f(0)$: \begin{align}\text{MSE}(x_0) &= E_\mathcal T[f(x_0)-\hat y_0]^2\\ &= E_\mathcal T[\hat y_0 - E_\mathcal T(\hat y_0)]^2 + [E_\mathcal T(\hat y_0)-f(x_0)]^2\\ &= \mbox{Var}_\mathcal T(\hat y_0) + \mbox{Bias}^2(\hat y_0) \end{align} I don't quite understand what they mean by the problem being deterministic. Also, how exactly do they get from the first line to the second line? I played around with the binomial formula but I can't seem to derive this.","I found the following in The Elements of Statistical Learning . Suppose we have 1000 training examples $x_i$ generated uniformly on $[-1,1]^p$. Assume that the true relationship between $X$ and $Y$ is  $$Y = f(X) = e^{-8||X||^2}$$ without any measurement error. We use the 1-nearest-neighbour rule to predict $y_0$ at the test-point $x_0 = 0$. Denote the training set by $\mathcal T$. We can compute the expected prediction error at $x_0$ for our procedure, averaging over all such examples of size 1000. Since the problem is deterministic, this is the mean squared error (MSE) for estimating $f(0)$: \begin{align}\text{MSE}(x_0) &= E_\mathcal T[f(x_0)-\hat y_0]^2\\ &= E_\mathcal T[\hat y_0 - E_\mathcal T(\hat y_0)]^2 + [E_\mathcal T(\hat y_0)-f(x_0)]^2\\ &= \mbox{Var}_\mathcal T(\hat y_0) + \mbox{Bias}^2(\hat y_0) \end{align} I don't quite understand what they mean by the problem being deterministic. Also, how exactly do they get from the first line to the second line? I played around with the binomial formula but I can't seem to derive this.",,"['statistics', 'machine-learning', 'variance']"
98,Homogeneity or heterogeneity of variance,Homogeneity or heterogeneity of variance,,"The international comparative school performance study PIRLS raised in 2011 the reading competences of the fourth graders in more than $40$ countries, among others in Germany. They would now like to investigate how the cultural capital of the parents affects the reading competencies of the children in the fourth class. They operationalize the cultural capital as the volume of the books in the parents' house. They want to find out if there is a significant difference between children whose parents have over $100$ books, and children whose parents have a maximum of one hundred books. A table of statistic is given: where Mittelwert=mean value, Standardabweichung=standard deviation, Standardfehler des Mittelswertes=standard error of mean value. We have to compute a T-Test to find out, if we can confirm out hypothesis and to get a statistic significant relation.  First we have to prove with a F-Test, if we have to apply a double T-Test  or a Test of Welch.  At the F-Test and the T-Test we have a significance level of $5 \%$. $$$$ How can we check what T-Test we have to apply? The null hypothesis is that when the parents have more than $100$ books then the children are better in reading than others, or not? Do we get from that that we don't need a double T-Test? Also using the F-Test how can we check if there is an homogeneity or a heterogeneity of variance? Do  we have to use this formula ?","The international comparative school performance study PIRLS raised in 2011 the reading competences of the fourth graders in more than $40$ countries, among others in Germany. They would now like to investigate how the cultural capital of the parents affects the reading competencies of the children in the fourth class. They operationalize the cultural capital as the volume of the books in the parents' house. They want to find out if there is a significant difference between children whose parents have over $100$ books, and children whose parents have a maximum of one hundred books. A table of statistic is given: where Mittelwert=mean value, Standardabweichung=standard deviation, Standardfehler des Mittelswertes=standard error of mean value. We have to compute a T-Test to find out, if we can confirm out hypothesis and to get a statistic significant relation.  First we have to prove with a F-Test, if we have to apply a double T-Test  or a Test of Welch.  At the F-Test and the T-Test we have a significance level of $5 \%$. $$$$ How can we check what T-Test we have to apply? The null hypothesis is that when the parents have more than $100$ books then the children are better in reading than others, or not? Do we get from that that we don't need a double T-Test? Also using the F-Test how can we check if there is an homogeneity or a heterogeneity of variance? Do  we have to use this formula ?",,"['statistics', 'hypothesis-testing', 'variance']"
99,Probability with loaded and fair dice,Probability with loaded and fair dice,,"I own five different six-sided dice. Four of the dice are fair dice, meaning they have values 1, 2, 3, 4, 5, 6. However, one of the dice is loaded; thus, it never shows 1, 2 or 3, but is equally likely to show the values 4, 5, or 6. For my experiment, I will pick up one random dice and roll it twice. The first thing I would like to calculate is the probability of getting two sixes. To calculate this, I first calculated the probability of getting one six and multiplied it by two. Suppose $S$ = event that two sixes are rolled. $$P(S) = 2(\frac45(\frac16) + \frac15(\frac13)) = .4 $$ However, I am not sure if this is correct. I need to calculate this because I would also like to calculate $P(L|S)$ where L = event that a loaded die was picked. Additionally, I feel this is incorrect, because if I change the '2' to a '10' to calculate it for 10 rolls instead of 2, I get a value over 1 which makes no sense. To summarize, how can I calculate $P(S)$ properly so I can calculate $P(L|S)$?","I own five different six-sided dice. Four of the dice are fair dice, meaning they have values 1, 2, 3, 4, 5, 6. However, one of the dice is loaded; thus, it never shows 1, 2 or 3, but is equally likely to show the values 4, 5, or 6. For my experiment, I will pick up one random dice and roll it twice. The first thing I would like to calculate is the probability of getting two sixes. To calculate this, I first calculated the probability of getting one six and multiplied it by two. Suppose $S$ = event that two sixes are rolled. $$P(S) = 2(\frac45(\frac16) + \frac15(\frac13)) = .4 $$ However, I am not sure if this is correct. I need to calculate this because I would also like to calculate $P(L|S)$ where L = event that a loaded die was picked. Additionally, I feel this is incorrect, because if I change the '2' to a '10' to calculate it for 10 rolls instead of 2, I get a value over 1 which makes no sense. To summarize, how can I calculate $P(S)$ properly so I can calculate $P(L|S)$?",,"['probability', 'statistics', 'elementary-set-theory', 'dice']"
