,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"If n people enter a hall through any one of n doors, what is the probability at least one door is not used?","If n people enter a hall through any one of n doors, what is the probability at least one door is not used?",,"The probem is I have 2 conflicting solutions. Solution 1: Since there is $n$ people who each choose any one of $n$ doors, the total number of ways $n$ people enter the hall is $n^n$. (correct?) then I used, $P($one door not chosen$)=1-P($all doors chosen$)$ which (to me) implies a 1 to 1 matching of people to doors, which can only happen in $n!$ ways (equivalent to the number of arrangements of $n$ objects). ie. $P($all doors chosen$)=\dfrac{n!}{n^n}$ and so the required probability is $1-\dfrac{n!}{n^n}$ Solution 2: Consider all cases where at least $1$ door is not used: If $n$ people enter through $1$ door there is $1^n=1$ arrangements If $n$ people enter through $2$ doors there is $2^n$ arrangements If $n$ people enter through $3$ doors there is $3^n$ arrangements ... If $n-1$ people enter through $n-1$ doors there is $(n-1)^n$ arrangements (Note: $n$ cannot enter through all $n$ doors or required condition is not met) In this formulation the required probability is given by  $\dfrac{1^n+2^n+3^n+...+(n-1)^n}{n^n}$ So which solution (if either) is correct?","The probem is I have 2 conflicting solutions. Solution 1: Since there is $n$ people who each choose any one of $n$ doors, the total number of ways $n$ people enter the hall is $n^n$. (correct?) then I used, $P($one door not chosen$)=1-P($all doors chosen$)$ which (to me) implies a 1 to 1 matching of people to doors, which can only happen in $n!$ ways (equivalent to the number of arrangements of $n$ objects). ie. $P($all doors chosen$)=\dfrac{n!}{n^n}$ and so the required probability is $1-\dfrac{n!}{n^n}$ Solution 2: Consider all cases where at least $1$ door is not used: If $n$ people enter through $1$ door there is $1^n=1$ arrangements If $n$ people enter through $2$ doors there is $2^n$ arrangements If $n$ people enter through $3$ doors there is $3^n$ arrangements ... If $n-1$ people enter through $n-1$ doors there is $(n-1)^n$ arrangements (Note: $n$ cannot enter through all $n$ doors or required condition is not met) In this formulation the required probability is given by  $\dfrac{1^n+2^n+3^n+...+(n-1)^n}{n^n}$ So which solution (if either) is correct?",,['probability']
1,Is probability objective?,Is probability objective?,,"As we know, probability is a measure of events. However, is it an objectively attribute of events, or just an illusion in ones' mind? For example, suppose that there is an empty black box with an autoclosing cover in its top-side. And 2 players: Alice and Bob. Then 1)Alice put a white ball and a black ball into the box. Bob saw the process. 2)Alice closed her eyes. 3)Bob took out one ball and ate it up. He knew which ball he took whereas Alice did not know(but she knew Bob had taken one ball). 4)Alice opened her eyes. 5)They took a paper respectively, and wrote down the probability of 'the box has a white ball'. Now we are sure that numbers they wrote are different, Alice must wrote a number equals to 0.5 whereas Bob's was either 0 or 1. So my question is 'who is correct?' And furthermore if both are right then does one event can has more than one probability at the same time? If only one is right, why the other is wrong?","As we know, probability is a measure of events. However, is it an objectively attribute of events, or just an illusion in ones' mind? For example, suppose that there is an empty black box with an autoclosing cover in its top-side. And 2 players: Alice and Bob. Then 1)Alice put a white ball and a black ball into the box. Bob saw the process. 2)Alice closed her eyes. 3)Bob took out one ball and ate it up. He knew which ball he took whereas Alice did not know(but she knew Bob had taken one ball). 4)Alice opened her eyes. 5)They took a paper respectively, and wrote down the probability of 'the box has a white ball'. Now we are sure that numbers they wrote are different, Alice must wrote a number equals to 0.5 whereas Bob's was either 0 or 1. So my question is 'who is correct?' And furthermore if both are right then does one event can has more than one probability at the same time? If only one is right, why the other is wrong?",,"['probability', 'probability-theory', 'information-theory', 'philosophy']"
2,How a random variable takes a value?,How a random variable takes a value?,,"Why do mathematicians say a random variable takes on a value?  Is it just for convenience? My understanding is that a random variable is a function mapping the sample space of an experiment to a codomain of interest.  When I read ""$X$ takes on the value ... "" , I take it to mean we apply $X$ to the outcome $s$ of some experiment, and use the value of this for some purpose (e.g. assign a probability to it).  That is, we use $a$ when $a=X(s)$. I don't understand when people say formally or informally things like $X=4$ or even $X=X(s)$.  Isn't this wrong?","Why do mathematicians say a random variable takes on a value?  Is it just for convenience? My understanding is that a random variable is a function mapping the sample space of an experiment to a codomain of interest.  When I read ""$X$ takes on the value ... "" , I take it to mean we apply $X$ to the outcome $s$ of some experiment, and use the value of this for some purpose (e.g. assign a probability to it).  That is, we use $a$ when $a=X(s)$. I don't understand when people say formally or informally things like $X=4$ or even $X=X(s)$.  Isn't this wrong?",,"['probability', 'notation', 'random-variables']"
3,Repeatedly rolling a die and the tails of the multinomial distribution.,Repeatedly rolling a die and the tails of the multinomial distribution.,,"For $1\leq i\leq n$ let $X_i$ be independent random variables, and let each $X_i$ be the uniform distribution on the set ${0,1,2,\dots,m}$ so that $X_i$ is like an $m+1$ sided die.  Let $$Y=\frac{1}{n}\sum_{i=1}^n \frac{1}{m} X_i,$$ so that $\mathbb{E}(Y)=\frac{1}{2}$.  I am interested in the tails of this distribution, that is the size of $$\Pr\left( Y \geq k\right)$$ where $\frac{1}{2}< k\leq 1$ is a constant. In the case where $m=1$, we are looking at the binomial distribution, and $$\Pr\left( Y \geq k\right)= \frac{1}{2^n}\sum_{i=0}^{(1-k) n} \binom{n}{i}$$ and we can bound this above by $(1-k)n \binom{n}{(1-k)n}$ and below by $\binom{n}{(1-k)n}$ which yields $$\Pr\left( Y \geq k\right)\approx \frac{1}{2^n} e^{n H(k)}$$ where $H(x)=-\left(x\log x+(1-x)\log (1-x)\right)$ is the entropy function.  (I use approx liberally) What kind of similar bounds do we have on the tails of this distribution when  $m\geq 2$?  I am looking to use the explicit multinomial properties to get something stronger than what you would get using Chernoff of Hoeffding.","For $1\leq i\leq n$ let $X_i$ be independent random variables, and let each $X_i$ be the uniform distribution on the set ${0,1,2,\dots,m}$ so that $X_i$ is like an $m+1$ sided die.  Let $$Y=\frac{1}{n}\sum_{i=1}^n \frac{1}{m} X_i,$$ so that $\mathbb{E}(Y)=\frac{1}{2}$.  I am interested in the tails of this distribution, that is the size of $$\Pr\left( Y \geq k\right)$$ where $\frac{1}{2}< k\leq 1$ is a constant. In the case where $m=1$, we are looking at the binomial distribution, and $$\Pr\left( Y \geq k\right)= \frac{1}{2^n}\sum_{i=0}^{(1-k) n} \binom{n}{i}$$ and we can bound this above by $(1-k)n \binom{n}{(1-k)n}$ and below by $\binom{n}{(1-k)n}$ which yields $$\Pr\left( Y \geq k\right)\approx \frac{1}{2^n} e^{n H(k)}$$ where $H(x)=-\left(x\log x+(1-x)\log (1-x)\right)$ is the entropy function.  (I use approx liberally) What kind of similar bounds do we have on the tails of this distribution when  $m\geq 2$?  I am looking to use the explicit multinomial properties to get something stronger than what you would get using Chernoff of Hoeffding.",,"['probability', 'probability-theory', 'inequality', 'probability-distributions', 'distribution-tails']"
4,"Modified two child problem. Find the probability that both are girls, given that at least one is a girl born in March.","Modified two child problem. Find the probability that both are girls, given that at least one is a girl born in March.",,"A family has two children. Assume that birth month is independent of gender,  with boys and girls equally likely and all months equally likely, and assume that the  elder child’s characteristics are independent of the younger child’s characteristics). What is the probability that both are girls, given that at least one is a girl who was  born in March.","A family has two children. Assume that birth month is independent of gender,  with boys and girls equally likely and all months equally likely, and assume that the  elder child’s characteristics are independent of the younger child’s characteristics). What is the probability that both are girls, given that at least one is a girl who was  born in March.",,['probability']
5,Probability of Heads in a coin,Probability of Heads in a coin,,"I was wondering, if you flip a fair coin $5$ times, whether you can calculate the probability of getting at least one head is calculated like this: You can do the complement of getting at least one head which is TTTTT: $\dfrac1{2^5} =\dfrac1{32}$ Then you do $$1-\frac1{32}= \frac{31}{32}\;,$$ so that's the possibility of getting at least one head after a flip? Thanks!","I was wondering, if you flip a fair coin $5$ times, whether you can calculate the probability of getting at least one head is calculated like this: You can do the complement of getting at least one head which is TTTTT: $\dfrac1{2^5} =\dfrac1{32}$ Then you do $$1-\frac1{32}= \frac{31}{32}\;,$$ so that's the possibility of getting at least one head after a flip? Thanks!",,['probability']
6,Sex distribution,Sex distribution,,"Suppose there are N male and N female students. They are randomly distributed into k groups. Is it more probable for a male student to find himself in a group with more guys and for female student to find herself in a group with more girls? The question is motivated by an argument with my mother. She claimed that in the majority of collectives where she was, the number of women besides her was greater than men, while I had the opposite impression, that in most collectives where I was a member there was more men even if not to count myself. I never was in a majority-girls collective, so I think for a male it is more probable to find oneself in a majority-male collective (even if we exclude oneself).","Suppose there are N male and N female students. They are randomly distributed into k groups. Is it more probable for a male student to find himself in a group with more guys and for female student to find herself in a group with more girls? The question is motivated by an argument with my mother. She claimed that in the majority of collectives where she was, the number of women besides her was greater than men, while I had the opposite impression, that in most collectives where I was a member there was more men even if not to count myself. I never was in a majority-girls collective, so I think for a male it is more probable to find oneself in a majority-male collective (even if we exclude oneself).",,"['probability', 'probability-theory']"
7,Expectation Problem (Khintchine's Inequality),Expectation Problem (Khintchine's Inequality),,"As a reference, this is Problem 6.2 in Albiac and Kalton's Topics in Banach Space Theory The question involves a direct proof of Khintchine's Inequality.  In part (1), we are to prove that $\cosh(t)\leq e^{t^2/2}$ for all $t\in\mathbb{R}$.  Part (2) (which I don't think I need yet for the step I am stuck on) is to show that for $p\geq1$, $t^p\leq p^pe^{-p}e^t$ for I assume $t>0$.  These steps I have done. The part I am stuck on is this: Let $(\varepsilon_n)_{n=1}^\infty$ be a sequence of Rademachers (i.e. independent random variables taking on values $\pm1$ with equal probability), and let $(a_k)_{k=1}^\infty\subset\mathbb{R}$ such that $$\underset{k=1}{\overset{n}{\sum}}a_k^2=1$$ and define $$f=\underset{k=1}{\overset{n}{\sum}}a_k\varepsilon_k$$ We want to show that $$\mathbb{E}(e^f)\leq e$$ (If you have any doubts, this is expectation over the epsilons). Here has been my attempt so far: $$\mathbb{E}(e^f)=\mathbb{E}\left(\exp\left(\underset{k=1}{\overset{n}{\sum}}a_k\varepsilon_k\right)\right)=\mathbb{E}\left(\underset{k=1}{\overset{n}{\prod}}\exp(a_k\varepsilon_k)\right)=\underset{k=1}{\overset{n}{\prod}}\left(\mathbb{E}\exp(a_k\varepsilon_k)\right)\leq\underset{k=1}{\overset{n}{\prod}}\left(\mathbb{E}(2\cosh(a_k\varepsilon_k))\right)$$ the second to last equality is by independence, and the last inequality here follows from the definition of $\cosh$.  Then by part (1), we have $$\leq\underset{k=1}{\overset{n}{\prod}}\left(\mathbb{E}\left(2\exp\left(\frac{a_k^2\varepsilon_k^2}{2}\right)\right)\right)$$ But since $\varepsilon_k^2=1$ for all $k$, and we can move the expectation back outside, and put the product back into the exponent as a sum, we will get something like $$2^n\exp\left(\underset{k=1}{\overset{n}{\sum}}\frac{a_k^2}{2}\right)=2^n\exp\left(\frac{1}{2}\right)$$ by assumption on the sum of $a_n^2$.  So this does not give the inequality we want. Any suggestions or hints would be appreciated (be gentle, I am far from knowledgeable of probability theory)","As a reference, this is Problem 6.2 in Albiac and Kalton's Topics in Banach Space Theory The question involves a direct proof of Khintchine's Inequality.  In part (1), we are to prove that $\cosh(t)\leq e^{t^2/2}$ for all $t\in\mathbb{R}$.  Part (2) (which I don't think I need yet for the step I am stuck on) is to show that for $p\geq1$, $t^p\leq p^pe^{-p}e^t$ for I assume $t>0$.  These steps I have done. The part I am stuck on is this: Let $(\varepsilon_n)_{n=1}^\infty$ be a sequence of Rademachers (i.e. independent random variables taking on values $\pm1$ with equal probability), and let $(a_k)_{k=1}^\infty\subset\mathbb{R}$ such that $$\underset{k=1}{\overset{n}{\sum}}a_k^2=1$$ and define $$f=\underset{k=1}{\overset{n}{\sum}}a_k\varepsilon_k$$ We want to show that $$\mathbb{E}(e^f)\leq e$$ (If you have any doubts, this is expectation over the epsilons). Here has been my attempt so far: $$\mathbb{E}(e^f)=\mathbb{E}\left(\exp\left(\underset{k=1}{\overset{n}{\sum}}a_k\varepsilon_k\right)\right)=\mathbb{E}\left(\underset{k=1}{\overset{n}{\prod}}\exp(a_k\varepsilon_k)\right)=\underset{k=1}{\overset{n}{\prod}}\left(\mathbb{E}\exp(a_k\varepsilon_k)\right)\leq\underset{k=1}{\overset{n}{\prod}}\left(\mathbb{E}(2\cosh(a_k\varepsilon_k))\right)$$ the second to last equality is by independence, and the last inequality here follows from the definition of $\cosh$.  Then by part (1), we have $$\leq\underset{k=1}{\overset{n}{\prod}}\left(\mathbb{E}\left(2\exp\left(\frac{a_k^2\varepsilon_k^2}{2}\right)\right)\right)$$ But since $\varepsilon_k^2=1$ for all $k$, and we can move the expectation back outside, and put the product back into the exponent as a sum, we will get something like $$2^n\exp\left(\underset{k=1}{\overset{n}{\sum}}\frac{a_k^2}{2}\right)=2^n\exp\left(\frac{1}{2}\right)$$ by assumption on the sum of $a_n^2$.  So this does not give the inequality we want. Any suggestions or hints would be appreciated (be gentle, I am far from knowledgeable of probability theory)",,"['probability', 'functional-analysis', 'banach-spaces']"
8,probability of being dealt four of a kind in poker,probability of being dealt four of a kind in poker,,I have to resolve this exercise: I have 52 cards. I get 5 cards. Calculate the probability I get a poker hand of four-of-a-kind. well I applied  the formula ${52 \choose 5}=\frac{52!}{5!47!}$ to calculate every possible combination.but after here I don't know how can go ahead. Some help? Edit I made mistake.Now it's correct,I have to resolve this exercise: I have 52 cards. I get 5 cards. Calculate the probability I get a poker hand of four-of-a-kind. well I applied  the formula ${52 \choose 5}=\frac{52!}{5!47!}$ to calculate every possible combination.but after here I don't know how can go ahead. Some help? Edit I made mistake.Now it's correct,,['probability']
9,Convergence of Random Variable,Convergence of Random Variable,,"Let $X_1, X_2, ...$ be identically distributed nonnegative random variable with $EX_1<\infty$. Prove that $\frac{X_n}{n}\rightarrow 0$ almost surely. I am trying to use Borel Cantelli Lemma (part one) to prove this.","Let $X_1, X_2, ...$ be identically distributed nonnegative random variable with $EX_1<\infty$. Prove that $\frac{X_n}{n}\rightarrow 0$ almost surely. I am trying to use Borel Cantelli Lemma (part one) to prove this.",,"['probability', 'probability-theory', 'probability-distributions']"
10,Calculating conditional probability given Poisson variable,Calculating conditional probability given Poisson variable,,"I encountered a set of problems while studying statistics for research which I have combined to get a broader question. I want to know if this is a solvable problem with enough information specifically under what assumptions or approach. Given that a minesweeper has encountered exactly 5 landmines in a particular 10 mile stretch, what is the probability that he will encounter exactly 6 landmines on the next 10 mile stretch. (Average number of landmines is 0.6 per mile in the 50 mile stretch) I have figured that the approach involves finding out the Poisson probabilities of the discrete random variable with the combination of Bayes Conditional probability. But am stuck with proceeding on applying the Bayes rule. i.e $\Pr(X=6\mid X=5)$ I know that $\Pr(X=5)=e^{-6}5^6/5!.$  Here $\lambda=0.6\cdot 10$ and $X=5$) Similarly for $\Pr(X=6)$. Is Bayes rule useful here: $P(Y\mid A) = \Pr(A\mid Y)\Pr(Y)/ (\Pr(A\mid Y)\Pr(Y) + \Pr(A\mid N)\Pr(N))$? Would appreciate any hints on proceeding with these types of formulations for broadening my understanding.","I encountered a set of problems while studying statistics for research which I have combined to get a broader question. I want to know if this is a solvable problem with enough information specifically under what assumptions or approach. Given that a minesweeper has encountered exactly 5 landmines in a particular 10 mile stretch, what is the probability that he will encounter exactly 6 landmines on the next 10 mile stretch. (Average number of landmines is 0.6 per mile in the 50 mile stretch) I have figured that the approach involves finding out the Poisson probabilities of the discrete random variable with the combination of Bayes Conditional probability. But am stuck with proceeding on applying the Bayes rule. i.e $\Pr(X=6\mid X=5)$ I know that $\Pr(X=5)=e^{-6}5^6/5!.$  Here $\lambda=0.6\cdot 10$ and $X=5$) Similarly for $\Pr(X=6)$. Is Bayes rule useful here: $P(Y\mid A) = \Pr(A\mid Y)\Pr(Y)/ (\Pr(A\mid Y)\Pr(Y) + \Pr(A\mid N)\Pr(N))$? Would appreciate any hints on proceeding with these types of formulations for broadening my understanding.",,"['probability', 'statistics']"
11,Probability game with three players winning plus losing doesn't equal 1,Probability game with three players winning plus losing doesn't equal 1,,"I have a game of cards numbered 1-10.  Three players draw cards, the highest card wins.  Cards are not replaced. The first player draws a 9.  I want to work out the probability of that player winning and losing. The probability of winning is the product of the two other players picking up lower cards in this case $$\frac{8}{9} \cdot \frac{7}{8} = \frac{56}{72}$$ The probability of losing is the probability that either player draws a higher card (the ten) $$\frac{1}{9} + \frac{1}{8} = \frac{17}{72}$$ Given that player 1 can only win or lose and there is no way that there can be a draw why do these probabilities add to make more than one? I've tried a couple of different values and they always come out more than one.","I have a game of cards numbered 1-10.  Three players draw cards, the highest card wins.  Cards are not replaced. The first player draws a 9.  I want to work out the probability of that player winning and losing. The probability of winning is the product of the two other players picking up lower cards in this case $$\frac{8}{9} \cdot \frac{7}{8} = \frac{56}{72}$$ The probability of losing is the probability that either player draws a higher card (the ten) $$\frac{1}{9} + \frac{1}{8} = \frac{17}{72}$$ Given that player 1 can only win or lose and there is no way that there can be a draw why do these probabilities add to make more than one? I've tried a couple of different values and they always come out more than one.",,['probability']
12,Expected tail and head length of $\rho$ for a finite random function,Expected tail and head length of  for a finite random function,\rho,"Let $F: D \rightarrow D$ be a random function on finite domain $D$ of size $n$.  It is well-known that, from any $x \in D$, iterating $F$ on $x$ traces out a sequence of values $x, F(x), F(F(x)), \ldots$ that must eventually repeat (since $D$ is finite) and resembles a Greek ""$\rho$"".  The expected number of total elements in this $\rho$ is $\sqrt{n\pi/2}$ with half being on the tail and half on the head.  I've seen this stated in several places, but cannot find a proof. Illustration: In order to clarify what I'm asking (since the comments make it clear that this might be useful), fix an integer $n$.  Uniformly sample $a_0 \in [1,n]$ and make a vertex with label $a_0$.  Do this again, obtaining a vertex labeled $a_1$ and make a directed edge from $a_0$ to $a_1$ (this does not preclude $a_0 = a_1$).  The pigeon-hole principle tells us that eventually we will create a cycle in this digraph; we terminate the process when this occurs. Clearly the graph will look like the letter $\rho$: the ""tail"" is the part of the graph before we enter the cycle (the tail might have length zero, of course), and the ""head"" is the is part of the graph comprising the cycle. My question is this: what is the expected length of the tail and head, in terms of $n$.  Asymptotically, the answer is known to be $\sqrt{n\pi/8}$, but I cannot find a derivation.","Let $F: D \rightarrow D$ be a random function on finite domain $D$ of size $n$.  It is well-known that, from any $x \in D$, iterating $F$ on $x$ traces out a sequence of values $x, F(x), F(F(x)), \ldots$ that must eventually repeat (since $D$ is finite) and resembles a Greek ""$\rho$"".  The expected number of total elements in this $\rho$ is $\sqrt{n\pi/2}$ with half being on the tail and half on the head.  I've seen this stated in several places, but cannot find a proof. Illustration: In order to clarify what I'm asking (since the comments make it clear that this might be useful), fix an integer $n$.  Uniformly sample $a_0 \in [1,n]$ and make a vertex with label $a_0$.  Do this again, obtaining a vertex labeled $a_1$ and make a directed edge from $a_0$ to $a_1$ (this does not preclude $a_0 = a_1$).  The pigeon-hole principle tells us that eventually we will create a cycle in this digraph; we terminate the process when this occurs. Clearly the graph will look like the letter $\rho$: the ""tail"" is the part of the graph before we enter the cycle (the tail might have length zero, of course), and the ""head"" is the is part of the graph comprising the cycle. My question is this: what is the expected length of the tail and head, in terms of $n$.  Asymptotically, the answer is known to be $\sqrt{n\pi/8}$, but I cannot find a derivation.",,"['probability', 'random-functions']"
13,expectation of $ \left(\sum_{i=1}^n {x_i} \right)^2 $,expectation of, \left(\sum_{i=1}^n {x_i} \right)^2 ,"If $x_i$ is exponentially distributed $(i=1,...,n)$ with parameter $\lambda$ and $x_i$'s are mutually independent, what is the expectation of $\left(\sum_{i=1}^n {x_i} \right)^2$ in terms of $n$ and $\lambda$ and possibly other constants? Note: This question has gotten a statistical answer on https://stats.stackexchange.com/q/4959/2148 . The readers would take a look at it too.","If $x_i$ is exponentially distributed $(i=1,...,n)$ with parameter $\lambda$ and $x_i$'s are mutually independent, what is the expectation of $\left(\sum_{i=1}^n {x_i} \right)^2$ in terms of $n$ and $\lambda$ and possibly other constants? Note: This question has gotten a statistical answer on https://stats.stackexchange.com/q/4959/2148 . The readers would take a look at it too.",,"['statistics', 'probability']"
14,Contest Problem from Berkeley math tournament - 2023.,Contest Problem from Berkeley math tournament - 2023.,,"Contest Problem from Berkeley math tournament - 2023 : Maria and Skyler have a square shaped cookie with a side length of 1 inch. They split the cookie by choosing two points on distinct sides of the cookie uniformly at random and cutting across the line segment formed by connecting the two points. If Maria always gets the larger piece, what is the expected amount of extra cookie in Maria's piece compared to Skyler's in square inches. Does this problem indicate that the choosing of sides and cutting across in the same cookie every time or a different square shaped cookie.","Contest Problem from Berkeley math tournament - 2023 : Maria and Skyler have a square shaped cookie with a side length of 1 inch. They split the cookie by choosing two points on distinct sides of the cookie uniformly at random and cutting across the line segment formed by connecting the two points. If Maria always gets the larger piece, what is the expected amount of extra cookie in Maria's piece compared to Skyler's in square inches. Does this problem indicate that the choosing of sides and cutting across in the same cookie every time or a different square shaped cookie.",,"['probability', 'solution-verification', 'contest-math', 'expected-value']"
15,"When flipping coins, what is the probability that you'll repeat yourself completely at some point?","When flipping coins, what is the probability that you'll repeat yourself completely at some point?",,"You play the following game: you flip a coin over and over again and write down the sequence of Heads (H) and Tails (T). The game ends when you repeat the whole sequence from the beginning. So for example HH or HTHHHTHH would terminate. What is the probability that the game is finite? It has to be bigger than 50% since this is the probability of starting with HH or TT, ending the game instantly. But the longer the game goes, the less likely it becomes that it ends at all. I ran the game a few times with the computer and it seems that a probability of 75% may be the answer.","You play the following game: you flip a coin over and over again and write down the sequence of Heads (H) and Tails (T). The game ends when you repeat the whole sequence from the beginning. So for example HH or HTHHHTHH would terminate. What is the probability that the game is finite? It has to be bigger than 50% since this is the probability of starting with HH or TT, ending the game instantly. But the longer the game goes, the less likely it becomes that it ends at all. I ran the game a few times with the computer and it seems that a probability of 75% may be the answer.",,"['probability', 'sequences-and-series', 'stochastic-processes']"
16,CMIMC 2016 Math Contest Question - Solution Verification,CMIMC 2016 Math Contest Question - Solution Verification,,"CMIMC 2016 Math Contest Question Shen, Ling, and Ru each place four slips of paper with their name on it into a bucket. They then play the following game: Slips are removed one at a time from the bucket, and whoever has all of their slips removed first wins. Shen cheats, however, and adds an extra slip of paper into the bucket, and will win when four (out of five) of his are drawn. Compute the probability that Shen wins by cheating. The solution is given in this link : https://cmimc.math.cmu.edu/math/past-problems/2016 I would like to challenge that answer with my approach. Let me know where I am making a mistake. I would like to invoke negative multinomial distribution to solve this $P(X = x_o) =\gamma{(n)}\frac{p_0^{x_0}}{\gamma{(x_0)}}$$\prod{\frac{p_i^{x_i}}{x_i!}}$ $P_0 = \frac{5}{13}$ $P_1 = \frac{4}{13}$ , $P_2 = \frac{4}{13}$ $x_0$ = Shen's number of slips, $x_1$ = Ling's number of slips and $x_2$ = Ru's number of slips. According to the Solution posted, answer is $\frac{67}{117} = 0.57265$ Let me know where I am going wrong.","CMIMC 2016 Math Contest Question Shen, Ling, and Ru each place four slips of paper with their name on it into a bucket. They then play the following game: Slips are removed one at a time from the bucket, and whoever has all of their slips removed first wins. Shen cheats, however, and adds an extra slip of paper into the bucket, and will win when four (out of five) of his are drawn. Compute the probability that Shen wins by cheating. The solution is given in this link : https://cmimc.math.cmu.edu/math/past-problems/2016 I would like to challenge that answer with my approach. Let me know where I am making a mistake. I would like to invoke negative multinomial distribution to solve this , = Shen's number of slips, = Ling's number of slips and = Ru's number of slips. According to the Solution posted, answer is Let me know where I am going wrong.",P(X = x_o) =\gamma{(n)}\frac{p_0^{x_0}}{\gamma{(x_0)}}\prod{\frac{p_i^{x_i}}{x_i!}} P_0 = \frac{5}{13} P_1 = \frac{4}{13} P_2 = \frac{4}{13} x_0 x_1 x_2 \frac{67}{117} = 0.57265,"['probability', 'combinatorics', 'solution-verification']"
17,"Bunny & Probability, How to solve this probability?","Bunny & Probability, How to solve this probability?",,"Two boxes, $X$ and $Y$ , each box contains two bunnies, let $x_w$ be the white bunny in box $X$ ,and let $x_b$ be the black bunny in box $X$ . Similarly, $y_w$ be the white bunny in box $Y$ and $y_b$ be the black bunny in box $Y$ . We define the operation as: Simultaneously taking one bunny from $X$ , and taking one bunny from $Y$ , then interchange them to the other box. For example, initially, we have $X=(x_w, x_b), Y=(y_w, y_b)$ , if we choose $x_w$ from $X$ , and $y_b$ from $Y$ , after this operation, we get $X=(y_b, x_b), Y=(y_w, x_w)$ . After we do this operation by $n$ times, what is the probability that each box still contains one white bunny and one black bunny? (Not necessarily to be the original bunnies in boxes.) My attempt: If we define $P_n$ as the probability after $n$ operations. For $n=1$ , we have totally four cases, here we only need to look at one box, say box $X$ , since $Y$ is automatically settled once $X$ is settled. After the first operation, box $X$ could be $(y_w, x_b), (y_b, x_b), (x_w, y_w), (x_w, y_b)$ , so we know $P_1=2/4$ After two operations, each of above four cases will generate another four subcases, totally $16$ subcases, and some of them are repeated, by counting, I get $P_2=12/16$ But how to derive a general formula for $P_n$ ? Update Thank you for your help, and I can understand this problem now. But I have another question. (Should I ask here or put in a new question?) @A.J. uses the three term homogeneous equation, but @Mathfail uses two term inhomogeneous equation, and both give the same result. Is there a more deep connection between the two methods? Can I say, algebraically, the three term homogeneous equation is equivalent to two term inhomogeneous equation?","Two boxes, and , each box contains two bunnies, let be the white bunny in box ,and let be the black bunny in box . Similarly, be the white bunny in box and be the black bunny in box . We define the operation as: Simultaneously taking one bunny from , and taking one bunny from , then interchange them to the other box. For example, initially, we have , if we choose from , and from , after this operation, we get . After we do this operation by times, what is the probability that each box still contains one white bunny and one black bunny? (Not necessarily to be the original bunnies in boxes.) My attempt: If we define as the probability after operations. For , we have totally four cases, here we only need to look at one box, say box , since is automatically settled once is settled. After the first operation, box could be , so we know After two operations, each of above four cases will generate another four subcases, totally subcases, and some of them are repeated, by counting, I get But how to derive a general formula for ? Update Thank you for your help, and I can understand this problem now. But I have another question. (Should I ask here or put in a new question?) @A.J. uses the three term homogeneous equation, but @Mathfail uses two term inhomogeneous equation, and both give the same result. Is there a more deep connection between the two methods? Can I say, algebraically, the three term homogeneous equation is equivalent to two term inhomogeneous equation?","X Y x_w X x_b X y_w Y y_b Y X Y X=(x_w, x_b), Y=(y_w, y_b) x_w X y_b Y X=(y_b, x_b), Y=(y_w, x_w) n P_n n n=1 X Y X X (y_w, x_b), (y_b, x_b), (x_w, y_w), (x_w, y_b) P_1=2/4 16 P_2=12/16 P_n",['probability']
18,Find the number of ways to arrange so that at least two are followed,Find the number of ways to arrange so that at least two are followed,,"Suppose there is a sentence containing only sequences of three characters and nothing more. The three characters are $X,Y,Z$ and it is given that $X$ has occurred $a$ times, $Y$ has occurred $b$ times and $Z$ has occurred $c$ times in the sentence. What is the probability that a $X$ will  be followed by a $Y$ at least $2$ times $?$ This problem looks a bit complicated to me, so I decided to break it into some parts. At least $2$ times means, all cases $-$$($ exactly $0$ time $+$ exactly $1$ time $)$ The number of all cases are simply $$\binom{a+b+c}{a}\binom{b+c}{b}\binom{c}{c}$$ For the exactly $0$ times I'm able to think of a logic but it is hard to explain. I think that we should first select a $Z$ and fix it. Then we arrange $b$ $Y's$ and $(c-1)$ $Z's$ on one side of the fixed $Z$ and on other side put all the $X$ . One more is to put all the $X's$ between two $Z's$ and then do the rest of the arrangement. So you see I'm not been able to think of all such cases. One more is put all $Y's$ and then arrange rest such that no $X$ goes behind any $Y$ . For exactly $1$ I have similar incomplete cases. How to find all possible cases in each sub problem $?$ Any help is greatly appreciated.","Suppose there is a sentence containing only sequences of three characters and nothing more. The three characters are and it is given that has occurred times, has occurred times and has occurred times in the sentence. What is the probability that a will  be followed by a at least times This problem looks a bit complicated to me, so I decided to break it into some parts. At least times means, all cases exactly time exactly time The number of all cases are simply For the exactly times I'm able to think of a logic but it is hard to explain. I think that we should first select a and fix it. Then we arrange and on one side of the fixed and on other side put all the . One more is to put all the between two and then do the rest of the arrangement. So you see I'm not been able to think of all such cases. One more is put all and then arrange rest such that no goes behind any . For exactly I have similar incomplete cases. How to find all possible cases in each sub problem Any help is greatly appreciated.","X,Y,Z X a Y b Z c X Y 2 ? 2 -( 0 + 1 ) \binom{a+b+c}{a}\binom{b+c}{b}\binom{c}{c} 0 Z b Y's (c-1) Z's Z X X's Z's Y's X Y 1 ?","['probability', 'combinatorics']"
19,Limit and conditional expectation commute in a uniformly integrable sequence,Limit and conditional expectation commute in a uniformly integrable sequence,,"I am thinking of the next proposition: Proposition. Let $(\Omega, \mathcal{F}, P)$ a probability space, and $\{X_n\}_{n=1,\cdots}$ a uniformly integrable r.v. sequence s.t. $X_n \rightarrow X ~ \text{a.s.}$ , where $X$ is an $L^1$ r.v. Then for any $\mathcal{G}$ : a sub- $\sigma$ -algebra of $\mathcal{F}$ , we have \begin{equation*} \lim_{n\rightarrow \infty} \mathbb{E}[X_n | \mathcal{G}] = \mathbb{E}[X | \mathcal{G}] \quad \text{a.s.} \end{equation*} My quesiont is: Is this proposition true? I consider it indeed is true, for the following reason. For any $A\in \mathcal{G}$ and $R > 0$ , $\{X_n 1_A\}$ is a u.i. sequence, which is shown by \begin{equation*} \sup_n \mathbb{E}[ |X_n 1_A|, |X_n 1_A| > R ] \leq \sup_n \mathbb{E}[ |X_n |, |X_n| > R ] \end{equation*} with the right hand side going to $0$ with $R \rightarrow \infty$ (we denote $\mathbb{E}[X,A] = \mathbb{E}[X1_A]$ ). Thus by the commutability of conditional expectation and limit in a u.i. sequence we have \begin{equation*} \mathbb{E}[X1_A] = \mathbb{E}[ \lim_{n \rightarrow \infty}X_n 1_A] = \lim_{n \rightarrow \infty} \mathbb{E}[X_n 1_A]. \qquad (1) \end{equation*} On the other hand, \begin{align*} & \mathbb{E}[ \lim_{n \rightarrow \infty} \mathbb{E}[X_n | \mathcal{G}], A] = \mathbb{E}[ ( \lim_{n \rightarrow \infty} \mathbb{E}[X_n | \mathcal{G}]) 1_A]  = \mathbb{E}[ \lim_{n \rightarrow \infty} (\mathbb{E}[X_n | \mathcal{G}] 1_A)] \\ =~& \mathbb{E}[ \lim_{n \rightarrow \infty} (\mathbb{E}[X_n 1_A | \mathcal{G}])]\\ =~& \lim_{n \rightarrow \infty} \mathbb{E}[ \mathbb{E}[X_n 1_A | \mathcal{G}]]\\ =~& \lim_{n \rightarrow \infty} \mathbb{E}[X_n 1_A]. \qquad (2) \end{align*} Here to swap the expectation and the limit I used the fact that the sequence $\{ \mathbb{E}[Y_n | \mathcal{G}] \}_{n=1,\cdots}$ with a u.i. r.v. sequence $\{ Y_n \}_{n=1,\cdots}$ is u.i., which I presume is true. Therefore by (1) and (2), the above proposition holds. A previous post ( https://mathoverflow.net/questions/124589/uniformly-integrable-sequence-such-that-a-s-limit-and-conditional-expectation-d ) claims otherwise, which I guess is wrong: I think one can't compose a u.i. sequence $\{Z_n\} = \{X_n Y_n\}$ that satisfies the conditions written in the link above.","I am thinking of the next proposition: Proposition. Let a probability space, and a uniformly integrable r.v. sequence s.t. , where is an r.v. Then for any : a sub- -algebra of , we have My quesiont is: Is this proposition true? I consider it indeed is true, for the following reason. For any and , is a u.i. sequence, which is shown by with the right hand side going to with (we denote ). Thus by the commutability of conditional expectation and limit in a u.i. sequence we have On the other hand, Here to swap the expectation and the limit I used the fact that the sequence with a u.i. r.v. sequence is u.i., which I presume is true. Therefore by (1) and (2), the above proposition holds. A previous post ( https://mathoverflow.net/questions/124589/uniformly-integrable-sequence-such-that-a-s-limit-and-conditional-expectation-d ) claims otherwise, which I guess is wrong: I think one can't compose a u.i. sequence that satisfies the conditions written in the link above.","(\Omega, \mathcal{F}, P) \{X_n\}_{n=1,\cdots} X_n \rightarrow X ~ \text{a.s.} X L^1 \mathcal{G} \sigma \mathcal{F} \begin{equation*}
\lim_{n\rightarrow \infty} \mathbb{E}[X_n | \mathcal{G}] = \mathbb{E}[X | \mathcal{G}] \quad \text{a.s.}
\end{equation*} A\in \mathcal{G} R > 0 \{X_n 1_A\} \begin{equation*}
\sup_n \mathbb{E}[ |X_n 1_A|, |X_n 1_A| > R ] \leq \sup_n \mathbb{E}[ |X_n |, |X_n| > R ]
\end{equation*} 0 R \rightarrow \infty \mathbb{E}[X,A] = \mathbb{E}[X1_A] \begin{equation*}
\mathbb{E}[X1_A] = \mathbb{E}[ \lim_{n \rightarrow \infty}X_n 1_A] = \lim_{n \rightarrow \infty} \mathbb{E}[X_n 1_A]. \qquad (1)
\end{equation*} \begin{align*}
& \mathbb{E}[ \lim_{n \rightarrow \infty} \mathbb{E}[X_n | \mathcal{G}], A] = \mathbb{E}[ ( \lim_{n \rightarrow \infty} \mathbb{E}[X_n | \mathcal{G}]) 1_A]  = \mathbb{E}[ \lim_{n \rightarrow \infty} (\mathbb{E}[X_n | \mathcal{G}] 1_A)] \\
=~& \mathbb{E}[ \lim_{n \rightarrow \infty} (\mathbb{E}[X_n 1_A | \mathcal{G}])]\\
=~& \lim_{n \rightarrow \infty} \mathbb{E}[ \mathbb{E}[X_n 1_A | \mathcal{G}]]\\
=~& \lim_{n \rightarrow \infty} \mathbb{E}[X_n 1_A]. \qquad (2)
\end{align*} \{ \mathbb{E}[Y_n | \mathcal{G}] \}_{n=1,\cdots} \{ Y_n \}_{n=1,\cdots} \{Z_n\} = \{X_n Y_n\}","['probability', 'probability-theory', 'conditional-expectation', 'uniform-integrability']"
20,Markov strong property exercise,Markov strong property exercise,,"Let $(X_n)$ be a Markov chain with Q being its transition matrix. Let $T=\inf\{n \ge 0:X_n \in A\}$ and Let $u(x)=P_x(T<+\infty)$ . Prove that $u$ verifies the system : $$ \begin{cases}{} u(x)=1 &\text{if } x\in A   \\  u(x)=Pu(x) &\text{if } x\notin A \end{cases} .  $$ My attempt and understanding : My understanding is that $T$ represents ""the first time we get to the subset $A$ "" and $u(x)$ represents ""the probability of hitting the subset $A$ starting from a $X=x$ "" ( because $P_x(T=+\infty)$ should represent the probability of never getting in the subset $A$ ). That being said, the first part of the system makes sense because if $(X=x) \in A$ then $T=0$ ( smallest $n$ ) and we are already in $A$ so the probability of getting to $A$ should be equal to $1$ . The problem is the second part, how am I going to use markov strong property to prove that?","Let be a Markov chain with Q being its transition matrix. Let and Let . Prove that verifies the system : My attempt and understanding : My understanding is that represents ""the first time we get to the subset "" and represents ""the probability of hitting the subset starting from a "" ( because should represent the probability of never getting in the subset ). That being said, the first part of the system makes sense because if then ( smallest ) and we are already in so the probability of getting to should be equal to . The problem is the second part, how am I going to use markov strong property to prove that?","(X_n) T=\inf\{n \ge 0:X_n \in A\} u(x)=P_x(T<+\infty) u 
\begin{cases}{}
u(x)=1 &\text{if } x\in A   \\ 
u(x)=Pu(x) &\text{if } x\notin A
\end{cases}
. 
 T A u(x) A X=x P_x(T=+\infty) A (X=x) \in A T=0 n A A 1","['probability', 'probability-theory', 'stochastic-processes', 'markov-chains', 'markov-process']"
21,6 Heads Followed by 6 Tails (Coin Flipping),6 Heads Followed by 6 Tails (Coin Flipping),,"You throw a fair coin one million times. What is the expected number of strings of 6 heads followed by 6 tails? The answer given is: There are $1,000,000 - 11$ possible slots for the sequence to occur. In each of these slots, the probability is $2^{-12}$ . Due to linearity of expected value, the answer is therefore $(1,000,000 - 11)\times2^{-12}$ . I don't understand why this solution works. Shouldn't there be any consideration of the fact that at most only $\dfrac{1,000,000 - 11}{12} \approx 83332$ strings of 6 heads followed by 6 tails can occur. Can anyone please help me understand the solution?","You throw a fair coin one million times. What is the expected number of strings of 6 heads followed by 6 tails? The answer given is: There are possible slots for the sequence to occur. In each of these slots, the probability is . Due to linearity of expected value, the answer is therefore . I don't understand why this solution works. Shouldn't there be any consideration of the fact that at most only strings of 6 heads followed by 6 tails can occur. Can anyone please help me understand the solution?","1,000,000 - 11 2^{-12} (1,000,000 - 11)\times2^{-12} \dfrac{1,000,000 - 11}{12} \approx 83332","['probability', 'combinatorics', 'probability-theory', 'probability-distributions', 'recreational-mathematics']"
22,"Sum of the total number of ""moves"" needed to sort the n cards for each permutation.","Sum of the total number of ""moves"" needed to sort the n cards for each permutation.",,"We have n cards numbered from 1 to n. We get pre-shuffled cards and we have to sort them in ascending order according to some rules: we look at the number of the first card in the sequence (let's denote it by x) and find the card with the number x - 1 (unless x = 1, it then finds card number n), and then translates it to the beginning. The time it takes to complete this operation is proportional to the distance of the found card from the beginning of the sequence of cards. The time for sorting cards is the sum of all times operations performed. We are to calculate the sum of the operations for all n! permutation. Example: for n = 3 we have 6 permutation 1 2 3   (0 moves) 1 3 2   ( 1 move ) 3 1 2 (2 moves) 2 3 1 (2 moves) 1 2 3 (summary 5 moves) 2 1 3   ( 1 move ) 1 2 3 2 3 1   (2 moves) 1 2 3 3 1 2   (2 moves) 2 3 1 (2 moves) 1 2 3 (summary 4 moves) 3 2 1   ( 1 move ) 2 3 1 (2 moves) 1 2 3 (summary 3 moves) getting it all together to sort 3! permutations we needs 15 moves. for n = 4 we needs 168 moves for n = 5 we needs 1700 moves for n = 6 we needs 17,220 moves for n = 7 we needs 182,406 moves for n = 8 we needs 2,055,200 moves For small n we can easly calculate it using a brute force algorithm, but how to do it for large n like 100,000 or bigger. Is it any mathematical formula to count all possibilities ? I have no idea how to go about it, do you have any usefull tips ? What area of mathematics describes this issue ?????????","We have n cards numbered from 1 to n. We get pre-shuffled cards and we have to sort them in ascending order according to some rules: we look at the number of the first card in the sequence (let's denote it by x) and find the card with the number x - 1 (unless x = 1, it then finds card number n), and then translates it to the beginning. The time it takes to complete this operation is proportional to the distance of the found card from the beginning of the sequence of cards. The time for sorting cards is the sum of all times operations performed. We are to calculate the sum of the operations for all n! permutation. Example: for n = 3 we have 6 permutation 1 2 3   (0 moves) 1 3 2   ( 1 move ) 3 1 2 (2 moves) 2 3 1 (2 moves) 1 2 3 (summary 5 moves) 2 1 3   ( 1 move ) 1 2 3 2 3 1   (2 moves) 1 2 3 3 1 2   (2 moves) 2 3 1 (2 moves) 1 2 3 (summary 4 moves) 3 2 1   ( 1 move ) 2 3 1 (2 moves) 1 2 3 (summary 3 moves) getting it all together to sort 3! permutations we needs 15 moves. for n = 4 we needs 168 moves for n = 5 we needs 1700 moves for n = 6 we needs 17,220 moves for n = 7 we needs 182,406 moves for n = 8 we needs 2,055,200 moves For small n we can easly calculate it using a brute force algorithm, but how to do it for large n like 100,000 or bigger. Is it any mathematical formula to count all possibilities ? I have no idea how to go about it, do you have any usefull tips ? What area of mathematics describes this issue ?????????",,"['probability', 'combinatorics', 'discrete-mathematics', 'card-games']"
23,Continuous Poisson Distribution,Continuous Poisson Distribution,,"Is there a Continuous analogous of the Poisson Distribution? Under the analogous, I mean such a distribution that: It is a one-parameter distribution Its distribution function is similar to the Poisson one","Is there a Continuous analogous of the Poisson Distribution? Under the analogous, I mean such a distribution that: It is a one-parameter distribution Its distribution function is similar to the Poisson one",,"['probability', 'probability-distributions', 'poisson-distribution']"
24,Derivation of Stirling approximation from CLT,Derivation of Stirling approximation from CLT,,"In Casella Berger exercise 5.35, we have a sequence of sample means from the exponential $exp(1)$ . Then, we can derive: $$P\Bigg(\frac{\overline{X}_n - 1}{1/\sqrt{n}} \leq x\Bigg) \rightarrow P(Z \leq x)$$ where $Z \sim n(0, 1)$ . Now, the author suggests to show the Stirling approximation by taking the derivative on both sides. In short, he suggests: $$\frac{d}{dx}P\Bigg(\frac{\overline{X}_n - 1}{1/\sqrt{n}} \leq x\Bigg) \approx \frac{d}{dx}P(Z \leq x)$$ from where the Stirling approximation follows. Why are we justified doing this? Wiki article on weak convergence says: "" In general, convergence in distribution does not imply that the sequence of corresponding probability density functions will also converge. "" Why is taking the derivative justified according to the author? If it is allowed, can be it shown in some simple way without measure theory? Or link to the relevant theorems.","In Casella Berger exercise 5.35, we have a sequence of sample means from the exponential . Then, we can derive: where . Now, the author suggests to show the Stirling approximation by taking the derivative on both sides. In short, he suggests: from where the Stirling approximation follows. Why are we justified doing this? Wiki article on weak convergence says: "" In general, convergence in distribution does not imply that the sequence of corresponding probability density functions will also converge. "" Why is taking the derivative justified according to the author? If it is allowed, can be it shown in some simple way without measure theory? Or link to the relevant theorems.","exp(1) P\Bigg(\frac{\overline{X}_n - 1}{1/\sqrt{n}} \leq x\Bigg) \rightarrow P(Z \leq x) Z \sim n(0, 1) \frac{d}{dx}P\Bigg(\frac{\overline{X}_n - 1}{1/\sqrt{n}} \leq x\Bigg) \approx \frac{d}{dx}P(Z \leq x)","['probability', 'probability-theory', 'central-limit-theorem', 'probability-limit-theorems']"
25,"$ X>0, E(X)=1, E(X^2)=b, \forall a\in (0,1): P(X>a)\geq \frac{(1-a)^2}{b} $",," X>0, E(X)=1, E(X^2)=b, \forall a\in (0,1): P(X>a)\geq \frac{(1-a)^2}{b} ","Suppose that $ X>0, E(X)=1, E(X^2)=b $ And We should prove for every $ a $ such that $ 0 < a < 1 $ the following statement: $ P(X>a)\geq \frac{(1-a)^2}{b} $ This is a preliminary course of probability and we learned only basic formulas and inequalities of Markov and Chebyshev's. I would be happy if you keep the answer simple as much as possible. Thanks.",Suppose that And We should prove for every such that the following statement: This is a preliminary course of probability and we learned only basic formulas and inequalities of Markov and Chebyshev's. I would be happy if you keep the answer simple as much as possible. Thanks.," X>0, E(X)=1, E(X^2)=b   a   0 < a < 1   P(X>a)\geq \frac{(1-a)^2}{b} ",['probability']
26,Towards a consistent notation for entropy and cross-entropy,Towards a consistent notation for entropy and cross-entropy,,"I am learning about the cross entropy, defined by Wikipedia as $$H(P,Q)=-\text{E}_P[\log Q]$$ for distributions $P,Q$ . I'm not happy with that notation, because it implies symmetry, $H(X,Y)$ is often used for the joint entropy and lastly, I want to use a notation which is consistent with the notation for entropy: $$H(X)=-\text{E}_P[\log P(X)]$$ When dealing with multiple distributions, I like to write $H_P(X)$ so it's clear with respect to which distribution I'm taking the entropy. When dealing with multiple random variables, it thinks it's sensible to make precise the random variable with respect to which the expectation is taken by using the subscript $_{X\sim P}$ . My notation for entropy thus becomes $$H_{X\sim P}(X)=-\text{E}_{X\sim P}[\log P(X)]$$ Now comes the point I don't understand about the definition of cross entropy: Why doesn't it reference a random variable $X$ ? Applying analogous reasoning as above, I would assume that cross entropy has the form \begin{equation}H_{X\sim P}(Q(X))=-\text{E}_{X\sim P}[\log Q(X)]\tag{1}\end{equation} however, Wikipedia makes no mention of any such random variable $X$ in the article on cross entropy. It speaks of the cross-entropy between two probability distributions $p$ and $q$ which, like the notation $H(P,Q)$ , implies a function whose argument is a pair of distributions, whereas entropy $H(X)$ is said to be a function of a random variable. In any case, to take an expected value I need a (function of) a random variable, which $P$ and $Q$ are not. Comparing the definitions for the discrete case: $$H(p,q)=-\sum_{x\in\mathcal{X}}p(x)\log q(x)$$ and $$H(X)=-\sum_{i=1}^n P(x_i)\log P(x_i)$$ where $\mathcal{X}$ is the support of $P$ and $Q$ , there would only be a qualitative difference if the events $x_i$ didn't cover the whole support (though I could just choose an $X$ which does). My questions boil down to the following: Where is the random variable necessary to take the expected value which is used to define the cross entropy $H(P,Q)=-\text{E}_{P}[\log Q]$ If I am correct in my assumption that one needs to choose a random variable $X$ to compute the cross entropy, is the notation I used for (1) free of ambiguities.","I am learning about the cross entropy, defined by Wikipedia as for distributions . I'm not happy with that notation, because it implies symmetry, is often used for the joint entropy and lastly, I want to use a notation which is consistent with the notation for entropy: When dealing with multiple distributions, I like to write so it's clear with respect to which distribution I'm taking the entropy. When dealing with multiple random variables, it thinks it's sensible to make precise the random variable with respect to which the expectation is taken by using the subscript . My notation for entropy thus becomes Now comes the point I don't understand about the definition of cross entropy: Why doesn't it reference a random variable ? Applying analogous reasoning as above, I would assume that cross entropy has the form however, Wikipedia makes no mention of any such random variable in the article on cross entropy. It speaks of the cross-entropy between two probability distributions and which, like the notation , implies a function whose argument is a pair of distributions, whereas entropy is said to be a function of a random variable. In any case, to take an expected value I need a (function of) a random variable, which and are not. Comparing the definitions for the discrete case: and where is the support of and , there would only be a qualitative difference if the events didn't cover the whole support (though I could just choose an which does). My questions boil down to the following: Where is the random variable necessary to take the expected value which is used to define the cross entropy If I am correct in my assumption that one needs to choose a random variable to compute the cross entropy, is the notation I used for (1) free of ambiguities.","H(P,Q)=-\text{E}_P[\log Q] P,Q H(X,Y) H(X)=-\text{E}_P[\log P(X)] H_P(X) _{X\sim P} H_{X\sim P}(X)=-\text{E}_{X\sim P}[\log P(X)] X \begin{equation}H_{X\sim P}(Q(X))=-\text{E}_{X\sim P}[\log Q(X)]\tag{1}\end{equation} X p q H(P,Q) H(X) P Q H(p,q)=-\sum_{x\in\mathcal{X}}p(x)\log q(x) H(X)=-\sum_{i=1}^n P(x_i)\log P(x_i) \mathcal{X} P Q x_i X H(P,Q)=-\text{E}_{P}[\log Q] X","['probability', 'probability-theory', 'notation', 'information-theory', 'entropy']"
27,Are two Poisson random variables independent if their sum is also Poisson?,Are two Poisson random variables independent if their sum is also Poisson?,,"Suppose $X\sim \text{Poisson}(\lambda)$ , $Y\sim \text{Poisson}(\mu)$ . If $X+Y\sim \text{Poisson}(\lambda+\mu),$ can we conclude that $X$ and $Y$ are independent? I know that, if we assume that the conditional distribution of $X$ given $W=X+Y$ is a Binomial distribution, then $X, Y$ can be shown to be independent. I wonder whether the independence holds in absence of any such assumption.","Suppose , . If can we conclude that and are independent? I know that, if we assume that the conditional distribution of given is a Binomial distribution, then can be shown to be independent. I wonder whether the independence holds in absence of any such assumption.","X\sim \text{Poisson}(\lambda) Y\sim \text{Poisson}(\mu) X+Y\sim \text{Poisson}(\lambda+\mu), X Y X W=X+Y X, Y","['probability', 'probability-theory', 'probability-distributions', 'independence', 'poisson-distribution']"
28,Finitely generated group unlikely to be generated by randomly chosen elements.,Finitely generated group unlikely to be generated by randomly chosen elements.,,"A well known interesting fact is that if two integers are picked ""at random"" (in an appropriate asymptotic sense), the chances they generate the integers is $6/\pi^2$ . So, the integers can be generated by two randomly selected elements with non-vanishing probability. I am wondering if there exists a (finitely generated) group that is almost certainly not generated by any finite number of ""randomly chosen"" elements. That is, is there a group $G$ with generators $\{g_1, ... , g_n\}$ such that for every $k$ , our if we pick $k$ elements of $G$ at random, there is a vanishingly small probability that the selected elements generate $G$ . To clarify, select elements randomly from balls of a given radius in the group, using the word metric with given generators, and see if the probability $k$ elements chosen from the ball generate $G$ tends to $0$ as the radius of the balls grows. This definition is compatible with the above result about the integers.","A well known interesting fact is that if two integers are picked ""at random"" (in an appropriate asymptotic sense), the chances they generate the integers is . So, the integers can be generated by two randomly selected elements with non-vanishing probability. I am wondering if there exists a (finitely generated) group that is almost certainly not generated by any finite number of ""randomly chosen"" elements. That is, is there a group with generators such that for every , our if we pick elements of at random, there is a vanishingly small probability that the selected elements generate . To clarify, select elements randomly from balls of a given radius in the group, using the word metric with given generators, and see if the probability elements chosen from the ball generate tends to as the radius of the balls grows. This definition is compatible with the above result about the integers.","6/\pi^2 G \{g_1, ... , g_n\} k k G G k G 0","['probability', 'abstract-algebra', 'group-theory', 'finitely-generated', 'combinatorial-group-theory']"
29,"Are min$(X_1,\ldots,X_n)$ and min$(X_1Y_1,\ldots,X_nY_n)$ independent for $n$ to infinity?",Are min and min independent for  to infinity?,"(X_1,\ldots,X_n) (X_1Y_1,\ldots,X_nY_n) n","This is a question that I posted on stats.stackexchange.com but since I received no satisfying answer but still the question was upvoted by many, I want to use the oppurtunity to further extend the question and hopefully address a larger audience; The original question can be found here: https://stats.stackexchange.com/questions/432396/are-minx-1-ldots-x-n-and-minx-1y-1-ldots-x-ny-n-independent-for-n-to Assume that we have given two continuous iid random variables $X$ and $Y$ with support $[1,c)$ , where $c$ is some constant greater than one. (The exact value is probably unimportant anyway) Now assume I have a given iid sample $X_1, \ldots,X_n$ and $Y_1, \ldots,Y_n$ (so absolutely no dependence here). Imagine that I know that: $$(1): \mathbb P \left(\frac{\min(X_1,\ldots,X_n)-a_n}{b_n}\leq x_1\right) \sim F(x_1), \text{ for }n \to \infty,$$ where $F(x_1)$ is some non-degenerate cdf; Given some weak condition, it is usually quite easy to derive sequences $a_n$ , $b_n$ and the limit distribution $F$ , since it is very much connected to Extreme Value theory; Moreover, I know $$(2):\mathbb P \left(\frac{\min(X_1Y_1,\ldots,X_nY_n)-\bar a_n}{\bar b_n}\leq x_2\right) \sim G(x_2), \text{ for }n \to \infty,$$ Is it true that then it also follows that $$(3):\mathbb P \left(\frac{\min(X_1,\ldots,X_n)-a_n}{b_n}\leq x_1,\frac{\min(X_1Y_1,\ldots,X_nY_n)-\bar a_n}{\bar b_n}\leq x_2\right) \sim F(x_1) G(x_2),$$ for $n$ to infinity? At first I thought this cannot work since $X$ and $XY$ are obviously absolutely dependent; But then I thought the following: The probability that the minimum of $X_1,\ldots,X_n$ and the minimum $X_1Y_1,\ldots,X_nY_n$ is obtained in the same realization converges to zero for n to infinity Since the sample itself is iid, the minima should be kinda independent; So I don't know if this is true; Unfortunately, I cannot think of a counterexample and I also have no idea how to prove it; The only thing I thought of to prove 1. is: The probability, that the minimum is obtained in the same realization is given by \begin{align*} &\sum_{i=1}^n\mathbb P\big(X_i=\min(X_1,\ldots,X_n), X_iY_i=\min(X_1Y_1,\ldots,X_nY_n)\big) \\ \leq &\sum_{i=1}^n\mathbb P\big(X_i=\min(X_1,\ldots,X_n), Y_i \leq \min(X_1Y_1,\ldots,X_nY_n)\big) \\ =  &n \cdot \mathbb P\big(X_i=\min(X_1,\ldots,X_n)\big) \mathbb P\big(  Y_i \leq \min(X_1Y_1,\ldots,X_nY_n) \vert X_i=\min(X_1,\ldots,X_n) \big) \\ = &n \cdot 1/n \mathbb P\big(  Y_i \leq \min(X_1Y_1,\ldots,X_nY_n) \vert X_i=\min(X_1,\ldots,X_n) \big) \end{align*} where the latter probability converges to zero, since $\min(X_1Y_1,\ldots,X_nY_n)$ gets arbitrarily close to 1 for $n \to \infty$ (and the condition does not seem to change that). Therefore, the probability, that the minimum is realized in the same observation is something like $n \cdot 1/n \cdot o(1)=o(1)$ , so converges to zero... Now this is obviously not a rigorous proof; So is there anyone smarter or more knowledgeable than me with an idea of a proof or a counterexample why my idea is wrong? Since antkam was commenting something, I want to give a brief look into Extreme Value Theory: Most people probably know the central limit theorem: $$\frac{S_n-n\mu}{\sqrt n \sigma} \xrightarrow[]{D}N(0,1) $$ So something similar, we can get for maxima which is based on Extreme Value Theory; It is known that $$\frac{\vee X-a_n}{b_n}$$ can only converge to one of the three extreme value distributions (or to some constant), where $\vee X= \max(X_1,\ldots,X_n)$ . So for minima, we can use this and also find limit distributions and those sequences, such that $$\frac{\land X-a_n}{b_n}$$ converges to some distribution; Now what you, antkam, probably wanted to say: If $\frac{\land XY−\bar a_n}{\bar b_n}\leq x_2$ , then it holds that $\land XY \leq x_2 \bar b_n+\bar a_n$ and therefore in particular it holds that: $\land X \leq x_2 \bar b_n+\bar a_n$ (Since $Y \geq 1$ ) Now $\frac{\land X− a_n}{ b_n}\leq x_1$ is equivalent to $\land X \leq x_1 b_n + a_n$ . So if $x_2 \bar b_n+\bar a_n \leq x_1 b_n+ a_n$ , then $\frac{\land XY−\bar a_n}{\bar b_n}\leq x_2$ already implies $\frac{\land X− a_n}{ b_n}\leq x_1$ ; That was by the way also a way, someone wanted to prove that this in incorrect;  Therefore, if we solve it for $x_1$ we get: $$ x_1  \geq\frac{ x_2 \bar b_n+\bar a_n- a_n}{b_n}$$ So if $x_1$ is greater than the term, then it certainly is incorrect; The problem is, that we do not know these sequences and therefore, the right term could (and probably will) go to infinity and then it does not work, obviously. So if you want to prove it like that, there are basically just 2 ways I can think of: You take some distribution for $X$ and $Y$ , calculate the corresponding sequences and show that the right term indeed does not go to infinity; You take some other sequences $a_n$ , $b_n$ , $\bar a_n$ , $\bar b_n$ (I never said that it only works for the sequences such that we can get some nice limit distribution) but then the limit of $\frac{\land X-a_n}{b_n}$ would somewhat converge in probability only to a fixed number, or it goes to infinity or -infinity or something like that; So the cdf of $\frac{\land X-a_n}{b_n}$ or also $\frac{\land XY-\bar a_n}{\bar b_n}$ would only have values 0 and 1... Hope this helps some people to understand the problem a bit better... :-) @ Sangchul Lee: Thank you very much, for your answer; This is actually very interesting, because given the uniform distribution,then $X-1$ and $\ln(X)$ is regularly varying at zero with exponent $\alpha=1$ ; This is equivalent to $1/(X-1)$ or $1/\ln(X)$ being regularly varying at infinity with exponent $\alpha=-1$ ; Using well-known results, we can show that $\frac{\lor (1/\ln(X))}{b_n}$ then converges to a Frechet distribution with exponent $\alpha$ and by the close connection we know that $$\mathbb P\left(\frac{\land \ln(X)}{b_n}\leq x\right)$$ or also $$\mathbb P\left(\frac{\land X-1}{b_n}\leq x\right)$$ converges to $(1-\phi_{\alpha}(1/x))$ and since $XY-1$ is regularly varying with exponent $2\alpha$ , you get this convergence with $e^{-t^2}$ for the tail; Which brings me back to your idea; I suspect that it might be possible to prove it more generally for all regularly varying random variables, but I need to think about it tomorrow... But it is definitely a very interesting and smart answer, thank you so much for it... :-) Okay, I give things a try, although I did not manage to use the regular variation and I have no idea how I could use it; Also, I am not sure if this is a proper proof; The only thing I am going to use is that $\mathbb P(Y \in [1,a])\to 0$ for $a \downarrow 1$ and $\mathbb P(X \in [1,a])\to 0$ for $a \downarrow 1$ . (Does this follow from the continuity or can we construct some weird distribution that is continous but still does not satisfy this?) Anyway: We generally assume $X$ and $Y$ to be independent but not necessarily identically distributed; We know that $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{\land X-1}{b_n}> x_1 \right) = \exp(-x_1^{\alpha_1})$$ and $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{\land XY-1}{\bar b_n}> x_2 \right) = \exp(-x_2^{\alpha_2})$$ So these are our assumptions; Then it follows that: $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{\land X-1}{b_n}> x_1 \right) =\lim\limits_{n \to \infty} \mathbb P \left(\frac{ X-1}{b_n}> x_1 \right)^n= \exp(-x_1^{\alpha_1})$$ and therefore, in particular: $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{ X-1}{b_n}> x_1 \right)=\left(1-\frac{x_1^{\alpha_1}}{n}\right)$$ respectively $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{ X-1}{b_n}\leq x_1 \right)=\frac{x_1^{\alpha_1}}{n}$$ and also it holds that $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{ XY-1}{\bar b_n}> x_2 \right)=\left(1-\frac{x_2^{\alpha_2}}{n}\right)$$ respectively $$\lim\limits_{n \to \infty} \mathbb P \left(\frac{ XY-1}{\bar b_n}\leq x_2 \right)=\frac{x_2^{\alpha_2}}{n}$$ Now we have: $$\lim\limits_{n \to \infty}\mathbb P \left( \frac{\land X-1}{ b_n}> x_1 \right) \mathbb P \left(\frac{\land XY-1}{\bar b_n}> x_2 \right)=\exp(-x_1^{\alpha_1})\exp(-x_2^{\alpha_2})=\exp(-x_1^{\alpha_1}-x_2^{\alpha_2})$$ and $$\lim\limits_{n \to \infty}\mathbb P \left(\frac{ X-1}{ b_n}> x_1,\frac{ XY-1}{\bar b_n}> x_2 \right)\\ =\lim\limits_{n \to \infty}\left(1- \mathbb P \left(\frac{ X-1}{ b_n}\leq x_1\right)-\mathbb P \left(\frac{ XY-1}{\bar b_n}\leq x_2\right)+\mathbb P \left(\frac{ X-1}{ b_n}\leq x_1,\frac{ XY-1}{\bar b_n}\leq x_2\right)\right)\\ =\lim\limits_{n \to \infty}\left(1-\frac{x_1^{\alpha_1}}{n}-\frac{x_2^{\alpha_2}}{n} +\mathbb P \left(\frac{ X-1}{ b_n}\leq x_1,\frac{ XY-1}{\bar b_n}\leq x_2\right)\right)$$ Moreover, we have $$\mathbb P \left(\frac{ X-1}{ b_n}\leq x_1,\frac{ XY-1}{\bar b_n}\leq x_2\right) = \mathbb P \left(\frac{ X-1}{ b_n}\leq x_1\right) \mathbb P \left(\frac{ XY-1}{\bar b_n}\leq x_2 \bigg \vert \frac{ X-1}{ b_n}\leq x_1\right) \\  \leq \frac{x_1^{\alpha_1}}{n} \mathbb P \left(\frac{Y-1}{\bar b_n}\leq x_2 \bigg \vert \frac{ X-1}{ b_n}\leq x_1\right)=\frac{x_1^{\alpha_1}}{n} \mathbb P \left(\frac{Y-1}{\bar b_n}\leq x_2\right)=\frac{x_1^{\alpha_1}}{n} o(1), $$ since $\bar b_{n} \to 0$ for $n \to \infty$ . Therefore we can see that $$\lim\limits_{n \to \infty}\mathbb P \left(\frac{\land X-1}{ b_n}>x_1,\frac{ XY-1}{\bar b_n}> x_2 \right)\\ =\lim\limits_{n \to \infty}\left(1-\frac{x_1^{\alpha_1}}{n}(1-o(1))-\frac{x_2^{\alpha_2}}{n} \right)^n = \exp\left(-x_1^{\alpha_1}-x_2^{\alpha_2}\right)$$ Now I am curious: Is this a valid proof?","This is a question that I posted on stats.stackexchange.com but since I received no satisfying answer but still the question was upvoted by many, I want to use the oppurtunity to further extend the question and hopefully address a larger audience; The original question can be found here: https://stats.stackexchange.com/questions/432396/are-minx-1-ldots-x-n-and-minx-1y-1-ldots-x-ny-n-independent-for-n-to Assume that we have given two continuous iid random variables and with support , where is some constant greater than one. (The exact value is probably unimportant anyway) Now assume I have a given iid sample and (so absolutely no dependence here). Imagine that I know that: where is some non-degenerate cdf; Given some weak condition, it is usually quite easy to derive sequences , and the limit distribution , since it is very much connected to Extreme Value theory; Moreover, I know Is it true that then it also follows that for to infinity? At first I thought this cannot work since and are obviously absolutely dependent; But then I thought the following: The probability that the minimum of and the minimum is obtained in the same realization converges to zero for n to infinity Since the sample itself is iid, the minima should be kinda independent; So I don't know if this is true; Unfortunately, I cannot think of a counterexample and I also have no idea how to prove it; The only thing I thought of to prove 1. is: The probability, that the minimum is obtained in the same realization is given by where the latter probability converges to zero, since gets arbitrarily close to 1 for (and the condition does not seem to change that). Therefore, the probability, that the minimum is realized in the same observation is something like , so converges to zero... Now this is obviously not a rigorous proof; So is there anyone smarter or more knowledgeable than me with an idea of a proof or a counterexample why my idea is wrong? Since antkam was commenting something, I want to give a brief look into Extreme Value Theory: Most people probably know the central limit theorem: So something similar, we can get for maxima which is based on Extreme Value Theory; It is known that can only converge to one of the three extreme value distributions (or to some constant), where . So for minima, we can use this and also find limit distributions and those sequences, such that converges to some distribution; Now what you, antkam, probably wanted to say: If , then it holds that and therefore in particular it holds that: (Since ) Now is equivalent to . So if , then already implies ; That was by the way also a way, someone wanted to prove that this in incorrect;  Therefore, if we solve it for we get: So if is greater than the term, then it certainly is incorrect; The problem is, that we do not know these sequences and therefore, the right term could (and probably will) go to infinity and then it does not work, obviously. So if you want to prove it like that, there are basically just 2 ways I can think of: You take some distribution for and , calculate the corresponding sequences and show that the right term indeed does not go to infinity; You take some other sequences , , , (I never said that it only works for the sequences such that we can get some nice limit distribution) but then the limit of would somewhat converge in probability only to a fixed number, or it goes to infinity or -infinity or something like that; So the cdf of or also would only have values 0 and 1... Hope this helps some people to understand the problem a bit better... :-) @ Sangchul Lee: Thank you very much, for your answer; This is actually very interesting, because given the uniform distribution,then and is regularly varying at zero with exponent ; This is equivalent to or being regularly varying at infinity with exponent ; Using well-known results, we can show that then converges to a Frechet distribution with exponent and by the close connection we know that or also converges to and since is regularly varying with exponent , you get this convergence with for the tail; Which brings me back to your idea; I suspect that it might be possible to prove it more generally for all regularly varying random variables, but I need to think about it tomorrow... But it is definitely a very interesting and smart answer, thank you so much for it... :-) Okay, I give things a try, although I did not manage to use the regular variation and I have no idea how I could use it; Also, I am not sure if this is a proper proof; The only thing I am going to use is that for and for . (Does this follow from the continuity or can we construct some weird distribution that is continous but still does not satisfy this?) Anyway: We generally assume and to be independent but not necessarily identically distributed; We know that and So these are our assumptions; Then it follows that: and therefore, in particular: respectively and also it holds that respectively Now we have: and Moreover, we have since for . Therefore we can see that Now I am curious: Is this a valid proof?","X Y [1,c) c X_1, \ldots,X_n Y_1, \ldots,Y_n (1): \mathbb P \left(\frac{\min(X_1,\ldots,X_n)-a_n}{b_n}\leq x_1\right) \sim F(x_1), \text{ for }n \to \infty, F(x_1) a_n b_n F (2):\mathbb P \left(\frac{\min(X_1Y_1,\ldots,X_nY_n)-\bar a_n}{\bar b_n}\leq x_2\right) \sim G(x_2), \text{ for }n \to \infty, (3):\mathbb P \left(\frac{\min(X_1,\ldots,X_n)-a_n}{b_n}\leq x_1,\frac{\min(X_1Y_1,\ldots,X_nY_n)-\bar a_n}{\bar b_n}\leq x_2\right) \sim F(x_1) G(x_2), n X XY X_1,\ldots,X_n X_1Y_1,\ldots,X_nY_n \begin{align*}
&\sum_{i=1}^n\mathbb P\big(X_i=\min(X_1,\ldots,X_n), X_iY_i=\min(X_1Y_1,\ldots,X_nY_n)\big) \\ \leq &\sum_{i=1}^n\mathbb P\big(X_i=\min(X_1,\ldots,X_n), Y_i \leq \min(X_1Y_1,\ldots,X_nY_n)\big) \\
=  &n \cdot \mathbb P\big(X_i=\min(X_1,\ldots,X_n)\big) \mathbb P\big(  Y_i \leq \min(X_1Y_1,\ldots,X_nY_n) \vert X_i=\min(X_1,\ldots,X_n) \big) \\
= &n \cdot 1/n \mathbb P\big(  Y_i \leq \min(X_1Y_1,\ldots,X_nY_n) \vert X_i=\min(X_1,\ldots,X_n) \big)
\end{align*} \min(X_1Y_1,\ldots,X_nY_n) n \to \infty n \cdot 1/n \cdot o(1)=o(1) \frac{S_n-n\mu}{\sqrt n \sigma} \xrightarrow[]{D}N(0,1)  \frac{\vee X-a_n}{b_n} \vee X= \max(X_1,\ldots,X_n) \frac{\land X-a_n}{b_n} \frac{\land XY−\bar a_n}{\bar b_n}\leq x_2 \land XY \leq x_2 \bar b_n+\bar a_n \land X \leq x_2 \bar b_n+\bar a_n Y \geq 1 \frac{\land X− a_n}{ b_n}\leq x_1 \land X \leq x_1 b_n + a_n x_2 \bar b_n+\bar a_n \leq x_1 b_n+ a_n \frac{\land XY−\bar a_n}{\bar b_n}\leq x_2 \frac{\land X− a_n}{ b_n}\leq x_1 x_1  x_1  \geq\frac{ x_2 \bar b_n+\bar a_n- a_n}{b_n} x_1 X Y a_n b_n \bar a_n \bar b_n \frac{\land X-a_n}{b_n} \frac{\land X-a_n}{b_n} \frac{\land XY-\bar a_n}{\bar b_n} X-1 \ln(X) \alpha=1 1/(X-1) 1/\ln(X) \alpha=-1 \frac{\lor (1/\ln(X))}{b_n} \alpha \mathbb P\left(\frac{\land \ln(X)}{b_n}\leq x\right) \mathbb P\left(\frac{\land X-1}{b_n}\leq x\right) (1-\phi_{\alpha}(1/x)) XY-1 2\alpha e^{-t^2} \mathbb P(Y \in [1,a])\to 0 a \downarrow 1 \mathbb P(X \in [1,a])\to 0 a \downarrow 1 X Y \lim\limits_{n \to \infty} \mathbb P \left(\frac{\land X-1}{b_n}> x_1 \right) = \exp(-x_1^{\alpha_1}) \lim\limits_{n \to \infty} \mathbb P \left(\frac{\land XY-1}{\bar b_n}> x_2 \right) = \exp(-x_2^{\alpha_2}) \lim\limits_{n \to \infty} \mathbb P \left(\frac{\land X-1}{b_n}> x_1 \right) =\lim\limits_{n \to \infty} \mathbb P \left(\frac{ X-1}{b_n}> x_1 \right)^n= \exp(-x_1^{\alpha_1}) \lim\limits_{n \to \infty} \mathbb P \left(\frac{ X-1}{b_n}> x_1 \right)=\left(1-\frac{x_1^{\alpha_1}}{n}\right) \lim\limits_{n \to \infty} \mathbb P \left(\frac{ X-1}{b_n}\leq x_1 \right)=\frac{x_1^{\alpha_1}}{n} \lim\limits_{n \to \infty} \mathbb P \left(\frac{ XY-1}{\bar b_n}> x_2 \right)=\left(1-\frac{x_2^{\alpha_2}}{n}\right) \lim\limits_{n \to \infty} \mathbb P \left(\frac{ XY-1}{\bar b_n}\leq x_2 \right)=\frac{x_2^{\alpha_2}}{n} \lim\limits_{n \to \infty}\mathbb P \left( \frac{\land X-1}{ b_n}> x_1 \right) \mathbb P \left(\frac{\land XY-1}{\bar b_n}> x_2 \right)=\exp(-x_1^{\alpha_1})\exp(-x_2^{\alpha_2})=\exp(-x_1^{\alpha_1}-x_2^{\alpha_2}) \lim\limits_{n \to \infty}\mathbb P \left(\frac{ X-1}{ b_n}> x_1,\frac{ XY-1}{\bar b_n}> x_2 \right)\\
=\lim\limits_{n \to \infty}\left(1- \mathbb P \left(\frac{ X-1}{ b_n}\leq x_1\right)-\mathbb P \left(\frac{ XY-1}{\bar b_n}\leq x_2\right)+\mathbb P \left(\frac{ X-1}{ b_n}\leq x_1,\frac{ XY-1}{\bar b_n}\leq x_2\right)\right)\\
=\lim\limits_{n \to \infty}\left(1-\frac{x_1^{\alpha_1}}{n}-\frac{x_2^{\alpha_2}}{n} +\mathbb P \left(\frac{ X-1}{ b_n}\leq x_1,\frac{ XY-1}{\bar b_n}\leq x_2\right)\right) \mathbb P \left(\frac{ X-1}{ b_n}\leq x_1,\frac{ XY-1}{\bar b_n}\leq x_2\right) = \mathbb P \left(\frac{ X-1}{ b_n}\leq x_1\right) \mathbb P \left(\frac{ XY-1}{\bar b_n}\leq x_2 \bigg \vert \frac{ X-1}{ b_n}\leq x_1\right) \\ 
\leq \frac{x_1^{\alpha_1}}{n} \mathbb P \left(\frac{Y-1}{\bar b_n}\leq x_2 \bigg \vert \frac{ X-1}{ b_n}\leq x_1\right)=\frac{x_1^{\alpha_1}}{n} \mathbb P \left(\frac{Y-1}{\bar b_n}\leq x_2\right)=\frac{x_1^{\alpha_1}}{n} o(1),  \bar b_{n} \to 0 n \to \infty \lim\limits_{n \to \infty}\mathbb P \left(\frac{\land X-1}{ b_n}>x_1,\frac{ XY-1}{\bar b_n}> x_2 \right)\\
=\lim\limits_{n \to \infty}\left(1-\frac{x_1^{\alpha_1}}{n}(1-o(1))-\frac{x_2^{\alpha_2}}{n} \right)^n = \exp\left(-x_1^{\alpha_1}-x_2^{\alpha_2}\right)","['probability', 'statistics', 'extreme-value-analysis']"
30,PDF of sum of exponentials conditioned on equality.,PDF of sum of exponentials conditioned on equality.,,"I am trying to solve the following exam practice problem: Let $X_1, X_2$ be independent exponential random variables with   parameter $1$ . Find the conditional PDF of $X_1+X_2$ given that $\frac{X_1}{X_2}=1.$ Find the conditional PDF of $X_1+X_2$ given that $X_1-X_2=0.$ The events $\frac{X_1}{X_2}=1$ and $X_1-X_2=0$ are the same. Does this mean that conditioning on either of these two events should give   the same answer? Here is my approach: For part 1, I computed the joint PDF of $Y= X_1+X_2$ and $Z=\frac{X_1}{X_2}$ and my result was $$f_{Y,Z}(y,z) = \frac{ye^{-y}}{(z+1)^2}\mathbf{1}_{\{y\geq 0, z \geq 0\}}.$$ This shows that $Y$ and $Z$ are independent. So, conditioned on $Z=1$ , we get that $$f_{Y|Z}(y|1) = ye^{-y}\mathbf{1}_{\{y\geq 0\}}.$$ For part 2, I computed the joint PDF of $U=X_1+X_2$ and $V=X_1-X_2$ and got $$f_{U,V}(u,v) = \frac{1}{2}e^{-u}\mathbf{1}_{\{u\geq |v|\}}.$$ I then used Bayes' theorem to get that $$f_{U|V}(u|0) = e^{-u}\mathbf{1}_{\{u\geq 0\}}.$$ My question regards part 3. Firt of all, part 3 states that the events $X_1-X_2=0$ and $\frac{X_1}{X_2}=1$ are the same. But, I disagree because the event $X_1-X_2=0$ contains the measure-zero event $X_1=X_2=0$ , which is not contained in the event $\frac{X_1}{X_2}=1$ . Is this not the case? Second of all, if these two events were indeed the same, then why would conditioning on them produce a different result?","I am trying to solve the following exam practice problem: Let be independent exponential random variables with   parameter . Find the conditional PDF of given that Find the conditional PDF of given that The events and are the same. Does this mean that conditioning on either of these two events should give   the same answer? Here is my approach: For part 1, I computed the joint PDF of and and my result was This shows that and are independent. So, conditioned on , we get that For part 2, I computed the joint PDF of and and got I then used Bayes' theorem to get that My question regards part 3. Firt of all, part 3 states that the events and are the same. But, I disagree because the event contains the measure-zero event , which is not contained in the event . Is this not the case? Second of all, if these two events were indeed the same, then why would conditioning on them produce a different result?","X_1, X_2 1 X_1+X_2 \frac{X_1}{X_2}=1. X_1+X_2 X_1-X_2=0. \frac{X_1}{X_2}=1 X_1-X_2=0 Y= X_1+X_2 Z=\frac{X_1}{X_2} f_{Y,Z}(y,z) = \frac{ye^{-y}}{(z+1)^2}\mathbf{1}_{\{y\geq 0, z \geq 0\}}. Y Z Z=1 f_{Y|Z}(y|1) = ye^{-y}\mathbf{1}_{\{y\geq 0\}}. U=X_1+X_2 V=X_1-X_2 f_{U,V}(u,v) = \frac{1}{2}e^{-u}\mathbf{1}_{\{u\geq |v|\}}. f_{U|V}(u|0) = e^{-u}\mathbf{1}_{\{u\geq 0\}}. X_1-X_2=0 \frac{X_1}{X_2}=1 X_1-X_2=0 X_1=X_2=0 \frac{X_1}{X_2}=1","['probability', 'conditional-probability']"
31,Why does deep learning work despite the surprising behavior of probability distributions in high dimensions?,Why does deep learning work despite the surprising behavior of probability distributions in high dimensions?,,"This question is meant to be very specific. How/why is deep learning successful at learning a classification function/hyperplane given the challenges of probability distributions and distance metrics in high dimensional spaces. Deep learning or Deep Neural Networks are a big area of research and activity in machine learning right now and for the past few years. These models are constructed for a large number of layers of latent variables. In a simple convolutional neural network for image classification, or object detection, it is easy to have a million or more parameters. Now there are more than a few references that discuss how in high dimensions, probability distributions behave in very odd ways and distance metrics on those probability distributions also behave in odd ways. Without getting into the details, in high dimension everything is essentially far apart, so the probability density mass becomes more diffuse over its support. Further if you follow the sphere-packing literature, there are very odd phenomena which occur such as most of the volume of a high-dimensional hyper-sphere is on its skin or surface--as opposed to its center. In supervised deep learning, the loss function will govern the learning process. This loss function is usually based upon a distance metric that compares two high-dimensional distributions. So the common loss functions are metrics like cross entropy between two distributions or the KL-Divergence between two distributions. The idea is to understand the distance between the probability of a point on the candidate distribution versus the actual distribution. So I am just trying to understand why deep learning works so well if the high-dimensionality of the data should create such odd behavior in the associated loss functions/metrics. I mean if the probability distributions become so diffuse as dimensionality increases, then the distributions should become less informative as there are more and more ways to obtain the same probability. Some articles or posts I have read suggest that the usual 'manifold assumption' is at work where the high-dimensional data lives on some lower-dimensional manifold. I can understand that idea. But then by that logic the curse of dimensionality should never create a problem for any statistical method on high-dimensional data--since all high-dimensional data is intrinsically low-dimensional. So what I am looking for is a bit more precision in the analysis. How does this manifold assumption--if that is indeed the answer--operate at each level of the network to make it not fall victim to the usual curses of dimensionality. It might be that I am just looking at the problem from the wrong angle--and this is what I wanted to validate. So if I am looking at an image segmentation problem and I have 512x512 image, then I am classifying each pixel with a class label, that means I am assigning about 262,000 labels. Now am I really assigning the labels over a 262,000 dimensional space, or a higher-dimensional space because I am not including the parameters from the lower layers of the network. Or am I just classifying over say a 2 or 5 dimensional space--based upon the possible class label values. Or do neural networks operate like dynamic programming problems where solving the value for each node in the network together will generate some optimal solution overall?","This question is meant to be very specific. How/why is deep learning successful at learning a classification function/hyperplane given the challenges of probability distributions and distance metrics in high dimensional spaces. Deep learning or Deep Neural Networks are a big area of research and activity in machine learning right now and for the past few years. These models are constructed for a large number of layers of latent variables. In a simple convolutional neural network for image classification, or object detection, it is easy to have a million or more parameters. Now there are more than a few references that discuss how in high dimensions, probability distributions behave in very odd ways and distance metrics on those probability distributions also behave in odd ways. Without getting into the details, in high dimension everything is essentially far apart, so the probability density mass becomes more diffuse over its support. Further if you follow the sphere-packing literature, there are very odd phenomena which occur such as most of the volume of a high-dimensional hyper-sphere is on its skin or surface--as opposed to its center. In supervised deep learning, the loss function will govern the learning process. This loss function is usually based upon a distance metric that compares two high-dimensional distributions. So the common loss functions are metrics like cross entropy between two distributions or the KL-Divergence between two distributions. The idea is to understand the distance between the probability of a point on the candidate distribution versus the actual distribution. So I am just trying to understand why deep learning works so well if the high-dimensionality of the data should create such odd behavior in the associated loss functions/metrics. I mean if the probability distributions become so diffuse as dimensionality increases, then the distributions should become less informative as there are more and more ways to obtain the same probability. Some articles or posts I have read suggest that the usual 'manifold assumption' is at work where the high-dimensional data lives on some lower-dimensional manifold. I can understand that idea. But then by that logic the curse of dimensionality should never create a problem for any statistical method on high-dimensional data--since all high-dimensional data is intrinsically low-dimensional. So what I am looking for is a bit more precision in the analysis. How does this manifold assumption--if that is indeed the answer--operate at each level of the network to make it not fall victim to the usual curses of dimensionality. It might be that I am just looking at the problem from the wrong angle--and this is what I wanted to validate. So if I am looking at an image segmentation problem and I have 512x512 image, then I am classifying each pixel with a class label, that means I am assigning about 262,000 labels. Now am I really assigning the labels over a 262,000 dimensional space, or a higher-dimensional space because I am not including the parameters from the lower layers of the network. Or am I just classifying over say a 2 or 5 dimensional space--based upon the possible class label values. Or do neural networks operate like dynamic programming problems where solving the value for each node in the network together will generate some optimal solution overall?",,"['probability', 'statistics', 'machine-learning', 'neural-networks']"
32,Distribution at First Time a Sum Reaches a Threshold,Distribution at First Time a Sum Reaches a Threshold,,"Consider the following problem. Roll a die many times, and stop when the total exceeds $M$ , for some prescribed threshold $M$ . Call this time $\tau$ , and call the running score after $n$ rolls $X_n$ . What is the distribution of $X_\tau$ ? Of course $X_\tau \in [M,M+5]$ . However, I can get little beyond this. Moreover, for small $M$ the distribution should be very sensitive, and I doubt have a nice form. However, if I define $X'_\tau = X_\tau - M$ , then it seems to me that $X'_\tau$ should have a limiting distribution at $M \to \infty$ . Extra comments. $\quad$ Note that there are various related questions here on maths.SE, but these (as far as I have seen) are about determining $\tau$ , in particular its expectation, $E(\tau)$ . Also, one can solve for $E(\tau)$ directly. If I write $k_M$ for $E(\tau)$ with threshold $M$ , then $$ \textstyle k_M = \tfrac16 \sum_{j=1}^6 k_{M-j} + 1, $$ and writing $\ell_M = k_M - k_{M-1}$ we get $$ \textstyle \ell_M = \tfrac16 \sum_{j=1}^6 \ell_{M-j}. $$ In principle, by trying the solution $\ell_r = r^\lambda$ and solving $$ 6 \lambda^6 - \lambda^5 - \lambda^4 - \lambda^3 - \lambda^2 - \lambda - 1 = 0, $$ (see this WolframAlpha computation ), and calculating some initial conditions by hand, then one can find $k_M = E(\tau)$ . Note that this would involve finding six initial conditions and solving a set of six simultaneous equations. This is only for the $\ell$ -s; one then needs to convert this into the $k$ -s. This does not sound like fun to me! =P By a simple martingale argument--- $(X_n - \tfrac72n)_{n\ge0}$ is a martingale, and $\tau$ is a deterministically bounded stopping time---from this one immediately gets $E(X_\tau) = \tfrac27 E(\tau)$ (for any $M$ ).","Consider the following problem. Roll a die many times, and stop when the total exceeds , for some prescribed threshold . Call this time , and call the running score after rolls . What is the distribution of ? Of course . However, I can get little beyond this. Moreover, for small the distribution should be very sensitive, and I doubt have a nice form. However, if I define , then it seems to me that should have a limiting distribution at . Extra comments. Note that there are various related questions here on maths.SE, but these (as far as I have seen) are about determining , in particular its expectation, . Also, one can solve for directly. If I write for with threshold , then and writing we get In principle, by trying the solution and solving (see this WolframAlpha computation ), and calculating some initial conditions by hand, then one can find . Note that this would involve finding six initial conditions and solving a set of six simultaneous equations. This is only for the -s; one then needs to convert this into the -s. This does not sound like fun to me! =P By a simple martingale argument--- is a martingale, and is a deterministically bounded stopping time---from this one immediately gets (for any ).","M M \tau n X_n X_\tau X_\tau \in [M,M+5] M X'_\tau = X_\tau - M X'_\tau M \to \infty \quad \tau E(\tau) E(\tau) k_M E(\tau) M  \textstyle k_M = \tfrac16 \sum_{j=1}^6 k_{M-j} + 1,  \ell_M = k_M - k_{M-1}  \textstyle \ell_M = \tfrac16 \sum_{j=1}^6 \ell_{M-j}.  \ell_r = r^\lambda  6 \lambda^6 - \lambda^5 - \lambda^4 - \lambda^3 - \lambda^2 - \lambda - 1 = 0,  k_M = E(\tau) \ell k (X_n - \tfrac72n)_{n\ge0} \tau E(X_\tau) = \tfrac27 E(\tau) M","['probability', 'probability-theory', 'probability-distributions', 'recreational-mathematics', 'martingales']"
33,Egg drop problem - minimize AVERAGE case,Egg drop problem - minimize AVERAGE case,,"In the egg drop problem ,  you have two eggs and you want to determine from which floors in a $n$ - floor building you can drop an egg such that is doesn't break. You are to determine the minimum number of attempts you need in order to find the critical floor in the worst case while using the best strategy. Some assumptions : If the egg doesn't break at a certain floor, it will not break at any floor below. If the eggs breaks at a certain floor, it will break at any floor above. The egg may break at the first floor. The egg may not break at the last floor. This problem is popular and there are many ways to solve it. I am wondering how to solve it with the following twist : instead of minimizing the worst case, how would one minimize the average case ? We consider that the egg has equal probability $1/n$ to break on a given floor (and above). An attempt : Let $f(2,n)$ denote the minimum number of drops needed to cover $n$ floors with the $2$ eggs. For the worst case, the following recursive equation holds $$ f(2,n) = 1 + \min_{1 \le x \le n} \bigl\{ \max\{f(1,x-1),f(2,n-x \} \bigr \} $$ Roughly speaking, when throwing the first egg from a given floor $x$ , there are two scenarios : If the egg breaks, we are left with $1$ egg and have to check the $x-1$ floors below If the egg does not break, the $n-x$ floors above $x$ have to be checked with the $2$ eggs Since  the number of trials in worst case is minimized, we take the maximum of these two cases for every floor and choose the floor which yields minimum number of drops. The extra $1$ accounts for the drop before knowing if the egg broke or not. The base cases are trivially $f(1,0)=f(2,0)=0$ (no drops needed if there are no floors), $f(1,1)= f(2,1) = 1$ (one drop is sufficient if there is only one floor), and $f(1,x) = x$ ( $x$ drops needed if only one egg is available - each floor must be tested one by one). For the average case, I believe the recursive equation becomes $$ f(2,n) = 1 + \min_{1 \le x \le n} \bigl\{ p(x)f(1,x-1)+(1-p(x))f(2,n-x) \bigr \}, $$ where $p(x)$ is the probability that the egg breaks on floor $x$ , i.e., the probability that $x$ is above the critical floor. If this critical floor is represented by a discrete random variable $Y$ with uniform distribution on $[1,n]$ : $$ p(x) = P(x\ge Y)  $$ I am not sure if this correct. And if so, how can this expression be simplified? Also, the base cases remain $f(1,0)=f(2,0)=0$ , $f(1,1)=f(2,1)=1$ , but $f(1,x)$ no longer equals $x$ , since we are interested in the average case. Any help is welcome, any other approach is welcome. Thanks.","In the egg drop problem ,  you have two eggs and you want to determine from which floors in a - floor building you can drop an egg such that is doesn't break. You are to determine the minimum number of attempts you need in order to find the critical floor in the worst case while using the best strategy. Some assumptions : If the egg doesn't break at a certain floor, it will not break at any floor below. If the eggs breaks at a certain floor, it will break at any floor above. The egg may break at the first floor. The egg may not break at the last floor. This problem is popular and there are many ways to solve it. I am wondering how to solve it with the following twist : instead of minimizing the worst case, how would one minimize the average case ? We consider that the egg has equal probability to break on a given floor (and above). An attempt : Let denote the minimum number of drops needed to cover floors with the eggs. For the worst case, the following recursive equation holds Roughly speaking, when throwing the first egg from a given floor , there are two scenarios : If the egg breaks, we are left with egg and have to check the floors below If the egg does not break, the floors above have to be checked with the eggs Since  the number of trials in worst case is minimized, we take the maximum of these two cases for every floor and choose the floor which yields minimum number of drops. The extra accounts for the drop before knowing if the egg broke or not. The base cases are trivially (no drops needed if there are no floors), (one drop is sufficient if there is only one floor), and ( drops needed if only one egg is available - each floor must be tested one by one). For the average case, I believe the recursive equation becomes where is the probability that the egg breaks on floor , i.e., the probability that is above the critical floor. If this critical floor is represented by a discrete random variable with uniform distribution on : I am not sure if this correct. And if so, how can this expression be simplified? Also, the base cases remain , , but no longer equals , since we are interested in the average case. Any help is welcome, any other approach is welcome. Thanks.","n 1/n f(2,n) n 2 
f(2,n) = 1 + \min_{1 \le x \le n} \bigl\{ \max\{f(1,x-1),f(2,n-x \} \bigr \}
 x 1 x-1 n-x x 2 1 f(1,0)=f(2,0)=0 f(1,1)= f(2,1) = 1 f(1,x) = x x 
f(2,n) = 1 + \min_{1 \le x \le n} \bigl\{ p(x)f(1,x-1)+(1-p(x))f(2,n-x) \bigr \},
 p(x) x x Y [1,n] 
p(x) = P(x\ge Y) 
 f(1,0)=f(2,0)=0 f(1,1)=f(2,1)=1 f(1,x) x","['probability', 'optimization', 'recreational-mathematics', 'puzzle']"
34,Probability that a graph is bipartite,Probability that a graph is bipartite,,"Given the empty graph on $n$ vertices, we add $m$ of the $\binom{n}{2}$ possible edges, uniformly at random. What is the probability that the resulting graph is bipartite (equivalently, contains no odd cycles) ? Alternative formulation: In the random graph model $G(n,m)$ , how many of the $\binom{\binom{n}{2}}{m}$ graphs are bipartite ?","Given the empty graph on vertices, we add of the possible edges, uniformly at random. What is the probability that the resulting graph is bipartite (equivalently, contains no odd cycles) ? Alternative formulation: In the random graph model , how many of the graphs are bipartite ?","n m \binom{n}{2} G(n,m) \binom{\binom{n}{2}}{m}","['probability', 'graph-theory', 'random-graphs']"
35,"Borel-Cantelli and ""infinitely often""","Borel-Cantelli and ""infinitely often""",,"The problem: Let $(X_n)_{n\geq 1}$ be a real-valued sequence of i.i.d. random variables and let $c > 0$ . Use Borel-Cantelli's lemma to show that $$\sum_{n=1}^\infty P(X_n^2 > n) < \infty \Rightarrow P(|X_n|\geq c\sqrt{n} \hspace{7pt} \text{i.o.} \hspace{7pt} )=0.$$ My attempt: So from Borel-Cantelli we have $$P(X_n^2 > n \hspace{7pt}\text{i.o.}\hspace{7pt})=0$$ and using the definition of ""infinitely often"": $$\bigcap_{m=1}^\infty \bigcup_{n=1}^\infty (X_n^2 > n)=\bigcap_{m=1}^\infty \bigcup_{n=1}^\infty (|X_n| > \sqrt{n})$$ But I don't see how I get the inclusion into the event containing $c$ .","The problem: Let be a real-valued sequence of i.i.d. random variables and let . Use Borel-Cantelli's lemma to show that My attempt: So from Borel-Cantelli we have and using the definition of ""infinitely often"": But I don't see how I get the inclusion into the event containing .",(X_n)_{n\geq 1} c > 0 \sum_{n=1}^\infty P(X_n^2 > n) < \infty \Rightarrow P(|X_n|\geq c\sqrt{n} \hspace{7pt} \text{i.o.} \hspace{7pt} )=0. P(X_n^2 > n \hspace{7pt}\text{i.o.}\hspace{7pt})=0 \bigcap_{m=1}^\infty \bigcup_{n=1}^\infty (X_n^2 > n)=\bigcap_{m=1}^\infty \bigcup_{n=1}^\infty (|X_n| > \sqrt{n}) c,"['probability', 'probability-theory', 'probability-limit-theorems', 'borel-cantelli-lemmas']"
36,Finding the probability that an inner product is positive,Finding the probability that an inner product is positive,,"Let $x$ be a fixed vector in $\mathbb{R}^n$ . Let $a=[a_1,\cdots,a_n]^T$ be a random vector whose entries are iid random variables, say, $a_i \sim P$ . I would like to compute $$ P(\langle a, x \rangle > 0)  $$ Here is my attempt. Without loss of generality, let $x_1 > 0$ .  Then \begin{align*} P(\langle a, x \rangle > 0) = \int_\mathbb{R} \dots \int_{\mathbb{R}} \int_{a_1 > -\frac{\sum_{j=2}^na_jx_j}{x_1}} p(a_1)\cdots p(a_n)da_1\cdots da_n. \end{align*} If $P$ is an uniform distribution on $[-M,M]$ , the above becomes \begin{align*} P(\langle a, x \rangle > 0) &= \frac{1}{(2M)^n}\int_{-M}^M \dots \int_{-M}^M \int_{a_1 > -\frac{\sum_{j=2}^na_jx_j}{x_1}} da_1\cdots da_n \\ &=\frac{1}{(2M)^n}\int_{-M}^M \dots \int_{-M}^M \left(M + \frac{\sum_{j=2}^na_jx_j}{x_1}\right) da_2\cdots da_n \\ &= \frac{1}{2}. \end{align*} If $a_i \sim \mathcal{N}(0,\sigma^2)$ , since $\langle a, x \rangle = \sum_{i=1}^na_ix_i \sim \mathcal{N}(0,\|x\|^2\sigma^2)$ , one can easily conclude $P(\langle a,x \rangle > 0) = 0.5$ . Question In what class of distribution $P$ , can we derive $$P(\langle a,x \rangle > 0) = 0.5?$$ I thought the symmetric distribution around 0 could result in the same result, however, it is unclear to me. Any comments/answers will very be appreciated.","Let be a fixed vector in . Let be a random vector whose entries are iid random variables, say, . I would like to compute Here is my attempt. Without loss of generality, let .  Then If is an uniform distribution on , the above becomes If , since , one can easily conclude . Question In what class of distribution , can we derive I thought the symmetric distribution around 0 could result in the same result, however, it is unclear to me. Any comments/answers will very be appreciated.","x \mathbb{R}^n a=[a_1,\cdots,a_n]^T a_i \sim P 
P(\langle a, x \rangle > 0) 
 x_1 > 0 \begin{align*}
P(\langle a, x \rangle > 0) = \int_\mathbb{R} \dots \int_{\mathbb{R}} \int_{a_1 > -\frac{\sum_{j=2}^na_jx_j}{x_1}} p(a_1)\cdots p(a_n)da_1\cdots da_n.
\end{align*} P [-M,M] \begin{align*}
P(\langle a, x \rangle > 0) &= \frac{1}{(2M)^n}\int_{-M}^M \dots \int_{-M}^M \int_{a_1 > -\frac{\sum_{j=2}^na_jx_j}{x_1}} da_1\cdots da_n \\
&=\frac{1}{(2M)^n}\int_{-M}^M \dots \int_{-M}^M \left(M + \frac{\sum_{j=2}^na_jx_j}{x_1}\right) da_2\cdots da_n \\
&= \frac{1}{2}.
\end{align*} a_i \sim \mathcal{N}(0,\sigma^2) \langle a, x \rangle = \sum_{i=1}^na_ix_i \sim \mathcal{N}(0,\|x\|^2\sigma^2) P(\langle a,x \rangle > 0) = 0.5 P P(\langle a,x \rangle > 0) = 0.5?","['probability', 'probability-distributions', 'inner-products']"
37,Seating arrangements: Question about the book solution and summation indices,Seating arrangements: Question about the book solution and summation indices,,"$20$ people are to be seated at seven tables, three of which have 4 seats and four of which have 2 seats. If the people are randomly seated, find the expected value of the number of married couples that are seated at the same table. My question 1: In the book solution (see screenshot below) I don't understand the last equality, specifically the indexes that go from $i=1$ to $22$ and $19$ respectively. It seems they should both go from $i=1$ to $10$ . My question 2: Where am I going wrong in my approach? Let $X$ be the number of married couples sitting at the same table. Let $X_i = 1$ if couple $i$ is sitting at the same table for $i=1,...,10$ . Then $$E[X] = E\left[\sum_{i=1}^{10} X_i \right] = \sum_{i=1}^{10} E\left[X_i \right] = 10 \cdot P(X_1 = 1)$$ where the last equality comes from LOE and symmetry. To find $P(X_1 = 1)$ I will condition on the event $A =$ the husband is at a table with $4$ seats and the event $B =$ the husband is at a table with $2$ seats. $$P(X_1 = 1) = P(X_1 = 1 \mid A)P(A) + P(X_1 = 1 \mid B)P(B)$$ $$=\frac{3}{19}\frac{12}{20}+\frac{1}{19}\frac{8}{20} \approx .1157$$ and so $E[X] \approx 1.157$ which does not match the book solution of $2.48$ . Where am I going off the rails? Thanks for your help and patience. Book solution","people are to be seated at seven tables, three of which have 4 seats and four of which have 2 seats. If the people are randomly seated, find the expected value of the number of married couples that are seated at the same table. My question 1: In the book solution (see screenshot below) I don't understand the last equality, specifically the indexes that go from to and respectively. It seems they should both go from to . My question 2: Where am I going wrong in my approach? Let be the number of married couples sitting at the same table. Let if couple is sitting at the same table for . Then where the last equality comes from LOE and symmetry. To find I will condition on the event the husband is at a table with seats and the event the husband is at a table with seats. and so which does not match the book solution of . Where am I going off the rails? Thanks for your help and patience. Book solution","20 i=1 22 19 i=1 10 X X_i = 1 i i=1,...,10 E[X] = E\left[\sum_{i=1}^{10} X_i \right] = \sum_{i=1}^{10} E\left[X_i \right] = 10 \cdot P(X_1 = 1) P(X_1 = 1) A = 4 B = 2 P(X_1 = 1) = P(X_1 = 1 \mid A)P(A) + P(X_1 = 1 \mid B)P(B) =\frac{3}{19}\frac{12}{20}+\frac{1}{19}\frac{8}{20} \approx .1157 E[X] \approx 1.157 2.48","['probability', 'combinatorics', 'conditional-expectation', 'expected-value']"
38,Order in a Biased Coin [closed],Order in a Biased Coin [closed],,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Situation: Consider the classic coin tossing experiment. We want to explore if the coin is biased.  Coin 1: Coin is tossed $50$ times. We get $20$T and then $30$H, in that sequence Coin 2: Coin is tossed $50$ times. We get $4$T $6$H, $4$T $6$H, $4$T $6$H, $4$T $6$H, $4$T $6$H in that sequence. Q1 : Is the probability of Coin 1 and Coin 2 being biased is the same? My gut feel is that yes, because each event is independent so the order of the events doesn't matter at all. Q2:  I have a coin toss where observations are not independent. P(H | Previous toss is tail) = $0.6$ and P(T | Previous Head) = $0.5$  What kind of statistical test can I use to check the probability of coin being biased?","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Situation: Consider the classic coin tossing experiment. We want to explore if the coin is biased.  Coin 1: Coin is tossed $50$ times. We get $20$T and then $30$H, in that sequence Coin 2: Coin is tossed $50$ times. We get $4$T $6$H, $4$T $6$H, $4$T $6$H, $4$T $6$H, $4$T $6$H in that sequence. Q1 : Is the probability of Coin 1 and Coin 2 being biased is the same? My gut feel is that yes, because each event is independent so the order of the events doesn't matter at all. Q2:  I have a coin toss where observations are not independent. P(H | Previous toss is tail) = $0.6$ and P(T | Previous Head) = $0.5$  What kind of statistical test can I use to check the probability of coin being biased?",,['probability']
39,Probability of k random integers being coprimes,Probability of k random integers being coprimes,,"In this section of the Wikipedia article on coprime integers, it is stated that: More generally, the probability of $k$ randomly chosen integers being coprime is $1/\zeta(k)$. where $\zeta$ is the Riemann zeta function. Although there is no proof of this, the statement follows a fleshed out explanation about the probability of two random integers being coprime. Briefly, it proceeds as follows: The probability of any integer being divisible by some other integer $p$ is $1/p$ (because every $p^\mathrm{th}$ integer is). The probability that two random integers (chosen independently) are both divisible by $p$ is $1/p^2$. For a given integer, being divisible by primes $p$ or $q$ are two independent events, and hence the probability of both being true is $1/pq$. Therefore the probability that two random integers are coprimes (i.e. that they are not both divisible by any prime) is: $$ \prod_{\text{prime } p} 1 - \frac{1}{p^2}  = \left( \prod_{\text{prime } p} \frac{1}{1 - p^{-2}} \right)^{-1} = \frac{1}{\zeta(2)} \approx 0.61 $$ And it seems straightforward to apply the same reasoning for $k$ integers: The probability of $k$ random integers chosen independently being divisible by $p$ is $1/p^k$. Hence the probability that they are all coprimes is obtained when they are not all divisible by any prime: $$ \prod_{\text{prime } p} 1 - \frac{1}{p^k}  = \frac{1}{\zeta(k)} $$ My problem with this is one of intuition ; intuitively, considering several integers should increase the chances of two of them having a common prime factor, and therefore the probability of $k$ random integers being coprime should asymptotically decrease with $k$. However I think $1/\zeta$ is increasing on $[1, \infty)$, and quickly converges to 1. Where am I going wrong? And what is the right way to think about this?","In this section of the Wikipedia article on coprime integers, it is stated that: More generally, the probability of $k$ randomly chosen integers being coprime is $1/\zeta(k)$. where $\zeta$ is the Riemann zeta function. Although there is no proof of this, the statement follows a fleshed out explanation about the probability of two random integers being coprime. Briefly, it proceeds as follows: The probability of any integer being divisible by some other integer $p$ is $1/p$ (because every $p^\mathrm{th}$ integer is). The probability that two random integers (chosen independently) are both divisible by $p$ is $1/p^2$. For a given integer, being divisible by primes $p$ or $q$ are two independent events, and hence the probability of both being true is $1/pq$. Therefore the probability that two random integers are coprimes (i.e. that they are not both divisible by any prime) is: $$ \prod_{\text{prime } p} 1 - \frac{1}{p^2}  = \left( \prod_{\text{prime } p} \frac{1}{1 - p^{-2}} \right)^{-1} = \frac{1}{\zeta(2)} \approx 0.61 $$ And it seems straightforward to apply the same reasoning for $k$ integers: The probability of $k$ random integers chosen independently being divisible by $p$ is $1/p^k$. Hence the probability that they are all coprimes is obtained when they are not all divisible by any prime: $$ \prod_{\text{prime } p} 1 - \frac{1}{p^k}  = \frac{1}{\zeta(k)} $$ My problem with this is one of intuition ; intuitively, considering several integers should increase the chances of two of them having a common prime factor, and therefore the probability of $k$ random integers being coprime should asymptotically decrease with $k$. However I think $1/\zeta$ is increasing on $[1, \infty)$, and quickly converges to 1. Where am I going wrong? And what is the right way to think about this?",,"['probability', 'elementary-number-theory', 'prime-numbers', 'coprime']"
40,"Maximum Likelihood Estimator for $\theta$ when $X_1,\dots, X_n \sim U(-\theta,\theta)$",Maximum Likelihood Estimator for  when,"\theta X_1,\dots, X_n \sim U(-\theta,\theta)","Exercise : Calculate a Maximum Likelihood Estimator for the model $X_1,\dots, X_n \; \sim U(-\theta,\theta)$. Solution : The distribution function $f(x)$ for the given Uniform model is : $$f(x) = \begin{cases} 1/2\theta, \; \; -\theta \leq x \leq \theta \\ 0 \quad \; \; , \quad\text{elsewhere} \end{cases}$$ Thus, we can calculate the likelihood function as : $$L(\theta)=\bigg(\frac{1}{2\theta}\bigg)^n\prod_{i=1}^n\mathbb I_{[-\theta,\theta]}(x_i)= \bigg(\frac{1}{2\theta}\bigg)^n\prod_{i=1}^n \mathbb I_{[0,\theta]}(|x_i|) $$ $$=$$ $$\bigg(\frac{1}{2\theta}\bigg)^n\prod_{i=1}^n \mathbb I_{[-\infty,\theta]}(|x_i|)\prod_{i=1}^n \mathbb I_{[0, +\infty]}(|x_i|)$$ $$=$$ $$\boxed{\bigg(\frac{1}{2\theta}\bigg)^n\prod_{i=1}^n \mathbb I_{[-\infty,\theta]}(\max|x_i|)}$$ Question : How does one derive the final expression in the box from the previous one ? I can't seem to comprehend how this is equal to the step before. Other than that, to find the maximum likelihood estimator you need a $\theta$ sufficiently small but also $\max |x_i| \leq \theta$ which means that the MLE is : $\hat{\theta} = \max |x_i|$.","Exercise : Calculate a Maximum Likelihood Estimator for the model $X_1,\dots, X_n \; \sim U(-\theta,\theta)$. Solution : The distribution function $f(x)$ for the given Uniform model is : $$f(x) = \begin{cases} 1/2\theta, \; \; -\theta \leq x \leq \theta \\ 0 \quad \; \; , \quad\text{elsewhere} \end{cases}$$ Thus, we can calculate the likelihood function as : $$L(\theta)=\bigg(\frac{1}{2\theta}\bigg)^n\prod_{i=1}^n\mathbb I_{[-\theta,\theta]}(x_i)= \bigg(\frac{1}{2\theta}\bigg)^n\prod_{i=1}^n \mathbb I_{[0,\theta]}(|x_i|) $$ $$=$$ $$\bigg(\frac{1}{2\theta}\bigg)^n\prod_{i=1}^n \mathbb I_{[-\infty,\theta]}(|x_i|)\prod_{i=1}^n \mathbb I_{[0, +\infty]}(|x_i|)$$ $$=$$ $$\boxed{\bigg(\frac{1}{2\theta}\bigg)^n\prod_{i=1}^n \mathbb I_{[-\infty,\theta]}(\max|x_i|)}$$ Question : How does one derive the final expression in the box from the previous one ? I can't seem to comprehend how this is equal to the step before. Other than that, to find the maximum likelihood estimator you need a $\theta$ sufficiently small but also $\max |x_i| \leq \theta$ which means that the MLE is : $\hat{\theta} = \max |x_i|$.",,"['probability', 'statistics', 'maximum-likelihood']"
41,Continuous-time Markov Chain question,Continuous-time Markov Chain question,,"Consider an immigration-death model $X = (X_t)_{t\geq0}$, i.e. a model where immigrants arrive according to a Poisson process with rate $\lambda$ and individuals have independent $Exp(\mu)$ lifetimes. Suppose $\lambda > 0$. Suppose $X_0 = 0$. (i) Perform a one-step analysis to justify carefully that $\Phi(t) = \mathbb{E}[z^{X_t} ]$ satisfies  $$\Phi(t)=e^{-\lambda t}+\int_0^t\lambda e^{-\lambda s}\Phi(t-s)(1-(1-z)e^{-\mu(t-s)})ds$$ (ii) Solve the integral equation to determine the distribution of $X_t$. This is an exam question but I can't seem to wrap my head around it: My attempt: $\mathbb{E}[z^{X_t}]=\int_0^{\infty}\mathbb{E}[z^{X_t}|T_1=s]\lambda e^{-\lambda s}ds=\int_0^t\mathbb{E}[z^{X_t}|T_1=s]\lambda e^{-\lambda s}ds+\int_t^{\infty}\mathbb{E}[z^{X_t}|T_1=s]\lambda e^{-\lambda s}ds=\int_0^t\mathbb{E}[z^{X_t}|T_1=s]\lambda e^{-\lambda s}ds+e^{-\lambda t}$ Now my idea is to apply the Markov property at the time $T_1=s$ which is a stopping time. However, I can't finish the proof since I don't get the extra multiplier $(1-(1-z)e^{-\mu(t-s)})ds$ For (ii) I did a standard multiplication and differentiation to get that the following holds: $$\Phi'(t)=-\lambda(1-z)e^{-\mu t}\Phi(t)$$ which after integrating gives $$\Phi(t)=Ce^{\frac{\lambda(1-z)}{\mu}e^{-\mu t}} \text{  for } \mu\neq 0$$ and $$\Phi(t)=Ce^{-\lambda(1-z)t} \text{  for } \mu=0$$ I can't associate this with any distribution and furthermore, I have a feeling I've messed up the calculations since $\Phi(0)$ should equal $0$. Any help is appreciated!","Consider an immigration-death model $X = (X_t)_{t\geq0}$, i.e. a model where immigrants arrive according to a Poisson process with rate $\lambda$ and individuals have independent $Exp(\mu)$ lifetimes. Suppose $\lambda > 0$. Suppose $X_0 = 0$. (i) Perform a one-step analysis to justify carefully that $\Phi(t) = \mathbb{E}[z^{X_t} ]$ satisfies  $$\Phi(t)=e^{-\lambda t}+\int_0^t\lambda e^{-\lambda s}\Phi(t-s)(1-(1-z)e^{-\mu(t-s)})ds$$ (ii) Solve the integral equation to determine the distribution of $X_t$. This is an exam question but I can't seem to wrap my head around it: My attempt: $\mathbb{E}[z^{X_t}]=\int_0^{\infty}\mathbb{E}[z^{X_t}|T_1=s]\lambda e^{-\lambda s}ds=\int_0^t\mathbb{E}[z^{X_t}|T_1=s]\lambda e^{-\lambda s}ds+\int_t^{\infty}\mathbb{E}[z^{X_t}|T_1=s]\lambda e^{-\lambda s}ds=\int_0^t\mathbb{E}[z^{X_t}|T_1=s]\lambda e^{-\lambda s}ds+e^{-\lambda t}$ Now my idea is to apply the Markov property at the time $T_1=s$ which is a stopping time. However, I can't finish the proof since I don't get the extra multiplier $(1-(1-z)e^{-\mu(t-s)})ds$ For (ii) I did a standard multiplication and differentiation to get that the following holds: $$\Phi'(t)=-\lambda(1-z)e^{-\mu t}\Phi(t)$$ which after integrating gives $$\Phi(t)=Ce^{\frac{\lambda(1-z)}{\mu}e^{-\mu t}} \text{  for } \mu\neq 0$$ and $$\Phi(t)=Ce^{-\lambda(1-z)t} \text{  for } \mu=0$$ I can't associate this with any distribution and furthermore, I have a feeling I've messed up the calculations since $\Phi(0)$ should equal $0$. Any help is appreciated!",,"['probability', 'probability-theory', 'probability-distributions', 'markov-chains', 'markov-process']"
42,Disk on a tile floor [duplicate],Disk on a tile floor [duplicate],,"This question already has an answer here : Hidden variables in probability (1 answer) Closed 6 years ago . A disk 2 inches in diameter is thrown at random on a tiled floor, where each tile is a square with sides 4 inches in length. Let C be the event that the disk will land entirely on one tile. In order to assign a value to P(C), consider the center of the disk. In what region must the center lie to ensure that the disk lies entirely on one tile? If you draw a picture, it should be clear that the center must lie within a square having sides of length 2 and with its center coincident with the center of a tile. Since the area of this square is 4 and the area of a tile is 16, it makes sense to let $P(C) = \frac{4}{16}.$ I don't really understand the statement for the last sentence ""Since the area of this square is 4 and the area of a tile is 16, it makes sense to let $P(C) = \frac{4}{16}.$""","This question already has an answer here : Hidden variables in probability (1 answer) Closed 6 years ago . A disk 2 inches in diameter is thrown at random on a tiled floor, where each tile is a square with sides 4 inches in length. Let C be the event that the disk will land entirely on one tile. In order to assign a value to P(C), consider the center of the disk. In what region must the center lie to ensure that the disk lies entirely on one tile? If you draw a picture, it should be clear that the center must lie within a square having sides of length 2 and with its center coincident with the center of a tile. Since the area of this square is 4 and the area of a tile is 16, it makes sense to let $P(C) = \frac{4}{16}.$ I don't really understand the statement for the last sentence ""Since the area of this square is 4 and the area of a tile is 16, it makes sense to let $P(C) = \frac{4}{16}.$""",,"['probability', 'probability-theory']"
43,Conditional expectation for number of heads in a coin tossing depending on number shown on die,Conditional expectation for number of heads in a coin tossing depending on number shown on die,,"A fair die is tossed and a coin is thrown the number of times as the score shown on the die. If any heads are shown in the throws of the coin, we sop, otherwise, we continue the experiment of tossing the die and coin until at least one head is shown. Find the expected number of throws of the coin before we stop. Denote $ N: N ^o$ shown on dice : ${1,2,3,4,5,6}$ $ \therefore  \mathbf{P} (N=n) = \frac{1}{6} $ $ \mathbf{P} (X=H) = \frac{1}{2}$ $ \mathbf{E}(N=n) = \sum \mathbf{E}(N=n|X=H) \ .\ \mathbf{P}(X=H) $ $ =\frac{1}{2}   \sum \mathbf{E}(N=n|X=H) $ Then I am not sure if I am on the right track. The answer is $\frac{448}{107}$","A fair die is tossed and a coin is thrown the number of times as the score shown on the die. If any heads are shown in the throws of the coin, we sop, otherwise, we continue the experiment of tossing the die and coin until at least one head is shown. Find the expected number of throws of the coin before we stop. Denote $ N: N ^o$ shown on dice : ${1,2,3,4,5,6}$ $ \therefore  \mathbf{P} (N=n) = \frac{1}{6} $ $ \mathbf{P} (X=H) = \frac{1}{2}$ $ \mathbf{E}(N=n) = \sum \mathbf{E}(N=n|X=H) \ .\ \mathbf{P}(X=H) $ $ =\frac{1}{2}   \sum \mathbf{E}(N=n|X=H) $ Then I am not sure if I am on the right track. The answer is $\frac{448}{107}$",,"['probability', 'conditional-expectation']"
44,Poisson random variable is not sub gaussian,Poisson random variable is not sub gaussian,,"I am reading a chapter on concentration inequalities, and I am struggling to make connections on sub-Gaussian random variables.  A random variable is sub-Gaussian if there exists $C > 0$ such that $P(|X| \geq t) \leq 2\exp(-t^2/C^2)$.  The text gives numerous equivalent conditions to this one. I have an exercise:  If $X \sim Poi(\lambda)$, show that $X$ is not sub-Gaussian. My setup is $P(|X| \geq t) = P(X \geq t) = \sum_{k=t}^{\infty} \frac{e^{-\lambda}\lambda^{k}}{k!}$ since the Poisson random variable is supported on nonnegative integers.  My first thought was to use Stirling's formula on the $k!$, but I wouldn't know how to sum the resulting series.  I appreciate any help on this question!","I am reading a chapter on concentration inequalities, and I am struggling to make connections on sub-Gaussian random variables.  A random variable is sub-Gaussian if there exists $C > 0$ such that $P(|X| \geq t) \leq 2\exp(-t^2/C^2)$.  The text gives numerous equivalent conditions to this one. I have an exercise:  If $X \sim Poi(\lambda)$, show that $X$ is not sub-Gaussian. My setup is $P(|X| \geq t) = P(X \geq t) = \sum_{k=t}^{\infty} \frac{e^{-\lambda}\lambda^{k}}{k!}$ since the Poisson random variable is supported on nonnegative integers.  My first thought was to use Stirling's formula on the $k!$, but I wouldn't know how to sum the resulting series.  I appreciate any help on this question!",,['probability']
45,Probability that no letter is in alphabetical order,Probability that no letter is in alphabetical order,,"Given a random string of distinct letters, find the probability that none of the letters are in order. A letter is in order when every letter preceding it is of lower alphabetical value and every letter after it is higher. having trouble with the combinatorial approach because there seems to be no easy way to avoid over-counting possibilities. Example of none in order: dbac d: bac should precede it - not in order b: a should precede it and cd should come after - not in order a: bac should come after - not in order c: ab should precede it, d should come after it - not in order","Given a random string of distinct letters, find the probability that none of the letters are in order. A letter is in order when every letter preceding it is of lower alphabetical value and every letter after it is higher. having trouble with the combinatorial approach because there seems to be no easy way to avoid over-counting possibilities. Example of none in order: dbac d: bac should precede it - not in order b: a should precede it and cd should come after - not in order a: bac should come after - not in order c: ab should precede it, d should come after it - not in order",,"['probability', 'combinatorics', 'statistics']"
46,Concentration inequality for covariance,Concentration inequality for covariance,,"Is there any concentration inequality for the covaraince of two scalar random variables? For example, how can I found a tight upper bound for the following probability? $$\Pr\left( {\left| {{\mathop{\rm cov}} (x,y) - \overline {{\mathop{\rm cov}} (x,y)} } \right| \le \varepsilon } \right)$$ where $\overline {{\mathop{\rm cov}} (x,y)}  = E\left( {x - E(x)} \right)E\left( {y - E(y)} \right)$ is the actual covariance and ${\mathop{\rm cov}} (x,y) = \frac{1}{N}\sum\limits_{n = 1}^N {\left( {{x_i} - \hat x} \right)\left( {{y_i} - \hat y} \right)} $ is the sample covariance.","Is there any concentration inequality for the covaraince of two scalar random variables? For example, how can I found a tight upper bound for the following probability? $$\Pr\left( {\left| {{\mathop{\rm cov}} (x,y) - \overline {{\mathop{\rm cov}} (x,y)} } \right| \le \varepsilon } \right)$$ where $\overline {{\mathop{\rm cov}} (x,y)}  = E\left( {x - E(x)} \right)E\left( {y - E(y)} \right)$ is the actual covariance and ${\mathop{\rm cov}} (x,y) = \frac{1}{N}\sum\limits_{n = 1}^N {\left( {{x_i} - \hat x} \right)\left( {{y_i} - \hat y} \right)} $ is the sample covariance.",,"['probability', 'probability-theory', 'random-variables', 'covariance', 'concentration-of-measure']"
47,Number of successes when the successes are positively correlated,Number of successes when the successes are positively correlated,,"In $n$ trials, each with success-probability $p$, what is the probability of at least $k$ successes, $P[n,k]$ ? The answer depends on the dependence between the trials: A. If the trials are independent, then the number of successes is a Binomial variable, so: $$ P^{\text{ind}}[n,k] = \sum_{i=k}^n {n\choose k} p^i (1-p)^{n-1} $$ B. If the trials can be arbitrarily dependent, then $P[n,k]$ might be 0. E.g, take $p=1/2$, $n=2,k=2$, and assume $X_2 = 1 - X_1$ (trial 2 fails iff 1 succeeds; from Henry's comment). C. Now suppose that the variables are dependent ""in the good direction"": $$ \forall i_1,\ldots,i_j: \Pr[X_{i_1}=1\cap\cdots\cap X_{i_j}=1] \geq \Pr[X_{i_1}=1]\cdots\Pr[X_{i_j}=1] = p^j $$ that is, if we already had some successes, the probability of another success weakly increases. Here, $P[n,k]$ cannot be zero. A trivial upper bound is $P[n,k] \geq p^k$ (as commented by muzzlator), since even if we consider only trials $1,\ldots,k$, the assumption implies that the probability  all of them succeed is at least $p^k$ Is there a better lower bound for $P[n,k]$ in this case? Initially I thought that $P[n,k]\geq P^{\text{ind}}[n,k]$, since intuitively, the dependence between the variables can only increase the number of successes. However, this is not true. As a simple example (from muzzlator's comment), take $k=1, n\to\infty$ and assume that all variables are the same: $X_i=X_1$ for all $i$. Then, $P^{\text{ind}}[n,1]\to 1$ while $P[n,1]=p$. So, what is a correct lower bound on $P[n,k]$?","In $n$ trials, each with success-probability $p$, what is the probability of at least $k$ successes, $P[n,k]$ ? The answer depends on the dependence between the trials: A. If the trials are independent, then the number of successes is a Binomial variable, so: $$ P^{\text{ind}}[n,k] = \sum_{i=k}^n {n\choose k} p^i (1-p)^{n-1} $$ B. If the trials can be arbitrarily dependent, then $P[n,k]$ might be 0. E.g, take $p=1/2$, $n=2,k=2$, and assume $X_2 = 1 - X_1$ (trial 2 fails iff 1 succeeds; from Henry's comment). C. Now suppose that the variables are dependent ""in the good direction"": $$ \forall i_1,\ldots,i_j: \Pr[X_{i_1}=1\cap\cdots\cap X_{i_j}=1] \geq \Pr[X_{i_1}=1]\cdots\Pr[X_{i_j}=1] = p^j $$ that is, if we already had some successes, the probability of another success weakly increases. Here, $P[n,k]$ cannot be zero. A trivial upper bound is $P[n,k] \geq p^k$ (as commented by muzzlator), since even if we consider only trials $1,\ldots,k$, the assumption implies that the probability  all of them succeed is at least $p^k$ Is there a better lower bound for $P[n,k]$ in this case? Initially I thought that $P[n,k]\geq P^{\text{ind}}[n,k]$, since intuitively, the dependence between the variables can only increase the number of successes. However, this is not true. As a simple example (from muzzlator's comment), take $k=1, n\to\infty$ and assume that all variables are the same: $X_i=X_1$ for all $i$. Then, $P^{\text{ind}}[n,1]\to 1$ while $P[n,1]=p$. So, what is a correct lower bound on $P[n,k]$?",,"['probability', 'independence', 'correlation']"
48,Is the maximum likelihood estimator always a sufficient statistic?,Is the maximum likelihood estimator always a sufficient statistic?,,"Here's an example of what I am asking : $X_1,\ldots,X_n $ i.i.d $N(\phi , 1)$ where $\phi \in \mathbb{R}$ . Let $\gamma = P(X_1\leq 1)$ . Give a sufficient statistic of $\gamma$ . This question is part of a midterm exam I took on april. I see that $\gamma_{m\ell} = \Phi(1 - \phi_{m\ell}))$ , where $\Phi$ is the standard normal distribution function. I know that in this case, $\phi_{m\ell} = n^{-1}\sum(X_i^2)$ which is a sufficient statistic. Does that make $\gamma_{m\ell}$ one also ? Thank you for your answers.","Here's an example of what I am asking : i.i.d where . Let . Give a sufficient statistic of . This question is part of a midterm exam I took on april. I see that , where is the standard normal distribution function. I know that in this case, which is a sufficient statistic. Does that make one also ? Thank you for your answers.","X_1,\ldots,X_n  N(\phi , 1) \phi \in \mathbb{R} \gamma = P(X_1\leq 1) \gamma \gamma_{m\ell} = \Phi(1 - \phi_{m\ell})) \Phi \phi_{m\ell} = n^{-1}\sum(X_i^2) \gamma_{m\ell}","['probability', 'statistics']"
49,Probability of first complete set from a standard 52 card deck,Probability of first complete set from a standard 52 card deck,,"Suppose we have a standard, $52$-card deck that's been shuffled using a strong RNG and an unbiased shuffling algorithm. We draw one card at a time, without replacement, and stop as soon as we observe all four Aces. Given $k$, what is the probability that we observe all four Aces before we observe four of any other rank on the $k$th card, where $k = 4, 5, \ldots, 40$? In other words, and for example, if $k = 20$, what is the probability that the first $20$ cards contain exactly four Aces, with the fourth Ace being drawn on the $20th$ card and that the remaining sixteen cards are not four of any other rank? For $k = 4, 5, 6, 7$ this looks to be straightforward. But for $k \geq 8$ this feels increasingly headache-y. Thoughts on an approach For the $k = 20$ case from above, it seems like I have to, among other things, count the number of ways to partition $16$ [the remaining cards] with $12$ objects [the remaining ranks] where each object can get no more than three of a copy and then weighting each of these partitions. For example, I could observe a partition like $3 + 3 + 3 + 3 + 3 + 1$ as permutations of KKKQQQJJJTTT9998AAAA where one of the As is always at the end. Then I would weight each partition of $16$ from $12$ by the number of permutations and number of ways of choosing similar ranks. I can always simulate this, but for fun, I'm trying to see if there is a practical, exact approach. EDIT 6/9/2017 I seem to have caused some confusion with my example and original wording. Here is (hopefully) better wording of what I'm after. Fix $k$. The sample space is the set of permutations in which the fourth ace is drawn on the $k$th card. What proportion of those permutations contain only three or fewer occurrences of all other ranks in the first $k$ cards?","Suppose we have a standard, $52$-card deck that's been shuffled using a strong RNG and an unbiased shuffling algorithm. We draw one card at a time, without replacement, and stop as soon as we observe all four Aces. Given $k$, what is the probability that we observe all four Aces before we observe four of any other rank on the $k$th card, where $k = 4, 5, \ldots, 40$? In other words, and for example, if $k = 20$, what is the probability that the first $20$ cards contain exactly four Aces, with the fourth Ace being drawn on the $20th$ card and that the remaining sixteen cards are not four of any other rank? For $k = 4, 5, 6, 7$ this looks to be straightforward. But for $k \geq 8$ this feels increasingly headache-y. Thoughts on an approach For the $k = 20$ case from above, it seems like I have to, among other things, count the number of ways to partition $16$ [the remaining cards] with $12$ objects [the remaining ranks] where each object can get no more than three of a copy and then weighting each of these partitions. For example, I could observe a partition like $3 + 3 + 3 + 3 + 3 + 1$ as permutations of KKKQQQJJJTTT9998AAAA where one of the As is always at the end. Then I would weight each partition of $16$ from $12$ by the number of permutations and number of ways of choosing similar ranks. I can always simulate this, but for fun, I'm trying to see if there is a practical, exact approach. EDIT 6/9/2017 I seem to have caused some confusion with my example and original wording. Here is (hopefully) better wording of what I'm after. Fix $k$. The sample space is the set of permutations in which the fourth ace is drawn on the $k$th card. What proportion of those permutations contain only three or fewer occurrences of all other ranks in the first $k$ cards?",,"['probability', 'combinatorics']"
50,Reciprocal of Expectation,Reciprocal of Expectation,,"Let $X$ be a random variable. Is it ever true that $$E\left({\frac 1 X}\right) \stackrel{?}{=} \frac 1 {E(X)} \text{ ?}$$ I'll assume $X$ never takes on the value of $0$. I'll use the notation for discrete RV's, otherwise replace $\sum$ with $\int$. Let $f$ be the mass/density function. $$E\left({\frac 1 X}\right) = \sum \frac 1 x f(x)$$ $$\frac 1 {E(X)} = \frac 1 {\sum x f(x)}$$ Assume they're equal. Then: $$\sum \frac 1 x f(x) = \frac 1 {\sum x f(x)}$$ $$\implies \left(\sum \frac 1 x f(x)\right)\left(\sum xf(x)\right) = 1$$ This is a type of convolution, which would mean that $$\dfrac {f(x)}{x} = (x f(x))^{-1}$$ in regards to this convolution. Is that possible? Do functions have inverses under convolution?","Let $X$ be a random variable. Is it ever true that $$E\left({\frac 1 X}\right) \stackrel{?}{=} \frac 1 {E(X)} \text{ ?}$$ I'll assume $X$ never takes on the value of $0$. I'll use the notation for discrete RV's, otherwise replace $\sum$ with $\int$. Let $f$ be the mass/density function. $$E\left({\frac 1 X}\right) = \sum \frac 1 x f(x)$$ $$\frac 1 {E(X)} = \frac 1 {\sum x f(x)}$$ Assume they're equal. Then: $$\sum \frac 1 x f(x) = \frac 1 {\sum x f(x)}$$ $$\implies \left(\sum \frac 1 x f(x)\right)\left(\sum xf(x)\right) = 1$$ This is a type of convolution, which would mean that $$\dfrac {f(x)}{x} = (x f(x))^{-1}$$ in regards to this convolution. Is that possible? Do functions have inverses under convolution?",,"['probability', 'expectation', 'convolution']"
51,Show that the pdf is ${1 \over {(n-1)}!} \sum\limits_{n \leq j \leq \lfloor x\rfloor}(-1)^j \binom{n}{j}(x-j)^{n-1}$,Show that the pdf is,{1 \over {(n-1)}!} \sum\limits_{n \leq j \leq \lfloor x\rfloor}(-1)^j \binom{n}{j}(x-j)^{n-1},"Let $(X_n)_{n \in \mathbb N}$ be an independent, on $[0,1]$ uniformly distributed random variable. How can I show that the probability density function of $$\sum\limits_{i=1}^nX_i={1 \over {(n-1)}!} \sum_{n \leq j \leq \lfloor x\rfloor}(-1)^j \binom{n}{j}(x-j)^{n-1} \qquad 0 \leq x \leq n$$ So for a single random variable the pdf would be just $f(x)=1$ I guess? Not sure if this is gonna help me in some way.. Some hints would be much appreciated.","Let $(X_n)_{n \in \mathbb N}$ be an independent, on $[0,1]$ uniformly distributed random variable. How can I show that the probability density function of $$\sum\limits_{i=1}^nX_i={1 \over {(n-1)}!} \sum_{n \leq j \leq \lfloor x\rfloor}(-1)^j \binom{n}{j}(x-j)^{n-1} \qquad 0 \leq x \leq n$$ So for a single random variable the pdf would be just $f(x)=1$ I guess? Not sure if this is gonna help me in some way.. Some hints would be much appreciated.",,"['probability', 'probability-theory', 'density-function']"
52,Chess board probability problem.,Chess board probability problem.,,Three random squares are chosen from a regular chess board. Find the probability that they form the letter 'L'. I cannot think about a general way to go about these type of questions. Need hints or solutions.,Three random squares are chosen from a regular chess board. Find the probability that they form the letter 'L'. I cannot think about a general way to go about these type of questions. Need hints or solutions.,,"['probability', 'combinatorics']"
53,Simple Random Walk: two questions,Simple Random Walk: two questions,,"I am having difficulty in finding right resource to review. I am preparing interview on probability. One particular topic that I struggle the most is Simple Random Walk. I just want to know the following: 1) finding the first $n$ for which $S_n$ reaches a defined threshold $\alpha$. 2) the probability that $S_n$ reaches $\alpha$ for any given value of $n$. 3) Expected number of steps to reach an end point. I am wondering where I can find examples specifically for these 3 types of question? Thanks. ------- Maybe I can try to answer my questions.  Assume symmetric. Assume we start from 0. 1) the probability of first n for which $S_n$ reaches 10 is P($S_{10} =10) = 1/2^{10}$ 2) for given value of $n$, we want to know the probability that $S_n$ reaches $\alpha$. This is equivalent as if asking $\max(S_1,S_2,S_3,\dots, S_n) \geq \alpha$. And the maximum of this probability formula is given by here : http://www.randomservices.org/random/bernoulli/Walk.html 3) This is simply the gambler ruin's problem, and you can look up the expected time for a player to ruin, which is $\alpha / (\alpha + \beta)$","I am having difficulty in finding right resource to review. I am preparing interview on probability. One particular topic that I struggle the most is Simple Random Walk. I just want to know the following: 1) finding the first $n$ for which $S_n$ reaches a defined threshold $\alpha$. 2) the probability that $S_n$ reaches $\alpha$ for any given value of $n$. 3) Expected number of steps to reach an end point. I am wondering where I can find examples specifically for these 3 types of question? Thanks. ------- Maybe I can try to answer my questions.  Assume symmetric. Assume we start from 0. 1) the probability of first n for which $S_n$ reaches 10 is P($S_{10} =10) = 1/2^{10}$ 2) for given value of $n$, we want to know the probability that $S_n$ reaches $\alpha$. This is equivalent as if asking $\max(S_1,S_2,S_3,\dots, S_n) \geq \alpha$. And the maximum of this probability formula is given by here : http://www.randomservices.org/random/bernoulli/Walk.html 3) This is simply the gambler ruin's problem, and you can look up the expected time for a player to ruin, which is $\alpha / (\alpha + \beta)$",,"['probability', 'probability-theory', 'martingales', 'random-walk']"
54,Distribution of the index of the variable which achieves the minimum of exponential random variables,Distribution of the index of the variable which achieves the minimum of exponential random variables,,"I am reading Exponential distribution from Wiki, and it is said that the index of the variable which achieves the minimum is distributed according to the law $$P(k|X_k=min\{X_1,X_2,...,X_n\})=\frac{\lambda_k}{\lambda_1+...+\lambda_n}$$ I don't know how to prove this property. I try the case $n=2$ in different ways. First, I find $P(X_1\le X_2)=\frac{\lambda_1}{\lambda_1+\lambda_2}$, but I can't change this to the conditional probability formally. Second, I try to prove this through pdf. Let $Y=min\{X_1,X_2\}$, I want to calculate $f_{X_1|Y}(x_1,y)$. However I find that there should be infinite value of $f_{X_1|Y}(x_1,y)$ at $x_1=y$ , since the conditional probability is actually a discrete distribution. I don't know how to obtain the discrete distribution from continuous pdf. Please tell me how can I continue my proof or give another formal proof, Thanks!!","I am reading Exponential distribution from Wiki, and it is said that the index of the variable which achieves the minimum is distributed according to the law $$P(k|X_k=min\{X_1,X_2,...,X_n\})=\frac{\lambda_k}{\lambda_1+...+\lambda_n}$$ I don't know how to prove this property. I try the case $n=2$ in different ways. First, I find $P(X_1\le X_2)=\frac{\lambda_1}{\lambda_1+\lambda_2}$, but I can't change this to the conditional probability formally. Second, I try to prove this through pdf. Let $Y=min\{X_1,X_2\}$, I want to calculate $f_{X_1|Y}(x_1,y)$. However I find that there should be infinite value of $f_{X_1|Y}(x_1,y)$ at $x_1=y$ , since the conditional probability is actually a discrete distribution. I don't know how to obtain the discrete distribution from continuous pdf. Please tell me how can I continue my proof or give another formal proof, Thanks!!",,"['probability', 'probability-distributions']"
55,3 plate dinner problem,3 plate dinner problem,,Consider n people dining in a circular table. Each of them is ordering one of three plates. What is the probability that no two people sitting next to one another will order the same plate? I intuitively think that every person except the first one has 2 choices as he cannot order the same as the one preceding him. However i can't figure out what happens with the last person as he can have either 1 or 2 choices depending whether the person before him had chosen the same dinner as the first person.,Consider n people dining in a circular table. Each of them is ordering one of three plates. What is the probability that no two people sitting next to one another will order the same plate? I intuitively think that every person except the first one has 2 choices as he cannot order the same as the one preceding him. However i can't figure out what happens with the last person as he can have either 1 or 2 choices depending whether the person before him had chosen the same dinner as the first person.,,"['probability', 'combinatorics']"
56,Uniform Integrability implies boundedness of $\sup_i\int|f_i|dP$?,Uniform Integrability implies boundedness of ?,\sup_i\int|f_i|dP,"We defined a family of functions $\{f_i\}_{ i\in I}$ to be uniformly   integrable if for every $\epsilon>0$ there is a number M s.t $\forall$   i: $\int_{|f_i|>M}|f_i|dP<\epsilon$ a) $\{f_i\}_{i\in I}$ are uniformly integrable b) $\sup_i\int|f_i|dP<\infty$. 1) Does $b) \Rightarrow a)$? 2) Does $a) \Rightarrow b)$? So for 1) I think I have a good counter example to disprove, I used $ f_i=i\mathbb{1}_{[0,\frac{1}i]}$. Obviously $\sup_i\int|f_i|dP=1<\infty$.  I want to prove that it's not U.I so by definition I want to show that there exists $\epsilon>0 \ s.t\   \forall M, \exists i\ s.t \int_{|f_i|>M}|f_i|dP\geq\epsilon$ So if we choose $\epsilon$ to be 1 then for every $i>M$ we will get $\int_{|f_i|>M} |f_i|dP=1$. Now I'm not sure about proving $a) \Rightarrow b)$, what I did is the following: Because $\{f_i\}_{ i\in I}$ are UI, $\exists M s.t\  \forall i\ \int_{|f_i|>M}|f_i|dP<3$, so we have: $\sup_i\int|f_i|dP=\sup_i(\int_{|f_i|\leq M}|f_i|dP+\int_{|f_i|>M}|f_i|dP)\leq\sup_i\int_{|f_i|\leq M}MdP +3\leq \int_{0}^MMdP=M^2+3<\infty$ Is this correct?","We defined a family of functions $\{f_i\}_{ i\in I}$ to be uniformly   integrable if for every $\epsilon>0$ there is a number M s.t $\forall$   i: $\int_{|f_i|>M}|f_i|dP<\epsilon$ a) $\{f_i\}_{i\in I}$ are uniformly integrable b) $\sup_i\int|f_i|dP<\infty$. 1) Does $b) \Rightarrow a)$? 2) Does $a) \Rightarrow b)$? So for 1) I think I have a good counter example to disprove, I used $ f_i=i\mathbb{1}_{[0,\frac{1}i]}$. Obviously $\sup_i\int|f_i|dP=1<\infty$.  I want to prove that it's not U.I so by definition I want to show that there exists $\epsilon>0 \ s.t\   \forall M, \exists i\ s.t \int_{|f_i|>M}|f_i|dP\geq\epsilon$ So if we choose $\epsilon$ to be 1 then for every $i>M$ we will get $\int_{|f_i|>M} |f_i|dP=1$. Now I'm not sure about proving $a) \Rightarrow b)$, what I did is the following: Because $\{f_i\}_{ i\in I}$ are UI, $\exists M s.t\  \forall i\ \int_{|f_i|>M}|f_i|dP<3$, so we have: $\sup_i\int|f_i|dP=\sup_i(\int_{|f_i|\leq M}|f_i|dP+\int_{|f_i|>M}|f_i|dP)\leq\sup_i\int_{|f_i|\leq M}MdP +3\leq \int_{0}^MMdP=M^2+3<\infty$ Is this correct?",,"['probability', 'integration', 'probability-theory', 'measure-theory', 'uniform-integrability']"
57,Converge in probability and Expectation.,Converge in probability and Expectation.,,"Let $(\Omega, F , P)$ be probability space. Let $X_i$ be sequence of random variable and $X$ be Random variable. Claim. $X_n \to X$ in probability if and only if $E(\frac{|X_n-X|}{1+|X_n-X|})$ $\to 0$ I can not catch any hint. Can I get some hints?","Let $(\Omega, F , P)$ be probability space. Let $X_i$ be sequence of random variable and $X$ be Random variable. Claim. $X_n \to X$ in probability if and only if $E(\frac{|X_n-X|}{1+|X_n-X|})$ $\to 0$ I can not catch any hint. Can I get some hints?",,"['probability', 'random-variables', 'expectation']"
58,Sample proportion and the Central Limit Theorem,Sample proportion and the Central Limit Theorem,,"Suppose that $ (\Omega,\Sigma,\mathsf{P}) $ is a probability space and that $ (X_{k})_{k \in \mathbb{N}} $ is a sequence of i.i.d. Bernoulli trials on $ (\Omega,\Sigma,\mathsf{P}) $, each with probability of success $ p \in (0,1) $. If we define another sequence $ (\hat{P}_{n})_{n \in \mathbb{N}} $ of random variables on $ (\Omega,\Sigma,\mathsf{P}) $ by $$ \forall n \in \mathbb{N}: \qquad \hat{P}_{n} \stackrel{\text{df}}{=} \frac{1}{n} \sum_{k = 1}^{n} X_{k}, $$ then according to the Central Limit Theorem, we have $$ \forall z \in \mathbb{R}: \qquad   \lim_{n \to \infty}   \mathsf{P} \!   \left( \frac{\hat{P}_{n} - p}{\sqrt{p (1 - p) / n}} \leq z \right) = \Phi(z), $$ where $ \Phi $ denotes the standard normal c.d.f. For each $ n \in \mathbb{N} $, we call $ \hat{P}_{n} $ a sample proportion for a sample of size $ n $ . When most statistics textbooks discuss confidence intervals for a sample proportion, they implicitly claim that $$ \frac{\hat{P}_{n} - p}{\sqrt{\hat{P}_{n} (1 - \hat{P}_{n}) / n}} \stackrel{\text{d}}{\longrightarrow} \operatorname{N}(0,1), $$ which is the same as saying that $$ \forall z \in \mathbb{R}: \qquad   \lim_{n \to \infty}   \mathsf{P} \!   \left(   \frac{\hat{P}_{n} - p}{\sqrt{\hat{P}_{n} (1 - \hat{P}_{n}) / n}} \leq z   \right) = \Phi(z). $$ However, I was unable to rigorously establish this claim using the Central Limit Theorem. Could anyone kindly provide references? Thanks!","Suppose that $ (\Omega,\Sigma,\mathsf{P}) $ is a probability space and that $ (X_{k})_{k \in \mathbb{N}} $ is a sequence of i.i.d. Bernoulli trials on $ (\Omega,\Sigma,\mathsf{P}) $, each with probability of success $ p \in (0,1) $. If we define another sequence $ (\hat{P}_{n})_{n \in \mathbb{N}} $ of random variables on $ (\Omega,\Sigma,\mathsf{P}) $ by $$ \forall n \in \mathbb{N}: \qquad \hat{P}_{n} \stackrel{\text{df}}{=} \frac{1}{n} \sum_{k = 1}^{n} X_{k}, $$ then according to the Central Limit Theorem, we have $$ \forall z \in \mathbb{R}: \qquad   \lim_{n \to \infty}   \mathsf{P} \!   \left( \frac{\hat{P}_{n} - p}{\sqrt{p (1 - p) / n}} \leq z \right) = \Phi(z), $$ where $ \Phi $ denotes the standard normal c.d.f. For each $ n \in \mathbb{N} $, we call $ \hat{P}_{n} $ a sample proportion for a sample of size $ n $ . When most statistics textbooks discuss confidence intervals for a sample proportion, they implicitly claim that $$ \frac{\hat{P}_{n} - p}{\sqrt{\hat{P}_{n} (1 - \hat{P}_{n}) / n}} \stackrel{\text{d}}{\longrightarrow} \operatorname{N}(0,1), $$ which is the same as saying that $$ \forall z \in \mathbb{R}: \qquad   \lim_{n \to \infty}   \mathsf{P} \!   \left(   \frac{\hat{P}_{n} - p}{\sqrt{\hat{P}_{n} (1 - \hat{P}_{n}) / n}} \leq z   \right) = \Phi(z). $$ However, I was unable to rigorously establish this claim using the Central Limit Theorem. Could anyone kindly provide references? Thanks!",,"['probability', 'probability-theory', 'statistics', 'central-limit-theorem', 'probability-limit-theorems']"
59,Variable triples vs. single quad card game. Who has the advantage?,Variable triples vs. single quad card game. Who has the advantage?,,"$2$ people decide to count wins on a card game using a well shuffled standard $52$ card deck. Community (shared) cards are drawn one at a time from the deck without replacement until there is a winner for a hand. The rules are player A can initially win if $4$ triples appear (such as $KKK,444,AAA,777$).  Player B wins if a single quad appears (such as $QQQQ$).  As soon as there is a winner, the hand is finished, the win is awarded, all drawn cards are returned to the deck, the cards are reshuffled well, and the next hand will be drawn.  There is one twist however. If B wins a hand, then next hand will have a lower win threshold for A.  For example, initially the win threshold for A is $4$ triples.  However, if B wins the first hand, then the new threshold will be $3$ triples.  Conversely, each time A wins, A's threshold will be increased by $1$.  So for example, if A wins the first hand, A's new win threshold will be $5$ triples to win.  The minimum # of required triples is $1$. No more than $39$ cards will ever need to be drawn to determine a winner since even if $13$ triples are required, the $39$th card will guarantee a winner but it is likely a quad would have appeared way before then. Note that the triples and quads need not be in any order.  For example, $Q,2,Q,4,Q,7,A,10,Q$ is a quad.  The requirement is not $4$ like ranks in a row however that is also a quad but very unlikely. I am thinking since the difficulty of A winning is variable based on who wins the previous hand(s), this should reach some type of equilibrium where it is about equally likely for each to win in the longrun but how can I show this mathematically? I ran a computer simulation and it looks like my initial hypothesis is right.  There seems to be an equally likely chance for A or B to win on average.  Even with as few as $10$ trials ($10$ winning hands), I am seeing mostly $5$ wins for each but sometimes $4$ vs. $6$ but I didn't even see $3$ vs. $7$ yet so it seems to hit equilibrium VERY quickly.  Since the # of required trials (wins) is so low, you can probably just try this with a real deck of cards and confirm it is about $50/50$ with as few as $10$ winning hands.  Just remember to update the winning threshold for A properly (for example, $4, 5, 4, 3$...).  Unlike a fair coin toss where $8$, $9$, or even $10$ heads are possible, it seems almost impossible for that to happen with this type of ""self adjusting"" game. So how can it be shown mathematically that this type of game will reach equilibrium where either player has about the same chance to win overall? If you do a computer simulation of this, it is somewhat amazing how many times it will hit exactly half and half.  For example, if I run $1000$ decisions, I usually get $500$ A wins and $500$ B wins.  It is very consistent (yet I am using different random numbers each run).  This is WAY more predictable than something like fair coin flips which could get off to a ""rocky"" start.  I think you can call this type of game ""self adjusting"" in that it will reach a ""fair equilibrium"" very quickly. This seems like a hard problem to state mathematically so I put a bounty of $100$ points on it as an extra incentive to those of you who want to try to solve it.  If $2$ different users submit good answers then what I usually do is give one person the checkmark and the other the bounty to be more fair to both.  Good luck.","$2$ people decide to count wins on a card game using a well shuffled standard $52$ card deck. Community (shared) cards are drawn one at a time from the deck without replacement until there is a winner for a hand. The rules are player A can initially win if $4$ triples appear (such as $KKK,444,AAA,777$).  Player B wins if a single quad appears (such as $QQQQ$).  As soon as there is a winner, the hand is finished, the win is awarded, all drawn cards are returned to the deck, the cards are reshuffled well, and the next hand will be drawn.  There is one twist however. If B wins a hand, then next hand will have a lower win threshold for A.  For example, initially the win threshold for A is $4$ triples.  However, if B wins the first hand, then the new threshold will be $3$ triples.  Conversely, each time A wins, A's threshold will be increased by $1$.  So for example, if A wins the first hand, A's new win threshold will be $5$ triples to win.  The minimum # of required triples is $1$. No more than $39$ cards will ever need to be drawn to determine a winner since even if $13$ triples are required, the $39$th card will guarantee a winner but it is likely a quad would have appeared way before then. Note that the triples and quads need not be in any order.  For example, $Q,2,Q,4,Q,7,A,10,Q$ is a quad.  The requirement is not $4$ like ranks in a row however that is also a quad but very unlikely. I am thinking since the difficulty of A winning is variable based on who wins the previous hand(s), this should reach some type of equilibrium where it is about equally likely for each to win in the longrun but how can I show this mathematically? I ran a computer simulation and it looks like my initial hypothesis is right.  There seems to be an equally likely chance for A or B to win on average.  Even with as few as $10$ trials ($10$ winning hands), I am seeing mostly $5$ wins for each but sometimes $4$ vs. $6$ but I didn't even see $3$ vs. $7$ yet so it seems to hit equilibrium VERY quickly.  Since the # of required trials (wins) is so low, you can probably just try this with a real deck of cards and confirm it is about $50/50$ with as few as $10$ winning hands.  Just remember to update the winning threshold for A properly (for example, $4, 5, 4, 3$...).  Unlike a fair coin toss where $8$, $9$, or even $10$ heads are possible, it seems almost impossible for that to happen with this type of ""self adjusting"" game. So how can it be shown mathematically that this type of game will reach equilibrium where either player has about the same chance to win overall? If you do a computer simulation of this, it is somewhat amazing how many times it will hit exactly half and half.  For example, if I run $1000$ decisions, I usually get $500$ A wins and $500$ B wins.  It is very consistent (yet I am using different random numbers each run).  This is WAY more predictable than something like fair coin flips which could get off to a ""rocky"" start.  I think you can call this type of game ""self adjusting"" in that it will reach a ""fair equilibrium"" very quickly. This seems like a hard problem to state mathematically so I put a bounty of $100$ points on it as an extra incentive to those of you who want to try to solve it.  If $2$ different users submit good answers then what I usually do is give one person the checkmark and the other the bounty to be more fair to both.  Good luck.",,"['probability', 'card-games']"
60,Prove the intersection of events to be 1,Prove the intersection of events to be 1,,"$B_n$, $n$ from $1$ to infinity is countably infinite sequence of events and each event has probability $1$. How do I formally prove that the probability of intersection of $B_n$ from $n = 1$ to infinity is also $1$. Intuitively I know because $P[B_i] = 1$, so all events are equivalent to the total sample space, and so does its intersection. But how to prove it in formal way?","$B_n$, $n$ from $1$ to infinity is countably infinite sequence of events and each event has probability $1$. How do I formally prove that the probability of intersection of $B_n$ from $n = 1$ to infinity is also $1$. Intuitively I know because $P[B_i] = 1$, so all events are equivalent to the total sample space, and so does its intersection. But how to prove it in formal way?",,"['probability', 'elementary-set-theory']"
61,Discrete and continuous Girsanov,Discrete and continuous Girsanov,,"I'm trying to write a proof of the Girsanov theorem based on a discrete version of it. Discrete version Suppose that I have a random vector $X$ and two equivalent probability measures $\mathbb{P}, \mathbb{Q}$. In $\mathbb{P}$, $X$ is an uncorrelated multivariate normal random variable. In $\mathbb{Q}$, $X$ is an uncorrelated multivariate normal random variable with mean $0$ (and the same variance). The goal is to find the change of measure $\frac{d\mathbb{Q}}{d\mathbb{P}} : \Omega \to \mathbb{R}$ to make this happen. So in symbols, we have $$ X_* \mathbb{P} \sim N(\mu, \mathrm{diag}(\sigma))$$ $$ X_* \mathbb{Q} \sim N(0, \mathrm{diag}(\sigma))$$ Writing $\lambda^n$ for the Lebesgue measure on $\mathbb{R}^n$ and writing out the densities we get: $$ \frac{d X_* \mathbb{Q}}{dX_* \mathbb{P}} (\mathbf{x}) =   \frac{d X_* \mathbb{Q}}{dX_* \lambda^n} (\mathbf{x}) \frac{d X_* \lambda^n}{dX_* \mathbb{P}} (\mathbf{x}) \\ =  \frac{\prod_{i=1}^n \frac{1}{\sqrt{2 \pi} \sigma_i} \exp\left\{-\frac{x_i^2}{2\sigma_i^2} \right\}} {\prod_{i=1}^n \frac{1}{\sqrt{2 \pi} \sigma_i} \exp\left\{-\frac{(x_i-\mu_i)^2}{2\sigma_i^2} \right\}} \\ = \exp\left\{ \sum_{i=1}^n \frac{-2\mu_ix_i + \mu_i^2}{2\sigma_i^2} \right\} \\ = \exp\left\{ - \sum_{i=1}^n \frac{\mu_ix_i}{\sigma_i^2} + \frac{1}{2} \sum_{i=1}^n\frac{\mu_i^2}{\sigma_i^2} \right\} $$ Now let $B \in \mathcal{B}^n$ a Borel set in $\mathbb{R}^n$. $$ \mathbb{Q} (X^{-1} (B)) = X_* \mathbb{Q} (B) \\ = \int_B \frac{dX_*\mathbb{Q}}{dX_*\mathbb{P}} dX_*\mathbb{P} \\ = \int_{X^{-1}(B)} \left( \frac{dX_*\mathbb{Q}}{dX_*\mathbb{P}} \circ X  \right) d\mathbb{P} $$ So it is sufficient to choose $\frac{d\mathbb{Q}}{d\mathbb{P}} = \frac{dX_*\mathbb{Q}}{dX_*\mathbb{P}} \circ X$. Therefore $$\frac{d\mathbb{Q}}{d\mathbb{P}} = \exp\left\{ - \sum_{i=1}^n \frac{\mu_iX_i}{\sigma_i^2} + \frac{1}{2} \sum_{i=1}^n\frac{\mu_i^2}{\sigma_i^2} \right\}$$. This result is what I would call the ""discrete Girsanov formula"". My question is whether it is possible to prove the continuous version as a limit of this one. Continuous version $X(t) = W(t) + \int_0^t \Theta(u) du$ where $W(t)$ is a $\mathbb{P}$ Brownian motion and $\Theta(u)$ is an adapted process. Assuming that $X(t)$ is a $\mathbb{Q}$ Brownian motion, $$ \frac{d\mathbb{Q}}{d\mathbb{P}} = \exp\left\{ \int_0^t \Theta(u) dW(u) + \frac{1}{2} \int_0^t \Theta(u)^2 du  \right\} \\ = \exp\left\{ -\int_0^t \Theta(u) dX(u) + \frac{1}{2} \int_0^t \Theta(u)^2 du  \right\}$$ (This statement is paraphrased from Shreve's Stochastic Calculus for Finance, and is probably missing the $L^2$ condition) It looks an awful lot like the discrete version! My question is: is it possible to pass from the discrete to the continuous version by a partitioning argument? It doesn't have to be super-rigorous (I don't even fully understand the construction of Brownian motion).","I'm trying to write a proof of the Girsanov theorem based on a discrete version of it. Discrete version Suppose that I have a random vector $X$ and two equivalent probability measures $\mathbb{P}, \mathbb{Q}$. In $\mathbb{P}$, $X$ is an uncorrelated multivariate normal random variable. In $\mathbb{Q}$, $X$ is an uncorrelated multivariate normal random variable with mean $0$ (and the same variance). The goal is to find the change of measure $\frac{d\mathbb{Q}}{d\mathbb{P}} : \Omega \to \mathbb{R}$ to make this happen. So in symbols, we have $$ X_* \mathbb{P} \sim N(\mu, \mathrm{diag}(\sigma))$$ $$ X_* \mathbb{Q} \sim N(0, \mathrm{diag}(\sigma))$$ Writing $\lambda^n$ for the Lebesgue measure on $\mathbb{R}^n$ and writing out the densities we get: $$ \frac{d X_* \mathbb{Q}}{dX_* \mathbb{P}} (\mathbf{x}) =   \frac{d X_* \mathbb{Q}}{dX_* \lambda^n} (\mathbf{x}) \frac{d X_* \lambda^n}{dX_* \mathbb{P}} (\mathbf{x}) \\ =  \frac{\prod_{i=1}^n \frac{1}{\sqrt{2 \pi} \sigma_i} \exp\left\{-\frac{x_i^2}{2\sigma_i^2} \right\}} {\prod_{i=1}^n \frac{1}{\sqrt{2 \pi} \sigma_i} \exp\left\{-\frac{(x_i-\mu_i)^2}{2\sigma_i^2} \right\}} \\ = \exp\left\{ \sum_{i=1}^n \frac{-2\mu_ix_i + \mu_i^2}{2\sigma_i^2} \right\} \\ = \exp\left\{ - \sum_{i=1}^n \frac{\mu_ix_i}{\sigma_i^2} + \frac{1}{2} \sum_{i=1}^n\frac{\mu_i^2}{\sigma_i^2} \right\} $$ Now let $B \in \mathcal{B}^n$ a Borel set in $\mathbb{R}^n$. $$ \mathbb{Q} (X^{-1} (B)) = X_* \mathbb{Q} (B) \\ = \int_B \frac{dX_*\mathbb{Q}}{dX_*\mathbb{P}} dX_*\mathbb{P} \\ = \int_{X^{-1}(B)} \left( \frac{dX_*\mathbb{Q}}{dX_*\mathbb{P}} \circ X  \right) d\mathbb{P} $$ So it is sufficient to choose $\frac{d\mathbb{Q}}{d\mathbb{P}} = \frac{dX_*\mathbb{Q}}{dX_*\mathbb{P}} \circ X$. Therefore $$\frac{d\mathbb{Q}}{d\mathbb{P}} = \exp\left\{ - \sum_{i=1}^n \frac{\mu_iX_i}{\sigma_i^2} + \frac{1}{2} \sum_{i=1}^n\frac{\mu_i^2}{\sigma_i^2} \right\}$$. This result is what I would call the ""discrete Girsanov formula"". My question is whether it is possible to prove the continuous version as a limit of this one. Continuous version $X(t) = W(t) + \int_0^t \Theta(u) du$ where $W(t)$ is a $\mathbb{P}$ Brownian motion and $\Theta(u)$ is an adapted process. Assuming that $X(t)$ is a $\mathbb{Q}$ Brownian motion, $$ \frac{d\mathbb{Q}}{d\mathbb{P}} = \exp\left\{ \int_0^t \Theta(u) dW(u) + \frac{1}{2} \int_0^t \Theta(u)^2 du  \right\} \\ = \exp\left\{ -\int_0^t \Theta(u) dX(u) + \frac{1}{2} \int_0^t \Theta(u)^2 du  \right\}$$ (This statement is paraphrased from Shreve's Stochastic Calculus for Finance, and is probably missing the $L^2$ condition) It looks an awful lot like the discrete version! My question is: is it possible to pass from the discrete to the continuous version by a partitioning argument? It doesn't have to be super-rigorous (I don't even fully understand the construction of Brownian motion).",,"['probability', 'stochastic-processes']"
62,"In how many different ways can $3$ red, $4$ yellow and $2$ blue bulbs be arranged in a row?","In how many different ways can  red,  yellow and  blue bulbs be arranged in a row?",3 4 2,"$1)$ How many different ways can $3$ red, $4$ yellow and $2$ blue bulbs be arranged in a row? Do I just say $3! 4! 2! = 288$ ? $2)$ On a shelf there are $4$ different math books and $8$ different English books. a. If the books are to be arranged so that the math books are together, how many ways can this be done? b. What is the probability that all the math books will be together? For part $a)$,  I put $8! 4! = 967680$ and part $b)$, $\dfrac{8!4!}{ 12!}$ I'm not too sure if i did these right and I would appreciate some help, thanks.","$1)$ How many different ways can $3$ red, $4$ yellow and $2$ blue bulbs be arranged in a row? Do I just say $3! 4! 2! = 288$ ? $2)$ On a shelf there are $4$ different math books and $8$ different English books. a. If the books are to be arranged so that the math books are together, how many ways can this be done? b. What is the probability that all the math books will be together? For part $a)$,  I put $8! 4! = 967680$ and part $b)$, $\dfrac{8!4!}{ 12!}$ I'm not too sure if i did these right and I would appreciate some help, thanks.",,"['probability', 'combinatorics', 'permutations']"
63,What is the intuition for permuting $n$ objects where $p$ are alike,What is the intuition for permuting  objects where  are alike,n p,"If we have $n$ objects in which $p$ are objects are alike and rest are all different, then the number of permutations is $\frac{n!}{p!}$. Is there some intuition on how this is correct? why do we have to divide by $p!$","If we have $n$ objects in which $p$ are objects are alike and rest are all different, then the number of permutations is $\frac{n!}{p!}$. Is there some intuition on how this is correct? why do we have to divide by $p!$",,"['probability', 'permutations']"
64,Probability distributions parameterized by the median / mode / mean absolute deviation?,Probability distributions parameterized by the median / mode / mean absolute deviation?,,"Some probability density functions' parameters include the mean and the variance (like the normal distribution). Are there examples of probability distributions that are parameterized by other measures of central tendency (median, mode) and dispersion (mean absolute deviation)?","Some probability density functions' parameters include the mean and the variance (like the normal distribution). Are there examples of probability distributions that are parameterized by other measures of central tendency (median, mode) and dispersion (mean absolute deviation)?",,"['probability', 'probability-distributions']"
65,Finding the inverse of the binomial cumulative distribution function,Finding the inverse of the binomial cumulative distribution function,,"I am trying to find a mathematical solution to the inverse of the binomial cumulative distrbution function, essentially mathematically representing the Excel function BINOM.INV . Given a number of trials $n$, the probability of success $p$, and the cumulative area of the binomial distribution $\alpha$, I need to find a value $x$ such that $$ \alpha = \sum_{k\ =\ 0}^x\binom{n}{k}\ p^k\;\left(1-p\right)^{\ n-k} $$ So that if $$ \begin{align} \alpha &= 0.7 \\ p &= 0.3 \\ n &= 100 \\ \end{align} $$ $x$ would be equal to $32$, replicating the following Excel formula: 32 = BINOM.INV(100, 0.3, 0.7) How could I achieve such a value mathematically, not using Excel?","I am trying to find a mathematical solution to the inverse of the binomial cumulative distrbution function, essentially mathematically representing the Excel function BINOM.INV . Given a number of trials $n$, the probability of success $p$, and the cumulative area of the binomial distribution $\alpha$, I need to find a value $x$ such that $$ \alpha = \sum_{k\ =\ 0}^x\binom{n}{k}\ p^k\;\left(1-p\right)^{\ n-k} $$ So that if $$ \begin{align} \alpha &= 0.7 \\ p &= 0.3 \\ n &= 100 \\ \end{align} $$ $x$ would be equal to $32$, replicating the following Excel formula: 32 = BINOM.INV(100, 0.3, 0.7) How could I achieve such a value mathematically, not using Excel?",,"['probability', 'probability-theory', 'binomial-theorem']"
66,"Suppose $\xi_1, \xi_2,\ldots$ are i.i.d. random variables with mean $\mu$, variance $\sigma^2$. Form the random sum $S_{N} = \xi_{1}+\cdots+\xi_{N}$.","Suppose  are i.i.d. random variables with mean , variance . Form the random sum .","\xi_1, \xi_2,\ldots \mu \sigma^2 S_{N} = \xi_{1}+\cdots+\xi_{N}","(a) Derive the mean and variance of $S_{N}$ when $N$ has Poisson distribution with parameter $\lambda$. So far, for the mean, I have the following: $E[S_{N}] = E[E[S_{N}\mid N=n]]$ $$ = \sum_{n=1}^{\infty} E[\xi_{1}+\cdots+\xi_{N}\mid N=n] p_{N}(n)$$ $$ = \sum_{n=1}^{\infty} E[\xi_{1}+\cdots+\xi_{n}\mid N=n] p_{N}(n)$$ $$ = \sum_{n=1}^{\infty} E[\xi_{1}+\cdots+\xi_{n}] p_{N}(n)$$ $$ = \sum_{n=1}^{\infty} (E[\xi_{1}]+\cdots+E[\xi_{n}]) p_{N}(n)$$ $$ = \sum_{n=1}^{\infty} n\mu p_{N}(n)$$ $$ = \mu \sum_{n=1}^{\infty} n p_{N}(n)$$ $ = \mu \lambda$. Are my steps legal? Also, for the variance I have the following: $\operatorname{var}(S_{N}) = \operatorname{var}(E[S_{N}\mid N=n]) + E[\operatorname{var}(S_{N}\mid N=n)]$ $$ = \operatorname{var}(E[\xi_{1}+\cdots+\xi_{n}\mid N=n]) + E[\operatorname{var}(\xi_{1}+\cdots+\xi_{n}\mid N=n)]$$ $$ = ? + \sum_{n=1}^{\infty} \operatorname{var}(\xi_{1}+\cdots+\xi_{n}) p_{N}(n)$$ $$ = ? + \sum_{n=1}^{\infty} \operatorname{var}(\xi_{1}+\cdots+\xi_{n}) p_{N}(n)$$ $$ = ? + \sigma^2 \sum_{n=1}^{\infty} n p_{N}(n)$$ $ = ? + \sigma^2 \lambda$. Obviously, I am stuck and do not know how to handle the left side of the sum. Can somebody please explain in detail the next steps. Thank you.","(a) Derive the mean and variance of $S_{N}$ when $N$ has Poisson distribution with parameter $\lambda$. So far, for the mean, I have the following: $E[S_{N}] = E[E[S_{N}\mid N=n]]$ $$ = \sum_{n=1}^{\infty} E[\xi_{1}+\cdots+\xi_{N}\mid N=n] p_{N}(n)$$ $$ = \sum_{n=1}^{\infty} E[\xi_{1}+\cdots+\xi_{n}\mid N=n] p_{N}(n)$$ $$ = \sum_{n=1}^{\infty} E[\xi_{1}+\cdots+\xi_{n}] p_{N}(n)$$ $$ = \sum_{n=1}^{\infty} (E[\xi_{1}]+\cdots+E[\xi_{n}]) p_{N}(n)$$ $$ = \sum_{n=1}^{\infty} n\mu p_{N}(n)$$ $$ = \mu \sum_{n=1}^{\infty} n p_{N}(n)$$ $ = \mu \lambda$. Are my steps legal? Also, for the variance I have the following: $\operatorname{var}(S_{N}) = \operatorname{var}(E[S_{N}\mid N=n]) + E[\operatorname{var}(S_{N}\mid N=n)]$ $$ = \operatorname{var}(E[\xi_{1}+\cdots+\xi_{n}\mid N=n]) + E[\operatorname{var}(\xi_{1}+\cdots+\xi_{n}\mid N=n)]$$ $$ = ? + \sum_{n=1}^{\infty} \operatorname{var}(\xi_{1}+\cdots+\xi_{n}) p_{N}(n)$$ $$ = ? + \sum_{n=1}^{\infty} \operatorname{var}(\xi_{1}+\cdots+\xi_{n}) p_{N}(n)$$ $$ = ? + \sigma^2 \sum_{n=1}^{\infty} n p_{N}(n)$$ $ = ? + \sigma^2 \lambda$. Obviously, I am stuck and do not know how to handle the left side of the sum. Can somebody please explain in detail the next steps. Thank you.",,"['probability', 'stochastic-processes', 'conditional-expectation']"
67,ARD Kernel - explanation,ARD Kernel - explanation,,"The following text discusses that the ARD kernel is a regular gaussian kernel but one where $\Sigma$ is diagnonal and one where the $\sigma$'s go to infinity. It seems that the $\kappa$(x,x') would simply be 1 in that case? why would this be a useful kernel?","The following text discusses that the ARD kernel is a regular gaussian kernel but one where $\Sigma$ is diagnonal and one where the $\sigma$'s go to infinity. It seems that the $\kappa$(x,x') would simply be 1 in that case? why would this be a useful kernel?",,"['probability', 'exponential-function', 'machine-learning']"
68,Independence of Poisson random variables coming from Poisson sampling,Independence of Poisson random variables coming from Poisson sampling,,"Context: Let $x \in \mathbb{R}^n$ be the unknown probability vector of a finite discrete distribution $X$. We are able to sample $X$ and we want to learn $x$. Poissonization: Each observation belongs to the $i^\text{th}$ category with probability $x_i$, thus for a sample of size $m \in \mathbb{N}$, the sum of the $i^\text{th}$ category follows a binomial distribution $B(x_i\ ,\ m)$. These binomial random variables are not independent since their sum is $m$ ($X$ is a distribution). However, I found a trick online : if you sum over a $M \sim \mathrm{Poisson}(m)$ sample size rather than $m$, the sum $M_i$ of the $i^\text{th}$ category is no longer binomial but follows $\mathrm{Poisson}(m \times x_i)$ law. Furthermore, these $n$ Poisson random variables are independent! I try to prove this. My approach I proved that $M_i \sim \mathrm{Poisson}(m \times x_i)$ for any $i$ : $$ \sum_{j=0}^\infty \left(\ P[\mathrm{Poisson}(m)=j] \times P[B(p,j)=k]\ \right) = P[\mathrm{Poisson}(p\times m)=k]$$ $\iff$ $$ \sum_{j=k}^{\infty} \left(\ P[\mathrm{Poisson}(m)=j] \times P[B(p,j)=k]\ \right) = P[\mathrm{Poisson}(p\times m)=k]$$ $\iff$ $$ \sum_{j=k}^{\infty} \left(\frac{e^{-m} m^j}{j!} \times \frac{j!\ p^k (1-p)^{j-k}}{k!\ (j-k)!} \right) = \frac{e^{-pm} (p m)^k}{k!}$$ $\iff$ $$e^{-m} \sum_{j=k}^\infty \left(\frac{m^j (1-p)^{j-k}}{(j-k)!} \right) = e^{-pm} m^k$$ $\iff$ $$e^{-m} \sum_{j'=0}^\infty \left(\frac{m^{j'+k} (1-p)^{j'}}{j'!} \right) = e^{-pm} m^k$$ $\iff$ $$e^{-m} e^{m(1-p)} m^k = e^{-pm}m^k $$ $\square$ Now, how can I prove that all those $n$ random variables are independent ? I found this paper which states that even if $X_1$, $X_2$ and $X_1 + X_2$ are $\mathrm{Poisson}$, $X_1$ and $X_2$ don't have to be independent. But here we are not in the general case since we have $\sum\limits_{i=1}^n x_i = 1$.","Context: Let $x \in \mathbb{R}^n$ be the unknown probability vector of a finite discrete distribution $X$. We are able to sample $X$ and we want to learn $x$. Poissonization: Each observation belongs to the $i^\text{th}$ category with probability $x_i$, thus for a sample of size $m \in \mathbb{N}$, the sum of the $i^\text{th}$ category follows a binomial distribution $B(x_i\ ,\ m)$. These binomial random variables are not independent since their sum is $m$ ($X$ is a distribution). However, I found a trick online : if you sum over a $M \sim \mathrm{Poisson}(m)$ sample size rather than $m$, the sum $M_i$ of the $i^\text{th}$ category is no longer binomial but follows $\mathrm{Poisson}(m \times x_i)$ law. Furthermore, these $n$ Poisson random variables are independent! I try to prove this. My approach I proved that $M_i \sim \mathrm{Poisson}(m \times x_i)$ for any $i$ : $$ \sum_{j=0}^\infty \left(\ P[\mathrm{Poisson}(m)=j] \times P[B(p,j)=k]\ \right) = P[\mathrm{Poisson}(p\times m)=k]$$ $\iff$ $$ \sum_{j=k}^{\infty} \left(\ P[\mathrm{Poisson}(m)=j] \times P[B(p,j)=k]\ \right) = P[\mathrm{Poisson}(p\times m)=k]$$ $\iff$ $$ \sum_{j=k}^{\infty} \left(\frac{e^{-m} m^j}{j!} \times \frac{j!\ p^k (1-p)^{j-k}}{k!\ (j-k)!} \right) = \frac{e^{-pm} (p m)^k}{k!}$$ $\iff$ $$e^{-m} \sum_{j=k}^\infty \left(\frac{m^j (1-p)^{j-k}}{(j-k)!} \right) = e^{-pm} m^k$$ $\iff$ $$e^{-m} \sum_{j'=0}^\infty \left(\frac{m^{j'+k} (1-p)^{j'}}{j'!} \right) = e^{-pm} m^k$$ $\iff$ $$e^{-m} e^{m(1-p)} m^k = e^{-pm}m^k $$ $\square$ Now, how can I prove that all those $n$ random variables are independent ? I found this paper which states that even if $X_1$, $X_2$ and $X_1 + X_2$ are $\mathrm{Poisson}$, $X_1$ and $X_2$ don't have to be independent. But here we are not in the general case since we have $\sum\limits_{i=1}^n x_i = 1$.",,"['probability', 'sampling', 'poisson-distribution', 'binomial-distribution']"
69,"German Tank Problem, Confidence Level","German Tank Problem, Confidence Level",,"Suppose you're in a city with n cabs. Each cab has a distinctive number from 0 to n. You take a cab 10 times, the choice of the cabs is independent and equiprobable. The cab with the biggest number has number 100. Determine a preferably small upper limit for n with a confidence level of $\frac{9}{10}$. To me, this sounds like the German Tank Problem. So, depending on the approach I get different estimates for n. But I don't know how to include the confidence level. Do you have any ideas?","Suppose you're in a city with n cabs. Each cab has a distinctive number from 0 to n. You take a cab 10 times, the choice of the cabs is independent and equiprobable. The cab with the biggest number has number 100. Determine a preferably small upper limit for n with a confidence level of $\frac{9}{10}$. To me, this sounds like the German Tank Problem. So, depending on the approach I get different estimates for n. But I don't know how to include the confidence level. Do you have any ideas?",,"['probability', 'statistics']"
70,Is my answer correct? Expected number of coin flips to get 5 consecutive heads,Is my answer correct? Expected number of coin flips to get 5 consecutive heads,,"I found this puzzle online. Before my question gets flagged or on hold, I know the question has already been answered here but since I tried to solve it on my own I want to check if it's correct or not. The puzzle is ""What is the expected Number of Coin Tosses to Get Five Consecutive Heads"". My answer is the following: Because a coin has two sides, one for tails and one for heads, let $p$ be the probability of getting heads. Then the probability of getting tails is $1-p$ . For obvious reasons I shall assume that the coin is fair, thus the probability of getting heads is $\dfrac{1}{2}$ and tails $\dfrac{1}{2}$ as well. Let $X_i$ be a random variable indicating the number of flips required to get heads, when $i-1$ was heads. What we are looking for is the number of tries for a single success, thus we shall assume the random variable $X_i$ is following the Geometric Distribution with Probability $P(X_i = x) = p (1-p)^{x-1}$ . The expected number of coin flips to get 5 consecutive heads is given by the expected average $E(X) = \dfrac{1}{p}$ . The probability of the random variable $X_i$ is $\dfrac{1}{2^{i}}$ since the coin is fair. Thus for $5$ consecutive heads $E(X) = E(X_1) + ... + E(X_5) = \sum_{i=1}^{5}E(X_i) = \sum_{i=1}^{5}2^i = 62$ So the expected number of coin flips to get $5$ consecutive heads is $62$ .","I found this puzzle online. Before my question gets flagged or on hold, I know the question has already been answered here but since I tried to solve it on my own I want to check if it's correct or not. The puzzle is ""What is the expected Number of Coin Tosses to Get Five Consecutive Heads"". My answer is the following: Because a coin has two sides, one for tails and one for heads, let be the probability of getting heads. Then the probability of getting tails is . For obvious reasons I shall assume that the coin is fair, thus the probability of getting heads is and tails as well. Let be a random variable indicating the number of flips required to get heads, when was heads. What we are looking for is the number of tries for a single success, thus we shall assume the random variable is following the Geometric Distribution with Probability . The expected number of coin flips to get 5 consecutive heads is given by the expected average . The probability of the random variable is since the coin is fair. Thus for consecutive heads So the expected number of coin flips to get consecutive heads is .",p 1-p \dfrac{1}{2} \dfrac{1}{2} X_i i-1 X_i P(X_i = x) = p (1-p)^{x-1} E(X) = \dfrac{1}{p} X_i \dfrac{1}{2^{i}} 5 E(X) = E(X_1) + ... + E(X_5) = \sum_{i=1}^{5}E(X_i) = \sum_{i=1}^{5}2^i = 62 5 62,"['probability', 'statistics', 'puzzle']"
71,off by 1 lottery probability,off by 1 lottery probability,,"A lottery, where 6 balls out of 50 are drawn randomly without replacement, allows players of the lottery to be off by at most 1 for each number on their lottery ticket.  Additional rules of the lottery are as follows: The balls are numbered 1 thru 50 and are all equally likely to be drawn. Both the winning numbers drawn and the players tickets contain random numbers (from 1 to 50).  That is, the player does not pick/choose their own numbers, it is done randomly for them via computer. Both the winning numbers and the numbers on lottery tickets are sorted in ascending order and winning tickets must match/map in that order.  For example, if the winning numbers drawn are (sorted) 5, 10, 15, 16, 20, and 30 and the ticketholder has 5, 10, 16, 17, 20, and 30 (also sorted), the 16s will not map together.  The matching numbers are (5:5), (10:10), (15:16), (16:17), (20:20), and (30:30).  That is a winning ticket. So the question is how much more likely is a single lottery ticket expected to be a winner using the off by 1 rule verses a similar but much stricter lottery (also 6 out of 50 balls) but which disallows the off by 1 rule (numbers must match exactly)? Work done so far: The strict lottery chances of winning on a single ticket are 1 in about 15.9 million which is 1 / (50 choose 6). The off by 1 lottery was simulated on computer and I am seeing a 503 increase in probability (about 1 in 31,600 chance of winning). I would like to know if this type of problem can be done mathematically or if it is too difficult.  Also, I was hoping someone could do a simulation also to help verify my finding of 503x increase in chances of winning. Additional info that may be helpful for analysis...  The worst case boost in expected win probability is 7x if (for example), the drawn balls are 1,2,3,4,5,6.  This would match the following tickets: (1,2,3,4,5,6), (1,2,3,4,5,7), (1,2,3,4,6,7), (1,2,3,5,6,7), (1,2,4,5,6,7), (1,3,4,5,6,7), and (2,3,4,5,6,7).  Best case boost is 729x where the drawn balls are something like 5, 10, 15, 20, 23, 30. The 3 most frequent boost scenarios (from simulation results) are 486x, 648x, and 729x.  These are 3 out of 137 scenarios I am seeing in simulation of 100,000,000 decisions. I am not yet sure if there more. If something is unclear, please ask before attempting to answer the question and I will clarify.","A lottery, where 6 balls out of 50 are drawn randomly without replacement, allows players of the lottery to be off by at most 1 for each number on their lottery ticket.  Additional rules of the lottery are as follows: The balls are numbered 1 thru 50 and are all equally likely to be drawn. Both the winning numbers drawn and the players tickets contain random numbers (from 1 to 50).  That is, the player does not pick/choose their own numbers, it is done randomly for them via computer. Both the winning numbers and the numbers on lottery tickets are sorted in ascending order and winning tickets must match/map in that order.  For example, if the winning numbers drawn are (sorted) 5, 10, 15, 16, 20, and 30 and the ticketholder has 5, 10, 16, 17, 20, and 30 (also sorted), the 16s will not map together.  The matching numbers are (5:5), (10:10), (15:16), (16:17), (20:20), and (30:30).  That is a winning ticket. So the question is how much more likely is a single lottery ticket expected to be a winner using the off by 1 rule verses a similar but much stricter lottery (also 6 out of 50 balls) but which disallows the off by 1 rule (numbers must match exactly)? Work done so far: The strict lottery chances of winning on a single ticket are 1 in about 15.9 million which is 1 / (50 choose 6). The off by 1 lottery was simulated on computer and I am seeing a 503 increase in probability (about 1 in 31,600 chance of winning). I would like to know if this type of problem can be done mathematically or if it is too difficult.  Also, I was hoping someone could do a simulation also to help verify my finding of 503x increase in chances of winning. Additional info that may be helpful for analysis...  The worst case boost in expected win probability is 7x if (for example), the drawn balls are 1,2,3,4,5,6.  This would match the following tickets: (1,2,3,4,5,6), (1,2,3,4,5,7), (1,2,3,4,6,7), (1,2,3,5,6,7), (1,2,4,5,6,7), (1,3,4,5,6,7), and (2,3,4,5,6,7).  Best case boost is 729x where the drawn balls are something like 5, 10, 15, 20, 23, 30. The 3 most frequent boost scenarios (from simulation results) are 486x, 648x, and 729x.  These are 3 out of 137 scenarios I am seeing in simulation of 100,000,000 decisions. I am not yet sure if there more. If something is unclear, please ask before attempting to answer the question and I will clarify.",,['probability']
72,Rooks Attacking Every Square on a Chess Board [duplicate],Rooks Attacking Every Square on a Chess Board [duplicate],,"This question already has answers here : In how many different ways can we place $8$ identical rooks on a chess board so that no two of them attack each other? (4 answers) Rooks on a 8 by 8 checker board. Probability problem (3 answers) Closed 4 years ago . 8 rooks are randomly placed on different squares of a chessboard. A rook is said to attack all of the squares in its row and its column.  Compute the probability that every square is occupied or attacked by at least 1 rook. The first step I took was to state that there are $64C8$ ways to decide how to place the 8 rooks on the chessboard. Next, I tried to experiment with a physical chessboard to see how this could be done. The only way I found that every square on the board can be attacked is if one rook is in either every horizontal row or every vertical row. Therefore, there are $2 * 8^8 - 2$ ways to place the rooks. To clarify, it is ""-2"" because the diagonals are counted twice. Is there a case that I overlooked, or did I solve the problem correctly? Thanks, You Know Me......","This question already has answers here : In how many different ways can we place $8$ identical rooks on a chess board so that no two of them attack each other? (4 answers) Rooks on a 8 by 8 checker board. Probability problem (3 answers) Closed 4 years ago . 8 rooks are randomly placed on different squares of a chessboard. A rook is said to attack all of the squares in its row and its column.  Compute the probability that every square is occupied or attacked by at least 1 rook. The first step I took was to state that there are $64C8$ ways to decide how to place the 8 rooks on the chessboard. Next, I tried to experiment with a physical chessboard to see how this could be done. The only way I found that every square on the board can be attacked is if one rook is in either every horizontal row or every vertical row. Therefore, there are $2 * 8^8 - 2$ ways to place the rooks. To clarify, it is ""-2"" because the diagonals are counted twice. Is there a case that I overlooked, or did I solve the problem correctly? Thanks, You Know Me......",,"['probability', 'combinatorics']"
73,"$X_1, \dots, X_n$ are independent random variables. Suppose $M = \min(X_1, X_2, \dots, X_n)$",are independent random variables. Suppose,"X_1, \dots, X_n M = \min(X_1, X_2, \dots, X_n)","Given that $X_1,\dots, X_n$ are independent random variables. Suppose $M = \min(X_1, X_2,\dots, X_n)$ and $X_i$ are exponential random variables with parameter $λ_i$, compute $E[M  X_j | M = X_i]$ where $i \ne j$. I have got the pdf and pmf of $M$. But I still don't know how to solve. Anyone can help?","Given that $X_1,\dots, X_n$ are independent random variables. Suppose $M = \min(X_1, X_2,\dots, X_n)$ and $X_i$ are exponential random variables with parameter $λ_i$, compute $E[M  X_j | M = X_i]$ where $i \ne j$. I have got the pdf and pmf of $M$. But I still don't know how to solve. Anyone can help?",,['probability']
74,When does the variance of a consistent estimator go to zero?,When does the variance of a consistent estimator go to zero?,,"I came across the following statement (marked as true ) in multiple-choice section of an old exam: The variance of a consistent estimator goes to zero with the growing sample size. As far as I can tell, it can be translated as Convergence in probability to a constant implies convergence in $L^2$. Which is clearly false. Is there a way to repair the statement? I mean maybe the professor forgot to mention some additional assumption typical for the context. (E.g. how convergence in probability and uniform integrability together imply $L^1$ convergence, but it seems to be irrelevant for the above statement.)","I came across the following statement (marked as true ) in multiple-choice section of an old exam: The variance of a consistent estimator goes to zero with the growing sample size. As far as I can tell, it can be translated as Convergence in probability to a constant implies convergence in $L^2$. Which is clearly false. Is there a way to repair the statement? I mean maybe the professor forgot to mention some additional assumption typical for the context. (E.g. how convergence in probability and uniform integrability together imply $L^1$ convergence, but it seems to be irrelevant for the above statement.)",,"['probability', 'statistics', 'probability-theory', 'convergence-divergence', 'statistical-inference']"
75,Does this sequence converge almost surely or not?,Does this sequence converge almost surely or not?,,"I have a sequence of independent random variables $X_1, X_2,...$ such that $P(X_n = 1) = \frac{1}{n}$ and $P(X_n = 0) = 1 - \frac{1}{n}$. Using the second Borel-Cantelli Lemma, we have $\sum P(X_n \neq 0) = \sum P(X_n = 1) = \sum \frac{1}{n} = \infty$ implies that $X_n$ does not converge to 0 a.s. But I feel like we can construct an example where almost convergence occurs. Let $\Omega = (0, 1)$, and let the probability measure be the Lebesgue measure restricted to $(0, 1)$. Finally, let $X_n = \mathbb{I}\{(0, \frac{1}{n})\}$. Then for any $\omega \in \Omega$, $\underset{n \to \infty}{\lim} X_n(\omega) = 0$, so $X_n$ converges to 0 a.s. I don't understand why I'm getting contradicting conclusions. Any help would be much appreciated. Thank you!","I have a sequence of independent random variables $X_1, X_2,...$ such that $P(X_n = 1) = \frac{1}{n}$ and $P(X_n = 0) = 1 - \frac{1}{n}$. Using the second Borel-Cantelli Lemma, we have $\sum P(X_n \neq 0) = \sum P(X_n = 1) = \sum \frac{1}{n} = \infty$ implies that $X_n$ does not converge to 0 a.s. But I feel like we can construct an example where almost convergence occurs. Let $\Omega = (0, 1)$, and let the probability measure be the Lebesgue measure restricted to $(0, 1)$. Finally, let $X_n = \mathbb{I}\{(0, \frac{1}{n})\}$. Then for any $\omega \in \Omega$, $\underset{n \to \infty}{\lim} X_n(\omega) = 0$, so $X_n$ converges to 0 a.s. I don't understand why I'm getting contradicting conclusions. Any help would be much appreciated. Thank you!",,"['probability', 'probability-theory', 'convergence-divergence']"
76,Choosing exactly 2 damaged pieces (Probability),Choosing exactly 2 damaged pieces (Probability),,"From $27$ pieces of luggage, an airline handler damages a random sample of $4$. The probability that exactly one of the damaged pieces of luggage is insured is twice the probability that none of the damaged pieces are insured. Calculate the probability that exactly two of the four damaged pieces are insured. Hey guys! I tried solving this problem by assuming that there were $18$ insured pieces of luggage and 9 not insured pieces of luggage because of ""The probability that exactly one of the damaged pieces of luggage is insured is twice the probability that none of the damaged pieces are insured"". Then I did $\frac{\binom{18}{2} \times \binom{9}{2}}{\binom{27}{4}}$. It didn't end up with the right answer. The right answer is $.27$ (rounded). Can you guys please explain to me what I did wrong or if I made a wrong assumption? Thank you so much!","From $27$ pieces of luggage, an airline handler damages a random sample of $4$. The probability that exactly one of the damaged pieces of luggage is insured is twice the probability that none of the damaged pieces are insured. Calculate the probability that exactly two of the four damaged pieces are insured. Hey guys! I tried solving this problem by assuming that there were $18$ insured pieces of luggage and 9 not insured pieces of luggage because of ""The probability that exactly one of the damaged pieces of luggage is insured is twice the probability that none of the damaged pieces are insured"". Then I did $\frac{\binom{18}{2} \times \binom{9}{2}}{\binom{27}{4}}$. It didn't end up with the right answer. The right answer is $.27$ (rounded). Can you guys please explain to me what I did wrong or if I made a wrong assumption? Thank you so much!",,['probability']
77,What is the expected value of the number of anchors of $S$?,What is the expected value of the number of anchors of ?,S,"For any subset $S\subseteq\{1,2,\ldots,15\}$, call a number $n$ an anchor for $S$ if $n$ and $n+ |S|$ are both elements of $S$. For example, $4$ is an anchor of the set $S=\{4,7,14\}$, since $4\in S$ and $4+ |S| = 4+3 = 7\in S$. Given that $S$ is randomly chosen from all $2^{15}$ subsets of $\{1,2,\ldots,15\}$ (with each subset being equally likely), what is the expected value of the number of anchors of $S$?","For any subset $S\subseteq\{1,2,\ldots,15\}$, call a number $n$ an anchor for $S$ if $n$ and $n+ |S|$ are both elements of $S$. For example, $4$ is an anchor of the set $S=\{4,7,14\}$, since $4\in S$ and $4+ |S| = 4+3 = 7\in S$. Given that $S$ is randomly chosen from all $2^{15}$ subsets of $\{1,2,\ldots,15\}$ (with each subset being equally likely), what is the expected value of the number of anchors of $S$?",,"['probability', 'expectation']"
78,Expectation of $x^4$ [closed],Expectation of  [closed],x^4,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Can anyone help me prove that Expected Value of $X^4$ is $3\,($Var$(X))^4$, if the Expected Value of $X$ is zero and Var$(X)$ is the Variance of $X$ $(N(0,\sigma^2))$.","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Can anyone help me prove that Expected Value of $X^4$ is $3\,($Var$(X))^4$, if the Expected Value of $X$ is zero and Var$(X)$ is the Variance of $X$ $(N(0,\sigma^2))$.",,"['probability', 'probability-theory', 'probability-distributions', 'expectation']"
79,Expected value of 2 Poisson distributions,Expected value of 2 Poisson distributions,,"Let $X$ and $Y$ be independet Poisson random variables with parameters $\lambda$ and $\mu$. I have to calculate $E((X+Y)^2)$ . What I did: $E[(X+Y)^2]=E[X^2]+E[Y^2]+2EXEY$ I know that $2EXEY=2\lambda\mu$, but I don't know how to calculate the squared expected values. Thanks in advance!","Let $X$ and $Y$ be independet Poisson random variables with parameters $\lambda$ and $\mu$. I have to calculate $E((X+Y)^2)$ . What I did: $E[(X+Y)^2]=E[X^2]+E[Y^2]+2EXEY$ I know that $2EXEY=2\lambda\mu$, but I don't know how to calculate the squared expected values. Thanks in advance!",,"['probability', 'probability-distributions', 'expectation']"
80,How to find a confidence interval for a Maximum Likelihood Estimate,How to find a confidence interval for a Maximum Likelihood Estimate,,"My cousin is at elementary school and every week is given a book by his teacher. He then reads it and returns it in time to get another one the next week. After a while we started noticing that he was getting books he had read before and this became gradually more common over time. Naturally, I started to wonder how one could estimate the total number of books in their library. Say the true number of books in the library is $N$ and the teacher picks one uniformly at random (with replacement) to give to you each week. If at week $t$ you have received a book you have read before on $x$ occasions, then I can produce a maximum likelihood estimate for the number of books in the library following How many books are in a library? . Clarification. If the books he receives are named $A,B,C,B, A, D$ then $x$ will be $0,0,0,1,2,2$ at successive weeks. However, is there a mathematical formula as a function of $t$ and $x$ which will give me a 95% confidence interval for this estimate?","My cousin is at elementary school and every week is given a book by his teacher. He then reads it and returns it in time to get another one the next week. After a while we started noticing that he was getting books he had read before and this became gradually more common over time. Naturally, I started to wonder how one could estimate the total number of books in their library. Say the true number of books in the library is $N$ and the teacher picks one uniformly at random (with replacement) to give to you each week. If at week $t$ you have received a book you have read before on $x$ occasions, then I can produce a maximum likelihood estimate for the number of books in the library following How many books are in a library? . Clarification. If the books he receives are named $A,B,C,B, A, D$ then $x$ will be $0,0,0,1,2,2$ at successive weeks. However, is there a mathematical formula as a function of $t$ and $x$ which will give me a 95% confidence interval for this estimate?",,['probability']
81,Asymptotics of probability of Newton-Pepys problem,Asymptotics of probability of Newton-Pepys problem,,"In Newton-Pepys problem one is interested in probability $p_n$ of getting at least $n$ sixes in $6 n$ independent throws of regular 6-sided die. The number of sixes $S_m$ obtained in $m$ throws follows a Binomial distribution $\operatorname{Bin}(m,p)$, where $p=\frac{1}{6}$ is the probability of getting a six in a single throw. Thus $$      p_n = \Pr\left(S_{6n} \geqslant n\right) $$ Notice that $\mathsf{E}(S_{6n}) = 6 n p = n$, and variance $\mathbb{Var}(S_{6n}) = \sqrt{5 n/6}$, hence in the large $n$ limit $$     \lim_{n \to \infty} p_n = \lim_{n \to \infty} \Pr\left(\frac{S_{6n}-n}{\sqrt{5 n/6} }\geqslant 0\right) \stackrel{\text{CLT}}{=} \Pr(Z \geqslant 0) = \frac{1}{2} $$ In fact $p_n$ is monotonically decreasing sequence: Q. : How can one find the large $n$ asymptotics of $p_n$?","In Newton-Pepys problem one is interested in probability $p_n$ of getting at least $n$ sixes in $6 n$ independent throws of regular 6-sided die. The number of sixes $S_m$ obtained in $m$ throws follows a Binomial distribution $\operatorname{Bin}(m,p)$, where $p=\frac{1}{6}$ is the probability of getting a six in a single throw. Thus $$      p_n = \Pr\left(S_{6n} \geqslant n\right) $$ Notice that $\mathsf{E}(S_{6n}) = 6 n p = n$, and variance $\mathbb{Var}(S_{6n}) = \sqrt{5 n/6}$, hence in the large $n$ limit $$     \lim_{n \to \infty} p_n = \lim_{n \to \infty} \Pr\left(\frac{S_{6n}-n}{\sqrt{5 n/6} }\geqslant 0\right) \stackrel{\text{CLT}}{=} \Pr(Z \geqslant 0) = \frac{1}{2} $$ In fact $p_n$ is monotonically decreasing sequence: Q. : How can one find the large $n$ asymptotics of $p_n$?",,"['probability', 'dice']"
82,What is difference between stochastic process and a sequence of random variables?,What is difference between stochastic process and a sequence of random variables?,,"Suppose that I have a sequence of random variables $X=\langle X_1,...,X_n\rangle$ and I do not have any assumption about these continuous random variables, $X_i$'s, (they can be dependent/independent, identically distributed or not). Again suppose that $Y=\langle Y_1,...,Y_n\rangle$ is a stochastic process. Is there any difference between X and Y? can I say that X is also a stochastic process? I would be grateful if you could help me.","Suppose that I have a sequence of random variables $X=\langle X_1,...,X_n\rangle$ and I do not have any assumption about these continuous random variables, $X_i$'s, (they can be dependent/independent, identically distributed or not). Again suppose that $Y=\langle Y_1,...,Y_n\rangle$ is a stochastic process. Is there any difference between X and Y? can I say that X is also a stochastic process? I would be grateful if you could help me.",,"['probability', 'probability-theory', 'stochastic-processes', 'random-variables']"
83,Infinite Probability,Infinite Probability,,"A has 2 dollars and B has 3 dollars. They toss a coin. If it is heads A gives 1 dollar to B, B gives 1 dollar to A otherwise. P(Heads) = 1/3 What is the probability of B's winning? I tried too much but I could not solve this. I found P(B) = {HH, HTTT, THTT, ...} but I couldnt find a pattern for this. Thanks in advance.","A has 2 dollars and B has 3 dollars. They toss a coin. If it is heads A gives 1 dollar to B, B gives 1 dollar to A otherwise. P(Heads) = 1/3 What is the probability of B's winning? I tried too much but I could not solve this. I found P(B) = {HH, HTTT, THTT, ...} but I couldnt find a pattern for this. Thanks in advance.",,['probability']
84,Background for studying and understanding Stochastic differential equations,Background for studying and understanding Stochastic differential equations,,"Assume I have back ground of the following knowledge based on the textbook as : ODE : ODE by Tenenbaum Baby probability : Ross 's baby probability Baby real anlysis : Bartle's introduction to real analysis (1st undergrad course in advanced cal) Baby measure theory : Bartle 's element of Lebesgue measure and integration 1 course of undergrad linear algebra, and cal 1-3 what specific areas of math would I need to learn more in order to understand the SDE book of Oksendal : Stochastic Differential Equations: An Introduction with Applications . Many thanks for the suggestion about my background. I will take the 1st graduate course of SDE in the Spring","Assume I have back ground of the following knowledge based on the textbook as : ODE : ODE by Tenenbaum Baby probability : Ross 's baby probability Baby real anlysis : Bartle's introduction to real analysis (1st undergrad course in advanced cal) Baby measure theory : Bartle 's element of Lebesgue measure and integration 1 course of undergrad linear algebra, and cal 1-3 what specific areas of math would I need to learn more in order to understand the SDE book of Oksendal : Stochastic Differential Equations: An Introduction with Applications . Many thanks for the suggestion about my background. I will take the 1st graduate course of SDE in the Spring",,"['probability', 'ordinary-differential-equations', 'stochastic-processes', 'stochastic-calculus', 'applications']"
85,Expected Value of Game,Expected Value of Game,,"The game is as follows: Start with 1. If you are at $n$, add one with a probability $\frac{1}{n+1}$, otherwise subtract one. End the game if you hit 0. What is the expected number of rounds that this game lasts? Through empirical data, I have calculated this value to be approximately $2e-1$, but am unsure as to how to prove this result.","The game is as follows: Start with 1. If you are at $n$, add one with a probability $\frac{1}{n+1}$, otherwise subtract one. End the game if you hit 0. What is the expected number of rounds that this game lasts? Through empirical data, I have calculated this value to be approximately $2e-1$, but am unsure as to how to prove this result.",,"['probability', 'combinatorics']"
86,Probability of getting red ball at ith step,Probability of getting red ball at ith step,,"An urn has r red and w white balls that are randomly removed one at a   time. Let $R_i$ be the event that the $i$th ball removed is red. Find   $P(R_i)$ I started with calculating $P(R_1) = \frac{r}{r+w}$. Next,  $$P(R_2) = P(R_2|R_1)\cdot P(R_1) + P(R_2|W_1)\cdot P(W_1) = $$ $$ = \frac{r-1}{r-1+w}\cdot\frac{r}{r+w}+\frac{r}{r+w-1}\cdot\frac{w}{r+w} = \frac{\left(r-1+w\right)r}{\left(r+w-1\right)\left(r+w\right)} = \frac{r}{r+w}$$ And so on. The answer is $P(R_i) = \frac{r}{r+w}$. The textbook's answer shows that the calculations weren't necessary: $\frac{r}{r+w}$ because each of the $r + w$ balls is equally likely to   be the $i$th ball removed. I don't see why each ball is equally likely. At any ith step we don't have $(i-1)$ balls! Could someone help me to understand this?","An urn has r red and w white balls that are randomly removed one at a   time. Let $R_i$ be the event that the $i$th ball removed is red. Find   $P(R_i)$ I started with calculating $P(R_1) = \frac{r}{r+w}$. Next,  $$P(R_2) = P(R_2|R_1)\cdot P(R_1) + P(R_2|W_1)\cdot P(W_1) = $$ $$ = \frac{r-1}{r-1+w}\cdot\frac{r}{r+w}+\frac{r}{r+w-1}\cdot\frac{w}{r+w} = \frac{\left(r-1+w\right)r}{\left(r+w-1\right)\left(r+w\right)} = \frac{r}{r+w}$$ And so on. The answer is $P(R_i) = \frac{r}{r+w}$. The textbook's answer shows that the calculations weren't necessary: $\frac{r}{r+w}$ because each of the $r + w$ balls is equally likely to   be the $i$th ball removed. I don't see why each ball is equally likely. At any ith step we don't have $(i-1)$ balls! Could someone help me to understand this?",,['probability']
87,How to prove that the binomial distribution is approximately close to the normal distribution when $np(1-p) \geq 10$,How to prove that the binomial distribution is approximately close to the normal distribution when,np(1-p) \geq 10,"I would like a formal proof for this ""rule of thumb."" Can you assist me in getting to this solution? I require the insights and creativities of mathematicians. We know that if $np(1-p) \geq 10$ the binomial random variable $X$ is approximately normally distributed with mean $np$ and standard deviation $\sqrt{np(1-p)}$","I would like a formal proof for this ""rule of thumb."" Can you assist me in getting to this solution? I require the insights and creativities of mathematicians. We know that if $np(1-p) \geq 10$ the binomial random variable $X$ is approximately normally distributed with mean $np$ and standard deviation $\sqrt{np(1-p)}$",,"['probability', 'statistics', 'probability-theory', 'probability-distributions']"
88,Almost sure convergence proof,Almost sure convergence proof,,Cud someone please explain  the proof of $ P(X_n \to X)=1 $ iff $$ \lim_{n \to \infty}P(\sup_{m \ge n} |X_m -X|>\epsilon) \to 0  $$. Im not able to understand the meaning of the various sets they take during the course of the proof.,Cud someone please explain  the proof of $ P(X_n \to X)=1 $ iff $$ \lim_{n \to \infty}P(\sup_{m \ge n} |X_m -X|>\epsilon) \to 0  $$. Im not able to understand the meaning of the various sets they take during the course of the proof.,,"['probability', 'convergence-divergence']"
89,Complex Analysis and Probability Theory,Complex Analysis and Probability Theory,,"My question is a general one. I know that in complex analysis we find some very powerful theorems but given that my main area of study is Statistics and Probability, does complex analysis have applications in those areas? I know that it is helpful for example in the characteristic functions but do you think a deeper level of understanding can make a big difference? Will it make me a better Statistician? Thank you.","My question is a general one. I know that in complex analysis we find some very powerful theorems but given that my main area of study is Statistics and Probability, does complex analysis have applications in those areas? I know that it is helpful for example in the characteristic functions but do you think a deeper level of understanding can make a big difference? Will it make me a better Statistician? Thank you.",,"['probability', 'complex-analysis', 'reference-request', 'soft-question']"
90,Show a stochastic process is a martingale using Ito's lemma,Show a stochastic process is a martingale using Ito's lemma,,"Let $W_t$ be standard Brownian motion. It is well known that $W_t^2-t$ is a martingale. One way to show this is by applying Ito's lemma to calculate that $d(W_t^2-t)/dt = 2W_t dW_t$, which has no drift. Therefore  $W_t^2-t$ is a martingale. I am a novice in stochastic process so I want to ask which theorems one use in this proof?","Let $W_t$ be standard Brownian motion. It is well known that $W_t^2-t$ is a martingale. One way to show this is by applying Ito's lemma to calculate that $d(W_t^2-t)/dt = 2W_t dW_t$, which has no drift. Therefore  $W_t^2-t$ is a martingale. I am a novice in stochastic process so I want to ask which theorems one use in this proof?",,"['probability', 'stochastic-processes']"
91,Puzzle about voting,Puzzle about voting,,"I came across about this puzzle which I'm not sure how to go about. Suppose there are $L$ leaders and $F$ followers, with $1 < L<<F$. A leader makes a binary decision, $0$ or $1$ with same probability. Each follower copies the decision of one of the leaders, for simplicity, choosing one of them with uniform distribution. The experiment is repeated many times, as many as you like. Every follower copies the vote of a leader for a random number of times $k\in {1, 2, ..., K}$, with $K$ equal to some positive integer. After that, the follower chooses another leader or the same, again, uniformly at random. Is there anything in the literature that can be used to solve this problem, that is, to identify or guess who the leaders are?","I came across about this puzzle which I'm not sure how to go about. Suppose there are $L$ leaders and $F$ followers, with $1 < L<<F$. A leader makes a binary decision, $0$ or $1$ with same probability. Each follower copies the decision of one of the leaders, for simplicity, choosing one of them with uniform distribution. The experiment is repeated many times, as many as you like. Every follower copies the vote of a leader for a random number of times $k\in {1, 2, ..., K}$, with $K$ equal to some positive integer. After that, the follower chooses another leader or the same, again, uniformly at random. Is there anything in the literature that can be used to solve this problem, that is, to identify or guess who the leaders are?",,"['probability', 'combinatorics', 'algorithms', 'machine-learning']"
92,The Birthday Problem,The Birthday Problem,,"I've been reading about the birthday problem which, as I'm sure many of you will know, is a statistical problem which aims at finding out the how many people you would need in a random group to be certain that two of them shared a birthday. I've read the wikipedia article and am happy with the concept and the answers to this problem. What I'm interested in doing is expanding the principle. I've been trying to work out the answer to a similar problem, but where you simply wanted to know the probability that two people were born in the same week and in the same month. I'm not really sure how to go about this, though, so my first question is: is there a general equation I can use to extend the problem to these cases? But, I know there are many articles and stackexchange questions on this, so I wouldn't ask unless I had a specific problem, which is this: Suppose a person has met 500 people in their lifetime. What is the probability that seven of those 500 share a birthday in the same two month period? I think the answer to my last question is that it's certain. But could I ask what is the smallest number of people you would need for the probability that - in a group of 500 people - the probability of them sharing a birthday in a two month period is less that 50%? If that makes sense? Okay, thank you everybody, edited to tidy up: Question 1: What is the smallest group of randomly selected people required such that the probability that two of them share a birthday within one week of each other is at least 75%? Question 2: What is the smallest group of randomly selected people required such that the probability that two of them share a birthday within thirty days of each other is at least 75%? Question 3: What is the smallest group of randomly selected people required such that the probability that seven of them share a birthday within sixty days of each other is at least 75%? Question 4: In a group of 30 randomly selected people, what is the probability that seven of them will share a birthday with fifty days? I hope that's a lot clearer. I had no idea how to word these questions until I posted this and am grateful to everyone who's contributed for helping me do so :)","I've been reading about the birthday problem which, as I'm sure many of you will know, is a statistical problem which aims at finding out the how many people you would need in a random group to be certain that two of them shared a birthday. I've read the wikipedia article and am happy with the concept and the answers to this problem. What I'm interested in doing is expanding the principle. I've been trying to work out the answer to a similar problem, but where you simply wanted to know the probability that two people were born in the same week and in the same month. I'm not really sure how to go about this, though, so my first question is: is there a general equation I can use to extend the problem to these cases? But, I know there are many articles and stackexchange questions on this, so I wouldn't ask unless I had a specific problem, which is this: Suppose a person has met 500 people in their lifetime. What is the probability that seven of those 500 share a birthday in the same two month period? I think the answer to my last question is that it's certain. But could I ask what is the smallest number of people you would need for the probability that - in a group of 500 people - the probability of them sharing a birthday in a two month period is less that 50%? If that makes sense? Okay, thank you everybody, edited to tidy up: Question 1: What is the smallest group of randomly selected people required such that the probability that two of them share a birthday within one week of each other is at least 75%? Question 2: What is the smallest group of randomly selected people required such that the probability that two of them share a birthday within thirty days of each other is at least 75%? Question 3: What is the smallest group of randomly selected people required such that the probability that seven of them share a birthday within sixty days of each other is at least 75%? Question 4: In a group of 30 randomly selected people, what is the probability that seven of them will share a birthday with fifty days? I hope that's a lot clearer. I had no idea how to word these questions until I posted this and am grateful to everyone who's contributed for helping me do so :)",,"['probability', 'combinatorics', 'statistics', 'birthday']"
93,Intuition about whether to switch in box problem,Intuition about whether to switch in box problem,,"I ran across an apparent paradox which I then located in the paper The Box Problem: To Switch or Not to Switch as such: Imagine that you are shown two identical boxes. You know that one of   them contains. \$b and the other \$2b. Picking one at random and opening   it, you must decide whether to keep it (and its contents), or exchange   it for the other box. In short, when you find $x dollars in a box, the expected value of the other box is .5*.5x + .5*2x = 1.25x, meaning that it's always better to switch. This appears to violate the symmetry of the problem and the fact that you still know nothing meaningful about either box. The paper goes another direction with it, talking about how having prior knowledge of expected values gives a more meaningful analysis (along with other discussions). However, what if there's no prior knowledge, and we have the original problem as stated. Can anyone give me some intuition to make sense of this? EDIT: Someone found this question which asks a slightly different formulation, but an identical problem. The accepted answer there just points to a paper , and I'm having difficulty understanding the paper. It explains away the paradox by noting the expectation is based on an infinite sum, and the value depends on the order the sum is evaluated. I'm not familiar with how the order of a sum can change a value, and I also don't see talking about a different way to evaluate the expectation explains the strange result outlined above. My math understanding is primarily based on reading textbooks as a hobby, and I haven't yet worked up to fully understanding math academic papers, so a simpler explanation would be helpful.","I ran across an apparent paradox which I then located in the paper The Box Problem: To Switch or Not to Switch as such: Imagine that you are shown two identical boxes. You know that one of   them contains. \$b and the other \$2b. Picking one at random and opening   it, you must decide whether to keep it (and its contents), or exchange   it for the other box. In short, when you find $x dollars in a box, the expected value of the other box is .5*.5x + .5*2x = 1.25x, meaning that it's always better to switch. This appears to violate the symmetry of the problem and the fact that you still know nothing meaningful about either box. The paper goes another direction with it, talking about how having prior knowledge of expected values gives a more meaningful analysis (along with other discussions). However, what if there's no prior knowledge, and we have the original problem as stated. Can anyone give me some intuition to make sense of this? EDIT: Someone found this question which asks a slightly different formulation, but an identical problem. The accepted answer there just points to a paper , and I'm having difficulty understanding the paper. It explains away the paradox by noting the expectation is based on an infinite sum, and the value depends on the order the sum is evaluated. I'm not familiar with how the order of a sum can change a value, and I also don't see talking about a different way to evaluate the expectation explains the strange result outlined above. My math understanding is primarily based on reading textbooks as a hobby, and I haven't yet worked up to fully understanding math academic papers, so a simpler explanation would be helpful.",,"['probability', 'paradoxes']"
94,$\lim_n \frac{1}{n} E(\max_{1\le j\le n} |X_j|) = 0$,,\lim_n \frac{1}{n} E(\max_{1\le j\le n} |X_j|) = 0,"If $\{X_n\}$ is a sequence of identically distributed r.v.'s with finite mean, then $$\lim_n \frac{1}{n} E(\max_{1\le j\le n} |X_j|) = 0$$ The inequality $$\frac{1}{n}E(\max_{1\le j\le n} |X_j|) \le \frac{1}{n}E(|X_1| + \cdots + |X_n|)=E(|X_1|)$$ suggests that the result has something to do with the ""graphs"" of the $|X_j|$ overlapping so that $E(|X_j|)=\int_0^\infty |X_j| P(d\omega)$ isn't fully counted $n$ times in the value of $E(\max_{1\le j\le n} |X_j|)$. But how do we make this rigorous? Notice there is no mention of the $X_j$ being independent.","If $\{X_n\}$ is a sequence of identically distributed r.v.'s with finite mean, then $$\lim_n \frac{1}{n} E(\max_{1\le j\le n} |X_j|) = 0$$ The inequality $$\frac{1}{n}E(\max_{1\le j\le n} |X_j|) \le \frac{1}{n}E(|X_1| + \cdots + |X_n|)=E(|X_1|)$$ suggests that the result has something to do with the ""graphs"" of the $|X_j|$ overlapping so that $E(|X_j|)=\int_0^\infty |X_j| P(d\omega)$ isn't fully counted $n$ times in the value of $E(\max_{1\le j\le n} |X_j|)$. But how do we make this rigorous? Notice there is no mention of the $X_j$ being independent.",,"['probability', 'probability-theory']"
95,"How can a $\sigma$-algebra be ""treated"" or computed? Example","How can a -algebra be ""treated"" or computed? Example",\sigma,"My question is: I have a random variable $X:\Omega \rightarrow \mathbb{R}$, the $\sigma$-algebra generated by $X$ is: $\sigma(X) := \{X^{-1}(B), B\in \mathcal{B}(\mathbb{R})\}$. But, imagine now that $X=\exp \{\mu + \sigma Z\}$ with $Z \sim N(0,1)$. Does this help when trying to compute $\sigma(X)$? Like... can we say something like $\sigma(X) = \sigma(Z)$? or similar? Is this special case easily solvable? I mean, write explicitly $\sigma(X)$. Thank you very much for your time and help!","My question is: I have a random variable $X:\Omega \rightarrow \mathbb{R}$, the $\sigma$-algebra generated by $X$ is: $\sigma(X) := \{X^{-1}(B), B\in \mathcal{B}(\mathbb{R})\}$. But, imagine now that $X=\exp \{\mu + \sigma Z\}$ with $Z \sim N(0,1)$. Does this help when trying to compute $\sigma(X)$? Like... can we say something like $\sigma(X) = \sigma(Z)$? or similar? Is this special case easily solvable? I mean, write explicitly $\sigma(X)$. Thank you very much for your time and help!",,"['probability', 'measure-theory', 'probability-theory', 'probability-distributions']"
96,How to find limits of integration on a convolution of CRVs,How to find limits of integration on a convolution of CRVs,,"In finding the convolution of two independent and continuous random variables, I am struggling with limits of integration.  I cannot seem to figure out over what intervals the probability density function $f_{Z}(a) = f_{X+Y}(a)$ breaks out to. The most basic example is where $f_{X}(a)$ and $f_{Y}(a)$ are both uniform over $[0,1]$ and independent.  The intervals for $f_{Z}(a)$ are $(0,1)$ and $(1,2)$.  But why? There are also more complicated cases such as two exponential  R.V.s (say with parameters $\lambda$ and $2\lambda$) or an exponential and a uniform, (say $\lambda$ and $[0, 1]$), etc. For reference: $$f_{x+y}(a) = \int_{-\infty}^{\infty} f_X(a-y)f_Y(y)~dy$$ Once I can set it up, the integration is (usually) no trouble.","In finding the convolution of two independent and continuous random variables, I am struggling with limits of integration.  I cannot seem to figure out over what intervals the probability density function $f_{Z}(a) = f_{X+Y}(a)$ breaks out to. The most basic example is where $f_{X}(a)$ and $f_{Y}(a)$ are both uniform over $[0,1]$ and independent.  The intervals for $f_{Z}(a)$ are $(0,1)$ and $(1,2)$.  But why? There are also more complicated cases such as two exponential  R.V.s (say with parameters $\lambda$ and $2\lambda$) or an exponential and a uniform, (say $\lambda$ and $[0, 1]$), etc. For reference: $$f_{x+y}(a) = \int_{-\infty}^{\infty} f_X(a-y)f_Y(y)~dy$$ Once I can set it up, the integration is (usually) no trouble.",,"['probability', 'multivariable-calculus', 'integration', 'definite-integrals', 'actuarial-science']"
97,Random Variable Probability Russian Roulette,Random Variable Probability Russian Roulette,,"I am very confused on this one. Any help in how to solve it and what probability rule to use would be appreciated. Consider the situation in which a person played Russian roulette (one bullet, 6 chambers) until the bullet fires. Let X be the random variable that represents the number of shots until the game ends. Clearly, this number includes all shots, including the one in which bullet fires. a) What values can X take?  b) Find the probability for each of the following:     X = 1, 3, 5, 7, 9","I am very confused on this one. Any help in how to solve it and what probability rule to use would be appreciated. Consider the situation in which a person played Russian roulette (one bullet, 6 chambers) until the bullet fires. Let X be the random variable that represents the number of shots until the game ends. Clearly, this number includes all shots, including the one in which bullet fires. a) What values can X take?  b) Find the probability for each of the following:     X = 1, 3, 5, 7, 9",,"['probability', 'statistics', 'random']"
98,What are the chances of three consecutive hands of all vowels in Scrabble?,What are the chances of three consecutive hands of all vowels in Scrabble?,,"We’ve all been confounded by an all-vowel “hand” in Scrabble. What is the likelihood of it happening 3 turns in a row? Some assumptions are necessary. Let’s use these: Two-player game English tile distribution Vowels are AEIOU (ie, exclude Y) The situation is drawing all vowels on the first pull, followed by being in that situation on the subsequent two turns. The subject draws first. The opponent plays first (this probably contradicts the game rules but is convenient for this experiment). The opponent plays a four-letter word on every turn, and that word is two consonants and two vowels. The subject is able to play two of their vowels on each turn, requiring a draw of two new tiles. (If any of the above assumptions is logically impossible, please correct.) An interesting bit is that blanks can be counted as either a consonant or vowel. Distribution of English words and letter usage is disregarded for this calculation. Though if you want to throw it in for extra interest, great.","We’ve all been confounded by an all-vowel “hand” in Scrabble. What is the likelihood of it happening 3 turns in a row? Some assumptions are necessary. Let’s use these: Two-player game English tile distribution Vowels are AEIOU (ie, exclude Y) The situation is drawing all vowels on the first pull, followed by being in that situation on the subsequent two turns. The subject draws first. The opponent plays first (this probably contradicts the game rules but is convenient for this experiment). The opponent plays a four-letter word on every turn, and that word is two consonants and two vowels. The subject is able to play two of their vowels on each turn, requiring a draw of two new tiles. (If any of the above assumptions is logically impossible, please correct.) An interesting bit is that blanks can be counted as either a consonant or vowel. Distribution of English words and letter usage is disregarded for this calculation. Though if you want to throw it in for extra interest, great.",,"['probability', 'recreational-mathematics']"
99,"Stuck with handling of conditional probability in Bishop's ""Pattern Recognition and Machine Learning"" (1.66)","Stuck with handling of conditional probability in Bishop's ""Pattern Recognition and Machine Learning"" (1.66)",,"I've just started working through the book, and I'm stuck with how the author handles conditional probability in (1.66). The context is as follows. In this chapter we are working with a curve fitting task: we try to fit a polynomial $\sum w_ix^i$ to a training set $\{\mathbf {x}, \mathbf {t}\}$ with  an assumption of Gaussian noise, i.e. for a single observed value $t$ for $x$, $p(t|x,\mathbf{w},\beta)=N(t|\sum w_ix^i,\beta^{-1})$, and assuming independence of data points, the likelihood is  $p(\mathbf{t}|\mathbf{x},\mathbf{w},\beta)=\prod N(t_n|\sum w_ix^i,\beta^{-1}).$ Prior distribution of $\mathbf{w}$ is a multivariate Gaussian: $p(\mathbf{w}|\alpha)=N(\mathbf{0},\alpha^{-1}\mathbf{I})$. Now, the author states: ""Using Bayes’ theorem, the posterior distribution for w is proportional to the product of the prior distribution and the likelihood function: $$p(\mathbf{w}|\mathbf{x},\mathbf{t},\alpha,\beta)\propto p(\mathbf{t}|\mathbf{x},\mathbf{w},\beta)p(\mathbf{w}|\alpha) \tag{1.66}$$ Later, this proportionality is used to maximize the probability on the left to obtain MAP value of $\mathbf{w}$, so significant factors cannot be simply omitted on the right. This is where I'm stuck. The problem is I don't understand how he applies Bayes here. I tried to derive it, and that's what I've got: $$p(\mathbf{w}|\mathbf{x},\mathbf{t},\alpha,\beta)\propto p(\mathbf{x},\mathbf{t},\mathbf{w},\alpha,\beta)=p(\mathbf{t}|\mathbf{x,w},\alpha,\beta)p(\mathbf{x,w},\alpha,\beta)\tag{A}$$ where the latter equals to $p(\mathbf{w}|\alpha,\beta,\mathbf{x})p(\alpha,\beta,\mathbf{x})$. I see that we can get rid of the second factor here because it is not really interesting if we want to maximize the expression — these are just model parameters or the data which is given. Also, I see that we can rewrite the first factor as $p(\mathbf{w}|\alpha)$. That's why: say we have $p(A|BC)\text{, then } p(A|BC)=\frac {p(ABC)}{p(BC)}=\frac {p(AB)p(C)}{p(B)p(C)}=p(A|B)$, which holds if both ($AB$ and $C$) and ($B$ and $C$) are independent, and indeed both $\mathbf{w}$ and $\alpha$ are independent with both $\beta\text{ and }\mathbf{x}$. But I don't see why we can ""remove"" $\alpha$ from the first factor of (A). For one, $\mathbf{w}$ and $\alpha$ are not independent. Maybe I don't understand the problem well enough? Maybe this probability can be factorized so that we can say ""this factor is irrelevant,let's hide it under $\propto$""? So, the questions: 1) Please help with understanding of the proportionality (1.66). 2) It's hard for me to see the benefit of using conditional distributions on things like $\alpha$ and $\beta$. Is, for example, $p(\mathbf{w})$ of any interest in this task? I don't see how any meaning can be attributed to it. Could someone explain that? 3) Is there some common knowledge about conditioning on several random variables, like the one I used ($p(A|BC)=p(A|B)$ if both pairs ($AB$ and $C$) and ($B$ and $C$) are independent) , that make it obvious?","I've just started working through the book, and I'm stuck with how the author handles conditional probability in (1.66). The context is as follows. In this chapter we are working with a curve fitting task: we try to fit a polynomial $\sum w_ix^i$ to a training set $\{\mathbf {x}, \mathbf {t}\}$ with  an assumption of Gaussian noise, i.e. for a single observed value $t$ for $x$, $p(t|x,\mathbf{w},\beta)=N(t|\sum w_ix^i,\beta^{-1})$, and assuming independence of data points, the likelihood is  $p(\mathbf{t}|\mathbf{x},\mathbf{w},\beta)=\prod N(t_n|\sum w_ix^i,\beta^{-1}).$ Prior distribution of $\mathbf{w}$ is a multivariate Gaussian: $p(\mathbf{w}|\alpha)=N(\mathbf{0},\alpha^{-1}\mathbf{I})$. Now, the author states: ""Using Bayes’ theorem, the posterior distribution for w is proportional to the product of the prior distribution and the likelihood function: $$p(\mathbf{w}|\mathbf{x},\mathbf{t},\alpha,\beta)\propto p(\mathbf{t}|\mathbf{x},\mathbf{w},\beta)p(\mathbf{w}|\alpha) \tag{1.66}$$ Later, this proportionality is used to maximize the probability on the left to obtain MAP value of $\mathbf{w}$, so significant factors cannot be simply omitted on the right. This is where I'm stuck. The problem is I don't understand how he applies Bayes here. I tried to derive it, and that's what I've got: $$p(\mathbf{w}|\mathbf{x},\mathbf{t},\alpha,\beta)\propto p(\mathbf{x},\mathbf{t},\mathbf{w},\alpha,\beta)=p(\mathbf{t}|\mathbf{x,w},\alpha,\beta)p(\mathbf{x,w},\alpha,\beta)\tag{A}$$ where the latter equals to $p(\mathbf{w}|\alpha,\beta,\mathbf{x})p(\alpha,\beta,\mathbf{x})$. I see that we can get rid of the second factor here because it is not really interesting if we want to maximize the expression — these are just model parameters or the data which is given. Also, I see that we can rewrite the first factor as $p(\mathbf{w}|\alpha)$. That's why: say we have $p(A|BC)\text{, then } p(A|BC)=\frac {p(ABC)}{p(BC)}=\frac {p(AB)p(C)}{p(B)p(C)}=p(A|B)$, which holds if both ($AB$ and $C$) and ($B$ and $C$) are independent, and indeed both $\mathbf{w}$ and $\alpha$ are independent with both $\beta\text{ and }\mathbf{x}$. But I don't see why we can ""remove"" $\alpha$ from the first factor of (A). For one, $\mathbf{w}$ and $\alpha$ are not independent. Maybe I don't understand the problem well enough? Maybe this probability can be factorized so that we can say ""this factor is irrelevant,let's hide it under $\propto$""? So, the questions: 1) Please help with understanding of the proportionality (1.66). 2) It's hard for me to see the benefit of using conditional distributions on things like $\alpha$ and $\beta$. Is, for example, $p(\mathbf{w})$ of any interest in this task? I don't see how any meaning can be attributed to it. Could someone explain that? 3) Is there some common knowledge about conditioning on several random variables, like the one I used ($p(A|BC)=p(A|B)$ if both pairs ($AB$ and $C$) and ($B$ and $C$) are independent) , that make it obvious?",,"['probability', 'machine-learning', 'pattern-recognition', 'bayesian']"
