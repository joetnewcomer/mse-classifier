,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,L2 boundness of Integral Operator,L2 boundness of Integral Operator,,"If we have the kernel $k(x,y)=\frac{x}{(y-x)^2}$ for $(x,y)\in(0,\infty)\times(-\infty,0)$ , my question is: is this operator $$Tf(x)=\int_{-\infty}^{0}k(x,y)f(y)dy$$ bounded in $L^2(\mathbb{R}^{+})$ ? In other words: $$\int_{0}^{\infty}\left|Tf(x)\right|^2dx< C ||f||_{L^2(\mathbb{R}^{+})}$$ for some $C>0$ ? I tried to see if $k\in L^2(\mathbb{R}^2)$ , but it seems not to be true. Not all the singular integrals have integrable kernel, but I don't know how to follow at this point. Maybe it could be related to the derivative of the Hilbert transform, but I have an "" $x$ "" in the numerator anyways. Thank you in advance!","If we have the kernel for , my question is: is this operator bounded in ? In other words: for some ? I tried to see if , but it seems not to be true. Not all the singular integrals have integrable kernel, but I don't know how to follow at this point. Maybe it could be related to the derivative of the Hilbert transform, but I have an "" "" in the numerator anyways. Thank you in advance!","k(x,y)=\frac{x}{(y-x)^2} (x,y)\in(0,\infty)\times(-\infty,0) Tf(x)=\int_{-\infty}^{0}k(x,y)f(y)dy L^2(\mathbb{R}^{+}) \int_{0}^{\infty}\left|Tf(x)\right|^2dx< C ||f||_{L^2(\mathbb{R}^{+})} C>0 k\in L^2(\mathbb{R}^2) x","['functional-analysis', 'analysis']"
1,"If $(M,d)$ is complete, is every non-empty open set a second category set?","If  is complete, is every non-empty open set a second category set?","(M,d)","If $(M,d)$ is complete, is every non-empty open set a second category set? I know Baire's Category Theorem for Metric Spaces, which says: ""A complete metric space is of the second category in itself, i.e. if we write $M = \bigcup_{n\ge 1} E_n$ , then the closure of some $E_n$ contains an open ball. Equivalently, if $(G_n)$ is a sequence of dense open sets in $M$ , then $\bigcup_{n\ge 1} G_n \ne \varnothing$ . In fact $\bigcap_{n\ge 1} G_n$ is dense in $M$ ."" Somehow, I need to use this (or more) to answer the question above. I feel that the answer is Yes , but I'm not sure how to prove/disprove it. Here's what I've tried so far: Suppose $S \ne \varnothing$ , and $S$ is open. So for every $x\in S$ , there exists $r_x > 0$ such that $B(x,r_x) \subset S$ . Now there are two possibilities, $S$ is either first category or second. Attempt 1: If $S$ is first category in $M$ , then $S$ can be written as $$S = \bigcup_{n\ge 1} S_n$$ where $S_n$ is nowhere dense in $M$ for every $n\ge 1$ (so, $\text{int}(\overline{S_n}) = \emptyset$ ). To find a contradiction, I must get hold of some $n$ such that $\text{int}(\overline{S_n}) \ne \emptyset$ . What do I do? Attempt 2 : Consider $M_n$ such that $M = \bigcup_{n\ge 1} M_n$ and $A \subset M$ is open. Since $M$ is complete, it is second category in itself, i.e. there is some $n$ such that $M_n$ is not nowhere dense. $M \cap A = A$ , so we get $$A = A \cap \bigcup_{n\ge 1} M_n = \bigcup_{n\ge 1} (M_n \cap A)$$ Now if we can prove that $M_n \cap A$ is not nowhere dense, then we are done (maybe it isn't true, maybe this isn't the right way). We know that $\text{int}(\overline{M_n}) \ne \emptyset$ , what can we say about $\text{int}(\overline{ M_n \cap A})$ ? A link in the comments proves that every open subset of a Baire space is also a Baire space. How does that help? $(M,d)$ is a complete metric space, and so a Baire space. $A\subset M$ is open, and by the statement above, $A$ is also a Baire space. How do I know that $(A,d)$ is complete? Are complete metric spaces the only metric spaces which are Baire spaces? If yes, then knowing that $A$ is complete - I can use Baire's theorem to conclude that $A$ is second category.","If is complete, is every non-empty open set a second category set? I know Baire's Category Theorem for Metric Spaces, which says: ""A complete metric space is of the second category in itself, i.e. if we write , then the closure of some contains an open ball. Equivalently, if is a sequence of dense open sets in , then . In fact is dense in ."" Somehow, I need to use this (or more) to answer the question above. I feel that the answer is Yes , but I'm not sure how to prove/disprove it. Here's what I've tried so far: Suppose , and is open. So for every , there exists such that . Now there are two possibilities, is either first category or second. Attempt 1: If is first category in , then can be written as where is nowhere dense in for every (so, ). To find a contradiction, I must get hold of some such that . What do I do? Attempt 2 : Consider such that and is open. Since is complete, it is second category in itself, i.e. there is some such that is not nowhere dense. , so we get Now if we can prove that is not nowhere dense, then we are done (maybe it isn't true, maybe this isn't the right way). We know that , what can we say about ? A link in the comments proves that every open subset of a Baire space is also a Baire space. How does that help? is a complete metric space, and so a Baire space. is open, and by the statement above, is also a Baire space. How do I know that is complete? Are complete metric spaces the only metric spaces which are Baire spaces? If yes, then knowing that is complete - I can use Baire's theorem to conclude that is second category.","(M,d) M = \bigcup_{n\ge 1} E_n E_n (G_n) M \bigcup_{n\ge 1} G_n \ne \varnothing \bigcap_{n\ge 1} G_n M S \ne \varnothing S x\in S r_x > 0 B(x,r_x) \subset S S S M S S = \bigcup_{n\ge 1} S_n S_n M n\ge 1 \text{int}(\overline{S_n}) = \emptyset n \text{int}(\overline{S_n}) \ne \emptyset M_n M = \bigcup_{n\ge 1} M_n A \subset M M n M_n M \cap A = A A = A \cap \bigcup_{n\ge 1} M_n = \bigcup_{n\ge 1} (M_n \cap A) M_n \cap A \text{int}(\overline{M_n}) \ne \emptyset \text{int}(\overline{ M_n \cap A}) (M,d) A\subset M A (A,d) A A","['real-analysis', 'analysis', 'metric-spaces', 'baire-category']"
2,"Incompleteness of $(C[a,b], \| \cdot \|_1)$ [duplicate]",Incompleteness of  [duplicate],"(C[a,b], \| \cdot \|_1)","This question already has an answer here : Completeness of $\langle \mathscr{C} [0, 1], \| \cdot \|_1 \rangle$ (1 answer) Closed 3 years ago . Consider $C([a,b])$ , the vector space of continuous functions in $[a,b] \subset \mathbb{R}$ to $\mathbb{R}$ . Let be $f \in C([a,b])$ , we define $\|\cdot\|_1$ as follows: \begin{equation*}  \|f\|_1 =  \int_{a}^{b}|f(x)|dx  \end{equation*} I must show that $(C([a,b]), \|\cdot\|_1)$ is not a Banach space. This is my answer the which one is wrong according to my teacher. Consider the following counterexample. Let $c =2^{-1}(a+b)$ y $f_n:[a,b] \rightarrow \mathbb{R} \hspace{.1cm} \forall n \in \mathbb{N}$ with the following association rule and graph: It's clear that $\{f_n\}_{n \in \mathbb{N}} \subset C([a,b])$ . We will show that $\{f_n\}_{n \in \mathbb{N}}$ it's a $||\cdot||_1-Cauchy$ sequence. Let be $\epsilon > 0$ and $N = \displaystyle \left\lceil{ \frac{1}{2 \epsilon}}\right\rceil$ , then: \begin{equation*}     ||f_n-f_m||_1 = \int_{a}^{b} |f_n(x)-f_m(x)|dx = \left| \frac{1}{2n}-\frac{1}{2m} \right| \leq \max \left\{ \frac{1}{2n}, \frac{1}{2m}\right\} < \epsilon, \hspace{.1cm} \forall n,m \geq N \end{equation*} So the sequence is $||\cdot||_1-Cauchy$ , however though this sequence converges to the following limit function, the limit function is clearly discontinuous. Let be $f:[a,b] \rightarrow \mathbb{R}$ : \begin{equation*} f(x) =      \begin{cases}         1 & \text{si $x \leq c$}\\         0 & \text{si $x > c$}     \end{cases} \end{equation*} Fixed $\epsilon_1 > 0$ and $N_1 = \left\lceil{\displaystyle\frac{1}{2 \epsilon_1}}\right\rceil$ , then: \begin{equation*} ||f_n-f||_1 = \displaystyle\int_{0}^{1} |f_n(x)-f(x)| = \frac{1}{2n} < \epsilon, \hspace{.1cm} \forall n \geq N_1 \end{equation*} $\textbf{Now, the problem is that}$ (according to my teacher) I only have propose a discontinuous function wich is a limit of the sequence, this does not means that do not exits another continuous function wich is also the limit of the sequence. I understand the problem, but nothing comes up to me to solve this.","This question already has an answer here : Completeness of $\langle \mathscr{C} [0, 1], \| \cdot \|_1 \rangle$ (1 answer) Closed 3 years ago . Consider , the vector space of continuous functions in to . Let be , we define as follows: I must show that is not a Banach space. This is my answer the which one is wrong according to my teacher. Consider the following counterexample. Let y with the following association rule and graph: It's clear that . We will show that it's a sequence. Let be and , then: So the sequence is , however though this sequence converges to the following limit function, the limit function is clearly discontinuous. Let be : Fixed and , then: (according to my teacher) I only have propose a discontinuous function wich is a limit of the sequence, this does not means that do not exits another continuous function wich is also the limit of the sequence. I understand the problem, but nothing comes up to me to solve this.","C([a,b]) [a,b] \subset \mathbb{R} \mathbb{R} f \in C([a,b]) \|\cdot\|_1 \begin{equation*} 
\|f\|_1 =  \int_{a}^{b}|f(x)|dx 
\end{equation*} (C([a,b]), \|\cdot\|_1) c =2^{-1}(a+b) f_n:[a,b] \rightarrow \mathbb{R} \hspace{.1cm} \forall n \in \mathbb{N} \{f_n\}_{n \in \mathbb{N}} \subset C([a,b]) \{f_n\}_{n \in \mathbb{N}} ||\cdot||_1-Cauchy \epsilon > 0 N = \displaystyle \left\lceil{ \frac{1}{2 \epsilon}}\right\rceil \begin{equation*}
    ||f_n-f_m||_1 = \int_{a}^{b} |f_n(x)-f_m(x)|dx = \left| \frac{1}{2n}-\frac{1}{2m} \right| \leq \max \left\{ \frac{1}{2n}, \frac{1}{2m}\right\} < \epsilon, \hspace{.1cm} \forall n,m \geq N
\end{equation*} ||\cdot||_1-Cauchy f:[a,b] \rightarrow \mathbb{R} \begin{equation*} f(x) = 
    \begin{cases}
        1 & \text{si x \leq c}\\
        0 & \text{si x > c}
    \end{cases}
\end{equation*} \epsilon_1 > 0 N_1 = \left\lceil{\displaystyle\frac{1}{2 \epsilon_1}}\right\rceil \begin{equation*}
||f_n-f||_1 = \displaystyle\int_{0}^{1} |f_n(x)-f(x)| = \frac{1}{2n} < \epsilon, \hspace{.1cm} \forall n \geq N_1
\end{equation*} \textbf{Now, the problem is that}","['calculus', 'functional-analysis', 'analysis', 'banach-spaces', 'normed-spaces']"
3,"Degree, Angle, Period and maps onto the unit circle","Degree, Angle, Period and maps onto the unit circle",,"This question is a follow-up from this question , whose accepted answer was to go and read up Allen Hatcher's book on Algebraic Topology, Chapter 1. I read it, but it only helps up to a point. The question is: suppose that $\Omega=D_2\setminus \overline{D_1}=\{(x_1,x_2) : 1< x_1^2+x_2^2 <4\}$ , and let $\phi$ be a smooth map from $\Omega$ to $S^1=\{(x_1,x_2) : 1= x_1^2+x_2^2\}$ . Can I write $$ \phi = \left(\cos(\theta),\sin(\theta)\right), $$ for some function $\theta\in C^\infty(\Omega;\mathbb{R}/(2\pi k\mathbb{Z}))$ , for some $k$ possibly depending on $\phi$ ? And if not, what can I write? I am doing a bit of cargo cult mathematics here, because I think this is what I read means, but am certainly not confident and I don't master the machinery. If $\phi$ was a map from $S^1$ to $S^1$ , then I think this is what it would be, and $k$ would be the Brouwer degree of $\phi$ . And since $\Omega$ is $2$ dimensional but homotopic to the circle, it should be the same...but is that true? Going further, I can understand functions $ C^\infty(\Omega;\mathbb{R}/(2\pi\mathbb{Z}))$ . So is it the case that, in fact, degree has nothing to do with it and the answer is simply $$ \theta\in C^\infty(\Omega;\mathbb{R}/(2\pi \mathbb{Z})) $$ (and Algebraic Topology says something simple, with no $k$ index subgroup involved..)?","This question is a follow-up from this question , whose accepted answer was to go and read up Allen Hatcher's book on Algebraic Topology, Chapter 1. I read it, but it only helps up to a point. The question is: suppose that , and let be a smooth map from to . Can I write for some function , for some possibly depending on ? And if not, what can I write? I am doing a bit of cargo cult mathematics here, because I think this is what I read means, but am certainly not confident and I don't master the machinery. If was a map from to , then I think this is what it would be, and would be the Brouwer degree of . And since is dimensional but homotopic to the circle, it should be the same...but is that true? Going further, I can understand functions . So is it the case that, in fact, degree has nothing to do with it and the answer is simply (and Algebraic Topology says something simple, with no index subgroup involved..)?","\Omega=D_2\setminus \overline{D_1}=\{(x_1,x_2) : 1< x_1^2+x_2^2 <4\} \phi \Omega S^1=\{(x_1,x_2) : 1= x_1^2+x_2^2\} 
\phi = \left(\cos(\theta),\sin(\theta)\right),
 \theta\in C^\infty(\Omega;\mathbb{R}/(2\pi k\mathbb{Z})) k \phi \phi S^1 S^1 k \phi \Omega 2  C^\infty(\Omega;\mathbb{R}/(2\pi\mathbb{Z})) 
\theta\in C^\infty(\Omega;\mathbb{R}/(2\pi \mathbb{Z}))
 k","['complex-analysis', 'analysis']"
4,"Prove that if b is an undefined dot product, then there is a base of time vectors, another of light vectors, and another of space vectors","Prove that if b is an undefined dot product, then there is a base of time vectors, another of light vectors, and another of space vectors",,"Prove that if b is an undefined dot product, then there is a base of time vectors, another of light vectors, and another of space vectors. Taking into account that: Luminous implies $ b (v, v) = 0 $ Time implies $ b (v, v) <0 $ Spatially implies $ b (v, v)> 0 $ I think that one should start with the basis of orthonormal vectors. Since this is always possible because by definition a dot product is always non-degenerate. Then, from that orthonormal basis, it would be possible to build the required bases. But I wouldn't know how to do it","Prove that if b is an undefined dot product, then there is a base of time vectors, another of light vectors, and another of space vectors. Taking into account that: Luminous implies Time implies Spatially implies I think that one should start with the basis of orthonormal vectors. Since this is always possible because by definition a dot product is always non-degenerate. Then, from that orthonormal basis, it would be possible to build the required bases. But I wouldn't know how to do it"," b (v, v) = 0   b (v, v) <0   b (v, v)> 0 ","['geometry', 'analysis']"
5,Convergence of partial sums to Fourier series?,Convergence of partial sums to Fourier series?,,"I am relatively new to Fourier series. Let $f \in L^2([-\pi, \pi])$ with $f(-\pi) = f(\pi)$ . The Fourier coefficients of $f$ are given by \begin{align*} \hat{f}(n) = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t) e^{-int} \, dt, \end{align*} the partial sums of the Fourier series of $f$ are given by \begin{align*} s_N(t) = \sum \limits_{-N}^N \hat{f}(n) e^{int}, \end{align*} and the Fourier series of $f$ is given by \begin{align*} F(t) = \sum_{-\infty}^\infty \hat{f}(n) e^{int}. \end{align*} My question is: do the functions $s_N$ converge pointwise to $F$ ? It seems to me that this should both be true, but then I derive a contradiction. It is known that the $s_N$ converge to $f$ in the $L^2$ norm, and so a subsequence $(s_{N_k})_k$ converges pointwise to $f$ . This implies that $f=F$ almost everywhere, but since $s_N \rightarrow F$ pointwise, we get $s_N \rightarrow f$ pointwise almost everywhere. But as far as I can tell, pointwise convergence of the Fourier series to the original function $f$ is a very difficult question. Is the ""almost everywhere"" what makes a difference? Did I make a mistake in my argument? Or do the $s_N$ just not converge pointwise to $F$ ?","I am relatively new to Fourier series. Let with . The Fourier coefficients of are given by the partial sums of the Fourier series of are given by and the Fourier series of is given by My question is: do the functions converge pointwise to ? It seems to me that this should both be true, but then I derive a contradiction. It is known that the converge to in the norm, and so a subsequence converges pointwise to . This implies that almost everywhere, but since pointwise, we get pointwise almost everywhere. But as far as I can tell, pointwise convergence of the Fourier series to the original function is a very difficult question. Is the ""almost everywhere"" what makes a difference? Did I make a mistake in my argument? Or do the just not converge pointwise to ?","f \in L^2([-\pi, \pi]) f(-\pi) = f(\pi) f \begin{align*}
\hat{f}(n) = \frac{1}{2\pi} \int_{-\pi}^{\pi} f(t) e^{-int} \, dt,
\end{align*} f \begin{align*}
s_N(t) = \sum \limits_{-N}^N \hat{f}(n) e^{int},
\end{align*} f \begin{align*}
F(t) = \sum_{-\infty}^\infty \hat{f}(n) e^{int}.
\end{align*} s_N F s_N f L^2 (s_{N_k})_k f f=F s_N \rightarrow F s_N \rightarrow f f s_N F","['analysis', 'fourier-analysis', 'hilbert-spaces', 'fourier-series', 'lp-spaces']"
6,Does $\int_{0}^{1} e^{-nx}d\alpha (x)=\int_{0}^{1} e^{-nx}d\beta (x)$ imply $\alpha (x) = \beta (x)$ in general?,Does  imply  in general?,\int_{0}^{1} e^{-nx}d\alpha (x)=\int_{0}^{1} e^{-nx}d\beta (x) \alpha (x) = \beta (x),"For $\alpha$ and $\beta$ be monotone non-decreasing continuous real-valued functions with $\alpha (0)=\beta (0)$ . The equation $\int_{0}^{1} e^{-nx}d\alpha (x)=\int_{0}^{1} e^{-nx}d\beta  (x)$ holds for any positive integer $n$ . Is $\alpha (x) = \beta (x)$ on $[0,1]$ true in general? I guess it's not so I'm trying to find a counterexample. I assume $\alpha$ and $\beta$ are differential then I get $\int_{0}^{1} e^{-nx}(\alpha '(x)-\beta '(x))dx=0$ , I think if we build $\alpha '$ and $\beta '$ wisely, we can make this equation hold(but maybe it can’t hold for every $n$ , I feel I’m not on the right way...). Anyway, I get further equation $e^{-n}(\alpha - \beta)+\int_{0}^{1} e^{-nx}(\alpha (x)-\beta(x))dx=0$ . Then I'm stucked here. Can anyone give a hint?","For and be monotone non-decreasing continuous real-valued functions with . The equation holds for any positive integer . Is on true in general? I guess it's not so I'm trying to find a counterexample. I assume and are differential then I get , I think if we build and wisely, we can make this equation hold(but maybe it can’t hold for every , I feel I’m not on the right way...). Anyway, I get further equation . Then I'm stucked here. Can anyone give a hint?","\alpha \beta \alpha (0)=\beta (0) \int_{0}^{1} e^{-nx}d\alpha (x)=\int_{0}^{1} e^{-nx}d\beta  (x) n \alpha (x) = \beta (x) [0,1] \alpha \beta \int_{0}^{1} e^{-nx}(\alpha '(x)-\beta '(x))dx=0 \alpha ' \beta ' n e^{-n}(\alpha - \beta)+\int_{0}^{1} e^{-nx}(\alpha (x)-\beta(x))dx=0","['real-analysis', 'functional-analysis', 'analysis']"
7,Differential geometry : crofton formula,Differential geometry : crofton formula,,"So I was reading an article, and it says that if a parametric curve $s(t)$ defined on I has values in a sphere $S^{2}$ , the length of $s$ , can be deduced by its intersection with great circles using the following formula: $$\operatorname{length}(s)=\frac{1}{4}\int_{S^{2}}\operatorname{card}\{t \in I/s(t).N=0\}\,\mathrm{d}N$$ Can someone give me an example where he applies the formula on a parametic curve so that I understand how it works and how to apply it.","So I was reading an article, and it says that if a parametric curve defined on I has values in a sphere , the length of , can be deduced by its intersection with great circles using the following formula: Can someone give me an example where he applies the formula on a parametic curve so that I understand how it works and how to apply it.","s(t) S^{2} s \operatorname{length}(s)=\frac{1}{4}\int_{S^{2}}\operatorname{card}\{t \in I/s(t).N=0\}\,\mathrm{d}N",['calculus']
8,Why is such function continuous?,Why is such function continuous?,,"Can anyone help me see why the function $f: \bar{\mathbb{Q}} \to \mathbb{Q} $ where $\bar{\mathbb{Q}}$ denotes the set of algebraic numbers, $$f(x)=\begin{cases}1,&\ \text{if the real part of $x$ is greater than $\pi$, and}\\ 0,&\ \text{otherwise.}\end{cases}$$ is continuous?","Can anyone help me see why the function where denotes the set of algebraic numbers, is continuous?","f: \bar{\mathbb{Q}} \to \mathbb{Q}  \bar{\mathbb{Q}} f(x)=\begin{cases}1,&\ \text{if the real part of x is greater than \pi, and}\\
0,&\ \text{otherwise.}\end{cases}","['general-topology', 'analysis']"
9,Courses closely following Rudin's Real and Complex Analysis,Courses closely following Rudin's Real and Complex Analysis,,"I am self-studying Rudin's Real and Complex Analysis, and for now, my goal is to work through the first two chapters - Abstract Integration and Positive Borel Measures. I am required by my supervisor (I am an undergraduate mathematics major, by the way) to learn from this book - but I find the exposition very terse, and sometimes it takes an hour to figure out the stuff on one page. I know that math demands time and effort, I am not shying away from that, I'm just looking for any course material online (lecture notes, slides, videos, problem sets, etc.) that could effectively supplement my reading of this text, and make my journey somewhat easier . Could you please help me out by suggesting any courses (in the form of videos, lecture notes/slides, problem sets, etc.) that could supplement my reading and make the job of understanding Rudin easier? If not, then it would be good to know of any other books that closely follow the content in Rudin but are easier to understand . Anyway, Rudin intended the book to be a text for graduate students - and as an undergraduate student, I guess I could use some help. Thanks a lot! To summarize the recommendations I've gotten so far: Gerald Folland's Real Analysis: Modern Techniques and Their Applications . Sheldon Axler's Measure, Integration & Real Analysis .","I am self-studying Rudin's Real and Complex Analysis, and for now, my goal is to work through the first two chapters - Abstract Integration and Positive Borel Measures. I am required by my supervisor (I am an undergraduate mathematics major, by the way) to learn from this book - but I find the exposition very terse, and sometimes it takes an hour to figure out the stuff on one page. I know that math demands time and effort, I am not shying away from that, I'm just looking for any course material online (lecture notes, slides, videos, problem sets, etc.) that could effectively supplement my reading of this text, and make my journey somewhat easier . Could you please help me out by suggesting any courses (in the form of videos, lecture notes/slides, problem sets, etc.) that could supplement my reading and make the job of understanding Rudin easier? If not, then it would be good to know of any other books that closely follow the content in Rudin but are easier to understand . Anyway, Rudin intended the book to be a text for graduate students - and as an undergraduate student, I guess I could use some help. Thanks a lot! To summarize the recommendations I've gotten so far: Gerald Folland's Real Analysis: Modern Techniques and Their Applications . Sheldon Axler's Measure, Integration & Real Analysis .",,"['analysis', 'reference-request', 'self-learning', 'book-recommendation']"
10,Using definition of limits,Using definition of limits,,"Let $c∈\mathbb{R}$ and let $f:\mathbb{R}\setminus{c}\rightarrow\mathbb{R}$ be a function such that $f(x)>0$ for all $x∈\mathbb{R}$ . Use the definition of limits to prove that $$ \lim_{x\to c}f(x)=\infty  \space\space\space\space\space\space\space\space\space\space\space   \text{iff}  \space\space\space\space\space\space\space\space\space\space\space    \lim_{x\to c}\frac{1}{f(x)}=0. $$ Proving the "" $\Rightarrow$ "": Here is the definition: $\lim_{x\to c}f(x)=\infty$ if $\forall M∈\mathbb{R},\exists\delta>0$ such that $\forall x∈\mathbb{R}, 0<|x-c|<\delta\Rightarrow f(x)>M$ . Here is my proof: Let $\epsilon >0$ and set $M=\frac{1}{\epsilon}$ . Since $\lim_{x\to c}f(x)=\infty$ , we can find a $\epsilon >0$ such that $f(x)>M$ whenever $0<|x-c|<\delta$ . Thus $0<\frac{1}{f(x)}<\frac{1}{\epsilon}$ whenever $0<|x-c|<\delta$ . This implies that it is possible to find a $\delta>0$ such that $|\frac{1}{f(x)}|<\epsilon$ whenever $0<|x-c|<\delta$ . Since $\epsilon$ is arbitrary, we have proved that $\lim_{x\to c}\frac{1}{f(x)}=0$ . Proving the "" $\Leftarrow$ "" : This proof I am unsure of. I know that by the definition of a limit, $\lim_{x\to c} f(x) = 0$ if $\forall\epsilon>0, \exists\delta>0$ such that $\forall x∈\mathbb{R}\setminus{c}, 0<|x-c|<\delta \Rightarrow |f(x)-0|<\epsilon$ . I am unsure of how to define $\lim_{x\to c}\frac{1}{f(x)}=0$ in a similar way. Any advice would be greatly appreciated.","Let and let be a function such that for all . Use the definition of limits to prove that Proving the "" "": Here is the definition: if such that . Here is my proof: Let and set . Since , we can find a such that whenever . Thus whenever . This implies that it is possible to find a such that whenever . Since is arbitrary, we have proved that . Proving the "" "" : This proof I am unsure of. I know that by the definition of a limit, if such that . I am unsure of how to define in a similar way. Any advice would be greatly appreciated.","c∈\mathbb{R} f:\mathbb{R}\setminus{c}\rightarrow\mathbb{R} f(x)>0 x∈\mathbb{R} 
\lim_{x\to c}f(x)=\infty  \space\space\space\space\space\space\space\space\space\space\space   \text{iff}  \space\space\space\space\space\space\space\space\space\space\space    \lim_{x\to c}\frac{1}{f(x)}=0.
 \Rightarrow \lim_{x\to c}f(x)=\infty \forall M∈\mathbb{R},\exists\delta>0 \forall x∈\mathbb{R}, 0<|x-c|<\delta\Rightarrow f(x)>M \epsilon >0 M=\frac{1}{\epsilon} \lim_{x\to c}f(x)=\infty \epsilon >0 f(x)>M 0<|x-c|<\delta 0<\frac{1}{f(x)}<\frac{1}{\epsilon} 0<|x-c|<\delta \delta>0 |\frac{1}{f(x)}|<\epsilon 0<|x-c|<\delta \epsilon \lim_{x\to c}\frac{1}{f(x)}=0 \Leftarrow \lim_{x\to c} f(x) = 0 \forall\epsilon>0, \exists\delta>0 \forall x∈\mathbb{R}\setminus{c}, 0<|x-c|<\delta \Rightarrow |f(x)-0|<\epsilon \lim_{x\to c}\frac{1}{f(x)}=0","['real-analysis', 'limits', 'analysis']"
11,convex hull of roots of a polynomial.,convex hull of roots of a polynomial.,,"This question deals with some consequences of the Gauss-Lucas theorem. Suppose that $f:\mathbb{C}\rightarrow \mathbb{C}$ is a polynomial. If I know that $f^{\prime}$ has precisely two positive real roots, for instance, what exactly can I deduce from the Gauss-Lucas theorem about the roots of $f$ ? The only conclusion I can draw is that the convex hull of the roots of $f$ must enclose a portion of the positive real axis. My wish is to be able to say something about the roots of $f$ - how many exist, and whether they are (real) positive as well. It would also be nice to be able to completely describe the convex hull of the roots of $f$ .","This question deals with some consequences of the Gauss-Lucas theorem. Suppose that is a polynomial. If I know that has precisely two positive real roots, for instance, what exactly can I deduce from the Gauss-Lucas theorem about the roots of ? The only conclusion I can draw is that the convex hull of the roots of must enclose a portion of the positive real axis. My wish is to be able to say something about the roots of - how many exist, and whether they are (real) positive as well. It would also be nice to be able to completely describe the convex hull of the roots of .",f:\mathbb{C}\rightarrow \mathbb{C} f^{\prime} f f f f,"['real-analysis', 'analysis', 'polynomials', 'complex-numbers']"
12,Reference request: Commutator estimate,Reference request: Commutator estimate,,"I am wondering if the following commutator estimate is true, and in such case, where can I find a proof for it. Let $N\in 2^{\mathbb{Z}}$ dyadic, and let's denote by $P_N$ the standard Littlewood-Paley projectors, that is, $P_N$ projects into frequencies $\{\vert\xi\vert\sim N\}$ . Finally, consider two functions in the Schwartz class $f,g\in \mathcal{S}(\mathbb{R})$ . Then, there exists $C>0$ such that $$ \big\Vert [P_N\partial_x,g]f\big\Vert_{L^2(\mathbb{R})}\leq C\Vert g_x\Vert_{L^\infty(\mathbb{R})}\Vert f\Vert_{L^2(\mathbb{R})}, $$ where $[\cdot,\cdot]$ stands for the commutator operator $[A,B]=AB-BA$ . Does anyone knows if such inequality holds and where can I find it?","I am wondering if the following commutator estimate is true, and in such case, where can I find a proof for it. Let dyadic, and let's denote by the standard Littlewood-Paley projectors, that is, projects into frequencies . Finally, consider two functions in the Schwartz class . Then, there exists such that where stands for the commutator operator . Does anyone knows if such inequality holds and where can I find it?","N\in 2^{\mathbb{Z}} P_N P_N \{\vert\xi\vert\sim N\} f,g\in \mathcal{S}(\mathbb{R}) C>0 
\big\Vert [P_N\partial_x,g]f\big\Vert_{L^2(\mathbb{R})}\leq C\Vert g_x\Vert_{L^\infty(\mathbb{R})}\Vert f\Vert_{L^2(\mathbb{R})},
 [\cdot,\cdot] [A,B]=AB-BA","['analysis', 'partial-differential-equations', 'reference-request', 'harmonic-analysis', 'littlewood-paley-theory']"
13,"Cauchy product rule to replace ""pi"" with a different infinite series in a particular ellipse","Cauchy product rule to replace ""pi"" with a different infinite series in a particular ellipse",,"The circumference of a circle has a simple formula $ c = 2 r \pi $ , but there is no simple formula for circumference of an ellipse with major and minor axes of a and b. There are approximations, including the not very close $ c=(a+b)\pi$ , but this infinite series $$ h = \frac{(a^2)-(b^2)}{(a^2)+(b^2)}$$ $$p=\pi (a+b)\sum_{i=0}^\infty \left(\frac{(2n-3)!!}{2^n n!}\right)^2 h^n$$ gave me an idea. In some ways you can think of the summation part as a correction factor to $\pi$ . Let's say that we pick a particular ellipse with a=5 and b=1. Then $\pi$ is a few percent too low to make $ c=(a+b)\pi$ work, so we need a different constant to multiply times (a+b). Can we make a new infinite series not containing the letter $\pi$ for this very specific ellipse? I know this is kind of a silly task since every specific shaped ellipse would need its own infinite series, but it would be useful for a talk I'm to give. My thought was to Cauchy product rule to multiply the summation part of the formula above with one of the infinite series for $\pi$ , but it turns out that is beyond my skills. Can someone help? Thank you in advance.","The circumference of a circle has a simple formula , but there is no simple formula for circumference of an ellipse with major and minor axes of a and b. There are approximations, including the not very close , but this infinite series gave me an idea. In some ways you can think of the summation part as a correction factor to . Let's say that we pick a particular ellipse with a=5 and b=1. Then is a few percent too low to make work, so we need a different constant to multiply times (a+b). Can we make a new infinite series not containing the letter for this very specific ellipse? I know this is kind of a silly task since every specific shaped ellipse would need its own infinite series, but it would be useful for a talk I'm to give. My thought was to Cauchy product rule to multiply the summation part of the formula above with one of the infinite series for , but it turns out that is beyond my skills. Can someone help? Thank you in advance.", c = 2 r \pi   c=(a+b)\pi  h = \frac{(a^2)-(b^2)}{(a^2)+(b^2)} p=\pi (a+b)\sum_{i=0}^\infty \left(\frac{(2n-3)!!}{2^n n!}\right)^2 h^n \pi \pi  c=(a+b)\pi \pi \pi,"['sequences-and-series', 'analysis', 'cauchy-sequences']"
14,Fourier Transform of $\frac{1}{1+|x|^\alpha}$,Fourier Transform of,\frac{1}{1+|x|^\alpha},"I'm trying to figure out either of these two things: $\int_w \frac{e^{iwt}}{1+|w|^\alpha}$ or $\sum_{n \in Z} \frac{e^{int}}{1+|n|^\alpha}$ , for $0<\alpha<1$ I think I know roughly what the asymptotics would look like for small t (UV): one can do a inverse fourier transform of $|w|^{-\alpha}$ and the answer would look like $\propto |t|^{\alpha-1}$ . Any help would be appreciated, thank you!","I'm trying to figure out either of these two things: or , for I think I know roughly what the asymptotics would look like for small t (UV): one can do a inverse fourier transform of and the answer would look like . Any help would be appreciated, thank you!",\int_w \frac{e^{iwt}}{1+|w|^\alpha} \sum_{n \in Z} \frac{e^{int}}{1+|n|^\alpha} 0<\alpha<1 |w|^{-\alpha} \propto |t|^{\alpha-1},"['calculus', 'complex-analysis', 'analysis', 'fourier-analysis']"
15,Doubt with improper integral,Doubt with improper integral,,"I have to study the convergence of the improper integral $$\int_0^1 \frac{\sin(\sqrt{x})}{x(x-1)(\sqrt{x+2})}dx$$ So I split it $$\int_0^a \frac{\sin(\sqrt{x})}{x(x-1)(\sqrt{x+2})}dx + \int_a^1 \frac{\sin(\sqrt{x})}{x(x-1)(\sqrt{x+2})}dx$$ the first is asymptotic to $$\frac{\sqrt{x}}{x}=\frac{1}{\sqrt{x}} $$ that converges. The second is asymptotic to $$\frac{\sin{1}}{(x-1)(\sqrt{2})} $$ that diverges. So the integral diverges, is that correct?","I have to study the convergence of the improper integral So I split it the first is asymptotic to that converges. The second is asymptotic to that diverges. So the integral diverges, is that correct?",\int_0^1 \frac{\sin(\sqrt{x})}{x(x-1)(\sqrt{x+2})}dx \int_0^a \frac{\sin(\sqrt{x})}{x(x-1)(\sqrt{x+2})}dx + \int_a^1 \frac{\sin(\sqrt{x})}{x(x-1)(\sqrt{x+2})}dx \frac{\sqrt{x}}{x}=\frac{1}{\sqrt{x}}  \frac{\sin{1}}{(x-1)(\sqrt{2})} ,"['analysis', 'improper-integrals']"
16,Can any function of two variables be expressed as a linear combination?,Can any function of two variables be expressed as a linear combination?,,"Can any function of two variables be expressed as a linear combination (may be infinite) of a product of two single-variable functions? $f(x,y) = \sum A_n g_n(x) h_n(y)$",Can any function of two variables be expressed as a linear combination (may be infinite) of a product of two single-variable functions?,"f(x,y) = \sum A_n g_n(x) h_n(y)",['analysis']
17,Orthogonal projection onto a subvector space of $L^2(\mathbb{R})$,Orthogonal projection onto a subvector space of,L^2(\mathbb{R}),"Consider the subvectorspace $K\subset L^2(\mathbb{R})$ defined as $$K=\left\{f\in L^2(\mathbb{R}) | \forall n\in\mathbb{Z}:\int_n^{n+1}f(x)=0\right\}.$$ I'm trying to find the orthogonal projection $p_K:L^2(\mathbb{R})\rightarrow K$ . I know I can find $p_K$ by finding an orthonormal basis $(e_k)_{k\in\mathbb{Z}}$ , for $K$ . Since $K$ is closed we hopefully can conclude that $K=\text{closure(span}\{e_k|k\in\mathbb{Z}\})$ (I will not state a proof here). Then, for every $f\in L^2(\mathbb{R})$ , we would have $$p_K(f)=\sum_{k\in\mathbb{Z}}\langle f,e_k\rangle e_k.$$ I found an orthonormal set $(e_k)_{k\in\mathbb{Z}}$ where $e_k:\mathbb{R}\rightarrow\mathbb{C}:x\mapsto e^{2\pi ixk}\chi_{[k,k+1]}(x)$ . It is easy to check that this set is indeed orthonormal. I now claim that this set is maximal orthonormal in $K$ , meaning that if for $f\in K$ we have that $\langle f,e_k\rangle=0$ for all $k\in\mathbb{Z}$ , then $f=0$ . Take an $f\in K$ such that $\langle f,e_k\rangle=0$ for all $k\in\mathbb{Z}$ and consider the inner product: $$\langle f,e_k\rangle = \int_\mathbb{R}f(x)e^{-2\pi ikx}\chi_{[k,k+1]}(x)dx=0.$$ Define the functions $h_k:\mathbb{R}\rightarrow\mathbb{C}:x\mapsto f(x)\chi_{[k,k+1]}(x)$ . We then get $\langle f,e_k\rangle=\widehat{h_k}(k)=0$ for all $k\in\mathbb{Z}$ , where $\widehat{h_k}$ denotes the Fouriertransform of $h_k$ . Since the Fouriertransform of $h_k$ is always $0$ we find that $h_k$ is zero almost everywhere. Now $f=\sum_{k\in\mathbb{Z}h_k}$ so $f$ is zero almost everywhere and thus $f=0$ in $L^2(\mathbb{R})$ . So $\text{span}\{e_k|k\in\mathbb{Z}\}$ is dense in $K$ meaning for every $f\in L^2(\mathbb{R})$ the orthogonal projection onto $K$ is given by $$p_K(f)=\sum_{k\in\mathbb{Z}}\langle f,e_k\rangle e_k,$$ where $e_k(x)=e^{2\pi ikx}\chi_{[k,k+1]}(x)$ and $\langle f,e_k\rangle=\int_k^{k+1}f(x)e^{-2\pi ikx}dx$ . Is this reasoning correct?","Consider the subvectorspace defined as I'm trying to find the orthogonal projection . I know I can find by finding an orthonormal basis , for . Since is closed we hopefully can conclude that (I will not state a proof here). Then, for every , we would have I found an orthonormal set where . It is easy to check that this set is indeed orthonormal. I now claim that this set is maximal orthonormal in , meaning that if for we have that for all , then . Take an such that for all and consider the inner product: Define the functions . We then get for all , where denotes the Fouriertransform of . Since the Fouriertransform of is always we find that is zero almost everywhere. Now so is zero almost everywhere and thus in . So is dense in meaning for every the orthogonal projection onto is given by where and . Is this reasoning correct?","K\subset L^2(\mathbb{R}) K=\left\{f\in L^2(\mathbb{R}) | \forall n\in\mathbb{Z}:\int_n^{n+1}f(x)=0\right\}. p_K:L^2(\mathbb{R})\rightarrow K p_K (e_k)_{k\in\mathbb{Z}} K K K=\text{closure(span}\{e_k|k\in\mathbb{Z}\}) f\in L^2(\mathbb{R}) p_K(f)=\sum_{k\in\mathbb{Z}}\langle f,e_k\rangle e_k. (e_k)_{k\in\mathbb{Z}} e_k:\mathbb{R}\rightarrow\mathbb{C}:x\mapsto e^{2\pi ixk}\chi_{[k,k+1]}(x) K f\in K \langle f,e_k\rangle=0 k\in\mathbb{Z} f=0 f\in K \langle f,e_k\rangle=0 k\in\mathbb{Z} \langle f,e_k\rangle = \int_\mathbb{R}f(x)e^{-2\pi ikx}\chi_{[k,k+1]}(x)dx=0. h_k:\mathbb{R}\rightarrow\mathbb{C}:x\mapsto f(x)\chi_{[k,k+1]}(x) \langle f,e_k\rangle=\widehat{h_k}(k)=0 k\in\mathbb{Z} \widehat{h_k} h_k h_k 0 h_k f=\sum_{k\in\mathbb{Z}h_k} f f=0 L^2(\mathbb{R}) \text{span}\{e_k|k\in\mathbb{Z}\} K f\in L^2(\mathbb{R}) K p_K(f)=\sum_{k\in\mathbb{Z}}\langle f,e_k\rangle e_k, e_k(x)=e^{2\pi ikx}\chi_{[k,k+1]}(x) \langle f,e_k\rangle=\int_k^{k+1}f(x)e^{-2\pi ikx}dx","['analysis', 'vector-spaces', 'solution-verification', 'hilbert-spaces', 'orthogonality']"
18,Surface in $\mathbb{R}^3$ implicitely defined by a $C^1$-function is locally similar to $xy$-plane,Surface in  implicitely defined by a -function is locally similar to -plane,\mathbb{R}^3 C^1 xy,"Let $F:\mathbb{R}^3\to \mathbb{R}$ be a $C^1$ -function and suppose that $(dF)(x,y,z)\not=0$ wherever $F(x,y,z)=0$ . Call $$O = \{(x,y,z)\mid F(x,y,z)=0\}.$$ Then, for every point $p\in O$ , there is an open neighborhood $V\subseteq \mathbb{R}^3$ of $p$ and an open neighborhood $U\subseteq\mathbb{R}^3$ of $(0,0,0)$ and a bijective $C^1$ -map $\varphi:U\to V$ with a $C^1$ -inverse such that $\varphi(U\cap(\mathbb{R}^2\times\{0\}))=V\cap O.$ Proof: Choose an arbitrary point $p\in O$ . We first note that the rank of $(dF)(p)$ is $1$ , since $(dF)(p)$ is a $1\times 3$ non-zero matrix. Hence, since it only has one row, it has full rank i.e it has rank $1$ . Now it is possible to parameterize $O$ with a $C^1$ -map $$g:U_0\subseteq \mathbb{R}^2\to \mathbb{R}^3$$ such that $g(0,0) = p$ , where $U_0\subseteq \mathbb{R}^2$ is an open neighborhood of $(0,0)$ and $V_0\subseteq \mathbb{R}^3$ is an open neighborhood of $p$ and $g(U_0)=V_0\cap O.$ Since $F$ is $C^1$ , so too is $g$ . We now define the map $$\tilde{\varphi}:U_0\times\mathbb{R}\to\mathbb{R}^3:(x,y,z)\mapsto g(x,y)+(z,z,z).$$ It is easily verified that for all $(x,y,z)\in U_0\times \mathbb{R}$ , the total derivative $(d\tilde{\varphi})(x,y,z)$ is invertible and that $\tilde{\varphi}$ is a $C^1$ -map. The inverse function theorem now tells us that there are open neighborhoods $U\subseteq U_0\times\mathbb{R}$ of $(0,0,0)$ and $V\subseteq \mathbb{R}^3$ of $p=\tilde{\varphi}(0,0,0)$ such that $\tilde{\varphi}\mid_U:U\to V$ is $C^1$ -bijection with a $C^1$ -inverse. Call this restriction $\varphi$ . Then it is easily seen that $\varphi(U\cap(\mathbb{R}^2\times\{0\}))=V\cap O$ . Could someone verify if this proof is correct?","Let be a -function and suppose that wherever . Call Then, for every point , there is an open neighborhood of and an open neighborhood of and a bijective -map with a -inverse such that Proof: Choose an arbitrary point . We first note that the rank of is , since is a non-zero matrix. Hence, since it only has one row, it has full rank i.e it has rank . Now it is possible to parameterize with a -map such that , where is an open neighborhood of and is an open neighborhood of and Since is , so too is . We now define the map It is easily verified that for all , the total derivative is invertible and that is a -map. The inverse function theorem now tells us that there are open neighborhoods of and of such that is -bijection with a -inverse. Call this restriction . Then it is easily seen that . Could someone verify if this proof is correct?","F:\mathbb{R}^3\to \mathbb{R} C^1 (dF)(x,y,z)\not=0 F(x,y,z)=0 O = \{(x,y,z)\mid F(x,y,z)=0\}. p\in O V\subseteq \mathbb{R}^3 p U\subseteq\mathbb{R}^3 (0,0,0) C^1 \varphi:U\to V C^1 \varphi(U\cap(\mathbb{R}^2\times\{0\}))=V\cap O. p\in O (dF)(p) 1 (dF)(p) 1\times 3 1 O C^1 g:U_0\subseteq \mathbb{R}^2\to \mathbb{R}^3 g(0,0) = p U_0\subseteq \mathbb{R}^2 (0,0) V_0\subseteq \mathbb{R}^3 p g(U_0)=V_0\cap O. F C^1 g \tilde{\varphi}:U_0\times\mathbb{R}\to\mathbb{R}^3:(x,y,z)\mapsto g(x,y)+(z,z,z). (x,y,z)\in U_0\times \mathbb{R} (d\tilde{\varphi})(x,y,z) \tilde{\varphi} C^1 U\subseteq U_0\times\mathbb{R} (0,0,0) V\subseteq \mathbb{R}^3 p=\tilde{\varphi}(0,0,0) \tilde{\varphi}\mid_U:U\to V C^1 C^1 \varphi \varphi(U\cap(\mathbb{R}^2\times\{0\}))=V\cap O","['analysis', 'differential-geometry', 'solution-verification', 'inverse-function-theorem']"
19,"Convergence of $\overset{\infty}{\underset{n=1}{\sum}}\dfrac{\log(1+e^{\alpha n})}{1+n^{\alpha}}(x-e^{\alpha})^n$ in $(2,3)$",Convergence of  in,"\overset{\infty}{\underset{n=1}{\sum}}\dfrac{\log(1+e^{\alpha n})}{1+n^{\alpha}}(x-e^{\alpha})^n (2,3)","I have to study for which values $\alpha\in\mathbb R$ , the following sum converges $\forall x\in(2,3)$ : $$\overset{\infty}{\underset{n=1}{\sum}}\dfrac{\log(1+e^{\alpha n})}{1+n^{\alpha}}(x-e^{\alpha})^n.$$ I made a substitution $u=x-e^{\alpha }$ and I've studied the succession $a_n:=\dfrac{\log(1+e^{\alpha n})}{1+n^{\alpha}}$ in order to calculate the radius of convergence of the series $\overset{\infty}{\underset{n=1}{\sum}a_nu^n}$ . Omitting the symbol of limit, I found $\bigg(\dfrac{\log(1+e^{\alpha n})}{1+n^{\alpha}}\bigg)^{\frac{1}{n}}=e^{\log\Big({\Big(\frac{\log(1+e^{\alpha n})}{1+n^{\alpha}}\Big)^{\frac{1}{n}}}\Big)}=e^{\frac{\log(\log(1+e^{\alpha n}))-\log(1+n^{\alpha})}{n}}=e^{\frac{\log(\log(1+e^{\alpha n}))}{n}-\frac{\log(1+n^{\alpha})}{n}}$ . Now I think that, if $\alpha$ is positive, the order of infinity of $n$ is greater than the order of infinity of the $\log$ term. I tried to study the case $\alpha <0$ and I wrote the Taylor's polynomial for $\log(1+e^{\alpha n})$ and $\log(1+n^{\alpha})$ , since $n^{\alpha},e^{\alpha n}\underset{n\to\infty}{\longrightarrow}0$ . The problem is that I don't manage to find a useful condition for the parameter. Furthermore, studying $2<u<3$ , I got the condition $\alpha\in[\log 2,\log3]$ but I can't find other conditions (I know there are other conditions because I know the risult)... Thank you in advance.","I have to study for which values , the following sum converges : I made a substitution and I've studied the succession in order to calculate the radius of convergence of the series . Omitting the symbol of limit, I found . Now I think that, if is positive, the order of infinity of is greater than the order of infinity of the term. I tried to study the case and I wrote the Taylor's polynomial for and , since . The problem is that I don't manage to find a useful condition for the parameter. Furthermore, studying , I got the condition but I can't find other conditions (I know there are other conditions because I know the risult)... Thank you in advance.","\alpha\in\mathbb R \forall x\in(2,3) \overset{\infty}{\underset{n=1}{\sum}}\dfrac{\log(1+e^{\alpha n})}{1+n^{\alpha}}(x-e^{\alpha})^n. u=x-e^{\alpha } a_n:=\dfrac{\log(1+e^{\alpha n})}{1+n^{\alpha}} \overset{\infty}{\underset{n=1}{\sum}a_nu^n} \bigg(\dfrac{\log(1+e^{\alpha n})}{1+n^{\alpha}}\bigg)^{\frac{1}{n}}=e^{\log\Big({\Big(\frac{\log(1+e^{\alpha n})}{1+n^{\alpha}}\Big)^{\frac{1}{n}}}\Big)}=e^{\frac{\log(\log(1+e^{\alpha n}))-\log(1+n^{\alpha})}{n}}=e^{\frac{\log(\log(1+e^{\alpha n}))}{n}-\frac{\log(1+n^{\alpha})}{n}} \alpha n \log \alpha <0 \log(1+e^{\alpha n}) \log(1+n^{\alpha}) n^{\alpha},e^{\alpha n}\underset{n\to\infty}{\longrightarrow}0 2<u<3 \alpha\in[\log 2,\log3]","['analysis', 'power-series', 'uniform-convergence']"
20,counting cycles and paths in simple graphs,counting cycles and paths in simple graphs,,I'm newly studying graphs in my school courses and I've been facing this common type of questions about how many cycles/paths are there between some vertices I asked my teacher for a simpler or more algorithmic way to count them but he said except for Kn graphs. so I started to search for such an algorithms and I found out there are such algorithms but I have no idea how to use them(as I said I'm a high-school student). these are the papers I took a look at: https://www.sciencedirect.com/science/article/abs/pii/0020019094001510 http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.156.345 so I'm seeking a way to solve such problems more efficiently and with less probability of making ridiculous mistakes. any help is appreciated,I'm newly studying graphs in my school courses and I've been facing this common type of questions about how many cycles/paths are there between some vertices I asked my teacher for a simpler or more algorithmic way to count them but he said except for Kn graphs. so I started to search for such an algorithms and I found out there are such algorithms but I have no idea how to use them(as I said I'm a high-school student). these are the papers I took a look at: https://www.sciencedirect.com/science/article/abs/pii/0020019094001510 http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.156.345 so I'm seeking a way to solve such problems more efficiently and with less probability of making ridiculous mistakes. any help is appreciated,,"['combinatorics', 'analysis', 'graph-theory', 'algorithms', 'planar-graphs']"
21,Least concave majorant of a non-decreasing function,Least concave majorant of a non-decreasing function,,"Consider a non-decreasing discrete function $f$ defined over $\{0,1,\ldots,n\}$ . Let $g$ be the least concave majorant of $f$ . Assume that for some $i\in \{1,\ldots,n-1\}$ , $g(i)\neq f(i)$ . How to prove that $g$ is linear over $\{i-1, i ,i+1 \}$ , i.e., $g(i)-g(i-1) = g(i+1)-g(i)$ ? I tried the following: Suppose that $g(i)-g(i-1) \neq g(i+1)-g(i)$ , then we must have that $g(i)$ is above the line that interpolates $g(i-1)$ and $g(i+1)$ by concavity of $g$ . Then, I think that one might be able to find a contradiction with the fact that $g$ is the least concave majorant but cannot formalize it.","Consider a non-decreasing discrete function defined over . Let be the least concave majorant of . Assume that for some , . How to prove that is linear over , i.e., ? I tried the following: Suppose that , then we must have that is above the line that interpolates and by concavity of . Then, I think that one might be able to find a contradiction with the fact that is the least concave majorant but cannot formalize it.","f \{0,1,\ldots,n\} g f i\in \{1,\ldots,n-1\} g(i)\neq f(i) g \{i-1, i ,i+1 \} g(i)-g(i-1) = g(i+1)-g(i) g(i)-g(i-1) \neq g(i+1)-g(i) g(i) g(i-1) g(i+1) g g","['real-analysis', 'analysis', 'convex-analysis']"
22,Does $x^{x} = \sum\limits_{n=0}^{\infty}\frac{x^n \ln(x)^n}{n!}$ converges uniformly for $x = 0$?,Does  converges uniformly for ?,x^{x} = \sum\limits_{n=0}^{\infty}\frac{x^n \ln(x)^n}{n!} x = 0,"For this exercise we can use that $e^x = \sum\limits_{n=0}^{\infty}\frac{x^n}{n!}$ converges uniformly in all limited subset of $\mathbb{R}$ and have to show that $x^{x} = \sum\limits_{n=0}^{\infty}\frac{x^n \ln(x)^n}{n!}$ converges uniformly for all $x \in [0,1]$ . I can show that it converges uniformly for all $x \in (0,1)$ by Dirichlet Test and $x=1$ by particular case, but I don't know if it is really true for $x = 0$ . Is it possible to converge uniformly even having $ln(0)$ not existing? (As for $0^0$ , we have a note saying to consider that as $1$ ). Also, the next question is to show that $\int _0^1 \! x^x \, \mathrm{d}x = \sum\limits_{n=0}^{\infty} \frac{1}{n!}\int _0^1\! {x^n\ln(x)^n} \, \mathrm{d}x$ . This would be easy if I had uniform convergence for $x = 0$ , because I could use a Theorem that says that I can interchange sum and integral when convergence is uniform for all $x$ in integral limits.","For this exercise we can use that converges uniformly in all limited subset of and have to show that converges uniformly for all . I can show that it converges uniformly for all by Dirichlet Test and by particular case, but I don't know if it is really true for . Is it possible to converge uniformly even having not existing? (As for , we have a note saying to consider that as ). Also, the next question is to show that . This would be easy if I had uniform convergence for , because I could use a Theorem that says that I can interchange sum and integral when convergence is uniform for all in integral limits.","e^x = \sum\limits_{n=0}^{\infty}\frac{x^n}{n!} \mathbb{R} x^{x} = \sum\limits_{n=0}^{\infty}\frac{x^n \ln(x)^n}{n!} x \in [0,1] x \in (0,1) x=1 x = 0 ln(0) 0^0 1 \int _0^1 \! x^x \, \mathrm{d}x = \sum\limits_{n=0}^{\infty} \frac{1}{n!}\int _0^1\! {x^n\ln(x)^n} \, \mathrm{d}x x = 0 x","['real-analysis', 'calculus', 'sequences-and-series', 'analysis', 'uniform-convergence']"
23,"Sequence space s (Functional analysis, Kreyszig)","Sequence space s (Functional analysis, Kreyszig)",,"I'm reading Kreyszig's functional analysis book, in which are the following example: $\textbf{1.2-1 Sequence space s.}$ This space consists of the set of all (bounded or unbounded) sequences of complex numbers and the metric $d$ defined by $$ {d(x,y) = \sum_{j=1}^{\infty} \frac{1}{2^j}\frac{|\epsilon_{j}-\eta_{j}|}{1 + |\epsilon_{j}-\eta_{j}|}}, $$ where $x = (\epsilon_{j})$ and $y = (\eta_{j})$ . Could you give me more references about this sequence space (I didn't find any on the Internet by this name), or about metric spaces like this (namely, defined by series)? Note: Much more, better, because I'm a beginner in functional analysis.","I'm reading Kreyszig's functional analysis book, in which are the following example: This space consists of the set of all (bounded or unbounded) sequences of complex numbers and the metric defined by where and . Could you give me more references about this sequence space (I didn't find any on the Internet by this name), or about metric spaces like this (namely, defined by series)? Note: Much more, better, because I'm a beginner in functional analysis.","\textbf{1.2-1 Sequence space s.} d 
{d(x,y) = \sum_{j=1}^{\infty} \frac{1}{2^j}\frac{|\epsilon_{j}-\eta_{j}|}{1 + |\epsilon_{j}-\eta_{j}|}},
 x = (\epsilon_{j}) y = (\eta_{j})","['sequences-and-series', 'functional-analysis', 'analysis', 'reference-request']"
24,Partition function for a general Ising model as Gaussian integral,Partition function for a general Ising model as Gaussian integral,,"Consider the Hamiltonian of Ising model $$H= - \sum_{x,y} J_{x,y}\sigma_x \sigma_y -  \sum_x h\sigma_x$$ where $\sigma_x$ can be $1$ or $-1$ and the number of sites is finite ( i.e. there are a finite number of $x,y$ ) and where $x,y$ are points of the a $d$ -lattice with side $L$ (i.e. $x,y \in \mathbb{Z} \cap[-L/2,L/2].$ So the partiction function is $$Z=\sum_{\sigma}e^{-\beta H}=\sum_{\sigma}e^{\beta\sum_{x,y} J_{x,y}\sigma_x \sigma_y} e^{\sum_x h\sigma_x}$$ where the sum is over all the possible choice of $\sigma=\pm 1$ . The goal is to write the partiction function as a Gaussian integral using the following formula $$\int e^{-\frac{1}{2}\sum\limits_{i,j=1}^{n}A_{ij} x_i x_j+\sum\limits_{i=1}^{n}B_i x_i} d^nx=\int e^{-\frac{1}{2}\vec{x}^T \mathbf{A} \vec{x}+\vec{B}^T \vec{x}} d^nx= \sqrt{ \frac{(2\pi)^n}{\det{A}} }e^{\frac{1}{2}\vec{B}^{T}\mathbf{A}^{-1}\vec{B}}$$ in view of the fact that $e^{\beta\sum_{x,y} J_{x,y}\sigma_x \sigma_y}$ has the form $e^{\frac{1}{2}\vec{B}^{T}\mathbf{A}^{-1}\vec{B}}$ ( thinking $\sigma=(\sigma_{x_1},...)$ as a vector with $L^d$ components and $J=(J_{xy})$ as a matrix $L^d\times L^d$ ) . Now, what i read on the notes is that $$e^{\beta\sum_{x,y} J_{x,y}\sigma_x \sigma_y}= (2\pi)^ {L^d}(detJ)^{-1/2}\int_{-\infty}^{+\infty}\prod_{x} d \varphi_x e^{-\frac{1}{2\beta}\sum_{x,y}\varphi_x J^{-1}_{xy}\varphi_y + \sum_{x}\varphi_x \sigma_x}.$$ To verify the last formula I tried to calculate the integral in the right head side using the classical techniques for the Gaussian integral ( completion of the square), but I didn't found the last formula. So my question is: is there something wrong with the last expression for $e^{\beta\sum_{x,y} J_{x,y}\sigma_x \sigma_y}$ as Gaussian integral?","Consider the Hamiltonian of Ising model where can be or and the number of sites is finite ( i.e. there are a finite number of ) and where are points of the a -lattice with side (i.e. So the partiction function is where the sum is over all the possible choice of . The goal is to write the partiction function as a Gaussian integral using the following formula in view of the fact that has the form ( thinking as a vector with components and as a matrix ) . Now, what i read on the notes is that To verify the last formula I tried to calculate the integral in the right head side using the classical techniques for the Gaussian integral ( completion of the square), but I didn't found the last formula. So my question is: is there something wrong with the last expression for as Gaussian integral?","H= - \sum_{x,y} J_{x,y}\sigma_x \sigma_y -  \sum_x h\sigma_x \sigma_x 1 -1 x,y x,y d L x,y \in \mathbb{Z} \cap[-L/2,L/2]. Z=\sum_{\sigma}e^{-\beta H}=\sum_{\sigma}e^{\beta\sum_{x,y} J_{x,y}\sigma_x \sigma_y} e^{\sum_x h\sigma_x} \sigma=\pm 1 \int e^{-\frac{1}{2}\sum\limits_{i,j=1}^{n}A_{ij} x_i x_j+\sum\limits_{i=1}^{n}B_i x_i} d^nx=\int e^{-\frac{1}{2}\vec{x}^T \mathbf{A} \vec{x}+\vec{B}^T \vec{x}} d^nx= \sqrt{ \frac{(2\pi)^n}{\det{A}} }e^{\frac{1}{2}\vec{B}^{T}\mathbf{A}^{-1}\vec{B}} e^{\beta\sum_{x,y} J_{x,y}\sigma_x \sigma_y} e^{\frac{1}{2}\vec{B}^{T}\mathbf{A}^{-1}\vec{B}} \sigma=(\sigma_{x_1},...) L^d J=(J_{xy}) L^d\times L^d e^{\beta\sum_{x,y} J_{x,y}\sigma_x \sigma_y}= (2\pi)^ {L^d}(detJ)^{-1/2}\int_{-\infty}^{+\infty}\prod_{x} d \varphi_x e^{-\frac{1}{2\beta}\sum_{x,y}\varphi_x J^{-1}_{xy}\varphi_y + \sum_{x}\varphi_x \sigma_x}. e^{\beta\sum_{x,y} J_{x,y}\sigma_x \sigma_y}","['analysis', 'dynamical-systems', 'quantum-mechanics', 'gaussian-integral', 'statistical-mechanics']"
25,$\bigcap_{n\in\mathbb{N}}{F_{n}}$ is dense in $X_{0}$,is dense in,\bigcap_{n\in\mathbb{N}}{F_{n}} X_{0},"Let $\{(X_{n},d_{n})\}_{n\in\mathbb{N}}$ be a sequence of complete metric spaces. If $\{f_{n}\colon X_{n}\to X_{n-1}\}_{n\in\mathbb{N}}$ is a sequence of functions continuous such that \begin{equation} \overline{f_{n}(X_{n})}=X_{n-1},\,\forall\,n\in\mathbb{N} \end{equation} show that $\overline{\bigcap_{n\in\mathbb{N}}{(f_{1}\circ\cdots\circ f_{n})(X_{n})}}=X_{0}.$ My attempt: Let $F_{n}=(f_{1}\circ\cdots\circ f_{n})(X_{n})$ be. Let's see what \begin{equation*}     \overline{\textstyle\bigcap_{n\in\mathbb{N}}{F_{n}}}=X_{0}.\tag{*} \end{equation*} Note that \begin{align*}     F_{n+1}&=(f_{1}\circ\cdots\circ f_{n}\circ f_{n+1})(X_{n+1})\\     &=(f_{1}\circ\cdots\circ f_{n})( f_{n+1}(X_{n+1}))\\     &\subseteq{(f_{1}\circ\cdots\circ f_{n})( \overline{f_{n+1}(X_{n+1})})}\\     &=(f_{1}\circ\cdots\circ f_{n})( X_{n})\\     &=F_{n}. \end{align*} \begin{align*}     F_{n}&=(f_{1}\circ\cdots\circ f_{n})( X_{n})\\     &=(f_{1}\circ\cdots\circ f_{n})( \overline{f_{n+1}(X_{n+1})})\\     &\subseteq{\overline{(f_{1}\circ\cdots\circ f_{n})(f_{n+1}(X_{n+1}))}}\\     &=\overline{(f_{1}\circ\cdots\circ f_{n+1})( X_{n+1})}\\     &=\overline{F_{n+1}}. \end{align*} Therefore, we have \begin{equation}     F_{n+1}\subseteq{F_{n}}\subseteq{X_{0}}\subseteq{\overline{F_{n}}=\overline{F_{n+1}}},\,\forall\, n\in\mathbb{N}.\tag{1} \end{equation} By $ (1), $ it follows that \begin{equation*}     \overline{\textstyle\bigcap_{n\in\mathbb{N}}{F_{n}}}\subseteq{X_{0}}. \end{equation*} Let $x\in X_{0}.$ By $(1),$ exists a sequence $(F_{n}(x_{n}))_{n\in\mathbb{N}}\subseteq{X_{0}}$ such that $d_{0}(x,F_{n}(x_{n}))<1/n.$ Now a question arises: is the sequence $ (F_{n} (x_{n})) _{n \in \mathbb {N}}$ a Cauchy sequence at $X_{0} $ ? If that's true, then it must converge because $ X_{0} $ is a complete metric space. Let's say $ F_{n} (X_{n}) \to y \in X_{0}. $ Like $ d_{0} (x, F_{n} (x_{n})) <1 / n, $ then $ x = y$ and it follows that $ x \in \overline {\bigcap_ {n \in \mathbb {N}} {F_ {n}}}. $ I have doubts with the previous argument. I don't know if that's enough to justify that $ X_{0} \subseteq {\overline{\textstyle\bigcap_{n\in\mathbb{N}}{F_{n}}}}. $ Another question: If for every $ n \in \mathbb {N}, $ exists a open set $ U_{n} \subseteq {X_{n}} $ such that $ \overline {f_{n} ({U_{n}})} = X_{n-1}, $ then is the previous result still valid? Hope you can help me with some suggestions please.","Let be a sequence of complete metric spaces. If is a sequence of functions continuous such that show that My attempt: Let be. Let's see what Note that Therefore, we have By it follows that Let By exists a sequence such that Now a question arises: is the sequence a Cauchy sequence at ? If that's true, then it must converge because is a complete metric space. Let's say Like then and it follows that I have doubts with the previous argument. I don't know if that's enough to justify that Another question: If for every exists a open set such that then is the previous result still valid? Hope you can help me with some suggestions please.","\{(X_{n},d_{n})\}_{n\in\mathbb{N}} \{f_{n}\colon X_{n}\to X_{n-1}\}_{n\in\mathbb{N}} \begin{equation}
\overline{f_{n}(X_{n})}=X_{n-1},\,\forall\,n\in\mathbb{N}
\end{equation} \overline{\bigcap_{n\in\mathbb{N}}{(f_{1}\circ\cdots\circ f_{n})(X_{n})}}=X_{0}. F_{n}=(f_{1}\circ\cdots\circ f_{n})(X_{n}) \begin{equation*}
    \overline{\textstyle\bigcap_{n\in\mathbb{N}}{F_{n}}}=X_{0}.\tag{*}
\end{equation*} \begin{align*}
    F_{n+1}&=(f_{1}\circ\cdots\circ f_{n}\circ f_{n+1})(X_{n+1})\\
    &=(f_{1}\circ\cdots\circ f_{n})( f_{n+1}(X_{n+1}))\\
    &\subseteq{(f_{1}\circ\cdots\circ f_{n})( \overline{f_{n+1}(X_{n+1})})}\\
    &=(f_{1}\circ\cdots\circ f_{n})( X_{n})\\
    &=F_{n}.
\end{align*} \begin{align*}
    F_{n}&=(f_{1}\circ\cdots\circ f_{n})( X_{n})\\
    &=(f_{1}\circ\cdots\circ f_{n})( \overline{f_{n+1}(X_{n+1})})\\
    &\subseteq{\overline{(f_{1}\circ\cdots\circ f_{n})(f_{n+1}(X_{n+1}))}}\\
    &=\overline{(f_{1}\circ\cdots\circ f_{n+1})( X_{n+1})}\\
    &=\overline{F_{n+1}}.
\end{align*} \begin{equation}
    F_{n+1}\subseteq{F_{n}}\subseteq{X_{0}}\subseteq{\overline{F_{n}}=\overline{F_{n+1}}},\,\forall\, n\in\mathbb{N}.\tag{1}
\end{equation}  (1),  \begin{equation*}
    \overline{\textstyle\bigcap_{n\in\mathbb{N}}{F_{n}}}\subseteq{X_{0}}.
\end{equation*} x\in X_{0}. (1), (F_{n}(x_{n}))_{n\in\mathbb{N}}\subseteq{X_{0}} d_{0}(x,F_{n}(x_{n}))<1/n.  (F_{n} (x_{n})) _{n \in \mathbb {N}} X_{0}   X_{0}   F_{n} (X_{n}) \to y \in X_{0}.   d_{0} (x, F_{n} (x_{n})) <1 / n,   x = y  x \in \overline {\bigcap_ {n \in \mathbb {N}} {F_ {n}}}.   X_{0} \subseteq {\overline{\textstyle\bigcap_{n\in\mathbb{N}}{F_{n}}}}.   n \in \mathbb {N},   U_{n} \subseteq {X_{n}}   \overline {f_{n} ({U_{n}})} = X_{n-1}, ","['real-analysis', 'general-topology', 'analysis', 'the-baire-space']"
26,When does convolution create turning points?,When does convolution create turning points?,,"Suppose $f$ and $k$ are Lebesgue measurable functions $\mathbf R\to\mathbf R$ and satisfy $k(x)\geq 0$ for all $x$ , and $\int_{\mathbf R} k(x)\,dx=1$ . Also suppose $f$ has bounded variation, and $f(-\infty)=0$ and $f(\infty)=1$ . Let $$(k*f)(x) = \int_{-\infty}^{\infty}k(x-y)f(y)dy$$ denote the convolution of $k$ and $f$ . Let $TP(f)$ denote the number of turning points of a measurable function. We can define turning points like follows: let $P=\{a=x_0,x_1,\dots,x_n=b\}$ be a partition of $\mathbf R$ . Then we can define $$ D(f; P) = \left\{ f(x_1)-f(x_0),\dots, f(x_n)-f(x_{n-1}) \right\}$$ $$ TP(f) = \sup_P (\# \text{sign changes of }D(f; P))$$ My question: When does $k*f$ have more turning points than $f$ ? Equivalently, under what conditions will $k*f$ have at most as many turnings points as $f$ ? Since $f$ has bounded variation we know that $TP(f)$ is at most countably infinite. But consider an example like the Gaussian kernel, especially in the context of heat flow. It is intuitively obvious that heat flow will not create new turning points. I cannot seem to prove this in the language of functional analysis. It is easy to show $(k*f)(-\infty)=0$ and $(k*f)(\infty)=1$ , and also that the variation of $k*f$ is not greater than the variation of $f$ . ALso, if $f$ has no turning points then neither does $k*f$ , because if $f$ is monotone then the variation of $f$ is $1$ , and if $k*f$ has a turning point then its variation is strictly greater than $1$ , a contradiction.","Suppose and are Lebesgue measurable functions and satisfy for all , and . Also suppose has bounded variation, and and . Let denote the convolution of and . Let denote the number of turning points of a measurable function. We can define turning points like follows: let be a partition of . Then we can define My question: When does have more turning points than ? Equivalently, under what conditions will have at most as many turnings points as ? Since has bounded variation we know that is at most countably infinite. But consider an example like the Gaussian kernel, especially in the context of heat flow. It is intuitively obvious that heat flow will not create new turning points. I cannot seem to prove this in the language of functional analysis. It is easy to show and , and also that the variation of is not greater than the variation of . ALso, if has no turning points then neither does , because if is monotone then the variation of is , and if has a turning point then its variation is strictly greater than , a contradiction.","f k \mathbf R\to\mathbf R k(x)\geq 0 x \int_{\mathbf R} k(x)\,dx=1 f f(-\infty)=0 f(\infty)=1 (k*f)(x) = \int_{-\infty}^{\infty}k(x-y)f(y)dy k f TP(f) P=\{a=x_0,x_1,\dots,x_n=b\} \mathbf R  D(f; P) = \left\{ f(x_1)-f(x_0),\dots, f(x_n)-f(x_{n-1}) \right\}  TP(f) = \sup_P (\# \text{sign changes of }D(f; P)) k*f f k*f f f TP(f) (k*f)(-\infty)=0 (k*f)(\infty)=1 k*f f f k*f f f 1 k*f 1",['functional-analysis']
27,Inequality with generalized Hardy operator,Inequality with generalized Hardy operator,,"I am studying the property of Generalized Hardy operators and I am wondering under  which conditions on $\phi_1, \phi_2$ and $k$ the following inequality would hold: $$ \left(\int_0^{\infty}\left(\int_t^{\infty}k^*(s)g^*(s)\right)t^q\phi_1(t)dt\right)^{1/q}\leq C \left(\int_0^{\infty}g^{**}(t)^p\phi_2(t)dt\right)^{1/p}, $$ where $p,q\geq 2$ and $\phi_1, \phi_2$ are weights.",I am studying the property of Generalized Hardy operators and I am wondering under  which conditions on and the following inequality would hold: where and are weights.,"\phi_1, \phi_2 k 
\left(\int_0^{\infty}\left(\int_t^{\infty}k^*(s)g^*(s)\right)t^q\phi_1(t)dt\right)^{1/q}\leq C \left(\int_0^{\infty}g^{**}(t)^p\phi_2(t)dt\right)^{1/p},
 p,q\geq 2 \phi_1, \phi_2","['real-analysis', 'functional-analysis', 'analysis', 'metric-spaces', 'normed-spaces']"
28,"Show that the metric space $(\overline{\mathbb{R}},d)$ is complete, if $d(x,y):=|\arctan x-\arctan y|$","Show that the metric space  is complete, if","(\overline{\mathbb{R}},d) d(x,y):=|\arctan x-\arctan y|","Consider the space $\mathbb{R}$ equipped with the distance function $$d(x,y):=|\arctan x-\arctan y|.$$ There are several posts to prove that $(\mathbb{R},d)$ is not complete. For instance, Real numbers equipped with the metric $ d (x,y) = | \arctan(x) - \arctan(y)| $ is an incomplete metric space and Show that $(\mathbb{R},d)$ is not complete , where $d(x,y) =|\arctan(x) - \arctan(y)|$. From these posts, I saw that the problematic term is $\pm\infty$ , so I think perhaps we can complete this metric space by simply adding $\pm\infty$ . That is, if denote $\overline{\mathbb{R}}:=[-\infty,\infty]$ , I want to show that $(\overline{\mathbb{R}},d)$ is complete. To this end, I need to show every Cauchy sequence in this space with respect to the metric $d$ converges in $\overline{\mathbb{R}}$ . I tried to separate the case into bounded Cauchy sequence and unbounded Cauchy sequence, but I did not see how to show the convergence is in the space. For example, let $(x_{n})$ be a Cauchy sequence in $(\overline{\mathbb{R}},d)$ . Suppose it is bounded, that is $|x_{n}|\leq M$ for all $n$ and for some constant $M$ . But then what should I do? For unbounded sequence, then $x_{n}\nearrow\infty$ directly and $\infty$ has been included in $\overline{\mathbb{R}}$ , so we are done in this case, right? For the boundedness here, I mean a Cauchy sequence with respect to the arctan metric but is bounded/unbounded in the usual absolute value metric. Thanks for any help! Edit 1: As suggested by the comment, I need to define $$d(-\infty,\infty):=|-\dfrac{\pi}{2}-\dfrac{\pi}{2}|=\pi,\ \ \ d(x,\infty):=|\arctan x-\dfrac{\pi}{2}|,\ \ \ d(-\infty, x):=|-\dfrac{\pi}{2}-\arctan x|.$$ But then what I should do? Edit 2: A possible solution: Okay, I think I solved it using isometry. I am not sure if I am correct, so I added a tag of proof verification. The followings are the complete proof: Denote $\overline{\mathbb{R}}:=[-\infty,\infty]$ . We firstly need to extend the definition of $d(x,y)$ such that it is defined on such a space. That is, we further define $$d(x,y):=\left\{ 		\begin{array}{ll} 		|\arctan(x)-\arctan(y)|\ \ \text{if}\ \ x,y\in\mathbb{R}\\ 		\frac{\pi}{2}-\arctan (y)\ \ \text{if}\ \ x=\infty\ \ \text{and}\ \ y\in\mathbb{R}\\ 		\frac{\pi}{2}+\arctan (y)\ \ \text{if}\ \ x=-\infty\ \ \text{and}\ \ y\in\mathbb{R}\\ 		\pi\ \ \text{if}\ \ x=-\infty\ \ \text{and}\ \ y=\infty. 		\end{array} \right. $$ As $\lim_{x\rightarrow\infty}\arctan(x)=\frac{\pi}{2}$ and $\lim_{x\rightarrow-\infty}\arctan(x)=-\frac{\pi}{2}$ , the above extended definition is clearly well-defined. We now prove that $(\overline{\mathbb{R}},d)$ is complete. To this end, we define $f:\overline{\mathbb{R}}\longrightarrow [-\frac{\pi}{2},\frac{\pi}{2}]$ by $$f(x):=\left\{ 		\begin{array}{ll} 		\arctan(x)\ \ \text{if}\ \ x\in\mathbb{R}\\ 		\frac{\pi}{2}\ \ \text{if}\ \ x=\infty\\ 		-\frac{\pi}{2}\ \ \text{if}\ \ x=-\infty. 		\end{array} \right. $$ Then, defining $d'(x,y):=|x-y|$ , we see that for any $a,b\in \overline{\mathbb{R}}$ , we have $$d'(f(a),f(b))=d(a,b).$$ Therefore, we see that $(\overline{\mathbb{R}},d)$ and $([-\frac{\pi}{2},\frac{\pi}{2}],d')$ are isometric. Hence, it follows from general topology that $(\overline{\mathbb{R}},d)$ is complete if and only if $([-\frac{\pi}{2},\frac{\pi}{2}],d')$ is complete. But note that $[-\frac{\pi}{2},\frac{\pi}{2}]$ is compact with respect to the usual metric $d'$ , and we recall from general topology that every compact metric space is complete. Hence, $([-\frac{\pi}{2},\frac{\pi}{2}],d')$ is complete. The proof is then concluded. Please let me know if I am correct!","Consider the space equipped with the distance function There are several posts to prove that is not complete. For instance, Real numbers equipped with the metric $ d (x,y) = | \arctan(x) - \arctan(y)| $ is an incomplete metric space and Show that $(\mathbb{R},d)$ is not complete , where $d(x,y) =|\arctan(x) - \arctan(y)|$. From these posts, I saw that the problematic term is , so I think perhaps we can complete this metric space by simply adding . That is, if denote , I want to show that is complete. To this end, I need to show every Cauchy sequence in this space with respect to the metric converges in . I tried to separate the case into bounded Cauchy sequence and unbounded Cauchy sequence, but I did not see how to show the convergence is in the space. For example, let be a Cauchy sequence in . Suppose it is bounded, that is for all and for some constant . But then what should I do? For unbounded sequence, then directly and has been included in , so we are done in this case, right? For the boundedness here, I mean a Cauchy sequence with respect to the arctan metric but is bounded/unbounded in the usual absolute value metric. Thanks for any help! Edit 1: As suggested by the comment, I need to define But then what I should do? Edit 2: A possible solution: Okay, I think I solved it using isometry. I am not sure if I am correct, so I added a tag of proof verification. The followings are the complete proof: Denote . We firstly need to extend the definition of such that it is defined on such a space. That is, we further define As and , the above extended definition is clearly well-defined. We now prove that is complete. To this end, we define by Then, defining , we see that for any , we have Therefore, we see that and are isometric. Hence, it follows from general topology that is complete if and only if is complete. But note that is compact with respect to the usual metric , and we recall from general topology that every compact metric space is complete. Hence, is complete. The proof is then concluded. Please let me know if I am correct!","\mathbb{R} d(x,y):=|\arctan x-\arctan y|. (\mathbb{R},d) \pm\infty \pm\infty \overline{\mathbb{R}}:=[-\infty,\infty] (\overline{\mathbb{R}},d) d \overline{\mathbb{R}} (x_{n}) (\overline{\mathbb{R}},d) |x_{n}|\leq M n M x_{n}\nearrow\infty \infty \overline{\mathbb{R}} d(-\infty,\infty):=|-\dfrac{\pi}{2}-\dfrac{\pi}{2}|=\pi,\ \ \ d(x,\infty):=|\arctan x-\dfrac{\pi}{2}|,\ \ \ d(-\infty, x):=|-\dfrac{\pi}{2}-\arctan x|. \overline{\mathbb{R}}:=[-\infty,\infty] d(x,y) d(x,y):=\left\{
		\begin{array}{ll}
		|\arctan(x)-\arctan(y)|\ \ \text{if}\ \ x,y\in\mathbb{R}\\
		\frac{\pi}{2}-\arctan (y)\ \ \text{if}\ \ x=\infty\ \ \text{and}\ \ y\in\mathbb{R}\\
		\frac{\pi}{2}+\arctan (y)\ \ \text{if}\ \ x=-\infty\ \ \text{and}\ \ y\in\mathbb{R}\\
		\pi\ \ \text{if}\ \ x=-\infty\ \ \text{and}\ \ y=\infty.
		\end{array}
\right.
 \lim_{x\rightarrow\infty}\arctan(x)=\frac{\pi}{2} \lim_{x\rightarrow-\infty}\arctan(x)=-\frac{\pi}{2} (\overline{\mathbb{R}},d) f:\overline{\mathbb{R}}\longrightarrow [-\frac{\pi}{2},\frac{\pi}{2}] f(x):=\left\{
		\begin{array}{ll}
		\arctan(x)\ \ \text{if}\ \ x\in\mathbb{R}\\
		\frac{\pi}{2}\ \ \text{if}\ \ x=\infty\\
		-\frac{\pi}{2}\ \ \text{if}\ \ x=-\infty.
		\end{array}
\right.
 d'(x,y):=|x-y| a,b\in \overline{\mathbb{R}} d'(f(a),f(b))=d(a,b). (\overline{\mathbb{R}},d) ([-\frac{\pi}{2},\frac{\pi}{2}],d') (\overline{\mathbb{R}},d) ([-\frac{\pi}{2},\frac{\pi}{2}],d') [-\frac{\pi}{2},\frac{\pi}{2}] d' ([-\frac{\pi}{2},\frac{\pi}{2}],d')","['real-analysis', 'general-topology', 'analysis', 'metric-spaces', 'solution-verification']"
29,Proving equality between two limsups,Proving equality between two limsups,,"Say we have two bounded sequences $a_{n}, b_{n}$ , where $\lim_{n\to\infty}a_{n}=1$ . I have to prove that $\limsup_{n\to\infty}a_{n}b_{n}=\limsup_{n\to\infty}b_{n}$ . My idea was as following: First $b_{n}$ is bounded, so we can denote $\limsup b_{n}=L$ . There exists a sub-sequence of $b_{n}$ , $b_{n_{k}}$ that satisfies $\lim b_{n_{k}}=L$ . Now we look at the sub-sequence of $a_{n}b_{n} - a_{n_{k}}b_{n_{k}}$ . $\lim a_{n_{k}}=1$ as a sub-sequence of a converging sequence. So using limit arithmetic rules, $\lim a_{n_{k}}b_{n_{k}}=L=\limsup b_{n}$ . Now whats left is to prove that $\lim a_{n_{k}}b_{n_{k}}=\limsup a_{n}b_{n}$ . Let's assume by contradiction that this is false. So there exists a sub-sequence $a_{n_{j}}b_{n_{j}}$ that satisfies $\lim a_{n_{j}}b_{n_{j}}=K, K>L$ . $a_{n_{j}}$ converges to $1$ and $a_{n_{j}}b_{n_{j}}$ converges to $K$ , so we can assume that $b_{n_{j}}$ converges to $K$ , in contradiction to the assumption that $\limsup b_{n}=L$ . Is my argument correct or am I missing something?","Say we have two bounded sequences , where . I have to prove that . My idea was as following: First is bounded, so we can denote . There exists a sub-sequence of , that satisfies . Now we look at the sub-sequence of . as a sub-sequence of a converging sequence. So using limit arithmetic rules, . Now whats left is to prove that . Let's assume by contradiction that this is false. So there exists a sub-sequence that satisfies . converges to and converges to , so we can assume that converges to , in contradiction to the assumption that . Is my argument correct or am I missing something?","a_{n}, b_{n} \lim_{n\to\infty}a_{n}=1 \limsup_{n\to\infty}a_{n}b_{n}=\limsup_{n\to\infty}b_{n} b_{n} \limsup b_{n}=L b_{n} b_{n_{k}} \lim b_{n_{k}}=L a_{n}b_{n} - a_{n_{k}}b_{n_{k}} \lim a_{n_{k}}=1 \lim a_{n_{k}}b_{n_{k}}=L=\limsup b_{n} \lim a_{n_{k}}b_{n_{k}}=\limsup a_{n}b_{n} a_{n_{j}}b_{n_{j}} \lim a_{n_{j}}b_{n_{j}}=K, K>L a_{n_{j}} 1 a_{n_{j}}b_{n_{j}} K b_{n_{j}} K \limsup b_{n}=L","['sequences-and-series', 'analysis', 'solution-verification', 'limsup-and-liminf']"
30,The operator norm $\|L\|$,The operator norm,\|L\|,"Let $C_0([0, 1])$ be a subspace of $C([0, 1])$ , a functional space consisting of real-value continuous functions over the interval $[0, 1]$ , such that $C_0 ([0, 1]) = \left\{ f \in C([0, 1]) \mid \int_0^1 f(t) dt = 0 \right\}$ , and define the norm as $\| f \|_\infty = \sup_{x \in [0, 1]} |f(x)|$ . Then, define linear operator $L: C_0 ([0, 1]) \to C ([0, 1])$ as $(Lf)(x) = \int_0^x (x-t)f(t)\, dt\quad (x \in [0, 1])$ I can show that $L$ is bounded by using some inequalities, but what is the operator norm $||L||$ ? So far, I hypothesize that $\|L\| = \frac{1}{4}$ , by considering the definition $\|L\| = \sup\{\|Lf\|_\infty: f \in C_0 ([0, 1]) {\rm with} \|f\|_\infty = 1\}$ , and then thinking of a continuous function that is very close to this one: $f(x) =  \left\{ \begin{array}{ll}     1 & (0 \leq x \leq 1/2) \\     -1 & (1/2 < x \leq 1)   \end{array} \right.$ (I know this is not even continuous, but I'm thinking of an intuitive way to estimate $\|L\|$ by thinking of a function $f$ that satisfies $\|f\|_\infty = 1$ , and would give the maximum of $\|Lf\|_\infty$ .) And then I get the $\frac{1}{4}$ by calculating (assume $x > 1/2$ ) $\int_0^x (x-t)f(t)\, dt = \int_0^{1/2} (x - t)\, dt + \int_{1/2}^x (t-x)\, dt = -\frac{1}{2}x^2 + x -\frac{1}{4}$ and finding the maximum value of the result ( $\frac{1}{4}$ at $x = 1$ ) Where do I go from here? How can I give a more mathematical approach to calculating $\|L\|$ ? Thank you in advance.","Let be a subspace of , a functional space consisting of real-value continuous functions over the interval , such that , and define the norm as . Then, define linear operator as I can show that is bounded by using some inequalities, but what is the operator norm ? So far, I hypothesize that , by considering the definition , and then thinking of a continuous function that is very close to this one: (I know this is not even continuous, but I'm thinking of an intuitive way to estimate by thinking of a function that satisfies , and would give the maximum of .) And then I get the by calculating (assume ) and finding the maximum value of the result ( at ) Where do I go from here? How can I give a more mathematical approach to calculating ? Thank you in advance.","C_0([0, 1]) C([0, 1]) [0, 1] C_0 ([0, 1]) = \left\{ f \in C([0, 1]) \mid \int_0^1 f(t) dt = 0 \right\} \| f \|_\infty = \sup_{x \in [0, 1]} |f(x)| L: C_0 ([0, 1]) \to C ([0, 1]) (Lf)(x) = \int_0^x (x-t)f(t)\, dt\quad (x \in [0, 1]) L ||L|| \|L\| = \frac{1}{4} \|L\| = \sup\{\|Lf\|_\infty: f \in C_0 ([0, 1]) {\rm with} \|f\|_\infty = 1\} f(x) = 
\left\{ \begin{array}{ll}
    1 & (0 \leq x \leq 1/2) \\
    -1 & (1/2 < x \leq 1)
  \end{array} \right. \|L\| f \|f\|_\infty = 1 \|Lf\|_\infty \frac{1}{4} x > 1/2 \int_0^x (x-t)f(t)\, dt = \int_0^{1/2} (x - t)\, dt + \int_{1/2}^x (t-x)\, dt = -\frac{1}{2}x^2 + x -\frac{1}{4} \frac{1}{4} x = 1 \|L\|","['integration', 'analysis', 'normed-spaces']"
31,Is this statement about change of variables in limits true?,Is this statement about change of variables in limits true?,,"I was brainstorming on this and on some related thoughts in the past 2-3 days so any help here would be much appreciated. What put me in these thoughts... was one proof (from a textbook) which I read and which was using the implication b) (see below) very casually and as if it is quite natural to assume that this is true. So I realized this needs more rigorous justification. Is the following statement/theorem true? Given are the two functions: $$g(x): (a,b) \rightarrow (c,d) \tag{1}$$ $$f(x): (c,d) \rightarrow \mathbb{R} \tag{2}$$ The function $g$ is continuous, monotonic and takes all values between $c$ and $d$ (it is surjective but possibly not bijective) Let also: $$\lim\limits_{x \to a} g(x) = c$$ Then: a) if the limit $\lim\limits_{y \to c} f(y)$ exists and is $L$ , then $\lim\limits_{x \to a} f(g(x))$ also exists and is equal to $L$ b) if the limit $\lim\limits_{x \to a} f(g(x))$ exists and is $L$ , then $\lim\limits_{y \to c} f(y)$ also exists and is equal to $L$ Note 1 : Here the symbols $a,c$ may denote numbers or $-\infty$ , and the symbols $b,d$ may denote numbers or $+\infty$ Note 2 : By limit exists and equals $L$ , it is meant that the limit is either a number, or also an infinity (positive or negative) I think this theorem is true and it is what justifies when people do simple variable substitutions in limits (almost mechanically) and write casually that: $$\lim\limits_{t \to 0+} \phi(1/t) = \lim\limits_{x \to \infty} \phi(x)$$ or say $$\lim\limits_{t \to \infty} \phi((t-2)^{2}) = \lim\limits_{x \to \infty} \phi(x)$$ or e.g. $$\lim\limits_{t \to 0} \phi(1/t^2) = \lim\limits_{x \to \infty} \phi(x)$$ I think I was able to prove both a) and b). The part a) is proved easily, it even follows from Limit of composite functions theorem But the proof of part b) substantially uses the assumption that $g$ takes all values between $c$ and $d$ (otherwise b) would not hold true, right?). But my point is that when we do almost mechanically such changes of variables in limits (from $t$ to $x$ or from $x$ to $y$ , etc.)... we don't really think about the Limit of composite functions theorem and check its conditions, right? Instead we just think of $g$ as bijection and we assume by intuition that changing the variable works OK because of the bijective behavior of $g$ . Would anyone agree with that? And... could someone confirm this theorem is true? Also, do we need any restrictions on f in this theorem? I think not. Also, can we remove the condition that g is continuous? Or relax the conditions of the theorem in some other way?","I was brainstorming on this and on some related thoughts in the past 2-3 days so any help here would be much appreciated. What put me in these thoughts... was one proof (from a textbook) which I read and which was using the implication b) (see below) very casually and as if it is quite natural to assume that this is true. So I realized this needs more rigorous justification. Is the following statement/theorem true? Given are the two functions: The function is continuous, monotonic and takes all values between and (it is surjective but possibly not bijective) Let also: Then: a) if the limit exists and is , then also exists and is equal to b) if the limit exists and is , then also exists and is equal to Note 1 : Here the symbols may denote numbers or , and the symbols may denote numbers or Note 2 : By limit exists and equals , it is meant that the limit is either a number, or also an infinity (positive or negative) I think this theorem is true and it is what justifies when people do simple variable substitutions in limits (almost mechanically) and write casually that: or say or e.g. I think I was able to prove both a) and b). The part a) is proved easily, it even follows from Limit of composite functions theorem But the proof of part b) substantially uses the assumption that takes all values between and (otherwise b) would not hold true, right?). But my point is that when we do almost mechanically such changes of variables in limits (from to or from to , etc.)... we don't really think about the Limit of composite functions theorem and check its conditions, right? Instead we just think of as bijection and we assume by intuition that changing the variable works OK because of the bijective behavior of . Would anyone agree with that? And... could someone confirm this theorem is true? Also, do we need any restrictions on f in this theorem? I think not. Also, can we remove the condition that g is continuous? Or relax the conditions of the theorem in some other way?","g(x): (a,b) \rightarrow (c,d) \tag{1} f(x): (c,d) \rightarrow \mathbb{R} \tag{2} g c d \lim\limits_{x \to a} g(x) = c \lim\limits_{y \to c} f(y) L \lim\limits_{x \to a} f(g(x)) L \lim\limits_{x \to a} f(g(x)) L \lim\limits_{y \to c} f(y) L a,c -\infty b,d +\infty L \lim\limits_{t \to 0+} \phi(1/t) = \lim\limits_{x \to \infty} \phi(x) \lim\limits_{t \to \infty} \phi((t-2)^{2}) = \lim\limits_{x \to \infty} \phi(x) \lim\limits_{t \to 0} \phi(1/t^2) = \lim\limits_{x \to \infty} \phi(x) g c d t x x y g g","['real-analysis', 'limits', 'analysis', 'change-of-variable']"
32,"The least distance of $f\in\ell_\infty(K,\mathbb C)$ from $C(K,\mathbb C)$",The least distance of  from,"f\in\ell_\infty(K,\mathbb C) C(K,\mathbb C)","Suppose that $K$ is a compact Hausdorff space. Consider a bounded function $f:K\to\mathbb R$ not necessarily continuous, that is, $f\in\ell_\infty(K,\mathbb R)$ . It's a well-known fact that the least distance of $f$ from some continuous $g:K\to \mathbb R$ is half of the maximum oscilation of $f$ . More precisely, we define the oscilation of $f$ at $k$ by $$ osc_f(k) = \inf_{U\in\mathcal V_k} \sup_{u,v\in U} |f(u)-f(v)|, $$ where $\mathcal V_k$ is the set of open neighborhoods of $k$ , and we have the following result, which is the Proposition 1.18.(ii) of the book Geometric Nonlinear Functional Analysis , by Y. Benyaminni and J. Lindenstrauss: Let $f:K\to\mathbb R$ and put $\delta =\|osc_f\|_\infty$ , where $\|\cdot\|_\infty$ is the supremum norm. Then, there exists a continuous function $g:K\to\mathbb R$ such that $$ \|f-g\|_\infty=\frac{\delta}{2}, $$ and this is the least distance of a continuous function from $f$ . So, here is my question: Is there an analogous result for $\mathbb C$ , instead of $\mathbb R$ ? EXAMPLE: Let us see a function $f: K \to \mathbb C$ such that the least distance of this function to a continuous function is not $\delta/2$ . Put $K=[0,1]$ and define for each $k\in[0,1]$ : $$ f(k) = \left\{\begin{array}{rl} 1 &,\mbox{ if $k$ is rational}\\ e^{\frac{\pi i}{3}} &,\mbox{if $k$ is not rational, but is algebraic} \\ e^{-\frac{\pi i}{3}} &, \mbox{if $k$ is transcendental}\end{array}\right. $$ The image of this function is equal to the vertices of an equilateral triangle centered at the origin of the complex plane. The oscillation of this function in any $k$ is equal to the length of the side of this triangle, that is, $\sqrt3$ . But there is no point whose distance is less than $\sqrt3/2$ to all the vertices of the triangle. The continuous function that has the least distance to $f$ is the zero function $k\mapsto 0$ , and this distance is equal to $1$ .","Suppose that is a compact Hausdorff space. Consider a bounded function not necessarily continuous, that is, . It's a well-known fact that the least distance of from some continuous is half of the maximum oscilation of . More precisely, we define the oscilation of at by where is the set of open neighborhoods of , and we have the following result, which is the Proposition 1.18.(ii) of the book Geometric Nonlinear Functional Analysis , by Y. Benyaminni and J. Lindenstrauss: Let and put , where is the supremum norm. Then, there exists a continuous function such that and this is the least distance of a continuous function from . So, here is my question: Is there an analogous result for , instead of ? EXAMPLE: Let us see a function such that the least distance of this function to a continuous function is not . Put and define for each : The image of this function is equal to the vertices of an equilateral triangle centered at the origin of the complex plane. The oscillation of this function in any is equal to the length of the side of this triangle, that is, . But there is no point whose distance is less than to all the vertices of the triangle. The continuous function that has the least distance to is the zero function , and this distance is equal to .","K f:K\to\mathbb R f\in\ell_\infty(K,\mathbb R) f g:K\to \mathbb R f f k 
osc_f(k) = \inf_{U\in\mathcal V_k} \sup_{u,v\in U} |f(u)-f(v)|,
 \mathcal V_k k f:K\to\mathbb R \delta =\|osc_f\|_\infty \|\cdot\|_\infty g:K\to\mathbb R 
\|f-g\|_\infty=\frac{\delta}{2},
 f \mathbb C \mathbb R f: K \to \mathbb C \delta/2 K=[0,1] k\in[0,1] 
f(k) = \left\{\begin{array}{rl} 1 &,\mbox{ if k is rational}\\
e^{\frac{\pi i}{3}} &,\mbox{if k is not rational, but is algebraic} \\
e^{-\frac{\pi i}{3}} &, \mbox{if k is transcendental}\end{array}\right.
 k \sqrt3 \sqrt3/2 f k\mapsto 0 1","['complex-analysis', 'functional-analysis', 'analysis']"
33,Convergence/divergence of a function of the first $n$ terms of a sequence,Convergence/divergence of a function of the first  terms of a sequence,n,"Let $(x_n)$ be an arbitrary sequence in $\mathbb R$ and let $y_n=y_n(x_1,\ldots,x_n)$ be a function which depends solely on the first $n$ terms of $x$ . My question concerns the convergence and divergence of $(y_n)$ . Firstly consider the case that $y_n=\sqrt{x_1+\sqrt {x_2+\sqrt{\ldots+\sqrt{x_n}}}}$ and $x_n\geq 0$ . Clearly, $y_n$ is an non-decreasing sequence. Also, if the sequences $(x_i^{(1)})_{i=1}^\infty, (x_i^{(2)})_{i=1}^\infty$ satisfies $\lim x_i^{(1)}/x_i^{(2)}<\infty$ , then $y_n(x^{(1)})$ converges whenever $y_n(x_i^{(2)})$ converges. The two sequences is said to have the same order of infinity (which is an equivalence relation) if $0<\lim x_i^{(1)}/x_i^{(2)}<\infty$ . Define the set of orders of infinity to be the set of such equivalence classes of sequences. By using zorn's lemma in some way, I proved that there exist an order of infinity, represented by the sequence $c_n$ , such that whenever $\lim x_n/c_n=0$ , $y_n(x)$ converges, and  whenever $\lim x_n/c_n=\infty$ , $y_n(x)$ diverges. Now, the question is: could I find the sequence $c_n$ explicitly? Of course, the sequence $x_n=Ae^{2^nk}$ will fit into the square roots nicely, but this is clearly not satisfying the requirement of $c_n$ .","Let be an arbitrary sequence in and let be a function which depends solely on the first terms of . My question concerns the convergence and divergence of . Firstly consider the case that and . Clearly, is an non-decreasing sequence. Also, if the sequences satisfies , then converges whenever converges. The two sequences is said to have the same order of infinity (which is an equivalence relation) if . Define the set of orders of infinity to be the set of such equivalence classes of sequences. By using zorn's lemma in some way, I proved that there exist an order of infinity, represented by the sequence , such that whenever , converges, and  whenever , diverges. Now, the question is: could I find the sequence explicitly? Of course, the sequence will fit into the square roots nicely, but this is clearly not satisfying the requirement of .","(x_n) \mathbb R y_n=y_n(x_1,\ldots,x_n) n x (y_n) y_n=\sqrt{x_1+\sqrt {x_2+\sqrt{\ldots+\sqrt{x_n}}}} x_n\geq 0 y_n (x_i^{(1)})_{i=1}^\infty, (x_i^{(2)})_{i=1}^\infty \lim x_i^{(1)}/x_i^{(2)}<\infty y_n(x^{(1)}) y_n(x_i^{(2)}) 0<\lim x_i^{(1)}/x_i^{(2)}<\infty c_n \lim x_n/c_n=0 y_n(x) \lim x_n/c_n=\infty y_n(x) c_n x_n=Ae^{2^nk} c_n","['real-analysis', 'sequences-and-series', 'limits', 'analysis', 'convergence-divergence']"
34,Perturbation property of Orthonormal basis for Hilbert space,Perturbation property of Orthonormal basis for Hilbert space,,"Suppose $\{e_i\}_{i\in I}$ is an orthonormal basis for some Hilbert space. $\{f_i\}_{i\in I}$ is an orthonormal family with the property $\sum^\infty_{i\in I} \|e_i-f_i\|^2<\infty$, show that $\{f_i\}_{i\in I}$ is also orthonormal basis. I can think of for a $x$ with $(x,f_i)=0$ for all $i$, prove $x=0$. However, I don't know how to continue to the next step.","Suppose $\{e_i\}_{i\in I}$ is an orthonormal basis for some Hilbert space. $\{f_i\}_{i\in I}$ is an orthonormal family with the property $\sum^\infty_{i\in I} \|e_i-f_i\|^2<\infty$, show that $\{f_i\}_{i\in I}$ is also orthonormal basis. I can think of for a $x$ with $(x,f_i)=0$ for all $i$, prove $x=0$. However, I don't know how to continue to the next step.",,"['functional-analysis', 'hilbert-spaces']"
35,stochastic integral is unique,stochastic integral is unique,,"If you know all about stochastic integration please just skip to my question; the first part is just fixing notation and background. Let $\Omega, \mathcal{A},\mu$ be a probability space. And let $W:\Omega \times [0,T]\rightarrow \mathbb{R}$ be a Brownian motion adapted to a filtration $\mathcal{F}_t\subset \mathcal{A}, t\in [0,T]$ . I just read how one may define a stochastic integral $\int_0^TXdW$ where $$X: \Omega \times [0,T]\rightarrow \mathbb{R}$$ is $ \mathcal{A}\otimes\mathcal{B}([0,T])$ -measurable, X is $\mathcal{F}_t$ -adapted, and where $X\in L^2(\Omega \times [0,T])$ ).  The proof is based on the fact that the $L^2$ condition implies that there is a sequence of adapted $L^2$ ""simple processes"", $$\sigma_n(\omega,t)=\sum_{i=0}^{k}f_i(\omega)\cdot \mathbb{1}_{(t_i,t_{i+1}]}(t), ~~~(f_i\in L^2(\Omega,~\mathcal{F}_{t_i}),~t_0=0,~t_{k_n+1}=T)$$ converging to $X$ in $L^2(\Omega \times [0,T]).$ For $t\in (t_j,t_{j+1}]$ one defines $$(\int_0^T\sigma_ndW)(\omega,t)=f_j\cdot(W_{t}-W_{t_j}) +\sum_{i=0}^{j-1}f_i\cdot(W_{t_{i+1}}-W_{t_i}),$$ and then one shows that $t\mapsto (\int_0^T\sigma_ndW)(\omega,t)$ is continuous for all $\omega,n$ , and that for almost every $\omega$ , the functions $t\mapsto (\int_0^T\sigma_ndW)(\omega,t)$ satisfy the Weierstrass M test, and thus converge to a continuous function $t\mapsto Y(\omega,t)$ .  We define $\int_0^TXdW:=Y$ . That is all clear. Now the book I'm reading ( Intro to Stochastic Calculus Applied to Finance by Lamberton and Lapeyre) says we need to extend this integral by relaxing the $X\in L^2(\Omega \times [0,T])$ condition.  Namely the claim is that if $$\mathcal{H}= \{ X: \Omega \times [0,T]\rightarrow \mathbb{R}~~~|~~X ~is~\mathcal{F}_t- adapted, ~ and~for~almost~all~\omega~we~have~\int_0^T|X(\omega,t)|^2dt<\infty \}$$ and if $\mathcal{C}$ is the space of adapted and almost surely continuous processes on $\Omega\times [0,T]$ , then there exists a unique linear mapping $$\mathcal{H}\rightarrow\mathcal{C},~X\mapsto \int_0^T XdW$$ satisfying the following two properties This integral agrees with the previous definition for simple processes (the third centered line in this post). If $H_n$ is a sequence of processes in $\mathcal{H}$ such that the sequence of function $\bigg(\omega\mapsto\int_0^TH_n^2(\omega,t)dt\bigg)_{n\in \mathbb{N}}$ converges in probability to $0$ , then the sequence $$\bigg(\omega\mapsto sup_{t\leq T}|(\int_0^T H_ndW)(\omega,t)|\bigg)_{n\in \mathbb{N}}$$ also converges to $0$ in probability Finally here's my question: The book says that it is clear that conditions 1 and 2 imply that the new integral agrees with the old (almost surely for all $t\in [0,T]$ ) whenever the integrand $X$ is in $L^2(\Omega \times [0,T])$ . Why?? The only thing I can think to do is to again take a sequence of simple processes, $\sigma_n$ , converging to $X$ in $L^2(\Omega \times [0,T])$ .  Then clearly the sequence $$\bigg(\omega\mapsto\int_0^T|\sigma_n(s,\omega) ds - X(s,\omega)|^2 ds\bigg)_{n \in \mathbb{N}}$$ converges to $0$ in $L^1(\Omega)$ and therefore converges to $0$ in probability. Thus by 2, we have that $$\bigg(\omega \mapsto sup_{0\leq t \leq T}\bigg|(\int_0^T \sigma_ndW)(\omega, t)-(\int_0^TXdW)(\omega, t)\bigg|\bigg)_{n\in \mathbb{N}}$$ converges to zero in probability.  Now what we want is that for almost all $\omega$ , for all $t$ we have $$(\int_0^T \sigma_ndW)(\omega, t)-(\int_0^TXdW)(\omega, t)\rightarrow 0.$$ This would suffice because we know $\int_0^T \sigma_ndW$ agrees with the previously defined integral by property 1, and the sequence $\int_0^T \sigma_ndW$ converges for almost all $\omega$ for all $t$ to the previously defined integral of $X$ .  But as far as I can tell, convergence in probability doesn't guarantee this.","If you know all about stochastic integration please just skip to my question; the first part is just fixing notation and background. Let be a probability space. And let be a Brownian motion adapted to a filtration . I just read how one may define a stochastic integral where is -measurable, X is -adapted, and where ).  The proof is based on the fact that the condition implies that there is a sequence of adapted ""simple processes"", converging to in For one defines and then one shows that is continuous for all , and that for almost every , the functions satisfy the Weierstrass M test, and thus converge to a continuous function .  We define . That is all clear. Now the book I'm reading ( Intro to Stochastic Calculus Applied to Finance by Lamberton and Lapeyre) says we need to extend this integral by relaxing the condition.  Namely the claim is that if and if is the space of adapted and almost surely continuous processes on , then there exists a unique linear mapping satisfying the following two properties This integral agrees with the previous definition for simple processes (the third centered line in this post). If is a sequence of processes in such that the sequence of function converges in probability to , then the sequence also converges to in probability Finally here's my question: The book says that it is clear that conditions 1 and 2 imply that the new integral agrees with the old (almost surely for all ) whenever the integrand is in . Why?? The only thing I can think to do is to again take a sequence of simple processes, , converging to in .  Then clearly the sequence converges to in and therefore converges to in probability. Thus by 2, we have that converges to zero in probability.  Now what we want is that for almost all , for all we have This would suffice because we know agrees with the previously defined integral by property 1, and the sequence converges for almost all for all to the previously defined integral of .  But as far as I can tell, convergence in probability doesn't guarantee this.","\Omega, \mathcal{A},\mu W:\Omega \times [0,T]\rightarrow \mathbb{R} \mathcal{F}_t\subset \mathcal{A}, t\in [0,T] \int_0^TXdW X: \Omega \times [0,T]\rightarrow \mathbb{R}  \mathcal{A}\otimes\mathcal{B}([0,T]) \mathcal{F}_t X\in L^2(\Omega \times [0,T]) L^2 L^2 \sigma_n(\omega,t)=\sum_{i=0}^{k}f_i(\omega)\cdot \mathbb{1}_{(t_i,t_{i+1}]}(t), ~~~(f_i\in L^2(\Omega,~\mathcal{F}_{t_i}),~t_0=0,~t_{k_n+1}=T) X L^2(\Omega \times [0,T]). t\in (t_j,t_{j+1}] (\int_0^T\sigma_ndW)(\omega,t)=f_j\cdot(W_{t}-W_{t_j}) +\sum_{i=0}^{j-1}f_i\cdot(W_{t_{i+1}}-W_{t_i}), t\mapsto (\int_0^T\sigma_ndW)(\omega,t) \omega,n \omega t\mapsto (\int_0^T\sigma_ndW)(\omega,t) t\mapsto Y(\omega,t) \int_0^TXdW:=Y X\in L^2(\Omega \times [0,T]) \mathcal{H}= \{ X: \Omega \times [0,T]\rightarrow \mathbb{R}~~~|~~X ~is~\mathcal{F}_t- adapted, ~ and~for~almost~all~\omega~we~have~\int_0^T|X(\omega,t)|^2dt<\infty \} \mathcal{C} \Omega\times [0,T] \mathcal{H}\rightarrow\mathcal{C},~X\mapsto \int_0^T XdW H_n \mathcal{H} \bigg(\omega\mapsto\int_0^TH_n^2(\omega,t)dt\bigg)_{n\in \mathbb{N}} 0 \bigg(\omega\mapsto sup_{t\leq T}|(\int_0^T H_ndW)(\omega,t)|\bigg)_{n\in \mathbb{N}} 0 t\in [0,T] X L^2(\Omega \times [0,T]) \sigma_n X L^2(\Omega \times [0,T]) \bigg(\omega\mapsto\int_0^T|\sigma_n(s,\omega) ds - X(s,\omega)|^2 ds\bigg)_{n \in \mathbb{N}} 0 L^1(\Omega) 0 \bigg(\omega \mapsto sup_{0\leq t \leq T}\bigg|(\int_0^T \sigma_ndW)(\omega, t)-(\int_0^TXdW)(\omega, t)\bigg|\bigg)_{n\in \mathbb{N}} \omega t (\int_0^T \sigma_ndW)(\omega, t)-(\int_0^TXdW)(\omega, t)\rightarrow 0. \int_0^T \sigma_ndW \int_0^T \sigma_ndW \omega t X","['analysis', 'stochastic-processes', 'stochastic-calculus']"
36,"A Question about rationality , irrationality or Transcendence of definite integral","A Question about rationality , irrationality or Transcendence of definite integral",,( this is my first question on the site so please forgive any possible mistake ) Consider integral of the form : $$\int_0^\infty f(x)dx$$ Can we have a set of conditions of $f(x)$ such that we can assure the value is rational or irrational( transcendental ) ? Is/are there such type of standard result in literature ?,( this is my first question on the site so please forgive any possible mistake ) Consider integral of the form : Can we have a set of conditions of such that we can assure the value is rational or irrational( transcendental ) ? Is/are there such type of standard result in literature ?,\int_0^\infty f(x)dx f(x),"['analysis', 'reference-request']"
37,the limit of absolute value is the absolute value of the limit,the limit of absolute value is the absolute value of the limit,,"Let A $\subset$ $\Bbb R$ , let $f$ : A $\to$ $\Bbb R$ and let $c$ $\in$ $\Bbb R$ a acumulation point of A. If $\lim_{x\to c} f$ exist and if $|f|$ is the function defined for $x$ $\in$ A, by $|f|(x) = |f(x)|$ , then prove that $\lim_{x\to c} |f| =$ $\ $ | $ lim_{x\to c} f $ | $ $ I have the idea to prove it, with the triangular inequality, but... it's not clear to me. Edit: This is the proof Let $L$ = $\lim_{x\to c} f$ , by definition of limit, for all $\epsilon > 0$ , exist $\delta >0$ such that $0<|x-c|<\delta $ then $|f(x)-L|<\epsilon$ , now, by triangular inequality $||f(x)|-|L||<|f(x)-L|$ , so, when $|x-c|<\delta $ , $||f(x)|-|L||<|f(x)-L|<\epsilon$ , so , $\lim_{x\to c} |f| = |L| $ and $\lim_{x\to c} |f| =$ $\ $ | $ lim_{x\to c} f $ | $ $","Let A , let : A and let a acumulation point of A. If exist and if is the function defined for A, by , then prove that | | I have the idea to prove it, with the triangular inequality, but... it's not clear to me. Edit: This is the proof Let = , by definition of limit, for all , exist such that then , now, by triangular inequality , so, when , , so , and | |",\subset \Bbb R f \to \Bbb R c \in \Bbb R \lim_{x\to c} f |f| x \in |f|(x) = |f(x)| \lim_{x\to c} |f| = \   lim_{x\to c} f    L \lim_{x\to c} f \epsilon > 0 \delta >0 0<|x-c|<\delta  |f(x)-L|<\epsilon ||f(x)|-|L||<|f(x)-L| |x-c|<\delta  ||f(x)|-|L||<|f(x)-L|<\epsilon \lim_{x\to c} |f| = |L|  \lim_{x\to c} |f| = \   lim_{x\to c} f   ,"['real-analysis', 'limits', 'analysis']"
38,Laplacian of Kelvin Transform - Confusion about notation,Laplacian of Kelvin Transform - Confusion about notation,,"There is an exercise in Evans' book (chapter 2, problem #11). Suppose $u:\mathbb{R}^n\to\mathbb{R}$ is harmonic. Show that $\overline{u}(x)=u(\overline{x})|\overline{x}|^{n-2}=u(x/|x|^2)|x|^{2-n}$ is harmonic, where $\overline{x}=x/|x|^2$ . I'm less interested in the result, but I like this problem because it seems to be good practice in vector calculus. I want to improve my understanding of operations in $\mathbb{R}^n$ , as my research thus far has been in $\mathbb{R}$ . Anyways, I am rather confused about applying a Laplacian to this function. The hint is to show that $(D_x \overline{x})(D_x \overline{x})^T=|x|^4 I$ , which I have completed. But I want to understand the Laplacian in terms of matrices, products, transposes, traces, etc. I know that $\Delta(fg)=g\Delta f+f\Delta g+2(Df)\cdot (Dg).$ If $f(x)=u(x/|x|^2)$ and $g(x)=|x|^{2-n}$ , then this formula can be applied. But I am a bit lost on calculating $\Delta (u(x/|x|^2))$ . Any insight would be helpful. Thank you.","There is an exercise in Evans' book (chapter 2, problem #11). Suppose is harmonic. Show that is harmonic, where . I'm less interested in the result, but I like this problem because it seems to be good practice in vector calculus. I want to improve my understanding of operations in , as my research thus far has been in . Anyways, I am rather confused about applying a Laplacian to this function. The hint is to show that , which I have completed. But I want to understand the Laplacian in terms of matrices, products, transposes, traces, etc. I know that If and , then this formula can be applied. But I am a bit lost on calculating . Any insight would be helpful. Thank you.",u:\mathbb{R}^n\to\mathbb{R} \overline{u}(x)=u(\overline{x})|\overline{x}|^{n-2}=u(x/|x|^2)|x|^{2-n} \overline{x}=x/|x|^2 \mathbb{R}^n \mathbb{R} (D_x \overline{x})(D_x \overline{x})^T=|x|^4 I \Delta(fg)=g\Delta f+f\Delta g+2(Df)\cdot (Dg). f(x)=u(x/|x|^2) g(x)=|x|^{2-n} \Delta (u(x/|x|^2)),"['analysis', 'multivariable-calculus', 'laplacian', 'partial-differential-equations']"
39,Definition of Differentials through Surreal Numbers,Definition of Differentials through Surreal Numbers,,"It is fairly well known that nowadays derivatives are defined primarily by use of a limit argument. I think I recall that when the idea of derivatives was first introduced (and limits had not yet been used) there was some criticism because the method of computing derivatives/justifying them required the notion of an absolutely small positive number (epsilon). I am wondering whether the concept of surreal numbers would allow for exactly this definition of derivatives/differentials to be mathematically valid as well. After all, surreal numbers would allow for an epsilon that fits the description above. It would also justify why higher-order terms can be neglected in a derivative (because higher order terms represent differentials raised to even higher powers, which must then belong to even smaller groups of surreal numbers. Thanks for any help on this.","It is fairly well known that nowadays derivatives are defined primarily by use of a limit argument. I think I recall that when the idea of derivatives was first introduced (and limits had not yet been used) there was some criticism because the method of computing derivatives/justifying them required the notion of an absolutely small positive number (epsilon). I am wondering whether the concept of surreal numbers would allow for exactly this definition of derivatives/differentials to be mathematically valid as well. After all, surreal numbers would allow for an epsilon that fits the description above. It would also justify why higher-order terms can be neglected in a derivative (because higher order terms represent differentials raised to even higher powers, which must then belong to even smaller groups of surreal numbers. Thanks for any help on this.",,"['calculus', 'analysis', 'surreal-numbers']"
40,Motivation of geometric analysis,Motivation of geometric analysis,,Geometric analysis in some sense tries to make sense of calculus on spaces where we don't require any smoothness (for example metric measure spaces). What are the type of problems that motivate these kind of studies?,Geometric analysis in some sense tries to make sense of calculus on spaces where we don't require any smoothness (for example metric measure spaces). What are the type of problems that motivate these kind of studies?,,"['analysis', 'metric-spaces', 'geometric-measure-theory']"
41,Poincare lemma: from local to global solutions,Poincare lemma: from local to global solutions,,"I am reading a book by Moskowitz and Paliogiannis where I miss a point when trying to complete a proof of the Poincare lemma (which is left to the reader). The following facts are proved clearly in the book. Fact 1: Let $F:\Omega\to\mathbb{R}^{n}$ be a $C^{1}$ -mapping (vector field), $\Omega$ an open ball in $\mathbb{R}^{n}$ , and $$  \forall i,j\in\{1,\dots,n\}\; \forall x\in\Omega: \quad \frac{\partial F_{i}}{\partial x_{j}}(x)=\frac{\partial F_{j}}{\partial x_{i}}(x). $$ Then $F$ is conservative, i.e., there is a $C^{1}$ -funcion $f:\Omega\to\mathbb{R}$ such that $\mbox{grad} f =F$ . Fact 2: Let $F:\Omega\to\mathbb{R}^{n}$ be a $C^{1}$ -mapping, $\Omega$ a region (=open and connected) in $\mathbb{R}^{n}$ , and for any closed $C^{1}$ -curve $\gamma$ in $\Omega$ , it holds that $$ \int_{\gamma}F\cdot ds=0. $$ Then $F$ is conservative. The goal is to prove the following: Theorem (Poincare lemma): Fact 1 remains true if $\Omega$ is any simply connected subset of $\mathbb{R}^{n}$ . I miss a point when trying to pass from balls to simply connected domains. I suppose that one should elaborate on Fact 2 and use the simple connectedness of $\Omega$ . Question: How should I construct the potential $f$ on a simply connected domain $\Omega$ provided that I am aware of Facts 1 and 2?","I am reading a book by Moskowitz and Paliogiannis where I miss a point when trying to complete a proof of the Poincare lemma (which is left to the reader). The following facts are proved clearly in the book. Fact 1: Let be a -mapping (vector field), an open ball in , and Then is conservative, i.e., there is a -funcion such that . Fact 2: Let be a -mapping, a region (=open and connected) in , and for any closed -curve in , it holds that Then is conservative. The goal is to prove the following: Theorem (Poincare lemma): Fact 1 remains true if is any simply connected subset of . I miss a point when trying to pass from balls to simply connected domains. I suppose that one should elaborate on Fact 2 and use the simple connectedness of . Question: How should I construct the potential on a simply connected domain provided that I am aware of Facts 1 and 2?","F:\Omega\to\mathbb{R}^{n} C^{1} \Omega \mathbb{R}^{n} 
 \forall i,j\in\{1,\dots,n\}\; \forall x\in\Omega: \quad \frac{\partial F_{i}}{\partial x_{j}}(x)=\frac{\partial F_{j}}{\partial x_{i}}(x).
 F C^{1} f:\Omega\to\mathbb{R} \mbox{grad} f =F F:\Omega\to\mathbb{R}^{n} C^{1} \Omega \mathbb{R}^{n} C^{1} \gamma \Omega 
\int_{\gamma}F\cdot ds=0.
 F \Omega \mathbb{R}^{n} \Omega f \Omega","['real-analysis', 'analysis', 'vector-analysis']"
42,On the qualitative behaviour of functions of the form $f’(x) = f(x)$ as x decreases,On the qualitative behaviour of functions of the form  as x decreases,f’(x) = f(x),"Pretend that you don’t know about the function $ e^x$ (so pretend to erase it and everything you know about it from your memory, for argument’s sake). You want to draw graphs of functions that satisfy $f’(x) = f(x)$ . Not precisely, but you want to know it’s qualitative properties. $f(x) = 0 \forall x$ is a trivial solution, so let’s put this to one side. Now you also quickly realise that if $f(x) > 0$ for some $x,\ $ then the following qualitative behaviour of the function $f(x)$ must be true: both $f(x)$ and $f’(x)$ must be strictly positive and increasing as $x {\to} \infty$ . You realised this by drawing a small straight line of gradient 1 when $f(x) = 1$ and then $f’(x) = 1.1$ just to the right, when $f(x) = 1.1$ etc etc. Are there any arguments- graphical, using calculus and continuity and any other tools you might have, to determine whether or not $f(c)=0 $ for some $ c \in \mathbb{R}$ as $x$ decreases? If such a $c$ does exist, then it follows that $f(x) = 0 \forall x<c$ also, since: if $f(a) < 0$ for some $a<c$ then $\exists b $ with $a<b<c$ such that $f’(b) > 0$ , so $f’(b) \neq f(b)$ , a contradiction to the original condition. A similar argument can be made to show that $f(d) > 0$ for some $ d<c$ leads to a contradiction. But it’s not clear to me how you can argue that the graph wouldn’t attain f(x) = 0 for some x. I don’t see why our graph can’t look like for example, the piecewise curve $y=0$ for $x < 0$ and $y= x^4$ for $x \geq 0$ . Can an argument be made for why this isn’t possible this in a similar vein to one of my arguments/lines of reasoning above?","Pretend that you don’t know about the function (so pretend to erase it and everything you know about it from your memory, for argument’s sake). You want to draw graphs of functions that satisfy . Not precisely, but you want to know it’s qualitative properties. is a trivial solution, so let’s put this to one side. Now you also quickly realise that if for some then the following qualitative behaviour of the function must be true: both and must be strictly positive and increasing as . You realised this by drawing a small straight line of gradient 1 when and then just to the right, when etc etc. Are there any arguments- graphical, using calculus and continuity and any other tools you might have, to determine whether or not for some as decreases? If such a does exist, then it follows that also, since: if for some then with such that , so , a contradiction to the original condition. A similar argument can be made to show that for some leads to a contradiction. But it’s not clear to me how you can argue that the graph wouldn’t attain f(x) = 0 for some x. I don’t see why our graph can’t look like for example, the piecewise curve for and for . Can an argument be made for why this isn’t possible this in a similar vein to one of my arguments/lines of reasoning above?"," e^x f’(x) = f(x) f(x) = 0 \forall x f(x) > 0 x,\  f(x) f(x) f’(x) x {\to} \infty f(x) = 1 f’(x) = 1.1 f(x) = 1.1 f(c)=0   c \in \mathbb{R} x c f(x) = 0 \forall x<c f(a) < 0 a<c \exists b  a<b<c f’(b) > 0 f’(b) \neq f(b) f(d) > 0  d<c y=0 x < 0 y= x^4 x \geq 0","['real-analysis', 'calculus', 'geometry', 'algebra-precalculus', 'analysis']"
43,Expansion about $x=1$ for $\sum_{n=2}^\infty \frac{x^n}{n\log n}$,Expansion about  for,x=1 \sum_{n=2}^\infty \frac{x^n}{n\log n},"$$\sum_{n=2}^N \frac{1}{n\log n}$$ diverges as $N\rightarrow \infty$ , because the integral $$\int_2^N \frac{{\rm d}t}{t\log t}=\log(\log N) - \log(\log 2)$$ diverges. The singularity is double logarithmic and I therefore expect the series $$f(x)=\sum_{n=2}^\infty \frac{x^n}{n\log n}$$ to have a double logarithmic singularity at $x=1$ i.e. $$f(x) \sim \log \left(-\log\left(1-x\right)\right)$$ as $x\rightarrow 1$ . Is there a simple way to derive the expansion about $x=1$ ? Since $\frac{x^n}{n\log n}$ is monotonic, it may be useful/effective to deploy Euler-Maclaurin Expansion i.e. calculate the integral $$\int_2^\infty \frac{e^{n\log x}}{n\log n} \, {\rm d}n \, ,$$ but no anti-derivative seems to exist. Ultimately this question is related to the existence of these types of integrals $$\int_2^\infty \frac{e^{-tx}}{\log x} \, {\rm d}x$$ for $t>0$ and $t\rightarrow 0$ in terms of (semi)-elementary functions. I found the following asymptotic expansion for any $a>0$ \begin{align} \int_a^\infty \frac{e^{-xt}}{\log x} \, {\rm d}x &\stackrel{u=xt}{=} \frac{-1}{t\log t} \int_{at}^\infty \frac{e^{-u}}{1 - \frac{\log u}{\log t}} \, {\rm d}u \\ &= \frac{-1}{t\log t} \sum_{n=0}^\infty \frac{1}{\log^n t} \int_{at}^\infty e^{-u} \log^n u \, {\rm d}u \\ &\stackrel{t\rightarrow 0}{=} \frac{-1}{t\log t} \sum_{n=0}^\infty \frac{\Gamma^{(n)}(1)}{\log^n t} \, . \tag{1} \end{align} The limiting integral for $t\rightarrow 0$ converges and the error by extending the range to $0$ is ${\cal O}\left(t\log^n(t)\right)$ . Therefore the asymptotic expansion follows. Integrating the asymptotic expansion (1) with respect to $t$ then yields $$\int_a^\infty \frac{e^{-xt}}{x\log x} \, {\rm d}x = \log(-\log(t)) - \sum_{n=1}^\infty \frac{\Gamma^{(n)}(1)}{n\log^n t} + C$$ and it can be shown $C=0$ .","diverges as , because the integral diverges. The singularity is double logarithmic and I therefore expect the series to have a double logarithmic singularity at i.e. as . Is there a simple way to derive the expansion about ? Since is monotonic, it may be useful/effective to deploy Euler-Maclaurin Expansion i.e. calculate the integral but no anti-derivative seems to exist. Ultimately this question is related to the existence of these types of integrals for and in terms of (semi)-elementary functions. I found the following asymptotic expansion for any The limiting integral for converges and the error by extending the range to is . Therefore the asymptotic expansion follows. Integrating the asymptotic expansion (1) with respect to then yields and it can be shown .","\sum_{n=2}^N \frac{1}{n\log n} N\rightarrow \infty \int_2^N \frac{{\rm d}t}{t\log t}=\log(\log N) - \log(\log 2) f(x)=\sum_{n=2}^\infty \frac{x^n}{n\log n} x=1 f(x) \sim \log \left(-\log\left(1-x\right)\right) x\rightarrow 1 x=1 \frac{x^n}{n\log n} \int_2^\infty \frac{e^{n\log x}}{n\log n} \, {\rm d}n \, , \int_2^\infty \frac{e^{-tx}}{\log x} \, {\rm d}x t>0 t\rightarrow 0 a>0 \begin{align}
\int_a^\infty \frac{e^{-xt}}{\log x} \, {\rm d}x &\stackrel{u=xt}{=} \frac{-1}{t\log t} \int_{at}^\infty \frac{e^{-u}}{1 - \frac{\log u}{\log t}} \, {\rm d}u \\
&= \frac{-1}{t\log t} \sum_{n=0}^\infty \frac{1}{\log^n t} \int_{at}^\infty e^{-u} \log^n u \, {\rm d}u \\
&\stackrel{t\rightarrow 0}{=} \frac{-1}{t\log t} \sum_{n=0}^\infty \frac{\Gamma^{(n)}(1)}{\log^n t} \, . \tag{1}
\end{align} t\rightarrow 0 0 {\cal O}\left(t\log^n(t)\right) t \int_a^\infty \frac{e^{-xt}}{x\log x} \, {\rm d}x = \log(-\log(t)) - \sum_{n=1}^\infty \frac{\Gamma^{(n)}(1)}{n\log^n t} + C C=0","['complex-analysis', 'analysis', 'power-series']"
44,Function spaces on manifolds via category theory,Function spaces on manifolds via category theory,,"In order to define function spaces on manifolds one usually follows the same recipe and this begs for a categorial formulation. I would like to know whether there is a reference that discusses this in some detail. Here is an example of what I have in mind: Let $E$ be a function space on $\mathbb{R}^d$ , say $E=L^2(\mathbb{R}^d)$ or some Sobolev space. Then for $U\subset \mathbb{R}^d$ define $E_c(U)$ to consist of restrictions to $U$ of elements in $E$ with support compactly contained in $U$ . Then $E_c(U)$ admits a natural Frechet space structure and for the examples above any diffeo $U\cong V$ of subsets of $\mathbb{R}^d$ induces an isomorphism $E_c(U)\cong E_c(V)$ of Frechet spaces (coordinate invariance). Now if $M$ is a manifold, then one can define $E_c(U)$ whenever $U\subset M$ is a chart domain and for general $U$ via a partition of unity argument. The two properties that allowed us to make the construction above are: 1) Elements of $E$ can be restricted, we can e.g. assume that $E$ embeds into the space of distributions on $\mathbb{R}^d$ and use the natural restriction there. 2) The local spaces $E_c$ are coordinate invariant. I assume that there the construction can be formalised in the following way: For any function space $E$ with the properties 1) and 2) there exists a functor $\mathcal E_c: \mathsf{Man}\rightarrow \mathsf{LCTVS}$ from the category of smooth manifolds to the category of locally convex topological spaces such that $\mathcal{E}_c(U) \cong E_c(U)$ in a natural way when $U$ is an open subset of $\mathbb{R}^d$ .  What I am interested in is whether this in turn gives rise to a functor $E \mapsto \mathcal{E}_c$ with nice properties, e.g preserving compact embeddings, exact sequences and so on. One very concrete example that I am interested in occurs in the setting of pseudodifferential operators. On $\mathbb{R}^d$ they are easy to define and one proofs e.g. that the symbol map gives rise to a short exact sequence of Frechet spaces. The same result is true on closed manifolds and I believe that this should follow from abstract nonsense.","In order to define function spaces on manifolds one usually follows the same recipe and this begs for a categorial formulation. I would like to know whether there is a reference that discusses this in some detail. Here is an example of what I have in mind: Let be a function space on , say or some Sobolev space. Then for define to consist of restrictions to of elements in with support compactly contained in . Then admits a natural Frechet space structure and for the examples above any diffeo of subsets of induces an isomorphism of Frechet spaces (coordinate invariance). Now if is a manifold, then one can define whenever is a chart domain and for general via a partition of unity argument. The two properties that allowed us to make the construction above are: 1) Elements of can be restricted, we can e.g. assume that embeds into the space of distributions on and use the natural restriction there. 2) The local spaces are coordinate invariant. I assume that there the construction can be formalised in the following way: For any function space with the properties 1) and 2) there exists a functor from the category of smooth manifolds to the category of locally convex topological spaces such that in a natural way when is an open subset of .  What I am interested in is whether this in turn gives rise to a functor with nice properties, e.g preserving compact embeddings, exact sequences and so on. One very concrete example that I am interested in occurs in the setting of pseudodifferential operators. On they are easy to define and one proofs e.g. that the symbol map gives rise to a short exact sequence of Frechet spaces. The same result is true on closed manifolds and I believe that this should follow from abstract nonsense.",E \mathbb{R}^d E=L^2(\mathbb{R}^d) U\subset \mathbb{R}^d E_c(U) U E U E_c(U) U\cong V \mathbb{R}^d E_c(U)\cong E_c(V) M E_c(U) U\subset M U E E \mathbb{R}^d E_c E \mathcal E_c: \mathsf{Man}\rightarrow \mathsf{LCTVS} \mathcal{E}_c(U) \cong E_c(U) U \mathbb{R}^d E \mapsto \mathcal{E}_c \mathbb{R}^d,"['analysis', 'differential-geometry', 'category-theory']"
45,prove the identity for a differentiable function,prove the identity for a differentiable function,,"Let $f$ be a smooth real function and $f(0)=0, f (1)=1$ , prove that exists various $x_1$ , $x_2$ $ \in [0;1] $ such that $$\frac{1}{f'(x_1)}+\frac{1}{f'(x_2)}=2$$ I tried to use the mean value theorem for different intervals, but it didn't help me. Also I can't understand the behavior of this function. Thank you for help!","Let be a smooth real function and , prove that exists various , such that I tried to use the mean value theorem for different intervals, but it didn't help me. Also I can't understand the behavior of this function. Thank you for help!","f f(0)=0, f (1)=1 x_1 x_2  \in [0;1]  \frac{1}{f'(x_1)}+\frac{1}{f'(x_2)}=2","['analysis', 'differential']"
46,"Prove that for the defined $\langle .,. \rangle$ there exist $0 < a \le b$ such that $a\|x\| \le \|x\|_\ast ≤ b\|x\|$ for all $x \in H$.",Prove that for the defined  there exist  such that  for all .,"\langle .,. \rangle 0 < a \le b a\|x\| \le \|x\|_\ast ≤ b\|x\| x \in H","Let $H$ be a Hilbert space over $\mathbb{R}$ with an inner product $(·, ·)$ and the norm $\|x\| = \sqrt {(x, x)}$ . Let $A$ be a bounded strictly positive definite linear operator on $H$ with $A^\ast = A$ . For $x, y \in H$ , let $\langle x, y \rangle = (Ax, y)$ and $\|x\|_\ast = \sqrt{\langle x, x \rangle}$ . Prove that $\langle .,. \rangle$ is an inner product on the vector space $H$ , and that there exist constants $0 < a \le b$ such that $a\|x\| \le \|x\|_\ast ≤ b\|x\|$ for all $x \in H$ . $\text{My Attempt}$ : for the first part to show that it is inner product: 1- $\langle x, x \rangle = (Ax, x) \ge 0$ and $(Ax, x)=0$ iff $x=0$ 2- $\langle x, y \rangle = (Ax, y) = (x, Ay)= \langle y , x \rangle$ 3- $\langle x+z, y \rangle = (Ax+Az, y) = (Ax, y) +(Az, y) =\langle x, y \rangle +\langle z, y \rangle$ 4- $\langle \alpha x, y \rangle = (\alpha Ax, y)= \alpha (Ax, y)=\alpha \langle x, y \rangle$ For the second part 1- Since $A$ is a positive definite linear operator $\|x\|_\ast = \sqrt{\langle x, x \rangle} = \sqrt{(Ax,x)} \ge\sqrt{\beta \|x\|^2} = \sqrt{\beta} \|x\|\equiv b \|x\|$ 2- Let $\|A\| = \alpha$ . Since $A$ is symmetric $\|x\|_\ast = \sqrt{\langle x, x \rangle} = \sqrt{(Ax,x)} = \sqrt{(x,Ax)} \le\sqrt{ \|A\| \|x\|^2} = \sqrt{\alpha} \|x\|\equiv a \|x\|$","Let be a Hilbert space over with an inner product and the norm . Let be a bounded strictly positive definite linear operator on with . For , let and . Prove that is an inner product on the vector space , and that there exist constants such that for all . : for the first part to show that it is inner product: 1- and iff 2- 3- 4- For the second part 1- Since is a positive definite linear operator 2- Let . Since is symmetric","H \mathbb{R} (·, ·) \|x\| = \sqrt
{(x, x)} A H A^\ast = A x, y \in H \langle x, y \rangle = (Ax, y) \|x\|_\ast = \sqrt{\langle x, x \rangle} \langle .,. \rangle H 0 < a \le b a\|x\| \le \|x\|_\ast ≤ b\|x\| x \in H \text{My Attempt} \langle x, x \rangle = (Ax, x) \ge 0 (Ax, x)=0 x=0 \langle x, y \rangle = (Ax, y) = (x, Ay)= \langle y , x \rangle \langle x+z, y \rangle = (Ax+Az, y) = (Ax, y) +(Az, y) =\langle x, y \rangle +\langle z, y \rangle \langle \alpha x, y \rangle = (\alpha Ax, y)= \alpha (Ax, y)=\alpha \langle x, y \rangle A \|x\|_\ast = \sqrt{\langle x, x \rangle} = \sqrt{(Ax,x)} \ge\sqrt{\beta \|x\|^2} = \sqrt{\beta} \|x\|\equiv b \|x\| \|A\| = \alpha A \|x\|_\ast = \sqrt{\langle x, x \rangle} = \sqrt{(Ax,x)} = \sqrt{(x,Ax)} \le\sqrt{ \|A\| \|x\|^2} = \sqrt{\alpha} \|x\|\equiv a \|x\|","['functional-analysis', 'analysis', 'inner-products', 'solution-verification', 'adjoint-operators']"
47,A function satisfying a series equation,A function satisfying a series equation,,"We already solved or argued differential equations, integral equations and even function equations. But I encounter the following, which I call it ""series equation"": Is there any nonzero function $f$ satisfying the series equation $$f(x)=\sum_{n=0}^{\infty}\frac{f(n)}{n!}x^n \tag{*} $$ The source: I interested to solve the equation $$f'(x)=f(x+1)$$ and I found out $$f^{(n)}(x)=f(x+n)$$ thus: $f^{(n)}(0)=f(n)$ and if we consider $f$ having a taylor series, then the expression $(*)$ will be the case. But I have no idea how $f$ would be. Is such a function exists? Thanks.","We already solved or argued differential equations, integral equations and even function equations. But I encounter the following, which I call it ""series equation"": Is there any nonzero function satisfying the series equation The source: I interested to solve the equation and I found out thus: and if we consider having a taylor series, then the expression will be the case. But I have no idea how would be. Is such a function exists? Thanks.",f f(x)=\sum_{n=0}^{\infty}\frac{f(n)}{n!}x^n \tag{*}  f'(x)=f(x+1) f^{(n)}(x)=f(x+n) f^{(n)}(0)=f(n) f (*) f,"['real-analysis', 'ordinary-differential-equations', 'analysis', 'taylor-expansion', 'delay-differential-equations']"
48,Complex Analysis vs. $\mathbb{R}^2$ Integrals,Complex Analysis vs.  Integrals,\mathbb{R}^2,"I'm wondering about the following I'm somewhat shocked about, since I wasn't too aware of it, so I'm asking here for clarification or if it is even true... Let $f(z)$ be holomorphic. When calculating the closed loop integral over the contour $C$ and converting to real and imaginary parts, I get the following $$0=\oint_C f(z) \, {\rm d}z = \oint_C (u+iv) \, {\rm d}(x+iy) \\ =\oint_C \left\{ u \, {\rm d}x - v \, {\rm d}y + i\left(u \, {\rm d}y + v \, {\rm d}x\right) \right\} \\ =\oint_C \begin{pmatrix}u \\ -v \\ 0\end{pmatrix} \cdot \begin{pmatrix} {\rm d}x \\ {\rm d}y \\ {\rm d}z \end{pmatrix} + i \oint_C \begin{pmatrix}v \\ u \\ 0\end{pmatrix} \cdot \begin{pmatrix} {\rm d}x \\ {\rm d}y \\ {\rm d}z \end{pmatrix} \, . $$ In the last step I interpreted the planar curve $C$ as part of ${\mathbb{R}}^3$ . Now because of the $i$ each term has to vanish separately. Let's consider the first term first: Because $u$ and $v$ are harmonic functions, they are differentiable and thus Stokes theorem can be applied. The first term then becomes $$0=\int_A {\rm d}\vec{A} \cdot \nabla \times \begin{pmatrix}u \\ -v \\ 0\end{pmatrix} = -\int_A {\rm d}\vec{A} \cdot \begin{pmatrix} 0 \\ 0 \\ \partial_x v + \partial_y u \end{pmatrix}$$ where $A$ is any oriented area bounded by the planar curve $C$ . Therefore $\begin{pmatrix}u \\ -v \\ 0\end{pmatrix}$ is integrable and can be expressed as a gradient i.e. $\begin{pmatrix}u \\ -v \\ 0\end{pmatrix}=\nabla \phi(x,y)$ . Plugging this into the CR-equations, it all makes sense $$\frac{\partial u}{\partial x} = \frac{\partial^2 \phi}{\partial x^2} = \frac{\partial v}{\partial y} = - \frac{\partial^2 \phi}{\partial y^2} \quad \Rightarrow \quad \Delta \phi = 0$$ i.e. $\phi$ is harmonic. The second equation reads $$\frac{\partial u}{\partial y} = \frac{\partial^2 \phi}{\partial y \partial x} = -\frac{\partial v}{\partial x} = \frac{\partial^2 \phi}{\partial x \partial y}$$ which is true due to Schwarz. I see that starting from the CR-equations and expressing $u$ and $v$ as a gradient, they are automatically fulfilled. But does this also mean that this is true in all generality? For every holomorphic function, the real and imaginary part is a gradient of some scalar potential $\phi$ that is harmonic i.e. every holomorphic function $f(z)$ can be decomposed as $$f(z) = \partial_x \phi - i\partial_y \phi = 2\,\partial_z \phi \, .$$ Since $\phi$ is harmonic, i.e. $4\partial_\bar{z}\partial_z \phi=0$ , it can be written as $$\phi(x,y) = \tilde{\phi}(z,\bar{z}) = \phi_1(z) + \phi_2(\bar{z}) \, ,$$ since mixed terms can not arise. But now $\phi$ is real $\forall x,y \in \mathbb{R}$ , so $\phi_2=\bar{\phi}_1$ . So essentially it boils down to the fact that the 2d-laplacian nicely factorizes upon the variable transformation $(x,y)\rightarrow (z,\bar{z})$ .","I'm wondering about the following I'm somewhat shocked about, since I wasn't too aware of it, so I'm asking here for clarification or if it is even true... Let be holomorphic. When calculating the closed loop integral over the contour and converting to real and imaginary parts, I get the following In the last step I interpreted the planar curve as part of . Now because of the each term has to vanish separately. Let's consider the first term first: Because and are harmonic functions, they are differentiable and thus Stokes theorem can be applied. The first term then becomes where is any oriented area bounded by the planar curve . Therefore is integrable and can be expressed as a gradient i.e. . Plugging this into the CR-equations, it all makes sense i.e. is harmonic. The second equation reads which is true due to Schwarz. I see that starting from the CR-equations and expressing and as a gradient, they are automatically fulfilled. But does this also mean that this is true in all generality? For every holomorphic function, the real and imaginary part is a gradient of some scalar potential that is harmonic i.e. every holomorphic function can be decomposed as Since is harmonic, i.e. , it can be written as since mixed terms can not arise. But now is real , so . So essentially it boils down to the fact that the 2d-laplacian nicely factorizes upon the variable transformation .","f(z) C 0=\oint_C f(z) \, {\rm d}z = \oint_C (u+iv) \, {\rm d}(x+iy) \\
=\oint_C \left\{ u \, {\rm d}x - v \, {\rm d}y + i\left(u \, {\rm d}y + v \, {\rm d}x\right) \right\} \\
=\oint_C \begin{pmatrix}u \\ -v \\ 0\end{pmatrix} \cdot \begin{pmatrix} {\rm d}x \\ {\rm d}y \\ {\rm d}z \end{pmatrix} + i \oint_C \begin{pmatrix}v \\ u \\ 0\end{pmatrix} \cdot \begin{pmatrix} {\rm d}x \\ {\rm d}y \\ {\rm d}z \end{pmatrix} \, .
 C {\mathbb{R}}^3 i u v 0=\int_A {\rm d}\vec{A} \cdot \nabla \times \begin{pmatrix}u \\ -v \\ 0\end{pmatrix} = -\int_A {\rm d}\vec{A} \cdot \begin{pmatrix} 0 \\ 0 \\ \partial_x v + \partial_y u \end{pmatrix} A C \begin{pmatrix}u \\ -v \\ 0\end{pmatrix} \begin{pmatrix}u \\ -v \\ 0\end{pmatrix}=\nabla \phi(x,y) \frac{\partial u}{\partial x} = \frac{\partial^2 \phi}{\partial x^2} = \frac{\partial v}{\partial y} = - \frac{\partial^2 \phi}{\partial y^2} \quad \Rightarrow \quad \Delta \phi = 0 \phi \frac{\partial u}{\partial y} = \frac{\partial^2 \phi}{\partial y \partial x} = -\frac{\partial v}{\partial x} = \frac{\partial^2 \phi}{\partial x \partial y} u v \phi f(z) f(z) = \partial_x \phi - i\partial_y \phi = 2\,\partial_z \phi \, . \phi 4\partial_\bar{z}\partial_z \phi=0 \phi(x,y) = \tilde{\phi}(z,\bar{z}) = \phi_1(z) + \phi_2(\bar{z}) \, , \phi \forall x,y \in \mathbb{R} \phi_2=\bar{\phi}_1 (x,y)\rightarrow (z,\bar{z})","['complex-analysis', 'analysis']"
49,Can $\ell_p$ unit-balls have more than 4 nearest points?,Can  unit-balls have more than 4 nearest points?,\ell_p,"For a certain $2\times 2$ matrix $M$ I would like to show the inequality $$\|M x\|_{s'} \le \|x\|_s $$ where $\frac 1s+\frac1{s'}=1$ and $s\in[1,2]$ . (This is equivalent to bounding the induced norm $\|M\|_{s'\to s}$ .) I have that $M=\begin{bmatrix}a&b\\c&a\end{bmatrix}$ has all positive values and $a\ge\max\{b,c\}$ , so $M$ is positive semidefinite. I can thus think of this as comparing a unit ball to another unit ball, given a simple rotation, shearing and scaling of the axes. W.log. I can assume $\|x\|_s=1$ . I have identified the four points in which the two unit-balls touch each other. (See the picture below for an example.) I now want to prove that the two balls can not touch each other in more than the four points I have identified. This seems to stem from the fact that $\ell_p$ unit-balls are somewhat ""square"", since e.g. hexagonal unit-balls could of course easily have more points of touch. I don't know how to formalize this ""squareness"" however. I don't know if it is necessary that one of the norms is the dual norm to the other (i.e. $\frac 1s+\frac1{s'}=1$ ), but it probably is required that $s,s'\ge 1$ . Do you know of some way to formalize the above intuition? Or some topological lemma related to distances between balls like this?","For a certain matrix I would like to show the inequality where and . (This is equivalent to bounding the induced norm .) I have that has all positive values and , so is positive semidefinite. I can thus think of this as comparing a unit ball to another unit ball, given a simple rotation, shearing and scaling of the axes. W.log. I can assume . I have identified the four points in which the two unit-balls touch each other. (See the picture below for an example.) I now want to prove that the two balls can not touch each other in more than the four points I have identified. This seems to stem from the fact that unit-balls are somewhat ""square"", since e.g. hexagonal unit-balls could of course easily have more points of touch. I don't know how to formalize this ""squareness"" however. I don't know if it is necessary that one of the norms is the dual norm to the other (i.e. ), but it probably is required that . Do you know of some way to formalize the above intuition? Or some topological lemma related to distances between balls like this?","2\times 2 M \|M x\|_{s'} \le \|x\|_s  \frac 1s+\frac1{s'}=1 s\in[1,2] \|M\|_{s'\to s} M=\begin{bmatrix}a&b\\c&a\end{bmatrix} a\ge\max\{b,c\} M \|x\|_s=1 \ell_p \frac 1s+\frac1{s'}=1 s,s'\ge 1","['real-analysis', 'general-topology', 'geometry', 'analysis']"
50,Set inequalities,Set inequalities,,"I would like if someone could help me out with this inequality,  Let $S \subset \mathbb{R}^d$ be a Legesgue measurable set ( with non-zero measure) then, $$ \int_{S} |x| \  dx \geq C |S|^{\frac{d+1}{d}}, $$ where C is a constant independent of $S$ , depending only on dimension. One can see that this is true for balls, using polar coordinates, but I just don't know how to generalize this to any Lebesgue measurable set. Thank you for your help.","I would like if someone could help me out with this inequality,  Let be a Legesgue measurable set ( with non-zero measure) then, where C is a constant independent of , depending only on dimension. One can see that this is true for balls, using polar coordinates, but I just don't know how to generalize this to any Lebesgue measurable set. Thank you for your help.","S \subset \mathbb{R}^d 
\int_{S} |x| \
 dx \geq C |S|^{\frac{d+1}{d}},
 S","['analysis', 'measure-theory', 'harmonic-analysis']"
51,What is the decay of the convolution of these two slowly decaying functions?,What is the decay of the convolution of these two slowly decaying functions?,,"Let $f(x) = \sqrt{1+x^2}^{-\alpha}$ , and $g(x) = \sqrt{1+x^2}^{-\beta}$ for some $\alpha, \beta > 0$ satisfying $\alpha + \beta >1$ . Then let $$(f * g)(y) := \int _{\mathbb{R}} \frac{1}{(1+x^2)^{\alpha/2} (1+(x-y)^2)^{\beta/2}} dx~.$$ For all fixed $y$ the above integral converges, thanks to $\alpha+\beta >1$ , but we don't assume $f, g \in L^1$ , only $fg \in L^1$ . How to prove the decay rate of $f*g$ ? I suspect it's something like $O(y^{-\min(\alpha,\beta)})$ . Thanks EDIT: Thanks to the calculation of reuns, I also wonder what is the answer in the case where $\alpha = 1$ , $\beta >0$ ? ! Thanks","Let , and for some satisfying . Then let For all fixed the above integral converges, thanks to , but we don't assume , only . How to prove the decay rate of ? I suspect it's something like . Thanks EDIT: Thanks to the calculation of reuns, I also wonder what is the answer in the case where , ? ! Thanks","f(x) = \sqrt{1+x^2}^{-\alpha} g(x) = \sqrt{1+x^2}^{-\beta} \alpha, \beta > 0 \alpha + \beta >1 (f * g)(y) := \int _{\mathbb{R}} \frac{1}{(1+x^2)^{\alpha/2} (1+(x-y)^2)^{\beta/2}} dx~. y \alpha+\beta >1 f, g \in L^1 fg \in L^1 f*g O(y^{-\min(\alpha,\beta)}) \alpha = 1 \beta >0","['analysis', 'fourier-analysis', 'convolution', 'approximation-theory']"
52,Orthogonality with respect to a matrix,Orthogonality with respect to a matrix,,"Consider two vectors $x\in\mathbb{R}^n$ and $y\in\mathbb{R}^n$ . Obviously, $x^\top y = 0$ implies that $x$ and $y$ are orthogonal. Now, let $A\in\mathbb{R}^{n \times n}$ be a matrix. What is the meaning of $$x^\top A y = 0$$ ? Does this imply orthogonality of $x$ and $y$ with respect to something?","Consider two vectors and . Obviously, implies that and are orthogonal. Now, let be a matrix. What is the meaning of ? Does this imply orthogonality of and with respect to something?",x\in\mathbb{R}^n y\in\mathbb{R}^n x^\top y = 0 x y A\in\mathbb{R}^{n \times n} x^\top A y = 0 x y,"['linear-algebra', 'matrices', 'analysis', 'eigenvalues-eigenvectors']"
53,Intuition behind upper and lower hemicontinuity,Intuition behind upper and lower hemicontinuity,,I've been studying quite a bit about upper and lower hemicontinuity with reference to correspondences. However im having a hard time seeing the advantage of this definition and what it adds to the overall understanding of continuity in general. Whats an intuitive way to understand upper and lower hemicontiuity/ Why is it important?,I've been studying quite a bit about upper and lower hemicontinuity with reference to correspondences. However im having a hard time seeing the advantage of this definition and what it adds to the overall understanding of continuity in general. Whats an intuitive way to understand upper and lower hemicontiuity/ Why is it important?,,['analysis']
54,When is $\tan(x) \in \mathbb{R} - \mathbb{Q}(\sqrt{2})$,When is,\tan(x) \in \mathbb{R} - \mathbb{Q}(\sqrt{2}),"While solving a different problem, I found Lambert’s proof (and later Laczkovich’s simplification of Lambert’s proof) that $\pi$ is irrational: https://en.m.wikipedia.org/wiki/Proof_that_π_is_irrational#Laczkovich.27s_proof But then I found this question on SE Prove that if $x$ is a non-zero rational number, then $\tan(x)$ is not a rational number and use this to prove that $\pi$ is not a rational number. That asks for a proof for the fact that given a nonzero rational $x$ , show that $\tan(x)$ is irrational (ironically, in order to later show that $\pi$ is irrational). This got me thinking about a problem I’ve now been stuck on for a bit. If I want to generalize this claim a little more, then what can we say about $\tan$ with respect to $\mathbb{Q}(\sqrt{2})$ . Is it true that for $x \in \mathbb{Q}(\sqrt{2})$ we have $\tan(x) \in \mathbb{R} - \mathbb{Q}(\sqrt{2})$ ? If not that when can we say that $\tan(x) \in \mathbb{R} - \mathbb{Q}(\sqrt{2})$ ? Edit: where $x$ is nonzero of course","While solving a different problem, I found Lambert’s proof (and later Laczkovich’s simplification of Lambert’s proof) that is irrational: https://en.m.wikipedia.org/wiki/Proof_that_π_is_irrational#Laczkovich.27s_proof But then I found this question on SE Prove that if $x$ is a non-zero rational number, then $\tan(x)$ is not a rational number and use this to prove that $\pi$ is not a rational number. That asks for a proof for the fact that given a nonzero rational , show that is irrational (ironically, in order to later show that is irrational). This got me thinking about a problem I’ve now been stuck on for a bit. If I want to generalize this claim a little more, then what can we say about with respect to . Is it true that for we have ? If not that when can we say that ? Edit: where is nonzero of course",\pi x \tan(x) \pi \tan \mathbb{Q}(\sqrt{2}) x \in \mathbb{Q}(\sqrt{2}) \tan(x) \in \mathbb{R} - \mathbb{Q}(\sqrt{2}) \tan(x) \in \mathbb{R} - \mathbb{Q}(\sqrt{2}) x,"['analysis', 'trigonometry', 'field-theory']"
55,Does the Dominated Convergence Theorem Hold Almost Surely.,Does the Dominated Convergence Theorem Hold Almost Surely.,,"This question is about the dominated convergence theorem for a stochastic process. Let $X_t$ be a sequence of stochastic process on some probability space $(\Omega,\mathcal{F}_t,\mathbb{P})$ . For each $\omega$ assume $X_\cdot(\omega)$ is continuous on $[0,T]$ (i.e the realisations of the stochastic process are continuous paths). Say $X_t$ take values in $\mathbb{R}^d$ . Let $f^n$ be a sequence of functions $f^n:\mathbb{R}^d \to \mathbb{R}$ converging point wise to some function $f$ . Consider an integral $$\lim_{n\to \infty}\int_0^T f^n(X_s)\,ds $$ Assume that for almost every $\omega$ there exists a function $g^\omega$ such that $f^n(X_s(\omega))\leq g^\omega(s)$ , where $g^\omega$ is integrable, i.e $\int_0^T g^\omega(s)ds<\infty .$ Can one apply the dominated convergence theorem to say $$\lim_{n\to \infty}\int_0^T f^n(X_s)\,ds=\int_0^T\lim_{n\to \infty} f^n(X_s)\,ds \text{?} ~~~\text{almost surely}$$ What would be the case if there was a $g$ (independent of $\omega$ ) which did the dominating job for a.e $\omega$ .","This question is about the dominated convergence theorem for a stochastic process. Let be a sequence of stochastic process on some probability space . For each assume is continuous on (i.e the realisations of the stochastic process are continuous paths). Say take values in . Let be a sequence of functions converging point wise to some function . Consider an integral Assume that for almost every there exists a function such that , where is integrable, i.e Can one apply the dominated convergence theorem to say What would be the case if there was a (independent of ) which did the dominating job for a.e .","X_t (\Omega,\mathcal{F}_t,\mathbb{P}) \omega X_\cdot(\omega) [0,T] X_t \mathbb{R}^d f^n f^n:\mathbb{R}^d \to \mathbb{R} f \lim_{n\to \infty}\int_0^T f^n(X_s)\,ds  \omega g^\omega f^n(X_s(\omega))\leq g^\omega(s) g^\omega \int_0^T g^\omega(s)ds<\infty . \lim_{n\to \infty}\int_0^T f^n(X_s)\,ds=\int_0^T\lim_{n\to \infty} f^n(X_s)\,ds \text{?} ~~~\text{almost surely} g \omega \omega","['analysis', 'measure-theory', 'stochastic-processes', 'lebesgue-measure', 'almost-everywhere']"
56,When a nonlinear operator has no fixed point?,When a nonlinear operator has no fixed point?,,Let B be a normed vector space and $f: B \to B$ be a nonlinear operator. It is well known that the Banach fixed-point theorem or  the Brouwer fixed-point theorem gives conditions which imply that a operator $f$ has a fixed point. My question is that is there a result on nonexistence of fixed points of a nonlinear operator. That is conditions which ensure that the operator $f$ has no fixed point.,Let B be a normed vector space and be a nonlinear operator. It is well known that the Banach fixed-point theorem or  the Brouwer fixed-point theorem gives conditions which imply that a operator has a fixed point. My question is that is there a result on nonexistence of fixed points of a nonlinear operator. That is conditions which ensure that the operator has no fixed point.,f: B \to B f f,"['functional-analysis', 'analysis', 'nonlinear-analysis']"
57,"For $0\le m\le M$, let $p$ satisfy $\left(1-\frac1M\right)^m(1-p)=\frac1e-\frac1M$. Why is it that $p=O\left(\frac{M-m}{M}\right)$?","For , let  satisfy . Why is it that ?",0\le m\le M p \left(1-\frac1M\right)^m(1-p)=\frac1e-\frac1M p=O\left(\frac{M-m}{M}\right),"For $M$ large enough, by taking logarithms and Taylor series, it can be checked that $$\left(1-\frac1M\right)^M \ge \frac1e - \frac1M$$ For $0\le m\le M$ , let $p$ be a parameter satisfying $$\left(1-\frac1M\right)^m(1-p)= \frac1e - \frac1M$$ Someone claims $$p=O\left(\frac{M-m}{M}\right)$$ I am wondering why it is true? **Further information: ** we can assume $M, m$ are nonnegative integers and $m<M$ .","For large enough, by taking logarithms and Taylor series, it can be checked that For , let be a parameter satisfying Someone claims I am wondering why it is true? **Further information: ** we can assume are nonnegative integers and .","M \left(1-\frac1M\right)^M \ge \frac1e - \frac1M 0\le m\le M p \left(1-\frac1M\right)^m(1-p)= \frac1e - \frac1M p=O\left(\frac{M-m}{M}\right) M, m m<M","['analysis', 'inequality']"
58,"Minimizing a function related to ""The Median Minimizing the Sum of Absolute Deviations""","Minimizing a function related to ""The Median Minimizing the Sum of Absolute Deviations""",,"The function $f:\mathbb{R}^n\rightarrow\mathbb{R}$ to minimize has the following form: $$f(x)=\displaystyle\sum_{i=1}^n\sum_{j=1}^n|s_{ij}-x_ix_j|$$ where the $s_{ij}$ 's are given real numbers between $0$ and $M>0$ . In order to find the least value of $f$ , I want to apply a grid search by discretizing every variable $x_i$ . My question is the following: in order to restrict my grid search, is it possible to find a lower and an upper bound on $x_i^{\ast}$ for   an optimal solution $x^{\ast}$ ? I suspect that we have $0\leq x_i^{\ast} \leq M$ or maybe even better $0\leq x_i^{\ast} \leq \sqrt{M}$ but I am not able to prove it. It is clear that for the well-studied one-dimensional function $f(x)=\displaystyle\sum_{i=1}^n|s_{i}-x|$ , we have $\min_i(s_i)\leq x^{\ast} \leq \max_i(s_i)$ since $x^{\ast}$ is the median of the $s_i$ 's. Thank you very much!","The function to minimize has the following form: where the 's are given real numbers between and . In order to find the least value of , I want to apply a grid search by discretizing every variable . My question is the following: in order to restrict my grid search, is it possible to find a lower and an upper bound on for   an optimal solution ? I suspect that we have or maybe even better but I am not able to prove it. It is clear that for the well-studied one-dimensional function , we have since is the median of the 's. Thank you very much!",f:\mathbb{R}^n\rightarrow\mathbb{R} f(x)=\displaystyle\sum_{i=1}^n\sum_{j=1}^n|s_{ij}-x_ix_j| s_{ij} 0 M>0 f x_i x_i^{\ast} x^{\ast} 0\leq x_i^{\ast} \leq M 0\leq x_i^{\ast} \leq \sqrt{M} f(x)=\displaystyle\sum_{i=1}^n|s_{i}-x| \min_i(s_i)\leq x^{\ast} \leq \max_i(s_i) x^{\ast} s_i,"['calculus', 'analysis', 'optimization', 'absolute-value', 'median']"
59,Characteristic method to $u_t+au_x-u^2=1$,Characteristic method to,u_t+au_x-u^2=1,"Consider the following hyperbolic equation for $u=u(t,x)$ , $a\in\mathbb{R}$ $$ \frac{\partial u }{\partial t} + a\frac{\partial u }{\partial x} -u^2 = 1.  $$ I want to show that there is no global solution, that is, there exists a $t_*$ such that the solution $u$ goes to infinity as $t\rightarrow t_*$ . In order to do that I am thinking in showing that any two characteristic cruves intersec each other. My problem is to calculate the characteristic curves of the considered equation . Any help or idea is welcome! Thank you.","Consider the following hyperbolic equation for , I want to show that there is no global solution, that is, there exists a such that the solution goes to infinity as . In order to do that I am thinking in showing that any two characteristic cruves intersec each other. My problem is to calculate the characteristic curves of the considered equation . Any help or idea is welcome! Thank you.","u=u(t,x) a\in\mathbb{R}  \frac{\partial u }{\partial t} + a\frac{\partial u }{\partial x} -u^2 = 1.   t_* u t\rightarrow t_*","['analysis', 'partial-differential-equations', 'characteristics']"
60,Continuous extension in Implicit Function theorem?,Continuous extension in Implicit Function theorem?,,"In an application of the Implicit Function theorem, I asked myself the following question. Let $f\colon [a,b] \times \mathbb R \times \mathbb R \to \mathbb R$ be a continuously differentiable function on $(a,b) \times \mathbb R \times \mathbb R$ such that $f$ as well as all partial derivatives $\partial_if$ have continuous extensions to $[a,b] \times \mathbb R \times \mathbb R$ . Assume that $f(x,u,\xi) = 0$ and $\partial_\xi f(x,u,\xi) > 0$ for all $(x,u,\xi) \in [a,b] \times \mathbb R \times \mathbb R$ . By the implicit function theorem there exists a unique continuously differentiable $g:(a,b) \times \mathbb R \times \mathbb R \to \mathbb R$ such that $f(x,u,g(x,u)) = 0$ . Question: Can I also extend $g$ as well as all partials of $g$ continously to $[a,b] \times \mathbb R \times \mathbb R$ ? I assume I can at least for the partials since the implicit function theorem also provides me with an explicit formula for the latter from which I might be able to conclude. But what about $g$ itself?","In an application of the Implicit Function theorem, I asked myself the following question. Let be a continuously differentiable function on such that as well as all partial derivatives have continuous extensions to . Assume that and for all . By the implicit function theorem there exists a unique continuously differentiable such that . Question: Can I also extend as well as all partials of continously to ? I assume I can at least for the partials since the implicit function theorem also provides me with an explicit formula for the latter from which I might be able to conclude. But what about itself?","f\colon [a,b] \times \mathbb R \times \mathbb R \to \mathbb R (a,b) \times \mathbb R \times \mathbb R f \partial_if [a,b] \times \mathbb R \times \mathbb R f(x,u,\xi) = 0 \partial_\xi f(x,u,\xi) > 0 (x,u,\xi) \in [a,b] \times \mathbb R \times \mathbb R g:(a,b) \times \mathbb R \times \mathbb R \to \mathbb R f(x,u,g(x,u)) = 0 g g [a,b] \times \mathbb R \times \mathbb R g","['real-analysis', 'analysis', 'implicit-differentiation', 'implicit-function-theorem']"
61,Linear Operator in a Banach Space,Linear Operator in a Banach Space,,"I'm trying to get my head around linear operators and their usage with Banach spaces. Could someone help me understand how some properties relate to the following operator? We have Banach space $L^2$ , whose elements are real valued sequences $x = (ξ_{j} ) = (ξ_{1}, ξ_{2}, . . .)$ such that $\sum_{j=1}^∞ |ξ_{j}|^2 < ∞$ , Define the operator $T : L^2 → L^2$ as: $(T x) = (ξj/j)$ , for every $x = (ξ_{j} ) ∈ L^2$ That is, $T x = (ξ_{1},ξ_{2}/2, ξ_{3}/3, . . . ,)$ for every $x = (ξ_{1}, ξ_{2}, ξ_{3}, . . .) ∈ L^2$ It is easy to show that the operator T is linear (as it satisfies the definition). I think that the inverse of $T$ is $T^{-1} x = (j ξ_{j})$ , how can I show that T is both injective and surjective? Furthermore, how would one go about showing that T is bounded, continuous and then show what $||T||$ is?","I'm trying to get my head around linear operators and their usage with Banach spaces. Could someone help me understand how some properties relate to the following operator? We have Banach space , whose elements are real valued sequences such that , Define the operator as: , for every That is, for every It is easy to show that the operator T is linear (as it satisfies the definition). I think that the inverse of is , how can I show that T is both injective and surjective? Furthermore, how would one go about showing that T is bounded, continuous and then show what is?","L^2 x = (ξ_{j} ) = (ξ_{1}, ξ_{2}, . . .) \sum_{j=1}^∞ |ξ_{j}|^2 < ∞ T : L^2 → L^2 (T x) = (ξj/j) x = (ξ_{j} ) ∈ L^2 T x = (ξ_{1},ξ_{2}/2, ξ_{3}/3, . . . ,) x = (ξ_{1}, ξ_{2}, ξ_{3}, . . .) ∈ L^2 T T^{-1} x = (j ξ_{j}) ||T||","['functional-analysis', 'analysis', 'operator-theory']"
62,convergence in a special sequense,convergence in a special sequense,,If $(a_{n})$ is a decreasing sequence which is positive for all $n$ and the series $a_{1}+a_{2}+a_{3}+...$ is divergent:prove this sequence is convergent to one $(a_{1}+a_{3}+...+a_{2n-1})/(a_{2}+a_{4}+...+a_{2n})$ i have tried solving this a couple of times but had no luck at first i tried to list when a decreasing  sequence is divergent but i couldn't match the result with the rest of the assumptions and it has been a while since i have had this at the back of my mind so if you have any input  your help would be appreciated.,If is a decreasing sequence which is positive for all and the series is divergent:prove this sequence is convergent to one i have tried solving this a couple of times but had no luck at first i tried to list when a decreasing  sequence is divergent but i couldn't match the result with the rest of the assumptions and it has been a while since i have had this at the back of my mind so if you have any input  your help would be appreciated.,(a_{n}) n a_{1}+a_{2}+a_{3}+... (a_{1}+a_{3}+...+a_{2n-1})/(a_{2}+a_{4}+...+a_{2n}),"['sequences-and-series', 'analysis']"
63,$f^n(x)$ where $n \in \mathbb{R}^+$,where,f^n(x) n \in \mathbb{R}^+,"Given a function $f(x)$ whose image is a subset of its domain, we can define $$ f^n(x) = \underbrace{f(f(f(\dots f(x) \dots )))}_{n \text{ times}} $$ This makes sense when $n$ is a nonnegative integer. Can we extend this definition to continuous values of $n$ ? Such as $f^{\frac{1}{2}}(x)$ ?","Given a function whose image is a subset of its domain, we can define This makes sense when is a nonnegative integer. Can we extend this definition to continuous values of ? Such as ?","f(x) 
f^n(x) = \underbrace{f(f(f(\dots f(x) \dots )))}_{n \text{ times}}
 n n f^{\frac{1}{2}}(x)",['analysis']
64,Definite integral of split function,Definite integral of split function,,Given the split functions $$f_{\lambda}(x) = \begin{cases}        \lambda\exp(-\lambda x) &\quad\text{if } x>0\\        0&\quad\text{otherwise.}\\ \end{cases}$$ The solution in my textbook to the definite integral $$\int_{-\infty}^{\infty}f_{\lambda}(x)dx$$ is the following $$ \begin{aligned} \int_{-\infty}^{\infty} f_{\lambda}(x) d x &=\int_{0}^{\infty} \lambda \exp (-\lambda x) d x=[-\exp (-\lambda x)]_{0}^{\infty} \\ &=\lim _{s \rightarrow \infty}-\exp (-\lambda s)+\exp (0)=0+1=1 \end{aligned} $$ But since the $\lambda\exp(-\lambda x)$ part of the split function isn't defined at 0 i am unsure as to why we can insert it into the function when we insert the boundaries to find the definite integral? Meaning why is the solution not the following $$ \begin{aligned} \int_{-\infty}^{\infty} f_{\lambda}(x) d x &=\int_{0}^{\infty} \lambda \exp (-\lambda x) d x=[-\exp (-\lambda x)]_{0}^{\infty} \\ &=\lim _{s \rightarrow \infty}-\exp (-\lambda s)+0=0+0=0\end{aligned} $$,Given the split functions The solution in my textbook to the definite integral is the following But since the part of the split function isn't defined at 0 i am unsure as to why we can insert it into the function when we insert the boundaries to find the definite integral? Meaning why is the solution not the following,"f_{\lambda}(x) = \begin{cases}
       \lambda\exp(-\lambda x) &\quad\text{if } x>0\\
       0&\quad\text{otherwise.}\\
\end{cases} \int_{-\infty}^{\infty}f_{\lambda}(x)dx 
\begin{aligned} \int_{-\infty}^{\infty} f_{\lambda}(x) d x &=\int_{0}^{\infty} \lambda \exp (-\lambda x) d x=[-\exp (-\lambda x)]_{0}^{\infty} \\ &=\lim _{s \rightarrow \infty}-\exp (-\lambda s)+\exp (0)=0+1=1 \end{aligned}
 \lambda\exp(-\lambda x) 
\begin{aligned} \int_{-\infty}^{\infty} f_{\lambda}(x) d x &=\int_{0}^{\infty} \lambda \exp (-\lambda x) d x=[-\exp (-\lambda x)]_{0}^{\infty} \\ &=\lim _{s \rightarrow \infty}-\exp (-\lambda s)+0=0+0=0\end{aligned}
","['real-analysis', 'integration']"
65,What is a formal example of a non-convergent Cauchy sequence in a metric space of rationals?,What is a formal example of a non-convergent Cauchy sequence in a metric space of rationals?,,"With respect to a space $\mathbb{Q}$ of purely rational numbers with the standard metric $|p-q|$ and its properties, how does one prove a series like $$\sum_{n=0}^{\infty}\frac{(2k+1)!}{2^{3k+1}(k!)^{2}} \ \ \text{or} \ \  \sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n}$$ is Cauchy but does not converge in that space, without relying on knowing pre-hand that these converge to $\ln(2)$ and $\sqrt{2}$ respectively? This means if you were to address the problem, you would pretend only rational numbers exist and act as though you don't know any explicit values that are irrational.","With respect to a space of purely rational numbers with the standard metric and its properties, how does one prove a series like is Cauchy but does not converge in that space, without relying on knowing pre-hand that these converge to and respectively? This means if you were to address the problem, you would pretend only rational numbers exist and act as though you don't know any explicit values that are irrational.",\mathbb{Q} |p-q| \sum_{n=0}^{\infty}\frac{(2k+1)!}{2^{3k+1}(k!)^{2}} \ \ \text{or} \ \  \sum_{n=1}^{\infty}\frac{(-1)^{n+1}}{n} \ln(2) \sqrt{2},"['sequences-and-series', 'analysis', 'cauchy-sequences']"
66,"Is it meaningful to say ""take a random measurable subset of [0, 1] with measure 0.5""?","Is it meaningful to say ""take a random measurable subset of [0, 1] with measure 0.5""?",,"Is it meaningful to say take a random measurable subset of [0, 1] with measure 0.5? If it is, is it then true that for any $r\in[0, 1]$ , the probability that $r$ is in a random measurable subset of $[0, 1]$ with measure 0.5 is $0.5$ ? Context: I was studying entropy encoding and thought that one way to think about why entropy encoding efficient is this: for each finite code word $s$ of length $d$ , assign a subset $A = f(s)$ of $[0, 1)$ such that $\{f(s) : s \text{ is a code word of length } d\}$ partitions $[0, 1)$ , and $$Pr(s' = s | s' \text{ is of length }d) = |A|$$ For example, in arithmetic encoding, we have $f(0) = [0, 1/2), f(1) = [1/2, 1), f(00) = [0, 1/4), ...$ . Then, knowing the length $d$ , $f(s)$ can be represented by any real number $\bar s\in f(s)$ . Now, the crucial part of entropy encoding is that in general, for each code $s$ , there exists a real number $\bar s\in f(s)$ that is representable in under $$-\log_2 (Pr(s' = s | s' \text{ is of length }d)) = -\log_2 |f(s)|$$ bits. For example, the binary string $b_1...b_n$ represents the fraction $(b_1...b_n)_2/2^n$ , where $(b_1...b_n)_2$ denotes the string decoded as the binary code of an integer. If we assume that it is nonzero, then $b_n$ can always be chosen as $1$ , that is, $(b_1...b_n)_2$ is an odd number. There are $2^{n-1}$ such numbers, and indeed, since $b_n$ is always $1$ , it can be dropped. So the new encoding would have $b_1...b_{n-1} \mapsto (b_1...b_{n-1}1 )_2/2^n$ . However, I thought that it is not necessary to use binary fractions. The crucial thing is that we have an injective ""coding"" function $\{0, 1\}^n \to [0, 1)$ , and on average , each $f(s)$ contains one image of the coding function. Suppose it can be proven that for any $r\in [0, 1)$ , the probability that a random measurable subset $S\in[0, 1)$ contains $r$ is $|S|$ , then it is immediate that arithmetic coding would work, because on average each $f(s)$ would contain one number with codelength $-\log_2|f(s)|$ .","Is it meaningful to say take a random measurable subset of [0, 1] with measure 0.5? If it is, is it then true that for any , the probability that is in a random measurable subset of with measure 0.5 is ? Context: I was studying entropy encoding and thought that one way to think about why entropy encoding efficient is this: for each finite code word of length , assign a subset of such that partitions , and For example, in arithmetic encoding, we have . Then, knowing the length , can be represented by any real number . Now, the crucial part of entropy encoding is that in general, for each code , there exists a real number that is representable in under bits. For example, the binary string represents the fraction , where denotes the string decoded as the binary code of an integer. If we assume that it is nonzero, then can always be chosen as , that is, is an odd number. There are such numbers, and indeed, since is always , it can be dropped. So the new encoding would have . However, I thought that it is not necessary to use binary fractions. The crucial thing is that we have an injective ""coding"" function , and on average , each contains one image of the coding function. Suppose it can be proven that for any , the probability that a random measurable subset contains is , then it is immediate that arithmetic coding would work, because on average each would contain one number with codelength .","r\in[0, 1] r [0, 1] 0.5 s d A = f(s) [0, 1) \{f(s) : s \text{ is a code word of length } d\} [0, 1) Pr(s' = s | s' \text{ is of length }d) = |A| f(0) = [0, 1/2), f(1) = [1/2, 1), f(00) = [0, 1/4), ... d f(s) \bar s\in f(s) s \bar s\in f(s) -\log_2 (Pr(s' = s | s' \text{ is of length }d)) = -\log_2 |f(s)| b_1...b_n (b_1...b_n)_2/2^n (b_1...b_n)_2 b_n 1 (b_1...b_n)_2 2^{n-1} b_n 1 b_1...b_{n-1} \mapsto (b_1...b_{n-1}1 )_2/2^n \{0, 1\}^n \to [0, 1) f(s) r\in [0, 1) S\in[0, 1) r |S| f(s) -\log_2|f(s)|","['probability', 'analysis', 'entropy']"
67,"If a trigonometric series converges to a function $f$, is the Fourier series of $f$ the original series?","If a trigonometric series converges to a function , is the Fourier series of  the original series?",f f,"If a trigonometric series $\frac{1}{2}a_0+\sum_{n=1}^\infty (a_n\cos(nx)+b_n\sin(nx))$ converges pointwise to a function $f$ , then is the Fourier series of $f$ the original trigonometric series? I know that if we have uniform convergence, then we can integrate term by term and the conclusion is true. But what if we only have pointwise convergence? I appreciate any help!","If a trigonometric series converges pointwise to a function , then is the Fourier series of the original trigonometric series? I know that if we have uniform convergence, then we can integrate term by term and the conclusion is true. But what if we only have pointwise convergence? I appreciate any help!",\frac{1}{2}a_0+\sum_{n=1}^\infty (a_n\cos(nx)+b_n\sin(nx)) f f,"['analysis', 'fourier-analysis', 'fourier-series', 'trigonometric-series']"
68,Proving norm closure of a set of functions in $L^1$,Proving norm closure of a set of functions in,L^1,"I'm working at optimal control and I came across this situation. We have a set of functions that consists of all the measurable functions with values in $U \subset \mathbb{R}^m$ a.e., where $U$ is compact. Consider a function $$F:\mathbb{R} \times \mathbb{R}^m \to \mathbb{R}^n$$ $$(t,u) \mapsto F(t,u)$$ such that $F$ is continuous in $u$ for each $t$ and such that $F$ is measurable in $t$ for each $u$ . Clearly the set of functions defined on a finite interval $I$ $$K = \{u \mid u(t) \in U \; a.e. \; \mathrm{in} \; I \}$$ is closed in $\Vert .\Vert_1$ . Here, as usual, by $\Vert .\Vert_p$ of a function $f : \mathbb{R} \to \mathbb{R}^n$ we mean $$\Vert f\Vert_p^p = \int_I \sum_{i=1}^n \mid f^i(t) \mid^p dt$$ where $f^i$ are the coordinate functions of $f$ . $\textbf{The problem is}$ Prove that the set of functions $$H = \{v \mid v(t) = F(t,u(t)) \; u \in K \}$$ is closed too. We have also the hypotesis that $$\Vert F(t,u) \Vert \leq m(t)$$ with $m$ a function that is integrable on the finite intervals of $\mathbb{R}$ . I tried, but I can't prove it. Does someone have any ideas? Thanks to everyone who wants to help me! P.S. Note that if $F$ is injective in $u$ a.e. then the statement is clearly true, and note also that if $m=1$ the statement is clearly true either. $\textbf{Edit}$ Maybe I found something. Let $u_n$ be the sequence of functions as above. Then all these functions are in $L^2(I)$ , thanks to the fact that $U$ is compact. The sequence is also bounded, clearly. $L^2$ is reflexive, so we can use the following well known theorem of functional analysis $\textbf{Theorem}$ Let $E$ be a reflexive normed space and $x_n$ a bounded sequence. Then there exists a sub-sequence converging weakly. In our case, we can then extract a subsequence $u_{n_k}$ converging weakly to something, and so it must be pointwise convergent to some function in K, thanks to the closure of $U$ . (Clearly this doesn't work as weakly convergence doesn't implie pointwise convergence.) $\textbf{Edit}$ If $u$ was a function with real values (not in $\mathbb{R}^m$ ) we could just take $\limsup u_n(t)$ . Is there something similar we can do in general here? $\textbf{Edit}$ Maybe we could define $f(s) = \lim_{n \to \infty} F(s,u_n(s))$ and then consider the multivalued function $$G(t) = \left\{u \in U \mid f(t) = F(t,u) \right\}$$ Now if we can prove that there exists at least one measurable selection of $G(t)$ we could take that selection to prove the closure of $H$ ! Now $G(t)$ is closed valued and non empty, so by a known theorem there exists a measurable selection if $G$ is measurable... so can someone prove that $G$ is measurable? $\textbf{Definition}$ A set valued map $G: \Omega \to \mathbb{R}^n$ is measurable if for every closed set $C \subset \mathbb{R}^n$ the set $$\left\{t \in \Omega \mid G(t) \cap C \neq \emptyset \right\}$$ is measurable.","I'm working at optimal control and I came across this situation. We have a set of functions that consists of all the measurable functions with values in a.e., where is compact. Consider a function such that is continuous in for each and such that is measurable in for each . Clearly the set of functions defined on a finite interval is closed in . Here, as usual, by of a function we mean where are the coordinate functions of . Prove that the set of functions is closed too. We have also the hypotesis that with a function that is integrable on the finite intervals of . I tried, but I can't prove it. Does someone have any ideas? Thanks to everyone who wants to help me! P.S. Note that if is injective in a.e. then the statement is clearly true, and note also that if the statement is clearly true either. Maybe I found something. Let be the sequence of functions as above. Then all these functions are in , thanks to the fact that is compact. The sequence is also bounded, clearly. is reflexive, so we can use the following well known theorem of functional analysis Let be a reflexive normed space and a bounded sequence. Then there exists a sub-sequence converging weakly. In our case, we can then extract a subsequence converging weakly to something, and so it must be pointwise convergent to some function in K, thanks to the closure of . (Clearly this doesn't work as weakly convergence doesn't implie pointwise convergence.) If was a function with real values (not in ) we could just take . Is there something similar we can do in general here? Maybe we could define and then consider the multivalued function Now if we can prove that there exists at least one measurable selection of we could take that selection to prove the closure of ! Now is closed valued and non empty, so by a known theorem there exists a measurable selection if is measurable... so can someone prove that is measurable? A set valued map is measurable if for every closed set the set is measurable.","U \subset \mathbb{R}^m U F:\mathbb{R} \times \mathbb{R}^m \to \mathbb{R}^n (t,u) \mapsto F(t,u) F u t F t u I K = \{u \mid u(t) \in U \; a.e. \; \mathrm{in} \; I \} \Vert .\Vert_1 \Vert .\Vert_p f : \mathbb{R} \to \mathbb{R}^n \Vert f\Vert_p^p = \int_I \sum_{i=1}^n \mid f^i(t) \mid^p dt f^i f \textbf{The problem is} H = \{v \mid v(t) = F(t,u(t)) \; u \in K \} \Vert F(t,u) \Vert \leq m(t) m \mathbb{R} F u m=1 \textbf{Edit} u_n L^2(I) U L^2 \textbf{Theorem} E x_n u_{n_k} U \textbf{Edit} u \mathbb{R}^m \limsup u_n(t) \textbf{Edit} f(s) = \lim_{n \to \infty} F(s,u_n(s)) G(t) = \left\{u \in U \mid f(t) = F(t,u) \right\} G(t) H G(t) G G \textbf{Definition} G: \Omega \to \mathbb{R}^n C \subset \mathbb{R}^n \left\{t \in \Omega \mid G(t) \cap C \neq \emptyset \right\}","['real-analysis', 'functional-analysis', 'analysis', 'optimal-control']"
69,What is exactly relationship between scheme $X$ and its global sections in comparison to $C^\star$ algebra,What is exactly relationship between scheme  and its global sections in comparison to  algebra,X C^\star,"This is just random thought based upon Iitaka's Algebraic Geometry Thm 1.15 proof. Let $X$ be a scheme and denote $A(X)$ ring of global sections of $X$ associated to the scheme. In the proof of $Spec,A(-)$ adjunction(to see adjunction, use $Sch^{op}$ category instead), $Hom_{Sch}(X,Spec B)\cong Hom_{Ring}(B,A(X))$ , the book constructed an arrow $X\to Spec(A(X))$ which enjoys universal arrow property. In other words, given $X\to Spec(B)$ , the map $X\to Spec(B)$ factors through $X\to Spec(A(X))\to Spec(B)$ . Recall Gelfand transform.(in accordance to Lang's Real and Functional Analysis book.) Let $A$ be a commutative Banach algebra. Let $M$ be the set of maximal ideals of $A$ . Now we have $A\to C(M,C)$ map where $M$ is really in bijection to characters of $A$ . So in effectives $A\to C(\hat{A},C)$ where $\hat{A}$ is the set of characters and this map is given by evaluation. $\textbf{Q:}$ It is clear that $A(X)$ is functions globally defined over $X$ . Both construction maps starting space into a function space. What is exactly analogy between them?(i.e. $X\to Spec(A(X))$ and $A\to C(\hat{A},C)$ ) Note that $X\to Spec(A(X))$ is an injection for topological space and for sheaves, there is no guarantee to be injection whereas $A\to C(\hat{A},C)$ is just a map which may not be bijection unless $A$ is commutative unital Banach algebra.","This is just random thought based upon Iitaka's Algebraic Geometry Thm 1.15 proof. Let be a scheme and denote ring of global sections of associated to the scheme. In the proof of adjunction(to see adjunction, use category instead), , the book constructed an arrow which enjoys universal arrow property. In other words, given , the map factors through . Recall Gelfand transform.(in accordance to Lang's Real and Functional Analysis book.) Let be a commutative Banach algebra. Let be the set of maximal ideals of . Now we have map where is really in bijection to characters of . So in effectives where is the set of characters and this map is given by evaluation. It is clear that is functions globally defined over . Both construction maps starting space into a function space. What is exactly analogy between them?(i.e. and ) Note that is an injection for topological space and for sheaves, there is no guarantee to be injection whereas is just a map which may not be bijection unless is commutative unital Banach algebra.","X A(X) X Spec,A(-) Sch^{op} Hom_{Sch}(X,Spec B)\cong Hom_{Ring}(B,A(X)) X\to Spec(A(X)) X\to Spec(B) X\to Spec(B) X\to Spec(A(X))\to Spec(B) A M A A\to C(M,C) M A A\to C(\hat{A},C) \hat{A} \textbf{Q:} A(X) X X\to Spec(A(X)) A\to C(\hat{A},C) X\to Spec(A(X)) A\to C(\hat{A},C) A","['abstract-algebra', 'functional-analysis', 'analysis', 'algebraic-geometry', 'c-star-algebras']"
70,"Test for zeros in [0,1] of polynomial in $\mathbb{Z}[x]$","Test for zeros in [0,1] of polynomial in",\mathbb{Z}[x],"I'm looking for a general non-constructive way (I assume there isn't a constructive way in general) to test if a polynomial with integer coefficients has a zero in [0,1]. I am aware of the rational root theorem, but I don't believe this is useful in all cases. Could anyone point me toward literature on this subject? Or directly tell me if such a test is possible and/or what it is? This question can be approached in many ways - sorry if I have tagged incorrectly!","I'm looking for a general non-constructive way (I assume there isn't a constructive way in general) to test if a polynomial with integer coefficients has a zero in [0,1]. I am aware of the rational root theorem, but I don't believe this is useful in all cases. Could anyone point me toward literature on this subject? Or directly tell me if such a test is possible and/or what it is? This question can be approached in many ways - sorry if I have tagged incorrectly!",,"['analysis', 'algebraic-geometry', 'polynomials']"
71,"if $h$ is twice differentiable, what is the largest set on which $|h|$ is twice differentiable?","if  is twice differentiable, what is the largest set on which  is twice differentiable?",h |h|,"Let $h:\mathbb R\to\mathbb R$ be twice differentiable. What is the largest set on which $|h|$ is twice differentiable? By the chain rule, $|h|$ is differentiable at $a$ with $$|h|'(a)=h'(a)\operatorname{sgn}h(a)\tag1$$ for all $a\in\{h\ne0\}$ . Moreover, if $a\in\{h'=0\}$ , then $$\left|\frac{|h|(a+t)-|h|(a)}t\right|\le\left|\frac{h(a+t)-h(a)}t\right|\xrightarrow{t\to0}0\tag2$$ by the reverse triangle inequaly and hence $|h|$ is differentiable at $a$ with $$|h|'(a)=0=h'(a)\operatorname{sgn}h(a)\tag3.$$ So, $|h|$ is differentiable at least on $D_1:=\{h\ne0\}\cup\{h'=0\}$ with derivative $h'\operatorname{sgn}h$ . (Can we enlarge $D_1$ ?) Turning to the second derivative: Using that $\operatorname{sgn}h$ is differentiable at $a$ with $$(\operatorname{sgn}h)'(a)=0\tag4$$ for all $a\in\{h\ne0\}\cup\{h=0\}^\circ$ , we obtain (by the chain rule, again) that $|h|$ is twice differentiable at $a$ with $$|h|''(a)=h''(a)\operatorname{sgn}h(a)\tag5$$ for all $a\in D_2:=\{h\ne0\}\cup\{h=0\}^\circ$ (noting that $\{h=0\}^\circ\subseteq\{h'=0\}$ ). So, $|h|'$ is differentiable at least on $D_2$ . Can we enlarge $D_2$ ? On the other hand, we can show that $|h|'$ is differentiable at $a$ with $$|h|''(a)=|h''(a)|$$ for all $$a\in D_3:=\{h=0\}\cap\{h'=0\}\cap(\{h''\ne0\}\cup\{h''=0\}\cap D_1^\circ).$$ However, I'm struggling to see how $D_2$ and $D_3$ are related and hence whether the latter yields an enlargement of $D_2$ . EDIT : And as a third option, it's possible to show that $|h|$ is twice differentiable on $D_4:=\mathbb R\setminus\overline{N'}$ , where $N':=\left\{a\in\mathbb R:a\text{ is an isolated point of }\left\{h=0\right\}\right\}$ ; see revision 3 of this answer (and the comments below): https://math.stackexchange.com/a/3210082/47771 . I'm really struggling to see which result is the strongest.","Let be twice differentiable. What is the largest set on which is twice differentiable? By the chain rule, is differentiable at with for all . Moreover, if , then by the reverse triangle inequaly and hence is differentiable at with So, is differentiable at least on with derivative . (Can we enlarge ?) Turning to the second derivative: Using that is differentiable at with for all , we obtain (by the chain rule, again) that is twice differentiable at with for all (noting that ). So, is differentiable at least on . Can we enlarge ? On the other hand, we can show that is differentiable at with for all However, I'm struggling to see how and are related and hence whether the latter yields an enlargement of . EDIT : And as a third option, it's possible to show that is twice differentiable on , where ; see revision 3 of this answer (and the comments below): https://math.stackexchange.com/a/3210082/47771 . I'm really struggling to see which result is the strongest.",h:\mathbb R\to\mathbb R |h| |h| a |h|'(a)=h'(a)\operatorname{sgn}h(a)\tag1 a\in\{h\ne0\} a\in\{h'=0\} \left|\frac{|h|(a+t)-|h|(a)}t\right|\le\left|\frac{h(a+t)-h(a)}t\right|\xrightarrow{t\to0}0\tag2 |h| a |h|'(a)=0=h'(a)\operatorname{sgn}h(a)\tag3. |h| D_1:=\{h\ne0\}\cup\{h'=0\} h'\operatorname{sgn}h D_1 \operatorname{sgn}h a (\operatorname{sgn}h)'(a)=0\tag4 a\in\{h\ne0\}\cup\{h=0\}^\circ |h| a |h|''(a)=h''(a)\operatorname{sgn}h(a)\tag5 a\in D_2:=\{h\ne0\}\cup\{h=0\}^\circ \{h=0\}^\circ\subseteq\{h'=0\} |h|' D_2 D_2 |h|' a |h|''(a)=|h''(a)| a\in D_3:=\{h=0\}\cap\{h'=0\}\cap(\{h''\ne0\}\cup\{h''=0\}\cap D_1^\circ). D_2 D_3 D_2 |h| D_4:=\mathbb R\setminus\overline{N'} N':=\left\{a\in\mathbb R:a\text{ is an isolated point of }\left\{h=0\right\}\right\},"['calculus', 'analysis', 'derivatives']"
72,Group homomorphism on $\mathbb{R}$,Group homomorphism on,\mathbb{R},Let $f :\mathbb{R} \rightarrow \mathbb{R}$ be a group homomorphism W. R. T.  Usual addition such that $f$ is bounded at a neighborhood of $0$ . Can you conclude that $f$ is continuous at $0$ ?,Let be a group homomorphism W. R. T.  Usual addition such that is bounded at a neighborhood of . Can you conclude that is continuous at ?,f :\mathbb{R} \rightarrow \mathbb{R} f 0 f 0,"['abstract-algebra', 'analysis']"
73,Fundamental Lemma of Variational Calculus,Fundamental Lemma of Variational Calculus,,"I'm reading over the following lemma and proof related to the fundamental lemma of variational calculus. If $\color{red}{\alpha ∈ C([a,b])}$ and $$\int^b_a [\alpha(x)h(x) + \beta(x)h'(x)] \ dx = 0$$ for every function $h ∈ C^1([a,b])$ such that $h(a) = h(b) = 0$ , then (1) $\beta(x)$ is differentiable almost everywhere on $[a, b]$ (2) $\beta'(x)$ = $\alpha(x)$ I understand the proof for the above lemma.  However, one of the exercises at the end of the section is as follows. If $\color{red}{\alpha ∈ L^1([a,b])}$ and $$\int^b_a [\alpha(x)h(x) + \beta(x)h'(x)] \ dx = 0$$ for every function $h ∈ C^1([a,b])$ such that $h(a) = h(b) = 0$ , then (1) $\beta(x)$ is differentiable almost everywhere on $[a, b]$ (2) $\beta'(x)$ = $\alpha(x)$ My question: wouldn't both proofs be relatively the same?","I'm reading over the following lemma and proof related to the fundamental lemma of variational calculus. If and for every function such that , then (1) is differentiable almost everywhere on (2) = I understand the proof for the above lemma.  However, one of the exercises at the end of the section is as follows. If and for every function such that , then (1) is differentiable almost everywhere on (2) = My question: wouldn't both proofs be relatively the same?","\color{red}{\alpha ∈ C([a,b])} \int^b_a [\alpha(x)h(x) + \beta(x)h'(x)] \ dx = 0 h ∈ C^1([a,b]) h(a) = h(b) = 0 \beta(x) [a, b] \beta'(x) \alpha(x) \color{red}{\alpha ∈ L^1([a,b])} \int^b_a [\alpha(x)h(x) + \beta(x)h'(x)] \ dx = 0 h ∈ C^1([a,b]) h(a) = h(b) = 0 \beta(x) [a, b] \beta'(x) \alpha(x)","['functional-analysis', 'analysis']"
74,Find the modulus of continuity of a function,Find the modulus of continuity of a function,,"I have the following Dirichlet problem \begin{equation} \begin{cases} a(x)Du\cdot Du-b(x)\cdot Du=0 \ \ \ \ \text{in} \ \Omega, \\u(x) = g(x) \ \ \ \ \text{on} \ \partial\Omega. \end{cases} \end{equation} Here $\Omega$ is an open bounded subset of $\mathbb{R}^N$ , $a \in C(\bar\Omega, \mathbb{R}^{N \times N})$ , $b \in C(\bar\Omega, \mathbb{R}^N)$ and $g \in C(\partial\Omega)$ . I make the following assumptions: 1) $a(x)\xi \cdot \xi \geq |\xi|^2$ for $x \in \Omega$ and $\xi \in \mathbb{R}^N$ . 2) There is a function $\psi \in C^1(\bar\Omega)$ such that $b(x) \cdot D\psi(x) \geq 1$ for $x \in \Omega$ . Now, I define $H(p,x) = a(x)p \cdot p- b(x) \cdot p$ How can I prove (if it is possible) that $H$ satisfies \begin{equation} |H(x,p)-H(y,p)| \leq \omega(|x-y|(1+|p|)) \ \ \ \text{for} \ x,y \in \Omega \ \text{and} \ p \in \mathbb{R}^N? \end{equation} Here $\omega$ is a modulus, i.e. a function $\omega\colon[0,+\infty[\to[0,+\infty[$ continuous, nondecreasing, and such that $\omega(0) = 0$ . My attempt: \begin{align} |H(x,p)-H(y,p)|&=|a(x)p \cdot p- b(x) \cdot p - a(y)p \cdot p + b(y) \cdot p|\\ &=|(a(x)-a(y))p \cdot p - (b(x)-b(y)) \cdot p| \end{align} but I don't know how to proceed from here. Any help would be appreciated. Thanks in advance.","I have the following Dirichlet problem Here is an open bounded subset of , , and . I make the following assumptions: 1) for and . 2) There is a function such that for . Now, I define How can I prove (if it is possible) that satisfies Here is a modulus, i.e. a function continuous, nondecreasing, and such that . My attempt: but I don't know how to proceed from here. Any help would be appreciated. Thanks in advance.","\begin{equation}
\begin{cases} a(x)Du\cdot Du-b(x)\cdot Du=0 \ \ \ \ \text{in} \ \Omega, \\u(x) = g(x) \ \ \ \ \text{on} \ \partial\Omega. \end{cases}
\end{equation} \Omega \mathbb{R}^N a \in C(\bar\Omega, \mathbb{R}^{N \times N}) b \in C(\bar\Omega, \mathbb{R}^N) g \in C(\partial\Omega) a(x)\xi \cdot \xi \geq |\xi|^2 x \in \Omega \xi \in \mathbb{R}^N \psi \in C^1(\bar\Omega) b(x) \cdot D\psi(x) \geq 1 x \in \Omega H(p,x) = a(x)p \cdot p- b(x) \cdot p H \begin{equation}
|H(x,p)-H(y,p)| \leq \omega(|x-y|(1+|p|)) \ \ \ \text{for} \ x,y \in \Omega \ \text{and} \ p \in \mathbb{R}^N?
\end{equation} \omega \omega\colon[0,+\infty[\to[0,+\infty[ \omega(0) = 0 \begin{align}
|H(x,p)-H(y,p)|&=|a(x)p \cdot p- b(x) \cdot p - a(y)p \cdot p + b(y) \cdot p|\\
&=|(a(x)-a(y))p \cdot p - (b(x)-b(y)) \cdot p|
\end{align}","['analysis', 'partial-differential-equations']"
75,Motion of a particle described by a simple differential equation,Motion of a particle described by a simple differential equation,,"Let $X_0 = \frac12$ . Let $a : \mathbb R_+ \to \{-1,0,1\}$ be a measurable function. A particle moves in the interval $[0,1]$ as follows: $$ dX_t = a_t dt \implies X_t = X_0 + \int_0^t a_s ds$$ where the integral is the Lebesgue integral. Both $0$ and $1$ are absorbing, i.e. once $X_t$ hits $0$ or $1$ it remains there. Suppose $a_t$ is Markovian. That is, $\exists$ a function $g: [0,1] \to \{-1,0,1\}$ such that, $a_s = g(X_s)$ for all $s$ . Therefore, $$X_t = X_0 + \int_0^t g(X_s) ds$$ I would like to claim the following: Claim: $X_t$ moves only in one direction. That is, if $X_t = x >X_0$ for some $t$ . Then, $\nexists s > t$ such that $X_s \in (X_0,x)$ . The reasoning is the following. Since $X_t$ reached $x$ and $a_t$ is Markovian, for any $y \in (X_0,x)$ , $a(y) = 1$ (with some abuse of notation). So, it should be impossible for the particle to return back to $y$ once it moves forward. How do I argue this formally? Can we write the claim for when $a_t$ is everywhere Markovian? Thanks.","Let . Let be a measurable function. A particle moves in the interval as follows: where the integral is the Lebesgue integral. Both and are absorbing, i.e. once hits or it remains there. Suppose is Markovian. That is, a function such that, for all . Therefore, I would like to claim the following: Claim: moves only in one direction. That is, if for some . Then, such that . The reasoning is the following. Since reached and is Markovian, for any , (with some abuse of notation). So, it should be impossible for the particle to return back to once it moves forward. How do I argue this formally? Can we write the claim for when is everywhere Markovian? Thanks.","X_0 = \frac12 a : \mathbb R_+ \to \{-1,0,1\} [0,1]  dX_t = a_t dt \implies X_t = X_0 + \int_0^t a_s ds 0 1 X_t 0 1 a_t \exists g: [0,1] \to \{-1,0,1\} a_s = g(X_s) s X_t = X_0 + \int_0^t g(X_s) ds X_t X_t = x >X_0 t \nexists s > t X_s \in (X_0,x) X_t x a_t y \in (X_0,x) a(y) = 1 y a_t","['calculus', 'analysis', 'dynamical-systems']"
76,What is the geometric intuition of $n/p$?,What is the geometric intuition of ?,n/p,"(volume for ball): Let B $_{r}^{p}(0):=\{x\in\mathbb R^n; \|x\|_p\leq r\}$ . Then the volume of B $_{r}^{p}(0)$ is \begin{align} {\rm V}_r^{p}=2^n\cdot\frac{\left\{\Gamma(\frac{1}{p}+1)\right\}^n}{\Gamma\left(\frac{n}{p}+1\right)}\cdot r^n. \quad\text{(calculation of multi-integral)} \end{align} (Sobolev embedding): Sobolev space $W^{k,p}(n)(1\leq p<\infty)\rightarrow \frac{n}{p}-k:=i$ . If $0<i$ , then $W^{k,p}\hookrightarrow W^{\ell,q}$ , where $\frac{n}{q}-\ell=i, k>\ell$ . If $0>i$ , then $W^{k,p}\hookrightarrow C^{r,\alpha}$ , where $-(r+\alpha)=i, 0<\alpha\leq1$ . Notice that there is a well-marked factor \begin{align} \frac{n}{p}=\frac{\text{dimension of space}}{\text{norm index of space}}.\end{align} What is the significance or geometric intuition of $\frac{n}{p}$ , which I think is essential and fundamental? (Maybe I think too much, thanks!)","(volume for ball): Let B . Then the volume of B is (Sobolev embedding): Sobolev space . If , then , where . If , then , where . Notice that there is a well-marked factor What is the significance or geometric intuition of , which I think is essential and fundamental? (Maybe I think too much, thanks!)","_{r}^{p}(0):=\{x\in\mathbb R^n; \|x\|_p\leq r\} _{r}^{p}(0) \begin{align}
{\rm V}_r^{p}=2^n\cdot\frac{\left\{\Gamma(\frac{1}{p}+1)\right\}^n}{\Gamma\left(\frac{n}{p}+1\right)}\cdot r^n. \quad\text{(calculation of multi-integral)}
\end{align} W^{k,p}(n)(1\leq p<\infty)\rightarrow \frac{n}{p}-k:=i 0<i W^{k,p}\hookrightarrow W^{\ell,q} \frac{n}{q}-\ell=i, k>\ell 0>i W^{k,p}\hookrightarrow C^{r,\alpha} -(r+\alpha)=i, 0<\alpha\leq1 \begin{align}
\frac{n}{p}=\frac{\text{dimension of space}}{\text{norm index of space}}.\end{align} \frac{n}{p}",[]
77,Non-homeomorphic subsets of the Cantor set,Non-homeomorphic subsets of the Cantor set,,"Are there uncountably many subsets of the Cantor set such that they are not homeomorphic to each other? Motivation: Let $X$ be the space of infinite binary tree. Then $Ends(X):=\varprojlim_{K,\text{compact}}\pi_0(X-K)$ has to topology of Cantor set. Let $\Sigma_X$ be a surface which handles are glued to a $2$ -sphere along the tree $X$ . Then $Ends(X)=Ends(\Sigma_X)$ has the topology of Cantor set. If there are uncountably many subsets of the Cantor set such that they are not homeomorphic to each other, then I can construct an uncountably infinite family of surfaces $\{\Sigma_{X'}\}_{X'\subseteq X}$ such that they are pairwise non-homeomorphic, and each is obtained by gluing countably many handles.","Are there uncountably many subsets of the Cantor set such that they are not homeomorphic to each other? Motivation: Let be the space of infinite binary tree. Then has to topology of Cantor set. Let be a surface which handles are glued to a -sphere along the tree . Then has the topology of Cantor set. If there are uncountably many subsets of the Cantor set such that they are not homeomorphic to each other, then I can construct an uncountably infinite family of surfaces such that they are pairwise non-homeomorphic, and each is obtained by gluing countably many handles.","X Ends(X):=\varprojlim_{K,\text{compact}}\pi_0(X-K) \Sigma_X 2 X Ends(X)=Ends(\Sigma_X) \{\Sigma_{X'}\}_{X'\subseteq X}","['general-topology', 'analysis', 'measure-theory']"
78,Partial summation to prove the limit of the series is 0,Partial summation to prove the limit of the series is 0,,"Hi I was given the following problem. Let $a_{n}>0$ increasing monotonically to $\infty$ as $ n\to\infty$ and $\sum_{n=1}^{\infty}\frac{y_{n}}{a_{n}}$ is convergent. Use summation by parts to prove that $\lim_{n\to\infty}\frac{1}{a_{n}}\sum_{i=1}^{n}y_{i}=0$ My approach was let $Y_{n} = \sum_{i=1}^{n}y_{i}$ $$ \sum_{k=q}^{p} \frac{y_{k}}{a_{k}} = \sum_{k=q}^{p}Y_{k}(\frac{1}{a_{k}}-\frac{1}{a_{k+1}}) + \frac{Y_{p}}{a_{p+1}}-\frac{Y_{q-1}}{a_{q}} $$ since the sum of $\frac{y_{k}}{a_{k}} $ converges, the RHS can be bounded from above by $\epsilon > 0$ for q,p large enough as the partial sums $\sum^{n}_{k=1}\frac{y_{k}}{a_{k}} $ form a Cacuhy sequence. But how does one reach the conclusion that $\frac{Y_{k}}{a_{k}} \to 0$ or is the approach wrong?","Hi I was given the following problem. Let increasing monotonically to as and is convergent. Use summation by parts to prove that My approach was let since the sum of converges, the RHS can be bounded from above by for q,p large enough as the partial sums form a Cacuhy sequence. But how does one reach the conclusion that or is the approach wrong?",a_{n}>0 \infty  n\to\infty \sum_{n=1}^{\infty}\frac{y_{n}}{a_{n}} \lim_{n\to\infty}\frac{1}{a_{n}}\sum_{i=1}^{n}y_{i}=0 Y_{n} = \sum_{i=1}^{n}y_{i}  \sum_{k=q}^{p} \frac{y_{k}}{a_{k}} = \sum_{k=q}^{p}Y_{k}(\frac{1}{a_{k}}-\frac{1}{a_{k+1}}) + \frac{Y_{p}}{a_{p+1}}-\frac{Y_{q-1}}{a_{q}}  \frac{y_{k}}{a_{k}}  \epsilon > 0 \sum^{n}_{k=1}\frac{y_{k}}{a_{k}}  \frac{Y_{k}}{a_{k}} \to 0,"['sequences-and-series', 'analysis', 'convergence-divergence']"
79,"If $f(0) = f(n)$ then there are $n$ different $(x , y)$ such that $f(x) = f(y)$ and $x-y \in \mathbb{Z}/\{0\}$",If  then there are  different  such that  and,"f(0) = f(n) n (x , y) f(x) = f(y) x-y \in \mathbb{Z}/\{0\}","$f$ is continuous on $[0,n]$ and $f(0) = f(n)$ . Prove that there are $n$ different $(x,y)$ (regardless of order) such that $f(x) = f(y)$ and $x - y \in \mathbb{Z}/\{0\}$ . I have no idea how to do it.",is continuous on and . Prove that there are different (regardless of order) such that and . I have no idea how to do it.,"f [0,n] f(0) = f(n) n (x,y) f(x) = f(y) x - y \in \mathbb{Z}/\{0\}","['analysis', 'continuity']"
80,"Is Fractional Calculus an important research topic ""in pure mathematics"" today?","Is Fractional Calculus an important research topic ""in pure mathematics"" today?",,"Being a potential graduate student, I would like to know if fractional calculus is an actively developing research topic in the area of pure mathematics today.","Being a potential graduate student, I would like to know if fractional calculus is an actively developing research topic in the area of pure mathematics today.",,"['analysis', 'soft-question', 'fractional-calculus', 'research']"
81,Integral Using Argument Principle,Integral Using Argument Principle,,"I think I have the following problem solved, but I'm not completely sure my reasoning is sound: Let $n\in\mathbb{N}$ and let $C$ denote the unit circle with the counterclockwise orientation. Evaluate $$\frac{1}{2\pi i}\int_{C}\frac{z^{n-1}}{3z^{n}-1}~dz.$$ My attempt: We first recall the Argument Principle: Theorem. Let $G$ be a domain in $\mathbb{C}$ and let $\gamma$ be a simple contour whose interior is contained in $G$ . Let $f$ be a holomorphic function in $G$ without zeros on $\gamma$ , then the number of zeros of $f$ in the interior of $\gamma$ (taking into account multiplicities) is $$\frac{1}{2\pi i}\int_{\gamma}\frac{f'(z)}{f(z)}~dz.$$ In our case, we note that $$\frac{1}{2\pi i}\int_{C}\frac{z^{n-1}}{3z^{n}-1}~dz=\frac{1}{3n}\cdot\frac{1}{2\pi i}\int_{C}\frac{nz^{n-1}}{z^{n}-\frac{1}{3}}~dz.$$ Now, $g(z)=z^{n}-\frac{1}{3}$ is an entire function (since $n>0$ by assumption) and $g\not\equiv 0$ on the unit circle $C$ . Moreover, $g'(z)=nz^{n-1}$ , so it follows by the Argument Principle that \begin{equation}\frac{1}{2\pi i}\int_{C}\frac{nz^{n-1}}{z^{n}-\frac{1}{3}}~dz=\frac{1}{2\pi i}\int_{C}\frac{g'(z)}{g(z)}~dz\tag{*}\end{equation} is none other than the number of zeros of $g(z)=z^{n}-\frac{1}{3}$ in the interior of the unit circle. Counting multiplicities, there are $n$ zeros of $g$ in the interior of $C$ (namely the $n$ th roots of $\frac{1}{3}$ ), so we conclude that the quantity in (*) is exactly $n$ . Hence, $$\frac{1}{2\pi i}\int_{C}\frac{z^{n-1}}{3z^{n}-1}~dz=\frac{n}{3n}=\frac{1}{3}.$$ My questions: Does the above work look okay? I think it's right, but it felt like too simple of an argument (I know that the Argument Principle is a pretty useful tool, but still). Thank you in advance for any comments!","I think I have the following problem solved, but I'm not completely sure my reasoning is sound: Let and let denote the unit circle with the counterclockwise orientation. Evaluate My attempt: We first recall the Argument Principle: Theorem. Let be a domain in and let be a simple contour whose interior is contained in . Let be a holomorphic function in without zeros on , then the number of zeros of in the interior of (taking into account multiplicities) is In our case, we note that Now, is an entire function (since by assumption) and on the unit circle . Moreover, , so it follows by the Argument Principle that is none other than the number of zeros of in the interior of the unit circle. Counting multiplicities, there are zeros of in the interior of (namely the th roots of ), so we conclude that the quantity in (*) is exactly . Hence, My questions: Does the above work look okay? I think it's right, but it felt like too simple of an argument (I know that the Argument Principle is a pretty useful tool, but still). Thank you in advance for any comments!",n\in\mathbb{N} C \frac{1}{2\pi i}\int_{C}\frac{z^{n-1}}{3z^{n}-1}~dz. G \mathbb{C} \gamma G f G \gamma f \gamma \frac{1}{2\pi i}\int_{\gamma}\frac{f'(z)}{f(z)}~dz. \frac{1}{2\pi i}\int_{C}\frac{z^{n-1}}{3z^{n}-1}~dz=\frac{1}{3n}\cdot\frac{1}{2\pi i}\int_{C}\frac{nz^{n-1}}{z^{n}-\frac{1}{3}}~dz. g(z)=z^{n}-\frac{1}{3} n>0 g\not\equiv 0 C g'(z)=nz^{n-1} \begin{equation}\frac{1}{2\pi i}\int_{C}\frac{nz^{n-1}}{z^{n}-\frac{1}{3}}~dz=\frac{1}{2\pi i}\int_{C}\frac{g'(z)}{g(z)}~dz\tag{*}\end{equation} g(z)=z^{n}-\frac{1}{3} n g C n \frac{1}{3} n \frac{1}{2\pi i}\int_{C}\frac{z^{n-1}}{3z^{n}-1}~dz=\frac{n}{3n}=\frac{1}{3}.,"['complex-analysis', 'analysis', 'proof-verification', 'contour-integration']"
82,Definition approximation of smooth function by Morse function,Definition approximation of smooth function by Morse function,,Hi everyone I'm currently studying Morse Theory on my own and I've come across a proposition where I would need a precise definition of what is meant. Here is the proposition: Let $M$ be a manifold that can be embedded as a submanifold into a Euclidean space and let $ f : M → \mathbb{R}$ be a $C_{\infty}$ function. Let $k$ be an integer. Then $f$ and all its derivatives of order $≤ k$ can be uniformly approximated by Morse functions on every compact subset. The part that confuses me is what is meant by uniform approximation of the derivatives of order ${}$ $ ≤ k$ . An example of a problem I have is with what is meant by derivatives of order bigger than 1. Because from my (very basic) understanding of differential geometry we cannot even generally define the second derivative. It's only well defined on the set of critical points. I have a lot of other problems with this proposition and I took a look at the proof hoping to find answers to my questions. Sadly the books only proves the case $k=0$ . What I would need is a very pedantic restatement of the proposition that explains what we mean by approximating higher order derivatives on a compact set and what we even mean by higher order derivatives. I tried to look on the internet but this exact proposition was nowhere to be found so I really hope someone here knows what it means.,Hi everyone I'm currently studying Morse Theory on my own and I've come across a proposition where I would need a precise definition of what is meant. Here is the proposition: Let be a manifold that can be embedded as a submanifold into a Euclidean space and let be a function. Let be an integer. Then and all its derivatives of order can be uniformly approximated by Morse functions on every compact subset. The part that confuses me is what is meant by uniform approximation of the derivatives of order . An example of a problem I have is with what is meant by derivatives of order bigger than 1. Because from my (very basic) understanding of differential geometry we cannot even generally define the second derivative. It's only well defined on the set of critical points. I have a lot of other problems with this proposition and I took a look at the proof hoping to find answers to my questions. Sadly the books only proves the case . What I would need is a very pedantic restatement of the proposition that explains what we mean by approximating higher order derivatives on a compact set and what we even mean by higher order derivatives. I tried to look on the internet but this exact proposition was nowhere to be found so I really hope someone here knows what it means.,M  f : M → \mathbb{R} C_{\infty} k f ≤ k {}  ≤ k k=0,"['analysis', 'differential-geometry', 'smooth-manifolds', 'morse-theory']"
83,Approximating a function as close as we wish by interpolating polynomials,Approximating a function as close as we wish by interpolating polynomials,,"So yesterday in lecture one of my professors gave a proof that if $f:[a,b]\to\mathbb{R}$ with $f(0)=0,$ then for all $\epsilon>0$ there exists a polynomial $p$ such that $p(0)=0$ and $\|f-p\|_{[a,b]}<\epsilon.$ The proof was rather complicated, and I think I've found a simpler one. Also, it doesn't require $f(0)=0.$ Could anyone verify if its correct? thank you. $\textbf{Proof:}$ By Stone-Weierstrass chose polynomials $p_n$ such that $p_n\to f$ uniformly. Then we consider $p_n+(f(0)-p_n(0)).$ Let $\epsilon\geq0$ and choose $N$ large so that for all $n\geq N$ we have $\|p_n-f\|_{[a,b]}<\epsilon/2.$ It follows that $$\|f-(p_n+(f(0)-p_n(0)))\|_{[a,b]}\leq\|f-p_n\|_{[a,b]}+\|f(0)-p_n(0)\|_{[a,b]}$$ $$<\epsilon/2+\epsilon/2=\epsilon.$$ Thus $p_n+(f(0)-p_n(0))\to f$ uniformly, and $$[p_n+(f(0)-p_n(0))](0)=f(0).$$ This completes the proof. $\blacksquare$ To add, we could generalized if we replaced $(f(0)-p_n(0))$ by $$\sum_{i=1}^n(f(x_i)-p_n(x_i))\ell_i,$$ where $x_1,...,x_n$ are a set of nodes, and $\ell_i$ are Lagrange Basis Polynomials.","So yesterday in lecture one of my professors gave a proof that if with then for all there exists a polynomial such that and The proof was rather complicated, and I think I've found a simpler one. Also, it doesn't require Could anyone verify if its correct? thank you. By Stone-Weierstrass chose polynomials such that uniformly. Then we consider Let and choose large so that for all we have It follows that Thus uniformly, and This completes the proof. To add, we could generalized if we replaced by where are a set of nodes, and are Lagrange Basis Polynomials.","f:[a,b]\to\mathbb{R} f(0)=0, \epsilon>0 p p(0)=0 \|f-p\|_{[a,b]}<\epsilon. f(0)=0. \textbf{Proof:} p_n p_n\to f p_n+(f(0)-p_n(0)). \epsilon\geq0 N n\geq N \|p_n-f\|_{[a,b]}<\epsilon/2. \|f-(p_n+(f(0)-p_n(0)))\|_{[a,b]}\leq\|f-p_n\|_{[a,b]}+\|f(0)-p_n(0)\|_{[a,b]} <\epsilon/2+\epsilon/2=\epsilon. p_n+(f(0)-p_n(0))\to f [p_n+(f(0)-p_n(0))](0)=f(0). \blacksquare (f(0)-p_n(0)) \sum_{i=1}^n(f(x_i)-p_n(x_i))\ell_i, x_1,...,x_n \ell_i","['real-analysis', 'analysis', 'polynomials', 'lagrange-interpolation']"
84,Product of two functions with compactly supported Fourier transforms,Product of two functions with compactly supported Fourier transforms,,"Question 1: Suppose $a,b\in C^\infty(\mathbb{R})\cap L^\infty(\mathbb{R})$ . Is it possible that the pointwise product $ab$ equals the constant function $1$ and both $a$ and $b$ have compactly supported Fourier transforms? I would like to understand a specific example (if it exists) but don't know enough of the theory to find one. Question 2: A more specific problem that I have in mind is whether there exist functions $a$ and $b$ defined on $\mathbb{R}\backslash U$ , where $U$ is a small open neighbourhood around $0$ , such that: $a,b$ are odd functions, $a\rightarrow\pm 1$ at $\pm\infty$ , $ab=1$ on $\mathbb{R}\backslash U$ , both $a$ and $b$ have compactly supported Fourier transforms. Remark: To provide some context for the question, my motivation for considering whether such $a$ and $b$ exist comes from trying to do functional calculus of a self-adjoint elliptic differential operator $D$ , for example $D=-i\frac{d}{dx}$ on $\mathbb{R}$ . The compactness of the supports of $\hat{a}$ and $\hat{b}$ have to do with $a(D)$ and $b(D)$ having ""finite propagation"".","Question 1: Suppose . Is it possible that the pointwise product equals the constant function and both and have compactly supported Fourier transforms? I would like to understand a specific example (if it exists) but don't know enough of the theory to find one. Question 2: A more specific problem that I have in mind is whether there exist functions and defined on , where is a small open neighbourhood around , such that: are odd functions, at , on , both and have compactly supported Fourier transforms. Remark: To provide some context for the question, my motivation for considering whether such and exist comes from trying to do functional calculus of a self-adjoint elliptic differential operator , for example on . The compactness of the supports of and have to do with and having ""finite propagation"".","a,b\in C^\infty(\mathbb{R})\cap L^\infty(\mathbb{R}) ab 1 a b a b \mathbb{R}\backslash U U 0 a,b a\rightarrow\pm 1 \pm\infty ab=1 \mathbb{R}\backslash U a b a b D D=-i\frac{d}{dx} \mathbb{R} \hat{a} \hat{b} a(D) b(D)","['functional-analysis', 'analysis', 'operator-theory', 'functional-calculus']"
85,First Area Variation,First Area Variation,,"Could someone help me to solve this limit? Let $X:\Omega \subset \mathbb{R}^{2}\rightarrow \mathbb{R}^{3}$ imersion and ${\LARGE \chi }_{t}:\Omega \subset \mathbb{R}^{2}\rightarrow \mathbb{R}^{3}$ , $t\in \left( -\epsilon ,\epsilon \right) $ a variation of $X$ in the form ${\LARGE \chi }\left( p,t\right) =X\left( p\right) +tf\left( p\right) N\left( p\right) $ , where $N$ is normal of Gauss and $f:\Omega\rightarrow\mathbb{R}^3$ function differentiable. Show that $$ A^{\prime }\left( 0\right) =\int_{\Omega }\left. \frac{d}{dt}\right\vert _{t=0}{Jac}({\LARGE \chi }_{t})(p)dA.$$ $A^{\prime }\left( 0\right) =\underset{t\rightarrow 0}{\lim }\frac{1}{t}% \left( A\left( t\right) -A\left( 0\right) \right) =\underset{t\rightarrow 0}{% \lim }\frac{1}{t}\left( \int_{\Omega }\sqrt{E_{t}G_{t}-F_{t}^{2}}% dudv-\int_{\Omega }\sqrt{EG-F^{2}}dudv\right) $ $=\underset{t\rightarrow 0}{\lim }\frac{1}{t}\int_{\Omega }\sqrt{% E_{t}G_{t}-F_{t}^{2}}-\sqrt{EG-F^{2}}dudv=?$","Could someone help me to solve this limit? Let imersion and , a variation of in the form , where is normal of Gauss and function differentiable. Show that","X:\Omega \subset \mathbb{R}^{2}\rightarrow \mathbb{R}^{3} {\LARGE \chi }_{t}:\Omega \subset \mathbb{R}^{2}\rightarrow \mathbb{R}^{3} t\in \left( -\epsilon ,\epsilon \right)  X {\LARGE \chi }\left( p,t\right) =X\left( p\right) +tf\left( p\right)
N\left( p\right)  N f:\Omega\rightarrow\mathbb{R}^3  A^{\prime }\left( 0\right) =\int_{\Omega }\left. \frac{d}{dt}\right\vert
_{t=0}{Jac}({\LARGE \chi }_{t})(p)dA. A^{\prime }\left( 0\right) =\underset{t\rightarrow 0}{\lim }\frac{1}{t}%
\left( A\left( t\right) -A\left( 0\right) \right) =\underset{t\rightarrow 0}{%
\lim }\frac{1}{t}\left( \int_{\Omega }\sqrt{E_{t}G_{t}-F_{t}^{2}}%
dudv-\int_{\Omega }\sqrt{EG-F^{2}}dudv\right)  =\underset{t\rightarrow 0}{\lim }\frac{1}{t}\int_{\Omega }\sqrt{%
E_{t}G_{t}-F_{t}^{2}}-\sqrt{EG-F^{2}}dudv=?","['analysis', 'multivariable-calculus', 'differential-geometry', 'riemannian-geometry']"
86,Does $\sum_{n=1}^\infty \frac{\cos{(\sqrt{n})}}{n}$ converge?,Does  converge?,\sum_{n=1}^\infty \frac{\cos{(\sqrt{n})}}{n},"The series is: $$\sum_{n=1}^\infty \frac{\cos(\sqrt{n})}{n}$$ Considering it isn't always positive, I replace $\frac{\cos{\sqrt{n}}}{n}$ with its absolute value and I find that: $$\vert \frac{\cos{\sqrt{n}}}{n}\vert\gt \frac{\cos^2{\sqrt{n}}}{n}=\frac{\ 1+\cos{2\sqrt{n}}}{2n}=\frac{1}{2n}+\frac{\cos{2\sqrt{n}}}{2n}$$ if $\sum_{n=1}^\infty \vert\frac{\cos{\sqrt{n}}}{n}\vert $ converges, then $\sum_{n=1}^\infty \frac{\cos{\sqrt{n}}}{n}$ converges.Using Comparison test,we can draw the conclusion that $\sum_{n=1}^\infty\frac{1}{2n}$ converges , which is impossible. So I get that $\sum_{n=1}^\infty \frac{\cos{\sqrt{n}}}{n}$ absolutely diverges. But I can't figure out whether $\sum_{n=1}^\infty \frac{\cos{\sqrt{n}}}{n}$ converges or not. I have tried Dirichlet's test, but I can't figure out whether $$S_{n}=\sum_{k=1}^n \cos{\sqrt{k}}$$ is bounded. (This is my first time to ask question.Maybe there exist some mistakes in my conclusion.Thanks. :)","The series is: Considering it isn't always positive, I replace with its absolute value and I find that: if converges, then converges.Using Comparison test,we can draw the conclusion that converges , which is impossible. So I get that absolutely diverges. But I can't figure out whether converges or not. I have tried Dirichlet's test, but I can't figure out whether is bounded. (This is my first time to ask question.Maybe there exist some mistakes in my conclusion.Thanks. :)",\sum_{n=1}^\infty \frac{\cos(\sqrt{n})}{n} \frac{\cos{\sqrt{n}}}{n} \vert \frac{\cos{\sqrt{n}}}{n}\vert\gt \frac{\cos^2{\sqrt{n}}}{n}=\frac{\ 1+\cos{2\sqrt{n}}}{2n}=\frac{1}{2n}+\frac{\cos{2\sqrt{n}}}{2n} \sum_{n=1}^\infty \vert\frac{\cos{\sqrt{n}}}{n}\vert  \sum_{n=1}^\infty \frac{\cos{\sqrt{n}}}{n} \sum_{n=1}^\infty\frac{1}{2n} \sum_{n=1}^\infty \frac{\cos{\sqrt{n}}}{n} \sum_{n=1}^\infty \frac{\cos{\sqrt{n}}}{n} S_{n}=\sum_{k=1}^n \cos{\sqrt{k}},"['sequences-and-series', 'analysis', 'trigonometry']"
87,Tangent space at a point and optimization under constraint with the sub-manifold $x^2+2y^2-z^2=1$,Tangent space at a point and optimization under constraint with the sub-manifold,x^2+2y^2-z^2=1,"Given is the following set M containing all points $\subset \mathbb{R}^3$ such that $x^2+2y^2-z^2=1$ a) Prove that M is a two dimensional sub-manifold of $\mathbb{R}^3$ b) Calculate the tangential space of M in the point $p=(1,0,0)$ c) Determine the points on M with minimal distance to the origin with respect to the euclidean Norm. This problem is very similar to  this one that I asked a month ago : How to find the set of all points in a submanifold minimizing the distance to a given point? But I want to be sure that I proceeded correctly and made no mistakes/ forgot nothing. Thanks for your feedback. a) To prove that this is a sub-manifold, we simply take the gradient, find all points that yield zero, and plug those points into our set. If they aren't included in the set, then this set is a sub-manifold. So $(2x,4y,-2z)=(0,0,0)$ which only yields the point $x=y=z=0$ which is not included in our set because $0\neq 1$ . The dimension is $2$ because the set takes a point in $\mathbb{R}^3$ and outputs a point in $\mathbb{R}^1$ so $3-1=2$ b) To find the tangent space in a point, we input our point into the gradient and find its kernel. So $(2,0,0)\cdot v =(0,0,0)$ . So $a_1=0$ and $ a_2 = b, a_3=c$ . So $a_2, a_3$ are free variables. Thus the kernel is the span of $b(0,1,0)+c(0,0,1)$ c) This is an optimization problem with constraint. The function we want to optimize is the euclidean distance squared (We can square it because it does not change the result since it is a ""monotonically increasing"" function. This allows us to get rid of the square roots to make the algebra nicer.) Our constraint is the set M. So $f(x,y,z)=(x-0)^2+(y-0)^2+(z-0)^2$ and $g(x,y,z)=x^2+2y^2-z^2-1$ . Using Lagrange multiplier, we get : $(2x,2y,2z)=\lambda (2x,4y,-2z)$ . If $\lambda =1, y=z=0, x^2=1-2*0^2+0^2=1$ so $x=\pm 1$ If $\lambda = -1, x=y=0$ then we have no solutions since $-z^2$ never equals $1$ If $\lambda = \frac{1}{2}, x=z=0, 2y^2=1$ so $y=\pm \frac{1}{\sqrt{2}}$ So our candidates are $(\pm 1,0,0), (0,\pm \frac{1}{\sqrt{2}},0)$ . Now,given that $(0,\pm \frac{1}{\sqrt{2}},0)$ is closer to the origin than $(\pm 1,0,0)$ , our set of points is $(0,\pm \frac{1}{\sqrt{2}},0)$ Thanks for your feedback !","Given is the following set M containing all points such that a) Prove that M is a two dimensional sub-manifold of b) Calculate the tangential space of M in the point c) Determine the points on M with minimal distance to the origin with respect to the euclidean Norm. This problem is very similar to  this one that I asked a month ago : How to find the set of all points in a submanifold minimizing the distance to a given point? But I want to be sure that I proceeded correctly and made no mistakes/ forgot nothing. Thanks for your feedback. a) To prove that this is a sub-manifold, we simply take the gradient, find all points that yield zero, and plug those points into our set. If they aren't included in the set, then this set is a sub-manifold. So which only yields the point which is not included in our set because . The dimension is because the set takes a point in and outputs a point in so b) To find the tangent space in a point, we input our point into the gradient and find its kernel. So . So and . So are free variables. Thus the kernel is the span of c) This is an optimization problem with constraint. The function we want to optimize is the euclidean distance squared (We can square it because it does not change the result since it is a ""monotonically increasing"" function. This allows us to get rid of the square roots to make the algebra nicer.) Our constraint is the set M. So and . Using Lagrange multiplier, we get : . If so If then we have no solutions since never equals If so So our candidates are . Now,given that is closer to the origin than , our set of points is Thanks for your feedback !","\subset \mathbb{R}^3 x^2+2y^2-z^2=1 \mathbb{R}^3 p=(1,0,0) (2x,4y,-2z)=(0,0,0) x=y=z=0 0\neq 1 2 \mathbb{R}^3 \mathbb{R}^1 3-1=2 (2,0,0)\cdot v =(0,0,0) a_1=0  a_2 = b, a_3=c a_2, a_3 b(0,1,0)+c(0,0,1) f(x,y,z)=(x-0)^2+(y-0)^2+(z-0)^2 g(x,y,z)=x^2+2y^2-z^2-1 (2x,2y,2z)=\lambda (2x,4y,-2z) \lambda =1, y=z=0, x^2=1-2*0^2+0^2=1 x=\pm 1 \lambda = -1, x=y=0 -z^2 1 \lambda = \frac{1}{2}, x=z=0, 2y^2=1 y=\pm \frac{1}{\sqrt{2}} (\pm 1,0,0), (0,\pm \frac{1}{\sqrt{2}},0) (0,\pm \frac{1}{\sqrt{2}},0) (\pm 1,0,0) (0,\pm \frac{1}{\sqrt{2}},0)","['real-analysis', 'general-topology']"
88,Proof that $fg \geq 1 \implies \int_E f\ d\mu\int_E g\ d\mu \geq \mu^2(E)$,Proof that,fg \geq 1 \implies \int_E f\ d\mu\int_E g\ d\mu \geq \mu^2(E),"$(E,A,\mu)$ is a finite measure space, $f$ and $g$ are both positive and measurable functions from $E$ to $\mathbb{R}$ such that $fg \geq 1$ by holder we have $$(\int_E f^2 \ d\mu)(\int_E g^2\ d\mu) \geq (\int_E fg \ d\mu)^2 \geq (\int_E \ d\mu)^2 \geq \mu^2(E)$$ how about this one though : $\int_E f\ d\mu\int_E g\ d\mu \geq \mu^2(E)$ if $f = h^2, \, g = w^2$ then $|h||w| \geq 1$ and $|h|,\,|w| \geq 0$ and measurable therefore by what precedes $\int_E f\ d\mu\int_E g\ d\mu = (\int_E h^2 \ d\mu)(\int_E w^2\ d\mu)  \geq  \mu^2(E)$ Is my approach correct?","is a finite measure space, and are both positive and measurable functions from to such that by holder we have how about this one though : if then and and measurable therefore by what precedes Is my approach correct?","(E,A,\mu) f g E \mathbb{R} fg \geq 1 (\int_E f^2 \ d\mu)(\int_E g^2\ d\mu) \geq (\int_E fg \ d\mu)^2 \geq (\int_E \ d\mu)^2 \geq \mu^2(E) \int_E f\ d\mu\int_E g\ d\mu \geq \mu^2(E) f = h^2, \, g = w^2 |h||w| \geq 1 |h|,\,|w| \geq 0 \int_E f\ d\mu\int_E g\ d\mu = (\int_E h^2 \ d\mu)(\int_E w^2\ d\mu)  \geq  \mu^2(E)","['integration', 'analysis', 'measure-theory', 'proof-verification']"
89,Shape of $\{a>0:\int |f|^a \text{d}\mu<\infty\}$,Shape of,\{a>0:\int |f|^a \text{d}\mu<\infty\},"Given a measure space $\mathcal{M}=(X,\mathcal{A},\mu)$ and a measurable function $f:X\to\mathbf{R}$ , what shapes can the following set take? $$\{a>0:\int |f|^a \text{d}\mu<\infty\}$$ Is it always a half-open intervall, can it contain isolated points, etc.?","Given a measure space and a measurable function , what shapes can the following set take? Is it always a half-open intervall, can it contain isolated points, etc.?","\mathcal{M}=(X,\mathcal{A},\mu) f:X\to\mathbf{R} \{a>0:\int |f|^a \text{d}\mu<\infty\}","['analysis', 'measure-theory']"
90,"Let $f:(0;\infty)\times (0;\infty) \rightarrow \Bbb R$ be a Lebesgue measurable function, is $T_f(y)=\int_0^{+\infty} f(x,y) \, dy$ measurable?","Let  be a Lebesgue measurable function, is  measurable?","f:(0;\infty)\times (0;\infty) \rightarrow \Bbb R T_f(y)=\int_0^{+\infty} f(x,y) \, dy","Let $f:(0;\infty)\times (0;\infty) \rightarrow \Bbb R$ be a Lebesgue measurable function. Lets define $$T_f(y)=\int_0^{+\infty} f(x,y) \, dy$$ $$G_f(y)=\int_0^{+\infty} |f(x,y)| \, dy$$ By Tonelli's Theorem I know $G_f$ is measurable and I also know $G_f \in L^p(0;+\infty)$ (take it as an hypothesis). I would like to say $\Vert T_f \Vert _p \le \Vert G_f \Vert _p$ so $T_f \in L^p(0;+\infty)$ but I need $T_f$ to be measurable in order to be abel to calculate $\Vert T_f \Vert _p$ . Is this true? How can I prove it?",Let be a Lebesgue measurable function. Lets define By Tonelli's Theorem I know is measurable and I also know (take it as an hypothesis). I would like to say so but I need to be measurable in order to be abel to calculate . Is this true? How can I prove it?,"f:(0;\infty)\times (0;\infty) \rightarrow \Bbb R T_f(y)=\int_0^{+\infty} f(x,y) \, dy G_f(y)=\int_0^{+\infty} |f(x,y)| \, dy G_f G_f \in L^p(0;+\infty) \Vert T_f \Vert _p \le \Vert G_f \Vert _p T_f \in L^p(0;+\infty) T_f \Vert T_f \Vert _p","['real-analysis', 'analysis', 'lebesgue-integral', 'lebesgue-measure']"
91,Real world applications of Riemann surfaces of holomorphic functions [closed],Real world applications of Riemann surfaces of holomorphic functions [closed],,Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question The maximal analytic continuation of a holomorphic function is an example of Riemann surfaces. What is it  used for? Please edit the question to limit it to a specific problem with enough detail to identify an adequate answer. I want to know the applications related to the real world  rather than pure mathematics.,Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question The maximal analytic continuation of a holomorphic function is an example of Riemann surfaces. What is it  used for? Please edit the question to limit it to a specific problem with enough detail to identify an adequate answer. I want to know the applications related to the real world  rather than pure mathematics.,,"['complex-analysis', 'analysis']"
92,Estimate of a weak solution in a nonhomogeneous equation,Estimate of a weak solution in a nonhomogeneous equation,,"$\textbf{Problem}$ Let $\Omega \subset \mathbb{R}^n$ be open, bounded and connected with $\partial \Omega\in C^1$ . For each $i,j=1,\cdots,n$ , assume that $a_{ij},b_i,c \in L^{\infty}(\Omega)$ (real valued function) , and assume that there exists a constant $\mu \in (0,1)$ satisfying \begin{align*} \mu \vert \xi \vert^2\leq \sum_{i,j=1}^n a_{ij}\xi_i\xi_j \leq \frac{1}{\mu} \vert \xi \vert ^2 \; \textrm{a.e. in } \Omega, \; \textrm{ for all } \xi\in \mathbb{R^n} \end{align*} Define \begin{align*} Lu:=-\sum_{i,j=1}^n\partial_{j}(a_{ij}\partial_iu)+\sum_{i=1}^nb_i \partial_iu+cu \end{align*} For given functions $f\in L^2(\Omega)$ and $g\in H^{1}(\Omega)$ , suppose that $u \in H^1(\Omega)$ is a weak solution to the following boundary value problem \begin{align*} \begin{cases} Lu=f & \textrm{ in } \Omega \\ u=g & \textrm{ on } \partial \Omega \end{cases} \end{align*} If the homogeneous boundary value problem \begin{align*} \begin{cases} Lu=0 & \textrm{ in } \Omega \\ u=0 & \textrm{ on } \partial \Omega \end{cases} \end{align*} has only trivial weak solution, then prove that there exists a constant $C>0$ (independent of $u,f$ and $g$ ) so that \begin{align*} \Vert u \Vert_{H^1(\Omega)} \leq C(\Vert f \Vert_{L^2(\Omega)}+\Vert g \Vert_{H^1(\Omega)}) \end{align*} $\textbf{Attempt}$ Take $w:=u-g $ . Then, $w$ satisfies the following boundary value problem \begin{align*} \begin{cases} Lw=f-Lg & \textrm{ in } \Omega \\ w=0 & \textrm{ on } \partial \Omega \end{cases} \end{align*} We get $w \in H^1_0(\Omega)$ . Also, \begin{align*} \Vert u \Vert _{H^1(\Omega)} &= \Vert w+g \Vert_{H^1(\Omega)}\\  &\leq \Vert w \Vert_{H^1(\Omega)} + \Vert g \Vert_{H^1(\Omega)}\\ \end{align*} ( $\textbf{Update}$ ) We remain that $\Vert w \Vert_{H^1(\Omega)}$ is bounded by $\Vert f \Vert _{L^2(\Omega)}$ and $ \Vert g \Vert_{H^1(\Omega)}$ . Define a bilinear map $B[\cdot,\cdot]$ from $H^1(\Omega)$ to $H^1(\Omega)$ by \begin{align*} B[w,v]:= \int_{\Omega} \sum_{i,j=1}^n a_{ij} \partial_i w \partial_j v +(\sum_{i=1}^n b_i\partial_i w + cw) v \; dx  \end{align*} Then, we easily check that \begin{align*} B[w,w]=\int_{\Omega} fw \; dx -\int_{\Omega} \sum_{i,j=1}^n a_{ij} \partial_i g \partial_j w +(\sum_{i=1}^n b_i\partial_i g +cg)w \; dx \end{align*} $\textbf{Note}$ We have the following properties: \begin{align*} &(1) \; \beta \Vert w \Vert_{H^1(\Omega)}^2 \leq B[w,w]+\gamma \Vert w \Vert _{L^2(\Omega)}^2 \; \textrm{for some constants }\beta>0, \gamma\geq 0 \\ &(2) \; \Vert w \Vert_{L^2(\Omega)}\leq C_p \Vert Dw \Vert_{L^2(\Omega)} \; \textrm{(Poincare's inequality)}\\ &(3) \; \Vert w \Vert_{L^2(\Omega)} \leq \Vert w \Vert_{H^1(\Omega)},\; \Vert Dw \Vert _{L^2(\Omega)} \leq \Vert w \Vert_{H^1(\Omega)} \\ &(4) \; ab\leq \epsilon a^2 +\frac{b^2}{4\epsilon} \; (a,b>0, \epsilon>0) \; \textrm{(Cauchy's inequality with }\epsilon) \end{align*} By using the properties and Holder's inequality, I induced \begin{align*} \beta \Vert w \Vert_{H^1(\Omega)}^2 \leq \Vert f \Vert_{L^2(\Omega)} \Vert w \Vert _{L^2(\Omega)} +C_1\Vert g \Vert_{H^1(\Omega)}\Vert Dw \Vert_{L^2(\Omega)} +C_2 \Vert g \Vert _{H^1(\Omega)} \Vert w\Vert_{L^2(\Omega)} +\gamma \Vert w \Vert _{L^2(\Omega)}^2   \end{align*} However, I stuck $\Vert w \Vert_{H^1(\Omega)}$ is bounded by $\Vert f \Vert_{L^2(\Omega)}$ and $\Vert g \Vert_{H^1(\Omega)}$ because of the last term $\Vert w \Vert_{L^2(\Omega)}^2$ Any help is appreciated... Thank you!","Let be open, bounded and connected with . For each , assume that (real valued function) , and assume that there exists a constant satisfying Define For given functions and , suppose that is a weak solution to the following boundary value problem If the homogeneous boundary value problem has only trivial weak solution, then prove that there exists a constant (independent of and ) so that Take . Then, satisfies the following boundary value problem We get . Also, ( ) We remain that is bounded by and . Define a bilinear map from to by Then, we easily check that We have the following properties: By using the properties and Holder's inequality, I induced However, I stuck is bounded by and because of the last term Any help is appreciated... Thank you!","\textbf{Problem} \Omega \subset \mathbb{R}^n \partial \Omega\in C^1 i,j=1,\cdots,n a_{ij},b_i,c \in L^{\infty}(\Omega) \mu \in (0,1) \begin{align*}
\mu \vert \xi \vert^2\leq \sum_{i,j=1}^n a_{ij}\xi_i\xi_j \leq \frac{1}{\mu} \vert \xi \vert ^2 \; \textrm{a.e. in } \Omega, \; \textrm{ for all } \xi\in \mathbb{R^n}
\end{align*} \begin{align*}
Lu:=-\sum_{i,j=1}^n\partial_{j}(a_{ij}\partial_iu)+\sum_{i=1}^nb_i \partial_iu+cu
\end{align*} f\in L^2(\Omega) g\in H^{1}(\Omega) u \in H^1(\Omega) \begin{align*}
\begin{cases}
Lu=f & \textrm{ in } \Omega \\
u=g & \textrm{ on } \partial \Omega
\end{cases}
\end{align*} \begin{align*}
\begin{cases}
Lu=0 & \textrm{ in } \Omega \\
u=0 & \textrm{ on } \partial \Omega
\end{cases}
\end{align*} C>0 u,f g \begin{align*}
\Vert u \Vert_{H^1(\Omega)} \leq C(\Vert f \Vert_{L^2(\Omega)}+\Vert g \Vert_{H^1(\Omega)})
\end{align*} \textbf{Attempt} w:=u-g  w \begin{align*}
\begin{cases}
Lw=f-Lg & \textrm{ in } \Omega \\
w=0 & \textrm{ on } \partial \Omega
\end{cases}
\end{align*} w \in H^1_0(\Omega) \begin{align*}
\Vert u \Vert _{H^1(\Omega)} &= \Vert w+g \Vert_{H^1(\Omega)}\\ 
&\leq \Vert w \Vert_{H^1(\Omega)} + \Vert g \Vert_{H^1(\Omega)}\\
\end{align*} \textbf{Update} \Vert w \Vert_{H^1(\Omega)} \Vert f \Vert _{L^2(\Omega)}  \Vert g \Vert_{H^1(\Omega)} B[\cdot,\cdot] H^1(\Omega) H^1(\Omega) \begin{align*}
B[w,v]:= \int_{\Omega} \sum_{i,j=1}^n a_{ij} \partial_i w \partial_j v +(\sum_{i=1}^n b_i\partial_i w + cw) v \; dx 
\end{align*} \begin{align*}
B[w,w]=\int_{\Omega} fw \; dx -\int_{\Omega} \sum_{i,j=1}^n a_{ij} \partial_i g \partial_j w +(\sum_{i=1}^n b_i\partial_i g +cg)w \; dx
\end{align*} \textbf{Note} \begin{align*}
&(1) \; \beta \Vert w \Vert_{H^1(\Omega)}^2 \leq B[w,w]+\gamma \Vert w \Vert _{L^2(\Omega)}^2 \; \textrm{for some constants }\beta>0, \gamma\geq 0 \\
&(2) \; \Vert w \Vert_{L^2(\Omega)}\leq C_p \Vert Dw \Vert_{L^2(\Omega)} \; \textrm{(Poincare's inequality)}\\
&(3) \; \Vert w \Vert_{L^2(\Omega)} \leq \Vert w \Vert_{H^1(\Omega)},\; \Vert Dw \Vert _{L^2(\Omega)} \leq \Vert w \Vert_{H^1(\Omega)} \\
&(4) \; ab\leq \epsilon a^2 +\frac{b^2}{4\epsilon} \; (a,b>0, \epsilon>0) \; \textrm{(Cauchy's inequality with }\epsilon)
\end{align*} \begin{align*}
\beta \Vert w \Vert_{H^1(\Omega)}^2 \leq \Vert f \Vert_{L^2(\Omega)} \Vert w \Vert _{L^2(\Omega)} +C_1\Vert g \Vert_{H^1(\Omega)}\Vert Dw \Vert_{L^2(\Omega)} +C_2 \Vert g \Vert _{H^1(\Omega)} \Vert w\Vert_{L^2(\Omega)} +\gamma \Vert w \Vert _{L^2(\Omega)}^2  
\end{align*} \Vert w \Vert_{H^1(\Omega)} \Vert f \Vert_{L^2(\Omega)} \Vert g \Vert_{H^1(\Omega)} \Vert w \Vert_{L^2(\Omega)}^2","['analysis', 'partial-differential-equations']"
93,Rudin's Proof about Winding Numbers,Rudin's Proof about Winding Numbers,,"This is kind of a softball question, an untied loose end that has always bugged me. It is well-known that if $\Gamma_1\sim \Gamma_2$ are two homotopic closed paths in a region $\Omega$ , and if $\alpha\notin \Omega$ , then $n(\Gamma_1;\alpha)=n(\Gamma_2;\alpha).$ I've seen several proofs of this, using approximation by polygonal paths. Rudin's is (surprise!) the slickest, but of course, he leaves some of the details to the reader, and when I do the calculation, I am off by a factor of two at a certain step, which does not affect the proof (one can scale the original hypothesis), but I must be making a mistake, and it has always bugged me. So I'd like to see where my error is. Let $H:I\times I\to \Omega$ be the homotopy. and choose an integer $n$ such that $|s-t|+|s'-t'|<1/n\Rightarrow$ $ |H(s)-H(t)|+|H(s')-H(t')|<\epsilon. $ Define the paths $\{\gamma_0,\cdots ,\gamma_n\}$ by $\gamma_k(s)=H(i/k,k/n)(ns+1-i)+H((i-1)/n,k/n)(i-ns)$ if $i-1\le ns\le i.$ The claim is then that $|\gamma_k(s)-H(s,k/n)|<\epsilon.$ Here is what I am getting, after substituting and applying the triangle inequality: $|H(i/n,k/n)-H((i-1)/n,k/n)|(ns-i)+|H(i/n,k/n)-H(s,k/n)|$ which is easily seen to be $<2\epsilon.$ It seems like the only way to avoid the factor of two, would be to arrive at a tractable expression without using the triangle inequality. But I do not see how to do this. Unless at the outset, we should have simply required that $|s-t|+|s'-t'|<1/n\Rightarrow$ $|H(s)-H(t)|+|H(s')-H(t')|<\epsilon/2. $","This is kind of a softball question, an untied loose end that has always bugged me. It is well-known that if are two homotopic closed paths in a region , and if , then I've seen several proofs of this, using approximation by polygonal paths. Rudin's is (surprise!) the slickest, but of course, he leaves some of the details to the reader, and when I do the calculation, I am off by a factor of two at a certain step, which does not affect the proof (one can scale the original hypothesis), but I must be making a mistake, and it has always bugged me. So I'd like to see where my error is. Let be the homotopy. and choose an integer such that Define the paths by if The claim is then that Here is what I am getting, after substituting and applying the triangle inequality: which is easily seen to be It seems like the only way to avoid the factor of two, would be to arrive at a tractable expression without using the triangle inequality. But I do not see how to do this. Unless at the outset, we should have simply required that","\Gamma_1\sim \Gamma_2 \Omega \alpha\notin \Omega n(\Gamma_1;\alpha)=n(\Gamma_2;\alpha). H:I\times I\to \Omega n |s-t|+|s'-t'|<1/n\Rightarrow  |H(s)-H(t)|+|H(s')-H(t')|<\epsilon.  \{\gamma_0,\cdots ,\gamma_n\} \gamma_k(s)=H(i/k,k/n)(ns+1-i)+H((i-1)/n,k/n)(i-ns) i-1\le ns\le i. |\gamma_k(s)-H(s,k/n)|<\epsilon. |H(i/n,k/n)-H((i-1)/n,k/n)|(ns-i)+|H(i/n,k/n)-H(s,k/n)| <2\epsilon. |s-t|+|s'-t'|<1/n\Rightarrow |H(s)-H(t)|+|H(s')-H(t')|<\epsilon/2. ","['complex-analysis', 'analysis', 'analytic-geometry', 'winding-number']"
94,"Is the ""immersed proper"" hypothesis necessary in Half-space Theorem?","Is the ""immersed proper"" hypothesis necessary in Half-space Theorem?",,"I'm using the following version of Half-space Theorem: $\textbf{Theorem}$ (Half-space) A connected, immersed proper, nonplanar minimal surface $M$ in $\mathbb{R}^3$ is not contained in a halfspace. I supposedly proved this theorem using the closed and complete hypothesis instead of immersed proper. Is the ""immersed proper"" hypothesis necessary? The original statement is found in the paper ""The strong halfspace theorem for minimal surfaces"" by D. Hoffman and W. H. Meeks, III, 1990. In the statement they use the hypothesis of immersed proper and allow the surface to be ""possibly branched"". Does the fact that the surface is ""possibly branched"" need the proper immersed hypothesis? Can someone help me understand this better? Follow the link in the paper below http://www.math.jhu.edu/~js/Math748/hoffman-meeks.halfspace.pdf","I'm using the following version of Half-space Theorem: (Half-space) A connected, immersed proper, nonplanar minimal surface in is not contained in a halfspace. I supposedly proved this theorem using the closed and complete hypothesis instead of immersed proper. Is the ""immersed proper"" hypothesis necessary? The original statement is found in the paper ""The strong halfspace theorem for minimal surfaces"" by D. Hoffman and W. H. Meeks, III, 1990. In the statement they use the hypothesis of immersed proper and allow the surface to be ""possibly branched"". Does the fact that the surface is ""possibly branched"" need the proper immersed hypothesis? Can someone help me understand this better? Follow the link in the paper below http://www.math.jhu.edu/~js/Math748/hoffman-meeks.halfspace.pdf",\textbf{Theorem} M \mathbb{R}^3,"['analysis', 'differential-geometry', 'riemannian-geometry', 'surfaces', 'minimal-surfaces']"
95,Locality and Hilbert Curve,Locality and Hilbert Curve,,"I have a hilbert curve index based on this algorithm. I take two to four values (latitude, longitude, time in unix format and an id code) and create a 1-d hilbert curve. I'm looking for a way to use this data to create a bounding box query (i.e. ""find all ids within this rectangle). I'm looking for a way to do so without decoding the 1d Hilbert code back into its constituent parts. My question is: if I created a 2d hilbert curve range (i.e. I converted the range of the box into a hilbert curve so x1y1-> hilbert value1 and x2y2-> hilbertvalue2) would all values of corresponding 2d hilbert values fall within their range? E.g. If I converted (1,2) and (20,30) into Hilbert values and then searched for all values between hilbertvalue1 and hilbertvalue2, would all the values I get decode to fall within (1,2) and (20, 30), or would I have to perform additional transformations? When I set all my values to 2^a* X and 2^a * y (a being a positive integer multiplier) it seems to be true. However, is there a way to use a range search on the 4d hilbert curve? I.e., if I have a Hilbert Curve made of 4 values and I have a bounding box query, can I see which hilbert values fall within that bounding box without decoding the entire Hilbert curve and checking? Thanks.","I have a hilbert curve index based on this algorithm. I take two to four values (latitude, longitude, time in unix format and an id code) and create a 1-d hilbert curve. I'm looking for a way to use this data to create a bounding box query (i.e. ""find all ids within this rectangle). I'm looking for a way to do so without decoding the 1d Hilbert code back into its constituent parts. My question is: if I created a 2d hilbert curve range (i.e. I converted the range of the box into a hilbert curve so x1y1-> hilbert value1 and x2y2-> hilbertvalue2) would all values of corresponding 2d hilbert values fall within their range? E.g. If I converted (1,2) and (20,30) into Hilbert values and then searched for all values between hilbertvalue1 and hilbertvalue2, would all the values I get decode to fall within (1,2) and (20, 30), or would I have to perform additional transformations? When I set all my values to 2^a* X and 2^a * y (a being a positive integer multiplier) it seems to be true. However, is there a way to use a range search on the 4d hilbert curve? I.e., if I have a Hilbert Curve made of 4 values and I have a bounding box query, can I see which hilbert values fall within that bounding box without decoding the entire Hilbert curve and checking? Thanks.",,['analysis']
96,Computing the value of a function based on the geometric series of 1/2^n,Computing the value of a function based on the geometric series of 1/2^n,,"So in John B. Conway's First Course in Analysis Book we're told to consider the interval $[a,b],$ and let $\{r_n\}$ denote a sequence of all rationals in said interval. Next, we define a map $\alpha:[a,b]\to\mathbb{R}$ by $$\alpha(t)=\sum_{\substack{n\in\mathbb{N}\\r_n<t}}\dfrac{1}{2^n}.$$ Totally fine with that definition. Where I'm confused is when he says that $\alpha(b)=1.$ If $b$ is irrational surely that would be the case as $\alpha(b)$ would just be a geometric sum which converges to $1$ . If $b$ is rational, however, it seems like $\alpha(b)$ would never be $1$ . Consider $r_n=b$ for some $n\in\mathbb{N}$ , hence $\alpha(b)=1-1/2^n<1.$ So I'm confused as to why Conway says it's $1$ . Could anyone explain? Thank you.","So in John B. Conway's First Course in Analysis Book we're told to consider the interval and let denote a sequence of all rationals in said interval. Next, we define a map by Totally fine with that definition. Where I'm confused is when he says that If is irrational surely that would be the case as would just be a geometric sum which converges to . If is rational, however, it seems like would never be . Consider for some , hence So I'm confused as to why Conway says it's . Could anyone explain? Thank you.","[a,b], \{r_n\} \alpha:[a,b]\to\mathbb{R} \alpha(t)=\sum_{\substack{n\in\mathbb{N}\\r_n<t}}\dfrac{1}{2^n}. \alpha(b)=1. b \alpha(b) 1 b \alpha(b) 1 r_n=b n\in\mathbb{N} \alpha(b)=1-1/2^n<1. 1","['real-analysis', 'analysis', 'geometric-series']"
97,Does this oscillatory integral exist?,Does this oscillatory integral exist?,,"Let $n\geq 2$ and consider the improper integral $$I:=\int_{\mathbb{R}^{n}}F(x)dx$$ where $F$ is a continuous function. If $I$ exists then $$I=\lim_{R\rightarrow +\infty}\int_{B_{R}}F(x)dx,$$ where $B_{R}$ is a ball with radius $R$ .  So if this limit does not exist we know that the integral does not exist.   Does the existence of this limit imply the existence of the integral ? Motivation: I am interested in the existence of the integral $$\int_{\mathbb{R}^{3}}\frac{e^{\dot{\imath}|x-y|^2}}{1+|y|}dy.$$ Using spherical coordinates (I do not even know if we are allowed to change variables here. Are we ? ) $$\int_{\mathbb{R}^{3}}\frac{e^{\dot{\imath}|x-y|^2}}{1+|y|}dy= \int_{\mathbb{S}^{2}}\int_{0}^{\infty} \frac{e^{\dot{\imath}|\rho\omega-x|^2}\rho^2}{1+\rho}d\rho d\omega\\ =e^{i|x|^{2}}\int_{\mathbb{S}^{2}}\int_{0}^{\infty} \frac{e^{\dot{\imath} (\rho^2-2x\cdot \omega\,\rho)}\rho^2}{1+\rho}d\rho d\omega.$$ Observations: 1-The inner integral does not exist for any $x$ and $\omega$ . 2-We can not change order of integration 3-The limit $$\lim_{R\rightarrow \infty}\int_{\mathbb{S}^{2}}\int_{0}^{R} \frac{e^{\dot{\imath} (\rho^2-2x\cdot \omega\,\rho)}\rho^2}{1+\rho}d\rho d\omega$$ exists. Simply apply the very nice formula [Grafakos, classical Fourier analysis-Appendix D]: $$\int_{\mathbb{S}^{n-1}} F(x.\omega)d\omega=c \int_{-1}^{1}(\sqrt{1-s^2})^{n-3} F(s|x|)ds.$$ then benefit from the oscillation in both variables $\rho$ and $\omega$ and integrate by parts in both variables. Any ideas how to handle this ? Thank you so much","Let and consider the improper integral where is a continuous function. If exists then where is a ball with radius .  So if this limit does not exist we know that the integral does not exist.   Does the existence of this limit imply the existence of the integral ? Motivation: I am interested in the existence of the integral Using spherical coordinates (I do not even know if we are allowed to change variables here. Are we ? ) Observations: 1-The inner integral does not exist for any and . 2-We can not change order of integration 3-The limit exists. Simply apply the very nice formula [Grafakos, classical Fourier analysis-Appendix D]: then benefit from the oscillation in both variables and and integrate by parts in both variables. Any ideas how to handle this ? Thank you so much","n\geq 2 I:=\int_{\mathbb{R}^{n}}F(x)dx F I I=\lim_{R\rightarrow +\infty}\int_{B_{R}}F(x)dx, B_{R} R \int_{\mathbb{R}^{3}}\frac{e^{\dot{\imath}|x-y|^2}}{1+|y|}dy. \int_{\mathbb{R}^{3}}\frac{e^{\dot{\imath}|x-y|^2}}{1+|y|}dy=
\int_{\mathbb{S}^{2}}\int_{0}^{\infty}
\frac{e^{\dot{\imath}|\rho\omega-x|^2}\rho^2}{1+\rho}d\rho d\omega\\
=e^{i|x|^{2}}\int_{\mathbb{S}^{2}}\int_{0}^{\infty}
\frac{e^{\dot{\imath} (\rho^2-2x\cdot \omega\,\rho)}\rho^2}{1+\rho}d\rho d\omega. x \omega \lim_{R\rightarrow \infty}\int_{\mathbb{S}^{2}}\int_{0}^{R}
\frac{e^{\dot{\imath} (\rho^2-2x\cdot \omega\,\rho)}\rho^2}{1+\rho}d\rho d\omega \int_{\mathbb{S}^{n-1}}
F(x.\omega)d\omega=c \int_{-1}^{1}(\sqrt{1-s^2})^{n-3}
F(s|x|)ds. \rho \omega","['real-analysis', 'analysis', 'fourier-analysis', 'harmonic-analysis', 'spherical-harmonics']"
98,Prove that $\prod_{k=2}^{+\infty} (1+1/k^2) = \sinh(\pi)/(2 \pi)$. [duplicate],Prove that . [duplicate],\prod_{k=2}^{+\infty} (1+1/k^2) = \sinh(\pi)/(2 \pi),"This question already has answers here : What is the value of $\prod_{n=1}^\infty (1+\frac{1}{n^2})$? (2 answers) Closed 4 years ago . My attempt 1 : Let $x_n=\left(1+\frac{1}{2^2} + \cdots + \frac{1}{n^2} \right)$ , and we have $x_{n+1}>x_n$ . Since $$ 1+\frac{1}{n^2} \le 1+ \frac{1}{(n-1)(n+1)} = \frac{n}{n-1} \cdot \frac{n}{n+1}, $$ then $ x_n < \frac{2n}{n+1} <2$ . Hence $\{x_n\}_{n=2}^{\infty}$ converges. Let $$ \lim_{n \to \infty} x_n =a, $$ and notice that $x_{n+1}=\left(1+\frac{1}{(n+1)^2}\right)x_n $ . By this, we can only get $a=a$ . We can't know the value of $a$ . My attempt 2 : We write $$ \prod_{k=2}^n \left(1+\frac{1}{k^2} \right)=\exp\left(\sum_{k=2}^n \log\left(1+\frac{1}{k^2}\right)\right) $$ So it suffices to know what's the limit of the serise on the right. I still don't know how to finish it. Finally,I used Mathematica to calculate the limit, and it tells me that is $\frac{\sinh(\pi)}{2\pi}$ . But I don't know how to know it without computer. Can you help me?","This question already has answers here : What is the value of $\prod_{n=1}^\infty (1+\frac{1}{n^2})$? (2 answers) Closed 4 years ago . My attempt 1 : Let , and we have . Since then . Hence converges. Let and notice that . By this, we can only get . We can't know the value of . My attempt 2 : We write So it suffices to know what's the limit of the serise on the right. I still don't know how to finish it. Finally,I used Mathematica to calculate the limit, and it tells me that is . But I don't know how to know it without computer. Can you help me?","x_n=\left(1+\frac{1}{2^2} + \cdots + \frac{1}{n^2} \right) x_{n+1}>x_n 
1+\frac{1}{n^2} \le 1+ \frac{1}{(n-1)(n+1)} = \frac{n}{n-1} \cdot \frac{n}{n+1},
  x_n < \frac{2n}{n+1} <2 \{x_n\}_{n=2}^{\infty} 
\lim_{n \to \infty} x_n =a,
 x_{n+1}=\left(1+\frac{1}{(n+1)^2}\right)x_n  a=a a 
\prod_{k=2}^n \left(1+\frac{1}{k^2} \right)=\exp\left(\sum_{k=2}^n \log\left(1+\frac{1}{k^2}\right)\right)
 \frac{\sinh(\pi)}{2\pi}","['calculus', 'limits', 'analysis']"
99,Some means and sequence limit,Some means and sequence limit,,"If two sequences $\{a_n\},\{b_n\}$ satisfy $a_0=a,b_0=b(a>0,b>0)$ , and $$\begin{cases}     a_{n+1}=\frac{a_n+b_n}{2}\\     b_{n+1}=\sqrt{\frac{a_n^2+b_n^2}{2}}.   \end{cases}(n=0,1,2,\ldots)$$ Find the limit of $\{a_n\},\{b_n\}$ . How about $$\begin{cases}     a_{n+1}=\frac{2a_nb_n}{a_n+b_n}\\     b_{n+1}=\sqrt{\frac{a_n^2+b_n^2}{2}}.   \end{cases}(n=0,1,2,\ldots)$$ As we know, the Arithmetic-Geometric Mean is, if two sequences $\{a_n\},\{b_n\}$ satisfy $a_0=a,b_0=b(a>0,b>0)$ , and $$a_ {n+1}=\frac{a_n+b_n}2,\qquad b_{n+1}=\sqrt{a_nb_n}.$$ Let $\mathrm{AGM}(a,b)$ be the limit of $\{a_n\},\{b_n\}$ ,Then $$\int_ {0}^{\pi/2}\frac{1}{\sqrt{a^2\cos^2t+b^2\sin^2t}}dt=\frac{\pi}{2\mathrm{AGM}(a,b)}.$$","If two sequences satisfy , and Find the limit of . How about As we know, the Arithmetic-Geometric Mean is, if two sequences satisfy , and Let be the limit of ,Then","\{a_n\},\{b_n\} a_0=a,b_0=b(a>0,b>0) \begin{cases}
    a_{n+1}=\frac{a_n+b_n}{2}\\
    b_{n+1}=\sqrt{\frac{a_n^2+b_n^2}{2}}.
  \end{cases}(n=0,1,2,\ldots) \{a_n\},\{b_n\} \begin{cases}
    a_{n+1}=\frac{2a_nb_n}{a_n+b_n}\\
    b_{n+1}=\sqrt{\frac{a_n^2+b_n^2}{2}}.
  \end{cases}(n=0,1,2,\ldots) \{a_n\},\{b_n\} a_0=a,b_0=b(a>0,b>0) a_ {n+1}=\frac{a_n+b_n}2,\qquad b_{n+1}=\sqrt{a_nb_n}. \mathrm{AGM}(a,b) \{a_n\},\{b_n\} \int_ {0}^{\pi/2}\frac{1}{\sqrt{a^2\cos^2t+b^2\sin^2t}}dt=\frac{\pi}{2\mathrm{AGM}(a,b)}.","['calculus', 'integration', 'sequences-and-series', 'limits', 'analysis']"
