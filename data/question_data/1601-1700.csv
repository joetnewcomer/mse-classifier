,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,"Evaluating $\int_0^1 \frac{z \log ^2\left(\sqrt{z^2+1}-1\right)}{\sqrt{1-z^2}} \, dz$",Evaluating,"\int_0^1 \frac{z \log ^2\left(\sqrt{z^2+1}-1\right)}{\sqrt{1-z^2}} \, dz","What kind of real analysis tools would you employ for this integral? $$\int_0^1 \frac{z \log ^2\left(\sqrt{1+z^2}-1\right)}{\sqrt{1-z^2}} \, dz$$ EDIT: Here is a supplementary question , the cubic log integral version  $$\int_0^1 \frac{z \log ^3\left(\sqrt{1+z^2}-1\right)}{\sqrt{1-z^2}} \, dz$$","What kind of real analysis tools would you employ for this integral? $$\int_0^1 \frac{z \log ^2\left(\sqrt{1+z^2}-1\right)}{\sqrt{1-z^2}} \, dz$$ EDIT: Here is a supplementary question , the cubic log integral version  $$\int_0^1 \frac{z \log ^3\left(\sqrt{1+z^2}-1\right)}{\sqrt{1-z^2}} \, dz$$",,"['calculus', 'real-analysis', 'integration', 'definite-integrals', 'improper-integrals']"
1,Monotone convergence theorem by Fatou's lemma,Monotone convergence theorem by Fatou's lemma,,"I want to prove the monotone convergence theorem using Fatou's lemma (and its reverse) as exercise, and I need a check; I will use also the following properties of limit inferior and limit superior: Let $f,g: D \to \mathbb{R}$ be functions. Then if $\lim_{x \to c} f(x)$ exists in $\tilde{\mathbb{R}}=\mathbb{R} \cup \{-\infty, + \infty \}$ we have $$\liminf_{x \to c} (f(x) +g(x))=\lim_{x \to c} f(x) + \liminf_{x \to c} g(x)$$ and the same for limit superior. Statement. Let $(X, \mathcal{M}, \mu)$ be a measure space. Assume that $f_0 \le f_1 \le f_2 \le \dots$ is an increasing sequence of functions in $L^{+}(X)$ ($=$ the set of all extended real valued positive measurable functions), such that $f_n \uparrow f$ pointwise. Then $$ \int_X f = \lim_{n \to \infty} \int_X f_n $$ Proof. $g_n=(f-f_n)$ is a sequence of positive measurable functions, and then I can apply Fatou's lemma ; it is $$\int_X \liminf_n (f-f_n) d\mu \le \liminf_n \int_X (f-f_n) d \mu$$Using the property above we have that $$\int_X \liminf_n (f-f_n) d\mu =\int_X (f - \liminf_n f_n) d \mu=0$$ and $$\liminf_n \int_X (f-f_n) d \mu=\liminf_n \left[ \int_X f d\mu - \int_X f_n d\mu \right]=\int_X f d \mu - \liminf_n \int_X f_n d \mu$$ So it is $$\int_X f d \mu \ge \liminf_n \int_X f_ d \mu \ge \int_X \liminf_n f_n d \mu=\int_X f d \mu$$using the lemma on $f_n$, which implies $$\int_X f d \mu = \liminf_n \int_X f_n d \mu$$Now: if $$\begin{split} \int_X f d \mu=\infty & \ \longrightarrow \ \underbrace{\liminf_n \int_X f_n d \mu}_{=\infty}  \le \limsup_n \int_X f_n d \mu = \infty \\ & \ \longrightarrow \ \int_X f d \mu = \lim_n \int_X f_n d \mu = \infty \end{split}$$ and if $$\int_X f d \mu < \infty$$ We can apply, in a similar way as above, the reverse of Fatou's lemma considering that $f-f_n \le f - f_0$ and that $f-f_0$ is integrable ( - here I've used the fundamental hypothesis of $(f_n)_{n \in \mathbb{N}}$ increasing). We obtain $$ \int_X f d \mu=\limsup_n \int_X f_n d \mu $$and then the thesis. What do you think about it? Thank you very much.","I want to prove the monotone convergence theorem using Fatou's lemma (and its reverse) as exercise, and I need a check; I will use also the following properties of limit inferior and limit superior: Let $f,g: D \to \mathbb{R}$ be functions. Then if $\lim_{x \to c} f(x)$ exists in $\tilde{\mathbb{R}}=\mathbb{R} \cup \{-\infty, + \infty \}$ we have $$\liminf_{x \to c} (f(x) +g(x))=\lim_{x \to c} f(x) + \liminf_{x \to c} g(x)$$ and the same for limit superior. Statement. Let $(X, \mathcal{M}, \mu)$ be a measure space. Assume that $f_0 \le f_1 \le f_2 \le \dots$ is an increasing sequence of functions in $L^{+}(X)$ ($=$ the set of all extended real valued positive measurable functions), such that $f_n \uparrow f$ pointwise. Then $$ \int_X f = \lim_{n \to \infty} \int_X f_n $$ Proof. $g_n=(f-f_n)$ is a sequence of positive measurable functions, and then I can apply Fatou's lemma ; it is $$\int_X \liminf_n (f-f_n) d\mu \le \liminf_n \int_X (f-f_n) d \mu$$Using the property above we have that $$\int_X \liminf_n (f-f_n) d\mu =\int_X (f - \liminf_n f_n) d \mu=0$$ and $$\liminf_n \int_X (f-f_n) d \mu=\liminf_n \left[ \int_X f d\mu - \int_X f_n d\mu \right]=\int_X f d \mu - \liminf_n \int_X f_n d \mu$$ So it is $$\int_X f d \mu \ge \liminf_n \int_X f_ d \mu \ge \int_X \liminf_n f_n d \mu=\int_X f d \mu$$using the lemma on $f_n$, which implies $$\int_X f d \mu = \liminf_n \int_X f_n d \mu$$Now: if $$\begin{split} \int_X f d \mu=\infty & \ \longrightarrow \ \underbrace{\liminf_n \int_X f_n d \mu}_{=\infty}  \le \limsup_n \int_X f_n d \mu = \infty \\ & \ \longrightarrow \ \int_X f d \mu = \lim_n \int_X f_n d \mu = \infty \end{split}$$ and if $$\int_X f d \mu < \infty$$ We can apply, in a similar way as above, the reverse of Fatou's lemma considering that $f-f_n \le f - f_0$ and that $f-f_0$ is integrable ( - here I've used the fundamental hypothesis of $(f_n)_{n \in \mathbb{N}}$ increasing). We obtain $$ \int_X f d \mu=\limsup_n \int_X f_n d \mu $$and then the thesis. What do you think about it? Thank you very much.",,"['real-analysis', 'measure-theory']"
2,Compact = Closed + Bounded + (?),Compact = Closed + Bounded + (?),,"In $\mathbb{R}^n$ we know (Heine-Borel Theorem) that a set is compact if and only if it is closed and bounded. In $C(X)$ for a compact metric space $X$, we know (corollary of Ascoli-Arzela Theorem) that a set is compact if and only if it is closed, bounded, and equicontinuous. I am looking for as many examples as I can of other spaces where the extra condition for compactness is known. Also, I am looking for as many examples as I can of (important) spaces where the extra condition is not currently known. I am planning on doing some research (under a professor) and I thought this topic was particularly interesting, so I would very much appreciate some examples to start off with, just so I can get a feel for the problem.","In $\mathbb{R}^n$ we know (Heine-Borel Theorem) that a set is compact if and only if it is closed and bounded. In $C(X)$ for a compact metric space $X$, we know (corollary of Ascoli-Arzela Theorem) that a set is compact if and only if it is closed, bounded, and equicontinuous. I am looking for as many examples as I can of other spaces where the extra condition for compactness is known. Also, I am looking for as many examples as I can of (important) spaces where the extra condition is not currently known. I am planning on doing some research (under a professor) and I thought this topic was particularly interesting, so I would very much appreciate some examples to start off with, just so I can get a feel for the problem.",,"['real-analysis', 'soft-question']"
3,"Does every positive, decreasing, real sequence whose series converges have a corresponding convex sequence greater than it whose series converges?","Does every positive, decreasing, real sequence whose series converges have a corresponding convex sequence greater than it whose series converges?",,"Definition: A real sequence $\ (x_n)_n\ $ is convex if $\ x_n - x_{n+1} \geq x_{n+1} - x_{n+2}\quad \forall\ n\in\mathbb{N}. $ Suppose $\ (x_n)_n\ $ is a positive, decreasing sequence of real numbers such that $\ \displaystyle\sum x_n\ $ converges. Proposition: There exists a convex sequence $\ (y_n)_n,\ $ such that: $\ y_n\geq x_n\quad \forall\ n\in\mathbb{N},\ $ and $\ \displaystyle \sum_n y_n\ $ converges. I suspect there is some counter-example, and I think my best attempt to find one is that maybe there is an increasing subsequence $\ (A_n)_n\ $ of $\ \mathbb{N}\ $ such that $\ x_n:= \frac{1}{A_k}\ $ for all $\ n\ $ with $\ A_{k-1} < n \leq A_k,\ $ and then maybe the convexity of $\ (y_n)_n\ $ sort of forces $\ y_n \approx \frac{1}{n}\ ?$ But I'm not sure if this is true or how to prove this. Edit: I have spent more time thinking about this problem without further progress towards a solution. Therefore I have added a bounty of $+50$ .","Definition: A real sequence is convex if Suppose is a positive, decreasing sequence of real numbers such that converges. Proposition: There exists a convex sequence such that: and converges. I suspect there is some counter-example, and I think my best attempt to find one is that maybe there is an increasing subsequence of such that for all with and then maybe the convexity of sort of forces But I'm not sure if this is true or how to prove this. Edit: I have spent more time thinking about this problem without further progress towards a solution. Therefore I have added a bounty of .","\ (x_n)_n\  \ x_n - x_{n+1} \geq x_{n+1} - x_{n+2}\quad \forall\ n\in\mathbb{N}.  \ (x_n)_n\  \ \displaystyle\sum x_n\  \ (y_n)_n,\  \ y_n\geq x_n\quad \forall\ n\in\mathbb{N},\  \ \displaystyle \sum_n y_n\  \ (A_n)_n\  \ \mathbb{N}\  \ x_n:= \frac{1}{A_k}\  \ n\  \ A_{k-1} < n \leq A_k,\  \ (y_n)_n\  \ y_n \approx \frac{1}{n}\ ? +50","['real-analysis', 'sequences-and-series', 'examples-counterexamples', 'recreational-mathematics', 'problem-solving']"
4,"$f(x) = x$ when x is rational, $f(x) = 0$ when x is irrational. Find all points at which $f$ is continous.","when x is rational,  when x is irrational. Find all points at which  is continous.",f(x) = x f(x) = 0 f,"Let $f:\mathbf{R} \to \mathbf{R}$ defined as : $f(x) = x$ when x is   rational, $f(x) = 0$ when x is irrational. Find all points at which $f$ is continous. Let $c \in Q - \{0\}$ $ \exists \langle x_n\rangle$ in $R \setminus Q $ such that $x_n \to c $ $f(x_n) = 0 \to 0 $ now $ f(c) = c \neq 0$ Therefore, $ f(x_n)$ does not converges to $f(c)$ Hence $f$ is discontinuous on $Q - \{0\}$ Let $c \in R \setminus Q$ $\exists \langle y_n\rangle$ in $Q$ such that $y_n \to c$ $f(y_n) = y_n \to c$ now $f(c) = 0$ because $c \in R\setminus Q.$ $\; \;$ also $c \neq 0$ (same reason) therefore, $f$ is discontinuous on $R\setminus Q$ Finally for $c=0$ we have $|f(x) - f(0)| \leq ||x| - 0| \leq |x|$ Therefore $\forall \; \; \epsilon>0 \; \; $$\exists \; \; \delta  = \epsilon> 0$ such that if $|x|< \delta \Rightarrow |f(x)-f(0)|<\epsilon$ Therefore $f$ is continuous at $c=0$ , and it is the only point at which it is continuous Is my proof correct ? (particularly for c=0 part)","Let defined as : when x is   rational, when x is irrational. Find all points at which is continous. Let in such that now Therefore, does not converges to Hence is discontinuous on Let in such that now because also (same reason) therefore, is discontinuous on Finally for we have Therefore such that if Therefore is continuous at , and it is the only point at which it is continuous Is my proof correct ? (particularly for c=0 part)",f:\mathbf{R} \to \mathbf{R} f(x) = x f(x) = 0 f c \in Q - \{0\}  \exists \langle x_n\rangle R \setminus Q  x_n \to c  f(x_n) = 0 \to 0   f(c) = c \neq 0  f(x_n) f(c) f Q - \{0\} c \in R \setminus Q \exists \langle y_n\rangle Q y_n \to c f(y_n) = y_n \to c f(c) = 0 c \in R\setminus Q. \; \; c \neq 0 f R\setminus Q c=0 |f(x) - f(0)| \leq ||x| - 0| \leq |x| \forall \; \; \epsilon>0 \; \; \exists \; \; \delta  = \epsilon> 0 |x|< \delta \Rightarrow |f(x)-f(0)|<\epsilon f c=0,"['real-analysis', 'proof-verification', 'continuity']"
5,Differentiating Under the Integral Proof,Differentiating Under the Integral Proof,,"There are many variations of ""differentiating under the integral sign"" theorem; here is one: If $U$ is an open subset of $\mathbb{R}^n$ and $f:U \times [a,b] \rightarrow \mathbb{R}$ is continuous with continuous partial derivatives $\partial_1 f, \dots \partial_n f$ then the function $$ \phi(x) = \int^b_a f(x,t)dt $$ is continuously differentiable and $$ \partial_i \phi (x) = \int^b_a \partial_i f(x,t)dt $$ Can anyone suggest a textbook that provides a proof of this version of the theorem?","There are many variations of ""differentiating under the integral sign"" theorem; here is one: If $U$ is an open subset of $\mathbb{R}^n$ and $f:U \times [a,b] \rightarrow \mathbb{R}$ is continuous with continuous partial derivatives $\partial_1 f, \dots \partial_n f$ then the function $$ \phi(x) = \int^b_a f(x,t)dt $$ is continuously differentiable and $$ \partial_i \phi (x) = \int^b_a \partial_i f(x,t)dt $$ Can anyone suggest a textbook that provides a proof of this version of the theorem?",,"['real-analysis', 'reference-request', 'leibniz-integral-rule']"
6,Hardy's Inequality: Problems $3.14$ and $3.15$ in Rudin's RCA,Hardy's Inequality: Problems  and  in Rudin's RCA,3.14 3.15,"In Problem $3.14$ , we prove (a) Hardy's inequality, (b) the condition for equality, and I shall talk about (c), (d) below. Problem $3.15$ is the discrete case of Hardy's inequality. I have asked three related questions in a single post itself, since all of them are related to Hardy's inequality , and none should be too involved. There are some existing posts on MSE related to these topics, so I shall link them right away and point out that my question is not a duplicate: Post 1 , Post 2 , Post 3 , Post 4 . For the sake of mentioning it, Hardy's inequality is: For $p\in (1,\infty)$ , $f\in L^p((0,\infty))$ relative to the Lebesgue measure, and $$F(x) = \frac{1}{x}\int_0^x f(t)\ dt\quad (0 < x < \infty)$$ we have $$\|F\|_p \le \frac{p}{p-1} \|f\|_p$$ Question 1: This is Problem $3.14(c)$ in Rudin's book. Prove that the constant $p/(p-1)$ cannot be replaced by a smaller one. In one of the linked posts, there is some discussion on how this is the best constant, but I was unable to follow it. My sense is that it suffices to find a counterexample, i.e. for every constant $\beta$ smaller than $p/(p-1)$ , we need a function $f_\beta\in L^p((0,\infty))$ which does not satisfy the required inequality. Why are we complicating things? If I'm thinking right, could someone help me find a counterexample ? Question 2: This appears as Problem $3.14(d)$ of Rudin's book. The author is trying to emphasize that the inequality is not for $p = 1$ . If $f > 0$ and $f\in L^1$ , prove that $F\notin L^1$ . I found an example, $f(x) = e^{-x}$ . Then $F(x) = \frac{1-e^{-x}}{x}$ . $F$ 's integral diverges, since the integral of $1/x$ diverges (use limit comparison test for integrals). However, as @David C. Ullrich pointed out, this is not enough. Question 3: This is Problem $3.15$ in the same book and is the discrete case of Hardy's inequality. Suppose $\{a_n\}$ is a sequence of positive numbers. Prove that $$\sum_{N=1}^\infty \left(\frac{1}{N} \sum_{n=1}^N a_n \right)^p \le \left(\frac{p}{p-1} \right)^p \sum_{n=1}^\infty a_n^p$$ if $1 < p < \infty$ . If $a_n\ge a_{n+1}$ , the result can be made to follow from Hardy's inequality. This special case implies the general one. I took $f = \sum_{n=1}^\infty a_n \mathbf{1}_{[n,n+1]}$ . Then $f\in L^p$ only if $\sum_{n=1}^\infty a^p_n < \infty$ . If $f\notin L^p$ , the inequality is trivial. So let's take $f\in L^p$ . Now using Hardy's inequality, we have $\|F\|_p \le \frac{p}{p-1} \|f\|_p$ . What is $F$ ? $$F(x) = \frac{1}{x}\int_0^x \sum_{n=1}^\infty a_n\mathbf{1}_{[n,n+1]}(t)\ dt = \frac{1}{x}\left(\sum_{n=1}^{\lfloor x\rfloor} a_n + (x - \lfloor x\rfloor)a_{\lfloor x\rfloor + 1} \right)$$ How do I proceed? P.S. I have already solved Problems $3.14(a)$ and $3.14(b)$ , i.e. proving Hardy's inequality and showing that equality holds iff $f = 0$ a.e.","In Problem , we prove (a) Hardy's inequality, (b) the condition for equality, and I shall talk about (c), (d) below. Problem is the discrete case of Hardy's inequality. I have asked three related questions in a single post itself, since all of them are related to Hardy's inequality , and none should be too involved. There are some existing posts on MSE related to these topics, so I shall link them right away and point out that my question is not a duplicate: Post 1 , Post 2 , Post 3 , Post 4 . For the sake of mentioning it, Hardy's inequality is: For , relative to the Lebesgue measure, and we have Question 1: This is Problem in Rudin's book. Prove that the constant cannot be replaced by a smaller one. In one of the linked posts, there is some discussion on how this is the best constant, but I was unable to follow it. My sense is that it suffices to find a counterexample, i.e. for every constant smaller than , we need a function which does not satisfy the required inequality. Why are we complicating things? If I'm thinking right, could someone help me find a counterexample ? Question 2: This appears as Problem of Rudin's book. The author is trying to emphasize that the inequality is not for . If and , prove that . I found an example, . Then . 's integral diverges, since the integral of diverges (use limit comparison test for integrals). However, as @David C. Ullrich pointed out, this is not enough. Question 3: This is Problem in the same book and is the discrete case of Hardy's inequality. Suppose is a sequence of positive numbers. Prove that if . If , the result can be made to follow from Hardy's inequality. This special case implies the general one. I took . Then only if . If , the inequality is trivial. So let's take . Now using Hardy's inequality, we have . What is ? How do I proceed? P.S. I have already solved Problems and , i.e. proving Hardy's inequality and showing that equality holds iff a.e.","3.14 3.15 p\in (1,\infty) f\in L^p((0,\infty)) F(x) = \frac{1}{x}\int_0^x f(t)\ dt\quad (0 < x < \infty) \|F\|_p \le \frac{p}{p-1} \|f\|_p 3.14(c) p/(p-1) \beta p/(p-1) f_\beta\in L^p((0,\infty)) 3.14(d) p = 1 f > 0 f\in L^1 F\notin L^1 f(x) = e^{-x} F(x) = \frac{1-e^{-x}}{x} F 1/x 3.15 \{a_n\} \sum_{N=1}^\infty \left(\frac{1}{N} \sum_{n=1}^N a_n \right)^p \le \left(\frac{p}{p-1} \right)^p \sum_{n=1}^\infty a_n^p 1 < p < \infty a_n\ge a_{n+1} f = \sum_{n=1}^\infty a_n \mathbf{1}_{[n,n+1]} f\in L^p \sum_{n=1}^\infty a^p_n < \infty f\notin L^p f\in L^p \|F\|_p \le \frac{p}{p-1} \|f\|_p F F(x) = \frac{1}{x}\int_0^x \sum_{n=1}^\infty a_n\mathbf{1}_{[n,n+1]}(t)\ dt = \frac{1}{x}\left(\sum_{n=1}^{\lfloor x\rfloor} a_n + (x - \lfloor x\rfloor)a_{\lfloor x\rfloor + 1} \right) 3.14(a) 3.14(b) f = 0","['real-analysis', 'functional-analysis', 'inequality', 'lp-spaces']"
7,"If $f$ is Lebesgue integrable on $[0,2]$ and $\int_E fdx=0$ for all measurable set E such that $m(E)=\pi/2$. Prove or disprove that $f=0$ a.e.",If  is Lebesgue integrable on  and  for all measurable set E such that . Prove or disprove that  a.e.,"f [0,2] \int_E fdx=0 m(E)=\pi/2 f=0","Let $f$ be a Lebesgue integrable function on $[0,2]$. If $\int_E fdx=0$ for all measurable set $E$, such that  $m(E)=\pi/2$. Is $f=0$ a.e. Prove or disprove I could not figure out anything. Can a function be very oscillatory so that on every interval its integral is zero? Any hint and approach are welcome.","Let $f$ be a Lebesgue integrable function on $[0,2]$. If $\int_E fdx=0$ for all measurable set $E$, such that  $m(E)=\pi/2$. Is $f=0$ a.e. Prove or disprove I could not figure out anything. Can a function be very oscillatory so that on every interval its integral is zero? Any hint and approach are welcome.",,"['real-analysis', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
8,"How to show $E:=\{x\in [0,1]: |f^{-1}(\{x\})|\,\mbox{ is even} \}$ is countable?",How to show  is countable?,"E:=\{x\in [0,1]: |f^{-1}(\{x\})|\,\mbox{ is even} \}","Hi everybody I have seen the following question which I could not solve it, so I thought I can share the question with you and ask for help. Question: Let $f:[0,1]\to [0,1]$  be a continuous function such that $f(0)=0$ and $f(1)=1$. Moreover assume $f^{-1}(\{x\})$ is finite for all $x$. Prove  $$E:=\{x\in [0,1]: |f^{-1}(\{x\})|\,\mbox{ is even} \}$$ is countable.","Hi everybody I have seen the following question which I could not solve it, so I thought I can share the question with you and ask for help. Question: Let $f:[0,1]\to [0,1]$  be a continuous function such that $f(0)=0$ and $f(1)=1$. Moreover assume $f^{-1}(\{x\})$ is finite for all $x$. Prove  $$E:=\{x\in [0,1]: |f^{-1}(\{x\})|\,\mbox{ is even} \}$$ is countable.",,['real-analysis']
9,A question about continuous curves in $\mathbb{R}^2$,A question about continuous curves in,\mathbb{R}^2,"Let $f:[a,b]\longrightarrow\mathbb{R}^2$ be a continuous function such that  $$f(a)=(0,0),\ f(b)=(0,1).$$ Is it true that there must exist $t_1,t_2\in [a,b]$ such that $\displaystyle f(t_1)-f(t_2)=(0,\frac{1}{2})?$ If not, please give a counterexample.","Let $f:[a,b]\longrightarrow\mathbb{R}^2$ be a continuous function such that  $$f(a)=(0,0),\ f(b)=(0,1).$$ Is it true that there must exist $t_1,t_2\in [a,b]$ such that $\displaystyle f(t_1)-f(t_2)=(0,\frac{1}{2})?$ If not, please give a counterexample.",,"['real-analysis', 'analysis']"
10,Taylor expansion of $(1+x)^α$ to binomial series – why does the remainder term converge?,Taylor expansion of  to binomial series – why does the remainder term converge?,(1+x)^α,"For $α ∈ ℝ$ the function $g_α \colon B_1(0) → ℝ, x ↦ (1+x)^α$ is $C^∞$ and $g_α^{(n)}(x) = n! \tbinom{α}{n}(1+x)^{α-n}$, where $\tbinom{α}{n} = \frac{α(α-1)\cdots(α-n+1)}{n!}$ is the generalized binomial coefficient. I want to show that $g_α$ has a Taylor expansion $g_α (x) = \sum_{k=0}^∞ \tbinom{α}{k}x^k$. I can easily show that the series really converges for $|x| < 1$, but can I use this fact to show that the remainder term $$R_n(x) = \int_0^x \frac{(x-t)^n}{n!} (n+1)! \tbinom{α}{n+1}(1+t)^{α-n-1} dt$$ or in Lagrange form $$\quad R_n(x) = \frac{x^{n+1}}{(n+1)!} (n+1)! \binom{α}{n+1}(1+ξ)^{α-n-1} = x^{n+1} \binom{α}{n+1}(1+ξ)^{α-n-1}$$ converges to zero for $|x| < 1$? How else can I show that the remainder term converges to zero for $|x| < 1$? Progress : Using the Lagrange form, for $x ∈ (-1,1)$ and $n ∈ ℕ$ there is an $h=h(n,x) ∈ (0,1)$ such that: $$|R_n(x)| = \Big| x^{n+1} \binom{α}{n+1}(1+hx)^{α-n-1} \Big| = \Big| \binom{α}{n+1}\big(\frac{x}{1+hx}\big)^{n+1}(1+ξ)^{α} \Big| $$ If $x > 0$, then $\big|\frac{x}{1+hx}\big| < {\big|\frac{x}{1+x}\big|}< 1$ and so for $q := \big|\frac{x}{1+x}\big|$, since $\sum_{k=0}^∞ \tbinom{α}{n} q^n$ converges: $$|R_n(x)| ≤ \Big| \binom{α}{n+1} · q^{n+1} ·(1+ξ)^{α} \Big| \overset{n → ∞}{\longrightarrow} 0$$ The problem is still open for $x < 0$. Mhenni suggested to use Stirling approximation, but I want to do this more elementary and I think it is possible.","For $α ∈ ℝ$ the function $g_α \colon B_1(0) → ℝ, x ↦ (1+x)^α$ is $C^∞$ and $g_α^{(n)}(x) = n! \tbinom{α}{n}(1+x)^{α-n}$, where $\tbinom{α}{n} = \frac{α(α-1)\cdots(α-n+1)}{n!}$ is the generalized binomial coefficient. I want to show that $g_α$ has a Taylor expansion $g_α (x) = \sum_{k=0}^∞ \tbinom{α}{k}x^k$. I can easily show that the series really converges for $|x| < 1$, but can I use this fact to show that the remainder term $$R_n(x) = \int_0^x \frac{(x-t)^n}{n!} (n+1)! \tbinom{α}{n+1}(1+t)^{α-n-1} dt$$ or in Lagrange form $$\quad R_n(x) = \frac{x^{n+1}}{(n+1)!} (n+1)! \binom{α}{n+1}(1+ξ)^{α-n-1} = x^{n+1} \binom{α}{n+1}(1+ξ)^{α-n-1}$$ converges to zero for $|x| < 1$? How else can I show that the remainder term converges to zero for $|x| < 1$? Progress : Using the Lagrange form, for $x ∈ (-1,1)$ and $n ∈ ℕ$ there is an $h=h(n,x) ∈ (0,1)$ such that: $$|R_n(x)| = \Big| x^{n+1} \binom{α}{n+1}(1+hx)^{α-n-1} \Big| = \Big| \binom{α}{n+1}\big(\frac{x}{1+hx}\big)^{n+1}(1+ξ)^{α} \Big| $$ If $x > 0$, then $\big|\frac{x}{1+hx}\big| < {\big|\frac{x}{1+x}\big|}< 1$ and so for $q := \big|\frac{x}{1+x}\big|$, since $\sum_{k=0}^∞ \tbinom{α}{n} q^n$ converges: $$|R_n(x)| ≤ \Big| \binom{α}{n+1} · q^{n+1} ·(1+ξ)^{α} \Big| \overset{n → ∞}{\longrightarrow} 0$$ The problem is still open for $x < 0$. Mhenni suggested to use Stirling approximation, but I want to do this more elementary and I think it is possible.",,"['calculus', 'real-analysis', 'power-series', 'taylor-expansion']"
11,"If $f:[a,b]\to\mathbb{R}$ continuous at all but countably infinitely many points, $g:[0,1]\to[a,b]$ continuous, can $f\circ g$ be a.e. discontinuous?","If  continuous at all but countably infinitely many points,  continuous, can  be a.e. discontinuous?","f:[a,b]\to\mathbb{R} g:[0,1]\to[a,b] f\circ g","Let $f:[a,b]\to\mathbb{R}$ be continuous except at $\{x_1,x_2,\cdots,x_n,\cdots\}$ , $g:[0,1]\to[a,b]$ be a continuous function. My question is: can the points of discontinuity of $f\circ g$ have full measure? The set of points of discontinuity of $f\circ g$ is included in $\bigcup^\infty_{n=1} \partial g^{-1}(x_n)$ , where $\{\partial g^{-1}(x_n)\}_{n\ge 1}$ is a countably infinite collection of disjoint closed sets with empty interior. I'm well aware that a subset of $[0,1]$ with full measure can be written as a countable union of closed sets with empty interior (there are many simple constructions), but I don't know if there is a subset of $[0,1]$ with full measure that is a countable union of disjoint closed sets with empty interior. Also, even there is a subset $E$ of $[0,1]$ with full measure such that $E = \bigcup^\infty_{n=1} C_n$ , where $\{C_n\}$ are disjoint closed sets with empty interior, can we find $\{x_1,x_2,\cdots,x_n,\cdots\}$ and define a continuous function $g$ such that $g^{-1}(x_n) = C_n$ ? Edit : Note that $f\circ g$ cannot be a.e. discontinuous if $f$ has only finitely many points of discontinuity, since $\bigcup^{N}_{n=1} \partial g^{-1}(x_n)$ would be closed and, by Baire category theorem, have empty interior, so it cannot have full measure. On the other hand, the set of points of discontinuity of $f\circ g$ can have measure arbitrarily close to 1, even if $f$ is just discontinuous at one point.","Let be continuous except at , be a continuous function. My question is: can the points of discontinuity of have full measure? The set of points of discontinuity of is included in , where is a countably infinite collection of disjoint closed sets with empty interior. I'm well aware that a subset of with full measure can be written as a countable union of closed sets with empty interior (there are many simple constructions), but I don't know if there is a subset of with full measure that is a countable union of disjoint closed sets with empty interior. Also, even there is a subset of with full measure such that , where are disjoint closed sets with empty interior, can we find and define a continuous function such that ? Edit : Note that cannot be a.e. discontinuous if has only finitely many points of discontinuity, since would be closed and, by Baire category theorem, have empty interior, so it cannot have full measure. On the other hand, the set of points of discontinuity of can have measure arbitrarily close to 1, even if is just discontinuous at one point.","f:[a,b]\to\mathbb{R} \{x_1,x_2,\cdots,x_n,\cdots\} g:[0,1]\to[a,b] f\circ g f\circ g \bigcup^\infty_{n=1} \partial g^{-1}(x_n) \{\partial g^{-1}(x_n)\}_{n\ge 1} [0,1] [0,1] E [0,1] E = \bigcup^\infty_{n=1} C_n \{C_n\} \{x_1,x_2,\cdots,x_n,\cdots\} g g^{-1}(x_n) = C_n f\circ g f \bigcup^{N}_{n=1} \partial g^{-1}(x_n) f\circ g f","['real-analysis', 'measure-theory', 'continuity']"
12,On the integral $\int_0^1\frac{\arctan\sqrt{t^2+a}}{(t^2+b)\sqrt{t^2+a}}dt$,On the integral,\int_0^1\frac{\arctan\sqrt{t^2+a}}{(t^2+b)\sqrt{t^2+a}}dt,"Let $0<b<a$ and define $$J(a,b)=\int_0^1\frac{\arctan\sqrt{t^2+a}}{(t^2+b)\sqrt{t^2+a}}dt.\tag1$$ I am seeking a closed form for $J(a,b)$ . I was motivated to find a closed form for $(1)$ after seeing the approach taken in this answer and thinking that it could be generalized. I have actually succeeded in finding a closed form for the special case $(a,b)=(a,\tfrac{a}{2})$ : $$J(a,\tfrac{a}{2})=\frac{\pi^2}{2a}-\frac{\pi}{a}\arctan\sqrt{a+1}-\frac1a\arctan^2\sqrt{\frac{2}{a}},$$ which comes from the more general result $$J(a,b)+J(a,a-b)=\phi_2(a,b)-\phi_1(b)\phi_1(a-b)+\phi_2(a,a-b),\tag2$$ where $$\phi_1(x)=\int_0^1\frac{dt}{x+t^2}=\int_1^\infty\frac{dt}{1+xt^2}=\frac1{\sqrt x}\arctan\frac1{\sqrt x},$$ and $$\phi_2(a,b)=\frac{\pi}{2\sqrt{b(a-b)}}\arctan\sqrt{\frac{a-b}{b(a+1)}}.$$ I will supply a proof below. Proof of $(2)$ . Let $$f(z)=\int_0^1\frac{\arctan(z\sqrt{t^2+a})}{(t^2+b)\sqrt{t^2+a}}dt,$$ so that $J(a,b)=f(1)$ . Then clearly $$J(a,b)=f(1)=\lim_{z\to\infty}f(z)-\int_1^\infty f'(z)dz.$$ Then since $\lim_{z\to\infty}\arctan(xz)=\pi/2$ for $x>0$ , we have $$J(a,b)=\frac\pi2\int_0^1\frac{dt}{(t^2+b)\sqrt{t^2+a}}-\int_1^\infty\int_0^1\frac{dt}{(t^2+b)(z^2t^2+az^2+1)}dz.$$ The first integral is relatively simple: $$\begin{align} \int_0^1\frac{dt}{(t^2+b)\sqrt{t^2+a}}&=\int_0^{1/\sqrt{a}}\frac{du}{(au^2+b)\sqrt{u^2+1}}\qquad[t\mapsto u\sqrt{a}]\\ &=\int_0^{\arctan 1/\sqrt{a}}\frac{\sec^2x\,dx}{(a\tan^2x+b)\sqrt{\tan^2x+1}}\qquad[u\mapsto \tan x]\\ &=\int_0^{\arctan 1/\sqrt{a}}\frac{\cos x\,dx}{b+(a-b)\sin^2x}\\ &=\int_0^{1/\sqrt{a+1}}\frac{dt}{b+(a-b)t^2}\qquad [\sin x\mapsto t]\\ &=\left.\frac{1}{\sqrt{b(a-b)}}\arctan\left(t\sqrt{\frac{a-b}{b}}\right)\right|_0^{1/\sqrt{a+1}}\\ &=\frac{1}{\sqrt{b(a-b)}}\arctan\sqrt{\frac{a-b}{b(a+1)}}=\frac2\pi\phi_2(a,b).\tag3 \end{align}$$ The next integral is also manageable: $$\begin{align} \int_1^\infty f'(z)dz&=\int_1^\infty\int_0^1\frac{dt}{(t^2+b)(z^2t^2+az^2+1)}dz\\ &=\int_1^\infty\int_0^1\frac{1}{1+(a-b)z^2}\left(\frac{1}{t^2+b}-\frac{1}{t^2+a+1/z^2}\right)dtdz\\ &=\int_1^\infty\frac{1}{1+(a-b)z^2}\left(\phi_1(b)-\phi_1(a+1/z^2)\right)dz\\ &=\phi_1(b)\phi_1(a-b)-\int_1^\infty \frac{\arctan(1/\sqrt{a+1/z^2})}{(1+(a-b)z^2)\sqrt{a+1/z^2}}dz\\ &=\phi_1(b)\phi_1(a-b)-\int_0^1 \frac{\arctan(1/\sqrt{t^2+a})}{(t^2+(a-b))\sqrt{t^2+a}}dt\qquad [z\mapsto 1/t]\\ &=\phi_1(b)\phi_1(a-b)-\int_0^1 \frac{\tfrac\pi2-\arctan\sqrt{t^2+a}}{(t^2+(a-b))\sqrt{t^2+a}}dt\\ &=\phi_1(b)\phi_1(a-b)-\frac\pi2\int_0^1 \frac{dt}{(t^2+(a-b))\sqrt{t^2+a}}+\int_0^1 \frac{\arctan\sqrt{t^2+a}}{(t^2+(a-b))\sqrt{t^2+a}}dt\\ &=\phi_1(b)\phi_1(a-b)-\phi_2(a,a-b)+J(a,a-b).\tag4 \end{align}$$ Then from $(3)$ and $(4)$ , $$J(a,b)=\phi_2(a,b)-\phi_1(b)\phi_1(a-b)+\phi_2(a,a-b)-J(a,a-b),$$ as desired. Is there some way to find a closed form for $J$ ? EDIT: One may use the series $$g(z)=\frac{\arctan\sqrt z}{\sqrt z}=\sum_{n\ge1}\frac{(-1)^n}{2n-1}\left(\frac{1}{z^n}-\frac{2}{\sqrt z}\right)\qquad z\ge1$$ to get $$J(a,b)=-\phi_2(a,b)+\sum_{n\ge1}\frac{(-1)^n}{2n-1}\int_0^1\frac{dt}{(t^2+b)(t^2+a)^n},$$ where $\phi_2$ is defined above. The integral $\int_0^1\frac{dt}{(t^2+b)(t^2+a)^n}$ is really not nice though.","Let and define I am seeking a closed form for . I was motivated to find a closed form for after seeing the approach taken in this answer and thinking that it could be generalized. I have actually succeeded in finding a closed form for the special case : which comes from the more general result where and I will supply a proof below. Proof of . Let so that . Then clearly Then since for , we have The first integral is relatively simple: The next integral is also manageable: Then from and , as desired. Is there some way to find a closed form for ? EDIT: One may use the series to get where is defined above. The integral is really not nice though.","0<b<a J(a,b)=\int_0^1\frac{\arctan\sqrt{t^2+a}}{(t^2+b)\sqrt{t^2+a}}dt.\tag1 J(a,b) (1) (a,b)=(a,\tfrac{a}{2}) J(a,\tfrac{a}{2})=\frac{\pi^2}{2a}-\frac{\pi}{a}\arctan\sqrt{a+1}-\frac1a\arctan^2\sqrt{\frac{2}{a}}, J(a,b)+J(a,a-b)=\phi_2(a,b)-\phi_1(b)\phi_1(a-b)+\phi_2(a,a-b),\tag2 \phi_1(x)=\int_0^1\frac{dt}{x+t^2}=\int_1^\infty\frac{dt}{1+xt^2}=\frac1{\sqrt x}\arctan\frac1{\sqrt x}, \phi_2(a,b)=\frac{\pi}{2\sqrt{b(a-b)}}\arctan\sqrt{\frac{a-b}{b(a+1)}}. (2) f(z)=\int_0^1\frac{\arctan(z\sqrt{t^2+a})}{(t^2+b)\sqrt{t^2+a}}dt, J(a,b)=f(1) J(a,b)=f(1)=\lim_{z\to\infty}f(z)-\int_1^\infty f'(z)dz. \lim_{z\to\infty}\arctan(xz)=\pi/2 x>0 J(a,b)=\frac\pi2\int_0^1\frac{dt}{(t^2+b)\sqrt{t^2+a}}-\int_1^\infty\int_0^1\frac{dt}{(t^2+b)(z^2t^2+az^2+1)}dz. \begin{align}
\int_0^1\frac{dt}{(t^2+b)\sqrt{t^2+a}}&=\int_0^{1/\sqrt{a}}\frac{du}{(au^2+b)\sqrt{u^2+1}}\qquad[t\mapsto u\sqrt{a}]\\
&=\int_0^{\arctan 1/\sqrt{a}}\frac{\sec^2x\,dx}{(a\tan^2x+b)\sqrt{\tan^2x+1}}\qquad[u\mapsto \tan x]\\
&=\int_0^{\arctan 1/\sqrt{a}}\frac{\cos x\,dx}{b+(a-b)\sin^2x}\\
&=\int_0^{1/\sqrt{a+1}}\frac{dt}{b+(a-b)t^2}\qquad [\sin x\mapsto t]\\
&=\left.\frac{1}{\sqrt{b(a-b)}}\arctan\left(t\sqrt{\frac{a-b}{b}}\right)\right|_0^{1/\sqrt{a+1}}\\
&=\frac{1}{\sqrt{b(a-b)}}\arctan\sqrt{\frac{a-b}{b(a+1)}}=\frac2\pi\phi_2(a,b).\tag3
\end{align} \begin{align}
\int_1^\infty f'(z)dz&=\int_1^\infty\int_0^1\frac{dt}{(t^2+b)(z^2t^2+az^2+1)}dz\\
&=\int_1^\infty\int_0^1\frac{1}{1+(a-b)z^2}\left(\frac{1}{t^2+b}-\frac{1}{t^2+a+1/z^2}\right)dtdz\\
&=\int_1^\infty\frac{1}{1+(a-b)z^2}\left(\phi_1(b)-\phi_1(a+1/z^2)\right)dz\\
&=\phi_1(b)\phi_1(a-b)-\int_1^\infty \frac{\arctan(1/\sqrt{a+1/z^2})}{(1+(a-b)z^2)\sqrt{a+1/z^2}}dz\\
&=\phi_1(b)\phi_1(a-b)-\int_0^1 \frac{\arctan(1/\sqrt{t^2+a})}{(t^2+(a-b))\sqrt{t^2+a}}dt\qquad [z\mapsto 1/t]\\
&=\phi_1(b)\phi_1(a-b)-\int_0^1 \frac{\tfrac\pi2-\arctan\sqrt{t^2+a}}{(t^2+(a-b))\sqrt{t^2+a}}dt\\
&=\phi_1(b)\phi_1(a-b)-\frac\pi2\int_0^1 \frac{dt}{(t^2+(a-b))\sqrt{t^2+a}}+\int_0^1 \frac{\arctan\sqrt{t^2+a}}{(t^2+(a-b))\sqrt{t^2+a}}dt\\
&=\phi_1(b)\phi_1(a-b)-\phi_2(a,a-b)+J(a,a-b).\tag4
\end{align} (3) (4) J(a,b)=\phi_2(a,b)-\phi_1(b)\phi_1(a-b)+\phi_2(a,a-b)-J(a,a-b), J g(z)=\frac{\arctan\sqrt z}{\sqrt z}=\sum_{n\ge1}\frac{(-1)^n}{2n-1}\left(\frac{1}{z^n}-\frac{2}{\sqrt z}\right)\qquad z\ge1 J(a,b)=-\phi_2(a,b)+\sum_{n\ge1}\frac{(-1)^n}{2n-1}\int_0^1\frac{dt}{(t^2+b)(t^2+a)^n}, \phi_2 \int_0^1\frac{dt}{(t^2+b)(t^2+a)^n}","['real-analysis', 'calculus', 'integration', 'definite-integrals', 'closed-form']"
13,Using Wallis' formula to show the limit of $a_n:=n!\left(\frac{e}{n}\right)^n n^{-1/2}.$,Using Wallis' formula to show the limit of,a_n:=n!\left(\frac{e}{n}\right)^n n^{-1/2}.,Let $$a_n:=n!\bigg(\frac{e}{n}\bigg)^n n^{-1/2}.$$ With the help of Wallis' formula  $$\frac{\pi}{2} = \prod_{n=1}^\infty \frac{4n^2}{4n^2-1}=\lim_{m\to \infty}\frac{2^{4m}(m!)^4}{((2m)!)^2 (2m+1)}$$ show that the limit of $a_n$ is $\sqrt{2\pi}$. I don't have any idea on how to derive the limit from this formula. I would greatly appreciate any hints or solutions.,Let $$a_n:=n!\bigg(\frac{e}{n}\bigg)^n n^{-1/2}.$$ With the help of Wallis' formula  $$\frac{\pi}{2} = \prod_{n=1}^\infty \frac{4n^2}{4n^2-1}=\lim_{m\to \infty}\frac{2^{4m}(m!)^4}{((2m)!)^2 (2m+1)}$$ show that the limit of $a_n$ is $\sqrt{2\pi}$. I don't have any idea on how to derive the limit from this formula. I would greatly appreciate any hints or solutions.,,"['calculus', 'real-analysis', 'limits']"
14,The completion of the Borel $\sigma$-algebra the same as the completion of the Lebesgue outer measure?,The completion of the Borel -algebra the same as the completion of the Lebesgue outer measure?,\sigma,"My study group and I were discussing this question today. We can construct the Lebesgue measure using Caratheodory's extension theorem in the usual way: Given the function $F(x) = x$, we can construct a pre-measure $\mu_F$ associated with $F(x)$ defined on an algebra of ""intervals"" (Folland uses right-closed h-intervals); From this premeasure, we can induce an outer measure $\mu^*$ on the power-set of the reals; Using Caratheodory's extension theorem, the collection of $\mu^*$-measurable sets is a complete $\sigma$-algebra, and $\mu^*$ restricted to this $\sigma$-algebra is a complete measure. This complete $\sigma$-algebra is in some sense a ""big"" structure; it is certainly larger than the Borel sets, and it must contain all of the Lebesgue measurable sets. However, Folland provides a related Theorem: Theorem 1.14 Let $\mathcal{A} \subset \mathcal{P}(X)$ be an algebra, $\mu_0$ a premeasure on $\mathcal{A}$, and $\mathcal{M}$ the $\sigma$-algebra generated by $\mathcal{A}$. There exists a measure $\mu$ on $\mathcal{M}$ whose restriction to $\mathcal{A}$ is $\mu_0$ -- namely, $\mu = \mu^*|_\mathcal{M}$, where $\mu^*$ is given by $$\mu^*(E) = \inf \left\{\sum_1^\infty \mu_0(A_j) : A_j \in \mathcal{A}, E \subset \bigcup_1^\infty A_j\right\}.$$ He then goes on to claim uniqueness. The proof of the theorem invokes Caratheodory, but there is a question that remains. I will try to make my thoughts clear: Caratheodory gives us a complete measure space, this structure is large. Theorem 1.14 as written tells us that we can use a premeasure on an algebra $\mathcal{A}$ to generate a measure on the $\sigma$-algebra generated by $\mathcal{A}$ -- this $\sigma$-algebra is not necessarily complete. In fact, this $\sigma$-algebra is just the Borel $\sigma$-algebra. We can complete the Borel $\sigma$-algebra to obtain the Lebesgue measurable sets. However, is the completion of the Borel $\sigma$-algebra the same thing as we get by just applying Caratheodory's extension theorem to the Lebesgue outer measure to directly obtain a complete $\sigma$-algebra? Or is this a ""bigger"" structure than the completion of the Borel $\sigma$-algebra?","My study group and I were discussing this question today. We can construct the Lebesgue measure using Caratheodory's extension theorem in the usual way: Given the function $F(x) = x$, we can construct a pre-measure $\mu_F$ associated with $F(x)$ defined on an algebra of ""intervals"" (Folland uses right-closed h-intervals); From this premeasure, we can induce an outer measure $\mu^*$ on the power-set of the reals; Using Caratheodory's extension theorem, the collection of $\mu^*$-measurable sets is a complete $\sigma$-algebra, and $\mu^*$ restricted to this $\sigma$-algebra is a complete measure. This complete $\sigma$-algebra is in some sense a ""big"" structure; it is certainly larger than the Borel sets, and it must contain all of the Lebesgue measurable sets. However, Folland provides a related Theorem: Theorem 1.14 Let $\mathcal{A} \subset \mathcal{P}(X)$ be an algebra, $\mu_0$ a premeasure on $\mathcal{A}$, and $\mathcal{M}$ the $\sigma$-algebra generated by $\mathcal{A}$. There exists a measure $\mu$ on $\mathcal{M}$ whose restriction to $\mathcal{A}$ is $\mu_0$ -- namely, $\mu = \mu^*|_\mathcal{M}$, where $\mu^*$ is given by $$\mu^*(E) = \inf \left\{\sum_1^\infty \mu_0(A_j) : A_j \in \mathcal{A}, E \subset \bigcup_1^\infty A_j\right\}.$$ He then goes on to claim uniqueness. The proof of the theorem invokes Caratheodory, but there is a question that remains. I will try to make my thoughts clear: Caratheodory gives us a complete measure space, this structure is large. Theorem 1.14 as written tells us that we can use a premeasure on an algebra $\mathcal{A}$ to generate a measure on the $\sigma$-algebra generated by $\mathcal{A}$ -- this $\sigma$-algebra is not necessarily complete. In fact, this $\sigma$-algebra is just the Borel $\sigma$-algebra. We can complete the Borel $\sigma$-algebra to obtain the Lebesgue measurable sets. However, is the completion of the Borel $\sigma$-algebra the same thing as we get by just applying Caratheodory's extension theorem to the Lebesgue outer measure to directly obtain a complete $\sigma$-algebra? Or is this a ""bigger"" structure than the completion of the Borel $\sigma$-algebra?",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
15,"If $x,y,z\in[-1,1]$ and $1+2xyz\geq x^2+y^2+z^2$, then can we infer $1+2(xyz)^n\geq x^{2n}+y^{2n}+z^{2n}$?","If  and , then can we infer ?","x,y,z\in[-1,1] 1+2xyz\geq x^2+y^2+z^2 1+2(xyz)^n\geq x^{2n}+y^{2n}+z^{2n}","This problem was in IMC 2010. Assuming $x,y,z\in [-1,1]$ , suppose that $$1+2xyz\geqslant x^2 + y^2 + z^2$$ Can we infer from this that $$1+2(xyz)^n\geqslant x^{2n} + y^{2n} + z^{2n}$$ for any positive integer $n$ ?","This problem was in IMC 2010. Assuming , suppose that Can we infer from this that for any positive integer ?","x,y,z\in [-1,1] 1+2xyz\geqslant x^2 + y^2 + z^2 1+2(xyz)^n\geqslant x^{2n} + y^{2n} + z^{2n} n","['real-analysis', 'inequality', 'contest-math', 'cauchy-schwarz-inequality']"
16,How does one prove that a multivariate function is univariate?,How does one prove that a multivariate function is univariate?,,"The question resembles How does one prove that a multivariate function is constant? but appears to be more difficult. Suppose that $u\colon\mathbb R^2\to\mathbb R$ is a continuous function such that at every point $(x,y)\in\mathbb R^2$ at least one of the following is true: the partial derivative $\displaystyle \frac{\partial u}{\partial x}$ exists and is equal to $0$. the partial derivative $\displaystyle \frac{\partial u}{\partial y}$ exists and is equal to $0$. Does it follow that $u$ depends only on one of two variables $x,y$? In other words, can we prove that either 1 holds for all points or 2 holds for all points? So far I can prove this only under the additional assumption $u\in C^1$. Edit: Here is a proof of the $C^1$ case. Claim 1: If $u_x(a,b)\ne 0$ for some $(a,b)$, then $u_x(a,y)=u_x(a,b)$ for all $y\in\mathbb R$. Claim 2: If $u_y(c,d)\ne 0$ for some $(c,d)$, then $u_y(x,d)=u_y(c,d)$ for all $x\in\mathbb R$. Once Claims 1 and 2 are proved, it follows that the premise of one of them never holds, otherwise both derivatives would be nonzero at $(a,d)$. By symmetry it suffices to prove Claim 1. The set $E=\{y\in\mathbb R\colon u_x(a,y)=u_x(a,b)\}$ is closed by the continuity of $u_x$. Also, if $t\in E$ then $u_x\ne 0$ in some neighborhood of $(a,t)$ (again by continuity), which by assumption yields $u_y=0$ in this neighborhood, meaning $u$ does not depend on $y$ there. Thus, $E$ is also open. We conclude with $E=\mathbb R$, proving the claim.","The question resembles How does one prove that a multivariate function is constant? but appears to be more difficult. Suppose that $u\colon\mathbb R^2\to\mathbb R$ is a continuous function such that at every point $(x,y)\in\mathbb R^2$ at least one of the following is true: the partial derivative $\displaystyle \frac{\partial u}{\partial x}$ exists and is equal to $0$. the partial derivative $\displaystyle \frac{\partial u}{\partial y}$ exists and is equal to $0$. Does it follow that $u$ depends only on one of two variables $x,y$? In other words, can we prove that either 1 holds for all points or 2 holds for all points? So far I can prove this only under the additional assumption $u\in C^1$. Edit: Here is a proof of the $C^1$ case. Claim 1: If $u_x(a,b)\ne 0$ for some $(a,b)$, then $u_x(a,y)=u_x(a,b)$ for all $y\in\mathbb R$. Claim 2: If $u_y(c,d)\ne 0$ for some $(c,d)$, then $u_y(x,d)=u_y(c,d)$ for all $x\in\mathbb R$. Once Claims 1 and 2 are proved, it follows that the premise of one of them never holds, otherwise both derivatives would be nonzero at $(a,d)$. By symmetry it suffices to prove Claim 1. The set $E=\{y\in\mathbb R\colon u_x(a,y)=u_x(a,b)\}$ is closed by the continuity of $u_x$. Also, if $t\in E$ then $u_x\ne 0$ in some neighborhood of $(a,t)$ (again by continuity), which by assumption yields $u_y=0$ in this neighborhood, meaning $u$ does not depend on $y$ there. Thus, $E$ is also open. We conclude with $E=\mathbb R$, proving the claim.",,[]
17,Must there be a sequence $(\epsilon_n)$ of signs such that $\sum\epsilon_nx_n$ and $\sum\epsilon_ny_n$ are both convergent?,Must there be a sequence  of signs such that  and  are both convergent?,(\epsilon_n) \sum\epsilon_nx_n \sum\epsilon_ny_n,"Let $(x_n)$ and $(y_n)$ be real sequences. (i) Suppose $x_n \rightarrow 0$ as $n \rightarrow \infty.$ Show that   there is a sequence $(\epsilon_n)$ of signs (i.e., $\epsilon_n \in  \{−1, +1\}$ for all $n$) such that $\sum \epsilon_nx_n$ is convergent. (ii) Suppose $x_n \rightarrow 0$ and $y_n \rightarrow 0.$ Must there   be a sequence $(\epsilon_n)$ of signs such that $\sum\epsilon_nx_n$   and $\sum\epsilon_ny_n$ are both convergent? I'm struggling to come up with formal proofs, for (i) I've seen that we simply pick a limit and and then as soon as our sum passes the limit we set $\epsilon_n=-1$ until we pass it again and so on, oscillating about the limit but as $x_n \rightarrow 0$ we converge to it. for (ii) I don't think there must be such a sequence of $\epsilon_n$ but I can't construct a proof or counter example. So I would ask for a solution to (ii) and possibly a better way of constructing answers/tackling these problems in general. Thank you","Let $(x_n)$ and $(y_n)$ be real sequences. (i) Suppose $x_n \rightarrow 0$ as $n \rightarrow \infty.$ Show that   there is a sequence $(\epsilon_n)$ of signs (i.e., $\epsilon_n \in  \{−1, +1\}$ for all $n$) such that $\sum \epsilon_nx_n$ is convergent. (ii) Suppose $x_n \rightarrow 0$ and $y_n \rightarrow 0.$ Must there   be a sequence $(\epsilon_n)$ of signs such that $\sum\epsilon_nx_n$   and $\sum\epsilon_ny_n$ are both convergent? I'm struggling to come up with formal proofs, for (i) I've seen that we simply pick a limit and and then as soon as our sum passes the limit we set $\epsilon_n=-1$ until we pass it again and so on, oscillating about the limit but as $x_n \rightarrow 0$ we converge to it. for (ii) I don't think there must be such a sequence of $\epsilon_n$ but I can't construct a proof or counter example. So I would ask for a solution to (ii) and possibly a better way of constructing answers/tackling these problems in general. Thank you",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
18,How is Riemann–Stieltjes integral a special case of  Lebesgue–Stieltjes integral?,How is Riemann–Stieltjes integral a special case of  Lebesgue–Stieltjes integral?,,"Thanks for reading! My questions are based on the following quotes from Wikipedia: About the existence of  Lebesgue–Stieltjes integral : The Lebesgue–Stieltjes integral $  \int_a^b f(x)\,dg(x)$ is defined   when   ƒ : [a,b] → R is Borel-measurable and   bounded and g : [a,b] → R is of   bounded variation in [a,b] and   right-continuous, or when ƒ is   non-negative and g is monotone and   right-continuous. I was wondering if this is the right condition for its existence? About the existence of Riemann–Stieltjes integral : The best simple existence theorem   states that if f is continuous and g   is of bounded variation on [a, b],   then the integral exists. A function g   is of bounded variation if and only if   it is the difference between two   monotone functions. If g is not of   bounded variation, then there will be   continuous functions which cannot be   integrated with respect to g. In   general, the integral is not   well-defined if f and g share any   points of discontinuity, but this   sufficient condition is not necessary. On the other hand, a classical result   of Young (1936) states that the   integral is well-defined if f is   α-Hölder continuous and g is β-Hölder   continuous with α + β > 1. For the question in the part 3, I was wondering for Riemann–Stieltjes integral $\int_a^b f(x) \, dg(x) $ to exist, must g be nondecreasing? It looks like not the case quoted above. Specialization from Lebesgue–Stieltjes integral to Riemann–Stieltjes integral : Where f is a continuous real-valued   function of a real variable and g is a   non-decreasing real function, the   Lebesgue–Stieltjes integral is   equivalent to the Riemann–Stieltjes   integral, I was wondering  why it only mentions the case when g is nondecreasing? Is this the necessary condition for existence of Riemann-Stieltjes integral? Do Lebesgue–Stieltjes integral and Riemann–Stieltjes integral generally use the same notation $  \int_a^b f(x)\,dg(x)$? How does one know which one the notation refers to? Thanks for helping!","Thanks for reading! My questions are based on the following quotes from Wikipedia: About the existence of  Lebesgue–Stieltjes integral : The Lebesgue–Stieltjes integral $  \int_a^b f(x)\,dg(x)$ is defined   when   ƒ : [a,b] → R is Borel-measurable and   bounded and g : [a,b] → R is of   bounded variation in [a,b] and   right-continuous, or when ƒ is   non-negative and g is monotone and   right-continuous. I was wondering if this is the right condition for its existence? About the existence of Riemann–Stieltjes integral : The best simple existence theorem   states that if f is continuous and g   is of bounded variation on [a, b],   then the integral exists. A function g   is of bounded variation if and only if   it is the difference between two   monotone functions. If g is not of   bounded variation, then there will be   continuous functions which cannot be   integrated with respect to g. In   general, the integral is not   well-defined if f and g share any   points of discontinuity, but this   sufficient condition is not necessary. On the other hand, a classical result   of Young (1936) states that the   integral is well-defined if f is   α-Hölder continuous and g is β-Hölder   continuous with α + β > 1. For the question in the part 3, I was wondering for Riemann–Stieltjes integral $\int_a^b f(x) \, dg(x) $ to exist, must g be nondecreasing? It looks like not the case quoted above. Specialization from Lebesgue–Stieltjes integral to Riemann–Stieltjes integral : Where f is a continuous real-valued   function of a real variable and g is a   non-decreasing real function, the   Lebesgue–Stieltjes integral is   equivalent to the Riemann–Stieltjes   integral, I was wondering  why it only mentions the case when g is nondecreasing? Is this the necessary condition for existence of Riemann-Stieltjes integral? Do Lebesgue–Stieltjes integral and Riemann–Stieltjes integral generally use the same notation $  \int_a^b f(x)\,dg(x)$? How does one know which one the notation refers to? Thanks for helping!",,"['real-analysis', 'integration', 'measure-theory', 'functional-analysis']"
19,Finding $f$ such that $ \int f = \sum f$,Finding  such that,f  \int f = \sum f,"Please see the problem 5 of the given link: http://www.artofproblemsolving.com/Forum/resources.php?c=2&cid=59&year=2005&sid=722231ab4ec5ce280584eb8f24f07656 It asks us to prove that $$\sum\limits_{n=1}^{\infty} \frac{1}{n^n} = \int\limits_{0}^{1} x^{x} \ dx $$ Natural question which one gets by seeing this type of question is, find all continuous functions which satisfy $$\int\limits_{-\infty}^{\infty} f(x) \ dx = \sum\limits_{n= -\infty}^{\infty} f(n)$$","Please see the problem 5 of the given link: http://www.artofproblemsolving.com/Forum/resources.php?c=2&cid=59&year=2005&sid=722231ab4ec5ce280584eb8f24f07656 It asks us to prove that $$\sum\limits_{n=1}^{\infty} \frac{1}{n^n} = \int\limits_{0}^{1} x^{x} \ dx $$ Natural question which one gets by seeing this type of question is, find all continuous functions which satisfy $$\int\limits_{-\infty}^{\infty} f(x) \ dx = \sum\limits_{n= -\infty}^{\infty} f(n)$$",,['real-analysis']
20,Complement of a totally disconnected compact subset of the plane,Complement of a totally disconnected compact subset of the plane,,Let $E \subset \mathbb{C}$ be compact and totally disconnected. Is there an elementary way to prove that $\mathbb{C} \setminus E$ is connected?,Let $E \subset \mathbb{C}$ be compact and totally disconnected. Is there an elementary way to prove that $\mathbb{C} \setminus E$ is connected?,,"['real-analysis', 'general-topology', 'connectedness']"
21,Zeros of two equations,Zeros of two equations,,"Consider the equations $$  1+\frac{1}{z^k}=0 \quad\mbox{and}\quad 1+\frac{1}{z^k}+\frac{1}{(z+1)^k}=0, $$ where $k$ is a positive integer $\ge 4$ . I would like to show for instance that the number of zeros in $\frac{\pi}{2}\lt \mbox{Arg}(z) \lt \frac{2\pi}{3}$ is the same for both equations. I'm not sure if this is true, I'm basing this on numerical evidence. Of course the first equation is equivalent to $z^k=-1$ , so all the zeros lie on the unit circle and it is easy to explicitely compute the zeros. The problem is with the second equation, there can be some zeros outside the unit circle, so the condition $\frac{\pi}{2}\lt \mbox{Arg}(z) \lt \frac{2\pi}{3}$ is really important here. Intuitively what I think happens is that in this region the term $1/(z+1)^k$ is small for big $k$ , but when $\mbox{Arg}(z)$ is close to $2\pi/3$ this is not always the case (in that case $|1/(z+1)^k|\approx 1$ , so it does contribute to the second equation). Is there a way to get past the issue near $2\pi/3$ and show that the two equations have the same number of zeros and moreover the solutions are close to each other? (Again this is from numerical evidence)","Consider the equations where is a positive integer . I would like to show for instance that the number of zeros in is the same for both equations. I'm not sure if this is true, I'm basing this on numerical evidence. Of course the first equation is equivalent to , so all the zeros lie on the unit circle and it is easy to explicitely compute the zeros. The problem is with the second equation, there can be some zeros outside the unit circle, so the condition is really important here. Intuitively what I think happens is that in this region the term is small for big , but when is close to this is not always the case (in that case , so it does contribute to the second equation). Is there a way to get past the issue near and show that the two equations have the same number of zeros and moreover the solutions are close to each other? (Again this is from numerical evidence)"," 
1+\frac{1}{z^k}=0 \quad\mbox{and}\quad 1+\frac{1}{z^k}+\frac{1}{(z+1)^k}=0,
 k \ge 4 \frac{\pi}{2}\lt \mbox{Arg}(z) \lt \frac{2\pi}{3} z^k=-1 \frac{\pi}{2}\lt \mbox{Arg}(z) \lt \frac{2\pi}{3} 1/(z+1)^k k \mbox{Arg}(z) 2\pi/3 |1/(z+1)^k|\approx 1 2\pi/3","['real-analysis', 'calculus', 'complex-analysis', 'complex-numbers']"
22,Proving $ \lim\limits_{n\to\infty} \frac{8n^2-5}{4n^2+7} = 2$ using $\epsilon-\delta-$ definition.,Proving  using  definition., \lim\limits_{n\to\infty} \frac{8n^2-5}{4n^2+7} = 2 \epsilon-\delta-,"I'm trying to prove the limit of this sequence using the formal definition. I've looked at other questions on the site but from the ones I've seen, the $n^2$ term always seems to cancel out, making it simpler. Show that $$ \lim_{n\to\infty} \frac{8n^2-5}{4n^2+7} = 2$$ So this is how I started: $$ \left|\frac{8n^2-5}{4n^2+7} -2 \right| = \left|\frac{-19}{4n^2+7}\right| = \frac{19}{4n^2+7} \leq \frac{19}{4n^2} = \epsilon$$ Let $\epsilon > 0 \implies n = \sqrt{\frac{19}{4\epsilon}}$ By Archimedian Property: $ N > \sqrt{\frac{19}{4\epsilon}}  $ If $ n \geq N \geq \sqrt{\frac{19}{4\epsilon}} $ $$ \left|\frac{8n^2-5}{4n^2+7} -2 \right| \leq \frac{19}{4n^2} < \frac{19}{4(\frac{19}{4\epsilon})} = \epsilon $$","I'm trying to prove the limit of this sequence using the formal definition. I've looked at other questions on the site but from the ones I've seen, the $n^2$ term always seems to cancel out, making it simpler. Show that $$ \lim_{n\to\infty} \frac{8n^2-5}{4n^2+7} = 2$$ So this is how I started: $$ \left|\frac{8n^2-5}{4n^2+7} -2 \right| = \left|\frac{-19}{4n^2+7}\right| = \frac{19}{4n^2+7} \leq \frac{19}{4n^2} = \epsilon$$ Let $\epsilon > 0 \implies n = \sqrt{\frac{19}{4\epsilon}}$ By Archimedian Property: $ N > \sqrt{\frac{19}{4\epsilon}}  $ If $ n \geq N \geq \sqrt{\frac{19}{4\epsilon}} $ $$ \left|\frac{8n^2-5}{4n^2+7} -2 \right| \leq \frac{19}{4n^2} < \frac{19}{4(\frac{19}{4\epsilon})} = \epsilon $$",,"['real-analysis', 'sequences-and-series', 'limits', 'proof-verification', 'epsilon-delta']"
23,"Two sets $X,Y \subset [0,1]$ such that $X+Y=[0,2]$",Two sets  such that,"X,Y \subset [0,1] X+Y=[0,2]","A set $X\subset \mathbb{R}$ is called nice if, for every $\epsilon > 0$, there are a    positive integer $k$ and some bounded intervals $I_1,I_2,...,I_k$ such that    $X \subset I_1 \cup I_2 \cup \cdots \cup I_k$ and $\sum\limits_{j=1}^k |I_j|^{\epsilon} < \epsilon$. Prove that there exist sets $X,Y \subset [0,1]$, both of them nice , such that $X+Y = [0,2]$, where $X+Y:=\{x+y\mid x\in X,y\in Y\}.$ This question is from an iberoamerican exam for undergraduate students (ciim 2010). An attempt to solve this problem can be found at aops but it doesn't seem to be complete or correct. Any help is welcome.","A set $X\subset \mathbb{R}$ is called nice if, for every $\epsilon > 0$, there are a    positive integer $k$ and some bounded intervals $I_1,I_2,...,I_k$ such that    $X \subset I_1 \cup I_2 \cup \cdots \cup I_k$ and $\sum\limits_{j=1}^k |I_j|^{\epsilon} < \epsilon$. Prove that there exist sets $X,Y \subset [0,1]$, both of them nice , such that $X+Y = [0,2]$, where $X+Y:=\{x+y\mid x\in X,y\in Y\}.$ This question is from an iberoamerican exam for undergraduate students (ciim 2010). An attempt to solve this problem can be found at aops but it doesn't seem to be complete or correct. Any help is welcome.",,"['real-analysis', 'measure-theory', 'sumset']"
24,Heat Kernel Property,Heat Kernel Property,,"Let $\phi$ be the Heat Kernel in $\mathbb{R}^n$ , i. e. $$\phi (x,t)={(4\pi t)}^{-n/2}\exp\left( - \frac{\mid x \mid ^2}{4t}\right)$$ and let $u$ satisfy Heat equation. Show that: $$\frac{d}{dt}\int\limits_{\mathbb{R}^n}\phi |Du|^2 dx=-2\int\limits_{\mathbb{R}^n} \phi (\Delta u)^2 dx$$ What I have tried: We know $\phi_{t}=\Delta\phi$ . We get: $$\frac{d}{dt}\int\limits_{\mathbb{R}^n}\phi |Du|^2dx=\int\limits_{\mathbb{R}^n}\phi_{t}\langle Du,Du \rangle dx+\int\limits_{\mathbb{R}^n}2\phi\langle Du_{t},Du \rangle dx$$ Then you can put $u_{t}=\Delta u$ and apply integration by parts for the second term. But I do not know how to get rid of $\phi$ .","Let be the Heat Kernel in , i. e. and let satisfy Heat equation. Show that: What I have tried: We know . We get: Then you can put and apply integration by parts for the second term. But I do not know how to get rid of .","\phi \mathbb{R}^n \phi (x,t)={(4\pi t)}^{-n/2}\exp\left( - \frac{\mid x \mid ^2}{4t}\right) u \frac{d}{dt}\int\limits_{\mathbb{R}^n}\phi |Du|^2 dx=-2\int\limits_{\mathbb{R}^n} \phi (\Delta u)^2 dx \phi_{t}=\Delta\phi \frac{d}{dt}\int\limits_{\mathbb{R}^n}\phi |Du|^2dx=\int\limits_{\mathbb{R}^n}\phi_{t}\langle Du,Du \rangle dx+\int\limits_{\mathbb{R}^n}2\phi\langle Du_{t},Du \rangle dx u_{t}=\Delta u \phi","['real-analysis', 'functional-analysis', 'partial-differential-equations']"
25,Evaluate $\int_0^{\infty} \frac{1-e^{-ax}}{x e^x} dx$,Evaluate,\int_0^{\infty} \frac{1-e^{-ax}}{x e^x} dx,"I found two different approaches, both is giving the same answer. Fubini:  $$ \begin{align} \int_0^{\infty} \frac{1-e^{-ax}}{x e^x} \,dx &= \int_0^{\infty} e^{-x} \int_0^a e^{-xy} \,dy\, dx \\ &=  \int_0^a \int_0^{\infty} e^{-x(1+y)}\, dx \,dy \\ &= \int_0^{a} \frac{1}{1+y}\, dy\\ &=\log (a+1) , a>-1 \end{align} $$ Differentiation of the parameter: Denote $\displaystyle K(a) = \int_0^{\infty} \frac{1-e^{-ax}}{x e^x}\, dx$, differentiate w.r.t. $a$. Also, note that $K(0)=0$. $$ \begin{align} K'(a) &= \int_0^{\infty} \frac{e^{-ax}}{e^x} \,dx\\ &=\int_0^{\infty} e^{-x(a+1)} \,dx\\ &=\frac{1}{a+1}\\ \end{align} $$ Now we integrate back to get $\displaystyle K(a) = \int K'(a) da = \log(a+1), a>-1$ The requirements of the Fubini theorem are that $f(a,x)$ is a measurable function and $(0,a) \times (0,\infty)$ is a measurable set, right? To differentiate w.r.t. a parameter, we need that $\displaystyle | e^{-x(a+1)}| \le g(x)$ which has to be an integrable function. Here we could have $g(x)=e^{-x}$ for instance. So my question now is, whether one of the approaches is more correct than the other. I used 1. in an exam, and got a really low score (so I'm surprised).","I found two different approaches, both is giving the same answer. Fubini:  $$ \begin{align} \int_0^{\infty} \frac{1-e^{-ax}}{x e^x} \,dx &= \int_0^{\infty} e^{-x} \int_0^a e^{-xy} \,dy\, dx \\ &=  \int_0^a \int_0^{\infty} e^{-x(1+y)}\, dx \,dy \\ &= \int_0^{a} \frac{1}{1+y}\, dy\\ &=\log (a+1) , a>-1 \end{align} $$ Differentiation of the parameter: Denote $\displaystyle K(a) = \int_0^{\infty} \frac{1-e^{-ax}}{x e^x}\, dx$, differentiate w.r.t. $a$. Also, note that $K(0)=0$. $$ \begin{align} K'(a) &= \int_0^{\infty} \frac{e^{-ax}}{e^x} \,dx\\ &=\int_0^{\infty} e^{-x(a+1)} \,dx\\ &=\frac{1}{a+1}\\ \end{align} $$ Now we integrate back to get $\displaystyle K(a) = \int K'(a) da = \log(a+1), a>-1$ The requirements of the Fubini theorem are that $f(a,x)$ is a measurable function and $(0,a) \times (0,\infty)$ is a measurable set, right? To differentiate w.r.t. a parameter, we need that $\displaystyle | e^{-x(a+1)}| \le g(x)$ which has to be an integrable function. Here we could have $g(x)=e^{-x}$ for instance. So my question now is, whether one of the approaches is more correct than the other. I used 1. in an exam, and got a really low score (so I'm surprised).",,"['real-analysis', 'multivariable-calculus']"
26,Properties of the projection onto a nonconvex set,Properties of the projection onto a nonconvex set,,"Consider a set $\Omega\subseteq\mathbb{R}^n$ being ""sufficiently regular"", for example being the image of a $C^1$ mapping from $\mathbb{R}^p$ for some $p\ge1$. We may then consider the mapping $$   g:\mathbb{R}^n \to \mathbb{R}^n \\   g(y) = \mathrm{argmin}_{\theta\in\Omega}\|y - \theta\|_2, $$ where $\mathrm{argmin}$ denotes the argument minimum. It may be that the argument minimum is not unique, hopefully, however, it will be in most cases. I am interested in criteria on $\Omega$ to ensure that: The argument minimum defining $g$ is Lebesgue almost everwhere unique, such that $g$ is Lebesgue almost surely well-defined. $g$ is Lebesgue almost everywhere differentiable. This problem is relevant in the context of for example shape restricted regression, see the article Meyer & Woodroofe (2000), where $\Omega$ is taken to be convex. In the case where $\Omega$ is convex, it is known that $g$ is well-defined, it is then ordinarily known as the ""projection onto a convex set"", also, $g$ is in this case a contraction and is in particular Lebesgue almost everywhere differentiable by Rademacher's theorem. As regards the nonconvex case, it is easy to see that $g$ in general will not be everywhere well-defined, nor even everywhere continuous. If, for example, $\Omega$ is $\mathbb{R}^n$ minus a cone, moving $y$ along the axis of a cone, small perturbations of $y$ will make the projection onto $\Omega$ move wildly, and so $g$ will not have any chance of even being continuous on this axis.","Consider a set $\Omega\subseteq\mathbb{R}^n$ being ""sufficiently regular"", for example being the image of a $C^1$ mapping from $\mathbb{R}^p$ for some $p\ge1$. We may then consider the mapping $$   g:\mathbb{R}^n \to \mathbb{R}^n \\   g(y) = \mathrm{argmin}_{\theta\in\Omega}\|y - \theta\|_2, $$ where $\mathrm{argmin}$ denotes the argument minimum. It may be that the argument minimum is not unique, hopefully, however, it will be in most cases. I am interested in criteria on $\Omega$ to ensure that: The argument minimum defining $g$ is Lebesgue almost everwhere unique, such that $g$ is Lebesgue almost surely well-defined. $g$ is Lebesgue almost everywhere differentiable. This problem is relevant in the context of for example shape restricted regression, see the article Meyer & Woodroofe (2000), where $\Omega$ is taken to be convex. In the case where $\Omega$ is convex, it is known that $g$ is well-defined, it is then ordinarily known as the ""projection onto a convex set"", also, $g$ is in this case a contraction and is in particular Lebesgue almost everywhere differentiable by Rademacher's theorem. As regards the nonconvex case, it is easy to see that $g$ in general will not be everywhere well-defined, nor even everywhere continuous. If, for example, $\Omega$ is $\mathbb{R}^n$ minus a cone, moving $y$ along the axis of a cone, small perturbations of $y$ will make the projection onto $\Omega$ move wildly, and so $g$ will not have any chance of even being continuous on this axis.",,"['real-analysis', 'geometry', 'differential-geometry', 'optimization', 'manifolds']"
27,Why is the supremum of the empty set $-\infty$ and the infimum $\infty$? [duplicate],Why is the supremum of the empty set  and the infimum ? [duplicate],-\infty \infty,This question already has answers here : Infimum and supremum of the empty set (6 answers) Closed 10 years ago . I read in a paper on set theory that the supremum and the infimum of the empty set are defined as $\sup(\{\})=-\infty$ and $\inf(\{\})=\infty$. But intuitively I can't figure out why that is the case. Is there a reason why the supremum and infimum of the empty set are defined this way?,This question already has answers here : Infimum and supremum of the empty set (6 answers) Closed 10 years ago . I read in a paper on set theory that the supremum and the infimum of the empty set are defined as $\sup(\{\})=-\infty$ and $\inf(\{\})=\infty$. But intuitively I can't figure out why that is the case. Is there a reason why the supremum and infimum of the empty set are defined this way?,,"['real-analysis', 'elementary-set-theory']"
28,What is a subsequence in calculus?,What is a subsequence in calculus?,,"For example, if I have the sequence $(1,2,3,4,5,6,7,8,\ldots)$ i.e $x(n) = n$ for all natural numbers, then is the subsequence $(1,1,1,1,1...)$ valid? Or can I only take one element from the sequence once? Would the subsequence $(1,2,1,2,1,2...)$ be a valid subsequence? Thanks.","For example, if I have the sequence $(1,2,3,4,5,6,7,8,\ldots)$ i.e $x(n) = n$ for all natural numbers, then is the subsequence $(1,1,1,1,1...)$ valid? Or can I only take one element from the sequence once? Would the subsequence $(1,2,1,2,1,2...)$ be a valid subsequence? Thanks.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
29,Functions that are continuous only at two points?,Functions that are continuous only at two points?,,"I need to find a function $f:\mathbb{R}\to\mathbb{R}$ which is continuous only at two points, but discontinuous everywhere else. How on earth would I go about doing this? I can't think of any function like this. Thanks in advance. Edit: I've seen examples including the indicator function for rationals. Is this the only method of finding such functions?","I need to find a function $f:\mathbb{R}\to\mathbb{R}$ which is continuous only at two points, but discontinuous everywhere else. How on earth would I go about doing this? I can't think of any function like this. Thanks in advance. Edit: I've seen examples including the indicator function for rationals. Is this the only method of finding such functions?",,"['real-analysis', 'functions', 'continuity']"
30,Sum of two velocities is smaller than the speed of light,Sum of two velocities is smaller than the speed of light,,"Using the Lorentz transformation from special relativity, we get that the sum of two velocities can be expressed as $$u=\frac{u'+v}{1+\frac{u'v}{c^2}}.$$ Given that $|u'|,|v| \le c$ , I want to prove that $|u| \le c$ , ie. that the velocity never exceeds $c$ . However I am struggling to produce this bound. I have tried to bound the denominator from above but this produces zero and have tried a case wise approach but this has got me no where either.","Using the Lorentz transformation from special relativity, we get that the sum of two velocities can be expressed as Given that , I want to prove that , ie. that the velocity never exceeds . However I am struggling to produce this bound. I have tried to bound the denominator from above but this produces zero and have tried a case wise approach but this has got me no where either.","u=\frac{u'+v}{1+\frac{u'v}{c^2}}. |u'|,|v| \le c |u| \le c c","['real-analysis', 'special-relativity']"
31,"$X$ compact metric space, $f:X\rightarrow\mathbb{R}$ continuous attains max/min","compact metric space,  continuous attains max/min",X f:X\rightarrow\mathbb{R},"Let $X$ be a compact metric space, show that a continuous function $f:X\rightarrow\mathbb{R}$ attains a maximum and a minimum value on $X$. Attempt: So the important thing is that I have previously shown that such a function is bounded and that for compact $X$, $f(X)$ is compact given $f$ continuous. In $\mathbb{R}$, compact $\implies$ closed and bounded. So $f(X)$ is closed and contains its accumulation points, and it is bounded so $\exists \sup(A),\inf(A)$ and since closed $\implies \sup(A)\in A, \inf(A)\in A$. Did I miss anything/make an unwarranted leap of logic?","Let $X$ be a compact metric space, show that a continuous function $f:X\rightarrow\mathbb{R}$ attains a maximum and a minimum value on $X$. Attempt: So the important thing is that I have previously shown that such a function is bounded and that for compact $X$, $f(X)$ is compact given $f$ continuous. In $\mathbb{R}$, compact $\implies$ closed and bounded. So $f(X)$ is closed and contains its accumulation points, and it is bounded so $\exists \sup(A),\inf(A)$ and since closed $\implies \sup(A)\in A, \inf(A)\in A$. Did I miss anything/make an unwarranted leap of logic?",,"['real-analysis', 'metric-spaces', 'compactness']"
32,Measure of the Cantor set plus the Cantor set,Measure of the Cantor set plus the Cantor set,,"The Sum of 2 sets of Measure zero might well be very large for example, the sum of $x$-axis and $y$-axis, is nothing but the whole plane. Similarly one can ask this question about Cantor sets: If $C$ is the cantor set, then what is the measure of $C+C$?","The Sum of 2 sets of Measure zero might well be very large for example, the sum of $x$-axis and $y$-axis, is nothing but the whole plane. Similarly one can ask this question about Cantor sets: If $C$ is the cantor set, then what is the measure of $C+C$?",,['real-analysis']
33,Can we have an uncountable number of isolated points?,Can we have an uncountable number of isolated points?,,"Is this possible? I've been trying to think of an example or defend why not, and I'm struggling in both directions.","Is this possible? I've been trying to think of an example or defend why not, and I'm struggling in both directions.",,['real-analysis']
34,Why can complex numbers be written in exponential form? $z=r(\cos \theta+i\sin \theta)$ is $z=re^{i\theta}$.,Why can complex numbers be written in exponential form?  is .,z=r(\cos \theta+i\sin \theta) z=re^{i\theta},Why can complex numbers be written in exponential form? $z=r(\cos \theta+i\sin \theta)$ is $z=re^{i\theta}$ . I have studied that the exponential form of a complex number $z=r(\cos \theta+i\sin \theta)$ is $z=re^{i\theta}$ . Can someone explain why?,Why can complex numbers be written in exponential form? is . I have studied that the exponential form of a complex number is . Can someone explain why?,z=r(\cos \theta+i\sin \theta) z=re^{i\theta} z=r(\cos \theta+i\sin \theta) z=re^{i\theta},"['real-analysis', 'algebra-precalculus', 'complex-numbers']"
35,Why weak convergence doesn't imply convergence?,Why weak convergence doesn't imply convergence?,,"Let $(H,\langle,\rangle)$ be a Hilbert space. We have that $u_n$ converges to $u$ weakly if $$\lim_{n\to \infty}\langle u_n,v\rangle=\langle u,v\rangle$$ for all $v\in H$. But why doesn't it converge strongly? Indeed, if $v=u_m$, then, $$\lim_{n\to \infty }\langle u_n,u_m\rangle=\langle u,u_m\rangle$$ for all $m$, and thus $$\lim_{m\to \infty }\lim_{n\to \infty }\langle u_n,u_m\rangle=\langle u,u_m\rangle\lim_{m\to \infty }\langle u,u_m\rangle=\langle u,u\rangle$$ therefore, $$\lim_{n\to \infty }\langle u_n,u_n\rangle=\langle u,u\rangle\implies \lim_{n\to \infty }\|u_n\|=\|u\|$$ and thus it converges weakly. What's wrong here?","Let $(H,\langle,\rangle)$ be a Hilbert space. We have that $u_n$ converges to $u$ weakly if $$\lim_{n\to \infty}\langle u_n,v\rangle=\langle u,v\rangle$$ for all $v\in H$. But why doesn't it converge strongly? Indeed, if $v=u_m$, then, $$\lim_{n\to \infty }\langle u_n,u_m\rangle=\langle u,u_m\rangle$$ for all $m$, and thus $$\lim_{m\to \infty }\lim_{n\to \infty }\langle u_n,u_m\rangle=\langle u,u_m\rangle\lim_{m\to \infty }\langle u,u_m\rangle=\langle u,u\rangle$$ therefore, $$\lim_{n\to \infty }\langle u_n,u_n\rangle=\langle u,u\rangle\implies \lim_{n\to \infty }\|u_n\|=\|u\|$$ and thus it converges weakly. What's wrong here?",,"['real-analysis', 'convergence-divergence', 'hilbert-spaces']"
36,Proof that $\sqrt x$ is absolutely continuous.,Proof that  is absolutely continuous.,\sqrt x,"I want to prove that $f(x)=\sqrt x$ is absolutely continuous. So I must show that for every $\epsilon>0$,there is a $\delta>0$ that if $\{[a_k,b_k]\}_1^n$ is a disjoint collection of intervals that $\sum_{k=1}^n (b_k-a_k)<\delta$, then $\sum_{k=1}^n \left(\sqrt{b_k}-\sqrt{a_k}\right)< \epsilon$. I tried but I can't obtain $\delta$ independent from $n$. What is my mistake? Is there any hint? Thank you.","I want to prove that $f(x)=\sqrt x$ is absolutely continuous. So I must show that for every $\epsilon>0$,there is a $\delta>0$ that if $\{[a_k,b_k]\}_1^n$ is a disjoint collection of intervals that $\sum_{k=1}^n (b_k-a_k)<\delta$, then $\sum_{k=1}^n \left(\sqrt{b_k}-\sqrt{a_k}\right)< \epsilon$. I tried but I can't obtain $\delta$ independent from $n$. What is my mistake? Is there any hint? Thank you.",,"['real-analysis', 'continuity']"
37,What is meant by limit of sets?,What is meant by limit of sets?,,"I am having a hard time grasping the limit of sets as used in $\sigma$ algebra and probability space. What does the notation $\lim_{n\rightarrow\infty}S_n=S$ mean when all these $S_n$ are sets. I have some intuitive idea. But I understand limit in real analysis with the $\delta$-$\epsilon$ concept. So what is the rigorous definition of limit applied to sets, a definition parallel to the $\delta$-$\epsilon$ notation as applied to real analysis? Citing one or two examples will be great.","I am having a hard time grasping the limit of sets as used in $\sigma$ algebra and probability space. What does the notation $\lim_{n\rightarrow\infty}S_n=S$ mean when all these $S_n$ are sets. I have some intuitive idea. But I understand limit in real analysis with the $\delta$-$\epsilon$ concept. So what is the rigorous definition of limit applied to sets, a definition parallel to the $\delta$-$\epsilon$ notation as applied to real analysis? Citing one or two examples will be great.",,"['real-analysis', 'probability-theory', 'measure-theory']"
38,Integral $I=\int_0^\infty \frac{\ln(1+x)\ln(1+x^{-2})}{x} dx$,Integral,I=\int_0^\infty \frac{\ln(1+x)\ln(1+x^{-2})}{x} dx,"Hi I am stuck on showing that $$ \int_0^\infty \frac{\ln(1+x)\ln(1+x^{-2})}{x} dx=\pi G-\frac{3\zeta(3)}{8} $$ where G is the Catalan constant and $\zeta(3)$ is the Riemann zeta function.  Explictly they are given by $$ G=\beta(2)=\sum_{n=0}^\infty \frac{(-1)^n}{(2n+1)^2}, \ \zeta(3)=\sum_{n=1}^\infty \frac{1}{n^3}. $$  I have tried using  $$ \ln(1+x)=\sum_{n=1}^\infty \frac{(-1)^{n+1} x^n}{n}, $$  but didn't get very far.","Hi I am stuck on showing that $$ \int_0^\infty \frac{\ln(1+x)\ln(1+x^{-2})}{x} dx=\pi G-\frac{3\zeta(3)}{8} $$ where G is the Catalan constant and $\zeta(3)$ is the Riemann zeta function.  Explictly they are given by $$ G=\beta(2)=\sum_{n=0}^\infty \frac{(-1)^n}{(2n+1)^2}, \ \zeta(3)=\sum_{n=1}^\infty \frac{1}{n^3}. $$  I have tried using  $$ \ln(1+x)=\sum_{n=1}^\infty \frac{(-1)^{n+1} x^n}{n}, $$  but didn't get very far.",,"['real-analysis', 'integration', 'definite-integrals', 'contour-integration', 'catalans-constant']"
39,Proving that $\int_a^b\frac{dx}{\sqrt{(x-a)(b-x)}}=\pi$,Proving that,\int_a^b\frac{dx}{\sqrt{(x-a)(b-x)}}=\pi,How do I prove that  $$\int_a^b\frac{dx}{\sqrt{(x-a)(b-x)}}=\pi?$$ I'm just wondering if LHS even equal to the RHS in the first place?  Thanks for the help!,How do I prove that  $$\int_a^b\frac{dx}{\sqrt{(x-a)(b-x)}}=\pi?$$ I'm just wondering if LHS even equal to the RHS in the first place?  Thanks for the help!,,['real-analysis']
40,Is $\ln(x)$ uniformly continuous?,Is  uniformly continuous?,\ln(x),"Let $x\in[1,\infty)$. Is $\ln x$ uniformly continuous? I took this function to be continuous and wrote the following proof which I'm not entirely sure of. Let $\varepsilon>0 $, $x,y\in[1, ∞)$ and $x>y$.  Then, $\ln x< x$ and $\ln y< y$ and this follows that $0<|\ln x-\ln y|<|x-y|$ since $x> y$. Choose $δ=ϵ$. Now suppose $|x-y|< δ$. Then, $|\ln x-\ln y|<|x-y|<\varepsilon$ It would be much appreciated if someone could validate my proof","Let $x\in[1,\infty)$. Is $\ln x$ uniformly continuous? I took this function to be continuous and wrote the following proof which I'm not entirely sure of. Let $\varepsilon>0 $, $x,y\in[1, ∞)$ and $x>y$.  Then, $\ln x< x$ and $\ln y< y$ and this follows that $0<|\ln x-\ln y|<|x-y|$ since $x> y$. Choose $δ=ϵ$. Now suppose $|x-y|< δ$. Then, $|\ln x-\ln y|<|x-y|<\varepsilon$ It would be much appreciated if someone could validate my proof",,"['real-analysis', 'analysis', 'continuity', 'uniform-continuity']"
41,Where's the error in this $2=1$ fake proof? [duplicate],Where's the error in this  fake proof? [duplicate],2=1,"This question already has answers here : Finding the error in this proof that 1=2 (4 answers) Closed 11 years ago . I'm reading Spivak's Calculus : 2 What's wrong with the following ""proof""? Let $x=y$ . Then $$x^2=xy\tag{1}$$ $$x^2-y^2=xy-y^2\tag{2}$$ $$(x+y)(x-y)=y(x-y)\tag{3}$$ $$x+y=y\tag{4}$$ $$2y=y\tag{5}$$ $$2=1\tag{6}$$ I guess the problem  is in $(3)$ , it seems he tried to divide both sides by $(x-y)$ . The operation would be acceptable in an example such as: $$12x=12\tag{1}$$ $$\frac{12x}{12}=\frac{12}{12}\tag{2}$$ $$x=1\tag{3}$$ I'm lost at what should be causing this, my naive exploration in the nature of both examples came to the following: In the case of $12x=12$ , we have an imbalance: We have $x$ in only one side then operations and dividing both sides by $12$ make sense. Also, In $\color{red}{12}\color{green}{x}=12$ we have a $\color{red}{coefficient}$ and a $\color{green}{variable}$ , the nature of those seems to differ from the nature of $$\color{green}{(x+y)}\color{red}{(x-y)}=y(x-y)$$ It's like: It's okay to do the thing in $12x=12$ , but for doing it on $(x+y)(x-y)=y(x-y)$ we need first to simplify $(x+y)(x-y)$ to $x^2-y^2$ .","This question already has answers here : Finding the error in this proof that 1=2 (4 answers) Closed 11 years ago . I'm reading Spivak's Calculus : 2 What's wrong with the following ""proof""? Let . Then I guess the problem  is in , it seems he tried to divide both sides by . The operation would be acceptable in an example such as: I'm lost at what should be causing this, my naive exploration in the nature of both examples came to the following: In the case of , we have an imbalance: We have in only one side then operations and dividing both sides by make sense. Also, In we have a and a , the nature of those seems to differ from the nature of It's like: It's okay to do the thing in , but for doing it on we need first to simplify to .",x=y x^2=xy\tag{1} x^2-y^2=xy-y^2\tag{2} (x+y)(x-y)=y(x-y)\tag{3} x+y=y\tag{4} 2y=y\tag{5} 2=1\tag{6} (3) (x-y) 12x=12\tag{1} \frac{12x}{12}=\frac{12}{12}\tag{2} x=1\tag{3} 12x=12 x 12 \color{red}{12}\color{green}{x}=12 \color{red}{coefficient} \color{green}{variable} \color{green}{(x+y)}\color{red}{(x-y)}=y(x-y) 12x=12 (x+y)(x-y)=y(x-y) (x+y)(x-y) x^2-y^2,"['real-analysis', 'analysis', 'fake-proofs']"
42,Irrational Cantor set?,Irrational Cantor set?,,Can someone help me? How can I prove that exists a number $k \in \mathbb R$ that $$A = \{x + k;\ x \in \text{Cantor set} \} \subset\text{ Irrationals}\;?$$,Can someone help me? How can I prove that exists a number $k \in \mathbb R$ that $$A = \{x + k;\ x \in \text{Cantor set} \} \subset\text{ Irrationals}\;?$$,,"['real-analysis', 'general-topology']"
43,How does one begin to even write a proof?,How does one begin to even write a proof?,,"I'm in my first proof based class and I'm just having a lot of trouble writing proofs. I mean I know it's not going to come natural and it will take time, but seroiusly, how does someone begin to write a proof and formulate a game plan? For example, I'm trying to write up a solution for this problem but I can't seem to figure out where to start. Given $y \in \mathbb R, n\in\mathbb N$, and $\epsilon \gt 0$, show that for some $\delta \gt 0$, if $u\in \mathbb R$, and $|u-y|\lt \delta$, then $|u^n-y^n|\lt \epsilon$. Any suggestions on how to start or even what some first things I should think about are when faced with a ""Show that..."" problem? I'm really looking for a combination of where to start for this and some proof writing in general.","I'm in my first proof based class and I'm just having a lot of trouble writing proofs. I mean I know it's not going to come natural and it will take time, but seroiusly, how does someone begin to write a proof and formulate a game plan? For example, I'm trying to write up a solution for this problem but I can't seem to figure out where to start. Given $y \in \mathbb R, n\in\mathbb N$, and $\epsilon \gt 0$, show that for some $\delta \gt 0$, if $u\in \mathbb R$, and $|u-y|\lt \delta$, then $|u^n-y^n|\lt \epsilon$. Any suggestions on how to start or even what some first things I should think about are when faced with a ""Show that..."" problem? I'm really looking for a combination of where to start for this and some proof writing in general.",,"['real-analysis', 'soft-question', 'proof-writing']"
44,"What's a ""deleted neighborhood""? (other than very very confusing)","What's a ""deleted neighborhood""? (other than very very confusing)",,"My text has the following definitions: 3.4.1 DEFINITION Let $x\in\mathbb{R}$ and let $\epsilon>0$.  A neighborhood of $x$ (or an $\epsilon$-neighborhood of $x$)† is a set of the form $$N(x; \epsilon) = \{y\in\mathbb{R} : |x-y|<\epsilon\}.$$ 3.4.2 DEFINITION Let $x\in\mathbb{R}$ and let $\epsilon>0$.  A deleted neighborhood of $x$ is a set of the form $$N^*(x; \epsilon) = \{y\in\mathbb{R} : 0 < |x-y| < \epsilon\}.^‡$$ Please explain to me how these are different.  The ONLY change I see is the second one has $0<$, which I don't see as necessary as the absolute value is ALWAYS positive.  It's part of its definition. I've tried getting clarification from my professor and the TA, and it's just SO confusing.  (Mostly this comes from trying to put accumulation points into context, as its definition comes from the deleted neighborhoods.) (Definitions from Analysis, With An Introduction to Proof , by Steven Lay.  Page 135.)","My text has the following definitions: 3.4.1 DEFINITION Let $x\in\mathbb{R}$ and let $\epsilon>0$.  A neighborhood of $x$ (or an $\epsilon$-neighborhood of $x$)† is a set of the form $$N(x; \epsilon) = \{y\in\mathbb{R} : |x-y|<\epsilon\}.$$ 3.4.2 DEFINITION Let $x\in\mathbb{R}$ and let $\epsilon>0$.  A deleted neighborhood of $x$ is a set of the form $$N^*(x; \epsilon) = \{y\in\mathbb{R} : 0 < |x-y| < \epsilon\}.^‡$$ Please explain to me how these are different.  The ONLY change I see is the second one has $0<$, which I don't see as necessary as the absolute value is ALWAYS positive.  It's part of its definition. I've tried getting clarification from my professor and the TA, and it's just SO confusing.  (Mostly this comes from trying to put accumulation points into context, as its definition comes from the deleted neighborhoods.) (Definitions from Analysis, With An Introduction to Proof , by Steven Lay.  Page 135.)",,"['real-analysis', 'general-topology', 'real-numbers']"
45,Is the indicator function of the rationals Riemann integrable?,Is the indicator function of the rationals Riemann integrable?,,"$f(x) = \begin{cases} 1  & x\in\Bbb Q \\[2ex] 0 & x\notin\Bbb Q \end{cases}$ Is this function Riemann integrable on $[0,1]$? Since rational and irrational numbers are dense on $[0,1]$, no matter what partition I choose, there will always be rational and irrational numbers in every small interval. So the upper sum and lower of will always differ by $1$. However, I know rational numbers in $[0,1]$ are countable, so I can index them from 1 to infinity. For each rational number $q$ in $[0,1]$, I can cover it by $[q-\frac\epsilon{2^i},q+\frac\epsilon{2^i}]$. So all rational numbers in $[0,1]$ can be covered by a set of measure $\epsilon$. On this set, the upper sum is $1\times\epsilon=\epsilon$. Out of this set, the upper sum is 0. So the upper sum and lower sum differ by any arbitrary $\epsilon$. Thus, the function is integrable. One of the above arguments must be wrong. Please let me know which one is wrong and why. Any help is appreciated.","$f(x) = \begin{cases} 1  & x\in\Bbb Q \\[2ex] 0 & x\notin\Bbb Q \end{cases}$ Is this function Riemann integrable on $[0,1]$? Since rational and irrational numbers are dense on $[0,1]$, no matter what partition I choose, there will always be rational and irrational numbers in every small interval. So the upper sum and lower of will always differ by $1$. However, I know rational numbers in $[0,1]$ are countable, so I can index them from 1 to infinity. For each rational number $q$ in $[0,1]$, I can cover it by $[q-\frac\epsilon{2^i},q+\frac\epsilon{2^i}]$. So all rational numbers in $[0,1]$ can be covered by a set of measure $\epsilon$. On this set, the upper sum is $1\times\epsilon=\epsilon$. Out of this set, the upper sum is 0. So the upper sum and lower sum differ by any arbitrary $\epsilon$. Thus, the function is integrable. One of the above arguments must be wrong. Please let me know which one is wrong and why. Any help is appreciated.",,"['real-analysis', 'integration', 'analysis', 'definite-integrals', 'riemann-sum']"
46,"$f \in L^1$, but $f \not\in L^p$ for all $p > 1$",", but  for all",f \in L^1 f \not\in L^p p > 1,"""Find an $f \in [0,1]$ such that $f \in L^1$ but $f \not\in L^p$ for any $p > 1$."" I've thought about doing something like $$f(x) = \frac{1}{x}$$ where $|f|^p = \frac{1}{x^p}$ doesn't converge when $p > 1$.  But this function isn't itself in $L^1$.  Could someone please give me a hint for how to solve this problem?  I wish there were a situation where you had convergence on the closed half disc $[0,1]$ and divergence on $(1, \infty)$, rather than my current predicament where I have convergence on the open half-disc $[0, 1)$ and divergence on $[1, \infty)$.","""Find an $f \in [0,1]$ such that $f \in L^1$ but $f \not\in L^p$ for any $p > 1$."" I've thought about doing something like $$f(x) = \frac{1}{x}$$ where $|f|^p = \frac{1}{x^p}$ doesn't converge when $p > 1$.  But this function isn't itself in $L^1$.  Could someone please give me a hint for how to solve this problem?  I wish there were a situation where you had convergence on the closed half disc $[0,1]$ and divergence on $(1, \infty)$, rather than my current predicament where I have convergence on the open half-disc $[0, 1)$ and divergence on $[1, \infty)$.",,"['real-analysis', 'lebesgue-integral', 'lp-spaces']"
47,No cont function $f\colon\mathbb{R}\to\mathbb{R}$ with $f(x)$ rational $\iff f(x+1)$ irrational.,No cont function  with  rational  irrational.,f\colon\mathbb{R}\to\mathbb{R} f(x) \iff f(x+1),"Prove that there are no continuous functions $f\colon \mathbb{R} \to \mathbb{R}$ with the property: For any $x \in \mathbb{R}$, $f(x)$ is a rational number if and only if $f(x+1)$ is an irrational number. Source: 6th University of Rochester Math Olympiad .","Prove that there are no continuous functions $f\colon \mathbb{R} \to \mathbb{R}$ with the property: For any $x \in \mathbb{R}$, $f(x)$ is a rational number if and only if $f(x+1)$ is an irrational number. Source: 6th University of Rochester Math Olympiad .",,"['real-analysis', 'contest-math']"
48,How do you prove the $p$-norm is not a norm in $\mathbb R^n$ when $0<p<1$?,How do you prove the -norm is not a norm in  when ?,p \mathbb R^n 0<p<1,I see that it fails to satisfy the triangle inequality by example but I don't see how to prove this is the case for all $0 < p < 1$. The definition I am using for $p$-norm is $$ \|A\|_p= \left(\sum_{k=1}^{n} |x_k|^p\right)^{1/p}.$$,I see that it fails to satisfy the triangle inequality by example but I don't see how to prove this is the case for all $0 < p < 1$. The definition I am using for $p$-norm is $$ \|A\|_p= \left(\sum_{k=1}^{n} |x_k|^p\right)^{1/p}.$$,,"['real-analysis', 'normed-spaces', 'examples-counterexamples', 'lp-spaces']"
49,Upper Bound vs. Least Upper Bound,Upper Bound vs. Least Upper Bound,,"I am reading Rudin's Principles of Mathematical Analysis in order to prepare for my first course in Real Analysis I intend to take this fall. The book just defined what an upper bound is and then defined supremum/ least upper bound as: Suppose $S$ is an ordered set, with $E$ as a subset of $S$ , where $E$ is bounded above. Suppose there exists an $\alpha \in S$ with the following properties: A) $\alpha$ is an upper bound of $E$ . B) If $\gamma < \alpha$ then $\gamma$ is not an upper bound of $E$ . I do not understand the difference between upper bound and least upper bound. If someone could explain the difference between the two and possibly provide an example, it would be much appreciated. Thanks.","I am reading Rudin's Principles of Mathematical Analysis in order to prepare for my first course in Real Analysis I intend to take this fall. The book just defined what an upper bound is and then defined supremum/ least upper bound as: Suppose is an ordered set, with as a subset of , where is bounded above. Suppose there exists an with the following properties: A) is an upper bound of . B) If then is not an upper bound of . I do not understand the difference between upper bound and least upper bound. If someone could explain the difference between the two and possibly provide an example, it would be much appreciated. Thanks.",S E S E \alpha \in S \alpha E \gamma < \alpha \gamma E,"['real-analysis', 'upper-lower-bounds']"
50,"For $x_{n+1}=x_n^2-2$, show $\lim_{n\to\infty}\frac{x_n}{x_0x_1\cdots x_{n-1}}=2$ [duplicate]","For , show  [duplicate]",x_{n+1}=x_n^2-2 \lim_{n\to\infty}\frac{x_n}{x_0x_1\cdots x_{n-1}}=2,"This question already has answers here : If $x_1=5$, $x_{n+1}=x_n^2-2$, find $\lim x_{n+1}/(x_1\cdots x_n)$ (2 answers) Closed 6 years ago . Suppose $x_0:=2\sqrt{2}$ and $x_{n+1}=x_n^2-2$ for $n\ge1$. We have to show $$\lim_{n\to\infty}\frac{x_n}{x_0x_1\cdots x_{n-1}}=2$$ Establishing convergence is pretty direct but I'm having trouble evaluating the limit. I have tried using the relation $\frac{x_{n}}{x_{0}x_{1}\cdots x_{n-1}}=\frac{x_{n-1}^{2}-2}{x_{0}x_{1}\cdots x_{n-1}}=\frac{x_{n-1}}{x_{0}x_{1}\cdots x_{n-2}}-\frac{2}{x_{0}x_{1}\cdots x_{n-1}}=\cdots=\frac{3}{\sqrt{2}}-2\sum_{k=1}^{n}\frac{1}{x_{0}x_{1}\cdots x_{k}}$ but that doesn't really shed light on the exact value of the limit. Any and all help appreciated. Thanks!","This question already has answers here : If $x_1=5$, $x_{n+1}=x_n^2-2$, find $\lim x_{n+1}/(x_1\cdots x_n)$ (2 answers) Closed 6 years ago . Suppose $x_0:=2\sqrt{2}$ and $x_{n+1}=x_n^2-2$ for $n\ge1$. We have to show $$\lim_{n\to\infty}\frac{x_n}{x_0x_1\cdots x_{n-1}}=2$$ Establishing convergence is pretty direct but I'm having trouble evaluating the limit. I have tried using the relation $\frac{x_{n}}{x_{0}x_{1}\cdots x_{n-1}}=\frac{x_{n-1}^{2}-2}{x_{0}x_{1}\cdots x_{n-1}}=\frac{x_{n-1}}{x_{0}x_{1}\cdots x_{n-2}}-\frac{2}{x_{0}x_{1}\cdots x_{n-1}}=\cdots=\frac{3}{\sqrt{2}}-2\sum_{k=1}^{n}\frac{1}{x_{0}x_{1}\cdots x_{k}}$ but that doesn't really shed light on the exact value of the limit. Any and all help appreciated. Thanks!",,"['real-analysis', 'recurrence-relations']"
51,Prove the time inversion formula is brownian motion [duplicate],Prove the time inversion formula is brownian motion [duplicate],,"This question already has answers here : Show that $X(t)=t W(1/t)$ is a Brownian motion if $W(t)$ is a Brownian motion. (3 answers) Closed 2 years ago . Let $B=(B_t)_{t\geq 0}$ be a brownian motion. Show the time inversion formula $\hat{B}=(\hat B_t)_t\geq0$ is a brownian motion, where for $t \geq 0$ we set $\hat{B}_0=0$ and $\hat{B}_t=tB_{1/t}$ for $t>0$ . I cant figure out how to do this question , can someone help me ? Thanks","This question already has answers here : Show that $X(t)=t W(1/t)$ is a Brownian motion if $W(t)$ is a Brownian motion. (3 answers) Closed 2 years ago . Let be a brownian motion. Show the time inversion formula is a brownian motion, where for we set and for . I cant figure out how to do this question , can someone help me ? Thanks",B=(B_t)_{t\geq 0} \hat{B}=(\hat B_t)_t\geq0 t \geq 0 \hat{B}_0=0 \hat{B}_t=tB_{1/t} t>0,"['real-analysis', 'probability', 'probability-theory', 'stochastic-processes', 'brownian-motion']"
52,"If $f'$ tends to a positive limit as $x$ approaches infinity, then $f$ approaches infinity","If  tends to a positive limit as  approaches infinity, then  approaches infinity",f' x f,"Some time ago, I asked this here. A restricted form of the second question could be this: If $f$ is a function with continuous first derivative in $\mathbb{R}$ and such that $$\lim_{x\to \infty} f'(x) =a,$$ with $a\gt 0$, then $$\lim_{x\to\infty}f(x)=\infty.$$ To prove it, I tried this: There exist $x_0\in \mathbb{R}$ such that for $x\geq x_0$, $$f'(x)\gt \frac{a}{2}.$$ There exist $\delta_0\gt 0$ such that for $x_0\lt x\leq x_0+ \delta_0$ $$\begin{align*}\frac{f(x)-f(x_0)}{x-x_0}-f'(x_0)&\gt -\frac{a}{4}\\ \frac{f(x)-f(x_0)}{x-x_0}&\gt f'(x_0)-\frac{a}{4}\\ &\gt \frac{a}{2}-\frac{a}{4}=\frac{a}{4}\\ f(x)-f(x_0)&\gt \frac{a}{4}(x-x_0)\end{align*}.$$ We can assume that $\delta_0\geq 1$. If $\delta_0 \lt 1$, then $x_0+2-\delta_0\gt x_0$ and then $$f'(x_0+2-\delta_0)\gt \frac{a}{2}.$$ Now, there exist $\delta\gt 0$ such that for $x_0+2-\delta_0\lt x\leq x_0+2-\delta_0+\delta$ $$f(x)-f(x_0+2-\delta_0)\gt \frac{a}{4}(x-(x_0+2-\delta_0))= \frac{a}{4}(x-x_0-(2-\delta_0))\gt \frac{a}{4}(x-x_0).$$ It is clear that $x\in (x_0,x_0+2-\delta_0+\delta]$ and $2-\delta_0+\delta\geq 1$. Therefore, we can take $x_1=x_0+1$. Then $f'(x_1)\gt a/2$ and then there exist $\delta_1\geq 1$ such that for $x_1\lt x\leq x_1+\delta_1$ $$f(x)-f(x_1)\gt \frac{a}{4}(x-x_1).$$ Take $x_2=x_1+1$ and so on. If $f$ is bounded, $(f(x_n))_{n\in \mathbb{N}}$ is a increasing bounded sequence and therefore it has a convergent subsequence. Thus, this implies that the sequence $(x_n)$: $$x_{n+1}=x_n+1,$$ have a Cauchy's subsequence and that is a contradiction. Therefore $\lim_{x\to \infty} f(x)=\infty$. I want to know if this is correct, and if there is a simpler way to prove this. Thanks.","Some time ago, I asked this here. A restricted form of the second question could be this: If $f$ is a function with continuous first derivative in $\mathbb{R}$ and such that $$\lim_{x\to \infty} f'(x) =a,$$ with $a\gt 0$, then $$\lim_{x\to\infty}f(x)=\infty.$$ To prove it, I tried this: There exist $x_0\in \mathbb{R}$ such that for $x\geq x_0$, $$f'(x)\gt \frac{a}{2}.$$ There exist $\delta_0\gt 0$ such that for $x_0\lt x\leq x_0+ \delta_0$ $$\begin{align*}\frac{f(x)-f(x_0)}{x-x_0}-f'(x_0)&\gt -\frac{a}{4}\\ \frac{f(x)-f(x_0)}{x-x_0}&\gt f'(x_0)-\frac{a}{4}\\ &\gt \frac{a}{2}-\frac{a}{4}=\frac{a}{4}\\ f(x)-f(x_0)&\gt \frac{a}{4}(x-x_0)\end{align*}.$$ We can assume that $\delta_0\geq 1$. If $\delta_0 \lt 1$, then $x_0+2-\delta_0\gt x_0$ and then $$f'(x_0+2-\delta_0)\gt \frac{a}{2}.$$ Now, there exist $\delta\gt 0$ such that for $x_0+2-\delta_0\lt x\leq x_0+2-\delta_0+\delta$ $$f(x)-f(x_0+2-\delta_0)\gt \frac{a}{4}(x-(x_0+2-\delta_0))= \frac{a}{4}(x-x_0-(2-\delta_0))\gt \frac{a}{4}(x-x_0).$$ It is clear that $x\in (x_0,x_0+2-\delta_0+\delta]$ and $2-\delta_0+\delta\geq 1$. Therefore, we can take $x_1=x_0+1$. Then $f'(x_1)\gt a/2$ and then there exist $\delta_1\geq 1$ such that for $x_1\lt x\leq x_1+\delta_1$ $$f(x)-f(x_1)\gt \frac{a}{4}(x-x_1).$$ Take $x_2=x_1+1$ and so on. If $f$ is bounded, $(f(x_n))_{n\in \mathbb{N}}$ is a increasing bounded sequence and therefore it has a convergent subsequence. Thus, this implies that the sequence $(x_n)$: $$x_{n+1}=x_n+1,$$ have a Cauchy's subsequence and that is a contradiction. Therefore $\lim_{x\to \infty} f(x)=\infty$. I want to know if this is correct, and if there is a simpler way to prove this. Thanks.",,"['real-analysis', 'analysis']"
53,Prove that function is continuous without knowing the function explicitly,Prove that function is continuous without knowing the function explicitly,,"Let $f\colon \mathbb R^+\to\mathbb R$ be a function that satisfies the following conditions: $$\tag1 \lim_{x\to 1}f(x)=0 $$ $$\tag2f(x_1)+f(x_2)=f(x_1x_2)$$ Show that $f$ is continuous in its domain. I managed to show that $f$ is continuous at $x=1$, but I have no idea how to continue from there. Here's what I've done so far: Because $\lim_{x\to 1}f(x)=0$, for every ϵ>0 there exists a δ>0 so that $$0<|x - 1|<δ⇒|f(x)-0|<ϵ$$ To prove continuity at $x=1$ it's enough to show that $f(1)=0$ using the condition 2): $$f(1)+f(1)=f(1 ·1)$$ $$f(1)=f(1)-f(1)$$ $$f(1)=0$$ So now we have the definition of continuity at $x=1$: $$|x - 1|<δ⇒|f(x)-f(1)|<ϵ$$","Let $f\colon \mathbb R^+\to\mathbb R$ be a function that satisfies the following conditions: $$\tag1 \lim_{x\to 1}f(x)=0 $$ $$\tag2f(x_1)+f(x_2)=f(x_1x_2)$$ Show that $f$ is continuous in its domain. I managed to show that $f$ is continuous at $x=1$, but I have no idea how to continue from there. Here's what I've done so far: Because $\lim_{x\to 1}f(x)=0$, for every ϵ>0 there exists a δ>0 so that $$0<|x - 1|<δ⇒|f(x)-0|<ϵ$$ To prove continuity at $x=1$ it's enough to show that $f(1)=0$ using the condition 2): $$f(1)+f(1)=f(1 ·1)$$ $$f(1)=f(1)-f(1)$$ $$f(1)=0$$ So now we have the definition of continuity at $x=1$: $$|x - 1|<δ⇒|f(x)-f(1)|<ϵ$$",,"['real-analysis', 'continuity', 'functional-equations']"
54,"How to prove that there exist no functions $f,g:\Bbb{R}\to\Bbb{R}$ such that $f(g(x))=x^{2018}$ and $g(f(x))=x^{2019}$?",How to prove that there exist no functions  such that  and ?,"f,g:\Bbb{R}\to\Bbb{R} f(g(x))=x^{2018} g(f(x))=x^{2019}","I have tried a little bit to solve the problem which goes as follows: My intuition says that there exist no $f:\Bbb R\to\Bbb R$ such that $$f(g(x))=x^{2018}\text{ and }g(f(x))=x^{2019}.$$ Note that $$f(g(f(x)))=f(x)^{2018}\implies f(x^{2019})=f(x)^{2018}$$ Similarly, $$g(x^{2018})=g(x)^{2019}$$ Putting $x=1$ in $f(x^{2019})=f(x)^{2018}$ , we get $f(1)=f(1)^{2018}$ and thus $f(1)=0$ or $f(1)=1$ . Similarly, putting $x=1$ in $g(x^{2018})=g(x)^{2019}$ we get $g(1)=g(1)^{2019}$ and thus $g(1)=0,1,-1$ . Now, I can't proceed further. Can anybody solve it? Thanks for assistance in advance.","I have tried a little bit to solve the problem which goes as follows: My intuition says that there exist no such that Note that Similarly, Putting in , we get and thus or . Similarly, putting in we get and thus . Now, I can't proceed further. Can anybody solve it? Thanks for assistance in advance.","f:\Bbb R\to\Bbb R f(g(x))=x^{2018}\text{ and }g(f(x))=x^{2019}. f(g(f(x)))=f(x)^{2018}\implies f(x^{2019})=f(x)^{2018} g(x^{2018})=g(x)^{2019} x=1 f(x^{2019})=f(x)^{2018} f(1)=f(1)^{2018} f(1)=0 f(1)=1 x=1 g(x^{2018})=g(x)^{2019} g(1)=g(1)^{2019} g(1)=0,1,-1","['real-analysis', 'calculus', 'functions']"
55,Why can't there be a monotone function with domain $\mathbb{R}$ and range $\mathbb{R} \setminus \mathbb{Q}$?,Why can't there be a monotone function with domain  and range ?,\mathbb{R} \mathbb{R} \setminus \mathbb{Q},"Why can't there be an increasing function with domain $\mathbb{R}$ and range $\mathbb{R} \setminus \mathbb{Q}$? Edit: By range I mean the image of the function's domain, i.e. the function admits every irrational value. I feel like there should be a bijection between every irrational value it takes and the number of discontinuities it has, and I know monotone functions have at most countably many discontinuities, so this would be a contradiction. But I don't know how to show it.","Why can't there be an increasing function with domain $\mathbb{R}$ and range $\mathbb{R} \setminus \mathbb{Q}$? Edit: By range I mean the image of the function's domain, i.e. the function admits every irrational value. I feel like there should be a bijection between every irrational value it takes and the number of discontinuities it has, and I know monotone functions have at most countably many discontinuities, so this would be a contradiction. But I don't know how to show it.",,['real-analysis']
56,Example of an uncountable dense set with measure zero,Example of an uncountable dense set with measure zero,,"As stated in the title, I am trying to find an example of an uncountable dense subset of $[0,1]$ that has measure zero. My intuition is that such a subset cannot exist, but I do not have a proof of this. Currently, I can construct an uncountable dense subset that has arbitrarily small measure. Also, it is easy to construct an uncountable subset that has zero measure.","As stated in the title, I am trying to find an example of an uncountable dense subset of that has measure zero. My intuition is that such a subset cannot exist, but I do not have a proof of this. Currently, I can construct an uncountable dense subset that has arbitrarily small measure. Also, it is easy to construct an uncountable subset that has zero measure.","[0,1]",['real-analysis']
57,Computing $ \int_{0}^{2\pi} \frac{\sin(nx)^2}{\sin(x)^2} \mathrm dx $,Computing, \int_{0}^{2\pi} \frac{\sin(nx)^2}{\sin(x)^2} \mathrm dx ,I would like to compute: $$ \int_{0}^{2\pi} \frac{\sin(nx)^2}{\sin(x)^2} \mathrm dx $$ I think that : $$ \int_{0}^{2\pi} \frac{\sin(nx)^2}{\sin(x)^2} \mathrm dx=2n\pi $$ I tried to use induction: $$ \sin((n+1)x)^2=\sin(nx)^2(1-\sin(x)^2)+\sin(x)^2(1-\sin(nx)^2)+2\sin(x)\cos(x)\sin(nx)\cos(nx)$$ $$ \frac{\sin((n+1)x)^2}{\sin(nx)}=\frac{\sin(nx)^2}{\sin(x)^2}-\sin(nx)^2+1-\sin(nx)^2+2\frac{\cos(x)}{\sin(x)}\sin(nx)\cos(nx)$$ $$ \int_{0}^{2\pi}\frac{\sin((n+1)x)^2}{\sin(nx)} \mathrm dx=2(n+1)\pi+2\int_{0}^{2\pi}\sin(nx)(\frac{\cos(x)}{\sin(x)}\cos(nx)-\sin(nx))  \mathrm dx $$ So does $$ \int_{0}^{2\pi}\sin(nx)(\frac{\cos(x)}{\sin(x)}\cos(nx)-\sin(nx))  \mathrm dx=0 $$ hold? But this is probably not the best method. Could you help me?,I would like to compute: $$ \int_{0}^{2\pi} \frac{\sin(nx)^2}{\sin(x)^2} \mathrm dx $$ I think that : $$ \int_{0}^{2\pi} \frac{\sin(nx)^2}{\sin(x)^2} \mathrm dx=2n\pi $$ I tried to use induction: $$ \sin((n+1)x)^2=\sin(nx)^2(1-\sin(x)^2)+\sin(x)^2(1-\sin(nx)^2)+2\sin(x)\cos(x)\sin(nx)\cos(nx)$$ $$ \frac{\sin((n+1)x)^2}{\sin(nx)}=\frac{\sin(nx)^2}{\sin(x)^2}-\sin(nx)^2+1-\sin(nx)^2+2\frac{\cos(x)}{\sin(x)}\sin(nx)\cos(nx)$$ $$ \int_{0}^{2\pi}\frac{\sin((n+1)x)^2}{\sin(nx)} \mathrm dx=2(n+1)\pi+2\int_{0}^{2\pi}\sin(nx)(\frac{\cos(x)}{\sin(x)}\cos(nx)-\sin(nx))  \mathrm dx $$ So does $$ \int_{0}^{2\pi}\sin(nx)(\frac{\cos(x)}{\sin(x)}\cos(nx)-\sin(nx))  \mathrm dx=0 $$ hold? But this is probably not the best method. Could you help me?,,"['real-analysis', 'integration', 'definite-integrals']"
58,If $\int_0^\infty {e^{-\lambda t}f(t){\rm d}t} = 0$ for all $\lambda >0$ then $f=0$ a.e.?,If  for all  then  a.e.?,\int_0^\infty {e^{-\lambda t}f(t){\rm d}t} = 0 \lambda >0 f=0,"I am studying a paper in which the author uses something like that: Let $f$ be a bounded and Lebesgue measurable function. If $$\int_0^\infty {e^{-\lambda t}f(t)\,{\rm d}t} = 0  \qquad\text{for all} \qquad \lambda \gt0$$ then $f=0$ almost everywhere. Could you point me to a proof of this theorem? It is very important for me. Thanks!","I am studying a paper in which the author uses something like that: Let $f$ be a bounded and Lebesgue measurable function. If $$\int_0^\infty {e^{-\lambda t}f(t)\,{\rm d}t} = 0  \qquad\text{for all} \qquad \lambda \gt0$$ then $f=0$ almost everywhere. Could you point me to a proof of this theorem? It is very important for me. Thanks!",,"['real-analysis', 'integration']"
59,Evaluate $\lim\limits_{n \to \infty}\frac{(-1)^n}{n!}\int_0^n (x-1)(x-2)\cdots(x-n){\rm d}x$,Evaluate,\lim\limits_{n \to \infty}\frac{(-1)^n}{n!}\int_0^n (x-1)(x-2)\cdots(x-n){\rm d}x,"Evaluate $$\lim\limits_{n \to \infty}\frac{(-1)^n}{n!}\int_0^n (x-1)(x-2)\cdots(x-n){\rm d}x\,.$$ \begin{align*} I_n:&=\frac{(-1)^n}{n!}\int_0^n\prod_{k=1}^{n}(x-k){\rm d}x=\frac{(-1)^n}{n!}\int_0^n\prod_{k=1}^{n}(n-k-x){\rm d}x\\&=\frac{(-1)^n}{n!}\int_0^n\prod_{k=0}^{n-1}(k-x){\rm d}x=\frac{1}{n!}\int_0^n\prod_{k=0}^{n-1}(x-k){\rm d}x\\ &=\int_0^n\binom{x}{n}{\rm d}x, \end{align*} but it's hard to go forward.",Evaluate but it's hard to go forward.,"\lim\limits_{n \to \infty}\frac{(-1)^n}{n!}\int_0^n (x-1)(x-2)\cdots(x-n){\rm d}x\,. \begin{align*} I_n:&=\frac{(-1)^n}{n!}\int_0^n\prod_{k=1}^{n}(x-k){\rm d}x=\frac{(-1)^n}{n!}\int_0^n\prod_{k=1}^{n}(n-k-x){\rm d}x\\&=\frac{(-1)^n}{n!}\int_0^n\prod_{k=0}^{n-1}(k-x){\rm d}x=\frac{1}{n!}\int_0^n\prod_{k=0}^{n-1}(x-k){\rm d}x\\ &=\int_0^n\binom{x}{n}{\rm d}x, \end{align*}","['real-analysis', 'calculus', 'integration', 'limits', 'definite-integrals']"
60,Definition of a bounded sequence,Definition of a bounded sequence,,"My professor gave the following definition: A sequence $\{x_n \}$ is said to be bounded if $\exists M > 0$ such that $|x_n| \le M$ for all $n \in \mathbb N^+.$ But then what about the sequence $(0, 0, ...)$? In that case, can't $M$ be $0$? Wikipedia provides the following definition, which seems more reasonable to me: A sequence $\{x_n \}$ is said to be bounded if $\exists M \in \mathbb R$ such that $|x_n| \le M$ for all $n \in \mathbb N^+.$ Is my professor's definition inaccurate or imprecise in any way?","My professor gave the following definition: A sequence $\{x_n \}$ is said to be bounded if $\exists M > 0$ such that $|x_n| \le M$ for all $n \in \mathbb N^+.$ But then what about the sequence $(0, 0, ...)$? In that case, can't $M$ be $0$? Wikipedia provides the following definition, which seems more reasonable to me: A sequence $\{x_n \}$ is said to be bounded if $\exists M \in \mathbb R$ such that $|x_n| \le M$ for all $n \in \mathbb N^+.$ Is my professor's definition inaccurate or imprecise in any way?",,"['real-analysis', 'sequences-and-series', 'definition']"
61,Prove that $f$ is integrable if and only if $\sum^\infty_{n=1} \mu(\{x \in X : f(x) \ge n\}) < \infty$,Prove that  is integrable if and only if,f \sum^\infty_{n=1} \mu(\{x \in X : f(x) \ge n\}) < \infty,"Problem statement: Suppose that $\mu$ is a finite measure. Prove that a measurable, non-negative function $f$ is integrable if and only if $\sum^\infty_{n=1} \mu(\{x \in X : f(x) \ge n\}) < \infty$. My attempt at a solution: Let $A_n = \{x \in X : f(x) \ge n\}$. To show that $\sum^\infty_{n=1} \mu (\{x \in X : f(x) \ge n\}) < \infty$ implies $f$ is integrable, the Borel Cantelli lemma tells us that almost all $x \in X$ belong to at most finitely many $A_n$. Thus, the set $\{x \in X : f(x) = \infty\}$ has measure $0$. Now, this, together with the fact that $\mu(X) < \infty$, should give us that $f$ is integrable, but I can't figure out how to prove that! It seems fairly obvious, but I can't figure out if it is then ok to say that $f$ is bounded almost everywhere? It seems like $f$ is then pointwise bounded, but I'm not sure if that means I can find some $M$ such that $f(x) \le M$ for all $x$. For the reverse implication, I haven't come up with anything useful - I have been trying to show that the sequence of partial sums, $\sum^m_{n=1}\mu(A_n)$, is bounded, but I'm not sure how to do so.","Problem statement: Suppose that $\mu$ is a finite measure. Prove that a measurable, non-negative function $f$ is integrable if and only if $\sum^\infty_{n=1} \mu(\{x \in X : f(x) \ge n\}) < \infty$. My attempt at a solution: Let $A_n = \{x \in X : f(x) \ge n\}$. To show that $\sum^\infty_{n=1} \mu (\{x \in X : f(x) \ge n\}) < \infty$ implies $f$ is integrable, the Borel Cantelli lemma tells us that almost all $x \in X$ belong to at most finitely many $A_n$. Thus, the set $\{x \in X : f(x) = \infty\}$ has measure $0$. Now, this, together with the fact that $\mu(X) < \infty$, should give us that $f$ is integrable, but I can't figure out how to prove that! It seems fairly obvious, but I can't figure out if it is then ok to say that $f$ is bounded almost everywhere? It seems like $f$ is then pointwise bounded, but I'm not sure if that means I can find some $M$ such that $f(x) \le M$ for all $x$. For the reverse implication, I haven't come up with anything useful - I have been trying to show that the sequence of partial sums, $\sum^m_{n=1}\mu(A_n)$, is bounded, but I'm not sure how to do so.",,"['real-analysis', 'analysis', 'lebesgue-integral', 'lebesgue-measure']"
62,Set of points at which sequence of measurable functions converge (another approach),Set of points at which sequence of measurable functions converge (another approach),,"Question is to prove that : Set of all points at which a sequence of measurable functions converge is a measurable set.. What i have tried is as follows : We are looking at the following set : $$\{x\in X : (f_n(x)) \text{converges}\}$$ Which is same as  $$\{x\in X : (f_n(x)) \text{is cauchy}\}$$ Which is same as $$\{x\in X : \text{given $\epsilon>0$ there exists $N\in \mathbb{N}$ such that $|f_n(x)-f_m(x)|<\epsilon$ for all $m,n\geq N$}\}$$ I want to write this as unions and intersections of measurable sets and then conlcude this is measurable.. Something like : $$\bigcap_{p\in \mathbb{R}??}\bigcup_{m,n\in \mathbb{N}??}\{x:|(f_n-f_m)(x)|<p\}$$ Now, As $f_n,f_m$ are measurable so is $f_n-f_m$ and so is $|f_n-f_m|$ and  $\{x:|(f_n-f_m)(x)|<p\}$ being inverse image of open set is measurable.. I am not so sure how to write that as unions and intersections...","Question is to prove that : Set of all points at which a sequence of measurable functions converge is a measurable set.. What i have tried is as follows : We are looking at the following set : $$\{x\in X : (f_n(x)) \text{converges}\}$$ Which is same as  $$\{x\in X : (f_n(x)) \text{is cauchy}\}$$ Which is same as $$\{x\in X : \text{given $\epsilon>0$ there exists $N\in \mathbb{N}$ such that $|f_n(x)-f_m(x)|<\epsilon$ for all $m,n\geq N$}\}$$ I want to write this as unions and intersections of measurable sets and then conlcude this is measurable.. Something like : $$\bigcap_{p\in \mathbb{R}??}\bigcup_{m,n\in \mathbb{N}??}\{x:|(f_n-f_m)(x)|<p\}$$ Now, As $f_n,f_m$ are measurable so is $f_n-f_m$ and so is $|f_n-f_m|$ and  $\{x:|(f_n-f_m)(x)|<p\}$ being inverse image of open set is measurable.. I am not so sure how to write that as unions and intersections...",,['real-analysis']
63,$f_n$ uniformly converge to $f$ and $g_n$ uniformly converge to $g$ then $f_n \cdot g_n$ uniformly converge to $f\cdot g$,uniformly converge to  and  uniformly converge to  then  uniformly converge to,f_n f g_n g f_n \cdot g_n f\cdot g,"Please help me check, if $f_n$ uniformly converge to $f$ and $g_n$ uniformly converge to $g$ then $f_n + g_n$ uniformly converge to $f+g$ $f_n$ uniformly converge to $f$ and $g_n$ uniformly converge to $g$ then $f_n \cdot g_n$ uniformly converge to $f\cdot g$ What I've done: I Guess that first statement is true, second is false. I've tried to calculate directly from definition, nothing seems correct.","Please help me check, if $f_n$ uniformly converge to $f$ and $g_n$ uniformly converge to $g$ then $f_n + g_n$ uniformly converge to $f+g$ $f_n$ uniformly converge to $f$ and $g_n$ uniformly converge to $g$ then $f_n \cdot g_n$ uniformly converge to $f\cdot g$ What I've done: I Guess that first statement is true, second is false. I've tried to calculate directly from definition, nothing seems correct.",,"['real-analysis', 'analysis', 'convergence-divergence', 'uniform-convergence']"
64,Let $f$ be a continuous function satisfying $\lim \limits_{n \to \infty}f(x+n) = \infty$ for all $x$. Does $f$ satisfy $f(x) \to \infty$?,Let  be a continuous function satisfying  for all . Does  satisfy ?,f \lim \limits_{n \to \infty}f(x+n) = \infty x f f(x) \to \infty,"Let $f: \Bbb R \to \Bbb R$ be a continuous function satisfying $f(x+n) \to \infty$ as a sequence in $n$, for all $x$. Does $f$ satisfy $f(x) \to \infty$ as $x\to \infty$? If we drop the continuity assumption then the claim is false, by considering a function tending more and more slowly to $\infty$ as we start at larger values in $(0,1)$. (Or many other examples) As for context: a variant of this claim (when $f$ is analytic and we replace $n$ by a general increasing sequence $a_n$) could be useful to me at some technical exercise, and this is a simplification which I still cannot tackle. Assuming by contradiction that $\exists M\forall x \exists x_0>x: f(x_0)\leqslant M$, we want to show that $\exists x \exists N>0 \forall n \exists n_0>n: f(x+n) \leqslant N$. No ""quantifiers-level"" logic seems to apply here. Any ideas?","Let $f: \Bbb R \to \Bbb R$ be a continuous function satisfying $f(x+n) \to \infty$ as a sequence in $n$, for all $x$. Does $f$ satisfy $f(x) \to \infty$ as $x\to \infty$? If we drop the continuity assumption then the claim is false, by considering a function tending more and more slowly to $\infty$ as we start at larger values in $(0,1)$. (Or many other examples) As for context: a variant of this claim (when $f$ is analytic and we replace $n$ by a general increasing sequence $a_n$) could be useful to me at some technical exercise, and this is a simplification which I still cannot tackle. Assuming by contradiction that $\exists M\forall x \exists x_0>x: f(x_0)\leqslant M$, we want to show that $\exists x \exists N>0 \forall n \exists n_0>n: f(x+n) \leqslant N$. No ""quantifiers-level"" logic seems to apply here. Any ideas?",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
65,"If $a_nb_n\to 1$ and $a_n+b_n\to2$, do $a_n,b_n\to1$?","If  and , do ?","a_nb_n\to 1 a_n+b_n\to2 a_n,b_n\to1","Suppose that $a_n$ and $b_n$ are two real sequences satisfying $a_nb_n\to 1$ and $a_n+b_n\to 2$ . Does it follow that $a_n$ and $b_n$ both converge to $1$ ? I was working on this problem as one of the exercises from this online source of problems in analysis . I have not been able to find a counterexample, but I have deduced some necessary conditions on $a_n,b_n$ : Eventually $a_n$ and $b_n$ have the same sign, and this must be $>0$ : If they do not eventually share the same sign, then $a_nb_n$ can't converge to $1$ . If they are not both positive, their sum can't converge to $2$ . There exists $t>0$ and $N$ such that $a_n>t$ for all $n\ge N$ . [Hence the same conclusion holds for $b_n$ .]: If not, there is some subsequence $a_{n_k}$ with $a_{n_k}\to 0$ . But then $b_{n_k}\to\infty$ as $a_{n_k}b_{n_k}\to1$ . Then $a_{n_k}+b_{n_k}\to\infty$ , absurd. $a_n$ and $b_n$ are both bounded above: Otherwise, there must be a subsequence of the other that tends to $0$ , which is impossible by 2. I imagine a counterexample would be very strange, but I feel like a proof is also not too far out of reach. Thanks for any suggestions. P.S. Does the conclusion change if we allow complex sequences $a_n,b_n$ ?","Suppose that and are two real sequences satisfying and . Does it follow that and both converge to ? I was working on this problem as one of the exercises from this online source of problems in analysis . I have not been able to find a counterexample, but I have deduced some necessary conditions on : Eventually and have the same sign, and this must be : If they do not eventually share the same sign, then can't converge to . If they are not both positive, their sum can't converge to . There exists and such that for all . [Hence the same conclusion holds for .]: If not, there is some subsequence with . But then as . Then , absurd. and are both bounded above: Otherwise, there must be a subsequence of the other that tends to , which is impossible by 2. I imagine a counterexample would be very strange, but I feel like a proof is also not too far out of reach. Thanks for any suggestions. P.S. Does the conclusion change if we allow complex sequences ?","a_n b_n a_nb_n\to 1 a_n+b_n\to 2 a_n b_n 1 a_n,b_n a_n b_n >0 a_nb_n 1 2 t>0 N a_n>t n\ge N b_n a_{n_k} a_{n_k}\to 0 b_{n_k}\to\infty a_{n_k}b_{n_k}\to1 a_{n_k}+b_{n_k}\to\infty a_n b_n 0 a_n,b_n","['real-analysis', 'sequences-and-series']"
66,Proof: $\cos^p (\theta) \le \cos(p\theta)$,Proof:,\cos^p (\theta) \le \cos(p\theta),"I came across this problem when I was at a book store inside of a book made to prepare Berkeley graduates to pass a mandatory exam. I wanted to buy the book, but, alas, I didn't have the money (forty bucks is a lot of money when you don't have a job). So I took my phone out and started taking as many pictures as I could. Unfortunately, I didn't take any pictures of the solutions! Enough backstory. Time for math. The problem is as follows: Prove that $\cos^p( \theta) \le \cos(p \theta)$ if $0\le\theta\le\frac\pi2$ and $0\le p\le 1$ . I tried using the series expansion for cosine, but that seemed to be a dead end. Then I tried using Euler's theorem, but I got stuck. Then I got distracted and started to think about other cosine identities. For example, $$\cos\left(\frac\theta2\right)=\pm\sqrt{\frac{1+\cos(\theta)}{2}}.$$ Then what is $\cos(\frac\theta3)$ equal to? I tried to figure it out then I realized that I needed to find the root of a cubic polynomial. Then I realized that I'm better at staying focused than finding the roots of cubic polynomials. Anyways, a proof would be nice here. I really appreciate any hints or answers. I apologize for my digressions!","I came across this problem when I was at a book store inside of a book made to prepare Berkeley graduates to pass a mandatory exam. I wanted to buy the book, but, alas, I didn't have the money (forty bucks is a lot of money when you don't have a job). So I took my phone out and started taking as many pictures as I could. Unfortunately, I didn't take any pictures of the solutions! Enough backstory. Time for math. The problem is as follows: Prove that if and . I tried using the series expansion for cosine, but that seemed to be a dead end. Then I tried using Euler's theorem, but I got stuck. Then I got distracted and started to think about other cosine identities. For example, Then what is equal to? I tried to figure it out then I realized that I needed to find the root of a cubic polynomial. Then I realized that I'm better at staying focused than finding the roots of cubic polynomials. Anyways, a proof would be nice here. I really appreciate any hints or answers. I apologize for my digressions!",\cos^p( \theta) \le \cos(p \theta) 0\le\theta\le\frac\pi2 0\le p\le 1 \cos\left(\frac\theta2\right)=\pm\sqrt{\frac{1+\cos(\theta)}{2}}. \cos(\frac\theta3),"['real-analysis', 'trigonometry', 'inequality']"
67,"Prime ideals in $C[0,1]$",Prime ideals in,"C[0,1]","Are there any prime ideals in the ring $C[0,1]$ of continuous functions $[0,1]\rightarrow \mathbb{R}$, which are not maximal? Perhaps, I duplicate smb's question, but this is an interesting problem! Could you give me any hint or give a link to some literature?","Are there any prime ideals in the ring $C[0,1]$ of continuous functions $[0,1]\rightarrow \mathbb{R}$, which are not maximal? Perhaps, I duplicate smb's question, but this is an interesting problem! Could you give me any hint or give a link to some literature?",,['real-analysis']
68,Big Rudin Exercise 3.26 - Which integral is larger,Big Rudin Exercise 3.26 - Which integral is larger,,"This is exercise 3.26 in Rudin's Real & Complex Analysis: If $f$ is a positive measurable function on $[0,1]$, which is larger,   $$\int_0^1 f(x) \log f(x) \, dx$$   or   $$\int_0^1 f(s) \, ds \int_0^1 \log f(t) \, dt$$ I tried a bunch of functions and always got the first to be larger, which suggests that Hölder's inequality won't help here (at least not a direct application). I couldn't find an example that made the second larger. I'm stuck otherwise. (This is self-study, not homework) Clarification : The integral here is the Lebesgue integral. The only answer so far is only applicable to Riemann integrable functions.","This is exercise 3.26 in Rudin's Real & Complex Analysis: If $f$ is a positive measurable function on $[0,1]$, which is larger,   $$\int_0^1 f(x) \log f(x) \, dx$$   or   $$\int_0^1 f(s) \, ds \int_0^1 \log f(t) \, dt$$ I tried a bunch of functions and always got the first to be larger, which suggests that Hölder's inequality won't help here (at least not a direct application). I couldn't find an example that made the second larger. I'm stuck otherwise. (This is self-study, not homework) Clarification : The integral here is the Lebesgue integral. The only answer so far is only applicable to Riemann integrable functions.",,"['real-analysis', 'measure-theory']"
69,Limit using Poisson distribution [duplicate],Limit using Poisson distribution [duplicate],,This question already has answers here : Evaluating $\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$ (9 answers) Closed 10 years ago . Show using the Poisson distribution that $$\lim_{n \to +\infty} e^{-n} \sum_{k=1}^{n}\frac{n^k}{k!} = \frac {1}{2}$$,This question already has answers here : Evaluating $\lim\limits_{n\to\infty} e^{-n} \sum\limits_{k=0}^{n} \frac{n^k}{k!}$ (9 answers) Closed 10 years ago . Show using the Poisson distribution that $$\lim_{n \to +\infty} e^{-n} \sum_{k=1}^{n}\frac{n^k}{k!} = \frac {1}{2}$$,,"['real-analysis', 'probability-distributions']"
70,Computing $\sum _{k=1}^{\infty } \frac{\Gamma \left(\frac{k}{2}+1\right)}{k^2 \Gamma \left(\frac{k}{2}+\frac{3}{2}\right)}$ in closed form,Computing  in closed form,\sum _{k=1}^{\infty } \frac{\Gamma \left(\frac{k}{2}+1\right)}{k^2 \Gamma \left(\frac{k}{2}+\frac{3}{2}\right)},What tools other than beta function you might like to use here? $$\sum _{k=1}^{\infty } \frac{\displaystyle \Gamma \left(\frac{k}{2}+1\right)}{\displaystyle k^2 \Gamma \left(\frac{k}{2}+\frac{3}{2}\right)}\approx 1.27541$$ Supplementary question: calculating $$\sum _{k=1}^{\infty } \frac{\displaystyle \Gamma \left(\frac{k}{2}+1\right)}{\displaystyle k^3 \Gamma \left(\frac{k}{2}+\frac{3}{2}\right)}\approx 1.02593$$ Is there a way to generalize it and get such a calculation? $$\sum _{k=1}^{\infty } \frac{\displaystyle \Gamma \left(\frac{k}{2}+1\right)}{\displaystyle k^n \Gamma \left(\frac{k}{2}+\frac{3}{2}\right)}$$,What tools other than beta function you might like to use here? $$\sum _{k=1}^{\infty } \frac{\displaystyle \Gamma \left(\frac{k}{2}+1\right)}{\displaystyle k^2 \Gamma \left(\frac{k}{2}+\frac{3}{2}\right)}\approx 1.27541$$ Supplementary question: calculating $$\sum _{k=1}^{\infty } \frac{\displaystyle \Gamma \left(\frac{k}{2}+1\right)}{\displaystyle k^3 \Gamma \left(\frac{k}{2}+\frac{3}{2}\right)}\approx 1.02593$$ Is there a way to generalize it and get such a calculation? $$\sum _{k=1}^{\infty } \frac{\displaystyle \Gamma \left(\frac{k}{2}+1\right)}{\displaystyle k^n \Gamma \left(\frac{k}{2}+\frac{3}{2}\right)}$$,,"['calculus', 'real-analysis', 'sequences-and-series', 'special-functions']"
71,A set $A \subseteq \mathbb{R}$ is closed if and only if every convergent sequence in $\mathbb{R}$ completely contained in $A$ has its limit in $A$,A set  is closed if and only if every convergent sequence in  completely contained in  has its limit in,A \subseteq \mathbb{R} \mathbb{R} A A,"Real analysis is a topic I'm unfamiliar with and I'm confused on how to write proofs on them. In order to prove that: A set $A \subseteq \mathbb{R}$ is closed (1) $\iff$ Every convergent sequence in $\mathbb{R}$ completely contained in A has its limit in A (2) The definition of closed I am using is that if A is closed, then $\mathbb{R} \setminus A $ is open (i.e. $\mathbb{R} \setminus A \in \tau$. I realize that I have to prove both directions. First, in order to prove that (1) $\implies$ (2): Let $A \subseteq \mathbb{R}$ be a closed set and let $(x_{n})_{n \in \mathbb{N}}$ be a sequence of reals with $x_{n} \in A$ for every $n \in \mathbb{N}$ converging to $x_{\infty} \in \mathbb{R}$. From here, I'm not sure on how to continue writing the proof. I am trying to ultimately prove that $x_{\infty} \in A$.","Real analysis is a topic I'm unfamiliar with and I'm confused on how to write proofs on them. In order to prove that: A set $A \subseteq \mathbb{R}$ is closed (1) $\iff$ Every convergent sequence in $\mathbb{R}$ completely contained in A has its limit in A (2) The definition of closed I am using is that if A is closed, then $\mathbb{R} \setminus A $ is open (i.e. $\mathbb{R} \setminus A \in \tau$. I realize that I have to prove both directions. First, in order to prove that (1) $\implies$ (2): Let $A \subseteq \mathbb{R}$ be a closed set and let $(x_{n})_{n \in \mathbb{N}}$ be a sequence of reals with $x_{n} \in A$ for every $n \in \mathbb{N}$ converging to $x_{\infty} \in \mathbb{R}$. From here, I'm not sure on how to continue writing the proof. I am trying to ultimately prove that $x_{\infty} \in A$.",,['real-analysis']
72,$|f(x)|\leq \sqrt{\frac{\pi}{3}\int_0^\pi f'^2}$,,|f(x)|\leq \sqrt{\frac{\pi}{3}\int_0^\pi f'^2},"Let $f\in C^1([0,\pi],\mathbb R)$ such that $\displaystyle\int_0^\pi f(t) dt=0$ Prove that $\forall x\in [0,\pi],\displaystyle|f(x)|\leq \sqrt{\frac{\pi}{3}\int_0^\pi f'^2(t)dt}$ Failed natural attempt $\int_0^\pi f(t) dt=0$ tells us that there is some $\beta\in [0,1]$ such that $f(\beta)=0$ Using the fundamental theorem of calculus and Cauchy-Schwarz inequality, $\displaystyle |f(x)|=|f(x)-f(\beta)|\leq\int_x^\beta |f'(t)|dt\leq \int_0^\pi |f'(t)|dt \leq \sqrt{\pi}  \sqrt{\int_0^\pi f'^2}$ It is not sharp enough. This might have something to do with Fourier series.","Let such that Prove that Failed natural attempt tells us that there is some such that Using the fundamental theorem of calculus and Cauchy-Schwarz inequality, It is not sharp enough. This might have something to do with Fourier series.","f\in C^1([0,\pi],\mathbb R) \displaystyle\int_0^\pi f(t) dt=0 \forall x\in [0,\pi],\displaystyle|f(x)|\leq \sqrt{\frac{\pi}{3}\int_0^\pi f'^2(t)dt} \int_0^\pi f(t) dt=0 \beta\in [0,1] f(\beta)=0 \displaystyle |f(x)|=|f(x)-f(\beta)|\leq\int_x^\beta |f'(t)|dt\leq \int_0^\pi |f'(t)|dt \leq \sqrt{\pi}  \sqrt{\int_0^\pi f'^2}","['real-analysis', 'inequality', 'integral-inequality']"
73,Why is $e^{x}$ not uniformly continuous on $\mathbb{R}$?,Why is  not uniformly continuous on ?,e^{x} \mathbb{R},"It seems intuitively very clear that $e^{x}$ is not uniformly continuous on $\mathbb{R}$. I'm looking to 'prove' it using $\epsilon$-$\delta$ analysis though. I reason as follows: Suppose $\epsilon > 0$; in fact, fix it to be $\epsilon=1$. For contradiction, suppose that $\exists \delta >0$ s.t. $$ (\star) \ |x-y|<\delta \Rightarrow |e^{x}-e^{y}|<\epsilon=1 \text{  for  } x,y \in \mathbb{R}.$$ Note that $e^{x+\delta}-e^{x}=e^{x}(e^{\delta}-1)$. So, for $x$ large enough (so that RHS $>1$), the relation $(\star)$ does not hold. This is our contradiction, and so the exponential function is not uniformly continuous on $\mathbb{R}$. Is this reasoning correct and sufficient? Thanks.","It seems intuitively very clear that $e^{x}$ is not uniformly continuous on $\mathbb{R}$. I'm looking to 'prove' it using $\epsilon$-$\delta$ analysis though. I reason as follows: Suppose $\epsilon > 0$; in fact, fix it to be $\epsilon=1$. For contradiction, suppose that $\exists \delta >0$ s.t. $$ (\star) \ |x-y|<\delta \Rightarrow |e^{x}-e^{y}|<\epsilon=1 \text{  for  } x,y \in \mathbb{R}.$$ Note that $e^{x+\delta}-e^{x}=e^{x}(e^{\delta}-1)$. So, for $x$ large enough (so that RHS $>1$), the relation $(\star)$ does not hold. This is our contradiction, and so the exponential function is not uniformly continuous on $\mathbb{R}$. Is this reasoning correct and sufficient? Thanks.",,"['real-analysis', 'exponential-function']"
74,"Polynomial $P(x,y)$ with $\inf_{\mathbb{R}^2} P=0$, but without any point where $P=0$","Polynomial  with , but without any point where","P(x,y) \inf_{\mathbb{R}^2} P=0 P=0","Recently I've came across such problem: give a polynomial $P(x,y)$, with $\inf_{\mathbb{R}^2} P=0$, but there is no point on the plane where $P=0$. I couldn't solve it after a day, and seriously doubt whether such a function exists, however its source claims that there is. Is that really possible?","Recently I've came across such problem: give a polynomial $P(x,y)$, with $\inf_{\mathbb{R}^2} P=0$, but there is no point on the plane where $P=0$. I couldn't solve it after a day, and seriously doubt whether such a function exists, however its source claims that there is. Is that really possible?",,"['real-analysis', 'polynomials', 'supremum-and-infimum']"
75,"On the harmonic number ($H_n$) upper and lower ""classical"" bounds: which of those is closest to $H_n$?","On the harmonic number () upper and lower ""classical"" bounds: which of those is closest to ?",H_n H_n,"It is a well-known fact that the harmonic number $$\displaystyle H_n = \sum_{k=1}^n \frac{1}{k}$$ satisfies the following inequality: $$\displaystyle \ln(n) + \frac{1}{n} \;\leq \; H_n \; \leq \; \ln(n) + 1$$ as it is stated on page 26 of this notes . Is it true that $H_n$ is closer to $\ln(n) + 1$ than $H_n$ is to $\displaystyle \ln(n) + \frac{1}{n}$ ? If so, how to prove that?","It is a well-known fact that the harmonic number satisfies the following inequality: as it is stated on page 26 of this notes . Is it true that is closer to than is to ? If so, how to prove that?",\displaystyle H_n = \sum_{k=1}^n \frac{1}{k} \displaystyle \ln(n) + \frac{1}{n} \;\leq \; H_n \; \leq \; \ln(n) + 1 H_n \ln(n) + 1 H_n \displaystyle \ln(n) + \frac{1}{n},"['calculus', 'real-analysis', 'discrete-mathematics']"
76,Geodesic between two points,Geodesic between two points,,"I have a question about geodesics. So far I know that for any surface $S$ defined by some immersion $f: U \subset\mathbb{R}^2 \rightarrow S \subset \mathbb{R}^3,$ we have that for any point on the surface and any direction, there exists locally a geodesic ( due to Picard-Lindelöf applied to the geodesic equation). But what can we say globally? To me it would be more natural to ask that if our surface is path-connected, does this imply that there exists a geodesic between any two points? What about more general manifolds, is this then true? Is there a similar existence theorem for the geodesic ODE in the context of this boundary value problem?","I have a question about geodesics. So far I know that for any surface $S$ defined by some immersion $f: U \subset\mathbb{R}^2 \rightarrow S \subset \mathbb{R}^3,$ we have that for any point on the surface and any direction, there exists locally a geodesic ( due to Picard-Lindelöf applied to the geodesic equation). But what can we say globally? To me it would be more natural to ask that if our surface is path-connected, does this imply that there exists a geodesic between any two points? What about more general manifolds, is this then true? Is there a similar existence theorem for the geodesic ODE in the context of this boundary value problem?",,"['real-analysis', 'ordinary-differential-equations']"
77,A set with measure $0$ has a translate containing no rational number.,A set with measure  has a translate containing no rational number.,0,"Suppose $E$ is a set with measure $0$ . Show there exists $t\in \mathbb{R}$ such that $E+t$ contains no rational number. My idea is to find an interval in $E$ , then we can get a contradiction. I try to begin with a point in $E$ and then consider if there is an interval containing this point in $E$ . But I don't know how to start. Maybe, we can go by contradiction.  If $E+t$ contains a rational number $q_t$ for every $t\in \Bbb R$ , then we have a function $f:\mathbb{R}\to\mathbb{Q}$ , $t\mapsto q_t$ .  But this idea leads nowhere.","Suppose is a set with measure . Show there exists such that contains no rational number. My idea is to find an interval in , then we can get a contradiction. I try to begin with a point in and then consider if there is an interval containing this point in . But I don't know how to start. Maybe, we can go by contradiction.  If contains a rational number for every , then we have a function , .  But this idea leads nowhere.",E 0 t\in \mathbb{R} E+t E E E E+t q_t t\in \Bbb R f:\mathbb{R}\to\mathbb{Q} t\mapsto q_t,"['real-analysis', 'measure-theory', 'lebesgue-measure', 'rational-numbers']"
78,Proving a set in $\mathbb{Q}$ that is bounded above to have no largest number (greatest element),Proving a set in  that is bounded above to have no largest number (greatest element),\mathbb{Q},"I'm studying the set $Q$. At this point I don't know the real numbers. This is the theorem: Let $A$ be the set such that $A=\{p\in \mathbb{Q}^{+}:p^{2}<2\}$. Then $A$ contains no largest number. The prove given in Rudin's book of analysis goes something like this: Given $p\in A$, let's consider the number   $q=p-\dfrac{p^2-2}{p+2}=\dfrac{2p+2}{p+2}$. Then   $q^2-2=\dfrac{2(p^2-2)}{(p+2)^2}$.  This prove that if $p\in A$ then   $p^2-2<0$, $q>p$, and $q^2<2$. Thus $q\in A$ and $q>p$. Therefore, for   every $p\in A$ we can find a rational $q\in A$ such that $p<q$. Now here is my problem. First of all I understand the proof perfectly, but I don't see the motivation for finding $q$. Let's suppose I wanted to find $q$ in the set $B=\{p\in Q^{+}: p^{2}<3\}$. I wouldn't now how to find it. Also I would like to know some 'standard', if there is some, process to apply in this cases. For example in my try, before looking at the solution, I was thinking in this way: I need to find a number $q$ such that  $p^{2}<q^{2}<2$ in such a way that $p<q$"". If I set this numbers on the real line I can observe essentially two ways to follow. In the first one, if try to find the number $q^2$ such that $p^{2}<q^{2}<2$ I can think of some of the form $q^{2}=p^{2}+(\dfrac{2-p^2}{n})$ where $n$ is a natural number sufficiently large as necessary (here I'm using Archimedian property) to find a perfect square. In my second approach I'd like to find the number $q$ such that $p<q$. In this case I would think almost in the same way as before. I can think of $q$ as something sufficiently close to $p$ in the form $q=p+\dfrac{1}{n}$ with $n$ sufficiently large so that $p^2<q^{2}<2$. So far I haven't been able to solve the problem with any of my tries and I don't know if I'm in the right direction or it's just an obsession to use the Archimedian property.","I'm studying the set $Q$. At this point I don't know the real numbers. This is the theorem: Let $A$ be the set such that $A=\{p\in \mathbb{Q}^{+}:p^{2}<2\}$. Then $A$ contains no largest number. The prove given in Rudin's book of analysis goes something like this: Given $p\in A$, let's consider the number   $q=p-\dfrac{p^2-2}{p+2}=\dfrac{2p+2}{p+2}$. Then   $q^2-2=\dfrac{2(p^2-2)}{(p+2)^2}$.  This prove that if $p\in A$ then   $p^2-2<0$, $q>p$, and $q^2<2$. Thus $q\in A$ and $q>p$. Therefore, for   every $p\in A$ we can find a rational $q\in A$ such that $p<q$. Now here is my problem. First of all I understand the proof perfectly, but I don't see the motivation for finding $q$. Let's suppose I wanted to find $q$ in the set $B=\{p\in Q^{+}: p^{2}<3\}$. I wouldn't now how to find it. Also I would like to know some 'standard', if there is some, process to apply in this cases. For example in my try, before looking at the solution, I was thinking in this way: I need to find a number $q$ such that  $p^{2}<q^{2}<2$ in such a way that $p<q$"". If I set this numbers on the real line I can observe essentially two ways to follow. In the first one, if try to find the number $q^2$ such that $p^{2}<q^{2}<2$ I can think of some of the form $q^{2}=p^{2}+(\dfrac{2-p^2}{n})$ where $n$ is a natural number sufficiently large as necessary (here I'm using Archimedian property) to find a perfect square. In my second approach I'd like to find the number $q$ such that $p<q$. In this case I would think almost in the same way as before. I can think of $q$ as something sufficiently close to $p$ in the form $q=p+\dfrac{1}{n}$ with $n$ sufficiently large so that $p^2<q^{2}<2$. So far I haven't been able to solve the problem with any of my tries and I don't know if I'm in the right direction or it's just an obsession to use the Archimedian property.",,"['real-analysis', 'elementary-set-theory']"
79,Does $\sum_{n\ge1} \sin (\pi \sqrt{n^2+1}) $ converge/diverge?,Does  converge/diverge?,\sum_{n\ge1} \sin (\pi \sqrt{n^2+1}) ,How would you prove convergence/divergence of the following series? $$\sum_{n\ge1} \sin (\pi \sqrt{n^2+1}) $$ I'm interested in more ways of proving convergence/divergence for this series. Thanks. EDIT I'm going to post the solution I've found here: $$a_{n}= \sin (\pi \sqrt{n^2+1})=\sin (\pi (\sqrt{n^2+1}-n)+n\pi)=(-1)^n \sin (\pi (\sqrt{n^2+1}-n))=$$ $$ (-1)^n \sin \frac{\pi}{\sqrt{n^2+1}+n}$$ The sequence $b_{n} = \sin \frac{\pi}{\sqrt{n^2+1}+n}$ monotonically decreases to $0$. Since our series is an alternating series then it converges.,How would you prove convergence/divergence of the following series? $$\sum_{n\ge1} \sin (\pi \sqrt{n^2+1}) $$ I'm interested in more ways of proving convergence/divergence for this series. Thanks. EDIT I'm going to post the solution I've found here: $$a_{n}= \sin (\pi \sqrt{n^2+1})=\sin (\pi (\sqrt{n^2+1}-n)+n\pi)=(-1)^n \sin (\pi (\sqrt{n^2+1}-n))=$$ $$ (-1)^n \sin \frac{\pi}{\sqrt{n^2+1}+n}$$ The sequence $b_{n} = \sin \frac{\pi}{\sqrt{n^2+1}+n}$ monotonically decreases to $0$. Since our series is an alternating series then it converges.,,"['calculus', 'real-analysis', 'sequences-and-series', 'trigonometry', 'convergence-divergence']"
80,Arc length of the Cantor function,Arc length of the Cantor function,,"How does one find the arc length of the Cantor function? Wikipedia says that the length is $2$ . I can ""see"" that the length is at most $2$ by a simple triangle inequality argument. I am struggling to come up with a partition $P$ such that the arc length is at least 2. I tried a partition of the form $\{  1/ 3^n : 0 \le k \le n \}$ but I guess I am making some mistake in my calculation so that I get the length as $3/4$ instead of close to $2$ .","How does one find the arc length of the Cantor function? Wikipedia says that the length is . I can ""see"" that the length is at most by a simple triangle inequality argument. I am struggling to come up with a partition such that the arc length is at least 2. I tried a partition of the form but I guess I am making some mistake in my calculation so that I get the length as instead of close to .",2 2 P \{  1/ 3^n : 0 \le k \le n \} 3/4 2,"['real-analysis', 'cantor-set']"
81,Prove $x^n$ is not uniformly convergent,Prove  is not uniformly convergent,x^n,"This question pertains to the sequence of functions $f_n(x)=x^n$ on the interval $[0,1]$. It can be shown this sequence of functions ${f_n}$ converges point-wise to the limit $f$ where $f$ is defined by $f(x)=0$ on $[0,1)$ and $f(x)=1$ at $x=1$. However, this sequence of functions ${f_n}$ does not converge uniformly to $f$. One way to prove this (which I have seen) is via a theorem which proves that if a sequence of functions ${f_n}$ converges uniformly to $f$, then $f$ is continuous. And clearly it is not the case that $f$ is continuous in our example, so our convergence is not uniform. However, I have seen another test for uniform convergence on an interval $s$. That is that: $${\rm lim}\ [{\rm sup}\ \{ |  f_n(x)-f(x)|\ :\ x\in S\ \}]=0$$ This conception of uniform convergence can be found in my Walter Rudin analysis book for example. My question is, how can we use this definition of uniform convergence to show $x^n$ is not uniformly convergent? I am having trouble seeing how to apply this definition to this example, especially since the limit $f$ is defined piece-wise. But I know I need to find the sup of the difference between $f_n$ and $f$ over the interval $[0,1]$ (how do I find this sup?). And then once I find it, I need to take the limit as $n\to\infty$ and show it does not equal $0$. Thank you for your help!","This question pertains to the sequence of functions $f_n(x)=x^n$ on the interval $[0,1]$. It can be shown this sequence of functions ${f_n}$ converges point-wise to the limit $f$ where $f$ is defined by $f(x)=0$ on $[0,1)$ and $f(x)=1$ at $x=1$. However, this sequence of functions ${f_n}$ does not converge uniformly to $f$. One way to prove this (which I have seen) is via a theorem which proves that if a sequence of functions ${f_n}$ converges uniformly to $f$, then $f$ is continuous. And clearly it is not the case that $f$ is continuous in our example, so our convergence is not uniform. However, I have seen another test for uniform convergence on an interval $s$. That is that: $${\rm lim}\ [{\rm sup}\ \{ |  f_n(x)-f(x)|\ :\ x\in S\ \}]=0$$ This conception of uniform convergence can be found in my Walter Rudin analysis book for example. My question is, how can we use this definition of uniform convergence to show $x^n$ is not uniformly convergent? I am having trouble seeing how to apply this definition to this example, especially since the limit $f$ is defined piece-wise. But I know I need to find the sup of the difference between $f_n$ and $f$ over the interval $[0,1]$ (how do I find this sup?). And then once I find it, I need to take the limit as $n\to\infty$ and show it does not equal $0$. Thank you for your help!",,"['real-analysis', 'sequences-and-series', 'analysis', 'convergence-divergence', 'uniform-convergence']"
82,"If $\sum\limits_{n=1}^\infty na_n$ converges , does $\sum\limits_{n=1}^\infty na_{n+1}$ converge?","If  converges , does  converge?",\sum\limits_{n=1}^\infty na_n \sum\limits_{n=1}^\infty na_{n+1},"I ask for some help with this question: Prove or provide counter example: If $\sum\limits_{n=1}^\infty na_n$ converges then $\sum\limits_{n=1}^\infty na_{n+1}$ also converges. I tries this way: If $\sum\limits_{n=1}^\infty na_n$ converges then $na_n \to 0$, therefore $a_n \to 0$. There are 3 possible cases: 1) If $a_n >0 $ and $a_n$ is monotonic decreasing sequence then $na_{n+1}<na_n$ and $\sum_{n=1}^\infty na_{n+1}$ converges by Comparison Test. 2) If $a_n >0 $ and $a_n$ is not monotonic decreasing sequence : it is not possible that $a_{n+1}>a_n$ because in this case $a_n \to \infty$, therefore it must be $a_{n+1} \le a_n$ and $\sum_{n=1}^\infty na_{n+1}$ converges by Comparison Test. 3) If $a_n$ is sign-alternating series. There I have a problem to find a solution. Thanks.","I ask for some help with this question: Prove or provide counter example: If $\sum\limits_{n=1}^\infty na_n$ converges then $\sum\limits_{n=1}^\infty na_{n+1}$ also converges. I tries this way: If $\sum\limits_{n=1}^\infty na_n$ converges then $na_n \to 0$, therefore $a_n \to 0$. There are 3 possible cases: 1) If $a_n >0 $ and $a_n$ is monotonic decreasing sequence then $na_{n+1}<na_n$ and $\sum_{n=1}^\infty na_{n+1}$ converges by Comparison Test. 2) If $a_n >0 $ and $a_n$ is not monotonic decreasing sequence : it is not possible that $a_{n+1}>a_n$ because in this case $a_n \to \infty$, therefore it must be $a_{n+1} \le a_n$ and $\sum_{n=1}^\infty na_{n+1}$ converges by Comparison Test. 3) If $a_n$ is sign-alternating series. There I have a problem to find a solution. Thanks.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
83,How many smooth functions are non-analytic?,How many smooth functions are non-analytic?,,"We know from example that not all smooth (infinitely differentiable) functions are analytic (equal to their Taylor expansion at all points). However, the examples on the linked page seem rather contrived, and most smooth functions that I've encountered in math and physics are analytic. How many smooth functions are not analytic (in terms of measure or cardinality)? In what situations are such functions encountered? Are they ever encountered outside of real analysis (e.g. in physics)?","We know from example that not all smooth (infinitely differentiable) functions are analytic (equal to their Taylor expansion at all points). However, the examples on the linked page seem rather contrived, and most smooth functions that I've encountered in math and physics are analytic. How many smooth functions are not analytic (in terms of measure or cardinality)? In what situations are such functions encountered? Are they ever encountered outside of real analysis (e.g. in physics)?",,"['real-analysis', 'analysis', 'taylor-expansion', 'analyticity']"
84,Are there any functions that are (always) continuous yet not differentiable? Or vice-versa?,Are there any functions that are (always) continuous yet not differentiable? Or vice-versa?,,"It seems like functions that are continuous always seem to be differentiable, to me.  I can't imagine one that is not.  Are there any examples of functions that are continuous, yet not differentiable? The other way around seems a bit simpler -- a differentiable function is obviously always going to be continuous.  But are there any that do not satisfy this?","It seems like functions that are continuous always seem to be differentiable, to me.  I can't imagine one that is not.  Are there any examples of functions that are continuous, yet not differentiable? The other way around seems a bit simpler -- a differentiable function is obviously always going to be continuous.  But are there any that do not satisfy this?",,"['real-analysis', 'continuity']"
85,"Counterexample: Continuous, but not uniformly continuous functions do not preserve Cauchy Sequences","Counterexample: Continuous, but not uniformly continuous functions do not preserve Cauchy Sequences",,"I want to prove this: There exists a continuous function $f:\mathbb{Q}\to\mathbb{Q}$, but not uniformly continuous, and a Cauchy sequence $\{x_n\}_{n\in\mathbb{N}}$ of rational numbers such that $\{f(x_n)\}_{n\in\mathbb{N}}$ is not a Cauchy sequence. More particular: Does there exist a Cauchy sequence $\{x_n\}_{n\in\mathbb{N}}$ of rational numbers such that $\{x_n^2\}$ is not Cauchy? I think that would be weird, and the counterexample should be with some function that is continuous in $\mathbb{Q}$ but not in $\mathbb{R}$. Am I right? Which would be some example of that?","I want to prove this: There exists a continuous function $f:\mathbb{Q}\to\mathbb{Q}$, but not uniformly continuous, and a Cauchy sequence $\{x_n\}_{n\in\mathbb{N}}$ of rational numbers such that $\{f(x_n)\}_{n\in\mathbb{N}}$ is not a Cauchy sequence. More particular: Does there exist a Cauchy sequence $\{x_n\}_{n\in\mathbb{N}}$ of rational numbers such that $\{x_n^2\}$ is not Cauchy? I think that would be weird, and the counterexample should be with some function that is continuous in $\mathbb{Q}$ but not in $\mathbb{R}$. Am I right? Which would be some example of that?",,"['real-analysis', 'examples-counterexamples', 'uniform-continuity', 'cauchy-sequences']"
86,Is the product of a Cesàro summable sequence of $0$s and $1$s Cesàro summable?,Is the product of a Cesàro summable sequence of s and s Cesàro summable?,0 1,"Suppose $a_n$ and $b_n$ to be Cesàro summable sequences of zeros and ones, $a_n\in\{0,1\}$ and $b_n\in\{0,1\}$, i.e. the limits $$ \lim_{N\rightarrow\infty}\frac{1}{N}\sum_{n=1}^{N}a_n,  $$ and  $$ \lim_{N\rightarrow\infty}\frac{1}{N}\sum_{n=1}^{N}b_n,  $$ do exist. Is the product sequence $c_n=a_nb_n$ always Cesàro summable?","Suppose $a_n$ and $b_n$ to be Cesàro summable sequences of zeros and ones, $a_n\in\{0,1\}$ and $b_n\in\{0,1\}$, i.e. the limits $$ \lim_{N\rightarrow\infty}\frac{1}{N}\sum_{n=1}^{N}a_n,  $$ and  $$ \lim_{N\rightarrow\infty}\frac{1}{N}\sum_{n=1}^{N}b_n,  $$ do exist. Is the product sequence $c_n=a_nb_n$ always Cesàro summable?",,"['real-analysis', 'cesaro-summable']"
87,A function vanishing at infinity is uniformly continuous,A function vanishing at infinity is uniformly continuous,,"If $f\in C_0(\mathbb{R})$ (i.e. $f$ continuous and for all $\varepsilon>0$ there is $R>0$ such that $|f(x)|<\varepsilon$ whenever $|x|>R$), then why is $f$ uniformly continuous? I know that we should somehow use that $f$ is ""small"" outside a compact interval (on which it is uniformly continuous), how can we nicely write down the $\delta$?","If $f\in C_0(\mathbb{R})$ (i.e. $f$ continuous and for all $\varepsilon>0$ there is $R>0$ such that $|f(x)|<\varepsilon$ whenever $|x|>R$), then why is $f$ uniformly continuous? I know that we should somehow use that $f$ is ""small"" outside a compact interval (on which it is uniformly continuous), how can we nicely write down the $\delta$?",,"['calculus', 'real-analysis']"
88,The cardinality of Lebesgue sets,The cardinality of Lebesgue sets,,"Suppose $A=\{S\;|\;S \subset \mathbb R^n, S\text{ is Lebesgue measurable}\}$. What is the cardinality of $A$? Is it the same as the cardinality of all of the real numbers?","Suppose $A=\{S\;|\;S \subset \mathbb R^n, S\text{ is Lebesgue measurable}\}$. What is the cardinality of $A$? Is it the same as the cardinality of all of the real numbers?",,"['real-analysis', 'measure-theory', 'elementary-set-theory']"
89,Why is the total derivative of a diffeomorphism invertible?,Why is the total derivative of a diffeomorphism invertible?,,"I'm trying to brush up on some differential geometry, but there's a subtle point I don't understand. Suppose $h$ is a diffeomorphism. Then the lecture notes here suggest that it's derivative $df_x$ is an invertible linear map. Why precisely does the invertibility of $df_x$ follow from that of $f$? Apologies if this is a trivial question - I'm a little out of practise with total derivatives!","I'm trying to brush up on some differential geometry, but there's a subtle point I don't understand. Suppose $h$ is a diffeomorphism. Then the lecture notes here suggest that it's derivative $df_x$ is an invertible linear map. Why precisely does the invertibility of $df_x$ follow from that of $f$? Apologies if this is a trivial question - I'm a little out of practise with total derivatives!",,"['real-analysis', 'differential-geometry', 'derivatives', 'inverse']"
90,"between Borel $\sigma$ algebra and Lebesgue $\sigma$ algebra, are there any other $\sigma$ algebra?","between Borel  algebra and Lebesgue  algebra, are there any other  algebra?",\sigma \sigma \sigma,"Is there any $\sigma$-algebra that is strictly between the Borel $\sigma$-algebra and the Lebesgue $\sigma$-algebra? How about not in between the two, but in general, are there any other $\sigma$ algebra(s)? What can be concluded about measure too, e.g. is Lebesgue measure the only measure for Lebesgue $\sigma$ algebra?","Is there any $\sigma$-algebra that is strictly between the Borel $\sigma$-algebra and the Lebesgue $\sigma$-algebra? How about not in between the two, but in general, are there any other $\sigma$ algebra(s)? What can be concluded about measure too, e.g. is Lebesgue measure the only measure for Lebesgue $\sigma$ algebra?",,['real-analysis']
91,Continuous injective map is strictly monotonic,Continuous injective map is strictly monotonic,,"Show that if $f: \mathbb{R} \to \mathbb{R}$ is a continuous injective map, then it is strictly monotonic. Could someone give me a proof for this? I have the intuition for why it's true - I'm just having trouble expressing that intuition in a rigorous manner. Basically consider two points $x_1, x_2 \in \mathbb{R}$. By the problem statement, $f$ is continuous on $[x_1, \, x_2]$. WLOG, assume that $f$ is strictly increasing. It there exists a point where it is not increasing, then $f$ hits a value twice, and it's not injective.","Show that if $f: \mathbb{R} \to \mathbb{R}$ is a continuous injective map, then it is strictly monotonic. Could someone give me a proof for this? I have the intuition for why it's true - I'm just having trouble expressing that intuition in a rigorous manner. Basically consider two points $x_1, x_2 \in \mathbb{R}$. By the problem statement, $f$ is continuous on $[x_1, \, x_2]$. WLOG, assume that $f$ is strictly increasing. It there exists a point where it is not increasing, then $f$ hits a value twice, and it's not injective.",,['real-analysis']
92,"For a set of positive measure there is an interval in which its density is high, $\mu(E\cap I)> \rho \mu(I)$","For a set of positive measure there is an interval in which its density is high,",\mu(E\cap I)> \rho \mu(I),"In my self-study, I came across the following two interesting, related results: Let $E$ be Lebesgue measurable, with $\mu(E)>0$ (here $\mu$ denotes the Lebesgue measure). Then: for any $0<\rho<1$, there exists an open interval $I$ such that $\mu(E \cap I)> \rho \cdot \mu(I)$. In turn we should be able to use (1) with $\rho > 3/4$ to prove that the set $E-E = \{x-y : x, y \in E\}$ contains an (open) interval centered at $0$ (in particular, if $\rho > 3/4$, the text I am using suggests that $(-\frac{1}{2} \mu(I), \frac{1}{2}\mu(I)) \subseteq E-E$). I would like to see if anyone visiting today would be up for proving (1) or (2) (inclusive-or). I have tried to work out (1) a few times, but none of my attempts have been satisfactory.","In my self-study, I came across the following two interesting, related results: Let $E$ be Lebesgue measurable, with $\mu(E)>0$ (here $\mu$ denotes the Lebesgue measure). Then: for any $0<\rho<1$, there exists an open interval $I$ such that $\mu(E \cap I)> \rho \cdot \mu(I)$. In turn we should be able to use (1) with $\rho > 3/4$ to prove that the set $E-E = \{x-y : x, y \in E\}$ contains an (open) interval centered at $0$ (in particular, if $\rho > 3/4$, the text I am using suggests that $(-\frac{1}{2} \mu(I), \frac{1}{2}\mu(I)) \subseteq E-E$). I would like to see if anyone visiting today would be up for proving (1) or (2) (inclusive-or). I have tried to work out (1) a few times, but none of my attempts have been satisfactory.",,"['real-analysis', 'measure-theory']"
93,Proving that $\mathbb{Q}$ is neither open or closed in $\mathbb{R}$,Proving that  is neither open or closed in,\mathbb{Q} \mathbb{R},"I need to figure out if $\mathbb{Q}$ is either open or closed in $\mathbb{R}$ and prove my assertions. I don't think it's either. I would like a review/critique of my proofs to make sure that there's not an easier way to do it. To see that it's not open, take some $\frac{m}{n}$ with both $m$ and $n$ perfect squares so that $\left(\frac{m}{n}\right)^{\frac{1}{2}}$ is rational. We have that $x \mapsto x^{\frac{1}{2}}$ is continuous. Hence for any $\epsilon$-neighborhood of $\left(\frac{m}{n}\right)^{\frac{1}{2}}$, there exists a $\delta$-neighborhood of $\frac{m}{n}$ that maps into it. but $$ \begin{eqnarray*} \frac{m}{n} - \frac{cm}{cn + 1} < \delta &\Leftrightarrow& \frac{m}{n(cn + 1)} < \delta n(cn + 1) \\ &\Leftrightarrow& \frac{m}{\delta n^{2}} < \left(nc^{2} + 1\right)^{2} \\ &\Leftrightarrow& \frac{m}{\delta n^{4}} < c^2 + \frac{2}{n}c + \frac{1}{n^{2}} \\ &\Leftrightarrow& \frac{m}{\delta n^{4}} < \left(c + \frac{1}{n}\right)^{2} \\ &\Leftrightarrow& \left(\frac{m}{\delta n^{4}}\right)^{\frac{1}{2}} - \frac{1}{n} < c \end{eqnarray*} $$ So we can make $\frac{cm}{cm + 1}$ as close to $\frac{m}{n}$ as we like by taking $c$ sufficiently large. If $cm$ is a perfect square, then $c$ must be a perfect square because $m$ is so we can just choose a $c$ that's not a perfect square and hence $\left(\frac{cm}{cn +1}\right)^{\frac{1}{2}}$ is irrational. Because we can place an irrational arbitrarily close to at least one positive rational, $\mathbb{Q}$ is not open. To show that $\mathbb{Q}$ is not closed, simply pick some irrational $\alpha$ and let $\epsilon_n = \frac{1}{n}$. Let $x_n$ be chosen from between $\alpha - \epsilon_n$ and $\alpha$. Then the sequence $x_n$ converges to $\alpha$ which shows that the complement of $\mathbb{q}$ in $\mathbb{R}$ is not open as we can place a rational in any $\epsilon$-neighborhood of any irrational. Proving that the rationals aren't open was so much harder than proving that they're not closed because my book hadn't proved that the irrationals are dense in $\mathbb{R}$ and so I couldn't use that. Is there some other way that I could do this?","I need to figure out if $\mathbb{Q}$ is either open or closed in $\mathbb{R}$ and prove my assertions. I don't think it's either. I would like a review/critique of my proofs to make sure that there's not an easier way to do it. To see that it's not open, take some $\frac{m}{n}$ with both $m$ and $n$ perfect squares so that $\left(\frac{m}{n}\right)^{\frac{1}{2}}$ is rational. We have that $x \mapsto x^{\frac{1}{2}}$ is continuous. Hence for any $\epsilon$-neighborhood of $\left(\frac{m}{n}\right)^{\frac{1}{2}}$, there exists a $\delta$-neighborhood of $\frac{m}{n}$ that maps into it. but $$ \begin{eqnarray*} \frac{m}{n} - \frac{cm}{cn + 1} < \delta &\Leftrightarrow& \frac{m}{n(cn + 1)} < \delta n(cn + 1) \\ &\Leftrightarrow& \frac{m}{\delta n^{2}} < \left(nc^{2} + 1\right)^{2} \\ &\Leftrightarrow& \frac{m}{\delta n^{4}} < c^2 + \frac{2}{n}c + \frac{1}{n^{2}} \\ &\Leftrightarrow& \frac{m}{\delta n^{4}} < \left(c + \frac{1}{n}\right)^{2} \\ &\Leftrightarrow& \left(\frac{m}{\delta n^{4}}\right)^{\frac{1}{2}} - \frac{1}{n} < c \end{eqnarray*} $$ So we can make $\frac{cm}{cm + 1}$ as close to $\frac{m}{n}$ as we like by taking $c$ sufficiently large. If $cm$ is a perfect square, then $c$ must be a perfect square because $m$ is so we can just choose a $c$ that's not a perfect square and hence $\left(\frac{cm}{cn +1}\right)^{\frac{1}{2}}$ is irrational. Because we can place an irrational arbitrarily close to at least one positive rational, $\mathbb{Q}$ is not open. To show that $\mathbb{Q}$ is not closed, simply pick some irrational $\alpha$ and let $\epsilon_n = \frac{1}{n}$. Let $x_n$ be chosen from between $\alpha - \epsilon_n$ and $\alpha$. Then the sequence $x_n$ converges to $\alpha$ which shows that the complement of $\mathbb{q}$ in $\mathbb{R}$ is not open as we can place a rational in any $\epsilon$-neighborhood of any irrational. Proving that the rationals aren't open was so much harder than proving that they're not closed because my book hadn't proved that the irrationals are dense in $\mathbb{R}$ and so I couldn't use that. Is there some other way that I could do this?",,"['real-analysis', 'general-topology']"
94,"Solving (quadratic) equations of iterated functions, such as $f(f(x))=f(x)+x$","Solving (quadratic) equations of iterated functions, such as",f(f(x))=f(x)+x,"In this thread, the question was to find a $f: \mathbb{R} \to \mathbb{R}$ such that $$f(f(x)) = f(x) + x$$ (which was revealed in the comments to be solved by $f(x) = \varphi x$ where $\varphi$ is the golden ratio $\frac{1+\sqrt{5}}{2}$). Having read about iterated functions shortly before though, I came up with this train of thought: $$f(f(x)) = f(x) + x$$ $$\Leftrightarrow f^2 = f^1 + f^0$$ $$f^2 - f - f^0 = 0$$ where $f^n$ denotes the $n$'th iterate of $f$. Now I solved the resulting quadratic equation much as I did with plain numbers $$f = \frac{1}{2} \pm \sqrt{\frac{1}{4} + 1}$$ $$f = \frac{1 \pm \sqrt{1+4}}{2} = \frac{1 \pm \sqrt{5}}{2}\cdot f^0$$ And finally the solution $$f(x) = \frac{1 \pm \sqrt{5}}{2} x .$$ Now my question is: *Is it somehow allowed to work with functions in that way?** I know that in the above, there are denotational ambiguities as $1$ is actually treated as $f^0 = id$ ... But since the result is correct, there seems to be some correct thing in this approach. So can I actually solve certain functional equations like this? And if true, how would the correct notation of the above be ?","In this thread, the question was to find a $f: \mathbb{R} \to \mathbb{R}$ such that $$f(f(x)) = f(x) + x$$ (which was revealed in the comments to be solved by $f(x) = \varphi x$ where $\varphi$ is the golden ratio $\frac{1+\sqrt{5}}{2}$). Having read about iterated functions shortly before though, I came up with this train of thought: $$f(f(x)) = f(x) + x$$ $$\Leftrightarrow f^2 = f^1 + f^0$$ $$f^2 - f - f^0 = 0$$ where $f^n$ denotes the $n$'th iterate of $f$. Now I solved the resulting quadratic equation much as I did with plain numbers $$f = \frac{1}{2} \pm \sqrt{\frac{1}{4} + 1}$$ $$f = \frac{1 \pm \sqrt{1+4}}{2} = \frac{1 \pm \sqrt{5}}{2}\cdot f^0$$ And finally the solution $$f(x) = \frac{1 \pm \sqrt{5}}{2} x .$$ Now my question is: *Is it somehow allowed to work with functions in that way?** I know that in the above, there are denotational ambiguities as $1$ is actually treated as $f^0 = id$ ... But since the result is correct, there seems to be some correct thing in this approach. So can I actually solve certain functional equations like this? And if true, how would the correct notation of the above be ?",,"['real-analysis', 'abstract-algebra', 'notation', 'functional-equations']"
95,"Proof of existence of a limit for the sequence recursively-defined with $a_1=1$, $a_2=1$ and $a_n=\frac{1}{a_{n-1}}+\frac{1}{a_{n-2}}$ for $n\ge2$","Proof of existence of a limit for the sequence recursively-defined with ,  and  for",a_1=1 a_2=1 a_n=\frac{1}{a_{n-1}}+\frac{1}{a_{n-2}} n\ge2,"I have a sequence defined by $$ a_1=1,\quad a_2=1,\quad a_n=\frac{1}{a_{n-1}}+\frac{1}{a_{n-2}}\text{ for } n\ge2\text. $$ Now, if $\lim\limits_{n\to \infty}a_n=g$ then $\lim\limits_{n\to \infty}a_n=\lim\limits_{n\to \infty}\Bigl(\frac{1}{a_{n-1}}+\frac{1}{a_{n-2}}\Bigr)=\frac{2}{g}$ , so $g=\sqrt{2}$ or $g=-\sqrt{2}$ , but $a_n>0$ , so $g=\sqrt{2}$ . Now, how do I prove that it has an actual limit? Also, it can be proven that $1\le a_n\le2$ , and it's not monotonic because $a_4 \gt a_5 \lt a_6$ . Also, it's not monotonic after any $N\in\mathbb N$ .","I have a sequence defined by Now, if then , so or , but , so . Now, how do I prove that it has an actual limit? Also, it can be proven that , and it's not monotonic because . Also, it's not monotonic after any .","
a_1=1,\quad a_2=1,\quad a_n=\frac{1}{a_{n-1}}+\frac{1}{a_{n-2}}\text{ for } n\ge2\text.
 \lim\limits_{n\to \infty}a_n=g \lim\limits_{n\to \infty}a_n=\lim\limits_{n\to \infty}\Bigl(\frac{1}{a_{n-1}}+\frac{1}{a_{n-2}}\Bigr)=\frac{2}{g} g=\sqrt{2} g=-\sqrt{2} a_n>0 g=\sqrt{2} 1\le a_n\le2 a_4 \gt a_5 \lt a_6 N\in\mathbb N","['real-analysis', 'sequences-and-series', 'limits', 'recurrence-relations']"
96,A curious algebraic fraction that converges to $\frac{\sqrt{2}}{2}$,A curious algebraic fraction that converges to,\frac{\sqrt{2}}{2},"I have noticed that the  algebraic fraction $\frac{3a+2b}{4a+3b} $ Gives better and better approximations to $\sin  45^\circ = \frac{\sqrt{2}}{2} $ For $ a = b = 1$ we get $5/7 \approx  0.714 $ Now, taking $ a = 5, b = 7$, we get $ 29/41 \approx 0.707$ All the times, take $ a_{n+1} = 3a_n + 2b_n$ $ b_{n+1} = 4a_n+3b_n$ And this process converges to $\frac{\sqrt{2}}{2} $ I have arrived at this result by a casual consideration. When I take the approximation $5/7$, I wonder what happens if I rewrite the numbers $5$ and $7$ as a function of two quantities $a, b$  in order to get a better approximation by successive iterations, in which the first approximation is the case $ a = b = 1$. Thus I have written $ 5 = 3a + 2b$ and the number $7 = 4a+3b$ I have selected both numerator and denominator to be consecutive numbers, and surprisingly this gives better approximations to $\frac{\sqrt{2}}{2}$ The question now is: how to prove that, in fact, this  algebraic fraction, whit the described iteration above, converges to the desired value?","I have noticed that the  algebraic fraction $\frac{3a+2b}{4a+3b} $ Gives better and better approximations to $\sin  45^\circ = \frac{\sqrt{2}}{2} $ For $ a = b = 1$ we get $5/7 \approx  0.714 $ Now, taking $ a = 5, b = 7$, we get $ 29/41 \approx 0.707$ All the times, take $ a_{n+1} = 3a_n + 2b_n$ $ b_{n+1} = 4a_n+3b_n$ And this process converges to $\frac{\sqrt{2}}{2} $ I have arrived at this result by a casual consideration. When I take the approximation $5/7$, I wonder what happens if I rewrite the numbers $5$ and $7$ as a function of two quantities $a, b$  in order to get a better approximation by successive iterations, in which the first approximation is the case $ a = b = 1$. Thus I have written $ 5 = 3a + 2b$ and the number $7 = 4a+3b$ I have selected both numerator and denominator to be consecutive numbers, and surprisingly this gives better approximations to $\frac{\sqrt{2}}{2}$ The question now is: how to prove that, in fact, this  algebraic fraction, whit the described iteration above, converges to the desired value?",,"['real-analysis', 'limits']"
97,Dense set in the unit circle- reference needed,Dense set in the unit circle- reference needed,,"For $x \notin \pi\mathbb Q$, that is, a real $x$ that is not a rational multiple of $\pi$, consider the set $$\{(\cos nx,\sin nx):n = 0,1,2,...\}.$$ It is known that this set is dense in the unit circle $B(0,1)$ of $\mathbb R^2$. Could someone please give me a proof or reference for a proof?","For $x \notin \pi\mathbb Q$, that is, a real $x$ that is not a rational multiple of $\pi$, consider the set $$\{(\cos nx,\sin nx):n = 0,1,2,...\}.$$ It is known that this set is dense in the unit circle $B(0,1)$ of $\mathbb R^2$. Could someone please give me a proof or reference for a proof?",,"['real-analysis', 'functions', 'reference-request', 'dynamical-systems', 'irrational-numbers']"
98,Why do we consider Borel sets instead of (Lebesgue) measurable sets?,Why do we consider Borel sets instead of (Lebesgue) measurable sets?,,"Dumb/ Challenging conventional wisdom question possibly related to my previous question . Why do we sometimes consider a measure space $(S, \Sigma, \mu) = (\mathbb{R}, \mathscr{B}(\mathbb{R}), \lambda)$ where $\lambda$ is Lebesgue measure rather than $(S, \Sigma, \mu) = (\mathbb{R}, \mathscr{M}(\mathbb{R}), \lambda)$ where $\mathscr{M}(\mathbb{R})$ is the set of $\lambda$ -measurable subsets of $\mathbb{R}$ ? I mean, there are subsets of $\mathbb{R}$ that are not Borel sets but $\lambda$ -measurable right ? If there are none, I guess that answers the first question. Possibly answered by above but why, in my previous question , is it 'natural' to consider $\mathscr{F}$ ? I'm guessing it's like why it's 'natural' to consider $\mathscr{B}(\mathbb{R})$ . Possibly related: Why do probabilists take random variables to be Borel (and not Lebesgue) measurable?","Dumb/ Challenging conventional wisdom question possibly related to my previous question . Why do we sometimes consider a measure space where is Lebesgue measure rather than where is the set of -measurable subsets of ? I mean, there are subsets of that are not Borel sets but -measurable right ? If there are none, I guess that answers the first question. Possibly answered by above but why, in my previous question , is it 'natural' to consider ? I'm guessing it's like why it's 'natural' to consider . Possibly related: Why do probabilists take random variables to be Borel (and not Lebesgue) measurable?","(S, \Sigma, \mu) = (\mathbb{R}, \mathscr{B}(\mathbb{R}), \lambda) \lambda (S, \Sigma, \mu) = (\mathbb{R}, \mathscr{M}(\mathbb{R}), \lambda) \mathscr{M}(\mathbb{R}) \lambda \mathbb{R} \mathbb{R} \lambda \mathscr{F} \mathscr{B}(\mathbb{R})","['real-analysis', 'probability-theory', 'measure-theory', 'lebesgue-measure', 'borel-sets']"
99,$0.101001000100001000001$... is irrational. [duplicate],... is irrational. [duplicate],0.101001000100001000001,"This question already has answers here : Is $0.1010010001000010000010000001 \ldots$ transcendental? (1 answer) Prove that 2.101001000100001... is an irrational number. (5 answers) Closed 8 years ago . How do I show that $0.101001000100001000001...$ is irrational? How do I generalize this to decimal expansions of a similar type, i.e. what is a criterion for decimal expansions with $0$'s and $1$'s involving long gaps of zeros and which guarantees irrationality?","This question already has answers here : Is $0.1010010001000010000010000001 \ldots$ transcendental? (1 answer) Prove that 2.101001000100001... is an irrational number. (5 answers) Closed 8 years ago . How do I show that $0.101001000100001000001...$ is irrational? How do I generalize this to decimal expansions of a similar type, i.e. what is a criterion for decimal expansions with $0$'s and $1$'s involving long gaps of zeros and which guarantees irrationality?",,['real-analysis']
