,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Chebyshev's Theorem regarding real polynomials: Why do only the Chebyshev polynomials achieve equality in this inequality?,Chebyshev's Theorem regarding real polynomials: Why do only the Chebyshev polynomials achieve equality in this inequality?,,In the book Proofs from The Book by Aigner and Ziegler there is a proof of 'Chebyshev's Theorem' which states that if $p(x)$ is a real polynomial of degree n with leading coefficient $1$ then $$ \max_{-1 \leq x \leq 1} |p(x)| \geq \frac{1}{2^{n-1}}$$ In the proof $g( \theta ) = p( \cos \theta )$ is written as a cosine polynomial $ \sum_{k=0}^n \lambda_k \cos(k \theta)$ and it is then proved that this can't be less than $|\lambda_n|$ (which happens to be $\frac{1}{2^{n-1}}$) everywhere. Afterwards it is claimed that 'The reader can easily complete the analysis' to show that the Chebyshev polynomials are the only ones for which equality occurs in the above inequality. I haven't been able to figure this out. Can someone explain why this is true? (preferably building on the proof as it was done in the book or by some other simple argument.) edit: here by Chebyshev polynomial I actually mean the monic polynomial that you get after dividing the $n$-th Chebyshev polynomial by $2^{n-1}$.,In the book Proofs from The Book by Aigner and Ziegler there is a proof of 'Chebyshev's Theorem' which states that if $p(x)$ is a real polynomial of degree n with leading coefficient $1$ then $$ \max_{-1 \leq x \leq 1} |p(x)| \geq \frac{1}{2^{n-1}}$$ In the proof $g( \theta ) = p( \cos \theta )$ is written as a cosine polynomial $ \sum_{k=0}^n \lambda_k \cos(k \theta)$ and it is then proved that this can't be less than $|\lambda_n|$ (which happens to be $\frac{1}{2^{n-1}}$) everywhere. Afterwards it is claimed that 'The reader can easily complete the analysis' to show that the Chebyshev polynomials are the only ones for which equality occurs in the above inequality. I haven't been able to figure this out. Can someone explain why this is true? (preferably building on the proof as it was done in the book or by some other simple argument.) edit: here by Chebyshev polynomial I actually mean the monic polynomial that you get after dividing the $n$-th Chebyshev polynomial by $2^{n-1}$.,,"['real-analysis', 'polynomials', 'chebyshev-polynomials']"
1,"limsup of the product of two sequences, of which one converges","limsup of the product of two sequences, of which one converges",,"Let $\{a_n\}_{n=1 }^{\infty}$ and $\{b_n\}_{n=1}^{\infty} $ be two sequences in $\mathbb{R}$, with the first sequence convergent . Prove that   $$ \limsup\limits_{n\to \infty}  a_n b_n =\lim\limits_{n\to\infty} a_n  \limsup\limits_{n\to\infty} b_n$$ I tried following: $ \limsup\limits_{n\to \infty}  a_n b_n \leq \limsup\limits_{n\to\infty} a_n  \limsup\limits_{n\to\infty} b_n$. Since $\{a_n\}$ is convergent, $\limsup\limits_{n\to\infty} a_n =\lim\limits_{n\to\infty} a_n $ gives one  inequality along with one of  the property of the limsup of the product of two sequences i.e  $ \limsup\limits_{n\to \infty}  a_n b_n \leq \lim\limits_{n\to\infty} a_n  \limsup\limits_{n\to\infty} b_n$. I want to prove  $ \limsup\limits_{n\to \infty}  a_n b_n \geq \limsup\limits_{n\to\infty} a_n  \limsup\limits_{n\to\infty} b_n$. Can anyone help me on this?","Let $\{a_n\}_{n=1 }^{\infty}$ and $\{b_n\}_{n=1}^{\infty} $ be two sequences in $\mathbb{R}$, with the first sequence convergent . Prove that   $$ \limsup\limits_{n\to \infty}  a_n b_n =\lim\limits_{n\to\infty} a_n  \limsup\limits_{n\to\infty} b_n$$ I tried following: $ \limsup\limits_{n\to \infty}  a_n b_n \leq \limsup\limits_{n\to\infty} a_n  \limsup\limits_{n\to\infty} b_n$. Since $\{a_n\}$ is convergent, $\limsup\limits_{n\to\infty} a_n =\lim\limits_{n\to\infty} a_n $ gives one  inequality along with one of  the property of the limsup of the product of two sequences i.e  $ \limsup\limits_{n\to \infty}  a_n b_n \leq \lim\limits_{n\to\infty} a_n  \limsup\limits_{n\to\infty} b_n$. I want to prove  $ \limsup\limits_{n\to \infty}  a_n b_n \geq \limsup\limits_{n\to\infty} a_n  \limsup\limits_{n\to\infty} b_n$. Can anyone help me on this?",,"['calculus', 'real-analysis']"
2,Real-analytic $f(z)=f\left(\sqrt z\right) + f\left(-\sqrt z\right)$?,Real-analytic ?,f(z)=f\left(\sqrt z\right) + f\left(-\sqrt z\right),"Are there nonconstant real-analytic functions $f(z)$ such that $$ f(z)=f\left(\sqrt z\right) + f\left(-\sqrt z\right)$$ is satisfied near the real line? Also can such functions be entire? And/Or can they be periodic with a real period $p>0$ ? Does the set of equations $$ f(z)=f\left(\sqrt z\right) + f\left(-\sqrt z\right)$$ $$ f(z)=f(z+p)$$ $$ f ' (0) > 0$$ imply that $f(z)= 0 + a_1 z + a_2 z^2 + a_3 z^3 + \dots$ , where more than $50\%$ of the nonzero signs of the $a_n$ are positive? Related: Real-analytic periodic $f(z)$ that has more than 50 % of the derivatives positive?","Are there nonconstant real-analytic functions such that is satisfied near the real line? Also can such functions be entire? And/Or can they be periodic with a real period ? Does the set of equations imply that , where more than of the nonzero signs of the are positive? Related: Real-analytic periodic $f(z)$ that has more than 50 % of the derivatives positive?",f(z)  f(z)=f\left(\sqrt z\right) + f\left(-\sqrt z\right) p>0  f(z)=f\left(\sqrt z\right) + f\left(-\sqrt z\right)  f(z)=f(z+p)  f ' (0) > 0 f(z)= 0 + a_1 z + a_2 z^2 + a_3 z^3 + \dots 50\% a_n,"['real-analysis', 'power-series', 'functional-equations', 'analyticity']"
3,Construct a function that is nowhere differentiable.,Construct a function that is nowhere differentiable.,,"I have been working on this question for a very long time now and seem to have reached a dead end, I will show all my attempted solutions, and any help on the various parts of the question would be much appreciated. Now for Part $(a)$, the graphs I got are the following ; $f_{0}(x)=\sum_{r=0}^{0}2^{-r}K(2^{r}x)$: $f_{1}(x)=\sum_{r=0}^{1}2^{-r}K(2^{r}x)$ $f_{2}(x)=\sum_{r=0}^{2}2^{-r}K(2^{r}x)$ $f_{3}(x)=\sum_{r=0}^{3}2^{-r}K(2^{r}x)$ $f_{10}(x)=\sum_{r=0}^{10}2^{-r}K(2^{r}x)$ Now it is obvious to see that the larger $n$ become the more complicated and horrible looking $f_{n}$ becomes, and to me it looks very discontinuous, even though I know its continuous, For $(b)$ I feel quite lost as to how to approach this, I tried to decipher the hint , now if $f_{n_{0}}$ is continuous, by definition For all $\epsilon>0$ $\exists$ $\delta>0$ such that $|x-c|<\delta$ implies $|\sum_{r=0}^{n_{0}}2^{-r}K(2^{r}x)-\sum_{r=0}^{n_{0}}2^{-r}K(2^{r}c)|<\epsilon$ Now I don't know how to go further to incorporate the hint.","I have been working on this question for a very long time now and seem to have reached a dead end, I will show all my attempted solutions, and any help on the various parts of the question would be much appreciated. Now for Part $(a)$, the graphs I got are the following ; $f_{0}(x)=\sum_{r=0}^{0}2^{-r}K(2^{r}x)$: $f_{1}(x)=\sum_{r=0}^{1}2^{-r}K(2^{r}x)$ $f_{2}(x)=\sum_{r=0}^{2}2^{-r}K(2^{r}x)$ $f_{3}(x)=\sum_{r=0}^{3}2^{-r}K(2^{r}x)$ $f_{10}(x)=\sum_{r=0}^{10}2^{-r}K(2^{r}x)$ Now it is obvious to see that the larger $n$ become the more complicated and horrible looking $f_{n}$ becomes, and to me it looks very discontinuous, even though I know its continuous, For $(b)$ I feel quite lost as to how to approach this, I tried to decipher the hint , now if $f_{n_{0}}$ is continuous, by definition For all $\epsilon>0$ $\exists$ $\delta>0$ such that $|x-c|<\delta$ implies $|\sum_{r=0}^{n_{0}}2^{-r}K(2^{r}x)-\sum_{r=0}^{n_{0}}2^{-r}K(2^{r}c)|<\epsilon$ Now I don't know how to go further to incorporate the hint.",,"['real-analysis', 'analysis', 'derivatives', 'continuity']"
4,Infinite Series $\sum\limits_{n=2}^{\infty}\frac{(-1)^n}{n^k}\zeta(n)$,Infinite Series,\sum\limits_{n=2}^{\infty}\frac{(-1)^n}{n^k}\zeta(n),"We can prove that  $$\sum_{n=2}^{\infty}\frac{(-1)^n}{n}\zeta(n)=\gamma$$ In fact, If we let $f(z)=\sum_{m=2}^\infty\frac{(-1)^m}m z^m$, then by the method which used in this question , $$\sum_{n=2}^{\infty}\frac{(-1)^n}{n}\zeta(n)=\sum_{n=1}^nf\left(\frac1 n\right)=\sum_{n=1}^\infty\sum_{m=2}^\infty\frac{(-1)^m}{mn^m}=\sum_{n=1}^\infty\left(\frac1 n+\log\left(1-\frac1 n\right)\right)=\gamma$$ Is there any known value for $\displaystyle \sum_{n=2}^{\infty}\frac{(-1)^n}{n^k}\zeta(n)$ for every natural number $k\ge2$? What is the best result?","We can prove that  $$\sum_{n=2}^{\infty}\frac{(-1)^n}{n}\zeta(n)=\gamma$$ In fact, If we let $f(z)=\sum_{m=2}^\infty\frac{(-1)^m}m z^m$, then by the method which used in this question , $$\sum_{n=2}^{\infty}\frac{(-1)^n}{n}\zeta(n)=\sum_{n=1}^nf\left(\frac1 n\right)=\sum_{n=1}^\infty\sum_{m=2}^\infty\frac{(-1)^m}{mn^m}=\sum_{n=1}^\infty\left(\frac1 n+\log\left(1-\frac1 n\right)\right)=\gamma$$ Is there any known value for $\displaystyle \sum_{n=2}^{\infty}\frac{(-1)^n}{n^k}\zeta(n)$ for every natural number $k\ge2$? What is the best result?",,"['real-analysis', 'sequences-and-series', 'complex-analysis', 'closed-form', 'riemann-zeta']"
5,how can we convert sin function into continued fraction?,how can we convert sin function into continued fraction?,,"how can we convert sin function into continued fraction ? for example http://mathworld.wolfram.com/EulersContinuedFraction.html how can we convert sin to simmilar continued fraction ?? and what about sinh and cosh ? arcsin ? arctan ? cos ? arccos ?? in general , how can convert any function to continued fraction ??? my friend asked me this question , so i hope that you help me to be enabled to help him thanx for all of you","how can we convert sin function into continued fraction ? for example http://mathworld.wolfram.com/EulersContinuedFraction.html how can we convert sin to simmilar continued fraction ?? and what about sinh and cosh ? arcsin ? arctan ? cos ? arccos ?? in general , how can convert any function to continued fraction ??? my friend asked me this question , so i hope that you help me to be enabled to help him thanx for all of you",,"['calculus', 'real-analysis', 'functions']"
6,Prove that the normed space $L^{\infty}$ equipped with $\lVert\cdot\rVert_{\infty}$ is complete. [duplicate],Prove that the normed space  equipped with  is complete. [duplicate],L^{\infty} \lVert\cdot\rVert_{\infty},"This question already has an answer here : Closed 12 years ago . Possible Duplicate: Understanding proof of completeness of $L^{\infty}$ Most of the materials I have in Real Analysis consider this statement as a trivial one: ""The normed space $L^{\infty}$ equipped with $\lVert\cdot\rVert_{\infty}$ is  complete"". But to my suprise I can't see the triviality. I am searching for it now... Anybody with hint?","This question already has an answer here : Closed 12 years ago . Possible Duplicate: Understanding proof of completeness of $L^{\infty}$ Most of the materials I have in Real Analysis consider this statement as a trivial one: ""The normed space $L^{\infty}$ equipped with $\lVert\cdot\rVert_{\infty}$ is  complete"". But to my suprise I can't see the triviality. I am searching for it now... Anybody with hint?",,"['real-analysis', 'measure-theory', 'banach-spaces']"
7,A second proof of the fact that a linear functional $f$ on a normed vector space $\mathcal{X}$ is bounded iff $f^{-1}(0)$ is closed.,A second proof of the fact that a linear functional  on a normed vector space  is bounded iff  is closed.,f \mathcal{X} f^{-1}(0),"I am working through Folland, not for homework, and have come across this problem (Ch 5 #17): A linear functional $f$ on a normed vector space $\mathcal{X}$ is bounded iff $f^{-1}(0)$ is closed. Note the ""only if"" direction is trivial. Folland suggests that we use the following fact in our proof: for all $\epsilon>0$ there exists an $x\in\mathcal{X}$ such that $\|x\|=1$ and $\|x+\mathcal{M}\|\ge 1-\epsilon$, where $\mathcal{M}$ is a proper closed subspace of $\mathcal{X}$. I can prove both the hint, and the exercise, but not the latter using the former! My proof is to suppose (toward a contradiction), that there existed some sequence $\|x_n\|=1$ such that $f(x_n)>n$ for each $n\in\mathbb{N}$, and then consider the sequence $\{x-f(x)\frac{x_n}{f(x_n)}\} $ and observe that this is in the kernel of $f$ for each $n$ and approaches $x$ in the limit, from which the statement should follow since $x$ is arbitrary. Can anyone tell me how to use the hint? I'm guessing the closed subspace we consider is the kernel of $f$, but it's not clear how to go from there. Any help would be appreciated.","I am working through Folland, not for homework, and have come across this problem (Ch 5 #17): A linear functional $f$ on a normed vector space $\mathcal{X}$ is bounded iff $f^{-1}(0)$ is closed. Note the ""only if"" direction is trivial. Folland suggests that we use the following fact in our proof: for all $\epsilon>0$ there exists an $x\in\mathcal{X}$ such that $\|x\|=1$ and $\|x+\mathcal{M}\|\ge 1-\epsilon$, where $\mathcal{M}$ is a proper closed subspace of $\mathcal{X}$. I can prove both the hint, and the exercise, but not the latter using the former! My proof is to suppose (toward a contradiction), that there existed some sequence $\|x_n\|=1$ such that $f(x_n)>n$ for each $n\in\mathbb{N}$, and then consider the sequence $\{x-f(x)\frac{x_n}{f(x_n)}\} $ and observe that this is in the kernel of $f$ for each $n$ and approaches $x$ in the limit, from which the statement should follow since $x$ is arbitrary. Can anyone tell me how to use the hint? I'm guessing the closed subspace we consider is the kernel of $f$, but it's not clear how to go from there. Any help would be appreciated.",,['real-analysis']
8,TFAE: Completeness Axiom and Monotone Convergence Theorem,TFAE: Completeness Axiom and Monotone Convergence Theorem,,"I'm looking to write a proof that shows Completeness Axiom / L.U.B. Property of $\mathbb{R}$$\iff$ Monotone Convergence Theorem. However, it's quite perplexing to me how a proposition about real numbers can imply such an in-depth result about sequences and series. Could anyone offer me some starting points? I'm thinking by contradiction for $\Leftarrow$, but I have no idea how to do $\Rightarrow$.","I'm looking to write a proof that shows Completeness Axiom / L.U.B. Property of $\mathbb{R}$$\iff$ Monotone Convergence Theorem. However, it's quite perplexing to me how a proposition about real numbers can imply such an in-depth result about sequences and series. Could anyone offer me some starting points? I'm thinking by contradiction for $\Leftarrow$, but I have no idea how to do $\Rightarrow$.",,"['real-analysis', 'sequences-and-series', 'analysis']"
9,L'H么pital in several variables,L'H么pital in several variables,,I am wondering if there is a multidimensional analog of l'H么pital's rule for functions of several variables. I have searched online for a while and have found people that argue both sides. One said it was possible by replacing the derivative with the directional derivative but I didn't quite understand.,I am wondering if there is a multidimensional analog of l'H么pital's rule for functions of several variables. I have searched online for a while and have found people that argue both sides. One said it was possible by replacing the derivative with the directional derivative but I didn't quite understand.,,"['real-analysis', 'multivariable-calculus']"
10,If $f(1)=f(2)=0$ then exist $c$ such that $cf''(c)+2f'(c)=0$,If  then exist  such that,f(1)=f(2)=0 c cf''(c)+2f'(c)=0,"Problem. Let the function $f$ be continuous on $[0,2]$ , second-order differentiable on the interval $(0,2)$ and satisfy the condition $f(1)=f(2)=0$ . Prove that there exists a real number $c$ in $(0,2)$ such that $$ cf''(c)+2f'(c)=0 $$ My idea is use MVT to prove that $f'(c)=\frac{-f(0)}{2}$ and use idea from the problem If $f(0)=f(1)=f(2)=0$ , $\forall x, \exists c, f(x)=\frac{1}{6}x(x-1)(x-2)f'''(c)$ , but I stuck at proving it. Can someone help me please, thank you.","Problem. Let the function be continuous on , second-order differentiable on the interval and satisfy the condition . Prove that there exists a real number in such that My idea is use MVT to prove that and use idea from the problem If , , but I stuck at proving it. Can someone help me please, thank you.","f [0,2] (0,2) f(1)=f(2)=0 c (0,2)  cf''(c)+2f'(c)=0  f'(c)=\frac{-f(0)}{2} f(0)=f(1)=f(2)=0 \forall x, \exists c, f(x)=\frac{1}{6}x(x-1)(x-2)f'''(c)",['real-analysis']
11,Find the sum $\sum\limits_{k=0}^\infty (-2)^k\frac{k+2}{k+1}x^k$,Find the sum,\sum\limits_{k=0}^\infty (-2)^k\frac{k+2}{k+1}x^k,"$$\displaystyle\sum_{k=0}^\infty (-2)^k\dfrac{k+2}{k+1}x^k.$$ I showed that this series converges when $|x|<\dfrac{1}{2}$ because $$\displaystyle\lim_{k\to\infty}\left|\dfrac{a_{k+1}}{a_k}\right|=2|x|.$$ Now I have to find the sum result. I tried so far trying to combine $$\ln(x+1)=\displaystyle\sum_{k=0}^\infty \dfrac{(-1)^kx^{k+1}}{k+1}$$ and $$\arctan(x)=\displaystyle\sum_{k=0}^\infty \dfrac{(-1)^kx^{2k+1}}{2k+1},$$ and trying many substitutions with multiples of $x$ , $x^2$ and got close to the result, but $\dfrac{k+2}{k+1}$ confuses me. Any help would be appreciated.","I showed that this series converges when because Now I have to find the sum result. I tried so far trying to combine and and trying many substitutions with multiples of , and got close to the result, but confuses me. Any help would be appreciated.","\displaystyle\sum_{k=0}^\infty (-2)^k\dfrac{k+2}{k+1}x^k. |x|<\dfrac{1}{2} \displaystyle\lim_{k\to\infty}\left|\dfrac{a_{k+1}}{a_k}\right|=2|x|. \ln(x+1)=\displaystyle\sum_{k=0}^\infty \dfrac{(-1)^kx^{k+1}}{k+1} \arctan(x)=\displaystyle\sum_{k=0}^\infty \dfrac{(-1)^kx^{2k+1}}{2k+1}, x x^2 \dfrac{k+2}{k+1}","['real-analysis', 'calculus', 'sequences-and-series', 'power-series']"
12,"Integral $\lim _{n\to \infty}\int _0^1\sqrt{\frac{1}{x}+n^2x^{2n}}\,dx$",Integral,"\lim _{n\to \infty}\int _0^1\sqrt{\frac{1}{x}+n^2x^{2n}}\,dx","Evaluate $\displaystyle \lim \limits _{n\to \infty}\int \limits _0^1\sqrt{\frac{1}{x}+n^2x^{2n}}\,dx$ . The integral is non asymptotic. The convergence is non uniform at both $x=0$ and $x=1$ . I'm not sure how to proceed.",Evaluate . The integral is non asymptotic. The convergence is non uniform at both and . I'm not sure how to proceed.,"\displaystyle \lim \limits _{n\to \infty}\int \limits _0^1\sqrt{\frac{1}{x}+n^2x^{2n}}\,dx x=0 x=1","['real-analysis', 'calculus', 'integration', 'limits', 'measure-theory']"
13,"Does there exist a function $f_{\Box,\Box}(\Box)$ making the formula $a + (b \oplus c) = (f_{b,c}(a)+b) \oplus (f_{c,b}(a)+c)$ true?",Does there exist a function  making the formula  true?,"f_{\Box,\Box}(\Box) a + (b \oplus c) = (f_{b,c}(a)+b) \oplus (f_{c,b}(a)+c)","Let $a$ and $b$ denote the resistances of two resistors. If they're put in series, the total resistance is $a+b$ . If they're put in parallel, the total resistance is $$a \oplus b := \frac{1}{\frac{1}{a}+\frac{1}{b}} = \frac{ab}{a+b}.$$ I suspect there's a formula describing how $+$ ""distributes over"" $\oplus$ . It should be of the form $$a + (b \oplus c) = (f_{b,c}(a)+b) \oplus (f_{c,b}(a)+c)$$ for an appropriate choice of $f$ . I haven't been able to find an $f$ that works, however. Question. Does there exist an $f$ making the above formula true? If not, why not? Remark. Unpacking the definitions, we're looking for a function $f$ such that $$(ab+ac+bc)(f_{b,c}(a)+b + f_{c,b}(a)+c)$$ and $$(b+c)(f_{b,c}(a)+b) (f_{c,b}(a)+c)$$ are equal.","Let and denote the resistances of two resistors. If they're put in series, the total resistance is . If they're put in parallel, the total resistance is I suspect there's a formula describing how ""distributes over"" . It should be of the form for an appropriate choice of . I haven't been able to find an that works, however. Question. Does there exist an making the above formula true? If not, why not? Remark. Unpacking the definitions, we're looking for a function such that and are equal.","a b a+b a \oplus b := \frac{1}{\frac{1}{a}+\frac{1}{b}} = \frac{ab}{a+b}. + \oplus a + (b \oplus c) = (f_{b,c}(a)+b) \oplus (f_{c,b}(a)+c) f f f f (ab+ac+bc)(f_{b,c}(a)+b + f_{c,b}(a)+c) (b+c)(f_{b,c}(a)+b) (f_{c,b}(a)+c)","['real-analysis', 'algebra-precalculus', 'functional-equations', 'rational-functions']"
14,Is the sum of two irrational numbers almost always irrational?,Is the sum of two irrational numbers almost always irrational?,,"Clearly the sum of two irrational numbers is not necessarily irrational. But is it true that it is 'almost always' irrational, in the sense that $$\displaystyle\lim_{x\to\infty}\dfrac{\lambda(P\cap B(x))}{\lambda(R\cap B(x))}=1$$ where $\lambda$ is Lebesgue measure, $R\subset \mathbb{R}^2$ is the set of points with irrational coordinates and $P\subset R$ is the set of those points the sum of whose coordinates is irrational (and $B(x)$ the disc of radius $x$ centred at the origin)? And I guess the same question applies to transcendental numbers. Intuitively it seems true, but I don't know how one would prove this. If not, then there is the question of whether the limit exists and if it does what is it.","Clearly the sum of two irrational numbers is not necessarily irrational. But is it true that it is 'almost always' irrational, in the sense that where is Lebesgue measure, is the set of points with irrational coordinates and is the set of those points the sum of whose coordinates is irrational (and the disc of radius centred at the origin)? And I guess the same question applies to transcendental numbers. Intuitively it seems true, but I don't know how one would prove this. If not, then there is the question of whether the limit exists and if it does what is it.",\displaystyle\lim_{x\to\infty}\dfrac{\lambda(P\cap B(x))}{\lambda(R\cap B(x))}=1 \lambda R\subset \mathbb{R}^2 P\subset R B(x) x,"['real-analysis', 'number-theory', 'probability-theory']"
15,Why exactly limit in polar coordinates isn't sufficient to find the limit in two variables?,Why exactly limit in polar coordinates isn't sufficient to find the limit in two variables?,,"I'm currently facing the following problem, my math teacher told us that the following statement is true: $\lim_{x\to(a,b)} f(x)=L \iff \forall \theta \lim_{r\to0} f(a+r\cos\theta, b+r\sin\theta)=L$ And in an attempt to better understand why this is true, I researched here for related questions and attempted to prove this myself. But I failed in finding any proof and my research led me to believe that the statement is actually false. Here is a link to one of the questions: Limit $\frac{x^2y}{x^4+y^2}$ is found using polar coordinates but it is not supposed to exist. In this question, he shows what I think is a counterexample to the statement above: $$\lim_{(x,y)\to(0,0)}\frac{x^2y}{x^4+y^2}$$ in which using polar coordinates to find the limit returns $0$ regardless of the value of $\theta$ . But by making $y=x^2$ the limit is equal to $1/2$ . Therefore, the limit doesn't exists. The intuitive explanation for this, given in the accepted answer is that using polar coordinates only checks the limit in the directions of straight lines. But that explanation doesn't satisfy me at all, I still can't explain why checking the limit in every straight direction isn't sufficient. I feel that if I could come up with counterexample on my own that would mean I truly understand. So here are my first questions: Could you build your own counterexamples? And how would/did you go about doing this? Are there any counterexamples you know? Am I really correct in assuming that the statement is actually false? Since the polar limit checks in every straight direction, the following statement is true? $\forall y\in\mathbb{R}^2\lim_{t\to0} f((a,b)+ty)=L \iff \forall \theta \lim_{r\to0} f(a+r\cos\theta, b+r\sin\theta)=L$ Don't know if that is particularly useful.","I'm currently facing the following problem, my math teacher told us that the following statement is true: And in an attempt to better understand why this is true, I researched here for related questions and attempted to prove this myself. But I failed in finding any proof and my research led me to believe that the statement is actually false. Here is a link to one of the questions: Limit is found using polar coordinates but it is not supposed to exist. In this question, he shows what I think is a counterexample to the statement above: in which using polar coordinates to find the limit returns regardless of the value of . But by making the limit is equal to . Therefore, the limit doesn't exists. The intuitive explanation for this, given in the accepted answer is that using polar coordinates only checks the limit in the directions of straight lines. But that explanation doesn't satisfy me at all, I still can't explain why checking the limit in every straight direction isn't sufficient. I feel that if I could come up with counterexample on my own that would mean I truly understand. So here are my first questions: Could you build your own counterexamples? And how would/did you go about doing this? Are there any counterexamples you know? Am I really correct in assuming that the statement is actually false? Since the polar limit checks in every straight direction, the following statement is true? Don't know if that is particularly useful.","\lim_{x\to(a,b)} f(x)=L \iff \forall \theta \lim_{r\to0} f(a+r\cos\theta, b+r\sin\theta)=L \frac{x^2y}{x^4+y^2} \lim_{(x,y)\to(0,0)}\frac{x^2y}{x^4+y^2} 0 \theta y=x^2 1/2 \forall y\in\mathbb{R}^2\lim_{t\to0} f((a,b)+ty)=L \iff \forall \theta \lim_{r\to0} f(a+r\cos\theta, b+r\sin\theta)=L","['real-analysis', 'limits', 'multivariable-calculus', 'polar-coordinates']"
16,"Prove $\lim\limits_{h\to0}\int_a^b\left|f(x+h)-f(x)\right|\,\mathrm dx=0$",Prove,"\lim\limits_{h\to0}\int_a^b\left|f(x+h)-f(x)\right|\,\mathrm dx=0","Let $f(x)$ be a Riemann integrable function on any finite interval $I\subset\mathbb{R}$. Supposing $f(x)=0$ for $x\notin[a,b]$, show that $$\lim\limits_{h\to0}\int_a^b\left|f(x+h)-f(x)\right|\,\mathrm{d}x=0.$$ How to prove that if not familiar with the fact that Riemann integrable function is almost everywhere continuous?","Let $f(x)$ be a Riemann integrable function on any finite interval $I\subset\mathbb{R}$. Supposing $f(x)=0$ for $x\notin[a,b]$, show that $$\lim\limits_{h\to0}\int_a^b\left|f(x+h)-f(x)\right|\,\mathrm{d}x=0.$$ How to prove that if not familiar with the fact that Riemann integrable function is almost everywhere continuous?",,"['real-analysis', 'riemann-integration']"
17,Is the intersection of $\sin(\mathbb{N})$ and $\cos(\mathbb{N})$ empty?,Is the intersection of  and  empty?,\sin(\mathbb{N}) \cos(\mathbb{N}),"My guess is that the intersection is empty and this is as far as I got in an attempt to prove this by contradiction: $\exists n,m \in \mathbb{N}, \cos(n)=\sin(m) \land n \neq m \quad (1)$ $\cos^2(n)=1-\cos^2(m) \iff \cos^2(n)+\cos^2(m)=1 \quad (2)$ I'm almost certain that the last equation can't be satisfied but I'm not sure how to proceed.","My guess is that the intersection is empty and this is as far as I got in an attempt to prove this by contradiction: $\exists n,m \in \mathbb{N}, \cos(n)=\sin(m) \land n \neq m \quad (1)$ $\cos^2(n)=1-\cos^2(m) \iff \cos^2(n)+\cos^2(m)=1 \quad (2)$ I'm almost certain that the last equation can't be satisfied but I'm not sure how to proceed.",,['real-analysis']
18,Can the limit of a sequence of bounded functions be unbounded?,Can the limit of a sequence of bounded functions be unbounded?,,"It is assumed that the sequence of functions converges and that each function is bounded, then can the limiting function be unbounded?","It is assumed that the sequence of functions converges and that each function is bounded, then can the limiting function be unbounded?",,['real-analysis']
19,The equivalence of definitions Riemann integral,The equivalence of definitions Riemann integral,,"First definition of Riemann integrable function. Let $f:[a,b] \to \mathbb{R}$ be a bounded function and $P=\{x_0,x_1,\dots, x_n\}$ partition of $[a,b]$. Define $U(P,f):=\sum \limits_{i=1}^{n}M_i\Delta x_i$ and $L(P,f):=\sum \limits_{i=1}^{n}m_i\Delta x_i$ where $M_i=\sup\limits_{[x_{i-1},x_i]} f(x),\quad m_i=\inf\limits_{[x_{i-1},x_i]} f(x), \Delta x_i=x_i-x_{i-1}.$ Let $\inf \limits_{P}U(P,f)=I^*$ and $\sup \limits_{P}L(P,f)=I_*$. If $I^*=I_*=I$ then we called $f(x)$ is Riemann integrable function on $[a,b]$ with integral $I$. Second definition of Riemann integrable function. Let $P=\{x_0,x_1,\dots, x_n\}$ is a partition of $[a,b]$ with $\xi_i\in [x_{i-1},x_i]$ and we define Riemann-integral sum $\sigma(P):=\sum \limits_{i=1}^{n}f(\xi_i)\Delta x_i$ and $\lVert P\rVert=\max\limits_{i}\Delta x_i$. If the following limit $\lim \limits_{\lVert P\rVert\to 0}\sigma(P)$ exists and has value $L$ we say that $f(x)$ is Riemann integrable function on $[a,b]$ with integral $L$. The first definition is from Rudin's PMA book but in other books I met the second definition. But I can't prove the equivalence of these definitions for a couple days. Can anyone show to me a strict and rigorous proof? I would be very grateful for your help! P.S. Happy New Year! :)","First definition of Riemann integrable function. Let $f:[a,b] \to \mathbb{R}$ be a bounded function and $P=\{x_0,x_1,\dots, x_n\}$ partition of $[a,b]$. Define $U(P,f):=\sum \limits_{i=1}^{n}M_i\Delta x_i$ and $L(P,f):=\sum \limits_{i=1}^{n}m_i\Delta x_i$ where $M_i=\sup\limits_{[x_{i-1},x_i]} f(x),\quad m_i=\inf\limits_{[x_{i-1},x_i]} f(x), \Delta x_i=x_i-x_{i-1}.$ Let $\inf \limits_{P}U(P,f)=I^*$ and $\sup \limits_{P}L(P,f)=I_*$. If $I^*=I_*=I$ then we called $f(x)$ is Riemann integrable function on $[a,b]$ with integral $I$. Second definition of Riemann integrable function. Let $P=\{x_0,x_1,\dots, x_n\}$ is a partition of $[a,b]$ with $\xi_i\in [x_{i-1},x_i]$ and we define Riemann-integral sum $\sigma(P):=\sum \limits_{i=1}^{n}f(\xi_i)\Delta x_i$ and $\lVert P\rVert=\max\limits_{i}\Delta x_i$. If the following limit $\lim \limits_{\lVert P\rVert\to 0}\sigma(P)$ exists and has value $L$ we say that $f(x)$ is Riemann integrable function on $[a,b]$ with integral $L$. The first definition is from Rudin's PMA book but in other books I met the second definition. But I can't prove the equivalence of these definitions for a couple days. Can anyone show to me a strict and rigorous proof? I would be very grateful for your help! P.S. Happy New Year! :)",,"['real-analysis', 'riemann-integration']"
20,I found only one critical point using Lagrange multipliers. Must it be a minimizer?,I found only one critical point using Lagrange multipliers. Must it be a minimizer?,,"I am trying to minimize $$V(x,y,z) = \frac {a^2b^2c^2}{6xyz}$$ subject to $$\frac {x^2}{a^2} + \frac{y^2}{b^2} + \frac {z^2}{c^2} = 1$$ and for $x,y,z>0$. I found one critical point; evaluating it gives a function value of $$V= \frac {\sqrt{3}}{2}|abc|$$ This agrees with the solution that I am looking at, but why does it have to be a minimum? Thanks,","I am trying to minimize $$V(x,y,z) = \frac {a^2b^2c^2}{6xyz}$$ subject to $$\frac {x^2}{a^2} + \frac{y^2}{b^2} + \frac {z^2}{c^2} = 1$$ and for $x,y,z>0$. I found one critical point; evaluating it gives a function value of $$V= \frac {\sqrt{3}}{2}|abc|$$ This agrees with the solution that I am looking at, but why does it have to be a minimum? Thanks,",,"['calculus', 'real-analysis', 'multivariable-calculus', 'optimization', 'lagrange-multiplier']"
21,Continuous function with compact domain has continuous inverse,Continuous function with compact domain has continuous inverse,,"Let $A,B\subset\mathbb R$ and $f:A\to B$ be an invertible function (so 1-1 and onto). Prove that if $A$ is compact and $f$ is continuous, then the inverse $f^{-1}:B\to A$ is continuous. And give a counterexample when $A$ is not compact (no transcendentals). If $f$ is continuous, then for every sequence $\{x_n\}$ in $A$ that converges to $L$, $\lim_{n\to\infty}f(x_n)=f(L)$. We need to prove that for every sequence $\{y_n\}$ in $B$ that converges to $K$, $\lim_{n\to\infty}f^{-1}(y_n)=f^{-1}(K)$ (Is this sufficient to prove that $f^{-1}$ is continuous?), I'm not sure how to proceed from there, and I'm having trouble coming up with a counterexample as well.","Let $A,B\subset\mathbb R$ and $f:A\to B$ be an invertible function (so 1-1 and onto). Prove that if $A$ is compact and $f$ is continuous, then the inverse $f^{-1}:B\to A$ is continuous. And give a counterexample when $A$ is not compact (no transcendentals). If $f$ is continuous, then for every sequence $\{x_n\}$ in $A$ that converges to $L$, $\lim_{n\to\infty}f(x_n)=f(L)$. We need to prove that for every sequence $\{y_n\}$ in $B$ that converges to $K$, $\lim_{n\to\infty}f^{-1}(y_n)=f^{-1}(K)$ (Is this sufficient to prove that $f^{-1}$ is continuous?), I'm not sure how to proceed from there, and I'm having trouble coming up with a counterexample as well.",,"['real-analysis', 'continuity']"
22,Prove that the Cartesian product of two topological manifolds is a topological manifold.,Prove that the Cartesian product of two topological manifolds is a topological manifold.,,"I need help on the following problem, any responses would be greatly appreciated: Let $M$ be a topological $m$-manifold and $N$ be a topological $n$-manifold. Prove that $M \times N$ is a topological $(m + n)$-manifold. I know how to prove that $M \times N$ is Hausdorff and that it has a countable base for its topology (namely the product topology). However, I am unsure on how to prove that it is locally Euclidean I.e. It has an open cover by sets that are homeomorphic to open subsets of $\mathbb{R}^{m+n}$.","I need help on the following problem, any responses would be greatly appreciated: Let $M$ be a topological $m$-manifold and $N$ be a topological $n$-manifold. Prove that $M \times N$ is a topological $(m + n)$-manifold. I know how to prove that $M \times N$ is Hausdorff and that it has a countable base for its topology (namely the product topology). However, I am unsure on how to prove that it is locally Euclidean I.e. It has an open cover by sets that are homeomorphic to open subsets of $\mathbb{R}^{m+n}$.",,"['real-analysis', 'general-topology', 'manifolds', 'euclidean-geometry']"
23,What is wrong with this proof of $0=1$?,What is wrong with this proof of ?,0=1,"I am trying to understand what is wrong with the proof posted here that $0=1$ ( source ): Given any $x$, we have (by using the substitution $u=x^2/y$) $$\large\int_0^1\frac{x^3}{y^2}e^{-x^2/y}\,dy=\Biggl[xe^{-x^2/y}\Biggr]_0^1=xe^{-x^2}.$$   Therefore, for all $x$,   \begin{align} e^{-x^2}(1-2x^2)&=\frac{d}{dx}(xe^{-x^2})\\[0.5em] &= \frac{d}{dx}\int_0^1\frac{x^3}{y^2}e^{-x^2/y}\,dy\\[0.5em] &= \int_0^1\frac{\partial}{\partial x}\Biggl(\frac{x^3}{y^2}e^{-x^2/y}\Biggr)\,dy\\[0.5em] &= \int_0^1 e^{-x^2/y}\Biggl(\frac{3x^2}{y^2}-\frac{2x^4}{y^3}\Biggr)\,dy. \end{align}   Now set $x=0$; the left-hand side is $e^0(1-0)=1$, but the right-hand side is $\int_0^1 0\,dy=0$. Surely it must be incorrect somewhere however I am not sure at which step it fails.","I am trying to understand what is wrong with the proof posted here that $0=1$ ( source ): Given any $x$, we have (by using the substitution $u=x^2/y$) $$\large\int_0^1\frac{x^3}{y^2}e^{-x^2/y}\,dy=\Biggl[xe^{-x^2/y}\Biggr]_0^1=xe^{-x^2}.$$   Therefore, for all $x$,   \begin{align} e^{-x^2}(1-2x^2)&=\frac{d}{dx}(xe^{-x^2})\\[0.5em] &= \frac{d}{dx}\int_0^1\frac{x^3}{y^2}e^{-x^2/y}\,dy\\[0.5em] &= \int_0^1\frac{\partial}{\partial x}\Biggl(\frac{x^3}{y^2}e^{-x^2/y}\Biggr)\,dy\\[0.5em] &= \int_0^1 e^{-x^2/y}\Biggl(\frac{3x^2}{y^2}-\frac{2x^4}{y^3}\Biggr)\,dy. \end{align}   Now set $x=0$; the left-hand side is $e^0(1-0)=1$, but the right-hand side is $\int_0^1 0\,dy=0$. Surely it must be incorrect somewhere however I am not sure at which step it fails.",,"['calculus', 'real-analysis', 'fake-proofs']"
24,Application of the mean value theorem to find $\lim_{n\to\infty} n(1 - \cos(1/n))$,Application of the mean value theorem to find,\lim_{n\to\infty} n(1 - \cos(1/n)),"While reading Heuser (2009) ""Lehrbuch der Analysis Teil I"" on page 286, I got this question: Find $$\lim\limits_{n \rightarrow \infty} n\Big(1 - \cos\Big(\frac{1}{n}\Big)\Big)$$ with the help of the Mean Value Theorem. How do you apply the Mean Value Theorem to this problem?","While reading Heuser (2009) ""Lehrbuch der Analysis Teil I"" on page 286, I got this question: Find $$\lim\limits_{n \rightarrow \infty} n\Big(1 - \cos\Big(\frac{1}{n}\Big)\Big)$$ with the help of the Mean Value Theorem. How do you apply the Mean Value Theorem to this problem?",,"['calculus', 'real-analysis', 'limits']"
25,"$\bigcup_{i=0}^\infty I_i $ not superset of $[0,1]$?",not superset of ?,"\bigcup_{i=0}^\infty I_i  [0,1]","Index all rational numbers in $[0,1]$ from 1 to infinity. For each rational number $q$ in $[0,1]$, form a set $I_i=[q-\frac\epsilon{2^i},q+\frac\epsilon{2^i}]$, where $\epsilon$ is a small number, maybe $\epsilon=0.01$. Now consider the set $$A=\bigcup_{i=0}^\infty I_i $$ Set $A$ seems to be a superset of $[0,1]$ because for any number $x\in[0,1]$, there is a rational number that is very close to it. So $x\in I_i$ for some $i$. But the measure of $A$ is $\epsilon$ and the measure of $[0,1]$ is $1$. Does this mean there must be numbers in $[0,1]$ but not $A$? If so, what are those numbers or how to construct them? Also why is the argument for $x\in I_i$ for some $i$ incorrect?","Index all rational numbers in $[0,1]$ from 1 to infinity. For each rational number $q$ in $[0,1]$, form a set $I_i=[q-\frac\epsilon{2^i},q+\frac\epsilon{2^i}]$, where $\epsilon$ is a small number, maybe $\epsilon=0.01$. Now consider the set $$A=\bigcup_{i=0}^\infty I_i $$ Set $A$ seems to be a superset of $[0,1]$ because for any number $x\in[0,1]$, there is a rational number that is very close to it. So $x\in I_i$ for some $i$. But the measure of $A$ is $\epsilon$ and the measure of $[0,1]$ is $1$. Does this mean there must be numbers in $[0,1]$ but not $A$? If so, what are those numbers or how to construct them? Also why is the argument for $x\in I_i$ for some $i$ incorrect?",,"['real-analysis', 'analysis']"
26,Formula for the series $f(x):=\sum\limits_{n=1}^\infty\frac{x}{x^2+n^2}$,Formula for the series,f(x):=\sum\limits_{n=1}^\infty\frac{x}{x^2+n^2},"Good evening! Recently I had to study the properties of a function $$f(x):=\sum\limits_{n=1}^\infty\displaystyle\frac{x}{x^2+n^2}.$$ I found its supremum and proved that $\lim\limits_{x\to+\infty}f(x)=\displaystyle\frac{\pi}{2}$. But as far as I remember such sums can be calculated explicitly. Maybe someone knows a quick hint to this? And in general: a lot of sums can be calculated explicitly by reducing to some Taylor and Laurent series, special functions, etc., and the methods are usually special for every particular problem. Is there any book, where a complete list of known examples can be found?","Good evening! Recently I had to study the properties of a function $$f(x):=\sum\limits_{n=1}^\infty\displaystyle\frac{x}{x^2+n^2}.$$ I found its supremum and proved that $\lim\limits_{x\to+\infty}f(x)=\displaystyle\frac{\pi}{2}$. But as far as I remember such sums can be calculated explicitly. Maybe someone knows a quick hint to this? And in general: a lot of sums can be calculated explicitly by reducing to some Taylor and Laurent series, special functions, etc., and the methods are usually special for every particular problem. Is there any book, where a complete list of known examples can be found?",,"['calculus', 'real-analysis', 'sequences-and-series', 'complex-analysis']"
27,A cute limit $\lim_{m\to\infty}\left(\left(\sum_{n=1}^{m}\frac{1}{n}\sum_{k=1}^{n-1}\frac{(-1)^k}{k}\right)+\log(2)H_m\right)$,A cute limit,\lim_{m\to\infty}\left(\left(\sum_{n=1}^{m}\frac{1}{n}\sum_{k=1}^{n-1}\frac{(-1)^k}{k}\right)+\log(2)H_m\right),"I'm sure that for many of you this is a limit pretty easy to compute, but my concern here is a bit different, and I'd like to know if I can nicely compute it without using special functions. Do you have in mind such ways? $$\lim_{m\to\infty}\left(\left(\sum_{n=1}^{m}\frac{1}{n}\sum_{k=1}^{n-1}\frac{(-1)^k}{k}\right)+\log(2)H_m\right)$$","I'm sure that for many of you this is a limit pretty easy to compute, but my concern here is a bit different, and I'd like to know if I can nicely compute it without using special functions. Do you have in mind such ways? $$\lim_{m\to\infty}\left(\left(\sum_{n=1}^{m}\frac{1}{n}\sum_{k=1}^{n-1}\frac{(-1)^k}{k}\right)+\log(2)H_m\right)$$",,"['calculus', 'real-analysis', 'integration', 'sequences-and-series', 'harmonic-numbers']"
28,Where is the error in my proof that all derivatives are continuous?,Where is the error in my proof that all derivatives are continuous?,,"I know that this can not be true due to counter-examples but I don't know where the error in my reasoning is. Assumption: If $f(x)$ is differentiable in $\mathbb{R}$ then the derivative $f'(x)$ is continuous in  $\mathbb{R}$. Faulty Proof: For every $c \in \mathbb{R}$, using the mean value theorem for $f(x),$ on the interval $x \in [c, c + h] $ where $h$ is positive. $$ \frac{f(c + h) - f(c)}{h} = f'(\xi(h)) $$ Where $\xi(h) \in (c,c+h)$. Because this equation holds for every $h>0$. It must hold in the limit as $h \rightarrow 0^+$. $$ \lim_{h\to 0^+}\frac{f(c + h) - f(c)}{h} = \lim_{h\to 0^+}f'(\xi(h))  $$ But the left side of the equation is the right one sided derivative. $$ f'_{+}(c) = \lim_{h\to 0^+}\frac{f(c + h) - f(c)}{h} = \lim_{h\to 0^+}f'(\xi(h))  $$ The same can be done for $h$ being negative, but because of differentiability at every point the left and right derivatives must be equal. $$ f'(c) = f'_{+}(c) = f'_{-}(c) = \lim_{h\to 0^-}\frac{f(c + h) - f(c)}{h} = \lim_{h\to 0^-}f'(\xi(h))  $$ As $h \rightarrow 0^+$, $\xi(h) \rightarrow c$. So because the limit $\lim_{h\to 0^+}f'(\xi(h))$ exists and $\xi(h) \neq c$, it is equal to $\lim_{x\to c^+}f'(x)$ It follows that $\lim_{x\to c^+}f'(x) = \lim_{x\to c^-}f'(x) = f'(c)$ so the function $f'(x)$ is continuous.","I know that this can not be true due to counter-examples but I don't know where the error in my reasoning is. Assumption: If $f(x)$ is differentiable in $\mathbb{R}$ then the derivative $f'(x)$ is continuous in  $\mathbb{R}$. Faulty Proof: For every $c \in \mathbb{R}$, using the mean value theorem for $f(x),$ on the interval $x \in [c, c + h] $ where $h$ is positive. $$ \frac{f(c + h) - f(c)}{h} = f'(\xi(h)) $$ Where $\xi(h) \in (c,c+h)$. Because this equation holds for every $h>0$. It must hold in the limit as $h \rightarrow 0^+$. $$ \lim_{h\to 0^+}\frac{f(c + h) - f(c)}{h} = \lim_{h\to 0^+}f'(\xi(h))  $$ But the left side of the equation is the right one sided derivative. $$ f'_{+}(c) = \lim_{h\to 0^+}\frac{f(c + h) - f(c)}{h} = \lim_{h\to 0^+}f'(\xi(h))  $$ The same can be done for $h$ being negative, but because of differentiability at every point the left and right derivatives must be equal. $$ f'(c) = f'_{+}(c) = f'_{-}(c) = \lim_{h\to 0^-}\frac{f(c + h) - f(c)}{h} = \lim_{h\to 0^-}f'(\xi(h))  $$ As $h \rightarrow 0^+$, $\xi(h) \rightarrow c$. So because the limit $\lim_{h\to 0^+}f'(\xi(h))$ exists and $\xi(h) \neq c$, it is equal to $\lim_{x\to c^+}f'(x)$ It follows that $\lim_{x\to c^+}f'(x) = \lim_{x\to c^-}f'(x) = f'(c)$ so the function $f'(x)$ is continuous.",,"['calculus', 'real-analysis', 'derivatives', 'continuity']"
29,Does the series $\sum\limits_{n=1}^{\infty }\left ( 1-\frac{\ln(n)}{n} \right )^{2n}$ diverge?,Does the series  diverge?,\sum\limits_{n=1}^{\infty }\left ( 1-\frac{\ln(n)}{n} \right )^{2n},"could anyone help me figure out whether this infinite series $$\sum_{n=1}^{\infty }\left ( 1-\frac{\ln(n)}{n} \right )^{2n}$$ diverges? I've tried using Cauchy's and d'Alembert's limit tests but both gave the result 1. I've also tried the necessary condition for convergence, but  $$\lim_{n\rightarrow \infty }\left ( 1-\frac{\ln(n)}{n} \right )^{2n}=0$$","could anyone help me figure out whether this infinite series $$\sum_{n=1}^{\infty }\left ( 1-\frac{\ln(n)}{n} \right )^{2n}$$ diverges? I've tried using Cauchy's and d'Alembert's limit tests but both gave the result 1. I've also tried the necessary condition for convergence, but  $$\lim_{n\rightarrow \infty }\left ( 1-\frac{\ln(n)}{n} \right )^{2n}=0$$",,"['real-analysis', 'sequences-and-series']"
30,Is a bounded (Riemannian) manifold always totally bounded?,Is a bounded (Riemannian) manifold always totally bounded?,,"Let $(M,g)$ be a finite dimensional Riemannian manifold. Suppose it is bounded as a metric space, i.e. $$\sup \{ d(x,y) | x,y \in M \} < \infty$$ where $d$ is the distance induced by $g$. Is $M$ also totally bounded ? I know bounded spaces are not totally bounded in general, but the only examples I know are in some infinite dimensional Banach space. I think the answer should be yes for manifolds, since by Nash Embedding Theorem they are (isometric to) subspaces of some Euclidean space, and bounded subspaces of $\mathbb{R}^n$ are also totally bounded. If this is true  (which assumes I correctly understand the statement of Nash Theorem), is there a more concrete proof?","Let $(M,g)$ be a finite dimensional Riemannian manifold. Suppose it is bounded as a metric space, i.e. $$\sup \{ d(x,y) | x,y \in M \} < \infty$$ where $d$ is the distance induced by $g$. Is $M$ also totally bounded ? I know bounded spaces are not totally bounded in general, but the only examples I know are in some infinite dimensional Banach space. I think the answer should be yes for manifolds, since by Nash Embedding Theorem they are (isometric to) subspaces of some Euclidean space, and bounded subspaces of $\mathbb{R}^n$ are also totally bounded. If this is true  (which assumes I correctly understand the statement of Nash Theorem), is there a more concrete proof?",,"['real-analysis', 'metric-spaces', 'manifolds', 'riemannian-geometry']"
31,Compare the integrals $\int_0^{\frac{\pi}{2}}\sin(\cos x)dx$ and $\int_0^{\frac{\pi}{2}}\cos(\sin x)dx$,Compare the integrals  and,\int_0^{\frac{\pi}{2}}\sin(\cos x)dx \int_0^{\frac{\pi}{2}}\cos(\sin x)dx,"Compare the following two integrals: $$\int_0^{\frac{\pi}{2}}\sin(\cos x)dx,\quad \int_0^{\frac{\pi}{2}}\cos(\sin x)dx$$ First I observe that by making the change of variable $x=\frac{\pi}{2}-x$,we have $$\int_0^{\frac{\pi}{2}}\sin(\cos x)dx=\int_0^{\frac{\pi}{2}}\sin(\sin x)dx$$ Then I consider the function $f(x)=\sin(\sin x)-\cos(\sin x)$,after some simplification we have $$f(x)=\frac{1}{\sqrt{2}}\sin(\sin x-\frac{\pi}{4})$$ Then I tried to determine the sign of $\int_0^{\frac{\pi}{2}}f(x)dx$ and I don't know how to proceed.","Compare the following two integrals: $$\int_0^{\frac{\pi}{2}}\sin(\cos x)dx,\quad \int_0^{\frac{\pi}{2}}\cos(\sin x)dx$$ First I observe that by making the change of variable $x=\frac{\pi}{2}-x$,we have $$\int_0^{\frac{\pi}{2}}\sin(\cos x)dx=\int_0^{\frac{\pi}{2}}\sin(\sin x)dx$$ Then I consider the function $f(x)=\sin(\sin x)-\cos(\sin x)$,after some simplification we have $$f(x)=\frac{1}{\sqrt{2}}\sin(\sin x-\frac{\pi}{4})$$ Then I tried to determine the sign of $\int_0^{\frac{\pi}{2}}f(x)dx$ and I don't know how to proceed.",,"['real-analysis', 'integration', 'inequality', 'definite-integrals']"
32,Show the usual Schwartz semi-norm is a norm on the Schwartz space,Show the usual Schwartz semi-norm is a norm on the Schwartz space,,"Let $f \in C^\infty(\mathbb R)$ . Define the semi-norm $$ \|f\|_{a,b}=\sup_{x \in \mathbb R} |x^af^{(b)}(x)| $$ where $a,b \in \mathbb Z_+$ , and $f^{(b)}$ is the $b$ -th derivative of $f$ . Show $\|\cdot\|_{a,b}$ is a norm on the Schwartz space $S(\mathbb R)$ . I don't see how to prove this direction of the nonnegativity requirement for the norm $$ \|f\|_{a,b} =0 \text{ implies } f=0. $$ If one of the derivative is zero, how can I infer that the original function is also zero?","Let . Define the semi-norm where , and is the -th derivative of . Show is a norm on the Schwartz space . I don't see how to prove this direction of the nonnegativity requirement for the norm If one of the derivative is zero, how can I infer that the original function is also zero?","f \in C^\infty(\mathbb R) 
\|f\|_{a,b}=\sup_{x \in \mathbb R} |x^af^{(b)}(x)|
 a,b \in \mathbb Z_+ f^{(b)} b f \|\cdot\|_{a,b} S(\mathbb R) 
\|f\|_{a,b} =0 \text{ implies } f=0.
","['real-analysis', 'analysis']"
33,Improper Integral Question: $ \int_0 ^ \infty\frac{x\log x}{(1+x^2)^2} dx $,Improper Integral Question:, \int_0 ^ \infty\frac{x\log x}{(1+x^2)^2} dx ,"I have to test the convergence of the integral : $$ \int_0 ^ \infty\frac{x\log x}{(1+x^2)^2} dx $$ Please suggest. Also, have to show that the value of the integral is zero ?","I have to test the convergence of the integral : $$ \int_0 ^ \infty\frac{x\log x}{(1+x^2)^2} dx $$ Please suggest. Also, have to show that the value of the integral is zero ?",,"['real-analysis', 'integration', 'improper-integrals']"
34,Does the converse of uniform continuity -> Preservance of Cauchy sequences hold?,Does the converse of uniform continuity -> Preservance of Cauchy sequences hold?,,"We know that if a function $f$ is uniformly continuous on an interval $I$ and $(x_n)$ is a Cauchy sequence in $I$, then $f(x_n)$ is a Cauchy sequence as well. Now, I would like to ask the following question: The function $g:(0,1) \rightarrow \mathbb{R}$ has the following property: for every Cauchy sequence $(x_n)$ in $(0,1)$, $(g(x_n))$ is also a Cauchy sequence. Prove that g is uniformly continuous on $(0,1)$. How do we go about doing it?","We know that if a function $f$ is uniformly continuous on an interval $I$ and $(x_n)$ is a Cauchy sequence in $I$, then $f(x_n)$ is a Cauchy sequence as well. Now, I would like to ask the following question: The function $g:(0,1) \rightarrow \mathbb{R}$ has the following property: for every Cauchy sequence $(x_n)$ in $(0,1)$, $(g(x_n))$ is also a Cauchy sequence. Prove that g is uniformly continuous on $(0,1)$. How do we go about doing it?",,['real-analysis']
35,Countability of disjoint intervals,Countability of disjoint intervals,,"According this problem/solution set from an MIT class (http://ocw.mit.edu/courses/mathematics/18-100c-analysis-i-spring-2006/exams/exam1_sol.pdf), the assertion: ""Every collection of disjoint intervals in R is countable."" is True, because ""every interval contains a rational number"", and the rationals are countable. It seems to me this should be False, with possible counterexample: { [x,x] | x is an element of R} ie the set of all singelton intervals on R. Why isn't this a valid counterexample?","According this problem/solution set from an MIT class (http://ocw.mit.edu/courses/mathematics/18-100c-analysis-i-spring-2006/exams/exam1_sol.pdf), the assertion: ""Every collection of disjoint intervals in R is countable."" is True, because ""every interval contains a rational number"", and the rationals are countable. It seems to me this should be False, with possible counterexample: { [x,x] | x is an element of R} ie the set of all singelton intervals on R. Why isn't this a valid counterexample?",,['real-analysis']
36,Proving a multivariate normal distribution gets the maximum entropy when mean and covariance are given,Proving a multivariate normal distribution gets the maximum entropy when mean and covariance are given,,"I'm working on a homework question. The first part was: Given an unbounded one dimensional continuous random variable: $X\in\left(-\infty,\infty\right)$ , that satisfies: $\left\langle X\right\rangle =\mu,\;\left\langle \left(X-\mu\right)^{2}\right\rangle =\sigma^{2}$ Show that the distribution that maximizes entropy is Gaussian $X\sim N\left(\mu,\sigma^{2}\right)$ . I've solved this using Lagrange multipliers method. The next part is proving the same holds in the case of multivariate distributions. Generalize the previous part to a $k$ dimensional variable $X$ with given expectation value $\vec{\mu}$ and covariance matrix $\Sigma$ . I started the same way when I define the proper functional I wish to optimize: $$ F\left[f_{X}\left(\overline{x}\right)\right]=H\left(X\right)+\lambda\left(1-\intop_{\mathbb{R}^{k}}f_{X}\left(\overline{x}\right)d\overline{x}\right)+\sum_{i\in\left[k\right]}\varGamma_{i}\left(\mu_{i}-\intop_{\mathbb{R}^{k}}\overline{x}_{i}f_{X}\left(\overline{x}\right)d\overline{x}\right)+\sum_{i,j\in\left[k\right]}\Lambda_{ij}\left(\Sigma_{ij}-\intop_{\mathbb{R}^{k}}\left(\mu_{i}-\overline{x}_{i}\right)\left(\mu_{j}-\overline{x}_{j}\right)f_{X}\left(\overline{x}\right)d\overline{x}\right)$$ After taking the functional derivative with regard to $f_X(\overline{x})$ and extracting the PDF, I get the following term with the Lagrange multipliers: $$f_{X}\left(\overline{x}\right)=\exp\left(\lambda-1\right)\exp\left(-\Gamma\cdot\overline{x}-\left(\vec{\mu}-\overline{x}\right)^{T}\Lambda\left(\vec{\mu}-\overline{x}\right)\right)$$ Where $\Gamma(k\times 1),\Lambda (k\times k),\lambda(1\times 1)$ are the multipliers. I wish to show that these multipliers must equal the correct terms for this PDF to be a multivariate Gaussian. For some reason this is where I get stuck, I've tried various algebraic manipulations, but the term $\exp{(-\Gamma\cdot\overline{x})}$ keeps messing up my calculations. I'm leaving out the constraints themselves since they are written within the optimization term, and they leave me in the mess with $\exp{(-\Gamma\cdot\overline{x})}$ when I try to solve the integrals. I feel like I'm missing something. Would really appreciate it! Edit: Badly enough the answers for this exercise just say - ""yeah this looks gaussian, so we can find the parameters so it works out"", albeit this isn't strictly a math course, I have a very hard time accepting this answer, so the question is highly relevant. There might be an easier way to solve this via KL divergence, but since the first part of the question was required for the next part I would really like to see this through. Another Edit: I get the calculations needed, my main issue is with the fact that the constraint $$\sum_{i\in\left[k\right]}\varGamma_{i}\left(\mu_{i}-\intop_{\mathbb{R}^{k}}\overline{x}_{i}f_{X}\left(\overline{x}\right)d\overline{x}\right)$$ is not needed, since when solving the equation I can assign $\Gamma=0$ and get the proper results. I would like a rigorous explanation why the mean is determined solely by the 3rd constraint.","I'm working on a homework question. The first part was: Given an unbounded one dimensional continuous random variable: , that satisfies: Show that the distribution that maximizes entropy is Gaussian . I've solved this using Lagrange multipliers method. The next part is proving the same holds in the case of multivariate distributions. Generalize the previous part to a dimensional variable with given expectation value and covariance matrix . I started the same way when I define the proper functional I wish to optimize: After taking the functional derivative with regard to and extracting the PDF, I get the following term with the Lagrange multipliers: Where are the multipliers. I wish to show that these multipliers must equal the correct terms for this PDF to be a multivariate Gaussian. For some reason this is where I get stuck, I've tried various algebraic manipulations, but the term keeps messing up my calculations. I'm leaving out the constraints themselves since they are written within the optimization term, and they leave me in the mess with when I try to solve the integrals. I feel like I'm missing something. Would really appreciate it! Edit: Badly enough the answers for this exercise just say - ""yeah this looks gaussian, so we can find the parameters so it works out"", albeit this isn't strictly a math course, I have a very hard time accepting this answer, so the question is highly relevant. There might be an easier way to solve this via KL divergence, but since the first part of the question was required for the next part I would really like to see this through. Another Edit: I get the calculations needed, my main issue is with the fact that the constraint is not needed, since when solving the equation I can assign and get the proper results. I would like a rigorous explanation why the mean is determined solely by the 3rd constraint.","X\in\left(-\infty,\infty\right) \left\langle X\right\rangle =\mu,\;\left\langle \left(X-\mu\right)^{2}\right\rangle =\sigma^{2} X\sim N\left(\mu,\sigma^{2}\right) k X \vec{\mu} \Sigma  F\left[f_{X}\left(\overline{x}\right)\right]=H\left(X\right)+\lambda\left(1-\intop_{\mathbb{R}^{k}}f_{X}\left(\overline{x}\right)d\overline{x}\right)+\sum_{i\in\left[k\right]}\varGamma_{i}\left(\mu_{i}-\intop_{\mathbb{R}^{k}}\overline{x}_{i}f_{X}\left(\overline{x}\right)d\overline{x}\right)+\sum_{i,j\in\left[k\right]}\Lambda_{ij}\left(\Sigma_{ij}-\intop_{\mathbb{R}^{k}}\left(\mu_{i}-\overline{x}_{i}\right)\left(\mu_{j}-\overline{x}_{j}\right)f_{X}\left(\overline{x}\right)d\overline{x}\right) f_X(\overline{x}) f_{X}\left(\overline{x}\right)=\exp\left(\lambda-1\right)\exp\left(-\Gamma\cdot\overline{x}-\left(\vec{\mu}-\overline{x}\right)^{T}\Lambda\left(\vec{\mu}-\overline{x}\right)\right) \Gamma(k\times 1),\Lambda (k\times k),\lambda(1\times 1) \exp{(-\Gamma\cdot\overline{x})} \exp{(-\Gamma\cdot\overline{x})} \sum_{i\in\left[k\right]}\varGamma_{i}\left(\mu_{i}-\intop_{\mathbb{R}^{k}}\overline{x}_{i}f_{X}\left(\overline{x}\right)d\overline{x}\right) \Gamma=0","['real-analysis', 'probability', 'statistics', 'optimization', 'information-theory']"
37,"Can Stolz-Cesaro theorem be applied to this problem? If $\lim\limits_{x\to\infty}(f(x+1)-f(x))=l$, Prove that $\lim\limits_{x\to\infty}\frac{f(x)}x=l$","Can Stolz-Cesaro theorem be applied to this problem? If , Prove that",\lim\limits_{x\to\infty}(f(x+1)-f(x))=l \lim\limits_{x\to\infty}\frac{f(x)}x=l,"If $f :(a , \infty ) \to \mathbb{R}$ and $f $ is bounded on every $(a,b)$ such that $a<b <\infty$ ,  prove that $\lim\limits_{x \to \infty }(f(x+1) - f(x))=l$ implies $\lim\limits_{x \to \infty  }\frac{f(x)}{x}=l$ . The first thing that came to my mind was Stolz-Cesro theorem case $\frac{*}{\infty}$ :- If $b_n $ is a monotone increasing sequence and $\lim \limits_{n \to \infty} b_n = \infty $ , and if $\lim \limits_{n \to \infty} \frac{a_{n+1}-a_n}{b_{n+1}- b_n}= l \in \overline{\mathbb{R}} $ , then $\lim \limits_{n \to \infty} \frac{a_n }{b_n}=l$ . Does this really solve this problem? There are uncountably many sub-sequences with the limit $l$ , i.e., there is a subsequence for all $x\in (a, a+1]$ with limit $l$ , but this doesn't imply that the sequence has the limit $l$ . One famous example where there are infinitely many sub-sequences with the same limit but the limit of the sequence doesn't exist is: $$a_n = \begin{cases} \frac{1}{r},  & \text{if $n$ is a power of prime $n = p^r \ : r\ge1$} \\[2ex] 0, & \text{if $n$ is not a power of prime  } \end{cases}$$ One can make infinitely many sub-sequences that converge to $0$ although $a_n $ doesn't converge to $0$ . After a lot of time thinking, I couldn't prove this problem, so I searched on MSE for a solution and found this , which gives a general proof for this problem. But my question is: Can we use the Stolz-Cesro theorem to solve this problem? If we can use the Stolz-Cesro theorem, how do we complete the proof?","If and is bounded on every such that ,  prove that implies . The first thing that came to my mind was Stolz-Cesro theorem case :- If is a monotone increasing sequence and , and if , then . Does this really solve this problem? There are uncountably many sub-sequences with the limit , i.e., there is a subsequence for all with limit , but this doesn't imply that the sequence has the limit . One famous example where there are infinitely many sub-sequences with the same limit but the limit of the sequence doesn't exist is: One can make infinitely many sub-sequences that converge to although doesn't converge to . After a lot of time thinking, I couldn't prove this problem, so I searched on MSE for a solution and found this , which gives a general proof for this problem. But my question is: Can we use the Stolz-Cesro theorem to solve this problem? If we can use the Stolz-Cesro theorem, how do we complete the proof?","f :(a , \infty ) \to \mathbb{R} f  (a,b) a<b <\infty \lim\limits_{x \to \infty }(f(x+1) - f(x))=l \lim\limits_{x \to \infty  }\frac{f(x)}{x}=l \frac{*}{\infty} b_n  \lim \limits_{n \to \infty} b_n = \infty  \lim \limits_{n \to \infty} \frac{a_{n+1}-a_n}{b_{n+1}- b_n}= l \in \overline{\mathbb{R}}  \lim \limits_{n \to \infty} \frac{a_n }{b_n}=l l x\in (a, a+1] l l a_n =
\begin{cases}
\frac{1}{r},  & \text{if n is a power of prime n = p^r \ : r\ge1} \\[2ex]
0, & \text{if n is not a power of prime  }
\end{cases} 0 a_n  0","['real-analysis', 'sequences-and-series', 'limits', 'analysis', 'alternative-proof']"
38,Solution to $u''(x)-V(x)u(x) = 0$ is identically zero,Solution to  is identically zero,u''(x)-V(x)u(x) = 0,"I am struggling to prove a property of a solution to the following ODE: Suppose $u\in \mathcal{C}([a,b])$ is twice continuously differentiable, $V\in\mathcal{C}([a,b])$ , $V(x)\geq 0$ $\forall x \in > [a,b]$ , and $$u''(x) - V(x)u(x)=0 \quad x\in[a,b],$$ $$u(a)=u(b)=0.$$ Then $u(x)=0$ for all $x$ in $[a,b]$ . I've tried a few different things to prove this. We know that $u$ is continuous on $[a,b]$ and since $u(a)=u(b)=0$ , then by Rolle's theorem $\exists c\in(a,b)$ such that $u'(c)=0.$ Then I would want to somehow prove that $u'(x)=0$ over the whole interval, meaning that $u$ is constant at 0, but I am not sure how to get there. Something else I noticed was that from the equation we have that $u''(a)=u''(b)=0$ , so by the fundamental theorem of calculus, $$0=f''(b)-f''(a)=\int_a^b f'''(x)\mathrm{d}x,$$ where by the product rule, $u'''(x) = V(x)u'(x) + V'(x)u(x)$ . Then I thought to proceed via integration by parts, but I didn't get anywhere that way either. Any help is appreciated.","I am struggling to prove a property of a solution to the following ODE: Suppose is twice continuously differentiable, , , and Then for all in . I've tried a few different things to prove this. We know that is continuous on and since , then by Rolle's theorem such that Then I would want to somehow prove that over the whole interval, meaning that is constant at 0, but I am not sure how to get there. Something else I noticed was that from the equation we have that , so by the fundamental theorem of calculus, where by the product rule, . Then I thought to proceed via integration by parts, but I didn't get anywhere that way either. Any help is appreciated.","u\in \mathcal{C}([a,b]) V\in\mathcal{C}([a,b]) V(x)\geq 0 \forall x \in
> [a,b] u''(x) - V(x)u(x)=0 \quad x\in[a,b], u(a)=u(b)=0. u(x)=0 x [a,b] u [a,b] u(a)=u(b)=0 \exists c\in(a,b) u'(c)=0. u'(x)=0 u u''(a)=u''(b)=0 0=f''(b)-f''(a)=\int_a^b f'''(x)\mathrm{d}x, u'''(x) = V(x)u'(x) + V'(x)u(x)","['real-analysis', 'ordinary-differential-equations']"
39,Prove that $\int_{0}^{2\pi}f(x)\cos(kx)dx \geq 0$ for every $k \geq 1$ given that $f$ is convex.,Prove that  for every  given that  is convex.,\int_{0}^{2\pi}f(x)\cos(kx)dx \geq 0 k \geq 1 f,"Given $f: [0, 2\pi] \to \mathbb{R}$ convex function, prove that for every $k\geq1$ \begin{align} \int_{0}^{2\pi}f(x) \cos (kx)dx \geq 0 \end{align} I am completely stumped. What I have tried to do is return the query for $k=1$ , and for that value of $k$ try to write the integral from $0$ to $2\pi$ as a sum of four integrals from $0$ to $\dfrac{\pi}{2}$ and use the the theorem for first derivative monotony . No luck so far. Any help would be much appreciated. Edit 1: I saw the link here about a similarly asked topic. However, this process gets the general case as I perceive it and I am really supposed to use the method described above. I will try this and come back with a definitive answer. Edit 2: I cannot use the $f''(x) \geq 0$ argument due to the simple fact that I have not been formally taught it as part of the class. Edit 3: Final proof, with thanks to the contributors below. Let's start by setting $A = \frac{\pi}{2}$ and $B = \frac{3\pi}{2}$ . It follows from basic trigonometry that: \begin{align} &\cos x \geq 0, \ x \in [A,B] \ \text{and}\\ &\cos x \leq 0, \ x \in [0, A] \cap [B, 2\pi]. \end{align} And we also set $L$ to be the line segment such that $L(A) = f(A), \ L(B) = f(B)$ . We will prove a basic property of said line in regards to the convex function $f$ . I can take for granted that (we proved this in class) \begin{align} L(x) = \dfrac{x-A}{B-A}f(B) + \dfrac{B-x}{B-A}f(A) \end{align} so for $x \in [A,B]$ there exists $\lambda \in [0,1]$ such that: $x = \lambda A + (1-\lambda) B$ . Taking the aforementioned expression and replacing it on $L(x)$ we get (I omit trivial algebra) \begin{align} L(x) = (1-\lambda) f(B) + \lambda f(A). \end{align} Since $f$ is convex, we can write \begin{align} &f(\lambda A + (1-\lambda) B) \leq \lambda f(A) + (1-\lambda) f(B)\\ \implies &f(x) \leq L(x), \ \forall \ x \in [A,B] \ \text{and} \ \lambda \in [0,1]. \end{align} $\blacksquare$ We assume that there exists $x \in [0,A]: \ f(x) < L(x)$ . Then there exists $A \in [x, B] \ \text{and} \ \lambda \in [0,1]: \ A = \lambda x + (1-\lambda) B$ . Then \begin{align} &L(A) = \dfrac{A-x}{B-x}f(B) + \dfrac{B-A}{B-x}f(x), \ A \in [x,B]\\ \implies &L(A) = (1-\lambda)L(B)+\lambda L(x)\\ \implies &L(A) = (1-\lambda)f(B) + \lambda L(x). \end{align} Then assuming that $L(x) > f(x)$ we get \begin{align} L(A) > (1-\lambda) f(B) + \lambda f(x) \geq f(A), \ \text{assuming convexity}. \end{align} Because $L(A) = f(A)$ the above inequality becomes $L(A) > f(A)$ which is a contradiction. $\blacksquare$ In the same spirit, for $x \in [B, 2\pi]$ doing the exact same replacements and applying the exact same principles we also get \begin{align} &f(\lambda B + (1-\lambda) 2 \pi) \leq \lambda f(B) + (1-\lambda) f(2\pi)\\ \implies &f(x) \leq L(x). \end{align} $\blacksquare$ We then set $g(x) = f(x) - L(x)$ and from (1) and (2) above it is safe to assume that $\cos x$ and $g(x)$ will have the same sign in the whole domain, that is: \begin{align} &g(x) \geq 0, \ \text{where} \ \cos x \geq 0\\ &g(x) \leq 0, \ \text{where} \ \cos x \geq 0. \end{align} We have \begin{align} \int_0^{2\pi} g(x) \cos x dx = \int_0^{\frac{\pi}{2}} g(x) \cos x dx + \int_{\frac{\pi}{2}}^{\frac{3 \pi}{2}} g(x) \cos x dx + \int_{\frac{3\pi}{2}}^{2\pi}g(x) \cos x dx \geq 0, \end{align} because \begin{align} &\text{in} \left[0, \frac{\pi}{2} \right], \ \cos x \geq 0 \implies g(x) \geq 0\\ &\text{in} \left[\frac{\pi}{2}, \frac{3 \pi}{2} \right], \ \cos x \leq 0 \implies g(x) \leq 0\\ &\text{in} \left[\frac{3 \pi}{2}, 2 \pi \right], \ \cos x \geq 0 \implies g(x) \geq 0.\\ \end{align} We have that \begin{align} &\int_0^{2 \pi}\cos x dx = 0 \ \text{trivial}\\ &\int_0^{2 \pi}x \cos x dx = \int_0^{2 \pi}x (\sin x)' dx = \left[ x \sin x \right]_0^{2\pi} - \int_0^{2 \pi} \sin x dx = 0. \end{align} It then follows that \begin{align} \int_0^{2 \pi} f(x) \cos x dx = \int_0^{2 \pi} g(x) \cos x dx \geq 0 \end{align} which was previously proven. For the case $k=1$ , the proof is over. For $k>1$ , we have: \begin{align} \int_0^{2\pi}f(x) \cos (kx) dx = \sum_{i=0}^{k-1} \int_{\frac{2\pi i}{k}}^{\frac{2\pi (i+1)}{k}} f(x) \cos (kx) dx. \end{align} We perform the change of variable \begin{align} x = \dfrac{y+2\pi i}{k}\\ \implies \begin{cases}dx = \dfrac{1}{k}dy\\ x = \dfrac{2\pi i}{k} \to y=0\\ x = \dfrac{2\pi (i+1)}{k} \to y = 2\pi \end{cases}. \end{align} So the above sum becomes \begin{align} \sum_{i=0}^{k-1} \dfrac{1}{k} \int_{0}^{2\pi}f \left(\dfrac{y+2 \pi i}{k} \right) \cos (y+2\pi i)dy. \end{align} Because $f$ is convex in $[0, 2\pi]$ there exists $\lambda \in [0,1]$ such that: \begin{align} 	\theta f\left(\frac{y_1 + 2\pi i}{k}\right) 	+ (1 - \theta)f\left(\frac{y_2 + 2\pi i}{k}\right) 	&\ge f\left(\theta\frac{y_1 + 2\pi i}{k} 	+ (1 - \theta) \frac{y_2 + 2\pi i}{k}\right)\\ 	&= f\left(\frac{\theta y_1 + (1 - \theta)y_2 + 2\pi i}{k}\right). \end{align} Having performed the change of variables: \begin{align} x_1 = \dfrac{y_1 + 2\pi i}{k} \ \text{and} \ x_2 = \dfrac{y_2 + 2\pi i}{k}. \end{align} So $f \left( \dfrac{y + 2\pi i}{k}\right)$ convex on $[0, 2\pi]$ . Using the result from $k=1$ we have \begin{align} \int_0^{2\pi} f \left( \dfrac{y + 2\pi i}{k} \right)\cos y dy \geq 0. \end{align} $\blacksquare$","Given convex function, prove that for every I am completely stumped. What I have tried to do is return the query for , and for that value of try to write the integral from to as a sum of four integrals from to and use the the theorem for first derivative monotony . No luck so far. Any help would be much appreciated. Edit 1: I saw the link here about a similarly asked topic. However, this process gets the general case as I perceive it and I am really supposed to use the method described above. I will try this and come back with a definitive answer. Edit 2: I cannot use the argument due to the simple fact that I have not been formally taught it as part of the class. Edit 3: Final proof, with thanks to the contributors below. Let's start by setting and . It follows from basic trigonometry that: And we also set to be the line segment such that . We will prove a basic property of said line in regards to the convex function . I can take for granted that (we proved this in class) so for there exists such that: . Taking the aforementioned expression and replacing it on we get (I omit trivial algebra) Since is convex, we can write We assume that there exists . Then there exists . Then Then assuming that we get Because the above inequality becomes which is a contradiction. In the same spirit, for doing the exact same replacements and applying the exact same principles we also get We then set and from (1) and (2) above it is safe to assume that and will have the same sign in the whole domain, that is: We have because We have that It then follows that which was previously proven. For the case , the proof is over. For , we have: We perform the change of variable So the above sum becomes Because is convex in there exists such that: Having performed the change of variables: So convex on . Using the result from we have","f: [0, 2\pi] \to \mathbb{R} k\geq1 \begin{align}
\int_{0}^{2\pi}f(x) \cos (kx)dx \geq 0
\end{align} k=1 k 0 2\pi 0 \dfrac{\pi}{2} f''(x) \geq 0 A = \frac{\pi}{2} B = \frac{3\pi}{2} \begin{align}
&\cos x \geq 0, \ x \in [A,B] \ \text{and}\\
&\cos x \leq 0, \ x \in [0, A] \cap [B, 2\pi].
\end{align} L L(A) = f(A), \ L(B) = f(B) f \begin{align}
L(x) = \dfrac{x-A}{B-A}f(B) + \dfrac{B-x}{B-A}f(A)
\end{align} x \in [A,B] \lambda \in [0,1] x = \lambda A + (1-\lambda) B L(x) \begin{align}
L(x) = (1-\lambda) f(B) + \lambda f(A).
\end{align} f \begin{align}
&f(\lambda A + (1-\lambda) B) \leq \lambda f(A) + (1-\lambda) f(B)\\
\implies &f(x) \leq L(x), \ \forall \ x \in [A,B] \ \text{and} \ \lambda \in [0,1].
\end{align} \blacksquare x \in [0,A]: \ f(x) < L(x) A \in [x, B] \ \text{and} \ \lambda \in [0,1]: \ A = \lambda x + (1-\lambda) B \begin{align}
&L(A) = \dfrac{A-x}{B-x}f(B) + \dfrac{B-A}{B-x}f(x), \ A \in [x,B]\\
\implies &L(A) = (1-\lambda)L(B)+\lambda L(x)\\
\implies &L(A) = (1-\lambda)f(B) + \lambda L(x).
\end{align} L(x) > f(x) \begin{align}
L(A) > (1-\lambda) f(B) + \lambda f(x) \geq f(A), \ \text{assuming convexity}.
\end{align} L(A) = f(A) L(A) > f(A) \blacksquare x \in [B, 2\pi] \begin{align}
&f(\lambda B + (1-\lambda) 2 \pi) \leq \lambda f(B) + (1-\lambda) f(2\pi)\\
\implies &f(x) \leq L(x).
\end{align} \blacksquare g(x) = f(x) - L(x) \cos x g(x) \begin{align}
&g(x) \geq 0, \ \text{where} \ \cos x \geq 0\\
&g(x) \leq 0, \ \text{where} \ \cos x \geq 0.
\end{align} \begin{align}
\int_0^{2\pi} g(x) \cos x dx = \int_0^{\frac{\pi}{2}} g(x) \cos x dx + \int_{\frac{\pi}{2}}^{\frac{3 \pi}{2}} g(x) \cos x dx + \int_{\frac{3\pi}{2}}^{2\pi}g(x) \cos x dx \geq 0,
\end{align} \begin{align}
&\text{in} \left[0, \frac{\pi}{2} \right], \ \cos x \geq 0 \implies g(x) \geq 0\\
&\text{in} \left[\frac{\pi}{2}, \frac{3 \pi}{2} \right], \ \cos x \leq 0 \implies g(x) \leq 0\\
&\text{in} \left[\frac{3 \pi}{2}, 2 \pi \right], \ \cos x \geq 0 \implies g(x) \geq 0.\\
\end{align} \begin{align}
&\int_0^{2 \pi}\cos x dx = 0 \ \text{trivial}\\
&\int_0^{2 \pi}x \cos x dx = \int_0^{2 \pi}x (\sin x)' dx = \left[ x \sin x \right]_0^{2\pi} - \int_0^{2 \pi} \sin x dx = 0.
\end{align} \begin{align}
\int_0^{2 \pi} f(x) \cos x dx = \int_0^{2 \pi} g(x) \cos x dx \geq 0
\end{align} k=1 k>1 \begin{align}
\int_0^{2\pi}f(x) \cos (kx) dx = \sum_{i=0}^{k-1} \int_{\frac{2\pi i}{k}}^{\frac{2\pi (i+1)}{k}} f(x) \cos (kx) dx.
\end{align} \begin{align}
x = \dfrac{y+2\pi i}{k}\\
\implies \begin{cases}dx = \dfrac{1}{k}dy\\ x = \dfrac{2\pi i}{k} \to y=0\\ x = \dfrac{2\pi (i+1)}{k} \to y = 2\pi \end{cases}.
\end{align} \begin{align}
\sum_{i=0}^{k-1} \dfrac{1}{k} \int_{0}^{2\pi}f \left(\dfrac{y+2 \pi i}{k} \right) \cos (y+2\pi i)dy.
\end{align} f [0, 2\pi] \lambda \in [0,1] \begin{align}
	\theta f\left(\frac{y_1 + 2\pi i}{k}\right)
	+ (1 - \theta)f\left(\frac{y_2 + 2\pi i}{k}\right)
	&\ge f\left(\theta\frac{y_1 + 2\pi i}{k}
	+ (1 - \theta) \frac{y_2 + 2\pi i}{k}\right)\\
	&= f\left(\frac{\theta y_1 + (1 - \theta)y_2 + 2\pi i}{k}\right).
\end{align} \begin{align}
x_1 = \dfrac{y_1 + 2\pi i}{k} \ \text{and} \ x_2 = \dfrac{y_2 + 2\pi i}{k}.
\end{align} f \left( \dfrac{y + 2\pi i}{k}\right) [0, 2\pi] k=1 \begin{align}
\int_0^{2\pi} f \left( \dfrac{y + 2\pi i}{k} \right)\cos y dy \geq 0.
\end{align} \blacksquare","['real-analysis', 'functional-analysis', 'convex-analysis']"
40,Elliptic integrals and $\zeta(5)$.,Elliptic integrals and .,\zeta(5),Who can evaluate this one? $$ \int_0^1 \frac{K'(k)^4}{K(k)^2} \;k\;dk = \frac{31}{8} \zeta(5) . $$ Note: I used the elliptic modulus $k$ (and not the parameter $m = k^2$ commonly seen in Mathematica).  That is: \begin{align} K(k) &:= \int_0^1\frac{dt}{\sqrt{(1-t^2)(1-k^2 t^2)}} \\ K'(k) &:= K(\sqrt{1-k^2}\;) \end{align},Who can evaluate this one? Note: I used the elliptic modulus (and not the parameter commonly seen in Mathematica).  That is:,"
\int_0^1 \frac{K'(k)^4}{K(k)^2} \;k\;dk = \frac{31}{8} \zeta(5) .
 k m = k^2 \begin{align}
K(k) &:= \int_0^1\frac{dt}{\sqrt{(1-t^2)(1-k^2 t^2)}}
\\
K'(k) &:= K(\sqrt{1-k^2}\;)
\end{align}","['real-analysis', 'definite-integrals', 'modular-forms', 'elliptic-integrals']"
41,Every even degree polynomial is eventually symmetric,Every even degree polynomial is eventually symmetric,,"Let  we have  a  polynomial function $F:\mathbb{R} \to \mathbb{R}$ with $F(x)=ax^{2n}+bx^{2n-1}+\ldots+px+q$ . We assume  that $a>0$ . For    sufficiently large $y$ ,  let $A(y), B(y)$ be two distinct right inverses of $F$ , that  is $F(A(y))=F(B(y))=y$ please see the picture of this  linked page Prove that $$\lim_{y\to{\infty}} A(y)+B(y)=-b/na$$ I  had  and I  have a  proof  for  this exercise but  I am searching for  some  other proofs or some other elementary proofs. Moreover   I wish to check and examine  whether it is really a very trivial elementary exercise or it is a bit nontrivial. Note that for  higher degrees, according to Galois, we have  no  a  precise  formula for $A(y)$ and $B(y)$ . Pleease see page 4, item III line $-3$ of    my paper  below. The journal who accepted my paper (year 2002), did not  asked me to provide any  proof. Regarding this limit , inside the paper below I wrote that  ""it is a simple  exercise"". I did not write any proof of this limit in my thesis. No one in my defense committee asked me any proof of this limit . After all I think that it is  quite easy to proof. Thought it is  a  very simple  limit but playt a  crucial role to determine the  stability of  the  homoclinic loop based at equattor of Poincare sphere: https://arxiv.org/pdf/math/0409594.pdf RemarK: This  actualy gives us some information on the  sum of  complex  preimages $F^{-1}(y) \subset \mathbb{C}$ as $y$ goes to $\infty$ . On the other hand, inspired by this post one may think to upper and  lower bound on  the norm of  subsets of $F^{-1}(y)$","Let  we have  a  polynomial function with . We assume  that . For    sufficiently large ,  let be two distinct right inverses of , that  is please see the picture of this  linked page Prove that I  had  and I  have a  proof  for  this exercise but  I am searching for  some  other proofs or some other elementary proofs. Moreover   I wish to check and examine  whether it is really a very trivial elementary exercise or it is a bit nontrivial. Note that for  higher degrees, according to Galois, we have  no  a  precise  formula for and . Pleease see page 4, item III line of    my paper  below. The journal who accepted my paper (year 2002), did not  asked me to provide any  proof. Regarding this limit , inside the paper below I wrote that  ""it is a simple  exercise"". I did not write any proof of this limit in my thesis. No one in my defense committee asked me any proof of this limit . After all I think that it is  quite easy to proof. Thought it is  a  very simple  limit but playt a  crucial role to determine the  stability of  the  homoclinic loop based at equattor of Poincare sphere: https://arxiv.org/pdf/math/0409594.pdf RemarK: This  actualy gives us some information on the  sum of  complex  preimages as goes to . On the other hand, inspired by this post one may think to upper and  lower bound on  the norm of  subsets of","F:\mathbb{R} \to \mathbb{R} F(x)=ax^{2n}+bx^{2n-1}+\ldots+px+q a>0 y A(y), B(y) F F(A(y))=F(B(y))=y \lim_{y\to{\infty}} A(y)+B(y)=-b/na A(y) B(y) -3 F^{-1}(y) \subset \mathbb{C} y \infty F^{-1}(y)","['real-analysis', 'calculus', 'limits', 'analysis', 'polynomials']"
42,Proof of the inequality $||a+b|^p - |a|^p| \leq \epsilon |a|^p + C_{\epsilon} |b|^p$,Proof of the inequality,||a+b|^p - |a|^p| \leq \epsilon |a|^p + C_{\epsilon} |b|^p,"In this paper , the following theorem is used implicitly (see Example (a) on page 488): Let $0<p<\infty$. For every $\epsilon > 0$, there exists some $C_{\epsilon} > 0$ such that $\forall a,b \in \mathbb{C}$:   $$ \left||a+b|^p - |a|^p \right| \leq \epsilon |a|^p + C_{\epsilon} |b|^p $$ Is this a well-known result? I would like to know its proof, and its name if it has one. Can someone point me towards a reference to this result which includes a proof, or give me a proof outline? I've tried using Jensen's Inequality for the case $p>1, a,b>0$, but it doesn't seem to work out.","In this paper , the following theorem is used implicitly (see Example (a) on page 488): Let $0<p<\infty$. For every $\epsilon > 0$, there exists some $C_{\epsilon} > 0$ such that $\forall a,b \in \mathbb{C}$:   $$ \left||a+b|^p - |a|^p \right| \leq \epsilon |a|^p + C_{\epsilon} |b|^p $$ Is this a well-known result? I would like to know its proof, and its name if it has one. Can someone point me towards a reference to this result which includes a proof, or give me a proof outline? I've tried using Jensen's Inequality for the case $p>1, a,b>0$, but it doesn't seem to work out.",,"['real-analysis', 'inequality']"
43,Interchange between Integral and Sup?,Interchange between Integral and Sup?,,"What is the analysis theorem that gives interchange between Integral and Sup. In particular, let $f_r (t)$ be a function on the circle $U = \{z\in \mathbb C: | z | = 1\}$, which condition must check $f_r (t)$ so that we can interchange Integral and sup: $$\sup_{0\leq r<1}\int_{\mathbb U}f_r(t)  \, dt = \int_{\mathbb U}\,  \sup_{0\leq r<1} f_r(t)  \, dt\quad ?$$ Thank you in advance","What is the analysis theorem that gives interchange between Integral and Sup. In particular, let $f_r (t)$ be a function on the circle $U = \{z\in \mathbb C: | z | = 1\}$, which condition must check $f_r (t)$ so that we can interchange Integral and sup: $$\sup_{0\leq r<1}\int_{\mathbb U}f_r(t)  \, dt = \int_{\mathbb U}\,  \sup_{0\leq r<1} f_r(t)  \, dt\quad ?$$ Thank you in advance",,"['real-analysis', 'integration', 'complex-analysis', 'lebesgue-integral']"
44,Does $\sum\limits_{n=1}^{\infty}\frac{\cos^{2}(n+1)}{n}$ converge?,Does  converge?,\sum\limits_{n=1}^{\infty}\frac{\cos^{2}(n+1)}{n},"The original question, given to my Calculus II recitation class, was: Determine if the series $$\sum\limits_{n=1}^{\infty}\frac{(-1)^{n}\cos^{2}(n+1)}{n}$$ converges absolutely, conditionally, or diverges. I can kind of see a comparison with the alternating harmonic series here, but making that formal is tough. With the absolute series $\sum\limits_{n=1}^{\infty} \frac{\cos^{2}(n+1)}{n}$ , I'm not sure what test to apply. What I've Tried: No tests (in the classical Calc II curriculum) work. I've tried expanding $\cos^{2}(n+1)$ into a power series within the series in question, but I'm not really sure where to go from there. My intuition tells me this series will diverge, since it seems ""close"" to the harmonic series; but $\cos(x)$ is less than $1$ infinitely often.","The original question, given to my Calculus II recitation class, was: Determine if the series converges absolutely, conditionally, or diverges. I can kind of see a comparison with the alternating harmonic series here, but making that formal is tough. With the absolute series , I'm not sure what test to apply. What I've Tried: No tests (in the classical Calc II curriculum) work. I've tried expanding into a power series within the series in question, but I'm not really sure where to go from there. My intuition tells me this series will diverge, since it seems ""close"" to the harmonic series; but is less than infinitely often.",\sum\limits_{n=1}^{\infty}\frac{(-1)^{n}\cos^{2}(n+1)}{n} \sum\limits_{n=1}^{\infty} \frac{\cos^{2}(n+1)}{n} \cos^{2}(n+1) \cos(x) 1,"['real-analysis', 'calculus', 'sequences-and-series', 'trigonometry']"
45,Monotonicity of $\frac{n}{\sqrt[n]{(n!)}}$,Monotonicity of,\frac{n}{\sqrt[n]{(n!)}},It is known that when $n\rightarrow\infty$ the sequence $$\frac{n}{\sqrt[n]{(n!)}}$$ has limit $e$ but I don't know how to prove it's monotonicity. After a short calculus using WolframAlpha I found that this sequence is actually increasing. I tried to compare $2$ consecutive members but I couldn't manage to show something. It is obvious that $$\frac{n^n}{n!}$$ is increasing but that don't help us much (I think),It is known that when $n\rightarrow\infty$ the sequence $$\frac{n}{\sqrt[n]{(n!)}}$$ has limit $e$ but I don't know how to prove it's monotonicity. After a short calculus using WolframAlpha I found that this sequence is actually increasing. I tried to compare $2$ consecutive members but I couldn't manage to show something. It is obvious that $$\frac{n^n}{n!}$$ is increasing but that don't help us much (I think),,"['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'radicals']"
46,"For function $f:[0,1]\rightarrow \Bbb R$ we know : $f(x+y)\geq f(x)+f(y)$.Prove that $f(x)\leq 2x$",For function  we know : .Prove that,"f:[0,1]\rightarrow \Bbb R f(x+y)\geq f(x)+f(y) f(x)\leq 2x","Function $f:[0,1]\rightarrow \Bbb R$ holds in the following conditions : 1) $f(1)=1$ 2) $\forall x \in [0,1]: f(x)\geq 0$ 3) $\forall x,y,x+y \in [0,1]: f(x+y)\geq f(x)+f(y)$. Prove that $\forall x \in [0,1] : f(x)\leq 2x$ I tested values $0,\frac12$ in the given formula and got the following results: $$f(0)=0 , f(\frac12)\leq \frac12,f(x)\leq \frac{f(2x)}{2}$$ Please help complete the proof.","Function $f:[0,1]\rightarrow \Bbb R$ holds in the following conditions : 1) $f(1)=1$ 2) $\forall x \in [0,1]: f(x)\geq 0$ 3) $\forall x,y,x+y \in [0,1]: f(x+y)\geq f(x)+f(y)$. Prove that $\forall x \in [0,1] : f(x)\leq 2x$ I tested values $0,\frac12$ in the given formula and got the following results: $$f(0)=0 , f(\frac12)\leq \frac12,f(x)\leq \frac{f(2x)}{2}$$ Please help complete the proof.",,['real-analysis']
47,Could the orbit of $0$ through a polynomial be dense in $\mathbb R$?,Could the orbit of  through a polynomial be dense in ?,0 \mathbb R,"Could we construct a polynomial $P(x)\in\mathbb R[x]$ such that the sequence $(x_n)$ which is given by $x_0=P(0)$ and $x_{n+1}=P(x_n)$ for any $n\ge0$, is dense in $\mathbb R$ ?. I guess the answer is no, but I could not prove it.","Could we construct a polynomial $P(x)\in\mathbb R[x]$ such that the sequence $(x_n)$ which is given by $x_0=P(0)$ and $x_{n+1}=P(x_n)$ for any $n\ge0$, is dense in $\mathbb R$ ?. I guess the answer is no, but I could not prove it.",,"['calculus', 'real-analysis']"
48,Dirac measures are extreme points of unit ball of $C(K)^*$.,Dirac measures are extreme points of unit ball of .,C(K)^*,"I've seen proofs that Dirac measures are the extreme points of probability measures, but how do we prove it for general complex Borel measures with total variation norm 1? I only want to know why they're in the set of all extreme points.","I've seen proofs that Dirac measures are the extreme points of probability measures, but how do we prove it for general complex Borel measures with total variation norm 1? I only want to know why they're in the set of all extreme points.",,"['real-analysis', 'functional-analysis', 'measure-theory']"
49,"Rigorous, real analysis, proof of De MoivreLaplace theorem","Rigorous, real analysis, proof of De MoivreLaplace theorem",,"The undergraduate books in probability theory that I know do not provide a rigorous proof of even a weak version of the central limit theorem. Instead, they rely on L茅vy's continuity theorem, whose proof they choose to omit due to it allegedly being too technical. It seems (see comments here ) that the proof of the De MoivreLaplace theorem which is just a special case of the central limit theorem is not as difficult to prove and I've been searching for a sufficiently rigorous proof. However, in order to prove it everyone either refers to the central limit theorem which they as aforementioned do not provide a proof of or they provide some non-rigorous proof. I've searched on google for quite a while now and I am unable to find a proof that is rigorous and does not refer to the central limit theorem. This makes me think that a proper proof of the De MoivreLaplace theorem is beyond the scope of undergraduate mathematics students as well. Is this correct? If not, how does one prove this thing? Preferably I'd like it to be proven with some ordinary real analysis methods if possible. If I am to do it by myself, where do I start? And no, I don't like the wikipedia proof.","The undergraduate books in probability theory that I know do not provide a rigorous proof of even a weak version of the central limit theorem. Instead, they rely on L茅vy's continuity theorem, whose proof they choose to omit due to it allegedly being too technical. It seems (see comments here ) that the proof of the De MoivreLaplace theorem which is just a special case of the central limit theorem is not as difficult to prove and I've been searching for a sufficiently rigorous proof. However, in order to prove it everyone either refers to the central limit theorem which they as aforementioned do not provide a proof of or they provide some non-rigorous proof. I've searched on google for quite a while now and I am unable to find a proof that is rigorous and does not refer to the central limit theorem. This makes me think that a proper proof of the De MoivreLaplace theorem is beyond the scope of undergraduate mathematics students as well. Is this correct? If not, how does one prove this thing? Preferably I'd like it to be proven with some ordinary real analysis methods if possible. If I am to do it by myself, where do I start? And no, I don't like the wikipedia proof.",,"['real-analysis', 'probability-theory', 'binomial-distribution', 'central-limit-theorem', 'probability-limit-theorems']"
50,Curious relation between $e$ and $\pi$ that produces almost integers,Curious relation between  and  that produces almost integers,e \pi,"I have seen in an article, without proof, that the following expression involving $e$ and $\pi$ is an almost integer very close to 1: $ e^{-\frac{\pi}{9}} + e^{-4\frac{\pi}{9}} + e^{-9\frac{\pi}{9}} + e^{-16\frac{\pi}{9}} + e^{-25\frac{\pi}{9}} + e^{-36\frac{\pi}{9}} + e^{-49\frac{\pi}{9}} + e^{-64\frac{\pi}{9}} = 1.0000000000010504... $ Furthermore, I have read that the following ""approximate theorem"" holds If $n$ is an odd square, then $\sum\limits_{k = 1}^{n-1} e^{-k^2\frac{\pi}{n}}$ is an almost integer. By some experiments I see easily that the integer being approximated is ($\sqrt{n} - 1)/2$ How to prove that this curious relation holds?","I have seen in an article, without proof, that the following expression involving $e$ and $\pi$ is an almost integer very close to 1: $ e^{-\frac{\pi}{9}} + e^{-4\frac{\pi}{9}} + e^{-9\frac{\pi}{9}} + e^{-16\frac{\pi}{9}} + e^{-25\frac{\pi}{9}} + e^{-36\frac{\pi}{9}} + e^{-49\frac{\pi}{9}} + e^{-64\frac{\pi}{9}} = 1.0000000000010504... $ Furthermore, I have read that the following ""approximate theorem"" holds If $n$ is an odd square, then $\sum\limits_{k = 1}^{n-1} e^{-k^2\frac{\pi}{n}}$ is an almost integer. By some experiments I see easily that the integer being approximated is ($\sqrt{n} - 1)/2$ How to prove that this curious relation holds?",,"['real-analysis', 'taylor-expansion']"
51,Filling a unit cube with countable balls.,Filling a unit cube with countable balls.,,"Is it possible to fill an $n-$dimensional unit cube with countable number of non-overlapping $n-$dimensional balls? By $n-$dimensional unit cube, I am thinking of the set $$ C:=\{\ x\in\Bbb R^n\ |\ 0\le x_i\le 1\ \text{for all}\ i=1,2,\dots,n\ \} $$ where $x=(x_1,\dots,x_n)$. An $n-$dimensional ball centered at $x_0$ is defined to be $$ B(x_0;r):=\{\ x\in\Bbb R^n\ |\ d(x_0,x)<r\ \} $$  where $d$ is the Euclidean distance function. A sequence of balls $(B_1,B_2,\dots)$ is said to fill the unit cube if they are pairwise-disjoint , each $B_i\subset C$ and   $$ \sum_{i=1}^{\infty}\mu(B_i)=1 $$    where $\mu$ is the Lebesgue measure on $\Bbb R^n$. Intuitively, I feel that it should be possible to fill the unit cube with many balls. However, I can't think of a way to prove it. Since $C$ is separable, I thought about enumerating its rational points into a sequence $\{c_1,c_2,c_3,\dots \}$ and form balls around them such that $B_i:=B(c_i,r_i)$ where $$ r_i:=\sup\{\ r\in\Bbb R^+\ |\ B(c_i;r)\ \text{is contained in $C$ and doesn't intersect $B_1,B_2,\dots,B_{i-1}$} \}. $$  I don't know if this method works or not. I would really appreciate if anyone can suggest a way to finish the prove, suggest a new method, or give a negative answer to my question. An answer that only works in dimension $2$ or $3$ is also very welcomed.","Is it possible to fill an $n-$dimensional unit cube with countable number of non-overlapping $n-$dimensional balls? By $n-$dimensional unit cube, I am thinking of the set $$ C:=\{\ x\in\Bbb R^n\ |\ 0\le x_i\le 1\ \text{for all}\ i=1,2,\dots,n\ \} $$ where $x=(x_1,\dots,x_n)$. An $n-$dimensional ball centered at $x_0$ is defined to be $$ B(x_0;r):=\{\ x\in\Bbb R^n\ |\ d(x_0,x)<r\ \} $$  where $d$ is the Euclidean distance function. A sequence of balls $(B_1,B_2,\dots)$ is said to fill the unit cube if they are pairwise-disjoint , each $B_i\subset C$ and   $$ \sum_{i=1}^{\infty}\mu(B_i)=1 $$    where $\mu$ is the Lebesgue measure on $\Bbb R^n$. Intuitively, I feel that it should be possible to fill the unit cube with many balls. However, I can't think of a way to prove it. Since $C$ is separable, I thought about enumerating its rational points into a sequence $\{c_1,c_2,c_3,\dots \}$ and form balls around them such that $B_i:=B(c_i,r_i)$ where $$ r_i:=\sup\{\ r\in\Bbb R^+\ |\ B(c_i;r)\ \text{is contained in $C$ and doesn't intersect $B_1,B_2,\dots,B_{i-1}$} \}. $$  I don't know if this method works or not. I would really appreciate if anyone can suggest a way to finish the prove, suggest a new method, or give a negative answer to my question. An answer that only works in dimension $2$ or $3$ is also very welcomed.",,"['real-analysis', 'geometry', 'measure-theory', 'geometric-measure-theory']"
52,Are minimizing a function and root finding the same?,Are minimizing a function and root finding the same?,,"What is the relationship between minimizing a function and finding a root of an equation? Are the the same? I know in both problem we have similar algorithms, such as gradient decent, or newton's methods. For example, Let use assume $x$ is a scalar. Finding a root for an equation $f(x)=b$ is checking where the function $f(x)-b$ cross the x axis. This is definitely not equal to minimize $f(x)-b$. But in the convex optimization book , Minimizing $\|Ax-b\|_2^2$ is equal to the solution of the linear system $Ax=b$. What I am missing? in which case we can transfer optimization to root finding?","What is the relationship between minimizing a function and finding a root of an equation? Are the the same? I know in both problem we have similar algorithms, such as gradient decent, or newton's methods. For example, Let use assume $x$ is a scalar. Finding a root for an equation $f(x)=b$ is checking where the function $f(x)-b$ cross the x axis. This is definitely not equal to minimize $f(x)-b$. But in the convex optimization book , Minimizing $\|Ax-b\|_2^2$ is equal to the solution of the linear system $Ax=b$. What I am missing? in which case we can transfer optimization to root finding?",,"['real-analysis', 'optimization', 'self-learning', 'roots', 'maxima-minima']"
53,What is the most general integral on $\mathbb{R}$?,What is the most general integral on ?,\mathbb{R},"The day I learned about the Lebesgue integral was very exciting. A more general integral than Riemann, which is equal to it for all Riemann integrable functions (on finite domains)? Very cool. Unfortunately, my curiosity led me to google, and my search results showed: It turns out I'm more naive than I ever knew. The question: Is there a ""most general integral"" of real-valued functions on the real line? One which agrees with the others where they are defined, but is defined on a superset of their domains? (""defined"", for me, includes infinite integrals). The Khinchin integral seems like a candidate. Note: I saw another similar question but it didn't ask about $\mathbb{R}$ specifically, which is my interest. Note2: I don't mean ""trivial"" integrals, like one which is defined to be 0 whenever the Riemann integral is not defined, or equal to it otherwise. The answer would presumably have its own wikipedia page.","The day I learned about the Lebesgue integral was very exciting. A more general integral than Riemann, which is equal to it for all Riemann integrable functions (on finite domains)? Very cool. Unfortunately, my curiosity led me to google, and my search results showed: It turns out I'm more naive than I ever knew. The question: Is there a ""most general integral"" of real-valued functions on the real line? One which agrees with the others where they are defined, but is defined on a superset of their domains? (""defined"", for me, includes infinite integrals). The Khinchin integral seems like a candidate. Note: I saw another similar question but it didn't ask about $\mathbb{R}$ specifically, which is my interest. Note2: I don't mean ""trivial"" integrals, like one which is defined to be 0 whenever the Riemann integral is not defined, or equal to it otherwise. The answer would presumably have its own wikipedia page.",,"['calculus', 'real-analysis', 'integration']"
54,Some clarification needed on the Relation between Total Derivative and Directional Derivative,Some clarification needed on the Relation between Total Derivative and Directional Derivative,,"I will consider here functions of several variables only. If both directional derivative $D_{v}f(x)$  at  $x$  along  $v$   and total derivative $D f(x)$ at  $x$  exist  then $$D_{v}f(x)=Df(x)(v).$$ Existence of total  derivative ensures  that  of directional derivative in every direction but not the other way round. There are functions who have, at some point of the domain, directional derivative in every direction but not differentiable at that point i.e. the total derivative at that point does exist. Now, all of my knowledge is theoretical. I cannot see the picture clearly, i.e.  the picture of the two kinds of derivative existing together, or one existing and not the other - how do these work? I mean some geometrical interpretation for say $2$ or $3$ dimensional space would help. I am so confused with this thing, I am not even sure if I have managed to convey my problem properly. Please help with some clarification. Thanks.","I will consider here functions of several variables only. If both directional derivative $D_{v}f(x)$  at  $x$  along  $v$   and total derivative $D f(x)$ at  $x$  exist  then $$D_{v}f(x)=Df(x)(v).$$ Existence of total  derivative ensures  that  of directional derivative in every direction but not the other way round. There are functions who have, at some point of the domain, directional derivative in every direction but not differentiable at that point i.e. the total derivative at that point does exist. Now, all of my knowledge is theoretical. I cannot see the picture clearly, i.e.  the picture of the two kinds of derivative existing together, or one existing and not the other - how do these work? I mean some geometrical interpretation for say $2$ or $3$ dimensional space would help. I am so confused with this thing, I am not even sure if I have managed to convey my problem properly. Please help with some clarification. Thanks.",,"['real-analysis', 'multivariable-calculus', 'derivatives', 'intuition']"
55,Prove that jump functions are measurable,Prove that jump functions are measurable,,"This question comes from the exercises of Stein and Shakarchi's Real analysis Ex. 5.14. Define $$     j_n(x)=  \begin{cases}     0& \text{if } x< x_n\\     \theta_n              & \text{if } x=x_n\\     1  &\text{if } x>x_n \end{cases} $$ where $\{x_n\}$ is a sequence of real number and $0\le\theta_n\le 1$. Further define $$J(x)=\sum^{\infty}_{n=1}\alpha_nj_n(x)$$ where $\alpha_n>0$ and $\sum\alpha_n$ converges. Prove that for any $\epsilon$, the set of $x$ satisfying  $$\limsup_{h\to0}\frac{J(x+h)-J(x)}{h}>\epsilon$$ is measurable. So far what I can prove is that the sum in $J(x)$ can converge absolutely. And I know the limit of measurable functions is measurable. However, Since $J$ is not continuous, I cannot assume $h\to0$ as $\frac1n;n\to\infty$, then I cannot figure out whether the above set is measurable. Edit: There is a hint in the book: consider $$F^N_{k,m}(x)=\sup_{1/k\le|h|\le1/m}\left|\frac{J_N(x+h)-J_N(x)}{h}\right|$$ where $J_N$ is the $N^{\text{th}}$ partial sum of $J$ Show that $F^N_{k,m}$ is measurable (which I couldn't tell why) and so does  $$\lim_{m\to\infty}\lim_{k\to\infty}\lim_{N\to\infty}F^N_{k,m}(x)$$ (Which I also cannot relate it to the original set because I am confused of the multiple limit and supremum)","This question comes from the exercises of Stein and Shakarchi's Real analysis Ex. 5.14. Define $$     j_n(x)=  \begin{cases}     0& \text{if } x< x_n\\     \theta_n              & \text{if } x=x_n\\     1  &\text{if } x>x_n \end{cases} $$ where $\{x_n\}$ is a sequence of real number and $0\le\theta_n\le 1$. Further define $$J(x)=\sum^{\infty}_{n=1}\alpha_nj_n(x)$$ where $\alpha_n>0$ and $\sum\alpha_n$ converges. Prove that for any $\epsilon$, the set of $x$ satisfying  $$\limsup_{h\to0}\frac{J(x+h)-J(x)}{h}>\epsilon$$ is measurable. So far what I can prove is that the sum in $J(x)$ can converge absolutely. And I know the limit of measurable functions is measurable. However, Since $J$ is not continuous, I cannot assume $h\to0$ as $\frac1n;n\to\infty$, then I cannot figure out whether the above set is measurable. Edit: There is a hint in the book: consider $$F^N_{k,m}(x)=\sup_{1/k\le|h|\le1/m}\left|\frac{J_N(x+h)-J_N(x)}{h}\right|$$ where $J_N$ is the $N^{\text{th}}$ partial sum of $J$ Show that $F^N_{k,m}$ is measurable (which I couldn't tell why) and so does  $$\lim_{m\to\infty}\lim_{k\to\infty}\lim_{N\to\infty}F^N_{k,m}(x)$$ (Which I also cannot relate it to the original set because I am confused of the multiple limit and supremum)",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
56,Derivative that doesn't care about countable subsets?,Derivative that doesn't care about countable subsets?,,"In Lebesgue integration, if you change a function on a countable subset of its domain, neither integrability nor the value of the integral changes. The same is obviously not true for differentiation, which is locally defined through a limit. Now I wonder: Is there a definition of differentiation that also is ""immune"" against changes on countable subsets? Well, I guess one could simply define ""a function $f$ is X-differentiable in $x$ if there exists a function $g$ that is differentiable in $x$ and agrees with $f$ almost everywhere"", but that's sounds like cheating. So actually my question is: Does there exist a natural definition of derivative with this property? Where ""natural"" means that the definition makes sense even if you don't know about the property that it should be ""immune"" against changes on countable sets.","In Lebesgue integration, if you change a function on a countable subset of its domain, neither integrability nor the value of the integral changes. The same is obviously not true for differentiation, which is locally defined through a limit. Now I wonder: Is there a definition of differentiation that also is ""immune"" against changes on countable subsets? Well, I guess one could simply define ""a function $f$ is X-differentiable in $x$ if there exists a function $g$ that is differentiable in $x$ and agrees with $f$ almost everywhere"", but that's sounds like cheating. So actually my question is: Does there exist a natural definition of derivative with this property? Where ""natural"" means that the definition makes sense even if you don't know about the property that it should be ""immune"" against changes on countable sets.",,['real-analysis']
57,"Given $n$ points, the difference of $2$ of them is $1/n$ close to an integer","Given  points, the difference of  of them is  close to an integer",n 2 1/n,"From today's ENS Ulm Math D exam Let $x_1,\ldots,x_n$ be real numbers Prove there exists $i\neq j $ and $h\in \mathbb Z$ such that $|x_i-x_j-h|\leq \frac{1}{n}$ I tried contradiction and pigeonhole principle for 45 minutes, to no avail.","From today's ENS Ulm Math D exam Let be real numbers Prove there exists and such that I tried contradiction and pigeonhole principle for 45 minutes, to no avail.","x_1,\ldots,x_n i\neq j  h\in \mathbb Z |x_i-x_j-h|\leq \frac{1}{n}","['real-analysis', 'combinatorics', 'contest-math']"
58,"Composition of measurable & continuous functions, is it measurable?","Composition of measurable & continuous functions, is it measurable?",,"I am working on this problem $^{(1)}$ on Lebesgue measurability of composition of Lebesgue measurable function and a continuous function: Show that $g \circ f$ is Lebesgue measurable, if $f: X \to \mathbb R$ is Lebesgue measurable and if $g: \mathbb R \to \mathbb R$ is continuous. Prior to this posting, I did lots of research online. First, I found this 6-year-old solution to a problem very similar to mine here at MathHelpForum, but on closer inspection I think not only the author left lots of gap, but also I am not sure if this is a correct solution, and on top of it I do not really understand it. And then internally in MSE, I found this 2012's posting and also this 2013's posting , which are similar but not exactly the same. In my naive logic, I am thinking of first proving that $g$ is measurable because of its continuity, and then since composition of 2 measurable functions is measurable, therefore $g \circ f$ is measurable. But my logic is unreliable, please help me with the right direction and also steps to solve this question. Thank you very much for your time and help. Oops! I forget to include definition to Lebesgue measurability and Lebesgue measurable function until @PhoemueX brought it up. The problem with this text is that it does not have one nice, stand-alone paragraph definition. According to its index, it is written here and there on pages 21, 27 and 39. Here is what I managed to piece them together from those pages: Let $X = \mathbb R$ and let $\mathcal C$ be the collection of intervals of the form $(a, b]$ ... Let $\mathcal l(I) = b - a$ if $I = (a, b]$ ... Define $\mu^*$ as an outer measure... however, that if we restrict $\mu^*$ to a $\sigma$ -algebra $\mathcal L$ which is strictly smaller than the collection of all subsets of $\mathbb R$ , then $\mu^*$ will be a measure on $\mathcal L$ . That measure is what is known as Lebesgue measure. The $\sigma$ -algebra $\mathcal L$ is called the Lebesgue $\sigma$ -algebra .... A set is Lebesgue measurable if it is in the Lebesgue $\sigma$ -algebra. If $X$ is a metric space, $\mathcal B$ is the Borel $\sigma$ -algebra, and $f: X \to \mathbb R$ is measurable with respect to $\mathcal B$ , we say $f$ is Borel measurable. If $f : \mathbb R \to \mathbb R$ is measurable with respect to the Lebesgue $\sigma$ -algebra, we say $f$ is Lebesgue measurable function . Footnotes: (1) Richard F. Bass' Real Analysis , 2nd. edition, chapter 5: Measurable Functions, Exercise 5.6, page 44.","I am working on this problem on Lebesgue measurability of composition of Lebesgue measurable function and a continuous function: Show that is Lebesgue measurable, if is Lebesgue measurable and if is continuous. Prior to this posting, I did lots of research online. First, I found this 6-year-old solution to a problem very similar to mine here at MathHelpForum, but on closer inspection I think not only the author left lots of gap, but also I am not sure if this is a correct solution, and on top of it I do not really understand it. And then internally in MSE, I found this 2012's posting and also this 2013's posting , which are similar but not exactly the same. In my naive logic, I am thinking of first proving that is measurable because of its continuity, and then since composition of 2 measurable functions is measurable, therefore is measurable. But my logic is unreliable, please help me with the right direction and also steps to solve this question. Thank you very much for your time and help. Oops! I forget to include definition to Lebesgue measurability and Lebesgue measurable function until @PhoemueX brought it up. The problem with this text is that it does not have one nice, stand-alone paragraph definition. According to its index, it is written here and there on pages 21, 27 and 39. Here is what I managed to piece them together from those pages: Let and let be the collection of intervals of the form ... Let if ... Define as an outer measure... however, that if we restrict to a -algebra which is strictly smaller than the collection of all subsets of , then will be a measure on . That measure is what is known as Lebesgue measure. The -algebra is called the Lebesgue -algebra .... A set is Lebesgue measurable if it is in the Lebesgue -algebra. If is a metric space, is the Borel -algebra, and is measurable with respect to , we say is Borel measurable. If is measurable with respect to the Lebesgue -algebra, we say is Lebesgue measurable function . Footnotes: (1) Richard F. Bass' Real Analysis , 2nd. edition, chapter 5: Measurable Functions, Exercise 5.6, page 44.","^{(1)} g \circ f f: X \to \mathbb R g: \mathbb R \to \mathbb R g g \circ f X = \mathbb R \mathcal C (a, b] \mathcal l(I) = b - a I = (a, b] \mu^* \mu^* \sigma \mathcal L \mathbb R \mu^* \mathcal L \sigma \mathcal L \sigma \sigma X \mathcal B \sigma f: X \to \mathbb R \mathcal B f f : \mathbb R \to \mathbb R \sigma f","['real-analysis', 'analysis', 'measure-theory', 'lebesgue-measure']"
59,Intuitive understanding of the operator norm?,Intuitive understanding of the operator norm?,,"I understand various vector norms, but I don't understand operator norms.  Specifically, norms on linear operators. Can anyone explain them?","I understand various vector norms, but I don't understand operator norms.  Specifically, norms on linear operators. Can anyone explain them?",,"['real-analysis', 'linear-algebra', 'analysis', 'functional-analysis', 'operator-theory']"
60,Does equicontinuity imply uniform continuity?,Does equicontinuity imply uniform continuity?,,"If $\{f_n(x)\}$ is an equicontinuous family of functions, does it follow that each function is uniformly continuous?  I am a bit confused since in the Arzela-Ascoli theorem, equicontinuity is seems to mean ""$\delta$ depends neither on $n$ nor $x$.""  For example, Wikipedia says: ""The sequence is equicontinuous if, for every $\epsilon > 0$, there exists $\delta>0$ such that $$|f_n(x)-f_n(y)| < \epsilon$$ whenever $|x  y| < \delta$ for all functions $f_n$ in the sequence."" but I also seem to recall such a term as ""uniform equicontinuity.""  Is the definition not universal?","If $\{f_n(x)\}$ is an equicontinuous family of functions, does it follow that each function is uniformly continuous?  I am a bit confused since in the Arzela-Ascoli theorem, equicontinuity is seems to mean ""$\delta$ depends neither on $n$ nor $x$.""  For example, Wikipedia says: ""The sequence is equicontinuous if, for every $\epsilon > 0$, there exists $\delta>0$ such that $$|f_n(x)-f_n(y)| < \epsilon$$ whenever $|x  y| < \delta$ for all functions $f_n$ in the sequence."" but I also seem to recall such a term as ""uniform equicontinuity.""  Is the definition not universal?",,"['calculus', 'real-analysis', 'functional-analysis']"
61,"Prove that $ \int_0^{\pi} \frac{(\cos x)^2}{1 + \cos x \sin x} \,\mathrm{d}x =\int_0^{\pi} \frac{(\sin x)^2}{1 + \cos x \sin x} \,\mathrm{d}x $",Prove that," \int_0^{\pi} \frac{(\cos x)^2}{1 + \cos x \sin x} \,\mathrm{d}x =\int_0^{\pi} \frac{(\sin x)^2}{1 + \cos x \sin x} \,\mathrm{d}x ","In a related question the following integral was evaluated $$        \int_0^{\pi} \frac{(\cos x)^2}{1 + \cos x \sin x} \,\mathrm{d}x      =\int_0^{\pi} \frac{\mathrm{d}x/2}{1 + \cos x \sin x}       =\int_0^{2\pi}  \frac{\mathrm{d}x/2}{2 + \sin x} \,\mathrm{d}x      =\int_{-\infty}^\infty \frac{\mathrm{d}x/2}{1+x+x^2} $$ I noticed something interesting, namely that $$  \begin{align*}         \int_0^{\pi} \frac{(\cos x)^2}{1 + \cos x \sin x} \,\mathrm{d}x     & = \int_0^{\pi} \frac{(\sin x)^2}{1 + \cos x \sin x} \,\mathrm{d}x \\     & = \int_0^{\pi} \frac{(\cos x)^2}{1 - \cos x \sin x} \,\mathrm{d}x       = \int_0^{\pi} \frac{(\sin x)^2}{1 - \cos x \sin x} \,\mathrm{d}x \end{align*} $$ The same trivially holds if the upper limits are changed to $\pi/2$ as well ($x \mapsto \pi/2 -u$). But I had problems proving the first equality. Does anyone have some quick hints?","In a related question the following integral was evaluated $$        \int_0^{\pi} \frac{(\cos x)^2}{1 + \cos x \sin x} \,\mathrm{d}x      =\int_0^{\pi} \frac{\mathrm{d}x/2}{1 + \cos x \sin x}       =\int_0^{2\pi}  \frac{\mathrm{d}x/2}{2 + \sin x} \,\mathrm{d}x      =\int_{-\infty}^\infty \frac{\mathrm{d}x/2}{1+x+x^2} $$ I noticed something interesting, namely that $$  \begin{align*}         \int_0^{\pi} \frac{(\cos x)^2}{1 + \cos x \sin x} \,\mathrm{d}x     & = \int_0^{\pi} \frac{(\sin x)^2}{1 + \cos x \sin x} \,\mathrm{d}x \\     & = \int_0^{\pi} \frac{(\cos x)^2}{1 - \cos x \sin x} \,\mathrm{d}x       = \int_0^{\pi} \frac{(\sin x)^2}{1 - \cos x \sin x} \,\mathrm{d}x \end{align*} $$ The same trivially holds if the upper limits are changed to $\pi/2$ as well ($x \mapsto \pi/2 -u$). But I had problems proving the first equality. Does anyone have some quick hints?",,"['calculus', 'real-analysis', 'integration', 'trigonometry', 'definite-integrals']"
62,Characterization convex function. [closed],Characterization convex function. [closed],,"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $f:[0,1]\rightarrow\mathbb{R}$ be a continuous function such that for all $a,b\in [0,1]$ with $a<b$ $$f\left(\frac{a+b}{2}\right)\leq\frac{1}{b-a}\int_a^b f(x)\,dx.$$ How to prove that $f$ is a convex function? Thank you for your help.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let be a continuous function such that for all with How to prove that is a convex function? Thank you for your help.","f:[0,1]\rightarrow\mathbb{R} a,b\in [0,1] a<b f\left(\frac{a+b}{2}\right)\leq\frac{1}{b-a}\int_a^b f(x)\,dx. f","['real-analysis', 'convex-analysis']"
63,Could non-continuous sequence of functions converge uniformly to continuous function?,Could non-continuous sequence of functions converge uniformly to continuous function?,,"If continuous sequence $ \left( f_n\left(x\right) \right)$ converges uniformly to function $f\left(x\right)$ in some interval of real numbers, than $f\left(x\right)$ must be also continuous. But if non-continuous sequence $ \left( f_n\left(x\right) \right)$ converges uniformly to $f\left(x\right)$ , can $f\left(x\right)$ be continuous ? Thanks.","If continuous sequence $ \left( f_n\left(x\right) \right)$ converges uniformly to function $f\left(x\right)$ in some interval of real numbers, than $f\left(x\right)$ must be also continuous. But if non-continuous sequence $ \left( f_n\left(x\right) \right)$ converges uniformly to $f\left(x\right)$ , can $f\left(x\right)$ be continuous ? Thanks.",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
64,"Meaning and Intuition behind product $\sigma$ algebras , Folland Definition","Meaning and Intuition behind product  algebras , Folland Definition",\sigma,"I am currently studying Real Analysis using Folland's book by the same name, but I have troubles in understanding the product $\sigma$-algebras definition which is as follows: Let $\{ X_{\alpha} \}_{\alpha \in A}$ be an indexed collection of nonempty sets, $X = \prod_{\alpha \in A}X_{\alpha}$, and $\pi_{\alpha}: X \rightarrow X_{\alpha}$ the coordinate maps. If $\mathcal{M}_{\alpha}$ is a $\sigma$-algebra on $X_{\alpha}$ for each $\alpha$, the $\textbf{product $\sigma$-algebra}$ on $X$ is the $\sigma$-algebra generated by : $$\{ \pi^{-1}_{\alpha}(E_{\alpha}):E_{\alpha} \in \mathcal{M}_{\alpha}, \alpha \in A \}.$$ First I don't know what is the definition for the coordinate maps and then what is the intuition or how can I understand more practically product $\sigma$-algebras ? Thank you!","I am currently studying Real Analysis using Folland's book by the same name, but I have troubles in understanding the product $\sigma$-algebras definition which is as follows: Let $\{ X_{\alpha} \}_{\alpha \in A}$ be an indexed collection of nonempty sets, $X = \prod_{\alpha \in A}X_{\alpha}$, and $\pi_{\alpha}: X \rightarrow X_{\alpha}$ the coordinate maps. If $\mathcal{M}_{\alpha}$ is a $\sigma$-algebra on $X_{\alpha}$ for each $\alpha$, the $\textbf{product $\sigma$-algebra}$ on $X$ is the $\sigma$-algebra generated by : $$\{ \pi^{-1}_{\alpha}(E_{\alpha}):E_{\alpha} \in \mathcal{M}_{\alpha}, \alpha \in A \}.$$ First I don't know what is the definition for the coordinate maps and then what is the intuition or how can I understand more practically product $\sigma$-algebras ? Thank you!",,"['real-analysis', 'measure-theory']"
65,Weighted uniform convergence of Taylor series of exponential function,Weighted uniform convergence of Taylor series of exponential function,,"Is the limit    $$ e^{-x}\sum_{n=0}^N \frac{(-1)^n}{n!}x^n\to e^{-2x}  \quad \text{as } \ N\to\infty \tag1 $$   uniform on $[0,+\infty)$? Numerically this appears to be true: see the difference of two sides in (1)  for $N=10$ and $N=100$ plotted  below. But the convergence is very slow ( logarithmic error $\approx N^{-1/2}$ as shown by Antonio Vargas in his answer). In particular, putting $e^{-0.9x}$ and $e^{-1.9x}$ in (1) clearly makes convergence non-uniform. One difficulty here is that the Taylor remainder formula is effective only up to $x\approx N/e$, and the maximum of the difference is at $x\approx N$. The question is inspired by an attempt to find an alternative proof of $\epsilon>0$ there is a polynomial $p$ such that $|f(x)-e^{-x}p|<\epsilon\forall x\in[0,\infty)$ .","Is the limit    $$ e^{-x}\sum_{n=0}^N \frac{(-1)^n}{n!}x^n\to e^{-2x}  \quad \text{as } \ N\to\infty \tag1 $$   uniform on $[0,+\infty)$? Numerically this appears to be true: see the difference of two sides in (1)  for $N=10$ and $N=100$ plotted  below. But the convergence is very slow ( logarithmic error $\approx N^{-1/2}$ as shown by Antonio Vargas in his answer). In particular, putting $e^{-0.9x}$ and $e^{-1.9x}$ in (1) clearly makes convergence non-uniform. One difficulty here is that the Taylor remainder formula is effective only up to $x\approx N/e$, and the maximum of the difference is at $x\approx N$. The question is inspired by an attempt to find an alternative proof of $\epsilon>0$ there is a polynomial $p$ such that $|f(x)-e^{-x}p|<\epsilon\forall x\in[0,\infty)$ .",,"['real-analysis', 'sequences-and-series', 'taylor-expansion', 'uniform-convergence']"
66,What does it exactly mean for a subspace to be dense?,What does it exactly mean for a subspace to be dense?,,"My understanding of rationals being dense in real numbers: I know when we say the rationals are dense in real is because between any two rationals we can find a irrational number. In other words we can approximate irrational numbers using rationals. I think a more precise definition would be is that any open ball around a irrational number will contain a rational number. If what I said is correct, I am trying to think about what it means for $C[a,b]$ (which are the continuous complex valued functions on [a.b]) to be dense subspace of $L^2[a,b]$. From what I said above, I want to say that all functions in $L^2[a,b]$ can be approximated by functions from $C[a,b]$. Is the intuition correct here, what would the precise definition in this case?","My understanding of rationals being dense in real numbers: I know when we say the rationals are dense in real is because between any two rationals we can find a irrational number. In other words we can approximate irrational numbers using rationals. I think a more precise definition would be is that any open ball around a irrational number will contain a rational number. If what I said is correct, I am trying to think about what it means for $C[a,b]$ (which are the continuous complex valued functions on [a.b]) to be dense subspace of $L^2[a,b]$. From what I said above, I want to say that all functions in $L^2[a,b]$ can be approximated by functions from $C[a,b]$. Is the intuition correct here, what would the precise definition in this case?",,"['real-analysis', 'analysis', 'functional-analysis']"
67,Prove that $\frac{x_n}{n}$ is convergent if $x_{m+n} \ge x_m + x_n$,Prove that  is convergent if,\frac{x_n}{n} x_{m+n} \ge x_m + x_n,"Given a positive sequence $x_1 , x_2 , ...$ such that $x_{m+n} \ge x_m + x_n \forall m,n \in N$ Prove that $\frac{x_n}{n} \to l $ where $l$ may be a number or infinity. Here's my original approach: It is easy to prove that $\displaystyle \frac{x_1}{1} \le \frac{x_2}{2} \le \frac{x_4}{4} \le ...$ so the sequence $\frac{x_{2^n}}{2^n}$ has a limit c (*) (i'm supposing $\frac{x_n}{n}$ is bounded). So it is sufficient to prove that for any fixed odd number d, the sequence $\frac{x_{d.2^n}}{d.2^n}$ has a limit c too. Any number d can be represented as $\displaystyle d=2^{e_1} + 2^{e_2} + ... + 2^{e_k}$ So $\frac{x_{d.2^n}}{d.2^n} \ge \frac{x_{2^{e_1 + n}} + x_{2^{e_2 + n}} + ... + x_{2^{e_k + n}}}{d.2^n}$ It is easy to check that the right side of this ineq tends to c using (*). Similarly, d can also be represented as $\displaystyle d+2^{f_1} + 2^{f_2} + ... + 2^{f_t} = 2^g$ so $\frac{x_{d.2^n}}{d.2^n} \le \frac{x_{2^{g+n}}-x_{2^{f_1+n}} - x_{2^{f_2+n}} - ... - x_{2^{f_t+n}}}{d.2^n} $. Again, from (*) one can check that the right side tends to c when n tends to infinity. Thus the proof is complete. This proof looks suspicious, so I would be glad if someone can help me verify it. Thank you. EDIT: The proof is wrong, as pointed out in one of the answer below.","Given a positive sequence $x_1 , x_2 , ...$ such that $x_{m+n} \ge x_m + x_n \forall m,n \in N$ Prove that $\frac{x_n}{n} \to l $ where $l$ may be a number or infinity. Here's my original approach: It is easy to prove that $\displaystyle \frac{x_1}{1} \le \frac{x_2}{2} \le \frac{x_4}{4} \le ...$ so the sequence $\frac{x_{2^n}}{2^n}$ has a limit c (*) (i'm supposing $\frac{x_n}{n}$ is bounded). So it is sufficient to prove that for any fixed odd number d, the sequence $\frac{x_{d.2^n}}{d.2^n}$ has a limit c too. Any number d can be represented as $\displaystyle d=2^{e_1} + 2^{e_2} + ... + 2^{e_k}$ So $\frac{x_{d.2^n}}{d.2^n} \ge \frac{x_{2^{e_1 + n}} + x_{2^{e_2 + n}} + ... + x_{2^{e_k + n}}}{d.2^n}$ It is easy to check that the right side of this ineq tends to c using (*). Similarly, d can also be represented as $\displaystyle d+2^{f_1} + 2^{f_2} + ... + 2^{f_t} = 2^g$ so $\frac{x_{d.2^n}}{d.2^n} \le \frac{x_{2^{g+n}}-x_{2^{f_1+n}} - x_{2^{f_2+n}} - ... - x_{2^{f_t+n}}}{d.2^n} $. Again, from (*) one can check that the right side tends to c when n tends to infinity. Thus the proof is complete. This proof looks suspicious, so I would be glad if someone can help me verify it. Thank you. EDIT: The proof is wrong, as pointed out in one of the answer below.",,"['calculus', 'real-analysis', 'sequences-and-series', 'limits']"
68,"If a series is conditionally convergent, then the series of positive and negative terms are both divergent","If a series is conditionally convergent, then the series of positive and negative terms are both divergent",,"Let $ \sum_{n=1}^\infty a_n  $ be  a conditionally convergent series. Then the series $\sum_{n=1}^\infty b_n$ of positive terms of $\{a_n\}_{n=1}^\infty$ and $\sum_{n=1}^\infty c_n$ of negative terms are divergent. My proof : if $\sum_{n=1}^\infty a_n$ is conditionally convergent, then $\sum_{n=1}^\infty |a_n| = \infty$. Let's suppose that $\sum_{n=1}^\infty b_n$ and $\sum_{n=1}^\infty c_n$ are convergent, so since they are composed of positive and negative terms respectively, then $\sum_{n=1}^\infty |b_n| < \infty$ and $\sum_{n=1}^\infty |c_n| <\infty$ and we have that $$ \sum_{n=1}^\infty |a_n| = \sum_{n=1}^\infty |b_n| + \sum_{n=1}^\infty |c_n| < \infty $$ which is absurd since we supposed that the series was conditionally convergent. Edit Can I define $b_n := \left\{\begin{array}{ll} a_n &\text{if } a_n > 0 \\ 0 &\text{otherwise} \end{array} \right.$ and $c_n$ analogously for negative terms and say that $\sum a_n = \sum b_n + \sum c_n $ and, since $\sum a_n$ converges then both $\sum b_n, \sum c_n$ converge? Are the new series equal to the series of positive (and negative) terms? Is this correct? Thanks in advance","Let $ \sum_{n=1}^\infty a_n  $ be  a conditionally convergent series. Then the series $\sum_{n=1}^\infty b_n$ of positive terms of $\{a_n\}_{n=1}^\infty$ and $\sum_{n=1}^\infty c_n$ of negative terms are divergent. My proof : if $\sum_{n=1}^\infty a_n$ is conditionally convergent, then $\sum_{n=1}^\infty |a_n| = \infty$. Let's suppose that $\sum_{n=1}^\infty b_n$ and $\sum_{n=1}^\infty c_n$ are convergent, so since they are composed of positive and negative terms respectively, then $\sum_{n=1}^\infty |b_n| < \infty$ and $\sum_{n=1}^\infty |c_n| <\infty$ and we have that $$ \sum_{n=1}^\infty |a_n| = \sum_{n=1}^\infty |b_n| + \sum_{n=1}^\infty |c_n| < \infty $$ which is absurd since we supposed that the series was conditionally convergent. Edit Can I define $b_n := \left\{\begin{array}{ll} a_n &\text{if } a_n > 0 \\ 0 &\text{otherwise} \end{array} \right.$ and $c_n$ analogously for negative terms and say that $\sum a_n = \sum b_n + \sum c_n $ and, since $\sum a_n$ converges then both $\sum b_n, \sum c_n$ converge? Are the new series equal to the series of positive (and negative) terms? Is this correct? Thanks in advance",,"['real-analysis', 'sequences-and-series', 'convergence-divergence']"
69,Swapping a limit and a $\sup$,Swapping a limit and a,\sup,"In this proof of the completeness of $(C(K), \| \cdot \|_\infty)$ they use the following inequality: $$ \sup_{x \in K} \lim_{m \to \infty} | f_n(x) - f_m(x) | \leq \liminf_{m \to \infty}\, \sup_{x \in K} | f_n(x) - f_m(x) |.$$ Can someone explain to me why this is true? Presumably, they first write $$ \sup_{x \in K} \lim_{m \to \infty} | f_n(x) - f_m(x) | = \sup_{x \in K} \,\liminf_{m \to \infty} | f_n(x) - f_m(x) |$$ and then swap but the exact argument why this gives the inequality is unclear to me. Many thanks for your help.","In this proof of the completeness of $(C(K), \| \cdot \|_\infty)$ they use the following inequality: $$ \sup_{x \in K} \lim_{m \to \infty} | f_n(x) - f_m(x) | \leq \liminf_{m \to \infty}\, \sup_{x \in K} | f_n(x) - f_m(x) |.$$ Can someone explain to me why this is true? Presumably, they first write $$ \sup_{x \in K} \lim_{m \to \infty} | f_n(x) - f_m(x) | = \sup_{x \in K} \,\liminf_{m \to \infty} | f_n(x) - f_m(x) |$$ and then swap but the exact argument why this gives the inequality is unclear to me. Many thanks for your help.",,"['real-analysis', 'limits', 'banach-spaces']"
70,A question about poisson processes,A question about poisson processes,,"Reading through the book ""Brownian Motion & Stochastic Calculus"" by Karatzas and Shreve,  I found the following exercise (problem 3.9, page 15): Let $ \ N \ $ be a poisson process with intensity $ \lambda > 0 $ (this means, in particular, that $ N_t $ is poisson-$\lambda t$-distributed, i.e. $ P(N_t = k ) = \exp(-\lambda t) \frac{(\lambda t)^k}{k!}, \ \forall \ k \geq 0$) Use Stirling's approximation to show that  $\  \lim_{t \to \infty} (1/\sqrt{\lambda t} ) \ E( N_t - \lambda t )^+ = \frac{1}{2 \pi}$. Trying to prove the claim, I started out like this: $$ \frac{1}{\sqrt{\lambda t}} E( N_t - \lambda t )^+ = \frac{1}{\sqrt{\lambda t}} \exp(-\lambda t) \  \sum_{k \geq \lambda t} \ (k - \lambda t) \frac{(\lambda t)^k}{k!}  $$ $$ \approx \frac{1}{\sqrt{\lambda t}} \exp(-\lambda t) \  \sum_{k \geq \lambda t} \ (k - \lambda t) \frac{(\lambda t)^k}{\sqrt{2 \pi k} \left( \frac{k}{e} \right) ^k}$$ Does anybody know how to finish the proof? Thanks a lot for your help! Regards, Si","Reading through the book ""Brownian Motion & Stochastic Calculus"" by Karatzas and Shreve,  I found the following exercise (problem 3.9, page 15): Let $ \ N \ $ be a poisson process with intensity $ \lambda > 0 $ (this means, in particular, that $ N_t $ is poisson-$\lambda t$-distributed, i.e. $ P(N_t = k ) = \exp(-\lambda t) \frac{(\lambda t)^k}{k!}, \ \forall \ k \geq 0$) Use Stirling's approximation to show that  $\  \lim_{t \to \infty} (1/\sqrt{\lambda t} ) \ E( N_t - \lambda t )^+ = \frac{1}{2 \pi}$. Trying to prove the claim, I started out like this: $$ \frac{1}{\sqrt{\lambda t}} E( N_t - \lambda t )^+ = \frac{1}{\sqrt{\lambda t}} \exp(-\lambda t) \  \sum_{k \geq \lambda t} \ (k - \lambda t) \frac{(\lambda t)^k}{k!}  $$ $$ \approx \frac{1}{\sqrt{\lambda t}} \exp(-\lambda t) \  \sum_{k \geq \lambda t} \ (k - \lambda t) \frac{(\lambda t)^k}{\sqrt{2 \pi k} \left( \frac{k}{e} \right) ^k}$$ Does anybody know how to finish the proof? Thanks a lot for your help! Regards, Si",,"['real-analysis', 'probability-theory', 'stochastic-processes']"
71,Showing $\sup \{ \sin n \mid n\in \mathbb N \} =1$ [closed],Showing  [closed],\sup \{ \sin n \mid n\in \mathbb N \} =1,"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question how to prove  $\sup \{ \sin n \mid n\in \mathbb N \} =1$","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question how to prove  $\sup \{ \sin n \mid n\in \mathbb N \} =1$",,"['real-analysis', 'trigonometry']"
72,"Nearly, but not almost, continuous","Nearly, but not almost, continuous",,"Lusin's Theorem asserts that a measurable function f is nearly continuous in the sense that for all $\epsilon>0$ there is a set S of measure less than $\epsilon$ such that f is continuous on the complement of S. I am looking for an example of such a function which is not continuous on the complement of any measure zero set. For instance, something like the characteristic function of the rationals won't do, since it is continuous on the complement of the rationals, which is a zero set.","Lusin's Theorem asserts that a measurable function f is nearly continuous in the sense that for all $\epsilon>0$ there is a set S of measure less than $\epsilon$ such that f is continuous on the complement of S. I am looking for an example of such a function which is not continuous on the complement of any measure zero set. For instance, something like the characteristic function of the rationals won't do, since it is continuous on the complement of the rationals, which is a zero set.",,"['real-analysis', 'measure-theory']"
73,Computing the Lebesgue integral of $\frac{1}{1+x^2}$,Computing the Lebesgue integral of,\frac{1}{1+x^2},"I'm asked to show that $f(x)=\frac{1}{1+x^2}$ is Lebesgue integrable over the real line and that its integral is $\pi$. I can bound this function by the measurable function $\phi=\sum_{n=0}^{\infty}\frac{1}{1+n^2}\chi_{I_n}$ where $I_n = (n-1,n]\cup [n,n+1)$ and $\chi$ means characteristic function.  This function $\phi$ can be written as the limit of its increasing sequence of partial sums, so I know that its integral is $2\sum_{n=0}^{\infty}\frac{1}{1+n^2}$ which converges.  This ensures that the integral of $f$ is bounded, and hence $f$ is Lebesgue integrable. I'm a bit stuck on how to show that $\int f$ is $\pi$ however.  I suspect that I'm intended to write $\int f$ as a limit of series like that above.  By cutting up those characteristic functions ever more finely, I can express the integral of $f$ as $$\lim_{k\to \infty} \frac{2}{k} \sum_{n=0}^{\infty} \frac{1}{1+(n/k)^2}$$ but I don't at all know how to evaluate that limit (though apparently it does evaluate to $\pi$). So, I suppose my question is whether this is a good way to go about this (the overall point is to use the theory of Lebesgue measure and Lebesgue integration), and whether this last limit I've written is somehow easily seen to converge to $\pi$?  Or, should I try to express the function as a limit of some other functions?","I'm asked to show that $f(x)=\frac{1}{1+x^2}$ is Lebesgue integrable over the real line and that its integral is $\pi$. I can bound this function by the measurable function $\phi=\sum_{n=0}^{\infty}\frac{1}{1+n^2}\chi_{I_n}$ where $I_n = (n-1,n]\cup [n,n+1)$ and $\chi$ means characteristic function.  This function $\phi$ can be written as the limit of its increasing sequence of partial sums, so I know that its integral is $2\sum_{n=0}^{\infty}\frac{1}{1+n^2}$ which converges.  This ensures that the integral of $f$ is bounded, and hence $f$ is Lebesgue integrable. I'm a bit stuck on how to show that $\int f$ is $\pi$ however.  I suspect that I'm intended to write $\int f$ as a limit of series like that above.  By cutting up those characteristic functions ever more finely, I can express the integral of $f$ as $$\lim_{k\to \infty} \frac{2}{k} \sum_{n=0}^{\infty} \frac{1}{1+(n/k)^2}$$ but I don't at all know how to evaluate that limit (though apparently it does evaluate to $\pi$). So, I suppose my question is whether this is a good way to go about this (the overall point is to use the theory of Lebesgue measure and Lebesgue integration), and whether this last limit I've written is somehow easily seen to converge to $\pi$?  Or, should I try to express the function as a limit of some other functions?",,"['real-analysis', 'measure-theory', 'lebesgue-integral']"
74,Convergence tests for improper multiple integrals,Convergence tests for improper multiple integrals,,"For improper single integrals with positive integrands of the first, second or mixed type there are the comparison and the limit tests to determine their convergence or divergence. There is also the absolute convergence theorem. For multiple integrals I know that the comparison test can be used as well. Question : But can the limit or other tests be generalized to multiple integrals? Could you provide some references? Added : I have thought of Riemann integration only. Example : When we evaluate (Proof 1 of this and Apostol's article )  this improper double integral $$\int_{0}^{1}\int_{0}^{1}\left(\dfrac{1}{1-xy}\right) \mathrm{d}x\mathrm{d}y,$$ we conclude that it is finite. Question 2: Which test should we apply to know in advance that it converges?","For improper single integrals with positive integrands of the first, second or mixed type there are the comparison and the limit tests to determine their convergence or divergence. There is also the absolute convergence theorem. For multiple integrals I know that the comparison test can be used as well. Question : But can the limit or other tests be generalized to multiple integrals? Could you provide some references? Added : I have thought of Riemann integration only. Example : When we evaluate (Proof 1 of this and Apostol's article )  this improper double integral we conclude that it is finite. Question 2: Which test should we apply to know in advance that it converges?","\int_{0}^{1}\int_{0}^{1}\left(\dfrac{1}{1-xy}\right) \mathrm{d}x\mathrm{d}y,","['calculus', 'real-analysis', 'analysis', 'reference-request', 'multivariable-calculus']"
75,Cartesian Product of Two Complete Metric Spaces is Complete,Cartesian Product of Two Complete Metric Spaces is Complete,,"This is a problem I'm stuck on that our professor gave us for additional practice (not homework, but its recommended that we understand how to prove it). We know X and Y are complete metric spaces, and we need to show that $X \times Y$ is complete. I'm really lost on the proof technique. We were given an outline as follows, but I could only fully figure out (1). Part 3 is what we've been really stuck on though.  I was wondering whether someone could give an proof for say a more specific space where $X = \mathbb{R}, Y = \mathbb{R}$, so I could understand the principle. Outlined: 1) Show that $d_{X \times Y} ( (a_1,b_1) , (a_2,b_2)) = \max \{ d_X (a_1,a_2) , d_Y (b_1, b_2)\}$ is a metric. 2) Prove that this gives the product topology on $X \times Y$. 3) Prove that  if  $a_n, b_n$ are Cauchy sequences, where $a_n \in X$ and $b_n \in Y$, then $(a_n,b_n )$ is Cauchy.","This is a problem I'm stuck on that our professor gave us for additional practice (not homework, but its recommended that we understand how to prove it). We know X and Y are complete metric spaces, and we need to show that $X \times Y$ is complete. I'm really lost on the proof technique. We were given an outline as follows, but I could only fully figure out (1). Part 3 is what we've been really stuck on though.  I was wondering whether someone could give an proof for say a more specific space where $X = \mathbb{R}, Y = \mathbb{R}$, so I could understand the principle. Outlined: 1) Show that $d_{X \times Y} ( (a_1,b_1) , (a_2,b_2)) = \max \{ d_X (a_1,a_2) , d_Y (b_1, b_2)\}$ is a metric. 2) Prove that this gives the product topology on $X \times Y$. 3) Prove that  if  $a_n, b_n$ are Cauchy sequences, where $a_n \in X$ and $b_n \in Y$, then $(a_n,b_n )$ is Cauchy.",,"['real-analysis', 'analysis', 'general-topology']"
76,Convergence of $\sum_{k=1}^{n} f(k) - \int_{1}^{n} f(x) dx$,Convergence of,\sum_{k=1}^{n} f(k) - \int_{1}^{n} f(x) dx,"I had asked this question sometime ago here. Now I have a question which I think is related to it. Let $f$ be an increasing function (continuous, of course!) with $f(1)=0$. Consider the sequence $s_{n}= ( \sum\limits_{k=1}^{n} f(k) - \int\limits_{1}^{n} f(x) dx )$. When does $s_{n}$ converge?","I had asked this question sometime ago here. Now I have a question which I think is related to it. Let $f$ be an increasing function (continuous, of course!) with $f(1)=0$. Consider the sequence $s_{n}= ( \sum\limits_{k=1}^{n} f(k) - \int\limits_{1}^{n} f(x) dx )$. When does $s_{n}$ converge?",,['real-analysis']
77,"An accurate, rapidly converging estimate of $\sum_{j = 1}^{\infty}\frac 1 {j^2}$ (the Basel Problem) using only elementary calculus","An accurate, rapidly converging estimate of  (the Basel Problem) using only elementary calculus",\sum_{j = 1}^{\infty}\frac 1 {j^2},"Although the Basel Problem $$\zeta(2) = \sum_{j = 1}^{\infty}\frac 1 {j^2}$$ took a hundred years to solve analytically, using elementary calculus we can easily get strikingly accurate bounds: $$\zeta(2) = \sum_{j = 1}^{n}\frac 1 {j^2} + \frac {t(n)}{n} + \frac{1 - t(n)}{n+1}$$ (for all $n$ ) for some $$0 < t(n) < 1.$$ Proof: This follows from $$\sum_{j = n}^{\infty}\frac 1 {j^2} = \int_n^\infty \frac 1 {\lfloor x \rfloor^2} \quad dx.$$ This formula gives at the midpoint ( $t = \frac 1 2$ ) a very accurate estimate, with $< 0.23\%$ relative error after just a few terms ( $n = 4$ ).  It seems to be both simpler and more accurate than some of the other elementary approaches tried : What's striking is that $t$ clearly seems to approach $1/2$ as $n \to \infty$ : as this plot suggests: Question Can we prove (or even just explain intuitively) that as $n \to \infty, t(n) \to 1/2$ ? Can we derive a formula or estimate for $t(n)$ (such as e.g. $t(n) = \frac 1 2 - \frac 1 {n^{6/5}} + ...$ )? Can this be used to solve the Basel Problem?","Although the Basel Problem took a hundred years to solve analytically, using elementary calculus we can easily get strikingly accurate bounds: (for all ) for some Proof: This follows from This formula gives at the midpoint ( ) a very accurate estimate, with relative error after just a few terms ( ).  It seems to be both simpler and more accurate than some of the other elementary approaches tried : What's striking is that clearly seems to approach as : as this plot suggests: Question Can we prove (or even just explain intuitively) that as ? Can we derive a formula or estimate for (such as e.g. )? Can this be used to solve the Basel Problem?","\zeta(2) = \sum_{j = 1}^{\infty}\frac 1 {j^2} \zeta(2) = \sum_{j = 1}^{n}\frac 1 {j^2} + \frac {t(n)}{n} + \frac{1 - t(n)}{n+1} n 0 < t(n) < 1. \sum_{j = n}^{\infty}\frac 1 {j^2} = \int_n^\infty \frac 1 {\lfloor x \rfloor^2} \quad dx. t = \frac 1 2 < 0.23\% n = 4 t 1/2 n \to \infty n \to \infty, t(n) \to 1/2 t(n) t(n) = \frac 1 2 - \frac 1 {n^{6/5}} + ...","['real-analysis', 'sequences-and-series', 'number-theory', 'asymptotics', 'analytic-number-theory']"
78,"Taylor polynomial: the higher the degree, the better the approximation?","Taylor polynomial: the higher the degree, the better the approximation?",,"Let $f$ be an infinite times differentiable function. Is it true that: the higher the degree $n$ of the Taylor polynomial $T_{n,f,x_0}$ of $f$ around $x_0$ , the better the approximation? Some thoughts . Given $n$ , polynomial $T_{n,f,x_0}$ is the best approximation of $f$ near $x_0$ that fulfils the requirement of equal derivatives with $f$ at $x_0$ . So regarding polynomials of degree  at most $n$ , $T_{n,f,x_0}$ is the winner. On the other hand, although one would like $T_{n,f,x_0}$ to fit better function $f$ , as $n$ grows larger, it seems to me that there is no reason for this to happen. Of course, one should define what ""fit better"" means. In our case, it would be something like: $$\sup_{x\in I}|T_{n+1,f,x_0}(x)-f(x)|\leqslant \sup_{x\in I}|T_{n,f,x_0}(x)-f(x)|$$ in a neighborhood $I$ of $x_0$ . Of course, I must admit that the cases I see graphically most of the times, fulfill the last requirement, by fitting better and better the graph of $f.$ Thank in advance for the help.","Let be an infinite times differentiable function. Is it true that: the higher the degree of the Taylor polynomial of around , the better the approximation? Some thoughts . Given , polynomial is the best approximation of near that fulfils the requirement of equal derivatives with at . So regarding polynomials of degree  at most , is the winner. On the other hand, although one would like to fit better function , as grows larger, it seems to me that there is no reason for this to happen. Of course, one should define what ""fit better"" means. In our case, it would be something like: in a neighborhood of . Of course, I must admit that the cases I see graphically most of the times, fulfill the last requirement, by fitting better and better the graph of Thank in advance for the help.","f n T_{n,f,x_0} f x_0 n T_{n,f,x_0} f x_0 f x_0 n T_{n,f,x_0} T_{n,f,x_0} f n \sup_{x\in I}|T_{n+1,f,x_0}(x)-f(x)|\leqslant \sup_{x\in I}|T_{n,f,x_0}(x)-f(x)| I x_0 f.","['real-analysis', 'calculus', 'analysis', 'taylor-expansion']"
79,Relation between two rational sequences approximating square root 2,Relation between two rational sequences approximating square root 2,,"We define recursive sequences $a_{n+1}=1+\frac 1{1+a_n}$ , $a_1=1$ and $b_{n+1}=\frac{b_n^2+2}{2b_n}$ , $b_1=1$ . I wish to show that $b_{n+1}=a_{2^n}$ . This can be proven using closed forms expressions related to continued fractions. I know that $a_n$ can be expressed as $$a_n=\sqrt2\cdot  \frac{(1+\sqrt 2)^n +(1-\sqrt 2)^n}{(1+\sqrt 2)^n - (1-\sqrt 2)^n}$$ On the other hand, we can prove inductively that $$\frac{b_{n+1}-\sqrt 2}{b_{n+1}+\sqrt 2}=\left(\frac{1-\sqrt 2}{1+\sqrt 2}\right)^{2^n}$$ So the relation $a_{2^n}=b_{n+1}$ can be deduced by expanding the fractions. However the computation is rather tedious, I am looking for a proof that does not involve expanding everything into closed form expressions. Thanks.","We define recursive sequences , and , . I wish to show that . This can be proven using closed forms expressions related to continued fractions. I know that can be expressed as On the other hand, we can prove inductively that So the relation can be deduced by expanding the fractions. However the computation is rather tedious, I am looking for a proof that does not involve expanding everything into closed form expressions. Thanks.",a_{n+1}=1+\frac 1{1+a_n} a_1=1 b_{n+1}=\frac{b_n^2+2}{2b_n} b_1=1 b_{n+1}=a_{2^n} a_n a_n=\sqrt2\cdot  \frac{(1+\sqrt 2)^n +(1-\sqrt 2)^n}{(1+\sqrt 2)^n - (1-\sqrt 2)^n} \frac{b_{n+1}-\sqrt 2}{b_{n+1}+\sqrt 2}=\left(\frac{1-\sqrt 2}{1+\sqrt 2}\right)^{2^n} a_{2^n}=b_{n+1},"['real-analysis', 'sequences-and-series', 'recurrence-relations']"
80,Is the result for $3\sum\limits_{n=1}^\infty\frac{H_nH_n^{(2)}}{n^6}+\sum\limits_{n=1}^\infty\frac{H_nH_n^{(3)}}{n^5}$ known in the literature?,Is the result for  known in the literature?,3\sum\limits_{n=1}^\infty\frac{H_nH_n^{(2)}}{n^6}+\sum\limits_{n=1}^\infty\frac{H_nH_n^{(3)}}{n^5},I was able to get the following result $$3\sum\limits_{n=1}^\infty\frac{H_nH_n^{(2)}}{n^6}+\sum\limits_{n=1}^\infty\frac{H_nH_n^{(3)}}{n^5}=11\zeta(3)\zeta(6)+\frac52\zeta(4)\zeta(5)-\frac{13}{6}\zeta^3(3)-2\zeta(2)\zeta(7)-5\zeta(9)$$ where $H_n^{(p)}=1+\frac1{2^p}+\cdots+\frac1{n^p}$ is the $n$ th generalized harmonic number of order $p$ . based on a nice identity and some manageable Euler sums.  Is this result known in the literature?  Can we evaluate the terms separately?,I was able to get the following result where is the th generalized harmonic number of order . based on a nice identity and some manageable Euler sums.  Is this result known in the literature?  Can we evaluate the terms separately?,3\sum\limits_{n=1}^\infty\frac{H_nH_n^{(2)}}{n^6}+\sum\limits_{n=1}^\infty\frac{H_nH_n^{(3)}}{n^5}=11\zeta(3)\zeta(6)+\frac52\zeta(4)\zeta(5)-\frac{13}{6}\zeta^3(3)-2\zeta(2)\zeta(7)-5\zeta(9) H_n^{(p)}=1+\frac1{2^p}+\cdots+\frac1{n^p} n p,"['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'harmonic-numbers']"
81,Evaluating $\int_{1}^{\infty}\frac{1}{x^2}\prod_{k=1}^{n}\left[1-\frac{1}{2k+x}\right]\text{d}x$,Evaluating,\int_{1}^{\infty}\frac{1}{x^2}\prod_{k=1}^{n}\left[1-\frac{1}{2k+x}\right]\text{d}x,"I'm trying to find a closed form for the following integral, for all $n\in\mathbb{N}^*$ : $$\int_{1}^{\infty}\frac{1}{x^2}\prod_{k=1}^{n}\left[1-\frac{1}{2k+x}\right]\text{d}x$$ As suggested by Zacky, a $x=\frac{1}{t}$ substitution gives : $$\int_{0}^{1}\prod_{k=1}^{n}\left[\frac{(2k-1)x+1}{2kx+1}\right]\text{d}x$$ The thing is that, for every specific $n\in\mathbb{N}^*$ , mathematica is able to compute the integral in terms of logarithms only. In fact, it can always find the antiderivative in terms of logarithms. What I'm looking for is a general expression for all $n\in\mathbb{N}^*$ , if given that it exists. Now the integrand is an already factorized rational function, so I assumed I could tackle this with some partial fraction decomposition, but I'm not good enough with this method to lead to anything. I'm also not familiar with complex analysis so I haven't tried contour integration, but if you think it can lead to anything, feel free to do so. Any suggestion ?","I'm trying to find a closed form for the following integral, for all : As suggested by Zacky, a substitution gives : The thing is that, for every specific , mathematica is able to compute the integral in terms of logarithms only. In fact, it can always find the antiderivative in terms of logarithms. What I'm looking for is a general expression for all , if given that it exists. Now the integrand is an already factorized rational function, so I assumed I could tackle this with some partial fraction decomposition, but I'm not good enough with this method to lead to anything. I'm also not familiar with complex analysis so I haven't tried contour integration, but if you think it can lead to anything, feel free to do so. Any suggestion ?",n\in\mathbb{N}^* \int_{1}^{\infty}\frac{1}{x^2}\prod_{k=1}^{n}\left[1-\frac{1}{2k+x}\right]\text{d}x x=\frac{1}{t} \int_{0}^{1}\prod_{k=1}^{n}\left[\frac{(2k-1)x+1}{2kx+1}\right]\text{d}x n\in\mathbb{N}^* n\in\mathbb{N}^*,"['real-analysis', 'integration', 'definite-integrals']"
82,find the value of $p(-\pi)$ and $p(0)$?,find the value of  and ?,p(-\pi) p(0),Let $p(x)$ be a polynomial of degree $7$ with real coefficient such that $p(\pi)=\sqrt3.$ and $$\int_{-\pi}^{\pi}x^{k}p(x)dx=0$$ for $0\leq k \leq 6$. Then find the value of $p(-\pi)$ and $p(0)$? If I take general polynomial then it is difficult to find and also time consuming so please help to solve. Thanks,Let $p(x)$ be a polynomial of degree $7$ with real coefficient such that $p(\pi)=\sqrt3.$ and $$\int_{-\pi}^{\pi}x^{k}p(x)dx=0$$ for $0\leq k \leq 6$. Then find the value of $p(-\pi)$ and $p(0)$? If I take general polynomial then it is difficult to find and also time consuming so please help to solve. Thanks,,"['real-analysis', 'analysis', 'polynomials']"
83,Integral $I_m=\int_0^1 \frac{x^m-x}{\sin \pi x} \:dx $ expressed as finite sum of $\frac{r_n \zeta(2n+1)}{\pi^{2n+1}}$,Integral  expressed as finite sum of,I_m=\int_0^1 \frac{x^m-x}{\sin \pi x} \:dx  \frac{r_n \zeta(2n+1)}{\pi^{2n+1}},"The integral \begin{equation} I_m=\int_0^1 \frac{x^m-x}{\sin \pi x} \:dx \end{equation} is expressed as a finite sum of terms of the shape $r_n \zeta(2n+1) \pi^{-(2n+1)}$, where $r_n$ is a rational number that depends on $n$. In this question Evaluating $\int_0^1 \frac{x^2-x}{\sin \pi x} dx = - \frac{7 \zeta(3)}{\pi^3}.$ The user ""user90369"" provided a answer for $I_2$. So I followed his steps and generalized the integral. Define: \begin{align} &f(a) = \int_0^1 x e^{ax} dx= \frac{1+e^a(a-1)}{a^2} \\ &g(a) = \int_0^1 x^m e^{ax} dx = \frac{e^a}{a^{m+1}} \left(  (-1)^mm!  + \sum_{n=1}^{m}  (-1)^{m+n} \frac {a^n}{n!} \right) - \frac{(-1)^mm!}{a^{m+1}}	 \end{align} Rewrite $I_m$ as: \begin{align} I_m &= \int_0^1 \frac{x^m-x}{\sin \pi x} = 2i \int_0^1 \frac{x^m-x}{ e^{i\pi x } - e^{-i \pi x} }\nonumber\\ &= 2i \sum_{k=0}^{\infty} \int_0^1 (x^m-x)e^{-i\pi x(2k+1) } dx \\ &= 2i \sum_{k=0}^{\infty} g(-i\pi(2k+1)) - f(-i\pi (2k+1)) \nonumber \end{align} From that I arrived at the following formulas for $I_m$. When $m$ is even and $\geq 2$: \begin{align*} I_m =  \frac{4m!i^m}{\pi^{m+1}} \left( 1-\frac{1}{2^{m+1}}\right)  \zeta(m+1) + 2 m! \sum_{n\:odd \geq 0}^{m-2} \frac{i\: \zeta (m-n)}{(n+1)!(i\pi)^{m-n}}   \left(  1-\frac{1}{2^{m-n}} \right) \end{align*} When $m$ is odd and $\geq 3$: \begin{align*} I_m = 2 m! \sum_{n\:even \geq 0}^{m-2} \frac{i\: \zeta (m-n)}{(n+1)!(i\pi)^{m-n}}   \left(  1-\frac{1}{2^{m-n}} \right). \end{align*} A few values for $I_m$: \begin{align*} & I_2 = - \frac{7\zeta(3)}{\pi^3} \\ & I_3 = -\frac{21 \zeta(3)}{2\pi^3}\\ & I_4 =  \frac{93\zeta(5)}{\pi^5}-\frac{21\zeta(3)}{\pi^3}	\\ & I_5 =  \frac{465\zeta(5)}{2\pi^5} - \frac{35\zeta(3)}{\pi^3}	\\ & I_6 = -\frac{1}{2} \left(  \frac{5715\zeta(7)}{\pi^7} - \frac{1395\zeta(5)}{\pi^5} + \frac{105\zeta(3)}{\pi^3}  \right) \\ & I_7 = -\frac{1}{2} \left(  \frac{40005\zeta(7)}{2\pi^7} - \frac{3255\zeta(5)}{\pi^5} + \frac{147\zeta(3)}{\pi^3} \right) \\ &I_8 = \frac{160965\zeta(9)}{\pi^9} - \frac{40005\zeta(7)}{\pi^7} + \frac{3255\zeta(5)}{\pi^5}-\frac{98\zeta(3)}{\pi^3} \end{align*} It's also possible to integrate by parts $I_m$. Anyway, I found this integral interesting and wanted to share. Perphaps someone can do something cool with it, or not. Sorry for the weird/sloppy english.","The integral \begin{equation} I_m=\int_0^1 \frac{x^m-x}{\sin \pi x} \:dx \end{equation} is expressed as a finite sum of terms of the shape $r_n \zeta(2n+1) \pi^{-(2n+1)}$, where $r_n$ is a rational number that depends on $n$. In this question Evaluating $\int_0^1 \frac{x^2-x}{\sin \pi x} dx = - \frac{7 \zeta(3)}{\pi^3}.$ The user ""user90369"" provided a answer for $I_2$. So I followed his steps and generalized the integral. Define: \begin{align} &f(a) = \int_0^1 x e^{ax} dx= \frac{1+e^a(a-1)}{a^2} \\ &g(a) = \int_0^1 x^m e^{ax} dx = \frac{e^a}{a^{m+1}} \left(  (-1)^mm!  + \sum_{n=1}^{m}  (-1)^{m+n} \frac {a^n}{n!} \right) - \frac{(-1)^mm!}{a^{m+1}}	 \end{align} Rewrite $I_m$ as: \begin{align} I_m &= \int_0^1 \frac{x^m-x}{\sin \pi x} = 2i \int_0^1 \frac{x^m-x}{ e^{i\pi x } - e^{-i \pi x} }\nonumber\\ &= 2i \sum_{k=0}^{\infty} \int_0^1 (x^m-x)e^{-i\pi x(2k+1) } dx \\ &= 2i \sum_{k=0}^{\infty} g(-i\pi(2k+1)) - f(-i\pi (2k+1)) \nonumber \end{align} From that I arrived at the following formulas for $I_m$. When $m$ is even and $\geq 2$: \begin{align*} I_m =  \frac{4m!i^m}{\pi^{m+1}} \left( 1-\frac{1}{2^{m+1}}\right)  \zeta(m+1) + 2 m! \sum_{n\:odd \geq 0}^{m-2} \frac{i\: \zeta (m-n)}{(n+1)!(i\pi)^{m-n}}   \left(  1-\frac{1}{2^{m-n}} \right) \end{align*} When $m$ is odd and $\geq 3$: \begin{align*} I_m = 2 m! \sum_{n\:even \geq 0}^{m-2} \frac{i\: \zeta (m-n)}{(n+1)!(i\pi)^{m-n}}   \left(  1-\frac{1}{2^{m-n}} \right). \end{align*} A few values for $I_m$: \begin{align*} & I_2 = - \frac{7\zeta(3)}{\pi^3} \\ & I_3 = -\frac{21 \zeta(3)}{2\pi^3}\\ & I_4 =  \frac{93\zeta(5)}{\pi^5}-\frac{21\zeta(3)}{\pi^3}	\\ & I_5 =  \frac{465\zeta(5)}{2\pi^5} - \frac{35\zeta(3)}{\pi^3}	\\ & I_6 = -\frac{1}{2} \left(  \frac{5715\zeta(7)}{\pi^7} - \frac{1395\zeta(5)}{\pi^5} + \frac{105\zeta(3)}{\pi^3}  \right) \\ & I_7 = -\frac{1}{2} \left(  \frac{40005\zeta(7)}{2\pi^7} - \frac{3255\zeta(5)}{\pi^5} + \frac{147\zeta(3)}{\pi^3} \right) \\ &I_8 = \frac{160965\zeta(9)}{\pi^9} - \frac{40005\zeta(7)}{\pi^7} + \frac{3255\zeta(5)}{\pi^5}-\frac{98\zeta(3)}{\pi^3} \end{align*} It's also possible to integrate by parts $I_m$. Anyway, I found this integral interesting and wanted to share. Perphaps someone can do something cool with it, or not. Sorry for the weird/sloppy english.",,"['real-analysis', 'integration', 'definite-integrals', 'riemann-zeta', 'trigonometric-integrals']"
84,How to prove this limit does not exist,How to prove this limit does not exist,,"I'm a relative novice to epsilon-delta proofs. My professor assigned this practice problem and I'm having terrible trouble understanding the answer he gave. Moreover, I can't find a good account for a general strategy for how to do these sorts of proofs. I understand the epsilon delta definition, I understand what I'm supposed to do, but I need advice on strategy. The actual problem. Prove that the limit $$ \lim_{x \rightarrow 2} \frac{x^3}{x-2} $$ does not exist. So far I'm pretty stumped; I know I need to show that there is some $\epsilon$ st. such that x being arbitrarily close to 2 does not guarantee that f(x) is within epsilon of L, but that's all I've got. Thanks.","I'm a relative novice to epsilon-delta proofs. My professor assigned this practice problem and I'm having terrible trouble understanding the answer he gave. Moreover, I can't find a good account for a general strategy for how to do these sorts of proofs. I understand the epsilon delta definition, I understand what I'm supposed to do, but I need advice on strategy. The actual problem. Prove that the limit $$ \lim_{x \rightarrow 2} \frac{x^3}{x-2} $$ does not exist. So far I'm pretty stumped; I know I need to show that there is some $\epsilon$ st. such that x being arbitrarily close to 2 does not guarantee that f(x) is within epsilon of L, but that's all I've got. Thanks.",,"['real-analysis', 'limits', 'epsilon-delta']"
85,Sum of a series indexed by ordinals,Sum of a series indexed by ordinals,,"If $\mu$ is an ordinal, how can we formalize that $$   \sum_{\lambda<\mu}x_{\lambda}=z $$ When $\mu=\omega$, this is just the usual infinite series, the partial sums converge to $z$.  What is the definition for higher ordinals? I don't think this is quite the same as an uncountable sum, it is a sum over an well-ordered index. We can define it as a limit points of finite sums, but I was wondering whether there is a ""better"" (possibly equivalent) definition that takes advantage of the extra structure (well-oredring of the index). I suspect this will make sense only if countably many elements are non-zero. As for the context, I guess the most general one could be that of topological vector spaces, so that we have the vector operations and limits.","If $\mu$ is an ordinal, how can we formalize that $$   \sum_{\lambda<\mu}x_{\lambda}=z $$ When $\mu=\omega$, this is just the usual infinite series, the partial sums converge to $z$.  What is the definition for higher ordinals? I don't think this is quite the same as an uncountable sum, it is a sum over an well-ordered index. We can define it as a limit points of finite sums, but I was wondering whether there is a ""better"" (possibly equivalent) definition that takes advantage of the extra structure (well-oredring of the index). I suspect this will make sense only if countably many elements are non-zero. As for the context, I guess the most general one could be that of topological vector spaces, so that we have the vector operations and limits.",,"['real-analysis', 'functional-analysis', 'logic']"
86,"Is $\{n\pmod \pi: n \in \Bbb N\}$ dense in $[0,\pi]$?",Is  dense in ?,"\{n\pmod \pi: n \in \Bbb N\} [0,\pi]","Is $\{n\pmod\pi: n\in\Bbb N\}$ dense in $[0,\pi]$ ? Is there a proof or well known theorem for this result? My intuition would say that it is dense.",Is dense in ? Is there a proof or well known theorem for this result? My intuition would say that it is dense.,"\{n\pmod\pi: n\in\Bbb N\} [0,\pi]",['real-analysis']
87,Saturation of a measure Folland Problem 1.3.16,Saturation of a measure Folland Problem 1.3.16,,"Exercise 16 - Let $(X,M,\mu)$ be a measure space. A set $E\subset X$ is called locally measurable if $E\cap A\in M$ for all $A\in M$ such that $\mu(A) < \infty$. Let $\tilde{M}$ be the collection of all locally measurable sets. Clearly, $M\subset \tilde{M}$, if $M = \tilde{M}$, then $\mu$ is saturated. a.) If $\mu$ is $\sigma$-finite, then $\mu$ is saturated. Proof - Suppose $\mu$ is $\sigma$-finite. Let $A\in\tilde{M}$, and let $X = \bigcup_{1}^{\infty}E_j$ where $E_j\in M$ and $\mu(E_j) < \infty$ for all $j$. We know $M\subset \tilde{M}$ what we want to show is that $\tilde{M}\subset M$. We can write $$A = A\cap X = A\cap \left(\bigcup_{1}^{\infty}E_j\right) = \bigcup_{1}^{\infty}E_j\cap A$$ Since $\mu(E_j)<\infty$ we have $E_j\cap A\in M$ for all $j$. Therefore, $\tilde{M}\subset M$, thus $\mu$ is saturated. b.) $\tilde{M}$ is $\sigma$-algebra. Proof - i.) $\emptyset\in M\subset \tilde{M}$, so $\emptyset\in \tilde{M}$. ii.) Let $B\in \tilde{M}$. Take any $E\in M$ such that $\mu(E) < \infty$. Then $$E\setminus B = E\cap B^c = E\cap (E\cap B)^c$$ since $E\in M$ and $(E\cap B)\in M$ then $(E\cap (E\cap B)^c\in M$. Thus we have $B^c\in \tilde{M}$. iii.) Let $\{B_j\}_{1}^{\infty}\in \tilde{M}$. Take any $E\in M$ with $\mu(E)< \infty$. Then, $$\left(\bigcup_{1}^{\infty}B_j\right)\cap E = \bigcup_{1}^{\infty}(B_j\cap E)\in M$$  so, by definition of $\tilde{M}$, $\bigcup_{1}^{\infty}B_j\in \tilde{M}$. Therefore $\tilde{M}$ us a $\sigma$-algebra. c.) Define $\tilde{\mu}$ on $\tilde{M}$ by $\tilde{\mu}(E) = \mu(E)$ if $E\in M$ and $\tilde{\mu}(E) = \infty$ otherwise. Then $\tilde{\mu}$ is a saturated measure on $\tilde{M}$, called the saturation of $\mu$. Step 1: Show that $\tilde{\mu}$ is a measure on $\tilde{M}$. Proof - i.) $\tilde{\mu}(\emptyset) = \mu(\emptyset) = 0$. ii.) Let $\{E_j\}_{1}^{\infty}\in \tilde{M}$ that is pairwise disjoint. Let $$E = \bigcup_{1}^{\infty}E_j$$ If $E \in M$ then \begin{align*} \tilde{\mu}(E) = \tilde{\mu}\left(\bigcup_{1}^{\infty}E_j\right) &= \mu\left(\bigcup_{1}^{\infty}E_j\right)\\ &= \sum_{1}^{\infty}\mu(E_j)\\ &= \sum_{1}^{\infty}\tilde{\mu}(E_j) \end{align*} If $E\notin M$ then $\tilde{\mu}(E) = \infty$... not sure where to go from here. Step 2 - $\tilde{\mu}$ is saturated. Proof - Let $E\subset X$ such that $E\cap A\in \tilde{M}$ when $\tilde{\mu}(A) < \infty$. Choose a $B\in M$ such that $\mu(B)<\infty$. Then, clearly $\tilde{\mu}(B)<\infty$ and $E\cap B\in \tilde{M}$. So, $E\cap B = (E\cap B)\cap B\in M$ so $E\in \tilde{M}$ it thus follows that $\tilde{\mu}$ is saturated. d.) If $\mu$ is complete, so is $\tilde{\mu}$. Proof - Suppose $\mu$ is complete. Let $A\subset X$ and suppose there is a $B\in \tilde{M}$ such that $A\subset B$ and $\mu(B) = 0$. Since $B\in\tilde{M}$ and $\mu(B) = 0$ then $\tilde{\mu}(B) < \infty$ and hence $B\in M$. This, since $A\subset B$ we have $A\in M$ by completeness of $\mu$. Therefore, $A\in \tilde{M}$ and $\tilde{\mu}$ is complete. e.) Suppose that $\mu$ is semifinite. For $E\in\tilde{M}$, define $\underline{\mu}(E) = \sup\{\mu(A):A\in M, A\subset E\}$. Then $\underline{\mu}$ is a saturated measure on $\tilde{M}$ that extends $\mu$. Step 1 - $\underline{\mu}$ is a measure. Proof - i.) $\overline{\mu}(\emptyset) = \mu(\emptyset) = 0$ ii.) Let $\{E_j\}_{1}^{\infty}$ be a sequence of disjoint sets in $\tilde{M}$. Set, $$E = \bigcup_{1}^{\infty}E_j$$ then by definition of $\tilde{M}$ there is an $A\in M$ and $A\subset E$. Case 1 - $\mu(A) < \infty$. Then $$\mu(A) = \mu\left(\bigcup_{1}^{\infty}E_j\cap A\right) = \sum_{1}^{\infty}\mu(E_j\cap A)\leq \sum_{1}^{\infty}\underline{\mu}(E_j)$$ Case 2 - $\mu(A) = \infty$. By semifiniteness, for all $C>0$ there exists a $F\subset A$ such that $F\in M$ and $\mu(F) = C$. Then by case 1, $\leq \sum_{1}^{\infty}\underline{\mu}(E_j) = \infty$. Therefore, $\mu(A) \leq \sum_{1}^{\infty}\underline{\mu}(E_j)$. Taking the supremum over all $A$ we have $$\underline{\mu}\left(\bigcup_{1}^{\infty}E_j\right)\leq \sum_{1}^{\infty}\underline{\mu}(E_j)$$ Now we need to show the reverse inequality. By the definition of supremum there exists a sequence $\{B_i\}_{1}^{\infty}\in M$ and $B_i\subset E_i$ for all $i$. Thus, $\underline{\mu}(E_i)\leq \mu(B_i) + \epsilon 2^{-i}$. Therefore,  \begin{align*} \sum_{1}^{\infty}\underline{\mu}(E_i) &\leq \sum_{1}^{\infty}\mu(B_i) + \epsilon\\ &= \mu\left(\bigcup_{1}^{\infty}B_i\right) + \epsilon \ \ \text{is this true because of case 1?}\\ &\leq \underline{\mu}\left(\bigcup_{1}^{\infty}E_i\right) + \epsilon \end{align*} Since this holds for all $\epsilon > 0$, we have $$\sum_{1}^{\infty}\underline{\mu}(E_j)\leq \underline{\mu}\left(\bigcup_{1}^{\infty}E_j\right)$$ Therefore, $$\underline{\mu}\left(\bigcup_{1}^{\infty}E_j\right) = \sum_{1}^{\infty}\underline{\mu}(E_j)$$ and hence $\underline{\mu}$ is a measure. Step 2 - $\underline{\mu}$ is saturated. Proof - Let $E\subset X$ be such that $E\cap A\in \tilde{M}$ when $\underline{\mu}(A)< \infty$. Take any $B\in M$ such that $\mu(B) < \infty$. Then $\underline{\mu}(B) < \infty$ so $E\cap B\in \tilde{M}$. Thus, $E\cap B = (E\cap B)\cap B\in M$ and $E\in\tilde{M}$, hence, $\underline{\mu}$ is saturated. Step 3 - $\underline{\mu}$ is an extention of $\mu$. Proof - Let $E\in M$. For any $A\in M$ such that $A\subset E$, we have by monotonicity that $\mu(A)\leq \mu(E)$. Since $\underline{\mu}(E)$ is the supremum over all such $A$, we must have that $\underline{\mu}(E)\leq \mu(E)$. OTOH.... not sure how to show the reverse inequality. f.) Let $X_1$ and $X_2$ be disjoint uncountable sets, $X = X_1\cup X_2$, and $M$ the $\sigma$-alegbra of countable or co-countable sets in $X$. Let $\mu_0$ be counting measure on $\mathcal{P}(X_1)$ and define $\mu$ on $M$ by $\mu(E) = \mu_0(E\cap X_1)$. Then $\mu$ is a measure on $M$, $\tilde{M} = \mathcal{P}(X)$, and in the notation of parts (c) and (e), $\tilde{\mu}\neq \underline{\mu}$. Step 1 - $\mu$ is a measure on $M$. Proof - i.) $\mu(\emptyset) = \mu_0(\emptyset\cap X_1) = 0$ ii.) Let $\{E_j\}_{1}^{\infty}$ be a sequence of disjoint sets in $M$, then  \begin{align*} \mu\left(\bigcup_{1}^{\infty}E_j\right) &= \mu_0\left(\bigcup_{1}^{\infty}E_j\cap X_1\right)\\ &= \sum_{1}^{\infty}\mu_0(E_j\cap X_1)\\ &= \sum_{1}^{\infty}\mu(E_j) \ \ \ \text{is this true because} \ \mu_0 \ \text{is a counting measure?} \end{align*} Therefore $\mu$ is a measure on $M$. Step 2 - $\tilde{M} = \mathcal{P}(X)$ Proof - Step 3 - $\tilde{\mu}\neq \underline{\mu}$ Proof - Take $y_1,y_2\in X_1$. Let $E = \{y_1,y_2\}\cup X_2$. Then $E\notin M$, so $\tilde{\mu}(E) = \infty$. However, $\underline{\mu}(E) = 2$. I will re-edit my question and include these other proofs as I continue to do them. I am pretty sure my proof for $\tilde{\mu}$ is a measure is incorrect. But I am not really sure how to do it. Any suggestions on any of these is greatly appreciated.","Exercise 16 - Let $(X,M,\mu)$ be a measure space. A set $E\subset X$ is called locally measurable if $E\cap A\in M$ for all $A\in M$ such that $\mu(A) < \infty$. Let $\tilde{M}$ be the collection of all locally measurable sets. Clearly, $M\subset \tilde{M}$, if $M = \tilde{M}$, then $\mu$ is saturated. a.) If $\mu$ is $\sigma$-finite, then $\mu$ is saturated. Proof - Suppose $\mu$ is $\sigma$-finite. Let $A\in\tilde{M}$, and let $X = \bigcup_{1}^{\infty}E_j$ where $E_j\in M$ and $\mu(E_j) < \infty$ for all $j$. We know $M\subset \tilde{M}$ what we want to show is that $\tilde{M}\subset M$. We can write $$A = A\cap X = A\cap \left(\bigcup_{1}^{\infty}E_j\right) = \bigcup_{1}^{\infty}E_j\cap A$$ Since $\mu(E_j)<\infty$ we have $E_j\cap A\in M$ for all $j$. Therefore, $\tilde{M}\subset M$, thus $\mu$ is saturated. b.) $\tilde{M}$ is $\sigma$-algebra. Proof - i.) $\emptyset\in M\subset \tilde{M}$, so $\emptyset\in \tilde{M}$. ii.) Let $B\in \tilde{M}$. Take any $E\in M$ such that $\mu(E) < \infty$. Then $$E\setminus B = E\cap B^c = E\cap (E\cap B)^c$$ since $E\in M$ and $(E\cap B)\in M$ then $(E\cap (E\cap B)^c\in M$. Thus we have $B^c\in \tilde{M}$. iii.) Let $\{B_j\}_{1}^{\infty}\in \tilde{M}$. Take any $E\in M$ with $\mu(E)< \infty$. Then, $$\left(\bigcup_{1}^{\infty}B_j\right)\cap E = \bigcup_{1}^{\infty}(B_j\cap E)\in M$$  so, by definition of $\tilde{M}$, $\bigcup_{1}^{\infty}B_j\in \tilde{M}$. Therefore $\tilde{M}$ us a $\sigma$-algebra. c.) Define $\tilde{\mu}$ on $\tilde{M}$ by $\tilde{\mu}(E) = \mu(E)$ if $E\in M$ and $\tilde{\mu}(E) = \infty$ otherwise. Then $\tilde{\mu}$ is a saturated measure on $\tilde{M}$, called the saturation of $\mu$. Step 1: Show that $\tilde{\mu}$ is a measure on $\tilde{M}$. Proof - i.) $\tilde{\mu}(\emptyset) = \mu(\emptyset) = 0$. ii.) Let $\{E_j\}_{1}^{\infty}\in \tilde{M}$ that is pairwise disjoint. Let $$E = \bigcup_{1}^{\infty}E_j$$ If $E \in M$ then \begin{align*} \tilde{\mu}(E) = \tilde{\mu}\left(\bigcup_{1}^{\infty}E_j\right) &= \mu\left(\bigcup_{1}^{\infty}E_j\right)\\ &= \sum_{1}^{\infty}\mu(E_j)\\ &= \sum_{1}^{\infty}\tilde{\mu}(E_j) \end{align*} If $E\notin M$ then $\tilde{\mu}(E) = \infty$... not sure where to go from here. Step 2 - $\tilde{\mu}$ is saturated. Proof - Let $E\subset X$ such that $E\cap A\in \tilde{M}$ when $\tilde{\mu}(A) < \infty$. Choose a $B\in M$ such that $\mu(B)<\infty$. Then, clearly $\tilde{\mu}(B)<\infty$ and $E\cap B\in \tilde{M}$. So, $E\cap B = (E\cap B)\cap B\in M$ so $E\in \tilde{M}$ it thus follows that $\tilde{\mu}$ is saturated. d.) If $\mu$ is complete, so is $\tilde{\mu}$. Proof - Suppose $\mu$ is complete. Let $A\subset X$ and suppose there is a $B\in \tilde{M}$ such that $A\subset B$ and $\mu(B) = 0$. Since $B\in\tilde{M}$ and $\mu(B) = 0$ then $\tilde{\mu}(B) < \infty$ and hence $B\in M$. This, since $A\subset B$ we have $A\in M$ by completeness of $\mu$. Therefore, $A\in \tilde{M}$ and $\tilde{\mu}$ is complete. e.) Suppose that $\mu$ is semifinite. For $E\in\tilde{M}$, define $\underline{\mu}(E) = \sup\{\mu(A):A\in M, A\subset E\}$. Then $\underline{\mu}$ is a saturated measure on $\tilde{M}$ that extends $\mu$. Step 1 - $\underline{\mu}$ is a measure. Proof - i.) $\overline{\mu}(\emptyset) = \mu(\emptyset) = 0$ ii.) Let $\{E_j\}_{1}^{\infty}$ be a sequence of disjoint sets in $\tilde{M}$. Set, $$E = \bigcup_{1}^{\infty}E_j$$ then by definition of $\tilde{M}$ there is an $A\in M$ and $A\subset E$. Case 1 - $\mu(A) < \infty$. Then $$\mu(A) = \mu\left(\bigcup_{1}^{\infty}E_j\cap A\right) = \sum_{1}^{\infty}\mu(E_j\cap A)\leq \sum_{1}^{\infty}\underline{\mu}(E_j)$$ Case 2 - $\mu(A) = \infty$. By semifiniteness, for all $C>0$ there exists a $F\subset A$ such that $F\in M$ and $\mu(F) = C$. Then by case 1, $\leq \sum_{1}^{\infty}\underline{\mu}(E_j) = \infty$. Therefore, $\mu(A) \leq \sum_{1}^{\infty}\underline{\mu}(E_j)$. Taking the supremum over all $A$ we have $$\underline{\mu}\left(\bigcup_{1}^{\infty}E_j\right)\leq \sum_{1}^{\infty}\underline{\mu}(E_j)$$ Now we need to show the reverse inequality. By the definition of supremum there exists a sequence $\{B_i\}_{1}^{\infty}\in M$ and $B_i\subset E_i$ for all $i$. Thus, $\underline{\mu}(E_i)\leq \mu(B_i) + \epsilon 2^{-i}$. Therefore,  \begin{align*} \sum_{1}^{\infty}\underline{\mu}(E_i) &\leq \sum_{1}^{\infty}\mu(B_i) + \epsilon\\ &= \mu\left(\bigcup_{1}^{\infty}B_i\right) + \epsilon \ \ \text{is this true because of case 1?}\\ &\leq \underline{\mu}\left(\bigcup_{1}^{\infty}E_i\right) + \epsilon \end{align*} Since this holds for all $\epsilon > 0$, we have $$\sum_{1}^{\infty}\underline{\mu}(E_j)\leq \underline{\mu}\left(\bigcup_{1}^{\infty}E_j\right)$$ Therefore, $$\underline{\mu}\left(\bigcup_{1}^{\infty}E_j\right) = \sum_{1}^{\infty}\underline{\mu}(E_j)$$ and hence $\underline{\mu}$ is a measure. Step 2 - $\underline{\mu}$ is saturated. Proof - Let $E\subset X$ be such that $E\cap A\in \tilde{M}$ when $\underline{\mu}(A)< \infty$. Take any $B\in M$ such that $\mu(B) < \infty$. Then $\underline{\mu}(B) < \infty$ so $E\cap B\in \tilde{M}$. Thus, $E\cap B = (E\cap B)\cap B\in M$ and $E\in\tilde{M}$, hence, $\underline{\mu}$ is saturated. Step 3 - $\underline{\mu}$ is an extention of $\mu$. Proof - Let $E\in M$. For any $A\in M$ such that $A\subset E$, we have by monotonicity that $\mu(A)\leq \mu(E)$. Since $\underline{\mu}(E)$ is the supremum over all such $A$, we must have that $\underline{\mu}(E)\leq \mu(E)$. OTOH.... not sure how to show the reverse inequality. f.) Let $X_1$ and $X_2$ be disjoint uncountable sets, $X = X_1\cup X_2$, and $M$ the $\sigma$-alegbra of countable or co-countable sets in $X$. Let $\mu_0$ be counting measure on $\mathcal{P}(X_1)$ and define $\mu$ on $M$ by $\mu(E) = \mu_0(E\cap X_1)$. Then $\mu$ is a measure on $M$, $\tilde{M} = \mathcal{P}(X)$, and in the notation of parts (c) and (e), $\tilde{\mu}\neq \underline{\mu}$. Step 1 - $\mu$ is a measure on $M$. Proof - i.) $\mu(\emptyset) = \mu_0(\emptyset\cap X_1) = 0$ ii.) Let $\{E_j\}_{1}^{\infty}$ be a sequence of disjoint sets in $M$, then  \begin{align*} \mu\left(\bigcup_{1}^{\infty}E_j\right) &= \mu_0\left(\bigcup_{1}^{\infty}E_j\cap X_1\right)\\ &= \sum_{1}^{\infty}\mu_0(E_j\cap X_1)\\ &= \sum_{1}^{\infty}\mu(E_j) \ \ \ \text{is this true because} \ \mu_0 \ \text{is a counting measure?} \end{align*} Therefore $\mu$ is a measure on $M$. Step 2 - $\tilde{M} = \mathcal{P}(X)$ Proof - Step 3 - $\tilde{\mu}\neq \underline{\mu}$ Proof - Take $y_1,y_2\in X_1$. Let $E = \{y_1,y_2\}\cup X_2$. Then $E\notin M$, so $\tilde{\mu}(E) = \infty$. However, $\underline{\mu}(E) = 2$. I will re-edit my question and include these other proofs as I continue to do them. I am pretty sure my proof for $\tilde{\mu}$ is a measure is incorrect. But I am not really sure how to do it. Any suggestions on any of these is greatly appreciated.",,"['real-analysis', 'measure-theory', 'proof-verification']"
88,Solutions manual for Analysis On Manifolds,Solutions manual for Analysis On Manifolds,,"A few months ago,I wanted to learn something fundmental about manifolds. From highly recommend , I decided to choice Analysis on Manifolds by James R.Munkres as my self-learning textbook.Until now ,I have finished the first two chapter's solutions. But I am not sure my answer to this exerices abusolutly right.Is there   some solutions manual for this book? Can anyone  provides free downloads ?  I appreciate it indeed!","A few months ago,I wanted to learn something fundmental about manifolds. From highly recommend , I decided to choice Analysis on Manifolds by James R.Munkres as my self-learning textbook.Until now ,I have finished the first two chapter's solutions. But I am not sure my answer to this exerices abusolutly right.Is there   some solutions manual for this book? Can anyone  provides free downloads ?  I appreciate it indeed!",,"['real-analysis', 'reference-request']"
89,When is it possible to have $f(x+y)=f(x)+f(y)+g(xy)$?,When is it possible to have ?,f(x+y)=f(x)+f(y)+g(xy),"According to the question mentioned here , it seems that there is no function $f(x)$ such that the functional equation $$f(x+y)=f(x)+f(y)-(xy-1)^2$$ can hold. Motivated by this question, I found it interesting to somehow extend the question. What conditions are required for a given function $g(x)$ such that there exists a function $f(x)$ that can satisfy the following equality   $$f(x+y)=f(x)+f(y)+g(xy)$$   where $f(x)$ and $g(x)$ are real valued functions of real variable. Any hint or help is appreciated. :)","According to the question mentioned here , it seems that there is no function $f(x)$ such that the functional equation $$f(x+y)=f(x)+f(y)-(xy-1)^2$$ can hold. Motivated by this question, I found it interesting to somehow extend the question. What conditions are required for a given function $g(x)$ such that there exists a function $f(x)$ that can satisfy the following equality   $$f(x+y)=f(x)+f(y)+g(xy)$$   where $f(x)$ and $g(x)$ are real valued functions of real variable. Any hint or help is appreciated. :)",,"['calculus', 'real-analysis', 'analysis', 'functional-equations']"
90,"Putnam 2015 B6, sum involving number of odd divisors on an interval.","Putnam 2015 B6, sum involving number of odd divisors on an interval.",,"For each positive integer $k$, let $A(k)$ be the number of odd divisors of $k$ in the interval $[1, \sqrt{2k})$. What is$$\sum_{k=1}^\infty (-1)^{k-1} {{A(k)}\over{k}}?$$","For each positive integer $k$, let $A(k)$ be the number of odd divisors of $k$ in the interval $[1, \sqrt{2k})$. What is$$\sum_{k=1}^\infty (-1)^{k-1} {{A(k)}\over{k}}?$$",,"['real-analysis', 'sequences-and-series']"
91,Suppose that $f: \mathbb{R^+}\to \mathbb{R}$ satisfies $\lim_{x\to \infty} (f+f')(x)=0$. Show that $\lim_{x\to \infty} f(x)=0$. [duplicate],Suppose that  satisfies . Show that . [duplicate],f: \mathbb{R^+}\to \mathbb{R} \lim_{x\to \infty} (f+f')(x)=0 \lim_{x\to \infty} f(x)=0,"This question already has answers here : How can this be proved $\lim_{x\to\infty}(f(x)+f'(x))=l$ [duplicate] (5 answers) Closed 8 years ago . Suppose that $f: \mathbb{R^+}\to \mathbb{R}$ satisfies $\lim_{x\to \infty} (f+f')(x)=0$. Show that $\lim_{x\to \infty} f(x)=0$. This is one solution I found to this problem. Solution: If $x=a$ is in the domain of the function $f$, then by the generalized mean value theorem applied to the functions $e^x f(x)$ and $e^x$, there exists $c\in (a,x)$ such that $\frac{e^x f(x)-e^a f(a)}{e^x -e^a}=\frac{e^c f'(c)+e^c f(c)}{e^c}.$ This yields $\frac{e^x}{e^x -e^a} f(x)-\frac{e^a}{e^x - e^a}f(a)=f'(c)+f(c)$. Taking limits as $c$ tends to infinity we have,  $\lim_{c\to \infty}\frac{e^x}{e^x -e^a} \lim_{c\to \infty}f(x)-\lim_{c\to \infty}\frac{e^a}{e^x - e^a}\lim_{c\to \infty}f(a)=\lim_{c\to \infty}[f'(c)+f(c)]$. But $c\lt x$, thus we obtain $1 \cdot \lim_{x\to \infty}f(x)-0\cdot f(a)=\lim_{c\to \infty}[f'(c)+f(c)].$ Hence, since the right hand side is equal to $0$, it follows that $\lim_{x\to \infty}f(x)=0.$ Taking a close look at this solution, however, I think it may not be correct since the $c$ used in the answer is actually a function of $x$ and it may not make sense to take $c\to \infty$. I mean how do we know that there will always be such a $c$ greater than any given positive number? Is this solution correct? If so, how can my question be answered? I would greatly appreciate any help.","This question already has answers here : How can this be proved $\lim_{x\to\infty}(f(x)+f'(x))=l$ [duplicate] (5 answers) Closed 8 years ago . Suppose that $f: \mathbb{R^+}\to \mathbb{R}$ satisfies $\lim_{x\to \infty} (f+f')(x)=0$. Show that $\lim_{x\to \infty} f(x)=0$. This is one solution I found to this problem. Solution: If $x=a$ is in the domain of the function $f$, then by the generalized mean value theorem applied to the functions $e^x f(x)$ and $e^x$, there exists $c\in (a,x)$ such that $\frac{e^x f(x)-e^a f(a)}{e^x -e^a}=\frac{e^c f'(c)+e^c f(c)}{e^c}.$ This yields $\frac{e^x}{e^x -e^a} f(x)-\frac{e^a}{e^x - e^a}f(a)=f'(c)+f(c)$. Taking limits as $c$ tends to infinity we have,  $\lim_{c\to \infty}\frac{e^x}{e^x -e^a} \lim_{c\to \infty}f(x)-\lim_{c\to \infty}\frac{e^a}{e^x - e^a}\lim_{c\to \infty}f(a)=\lim_{c\to \infty}[f'(c)+f(c)]$. But $c\lt x$, thus we obtain $1 \cdot \lim_{x\to \infty}f(x)-0\cdot f(a)=\lim_{c\to \infty}[f'(c)+f(c)].$ Hence, since the right hand side is equal to $0$, it follows that $\lim_{x\to \infty}f(x)=0.$ Taking a close look at this solution, however, I think it may not be correct since the $c$ used in the answer is actually a function of $x$ and it may not make sense to take $c\to \infty$. I mean how do we know that there will always be such a $c$ greater than any given positive number? Is this solution correct? If so, how can my question be answered? I would greatly appreciate any help.",,"['calculus', 'real-analysis', 'analysis', 'limits', 'derivatives']"
92,Understanding product $\sigma$-algebra,Understanding product -algebra,\sigma,"Let $\{X_\alpha\}_{\alpha \in A}$ be an indexed collection of nonempty sets, $X = \prod _{\alpha \in A}X_\alpha$, and $\pi _\alpha: X \rightarrow X_\alpha$ the coordinate maps. If $M_\alpha$ is a $\sigma$-algebra on $X_\alpha$ for each $\alpha$, what is the product $\sigma$-algebra on $X$? I am reading a book, and it says that the product $\sigma$-algebra on $X$ is the $\sigma$-algebra generated by  $$ \{\pi_\alpha^{-1}(E_\alpha): E_\alpha \in M_\alpha, \alpha \in A\} $$ I was wondering: Why isn't it simply $\prod _{\alpha \in A}M_\alpha$? I am completely confused with this statement of the book, can anyone give a simple example to clarify what the book is trying to say?","Let $\{X_\alpha\}_{\alpha \in A}$ be an indexed collection of nonempty sets, $X = \prod _{\alpha \in A}X_\alpha$, and $\pi _\alpha: X \rightarrow X_\alpha$ the coordinate maps. If $M_\alpha$ is a $\sigma$-algebra on $X_\alpha$ for each $\alpha$, what is the product $\sigma$-algebra on $X$? I am reading a book, and it says that the product $\sigma$-algebra on $X$ is the $\sigma$-algebra generated by  $$ \{\pi_\alpha^{-1}(E_\alpha): E_\alpha \in M_\alpha, \alpha \in A\} $$ I was wondering: Why isn't it simply $\prod _{\alpha \in A}M_\alpha$? I am completely confused with this statement of the book, can anyone give a simple example to clarify what the book is trying to say?",,"['real-analysis', 'measure-theory']"
93,Infinite sum: prove or disprove this statement,Infinite sum: prove or disprove this statement,,"In one of my textbook I was asked to prove: Suppose $0<p_1<p_2<\cdots<p_n<\cdots$, prove:   $$\sum_{n=1}^{\infty}\frac{1}{p_n}\quad\text{converges}\Leftrightarrow\sum_{n=1}^{\infty}\frac{n}{p_1+p_2+\cdots+p_n}\quad\text{converges}$$ My notion was that I should be able to prove that when $n\to\infty$,  $$\frac{1}{p_n}\sim\frac{n}{p_1+p_2+\cdots+p_n}$$ but I failed. I could not even think of an effective method to prove the $\implies$ part, my failed attempt is as follows: If $\sum_{n=1}^{\infty}\frac{1}{p_n}$ converges, then I want to use comparison test:  $$\frac{\frac{n}{p_1+p_2+\cdots+p_n}}{\frac{1}{p_n}}\le\frac{\sum_{n=1}^{\infty}\frac{1}{p_n}}{n\cdot\frac{1}{p_n}}$$ I tried to upper-bound RHS, but since $\sum_{n=1}^{\infty}\frac{1}{p_n}$ converges, by comparison test we have $$\frac{\frac{1}{p_n}}{\frac1n}\to 0^+\quad\text{as}\quad n\to\infty$$ and thus I could not bound RHS. Can anyone help me with this problem? Best regards!","In one of my textbook I was asked to prove: Suppose $0<p_1<p_2<\cdots<p_n<\cdots$, prove:   $$\sum_{n=1}^{\infty}\frac{1}{p_n}\quad\text{converges}\Leftrightarrow\sum_{n=1}^{\infty}\frac{n}{p_1+p_2+\cdots+p_n}\quad\text{converges}$$ My notion was that I should be able to prove that when $n\to\infty$,  $$\frac{1}{p_n}\sim\frac{n}{p_1+p_2+\cdots+p_n}$$ but I failed. I could not even think of an effective method to prove the $\implies$ part, my failed attempt is as follows: If $\sum_{n=1}^{\infty}\frac{1}{p_n}$ converges, then I want to use comparison test:  $$\frac{\frac{n}{p_1+p_2+\cdots+p_n}}{\frac{1}{p_n}}\le\frac{\sum_{n=1}^{\infty}\frac{1}{p_n}}{n\cdot\frac{1}{p_n}}$$ I tried to upper-bound RHS, but since $\sum_{n=1}^{\infty}\frac{1}{p_n}$ converges, by comparison test we have $$\frac{\frac{1}{p_n}}{\frac1n}\to 0^+\quad\text{as}\quad n\to\infty$$ and thus I could not bound RHS. Can anyone help me with this problem? Best regards!",,"['calculus', 'real-analysis', 'sequences-and-series']"
94,"Can a ""well behaved"" function oscilate between two different growth orders?","Can a ""well behaved"" function oscilate between two different growth orders?",,"Let $f:\mathbb{R}_+\rightarrow\mathbb{R}_+$ be a monotonic nonnegative non-decreasing $C^1$ divergent function, with $f'$ being monotonic nonnegative and bounded. I'm trying to prove that it is not possible to exist strictly increasing divergent real sequences $(a_n)_n$ and $(b_n)_n$ such that $$ \lim_{n\to\infty} \frac{f(a_n)}{a_n^{1/3}} = \lim_{n\to\infty} \frac{f(b_n)}{b_n^{2/3}} = 1. $$ The only thing I've thought was to take a strictly increasing divergent sequence $(c_n)_n$ with $c_k \in (a_n)_n$ when $k$ even and $c_k \in (b_n)_n$ when $k$ odd, and to use the Mean value Theorem to try to control the growth of $f'$, but it was not very effective. I would appreciate any suggestion.","Let $f:\mathbb{R}_+\rightarrow\mathbb{R}_+$ be a monotonic nonnegative non-decreasing $C^1$ divergent function, with $f'$ being monotonic nonnegative and bounded. I'm trying to prove that it is not possible to exist strictly increasing divergent real sequences $(a_n)_n$ and $(b_n)_n$ such that $$ \lim_{n\to\infty} \frac{f(a_n)}{a_n^{1/3}} = \lim_{n\to\infty} \frac{f(b_n)}{b_n^{2/3}} = 1. $$ The only thing I've thought was to take a strictly increasing divergent sequence $(c_n)_n$ with $c_k \in (a_n)_n$ when $k$ even and $c_k \in (b_n)_n$ when $k$ odd, and to use the Mean value Theorem to try to control the growth of $f'$, but it was not very effective. I would appreciate any suggestion.",,"['real-analysis', 'sequences-and-series']"
95,Monotone functions and Borel sets,Monotone functions and Borel sets,,"I'm studying measure theory and two question came to my mind: If $f:\mathbb{R}\to\mathbb{R}$ is monotone and $B\subseteq\mathbb{R}$ is borel, is the image $f(B)$ borel? If $f:\mathbb{R}\to\mathbb{R}$ is a monotone function (say, non-decreasing), does there exist a sequence of continuous functions $f_n:\mathbb{R}\to\mathbb{R}$ converging pointwise to $f$? Here's the motivation for those questions: Let $(X,M)$ and $(Y,N)$ be measure spaces. It's known that if $\mu$ is a measure on $(X,M)$ and $f:X\to Y$ is measure, then we have the pushforward measure $f_*\mu(A)=\mu(f^{-1}(A))$ on $(Y,N)$. What if we were to define a ""pullback measure""? Given a function $f:X\to Y$ such that $f(M)\subseteq N$ (i.e. $f$ maps measurable sets to measurable sets) and a measure $\nu$ on $N$, the natural formula would be $f^*\nu(A)=\nu(f(A))$. So we ask if is there a good amount of functions which map measurable sets to measurable sets in $\mathbb{R}$, and monotone functions seem like good candidates (for strictly monotone, continuous functions, the result is valid and there are several answers on the web). If this were true, maybe we could use some convergence argument to solve the problem above.","I'm studying measure theory and two question came to my mind: If $f:\mathbb{R}\to\mathbb{R}$ is monotone and $B\subseteq\mathbb{R}$ is borel, is the image $f(B)$ borel? If $f:\mathbb{R}\to\mathbb{R}$ is a monotone function (say, non-decreasing), does there exist a sequence of continuous functions $f_n:\mathbb{R}\to\mathbb{R}$ converging pointwise to $f$? Here's the motivation for those questions: Let $(X,M)$ and $(Y,N)$ be measure spaces. It's known that if $\mu$ is a measure on $(X,M)$ and $f:X\to Y$ is measure, then we have the pushforward measure $f_*\mu(A)=\mu(f^{-1}(A))$ on $(Y,N)$. What if we were to define a ""pullback measure""? Given a function $f:X\to Y$ such that $f(M)\subseteq N$ (i.e. $f$ maps measurable sets to measurable sets) and a measure $\nu$ on $N$, the natural formula would be $f^*\nu(A)=\nu(f(A))$. So we ask if is there a good amount of functions which map measurable sets to measurable sets in $\mathbb{R}$, and monotone functions seem like good candidates (for strictly monotone, continuous functions, the result is valid and there are several answers on the web). If this were true, maybe we could use some convergence argument to solve the problem above.",,['real-analysis']
96,Composition and Limits at Infinity,Composition and Limits at Infinity,,"It is a well known result that if a function $f$ is continuous at $b$ and $\lim_{x\rightarrow a} g(x)=b$, then $\lim_{x\rightarrow a} f(g(x))=f(b)$. When does this hold at infinity? If $f$ is continuous and $\lim_{x\rightarrow \infty} g(x)=b$, is it true that $\lim_{x\rightarrow \infty} f(g(x))=f(b)$?","It is a well known result that if a function $f$ is continuous at $b$ and $\lim_{x\rightarrow a} g(x)=b$, then $\lim_{x\rightarrow a} f(g(x))=f(b)$. When does this hold at infinity? If $f$ is continuous and $\lim_{x\rightarrow \infty} g(x)=b$, is it true that $\lim_{x\rightarrow \infty} f(g(x))=f(b)$?",,"['real-analysis', 'limits']"
97,The dual space of locally integrable function space,The dual space of locally integrable function space,,"I'm strongly interested in dual spaces. I learned the dual spaces of $L^p$ , $L^{\infty}$ , $C(X)$ and $M(X)$ . I wonder the dual space of locally integrable function space in $\mathbb{R}^n$ . The topology on $L_{loc}^1$ is generated by seminorms $$p_{K}(f-f_{0})=\int_{K}|f-f_{0}| \ dx $$ where $K$ is a compact set. I believe that this is standard topology of $L_{loc}^1$ . I have some observations. Observation 1. Define $$L(f)=\int_{\mathbb R} f(x) \ g(x) \ \chi_{K}(x) \ dx$$ where $K$ is a compact set, $f \in L_{loc}^1$ , $g \in L^\infty$ and $\chi_{K}$ is the characteristic function of $K$ . Then $L$ is a continuous linear functional. Obeservation 2. If $f \mapsto L(f)=\int f(x) \ g(x) \ dx$ is a linear functional, then $g$ should be of the form of $h \ \chi_{K}$ for some $h \in L^\infty$ and some compact $K$ . The above observation can be proved by using reductio ad absurdum. By the observations, I guess that $(L_{loc}^1)^*$ is the following space $$\{ \chi_{K}\ g : g \in L^\infty, \text{ compact } K \}$$ Is it true? If not, what is the counterexample?, and is there any reference about dual space of $L_{loc}^1$ ?","I'm strongly interested in dual spaces. I learned the dual spaces of , , and . I wonder the dual space of locally integrable function space in . The topology on is generated by seminorms where is a compact set. I believe that this is standard topology of . I have some observations. Observation 1. Define where is a compact set, , and is the characteristic function of . Then is a continuous linear functional. Obeservation 2. If is a linear functional, then should be of the form of for some and some compact . The above observation can be proved by using reductio ad absurdum. By the observations, I guess that is the following space Is it true? If not, what is the counterexample?, and is there any reference about dual space of ?","L^p L^{\infty} C(X) M(X) \mathbb{R}^n L_{loc}^1 p_{K}(f-f_{0})=\int_{K}|f-f_{0}| \ dx  K L_{loc}^1 L(f)=\int_{\mathbb R} f(x) \ g(x) \ \chi_{K}(x) \ dx K f \in L_{loc}^1 g \in L^\infty \chi_{K} K L f \mapsto L(f)=\int f(x) \ g(x) \ dx g h \ \chi_{K} h \in L^\infty K (L_{loc}^1)^* \{ \chi_{K}\ g : g \in L^\infty, \text{ compact } K \} L_{loc}^1","['real-analysis', 'functional-analysis', 'analysis', 'measure-theory', 'lp-spaces']"
98,Functions with every point being a Lebesgue point,Functions with every point being a Lebesgue point,,"For a locally integrable function $f$ a point $x$ is a Lebesgue point if the integral averages of deviations from $f(x)$ over balls centered at $x$ converge to $0$ as the balls shrink to the point. According to a theorem of Lebesgue almost every point is a Lebesgue point, and obviously every point is Lebesgue if $f$ is continuous. Is the converse true? Are everywhere Lebesgue functions necessarily continuous? Such questions usually have a ""no"" answer but I don't see an obvious counterexample. It's easy to change a continuous function into a discontinuous one at a point, that's still Lebesgue at that point, by altering values on a sequence that converges to it. But that would apparently destroy Lebesgueness at the points of the sequence.","For a locally integrable function $f$ a point $x$ is a Lebesgue point if the integral averages of deviations from $f(x)$ over balls centered at $x$ converge to $0$ as the balls shrink to the point. According to a theorem of Lebesgue almost every point is a Lebesgue point, and obviously every point is Lebesgue if $f$ is continuous. Is the converse true? Are everywhere Lebesgue functions necessarily continuous? Such questions usually have a ""no"" answer but I don't see an obvious counterexample. It's easy to change a continuous function into a discontinuous one at a point, that's still Lebesgue at that point, by altering values on a sequence that converges to it. But that would apparently destroy Lebesgueness at the points of the sequence.",,"['real-analysis', 'integration', 'measure-theory', 'lebesgue-integral', 'lebesgue-measure']"
99,Measure which does not grow faster than Lebesgue,Measure which does not grow faster than Lebesgue,,"Is there an example of a measure $\mu$ on $\mathbb{R}$ which is not absolutely continuous with respect to Lebesgue measure such that $\mu[\mathbb{R}]=+\infty$ but $$\limsup_{a\to +\infty}\frac{a}{\mu[-a,a]}<+\infty.$$","Is there an example of a measure $\mu$ on $\mathbb{R}$ which is not absolutely continuous with respect to Lebesgue measure such that $\mu[\mathbb{R}]=+\infty$ but $$\limsup_{a\to +\infty}\frac{a}{\mu[-a,a]}<+\infty.$$",,"['real-analysis', 'measure-theory', 'lebesgue-measure']"
