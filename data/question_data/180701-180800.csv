,title_raw,title_text,title_latex,body_raw,body_text,body_latex,tags
0,Can someone show me how to prove the following?,Can someone show me how to prove the following?,,"I have $f(x)=g(ax+b)$, a and b are constant. I need to show that $\nabla f(x)=a\nabla g(x)$ and $\nabla^2 f(x)=a^2\nabla^2 g(x)$... I was thinking that the final answer should have ax+b in it, but apparently it can be shown that the above is true???","I have $f(x)=g(ax+b)$, a and b are constant. I need to show that $\nabla f(x)=a\nabla g(x)$ and $\nabla^2 f(x)=a^2\nabla^2 g(x)$... I was thinking that the final answer should have ax+b in it, but apparently it can be shown that the above is true???",,"['calculus', 'linear-algebra', 'multivariable-calculus']"
1,"Calculate the partial derivative, local minima/maxima, and saddle points.","Calculate the partial derivative, local minima/maxima, and saddle points.",,I am having trouble finding the partial derivative. And clues or hints regarding said problem and how to find saddle points/local maxima is appreciated.,I am having trouble finding the partial derivative. And clues or hints regarding said problem and how to find saddle points/local maxima is appreciated.,,"['multivariable-calculus', 'derivatives']"
2,Directional Derivative,Directional Derivative,,"Given the following equation: $V(x,y,z)=5x^2-3xy+xyz$ Part 1:  At point $P(3,4,5)$, find the rate of change in the direction of the vector $\langle1,1,-1\rangle$ Part 2: Find the direction in which $V$ changes most rapidly at $P(3,4,5)$ Part 3: Find the maximum rate of change at $P(3,4,5)$ I think I've managed to do part 1. Here's what I've done so far - but I am clueless as to how to find the max rate of change at $P$ and the direction in which the change occurs most rapidly. $V_x(x,y,z)=10x+y(z-3)$ $V_y(x,y,z)=x(z-3)$ $V_z(x,y,z)=xy$ Based on the above partial derivatives, $$\begin{align} \nabla V(3,4,5) &=\left.\langle10x+y(z-3),\ x(z-3),\ xy\rangle\right|_{(3,4,5)} \\ &=\langle38,6,12\rangle \end{align}$$ Unit vector of $\langle1,1,-1\rangle$ is $\vec u=\langle\frac{1}{\sqrt{3}},\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}}\rangle$. Directional derivative at $(3,4,5)$ in the direction of $\langle1,1,-1\rangle$ is: $$\begin{align} D_\vec uV(3,4,5) &= \nabla V(3,4,5) \cdot \vec u \\ &=\frac{32}{\sqrt{3}} \end{align}$$ How should I proceed?","Given the following equation: $V(x,y,z)=5x^2-3xy+xyz$ Part 1:  At point $P(3,4,5)$, find the rate of change in the direction of the vector $\langle1,1,-1\rangle$ Part 2: Find the direction in which $V$ changes most rapidly at $P(3,4,5)$ Part 3: Find the maximum rate of change at $P(3,4,5)$ I think I've managed to do part 1. Here's what I've done so far - but I am clueless as to how to find the max rate of change at $P$ and the direction in which the change occurs most rapidly. $V_x(x,y,z)=10x+y(z-3)$ $V_y(x,y,z)=x(z-3)$ $V_z(x,y,z)=xy$ Based on the above partial derivatives, $$\begin{align} \nabla V(3,4,5) &=\left.\langle10x+y(z-3),\ x(z-3),\ xy\rangle\right|_{(3,4,5)} \\ &=\langle38,6,12\rangle \end{align}$$ Unit vector of $\langle1,1,-1\rangle$ is $\vec u=\langle\frac{1}{\sqrt{3}},\frac{1}{\sqrt{3}},-\frac{1}{\sqrt{3}}\rangle$. Directional derivative at $(3,4,5)$ in the direction of $\langle1,1,-1\rangle$ is: $$\begin{align} D_\vec uV(3,4,5) &= \nabla V(3,4,5) \cdot \vec u \\ &=\frac{32}{\sqrt{3}} \end{align}$$ How should I proceed?",,"['multivariable-calculus', 'derivatives']"
3,'Definition' of the Lagrange multipliers,'Definition' of the Lagrange multipliers,,"I do not fully understand the 'definition' of the Lagrange multipliers. I do understand that a maximum occurs when the constraint and the objective function are tangent to eachother. However, I do not understand why this implies that the $\nabla{f}=\lambda\times\nabla{g}$. Why is it not true that the gradient of $f$ IS EQUAL to the gradient of $g$? Doesn't the fact that the level curves are parallel imply that the derivatives are equal and thus that the gradients are equal (and not a multiple of eachother)?","I do not fully understand the 'definition' of the Lagrange multipliers. I do understand that a maximum occurs when the constraint and the objective function are tangent to eachother. However, I do not understand why this implies that the $\nabla{f}=\lambda\times\nabla{g}$. Why is it not true that the gradient of $f$ IS EQUAL to the gradient of $g$? Doesn't the fact that the level curves are parallel imply that the derivatives are equal and thus that the gradients are equal (and not a multiple of eachother)?",,['multivariable-calculus']
4,Mass of wire described by curve with specified density function (Double integral),Mass of wire described by curve with specified density function (Double integral),,"Mass of the wire of a curve described by $$y = x^2 +1 $$ where $0\le x\le 1$ with density $$ \rho(x,y) = 12x $$ I couldn't get the correct answer for this one. What I did: $$ m = \int^{0}_{1}\int^{1}_{x^2} 12 x dy dx $$ that gives me $3$, which is not correct.","Mass of the wire of a curve described by $$y = x^2 +1 $$ where $0\le x\le 1$ with density $$ \rho(x,y) = 12x $$ I couldn't get the correct answer for this one. What I did: $$ m = \int^{0}_{1}\int^{1}_{x^2} 12 x dy dx $$ that gives me $3$, which is not correct.",,"['integration', 'multivariable-calculus']"
5,Functions satisfying Cauchy Riemann equations at a point,Functions satisfying Cauchy Riemann equations at a point,,"Do there exist two functions $u,v$ defined on an open set containing $(0,0)$ with values in $\mathbb{R}$ such that (1) $u,v$ are differentiable at $(0,0)$; (2) $u_x=v_y, u_y=-v_x$ at $(0,0)$; (3) at least one of $u_x,u_y,v_x,v_y$ is not continuous at $(0,0)$ ?","Do there exist two functions $u,v$ defined on an open set containing $(0,0)$ with values in $\mathbb{R}$ such that (1) $u,v$ are differentiable at $(0,0)$; (2) $u_x=v_y, u_y=-v_x$ at $(0,0)$; (3) at least one of $u_x,u_y,v_x,v_y$ is not continuous at $(0,0)$ ?",,"['complex-analysis', 'multivariable-calculus']"
6,A multivariate function which is $C^1$ in each variable,A multivariate function which is  in each variable,C^1,"Let $U$ be an open subset of $\mathbb{R}^n$. Let $f\colon U \rightarrow \mathbb{R}$ be a function. Suppose the partial derivative $f_{x_i}$ exists and it is continuous on $U$ for $i = 1, \dots, n$. Is $f$ continuous?","Let $U$ be an open subset of $\mathbb{R}^n$. Let $f\colon U \rightarrow \mathbb{R}$ be a function. Suppose the partial derivative $f_{x_i}$ exists and it is continuous on $U$ for $i = 1, \dots, n$. Is $f$ continuous?",,['multivariable-calculus']
7,Need help understanding proof about critical values of the determinant map.,Need help understanding proof about critical values of the determinant map.,,"In , problem 5 the author shows that the differential of the determinant map $d(\det)_A$ for an invertible matrix $A$ is nonsingular by only showing that $d(\det)_A(A) \ne 0$.  I don't really see why this shows that $d(\det)_A$ is nonsingular. I would like some further clarification on this point.","In , problem 5 the author shows that the differential of the determinant map $d(\det)_A$ for an invertible matrix $A$ is nonsingular by only showing that $d(\det)_A(A) \ne 0$.  I don't really see why this shows that $d(\det)_A$ is nonsingular. I would like some further clarification on this point.",,"['multivariable-calculus', 'differential-topology']"
8,A question on a integral in radial direction.,A question on a integral in radial direction.,,"How can one compute the following integral?  $$ \frac{\partial}{\partial r}\int_{\partial B(0,r)}f(r,x)dx.  $$ I know a similar integral $$ \frac{\partial}{\partial r}\int_{B(0,r)}f(r)dx =\int_{\partial B(0,r)}f(r)dx+\int_{B(0,r)}\frac{\partial}{\partial r}f(r)dx.  $$ Maybe I don't fully understand the latter integral either. I would appreciate it if someone could kindly explain what is going on. More specifically I want to compute something like $$ \frac{\partial}{\partial r}\int_{\partial B(0,r)}\frac{f(x)}{r}dx. $$","How can one compute the following integral?  $$ \frac{\partial}{\partial r}\int_{\partial B(0,r)}f(r,x)dx.  $$ I know a similar integral $$ \frac{\partial}{\partial r}\int_{B(0,r)}f(r)dx =\int_{\partial B(0,r)}f(r)dx+\int_{B(0,r)}\frac{\partial}{\partial r}f(r)dx.  $$ Maybe I don't fully understand the latter integral either. I would appreciate it if someone could kindly explain what is going on. More specifically I want to compute something like $$ \frac{\partial}{\partial r}\int_{\partial B(0,r)}\frac{f(x)}{r}dx. $$",,"['calculus', 'multivariable-calculus']"
9,Finding minimum and maximum of a function inside a triangle,Finding minimum and maximum of a function inside a triangle,,"I have an assignment I will hand in for grading. First part I want to see if I have understood the consept correctly. Determine max and min of $f(x,y)=xy $ on the triangle T with the corners $(0, 0), (0, 1)\:and\:(1, 0)$ I derived and got a critical point at $(0,0)$ which I evaluated to $0$. Secondly I inspected the boundary lines of which only the hypotenuse was interesting x+y=1 where I only found one critical point and that was larger than 0. Is it correct minimum value was 0? Second part Are there any analytical online tools where I can write functions and boundaries and get the max and min values?","I have an assignment I will hand in for grading. First part I want to see if I have understood the consept correctly. Determine max and min of $f(x,y)=xy $ on the triangle T with the corners $(0, 0), (0, 1)\:and\:(1, 0)$ I derived and got a critical point at $(0,0)$ which I evaluated to $0$. Secondly I inspected the boundary lines of which only the hypotenuse was interesting x+y=1 where I only found one critical point and that was larger than 0. Is it correct minimum value was 0? Second part Are there any analytical online tools where I can write functions and boundaries and get the max and min values?",,[]
10,Implicit Function theorem and Bifurcation points,Implicit Function theorem and Bifurcation points,,"So let us say we have a function $\dot{x} = f(x,r)$ that has some critical point at $(x_0,r_0)$ such that $f(x_0,r_0)=0$. The question now is: when is this a bifurcation point? I understand that $\frac{\partial{f(x_0,r_0)}}{\partial{x}} = 0$ works in practice, but I have two questions. 1.) Intuitively, why is this the case? 2.) The proof for this appealed to the implicit function theorem and said that if the derivative was non-zero, then there would be a (local) solution $x=X(r)$ such that $X(r_0)=x_0$, which cannot happen if the point is a bifurcation point. This doesn't click with me, why would this be not work if the point was a bifurcation? Also, how is it the case that the Jacobian is zero if only one entry is zero (namely the x-derivative entry)? Shouldn't it be the case that any 2 entries on opposite columns need to be zero? Thanks","So let us say we have a function $\dot{x} = f(x,r)$ that has some critical point at $(x_0,r_0)$ such that $f(x_0,r_0)=0$. The question now is: when is this a bifurcation point? I understand that $\frac{\partial{f(x_0,r_0)}}{\partial{x}} = 0$ works in practice, but I have two questions. 1.) Intuitively, why is this the case? 2.) The proof for this appealed to the implicit function theorem and said that if the derivative was non-zero, then there would be a (local) solution $x=X(r)$ such that $X(r_0)=x_0$, which cannot happen if the point is a bifurcation point. This doesn't click with me, why would this be not work if the point was a bifurcation? Also, how is it the case that the Jacobian is zero if only one entry is zero (namely the x-derivative entry)? Shouldn't it be the case that any 2 entries on opposite columns need to be zero? Thanks",,"['real-analysis', 'multivariable-calculus', 'dynamical-systems', 'fixed-point-theorems']"
11,Optimize function: Lagrange multipliers,Optimize function: Lagrange multipliers,,"I have a function of 4 variables: (distance function) $d(x,x_1,y,y_1 )=(x-x_1 )^2+(y-y_1 )^2$ subject to 2 constraints: 1. $g(x,x_1,y,y_1 )=ax^2+2hxy+2gx+by^2+2fy+c=0$ 2. $h(x,x_1,y,y_1 )= a_1 x_1^2+2h_1 x_1 y_1+2g_1 x_1+b_1 y_1^2+2f_1 y_1+c_1=0$ Using lagrange multipliers, and partial differentiation, what should be the values of $x,x_1,y,y_1$ in terms of $a,b,c,a_1,b_1,c_1,f,g,h,f_1,g_1,h_1$, with aforementioned constraints?","I have a function of 4 variables: (distance function) $d(x,x_1,y,y_1 )=(x-x_1 )^2+(y-y_1 )^2$ subject to 2 constraints: 1. $g(x,x_1,y,y_1 )=ax^2+2hxy+2gx+by^2+2fy+c=0$ 2. $h(x,x_1,y,y_1 )= a_1 x_1^2+2h_1 x_1 y_1+2g_1 x_1+b_1 y_1^2+2f_1 y_1+c_1=0$ Using lagrange multipliers, and partial differentiation, what should be the values of $x,x_1,y,y_1$ in terms of $a,b,c,a_1,b_1,c_1,f,g,h,f_1,g_1,h_1$, with aforementioned constraints?",,"['multivariable-calculus', 'optimization']"
12,Calculate the flux of the vector field through the sphere - please help me understand the solution,Calculate the flux of the vector field through the sphere - please help me understand the solution,,"Calculate the flux of the field $F(x,y,z) = (yz, xz, xy)$ through the sphere: $$ x,y,z > 0, \space x^2 + y^2 + z^2 = a^2 $$ With outer normal. Solution says: The normal is $N = \frac{1}{a}(x,y,z)$, hence $\langle F,N \rangle = \frac{3}{a}xyz$, and using spherical coordinates, we get: $$ flux_F(M) = \int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{\pi}{2}} \frac{3}{a}(a\cos(\varphi)\sin(\theta)\times a\sin(\varphi)\sin(\theta)\times a\cos(\theta))\times  {\color{Red} \sin(\theta)} d\theta d\varphi = ... = \frac{3a^2}{8} $$ My question is - shouldn't the part marked in red be $a^2\sin(\theta)$? Because $r=(a\cos(\varphi)\sin(\theta), a\sin(\varphi)\sin(\theta), a\cos(\theta))$ is a mapping $r:\mathbb{R}^2 \rightarrow \mathbb{R}^3$, the formula for surface integrals is: $$ \int_{M}f = \int_{\Omega}f\circ r \sqrt{det(D_r^T D_r)} $$ and $\sqrt{det(D_r^T D_r)} = a^2\sin(\theta)$. Thanks!","Calculate the flux of the field $F(x,y,z) = (yz, xz, xy)$ through the sphere: $$ x,y,z > 0, \space x^2 + y^2 + z^2 = a^2 $$ With outer normal. Solution says: The normal is $N = \frac{1}{a}(x,y,z)$, hence $\langle F,N \rangle = \frac{3}{a}xyz$, and using spherical coordinates, we get: $$ flux_F(M) = \int_{0}^{\frac{\pi}{2}}\int_{0}^{\frac{\pi}{2}} \frac{3}{a}(a\cos(\varphi)\sin(\theta)\times a\sin(\varphi)\sin(\theta)\times a\cos(\theta))\times  {\color{Red} \sin(\theta)} d\theta d\varphi = ... = \frac{3a^2}{8} $$ My question is - shouldn't the part marked in red be $a^2\sin(\theta)$? Because $r=(a\cos(\varphi)\sin(\theta), a\sin(\varphi)\sin(\theta), a\cos(\theta))$ is a mapping $r:\mathbb{R}^2 \rightarrow \mathbb{R}^3$, the formula for surface integrals is: $$ \int_{M}f = \int_{\Omega}f\circ r \sqrt{det(D_r^T D_r)} $$ and $\sqrt{det(D_r^T D_r)} = a^2\sin(\theta)$. Thanks!",,['multivariable-calculus']
13,Green's theorem for conservative fields - are partials equal?,Green's theorem for conservative fields - are partials equal?,,"I have just watched the Green's theorem proof by Khan . At 7:40 he explains why for a conservative field, the partial differentials under the double integral: $$\int \int_R \left( \frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right ) dA$$ must be equal. He says: (...) if F is conservative, which means it's the gradient of some function, it's path-independent, the closed integral around any path is equal to 0. (...) this thing right here [the difference under the integral] must be equal to 0. That's the only way that you're always going to enforce that this whole integral is going to be equal to 0 over any region. I'm sure you could think of situations where they cancel each other out, but really over any region that's the only way that this is going to be true. 1) What are the examples of two partials such that $\frac{\partial Q}{\partial x} \neq \frac{\partial P}{\partial y}$ whose double integrals are equal over a region and therefore cancel each other out? 2) If there exist examples for 1) (cancelling out locally), why is it impossible to define partials whose integrals cancel out over any region?","I have just watched the Green's theorem proof by Khan . At 7:40 he explains why for a conservative field, the partial differentials under the double integral: $$\int \int_R \left( \frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right ) dA$$ must be equal. He says: (...) if F is conservative, which means it's the gradient of some function, it's path-independent, the closed integral around any path is equal to 0. (...) this thing right here [the difference under the integral] must be equal to 0. That's the only way that you're always going to enforce that this whole integral is going to be equal to 0 over any region. I'm sure you could think of situations where they cancel each other out, but really over any region that's the only way that this is going to be true. 1) What are the examples of two partials such that $\frac{\partial Q}{\partial x} \neq \frac{\partial P}{\partial y}$ whose double integrals are equal over a region and therefore cancel each other out? 2) If there exist examples for 1) (cancelling out locally), why is it impossible to define partials whose integrals cancel out over any region?",,['multivariable-calculus']
14,Surface Integral (Flux?),Surface Integral (Flux?),,"Evaluate the surface integral: $$\iint_S \mathbf{\vec F} \cdot d\mathbf{\vec S}$$ for the vector field $$  \mathbf{\vec F}(x,y,z) = xze^y \mathbf{ \hat i} - xze^y \mathbf{\hat j} + z\mathbf{\hat k}$$ where $S$ is part of the plane $x + y + z = 1$ in the first octant and has a downward orientation.","Evaluate the surface integral: $$\iint_S \mathbf{\vec F} \cdot d\mathbf{\vec S}$$ for the vector field $$  \mathbf{\vec F}(x,y,z) = xze^y \mathbf{ \hat i} - xze^y \mathbf{\hat j} + z\mathbf{\hat k}$$ where $S$ is part of the plane $x + y + z = 1$ in the first octant and has a downward orientation.",,['multivariable-calculus']
15,Surface integral - need a bit of explaining,Surface integral - need a bit of explaining,,"I'm going through some old calculus, and I'm struggling a bit with surface integrals. Here's the problem: Compute the integral $$\iint\limits_{\sigma} (x-y-z)d\sigma$$ where $\sigma$ is the plane $x+y=1$ in the first octant, limited by $z=0$ and $z=1$. So, what I've done so far is to convert the equation for sigma as a function of $y$, i.e. $y = 1-x$ $\therefore\quad\frac{\partial y}{\partial x} = -1,\quad\frac{\partial y}{\partial z} = 0\\ \therefore\quad\sqrt{(\frac{\partial y}{\partial x})^2 + (\frac{\partial y}{\partial z})^2+1} = \sqrt{2}\\ \therefore\quad\displaystyle{\iint\limits_{\sigma}} (x-y-z)d\sigma = \sqrt{2} \displaystyle{\iint\limits_{R}} (x-(1-x)-z)dxdz $ where $R$ is the projection of the given region $\sigma$ on the $xz$ plane. Simplifying: $$\sqrt{2}\iint\limits_{R}(2x-1-z)dxdz$$ So, it seems that the projection is a right triangle, with vertices at $(0,0,0), (0,0,1), (1,0,0)$. Am I on the right track, and, how do I proceed from here? I full worked out example would help me a lot.","I'm going through some old calculus, and I'm struggling a bit with surface integrals. Here's the problem: Compute the integral $$\iint\limits_{\sigma} (x-y-z)d\sigma$$ where $\sigma$ is the plane $x+y=1$ in the first octant, limited by $z=0$ and $z=1$. So, what I've done so far is to convert the equation for sigma as a function of $y$, i.e. $y = 1-x$ $\therefore\quad\frac{\partial y}{\partial x} = -1,\quad\frac{\partial y}{\partial z} = 0\\ \therefore\quad\sqrt{(\frac{\partial y}{\partial x})^2 + (\frac{\partial y}{\partial z})^2+1} = \sqrt{2}\\ \therefore\quad\displaystyle{\iint\limits_{\sigma}} (x-y-z)d\sigma = \sqrt{2} \displaystyle{\iint\limits_{R}} (x-(1-x)-z)dxdz $ where $R$ is the projection of the given region $\sigma$ on the $xz$ plane. Simplifying: $$\sqrt{2}\iint\limits_{R}(2x-1-z)dxdz$$ So, it seems that the projection is a right triangle, with vertices at $(0,0,0), (0,0,1), (1,0,0)$. Am I on the right track, and, how do I proceed from here? I full worked out example would help me a lot.",,['multivariable-calculus']
16,Evaluating the line integral of $F=\frac{-y}{x^2+y^2}i+\frac{x}{x^2+y^2}j$ along $0 \le t \le 2\pi $,Evaluating the line integral of  along,F=\frac{-y}{x^2+y^2}i+\frac{x}{x^2+y^2}j 0 \le t \le 2\pi ,"Recently, I had an exam and in that I was asked to evaluate the line integral of the function $$F=\frac{-y}{x^2+y^2}i+\frac{x}{x^2+y^2}j$$ alongside the unit circle, $0 \le t \le 2\pi $ . Moreover, it was asked if this integral could be carried out with using Green's Theorem or not and why? For the first, I did the following: $$\oint_C F\cdot dr=\int_0^{2\pi}F\big (\cos(t),\sin(t)\big)\cdot\big(-\sin(t),\cos(t)\big)dt$$ which is $2\pi$. But always I have problem with above theorem and I do know I didn't pass this part of question correctly. May I ask to help me? Thank you.","Recently, I had an exam and in that I was asked to evaluate the line integral of the function $$F=\frac{-y}{x^2+y^2}i+\frac{x}{x^2+y^2}j$$ alongside the unit circle, $0 \le t \le 2\pi $ . Moreover, it was asked if this integral could be carried out with using Green's Theorem or not and why? For the first, I did the following: $$\oint_C F\cdot dr=\int_0^{2\pi}F\big (\cos(t),\sin(t)\big)\cdot\big(-\sin(t),\cos(t)\big)dt$$ which is $2\pi$. But always I have problem with above theorem and I do know I didn't pass this part of question correctly. May I ask to help me? Thank you.",,"['integration', 'multivariable-calculus']"
17,"Example that $u\in W^{1,2}$, but $u \notin W^{1,3}$","Example that , but","u\in W^{1,2} u \notin W^{1,3}","I'm doing the calculations about the following assertion Let $\Omega$ be $\{(x,y):0<y<x^2, 0<x<1\}$. The function $u(x,y)=\log (x^2+y^2)$ belongs to $W^{1,2}(\Omega)$, which you can check by integrating $|\nabla u|^2\approx 1/x^2$ within $\Omega$. We have $\Delta u=0$, which  is the nicest equation one could ask for. However, $u$ does not belong to $W^{1,3}(\Omega)$, which you also can check by integration. My efforts: $| \nabla u |^2 = 4/(x^2+y^2)$ and  \begin{equation} \int_{0}^{1}\int_{0}^{x^2} \dfrac{1}{x^2+y^2}dy dx = \int_{0}^{1} \dfrac{\arctan}{x}dx < \infty \end{equation} because $\lim_{x\rightarrow 0}\dfrac{\arctan}{x} = 1$ and $\dfrac{\arctan}{x}$ is bounded in(0,1). Hence $\nabla u \in L^{2}(\Omega)$. Am I right here? I don't know how $\int_{0}^{1}\int_{0}^{x^2} \dfrac{1}{(x^2+y^2)^{^3/2}}dy dx$ diverges.  Thank you. 3. \begin{eqnarray} \int_{0}^{1}\int_{0}^{x^2} \ln(x^2+y^2)dydx &=& \int_{0}^{1}\int_{0}^{x^4} u\ln udu dx\\ & = &\int_{0}^{1} \left \{\dfrac{1}{2} u^2 \ln  + \dfrac{1}{4}u^2 \right \}_{0}^{x^4}dx\\ &=& \int_{0}^{1} (x^8 \ln(x^4) -\dfrac{1}{4}x^8) dx. \end{eqnarray} In the second equality used that $\lim_{x\rightarrow 0}x^2 \ln x = 0$. Moreover, we obatain that $x^2 \ln(x^2)$ is finite in $(0,\infty)$ and the integral above is finite. Am I right here too?","I'm doing the calculations about the following assertion Let $\Omega$ be $\{(x,y):0<y<x^2, 0<x<1\}$. The function $u(x,y)=\log (x^2+y^2)$ belongs to $W^{1,2}(\Omega)$, which you can check by integrating $|\nabla u|^2\approx 1/x^2$ within $\Omega$. We have $\Delta u=0$, which  is the nicest equation one could ask for. However, $u$ does not belong to $W^{1,3}(\Omega)$, which you also can check by integration. My efforts: $| \nabla u |^2 = 4/(x^2+y^2)$ and  \begin{equation} \int_{0}^{1}\int_{0}^{x^2} \dfrac{1}{x^2+y^2}dy dx = \int_{0}^{1} \dfrac{\arctan}{x}dx < \infty \end{equation} because $\lim_{x\rightarrow 0}\dfrac{\arctan}{x} = 1$ and $\dfrac{\arctan}{x}$ is bounded in(0,1). Hence $\nabla u \in L^{2}(\Omega)$. Am I right here? I don't know how $\int_{0}^{1}\int_{0}^{x^2} \dfrac{1}{(x^2+y^2)^{^3/2}}dy dx$ diverges.  Thank you. 3. \begin{eqnarray} \int_{0}^{1}\int_{0}^{x^2} \ln(x^2+y^2)dydx &=& \int_{0}^{1}\int_{0}^{x^4} u\ln udu dx\\ & = &\int_{0}^{1} \left \{\dfrac{1}{2} u^2 \ln  + \dfrac{1}{4}u^2 \right \}_{0}^{x^4}dx\\ &=& \int_{0}^{1} (x^8 \ln(x^4) -\dfrac{1}{4}x^8) dx. \end{eqnarray} In the second equality used that $\lim_{x\rightarrow 0}x^2 \ln x = 0$. Moreover, we obatain that $x^2 \ln(x^2)$ is finite in $(0,\infty)$ and the integral above is finite. Am I right here too?",,"['multivariable-calculus', 'sobolev-spaces']"
18,Partial Derivative Question,Partial Derivative Question,,"I am given with the function: $ f(x,y) = \frac{y\ln(1+x^2 + ay^2) } {x^2 + 2y^2} $ when $ (x,y)\neq (0,0)$, and $f(0,0)=0$ . There is another given data; $ f_y (0,0) = 2 $ .  What is the value of $a$ ? I've tried computing the limit $  \frac{f(0,h)- f(0,0)}{h} $ , but it seems like it's always zero, contradicting the fact that $f_y(0,0)=2 $ ! Can someone help me understand my mistake? Thanks !","I am given with the function: $ f(x,y) = \frac{y\ln(1+x^2 + ay^2) } {x^2 + 2y^2} $ when $ (x,y)\neq (0,0)$, and $f(0,0)=0$ . There is another given data; $ f_y (0,0) = 2 $ .  What is the value of $a$ ? I've tried computing the limit $  \frac{f(0,h)- f(0,0)}{h} $ , but it seems like it's always zero, contradicting the fact that $f_y(0,0)=2 $ ! Can someone help me understand my mistake? Thanks !",,['multivariable-calculus']
19,Are these integrals of motion?,Are these integrals of motion?,,"What are the integrals of motion of a system with the following Lagrangian? $$L=a\dot{\phi_1}^2+b\dot{\phi_2}^2+c\cos(\phi_1-\phi_2)$$? where $a,b,c$ are constants, $\phi_1,\phi_2$ are angles and $\dot{\phi_i}$ represents differentiation wrt time. I believe the Hamiltonian is conserved, but are there any more? Perhaps there is an isotropy of space here, since $\phi_1,\phi_2$ only exist as a difference $\phi_1-\phi_2$? So angular momentum? Are the above 2 right? Are there any more? Thanks. ADDED: ""integrals of motion"" are sometimes referred to elsewhere as ""constants of motions"" or ""conserved quantities"".","What are the integrals of motion of a system with the following Lagrangian? $$L=a\dot{\phi_1}^2+b\dot{\phi_2}^2+c\cos(\phi_1-\phi_2)$$? where $a,b,c$ are constants, $\phi_1,\phi_2$ are angles and $\dot{\phi_i}$ represents differentiation wrt time. I believe the Hamiltonian is conserved, but are there any more? Perhaps there is an isotropy of space here, since $\phi_1,\phi_2$ only exist as a difference $\phi_1-\phi_2$? So angular momentum? Are the above 2 right? Are there any more? Thanks. ADDED: ""integrals of motion"" are sometimes referred to elsewhere as ""constants of motions"" or ""conserved quantities"".",,"['multivariable-calculus', 'classical-mechanics']"
20,gradient in polar coordinate by changing gradient in Cartesian coordinate,gradient in polar coordinate by changing gradient in Cartesian coordinate,,"I'm tried to do following and I can't see what went wrong. $$\begin{bmatrix} \hat r\\ \hat \theta  \end{bmatrix} = \begin{bmatrix} \cos \theta & \sin \theta \\  -\sin \theta & \cos \theta \end{bmatrix}   \times \begin{bmatrix} \hat i\\ \hat j  \end{bmatrix} $$ Taking inverse,  $$ \begin{bmatrix} \hat i\\ \hat j  \end{bmatrix} =  \begin{bmatrix} \cos \theta & -\sin \theta \\  \sin \theta & \cos \theta \end{bmatrix} \times \begin{bmatrix} \hat r\\ \hat \theta  \end{bmatrix} $$ The gradient of function $ \phi $ $$\nabla \phi = \hat i \frac{\partial \phi}{\partial x} + \hat j \frac{\partial \phi}{\partial y}$$ $$= \hat r \left( \cos \theta \frac{\partial \phi}{\partial x}  + \sin \theta \frac{\partial \phi}{\partial y} \right) + \hat \theta \left( -\sin \theta \frac{\partial \phi}{\partial x}  +  \cos \theta \frac{\partial \phi}{\partial y} \right)$$ $$= \hat r \left( \cos \theta \frac{\partial \phi}{\partial r} \frac{\partial r}{\partial x}  + \sin \theta \frac{\partial \phi}{\partial r} \frac{\partial r}{\partial y} \right) + \hat \theta \left( -\sin \theta \frac{\partial \phi}{\partial \theta} \frac{\partial \theta}{\partial x}  +  \cos \theta \frac{\partial \phi}{\partial \theta}\frac{\partial \theta}{\partial y} \right)$$ $$= \hat r \left( \cos \theta \frac{\partial \phi}{\partial r} \frac{1}{\cos \theta}   + \sin \theta \frac{\partial \phi}{\partial r} \frac{1}{\sin \theta} \right) + \hat \theta  \left( -\sin \theta \frac{\partial \phi}{\partial \theta} \frac{1}{- r \sin \theta }  + \cos \theta \frac{\partial \phi}{\partial \theta}\frac{1}{ r\cos \theta } \right)$$ $$ = 2 \left (  \frac{\partial \phi }{\partial r}\hat r + \frac{1}{r} \frac{\partial \phi}{\partial \theta } \hat \theta\right  )$$ I don't know what went wrong. Please help. Thank you!!","I'm tried to do following and I can't see what went wrong. $$\begin{bmatrix} \hat r\\ \hat \theta  \end{bmatrix} = \begin{bmatrix} \cos \theta & \sin \theta \\  -\sin \theta & \cos \theta \end{bmatrix}   \times \begin{bmatrix} \hat i\\ \hat j  \end{bmatrix} $$ Taking inverse,  $$ \begin{bmatrix} \hat i\\ \hat j  \end{bmatrix} =  \begin{bmatrix} \cos \theta & -\sin \theta \\  \sin \theta & \cos \theta \end{bmatrix} \times \begin{bmatrix} \hat r\\ \hat \theta  \end{bmatrix} $$ The gradient of function $ \phi $ $$\nabla \phi = \hat i \frac{\partial \phi}{\partial x} + \hat j \frac{\partial \phi}{\partial y}$$ $$= \hat r \left( \cos \theta \frac{\partial \phi}{\partial x}  + \sin \theta \frac{\partial \phi}{\partial y} \right) + \hat \theta \left( -\sin \theta \frac{\partial \phi}{\partial x}  +  \cos \theta \frac{\partial \phi}{\partial y} \right)$$ $$= \hat r \left( \cos \theta \frac{\partial \phi}{\partial r} \frac{\partial r}{\partial x}  + \sin \theta \frac{\partial \phi}{\partial r} \frac{\partial r}{\partial y} \right) + \hat \theta \left( -\sin \theta \frac{\partial \phi}{\partial \theta} \frac{\partial \theta}{\partial x}  +  \cos \theta \frac{\partial \phi}{\partial \theta}\frac{\partial \theta}{\partial y} \right)$$ $$= \hat r \left( \cos \theta \frac{\partial \phi}{\partial r} \frac{1}{\cos \theta}   + \sin \theta \frac{\partial \phi}{\partial r} \frac{1}{\sin \theta} \right) + \hat \theta  \left( -\sin \theta \frac{\partial \phi}{\partial \theta} \frac{1}{- r \sin \theta }  + \cos \theta \frac{\partial \phi}{\partial \theta}\frac{1}{ r\cos \theta } \right)$$ $$ = 2 \left (  \frac{\partial \phi }{\partial r}\hat r + \frac{1}{r} \frac{\partial \phi}{\partial \theta } \hat \theta\right  )$$ I don't know what went wrong. Please help. Thank you!!",,"['multivariable-calculus', 'coordinate-systems', 'polar-coordinates']"
21,Differential operators on the sphere,Differential operators on the sphere,,"The sphere $\mathbb{S}^2$ is a Riemannian submanifold of the Euclidean space $\mathbb{R}^3$ and as such comes equipped with an array of differential operators, particularly gradient, divergence and Laplace-Beltrami. Can we compute them in terms of the corresponding Euclidean operators? Specifically: Let $f$ be a smooth function and $\mathbb{A}$ a smooth vector field on the unit sphere. Denote $\tilde{f}, \tilde{\mathbf{A}}$ the smooth function and vector field on $\mathbb{R}^3 \setminus \{O\}$ defined by the identity $$\tilde{f}(x)=f\left(\frac{x}{\lvert x \rvert}\right),\ \tilde{\mathbf{A}}(x)=\mathbf{A}\left( \frac{x}{\lvert x \rvert}\right).$$ Is it true that $\mathrm{grad}_{\mathbb{S}^2} f(y)=\mathrm{grad}_{\mathbb{R}^3} \tilde{f}(y)$ ; $\mathrm{div}_{\mathbb{S}^2}\mathbf{A}(y)=\mathrm{div}_{\mathbb{R}^3} \tilde{\mathbf{A}}(y)$ ; $\Delta_{\mathbb{S}^2}f(y)=\Delta_{\mathbb{R}^3}\tilde{f}(y)$ ; for all $y \in \mathbb{S}^2$ ? (secondary) More generally, if $\mathbf{T}$ is a tensor field on $\mathbb{S}^2$ and $\tilde{\mathbf{T}}(x)=\mathbf{T}(x/\lvert x \rvert)$ is the corresponding tensor field on $\mathbb{R}^3\setminus\{O\}$ , is there any relationship similar to the ones above between the covariant derivative $\nabla^{(\mathbb{S}^2)}_X \mathbf{T}$ and the Euclidean derivative of $\tilde{\mathbf{T}}$ ?","The sphere is a Riemannian submanifold of the Euclidean space and as such comes equipped with an array of differential operators, particularly gradient, divergence and Laplace-Beltrami. Can we compute them in terms of the corresponding Euclidean operators? Specifically: Let be a smooth function and a smooth vector field on the unit sphere. Denote the smooth function and vector field on defined by the identity Is it true that ; ; ; for all ? (secondary) More generally, if is a tensor field on and is the corresponding tensor field on , is there any relationship similar to the ones above between the covariant derivative and the Euclidean derivative of ?","\mathbb{S}^2 \mathbb{R}^3 f \mathbb{A} \tilde{f}, \tilde{\mathbf{A}} \mathbb{R}^3 \setminus \{O\} \tilde{f}(x)=f\left(\frac{x}{\lvert x \rvert}\right),\ \tilde{\mathbf{A}}(x)=\mathbf{A}\left( \frac{x}{\lvert x \rvert}\right). \mathrm{grad}_{\mathbb{S}^2} f(y)=\mathrm{grad}_{\mathbb{R}^3} \tilde{f}(y) \mathrm{div}_{\mathbb{S}^2}\mathbf{A}(y)=\mathrm{div}_{\mathbb{R}^3} \tilde{\mathbf{A}}(y) \Delta_{\mathbb{S}^2}f(y)=\Delta_{\mathbb{R}^3}\tilde{f}(y) y \in \mathbb{S}^2 \mathbf{T} \mathbb{S}^2 \tilde{\mathbf{T}}(x)=\mathbf{T}(x/\lvert x \rvert) \mathbb{R}^3\setminus\{O\} \nabla^{(\mathbb{S}^2)}_X \mathbf{T} \tilde{\mathbf{T}}","['multivariable-calculus', 'riemannian-geometry']"
22,Does this multivariate function have only one maximum?,Does this multivariate function have only one maximum?,,"Let $X_1$ and $X_2$ be random variables (not of the same distribution and not independent). Both have a zero probability of being below $-1$. Their joint density is $\rho(x_1,x_2)$. Also, they both have finite expectations. Now, define the region $A = \{ (t_1,t_2)\in\mathbb{R}^2 \mid 0\le t_1,t_2<1  \text{ and }t_1+t_2<1  \}$, and define the function $f:A\to\mathbb{R}$ with $$f(t_1,t_2) = \int_{-1}^\infty\int_{-1}^\infty \log(1+t_1x_1+t_2x_2)\rho(x_1,x_2)\,dx_1dx_2.$$ Can we say something interesting about $f$? For example, does $f$ have at most only one local maximum?","Let $X_1$ and $X_2$ be random variables (not of the same distribution and not independent). Both have a zero probability of being below $-1$. Their joint density is $\rho(x_1,x_2)$. Also, they both have finite expectations. Now, define the region $A = \{ (t_1,t_2)\in\mathbb{R}^2 \mid 0\le t_1,t_2<1  \text{ and }t_1+t_2<1  \}$, and define the function $f:A\to\mathbb{R}$ with $$f(t_1,t_2) = \int_{-1}^\infty\int_{-1}^\infty \log(1+t_1x_1+t_2x_2)\rho(x_1,x_2)\,dx_1dx_2.$$ Can we say something interesting about $f$? For example, does $f$ have at most only one local maximum?",,"['calculus', 'probability', 'multivariable-calculus']"
23,Finding extremas of a three variable function,Finding extremas of a three variable function,,"Find all points on he portion of the plane $x+y+z=5$ in the first octant at which $f(x,y,z)=xy^2z^2$ has a maximum value. Attempt ; Since $x+y+z=5$; $x=5-y-z$. I plug this into the $f(x,y,z)$: $$f(5-y-z,y,z)=u(y,z)=y^2 z^2 (5-y-z)=\text{5 }y^2 z^2-y^3 z^2-y^2 z^3$$ Now I find critical points: $$u_y=10 yz^2-3y^2z^2-2yz^3=0$$ $$u_z=10 y^2 z-2 y^3 z-3 y^2 z^2=0$$ The solution for this system of equations is $y=z=0$.Therefore, $x=5$. So thats the only critical point $(0,0,5)$ I get and $f(0,0,5)=0$. The answer should be a max at $(1,1,2)$ according to the answer key. How do I get it? Any hints please.","Find all points on he portion of the plane $x+y+z=5$ in the first octant at which $f(x,y,z)=xy^2z^2$ has a maximum value. Attempt ; Since $x+y+z=5$; $x=5-y-z$. I plug this into the $f(x,y,z)$: $$f(5-y-z,y,z)=u(y,z)=y^2 z^2 (5-y-z)=\text{5 }y^2 z^2-y^3 z^2-y^2 z^3$$ Now I find critical points: $$u_y=10 yz^2-3y^2z^2-2yz^3=0$$ $$u_z=10 y^2 z-2 y^3 z-3 y^2 z^2=0$$ The solution for this system of equations is $y=z=0$.Therefore, $x=5$. So thats the only critical point $(0,0,5)$ I get and $f(0,0,5)=0$. The answer should be a max at $(1,1,2)$ according to the answer key. How do I get it? Any hints please.",,['multivariable-calculus']
24,Is there a constant $C$ such that $z=x^2+y^2+C$ is tangent to $x^2+y^2=z^2$?,Is there a constant  such that  is tangent to ?,C z=x^2+y^2+C x^2+y^2=z^2,"Per the title, does a constant $C$ exist such that the surface of the paraboloid $z=x^2+y^2+C$ is tangent to the surface of the cone $x^2+y^2=z^2$? How would I find this constant? Thanks a lot!","Per the title, does a constant $C$ exist such that the surface of the paraboloid $z=x^2+y^2+C$ is tangent to the surface of the cone $x^2+y^2=z^2$? How would I find this constant? Thanks a lot!",,['multivariable-calculus']
25,Solve equation system with trigonometric functions,Solve equation system with trigonometric functions,,"I need to maximize the function $$f(x,\theta) =x\sin\theta(xcos\theta + w - 2x)$$ which defines the area enclosed by a folded plate that forms a canal, where $w$ is the length of the plate, $x$ is the length of each folded piece, $w - 2x$ is the length of the part that isn't folded and $\theta$ is the angle at which the plate is folded. So I already found the partial derivatives of the function, which are $$ f_{x} = \sin\theta(2x\cos\theta +w- 4x) $$ $$ f_{\theta} = x[x\cos(2\theta) + \cos\theta(w-2x)] $$ And I have to solve the system  $$ \sin\theta(2x\cos\theta +w- 4x) = 0  $$ $$ [x\cos(2\theta) + \cos\theta(w-2x)] = 0 $$ but I have no idea how to start. The only solution I could find was $x = 0$ and $\sin\theta = 0$, but this solution is obviously useless because then it wouldn't be a canal but a flat unfolded plate.","I need to maximize the function $$f(x,\theta) =x\sin\theta(xcos\theta + w - 2x)$$ which defines the area enclosed by a folded plate that forms a canal, where $w$ is the length of the plate, $x$ is the length of each folded piece, $w - 2x$ is the length of the part that isn't folded and $\theta$ is the angle at which the plate is folded. So I already found the partial derivatives of the function, which are $$ f_{x} = \sin\theta(2x\cos\theta +w- 4x) $$ $$ f_{\theta} = x[x\cos(2\theta) + \cos\theta(w-2x)] $$ And I have to solve the system  $$ \sin\theta(2x\cos\theta +w- 4x) = 0  $$ $$ [x\cos(2\theta) + \cos\theta(w-2x)] = 0 $$ but I have no idea how to start. The only solution I could find was $x = 0$ and $\sin\theta = 0$, but this solution is obviously useless because then it wouldn't be a canal but a flat unfolded plate.",,"['linear-algebra', 'trigonometry', 'multivariable-calculus', 'optimization']"
26,A mountain given my an elliptic paraboloid.,A mountain given my an elliptic paraboloid.,,"The Question Suppose that a mountain has the shape of an elliptic paraboloid given by $z = c - ax^2 - by^2, a,b,c \in (0,\infty)$ and $x$ and $y$ are the east-west and north-south map coordinates, and $z$ is the altitude above sea level. At point $(1,1)$ , in what direction is the altitude increasing most rapidly? If a marble were released at $(1,1)$ in what direction would it begin to roll? My answer: To start I turned the paraboloid into a level set given by $ax^2 + by^2 + z = c$ and then found the gradient function $\nabla f = (2ax, 2by, 1)$ which at $(1,1)$ is $(2a, 2b, 1)$. But I know that this is the direction of the normal to the 'mountain' so it can't be the direction that the altitude is increasing most rapidly. Can someone please shed some light? For the second part I assume that the marble would roll in the opposite direction to the direction the altitude is increasing most rapidly. So just the negative of the answer to the last part!","The Question Suppose that a mountain has the shape of an elliptic paraboloid given by $z = c - ax^2 - by^2, a,b,c \in (0,\infty)$ and $x$ and $y$ are the east-west and north-south map coordinates, and $z$ is the altitude above sea level. At point $(1,1)$ , in what direction is the altitude increasing most rapidly? If a marble were released at $(1,1)$ in what direction would it begin to roll? My answer: To start I turned the paraboloid into a level set given by $ax^2 + by^2 + z = c$ and then found the gradient function $\nabla f = (2ax, 2by, 1)$ which at $(1,1)$ is $(2a, 2b, 1)$. But I know that this is the direction of the normal to the 'mountain' so it can't be the direction that the altitude is increasing most rapidly. Can someone please shed some light? For the second part I assume that the marble would roll in the opposite direction to the direction the altitude is increasing most rapidly. So just the negative of the answer to the last part!",,['multivariable-calculus']
27,Integration over a triangle,Integration over a triangle,,"Let $\Delta$ be a triangle with vertices $(0,0)$, $(0,1)$ and $(1,0)$ in ${\bf R}^2$. I want to compute $$I=\int\limits_\Delta x^2\mathrm{e}^{y^2}\;\mathrm{d}A.$$ This is what I've done so far: Note that the hypotenuse is given by the line $y=1-x$. Keep $x\in[0,1]$ fixed, then $y$ is between $1$ and $1-x$ this gives $I=\int_0^1\int_1^{1-x}x^2\mathrm{e}^{y^2}\;\mathrm{d}y\;\mathrm{d}x$, which can't be computed. But when $y\in[0,1]$ is fixed, $x$ is between $0$ and $1-y$, which gives $$I=\int\limits_0^1\int\limits_1^{1-y}x^2\mathrm{e}^{y^2}\;\mathrm{d}x\;\mathrm{d}y=\frac13\int\limits_0^1(1-y)^3\mathrm{e}^{y^2}\;\mathrm{d}y,$$ which gives the same trouble. As you can see, I keep ending up with some sort of Gaussian integral, which is impossible to compute. Does anyone know how to compute this integral?","Let $\Delta$ be a triangle with vertices $(0,0)$, $(0,1)$ and $(1,0)$ in ${\bf R}^2$. I want to compute $$I=\int\limits_\Delta x^2\mathrm{e}^{y^2}\;\mathrm{d}A.$$ This is what I've done so far: Note that the hypotenuse is given by the line $y=1-x$. Keep $x\in[0,1]$ fixed, then $y$ is between $1$ and $1-x$ this gives $I=\int_0^1\int_1^{1-x}x^2\mathrm{e}^{y^2}\;\mathrm{d}y\;\mathrm{d}x$, which can't be computed. But when $y\in[0,1]$ is fixed, $x$ is between $0$ and $1-y$, which gives $$I=\int\limits_0^1\int\limits_1^{1-y}x^2\mathrm{e}^{y^2}\;\mathrm{d}x\;\mathrm{d}y=\frac13\int\limits_0^1(1-y)^3\mathrm{e}^{y^2}\;\mathrm{d}y,$$ which gives the same trouble. As you can see, I keep ending up with some sort of Gaussian integral, which is impossible to compute. Does anyone know how to compute this integral?",,"['calculus', 'integration', 'multivariable-calculus', 'vector-analysis']"
28,Jacobian matrix normalization,Jacobian matrix normalization,,"I have a problem with normalization of the Jacobian matrix. There seems to be no clear method for doing it: in some literature, it has been normalized by using some characteristic length, which is mathematically correct, but the problem is what this characteristic length should be. (Mostly, in robotics, the characteristic length is the distance from the base coordinates to the platform (end effector) coordinates.) It is confusing. My Jacobian matrix is $6 \times 6$, with the first three columns having units of $\mathrm{rad}/\mathrm{L}$ and the last three columns being dimensionless. My question is, how can I make the entire Jacobian matrix dimensionless? Is there a way to normalize this matrix according to some mathematical relations (theory)?","I have a problem with normalization of the Jacobian matrix. There seems to be no clear method for doing it: in some literature, it has been normalized by using some characteristic length, which is mathematically correct, but the problem is what this characteristic length should be. (Mostly, in robotics, the characteristic length is the distance from the base coordinates to the platform (end effector) coordinates.) It is confusing. My Jacobian matrix is $6 \times 6$, with the first three columns having units of $\mathrm{rad}/\mathrm{L}$ and the last three columns being dimensionless. My question is, how can I make the entire Jacobian matrix dimensionless? Is there a way to normalize this matrix according to some mathematical relations (theory)?",,"['matrices', 'multivariable-calculus', 'physics', 'coordinate-systems']"
29,Differentiable at the orgin and plot of a function.,Differentiable at the orgin and plot of a function.,,"My first question is: 1) Is $f(x,y)=\dfrac {xy(x^{2}-y^{2})}{{(x^{2}+y^{2})}^{3/2}}$ differntiable at $(0,0)$ ? Considering polar co-ordinates: $x=r\cos \theta$ and $y=r\sin \theta$ . $$\begin{align*} \Rightarrow f(x,y) &= \frac {r\cos \theta r\sin \theta (r^{2}\cos ^{2}\theta-r^{2}\sin ^{2}\theta)}{{(r^{2}\cos ^{2}\theta+r^{2}\sin ^{2}\theta)}^{3/2}} \\ \\ &= \frac {r^{4}\cos\theta\sin\theta(\cos ^{2}\theta-\sin ^{2}\theta)}{r^{3}} \\ \\ &= r\cos\theta\sin\theta(\cos ^{2}\theta-\sin ^{2}\theta) \end{align*}$$ Hence linear in $r$ . Therefore there doesn't exist a unique tangent plane at $(0,0)$ . Therefore $f(x,y)$ is not differentiable there. 2) Considering $$f(x,y)= \left\{\begin{array}{ll} \dfrac {x^{3}y}{x^{6}+y^{2}} & \text{if }(x,y) \neq 0, \\ \\ 0 & \text{if }(x,y)=(0,0). \end{array}\right.$$ If you were to plot the function $\theta \mapsto f(r\cos\theta, r\sin\theta)$ for $\theta \in [0,2\pi]$ what might the plot look like. Justify your answer. I am bit unsure on what this plot would be and the reason. I'm guessing at the crinkle function? Many thanks in advance.",My first question is: 1) Is differntiable at ? Considering polar co-ordinates: and . Hence linear in . Therefore there doesn't exist a unique tangent plane at . Therefore is not differentiable there. 2) Considering If you were to plot the function for what might the plot look like. Justify your answer. I am bit unsure on what this plot would be and the reason. I'm guessing at the crinkle function? Many thanks in advance.,"f(x,y)=\dfrac {xy(x^{2}-y^{2})}{{(x^{2}+y^{2})}^{3/2}} (0,0) x=r\cos \theta y=r\sin \theta \begin{align*}
\Rightarrow f(x,y) &= \frac {r\cos \theta r\sin \theta (r^{2}\cos ^{2}\theta-r^{2}\sin ^{2}\theta)}{{(r^{2}\cos ^{2}\theta+r^{2}\sin ^{2}\theta)}^{3/2}} \\
\\
&= \frac {r^{4}\cos\theta\sin\theta(\cos ^{2}\theta-\sin ^{2}\theta)}{r^{3}} \\
\\
&= r\cos\theta\sin\theta(\cos ^{2}\theta-\sin ^{2}\theta)
\end{align*} r (0,0) f(x,y) f(x,y)= \left\{\begin{array}{ll}
\dfrac {x^{3}y}{x^{6}+y^{2}} & \text{if }(x,y) \neq 0, \\
\\
0 & \text{if }(x,y)=(0,0).
\end{array}\right. \theta \mapsto f(r\cos\theta, r\sin\theta) \theta \in [0,2\pi]","['calculus', 'multivariable-calculus']"
30,Curvature from implicit differentiation,Curvature from implicit differentiation,,"I'm wrestling with a problem from Calculus 3 and I would appreciate a slight push in the right direction. This is how the problem is stated: The equations $x + xy + z^3 = 0$ and $\sin (xyz) = 0$ define a plane in $\mathbb{R}^3$. a) Use implicit differentiation to determine if the equations define $y$ and $z$ as functions of $x$ in a neighborhood around the point $(1,0,−1)$ and find $(\frac{dy}{dx},\frac{dz}{dx})$ in that point. b) Determine the curvature of the intersection of the planes at the point $(1,0,-1)$ Solution to a: The solution is a bit tedious to type in latex so I'll give you the cliffs. Define $F(x,y,z) = x + xy + z^3$  and $G(x,y,z) = \sin (xyz)$. $y$ and $z$ are indeed functions of $x$ in the neighborhood of $(1,0,-1)$ and $\left.(\frac{dy}{dx},\frac{dz}{dx})\right|_{(1,0,-1)} = (0,-\frac{1}{3})$ Solution to b) This is where I get stumped. I have no idea how to continue solving this problem. I understand that to find the curvature I need to transform something into a parametric equation, differentiate that etc but as I said - completely stumped. Any hint will be welcomed. Thank you for taking the time to read this - I appreciate it!","I'm wrestling with a problem from Calculus 3 and I would appreciate a slight push in the right direction. This is how the problem is stated: The equations $x + xy + z^3 = 0$ and $\sin (xyz) = 0$ define a plane in $\mathbb{R}^3$. a) Use implicit differentiation to determine if the equations define $y$ and $z$ as functions of $x$ in a neighborhood around the point $(1,0,−1)$ and find $(\frac{dy}{dx},\frac{dz}{dx})$ in that point. b) Determine the curvature of the intersection of the planes at the point $(1,0,-1)$ Solution to a: The solution is a bit tedious to type in latex so I'll give you the cliffs. Define $F(x,y,z) = x + xy + z^3$  and $G(x,y,z) = \sin (xyz)$. $y$ and $z$ are indeed functions of $x$ in the neighborhood of $(1,0,-1)$ and $\left.(\frac{dy}{dx},\frac{dz}{dx})\right|_{(1,0,-1)} = (0,-\frac{1}{3})$ Solution to b) This is where I get stumped. I have no idea how to continue solving this problem. I understand that to find the curvature I need to transform something into a parametric equation, differentiate that etc but as I said - completely stumped. Any hint will be welcomed. Thank you for taking the time to read this - I appreciate it!",,['multivariable-calculus']
31,differentiability/partial derivatives,differentiability/partial derivatives,,"This is a problem from a previous graduate preliminary exam in multivariable analysis/calculus that I am trying to solve for my own practice: Problem: Let $f:\mathbb{R}^{2}\rightarrow \mathbb{R}$ be a twice continuously differentiable function satisfying: $f\left ( 0,y \right )=0$ for all $y\in \mathbb{R}$. 1- Prove that $f\left ( x,y \right )=x.g\left ( x,y \right )$ for all $\left ( x,y \right )\in \mathbb{R}^{2}$, where:  $g\left ( x,y \right )=\int_{0}^{1}\frac{\partial f\left ( tx,y \right )}{\partial x}dt$. 2- Show that $g$ is continuously differentiable, and that for all $x$ in $\mathbb{R}$: $g\left ( 0,y \right )=\frac{\partial f}{\partial x}\left ( 0,y \right )$ and $\frac{\partial g}{\partial y}\left ( 0,y \right )=\frac{\partial ^{2}f}{\partial x\partial y}\left ( 0,y \right )$ For the first part: The only thing I could do so far is the following: I am basically trying to start from the right hand side to reach the left hand side. Note that: $g\left ( x,y \right )=\int_{0}^{1}\frac{\partial f\left ( tx,y \right )}{\partial x}dt=\int_{0}^{1}t\frac{\partial f\left ( u,y \right )}{\partial u}dt$ where $u=tx$. Then, I have no idea how to go forward? Any help is appreciated.","This is a problem from a previous graduate preliminary exam in multivariable analysis/calculus that I am trying to solve for my own practice: Problem: Let $f:\mathbb{R}^{2}\rightarrow \mathbb{R}$ be a twice continuously differentiable function satisfying: $f\left ( 0,y \right )=0$ for all $y\in \mathbb{R}$. 1- Prove that $f\left ( x,y \right )=x.g\left ( x,y \right )$ for all $\left ( x,y \right )\in \mathbb{R}^{2}$, where:  $g\left ( x,y \right )=\int_{0}^{1}\frac{\partial f\left ( tx,y \right )}{\partial x}dt$. 2- Show that $g$ is continuously differentiable, and that for all $x$ in $\mathbb{R}$: $g\left ( 0,y \right )=\frac{\partial f}{\partial x}\left ( 0,y \right )$ and $\frac{\partial g}{\partial y}\left ( 0,y \right )=\frac{\partial ^{2}f}{\partial x\partial y}\left ( 0,y \right )$ For the first part: The only thing I could do so far is the following: I am basically trying to start from the right hand side to reach the left hand side. Note that: $g\left ( x,y \right )=\int_{0}^{1}\frac{\partial f\left ( tx,y \right )}{\partial x}dt=\int_{0}^{1}t\frac{\partial f\left ( u,y \right )}{\partial u}dt$ where $u=tx$. Then, I have no idea how to go forward? Any help is appreciated.",,"['real-analysis', 'analysis', 'multivariable-calculus']"
32,Elementary vector calculus: Divergence of a field,Elementary vector calculus: Divergence of a field,,"How do you find the divergence of a field $f(\vec{r})=\vec{r}\exp(r^2)$ where $\vec{r}$ is the position vector and $r$ is its magnitude? In other words, how does one evaluate $\nabla\cdot[\vec{r}\exp(r^2)]$? I think we could do it by writing $f(\vec{r})$ as a column vector and differentiating each component wrt their variable and I get $\exp(r^2)(3+2r^2)$ is that right? Is there a quicker way to do this?","How do you find the divergence of a field $f(\vec{r})=\vec{r}\exp(r^2)$ where $\vec{r}$ is the position vector and $r$ is its magnitude? In other words, how does one evaluate $\nabla\cdot[\vec{r}\exp(r^2)]$? I think we could do it by writing $f(\vec{r})$ as a column vector and differentiating each component wrt their variable and I get $\exp(r^2)(3+2r^2)$ is that right? Is there a quicker way to do this?",,"['multivariable-calculus', 'vector-spaces']"
33,Need help with multi-variable calculus,Need help with multi-variable calculus,,"I have an expression of the form: $$\int\limits_{-\infty}^{\infty}\exp \left\{ \frac{-1}{2\sigma^2} (x-\mu)^2 \right\} dx = \sqrt{2\pi\sigma^2}$$ and I need to take its derivative with respect to $\sigma^2$.  The right hand side seems easy enough: $$\frac{\partial}{\partial\sigma^2}{(2\pi\sigma^2)}^{\frac{1}{2}} =  \frac{1}{2}\sqrt{2\pi}(\sigma^2)^{\frac{-1}{2}},$$ right?  What about the left hand side?  I don't know how to take the derivative of something that's inside an integral -- can someone please help?  I'm more interested in understanding how it would work then just the answer itself. EDIT: corrected RHS. EDIT 2: Through applying the chain rule, the LHS becomes: $$\int\limits_{-\infty}^\infty \frac{1}{2}(x-\mu)^2\frac{1}{\sigma^4} \exp{\lbrace \frac{-1}{2\sigma^2} (x-\mu)^2 \rbrace } dx $$ Does this look right?  I'm unsure of how to handle the derivative of a function w.r.t to a squared variable (e.g. $\frac{\partial}{\partial\sigma^2}$ as opposed to $\frac{\partial}{\partial\alpha}$).  For example, is this true: $\frac{\partial}{\partial\sigma^2} \sigma^{-2} = -\sigma^{-4}$ ?","I have an expression of the form: $$\int\limits_{-\infty}^{\infty}\exp \left\{ \frac{-1}{2\sigma^2} (x-\mu)^2 \right\} dx = \sqrt{2\pi\sigma^2}$$ and I need to take its derivative with respect to $\sigma^2$.  The right hand side seems easy enough: $$\frac{\partial}{\partial\sigma^2}{(2\pi\sigma^2)}^{\frac{1}{2}} =  \frac{1}{2}\sqrt{2\pi}(\sigma^2)^{\frac{-1}{2}},$$ right?  What about the left hand side?  I don't know how to take the derivative of something that's inside an integral -- can someone please help?  I'm more interested in understanding how it would work then just the answer itself. EDIT: corrected RHS. EDIT 2: Through applying the chain rule, the LHS becomes: $$\int\limits_{-\infty}^\infty \frac{1}{2}(x-\mu)^2\frac{1}{\sigma^4} \exp{\lbrace \frac{-1}{2\sigma^2} (x-\mu)^2 \rbrace } dx $$ Does this look right?  I'm unsure of how to handle the derivative of a function w.r.t to a squared variable (e.g. $\frac{\partial}{\partial\sigma^2}$ as opposed to $\frac{\partial}{\partial\alpha}$).  For example, is this true: $\frac{\partial}{\partial\sigma^2} \sigma^{-2} = -\sigma^{-4}$ ?",,"['calculus', 'multivariable-calculus']"
34,passing the Derivative inside an integral,passing the Derivative inside an integral,,"Question: Suppose we have: $F(x)=\int_{a(x)}^{b(x)}e^{h(x,t)}dt$. Is it true that $F^{'}(x)=\int_{a(x)}^{b(x)}\frac{\partial h(x,t)}{\partial x}.e^{h(x,t)}dt$  ? Please tell me under what condition(s) am I allowed to use this?","Question: Suppose we have: $F(x)=\int_{a(x)}^{b(x)}e^{h(x,t)}dt$. Is it true that $F^{'}(x)=\int_{a(x)}^{b(x)}\frac{\partial h(x,t)}{\partial x}.e^{h(x,t)}dt$  ? Please tell me under what condition(s) am I allowed to use this?",,"['calculus', 'real-analysis', 'analysis', 'multivariable-calculus']"
35,Switching the order of integraton,Switching the order of integraton,,"I'm trying to figure out how to switch the order of integration for this problem.  I'm given the region (I don't know how to use latex, maybe someone can clean this up for me?) $z$ from $0$ to $x+y$ $y$ from $0$ to $1-x$ $x$ from $0$ to $1$ That was for the integral $dz\;dy\;dx$ and I have to change it to $dy\;dx\;dz$.  I think maybe I drew out the region wrong and that's why I can't figure this out.  The one I drew looks like a right triangle.  In the first octant The $x$ axis from 0 to 1 and the hypotenuse of the triangle is the line $1-x$ then it goes out in $y$ from 0 to 1 (I can upload the picture if it helps).  I don't think I'm taking the $x+y$ part into account anywhere?  The new integral I came up with is $y$ from $0$ to $1$ $x$ from $0$ to $1-y$ $z$ from $0$ to $1$","I'm trying to figure out how to switch the order of integration for this problem.  I'm given the region (I don't know how to use latex, maybe someone can clean this up for me?) $z$ from $0$ to $x+y$ $y$ from $0$ to $1-x$ $x$ from $0$ to $1$ That was for the integral $dz\;dy\;dx$ and I have to change it to $dy\;dx\;dz$.  I think maybe I drew out the region wrong and that's why I can't figure this out.  The one I drew looks like a right triangle.  In the first octant The $x$ axis from 0 to 1 and the hypotenuse of the triangle is the line $1-x$ then it goes out in $y$ from 0 to 1 (I can upload the picture if it helps).  I don't think I'm taking the $x+y$ part into account anywhere?  The new integral I came up with is $y$ from $0$ to $1$ $x$ from $0$ to $1-y$ $z$ from $0$ to $1$",,"['calculus', 'multivariable-calculus']"
36,A question regarding local minimizer of a function restricted on a circle,A question regarding local minimizer of a function restricted on a circle,,"I have a quadratic function $f: \mathbb{R}^2 \rightarrow \mathbb{R}$, $f(\mathbf{x}) = (\mathbf{x}-\mathbf{p})^\top \mathbf{Q} (\mathbf{x} - \mathbf{p})$ where $\mathbf{Q}$ is positive definite and $\mathbf{p} \in \mathbb{R}^2$. I want to find $\mathbf{x}$ satisfying $\| \mathbf{x} \|_2 = 1$ that (locally) minimizes $f$. The condition for a point $\mathbf{x}$ to be a critical point should be: $$ \nabla f(\mathbf{x}) = \lambda\mathbf{x} $$ $$ 2\mathbf{Q}(\mathbf{x}-\mathbf{p}) = \lambda\mathbf{x} $$ for some $\lambda$. My questions is: Does the condition that a critical point $\mathbf{x}$ (locally) minimizes $f$ is as follows? $$ \left<\nabla \left<\nabla f(\mathbf{x}), \mathbf{x}^\perp \right>, \mathbf{x}^\perp \right> > 0 $$ where  $\left<  ,  \right>$ is the dot product, and $\left< \mathbf{x}^\perp, \mathbf{x}\right> = 0$. (the second-order directional derivative of f at $\mathbf{x}$, direction: $\mathbf{x}^\perp$, is greater than zero.) If not, what is it? Thanks in advance.","I have a quadratic function $f: \mathbb{R}^2 \rightarrow \mathbb{R}$, $f(\mathbf{x}) = (\mathbf{x}-\mathbf{p})^\top \mathbf{Q} (\mathbf{x} - \mathbf{p})$ where $\mathbf{Q}$ is positive definite and $\mathbf{p} \in \mathbb{R}^2$. I want to find $\mathbf{x}$ satisfying $\| \mathbf{x} \|_2 = 1$ that (locally) minimizes $f$. The condition for a point $\mathbf{x}$ to be a critical point should be: $$ \nabla f(\mathbf{x}) = \lambda\mathbf{x} $$ $$ 2\mathbf{Q}(\mathbf{x}-\mathbf{p}) = \lambda\mathbf{x} $$ for some $\lambda$. My questions is: Does the condition that a critical point $\mathbf{x}$ (locally) minimizes $f$ is as follows? $$ \left<\nabla \left<\nabla f(\mathbf{x}), \mathbf{x}^\perp \right>, \mathbf{x}^\perp \right> > 0 $$ where  $\left<  ,  \right>$ is the dot product, and $\left< \mathbf{x}^\perp, \mathbf{x}\right> = 0$. (the second-order directional derivative of f at $\mathbf{x}$, direction: $\mathbf{x}^\perp$, is greater than zero.) If not, what is it? Thanks in advance.",,"['calculus', 'multivariable-calculus', 'optimization', 'quadratic-forms']"
37,Vector field and normal of the field are both gradient fields,Vector field and normal of the field are both gradient fields,,"Are there any general conditions to use to find a vector field f(x,y) that is a gradient field and f(-y,x) is also a gradient field.  It seems to me like if their second partial derivatives are zero then this is true or at least I haven't find an exception to that yet.","Are there any general conditions to use to find a vector field f(x,y) that is a gradient field and f(-y,x) is also a gradient field.  It seems to me like if their second partial derivatives are zero then this is true or at least I haven't find an exception to that yet.",,['multivariable-calculus']
38,Why is the following a solution to the system?,Why is the following a solution to the system?,,"I have the following in my notes, but I can't remember how it works. Please help! $\nabla^2\psi=0, \quad\psi\to 0\quad\text{as}\quad x^2+y^2\to\infty, \quad\psi (x,y,0)$ is continuous Then by using Green's function, we get the solution to be $$\psi(x',y',z')={z'\over 2\pi}\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty [(x-x')^2+(y-y')^2+z'^2]^{-3\over 2}\psi(x,y,0)\,\,\,dxdy\;.$$ (This part I am sure about.) The primed $x',y',z'$ are the variables introduced when using the Green's function $G(\vec{x};\vec{x'})$. Why does this satisfy the boundary conditions? I am thinking that this solution is equivalent to $$\psi(x,y,z)={z\over 2\pi}\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty [(x-x')^2+(y-y')^2+z^2]^{-3\over 2}\psi(x',y',0)\,\,\,dx'dy'\;.$$ But doesn't this imply that $\psi(x,y,0)\equiv 0? $ -- Not supposed to be true. As an aside, are harmonic functions always spherically symmetrical? Also, is it possible to actually evaluate that integral?","I have the following in my notes, but I can't remember how it works. Please help! $\nabla^2\psi=0, \quad\psi\to 0\quad\text{as}\quad x^2+y^2\to\infty, \quad\psi (x,y,0)$ is continuous Then by using Green's function, we get the solution to be $$\psi(x',y',z')={z'\over 2\pi}\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty [(x-x')^2+(y-y')^2+z'^2]^{-3\over 2}\psi(x,y,0)\,\,\,dxdy\;.$$ (This part I am sure about.) The primed $x',y',z'$ are the variables introduced when using the Green's function $G(\vec{x};\vec{x'})$. Why does this satisfy the boundary conditions? I am thinking that this solution is equivalent to $$\psi(x,y,z)={z\over 2\pi}\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty [(x-x')^2+(y-y')^2+z^2]^{-3\over 2}\psi(x',y',0)\,\,\,dx'dy'\;.$$ But doesn't this imply that $\psi(x,y,0)\equiv 0? $ -- Not supposed to be true. As an aside, are harmonic functions always spherically symmetrical? Also, is it possible to actually evaluate that integral?",,"['integration', 'multivariable-calculus', 'partial-differential-equations']"
39,Calculate curl of vector function in $\mathbb{R}^3$,Calculate curl of vector function in,\mathbb{R}^3,"I know from definition that if  some   vector  function $\mathbf{u}$ is given in three dimensional  space, then curl is defined by this $$\operatorname{curl}\mathbf{u}=\nabla\times \mathbf{u}=\left|\begin{matrix}\mathbf{i} & \mathbf{j} & \mathbf{k}\\ D_x & D_y & D_z\\ u_x & u_y & u_z\end{matrix}\right|$$ but unfortunately I forgot  what represents subscript $D_x$. Is it the same as $u_x$? Because  last one represents   partial derivative  and first one what is it? Please help me.","I know from definition that if  some   vector  function $\mathbf{u}$ is given in three dimensional  space, then curl is defined by this $$\operatorname{curl}\mathbf{u}=\nabla\times \mathbf{u}=\left|\begin{matrix}\mathbf{i} & \mathbf{j} & \mathbf{k}\\ D_x & D_y & D_z\\ u_x & u_y & u_z\end{matrix}\right|$$ but unfortunately I forgot  what represents subscript $D_x$. Is it the same as $u_x$? Because  last one represents   partial derivative  and first one what is it? Please help me.",,['multivariable-calculus']
40,Volume of a cut sphere,Volume of a cut sphere,,The sphere $x^2 + y^2 + z^2 = 4$ is cut by the plane $z = 1/2$. How do you calculate the volume of two parts of the sphere using integrals? Thank you!,The sphere $x^2 + y^2 + z^2 = 4$ is cut by the plane $z = 1/2$. How do you calculate the volume of two parts of the sphere using integrals? Thank you!,,['multivariable-calculus']
41,Triple integral integration limits,Triple integral integration limits,,Integral $\iiint\limits_D \frac{1}{(x+y+z)^3} dxdydz$ should be evaluated. D is area bounded by coordinate planes and $x+y+z=1$ plane. I need help with determining integration limits. What software would you recommend for drawing 3D objects to develop reasoning for this kind of problems?,Integral $\iiint\limits_D \frac{1}{(x+y+z)^3} dxdydz$ should be evaluated. D is area bounded by coordinate planes and $x+y+z=1$ plane. I need help with determining integration limits. What software would you recommend for drawing 3D objects to develop reasoning for this kind of problems?,,['multivariable-calculus']
42,Matrix inversion approach using gradient descent curve,Matrix inversion approach using gradient descent curve,,"Suppose i have a square matrix $A_{ij}$ and a test matrix $M_{ij}$. I want to find the integral curve that joins $M_{ij}$ to $A^{-1}_{ij}$ Following the gradient descent path of the following cost function: $$ F_{A}(M) := \sum_{ij} [( M A - I )_{ij}]^2  $$ Of course, the curve does not always end in to $A^{-1}$ because sometimes $M$ lies in a local-minima basin (i.e. not connected to the global minima). Locally, the gradient descent path is described by the gradient of $F_{A}$ $$ \nabla_{M} F_{A} = 2 (M A - I ) A^{T}  $$ or in component notation (trying very hard to not use Einstein summation notation), looks like: $$ \frac{\partial F_{A}^{(ij)} }{ \partial M_{xy}} = 2 \sum_{k} ( \sum_{p} (M_{ip} A_{pk}) - \delta_{ik} ) A_{jk}   $$ now, it is pretty clear from the above expression that its third derivative is zero. So if i try to compute the gradient descent path to higher orders, it seems that i'm left with a quadratic curve with no further corrections. Question: what is wrong with this approach to obtain a matrix inverse? It seems strange to me that the gradient descent curve is a simple polynomial form. Maybe i'm doing something wrong? I feel like i'm missing something very obvious, but i can't think right now what is it.","Suppose i have a square matrix $A_{ij}$ and a test matrix $M_{ij}$. I want to find the integral curve that joins $M_{ij}$ to $A^{-1}_{ij}$ Following the gradient descent path of the following cost function: $$ F_{A}(M) := \sum_{ij} [( M A - I )_{ij}]^2  $$ Of course, the curve does not always end in to $A^{-1}$ because sometimes $M$ lies in a local-minima basin (i.e. not connected to the global minima). Locally, the gradient descent path is described by the gradient of $F_{A}$ $$ \nabla_{M} F_{A} = 2 (M A - I ) A^{T}  $$ or in component notation (trying very hard to not use Einstein summation notation), looks like: $$ \frac{\partial F_{A}^{(ij)} }{ \partial M_{xy}} = 2 \sum_{k} ( \sum_{p} (M_{ip} A_{pk}) - \delta_{ik} ) A_{jk}   $$ now, it is pretty clear from the above expression that its third derivative is zero. So if i try to compute the gradient descent path to higher orders, it seems that i'm left with a quadratic curve with no further corrections. Question: what is wrong with this approach to obtain a matrix inverse? It seems strange to me that the gradient descent curve is a simple polynomial form. Maybe i'm doing something wrong? I feel like i'm missing something very obvious, but i can't think right now what is it.",,"['linear-algebra', 'matrices', 'multivariable-calculus', 'gradient-descent']"
43,Div and Curl Identities,Div and Curl Identities,,"If $F$ is a vector field, I understand that the div(curl $F$) = 0. But would the curl(div $F$) have any interpretation?","If $F$ is a vector field, I understand that the div(curl $F$) = 0. But would the curl(div $F$) have any interpretation?",,['multivariable-calculus']
44,Elementary question in partial differentiation,Elementary question in partial differentiation,,"Let's say we have a function of the form $f(x+vt)$ where $v$ is a constant and $x,t$ are independent variables. How is $\frac{\partial f}{\partial x} = \frac{1}{v}\frac{\partial f}{\partial t}$ equal to   $f$? If I let $u=x+vt$ then $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial u}\frac{\partial u}{\partial x} = \frac{\partial f/\partial t}{\partial u/\partial t}\frac{\partial u}{\partial x}=\frac{1}{v}\frac{\partial f}{\partial t}$  but I cannot infer that $ \frac{1}{v}\frac{\partial f}{\partial t} =  f$ unless I assume the form of D'Alembert's Solution to be the harmonic (exponential). For the general solution I do not know how this was arrived at. Edit: I still don't get it, as the context does not help. But I assume since it is a physics text, $f$ can be written as a Fourier series/integral of exponentials. Assuming that, the above holds.","Let's say we have a function of the form $f(x+vt)$ where $v$ is a constant and $x,t$ are independent variables. How is $\frac{\partial f}{\partial x} = \frac{1}{v}\frac{\partial f}{\partial t}$ equal to   $f$? If I let $u=x+vt$ then $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial u}\frac{\partial u}{\partial x} = \frac{\partial f/\partial t}{\partial u/\partial t}\frac{\partial u}{\partial x}=\frac{1}{v}\frac{\partial f}{\partial t}$  but I cannot infer that $ \frac{1}{v}\frac{\partial f}{\partial t} =  f$ unless I assume the form of D'Alembert's Solution to be the harmonic (exponential). For the general solution I do not know how this was arrived at. Edit: I still don't get it, as the context does not help. But I assume since it is a physics text, $f$ can be written as a Fourier series/integral of exponentials. Assuming that, the above holds.",,['multivariable-calculus']
45,Vector Theory Question,Vector Theory Question,,"I am having trouble getting started on this multi-part problem. Could anyone take a look and provide some insight on how I might go about coming to a solution for the first part. Q: Assume for two arbitrary vectors v and u that || u || = 3 and || v || = 5. What is the maximum value of || v + u ||? I've rewritten the magnitudes in their equation form but still am not having any light bulbs go off: || u ||=$\sqrt{(u_1)^2+(u_2)^2}$ || v ||=$\sqrt{(v_1)^2+(v_2)^2}$ Any help is appreciated, thanks.","I am having trouble getting started on this multi-part problem. Could anyone take a look and provide some insight on how I might go about coming to a solution for the first part. Q: Assume for two arbitrary vectors v and u that || u || = 3 and || v || = 5. What is the maximum value of || v + u ||? I've rewritten the magnitudes in their equation form but still am not having any light bulbs go off: || u ||=$\sqrt{(u_1)^2+(u_2)^2}$ || v ||=$\sqrt{(v_1)^2+(v_2)^2}$ Any help is appreciated, thanks.",,"['multivariable-calculus', 'optimization']"
46,"Points, Vectors, and Planes Oh My! (Finding parallel vectors and intersecting vectors)","Points, Vectors, and Planes Oh My! (Finding parallel vectors and intersecting vectors)",,"I'm quite stumped on the following problem: Consider the planes 4x + 1y + 1z = 1 and 4x + 1z = 0. (A) Find the unique point P on the y-axis which is on both planes. (_ , _ , __) (B) Find a unit vector u with positive first coordinate that is parallel to both planes __ I + _ _ J + _K (C) Use the vectors found in parts (A) and (B) to find a vector equation for the line of intersection of the two planes r(t) = __ I + _ _ J + _K Work thus far: I've figured out (A) is (0,1,0) Now, I know that the dot product of <4,1,1> and u will = 0, as well as the dot product between <4,0,1> and u ... I'm quite stumped. Can anyone help me out by giving me hints? I'd prefer that the entire solution isn't given yet, so I can work through it. I'll respond as quickly as I can. Thank you, Landon","I'm quite stumped on the following problem: Consider the planes 4x + 1y + 1z = 1 and 4x + 1z = 0. (A) Find the unique point P on the y-axis which is on both planes. (_ , _ , __) (B) Find a unit vector u with positive first coordinate that is parallel to both planes __ I + _ _ J + _K (C) Use the vectors found in parts (A) and (B) to find a vector equation for the line of intersection of the two planes r(t) = __ I + _ _ J + _K Work thus far: I've figured out (A) is (0,1,0) Now, I know that the dot product of <4,1,1> and u will = 0, as well as the dot product between <4,0,1> and u ... I'm quite stumped. Can anyone help me out by giving me hints? I'd prefer that the entire solution isn't given yet, so I can work through it. I'll respond as quickly as I can. Thank you, Landon",,['multivariable-calculus']
47,distance between lines in the space (with calculus),distance between lines in the space (with calculus),,"If I have two lines $$ \eqalign{   & L_1 \left( t \right):p_1  + td_1   \cr    & L_2 \left( q \right):p_2  + qd_2  \cr}  $$ living in $\mathbb{R}^n$, there exists a classical formula to find the distance between them involving dot and cross products.  The question is:  can I deduce that formula only using calculus? (In this case, 2 variables) i.e., find the values such that the function $$ f\left( {t,q} \right) = \left \| L_1 (t) - L_2(t) \right \| = \left \| p_1  + td_1  - p_2  - qd_2   \right\| $$ reaches its minimum value. Oh sorry; for simplicity, to have the natural cross product, just take $\mathbb{R}^3$.","If I have two lines $$ \eqalign{   & L_1 \left( t \right):p_1  + td_1   \cr    & L_2 \left( q \right):p_2  + qd_2  \cr}  $$ living in $\mathbb{R}^n$, there exists a classical formula to find the distance between them involving dot and cross products.  The question is:  can I deduce that formula only using calculus? (In this case, 2 variables) i.e., find the values such that the function $$ f\left( {t,q} \right) = \left \| L_1 (t) - L_2(t) \right \| = \left \| p_1  + td_1  - p_2  - qd_2   \right\| $$ reaches its minimum value. Oh sorry; for simplicity, to have the natural cross product, just take $\mathbb{R}^3$.",,"['multivariable-calculus', 'optimization']"
48,understanding of a proof of the invariance of 3-D laplacian?,understanding of a proof of the invariance of 3-D laplacian?,,"I learned the proof of the fact that 3-D laplacian is invariant under all rigid motions in space in Strauss's Partial Differential Equations : Any rotation in three dimensions is given by   $${\bf x'}=B{\bf x},$$   where $B$ is an orthogonal matrix. The laplacian is   $$\Delta u=\sum_{i=1}^3 u_{ii}=\sum_{i,j=1}^3\delta_{ij}u_{ij}$$   where the subscripts on $u$ denote partial derivatives. Therefore,   $$ \Delta u=\sum_{k,l}\Big(\sum_{i,j}b_{ki}\delta_{ij}b_{lj}\Big)u_{k'l'} =\sum_{k,l}\delta_{kl}u_{k'l'} =\sum_{k}u_{k'k'}. $$ Added : I believe the chain rule is needs here, but I don't see how it is applied here. Here is my question : How can I get the first equality, i.e. $$\Delta u=\sum_{k,l}\Big(\sum_{i,j}b_{ki}\delta_{ij}b_{lj}\Big)u_{k'l'}?$$","I learned the proof of the fact that 3-D laplacian is invariant under all rigid motions in space in Strauss's Partial Differential Equations : Any rotation in three dimensions is given by   $${\bf x'}=B{\bf x},$$   where $B$ is an orthogonal matrix. The laplacian is   $$\Delta u=\sum_{i=1}^3 u_{ii}=\sum_{i,j=1}^3\delta_{ij}u_{ij}$$   where the subscripts on $u$ denote partial derivatives. Therefore,   $$ \Delta u=\sum_{k,l}\Big(\sum_{i,j}b_{ki}\delta_{ij}b_{lj}\Big)u_{k'l'} =\sum_{k,l}\delta_{kl}u_{k'l'} =\sum_{k}u_{k'k'}. $$ Added : I believe the chain rule is needs here, but I don't see how it is applied here. Here is my question : How can I get the first equality, i.e. $$\Delta u=\sum_{k,l}\Big(\sum_{i,j}b_{ki}\delta_{ij}b_{lj}\Big)u_{k'l'}?$$",,['linear-algebra']
49,Second differential at extremum,Second differential at extremum,,"As we learned in calculus, if a function $f:\mathbb{R} \rightarrow \mathbb{R}$ is twice differentiable, and has a local maximum at point $x_0$, then $f'(x_0) = 0$ and $f''(x_0) \leq 0$. In addition, it's not hard to show. What I have trouble with is the higher dimensional analogue: Let $f:\mathbb{R}^n \rightarrow \mathbb{R}$ be twice differentiable, and has a local maximum at $x_0\in \mathbb{R}^n$, then $Df(x_0) = 0$ and $D^2f(x_0) \leq 0$ (i.e. the symmetric matrix $-D^2f(x_0)$ is positive semidefinite).","As we learned in calculus, if a function $f:\mathbb{R} \rightarrow \mathbb{R}$ is twice differentiable, and has a local maximum at point $x_0$, then $f'(x_0) = 0$ and $f''(x_0) \leq 0$. In addition, it's not hard to show. What I have trouble with is the higher dimensional analogue: Let $f:\mathbb{R}^n \rightarrow \mathbb{R}$ be twice differentiable, and has a local maximum at $x_0\in \mathbb{R}^n$, then $Df(x_0) = 0$ and $D^2f(x_0) \leq 0$ (i.e. the symmetric matrix $-D^2f(x_0)$ is positive semidefinite).",,['multivariable-calculus']
50,Deducing the Definition of the Directional Derivative,Deducing the Definition of the Directional Derivative,,"I want to show that if a map $f$, defined on an open subset $X$ of a Banach space $E$ to another Banach space $Y$, is differentiable at a point $x_0 \in X$ then the directional derivative with respect to a nonzero vector $v \in E$ of $f$ at $x_0$ also exists. For this purpose, I will not assume the definition of the directional derivative but infer it from the consequences of the hypotheses above. There are two basic questions about this I would like to ask: Is my argument sound and is there a more concise argument that lead to the same conclusions? Now, assuming the above hypotheses, since $X$ is open we can take a sufficiently small $t$ such that $x_0 + tv$ lies in $X$. That is, there exists $\epsilon >0$ such that $x_0 + tv \in X$ for $|t| < \epsilon$. This means that the function $$ g(t) = f(x_0 + tv) $$ is defined on a neighborhood of $x_0$ Denote the derivative of $f$ at $x_0$ by $\partial f(x_0)$. Then, there exists a continuous map $r:X \rightarrow F$ such that $r(x_0) = 0$ and $$ f(x) = f(x_0) + \partial f(x_0)(x - x_0) + r(x)||x - x_0|| $$ Then, $$ g(t) = f(x_0 + tv) $$ $$ = f(x_0) + \partial f(x_0)( (x_0 + tv) - x_0) + r(x_0 + tv)||(x + tv) - x_0|| $$ $$ = f(x_0) +\partial f(x_0)(tv) + r(x_0 +tv)||tv|| $$ By homogeneity of the norm and linearity of the derivative, it follows that $$ f(x_0 + tv) = f(x_0) + t \partial f(x_0)(v) + |t|r(x_0 + tv)||v|| $$ From algebraic manipluation, it follows that $$ \frac{f(x_0 + tv) - f(x_0)}{t} - \partial f(x_0)(v) = \frac{|t|}{t} ||v||r(x_0 + tv) $$ Note that as $t \rightarrow 0$ the right hand side of the equation approaches $0$ since $\frac{|t|}{t}||v|| = ±||v||$ is constant and $$ \lim_{t \to 0} \; r(x_0 + tv) = r(x_0) = 0 $$ where the first equality follows from the continuity of $r$ and the last follows from differentiability of $f$ as noted above. Therefore, $$ \lim_{t \to 0} \frac{f(x_0 + tv) - f(x_0)}{t} = \partial f(x_0)(v) = D_v f(x_0) $$ where we obtain in this last expression the usual definition of the directional derivative $D_v f(x_0)$ of a function $f$ at $x_0$ with respect to the vector $v$.","I want to show that if a map $f$, defined on an open subset $X$ of a Banach space $E$ to another Banach space $Y$, is differentiable at a point $x_0 \in X$ then the directional derivative with respect to a nonzero vector $v \in E$ of $f$ at $x_0$ also exists. For this purpose, I will not assume the definition of the directional derivative but infer it from the consequences of the hypotheses above. There are two basic questions about this I would like to ask: Is my argument sound and is there a more concise argument that lead to the same conclusions? Now, assuming the above hypotheses, since $X$ is open we can take a sufficiently small $t$ such that $x_0 + tv$ lies in $X$. That is, there exists $\epsilon >0$ such that $x_0 + tv \in X$ for $|t| < \epsilon$. This means that the function $$ g(t) = f(x_0 + tv) $$ is defined on a neighborhood of $x_0$ Denote the derivative of $f$ at $x_0$ by $\partial f(x_0)$. Then, there exists a continuous map $r:X \rightarrow F$ such that $r(x_0) = 0$ and $$ f(x) = f(x_0) + \partial f(x_0)(x - x_0) + r(x)||x - x_0|| $$ Then, $$ g(t) = f(x_0 + tv) $$ $$ = f(x_0) + \partial f(x_0)( (x_0 + tv) - x_0) + r(x_0 + tv)||(x + tv) - x_0|| $$ $$ = f(x_0) +\partial f(x_0)(tv) + r(x_0 +tv)||tv|| $$ By homogeneity of the norm and linearity of the derivative, it follows that $$ f(x_0 + tv) = f(x_0) + t \partial f(x_0)(v) + |t|r(x_0 + tv)||v|| $$ From algebraic manipluation, it follows that $$ \frac{f(x_0 + tv) - f(x_0)}{t} - \partial f(x_0)(v) = \frac{|t|}{t} ||v||r(x_0 + tv) $$ Note that as $t \rightarrow 0$ the right hand side of the equation approaches $0$ since $\frac{|t|}{t}||v|| = ±||v||$ is constant and $$ \lim_{t \to 0} \; r(x_0 + tv) = r(x_0) = 0 $$ where the first equality follows from the continuity of $r$ and the last follows from differentiability of $f$ as noted above. Therefore, $$ \lim_{t \to 0} \frac{f(x_0 + tv) - f(x_0)}{t} = \partial f(x_0)(v) = D_v f(x_0) $$ where we obtain in this last expression the usual definition of the directional derivative $D_v f(x_0)$ of a function $f$ at $x_0$ with respect to the vector $v$.",,"['real-analysis', 'multivariable-calculus']"
51,Get vector components from from magnitude and angle,Get vector components from from magnitude and angle,,"I am given the length and the direction of a vector, and I need to get the the X,Y components.  I can go one way, but going the other has me a little lost. Example: A man walks 3.50 m south, then 8.20 m at an angle 24.6 degrees north of east, and finally 15.0 m west. What is the magnitude of the man's total displacement (m)? What is the direction of the man's total displacement where directly east is taken as zero degrees and counter-clockwise is positive (degrees)? I'm pretty sure that the answer will be  $$\langle 0, -3.5\rangle + \langle \text{?}, \text{?}\rangle + \langle-15, 0\rangle$$  where $\langle\text{?},\text{?}\rangle$ is the components of $\lVert v\rVert = 8.2$ and $\text{angle} = 24.6$.  From there I can find the magnitude and angle of the final vector which should be the answers to the 2 questions.  I'm just struggling with getting the components.","I am given the length and the direction of a vector, and I need to get the the X,Y components.  I can go one way, but going the other has me a little lost. Example: A man walks 3.50 m south, then 8.20 m at an angle 24.6 degrees north of east, and finally 15.0 m west. What is the magnitude of the man's total displacement (m)? What is the direction of the man's total displacement where directly east is taken as zero degrees and counter-clockwise is positive (degrees)? I'm pretty sure that the answer will be  $$\langle 0, -3.5\rangle + \langle \text{?}, \text{?}\rangle + \langle-15, 0\rangle$$  where $\langle\text{?},\text{?}\rangle$ is the components of $\lVert v\rVert = 8.2$ and $\text{angle} = 24.6$.  From there I can find the magnitude and angle of the final vector which should be the answers to the 2 questions.  I'm just struggling with getting the components.",,"['geometry', 'multivariable-calculus']"
52,Multiple Integral Problem with Dirac Delta Constraint: Seeking Guidance,Multiple Integral Problem with Dirac Delta Constraint: Seeking Guidance,,"I am working on a challenging multiple integral problem and would appreciate any assistance. The integral is as follows: $$ \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} \ldots \int_{-\infty}^{+\infty} \log(\sqrt{x_1^2+y_1^2}) \cdot \log(\sqrt{x_2^2+y_2^2}) \cdot \ldots \cdot \log(\sqrt{x_m^2+y_m^2}) \cdot \delta\left(\sum_{i=1}^{N} (x_i^2+y_i^2)-1\right) \, dx_1 \, dy_1 \, dx_2 \, dy_2 \ldots dx_N \, dy_N $$ where $ m $ is less than $ N $ , and $ N $ is a large natural number. The Dirac delta function ( $ \delta $ ) imposes a constraint on the problem. I have seen some of your insightful answers on related topics, @achille hui, and I was wondering if you could offer any guidance or suggestions on this problem. Your expertise would be greatly appreciated. Thank you,","I am working on a challenging multiple integral problem and would appreciate any assistance. The integral is as follows: where is less than , and is a large natural number. The Dirac delta function ( ) imposes a constraint on the problem. I have seen some of your insightful answers on related topics, @achille hui, and I was wondering if you could offer any guidance or suggestions on this problem. Your expertise would be greatly appreciated. Thank you,","
\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} \ldots \int_{-\infty}^{+\infty} \log(\sqrt{x_1^2+y_1^2}) \cdot \log(\sqrt{x_2^2+y_2^2}) \cdot \ldots \cdot \log(\sqrt{x_m^2+y_m^2}) \cdot \delta\left(\sum_{i=1}^{N} (x_i^2+y_i^2)-1\right) \, dx_1 \, dy_1 \, dx_2 \, dy_2 \ldots dx_N \, dy_N
  m   N   N   \delta ","['real-analysis', 'calculus', 'integration', 'multivariable-calculus']"
53,'Integrating' a matrix times a gradient,'Integrating' a matrix times a gradient,,"Suppose $f:\mathbb{R}^n\to \mathbb{R}$ is a differentiable function such that $\nabla f = g$ , where $g:\mathbb{R}^n\to \mathbb{R}^n$ . Further let $A\in \mathbb{R}^{n\times n}$ . Does there always exist a function $h:\mathbb{R}^n\to \mathbb{R}$ such that $\nabla h = Ag$ ? If so, can it be recovered from $f$ ? Feel free to make any assumptions on $A$ .","Suppose is a differentiable function such that , where . Further let . Does there always exist a function such that ? If so, can it be recovered from ? Feel free to make any assumptions on .",f:\mathbb{R}^n\to \mathbb{R} \nabla f = g g:\mathbb{R}^n\to \mathbb{R}^n A\in \mathbb{R}^{n\times n} h:\mathbb{R}^n\to \mathbb{R} \nabla h = Ag f A,['multivariable-calculus']
54,"Argue whether the generalized double integral $\int \int_D \frac{x^2-y^2}{x^2+y^2}dxdy$ where $D$ is $x\geq1 , y\geq1$ converge or diverge. [closed]",Argue whether the generalized double integral  where  is  converge or diverge. [closed],"\int \int_D \frac{x^2-y^2}{x^2+y^2}dxdy D x\geq1 , y\geq1","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 months ago . Improve this question In the first part of the problem, I proved that $$ \int_1^\infty (\int_1^\infty \frac{x^2-y^2}{(x^2+y^2)^2}dy)dx $$ and $$ \int_1^\infty (\int_1^\infty \frac{x^2-y^2}{(x^2+y^2)^2}dx)dy $$ converge, $\frac{-\pi}{4}$ and $\frac{\pi}{4}$ . The second part of the problem is whether this result is enough to show that $$\int \int_D \frac{x^2-y^2}{x^2+y^2}dxdy\ \text{where}\  D\  \text{is}\  x\geq1 , y\geq1$$ also converges. I am tempted to use the comparison criteria but the domain not being a circle confuses me as to what I should use as comparison.","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 months ago . Improve this question In the first part of the problem, I proved that and converge, and . The second part of the problem is whether this result is enough to show that also converges. I am tempted to use the comparison criteria but the domain not being a circle confuses me as to what I should use as comparison."," \int_1^\infty (\int_1^\infty \frac{x^2-y^2}{(x^2+y^2)^2}dy)dx   \int_1^\infty (\int_1^\infty \frac{x^2-y^2}{(x^2+y^2)^2}dx)dy  \frac{-\pi}{4} \frac{\pi}{4} \int \int_D \frac{x^2-y^2}{x^2+y^2}dxdy\ \text{where}\  D\  \text{is}\  x\geq1 , y\geq1",['multivariable-calculus']
55,A problem using Chain Rule theorem,A problem using Chain Rule theorem,,"Let $F(u,v)$ be of class $C^1(\mathbb{R}^2)$ and $z=z(x,y)$ be the function defined by the equation $$F\left(x+\dfrac{z}{y},y+\dfrac{z}{x}\right)=0.$$ Prove that the function $z(x,y)$ satisfies the following equation $$x\dfrac{\partial  z}{\partial x}+y\dfrac{\partial z}{\partial y}=z-xy.$$ My attempt: Let $u=x+\dfrac{z}{y}, v=y+\dfrac{z}{x}$ . Then $F(u,v)=0$ Using the Chain rule, I have $$\dfrac{\partial F}{\partial u}.\dfrac{\partial u}{\partial x}+\dfrac{\partial F}{\partial v}.\dfrac{\partial v}{\partial x}=0 \quad (1)$$ Since $\dfrac{\partial u}{\partial x}=1+\dfrac{1}{y}.\dfrac{\partial z}{\partial x}, \dfrac{\partial v}{\partial x}=\dfrac{\partial z}{\partial x}.\dfrac{1}{x}-\dfrac{z}{x^2}$ , $(1)$ becomes $$ \dfrac{\partial F}{\partial u}\dfrac{\partial z}{\partial x}\dfrac{1}{y}+\dfrac{\partial F}{\partial v}\dfrac{\partial z}{\partial x}\dfrac{1}{x}+\dfrac{\partial F}{\partial u}+\dfrac{\partial F}{\partial v}\dfrac{-z}{x^2}=0$$ Similarly $$\dfrac{\partial F}{\partial u}\dfrac{\partial z}{\partial y}\dfrac{1}{y}+\dfrac{\partial F}{\partial v}\dfrac{\partial z}{\partial y}\dfrac{1}{x}+\dfrac{\partial F}{\partial v}+\dfrac{\partial F}{\partial u}\dfrac{-z}{y^2}=0 $$ So we have the following equations: $$\begin{cases} \dfrac{\partial F}{\partial u}\left(\dfrac{\partial z}{\partial y}\dfrac{1}{y}-\dfrac{z}{y^2}\right)=-\dfrac{\partial F}{\partial v}\left(\dfrac{\partial z}{\partial y}\dfrac{1}{x}+1\right) \quad (2)\\\\ \dfrac{\partial F}{\partial u}\left(\dfrac{\partial z}{\partial x}\dfrac{1}{y}+1\right)=-\dfrac{\partial F}{\partial v}\left(\dfrac{\partial z}{\partial x}\dfrac{1}{x}-\dfrac{z}{x^2}\right)\quad (3).\end{cases}$$ If we divide (2) with (3) side by side, we have the equality we wanna proof. But the problem is what if one of the terms is $0$ ? How can I deal with this case? Could someone help me or have other way to crack this problem? Thanks in advance!","Let be of class and be the function defined by the equation Prove that the function satisfies the following equation My attempt: Let . Then Using the Chain rule, I have Since , becomes Similarly So we have the following equations: If we divide (2) with (3) side by side, we have the equality we wanna proof. But the problem is what if one of the terms is ? How can I deal with this case? Could someone help me or have other way to crack this problem? Thanks in advance!","F(u,v) C^1(\mathbb{R}^2) z=z(x,y) F\left(x+\dfrac{z}{y},y+\dfrac{z}{x}\right)=0. z(x,y) x\dfrac{\partial
 z}{\partial x}+y\dfrac{\partial z}{\partial y}=z-xy. u=x+\dfrac{z}{y}, v=y+\dfrac{z}{x} F(u,v)=0 \dfrac{\partial F}{\partial u}.\dfrac{\partial u}{\partial x}+\dfrac{\partial F}{\partial v}.\dfrac{\partial v}{\partial x}=0 \quad (1) \dfrac{\partial u}{\partial x}=1+\dfrac{1}{y}.\dfrac{\partial z}{\partial x}, \dfrac{\partial v}{\partial x}=\dfrac{\partial z}{\partial x}.\dfrac{1}{x}-\dfrac{z}{x^2} (1)  \dfrac{\partial F}{\partial u}\dfrac{\partial z}{\partial x}\dfrac{1}{y}+\dfrac{\partial F}{\partial v}\dfrac{\partial z}{\partial x}\dfrac{1}{x}+\dfrac{\partial F}{\partial u}+\dfrac{\partial F}{\partial v}\dfrac{-z}{x^2}=0 \dfrac{\partial F}{\partial u}\dfrac{\partial z}{\partial y}\dfrac{1}{y}+\dfrac{\partial F}{\partial v}\dfrac{\partial z}{\partial y}\dfrac{1}{x}+\dfrac{\partial F}{\partial v}+\dfrac{\partial F}{\partial u}\dfrac{-z}{y^2}=0  \begin{cases} \dfrac{\partial F}{\partial u}\left(\dfrac{\partial z}{\partial y}\dfrac{1}{y}-\dfrac{z}{y^2}\right)=-\dfrac{\partial F}{\partial v}\left(\dfrac{\partial z}{\partial y}\dfrac{1}{x}+1\right) \quad (2)\\\\ \dfrac{\partial F}{\partial u}\left(\dfrac{\partial z}{\partial x}\dfrac{1}{y}+1\right)=-\dfrac{\partial F}{\partial v}\left(\dfrac{\partial z}{\partial x}\dfrac{1}{x}-\dfrac{z}{x^2}\right)\quad (3).\end{cases} 0","['real-analysis', 'calculus']"
56,"Prove that that the function $f:\mathbb R^3\to \mathbb R$ defined by $f(x,y,z)=ye^x+xz^2$ is differentiable at the point (0,-4,2) using definition.","Prove that that the function  defined by  is differentiable at the point (0,-4,2) using definition.","f:\mathbb R^3\to \mathbb R f(x,y,z)=ye^x+xz^2","Prove that that the function $f:\mathbb R^3\to \mathbb R$ defined by $f(x,y,z)=ye^x+xz^2$ is differentiable at the point $\vec a=\langle 0,-4,2\rangle$ using definition. My attempt:- Let $\vec{h}=\langle h_1,h_2,h_3\rangle.$ Then we need to prove $$\lim_{\vec{h}\to \vec 0}\frac{f(\vec a+\vec{h})-f(\vec a)-f_x(\vec a)h_1-f_y(\vec a)h_2-f_z(\vec a)h_3}{||\vec h||}=0.$$ Consider the LHS $$\lim_{\vec{h}\to \vec 0}\frac{f(\vec a+\vec{h})-f(\vec a)-f_x(\vec a)h_1-f_y(\vec a)h_2-f_z(\vec a)h_3}{||\vec h||}=\\\lim_{\vec{h}\to \vec 0}\frac{(-4+h_2)e^{h_1}+h_1(2+h_3)^2+4-h_2}{\sqrt{h_1^2+h_2^2+h_3^2}}$$ I can use $(-4+h_2)\leq h_2$ . I don't know how to proceed after that. I used spherical coordinated. I used all the possibilities available for me. Could you help me?",Prove that that the function defined by is differentiable at the point using definition. My attempt:- Let Then we need to prove Consider the LHS I can use . I don't know how to proceed after that. I used spherical coordinated. I used all the possibilities available for me. Could you help me?,"f:\mathbb R^3\to \mathbb R f(x,y,z)=ye^x+xz^2 \vec a=\langle 0,-4,2\rangle \vec{h}=\langle h_1,h_2,h_3\rangle. \lim_{\vec{h}\to \vec 0}\frac{f(\vec a+\vec{h})-f(\vec a)-f_x(\vec a)h_1-f_y(\vec a)h_2-f_z(\vec a)h_3}{||\vec h||}=0. \lim_{\vec{h}\to \vec 0}\frac{f(\vec a+\vec{h})-f(\vec a)-f_x(\vec a)h_1-f_y(\vec a)h_2-f_z(\vec a)h_3}{||\vec h||}=\\\lim_{\vec{h}\to \vec 0}\frac{(-4+h_2)e^{h_1}+h_1(2+h_3)^2+4-h_2}{\sqrt{h_1^2+h_2^2+h_3^2}} (-4+h_2)\leq h_2","['multivariable-calculus', 'derivatives']"
57,Image of a two-parameter function over a set,Image of a two-parameter function over a set,,"I have the following problem: Let $B$ be the set $B=\{(x_1 , x_2) \in \mathbb{R}^2\mid x_1^2 + x_2^2 \leq 1\text{ and }x_1 \geq 0\}$ . A function $f:B \rightarrow \mathbb{R}$ is given by $$ f(x_1 ,x_2 )=x_1^2 x_2^2 +3x_1 x_2 +x_2 -4.$$ State the image of $f$ . To find the image, I want to do an extrema investigation. First, I find all the stationary points. There is one at $\left(-\frac13, 0\right)$ , but based on the Hessian matrix, I conclude it is a saddle point. Next, I want to do a boundary investigation. I parametrize the circular part of the boundary of the set as $$s(u) = (\cos u,\sin u),\quad u \in\left[-\frac\pi2,\frac\pi2\right].$$ However, this is where my problem starts. I make a composite function $f \circ s$ - this describes the values of $f$ along the circular boundary. I then differentiate this composite function and get $$- 2 \sin^3u\,\cos u- 3 \sin^2u+ 2 \sin u\,\cos^3u+ 3 \cos^2u+ \cos u.$$ Now, I want to set it equal to $0$ and solve, since the stationary points on the composite function constitute possible extrema of $f$ on $B$ . However, I am not able to solve this. It there any trick or another way to find these extrema?","I have the following problem: Let be the set . A function is given by State the image of . To find the image, I want to do an extrema investigation. First, I find all the stationary points. There is one at , but based on the Hessian matrix, I conclude it is a saddle point. Next, I want to do a boundary investigation. I parametrize the circular part of the boundary of the set as However, this is where my problem starts. I make a composite function - this describes the values of along the circular boundary. I then differentiate this composite function and get Now, I want to set it equal to and solve, since the stationary points on the composite function constitute possible extrema of on . However, I am not able to solve this. It there any trick or another way to find these extrema?","B B=\{(x_1 , x_2) \in \mathbb{R}^2\mid x_1^2 + x_2^2 \leq 1\text{ and }x_1 \geq 0\} f:B \rightarrow \mathbb{R} 
f(x_1 ,x_2 )=x_1^2 x_2^2 +3x_1 x_2 +x_2 -4. f \left(-\frac13, 0\right) s(u) = (\cos u,\sin u),\quad u \in\left[-\frac\pi2,\frac\pi2\right]. f \circ s f - 2 \sin^3u\,\cos u- 3 \sin^2u+ 2 \sin u\,\cos^3u+ 3 \cos^2u+ \cos u. 0 f B","['calculus', 'linear-algebra', 'multivariable-calculus', 'convex-optimization']"
58,"Prove the limit $\lim_{(x,y)\to(0,0)} x^y (x>0)$ doesn't exist",Prove the limit  doesn't exist,"\lim_{(x,y)\to(0,0)} x^y (x>0)","Given the function $f(x,y)= x^y (x>0)$ Prove that the limit $\lim_{(x,y)\to(0,0)} f(x,y)$ does not exist I think I would choose 2 sequences $(x,y) \to (0,0)$ such that the two limits do not agree, concluding that the limit doesn't exist. But I just found a sequence $\left(\left(\frac{1}{n},0\right)\right)_{n\geq1}$ and found that $\lim_\limits{n\to\infty} f\left(\frac{1}n, 0\right) =1$ , but I don't know how to choose the second sequence. Can you explain or any way to prove it","Given the function Prove that the limit does not exist I think I would choose 2 sequences such that the two limits do not agree, concluding that the limit doesn't exist. But I just found a sequence and found that , but I don't know how to choose the second sequence. Can you explain or any way to prove it","f(x,y)= x^y (x>0) \lim_{(x,y)\to(0,0)} f(x,y) (x,y) \to (0,0) \left(\left(\frac{1}{n},0\right)\right)_{n\geq1} \lim_\limits{n\to\infty} f\left(\frac{1}n, 0\right) =1","['calculus', 'limits', 'multivariable-calculus']"
59,Unclear step in the proof of Inverse Function Theorem,Unclear step in the proof of Inverse Function Theorem,,"There is one step the necessity of which I don’t understand in the proof of Inverse Function Theorem in Jerry Shurman’s $Calculus$ $and$ $Analysis$ $in$ $Euclidean$ $Space$ (available here https://www.professores.uff.br/diomarcesarlobao/wp-content/uploads/sites/85/2017/09/Multi_calculus_BOOK.pdf ). After having established injectivity of $f$ in some closed ball $\overline B$ around $a$ and introducing an open ball $W$ around $f(a)$ s.t. $|f(a) - y| < |f(x) - y|$ for every $y\in W$ and every $x\in \partial \overline B$ , the author goes on to prove, using Critical Point Theorem and Chain Rule, that for every $y\in W$ there is one and only one $x\in int\overline B$ s.t. $f(x) = y$ . Why do we need to do this, or, more concretely, why does the proof of this fact need to be so sophisticated? Why can’t we just take $V$ to be the pre-image of $W$ , i.e. $V = f^{-1}(W)$ (every point of $W$ has a pre-image in $\overline B$ , because $W$ is a subset of $f(\overline B)$ ; moreover, this pre-image should be unique due to injectivity of $f$ and not lie on the boundary of $\overline B$ by construction), and then $f$ becomes a bijective, and thus invertible map from V to W?","There is one step the necessity of which I don’t understand in the proof of Inverse Function Theorem in Jerry Shurman’s (available here https://www.professores.uff.br/diomarcesarlobao/wp-content/uploads/sites/85/2017/09/Multi_calculus_BOOK.pdf ). After having established injectivity of in some closed ball around and introducing an open ball around s.t. for every and every , the author goes on to prove, using Critical Point Theorem and Chain Rule, that for every there is one and only one s.t. . Why do we need to do this, or, more concretely, why does the proof of this fact need to be so sophisticated? Why can’t we just take to be the pre-image of , i.e. (every point of has a pre-image in , because is a subset of ; moreover, this pre-image should be unique due to injectivity of and not lie on the boundary of by construction), and then becomes a bijective, and thus invertible map from V to W?",Calculus and Analysis in Euclidean Space f \overline B a W f(a) |f(a) - y| < |f(x) - y| y\in W x\in \partial \overline B y\in W x\in int\overline B f(x) = y V W V = f^{-1}(W) W \overline B W f(\overline B) f \overline B f,"['calculus', 'analysis', 'multivariable-calculus', 'inverse-function', 'theorem-provers']"
60,Limits of triple integral over a tetrahedron.,Limits of triple integral over a tetrahedron.,,"The tetrahedron has vertices $O(0,0,0); A(0,0,2); B(0,2,0); C(1,0,0)$ I was thinking that the plane $ABC$ has equation $2x+y+z=2$ since: $\vec{BA}= \langle 0,-2,2 \rangle$ and $\vec{CA}= \langle -1,0,2 \rangle$ with a cross product $\langle 4,2,2 \rangle$ hence $4x+2y+2z=4 \cdot (1) + 2 \cdot (0) +2 \cdot (0) =4 $ So $$2x+y+z=2 \tag{1}$$ The question asked to follow the order $dz\,dy\,dx$ so I got: $$I=\int^1_0\int^{2-2x}_0\int^{2-2x-y}_0 \; dz\,dy\,dx \tag{2}$$ Is there anything wrong with my work? I'm particularly skeptical of the $z$ limit. Also, Is there a more straightforward way of doing it? **edited for $dy$ limits.","The tetrahedron has vertices I was thinking that the plane has equation since: and with a cross product hence So The question asked to follow the order so I got: Is there anything wrong with my work? I'm particularly skeptical of the limit. Also, Is there a more straightforward way of doing it? **edited for limits.","O(0,0,0); A(0,0,2); B(0,2,0); C(1,0,0) ABC 2x+y+z=2 \vec{BA}= \langle 0,-2,2 \rangle \vec{CA}= \langle -1,0,2 \rangle \langle 4,2,2 \rangle 4x+2y+2z=4 \cdot (1) + 2 \cdot (0) +2 \cdot (0) =4  2x+y+z=2 \tag{1} dz\,dy\,dx I=\int^1_0\int^{2-2x}_0\int^{2-2x-y}_0 \; dz\,dy\,dx \tag{2} z dy","['multivariable-calculus', 'solution-verification', 'vector-analysis', 'volume', 'iterated-integrals']"
61,The Jacobian of $g(\vec{x}) = f(A\vec{x} + \vec{b})\vec{x}$.,The Jacobian of .,g(\vec{x}) = f(A\vec{x} + \vec{b})\vec{x},"Let $A = \mathbb{R}^{n \times n}$ and $f: \mathbb{R^{n}} \mapsto \mathbb{R}$ I can compute Jacobians of simple functions, but this question obliterated me, and I have spent days trying to understand it. Within the solution they derive that $[D(\vec{g}(\vec{x}))]_{jk} = f(\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})\frac{\partial \vec{x}_j}{\partial x_k} + \vec{x}_j \frac{\partial f(\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})}{\partial x_k}$ This is fine as it is just chain rule, but where they lose me is when they change to summation: $f(\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})\frac{\partial \vec{x}_j}{\partial x_k} + \vec{x}_j \sum_{\ell=1}^{n} \frac{\partial f(\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})}{\partial (\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})_{\ell}} \cdot \frac{\partial (\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})_{\ell}}{\partial x_k}$ I've tried coming up with a simple example using the 1-norm of an A $\mathbb{R}^{2 \times 2}$ , and the accompanying x and b vectors, but it doesn't help because it is too specific compared to how general this solution is. If anyone can explain the change to summation, I'd be greatly appreciative.","Let and I can compute Jacobians of simple functions, but this question obliterated me, and I have spent days trying to understand it. Within the solution they derive that This is fine as it is just chain rule, but where they lose me is when they change to summation: I've tried coming up with a simple example using the 1-norm of an A , and the accompanying x and b vectors, but it doesn't help because it is too specific compared to how general this solution is. If anyone can explain the change to summation, I'd be greatly appreciative.",A = \mathbb{R}^{n \times n} f: \mathbb{R^{n}} \mapsto \mathbb{R} [D(\vec{g}(\vec{x}))]_{jk} = f(\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})\frac{\partial \vec{x}_j}{\partial x_k} + \vec{x}_j \frac{\partial f(\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})}{\partial x_k} f(\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})\frac{\partial \vec{x}_j}{\partial x_k} + \vec{x}_j \sum_{\ell=1}^{n} \frac{\partial f(\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})}{\partial (\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})_{\ell}} \cdot \frac{\partial (\mathbf{A}\vec{\mathbf{x}} + \mathbf{b})_{\ell}}{\partial x_k} \mathbb{R}^{2 \times 2},"['linear-algebra', 'multivariable-calculus', 'optimization', 'jacobian']"
62,Finding the ideal B-spline through data points using Euler-Lagrange: is it just too hard to do?,Finding the ideal B-spline through data points using Euler-Lagrange: is it just too hard to do?,,"I am not even sure I have a question anymore (I will just give up)... in the past month or so I have been researching cubic Bézier curves. The idea was to find a fit through data points, using piecewise Bézier curves and the Euler-Lagrange equation to minimize an ""energy"" proportional with the square of the length of the (total) curve (elastic energy used to stretch) plus the integral of the square of the curvature $\kappa$ of the curve (energy required to bend the curve). Edit: it took me three whole days to type this ""question"". Please don't get mad for wasting your time if you choose to read it. Here is what I have done so far with respect to the length of the curve. Let $$\begin{equation}\begin{aligned}\vec{P}(t) &= (1-t)^3 \vec{P_0} + 3(1-t)^2t \vec{C_0} + 3(1-t)t^2 \vec{C_1} + t^3 \vec{P_1} \\ &= \vec{P_0} + 3(\vec{C_0} - \vec{P_0})t + 3(\vec{C_1} - 2\vec{C_0} + \vec{P_0})t² + (\vec{P_1} - 3\vec{C_1} + 3\vec{C_0} - \vec{P_0})t³\end{aligned} \end{equation}\tag{1}$$ be a cubic Bézier curve defined by its begin and end points $\vec{P_0}$ and $\vec{P_1}$ , and control points $\vec{C_0}$ and $\vec{C_1}$ . Instead of $\{\vec{P_0}, \vec{C_0}, \vec{C_1}, \vec{P_1}\}$ to define the curve, lets use $\{\vec{B}, \vec{V}, \vec{A}, \vec{J}\}$ where $$\begin{align}\vec{B}&=\vec{P_0} \\ \vec{V}&=3(\vec{C_0} - \vec{P_0}) \\ \vec{A}&=6((\vec{C_1} - \vec{P_0}) - 2(\vec{C_0} - \vec{P_0})) \\ \vec{J}&=6((\vec{P_1} - \vec{P_0}) - 3(\vec{C_1} - \vec{C_0}))\end{align} \tag{2}$$ This is not random, you will thank me later; the idea behind it is that the coefficients other than the constant term represent the velocity, acceleration and jerk (if $t$ was ""time"") with respect to $t$ , which only depends on the shape of the curve, not its position. Therefore those coefficients should be expressed in terms of differences between any two of $\{\vec{P_0}, \vec{C_0}, \vec{C_1}, \vec{P_1}\}$ . Substituting this into $(1)$ gives, $$\vec{P}(t) = \vec{B} + \vec{V}t + \frac{1}{2}\vec{A}t^2 + \frac{1}{6}\vec{J}t^3 \tag{3}$$ Then the chord length of the curve between $\vec{P_0}$ and $\vec{P_1}$ is given by $$chord\_length = \int_0^1 \lVert \vec{P'}(t) \rVert \,dt \tag{4}$$ where $\vec{P'}(t)$ is the derivative of $\vec{P}(t)$ with respect to $t$ , also called the velocity : $$\vec{P'}(t) = \vec{V} + \vec{A}t + \frac{1}{2}\vec{J}t^2 \tag{5}$$ In order to calculate the norm $\lVert \vec{P'}(t) \rVert$ we can take the square root of the dot product with itself: $$\lVert \vec{P'}(t) \rVert = \sqrt{\vec{P'}(t) \cdot \vec{P'}(t)} \tag{6}$$ Working out this dot product we get a fourth degree polynomial in $t$ : $$\vec{P'}(t) \cdot \vec{P'}(t) = c_4 t^4 + c_3 t^3 + c_2 t^2 + c_1 t + c_0 \tag{7}$$ where $$\begin{aligned}c_0 &= \lVert \vec{V} \rVert^2 \\ c_1 &= 2 (\vec{V} \cdot \vec{A}) \\ c_2 &= \vec{V} \cdot \vec{J} + \lVert \vec{A} \rVert^2 \\ c_3 &= \vec{A} \cdot \vec{J} \\ c_4 &= \frac{1}{4}\lVert \vec{J} \rVert^2\end{aligned} \tag{8}$$ Note how all of this holds for vectors of dimension two or more . We're not talking about $x$ and $y$ or whatever. We might also be interested in a polynomial in $u$ after a substitution $t = u + s$ , where we'll integrate from $u=-s$ to $u=-s+1$ . After substitution we end up with: $$\begin{align}chord\_length &= \int_{-s}^{-s+1} \sqrt{c_4' u^4 + c_3' u^3 + c_2' u^2 + c_1' u + c_0'} \space \,du \\ &= \sqrt{c_0'} \int_{-s}^{-s+1} \sqrt{\frac{c_4'}{c_0'}u^4 + \frac{c_3'}{c_0'}u^3 + \frac{c_2'}{c_0'} u^2 + \frac{c_1'}{c_0'} + 1}\space\space \,du \end{align} \tag{9}$$ where $$\begin{aligned}c_0' &= c_0 + c_1 s + c_2 s^2 + c_3 s^3 + c_4 s^4 \\ c_1' &= c_1 + 2 c_2 s + 3 c_3 s^2 + 4 c_4 s^3 \\ c_2' &= c_2 + 3 c_3 s + 6 c_4 s^2 \\ c_3' &= c_3 + 4 c_4 s \\ c_4' &= c_4\end{aligned} \tag{10}$$ Let $$\begin{aligned}d_1 &= \frac{c_1'}{c_0'} \\ d_2 &= \frac{c_2'}{c_0'} \\ d_3 &= \frac{c_3'}{c_0'} \\ d_4 &= \frac{c_4'}{c_0'}\end{aligned} \tag{11}$$ So that the problem of finding the chord length becomes integration of $\sqrt{1 + d_1 u + d_2 u^2 + d_3 u^3 + d_4 u^4}$ . Since there is no known algebraic expression for that, we can proceed by finding the Taylor series of this square root around $u=0$ ( $t=s$ ). Lets write $$\sqrt{1 + d_1 u + d_2 u^2 + d_3 u^3 + d_4 u^4} = 1 + \sum_{n=1}^{\infty} \frac{1}{n}g_n u^n \tag{12}$$ then the following recursive relationship for the $g_n$ can be found (with three days of hard work): $$\begin{aligned} g_1 &= +\frac{1}{2}d_1 \\ g_2 &= -\frac{1}{2}d_1 g_1 + \frac{1}{1}d_2 \\ g_3 &= -\frac{3}{4}d_1 g_2 - \frac{0}{1}d_2 g_1 + \frac{3}{2}d_3 \\ g_4 &= -\frac{5}{6}d_1 g_3 - \frac{1}{2}d_2 g_2 + \frac{1}{2}d_3 g_1 + \frac{2}{1}d_4 \\ g_5 &= -\frac{7}{8}d_1 g_4 - \frac{2}{3}d_2 g_3 - \frac{1}{4}d_3 g_2 + \frac{1}{1}d_4 g_1 \\  &\space\space\vdots \notag \\ g_n &= -\frac{2n-3}{2n-2}d_1 g_{n-1} - \frac{n-3}{n-2}d_2 g_{n-2} - \frac{2n-9}{2n-6}d_3 g_{n-3} - \frac{n-6}{n-4}d_4 g_{n-4}\end{aligned} \tag{13}$$ This Taylor series should converge if $u$ is close enough to zero. In fact, it turns out that for tame curves where, say the deviation from a straight line is less than a third of the distance between $\vec{P_0}$ and $\vec{P_1}$ , one can calculate the chord length with a single integral taking $s=1/2$ and integrating from $-1/2$ to $+1/2$ . Otherwise we might have to do it piecewise and add up a few chord length pieces. For example one could pick six pieces where $s$ is one of $\{\frac{1}{12}, \frac{3}{12}, \frac{5}{12}, \frac{7}{12}, \frac{9}{12}, \frac{11}{12} \}$ and then integrate (six times) from $-\frac{1}{12}$ to $+\frac{1}{12}$ . Thus, in general we are interested in obtaining the integral between $-\delta$ and $+\delta$ where $\delta$ is some fixed, positive real less than or equal $1/2$ . Combining equation $(9)$ through $(13)$ we can write, $$chord\_length\_piece = \sqrt{c_0'} \int_{-\delta}^{\delta}\left(1+\sum_{n=1}^{\infty}\frac{1}{n}g_n u^n\right)\,du \tag{14}$$ Assuming $\delta$ is small enough that the sum converges, we can swap the sum and integration and get $$chord\_length\_piece = 2\delta\sqrt{c_0'} \left( 1 + \sum_{even\ n\geq 2}^{\infty} \frac{\delta^n}{n(n+1)}g_n \right) \tag{15}$$ Note that only the $g_n$ for even $n$ are left because the odd ones cancel. For example $\int_{-\delta}^{\delta} g_1 u\,du = \left[\frac{1}{2}g_1u^2\right]_{-\delta}^{\delta} = \frac{1}{2}g_1\delta^2 - \frac{1}{2}g_1(-\delta)^2 = 0$ . The other terms are doubled, hence the leading 2. I also moved one factor of $\delta$ to the front, making all exponents of $\delta$ equal to the index of $g_n$ again. For a straight line, the sum is zero and $\vec{c_0'} = \lVert \vec{V_s} \rVert^2$ the square of the velocity at $t=s$ . The total chord length then can be found by summing $(15)$ for all values of $s$ involved. Let $\delta = \frac{1}{2N}$ for some positive integer $N$ , the number of pieces we will cut the curve into. Then we need $N$ values of $s$ : $$s = (2k + 1)\delta \qquad\text{ for } k = \mathbin{{0}{...}{N}{-}{1}}$$ and $$\begin{aligned}chord\_length &= 2 \delta \sum_{k=0}^{N-1} \sqrt{c_0'^{(s)}} \left( 1 + \sum_{even\ n\geq 2}^{\infty} \frac{\delta^n}{n(n+1)}g_n^{(s)} \right) \\ &= 2 \delta \sum_{k=0}^{N-1} \sqrt{c_0'^{(s)}}  + 2 \delta \sum_{even\ n\geq 2}^{\infty} \frac{\delta^n}{n(n+1)} \sum_{k=0}^{N-1} g_n^{(s)} \end{aligned} \tag{16}$$ Recall that the $g_n$ are a function of the $d_i$ which in turn are a function of the $c_i'$ which are polynomials in $s$ (hence the notation $g_n^{(s)})$ with coefficients $c_i$ , where $s = (2k + 1)\delta$ . Since $\delta$ is a known, fixed constant, the second term of equation $(16)$ is a multivariate polynomial in the $c_i$ . The first term contains the $\sqrt{c_0'^{(s)}}$ , which will carry over to a lot of other terms after squaring the length; but is still straightforward to differentiate. Note that $c_0'$ is the square of the velocity at $t=s$ , so that if we make $N$ arbitrary large (causing $\delta$ to go to zero) the first sum turns into the original integral (equation $(4)$ ). At this point I feel like giving up though: assuming I can come up with a differentiable equation for the bending energy part, I am starting to think this is going to be a dead-end. In order to calculate the ideal curve I'd have to minimize the energy of the whole curve, all Bézier curves combined, where one curve influences the next only due to the demand that the curve must have $G_1$ continuity. Am I right to give up, or does anyone have an idea that can be done in a reasonable amount of time?","I am not even sure I have a question anymore (I will just give up)... in the past month or so I have been researching cubic Bézier curves. The idea was to find a fit through data points, using piecewise Bézier curves and the Euler-Lagrange equation to minimize an ""energy"" proportional with the square of the length of the (total) curve (elastic energy used to stretch) plus the integral of the square of the curvature of the curve (energy required to bend the curve). Edit: it took me three whole days to type this ""question"". Please don't get mad for wasting your time if you choose to read it. Here is what I have done so far with respect to the length of the curve. Let be a cubic Bézier curve defined by its begin and end points and , and control points and . Instead of to define the curve, lets use where This is not random, you will thank me later; the idea behind it is that the coefficients other than the constant term represent the velocity, acceleration and jerk (if was ""time"") with respect to , which only depends on the shape of the curve, not its position. Therefore those coefficients should be expressed in terms of differences between any two of . Substituting this into gives, Then the chord length of the curve between and is given by where is the derivative of with respect to , also called the velocity : In order to calculate the norm we can take the square root of the dot product with itself: Working out this dot product we get a fourth degree polynomial in : where Note how all of this holds for vectors of dimension two or more . We're not talking about and or whatever. We might also be interested in a polynomial in after a substitution , where we'll integrate from to . After substitution we end up with: where Let So that the problem of finding the chord length becomes integration of . Since there is no known algebraic expression for that, we can proceed by finding the Taylor series of this square root around ( ). Lets write then the following recursive relationship for the can be found (with three days of hard work): This Taylor series should converge if is close enough to zero. In fact, it turns out that for tame curves where, say the deviation from a straight line is less than a third of the distance between and , one can calculate the chord length with a single integral taking and integrating from to . Otherwise we might have to do it piecewise and add up a few chord length pieces. For example one could pick six pieces where is one of and then integrate (six times) from to . Thus, in general we are interested in obtaining the integral between and where is some fixed, positive real less than or equal . Combining equation through we can write, Assuming is small enough that the sum converges, we can swap the sum and integration and get Note that only the for even are left because the odd ones cancel. For example . The other terms are doubled, hence the leading 2. I also moved one factor of to the front, making all exponents of equal to the index of again. For a straight line, the sum is zero and the square of the velocity at . The total chord length then can be found by summing for all values of involved. Let for some positive integer , the number of pieces we will cut the curve into. Then we need values of : and Recall that the are a function of the which in turn are a function of the which are polynomials in (hence the notation with coefficients , where . Since is a known, fixed constant, the second term of equation is a multivariate polynomial in the . The first term contains the , which will carry over to a lot of other terms after squaring the length; but is still straightforward to differentiate. Note that is the square of the velocity at , so that if we make arbitrary large (causing to go to zero) the first sum turns into the original integral (equation ). At this point I feel like giving up though: assuming I can come up with a differentiable equation for the bending energy part, I am starting to think this is going to be a dead-end. In order to calculate the ideal curve I'd have to minimize the energy of the whole curve, all Bézier curves combined, where one curve influences the next only due to the demand that the curve must have continuity. Am I right to give up, or does anyone have an idea that can be done in a reasonable amount of time?","\kappa \begin{equation}\begin{aligned}\vec{P}(t) &= (1-t)^3 \vec{P_0} + 3(1-t)^2t \vec{C_0} + 3(1-t)t^2 \vec{C_1} + t^3 \vec{P_1} \\
&= \vec{P_0} + 3(\vec{C_0} - \vec{P_0})t + 3(\vec{C_1} - 2\vec{C_0} + \vec{P_0})t² + (\vec{P_1} - 3\vec{C_1} + 3\vec{C_0} - \vec{P_0})t³\end{aligned} \end{equation}\tag{1} \vec{P_0} \vec{P_1} \vec{C_0} \vec{C_1} \{\vec{P_0}, \vec{C_0}, \vec{C_1}, \vec{P_1}\} \{\vec{B}, \vec{V}, \vec{A}, \vec{J}\} \begin{align}\vec{B}&=\vec{P_0} \\
\vec{V}&=3(\vec{C_0} - \vec{P_0}) \\
\vec{A}&=6((\vec{C_1} - \vec{P_0}) - 2(\vec{C_0} - \vec{P_0})) \\
\vec{J}&=6((\vec{P_1} - \vec{P_0}) - 3(\vec{C_1} - \vec{C_0}))\end{align} \tag{2} t t \{\vec{P_0}, \vec{C_0}, \vec{C_1}, \vec{P_1}\} (1) \vec{P}(t) = \vec{B} + \vec{V}t + \frac{1}{2}\vec{A}t^2 + \frac{1}{6}\vec{J}t^3 \tag{3} \vec{P_0} \vec{P_1} chord\_length = \int_0^1 \lVert \vec{P'}(t) \rVert \,dt \tag{4} \vec{P'}(t) \vec{P}(t) t \vec{P'}(t) = \vec{V} + \vec{A}t + \frac{1}{2}\vec{J}t^2 \tag{5} \lVert \vec{P'}(t) \rVert \lVert \vec{P'}(t) \rVert = \sqrt{\vec{P'}(t) \cdot \vec{P'}(t)} \tag{6} t \vec{P'}(t) \cdot \vec{P'}(t) = c_4 t^4 + c_3 t^3 + c_2 t^2 + c_1 t + c_0 \tag{7} \begin{aligned}c_0 &= \lVert \vec{V} \rVert^2 \\
c_1 &= 2 (\vec{V} \cdot \vec{A}) \\
c_2 &= \vec{V} \cdot \vec{J} + \lVert \vec{A} \rVert^2 \\
c_3 &= \vec{A} \cdot \vec{J} \\
c_4 &= \frac{1}{4}\lVert \vec{J} \rVert^2\end{aligned} \tag{8} x y u t = u + s u=-s u=-s+1 \begin{align}chord\_length &= \int_{-s}^{-s+1} \sqrt{c_4' u^4 + c_3' u^3 + c_2' u^2 + c_1' u + c_0'} \space \,du \\
&= \sqrt{c_0'} \int_{-s}^{-s+1} \sqrt{\frac{c_4'}{c_0'}u^4 + \frac{c_3'}{c_0'}u^3 + \frac{c_2'}{c_0'} u^2 + \frac{c_1'}{c_0'} + 1}\space\space \,du \end{align} \tag{9} \begin{aligned}c_0' &= c_0 + c_1 s + c_2 s^2 + c_3 s^3 + c_4 s^4 \\
c_1' &= c_1 + 2 c_2 s + 3 c_3 s^2 + 4 c_4 s^3 \\
c_2' &= c_2 + 3 c_3 s + 6 c_4 s^2 \\
c_3' &= c_3 + 4 c_4 s \\
c_4' &= c_4\end{aligned} \tag{10} \begin{aligned}d_1 &= \frac{c_1'}{c_0'} \\
d_2 &= \frac{c_2'}{c_0'} \\
d_3 &= \frac{c_3'}{c_0'} \\
d_4 &= \frac{c_4'}{c_0'}\end{aligned} \tag{11} \sqrt{1 + d_1 u + d_2 u^2 + d_3 u^3 + d_4 u^4} u=0 t=s \sqrt{1 + d_1 u + d_2 u^2 + d_3 u^3 + d_4 u^4} = 1 + \sum_{n=1}^{\infty} \frac{1}{n}g_n u^n \tag{12} g_n \begin{aligned}
g_1 &= +\frac{1}{2}d_1 \\
g_2 &= -\frac{1}{2}d_1 g_1 + \frac{1}{1}d_2 \\
g_3 &= -\frac{3}{4}d_1 g_2 - \frac{0}{1}d_2 g_1 + \frac{3}{2}d_3 \\
g_4 &= -\frac{5}{6}d_1 g_3 - \frac{1}{2}d_2 g_2 + \frac{1}{2}d_3 g_1 + \frac{2}{1}d_4 \\
g_5 &= -\frac{7}{8}d_1 g_4 - \frac{2}{3}d_2 g_3 - \frac{1}{4}d_3 g_2 + \frac{1}{1}d_4 g_1 \\ 
&\space\space\vdots \notag \\
g_n &= -\frac{2n-3}{2n-2}d_1 g_{n-1} - \frac{n-3}{n-2}d_2 g_{n-2} - \frac{2n-9}{2n-6}d_3 g_{n-3} - \frac{n-6}{n-4}d_4 g_{n-4}\end{aligned} \tag{13} u \vec{P_0} \vec{P_1} s=1/2 -1/2 +1/2 s \{\frac{1}{12}, \frac{3}{12}, \frac{5}{12}, \frac{7}{12}, \frac{9}{12}, \frac{11}{12} \} -\frac{1}{12} +\frac{1}{12} -\delta +\delta \delta 1/2 (9) (13) chord\_length\_piece = \sqrt{c_0'} \int_{-\delta}^{\delta}\left(1+\sum_{n=1}^{\infty}\frac{1}{n}g_n u^n\right)\,du \tag{14} \delta chord\_length\_piece = 2\delta\sqrt{c_0'} \left( 1 + \sum_{even\ n\geq 2}^{\infty} \frac{\delta^n}{n(n+1)}g_n \right) \tag{15} g_n n \int_{-\delta}^{\delta} g_1 u\,du = \left[\frac{1}{2}g_1u^2\right]_{-\delta}^{\delta} = \frac{1}{2}g_1\delta^2 - \frac{1}{2}g_1(-\delta)^2 = 0 \delta \delta g_n \vec{c_0'} = \lVert \vec{V_s} \rVert^2 t=s (15) s \delta = \frac{1}{2N} N N s s = (2k + 1)\delta \qquad\text{ for } k = \mathbin{{0}{...}{N}{-}{1}} \begin{aligned}chord\_length &= 2 \delta \sum_{k=0}^{N-1} \sqrt{c_0'^{(s)}} \left( 1 + \sum_{even\ n\geq 2}^{\infty} \frac{\delta^n}{n(n+1)}g_n^{(s)} \right) \\
&= 2 \delta \sum_{k=0}^{N-1} \sqrt{c_0'^{(s)}}
 + 2 \delta \sum_{even\ n\geq 2}^{\infty} \frac{\delta^n}{n(n+1)} \sum_{k=0}^{N-1} g_n^{(s)} \end{aligned} \tag{16} g_n d_i c_i' s g_n^{(s)}) c_i s = (2k + 1)\delta \delta (16) c_i \sqrt{c_0'^{(s)}} c_0' t=s N \delta (4) G_1","['multivariable-calculus', 'euler-lagrange-equation', 'arc-length', 'bezier-curve']"
63,Differentiating a function with respect to another,Differentiating a function with respect to another,,"Motivation: in a computer science book, there are two $\mathbb{R}^d\to\mathbb{R}$ functions $f$ and $g$ related by $$f(\textbf{x}) = x_kA\left(g(\textbf{x})\right)+ B(\textbf{x})$$ for functions $A:\mathbb{R}\to\mathbb{R}$ and $B:\mathbb{R}^d\to\mathbb{R}$ . The author casually writes $$\frac{\partial f}{\partial g}(\textbf{x}) = x_kA'\left(g(\textbf{x})\right)$$ and, while I understand the intuition, I'm struggling to formally and generally define $\partial f/\partial g$ . The question: given two $\mathbb{R}^d\to\mathbb{R}$ functions $f$ and $g$ , when and how are we to define $$\frac{\partial f}{\partial g}?$$ My attempts: here are two definitions that have failed so far: Definition $1$ : we say $f$ is differentiable with respect to $g$ if there is a function $\phi$ such that $$f = \phi\circ g$$ in which case we define $$\frac{\partial f}{\partial g}(\textbf{x}) = \phi'(g(\textbf{x})).$$ The issue here is that such $\phi$ (almost) never exists. Furthermore, the original relation in the book does not follow this form. We need something more general. Definition $2$ : we say $f$ is differentiable with respect to $g$ iff there is a function $\phi$ such that $$f(\textbf{x}) = \phi\left(g(\textbf{x}),\textbf{x}\right)$$ for any $\textbf{x}\in\mathbb{R}^d$ , in which case we define $$\frac{\partial f}{\partial g}(\textbf{x}) = \frac{\partial \phi}{x^0}(g(\textbf{x}),\textbf{x})$$ in coordinates $\phi(x^0,x^1\ldots,x^d) = \phi(x^0,\textbf{x})$ . Letting $\phi : (x^0,\textbf{x})\mapsto x_kA(x^0)+B(\textbf{x})$ we find that the example at the beginning is covered by this definition. The issue now has to do with the domain of $\phi$ ; I did not specify it in the definition because I do not know what its domain should be. If we set the domain of $\phi$ as $$\bigg\{(g(\textbf{x}),\textbf{x}) : \textbf{x}\in\mathbb{R}^d\bigg\}$$ then it is often so small that it does not allow us to differentiate: if $g$ is given by $(x,y)\mapsto x$ , we'd ideally wish for $$\frac{\partial f}{\partial g} = \frac{\partial f}{\partial x},$$ but in such case $\text{dom}(\phi)$ is $$\bigg\{ (t,x,y) : t = x\bigg\},$$ which does not allow us to differentiate. On the other hand, if we require the domain of $\phi$ to be $\mathbb{R}^{d+1}$ , then the derivative is no longer unique: simply let $f,g:\mathbb{R}\to\mathbb{R}$ be the identity. Then both $$\phi:(t,x) \mapsto x \ \ \ \ \text{ and } \psi:(t,x) \mapsto t$$ comply with the property $$x = \phi(x,x) = \psi(x,x),$$ yet $$0 = \frac{\partial\phi}{\partial x^0} \neq  \frac{\partial\psi}{\partial x^0} = 1.$$","Motivation: in a computer science book, there are two functions and related by for functions and . The author casually writes and, while I understand the intuition, I'm struggling to formally and generally define . The question: given two functions and , when and how are we to define My attempts: here are two definitions that have failed so far: Definition : we say is differentiable with respect to if there is a function such that in which case we define The issue here is that such (almost) never exists. Furthermore, the original relation in the book does not follow this form. We need something more general. Definition : we say is differentiable with respect to iff there is a function such that for any , in which case we define in coordinates . Letting we find that the example at the beginning is covered by this definition. The issue now has to do with the domain of ; I did not specify it in the definition because I do not know what its domain should be. If we set the domain of as then it is often so small that it does not allow us to differentiate: if is given by , we'd ideally wish for but in such case is which does not allow us to differentiate. On the other hand, if we require the domain of to be , then the derivative is no longer unique: simply let be the identity. Then both comply with the property yet","\mathbb{R}^d\to\mathbb{R} f g f(\textbf{x}) = x_kA\left(g(\textbf{x})\right)+ B(\textbf{x}) A:\mathbb{R}\to\mathbb{R} B:\mathbb{R}^d\to\mathbb{R} \frac{\partial f}{\partial g}(\textbf{x}) = x_kA'\left(g(\textbf{x})\right) \partial f/\partial g \mathbb{R}^d\to\mathbb{R} f g \frac{\partial f}{\partial g}? 1 f g \phi f = \phi\circ g \frac{\partial f}{\partial g}(\textbf{x}) = \phi'(g(\textbf{x})). \phi 2 f g \phi f(\textbf{x}) = \phi\left(g(\textbf{x}),\textbf{x}\right) \textbf{x}\in\mathbb{R}^d \frac{\partial f}{\partial g}(\textbf{x}) = \frac{\partial \phi}{x^0}(g(\textbf{x}),\textbf{x}) \phi(x^0,x^1\ldots,x^d) = \phi(x^0,\textbf{x}) \phi : (x^0,\textbf{x})\mapsto x_kA(x^0)+B(\textbf{x}) \phi \phi \bigg\{(g(\textbf{x}),\textbf{x}) : \textbf{x}\in\mathbb{R}^d\bigg\} g (x,y)\mapsto x \frac{\partial f}{\partial g} = \frac{\partial f}{\partial x}, \text{dom}(\phi) \bigg\{ (t,x,y) : t = x\bigg\}, \phi \mathbb{R}^{d+1} f,g:\mathbb{R}\to\mathbb{R} \phi:(t,x) \mapsto x
\ \ \ \ \text{ and }
\psi:(t,x) \mapsto t x = \phi(x,x) = \psi(x,x), 0 = \frac{\partial\phi}{\partial x^0} \neq 
\frac{\partial\psi}{\partial x^0} = 1.","['real-analysis', 'calculus', 'multivariable-calculus', 'definition', 'partial-derivative']"
64,upper semicontinuous definitions,upper semicontinuous definitions,,"I'm learning about upper (lower) semicontinuous and saw $3$ defintions for this: Definition 1: The function $f: A\subset\mathbb{R}^n\to\mathbb{R}$ is called upper semicontinuous(resp.: lower semicontinuous) at $x_0$ if $\forall\epsilon>0,\exists\delta>0$ s.t: $f(x)-f(x_0)<\epsilon$ ( resp.: $f(x)-f(x_0)>\epsilon)$ $\forall x\in B(x_0,\delta)$ ; $f$ is upper semicontinuous (resp.: lower semicontinuous) on $A$ if $f$ is upper semicontinuous (resp.:  lower semicontinuous) at every point in $A$ . Definition 2 a) $\forall a\in\mathbb{R}$ , the set $\{x\in A: f(x)<a\}$ is open in $A$ (resp.: $\{x\in A: f(x)>a\}$ is open in $A$ ). Definition 3 b) $\forall b\in\mathbb{R}$ , the set $\{x\in A: f(x)\geq b\}$ is closed in $A$ (resp.: $\{x\in A: f(x)\leq b\}$ is closed in $A$ ). I can see that def2 is equivalent to def3. But why def1 is equipvalent to def2 (or def3)? Could someone explain to help me understand better? Thanks in advance","I'm learning about upper (lower) semicontinuous and saw defintions for this: Definition 1: The function is called upper semicontinuous(resp.: lower semicontinuous) at if s.t: ( resp.: ; is upper semicontinuous (resp.: lower semicontinuous) on if is upper semicontinuous (resp.:  lower semicontinuous) at every point in . Definition 2 a) , the set is open in (resp.: is open in ). Definition 3 b) , the set is closed in (resp.: is closed in ). I can see that def2 is equivalent to def3. But why def1 is equipvalent to def2 (or def3)? Could someone explain to help me understand better? Thanks in advance","3 f: A\subset\mathbb{R}^n\to\mathbb{R} x_0 \forall\epsilon>0,\exists\delta>0 f(x)-f(x_0)<\epsilon f(x)-f(x_0)>\epsilon) \forall x\in B(x_0,\delta) f A f A \forall a\in\mathbb{R} \{x\in A: f(x)<a\} A \{x\in A: f(x)>a\} A \forall b\in\mathbb{R} \{x\in A: f(x)\geq b\} A \{x\in A: f(x)\leq b\} A","['real-analysis', 'calculus']"
65,Term for injective/one-to-one with respect to just one of multiple variables?,Term for injective/one-to-one with respect to just one of multiple variables?,,"As I understand it , a function $z=f(x,y)$ would be one-to-one or injective if there is only one unique $(x_1,y_1)$ pair which yields some value $f(x_1,y_1)=z_1$ . In this definition, the pair of variables $(x,y)$ is the entity that maps one-to-one to $z$ . Is there a ""partial"" equivalent to this that treats $x$ and $y$ separately? I want to say something like "" $z=f(x,y)$ is one-to-one with respect to $x$ for all $y$ "" , meaning that for any choice of $y_1$ in the domain of $y$ , $f(x,y_1)$ maps $x$ to $z$ one-to-one. This property would mean that I can always ""invert"" a function $z=f(x,y)$ into $x=f'(z,y)$ , but I can't necessarily do the same for $y=f''(x,z)$ . In general , for a multi-variable function , replace "" $y$ "" with ""all other variables."" Is there a concise terminology for this?","As I understand it , a function would be one-to-one or injective if there is only one unique pair which yields some value . In this definition, the pair of variables is the entity that maps one-to-one to . Is there a ""partial"" equivalent to this that treats and separately? I want to say something like "" is one-to-one with respect to for all "" , meaning that for any choice of in the domain of , maps to one-to-one. This property would mean that I can always ""invert"" a function into , but I can't necessarily do the same for . In general , for a multi-variable function , replace "" "" with ""all other variables."" Is there a concise terminology for this?","z=f(x,y) (x_1,y_1) f(x_1,y_1)=z_1 (x,y) z x y z=f(x,y) x y y_1 y f(x,y_1) x z z=f(x,y) x=f'(z,y) y=f''(x,z) y","['multivariable-calculus', 'functions', 'notation', 'terminology', 'inverse']"
66,Differential Equation Absolute Value,Differential Equation Absolute Value,,I must solve $\frac{dy}{dt} = y(1-y)$ . I work through this using partial fractions and obtain | $\frac{y}{1-y}|$ $=$ $Ae^t$ . I am confused what to do with the absolute values. Would I have two separate general solutions being $\frac{y}{1-y}$ $=$ $Ae^t$ and $\frac{-y}{1-y}$ $=$ $Ae^t$ ?,I must solve . I work through this using partial fractions and obtain | . I am confused what to do with the absolute values. Would I have two separate general solutions being and ?,\frac{dy}{dt} = y(1-y) \frac{y}{1-y}| = Ae^t \frac{y}{1-y} = Ae^t \frac{-y}{1-y} = Ae^t,"['real-analysis', 'ordinary-differential-equations', 'multivariable-calculus', 'derivatives']"
67,"What is the derivative of $f(x, y) = x + y^2$ with respect to $x$ if $y^2 = x^4$?",What is the derivative of  with respect to  if ?,"f(x, y) = x + y^2 x y^2 = x^4","What is the derivative of $f(x, y) = x + y^2$ with respect to $x$ if $y^2 = x^4$ ? This is what I have tried: $ \frac{df}{dx} = \frac{\partial f}{\partial x} \frac{d x}{d x} + \frac{\partial f}{\partial y} \frac{d y}{d x} = 2y * (\pm) 2x $ . Could you please help me understand if my solution is correct?",What is the derivative of with respect to if ? This is what I have tried: . Could you please help me understand if my solution is correct?,"f(x, y) = x + y^2 x y^2 = x^4  \frac{df}{dx} = \frac{\partial f}{\partial x} \frac{d x}{d x} + \frac{\partial f}{\partial y} \frac{d y}{d x} = 2y * (\pm) 2x ","['calculus', 'multivariable-calculus']"
68,"I think the author's proof of Theorem 4-4(1) is not correct. (""Calculus on Manifolds"" by Michael Spivak)","I think the author's proof of Theorem 4-4(1) is not correct. (""Calculus on Manifolds"" by Michael Spivak)",,"I am reading ""Calculus on Manifolds"" by Michael Spivak. I think the author's proof of Theorem 4-4(1) is not correct. Am I right? The author wrote as follows in the proof of Theorem 4-4(1): $$\sum_{\sigma\in G\cdot\sigma_0} \text{sgn }\sigma\cdot S(v_{\sigma(1)},\dots,v_{\sigma(k)})\cdot T(v_{\sigma(k+1)},\dots,v_{\sigma(k+l)}) =\left[\text{sgn }\sigma_0\cdot\sum_{\sigma'\in G}\text{sgn }\sigma'\cdot S(w_{\sigma'(1)},\dots,w_{\sigma'(k)})\right]\cdot T(w_{k+1},\dots,w_{k+l}) =0.$$ But I think the following is correct: $$\sum_{\sigma\in \sigma_0\cdot G} \text{sgn }\sigma\cdot S(v_{\sigma(1)},\dots,v_{\sigma(k)})\cdot T(v_{\sigma(k+1)},\dots,v_{\sigma(k+l)}) \\=\sum_{\sigma'\in G} \text{sgn }(\sigma_0\cdot\sigma')\cdot S(v_{\sigma_0\cdot\sigma'(1)},\dots,v_{\sigma_0\cdot\sigma'(k)})\cdot T(v_{\sigma_0\cdot\sigma'(k+1)},\dots,v_{\sigma_0\cdot\sigma'(k+l)}) \\=\text{sgn }\sigma_0\cdot\sum_{\sigma'\in G} \text{sgn }\sigma'\cdot S(w_{\sigma'(1)},\dots,w_{\sigma'(k)})\cdot T(w_{\sigma'(k+1)},\dots,w_{\sigma'(k+l)}) \\=\text{sgn }\sigma_0\cdot\sum_{\sigma'\in G} \text{sgn }\sigma'\cdot S(w_{\sigma'(1)},\dots,w_{\sigma'(k)})\cdot T(w_{k+1},\dots,w_{k+l}) \\=\left[\text{sgn }\sigma_0\cdot\sum_{\sigma'\in G}\text{sgn }\sigma'\cdot S(w_{\sigma'(1)},\dots,w_{\sigma'(k)})\right]\cdot T(w_{k+1},\dots,w_{k+l}) =0.$$ The author defined $w_1,\dots,w_{k+l}$ as follows: $v_{\sigma_0(1)},\dots,v_{\sigma_0(k+l)}=w_1,\dots,w_{k+l}$ . So, $v_{\sigma_0\cdot\sigma'(1)},\dots,v_{\sigma_0\cdot\sigma'(k+l)}=w_{\sigma'(1)},\dots,w_{\sigma'(k+l)}$ .","I am reading ""Calculus on Manifolds"" by Michael Spivak. I think the author's proof of Theorem 4-4(1) is not correct. Am I right? The author wrote as follows in the proof of Theorem 4-4(1): But I think the following is correct: The author defined as follows: . So, .","\sum_{\sigma\in G\cdot\sigma_0} \text{sgn }\sigma\cdot S(v_{\sigma(1)},\dots,v_{\sigma(k)})\cdot T(v_{\sigma(k+1)},\dots,v_{\sigma(k+l)}) =\left[\text{sgn }\sigma_0\cdot\sum_{\sigma'\in G}\text{sgn }\sigma'\cdot S(w_{\sigma'(1)},\dots,w_{\sigma'(k)})\right]\cdot T(w_{k+1},\dots,w_{k+l})
=0. \sum_{\sigma\in \sigma_0\cdot G} \text{sgn }\sigma\cdot S(v_{\sigma(1)},\dots,v_{\sigma(k)})\cdot T(v_{\sigma(k+1)},\dots,v_{\sigma(k+l)})
\\=\sum_{\sigma'\in G} \text{sgn }(\sigma_0\cdot\sigma')\cdot S(v_{\sigma_0\cdot\sigma'(1)},\dots,v_{\sigma_0\cdot\sigma'(k)})\cdot T(v_{\sigma_0\cdot\sigma'(k+1)},\dots,v_{\sigma_0\cdot\sigma'(k+l)})
\\=\text{sgn }\sigma_0\cdot\sum_{\sigma'\in G} \text{sgn }\sigma'\cdot S(w_{\sigma'(1)},\dots,w_{\sigma'(k)})\cdot T(w_{\sigma'(k+1)},\dots,w_{\sigma'(k+l)})
\\=\text{sgn }\sigma_0\cdot\sum_{\sigma'\in G} \text{sgn }\sigma'\cdot S(w_{\sigma'(1)},\dots,w_{\sigma'(k)})\cdot T(w_{k+1},\dots,w_{k+l})
\\=\left[\text{sgn }\sigma_0\cdot\sum_{\sigma'\in G}\text{sgn }\sigma'\cdot S(w_{\sigma'(1)},\dots,w_{\sigma'(k)})\right]\cdot T(w_{k+1},\dots,w_{k+l})
=0. w_1,\dots,w_{k+l} v_{\sigma_0(1)},\dots,v_{\sigma_0(k+l)}=w_1,\dots,w_{k+l} v_{\sigma_0\cdot\sigma'(1)},\dots,v_{\sigma_0\cdot\sigma'(k+l)}=w_{\sigma'(1)},\dots,w_{\sigma'(k+l)}","['multivariable-calculus', 'tensors', 'multilinear-algebra']"
69,Limit of a double integral over the disk,Limit of a double integral over the disk,,"I am stuck with the following request. Compute $$ \lim_{r\to+\infty}e^{-r}\int_{B(0,r)}e^{|x|+|y|}dxdy $$ For the integral, one gets $$ \int_{B(0,r)}e^{|x|+|y|}dxdy=4\int_{0}^r\int_0^{\sqrt{r^2-x^2}}e^{x+y}dxdy=4\int_{0}^r e^x(e^{\sqrt{r^2-x^2}}-1)dx $$ but with this integral I don't know how to proceed (I tried with $x=r\cos(\theta)$ with no success).","I am stuck with the following request. Compute For the integral, one gets but with this integral I don't know how to proceed (I tried with with no success).","
\lim_{r\to+\infty}e^{-r}\int_{B(0,r)}e^{|x|+|y|}dxdy
 
\int_{B(0,r)}e^{|x|+|y|}dxdy=4\int_{0}^r\int_0^{\sqrt{r^2-x^2}}e^{x+y}dxdy=4\int_{0}^r e^x(e^{\sqrt{r^2-x^2}}-1)dx
 x=r\cos(\theta)","['integration', 'multivariable-calculus']"
70,Formal Justification for Jacobian as Area Element,Formal Justification for Jacobian as Area Element,,"Consider a function $\vec{f}:\Bbb R^2 \rightarrow \Bbb R^2$ with Frechet derivative $Df(\vec{a})$ at some $\vec{a} = (a_x, a_y) \in \Bbb R^2$ , and infinitesimal rectangle $R(\delta x, \delta y) = [a_x, a_x + \delta x] \times [a_y, a_y + \delta y]$ . Under transformation $\vec{f}$ , this infinitesimal rectangle is mapped to an infinitesimal parallelogram and its area is scaled by the Jacobian $J = \lvert \det(Df(\vec{a})) \rvert$ . Formally, for $R(\Delta x, \Delta y) = [a_x, a_x + \Delta x] \times [a_y, a_y + \Delta y]$ , $$ \lim_{\Delta \vec{x} \to \vec{0}} \frac{area(\vec{f}(R(\Delta \vec{x})))}{area(R(\Delta x, \Delta y))} = \lvert \det(D\vec{f}(a))) \rvert$$ Is it possible to formally prove this using the definition of the Frechet derivative? If not, how could it be done? EDIT: The definition of $area(\vec{f}(R(\Delta \vec{x})))$ is the value such that for any $\epsilon > 0$ there exists partition $P$ of some rectangle enclosing $S = \vec{f}(R(\Delta \vec{x}))$ such that upper and lower Riemann sums $U_P \vec{f} \chi_S - L_P \vec{f} \chi_S < \epsilon$ . Thanks in advance for your help.","Consider a function with Frechet derivative at some , and infinitesimal rectangle . Under transformation , this infinitesimal rectangle is mapped to an infinitesimal parallelogram and its area is scaled by the Jacobian . Formally, for , Is it possible to formally prove this using the definition of the Frechet derivative? If not, how could it be done? EDIT: The definition of is the value such that for any there exists partition of some rectangle enclosing such that upper and lower Riemann sums . Thanks in advance for your help.","\vec{f}:\Bbb R^2 \rightarrow \Bbb R^2 Df(\vec{a}) \vec{a} = (a_x, a_y) \in \Bbb R^2 R(\delta x, \delta y) = [a_x, a_x + \delta x] \times [a_y, a_y + \delta y] \vec{f} J = \lvert \det(Df(\vec{a})) \rvert R(\Delta x, \Delta y) = [a_x, a_x + \Delta x] \times [a_y, a_y + \Delta y]  \lim_{\Delta \vec{x} \to \vec{0}} \frac{area(\vec{f}(R(\Delta \vec{x})))}{area(R(\Delta x, \Delta y))} = \lvert \det(D\vec{f}(a))) \rvert area(\vec{f}(R(\Delta \vec{x}))) \epsilon > 0 P S = \vec{f}(R(\Delta \vec{x})) U_P \vec{f} \chi_S - L_P \vec{f} \chi_S < \epsilon","['real-analysis', 'multivariable-calculus', 'area', 'jacobian']"
71,What are the Units of Flux and How do They Relate to Their Physical Meaning,What are the Units of Flux and How do They Relate to Their Physical Meaning,,"While taking Calculus III, which included some vector calculus, we defined the flux of a vector field $\mathbf{F} \colon \mathbb{R}^3 \to \mathbb{R}^3$ through a surface $S \subset \mathbb{R}^3$ as $$\iint_S \mathbf{F} \cdot \mathbf{n} \, d\sigma.$$ We were told that if we thought of $\mathbf{F}$ as the vector field defining how a fluid moves through space, the flux calculates the rate at which this fluid flows through $S$ . Intuitively then, wouldn't the ""units"" then be something along the line of $$\frac{``\text{quantity""}}{\text{time}}.$$ So something like ""The flux through $S$ is the amount of quantity that moves through it, as defined by $\mathbf{F}$ , each unit of time."" However, looking at the integral, the units seem to be different. They seem to be $$\frac{``\text{quantity""}}{\text{time}} \cdot \text{area}.$$ I really don't understand what the extra area quantity is doing. If we want to know the rate at which a quantity moves through $S$ , doesn't this imply that $$\frac{``\text{quantity""}}{\text{time}}$$ measures the rate at which a quantity moves through $S$ per unit of area ? As if there lies in a secret dimension? Multiplying by area then gives the total flux for that small surface. Take for example finding the flux of an electric field through a surface. The units of electric flux is $$\frac{\text{N}}{\text{C}} \cdot \text{m}^2.$$ (More surprising here is that there isn't a notion of time; but I suppose that's because previously, we thought of the vector field defining motion?) What is the extra $\text{m}^2$ factor representing? Why wouldn't the total amount of ""electric field going through a surface"" simply be in the units of the electric field ( $\text{N}/\text{C}$ ) rather than including an extra ""area"" dimension?","While taking Calculus III, which included some vector calculus, we defined the flux of a vector field through a surface as We were told that if we thought of as the vector field defining how a fluid moves through space, the flux calculates the rate at which this fluid flows through . Intuitively then, wouldn't the ""units"" then be something along the line of So something like ""The flux through is the amount of quantity that moves through it, as defined by , each unit of time."" However, looking at the integral, the units seem to be different. They seem to be I really don't understand what the extra area quantity is doing. If we want to know the rate at which a quantity moves through , doesn't this imply that measures the rate at which a quantity moves through per unit of area ? As if there lies in a secret dimension? Multiplying by area then gives the total flux for that small surface. Take for example finding the flux of an electric field through a surface. The units of electric flux is (More surprising here is that there isn't a notion of time; but I suppose that's because previously, we thought of the vector field defining motion?) What is the extra factor representing? Why wouldn't the total amount of ""electric field going through a surface"" simply be in the units of the electric field ( ) rather than including an extra ""area"" dimension?","\mathbf{F} \colon \mathbb{R}^3 \to \mathbb{R}^3 S \subset \mathbb{R}^3 \iint_S \mathbf{F} \cdot \mathbf{n} \, d\sigma. \mathbf{F} S \frac{``\text{quantity""}}{\text{time}}. S \mathbf{F} \frac{``\text{quantity""}}{\text{time}} \cdot \text{area}. S \frac{``\text{quantity""}}{\text{time}} S \frac{\text{N}}{\text{C}} \cdot \text{m}^2. \text{m}^2 \text{N}/\text{C}","['multivariable-calculus', 'dimensional-analysis']"
72,A multivariable substitution generalization.,A multivariable substitution generalization.,,"I solved an integral using the variable substitution $$   x = \frac{u-v}{\sqrt{2}} $$ $$   y = \frac{u+v}{\sqrt{2}} $$ This is interesting because the Jacobian of this transformation is $1$ . Particularly, this transformation corresponds to a clockwise rotation of $\pi/4$ around the origin. The problem involved an integrand containing $xy$ , which is transformed to $1/2(u^2-v^2)$ . Now, out of curiosity, are there other transformations in higher dimensions sending $x_1x_2 \cdots x_n$ to $a\cdot(u_1^2 \pm u_2^2 \pm \ldots \pm u_n^2)$ for some constant $a$ ? What is the Jacobian, if such transformations exist? Do these transformations involve complex coefficients, and what are the transformations with real coefficients? Edit: As J.G. pointed out in the comments, it should probably be $a\cdot(u_1^n \pm u_2^n \pm \ldots \pm u_n^n)$ .","I solved an integral using the variable substitution This is interesting because the Jacobian of this transformation is . Particularly, this transformation corresponds to a clockwise rotation of around the origin. The problem involved an integrand containing , which is transformed to . Now, out of curiosity, are there other transformations in higher dimensions sending to for some constant ? What is the Jacobian, if such transformations exist? Do these transformations involve complex coefficients, and what are the transformations with real coefficients? Edit: As J.G. pointed out in the comments, it should probably be .","
  x = \frac{u-v}{\sqrt{2}}
 
  y = \frac{u+v}{\sqrt{2}}
 1 \pi/4 xy 1/2(u^2-v^2) x_1x_2 \cdots x_n a\cdot(u_1^2 \pm u_2^2 \pm \ldots \pm u_n^2) a a\cdot(u_1^n \pm u_2^n \pm \ldots \pm u_n^n)","['real-analysis', 'linear-algebra', 'geometry', 'multivariable-calculus', 'linear-transformations']"
73,Evaluate $\int_{2}^{\frac{2}{3}}\int_y^{2-2y}(x+2y)e^{y-x}dxdy$,Evaluate,\int_{2}^{\frac{2}{3}}\int_y^{2-2y}(x+2y)e^{y-x}dxdy,"Evaluate $$\int_{2}^{\frac{2}{3}}\int_y^{2-2y}(x+2y)e^{y-x}dxdy$$ My try: I actually started using the transformation $$u=x+2y, v=y-x$$ But I am really not sure, how to frame the limits for $u$ and $v$ . Any HINT please?","Evaluate My try: I actually started using the transformation But I am really not sure, how to frame the limits for and . Any HINT please?","\int_{2}^{\frac{2}{3}}\int_y^{2-2y}(x+2y)e^{y-x}dxdy u=x+2y, v=y-x u v","['calculus', 'integration', 'multivariable-calculus']"
74,If $F$ and $f$ are continuous show that $g$ is continuous,If  and  are continuous show that  is continuous,F f g,"Let $F:[a,b]\times [a,b] \rightarrow \mathbb{R}$ and $f:[a,b] \rightarrow \mathbb{R}$ be two continuous functions, let $g:[a,b] \rightarrow \mathbb{R}$ defined as: $$ g(x) = \int_a^b F(x,y) f(y) dy$$ Show that $g$ is continuous My attempt Since $F$ is continuous and it is defined on a compact subset of $\mathbb{R}^2$ then $F$ is uniformly continuous, so it is easy to see that for each $x,y \in [a,b]$ the functions $F_x, F_y:[a,b] \rightarrow \mathbb{R}$ defined as: $$ F_x(y) = F(x,y)$$ $$F_y(x) = F(x,y)$$ are also uniformly continuous. We can write $g$ as: $$ g(x) = \int_a^b F_y(x) f(y) dy$$ Note that: $$|g(x_1)-g(x_2)| = \left|\int_a^b F_y(x_1) f(y) dy - \int_a^b F_y(x_2) f(y) dy \right| = \left|\int_a^b (F_y(x_1)-F_y(x_2)) f(y) dy \right|$$ Let $\epsilon > 0$ , there exists $\delta >0$ such that if $|x_1-x_2| < \delta$ then $|F_y(x_1)-F_y(x_2)| < \dfrac{\epsilon}{b-a}$ $\Rightarrow \int_a^b |F_y(x_1)-F_y(x_2)|dy < \epsilon$ So we have: $$\left|\int_a^b F_y(x_1)-F_y(x_2) dy \right| \leq \int_a^b |F_y(x_1)-F_y(x_2)| dy < \epsilon$$ But the left side of the inequality is not exactly $|g(x_1)-g(x_2)|$ , so I'm having trouble trying to find an expression to prove the continuity of $g$ . Is there an easy way to prove it?","Let and be two continuous functions, let defined as: Show that is continuous My attempt Since is continuous and it is defined on a compact subset of then is uniformly continuous, so it is easy to see that for each the functions defined as: are also uniformly continuous. We can write as: Note that: Let , there exists such that if then So we have: But the left side of the inequality is not exactly , so I'm having trouble trying to find an expression to prove the continuity of . Is there an easy way to prove it?","F:[a,b]\times [a,b] \rightarrow \mathbb{R} f:[a,b] \rightarrow \mathbb{R} g:[a,b] \rightarrow \mathbb{R}  g(x) = \int_a^b F(x,y) f(y) dy g F \mathbb{R}^2 F x,y \in [a,b] F_x, F_y:[a,b] \rightarrow \mathbb{R}  F_x(y) = F(x,y) F_y(x) = F(x,y) g  g(x) = \int_a^b F_y(x) f(y) dy |g(x_1)-g(x_2)| = \left|\int_a^b F_y(x_1) f(y) dy - \int_a^b F_y(x_2) f(y) dy \right| = \left|\int_a^b (F_y(x_1)-F_y(x_2)) f(y) dy \right| \epsilon > 0 \delta >0 |x_1-x_2| < \delta |F_y(x_1)-F_y(x_2)| < \dfrac{\epsilon}{b-a} \Rightarrow \int_a^b |F_y(x_1)-F_y(x_2)|dy < \epsilon \left|\int_a^b F_y(x_1)-F_y(x_2) dy \right| \leq \int_a^b |F_y(x_1)-F_y(x_2)| dy < \epsilon |g(x_1)-g(x_2)| g","['real-analysis', 'multivariable-calculus', 'continuity', 'compactness']"
75,"How to prove $\int_{\mathbb R} \nabla_u F (x, u_k(x))\cdot\varphi dx \to \int_{\mathbb R} \nabla_u F (x, u(x))\cdot\varphi dx$ for all test function?",How to prove  for all test function?,"\int_{\mathbb R} \nabla_u F (x, u_k(x))\cdot\varphi dx \to \int_{\mathbb R} \nabla_u F (x, u(x))\cdot\varphi dx","Let $F(x, u)\in C^1(\mathbb R\times\mathbb R^n, \mathbb R)$ and let $(u_k)$ be a sequence such that $$ u_k\to u \text{ in } L^\infty_{loc}(\mathbb R, \mathbb R^n)$$ and $$ u_k\to u \text{ a.e. in } \mathbb R.$$ I want to use these information to get that $$\int_{\mathbb R} \nabla_u F (x, u_k(x))\cdot\varphi dx \to \int_{\mathbb R} \nabla_u F (x, u(x))\cdot\varphi dx \quad\forall \varphi\in C_c^\infty(\mathbb R, \mathbb R^n)$$ as $k\to +\infty$ . Anyone can please provide some hints? I can not use Fatou's Lemma (since I don not know if the sequence is nonnegative) and I can not use the dominated convergence theorem (since I can not find a function of $L^1$ to dominate that quantity. The only idea left is to observe that $$\int_{\mathbb R} \nabla_u F (x, u_k(x))\cdot\varphi dx = \int_{supp(\varphi)} \nabla_u F (x, u_k(x))\cdot\varphi dx,$$ but I am not sure how to use this information. Anyone could please help?",Let and let be a sequence such that and I want to use these information to get that as . Anyone can please provide some hints? I can not use Fatou's Lemma (since I don not know if the sequence is nonnegative) and I can not use the dominated convergence theorem (since I can not find a function of to dominate that quantity. The only idea left is to observe that but I am not sure how to use this information. Anyone could please help?,"F(x, u)\in C^1(\mathbb R\times\mathbb R^n, \mathbb R) (u_k)  u_k\to u \text{ in } L^\infty_{loc}(\mathbb R, \mathbb R^n)  u_k\to u \text{ a.e. in } \mathbb R. \int_{\mathbb R} \nabla_u F (x, u_k(x))\cdot\varphi dx \to \int_{\mathbb R} \nabla_u F (x, u(x))\cdot\varphi dx \quad\forall \varphi\in C_c^\infty(\mathbb R, \mathbb R^n) k\to +\infty L^1 \int_{\mathbb R} \nabla_u F (x, u_k(x))\cdot\varphi dx = \int_{supp(\varphi)} \nabla_u F (x, u_k(x))\cdot\varphi dx,","['real-analysis', 'calculus', 'limits', 'multivariable-calculus']"
76,I can't find the minimum point using Lagrange multipliers in that function,I can't find the minimum point using Lagrange multipliers in that function,,"I need to find the maximum and minimum points of the function $f(x,y,z) = x^2 + y^2 + z^2$ restricted to $x^4 + y^4 + z^4 = 1$ . I manage to find the maximum point, $\sqrt{3}$ . However, the template is also pointing to a minimum point equal to 1. I know that I can make $x = 0$ and $y = 0$ in the constraint, and thus conclude that $z= \pm 1$ , or just change the combinations. However, solving the system \begin{align*} 2x &= \lambda 4x^3 \\ 2y &= \lambda 4y^3 \\ 2z &= \lambda 4z^3 \\ x^4 + y^4 + z^4 &= 1 \end{align*} I can't find $z= \pm 1$ . Could anyone help me?","I need to find the maximum and minimum points of the function restricted to . I manage to find the maximum point, . However, the template is also pointing to a minimum point equal to 1. I know that I can make and in the constraint, and thus conclude that , or just change the combinations. However, solving the system I can't find . Could anyone help me?","f(x,y,z) = x^2 + y^2 + z^2 x^4 + y^4 + z^4 = 1 \sqrt{3} x = 0 y = 0 z= \pm 1 \begin{align*}
2x &= \lambda 4x^3 \\
2y &= \lambda 4y^3 \\
2z &= \lambda 4z^3 \\
x^4 + y^4 + z^4 &= 1
\end{align*} z= \pm 1","['multivariable-calculus', 'lagrange-multiplier']"
77,Area Calculation of Region A (Using Double Integral),Area Calculation of Region A (Using Double Integral),,"I'm calculating the area of the region $A$ defined by the following constraints: $$ A = \{(x, y) \mid x^2 + y^2 \geq 2, \; x^2 + y^2 \leq 2x, \; y \geq 0\} $$ To calculate the area of the region $A$ , we can set up the double integral in Cartesian coordinates as: $$ A = \iint_A \ dx \ dy $$ The constraints $x^2 + y^2 \geq 2$ and $x^2 + y^2 \leq 2x$ define region $A$ in the $xy$ -plane. Additionally, the constraint $y \geq 0$ ensures that the region is above the $x$ -axis. Transforming to cylindrical coordinates, where $x = r \cos\theta$ and $y = r \sin\theta$ , the constraints are expressed as $r^2 \geq 2$ and $r^2 \leq 2r \cos\theta$ . The angle $\theta$ varies between $0$ and $\pi$ to address the upper half of the $xy$ -plane. The integral to calculate the area is: $$ A = \int_{0}^{\pi} \int_{\sqrt{2}}^{2\cos\theta} \ r \ dr \ d\theta $$ I appreciate any additional guidance or methods that can facilitate solving this integral and calculating the area of region $A$ . Thank you in advance for your assistance!","I'm calculating the area of the region defined by the following constraints: To calculate the area of the region , we can set up the double integral in Cartesian coordinates as: The constraints and define region in the -plane. Additionally, the constraint ensures that the region is above the -axis. Transforming to cylindrical coordinates, where and , the constraints are expressed as and . The angle varies between and to address the upper half of the -plane. The integral to calculate the area is: I appreciate any additional guidance or methods that can facilitate solving this integral and calculating the area of region . Thank you in advance for your assistance!","A  A = \{(x, y) \mid x^2 + y^2 \geq 2, \; x^2 + y^2 \leq 2x, \; y \geq 0\}  A  A = \iint_A \ dx \ dy  x^2 + y^2 \geq 2 x^2 + y^2 \leq 2x A xy y \geq 0 x x = r \cos\theta y = r \sin\theta r^2 \geq 2 r^2 \leq 2r \cos\theta \theta 0 \pi xy  A = \int_{0}^{\pi} \int_{\sqrt{2}}^{2\cos\theta} \ r \ dr \ d\theta  A","['calculus', 'integration', 'multivariable-calculus', 'area', 'polar-coordinates']"
78,"Conservation of swept area (Kepler's $2$nd law), rigorous proof","Conservation of swept area (Kepler's nd law), rigorous proof",2,"Briefly, in class we proved Kepler's $2$ nd law like this: We've some random trajectory and two position vectors, $\mathbf r$ and $\mathbf{r}+d\mathbf{r}$ . Supposing $dr$ is small, we can approximate it to be the arc length, $ds$ . Thus, we've got that the differential area swept would be a triangle with height $r$ and base $dr$ : $$dA=\dfrac{1}{2}rdr\approx\dfrac{1}{2}rds=\dfrac{r^2}{2}d\theta.$$ We then ""divide"" by $dt$ and make use of conservation of angular momentum $\left(\dot \theta=\frac{L}{mr^2}\right)$ to finally prove it: $$\dfrac{dA}{dt}=\dfrac{r^2}{2}\dot \theta=\dfrac{L}{2m}=\text{ct.}$$ However, I'm not satisfied with this proof since there are some unrigorous steps and I was wondering whether maybe it could be done using double integrals and differentiation wrt time under the integral sign. If not, what other ways to prove it would be correct and rigorous too?","Briefly, in class we proved Kepler's nd law like this: We've some random trajectory and two position vectors, and . Supposing is small, we can approximate it to be the arc length, . Thus, we've got that the differential area swept would be a triangle with height and base : We then ""divide"" by and make use of conservation of angular momentum to finally prove it: However, I'm not satisfied with this proof since there are some unrigorous steps and I was wondering whether maybe it could be done using double integrals and differentiation wrt time under the integral sign. If not, what other ways to prove it would be correct and rigorous too?",2 \mathbf r \mathbf{r}+d\mathbf{r} dr ds r dr dA=\dfrac{1}{2}rdr\approx\dfrac{1}{2}rds=\dfrac{r^2}{2}d\theta. dt \left(\dot \theta=\frac{L}{mr^2}\right) \dfrac{dA}{dt}=\dfrac{r^2}{2}\dot \theta=\dfrac{L}{2m}=\text{ct.},"['multivariable-calculus', 'polar-coordinates', 'classical-mechanics']"
79,Derivative of indicator function involving time,Derivative of indicator function involving time,,"Take the function $f : \mathbb{R}^{+} \times \mathbb{R} \to \mathbb{R}$ defined by $$f(t,x) = \mathbb{1}_{[1-t,2-t]}(x) $$ I am trying to figure out what $\partial_{t}f$ should be in the distributional sense. My idea is to use the definition of the derivative $$\partial_{t}f = \lim_{h \to 0} \frac{f(t+h,x) - f(t,x)}{h}. $$ using that I got $\partial_{t}f = \delta(x+t-1) - \delta(x+t-2)$ . I am not sure if my answer is correct since the function is a multivariable function and I know that the derivative is often defined differently to what I gave above. If we were to find $\partial_{x}f$ then it would be $\delta(x+t-1) - \delta(x+t-2)$ and I am fine  with this. We can just fix time and treat it like a single variable function. But when time is involved I become unsure since we can't just fix $x$ and treat it as a single variable function..",Take the function defined by I am trying to figure out what should be in the distributional sense. My idea is to use the definition of the derivative using that I got . I am not sure if my answer is correct since the function is a multivariable function and I know that the derivative is often defined differently to what I gave above. If we were to find then it would be and I am fine  with this. We can just fix time and treat it like a single variable function. But when time is involved I become unsure since we can't just fix and treat it as a single variable function..,"f : \mathbb{R}^{+} \times \mathbb{R} \to \mathbb{R} f(t,x) = \mathbb{1}_{[1-t,2-t]}(x)  \partial_{t}f \partial_{t}f = \lim_{h \to 0} \frac{f(t+h,x) - f(t,x)}{h}.  \partial_{t}f = \delta(x+t-1) - \delta(x+t-2) \partial_{x}f \delta(x+t-1) - \delta(x+t-2) x","['functional-analysis', 'multivariable-calculus', 'partial-differential-equations', 'distribution-theory']"
80,"Where did the author use the condition that $C$ contains $x$? (Michael Spivak's ""Calculus on Manifolds"")","Where did the author use the condition that  contains ? (Michael Spivak's ""Calculus on Manifolds"")",C x,"For $\delta>0$ let $$M(a,f,\delta)=\sup\{f(x):x\in A\text{ and }|x-a|<\delta\},\\ m(a,f,\delta)=\inf\{f(x):x\in A\text{ and }|x-a|<\delta\}.$$ The oscillation $o(f,a)$ of $f$ at $a$ is defined by $o(f,a)=\lim_{\delta\to 0} [M(a,f,\delta)-m(a,f,\delta)]$ . This limit always exists, since $M(a,f,\delta)-m(a,f,\delta)$ decreases as $\delta$ decreases. In the proof of Theorem 1-11 in ""Calculus on Manifolds"" by Michael Spivak, the author wrote as follows: Let $C$ be an open rectangle containing $x$ such that ... Where did the author use the condition that $C$ contains $x$ ? Is the condition really necessary? 1-11 Theorem. Let $A\subset\mathbb{R}^n$ be closed. If $f:A\to\mathbb{R}$ is any bounded function, and $\varepsilon>0$ , then $\{x\in A:o(f,x)\geq\varepsilon\}$ is closed. Proof. Let $B=\{x\in A:o(f,x)\geq\varepsilon\}$ . We wish to show that $\mathbb{R}^n-B$ is open. If $x\in\mathbb{R}^n-B$ , then either $x\notin A$ or else $x\in A$ and $o(f,x)<\varepsilon$ . In the first case, since $A$ is closed, there is an open rectangle $C$ containing $x$ such that $C\subset\mathbb{R}^n-A\subset\mathbb{R}^n-B$ . In the second case there is a $\delta>0$ such that $M(x,f,\delta)-m(x,f,\delta)<\varepsilon$ . Let $C$ be an open rectangle containing $x$ such that $|x-y|<\delta$ for all $y\in C$ . Then if $y\in C$ there is a $\delta_1$ such that $|x-z|<\delta$ for all $z$ satisfying $|z-y|<\delta_1$ . Thus $M(y,f,\delta_1)-m(y,f,\delta_1)<\varepsilon$ , and consequently $o(y,f)<\varepsilon$ . Therefore $C\subset\mathbb{R}^n-B$ .","For let The oscillation of at is defined by . This limit always exists, since decreases as decreases. In the proof of Theorem 1-11 in ""Calculus on Manifolds"" by Michael Spivak, the author wrote as follows: Let be an open rectangle containing such that ... Where did the author use the condition that contains ? Is the condition really necessary? 1-11 Theorem. Let be closed. If is any bounded function, and , then is closed. Proof. Let . We wish to show that is open. If , then either or else and . In the first case, since is closed, there is an open rectangle containing such that . In the second case there is a such that . Let be an open rectangle containing such that for all . Then if there is a such that for all satisfying . Thus , and consequently . Therefore .","\delta>0 M(a,f,\delta)=\sup\{f(x):x\in A\text{ and }|x-a|<\delta\},\\
m(a,f,\delta)=\inf\{f(x):x\in A\text{ and }|x-a|<\delta\}. o(f,a) f a o(f,a)=\lim_{\delta\to 0} [M(a,f,\delta)-m(a,f,\delta)] M(a,f,\delta)-m(a,f,\delta) \delta C x C x A\subset\mathbb{R}^n f:A\to\mathbb{R} \varepsilon>0 \{x\in A:o(f,x)\geq\varepsilon\} B=\{x\in A:o(f,x)\geq\varepsilon\} \mathbb{R}^n-B x\in\mathbb{R}^n-B x\notin A x\in A o(f,x)<\varepsilon A C x C\subset\mathbb{R}^n-A\subset\mathbb{R}^n-B \delta>0 M(x,f,\delta)-m(x,f,\delta)<\varepsilon C x |x-y|<\delta y\in C y\in C \delta_1 |x-z|<\delta z |z-y|<\delta_1 M(y,f,\delta_1)-m(y,f,\delta_1)<\varepsilon o(y,f)<\varepsilon C\subset\mathbb{R}^n-B","['multivariable-calculus', 'proof-explanation']"
81,Multivariate Chain Rule Question from GRE September 2023 Practice Test - Question 45,Multivariate Chain Rule Question from GRE September 2023 Practice Test - Question 45,,"This question comes from the recently-released GRE Math Subject Test Form GR3768. The question is as follows: Let $u(x,y)$ and $v(x,y)$ be real-valued differentiable functions that are implicitly defined by the equations $x = f(u,v)$ and $y = g(u,v)$ , where $f$ and $g$ are real-valued differentiable functions. Which of the following is an expression for $\dfrac{\partial u}{\partial x}$ ? The correct answer to this question is $$\frac{\partial u}{\partial x} = \frac{\dfrac{\partial g}{\partial v}}{\dfrac{\partial f}{\partial u} \dfrac{\partial g}{\partial v}-\dfrac{\partial f}{\partial v} \dfrac{\partial g}{\partial u}}$$ if the denominator is different from zero. I'm stuck on where to start due to the way the functions are named confusing me. That being said, I did attempt to start with replacing the expressions for $x, y$ into the definitions of $u, v$ , but I wasn't sure how to deal with the recursive definitions of such functions.","This question comes from the recently-released GRE Math Subject Test Form GR3768. The question is as follows: Let and be real-valued differentiable functions that are implicitly defined by the equations and , where and are real-valued differentiable functions. Which of the following is an expression for ? The correct answer to this question is if the denominator is different from zero. I'm stuck on where to start due to the way the functions are named confusing me. That being said, I did attempt to start with replacing the expressions for into the definitions of , but I wasn't sure how to deal with the recursive definitions of such functions.","u(x,y) v(x,y) x = f(u,v) y = g(u,v) f g \dfrac{\partial u}{\partial x} \frac{\partial u}{\partial x} = \frac{\dfrac{\partial g}{\partial v}}{\dfrac{\partial f}{\partial u} \dfrac{\partial g}{\partial v}-\dfrac{\partial f}{\partial v} \dfrac{\partial g}{\partial u}} x, y u, v","['multivariable-calculus', 'chain-rule', 'gre-exam']"
82,Extending smooth maps from $\mathbb{R}^3$ to $S^3$,Extending smooth maps from  to,\mathbb{R}^3 S^3,"Suppose I have a smooth map $f\colon \mathbb{R}^3 \longrightarrow S^2$ . If I identify $\mathbb{R}^3$ with $U_S = S^3 - \{(0,0,1)\}$ via stereographic projection, $\varphi_S\colon U_S \longrightarrow \mathbb{R}^3$ , $\varphi_S(y^1, y^2, y^3, y^4) = \left(\frac{y^1}{1-y^4}, \frac{y^2}{1-y^4}, \frac{y^3}{1-y^4} \right)$ , and suppose $f(\vec{y})$ approaches a constant as $\lVert \vec{y}\rVert \rightarrow \infty$ , then $f \circ \varphi_S$ defines a map from $U_S$ to $S^2$ that extends via continuity to a map from $S^3$ to $S^2$ . My question now is, in what circumstance can we say that this extended map is also smooth?","Suppose I have a smooth map . If I identify with via stereographic projection, , , and suppose approaches a constant as , then defines a map from to that extends via continuity to a map from to . My question now is, in what circumstance can we say that this extended map is also smooth?","f\colon \mathbb{R}^3 \longrightarrow S^2 \mathbb{R}^3 U_S = S^3 - \{(0,0,1)\} \varphi_S\colon U_S \longrightarrow \mathbb{R}^3 \varphi_S(y^1, y^2, y^3, y^4) = \left(\frac{y^1}{1-y^4}, \frac{y^2}{1-y^4}, \frac{y^3}{1-y^4} \right) f(\vec{y}) \lVert \vec{y}\rVert \rightarrow \infty f \circ \varphi_S U_S S^2 S^3 S^2","['calculus', 'geometry', 'multivariable-calculus', 'differential']"
83,Is there any way I can calculate this double integral?,Is there any way I can calculate this double integral?,,"$$ S = \int_{-8}^{8} \int_{-\frac{21}{2}}^{21} \left(1 + \frac{1}{8} x^2 + \frac{64}{3969} y^2\right)^{\frac{1}{2}} dy \, dx $$ I'm trying to finish a high school assignment, and because of the question I chose for myself, where I'm calculating the area of a surface above some region $R$ in the $xy$ -plane, it turned into this double integral that I have tried to solve. My problem is with calculating the inner integral , where I cannot use the reverse chain rule for single-variable integrals because there are two variables, and I'm unsure what other methods I can use. Any help would be greatly appreciated, thanks!","I'm trying to finish a high school assignment, and because of the question I chose for myself, where I'm calculating the area of a surface above some region in the -plane, it turned into this double integral that I have tried to solve. My problem is with calculating the inner integral , where I cannot use the reverse chain rule for single-variable integrals because there are two variables, and I'm unsure what other methods I can use. Any help would be greatly appreciated, thanks!","
S = \int_{-8}^{8} \int_{-\frac{21}{2}}^{21} \left(1 + \frac{1}{8} x^2 + \frac{64}{3969} y^2\right)^{\frac{1}{2}} dy \, dx
 R xy","['integration', 'multivariable-calculus', 'definite-integrals', 'surface-integrals']"
84,Does cyclical monotonicity imply almost everywhere differentiability?,Does cyclical monotonicity imply almost everywhere differentiability?,,"By a theorem due to Lebesgue, it is known that monotonicity of a univariate, real-valued function defined on an interval implies that it must be differentiable almost everywhere. $\textbf{Question:}$ Can we conclude almost everywhere differentiability of a function $f:\mathbb{R}^n\to \mathbb{R}^n$ given that it is $\textit{cyclically monotone}$ ?, i.e., for every $j\geq 2$ and every set of points $x_1,x_2,\dots, x_j\in \mathbb{R}^n$ with $x_1=x_j$ that $$\sum_{k=1}^{j-1}\langle x_{k+1}, f(x_{k+1})-f(x_k)\rangle  \geq 0$$ where $\langle \cdot, \cdot \rangle$ denotes the usual inner product on $\mathbb{R}^n$ . It is easy to see that this property generalizes univariate monotonicity (increasing), hence my question.","By a theorem due to Lebesgue, it is known that monotonicity of a univariate, real-valued function defined on an interval implies that it must be differentiable almost everywhere. Can we conclude almost everywhere differentiability of a function given that it is ?, i.e., for every and every set of points with that where denotes the usual inner product on . It is easy to see that this property generalizes univariate monotonicity (increasing), hence my question.","\textbf{Question:} f:\mathbb{R}^n\to \mathbb{R}^n \textit{cyclically monotone} j\geq 2 x_1,x_2,\dots, x_j\in \mathbb{R}^n x_1=x_j \sum_{k=1}^{j-1}\langle x_{k+1}, f(x_{k+1})-f(x_k)\rangle  \geq 0 \langle \cdot, \cdot \rangle \mathbb{R}^n","['real-analysis', 'calculus', 'multivariable-calculus', 'functions', 'convex-analysis']"
85,When do function value and gradient along a curve determine the function in a neighbourhood of the curve?,When do function value and gradient along a curve determine the function in a neighbourhood of the curve?,,"I have two $C^1$ functions $ f, g : U \subseteq \mathbb{R}^n \rightarrow \mathbb{R} $ and a curve $c : [0, 1] \rightarrow U $ such that $ f(x) = g(x)$ and $\nabla f(x) = \nabla g(x)$ for all points $x$ on the curve $c$ . Are you aware of any conditions that would imply that $ f(x) = g(x) $ for all $x$ in a small neighbourhood of $c$ ? In some sense, this would be similar to the identity theorem , but ideally without requiring that the functions are analytic and instead knowing some information about the gradient. Any references are appreciated, thank you!","I have two functions and a curve such that and for all points on the curve . Are you aware of any conditions that would imply that for all in a small neighbourhood of ? In some sense, this would be similar to the identity theorem , but ideally without requiring that the functions are analytic and instead knowing some information about the gradient. Any references are appreciated, thank you!","C^1  f, g : U \subseteq \mathbb{R}^n \rightarrow \mathbb{R}  c : [0, 1] \rightarrow U   f(x) = g(x) \nabla f(x) = \nabla g(x) x c  f(x) = g(x)  x c","['calculus', 'multivariable-calculus', 'vector-analysis']"
86,"Minimizing $(q_1, q_2, \dots, q_K) \mapsto \left(\sum_{i=1}^K\frac{p_i^2\sigma_i^2}{q_i}\right)\left( \sum_{i=1}^Kq_i\tau_i\right)$ with a constraint",Minimizing  with a constraint,"(q_1, q_2, \dots, q_K) \mapsto \left(\sum_{i=1}^K\frac{p_i^2\sigma_i^2}{q_i}\right)\left( \sum_{i=1}^Kq_i\tau_i\right)","$$ \begin{array}{ll} \underset {q_1, q_2, \dots, q_K} {\text{minimize}} & \displaystyle \left( \sum\limits_{i=1}^K \frac{p_i^2\sigma_i^2}{q_i} \right) \left( \sum\limits_{i=1}^K q_i\tau_i\right) \\ \text{subject to} & \displaystyle \sum\limits_{i=1}^K q_i = 1 \end{array} $$ It turns out that, the optimal $q_i$ is $$ q_i^* = \dfrac{\frac{p_i\sigma_i}{\sqrt{\tau_i}}}{\sum\limits_{\ell = 1}^K \dfrac{p_{\ell}\sigma_{\ell}}{\sqrt{\tau_{\ell}}}}.$$ The first method that comes to mind is by using Lagrangian method. I have $$ \mathcal{L}(q,\lambda) = \left(\sum_{i=1}^K\frac{p_i^2\sigma_i^2}{q_i}\right)\left( \sum_{i=1}^Kq_i\tau_i\right) + \lambda\left(\sum_{i=1}^Kq_i - 1 \right). $$ Then, $$ \frac{\partial \mathcal{L}}{\partial q_j} = \left(-\frac{p_i^2\sigma_i^2}{q_i^2} \right)\left( \sum_{i=1}^Kq_i\tau_i\right) + \tau_j\left(\sum_{i=1}^K\frac{p_i^2\sigma_i^2}{q_i}\right) + \lambda. $$ So, $\frac{\partial \mathcal{L}}{\partial q_j} = 0$ implies $$ q_j = \frac{p_j\sigma_j\sqrt{\sum_{\ell = 1}^Kq_{\ell}\tau_{\ell}}}{\sqrt{\tau_j\sum_{\ell = 1}^K\frac{p_{\ell}^2q_{\ell}^2}{q_{\ell}} + \lambda}}. $$ Usually, to find $\lambda$ I would just solve $$ \sum_{j=1}^K \frac{p_j\sigma_j\sqrt{\sum_{\ell = 1}^Kq_{\ell}\tau_{\ell}}}{\sqrt{\tau_j\sum_{\ell = 1}^K\frac{p_{\ell}^2q_{\ell}^2}{q_{\ell}} + \lambda}} = 1 $$ for $\lambda$ . But the equation looks very difficult to solve. Is there a trick that I can use to solve this? Motivation This question comes from a theoretic study of Monte Carlo simulation for reducing variance. One technique to reduce variance is called stratified sampling . The idea is to create partition of the population you want to sample from. But, how do we choose the allocation? This optimization problem is to find the optimal allocation.","It turns out that, the optimal is The first method that comes to mind is by using Lagrangian method. I have Then, So, implies Usually, to find I would just solve for . But the equation looks very difficult to solve. Is there a trick that I can use to solve this? Motivation This question comes from a theoretic study of Monte Carlo simulation for reducing variance. One technique to reduce variance is called stratified sampling . The idea is to create partition of the population you want to sample from. But, how do we choose the allocation? This optimization problem is to find the optimal allocation."," \begin{array}{ll} \underset {q_1, q_2, \dots, q_K} {\text{minimize}} & \displaystyle \left( \sum\limits_{i=1}^K \frac{p_i^2\sigma_i^2}{q_i} \right) \left( \sum\limits_{i=1}^K q_i\tau_i\right) \\ \text{subject to} & \displaystyle \sum\limits_{i=1}^K q_i = 1 \end{array}  q_i  q_i^* = \dfrac{\frac{p_i\sigma_i}{\sqrt{\tau_i}}}{\sum\limits_{\ell = 1}^K \dfrac{p_{\ell}\sigma_{\ell}}{\sqrt{\tau_{\ell}}}}. 
\mathcal{L}(q,\lambda) = \left(\sum_{i=1}^K\frac{p_i^2\sigma_i^2}{q_i}\right)\left( \sum_{i=1}^Kq_i\tau_i\right) + \lambda\left(\sum_{i=1}^Kq_i - 1 \right).
 
\frac{\partial \mathcal{L}}{\partial q_j} = \left(-\frac{p_i^2\sigma_i^2}{q_i^2} \right)\left( \sum_{i=1}^Kq_i\tau_i\right) + \tau_j\left(\sum_{i=1}^K\frac{p_i^2\sigma_i^2}{q_i}\right) + \lambda.
 \frac{\partial \mathcal{L}}{\partial q_j} = 0 
q_j = \frac{p_j\sigma_j\sqrt{\sum_{\ell = 1}^Kq_{\ell}\tau_{\ell}}}{\sqrt{\tau_j\sum_{\ell = 1}^K\frac{p_{\ell}^2q_{\ell}^2}{q_{\ell}} + \lambda}}.
 \lambda 
\sum_{j=1}^K \frac{p_j\sigma_j\sqrt{\sum_{\ell = 1}^Kq_{\ell}\tau_{\ell}}}{\sqrt{\tau_j\sum_{\ell = 1}^K\frac{p_{\ell}^2q_{\ell}^2}{q_{\ell}} + \lambda}} = 1
 \lambda","['multivariable-calculus', 'optimization', 'lagrange-multiplier', 'monte-carlo']"
87,Derivation of integral formula with Dirac delta function over level sets,Derivation of integral formula with Dirac delta function over level sets,,"Let $\Omega \subset \mathbb{R}^{n}$ be open, $f: \Omega \to \mathbb{R}$ be smooth and satisfy $\nabla f(x) \neq 0$ for evert $x$ . I am trying to derive the following identity: $$\int_{\Omega} \delta(f(x))h(x)dx = \int_{f(x) = 0}\frac{h(x)}{|\nabla f(x)|}dS(x).$$ I know the right hand side of the above formula is defined to be the pullback of $\delta$ with $f$ . Let $u \in \mathcal{D}'(\mathbb{R})$ be a given distribution and $\varphi$ a given test function. The pullback $f^{*}u$ of $u$ with (the smooth function) $f$ is, by definition, the distribution defined by the equality: $$\langle f^{*}u,\varphi \rangle = \langle u, \varphi_{f}\rangle$$ with: $$\varphi_{f}(t) = \frac{d}{dt}\int_{\{x \in \Omega: f(x)<t\}}\varphi(x)dx.$$ I know the standard argument to start the proof of the desired formula. Let $x_{0} \in \Omega$ be fixed. Because $\nabla f(x_{0}) \neq 0$ , there exists some $j=1,...,n$ , which without loss of generality we assume to be $j=1$ , such that $\frac{\partial f(x_{0})}{\partial x_{1}} \neq 0$ . In this case, by the inverse function theorem, there exists some neighborhood $U_{x_{0}}$ of $x_{0}$ and a neighborhood $V_{x_{0}}$ of $f(x_{0})$ such that $f: U_{x_{0}}\to V_{x_{0}}$ is a diffeomorphism. Hence, the function $\rho: U_{x_{0}} \to V_{x_{0}}$ given by: $$\rho^{-1}: (x_{1},...,x_{n}) \mapsto (f(x),x_{2},...,x_{n}) = (y_{1},...,y_{n})$$ is a diffeomorphism. Let $\varphi$ be a test function which we can assume to be such that $\text{supp}\varphi \subset U_{x_{0}}$ . We can then change coordinates to obtain: $$\varphi_{f}(t) = \int_{\mathbb{R}^{n-1}}\varphi(\rho(t,y'))|\det D\rho(t,y')|dy'$$ If the underlying distributin $u$ is taken to be $u = \delta$ , then: $$\langle f^{*}\delta,\varphi\rangle = \varphi_{f}(0) = \int_{\mathbb{R}^{n-1}}\varphi(\rho(0,y'))|\det D\rho(0,y')|dy'$$ Could someone help me complete the proof? I was trying to follow the proof in these notes , but I was a little confused with their argument at each step that $y' \mapsto F(0,y')$ parametrizes $M = \{x \in \Omega: f(x) = 0\}$ so it is given by $F(0,y') = (g(y'),y')$ for some function $g$ given by the implicit function theorem. Could you guys help me?","Let be open, be smooth and satisfy for evert . I am trying to derive the following identity: I know the right hand side of the above formula is defined to be the pullback of with . Let be a given distribution and a given test function. The pullback of with (the smooth function) is, by definition, the distribution defined by the equality: with: I know the standard argument to start the proof of the desired formula. Let be fixed. Because , there exists some , which without loss of generality we assume to be , such that . In this case, by the inverse function theorem, there exists some neighborhood of and a neighborhood of such that is a diffeomorphism. Hence, the function given by: is a diffeomorphism. Let be a test function which we can assume to be such that . We can then change coordinates to obtain: If the underlying distributin is taken to be , then: Could someone help me complete the proof? I was trying to follow the proof in these notes , but I was a little confused with their argument at each step that parametrizes so it is given by for some function given by the implicit function theorem. Could you guys help me?","\Omega \subset \mathbb{R}^{n} f: \Omega \to \mathbb{R} \nabla f(x) \neq 0 x \int_{\Omega} \delta(f(x))h(x)dx = \int_{f(x) = 0}\frac{h(x)}{|\nabla f(x)|}dS(x). \delta f u \in \mathcal{D}'(\mathbb{R}) \varphi f^{*}u u f \langle f^{*}u,\varphi \rangle = \langle u, \varphi_{f}\rangle \varphi_{f}(t) = \frac{d}{dt}\int_{\{x \in \Omega: f(x)<t\}}\varphi(x)dx. x_{0} \in \Omega \nabla f(x_{0}) \neq 0 j=1,...,n j=1 \frac{\partial f(x_{0})}{\partial x_{1}} \neq 0 U_{x_{0}} x_{0} V_{x_{0}} f(x_{0}) f: U_{x_{0}}\to V_{x_{0}} \rho: U_{x_{0}} \to V_{x_{0}} \rho^{-1}: (x_{1},...,x_{n}) \mapsto (f(x),x_{2},...,x_{n}) = (y_{1},...,y_{n}) \varphi \text{supp}\varphi \subset U_{x_{0}} \varphi_{f}(t) = \int_{\mathbb{R}^{n-1}}\varphi(\rho(t,y'))|\det D\rho(t,y')|dy' u u = \delta \langle f^{*}\delta,\varphi\rangle = \varphi_{f}(0) = \int_{\mathbb{R}^{n-1}}\varphi(\rho(0,y'))|\det D\rho(0,y')|dy' y' \mapsto F(0,y') M = \{x \in \Omega: f(x) = 0\} F(0,y') = (g(y'),y') g","['analysis', 'multivariable-calculus', 'proof-explanation', 'distribution-theory', 'dirac-delta']"
88,Proof that $V$ is an open set,Proof that  is an open set,V,"Let $f:\mathbb{R}^{n}\times \mathbb{R}^{n}\to \mathbb{R}$ be of class $C^{k+2}$ , $k \ge 0$ and suppose that, for each $(x,y) \in \mathbb{R}^{n}\times \mathbb{R}^{n}$ , the matrix: $$\bigg{[}\frac{\partial^{2}f}{\partial y_{i}\partial y_{j}}(x,y)\bigg{]}_{1\le i,j\le n}$$ is positive. In other words, for each fixed $x \in \mathbb{R}^{n}$ , the mapping $y \mapsto f_{x}(y) = f(x,y)$ is strictly convex. For each fixed $x \in \mathbb{R}^{n}$ and $p \in \mathbb{R}^{n}$ , define $\varphi_{x,p}: \mathbb{R}^{n}\to \mathbb{R}$ by: $$\varphi_{x,p}(y) = \langle y, p \rangle - f(x,y).$$ It follows from the hypothesis on $f$ that $\varphi_{x,p}$ has at most one critical point and, any such critical point (if it exists) is a global maximum. I want to prove the following: Proposition: The set $V =\{(x,p)\in \mathbb{R}^{n}\times \mathbb{R}^{n}: \mbox{$\varphi_{x,p}$ has a global maximum}\}$ is nonempty and open. Attempted Proof: Note that $(x,p) \in V \iff \exists y_{0} \in \mathbb{R}^{n}$ such that $\nabla_{y}\varphi_{x,p}(y_{0}) = 0 \iff \nabla_{y}f(x,y_{0}) = p$ . Hence: $$V = \{(x,p) \in \mathbb{R}^{n}\times \mathbb{R}^{n}: \mbox{$ y_{0} \in \mathbb{R}^{n}$ such that $\nabla_{y}f(x,y_{0}) = p$}\}$$ For each $x \in \mathbb{R}^{n}$ fixed, $\nabla_{y}f(x,\cdot)$ is injective and, hence, invertible. Let $p$ be an element of its range. In this case, there exists a unique $y_{0} \in \mathbb{R}^{n}$ with $\nabla_{y}f(x,y_{0}) = p$ , so that $V \neq \emptyset$ . My question is: how to prove $V$ is open? I sketched the following proof. Let $V_{x} = \{p \in \mathbb{R}^{n}: \mbox{$p=\nabla_{y}f(x,y_{0})$ for some $y_{0}$}\}$ . This is open because it is the pre-image of $\nabla_{y}f(x,\cdot)$ . Now $V = \bigcup_{x\in \mathbb{R}^{n}}\{x\}\times V_{x}$ . But is this set open? Note: the notation $\nabla_{y}f(x,y)$ means $(\frac{\partial f}{\partial y_{1}}(x,y),...,\frac{\partial f}{\partial y_{n}}(x,y))$ .","Let be of class , and suppose that, for each , the matrix: is positive. In other words, for each fixed , the mapping is strictly convex. For each fixed and , define by: It follows from the hypothesis on that has at most one critical point and, any such critical point (if it exists) is a global maximum. I want to prove the following: Proposition: The set is nonempty and open. Attempted Proof: Note that such that . Hence: For each fixed, is injective and, hence, invertible. Let be an element of its range. In this case, there exists a unique with , so that . My question is: how to prove is open? I sketched the following proof. Let . This is open because it is the pre-image of . Now . But is this set open? Note: the notation means .","f:\mathbb{R}^{n}\times \mathbb{R}^{n}\to \mathbb{R} C^{k+2} k \ge 0 (x,y) \in \mathbb{R}^{n}\times \mathbb{R}^{n} \bigg{[}\frac{\partial^{2}f}{\partial y_{i}\partial y_{j}}(x,y)\bigg{]}_{1\le i,j\le n} x \in \mathbb{R}^{n} y \mapsto f_{x}(y) = f(x,y) x \in \mathbb{R}^{n} p \in \mathbb{R}^{n} \varphi_{x,p}: \mathbb{R}^{n}\to \mathbb{R} \varphi_{x,p}(y) = \langle y, p \rangle - f(x,y). f \varphi_{x,p} V =\{(x,p)\in \mathbb{R}^{n}\times \mathbb{R}^{n}: \mbox{\varphi_{x,p} has a global maximum}\} (x,p) \in V \iff \exists y_{0} \in \mathbb{R}^{n} \nabla_{y}\varphi_{x,p}(y_{0}) = 0 \iff \nabla_{y}f(x,y_{0}) = p V = \{(x,p) \in \mathbb{R}^{n}\times \mathbb{R}^{n}: \mbox{ y_{0} \in \mathbb{R}^{n} such that \nabla_{y}f(x,y_{0}) = p}\} x \in \mathbb{R}^{n} \nabla_{y}f(x,\cdot) p y_{0} \in \mathbb{R}^{n} \nabla_{y}f(x,y_{0}) = p V \neq \emptyset V V_{x} = \{p \in \mathbb{R}^{n}: \mbox{p=\nabla_{y}f(x,y_{0}) for some y_{0}}\} \nabla_{y}f(x,\cdot) V = \bigcup_{x\in \mathbb{R}^{n}}\{x\}\times V_{x} \nabla_{y}f(x,y) (\frac{\partial f}{\partial y_{1}}(x,y),...,\frac{\partial f}{\partial y_{n}}(x,y))","['real-analysis', 'multivariable-calculus', 'solution-verification', 'convex-analysis']"
89,"Solving $\iint_{\mathscr{D}}\frac{(x-x')^2f(x',y')\,\mathrm{d}x'\mathrm{d}y'}{\left((x-x')^2+(y-y')^2\right)^{3/2}}=1$ for the unknown function $f$",Solving  for the unknown function,"\iint_{\mathscr{D}}\frac{(x-x')^2f(x',y')\,\mathrm{d}x'\mathrm{d}y'}{\left((x-x')^2+(y-y')^2\right)^{3/2}}=1 f","While solving a fluid dynamical problem, the following integral equation arised: $$ \iint_\mathscr{D} \frac{(x-x')^2 f(x',y') \, \mathrm{d}x' \mathrm{d}y'}{\left( (x-x')^2 + (y-y')^2 \right)^{3/2}} = 1 $$ for the unknown function $f$ on the domain $\mathscr{D}$ defined as a square of unit length centered at the origin, i.e. $x,y \in [-1/2,1/2]$ What I tried is to expand $f$ in the form of multivariate Taylor series and evaluate the resulting integrals term by term. This approach does not seem to be of help since identification with the right-hand side of the above equation is not possible. It would be great if someone here could provide useful hints that could help solve this integral equation. Thank you!","While solving a fluid dynamical problem, the following integral equation arised: for the unknown function on the domain defined as a square of unit length centered at the origin, i.e. What I tried is to expand in the form of multivariate Taylor series and evaluate the resulting integrals term by term. This approach does not seem to be of help since identification with the right-hand side of the above equation is not possible. It would be great if someone here could provide useful hints that could help solve this integral equation. Thank you!","
\iint_\mathscr{D} \frac{(x-x')^2 f(x',y') \, \mathrm{d}x' \mathrm{d}y'}{\left( (x-x')^2 + (y-y')^2 \right)^{3/2}} = 1
 f \mathscr{D} x,y \in [-1/2,1/2] f","['real-analysis', 'calculus', 'integration', 'multivariable-calculus', 'greens-function']"
90,Convergence of multiple iterated series,Convergence of multiple iterated series,,"I am searching for sources or references for this proposition. Can anyone help me? Thank you in advance. Let $A_{n_1,n_2,...,n_k}$ be multiple sequence on $\mathbb{C}$ and $\phi: \mathbb{N} \rightarrow \mathbb{N}^k$ be bijection. If $\sum_{n=1}^\infty|A_{\phi(n)}|$ converge, then $\sum_{n_1=1}^\infty\sum_{n_2=1}^\infty\cdots\sum_{n_k=1}^\infty A_{n_1,n_2,...,n_k}=\sum_{n=1}^\infty A_{\phi(n)} $ In fact, I have already proven it using my own method. However, I would like to reference this proposition in my article. If there are existing documents that have presented the proof of this proposition, I would be able to reference them without wasting space in the article. Do you happen to have any references? Thank you in advance.","I am searching for sources or references for this proposition. Can anyone help me? Thank you in advance. Let be multiple sequence on and be bijection. If converge, then In fact, I have already proven it using my own method. However, I would like to reference this proposition in my article. If there are existing documents that have presented the proof of this proposition, I would be able to reference them without wasting space in the article. Do you happen to have any references? Thank you in advance.","A_{n_1,n_2,...,n_k} \mathbb{C} \phi: \mathbb{N} \rightarrow \mathbb{N}^k \sum_{n=1}^\infty|A_{\phi(n)}| \sum_{n_1=1}^\infty\sum_{n_2=1}^\infty\cdots\sum_{n_k=1}^\infty A_{n_1,n_2,...,n_k}=\sum_{n=1}^\infty A_{\phi(n)} ","['sequences-and-series', 'multivariable-calculus']"
91,Change of variable formula for multivariate integral with constrain,Change of variable formula for multivariate integral with constrain,,"Suppose $g: \mathbb{R}^{n} \to \mathbb{R}$ is $C^{1}$ and satisfies $\nabla g(x) \neq 0$ for every $x \in \mathbb{R}^{n}$ . Let $E \in \mathbb{R}$ be fixed. I want to find an explicit expression for the integral: $$\int_{g(x) = E}f(x)dx,$$ provided the integral exists. I sketched some calculations, but I am not completely sure if it is correct. There are some steps I am not still very convinced. My idea is the following. Let $\phi: \mathbb{R}^{n}\to U\subset \mathbb{R}^{n}$ be a change of variable transformation (diffeomorphism) given by: $$\phi(x) = (\phi_{1}(x),\phi_{2}(x),...,\phi_{n}(x)) \tag{1}\label{1}$$ where I choose $\phi_{1}(x) = g(x)$ and, for each $i=2,...,n$ , $\phi_{i}: \mathbb{R}^{n} \to \mathbb{R}$ are $C^{1}$ functions such that the change of variables $\phi$ is orthogonal, in the sense that $\langle \nabla \phi_{i}(x),\nabla\phi_{j}(x)\rangle = 0$ for every $i\neq j$ . Consider the region: $$R = \{y = (y_{1},...,y_{n}) \in \mathbb{R}^{n}: y_{1} = E\}\tag{2}\label{2}$$ so we have: $$\{x \in \mathbb{R}^{n}: g(x) = E\} = \phi^{-1}(R) = \{x \in \mathbb{R}^{n}: \phi_{1}(x) = g(x) = E\}.$$ Using the change of variables formula, we obtain: $$\int_{g(x) = E}f(x) dx = \int_{R}f(\phi^{-1}(y))\frac{1}{|\det D\phi(\phi^{-1}(y))|}dy = \int_{\mathbb{R}^{n}}f(E,y_{2},...,y_{n})\frac{1}{\|\nabla g(E,y_{2},...,y_{n})\|}d\Sigma_{E}$$ where $\|x\| = \sqrt{x_{1}+\cdots +x_{n}^{2}}$ is the usual Euclidean norm and: $$d\Sigma_{E} = \frac{dy_{2}\cdots dy_{n}}{\prod_{i=2}^{n}\|(\nabla\phi_{i})(E,y_{2},...,y_{n})\|}$$ First of all, I would like to know if my reasoning and formula are correct. If so: Can I always find a change of variable $\phi$ which is orthogonal and has one of its components, say, $\phi_{1} = g$ as I did? I am not entirely convinced about it.","Suppose is and satisfies for every . Let be fixed. I want to find an explicit expression for the integral: provided the integral exists. I sketched some calculations, but I am not completely sure if it is correct. There are some steps I am not still very convinced. My idea is the following. Let be a change of variable transformation (diffeomorphism) given by: where I choose and, for each , are functions such that the change of variables is orthogonal, in the sense that for every . Consider the region: so we have: Using the change of variables formula, we obtain: where is the usual Euclidean norm and: First of all, I would like to know if my reasoning and formula are correct. If so: Can I always find a change of variable which is orthogonal and has one of its components, say, as I did? I am not entirely convinced about it.","g: \mathbb{R}^{n} \to \mathbb{R} C^{1} \nabla g(x) \neq 0 x \in \mathbb{R}^{n} E \in \mathbb{R} \int_{g(x) = E}f(x)dx, \phi: \mathbb{R}^{n}\to U\subset \mathbb{R}^{n} \phi(x) = (\phi_{1}(x),\phi_{2}(x),...,\phi_{n}(x)) \tag{1}\label{1} \phi_{1}(x) = g(x) i=2,...,n \phi_{i}: \mathbb{R}^{n} \to \mathbb{R} C^{1} \phi \langle \nabla \phi_{i}(x),\nabla\phi_{j}(x)\rangle = 0 i\neq j R = \{y = (y_{1},...,y_{n}) \in \mathbb{R}^{n}: y_{1} = E\}\tag{2}\label{2} \{x \in \mathbb{R}^{n}: g(x) = E\} = \phi^{-1}(R) = \{x \in \mathbb{R}^{n}: \phi_{1}(x) = g(x) = E\}. \int_{g(x) = E}f(x) dx = \int_{R}f(\phi^{-1}(y))\frac{1}{|\det D\phi(\phi^{-1}(y))|}dy = \int_{\mathbb{R}^{n}}f(E,y_{2},...,y_{n})\frac{1}{\|\nabla g(E,y_{2},...,y_{n})\|}d\Sigma_{E} \|x\| = \sqrt{x_{1}+\cdots +x_{n}^{2}} d\Sigma_{E} = \frac{dy_{2}\cdots dy_{n}}{\prod_{i=2}^{n}\|(\nabla\phi_{i})(E,y_{2},...,y_{n})\|} \phi \phi_{1} = g","['real-analysis', 'calculus', 'analysis', 'multivariable-calculus', 'change-of-variable']"
92,Multivariable calculus texts,Multivariable calculus texts,,"I would like to buy a book to study multivariable calculus. Currently, the texts I have in mind are: Vector Calculus, Linear Algebra, and Differential Forms A Unified Approach by Hubbard & Hubbard Multivariable Calculus with Applications by Lax & Terrell Functions of Several Real Variables by Moskowitz & Paliogiannis I want a book that has a clear expositions of the subjects of multivariable calculus. Also, I would like a book that avoids leaving proofs as excercises to the reader, or at least that does not do it most of the time. If possible, a book that also contains multiple examples/exercises with (fully) detailed explanations/solutions to at least some of the examples/exercises. I do not mind a rigorous approach to the subject as long as the content is explained with detail. Which of the books mentioned above fits best the description? Also, if you have other books in mind, feel free to recommend them as well. Thanks in advance! Note: I have taken two proof-based calculus classes and one proof-based linear algebra class.","I would like to buy a book to study multivariable calculus. Currently, the texts I have in mind are: Vector Calculus, Linear Algebra, and Differential Forms A Unified Approach by Hubbard & Hubbard Multivariable Calculus with Applications by Lax & Terrell Functions of Several Real Variables by Moskowitz & Paliogiannis I want a book that has a clear expositions of the subjects of multivariable calculus. Also, I would like a book that avoids leaving proofs as excercises to the reader, or at least that does not do it most of the time. If possible, a book that also contains multiple examples/exercises with (fully) detailed explanations/solutions to at least some of the examples/exercises. I do not mind a rigorous approach to the subject as long as the content is explained with detail. Which of the books mentioned above fits best the description? Also, if you have other books in mind, feel free to recommend them as well. Thanks in advance! Note: I have taken two proof-based calculus classes and one proof-based linear algebra class.",,"['multivariable-calculus', 'soft-question', 'manifolds', 'vector-analysis']"
93,Differential of a function as a limit,Differential of a function as a limit,,"Based on intuitive definitions of the differential of a function, it seems to me that for $f:\mathbb{R}^n \to \mathbb{R}^m$ something like $$ df(a) = \lim_{|r| \to 0} \frac{|f(a+r)-f(a)|}{|r|} $$ should hold for $a \in \mathbb{R}^n$ , although this may be an abuse of notation. I haven't been able to prove it from the usual definition I've encountered that the differential is the linear map defined by $$ df(a)(v_1, \dots, v_n) = \left(\sum_{k=1}^n \frac{\partial f_1}{\partial x_k}(a)v_k, \dots, \sum_{k=1}^n \frac{\partial f_m}{\partial x_k}(a)v_k\right). $$ Does any such limit definition of the differential of a function exist? My motivation for asking this is from Narasimhan's Analysis on Real and Complex Manifolds where he uses in a proof that for $g: \mathbb{R}^n \to \mathbb{R}^n$ , $(dg)(0)=0$ implies that there exists a neighborhood $W$ of $0$ such that for any $x,y \in W$ , $$ |g(x)-g(y)| \leq \frac{1}{2}|x-y|. $$ This would follow directly from the limit formula that I gave, but is there a more rigorous way to show this? Edit: Of course, in the book $g$ is also specified to be continuously differentiable on an open subset $\Omega \subset \mathbb{R}^n$ containing the origin.","Based on intuitive definitions of the differential of a function, it seems to me that for something like should hold for , although this may be an abuse of notation. I haven't been able to prove it from the usual definition I've encountered that the differential is the linear map defined by Does any such limit definition of the differential of a function exist? My motivation for asking this is from Narasimhan's Analysis on Real and Complex Manifolds where he uses in a proof that for , implies that there exists a neighborhood of such that for any , This would follow directly from the limit formula that I gave, but is there a more rigorous way to show this? Edit: Of course, in the book is also specified to be continuously differentiable on an open subset containing the origin.","f:\mathbb{R}^n \to \mathbb{R}^m 
df(a) = \lim_{|r| \to 0} \frac{|f(a+r)-f(a)|}{|r|}
 a \in \mathbb{R}^n 
df(a)(v_1, \dots, v_n) = \left(\sum_{k=1}^n \frac{\partial f_1}{\partial x_k}(a)v_k, \dots, \sum_{k=1}^n \frac{\partial f_m}{\partial x_k}(a)v_k\right).
 g: \mathbb{R}^n \to \mathbb{R}^n (dg)(0)=0 W 0 x,y \in W 
|g(x)-g(y)| \leq \frac{1}{2}|x-y|.
 g \Omega \subset \mathbb{R}^n","['multivariable-calculus', 'differential-geometry', 'differential']"
94,"For what values of $r$ is $f(x,y,z)$ continuous on $\mathbb{R}^3$?",For what values of  is  continuous on ?,"r f(x,y,z) \mathbb{R}^3","This was one of the problems in my textbook. I've tried searching for an answer but they are all blocked by a paywall! So I want to see if my approach is correct. The question asks: For what values of the number $r$ is the function $$f(x,y,z) = \begin{cases} \dfrac{(x+y+z)^r}{x^2+y^2+z^2} & (x,y,z)\neq(0,0,0) \\[1ex] 0 & (x,y,z) = (0,0,0) \end{cases}$$ continuous on $\Bbb R^3$ ? Here is my approach, is it correct? Keep in mind that this is a draft and I am aware that some parts may be unclear and require further justification. I just want to make sure that the general argument makes sense. Let $\mathbf{r}(t)=\langle f(t), g(t), h(t) \rangle$ be the path that the point $(x,y,z)$ goes along as it approaches $(0,0,0)$ . Thus, since $\mathbf{r} \neq \mathbf{0}$ the function becomes $$ f(\mathbf{r}(t)) =\frac{(f(t)+g(t)+h(t))^r}{f(t)^2 + g(t)^2 + h(t)^2} $$ Notice that if $\mathbf{r}(t) \neq \mathbf{0}$ , then $f$ exists and the denominator in $f(x,y,z)$ is equal to $\|\mathbf{r}\|^2$ . Also notice that $f(t)+g(t)+h(t) = \mathbf{r}(t) \cdot \langle 1,1,1 \rangle$ . This in turn is equivalent to $\|\mathbf{r}\| \cdot \sqrt{3} \cdot \cos\theta$ . Thus $f$ becomes $$ f(\mathbf{r}(t)) =\frac{\left(\|\mathbf{r}(t)\| \cdot\sqrt{3} \cdot \cos\theta\right)^r}{\|\mathbf{r}(t)\|^2} $$ Let $t_0$ be the number such that $\mathbf{r}(t_0) = \mathbf{0}$ . Recall that a function $f$ is continuous if $\lim_{\mathbf{x}\to \mathbf{a}} f(\mathbf{x}) = f(\mathbf{a})$ , so if $\lim_{t\to t_0} f(\mathbf{r}(t)) = 0$ then $f$ is continuous at $0$ ( $f$ is already continuous anywhere else, assuming that it's defined there), which would imply that $f$ is continuous on $\mathbb{R}^3$ . If $r<0$ then $f$ the limit cannot exist at $(0,0,0)$ (I will make this statement more exact in the real proof, I'm running out of time here). Thus it must be the case that $r>0$ so $(\sqrt{3} \cos\theta)^r$ must exist. So $$ \lim_{t\to t_0} \frac{\left(\|\mathbf{r}\| \cdot \sqrt{3} \cdot \cos\theta \right)^r}{\|\mathbf{r}\|^2} = (\sqrt{3}\cos\theta)^r \lim_{t\to t_0} \frac{\|\mathbf{r}\|^r}{\|\mathbf{r}\|^2} $$ If we do the substitution $u=\|\mathbf{r}(t)\|$ ( $u>0$ so the limit approaches $0$ from both sides, just think of it as $|x|$ ) then the limit is $$ \lim_{u\to 0} \frac{u^r}{u^2}  $$ It can be shown that (and I will show it in the real proof) if $r > 2$ then this limit exists and is equal to $0$ . Therefore if $r>2$ then $f(x,y,z)$ is continuous on $\Bbb R^3$ .","This was one of the problems in my textbook. I've tried searching for an answer but they are all blocked by a paywall! So I want to see if my approach is correct. The question asks: For what values of the number is the function continuous on ? Here is my approach, is it correct? Keep in mind that this is a draft and I am aware that some parts may be unclear and require further justification. I just want to make sure that the general argument makes sense. Let be the path that the point goes along as it approaches . Thus, since the function becomes Notice that if , then exists and the denominator in is equal to . Also notice that . This in turn is equivalent to . Thus becomes Let be the number such that . Recall that a function is continuous if , so if then is continuous at ( is already continuous anywhere else, assuming that it's defined there), which would imply that is continuous on . If then the limit cannot exist at (I will make this statement more exact in the real proof, I'm running out of time here). Thus it must be the case that so must exist. So If we do the substitution ( so the limit approaches from both sides, just think of it as ) then the limit is It can be shown that (and I will show it in the real proof) if then this limit exists and is equal to . Therefore if then is continuous on .","r f(x,y,z) = \begin{cases} \dfrac{(x+y+z)^r}{x^2+y^2+z^2} & (x,y,z)\neq(0,0,0) \\[1ex] 0 & (x,y,z) = (0,0,0) \end{cases} \Bbb R^3 \mathbf{r}(t)=\langle f(t), g(t), h(t) \rangle (x,y,z) (0,0,0) \mathbf{r} \neq \mathbf{0} 
f(\mathbf{r}(t)) =\frac{(f(t)+g(t)+h(t))^r}{f(t)^2 + g(t)^2 + h(t)^2}
 \mathbf{r}(t) \neq \mathbf{0} f f(x,y,z) \|\mathbf{r}\|^2 f(t)+g(t)+h(t) = \mathbf{r}(t) \cdot \langle 1,1,1 \rangle \|\mathbf{r}\| \cdot \sqrt{3} \cdot \cos\theta f 
f(\mathbf{r}(t)) =\frac{\left(\|\mathbf{r}(t)\| \cdot\sqrt{3} \cdot \cos\theta\right)^r}{\|\mathbf{r}(t)\|^2}
 t_0 \mathbf{r}(t_0) = \mathbf{0} f \lim_{\mathbf{x}\to \mathbf{a}} f(\mathbf{x}) = f(\mathbf{a}) \lim_{t\to t_0} f(\mathbf{r}(t)) = 0 f 0 f f \mathbb{R}^3 r<0 f (0,0,0) r>0 (\sqrt{3} \cos\theta)^r 
\lim_{t\to t_0} \frac{\left(\|\mathbf{r}\| \cdot \sqrt{3} \cdot \cos\theta \right)^r}{\|\mathbf{r}\|^2} = (\sqrt{3}\cos\theta)^r \lim_{t\to t_0} \frac{\|\mathbf{r}\|^r}{\|\mathbf{r}\|^2}
 u=\|\mathbf{r}(t)\| u>0 0 |x| 
\lim_{u\to 0} \frac{u^r}{u^2} 
 r > 2 0 r>2 f(x,y,z) \Bbb R^3","['calculus', 'multivariable-calculus', 'continuity']"
95,How do we use the right hand rule for Stokes' theorem?,How do we use the right hand rule for Stokes' theorem?,,"Let $C$ be the intersection curve between the plane $z = 10 - x - y$ and the cylinder $x^2+y^2 = 1$ , oriented such that the projection of the curve onto the xy-plane is positively oriented. Determine the work done by the force field $\mathbf{F} = (x, x^3, z^3)$ in circulating around the curve $C$ . Long story short I parameterized the curve using polar coordinate: $$r(t,r) = (r\cos(t), r\sin(t), 10 - r\cos(t) - r\sin(t))$$ Then I partial differentiated with respect to t and r and then took their cross product to get the normal vector: $$r_{t} \times r_{r} = (-r, -r, -r)$$ The curl of $F$ became $F = (0,0, 3x^2) = (0,0, 3r^2\cos^2(t))$ and the dot product between the normal and $F$ is then $-3r^3\cos^2(t)$ . $$\iint_{D} -3r^3\cos^2(t) = -\frac{3\pi}4$$ The correct answer is positive, and it got me thinking is because it says in the question that curve projection on the xy-plane has a positive orientation and using the right hand rule, the normal points upwards. This means that I've to change the direction of my normal vector right? Is this the only reason? While we are on the subject, I don't need to use Jacobian determinant right? As in multiply by ""r"" when I parametrize it. Is it because I am already calculating the change in the area caused by the variable change when I take the cross product?","Let be the intersection curve between the plane and the cylinder , oriented such that the projection of the curve onto the xy-plane is positively oriented. Determine the work done by the force field in circulating around the curve . Long story short I parameterized the curve using polar coordinate: Then I partial differentiated with respect to t and r and then took their cross product to get the normal vector: The curl of became and the dot product between the normal and is then . The correct answer is positive, and it got me thinking is because it says in the question that curve projection on the xy-plane has a positive orientation and using the right hand rule, the normal points upwards. This means that I've to change the direction of my normal vector right? Is this the only reason? While we are on the subject, I don't need to use Jacobian determinant right? As in multiply by ""r"" when I parametrize it. Is it because I am already calculating the change in the area caused by the variable change when I take the cross product?","C z = 10 - x - y x^2+y^2 = 1 \mathbf{F} = (x, x^3, z^3) C r(t,r) = (r\cos(t), r\sin(t), 10 - r\cos(t) - r\sin(t)) r_{t} \times r_{r} = (-r, -r, -r) F F = (0,0, 3x^2) = (0,0, 3r^2\cos^2(t)) F -3r^3\cos^2(t) \iint_{D} -3r^3\cos^2(t) = -\frac{3\pi}4","['calculus', 'multivariable-calculus', 'definite-integrals', 'stokes-theorem', 'greens-theorem']"
96,Difficulty visualising surfaces and translating them into surface integral,Difficulty visualising surfaces and translating them into surface integral,,"I am reviewing old exam questions, and I am currently doubting everything that I have worked on for the past 3 weeks. The question is as follows [Solution is below, translated from Swedish] The surface $S$ in $\mathbb{R}^3$ is given as the intersection of the cylinder $x^2 + y^2 ≤ 1$ and the surface defined by $x^2 + z^2\leq 1$ with $z\geq 0$ . The vector field $F = (0, 0, x^2 + y^2)$ . (a) Give a parametrization of the surface $S$ . (b) Determine $$\iint\limits_{S} F \cdot N \, dS$$ where N is the unit normal field on $S$ with positive $z$ -coordinate. (3) Now here is where I am getting confused: I am assuming that S is the union of $S_1$ and $S_2$ , why are we adding $S_1$ and $S_2$ to the union only to then apply Gauss's divergence theorem? Why is the normal vector for $S_1$ directed downwards and not upwards? why would you parametrise the surface like that and not in cylindrical coordinates where you take $z$ between $0$ and $1$ ? I appreciate any help!","I am reviewing old exam questions, and I am currently doubting everything that I have worked on for the past 3 weeks. The question is as follows [Solution is below, translated from Swedish] The surface in is given as the intersection of the cylinder and the surface defined by with . The vector field . (a) Give a parametrization of the surface . (b) Determine where N is the unit normal field on with positive -coordinate. (3) Now here is where I am getting confused: I am assuming that S is the union of and , why are we adding and to the union only to then apply Gauss's divergence theorem? Why is the normal vector for directed downwards and not upwards? why would you parametrise the surface like that and not in cylindrical coordinates where you take between and ? I appreciate any help!","S \mathbb{R}^3 x^2 + y^2 ≤ 1 x^2 + z^2\leq 1 z\geq 0 F = (0, 0, x^2 + y^2) S \iint\limits_{S} F \cdot N \, dS S z S_1 S_2 S_1 S_2 S_1 z 0 1","['multivariable-calculus', 'divergence-theorem']"
97,Is it necessary to use critical points in this exercise?,Is it necessary to use critical points in this exercise?,,"I have a question regarding the resolution of the following exercise: Let be a quadratic form $f:\mathbb{R}^n \to \mathbb{R}$ , $f(x) = \langle A \cdot x, x \rangle$ , where $A$ is a symmetric matrix. Show that if $u \in \mathbb{R}^n$ is a critical point of restriction $f \vert_{S^{n-1}}$ then $Au = \lambda u$ , where $\lambda = f(u)$ . Comments: I thought of the following solution: Let $u \in S^{n-1}$ . Then $f(u) = \langle Au, u \rangle$ . On the other hand, $f(u) = f(u)\langle u, u \rangle = \langle f(u)u, u\rangle$ . Thus, $Au = f(u)u$ . However, I did not use the hypothesis, but I find where the error is. Thank you for your help.","I have a question regarding the resolution of the following exercise: Let be a quadratic form , , where is a symmetric matrix. Show that if is a critical point of restriction then , where . Comments: I thought of the following solution: Let . Then . On the other hand, . Thus, . However, I did not use the hypothesis, but I find where the error is. Thank you for your help.","f:\mathbb{R}^n \to \mathbb{R} f(x) = \langle A \cdot x, x \rangle A u \in \mathbb{R}^n f \vert_{S^{n-1}} Au = \lambda u \lambda = f(u) u \in S^{n-1} f(u) = \langle Au, u \rangle f(u) = f(u)\langle u, u \rangle = \langle f(u)u, u\rangle Au = f(u)u","['real-analysis', 'calculus', 'analysis', 'multivariable-calculus']"
98,How to use implicit function theorem and Lagrange multiplier to prove there exists a local minimum?,How to use implicit function theorem and Lagrange multiplier to prove there exists a local minimum?,,"Assume f(x,y) and g(x,y) are two smooth functions on $R^2$ Let $$S=\left \{ (x,y)|f(x,y)=0 \right \} $$ , and $$p=(a,b)\in S$$ such that $$\frac{\partial f}{\partial x} (p) = -4,\space\frac{\partial f}{\partial y} (p) = 2,\space\frac{\partial g}{\partial x} (p) = 12,\space \frac{\partial g}{\partial y} (p)=-6$$ And $$\begin{bmatrix} \frac{\partial^2}{\partial x^2}f(p)  &  \frac{\partial^2}{\partial x\partial y}f(p)  \\  \frac{\partial^2}{\partial y\partial x}f(p)  & \frac{\partial^2}{\partial x^2}f(p)\end{bmatrix}=\begin{bmatrix} 1 &2 \\ 2 &4\end{bmatrix}$$ $$\begin{bmatrix} \frac{\partial^2}{\partial x^2}g(p)  &  \frac{\partial^2}{\partial x\partial y}g(p)  \\  \frac{\partial^2}{\partial y\partial x}g(p)  & \frac{\partial^2}{\partial x^2}g(p)\end{bmatrix}=\begin{bmatrix} 3&-1 \\ -1 &2\end{bmatrix}$$ How to prove $ \exists \space R> 0 $ such that $\forall \space q \in {S}\cap\left \{(x,y)| x^2+y^2< R^2 \right \} $ there is always g(p)< g(q) The only thing I am certain is we can prove this by implicit function theorem and Lagrange multiplier, but other than that, I have absolutely no clue where I should start. Could someone give me some hint?","Assume f(x,y) and g(x,y) are two smooth functions on Let , and such that And How to prove such that there is always g(p)< g(q) The only thing I am certain is we can prove this by implicit function theorem and Lagrange multiplier, but other than that, I have absolutely no clue where I should start. Could someone give me some hint?","R^2 S=\left \{ (x,y)|f(x,y)=0 \right \}  p=(a,b)\in S \frac{\partial f}{\partial x} (p) = -4,\space\frac{\partial f}{\partial y} (p) = 2,\space\frac{\partial g}{\partial x} (p) = 12,\space \frac{\partial g}{\partial y} (p)=-6 \begin{bmatrix} \frac{\partial^2}{\partial x^2}f(p)  &  \frac{\partial^2}{\partial x\partial y}f(p)  \\  \frac{\partial^2}{\partial y\partial x}f(p)  & \frac{\partial^2}{\partial x^2}f(p)\end{bmatrix}=\begin{bmatrix} 1 &2 \\ 2 &4\end{bmatrix} \begin{bmatrix} \frac{\partial^2}{\partial x^2}g(p)  &  \frac{\partial^2}{\partial x\partial y}g(p)  \\  \frac{\partial^2}{\partial y\partial x}g(p)  & \frac{\partial^2}{\partial x^2}g(p)\end{bmatrix}=\begin{bmatrix} 3&-1 \\ -1 &2\end{bmatrix}  \exists \space R> 0  \forall \space q \in {S}\cap\left \{(x,y)| x^2+y^2< R^2 \right \} ","['multivariable-calculus', 'maxima-minima', 'implicit-function-theorem']"
99,Linkage between Lagrange multipliers and eigenvectors/eigenvalues?,Linkage between Lagrange multipliers and eigenvectors/eigenvalues?,,"I am a Freshmen Engineering Student, and this past Semester I took an Intro Multivariable Calculus Course, where we covered everything up to Lagrange Multipliers and Space Curves but not things like Surface and Line Integrals. At the same time, I also took an Intro to Linear Algebra Course that covered everything up to Determinanats, Eigenvectors/Eigenvalues and threw in Orthogonalization and Gram-Schmidt Orthonormalization. The set up for Eigenvalues is the solution to: A $\vec x$ = $\psi$$\vec x$ Simultaneously, we where taught that the method to find Lagrange Multipliers with 1 constraint was the solution to: $\nabla$$f(x, y)$ = $\lambda$$\nabla$$g(x, y)$ resulting in 3 equations: $\frac{\partial f}{\partial y}$ = $\frac{\partial g}{\partial y}$ $\frac{\partial f}{\partial x}$ = $\frac{\partial g}{\partial x}$ $g(x, y) = c$ Where $g(x, y) = c$ is the appropriate Level Curve given by the restraint conditions. With these equations, your unknowns are the appropriate $(x, y)$ and the corresponding $\lambda$ . My Multi Professor then said, ""From here it is as simple as solving the given system of equations and checking the whether or not they are Maximum or Minimums."" Myself and a friend of mine, whom is in the same Linear Class as I, immediately wondered if there is a way to use Linear Algebra to solve the system of equations, as we realized that the set up of: $\nabla$$f(x, y)$ = $\lambda$$\nabla$$g(x, y)$ looks remarkable similar to: A $\vec x$ = $\psi$$\vec x$ We realized quickly that if we consider the Linear Transformation Matrix: A : $\mathbb{R} ^ 3 \mapsto \mathbb{R} ^ 3$ $\nabla$$f(x, y)$ = A $\cdot$ $\nabla$$g(x, y)$ Then we can set up the entire Lagrange Multiplier Process as: A $\cdot$ $\nabla$$g(x, y)$ = $\psi$$\nabla$$g(x, y)$ and then from there we can solve for the Eigenvalues, which will be our Lagrange Multiplies, and our Eigenvectors, which will be our Critical Points. Now this seemed all well and good, and we confirmed with our Linear Professor that this set-up was valied, but we came upon a major questions that left all 3 of us stumped. Our Linear Professor told us that he would dig into it things a little more, as he started spewing math things that where way above my friend and I's head, and I figured this is a good place to ask them. How would we find the Transformation Matrix from $f(x, y)$ to $g(x, y)$ . This entire process hinges upon finding that Transformation Matrix, and assuming that you can find said Matrix and assuming that is is a valid Linear Transformation, then how would one go about finding it?","I am a Freshmen Engineering Student, and this past Semester I took an Intro Multivariable Calculus Course, where we covered everything up to Lagrange Multipliers and Space Curves but not things like Surface and Line Integrals. At the same time, I also took an Intro to Linear Algebra Course that covered everything up to Determinanats, Eigenvectors/Eigenvalues and threw in Orthogonalization and Gram-Schmidt Orthonormalization. The set up for Eigenvalues is the solution to: A = Simultaneously, we where taught that the method to find Lagrange Multipliers with 1 constraint was the solution to: = resulting in 3 equations: = = Where is the appropriate Level Curve given by the restraint conditions. With these equations, your unknowns are the appropriate and the corresponding . My Multi Professor then said, ""From here it is as simple as solving the given system of equations and checking the whether or not they are Maximum or Minimums."" Myself and a friend of mine, whom is in the same Linear Class as I, immediately wondered if there is a way to use Linear Algebra to solve the system of equations, as we realized that the set up of: = looks remarkable similar to: A = We realized quickly that if we consider the Linear Transformation Matrix: A : = A Then we can set up the entire Lagrange Multiplier Process as: A = and then from there we can solve for the Eigenvalues, which will be our Lagrange Multiplies, and our Eigenvectors, which will be our Critical Points. Now this seemed all well and good, and we confirmed with our Linear Professor that this set-up was valied, but we came upon a major questions that left all 3 of us stumped. Our Linear Professor told us that he would dig into it things a little more, as he started spewing math things that where way above my friend and I's head, and I figured this is a good place to ask them. How would we find the Transformation Matrix from to . This entire process hinges upon finding that Transformation Matrix, and assuming that you can find said Matrix and assuming that is is a valid Linear Transformation, then how would one go about finding it?","\vec x \psi\vec x \nablaf(x, y) \lambda\nablag(x, y) \frac{\partial f}{\partial y} \frac{\partial g}{\partial y} \frac{\partial f}{\partial x} \frac{\partial g}{\partial x} g(x, y) = c g(x, y) = c (x, y) \lambda \nablaf(x, y) \lambda\nablag(x, y) \vec x \psi\vec x \mathbb{R} ^ 3 \mapsto \mathbb{R} ^ 3 \nablaf(x, y) \cdot \nablag(x, y) \cdot \nablag(x, y) \psi\nablag(x, y) f(x, y) g(x, y)","['linear-algebra', 'matrices', 'multivariable-calculus', 'eigenvalues-eigenvectors', 'lagrange-multiplier']"
